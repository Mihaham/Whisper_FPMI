Давайте я напомню, чем мы закончили.
Значит, Jensen для условных средних.
Выглядит это так.
Выпуклая функция от условного мат ожидания оценивается через условные
мат ожидания от композиций, значит, V выпуклая.
Ну вот мы закончили на том, что я предложил это доказать каким-нибудь способом.
Ну и вот указал стандартный способ.
Это делается сначала для функций простых, ну а потом, если это для простых верно, то с помощью приближений
распространяется. Ну это стандартный способ.
А другой способ я предложил такой.
Просто проверить это, ну попытаться проверить это по определению.
Ведь тут что нужно, значит, смотрите, что такое неравенство для условных мат ожиданий для функций.
Это значит неравенство для интегралов по множествам из сигма-логебры.
Поэтому такой как бы напрашивающий способ проинтегрировать по множеству и пытаться применить обычного Янцина.
Но я потом стал в электричке это обдумывать и понял, что таким способом, наверное, не сделаешь.
Ну вот не знаю, может быть, у кого-нибудь из вас так получилось сделать.
Никто не думал над этим. Ну вот попробуйте, это любопытно, можно ли так сделать.
Ну а стандартный способ такой. Сначала для простых, потом для ограниченных измеримых, потом для всех.
Эти дальнейшие шаги рутинные, а давайте посмотрим вот этот первый шаг.
Тут уже понятно, что если для каких-то функций верно, и они равномерно сходятся, ну в пределе получится.
А для общих, ну там можно монотонные устаревать пределы. Это уже рутинная вещь.
А вот давайте посмотрим, как это сделать для простых функций.
Значит, смотрите, кто такая простая функция?
Значит, простая функция это линейная комбинация нескольких индикаторов, множеств измеримых.
Но, конечно же, не обязательно из-под сигма-алгебры. У нас ведь какая ситуация?
У нас под сигма-алгебра, значит, она часть большой сигма-алгебры.
И функции измеримые относительно вот этой большой.
А если бы функция была измерима относительно А, то тут вообще делать было бы нечего,
потому что тут тогда условная мат ожидания совпадает с самой функцией.
Поэтому содержательно это становится, когда функция как раз F относительно A не измерима, а измерима относительно большей.
И вот тогда, значит, для такой функции как выглядит условная мат ожидания?
Ну, условная мат ожидания это, очевидно, будет вот что.
Это из-за линейности.
Значит, константы вынесутся, и здесь будет стоять вот что.
Значит, вот так выглядит условная мат ожидания.
Давайте теперь заметим, что сумма индикаторов множеств...
Да, я забыл сказать, тут я имею в виду, конечно, что они дизюнкные.
Значит, они дизюнкные, так, эти множества.
Так всегда можно простую функцию записать с помощью дизюнкных множеств, перечислив все ее разные значения.
Значит, смотрите, что у нас есть. У нас поточечна, вот это, раз это разбиение пространства, то сумма их индикаторов поточечна тождественно равна единице.
Поэтому, когда мы переходим к условному мат ожиданию, то для условного мат ожидания получается вот такая вещь.
Значит, вот это равно единице почти всюду.
Ну, конечно, здесь тоже, как всегда для измеримых функций, тут, конечно, неравенство тоже почти всюду.
Значит, смотрите, от единицы условное мат ожидания единицы.
Но условное мат ожидания, как мы помним, это не одна какая-то функция, это целый класс измеримых между собой функций.
Ну, конечно, когда у вас это верно почти всюду, можно заменить на версии так, что будет просто всюду верно.
Значит, смотрите, какая тут оказалась интересная вещь.
Еще с учетом того, что вот эти неотрицательные, смотрите, что у нас получается.
У нас получились при каждом х, при каждом х, когда это неотрицательно и это единица.
Это просто числа неотрицательные, сумма единицы.
И тогда получаем, что почти всюду, ну, то есть всюду, где сумма единицы и они неотрицательные, а это все почти всюду выполнено.
А у нас тут стоит, вот тут стоит, выпуклая комбинация чисел.
Вот здесь стоит, видите, выпуклая комбинация чисел.
Поэтому получается v от того, что тут написано, функция v выпуклая.
Это значит, что когда аргумент есть выпуклая комбинация чего-то, то значение оценивается выпуклой комбинацией этих чего-то.
И поэтому получается, из-за выпуклости получается вот такая вещь.
Сейчас, только я не те числа стал вставлять.
Наоборот, вот эти числа, они умножаются на значения, вот так.
Потому что эти значения, берется их выпуклая комбинация.
Вот как раз вот эти вот условные мат ожидания, они выступают как коэффициенты.
А что же такое написано в правой части?
В правой части как раз написано условное мат ожидания тоже простой функции v от f.
Потому что f принимало значения c1, cn на каких-то множествах дизюнкных.
А v от f соответственно принимает значения на этих же дизюнкных множествах, но другие значения, в которые v подставили значение f.
Но это и есть, это тоже простая функция.
И точно также вот эта сумма, это есть ее условное мат ожидания.
Потому что v от f, кто такой v от f в этом представлении?
А в этом представлении это такая же штука, но тут вместо c появились v от c.
Ну вот так что видите, получается, что для простых функций это более-менее следствие, непосредственно можно сказать следствие выпуклости, определение выпуклости.
У нас такое определение выпуклости функции можно считать.
А дальше, значит дальше, от простых к ограниченным равномерный предел, а от ограниченных к неограниченным, ну вот с помощью срезок там дальнейший предел.
Но это я уже не буду на этом останавливаться.
При предельном переходе в каком месте?
Нет, нет, но почему же от простых к ограниченным как раз все окей.
Потому что представьте себе, что f равномерно сходится fn, ну вот пусть для fn это верно, и fn равномерно сходится к f.
Ну тогда композиции будут равномерно сходиться, выпуклая функция она же не прывна, а и fn тогда в этом случае принимают значение в каком-то отрезке.
Нет, ну как, если функции сходятся равномерно, то с предельным переходом под интегралом проблем не возникает.
Так что смотрите, если, давайте вот это я уберу, значит с этим закончили.
Значит смотрите, если fn равномерно сходится к f, то v от fn равномерно сходится к v от f.
Дальше, из сходимости равномерной функции следует, если fn сходится равномерно к f, то из свойств условных средних следует, что и условные средние тоже равномерно сходятся.
Так что, ну а поскольку всякую ограниченную измеримую можно равномерно приблизить простыми, то для ограниченных всё окей.
Больше забот может доставить следующий случай, смотрите, какой следующий случай. Следующий случай, когда вы это знаете для ограниченных всех измеримых.
А функция f у вас не ограниченная, вот тогда как быть, вот тогда возникает какой вопрос.
Если функция f уже не обязательно ограниченная, тогда, разумеется, в этом случае требуется, чтобы она была интегрируема, конечно, и чтобы вот это была интегрируема.
Потому что, если вы выпуклую подставили ограниченную, она может перестать быть интегрируемой.
Ну, например, была у вас f интегрируемая, а f квадрат нет, вот вы её в квадрат подставили, потеряли интегрируемость.
Поэтому в этом случае, конечно, дополнительно требуется, что это тоже интегрируемо.
Вот тогда вот это тут уже равномерных, тут уже равномерными переходами не обойтись.
Тут равномерными переходами не обойтись.
А значит, тут это надо отдельно обосновывать.
Ну, в этом случае, в этом случае используются стандартные срезки.
В этом случае функция обрезается.
Сначала замечаем, что равенство в силе, когда функция заменена на такую, там, где была от минус n до n, она прежняя.
Там, где она больше n, она от n делаем принудительно, а там, где меньше минус n, делаем минус n принудительно.
Для таких верно.
Ну и после этого, после этого надо проверить.
Давайте это не будем делать, потому что это нас как-то совсем уведёт куда-то в прошлый семестр, в интегралы.
В этом случае надо проверить, что будет по-прежнему сходимость.
Этот последний шаг, это действительно надо проверить.
Ну, в этом случае смотрите, что получается.
Когда вы функцию так заменяете на такие срезки, так, ну вот давайте я их напишу, на такие заменяете.
Значит, f, это если f по модулю меньше либо равно n, n если f больше n, и минус n если f меньше минус n.
Вот на такие заменяете.
То такие функции сходятся к f в L1.
Значит, они в среднем сходятся.
А v от них, v от них сходятся тоже в среднем к v от f.
Ну, это в общем довольно легко проверить.
И поэтому нам нужно ещё воспользоваться тем, нам нужно воспользоваться таким, таким, значит, свойством.
Ну, это свойство вытекает из того, что мы обсуждали.
Если phi n-ные сходятся в L1 к phi, то а условные мат ожидания сходятся в L1 к условному мат ожидания.
Ещё нам такое свойство.
Ну, это свойство вытекает из того способа построения условного среднего, который мы обсуждали.
Мы обсуждали, что интеграл, ну, мы обсуждали, что среднее, взятие условного среднего, это такое на пространстве L1, это такое сжатие, оно норму не увеличивает.
Поэтому следствием того, что у нас, смотрите, условное среднее, это такой линейный оператор на L1, который не увеличивает норму и на функциях A измеримых тождественной.
Ну, значит, на самом деле вот примерно этим всё и определяется.
И поэтому получается такое свойство.
Ну, тогда понятно, что можно дальше, ну, тогда вот можно такими пользоваться и переходить к пределу с помощью этого соображения в нерайности.
Но из Янсона следует, что условное среднее, что оно сжатие также на всех Lp.
Значит, следствие, значит, если P, значит, уже не обязательно единица, так, и F лежит в Lp, то тогда условное среднее, условное среднее тоже лежит в Lp.
И норма, ну, давайте, чтобы нормы не писать, давайте напишем через мат ожидания.
Мат ожидания условного среднего в степени P оценивается через мат ожидания просто самого F.
Ну, это надо применить Янсона.
К какой функции? Ну, понятно, к какой.
Нужно в качестве Vat взять модуль T в степени P.
Вот такую, ну, разумеется, надо убедиться, что это выпуклая функция.
Как убедиться, что это выпуклая функция?
Ну, два, например, дважды продиференцировав ее и воспользовавшись тем, что получится неотрицательное производное.
Значит, применив эту функцию, ну, собственно, как применяется здесь это нерая инстинкция Янсона?
Тут ведь так, чтобы его применять, вообще-то ведь надо уже заранее знать, что Vat F интегрируемо.
А в этом следствии это часть утверждения.
Видите, так что, чтобы вот этого написать, надо в Янсоне знать, что эта штука в Lp, что условное среднее в Lp.
Вот если мы это уже знаем, то Янсон говорит, что будет вот так.
Но откуда мы знаем, что условное среднее от функции за Lp, что она тоже в Lp?
Но это опять с помощью срезок.
Когда мы обрезаем эту функцию вот тем стандартным способом, то что же получается?
Получается, что вот те срезки сходятся к нашей функции в Lp, и их нормы, их Lp-нормы, оцениваются ее Lp-нормой.
Но тогда для них это не равнится, можно применять, они же ограниченные срезки.
Ну и тогда получается, что тут можно, как обычно это делают, когда что-то оценки какие-то доказывают,
сводят к функциям неотрицательным.
Если функция f неотрицательна, то вот этой части с минусом нет, и просто она по уровню n обрезается.
И тогда эти срезки к ней возрастают, ну и тогда видно, что условные средние возрастают к условному среднему.
Ну и поскольку все они в Lp ограничены, то предел тоже ограничен в Lp.
В итоге оказывается и функция нужная в Lp, и норма ее нужным образом оценивается.
Поэтому получается, видите, что условное среднее, оно не только на L1-сжатие, как это было по исходному построению,
но и на всех Lp. Ну я тут, кстати, исключил бесконечность, ее, конечно, можно включить.
То, что для бесконечности это верно, то есть что Supremum оценивается, это тоже в базовых свойствах, так что это тоже верно, конечно.
Но тут уже никакой Jensen не требуется, поэтому я тут включил в следствие только то, что реально с Jensen связано.
Так, теперь, значит, вот этот надо рассматривать.
Так, теперь, значит, вот этот надо рассматривать как некое длинное, так сказать, техническое отступление,
которое нужно для того, чтобы обсудить оставшиеся два класса процессов.
Это Мартин Галлы и Марковские процессы.
Я вот напоминаю, что в нашем курсе несколько классов конкретных процессов,
ну самые важные это Виннеровский и Пуассоновский, и несколько классов общих процессов,
представителями которых Виннеровский и Пуассоновский являются.
А эти общие классы, вот у нас был класс процессов с независимыми превращениями,
потом у нас был класс Гауссовских процессов, и еще парочка сейчас появится там как раз,
где нужны вот эти условные и средние. Для тех двух условных никаких средних не нужно было.
Значит, появятся у нас еще сейчас процессы, значит, еще два класса процессов, Мартин Галлы и Марковский.
Ну и в конце у нас появится некий подкласс, но он такой очень специфический,
поэтому это отдельно обсуждается. Это Марковские цепи и ветвящиеся процессы, это просто отдельные примеры у нас будут.
Так что это будет некий такой микс, так сказать, представителей конкретных процессов и конкретных классов.
Значит, вот следующий раздел фильтрации и Мартин Галлы.
Значит, вот есть основное вероятностное пространство,
и есть набор под сигма-алгебр, значит, вот в этой основной сигма-алгебре,
чтобы подчеркнуть, что это сигма-алгебры некой другой природы, я их буду обозначать другой буквы F,
чтобы не путать с основной сигма-алгеброй даже и без индексов.
Но при этом давайте предполагать, что T из какого-то множества напрямой,
но в действительности для определения Мартин Галла это не очень важно, и для определения фильтрации важно, чтобы это множество индексов было упорядочено.
Но для наших целей никаких других множеств индексов, кроме таких стандартных, что это полупрямая или натуральные числа или отрезок,
никаких других у нас не будет кроме этих стандартных.
Теперь говорят, что вот эта вот Ft фильтрация, если F от s содержится в F от t при s меньше t.
Я смотрю, чтобы не сойти с обозначений конспекта, потому что тут канонических каких-то нет обозначений, но, естественно, я стараюсь следовать тем, что в конспекте.
Смотрите, в большинстве приложений, какой практический смысл этой фильтрации.
T, как обычно, символизирует время, множество t символизирует время, а F от t это сигма-алгебра событий о наступлении или ненаступлении которых до момента времени t стало известно.
Вот в большинстве реальных задач и приложений вот такой практический смысл, и вот ради этого все это и вводится, чтобы обсуждать такие события.
Ну и вот определение. Процесс Xat согласован с фильтрацией,
если Xat Ft измеримо для всех t, ну из этого множество индексов.
Ну вот банальный пример. Ну он банальный, но, как ни странно, бывает часто полезен.
F от t это сигма-алгебра, порожденная случайными величинами Xs до момента t включительно.
Вот видите, пожалуйста, получается фильтрация, потому что с ростом t это увеличивается.
И по определению Xat t измерим. Так что вот часто такой используемый пример.
Теперь важное определение.
Сейчас, ну раз оно важное, давайте я даже сотру все, что было.
Значит, определение. Так давайте я тоже, чтобы смотрю, чтобы не было расхождений.
Прямо в точности буду писать, как конспекты.
Значит, мартингау относительно фильтрации.
Ну фильтрации еще, кстати, иногда называют потоками сигма-алгебр.
Если, ну это естественно процесс, если все вот эти входят в L1, то есть интегрируемые, значит у них есть условный средний.
И, значит, да, значит Xat согласован. Согласован с этой фильтрацией.
Ну то есть каждая Xt измерима относительно этой Ft.
И условное среднее Xat относительно Fs есть Xs при s меньше либо равном t.
Значит, это приобретает особо наглядный смысл, если эти функции лежат в L2, не в L1.
Я напомню, что когда функция из L2, то условное среднее это артагональная проекция на подпространство измеримых относительно сигма-алгебр функций.
И тут смотрите, что происходит. Когда мы варьируем вот этот индекс S, то это получаются такие возрастающие подпространства в L2.
И происходит следующее, когда вы функцию в момент времени t артагонально проектируете на подпространство с меньшим временем,
то должна получиться то, что было у этой функции в этот меньший момент времени.
То есть такая получается интересная спираль в Гильбертовом пространстве с такими согласованными проекциями.
У вас какая-то кривая функция, ксиатем можно считать, что такая кривая в Гильбертовом пространстве.
И у вас еще такая вот кривая сигма-алгебра, и они друг с другом согласованы.
Вот смысл этих артагональных проекций, когда вы проектируете с большего на меньшее, то получаете функцию в меньший момент времени.
Вот простейшие примеры, но они оказываются очень распространенными.
Значит давайте сделаем время. Сейчас только давайте я посмотрю, чтобы порядок примеров тоже совпадал с тем, что в конце.
Ну это конечно не обязательно, но все-таки лучше, чтобы так было.
Значит смотрите, пусть время это дискретное время, натуральные числа.
И ксиенные, сейчас только давайте будут не ксиенные, а этэнные независимые случайные величины.
Так, ну вот тут я уже кажется отошел от обозначений конспекта.
Да, ну ладно, не буду исправляться, немножко отошел.
Значит со средними, со средними ноль.
А ксиенные, это их сумма от 1 до n.
Ну вот у меня в конспекте сумма sn, и мартингал будет не ксин, а sn.
Но тут давайте, чтобы не было путаницы, раз в определении буква кси, то и пусть здесь будет буква кси.
Потому что, смотрите, мартингалом является не вот эта последовательность независимых, а мартингалом является последовательность сумм.
Видите, вот этих независимых.
Ну давайте сейчас это проверим, но прежде чем...
А, фильтрация, да, совершенно справедливый вопрос.
Фильтрация Fn это порожденная первыми n.
Ну то есть, так сказать, самопорожденная фильтрация.
Ну вообще эти самопорожденные фильтрации очень часто используются в качестве того относительно чего меряются всякие свойства мартингальные, марковские свойства и так далее.
Значит смотрите, тут еще уместно заметить про терминологию, что за странное название мартингал.
Но в точности не очень понятно, что имели в виду, так сказать, отцы-основатели, они это как-то не прокомментировали,
но в принципе есть, ну у слова мартингал есть два таких, так сказать, ну вот помимо вот этого уже чисто математического термина, то, так сказать, житейских, два таких есть значения.
Одно, это такое довольно старинное французское значение, это там часть, часть уздечки.
Ну имелось ли в виду это отцами-основателями, ну не очень ясно.
Еще это такое уже более позднее, но тоже довольно старое, тоже французское, такое немного жаргонное выражение для стратегии игры, ну вот такой типа, игры там типа Орлянки, в которой при проигрыше удваиваются ставки.
Ну вот что они имели в виду, ну вот не спросишь, так сказать, теперь.
Но термин, математический термин, это как раз в отличие от этих чисто лингвистических, он не очень старый, ну вот в тридцатых годах он возник.
Но теперь я думаю, что если наугад тыкнуть в интернет, то скорее всего большинство ссылок попадет скорее всего на этот математический термин, а вот не на те два, ну так сказать, несколько архаичных.
Значит, давайте проверим, значит, давайте проверим, почему, значит, это так.
Значит, проверка, пусть k меньше n, значит, следующее, что когда мы берем, проектируем на fk, то должно получиться вот это.
Ну понятно, что достаточно проверить для предыдущего, значит, достаточно проверить для k равного n-1, вот это достаточно проверить, потому что потом за несколько шагов можно к меньшему прийти будет.
Давайте, значит, тогда заметим, что эта штука, это же сумма, так, и в этой сумме смотрите, кто стоит.
Тут стоят, значит, n-, ну первые стоят, так, и потом стоит последняя, но первые измеримы относительно вот этой, так, ну давайте это я подробнее подробнее напишу.
Так, а значит, первые измеримы, поэтому это будет их сумма просто, значит, которая есть по определению, что у нас есть.
Так, а значит, первые измеримы, поэтому это будет их сумма просто, значит, которая есть по определению предыдущие, так, плюс условное мат ожидания последней штуки относительно сигма алгебры, порожденной предыдущими.
Так, и поэтому нам надо убедиться, что последнего просто нет, нам надо убедиться, что вот это равно нулю, вот в этом надо теперь убедиться.
Ну давайте в этом убедимся, давайте в этом убедимся.
Значит, смотрите, значит, что это значит, то есть это значит, что когда вы интегрируете по множеству a вот эту, это n, то это должно стать нулем для всех a,
для всех a из сигма алгебры, порожденной предыдущими.
А как устроена, как устроен индикатор такого множества?
Значит, смотрите, значит, это индикатор множества, ну или лучше сказать, функция измеримая относительно сигма алгебры, порожденной несколькими.
Но значит, из этого следует, что это есть результат подстановки в некую барелевскую функцию,
барелевскую на Rn-1. Вот этих функций, потому что множество измеримые, то есть множество из сигма алгебры, порожденные несколькими случайными величинами,
ну и вообще функции измеримой относительно сигма алгебры, порожденной несколькими случайными величинами, это не что иное, как результат подстановки этих случайных величин,
ну этих функций лучше сказать, ну в барелевские функции.
Когда вы перебираете всевозможные барелевские функции на Rn-1 и подставляете в них в качестве аргументов вот эти вот порождающие функции, то это дает вам все измеримые функции.
Но тогда, смотрите, что у вас получается, тогда у вас получается интеграл по омега вот от такой вот штуки умножить еще на вот это.
Но вспоминаем, что они независимы, так, поэтому это получается интеграл вот от этих на интеграл вот последний, значит, это независимость, вот это из-за независимости.
Значит, вспоминаем, что когда случайные величины независимы, то интеграл произведения раскалывается.
Но тут одна из них, это вот эта, а последняя с ними независима, значит, они раскололись.
Но интеграл ноль, потому что сейчас, я вот только забыл, я должен был указать, что они с нолевым от ожидания.
Но вот здесь как раз видно, зачем это надо. Здесь как раз видно, зачем это надо, чтобы мат ожидания было ноль.
Это как раз ровно для того, чтобы этот кусок пропал.
Ну вот, значит, видите, вот важный предел мартингала.
Значит, важность этого предела, важность этого примера еще и в том, что многие более общие мартингалы являются пределами таких вот, таких вот дискретных.
Это на самом деле такой вроде как игрушечный пример, но он на самом деле вот такой очень типичный из-за того, что пределами таких можно все описать.
Вот теперь давайте рассмотрим еще один пример, значит, еще один пример.
Значит, давайте рассмотрим еще один пример.
Значит, пример.
Пусть кси, ну какая-то интегрируемая случайная величина.
Пока никакого мартингала нет.
Но есть фильтрация.
Видите, в предыдущем примере не было фильтрации, а был сразу мартингал.
И мы под него подверстали фильтрацию.
Тут наоборот, пока мартингала никакого нет и процесса нет, а есть только фильтрация.
Значит, положим кси от t просто равным условному среднему.
Оно есть, потому что интегрируемо.
Значит, это получается мартингал.
Но это следует из свойств условных средних сразу.
Потому что, когда вы это мы обсуждали, что если берешь,
ну давайте посмотрим, почему это следует из условных средних,
из свойств условных средних.
Как это проверить?
Тут у меня еще на этот счет, кстати, отдельная будет теорема без доказательства.
Ну да, это у нас такое свойство было условных средних.
Значит, у нас было какое свойство?
Что если одна сигма-алгебра вложена в другую сигма-алгебру,
то когда вы взяли условное среднее относительно меньший,
и применили его к условному, то есть сначала спроектировали на большее,
а потом спроектировали на меньшее, то это то же самое, что сразу проектировать на меньшее.
Ну оно и понятно.
Это у нас было, но на уровне проекции это понятно, когда у вас есть замкнутое подпространство,
а в нем другое замкнутое подпространство.
И когда вы спроектировали на большее, а потом проекцию большего спроектировали на меньшее,
это то же самое, что вы сразу спроектировали на меньшую.
Вот поэтому это получается мартингал.
ну кажется что это какой-то такой ну страшно специальный случай так так на первый взгляд что
видите как-то мартингал получился видите из одной случайной величины так но оказывается что это
наоборот что это почти всегда так вот давайте я вам приведу без ну и без доказательства ну
доказательства не очень трудно но оно у нас бы в сторону увело поэтому доказательства не
будет а как факты это очень полезно знать значит теорема теорема пусть пусть мартингал
мартингал к сиате равномерно интегрируем то есть это значит следующее что когда вы берете
интеграл от к сиате по множеству где к сиате больше либо равно и берете супремум по т таких то
это дело стремится к нулю когда r идет к бесконечности то есть смотрите когда когда одна случайная величина
ну это понятное свойство ну там вытекает из там всяких ну понятно простых свойств интегралов
ну если угодно это абсолютная непрывность интеграл ли бега так но это когда одна
фиксирована а тут требуется видите чтобы эта штука стремилась по т равномерно к нулю вот это
называется равномерная интегрируемость ну например например из неравенства чебышова
и из неравенства каши следует что это верно если у них квадраты если у них равномерно
ограничены интеграла от квадратов вот если это так то это условия равномерной интегрируемости
выполнена так тогда тогда существует такая к си интегрируемая значит которая ну которая вот
дает их как условные средние от одной и той же ну а ну в одну стор в одну сторону это совсем
простое утверждение в одну сторону это из неравенства янсона легко вытекает в другую
сторону ну то есть то есть когда такая есть то из неравенства янсона довольно легко извлечь
что будет верно вот это так а и фильтрация какая угодно нет плейте фильтрация уже есть ведь
у нас уже мартингал есть он же мартингал относительно фильтрации и значит так что вопрос вопрос откуда
взять кси вот откуда эту кси взять вот вот в чем вопрос так но в каком-то смысле кси оказывается
их ну ну неким пределом так сказать но у кси нет такого явного описания но но оно есть и поэтому
поэтому вот этот странный пример на первый взгляд какой-то очень технический он на самом
деле большинство реальных мартингалов охватывает бывают бывают неравномерно интегрируемый
мартингал и такой конечно бывает но но но это надо довольно таки ну проявить некое усердие чтобы
построить так вот с качестве упражнения можете придумать мартингал который не является равномерно
интегрируем тогда он такого виду иметь не будет ну вот скажем с дискретным временем с натуральным
временем может такой пример попробовать придумать так вот но значит но это ну некие некие усилия
прилагают прилагаются а в приложениях большинство мартингалов вот вот даже
этому довольно жесткому условию удовлетворяют но это такое вот в этой науке довольно удобное
условие с квадратами всегда имеет дело ну и вот многие многие конкретные мартингалы этому
условию удовлетворяют ну не все конечно но обратите внимание чтобы проверить да давайте
я еще еще в качестве задачи значит упражнения но это уж вот совсем на янсона упражнения кси от
т ну неважно мартингал не мартингал равномерно интегрируемо просто семейство семейство случайных
величин равномерно интегрируемо тогда и только тогда когда существует выпуклая функция выпуклая
функция в которая растет быстрее которая растет которая растет на бесконечности быстрее
выпуклое не отрицательное ну или там давить ну да не отрицает выпуклое не отрицательное
которое растет на бесконечности быстрее ты а и такая такая что просто ограничены интегралы
вот от этих композиций так ну вот в частности вот это утверждение что квадратов ограниченных
хватает ну это простое следствие этого упражнения если в качестве в например можно взять т квадрат
годится вот ну тут видите получается значит смотрите смотрите как получается получается что
мартингал имеет вид вот очень такой конкретной полученной проектированием одной и той же
случайные величины в точности тогда когда можно подобрать выпуклую функцию ну быстро растущую на
бесконечности так что у них окажутся ограниченными интегралы значит ну геометрически вот это
выглядит так значит у вас ну вот в особенности когда это все в л2 происходит то геометрически
это выглядит очень наглядно у вас есть такая вот ну так сказать ну такая вот кривая замкнутых
подпространств расширяющихся так значит при каждом ты у вас есть некое замкнутое подпространство
и растут со временем так и мартингал и это просто получается проектированием фиксированного
элемента ну вот на эту спираль так сказать раскручивающиеся вот примерно такой наглядный
смысл так теперь значит про мартингал и значит про мартингал и ну ну кстати сказать вот эта
она не очень просто доказывается но но есть сравнительно простой случай в котором в котором
она довольно deportа ток Fear box это вот какой случай если время дискретно если время дискретно то
есть натурально время натурально так то есть это просто последовательная с unexpectedly так и vocês
аны вот это условия то есть видите это так сказать частный случай так вот в этом случае
это ксиен, элементы гильбертового пространства, и кси, вот эта желаемая кси, из них почти что явно
строится, а именно строится так. Смотрите, что у вас получается. У вас получается ограниченная
последовательность функций в гильбертовом пространстве, но в функциональном анализе
доказывается, что из такой последовательности можно извлечь слабосходящуюся подпоследовательность.
То есть слабосходящуюся, это значит не по норме гильбертового пространства, а так, что только
скалярные произведения будут с фиксированными векторами сходиться, ну как бы координаты, можно
так сказать, координаты будут сходиться, а по норме она не обязательно будет сходиться. Так вот,
если так сделать, то всякий такой предел слабой, сходящейся подпоследовательности, он и будет
вот этим желаемым кси. Но чтобы это проверить, ну вот надо воспользоваться этим сведением из
функционального анализа. Сейчас, а я вот только забыл, у вас функциональный анализ сейчас ведь
еще есть, а не было у вас вот там такого факта, что из ограниченной последовательности в гильбертовом
пространстве можно извлечь слабосходящуюся? Ну в ноябре, понятно, ну то есть это уже с диска стерто.
Ну в общем, нет, ну в общем, так сказать, вот то есть этот факт какой-то, который, ну по крайней
мере, так сказать, если даже его и не было, то наверняка будет, ну в общем, факт понятный. Ну он,
кстати, и доказывается не очень сложно, но это уж точно не, так сказать, предмет этого нашего курса.
Вот теперь, значит теперь, теперь спрашивается, когда мартингал сходится. Значит есть,
когда он сходится в l1, значит теорема, теорема дуба, значит теорема дуба,
значит пусть, значит ксин мартингал относительно, значит, возрастающей последовательности сигмалги.
Этот мартингал сходится в l1, ну то есть в среднем, тогда и только тогда, когда есть такая функция
из l1, что она имеет, ну что она его порождает. Значит, это равносильно, это равносильно тому,
что, ну вот эта последовательность равномерно интегрируемая. То есть видите, да, да, при этом,
при этом еще кси-энная атомига сходится кси-атомига почти всюду. У нас тут пока не было разговоров про
сходимости почти всюду, так, но, но, но, то есть смотрите, получается вот какая картина,
ну правда, в этой картине время должно стать дискретным, ну чтобы говорить про сходящиеся
последности, так, вот, вот в этом специальном виде, но он, впрочем, для приложения, конечно,
один из самых распространенных, когда мартингал это последовательность, то смотрите, что оказывается,
что мартингал сходится в среднем в точности, когда он имеет вот этот очень специфический вид,
так, и при этом есть сходимость еще и почти всюду, но, но, но тут эти сходимости они неравносильны,
ну вот, если вы там что-то припоминаете из курса интегрирования, то там есть два вида
таких основных сходимости почти всюду и в среднем, они между собой не очень связаны, так,
если что-то сходится в среднем, то не обязательно сходиться почти всюду, ну, там есть, правда,
какие-то связующие теоремы, типа, если сходится в среднем, то можно выбрать
подпоследовательность, которая сходится почти всюду, так, а если сходится почти всюду,
не обязательно сходиться в среднем, но если еще добавить равномерную интегрируемость,
то будет сходимость в среднем. Но вообще говоря, из сходимости в среднем для всех последовательностей
ничего сказать нельзя. Но тут тот случай, когда из сходимости в среднем вытекает сходимость почти
всюду. Вот если бы был общий случай, ну кто-то сходится в среднем, хорошо, тогда есть подпоследовательность,
сходящаяся почти всюду, но не вся сама. А тут не надо никаких подпоследователей брать, сразу вся
исходная последовательность сходится почти всюду. Это довольно полезное утверждение. Вот это,
можно сказать, основная теорема о сходимости марктингалов. Она доказывается не то чтобы длинно,
но это явно, так сказать, не умещается вот в этот, так сказать. А если взять время непрывной,
говорите про направленности, то с L1 всё будет так же, а вот почти всюду уже не так будет. Поэтому
не совсем такая будет теорема. Но вот та часть, которая с сходимостью в среднем, она останется.
Так, теперь, ну вот, что-то я ещё тут хотел сказать про марктингалы. Вот важная вещь.
Вот ещё важная вещь. Вот ещё важная вещь про марктингалы. Эта теорема без доказательства,
её просто полезно знать как факт. Сейчас про марктингалы мы сегодня закончим, и будет одна
теорема с простой формулировкой, с простым доказательством. Значит, связь с процессами
с независимыми превращениями. Значит, как только у нас появляются какие-то новые классы процессов,
их у нас немного, то всегда бывает поучительно как-то между собой их скрестить. Вот у нас будут
независимыми превращениями гауссовские, марктингалы и марковские. И вот можно много устроить из этого,
мне кажется, почти для всего курса, для присутствующих точно можно устроить такие задачи.
Поскрещивать какие-то комбинации из этих четырёх, что получится, когда вы поскрещиваете что-то.
Но некоторые скрещивания являются поглощающими, там ничего, так сказать, не происходит нового,
а в некоторых случаях при скрещиваниях что-то такое специфическое появляется.
Давайте посмотрим такой пример. Пусть CRT марктингал. Я не пишу вот эти f, потому что
определение само за собой тянет, он марктингал не сам по себе, а относительно какой-то фильтрации.
Например, хотя бы им самим порождённый, но надо помнить, что марктингальность это всегда что-то
апеллирующее к сигмалгипам каким-то, но для сокращения я не буду полностью всё это выписывать.
Марктингал и XAT, пусть они в L2. Это, естественно, дополнительное ограничение, потому что в
определении марктингала не требуется, чтобы они были в L2. Давайте посмотрим, что будет с приращениями.
Давайте вычислим. Я даже хочу буквы использовать такие, как у меня в конспекте.
Давайте сосчитаем к авариацию, вычислим.
К авариацию разности. Видите, мы вычисляем не к авариацию самого процесса, а вычисляем к авариацию приращений.
Почему? Потому что когда мы говорим о процессах с независимыми приращениями, то приращения должны быть независимы.
Но если они независимы, то они должны быть и некоррелированы. А вот сейчас мы намереваемся проверить
вот это более слабое свойство, некоррелированность приращений. Это не то же самое, что независимость, это немножко послабее.
Вот это хороший вопрос. Давайте, раз уж я забыл, то чуть позже это сделаем, что у марктингала среднее постоянно.
Вот это я забыл отметить, но это мы сейчас сделаем. Давайте сосчитаем. Смотрите, что тут получится.
Ну как полагается? Полагается честно перемножать их.
Вот так получается.
Ну сейчас, правда, может быть, я зря все расписал. Может быть, было даже покороче. Сейчас, ну давайте, раз уж расписал, так расписал.
Значит, вот что замечаем. Замечаем, что кси Т минус кси С
артагонально под пространство АФС измеримых. Ну когда С меньше либо равно Т.
Ну почему? Ну потому что у них одинаковые проекции по определению марктингала.
У них одинаковые проекции, поэтому разность артагональна этому подпространству.
Ну что это дает тогда? Давайте посмотрим тогда, что это дает. Разность артагональна, ну и что?
Ну, да, значит, скалярное произведение нулевое. Это значит кси Т минус кси С на кси С будет ноль.
Но из этого следует, что кси Т, кси С есть кси С в квадрате.
То есть смотрите, что происходит. Когда мы берем ковариацию самого процесса, не приращение самого процесса, то она равна вот этому.
Но теперь мы, теперь мы смотрим вот на это и видим, значит, смотрите, что здесь получается.
Значит, здесь, здесь получается квадрат вот этого, так? Вычитается квадрат вот этого.
Ну там все с подожиданиями, да? Здесь меньше Т2, наоборот, вычитается, то есть смотрите, вот этот и вот этот, они сокращаются, так?
А и вот этот и вот этот, они тоже сокращаются, потому что этот будет квадрат этого, ну он с плюсом, а тут с минусом.
Значит в итоге получается ноль, так? Значит в итоге получается ноль. То есть видите, оказалось, что приращение, значит приращение,
артагонально, так? Значит приращение артагонально. Итог, приращение артагонально.
Дальше, значит, в общем случае, вот это я забыл сразу сказать, это надо было отметить.
Значит в общем случае, средняя константа, так? Значит откуда это следует?
Ну да, ну это можно, это можно написать так. Значит, кси вот это, вот это, это есть, вот это есть среднее, это есть среднее.
Вот условного среднего, так? Ну и теперь, ну и теперь если сравнить, то поскольку среднее, среднее условного среднего всегда равно
абсолютно среднему, то получается, что средние равны, а из этого следует, что приращение, приращение еще не коррелированный.
Поэтому, значит, смотрите, что получается. Получается, ну это правда не какой угодный мартингал, а это мартингал из l2, так?
Ну и последняя теорема, значит, в другую сторону. Ну это так сказать, ну это не совсем обратно, а значит теорема, значит, ну вот заключительная теорема этого раздела,
единственная теорема, которую мы докажем. Ну сейчас, тут доказательств две строчки, но посмотрим, сейчас я успею их написать за минуту.
Значит, теорема такая, если, если ксиат-т процесс с независимыми приращениями, так, а и среднее постоянно,
то, то ксиат-т мартингал относительно порожденной инфильтрации.
Так, сейчас вот, но вот я не понимаю, сейчас, но кажется, уже нет у нас времени, потому что мы еще перрыв не делали.
Сейчас, пять минут еще есть. А, ну за пять минут, за пять минут мы успеем добежать до границы, как говорил тот персонаж.
Значит, доказательства. Значит, с меньше t, значит, смотрите, что получаем.
Значит, условное, среднее можно записать так. Значит, это есть вот это плюс вот это.
Вот так. Значит, при этом это будет, при этом это будет вот это, ну и равное ксиас.
Значит, так как условное, среднее разности равно нулю.
Значит, почему? Ну а оно равно, оно равно просто их обычному среднему разности.
Значит, давайте вот это проверим. Значит, давайте вот это проверим. Почему так?
Ну, потому что вот эта разность, она независима вот этой сигмалгеброй.
Ну, независима, это значит, она независима с каждым событием из этой сигмалгебры.
И поэтому из определения следует, что условное, среднее нулевое.
А почему она, почему эта разность независима с сигмалгеброй?
Ну, потому что она независима из-за приращений.
Так, потому что кси, т-ксис независима со всеми кситау, кситау при тау меньше либо равном с.
Ну, это, это из-за независимости приращений.
Из-за независимости приращений.
А когда случайная величина независима, ну, с каждой случайной величиной из некого набора,
то она независима со всеми из сигмалгебрами порожденных.
Ну, давайте вот это я уже в следующий раз поясню, чтобы это не комхать,
потому что это действительно в некотором пояснении нуждается. Почему так?
Ну, это интуитивно понятно, но, ну, в общем, строго говоря, это надо еще пояснить.
Вот этот момент, ну, он уже к маркингалам не имеет отношения.
Ну, это на самом деле нечто, что можно было бы в качестве заготовки сделать в условных средних.
А, значит, в конспекте, я в конспекте, между прочим, по-моему, из-за этого задачу сделал.
Да, в конспекте это у меня задача, но давайте я в следующий раз это поясню,
потому что все-таки это, так сказать, теориям с доказательством,
наверное, не очень хорошо, так сказать, куски доказательств загонять в задачи.
Хотя в этом что-то тоже есть. Все, давайте на этом закончим.

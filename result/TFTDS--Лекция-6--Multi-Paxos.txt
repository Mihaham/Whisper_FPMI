Привет! Сегодня мы продолжаем говорить про задачу консенсуса, про репликацию автомата,
и как обычно вспоминаем, на чем мы остановились в прошлый раз. Итак, мы с вами решаем задачу
репликации произвольного автомата. Мы ее решаем с помощью примитива под названием atomic broadcast.
Мы сказали, возьмем наш автомат, поместим его копию на каждую реплику. Когда какой-то
узел системы будет получать команду от пользователя, он будет с помощью atomic broadcast
раздавать ее всем остальным репликам. Ну и каждая реплика в силу свойств atomic broadcast или
totary order broadcast будет получать эти команды в одном и том же порядке. Она будет их применять,
и когда сам координатор получает команду, которую он сам же отправил через atomic broadcast,
он ее применяет и отдает ответ пользователя. Эту задачу мы в свою очередь свели к задаче консенсуса.
Ну, там было довольно ясно, что это две примерно одинаковые задачи, и в позапрошлый раз мы с вами
разбирались, а какие же ограничения на задачи консенсуса действуют, какие же там есть врожденные
невозможности, скажем так. И мы выяснили, что есть, поскольку задачу консенсуса необходимо решить,
чтобы сделать atomic broadcast, а atomic broadcast необходим для любого нетривиального RSM,
мы разбирались задачи консенсуса и выяснили, что во-первых, в любом решении задачи консенсуса
есть deadlock, а именно любое решение переживает только менее половина отказов узлов, и во-вторых,
в любом решении задачи консенсуса есть livelock, то есть даже если сбоев нет, просто есть
асинхронная сеть и детерминированный алгоритм, то этот алгоритм может сколько-годно долго не завершаться,
если для него складывается неудачно, если просто сеть доставляет сообщение в каком-то неподходящем
алгоритму порядке. Это была теорема FLP. И с первым результатом мы, видимо, поняли, что в алгоритме
консенсуса, который обязан блокироваться при большом количестве отказов, должны быть какие-то
хворомы, которые должны собираться или не собираться, если отказов слишком много, и в алгоритме консенсуса
должны быть какие-то циклы, чтобы он мог в livelock войти. Такие нехитрые соображения. И в прошлый раз,
на последнем занятии, мы с вами построили конкретный алгоритм, который решает эту задачу. Мы построили
алгоритм, который назывался single decreed access. Access одного указа. Таковы наши достижения в текущем
моменте. Вот план на сегодня. Вроде бы задача решена. То есть все, что мы написали, вроде бы дает нам
РСМ. По модулю каких-то мелочей, которые на самом деле, конечно, не мелочи. Нам не хватает
переконфигурации, нам не хватает экзеклиуанс, но это можно добыть. Но в принципе конструкция,
она до конца реализована. Мы умеем решать эту задачу, мы умеем сводить эту задачу к этой с
помощью серии консенсусов и reliable broadcast, где мы просто должны сообщение раздать всем. И мы умеем
сводить эту задачу к этой, мы даже проговорили уже как. То есть в принципе можно отдыхать и больше
ничего не делать. Но наша сегодняшняя цель, вот эту цепочку сведений размотать в обратную сторону и
получить более-менее цельную конструкцию. Потому что вот такая цепочка сведений, она сама по себе
очень неэффективна и нужно как-то аккуратнее все это сделать. Там по пути возникнет много мест,
где можно что-то оптимизировать, как-то более удачно скомпоновать. Это конечно пойдет, идет в
разрез. Вот с таким вот аккуратным сведением и модульностью, но с другой стороны мы получим что-то
более применимое на практике. Мы хотим построить алгоритм, который будет называться multiplex.
С помощью которого уже можно строить RSM, непосредственно RSM. Но для того, чтобы это сделать,
нам нужно аккуратно вспомнить, как алгоритм работает. Ну и плюс с учетом новой домашки,
тоже довольно актуально. Мы сейчас займемся очень скучным делом. Мы повторим просто,
как алгоритм работает, чтобы дальше из него строить более сложное что-то.
Итак, напомню, что в алгоритме single decreed access были роли. Были пропозеры и были аксепторы.
Пропозеры выдвигали предложение и выполняли процедуру прополз, которая необходима для решения
задач консенсуса, в которой она состоит. А аксепторы голосовали за предложение.
Предложение это пара N, V, где V — это значение, которое предлагается пропозером, а N — это
некоторый proposal number. Ну грустно, что он так называется, разумеется, но пока у нас нет для него
более разумного, более подходящего названия. Proposal number, который уникален глобально в нашем
алгоритме и который, видимо, является не просто некоторым числом, а он, скорее всего, является парой
из какого-то числа и уникального идентификатора данного пропозера. Итак, процедура прополз,
которую выполняет каждый пропозер. Обращаю ваше внимание, что алгоритм вообще не требуется,
чтобы пропозеры и аксепторы были одними и теми же узлами. Алгоритм пропозеров — произвольное
количество, аксепторы — фиксированное количество, потому что из них формы собираются. Вот оно выбрано,
раз и навсегда, и не меняется. Что делает пропозер? Пропозер проходит через две фазы. Вот есть он и
есть три аксептора. Разумеется, разные пропозеры действуют конкурентно, но мы смотрим только на
одного. Сначала пропозер выбирает себе некоторое число n, но не число, а вот этот пропозл number и
отправляет на аксептор сообщение prepare. Каждый аксептор хранит в себе np, максимальный n,
который он получал в сообщениях пропозера, и хранит n, a, v, a. Это предложение, за которое аксептор в данный
момент проголосовал. Это вот его текущий голос. В смысле, вся пара — текущий голос. Аксептор,
получая сообщение prepare от пропозера, смотрит на его n и сравнивает его со своим np. Если n
пропозера больше, чем np аксептора, то prepare принимается, аксептор обновляет свое локальное np,
после чего отвечает пропозеру сообщением promise, куда прикладывает свой голос. Напомню,
это сообщение служит сразу двум целям. Во-первых, если значение уже выбрано к этому моменту,
то пропозер узнает об этом значении, получив голос аксептора. А во-вторых, сообщение называется
promise, потому что аксептор заодно обещает не принимать сообщения, которые в второй
фазе выполняются от других пропозеров с номерами меньше, чем n. В этом суть названия. Пара помогала
узнать, что значение уже выбрано, а promise, то есть обещание не принимать более старые сообщения,
позволяет гарантировать, что если мы этот пропозер предложим эту пару, и она победит,
аксептор за нее проголосует, то никакие пропозлы с номерами меньше, чем наш, приняты уже не будут,
если мы успели собрать кворум из preparer. Если мы успели собрать кворум из preparer, то мы смотрим
на полученные пары N, A, V, A, и выбираем то значение, которое мы будем предлагать. Если мы получили
от аксепторов хотя бы один непустой голос, то мы выбираем в качестве значения для своего
предложения V, A с максимальным N, A. Если все голоса были пустые, то есть ни один аксептор еще ни за
что не проголосовал, то мы берем в качестве предлагаемого значения свое собственное V. Вот так
получается V со звездочкой. И после этого мы хотим записать его на кворум аксепторов. Для этого у нас
есть вторая фаза и сообщение accept. Если этот accept устраивает аксептора, то есть N по-прежнему не
меньше чем N, P, то аксептор принимает это предложение и обновляет свое N, A, V, A и отвечает
проповоду, что его предложение принято. Если же по какой-то причине кворум, сбор кворума провалился
на первой фазе или на второй. У нас сбор кворума может закончиться тремя способами. Он либо собрался
все-таки успешно, то есть мы получили большинство промесов, скажем, либо он может зависнуть,
потому что отказов слишком много, либо он может буквально провалиться, потому что мы получили
слишком много отказов вместо промесов или вместо accepted. Если мы проходим успешно через две фазы на
месте проповзора, то мы завершаем свой проповз, возвращаем его со звездочкой. Если же какая-то
фаза проваливается, то мы ретраемся и проходим через две фазы заново, выбирая какой-то свежий N,
который, видимо, больше, чем предшествующий в лексикографическом порядке, чтобы accepted все-таки
приняли. Вот такой был алгоритм. Мы рисовали какие-то картинки с исполнением, пытались уяснить,
в чем же смысл фаза prepare. Вроде бы выяснили это. Она с одной стороны позволяет узнать о значении,
которое уже выбрано, а с другой стороны позволяет заблокировать конкурентов, которые уже не должны
ничего предложить. И мы доказали safety. Мы определили, что такое выбор алгоритм Paxos. Мы сказали,
что предложение nv chosen, если большинство acceptors ответили на него сообщением accepted. Даже не то,
чтобы оно лежит где-то на большинстве acceptors в качестве голоса, просто acceptor какое-то
большинство за него проголосовало. Вернее, какой-то quorum. И дальше мы доказали коллекцию safety
свойств Paxos о том, что если с каким-то N пара nv была выбрана, то для всех N не меньше, чем это N,
никакое другое значение, в принципе, не может быть предложено. То есть, если мы выбираем значение,
то оно остается с нами навсегда. С другой стороны, эта теорема не обещала, что значение
когда-либо будет выбрано, и в этом сути full P. То есть, если Paxos завершится, то замечательно,
он свое решение не изменит. Мы гарантируем agreement. И agreement гарантировать для нас важно,
потому что от agreement зависит order в broadcast и корректность репликации. С другой стороны,
завершаемость мы гарантировать не могли, это было ограничение full P теоремы, мы не можем
предалить, но мы можем на практике как-то с ним бороться с помощью времени и чего-то подобного.
Про это сегодня еще поговорим подробнее. Да, вопрос.
Если у нас есть выбранное значение, а потом кто-то из похолода отправляет Paxos N больше, чем выбранное?
Ну смотри, каждый acceptor может голосовать много раз. Вот если он видит, если он уже выбрал,
если уже глобально значение выбрано, вот пара это выбрано, предложение выбрано, а потом появился
какой-то proposal с каким-то очень большим значением N, то он, конечно, проходит через фазу prepare и
делает новый accept и его все принимают. Но смысл нашей теоремы был в том, что если он так делает,
то он неизбежно будет предлагать то же самое V. С новым N, но V будет то же самое. В этом было
то есть каждый acceptor голосует повторно, и вот эти принимаются все новые и новые пары.
Но у каждой принятой пары V будет одно и то же. В этом была корректность. В этом было safety свойство,
свойство agreement. В конце концов наш алгоритм-то он наружу эти N не отдаёт, он просто выбирает
значение, а значение будет одно и то же. Мы посмотрели на все пары N, A, V, которые мы получили из
промесов, и если хотя бы одна пара была не пустая, но поначалу у каждого acceptor пустой голос
хранится. Там какой-нибудь ноль и пустое значение. И вот если мы получили только такие вот пустые
голоса, то мы предлагаем свое значение, потому что, видимо, ничего еще совсем не выбрано. Но
если мы хоть одно значение где-то увидели на quorum, то оно может быть выбрано. Мы в этом не
уверены, поэтому мы выбираем одно из предложенных, а именно с максимальным N, A, и это дает нам
корректность. Нет, нам нужен quorum, но мы проходим через первую фазу, мы ее завершаем только тогда,
когда мы получили промес, то есть мы захватили вынеплание, захватили блокировку на большинстве
acceptor. Quorum нам необходимы, когда мы этот quorum собрали, мы смотрим на все эти промесы,
у нас большинство промесов теперь, промесы с большинства acceptor, мы посмотрели на их пары,
выбрали себе максимум O, N, A, а если там везде пустые значения, то мы выбрали так уж и быть свое.
Ну, алгоритм консенсации не важно, что выбрать, главное, чтобы просто выбор был общий. Ну что,
это мы помним, да? Хорошо, тогда наша цель из этого алгоритма, из этого кирпичика построить atomic
broadcast, который у нас будет называться multi-Paxos, потому что вот этот Paxos выбирал один единственный
указ, судя по названию, а multi-Paxos или multi-degree Paxos выбирает серию указов, то есть это протокол
работы греческого парламента и, видимо, в нем запускается серия консенс. Каждый из этих консенсов
будет выбирать какое-то очередное значение, очередное сообщение в atomic broadcast и команду в RSM.
И вот прежде чем перейти к конкретному, к построению этого алгоритма, мы будем его не описывать,
а просто строить из каких-то общих соображений. Я хочу немного контекста дать. Мы будем решать
не совсем задачу atomic broadcast, то есть в теории мы решали ее, но я хочу перейти
к немного другой терминологии, а именно я хочу сказать, что мы вместо atomic broadcast решаем задачу
репликации лога. Мы хотим построить алгоритм, который строит реплицированный лог. В чем суть?
Ну понятие лога всем знакомое, это апенталонлиструктура данных, это может быть текстовый файл, куда
высыпаете события. Давайте дверь закроем, если можно. Это может быть write-ahead-лог для отказа
устойчивости на уровне одного узла, который пишет базу данных, или файловая система, или даже в
смысле SSD. Мы хотим говорить про лог в том же самом смысле. Мы хотим говорить про такую структуру,
у которой есть слоты, и куда будут помещаться команды пользователей. Вот команды у нас сегодня
будут очень условные, я буду сразу их рисовать вот примерно так. Команда jump, команда move. Какие-то
короткие ассемблерные мнимоники, потому что вам в конце концов не важно, что мы реплицируем. Вот мы
хотим реплицировать произвольный автомат, и что для нас он значит? Ну это какой-то черный ящик.
Что мы можем с ним делать? Мы можем взять его в некотором состоянии, и на этом состоянии сказать
точка apply, применить какую-то команду, и получить новое состояние s' и результат применения этой
команды, ну то есть какой-то операции пользователей. А дальше мы поместим на каждую реплику копию
такого автомата, и хотим, чтобы на этих автоматах, на разных репликах, команды применялись в том
же порядке. Вот порядок этих команд, он с одной стороны раньше задавал порядком доставки сообщения
в этом эго бродкасте, а сейчас мы скажем, что этот порядок материализован в виде лого на каждой
реплике. Вот у каждой реплики есть своя копия лого, которую помещают с команды, которые нужно
применять к автомату. У каждой реплики своя собственная копия, и, разумеется, эти копии должны
быть акты друг с другом синхронизированы. Эти копии не могут быть полностью тождественны в
каждый момент времени, потому что реплики разные. Эти копии будут отличаться, в каком-то логе будет
меньше, команд в каком-то больше, может быть даже команды в них будут разные, мы увидим почему. Но
тем не менее, эти логи должны заполняться, и по префиксам они должны матчиться друг с другом,
и вот каждая реплика будет по этому логу скользить. И наша задача вот реплицировать этот лог, то есть
не мстить на каждую копию, на каждую реплику этот лог, и заботиться об их согласованности, о том,
чтобы префиксы были одинаковыми. А что будет лежать в этом логе, команды, какие-то set или get для
киеварю хранилища, или команды jump, move и red, какие-то условные, для нас это совершенно не важно
уже. Для протокола репликации, вот этот автомат — это черный ящик, мы не знаем, как он устроен
внутри, ну то есть в коде это может быть какой-то интерфейс, вот такой вот, с командой apply,
с методом apply. И сами команды для нас тоже непрозрачны, то есть какие-то стерилизованные байты,
которые вот эта стейт-машина реализации этого интерфейса умеет интерпретировать. Мы, реплика,
ничего об этом не знаем. Мы просто храним свою копию лога, мы храним свою копию автомата,
там не знаю, если это киеварю хранилищ, то мы храним отображение из ключей в значение. И вот мы читаем
очередную команду и применяем ее. Мы помним, где мы остановились в логе, когда появляется новая
команда, мы достаем ее, применяем к автомату и переходим в новое состояние. Вот это буквально
называется RSM. И мы говорим сегодня про один способ реплицировать лог и с помощью него реализовать
RSM, это мультипаксис. Ну вот и теперь уже можно на экран посмотреть наконец. Это какое-то состояние
автомата, которое является отображением с ключей в значение. Мы говорили, что хотим построить базу
данных, а в базе данных у нас есть. База данных строится табличная модель с транзакциями, с запросами,
строится поверх киеварю хранилища, а киеварю хранилища большое, поэтому мы нарезаем его на шарды,
которые называются таблетами. Каждый таблет это относительно небольшое количество ключей в
значении, которые помещаются в одну машину. Мы их реплицируем с помощью, скажем, мультипакс,
который мы хотим сегодня построить. Ну а каждая конкретная реплика хранит свою собственную копию.
Эта копия была нарисована. И на свете существует довольно много, ну существуют разные алгоритмы,
которые решают задачу репликации этого самого лога. Ну во-первых мультипаксис, но он не первый,
и с ним есть некоторые сложности, я как раз о них хочу поговорить. А есть, наверное, исторически
первый, это ViewStamp Replication, который был придуман еще в 80-х, но, кажется, его в таком нечеловеческом
виде описали, что его трудно было понять, его переписали в 2000-х уже, вот это новая статья,
поэтому ревизит. Есть протокол репликации, который называется ZooKeeper Atomic Broadcast,
который используется в системе ZooKeeper, про которую мы как-то упоминали уже. Это RSM,
который хранит такой эрортический набор атомиков, из помощью которого можно конфигурации в системе
делать, задачу выбора лидера решать, но это вот отдельная лекция в параллельном курсе у вас будет.
Это еще один протокол. Он действует не совсем так, как реплицированный лог, как я объяснил,
но все же, в принципе, похож. Ну и есть, конечно же, алгоритм RAFT, в котором мы тоже идем,
который является вот альтернативой алгоритма Multipax, который мы строим сегодня. Ну и вот тут
смотрите, подходящая нам картинка про то, как в общем виде RSM функционирует. У нас есть реплики,
это вот эти дощечки. У каждой реплики есть собственный лог, и у каждой реплики есть копия автомата.
Когда мы клиент приходим с командой, то мы отдаем их условно такому модулю консенсуса,
как он называется. Этот консенсус выбирает, в какой слот лога на каждый из реплик эта команда
попадет. А сама реплика эти команды оттуда вычитывает в порядке, в котором они в этом логе
появляются, и меняет состояние своего автомата. Ну вот здесь, видимо, реплика дошла до третьей
команды, и вот поэтому Y сейчас равен 9. Ну вот такие алгоритмы, а есть Multipax. И здесь есть такой
очень неловкий момент, который, как бы его описать. Дело в том, что Multipax непонятно, что это.
Потому что вот если вы читаете статью про RAFT, то очень легко понять, что такое RAFT, потому что
тут есть страница, одна из первых, где алгоритм просто полностью выписан. То есть вот два типа
сообщений, два типа RPC-вызовов, append-enters и request-vote. Состояние каждой реплики и правило,
по которому реплика реагирует на вот эти сообщения, что у них включено, как она меняет свое состояние,
какие там роли есть. Ну в общем, если вы не хотите думать, то просто открываете эту страницу и
переписываете. Это неправда, конечно. Думать все равно придется. Но в целом протокол вот зафиксирован.
Если мы говорим про ViewStamp Replication, который придуман еще в 88 году, то тут тоже, в общем,
все довольно понятно. Вы читаете статью, у вас там есть пункт 4.1, где описано, как протокол работает
в такой стабильной конфигурации, когда никаких звуков нет. Мы получаем сообщение, отправляем
prepare, потом получаем ответ. Тут все аргументы перечислены, что нужно делать, как менять
состояние каждой реплики. Дальше есть протокол, который позволяет, если вдруг какой-то лидер
умер, поменять его. Ну в общем, перейти в новую эпоху. Какие-то правила очень ясные, все описано.
В протоколе звуки Peratomic Broadcast тоже описаны все сообщения, которые в протоколе участвуют.
Их там уже довольно много становится в этом алгоритме, но тем не менее можно прочесть статью
и всех выписать. А про multipaxes есть статья про греков. И вот там нет никакого канонического
описания этого самого протокола. Вот если вам говорят, что такое RAF, то RAF более-менее один.
Ну то есть, конечно же, есть некоторые вариации. Вот в голом RAF всего лишь два типа сообщений, но
при этом, если вы делаете переконфигурации, то появляются новые сообщения. Если вы
аккуратно справляетесь с асимметрией в сети, то появляются еще дополнительные сообщения,
дополнительные фазы. Но тем не менее, что такое RAF, в целом, все понимают и понимают
под этим одно и то же. Что такое звуки Peratomic Broadcast, тоже понимают, потому что он используется
ровно в одной системе, и там канонично описано и реализовано. Если мы говорим про
Ustream Replication, то тоже понятно, что такое Ustream Replication. Сообщение перечисленное, протокол
описан. Мы даже понимаем, что такое single decreed access, потому что фазы были нарисованы,
четыре сообщения, две фазы. Вот все очень конкретно. А multipaxes, у него канонического описания нет.
Сколько в нем типа в сообщении, неизвестно. Как именно на них реагируется, это тоже неизвестно,
потому что из предыдущего пункта следует. Это такой некоторый конструктор, некоторый фреймворк,
в котором можно делать что-то свое. То есть мы сегодня придумаем общую конструкцию и поймем,
то есть мы почти все придумаем, но у нас останутся какие-то пустые места, которые нужно чем-то
заполнять. Вот Lamport не стремится к этому. Он оставляет эту возможность человеку читать
ее внимательно в качестве упражнения. Ну и смотрите, чем это закончилось. Люди начали придумывать
свои алгоритмы Paxos, в смысле свои какие-то вариации. Они их придумывали раньше и придумывают до сих
пор. Вот я специально подобрал две свежие статьи 20 и 21 года. Они обе с названием Paxos. Это статья
про переконфигурации, задачу которой мы еще не решили, и 21 год идет, а ее все еще решают
подозрительно. Ну и еще одна статья с какими-то очень дикими названиями. Я не берусь это слово
прочитать. Про то, почему разумно разносить проповзоров, аксепторов и какие оптимизации
там можно строить. То есть Lamport дал нам большую свободу, и вот люди до сих пор ею пользуются и
генерируют безумное количество статей. Вот есть целый такой список статей про консенсус. Если вы
в них будете искать слово Paxos, то смотрите на счетчик. Тут 99 упоминаний. То есть можно до сих
пор в 21 году придумывать новые вариации на тему Paxos и что-то новое в нем, не знаю, открывать или
дорабатывать. В общем, мы сегодня обречены сразу на некоторое поражение. То есть наша работа не
будет завершенной, потому что Paxos мы построить не можем. Их бесконечно много. Ну вот, не знаю,
какой вывод нужно сейчас из этого сделать. Ну и с другой стороны, у вас же в домашне тоже будет
возможность написать свой собственный Paxos. Вот кто знает, какой он у вас получится? Он у вас может
получиться совершенно уникальный, ни на что не похожий. Ну что, давайте начинать строить алгоритм.
Вот я надеюсь, что мы сегодня, то есть мы знаем про single decreed Paxos, больше особо ничего не знаем,
и нам должно быть достаточно, чтобы построить что-то разумное и эффективное. Надеюсь, я вас не обману.
Итак, я сказал, что у каждой реплики, у каждой реплика занимается задача репликации лога команд.
Поверх этого реализован автомат, но как он устроен, мы не знаем, думать об этом не хотим,
черный ящик. Наша задача – строить лог. Ну вот давайте этот лог еще раз аккуратно нарисуем.
Задача мультипаксуса – поддерживать эти логи в согласованном состоянии. Ну я сказал,
что лог – это просто материализация порядка команд в Atomic Broadcast. Ну а собственно,
как добиться этого порядка, нужно использовать консенсус. У каждой реплики есть этот лог,
и реплика КХД может получать команды от клиента. Вот пусть мы какая-то реплика,
и мы получаем, в нашем логе уже лежат какие-то команды, и мы получили еще один мув. Что мы с ним
делаем? Ну мы хотим, чтобы он попал в лог, потому что если он попадет в лог, то его можно будет
применить к автомату и вернуть это пользователю. Первым делом мы смотрим на свой лог, ищем там
первый свободный слот, и пытаемся поместить команду мув туда. А что значит, что мы пытаемся
поместить туда команду? В конце концов, ну мы пытаемся, а другие реплики-то тоже пытаются,
потому что у них свои логи, и они тоже получают какие-то команды от пользователя. Ну вот что
значит поместить команду в лог? Скажите мне слуха бы. Видимо мы хотим в этом слоте стать
пропузером, в смысле алгоритмом single decreed access. Вот мы реплика R, ну допустим реплика R1, есть
реплики R2, R3, и что мы делаем? Мы начинаем фазу prepare. Ну с этого начинается алгоритм
паксис, мы отправляем и себе в том числе, и остальным сообщения prepare. Правда,
теперь вот это сообщение, вот этот консенсус, который мы устраиваем в третьем слоте, нужно
как-то отделить от консенсусов в других слотах, поэтому мы просто в каждое сообщение из этого
протокола базово добавим еще один параметр, это номер слота. Вот N у нас значит то же самое,
это proposal number, а K это номер слота, в котором мы все это устраиваем. В любом случае K это 3.
Мы же строим буквально atomic broadcast, только мы не говорим про операции sand и операции atomic broadcast
и обработчик ADDriver, мы вместо этого лог реплицируем. Ну потому что этот лог, удобно думать про лог,
потому что логи обычно хранятся на диске, а конечно же реплики должны запоминать порядок
команд на диске, потому что они могут перезагрузиться и забыть все, что у них в оперативной памяти,
а потом из этого лога можно восстановиться, восстановить локальное состояние. Поэтому удобно
говорить про состояние, то есть про кодюструктуру. Вот мы говорим про лог, но по смыслу это просто
серия консенсусов, каждый из которых выбирает команду вот в этом слоте, в каком слоте.
Ну сейчас они должны быть практически, я не знаю, что ты имеешь в виду, а то у них в broadcast
было свойство total order, оно говорило, что если мы берем префиксы двух любых, мы смотрим на порядок
доставки и берем два любых префикса двух узлов, то они являются префиксами друг, ну один из них
является префиксом другого. Вот здесь то же самое, если мы берем два префикса двух любых логов,
то они являются префиксами друг друга. Ну чуть аккуратнее, сейчас мы до этого дойдем. Я пока
просто поясняю, как мы используем консенсус. Мы в каждом слоте запускаем консенсус и к каждому,
чтобы различать разные консенсусы между собой, мы к каждому сообщению добавляем новый параметр
k. Вопрос у тебя. Когда реплика получает какую-то команду и хочет поместить ее в какой-то слот,
то она для этого слота становится прапоузером. Когда она получает команду prepare от другой
реплике с параметром k, то она для этого слота является аксептором. Я говорил, что аксепторы и
пропоузеры могут находиться на одном узле, и если мы говорим про задачу консенсуса просто, то
каждый узел является и пропоузером, и аксептором. Ну вот собираем promise, в общем, тут какая-то
стандартная история. Протокол таким образом бежит вперед. Да, разумеется, если нам пришли сразу две
команды, какой-нибудь еще один jump, то мы работаем с ними параллельно. То есть мы пытаемся поместить
их в четвертый слот лога. И очередную команду в следующий слот лога свободны.
Ну если там написано, что слот занят, если там ничего не написано, то он пустой.
Давай с этим как раз договоримся сейчас. Вот у нас есть слот. В каком состоянии он может быть на
каждой конкретной реплике? Во-первых, он может быть просто пуст. Вот буквально с этим слотом еще
ничего не происходило. Мы туда ничего не пытались положить, и другие реплики тоже не пытались
положить за что-то сами, и нам ничего об этом не говорили. Поэтому слот может быть пуст. Еще раз,
я говорю про конкретную реплику. То есть где-то на какой-то реплике может слот быть пуст,
а на другой уже не пуст. Смотрите, каждая реплика является аксептором для каждого отдельного слота.
А у аксептора есть состояние NP и NAVA. Вот это состояние мы поддерживаем для каждого слота.
То есть в каждом слоте на самом деле написано не просто команда, написано NP и NAVA. NP – это N,
с которым какую-то команду нам кто-то предложил, возможно, мы сами. А вот значение VA – это,
видимо, какая-то команда, за которую вы проголосовали. Так что когда я здесь пишу jump,
не совсем понятно, что же это значит. Этот jump можно просто принять данным аксептором.
То есть может быть вот в слоте K лежит пара NV. То есть данная реплика для данного слота
проголосовала за какое-то предложение. Это просто локальный голос. Он не значит,
что команду можно применять, потому что может быть этот голос изменится. Понимаете меня, да?
Может быть на других репликах сейчас другие голоса с другими N, другие команды с другими N.
Поэтому нам нужно различать состояние accept, где просто в логе лежит какая-то команда,
от состояния chosen, когда она точно уже выбрана. Но я chosen не буду говорить,
я буду говорить committed. То есть команда зафиксирована уже, всё. Она вот прибита в этом
месте, уже измениться не может. То есть в смысле паксиса содержимое слота зафиксировано. N там
может меняться, NP там может меняться, а вот в этом уже меняться не может. И поэтому очень полезно
для лога понимать, какой префикс его является закоммиченным. То есть мало просто иметь какое-то
содержимое лога. Мы можем применять это содержим, эти команды к автомату, только если мы уверены,
что вот до текущей позиции всё закоммичено. Мы не можем применить команду 2, потому что она только
принята сейчас данным accept, но может появиться новый проплоза, другая реплика, которая предложит
другое значение, у неё будет N больше, и мы должны этот jump стереть и записать что-то другое.
Поэтому нам важно очень отличать два этих состояния. Accepted и committed. Я в прошлый раз,
когда говорил про single decrep access, задавал такой странный вопрос, он был очень не в тему. Как
понять, что-то выбрано? То есть вы смотрите на accept-ов или вы являетесь accept-ом, как понять,
что-то выбрано? Изолированные задачи, это может быть не так важно, а здесь важно. Вот вы реплика,
вы смотрите на свой лог, там вроде бы слот занят, но он пока не финализирован. Непонятно,
можно его применять или нет. Вот нужно как-то об этом озаботиться, в этом подумать. Ну вот,
значит, слоты бывают в таких трёх состояниях. Их отличия понятны, да? Надеюсь, теперь. И когда
я говорю, что мы ищем первый пустой слот, я имею в виду в этом смысле пустой. То есть там просто
V пустой ещё. Ни одного голоса не было. Ну или даже, не знаю, ни одного prepr не было. Вот видите,
появляются варианты. Но это вам останется на собственное усмотрение, как это всё сделать. В конце
концов, вам дадут задачу и напишите её. То есть алгоритм здесь не фиксирует ничего конкретного,
нужно только понимать, что можно делать, а что нельзя. И это самое важное. В смысле,
что может нарушить вам корректность, а что не может нарушить. Ну в общем, понятно, да,
конструкция? И когда я говорю, что вы предлагаете, это означает, что вы выбираете n для этого слота и
выполняете сначала фазу prepare, потом фазу accept. Но может так получиться, что вы хотите поместить
move сюда, но проигрываете. То есть что-то выбирается, но не ваше значение. Да может быть и не accepted.
Ну просто ты проповзор в этом слоте, ты приходишь с командой с значением move, а в итоге выбирается не
твой move, а что-то другое. Вот. Но консенсус, это не печарило, потому что консенсусу плевать,
что будет выбрано. Но нам не плевать. К нам же пришёл клиент с командой move, он хочет её выполнить,
хочет получить ответ. Мы обязаны всё-таки в лог положить. Мы его держим, пока не положим в лог.
Что мы делаем? Ну если мы проиграли, если выиграл кто-то другой, какая-то другая реплика, положила
туда свою команду, конкурирующую с нами, то мы не печаримся, мы просто третраемся. Ищем новый
слот и пытаемся положить её заново. При этом мы можем вот параллельно пытаться заполнить несколько
слотов. Хороший вопрос, он нас не интересует. Что может произойти? У нас есть некоторый лог,
вот его префикс уже, допустим, даже закоммичен, и у нас появляются две команды A и B. И мы хотим
поместить вот эту сюда, а эту сюда. Но не выходит, точнее вот с этой команды выходит, и B появляется
здесь. Она комитится успешно. А в этом слоте появляется конкурент, и он кладёт сюда команду C.
Здесь будет C написано. Ну тогда мы ретраемся и пробуем поместить A вот сюда. Это не страшно,
потому что раз мы вообще параллельно запускали аксессы для команд A и B, это означает, что сами
вызовы у клиентов конкурировать друг с другом. Поэтому клиенты не могут полагаться на то,
что система применит эти команды A и B в определённом порядке. Сойдёт любой.
Тогда такой вопрос. Вот вроде бы мы заполняем лог слева направо. Может ли так получиться,
что в логе будут дырки? Ну вот мы реплика, и лог у нас выглядит так. Закомичный префикс,
потом пустое место буквально. Это слот допустим 5. А в слоте 6 лежит команда.
И мы прямо знаем, что она закомичена уже. Вот пустое в этом смысле. Вообще ничего нет.
Во-первых, мы не можем такую команду применить. У нас всё закомичено, эта команда закомичена,
применить мы её к себе не можем, потому что мы не знаем, что будет здесь. Здесь может оказаться
какая-то команда, которая может повлиять на результат этой команды. Поэтому мы сначала должны
заполнить этот слот. Ну а почему вообще дырка возникла, когда мы слева направо всё заполняли?
Ну потому что реплик несколько. И допустим это была реплика R2, а у нас всего их было R1, R2, R3.
И у нас были две команды. Два слота 5 и 6. И вот допустим две эти команды пришли обе на реплику
с индексом 3. И вот эта реплика назначила номера этим двум командам. 5 и 6. И для пятой команды она
собрала вот такой вот кворум. И закомиссила её даже. А для шестой она собрала вот такой кворум.
Понятно ли, что это значит? Понятно ли, что происходит вернее? Что у нас есть две реплики,
три реплики. И кто-то знает, у реплики 3 лог, префикс заполнен весь. А у реплики 2 и 1, у реплики 1 первые 5 слотов
заполнены на шестой пустой. У реплики 2 первые 4 заполнены, пятый пустой шестой заполнен. Что делать в реплике 2?
Ну вот смотри, буквально опять ты пишешь код, всё в твоих руках. Выдумывать то, что хочешь.
Лэмпард тебя ни в чём не ограничивает. Я бы сказал, что Лэмпард тебе говорит более-менее одну вещь, что пока ты в каждом слоте делаешь консенсус,
тебе ничего не угрожает. А вот когда ты начинаешь какое-то творчество, то тут уже ты идёшь по скользкой дорожке. Так вот, что делать-то?
И что они тебе скажут? Что там находится? И что с этим делать? В каком состоянии? И главное, при чём здесь паксис-то?
Как будто клиент обратился на вторую реплику. То есть, ты должен сначала пойти к другой реплике, узнать у неё команду, потом пытаться её положить к себе.
Сам, или ты про это говоришь? Здесь реплика 3 рассылала запросы. Да, и допустим она взорвалась. И вообще может быть так, что она уже параллельно это делала, комитила слоты 5 и 6.
И допустим она в шестой слот закомитила, а в пятом она просто отправила сообщение и взорвалась тут же. А сообщения ещё ни до кого не дошли.
Вот даже непонятно, этот слот есть чем заполнить или нет. Может быть, R3 мёртвая уже, но это нормально, мы один отказ можем пережить из трёх.
На двух других репликах пустое место и никогда она ничем не заполнится. А там тоже пусто. Что делать?
Ну вот вы видите дырку, она вам мешает. Вот нужно поместить туда команду NOP, которая просто занимает место.
Вот вы берёте и предлагаете NOP в любой непонятной ситуации. Если он выиграет, вы просто испортите слот.
И не важно, вам главное, чтобы можно было через него перешагнуть.
Мы выполняем propose с командой NOP в слоте 5.
Ещё раз, проблема у нас там, что здесь может быть что угодно, что-то, что может повлиять на команду jump.
Мы просто хотим как-то определить, что надо делать.
Вот мы просто хотим зафиксировать содержимое слота 5, нам не важно, что там будет.
Но если у нас есть своя собственная команда, то можно туда её положить.
А если нет своей собственной команды, то возьмём NOP, он всегда есть.
Этот NOP даже для автомата не вызывается, это просто пузырь.
Такой бабл, который занимает место в логе и тем самым говорит нам, что в этом месте уже кроме этого пузыря ничего быть не может.
А значит, можно спокойно этот слот пропустить и выполнять команду в слоте 6.
Убедил тебя?
При этом мы ничего не делаем, мы просто запускаем консенсус.
Пока мы запускаем консенсус, нас ничего не угрожает.
Это очень приятное свойство.
Но не факт, что оно выиграет, может быть мы предложим NOP, а там уже что-то другое лежит.
Допустим, вот реплика 3 успела записать, пройти через фазу accept для реплики 1.
И сейчас реплика 3 взорвалась, удаст пустой лог в этом слоте, пустой слот.
А у реплики 1 в слоте 5 лежит какая-то команда в состоянии accepted.
Мы предложим NOP, мы соберем quorum и через promise возьмем эту чужую команду.
Случится то, о чем ты говоришь, только не ручными действиями, он просто про такого консенсуса запустится и сам все это сделает.
И в этом его преимущество на тобой, что ему не нужно специально эти случаи обрабатывать.
Он просто сам разберется всегда.
Да кто знает, в зависимости от того, что он на других репликах.
Если там уже что-то лежало, то мы выполняя фазу prepare узнаем о чужой команде и в качестве высозвездочка выберем не свой NOP, а команду из чужого лога.
Если и там, и там было пусто, то мы выберем свой NOP.
И то, и другое нас устраивает, нам нужно чем-то заполнить слот, мы чем-то его заполним.
А пропозы на 1 и 6 слота?
Ну сейчас почему?
Потому что первый пропоз это был, когда на R1 и R3 собрали.
Сейчас, но пропозы тут делала R3.
R3 делала 2 пропозы для 5 и 6 слота.
Мы реплика R2.
Мы делаем пропозы для 5 слота с NOP.
Но это же разные реплики.
И разные пропозы.
Ну это же нормально.
В консенсусе есть разные пропозы.
Они все друг другом конкурируют и Paxos с этим разбирается.
Получилось?
Вообще ничего сложного.
Мы просто Paxos и вызываем.
О чем мы думаем вообще?
В любой непонятной ситуации.
Вот у нас есть пустое место в логе.
Нам нужно чем-то его заполнить.
Вы не знаете, что делать?
Что?
Вы не знаете, что делать и предлагать?
Да.
Ну вот предлагай NOP.
Всегда безопасно.
Не очень полезно, но безопасно.
А если NOP выиграет?
Если NOP выиграет, то он займет место в слоте 5.
Для этого он нам и был нужен, чтобы занять место.
А в каком случае он выиграет, если...
Ну если вдруг в этом слоте действительно пусто.
Если мы пойдем на Quorum и хоть один голос здесь увидим,
чужой, с чужой командой.
Вот отсюда.
То это означает, что мы уже
предложим не NOP.
Про NOP забудем, про свое значение, про свой вход.
А предложим что-то другое.
Но нам не важно.
Окей.
Могу это удалять, да?
Мы наконец-то это постигли.
Хорошо.
Тогда вопрос к вам.
Не кажется ли вам эта конструкция супер неэффективной?
Но она не похожа на что-то разумное,
потому что
у вас есть реплики,
они независимо получают команды
и предлагают в одних и тех же слотах
разные команды.
Вот реплика 1 получила одну команду,
хочет ее положить в слот 3,
другая реплика получила другую команду,
а пользователь хочет положить ее в слот 3.
И они начинают друг с другом конкурировать.
И мы помним, что в алгоритме
Single Decrep Access есть сценарий конкуренции,
но есть обязан быть LifeLog
и это LifeLog выглядит так.
Мы говорим Prepare 1,
Prepare 1 с синим пропаузером.
Потом красный говорит
Prepare 2, Prepare 2.
И перебивает
синего и не дает ему пройти
через вторую фазу.
Синий
проваливается на фазе
Accept,
ретравится,
выбирает синий пропаузер,
перебивает синий пропаузер,
перебивает синий пропаузер,
Accept ретравится,
выбирает себе новый ballot number,
p3, p3.
Его снова подрезают,
p4, p4,
и вот так продолжается бесконечно долго.
Не факт, что это продолжается бесконечно долго,
вряд ли это продолжается в реальности бесконечно долго,
точнее точно не продолжается бесконечно долго,
но все же тратится какое-то время.
Причем что неприятно,
неприятно, эти пропаузеры,
то есть реплики,
они могут друг с другом вот так вот соперничать,
даже если они предлагают вообще одно и то же.
То есть у них как бы команда общая,
они хотят записать в окна,
не могут, потому что мешают друг другу.
Вот хочется каким-то образом
такие сценарии исправить.
В Single Decrease Access мы говорили,
что для этого,
теория МФОП говорит,
что чтобы это исправить, вам нужно использовать время.
Время или рандомизация, или то и другое.
Ну и что мы могли бы сделать в Single Decrease Access?
Мы могли бы использовать exponential book-ов.
То есть когда мы
проваливаем какую-то фазу, первую или вторую,
мы перед тем, как повторить их заново,
основываем N,
некоторое время спим.
Время этого сна, этой паузы,
мы экспоненциально увеличиваем,
потому что надеемся, что за это время
другой наш конкурент успеет пройти
через две фазы,
я не знаю, сколько времени это у него займет,
поэтому мы увеличиваем и увеличиваем до тех пор,
пока он не влезет.
И мы рандомизируем эти паузы
для того, чтобы
избавиться от совсем уж полной симметрии,
если вдруг все работает абсолютно зеркально.
Но это мы так делали в Single Decrease Access,
когда у нас была задача
выбрать одно значение.
В задаче Multiprocess делать так неэффективно.
Можно сделать гораздо
и лучше,
и в каком-то смысле проще.
Я говорил вам,
когда мы обсуждали FOP,
что вот есть задача консенсуса,
и на ней все не заканчивается.
Можно свести ее к задаче построения
детектора сбоев.
То есть не пытаться время использовать
прямо в консенсусе,
а построить некоторый детектор,
который будет отличать
на каждом узле поместить детектор,
который будет отличать смертные узлы от живых,
и время использовать внутри него,
потому что в конце концов нам нужно
для того, чтобы сбои находить.
А консенсус, пусть просто
им пользуется как черным ящиком,
и времени мне оперирует.
И я даже сказал, как этот детектор должен быть устроен.
Самый простой детектор.
Это детектор выбора лидера,
который по текущему времени
на каждом узле возвращает узел,
который, по мнению
этого детектора, является
текущим лидером.
То есть смысл в том,
чтобы избавиться от конкуренции
выборов среди реплик
какого-то одного,
Лэмпард это называет выделенный пропозер.
Мы не Лэмпард,
мы будем говорить лидер.
Смысл в том, что лидер один,
и только ему разрешается предлагать команды.
Если вы не лидер,
то вы отдаете свою команду лидеру,
и пусть он ее предлагает.
Лидер фиксирует порядок команд,
выстраивает их в паслотам лога,
и пытается их закомиссить.
И если другие лидеры
не лидеры,
то они не лидеры.
И если другие ему не мешают,
то он быстро проходит через две фазы
и добивается успеха.
И вот именно в этой подзадаче
выбора лидера
можно и стоит
пользоваться временем и рандомизацией.
Причина, по которой Паксос
должен вам нравиться,
состоит в том, что
он очень сильно упрощает
рассуждение об этом лидере,
потому что Паксос очень хорошо
отделяет свойства
safety консенсуса от свойства
liveness консенсуса, termination.
Вот вы можете строить этот детектор
любым способом, абсолютно любым.
Скажем, в алгоритме,
где вы реплицировали key value,
вот ячейку памяти,
там вы должны были аккуратно пользоваться временем,
чтобы не нарушить линеризуемость.
Там локальные часы вообще не подходили,
где true time нужно было аккуратно
тоже использовать, там были какие-то нюансы.
Вот здесь творите, что хотите.
Лидер, неправильно
выбранный, неаккуратно выбранный,
не способен ничего сломать.
Если у вас лидер — это функция, которая возвращает
собственный идентификатор, то это тоже
легальная процедура, потому что
в любом случае, даже если у вас
лидеров несколько, все равно Паксос
с этим справится.
Но если вы все-таки выбираете лидера
таким образом, то есть eventually он будет
выбран все-таки один общий,
то станет хорошо.
Но плохо не станет.
Никогда. А хорошо может быть.
Вот в этом смысле
Paxos safety от liveness
декомпозирует совершенство.
Здесь вы можете
ничего не думать про задачу консенсуса.
Ну и что предлагает Lampard?
Как именно он предлагает
лидера выбирать?
Вопросы есть пока или нет?
Да.
Честно, он не применяет их, применяют их все.
Лидер выбирает порядок
и предлагает их комитет слоты.
Все остальные этого не делают, чтобы ему не мешать.
В этом смысле
это некоторое узкое место,
но такова конструкция
RSM by design практически.
Она по задумке
в этом и состоит.
Вопрос у тебя был в этом или
в чем-то другое?
Тогда
говорим про какую-то очень простую
процедуру выбора лидера,
которую предлагает сам Lampard.
Вот ничего проще
и глупее придумать невозможно.
У нас есть три реплики.
Вот давайте каждый из них запустит,
ну вот мы строим такой отдельный модуль,
каждый из них будет в фоне выполнять
такую процедуру.
Через равные промежутки времени
мы будем всем
другим узлам отправлять
сообщение, которое называется
heartbeat
с аргументом
ID, где ID
это наш идентификатор.
Некоторый постоянный.
Ну можно уже догадаться, как мы будем этим
пользоваться.
Вот давайте считать, что лидером
будет узел, у которого максимальный
ID, и который сейчас жив.
Ну как мы поймем, что он жив,
ну вот у него бьется сердце.
Мы продолжаем получать от него вот эти сообщения
heartbeat.
То есть у него есть
максимальный ID,
который сейчас жив.
Ну как мы понимаем, что он жив,
ну вот у него бьется сердце.
То есть с одной стороны мы отправляем
эти сообщения всем, а с другой стороны
мы слушаем эти сообщения других узлов
и в каком-то временном окне
просто запоминаем последнее,
старшее ID.
И вот по этому старшему ID мы
выбираем, кто сейчас лидер.
В этом смысле выбор
лидера всегда, ну
вы всегда знаете, кто лидер.
Не бывает такого, что лидер неизвестен сейчас.
По умолчанию это всегда вы,
но если вдруг вы получите heartbeat
с каким-то большим идентификатором,
то вы скажете, ну окей, значит не я лидер.
Перебьюсь.
Но если вдруг вам этот лидер
с большим ID перестанет присылать свои heartbeat,
то вы понимаете, что на видимого номера
и забудете про него.
Так вот я же говорю,
что мы выделили под задачу
построение детектора сбоя,
построение модуля выбора лидера,
и мы используем время.
И там мы можем сказать, вот мы ожидаем,
что все сообщения доставляются там
за 5 миллисекунд.
Мы ожидаем, что если за 30 секунд
мы heartbeat не получим,
то мы считаем, что машина мертва.
Вот каждое такое
предположение может быть неверным.
То есть машина жива,
но 30 секунд она ничего не управляла,
потому что я не знаю почему.
Это не страшно,
потому что наш модуль выбора
лидера ошибется.
Он исключит вот этот лидер
с большим узелом с большим ID,
кандидат.
Он перестанет считать его лидером.
Наша реплика перестанет считать этот узел
лидером.
И сама станет, допустим, лидером.
То есть сама себя посчитает.
Ну окей, пусть считает.
Тогда она тоже будет предлагать команды.
У нас появляется два пропаузера в системе.
Но два пропаузера – это нормальная ситуация
для Paxos. Paxos это переживет.
Paxos переживает, когда у нас все пропаузеры.
Но если здесь работает стабильно,
то, видимо, если 30 секунд машина
не отвечает, то действительно может быть она умерла
и нужно перестать на нее надеяться.
Почему всеми ко всем?
Ничего себе.
У тебя там 5 реплик, а не раз в 5 миллисекунд
на 1 сообщение друг другу.
У тебя в этом системе хочется
сотни тысячи в секунду запросов переживать,
а ты вот пытаешься это сообщение сэкономить.
Ну нет, вообще, на самом деле
зря я потешаюсь. На самом деле
это очень разумная вещь, говоришь.
Вот представим себе KVL-хранилище.
Много ключей, не все
в одну машину не помещаются.
Мы берем это пространство ключей, делим на диапазоны.
И говорим, вот каждый такой маленький диапазон
будет реплицироваться независимо.
Будет отдельным RSM-ом.
И для каждого такого диапазона
будут наборы, то есть у нас есть полмашины,
допустим, 100. Мы делим набор ключей
на тысячу диапазонов
и каждый из тысячи
диапазонов хранится на каких-то трех машинах
в виде RSM-а.
Но при этом
каждая отдельная машина, физическая,
она может хранить, быть репликами
для разных диапазонов.
И действительно в таком случае
не делают так, что для каждого диапазона
свои собственные хардбиты.
Можно их поклеить.
То есть если ваш узел является
репликой сразу для там
10 разных регионов
диапазонов ключей,
то можно склеить все хардбиты
для всех этих диапазонов в какое-то общее сообщение.
На этом
так-то экономить. Так что в продакшене
так делают. Мы так не делаем,
потому что мы до продакшена далеки.
Но если мы говорим
просто про 5-7 реплик
и сообщение раз в какое-то количество
миллисекунд, то они же
маленькие, их мало,
то есть это не страшно. У тебя не бывает здесь, что
тысячи реплик в RSM, у тебя их там 3-5-7
какой-то такое число.
Стоит ли так делать
в продакшене? Нет, не стоит, ни в коем случае
никогда так не делаете. Почему?
Потому что это плохо кончится.
Ну вот представьте,
что у вас 3 реплики.
Давайте я их логи так нарисую пока.
И среди них
случился partition.
Сеть раскололась
на две части,
и отрезала R3 от остальных.
Понятно, что в каждой части
будет собственный лидер,
потому что он всегда выбирается,
и здесь лидером будет
видимо R2,
а здесь R3.
Лидер R3
ничего плохого в принципе сделать не может,
потому что ни одного кворума не может собрать.
А лидер R2, он способен работать,
если он получает команды от пользователей,
то он может их писать в собственный лог
и копировать их на R1,
собрать кворум.
Ну вот,
это все.
Ну вот,
и
допустим,
он принял 100-500 команд.
Ну ладно, 100-500 много,
давайте 100.
Это уже будет больше похоже
на реальность.
Вот 100 каких-то команд
прошел через 100
параллельно
прошел через 100 паксусов.
И он-то знает,
что теперь эти команды закомичаны,
поэтому реплика R1 не знает про то, что они закомичаны,
она просто получила и проголосовала за них.
Что происходит дальше?
Дальше partition лечится.
R3 присоединяется к остальным.
И видимо,
все они понимают, что лидер теперь R3,
потому что у него самый большой идентификатор.
R2
не переживает
этой трагедии и умирает.
А он единственный знал про то,
что у нас 100 команд закомичаны.
В итоге теперь у нас есть лидер R3 с пустым логом
и R1, у которого
лог на 100 команд заполнен,
но он не знает, что они закомичаны.
И что делает R3?
Он получает команду свою
и начинает ее коммитить.
И понимает, что ничего не вышло,
потому что выиграет другая команда.
Реплика R1 на первой фазе пакса
соответствует этому непустым голосу.
И он будет так
долго не спеша
заполнять свой лог.
Команды были закомичаны,
поэтому они не потеряются.
Но тем не менее мы начали с нуля
и повторяем весь протокол заново.
Просто потому, что мы выбрали лидера
с пустым логом.
Возможно, так делать не стоит.
Лэмпард в своей статье про это пишет,
что из полугодового отпуска
вернулся человек,
которого звали вот так.
Намекает, что у него
очень большая буква в имени,
поэтому он стал президентом этого парламента.
Но он ничего не знает за 6 месяцев,
чем парламент занимался.
Но тем не менее он становится
этим лидером парламента
с пустыми записями.
Это корректность не ломает,
но это задерживает прогресс
всей системы.
Наверное, так в продакшене делать не стоит.
И в следующий раз,
когда у нас будет алгоритм RAFT,
мы увидим, что в этом алгоритме
лидер выбирается...
В RAFT все сложнее,
потому что там
протокол репликации
и выбор лидера, они друг с другом связаны.
У нас они перпендикулярно там связаны.
Это очень важное отличие.
И мы поймем, почему скоро.
Но
сбился смысл в свои.
Да, и в RAFT
лидер выбирается не абы как,
он выбирается только среди тех узлов,
у которых лог достаточно полный.
Там невозможно
выбрать лидера, у которого пустой лог.
С другой стороны, там можно вообще
не выбрать лидера, тогда вообще ничего сделать нельзя,
потому что без лидера протокол работать не будет.
Что?
Ну, лидер это, как написано
на тиске, выделенный пропозер.
Это узел, который предлагает команда.
И он нужен другим
для того, чтобы...
Мы его выбираем затем, чтобы
пропозеры, реплики не конкурировали.
У нас три реплики,
чтобы среди них был только один пропозер,
и он спокойно проходил через две фазы паксиса,
и система совершала прогресс.
Никто ему бы не мешал.
Мы можем выбрать
двух разных лидеров,
может случиться так, что в системе может быть несколько лидеров,
но это неизбежно.
Но сам протокол паксис к этому устойчив,
потому что он заточен
под разных пропозеров, которые
являются лидером.
Окей.
Промежуточный итог.
Чего мы достигли в текущем моменту?
Мы построили вот такой вот...
В целом, это уже можно написать.
Если вы это напишете, уже будет радость,
я буду делать еще крышей кавери.
Значит, вы
взяли три реплики.
Взяли клиента.
И он приходит в вашу систему и задает запрос.
Давайте пронумеруем.
Один, два, три.
И вот это лидер сейчас.
Значит, вы клиент, вы приходите со своей командой,
отправляете ее на какую-то машину.
Эта машина смотрит на
свой модуль выбора лидера, понимает, что она
не лидер. Она предлагать команду не может.
Она говорит, лидер, предложи.
Лидер предлагает.
Он говорит, prepare,
выбирает слот, говорит,
prepare,
promise,
accept,
accept.
После этого префикс лога
закомитился, допустим.
Мы применили команду.
Мы вернули результат
сюда.
И вернули пользователя.
Вот получилось
четыре раунд три.
Вот так делать нельзя, конечно.
Но потому что очень медленно.
Как сделать лучше?
Вот где здесь есть откровенная
тупизна?
Что значит справой?
Нет, ну есть
и есть,
но и есть.
Вот это
в этом
и есть.
И есть.
И есть.
И есть.
И есть.
И есть.
И есть.
Нет, ну еще раз, ты всегда собираешь корм с теми, кто быстрее, там с кем собрался, те и молодцы.
Ты не выбираешь, кто из них лучше, а кто хуже, ты отправляешь всем, кто быстрее ответил, то ты...
Не, я имею в виду, если нам вот левая рептика отправила команду, значит она уже как бы ответила.
Она с клиентом общалась, кто ему будет еще отвечать.
Ну, смотрите, мы выбираем лидера надолго.
Вот в продакшене у вас лидер может жить неделями, скажем, ну неделя может жить.
Никто ему не помешает, если работает хорошо.
Поэтому вот зачем нам тратить вот этот случайный хоп?
Вместо этого давайте...
Вот если мы клиент, и если у нас есть вот эта машина лидер,
то если мы придем не к лидеру сначала, это было 1.7,
приходим к неридеру, то вместо того, чтобы фарвардить команду к лидеру,
эта реплика скажет клиенту редирект.
А клиенту себя закаширует, что лидер сейчас 3 и будет ходить к нему сразу потом.
Ну то есть в первый раз он сделает лишний хоп,
а дальше будет потенциально бесконечное время экономить себе 1 раунд трип.
В итоге мы получим не 4 RTT, а 3 RTT, потому что вот этот 1 лишний точно будет.
Странный вопрос.
Еще раз, в алгоритме ПАКСС выбор лидера перпендикулярен сейфти.
Можно выбирать лидера неправильно и постоянно неправильно.
То есть разные узлы могут выбирать разных лидеров,
в смысле они могут считать лидерами разные реплики.
Эта корректность на корректность не влияет.
Твоя задача построить такой протокол выбора лидера,
который, когда все хорошо, позволяет поддерживать одного и того же лидера,
не меняя его постоянно.
Вот если мы эту задачу решили, то мы считаем, что наша процедура довольно успешна.
Если вместо того, чтобы углубить так вызовов, мы сделаем редирект,
то мы дальше долгое время будем ходить сразу к лидеру и экономить 1 RTT.
Когда-нибудь наш кэш протухнет, потому что лидер поменяется.
Узнаем нового и будем ходить дальше к новому.
Это было очевидно, можно было и не говорить.
А теперь самое главное.
Все это можно было бы уже считать мультипакссом,
но можно и не считать, потому что это каждому дураку понятно.
Все, что было до этого момента.
В принципе, можно на этом в домашней части ограничиться.
Уже будет достижением некоторым.
Но мультипаксс по-настоящему появляется тогда,
когда мы делаем вторую важную оптимизацию, помимо выбора лидера.
И без нее мультипаксс не мультипаксс, конечно.
Итак, у нас есть две реплики.
Внутри реплики есть следей них лидер и есть какая-то.
Вот мы сказали, что хотим заполнять логи параллельно.
Если у нас есть две команды, положим их в первый, в второй, в третий.
Сразу запустим там паксс.
Вот давайте так делать не будем прямо сейчас,
а будем делать хуже.
Будем заполнять логи последовательно.
Почему-то это нам способно помочь.
Итак, смотрите.
Я лидер, я получаю команду от клиента.
Я говорю...
prepare, slot k, ballot number n.
Ну и все.
Мне другая реплика отвечает.
Promise.
Я говорю, здорово.
Теперь accept.
Accept, k, slot k, ballot number n, значение...
Ну какое-то c.
Команда c.
Она отвечает.
Принято.
Вот я закомитил команду.
Теперь я начинаю следующий слот.
Говорю...
prepare, k плюс 1, какой-то n штрих.
Ну n штрих какой-то.
Получаю снова promise.
И дальше accept.
k плюс 1, n штрих.
Какая-то команда c штрих.
Ну вот делаю подряд.
Прохожу первую фазу, вторую фазу, первую фазу, вторую фазу.
Вот представьте, что вы процессор.
Собственно, поэтому я всякие мувовые джампы рисовал.
И вот у вас есть команда.
Вот у вас есть instruction pointer, он двигается вниз.
И вот циклы, которыми вы работаете.
И вот у вас есть инструкция.
Там есть фазы fetch, decode, execute, writeback.
Ну какие-то условные фазы.
Не думаю, что они буквально соблюдаются.
Но об этом можно было бы так думать.
Вот чтобы выполнить инструкцию, нужно сначала загрузить ее из памяти.
Потом декодировать ее внутри.
Потом, собственно, как-то ее исполнить.
Потом записать следы обратно в память.
Потом у вас есть вторая инструкция.
И вы говорите такие fetch, execute, fetch, decode, execute, writeback.
Потом третий и так далее.
Вот если вы так будете делать, то ваш компьютер будет работать бесконечно долго.
Вместо этого вы можете заметить, что, видимо, шаги в этих инструкциях разные,
потому что они проразные.
Видимо, они выполняются разными какими-то модулями, блоками в процессоре,
какими-то разными частями схемы.
Поэтому вы говорите, что пока я декодирую первую инструкцию,
я уже буду загружать вторую инструкцию.
А когда я выполняю первую инструкцию, я буду декодировать вторую
и загружать третью инструкцию.
Вот это называется конвер.
Конвер – это сложно, потому что нужно хорошо предсказывать следующие инструкции.
Нужен бранч-предиктор аккуратно, и нужно, чтобы зависимости между инструкциями
не было, чтобы все хорошо сложилось.
Но в идеале все вот так.
А у нас же тоже такие вот инструкции, которые есть двумя инструкциями.
А у нас же тоже такие вот инструкции, которые из двух фаз состоят.
И что можно заметить?
Что в фазе accept в слоте K и в фазе prepare в слоте K плюс 1
эти два консенсуса друг с другом напрямую никак не связаны.
У них разные слоты и, вообще говоря, разные N.
А может быть, не нужно N как-то независимой.
Может быть, можно воспользоваться тем, что N у нас будет одинаковая.
Независимые поэтому могут быть и одинаковыми.
То есть выберем для слотов одно и то же N.
А потом подумаем, почему мы вообще делаем prepare K плюс 1
после accept на слоте K?
Ведь для этого prepare даже команда не нужна.
То есть у нас была одна команда, мы ее получили и прошли через два фазы.
Следующей команды у нас еще нет.
Но это же нам не мешает взять и пройти prepare сразу вот здесь.
То есть когда мы отправляем команду accept в слоте K
с ballot number N и командой C,
мы можем в этом же сообщении сразу сделать prepare для слота K плюс 1.
И тогда, когда нам придет команда C штрих,
то мы ее получим от пользователя раз хоп,
а потом сразу перейдем к фазе accept.
Второй хоп, но второй round trip.
Мы не будем проходить после получения команды от пользователя
через две фазы, потому что через первую фазу мы уже прошли,
потому что эта фаза не зависела от самой команды.
И мы эту фазу уже прошли раньше.
То есть мы комития слот K
заодно прогреваем для будущей записи слот K плюс 1.
Ну а теперь можно задуматься, а почему мы прогреваем только один слот?
Давайте сделаем вместе с этим prepare еще один prepare
K плюс 2.
Но можно сделать три prepare.
То есть мы говорим с одной стороны accept.
Да, конечно же эта малина может сломаться,
потому что мы сделаем это prepare.
Мы прогрели будущий слот, потом сразу закомитили,
потом прогрели будущий слот, потом сразу закомитили в один хоп.
А потом делаем сразу accept, а он обламывается,
потому что появился какой-то другой конкурент,
конкурирующий с нами лидер, и он нас перебил.
Ну это просто сброс конвейера.
Тогда мы откатываемся на первую фазу, сделаем все заново.
Но если такого нет, то можно это дооптимизировать до каких-то безумных пределов.
А именно, вот выполняя accept для слота K с ballot number N
и какой-то командой C,
можно к нему прицепить prepare K плюс 1 N,
пользуясь тем, что у нас команды не требуются для prepare,
а ballot number независимая, поэтому пусть будет одни и те же K плюс 2 N.
Ну и когда мы остановимся?
Мы не остановимся никогда.
Мы отправим prepare такого вида.
Prepar, который говорит, я хочу захватить все слоты, начиная с K.
Да, все слоты начиная с K.
И если accept получит этот prepare и увидит, что у него все слоты пустые,
то он у себя запомнит, что начиная с некоторого суффикса K,
с некоторой позиции K, как будто бы для всех слотов
этот accept получил команду prepare с ballot number N.
То есть это бесконечно много prepare, но они описываются просто одной записью.
Ну а дальше, после того, как мы захватили бесконечную суффикс лога,
можно в одну фазу коммитить свои команды.
И так мы избавляемся от еще одного round trip.
Но мы одну посылаем.
У нее просто бесконечный смысл.
Ну короче, придумай, как правильно сказать об этом, а ты меня понял.
То есть мы говорим prepare на каждом будущем слоте, начиная с K.
Прямо на бесконечности.
Реплика запоминает, что все слоты, начиная с K,
они захвачены пропозером, который прислал этот prepare,
с ballot number N.
Если вдруг acceptor получит какой-то узел, который дал такое бесконечное обещание,
получит какой-то prepare с большим номером для какого-то слота,
то он может про него забыть.
Сказать, что все больше невалидно.
А acceptor, видимо, этот пропозер перестанет на это надеяться
и откатится обратно на первую фазу.
Но если ему удалось на quorum собрать такие бесконечные prepare,
то он захватил суффиксы лога и может коммитить все в один round trip.
И смотрите, что получается.
За один хоб мы кашируем лидера на клиенте, попадаем сразу в него,
а этот лидер, пользуясь тем, что он уже прогрел все суффиксы лога,
приходит к фазе accept.
Два RTT в итоге.
И что получилось?
С одной стороны, мы реплицируем таким образом произвольный автомат
с произвольными операциями.
На что, скажем, алгоритм со второй лекции был неспособен, который мы писали.
И при этом мы в нем делаем всего лишь одну фазу.
Собираем всего лишь один quorum синхронно.
Там было два quorums.
То есть мы решили более общую задачу и еще и быстрее.
Ну, по крайней мере, на быстром пути быстрее.
Иногда, конечно, так же, но это случается, допустим, редко,
если у нас сеть работает стабильно достаточно.
То есть мы решили более общую задачу более эффективно,
и при этом мы не потеряли коллекции нигде.
Да, мы как будто делаем одну фазу, но на самом деле мы не делаем одну фазу,
мы делаем две фазы.
Просто одна фаза делается таким батчем неявно.
Но при этом внутри каждого слота у вас алгоритм проходит
все-таки через две фазы, вы можете их восстановить логически.
Ну, если вы все корректно написали, конечно.
То есть мы по-прежнему пользуемся тем, что Paxos у нас работает в каждом слоте.
Просто одну фазу, первую фазу для разных слотов мы склеили в одно сообщение
и сделали один раз общий.
Вот то, что получилось, называется multipaxos.
Что?
И смотрите, вот теперь, во-первых, понятна ли оптимизация?
Понятна ли аналогия с конвейером, с бросом конвейера и всем этим?
Если понятно, то это для нас очень полезный момент,
потому что теперь можно объяснить, что значили все эти n, n-p,
и вот вся эта дичь, p-r и promise.
Потому что вот в этом алгоритме у них никакого смысла конкретного не было.
Невозможно было объяснить, что это все значит.
Теперь, имея multipaxos, можно легко объяснить, что такое n.
Вот смотрите, мы здесь, мы лидер, берем это n
и с помощью него захватываем все будущие слоты лога.
Кто нам способен помешать в будущем?
Какой-нибудь новый лидер, который, видимо, выберет больше n.
Так вот, как же можно было бы это n назвать? Каким словом?
Совсем не постарался.
Вот, кто-то говорит слово поколение, это хорошая идея.
Поколение, эпоха.
Ну и не знаю, можно думать, это бесконечное количество людовиков,
которые нумируются, они приходят к власти,
у них новый номер больше, чем предыдущий.
Вот здесь то же самое, у вас новый лидер,
то есть, конечно, тут вообще n и лидеры независимы,
то есть процедура выбора лидера никак совершенно не завязана.
Ну во-первых, рафта завязана,
ну и как бы по смыслу это явно должно быть согласовано,
то есть вы выбираете нового лидера,
и он должен захватить все слоты в логе,
начиная с некоторого k.
Для этого ему нужно выбрать n больше, чем был раньше.
Вот, так что это вот n, давайте где-то это писать.
N это эпоха.
В рафте это называется термом,
ну по смыслу эпоха поколения.
А что такое np у аксептора?
Вот мы, во-первых, сравниваем n и np,
и если n больше, чем np, то мы np обновляем,
а если меньше, то отвергаем.
Что? Я какую-то шутку пропустил, мне обидно сейчас.
Кто-нибудь.
Да, вот именно так.
То есть у нас каждый аксептор сейчас слушает
какого-то лидера с каким-то n.
Вот мы помним его поколение.
Вот, вот, вот.
Вот, вот.
Вот, вот.
Вот, вот.
Вот, вот.
Вот мы помним его поколение.
Когда мы получаем prepare n и сравниваем,
и оказывается, что np больше, чем n,
и мы отвергаем это сообщение, то что это означает?
То, что мы отвергаем о кого-то старого лидера.
Когда мы обновляем np,
потому что оно было больше, оно было меньше, чем n,
то это означает, что мы просто услышали нового лидера
и переходим в его эпоху.
И даем обещание вот здесь вот старых лидеров не слушать.
Вот.
А что такое prepare accept?
Ну вот prepare сообщение общее
это такое утверждение лидерства.
То есть я говорю, я лидер, мне говорят, хорошо.
Наверное присягивал.
Да.
Как удачно ты зашел с души.
Вот просто идеально подбираешь нужные слова.
Значит, здесь они нам присягают,
а тут мы просто реплицируем команду.
И на быстром пути у нас выполняется только репликация теперь.
То есть когда нам приходит клиент,
он отправляет команду сразу лидеру,
этот лидер сразу проходит через фазу accept,
то есть реплицирует ее сразу.
Ну потому что ему и так уже все верят.
Что?
Ну пока не появится новый, и все это как-то не собьется.
Ну все здорово,
только тут можно задуматься.
Мы с вами уже месяц приходим сюда с субботом
и придумали следующий алгоритм.
У нас есть клиент, он посылает команду лидеру,
лидер пишет ее на Quorum и подожидает ее нам.
Это какое-то очень скромное достижение,
можно подумать, что это можно было придумать гораздо быстрее.
Так вот, нет, нельзя было.
Потому что, да, 99,9% времени работает такой алгоритм,
который придумывает любой школьник.
Но с другой стороны, когда-то лидер меняется.
И вот тогда происходят все сложности.
Нужно аккуратно перейти от старого к новому
и ничего не потерять.
То есть все команды, которые применены,
должны переехать в новую эпоху и не потеряться там.
Вот ровно для этого алгоритм контакта.
Вот был озвучен некоторое вранье,
что алгоритм консенсуса нужен для того,
чтобы выбирать очередное сообщение.
Разумеется, нет, он не для этого нужен.
Алгоритм консенсуса не про выбор очередного сообщения.
Это просто дело лидер.
Просто выбирал у себя там очередное сообщение,
которое он поставит в очередной слот.
Алгоритм консенсуса Single Decree Paxos
это не выбор очередного сообщения,
это случай, когда человек не хочет
А алгоритм Paxos это не выбор очередного сообщения,
это случай конкуренции лидеров.
Это случай, когда у вас уже есть лидер,
появляется новый, и они должны друг с другом
не поломать всю конструкцию.
А алгоритм Paxos это такой изолированный случай
конкуренции лидеров, когда у вас каждый пропозор
это просто лидер.
А порядок команд выбирает, конечно, не консенсус,
выбирает просто лидер и назначает их в некотором.
Чтобы более наглядно это было,
вы уже читали статью про GFS,
и там были мутабельные чанки,
там чанки можно было перезаписывать.
Но мы на семинаре не стали так делать,
а в GFS стали когда-то.
И кто определял порядок записи в чанке?
Для каждого чанка там выбирался праймари,
то есть были копии чанка,
какой-то узел из них становился праймари,
и вот ему валились все записи,
и он нумеровал их.
А что было, когда он нумерал и выбирался новый?
Да какой-то бардак был,
потому что никаких разумных гарантий не было там.
То есть этот праймари получал какие-то команды,
сделал, например, получил пять команд,
пять записей,
четвертую и пятую записал на две реплики из трех,
потом взорвался.
Выбрался новый праймари,
и как бы нет никакого протокола,
там не было никакого протокола,
в котором все это придется в согласованное состояние.
Новый праймари не знал,
какие записи уже начал сделать тот,
он не может это никак узнать,
не может его разумным образом восстановить.
Ровно в этом месте
GFS не хватало консенсуса.
То есть там не консенсус,
а какие-то ручные костыли,
которые не работают,
но и там, собственно, статьи пишут,
что в случае смерти этого самого праймари
никаких гарантий разумно дальше нет.
Данные в несогласованном состоянии.
Сингл дикрипактес нужен для того,
чтобы как раз аккуратно перейти через эпоху
и оставить это на согласованном состоянии,
чтобы все логи по-прежнему
продолжали дальше бежать синхронно.
Вот такая глубочайшая мораль.
Но все было бы замечательно,
если бы мы построили алгоритм.
А мы не построили алгоритм.
Потому что, во-первых,
непонятно, как выбирать вот эти n.
Не было ни одного слова про это сказано.
Вы выбираете как-нибудь,
чтобы все было аккуратно, все хорошо, непонятно.
Непонятно, как именно реагировать
на команду prepare с бесконечным суффиксом.
Вот я получил такую команду,
а у меня уже есть какой-то там...
Я получил k100,
а у меня в 105-м слоте уже какая-то команда есть
с большим n. Что мне делать?
Что мне отвечать именно?
Можно по-разному это сделать?
Что мне делать,
когда я лидер захватил суффикс лога,
шлю всем accept,
а мне какая-то реплика говорит нет?
Понятно, что я должен
откатиться на первую фазу.
А как именно?
Никакого ответа на эти вопросы
не существует для нас.
Лэмпард на них не ответил.
И вот это
породило бесконечное количество вариаций
алгоритм-паксов, которые
решают эти проблемы
сериями или иным способом.
В общем, много пустых мест.
Интуитивно понятно, что можно чем-то их заполнить.
Но как именно непонятно.
И в следующий раз мы поговорим
как раз про алгоритм, который
автор утверждает, что он
в отличие от паксуса понятен,
а мне кажется, что они просто взяли
и эти места заполнили и получили алгоритм,
который называется raw.
И я попытаюсь вас убедить,
но авторы вас в одном будут убеждать,
а я буду убеждать в противоположном,
что в каком-то смысле
это вот такой мультипаксус,
в котором все зафиксировано.
Все свободные места зафиксированы.
Я бы сказал, что ничего, кроме мультипаксуса,
это все мультипаксус.
Все эти алгоритмы,
они в принципе устроены похожим образом.
И вот мультипаксус
из них самый общий, потому что
там просто много недосказанности.
В следующий раз мы зафиксируем это все
и в домашках мы напишем
сначала вы напишете синглудик и паксус,
а потом мы напишем мультипаксус
как-нибудь,
а потом напишем уже raft,
как советуют авторы в статье
с конкретным протоколом.
И увидим, что это проще было.
Ну или нет, я не знаю, что вы видите,
попробуйте сами.
На сегодня все.
Отдыхаем 10 минут.

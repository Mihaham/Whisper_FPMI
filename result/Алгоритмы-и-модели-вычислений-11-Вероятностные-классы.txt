Так, давайте начнём, значит сегодня у нас вторая лекция про вероятностные
вычисления, и сегодня мы, собственно, познакомимся с сложностными классами,
которые возникают вот из разного рода рандомизированных алгоритмов.
Ну прежде всего нужно остановиться на какой-то модели вероятностных вычислений.
Ну и мне будет удобно использовать вот такую вот модель.
Модель вероятностных вычислений заключается в том, что у меня есть просто какая-то функция
вычислимая от двух аргументов. Значит, здесь x это, собственно, вход, а r это
случайные биты. Ну и, соответственно, это вот получается функция, значит, вычислимая за время
по лином от длины x. Соответственно, нас интересует, ну получается, что значение это 0 или 1,
то есть получается результат работы, это, на самом деле, случайная величина вернулевская,
то есть она принимает значение 0 или 1. То есть результат работы, значит, это вернулевская,
вернулевская случайная величина, ну то есть она принимает 0 или 1 с какими-то вероятностями.
То есть тут механика такая, что длина x дает длину r, ну может, для разных x будут разные r,
тогда есть некоторые максимальные длина. И, соответственно, рассматривается равномерное
воспределение на всех возможных r. Ну и какие-то r дают исход 0, какие-то r дают исход 1,
но и вот эта вот вероятность, это просто доля, значит, доля тех r, которые дают 1 среди всех
возможных r. То есть вот поскольку всего два исхода, то есть одно число, которое описывает
эту случайную величину. Ну и вот теория ложности принята, давайте напишу равно 1, принято вот такое
вот обозначение, значит, вообще вероятность там по-всякому обозначают. Но когда у нас много
параметров, то можно, чтобы не запутаться, написать, что именно у нас случайное. То есть
здесь вот в этой величине v от x и r, r случайное, а вот x фиксированное. Поэтому
это подчеркивают, да, это подчеркивают, записав вероятность по r. Это важно, на самом деле,
потому что в других приложениях может быть и x случайное, да, то есть когда мы говорим
про какую-нибудь сложность в среднем, например, да, то тогда аргумент тоже будет случайным,
даже для для детерминированного вычисления, и там будет другая вероятность. Вот, но вот здесь нас
вот эта вероятность интересует. Вот, но и идея разных классов в том, что мы каким-то образом
ограничим вот эту вероятность для случая, когда x лежит в а, и когда x не лежит в а. Да, то есть,
значит, разные классы, значит, разные классы получаются из оценок
на вот эту вероятность при x лежащем и x нележащем. Так, но, наверное, нужно заметить,
что на самом деле те классы, которые мы уже знаем, можно в таких термах определить. Да, поэтому,
ну, в принципе, видно уже и через обозначение, да, то, что у нас здесь происходит, похоже на то,
что у нас в NP было, да, там у нас тоже было два аргумента, но определение было такое, что если
x лежит в а, тогда такой второй аргумент существует, да, то есть r, а если x не лежит в а,
то тогда такого аргумента не существует. Вот, ну, и получается, что можно сформулировать NP
в таких терминах. Значит, можно так описать NP. Значит, если x принжит а, то тогда вероятность
по r того, что v от x пр равно 1, это будет больше нуля просто, вот, ну, а если x не принжит а,
то тогда эта вероятность равна нулю. Вот, ну, и CoNP тоже на самом деле можно описать. Значит,
если CoNP, то можно считать, что здесь равно 1, а здесь меньше 1. Вот, то есть те кластеры,
у нас уже были, вот так описывают, но они, конечно, по сути, они, конечно, не являются вероятностами.
Вот, а почему вообще так, да, значит, почему не можем вероятностным алгоритмом решать все из NP?
Ну, вообще говоря, конечно, никто не знает, может быть, и можно, но почему не пройдет тривиальный
алгоритм, что мы там берем случайно r и, соответственно, если попали, то возвращаем 1,
если не попали, то возвращаем 0. Значит, это не работает просто потому, что этих r может быть
совсем мало, да, в крайнем случае может быть одно r из экспедиционного количества. И чтобы
иметь какую-то существенную вероятность в это r попасть, нужно как раз экспедиционничный попыток
сделать, и это не будет полиномиальный алгоритм. Вот, поэтому, чтобы, да, чтобы это каким-то образом
получалось, значит, нужно, чтобы вот не совсем такой маленький зазор был, чтобы он был такой
существенный. Вот, ну, и тут, значит, самый главный класс вероятностный это класс BPP.
Класс BPP, значит, это класс с двусторонней ошибкой. Значит, буквы здесь означают,
вот PPP это probabilistic polynomial, а B означает bounded error. То есть ошибка еще не только односторонняя,
но и ограниченная. Вот, ну да, и ограниченная она кем-то, что меньше 1 и 2. Смотрите, если можно
ошибаться ровно на 1 и 2, то можно просто монетку подкидывать на это, тогда с ровно 1 и 2 угадать,
с ровно 1 и 2 ошибетесь. Вот, поэтому, чтобы какой-то смысл был в алгоритме, нужно, чтобы ошибка
была меньше, чем 1 и 2. Вот, но вот bounded error означает, что они просто меньше, а существенные меньше.
Вот, ну, насколько им на самом деле не важно, значит, ну, классически говорят, соответственно,
что если x лежит в A, значит, я перепишу, то вероятность того, что v от xr равно 1,
эта вероятность будет больше, чем 2 трети, а если x метровим zeta, то тогда та же самая
вероятность будет меньше, чем 1 третий. Вот, соответственно, вероятность ошибки ограничена 1 третью.
Вот, ну, через некоторое время мы увидим, что вот это вот 1 треть и 2 третьи, это корея, да, не
традиция, а реально тут не важно, что писать, но чтобы был разрыв главный, да, то есть можно
писать, скажем, здесь 49% и здесь 51%, можно наоборот, здесь 1% и здесь 99%. Можно даже, чтобы это
были не симметричные, да, например, можно, чтобы здесь было, скажем, там 5% и здесь 20%,
да. Может быть даже, что это не константы, но соседние константы не могут быть, а то мы вот
вот сюда вот выродимся. Вот, но, например, может быть, разница будет какой-то
убывающей функцией, но не слишком быстро. Убывающей, да, например, тут может быть
там 1 вторая минус 1n, а здесь 1 вторая плюс 1n. Да, такое тоже подойдет. Вот, ну
значит, сейчас попозже поговорим про то, почему так можно делать. Так, значит,
еще есть классы РП и КОРП. Классы РП и КОРП. Это классы с односторонней
ошибкой. Ну, а здесь буквка Р означает рандомайст. Да, то есть вот так сложилось,
что пробабилистик вот тут вот, тут пробабилистик, а тут рандомайст, хотя это вроде
синонимы, но примедительно классом получается разное определение. Так, значит, ну и как
запомнить, какой стороны односторонние, потому что определение должно быть похоже
у РП на НП, а у КОРП на КОНП. Соответственно, у РП получается так, что если Х лежит в А,
то тогда вероятность того, что В от XR равно единице, будет больше, чем 1 вторая. Вот, а если Х не
лежит в А, то тогда вероятность равна НУДУ. Ну, а КОРП наоборот. Значит, тут равно единице,
а тут меньше, чем 1 вторая. Ну и реально на самом деле то, что мы изучали в прошлый раз,
то есть алгоритм проверки простоты и алгоритм проверки значения на равенство, они были
синонимой ошибкой. А что было в наших тестах простоты? Что если число простое, то тогда алгоритм
точно говорит, что оно простое. Если число составное, то алгоритм маловероятно, что говорит,
что оно простое. Но это вот ровно КОРП. Если это для простоты, это если тест простоты, то алгоритм
точно говорит, что оно простое. Если это число составное, то он маловероятно ошибает. Но это вот
Миллер Рабина и Соловая Штрассена, бывает другой тест Эйделмана Хуана, который у которого все на
оборот. То есть если число составное, то он точно говорит, что оно составное, а если оно простое, то
скорее всего говорит, что простое. Поэтому еще до того, как оказалось, что простые числа вообще
детерминированно решаются за полинальное время, уже были были известны вот такой алгоритм и вот такой
алгоритм. Ну еще есть класс с неограниченной ошибкой или наброс с нулевой ошибкой. ПП это
просто пробабилистик-полиномиал. Значит, это с неограниченной ошибкой. Вот, слушайте, я уже не буду
переписывать. Ну уж только чтобы нельзя было просто одну-вторую сделать. Ну, например, сверху у нас
будет больше ли равно одной-второй, а снизу меньше одной-второй. А так эти все выражения тут
одни и те же. Так что я их не переписываю. Значит, тут все-таки если одно и то же число не может быть,
и там и там, то все-таки нельзя тривиально это решить. Вот, и еще есть наоборот, значит, ZPP.
А Z это zero error. Значит, ZPP это с нулевой ошибкой. Ну, и это на самом деле как раз алгоритм
Лас-Вегаса. Значит, это алгоритм Лас-Вегаса. Значит, тут ответ всегда верный, но при этом
полиномиальное время только в среднем. То есть мот ожидания. Значит, ответ всегда верный,
но полиномиальное время только в среднем. Полиномиальное время только в среднем.
Ну, потому что если и ответ всегда верный, и полиномиальное время в худшем случае,
то это просто P будет, потому что можно что угодно дописать в качестве случайных битов. После этого
будет правильный ответ за полиномиальное время. Значит, если он в среднем полиномиальный,
то тогда будет правильный ответ, но не факт, что это за полиномиальное время. Поэтому то,
что ZPP больше, чем P, это не факт. Вот это основные классы, которые мы сейчас изучим. Давайте я нарисую
диаграмму, как они друг с другом соотносятся. Ну, можно, конечно, и P добавить. P будет то же
самое, только со временем в худшем случае. Вот так. Значит, диаграмм будет такой, что сам внизу будет P.
Потом будет ZPP. Значит, P вложено в ZPP, конечно, потому что можно случайно бит игнорировать,
и просто вычислить. Значит, ZPP будет вложено в RP и в QRP. И, на самом деле, верно даже больше.
Да, не просто ZPP вложено туда и туда, а на самом деле верно, что ZPP это будет пересечение RP в
QRP. Так, значит, дальше ERP и QRP будут вложены в BPP. Кроме того, RP вложено в NP, QRP вложено в QNP.
И, конечно, на самом верху они все вложены в PPP. Вот. Ну, вот это вот все известные соотношения.
То есть, например, неизвестно, как, скажем, RP и QRP соотносятся, как NP и BPP соотносятся.
Вот. Ну, а теперь давайте обсудим, обсудим, почему все эти верны соотношения. Так, ну,
тут есть парочка очевидных. Значит, очевидно, давайте я буду жирным обводить те, которые мы
доказали. Значит, вот это вот очевидно, да, что PP вложено в ZPP. Ой, в смысле просто P вложено в ZPP,
потому что можно вообще не использовать случайные биты, да, и будет правильный ответ. И тут даже в
худшем случае за параллельное время, значит, и в среднем тоже. Вот. И что очевидно вот это вот?
Просто потому что там усиление не равен, да, то есть, если у нас получилось больше двух третей,
то это больше одной второй, да, а если меньше одной третьей, то меньше одной второй. Так. Ну,
пожалуйста, все остальные уже не очень очевидны, да, какие-то рассуждения все-таки нужно проводить.
Но некоторые, а, не, все-таки вот это вот, вот RP в NP и QRP в QNP тоже очевидно, по той же причине. Да,
потому что больше одной второй значит больше нуля. Вот если мы NP так определили, да, то получается,
если тут больше одной второй, да, то тут получается больше нуля. Ну, и QNP, QRP также. Так. Ну,
также видна некоторая симметрия, да, то есть, что ясно, что вот это так же, как вот это должно быть,
а вот это так же, как вот это. То есть, фактически три вещи остались. Ну, и еще более сильное утверждение
вот это вот. Так. Ну, давайте. А, тут тоже нас симметрия, да, в общем, три вещи симметричные.
Так, давайте, например, взгляд PP вложено в RP. Ну, вот тут можно сказать так. Значит, там есть такое
вероятностное неравенство, называется неравенство Маркова. Это вы слышали такое? Так, ну, да-да-да,
что если у вас, если у вас положительная случайная величина, и у нее есть какое-то
мотожидание, да, то тогда вероятность того, что она там сильно больше мотожидания, маленькая,
а именно там меньше, чем один, делите на то, во сколько раз превышаем. Вот. Соответственно,
вот, как мы это здесь применяем. Так, начну. Пусть, пусть вот это вот V от XR работает в среднем
за какой-то, начнем P от M. Тогда с вероятностью больше, чем одна вторая, значит, средства больше,
чем одна вторая, он работает меньше либо равно за не более, чем два P от M. Ну, потому что даже
ноль шагов не может быть, да, надо договориться, что начальное и конечное состояние разные вещи,
да, так что хотя бы один шаг он делает. Ну, и тогда более-менее понятно, да, даже без неравенства
Маркова, да, что если таких будет больше половина, да, то уже из таких, где больше вот столько будет
хотя бы половина, то уже средний будет больше, чем P от M. Так, я понятные вещи говорю? Ну, то есть
у нас на половине у нас будет хотя бы столько, на другой половине хотя бы один, на среднем
должно быть больше, чем P от M. Вот, хорошо, ну и тогда, тогда, значит, алгоритм с односторонней
ошибкой, значит, алгоритм с односторонней ошибкой просто запустить вот на два P от M, да, значит,
если ответ есть, то его и выдать. Значит, запустить, наверное, V от XR на два P от N шагов,
N иметь в виду длина X, как обычно. Вот, соответственно, если остановился, то выдать результат,
если остановился, то вернуть результат. Если не остановился, ну, тогда, на самом деле, вопрос,
то, что выдать по умолчанию для RP и для CoRP, смотрите, если он не остановился, то мы не знаем
результатов, но в RP написано, что если к нему лежит VAT, то мы не можем выдать 1. Ну, соответственно,
если мы не знаем результатов, то надо 0 выдавать. Вот, ну а для CoRP надо 1 выдавать. Если не остановился,
то нужно выдать 0. Это для RP или 1 для CoRP? Средним по R, да, конечно. Да, это правильный вопрос,
но для каждого X средним по R полиномиальное время. Ну вот, тогда получается, что это алгоритм
полиномиальный, да, вот он 2P от N работает, плюс еще немножко. И, действительно, тут, значит,
условия выполняются, да, значит, для RP посмотрим. Если X не лежит в A, то тогда либо алгоритм
остановился и выдал верный ответ. Так, тут верно-то вернем. Значит, он выдал верный ответ,
и мы его вернем. А если он не остановился, то выдаст 0, и это тоже будет верный ответ.
Значит, если X не лежит в A, то все нормально. Если X лежит в A, то тогда, конечно, когда мы
выдадим 0, мы ошибемся, но это произойдет не больше 1 и 2, ровно вот из-за вот этого
свойства, что все-таки это остановится. В принципе, больше половины, и в этом случае выдават, опять же,
верный ответ. Вот. Так, ну что, понятно? Ну, то есть, суть такая, да, что если у нас в среднем
время полиномиальное, да, то есть мы достаточно много проработаем, то мы, скорее всего, остановимся,
и предположим правильный ответ. А если не остановится, не знаю, ничего выдавать,
ну выдадим, чтобы не было той ошибки, которую мы хотим избежать. Так, два, значит, РП вложено
в ВПП, но это даже проще, может быть, с этого стоило начать. Значит, РП вложено в ВПП, и тут вот идея
амплификации возникает. Идея амплификации.
Значит, мы просто запустим два раза. Значит, запустим два раза.
Собственно, если бы у нас там определение РП, а вместо 1-2 стояло бы 1 на треть, то вообще ничего
не надо было бы делать, да, можно было бы просто сослаться на то, что там, то есть сейчас, наоборот,
это в кое РП вместо 1-2 стояло бы 1 на треть, а в РП вместо 1-2 стояло бы 2 на треть. Вот.
Если бы это было так, то можно было просто сослаться, что там равно нулю, значит, меньше 1-3, и все работает.
Но если там 1-2, то все-таки нужно уже рассуждать о том, как от одной границы к другой переходить.
Вот. Ну и это вот совсем простой случай. Значит, запустим два раза.
Запустим два раза. Возьмем дизъюнцию.
Возьмем дизъюнцию
результатов.
Вот. Значит, что тут работает? Ну, значит, если х лежит 2, то тогда оба раза 0 будут
варианты чуть меньше, чем, если запуска независима, да, это, кстати, важно. Значит, два раза. Давайте пишу независимо.
Значит, если х лежит 2, тогда вероятность того, что оба раза 0,
вероятность нуля на обоих запусках,
значит, будет меньше, чем 1-2 на 1-2, которое есть 1 четверть.
Ну, соответственно, раз 3 единицы будет больше, чем 3 четверти.
Раз 1 единицы будет больше, чем 3 четверти, а это будет больше, чем 2 третья.
Значит, поэтому там все верно получается, в соответствии с определением. Вот. Ну, если х не лежит в А,
то тогда обязательно будет 0 на обоих запусках, и итоговый тоже. Итоговый дизъюнции точно 0.
Значит, если х не принадлежит, то получается оба раза 0, и тогда получается ответ 0.
Вот. Что такое это рассуждение?
И, в принципе, если его продолжать, то видно, что афер вместо 1-2 можно любую константу поставить.
Ну, то есть, например, ну, если мы это будем делать там n раз, то у нас тут место больше 3 четверти,
то получится больше, чем там 1 минус 1 вторая военная степень.
Значит, если повторить n раз, то тогда тут получится, значит, 0 останется 0,
потому что если единица не может быть выдана, всегда выдается 0, то дизъюнция тоже будет 0.
Вот. А тут будет, так, я тут уже не пишу, но у меня все время одни и те же вероятности,
если х лежит в А, то какая вероятность будет, если х не лежит в А,
вероятность будет, и тут будет, соответственно, 1 минус 1 вторая военная степень.
Вот. То есть, пресс может считать, что у меня ошибка совсем маленькая.
Так, а, слушайте, давайте я это отцеплю.
Значит, m не будет мином х, а будет каким-то словом повторов m.
То есть, если мы повторяем m раз, то будет 1 минус 1 вторая военная степень.
И, значит, если м это какая-то константа, то это может быть сколько угодно близко к 100%,
а может быть даже m какой-то полином, значит, m какой-то полином,
тогда это будет даже функция стремящейся к единице очень быстро.
Вот. И тогда даже видно, значит, видно, что изначально может быть вместо 1 и 2.
Да, то есть, мне что нужно? Мне нужно, чтобы вот эта вот штука стремилась бы к нулю.
И тогда вместо 1 и 2 может быть что угодно меньше единицы.
И даже может быть что-то стремящейся к единице, но не слишком быстро.
Например, есть второй замечательный предел,
что 1 минус 1m в степени m стремится к 1 делить на е.
Вот. И, соответственно, вот это у меня вероятность единицы
Сейчас, нет, это у меня вероятность нуля.
Вот это 1 минус 1m это вероятность нуля,
может быть, что вероятность единицы там будет вместо 1 и 2 будет больше 1m.
И тогда, если мы это раз повторим, то у нас вот это вот станет константой.
А это у нас константа чего? Это вероятность того, что все время ноль выдают.
Будет константа. А потом, можно, скажем, еще раз повторить.
Еще раз повторить, и это в m в степени возведется.
То есть если тут для m в квадрате написать, то тут будет е в степени m.
Ну вот, то есть вывод такой, что если ошибка односторонняя
и изначально отличается от единиц хотя бы на обратный полином,
то, повторив чуть больший полином раз,
и взяв дизюнцию, можно, наоборот, ошибку сделать специально маленькой.
Так, ну хорошо, сейчас передов сделаем.
Потом поговорим про то, что у меня здесь осталось вот это вот.
И про разные другие вещи.
Значит, у нас вот это получилось.
Значит, вот это получилось, вот это получилось,
вот это симметрично точно так же получается.
Остались вот эти вот соотношения.
Значит, mp и pp, и qnp и pp.
Значит, они тоже симметрично делаются.
Да, вообще, еще можно заметить, что смотрите,
тут есть как бы центральная ось,
и вот эти вот боковые ветви и боковые ветви симметричные.
То есть, например, если мы возьмем два раза полкласс,
то будет то же самое, что изначально.
Дополнение ко всем языкам из основного класса.
Но вот если по центру брать полкласс,
то будет просто все то же самое.
Потому что все вот эти языки должны относить к дополнению.
Значит, если мы возьмем дополнение,
то можно просто обратить ответ,
и это будет, поскольку все симметрично,
можно обратить ответ, и это будет
для дополнения алгоритма того же самого вида.
Если они симметричны, то нельзя просто обратить ответ.
С bp нет, почему не очевидно?
Симметричны больше двух третьей, меньше одной третьей.
А, для pp.
Да, это правильно говорите, это не очевидно.
Да, для bpp очевидно,
для pp не очевидно, потому что там точно равно.
На самом деле, если сделать
строго больше, строго меньше, то будет то же самое.
А как именно там делать,
это та же методика,
которая используется здесь.
Давайте я докажу,
что np вложено в pp.
Хорошо, значит, np вложено в pp.
Давайте
другому,
чуть-чуть по-другому напишем,
что у нас, если x лежит в a,
то тогда количество таких r,
то v от xr равно единице,
то это количество больше либо равно единице.
Если x не лежит в a,
то тогда то же самое количество
будет равно нулю.
Ну и идея сделать так, чтобы как раз
вот это изменение с нуля на хотя бы единицу
как раз бы передвигало в область
от меньше половины до хотя бы половины.
То есть теперь можно сделать
вот такую вот функцию v'.
Значит v', например, от x
и тех нулей
будет равняться нулю
v' от x
и 0
и еще там чего-нибудь,
0у будет равняться единице,
если у не из всех нулей,
если у
не только из нулей состоит,
ну а, наконец, если v'
от x и чего-то, что начинает съединиться,
то тогда мы запускаем старую машину
и получаем v от xr.
Вот.
Ну это, смотрите,
значит вот за счет вот этой части
у нас получается
ровно половина единиц, кроме одной.
Если у нас здесь хотя бы одна единица есть,
то у нас уже хотя бы половина единиц будет.
А если здесь одни нули, то половины не будет.
То есть получается, что
вероятность того, что
v' от xr,
нет, v от xr,
значит, вот это вот
больше нуля строго,
это тогда и только тогда,
когда вероятность того, что v'
от xz
равно единице,
будет больше либо равно, чем одна вторая.
Потому что ровно как бы
все кроме одной,
мы сделали, если еще одна добавится,
то будет уже хотя бы одна вторая.
Вот.
Ну вот,
поэтому NP вложена в PP.
Ну что, согласны?
Вот.
То есть, мне говорят, такие пороговые,
значит, пороговые классы, да,
и если мы
где-то научились порог делать,
то можно его и где-то еще провести.
Но это не значит, что можно обратно изменить,
потому что у нас все-таки
все-таки в NP у нас с одной стороны нуль,
поэтому нельзя так просто взять и обратно изменить.
Но в любом другом месте,
в любой другой константе границу точно
можно провести, будет то же самое.
Ну и действительно можно
сделать симметричное определение,
симметричное определение PP,
это когда будет строго больше одной из второй
или строго меньше одной из второй,
а ровно одной второй нигде не будет.
Ну давайте плановой картиной я расскажу,
как это делается.
Ну, идея точно такая же, да,
идея как-то так сдвинуть пороги,
чтобы оно вот в одну-вторую точно не попало,
чтобы если было меньше одной-второй,
осталось меньше, а если было хотя бы
одной-второй, то это станет строго больше.
Так, ну хорошо,
значит теперь давайте
тоже через количество я запишу.
Значит, количество таких y,
то v от x и y равно единице,
то v от x и y равно единице,
v от x и y равно единице,
значит, количество будет больше либо равно
чем 2 в степени
m-1, да, значит,
где m это длина y
и вот это вот всё
при х лежащем ва.
И соответственно, то же самое количество
будет меньше строго, чем 2 в степени
в степени m-1, если
не лежит ва.
Так, ну и теперь можно
построить новую
функцию,
там w,
вот их.
Так,
значит, и мы это сделаем
значит, таким образом.
Значит, ну, например, мы сделаем так, значит,
если все нули,
то мы выдадим единицу,
значит, если тут 0, 0,
а потом какой-то z не равно 0,
то это будет 0, да,
если z не равно 0,
w от
ну, в смысле, не из всех нулей.
Значит, дальше, например, w от
х, не, например, 1, 1 z,
это будет единиц, уже для всех z,
не только для не нулевых.
Ну, а дальше, значит, w от
х, скажем, 0, 1 у,
это будет w от
х на 1, 0 у,
а,
значит, это будет
равно w от х у.
Так, значит,
почему это сработает? Вроде должно сработать,
давайте проверим.
Значит, ну, во-первых, у двое или количество
вот здесь вот, значит,
если х между 2.
Значит, вот теперь
у нас будет
1, 0,
значит, вот теперь
количество
количества таких r
w от
х равно единице,
это будет следующее, значит,
это будет 2.
Так, ай, давайте это как-нибудь обозначим,
вот, то число, которое было, нам кое-нибудь k.
Да, вот это вот,
k. Значит, оно будет
а, оно будет, вот эта форма будет всегда
верна, значит, это будет 2k,
2k плюс
2 в степени m
плюс 1.
Где m старая? Значит, m старая, ну,
сперва у меня длина на 2 бита больше.
Так, значит, это всегда верно,
и это будет больше либо равно,
значит, больше либо равно,
чем 2 на 2 в степени m минус 1
плюс 2 в степени m плюс 1.
То есть, это будет
2 в степени m плюс 1
плюс 1,
но у нас длина теперь m плюс 2,
то есть, это как раз будет больше,
больше, чем 2 в степени
m плюс 2 минус 1.
Да, теперь у меня вместо m
плюс 2, но вместо больше либо равно стало больше.
Да, поэтому мы от 1 и 2
отодвинулись в плюс.
Вот, ну, а если х не лежит 2,
так, дайте я тут завершу,
не лежит 2, то тогда то же самое,
значит, 2k плюс 2 в степени m
плюс 1.
Так, а теперь,
смотрите, раз это меньше, чем 2 в степени m минус 1,
то оно получается меньше либо равно,
чем 2 в степени m минус 1 минус 1.
Потому что это же все целые числа,
мы, наверное, перешли к числам в целом,
то же самое минус 1.
Так, и это у меня будет меньше либо равно,
чем 2 умножить на 2 в степени m минус 1
минус 1
плюс 2 в степени m плюс 1.
Вот, и это будет равняться
2 в степени
m плюс 1 минус 1, да,
потому что минус 2 вот здесь, это плюс 1.
Ну вот, значит, это получается меньше,
чем 2 в степени m плюс 2 минус 1.
А, все.
Значит, меньше, чем вот столько.
Ну вот, получается, что там было больше,
а тут будет меньше.
Значит, первая варианта получается, что либо
трога больше одной в трое, либо трога меньше одной в трое,
и там уже можно переходить к дополнению,
так, ничего.
Понятно?
Есть какие-нибудь вопросы?
Так.
Нет, оно симметрия, что если вы замените
0 на 1 и 1 на 0,
то тогда, ну, то есть, если вы для
дополнения используете,
то вы можете просто заменить
0 на 1 и 1 на 0, эта варианта сменяется местами.
Ну да, но в исходном определении
там была, да,
одна была меньше, чем одна в второй,
а другая меньше одной в второй.
Если поменяться, то они будут наоборот.
А в новом определении обе ошибки меньше одной в второй,
если их менять местами, они тоже будут обе меньше одной в второй.
Ну да, если менять местами ответы машины
и считать, что у нас теперь это
машина для дополнения, а не для исходного множества.
Нет, исходная не симметрична, потому что она была
больше либо равной, и строго меньше.
Не, смотрите, вот дело в том, что ошибка,
это не то, что я здесь пишу, да, ошибка это,
вот внизу эта ошибка, а вверху это один
вот, соответственно, хотя тут они
больше одной в второй, но ошибка тоже будет меньше одной в второй.
Не, не, не, сейчас, смотрите, вот в верхней ситуации
у меня то, что равно единице, это правильный ответ.
Поэтому если у меня правильность больше одной в второй,
то ошибка становится меньше одной в второй.
Это в исходном плене меньше либо равно,
а я как раз про новое говорю.
Вот, а здесь наоборот, здесь равно единице, это неверный ответ,
поэтому то, что здесь написано, это и есть ошибка.
Вот, и поэтому тут ошибки одинаковые выглядят,
поэтому их можно менять местами.
А раньше было больше либо равно, поэтому их нельзя было менять местами,
да, значит, получилось наоборот.
Вот.
Так, ну все, значит, в этом, в этой диаграмме мы все доказали,
так что я это стираю.
Вот, и поговорю про другие вещи.
Такие более глобальные, что ли.
Такие более глобальные, что ли.
Такие более глобальные, что ли.
Такие более глобальные, что ли.
Такие более глобальные, что ли.
Так, значит, вот мы подробно обсудили амплификацию
для односторонней ошибки.
Для двухсторонней она тоже есть.
Так, ну вот.
Так, ну вот.
Так, ну вот.
Так, ну вот.
Так, ну вот.
Так.
Давайте ее обсудим.
Амплификация
для двухсторонней ошибки.
Значит, смотрите,
можно рассмотреть в некоторых общих случаях
ДПП, сынок, семиальфа и бета.
Здесь, значит, если х лежит в А,
то тогда, значит, вероятность того,
что В от хр
равно единице будет больше бета,
значит, если х не лежит в А,
то тогда вероятность того, что В от хр
равно единице будет меньше А.
Вот. Вот такой вот класс можно
рассмотреть.
Ну и, в принципе, все, что мы изучали,
ну, кроме излета ПП,
вот под это подходит.
Ну, там,
ну, то есть ПП, значит,
с учетом симметричного определения,
ПП получается, когда а и беты равны по 1 и 2,
когда там кто-то равен нулю,
получается односторонние классы и так далее.
Вот. Но вот на самом деле
общий тирем тут такая.
Теорема. Так, значит, если
1 делить
на 2 в степени полином,
значит, меньше а,
так, значит, меньше бета,
меньше, чем 1, минус 1 делить
на 2 в степени полином,
и при этом бета, минус а,
больше, чем 1 делить на просто полином,
значит, то
вот это вот БПП
а и бета равняются БПП.
Вот. Ну, это вот такая
самая общая тирем, которую здесь можно
спормулировать.
В общем, уж как минимум, если а и бета
это две константы, отличные друг от друга и от нуля,
то тогда это все будет то же самое.
Это вот такая тирема у независимости от порогов.
Ну, а вот то, что это прям функция,
изменяющаяся, это максимальная общность,
которой это можно доказать.
Ну, в принципе, это не очень важно.
Важно.
Вообще, наверное, на самом деле важно,
что это стремится к единице
и стремится к нулю. Это сейчас будет важно.
То есть, что именно вот так экспоненциально стремится.
То есть, понимать тирем можно так,
что можно считать,
что если ошибка была отделена от одной и второй,
то можно считать, что она вообще экспоненциально маленькая.
И что нам важно,
значит, важно, что она даже может быть меньше,
чем один делить на число
вообще возможных х.
То есть, число возможных х это какая-то фиксированная.
Ну, это, в общем, два в степени N,
а тут два в степени N,
которые может быть больше, чем N.
Поэтому можно считать,
что ошибка прям совсем маленькая.
Так, значит,
трогу я доказывать не буду.
Но идея, конечно, та же самая,
что много раз запустим.
Но смотрите, вот здесь у меня была хорошая функция
дезъюнция результатов,
что если там была хотя бы одна единица,
то это единица.
И тут я пользовался, что если на самом деле ноль, то единица быть не может.
Соответственно, такую простую функцию
с двусторонней ошибки я не могу взять,
то у меня могут быть и нули, и единицы,
я не могу брать дезъюнцию.
Но что вместо этого можно делать?
Ну, вот есть такой закон больших чисел
и связанный с ним центрально пределенный теорема.
Мы это вроде немножко обсуждали в прошлый раз.
И так, если по-простому,
то о чем эти теоремы говорят?
Закон больших чисел говорит, что если вы проводите очень много испытаний,
то та доля успехов, которые у вас возникает
в испытаниях, она приближается
к настоящей вероятности.
То есть как бы вероятность это предел частоты.
Вот про это более-менее закон больших чисел говорит.
Что если у вас реально много испытаний,
почему он больших чисел,
если у вас прям много испытаний,
то частота будет примерно такая вероятность.
Это говорит, насколько быстро приближается
к частоте.
То есть почти наоборот,
насколько быстрая частота приближается к вероятности.
Ну и, в общем, если отбросить конкретные формулы,
то что там и на колокол получается и так далее,
то суть такая,
что если вы проводите N испытаний,
то у вас отклонение частоты от вероятности в среднем
будет не из N.
Да, сейчас я прям запишу,
что такое идея центрально-предельной теоремы.
Ну там точно формулировка,
что там распределение вот этого среднего
стремится к нормальному, там с какой-то формулой.
Вот это у вас будет на сервере.
А идея следующая, значит, что
если провести N испытаний,
если провести N испытаний,
то частота
попадет в там
один злет на корень из N
окрестность вероятности.
Ну опять же, тут может быть не один на корень из N,
а там три на корень из N или шесть на корень из N,
в общем, смотря какой, так сказать,
доверительный интервал вы хотите получить.
И как это там еще, P значение это называется.
В общем, идея следующая.
Ну а тогда, смотрите,
вы можете проводить,
это вот зачем здесь нужно разницу
в один злет на полином.
Значит, вы можете проводить свои испытания
полинально-чепло-раз.
То есть вот это вот N
может быть даже полиновым от того N,
который был в длину Х.
Дальше того испытания вы можете провести.
И тогда вы можете
найти частоту, то есть сколько раз у вас
возникла единица.
И вот эта частота, она будет вот на таком
расстоянии от
настоящей вероятности.
Но дальше можно посмотреть, вот эта частота,
она попала в районе бета
или в районе альфа.
И тут реально можно будет отличить.
Потому что, смотрите,
вот есть альфа,
вот есть бета.
Значит, вот есть
средние между ними.
Значит, альфа плюс бета пополам.
Так, надо бы мне ниже очень нарисовать.
График ниже нарисовано.
И, соответственно, если у вас будет
настоящая вероятность альфа или меньше,
то у вас будет
распределение вот какой-то вот такой вот колокол,
где ширина будет порядка 1.9
на корень N.
Если настоящая бета или больше,
то у вас колокол будет где-то вот здесь.
Так, слушайте, тут у меня уже,
дайте, это N большое.
И тут 1.9 на корень N большого.
Значит, а вот это вот у вас 1.9 на полином.
Дайте, я напишу 1.9 на P от N.
Вот, ну отсюда видно,
значит, отсюда видно, что нужно N
взять порядка, вот это вот P от N
в квадрате.
Значит, если N порядка P от N в квадрате,
то тогда будет сосредоточена,
вот это вот будет сосредоточено
почти наверняка больше, чем середина
отрезка, а вот это вот почти наверняка
меньше, чем середина отрезка.
И, соответственно, идея такая, что вы можете запустить
такое число раз ваш алгоритм
со свежими включениями битами
и сравнить вот со средиум-архиметическим.
Получается алгоритм, значит,
запустить,
запустить V от XR
для N разных
независимых R
и сравнить
сравнить, соответственно,
с вот A и B половины.
Вот, ну, на самом деле тут все-таки идеи,
идеи тут немножко недостаточно,
нужна и инсомация ПТ, а точнее даже
как бы ее конечные варианты некоторые,
потому что все-таки П – это предельная теорема,
а нужно, что если мы вот столько испытаний сделаем,
то у нас уже будет достаточно.
Там есть разные, это называется неравенство больших уклонений,
там есть неравенство Чернова, еще там есть хевдинга,
в общем, разные есть варианты.
Но суть такая, что у нас
если достаточно много раз провести испытания,
то вероятность того, что мы окажемся
с другой стороны, вот с середины,
будет экспоненциально убывать.
Вот.
Ну что, есть ли какие-нибудь вопросы по этому рассуждению?
Да, надо понимать, что это все-таки не полное доказательство,
для полного доказательства нужны некоторые точные неравенства,
но в них я не хотел бы углубляться,
а на идейном уровне этого всего достаточно.
Вот, значит, в чем здесь
выгода, значит, выгода
от вот такого уменьшения ошибки,
выгода выступает тогда, как раз когда
вероятность ошибок получается меньше, чем
один делительное число,
число иксов.
Можно сделать
можно сделать вероятность ошибки
меньше
один делитель на два в титане N.
А что это означает?
Ну, смотрите, вот у вас есть большое пространство
всех R.
И дальше вот есть в нем, да,
это, конечно, можно сделать, только если длина R
больше, чем N.
Чтобы это было больше нуля,
ну, чтобы как бы мелкость была
еще сильнее.
Конечно, тут нужно,
чтобы длина R была больше N.
Но тут
никто не запрещает,
может, длина R больше N.
Вот, и та, смотрите, что получается.
Вот у нас есть большое пространство всех возможных R.
В нем есть маленький участок
всех R, которые не подходят для первого икса.
Есть какой-то другой маленький участок, который не подходит
для второго икса.
Может быть, какой-то R туда и туда не подходит,
а может быть, это разные участки. Есть какой-то участок,
но даже все вместе,
если они все разные и не пересекаются
для разных иксов, все равно они не покроют
пространство
всех возможных R.
Потому что как бы каждый икс высекает меньше,
чем 1 лидия на 2 степени N долю,
ну, высекается на среднем и не подходит,
дают неправильный ответ для этого икса.
Поэтому
всех иксов не хватит, чтобы все R как бы испортить
и останется R хорошее.
То есть получается
так, что если мы такое сделали,
то есть тогда
получается существует R
такое, что для любого икс
V от XR
V от XR
равно единице,
тогда и только тогда, когда
икс длинжита.
Ну, икс не совсем для любого икса,
а для любого икса, у которого длина равна N.
Значит, для любого икса, у которого длина равна N,
вот эта R подходит.
Так, понятен вывод?
Ну, вот из этого вывода
утекает такая чарема, называется чарема A долмана.
Чарема A долмана заключается в том, что BPP
вложено в P слэш поле.
И идея такая, что вот эта R,
которая существует,
мы как бы просто запаяем
схему и дальше будем
ее вычислять.
То есть идея доказательства,
что превратим
превратим VLTXR в схему.
А мы уже обсуждали, что это можно делать,
когда мы догадываем, что P вложено в P слэш поле.
И как бы запаяем туда
и запаяем
и запаяем
запаяем
в нее
универсальная R.
Универсальная, то есть как раз вот эта,
которая существует.
Вот. Получится тем,
полинаминального размера.
Но вот если вдруг это R еще можно
вычислять за полинаминальное время,
тогда вообще P равно DTP.
Если такое R можно вычислить
за полинаминальное время,
то тогда
P равно BPP.
Вот так. Ну у меня остается три минуты.
Давайте в эти три минуты я немножко позову про статус
вот этой гипотезы вообще.
Равны ли P и BPP?
Так. Но статус такой, смотрите.
Значит, если опять же кратенько,
рархей, если P равно NP,
то тогда P равно и BPP тоже.
Но это не тривиальная теорема, она довольно сложная.
Там нужно сначала изучить такую полинаминальная иерархия,
потом доказать,
что если P равно NP, то вся эта полинаминальная иерархия
равна тоже P.
И еще доказать, что BPP в эту полинаминальную иерархию входит.
Тогда, соответственно, это все слопнется,
и P равняется BPP тоже будет.
Вот. Это первое, что известно.
Второе, что известно, что если
условно говоря,
вот сейчас я совсем неформально пишу,
значит, если NP сильно больше P,
то тогда тоже P равно BPP.
Значит, и вот этого теорема
называется Hardness versus Randomness.
Hardness versus Randomness.
Ну, это все, что известно.
Ну, это все, что известно.
Ну, это все, что известно.
Ну, это все, что известно.
Hardness versus Randomness.
Ну, насколько это там вот есть
разная вариация.
Ну, значит, точная формулировка там кажется,
что в NP есть задачи,
которые не распознаются схемами
субэкспоненциального размера.
То есть не просто там нужны
специальное время, а даже схемы нужны
специального размера,
чтобы распознать NP. Вот это вот
сильно больше. Ну, а суть там такая.
Суть доказательств такая, что если NP
настолько больше, то есть некоторая
сложная функция в NP, а на ее базе
можно построить хороший
китил. А дальше можно в алгоритмы с BPP
вместо настоящего случайного китила
подставить вот эти вот плету случайной
число из генератора. И вот это вот получится
алгоритм с P.
Вот, это называется дарандомизация.
Дарандомизация, когда мы заменяем
вариационные алгоритмы на детерминированные.
Таким образом, для того, чтобы P не равнялась с BPP,
то есть с этими двумя крайними возможностями.
Но пока, исходя из вот этой
второй теории, мы большинство следует верить,
что на самом деле P равно BPP, а то, что мы там
не умеем решать равенство полиномов, но это так
получилось. Там даже есть некоторые объяснения,
почему именно задача сложная, но в этом
сейчас совсем не буду задаваться.
Ну вот,
в общем, вот тут мы довольно близко выходим, так сказать,
к переднему краю.
Хотя это уже почти 30 лет назад сделано,
но очень сильно дальше именно в этом направлении
с тех пор не продвинулись.
Ну, кроме, наверное, той статьи про как раз
равенство полиномов.
Ну, я думаю, на этом всё.
Спасибо за внимание.
Спасибо.

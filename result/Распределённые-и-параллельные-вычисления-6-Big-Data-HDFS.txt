Мы с вами начинаем модуль, посвященный обработке больших данных.
Вести его буду я, четыре лекции.
Сначала мы поговорим про охранение больших данных,
и потом еще три лекции про вычисление.
После чего будет одна приглашенная лекция.
Там вам расскажут про работу со Spark.
Сначала начнем с того, что вообще такое большие данные.
Большие данные – это какая-то отметка, после которой мы вдруг начали называть данные большими.
А это данные, которые характеризуются тремя вот такими свойствами.
Это volume, это, конечно же, объем, но он характеризуется не тем,
что данных больше, чем пять терабайт, например,
а тем, что мы просто не можем обработать эти данные на одном компьютере.
У нас не хватает оперативки, либо працов, либо дисков, либо всего вместе.
Второе свойство – это variety, то есть данные поступают из разных источников,
с разной периодичностью, с разной структурой,
и все это нужно как-то приводить к общему знаменателю.
И третий – это velocity, то есть способ, каким мы обрабатываем большие данные.
Либо это batch обработка, то есть с какой-то периодичностью мы
процессим то, что у нас накопилось, получаем какой-то отчет,
либо это обработка данных в реальном времени.
Ну и по поводу хранения данных.
Если у нас с вами не хватает одного сервера,
то можно отмасштабироваться вертикально, то есть купить еще один сервер побольше,
либо горизонтально и купить несколько серверов.
Ну и понятно, что первый способ он рано или поздно себя изживет,
у нас когда-нибудь этот огромный сервер ему все-таки перестанет хватать
и придется покупать несколько.
Вот, если несколько, то суммарная мощность увеличивается,
но надо как-то распределять задачи между этими серверами.
Надо как-то контролировать то, что у нас иногда сервера выходит из строя
и чтобы система все равно работала.
Ну и вообще нам нужна какая-то файловая система,
которая все вот эти новые проблемы, которые у нас появились,
она от нас это скроет.
И хочется, чтобы мы работали с вот этой системой
так же, как если бы мы работали у себя на компьютере.
В принципе, это и есть распределенная система,
когда с точки зрения пользователя это просто одна машинка,
а под капотом работают несколько серверов с разными ролями
и выполняют разные задачи.
Такая система есть, она называется Google File System.
Понятно, что первая компания, которая с большими данными столкнулась в реальной жизни,
это Google.
И вот еще в 2003 году, получается больше 15 лет,
вышла статья, которая называется Google File System.
Эта файловая система была закрыта,
но по этой статье и по некоторым другим статьям
появилась своя файловая система уже с открытым исходным кодом,
который называется Hadoop Distributed File System.
Hadoop – это фреймворк для вычисления поверх больших данных,
поверх этой системы.
И мы его тоже будем разбирать на следующих лекциях.
Вот как у нас устроена GFS и как вслед за ней устроена HDFS.
У нас есть три основных роли в этой системе.
Первая роль – это клиент.
На клиенте мы запускаем программы,
используем файловую систему,
но никаких содержательных данных мы тут не храним.
Это как такая веб-морда,
но на самом деле это не веб, а это сервер.
Дальше у нас есть еще одна роль – это датонода,
то есть узел данных.
Здесь хранятся данные,
они разбиваются на блоки большого размера,
например, 64 мегабайт, 128,
но это все настраивается.
Кстати, кто помнит,
какой размер блока в обычной файловой системе?
Можете посмотреть размерчик?
Размер блока.
У нас файловая система не только HDFS,
а любая файловая система,
она имеет, не то что любая,
но самая популярная,
они имеют блокную структуру
и там тоже есть размер блока.
32?
Нет, 32.
32.
32.
32.
Нет.
Ну, бывает и 32, конечно,
но на деле меньше, конечно.
Значит, либо 8, либо 16.
4 килобайта.
Это легко проверить.
Берете флешку, форматируете ее,
например, под MTFS
или под FAT32, не важно,
FAT32 – это уже совсем старье.
Закачиваете туда маленький файл,
например, TXT,
в котором хранится один символ.
И если посмотрите на свойства флешки,
сколько места там занято,
вы увидите 4 килобайта.
Так.
Это что касается датонода,
то есть устроено в принципе так же,
только блоки большие.
Есть еще name-нода.
Это узел имен, и он хранит имена,
он хранит пути к этим данным.
То есть сервера разные
и может быть такое,
что файл разбит на блоке,
блоки хранятся здесь, какие-то здесь,
какие-то где-то еще в другом месте.
И нужен какой-то единый мастер,
который знает о том,
где что хранится, и к нему мы приходим,
когда мы хотим прочитать файл.
Это и есть name-нода.
Она хранит такой слепок
файловой системы,
его структура похожа на дерево.
И это дерево все хранится
в оперативной памяти.
У кого есть идеи, почему?
Зачем так сделали разработчики
файловой системы,
если оперативка это
совсем ненадежная вещь,
у нас любой ребут оперативку сбрасывает в ноль.
Для быстроты?
Да, для скорости доступа.
Конечно, как мы увидим на практике,
файловая система HDFS
она вообще не про скорость,
потому что нам надо
на каждую команду идти
по сети, сюда обращаться,
потом сюда обращаться,
но мы это все более подробно разберем.
Пока только скажу, что HDFS
действительно медленная,
потому что там A. сеть и B. джава.
HDFS написан на джаве
и сам ходу план написан на джаве.
Давайте посмотрим на архитектуру HDFS.
Вот у нас есть
name-нода.
Это нанит вот это дерево файловой системы
и когда мы работаем с файловой системой,
мы постоянно
какие-то файлы добавляем,
какие-то удаляем, какие-то перемещаем.
В общем, это дерево находится в таком динамическом изменении.
Его нужно часто обновлять
и вот это обновление
занимает много ресурсов
процессора. Пока просто держим это
в голове, потом будем думать,
что с этим делать.
Какие проблемы в архитектуре
вот в такой вы видите?
Кто полегло,
ничего не слышно было.
Если проблема
с name-нодой, то все
недоступно. Да, ну и
SPOV, единая точка отказа.
Single Point of Failure.
Что с этим делать?
Ну, первое такое лобовое решение
проблем, это давайте сделаем бэкап,
чтобы у нас была не одна машинка,
а несколько, ну две хотя бы.
Чем это плохо?
Вот эта name-нода, она большая,
то есть там много оперативки,
там много процессоров, она стоит дорого
и нам придется держать еще
одну такую же и плюс
это большая нагрузка на сеть,
потому что нам нужно синхронизироваться
теперь с обеими name-нодами.
То есть у нас
у нас датоноды и клиент стучатся
и в эту, и в какую-нибудь
ее бэкап
тоже.
Не очень хорошо.
Вторая такая
попытка справиться
с SPOV, это secondary name-нода.
Следует сказать, что
в литературе по ходу
это одно из самых таких
запутывающих названий, потому что
secondary name-нода
это вторичная name-нода, и кажется, что
она должна быть заменой первичной,
но на самом деле это не так.
Все, что делает
secondary name-нода, это
вот эта функция,
то есть у нее функция одна, по сути.
Как мы знаем,
хранится слепок файловой системы
в памяти, и постоянно
какие-то происходят изменения
с этим слепком, и чтобы
не заниматься постоянным,
непрерывным обновлением
этого слепка,
оно происходит с каким-то периодом,
и пока у нас обновление
не произошло, все изменения
мы записываем в edit log.
То есть есть слепок файловой системы,
и есть логи того,
что мы с ним хотим сделать,
как мы хотим его обновить.
И вот это вот обновление, как я уже сказал,
оно занимает много ресурсов процессора,
поэтому name-нода отдает его
на outsource.
И вот secondary name-нода, SNN,
она занимается тем, что получает
FSImage, и она его хранит здесь,
правда он устаревший,
потому что логи изменений
она получает с каким-то периодом,
то есть она здесь хранит устаревший
FSImage, получает логи,
обновляет FSImage, возвращает обратно.
Через какое-то время она снова запрашивает
логи, вот query for edit logs
in regular intervals,
то есть с каким-то интервалом, достаточно
небольшим интервалом, она запрашивает
новые логи, мержит
и возвращает на выход.
Это все, что делает secondary name-нода.
Вот добавили мы здесь SNN, но
с единой точкой отказа мы не справились
со всем, мы немного разгрузили, конечно,
но вот
надо тоже понимать, об этом
спрашивают на собеседованиях, что
secondary name-нода,
она не является заменой name-ноды,
она, конечно, хранит
устаревший слепок файловой системы,
если name-нода все,
то с secondary можно
восстановиться руками
хотя бы часть информации, но это не заменой
и не backup.
Вот, ну и теперь еще попытки
решить проблему со споффом,
это HDFS Federation.
То есть у нас...
Сейчас можете повторить, пожалуйста, какие
ну вот
почему
не как backup, то есть что она
делает индивидуально?
Ну то есть name-нода, помимо того,
что она хранит слепок
и хранит вот эти логи,
она должна уметь отвечать на запросы.
Вот, посмотрим еще раз на эту схему.
Мы разберем отдельно,
как происходит чтение в этой HDFS,
как происходит запись,
но главное, что name-нода, она должна
уметь общаться
с остальными серверами в этой системе,
иметь для этого
API специальное, по которому
происходит взаимодействие. На это API мы
тоже посмотрим.
Вот, у secondary name-ноды такого API нет.
То есть она не умеет
быть name-нодой, она умеет
просто заниматься слиянием
слепкофайловой системы
с логами и отдавать обратно.
Все.
Сейчас стало понять? Нет.
Да, спасибо.
Окей.
Идем тогда дальше.
Вот, HDFS Federation
это несколько name-нод.
То есть у нас есть файловая система.
У нее есть там директории
верхнего уровня, условно, там
home, etc, bin и так далее.
User.
За каждую поддиректорию отвечает
своя name-нода. Если у нас
грохнется одна, мы хотя бы не потеряем все.
Второе, это
high availability name-нод.
Можно подумать, что это как
backup name-ноды.
Но на самом деле это не так.
Потому что у нас есть одна активная
name-нода, остальные stand-by
это значит, что они
не подвергаются такой большой
нагрузке со стороны
клиента и датонод.
Они могут хранить вот эти вот слепкие
на диске. Не обязательно для
этого иметь кучу памяти.
Они конечно синхронизируются
с основной name-нодой,
но они не так нагружены и в принципе
не обязаны быть таким же сильными, мощными.
Если основная
name-нода падает,
то одна из вот этих stand-by
подхватывает, как выбирается
одна из этих stand-by.
Это происходит алгоритм консенсуса,
выбор лидера.
То есть как бы
каждый из name-нод голосует
за другие.
И выбирается какая-то одна.
Она временно становится активной,
она выгружает с дисков память
слепок файловой системы.
Если
не хватает памяти,
то в принципе мы можем работать
прямо с диском, правда это будет долго.
Но это что касается name-ноды
и единой точки отказа.
Скажите, есть какие-нибудь сейчас вопросы
по работе name-ноды?
А вот этот алгоритм консенсуса
мы будем подробнее обсуждать?
В разделе ходу
не будем.
Но когда у нас закончится вот этот блок,
который я сказал,
после моих лекций начинается у вас Роман Липовский,
который там будет рассказывать
про консенсус,
про Каптиорему,
в применении уже скорее
не к ходу, а к Кавке.
То есть вопрос,
по какому принципу они голосуют
будет
на лекциях освещаться?
Если кратко,
то в принципе
по крайней мере в первых версиях
ходу голосование происходило
просто рандомно.
То есть кто больше набрал голосов,
тот и становится лидером.
Идем дальше тогда.
Датоноды.
Что такое датоноды?
Это обычные сервера под линуксом.
Нейнод тоже обычный сервер под линуксом.
И на этих серверах есть специальные папки,
в которых хранятся блоки.
Если им нод много,
и блоков тоже много,
при падении одной датоноды
у нас получится,
что часть блоков пропадет.
Это разрешать нельзя,
поэтому у нас есть репликация.
Каждый блок копируется
на несколько датонод.
И если у нас выпадет
одна датонода или несколько,
мы этого даже не заметим.
У нас будут копии блоков,
которые у нас есть.
Помимо этого,
нейнод хранит информацию
о расположении датоноды.
Вот это, кстати, к слову,
о том, почему секондрией нейнода
не может заменить нейнод.
Но секондрией нейнода
вот этого всего она не хранит.
Нейнод хранит данные о топологии сети,
где какая стойка,
где какой дата-центр.
И при записи блоков в HDFS
она старается эти блоки
Кстати, скажите, чем это хорошо
и чем это плохо?
Если какой-то дата-центр
потом произошел,
то у нас все не будет
в одном дата-центре,
хоть там и несколько серверов.
Хорошо, а чем это плохо?
Вот у нас дата-локалити.
Если произошло что-то плохое,
мы хотя бы не потеряем все.
А чем это плохо?
Какие-нибудь идеи есть?
Нагрузка на сеть
далеко пересылать?
Да.
Даже дело не в самой нагрузке на сеть,
а в том, что сеть просто
длинная, медленная,
через много серверов нужно ходить
и это будет долго.
Вот картинка, где мы видим,
как у нас разносятся блоки
по разным нодам.
То есть мы пишем файлик
вот этого,
вот этот и вот этот.
И вот у нас его блоки.
С0 находится здесь и здесь,
как можно дальше.
С1 находится рядом,
С3 находится здесь
и где-то еще.
Почему вот такой разброс
и почему некоторые блоки
все же находятся рядом?
Потому что у HDFS есть еще такой
процесс, который запускается
тоже с некой периодичностью,
это балансер.
И смотрит, если какая-то дата нода
перегружена,
то он блоки с нее перекидывает
на другие дата ноды.
Сообщает, конечно, об этом
в нейм ноде, чтобы она понимала, где лежат блоки.
Помимо этого, если какая-то
дата нода у нас недоступна
и пропала часть реплик,
часть копий блоков,
то балансер докопирует эти блоки
и чтобы коэффициент репликации
был такой, какой он должен быть.
Вот.
То есть так у нас выглядят
ноды.
Вот у нас блок 1,
он хранится здесь, здесь и здесь,
и вот у него реплика 1, реплика 3, реплика 2,
то есть вот так хранятся копии,
они хранятся на разных нодах.
Вот.
Еще я вам сказал, что блоки
одинакового размера,
но на самом деле это не всегда так.
Дело в том, что у нас
есть такая проблема,
и она известная проблема в ходу,
называется проблема маленьких файлов.
Что это значит?
Если мы захотим в HDFS
залить большущий файл,
то HDFS сам разобьет его на блоки,
сделает реплики,
разнесет по нодам
и с этим справится.
То есть для сколь угодно больших данных
HDFS будет работать в порядке.
Но главное, чтоб места хватило.
А что делать, если файлики маленькие?
Вот слепить два
или три файла в один блок HDFS
не может.
Ну просто потому, что мы потеряем
разделители между файлами.
Поэтому что у нас получается?
У нас получается,
если файлик маленький,
то на него все равно создается блок.
Блок меньшего размера.
Но проблема не в том, что блок
меньшего размера, а проблема в том,
что на каждый такой блок у нас хранится
метаинформация вот здесь,
в нейм-ноде.
Вот здесь.
И метаинформация хранится точно такая же.
Вот что блок полноценный,
что он маленький.
Поэтому получается, если у нас
много маленьких блоков,
то нейм-ноду мы загрузим,
а на диске останется много места.
И вот это вот, конечно, плохо.
Почему еще бывают
блоки не всегда одинакового размера?
Вот у нас есть какой-то
большой файл. Мы его разбили
на блоки, но понятно, что
кратно на размер
блока вряд ли мы его разобьем.
Последний блок будет меньше.
И это вот тоже тот случай,
последний блок может быть меньше.
Сейчас я вам даже покажу это на примере.
Так.
Скоро вам придут доступы
на вот этот квантор,
на котором вы будете делать домашки
все после куба.
Так.
Хорошо.
Ну и вот давайте посмотрим
на...
Это у нас юзер-интерфейс нейм-ноды.
И пойдем побродим
по файловой системе.
Найдем какой-нибудь файлик побольше.
Вот, например,
файл, который содержит
12 гигов.
И посмотрим,
какой он будет.
Вот, например,
файл, который содержит
12 гигов.
И посмотрим на его блоки.
Давайте посмотрим, как у нас вообще
хранятся блоки, какого они размера.
Вот первый блок, второй, третий.
Что мы видим? Что меняется?
ID-блока. Оно у каждого свое.
Blockpool — это
такая вот структура.
Она относится
к топологии сети.
То есть, если у нас
много дата-центров,
много стоек, мы об этом сообщаем
в специальном конфиге-нейм-ноде,
то она делает несколько блок-пулов.
В данном случае
на нашем ущельном кластере
фактически стойка одна, дата-центр один
и блок-пул тоже один.
Вот это вот некая константа,
на которую можно просто не обращать внимания.
Дальше, generation-stamp —
это версия.
То есть, мы работаем
с распределенной системой,
где машинки могут отваливаться.
И может быть такое,
что мы начали запись в HDFS,
машинка отвалилась,
потом она поднялась,
но на ней содержится более старая версия
этого же блока.
Как система понимает, что версия более старая,
вот как раз поэтому generation-stamp.
Дальше — размер.
И после размера
список узлов,
на которых хранятся
реплики этого блока.
Вот давайте покликаем по разным блокам,
потому что меняются ноты,
меняется ID, меняется stamp,
а вот размер не меняется нигде.
Опять не поменялся.
Опять не поменялся.
Идем до самого конца.
Вот 90-й блок. Опять не поменялся.
91-й.
Поменялся.
Последний блок чуть-чуть меньше.
Есть еще
один случай,
когда блоки другого размера —
это когда последний блок
совсем маленький,
ну и, как я уже сказал,
нам очень невыгодно хранить маленькие блоки в HDFS,
поэтому он прикрепляется
к предпоследнему, и предпоследний
становится чуть больше.
Совсем маленький — это там несколько процентов
от размера блоков.
Вот, скоро вам придут доступы
такого типа, правда,
будет по-другому называться хвост,
но примерно такие письма вам придут.
Дальше. Теперь поговорим
про чтение и запись. Пока мы к нему
не перешли, по поводу хранения
блоков какой-нибудь вопрос есть?
Где все? Почему-то у нас
аж 9
и 100.
На предыдущей лекции
примерно так и ходило,
может, чуть-чуть побольше.
Ну окей, вы все-таки
старайтесь ходить, тем более
лекции сейчас удаленные,
и, возможно, у нас получится пригласить
очень крутого специалиста
из заграничной компании,
но он будет вести лекцию на русском,
но это прям крутой специалист
в Spark.
Давайте возвращаемся к блокам.
Я так понимаю, что пока вопросов нет.
Тогда идем дальше.
И поговорим про чтение и запись.
Вообще ходу
как бы работает по
парадигме
write once, read many.
То есть пишем редко,
читаем часто. Сейчас мы поймем почему.
Когда мы читаем, вот что происходит.
Мы запрашиваем
у NameNode
какой-нибудь блок, который хотим прочитать.
Точнее, мы хотим прочитать
не блок, а файл обычно.
Мы говорим, мы хотим прочитать такой-то файл.
NameNode говорит,
что он находится там-то, на каких-то блоках.
Вот он отправляет назад блок ID,
блок location.
И дальше клиент уже без участия
NameNode идет напрямую к datanode,
причем к ближайшей datanode.
То есть у нас блоки
сфотографированы, их копии хранятся
на разных машинках.
Клиент выбирает ближайшую из этих копий
и ее выкачивает.
И назад получает блок дейта.
Жирная стрелочка.
Ну и
NameNode дает инструкции
к datanode.
Например, балансер, который тут работает,
он может перемещать блоки.
В свою очередь datanode отправляют
к NameNode hard биты.
Типа, у нас все хорошо, у нас все хорошо,
мы готовы работать.
Если они hard биты отправлять перестают,
то NameNode думает,
что datanode умерла.
Бывают еще
более выраженные случаи чтения,
когда мы, например,
не читаем файл какой-то,
а мы выполняем команду типа ls.
И вот для того, чтобы сделать ls,
нам не нужно идти в datanode,
нам нужно пойти только в NameNode,
и какие файлы по названиям
в этой папке хранятся.
Что касается записи.
Запись происходит медленно,
потому что она происходит вот так.
Мы хотим добавить файл,
идем с этим к NameNode.
NameNode нам выдает
место в datanode,
куда мы можем записать наши блоки.
И мы начинаем писать тоже напрямую
в datanode.
Записали на первую,
записали во вторую,
во вторую мы пишем уже
не с клиента, а с datanode,
то есть тут такой вот каскад идет
datanode, друг другу
передает вот этот новый блок.
Мы его пишем,
каждая datanode отчитывается в NameNode,
что запись завершена,
и сама команда завершится только тогда,
когда мы закончим всю эту запись.
То есть когда на всех
datanode появится блок.
Такой способ записи
называется синхронная репликация,
то есть пока мы не завершили
все, мы OK не получаем.
Есть еще асинхронная репликация,
она у нас будет с вами в кавке.
Когда мы записываем
информацию только
на какое-то подмножество серверов,
получаем OK, а остальное
доделываем уже в фоне,
в background.
Хорошо, есть ли какие-то вопросы
по поводу учтения записи?
Хорошо, идем дальше.
Посмотрим на UI HDFS,
что у нас тут вообще есть.
Вот здесь выдается
информация о том,
что какие-то блоки могут быть потеряны.
Например, у нас упало
сразу несколько машинок
или произошло что-то с диском.
В общем, все реплики у нас
пропали или недоступны.
Давайте посмотрим, что у нас здесь.
Здесь сказано, сколько мы
можем хранить данных в HDFS,
сколько у нас занято.
Non-DFS used
это сколько у нас используется
не для нужды HDFS.
Кстати, давайте подумаем,
а какие могут быть нужды помимо HDFS?
Вот представим, что у нас
на сервере есть
только HDFS,
нет там никаких других систем,
нет Hadoop, нет Spark,
только хранилка, все.
Куда могут идти вот эти вот
вот эта вот часть,
на что она может расходоваться?
Имперационная система?
Да.
Все сервера, на которых работает Hadoop,
это прежде всего обычные машинки,
на которых стоит
почти всегда Windows,
очень редко какие-то другие оси.
И на то, чтобы обслуживать этот Linux,
конечно, нужно место.
Дальше можем увидеть, сколько у нас
живых нод, как они между собой
распределены, то есть видите,
балансер
отработал, и в принципе
все ноды, на всех нодах
информации по поровну,
кроме пятой. Почему на пятой информации мало?
Потому что
она лежала, и только
недавно поднялась.
И что-то у нас
уже с вами семя.
Есть Rest API,
можно по нему
обращаться к файловой системе
и делать всякие операции
с файлами, и именно поэтому
порт вот этот закрыт.
То есть, когда я заходил
на кластер, вы видели,
что я использовал вот такую вот
команду
с пробросом портов.
То есть, порт наружу закрыт, мы делаем
SSH-туннель. Почему мы его делаем?
Потому что в принципе HDFS
не очень хорош в плане безопасности.
Если какая-то вебмонта торчит наружу,
то любой желающий
сможет подключиться
к ней по Rest API, например,
и что-нибудь нехорошее
сделать в файловой системе.
Вот.
Какой есть API у HDFS
и как мы можем с ним
взаимодействовать? Можно взаимодействовать
через HDFS Shell.
Это просто набор команд,
похожих на линуксовые.
То есть, вот мы пишем
префикс HadoopFS
либо HDFSDFS.
И так и так можно.
Но правда, HadoopFS это уже
устаревшая
как бы
устаревшая запись,
лучше использовать
HDFSDFS.
Вот, например,
HDFSDFS
MinusPlusVib
И вот мы вывели всех пользователей,
которые сейчас у нас есть на класты.
Поинма Shell
есть еще JavaP.
Как я уже сказал, Hadoop написан на Java
и у него есть
нативное JavaP.
Есть еще Rest API.
Мы на него тоже посмотрим чуть позже.
Ну и есть еще всякие обертки.
На семинарах их подробнее
обсудим.
Обертки на Python.
Есть библиотека HadoopPipes
на плюсах.
Вот.
Ну и давайте теперь разберем
немного кода.
Сейчас я выполню вот эту команду
и посмотрим,
что будет.
То есть это
Rest API.
Все знакомы с командой Curl
с такой утилитой.
Ну, вроде на косе было,
но не очень.
Нет.
На косе должно было быть, конечно.
Ну, может быть, подпустили.
По сути,
Curl выкачивает веб-страницу
локально.
Все, что нам от него сейчас нужно.
То есть,
как вы знаете, Rest API это, в принципе,
оно устроено так,
что у нас есть веб-страница.
По каждому запросу
у нас выдается некая веб-страница.
На ней вместо
какой-то красивой
веб-морды находится
JSON, который мы можем скачать
и как-то проанализировать.
Ну и в данном случае
мы хотим скачать, мы хотим посмотреть
первые 10 символов вот этого файла.
Что мы делаем?
Мы подключаемся к нейм-ноде,
идем по Rest API,
вводим путь к файлу
и вот дальше
дальше с помощью вот такого синтакса
вводим параметры.
Что вот это за вопрос,
это Get HTTP запрос.
То есть,
в Get запросах к веб-страницам
мы можем
через вот такой синтаксе
передавать параметры gay-value.
Вот первый параметр
и через ampersand второй параметр.
Вот, читаем
первые 10 символов,
Open открыть, 10 символов прочитать.
Видим вот такой результат.
Никаких символов нету,
кто может
рассказать почему, исходя
из того, что вы сегодня услышали.
Где все?
Есть ли какие-то идеи,
почему вы вместо символов
получили вот это
и что вот это все означает?
Но он тут указал
на какую-то другую локацию,
возможно, там надо смотреть.
Ну да, а почему
он нас куда-то перебросил?
Вот почему такое случилось?
Ну да, я не знаю.
Ну да, я не знаю.
Почему такое случилось?
Это какой-то баг системы
или что это?
Ну, вроде клиент
запрашивает на имноду
реплику блоков, а чтение нужно именно
с сервера, с ноды делать.
Да, так и есть.
То есть мы запросили у ноды
и нода нам сказала temporary redirect
и редиректную она вот сюда.
Вот это уже, как видите, написано
nip-node-03
и далее
и вот путь.
Давайте заменим этот путь и посмотрим, что будет.
Да, действительно мы видим первые 10 символов,
вот они у нас есть, первая строчка.
На самом деле можно это делать сразу,
то есть вот у нас есть
редирект вот такой,
можно это делать сразу.
Для этого нужно добавить
параметр
"-l",
это автоматическая
поддержка редиректов, то есть мы
пошли на страничку, если мы встретили
редирект, мы сразу по нему тоже пошли.
И вот у нас результат.
Вот еще примеры
команд SDFS.
Как я уже сказал,
они похожи на обычный Linux, правда
работают намного дольше.
Ну и вот
так.
Пока мы не перешли
к SCK, давайте я вот так
числами передвину и
пойдем сюда, то есть помимо
API Java, как я уже сказал,
есть обвертки на Python, есть
на C++, вот эти две,
которые выделены курсивом,
это те, с которыми я больше всего работал
и могу сказать, что да,
пойду бы она довольно удобная,
но она уже сейчас не обновляется
и она до этих пор сидит на втором
Python. SDFS CLI
это тоже удобная библиотека,
она обновляется, с ней все в порядке,
но она умеет работать
только с HDFS, то есть если
вот эти все библиотеки они умеют
производить и вычисления,
о том, как эти вычисления
работают, мы поговорим в следующий раз,
то SDFS CLI умеет
только работать с
файловой системы HDFS.
Вот, я вам
говорил, что вот есть
файловая система HDFS,
как ее вообще поймать,
вот мы заходим на сервер, тут у нас есть
обычная файловая система, мы
можем смотреть на какие-то файлы,
а можем выполнить вот такую
команду.
И вот это у нас будет другая файловая
система HDFS.
И вот это у нас будет другая файловая система,
то есть по сути, когда вы будете
работать на этом
пластере, у вас будет
две файловые системы, находящиеся
на одном сервере,
Локальная и HDFS.
Легко поверить, что они разные,
давайте вот выполним такую команду.
ls home
и вот в
home вы увидели
всех ваших коллег, которые сидят
на пластере, плюс там
студенты других курсов.
А если мы сделаем вот так,
HDFS,
HDFS-ls,
то home мы тут
не найдем, потому что в HDFS
пользователи хранятся
по умолчанию в директории user, вот здесь.
Ну и давайте еще выполним вот эту
команду.
И посмотрим
вот на что.
Все знают, что такое команда du,
что она меряет.
Видимо нет.
Это диск usage.
То есть мы
измеряем, сколько места занимает
папка в файловой системе, вот и все.
И в HDFS тоже есть диск usage,
но он почему-то вывел два числа.
Должен же быть один размер
папки или файла. Почему два?
Есть ли какие-то идеи?
Может с метаданами и без метада?
Метаданных у нас мало,
то есть на каждый блок,
а блок у нас там 6,4 мегабайта
или 128, мы храним
небольшую метаинформацию,
там пожалуй 600 байк
или что-то типа того на каждый блок.
Поэтому странно, чтобы у нас
метаинформация давала прирост в разы.
Ну там скорее всего еще
суммарный объем всех трех файлов,
всех копий, со всех реплик
с метаданами.
Так и есть. То есть вот этот размер,
это размер, который файл будет
занимать, если мы его выкачаем локально.
А это
с учетом всех его реплик,
то есть сколько места он
занимает реально в файловой системе.
Ну а теперь посмотрим на такую
вот команду FSTK.
Вообще опять вопрос к вам,
кто знает, что такое FSTK?
Что эта команда вообще делает?
Она есть не только в ходу,
она есть в любой файловой системе.
Это же тоже должно было быть на косе?
Нет, этого не было.
Окей.
Но возможно вы когда-нибудь исследовали
жесткий диск
у себя на компьютере,
на винде, например.
То есть, наверное, вам приходилось делать проверку
диска на предмет битых блоков,
битых секторов каких-нибудь,
если вы начали замечать,
что какие-то папки недоступны.
Да, команда FSTK
это File System Check,
проверка файловой системы.
Если в обычной файловой системе
FSTK нам выдает что-то нехорошее,
то это
действительно плохо,
потому что обычно у нас в компьютере один диск,
если он начал фрипаться,
то его надо менять, причем срочно.
В реальных системах FSTK
это скорее такая диагностика,
если у нас вылетела какая-то нода
или вылетел диск,
это не так критично.
Но зато FSTK позволяет поисследовать
файловую систему и понять,
где лежат блоки,
какого они размера, как они хранятся.
Вот давайте посмотрим.
На вот такую вот команду.
То есть мы прошли по папке,
вот такая у нас папка data.zps,
например, здесь мы
подключились к нейм-ноде
и выяснили статус, статус OK.
Смотрим на
количество подпапок,
количество файлов,
общий размер,
такой summary.
Смотрим на
репликацию блоков, вот кстати
default replication factor
и average block replication,
если они совпадают, это хорошо,
потому что если они не совпадают,
значит у нас где-то не хватает реплики
или какие-то лишние реплики
появились.
Но, кстати, про реплики.
У нас есть у реплик несколько состояний.
Есть misreplicated,
когда у нас нету реплики,
есть underreplicated,
когда у нас у блока меньше
реплик, меньше копий, чем мы хотели,
есть overreplicated.
Ну вот с mis и under все понятно,
у нас упала одна или несколько нот,
пропали реплики, поэтому under.
А в каких случаях может быть
overreplicated? То есть overreplicated
это больше реплик, чем мы
сконфигурировали, чем мы сказали.
Как такое может быть?
Может, например, какая-то нода
отвалилась, долго не отвечала,
мы реплицировали что-то, а потом
та нода проснулась.
Именно так. Все правильно.
Вот давайте я открою
другой файл, хотя можно и этот.
Просто возьмем вот такие
вот ключики.
Minus files, minus blocks, minus locations.
Вот таким образом мы узнаем,
где у нас хранятся блоки.
И мы видим, что
вот в этой папке у нас хранится
еще под папка, и здесь
хранится один блок. В этой папке
у нас хранится файл,
и в нем хранится один блок.
Что мы знаем про блок?
Мы знаем его блок pool, но, как я уже сказал,
в нашем случае это константа.
Мы знаем его ID, знаем его версию,
его длину.
Видите, у нас здесь блок вот такого размера,
а здесь блок вообще маленький.
Ну, файл просто маленький.
LiveReplix2, две реплики,
и про каждую реплику мы видим,
на какой ноде
она хранится.
IP-шник ноды, port.
Это ID-шник ноды внутренней.
То есть,
когда мы делаем кластер, понятно, что
в процессе работы
могут быть какие-то сетевые
перестройки, могут меняться
хосты, могут меняться
IP-шники, но за счет
внутреннего ID система
все-таки будет связываться с этими нодами.
Ну, и здесь еще указан диск.
То есть, в современных версиях Hadoop
датоноды могут
хранить блоки не только на диске,
а могут, например, хранить
в каком-нибудь облаке.
Пожалуй, облако
это самый яркий пример,
потому что сейчас, в принципе,
идет тренд
замены on-premises
систем, которые мы сами разворачиваем
и поддерживаем на какие-то
облачные вычисления.
Ну, и вот второй блок тоже.
Давайте я для примера
возьму другой файл, побольше размером,
вот, например, отсюда.
Вот, мы видим целую пучу блоков.
Вот, 0, 1, 2, 3 и так далее.
Вот они пронумерованы.
У каждого фактора репликации 3,
размер одинаковый.
Вот три датоноды,
где это все хранится.
Сколько блоков? 92.
Последний у нас чуть меньше
по размеру.
Ну, и в конце есть summary.
Мы действительно видим, что 92
блока.
Я это почему все рассказываю.
У вас будет, мы, конечно, посмотрим это
еще на семинарах, но у вас будет
домашка, в которой вы будете
исследовать файловую систему.
Вот, именно
отвечать
на вопросы типа
где, на какой ноде
и в какой бабке лежит первый блок
данного файла.
Вот, можно вывести информацию
и про отдельный блок.
Вот, возьмем блок.
Вот эту всю версию не берем.
Берем только блок.
Ну, и вот мы видим, что
у нас есть
один блок.
Ну, и мы видим, что
у нас есть один блок.
Ну, и вот мы видим, что
этот блок хранится на
трех нодах.
У него все в порядке с
репликацией, то есть
три expected реплика, три live реплика.
Никто не упал, все в порядке.
Хорошо, есть ли
какие-нибудь вопросы по вот этим вот
командам?
Как-то все очень тихо
и нас все меньше.
Видимо, нет вопросов.
Ну, окей, тогда идем дальше.
Давайте попробуем решить такую задачку.
Это...
Задача приближена к реальной,
когда вот
приходит заказчик и просит
спланировать кластер, или мы сами
планируем кластер для какого-то проекта.
Вот, у нас есть объем
всех дисков кластера.
Есть вот такой вот размер блока.
И вот
у нас есть
объем всех дисков кластера.
Есть вот такой вот размер блока.
Вот размер метаинформации.
То есть сколько памяти...
Сколько оперативной памяти в
нейминоде затрачивается на один
такой объем?
И количество реплик.
Нас поможет оценить
минимальный объем оперативки в нейминоде.
Сначала вопрос, а почему оценить?
Почему не посчитать точно?
И второе, как именно мы это будем считать?
Оценить, наверное, потому что
нацило не разделится?
Или может быть...
Да, верно.
Во-первых, нацило не разделится.
Во-вторых, мы вообще не знаем ничего про файлы.
То есть, да,
мы хотим хранить два битабайта.
Хорошо, а сколько у нас будет файлов?
Какого они будут размеры?
Или это будет один огромный файл величины 2 битабайта?
Или это будет
миллион файлов по несколько килобайт?
Это совершенно разные
затраты на нейминоду.
Ну и давайте для начала подумаем,
а в каких случаях
как нам хранить файлы,
чтобы затраты на нейминоду были минимальны?
Нужно, чтобы блоки были максимально большие.
Значит, все файлы должны быть
кратны, 64 мегабайт.
Да, или равны, или кратны.
Ну или вообще такой выраженный случай,
когда у нас будет один огромный файл
величиной 2 битабайта.
Но, правда, вряд ли такое можно представить.
Ну а теперь давайте оценим,
давайте подумаем, как мы можем это посчитать.
Тут я все-таки жду каких-то идей от вас.
Ну, давайте посчитаем, сколько всего
блоков нужно.
Ну это, видимо, где-то
2 в 21 выделить на 64.
Потом это нужно будет умножить на
600 байт.
Это то, сколько байт нам потребуется
суммарно.
Ну и, видимо, умножить на 3, потому что
для каждой реплики нам
тоже нужно вот эту метаинформацию хранить.
Тут метаинформация хранится
все-таки не для реплики, а для блока.
То есть мы берем 2 битабайта,
делим на 64,
а потом еще делим на 3,
потому что в этих 2 битабайтах
мы должны все реплики наши уместить.
Все блоки.
И вот если мы так делаем,
2 делить на 64, делить на 3,
у нас будет количество блоков.
И потом на каждый блок мы
даем 600 мегабайт.
Вот получается вот такая штука.
Ну и последнее,
что мне осталось тут вам рассказать,
это несколько статей,
которые вам понадобятся,
если будете погружаться
в HDFS
и вообще в ходу сильнее.
Статья, что в действительности
делает secondary node,
потому что, ну это
часто спрашивают нас,
что мы делаем в HDFS,
что мы делаем в HDFS,
что мы делаем в HDFS,
потому что это часто спрашивают
на собеседованиях. То, что я вам сказал,
она занимается слиянием,
но просто здесь подробно про это написано.
Дальше статья про архитектуру HDFS.
Была раньше компания Hortonworks,
она занималась разработкой
и поставкой
сборок Hadoop.
Потом не так давно,
пару лет назад, ее купила
другая компания Cloudair,
которая занималась
сборкой Hadoop.
И Hortonworks
перестал существовать,
по сути.
Сохранилась их статья
в китайском,
китайская веплика,
и здесь можно посмотреть,
как реально хранятся
блоки, файлы,
вот эти все слепки,
то есть действительно слепки
и эдитлоги, они хранятся в оперативке,
и имя нода их также вытапит на диск.
И вот как именно они хранятся,
мы можем это посмотреть.
То есть, по сути,
на каждом сервере, будь то имя нода
или дата нода у нас есть
просто папка,
там slash.dfs, она лежит в корне,
и внутри slash.dfs есть
папка для дата ноды
или папка для имя ноды,
все эти вещи хранятся.
Можно почитать про это подробнее,
есть также статья Константина Швачко
про устройство
файловой системы HDFS,
она уже такая более научная,
теоретическая, без каких-то примеров кода,
без файлов, но зато
с формулами и схемами.
Есть также вот этот вот пост,
это конспект по яндексовому
курсу на курсере по ходу,
и здесь тоже подробно
вот эти все схемы
нарисованы и рассказаны,
как, например, происходит репликация,
что сначала мы отреплицировали
на ближайшую ноду,
может быть даже на эту, на которой мы находимся,
но это не наш случай,
потому что у нас с вами есть клиентская машинка,
вот эта, на которой я сейчас
сижу, здесь нет
дата ноды,
но бывает так,
что клиент, он сам нодой и является,
такое тоже часто бывает,
поэтому мы реплицируем на него,
потом реплицируем на эту же стойку,
потом на этот же дата-центр и так далее.
И последнее, это глава третья
из книжки ходу подробное руководство,
у этой книжки есть
последнее издание, четвертое,
оно не переведено на русский,
а третье переведено,
и у него даже хороший перевод.
Окей, есть ли сейчас какие-нибудь вопросы?
Скоро домашка по пути появится?
На выходных ее Паша допилит,
я думаю, что формулировку мы выложим даже раньше,
а потом просто на выходных появятся тесты к ней.
В принципе, сейчас
с такой теоретической частью
по хранению данных
все, в следующий раз
на лекции мы начнем разбирать
MapReduce, то есть
вычисление поверх больших данных.
Окей, если вопросов нет,
то на этом все,
и тогда всем пока.

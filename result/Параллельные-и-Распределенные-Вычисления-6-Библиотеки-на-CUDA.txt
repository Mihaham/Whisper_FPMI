Всем доброго дня, мы с вами продолжаем. Сегодня у нас последняя лекция по куди, как ни
странно. Да, Данил этому рад, видимо, несказанно. Вот, и мы с вами, у нас сегодня большой план. Во-первых,
мы должны с вами разобрать, в чем заключается задача подсчета суммы на префиксе. Еще раз
детально разобраться и немного поговорить про конкарнси, как конкарнси устроен в видеокарте.
Вот наш план на сегодня. Итак, давайте вспомним, какую мы задачу с вами решали в прошлый раз. В
прошлый раз у нас была такая задача. У нас с вами есть массив, а мы, кстати, не хотим эти шторы
открыть. У нас с вами есть массив, и в нем есть элементы. Допустим, в нем 15 элементов. Нам
нужно посчитать сумму на префиксе. Значит, и по идее у нас должно быть в сумме один из двух видов
массивов. Значит, задача варьируется. В каком-то месте она называется inclusive scan. Это означает,
что мы берем элементы с нулевого по 15-й складывы. Есть exclusive scan? Что означает exclusive
scan? Это означает, что у нас сумма сначала 0, потом 0, потом 0.1, после этого 0.14 и 0.15. Мы с вами уже в
прошлый раз раз разобрались, как считать сумму в простом случае. То есть, когда у нас с вами есть
один блок, и мы хотим посчитать сумму внутри одного блока. Мы с вами рассмотрели один из алгоритмов.
Один из алгоритмов заключается в том, что мы берем наши элементы и складываем их последовательно.
То есть, складываем суммы по одному и складываем суммы по двойке. Вот такое вот решение мы с вами
рассмотрели. То есть, у нас есть элементы а0 и т.д., а7 или x0 и т.д., x7. И мы складываем элементы.
Сначала складываем по одному, потом складываем по двое, потом складываем со сдвигом 4,
потом делаем сдвиг на 8. Да, то есть, в итоге у нас получается итоговый результат в виде сумм чисел
на префиксе. Чем этот алгоритм неудачный? Давайте вспомним, что мы с вами обсуждали. Какая симптотика
у него будет? Ну, не квадрат будет. У нас высота этой... Да, а симптотика этого алгоритма будет
аналоган, если его делать в тупую. Значит, если мы будем воспроизводить это для всех элементов
массива. После этого мы сказали следующее, что если мы с вами научимся вычислять сумму на блоке,
а дальше научимся каким-то образом эти суммы агрегировать, желательно золотые,
то оказывается, что а симптотика алгоритма резко уменьшается. Ну, не так сильно она уменьшается,
но в целом ее можно будет оценить. Мы понимаем с вами, что на самом деле количество операций,
которое мы делаем для того, чтобы из N сделать N делить на блок size, у нас с вами сколько? У нас
с вами количество операций будет равное N на логарифм блок size. Потому что количество операций,
которые у нас идут, это N логарифм блок size. Но в итоге кажется, что мы с вами будем получать
тот же самый N логан, просто чуть-чуть хитрее. То есть за счет атомарных операций мы будем
работать чуть-чуть быстрее. Значит, мы будем повторять эти операции. То есть у нас N на логарифм
блок size плюс N делить на блок size, на логарифм блок size плюс 1, плюс и так далее. Значит,
если мы посчитаем эту сумму, то сколько у нас будет? Мне кажется, что будет сколько? Рубрика
арифметика. Так, один. Помогите мне, пожалуйста. Чего равняется эта сумма приблизительно? Мне
кажется, что эта сумма равняется... Что, правда? Нет, не сумма обратных, это сумма арифметической
прогрессии, о геометрической прогрессии. Да, в целом можно сказать, что для нашей задачи это будет
приблизительно 1. Вот, и плюс еще останется N на логарифм N по основанию блок size. Да, то есть мы
с вами экономим симптотику. Более того, что мы можем с вами сказать про константу этого алгоритма?
Поскольку у нас все действия будут с разделяемой памяти, то константа здесь будет меньше. Более того,
мы с вами помним, что если мы работаем с вами с нашими алгоритмами на видеокарте, то мы все это
делим на C, где C количество кудоядер. Ну, на самом деле не на C, но в какой-то степени мы будем это
делить. Вот, и оказывается, что, как ни странно, вот эта вот симптотика, она оказывается, даже с учетом
того, что нам приходится копировать наши данные с ЦПУ на ГПУ и обратно, она будет работать быстрее,
чем просто посчитать последнюю сумму элементов масси. Вот, поэтому этот алгоритм замечательный,
но если мы с вами научимся считать сумму чисел между блоками. И давайте как раз я еще раз напомню,
как считать сумму чисел между блоками. На самом деле алгоритм достаточно такой не сложный,
но, грубо говоря, в коде он нам позволит посчитать несколько более сложных операций. Так, заодно мы
этот алгоритм можем использовать и не только для этих задач. Так, стираем все с доски и получаем
следующее. Значит, представьте себе, что у нас были массивы. Я буду обозначать их сразу элементами.
0, 1, 2, 3, 4, 5, 6, 7. Вот, и давайте считать, что мы будем с вами считать эксклюзивный скан,
то есть считать суммы, начиная с нулевого элемента, и при этом последнее число будет
возвращаться как сумма чисел в блоке. Тогда смотрите, что у нас получается после выполнения
каждой операции. Значит, здесь у нас получается 0. Так, точнее сначала здесь пустота. Дальше здесь
еще раз пустота. Так, дальше у нас получается 4, 5, 6. Дальше еще одна пустота и еще одна пустота.
Так, здесь у нас будет сумма всех 12-15, здесь у нас будет сумма 8-11, здесь у нас будет сумма
4,7. Значит, теперь как у этого всего посчитать сумму чисел? Мы делаем следующее. Мы берем вот
эти вот штуки, загоняем в отдельный массив, получаем 0-3, 7, 12-15. И давайте для этой штуки тоже
прогоним эксклюзивный скан. Собственно, что у нас получится? У нас получится пустота. Дальше
получим 0-3. Этот с этим просуммируется, получим 0-7. Здесь получим 0-11. И на выходе мы получим
сумму чисел в массиве. Сколько у нас тут будет? У нас тут будет сумма с 0 по 15. А теперь что... То есть
у нас тут еще этот скан. Что мы с вами можем сделать? Мы можем сделать хитрую вещь. Мы можем
взять и сделать операцию суммирования каждого числа с номером потока, в котором он находится.
То есть здесь у нас будет сумма чисел 0-0-1-0-2. Значит, когда мы проинжектим вот этот блок,
вставим, мы получаем с вами 0-6. Значит, когда мы вставим вот эту сумму с 0-7, мы получим сумму
уже начиная 0-7, 0-8 и так далее. И когда мы вставим с вами последний блок, у нас будет общая сумма
чисел массива. То есть тем самым мы с вами реализовали опцию скан. То есть вот одна дополнительная
симпатика ОАТН, которая у нас получалась. Вот этот вот инжект. Это как раз у нас ОАТН.
Так, это я еще раз разъяснил тот алгоритм, который мы в прошлый раз делали. Так, понятен ли этот
алгоритм? Вот, отлично. Ну, спрашивается, а почему бы это все не реализовать за ОАТН?
В сумме. Кажется, что у нас не очень большое количество операций. То есть мы дублируем лишние
операции, потому что на слайде кажется, что много из этого не надо. Но давайте подумаем,
что мы можем сделать для того, чтобы попробовать не дублировать операции.
Давайте порисуем немного, что у нас в массиве находится. Так, мы будем с вами считать эксклюзивный
скан. Сразу скажу. И давайте вот представим себе, что у нас с вами есть числа. Так, 07. Хотим мы в сумме
посчитать следующую сумму. Пустое множество. Так, 0. 04. Да, что-то я промахнулся с количеством
операций. Вот так как-то. Лучше будем считать, что я ровно нарисовал каждую ячейку под каждой. Так,
ну давайте подумаем. Значит, что хотелось бы получить? Тут нужно сделать следующее. Давайте
разделим массив пополам. И поймем, что у нас есть в правой половине нашего разделения того,
чего нет в левой половине. Что есть общего у всех вот этих вот сумм, и нету вот этих сумм.
Нет, сумма 0.2 тоже здесь есть. Тройка, да. Смотрите, здесь у нас есть сумма 0.3. А где мы ее получим
здесь, в верхней части экрана? Наверное, мы ее получим вот таким образом. Если мы посмотрим
наш алгоритм классический, если мы уберем лишние стрелочки, то мы сумму чисел получим здесь,
за два шага. Вот, и каким-то образом нам нужно будет попытаться перенести эту сумму в правую
часть. Давайте подумаем, чего еще у нас есть в этой сумме. Дальше, если мы детально раскинем
нашу картинку, то мы увидим здесь следующее в водораздел. Что у этой половинки есть сумма чисел
от 4 до 5, а здесь нету суммы чисел от 4 до 5. То есть нам нужно каким-то образом еще посчитать
сумму чисел от 4 до 5. Вот, она каким-то образом накапливается. И давайте мы делаем следующее,
что мы будем накапливать только конкретно свою сумму, которая необходима. То есть у нас получается
с вами сумма чисел 4,5, 6,7. Дальше мы получаем сумму от 4 до 7. Вот, а теперь давайте подумаем,
что мы с этим можем с вами сделать. Поскольку у нас эксклюзивный скан, я хочу подчеркнуть,
что у нас скан эксклюзивный, то, в принципе, наверное, что-то с этим можно сделать. Давайте
я здесь выполню проекцию. 0, 1, 2, 4, 4, 5, 6, 4, 7. Давайте думать. Значит, идея такая, а давайте попробуем
те части, которые были делим наш массив пополам и перекидываем левую часть направо.
То есть нам нужно перекинуть левую часть направо. А с правой частью мы поступим хитрее. Смотрите,
здесь первый шаг нетривиальный. Мы забываем про вот этот вот элемент и прокидываем пустоту. То
есть смотрите, идея такая, что здесь сумма чисел от 0 до 3, здесь у нас пустой элемент. Что такое вот
это? Вот это 0,3 плюс пустота, вот это 0,3 плюс 4, вот это 0,3 плюс 4,5, это 0,3 плюс 4,6. А здесь пустота 0,
0,1, 0,2. Соответственно, мы делаем следующую вещь. Мы берем и прокидываем пустоту сюда.
То есть получаем здесь пустоту, а здесь сумму чисел от 0 до 3.
Ну смотрите, что у нас получается. У нас на самом деле в какой-то момент алгоритм перестанет
работать. Давайте посмотрим внимательно. Если бы мы получили бы элемент здесь и оставили бы его,
не ставили пустотой, то дальше бы мы эту сумму забыли затереть. Но давайте посмотрим внимательно.
Здесь у нас сумма чисел 4,5 и хочется, что нам сделать? У нас есть сумма 4,5 и давайте мы ее
сагрегируем. То есть сложим ее здесь и получим сумму чисел от 0 до 5. А здесь мы прокидываем число
от 0 до 3. То есть у нас получается такой перекрестный алгоритм сложения. То есть идея такая,
что левую часть мы перекидываем вправую и с ней складываем. А вправой мы берем и складываем ее,
правую часть просто перекидываем налево. То есть получается у нас такое перекрестное сложение.
Здесь у нас идет перекрестное сложение, здесь идет перекид. Вот такой алгоритм интересный.
У нас с вами получается так. Последняя стадия это перекрест. Я сейчас покажу слайд. Я просто
хотел показать идею того, как это работает. Никакую структуру данных вам это не напоминает?
Ну не совсем дерево отрезков. Другое дерево. Да, это дерево фенвика. Да, это ровно дерево
фенвика. Именно то, как в нем сумму вычисляется. Сумма числа на префикс. То есть она неявно
позволяет нам вычислить сумму числа на префикс. Кажется алгоритм замечательный. Да, он выполняет
обольшое аттен действий. Да. Но давайте подумаем, в чем здесь может быть проблема?
Во-первых, сколько нам действия придется теперь делать на каждый блок? Во-первых,
нам нужно будет сделать удвоенный алгоритм блок size действий. Ладно, это еще ничего страшного.
Вопрос. Сколько варпов при этом у нас будет задействовано?
В целом даже можно сказать, что у нас будет задействовано порядка от варпов, потому что,
наверное, если мы будем складывать сумму чисел 0 и первого элемента, неплохо было бы и делать
нулевым потоком. Да, а вот это сумму чисел было бы неплохо складывать первым потоком. Так,
но здесь у нас опять возникает некоторая проблема. Кто видит проблему? Мы в прошлой лекции переместили
арифметику сложения наших элементов, и внезапно наш код начал работать медленно.
Кто помнит по какой причине? Да, у нас бан конфликт возникает, когда мы в одну
ячейку в разделяемой памяти внутри одного варпа пишем с двух разных потоков. Давайте
посмотрим, где у нас тут варп конфликт наблюдается. Конфликт здесь наблюдается
в данном моменте времени. Представьте себе, что у нас с вами есть сложение нулевого и первого
элемента массива. Берется нулевой поток, и он складывает значение в первую ячейку массива.
Параллельно где-то есть у нас 32 и 33 ячейка массива, в которой складываются элементы с 16 потоком.
И запись идет в 33 ячейку. Завечу, что по модулю 32 эти ячейки имеют одинаковый индекс,
это означает, что мы с вами получаем варп конфликт. Казалось бы, как его можно попробовать
решить. Замечу, что нумерацию своим индексом делать тоже плохо, потому что тогда у нас
большое количество варпов начинает работать. Есть ли у кого-то еще вариант?
Ну можешь попробовать. Так. Да, я проспойлерил, черт. Жалко. Верите ли вы в паранормальное явление,
в мистику? Да. Кстати, как-то странно, я эту мистику показал ровно таким образом,
как она происходит. Явление 25-го кадра, слышали? А, блин, какое 25-е кадр сейчас расширение частота
мониторов 60 и 140 Гц. Ну да, 61-й кадр получается. То есть идея такая, что давайте вместо того,
чтобы... Какие мысли были? Что людям, чтобы внедрить какую-то рекламу или какое-то мнение,
нужно показывать один дополнительный кадр по телеку. Вот. Для того, чтобы им сомну шанс.
Ну типа того. Вот. А давайте мы как раз поступим здесь ровно таким же образом. Мы с вами введем
дополнительный кадр. Только у нас это будет не получается, не 25-й кадр, а 33-й кадр.
Значит, в чем суть? Суть в том, что вы берете вот этот вот элемент, вот этот наш чистый массив,
и закрашите каждый 32-й кадр. Тогда смотрите, как у нас меняется индексация. То есть у нас
получается, что теперь вот этот вот элемент меняется 32-м актуальным. Но на самом деле номер
кадра 33-й. Поэтому 16-й поток, несмотря на то, что он будет складывать элементы 32-й и 33-й,
он будет складывать на самом деле со сдвигом на один. То есть поток-то 16-й, кладет-то он тоже
в 33-ю ячейку массива. Но на самом деле это будет не 33-й элемент массива, а 34-й элемент массива.
Да, потому что у нас двиг произошел на одну ячейку. Более того, этот эффект будет повторяться в
дальнейшем. Какой-то момент оно опять совпадет. Ну через сколько она совпадет? Ну да, то есть на
самом деле номер актуального потока будет следующим. Н плюс Н делить на 32. Да, и как ни
странно, можно определить минимальное значение элемента И, в котором И поделить на 32 будет
равняться 32. Вопрос, чего тогда равняется И? Ой, внезапно, а у нас максимальный размер блока 1024.
Все. То есть сдвиг работает. На семинарских примерах, по-моему, там выигрыш идет... Сколько
будет 30 на 25? 20 процентов дает выигрыш. Вот этот вот способ. И несмотря на это, вот даже решение
задачи при помощи 33-го кадра будет на 20 процентов медленнее, чем тупое решение сложить все совсем.
Ну, то есть сделать N-логан действий, но при этом с хорошей симптотикой. Так. Есть ли вопросы
по этому алгоритму? Ну, собственно, если немножко раскурить и подумать, какие алгоритмы позволяют
решать дерево фенвика, то на видеокарте мы понимаем, что у нас есть параллельное дерево
фенвика под капотом. Вот. И в принципе его можно как раз использовать в параллель. Вот. Но это на самом
деле полезный момент. Да. Кстати, вот еще раз один индексации, что у нас с индексацией меняется.
Это мы тоже с вами посудили. Но можно сделать еще и так. Собственно, что делается здесь? Здесь
происходит наш хитрый момент, который заключается в том, что 1024 это 32 в квадрате.
Ну, а поэтому почему бы нам не использовать большое количество синхронизаций, а попробовать
сделать подсчет суммы внутри каждого варпа, потом раскинуть сумму на префиксе и посчитать
сумму еще один раз. Да, в итоге опять же мы с вами экономим количество операций синхронизации,
которого у нас имеется. И вот эта реализация алгоритма является одной из самых оптимальных
реализаций нашего кода. Да, тут говорится как раз, как распределяются вот эти вот агрегированные
суммы. То есть как раз, смотрите, у нас считается сумма на префиксе, как это у нас до этого было.
Дальше мы говорим с вами, что вот сумма чисел на префиксе как раз распределяется на свои собственные
потоки. Как раз вот тот алгоритм, который мы с вами рассказали, и потом считается сумма еще раз.
Вот такой вот хитрый алгоритм, который я сейчас реализую. Так, это мы с вами разобрали задачу
scan. Давайте поговорим про ее применение. Первая задача — это задача compaction,
кстати, которая вам предлагается реализовать в задании. Да, это последняя задача — функция
filter. В чем она заключается? Она заключается в том, что нужно оставить те элементы массива,
которые ультворяют определенному фильтру. То есть те значения, которые меньше какого-то
определенного х. И второй способ — это сортировка массива. Если вы понимаете,
как фильтровать значения меньше определенного, то какого алгоритма можно запустить?
Ну, quick sort, может быть даже merge sort. То есть, в принципе, на видеокарте можно запустить
merge sort. Более того, если вы умнеете правильно считать сумму на префиксе, то можно запустить
radix sort. То есть, разные виды сортировок можно применять. Причем radix sort с битовой арифметикой.
Ой, не с битовой, а с большой арифметикой. В принципе, можно будет подумать над этим.
Значит, quick sort по разряду сортировок. Значит, как работает compaction? Представьте себе, что у нас
есть исходный массив, и нам нужно отфильтровать массивы, которые ультворяют условия. То есть,
это массивы с элементом массива P. Дальше, что мы делаем? Мы говорим метки массивов. Значит,
у нас с вами получаются элементы 1 и 0. А после этого мы считаем сумму на префиксе. И смотрите,
что вы можете сказать? Мы можем взять те элементы, в которых значение единичка, посмотреть на их
индексы и взять как раз элементы с определенным индексом. То есть, образно говоря, смотрите,
мы берем вот эту и вот эту ячейку, единичку и пятерку. Значит, вот этот элемент массива должен
переехать на пятое место. Вот он переехал на пятое место у нас. Ну, если считать в один
нумерации. Если считать ноль нумерации, то он точно переедет на свое собственное место.
То есть, в зависимости от того типа скана, который вы делаете, inclusive или exclusive,
немного меняется индексация. То есть, в принципе, функции, как мы поняли с вами,
функция фильтр будет работать быстрее. На видеокарте. Да, опять же битовую маску тоже
можно считать достаточно быстро. То есть, здесь нужно будет реализовать скорее всего либо два
ядра, либо одно ядро. Так, понятно ли суть решения этой задачи? Отлично. Быстрая сортировка. Как
реализовать? Быстрая сортировка. Да, выбираем случайный пивот, но главное дальше делать
compaction in place одновременно для условий меньше и больше, чтобы не городить лишнюю память.
Потому что, я помню, в какие-то года мы давали задания, связанные с быстрой сортировкой, и у
людей оказывалось, что быстрая сортировка работает медленнее, чем обычная сортировка на
обычном компьютере. Более того, было такое, что типа quick sort вообще не работал. Там что-то
работал внутри одного блока, а между блоками не работает. Вот поэтому можно запускать рекурсию.
Вот. То есть, это что касается быстрой сортировки наших массивов. Значит, из таких интересных
вещей касательно вот этих алгоритмов. Мы рассмотрели с вами классические алгоритмы работы на видеокарте.
Из алгоритмов, которые не надо реализовывать самостоятельно. Самостоятельно не стоит
реализовать быстрое преобразование фурье. Значит, есть ее аналог в мире геометрии.
Задача. То есть, быстрое преобразование фурье на видеокарте это то же самое, что некоторая
геометрическая задача. Это задача построения правильного многоугольника с циркулем линейкой.
Как это ни странно. Объясняю почему. Вот мы с вами в концепции редакшн и скан пытались избавиться от
бан конфликтов. И дальше делается следующее, что если вы попробуете реализовывать быстрое
преобразование фурье, то для достаточно большого количества чисел вы нарветесь на такое большое
количество бан конфликтов, что если вы попробуете их реализовывать, вы получите еще больше бан
конфликтов. В итоге реализация становится неэффективной. И вот тут если мы пойдем с
вами и посмотрим библиотеку, связанную с быстрым преобразованием фурье. КУФФТ называется библиотека.
Вам говорят т-т-т-т. Все замечательно. Где оно? Здесь есть прямо этот. Во, нашел. Вот
прочитаем этот полк. Алгоритм хорошо сильно оптимизирован для размеров, которые представляют
собой произведение двоих, троих, пятерок, семерок. При этом, если это степень двойки,
у вас все замечательно. Если у вас эта степень двойки и тройки произведите, то чуть медленнее.
Да, если у вас внезапно вы хотите вычислить быстрое преобразование фурье на числах,
которые кратны 11 на размерах массива, извините, вам придется потерпеть. Вам вас переключат на
классическую реализацию. Просто потому что количество синхронизации будет достаточно большим. Это что
касается как раз видеокарты, библиотек на вот этих вещей. И сразу давайте скажу про те библиотеки,
которые есть на видеокарте. На самом деле их очень много. Их много всяких разных. И самая
классическая библиотека, которую можно попробовать использовать, это библиотека Кубласс. Собственно,
это библиотека для линейного алгебра. Как ни странно, если вы пользуетесь каким-нибудь пакетом
нампай, сцепай, либо еще что-нибудь, и внезапно перемножаете матрицы, то скорее всего вы
перемножаете их при помощи Бласса, определенной реализации. Это наиболее эффективное вычисление
операции в видеокарте, это при помощи Кубласса. Единственный момент, сразу скажу следующее,
что если вы попытаетесь реализовать какую-нибудь линейную алгебру при помощи Бласса, то скорее
всего код у вас будет сильно медленнее, если вы будете использовать чисто ее. То есть код на Куде
все-таки намного быстрее. Но при этом там есть удобные механизмы того, что вы можете алоцировать
массивы, заполнять их определенными элементами. Но правда, как обычно там сишный код, большое
количество параметров. Следующая библиотека это QFFT, после этого есть CUDA Mass Library, то есть это
стандартная библиотека математическая, то есть можно внутри кода ядер писать произвольные функции.
После этого есть библиотека Curant, которая позволяет запускать псевдослучайный генератор. И из таких
библиотек еще две есть. Интересных это Solver, который умеет решать линейные уравнения. Да,
как это ни странно, на видеокарте достаточно быстро. А мы понимаем, что решение уравнений на видеокарте
это полезная операция. Но я правда не знаю, фан может быть не сразу стоит, понятно, но там любая
физика, аэродинамика и так далее, это решение линейных уравнений. Да, в частных производных. Ну,
для этого как раз можно использовать видеокарту. И есть Qsparce, которая работает для Sparse Matrix,
правда, так сказать, она не всегда эффективна. Поскольку у нас появились операции для работы
с тензорами, то появилась как раз библиотека, связанная с тензорами. И есть еще внешняя библиотека,
которая позволяет запаковать все в определенный набор, это AMGX. Это что касается математики,
но куда же без параллельных вычислений, как говорится, не только на одном узле. И для этого есть
тоже две библиотеки. Первая это спецификация OpenShme, OpenSharedMemory, NVIDIA SharedMemory,
которая позволяет разделить общую память между несколькими видеокартами. Но на самом деле,
зачастую это не используется. Зачастую используется сеть библиотека по названиям NCCL. То есть она как
раз позволяет реализовывать коллективные операции в MPI на уровне видеокарт. То есть си реализовывает
распределенное обучение. А по факту, если вы хотите обучать нейросети, то как дистранда вам нужно
просто уметь считать средний градиент по больнице. Вот, между всеми вычислительными узлами. Ну и как
раз эта библиотека вполне спокойно это позволяет сделать. Так, кода и искусственный интеллект.
Куда же без него? Здесь есть библиотеки связанные с первой, две известные. Первая это QDNN,
Kuda Deep Neural Networks. Значит потихоньку от нее отходят от библиотеки QDNN. То есть ее еще
редко увидеть в классических реализациях. Но в свое время она была достаточно популярной. То есть
насколько я знаю современные библиотеки Torch, они могут работать и без QDNN. Вот. И вторая библиотека
это библиотека TanzRRT. Она позволяет сильно ускорить производство нейросети. То есть прогон нейросети
в режиме Production. Вот. Ну и на самом деле, если вы пользуетесь большим количеством плагинов,
то в принципе тоже могли замечать, что основная обработка голоса проводится на уровне видеокарты
и так далее. Значит если касаться того, какие есть аналоги Kuda. Потому что Kuda везде доступна.
Один из основных форматов это OpenCL. То есть это аналог Open Graphical Language для других видеокарт.
Да. И как ни странно, писать код на OpenCL это интересно. Можно зайти в репозиторию TanzRflow
и посмотреть, как реализован фреймворк TanzRflow Lite, который позволяет запускать
нейросети на мобильных устройствах. И там по факту идет просто огромный свитч на то,
какой у вас графический ускоритель. И после этого запускается. Значит. Конечно же у разных
сейчас производителей появляются тоже свои собственные механизмы. Если мы говорим про Apple,
то это механизм под названием MPS. Его поддерживает сейчас, кстати, Kuda. И если мы говорим про
AMD видеокарты, то это RockM. То есть у каждых движков есть свои собственные тоже механизмы. Но,
как ни странно, Kuda стала достаточно популярной и поэтому используют ее. Ну в принципе OpenCL
тоже поддерживается. Значит есть вообще, если вы вообще не хотите писать код на видеокарте ни в
какой степени, то для этого есть библиотека OpenACC. Значит на семинарах кто-то из вас проходил
библиотеку OpenMP, в которой все при помощи Pragma написано. Прагма OMP Parallel4, а под капотом вам
TreadLoop создается. Вот. Такое же самое можно организовать и на видеокарте. Значит из таких
дополнительных вещей тоже, значит, про библиотеки, если вы внезапно не хотите писать на питоне,
ой, писать на питоне, да, то, в общем, есть несколько разноуровневых механизмов. Ну,
первый это из пушки по воробьям. Вы не поверите. Нет, сам высокий уровень. Взять PyTorch, короче говоря.
Просто взять PyTorch и взять операцию с тензорами, которая у него там имеется. Работать будет медленно,
но будет работать. Тем более матрицу перемножать она сможет и на тензорных ядрах. Значит дальше
спускаться на уровень библиотек, то в питоне есть библиотека под названием Numba,
которая умеет работать с видеокартами. Если спускаться еще ниже, то здесь обычно существуют
две параллельных библиотеки, которые дружат друг с другом. Первая это называется PyCuda. Она
имеет как low-level interface, связанная с тем, что вы пишете ядро на C, потом вы компилируете,
и дальше из питона дергаете. И дополнительно есть набор примочек, связанных к ней. Это библиотека
Рейкна. В ней есть некоторые другие ядра, которые можно использовать. И в принципе в них как раз
уже задача редакшена и скана уже реализована. То есть по факту их с нуля обычно не приходится
писать. Так, это что касается задач редакшен скан и библиотек, которые мы с вами можем
использовать для работы со всем этим чудом. Так, давайте вопрос. Хорошо. Я на самом деле
подготовил Google Collab с этим с материалами по PyCuda, так что возможно на семинарах в этом
году как раз эти примеры будут разбираться. А мы поедем к следующей лекции. И как ни странно,
мы еще не все с вами разобрали с видеокартами. Последний блок будет посвящен тем, а как же
работают на самом деле операции на видеокарте. Мы с вами говорили следующее, что операция CUDA
memcpi у нас является... Какой? Блокирующей. Что произойдет, если ее сделать не блокирующей?
Не, на самом деле печаль не будет. Просто нужно будет понимать каким образом работает конвертность
видеокарте. И как раз для этого нам нужно будет разобрать нашу преамплу. Кажется,
что мы уже научились все делать на видеокарте. Но как ни странно, мы с вами до сих пор используем
всего лишь три параметра вызова ядра. А их четыре. Всего параметров вызова ядра четыре. И хотелось бы
разобраться, каким образом можно использовать четвертый параметр вызова ядра. Но не все так просто.
Так, смотрите. Вот обычно у нас выглядит вот набор команд выглядит вот таким образом. То есть у нас
есть с вами CUDA memcpi. Более того, мы можем ее использовать синхронно и можем использовать
ассинхронно. Если ассинхронную операцию CUDA memcpi async использовать, то у вас все будет выглядеть
ровно таким же образом, как в классическом коде на видеокарте. То есть вы можете этот async просто
убрать и у вас будет все то же самое. Изначально вот такой у нас поток команд. У нас время выполнения
команд ровно такое. Сразу скажу, что здесь скорость замера программы будет осуществляться и с учетом
операции копирования. Вот это очень важно. То есть мы делаем еще учет операции копирования. Можно
сделать так. Смотрите, мы делаем четыре потока, четыре стрима объекта и делаем следующее. У нас
у нас есть ядро один и после него мы сразу вызываем CUDA memcpi device to host синие стрелочки. И
оказывается, что поскольку у нас на видеокарте строится граф зависимости по коду, то в конвейере
получается так, что на видеокарте memcpi имеет свой собственный поток управления. А это означает,
что в принципе device to host, копирование хостана девайса с девайса на хост может осуществляться
параллельно. Но опять же главное, чтобы у нас стояла зависимость по данным, потому что если у нас нет
зависимости по данным, то контроля у нас не будет. Можно сделать вот так. Что мы говорим? Давайте мы
попробуем с вами использовать concurrency на уровне как раз потоков исчислений, потоков. То есть что
получается? Мы можем сделать еще одну операцию, так называемую freeway concurrency. Когда мы копировать
данные будем в одном стриме, потом сразу на нем будем выполнять код, выполнять код еда, а дальше
потом копировать снова. В итоге у нас получается вот такая пирамидка, называется freeway concurrency.
Но в принципе, если мы с вами умные люди, то можем понять, что при желании мы можем с
вами и четвертый поток исполнения использовать. Это поток исполнения на нашем ЦПУ. То есть мы
закинули код выполнения ядра, у нас все операции синхронные. Это означает, что наш основной поток
исполнения выполняется. Может выполнять какие-то действия. И дополнительно можно после этого
попробовать все это дело еще и атомизировать. То есть выполнять не одно ядро целиком, а разбить его
на поднаборы ядер. Тем самым у нас пирамидка увеличивается. То есть это сколько способов
осуществить concurrency у нас есть. И оказывается следующее, что если посмотреть на количество
операций в секунду, то на ЦПУ это все работает порядка 40 гигафлопс. Да, это кстати бенчмарки
до этого. А теперь смотрите, классический ГПУ нам от силы даст 120 гигафлопс с учетом операции
копирования туда-обратно. Но если попробовать использовать freeway concurrency, то есть скопировать
в одном месте, а потом копировать по факту исполнения обратно, то мы уже получаем ускорение в
полтора раза. Третье, что у нас есть, это если мы еще и копирование в одну сторону сделаем,
то мы ускоримся еще где-то практически в полтора раза. Если мы подключим поток исполнения на ЦПУ,
мы конечно не сильно преувеличим, но в принципе можем использовать наши операции. Вот, и в целом
в бесконечности в пределе у нас получается 330 гигафлопс в секунду. 330 гигафлопс. Тут, знаете,
есть какая абстракция. Значит, по-хорошему есть такая физическая задача. Значит, у вас есть
кирпич. Поверх него вы ставите еще один кирпич, чтобы первый кирпич не упал, максимально выпирающий.
Потом вы ставите второй кирпич таким образом, чтобы эти кирпичи не выпирали, не падала эта
конструкция. И повторяете так до победного. Вопрос, типа, а какое максимальное отклонение вы
можете получить? Вот, с этой конструкцией кирпичей, чтобы ее не перевернуло. Вот,
в принципе задача здесь ровно такая же. Слышали про такую физическую задачу?
Да, да, да. Сейчас, тут можно даже попробовать это. Ну да. Не, не, не, до задачи.
Вот она.
В двойке стремится ответ. Ну вот, здесь тоже самое на видеокарте мы получаем. Тот же самый эффект.
Вот, а это значит, что нам нужно познакомиться с этим объектом, который нам позволит это сделать.
И это объект, откуда стримы. Значит, говорить следующее, что если задача легкая,
почему бы ее не спустить в отдельном стриме? Сразу скажу, что поток и стрим, трет и стрим,
это две разных концепции. Более того, я скажу следующее, что если вы попробуетесь воспользоваться
библиотекой Рейкна, то там стрим называется поток. То есть, там стрим называется как трет.
Ну не знаю. Видимо не все. Вот. Чего? Да я не уверен.
Значит, смотрите, по умолчанию мы работаем с вами в нулевом потоке. В нулевом стриме, так сказать.
Извините, я что-то говорил. Вот говорит стрим. Вот. И этот стрим по умолчанию синхронный к хосту
и девайсу. То есть, как бы все операции, которые осуществляются в нулевом стриме,
они по умолчанию являются синхронным. То есть, как бы у вас could be spying a sync в нулевом стриме
будет вести себя ровно так же, как просто обычный could be spying. Вот. Значит, если вы не указываете
параметры, то у вас параметр равен нулю. Хорошо. Значит, да, синхронный на хосту и девайс.
Значит, в чему синхронный? Значит, стрим синхронен. Значит, следующая вещь. К запуску гидра. Раз. Дальше.
К операции could be spying. Два. Третья операция could be spying, но если вы копируете память с девайса
на девайс. То есть, вам не нужно синхронизировать хостай девайса. И как ни странно, стримы синхронны
еще к операции could be spying хоста на девайс. Да, потому что вы отправляете буферизированный вывод
на видеокарту. То есть, по факту мы даже, кажется, про это говорили, когда мы говорили, что мы с вами
можем и спин в памяти отправлять наши данные. То есть, нам достаточно сделать аналог МАП в виртуальной
памяти. То есть, пригвоздить нашу виртуальную память к реальной и сделать копирование. Все это
происходит на уровне одинкаширования. Так, вот он код. Пример этого кода. То есть, мы делаем
could be malloc, а дальше мы делаем could be memcpy. Это вот синхронное ядро. То есть, это то, как мы
раньше писали все. Грит блок ноль. А теперь смотрите, как у нас меняется код. Собственно, у нас
получается следующее, что при желании вот эти вот два кода, то есть, выполнение ядра два и выполнение
метода на CPU, они являются у нас потенциально пересекающимися друг с другом. То есть, они
выполняются параллельно. А теперь сделаем вот такую вещь. То есть, мы создадим объект вида
kuda-stream-t. Создадим kuda-stream-create. Сделаем malloc-host. То есть, привяжем нашу память. Это важно
после того, чтобы мы могли сделать memcpy. И в итоге получается следующее, что все эти операции
потенциально могут выполняться параллельно. Каждый в своем стриме. То есть, операции внутри стрима
выполняются последовательно. Между стримами нет никакой гарантии. Но как тогда определить,
когда у нас данные, грубо говоря, два ядра будут выполняться последовательно друг за другом? Есть
мысли? Ну, первое это барьер поставить, да. Не, но когда точно нам нужно сказать, что одно ядро точно
будет выполняться после второго. Но если есть общие элементы массива или общие просто нотации,
общие указатели на память. Да, мы в принципе на стадии статического анализа кода это можем выяснить.
Так что у нас типа тут у нас массив x, здесь у нас массив y. Это было в момент времени t1,
это было в момент времени t2. То есть, если у нас y зависит от x, да, какой-то степени. Желательно,
почему он мог зависеть? Либо у нас и произошло копирование из y в x где-нибудь, либо у нас
собственно в этом моменте времени t1 у нас тоже был в ядре параметр y. То есть, получается у нас
момент времени t2 зависит от момента t1. И даже вне зависимости от того, что у нас здесь один стрим,
а здесь второй стрим, они все равно будут выстраиваться последовательно. То есть,
все равно здесь неявный барьер будет, неявное ожидание получения результатов будет здесь,
здесь относительно этого. В принципе порядок не надо нарушать. Эта видеокарта будет учитывать.
Так, хорошо. Единственное, смотрите, нужно привязать нашу память, сделать ее пинт.
Ох, так дайте я остановлюсь здесь и спрошу, понятно ли то, что здесь произошло.
Ну и в целом мы можем создавать большое количество стримов. Но не все так просто. На самом деле в
видеокарте можно понять, возможна ли concurrent kernel execution, то есть можно ли выполнять их
параллельно. И по умолчанию, если у вас есть concurrent kernel execution, то от силы вы можете
выполнять два ядра параллельно. Больше вы никак не сможете. Ну как же нам все-таки решить нашу
проблему? Для этого как раз нам нужно понять, что есть отдельная очередь копирования. То есть,
у нас с вами, когда выполняется код на видеокарте, то у нас с вами есть очередь копирования с их
host на device. Дальше у нас с вами есть очередь для вычислений и после этого у нас есть очередь
копирования с device на host. Важный аспект заключается в том, что если вы какую-то операцию уже
запланировали, после этого кажется, что вы выполняете какую-то дополнительную операцию,
которая не должна зависеть от предыдущей, но вы ее поставили позже по времени, то она у вас
поставится позже по времени. То есть, здесь временная шкала является актуальной. Сейчас мы
это тоже посмотрим на примере. Смотрите, вот у нас есть две последние операции. Мы с вами делаем
следующее. У нас порядок вызова наших операций. У нас есть host device для массива A1, дальше host
device для массива B1, дальше мы выполняем kernel с первым элементом массива, после этого мы
выполняем device to host с первым элементом массива и после этого выполняем device to host для второго
элемента массива. И смотрите, что происходит. У нас получается следующее. У нас с вами DH2 никаким
образом не зависит от предыдущих операций, но из-за того, что DH2 стоит после всех операций,
он должен дождаться, пока выполнится DH1, потому что DH1 в очередь поставился раньше, чем DH2.
А все операции после этого являются блокирующим друг друга. Перед тем, как выполнить ядро K1,
нам нужно скопировать с host device массива A1 и B1. А потом, чтобы скопировать обратно,
нам нужно дождаться времени. В итоге у нас получается неприятная картина. То, что те массивы,
которые у нас с вами не зависят друг от друга, блокируют друг друга. Вопрос, каким образом можно
переставить порядок действий, чтобы код пополнялся на 25% быстрее. В общем, на самом деле все просто.
Нужно DH2 вынести в самое начало. Тогда у нас вот эта пирамидка, связанная с DH2,
провалится наверх. Важно, что если мы поставим ее где-то раньше, ну, допустим, после DH1.
Смотрите, тут важный момент, что если вы поставите ее сразу после выполнения HD1,
то она окажется параллельна с ней. Если вы поставите после HDB1, то она окажется параллельна
с HDB1. То есть время операции будет учитываться. То есть мы с вами строим граф порядка управления.
Вот, то есть еще раз давайте я нарисую, что если у нас, собственно, вот такая вот штука.
1, 2, 3, 4 и операция 5. Значит, если мы поставим, смотрите, 1, 5 штрих, то 5 штрих окажется здесь.
Если мы при этом укажем с вами другой порядок. Мы укажем с вами 1, 2, 5, 2 штриха,
то тогда операция окажется здесь. Ну, а если мы укажем 1, 2, 5, 3 штриха, то 5,
3 штриха окажется здесь. Оно не зависит, но зависит от того, в какой момент времени мы поставили,
то есть когда мы его задекларировали. Нет, нет, вообще никак не зависит. Они поэтому разными цветами
подсвечены. Так, хорошо. Смотрите, мы с вами разобрались. Вот, да, то есть если мы HDB2 переместим,
то мы перенесем в самое начало. Еще один кейс. Смотрите, значит, мы можем поставить наши массивы
вот таким образом. То есть KA1, KB1, то есть у нас в последствии операции. Но при этом у нас с
вами есть массивы, которые взаимодействуют с операциями, с массивом A и делаются несколько
разных операций. То есть у нас есть операция A1, у нас есть операция B1. И как раз когда мы объявляем
операцию A2 с нашим массивом, то есть у нас массив, допустим, используется чисто для чтения и больше
с ним ничего не делается, то как раз порядок выполнения операции становится таким, что у нас
A2 становится параллельно B1. С этим все замедляется на 50%. То есть мы ждем пока у нас B1 блокирует A1.
То есть это как раз стрим-исполнение у нас первый, а здесь у нас стрим-исполнение второй.
У нас A2 будет после D1. Если мы поменяем порядок, и оказывается, что в стриме A1 и A2 мы можем
работать с массивом A в режиме чтения. В принципе, ядро это может быстро достаточно
проанализировать. Это делается на уровне статического анализа. Вот, ну мы эти ядра можем поставить
параллельно. И исполнение можем поставить параллельно, и все у нас будет замечательно. То есть время
работы будет хорошо. То есть смотрите, цель следующая, что старайтесь параллельная операция располагать
именно параллельно друг к другу, причем в коде исполнения. То есть если у вас есть операция A и операция B,
которая ей параллельна, нужно сразу запустить и A и B, потому что если мы запустим C, которая идет
после A, то, извините, B уже будет выполняться параллельно с C, а не параллельно с A. Так, эта
идея понятна, да? И последняя вещь, значит как не стоит писать операцию. То есть, как говорится,
концепция напиши чистый и хороший код, на видеокарте не работает. То есть представьте себе идею такую,
что вы сначала хотите скопировать все с хаста на девайс, потом выполнить ядро, а потом все массивы
скопировать с девайса на хост. Ну, логично. Сделайте так. Ну, смотрите, что происходит. Мы
копируем с хаста на девайс. Даже все хорошо. Дальше выполняем ядро. Тоже кажется все замечательно.
А потом оказывается интересный момент времени, что когда мы копируем все с девайса на хост,
у нас с вами осуществляется блокирующая операция. То есть мы должны дождаться,
пока у нас ядро закончит работу. И тут как раз написано, что сигналы между запусками ядр у нас
отложены. То есть как бы хотелось бы, чтобы у нас операция, которая идет ДН1, она как бы выполнила
сразу после КА1. Но из-за задержки, из-за задержки в очередях у нас получается,
что образуется некоторая задержка по времени. И в итоге сигнал о том, что у нас в очередь стоит
ДН1, будет ставиться только после того, как у нас идет копирование выполнения ядра КА3. То есть мы
будем дожидаться результатом исполнения ядра КА3. Упс, неприятный момент. Как это можно ускорить?
Помните порядок по программе. То есть желтые-желтые, зеленые к зеленым,
синий к синим. Вот, если они друг от друга не зависят. То есть стараемся как раз писать все
операции таким образом, чтобы они у нас с вами не выполнялись последним. Так, ага. Теперь выводы
давайте произнесем для этой части. Значит у нас с вами есть три очереди. Это device to host, host device
и очередь ядра. И concurrency есть и в GPU. То есть мы с вами поняли, что код можно ускорять и в режиме
concurrency. Значит сразу скажу, какие темы мы с вами не прошли по коде. Мы с вами не прошли работу
с тензорными ядрами. Пока новая история. Возможно, что с ней будет в какое-то время работа в будущем.
И мы не прошли всякие мало битовые операции, которые сейчас полезны для всяких больших языковых моделей.
Сейчас, грубо говоря, из-за того, что вот эти вот большие языковые модели а-ля чат ГПТ, они очень
большие и хотелось бы их помещать в оперативную память. Желательно в оперативной памяти видеокарты,
их пережимают. Естественно там возникает новый способ вычисления операций с массивами. То есть
там даже дошло до того, что в современных версиях видеокарты можно использовать специальный тип данных.
То есть есть float16, который позволяет вам двухбайтный float иметь, а есть bfloat16. Который
позволяет там решить проблему перемножения нескольких флотовых чисел, не учитывая, грубо
говоря, более аккуратно учитываем антису, которая появится в данных. Вот так. Это что касается
куды. Значит следующие разы уже начнется блок по параллельным вычислением, по распределенным
вычислением. То есть я по факту завершил блок по параллельным вычислением. Вот. И небольшая
прямула к тому, что будет. Значит мы будем с вами в курсе проходить историю, связанную с тем,
а что будет, если вы внезапно выходите в реальный мир из учебной лаборатории, так сказать. Мы пока
находились с вами в какой-то учебной лаборатории, и все условия у нас были идеальными. Мы были
заточены именно под ускорение программ. А вот следующие разы как раз мы уже пойдем в реальный
мир и поймем, а что же будет происходить, если внезапно у вас один из компьютеров вылетит из
строя. Или один из компьютеров перестанет принимать сообщения по сети. Просто он прологал. То есть
всякие разные кейсы бывают, и как раз вот эти кейсы уже будут рассматриваться. Так что следующая
лекция у нас будет про то, как устроены блокчные файловые системы распределенные. А после того
будет обработка поверх блокчных распределенных систем. Ладно, всем спасибо тогда и до встречи
на следующих лекциях, семинарах и так далее.

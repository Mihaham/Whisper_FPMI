всем доброго дня мы сегодня с вами продолжаем изучать наш курс и сегодня у
нас будет новая тема посвященная грамматика внезапно но давайте прежде чем мы с вами
начнем поймем что мы уже с вами прошли какие конструкции мы с вами уже прошли
да мы с вами построили минимальный пдк напомню что это именно полный
детерминированный конечный автомат минимизировать обычный автомат нельзя и
кстати по моему одна задачка которая будет на семинарах про это говорит да
там про теоретика множественная операция а мы с вами это поняли раз что мы с
вами построили минимальный пдк а во вторых мы с вами поняли что класс языков
распознаваемых автоматами достаточно мало пример языка можете назвать который
не распознается автоматами а вн и бв слова простой длины ну с ними будет
некоторая проблема да то есть проверка на простоту это вообще вещь которая
тяжело распознается всякими автоматными историями или около
автомата то есть для них нужно уже использовать машину тюринга вот поэтому
план на сегодняшней лекции во первых ввести понятие порождающий грамматик во
вторых построить иерархию порождающий грамматик и в третьих понять какие
грамматики и квалетный автоматом то есть взять построить новую конструкцию
таким образом чтобы эта новая конструкция была связана с предыдущей
конструкцией давайте поймем следующее прежде чем я веду определение что по
вашему мнению такое грамматика
да правила по которым мы с вами можем писать представьте себе мы пишем с
вами предложение предложение у нас из чего состоит так нет ну подлежащие
исказуемость синтактический разбор значит у нас есть подлежащие у нас
исказуемые значит что относится к подлежащему что привязывает собой
подлежащие обычно не не это часть речи которая скрывается красное яблоко
определение то есть у нас есть именная группа которая состоит обычно из
подлежащего и набора так сейчас я заступил как называется это
определение и дальше у нас есть глагольная группа которая имеет с собой
связки множество дополнений и обстоятельств да то есть мы с вами можем
сказать что у нас его предложение
sentence состоит из наунгрупп и вертгрупп что такой наунгрупп наунгрупп у нас
может представлять собой либо собственно сабжект
не не не я пока просто пишу о том как это выглядит на практике примеры просто
приводим значит это либо сабжект либо допустим сабжект и как
определение по-английски я забыл все время не определение которое не то
определение который который это подчеркивает волной да давайте
назовем и аджиптик либо у нас аджиптик может стоять перед сабжектом и вертгрупп
что такое вертгрупп это у нас либо предикейт
это этот предикат это на самом деле всказуемый переводится английского языка
либо смотрите мы можем записывать правила так что у нас из этого вводится
либо предикейт либо вводится предикейт плюс отверт предикейт
ну или еще какие-нибудь другие правила да и смотрите дальше что у нас с вами
происходит а дальше нам нужно спуститься до части речи то есть попытаться
раскрутить эти правила до тех моментов пока мы дойдем до части речи когда мы
доходим уже до 4 речи мы с вами уже спускаемся в лексический анализ то есть
вот это у нас синтоксический анализ мы берем раскладываем предложение а есть у
нас лексический анализ и как вы думаете какими конструкциями шлятся лексический
анализ то есть определение части речи там определение склонений а нет какими
конструкциями
отлично а как задать множество слов имеющие окончание а я регулярными
выражениями или автоматами то есть на уровне лексического анализа мы с вами
работаем с автоматами регулярными выражениями здесь же мы будем работать с
такими объектами как грамматика я просто сейчас веду определение и чтобы
было был пример который показывает как это работает и так давайте ведем
первое определение порождающие грамматики
это такое кортеже который будет состоять из следующих символов n и сигма
ps где все множество конечны сразу скажу а
s это элемент нашего множества значит n называется не терминалы это множество
сигма как вы думаете что такое сигма алфавит он тоже конечный п кстати
спрошу здесь видно на записи все отлично не зря свет выключали это
под множество n объединить сигма с плюсом умножить на n объединить сигма со
звездой это правило так еще забыл сказать что в данном случае сигма и n
пересекаются по пустому множеству то есть они не пересекаются не терминалы
терминал это символов которых мы с вами можем заканчивать то есть обычно
алфавита как раз называются терминал символы алфавита называются терминальными
потому что их мы дальше раскрыть никак не сможем вот такое определение значит
п это правила которые будут записываться интересным образом на самом деле мы видим
с вами как они уже записываются то есть у нас а вот эта часть это левая часть из
которой мы будем что-то выводить а вот эта часть это правая часть в котором мы
будем подставлять и четвертая это с это стартовый не терминал
а вот это а все понял
все все я не буду писать
давайте в качестве примера поймем на вот этой штуке что есть что
давайте начнем с s как вы думаете чему здесь s равно
с чего мы начинаем разбор sentence да так п
стоится из всех правил так n
начинаем перечислять их
виртгрупп сабжик течекте
чему равняется алфавит вот этот самый интересный вопрос
нет n и сигма не должны пересекаться нет
этого уже раскрываете на самом деле мы можем сказать что в нашем случае африт это части
речи это подлежать это существительное это глагол это
препозицион и так далее потому что части речи мы уже будем обнаруживать
лексическим анализом на такой пример может быть сразу не тривиальный но зато
примеры с реальной жизнью в плане
ну смотрите правила имеют вид такой из альфа выводится бета да любой может иметь
вид на самом деле вы можете сказать что из аб допустим выводится какой-нибудь с
где а это у нас символ афавита об и не терминал да да да да знаете почему так
сделано немного забегу наперед подумайте как в этих терминах будет работать машина
тюринга что такое машина тюринга да ну нет как она работает у нас смотрите мы стоим в
каком-то состоянии куеты да у нас есть текущий символ который мы с вами читаем дальше у нас
есть какие действия да ну вот да и вы можете сделать следующее попробовать сделать снэпшот
состояние слева состояние до которого вас было то есть вы стояли в состоянии куета здесь у
нас был записан какой-то символ икс да и еще какие-то слова слева или справа и потом вы
указываете еще какое-то действие л я конечно не не очень тут честно пишу да потому что это можно
было сделать в одно слово и дальше вы можете перевести это все вы какое-то состояние кутжита
допустим если у нас состояние было слева то мы написали слово дубль там какой-то символ
дубль в справа оставили символ икс и мы находимся здесь то есть понимаете я к чему клоню что вот
такой вот самый общий способ позволяет задать машину тюринга слева это состояние до перехода
справа это состояние после перехода просто тут на самом деле последний момент который необходимо
обсудить это принимающее состояние в машине тюринга на котором мы программу заканчиваем тогда
нам нужно убрать наш символ и выдать пустое слово заменить вон пустое слово чтобы у нас
осталось выходное слово наши машины тюринга но нам нужно убрать грубо говоря принимающее состояние
то есть мы его просто газим добавляем такое правило вот вот я вкратце рассказал понятно что тут
нужны подробные доказательства того почему машина тюринга можно задать порождающей грамматикой
вот то есть это самый широкий класс который у нас существует а да кстати
да я убрал сигнусы звездой смотрите тут важно что да бак в определении блин вот так что слева
нам нужно хотя бы один порождающий символ один не терминальный символ а то смысл что-то раскрывать
да да да да да да да но собственно машине тюринга вот у нас куит и являются не терминалами
ну никто не запрещает
да но если у нас остались только терминальные символы в нашем слове ну все закончили разбор
нет вот в таком определении которые да но здесь уже нет
да да
не не нет это означает что просто правила давайте обсудим что правила вот слева альфа
справа бета а в альфе у нас не могут быть только символа алфавита то есть там как
минимум один должен быть не терминальный символ так хорошо но это самый общий класс грамматик
собственно правила грамматики задается таким образом вот пример нашей грамматики кстати
эта грамматика такая более приятная почему вот ту грамматика ту часть грамматики
которые мы с вами записали она более приятная в сравнению с той которую мы с вами обсудили для
машины тюринга да переходы чуть приятней смотрите у нас переходы всегда идут изне
терминала вы какую-то последность терминалов и не терминалов да да вот и вот как раз больше
часть этой темы мы будем изучать грамматики именно такого вида мы их дальше введем давайте
пример некоторый предположим что у нас n это саб а сигма это а и б и п будет состоять из вот таких
правил так давайте я перепишу этот пример это старт так bb и вот она порождающая грамматика
как понять что оно выводит ну неявный алгоритм такой давайте мы будем выводить какое-то слово
если у нас встречается какая-то последность символов мы ее можем заменить на ту которая
находится справа наша цель опять же распознавание какого-то слова поэтому наша цель именно получить
какое-то слово на выходе конкретно да смотрите давайте попробуем вывести какое-то слово
св это б дальше попробуем а раскрыть а так что получается да дальше у нас неоднозначность
уникает которую можно тут разрешать разными способами ну давайте кажется что здесь можно
разобрать bb и дальше из bb вывести а что правда o bb b b что получается дальше
нет смотрите
во то есть смотрите что мы сделали мы заменили вот эти 2 bb на bb дальше вот эти
две внутренних bb заменили на bb а дальше вот это заменили на
да мы с вами вывели слово а а б но да ну могли бы зайти в тупик да да да да то есть
видите у нас в этой грамматике есть какая-то неоднозначность причем не однозначность
которая в тупик приводит вот это проблема но в целом в машине тюринга кажется тупик тоже можно
зайти вот значит теперь вопрос что нам необходимо для того чтобы вот такую штуку писать
мне но это понятно дело не но можно было фломастер и маркер притащить тоже пошло бы
не как математические объекты чего нам не хватает какого определения да нам не хватает
определение штопора ничего давайте вводить штопор да я не помню какой пример а вот он
собственно отношение штопора это снова же наименьшее рефлексивное транзитивное
отношение которое определяется для нашей грамматики наименьшее рефлексивное транзитивное
отношение такое что для любого фи и пси принадлежащее n объединить сигма со звездой
для любого правила альфа бета принадлежащего нашей грамматики будет следовать что мы можем
написать вот такой вывод то есть по факту у нас есть лево слово фи есть справа слово
пси мы левую часть правила заменяем на правую часть правила и это именно рефлексивное транзитивное
отношение то есть у нас получается фи альфа пси всегда будет выводить фи альфа пси а если у
нас есть фи альфа пси выводит фи бета пси и допустим фи бета пси выводит фи гамма пси пси то у
нас получается фи альфа пси будет выводить фи гамма пси да что не знаю может потому что я не знаю
про него хорошо буду использовать ну это теховские буковки насколько я правильно понимаю да ну вот хорошо
буду знать я думаю что это только для epsilon работает вот а кстати вот тут баг вот смотрите
пример из а мы можем вывести аб и дальше за один шаг мы можем вывести аб ну и дальше мы можем
с вами вывести аб из этого а а из этого всего дела то есть понятно зачем нам нужно это отношение
хорошо теперь давайте подумаем над следующим как определить что слово лежит в языке задаваемым
грамматикой это конечно гениально но как то из стартового состояния можно вывести это слово вот
то есть мы покойно вам выводим вот мы говорим что слово вводим в грамматике если у нас из с выводится
пустое нашу текущее слово или мы говорим что язык задаваемый грамматика это множество всех
слов которые выводится из стартового символа да то есть логичное определение которого у нас
теперь должно быть пример выводимости который мы с вами тоже уже успели посмотреть да конечно
логично да все хорошо а теперь смотрите пример выводимости достаточно простой ну
который мы с вами уже разобрали то есть смотрите мы берем какую-то часть и заменяем ее на правую
часть то есть допустим здесь bb заменяем на bb так у нас ответ совпал это знаете как это вы
листаете потом в ответы и смотрите совпало оно или нет да не но я на доске допустил бак ну возможно
что я а не ну тут сложно наверное было прийти к неоднозначно глобально конечно же да ну
понятно что у нас языки вот так про штопор понятно так а теперь знакомимся с частными
случаями порождающих грамматик значит для этого есть иерархия хомского значит хомский один из
основоположников вообще компьютерной лигуистики и как раз ему принадлежит так сказать концепция
грамматик который мы с вами изучаем нам мы будем проходить нормальную форму хомского и другие
тоже вещи тоже значит давайте разграничим классы грамматик по виду правил значит наверху у нас
будут с вами порождающие здесь мы разрешаем любые правила раз дальше у нас возникают
контекстно зависимые грамматики у нас правила будут иметь
си альфа где альфа не пустое слово вот это важно то есть почему грамматика называется
контекстно зависимый потому что у нее слева есть контекст вывода и справа есть какой-то
конкретный контекст вывода это мы ограничиваем свод правил которые у нас существует так
следующий да да фиксированный контекст то есть у нас правила имеют фиксированный контекст то
есть у нас слева что-то есть есть право что-то да и посередине мы можем изменять не терминал
на какую-то правую часть да а из n то есть мы раскрываем один уже конкретный не терминал
а последовательность из терминалов не терминал давайте сразу скажу что важно большими буквами
обозначаем не терминалы маленькими буквами обозначаем терминалы а греческими буквами мы
обозначаем произвольную по сенсе терминалов и не терминалов если мы этого если мы не оговариваем
иное да большая с это старт у кроме тех случаев которые не ограничили следующий вид грамматика это
контекстно свободной грамматики как вы думаете что надо убрать здесь фи и все убрать надо только
важный момент здесь альфа может быть уже пустым а не нет каждый из этих классов будет более узок
давайте еще раз грамматика называется контекстно зависимый если все ее правила
имеет вот такой вид контекст свободной грамматика называется контекстно зависимый если все
ее правила имеет вот такой вид где вот это да одна буква и остался последний класс это
праволинейные грамматики или их еще на могут называть леволинейными грамматики
это либо а выводит в в либо у нас а выводит в дубль в аб это не терминал
вот это самый узкий класс да мы еще и альфа ограничили
вот да тут надо было это сразу сразу ставить презентацию что мы будем
возначать терминалами большими буквами да а потом обозначать любые греческие буквы мы
будем означать последствия терминалов и не терминалов так здесь на слайдах есть
примеры смотрите для кз грамматики пример смотрите у нас возможно правила от аб выводит
сб при этом левый контекст это пустое слово а правый контекст это слово б да то есть у нас
получается мы вместо вместо psi подставляем b а альфа в нашем случае это c значит следующий
о господи это не латинская буква это вообще да да да да да собственно есть такое
левый контекст и р hag context это пустые слова здесь левый контекст а то я а правый контекст
р hag context точнее это у нас пустое слово да то есть видите и мы можем прайт типа того и допустим
у нас есть еще слово ец ф его это еды и а ف а тогда у нас получается левый контекст это
а равый контекст это f. Внутри из c мы выводим de. Пример ks грамматики, скажите, вот так вот выводить проще.
Правда, мы можем вывести сильно меньше слов. Пример права линейной грамматики. Вот он, вот такой вот.
Да, кстати, давайте раз мы тут поговорим про грамматики, поймем, почему класс контекста свободных грамматик шире, чем класс автоматных языков.
Как распознать a в n и b в n при помощи...
Нет, не обязательно. Пример правильной скобочной последовательности.
А, или наоборот, что...
Нет, а если это буквы нету в левых частях правил?
Да.
Да.
Ну, слушайте...
Не, но это уже может привести к неоднозначности.
Это да.
Да.
Но при этом мы в итоге можем...
Нет, не всегда будет однозначный разбор.
Ну, пример тот же самый PSP. Там можно задать грамматику таким образом, что всегда слово будет получено ровно одним способом.
А можно задать так, что если вы посмотрите на вывод, который у вас получается, то там будет другой вывод.
А, ну, в этом плане надо подумать. Вполне возможно, что да.
Ну да, для контекста свободных так точно.
Не, ну там контекст может дополнительный появиться.
Так, давайте разберем пример АВН и БВН.
Какая грамматика подойдет нам?
С АСБ и С выводит Эпсилон. Все.
Вы можете вывести любое слово вида АВН и БВН.
Да здрасте. Я сейчас докажу теорему, которая вас опровергнет.
Все равно. Утверждение есть, что праволинейные грамматики точно те же самые, что конечные автоматы.
АВН и БВН – это автоматный язык у нас?
Нет.
Нет, так что не задать. Вот.
Вывод при этом будет такой.
АВН и БВН.
Обратно делаются индукции по количеству правил вывода.
Так. Иерархия Хомского. Вот. Теперь такая вот табличка. Что чему соответствует.
Значит, порождающие грамматики. Да, начнем с самого низа.
Праволинейные грамматики соответствуют недетерминированным конечным автоматам.
И это то, что мы сегодня будем доказывать.
Контекстно свободные автоматы эквивалентны МП-автоматам.
И эту конструкцию мы с вами будем разбирать.
Что такое МП-автомат? Это автомат, у которого сбоку приделан стэк.
Это МП-автомат, у которого, когда вы выводите какое-то слово, вы можете на стэк положить определенный набор символов.
Для того, что... А? МП. С магазинной памятью.
А?
А, новый термин появился.
Да.
Смотрите, сейчас будет еще более взрывной термин, когда я скажу, чему эквалентны контекстно зависимые грамматики.
Магазинный автомат-то интересный, а еще магазинный полуавтомат остался.
Котекста зависимой грамматики.
Как говорит научная литература, это ограничены недетерминированные машины тюринга.
Грубо говоря, чтобы вы понимали, это по факту машины тюринга, в которых можно делать произвольные переходы.
То есть вы из одного состояния можете перейти не в одно состояние, а в набор состояния.
Но важно, что у вас сверху есть консанта С, сверху по дополнительной памяти, которую вы можете использовать.
Да, нельзя слишком далеко уходить направо и налево.
И последние, это порождающие грамматики, они эквалентны машины тюринга.
С точки зрения задачи распознавания слов.
То есть каждый из этих классов более узок, чем предыдущий.
И давайте начнем как раз сегодня с доказательства первого факта.
Так, давайте я этот слайд остановлю. Есть ли вопросы?
Разве ограниченные ММЦ не эквалентны конечным и автоматным?
Не совсем.
А в чем там проблема?
В чем там проблема? Проблема там в том, что нужно контекст учитывать.
Ну то есть у вас есть состояние, вам надо посмотреть, что находится слева, что находится справа.
А там не получится это состояние?
Нет, не получится их, там может быть бесконечно много.
То есть по факту...
А контекстов бесконечно много, но при этом памяти вообще ограничены.
Да, да.
Поэтому автомат может не быть конечным, а бесконечные автоматы не гарантируют.
Ну да, типа того.
Так, по вот этому понятно?
Чего, вот это?
Значит смотрите, мы будем доказывать эквалентность первых двух строк.
Третье, четвертое для общего развития.
Возможно, на семинаре рассмотрите задачу, как построить порождающую грамматику
для распознавания каких-то слов, которые вот из машины тюринга.
Или проэмулируйте машину тюринга.
А зачем нам недетерминированность?
Недетерминированность нам нужна чисто по той причине, что с ней проще возиться.
То есть объекты, которые нам нужны, их проще строить.
Ну я думаю, что если подумать, то...
Да.
Ну, слушайте, тут тонкий момент, потому что с ограниченностью,
потому что это и прокатит, а с неограниченностью не прокатит.
Потому что машины тюринга задают класс P, допустим, если мы говорим за время,
а недетерминированные машины тюринга задают класс NP.
Ну, с точки зрения симптотики алгоритмов.
Да, любые машины тюринга, которые у вас определялись на курсе матлога.
Чего было, конечно.
Ну, не, качество состояния машины тюринга, конечно, будет конечным.
А, бесконечно...
В ограниченной машине у тебя конечная лента.
А просто в машине у тебя лента очевидная.
Так, чего переходим к доказательству следующего факта?
Следующая теорема.
Множество автоматных языков равно множеству языков, задаваемых праволинейными грамматиками.
Вот этот факт мы сегодня с вами будем доказывать.
Значит, сразу скажу, что мы с вами рассмотрим конструкции, посмотрим, как они работают,
а дальше, а дальше дело рук индукции, то есть к аккуратному выводу.
Значит, теорема.
Автоматный.
Тогда и только тогда, когда существует G праволинейная грамматика, такая, что L равняется LJ.
То есть наша идея какая?
Для каждой грамматики построить праволинейный автомат,
ой, наоборот, для каждого автомата построить праволинейную грамматику и для каждой грамматики построить автомат.
Собственно, идея, которая состоит в доказательстве этого факта, что состояние в автомате,
это следующее, что это не терминалы в грамматике плюс сток, плюс стоковая вершина.
Посмотрите, пожалуйста, вот внимательно на вот эти правила.
Из A выводят слово W и B.
Как вы думаете, что в терминах автоматов?
Мы по слову W перешли из A в B.
Да, смотрите, мы по слову W перешли из A в B.
А вот это?
Перешли в терминал по слову W.
Да, перешли в терминальное состояние, которое является ровным одним по слову W.
То есть мы просто каждой вершинице поставим, мы каждый не терминальный вершинец поставим какой-то символ?
Да, именно так.
То есть доказательство достаточно простое.
Да, давайте сначала докажем, получается, и справа влево.
Значит, построим автомат.
Значит, что у нас будет?
Это будет множество стоп.
Сейчас подумаю.
Тут баг.
Да, тут баг.
Найдите баг в презентации.
Да, N.
N объединяет с QF, значит, сигма, дельта, множество стартовое состояние и множество завершающее состояние.
Как определяются переходы?
Это у нас переход из A по слову W в слово W, если у нас было правило A выводит W в.
И второе правило, из A по слову W мы переходим в состояние QF, если у нас было правило в грамматике A выводит W.
Вот такая идея.
Значит, что нам нужно доказать теперь?
Для этого нам нужно доказать следующий факт.
Давайте будем доказывать более общий факт.
A в грамматике G будет выводить слово W тогда и только тогда, когда мы с вами...
Что получается?
Из A по W выводим в нашем автомате пару B.
В конфигурациях нашего автомата.
Это 1.
И 2.
Вот такой.
Логично.
Давайте разберем, наверное, следующий переход в качестве упражнения.
Мы будем доказывать переход с вами излево вправо.
Почему именно переход излево вправо?
Потому что хочется показать новый тип индукции.
Индукция будет подлиннее вывода в грамматике.
Справа налево. Как вы думаете, индукция будет почему?
Нет.
По длине вывода в автомате.
То есть индукция справа налево по длине вывода в автомате, слева направо индукция по длине вывода в грамматике.
У нас пока нет никаких...
Мы здесь вообще берем произвольный...
Мы же из грамматики автомат хотим построить сейчас.
Поэтому давайте сделаем так.
Я сейчас докажу факт слева направо, собственно, вывод.
А пункт справа налево делается похожим способом.
Просто это куча выводов буковок.
Этот пункт мы пропускаем.
Здесь база индукции делается аккуратно.
Говорю, написать это надо.
То есть база индукции, собственно, за ноль шагов что мы можем вывести.
Дальше посмотрим на последний переход, который здесь происходит.
На последнем переходе мы получаем с вами индукционный переход.
Или второй пункт, когда мы вводим эпсилон.
То есть здесь чисто написание индукции.
Теперь давайте мы подумаем с вами про вот эту вещь.
Длине вывода.
Давайте я буду написать.
По длине вывода в грамматике.
Значит, база.
Да, да. Именно так.
Да, он нам нужен только в доказательстве ЛЕМА.
Так, смотрите.
Значит, с другой стороны, индукция под длине вывода в грамматике.
База.
За сколько шагов мы можем что-то вывести?
За ноль.
Тогда у нас работает верхнее правило.
Мы в грамматике G за ноль шагов выводим слово, символ WB.
Из этого что следует?
Из этого следует, что W у нас пустое слово.
А B равняется A.
Ну и тогда мы можем с вами вывести, что из A эпсилон действительно выводится B эпсилон.
За ноль.
Мы никуда не перешли.
Нет, мы как бы перешли как бы по дыде.
Нет, мы остались на том же самом месте именно.
Если мы сделали вот так, мы и остались на дыде.
Ну, это называется переход по дыде.
Нет.
У нас нет дыди.
Нет, мы всегда говорили, что у нас вот это вот, смотрите, это рефлексивно-трандитивное отношение.
То есть в нем может быть переход за ноль шагов.
То есть мы могли отсюда-сюда попасть за ноль шагов.
Но просто это означает, что левая часть эквалентна правой.
То есть является то же самое, что и правая.
Так.
И единственный момент, который нужно проверить, это K равная единица.
Вот для вот этого случая.
Второй случай K равная единица.
Смотрите.
Из A за один шаг выявилось слово W.
Из этого следует, что A, W принадлежит нашим правилам грамматики.
Ну а из этого следует, что?
У нас что?
Следовательно, у нас из A по слову W мы так могли дойти до слова, до состояния QF, Epson.
Просто по той причине, что у нас есть вот этот вот факт.
Да, то есть что у нас есть вот это правило, по которому мы могли перейти.
Ну давайте, да, тоже разберем, тоже несложно.
Да, согласен.
Потому что индукционный переход будет одновременный.
Так, смотрите, значит, что у нас получается?
Ну, для вот этого факта.
То есть нам нужно будет вывод относительно этой штуки колдовать.
Ну, на самом деле, вот для первого правила тоже можно не доказывать.
Потому что он будет следовать из индукционного перехода.
Следовательно, у нас здесь существует правило.
Принадлежит дельта. Из A, W мы выводим B, Epson.
Ну, здесь ничего сложного нет.
Так.
А, ну тут даже, да, тут говорится, что даже переход индукции можно использовать.
Да, что это даже индукционный переход.
Ну что, давайте смотреть, что мы можем с вами сделать.
Ну давайте рассмотрим последний переход.
Да, переход.
Значит, давайте рассмотрим последнее правило.
Последний вывод.
Шаг.
Да.
Нет.
Ну да.
Ну можно, да, согласен.
Ну да, согласен, хорошо.
Ну да, давайте посмотрим, что мы дошли до какого-то терминала УС,
и дальше за один шаг в грамматике мы раскрыли УВБ.
Ну тогда смотрите, что у нас получается.
У нас получается интересная вещь.
Что по предположению индукции у нас с вами получается,
есть вывод из АУВЦ, да,
а вот отсюда у нас из правил грамматики будет следовать,
что из СВ мы выводим пару В,
ну и осталось сделать техники.
Да, АУВЦ мы можем съесть, получить СВ,
съедаем УСа входа, и дальше оставляем пару В.
Все, победили.
Второй пункт будем доказывать?
То же самое.
Поэтому здесь будет четыре одинаковых индукционных перехода,
в каждом из которых еще два пункта.
Честно, у меня была одна статья на матпраке по формальным языкам,
там будет нормальная форма, мы сделали обобщение,
там, по-моему, доказательства было пункта,
оно состояло в разборе семи случаев в одну сторону индукции,
и семи случаев в другую сторону индукции.
Итак, смотрите, допустим, мы дошли до такого правила,
за один шаг, но тогда отсюда у нас, по предположению индукции,
вот как раз нам нужна цепочка, вот та вот сверху,
по предположению индукции.
А вторая цепочка у нас получается из С по W, мы уходим в QF епсилон.
И в итоге из А у В мы получаем переход СВ
в QF епсилон.
Все, доказали переход в другую сторону.
То есть мы с вами доказали индукционный переход.
Так, теперь как доказать, имея вот этот вот факт
и имея вот эту вот лему, да, давайте я напишу специально
вот для тех, кто смотрит это,
в другую сторону смотри презентацию.
Нет.
Мы доказали здесь переход слева направо.
Нет, нет, нет.
Смотрите, как устроена будет процедура экзамена,
немножко поговорим, да, мы, значит, будем спрашивать
именно идеи доказательств, какие именно факты нужно доказать.
А дальше, если мы хотим уточнить всю эту историю,
мы попросим аккуратненько провести индукцию.
Причем все факты, которые необходимы для проведения индукции, мы дадим.
Нет, нет, нет. Ну то есть наша цель не зазубрить эти доказательства,
а научиться их понимать.
Ну они тупые, понимать нечего.
Там надо просто писать.
Можно просто говорить в языке графов?
Ну на полуформальном языке графов можно говорить.
То есть в целом можно объяснить по-настоящему,
как мы можем это сделать.
Да, да, да, да.
Отлично.
Нет, так не прокатит.
Ладно, давайте лирику в сторону докажем,
имея вот этот факт и имея вот эти две ремы,
то, что у нас есть, в общем-то,
и есть, в общем-то, и есть, в общем-то,
то, что у нас язык с задаваемого автомата
принадлежит языку с задаваемой грамматикой.
Итак, предположим, что у нас В лежит в языке с задаваемой грамматикой.
Это, верно, когда?
Помогайте.
А это и когда?
Выводимо qf эпсилон. qf у нас является единственным завершающим
состоянием. Все, все время осталось. Хорошо, а теперь вопрос, то есть понятно, как это
в одну сторону проходят? Нет, идея в чем? То есть мы берем стоковую вершину и делаем
переходы прямо как они написаны. Давайте подумаем, как в обратную сторону делать.
Да, по автомату грамматику. Нет, можно одно стоковое состояние оставить и вернуть,
доказать, что ровно таким же, как оно было. Да, каждая вершина это не терминал. Михаил
рассказал интересную идею. Чуть-чуть сложнее. По факту идея может так. Строим с одним завершающим
состоянием. И дальше, если у нас есть переход по v, то мы его превращаем в правило a вводит в b.
А если у нас с вами есть завершающее состояние q, то как его породить? Да, q эпсилон. На самом деле
можно просто не делать одно завершающее состояние, а просто для каждого завершающего состояния это написать.
Вот так. Спасибо. Вот. И опять же, просто аккуратно это надо написать.
Давайте посмотрим, как это аккуратно пишется.
Кстати, пример. Опять же, на презентациях есть примеры. Вот такой автомат получается,
допустим, для вот этой грамматики. Вот это? Это нужно было доказать. Смотрите, мы по грамматике
построили автомат. И нам нужно доказать, что эти языки, которые задают грамматику и задают автомат,
они совпадают. Мы этого пока не делали. То есть мы доказали вспомогать лему, которая нам помогла,
и дальше при помощи этой леммы уже доказали факт, что языки совпадают. Нет, ну смотрите,
здесь история такая, что здесь мы по грамматике построили, у нас была произвольная грамматика,
мы по ней построили автомат. А тут мы с вами будем делать следующее, что если у нас есть какой-то
автомат, как его в грамматику превратить? Да, то есть смотрите, здесь мы доказали,
что из грамматики существует автомат и показали. А теперь нам нужно делать обратно,
что из-за автомата можно построить грамматику. Вот, это пример. Смотрите, идея такая, не терминалы
в грамматике, это состояние в автомате. Ну и тут говорится, что добавить Epsilon переход. То есть
вот оно формальное определение, которое я сейчас нарисовал. То есть мы делаем вот такие переходы.
И тогда что нам нужно сделать? Ну, собственно, доказать еще раз тот же самый факт,
который нам нужен. Вопрос, хотим ли мы на это тратить время? Нет, не сильно фатально. То есть
вторая лемма, которая у нас будет, это получается... Ну да, тут лемма на самом деле будет ровно той же
самой. Вот, то есть опять же, идея какая, что если мы дошли до завершающего состояния,
тогда мы вводим слово. Если мы с вами не дошли до завершающего состояния, то мы вводим его с правым
символом. Опять же, смотрите, сколько тут замечательных пунктов, которые есть. Давайте
я их пропущу. Если вам, допустим, возникнут вопросы в доказательствах вот этого факта,
я просто отдельно сяду и запишу видео для этого всего. Договорились? Да,
в общем, аккуратненько делается вывод. Тут видите, сколько слайдов. В принадлежит языку,
сдаваемой грамматикой. Значит, если у нас имеется этот факт, то существует завершающее
состояние, которое это распознает. Ну а раз существует завершающее состояние, которое это распознает,
то вот у нас есть вывод, который у нас находится справа вверху, что из Q1 вводится слово W. Вот,
Q0 вводится слово W. А это значит, что слово лежит в языке, сдаваемой данной грамматикой. То есть
это дело техники, а не дело измышлений философских, которые у нас есть на это дело. Потому что,
я помню, всегда доказательства идут достаточно сложно. Главная идея, которая в этом доказательстве
есть, что, оказывается, можно проводить индукцию по длине вывода в грамматике. Все. И что, если у
нас есть индукция по длине вывода в грамматике, то мы можем раскрывать либо первое правило,
либо последнее. Мозги на этом закончились. Точнее измышления в мозгах. Вот тоже пример,
который нам это позволяет делать. Вот они все переходы, и дополнительно для завершающего
состояния мы добавляем переходы из Q0 в эпсилон и из Qs в эпсилон. Вот такая вот картинка. Давайте
я зафиксирую ее на минуту буквально. Зафиксировали картинку, чтобы она у нас в памяти зашла. Хорошо,
двигаемся дальше. То есть мы с вами поняли, что у нас классы языков, задаваемых автоматами и
праволинейными грамматиками, это одно и то же. Вопрос подвохом. Для леволинейных будет то же самое?
Да, просто там правила немного имеют другой вид. Если у нас праволинейных правила имеют вид вот такой вот.
Ну да. Хорошо, давайте двигаться дальше тогда. Теперь наконец-то мы переходим к контексте
свободного грамматика. И здесь уже можно делать некоторые новые определения. Определение работает
только для контекста свободного грамматика, подчеркну. Значит, на самом деле, вот если внимательно
посмотреть грамматику какую-нибудь, то мы вывод можем представить в виде дерева. Вот этого вот
интересного слова. То есть смотрите, что мы говорим. Мы можем последовательность выводов,
символов сделать так, что давайте зафиксируем вершину нашего дерева и не терминал,
попытаемся раскрыть как некоторое под дерево. То есть смотрите, тут есть цепочка выводов.
Мы рассмотрим с вами грамматику АСАА. Дальше, что там, из А выводят СА, да и С выводят АПСУ.
Значит, мы можем раскрывать это все. Допустим, мы из С выводим АСАА. Давайте нарисуем вершину дерева.
Да. Какие могут быть?
Ну, бывает.
Вот. И дальше тут можно это все дело раскрывать, раскрывать, раскрывать до тех пор, пока мы не
получим наше слово. А почему направленное цикли? Почему? Для конкретного слова это будет дерево.
Для конкретного слова это будет дерево. Просто это дерево можно будет получить несколькими способами.
Да, мы всегда... Да, конечно же. И здесь важно... Вот, смотрите, вот такое замечательное дерево получается.
То есть С раскрывается ВСА, и С раскрывается А. Вот. И дальше дается следующее определение. Дерево вывода.
Смотрите, дерево это просто называется в нашей эфемерной голове. На самом деле, если мы говорим строго математически,
это последовательность символов, каждый из элементов которых получается заменой одного символа из множества нетерминалов на правую часть вывода грамматики.
То есть это цепочка раскрытия наших штопоров. Заменили один нетерминал на последовательность терминалов нетерминалов.
Просто для контекста свободной грамматики мы можем это явно представить в виде дерева. И этим мы с вами будем пользоваться.
Я подчеркну этот момент, потому что на экзамене обычно говорят, ну дерево вывода это дерево. Но это неправда.
Это очень сильная абстракция, которую мы подгоняем, просто чтобы нам было удобно.
Да, именно так.
Я согласен, я сам биоинформатик, поэтому я скажу это. И поэтому, чтобы у нас всегда было понимание, как мы раскрываем наши правила, это будет важно для парсеров.
Да?
Да, бамбук. Просто мы этот бамбук можем представить в виде вот этого.
Нет.
Взяли какой-то конкретный...
Для контекста свободной грамматики это дерево, а для каких-то странах грамматика это дерево.
Ну да, для каких-то грамматика он вообще может быть отциклическим графом, каким-то сложным.
Вы это имеете в виду?
Нет, я имею в виду, смотрите, пример.
Давайте рассмотрим пример вот такой грамматики. Asb и sv.
Собственно, как выводить слово abab?
Значит, для вывода у нас получается, смотрите, если мы пишем цепочку выводов, то это будет asb, aasbb, aabb, то есть вот это бамбук у нас.
Но если мы попробуем это представить в виде дерева красивого...
Так, смотрите, что у нас получается.
У нас получается следующее, смотрите, как это будет.
s.
Дальше мы это раскрываем.
Во.
Тут есть большой вопрос. Зачем это вообще делать?
Для практиков это надо.
Ну просто мы написали не меньше текста, чем линейные.
Да, но просто вот этот вот вывод будет намного сложнее воспринимать, чем посмотреть на вот эту картинку.
Ну да.
Встретились с математикой практик, называется.
Так, ладно.
Давайте я продолжу.
Значит, вообще...
Да, давайте в куларах обсудим.
Значит, смотрите, для того, чтобы зафиксировать порядок, существует такое понятие, как правосторонний и левосторонний вывод.
Что означает левосторонний вывод?
Это означает, что мы всегда заменяем самый левый не терминал.
Правосторонний вывод означает, что мы всегда заменяем самый правый не терминал.
С точки зрения визуала дерева, это никак не влияет.
Но с точки зрения последовательности вот этих вот символов, которые у нас, они будут... цепочка у нас будет меняться.
Нет.
Для одного и того же самого слова, визуал дерева не поменяется.
Ну, вы решаете просто, когда какую гроздь раскрыть.
Нет.
Для одного и того же самого слова, визуал дерева не поменяется.
Ну, вы решаете просто, когда какую гроздь раскрыть.
Итоговый результат будет точно такой же.
Да.
Образно.
Нет.
Допустим, у нас вот такая вот грамматика, и мы с вами хотим, не знаю, S раскрыть A.
Что у нас получается?
Да, ну смотрите, если мы визуал, нам все равно это надо будет раскрыть в A.
И вот это надо будет раскрыть в A.
Просто последствия действия у нас будут другие.
Да.
Ну, смотрите, что у нас получается.
В стороннем выводе у нас будет вывод A, S, S, B, A, S, A, B, A, A, A, B.
А в левостороннем выводе у нас будет A, S, S, B, A, A, S, B.
Да, я...
Да, да, да.
Тут уже другое.
Поэтому всегда, когда хотят проверить однозначность,
хотят понять, а какое именно дерево вывода мы фиксируем.
Последствия вывода фиксируем левостороннее или правостороннее.
И поэтому KS-грамматика называется однозначной,
если для каждого слова W существует ровно одно правостороннее дерево вывода.
Правда ли, что если мы объединем как ровно одно левостороннее, то будет...
Да, да.
Фиксировано.
Вот, и спрашивается, любое ли контекса свободный язык можно задать однозначной грамматикой.
Ответ нет.
Ответ нет, потому что существует вот такой вот замечательный язык.
Только это существенный неоднозначный язык.
То есть, для вот этого языка, A, V, N, B, V, M, T, C, K, T,
у нас либо N равняется M,
либо M равняется K.
Нельзя построить однозначную грамматику.
На самом деле, тяжело понять,
как определение однозначности реально согласует с деревьями.
Согласует с деревьями так.
Сейчас давайте пример тогда.
Ну что ж, все равно, когда фиксируем дерево ровно одностороннее,
однозначно фиксируем, откуда мы должны объединять его.
Так.
Пример.
Что это такое у нас?
Узнаете, товарищи?
Правильные скобочные последовательности?
Ну, да.
Ну, давайте какой-нибудь зададим неоднозначность.
Вы знаете, что правильные скобочные последовательности,
они неоднозначно разбираются.
Вот именно такой грамматикой.
Ну вот такой.
Вот для такой грамматики
с однозначностью уже будет
получше.
А так, и что?
А можем вывести
АСС
и отсюда
АСБ
и отсюда
АВ.
Вот, видите, у нас два визуально
дерева разных.
Даже несмотря на то, что
если мы здесь будем одновременно раскрывать
самый левый не терминал.
Не типа того.
Вот, поэтому, значит,
вот этот язык, он
является существенно неоднозначным.
Да.
Да.
Называется.
Так.
Хорошо.
И еще один факт,
который я быстренько хочу рассказать.
Вот совсем быстренько.
Давайте подумаем,
относительно каких операций
замкнуты контекст
свободной грамматики.
Утверждение, которое мы
хотим доказать сейчас, что контекст
свободной грамматики...
Почему на презентации язык
А, Н, Б, М, Ц, К
подсчитан как неоднозначный?
Потому что
это... кто-то...
кто-то...
Это...
Хотите проект-диплом...
Хотите проект-дипломной работы?
Написать фреймворк для юни-тестирования ладьих презентаций?
А как же вы еще не написали?
Ну, вот...
Не...
Покрытие, а вы презентации дипестируете.
Да...
Остаток. Утверждение,
что на самом деле тоже нет в презентации.
КС языки
замкнуты
относительно
первое объединение,
второе конкатинация,
а этого на слайдах нет.
Да, я просто вспомнил,
что обычно это в программе есть,
но на слайдах этого нет.
А контексты свободной языки
замкнуты относительно
первое объединение,
второе конкатинация,
третья итерация к линии.
Отверждение.
Мы это докажем за две минуты.
Так, давайте объединение начнем.
Языки.
Поехали.
Объединение.
То есть мы идем либо в одну, либо в другую.
Но не из этой или из этой.
Конкатинация.
Вначале будем в первую часть,
потом в вторую часть.
Лучше так.
Из этой?
Из этой.
Больше однозначности появляется.
Так С1 же будет в целом то же самое?
Ну в целом да.
С1 это изначально?
Да, просто эта грамматика будет более однозначно, так сказать.
Если...
А с точки зрения...
С точки зрения распознавания...
Да-да-да.
Так, что, мы успели за две минуты?
Время еще не вышло.
Ну все.
Мы справились за 1 минуту.
Все, отлично.
Ну все, давайте на этом закончим.
Тогда в следующий раз мы будем упрощать нашу грамматику
вот к такому вот замечательному виду.
Собственно, определение
вот этой панормальной формы мы дадим.
И сделаем алгоритм приведения
к этой нормальной форме.
То есть у нас презентация получилась в чечне?
Ну оно всегда так идет.
Всегда по-разному.
Все, спасибо.

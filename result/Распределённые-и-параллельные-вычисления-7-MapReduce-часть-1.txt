Мы с вами в прошлый раз начали большие данные, и даже у кого-то они уже были на семинарах.
Мы начали пока обсуждать только хранение больших данных.
Сегодня и еще следующие три лекции у нас будет обработка больших данных.
И перед тем как перейти к обработке, давайте вспомним про основные особенности вот этой файловой системы,
которая мы обсуждали. Она называется ходу distributed file system, или HDFS.
HDFS заточены под хранение больших файлов.
Они разбиваются на блоки, и эти блоки хранятся на большом количестве серверов.
Так как серверов много, то они будут чаще падать, поэтому система должна быть устойчива к отказам.
Поэтому каждый блок у нас имеет несколько дублей, так называемые реплики, которые хранятся на каких-то других машинах.
Если одна упадет, то у нас все равно реплики останутся.
Ну и HDFS ориентирован на подход write once read main, не просто потому что во время записей нам нужно записать все реплики на n datanode.
Это происходит долго, а чтение у нас происходит гораздо быстрее, потому что мы выкачиваем данные напрямую из ближайшей реплики.
Ну это что касается хранения.
Теперь нам надо бы как-то научиться делать вычисления поверх этой распределенной системы.
Первая компания, которая столкнулась с проблемой в вычислении больших данных, это, конечно же, Google.
И в 2004 году появилась статья, которая называется Google MapReduce.
Система Google MapReduce была закрытой, как и система Google File System.
Но на основе статей появилась, собственно, система Hadoop, и с ней мы и будем работать.
У вас будет по ней семинары, два семинара даже, у вас будет по ней домашка.
И как вообще обрабатываются данные в Hadoop?
Используется вот такой вот подход, который называется MapReduce.
Мы все время работаем с парами типа ключ начения, K1, V1.
И вот эти пары у нас проходят несколько стадий.
Первая стадия – это мап, когда данные обрабатываются поэлементно.
Вот у нас была пара, и получилось одна или несколько новых пар.
Это происходит на стадии вот этого мапера.
Дальше идет стадия сортировки, где у нас полученные пары сортируются и группируются по ключам.
И выдаются на выход уже вот такие пары.
То есть ключ и не одно значение, а целая группа значений для этого ключа.
После этого у нас происходит стадия reduce.
Это когда мы берем эту группу, что-нибудь с ней делаем, например, какую-то агрегацию.
Например, считаем сумму, считаем среднее.
И у нас получается уже новая пара.
Может попутно ключ поменяться, и будет новая пара.
Чтобы было понятнее, как работает мап и как работает reduce,
скажите, что будет в результате вот этих вот кусочков кода?
Где все?
По-моему, мап вернет массив 1 до 5.
А reduce просуммирует все элементы от 0 до 4.
Да, range 5 это от 0 до 4.
x плюс 1 это мы просто независимо взяли каждый элемент и прибавили значку.
А reduce посчитал сумму от 0 до 4, и вот что получилось.
Да, все правильно.
Ну и разберем первый пример, с которого обычно начинают изучения мап и reduce.
Это word count.
То есть задача подсчета частоты слов в тексте.
Вот у нас есть такое предложение.
В реальной жизни, конечно, текста у нас намного больше.
И мы хотим с помощью мап и reduce понять, сколько раз каждое слово встречается в тексте.
Давайте подумаем, что мы будем делать на каждой из этих стадий.
Вот так вы думаете.
Мап вернет превратить в пары слова единичка.
Да, все так.
То есть сначала мы вот этот текст разбиваем по словам, формируем вот эти пары, сортировка и сложение группы.
Теперь давайте посмотрим немного на код.
То есть вот пример нашего мапера.
Все тот же word count, но, например, мы хотим по какой-то конкретной книге, которая лежит в текст-технике, посчитать word count.
Что мы делаем?
Мы ее выводим обычным кэтом или эхо или как-нибудь еще.
Она у нас выводится в стандартный поток угода.
Мы стандартные кэты выводим в стандартный поток угода.
И мы выводим в стандартный поток угода.
Что мы делаем?
Мы ее выводим обычным кэтом или эхо или как-нибудь еще.
Она у нас выводится в стандартный поток угода.
Мы стандартный поток парсим.
И получаем слова типа слова единичка.
Это вот такой мапер.
Теперь редьюсер.
В редьюсере у нас уже имеются пары типа слова единичка.
То есть они тоже в виде строки, но вот эти пары мы их уже можем вычислить.
То есть мы можем получить кей-каунт.
Ну и дальше мы считаем каунт для каждого кей.
То есть смотрите, что мы сделаем.
Мы берем входные строчки.
Получаем из них кей-каунт.
Мы знаем, что у нас прошел сорт.
То есть вот эта вот стадия сорт у нас уже прошла.
И поэтому кей у нас всегда одинаковый.
То есть идут постоянные кей-1, кей-1, кей-1.
В какой-то момент начинается кей-2.
И когда у нас уже кей-2 начался, кей-1 уже больше нигде не встретится.
Поэтому все, что нам нужно, нам нужно отслеживать то, что кей поменялся.
Если кей поменялся, мы выводим результат для первого кей и считаем следующий.
Вот как это все вместе выполнить?
Пока нет никакого ходу, мы не используем никакой фреймборд.
Мы просто пишем код и его выполняем.
Вот у нас есть файлик.
Мы его выводим.
Мы применяем к нему маппер.
Потом сортируем.
Потом применяем редьюсер.
Ну и потом еще идет одна сортировка.
Но это уже для красоты.
То есть мапр редьюсер весь заканчивается вот здесь.
Это пока без всякого ходу, но просто как иллюстрация того, какие части соответствуют этим же частям в ходу.
Ну и понятно, что у такого подхода у нас не будет хватать памяти.
То есть если вот эта дата вдруг окажется большой, мы не сможем ей сделать кед.
Она не поместится в память.
И тогда придется думать что-нибудь другое.
Вот давайте посмотрим, что именно.
Вот это уже то, как мап редьюс реализован в ходу.
То есть мы помним, что у нас есть HDFS, что данные хранятся там разбитые на блоке.
И Hadoop, как фреймворк для вычислений, он стартует процессы мапперов и редьюсеров там же, где лежат данные.
То есть на каких нодах данные лежат, на тех нодах и стартует маппер.
Идет обработка.
Потом идет сортировка.
И на редьюсере у нас есть гарантия того, что на одну ноду, на один редьюсер у нас...
То есть если сюда попал ключ один на одну ноду, то больше его уже нигде не будет.
То есть мы вот эти группы по разным процессам дробить не можем.
Вот сейчас есть какие-нибудь вопросы по этой схеме?
Эти блоки все еще на нодах хранятся или уже на...
Эти блоки хранятся на нодах, да.
Маппер их считывает себе в оперативную память.
Но сами блоки хранятся на нодах.
И тут у нас сами ноды уже отменялись данными или нет, когда мод сортировали тут?
Вот тут немного по-другому происходит.
У нас вычитался блок.
Мы сделали для него мап и положили данные на диск.
То есть мы не вот этот блок уже не с ним работаем, а с вот результатом маппер.
И его мы сортируем и переносим на другие машинки, да.
Есть ли еще какие-нибудь вопросы по этой схеме?
Не оченьerve.
Я 1982 год был в disappeared's apartment.
Это как бы функция маппера, функция редьюсера.
Хорошо, если вопросов нет, тогда у меня к вам вопрос.
Вот здесь указана такая штука.
Блок не равен блоковым HDFS.
Блоки, которые изображены здесь, они даже называются
на самом деле не блоки, а сплиты, то есть их размер
немного отличается.
Зачем это нужно вообще ходуку, зачем нужны эти отличия?
Как вы думаете?
Возможно блоки, которые хранятся слишком большие,
с ними неудобно работать, например?
В целом, МАГР работает в специальной такой абстракции,
которая называется контейнер, мы про это еще будем говорить
в следующий раз.
И у этого контейнера памяти где-то там 1-2 гигабайта,
может быть и больше.
А один блок у нас небольшой, ну 60 мегабайт, например.
Поэтому вряд ли что проблема была связана с тем, что
блоки большие.
И к тому же я говорю, что блок примерно равен, то
есть он не в два раза меньше или в пять раз, а он почти
такой же.
Есть ли какие-нибудь идеи еще?
Хорошо, дело в том, что данные, которые подаются на вход
на продюсер, они могут быть совершенно разные, это
не только текст.
А это, например, могут быть видео.
И мы с вами видели в прошлый раз, как работает HDFS.
Он не смотрит, какие там файлы, какой формат.
Он просто их бьет с точностью до бита на равные части.
И эти части улетают на разные ноды.
Файл таким образом не корраптится, не портится,
потому что когда мы делаем HDFS dfs-g, вычитываем его,
у нас получается, что опять целиком файл, вот он целый собрался со всех кусков.
Все хорошо.
А здесь-то мы не делаем get, а у нас здесь вот эти блоки,
они идут на мапперы, обрабатываются независимо,
потом перемешиваются, потом опять обрабатываются.
То есть никогда они уже не встретятся, кроме как в конце джобы.
В конце вот этой задачи.
Поэтому получается, что если у нас есть видео, например,
вот какие-то видео-файлы, то может оказаться,
что часть видео попадет в первый блок в HDFS, часть во второй.
У нас будут незаконченные фрагменты видео,
мы их не сможем никак ни декодировать, ни прочитать,
и придется вот эти крайние кусочки просто выкинуть.
Чтоб такого не происходило, ходу действует более умно.
А именно вот так.
То есть представим, что у нас вот тут вообще важно не запутаться,
потому что вот это у нас блоки, эти кусочки 1, 2, 3, 4, 5,
вот это внизу блоки в HDFS, то есть вот эти кусочки маленькие,
это блоки, относящиеся к формату хранения данных,
какие бывают блокчные форматы хранения.
Например, какие-нибудь архивы.
Или какие-нибудь видео-файлы, опять же.
То есть здесь вот какой-то файл лежит.
Внизу блок-баунды это граница блока в HDFS,
а вверху это граница сплита.
Сплит это то, что берет себе на вход MapReduce.
То есть как происходит чтение?
Мы вычитываем блок.
Дальше видим, вычитываем блок из HDFS.
Дальше видим, что блок из HDFS мы прочитали,
но у нас не завершенный вот этот кусочек.
То есть мы ничего с ним сделать не сможем, мы его не прочитаем.
Поэтому мы еще идем в соседний блок
и дочитываем вот этот оставшийся кусочек.
Получается сплит чуть больше, а здесь будет сплит чуть меньше.
Вот эти границы блоков, например, границы между 5 и 6 или 6 и 7,
их Hadoop распознает сам.
Мы можем ему подсказать, как распознавать.
То есть в Hadoop есть много встроенных форматов данных.
Можно указать, например, формат VZIP или видео MP4 какой-нибудь.
Точно также можно указать разделитель.
То есть если мы имеем дело с текстом,
то можно, например, указать разделитель символ конца строки.
Тогда у нас вся строка пойдет или сюда, или сюда.
Мы ее не разрежем пополам.
Или указать разделителем символ пробела.
Вот эти блоки маленькие, это будут слова.
А сейчас какие-нибудь вопросы появились.
Вот там на предыдущем слайде у нас для K2 и K3 принялась функция G.
Почему разве не к каждому G применяется отдельно K2 и отдельно K3?
Да, по идее?
Да, да.
Сейчас я обновлю страницу.
Изменил повинтажку.
Хорошо, давайте тогда идем дальше.
И посмотрим, как работает MapReduce уже более технически.
Вот у нас есть InputSplit.
Это вот этот блок с поправкой на формат файла.
Этот блок мы вычитываем в МАП, и у нас тут происходит МАП.
То есть блок целиком попадает в МАП,
а дальше внутри МАПа он может разделять и вычитывать.
А дальше внутри МАПа он может разделяться на какие-то более мелкие структуры.
И по сути МАП у нас работает вот так.
Вот, можно сказать, что наш МАП это функция Run.
И в принципе в ходу-то она такая и есть функция Run.
Внутри нее у нас есть Setup.
Есть Cleanup.
Setup и Cleanup это такие парные функции,
одна из которых инициализирует всякие переменные в начале МАПа.
Какие-то файлы может открывать.
В общем, делает подготовку.
А Cleanup все, что было в рамках этой подготовки сделано,
удаляет, пишет на диске и так далее.
Или просто удаляет.
Вот между Setup и Cleanup у нас есть самое главное, а это МАП.
Он работает в цикле.
То есть пока у нас существуют, например, строчки или слова,
в зависимости от того, как мы этот блок разбили,
по какому разделить его,
пока оно существует, мы выполняем МАПы последовательно.
То есть получается, блоки мы разбили параллельно,
а внутри блока мы последовательно выполняем несколько раз МАП.
Ну и на самом деле Reduce устроен точно так же.
То есть на вход подается вот эта штука.
Как она получается, мы сейчас разберемся.
Ну то есть это тоже некий блок, он подается на вход,
а в этом блоке может быть несколько групп.
Вот таких вот K2, K3 может быть групп несколько.
Поэтому мы тоже делаем Setup, потом While,
и вместо МАПа у нас применяется Reduce.
Давайте посмотрим, что происходит здесь.
То есть мы прочитали МАПу, что там,
обработали, и результат МАПа записали в память.
Если памяти у нас не хватает,
то мы это можем слить на диск и писать в памяти еще раз.
В общем, в конечном итоге весь вот этот буфер,
весь результат МАПера, он улетит на диск.
Видите, тут написано, что он улетит на диск.
Видите, тут написано Partition, Sort and Spill to Disk.
Что такое partition? Мы разберемся через два слайда.
Что такое sort? Мы понимаем, что это идет сортировка внутри файла.
Но и Spill to Disk это как бы бэкап на диск.
То есть вот эти у нас бэкапы на диск остаются,
их может быть несколько.
Потом мы это все мержим в один файл,
и у него получаются вот такие вот группы.
Это K1, это K2, это K3.
И дальше K1, например, у нас летит на первый редьюсер,
K2 на второй редьюсер, K3 на третий, и так далее.
Вот согласно этой вот схеме.
Что-то прилетает на один редьюсер, что-то прилетает на разные.
То есть K1 прилетел сюда,
и K1 прилетел еще с других мапперов, которые на других нодах считались.
Вот у нас, например, K1, вот здесь у нас K2, другая группа.
И дальше, когда у нас группы собрались, мы делаем мерж.
То есть все кусочки, относящиеся к одной группе, мы сливаем в один файл.
Слили здесь, слили здесь, вот у нас две группы,
и мы идем работать с редьюсером.
Вот, то есть у нас отработал маппер.
Вот здесь мы заполнили данные.
Дальше они объединились вот в этом кусочке K1.
Ну и потом этот K1 улетел на редьюсер.
Скажите, по этой схеме какие-нибудь вопросы есть?
Нет?
Ну тогда идем дальше.
Вот я говорил, что есть слово partition.
Что это такое?
В ходу есть инструмент, который решает,
по какому именно принципу вот эти ключи будут улетать на редьюсеры.
Ведь может быть такое, что все ключи улетели на один редьюсер,
а остальные простаивают.
Или, например, как нам гарантировать вот это требование,
что K1 есть только здесь и больше чего нигде нету.
За это отвечает специальный элемент ходу.
Вот, вот я сейчас расскажу вам про группу.
Более подробно я про него расскажу в следующий раз,
которая называется partition.
И по умолчанию он делает следующее.
Он берет хэш от ключа, по которому мы разбиваем на группы,
и берет остаток отделения на количество редьюсеров.
Вот, на выходе у нас получается число от 0 до R,
и в зависимости от этого числа данная конкретная пара
пойдет на один или другой редьюсер.
Вот, теперь поговорим немного про термины.
То есть, в общем говоря, в одной программе,
в которой мы пишем на ходу,
чаще всего будет несколько mappreduce задач.
Вот у вас в домашке будет две задачи.
В первой задаче будет один mappreduce,
одна цепочка, один такт mapp и reduce.
Во второй задаче у кого-то будет два друг за другом,
нужно будет закодить у кого-то три даже.
Ну а в реальной жизни там десятки бывают этих job.
Вот, ну и давайте посмотрим на термины.
То есть, вот эта вся программа,
которая использует Hadoop, называется application.
Внутри application один вот этот такт mappreduce называется job.
То есть, вот это все, что мы видим вот тут на схеме,
это называется job.
Один вот этот голубой прямоугольничек mapp или reduce
называется task.
И у каждой task может быть несколько попыток.
Они называются attempts, но попытки так и называются.
Откуда может взяться несколько попыток?
Во-первых, из-за падений.
Из-за падений ноды, из-за падений сети.
То есть, если у нас в процессе вычисления нода упала,
то Hadoop автоматически, даже никак нас не используя,
он перенесет вычисление на другую ноду,
и они досчитаются там.
Это первый случай, когда может быть много попыток.
Второй случай, когда может быть много попыток,
это когда падает наш код.
Мы будем разбирать чуть позже, как писать код на Hadoop.
И может быть такое, ну, логично, что наш код падает.
В таком случае система не понимает, что наш код падает,
она думает, что проблема на ее стороне пытается чинить.
Как она пытается чинить?
Она пытается перезапускать этот упавший редьюсер еще раз.
То есть, код падает, система перезапускает.
Он опять падает, она еще раз перезапускает.
Так может быть много раз, в зависимости от настроек данного кластера.
У нас так будет происходить всего три раза.
Если за три раза система не смогла выполнить ваш код,
она сдается и говорит все.
Я выполнять ничего не буду, и все падает.
И третий случай, когда может быть много попыток,
на этот раз у совершенно здоровых Hadoop,
это такой подход, как спекулятивное выполнение,
speculative execution.
Здесь имеется в виду, что на нашем кластере
очень много нот, но они все разные.
Разные характеристики, разные процессы,
разная надежность, разная сеть на этих машинках может быть.
И чтобы не заниматься исследованием
и не думать, на какой машинке что у нас быстрее посчитается,
мы просто запускаем сразу много попыток,
одной дольше задачи.
Какая первая посчиталась, тот результат мы и взяли.
Ну и давайте подведем такие промежуточные итоги.
То есть map работает с парами ключ значения.
Shaft and sort стадия сортирует данные по ключу и группирует,
или диагонально ссортирует данные по ключу,
или диагонально ссортирует данные по ключу.
И в этом стадии мы не будем делать много шансов,
но стадия сортирует данные по ключу и группирует,
и вот эти вот группы обрабатывает тоже независимо.
То есть так же, как map обрабатывал независимые пары,
редьюсер обрабатывает вот эти группы.
Ну и если проводить такие параллели с SQL,
то map это аналог селекта, аналог UBR,
то есть когда мы взяли какую-то одну строчку
и сказали селект, нам нужно 2 первых поля в этой строчке.
Или взяли where, когда сверили какое-то поле с условием.
Никакой зависимости от других строчек нет.
То есть селект и where это map,
а все остальное, например, joining, gobuy,
всякие аналитические функции, это все уже редьюсер.
То есть маппер это обычный фильтр, парсинг, форматирование,
редьюсер это обычная группировка агрегации.
Ну и давайте посмотрим, как это все работает
с точки зрения процессов.
То есть вот есть наша программа,
она форкается на мастер, на нейм-ноду,
она форкается на воркеры, на датоноды.
И вот input data это у нас HDFS,
мы бьем ее на сплиты, как я уже сказал,
каждый сплит обрабатывается одним маппером,
а результаты мапперов пишутся не в HDFS,
а пишутся в локальную файловую систему.
То есть если мы вернемся к этой схеме,
то вот тут у нас будет локальная файловая система,
вот это все хранится в локальной, на ноде.
Потом произойдет вот этот вот partitioning,
и данные улетят на другую ноду,
но это тоже локальная файловая система,
это не HDFS.
У нас просто HDFS медленнее даже чем обычный диск,
поэтому нет смысла на него каждый раз писать.
HDFS у нас только вот здесь и вот здесь в конце.
Еще есть такой момент,
это сам ходук написан на джаве.
То есть вот эти все вещи, мапы, редьюсы,
все, что мы сейчас разбирали,
как это написать в поди, лучше всего написать на джаве,
и ходук для этого больше всего приспособлен,
но джаву знают не все.
Поэтому есть такая технология, как ходук стриминг.
Что такое ходук стриминг?
Это по сути мы берем уже готовое,
написанное не нами приложение на ходуке,
вот оно верхняя строчка,
то есть тут указан и input формат,
то есть парсинг данных автоматический,
сортировка автоматическая, запись, все это есть.
То, что нам остается сделать, это мапер и редьюсер.
А вот мапер и редьюсер мы категорически не хотим писать на джаве,
потому что пишем его мы.
Мы хотим написать на питоне, например.
Мы можем написать на питоне,
мы можем использовать вот эту программу
и подставить туда свой скрип для мапера и для редьюсера,
но для этого надо, чтобы код на питоне, который мы напишем,
он соответствовал определенным требованиям,
а именно читал данные из STDI
Ну, то есть принты обычные делал.
Вот давайте посмотрим на вот эти примеры кода.
Действительно, вот мапер.
Он читает данные из STDI и пишет данные в STDI.
Редьюсер точно так же.
Читает отсюда, пишет принтами.
Вот, то есть мы можем писать программы на Hadoop
на самом деле на любом языке программируем.
Ну и давайте посмотрим, как вообще это все устроено.
Если мы берем Hadoop Streaming,
джаву я сейчас показывать не буду,
потому что не все ее знают.
Если мы берем Hadoop Streaming,
то по сути, чтобы запустить нашу программу на Hadoop,
мы пишем большую вот такую команду.
Вот это вот одна команда, посмотрите.
Мы указываем ajar files написанной не нами задачей.
Ну и дальше всякие настройки.
Как мы ее назовем?
Сколько будет редьюсеров?
Какие будут коды для мапера?
Какие для редьюсеров?
Входные и выходные данные.
Вот, ну если говорить про Hadoop Streaming в целом,
то мапер или юсер – это программа на любом языке программирования.
Тут даже написано или Java Class,
но если вы знаете джаву,
то лучше тогда Streaming не использовать.
Читают и стдин, пишут и стдраут.
Shaffron Sort – вот этот вот обеспечивает Hadoop,
и мы его никак не меняем.
То есть можем, конечно, на это повлиять,
но из коробки нет.
Нужно будет еще там
ставить в эту команду дополнительные конфиги,
которые будут влиять на Shaffron Sort.
Хорошо, есть ли какие-то вопросы по этой части?
Давайте я сейчас запущу код.
То есть мы уже с вами знаем мапер, знаем редьюсер.
Мы его видели вот здесь.
И знаем вот этот файл, который называется еще Streaming Driver,
и он именно запускает команду в Hadoop.
Вот, давайте сделаем run.
Говорит, что нету.
Сейчас проверим.
Говорит, что нету.
Говорит, что нету.
Сейчас проверим.
Ошибка в пути к данным.
Вот.
Вот что мы видим.
Мы подключаемся к менеджеру ресурсов.
раз. И видите, что мы тут видим? Мы тут видим total input pass 1, number of splits 2,
то есть у нас два сплита, два блока в этом объеме данных. Дальше мы видим,
что началось джоба, вот ее progress bar и счетчики. Это всякие системные счетчики,
сколько чего прочитано, сколько чего записано, где были ошибки. И на семинарах я вам покажу,
как эти счетчики делать самим, как можно добавлять сюда свои счетчики, это бывает удобно для отладки.
Ну и вот результат работы программы, то есть мы с вами хотели посчитать вот count,
сколько раз каждое слово встретилось, и мы это же и видим. Вот слово и число встречаемости.
Давайте для интереса, я зайду на кластик с вот таким пробросом, то есть вот это
1988, вот это порт для job history.
Есть, то есть вот это вот job history. И давайте, чтобы было понятно,
что тут вообще происходит и что можно посмотреть, давайте сделаем баг в задаче.
Я сейчас найду задачу, которую я вам показываю. И, например, в маппере, давайте лучше в редьюсере,
мы сделаем баг. Вот у нас нет функции вот такой form add, поэтому код будет падать,
но давайте мы посмотрим, как он это будет делать. Вот маппер, с маппером все в порядке,
а вот на редьюсере мы увидим, какие ошибки. Что это вообще за ошибки? По этим ошибкам сложно
что-то сказать, кроме того, что pipe map right output threads, то есть что это значит в переводе на русский.
Посмотрим еще раз на вот эту схему стриминга, как происходит передача данных. Вот работает
ходуб задач, которую не мы писали, она выдает какой-то результат, и он через pipe, через
конвейер, обычно линуксовый, передается в маппер. Маппер работает, снова через линуксовый
pipe передается в ходуб на сортировку, и также с редьюсером. В случае нашей ошибки редьюс
получает данные, но ничего не выдает на выход, потому что падает, и exception говорит, я не могу
дождаться данных. Но кроме того, что он не может дождаться данных, мы ничего совершенно определить
вот этим вот странным логом не можем. А как можем? Идем опять в job history, видим, что моя последняя
задачка, она failed, вот, и видим, что у нас тут два маппера, и 16 редьюсеров. Ну к редьюсерам мы
сейчас вернемся, мапперы у нас отработали правильно. Вопрос, почему на маппера два? Вот, от чего это
зависит? Два сегмента? Два сплита, да, если мы посмотрим на вот эти вот логи, то мы увидим
number of spilt 2. Все это железно говорит о том, что будет два маппер. То есть задать явно количество
мапперов мы вообще не можем. Мы можем задать количество редьюсеров, это да. Вот, а маппер мы можем
задать только через размер сплита. Дальше, вот мы видим количество редьюсеров 16, а здесь количество
редьюсеров 49. Попыток больше, чем, ну, чем редьюсеров, это уже говорит о том, что у нас что-то падает.
Тем более, если попыток у нас в три раза больше. Не на одну, на две, то есть вполне может быть такое,
что, ну, по каким-то причинам редьюсер где-то не сработал, может быть ноды что-то, мы переключились
на другую ноду, и все в порядке. А когда вот в три раза больше, то мы сразу вспоминаем, что я вам
рассказывал. Количество попыток на нашем кластере, три штуки. Если за три раза система не смогла
выполнить этот редьюсер, то она просто сдается и падает. Идем в эти попытки, выбираем любую,
мы-то знаем с вами, что у нас код неверный и он падает. Поэтому выбираем любую, идем в логи и видим
вот это. То есть, да, он отругался на form-add, как мы и хотели. Сейчас я верну это на место. То есть,
вот, чтобы смотреть логи, вам нужно пропросить порт JobHistory, зайти в джобу, зайти в попытки и
посмотреть логи. По вот этим ничего, кроме того, что падает ваш код, сказать нельзя.
Теперь у меня еще один вопрос к вам. Кто помнит, чем плохо, если у нас в HDFS хранятся маленькие файлы?
На каждый файл нужен свой блок. Хорошо, ну блок будет и что? У каждого блока есть мета. Да,
у каждого блока есть мета, которая хранится на нейм-ноде, которая, ну если не одна, то их какое-то
ограниченное количество и плюс это оперативка, которая тоже ограниченное количество. Поэтому с
маленькими файлами, с маленькими блоками надо быть осторожным. Вот есть много статей, про это
гуглите HDFS small files problem или Hadoop small files problem. Но главное это что? Вот у нас есть рам и саж.
Наш код, с помощью которого мы запускаем задачу и у него есть коды мапера и коды редьюсеров. Это
маленькие файлы и нам надо как-то их показать всем нодам, потому что мы же не знаем, где запустится,
на какой именно ноде запустится мапер или редьюсер. То есть вот тут мы не знаем,
где будет K1 обрабатываться, на первой ноде, на второй, на третий. И нам нужно, чтобы на всех
нодах где будет запускаться редьюсер вместе с самим редьюсером, чтобы к нам доехал код,
что собственно мы хотим выполнять. То есть вот эти файлы MapperPy и ReducerPy, чтобы они приехали
на ноды. Загружать их в HDFS плохо, потому что вряд ли у нас будет 64 мегабайта чистого кода.
Поэтому что приходится делать? В Hadoop есть такая структура, которая называется распределенный cache,
distributed cache. То есть что это за структура такая? Пользоваться ей достаточно легко,
достаточно указать минус files. И все, что мы сюда указали, это то, что добавится в этот распределенный
cache. Распределенный cache это определенная папка, которая находится на каждой ноде,
и именно туда помещаются вот эти файлы. Они не загружаются в HDFS, никак с ней не связаны,
но они редованы. То есть положить файлы сюда можно в момент запуска задачи, а когда задача
работает, изменять эти файлы уже не получится. Такое часто приходится делать, если у вас
какой-нибудь справочник. Вот, например, вам нужно выполнить простую задачу фильтр, например,
у вас есть куча данных, они лежат в HDFS, вы считаете их Hadoop, но попутно хотите отфильтровать те
данные, которые в каком-нибудь черном списке находятся. Вот, и этот черный список, он небольшой,
и вы его подаете в distributed cache вот сюда как файлик, и из маппера, из редьюзера можете это читать.
На семинарах я покажу примеры про это.
Вот, добавляем файлы в distributed cache, каждая нода будет иметь к ним доступ. Как только
задача закончит работать, этот cache уничтожится. Хорошо, какие вопросы сейчас по маппредьюзу в целом?
Это такое базовое понимание про маппредьюз. Дальше, но уже, наверное, в следующий раз мы
обсудим, как можно настраивать вот этот шахландсорт, то есть пока у нас есть маппер, есть редьюзер,
между ними какой-то шахландсорт, который как-то абсортирует данные, но мы не знаем, как на это
повлиять. Вот узнаем мы об этом в следующий раз. А какие вопросы сейчас по мапперу редьюзеру?
Я слышал, что маппредьюз позволяет очень сильно ускорять многие процессы, которые в обычных
ситуациях очень долго бы длились. Это из-за того, что в самом ходу эффективная реализация для чего-то есть?
Скорее мы ускоряем за счет того, что у нас много машинок, то есть мы не на одной сидим и
последовательно процессим процессик, а мы раскинули, посчитали параллельно, собрали,
получили результат. Вот за счет этого мы ускоряем, но в принципе я бы не сказал, что ходу быстрый,
то есть как раз он не быстрый. Если мы посмотрим на недостатки, они у ходу поесть, то во-первых,
ходу не умеет обрабатывать данные в реальном времени. Почему? Потому что смотрим на эту схему,
вот у нас сплиты, мы прочитали сплит, пошли его обрабатывать мапом, дальше пошли-пошли его как-то
сортировать, процессить. Если в процессе этой задачи она может несколько минут работать,
в ее процессе мы что-то дописали в сплит, то вот эти все стадии никак про этот новый сплит не
узнают. Вот это первый недостаток. Второе активное использование дисков, поэтому нельзя сказать,
что ходу такой быстрый. Мы данные пишем в HDFS, мы данные пишем на диск, мы читаем с диска,
все время идет взаимодействие с диском. Поэтому да, ходу позволяет просто обрабатывать большие
данные, без них вы не обработаете, но не так, чтобы это делать быстро. Я сейчас даже найду статью.
Вот, скину сейчас в чат зума. Называется common life tools can be 235 times faster than your ходу пластыр.
То есть в этой статье говорится, что в принципе, если у вас не очень много данных и вы можете
обработать их на одной машине, ходу вам не нужен, потому что ходу это джава, это постоянный старт и
стоп джава-машин, это постоянная работа с диском, работа с сетью на разных нодах.
И еще один недостаток, это то, что нужно писать много кода.
Например, чтобы сделать такую простую задачку на wordcount, нам нужно написать маппер, написать
редьюсер, написать еще вот этот вот стриминг драйвер. И конечно, хотя бы такие простые задачи
хотелось бы решать быстрее. Мы будем обсуждать, как это все ускорять, как это ускорять по времени,
потом как это ускорять по коду, но в базовой реализации это вот так. Хорошо, еще какие-нибудь
вопросы есть?
Хорошо, давайте тогда посмотрим на вот эту схему. Мы к ней вернемся более подробно в следующий раз,
это скорее такая максимально подробная схема, которую вообще удалось найти про то,
как работает MapReduce. Тут понятно, что у нас есть маппер, есть partitioner, который потом распорядляет
вот эти пары по редьюсерам. Что происходит вот здесь, это мы разберем в следующий раз.
Ну и потом кусочки данных из разных нод приходят на редьюс, и мы их начинаем мержить,
мержим в несколько этапов, вот final merge, и потом это все попадает на редьюсер.
Вот видите, здесь написана память и редьюсер input file, и точно так же в маппере.
Здесь буфер, но он может быть переполнен, и тогда будет spill to disk. То есть в принципе
ходуб нормально относится к тому, что у нас на выходе будет больше данных, чем на входе.
А можете привести какой-нибудь пример, вот когда маппер получил на вход, он же не так-то много
данных на вход получает, он получает один вот этот split, то есть один блок, и один блок это не так
уж много, ну 60 мегабайт, где-то даже 30. Как может быть так, что на выходе маппер у нас появилось
столько данных, что они в оперативную память на сервере не влазят? Сейчас понятен вопрос.
Да, вопрос понятен. Ну вот есть ли какой-нибудь пример, как такое может быть?
А мы должны им начисло сопоставлять каждому элементу или не обязательно?
Ты имеешь в виду ключ значения каждому ключу?
Да, значение может быть обязательно.
Не обязательно, все что угодно, могут быть любые объекты, мы это на семинарах будем разбирать,
это пара просто условно object-object, что там внутри, это уже в MapReduce не столь важно.
Главное, чтобы MapReduce умел это сравнивать, потому что ему сортировать надо будет. То есть какие-то
сравнимые объекты должны быть. Ну, например, мы будем каждый в подстроке что-то задавать,
а подстрок может быть сильно, когда мы храним подстроку, сумма всех подстрок больше, чем строка.
Сумма всех подстрок? А, ну, логично, да. То есть у тебя пришла строка, ты генерируешь все возможные подстроки,
или еще лучше, у тебя на вход приходят пары left-right, числовые промежутки,
а ты на маппере, ну, вот такая у тебя стоит задача, что ты на маппере генерируешь все числа от left-to-right,
и получается у тебя на вход пришло, например, два числа, один и пять миллионов, на выходе пять
миллионов чисел. Поэтому вполне себе можно придумать такой маппер, можно даже придумать маппер,
который вообще не берет входных данных никаких, это просто генерация чего-нибудь, на вход ничего,
на выход сколь угодно большая куча данных. Вот, и ходу, он умеет из коробки ничего для этого делать не надо,
он умеет справляться с тем, что у нас очень много данных, в результате получилось.
Вот, ну, схема страшная, не даром здесь стоит вот этот чувачок, но, в принципе,
следующий раз мы ее рассмотрим более подробно. Сейчас есть еще одна ссылка, это тьюториал от
Майкла Нола о том, как писать правильно программы на ходу к стриминге, но это тоже скорее программа
семинара уже, вам семинаристы рассказывают подробнее, будете разбирать разные задачки.
Вот, с таким базовым маппредьюсом сейчас все, в следующий раз мы разберем внутренность маппредьюса,
то есть как устроен шафлед-сорт, как можно его настраивать, и еще разберем как в маппредьюсе
делать джойны, потому что джойны в маппредьюсе это отдельное поле.
Даже раньше все это мы с вами завершили. Есть ли какие-то сейчас вопросы еще?
Почему именно маппредьюс, почему именно комбинация двух таких операций так изучается и применяется?
Ну, потому что это достаточно универсальный набор операций, то есть все, что мы делаем,
мы распределяем, считаем независимо, то есть получается мы можем или считать независимо,
или считать зависимо. Вот там, где мы независимо считаем, так называемые bites,itely operations,
мы не храним состояние, мы просто взяли элемент, посчитаем и написали куда-то, или state from
операцией, когда мы храним какое-то state состояние, и вот тут уже, где мы state храним,
то есть в принципе большая часть разных математических операций она ложительна на
предьюс. Вот чуть позже мы с вами будем разбирать SQL по верхам а предьюса,
то есть SQL тоже ложительна на предьюс.
Хорошо, есть ли еще какие-нибудь вопросы?
Если вопросов нет, тогда на этом все и всем спасибо.

Итак, давайте надеяться, что все будет хорошо.
И я еще раз повторю для записи, что мы собираемся поговорить про курс в целом,
резюмировать, что в нем происходило и как-то вычленить какую-то полезную интуицию,
какие-то полезные общие идеи, которые с вами останутся даже после того,
что вы видите, все конкретные детали рафтов и мультипаксов, которые вы писали.
И еще раз скажу, что если у вас есть какие-то свои впечатления,
делитесь ими, и мы будем вот в эту сторону двигаться.
Напомню, что начинали мы курс с того, что вообще задавались вопросом,
а почему мы говорим про распределенные системы?
И ответов у нас было, кажется, два. Мы говорили про распределенность,
потому что нас волновала отказоустойчивость, потому что отдельные машины могут отказывать,
и потому что нас волновала масштабируемость, когда даже в отказоустойчивую,
в надежную машину, в группу машин, все данные не помещаются.
Ну и давайте и про то, и про другое подробно поговорим.
С чем мы начнем? Наверное, с отказоустойчивости.
Вот у нас были протоколы консенсуса. Мы начинали с такого наивного алгоритма,
который реплицировал ячеек в памяти, и увидели, что даже наивный алгоритм,
пишем на 2 из 3, читаем с 2 из 3, сам по себе он не давал нужной нам гарантии линеризуемости.
Про линеризуемость чуть позже. Вот даже в такой модели мы увидели, что...
Нет, нельзя позже, конечно. Линеризуемость сейчас...
Нет, не хочу про линеризуемость. Хочу лишь сказать, что у нас был протокол
двухфазной АБД, который позволял нам работать с ячейкой памяти,
как с отказоустойчивой сущностью на трех машинах.
Но при этом мы столкнулись с проблемой, что этот протокол не обобщается
на более сложные операции. Мы не смогли сделать операцию CAS.
И проблема была в том, что разные реплики получали, апдейты все в эту ячейку
были версионируемые, разные реплики получали их в разном порядке.
И мы выдумали идею атомных бродкаста, выдумали такой примитив,
а дальше заметили, что если мы умеем решать задачу консенсуса,
если мы можем договориться об общем выборе, то мы можем с помощью консенсуса
договориться про порядок доставки сообщений и таким образом
линеризуемо реплицировать произвольное состояние, произвольный автомат.
И можно было бы думать, что задача консенсуса про упорядочивание.
Давайте сегодня так думать не будем и в будущем так думать не будем,
потому что смысл, конечно, не в этом. У нас когда-то была статья
про GFS, Google File System. И там тоже возникала задача упорядочивать что-то.
Файловая система имела довольно странный по нынешним меркам,
распределённая файловая система умела перезаписывать фрагменты файлов.
Я надеюсь, вы понимаете, перезаписывать фрагменты файлов не нужно.
Нужно уметь делать аппендо, потому что поверх аппендов
через райтах от логи, через LSM мы можем сделать более-менее произвольное состояние.
Так что сейчас бы мы такую файловую систему делать не стали.
И когда мы говорили про колосус, я обращал внимание, что в колосусе
в современном GFS операции перезаписи нет, но тогда она была.
И для того, чтобы упорядочивать записи в одни и те же фрагменты файлов,
в одни и те же чанки, на которые эти файлы дробились,
среди реплик каждого чанка выбиралась реплика Primary.
Но вот эта Primary, она брала у мастера Лизу, то есть блокировку временную.
Про это отдельная история, почему блокировки называют Лизами в распределённых системах.
Брала такую блокировку на время, арендовала её, чтобы было корректнее сказать,
и получая записи от клиентов, выстраивала их в некотором порядке
и применяла, расславала другим репликом.
Задача упорядочивания прекрасно решается одним узлом.
Но в конце концов, кто-то один выстраивает порядок.
И в GFS это был один узел.
И, конечно же, мы знаем, что такой дизайн не работает.
Но это уже немного другая история, скорее про RAFT и про Multi-Paxis,
про то, что вроде бы мы долго-долго с вами делали какие-то протоколы,
долго-долго выдумывали в сети Atomic Broadcast, наивный мультипакс,
потом строили Multi-Paxis, потом его оптимизировали,
и в конце концов получали очень наивный протокол,
где лидер получал команду от клиента, рассылал её на большинство,
получал подтверждение, и всё, и на этом всё заканчивалось.
Сейчас я две истории про то, что консенсус,
про упорядочивание и про то, что RAFT очень наивный.
Я хочу объединить примерно в одну историю и сказать, что, конечно же,
и то, и другое не так, потому что упорядочивает в конце концов только один узел.
Но, конечно же, вот такого наивного дизайна недостаточно.
То есть RAFT выглядит довольно наивно на быстром пути, когда всё стабильно.
Но RAFT и Multi-Paxis, и Singularity-Paxis, и все вот эти протоколы,
и задачи консенсуса нужны не для того, чтобы упорядочивать что-то,
а для того, чтобы пережить случаи, когда вот такой Primary умирает.
Сложность настоящая в этом. Сложность в том, когда даже не совсем может быть точно.
Самая большая сложность не в том, что Primary умирает,
а в том, что Primary на самом деле не умирает.
Но его лиза отбирается и даётся другому узлу.
Иначе говоря, у нас с системой появляются несколько лидеров.
Так что смысл консенсуса, это, конечно же, не фиксация порядка,
это на самом деле конкуренция лидеров.
И Algorithm-Singularity-Paxis – это как раз такой изолированный случай конкуренции этих лидеров.
И вот дуэль пропоузеров, которые там возникают, когда они перебивают друг друга,
реализация FLP-тиаремы – это конкуренция, которая не может завершиться никогда.
Суть консенсуса в этом. Да, он может принимать разные формы,
но фундаментальная сложность не в том, чтобы отказ пережить,
а в том, чтобы пережить конкуренцию, когда отказа нет.
Разумеется, мы не можем отличить медленный узел от отказавшего,
но про это FLP-тиарема и была, так и доказывалось.
И поэтому мы вынуждены с этой проблемой бороться.
Вот если мы говорим про распределённые блокировки,
то там проблема же точно такая же.
Мы не можем гарантировать, что распределённая блокировка гарантирует нам взаимные исключения.
Если мы делаем lock-сервис распределённый, если клиент может к нему прийти,
а потом мы клиентом можем отказать, то сервис блокировок должен нашу блокировку изъять.
Но опять же, он нас изъял, не потому что мы отказали,
а потому что у нас случилась пауза из-за сборки мусора.
И снова получилась конкуренция.
Снова мы клиенты оба думаем, что мы владеем блокировкой,
потому что этот клиент получил подтверждение от сервиса блокировок,
а этот клиент всё ещё думает, что он владеет блокировкой, хотя сервис уже думает иначе.
То есть точка, где всё понятно есть, но отдельные клиенты запутались.
И снова конкуренция.
Сложность всегда в этом.
Сложность в том, что нельзя гарантировать взаимные исключения, нельзя гарантировать в системе...
Немного аккуратнее, конечно, можно гарантировать, что одновременно лидера одного нет с помощью часов,
но, кажется, мы доказывали с вами на первой лекции ещё, что часы синхронизировать нельзя надёжно,
поэтому полагаться на них в общем случае не безопасно,
а просто на основе коммуникации гарантировать, что лидеры не пересекаются во времени, уже нельзя.
Поэтому мы весь этот курс живём в предположении, что лидеры конкурируют,
и они конкурируют внутри паксоса, они конкурируют внутри...
Они просто бай-дизайн конкурируют внутри мульти паксоса наивного,
они могут конкурировать внутри рафта,
а сервис распределённых блокировок, в конце концов, используется пользователем для того, чтобы выбирать лидеров в своих системах.
Я в параллельном курсе рассказывал немного про кафку,
когда в кафке лидер выбирается с помощью Зукипера.
В общем, это всё неизбежная проблема, и она должна как-то решаться,
и мы видели, как она решается в синхронизации крипаксоса.
Это был довольно странный, непонятный протокол, но когда мы начали его разворачивать,
оптимизировать и строить поверх него лог, то мы увидели, что все странные N, NP облетают некоторый смысл.
И мы увидели, что лидер всегда связан с некоторой эпохой.
Синхронизация крипаксоса – это был просто N, какой-то ballot number непонятный.
Но когда мы сделали мультипаксос, мы увидели, что этот N – это просто эпоха этого лидера.
И даже если лидеры конкурируют, чтобы блокировать старых лидеров,
нужно привязывать их к эпохам и просто запоминать, какую эпоху старшую мы слышали.
Так было и в RAF, так было и в сервисе блокировок.
В любом сервисе блокировок требуется, чтобы этот сервис вместе с блокировкой давал нам
еще и некоторый токен, с помощью которого какая-то третья система,
которая получает команды от двух наших конкурирующих клиентов,
понимала бы, что этот клиент новый, а этот клиент уже старый, и блокировку передали другому.
То есть принцип один и тот же. Мы связываем эпохи лидеров, и с помощью этих эпох
можем блокировать старых.
Но чтобы все аккуратно исключало друг друга, мы используем еще и Quora.
С мультипаксосом, вообще с консенсусом, была связана еще одна история,
а именно, как мы эти протоколы разрабатывали.
Но вот в курсе был рассказан некоторый неоптимальный,
выбран намеренный неоптимальный путь. Мы сначала говорили про синглы декрипаксос,
потом про мультипаксос, потом про RAF. В принципе, ничто не мешало бы начать именно с RAF.
Ну и скажем, в MIT так и делают. То есть рассказывают просто про RAF, про мультипаксос перестали.
Но мне кажется, что с точки зрения разработки, может быть,
эффективнее было бы только про RAF и сразу про RAF.
Но есть два аргумента против такого. Во-первых, авторы RAF утверждают,
что алгоритм проще, чем паксос, и про паксос знать в принципе не нужно.
Я же пытался до вас донести мысль, что он не проще, чем мультипаксос.
Это просто такое логическое продолжение. Мы с вами говорим про задачу консенсуса изолированную,
потом мы говорим про наивный мультипаксос, где мы просто выстраиваем параллельные
эти паксосы отдельные, а потом мы выдумываем некоторые оптимизации,
а именно выбор лидера и конвейер. Но просто их выдумываем, понимаем,
что их хорошо бы встроить сюда. Примерно понимаем, как их можно было бы встроить,
а потом уже говорим про RAF, как про мультипаксос, в котором все это строено, подогнано
и работает корректно. То есть это просто скорее еще одна ступень эволюции.
В мультипаксосе и в RAF-те слишком много похожих идей. Опять лидеры, привязанные к эпохам,
блокировка старых лидеров по этим самым эпохам.
Теоремы доказываются более-менее так же. И вообще все теоремы в курсе
доказываются примерно похожим образом, и видимо просто алгоритм паксос слишком
фундаментарен для этой области, и по-другому сделать нельзя.
Ну и еще одна причина, по которой в курсе есть паксос наравне с RAF-том,
потому что мне кажется, что реализуя мультипаксос, вы увидели, что проще не проще
можно сравнивать в разных смыслах. Можно бы оптимизировать метрику, насколько
просто его написать, а можно оптимизировать другую метрику, насколько, скажем,
код получается простой и модульный. Ну и если вы, скажем, писали паксос
и там писали выборы лидера в нем, ну что-то более-менее нетривиальное,
то вы должны были увидеть, что сам протокол просто хорошо декомпозируется.
Вы можете писать логику репликации логой, особо не думая про то,
как устроен концентр, это может быть почти что черный ящик.
В RAF-те, если вы пишете код, то вы в конце концов остаетесь таким вот классом
на 600 строк, и в нем очень плохо отделяются какие-то части,
и это, наверное, сложнее отлаживать. Ну не знаю, отлаживать проще сложнее не уверен,
но думать об этом коде большом монолитном сложнее. То есть, с одной стороны,
в RAF-те сплавливаются все евристики, но они сплавливаются в такой вот монолит,
и это некоторая проблема. И можно было бы подумать, что это так мир устроен,
что если вы хотите сделать что-то оптимальное, то нужно делать что-то такое
монолитное и сложное. Но вот оказывается, что нет. Я один раз про это рассказывал,
у нас был небольшой семинар про это, я коротко рассказал, что это пошло не так.
Очень обидно. Я рассказывал про эту идею, что в Facebook делали свой сервис координации,
свой Зукипер, и они как раз заботились о том, как сделать систему максимально модульной.
То есть, она должна быть эффективной, разумеется, и должна быть модульной.
И они все-таки придумали альтернативную декомпозицию, которая позволяла делать консенсус,
то есть реплицировать лог так же эффективно, как RAF, но при этом написать код модульно.
Обстракции были выдуманы другие, то есть декомпозиция не по слотам, а по сегментам лога
в другом измерении, но декомпозиция все же другая. Там выделялось понятие лог,
это такой фрагмент лога, и это был компонент, который отвечал за этот кусочек лога,
он был отказоустойчивый, но не сильно. Это три реплики, среди них есть лидер,
но лидер просто прибит гвоздями, он не может переехать на другую машину.
И вот этот логвит был очень простой, там лидер получал команду, писал ее в свой лог,
реплицировал на кворум, получал подтверждение, то есть вот такой наивный протокол.
Но у наивного протокола, как мы обсудили уже раньше, проблема, он не отказоустойчивый,
он не может справиться со сбоем лидера. Но вот этот протокол, этот логвит, этот модуль,
он не решал такую задачу, он отвечал именно за упорядочивание.
Поэтому я говорю, что консенсус для упорядочивания не нужен.
Вот этого компонента требовалось просто уметь запечататься, остановиться и не принимать новые команды.
Вот если он это делать умел, то поверх этого логвита можно было сделать вот непосредственно
уже распределенный лог, реплицированный, и его можно сделать было с помощью как раз мультипаксиса.
Вот на этом уровне, то есть на уровне упорядочивания, консенсус не нужен.
Консенсус нужен только для того, чтобы склеивать вот эти фрагменты друг с другом.
И оказалось, что для того, чтобы склеивать вот эти логвиты, склеивать эти сегменты достаточно,
достаточно написать вот просто обычный неоптимизированный мультипаксус наивный,
где в каждом слоте свой независимый двухфазный паксус даже без конвейера.
И на этом можно построить эффективный продакшен.
Так что, может быть, рафт это и не самая оптимальная идея.
Кроме того, рафт, помимо того, что монолитный, он еще и мешает некой оптимизации в пакса встраивать.
Например, я на одной из лекций рассказывал вам про лог-девайс,
про систему в Фейсбуке, которая реплицирует логи, то есть там не то чтобы автомат какой-то, там просто лог.
Сам лог – это есть данные.
И там была идея такая, что можно использовать более гибкую систему кворумов,
но вообще оказывается, что кворумы разных фаз должны пересекаться в паксусе, а кворумы одной фазы могут не пересекаться.
Поэтому, скажем, можно реплицировать одну запись в первом слоте лога на один набор аксепторов,
другую запись на другой набор аксепторов, потому что так можно увеличить пропускную способность.
Ну и получится что-то более эффективное на запись.
Но при этом в RAFT такая ауристика же не пройдет, потому что RAFT требует, чтобы когда вы добавляете что-то в лог,
если вы лидер в RAFT добавляете в лог реплики какие-то записи,
то реплика проверяет, что префикс лога сходится у вас и у лидера.
Для этого нужно иметь весь префикс, а здесь префикса никакого нет.
Здесь разные реплики, разные аксепторы хранят разные наборы данных, и просто мы их в merger склеиваем.
Можно придумывать разные ауристики, и оказывается, что в мультипаксусе фреймворк эти ауристики встраиваются,
а в RAFT все подогнано, и там что-то тюнить становится сложнее.
Ну или, скажем, у вас завтра на зачете будет вопрос, можно ли переконфигурации из паксуса применять в RAFT?
Вот он протокол, когда мы коммисили переконфигурацию просто в лог.
Вот в RAFT это тоже нельзя делать почему-то, хотя казалось бы, примерно похожая идея должна быть.
Ну в общем, RAFT гораздо менее гибкий, он быстрый, но можно сделать также быстрый, при этом модульный и более кастомизируемый.
Так что мне кажется, что и то, и другое знать полезно.
Ну и в конце концов много продакшн на свете написано именно через мультипаксус.
Ну по крайней мере его использует Google, потому что исторически его использует Amazon, потому что, видимо, он им больше нравится.
Ну а Open Source системы чаще всего используют RAFT, потому что есть Open Source реализация, мы ее копируем к себе, и мы счастливы.
У нас вопрос про мультипаксус на таблите в этом случае.
А таблит в этом случае, когда мы говорим о мультипаксусе, про усиление, про прочную способность, что несколько лидеров, если у нас очень мало клиентов, мы используем таблит?
Подожди, таблит, у нас слово появлялось в контексте масштабирования, про которое мы сейчас еще не говорим, не успели.
Это было шардирование, мы брали кивалию хранилища, брали таблицы, делили их по строкам на какие-то части.
И сказали, что за каждую часть таблицы отвечает свой набор реплик. В биктейбле это называлось таблит.
Да, про другое, про что тогда?
Я говорил, что в мультипаксусе у нас есть одна маленькая проблема с лидером, то, что мы добавляемся к конкретности, но в то же время у нас огромное количество таблет прямо от нас.
Но это про масштабируемость разговора, мы сейчас про другой уровень говорим, мы говорим про консенсус.
И последнее, я раз уж упомянул про MIT, что они рафт перестали учить, но с другой стороны у них есть проблемы, потому что рафт, хоть в рафте заявляют, что дизайн проще и понятнее,
но если вы читали статью и писали код, то вы знаете, что там есть некоторый обман, что вроде бы говорят, что есть выборы лидера независимые, есть репликация независимая, потом они друг о другу начинают зависеть.
И в мультипаксусе можно написать отдельный модуль выбора лидера абсолютно произвольным образом, а в рафте так сделать нельзя, потому что там нужно выбирать лидера очень аккуратно с полным логом.
В общем, некоторый обман есть, но это не то чтобы обман, и вообще сравнивать не нужно, нужно вынести мысль, что рафт это оптимизированный мультипаксус,
а мультипаксус это скорее фреймворк, где есть такой базовый ядросинг, и его уже можно по-разному разворачивать, по-разному декомпозировать и добавлять разные наборы лидер.
И в контексте этого GFS, с чего я начинал свой пример, что консенсус про конкуренцию лидеров, не про упорядочивание.
Примеры с Facebook, это как раз пример, где упорядочивание есть, а консенсуса нет, то есть они на разных уровнях.
Сложность в конкуренции лидеров, и поэтому в любом протоколе консенсуса ядро связано с тем, чтобы мы привязывали лидеров к эпохам и аккуратно эти эпохи обновляли.
В общем, два неразрывных понятия, в любом протоколе они будут.
Что еще можно про репликацию сказать?
Наверное, тот подход, который мы к репликации выбрали, логи и упорядочивание всего, он был связан с тем, чего мы ожидали от распределенной системы.
Мы в смысле клиенты. Я говорил вам в первом занятии, что про клиента вообще важно думать.
Клиенты тоже часть системы. Почему клиенты часть системы? Потому что они тоже отказывают, как и узлы.
Если вы вспомните блокировки распределенные, почему у нас везде не локи, а лизы?
Потому что клиенты отказывают, и мы не можем им отдать блокировку во владение.
И ровно из-за этого получается конкуренция неизбежная, которую мы героически преодолеваем.
Какие еще примеры отказов клиентов? Если вы вспомните лекцию про спандера, про распределенные транзакции,
то там же был двухфазный комит, который должен кто-то координировать.
И клиент, если умирал в середине двухфазного комита, то протокол блокировался.
Это сложная интуиция совсем.
Этим была посвящена лекция про детерминированные транзакции.
Там был разговор о том, что двухфазный комит вообще транзакциям не нужен, он менее фундаментальный, чем ПАКСС.
И от него можно избавиться, если сделать транзакции не интерактивными.
Потому что интерактивная транзакция – это транзакция, в которой пользователь может умереть,
которая ее выполняет. И транзакция может в середине откатиться из-за смерти пользователя.
Если мы избавимся от интерактивности, если мы потребуем, чтобы клиент отправил транзакцию целиком в системе,
то можно сделать что-то более эффективное.
В общем, про это тоже нужно думать. Нужно думать, потому что у клиентов есть ретраи.
И дисциплина не помогает, потому что соединение разрывается, гарантии пропадают внутри дисциплины соединения.
И причем ретраи – это, конечно, могут быть на одну машину, могут быть на разные машины.
Поэтому если мы хотим в нашем РСМе, в нашем РАФте, успешить экзоклюанс,
то мы должны попросить клиента помочь нам. Пусть он выдумывает идентификаторы.
В сервисе блокировок клиент еще пинги отправлял.
Ну, короче, клиент – полноценный участник, а еще клиент наблюдает поведение нашей системы.
И вот это был разговор. Мы заявили его в самом начале курса, он был нам важен.
И ровно поэтому мы дальше говорили именно про упорядочивание всего подряд.
Потому что клиенты могли работать с нашей системой конкурентно
и не хотели в свою очередь думать, что эта система распределенная.
Было бы удобно, если бы клиенты думали про систему, где там тысяча узлов,
как просто про один бесконечно большой, бесконечно емкий, бесконечно надежный компьютер.
И мы эти ожидания сформулировали в виде модели согласованности,
которая называлась линейализуемость. Мы говорили, что клиенты работают с системой конкурентно,
отправляют туда записи и чтения и ожидают, что эти записи и чтения исполнятся
как будто бы в некотором глобальном порядке. Причем этот порядок будет уважать
предшествование операции в реальном времени. Если запись завершилась до начала другой записи,
то в системе вторая запись все-таки останется в конце концов, первая перетрется.
И вот эту гарантию мы обеспечивали как? Ну, просто выстраивали внутри системы
все команды пользователей в порядке с помощью консенсуса,
с помощью мультипакса, сарафта, всех протоколов, которые у нас были.
Помните ли вы, почему мы требуем, почему мы вообще ожидаем, мы клиенты от системы,
что она будет учитывать предшествование операции в реальном времени?
Потому что это ведь странно, ведь мы не можем наблюдать это время реальное.
У нас есть часы, а часы неточные, синхронизировать их нельзя.
А при этом мы от системы требуем.
Мы на самом деле от системы хотим, чтобы она не время уважала, а heavens before наша.
То есть если мы сделали запись, получили подтверждение от системы,
сказали другому клиенту, что он сделал чтение, то он ожидает, конечно, что он увидит запись,
потому что он знает, что запись предшествовала его чтению, потому что ему об этом сказал другой клиент.
У нас причина есть между чтением и записью, между двумя операциями.
Но эта причина реализуется не внутри системы, она реализуется снаружи,
и система про эту причину ничего не знает.
Поэтому мы на месте системы делаем так. Мы консервативно предполагаем,
что если две операции у порядочного времени, то между ними могло быть heavens before,
и значит клиент мог чего-то ожидать. Поэтому мы вот пытаемся этой гарантии достичь.
Что еще про клиентов? Не знаю, про клиентов, наверное, все.
Впрочем, то, что мы внутри системы выстраиваем все в одном порядке,
все операции, чтобы реплики у нас не расходились, это же не единственный способ,
как можно задачу решать, задачу репликации в смысле.
Мы про это, правда, не поговорили совсем, но есть альтернативный подход,
который состоит в том, что мы просто реплицируем не произвольное состояние,
а некоторое специальное состояние, которое более устойчиво к реодолингу операций.
Но вот если две операции коммутируют, то не нужно их особо упорядочивать,
потому что их можно слить в конце концов и получить одно и то же состояние на разных репликах.
Вот это называется CRDT. Конечно, там гарантии более слабые, но принцип совершенно другой.
То есть мы на уровне структуры данных справляемся с тем,
что операции на разных репликах приходят в разном порядке, просто разные под наборы операций.
Нам важно, мы хотим достичь не линеризуемости, мы хотим достичь eventual consistency,
то есть чтобы в конце концов, когда все апдейты становятся,
реплики сошлись к одному и тому же состоянию.
Вот если у нас структура данных, это, например, множество растущее или счетчик,
то это можно сделать гораздо проще без рафта, без мультипаксов, без консенс.
Смотри, он используется... Google Doc это такой подход.
То есть это совсем другая задача. Не то чтобы у нас гигантская система, много клиентов,
но смысл примерно такой же. Вот у нас есть консенсус,
и консенсус говорит нам, что требует от нас, что в случае partition
наша система должна в одной из половин partition блокироваться.
То есть если мы оказывались в меньшей части partition, то апдейты останавливаются.
А теперь другая задача. У тебя мобильное приложение, у тебя много клиентов,
и клиент заходит в метро, и при этом у него связи нет с остальными,
но при этом он хочет свое состояние менять, просто локально в Google Doc что-то трогать.
Google Doc это, строго говоря, не CRDT, но пример подходящий к лаборативной редактировании текста.
Так вот, ты остался в изоляции, ты отдельная такая реплика,
и при этом ты в себе можешь аккумулировать апдейты, которые другие не видят.
Потом ты вернешься в сеть, раздашь эти апдейты, и они как-то смогут
с другими локальными апдейтами других пользователей смерзаться.
То есть здесь скорее оппозиция такая. Изабел, твой вопрос, прости, я в свою сторону ушел.
Что?
Ну, конфликты могут возникать, но они как-то разрешаются.
Вот если у тебя счетчик, то какие тут конфликты? Там плюс, тут плюс, и вот они слились.
Если обзац вставил, а в другой пользователь удалил,
то если можно аккуратно скомбинировать разумно, то это так будет сделано.
Если нельзя, то получится, возможно, какая-то ерунда.
Ну, собственно, CRDT отличается тем, то есть люди в этой области пытаются
брать более сложные структуры данных и при этом заставлять их вести себя разумно при таких конфликтах.
Не то чтобы эти цели всегда совместны, но какие-то усилия предполагаются.
Ну вот, это такой вот подход опять связанный с репликацией.
С репликацией у нас был еще один, мне кажется, важный сюжет.
Он уже не про алгоритмы, а про дизайн.
Если мы хотим сделать что-то отказоустойчивое, что-то маленькое, но отказоустойчивое,
мы берем три реплики, кладем на три диска этих реплик копии и состояния
и реплицируем их с помощью мультипаксиса, рафта, что вам больше нравится.
Ну вот, мы с вами видели, когда мы говорили про Bigtable, когда мы говорили про масштабируемость,
мы наблюдали другой подход.
Мы использовали другой подход, а именно вместо того, чтобы работать поверх
трех ненадежных файловых систем с ненадежными дисками
и реплицировать состояние через рафт, мы могли работать поверх одной файловой системы,
зато ненадежной.
И в обоих случаях, что в рафте, что в RSM, что в таком дизайне,
вот этот был дизайн таблета Bigtable, то есть кусочка таблицы,
который позволял по произвольному ключу писать и читать,
ну и реализовывал LSM поверх распределенной файловой системы.
В общем, в чем параллель? У нас есть три levelDB на трех дисках,
которые реплицируются с помощью мультипаксиса, или у нас есть один экземпляр этого LSM-дерева,
один levelDB, поверх распределенной файловой системы, отказоустойчивой.
Немного другой дизайн, и чем он отличается?
Во-первых, тем, что такому дизайну не нужны три точки обслуживания.
У нас в рафте или мультипаксисе три реплики или пять реплик, какое-то количество,
и они в памяти хранят все состояние, потому что они готовы переключиться стать лидером.
Вот здесь, в таком дизайне, у нас есть слой хранилища,
а точка обслуживания, то есть машина, которая хранит memtable,
которая принимает пут и геты и обслуживает пользователей, она одна для таблета.
Если эта машина отказывает, то просто точка обслуживания переезжает на другую физическую машину,
но данные не теряет, потому что они в подсистеме хранения лежат.
В случае, если машина умирает в рафте, то она умирает с дисками,
но, к счастью, у других дисков есть кое-какое состояние.
Здесь мы хранение абстрагируем от обслуживания и уделяем в отдельные подсистемы.
Это очень разумный дизайн, потому что он позволяет смотреть на машины физически иначе.
В наивном подходе у вас отдельные машины – это буквально узлы системы.
Узл системы тождественны некоторой машине. В таком дизайне узел системы – это набор ресурсов.
Это дисковые емкости и это процессор.
И одна и та же машина может быть частью системы хранения и частью системы, которая обслуживает пользователей.
Но скажем, у вас одна и та же машина может быть частью колосуса и хранить там чанки файлов,
и может быть частью спаннера, который этот колосус использует для записи в конце концов.
И что очень любопытно, в таком дизайне вы упрощаете себе репликацию.
Потому что если вы реплицируете буквально сложные мутабельные состояния, то вам нужен рафт.
Но в таком дизайне, где у вас репликация скрыта внутри GFS, вот здесь у вас эсэстейблы и мутабельные.
И у вас здесь таблет-лог, но если приложение бачит записи в него, он растет такими большими порциями, большими кусками.
И в итоге у вас здесь такие имутабельные чанки добавляются к этому файлу.
Здесь вот просто большие статичные файлы из статичных имутабельных чанков.
И имутабельные данные реплицировать гораздо проще.
Для этого не нужен рафт, и для этого можно добиться гораздо более эффективного хранения.
В рафте мы используем X3-репликацию, то есть три копии.
Состояние сложное. Здесь данные имутабельные, поэтому мы можем взять чанк эсэстейбла,
или даже чанк лога вот этого, побить его на кусочки, на шесть кусочков,
посчитать на них для этих шести кусков чексуммы с помощью кода Фридита Соломона.
И дальше вот 9 фрагментов, 9 блоков рассыпать по кластеру.
И теперь мы готовы в такой схеме пережить, во-первых, мы готовы пережить три отказа диска,
а раньше были только два готовы пережить.
А во-вторых, мы вместо тройной репликации используем полуторную.
То есть мы сэкономили себе половину дисков.
Если у нас данные хранятся на миллионах машин, то мы сэкономили себе миллионы дисков.
Это прям очень-очень большие деньги.
И вот в таком дизайне есть ещё одна интересная особенность.
Есть ли у меня готовая ссылка? Да, есть.
Вот такой дизайн был использован, когда мы говорили про распилённые транзакции,
у нас были детерминированные транзакции, где как раз мы избавлялись от двухфазного комита.
И я рассказывал про детерминированные транзакции на примере системы Яндекс.ДБ.
И в Яндекс.ДБ система выстроена из сущности, которая называется таблет.
Это не тот же таблет, что был Bigtable, чтобы мы ещё больше не запутались.
Суть такая. Система состоит из акторов, которые что-то делают, общаются друг с другом.
Есть акторы, которые отвечают за таблицы пользователей, есть служебные и системные акторы.
Но эти акторы отказоустойчивые. Отказоустойчивый актор в этой системе называется таблеткой.
Это актор, который хранит своё состояние в подсистеме хранения данных.
В отдельном blob storage он называется.
И смотрите, вот здесь это физическое воплощение этого актора, это некоторая одна машина.
Вот он там запускается и работает. Если он отказывает, то обслуживание данных этого актора возрождается на другой машине.
Так вот, что есть ещё любопытного? Можно заметить, что нам не нужно в таком дизайне...
Сейчас я найду нужный слайд. Нам не нужно в таком дизайне очень-очень много паксусов, очень много консенсусов.
Если вы вспомните, кто слушал про кавку, то там был такой дизайн, что был один зукипер, который делал консенсус.
И он был на всю кавку разом.
Вот здесь та же самая идея.
Для того, чтобы заводить очень-очень много отказоустойчивых акторов, нам не нужно для каждого из них делать независимый консенсус.
Нам нужен консенсус только в одном месте, при конкуренции.
Вот у нас старый экземпляр машины умирает, мы возрождаем актора на другой машине.
Но ещё раз, он там возрождается, а старый на самом деле не умер, просто он залип.
И в итоге у нас два инстанса одного и того же актора.
И два этих инстанса меняют одно и то же состояние.
И снова мы должны связать их с эпохами.
И под система хранения данных она, с одной стороны, отвечает за хранение логов этих акторов, чтобы обеспечить персистентное состояние.
А с другой стороны, она с помощью операции BLOCK решает проблему конкуренции.
Каждый актор возрождается в некотором поколении.
И мы можем просто заблокировать все предшествующие поколения.
И нам нужен консенсус только на этом уровне, под капотом этой системы, вот здесь.
А сверху у нас уже модель другая, где отдельные узлы, которые отказывают, мы их переселяем на другие машины.
В конце концов, консенсус нужен и снова для конкуренции, снова не для упорядочивания.
Это важный дизайн, сейчас он используется много где.
Какие-то системы его не используют, потому что этот дизайн требует больше аккуратности, требует большего количества абстракции под систем.
Но я бы сказал, что он, кажется, более современен.
И когда вы просто арендуете виртуалку в облаке, то в конце концов вы же тоже не заботитесь о том, что она откажет.
Конечно же, она откажет, потому что она на каком-то физическом узле живет.
Но диск в вашей виртуалке, он, конечно же, не локальный.
Этот диск это, на самом деле, некоторая абстракция.
И когда вы пишете туда блоки какие-то, то они отправляются в условный колоссус, и там хранятся.
Там как раз сырые же коды и, скорее всего, не потеряются.
Что еще про репликацию нужно сказать?
Мы использовали репликацию для отказа устойчивости,
и основной рабочий инструмент у нас при реализации отказа устойчивости был кворумы.
Мы их придумали на вторую лекцию.
Для этого вторая лекция была нужна.
Про ABD, про регистр.
Не потому, что это полезно, а потому, что там возникло упорядочивание, временные метки,
и потому, что там возникли кворумы.
И поначалу кворумы были очень простые.
Это были два узла из трех, ну или просто большинство узлов.
А дальше мы в течение курса эти кворумы усложняли.
В какой-то момент мы заметили, что можно использовать разные кворумы для разных фаз паксосе,
и что некоторые фазы частые, репликация на вторая фаза,
а некоторые фазы редкие выбора лидера.
Поэтому мы можем организовать реплики в такой прямоугольник
и сказать, что у нас кворум для одной фазы это строчка для другой столбец.
Можно здесь немного все тюнить, чтобы получить большую скорость.
Мы заметили, что кворумы могли бы учитывать разные домены отказов.
У нас с одной стороны кворум большинства защищает отказы меньшинства узлов,
но при этом, когда вы строите георазпределённую систему, то у вас больше проблем,
потому что вы должны думать не только про отказы узлов, а про отказы дата-центров.
И вы строите систему кворумов, которая бы, скажем, учитывала,
что у вас могут выходить из строя машины или целиком дата-центры,
или целиком даже группы регионов, вы можете потерять с ними связанность.
То есть кворумы здесь усложнялись.
Они ещё больше усложнились, когда мы перешли византийскую модель.
И там появилось два новых типа кворумов.
Кворумы, которые назывались маскирующие и которые назывались...
Где-то у меня здесь статья была.
Маскирующие кворумы и диссеминатинг. Такое странное название, диссеминатинг кворума.
Ладно, я потерял.
Вот, маскирующие кворумы.
Идея их состояла в том, что наши кворумы теперь пересекаются не по одному узлу,
потому что он может быть нечестным, а по большому количеству, а именно 2f+,1.
Почему 2f+,1? Потому что в пересечении есть точно узлы, которые знают свежую версию чего-нибудь.
Но непонятно, как их отделить от нечестных, которые просто выдумывают что-то.
И мы брали толстые пересечения, потому что в этом пересечении
обязательно честных узлов было больше, чем нечестных, f+,1 против f.
И таким образом мы сначала могли отсечь ответы просто по количеству,
потребовать, чтобы одинаково было f+,1, а потом уже отсечь по версии.
И вторая идея была в том, что система кворумов пересекалась по f+,1 узлу, то есть по одному честному.
Это работало, когда данные были подписаны.
То есть византийская реплика не могла их подделать, могла только скрыть какие-то.
Ну вот в PBFT это пример. Мы пересекаем кворумы на выбор лидера смены эпохи и репликацию.
Какие-то византийские узлы могут соврать, что у них транзакций нет,
а честный узел обязательно предложит сертификат и докажет новым правилам, что именно его стоит послушать.
А когда мы перешли к permissionless-системам, то стало еще сложнее,
потому что раньше у нас кворумы измерялись в голосах 3 из 5, 4 из 7.
Когда мы перешли в permissionless-модель, то число реплик больше неизвестно,
византийские узлы могут порождать свои византийские копии,
поэтому мы кворумы стали измерять не в голосах, не в участниках, а в процессорах,
ну или другой популярный способ в деньгах, то есть в ресурсах, которые сложно подделать.
Ну можно, не знаю, в дисковой емкости измерять кворумы, то есть такие подходы тоже есть, их много разных.
Ну что ж, про репликацию я не знаю, мне кажется, что это все самое важное, что в голову приходит.
Наверное, на этом уровне можно еще сказать, что мы должны заботиться не просто про...
Мы отказаустойчивости добиваемся не только тем, что мы используем там эти кворумы, алгоритмы,
мы должны подумать еще про отказаустойчивость, но просто на отдельных узлах,
потому что если, скажем, ваша реплика записала на диск снапшот,
потом диск немножко покорраптил этот снапшот, испортил какие-то байтики,
а потом вы скопировали эти байтики по сети на другую машину, то, кажется, у вас теперь три неправильные реплики разломанные.
Поэтому даже если вы используете репликацию, нужно быть очень аккуратным, что вы локализуете какие-то локальные...
Вы не распространяете какие-то локальные избои.
Вот проблема с дисками, проблема с процессором, проблема с проводами,
вот вы о всем этом с памятью должны думать. Вы должны рассчитывать, что в памяти перерачиваются биты,
вы должны думать, что диск портит данные, вы должны считать, что доверять чексумам TCP, IP и Ethernet всем трём разом нельзя.
Если вы им доверяете, то в большом масштабе, ну, может быть, если у вас три реплики,
то, скорее всего, вы, может быть, с этим не столкнетесь.
Если у вас 100 тысяч РСМов, 100 тысяч ПАКСов на десятках, сотнях тысяч узлов,
то в таком масштабе, в масштабе больших кустров любые редкие избои случаются более-менее регулярно.
Поэтому вы должны и на них внимание тоже обращать.
Ну, про это у нас тоже был отдельный семинар, но это, не знаю, как с файловой системой аккуратно работать.
Вот все ваши обещания, которые вы даёте в ПАКСовстве, в РАФте, в любом протоколе, вы должны хранить их вечно, вы не должны забывать их.
Для этого вы пишете их на диск, и нужно аккуратно заботиться, что вы сделали F-Sync, что вы сделали F-Sync в директории,
что вы сделали Write a headlock, который может потранкетиться, потому что апент неатомарный.
Ну, короче, миллион проблем инженерных уже, которые могут влиять.
Когда вы пишете однопоточное приложение, то у вас всё просто.
Если машина сломалась, то значит приложение больше не работает, думать о чём не нужно.
Вот сейчас мы должны думать буквально о том, что в любой момент между любыми двумя инструкциями машина может перезагрузиться,
что-то потерять, и мы должны восстановиться из-за того состояния.
Это требует большого усердия. Я бы сказал, есть такое мнение, Максим Багенко его озвучивал в каком-то интервью,
что хорошо брать на работу в инфраструктуру перфекционистов,
потому что перфекционист понимает, что всё может пойти не так и к этому готовиться.
Тут не работает подход, ну как-нибудь всё там принесёт. Нет.
Если что-то может случиться, то в большом масштабе оно обязательно случится. Нужно к этому готовиться.
Бог с ней с репликации надоело, давайте поговорим про дизайн.
Это была вторая причина, по которой мы используем распределённость, потому что данные не вмещаются всё в одну машину.
Ну и дальше мы говорим, что не беда, разрежем данные на части, и тогда заведём много отдельных.
Возьмём много отказоустойчивых акторов, отдельных машин, которые на самом деле РСМ,
и раздадим им кусочки данных, и тогда всё поместится.
Иногда это не работает, потому что непонятно, как полезны кусочки.
Мы говорили про файловые системы, и там был уровень данных, то есть дерево, iNode, дерево плохо режется на кусочки.
А даже если хорошо режется, потому что это не дерево, а таблица просто, то даже в таком дизайне у нас возникали проблемы.
У нас было занятие, где мы обсуждали, как масштабировать кейвалию хранилища и масштабировать файловую систему.
И та и другая задача сводилась к тому, чтобы найти узкое место в дизайне.
То есть то место, которое не может горизонтально масштабироваться, то место, в которое мы упираемся.
И оба раза это были метаданные.
Ну вот пусть у нас есть сотни тысяч машин, пусть мы разрезали наши данные на...
Открою картинку.
Пусть у нас есть тысячи машин, пусть у нас есть огромное количество таблиц.
Мы нарезали эти таблицы по ключам на диапазоны, раздали их разным машинам,
и теперь эти машины реплицируют эти диапазоны с помощью мультипаксиса.
Этого было мало, потому что когда к вам приходит клиент, вы должны же его направить в одну из машин,
которая является лидером для соответствующего рейнджа.
Для этого нужно просто хранить метаданные, отображение из диапазонов ключей в набор реплик.
Несложно сделать это отказоустойчиво, то есть выбрать группу машин, и пусть она хранит.
Беда в том, что эта группа реплик, этот RSM, который хранит отображение, может переполниться,
если у вас машин слишком много, диапазонов слишком много.
Это узкое место. В файловой системе это мастер, который хранит чанки iNode.
iNode здесь это мастер, который хранит карту кластера, где что лежит.
Проблемы одинаковые, и вроде бы машины можно добавлять, данных можно хранить больше,
а вот метаданные упираются в емкость одного диска.
И мы решали эти проблемы довольно занятным способом.
Мне кажется, что он занятный. В Bigtable как мы это делали?
Мы сказали, что вот это отображение, сам бекты будут кивали у хранилища,
и наши метаданные это тоже отображение исключили значение.
Но почему бы не использовать самих себя для того, чтобы хранить эти данные?
И мы сделали таблицу метаданных, где хранится для каждого таблета пользователя,
для каждого диапазона ключей, какими машинами он обслуживается.
Но вот беда. Чтобы эту таблицу читать, нужно найти таблет, который обслуживает таблет метадаты.
Для этого сделали ещё один уровень иерархии, второй уровень метаданных.
То есть в этом таблете было написано для каждого служебного таблета,
какие машины его обслуживают, и узнав эти машины, можно было пойти в эту таблицу,
прочитать из неё и пойти уже наконец обслужить пользователя.
Но опять та же самая проблема. Как же найти узлы, которые обслуживают этот корневой таблет?
Идея тут в том, что мы с каждым этим уровнем косвенности уменьшали объем метаданных.
И в какой-то момент, но после двух опов, их становится настолько мало,
что они просто умещаются в один RSM.
Причём этот RSM, этот Chabi назывался системой, можно было использовать для разных систем.
Для разных инсталляций Bigtable.
Или использовать его и для Bigtable, и для Colossus, который у нас позже в лекции возникал.
То есть ZooKeeper — это же система, которая нужна не для какой-то конкретной системы,
а ZooKeeper может использоваться разными системами, разными инсталляциями одной системы,
просто в принципе разными системами. Вы там можете Kafka использовать в ClickHouse,
они будут использовать один и тот же ZooKeeper.
Вот здесь, в конце концов, мы сошлись к такому Chabi, это Chabi 1,
и мы здесь снова переиспользуем консенсус.
Как я раньше говорил, что мы переиспользуем только на уровне хранения данных,
а отдельные акторы не используют консенсус.
Но вот здесь та же самая идея.
У нас здесь консенсуса меньше, потому что он есть в Chabi, и он есть...
Простите.
Где-то вот под капотом этого GFS тоже что-то есть.
У нас есть таблица, мы хотим в конце концов прочесть из этой таблицы
какой-то ключ, но мы не знаем, какие машины его обслуживают.
Для этого у нас есть методанные. В этой таблице написано, кто обслуживает твой ключ.
Но чтобы обратиться к этой таблице, ты должен знать, кто обслуживает чтение этой таблицы.
Поэтому ты отступаешь еще на шаг назад.
К чему я этот пример показываю? К тому, что система может хранить свои собственные методанные
и масштабировать. Если система может масштабированно хранить таблицы,
то если вы завели свои методанные в виде таблицы, то почему бы ее в себя не поместить?
И вот эта идея ценно как эти идеи придумывать.
Этот пример иллюстрирует нам такую общую технику, что в дизайне систем распределенных
не то чтобы какие-то новые и альтернативные всему алгоритмы,
вот консенсус — это вот про конкарнси, а систем дизайн — он про какие-то рецепты,
которые на самом деле уже известны людям.
И конкретно в Bigtable мы видим, что принципы, которые уже были придуманы гораздо раньше,
чем Bigtable, и в контексте просто одной машины, можно переиспользовать большим масштабе
в распределенных системах. Но просто одни и те же инженерные идеи.
Вот в самом деле эта же конструкция — это таблица страниц. У нас есть виртуальная память,
в ней есть страницы. Страницы есть данными пользователя, а есть служебные страницы,
которые хранят адреса других страниц. Это, собственно, page table.
И вот этот page table — это вот некоторые бор фиксированной глубины.
И вот этот бор фиксированной глубины.
И также, если этот бор переполнится, в смысле слишком много листьев станет,
то мы сможем достроить еще один уровень иерархии, и все снова станет масштабироваться.
Так же, как и в виртуальной памяти. У вас есть поинтеры, там 64 бита,
используется только 48, потому что таблица страниц имеет четыре уровня.
Если памяти станет слишком много, мы сделаем пятый уровень и будем использовать еще 8 бит.
Ну и точно так же, как в процессоре, который ищет таблицу страниц через регистр,
у нас есть чаби в виде такого отдельного регистра.
И так же, как в виртуальной памяти, процессор каждый раз по этой таблице страниц не ходит.
Если бы он делал вместо одного чтения логического 4 физических, все бы тормозило.
Он просто кэширует то, к чему он обращается часто.
Поэтому клиент в такой системе также точно кэширует адреса таблитов со своими данными
и просто скипывает поход по этой иерархии.
Если его отображение устарело, ну просто он перечитает это все один раз, обновит свой кэш.
То есть идея хорошо известная, мы чувствуем, что здесь та же самая проблема,
поэтому переиспользуем ее. Или скажем, вот LSM-дерево, оно же известно,
его придумали гораздо раньше, чем Bigtable.
А потом просто решили, а почему бы не построить этот LSM не поверх локальной файловой системы,
а поверх распределенной файловой системы?
Просто декрутив консенсус, который нужен для того, чтобы с конкуренцией точек обслуживания справляться.
То есть вот такие идеи, кажется, полезно знать, как устроен компьютер,
потому что потом эти идеи можно переиспользовать в гораздо большем масштабе.
Но в конце концов atomic broadcast это же идея опять из кэшей.
То есть у нас есть ядра, там есть своя память, и эти ядра с памятью связаны
с помощью шины данных, которые упорядочивают все транзакции, которые выполняются этими кэшами.
То есть в большом масштабе все то же самое, что и в маленьком.
В принципе ничего нового, в смысле дизайна.
Ну ладно, иногда все-таки новое придумывают.
Детерминированные транзакции – это довольно неожиданное изобретение,
которое придумали в 2013 году, и не то чтобы раньше так умели.
Долгое время ничего, кроме двухфазного комита, не было.
И еще, мне кажется, очень классный пример на дизайн был про колосус.
Как бы нам найти эти слайды сейчас?
Очень жаль, что про колосус мало что известно, есть только эти маленькие короткие слайды неподробные.
Но мне кажется, что я в лекции достаточно детально рассказал
о каком уровне, на котором это возможно, про то, как масштабируется колосус.
Потому что с ним возникла дополнительная трудность, а именно в колосусе нельзя...
Да, простите, я перепрыгнул.
Мы научились масштабировать key value, потом мы масштабировали файловую систему,
переложив данные в key value.
А потом мы столкнулись с проблемой, что мы хотим масштабировать файловую систему через key value,
но если мы гугл, то наша key value строится поверх файловой системы,
и у нас получается такой цикл.
Но мы этот цикл заменили рекурсией, потому что мы брали большую файловую систему
и хранили данные в метаданной этой системе Bigtable,
который работал с чуть меньшей файловой системой,
которая хранила метаданные в Bigtable, который работал с ещё более маленькой файловой системой.
Ну и, как гугл говорят, объём метаданных у них к данным примерно один в десяти тысячам.
Поэтому, сделав несколько таких итераций, мы сможем уменьшить объём метаданных
с петабайтов, с экзобайтов уже, наверное, до килобайт
и поместить эти килобайты опять в чаби, опять на некоторая точка входа.
Ну и сделать аккуратно, чтобы запись файл не приводила к тому,
что мы проваливаемся через этот бешенный каскад до самого основания, до чаби.
Чаби сошёл бы с ума.
Если мы всё аккуратно сделаем, то мы получаем такой bootstrapping,
и снова система масштабируется через саму себя, только уже более сложным образом.
Ну, как компилятор устроен, примерно так же.
В общем, такие очень неочевидные уроки дизайна.
Конечно, конкретные идеи привязаны к конкретным системам,
а общая идея такая, что просто переиспользуйте своё знание про устройство компьютера.
Вот любая деталь может пригодиться в совершенно другом масштабе.
Что ещё нам может прийти в голову? О чём мы ещё могли бы поговорить?
Не то чтобы было какое-то сложное наблюдение, но мне кажется, что всё уже полезное.
Про то, что при переходе к византийской модели мы в свой набор инструментов, достаточно незатейливый,
добавили ещё одно измерение, а именно криптографию.
Мы про это подробно не говорим. Надеюсь, у вас будет курс криптографии.
Где-нибудь, когда-нибудь вы про это всё узнаете.
Но какие инструменты мы использовали, когда мы перешли к блокчейнам?
Мы использовали цифровые подписи, огромное количество способов криптографических хеш-функций,
и мы совсем не поговорили про такую технику, с помощью которой мы можем доказывать что-то другим узлам,
не раскрывая содержание.
Эта идея, с помощью которой реализуется анонимность в современных блокчейнах,
очень красивая, очень эстроумная.
Переходя от блокчейн, от репликации не византийской к византийской, мы добавляем себе все эти проблемы,
и кажется, что их очень увлекательно решать, и люди до сих пор прямо сейчас этим занимаются.
Не так давно эти доказательства с нулевым расположением не интерактивные придумали
и стали их применять в криптовалютах, чтобы обеспечить анонимность, вместо псевдонимности.
Что еще? Если говорить про инженерные вещи, то мне кажется...
Кое-что еще. Наверное, полезно было бы заметить, как мы иногда обходили некоторые физические ограничения
в наших системах, а именно физические ограничения на коммуникацию.
У нас было два довольно странных примера, довольно радикальных.
Не то чтобы вы их будете часто использовать в своей жизни, но интересно на них обратить внимание.
У нас в биткоине была задача выбора лидера, а в распределённых системах, в базах данных, в спанре
была задача упроточения транзакций. И та и другая задача решается через коммуникацию.
Естественным образом решается через коммуникацию, потому что узлы должны всё-таки о чём-то договориться.
И мы в двух этих частных случаях смогли выдумать совершенно альтернативные решения, которые коммуникация не требовала.
В одном случае это был True Time, в другом случае это был Proof of Work.
И удивительно, что так иногда получается сделать. Обойти накладные расходы, связанные с тем, что узлы системы находятся далеко.
И коммуникация дорогая. В случае True Time мы использовали асинхронизацию часов через спутники, через GPS.
В случае биткоина мы использовали прожигание процессора и криптографичность функций.
Любопытно, как такие совершенно альтернативные инструменты помогают решать распределённые задачи, довольно типичные.
True Time – это, наверное, повод сказать про ещё одну деталь.
А именно, что если мы хотим строить отказоустойчивую систему, то ещё один организующий принцип – разумно как можно больше всего виртуализировать.
True Time – это надёжные часы. Мы не думаем, про то, как они под капотом устроены.
Это целый сервис, и он реализован даже не программно. Это железный сервис.
То есть это спутники, это GPS. А ещё, если мы хотим поддерживать достаточно узкое окно неопределённости на отдельных узлах,
то мы должны ещё и в нашу всю сетевую инфраструктуру тоже встроить протокол синхронизации.
Google научилась, кажется, делать, но, по крайней мере, они написали статью такую.
Она про research, а всё-таки не про production.
Но, тем не менее, они пишут, что если аккуратно покладываться в коробках с проводами
и настроить протокол на этом уровне, на уровне сетевых, на уровне сетей,
то можно добиться синхронизации часов до нанесекунд.
Что в масштабах большого кластера выглядит вообще как безумие, просто нанесекунды.
Мы виртуализируем время, мы, конечно, виртуализируем диски.
И если мы в Google находимся, то мы не думаем просто про диски, они не отказывают,
потому что диски все виртуальные. Мы виртуализируем, в конце концов, сами машины,
а вот актор в Яндекс.ДБ это такая отказоустойчивая единица.
Она последовательная, она надёжная, она последовательная и при этом надёжная.
Она может перезапускаться на разных машинах, и мы не думаем про то, что под ней откажет диск.
Я не знаю.
Если ты контролируешь время передачи данных,
если ты контролируешь все задержки, которые у тебя возникают в буферах,
ты можешь как-то это учитывать. Если ты контролируешь, что у тебя есть разные маршруты,
то проблема с синхронизацией часов была не в том, что долгоданные бегают,
а в том, что они бегают немного по разным путям.
Если у тебя есть симметрия, то ты измеришь время раунд трипа,
но ты не можешь измерить время в одну сторону и в другую сторону.
Там целая статья сложная, не буду ее пересказывать, я сам не понимаю просто,
но там инфраструктура на уровне коробок с проводами рассчитывает,
какие маршруты, какие поддеревья в сети есть, и понимает, что если кто-то отказал,
то нужно что-то делать по-другому.
Не нужно это делать нам, это нужно делать примерно в одном месте,
поскольку все машины находятся либо в Амазоне, либо в Google,
в конце концов никто больше не покупает особо.
По крайней мере, если вы стартап какого-то разумного размера, не слишком большой.
Если вы готовы строить свой ДЦ, строить свою инфраструктуру,
то это отдельная история, но мир централизован довольно сейчас.
Так что некоторые люди умеют, и скорее важно понимать,
на каких абстракциях мы можем строить свои алгоритмы.
Строить в XXI веке алгоритмы на основе дисков сбоенных и сбоенных машин,
это довольно непрактично.
Люди уже научились больше абстракций наворачивать, чтобы было проще в конце концов.
И лишние абстракции, мы видим, иногда приносят некоторое дополнительное удобство.
Абстракция хранения, если мы отделим процессоры от дисков,
то мы можем сделать репликацию более эффективной, используя R&J коды.
Актор — это в смысле модель акторов, где у тебя есть такие последовательные сущности,
которые обмениваются сообщениями асинхронными.
На слайдах янда вздеби написана именно в модели акторов.
Все сущности там являются такими.
Некоторые акторы рождаются, машина умерла, они погибают вместе с ней.
А есть акторы, которые могут возрождаться на другой машине
и убежать с того же места, где они остановились.
Это про модель программирования скорее, не про распределённые системы.
Это не как файбер, это не файбер, это актор.
Это та же область, это конкарнси, это модель конкурентного программирования.
Как ты организуешь свои конкурентные активности?
У каждого актора есть своё собственное состояние, оно не разделяется никогда,
оно только к одному актору привязано, и актор общается с сообщениями с другими акторами.
Это такой подход к программированию распределённых конкурентных систем,
который не то чтобы оптимальный, но у него есть много достоинств.
Как и у любого другого подхода, есть много достоинств своих.
В общем, ты выбираешь какой-то.
Раз уж ты про код спросил, давай скажем про код.
В нашем курсе мы программируем всё не на акторах, мы программируем всё на файберах.
И я надеюсь, что из тех домашних, которые вы написали,
понятно, что можно сделать лучше, но по крайней мере скажу свою задумку,
она однажды воплотится в реальность до конца.
Мы должны были увидеть, что, во-первых, разумно строить весь код на абстракциях,
потому что тогда он декомпозируется лучше.
Если вы работаете через STD file system, запускаете треды и пишете какой-то гигантский монорит,
то у вас есть разные проблемы, потому что всё сложно тестировать.
Если вы пишете код на абстракциях, то в смысле тестирования, вы можете позволить себе гораздо больше.
Вы можете тестировать конкурентность. Вы можете тестировать работу с файловой системой.
Вы можете тестировать ваш РПС протокол.
А если вы тестируете вот дет你說, то вы можете проверить гораздо больше частных случаев.
Если у вас совсем много желания, то вы можете в конце концов построить для всей вашей системы симулятор
и детерминированно тестировать уже не просто отдельные компоненты, а просто весь ваш алгоритм, все ваши узлы.
Это очень удобно, потому что представьте, что вы живёте в мире, где у вас нет этого симулятора,
оно так почти все и живут, и вы отложиваете какой-то баг.
Сейчас у нас есть один общий лог, это довольно удобно прочитал, то, что происходило раньше в логе,
предшествует другим событиям, и всё можно в одном текстовом файле посмотреть.
Если вы работаете в системе распределённой и недетерминированной, разумеется, мир недетерминирован,
то какие у вас проблемы? У вас есть два события, они произошли на разных узлах, вы знаете про один порядок этих событий,
а в логе они по часам порядочно иначе, потому что часы не синхронизированы.
Вам нужно сливать много логов, там таймстемпы разные, и не совсем понятно, как одно с другим сопоставлять.
Кроме того, вы не можете просто воспроизвести ошибку, потому что если случится раз в день,
то как вы собираетесь её воспроизводить? Не знаю, терпеливо ждать?
Или систему ломать снаружи, в смысле, задерживать сообщение, что-то ещё делать?
Но это сложно. Кроме того, если вы работаете таким подходом, где вы детерминированно всё можете воспроизводить,
вы можете ещё и... Мысль потеряла.
Ну ладно, потом найду. Так бывает. Другая причина, закруглим эту мысль, понятно, что детерминизм для тестирования очень приятен.
Другая польза от абстракций состоит в том, что вы просто очень аккуратно можете сфумулировать гарантии той или иной абстракции.
Вот вы работаете с RPC-протоколом, и у него какие-то гарантии.
Но вот эти гарантии строятся на основе гарантии протокола более низкого уровня, где просто шина сообщений.
У вас асинхронная отправка сообщения, получение сообщений без всякой семантики.
Имея абстракцию для такого уровня, вы можете для неё сфумулировать, какие же у вас гарантии в сети.
Вы ожидаете, что сообщение доставится или не ожидаете? Оно доставится однажды или может несколько раз доставиться?
Что если соединение порвётся? Вообще есть ли у вас понятие соединения?
Вот такой дизайн разумен ещё и потому, что вы можете подобрать максимально точные и максимально разумные гарантии
для этих отдельных компонентов, для отдельных границ с внешним миром.
Мы не думаем про TCP, когда мы пишем код. Мы думаем про сетевой транспорт с понятными гарантиями.
Мы отправляем сообщение, в смысле мы в нашем курсе, и либо с другой стороны вызовется handle message,
либо с нашей стороны вызовется handle disconnect.
Либо то, либо другое, либо, по крайней мере, что-то одно случится.
И вот всё, мы понимаем, чего мы ожидаем от сети.
Нам больше не нужно думать про гарантии TCP, которые могут быть очень сложными.
Это так делать разумно, так жить потом проще.
Я надеюсь, что мы увидели ещё такую важную деталь, что в распределённых системах важно иметь инструменты для того,
чтобы анализировать их поведение. Речь тут не про симулятор, я уже про это сказал,
а про то, чтобы в принципе понимать, как система себя ведёт, даже если она запущена в продакшене и всё недетерминировано.
Это речь про observability.
Observability – это, как правило, три разных измерения.
Это логирование, это tracing и это метрики.
Понятно, зачем нужны метрики для того, чтобы считать, сколько у вас переключений контекста, сколько у вас локаций,
сколько у вас запросов в секунду. Зачем нужен логирование? Зачем нужен tracing?
Для того, чтобы просто связывать разные машины друг с другом.
У вас много машин, если у вас система сложная, там есть какие-то подсистемы, какие-то микросервисы условно,
и клиент приходит и начинает блуждать по этим машинам, а внутри машин по файберам,
и какой-то ещё работы выполняет, конвейеры ассинхронные запускаются.
Хорошо бы понимать, как запрос выполнялся, на каких машинах вообще, и сколько времени он где проводил.
По логам эта задача не решается, потому что лог – это проекция на одну машину системы,
а trace – это проекция на один запрос.
И для трейсинга вам хочется рисовать... Давайте сейчас найдём какую-нибудь картинку.
Вот какие-то подобные конструкции.
Но не судите строго, случайная картинка из интернета.
Короче, вам хочется рисовать такие разноцветные колбаски, которые отвечают за какие-то стадии на каких-то машинах.
И хорошо бы вы могли выбрать ваш ID-запрос пользователя и нарисовать картинку,
как же ваш запрос по разным машинам путешествовал.
У нас с этим было довольно плохо, потому что из инструментов у нас было только trace ID и контексты.
В реальности есть помимо trace ID, который красит все действия, все события, связанные с одним запросом в один цвет,
есть ещё, как правило, и scope ID.
То есть вы можете...
Есть scoping, и вы можете не просто цепочки отслеживать действия,
вы можете ещё вкладывать одни действия в другие в пределах одной машины.
Ну или в пределах разных машин.
В общем, спаны и trace ID – это вся картинка, а спаны – это отдельная горизонтальная колбаска.
И вам нужно и то, и другое для того, чтобы такие картинки в итоге из системы забирать.
Вот без этого понимательная система работает довольно сложно.
У нас был трейсинг такой простой, мы рисовали асмысле иерархию именно.
Для этого тоже нужны инструменты.
Понятно, что по логу можно информацию взять при желании, но просто это другое измерение, на которое ты проецируешь другая ось.
Это ось машины, а тут ось запроса.
И для этого у тебя должны быть отдельные инструменты.
Они появятся в библиотеке с логингом Timber, который назывался у нас.
Она как раз про это и должна в конце концов быть.
То есть там должны быть инструменты для трейсинга.
В целом, я надеюсь, принцип дизайна нашего кода должен быть понятен.
У нас есть отдельная конкарнсия, у нас есть отдельная сеть IRPC,
у нас есть отдельная персистентность, то есть файловая система, write-head-log, логи RAF-to-Paxos,
и у нас есть обсервабилити отдельно, и у нас есть еще один компонент – это сериализация.
Мы использовали библиотеку, которая под C++ заточена.
Мне кажется, что ее нужно закопать, в смысле не библиотеку,
а то, что мы ее используем. Перейти на протобуф – это будет с одной стороны больнее немножко,
а с другой стороны будет менее больно, потому что меньше ошибок можно на границе допустить.
То есть код станет тупее, больше, но проще, надежнее.
В этом месте вы могли бы вынести, что сериализация устроена…
Во-первых, она бывает в бинарных текстах для разных целей, для отладки и для продакшена.
Что мы, наверное, в меньшей степени вынесли?
Это то, что есть, во-первых, разные подходы к сериализации.
Есть сериализация, где мы данные переупаковываем для того, чтобы отправить их в провод,
а есть сериализация Zerocopy, когда мы представляем данные в машине, в памяти и в проводе одним и тем же образом,
и можно делать намного эффективнее.
А еще, что сериализация тесно связана с фиксированием схемы протокола.
То есть у нас есть сообщение Pentantris, и оно описывается не в коде,
оно описывается в специальном протофайле, где написано, какие поля, каких типов.
А дальше Protobuf старается сделать так, чтобы эти схемы можно было аккуратно эволюционировать,
добавлять там и ударять в какие-то поля, и делать это так, чтобы другие узлы,
которые живут со старыми схемами, не поломались еще, потому что от АМАР нам все везде нельзя.
В общем, есть такой набор библиотек под задач, то есть декомпозиция уже не системы, а кода системы.
Еще раз перечислю, конкарнсия, сеть, конкарнсия, сеть, персистентность, сериализация, обсервабилити.
Все это по возможности друг от друга отрезано.
Там где-то зависит, но в целом почти перпендикулярно.
Тут уместно вспомнить, что язык Go, который мы не использовали, потому что он фиксирует runtime,
это runtime не кастомизируется, но который мы могли бы использовать, если мы пишем распределенную систему,
был задизайнен в принципе ровно для таких задач.
Вот этот язык, он про конкарнсия, про трейсинг, потому что есть все эти контексты,
про сериализацию, про RPC и так далее.
Вот язык создан именно для того, чтобы программировать вот такой вот код,
и по возможности делать это более надежно, чем C++, потому что сборка мусора,
потому что больше интеграции в сам язык, ну и много всего другого.
Я не знаю. Я думал, ты скажешь на раз, потому что когда перепишешь себе вы, то говорят обычно на раз.
Нет, я пока не планирую этого делать, потому что я не вижу большого пользы, кроме того, что тебя будет...
Вообще QAO это такая штука, которую можно, не знаю, забыть написать.
Ты пишешь там mutex log, а перед ним QAO должен стоять.
Неприятная ситуация.
Короче, я не вижу большого пользы, потому что в корутинах C++, к сожалению,
есть некоторый недетерминизм, связанный с локациями.
И непонятно просто, что он здесь даст.
Он может дать то, что там, не знаю, можно библиотеку Unifex использовать,
Unified Executors, которые пишут сейчас в Facebook, ну или кто сейчас пишет ее, даже непонятно.
Но прям прямой такой очевидный пользы от корутин я не вижу, кроме того, что будут корутины.
Ну да, немножко станет проще, потому что сейчас в рантайме узла есть такой компонент,
там, локатор стеков, грубо говоря, там ресурс менеджер он называется,
ну или как это называется, вот его бы не было.
Но это не самая устрая проблема, то есть это не то, чего не хватает в первую очередь.
Так что не знаю.
Да, я этим займусь после того, как я съезжу в Молмаск.
Я, к сожалению, погряз в реакциях сейчас, в зачетах во всем этом,
но вот я недельку проваляюсь на печи, а потом буду дописывать.
Ну это первое, что мне хочется сделать сейчас.
Ещё раз — код будет написан так, что это могут быть разные машины.
Если ты хочешь запустить это на одной машине, ну запусти три виртуалки.
В конце концов, у Джевсон так и работает.
Ты запускаешь три виртуалки, descрепляешь на Turk,
нуton и идем же через один месяц если даже Whilst pancakes.
В tinted, это только полетел х Climate night.
И код простоitute.
В конце концов, в Jefferson так и работает. Ты запускаешь три виртуальные машины,
но какая разница, это разные физические машины или разные виртуальные машины
на разных физических или на одной? Ладно, разница, конечно, есть.
Но еще раз, сам код никак этого зависеть не будет. Сам код, который мы писали,
не должен этого зависеть никак, то есть полагаться на то, что это один поток.
Если полагается, то это плохой код. Если он написан так, что он не полагается,
а все бюллетей, которые поверх runtime не полагаются, то этот код можно запускать
как отдельные процессы на одной машине, как отдельные процессы на разных машинах.
Следующий шаг он такой. В принципе, ничего не должно мешать
напрямую переехать из симулятора в определенный мир.
Собственно, в этом и задумка всей этой конструкции с симулятором, с runtime и со всем.
Потому что код, в котором полностью кастомизируется runtime, это очень сложный код.
Библиотеку с конкарнсом можно было во многих местах написать гораздо проще,
если предполагать, что у нас не будет определенной симуляции,
что все-таки у нас в конце концов будут потоки. Почти нет статиков,
потому что статики ломают что-то. С другой стороны, код статики это довольно альтернативно,
яфи часе плюс-плюс. Кажется, что можно и без нее жить, и возможно даже лучше всем будет.
Поэтому тут дело вкуса, мне кажется. Я запутал тебя, наверное. Ответ – да.
Что это значит? Какую задачу ты хочешь решать?
Разумеется, полезные вещи в блокчейнах пишут. Задача это другие.
Наши системы, которые были в первой половине курса, они про пропускную способность,
про много данных, про много запросов. Блокчейн не про много запросов,
но биткоин обрабатывает 7 транзакций в секунду. В Google внутри происходит 10-10 RPC вызовов в секунду.
Это немного разный масштаб. 10-10 и 7. Поэтому задача разумеется разная.
Задача какие-то – аукционы, выборы, что-то подобное.
Есть контракт, я давно про него читал, называется «Король эфира»,
где есть такой трон, можно на него залезть и заблокировать большую сумму денег.
Если ты хочешь залезть сам, ты должен поставить большую сумму.
И если тебя переплюну, то деньги тебе вернутся. Ну или нет.
Или, может быть, ты так останешься самым главным.
Кто в итоге больше поставит? Можно делать бессмысленные вещи, можно делать что-то более полезное.
Но это совсем другие задачи. Сам инструкцион-сет криптографический.
Он не про то, чтобы данные обрабатывать, а про то, чтобы решить подобные задачи.
Конечно же, сам эфириум – это распределенная система.
Что ты имеешь в виду? Состояние эфириума – это набор отображений,
дерево, бор, склеенный из меркла три.
И у эфириума цель – экономить это состояние и, по возможности, его не слишком часто обновлять.
В эфириуме не то, чтобы задачи похожи. Задачи не похожие, а проблемы похожие,
потому что блокчейнам пока масштабироваться не особо нужно.
Есть биткоин, он порождает блок раз в 10 минут, этот блок размером 1 мегабайт.
И мы знаем, что сейчас блокчейн биткоина весит 300 гигабайт, через год он будет весить 400,
через два года – 500, потому что просто фиксированная скорость публикации блоков.
Дисков, скорее всего, хватит на это, на такой рост.
Но если мы вдруг научимся делать блокчейны без пухфорка, которые будут хорошо масштабироваться,
станут популярны, то в них транзакций станет больше, блоков станет больше,
на диске они перестанут помещаться, и тогда нужно будет шардировать.
Эфириум как раз занимается сейчас следующей своей большой итерацией,
и там должна быть финализация блоков, и там есть планы про масштабирование,
про то, чтобы шардировать блокчейн. В этом смысле задачи похожие решаются.
Блокчейны, скорее, к ним с некоторым лагом приходят,
потому что сейчас рейт слишком низкий для того, чтобы эти проблемы возникали.
Но вот это воспроизводится.
Про транзакции мы тоже в блокчейнах говорили, что там какие-то похожие протоколы,
не похожие, но, опять, не похожие по конкретным механикам,
но похожие по структуре, по фазам, по лагике своей.
Хороший вопрос. Я про это ничего не знаю.
Можно придумать какие-то наивные способы.
В конце концов, кто тебе мешает?
Я просто в контексте этого курса думал, что можно было бы сделать.
Можно написать узел, который перехватывает сообщения по сети,
знает, что у них какая-то структура, то есть схема фиксирована,
и ты берешь, не знаю, сохраняешь старые сообщения,
передправляешь их другим узлам, делаешь какие-то вещи.
А так, чтобы тестировать в смысле координированная атака злоумышленников...
Либра паста, которую Facebook делала, а какая-то тестирующая система у них.
Как инспортируется это? Очень-очень сложно.
Ну, не то, чтобы... сейчас.
Нет, ну, понятно, что многие такие вещи можно делать.
То есть это просто такая проблема в таких системах не в том,
что где-то диск пропадет, а в том, что большая часть, значимая часть сети
будет вести себя как-то координированно и против нас.
То есть это какая-то сложная стратегия должна быть.
Она вот таким точечным fault injection не решается.
То есть что-то можно поймать, разумеется, оно...
Ну, не знаю, статьи пишут, доказывают что-то.
Прямо чтобы код тестировать так автоматом, я, честно говоря, не слышал.
А вот этих истих можно придумать, конечно, миллион...
Это скорее просто текущие подходы.
Что мне кажется, что если мне в голову ничего не приходит,
то самое важное я сказал.
То, что придет позже, было не очень важно.
Это, наверное, и есть.
Если у нас вопросов не осталось, мыслей, каких-то идей, чего-нибудь,
то спасибо вам большое, что были с нами.
Приходите еще завтра.
Мы проверим, что вы усвоили.

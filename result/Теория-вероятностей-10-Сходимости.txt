Так, мы в прошлый раз остановились на критерии сходимости
почти-наверное.
Вот я его выписал, это в прошлый раз мы записали.
И здесь пониже, ну, вам результат знаком, я его очень кратко
повторю, потому что мы из него серединку используем.
Значит, и вот здесь пониже я записал условия, кси
энное не сходится к си почти-наверное.
Правильно, да?
Существует n, для любого n найдется, о, прошу прощения,
n малое, больше ли равное n большое, такое, что наша
омега удовлетворяет вот такому условию, да, вот
не сходится.
Вот, значит, ну и давайте, так сказать, просто введем
обозначение.
Вот это множество обозначим b, n, m, а вот это множество
обозначим c, m.
Ну и давайте пусть, пусть, пусть, омега такие, что кси
n от омега, точнее говоря, вероятность не сходится
к си от омега, пусть это равно нулю.
Отсюда следует, вот смотрим на вот эту запись, что вероятность
счетного объединения c, m тоже равна нулю.
Отсюда следует, что вероятность c, m-того равно нулю для любого
m.
Первое наше, так сказать, рассуждение.
Второе наше, или на что обратим внимание, что для любого
m множество b, m, 1, стягиваются, вот по индексу n большое
является стягивающимися.
И p, c, m для любого m, которое с одной стороны равно нулю,
с другой стороны равно пределу вероятности b, n, m, когда n стремится
к бесконечности.
Теорема непрерывности вероятности.
Ну и давайте этот предел или p, b, n, выпишем так сказать
явно.
p, b, n, это вероятность события объединения по n больше равно
n, ω такие что, к си n от омега минус к си от омега больше
единицы на m, больше единицы на m, и это в точности равно,
вот здесь я напишу, а потом поясню, вероятности омега
таких что, супремум к си n от омега минус к си от омега
больше единицы на m, где супремум берется, прошу прощения,
пока больше или равно n.
Равенство вероятности, потому что эти множества тождественны.
Если вот этот супремум больше, значит на каком-то
индексе k при каком-то конкретном омеге вот эта величина
больше единицы на m и значит она войдет вот сюда и в обратную
сторону.
Ну собственно, тот хвостик, который нам как бы понадобится.
Вот, ну и видим, вероятность вот этого стремится к нулю
на основании этого, а вот это собственно с точностью
до замены единицы на m на эпсилон, что вопрос технический,
завершает доказательство в одну сторону и в обратную
сторону просто с конца.
То есть предполагаете, что вот это стремится к нулю
и назад-назад-назад приходите вот к тому, что вероятность
вот этого множества равна нулю.
Значит еще раз повторюсь, поскольку результат вам
известный, нам на самом деле надо вот это равенство,
значит считаем, что с этим нам все понятно.
Теперь у нас, я вот тут выписал, остались там хвостики
с прошлой лекции.
Мы с вами не доказали, что исходимость и почти наверное
следует исходимость по вероятности, а также две
условных исходимости.
Быстрая исходимость по вероятности, которая приводит
к тому, что исходимость и пиратность следует исходимость
почти наверное, и быстрая исходимость порядка R, из
которой тоже, если она имеет место, то из исходимости
порядка R следует исходимость почти наверное.
Ну давайте мы это по-быстрому, теперь это по-быстрому.
Значит смотрите, вот эта вот вероятность, вот позволю
себе вот так вот сделать, вот так вот написать, больше
или равна, это вероятность объединения, поэтому по крайней
мере это вероятность больше вероятности одного члена,
а именно с индексом n большое, n большое.
Ну позволю себе епсилон написать, вот, ну и поскольку
это стремится к нулю, то это стремится к нулю при
большом стремящемся к нулю бесконечности, а это по
определению исходимость по вероятности.
Дальше, с другой стороны, вот можно теперь вот так
вот напишу, с другой стороны, вот эта вероятность, поскольку
эта вероятность объединения, она меньше или равна суммы
вероятностей отдельных множеств, ну тоже себе
тут позволю уже без обега написать, кси n минус кси
больше епсилон, n от n большого до бесконечности.
Значит, если вот это выражение, если вот знак вопроса поставлю,
стремится к нулю при n большом, стремящемся к бесконечности,
то тогда вероятность вот этого множества стремится
к бесконечности, что означает исходимость почти наверная.
Но для того, чтобы вот это хвостик ряда, поэтому если
мы потребуем, чтобы вот этот ряд сходился, вот смотрите
вот здесь, то его хвостик будет стремиться к нулю,
то есть если вот этот ряд сходится, то его остаток
стремится к нулю, а это и означает, что выполнил
сходимость почти наверная.
Ну и еще буквально один шаг.
Позвольте я вот это сотру теперь и напишу.
По неравенству Чебушева меньше или равно 1 на епсилон
в степени r сумма n равно от n большое до бесконечности,
вероятность того, что кси, ой, невероятность,
прошу прощения, математическое ожидание модуля кси n минус
кси в степени r по неравенству Чебушева, вот это неравенство.
Правильно, да?
Ну и те же рассуждения, для того чтобы вот эта величина
стремилась к нулю, достаточно, чтобы вот этот ряд сходился,
то есть имела место так называемая быстрая сходимость
порядка r.
Из быстрой сходимости порядка r следует сходимость почти
наверная, и из быстрой сходимости по вероятности
тоже следует сходимость почти наверная.
Вот, нет вопросов?
Но это мы как бы добили хвостик с прошлого раза,
ну а теперь двинемся дальше.
Вот это я сотру.
Что еще нам нужно знать про эти типы сходимости?
Оказывается, что все четыре введенных у нас типа сходимости
случайных последовательств являются фундаментальными
покоши.
Ну, где-то это очень просто доказывается.
Значит, мы с вами для примера, и ввиду того, что будем этим
непосредственно пользоваться, докажем это для сходимости
почти наверная.
Значит, с одной стороны, вселенная сходится к С почти
наверная, а сходимость покоши я выпишу следующим
образом.
Так, вот тут стер, надо было написать другую формулировочку.
Я сейчас воспользуюсь чуть-чуть, может, менее привычной
формулировкой критерии сходимости почти наверная.
Я ему запишу так.
Вероятность того, supremum по k больше или равно нулю
кси n плюс k минус кси больше епсилон стремится к нулю.
Вот видите, просто вот так вот, индекс чуть по-другому
представил.
Это обычная сходимость.
То есть, я напишу, что вот сходимость почти наверная
– это то же самое, что вероятность того, что supremum k больше
равно нулю кси n плюс k минус кси больше епсилон стремится
к нулю.
А сходимость покоши мы так определим, supremum, пусть
также будет k больше равно нулю, кси n плюс k минус кси
больше епсилон, стремится к нулю при n, стремящихся
к бесконечности.
А вот так определим сходимость покоши.
Ну и вот оказывается, что один следует из другого,
как и для числовых последовательностей.
Из обычной сходимости следует сходимость покоши, и сходимость
покоши следует обычная сходимость.
Давайте мы этот факт для сходимости почти наверная
докажем.
Хотя, еще раз повторюсь, это имеет место для всех
типов сходимости.
А, извините, прошу прощения, конечно, прошу прощения.
Забыл.
Так.
Ну, в одну сторону совсем просто, то есть пусть есть
обычная сходимость, докажем, что имеет место сходимость
покоши.
Для этого рассмотрим вот такую разность по модулю,
которая меньше или равна.
Ну и, соответственно, я не буду переписывать, внизу
напишу.
Здесь возьму supremum k больше равно нуля, здесь возьму
supremum k больше равно нуля, а вот эту вот заменю на
вот этот supremum.
То есть напишу, что это меньше или равно, меньше или равно
двух supremum'ов k больше равно нуля, кси n плюс k минус
кси.
Законно, да.
Ну и отсюда видно, что если вот этот supremum превосходит
епсилон, то этот тоже превосходит епсилон.
Ну и как бы получается, что из сходимости обычной
почти, наверное, следует сходимость покоши.
Вот со вторым в обратную сторону чуть по занудливей,
ну так же, как и для числовых последовательностей.
Проблема в чем состоит в обратную сторону.
Что насуществование предела доказать, да, то есть доказать,
что из выполнения этого свойства, если говорить
про числовые последовательности, следует существование
ну предела вот этого, ну кси, грубо говоря, да.
Так, ну давайте сделаем это следующим образом.
Давайте введем специальную случайную величину, эта
Прям определимая так, это supremum k больше равно нуля
кси n плюс k минус кси n.
Для удобства.
Значит, тогда что вот это условие означает, если
мы так ввели такую случайную величину?
И это означает, что эта n отсюда следует сходится
по вероятности к нулю, правильно?
А отсюда следует, по теории Мириса, что существует такая
подпоследовательность nt такая, что эта nt стремится
к нулю почти наверно, правильно, да?
Давайте просто чуть подробнее напишем, что это значит,
вот это.
Это значит, что вероятностная мера Омега таких, что supremum
кси n plus k от Омега минус кси nt от Омега, ой, да, кси
t от Омега больше Эпсилон, стремится к нулю, точнее говоря, равна
единице.
Извините, supremum стремится к нулю, и эта вероятность
равна единице, вот, вот так, правильно, да, согласны?
То есть мощность Омега таких, что вот этот supremum стремится
к нулю, имеет мощность единицы, это множество, вероятность
единицы.
Так, а скажите, пожалуйста, если Омега фиксирована,
то вот эти последствия превращаются в числовые,
что вот это означает?
Ну, это просто такая немножко специфичная запись сходимости
по коши для числовых последовательств, да, то есть для каждого
фиксирована Омега из некого множества, единичная мера,
правда.
Существует такое число кси от Омега.
Для любого Омега существует, я его совершенно случайно
обозначу кси от Омега.
Вот такое, что кси НТ от Омега сходится кси от Омега
для любого множества из вот таких, да, то есть на множестве
единичной меры для каждого Омега из этого множества
кси НТ от Омега сходится кси от Омега, что означает
Какую сходимость?
Почти наверно, но возникает вопрос, мы когда говорим
о сходимости почти наверно, мы имеем в виду, что предел
почти наверно, это случайная величина, вот это вот таким
образом, все-таки довольно хитро построили, да, взяли
каждый Омега, нашли предел как числовая последовательсть,
получилось некое отображение, это будет случайная величина-то?
Тоесть свойство говорит о том, что это случайная
величина, это поточный предел измеримых функций, то
есть то, что мы построили, это корректно, это случайная
величина.
Ну вот, мы нашли вот эту случайную величину, к которой
все должно стремиться, теперь осталось так сказать проделать
некоторые, ну там не знаю, технические, не технические
ну некие выкладки, давайте возьмем кси n плюс k минус
кси, и тут прибавим, вычтем, кси n, это вот это n, и еще прибавим,
вычтем, кси nt, это не случайная nt, а из вот этой вот последовательности,
и еще нам будет удобно, а мы можем это сделать, ничто
нас не ограничивает, nt будем всегда выбирать больше
n.
И тогда эта разность, меньше или равна, кси n плюс k минус
кси n, плюс кси nt минус кси n, и плюс кси минус кси nt.
Так, кси n каплин, кси, два раза кси n и два раза кси
nt, правильно, да?
Теперь беру супремум, значит беру супремум по k больше
равно нуля, k больше равно нуля, он меньше, ну во-первых
супремум по кси n плюс k минус кси n, а вот это я заменю
на такой же супремум, поскольку nt больше n, значит nt это какой-то
из этих, заменю на супремум, хуже не станет, два супремума
пока больше равно нуля, ну а это пока без изменений.
Так, вот это я с вашего позволения сотру, можно.
Так, вероятность того, что супремум кси n плюс k минус
кси минус кси больше эпсилон, пока больше равно нуля, меньше
или равен вероятности того, что два супремума кси n
плюс k минус кси n плюс кси минус кси nt больше эпсилон,
правильно, да?
Ну а теперь прием, который вам, наверное, известен.
Сумма двух положительных чисел больше эпсилон, это
значит, что хотя бы одно из них больше эпсилон пополам,
то есть это меньше или равно вероятности объединения
вот таких событий.
Два супремума больше эпсилон пополам или кси минус кси
nt больше эпсилон пополам.
Меньше или равно, всегда объединение меньше равно
суммы, меньше равно вероятности два супремума
плюс вероятность того, что кси минус кси nt больше
эпсилон пополам.
Так, вот эта штука стремится к нулю ровно потому, что
у нас имеет место сходимость по каши.
А здесь вспоминаем, что, что за последствия нт?
На этой последовательности кси nt сходится почти наверно
к си, а значит она сходится по вероятности и значит
эта штука стремится к нулю, но не nt пишу, а n стремится
к бесконечности.
Что мне позволяет так написать?
Способ задания nt, мы его всегда брали больше n, видите?
Ну вот, мы с вами, собственно, закончили доказательство
такого, как потом увидим, очень важного факта, состоящего
в том, что сходимость почти наверно обладает фундаментально
по каши.
Еще раз скажу, что это имеет место и для всех остальных
видов сходимости, ну и там в качестве упражнения
можно, конечно, попробовать.
Так, давайте теперь вот это, ну, наверное, уже теперь
вот здесь можно стереть, да?
Значит, следующий факт или особенность сходимости,
которую нам нужно знать, это так называемое свойство
наследования сходимости.
Пусть у нас есть некая функция жиатекс, непрерывная, ну
и бареллевская, конечно же.
Но если у нас она непрерывная, что мы можем утверждать?
Что если хн сходится к х, то отсюда следует, что
ж от хн сходится к ж от х, правильно, да?
Ну и давайте проверим, есть ли такое свойство у сходимости.
Ну, возьмем сходимость почти наверная, например.
Ксин сходится к си почти наверная.
Когда мы можем записать аналогичное включение,
а именно, омега такие, что ксин от омега сходится
к си от омега, а мы знаем, что вероятность этого события
единица, потому что есть сходимость почти наверная.
Но, тем не менее, такое событие, влечет событие ж ксин от
омега сходится ж кси от омега.
Веду такое включение, вероятность вот этого события меньше
или равна вероятности вот этого, но вероятность
вот этого единицы, значит вероятность вот этого единицы.
И мы с вами получаем такое свойство, что если хн сходится
к си почти наверная, то для непрерывной функции
ж от ксинного будет сходиться к ж от кси.
Поэтическое название – свойство наследования сходимости.
А, сходится, кстати, в том же смысле почти наверная.
Вот, свойство наследования сходимости.
Ну, понятно, что ж может быть в принципе разрывно,
только она множество меры ноль.
Значит, для сходимости почти наверная наследование
имеет место.
Свойство наследования сходимости имеет место.
Ну, давайте посмотрим, что будет для вероятности,
например.
Для сходимости по вероятности.
Для сходимости по вероятности.
Хн сходится к си по вероятности.
Следует ли отсюда, пока знак вопрос поставим,
что ж от ксинного будет сходиться к ж от кси по вероятности?
Ну, давайте предположим, что это не так.
Пусть ж от ксиного не сходится по вероятности ж от кси.
Что это значит?
Напишу одну из интерпретаций, что это значит.
Это значит, что существуют два числа епсилон дельта
больше нуля, а также подпоследовательность n-каты такая что?
Вероятность того, что ж от кси n-катого минус ж от
кси больше епсилон больше дельта.
Вот посмотрите, правильно, да?
То есть я вот так записал условия отсутствия сходимости.
Через выделение по определению, существует, что для любого
n-большого всегда найдется такой, который дальше от
предела по расстоянию больше, чем епсилон.
Вот те, которые всегда найдутся, мне с ними и построила
такую последовательность.
Так Po吃 такая последовательность.
Хорошо, но в то же время кси от n-каты сходится кси по вероятности,
потому что это последовательность сходящейся последовательности,
любая подпоследовательность.
Но если и от сessedly от n-ка сходится кси по вероятности,
отсюда следует, что существует последовательность Б sam.
последовательность k,t
которая включена в последовательность
n,k
такая что
c,g,c
а ну да, для начала такая что
c,k,t
сходится к
ci почти наверное
в теории марриса да
выделил под последовательность которая сходится по
вероятности и из нее выделил вторую под последовательность
которая сходится почти наверное но если
с180 сходится и и XISH innerhalb
то что отсюда следует
что g
от x и к
сходится к g от x, почти, наверное, по ранее доказанному наследованию
сходимости, почти, наверное. Правильно? Но для любого kt, который является индексом из nt,
выполнено а, и значит отсюда уже следует, что g от x кt сходится g от x по вероятности. Правильно,
да? Но для любого kt, который из nt и nk последовательности, а для последовательности nk,
для каждого члена последовательности выполнено вот такое неравенство, это означает, что она не
может сходиться по вероятности. Получили противоречие. Значит, наше исходное положение
неверно, что не выполнено. Убираем знак вопроса и заключаем, что свойство наследования сходимости
выполнено и для сходимости по вероятности. Нетрудно убедиться, что для сходимости порядка r
свойство наследования не выполнено. То есть это не универсальное свойство.
Вот соответствующий пример довольно несложно построить, если кто заинтересуется,
полюбопытствует, то с легкостью его построит. Итак, значит, мы с вами установили свойство
наследования сходимости, для сходимости почти наверное и сходимости по вероятности.
Следующий наш как бы факт или утверждение, связанный со сходимостью,
это будет у нас теорема Слуцкого. Значит, Слуцкий наш соотечественник,
математик первой половины прошлого века, коллега Колмогорова, вообще-то являлся специалистом в
области экономической статистики и там у него ряд как бы важных работ связан с экономикой,
но тем не менее оставил след и в математической статистике и в теории вероятности отметился
именной теоремой. Значит, теорема выглядит следующим образом. Пусть кси n-ное сходится кси
по распределению, а это n-ное сходится к константе С по вероятности. Тогда отсюда следует, во-первых,
что кси n плюс это n сходится по распределению кси плюс с, а кси n умножить на это n сходится
по распределению c кси. Вот утверждение теоремы Слуцкого. Значит, мы с вами докажем одну часть из-за
нехватки времени, но как бы логика доказательства прослеживается. В принципе, вторую можно доказать
практически по аналогии. Есть, правда, кое-какие тонкости. Так, ну давайте, значит, первый факт,
который я хочу анонсировать, состоит в следующем, что в условиях теоремы кси n-ное плюс с сходится
по распределению кси плюс с. Первый-то реальный факт. Ну, позвольте, я не буду доказывать. Его
можно доказать непосредственно из-за определения или рассмотрев характеристические функции.
Поскольку кси n-ое сходится кси по распределению, то функция распределения кси n-ого сходится к
функции распределения кси, ну а константа, как мы знаем, это всегда множитель. Вот, теперь я выпишу,
можно сказать, ключевое неравенство. Весьма простое, но как бы здесь оно нам
очень сильно поможет. Пусть у нас есть две случайные величины x и y. Это случайные величины.
Тогда имеет место вот такое включение. Событие y меньше a влечет событие x меньше a плюс
епсилон и одновременно с этим x минус y по модулю больше епсилон. И это имеет место для любого
епсилона больше нуля. Ну и а, естественно, любых. Ничего сложного, нарисовать плоскость, посмотреть,
так сказать, вот эти области, ну и станет ясно, что слегку перекрывается. То есть,
простое соотношение, надо было догадаться, так сказать, ну его применить. Ну и, собственно,
теперь, честно говоря, дело техники дальше. Теперь ничего не перепутать. Так. Ну давайте,
значит, первый случай, ну как бы два случая нам понадобятся. Первый случай в качестве y возьмем
xn плюс это n. В качестве x возьмем xn плюс c. Ну и все. И выпишем, что получится. Выпишу в
вероятностях. Вероятность того, что xn плюс это n меньше a, меньше ли равна вероятности того,
что xn плюс c меньше a плюс епсилон, плюс вероятность того, их разность будет, представляете,
это n минус c больше епсилон. Первый случай. И второй случай. Так, в качестве y берем xn
плюс c. В качестве x берем xn плюс это n. И в качестве a берем a минус епсилон. Выписываем.
Вероятность того, что xn плюс c меньше a минус епсилон, меньше или равна вероятности того,
что xn плюс это n меньше a, плюс вероятность того же самого, это n минус c больше епсилон. Ну вот
эти, смотрите, штуки одинаковые. Я их как-нибудь обозначу там. Дельта n епсилон. Вот так. Так.
Ну и теперь, значит, вот так это перепишу. Вероятность того, что x это n меньше a с одной
стороны, меньше ли равна вероятности того, что xn меньше плюс c, плюс c меньше a плюс епсилон,
плюс дельта n епсилон, а с другой стороны, из второго, больше ли равна вероятности того,
что xn плюс c меньше a минус епсилон, минус дельта n епсилон. Устремляем к бесконечности.
Поскольку это n сходится к c по вероятности, то дельта n епсилон стремится к нулю. А вот это
к чему стремится? Вот сюда смотрю. К чему это стремится? Это стремится к функции распределения
случайной величины кси плюс с, взятой в точке а минус епсилон. Меньше или равно, здесь пока оставлю
в виде предела. Предел, пример стремяющийся к бесконечности. Функция распределения кси n
плюс это n в точке а, меньше или равно, f к си плюс с в точке а минус епсилон, ну а минус епсилон.
Вот так, да? Ну, дальше что? Дальше епсилон к нулю. Естественно, предполагаем, что точка а,
точка непрерывности, поскольку сходимость почти наверная, только в точке непрерывности. Так,
значит, это f к си плюс с в точке а, меньше или равно, ну не буду этот предел переписывать,
а здесь f тоже к си плюс с, тоже в точке а. Ну, отсюда сделаем вывод, естественно,
по теориям там о двух собачках, что вот этот предел как раз и равен функции распределения
случайной личины к си плюс с, что и требовалось доказать. Понятно я все изложил? Так, это звонок
или что? Звонок. Ну, давайте, как раз вот уложились, отдыхайте и продолжим. Так, ну мы закончили
с вами свойства сходимости. Это, конечно, не все, но тот минимум, который как бы нам нужно знать. И
переходим уже к таким финальным темам. Это закон больших чисел у нас теперь на очереди. Значит,
давайте определимся, что мы имеем в виду. Пусть у нас есть случайная последовательность,
счетная. И мы ведем вот такую случайную личину s,n с чертой сверху. Ну, довольно стандартное
обозначение. Это среднее значение. Среднее значение. Так вот, мы будем говорить, что случайная
последовательность подчиняется закону больших чисел ЗБЧ. Удобное сокращение. ЗБЧ. Если существует
числовая последовательность аэнная, такая что к си,н средняя минус аэнная сходится к нулю по
вероятности. Это общее определение. Мы будем говорить, что для случайной последствий к си,н выполнен
закон больших чисел, если существует такая числовая последовательность, что к си,н средняя
минус аэнная сходится по вероятности к нулю. Но в части ЗБЧ мы все результаты получим для
конкретного вида аэнная. Для нас аэнная это математическое ожидание к си,н среднего,
которое по линейности есть среднее математических ожиданий. И тогда вот это свойство можно
переписать в виде. Ну либо к си,н средняя минус, явно запишу среднее математических ожиданий,
или ввести просто обозначение с,н центрированное среднее, которое есть среднее значение
центрированных случайных величин, то есть из которых вычислим от ожидания. Вот наши результаты в
части выполнения ЗБЧ будут относиться вот к этому случаю. Ну собственно, как мы глобально ставим
задачу? Мы хотим описать свойство последствий к си,н, какими она должна обладать свойствами,
чтобы выполнялся ЗБЧ. Ну попросту говоря, для начала какие-нибудь достаточно условия. Ну и первое
достаточно условие дается нам теоремой Маркова и выглядит следующим образом. Если для последовательности
единица на,н квадрат дисперсия сумма ксикатых к от единицы до,н стремится к нулю, при,н стремясь к
бесконечности, то ЗБЧ. Если последовательность такова, что вот такая величина стремится к нулю,
никаких других условий мы не требуем, то для нее выполнен закон большой чисел. Доказательства,
ну оно в две строчки, ну надо его, значит напомню, неравенство Чебышева вот в такой форме, вероятность
того, что случайная величина это отклонится от своего математического ожидания на величину
большую чем,эпсилон не превосходит дисперсии это делить на,эпсилон квадрат. Правильно, да?
Ну давайте в качестве это возьмем ксиен средняя, давайте я вот здесь вот это напишу,
чтобы вторую доску так сказать осталась свободной, значит ну давайте просто воспользуемся неравенством
Чебышева, подставим чего надо. Вероятность того, ну здесь я могу смело написать кси-эн
центрированная средняя больше или равно,эпсилон меньше или равно дисперсии кси-эн центрированная
средняя делить на,эпсилон квадрат, что в свою очередь равно дисперсии, ну а здесь я напишу явно,
что из-за случайной величины, это 1-энная сумма кси-катых, минус мат ожидания кси-катых,
кат единицы до,эн в квадрате делить на,эпсилон квадрат. Единицы на,эн квадрат выносим за скобки,
ну а тут остается дисперсия, видно, что это как раз дисперсия суммы кси-катых, потому что
вычитание константы дисперсию не меняет, помните свойства. Ну и еще тут,эпсилон квадрат конечно,
ну вот там наш искомый член, если он стремится к нулю, то значит вот эта вероятность стремится к нулю,
что и означает сходимость по вероятности кси-эн центрированного среднего, что означает выполнение
закону больших чисел. Вот такое достачное условие. Следующее достачное условие задается теоремой
Хинчина, теорема Хинчина. Хинчин наш соотечественник, академик, академия СССР, ну вторая половина,
наверное такая, ранняя вторая половина двадцатого века. Вот, теорема Хинчина немножко другие
устанавливает достачатые условия. Пусть кселенная независимые одинаково распределенные случайные
величины, нор СВ. И еще известно, что существует математическое ожидание кси меньше бесконечности.
Я здесь не ставлю индексы, поскольку они все одинаково распределены, у них у всех одинаковое
мат ожидания. То тогда ЗБЧ. Вот такие вот достаточно условия. Последствия независимых одинаково
распределенных случайных величин, существует мат ожидания, выполнен закон большой чисел. В чем
отличие от теоремы Маркова? Не требует существования дисперсии, но правда требуется
такое сильное условие независимости, одинаковая распределенность. Ну как есть. Так, доказывается
эта теорема с помощью аппарата характеристических функций. Давайте найдем характеристическую
функцию кси Н средняя от Т. Что это такое? Это математическое ожидание Е в степени И,
Т единица на Н сумма ксикатых К от единицы до Н. Все ксикаты независимы, поэтому это
произведение характеристической функции, взятых в точке Т делить на Н. Но поскольку они еще одинаково
распределены, то еще характеристические функции одинаковые. Поэтому есть фиг С, взятая в точке Т
делить на Н в степени Н. Правильно, да? Ну давайте характеристическую функцию в окрестности нуля
разложим, как он говорит, маклорена. Фи от нуля чему равно? Чему равна характеристическая функция в нуле? Единицы всегда.
Плюс надо теперь взять производную в нуле. Производная в нуле чему равна характеристической функции?
А? И нам от ожидания, если оно существует, но, к счастью, у нас по условию теоремы оно существует.
Т на Н, ну и плюс умало от единицы на Н, все в степени Н. Чему это стремится? При Эн-слемящей бесконечности.
Это стремится к Е в степени И, Т, мотождание кси. А это что такое? А это характеристическая функция,
выраженная случайной величины мотождания кси, взятая в точке Т. Что отсюда следует? Отсюда следует,
что кси Эн-средняя сходится по распределению к мотожданию кси. Но тут вспоминаем, что исходимости
по распределению к константе, следует исходимость по вероятности. Отсюда следует, что кси Эн-средняя
сходится по вероятности к мотожданию кси. То есть выполнен закон больших чисел. Вот
достаточные условия выполнения закона больших чисел в форме хинчина. Ну и третье достаточное
условие, которое нам нужно знать, это теорема, ну раз есть Марков, кто должен быть? Чебышев,
да, теорема Чебышева. Значит тут такие условия накладываются. Значит есть у нас последовательная
случайная, могут быть зависимые, но не коррелированные. То есть кавариация кси-этое, кси-житое равна нулю.
Ну для любого и не равно жи. Первое. И второе. Дисперсия может себя как-то вести произвольно,
но с одним ограничением. Не очень быстро расти, а именно дисперсия кси-эн-ного должна быть
ограничена константой на n в степени альфа, где альфа строго меньше единицы, больше равно нуля.
То есть дисперсия не быстрее линейной должна быть. И если вот эти условия выполнены, то тогда забычье.
Ну такажем, естественно, использовать теорему Маркова. Значит возьмем единица на n квадрат,
дисперсия сумма кси-катых крат единицы до n. Для некоррелированных случайных величин дисперсия
сумма чему равна? Сумма дисперсии. Помним, что это не только для независимых или скорее не столько
для независимых, а для некоррелированных случайных величин. Поэтому есть сумма дисперсии кси-катых
делить на n квадрат. Это меньше или равно c делить на n квадрат сумма n в степени k в степени альфа,
альфа, где k изменяется от единицы до n. Ну а вот это уж, конечно, меньше или равно,
чем c делить на n квадрат интеграл от нуля, чтобы не возиться до n плюс один, х в степени альфа
dx, что равно c делить на n квадрат n плюс один в степени альфа плюс один делить на альфа плюс один.
Вот это заведомо меньше двух, строго меньше двух. А это означает, что принц, стремящийся к
бесконечности может быть и очень медленно, но это все-таки будет сходиться к нулю. Итак,
для такой последовательности некоррелированной из дисперсии, растущей не быстрее, чем линейная
функция, получается выполнено достаточно условия в форме Маркова. Ну что и заканчивает доказательство
теоремы Чебышева. Понятно, да? Нет вопросов? Ну, конечно, мы когда исследуем вот такие связи,
то помимо достаточных условий, что нам хочется? Критерий нам хочется еще. И он есть у меня для вас,
ну, точнее, не у меня, у Колмогорова. Теорема Колмогорова гласит.
Значит, теорема Колмогорова гласит, что закон больших чисел необходимый и достаточно для
последовательности, чтобы вот такое мат ожидания, кси n центрированная средняя в квадрате делит на
единица плюс кси n центрированная средняя в квадрате, сходилось к нулю. В общем случае,
а если кси n независимые случайные величины, то критерием является вот такая вещь. Сумма
математических ожиданий, кси k центрированная в квадрате делит на n квадрат плюс кси k центрированная
в квадрате стремится к нулю. При сумма k от единицы до n стремится к нулю при n стремящейся к бесконечности.
Значит, надо было вверху, но напишу здесь. Теорема Колмогорова.
Так, мы ее сейчас докажем в общем виде, но здесь мы не воспользуемся доказательством Колмогорова,
несмотря на то, что оно может быть даже проще, но здесь мы пойдем длинным путем.
Ну понятно, с извлечением дополнительной пользы.
Давайте рассмотрим какую-нибудь последнюю случайных величин Zn,
которые по распределению сходятся к нулю. Я утверждаю, что отсюда следует, что для любого альфа больше нуля
математическое ожидание модуль Zn в степени альфа делить на единицы плюс модуль Zn в степени альфа
тоже стремится к нулю при н стремящейся к бесконечности.
Вот сейчас стрельнут все те свойства, которые или многие, которые мы так сказать муторно выводили.
Итак, Zn стремится к нулю по вероятности. Я утверждаю, что отсюда следует, что Zn в степени альфа на единицы
плюс Zn в степени альфа тоже стремится к нулю при н стремящейся к бесконечности.
Откуда это следует? На следовании сходимости. Теперь обратите внимание, что вот эта вот
случайная величина с вероятностью единица ограничена единицей для любого n. А что мы знаем?
Какое знаем свойство сходящихся по вероятности последствий, которые с вероятностью единицы
ограничены для всех n? Для чего мы это делали? Чтобы пользоваться. Отсюда следует,
что имеет место сходимость порядка r и в частности сходимость в среднем, то есть c равном единице. А
для такой случайной величины сходимость в среднем это как раз и есть. Собственно,
вот это даже не буду писать. Согласны? Нет возражений? Теперь давайте в обратную сторону.
Пусть вот эта штука выполнена. Тогда отсюда что следует? Исходимость порядка 1. Сходимость
по вероятности. Правильно? Исходимость порядка 1 вот для такой случайной величины. Следует
сходимость по вероятности. А отсюда что следует? Так уж в лоб по наследованию сходимости zn по
модулю. Ну а сходимость zn по модулю к нулю это просто сходимость zn к нулю по вероятности.
Согласны? Ну смотрите, а мы с вами доказали, что если zn сходится к нулю, то вот это выполнено
не для какого-то альфа, для какого-то проверяли, а для любого. И отсюда мы получаем вот такой,
за две минуты с вами получили вот такой замечательный факт, что для любого альфа
zn сходится к нулю по вероятности. Это то же самое, что математическое ожидание zn модуль в степени
альфа единица плюс zn в степени альфа стремится к нулю, при н стремяющейся к бесконечности. Для
любого альфа, любое можно взять. Все будут критерии. Ну просто потому, что вот эти при любом альфа
одновременно либо стремятся к нулю, либо нет. Вот. Калмагора взял z равно 2. Вот если взять не z,
а альфа равно 2. Ну а в качестве zn, естественно, xn центрировано средне. Видите, да? Калмагора взял
альфа равно 2. Получился критерий Калмагорова. Но можно взять альфа равно единицы, например. Это
тоже будет критерий. Чем альфа равно единицы лучше или хуже любого другого, в частности альфа равна 2?
Непонятно, да? Ну смотрите, 2 чем хорошо? Модуль пропадает. Может быть, с вычислительной
точки зрения двойка удачный выбор. Но у альфа равного 1 тоже есть замечательное свойство. Оно
состоит в том, что мат ожидания, пусть будет zn, это метрика. Все как мы любим. Мы оказались
в метрическом пространстве. И все эти кладовые с теоремами метрических пространств теперь как бы
в нашем распоряжении. Этот фокус или прием называется метризация типов сходимости. Мы
с вами метризовали сходимость по вероятности. При альфа равном 2 это уже не метрика, но это критерий,
зато именной критерий Калмагорова. Естественный вопрос, который возникает. А другие типы
сходимости метризуются. Вообще говоря, нетрудно в этом убедиться. Пусть у нас есть сходимость порядка r,
но это то же самое, что мат ожидания xn-xr в степени 1 на r. А вот это уже метрика.
Для сходимости по распределению тоже есть метрика. Она немножко громоздкая. Называется
метрика Левия. Такая громоздкая немножко. Но если рассмотреть ее на классе непрерывных случайных
величин, непрерывные случайные величины, непрерывные не независимые, непрерывных случайных величин,
то тогда вот это вот в обе стороны оказывается. Эквивалентно вот чему, что supremum fxn-x-fx
от x, supremum по x стремится к нулю, прем стремящееся к бесконечности. Равномерная метрика.
Не, ну проверяем по определению. Ну чуть-чуть просто нет времени, к сожалению. Вот чуть-чуть надо
повозиться, так сказать. Доказывается, что это метрика. Причем это такая метрика, смотрите,
например, просто zt-ная по модулю может быть там равно бесконечности. А вот это ее нормируют на
единицу. Это такая метрика, что расстояние между двумя элементами никогда не будет больше единицы.
Вот, значит, итак, для сходимости по распределению тоже есть соответствующая метрика. Еще раз повторю,
в общем случае это метрика Леви, такая громоздкая, но для случайных величин, которые непрерывны,
она превращается в равномерную метрику. Ну и следующий вопрос, а какая же метрика для сходимости
почти наверное? И здесь нас ждет сюрприз. Сходимость почти наверное не метризуется. Не существуют
метрики эквивалентной сходимости почти наверное. Для тех, кто интересуется или заинтересуется,
полюбопытствует, это неплохая задачка, но вполне вам доступная. Так, с метризацией и критерием
выполнения закона больших чисел. Я заканчиваю. Вопросы есть? Вопросов нет. Буквально два слова. Я
когда вот это писал, ну как бы подразумевается обычно так по логике, что частные случаи независимой
случайной величины как-то следуют отсюда. Вы знаете, нет. То есть доказательства для независимости
случайных величин, это критерия, это отдельная работа Колмогорова, там я не знаю, там на десяток
страниц с формулами там, промежучными леммами и так далее. То есть тот случай, когда частный
случай в доказательстве намного сложнее общего. А вот из общей формулировки получить вот этот
частный случай, там нигде не встречал, не знаю как, всем студентам говорю, может кто-нибудь из вас
докажет это как следствие вот этой теоремы. Это будет хороший результат, между прочим. Вот,
потому что само по себе доказательство просто вообще другое. Оно построено по-другому и более
сложное. Ну, тогда все и двигаемся дальше. Так, очень бы хотелось успеть еще один результат довести
до конца, чтобы не начинать потом с совводной. Значит, без предисловий, все скажу потом. Сейчас
просто давайте рассмотрим с вами результат, который называется Леммаково. Коллеги, вот если вас
спрашивают, чей результат в теории вероятности, вам нужно говорить Колмогорова и в 90 процентах
вы окажетесь правы. То есть ошибка возможна, но почти все Колмогорова. Лемма. Значит, пусть у нас
есть последовательность случайных величин, но они независимые случайные величины. Это мы
такое накладываем ограничение. И введем такую частичную сумму СН, сумма ксикатых,
нет, ксиджитых здесь будет, ксиджитых, g от единицы до n и еще центрированных там уже. То есть
частичная сумма центрированных случайных величин. Так вот, утверждение Колмогорова состоит
следующим. Вероятность того, что supremum S Катова окажется больше епсилон, когда k пробегает
значение от 1 до n. Ну supremum тут как бы максимум, ну ладно, воспользуемся suprem. Меньше или равно
математического ожидания S N в квадрате делить на епсилон квадрат, что ввиду, ну как бы,
свойств S Нного на самом деле дисперсия S Нного в квадрате на епсилон квадрат или ввиду их
независимости, просто сумма дисперсии си джитых g равно от единицы до n делить на епсилон квадрат.
Что? Кто независимый? А я ж написал, независимый случайно. Ну да, независимый, независимый. Да,
в прошлый раз я отметил, что речь идет именно о непрерывных. Это независимые случайные
веричины. Значит, ну давайте как бы сразу в карьер. Значит, введем вот такое множество окатая. Это,
вспомним, происходное вероятностное пространство. Это Омега такие, что S житая от Омега, я здесь
подчеркну. Ну то есть смотрите, Омега, каждому Омега соответствует последовательность, каждой
последовательности соответствует последовательности S Нного. Поэтому я тут Омега. Модуль S Омега меньше
или равно епсилон, и это имеет место для всех g от единицы до k минус 1, а вот S ж s кт от Омега
по модулю уже больше епсилона. То есть смысл множества катая туда входят те последовательности,
которые до катой частичной суммы не превышают епсилон, и именно на катой ее в первый раз
превышают. Ну, во-первых, аитая, а житая равно пустому множеству. Объединение окатых кат единицы
до N мы для краткости обозначим А, но на самом деле это как раз и есть. Вот это множество Омега такие,
что supremum s кт больше епсилон, где кат единицы до N. Правильно, да? Вот. Ну и давайте тут же,
чтобы все условия тут описать, введем характеристическую функцию множества А,
которая ввиду того, что эти множества несовместны, на самом деле представляет
из себя сумму характеристических функций. Надо бы написать Акатая, но я, чтобы не таскать
двухэтажный индекс, напишу просто Хкатая от Омега. Катая это Акатая. Вот, значит,
все обозначения ввели. Давайте, двинемся дальше.
Так, ну давайте вот изучим вот это математическое ожидание Sn в квадрате.
Я так запишу, чтобы вы вспомнили, что это такое. Это Sn в квадрате от Омега pdОмега.
Здесь напишу, больше ли равно Sn в квадрате от Омега Ха от Омега pdОмега. Никто не возражает
Ха либо 0, либо 1. То есть эта функция заведомо, ну больше ли равна вот этой вот. Так, Ха это
можно представить в виде суммы, поэтому это на самом деле сумма. Теперь как бы интеграл,
что называется, сверну и напишу математическое ожидание Sn в квадрате на Хкт. К от 1 до n.
Правильно, да? Вот, ну а теперь давайте вот с таким от ожиданий для конкретного Ка повозимся,
напишем. Математическое ожидание Sn в квадрат на Хкт. Это есть математическое ожидание. Помни,
что такое Sn. Мы напишем, что это Скт плюс сумма ксиджитых центрированных g от k плюс 1 до n.
Все в квадрате на Хкт. Квадрат раскрываем, пользуемся линейностью и пишем, что это равно
математическое ожидание Скт в квадрате Хкт плюс, а вот эти два члена я здесь напишу. Там будет
математическое ожидание 2 Скт Хкт умножить на сумму ксиджитых центрированных g от k плюс 1 до n
плюс двойная сумма по g и по l, например. Математическое ожидание ксиджитого
центрированного на кси лт центрированное изобилищу Хкт. Давайте смотрим вот на этот
член. Смотрите, вот эта случайная величина определяется кси к плюс первым, кси к вторым,
кси н. Хкт это функция от кси 1 кси к. Скт это функция от кси 1 кси к. Мы с вами знаем,
что если случайные величины независимы, то любые функции на независимых случайных величинах тоже
независимы. Поэтому вот эти две случайные величины независимы. Математическое ожидание произведения
равно произведению математических ожиданий. А чему равно это ожидание вот этой случайной величины?
Суммы центрированных. Ну ноль. Мат ожидания суммы равно сумме мат ожиданий. Мат ожидания
центрированных случайных членов всегда равно нулю. То есть вот этого члена нет. Смотрим на этот
член. Та же логика. Хкт не зависит от вот этой суммы, потому что здесь индексы с k плюс 1 начинаются.
Поскольку они независимы, все перекрёстные мат ожидания к вариации равны нулю. Те, которые не
равны нулю этой дисперсии, больше или равны нуля. Поэтому вот эта вот вся двойная скобочка для нас
важно, что она просто больше равна нуля. И исходя из этого, мы можем теперь написать, что это больше или
равно математического ожидания sk в квадрате на хк. Мат ожидания sn в квадрате на хк больше
равно мат ожидания sk в квадрате на хк. Видите вот, что мы получили. Вот. Ну, дело за малым.
Значит, напишу, чему это равно в виде интеграла Либега. Это sk в квадрате от
омега на хк от омега. Значит, подинтегральная функция не равна нулю только тогда, когда хк не
равно нулю. Хк не равно нулю. А хк не равно нулю, это означает, что произошло событие акк. Осмотрим
на событие акк. Что оно нам гарантирует? Оно нам гарантирует, что sk по модулю больше
эпсилон. Видите, sk по модулю больше эпсилон. Поэтому все вот это больше или равно эпсилон
квадрат на интеграл хк от омега pd омега. Согласны? А это в свою очередь, ну просто
эпсилон квадрат на вероятность окатова. Возвращаемся вот сюда. Пишем больше или равно
эпсилон квадрат сумма вероятности окатова как единица dn. Поскольку окаты независимы,
то это эпсилон квадрат на вероятность а, но вероятность а это собственно то, что нам... это
событие а это то, что нам и надо. Ну на эпсилон квадрат делим и получаем ответ, так сказать.
Согласны? Отлично. Ну выразил бы некие эмоции, но некогда. Давай еще успеем сформулировать одну
теорему, с которой начнем в следующий раз. Как бы сразу, сейчас начнем получать приятные
последствия. Так, следующая теорема, которую нам нужно знать, ну и вообще весьма полезная,
в том числе и методологической точки зрения. Это теорема кого? Хинчина. Совершенно справедливо.
Теорема Калмагорова-Хинчина. Теорема Калмагорова-Хинчина. Значит, пусть у нас есть СН независимые
случайные величины, про которые известно. Значит, независимые случайные величины. Введем СН по тому же
правилу. Ксикаты и центрированные К, от единицы до Н. И еще известно, что ряд из дисперсии сходится.
Ряд из дисперсии сходится. Ну, это на самом деле означает, что они существуют, им от ожидания
существуют. Вот если для последующих случайных величин, сходя ряд из дисперсии сходится, тогда
существует некая случайная величина С от Омега. Такая, что СН от Омега сходится почти, наверное, С от Омега.
Вот давайте полезная интерпретация за ту минуту, что осталось. Мы на самом деле любым рядом, там я не знаю,
ряд из х-катых от единицы до бесконечности, это же обозначение. Мы на самом деле обозначаем предел
х-катых от единицы до Н, как до Н стремится к бесконечности, правильно? То есть предел частичных
сумм. И здесь, смотри, что написано, что частичные суммы сходятся к некоторому пределу. Но некий
предел частичных сумм, если он существует, это как раз и есть. СН сходится почти, наверное, к сумме
вот такого ряда. Так можно проинтерпретировать. Более того, поскольку это случайная величина,
то она меньше бесконечности, почти, наверное. Любая случайная величина, почти, наверное,
меньше бесконечности. Вот эта интерпретация, которая нам понадобится. Ну давайте тогда на
формулировке этой закончим и с этого момента начнем с вами в следующий раз. Нам осталось немного,
но как бы самое важное. Все, коллеги, давайте. Спасибо.

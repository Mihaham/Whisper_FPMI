День добрый. Рад всех вас видеть. Надеюсь, все с добрым здоровьем. У нас начинается курс лекций
«Параллельные алгоритмы и параллельные вычисления». Я не знаю, оба ли факультета
здесь присутствуют. ФУПМА есть люди, а САФРТК есть. Ну вот, называется они по-разному, но понятно,
что я буду читать один курс, и по сути он ближе как раз к параллельным алгоритмам. Мы на нем не
будем заниматься непосредственно решением каких-то задач, но будем смотреть, в помощи каких методов
можно решать задачи различных классов. Меня зовут Якововский Михаил Владимирович, я заместитель
директора по научной работе Института прикладной математики имени Келдыша,
Российская академия наук. И на протяжении уже многих лет вот специализируюсь как раз на решении
задач механики сплошной среды. Это газовая динамика, это химическая кинетика, это много
всего разного с помощью многопроцессорных вычислительных систем. Ну и понятно,
что вот именно мои интересы, они лежат в области, которая коротко называется HPC. Мы сегодня рассмотрим
области применения суперкомпьютерных систем, ну так как я это себя понимаю. Вот одна из четырех
это HPC, это высокопроизводительное вычисление. Отнюдь не исчерпывается ими многообразие
применения суперкомпьютеров параллельных систем, но тем не менее это важная область,
которая в рамках которой как раз лежат моделирование различных задач, решение вычислительно
сложных задач и многое другое. И вот это наибольший интерес, представляя это для меня и в существенной
мере это ориентация нашего института, который в свое время был создан для решения с помощью
высокопроизводительной вычислительной техники самых разных задач. Ну и в первую очередь это
конечно задача связанная с обороной, это формирование ракетно-ядерного счета нашей родины,
вот для этого был сделан институт. И так это и осталось, сейчас мы тоже занимаемся решением тех
задач, которые актуальны для нашей страны, которые интересны с фундаментальной точки зрения. И
используем для этого суперкомпьютерную технику, у нас есть свои вычислительные системы, мы
поддерживаем их в таком, скажем, состоянии, достаточном для того, чтобы можно было изучать их
свойства, решать с их помощью какие-то ограниченные подношества задач, но когда дело доходит до дела,
мы выходим на центр аспиративного пользования, самый разный это университетский и курчатского
института, и зарубежный, барселонский, в Японии, и считаем разные задачи там. И это к чему? Это к
тому, что сегодня я буду и дальше вот в рамках курса рассказывать методы, которые потенциально
позволяют вам использовать суперкомпьютеры. Не просто ноутбук, в котором один процессор четырех
ядерный там или восьмиядерный, хотя эти алгоритмы мы тоже будем рассматривать, мы тоже будем
рассматривать алгоритмы, которые на большое число процессоров не масштабируются, они хороши на одном,
двух, трех, но основная идея все-таки использовать большие системы, большое число процессоров,
значит писать алгоритмы, которые хорошо масштабируются. Вот эти алгоритмы мы будем в основном
рассматривать. Ниже написана ссылка, это веб-ссылка, там будут выкладываться материалы курса не в
ущерб тому, что будет на youtube-канале, там будет презентация, возможно, там будет информация по
контрольной, которая ожидается ближе к концу курса, но это будет отдельно решено, когда именно она будет,
в каком виде. И там же уже сейчас лежит список литературы, он с одной стороны довольно там старая
публикация в основном, а с другой стороны они своей актуальности не потеряли. Вот обновить его,
Рути не доходит годами, этот список обновить, аккуратно положить, но одну публикацию я добавил
свежую в самом конце, но она тоже интерес представляет. Значит, вот там будут материалы курса.
Соответственно, если заменить вот эту точку на собачку, то получится мой мэйл. ИМАМОТ это
институт математического моделирования, в какой-то момент я начинал работать в институте
приводной математики, потом в какой-то момент академик Самарский Александр Андреевич организовал
свой институт отдельный, 3-е отдел ИПМ и выделился с рядом примкнувших сотрудников, у нас там было
порядка 100 человек, и 10 с лишним лет мы существовали отдельно. Вот был институт математического
моделирования, оттуда остался сайт, осталась почта, потом объединились обратно. Вот судьба,
она так поворачивается, но сейчас это единое целое, институт приводной математики.
Ну вот параллельный алгоритм, почему параллельный? Я так помечтал сегодня немного, вот хорошо было бы
иметь процессор, который бесконечно быстро решает любую задачу, но его алгоритмы написали,
он бесконечно быстро его выполняет. Вот если бы такой процессор был, нам бы не пришлось сейчас
заниматься вот этими изучением параллельных алгоритмов. Дело это не самое простое, но алгоритмы
осознать ничего сложного нету, параллельная программа написать, чтобы она работала, вот это сложнее.
И был бы процессор, вот супер процессор, который бесконечно быстро работает, вот не нужно было бы
это и не занимались бы мы этим. Да, с точки зрения там познания природы, изучение параллельных
алгоритмов оно интересно, но с практической точки зрения полезнее было бы иметь супер процессор,
но его нет и не предвидится. Его нет и не предвидится уже достаточно давно, я сегодня
никакие слайды показывать не буду, а дальше, если все будет нормально, ну либо я здесь покажу
некоторые слайды на эту тему, либо как-то еще будут лекции проходить. Я-то за то, чтобы они очень
проходили, вот не болейте все, все будьте здоровы, пусть они будут здесь, я с радостью сюда приеду
я всегда сюда с удовольствием приезжал раньше. Так вот, не получается сделать супер процессор,
потому что есть некоторые ограничения. Вот некоторые ограничения связаны с физической
точки зрения, вот с физическими особенностями процессоров. Они сейчас, вот сегодня
основаны на, ну это электрические устройства, передаются, текут какие-то токи, заряжаются
какие-то емкости, вот они не могут иметь нулевого, емкость не может быть нулевого, нулевой величины,
она должна хранить заряд, он не может быть меньше, чем заряд электрона, а на практике это должен
быть более ощутимый заряд, а на то, чтобы этот заряд накопился на конденсаторе, вот, а потом с
него ушел, требуется какое-то время. Чем больше ток, тем меньше это время, но чем больше ток, тем
выше энергопотребление, тем выше тепловыделение. И вот все в комплексе это, плюс ограничение
естественным образом накладываемой скоростью света, привели к тому, что уже на лет 15, наверное,
если не больше, скорость одного процессорного ядра, она не растет совсем. Если вы посмотрите на список
топ-500, он там, в списке литературы, конечно же есть, вот top500.org, я буду на него периодически
ссылаться на этот список, потому что именно там сосредоточены открытые суперкомпьютеры мира,
те суперкомпьютеры, о которых известно, вот они есть, ко многим из них есть публичный доступ,
не ко всем, там, например, есть яндексовские машины, вот, а насчет публичного доступа к ним я сильно
не уверен, по крайней мере большинству из них они делались для корпоративного сегмента, но тем
не менее, это машины, о которых миры бы известны, известны как они устроены, известны какие у них
характеристики. Да вот если посмотреть на эти машины, на первые позиции, то выяснится, что
частота, на которой работают процессоры, из которых сделаны эти суперкомпьютеры, она порядка 2-2,5
гигагерц, не больше. Вот у вас дома, если у кого-то системный блок стоит, он скорее всего
шустрее, он скорее всего там 3 гигагерца, 4 может быть даже, а в этих системах нет, но и выше 4,5
это уже там жидкий азот дальше нужен для того, чтобы охлаждать, не поднимается частота, то есть
частота перестала расти очень давно. Значит, этот путь, он экстенсивный, и он на протяжении очень
многих лет обеспечивал рост производительности одного процессора, то есть сначала это были
какие-то механические устройства, потом там лампы, транзисторы, микросхемы, все выше частоты были,
потом они перестали расти. Вот первое, что перестало расти, второе, что перестало расти. Кроме
частоты, можно улучшать потребительские свойства самого процессора, те алгоритмы, которые в нем
заложены. Вы можете за один такт выполнить несколько инструкций, вы можете одну инструкцию разделить
на части, считать данные из оперативной памяти, вещественное число, например, нормализовать его,
потом сложить два числа, потом, соответственно, донормализовать и положить в оперативную память.
Это разные этапы, и в принципе никто не мешает сделать конвейер из этих этапов, и тогда вы как бы
за один такт будете получать не один результат, а четыре. Это архитектурная особенность процессора,
они сейчас везде есть, эксплуатируются практически во всех современных процессорах вот такого
сорта операций делаются. Но вот фантазия и сякла, в последнее время нету архитектурных решений
новых, которые увеличили бы скорость выполнения операций за счет того, что у вас много транзисторов,
вы можете их как-то использовать, например, сделать конвейер, например, сдублировать
некоторые устройства исполнительные и за один такт выполнять не одну операцию, а две операции.
Например, вы можете использовать транзисторы для того, чтобы сделать быструю кэш-память,
потому что доступ к кэш-памяти основной относительно скорости работы процессора
это медленная штука, а регистры быстрее и значительные, между ними где-то есть кэш-память,
и ее делают. Вот это архитектурное решение, которое тоже существенно ускоряли производительность
процессора, они и сякли в какой-то момент, достаточно давно уже. Это не значит, что никто
ничего не придумывает, но вот в последнее время никто ничего не придумывает. А транзисторов на
подложке все больше, вот на одном кристалле транзисторов все больше, они все меньше, техпроцесс растет,
там было 60 нанометров, 40, 27 нанометров сейчас, где-то так вот. Ну и причем давно уже предвещают
конец этого процессора, что вот все, вот уже меньше нельзя, но вот пока что еще вот меньше
удаётся делать. А это значит, что на один процессор можно очень много транзисторов выделить,
а куда их собственно использовать непонятно, и поэтому естественные решения технологов,
вот здесь они, аппаратчики идут впереди алгоритмистов. Давайте сделаем два процессорных ядра на одном
кристалле, скажем, что это процессор двухядерный, это два процессора, ну это два процессора. Вот там
четыре процессора, 48 процессоров, если посмотреть первый список строчку, вот топ-500, там 48 процессоров
для счета внутри одного кристалла, это 48 процессоров, 48 ядер, да еще четыре сервисных, там их 52,
но вот программисты 48. И вот за счет параллельности, за счет параллельности вы повышаете общую
производительность системы, это здорово, но вы берете последовательную программу, запускаете ее,
она не работает быстрее, она работает со скоростью одного процессорного ядра, у вас нету шанса
существенно ее ускорить, нет, немного она ускоряется, она ускоряется как раз вот за счет того,
что компилятор некоторые вещи видит, которые можно сделать одновременно, ну вот кое-что я
уже перечислил, ну что-то еще, например, он может увидеть, что у вас есть операция сложения двух
векторов, вы ее в цикле запрятали, а компилятор может это понять и одновременно складывать не по
одному числу, а по четыре числа, вот два вектора складываем, по четыре числа сразу, компилятор в
состоянии это сделать, это не очень интеллектуальная операция, хотя она тоже требует в общем-то усилий
со стороны компиляторщиков, но это тем не менее плохо масштабируемая вещь, то есть процессоров-то
сейчас счет идет уже на миллионы, вот нам обещали еще к 18 году, где-то в 16 году, обещали сделать
систему на 10-18 операций в секунду, вот экзофлопс пресловутый, да, в 18 году здесь 18 операции,
очень красиво звучит, я сказал, что давно уже производительность ядра толком не растет, она
порядка одного там 10 гигагерц, гигафлопс, да, ну так, условно, ну то есть их там 10-7 штук должно быть,
вот 10-7 процессоров, которые должны работать согласованы над решением одной вашей задачи,
вот у вас всегда есть возможность такой суперкомпьютер использовать иначе, вы можете
сказать, так, у меня большая система, давайте все ко мне, вот сотня рабочих групп по стране и по миру,
каждому по 10 процессоров, мы используем суперкомпьютер, вот HPC она не про это, HPC про то,
что вы берете весь компьютер или половину и используете решение одной задачи, вот там моделируете
столкновение галактик, например, или моделируете винт вертолета, где галактика, где вертолет,
а на самом деле, что сложнее, это еще надо подумать, потому что у вас очень разномасштабные процессы,
вот при моделировании таких сложных систем протекают, и вы должны с высоким разрешением
выполнять вычисления, вот для того, чтобы понять, где рождается шум у вертолетного винта и какую
нужно сделать форму винта, чтобы вот этот шум был меньше, то есть есть вот амбициозные цели довести
шум истребителя до хабанинного писка лет через 10-15, но с вертолетами тоже хотят потише, вот тот звук,
который мы слышим, вот тарахтение от вертолета, это ударная волна, ударная волна от лопастей,
вот они проходят мимо нас, от каждого лопастя распространяется ударная волна, мы ее слышим
каждый раз, на каждом обороте, не двигатель шумит, это вот винт шумит, несущий винт вертолета,
и для того, чтобы аккуратно промоделировать, вам нужна очень большая вычислительная мощность,
то есть были международные проекты, вот был проект прецезионного расчета шума от конструкции самолета,
от лайнера, от шасси, то есть понять, где шум возникает, вот есть только шасси, она шумит,
где она шумит, на самой стойке, сзади, где, эксперимент не очень-то поставишь, эксперимент
можно ставить, но вы его будете ставить где, в ординамической трубе, ординамическая труба сама по себе
шумит так, что мало не покажется, это цех, размером вот с этот корпус, цех металлический, который
резонирует, когда у вас включаются вентиляторы этой самой трубы, уловить там шум от вашей конструкции,
это тоже задача, это тоже задача для HPC, получить картину шума общую, и оттуда вырезать шум цеха,
и посмотреть, а что же шумит, как шумит ваша конструкция, так вот, их много этих очень устроится,
вот их там 10, 8, 10, 7, как я сказал, с оценками можно ошибаться, вот несколько лет назад я говорил,
что там будет миллиард исполнительных устройств, но как посчитать, потому что одно дело единиц,
которыми вы можете управлять, там миллиард или нет, а другое дело, сколько на микроуровне устройств,
ну вот вы знаете, что в процессорах стандартных есть такая вещь, как расширение векторное,
вы именно непосредственно можете, конечно, управлять когда-то для некоторых алгоритмов,
вот сколько там таких устроит, вот таких устройств, там уже счет идет на миллиарды,
когда у вас графическая карта, вроде это одно устройство, но там внутри у вас 700
метеотрядовых устройств, и вот их тоже уже на миллиарды, а для того, чтобы вы суперкомпьютерную
мощность использовали для решения задачи, чтобы она была полезна, вы должны, во-первых,
иметь задачу, в которой вот этот прелизм внутренний, он есть, хотя бы потенциально,
что там есть операции, которые можно выполнять независимо, это первое, это требование как бы
необходимое, потому что если их там нет, то вам бесполезно иметь много процессоров, а второе,
это как правильно ее взять, как правильно организовать вычисление так, чтобы у вас действительно
суперкомпьютер считал, чтобы вот эти ядра и исполнительные устройства, операционные устройства
не мешали друг другу, не ждали друг друга, чтобы были минимальные накладные расходы на организации
вычислений. Собственно говоря, они есть, эти расходы, даже когда у вас один процессор. Вот в свое время в сети
была замечательная лекция воеводина суперкомпьютер и КПД паровоза, в которой он популярно объяснял,
что один процессор, одно процессорное ядро при решении обычных задач не линпака, не специального
теста, который специально оптимизируется для того, чтобы вот здесь повыше подняться в этом списке,
повыше производительность показать. Обычных задач корпоративных кодов, научных кодов, пакетов
прикладных программ параллельных, она порядка трех процентов. И происходит это как раз во многом за счет того,
что вы, первая проблема стены памяти, это означает, что вы данные, которые расположены в оперативной
памяти, не успеваете с нужной скоростью передавать в процессор. Память работает гораздо медленнее.
И вот список тест линпак, это решение систем линейных уравнений, он как раз там все сделано для того,
чтобы эту проблему преодолеть, чтобы эффективно использовать кэш-процессор, регистр процессора,
вот там все сделано для этого. А когда вы решаете свою задачу как прикладной программист,
как специалист, которого позвали для оптимизации разработки нефтяного месторождения, вам не до теста
линпак, вам нужно решать задачу, вам нужно понять какие там уравнения, что там по физике происходит,
как сделать так, чтобы у вас результат вашего вычислительного эксперимента был похож на то,
что в природе происходит, чтобы его можно было валидировать, чтобы потом можно было
использовать госты, которые сейчас разрабатываются, использовать в производстве ваши расчеты,
к этому все идет. Вот вам этот момент для оптимизации кэша, хотя иногда это приходится делать,
и поэтому производительность одного процессора на уровне нескольких процентов от того, что он
мог бы делать, вы его далеко не полностью используете. А потом на это накладывается еще
эффективность использования нескольких процессоров, которые могут быть по-разному соединены,
они могут быть на общей оперативной памяти, многоядерной системе, но по определению
имеет доступ к общей оперативной памяти если несколько ядер, каждый из этих яpu entitled
имеет возможность адресовать любую ичейку оперативной памяти, а могут быть на распределенной
оперативной памяти, если у вас объединена сеть ноутбуки, то вот будет система с распределенной
оперативной памяти, и там своя эффективность, вы тот алгоритм, который у вас последовательно
выполнялся за час, на десяти процессорах он не будет за 6 минут выполняться,
он будет выполняться скорее всего там дольше. Возможно и обратно ситуация,
но скорее всего будет дольше выполняться. И наша зона внимания будет как раз вот в этой
части. Как использовать один процессор? Это отдельная история, и ее освещать довольно
тяжело. Ну хотя бы потому, что с течением времени меняется причина, по которой процессор работает
хуже, чем мы думаем. Иногда это связано действительно с тем, что мы не очень аккуратно используем
кэш-память. И некоторые рекомендации в этом смысле в наших лекциях будут, и я буду рассказывать
о способе, который позволяет вроде как на ровном месте существенно выигрывать производительность,
просто из-за того, что существенно поменялся порядок вычислений. Но это только одна причина.
А я уже сказал, что аппаратчики, вот те, кто делает аппаратуру, процессоры, они обгоняют
алгоритмистов очень сильно. И сейчас довольно существенная причина того, что процессор работает не так,
как вы думали. Такие вещи, как спекулятивное вычисление. Вы думаете, что вы написали программу,
в ней есть ветвление, и там у вас и в что-то такое больше нуля, и это что-то больше нуля,
а вы думаете, что выполняется левая веточка, которая соответствует больше нулю. А процессор
выполняет обе веточки. И уследить за этим на нашем уровне, на уровне системного, нет,
прикладного программиста. Системный программист, может быть, за этим уследит. А вот прикладной
вряд ли. А в результате процессор выполняет гораздо больше операций, чем вы думали. Ну, естественно,
он тратит на это больше времени. Поэтому вот в эту сторону нам сложно копать, я не берусь. А вот
пообсуждать и понять, как распределить работу между процессорами так, чтобы с нашей точки зрения
мы наиболее эффективно использовали много процессоров. Вот этим мы будем заниматься. Вот это
вот вещь, на которую мы можем влиять непосредственно, и которая нас будет интересовать. Значит,
я как бы сразу некоторую антирекламу сделаю, Курса, чтобы не было иллюзий. Я вот сейчас называл
системы, в которых много процессоров, они между собой могут быть по-разному связаны. Они могут
быть связаны общей памятью, распределенной памятью, с помощью сети передач и данных,
например. Они могут быть вообще не связаны, но при этом работать согласовано, как единое целое.
Это векторная система. Векторная система предполагает, что у вас несколько исполнительных устройств над
разными данными выполняет одну и ту же команду, например, складывает по элементам два вектора,
или вычитает, или умножает. Вот что в таком духе делают. Вот хороший пример, близкий пример,
живой такого сорта системы, это графические карты. Там вы когда запускаете свою программу,
вы на самом деле запускаете одновременно поток из большого количества. Поток,
выполняющий много одинаковых операций над разными данными. И это своя наука, как правильно сделать,
чтобы это хорошо работало. Если у вас в такой конструкции, не дай бог, есть ветвление, то вот
смотри пункт первый. Процессор не знает, что будет, он выполняет обе веточки. А хуже,
если у вас вектор данных, над ними выполняется одна и та же операция, и вот над элементами вектора
стоят, если элемент больше или меньше. И для половины он больше, а для половины он меньше.
Ну все, он будет выполнять обе веточки, причем не одновременно, а последовательно, процессор.
Так вот, все, что связано с программированием графических карт и так далее,
непосредственно в лекциях я рассматривать не буду. Но я буду говорить о некоторых
средствах автоматизации разработки параллельных программ, и если все сложится нормально,
будет одна или две лекции, посвященные системе DBM. А некоторые те, кто находится у нас на базе,
познакомиться с ней, я думаю, еще раньше. DBM и DBMH это система автоматизации разработки
параллельных программ. Вот это отдельная история, на каких языках вообще писать для
параллельных систем, для многопроцессорных систем, на чем писать. Но мы будем иметь в виду Си,
в основном. Вот я буду рассказывать, имею в виду Си. На фортране кто-нибудь пишет?
Раньше руки поднимались, а на ассембляре? Ну нет, на ассембляре писать не будем, хотя,
если кто-то там welcome, что называется для ускорения, все средства хороши. Так вот,
это Си, это обычный Си. Языки-то бывают разные, и когда появились массово, распространились первые
параллельные системы, вот именно пошли, действительно очень такое широкое распространение
получили, транспитерные системы. Они были одновременно, они были позже сделаны, чем язык
программирования для них. Было язык программирования АКАМ, вот такой философ известный, да, Лезвий АКАМ,
Бритова АКАМа, который говорит о том, что не надо придумывать сущности, надо обходиться
минимальным числом понятий. Так вот, был придуман язык АКАМ, создан, который был изначально ориентирован
на параллельную обработку. У него внутри языка были параллельные конструкции. Язык мощный, хороший,
но он не прижился. Он не прижился, во-первых, потому что сами транспитеры жили относительно
недолго. Хотя это было такое очень грамотное и хорошее решение. Это были кристаллы, процессоры,
ни одного сгоревшего я не видел, они были с военной приемкой, они летали в бортовых комплексах,
в космосе и где хотите. И они на борту содержали каналы связи, четыре канала передачи связи.
Вот мы Азернет отдельно подключаем плату, на материнской плате часто у нас есть интерфейс Азернет,
а там были линки, которые были прямо в процессоре. И относительно простыми вот этими линками это
были четыре проводочка, никак не экранированных, можно было соединять сколько угодно их и получать
параллельную систему. Но тем не менее, они прожили недолго, производительность каждого такого транспьютера,
если говорить о плавающей запятой, была порядка одного мегафлопа, миллионы операций в секунду с плавающей
запятой. Это был кристалл вот такой вот. Один мегафлоп в свое время это была машина БСМ-6,
два зала, один сверх, второй снизу с питанием, с охлаждением, подвал с охлаждением, вот три зала.
Так вот, потом была попытка их дальше продлить их жизнь за счет процессоров там Т9,
но совсем не прижились. Они прижились потому, что параллельно развивалась очень интенсивно,
процессорные технологии развивались, и появились процессоры U860, объемовские пауры, и транспьютеры
стали использовать просто как коммуникационные процессоры. Ну а в этом качестве они, естественно,
некоторое время жили, а потом не выдержали конкуренции со специальными адаптерами.
Так вот, там был язык АК, и он вместе с процессорами, вместе с транспьютерами так вот и пропал.
А языки общего назначения, C, FORTRAN, дополненные библиотеками передачи данных, синхронизации
примитивов, ну вот всем, что нужно, обвязка, позволяющая запускать параллельные программы,
ведь вы, когда обычно набираете команду, вот у себя в терминале в ринуксовом, или там под
Windows на Enter нажимаете на каком-нибудь экзофайле, он запускается локально, она теперь нужно
запускать на многопроцессорной системе, не только у вас, а еще и у соседа, если он через канал
связи подключен к вам. Вот вся эта обвязка, вот она была сделана поверх языков общегоупотребимых.
Плохо это или хорошо? Наверное, можно найти минусы, но я считаю, что это хорошо, потому что это
позволяет нам с вами оставаться в рамках тех парадигм, которые нам и так привычны. И вот основная
парадигма, в рамках которой мы будем работать, это слабосвязанные параллельно выполняющиеся
независимые процессы, то есть у нас есть некоторое количество процессов, мы пишем некоторое количество
программ, каждый из которых выполняется в своем процессе. Возможно, это одна программа, просто у нее
на входе есть номер процессора и она делает ту часть работы, которая ей предназначена согласно
номеру. Но вот эта идея, что мы пишем некоторое количество последовательных программ, которые
между собой иногда взаимодействуют, иногда передают данные и иногда обращаются к общим данным в
оперативной памяти. Вот эта вот идея плодотворна, она позволяет нам оставаться в рамках привычного
нам понимания сути программирования и описания алгоритмов на привычном нам языке, но при этом дает
возможность использовать параллельные системы. Это не единственный способ выражать свои мысли и не
единственный способ использовать параллельную систему. В той же системе DVM, например, вы вот этого
не делаете, вы пишете один код, последовательный код, а система автоматически вот этим занимается,
но не для любого кода, для кода, который ориентирован на определенный круг задач, ну, например, на
решение тех же задач механики сплошной среды, если у вас есть расчетная сетка большая достаточно,
вы покрыли область двумерную ячейками, у вас этих ячеек N1 на N2, то вот система DVM автоматически
может разложить работу связанная с обработкой вот этих ячеек на несколько процессоров, и вы
тогда не должны беспокоиться о низком уровне, и это здорово, и для некоторых задач это решение
действительно хорошее решение, DVM не единственная система, но вот она живая, есть разработчики,
ее можно пользоваться, но тем самым вы сужаете круг задач, которые вы можете рассматривать,
вот они вот такие, тут вычисления-то могут быть любые, но тем не менее идеологически это некая
расчетная сетка, и вы с ней что-то делаете, и что-то делаете хорошо при условии, что у нас тут
однородное вычисление, ну, например, мы теплопроводно считаем в каждом узле расчетной сетки,
мы находим среднеархиметическое четырех соседних величин, и мы получаем некоторую новую величину,
моделируем процесс распространения тепла или там диффузию, грубо говоря, но если у вас вычисление
не однородное, если у вас где-то здесь больше вычисления, где-то меньше, ну, например, где-то у
вас свечка горит, и там нужно посчитать еще и химию, то вот такой простой подход, он уже оказывается не
очень эффективен, и вам все равно надо понимать, что происходит на нижнем уровне, и поэтому мы
будем изучать именно алгоритмы, которые вот в таком стиле выполняются, каждый на своем процессоре,
и при этом иногда взаимодействует с соседями, но в основном выполняется автономно, в результате
вы имеете возможность писать эффективные последовательные коды, которые между собой
взаимодействуют, и дальше у вас задача минимизировать это взаимодействие, сделать так,
чтобы работа была сбалансированной, чтобы каждый процессор делал полезное дело в основном,
и только иногда передавал данные соседям или там что-то еще делал, ожидал других. Вот это то,
чем будем заниматься, и в этом смысле СИС здесь очень даже уместен, и фортрон тоже, ну,
мне так нет, хотя на фортроне остались большие библиотеки, которые постепенно портируются,
конечно, но где-то используются в первозданном виде. То есть вот основная вот концепция,
мы будем писать алгоритмы, описывая вот эти вот последовательные веточки, то есть если у нас
тысячи процессоров, понятно, что тысячу алгоритмов мы не напишем, но устанем просто,
поэтому мы очевидным образом будем писать провентализуемые алгоритмы, то есть мы будем
возможно писать управляющий какой-то, а остальные будут на вход принимать, ну, например,
вот две классические величины, число процессоров и номер процессора. Ну, вот эти слова они часто
встречаются, когда пишешь с помощью интерфейса параллельного программирования message passing
interface, то есть MPI это то, что вам надо будет знать, но то, что я вам рассказывать не буду. Его вам
будут рассказывать на семинарах, и детали как, что интерфейс достойный, очень много всего умеет,
там 300 с лишним под программой, если не больше уже, а мы из них будем на лекциях использовать
буквально десяток, этот десяток мы будем использовать в таком обобщенном виде операции
передачи данных, получения данных, синхронизации, барьерная синхронизация и так далее, их там вот не
очень много будет, а потом уже каждый из них, она может разворачиваться в целый набор других
операций, то есть вот MPI это первый интерфейс, который, я очень надеюсь, у всех будет практика,
во всяком случае все последние годы так было, и это позволяет вот учиться плаванию не в сухую,
а действительно попробовать параллельный алгоритм и посмотреть, а что же получается, и вы удивитесь,
но по моим наблюдениям у большинства получается как-то не очень, и что печально, большинство это не
осознает, то есть берем программу, она работает минуту, запускаем ее на 5 процессорах, она работает
20 минут, и человек счастлив, она работает параллельно, но вот хорошо бы, чтобы она работала меньше минуты,
вот вы когда параллельную программу будете писать, вы все-таки этим озаботьтесь, она должна работать не
просто быстрее, чем ваша последовательная, она должна работать быстрее, чем самая хорошая
последовательная, иначе зачем вам параллельный алгоритм, вот это мотив, зачем курс нужен, вот это
вот та точка, которая определяет, зачем нужен курс, я начал со слов, что написание параллельных
алгоритмов дело в общем-то хлопотное, потому что человек, не, внутри он там наверняка думает
параллельно, но я не специалист, не знаю как, никто по-моему не знает, но вот излагает свои мысли,
он последовательный, я вот стою здесь, и вот поток идет последовательный, и вы воспринимаете
последовательное, а вам надо написать параллельную программу, надо одновременно помнить о нескольких
потоках исполнения, вообще-то это напрягает, это довольно сложно, вот есть особенности
параллельных программ, которые мешают, они недотерминированы, они склонны к дедлокам, там
много всего есть неприятного, и поэтому первое желание как-то вот понять, ради чего, вот я ее
напишу эту параллельную программу, она будет быстрее работать или нет, и вот для этого и нужен курс,
курс нужен для того, чтобы вы могли до написания параллельной программы оценить, насколько она
потенциально при самом хорошем раскладе позволит вам ускориться, ну я эти слова часто произношу,
вот есть время выполнения последовательной программы, а есть время выполнений программы
на «П» процессорах, вот это вот ускорение, это ускорение параллельной программы, отношение
время работы одного на одном процессоре к «П» процессорам, так вот к работе на «П» процессорам.
Ну можно ещё это дело разделить на «П» и получить эффективность использования
в разъёме мощности процессоров равняется sp делим на p вот эти две величины они
вот будут постоянно звучать так вот до того как вы начнете писать последовать
параллельную программу неплохо было бы понять будет ли она работать быстрее
если там ресурс внутреннего параллелизма и возможно возможно надо подумать о том
а может быть можно другой последовательный алгоритм взять который
просто решить задачу за нужное вам время одно вас считается 10 минут может
быть если вы возьмете другой алгоритм вы сможете на одном процессоре посчитать
за минуту и то дать надо сделать а вот потом же именно
с этим временем вот в самом лучшем последовательном вот именно с ним и
сравнивайте вот самый быстрый последовательный алгоритм берем и с ним
сравним и мы на некоторых лекций будем этим заниматься мы будем отдельно
смотреть, какой алгоритм взять в качестве опорного
в качестве самого быстрого, ну из тех, что мы можем
придумать.
Пока я звездочку не поставил, самый быстрый алгоритм
определения было конструктивное, есть программа, запустили
на одном процессоре, а скорее всего вообще не работает,
потому что она параллельная, и на одном процессоре,
скорее всего, работать не будет, если вы так не написали
специально.
Но тем не менее, какой-то последний алгоритм у вас
есть, и поделили на время работы параллельные программы,
и получили ответ.
А вот где взять самую быструю?
Никто не знает, где ее взять, поэтому мы будем брать ту,
которую сможем сделать сами в самую быструю, и сравнивать
с ней.
И вот если здесь у вас на уровне оценки алгоритма,
да, вот вы посмотрите, сколько там операций, сколько там
операций будет выполняться в последовательности,
сколько параллельно, сколько операций идет на обмен данными,
на синхронизацию, и если ваша оценка покажет, что,
стоит попробовать, что на 10 процессорах 8 раз быстрее
мы можем ожидать, вот тогда, да, можно писать параллельную
программу.
А если эта оценка покажет, что никакого ускорения
не будет, а, скорее всего, будет замедление, такое
и бывает.
Ну, наверное, надо что-то другое делать, наверное,
не надо писать вот эту параллельную программу, которая будет
работать все равно плохо.
Может быть, надо взять другой алгоритм, на основе
которой вы будете делать параллельную программу,
и такие примеры тоже будут.
Будут примеры алгоритмов, которые хороши как последовательные,
они быстрее всех остальных работают, но они не распараллеливаются.
А для того, чтобы сделать хорошую параллельную программу,
надо взять плохой последовательный алгоритм, который работает
медленно, но зато очень высокой ресурс параллелизма
содержит, там много операций, которые можно делать одновременно
и независимо.
И вот он тогда будет выигрывать, и будет выигрывать кратно
в разы.
Вот это основной мотив, почему параллельные алгоритмы,
почему их надо изучать, чтобы не тратить свое время
на разработку программ, которые заведомо не будут
работать, и не создавать тем самым проблемы себе
и антирекламу отрасли в целом.
Потому что параллельная система – это то, что мы
имеем других способов ускорить вычисление, мы на сегодняшний
день не видим.
Я сейчас не говорю про квантовые компьютеры, я про них практически
ничего не знаю, у нас позиция очень простая.
Вот нам дадут квантовый компьютер, квантовый процессор,
дадут правила работы с ним, вот тогда мы будем с ним
работать.
А пока что нам никто его не дал.
Те редкие примеры, которые есть, там D-Wave-система, там
до сих пор разговоры идут о том, совсем он квантовый
или квантовый отжига, что там за система, ну в общем,
все еще впереди.
Это вот раз, а второе – класс алгоритмов, которые сегодня
умеют решать на квантовых компьютерах сегодняшнего
дня, он просто вот там раз-два-три, ну там Grover, в общем, их несколько
всего.
А это система общего назначения, поэтому мы будем про них
говорить.
Значит, еще про что мы не будем говорить.
Насколько я знаю, на фистехе есть курс, посвященный программированию
на логических, на программируемых логических интегральных
схемах, ПЛИС-ах.
Кто-нибудь слышал про ПЛИС-ы, FPGA, надеюсь, что кто-нибудь
дослышал.
Значит, тут я хочу следующие слова сказать, мы не будем
их рассматривать не потому, что это неинтересно, FPGA – это
одно и то же и ПЛИС, просто наше их название, а потому
что это предмет отдельного курса, и надо быть специалистом
в этой области, у меня только общие сведения, а они все
больше распространяются, их все проще попробовать.
Во-первых, они уже дешево, что они позволяют сделать?
Вот это тоже специальный вид процессора, что он
позволяет сделать?
Он позволяет взять ваш алгоритм и сделать процессор, который
выполняет именно ваш алгоритм, только ваш алгоритм, и больше
ничего.
И коль скоро я сказал, что обычный процессор работает
с эффективностью 3%, вот эта штука работает с эффективностью,
если все правильно сделано, близко к 100%, но это десятки
процентов, во всяком случае, и на определенном классе
задачи эта штука, вот эти процессоры, которые вы
сделали специально под задачу, они сильно выигрывают
в скорости, и задачи реального времени на них делают.
Другое дело, что для того, чтобы разработать алгоритм
для суперкомпьютера, некоторое количество материальных
денежных знаков нужно, а чтобы разработать алгоритм
для этой конструкции нужен чемодан денег, вот конструкция
примерно такая.
То есть это дорого, и дело не в том, что кто-то хочет
себе очень много денег, а дело в том, что это действительно
требует больших усилий и долга, нам периодически
обещают, что это вот, да нет, все уже, вот тут языки
есть, которые нам позволяют это делать быстро, в Таганруде
вот этим занимаются профессионально, но масса это пока не пошло,
поэтому я обращаю внимание на то, что это важное направление,
это направление интересное, иметь о нем представление
крайне желательно, уметь его использовать, тем более
желательно, вот, и в жизни пригодиться не пропадет,
FPGA, да, что-то я не написал, вот, но в нашем курсе этого
не будет, пока что это еще за рамками возможностей
обычного прикладного программиста использовать вот эти вещи
очень специфичная область, вообще говоря требующие
работы одновременная математика и инженера схемотехника,
их вообще готовить перестали, но, тем не менее, вот, такие
люди есть, и если у вас будет возможность познакомиться
с этой технологией, я вам рекомендую, ну вот, примерно
я рассказал, я примерно рассказал о том, что будет
в курсе, продолжим, я сейчас хочу две вещи еще обозначить,
значит, первая вещь связана с вопросом, а, собственно,
где параллельная система используется, вот, я вначале
я сказал, что HPC это наиболее интересная для меня область,
но это не единственная вещь, где суперкомпьютеры применяются,
я бы выделал четыре области, которые представляют интерес,
и что важно, для этих четырех областей алгоритмы нужны
разные, вот, разные требования к алгоритмам ставятся во
главу угла, вот, в частности, в контексте вот этих двух
величин, ускорение и эффективность, значит, ну, первое это HPC,
это то, о чем мы говорили, есть, например, вот эта комната,
вы разбиваете трехмерной расчетной сеткой кубическими
ячейками, небольшими эту комнату на фрагменты, и
дальше выписываете уравнение Эллера или Навъестокса,
или еще какие-нибудь, и пытаетесь моделировать, как будет
меняться распределение температуры, скорости воздуха,
концентрация там нехороших веществ разных, вот в этой
комнате, вот это вот здесь, в HPC, и совершенно так вот
интуитивно ясно, что для решения такого сорта задач,
если вы решаете масштабную задачу, процессоров вам
нужно много, скорее всего, у вас их на столе меньше,
чем вам хотелось бы, значит, вы идете в центр коллективного
пользования и берете вычислительную мощность там, а там администрация,
и администрация, с одной стороны, конечно, интересна,
чтобы вы решили важную задачу, написали статью, которая
опубликована и, значит, пошла в зачет, но с другой стороны,
она смотрит, слушай, процессоры используются там на одну
сотую, почти не используются, и один, и второй, и третий,
мы выделили 200 процессоров, а программа считает всего
вдвое быстрее, чем на одном, и вот совершенно понятно,
что если бы вам дали не 200 процессоров, а 10, вы бы
посчитали примерно за то же самое время, то есть
большое количество процессоров вам, в том случае, если ускорение
использование низкое, и вот эффективность использования
низкое, но ее еще можно в процентах выражать, скорее
всего, не дадут, то есть в HPC одно из требований к
алгоритму, ну, кроме того, что он должен правильно
работать там и так далее, относительно высокая эффективность,
ну, я вот так сейчас условно скажу, там больше 30-50%, если
это явные разностные схемы, то это под 90%, если это
не явные схемы или более сложная логика, то это вот
десятки процентов, но не проценты, вам просто не дадут
суперкомпьютер, вам дадут от него маленький кусочек.
Вот HPC, достаточно хорошая эффективность должна быть,
и мы с этой точки зрения будем вот некоторое количество
алгоритмов смотреть.
Вот, но это не единственное, значит, на что, зачем может
суперкомпьютер использоваться, есть такая штука, как решение
задач реального времени, и в реальном времени требование
совсем другое, вот я его все-таки здесь вот напишу
третьим, просто традиционно, вот реальное время, но для
контраста.
Значит, вообще никого не волнует, с какой эффективностью
решается задача, вычислительная система может простаивать
целый год, но в тот момент, когда к вам что-то полетело,
непонятно что, не очень понятно, и надо понять что
летит, у вас есть секунда на то, чтобы эту задачу решить.
Именно в эту задачу, в этот момент вся вычислительная
система должна сработать и сделать то, что к ней требуется,
и если при этом у нее очень большая вычислительная
мощность почти все время простаивает, ничего страшного,
то есть вот это область задач, в которой эффективность
оступает на второй, на третий или на четвертый план.
А более, скажем так, гражданский пример, это сортировочная
станция, железнодорожная сортировочная станция,
на удивление, те же самые транспьютеры, одно из таких
вот применений, довольно широких было, на железнодорожных
путях.
У вас идут составы, идут в реальном времени, и надо
в реальном времени решать какие составы, по каким
стрелкам, по каким путям разводить, чтобы комплектовать
составы, которые пойдут дальше.
Задача реального времени, не очень важно сколько там
будет процессоров лишних, их там должно быть столько,
сколько нужно, чтобы обслуживать поток задач реального времени.
Вот это вот второе применение.
Отдельный класс алгоритмов, но мы его как таковой в наших
лекциях, наверное, рассматривать не будем, просто за неимением
таких задач.
Ну, он есть, вы его себе представлять должны.
А вот что будем рассматривать, вот здесь второй, это промежуток
между ними, это задача обработка большие данные.
Вот я и с ним пишу big data, хотя мне самому вот это
словосочетание, не то что нравится или нет, оно объективным
существует, и многие пользуются, но что это такое, вот каждый
по-своему объяснит.
Обработка больших объемов данных, визуализация данных,
работа с большими коллекциями данных, работа с распределенными
данными по сети интернета, по разным серверам, визуализация
данных вычислительного эксперимента, вот здесь
вот когда вы проводите расчеты, вы результаты пишете куда-нибудь
на диск.
Это, кстати, не обязательно, у нас будет отдельная лекция
по визуализации, оффлайн онлайн визуализация, плюсы
и минусы, что там можно делать, но нормальный ход, когда
вы во время вычислений записываете данные, потом их смотрите,
когда вы данные считаете, у вас большой суперкомпьютер
и вам надо его использовать эффективно, а когда вы данные
начинаете смотреть, ну вы же человек, да, вы посмотрели
на картинку и вы пытаетесь ее осознать, в это время
вы думаете, компьютер простаивает, и это тоже не страшно.
Другое дело, что это не должен быть весь огромный
суперкомпьютер, там Фугаку или кто-то еще, или Ломоносов,
это должен быть кластер визуализации отдельный, относительно
небольшой, который, возможно, одновременно с вами обслуживает
и других пользователей, которые тоже любуются своими
картинками, то есть там требования к оперативной
памяти, как правило, высоки, а есть необходимость обеспечения
интерактивного режима работы, вот если говорить о визуализации,
это не режим реального времени, вы не знаете, когда к вам
придет запрос, у вас очень мало времени на ответ, и
не знаете, сколько их придет, вы можете закрепнуться
в потоке, это относительно равномерный поток запросов
от пользователя, но пользователь согласен сдать реакцию,
если он заказал увеличение фрагмента картинки, хочет
по детальнее ее посмотреть, несколько секунд он ждать
готов, но если речь заходит о нескольких минутах, он
теряет душевное равновесие, интерактивный режим не
позволяет нам выходить за рамки десятка секунд,
и нескольких секунд, и вот он здесь, то есть у вас
относительно небольшие вычислительные мощности
здесь требуется относительно, хотя для самой визуализации
вы можете использовать сколько угодно процессоров,
ну, фильмы, основанные сейчас на компьютерной визуализации
просто вот уже каждый день, и никогда не знаешь, чего
ты смотришь, живого человека или мультяшку, настолько
уже все реалистично, вот, там тот же аватар в помощь
и в пример, но это другое, это вы насчитываете большое
количество кадров, а вот если вы просто изучаете
результаты расчета, то у вас относительно небольшие
мощности, но интерактивный режим работы, и вот здесь
промежуточная ситуация, вы должны с одной стороны
довольно быстро человеку сказать, значит, показать
картинку, вот с другой стороны все-таки не так жестко
имитировано, как здесь, так что здесь какая-то промежуточная
ситуация, эффективность она не должна быть совсем
низкой, потому что иначе вы будете картинку строить
долго, но система в целом может простаивать просто
за счет того, что человек к ней не обращается, вот,
относительно небольшие кластеры используются вот
здесь, либо большие, интернет, обработка больших данных,
в общем здесь большое разнообразие и требований, и приложений,
я почему сказал, что совершенно разное мнение про big data,
ну потому что, например, есть еще результаты, поступающие
с установок то, что сейчас говорят мегасаинс, там
адронный коллайдер какой-нибудь, где на каждом метре стоит
там десяток или сотня, ну десяток датчиков, и с них
идет поток данных в момент проведения эксперимента,
и это огромные объемы данных, и тоже надо уметь правильно
сохранять с тем, чтобы потом интересующие данные можно
было бы работать, сохранить их все просто нельзя, поэтому
часть сохраняется и фильтруется непосредственно около установки,
ну а часть действительно передается уже для децентрализованного
хранения, в общем, вот эта вот область, она достаточно
большая, и мы ее затронем краем в том смысле, что
будем смотреть алгоритмы визуализации некоторые,
вот как это делать, и алгоритмы декомпозиции больших сеток,
в общем, здесь вот будут вопросы, значит, четвертой
области не хватает, вот четвертая область, которая
к HPC никакого отношения не имеет, но имеет самое
непосредственное отношение к многопроцессорности,
кто-нибудь сформулирует ограниченные многопроцессорности,
их тут несколько, нет, зачем, где еще нужна параллельная
система, ну просто вот совсем нужна, потому что иначе без
нее все потеряете, это бортовые системы, космический аппарат
запустили, и если там процессор один, то в тот момент, когда
с ним что-то случилось, а там высокий радиационный
фон, вы потеряли аппарат, поэтому вот любая такая
система, и космические, и необслуживаемые, и автономные,
и так далее, это резервирование я напишу, вместо одного
процессора их ставят несколько одинаковых, а может и не одинаковых,
в общем, все, что я видел одинаковых, хотя может
быть полезнее ставить разные, не знаю, они делают одно
и то же, они делают одно и то же и сравнивают между
собой результаты, и если у кого-то из них результат
вдруг разошелся, ведь не обязательно же отказ процессора,
это ну совсем отказ, перестал откликаться, это может быть
и переброшенный бит в оперативной памяти, но если он там
один переброшен, и если память поддерживает протоколы
схемы коррекции ошибок, то этот бит будет исправлен,
но если там переброшены несколько бит, то уже нет,
а ответ может быть неправильный, причем, по идее, опять же,
если схема коррекции ошибок есть, то система поймет,
что у нее ошибка, если нет, то может и не понять, и просто
ответ другой, то есть все говорят, что надо импульс
выдать в один ньютон, а один процессор говорит в
кому верить, но вот большинство решает, что надо верить ему,
и этот единственный, который выдал не тот ответ, перезапускается
просто, вот это резервирование тоже, эффективность здесь
очевидна какая, вот одна четверть, если их четыре,
и никого это не беспокоит, но зато это системы повышенной
надежности, они тоже есть, и там проверенная система
используется, мы эти алгоритмы смотреть не будем, но мы будем
смотреть другое, в свое время на первую позицию
в списке топ-500 вышел китайский компьютер, ничего не предвещало,
там всегда были американцы, японцы, американцы, и вдруг
вот Китай, что они сделали, они собрали, не знаю, со
всей страны или со всех геймеров графические карточки,
и поставили их в один суперкомпьютер, и вышли, сразу они перекрыли
производительность всех остальных, они сильно отрывались,
но они собрали карточки у геймеров, игровые видеокарты,
у которых вот этого вот не было, в результате эта
система работала непрерывно, ну, масштаб часа, наши специалисты
из Новосибирска, если я правильно помню, на ней считали,
как они на ней считали, они брали весь этот суперкомпьютер
и резали его на три части, вот весь суперкомпьютер,
и запускали одну и ту же задачу, три раза, ну и смотрели
на результаты, потом через некоторое время, через этот
самый час одна из частей говорила, ну все, а те две продолжали
считать, и там уже расходились результаты, но счет заканчивался,
и почему, потому что это были карты бытового уровня,
и во время расчета вот происходило то, о чем я сказал, если у вас
на графической карточке какой-то бит вдруг переброшен,
и вы в какой-то там на сотой секунде в верхнем углу
увидели вдруг мелькнул красный пиксел, вам абсолютно
все равно, но если вы используете память оперативную графической
карту для выполнения расчета, и там на сотой секунде у
вас вместо степени 10 оказалось степень 100, то вам не все
равно, а происходило именно это, поэтому, собственно
говоря, ровно то же самое делали отца-основатели
еще во времена, когда были машины стрела, ламповые
еще машины, они выполняли один расчет несколько
раз на такой машине, но резать ее было нельзя, она была
последовательная, но провести расчет дважды подряд можно
было, с тем, чтобы убедиться, что да, ответы правильные,
то есть резервирование, оно не только на космических
кораблях, оно вот и в HPC расчетах тоже в полный раз использовалось,
вот если вы сейчас заказываете систему даже X, либо еще что-то,
то у вас этой проблемы уже не будет, потому что уже
вот эти карты пошли в SGP, в SGPU, до общего назначения
графические карты не игровые, а общего назначения, вот
они уже с защитой памяти, вот системный блок, который
стоит в суперкомпьютере, он тоже с защитой памяти,
но понимаете, что это такое, я ЦС, это что такое, вот
у вас есть 8 микросхем памяти, там 1, 2, 3, 8, а рядом стоит
9, которая хранит контрольную сумму по модуле 2 всех вот
этих вот, если у вас, ну в простейшем случае, если
у вас какой-то бит перебросился, то вы об этом хотя бы узнали,
вы узнали, что у вас здесь где-то возникла ошибка, вот
если таких микросхем там две, то вы уже имеете возможность
ее исправить, и вот эти серверные материнские
платы, серверные системные блоки, они поддерживают такую
коррекцию ошибок, вот, так легчему, тут как бы вот
мы сейчас поговорили, ну все понятно, а вот теперь
смотрите, у нас 10.18 операции в секунду, это 10.9 исполнительных
устройств, вот по всем оценкам специалистов, когда такая
штука заработает, она непрерывно больше получаса работать
не будет, хотя бы один из процессоров из строя выйдет,
ну просто по определению, значит то ли это будет аппаратная
проблема, нет, он не скончается, он просто некорректный ответ
либо перестанет отзываться, ну с ним что-нибудь произойдет,
то ли из-за аппаратных особенностей, то ли из-за особенностей
того, что там сложное программное обеспечение в программе
будет сбоено, в общем где-нибудь что-нибудь да случится, за
полчаса, вот, а что делать, вот сломался процессор один,
и нормальный ход, он какой, ну мы иногда записываем
данные, и если что-то сломалось, то мы перезапускаем расчет,
так вот, на системе вот такого класса у вас этой возможности
не будет по определению, потому что для того, чтобы
записать такую согласованную контрольную точку, у вас
полчаса уйдет, а через полчаса, пока вы будете ее читать,
следующий процессор из строя выйдет, и вот алгоритмы,
которые позволяют, тем не менее, в этих условиях
непрерывно продолжать расчет так, как будто бы ничего
не происходит, так как будто ошибок нет, вот эти алгоритмы
мы посмотрим, ну это некоторые специальные классы алгоритмов,
а именно выполнение расчетов моделирования задач газовой
динамики, например, вот, и в этом смысле мы отказу
устойчивость затронем, то есть здесь аппаратчики
делают много, люди, которые занимаются аппаратурой,
но и с точки зрения алгоритмов нужно свое видение предложить.
Технологические основы есть, я говорил, что мы будем
использовать MPI, так вот, MPI, начиная с версии 4, вот,
у третьего еще возможности не было работать, если какой-то
процессор оказался выведенным из строя, а вот у четвертого
уже есть, дополнительные функции появились, которые
позволяют вам понять, что какой-то из процессоров
или несколько процессоров из строя вышли, из общего
вычислительного поля их логически вывести, а
на оставшихся продолжать считать, так что вот это
мы будем рассматривать.
Так, а какие у вас, собственно, есть вопросы, я тут много
чего говорил, я думаю, что, да, а вот со следующего
раза и будет, со следующего раза, я думаю, что я сейчас
не буду, осталось 10 минут, даже меньше, с учетом того,
что вот, я сейчас не буду, значит, в следующий раз что
будет, во-первых, будет описание функций, которыми
мы будем пользоваться, значит, это функция передачи
данных и, ну, и на первый раз достаточно будет, а во-вторых,
будут алгоритмы, значит, это сдваивание, геометрический
парализм, то, что с ними связано, так что там уже
пойдут вполне конкретные вещи, но как бы вот, не рассказав
вот всего этого, не очень понятно, что и ради чего.
Еще какие вопросы?
Понимаете, какая штука, я бы ее с удовольствием
отдал, но она под лицензионным договором с создательством,
то есть там ссылки нету, значит, я чего могу, я могу их несколько
просто привезти, экземпляров вот положить, берите, а на
сайте, я ее раздавать не могу, но я знаю, что люди
находили.
Нет, вопрос нет, я знаю, что находили, но эта позиция
издательства университета, она вот как-то так себя ведет.
Так, если вопросов больше нету, тогда я благодарю за
внимание, приглашаю вас на следующую лекцию, вот,
главное не болейте, всем здоровья, удачи.

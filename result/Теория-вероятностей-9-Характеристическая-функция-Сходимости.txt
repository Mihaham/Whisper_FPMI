все отлично так значит в прошлый раз мы с вами ввели характеристическую функцию на самом деле
это просто преобразование фурье наряду с характеристической функцией рассматривается еще
вот такие похожие на похоже на нее функция то есть она как характеристической только комплексные
экспоненты нет это как комплексной да и нету видите вот это функция называются характеристической
функцией моментов или производящие функции моментов так сказать в иностранной литературе у
нее есть название специальная мгф функция moment generation function ну и собственно чем она любопытна
по большому счету ничем свойства у ней схоже с свойством характеристической функции но в
частности ее катая производная в нуле в точности равна к этому моменту если он существует функция
моментов от суммы независимых случайных величин равна произведению этих моментных функций ну
и там остальные свойства типа изменения при линейном преобразовании и так далее все похоже
ну вот возникает вопрос зачем она нужна тем более что у нее есть такое свойство или точнее
возникает вопрос зачем нужна характеристическая функция если есть такая же значит дело в том что
характеристическая функция как мы знаем всегда ограничена по модулю единицей и всегда существует
для любой случайной величины характеристическая функция а-t это еще равномерно непрерывная
функция а вот функция моментов может вставать не всегда или не при всех t и она не ограничена то
То есть в теоретической точки зрения моментная функция менее удобна, чем характеристическая функция.
Но с другой стороны, моментную функцию можно вводить в курсы для студентов,
которые не знают теории функций комплексного переменного.
Это тоже с педагогической и методологической точки зрения полезно,
потому что в общем свойства-то в основном те же, а про комплексно-значную плоскость можно не говорить.
И еще наряду с функцией моментов, еще бывает частенько, но это вопрос удобства.
Используют функцию математическое ожидание t в степени кси.
Мы вообще знаем такую как бы функцию, что это такое, мы как называем ее?
Ну в общем случае, там сумма ряда там t в степени к или фетка.
Производящая функция, это производящая функция.
Ну и тоже в иностранной литературе она имеет собственное название pgf probability generation function.
То есть функция генерящая вероятности. Откуда это название берется?
Если кси у нас принимает значение, ну натуральные числа,
то тогда производящая функция обладает таким свойством.
Ее k-тое производное в нуле равно k-факториал на вероятность того, что кси равно k.
Иногда бывает удобно, но вот чем имеет место.
Ну вернемся к моментной функции, о которой мы, так сказать, вот тут поминули ее.
И отмечу, что есть у нее еще одно полезное свойство, чисто техническое, но важное.
И давайте мы как бы рассмотрим на конкретном примере, чем нам порой может помочь моментная функция.
Значит, давайте припомним гамма распределения,
которая зависит от двух параметров и задается своей плотностью.
α, λ, х все больше нуля.
Ну и задача найти характеристическую функцию гамма распределения.
Ну что мы должны сделать? Мы должны эту функцию умножить на e в степени и tx и проинтегрировать.
А что будет под интегралом? Тригинометрическая функция умножена на степенную функцию, умножена на экспонент.
Довольно сложная штука. Ну можно, конечно, безусловно, но получается сложновато.
Поэтому давайте мы для начала найдем моментную функцию гамма распределения.
Я тут без параметров буду писать.
И сделаю такие немудренные преобразования.
Правильно, да?
Ну вот это с этим сокращается. Вот она лямбда в степени α, а вот он e в степени xt.
Вот отсюда сразу видно, когда существует функция моментов.
Для гамма распределения она существует, но не для всех t.
А для каких t? Вот для таких t.
Вот для таких t она существует.
И если лямбда минус t больше 0, чем уровень вот этот интеграл?
Коллеги, это интеграл от плотности случайной величины, взятый по всему пространству.
Еще раз. Чему равен? Единицы.
Потому что вот под интегралом ничто иное, как гамма распределения с параметрами α и лямбда минус t.
Правильно, да? И отсюда мы получаем, собственно, вот ответ.
Вот это моментная функция, но в области лямбда минус t больше 0.
А теперь давайте заметим такую вещь, я ее вот здесь запишу.
Если в моментную функцию чисто формально подставите t, то получится характеристическое.
Видите, да?
Ну я подставлю сюда чисто формально и напишу, фиксия t равно лямбда на лямбда минус i t в степени альфа.
Могу я так? Вытирую функции комплексного переменного?
А вы ее еще не проходите?
Понятно.
Ну тогда скажу, вы в будущем это узнаете. Ну по крайней мере комплексную экспоненту вы знаете, да?
Значит, смотрите, вот эти функции от комплексной переменной, у них есть такое любопытное свойство.
Ну, правда, некий класс очень широкий называется аналитические функции.
У него есть такое свойство, если у вас две функции совпадают на какой-то сходящейся последовательности,
то есть значение двух функций совпадает на какой-то сходящейся последовательности или на каком-то интервальчике,
то эти функции равны вообще во всей комплексной плоскости.
Но для вас это просто формальная подстановка.
То есть законность этой подстановки это уже, ну, как бы некая теория.
Для вас это формальная подстановка.
Подставили ИТ и получили характеристическую функцию гамма-распределения.
Ну и отсюда всякие интересные выводы в принципе следуют.
Например, мы с вами вводили показательные распределения как гамма-распределения с параметрами 1 лямбда.
Ну и чему тогда равна характеристическая функция экспоненциального распределения?
Ну просто α равно 1, подставляем, получаем лямбда на лямбда минус ИТ.
Лямбда на лямбда минус ИТ.
А теперь давайте будем считать, что альфа у нас натуральная, альфа равно некому n.
Тогда характеристическая функция гамма-распределения с параметрами n и лямбда равно лямбда на лямбда минус ИТ в степени n.
Вот из этой формулы это следует.
Что отсюда, какой вывод мы делаем глядя на это соотношение?
Что сумма независимых случайных величин, имеющих экспоненциальное распределение в количестве n штук имеет гамма-распределение с параметрами n лямбда.
То есть отсюда мы сразу получаем, что гамма альфа лямбда равна сумма эксп лямбда ж, ж равно 1 до n.
Ну вот вывод, который без характеристических функций довольно сложно было бы получить.
Хотя еще раз повторю, аппарат характеристических функций – это техническое средство, не более того.
Теперь напомню вам, что мы еще вводили хи квадрат с n степенями свободы.
Что это такое было?
Гамма с какими параметрами?
n пополам, 1 вторая.
Напоминаю вам это.
Тогда характеристическая функция хи квадрат с n степенями свободы, ну просто подставляю 1 вторая, 1 вторая минус и t в степени n пополам.
Гамма от n да, совершенно справедливо, спасибо.
Ну еще тут для красоты, числитель знаменатель домножает на двойку, получается 1 делить на 1 минус 2 и t.
Хи квадрат с одной степенью свободы, что такое?
Ну это надо просто n подставить 1.
И что, какой вывод можем сделать, глядя на эти две формулы?
Что хи квадрат с n степенями свободы это сумма хи квадрат независимых величин с одной степенью свободы в количестве n штук.
Ну и это еще не все.
На самом деле хи квадрат с одной степенью свободы это известная нам случайная величина, но я сразу анонсирую.
Оказывается, что хи квадрат с одной степенью свободы это квадрат стандартной нормальной случайной величины.
Ну как это получить?
Ну давайте выпишем функцию плотности для хи квадрат с одной степенью свободы.
Это значит сюда нужно подставить альфа n пополам и лямбда 1 вторая.
Точнее говоря альфа 1 вторая, поскольку с одной степенью свободы.
Ну и получаем эту плотность, вот так я напишу, ее плотность.
Это единица на корень из двух, гамма от 1 и 2 чему равна?
Корень из пи, х в степени минус одна вторая, внесу сюда х и на е в степени минус х пополам.
Если вы, ну не знаю, давайте сделаем как бы.
Давайте найдем функцию плотности вот этой случайной величины квадрат стандартного нормального распределения.
Вот так хи напишу, fх от х это вероятность, точнее говоря, f большой функции распределения.
Это вероятность того, что стандартная квадрат стандартной нормальной величины окажется меньше х, причем х больше нуля, так с кубочками заметим.
Это равно вероятности того, что n01 будет меньше корень из х и больше минус корень из х.
Это есть функция стандартного нормального распределения в точке корень из х, минус f01 в точке минус корень из х.
Ну и для того, чтобы получить плотность, надо просто продифференцировать.
Если вы это сделаете, то получите точно такую плотность.
И это означает, что х квадрат с одной степенью это квадрат стандартной нормальной случайной величины.
И отсюда мы и делаем вывод, который вам будет полезен в дальнейшем.
Это представимый стихи квадрат степени n вот такой суммой.
Точнее говоря, для вас будет важно, что вот такая сумма имеет х квадрата распределения.
На таком весьма простом примере мы получили различные свойства, которые, в общем, так сходу и не поймешь.
А скажите, пожалуйста, х квадрат с двумя степенями свободы, какой имеет распределение?
Для этого вот сюда нужно поставить n равно 2.
Если вы поставите n равно 2, то получите вот эту формулу, где λ одно и второй.
То есть сумма квадратов двух стандартных нормальных случайных величин имеет показательное распределение с параметром 1 и 2.
Вот такие всякие соотношения, которые так с легкостью получаются с использованием аппарата характеристических функций, а без него получается сложно.
Давайте пойдем дальше еще нам немножко.
Напомню вам распределение каши, которое имеет два параметра. Я напишу для 0 и 1.
Или, ладно, напишу сразу а и сигма, которые имеют вид 1 делить на π сигма, 1 плюс х минус а делить на сигма, все в квадрате.
Ну, давайте считать сигма больше нуля, а если это не так, то модуль сигма тут надо ставить, вот здесь.
Ну, давайте считать сигма больше нуля.
Ну и так сказать, стандартное распределение каши с параметрами 0 и 1, соответственно, имеет вид 1 делить на π 1 плюс х квадрат.
Чему равно математическое ожидание этой случайной величины?
Оно не существует, и именно оно не равно плюс или минус бесконечности, то есть вот интеграл либега вот от этой функции х dx не существует.
В положительной части он плюс бесконечности, в отрицательной части минус бесконечности.
Вот это такое свойство, которое всегда упоминают и как примерное распределение, у которого нет мат ожидания, приводит распределение каши.
Что это за распределение? Ну вот, например, отношение двух независимых стандартных нормальных случайных величин имеет распределение каши.
Например, вот, нас интересует характеристическая функция, ну мы хотим получить характеристическую функцию.
Значит, ну попробуем, так удачно у нас получилось с моментной функцией, давайте моментную функцию сначала попробуем.
Чему этот интеграл равен?
Как е в степени t умножить tх, вот же определение.
Ну интеграл равен бесконечности, причем существенно для любого t, только при t равном нулю, как и любая характеристическая функция, он равен единице.
Вот, а так он равен бесконечности, то есть здесь нет никакого интервалчика из t, для которых существует моментная функция, поэтому этот фокус не пройдет, как у нас было в прошлый раз.
Вот, в принципе, давайте я напишу характеристическая функция, ну стандартная характеристическая функция,
у меня такая легко запоминающаяся характеристическая функция, минус модуль t, е в степени, минус модуль t.
Наиболее так элегантно и просто она получается с использованием аппарата функции комплексного переменного,
то есть надо просто взять интеграл е в степени tх, но вы этого делать не умеете, поэтому я собственно это опускаю часть,
но тем не менее можно получить эту характеристическую функцию просто обратным преобразованием в фурье,
но ввиду громоздкости делать не буду, кто как бы интересуется обратное преобразование в фурье можете взять и получить вот эту вот функцию.
А можно сделать, ну точнее говоря взять от этой функции обратное преобразование в фурье и получить вот эту функцию, вот по какому пути нужно пойти.
Так, значит, вот характеристическая функция, ну какой, так же как и для нормального распределения,
у распределения каши есть такое свойство, устанавливаемое, ну тоже вот по такой типа схеме.
То есть если у вас есть стандартная случайная величина каши с параметрами 0,1, то сигма на случайную величину плюс a,
вы получаете распределение каши с параметрами a сигма. Все понимают как это показать, да? Тогда я это опущу.
Вот, значит, имеет место такой факт. Ну и давайте напишем, чему равна характеристическая функция уже для произвольного распределения каши.
Она равна e в степени i, t, a на характеристическую функцию.
Она равна e в степени i, t, a на характеристическую функцию в точке сигма t.
Ну мы вроде договорились сигма больше нуля считать, тогда это минус сигма модуль t.
Минус сигма модуль t.
Ну и отсюда видно из характеристической функции, что сумма независимых случайных величин, имеющие распределение каши,
ну с разными параметрами, там a житая, сигма житая, тоже будет иметь распределение каши.
То есть если мы рассмотрим вот такую a житая, сигма житая, ну ж от единицы до некоего k, то она будет принадлежать каши с параметрами сумма a житая, сумма сигма житая.
Это всем понятно, да? Характеристическая функция суммы, произведение у характерических сумм функции умножаем.
Здесь собираются a житые, здесь собираются сигма житые и значит вот такой факт имеет место.
Вот это значит то, что касается распределения каши и соотношений, которые нам позволяют получить характеристические функции.
Значит для случайных величин аппарата характеристических функций мы на этом закончим.
Остальные там примеры довольно тривиальные, типа нахождения характеристической функции, там не знаю, Плассоновского, например, распределения.
Ничего интересного нет, поэтому не будем тратить время.
А сейчас мы рассмотрим еще один существенный важный пример применения характеристических функций, но только для случайных векторов.
Случайных векторов.
Случайных векторов.
Значит с вашего позволения вот это сотру.
И чуть-чуть преобразую определение характеристической функции.
Определение характеристической функции вектора это функция, аргументом которой является вектор той же размерности, что и сама случайно влечена.
И вот здесь вместо тксиста стоит т транспонированная кси.
Ну то есть если чуть подробнее.
Вот такое определение.
Ну для нас важно, что тут существует тоже взаимодназначное соответствие.
То есть каждый векторный функции характеристической функции соответствует случайный вектор однозначно.
Есть аналогичные свойства там с интегрированием, причем тут уже смешанные, производные, частные, но не будем на этом останавливаться.
Особо нет на это времени.
Вот и мы сейчас применим вот это определение характеристической функции для случайного вектора для введения в рассмотрение так называемого обобщенного нормального вектора.
Обобщенный нормальный вектор.
Ну сначала немножко вернемся назад, сами вспомним.
Значит у нас нормальный вектор имеет два параметра.
Это математическое ожидание вектора кси, математическое ожидание вектора и кавалиционную матрицу.
Кавалиционная матрица, напомню, это математическое ожидание, кси центрированное, на кси центрированное транспонированное.
интеллижено-транспонированная. Это матрица k на k. Если k, это у нас размерность вектора.
И мы нормальный вектор, увеличились его плотность.
Единица делить на 2p в степени k пополам. Корень квадратный из детермината Rc. Для нормального
вектора, который мы называем невырежденным, Rc это положительно определенная матрица.
Ну и соответственно, детерминат у нее положительный. А здесь экспонента минус,
x минус m это вектора, Rx минус m пополам. Вот так мы ввели нормальный случайный вектор через
его плотность. Припоминаете, да? Так, ну а теперь давайте сделаем вот что. Пока это еще
не характеристические функции, подготовительная работа, просто факт, который будет нам нужен.
Давайте рассмотрим невырожденное преобразование. Это равно некая матрица b на x.
B невырожденная матрица. Линейное невырожденное преобразование. Ну и давайте просто по правилам
замены переменных под интегралом, получим плотность этого нового вектора. Ну, во-первых,
x равно b минус 1 это. Значит, модуль якобиана равен, детерминат b минус 1 единица делить на
детерминат b по модулю. Но я это перепишу, дальше станет понятно почему. Вот таким образом,
единица делить на дет б, детерминат b, на детерминат b транспонированная, корень квадратный.
Избавился от модуля. Понятно, да? Так, значит, давайте посмотрим, как у нас преобразуется плотность. Ну,
во-первых, значит, в новых переменах, которые мы назовем у, это будет bх. Ну,
а соответственно, повторяюсь, x это будет b минус 1у. Ну и давайте плотность преобразуем.
Значит, я напишу fn, только тут уже напишу mθ rθ от y равна единица на 2πk пополам. Тут
напишу det rx, детерминат b, на детерминат b транспонированного, так длинновато получается,
умножить на экспоненту. Ну и тут честно все заменяем. x заменяем на b минус 1 y минус mx,
rx в минус 1, тут транспонированная, b в минус 1, y минус mx делить пополам. Давайте я пока
поработаю ровно вот с этим выражением. Транспонирую b в минус 1 вынесу, получу y минус b
mx транспонированная, b в минус 1 транспонированная, rx минус 1, b в минус 1, y минус bmx пополам.
b на mx это что такое?
Ну берем от ожидания от обоих сторон, здесь получается bmx, а здесь что? mθ, да? Это,
не тета, это. Значит равно, тогда вот я вот это выражение еще-еще преобразовываю, y минус m,
это транспонированная, вот эту штуку я таким образом сверну, b на rx, b транспонированная в
минус 1, на y минус m это делить пополам согласно с такими преобразованиями.
И получаем. Сейчас запишу, поясню. m это r это от y равно единицы делить на 2 pi
в степени k пополам, на корень квадратный из детерминанта b rx b экспонента минус,
а транспонирование надо, да. Значит минус y минус m это транспонированная b rx b транспонированная в минус 1,
y минус m это пополам. Ну и мы видим, что если исходить из того, что m это, это bmxy, а r это,
это b rx b транспонированная, то получается, что при линейном преобразовании, невыразденном,
нормальный случайный вектор переходит в нормальный случайный вектор. Правильно?
Нет вопросов? Ну иногда въедливый слушатель задает такой вопрос.
Когда мы здесь писали функцию плотности изначально, Туэр у нас имела вполне конкретный смысл, это
матрица с кавериацией. Здесь, конечно, можно так обозначить, но будет ли вот эта матрица,
матрица кавериации. Ну, очевидно, что да. Если сюда вместо кси подставить b в минус 1 это,
b в минус 1 это, то вы получите ту же самую матрицу. Точнее говоря, давайте, r это,
это по определению должно быть математическое ожидание, это транспонированное, это на это
центрированное транспонированное, вместо это подставляем bxy транспонированное, ну и тогда
получается, что вот эта кавериационная матрица как раз равна br, кси b транспонированное. То есть
действительно при невыразденном линейном преобразовании нормальный вектор переходит
в нормальный вектор, причем его параметры изменяются вот таким образом у нового вектора.
Это предварительно у нас. Давайте я вот сюда запишу и оставлю эту доску,
чтобы нам было напоминание. Это равно bxy, тогда получается нормальный вектор с параметрами bmx и
кавериционной матрицей brx, b транспонированно. Вот просто будем, запомним это. Так, пошли дальше.
Значит, давайте опять возьмем невыразденный нормальный вектор. Это. Невыразденный нормальный
вектор означает, что матрица кавериаций у него положительно определена, и еще она к тому же
симметрическая. Отсюда следует, что существует такая артагональная матрица U, такая, что U,
R это на U транспонированная, будет равно диагональной матрице sigma, которую запишем так sigma1 в
квадрате, sigma k в квадрате, а здесь везде нули. Правильно, да? Существует такая матрица. Ну а раз
такая матрица существует, то давайте ведем вектор, новый вектор. Этот у нас это, да? Ведем
новый вектор кси по правилу U на это. Ну, отдохните и продолжим. Так, ну теперь, собственно, как это,
попробуем воспользоваться аппаратом характеристических функций. Давайте найдем
характеристическую функцию вектора кси. phi кси от t по определению, это математическое ожидание
E в степени и сумма t житая кси житая, ж равно от единицы до k. Но у вектора кси матрица кавериации
диагональная. Что отсюда следует для нормального вектора? Что его компоненты независимы. Поэтому вот
это не что иное, как произведение характеристических функций, характеристических функций по определению E в
степени и т житая кси житая, ж равно от единицы до k. Вот именно то свойство, что для нормального вектора
из некоррелированности компонентов следует их независимость, позволяет нам так записать. Ну,
а характеристическую функцию нормального распределения мы ведь знаем, да? Это будет произведение E в
степени и т житая м житая. Я пока индекс и упущу, ну уберу, чтобы не писать тут кучу индексов. Умножить
на е в степени минус сигма житая в квадрате t квадрат пополам. Правильно? Вот это вот
характеристическая функция нормальной стандартной величины с дисперсией сигма житая им от ожиданий
м житая. А это я перепишу в такой форме. Е в степени и т транспонированная м кси минус т транспонированная
е т пополам. Правильно, да? Вот. Ну а теперь, как вы, наверное, догадываетесь, мы вернемся к
характеристической функции вектора это. Как мы это сделаем? Да по определению мы напишем фи это в
точке t, это математическое ожидание и транспонированное это, а вместо это мы напишем у транспонированной кси.
Матрица артагональная, поэтому обратная равна транспонированной. Ну или чуть покрасивее е в
степени и у т транспонированная на кси. И мы видим, что характеристическая функция случайной величины
это, это характеристическая функция случайной величины кси, только взятая в точке у t. Ну и поставим это.
Вот сюда подставляем. Е в степени и вместо т транспонированного пишем t транспонированная
u-транспонированная mx-и минус t-транспонированная, это будет у нас
t-транспонированная u-транспонированная
sigma ut пополам равно e в степени i t-транспонированная
u-транспонированная mx-и что такое матрица артагональна обратная равна
диагонали у транспонированная mx-и это что такое это м это вектор ну минус а
у транспонированная sigma u глядя на то что на это чему равна у транспонированная
sigma r это вот ну совершенно вот наш ответ вот характеристическая функция
невырожденного нормального распределения а теперь следующий шаг вы видите что в
этой формуле нету r-1 и детерминанта нету это означает что мы можем себе
позволить такое обобщение и учитывая наличие взаимооднозначного соответствия
между множеством характеристических функций и функции распределения мы можем
сказать так давайте нормальным вектором назовем любой который имеет вот такую
характеристическую функцию если r это положительно определенное ну тогда это
как бы наш обычно не вырожденный вектор а если не положительно определенное то это
другой вектор но он тоже нормальный по нашему определение мы называем его
нормально только вот такое определение называется обобщенный нормальный вектор
который включает себя как вырожденный так и не вырожденное распределение
ну поскольку эта матрица к вариации то она конечно всегда больше или равна нулю но есть
случаи когда она равна нулю это соответствует тому если у вас ну понятно между между строками
матрицы коллекционной матрицы есть там линейная связь да вот но тем не менее мы
даем такое определение сюда укладываются и наши ну так сказать обычные не вырожденные
нормальные распределения еще целый класс вырожденных нормальных распределений все это
обобщенный нормальный вектор но теперь следующий шаг давайте введем линейное
прообразование по правилу му равно f это только вот теперь f произвольная матрица ну например
какая-нибудь вот такая к л ну у меня здесь л меньше кану это не обязательно или может
и больше как главное что они могут быть неравны это может быть не квадратная матрица произвольная
матрица вот ну и давайте найдем характеристическую функцию это но в нового вектора мю
я здесь не буду писать тэ напишу другую букву например с почему ну потому что уже другая
размерность видите у нас у той характеристической функции было к размерность там ну аргумента а
здесь она после вот такого при на перемножение станет л поэтому я другую букву использую ну
и по определению это математическое ожидание е в степени и с транспонированная мю правильно
да ну дальше мне каса логика понятно вместо мю пишем f это и пишем что это математическое
ожидание е в степени и f транспонированная с транспонированная это не трудно проверить
что по размерности подходит то есть вот это f транспонированная с будет иметь размерность
к как у нас исходный вектор имеет ну и получается что это на самом деле вот здесь напишу вот это
получается равна f мю только взятая в точке f транспонированная с ну и давайте подставим так
вот вот это позволить и стереть
вот она на f
так f мю точки с равно подставляем е в степени и вместо т транспонированного подставляем
с транспонированная f это мю это извините это минус это все так сказать в показателе
экспоненты и с транспонированная f и это f транспонированная с пополам ну и все те же
правила что такое f на мат ожидания это это мат ожидания мю поэтому это есть е в степени и
с транспонированная на мат ожидания мю минус f rn f с транспонированная на
r мю уж какая получилась выразенная не выразена пополам то есть любое преобразование обобщенного
нормального вектора остается в классе этих обобщенных нормальных векторов в зависимости
от того как вы строили эти все преобразования вот это r мю может быть выраженной не выраженной
матрицей ну например может иметь размерность 1 если вы все компоненты нормального случайного
вектора не выраженного складываете у вас получается размерности 1 это р и будет не равно 0 то есть
получится несмотря на то что образование преобразование сильно выраженная итоговый
вектор получится ну не выраженной стандартной нормальной но тем не менее все эти преобразования
укладывается в класс обобщенного нормального распределения который задается своей
характеристической матрицей вот ну и помимо того что это такой красивый как мне кажется пример
использования и и ну идеи или технологии характеристических функций это еще ну важно
с практической точки зрения потому что нормальное распределение это но один из основных классов с
которыми работают и теория вероятности и случайный процесс и математическая статистика так
ну раз оно записывается матрица умножить на вектор это линейная вам как это так
определение преобразования не но если определить преобразование сохраняется размерность тогда это
не преобразование но я просто честно говоря как-то у меня так в голове не отложилось что слово
преобразование только сохранением размерности но если так пусть пусть будет так так все тогда
с этим заканчиваем и переходим ну по большому счету последней теме нашего курса
который из-за нехватки времени будет немножко сжатый к сожалению но по крайней мере я надеюсь что
все так сказать идеи мы сможем с вами усвоить и изложить значит следующий объект который мы
будем изучать это случайные последовательности но в качестве вводной напомню значит у нас было
отображение из-за мега в 1 мы это называли случайной величиной было отображение из-за мега в рк мы это
называли случайный вектор размера стека а если омега отображается в эр бесконечности то мы
называем случайной последовательностью то есть каждый омега ставится в соответствие счетное
число функции вот таких случайных величин точнее говоря не просто функции случайных величин вот
если есть такое отображение то мы имеем дело уже со случайными последовательности что про
случайные последовательности для начала нам нужно знать ну давайте рассмотрим множество омега таких
что инфинум кси к от омега меньше некого x но это вроде бы объединение множество типа омега такое
что кси к от омега меньше x правильно да если инфинум меньше значит какой-то найдется при этом
омега который меньше и если омега одному из этих принадлежит то значит и не смешь вот так вот что
отсюда следует отсюда следует что инфинум кси катах это случайная величина измеримая функция
совершенно аналогично супрэну там сильно аналогично супремум ксикаты это тоже измеримая функция то
есть случайно и влечена отсюда следует что например инфинум по n супрэмум пока больше равно
n ксикатова тоже измеримая величина это что такое это верхний предел ксикаты и нижний предел
аналогично измеримая функция вот как бы с этими объектами вот в этом смысле можем работать мы не
вводим здесь некий аналог функции распределения потому что это была бы функция со счетным
числом измерений и там возникают серьезные теоретические проблемы разрешенные в свое время
колмогоровым и которые лежат в основе определения случайного процесса этот объект который вы будете
дальше изучать нам пока вот нужно знать что вот нам понадобится инфы мы супремум и верхние нижние
пределы вот ну раз есть последовательности то следующее что возникает желание так сказать
какую аналогию провести математическом анализе чем из последовательности делаем
главным образом пределы ищем да ищем пределы но на числовой оси там как бы все просто да у
нас определение предела простое но в основе этого определения лежит то что в off
пределы чтоб то ни было да функции или число начиная с некоторые номера лежит ну все вся
под последовательность начинает с этого номера хвост последовательности лежит но для этого надо
определить расстояние что значит в окрестности в анализе это более менее ну не то что просто
что просто abyssal однозначно, особенно никак не определишь, а вот вероятности здесь есть разные
взгляды на эту текстатематику. И мы, вообще говоря, определяем четыре типа сходимости на
случайных последовательностях. Вот я их здесь запишу. Первое, это сходимость по распределению.
Кси n-ное сходится кси по распределению, обозначается это буквой D, distribution. Ну n
стремится к бесконечности. Если f кси n-ное от x сходится к f кси от x слабо, то есть поточечно,
слабо. Но, еще есть одно, еще одна оговорочка. Во всех точках непрерывности f кси предельной
функции. То есть, если предельная функция у нас разрывная, это значит, что сходится, ну там,
там к части, к дискретной, например, случайной величине, то в точках разрыва не обязательно
поточенная сходимость. Но во всех точках непрерывности она должна быть. Если есть,
то мы говорим, что кси n-ое сходится к си по распределению. И, кстати, когда я тут писал,
вот уже стер, там, например, что х квадрат с n степенями свободы равно сумме х квадрат
с одной степенями свободы, там, конечно, над знаком равно надо было писать букву D. То есть,
равны в смысле распределения. Вот такой тип сходимости мы вводим первым. Вторым мы вводим
такой тип сходимости. А, ну и почему эта сходимость понятна, потому что, ну вот это мера близости
в пространстве не прерыве, в пространстве ограниченных, не убывающих функций. Значит,
для любого епсилон больше нуля, а точнее говоря, сначала, что вводим, да, какую. Значит, кси n сходится к
си по вероятности, буквой P, probability, естественно. Если для любого епсилон больше нуля, вероятность
того, что кси n отклонится от кси по модулю больше, чем епсилон, стремится к нулю, при n стремяется к
бесконечности. Но этот тип сходимости вам, в принципе, знаком. Как он называется в общем случае?
Сходимость по мере. Ну, то есть, это обычная сходимость по мере, но поскольку у нас мера
всегда вероятностная, то мы называем сходимостью по вероятности. А так это сходимость по мере.
Третий тип сходимости, который мы вводим, кси n сходится к си порядка R и обозначается,
вот тут R в скобочках берется. Кси n сходится к си порядка R, если математическое ожидание
кси n-е минус кси по модулю в степени R стремится к нулю. Ну, можно, конечно, там в погоне за
R любые рассматривать, но мы будем рассматривать, иметь в виду, всегда R больше равно единице и
здесь есть два, так сказать, собственных имени. Если R равна единице, то такую сходимость называют
сходимостью как в среднем, а если R равно 2, то называют сходимостью в средне квадратичном. Вот,
значит, это третий тип сходимости. Ну и четвертый тип сходимости. Это сходимость почти
наверно или, так сказать, в классической математике функциональном анализе аналог сходимости почти
всюду. Но здесь она называется сходимостью Pn, почти наверно, или с вероятностью единица,
это термины, так сказать, полные синонимы, сходимость почти наверно или с вероятностью
единица. Есть ли вероятностная мера Омега таких, что кси n от Омега сходится кси от Омега, равна единица?
Понятно определение? Ну немножко трудно оно дается обычно, поэтому еще два слова буквально. Ну вот
что это такое? Вот вы берете некая Омега, как только вы зафиксировали, ваша последность
случайно превращается в числовую, и она сходится вот к этому кси от Омега. Перебирайте все такие
Омега, на которых выполнено то условие. Если их вероятность на меру единица, значит имеет
место сходимость с вероятностью единицы. А если на каком-то значимом множестве Омега это не
выполнено, значит нет сходимости с вероятностью единицы. Вот, значит, почему почти наверно,
то есть для чего дополнительный термин не почти всюду, да? Почти всюду все-таки речь идет о
конкретном, конкретной мере множества Лебеговой на отрезке, ну там на напрямой. А здесь может быть,
в принципе, любая. Здесь как-нибудь там какая-нибудь одна точка, которая содержит в себе там значительную
часть меры. С точки зрения сходимости, почти всюду это почти всюду, а с точки зрения теории вероятности,
это не сходимость почти, наверное. Вот, ну так небольшие, так сказать, в терминах отличия. Вот,
значит, эти типы сходимости неравнозначны, но заметно связаны. И я сейчас напишу, как они связаны.
Значит, самый слабый тип сходимости, давайте я это напишу вот на этой доске, мы будем сюда
обращаться. Хотя, так сказать, самый слабый тип сходимости, это сходимость по распределению,
которую мы назвали 1, он следует из сходимости 2, то есть по вероятности. Сходимость 2 следует
из сходимости 3, то есть порядка R. Сходимость 2 следует из сходимости 4, то есть почти, наверное,
почти, наверное. И, собственно, все. Но есть еще ряд связей, которые как-то имеют место при
некоторых условиях. Например, из сходимости по распределению следует сходимость почти,
наверное, если xi, то есть предельная функция, предельная случайно влечена, это константа,
то есть число вырожденное случайно влечено. Вот если предельная случайно влечена вырождена,
например, ноль, то сходимость по вероятности, это же действие на сходимости почти, наверное.
Сходимость почти, наверное, следует из сходимости по вероятности, если она,
так называемая, быстрая. То есть для любого епсилон больше нуля не только вот эта вот вероятность
стремится нулю, а этот ряд стремится к нулю. То есть этот ряд сходится, быстрая сходимость.
Вероятность xi-n-xi больше епсилон, сумма по n меньше бесконечности. То есть если сходимость
быстрая, то есть не просто вот эта вероятность стремится к нулю, она настолько быстро стремится
к нулю, что этот ряд сходится, то тогда этого достаточно для сходимости почти, наверное.
Аналогично и сходимости порядка r. Следует сходимость почти, наверное, тоже, если сходимость
быстрая, то есть сумма математических ожиданий xi-n-xi в степени r меньше бесконечности.
И наконец, со сходимости 2 следует сходимость порядка r, если вероятность того, что xi-n меньше
или равно m равна единице для любого n. То есть с вероятностью единица все xi-n-ы ограничены. Вот
если они все ограничены, то тогда сходимости по вероятности следует сходимость порядка r.
Ну и еще имеет место, как бы к этому относится, теория Мариса, знакомая вам, которая гласит,
вот здесь я, так сказать, напишу, что если xi-n-ая сходится к xi по вероятности, то бишь по мере,
то отсюда следует, что существует подпоследовательность n-катая, такая,
что xi и n-катая сходятся к xi в наших терминах почти, наверное, почти всюду с вероятностью
Так, ну вот, значит, нам предстоит поработать с этой таблицей, так сказать, убедиться. Ну и давайте
начнем, как говорится, дорогу осилит идущий. Ну давайте сначала из 2.1, то есть сходимости по
вероятности следует сходимость по распределению. Ну здесь по-разному можно доказывать, но раз уж мы
характеристические функции с вами изучили, давайте, значит, этим воспользуемся. Для того,
чтобы доказать сходимость по распределению, извините, чтобы не забыть, существует эквивалентное
определение сходимости по распределению, эквивалентное, которое выглядит так, для любой
непрерывной, ограниченной функции там phi от x, математическое ожидание phi от xi n-е
сходится к математическому ожиданию phi от xi. Если выполна такое свойство, то это часто берется
за определение сходимости по распределению. Ну, понятное дело, что если речь о том же сходимости,
то они эквивалентны, но вот это вот для понимания предметом мы доказывать это не будем, но имейте
в виду. Итак, из 2.1 сходимость по распределению эквивалентна сходимости характеристических
функций. Давайте для каждого t по точной сходимости. Давайте вот это phi xi n-е от t
минус phi от t по модулю. Пишу сразу меньше или равно интеграла e в степени i t xi n от
омега минус e в степени i t xi от омега по модулю p d омега. Равно. Разбиваю на два интеграла.
Xi n от омега минус xi от омега меньше или равно epsilon плюс интеграл xi n от омега минус
xi от омега больше epsilon. Ну и давайте с каждым из них отдельно разберемся. Если вот в этой области
мы смотрим. Вот эта функция, разность, непрерывная функция. Поэтому для любого
epsilon, если epsilon вот это мало, то и вот эта функция, модуль тоже достаточно мало. Обозначим
вот этот интеграл превратится в некое дельта от epsilon на p d омега. Вот взятый вот по этой области.
Ну понятно, что это меньше или равно дельта от epsilon, которая стремится к нулю, когда epsilon
стремится к нулю. Понятно, да, с этим? Вторая область. Ну вот эта разность заведомо меньше
двух. Поэтому вот этот вот интеграл меньше или равен два интеграла p d омега xi n минус
xi по модулю больше и epsilon. А вот это что такое? Вероятность какого события? Это две вероятности
того, что xi n минус xi по модулю больше и epsilon, правильно? А вот эта штука стремится к нулю,
потому что у нас есть сходимость по мере, по вероятности. Ну всё, таким образом,
всё это, так сказать, может быть сделано, сколько угодно, малым. И это означает сходимость
характеристических функций и, как следствие, сходимость функций распределения. Опять же,
во всех точках непрерывности предела. Вот мы, когда характеристические функции с вами смотрели,
тоже на этот эффект обращали внимание. Так.
Так, ну следующее, что нам малой кровью дастся, это из 3 в 2.
3 в 2. Мы пишем вероятность того, что xi минус xi больше и epsilon равна вероятности того,
что xi n минус xi в степени r больше и epsilon в степени r, а это меньше или равно, чем математическое
ожидание xi n минус xi в степени r делить на epsilon в степени r неравенство Чебышева, Маркова.
Или это уже Чебышева мы называли. Вот. Эта штука по условию стремится к нулю, значит и это стремится к нулю.
Так. Следующее, что мы без особых проблем получим, это из 1 в 2, когда xi равно константе.
Из 1 я вот тут в 2 так отрывистый чертой, что это не всегда. Нам по большому счету надо доказать,
что при наличии сходимости по распределению вероятность того, что xi минус а, некое число,
по модулю больше и epsilon будет стремиться к нулю. Но мы докажем обратное, рассмотрим вероятность
события xi n минус а меньше или равно epsilon и докажем, что это к единице стремится.
Значит, это равно у нас вероятности того, что xi n меньше или равно a плюс epsilon и больше или
равно a минус epsilon. Это вероятность больше или равна вероятности xi n больше или равно
а минус epsilon, но зато меньше, строго меньше, а плюс epsilon. Область с узелем вероятности
меньше. А вот это уже f xi n в точке a плюс epsilon минус f от xi n в точке a минус epsilon.
Поскольку xi n сходится к a по вероятности по распределению, а это значит, что функция
распределения к xi n сходится вот к такой функции. Вот это a, вот функция распределения
константа. A плюс epsilon это предел единицы, а a минус epsilon это ноль, то бишь равно единице.
Но это означает, что вот эта вероятность стремится к единице, точнее говоря не равно,
а стремится к единице. Понятно, да? Так, ну еще за 5 минут мы, наверное, успеем вот это доказать,
что если ограниченные последовательности, то исходимости вероятности следует исходимость порядка
r. Так, пишем. Математическое ожидание к xi n
минус xi в степени r, что это такое по определению, это интеграл модуль к xi n от омега минус
xi от омега в степени r pd омега равно. Разбиваю такой наш излюбленный прием, к xi n от омега
минус к xi от омега меньше равно epsilon, плюс интеграл к xi n от омега минус к xi от омега больше
epsilon. И здесь я напишу еще раз. Так, с этим все просто интегралом. Под интегральное выражение
меньше epsilon, мера нормированная единицу, значит эта штука сколь угодно малая. Выбором epsilon.
Смотрим вот сюда. Я напишу следующее. Меньше или равно, так, извините, немножко вот так
коряло. Меньше или равно supremum по омега к xi n от омега минус к xi от омега в степени r на
меру вот такого множества. А это ничто иное, как вероятность того, что к xi n минус к xi будет
больше epsilon. Вот этой штуке supremum существует? Ну смотрите, все хn с вероятностью единицы меньше
или равно m. Понятно. А вот это-то, про предел, что можно сказать? Это предел по вероятности. А здесь
вот как раз удачно теоремы Рисова воспользоваться. Поскольку к xi n сходится к xi по вероятности,
существует подпоследовательность единичной меры такая, что к xi nkt от омега минус к xi от омега
ну там, точнее говоря, к xi nkt от омега стремится к xi от омега и вероятностная мера равна единице.
Вот для любого из этих омега, которые здесь вероятностная мера единица, это же числовая
последовательность все члены, которые меньше или равно m. Это предел. Поэтому на множестве единичной
меры для каждого из этих омега к xi от омега тоже меньше m. Поэтому вот эта вся штука просто меньше
2m справедливости ради в степени r. На вот эту вероятность, вот эта вероятность стремится к нулю,
поскольку мы исходим из того, что имеет место сходимость по вероятности. Вот эта штука ограничена.
Все, мы как бы доказали то, чего хотели. Значит, давайте я вот здесь то, что мы заказали,
зачеркну, чтобы в следующий раз можно было вернуться. А то, что осталось доказать,
нам для этого потребуется критерий сходимости почти на верное. И если у меня есть минута,
то я его запишу просто, чтобы мы с этого начали в следующий раз.
Значит, для того, чтобы последовательность xn сходилась к xi почти на верное, необходимо
и достаточно, чтобы вероятность вот такого события, supremum, xi kt – xi, supremum берется
пока больше или равно некого n. Больше epsilon стремился к нулю при n, стремящемся к бесконечности.
Может, он нам даже знаком, да? Поскольку тут существует разница в мере. Быстро пробежимся,
там не такое сложное доказательство. Все, значит, в следующий раз начинаем с этого,
и тогда, значит, закончим вот эту табличку. Спасибо.

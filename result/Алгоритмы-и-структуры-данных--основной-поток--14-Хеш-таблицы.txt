У нас две лекции, мы поговорим про хэш таблицы и всякие
такие приколы с ними.
Вот, ну мы продолжаем жить в парадигме, что нам нужно
придумать какую-то структуру данных, которая умеет означать
на те же самые три запроса, insert, erase и find, но теперь
в каком-то смысле мы отказываемся от требования на упорядоченность,
потому что когда мы хранили дерево поиска, у нас в каком-то
смысле легко было восстановить порядок тех элементов, которые
в структуре лежат.
Если мы храним дерево поиска, то мы знаем, где лежит минимум,
это самая левая вершинка, если идти влево-влево, будет
минимум.
Потом там второе от него, это вторая порядка статистики
и так далее и так далее.
То есть на самом деле в дереве поиска всегда хранится
порядок элементов, можно его оттуда легко извлечь
линейным проходом, то есть за размер структуры можно
вывести по факту отсортированную версию текущего множества.
В hash таблицах у нас такое свойство пропадает, но
зато появляется огромное преимущество, что все запросы
работают, ну грубо говоря, за единицу.
Мы хотим делать все те же инсерты, рейсы и файнды.
Теперь все это будет амортизированная единица на запрос, и это будет
в среднем.
В среднем, то есть математическое ожидание времени ответа
на запрос будет единичкой в том же смысле, как когда
у нас это было про квиксорт, мы когда обсуждали, там,
где n log n в среднем, что мы там что-то случайный пивот
взяли, разделили и пошли в обе части рекурсивно,
вот здесь то же самое, мат ожидания будет единичная.
То есть в среднем все работает хорошо, но иногда в каких-то
очень выраженных случаях что-нибудь вырождается
и может работать долго, и тогда в идеале надо что-то
перестраивать.
Ну вот, представьте, что у нас явным образом описано
множество из которого черпаются ключи.
У это какое-то универсум, и все ключи, все элементы
которые надо в это множество добавлять, которые мы храним,
мы же множество, а С храним, все ключи, которые в С играют,
они все черпаются из у.
Ну мы там знаем какую-то структуру этого у, грубо говоря.
Хочется сделать следующее, хочется в каком-то смысле
все элементы этого у типа пронумеровать желательно
маленькими целыми числами и просто завести массив
такого размера, какой размер вот этого перенумерованного
множества.
Ну представьте, пусть у меня есть большой универсум
у, но нам откуда-то заранее известно, скажем, что интересующий
у меня числа, те, которые будут поступать в запросах,
вот эти вот х и у, они будут только из какого-то маленького
подможества.
Ну понятно, если у вас чисел много, а запросов мало,
то так или иначе интересующие вас числа это какое-то маленькое
подможество.
Ну если бы они все вам были известны заранее, если
за просы были известны заранее, то наверное понятно,
как поступить.
Можно просто все прочитать, запомнить все элементы,
которые были, как их отсортировать и пронумеровать.
Ну то есть тогда у меня о каждого элемента будет
номер.
будет там от 1 до n, где n количество запросов.
Ну и тогда можно просто массив длины n ввести,
и про каждый элемент, по его номеру, понимать, есть
он там или нет.
Понятно?
Ну, это как бы тут много проблем, во-первых, как минимум
то, что мне нужно, чтобы все запросы были известны
заранее.
Но идея такая, что каждому элементу мы в каком-то смысле
мы сопоставляем его номер, номер XA, она у нас будет
играть.
Вот.
А именно мы будем вот этот номер вычислять с помощью
какой-то функции h, х-функции.
Вот, сегодня будет немножко вероятности.
Представьте себе, что функция h бралась бы полностью случайно.
То есть, ну представьте, вот у меня h, это какая-нибудь
функция из u, скажем, в целые числа от 0 до и минус 1.
Пусть она случайна, вот прям полностью случайная
функция.
Что значит случайная функция?
Это значит, что на каждом элементе u у меня значение
h в этой точке случайно и равномерно распределено
по этому множеству.
То есть вероятность попасть в каждое конкретное число
одна и та же, один делить на m.
Ну а значит, если h случайна, давайте напишу полностью
случайно, прям честно, абсолютно честно, случайно, полностью
случайно, то вероятность по, собственно, случайной
h, того, что h от x равно какому-нибудь i, она была бы в точности
1mt.
Значит, давайте напишем, что для любого х из u, для
любого i, ну вот целого числа из этого диапазона.
0d минус 1.
Ну, обычная такая дискретная вероятность, у меня есть
там пространство всех функций, я вот из них выбираю одну
случайно равную вероятность, среди всех возможных функций
выбираю одну наугад.
Тогда понятно, что вероятность этой случайной функции
в конкретной точке быть конкретному числу равной,
это в точности 1mt, потому что в этой точке она какое-то
значение принимает, и среди всех возможных m она принимает
ровно 1 и равновероятно, все вероятности по всем
i должны быть равны, но раз у меня всего m образов, значит
вероятность 1mt.
Понятно, никого не шокирует.
Хорошо.
Ну вот представьте у нас такой идеальный мир, допустим,
мы вот эту процедуру извлечения номера по x реализуем с
помощью х-функции h, полностью случайной функции h.
Тогда было бы довольно неплохо, потому что мы
опять-таки реализовали вот эту вот процедуру перенумирования
всех элементов универсума, так чтобы они все поместились
в какое-то небольшое множество размера m, ну и тогда мы могли
бы делать просто следующее, когда мне приходит какой-то
элемент x, я сначала вычисляю его х-функцию, то есть его
номер в этом массиве, и дальше, например, кладу его в ту
ячейку, которая посчиталась.
У нас же всегда есть функция для конкретного элемента,
конкретная, одно и то же извлечение.
Да.
Я считаю, что у меня там один раз когда-то h вначале
сфиксировалось, я ее случайно сгенерировал, но теперь функция
она в каждой конкретной точке, сколько раз я ее не считаю
в одной точке, она считает одно и то же.
То есть это детерминированная функция, которая в начале
как-то случайно сгенерировалась.
Ну вот представьте, пусть у меня там пришел какой-то
x, я завел массив длины m, пришел x, я посчитал его
значение h от x, посмотрел на ячейку с этим номером
и сюда его положил.
Ну хорошо, понятно, вот он здесь лежит, все.
Дальше пришел какой-то y, мне нужно опять его положить
в мою структуру.
Я беру, считаю, скажем, h от y, это какой-то новый элемент,
я кладу y вот сюда, в эту ячейку массива, по номеру
собственно h значения и так далее.
Вот, ну какая бывает проблема?
Ну проблема бывает в том, что иногда происходят коллизии,
то есть когда h значение одинаковое, потому что когда я беру
случайную функцию и считаю ее значение, скажем, в разных
каких-то точках a и b, то у меня вполне возможно
h от a равно h от b.
Ну вот эта ситуация называется коллизией.
Ну в случае, когда a и b различны, значит, называется коллизия.
И тогда, если, скажем, у меня приходит запрос insert
a, insert b, то что делать, сходу непонятно, потому что они
должны лежать в какой-то одной и той же ячейке, вот
этой вот, скажем, такой, что h от a равно h от b, вот в ячейке
с таким номером они должны оба как бы лежать.
Вот, ну есть ли у вас предложение, что можно в таком случае
сделать?
Вот если они обе должны лежать здесь, да, ровно так
мы и сделаем.
Просто возьмем и вместо того, чтобы хранить одно
число в этом элементе, будем здесь хранить просто
список тех чисел, которые ровно этим хэшом обладают.
То есть вместо того, чтобы у меня было здесь одно число,
я просто здесь заведу односвязанную, кажется, достаточно односвязанную
списку, положу сюда a, положу сюда b, ну и так далее.
Каждое новое число, которое будет поступать, я буду
просто подвешивать в этот список.
Вот, ну примерно так работает хэштаблица.
Если h от x полностью случайно и верно лишь, то у меня всегда
для конкретного значения будет одно и то же возвращаться,
это ведь важно?
Да.
Да, будет.
И да, верно.
Ну то есть представляете себе это все следующим
образом.
Мы сначала до выполнения всех запросов сгенерировали
h.
В каком-то смысле мы обратились к каракулу, сказали, дай
мне случайную функцию.
Вот он дал.
Вот какая-то функция, написан код какой-то функции h.
Дальше, если вы в нее одно и то же подставляете, дальше
функция уже детерминирована.
Если вы в нее одно и то же много раз подаете, она возвращается
одно и то же.
В этом смысле, если вы многократно у вас х приходит,
insert, а потом find к х, то вы будете обращаться к одной
и той же чеке массива, потому что h уже детерминирована.
Если вы ее заранее сгенерировали, то теперь уже h от x одно и
то же будет.
И тогда проблем не будет.
Ну и, смотрите, давайте, например, посмотрим вероятность
коллизии.
Пусть мы фиксируем какие-то a и b, давайте зафиксируем
какие-то два элемента a и b из u, давайте, например,
посчитаем вероятность коллизии на этой паре элементов.
Да, будет ровно 1 мт, абсолютно верно.
Потому что, смотрите, у нас функция h случайна, и в каждой
точке она принимает свое значение независимо от
остальных.
То есть значение h от a и h от b – это независимые
реализации одной и той же случайной величины с равномерным
распределением вот в этом множестве.
То есть все значения принимаются равновероятно.
Ну, можно это так написать, если хочется.
Давайте просто переберем значение h от a.
Ну да, можно так записать.
Значение по i, вероятность того, что h от a равно i, равно
h от b.
То есть я вот это вот событие равенства вероятностей
разбиваю на дезинктное объединение.
То есть у меня понятно, если h от a равно h от b, то оно равно
какому-то конкретному i, давайте я по всем возможным
и это просуммирую.
Ну а такое событие, если у меня h от a и h от b – независимые
величины, я говорю, что в каждой конкретной точке
у меня значение h определяется независимо от остальных.
Тогда с какой вероятностью у меня и здесь выпала i, и
здесь выпала i?
Ну, значит, вот это выпадает значением i с вероятностью
1mt, и это тоже с вероятностью 1mt.
Поэтому эта сумма расписана следующим образом.
Это просто 1m2, потому что чтобы это выполнилось мне
нужно, чтобы и это было i, что происходит с вероятностью
1mt, и это было i, что происходит с вероятностью 1mt, потому
что все независимо и равновероятно.
Ну а это как раз 1mt в точности.
Ну я формально расписал.
То, что вы говорите, на самом деле я ровно это и записал,
просто как бы формулками скажем так.
Вообще правда, конечно, да.
Вот, значит, если бы мы брали случайную h, то в принципе
у меня коллизии довольно невероятны.
Если m достаточно большое, ну там не знаю, 10 в 5, грубо
говоря, то это как бы не очень большая вероятность.
Даже если она выпала на каких-то конкретных элементах,
то, наверное, длина вот этого списка будет не очень
большая.
Наверное, не очень много элементов будут попадать
в один и тот же список много раз.
Вот, ну хорошо.
Вопрос.
Можем ли мы вот прям вот это закодить и этим пользоваться
в реальной программе?
Смотря на сколько устойчивого вы хотите программовать.
Вот прям ровно это, вот то, что я описал, ровно это
я и хочу записать.
Просто можно ли это физически закодить?
Абсолютно разобавную функцию нельзя закодить.
Да.
Значит, чтобы сгенерировать случайную h, то есть я здесь
живу в предположении, что я как-то умудрился сгенерировать
вот такую функцию, которая на каждой точке принимает
независимый индекс, независимое значение.
Ну, это можно, конечно, ее можно просэмплировать,
ее можно сгенерировать на каждом элементе универсума,
как там одно из вот этих чисел, и все это сохранить.
То есть более-менее единственное, что здесь можно сделать,
это изначально.
То есть как вот эту h сгенерировать?
Мы проходимся по всем элементам универсума, на каждом элементе
подбрасываем нужную монетку и генерируем случайное
число из этого диапазона.
Тем самым мы для каждого х запоминаем значение h от
х.
Но это не очень хорошо, потому что у меня память
уже будет вот такая.
Если у меня такая память, то зачем мне вообще это сжимать?
Я мог бы просто хранить массив вот такого вот размера
и просто для каждого элемента универсума помечать, был
он или нет.
Поэтому как бы вот столько памяти нам точно не позволительно
хранить.
И более-менее никак по-другому случайную функцию не реализовать.
Настоящую случайную функцию, и именно вам нужно, чтобы,
если и много раз одну и ту же скороблю, чтобы она
возвращала одинаковое значение, более-менее никак по-другому
это не сделать.
Ну поэтому вот этого идеального мира мы переходим к более-менее
реалистичному миру, которым на самом деле всегда будем
пользоваться.
Ну давайте скажем определение.
Пусть, давайте напишу h красивое, это какое-то семейство
хэш-функций, занумерованных, ну пусть будет элементами
s из u в… Так, ну если я вот это вот обозначу за ZMT, никто
не испугается же.
Ну, у меня есть какое-то семейство функций, каждая
функция, каждая h-эстая бьет из u в целые числа от 0
до ими минус 1, и все они индексированы какими-то
s-ками.
Ну вот пока не буду говорить штуку s, это просто какая-то
характеризация, характеристика функций.
Вот, значит, пусть эта штука семейство функций, семейство
функций, значит, тогда h называется универсальным
семейством хэш-функций, если… Ну, давайте я напишу
обычное определение, значит, вероятность по… для любых
различных элементов, опять для любых различных x и y
из u, вероятность по выбору случайного s, вероятности
коллизии в этих двух точках, не больше 1 mt.
Вот, ну поскольку у меня теперь уже как бы h это не
полностью случайная функция, а функция, характеризующаяся
каким-то индексом s, то у меня теперь будет вероятность
по s.
Ну, я иногда буду так писать, иногда буду опять писать
просто ph-то.
То есть здесь вероятность по s, которыми проиндексированы
все вот эти вот функции, хэш-функции.
Ну, я требую, собственно, то же самое, что мне нужно
было вот в этом равномерном… вот в этом идеальном случае,
что на любых двух элементах вероятность коллизии маленькая.
Вот ровные-то условия, ну даже усиленные, что здесь
меньше либо равно.
Вот, хорошо.
Ну, это называется универсальное семейство.
Нам на самом деле везде для приложений будет достаточно
слабо универсальности, это когда здесь стоит не единица,
а какая-то константа.
Ну, понятно, от того, что у меня как бы здесь на константу
испортится, у меня, грубо говоря, все списки вырастут
в константу раз и ничего не сломается с точки зрения
симпотики.
Поэтому давайте я здесь сразу допишу слабо универсальное,
и здесь допишу какая-то c.
Если существует там какая-то константа c, такая, что вот
это вот верно.
Ну там, не знаю, 10, 15, что-то такое, не очень большая константа.
Вот.
Ну, хорошо.
Значит, тогда давайте считать, что мы будем жить в этом
мире.
То есть по факту самое главное, что я хотел от вот этого
идеального мира, это вот это свойство, что коллизии
маловероятны.
Ну, давайте это потребуем.
Это условие более слабое, чем условие на настоящие
равномерные функции, случайные из всего множества функций.
Да?
Вот.
Мне достаточно какого-то семейства.
Очень хорошо.
Значит, тогда давайте построим, собственно, х-таблицу с
цепочками.
Х-таблица с цепочками.
Значит, она реализуется следующим образом.
Ну, во-первых, мы, так, да, ну там изначально мы говорим,
что м какой-нибудь, не знаю, 8 или 16, ну какое-нибудь маленькое
число.
И генерируем случайное h-s-t.
Генерируем случайное, давайте я просто буду писать h, понимая,
что вот они из того семейства, из универсального семейства.
Вот, значит, будем хранить переменную n, пусть n, это всегда
количество элементов в таблице.
Количество элементов в таблице, ну, изначально ноль.
Вот, ну и, кажется, последнее обозначение альфа, это
отношение n делить на m, это так называемый load-фактор,
ну, соответственно, фактор загруженности, да, показатель
загруженности, степень загруженности, степень загруженности.
То есть, сколько у меня элементов множестве, деленное
на размер х-таблицы, на то, сколько ячеек у меня в массиве.
Вот, ну и мы хотим, чтобы эта штука была всегда не
очень большая, ну, потому что понятно, если элементов
сильно больше, чем размер х-таблицы, то там будут
длинные коллизии, там просто по принципу Дерехлей обязательно
будут, скажем, два числа, лежащие в одном списке, ну
и там, чем больше это отношение, тем больше чисел будет в
каждом списке.
Вот я хочу, чтобы альфа была всегда небольшая.
Вот, ну тогда, значит, давайте реализуем всякие наши
штуки.
Значит, во-первых, как делать find, find x.
А, ну и, так, пардон, я это не проговорил, но давайте
запишем, что у меня хранится массив односвязанных списков
в количестве m штук.
Значит, храним, давайте назову l, массив односвязанных
списков.
Ау?
А что значит односвязанных?
Ну изначально мне же нужно какого-то размера создать
х-таблицу.
Ну, давайте делать такую, это как в векторе, типа,
когда вы заводите пустой вектор, там же какая-то
память выделяется.
Ну там вот, там сколько, пусть будет 16, это неважно.
Значит, массив односвязанных списков.
Вот, и теперь как обрабатываться операцией.
Ну, во-первых, find, понятно, надо посчитать х же значение,
h от x, и проверить, есть ли x в том самом списке, в котором
он должен быть.
Значит, пусть и, это h от x, проверяем, есть ли x в l-item.
Вроде просто, если у него должен быть такой х, то он
должен быть в этом списке, в l-item списке, просто проходимся
за линию, по этому списку проверяем, он есть там или
нет.
Если есть, то он значит есть в структуре, если нет,
то его больше нигде нет, можно сказать, что его нет.
Insert, значит, то же самое, считаем х значение, ну и мне нужно
этот элемент добавить в l-item список, в l-item.
Вот, ну тут опять подробности, что делать в случае, когда
x и так уже есть в структуре, ну давайте я буду считать,
что таких запросов нет, что запросы хорошие, как бы
если x было, то его добавлять не просят, ну так или иначе
мы можем сначала запустить find и проверить, если он
там был, то добавлять не надо, значит, поэтому я считаю,
что x не было в нашем нольстве s, вот, ну тогда нужно просто
x добавить в список l-item, но я сделаю следующее, я добавлю
его не в конец, а в начало, добавить, добавим x в начало
l-item, вот, почему в начало, с точки зрения теории это
не особенно важно, потому что у меня все равно все списки
на самом деле будут в среднем маленького размера, ну там
константного размера на самом деле, и поэтому куда
я добавляю этот x в список константного размера, более-менее
не важно, можно хоть в конец, хоть в середину, куда угодно,
но с точки зрения, во-первых, константы, потому что, ну
если вы добавляете в начало, вам не нужно весь список
проходить, это выгоднее, во-вторых, ну если мы, скажем, вернемся
к чему-то похожему на сплей дерево, то у меня получится,
что недавно пришедшие элементы как бы ближе всего находятся
к началу списка, и если я недавно добавил x и потом
спросил, есть ли он в множестве или нет, то он будет близко
к началу списка, и я сэкономлю проход по всему списку, просто
посмотрю на первый элемент, вот, это выгодно, но с точки
зрения такой неосимпатической оптимизации.
Так, значит, третий, ну и race, ну тоже понятно, надо просто
в нужном списке его оттуда удалить, ну опять, пусть
и это h от x, просто берем и удаляем x из элитого.
Ну, понятно, вот, значит, ну, к сожалению, на этом закончить
нельзя, потому что мне нужно поддерживать не очень большое
значение load-фактора, потому что, как мы уже сказали, если
альфа становится сильно большим, если у меня в таблице
лежит слишком много элементов, чем, собственно, ее размерность,
тогда у него будет много коллизий и будут там списки
очень длинными, и будет все долго работать, ну, поэтому
ведем следующую операцию rehash, значит, мы ее вызываем
каждый раз, когда альфа превышает какой-нибудь
небольшой порог, ну, например, скажем, альфа стала
больше, чем 0,95, ну, что-то такое, тут, опять же, конкретная
константа не важна, но на практике вот что-то такое
выбирает, 0,95, 0,75, где-то вот в таких пределах она
выбирается, вот, и когда у меня альфа перешла за
этот порог, я просто делаю полный rehash, я полностью
перестраиваю мою хэштаблицу, предварительно увеличив
m в два раза, значит, я m увеличиваю в два раза и полностью
перестраиваем всю хэштаблицу, в частности, выбираем новое
hash, с новым hash, вот, тогда, значит, если я m увеличил
в двое, тогда у меня альфа автоматически уменьшилась
в двое, и я могу еще много операций делать, скажем,
если у меня там пришло восемь элементов, то у меня
стало альфа одна вторая, я в два раза увеличил m, альфа
стала 0,25, я теперь могу еще восемь элементов добавить
без перестраиваний, вот, кажется, все, значит, ну, понятно,
поскольку у меня каждый раз m увеличивается в двое,
то количество этих rehash и перестраиваний будет маленьким,
потому что, еще раз, каждый раз таблица расширяется
в два раза, даже несмотря на то, что у меня, то есть,
формально, вот этот шаг работает за отn, потому что мне нужно
все старые ключи из старой таблицы переложить в новую,
это работает, ну, как минимум, за отn, мне нужно все ключи
перебрать, переложить в новую таблицу, но поскольку
это происходит экспоненциально редко, то есть, грубо говоря,
сначала это было на восьмой операции, потом на шестнадцатый,
на тридцать второй, шестнадцать четвертый и так далее,
то, ну, там, как обычно, так же, как это в векторе было,
когда мы перекладываем, когда мы увеличиваем размер
вектора в двое и все перекладываем за линию, у меня это все суммируется
и будет амортизирована единица.
Вот теорема, да, ну, давайте без доказательства, потому
что там нужен какой-то тервер, значит, если h реально черпается
из универсального семейства, если h из универсального
семейства х-функции, то каждая операция, каждый запрос
обрабатывается за учетную единицу в среднем.
Ну, зависит от подробностей, если вам не гарантируется,
что x в таблице нет, тогда надо, то есть вам надо
весь список пройти, убедиться, что его там нет.
Если гарантируется, или если, скажем, вам нужны
дубликаты, то есть там вы храните мультим множество,
и если там x два раза инцертится, то вы его два раза инцертите,
тогда не надо, зависит от реализации.
Ну, на это все это не влияет.
Вот, значит здесь именно амортизированная единичка
в среднем, в среднем, потому что у меня есть элемент
вероятности, да, я вот здесь, на каждом шаге, сначала
выбираю случайную h, поэтому у меня все в среднем.
У меня все, ну, как бы, в среднем�� от ожидания
взята, то есть, мотожидание времени, ответа на запрос
эта единичка.
И не только в среднем, то есть, не только это
мотожидание, но еще и амортизированная единичка, потому, что
вот есть такие долгие записы, то есть, скажем, если
пришло 8 инсертов, это много инсертов, то мне нужно после этого сделать rehash.
rehash – тяжелая операция, но редкая. Ну и можно доказать, что амортизирована это будет
единичка. То есть, несмотря на то, что, как обычно, конкретная операция работает
долго, но учетно, если я все просуммирую, то можно считать, что как будто
каждый работает за единичку.
Так, ну здесь я это не прописал. Давайте я напишу чуть подробнее. Давайте напишу
здесь 1 плюс альфа. Вот так будет. 1 плюс альфа. 1 плюс альфа. Это как бы в частности
означает, что вам нужно следить за тем, что альфа не переполняется, что альфа всегда маленькая.
Ну я вот, я именно ее сюда добавил, чтобы
типа, грубо говоря, если бы мы вот здесь вот поставили альфа какую-нибудь другую,
2, 3, 5, тогда здесь настолько бы увеличилась асимпточка, скажем так. То есть, можно
реализовать все то же самое с другим порогом на loadfactor, тогда у вас
отечественным образом изменится асимпточка. Вот. Так, ну кажется все. Кажется все.
Вопросы может еще? Хорошо. Теперь давайте перейдем к другой задаче, но для этого
мне нужно будет следующий. Еще раз? Такая что? А, ну просто имперически что-то померили,
вот типа с ним хорошо. А, да, я забыл еще одну из вещей сделать. Собственно,
давайте построим универсальное семейство. Ну смотрите, еще раз, у меня есть M списков
односвязных. В них записана вся информация о том, какие ключи есть в таблице. Я просто их в тупую
прохожу, пересчитываю значение хэш-функции кладу в новую большую таблицу, просто за линию не думая.
Итак, утверждение. Пусть P простое и универсум каким-то волшебным образом вложен в ZP. То есть,
все ключи, все элементы, которые надо хранить, это какие-то целые числа от 0 до P-1.
Тогда, если я рассмотрю следующее семейство хэш-функций, параметризованных двумя числами A и B,
определенные следующим образом. Я сначала вычисляю AX плюс B по модулю P, беру остаток отделения,
потом от этого всего беру остаток отделения по модулю M. И здесь все A и B это произвольные
случайные числа из ZP. Такое семейство универсальное. Мне же нужно, я здесь написал,
пусть H из универсального семейства, мне надо построить. Вот такая штука является универсальной
семейством хэш-функций. Какое я хочу, такое и будет. У меня M по дороге 16, 32, 64. Ну и здесь тоже,
какое M надо, такое и будет. Для каждого M, имеется ввиду для каждого M, ну давайте подчеркнем,
что для любого M меньше ВП. Вот это верно. А в результате, откуда мы берем хэш-функцию,
если мы ее генерируем случайным образом, как вначале говорили, то она не факт, что она попадет?
Не совсем случайно, смотрите, тут написано, что мне ее достаточно брать не из всех функций
вообще в природе, а только из вот этого маленького универсального семейства. Теперь я его конкретно
демонстрирую. Вот это семейство универсально. Если я это докажу, тогда, чтобы сгенерировать
случайную хэш-функцию, достаточно случайно для наших целей, мне будет необходимо всего лишь
сгенерировать два случайных числа A и B. И дальше, если у меня A и B фиксированы, то чтобы посчитать
значение этой функции, вот этой конкретной функции в любой точке, я просто арифметически эти операции
делаю. То есть мне уже не нужна произвольная случайная функция, мне достаточно вот такой,
который я умею быстро, эффективно генерировать. Просто A и B сгенерирую и все получу.
Ну, М меняется. Не обязательно. Это правда, да. Это правда. Ну, типа, почему бы не сгенерить новое?
Вот. Доказательства. Ну, давайте посчитаем. Мне нужно всего лишь понять, с какой вероятностью
эти две штуки совпадают. То есть я фиксирую какие-то два элемента, пусть X, Y. Ну, давайте я буду считать,
что у меня универсум это просто ZP, не особо это важно. Значит, пусть X, Y произвольны элементы ZP,
которые не равны. Вот. Тогда мне нужно понять, с какой вероятностью по выбору случайных A и B,
то есть мне нужно понять, с какой вероятностью при случайных A и B, h от X равно h от Y. Мне нужно
такую вероятность оценить, чтобы она была не больше, чем c делит на m для какой-то константы c.
Ну, давайте думать, при каком условии вот это вот выражение при подстановке сюда X, Y будет
одинаковым. Значит, я тут по факту делаю следующее. Я сначала вычисляю AX плюс B, потом беру по
одному модулю, потом по другому. Вот вопрос. Могли ли они образовать коллизию при взятии по первому
модулю? Ну, то есть понятно, что если AX плюс B процент P равно AY плюс B процент P, то есть вот без
вот этого взятия по модулю, если бы они совпали в точках X и Y, тогда они бы и совпали по модулю m.
Ну, потому что два одинаковых числа по одинаковому модулю одно и то же. Давайте зададимся вопросом,
может ли быть такое, что AX плюс B процент P равно AY плюс B процент P? Бывает ли такое? Ну, не
бывает. Потому что если бы это было, то это означало бы, что у меня просто вот такие вот величины,
сравнимые по модулю P, B можно сократить. Здесь написано, что A на X минус Y сравнимо с нулем по
модулю P. Значит, не то что не бывает. Иногда бывает. Например, если нам не повезло и мы изгенерили A
равное нулю, то он точно совпадет. Вот. Можно, а можно и не палится. Ну, давайте я сделаю так. То есть
можно было бы потребовать, что то A, которое мы генерируем, оно обязательно не нулевое. Ну, понятно,
это не очень хорошая функция. Если вы взяли A равно нулю, тогда здесь просто написано 0, и все функции
вас равны B. Это не очень хорошая функция. Можно как бы такую заигнодировать. Ну, давайте даже не
делать этого. С точки зрения как бы реализации, проще просто сгенерить случайное число от нуля
до по минус одного. Оно и так почти, наверное, не будет нулем. И можно там, можно ничего лишнего не
фазить, скажем так. Ну, это, понятно, бывает только, бывает только если A равно нулю. Потому что X
не равно Y, значит их разность не делится на P, но чтобы произведение делилось на P, мне нужно,
чтобы хотя бы один из множеств делился на P, потому что P простое. Вот. Ну, с какой вероятностью это
происходит? 1P, это ясное дело. Вероятность по выбору, по случайному выбору A и B того,
что A равно нулю, это 1PT. Потому что я A генерирую случайно от нуля до по минус одного, B случайно
от нуля до по минус одного. Ну, вероятность того, что выпали конкретным нулем, это как раз 1PT.
Вот. Хорошо. Кроме вот этой маленькой поправочки, а мы понимаем, что если P больше чем M, то эта
штука не больше чем 1MT. Да, потому что M меньше P, все правильно. Вот. Значит, кроме этого случая,
у меня коллизии на первом шаге, вот на этом этапе, до взятия процента по модулю M, на этом этапе они
не совпадают. А кроме этого случая, кроме этого случая, у меня вот эти штуки не равны. Хорошо.
Значит, более того, я утверждаю, что после вот такого преобразования X и Y в AX плюс B и AY
плюс B, вероятность получить каждую конкретную пару, скажем, UV, если это обозначу за U, это обозначу
за V, которые различны, то я утверждаю, что вероятность получить конкретную пару UV,
это что-то типа 1 на P квадрат. Ну вот давайте, пусть U и V какие-то конкретные фиксированные числа,
пусть U не равно V, давайте посчитаем вероятность того, что, ну вот это вот произошло, что AX плюс B
это U, AY плюс B это V. Значит, можно записать вот это вот условие, можно записать как систему
уравнений и можно записать как систему уравнений в матричном виде. Я могу записать следующее,
вот то, что написано здесь, это следующее условие. Вот матрицы же умеете перемножать? Ну хорошо,
значит, ну вроде верно, да, X1 как раз AX плюс B должно быть равно U, AY плюс B должно быть равно V,
ну естественно все вычления по модулю P, ZP. Вот, и поскольку, смотрите, поскольку мы живем в ZP,
поскольку у меня X не равно Y, то у меня определитель этой матрицы будет X минус Y,
значит, не нулевой, да, значит, существует, если определить не нулевой, существует ровно
один столбец AB, для которого это верно. Согласны? Ну вот, значит, вероятность ровно одна P квадратная,
да, хорошо. Нечтяными словами мы получили следующее, что когда мы первый раз навешиваем первое
преобразование линейное, вот это вот, мы различные X и Y равновероятно перевели в различную пару UV по модулю P.
То есть, когда у меня было X и Y, я навесил на них случайную функцию вот эту и вот эту вот,
я равновероятно получаю любую пару U не равно V. Среди всех возможных пар U не равно V у меня все
равновероятны с вероятностью одна P квадратная. Вот, ну, значит, я могу просто переписать
мое условие. Я могу написать, что вероятность коллизии не больше чем, ну, смотрите, одна P-ты у меня
остается от случая A равно нулю, а дальше я могу переписать, что это просто вероятность по всем
различным парам UV у не равное V того, что U%M равно V%M. Вот, да, потому что еще раз, когда навешу первое
преобразование, у меня каждая конкретная пара X и Y равновероятно отображается в любую пару UV.
Ну, то есть, по факту у меня есть опять случайная равномерная распределенность всех вот таких парах.
Каждая такая пара реализуется с одинаковой вероятностью. Поэтому вместо того, чтобы брать вероятность по A и B,
я могу приспишать вероятность все равных U и V, потому что каждая пара UV однозначно соответствует паре AB.
Это, скорее всего, правда. Ну, то есть, скорее всего, сейчас, давайте подумаем. Мне кажется,
вот здесь должно быть, опять же, не P квадрат, а P на P-1. Ну, вот именно, да, что порядка все равно
будет по квадрату. То есть, вот здесь, скорее всего, написано P на P-1 должно быть, потому что как раз
я исключаю случай A равно нулю. Да, да, здесь будет как раз P на P-1. Вот, но если прям дотошничать,
то как раз P на P-1 и здесь вот как раз то, что вы сказали, оно не нужно. Но давайте я загруглю, скажу,
что это примерно 1P квадратное. Ну все, а теперь смотрите, вот картинка такая. Я хочу посчитать
такую вероятность. Что для этого происходит? Давайте я рассмотрю все ZP, занумеру их там.
Значит, я беру два случайных различных числа отсюда, хочу понять, с какой вероятностью они
равны по модулю M. Ну понятно, что такое равенство по модулю M. Вот если я нарисую где-нибудь U,
то какие числа с ним сравнимы по модулю M? Это U плюс M, U плюс 2M, ну и так далее. И здесь U минус M и
так далее. Все, что не выходит за границы, вот здесь вот, через ноль не проходит. Вот, ну сколько их?
Их примерно P делить на M. Потому что у меня есть точка, если я, скажем, U фиксирую, я могу направо
ходить на плюс M, налево ходить на минус M. По факту я рассматриваю все числа, сравнимые с U по
модулю M. Их примерно P делить на M. Значит, при фиксированном U подходящих V примерно P делить на M.
Но опять же, если точно расписывать, наверное не больше, чем верхняя целая часть P делить на M.
Я так оставлю. Вот, ну и поскольку у меня U случайно и V случайно, вот эта штука как раз будет
одна M. Потому что если U фиксировано, и V выбирается случайно среди всех P возможных реализаций,
то вероятность этой V попасть в подходящие – это вот это делить на P. Потому что подходящих P делить
на M, вероятность попасть в конкретное – это 1Pt. Значит, вероятность для данного V быть подходящим –
это вот это делить на P. То есть как раз 1Mt. Вот, значит, здесь эта вероятность просто равна 1Mt.
Ну и все получилось. Здесь 1Pt плюс 1Mt – это все не больше, чем 2 делить на M. Значит, семейство
универсальное, как и хотелось. Вот. Похоже? Вот здесь? Смотрите, для данного U подходящих V вот столько.
Теперь, если вы U фиксируете, а V генерируете случайно среди всех P возможных реализаций,
какой вероятностью вы попадете вот в одно из этих чисел? Вот это делить на P. Потому что столько
подходящих из всего P реализаций. Это делить на P – это как раз 1Mt. Вот. Значит, получается,
что если у нас, ну, например, задача такая, что мне нужно хранить множество чисел, скажем,
там, помещающихся в какой-нибудь int. Тогда нужно взять достаточно большое простое число P,
которое больше, чем все возможные потенциальные элементы, добавляемые в множество. Ну и вот
реализовать такую хэш-функцию, вот эту H, и с ней все будет работать хорошо. Все будет
работать за единичку в среднем амортизированно. Так. Ну вроде все. Теперь давайте перейдем к
совершенному хэшированию. Вот. И для этого мне нужна теорема, что ли. Тоже из теорвера.
Значит, у нас есть парадокс дня рождения. Наверняка многим знакомое.
Значит, парадокс такой. Представьте, что у вас есть n случайных величин. Давайте я их назову
x1 и так далее, xn. Случайные величины. Со значениями в Zm. Со значениями в Zm. Вот. Ну,
величины там независимые, да, они генерируются независимо друг от друга. Тогда, значит,
пункт первый. Значит, если m больше или равно 1 втраян квадрат, то вероятность того,
что произойдет коллизия маленькая. Вероятность того, что, ну, есть коллизия, давайте так и напишу,
есть коллизия не больше одной второй. Есть коллизия, то есть из вот этих вот n сгенерированных
случайных величин хотя бы две одинаковые. То есть формально вот это значит, что существуют
различные i и g такие, что x и t равно x и g. Вот. То есть, если взять m, если взять размер образа,
куда действуют все эти случайные величины, если взять m достаточно большим, то с хорошей
вероятностью у вас не будет коллизии. То есть, вероятность наличия коллизий будет не больше
половины. Значит, с дополнительной вероятностью хотя бы половина коллизий не будет. Ну и второй,
наоборот, если у вас m маленькая, давайте скажем что-нибудь типа 1 пятая n квадрата, то, наоборот,
вероятность того, что коллизия есть, да, все правильно, вероятность того, что коллизия есть,
будет, наоборот, хотя бы 1 вторая. Вот, и как раз вторая часть это парадокс дня рождения,
что если вы возьмете n человек, а в году у вас всего m дней, где m не больше чем 1 пятая n квадрат,
тогда с хорошей вероятностью у вас будет 2 человека с одинаковым днем рождения. Ну там,
на нормальных примерах, если у вас в классе там 20 человек и в году 365 дней, и мы считаем,
что даты рождения все независимые, случайные, распределенные во всем годе, то с вероятностью
хотя бы 1 вторая у вас там будет 2 человека с одним днем рождения. Ну 22, ну что-то такое, порядка 20.
То есть коллизия получается, когда m квадратично по n квадрату. Вот если m квадратично по n квадрату,
то для маленьких коэффициентов коллизии есть, для больших уже нет, ну опять же, с достаточно хорошей вероятностью.
Вот, опять-таки это в идеальном мире, если х были по-настоящему случайными и независимыми.
Значит, на самом деле, если в качестве х брать значение хэш-функции какой-нибудь,
тоже давайте, ну не знаю, замечание что ли, тоже без доказательства, что если вместо реальных х
брать значение хэш-функции в каких-то разных точках, то по крайней мере вот этот первый пункт будет
также верен. Давайте запишем, что если вместо x1 и так далее xn брать значение хэш-функции в каких-нибудь заранее
заодных точках, ну скажем y1 и так далее yn, где h опять-таки берется из универсального семейства хэш-функций.
То, по крайней мере, первый пункт тоже выполняется.
Вот, то есть достаточно требовать не полной независимости всех вот этих величин, а того,
что они сгенерированы как результаты применения случайной хэш-функции в данном наборе точек,
где эта хэш-функция взята из универсального семейства. Тогда опять-таки вероятность
наличия коллизии будет не больше 1 в 2. Понятно, что написано? Сейчас, хороший ответ.
Вот, то есть грубо говоря, что это значит? Это значит, что если вы считаете значение хэш-функции,
вот этой вот хэш-функции из нашего универсального семейства, если вы считаете значение хэш-функции
в n точках и хотите, чтобы они все были различными, тогда вам достаточно брать m вот такого порядка.
Тогда с хорошей вероятностью у вас реально все значения будут различными. То есть если ваша цель
сгенерировать, выбрать h такую, что все вот эти значения попарно различны, все различные,
то вам достаточно выбрать вот такое m в определении хэш-функции. И тогда с неплохой вероятностью вы
победите. То есть будут коллизии с маленькой вероятностью, не будет с вероятностью хотя бы 1 в 2.
В этом семействе, который вы описали, было простое число, оно должно быть порядком максимального
из предполагаемых? Ну типа если все там до 10 в 9, то порядка 10 в 9. Если все до 10 в 18, то порядка 10 в 18.
Вот, хорошо. Теперь на основе этого мы можем сделать совершенное хэширование. Совершенное
хэширование, немножко обрезанная задача по сравнению с полной. А именно, задача следующая,
у вас заранее известен набор ключей. Давайте скажем x1 и так далее, xn. Заранее известный набор ключей.
Ну и дальше, кажется можно так обобщить, что у вас во-первых поступают запросы insert
какой-то x и t и find произвольное y. То есть вам заранее сказали, какие потенциально ключи можно
добавлять в вашу базу данных, в вашу таблицу. Добавлять могут только их, ничего не удаляют, хотя в
принципе удалять тоже можно одни из них, но давайте без этого. Обычно это не реализуется. Значит
добавлять могут только их, а спрашивать про наличие могут чего угодно, если произвольный y в вашей
таблице. Например, откуда такое может быть? Не знаю, у вас есть какие-нибудь участники, которые
зарегистрировались на соревнования, и вы понимаете, что только они могут совершать какие-нибудь посылки
в контест. Тогда x это участники, вы знаете, что посылать будут только они. Ну и соответственно
инсерты, кто-нибудь что-нибудь послал и приходит там директор спрашивает, а Вася Петров что-нибудь
отправил или нет? Ну что-нибудь такое. То есть в принципе не совсем оторванная от реальности задача,
такое бывает. Вот, что мы сделаем? Давайте мы сначала сделаем то же самое. Давайте сделаем х-таблицу
типа с цепочками. Давайте возьмем m равное n просто-напросто. Построим вот такой вот массив размера m и
возьмем какую-нибудь внешнюю хэш-функцию, первую хэш-функцию hout, которая вот эти вот все x как-нибудь
раскидает по этому массиву. Тогда там могут быть коллизии, даже более того, скорее всего будут,
потому что m у меня равно n. Ну и там в какую-то ячейку может быть попало много чисел. Что делать?
Давайте мы вместо того, чтобы делать цепочки, давайте мы здесь сделаем еще одну хэш-таблицу.
Причем такую, в которой не будет коллизий вообще. А именно, пусть вот сюда попала циитая элементов,
попала циитая ключей. То есть, скажем, я сгенерировал случайную внешнюю хэш-функцию hout,
посчитал ее значение во всех иксах, ну и посчитал для каждого и сколько раз мы получили такое
значение хэша. Вот пусть для конкретного и сюда попала циитая ключей. Тогда моя цель, смотрите,
если я знаю, что здесь всего циитая ключей, и скажем, циитая не очень большая, а мы можем ожидать,
что циитая не очень большая, потому что мы, грубо говоря, равномерно размазали по этому массиву все
элементы. Вот, тогда давайте сделаем просто следующее. Давайте мы заведем массив размера циитой в
квадрате, точнее хэш-таблицу на циитой в квадрате элементов, хэш-таблица на циитой в квадрате
элементов. Вот, и здесь внутри сгенерируем новую внутреннюю хэш-функцию hout, которая все эти
элементы, которые сюда попали, будет рассовывать по этой хэш-таблице без коллизий. Поскольку,
если у меня всего в эту ящику попала циитая ключей, то с хорошей вероятностью мы ссылаемся на теорему
парадокса День рождения, что с хорошей вероятностью, если у вас всего столько элементов, а размер таблицы
такой, то при ровномерной генерации, при случайной генерации универсальной хэш-функции у вас с хорошей
вероятностью не будет коллизий. Вот, но ровно это мы и хотим. То есть мы будем генерировать такую h i,
чтобы не было коллизий. Все, и дальше, если мы такого добились, то есть если мы добились, что у меня,
ну там, все вот эти вот цшки не очень большие, что h i t внутренне распределяют без коллизий, тогда на
все запросы отжать элементарно. Мы для каждого х сначала считаем его значение относительно внешней
хэш-функции, понимаем, в какой внутренней хэш-таблице оно должно лежать, какой-то g. Затем
смотрим просто на эту таблицу, ей ассоциирована какая-то хэш-функция h g, которую легко запоминать,
там всего два числа надо хранить, a и b по моделю p. Ну и дальше в нее подставляем вот этот x и
смотрим есть он там или нет. Если нет, добавляем. То же самое с файндом. Мы сначала считаем внешнюю
хэш-функцию, потом внутреннюю, проверяем есть она внутри или нет. А чтобы с хорошей вероятностью не
было коллизий и все. В смысле не понял? 2c и почему не хватит? Ну вот почему например, потому что вот
если у вас размер менее чем квадратичен, то у вас есть коллизия с большой вероятностью и более
того чем меньше здесь констант, тем больше здесь вероятность. То есть тут именно как бы n квадрат
это именно прям ну грубо говоря точная точная граница. Что если больше, то коллизии нет, если
меньше, то коллизии есть. А мы хотим прям полностью без коллизий. Поэтому нам с квадратом придется
работать. Она оттуда же. У нас все функции из одного и того же универсального хэш-функции.
Итак, смотрите. Я сейчас все пропишу. Смотрите, что мы делаем сначала инициализация. У меня есть
какое-то там универсальное семейство хэш-функций, универсальное семейство хэш-функций. Пусть так будет
написано. Первое, что я делаю, это я хочу сгенерировать внешнюю h out так, чтобы сумма
циит и квадрат их была не очень большая. Потому что если циит и квадрат был очень большой, то мне
нужна квадратичная память. Это прям совсем от нас. Мы такого не хотим. Давайте делать следующее.
Генерируем случайную h out. Для нее считаем размеры всех ячеек, всех корзиночек. Вычисляем циит,
это количество, мощность множества тех х таких, что h out равно i. Это формально. По смыслу мы для
каждой ячейки запоминаем, сколько ключей туда попало. Для каждого и считаем, сколько х имеют
такой хэш. Дальше, если сумма квадратов циит больше чем 4n, то перегенерируем и начинаем заново.
Иначе мы говорим, что h out хорошая. Иначе фиксируем h out.
Грубо говоря, пока не получится, давайте просто генерировать новые хэш функции. Вот мы взяли один
раз h out, посчитали сумму квадратов циит. Если это слишком много, ну давайте много 4n. Если больше
чем 4n, все, говорим, что плохая хэш функция, она слишком многими обладает коллизиями, давайте
генерировать заново. Вот я утверждаю, что если мы такое условие напишем, то очень быстро мы победим.
Можно показать, что среднее количество шагов для генерации правильной h out будет двойка. То есть
у вас, если вы взяли случайную h out, то это событие происходит с вероятностью всего лишь 1 вторая,
опять-таки. Ну и значит с вероятностью 1 вторая мы побеждаем на каждом шаге. В среднем будет как
раз двойка шагов в среднем. Так или иначе мы просто генируем много раз h out, до тех пор пока не
получится, что сумма циит квадрат меньше чем 4n. Теперь у нас получается верно вот такое, что сумма
циит квадрат не больше чем 4n. А значит у нас как бы есть возможность для каждого и завести хэш
таблицу размера циит квадрат, потому что суммарный размер будет линейным, и нам нас линейная память
устраивает. То есть мы это делали, потому что мы не хотели квадратичную память, что-то большое. А линей
нам хватит. Ну тогда значит разбиваем все х на группы по равному значению h out, по значению h out
от x. Внутри этой группы нам нужна новая хэш таблица, которая имеет квадратичный размер.
Генерируем хэш таблицу размера циит в квадрате. Без коллизий давайте здесь напишем, без коллизий.
Как добиться того, чтобы она была без коллизий? Опять давайте брать просто случайную h i,
строить явным образом хэш таблицу, проверять нет ли там коллизий. По теореме с хорошей вероятностью
коллизий не будет, потому что у меня размер достаточно большой, размер квадратичен по
количеству элементов. С хорошей вероятностью коллизий вообще не будет. То есть если я возьму
случайную h it, то она с хорошей вероятностью не будет обладать коллизиями. Если нам не повезло
и мы взяли плохую h it, давайте перегенерируем. Здесь опять то же самое, что пока у меня есть
коллизии, то есть пока в конкретной it ячейке, в конкретной it таблице, пока есть коллизия, я беру
новую h i. Так много-много раз, пока не найдут ту h i, на которой нет коллизий. То есть генерируем h i,
пока не получится. h i, пока не получится.
Это нам не даст никакой победы. То есть, грубо говоря, если мы ограничим количество какой-то
константой, то, окей, я не уверен, но возможно тогда вместо квадрата нужно будет там типа, ну вот здесь
какой-нибудь корень писать. Короче, это не даст выигрыша никакого. То есть так можно было бы делать,
но не нужно. То есть это ничем не будет лучше, потому что у нас и так, смотрите, у меня и так
линейная память суммарно, у меня память вот такая, то есть 4n максимум. У меня время построения будет
линейное, потому что, ну что, я сначала несколько раз сделал вот это, в среднем там два раза надо
сделать, это за линию делается. Потом внутри каждой группы, для каждого i, я опять много раз генерирую,
пока не получится без коллизий. Ну суммарно, вот уже опять будет за линейное время. Больше оптимизировать
некуда. Вот, значит, тогда ответ на запрос будет за чистую единицу. Ответ на каждый запрос
за чистая у от одного, извините. Потому что как ответить на запрос? Мне нужно сначала к пришедшему
элементу применить внешнюю hash функцию hash out, сначала понять в какую из внутренних hash таблицы она
попала, а дальше внутри посчитать hash шитая от x и понять в каком элементе она лежит. И поскольку у меня
в этой таблице уже нет коллизий, то там все понятно, либо элемент есть, либо элемента нет, он только
там, да, мне не нужно ничего просматривать. Мне по факту нужно два раза посчитать hash функцию, внешнюю,
а потом внутреннюю, и понять есть элемент или нет. То есть мне не нужно пробегаться по списку,
у меня чистая единица. Да, значит, итог. Мы за у от н в среднем построили структуру размера у от н,
которая отвечает на все запросы, потом уже за от единицы чистая. Которая умеет отвечать
на запросы за от единицы. Ну, можно добавить на самом деле, то есть если рейсы тоже только с x
происходят, то почему бы нет. Ну, повторю, обычно типа этого не происходит, но добавить можно,
от этого ничего не поменяется. Это хороший вопрос, но вот на самом деле, если вам нужен порядок в
каком-то смысле, то надо. Грубо говоря, если вам там надо кроме всех обычных операций узнавать,
там если вы в контесте что-нибудь сдавали, там будет задача типа определить количество чисел
не больше, чем x. В дереве поиска делается на халя, вы просто спускаетесь и понимаете,
сколько элементов слева у вас было. А здесь, чтобы в хэш таблице что-то сделать, хэш таблица,
порядок элементов на у вообще полностью перемешивает, она абсолютно, грубо говоря,
случайно их все перемешивает. Тогда, чтобы понять, сколько элементов не больше, чем x, в хэш таблице
это непонятно как сделать. То есть лучше, что можно это все элементы явным образом перебрать,
а это линия. То есть если хоть что-то, кроме вот обычных операций, insert, find, erase, если хоть что-то
есть, связанное с порядком элементов, тогда вот дерево поиска выигрывает. Так, хорошо.
Вы имеете в виду типа завести массив для каждого х свой? Ну тогда непонятно,
ну вот это я так хорошо сказал, find x и t, на самом деле просто insert x. Мне не говорят его номер в этом
порядке. Это важное замечание. Мне не говорят номер элемента вот в этом его порядке. Мне говорят
просто какой-то Вася Петров. Мне не говорят, что Вася Петров имеет номер 148 в каком-то списке.
Мне просто говорят Вася Петров. Соответственно, этот x я не знаю, где он здесь лежит. Ну а собственно
это вопрос локализации. Мы такого делать не умеем, если у нас нет порядка на них. Вот, хорошо,
значит это все хорошо в теории. Что это значит? Работает за линию предпочета и за единицу на
запросы чистые. Прям супер. Но константа тут ну такая толстенькая, потому что у меня много маленьких
векторов. С памятью там не очень хорошо все это происходит. Ну в смысле с кэшом, с константой и
там. То есть теоретически это хорошо, но константа тут большая на самом деле. Вот, а значит как я
понимаю самый популярный способ реализации х-таблицы это следующий. Значит это х-таблицы с открытой
адресацией. Х-таблицы с открытой адресацией.
Ну давай сделаем следующее. Опять мы заведем массив, длины m. Но теперь у меня в каждой ячейке
будет не односвязанных писок, а просто число. Ну просто элемент лежать. То есть либо элемент,
либо ничего. Тогда, значит опять, вот пришел какой-то х, я посчитал у него h от x, положил х в эту ячейку.
Пришел какой-то у, я посчитал h от у, положил у сюда. Пришел какой-то z, я положил его сюда.
Да, мы помним, что проблема в том случае, когда возникают коллизии. Давайте коллизии будем
решать следующим образом. Не цепочками, а так. Вот представьте, пришел какой-нибудь t, у которого
h от t равно h от x. То есть, грубо говоря, t хочет встать вот сюда. Но тут занято. Значит давайте тогда вместо
этого мы просто пойдем направо и найдем первую свободную ячейку. Вот такая ивристика. То есть,
я просто вижу, что ага, тут занято. Ну ничего не поделаешь, давай порубим сюда. Тут занято.
Порубим сюда. Тут занято. Здесь свободно. Сюда ставим t и заканчиваемся. Все. Весь алгоритм.
Значит, ну там давайте скажем insert t. Сначала вычисляю, ну давайте g равно h от t. Нет,
не хочу. Давайте просто вычисляю h от t. Затем для каждого i давайте я определю run it. Это h
tt плюс i. Для i там 0, 1, 2 и так далее. Run it от t. То есть, run это последовательность ячейок, в которой я
смотрю одну за другой. Run 0 от t это просто h от t, куда я хочу попасть в самом начале. Run 1 это
следующий ячейк, run 2 здесь, run 4 здесь и так далее. Значит, я просто иду по списку вот этих значений,
то есть грубо говоря, я посчитал h от t и добавляю по единичке каждый раз. Когда нахожу свободную
ячейку туда и кладу t. На кладу t в первую свободную ячейку. В первую свободную ячейку. Вот. Так работает
insert. Find абсолютно аналогично. Если приходит find какой-то t, то же самое. Я считаю h от t,
значит мы понимаем, что если t где-то и лежит, то либо в точке h от t, либо ht плюс 1, либо и так далее,
до первой дырки. То есть, грубо говоря, если у меня есть какой-то блок заполненных элементов,
начиная с h от t, а вот здесь дырка, тут пусто, тогда t если где-то и присутствует, то где-то на этом
отрезке. Вот где-то здесь оно должно быть. Потому что если я дошел до дырки и t не встретил,
значит t не могло раньше прийти, потому что если оно пришло раньше, то оно прошло весь этот путь,
ну и закончилось до дырки. Значит t просто нет. Аналогично идем до свободного места.
Нет, ну это скорее описание процедуры, как мы ищем t.
Ну на самом деле хранить его не особенно есть смысл. Я обобщил, ну то есть я написал так,
потому что можно другие раны использовать. Чуть позже я скажу другие раны, но это просто тупой
линейный проход. У меня рана it, это it ячейка после h от t. Это я просто формально написал. На самом
деле можно просто считать, что иду слева направо, начиная с h от t. Ну тогда казалось бы, все идеально.
Значит здесь очень хорошо все с кэшом, все с памятью, потому что у меня нет вот этих вот
перескакиваний в памяти. В отличие от хэш таблицы со списками, когда я там прыгаю очень сильно по
памяти. Я сначала посчитал хэш, прыгнул сюда, потом еще по таблице пошел по списку, и сам список
это указатели, которые тоже бог пойми где в памяти лежат. Там с памяти все очень плохо,
константа очень большая, потому что мне нужно много прыгать по памяти. А здесь у меня все изменения
очень локальные. Если я один раз посчитал хэш, то дальше мне нужно пройти просто слева направо вот
во все эти ячейки посмотреть. Но память, доступ к памяти так работает, что если вы как бы сюда
смотрите, то следующие элементы у вас близко. Вам не нужно далеко перемещать указатель, и у вас
все все эти данные близко, и до них дойти можно быстро. Константа сильно меньше для вот этого
перехода. На практике здесь константа сильно лучше, работает сильно быстрее, чем цепочка.
Что же делать с эрейзом? Тут уже не сработает такая идея, что давайте просто встанем сюда,
найдем значение t и его отсюда удалим, просто удалим из таблицы. Потому что, например,
представьте себе у вас такая картинка. Значит, у вас был x, y, z. Пусть у них,
например, у всех одинаковый хэш. h от x равно h от y равно h от z. Вы их вот так вставили в
таком порядке, потом вызвали, например, race y. Если вы просто сотрете этот y и скажете,
что эта клетка пустая, то вы теперь z не сможете найти, у вас z как бы нету. Потому что чтобы найти z,
вы встаетесь сюда и идете до первой дырки, а дырку у вас вот сразу же вы z не найдете.
Поэтому мы просто так затирать нельзя. Вместо этого давайте мы оставим здесь лежать y,
просто пометим, ну или там, не важно, короче, мы пометим, что этот элемент удален. Мы явно не
будем его удалять из таблицы, мы просто пометим, булис и флаг какой-то на него повесим, скажем,
что этот человек удален. Это называется tombstone, типа могильный камень. Все, он от нас ушел,
мы пометили, что его там нет. Мы его удалили, но как бы не удалили, мы пометили, что он удален,
скорее так. И тогда у нас такой проблемы уже не будет, что когда я пытаюсь найти z, я точно так
же делаю, я встаю сюда, здесь не z, встаю сюда, это не дырка, tombstone не считается дыркой, я говорю,
что окей, его тут пока нет, иду дальше. И потом до него дошел. Вот, поэтому как бы find работает
так же корректно. Единственное, что нужно подправить, это insert. Нужно сделать следующее, нужно сделать так,
что insert может вставлять на место tombstone. То есть если он идет слева направо и нашел не дырку,
а элемент, который удален, тогда можно на его место что-то поставить. Потому что как раз удаленность,
удалять элементы нельзя, потому что мы нарушим связанность вот этого блока. А на его место ставить
что-то другое можно. То есть если у меня здесь был tombstone, а я сюда хочу что-то положить,
какой-нибудь t, то я просто говорю, что здесь t и снимаю метку удаленности. Давайте запишем,
можно класть во время инсерта новые элементы на место удаленных. То есть точно так же я иду
слева направо, жду либо дырки, либо tombstone. И в том и в другом случае я на это место кладу элемент,
который я хочу добавить. Так, ну вот теперь все. Давайте я только здесь еще формализую, что здесь
я беру все это по модулю m. Потому что если вдруг так вышло, что у вас h от x это последний элемент,
и вы хотите взять следующий, то вам надо чтобы следующий был. Поэтому я как бы массив зацикливаю
и говорю, что следующий это первый. А следующий за последним это первый. Но это тонкость.
Вот. Это реализация hash таблицы с открытой адресацией. Это называется линейное пробирование.
Вот такие раны это линейное пробирование. Линейное пробирование. То есть когда для
определения места положения x я просто иду слева направо, прибавляя единичку каждый раз.
Есть другие альтернативные реализации, есть квадратичные пробирования. Это когда у вас
ran it от t это h плюс и квадрат. Но опять-таки по модулю m. Вот. И еще более крутая реализация,
называется двойное хэширование. Это когда у вас ran it от t это h первое плюс и наш второе.
То есть у вас есть две хэш функции. Первая ячейка, в которую вы встаёте, это h1, а дальше вы идёте
линейно с шагом h2. Не с шагом 1, а с шагом h2, которая тоже хэш функция, которую вы заранее
фиксировали. Случайно. Ещё раз? Вроде хэширование. Вроде хэширование называется. По-моему это называется
так. Пробирование это какие элементы мы пробуем на то, чтобы вставить х, а здесь двойное пробивание
как-то странно. Мы не два раза пробили. Кажется так называется. Неважно. В чём смысл, в чём выигрыш
квадратичного и двойного? Например, смотрите, если так вышло, что у меня образовался какой-то
большой блок чисел, который заполнен x, y, z, t, u и так далее. Наверное, хоть хэши у них и близкие,
но наверное различные. Например, здесь было h от x, здесь было h от t, то есть сначала попробовал сюда,
потом добавил сюда. То есть у них хэши близкие, но различные. Если у вас начало различное и вы будете
идти нелинейным просто проходом слева направо, тогда они обязательно склеиваются, потому что они
близкие, вы просто вот так идёте, они склеиваются. А если вы будете прыгать как-то более нетривиально,
скажем вот так квадратично или с шагом равным другой хэш функции от элемента, тогда у вас скорее
всего вот эти раны будут как бы менее пересекаться. То есть у вас сейчас прикол в том, что когда вы
начинаете здесь и идёте слева направо и вот здесь идёте слева направо, у вас раны склеились,
у вас вот эти все элементы, и вот это одно и то же. А если бы вы начали здесь и здесь и шли бы с
другими последовательностями, не плюс один, плюс один, плюс один, а плюс h2 зависит ещё от x. Тогда у
вас они как бы разойдутся. Этот бы сразу перешёл сюда, а этот вот туда вот. Они были бы различны.
Ну нет, смотрите, потому что если раны определять именно как вот там, то у нас ничего не пропустится.
То есть там опять будет верно то же самое, что... Ну то есть представьте, что вот здесь вместо того,
чтобы идти слева направо, я иду там ну по квадратам, например, не плюс один, плюс один, плюс один,
а плюс один, плюс четыре, плюс девять и так далее. Тогда вот уже ничего не пропустится. Ну с инсертом
всё будет нормально, а с рейзами, смотрите, если я когда добавил x, то есть вот если я когда-то
куда-то вставил x, это значит я сначала посчитал h от x, потом вот по этому рану до него дошёл. Но
если я хочу теперь сделать рейс x, то я встаю сюда и иду по тому же рану. Я его не упущу никогда.
Поэтому как именно я реализовываю ран, это неважно. Если элемент когда-то вставил, то найти его я
смогу по тому же рану. Просто последовательность ячеек, которые я переводил, та же. Не важно,
как именно она устроена. Да, да. Значит вот есть теорема. Ну не знаю, наверное, в следующий раз
формализую, но, грубо говоря, давайте напишу формулировку, что если первое при линейном пробировании
значит вам нужно, чтобы семейство х-функций было 5 независимое, но, видимо, в следующий раз уже
дадим определение, но, грубо говоря, это более сильное требование. Если семейство х-функций 5
независимое, тогда у вас опять-таки будет, ну, учетная единичка на запрос. А если вы берете двойное
хэширование, тогда вам достаточно два независимых семейств, и тоже будет единичка. Вот. То есть двойное
идти получше. В следующий раз я доформализую, но мораль именно такая, что если вы вместо линейного
пробирования будете использовать двойное хэширование, то будет приятнее и быстрее. А по-разумевшим,
ничего не понятно. Там ничего не понятно. Все, спасибо.

Всем привет! У нас с вами две пары назад начался MapReduce, потом мы сделали такое отступление в
сторону джамвы и сегодня снова продолжим MapReduce. Сейчас я расшарю экран. Всем экран виден, да?
Отлично. Сначала повторим немного MapReduce, потому что было давно базовые вещи. То есть
парадигма MapReduce возникла в 2004 году на основе статьи от Google и работает она примерно так. То
есть работаем с парами ключ значения и эти пары проходят несколько стадий. Первая стадия это стадия
мап независимая обработка ключей значений. Потом эти пары сортируются и группируются по ключам.
Эти группы передаются на редьюсеры и на редьюсере уже происходит какая-то агрегация по этим группам.
Можно посмотреть, например, то есть мап работает примерно так. Берем элемент один и что-то с ним
делаем, а на редьюсере берем всю группу и, например, складываем все элементы ее.
Если посмотрим на эту задачу, то вспомним задачу на подсчет чистоты встречаемости слов в тексте,
то с помощью MapReduce мы это будем делать вот так. Сначала сформируем пары слова единичка,
потом от сортируем и сгруппируем их по словам и потом схлопнем вот эти группы.
Вот эта схема, к которой мы в прошлый раз часто возвращались, что есть такая особенность,
что на одном редьюсере может быть или один ключ или больше, чем один ключ, но если на одном
редьюсере уже попал ключ 1, то нигде на другом редьюсере ключ K1 уже не будет.
Кто помнит из позапрошлой лекции, почему здесь написано вот это, то есть почему блок,
который подается на маппер, примерно равен блоку HDFS, но не в точности равен.
Потому что они чуть меньше, может быть, или что-то такое?
Может быть и чуть больше, может и чуть меньше, а почему? Да, вопрос от Максима,
если K1 не вмещается в память, то это плохо, то как бы наш редьюсер будет падать,
мы должны с этим как-то справиться. Как справиться будет чуть позже,
но это как бы на стороне программиста. Карпухин Сергей говорит, потому что сплиты,
а более подробно, ну сплиты, там мы называли блоки, здесь называли сплиты, что поменялось.
Ну а что значит, зачем нам сплиты, почему нельзя просто работать с блоками,
и почему они отличаются по размеру? Мы хотим какие-то целостные файлы
передавать, не просто рандомно, как-то поделенные. Да, то есть блок это как бы,
мы взяли файл и разбили его с точностью до бита, и поэтому если формат хранения
какой-то необычный, то мы можем сломать этот формат и не сможем прочитать файл.
Да, вот все правильно Сергей написал. Ну и давайте перед тем, как мы пойдем дальше,
разберем несколько таких вот задачек. Есть задача grab, ну это как бы фильтрация. Как бы вы
решали такую задачу с помощью mapReduce? Фильтровали бы в мапе. Да, то есть мап это был контент,
здесь контент фильтра, то есть тот текст, который содержит слово, вот мы его в мапе
проверили, и в принципе все, reducer у нас нету, shuffled sort у нас нету, а слово передается в distributed
cache. Еще посмотрим на group by, тут как бы возможно два варианта. Если у нас агрегация есть,
а обычно она есть, то как бы мы на мапе не делаем ничего, вот в отличие от grab,
где мы все делали на мапе, тут мы не делаем ничего, зато мы на этапе shuffled sort группируем,
и на reducer делаем агрегацию, если она есть, то есть если от нас требуется просто сгруппировать
вот так, то тогда reducer не будет, а если от нас требуется сделать вот так, как здесь на этой
схеме, то есть было три записи, мы их сломнули в одну, тогда нужно делать eluse тоже. Вот,
ну и теперь посмотрим на вот такой вот кейс. Смотрите, это наша задача word count,
слово единичка, вот, и мы видим, что вот таких пар у нас много с одинаковыми словами,
их надо передать на reducer. Для того, чтобы меньше передавать данных по сети, мы можем еще на этапе
мапера вот эти вот пары с одинаковыми ключами просуммировать, и такая функция в mapp reducer
есть, она называется combiner, и работает она вот так, то есть если вы посмотрите на эту вот схемку,
то она вам напомнит обычный mapp reducer, ну как бы mapper, сортировка, reducer, но на самом деле вот эта
часть, она выполняется локально на одной ноде, то есть там, где отработал mapper, у нас выполняется
там же сортировка, и там же выполняется combiner. Вот, и после этого уже идет глобальный shell
class and sort, и потом reducer. Вот давайте посмотрим на эту схему еще раз с прошлого занятия, точнее.
Что у нас тут есть? У нас отработал mapper, результаты мы записали в буфер в памяти,
но мы помним, что я прошлый раз рассказывал, если буфера не хватает, мы скидываем это все на диск,
но так или иначе у нас результат хранится на этой моде. Тут же мы его можем отсортировать,
получить вот такие вот группы, и место для combiner оно вот здесь,
то есть прежде чем мы понесем вот эти группы дальше в reducer, мы их уменьшим, мы их схлопнем.
Вот, ну и чем отличается combiner от reducer, то есть он по сути делает похожие вещи,
но во-первых он сортирует результат работы одного mapper, поэтому данных мало,
и все происходит на той же ноде, значит нет передачи данных по сети. И вот combiner может
применяться несколько раз. Чтобы понять почему так, давайте я открою другую схему.
Это схема с предыдущей презентацией, и посмотрите как у нас работает MapReduce в деталях.
Вот mapper, вот у нас результат хранится в буфере, но этого буфера может не хватать.
Тогда мы делаем спилы, то есть мы скидываем лишние данные на диск.
То есть скинули один раз, скинули второй раз, и у нас получается много вот таких кусочков данных,
и мы начинаем их схлопывать, combining. Выполняем combiner для одного кусочка, для второго,
для третьего. В результате вот эти спилы мы схлопываем в один файл, вот merge,
еще раз выполняем combiner. А еще может быть такой кейс, когда у нас несколько мапперов отработали
на одной и той же ноде. Поэтому мы обработали данные с одного маппера от combiner, но видим,
что еще на этой же ноде у нас выполнялся другой маппер, и мы тоже можем не тащить эти два файла
на reduce, а обработать их combiner вот здесь. Поэтому вот merge final,
все обработали, и дальше уже вот этот померженный файл отправляем по сети на Shaft and Sort.
То есть, как вы видите, combiner может применяться сам к себе несколько раз, а может быть такое,
что он вообще не будет применяться. Здесь написано выраженный случай, когда у нас одна запись,
ну или даже не одна запись, а если там две или три записи в результате маппера, то Hadoop умный,
и он понимает, что стартовать combiner — это по сути стартовать контейнер, то есть нужны ресурсы в
ярне, нужно стартовать виртуальную машину Java, то есть это долго и сложно. Если записей мало,
то он запускать combiner не будет, а сразу прямиком отдаст это дело в наше отсутствие.
Вот главная вещь, что так как combiner у нас может применяться несколько раз к одному,
как бы к самому себе, то мы не должны менять тип ключ значения. Просто если мы поменяем и захотим
еще раз применить combiner, то входные данные уже будут другие, и combiner может упасть.
Так, хорошо, есть ли вопросы по combiner какие-нибудь?
Что происходит в том случае, если combiner не применяется? Ну как бы, как мы сказали,
combiner не меняет тип ключа значения, поэтому он не применился, мы просто дальше пошли с этим
блоком данных работать, так как будто combiner нет. Вот здесь он не применился,
допустим, и мы вот эти блоки не обработанные отдали дальше на reduce.
Хорошо, есть еще какие-нибудь вопросы по combiner?
Сейчас я найду схему.
Если вопросов нет, давайте тогда поработаем с combiner более так практически и попробуем
решить вот такие задачи. То есть у нас есть какие-то пары или просто может не пары,
числа и мы хотим на reduce посчитать сумму, среднюю и медиану. Давайте подумаем,
какой будет combiner в каждом из этих случаях.
Вот давайте начнем с суммы. Какие тут идеи есть?
Мы видим, после того как отсортировали, у нас подряд идут одинаковые ключи с разными
значениями и мы просто подряд их идущие просуммируем локально.
Да, вот все правильно, то есть по сути combiner повторяет reducer, мы считаем сумму ту же самую.
У нас не меняется тип ключа значения, то есть было слово число и стало слово число тоже.
Число увеличилось, но стало другое, но все равно число. А среднее надо хранить количество и сумму.
То есть да, у нас был маппер, в маппере мы хранили word count, но если мы будем считать
среднее и сохранять его в count, то нам получится так, что нужно считать среднее от среднего.
А среднее от среднего это уже неправильное среднее, поэтому вот здесь должно быть слово
единичка, то есть вместо одного каунта должно быть два числа. Я это сейчас покажу тоже на схеме,
поэтому если кто-то не понял, то не страшно. И давайте подумаем вот здесь, что будет в такой
компании. Разве медиана от медианы будет медианой?
Да, компанер не может изменить типы данных, но значение это он может изменить.
Но медиана от медианы не будет равна медиане, поэтому тут не так.
Вот, то есть вот что такое медиана, если кто-то забыл, то мы упорядочиваем наше число и берем среднее по
позиции. Если у нас число элементов четное, то берем два соседних и берем между ними среднее.
То есть главное, что нам нужно данные как-то отсортировать.
Вот, поэтому действительно на этапе мапера, на этапе комбайнера мы еще не можем глобально
отсортировать данные, поэтому для медианы комбайнер сделать не получится. Комбайнер,
который ничего не делает, в нем смысла нету. Ну мы просто, если он ничего не делает, то он нам
и не нужен, мы его не подключаем просто. Вот, на комбайнере можем прочитать сумму,
Святослав говорит, но нам нужно медиану, то есть нам не надо терять числа. Нам надо их
отсортировать и взять среднюю позицию. Поэтому если мы сделаем сумму, то мы потеряем порядок,
но надо еще порядок где-то хранить. Можно так что-нибудь попробовать, то есть уже с двумя
значениями, но в принципе комбайнер нам только усложнит все. То есть комбайнер нужно применять
не всегда. То есть видите, в редюсере комбайнер это сумма, в AVG в среднем комбайнер это сумма
и каунд, и при этом нам надо изменить маппер. То есть если у нас было маппер слово единичка,
то теперь чтобы у нас комбайнер не изменил ключ значения, то есть в результате комбайнера
получилось два числа, значит тут тоже должно быть два числа. Да, можно использовать формулу для
квантилий, ну то есть всякие статистические примочки есть, но как правильно говорит Сергей,
это будет уже не точно оценочно. Да, ну мы пока говорим про точный подсчет, а в принципе в реальной
жизни в таких вот задачах big data нам нужно часто что-нибудь оценить. У нас будет через пару лекций
хайф, и там мы в хайве будем использовать приблизительные подсчеты. Вот если приблизительно,
тогда да, тогда можно. Есть еще такая штука как in-mapper-combiner. Что это такое? Если мы до этого
смотрели на вот эту схему, и комбайнер это был запуск какого-то процесса, какого-то java.
Но в принципе, если мы знаем, что у нас данные устроены так, что прямо много повторяющихся слов,
то можем уже на маппере, то есть мы знаем, что записей у нас много пар ключ значения,
а уникальных слов мало, поэтому мы можем уже на маппере сделать дикт локальный, в этот дикт
по записывать слова, и уже здесь будет вот такая штука, то есть приходит одинаковое слово единичкой,
мы их тут же слопываем в слово n. Вот это очень удобно, не надо даже писать свой комбайнер,
не надо думать про то, что ключи значения не меняются, но у такого подхода есть минус,
кто подскажет какой.
Вот чем плохо, если мы будем делать функционал комбайнера на маппере?
Да, именно так. То есть вот тут, как я сказал, нам нужно хранить дикт дикшнри, чтобы по каждому
слову считать n, и может оказаться так, что у нас таких слов очень много, и до этого словаря не
будет хватать памяти, поэтому вот таким лучше не увлекаться в целом.
Вот с комбайнером пока все, появились ли вопросы по поводу комбайнера?
Так, если нет, идем дальше. Еще один элемент, это компаратор. Вот мы с вами на семинаре
Памаприюс на ближайшем разбирали задачу, где нужно что-то отсортировать в обратном
порядке, и оказалось, что ходоп у нас сортирует лексикографически все, как текст,
плюс он нас сортирует только в прямом порядке, поэтому чтобы сортировать в обратном, надо
писать вот такой костыль. Но костыль писать не хочется.
Что можно сделать? Можно сделать свой компаратор.
Свой компаратор можно написать только на джаме, или можно использовать уже готовые.
Для тех, кто знает джаву, надо написать вот такой простой класс, который реализует этот
интерфейс, и у этого класса, по сути, нужно реализовать только один метод, вот такой.
То есть компаратор для сравнения двух объектов.
Но это для джавистов. Для не джавистов можно использовать
кейфилд-бейст-компаратор.
Что здесь указано про этот компаратор?
То, что мы его можем подключить к нашей МАП-рью задачи, но помните, вот код на стриминге,
где мы подключали минус маппер, минус комбайнер, то есть можем подключить вот этот готовый
компаратор и использовать всякие опции с ним. То есть по какому полю сортируем?
Можно по нескольким полям даже.
Дальше как сортируем, так числа. В каком порядке сортируем?
Вот, ну тут еще есть дополнительные опции. В принципе, если хотите понять,
какие опции есть у кейфилд-бейст-компаратора, то просто сделайте man sort.
Вот, и опции, они в принципе совпадают с опциями команды sort.
Вот это что касается компаратора. То есть пишем только на джаве,
реализуем вот этот метод или используем готовый какой-то класс.
Еще один элемент – это партишнир. Вот кто помнит с прошлой пары,
вообще зачем нужен партишнир, что он делает?
Ну про это точно было. Никто не помнит?
Да, делит данные перед сортировкой. То есть нам нужно как-то обеспечить
вот это вот. Вот, то есть чтобы данные разбились по редьюсерам, причем разбились
правильно, чтобы у нас не были ключи разбросаны по всем редьюсерам.
Вот, и за это отвечает именно партишнир. Что он делает?
По умолчанию он работает вот так. Он берет хешет ключа, делит на количество редьюсеров,
и на выходе получается номер редьюсера. Но бывает так, что нам нужно использовать
какой-то свой партишнир. Например, у нас есть какой-то сложный ключ.
Вот здесь какой-нибудь ключ типа field1, field2, field3 и value. Мы будем редьюсить по вот этому
ключу всему, но мы хотим сделать так, чтобы вот все ключи, у которых одинаковый f1, попали на одну
ноду. Вот, что при этом можно сделать? Можно сделать свой партишнир, который будет работать вот так.
Хешет ключа 0, делит на r. То есть вот такой партишнир новый, он не нарушит вот эту нашу схему,
просто потому что не может быть так, чтобы хешет ключа 0 был разный, а вот эти значения все были
одинаковые. Поэтому с схемой у нас вот это не нарушится, но мы просто с помощью партишнира можем
сделать дополнительное условие, что вот эти k2, k3, k4 будут не просто какие-то ключи, а вот ключи с
одинаковым f1. Вот, ну и как его реализовать? Его реализовать можно тоже только на джаве или использовать
готовый. Для тех, кто это реализует на джаве, вам нужно реализовать всего одну функцию. Вот такую,
то есть на вход подается пара ключ значения и константа r количество редьюсеров. И дальше
нужно на основе этих трех значений выдать номер редьюсера. Как мы это будем делать? Это вот на
наше усмотрение. Вот, то есть мы с вами разобрали три дополнительных элемента mapReduce, то есть
есть не только map и есть. То есть можно чтобы элементы с одинаковыми ключами попали в разные
редьюсеры? Теоретически мы так можем сделать, но это будет нарушать mapReduce и ходу не гарантирует,
что в этом случае мы посчитаем правильно. А тогда можно сделать, можно вообще внутри партишнира
реализовать просто какой-нибудь рандом. Вот такую функцию условно int getPartition. Тут kvalue r и мы
делаем там вот так. То есть вообще какой-то рандомный редьюсер, мы это раскидываем как попало,
но потом-то у нас все сломается, потому что мы будем считать по ключам, а у нас ключ 1 есть и здесь,
и здесь у нас будут какие-то частичные результаты, которые нужно будет потом доагрегировать.
Ну да, написать такой код можно и он даже заработает.
Я правильно понимаю, что это вопрос к тому, что если у нас ключей k1 очень много, вот как здесь,
у нас много ключей k1 и они не влазят в память, что делать с ними? Ну давайте их рандомно раскидаем,
как вариант конечно да, но нам потом придется еще отлавливать эти ключи по всем редьюсерам и как бы
придется запускать по сути еще одну редьюсер задачу поверх этой первой. Поэтому если мы хотим
бороться с жирным вот этим ключом, то лучше сделать не так, лучше сделать еще на этапе мапера какой-нибудь
seed, то есть сделать k1 random например 4. Вот и все, у нас получается, что мы этот жирный ключ разбили
на 4 других ключа и ходу будет думать, что это разные ключи. Он их посчитает, а нам потом останется
как-нибудь вручную сложить пары типа k1 n1 и там k1 2, вот здесь k1 1 например. Вот такие пары нам
останется сложить вручную, но их уже будет немного, мы точно знаем сколько, точно знаем как они
выглядят, поэтому тут уже не мапредьюс, а питон нам в помощь. Да, вот если мы хотим бороться с
несбалансированными данными, то остается вот так только подсаливать ключи.
Хорошо, давайте разберем вот такую задачку. Это упрощенный обратный индекс. Что вообще такой
обратный индекс? Это алгоритм, который часто используется в всяких поисковых сервисах,
то есть у нас на вход подается список документов и какое-нибудь слово, и нам
нужно выдать поисковую выточу слова и список документов, в которых оно нашлось. В реальной
жизни мы там еще сортируем по встречательности этих слов, но это уже следующий этап. Давайте
пока посмотрим на эту задачу и подумаем, как бы вы делали ее с помощью мапредьюса.
Возможно, в маппере разделил бы контент по отдельным термам и далее уже передавал
просто сочетание обратное, типа пару из терм и doc ID. Да, все именно так. То есть в маппе мы
проверяем, если терм встретился, то мы формируем такую пару. Потом нам остается
это дело сгруппировать, потому что терма может быть несколько, и у нас будут вот такие группы,
терм и куча ID. На комбайнере мы, в принципе, делаем то же самое. То есть мы на комбайнере
делаем группировку из вот этих пар и на редьюсере продолжаем делать группировку.
Вот, ну и теперь давайте посмотрим на настоящий обратный индекс. То есть тут
появляется еще TF Term Frequency, и мы не просто формируем группу из терма и кучи ID,
а мы еще считаем у каждого документа Term Frequency и сортируем по этой Term Frequency.
Вот, как считать Term Frequency? То есть по сути мы на вот эту вот задачу, которую здесь решили,
мы еще накручиваем сверху word count. У нас получается на маппере Term Doc ID единичка,
на редьюсере мы делаем суммирование Term Doc ID. Вот, но на выходе-то у нас должно быть
выход вот такой. Term и список пар Doc ID TF Doc ID TF. То есть несмотря на то, что тут ключ Doc ID
и Term, вот видите, то есть по сути, если мы не будем смотреть на эту последнюю строчку,
то вот этот мап и вот этот редьюс это что? Это по сути word count. Только ключ тут стал больше,
он стал из двух элементов, но так это word count. А дальше, чтобы получить нам вот такой выход,
нам нужно обеспечить то, что пары с одинаковым термом придут на один редьюсер. То есть вот
вот это самое уже partitioner, когда у нас есть Term ID и сумма, и мы пишем partitioner,
который здесь сделает hash от терма делить на R. И тогда все пары с одинаковым термом,
они придут на один редьюсер. Это ничего не нарушает, но мы сможем составить из них вот такие вот
кортежи. Вот это как бы реальный случай, в котором нам нужен свой самописный partitioner.
Если какие-то вопросы по этой части, нам еще сегодня осталось две темы, одна маленькая,
другая большая. Если успеем, еще и третью рассмотрим, и тогда в теории мап редьюс будет закончен.
Хорошо, тогда идем дальше.
Вот данные на выходе мапера и редьюсера можем хранить двумя способами. Вот даже если рассмотрим
эту задачу с термом и Doc ID, у нас Doc ID может быть много, поэтому мы их можем хранить двумя вот
такими способами. Подход называется pairs и stripes. Первый способ, это когда мы храним терм,
и вот тут хранится коллекция из каких-то ключей. И второй способ, когда у нас на каждое значение
формируется своя пара, и получается много пар. Можете сказать, какие плюсы и минусы у каждого подхода?
Первое, по памяти меньше занимает? По памяти меньше, stripes это меньше по памяти. Ну вроде
когда, потому с ней нужно хоть раз терм повторять. Терм не надо повторять, но у нас одна запись,
вот эта вот, она может быть очень длинной. То есть как раз в плане памяти pairs лучше,
потому что да, записей много, но они короткие. И мы помним вот эту схему мапредьюса, и помним,
что мапредьюст не боится того, что у нас закончится бушер. То есть мы будем эти пары
скидывать, скидывать, скидывать в бушер. В какой-то момент бушер скажет, я переполнился, скидываю на
диск, и будем дальше скидывать. То есть тут можно как угодно, а вот что делать с тем,
что у нас огромная пара, на одну пару не хватило памяти, вот тут непонятно. Тут иметь в виду лучше
данные хранить для input в маппер? Для output в маппер. Что касается памяти, stripes в плане
памяти даже больше нагрузку дает. Но зато мы меньше передаем данных по сети, меньше пишем на диск,
то есть пары большие, но редко. То есть взяли одну пару, забакапили вторую, и их будет не очень много.
А что касается pairs, больше записей, то есть да, меньше нагрузка на память, потому что записи
маленькие, и то есть тут еще плохо то, что мы вот эту большую пару собираем и храним прямо внутри
маппера. А здесь у нас такого нет, мы просто маленькие пары скидываем в буфер, память не
грузим, но зато мы грузим диски, потому что надо все время backup-ить данные на диск, и грузим сеть,
потому что вот эти пары, то есть маппер отработал, он их забакапил, но потом вот эту всю кучу пар
придется передавать дальше на редьюсер. Поэтому вот такие вот особенности.
Хорошо, и теперь нам осталось, осталась самая большая боль маппредьюса, это joining. Именно
реализация joining в маппредьюсе привела к тому, что стали появляться какие-то новые системы,
в которых joining сделан проще, например там системы Hype, Spark, потому что как мы сейчас увидим,
joining это в маппредьюсе настоящая боль, и у меня к вам два вопроса. Первое, какие типы joining вы
помните? Left join, inner join. Да, left join, inner join, right join и full join, 4 join.
Вот такие joining. И второй вопрос, какая самая большая проблема, если мы захотим реализовать
join в маппредьюсе? С чем вы сразу столкнетесь? Что вам там не нравится? Есть какие-то две таблицы
или два депо сета, и мы хотим их поджоинить.
Есть ли какие-нибудь идеи? Какие будут проблемы вот сразу, как только мы начнем пытаться сделать
joining в маппредьюсе?
Хорошо, основная проблема тут в том, что депо сета два, то есть до этого
мы получали какой-то один депо сет, у него одна структура, и мы его дальше гнали через мапп,
потом через комбайн, потом через редьюс, а тут два депо сета совершенно разной структурой,
и мы не можем, вообще в маппредьюсе мы не можем сказать, что вот у нас два разных маппера,
и поэтому на таких нодах работает этот маппер, а на таких нодах работает другой маппер.
Так как так сделать нельзя, приходится вот так вот извращаться, то есть что мы делаем?
У нас два депо сета, ну и давайте, например, сделаем какой-то более реальный пример.
Ну, например, у нас есть таблица студентов, ID, name, фамилия, ну и что-нибудь еще, какие-нибудь
строковые данные. Ну и еще, что тут важно, это группа.
И есть отдельная группа. Ну и тут тоже какие-нибудь еще строковые данные, то есть
есть две таблички студенты группа, и когда мы хотим их поджойнить, даже без какой-то агрегации,
просто сделать join от этих двух таблиц, мы видим, что у нас разная структура. Здесь, например,
у нас четыре поля, а здесь два. А у нас разными ID-шниками, только в единственном экземпляре в депо сете?
Давайте считать, что да. Ну, то есть обычная таблица уникальные ID-шники.
То есть здесь ID-шник группа ID уникальный, а здесь ID-шник ID уникальный.
А вот группа ID в этой таблице не уникальна.
Да, мы можем сделать какой-то маппер, который будет эти структуры рассматривать как ID и
какой-то объект. В объект можно запихать и вот это все, и вот это, не важно, что будет.
Но получается другая проблема. Да, мы вот так записали, у нас отработал маппер, но дальше как
из этого объекта нам восстановить данные? Как понять, где хранится вот это, где хранится вот это?
Опять кастить, опять пытаться этот объект распарсить не очень хорошо, поэтому мы еще храним тег.
И в этом теге мы укажем, все-таки откуда пришла эта стака. Из первого датасета или
из второго. Тег это A и B, тут мы видим.
И получается, вот это будет группа ID, как раз таки. Да, это будет группа ID.
Вот, то есть добавили тег. Ну и теперь нам нужно эти данные отсортировать, группировать, то есть
запустить обычную стадию mapReduce. После этого у нас вот эти пары приходят на Reducer, сгруппированные
по группой ID. То есть у нас придут пары из тега равно A, из тега равно B, и нам останется уже
на стороне Reducer реализовать join. То есть, по сути, у нас на один Reducer придет группа ID
равно, допустим, один и куча вот таких пар. И значит, на выходе у нас будет, что у нас будет
на выходе, у нас будет куча строк, группа ID равно один. И также вот эти поля.
Зависит же от join. В смысле, зависит от join? В смысле, какой join реализован, left join?
Ну, не важно, все равно будет такая структура, то есть все равно останется ID, и вот какие-то
здесь вещи. Left join, right join и inner join, он влияет только на количество записей,
которые у нас будут на выходе. Тут реализуется уже логика в Reducer,
логика join. Как мы напишем код, так он и будет. Я к тому, что в одном join у нас,
получается, будет просто один результат. Еще раз, не совсем понял. Но у нас же в одной таблице только
единственный элемент групп ID и номер. Получается, если при каком-то join у нас будет единственный.
Там скорее связано с тем, что у нас где-то будут нулы, то есть где-то может быть группа,
у которой нет студентов, или студент, у которого нет группы, и там будет null. И мы вот эти нулы выкидываем.
Хорошо, а есть еще такая оптимизация join, когда у нас датасет, один из датасетов маленький. Сейчас я
тоже покажу. Если у нас оба датасета большие, вот такие, то нам ничего не остается, кроме как их
тегать, потом сортировать и редюсить на Reducer. А если у нас один из датасетов маленький, и мы
можем целиком его считать в памяти, то тогда получится, что мы имеем кусочек датасета A и
весь датасет B, и уже здесь мы можем этот кусочек поджоинить. Потом здесь поджоинили, здесь поджоинили.
Получили такой частичный join, и редюсера у нас нету, но в принципе можно сделать редюсер,
который просто сделает union. Хотя union можно сделать и без редюсера, то есть просто записи
у нас пишутся в HDFS, и потом они все пишутся в одну папку, у нас по сути union делается автоматически.
Вопрос всегда ли можно реализовать вот такой mapsite join, точнее все ли типы join можно так
реализовать. Вот у нас есть один большой датасет, другой маленький, и можно ли здесь реализовать
write join, live join, или будут какие-то проблемы.
Давайте придумаем какой-нибудь пример. Есть датасет A, и у него пары типа 1,
2, на другом мапере 3, 4, на третьем мапере 5, 6, и датасет B у которого
1 value, например 3 value и 4 value. Да, Святослав написал, нельзя реализовать live join,
делается это большой датасет. Почему? Ответ правильный, но хочется услышать пояснение.
Да, только часть датасета из A будет joining с B, то есть что у нас получается? Если live join,
это значит, что весь A должен быть под joining с B, и где-то будут нулы.
То есть что у нас получается в случае с live join? У нас получается, вот мы например,
joining B и первый мапер. У нас получается такая штука. 1 value, например под joining value и оттуда,
дальше 2, а двух у нас нету.
Да, давайте идти лучше по вот этому датасету, то есть 1 мы нашли, дальше 3, а 3 получается value null,
потом 4 у нас снова value null, а дальше берем этот мапер, и тут у нас получается как раз 1 value null,
а 3 и 4 есть. Вот и получается, что у нас будут вот такие вот пары,
которые будут повторяться, где-то будут нулы, а где-то будет значение. И нам придется после вот
такого live join еще делать маперию задачу, которая будет эти пары искать и схлопывать.
Вот основная проблема в этом. Это кажется, что в live join просто нулы же не нужны,
эти хранительные. Наоборот это будет проблема в right join и full join, или я что-то не понимаю.
В full join тоже такая же проблема будет с нулами. Да, кажется, Саша прав, это не left join, а right join.
А, ну да, точно, это right join, вот такая проблема тут есть.
Ну с left join в принципе даже проще, у нас там нет этих нулов, и обрабатывать ничего не нужно будет.
Есть еще одна оптимизация, называется bucket side join. То есть здесь было понятно, датасет большой,
датасет маленький. А вот здесь датасет, конечно, маленький, но считать его в distributed cache
целиком мы не можем, он не настолько маленький. Поэтому мы его бьем по частям и считаем каждую
часть в distributed cache на разных маперах. Здесь первая часть, вторая и третья. А здесь тоже кусочки
датасета. И давайте посмотрим теперь, а какой тип join вот здесь нельзя реализовать.
Если у нас и там кусочки датасета, и там кусочки датасета.
А имеется в виду, что мы эти кусочки делаем одинаковым partition, то есть типа у нас
ключи падут из разных датасетов все равно в одного только мапера, или как?
Ключи из разных датасетов нет, пока нет, пока просто мы разбили датасет как-то по сплетам,
и тут как-то его посплетили.
Какие типы join здесь нельзя реализовать? На самом деле никакие. Таким способом ни один join
реализовать не получится, потому что у нас и в случае left join будут лишние,
и в случае right join будут лишние нулы. Ничего не получится, поэтому для чего же нам этот bucket
side join нужен? Вот как правильно сейчас кто-то из вас сказал, что нам нужно подготовить данные.
И вот если у нас будут ключи, которые есть в кусочке одинцев, падать с ключами,
которые есть в этом кусочке один, то тогда мы просто можем сделать маленькие join независимые
на каждом мапере, и потом сделать union. Но нужно заранее подготовить данные и знать,
как мы их разбиваем. Есть ли какие-то вопросы под join? Мне пока нужно минута,
чтобы найти еще презентацию. Насколько вообще корректно делать join мы предьюсом,
если в целом большие базы оптимизированы под то, чтобы делать join на огромных масштабах?
Большие базы, ты имеешь в виду всякие там HBA из вот этого все, да?
Вообще, ну я меньше опыта имел с ними, поэтому не знаю, но кажется, что люди ж продакшен не
используют там какие-нибудь позгрессы и все остальное. Как-то им приходится, наверное,
иногда join. Ну смотри, позгрессы, если мы говорим про одну машинку, то есть база,
которая работает на одном сервере, там такой проблемы вообще нет. То есть там есть свои
оптимизации, индексирование, и join на одной машинке сделать можно. А если у нас машинок несколько,
и мы хотим сделать какой-то распределенный join, то там будет вот такой метод использовать.
Ну зависимость от того, есть мапредьюс или нет, если это что-то поверх мапредьюса,
тот же HBase, про который я сказал, или тот же Hive, там будет использоваться вот такой вот
сложный метод. Но правда, в продакшене мы вот это все писать сами не будем, мы этого даже
видеть не будем. Мы напишем команду join, а она уже сама поставит теги, сделает мапредьюс,
и мы этого не увидим. Хорошо, есть какие-нибудь еще вопросы? Пока я ищу презу.
Хорошо, если вопросов нет, тогда идем дальше, мы сегодня успеем как раз последнюю тему,
это планировка задач. Вот есть такая система yarn, мы про нее уже немного говорили на семинарах,
когда смотрели на вот эти вот application UI. В общем, очередь задач, и у нас получается много
пользователей, много нод на кластере, надо как-то организовать очередь так, чтобы задачи между
собой не конфликтовали. А тот yarn, который в Javascript, это другой yarn? Совсем другой,
это просто совпали аббревиатуры, ничего общего нет. Ну и как бы у нас постановка задачи вот такая,
у нас приходит одно приложение, потом приходит второе приложение, и нужно как-то решить вопрос
с ресурсами. Первое самое простое, что приходит в голову, это сделать очередь. То есть первое
приложение пришло, работает-работает-работает, пришло второе и оно ждет, пока первое не отработает.
Какие плюсы и минусы вот такого подхода, как вы думаете? Ну плюс, что просто реализовать,
минус в том, что вторая задача может на очень долго зависнуть, если первая там застакает,
или просто долго. Да, именно так. Реализовать очень просто, а у разных программ могут быть разные
требования, то есть вот этой второй программке может считаться одну минуту, а она будет ждать
два дня, пока посчитается первая, висеть в очередь. Вот поэтому второй вариант, это capacity
scheduler, и здесь у нас каждому приложению дается определенное capacity, то есть какое-то выделенное
количество ресурсов. Вот мы, например, знаем, что первое приложение у нас жирное, мы ему дали
75 процентов, а второму дали 25 процентов, и оно будет спокойно работать и никому не мешать.
Какие здесь плюсы и минусы? Не до конца железа тратим. Да, то есть у нас вот эти белые куски есть,
которые по сути это простой. То есть на самом деле вот такой capacity scheduler, он подходит для того,
когда у нас программа работает постоянно. То есть есть вот какой-то сервис, который мы запустили,
и он постоянно работает в реал тайме, отжирает 25 процентов ресурсов своих и все. То есть для
варианта он подходит. Сделать тоже несложно, но присвоил capacity каждому приложению и работаешь.
Вот, но минусы это то, что вот есть вот эти белые куски, которые простаивают. Ну и еще сказано,
что разные требования в разное время. Например, у нас может быть в задаче 90 блоков, значит будет
90 сплитов и будет 90 мапперов. Потом мы сделаем какой топ и после этого все погоним на сортировку
на один reducer. Например, такая задача. И у нас будет использоваться то 90 контейнеров, то один.
И получится, что нам нужно какое capacity сделать? Если capacity сделать маленькое, то что будет? У нас
будут вот эти 90 мапперов очень долго ждать. Например, capacity позволяет только 10 мапперов.
Как при этом будет работать задачка? Мапперы все будут работать, все в порядке, но только они будут
идти порциями по 10. Первые 10 отработали, закончили, вторые 10 и вот так будет вот такая локальная очередь
образовываться с мапперов. А если очень большое capacity поставим, да мапперы отработают быстро,
но зато на этапе reducer все будет остаивать. И третий планировщик, это так называемый честный
планировщик, он умеет динамически выделять ресурсы. То есть вот пришло первое приложение,
ну пока никого нет, бери все. Потом пришло второе и отбирает на себя часть ресурсов. Все, конечно,
хорошо и fairshedule, он самый такой, самый гибкий. Но вот если посмотреть именно на эту схему,
какие вот тут вы проблемы видите? Ну а как мы вообще возьмем посреди вышесрения отберем ресурс?
Вот, именно так. То есть то, что у нас есть в этой схеме, это значит, что у нас работало,
работало первое приложение, потом второе пришло и говорит, хочу ресурсов. Планировщик
берет и убивает половину мапперов. То, что они до этого момента работали и отработали почти там
на 90 процентов, это никого не волнует и мы просто убьем. И по сути это все равно, что если бы было
вот так, белое. То есть все равно, вычисления, которые были здесь, их прибили, результата нет,
и они получаются бесполезные. Что в этом случае можно сделать? Можно использовать вот такой подход.
То есть мы выделяем ресурсы не сразу, а вот такими ступеньками. То есть мы не убиваем мапперы,
а мы просто ждем, отработал один маппер, значит все, этой задачей ресурсы больше не даем,
отдаем другой. Отработал следующий маппер, опять забрали, опять забрали. Именно так у нас сделано
на кластере. Вы тоже можете увидеть, что вот у нас запустили какое-то приложение и будет написано,
сначала она использует там 10 контейнеров, потом 8, 7, 6 и так далее, меньше и меньше,
особенно если на кластере живет много пользователей.
Вот хорошо. Какие плюсы и минусы у этого подхода? Они тоже есть.
Плюс понятно, что гибкое планирование, а вот какие минусы тут есть?
Может быть какая-нибудь гранулярность ресурсов, то есть у нас как-то из-за маленького
количества ресурсов, но не очень. Мы не сможем всю джобу сразу запустить,
но не джобу, таску сегодня сможем запустить, к которому хотим.
Всю таску? Ну да, мы сможем запустить ее частями, то есть мы будем постепенно выделять ресурсы,
но по-другому никак не сделаешь. То есть да, постепенно, но это лучше, чем сразу прибить пол
таски или сидеть в очереди и долго ждать. По поводу гранулярности, то есть yarn работает в
терминах контейнеров, то есть вот эти все ступенечки это работа с контейнерами. Что
такое контейнер? Это некая такая абстракция, на которую выделяется, например, одно ядро
процессора и один гига оперативки, или там два ядра процессора, два гига оперативки. То есть
вот все ресурсы, которые есть у нас на кластере, они бьются на такие контейнеры в терминах yarn,
и yarn работает именно с ними. Размер ресурсов на самом контейнере, он тоже может меняться.
Например, один маппер работает в одном контейнере, один рьюсер работает в одном контейнере. Если у
нас в маппере такой код, что ему в процессе работы нужно чуть больше ресурсов, он будет до
какого-то момента эти ресурсы давать в один контейнер. Если этот один контейнер хочет очень
жирный ресурсов, то он его просто прибьет. Мы такое будем встречать в Spark. Очень часто встречается,
что вот у нас контейнер, в контейнере работает какое-нибудь преобразование над Spark'овским датасетом,
и мы в процессе преобразования просим еще ресурсы, просим-просим, нам всего не хватает,
и yarn в какой-то момент говорит контейнеру, ты слишком много попросил ресурсов, я тебя убью.
Убивает контейнер, рестартует, ну а после рестарта мы же запускаем на этом контейнере все равно один и тот же код,
поэтому этот код снова начинает просить ресурсы, снова жирнет-жирнет, его снова убивают. Ну и так
происходит несколько раз, пока не упадет все приложение. Такой подход есть. Ну и вот что еще
из этого следует, раз у нас четко так разбито по контейнерам, то непонятно, что делать вот в таком
случае, что ресурсы бывают разные. Например, у нас какой-то маппер хочет много процессоров и мало
памяти, потом мы переходим на reduce, а там работает по-другому. Много памяти, мало процессор. То есть
получается, что мы пробили данные на контейнеры, но в контейнерах у нас все время что-нибудь будет
простаивать. Или процессоры, или память. А есть вот еще, в одной из новых версий Hadoop появились
еще видеокарты, то есть еще один такой ресурс, и нам надо как-то между этим балансировать. Как
это сделать? На самом деле тут уже идет ручная настройка. То есть мы говорим, что на маппер мы
выделяем контейнеры с таким количеством процессовой памяти на редьюсере, вот с таким количеством. Для
этого есть специальные настройки, то есть там можно указать map minimum bytes, reduce максимум
ram. Вот все это можно указать и вот сделать такую тонкую настройку для контейнеров.
Вот есть ли какие-нибудь вопросы еще?
Мы можем как-то динамически писать приложения, которые пытаются там, допустим, выделить память
и выделяют ее до тех пор, как это возможно. А как только операционная система или контейнер ответила,
что все хватит, она использует то, что ей дали? Да, так можно. То есть мы же знаем, до какого предела
наш контейнер может расти, и мы можем на этот предел повесить проверку.
Окей, есть ли какие-нибудь еще вопросы?

величины вели с вами дискретные непрерывные случайные величины и собственно дальше нам
надо бы рассмотреть примеры этих случайных величин но чуть-чуть притормозим поскольку я забыл
вести еще одно понятие когда мы вводили функцию распределения случайной величины напомню да
функция распределения случайной величины кси в точке x это вероятностная мера омега таких
что кси отомина меньше х ну или более краткой вероятность того что кси меньше х вот значит
ввели функцию распределения случайной величины и совершенно аналогично мы можем ввести ну не
аналогично а следует теории можем ввести условную функцию распределения и в кси от x при условии
что омега принадлежит некоторому множеству сигма алгебры вот так ну множеству а которая
в свою очередь принадлежит сигма алгебре значит что это опять же по определению это вероятностная
мера омега таких что кси отоминга меньше х при условии что омега принадлежит а ну и как полагается
для там условных вероятностей мы знаем чему это равно это равно вероятности того точнее говоря
вероятности пересечения таких множеств кси отоминга меньше х одно множество в исходном
пространстве пересечение с множеством омега принадлежит а и поделить это на вероятность того
что омега принадлежит а то есть ну такое естественное обобщение если может так выразиться
поскольку условная вероятность это в общем обычная вероятность со всеми свойствами вероятности то
и условная функция распределения это тоже как бы обычная функция распределения тоже со всеми
свойствами но в качестве во первых примера а во вторых движение так сказать к нашей цели давайте
рассмотрим условную функцию распределения случайной величины кси при условии что случайная
величина это принадлежит некоторому множеству b это уже из барельевской сигма алгебры то есть
из пространства где значение у нас случайно влечено в принципе разумеется это все можно
переписать и в терминах исходного вероятностного пространства но я этот случай выделяю отдельно
но как бы одна из основных идей введения случайных величин состоит в том что мы
хотим отказаться от специфики исходного пространства мы хотим работать с числами или как бы
с числами со случайными величинами поэтому чем быстрее мы сможем отвязаться от исходного
вероятностного пространства тем ну по большому счету лучше для нашего дальнейшего изложения patterns
это вероятность того, что кси меньше х при условии, что другая случайная величина
принадлежит некому барелийскому множеству. Ну и как бы понятно, что это такое
значит дальше. Ну, например, еще один пример, еще более частный случай, да, кси х при
условии, что это меньше у, то есть вот это множество b, это вот от минуса бесконечности до у.
Ну тогда понятно, что это такое, это f, точнее говоря, вероятность того, что кси меньше х и
одновременно с этим это меньше у, делит на вероятность того, что это это меньше у.
Если, например, у нас это дискретная случайная величина, то, а ну да, понятное дело, что условная
функция распределения определяна только тогда, когда вероятность номера вот этого множества не равна
нулю, ну как и условная вероятность. Вот если, например, это дискретная случайная величина и
вероятность того, что это строго равно у, не равна нулю, то можно получить условную функцию
распределения, случайно кси в точке х при условии, что это в точности равна у. Тоже будет
корректная запись. Ну, конечно, посещает мысль рассмотреть ситуацию, когда это, то есть условие,
равно какому-то числу у как некий предельный переход. Но здесь единственно надо его корректно
описать, этот переход, потому что как бы ни получилось, что он будет зависеть от типа, от как бы
траектории стремления. Там, где это возможно сделать, мы воспользуемся чуть попозже, а пока вот просто как
понятие условная функция распределения, условная функция распределения. Вот, ну теперь давайте вернемся
к случайным величинам. Наконец, какие-то из них просто, что называется, посмотрим на них. Итак,
мы с вами ввели два класса случайных величин, непрерывные и дискретные. Ну, давайте начнем с
дискретной случайной величины, которая называется уассоновская случайная величина, зависит от одного
параметра лямбда. Эта случайная величина задается вероятностью конкретных значений своих и выглядит
так. Вероятность того, что, вот так вот сокращённо по, от лямбда будет равно к, есть лямбда в степени к
делит на к факториал на е в степени минус лямбда. Почему я начал с этой случайной величины? Потому,
что она вам знакома, правда? Это количество событий за единицу времени в поассоновском потоке,
в том специальной, в той специальной вероятностной мере, которую мы с вами вводили. Вот,
поассоновская случайная величина. Ну, строго говоря, я как бы постулировал, что случайная
величина определяется своей функцией распределения. А, ну, ещё-то здесь надо указать, чему к равно. К
равно 0, 1, 2 и так далее. Вот, ну, конечно, если имеете дело вот с такими вероятностями, то функцию
распределения в точке x нетрудно записать. Это сумма вероятностей того, что поассоновская
случайная величина равна k, когда k такое, что k строго меньше x. Правильно, да? Вот так
перейти от вероятностей каждого конкретного значения к функции распределения. Следующая
случайная величина, с которой, ну, может быть, надо начинать, но мы начали с поассоновской,
потому что мы с ней уже столкнулись. Это так называемая Бернулевская случайная величина,
B от P. Это дискретная случайная величина, принимающая два значения. Она принимает значение 1 с вероятностью
P малое и 0 с вероятностью q равна единица минус P. Но на самом деле мы с этой случайной величиной
тоже сталкивались. Какая случайная величина является по сути дела Бернулевской? Монетка. Да,
монетка. Но монетка это всё-таки частный случай, потому что P равно 1 и 2. У нас она была прям такая,
мы что-то с ней пробовали делать, умножали, складывали. Что за случайная величина?
Характеристическая функция множества. Напомню, это случайная величина, которая равна единице,
когда Омега принадлежит некому множеству А и нулю, если не принадлежит множеству А. Понятное дело,
что все характеристические функции множества принадлежат классу Бернулевских случайных величин.
Ну для Бернульской случайной величины, наверное, будет не лишнее нарисовать функцию распределения.
Мы это или делали, или почти делали, но тем не менее, как сказать, повторим. Это дискретная случайная
величина. Она принимает два значения 0 и 1. И здесь 1, 0 и 1. И для дискретной случайной величины,
которая имеет скачки в тех точках, значения которых она принимает, чему величина скачка равна?
Вероятности принятия значения. Вероятность принятия значения 0 равна Q. Значит, здесь будет скачок Q.
Она у нас непрерывно слева. И второй скачок P в точке 1.
Сверху? Не пойму вопроса. А, вот здесь, да-да-да, совершенно верно.
Так, значит, вот функции распределения Бернульской случайной величины. Ну, кстати,
еще одно свойство функции распределения. Мы знаем, что плюс бесконечности она равна единице,
а это значит, что сумма всех вероятностей тоже должна равняться единице. Но обращаю ваше внимание,
что если вот эту величину просуммировать про всем k, от 0 до бесконечности получится единица. Здесь,
собственно, два значения нужно просуммировать. P и Q тоже единица. И третья случайная величина,
которую мы с вами введем, дискретная, мы сейчас пока, как видите, по дискретным случайным величинам,
это так называемая биномиальная случайная величина. Биномиальная случайная величина,
только, извините, у нее уже два параметра, n и p. И задается она тоже своими вероятностями.
Вероятность того, что биномиальная случайная величина будет равна k, равно c из n по k,
p в степени k, u в степени n минус k. k принимает значение 0, 1, так далее n. То есть n плюс одно
значение. n плюс одно значение. Биномиальная случайная величина. Можете предложить какой-нибудь
мысленный эксперимент, результатом которого будет биномиальная случайная величина? Ну что вы к
монетке все. Монетка p равно 1 и 2, в частный случай. Ну пусть даже с монеткой. Ну хорошо,
откровенно скажу, не уловил вашу мысль. Значит, ну что тут можно заметить прежде всего, что если
по всем k вот эти вероятности просуммировать, то это получится p плюс q в степени n. Понятно,
что это должна равняться единице, ну значит соответственно q равно единица минус p, чего как бы
и требовалось. Значит, вот эта случайная величина, биномиальная случайная величина, называется схемой
Бернули независимых испытаний, n независимых испытаний. Значит, какой мысленный эксперимент,
так сказать, приводит к возникновению этой случайной величины? Мы проводим независимые
эксперименты с неким, определим некое событие как успех. Там в конкретной задаче успех, это может
быть совсем не успех с точки зрения здравого смысла, но это название. Есть некий эксперимент,
который с вероятностью p приводит к успеху и с вероятностью q приводит к неудаче. Ну,
например, бросаем кубик, успехом называем выпадение шестёрки, тогда p равно 1 шестой и q равно
5 шестых. Биномиальная случайная величина, это количество успешных опытов в серии из n,
из вот этого n, который параметр испытаний. Понятно? То есть мы проводим некий эксперимент,
можно даже бросать монеты, просто если мы бросаем монету, p равно q равно 1 и 2. Значит,
мы проводим некий эксперимент n раз, успехом мы считаем выпадение решки. И вот биномиальная
случайная величина, это такая, сколько решек у нас выпадет в серии из n испытаний. Понятно,
почему это вот так описывается? Откуда это берется? Ну, каждый успех с вероятностью p,
каждый неудач с вероятностью q, поэтому в серии испытаний, где k успехов и n-k неудач,
вероятность такой серии p в степени k на q в степени n-k и еще на разных местах этот может
произойти успех. То есть мы должны еще выбрать места для успеха. Вот такая вероятность соответствует
эксперименту проведения независимых испытаний с вероятностью успеха p в единичном испытании и
проводим n экспериментов. И случайной величиной является количество успехов в серии из n испытаний.
Вот это биномиальная случайная величина. Скажите, пожалуйста, как вам кажется,
биномиальная с биномиальной как-нибудь связана? То есть, видимо, вы хотите сказать,
что биномиальная с параметрами np это сумма биномиальных в количестве n штук. Правильно
я понял ваше мнение. Да, выглядит вполне здраво и соответствует логике. То есть что такое
Бернульские случайными величины? Мы проводим эксперимент с вероятностью успеха p, получаем
единицу, если успех. Потом мы проводим второй эксперимент, опять получаем единицу или ноль.
Если мы их проведем n штук и сложим эти случайные величины, то мы получим, естественно,
количество удачных экспериментов в серии из n испытаний. И в этом смысле можно сказать,
что биномиальная случайная величина с параметрами np равна сумме бернульских индекс g,
g равно от единицы до n. Вот так. Но это разные эксперименты. То есть если считать, что это
подбрасывание монеты, то каждое подбрасывание монеты это бернульское случайная величина. Но
когда мы ее подбрасываем n раз, мы получаем n бернульские случайные величины. Но вопрос не лишен
смысла так глобально, потому что вот это равенство можно понимать в разных смыслах. И мы пока
остановимся на том, что будем считать это равенство по распределению. То есть распределение суммы
бернульских случайных величин совпадает с биномиальным распределением. Вот по такой формуле.
Строго говоря, это равенство можно понимать по-разному. Например, давайте возьмем две
бернульские случайные величины, пусть бросание монеты, и первое, определимое в ратностном
пространстве орел-решка. Первая случайная величина равна единице, когда выпал орел, и ноль,
когда решка. А вторая случайная величина равна единице, когда выпала решка, и ноль, если выпал орел.
Эти две величины и та и другая бернульские. Но будет ли их сумма биномиальна? Нет, потому что их
сумма всегда равна единице. А биномиальная с двумя испытаниями, это значит, может значение равняться
0, 1 или 2. Поэтому это равенство нужно понимать, как бы обходиться с ним осторожно, но для первого
знакомства да. Вот в этом смысле оно вполне корректно. То есть функция распределения суммы
бернульских случайных величин, которые явились результатом независимых экспериментов, действительно
имеет то же распределение, что и биномиальная случайная величина. Следующая дискретная
случайная величина, с которой мы сегодня познакомимся, это так называемая геометрическая.
Гео p тоже зависит от одного параметра. Тоже задается вероятностью своего значения. Вероятность того,
что гео от p равно k, равно q в степени k умножить на p. k принимает значение 0, 1, 2 и так далее.
Но то, что формально это такие вероятности задают функцию распределения, понятно,
потому что сумма q в степени k на p по всем k от 0 до бесконечности даст нам единицу.
Скажите, пожалуйста, какому эксперименту соответствует вот эта случайная величина?
Как мне в эксперименте в мысленном получить случайную величину вот с таким распределением?
Какие-нибудь идеи есть? Ну хоть с монеткой. Монетка, кубик, как угодно.
Но если несколько раз подряд выпало, это значит, если условно говоря успех,
то это p в степени k. Если не удача, q в степени k. Ну да, это как бы формальная
интерпретация вот этой формулы, но уже из этого можно построить, так сказать, мысленный
эксперимент. Мы проводим эксперимент до первого успеха. И вот количество неудачных опытов,
которые мы сделаем до первого успеха, это и есть случайная величина, имеющая геометрическое
распределение. Понятно? Случайная величина, имеющая геометрическое распределение,
это количество неудачных опытов до первого успеха. Просто для, так сказать, информации
иногда вот эту случайную величину вводят чуть-чуть по-другому. q в степени k минус 1p и k равно
тогда, естественно, 1, 2 и так далее. Тогда это не количество неудачных опытов до первого успеха,
а количество экспериментов, включая первый успех. Это ничего не... просто можно и так.
Можно ввести и так. Но смысл именно в этом. Такой базовый эксперимент — это проведение каких-то
экспериментов до первого успеха. Количество вот этих неудачных шагов или количество плюс один
этих неудачных шагов — это вот и есть случайная величина, имеющая геометрическое распределение.
Следующая случайная величина — н-бене, но на самом деле это не н-бене, это отрицательно
бинемиальное распределение с двумя параметрами n и p. Задается тоже своими вероятностями. Вероятность
того, что отрицательно бинемиальная случайная величина будет равна k, значит это есть c из n
плюс k минус 1 по n минус 1, q в степени k, p в степени n. Значит k равно 0, 1, 2 и так далее.
Ну, я был бы рад, если бы вы мне сказали схему эксперимента, который приводит к такой случайной
величине. Хорошая идея, хорошая идея. Ну, значит прежде всего отметим, что сумма вот этих вот c из n
плюс k минус 1, n минус 1, q в степени k, p в степени n, сумма от нуля до бесконечности,
нетрудно убедиться, что это p делить на единица минус q в степени n, ну то есть единица.
Теперь интерпретация. Значит отрицательно бинемиальная случайная величина — это
количество неудачных опытов до n успеха. Количество неудачных опытов до n успеха. Вот,
так сказать, мысленный эксперимент, который приводит к этой случайной величине. Ну,
как здесь как бы схема-то строится. Значит нас интересуют такие последствия успехов неудач,
чтобы среди n плюс k минус 1 эксперимента был n минус 1 успех, а остальные неудачи. И потом
мы к этой последовательности на n плюс k это место приписываем p, то есть n успех, и наш
эксперимент прекращается мыслями. Ну вот, n плюс k минус 1, n минус 1 место для p, тогда q в
степени k, p в степени n минус 1 — это, собственно, вероятность появления цепочек, у которых n минус
1 успех и k неудач. И потом на последнем шаге делаем успешный эксперимент с вероятностью p,
поэтому это p в степени n минус 1 превращается p в степени n, и получается вот такая формула.
Понятно я объяснил? Ну и, собственно, аналогичное утверждение можно сделать. Отрицательно
биномеральная случайная величина с параметрами n и p в смысле распределения есть сумма геометрических
с параметром p в количестве n штук. Но это естественно. Если я беру там первую серию
испытаний до первого успеха, количество неудач — это геометрическая случайная величина. Потом
опять провожу эксперименты до второго успеха. Количество неудач от первого до второго успеха — это
тоже геометрическая случайная величина. Ну вот складываем их и получаем в конце концов количество
неудачных опытов до n успеха. Вот такие вот случайные величины. Так, сейчас соображу,
не забыл ли какую-нибудь важную из дискретных? Вроде нет. Здесь я хотел немножко пофилософствовать
и порассуждать, что такое случайная величина, потому что это важное понятие и не такое уж простое,
как кажется. С одной стороны, мы со случайными величинами ведёмся как с числами. Складываем,
умножаем их. Мы получили соответствующие результаты, что функция от случайной величины — случайная
величина, то есть измеримое отображение и так далее. Но в то же время это не числа. А что же
это такое? Что такое случайная величина? То есть мы с ними ведёмся как с числами, но на самом деле,
но очевидно, что это не числа. Это нечто другое. Как-то надо по-другому это описать. Вот то, что это
такая нетривиальная штука. Можно в качестве примера привести этот известный, как это назвать,
парадокс или байку про кота Шреггенгера. Знаете, да, вы в форситетической физике? Знаете про такой
эксперимент? Ну, на самом деле, никакого там парадокса нет. Это всё раздули журналисты. Значит,
так описали, да, суть эксперимента и придумали там какие-то страшных котов, которые наполовину мёртв,
наполовину жив. На самом деле это как бы... Все знают эту схему эксперимента мысленного,
да, не надо рассказывать. Вот, значит, на самом деле вот как раз вот этот псевдопарадокс возник от того,
что люди, которые это описывали, журналисты, не понимали, что такое случайная величина.
Значит, пока случайная величина... Вот пока мы не измерили случайную величину или не получили её
реализации, её нет ни в каком виде. То есть ни мёртвого кота нет, ни живого нет, его просто нет.
У нас есть случайная величина, такой абстрактный объект, а как только мы, опять же, так сказать,
наш мысленный эксперимент, заглянем в этот ящик, мы поймём, проведём наблюдение, наша случайная
величина получит какую-то реализацию. А в этом, в эксперименте с котов Шреднегера, их всего две.
Кот жив, кот мёртв. Это во-первых. Ну и во-вторых, что так, ну, отмечу, что никакого отношения к квантовым
эффектам этот эксперимент не имел, потому что вместо, значит, распада частиц, которые запускали
механизм умерщеления этого кота, можно было подбросить монету в микромирии, так сказать,
в квантовые эффекты. Решка, грубо говоря, кота умертвили, прошу прощения, за такие
живодёргские посылы. Если нет, коту повезло, он остался жив. Но ещё раз скажу, что мы с вами
рассуждаем в перименах мысленных экспериментов, поэтому, конечно, никакие животные у нас, значит,
не пострадали. Но, тем не менее, от непонимания, что такое случайная величина, возникает большое
количество недоразумений в быту. И основная здесь ошибка, ещё раз повторюсь, это сродник квантовым
эффектам, где находится электрон, описаный псифункцией, где угодно. И как только мы там,
ну, тем или иным способом его зафиксируем, вот тогда мы сможем сказать, где находится электрон. Правда,
с точностью до принципа неопределённости, тем не менее. Вот, значит, ну, тем не менее, так сказать,
более-менее можем сказать, где находится электрон. Пока мы этого не сделали, он везде и нигде. Это там,
ну, в терминах физики, волна. Вот, то есть, вот такая вот случайность, ну, как бы некое проявление
случайности, это случайная величина. Случайная величина ничему не равна. Возвращаясь к эту
Шрёдингера, он не... это не те термины, в которых можно рассуждать, жив он или мёртв. Случайная
величина может проявляться в своих реализациях, но она ничему не равна конкретно, потому что
случайная величина, ну, как бы одна из интерпретаций, это механизм генерации реализации,
механизм. Вот какой механизм такая случайная величина? Грубо говоря, случайная величина
определяется своим механизмом, но сама по себе она не является ни числом, ни там приводит систему
в какое-то состояние и так далее. Вот, я ещё раз скажу, что это очень непростая вещь, потому что,
например, даже Эйнштейн так, вообще говоря, до конца и не принял вот эту как бы идею,
такое некого вероятностного устройства мира. Ну, вы знаете его знаменитое выражение. Старик
не играет в кости, да, то есть там господь-бог, ну, как бы, там это всё задаёт детерминированно.
Если мы что-то не можем точно измерить, то это не следствие устройства мира, а просто там
недостаток нашего как бы эксперимента. И, собственно, человек, который там, ну, разумеется там,
как это, безусловный гений, ну, вот с этим, с этими вещами так и не смог до конца смириться. Вот,
поэтому это такое очень-очень непростое понятие случайная величина. Вот, если так уж думаться. Ну,
ещё одну, ещё приведу один пример тоже как бы с понятием случайной величины или со случайной
величиной. Значит, как вы знаете, если электроны пропускать через маленькую дырочку, то возникает
интерфюкционная картина, круги такие возникают. Ну, физики это говорят или там объясняют, или уже
может и не так объясняют. Ну, как бы такое для школьников старших классов в нынешнее время,
то есть когда-то это были нобелевские премии, а теперь это вот школьники старших классов, им как
бы можно объяснить, что электрон, когда он проходит через это отверстие, он проявляет
свойство волны. Вот как волны дают интерфюкционную картину, так и электрон дает, потому что он вот в
этом случае проявляет себя как волна. Ну и как бы так можно рассуждать, не то что можно, это там,
ну один из принципов основных так сказать, это дуализм, да, волна, частица. Ну, то есть это такая
ну фундаментальная концепция. Но давайте мы на это взглянем с точки зрения нашего предмета,
наших понятий, случайных величин. Вот, значит, где-то там, не знаю, в середине прошлого века
группой советских физиков, там лабораторий, там этот фабрикант, такой физик был у нас, я в школе
учебники физики, так сказать, его авторстве, так сказать, там, ну там использовал. Вот, значит,
был поставлен такой эксперимент. Значит, эксперимент как бы понятный, опять же это дырочка там
в свинцовой пластине пучок электронов и получается интерфюкционная картина. Но, значит, на этот раз
чуть-чуть изменили схему эксперимента, не пучок электронов, а по одному, по одному электрону пускали.
Причем время между, ну там от первого до второго и так далее, там в десятки раз,
превышало время, пока этот электрон фиксировался на фотопластинке. То есть каждый отдельный
электрон оставлял на этой фотопластинке точку, никакой интерфюкционной картины, просто точка,
как из краника вода капает, одна точка, вторая точка, третья точка. Ну и когда так с этих достаточно
много там пустили, получилась та же самая круговыми кольцами картина. С точки зрения нашей науки,
нашего подхода, нашего предмета, который мы изучаем, что мы можем сказать? Мы можем сказать,
что функция плотности, ну вот так вот, если вот это взять там у нас там, получается какая-то вот такая
картинка светлая-темная. Ну если взять радиус и вдоль радиуса посмотреть, то можно сказать,
что мы имеем дело со случайной влечиной, ну где-то вот с такой функцией плотности примерно.
То есть это немыслимый эксперимент, это реальный эксперимент. Каждый электрон реализовал какое-то
значение. Один попадал сюда, другой сюда, и вот в эту окрестность их попадало гораздо больше,
чем вот в эту окрестность. Мы не будем задаться вопросом почему, а просто скажем, что до того,
пока мы эти электроны не выпустили, они не обладали никакими свойствами и никакой не давали
картина. Но как только мы этот эксперимент провели, мы получили большое количество, как говорят,
реализации. То есть случайная влечина из функции, из механизма генерации, превратилась в число,
то есть реализация наступила. И вот эта вот реализация, вот она ведет себя вот таким образом.
А почему говорят об интерференции? Потому что, конечно, это очень похоже по картинке на интерференцию
волн световых. Заканчиваю философскую часть и как бы сделаю там вывод или подчеркну. Понятие
случайной величины совсем непростое. Это сложный объект, который в общем с трудом даже там давался
очень умным людям. Ну, со временем, время проходит как бы, то, что было когда-то там на передовом
краю становится бытовыми вещами, но я вам призываю вас не упрощать это понятие, понятие случайной
величины. Это не такой уж простой объект, как может быть, так сказать, может показаться вот на этих
примерах. Ну, теперь давайте рассмотрим примеры непрерывных случайных величин. Ну, и такая, в общем-то,
нам в целом знакомая, точнее говоря, сейчас вы ее узнаете, надеюсь. Это равномерная на АВ случайная
величина. Ее давайте для разнообразия зададим функцией распределения. АВ единица. Вот так выглядит
эта функция распределения этой случайной величины. И что мы отсюда видим, и какой вывод сразу
можем сделать. Вероятность того, что равномерно распределенная на АВ случайная величина попадет
в какой-то интервальщик, ну не знаю там, фиксированной длины, это вероятность одна и та же,
если этот интервальщик в АВ укладывается и не зависит от его местоположения. Это понятное
утверждение. Вероятность того, что случайная величина вот здесь попадет в небольшой
интервальщик, это вот она. И в любом другом месте, если такой же ширины интервальщик,
то будет та же самая вероятность. Что это нам напоминает? Какой мысленный эксперимент?
Мы с вами пользовались ну так, ну несколько раз, не один, может не многократно. Бросание точки
на удачу. То есть у нас был с вами такой как бы абстрактный эксперимент бросание точки на удачу.
То есть точка может попасть в любое место, никаких предпочтений нет, вероятность попадания в
какой-то интервал пропорциональна его длине. Ну собственно формализация вот этого мысленного
эксперимента, это случайная величина равномерно распределенная на АВ. Ну и собственно нетрудно
нарисовать плотность, поскольку это дискретная случайная величина, то непрерывная случайная
величина, то у нее плотность существует. Плотность выглядит вот так. Причем обращаю ваше внимание,
что чему эта функция равна в точках Х или В несущественно. Как бы мы ее не определили,
это будет множество нулевой меры, поэтому можно рассматривать случайную величину равномерно
на отрезке АВ, можно на интервале АВ, на полуинтервале АВ с точностью до множества нулевой меры. Это все одно
и то же. Если есть какая-то задача специфика, можно до определить, чему там в точках Х или В
равно, то ли нулю, то ли единица делить на В-А. Но с точки зрения функции плотности и там вероятности
событий это никакого значения не имеет. И это вообще характерное свойство непрерывных случайных
величин. Изменение на множестве меры ноль не меняет вероятности. Добавление множества меры ноль,
вычитание множества меры ноль и так далее. Вот первая случайная величина имеющая непрерывное
распределение равномерное. Вторая случайная величина, вам тоже в общем-то знакома, она называется
нормальной случайной величиной и зависит от двух параметров, m и σ квадрат. И задается своей,
традиционно задается своей функцией плотности. fnm σ квадрат от х это единица делить на σ корень
из 2p е в степени минус х минус m в квадрате делить на 2 σ квадрат. Знакомая функция? Ну,
я думаю, что совсем вы ее узнаете, если мы рассмотрим так называемое стандартное нормальное
распределение с параметрами 0,1, тогда плотность примет вид единицы делить на корень из 2p е в степени
минус х квадрат пополам. Так называемый интеграл ошибок, да, гауссова кривая. Да, действительно,
гауссова кривая это частный случай нормального распределения с параметрами 0,1. Ну, наверное,
возникает вопрос, какой содержательный смысл вот этих параметров m и σ квадрат.
Чтобы на него ответить, давайте рассмотрим случайную величину это, которая равна некому
a умножить на n 0,1 и плюс b. Ну, единственное необременительное требование a больше 0,
а больше 0, b произвольное. Ну и давайте попробуем функцию распределения, функцию плотности найти,
вот такой случайный, новый случайный величины. Значит, вероятность того, что это меньше х,
это есть вероятность того, что, ну, я сразу действие произведу, n 0,1 меньше, чем х минус b делить на a.
Правильно, да? Простейшие действия произвел. Вот это по определению, напоминаю, функция распределения
случайной величины это в точке х. И чтобы получить плотность этой случайной величины, нужно
продиференцировать функцию распределения. Если мы продиференцируем, мы получим f малая,
то есть плотность случайной величины это в точке х будет равна. А вот это, как мы видим,
функция распределения стандартной нормальной случайной величины только взятой в точке х
минус b делить на a. Поэтому это будет, во-первых, f n 0,1 в точке х минус b делить на a,
но еще надо продиференцировать вот это выражение, это единица на а.
Ну и давайте подставим вот сюда значение этой функции. Возьмем в точке х минус b делить
на а, ну и не забудем про единицу на а. Получим единица делить на а корень из 2p,
е в степени минус х минус b в квадрате делить на 2a квадрат. Правильно, да? То есть функция распределения,
вот такой вот линейной функции от стандартного нормального распределения, на самом деле это
тоже нормальное распределение, то есть вот при линейном преобразовании нормальность,
так сказать, сохраняется, и нам становится понятен смысл параметров m и sigma. m,
который у нас здесь буквой b обозначен, это, ну грубо говоря, параметр сдвига,
а корень sigma квадрат, тубиш sigma, это параметр масштаба a. Вот содержательный смысл этих
параметров. Как только мы a умножили на стандарт нормальную учину, у нас это a попало вот сюда и
сюда, то есть в точности как вот здесь sigma. То есть, значит, во-первых, при линейном преобразовании
нормальная величина остается нормальной и случайной величиной, и во-вторых, вот пока мы как бы потом
более точный смысл придадим, но тем не менее, пока так содержательно, параметр m это параметр сдвига,
а sigma это параметр масштаба относительно гауссовой этой вот случайной величины стандартной нормальной.
Да.
Даже не скажу вам. Я немецкий не знаю, но может быть это какой-нибудь рэндом,
ну могу только гипотезу высказывать, не знаю. Вот, значит, равномерная и нормальная случайная
величина. Ну и давайте еще введем тоже такую важную непрерывную случайную величину, даже
можно сказать целый класс. Это так называемая гамма-распределение. Гамма-распределение зависит
от двух параметров альфа и лямда и задается своей плотностью. f гамма от x равно лямда в степени
альфа делить на гамма функции от альфа x в степени альфа минус 1, e в степени минус лямда x. Значит,
x альфа и лямда все больше нуля. Но x, правда, можно больше или равно, несущественно, но вот так вот.
Ну, видимо, с такой функцией вы не сталкивались, да? Нигде, вроде, не могли столкнуться. Но что
такое гамма-функция, знаете, да? Вот, значит, вот таким образом получается у нас распределение. Если вы
проинтегрируете это от нуля до бесконечности, ну гамма альфа вынесите, ну, собственно, у вас
как раз и получится гамма-функция от альфа и интеграл от плотности равен единице, как это должно быть.
Значит, у вот этого распределения, семейства распределений есть там пару важных частных
случаев. Значит, первый частный случай это когда альфа равно единице, альфа равно единице. Тогда
функция плотности принимает вид лямбда на е в степени минус лямбда х. Ничего не напоминает вам?
Какое? Экспоненциальное. А вы этот термин откуда взяли? А, в семинарских занятиях было. Понятно.
Значит, ну давайте вот что сделаем, чтобы как узнать эту случайную величину. Давайте
напишем ее функцию распределения. Чтобы написать ее функцию распределения, мы должны ее проинтегрировать.
Ну, если мы это с учетом граничных условий сделаем, то мы получим, что функция распределения равна
единице минус е в степени лямбда х. Правильно, да? Напоминаю, что функция распределения это равно
вероятность того, что наша случайная величина, так ее обозначу Т-большое случайную величину,
меньше х. И вот эта вероятность равна вот такой величине. А тогда, соответственно,
вероятность того, что Т больше х, ну равно здесь не существенно, вот ввиду того,
что плотность существует. Естественно, есть е в степени минус лямбда х. Опять не узнаете?
Ну нет, Пуассон... или совсем не Пуассон даже сказал. Но тем не менее, когда мы с вами получали
вероятностную меру для Пуассоновского, для простейшего потока, мы с вами там получили
вероятность того, что за время Т не произошло ни одного события. Или в терминах случайных
величин, если Т это случайная величина соответствующая времени до события, то
событие Т больше х как раз означает, что за время от 0 до х не произошло ни одного события. И эта
вероятность как раз была равна, если вы помните, е в степени минус лямбда х. Помните, да? То есть это,
в принципе, тоже знакомая нам случайная величина, просто мы ее сразу не узнали, потому что надо было
ее так привести к какому-то виду. Но тем не менее, этот частный случай называется или задает
действительно показательное или экспоненциальное распределение. То есть случайная величина вот
с такой функцией распределения и вот с такой плотностью называется показательной или экспоненциальной.
Ну, обозначают, например, вот так, эксп от лямбда. Экспоненциальная случайная величина с параметром
лямбда. Видим, что она зависит от одного параметра. Это значит первый частный случай и второй важный
частный случай. Это когда α равно n пополам, а лямбда равно 1 вторая. Случайная величина с
такими параметрами, плотность которой есть 1 вторая в степени n пополам, делить на гамма от n пополам
на х, х в степени n пополам минус 1, e в степени минус х пополам. Вот с такой довольно сложной функцией
плотности. Но это всего лишь на всего частный случай гамма распределения. Называется х квадрат
случайной величиной с n степенями свободы или n еще просто индексом пишут иногда. То есть случайная
величина х квадрат с n степенями свободы, это гамма распределенная случайная величина с параметрами
α равно n пополам и лямбда равно 1 вторая. Эта случайная величина очень важное значение имеет
статистике, поэтому это случай хоть и частный, но очень важный. Итак, помимо самого семейства гамма
распределенных случайных величин мы выделили два частных случая. Показательное распределение и х квадрат
с n степенями свободы распределения.
Ну, анонсирую.
Не будем времени терять. Стандартная нормальная случайная величина в квадрате, оказывается, равна
по распределению х квадрат с одной степенью свободы. Ну, мы на семинаре это покажем. Ну, это одна из
связей, так сказать. Ну, и еще одна случайная величина непрерывная, это вот такая случайная
величина E в степени nm sigma квадрат. То есть, экспонента в степени нормальная случайная величина.
Эта случайная величина называется лог нормальный. Ну, естественно, зависит от двух параметров m и sigma
квадрат. Ну, и в данном случае мы ее задали, как вы видите, не функцией распределения или функции
плотности, а функциональным видом. То есть, мы знаем, что E в степени случайная величина, это тоже
случайная величина. Ну, теперь, значит, нам надо функцию плотности или функцию распределения
получить. Давайте это сделаем. Значит, f лог n в точке х это вероятность того, что E в степени nm
sigma квадрат будет меньше х. Значит, х больше нуля по понятным причинам, поэтому эта вероятность равна
вероятности nm sigma квадрат меньше логарифм х. Ну, а это не что иное, как функция распределения nm sigma
квадрат, взятая в точке логарифм х. Ну, и, соответственно, плотность f лог n в точке х,
это будет единица на х sigma, корень из двух π, E в степени минус логарифм х минус m квадрате
делить на 2 sigma квадрата. Вот функция плотности лог нормальной случайной величины. Ну,
помимо того, что вам просто нужно знать эту случайную величину, она там некоторыми свойствами
обладает полезными. Ну, например, произведение лог нормальной случайных величин лог нормальная
случайная величина, но еще здесь обратите внимание на как бы методику способ, каким я ее ввел. Я ее
описал как функцию от известной случайной величины и получил вот такой нехитрой техникой, получил ее
функцию распределения через нормальную и явно выписал функцию плотности этой случайной величины.
Вот так. Значит, это не все случайные величины, с которыми мы с вами познакомимся, но пока давайте
на этом остановимся и двинемся чуть-чуть дальше содержательно.
Значит, напомню, что мы с вами
ввели такой объект, случайная величина, да? Xi это некая f от ω, которая элементу из ω ставит
соответствие числу. Это мы назвали случайной величиной. Ну и давайте по аналогии введем
случайный вектор. Xi вектор от ω равный Xi1 от ω, так далее Xin от ω. И это тоже отображение из ω в Rn.
У нас еще будет один объект, это из омега в последовательности, ну пока значит мы остановимся
на векторе. Итак, первый это случайная величина, второй это случайный вектор. На что обращаем
внимание в определении случайного вектора? Вот это омега одна и та же для всех, так сказать,
компонент. То есть каждому омега, малому, каждому исходу вот отсюда ставится в соответствие весь
вектор сразу, а не так, что одному омега 1 ставится в соответствие Xi1 от омега 1 и так далее. Нет,
определение именно такое. Каждому элементарному исходу ставится в соответствие вектор. Вот такой
получился у нас объект. Ну и по аналогии мы введем функцию распределения случайного вектора.
Это есть вероятность того, что одновременно омега такое, что ксикатая от омега меньше
хкт, к от единицы до n. Ну такая очевидная аналогия. Значит, на какой вопрос надо ответить?
Эта вероятность определена ли? Ну здесь ответ довольно очевиден. Поскольку каждый ксикатый это
измеримое отображение, то вот это множество принадлежит сигма-алгебре. Ну и естественно,
что их конечное пересечение тоже принадлежит сигма-алгебре. То есть вот это вот измеримый
элемент. То есть вероятность его есть. То есть функция распределения определена корректно.
Ну и давайте свойства этой функции. Первое свойство, ну пройдемся по свойствам,
которые были у функции распределения случайно влечены, и просто покажем, что есть аналогичные.
Значит, первая функция распределения, ну я так для краткости иногда буду и тут вектор тоже
писать x-аргумент. Меньше или равна единице, больше или равна нуля. Ну как и полагается любой
там вероятности. Следующее свойство или там у нас промежуточное свойство, из которого потом
выведем остальные. Значит, функция распределения x1, вот этого вектора нашего, xk-1,
b, xk, плюс 1 и так далее, xn равна f, x, x1, xk-1, a, xk, плюс 1, xn, плюс вероятность
того, что xg меньше xg при g не равном k, и одновременно с этим xk принадлежит полуинтервалу a, b.
Вот это понятно? Ну аналогичную формулу мы с вами получали для функции распределения
случайной величины. Понятно, нет? То есть значение функции распределения это значит,
что xk меньше b, мы разбиваем это на два непересекающихся интервала, xk меньше a,
xk принадлежит от a до b вот такому полуинтервалу. Берем вероятности, эти вероятности это функция
распределения. Значит, понятное свойство. Ну и что отсюда следует? Отсюда следует,
так, свойство аналогичное функции распределения, это свойство монотонности нестрогой, то есть f,
xi, x1, xk-1, xk', меньше или равно f, xi, x1, xk-1, xk', как только x1', меньше xk',
меньше xk', понятно, да? И второе, что из этой формулы следует, это покомпонентная непрерывность
слева. Понятно, да, как ее доказывают? А n устремляем b, к b, которая возрастает, так сказать,
там стремится к b, тогда пересечение всех таких множеств дает пустое множество, и мы получаем то,
что предел вот этой функции, когда a, n возрастает, сходится к b, равно, собственно, вот этой точке,
что и означает непрерывность слева. Пока все аналогично, функции распределения случайно
влечены. xk-1, xk-0, xn равно f, xi, x1, и так далее, xk, и так далее, xn.
Что у нас там еще было в функциях распределения случайно влечены? Свойство было такое, что когда
x стремится к плюс бесконечности, функция распределения стремится к единице, и когда
стремится к минус бесконечности, стремится к нулю, да? Ну, давайте для примера исследуем ситуацию,
когда функция распределения, у нас есть некая xk-1, xn, и посмотрим, куда это стремится,
когда xk-1 стремится к минус бесконечности. xk-1 стремится к минус бесконечности. Ну, в
эта функция распределения по определению это вероятность того, что xi gt меньше xg, g не равно k,
и xi gt меньше xg. Вот эта вероятность меньше или равна вероятности того, что xi gt меньше xg,
поэтому если мы xg стремим к минус бесконечности, то не будет ни одного элементарного исхода,
который будет принадлежать тому множеству, поэтому я тут как бы понятно, что не говорю,
но пользуюсь теоремы непрерывности вероятностей. Это п пустое множество равно нулю. Ну и можно
доказать аналогично, а можно там разными способами. Второе свойство аналогичная функция распределения
случайной величины, что f xi x1 и так далее xn стремится к единице, если для любого g и gt стремится
к плюс бесконечности. То есть для случайной величины там у нас была одна переменная,
она стремилась к плюс бесконечности и функция распределения стремилась к единице, а здесь надо,
чтобы все стремились. Если все стремятся к единице, то есть аналогичное свойство,
но там чуть-чуть отличается. Все стремятся к единице, то и функция распределения стремится к единице.
Ну собственно все, что по аналогии у нас есть, но у случайных векторов есть еще одно или у функции
распределения случайной векторы. Есть еще одно свойство, которое выглядит так. f xi x1 и так далее xk-1 xk
и так далее xn стремится, когда xkt стремится к единице, то есть не все, а одно. Ой, бесконечности,
когда xkt стремится к бесконечности, это стремится к некой функции f xk-1 xk-1 xn, то есть у которой,
во-первых, на единицу меньше переменных. Такое свойство имеет место, как иногда, так сказать,
его пишут в учебниках. Но, с моей точки зрения, это в такой формулировке ни о чем, потому что,
если мы возьмем функцию n переменных и какую-нибудь переменную стремление бесконечности, то понятно,
что мы получим функцию с n-1 переменной. И что? И в чем фокус? Но здесь надо вот на что обратить
внимание. Вот это свойство, во-первых, называется свойством согласованности, а во-вторых,
означает, что когда вы какую-то компоненту, например, xk устремляете плюс бесконечности,
то функция n-1 переменной, которую вы получаете, это в точности функция распределения вектора
x' который имеет вид x1, xk-1, xk-1, xn. То есть это не произвольная функция от n-1 переменной,
это функция распределения случайного вектора, у которого нету катой компонента. Понятно, да?
Если у вас есть функция двух переменных, вы одну переменную устремляете к чему-нибудь,
к бесконечности. Если этот предел существует, у вас получается функция одной переменной.
Правильно, да? То есть здесь как раз ничего нет, это естественно. Особенно специфика,
почему это выделено в отдельное свойство. Не потому же, что при переходе к пределу по одной
переменной получается функция с одной переменной меньше, а потому, что та функция, которая
получается, это тоже функция распределения какого вектора. Практически вектор x, только у него
отсутствует катой компонента. Вот такое новое свойство, которого естественно нету просто для
функции распределения случайных величин. Так сейчас прибегут наши коллеги,
которые будут нас выгонять. Ну ладно, давайте, в принципе, докажем в следующий раз.

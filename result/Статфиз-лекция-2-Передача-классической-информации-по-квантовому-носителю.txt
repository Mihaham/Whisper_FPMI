Может ли фиксированное слово преобразоваться в любое другое слово по действиям такого канала?
Надо посмотреть, все ли вероятности не нулевые. Все не нулевые. Значит вообще любое входное слово
может в любое выходное слово преобразоваться. Поэтому да. И тогда встает вопрос, а раз вы можете
на выходе получить вообще любое слово, можно ли при этом передавать информацию надежно через
такой канал? Ответ какой? Можно. Восемтотики. Привет, стремящимся к бесконечности. Ошибка будет
стремиться к нулю. Не очень понятно. Тут так же, как я уже сто раз говорил, так же, как на английском
языке. Нужно понять, что от вас хотят. Что от вас хотят? От вас хотят узнать такое. Поняли ли вы вот
эту вот картинку? Пусть это множество слов на выходе. Вы какое бы фиксированное слово на входе
не взяли, оно может в любое перейти. Вообще в любое. Вот какое не тыкнем, вот в это. Может и в него
перейти. Вероятности перехода в эти все слова, они разные. И где-то будет скученность вот этих
вот слов, в которые вы можете перейти, а где-то будет разреженность. Когда Ян стремится к
бесконечности, у вас вероятность выйти за пределы этой какой-то границы будет стремиться к нулю.
Понимаете, она будет не ноль, но будет стремиться к нулю. Это понятно?
Если у вас на входе независимые случайно распределенные вот эти буквы, то да, всегда будет так.
Вот в чем нетривиальность теории Шеннона. У вас шум такой, видите, шум такой, что какой бы вы
слово на входе не взяли, на выходе все равно будет, любое слово может появиться. Но вероятность этих
слов разная. Когда длину слова увеличиваем, здесь будут не условно-типичные слова, а вот тут,
вот в этой области условно-типичные слова. Поэтому нам нужно разделить вот это все выходное
множество слов на некоторые дольки нарезать. Это области принятия решений. Слайды смогли посмотреть,
которые высылал. Там вот математические определения написаны. Все смогли или надо
повторить? Я тогда напишу. Могу переписать. Мне несложно переписать. Ну давайте перепишу,
тогда я открою. Там последние странички в общем про это. Кстати хорошая новость, на кафедру
привезли эти задавальнички. Так что тот, кто хочет на бумаге их смотреть, а не в электронном
виде, их можно взять в 508 главного корпуса. Открываем. Смотрите какие там определения. Есть
понятие кода. Что такое код? Код использует сколько-то слов. Давайте я напишу так. Код состоит
из N большой слов. Каждая длина N маленькая. N маленькая. Дальше вы что делаете? Вы вот это множество
выходных слов. Множество слов на выходе. Разбиваете на области принятия решений. Разбиваем. Эти области
принятия решения буквы V обозначены в книжке Холлива. Вот я также и напишу V, N. Там написано
подмножество Y в степени N. Что это значит? Что вы берете слова на выходе, Y это алфавит на выходе,
в степени N значит что длины N. Эти подмножества интерпретируются как области принятия решения.
Вот эти вот V это области принятия решения. То есть подмножество выходных слов. Что за решение?
Решение о том какое слово было на входе послано. Так, если вот там так написано,
если Y маленькая N принадлежит V житому, где ж пробегает значение от 1 до N большом,
то принимается решение, чтобы было послано слово W жита. Принимаем решение.
Так, если получено слово там из V0, то никакого определенного решения не принимается. Это,
грубо говоря, вы оставляете зазоры в этом разделении пирога. V0 это все,
что не вошло в эти кусочки. Это все будет V0. Не буду про это писать. Это все равно,
что вы ошибку допускаете. Эта ошибка связана с чем? С тем, что у вас всегда есть маленькая
вероятность попасть в какое-то даже нетипичное слово на выходе. Поэтому, чтобы все было строго,
вот это еще V0 пишут. Я не буду его писать. Максимальная вероятность ошибки P, E есть
максимум среди этих слов, которые мы выбрали. От чего? От единицы минус вероятность успешного.
Так, максимум. Давайте квадратные скобки здесь. Что здесь записано? Здесь стоит вероятность того,
что вы получите слово Yn из области принятия решения W жита при условии, что на входе было
слово W. Единица минус эта вероятность, это значит, что вы попадете куда-то в другую часть. Максимум по
всем словам, которые есть. Это будет максимальная вероятность ошибки. Дальше в общей теории
показывается, что максимальная вероятность ошибки ведет себя в асимптотике точно так же,
как и средняя вероятность ошибки. Между ними есть линейный коэффициент. Поэтому в общей теории
если книжку Холлива посмотрите, там будет доказываться для средней вероятности ошибки,
но максимальная связана со средним некоторым выражением. Поэтому можно в строгом определении
написать и про максимальную вероятность ошибки, что в принципе и сделано. Дальше вы что можете
делать? Вы можете варьировать выборы вот этих областей, сами нарезать пирог как хотите и выбирать
вот эти слова дубль W житы тоже можете как хотите. И теперь соответственно вам нужно
минимизировать эту вероятность ошибки по тому, как нарезать эти области. Давайте я здесь напишу,
минимизируем по выбору n слов и почему еще по разбиению на область принятия решения.
Вот если мы минимизируем эту вероятность ошибки, p, e, e от английского r, то мы получим с вами величину,
которая будет зависеть уже теперь только от n маленькая и n большое, от длины слов. В том тексте,
который я вам высылал, здесь вот p большое, давайте я напишу тоже здесь, а здесь уже p маленькое.
Ну вот и теперь вы хотите все измерять в битах, поэтому вот это n теперь записываете в другом
виде, просто эквивалентном, 2 в степени nr. Если бы r равнялось единице, то это было бы количество
битовых строк длины n маленькой, а в степени r показывает вам, что может быть там больше,
если алфавит больше, чем из двух букв стоит или меньше, если у вас есть ошибки. Просто другая
запись n равняется вот такому числу. Переопределили. И теперь вот эта величина r называется
достижимой скоростью передачи данных, если вот это вероятность p, e, n, а вместо n большого
пишем 2 в степени nr. Видите, теперь у нас одна всего буква осталась, n маленькая. Если эта величина
в пределе при n стремящемся к бесконечности дает вам ноль, то есть вот это и есть условие
асимпатически исчезающей ошибки. Понятно, что если вы r сделаете очень большим, то вы этого условия
не сможете достичь. Например, ошибок никаких нет вообще, используем только нолики и единички.
Вот ошибок нет, используем нолики и единички. Пусть длина слова n. Ну вот такое вот сообщение.
Раз ошибок нет, я его могу надежно принять в неизменном виде. Сколько всего слов я смогу
различных передать? Ну понятно, что начиная от всех ноликов и заканчивая всеми единичками,
тут всего количество слов есть 2 в степени n, правильно? Если же я захочу передавать больше слов,
то есть если r будет больше, чем единица, то тогда вот эта вероятность pen2n в степени r
уже не будет равна нулю при энстимящемся бесконечности. Понятно? Ну например, я веду
еще вместо нолика и единички еще символ там тройку какой-нибудь на выходе. Ну вот это понятно.
R показывает вам, грубо говоря, коэффициент в показателе экспоненты по отношению к идеальному
случаю. Это определение для r. Определение для вот этой вот достижимой скорости. Вот если r
больше единицы, то здесь и тогда это r недостижимый. Непонятно? Ну по-моему,
это просто формализация того, что мы в прошлый раз сказали. Теперь, значит, у вас, смотрите,
делать хуже, это особо труда не занимает. Поэтому сделать r маленькой, это не проблема. Вива стремить
ее к нулю тоже не проблема. Проблема в том, чтобы найти вот это максимальное значение у r. Вот точная
верхняя грань для r. Почему точная грань? Потому что математики, они не знают, что будет, если r точно
равно. Поскольку вы симптотики, то вот это вот точная верхняя грань для достижимых.
R это называется пропускной способностью вашего классического канала связи. Так,
записать supremum по r, где r достижима, есть c. Ну или давайте вот, ну да, по достижимым. Вот,
это пропускная способность классического канала связи. Есть и более строгие утверждения,
которые показывают, что происходит с ошибкой, с вот этой вот p,e. Если скорость будет больше,
чем r. Значит мы знаем, что p,e в симптотике предел приянсты мячемся бесконечности. Давайте я
напишу здесь, предел приянсты мячемся бесконечности. Это будет ноль, то есть вот здесь вероятность
ошибки ноль. Если будет больше, то значит смотрите, есть оценка, которая показывает,
что она будет расти. А есть еще такое понятие в математике, как strong converse с r.
Говорит, что вероятность ошибки будет стремиться к единице, если у вас скорость больше достижим.
Понятно? И обычно реализуется как раз таки вот этот случай. Но мы про саму вероятность ошибки не
говорим. Мы говорим с вами про то, что она должна равняться нулю в пределе. Вот такое вот свойство.
Интересное. В прошлый раз я кратко уже показал взаимную информацию. Сейчас просто напомню.
И с вами вот эту теорему Шинона сформулируем. Значит у нас была такая ситуация, что был
входной алфавит. Потом мы через какой-то классический канал связи передавали информацию,
получали элементы выходного алфавита. И у нас что было задано? У нас было распределение букв на входе.
У нас была стахастическая матрица, которая описывала вот этот классический канал. И мы
с вами могли написать, например, вероятность совместной вот этой случайной величины,
где x это буквы на входе, y это буквы на выходе. Это было просто px умножить на py при условии x.
В прошлый раз, по-моему, греческие какие-то буквы использовали, но сейчас так напишу.
Можно еще вероятность py найти, как сумма по x. Для каждого из этих распределений вы
можете составить энтропию. Здесь будет hx, здесь у вас будет энтропия совместного распределения hy,
здесь будет энтропия y. Эти энтропии задают количество типичных слов. В входном алфавите
есть типичные слова, в выходном алфавите есть тоже, в множестве выходных слов есть типичные
слова. Вот это множество типичных слов на выходе. Сколько таких слов на выходе? Множество типичных
слов на выходе. Это 2 в степени n h y. Вот сколько таких слов. Теперь смотрите, что мы делаем. Мы
выбираем n, слов случайным образом из множества слов на входе. Это так называемое случайное
кодирование, когда мы случайным образом выбираем эти n. Они преобразуются в условно-типичные слова.
Размерность вот этих условно-типичных слов. Сколько таких условно-типичных слов? Это 2 в
степени n h y при условии x. Условная энтропия. Писали ее в прошлый раз. Теперь если мы с вами
случайным образом выберем эти n, то они, поскольку мы их выбрали случайно, будут иметь вероятность
пересечения для выходов вот этих условно-типичных слов, стремящихся к нулю. То есть вот эти области не
будут пересекаться. Это основа доказательства теоремы Шеннона о том, что случайное кодирование
работает. На практике его не используют, потому что его сложно реализовать. Случайное кодирование.
И мы с вами понимаем, сколько n слов можно пересылать надежно. То есть так, чтобы не
пересекались вот эти вот области принятия решений. Это 2 в степени n h y разделить на 2 в
степени n h y при условии x. То есть 2 в степени n. Взаимная информация. И эта взаимная информация
допускает такую запись h y минус h y при условии x. Можно написать и поменяв y и x и h x минус h x
при условии y. То есть формула симметрична по перестановке x и y и записывается в таком
виде h x плюс h y минус энтропия совместного распределения. Вот такая формула. Взаимная информация.
Теперь давайте попробуем с нашим определением сопречь. Откуда мы эту оценку получили? Из того,
что всю размерность типичных слов на выходе разделили на размер условно-типичных слов.
Давайте я здесь напишу тогда меньше либо равно. Различимые слова.
Вот оказывается, что эта величина, которую мы здесь с вами написали, видите, она тоже входит в
показатель экспонент, так же как r входила скорость передачи данных. Что она как раз таки и есть вот тот
самый supremum, то есть верхняя грань точная для достижимых скоростей r. И это и есть иаремашина,
которая использует случайное кодирование. Формулировка ее очень простая. Пропускная
способность классического канала связи есть просто supremum этой вот взаимной информации.
Почему? По распределениям на входе. Потому что единственное, что вы можете делать в этой
ситуации, это вы можете изменять веса букв на входе. Вот что вы можете делать. Потому что все
остальное зафиксировано. Размер алфавита зафиксирован, поскольку классический канал связи
задается стахастической матрицей. Вот сколько элементов x принимает, вот столько и такова
размерность алфавита на входе. Y размерность алфавита на выходе тоже зафиксирована. Поскольку
вот эта матрица зафиксирована, то единственное, что вы можете варьировать здесь, это распределение
вероятностей, которые для этих букв используется. То есть вы можете, раз буква A, например,
встречается, ну, например, вот смотрите какая ситуация. Пусть буквы по-разному проходят через
канал. Ну, например, буква A теряется чаще, чем буква B. Значит, вы можете сдвинуть веса
в вот этих вероятностях и букве B больше вес приписать. Вот что вы можете сделать.
Да, но слова составлены из случайных независимо распределенных букв. Ну, те буквы,
вот представьте, вот канал устроен таким образом. Ну, или вот пример такой. Вот Хокинг писал
книжки. Может, слышали когда-нибудь про его книжки, про происхождение времени, вселенная,
в ореховую скорлупку и еще что-то. Он такую закономерность нашел, что чем больше я туда
включу формул, тем меньше будет продаваемость этих книжек. Поэтому, чтобы донести информацию
до читателя, он что делал? Он сместил акцент вот в этом распределении вероятностей на что? На
картинки и на текст, правильно? А формулы там тоже есть, но их очень мало. Им припишем маленькую
вероятность. И тогда информация, которую мы донесем до читателя, будет больше. Ведь ту же
самую информацию я могу донести в виде формул, правильно? Ну, тогда просто пропускная способность.
А вот, кстати, пропускная способность, она показывает вот максимум из того,
что можно выжить. Видите? Вы должны взять максимум по всевозможным распределениям вероятностей
на входе. Вот что вы должны делать. Это понятно? Вот теперь, как выглядел бы тест вот этот вот?
В следующий раз, не знаю, но, наверное, не будет такой задачки. Но как бы он выглядел? Теперь вот
вам задана стахастическая матрица для канала. Вы ее записали, я ее тоже записал. А теперь нужно
найти распределение букв на входе. То есть, с какой вероятностью нужно использовать букву А,
с какой вероятностью нужно использовать букву В, чтобы вот эта взаимная информация приняла
максимальное значение. И это и будет классическая пропускная способность этого канала связи. Понятно?
Все, делаем перерыв. Сейчас у нас с вами новая тема. Это передача классической информации,
закодированной в квантовые носители. Передача классической информации. То есть, текст какой-то,
буквы и так далее. Передача классической информации при кодировании в квантовые состояния.
В чем стоит задача? Представьте, что у вас теперь буква Х кодируется в некоторое квантовое
состояние, описываемое матрицей плотности РОХ. А потом вы декодируете и получаете какую-то
букву выходного алфавита, скажем Y. То есть, начало здесь классическое, конец классический,
а промежуточное звено квантовое. То есть, например, мне нужно переслать бит 0, я
приготовлю состояние горизонтальной поляризации, отправляю его через уже квантовый канал связи.
Например, оптоволоконная линия для этих фотонов. Потом измеряю его. Какое измерение можно сделать?
Например, с помощью поляризационного делителя пучка. Помните то самое, что было на первой лекции
в прошлом семестре. Этот клик детектора верхнего или нижнего я интерпретирую как 0 или единичку.
Значит, что у меня получается? Я могу, например, а единичку могу в вертикально поляризованную
состоянию кодера. Получается, что я использую квантовые носители информации, то есть вот эти
фотоны горизонтальной и вертикальной поляризации, пропускаю их через квантовый канал связи. А квантовый
канал связи у нас задается отображением фи, которое вы все знаете. Вполне положительно,
сохраняющийся след. И затем извлекаете классическую информацию снова путем измерения.
Значит, измерение квантовое позволяет вам вернуться в классический мир. А вот это кодирование,
как оно делается? Ну, вы приготовливаете эти состояния с горизонтальной и вертикальной
поляризации в зависимости от того, что на входе у вас есть. То есть понятно, начало классическое,
потом погружаемся в квантовую часть, а с помощью квантового измерения возвращаемся снова в классику.
Получается, что встает задача, какие здесь вы можете получить скорости передачи данных и какова
пропускная способность. Вот в таком сценарии. Понятно задача общая. Когда мы эту задачу формулируем,
у нас должен возникнуть некоторый диссонанс в голове. Сейчас объясню почему. Потому что здесь,
вот когда у вас какие-то алфавиты, булевые переменные или, например, алфавит из 33 букв,
у вас здесь все дискретно. Как только вы погружаетесь в квантовый мир, у вас множество
состоений для одного кубита, это уже континуум. Если вспомните шар Блоха, то что получается?
Получается, что тут-то уже у вас в континуум состоянии для шара Блоха задается непрерывным
вектором. Можно и в эту точку, и в эту, а можно и в бесконечно близкую к ней закодировать. РОХ
то может быть. Получается, что множество вот этих вот всяких РОХ, у вас его мощность,
бесконечность по сравнению с мощностью входного алфавита. Получается, что в один кубит
можно записать сколько угодно информации. Могу войну и мир сюда записать. Это будет вот эта
точечка. Могу конспекты всех наших лекций записать. Будет вот эта точка. Не похоже на войну и мир.
А могу другое произведение Толстого в воскресенье написать. Паспортные данные все ваши.
Это понятно, что здесь в одном кубите может содержаться бесконечное количество информации.
Виктория задала правильный вопрос. Записать в кубит мы можем сколько угодно информации.
То есть сделать соответствие такое. Эта книжка, эта точка, эти данные. А можем ли мы извлечь
информацию из этого кубита? То есть как нам узнать, что это именно вот эта точка? Этот парадокс
разрешается таким образом, что записать в кубит вы можете всё. А сколько информации вы можете
извлечь из кубита при проведении измерений? Давайте рассуждать так. Вот если у меня измерение с
двумя исходами. То есть могу измерить и получить там либо ноль, либо единицу. Получается, что за один
акт измерения я могу извлечь только один бит информации. Но либо ноль, либо единичку получить.
И поэтому кажется, что вот парадокс разрешился. Но у меня контр-аргумент к вам. Измерение в общем
случае описывается положительно операторно-значной мерой. И количество исходов измерения ничем не
ограничено. Хотите 10 исходов? Сделаю вам 10 исходов измерения. Сделаю прибор, у которого будет
10 лампочек и одна из них будет загораться. Пивовия. Общее описание измерения. Что это такое? В
самом простом случае. У вас есть отображение из множества исходов. Мы буквой у обозначали. Давайте
у в еу. Что такое еу? Еу это эрмитовые неотрицательно определенные операторы, которые суммируются в единичный
оператор. Так вспомнили? Теперь я говорю. Мой контр-аргумент к вашему. Возьму POVM с m исходами измерений.
И буду m увеличивать. Могу сделать тысячу исходов измерений. Могу миллион. И тогда кажется,
что раз у меня миллион, значит двоичный алгорифм от этого миллиона это количество битов,
которые я могу извлечь. А могу сделать миллиард. Получается, что тоже не ограничено. Понимаете?
Так, у меня к вам вопрос. Вы понимаете, что число исходов никак не ограничено. Внутренность
этой коробочки может устроено быть таким образом. У вас есть состояние на входе. Вы потом используете
вспомогательные какие-то кубиты или еще что-то, кутриты, что хотите. Разрешаете им взаимодействовать
с вашей системой. То есть перепутываете их. А потом делаете измерение уже того, что на выходе. А
когда измеряете, понятно, что количество исходов здесь никак не ограничено.
Теперь у нас встает с вами задачка, которую логично обсуждать в курсе кафедры теор-физики.
Так сколько же информации можно извлечь? Да, а тут получается хуже. Счетная, конечная даже,
но ничем не ограниченная. То есть если вы хотите, если так рассуждать, как вот здесь вот сейчас у нас,
то есть вы задаете мне наперед какую-то границу, ну например, извлекать не меньше 50 битов.
Информация. Я рисую два в пятидесятые там чего-нибудь, исходов и все. То есть какое число вы мне не задали,
я могу его реализовать. Но на самом деле задачка-то вот в чем состоит. У вас есть взаимная информация
и икс и у. И согласно теореме Шэннона, она показывает вам
достижимую скорость. И ее supremo по иксам, ой, по распределению на входе, показывает вам максимум
из того, что вы можете извлечь. Понятно? Вот эта величина показывает, сколько информации можно
извлечь. Показывает количество в кавычках извлекаемой информации. И теперь у нас с вами будет некоторое утверждение.
Ну давайте, чтобы сформулировать утверждение, надо еще ввести некоторые понятия. Давайте вернусь
сюда на эту картинку. Буквы икс у вас встречаются с вероятностями px. Здесь будьте внимательны,
потому что РО, поэтому отрицеплотность, а p распределение вероятностей. Пишется похожим
образом, но это разные выражения. Иногда в книжках пишут px. p теперь это не число p, а буква,
которая показывает распределение. Ну ладно, я не буду вас мучить такими обозначениями, поэтому давайте
латинские использовать. Так, значит, смотрите, получается, что вот эти состояния РО и икс будут с
вероятностью px реализовываться. Есть такое понятие среднее состояние ансамбла. Есть понятие
ансамбля. Что такое ансамбль? Ансамбль состояний. Ансамбль состояний это множество вот таких вот пар,
вероятность, с которой реализуется матрица плотности РО икс. Понятно? Множество таких пар.
Икс принадлежит вот этому входному алфавиту. Вот что такое ансамбль. Теперь есть понятие
среднего состояния ансамбля. Ну это очевидно. Среднее состояние ансамбля. РО с чертой сверху
напишем. Это есть сумма по иксам px РО икс. Среднее состояние ансамбля. И теперь я готов сформулировать утверждение.
Утверждение такое. Вот эта взаимная информация и икс-ы в этой схеме не превышает некоторого значения,
которое называется границей Холли. А вычисляется оно как? Как энтропия фон Эймана от
энтропии среднего состояния ансамбля минус средняя энтропия. Эта граница называется граница Холли.
Того самого, который написал книжку, которую я вам рекомендую. Само утверждение понятно?
Что такое С, помните? С от РО, а это есть минус след РО логарифм РО. Энтропия фон Эймана.
Давайте сначала проанализируем, чтобы для кубита уже понять. Энтропия может быть отрицательной?
Нет. Последний член только может уменьшить величину. Вот эта взаимная информация, следствие.
Мы еще не доказали, но уже обсуждаем. Следствие и икс-ы не превосходит средней энтропии среднего
состояния ансамбля. Теперь, если кодируете в кубитные состояния, вот здесь кубиты,
то РО среднее это матрица плотности какая-то кубита. Энтропия матрицы плотности 2 на 2 не
превышает в свою очередь 1 бита. А если бы у нас была матрица не кубитная, а например кудитная,
то есть если бы мы, например, использовали D-уровневые квантовые системы D на D, то тогда
у нас бы получилась с вами логарифм D бит. Таким образом, если мы с вами докажем это утверждение,
то это означает, что хотя в кубиты можно записать бесконечное количество информации,
пытаясь ее извлечь вне зависимости от того, сколько исходов измерения у меня будет,
то есть мой контраргумент не прокатит. Могу использовать хоть 10 измерений с 10 исходами,
хоть с миллионом, все равно больше чем один бит информации из одного кубита я извлечь не смогу.
А, ломается он вот в каком месте, потому что вероятности исходов, когда я делаю измерения
с многими исходами, они задаются некоторой формулой. Эта формула, это след РОСЕ и получается,
что у вас стехосидическая матрица имеет определенный вид, не произвольный, а определенный,
конкретный вид, который задается правилами квантовой механики. Сейчас запишу. И в этом месте как раз и ломается.
Все. Когда вы задали POVM, давайте теперь набросок доказательства сделаем. Наверное, все не успеем даже.
Вот вероятность получить исход у при измерении при условии, что вы хотели переслать букву х,
вот эта вероятность, как у вас запишется, это след того самого РОХ с ЕY. Вот это вот понятно.
Но если POVM помните, то вот эта формула должна сразу всплыть. И поскольку у вас вот это выполняется
свойство, вот поэтому благодаря ему, этому свойству у вас стехосидческая матрица имеет определенный вид,
вот задаваемый этой формуле. Именно из-за этого вы не можете извлечь больше информации, чем один бит, из одного кубит.
Так, теперь смотрите, что нам нужно сделать. Нам нужно эту формулу доказать, вот с этим неравенством.
А в доказательстве используются еще вспомогательные некоторые конструкции, поэтому нам нужно еще будут некоторые леммы.
Помните, что такое относительная энтропия квантовая? Вот я теперь в элементы этой относительной энтропии
вставлю действие канала на некоторое РО и пишу, что это будет не превышать РОС для любого
вполне положительного и сохраняющего след отображения ФИ. То есть для квантового канала ФИ, что происходит?
Что квантовая относительная энтропия обладает свойством монотонности. Физически как его нужно понимать?
Вот пусть у вас есть множество квантовых состояний. Вот у вас РО и СИГМА. Квантовая относительная энтропия своего рода расстояние,
но несимметричная по перестановке РО и СИГМА. То есть оно показывает, насколько отличаются РО и СИГМА.
Напомню, что равна нулю это увеличена тогда и только тогда, когда РО совпадает с СИГМА.
Помните? Так, им нужна какая-то реакция этого. Помните? Своего рода расстояние. Как действует отображение?
Вспоминайте задачки из прошлого семестра. Нарисуйте образ шара, блог под действием какого-то отображения.
Все сжимается, правильно? Либо к оси, либо к точке, либо еще куда-то. Образ всегда меньше, чем исходная множество.
Значит, ФИ РО будет лежать где-то здесь, ФИ СИГМА будет лежать где-то здесь.
Состояние что сделается под действием сжимающего отображения? Уменьшится. Вот эта величина меньше, чем исходная величина.
Вот геометрический смысл этой формы. Понятно?
Не доказываем. Если такая потребность возникнет, то будем доказывать.
Пока используем вот эту интуицию, которая здесь хорошо работает.
Кстати, недавно люди, совсем недавно, несколько лет назад года, в каком году? В 17-м, доказали, что это верно не только для вполне положительных отображений, но и просто положительных, что было некоторой сенсацией в нашей области науки.
Так, теперь другой пример. Давайте лему другую еще сделаем. Лемма 2.
Взятие частичного следа – это тоже канал. Мы про это раньше не думали, а сейчас обсудим.
Поэтому, это как бы даже утверждение сразу, следствие из этой леммы.
Поэтому относительно энтропия может только уменьшаться.
То есть, если вы возьмете составную систему РОАБСААВ, возьмете частичный след, то энтропия может только уменьшиться.
Так, сейчас объясню, что имеется в виду. Здесь имеется в виду конкретный вид ФИ.
Сейчас, сейчас напишу.
Сумма по ж и ж. Напоминаю вам формулы из предыдущего семестра.
И единичный оператор под системе А, ж в B.
Вы видите, что вот то, что здесь написано, это есть представление Крауса с вот такими операторами Крауса.
А поэтому, это квантовый канал. Но в чем особенности этого квантового канала?
Размерность на выходе не совпадает с размерностью на входе.
Так же, как при изучении языка сначала какие-то простые моменты проходят, потом обобщается, усложняется.
Точно так же и здесь. Мы с вами рассматривали каналы на какие,
которые на вход брали матрицу D на D, на выходе выдавали ту же самую матрицу D на D.
А здесь на выходе матрица меньшего размера получилась.
Но это тоже вполне положительное и сохраняющий след отображения.
Ну, след сохраняется, почему? Потому что у частичного следа,
как вы помните, есть такое свойство, что след сохраняется.
А вполне положительное следует из представления Крауса.
Так, вот здесь все понятно?
Взятие частичного следа, это тоже канал, поэтому к нему применима вот эта формула.
Понятно? Все. Теперь две леммы у нас есть, теперь будет вспомогательная конструкция.
Видите, какое непростое утверждение у нас с вами получается.
Вспомогательная конструкция такая. Будет у нас с вами классическая, как бы, часть.
Буквой P ее обозначим от английского preparation.
Будет квантовая часть. Это наша система.
И будет часть, связанная с измерением measurement.
И давайте теперь посмотрим вот на такой объект.
Ро PQM. Это будет эффективная трехсоставная система с такой матрицей плотности.
Сумма по х.
Дальше у нас будет стоять Px, вероятность появления х.
Потом будет стоять проектор на x.
Дальше сами ro x через тендерные произведения.
И 0, то, что стоит в измерении.
Так, я стираю левую доску.
Это вспомогательная конструкция.
То есть абстрагируйтесь пока от того, что было.
Посмотрите на этот объект по-новому.
Что мы здесь ввели? Мы ввели вспомогательное Гильбертово пространство с вот этими x классическими.
То есть вот эти x и x-штрих.
Если вы возьмете пространство, натянутое на такие векторы, то есть с Px возьмете линейную оболочку,
то это будет что? Это будет Гильбертово пространство, связанное с приготовлением P.
Это вспомогательное Гильбертово пространство.
Сами руками его сделали.
Значит, у вас еще есть пространство, связанное с вашей системой,
там, где матрица плотности ro x, это ваша обычный Гильбертово пространство,
в которое вы кодируете.
То есть у вас еще есть пространство, связанное с вашей системой,
там, где матрица плотности ro x.
Матрица плотности ro x, это ваша обычный Гильбертово пространство,
в которое вы кодируете ваш классический буквы.
А m – это результаты измерений.
Размерность этого h m – это есть как раз то количество исходов измерения.
Количество исходов измерений.
И оно ничем не ограничено.
То есть может быть миллион, может быть десять, может быть два.
Какое хотите – такое сделаем.
Теперь давайте посмотрим, что здесь есть.
Давайте посмотрим под систему.
Под систему вы видите, что сумму по x можно приписать только к первым двум выражениям.
Поэтому давайте посмотрим на вот эти первые два выражения.
Ro pq – редуцированная матрица плотности, в этом случае тривиальная.
Это будет сумма по x, px, x, x, ro x.
Ну и давайте найдем энтропию этого состояния.
Значит как мы будем искать энтропию?
Мы с вами поднимаем, как записать эту матрицу.
Эта матрица на самом деле в стандартном базисе из x и базисы для ro
будет иметь блочно диагональный вид.
На диагонали будут стоять p1, ro1, p2, ro2 и так далее.
Все остальные нулевые элементы.
Отхожу, смотрите, почему это так?
Потому что x – ортонормированный базис.
Значит возьму первый x, это будет 1 с нулями, 1 с нулями,
умножается на ro x тензорно, даст мне вот этот элемент.
Блочно диагональная структура, понятно?
Значит энтропия, как вы помните, связана с собственными значениями.
В данном случае, когда у вас блочно диагональная матрица,
это будет сумма px, энтропии вот этих блоков px, rox.
Так, правую доску сотру.
Или здесь, давайте здесь верхнюю часть.
Теперь вспоминаем, что такое энтропия.
s px, rox, это есть минус, след, дальше стоит px, rox и логарифм от этого px, rox.
Значит, логарифм произведения в школе – это была сумма логарифмов.
С матрицами почти то же самое, px – это у вас число, а rox – матрица.
Значит, у вас будет логарифм px умножить на единичную матрицу,
прибавить логарифм rox.
Понятно?
Что тогда получается?
p, ro, p и берем след.
Значит, от первого множителя просто возьмет след от матрицы плотности, будет единичка.
Значит, будет минус px, логарифм px.
От второго выражения что будет?
px число выношу, а будет след rox, логарифм rox.
Это просто энтропия.
Плюс px, s, rox.
Таким образом у нас что получается?
Что вот это s, p, q, s, ro, p, q.
Это есть...
Что такое?
Энтропия распределения px.
Энтропии для классических величины буквы h обычно обозначаются.
Прибавить сумма по x, px, s, rox.
Вот это h, px.
Это есть энтропия для классических величин, то есть минус px, лог px.
Теперь с этой энтропией разобрались.
Теперь нам нужно какую-то конструкцию сделать, включающую в себя относительную энтропию.
Что мы с вами здесь тогда посмотрим?
Отдельно можем еще найти rope из этой формулы.
Давайте отдельно найдем rope.
Редуцированная матрица плотности под системы p.
Это будет просто сумма px, px, xx.
А x-ы образуют ортонормированный базис.
Поэтому энтропия вот этого rope, p, это есть просто вот та h, px.
А что еще у нас есть?
У нас есть еще roe, q.
Roe, q – это взятие частичного следа, значит по этой первой части x-ы след единичный.
Поэтому px, roe, x будет просто сумма px, px, roe, x.
А это есть среднее состояние ансамбля, roe среднее.
И теперь мы с вами можем записать относительную энтропию.
Относительная энтропия чего?
Roe, p, q с одной стороны, а с другой стороны тензорное произведение.
Roe, p и roe, q.
И что это такое?
Так, это задачка из прошлого семестра.
Это есть энтропия roe одной подсистемы, плюс энтропия другой подсистемы, минус энтропия всего в целом.
Подставляем s, roe, p – это h, энтропия вот этого распределения классического.
Прибавить s, roe, q – это энтропия среднего состояния ансамбля.
И отнять вот эту величину, которую мы считали s, roe, p, q.
Значит, это будет минус h, этого px и минус средняя энтропия по ансамблю.
Видите, что h уходит, и в итоге получается та самая граница Холева, которая у нас есть.
s, roe средняя минус сумма по х, px, s, roe, x.
Значит, мы с вами правую часть в утверждении для границы Холева записали в виде относительной энтропии.
Дальше какой будет наш трюк?
Мы с вами придумаем сейчас некоторое вполне положительное отображение phi,
которое сведет вот эту запись к другой записи,
а эта другая запись будет как раз-таки равна взаимной информации и x, y.
Вот в чем трюк заключается. Понятно?
Тогда, используя лему про монотонность относительной энтропии,
мы получим с вами результат, который нужен.
Если успеем. Сколько у нас осталось там, минуточек?
Четыре.
Тогда, наверное, не успеем.
Значит, идея дальнейшая, смотрите, идея.
Найти, ну да, это не найти, построить, phi, построить phi.
Значит, смотрите, кстати, что мы можем с вами делать?
Что мы можем с вами делать?
Здесь только две было под системой из трех.
Почему? Потому что третья была вот в таком вот тривиальном состоянии для измерения изначально.
Но поскольку она в факторизованном виде, я могу ее приписать сюда,
ничего не потеряв.
То есть, перед этой идеей еще мы с вами вот что сделаем.
Я могу дописать P Q M,
Ро, так, П, М, так, правильно или нет?
Нет, Ро П и Q M, вот так вот.
Равно в точности вот тому выражению?
П
Поскольку, почему?
Поскольку мы знаем с вами формулу,
что энтропия тензорного произведения Ро на любой там другую матрицу Омега,
это будет S Ро плюс S Омега.
А если Омега это 0,0, то энтропия равна 0,
то получается просто S Ро.
Понятно?
Нет?
P Q, а да, перепутан.
Да, все перепутал, и тут должно быть тензорное произведение.
Ро П, ну то самое выражение, которое там Ро П.
Смотрите, теперь у нас здесь тройная часть, правильно?
И это тройное выражение для трех подсистем.
Это есть энтропия среднего минус средняя энтропия.
Значит, в чем стоит дальнейшая идея?
Построить ФИ, который нам,
подействовав на вот это Ро П Qm, даст некоторый объект,
подействовав на вот эту часть Ро П тензорно Ро Qm,
даст некоторый объект, который с одной стороны
будет равняться взаимной информации между X и Y.
И мы даже знаем, как его сделать.
Вам нужно сначала описать процесс измерения,
а потом выкинуть квантовую часть, правильно?
Тогда получится только P и M.
А с другой стороны, в силу того,
что имеет место монотонность относительно энтропии,
у вас это будет не превосходить исходной величины.
А исходная величина, это и есть граница Холева.
Вот в чем состоит идея.
То есть, как только мы этот ФИ предъявим,
который равняется вот этому ИХY,
то сразу все и получится.
И вот этот ФИ, он выглядит как
последовательно применение двух операций.
Сначала квантовый процесс измерения,
а потом вот этот ФИ,
частичный след по квантовой подсистеме.
Это измерение тоже могу написать,
но, наверное, уже в следующий раз,
чтобы не загромождать.
Логика какая?
Граница Холева устанавливает верхнюю
границу того, сколько энтропии есть,
а потом выкинуть квантовую часть,
а потом выкинуть квантовую часть,
то есть выкинуть границу того,
сколько информации можно выжать
из квантового объекта.
На семинарах тогда пойдете вперед
и посмотрите на конкретных задачах,
сколько информации можно получить
в конкретных задачах.
На сегодня все.

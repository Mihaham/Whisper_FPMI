На чём мы с вами в прошлый раз остановились? Мы с вами говорили про многомерное распределение,
про тиремовый в конце, я напомнил про тиремовый вине, который мы применили,
чтобы доказать форму свёрки. В общем, про распределение простучальной величины,
простучальные векторы, я самое главное сказал. Давайте теперь будем говорить про некоторые
характеристики распределения, которые нам впоследствии пригодятся, когда мы
всякие предельные теории будем доказывать. В частности, ещё эти характеристики распределения,
о которых мы будем сегодня говорить, они важны со статистической точки зрения. В частности,
математическое ожидание – это, грубо говоря, среднее значение, которое принимает случайная
величина. Как это можно понять? Можно это понять имперически, имея в виду, что среднее значение
случайной величины – это если бы вы могли это случайную величину много-много раз независимо
реализовать, провести много-много экспериментов, независимо в ходе которых ваш случайная величина
приняла бы то или иное значение. То есть, если проводить аналоги с температурой,
допустим, вы измеряете температуру при одних и тех же условиях в одном и том же месте много-много
раз, и получаете разные какие-то значения, и потом вы считаете просто империческое среднее,
то есть складываете и делите на количество. Оказывается, математическое ожидание,
о котором мы сегодня будем говорить, очень хорошо этим империческим приближается. Если вы будете в
реальность проводить какой-то эксперимент, и у вас будет правильное соображение на тему того,
что за распределение вашей случайной величины, если вы посчитаете империческое среднее и
теоретическое, они окажутся очень близки. Что же такое это самое теоретическое среднее,
ну то есть математическое ожидание. Но это, на самом деле, если что иное,
как просто интеграл либега от вашей случайной величины по вероятностной мере. То есть,
у вас есть вероятностное пространство омега фп, есть какая-то случайная величина,
которая действует из омега вр, на этом вероятностном пространстве. Ну и вы полагаете,
просто по определению, что мат ожидания кси, это есть не что иное, как интеграл по омега
от кси dp. То есть, просто интеграл либега от вашей функции кси по мере p.
Значит, оказывается, есть такая теорема либега, теорема, прошу прощения, о замене переменных
интеграль либега, которая утверждает, что это есть не что иное, как интеграл по r от x dp кси.
Да, то есть, на самом деле, вы можете интегрировать, вы можете перенести вероятностную меру вр,
и интегрировать просто x по соответственно этой мере будет тоже самое. Иван Генрихович рассказывал,
что такое теорема о замене переменных в интеграле либега? У нас на мата не было.
Можно просто, почему этот интеграл, почему вообще кси интегрируема по мере p?
Она может быть, конечно, не интегрируема, но если она интегрируема, то это с томат
Если она не интегрируема, то мат ожидания не существует. То есть, если интеграл не существует, то мат ожидания не существует тоже.
Как у нас вообще определяется интеграл либега? Мы говорим, что мы сначала его определяем для не отрицательных функций.
То есть, мы можем представить кси как кси плюс минус кси минус, и где и кси плюс и кси минус это не отрицательные функции,
которые определяются вот так. Кси плюс это максимум из кси нуля, а кси минус это максимум из минус кси нуля.
И находим, понятно, что у не отрицательных функций, в случае случайных величин, интеграл либега существует, но может быть бесконечный.
В том случае, когда только одна из этих величин бесконечна, если обе конечны, если там и интеграл от кси плюс,
то это означает, что и мат ожидания кси тоже бесконечна.
Если один из них бесконечен, ну скажем первый, а второй конечен, то это означает, что мат ожидания равна бесконечности.
В этом случае будет плюс бесконечность, а в случае, когда интеграл кси плюс ДП конечен, а второй интеграл бесконечен,
то это значит будет минус бесконечность. Ну и в случае, когда оба интеграла бесконечны, то есть,
иными словами, интеграл либега не существует, то и мат ожидания кси тоже не существует.
Ну короче говоря, это просто интеграл либега, он существует или не существует точно тогда же, когда и существует или не существует интеграл либега.
Это тут просто терема о замене переменных. Она утверждает следующее.
Ну в общем-то это и есть утверждение теремы о замене переменных. У вас есть какая-то функция кси от Омега, и вы ее заменили на х.
Вы сделали такую замену, и в результате у вас интеграл стал по r.
Вместо интегрирования по мере p, вы интегрируете по той мере, которая при такой замене у вас просто получается.
У вас мера p переходит в распределение случайного чдх7.
Ну хорошо, есть какие-то вопросы?
Ничего нового я пока не сказал, просто сказал, что мы должны посчитать интеграл либега, чтобы получить мат ожидания.
Давайте в каких-то частных случаях посмотрим что будет происходить.
У нас интересуют случаи дискретных распределений, и случаи абсолютнепрерывных распределений.
В archive по поводу распределения я не помню говорил ли мне это, я на всякий случай еще повторю.
Есть еще phr обłbyку.
Есть еще так называемое сингулярное распределение.
что такое сегулярное распределение? Оно очень похоже на дискретно, то есть на самом деле дискретно это частный случай.
То есть это такое распределение, что мером множества, на котором сосредоточена вся вероятность,
то есть вы нашли такое множество х, что его мера равна единице, при этом его мера либега равна нулю.
Когда мы говорим про дискретное распределение, вот этот носитель множества х, оно конечное или счётное.
Но понятно, что этими ситуациями не ограничиваются множество, у которых либега мера равна нулю.
Можно взять какое-то континуальное множество, у которого мера либега равна нулю.
И тогда, если вы задаете такое распределение вероятностей, что оно будет полностью сконцентрировано в этом множестве,
то есть вероятность этого множества на единице, то это распределение будет сегулярным.
Дискретное распределение получается частный случай.
Почему это важно отметить?
Потому что вообще все распределения представляются как линейная комбинация абсолютно непрерывного и сингулярного.
Поэтому на самом деле с помощью абсолютно непрерывных и сингулярных распределений можно классифицировать вообще все.
Но это такая теоретическая вещь, у нас в курсе совершенно нигде не будет возникать.
Эти распределения мы говорить не будем, поэтому оставим это за кадром.
Хорошо, так вот.
Когда мы считаем мат ожидания, нам важно понять, как оно считается в дискретном случае и в абсолютно непрерывном случае.
Давайте начнём с простейшего, с дискретного.
Пусть это кси, это дискретная случайная величина.
Что это значит?
Это значит, что есть какой-то носитель.
Х, носитель этого распределения, он не более чем счёт.
Всё распределение случайной величины кси сосредоточено на этом множестве.
Тогда во что превращается наш интеграл либега?
Понятно, это просто есть сумма.
Давайте смотреть на второй интеграл.
Так как вот это распределение является дискретным, то интеграл либега – это просто…
То есть вы интегрируете простую функцию данную или функцию, которая принимает счётное количество значений.
То есть вы на самом деле получаете сумму по всем х маленьким из х большого, х умножить на вероятность того, что кси равняется х.
Ну вот, в общем-то и всё. Давайте в каких-то ситуациях посчитаем, чему будет равно от ожидания.
Разберём примеры распределения.
Ну, во-первых, распределение Бернули.
То есть, что такое распределение Бернули?
Что я имею в виду, когда я пишу кси – волна Бернули с параметром p.
Я имею в виду, что распределение случайной величины кси, то есть вот это вот p кси, это вероятность на меры,
которая устроена следующим образом.
p кси от единицы – это p, а p кси от нуля – это 1-п.
То есть вероятность того, что случайная величина приняла значение 1, равняется p.
Вероятность того, что случайная величина приняла значение 0, равняется 1-п.
Поэтому от ожидания кси – это, конечно, просто 1 умножить на p, плюс 0 умножить на 1-п, то есть p.
Ну окей, давайте посмотрим на биномиальное распределение с параметром p.
Я напоминаю, что это означает, что вероятность того, что случайная величина попала в точку k,
это в точности csn по k на p в степени k на 1-p в степени аминус k.
При этом k пробегает значение от 0 до n, иначе вероятность равна 0.
Ну понятно, тогда можем опять же по нашей форме посчитать от ожидания кси, это будет сумма просто по всем k от 0 до n.
k умножить на csn по k на p в степени k на 1-p в степени аминус k.
Ну, сходу кто-то, конечно, понимает, чему равна эта сумма, кто-то нет, я сейчас ее посчитаю.
Но на самом деле, этому ожиданию можно посчитать сильно проще, и я об этом позже скажу.
Есть более простой способ вычисления от ожидания биномиального случайной величины, нежели вот этого сумму считать.
Как сумму можно посчитать? Ну, разные способы. Ну, например, можно сделать следующее.
Можно заметить, что csn по k по определению от n факториал поделить на k факториал, поделить на минус k факториал,
и сократить k факториал из номинателя вот этим k, то есть из номинателя, что получился k минус 1 факториал.
Но для этого нам нужно начать суммировать с единицы. Понятно, что мы это можем делать, потому что в 0 это штука просто 0.
Эта штука равна 0, если k равно 0. Поэтому мы ничего не потеряем, если просто здесь вместо 0 напишем единицу,
у нас сумма не изменится. Давайте так и сделаем.
А далее сократим этот k, то есть вместо csn по k напишем n факториал поделить на k минус 1 факториал, и на n минус k факториал.
Далее давайте вынесем за знак суммы np.
Если мы вынесем отсюда m, то вместо m получится n минус 1 факториал.
И тогда вот вся эта дробь, это есть не что иное, как csn минус 1 по k минус 1.
А это бином Ньютона просто, это в точности единица.
То есть на самом деле вы вместо k можете, как это понять, давайте заменим k минус 1 на s.
Вот такую вот сделаем замену, тогда будет здесь сумма по s от 0 до m минус 1.
Тогда здесь будет вместо k минус 1 здесь будет s, здесь будет соответственно тоже s, а здесь будет n минус 1 минус s.
Поэтому вся эта сумма, это просто есть не что иное, как бином Ньютона, то есть p плюс единица минус p в степени n минус 1,
что равно единице, и получаем просто np.
Если есть вопросы.
Так, окей, давайте теперь возьмем равномерное распиление на множестве каком-то.
Ну скажем, можно взять просто подряд числа от одного до m.
То есть что это означает? Это означает, что ваше распиление, распиление ваших случайночных сил,
примерно следующим образом, оно принимает значения от одного до n,
никакие другие, у всех основных значений вероятно 0.
И для чисел от одного до n она равна, ну она должна быть равномерна.
То есть вероятно, должна быть одинаковая, так как чисел у вас n, она просто равна одной n-той.
Для всех k от единицы до n, да, иначе 0.
Вот, это мат ожидания кси, это есть сумма, у k от 0 до n, извиняюсь, от единицы до n,
k умножить на одну n-тую, ну и это равно просто np.
Ну и последнее дискетное распиление, которое мы с вами разбирали в качестве примеров дискетных распилений,
плацоновская, с параметром лямда.
Я напомню, что это означает, что случайночина ваш принимает только целое неотрицательное значение,
и вероятность каждого к целого неотрицательного, это лямда степени k,
на е степени минус лямда выделить на k факториал.
Вот, тогда мат ожидания кси, это сумма по k от 0 до бесконечности,
k умножить на лямда степени k, на е степени минус лямда и выделить на k факториал.
Опять, при k равно 0, эта штука просто обращается в 0, и поэтому можно насуммировать, начиная с единицы.
И сделаем такой же точно трюк, который мы делали с беремиальным распредлением,
давайте сокращаем k факториал с беременателем, остается просто k минус 1 факториал.
Давайте теперь лямду умножить на е в степени минус лямда вынесем за знак суммы,
и получим вот такую штуку, лямда степени k минус 1 поделить на k минус 1 факториал.
Это в точности ряд Тейлора для экспонента.
То есть, если вы опять вместо k минус единицы напишите s, то есть обозначите s равно k минус 1,
то у вас будет сумма по всем s или от бесконечности, здесь будет лямда в степени s, а в знаменателе будет s факториал.
Поэтому это в точности ряд Тейлора для экспонента от лямда.
То есть, получаем лямда е в степени минус лямда умножить на е в степени лямда, что просто равно 0.
Так, есть ли какие-то вопросы?
Хорошо, давайте теперь рассмотрим ситуацию к такси.
Это абсолютно неприютно, случайно, вечно.
Возвращаемся к определению математического ожидания вот к этому.
В тех ситуациях, когда мы интегрируем патолий, вот мы делаем в этом случае,
то есть мы делаем в этом случае дизайнерский университет,
то есть мы делаем в этом случае дизайнерский университет,
В тех ситуациях, когда мы интегрируем по абсолютно неприродному распределению,
у него есть плотность, то есть означает, что вы плотность из-под дифференциала можете вынести,
и у вас получится просто плотность кси умножить на dx.
То есть этот интеграл превращается в интеграл по r от x пкси от x dx.
Вообще, давайте запишем такое утверждение, что если g произвольная баррельская функция zrvr,
то если у вас есть случайное распределение и пкси абсолютно непрерывно,
то тогда интеграл от g от x d пкси, это есть просто интеграл от g от x пкси от x dx.
Вот, и это утверждение доказать несложно, я не знаю, доказывает вам его Иван Генрихович или нет.
Давайте, что ли, быстренько коротко обсудим его доказательства, чтобы вы понимали,
это очень важный момент, почему для абсолютно неприродных распределений мы можем выносить
плотность из-под дифференциала. Понятно, что мы можем это делать для индикаторов просто по
определению абсолютно неприродного распределения. То есть если g от x – это индикатор того,
что x помежит кому-то баррельскому множеству b, то тогда интеграл от g от x d пкси – это просто есть
интеграл по множеству b d пкси. Ну, то есть на самом деле это есть не что иное, как пкси от b,
по определению интеграла Лебега. А по определению плотности, по определению
абсолютно неприродного распределения, вероятность любого баррельского множества – это есть просто
интеграл плотности. Ну или иными словами, интеграл по r от g от x на пкси от x d x. То есть для
индикаторов баррельских множеств – это просто есть не что иное, как определение абсолютно
неприродного распределения. И нам нужно увидеть, что это продолжается на вообще все баррельские
функции. Понятно, что это продолжается на простые баррельские функции. То есть если g простая,
что значит простая? Это значит, что она равна сумме по i от единицы до n, то есть в конечной сумме,
каких-то значений g it, умножительный индикатор того, что x принадлежит
Боите, а Боите – это какие-то баррельские множества. Ну полинейность интеграла Лебега тогда, конечно,
все следует. Полинейность интеграла Лебега тогда все следует. То есть интеграл по r g от x d пкси – это
есть интеграл от суммы, а интеграл Лебега линейен, поэтому получаем сумму по i от единицы до n g it
умножить на интеграл по r, индикатор от x принадлежит Боите, умножить на d пкси. Но для индикаторов мы уже
знаем, что наше утверждение это правда. Поэтому мы получаем теперь интеграл Лебега по классической
мере Лебега, и внутри интеграла еще появляется плотность. Обратно по линейности вносим сумму по
интегралу и получаем то, что нужно. Вот, ну а дальше осталось просто по
по там Лебега в жаревой сходимости, что-нибудь подобным, приблизить любую функцию простой,
приблизить любую функцию простой. Что нам нужно? Ну, понятно, что нам нужно, чтобы интеграл существовал,
чтобы вообще это равенство было верным. Нам нужно, чтобы интеграл существовал, поэтому...
Сейчас. Я просто пытаюсь понять. Ну, давай для не отрицательных сначала. Ладно, я думаю, что можно сразу
для произвольных. Давай сначала не отрицательных. Если g не отрицательна, то приблизим ее простыми,
причем приближать можно вот так, тоже не отрицательными. Ну, то есть, я имею в виду,
что последность g-н не убывает и стремится к g-поточечному. Вот, тогда интеграл от g от x
dpx-и по определению интеграла Лебега, именно по определению. Это есть предел
предель смещения бесконечности, интеграл по r от g-н от x dpx-и. Для простых мы умеем выносить плотность,
получаем предел интеграл по r g-н от x dpx-и. Вот. А дальше, так как эти функции не отрицательны,
и плотность не отрицательна, и g-н не отрицательны, то их произведение тоже не отрицательны. А значит,
мы можем внести предел внутри интеграла и получим интеграла g от x px-и от x dx, что требуется.
Да, то есть не отрицательны здесь нам сильно в помощь. Так просто мы бы здесь не смогли
воспользоваться тиремой Лебега, потому что непонятно, что с этим делать. Если мы не предположили
изначально, что g-н не отрицательна, тогда g-н тоже бы не отрицательны. Вот это произведение
нифига не простая функция, потому что g-н это простая, а px-и от x совсем нет, поэтому их произведение
не простая. Зато есть не отрицательность, поэтому мы можем вносить предел под интеграл. Ну и теперь,
если g произвольная, то представляем ее как g-плюс минус g-минус, то есть g-плюс это максимум
из g и ноль, g- это максимум из минус g и ноль, и пользуемся просто линейностью.
Пользуемся просто линейностью. Значит, интеграл от g от x dpx-и, если он существует,
то есть предполагаем, что он существует, тогда это интеграл от g-плюс dpx-и минус интеграл от g-плюс
dpx-и. Значит, если он существует, то это означает, что ровно один из этих интегралов, прошу прощения,
не более одного из этих интегралов, не более чем один из этих интегралов бесконечно. Да, то есть
два интеграла бесконечно быть не могут, только один. В любом случае, для не отрицательных функций
так или иначе мы умеем заменять уже интеграл на интеграл по классическому мере Либега,
поэтому получаем интеграл от g-плюс x-и dx. Так как между этими интегралами равенство стоит,
то раз только один из этих, не более чем один из этих интегралов может быть бесконечен,
то же самое верно и для этих двух интегралов. Не более чем один из этих двух интегралов может
быть бесконечен. А значит, это равно по линейности интегралу от g-пxy dx, что и требуется. Да, то есть
утверждение доказывается более-менее очевидно, просто основываясь на предельном переходе вот
здесь и на определение. Есть какие-то вопросы? Прекрасно, да. То есть теперь мы понимаем,
действительно, что просто из того, что мы умеем выносить плотность из-под дифференциала для
индикаторов, ну, то есть иными словами из-за того, что мы знаем вот это, из этого следует,
что внутри интеграла можно выносить плотность вообще для приинтегрирования любых баррельских
функций. Хорошо, теперь давайте примеры разберем. То есть я сейчас оправдал вот эту формулу,
и теперь я буду применять для каких-нибудь конкретных абсолютно неправильных распределений.
Так, какие у нас там были абсолютно неправильные? Ну, наверное, равномерные. Пусть x имеет
равномерное распределение на отрезки от А до B. Скажем. Что это значит? Это означает,
что плотность распределения этой случайной величины, которую мы обозначаем как x от x,
это есть единица девять на B минус А, множительный индикатор принадлежности отрезку от А до B.
По нашей формуле, в отождании x, это есть интеграл по r от x пекси от x до x. То есть
интеграл от A до B, x поделить на B минус А до x. Легко видеть, что это есть, когда мы интегрируем x,
мы получаем x в раз пополам. То есть, поставляя B и A, мы в итоге получаем B плюс A пополам.
Есть какие-то вопросы? Окей. Какой теперь? Давайте экспоненциально.
Экспоненциально распределение с параметром лямбда, это такое распределение, у которого плотность равна лямбда
е в степени минус лямбда x, умножить на индикатор того, что x не отрицает.
Ну тогда, от ожидания x, это есть интеграл по r от x, множить на пекси от x до x. И это равно интеграл
от нуля, потому что у нас есть такой индикатор. То есть, функция плотность вне луча на лямбда
бесконечна просто нулю равна, поэтому там можно интегрировать. Здесь будет лямбда x е в степени минус лямбда x dx.
Эта штука легко интегрируется по частям. Занося под дифференциал экспонента, мы получим
минус x умножить на d от е в степени минус лямбда x. Вот, значит, тогда интегрируя по частям,
мы получаем такую вот вещь. Первая слагаемая это ноль, первая слагаемая это ноль, остается только
вторая слагаемая. Ну, этот интеграл легко найти. Это минус е в степени минус лямбда x поддевить на лямбда.
И в итоге получаем просто 1 поддевить на лямбда. Значит, мат ожидания экспоненциально
случайно вещественные с параметром лямбда равна единицей на лямбда. Есть какие-то вопросы?
Ну и еще давайте для нормального распределения с параметром a и c в квадрате посчитаем. Значит,
забегая вперед, скажу сразу, что я бы мог сперва сформулировать какие-то полезные свойства
мат ожидания. И после этого мне было это мат ожидания такой случайно вещественной читать проще.
Я бы сначала посчитал мат ожидания стандартного нормального, то есть когда параметры равны 0 и 1,
это как бы технически проще, хотя идеологически ничем не отличается, но технически проще,
а потом бы легко бы из этого вывел мат ожидания вот такой случайно вещественной. Но для того,
чтобы наработать технику, я поступлю по-другому. Я сначала вас научу в сложном случае считать,
то есть чтобы техника была в вычислении таких интегралов, а потом еще раз вернусь к этому,
когда говорю про свойства математического ожидания. Значит, плотность такого распределения равна
по определению 1 петель на корень из 2 пи сима в квадрате, е в течение минус х минус а в квадрате
петель на 2 сима в квадрате. И здесь отличие от предыдущих в том, что носит все множество
издательных чисел. Здесь видно нет никаких индикаторов, у нас везде плотность нулевая.
По определению мат ожидания к сте это есть интеграл по r от x по x от x до x. И в нашем
случае получаем интеграл по r от x поделить на корень из 2 пи сима в квадрате. Экспонента
от минус х минус а в квадрате поделить на 2 сима в квадрате dx. Что с этим делать?
С этим можно сделать следующее. Давайте мы вот в этой дроби в числителе вычтем и добавим а. То есть
представим на самом деле в виде суммы двух интегралов 1 от х минус а поделить на корень из 2
пи сима в квадрате. Экспонента от минус х минус а в квадрате поделить на 2 сима в квадрате dx.
И плюс а интегралов по r от единицы поделить на корень из 2 пи сима в квадрате е в степени
минус х минус а в квадрате поделить на 2 сима в квадрате dx. Я иногда пишу е и степень,
иногда и экспонент. Теперь смотрите, вот какое наблюдение. Вот эта функция,
которая здесь написана, если сместить в точку а, то она окажется нечетной.
То есть иными словами, если вы нарисуете график график этой функции, тут у вас 0,
а вот тут у вас точка а. Если вы нарисуете график этой функции, то она в точке а равна нулю,
и относительно этой точки она будет симметрична. Справа положительно, слева отрицательно.
При этом интеграл, конечно, существует. Эта экспонента при больших х, при х мячности
бесконечности, эта экспонента убывает супер быстро, гораздо быстрее, чем эта штука возрастает.
То есть она их доминирует. Интеграл поэтому существует, он конечен. А раз он существует
и конечен, то интеграл от нечетной функции равен нулю. Просто интеграл от положительной части
интеграла от happy Negro mastеч率 равны. И по модулю вы получаете равный, podía типа у нас 0, 2000.
Здесь у нас к интегралу плотность написана. Это вмысленность плотности нашего
нормального распределения, тогда плотность равен единице и значит наш весь,
наш вся сумма просто равна а. Есть ли какие-то вопросы?
Вопросов нет, прекрасно. Давай теперь тогда, в общем, примеры основных
распределений мы взобрали, для них мы математические ожидания посчитали,
давайте поговорим про свойства математического ожидания.
Это на самом деле просто свойства интеграла Либега, так как математожжение просто интеграла
Либега, это те свойства, которые я перечитаю, просто стандартные свойства интеграла Либега,
которые вам Иван Геррихович, я надеюсь, тоже рассказывал, поэтому я их буду давать без доказательства,
может докажу, что только специфическое, именно то, что полезно именно для теории вероятности,
может быть, вам эти свойства Иван Геррихович не рассказывал, я их докажу, а стандартные
свойства интеграла Либега доказывать не стану. Итак, во-первых, если две функции,
две случайной величины, таковы, что одна больше, чем другая, и оба мат ожидания существуют,
они могут быть бесконечной, то мат ожидания СЕ больше не очень мат ожидания ЭТО, да, здесь,
мы когда пишем неравенство, имеется в виду, что бесконечности мы тоже умеем сравнивать,
то есть плюс бесконечность больше, чем минус бесконечность, и плюс бесконечность равно плюс
бесконечности, а минус бесконечности, все равно минус бесконечности. То есть здесь никого не
противоречит тем, что они могут быть бесконечными, мы их, тем не менее, тоже можем сравнить.
Значит второе, если кси не отрицательно, кси не отрицательно случайно вылечена, то и
моджетание кси тоже не отрицательно, но это просто следствие предыдущего свойства. Явное, да,
моджетание от нуля это ноль. Здесь мы напишем, здесь моджетание с двух сторон, мы получим ровно,
получим ровно. Вот это утверждение. Кроме того, если, если к тому же, не только кси больше
нуля, еще известно, что моджетание кси равно нулю, если кси больше нуля и моджетание кси равно нулю,
то тогда вероятность того, что кси равно нулю, равна единице.
Ну вот, вторая часть, это, наверное, специфическое свойство, именно для, который мы используем
категорию при стучании вечен, давайте его докажем. Значит, ну, если первое свойство очевидно,
первая часть второго свойства очевидна, ну, кроме того, она просто следует из определения
математического, из определения интеграла, что интеграл от не отрицательных функций является
не отрицательным величиной, то со вторым, казалось бы, сегодня так просто, но, тем не менее,
не должно быть сложно. Смотрите, значит, итак, пусть кси не отрицательно, тогда его можно приблизить
снизу простыми случайными величинами. То есть существует последовательность не отрицательных
простых случайных величин кси-н, которые снизу стремятся к си снизу, то есть они
последовательность не убывает и стремится поточь на кси. Вот, значит, раз они не превосходят кси,
то их мат ожидания тоже не превосходит мат ожидания кси. При этом мат ожидания кси равно 0,
и следовательно, мат ожидания кси-н тоже равно 0. Но для простых случайных величин не отрицательных,
если уж так вышло, что у них мат ожидания 0, то они равны 0 без вариантов. Почему? Потому что
простая случайная величина, это есть что такое? Это есть сумма каких-то там окатых, умножить на
вероятность того, что кси-н попал в какое-то там множество б-каты. Эта сумма хонечная, и вот эти все
окаты и не отрицательные. То есть все слагаемые в этой сумме не отрицательные, так этому мат ожидания,
и она равна 0. Сумма не отрицательных чисел равна 0 тогда и только тогда, когда все окаты равны 0.
А это означает, что действительно просто кси-н в точности равно 0.
Но раз кси-н стремится по точечкам кси, и они все равны 0, то это означает,
что вероятность того, что кси равно 0, тоже равна 1. Что это елос.
Так, есть какие-то вопросы?
Хорошо. Третье тоже, наверное, довольно специфическое свойство. Давайте мы его тоже докажем.
Значит, если существует мат ожидания кси, то для любого события ASF существует мат ожидания кси на
индикатор А. То же самое верно с конечностью. То есть, если модульма от ожидания кси меньше
бесконечности, то для любого ASF модульма от ожидания кси на индикатор А тоже меньше бесконечности.
Но не выглядит сложным. Смотрите, что значит, что существует мат ожидания кси.
Существует мат ожидания кси. Значит, либо мат ожидания кси плюс меньше бесконечности,
либо мат ожидания кси минус меньше бесконечности. Но давайте без ограничений
обществе, это случая симметричная, без ограничений обществе будем считать, что мат ожидания кси плюс меньше бесконечности.
Значит, не ограничивая без ограничения обществе, мат ожидания кси плюс меньше бесконечности.
Вот, теперь, что происходит, когда мы тоже представляем в таком виде, в виде разницы двух
неотрицательных величин, кси умножительного и��икатора. Ну смотрите, понятно, что можно вот так поступить,
можно сказать, ну кси, это же кси плюс минус кси минус.
Дальше раскрыть скобки. Мы получим кси плюс на индикатор А минус кси минус на индикатор А,
и обе величины будут неотрицательны. И вообще понятно, что кси плюс на индикатор А φbahn(?)
как Ciaoce bare
confirming plum as well
animals qil meant it.
bilcongfu
use
Yes.
fy
fee plus
fifty
what's going
what's
род
뭐 это же binary
скидка
золото на SE
плюс неBrief
aley
тем более
Да, well,
вовсе говоря,
не превосходит
отзезания
все плюс
так как кси-плюс на индикатора а не превосходит кси-плюс, да вот случайно
величину множество индикатора уменьшится, то есть для каких-то
положительных значений они просто обнудились, вот поэтому по свойству 1 из
вот этого следует нерадость для математических ожиданий, ну и все, а значит и
мат-зани кси-плюс на индикатора тоже конечна и тогда существует мат-ожидание.
существует... вот понятно, что для конечности всё то же самое, то есть
рассуждение, для случая когда модуль мат-ожидание кси меньше бесконечности, оно просто
аналогично, вы там скажете, ну окей, раз мат-ожидание кси конеч
кси минус меньше бесконечности, а дальше вы проделываете вот это рассуждение и для кси плюс,
и для кси минус. То есть вы делаете вывод, что ага, ну значит им от ожидания кси плюс на индикатора
конечна, им от ожидания кси минус на индикатора тоже конечна. Вот собственно и всё. Значит,
доказательства в случае модуль кси меньше бесконечности аналогично. Так, есть какие-то
вопросы. Можно вот начнить, кси с минусом это отрицательные числа. Еще раз. Кси с минусом это
отрицательные функции. Ну, сейчас. Кси с минусом это тоже не отрицательная функция. Значит, кси с
минусом это максимум из минус кси и ноль. Да, то есть это соответствует отрицательным значениям
случайно величины кси, но сама функция является не отрицательной. Спасибо. Ещё какие-то вопросы?
Хорошо, давайте двигаться дальше. Значит, следующее свойство. Следующее свойство.
Значит, если существует от ожидания кси, то модуль от ожидания меньше, чем раньше от
ожидания модуля. Да, но это как обычно классическое свойство интеграла Либерго. Я думаю, что Иван
Генрихович вам рассказывал. Дальше, линейность. Во-первых, что можно выносить константу из-под
математического ожидания. Опять надо сказать, что если моджедание кси существует, то для любого
действительно у числа c, мод ожидания от c кси равно c на мод ожидания кси. Да, но здесь понятно и дело,
что в случае, когда моджедание бесконечно, нужно понимать, что знак может меняться. То есть,
если у вас от ожидания кси равно плюс бесконечности, а c отрицательное число,
то значит, это будет тогда уже минус бесконечности. Я думаю, что вы понимаете, как мы тут
арифметические операции с бесконечностью производим. Хорошо, теперь с суммой. Ну,
можно какое-то более общее утверждение сформулировать, но нам это не потребуется.
Давайте считать для простоты, что оба мат ожидания конечны. Значит, если мат ожидания кси по модулю
и мат ожидания от по модулю конечны, то мат ожидания от суммы равна сумме мат ожиданий.
Тоже обычное свойство про линейность интеграла Ребега, которую вам Иван Генривович обязан был
рассказать. Что еще? Что еще? Давайте еще одну способность сформулируем. Значит, пусть существует
мат жаних си и мат ожиданий этой, пусть они конечны опять. А в шестом пункте, если у нас
мат ожидания бесконечная, но одного знака, в чем проблема? Давайте так. Понятно, что это шестой пункт,
его можно обобщать. В максимальной обществе нужно сформулировать несколько разных ситуаций.
То есть плохая ситуация, это когда оба бесконечны, причем разных знаков. То есть если они оба плюс
бесконечность, все ок, если они оба минус бесконечность, тоже все ок. Дальше, если один из них бесконечно,
второй констант, это тоже все ок. Давайте я сформулирую в более общем виде, как-то сказать коротко,
сейчас попробуем. Значит, нам не подходит только случай, пусть мат ожиданий си и мат жаних существует.
И к тому же, к тому же, что еще надо сказать? Надо сказать, что они, если и бесконечно, то одинаковых знаков.
Значит, и либо
не более одной, сейчас, как же просто сказать.
И так надо сказать, значит, и если они оба бесконечны, то одинаковых знаков. И если оба бесконечны,
то одинаковых знаков.
Тогда мат ожидания сумма равна сумму мат ожиданий. Наверное, так. Хорошо.
Значит, пусть теперь случайные величины имеют конечные математические ожидания,
и пусть для любого а из f мат ожидания кси на индикатор а меньше набрано, чем мат ожидания эт на индикатор а.
Тогда кси меньше набрано, чем эт с вероятностью 1.
Тоже довольно специфическое свойство, поэтому давайте его докажем.
Давайте докажем. Давайте рассмотрим такое вот множество b или a.
Обычно множество событий ab означает множество таких омега, что кси от омега больше, чем это от омега.
Значит, понятно, что это элемент f. Кстати, кто может объяснить, почему это элемент f?
Почему это событие?
Потому что разность измерима.
Yes. То есть вы из случайных величин кси это составляете вектор,
потом применяете к нему баррельскую функцию разность и получаете снова измеримую функцию.
То есть кси минус это, это измеримая функция.
У вас здесь интересует не что иное, а как прообраз луча от нуля до бесконечности.
Нам надо на самом деле доказать, что вероятность этого множества это 0.
Нам надо доказать, что вероятность этого множества это 0.
Ну вот у нас есть вот это утверждение, которое верно для любого a из f.
В частности, мы можем взять именно это a, которое мы только что выбрали.
Мы знаем, что мы от ожидания кси на индикатор a меньше набрано, чем от ожидания это на индикатор a.
Ну вот давайте теперь по линейности выясним, что это означает на самом деле,
что мы от ожидания это минус кси на индикатор a больше набрал нуля.
Теперь давайте поймем, что это за случайная величина такая.
Это минус кси на индикатор a.
Смотрите, значит индикатор a равен нулю, если кси больше чем это.
То есть это либо 0, если кси больше чем это.
Если же кси меньше набрано, чем это.
Значит, если же кси меньше набрано, чем это, то...
Сейчас.
А, наоборот, глупости говорю.
Значит, здесь же у нас индикатор того, что кси больше, чем это.
Значит, это 0 наоборот, если это не выполнено.
То есть, если кси меньше, но чем это.
Прекрасно.
А если этот индикатор выполнен, то есть, если все-таки кси больше, чем это,
то вот эта разность, она отрицательна.
То есть меньше нуля, если кси больше, чем это.
Ну короче говоря, это не отрицательно, это неположительная случайная величина.
В любом случае она не может быть положительна.
А значит, мы от ожидания от нее тоже не может быть положительной.
У нас есть теперь с вами два нераеста.
С одной стороны, мотождание не отрицательное, с другой
стороны оно неположительное, значит оно ровно нулю.
Ничего не остается, кроме как ему равняться нулю.
А у нас есть свойство номер два, которое мы вот здесь
сейчас применим, которое говорит, что если случайно
миличная не отрицательная, и мотождание от нее ноль,
то она просто равна нулю.
У нас здесь именно такая ситуация, но только не положительная,
но неважно, свойства все равно работают.
У нас случайная величина не положительная, а мотождание
от нее равно ноль.
Значит по свойству два.
Вероятность того, что это случайная величина равна
нулю равна единице.
Значит в каком случае эта случайная величина равна
нулю?
вот у меня здесь написано это случайно равна, случайно числа равна нулю только
если кси меньше 0 чем это, вот только в этом случае она равна нулю, то есть это в
точности вероятность того что кси меньше 0 чем это, что и требовалось, так
есть ли вопросы?
хорошо давайте теперь независимые случайные величины, последнее наверное
свойство и потом я напомню всякие теоремы которые у вас были в курсе с Иваном
Генриховичем, которые мы будем тоже использовать поэтому я их напомню
значит итак, если кси не зависит от этом и ну боксом давайте пусть конечным от
ожидания кси, мы от ожидания это
важно ли конечный, сейчас дайте я собираюсь в кучку
наверное неважно, давайте просто они будут существовать, давайте они просто будут
существовать, тогда мы от ожидания произведения
не очень понятно что такое, сейчас, сейчас секунду
нет, давайте они будут конечны, прошу прощения, пусть они будут все-таки
конечны, значит тогда мы от ожидания произведения равно произведению мы от
ожидания, это на самом деле, я докажу сейчас, это просто явное следствие
Фубини, значит смотрите, мы от ожидания произведения
это есть не что иное, как интеграл
от
кси отомега, это отомега dp
а это есть не что иное, как интеграл
значит мы можем заменить здесь кси отомега на х и это отомега на у, интегрировать
пр2, интегрировать пр2 d от декартового произведения по кси по это, ну это теорема
о замене переменов интегралили бега в точности, значит, то есть на самом деле
здесь должен стоять мера вектора кси это, распределение вектора кси это, но в силу
независимости распределение вектора это езде картового произведения, распределение
случайных величин кси и случайных величин это, то есть мы вот в этот момент применяем
независимость, говоря, что распределение вектора кси это, это в точности
это в точности декартового произведения распределения по кси по это
я сейчас докажу это свойство, потом я еще про теорему о замене переменных и
попробую в максимально общем виде сформулировать, в частности для функций
многих переменных, как это здесь происходит тоже, так, хорошо, значит, теперь мы применяем
теорему фубини и говорим, что это просто интеграл по r от x dp кси, множительно интеграл
по r от y dp это, это есть просто мы от ожидания кси нам от ожидания это
вот, ну давайте сразу тогда я сформулирую теорему о замене
переменных, а после этого еще напомню тему либега, а мы же о реме сходимости
теорема о замене переменных
так, значит, пусть у нас есть, пусть у нас есть, сейчас я собажу, пусть есть случайный вектор
x, размерности n или давайте лучше k, размерности k и пусть phi это баррельовская функция,
которая действует из rqr, тогда если существует мот ожидания от phi от x, то оно равно интегралу
по r степени k от phi от x dpх, так сейчас, я прошу прощения, одну секунду
прошу прощения, да, вот, собственно,
мы ее здесь и применяли, то есть мы говорили, что возьмем случайный вектор x равный кси это
и применю к нему функцию phi, которая равна просто произведению, да, тогда мот ожидания от phi от x
это в точности вот это вот мот ожидания от кси это, и по теореме о замене переменных мы получаем
вот такого интеграла, но я даже не знаю, доказывается эта теорема в точности также, как мы доказывали
аналогичное утверждение вот здесь, да, когда мы говорили, что плотность можно вынести из-под дифференциала
доказательственно аналогична, но давайте как-то коротко, что ли, по нему пробежимся, если Иван Генрихович
его не рассказывал, видимо, не рассказывал, раз вы спрашиваете, значит, давайте попробуем коротко
пробежаться, ну, во-первых, если phi это индикатор, пусть phi от x это индикатор того, что x принадлежит
баррельскому множеству b для некоторого баррельского множества rk
вот, тогда что такое мот ожидания phi от x, большое, это мот ожидания индикатора того, что x большое
то есть это в точности вероятность того, что x принадлежит b, значит, по определению интеграла
Лебега теперь по px, это есть не что иное, как интеграл от индикатора того, что x маленькое принадлежит b
умножить на, ну, для простоты давайте напишу так, интеграл по b от dpx
да, это тоже определение интеграла Лебега, то есть когда мы находим интеграл Лебега от индикатора того, что x принадлежит b
по какой-то мере, мы получаем просто меру этого множества
чтобы было еще понятнее, я давайте еще добавлю одно выражение, значит, по определению p от x принадлежит b, это просто в точности px от b
да, вот так должно быть понятнее, что и требовалось, собственно, да, это и есть интеграл от phi от x dpx
хорошо, теперь если phi простая, да, по линейности
я не буду расписывать с вашего позволения, понятно, что надо просто применить линейность как для левой части, так и для правой части
по линейности интеграл Лебега
вот, теперь если phi неотрицательная, тогда можем приблизить простыми, неотрицательными снизу
и дальше просто переходим в предел, давайте это аккуратно проделаем
значит, в ожидании phi от x, так как phi простая функция, phi от x будет простая случайная величина, она будет принимать только конечное количество значений
phi n, прошу прощения, phi n будет простая случайная величина, которая принимает конечное количество значений
при определении интеграла Лебега, или то же самое правление вматоожидания
вматоожидание phi от x, это есть просто предел при ностримящейся бесконечности, вматоожидание phi от x
для phi n мы умеем заменять на этот интеграл вматоожидания, получаем интеграл от phi от x dpx
опять, сил тоже phi n простые функции, мы можем занести предел внутри интеграла и получить интеграл от phi от x dpx
Ну и, наконец, для произвольных функций по линейности мы представляем фи как фи плюс минус фи минус,
где фи плюс, фи минус не отрицательные.
И понятно, что фи от х плюс, так я снизу написал, давайте я сверху напишу как обычно,
фи плюс минус фи минус, фи от х плюс это просто фи плюс от х, а фи от х минус это просто фи минус,
поэтому мот ожидания от фи от х, это просто мот ожиданne от фи плюс от х, минус мот ожидания
от фи минус от х, для фи плюс и для фи минус мы всё доказали, поэтому это есть
интеграл от фи плюс от х dpx, минус интеграл с фи минус от х dpx по определению, интеграл
лебего это интеграл от phi dx, что и требовалось, есть какие-то вопросы. Хорошо, значит, все, что осталось,
это я давайте напомню теорему лебего по мажорируемой сходимости, просто напомню,
тоже как свойства математического ожидания, и потом, если успеем, хотелось бы еще успеть вернуться к
двум примерам, во-первых, для биномиального распления и нормального распления, и увидеть вот то,
что я сказал, что можно посчитать от ожидания проще, уже имея то, что те свойства, которые я сформулировал.
Что она говорит? Она говорит, что, предположим, у вас есть последовательность случайных величин
ксен, которые мажорируются некоторым случайным величиной r, то есть модулем ожидания ксен меньше
чем какая-то случайная величина r, это мажорит, который конечен. Значит, тогда, если вероятность того,
что ксен стремится кси равна единице, ну, значит, что такое вероятность того, что ксен стремится кси
равна единице, это вероятность события, да, множество таких омега, что ксен от омега стремится кси от омега.
Если это вероятность равна единице, то тогда и мажорит ксен стремится к мажорит кси,
при этом мажорит кси, конечно, конечна.
И еще можно сказать, что мы от ожидания модуля разности между ксеноксистами и с к0.
Вот, это те ремы ли вега мы уже идем сходить, из которых вы прекрасно знаете. И теперь давайте вернемся к
двум примерам, к нормальному распределению и к бенминальному распределению, к нормальному
распределению. Поговорим, как проще найти мы от ожидания для этих двух распределений.
Во-первых, пусть кси имеет бенминальное распределение с параметром n и p. Заметим вот
что. Заметим, что что такое бенминальное распределение с параметром n и p? Это количество
успехов при n подбросках монетки. Вот вы n раз подброски независимо вероятность решки равна p,
ваша случайная величина это количество решек. Тогда вы можете сказать, ну окей, давайте возьмем n
случайных величин, кси1, кси n, которые имеют распределение Bernoulli с параметром p и которые
независимы. Это индикаторы решки. Да, кси1 это индикатор того, чтобы при первом подброске получили
решку, кси2 это индикатор того, чтобы при втором подброске получили решку и так далее. Понятно,
что если вы их сложите, вы получите ровно количество решек при n независимых подбросках.
То есть, иными словами, их сумма имеет то же самое бенминальное распределение с параметром n и p.
А когда у вас две случайные величины, имеют одно и то же распределение, у них одинаковый
мат ожидания. Поэтому мат ожидания кси, это в точности мат ожидания суммы ваших ксиитых.
По линейности мат ожидания, мат ожидания суммы всегда равна сумме мат ожидания. Вам даже
независимость нужна. Если бы они были зависимы, было бы все то же самое. Мат ожидания суммы равна
сумме мат ожиданий. У Bernoulli распределение мат ожидания равно p, то есть у всех этих ксиитых
мат ожидания равно p, значит получаем n, p. Да, вот без всяких вычислений, никакие вот эти трюки
с биномиальным коэффициентом не нужно производить, чтобы получить ответ. Есть какие-то вопросы?
А теперь возьмем нормальную случайную величину с параметром i7 в квадрате. И заметьте, вот какую
интересную вещь, что если мы возьмем теперь случайную величину, стандартную нормальную,
обозначим ее это, и возьмем и умножим это на корень i7 в квадрате и прибавим а, то мы получим
нормальное распределение с параметром i7 в квадрате. Почему так? Ну по определению, если вы посмотрите на
функцию распределения, вот этой случайной величины, корень i7 в квадрате на это плюс а в точке x,
это есть не что иное, как вероятность того, что корень i7 в квадрате на это плюс а меньше 0,
чем x. А это есть вероятность того, что это меньше 0, чем x-a поделить на корень i7 в квадрате.
Дальше, подставляя в определение функции распределения, которая для нормального
распределения есть просто интеграл, от минус бесконечности до вот этой вот точки x-a поделить на
корень i7 в квадрате, от плотности 1 поделить на корень i7 из 2 pi e в степени минус t в квадрате
пополам dt. Если вы сделаете замену, обозначите t за y-a поделить на корень i7 в квадрате,
то получите в точности интеграл от минус бесконечности до x от 1 поделить на корень из
2 pi e в степени минус y-a в квадрате поделить на 2 si в квадрате dy. А это есть как раз функция
распределения случайно-вечной кси в точке x. То есть, иными словами, вот это линейное
преобразование, которое вы можете проделать стандартным нормальным распределением, вам дает
любые параметры нормального распределения. То есть, умножив на корень из второго параметра и
прибавив первый параметр, вы получите нормальное распределение с параметром i7 в квадрате.
Это позволяет считать мотожидание только для стандартного нормального распределения,
потому что если вы знаете, так как вы знаете, что мотожидание это равно нулю, то из этого сразу
следует, что мотожидание кси совпадает с мотожиданием от корня i7 в квадрате это плюс а,
но совпадает то, что у них распределение одинаковое. И у кси нормальная с параметром i7 в квадрате,
и вот этой вот суммой тоже нормальная с параметром i7 в квадрате. А здесь по линейности вы получаете
корень из i7 в квадрате на мотожидание это и плюс а, и так как мотожидание это это ноль,
вы получаете а. То есть, достаточно было считать мотожидание только для стандартного нормального,
это чуть проще. Давайте вспомним, как вы там считали мотожидание для просто нормального
распределения. Вот нам тут пришлось представлять вот эти суммы. Но если бы у нас не было вот этого а,
то мы бы сразу сказали, что это функция нечетная. Вот эта вот функция под интегралом нечетный,
просто сразу сказали, что это ноль. А когда у нас есть а, ну вот нам пришлось вот так вот вычитать
а и добавлять а, и потом замечать, что вот это ноль. Так бы у нас вот этой операции бы не,
мы бы не проделывали, если бы мы умели бы сразу выводить из стандартного нормального распределения
любое другое. Так, ну на этом все, что я хотел рассказать сегодня. Если какие-то вопросы,
пожалуйста задавайте. Пока временно, к сожалению, лекции я вынужден перенести в онлайн-формат.
В общем, посмотрим, что будет происходить дальше. Смогу ли я выходить в аудиторию в какой-то
момент или нет. В общем, ближайшая лекция будет тоже в онлайне. Прошу за это прощение и надеюсь,
что это не сильно влияет на качество. Я буду своевременно выкладывать видеозаписи и конспекты
лекций, которые я пишу во время лекции. Надеюсь, что это будет даже удобнее, чем обычный формат.
Пожалуйста, не стесняйтесь во время лекции задавать. Можно больше вопросов, если они у вас есть,
не откладывайте это делом. Но потом всегда проще разобраться во время лекции, задать какие-то
вопросы, чем потом не понимать какие-то места при просмотре видео. До встречи в следующую субботу.

В прошлый раз мы с вами такое утверждение сформулировали, что при передаче классической
информации через закодированное в квантовые носители возникает такая граница Холева. То есть
взаимная информация между классическим входом, классическим выходом не превосходит вот такой
средней энтропии по ансамблю минус среднее состояние ансамблю минус средней энтропии
состояний входящих в этот ансамбль. Для доказательства мы с вами две леммы сформулировали.
Лемма одна заключается в том, что относительно энтропии она обладает свойством монотонности,
и у нас была интерпретация этого свойства, поскольку относительно энтропии квантовая имеет
смысл расстояния, хотя она не является расстоянием, потому что она несимметрична. Но под действием
некоторого отображения, которое есть квантовый канал, у вас происходит образ вот этих двух точек,
он куда-то вот сюда перейдет. Вот это относительно энтропии не увеличивается. Само доказательство
довольно сложное, в книжке Холева есть, но с физической точки зрения понятно. Ну и вам как
математикам тоже. Вот это вот это сжимающее отображение, фи, оно сжимает, то есть относительная
энтропия не увеличивается. И вторая была, которая является частным случаем, вот этой первой,
когда это отображение есть просто взятие частичного следа. Взятие частичного следа это
тоже канал, просто разменность входа больше, чем разменность выхода. Получается тогда вот
свойство такое СРОАБ относительно энтропии Сигма Б больше либо равно, чем СРОАРОБ. Здесь в
качестве этого фи выступает взятие частичного следа по Б. Так, что еще мы с вами сделали в прошлый
раз? Мы вспомогательную конструкцию ввели. Вот эти буквы Х из входного алфавита,
мы с вами как интерпретировали? Мы ввели вспомогательное пространство, размерность
которого равна размеру алфавита. Это у нас был вспомогательный пространство HP Preparation. Вот его
размерность есть размерность алфавита. Что еще мы с вами сделали? Мы с вами аналогичную
процедуру для Y сделали. То есть мы ввели вспомогательное пространство HM, M от английского
measurement. И это была у вас разменность выходного алфавита. И рассмотрели такое эффективное
состояние R, P, Q, M такого вида. Сумма по Х, по Х это ваша вероятность, с которой буквы возникают.
Вот теперь ket вектора Х это элементы вот этого вспомогательного пространства. И они
ортогональные друг другу, ортонормированные. Дальше идет ρх, те состояния, в которые вы
кодируете. И дальше идет 00. Это пока еще вы не провели измерение. Вот такой вот объект мы с
вами посмотрели. Тут 00 тензорно умножается на некоторый объект. Мы сначала с этим объектом
повозились и увидели с вами вот такую вот штуку, что энтропия Ро, П, Ку относительно
энтропии Ро, П, Ку. И Ро, П, тензорно Ро, Ку. Правильно пишут хоть. Она у вас чему равна? Она как
раз равна вот правой части в утверждении, которую мы хотим доказать. То есть это С, Ро среднее,
минус сумма по Х. Вот это мы с вами сделали. В том мы сказали. Тут видите, буковка М здесь просто
получается тензорномножение вот таким. Поэтому та величина, которая здесь стоит относительно
энтропии, она вся еще равна вот таком величине Р, П, Ку, М. А тут надо нам придумать, куда добавить это.
Ку, М. Я это М могу приписать или к П, или к Ку. Вот надо тут правильно приписать. Если вдруг ошибемся,
то потом поправим. Вот здесь в принципе мы и остановились. Дальше был набросок. В чем он
заключался? Он заключался в том, чтобы теперь применить измерение. То есть записать некоторую
операцию, которая даст результат измерения. Значит включаем измерение в рассмотрение.
Включаем измерение в рассмотрение. Вот смотрите, как я это сделаю. Я опишу вот это отображение ФИ,
которое потом буду использовать в лейме 1. Вот в таком виде оно будет на вход брать
некоторый оператор. На выходе выдавать мне вот такое вот выражение. То есть я сейчас написал
представление Крауса для канала ФИ. А сами операторы Крауса будут выглядеть вот таким вот образом.
На часть, которая связана с приготовлением, никак не влияем. Тут единичный оператор. На квантовую
часть мы действуем корнем из EY. Это корень из того POV элемента, который в измерении сидит. А здесь
будет такой оператор сдвига некоторый. Сейчас объясню чуть такое. Сумма по х, х. Ну ладно,
х не очень хорошо. Давайте другой элемент, потому что это с выходным алфавитом связано. Давайте
Y'. А тут будет Y'Y'. Ну плюсик, как надо понимать, плюсик по модулю размерности выхода выходного
алфавита. Mod, Dim, H, M. Так, конструкция понятна пока? Значит, вот это что за матрица у последнего
члена? Это просто сдвиг на Y. То есть это матрица перестановок. Если вы в стандартном байсе
запишете, это матрица перестановок. Понятно? Она унитарная. Унитарная. Значит, проверяем свойство,
нам же нужно, чтобы это было не просто вполне положительное отображение, которое мы сразу видим,
что является таким вот этой формой, но чтобы еще след сохранялся. То есть нам нужно проверить
условия, что сумма по Y, АY, крестик АY есть единичный оператор. Вот что нам нужно сделать.
Теперь как мы это видим? Это единичный оператор где? В PQM. Ну понятно, для P единичка так и осталась.
EY, корень из EY, умножить на корень из EY, будет EY. А сумма всех EY есть единичный оператор. Почему?
Потому что это измерение, ее VEM-элементы суммируются в единичный оператор. Поэтому на втором месте
тоже будет единичный. А на последнем всегда единичный, потому что здесь вы умножаете матрицу на
свою унитарную, на свою эрмитовосопряженную. Получается единичный оператор. Понятно? Значит это канал, это канал.
А значит для него верно вот это вот первое свойство. Давайте теперь этот канал применим к нашему
состоянию вот этому PQM здесь и применим к вот этому тензорному произведению справа. Понимаете?
Применяем теперь phi к этим величинам. Так, как действует phi на вот это RO-PQM?
С классической частью, с приготовлением ничего не происходит. Просто переписываем ее и все.
Потому что там действует единичный оператор. Дальше. С квантовой частью что происходит?
Слева умножается на корень из EY, справа домножается на корень из EY. Напомню, что EY-эрмитовый,
поэтому крестик не ставь. Что произойдет с этим нулем? Этот ноль просто заменится на Y и все.
Это и есть этот Y. Результат измерения. Просто теперь вы его представили в виде некоторого
объекта во вспомогательном Гильбертом пространстве. Ну в принципе этот объект
целиком можно обозначить RO-PQM, чтобы не писать phi все время. Значит, с левой частью мы знаем,
как поступить. Теперь нужно к правой части применить вот это все. Значит, если я phi,
подействую на RO-QM. Что здесь произойдет? Но RO-P не поменяется. Так и останется.
С квантовой частью что произойдет? У вас изначально вот эта вот часть как записывалась. Давайте
вспомним. Это была сумма по X, PX, RO-X, тензор на 0,0. Вот как она записывалась. RO-QM. Вот в прошлый
раз это делали. И теперь применяем этот phi. Значит, здесь опять корни из EY возникнут.
А это есть среднее состояние ансамбли. Кстати, можно коротко так написать. Ну ладно, уже написал.
Вот это все. Это есть корень из EY, RO среднее, корень из EY. Сообразили.
Теперь. Сейчас. Правильный вопрос. EY должны остаться. EY должны остаться. Совершенно верно. Спасибо.
Спасибо. Исправили. Что результат измерения не фиксирован уже. В принципе, мы первую лему теперь
используем. Говорим, что граница Холева, которую мы получили вот здесь подстановкой RO-P-QM, а тут вот
этого выражения. Заменяется на некоторое другое. Давайте я запишу последовательность операции,
которую мы делаем. Значит, вот начнем с этой границы Холева. S, RO среднее, минус сумма Px, Px, S, ROx. Мы с вами
помним, что это взаимная информация RO-P-QM, RO-P, RO-QM. Теперь мы применяем нашу лему и конкретный
вид канала phi применяем. Что мы получаем? Мы получаем S. Вот здесь будет стоять тогда RO-P-QM.
Давайте я перепишу сумма Pxy, Px, xx, корень из ey, rox, корень из ey, yy. Это мы переписали первое
выражение в этой относительной энтропии, а второе выражение в относительной энтропии это будет что?
RO-P. RO-P это вот такая величина сумма Px, Px, xx. Первая часть тензорное произведение. Теперь вторая
часть QM. Вот так вот ее запишем, как корень из ey, RO среднее, корень из ey и еще yy, но сумма по y, yy.
Так вы тогда себе, если здесь мы исправили сумму по y, тогда здесь тоже она останется.
Закрываю скобочки для этой относительной энтропии. Видите, до двух палочек первой
выражения, после двух палочек второй выражения. Теперь пользуемся второй леммой. Теперь я могу
взять частичный след по подсистеме Q, по квантовой. Выкинуть квантовую систему вообще из рассмотрения.
Беру частичный след по Q, относительная энтропия может только уменьшиться. Что я должен сделать?
Я должен взять след по вот этим вот частям. По вот этим частям я должен взять след. Вы видите,
что здесь получится. Здесь получится, смотрите какая величина, напишу под этим неравенством,
след от корень ey, rox, корень из ey. Под знаком следа можно переставлять. Получится ey, rox,
а это есть вероятность получить исход y при условии, что на входе была букой. То есть получится
условная вероятность. Ну и смотрим, что тогда получится. Будет относительная энтропия каких
величин. Значит здесь будет сумма по x, по y, по x. Дальше стоит py при условии x, xx, yx. Первая
часть упростилась. Вторая часть. Что это такое? Вторая часть. Здесь тогда будет тоже вот эта
вот условная вероятность. Уже со средним состоянием вот этого ансамбля. Это мне
нужно еще подумать, что это такое. Ну давайте я пока просто напишу. Сейчас сообразим. Сумма
по y, след ey, rox средняя и yy. Теперь нам нужно получившуюся величину упростить далее. С первым
членом все понятно. Смотрите, умножаем px на условную вероятность pyx, получаем просто вероятность
pxy. Понятно? По определению условной вероятности. То есть здесь у нас получится с вами что за объект.
Сумма по x, по y, pxy, xy и y. То есть это будет как раз-таки хороший объект в плане совместного
распределения вероятностей x и y. А вот что будет в правой части? Давайте посмотрим. Тут я не
очень понимаю, что такое будет ey, rox средняя? Ну вот вы имеете в виду объединить эту сумму.
Так давайте запишем.
Сейчас сообразим. Это сумма по x, px, py при условии x. Вот так вот это все понятно?
Не понял. Это будет, значит, вот это все вместе. Это есть pxy, сумма по x дает маргинальное распределение.
Значит, все, вторая часть тоже приняла очень хороший вид, очень красивый. Смотрите, первая
часть уже записана в том виде, который нам нужен. Это диагональная матрица, на диагонали которой
стоят px. А второе выражение приобрело тоже красивый вид. Это сумма по y, py, yy. Теперь смотрите, все
матрицы, которые сюда входят, они все диагональные. Все диагональные. Вот это py умножается на yy,
yy ортонормированные, xy тоже ортонормированные. Это получается классический аналог квантовой
относительной тропии. То есть это расхождение Кульбека Лейблера. У этих двух величий. А что это такое?
Здесь ставим знак равно для всего выражения с. Вот я здесь его продолжаю. И это все есть не что иное,
как просто расхождение Кульбека Лейблера для каких распределений? С одной стороны у вас стоит
распределение pxy xy, а с другой стороны у вас стоит произведение распределений px и py. Не знаю,
как это нарисовать красиво. Вот это? Это просто распределение вероятности. Вот когда я возьму
набор таких вероятностей с элементами, перечислив элементарные исходы x и y, то я получу распределение
вероятности. Расхождение Кульбека Лейблера оно для распределения вероятности делается. Вот одно
распределение вероятности, вот другое. Произведение маргинальных, напишу так,
произведение маргинальных распределений вероятности. И это есть не что иное, как hx
плюс hy минус hxy. То есть та самая взаимная информация. Так, последний шаг, понятен?
Екатерина, вопрос. И таким образом мы с вами закончили доказывать границу Холева. Смотрите,
с чего мы начинали? Мы начинали с вот этого выражения, а закончили взаимной информации
между x и y. То есть здесь присутствуют только классические объекты, вот в чем его нетривиальность.
Тут классические объекты. Распределение букв на входе и распределение результатов измерения на
выходе. А здесь присутствуют только квантовые объекты. Ро x, те операторы плотности, в которые
мы кодируем буквы. Ро среднее, среднее состояние ансамбля. От исходного, от исходной классики
остались только вот эти вот px, вероятности букв находит. Вот в чем его нетривиальность.
Это означает, как следствие из границы Холева, что из одного кубита вы можете максимум извлечь
сколько бит информации. Один бит. Следствие, если размерность вашего, вашей квантовой системы
равняется d, то смотрите, что получается. s, ro, среднее, минус сумма по x, по x, s, ro, x.
Интропия не отрицательна, поэтому все это меньше и равной, чем s, ro, среднее. А энтропия любого
состояния в демерном пространстве не превышает логарифмы d. Вот и получается, что хотя вы в кубит
можете закодировать сколько угодно информации, а извлечь вы можете не больше, чем один бит. Если
у вас d-уровневая система, то не больше, чем логарифмы d. Вот такое вот нетривиальное утверждение.
Если состояние вот здесь вот ro, x чистые, то тогда вы получите здесь s, ro, x, 0 и достигнете вот этой
вот величины. Теперь немножко по обозначениям. От чего зависит вот эта величина? Она зависит от
задания ансамбля, поэтому ее еще называют Hi-величиной ансамбля. Давайте такое. Hi-величина,
она определяется как буква Hi, на вход ей вы подаете ансамбль. Что такое ансамбль? Это совокупность
вероятностей p и x, которыми буквы возникают, и ro и x, те квантовые состояния, в которые вы их
кодируете. Hi-величина этого ансамбля по определению есть вот эта s, ro средняя минус сумма px, px, s, ro, x.
Давайте теперь информационный смысл придадим этой процедуре. У нас была классическая буква,
мы ее закодировали в ro, x, потом мы ее направим на измерительное устройство, но мы можем
трактовать, что мы пропустили это ro, x через тождественный канал, канал без шума. Потом мы что
с вами сделали? Взяли и измерили те состояния, которые к нам пришли с помощью некоторого QVM,
получили выход классический y. Получается, что то количество информации, которую можно передать
через канал без шумный, не превосходит вот этой величины Холева, а она в свою очередь не
превосходит вот этот логарифм D. А если вы в качестве этого ансамбля что сделаете? Возьмете
ну например пусть у вас алфавит тоже демерный, то тогда вы можете вот это ro среднее сделать как
1 на D единичный оператор, сейчас напишу, 1 делить на D, сумма по х, где здесь уже алфавит демерный. И получится,
что вы тогда через канал без шума можете передавать максимум вот это вот логарифм D.
Не понял вопрос еще. Чтобы получить вот это ансамбль для того случая, чтобы Х величина равнялась
логарифм D. Сами вот эти ro х это в этом случае просто xx. Спасибо за вопрос,
то я не четко сформулировал. То есть если шума нет, то все прекрасно. Теперь вы можете модифицировать
эту задачу и сказать, я заменю тождественный канал на канал с шумом. Что тогда у меня получится?
У меня получится тогда передача классических сообщений через квантовые каналы зашумленные.
Так, у вас не возникает вопросов, где здесь типичность, потому что в классике были какие-то
типичные слова. Здесь тоже это понятие можно ввести. То есть здесь будут типичные
подпространства и будут условно типичные подпространства. Но я не знаю, насколько это вам
интересно. Стоит об этом говорить или лучше двинуться вперед. Сейчас я посмотрю по времени. 33
минуты прошло уже. Давайте до перерыва я чуть-чуть проговорю про вот эти типичности. Откуда они
возникают? Вот смотрите, что мы сделали. Мы буквом поставили соответствие матрице плотности. Если мы
возьмем какое-то слово, то ему в соответствии будет какая матрица плотность? РОХ1, тензорный и так далее, РОХn.
Теперь вероятность слова у нас чему равнялась? Это вероятность Х1 и так далее, вероятность Хn
перемножая. А это можно назвать РОВ, матрица плотности слова, которая отвечает этому слову.
Значит, если я умножу ПВ на РОВ и просуммирую по вот этим словам, то что я получу? Это получу я
тогда матрицу плотности среднего слова, правильно? И как она будет выглядеть? Вы видите, что здесь
отдельно расцепится суммирование по Х1, ПХ1, РОХ1 и так далее, а здесь будет сумма по Хn, ПХn, РОХn.
А все эти суммы, это есть просто среднее состояние ансамбля. То есть получается, что матрица плотности
среднего слова это произведение nt-тензорная степень, вот такой вот значок еще используют,
nt-тензорная степень РОСредняя. Это будет у вас РОВСредняя. А теперь вы можете посмотреть на этот
оператор. У РОСреднего есть спектральное разложение. Что такое спектральное разложение? Вы берете
сумму по каким-то там к, лямбда кт, а тут ψкт и ψкт, это собственные векторы, нормированные у этого
РОСреднего, лямбда кт и собственное значение. И вам нужна тензорная степень. Что такое спектральное
разложение? Теперь давайте эту n-тензорную степень смотреть. Что это такое? Это будет
сумма, можно здесь ее так написать, k1 и так далее kn, будет лямбда k1 и так далее лямбда kn,
ну и дальше будут всякие выражения длинные типа ψk1, тензорную умножить на ψk2 и так далее,
ψkn, это будет вот ket-вектор некоторый длинный, ну и в обратную сторону, ψk1, бра-векторы пошли,
ψkn. Так, теперь смотрите, вот эти лямбда это распределение вероятностей тоже по k. Теперь
вы когда берете это произведение, вот у вас получается, вот смотрите, если произведение вот
этих лямбд лежит в некотором, сейчас сформулирую, вот смотрите на эти лямбда, как раньше смотрели на
p от x, то есть вот эти распределения у вас будут либо типичные, либо нетипичные вот эти вот вероятностей.
Не очень понятно. А с чем надо сравнивать?
Да, здесь вероятности вот такие, но если вы посчитаете энтропию среднего состояния ансамбля,
что это такое? Это вот как раз таки энтропия вот этого распределения лямбда-ката, это просто есть
минус сумма по k, лямбда-ката, алгорифм лямбда-ката. И получается, что типичные вклады вот в эту сумму,
типичные вклады в сумму, это какие? Это те, для которых произведение вот этих лямбда-катах,
давайте k1, лямбда-kn, оно зажато между 2 в степени минус, вот это n, s, ρ, средняя, тут будет у нас еще
минус какая-то дельта, а тут будет 2 в степени минус n, s, ρ, средняя, плюс дельта. Вот теперь лямбда
играет роль. У нас была раньше вероятность слова, а теперь произведение вот этих лямб, вместо вероятности слова.
И получается, что вот эти вот, если вы возьмете Psi с вот этим мультииндексом k большое, k большое
это вот этот мультииндекс, то тогда типичное подпространство, это что такое? Ну или можно
сказать дельта типичное подпространство. Это вот такая вот штука, это будет у вас, ну давайте назовем
его h, n, дельта, потому что зависит от того, какую дельту возьмете, и это будет линейная оболочка
чего? Вот таких Psi-катах. Каких? Таких, что вот эти лямбда-каты зажаты вот в этом диапазоне. Таких,
что выполнена вот эта звездочка. То есть вот где типичные возникают всякие объекты. Типичное
подпространство можно ввести, а когда мы смотрим фиксированное слово и куда оно может перейти,
то мы получим с вами условно типичные подпространства. Но я не буду в это угубляться,
в книжке Холева про это все написано. То есть идеология, которая была в классике, переносится
на квантовый случай. Ну конечно же, там нужно больше внимания уделять этим объектам. Еще вот
таким образом, когда вы передаете классическую информацию с помощью квантовых объектов,
вам нужно использовать в качестве кодирования вот такие вот объекты из типичного подпространства.
Но это еще называется сжатие шумахера. То есть вот Шеннон показывал, как сжимать информацию,
кодируя в типичные слова. А шумахер показал, как можно, так скажем, сжимать квантовую
информацию, кодируя вот в эти типичные подпространства. Ну все, не буду дальше
углубляться в эту тему. В книжке Холева все есть. Делаем перерыв. Так, вот картинку стираю, хотя зря.
Теперь смотрите, что мы будем с вами делать. Мы сейчас будем смотреть сценарии передачи
классической информации через квантовый канал. Их на самом деле существует несколько.
И мы сейчас увидим отличия от классики. Вот в чем оно будет проявляться.
Значит, вот если использовать, давайте так, сценарий 1 передачи, сценарий 1. Если вы каждую
букву кодируете в матрицу плотности, а потом каждую матрицу плотности измеряете,
то вы реализуете самую простую схему, которая возможна. То есть вы, закодировав сообщение,
давайте я нарисую вот эти вот кружочки. Это будет матрица плотности. Вот пусть это ro x1,
это ro xn-1, ro xn. Вы закодировали какое-то слово w равное x1 и так далее xn в квантовом виде.
Потом вы отправляете его уже через какой-то канал, который может шум привносить. Здесь я тоже напишу
буквку phi, но это phi отличается от того, что было в доказательстве. Понятно? Это phi показывает
уже шум в канале связи. Значит, что к вам придет? К вам придут зашумленные версии вот этих матриц
плотности. Помните, в прошлом семестре смотрели образ шара Блоха под действием какого-то шума,
диполяризующий или дефазирующий. Здесь зашумленные версии этих объектов к вам приходит. Здесь к вам
приходит phi, ro x1 и так далее. Здесь к вам приходит phi, ro xn. Что при этом подразумевается?
Подразумевается, что вот этот канал phi без памяти, отсутствуют эффекты памяти. То есть предыдущие
сообщения не влияют на то, как этот канал действует на следующий. Значит, что дальше вы
можете делать? Вы можете измерить каждый из этих кубит по отдельности. Это будет индивидуальное
измерение каждого кубита. У вас какие-то классические исходы здесь получатся. Здесь
будет какой-то y1, здесь будет какой-то yn. В принципе, вы ничем не ограничены и можете
дальше постобработать эти результаты. Классическая постобработка не запрещается. То есть что такое
классическая постобработка? Это некоторая стахастическая матрица, которая говорит,
как перенаправить эти y. Можно какие-то объединить, можно какие-то с вероятностями
направить в одну сторону, в другую сторону. Вот здесь вот классическая постобработка, это просто
некоторая стахастическая матрица. Это вы сами можете делать, если надо. Если не надо, можете
ничего не делать. Это будет просто стахастическая матрица в виде единичной матрицы. На выходе вы,
давайте я результат, который на выходе получается, напишу просто какой-то y большой, без индекса.
Результат классической постобработки. То есть в этом сценарии 1 первые два момента главных.
Первый это то, что состояние, которое, если вы хотите кодировать слово, они будут у вас
записываться в виде вот этих тензорных произведений. Первый момент. А второй момент заключается в том,
что POV ам элементы на выходе е, у. Это будет что такое? Сумма по вот этим y1 и так далее, y, n.
Тут какая-то стахастическая матрица. Давайте я напишу p, y, y1 и так далее, y, n. Вот это
стахастическая матрица. А здесь индивидуальные измерения. Индивидуальные измерения, они для каждого кубита
могут быть разными, могут быть одинаковыми, е, y1 тензорно, е, y, n. Что это такое? Мы задали POV ам элементы.
В этом случае, что вы можете делать? Вы можете только подбирать вероятности p, x и те буквы,
в которые вы кодируете, ρ, х. То есть вот что вы можете варировать. Вот это мы можем варировать,
то есть вы можете варировать ансамбль. Можем изменять вот эти величины.
Смотрите, что в этом случае вы получите. Если вы найдете взаимную информацию между x и y и
возьмете Supreme по вот этому ансамблям, которые вы используете, то в этом конкретном случае,
давайте здесь это подчеркнем, что здесь у вас вероятность получить y какой-то катой,
при условии, что на входе был x, это есть след е, y катова с phi, который действует на ρ, х. Вот где канал
сидит phi. Канал phi сидит вот здесь, в этих условных вероятностях. Если вы теперь возьмете
этот Supreme, то вы получите так называемую c11 пропускную способность канала phi.
В самом деле мы варьируем ансамбль, значит p, x пропадает, ρ, х пропадает. Что остается?
Остается только канал phi. А, и что мы еще? Я не дописал, извиняюсь. В Supreme мы еще дописываем
вот эти вот е, y. То есть я могу варьировать эту стокастическую матрицу, могу варьировать вот
эти вот p, y элементы. Значит остается только свойство канала phi. И это называется c11 пропускная
способность. Это свойство канала, характеристика канала. Вот одна из задачек задания как раз таки на
это и нацелено, когда у вас всего две буквы в алфавите, и вот эти состояния ρ чистые, но они
зафиксированы там. Теперь смотрите, что можно в этом сценарии поменять, чтобы получить что-то
получше. Да-да. Нет, нет, в этом и есть смысл, то есть смотрите, в этом случае, правильно,
спасибо за вопрос, в этом случае вот это c11 совпадает с классической пропускной способностью,
ну как это сказать, с шенненовской, с c шенненовской. Но здесь еще дополнительная у вас есть оптимизация
по вот этим измерениям. У шеннена сразу какая-то буква на выходе получается. С шенненовской, где,
вот смотрите, что нужно в шенненовской пропускной способности сделать. Нужно заменить вот это p,
y при условии x, но вот эту формулу, заменяете p, y при условии x на эту формулу, получаете
то же самое, что для квантового канала, или что для классического канала, извините. Но это самый
простой сценарий, который можно представить, поэтому не удивительно, что здесь получается полное
совпадение с классическим. Вот смотрите, это когда вот эти два условия выполнены, то есть слова
кодируем в индивидуальные, в тензорные произведения индивидуальных матриц плотности, а измеряем
каждую частицу по отдельности. В сценарии 2 он что подразумевает? Вот смотрите, мы же знаем,
что существуют перепутанные состояния, поэтому вы можете условия 1 изменить и сказать, что
ro-w не обязана быть вот таким вот тензорным произведением, вот это же факторизованное
состояние, а вы можете в какое-то коррелированное состояние, например, в перепутанное состояние
кодируем. То есть взять слово и ему в соответствие поставить уже некоторое, в общем случае перепутанное
состояние многих частиц. То есть если вы теперь ro-w представите вот в таком виде, который не
сводится к тензорным произведению индивидуальных букв, перешлете каждую из частичек, которые
составляет это состояние через квантовый канал phi, то вы получите зашумленную версию. Если шум
не очень сильный, то даже и квантовые корреляции могут остаться. То есть вот этими волнами я
пытаюсь показать, что состояние не имеет вид тензорного произведения. Но измеряете, если вы
по-прежнему каждую частицу индивидуально с возможной постобработкой, то вы получите то же самое,
к сожалению. То есть supremum уже теперь по каким величинам? Вы уже будете брать вероятности слов
ro-w, e-y, но они имеют e-y по-прежнему вот этот вот вид 2. То есть давайте я напишу, что это индивидуальные
измерения. От чего? От вот этой x, y, но под x теперь подразумеваются уже расширенные выражения. Это
будет так называемая c-бесконечность 1-пропускная способность канала phi. Бесконечность показывает,
что вы в кодировании можете допускать блочность, вот это блочное кодирование. И размер блока не
ограничен, то есть он в принципе размер блока может стремиться бесконечно. Так,
теперь почему я говорю, что выигрыша здесь нет? Значит, есть такое утверждение c-бесконечность
1 совпадает с c-1-1. Значит, на чем оно основано? Поскольку это есть, будьте здоровы, модификация
шенненовской пропускной способности, а для шенненовской пропускной способности и для
взаимной информации есть некоторое свойство. Так, мне бы его еще вспомнить.
Вот здесь я могу ошибиться, то есть это надо смотреть книжку Холева.
Тут где-то может условность стоять, я вот не помню, но это уже для доказательства этого утверждения.
При этом надо мне сделать еще некоторую оговорочку. Когда мы считаем вот эту взаимную
информацию, мы считаем ее в битах в пересчете на вот количество символов, которые используется.
Смотрите, что получается, что здесь в этом сценарии перепутанность никак не помогает,
если вы измеряете индивидуально. Теперь следующий сценарий, вот он как раз уже нетривиальный.
С Бесконечность это не меньше, чем СН. Сценарий 3. В нём, в этом сценарии вы как раз таки
свойства 1 оставляете, то есть кодируете каждую букву по отдельности. А что меняете?
Меняете вид измерений. Вы не накладываете теперь условия, что вы каждую частицу по
отдельности измеряете, а вы можете подождать, пока они к вам придут, и измерить их целиком, коллективно.
То есть рисуем такую картинку. В состоянии на входе у вас кодируют отдельные буквы,
потом они проходят через канал без памяти, получаем зашумленные версии этих букв. Но измерять
мы их все будем, коллективное измерение. Почему здесь может быть выигрыш? За счет того,
что у вас есть условно-типичные подпространства, и вы можете измерять вот в этих условно-типичных,
то есть брать проекторы на эти условно-типичные подпространства. Что значит коллективное измерение?
Значит у вас общего вида POVM просто для n-частиц на выходе. В этом случае вот этот
supremum и x, y, что вы опять можете варьировать? Вы можете варьировать ансамбль, но е,
у теперь у вас общего вида без ограничений, общего вида. Вы получите так называемую
c1-бесконечность, пропускную способность канала phi, и эта c1-бесконечность, пропускная способность,
она равна chi величине. Чего? Какого ансамбля? Давайте supremum надо написать. Supremum по ансамблю
только px rho x, chi величина, ансамбль тот же самый, а вот здесь стоит phi действуя на rho x.
Так, не очень мелко, видите, что пишу. Ну что такое chi величина? Это энтропия среднего,
минус средней энтропии. Получается, что вот эта c1-бесконечность задается другой формулой уже,
граница Холева достигается вот в этом пределе, когда длина слова становится бесконечной. Достигается
граница Холева, это результат 96-го года. Холева и независимо Вестмарленд и Шумахер,
который я уже упоминал, но не тот, который в формуле 1. Они чуть попозже, но по-другому доказывали,
по-моему 97-го года, кстати. Вот, поэтому этот результат про это равенство еще называется теоремой
Холева Вестмарленда Шумахера. Вот что он говорит. То есть коллективные измерения могут помочь.
Почему? Потому что вот эта величина уже может быть больше, есть такие примеры, когда она больше,
чем c1-1, который в свою очередь совпадает с c-бесконечностью. То есть если вы делаете
коллективные измерения, то вы можете получить результат получше.
C-шенона, она же определяется, вот классическая пропускная способность классического канала,
определяется только стахастической матрицей переходов. А в качестве этой стахастической
матрицы переходов берем вот такую формулу, правило Борона. Вероятность получить исход при y,
при условии, что на входе было вот такое состояние. Да-да-да, но на практике это обычно выполнено.
Да, кстати, на практике. По распределениям на входе. А здесь вы берете распределение не
только px, но еще тех состояний, в которые вы канируете, и еще тех измерений, которые вы используете.
Ну поэтому нетривиальный результат, что вот эти е и у, вот здесь формуле справа уже не участвует,
они ушли оттуда. Так, теперь у нас совсем чуть-чуть времени, наверное, остается. Сейчас я посмотрю,
сколько. Да, немножко. Будем двигаться уже в 21 век, потому что это еще прошлый. Четвертый сценарий.
Передача информации. Вы теперь можете использовать блокчное кодирование. Сейчас объясню, в чем его
суть. Я уже чуть-чуть в сценарии 2 здесь говорил, но очень кратко, поскольку там нет выигрыш.
Вот смотрите, что происходит, если вы будете два, две частицы рассматривать как одно целое,
но пропускать их каждую через один и тот же канал phi. Вы можете эту картинку, вот эту картинку,
развернуть и представить в тождественном виде вот так вот. Смотрите, я поворачиваю на 90 градусов
свою систему, но тогда каждый из этих частиц пройдет через канал phi. Просто нарисовал
эквивалентную схему. Каждый из частиц проходит через канал phi. На выходе какое-то состояние
получится. Если шума нет, то то же самое, если шум есть, то может оно перестать быть перепутанным,
Теперь как нам написать вот этот весь канал целиком? Ведь это же тоже некоторое преобразование.
Пишут его очень просто. Все в порядке у нас тут. Phi tensor на 2 степени. Потому что phi действует в
первом пространстве Гильбертовом, phi действует во втором одинаково, phi tensor на 2. Теперь вы
можете что делать? В качестве своих букв брать уже вот такие вот состояния двух частиц. Это
будет блокченоэкодирование. Выбирать вероятность появления таких букв. При этом вы что получите?
Вы получите в таком сценарии C. Давайте напишем 2 бесконечности. Если вы измерение общего вида
делаете. Сейчас нарисую. Вот измерение общего вида. Это означает, что здесь вы делаете измерение
общего вида. Вот так. Здесь вы, варьируя вот это pVM и вот эти состояния, получите C2 бесконечности.
Как написать эту C2 бесконечности в виде формулы? Это будет у вас, смотрите что. Сейчас напишем
аккуратненько. C1 бесконечности для phi tensor на 2. Пока сейчас понятно. Здесь я напишу просто
канал phi. Оставьте еще место. Сейчас чуть подправим эту формулу. C2 бесконечности phi равно C1
бесконечности phi tensor на 2. Но вы хотите считать эту пропускную способность по количеству частиц,
которых вы посылаете через канал. То есть считать реальные посылки через канал. А здесь вы посылаете
один раз, второй раз. То есть у вас две посылки через канал идут. Вот эту величину нужно поделить
нам. Думаем пока я стираю. Эдуард будет задавать вопрос. Объясню на формуле. Вот представьте,
что шума нет. Phi тождественный канал. Какая у нас там была формула для вот этого C1 бесконечности?
Это была phi-величина, ее максимум. У phi-величины максимум мы с вами вспоминали. Это логарифма
размерности пространства. А в каком пространстве действует phi tensor на 2? Он берет на вход
операторы в тензорном произведении Гильбертова пространства и переводит их в операторы такие.
У h размерность D. И получается, что если ваш канал phi тождественный, если phi равняется тождественному
каналу, то вы получите логарифм размерности этого пространства. А это есть D квадрат. Это два логарифма D.
Понимаете? Откуда двойка? Из-за того, что размерность пространства увеличилась.
Поэтому если я хочу считать по отношению к исходным частицам, сколько пересылок было сделано
через канал, то я должен поделить на вот эту степень, которая стоит вот здесь в степени канал.
Понятно? Вот из-за чего это двойка. Теперь вы можете сказать, а давайте-ка я рассмотрю предел
При n, стремящемся к бесконечности, 1 делить на n, c1 бесконечность, phi тензорно n. И эту
величину назову c бесконечность бесконечности. И это есть истинная пропускная способность.
Давайте напишем классическую пропускную способность квантового канала.
Понятно? То есть вы допускаете блочное кодирование неограниченной длины. Вот это вот максимум из того,
что вы можете выжать для передачи классической информации используя кодирование в квантовое
состояние. Теперь смотрите, как думали люди в 20 и 21 веке. Давайте найдем пример такого канала,
для которого будет вот эта величина больше, чем просто c1 бесконечность. Вот так. Короче,
чтобы сценарий 4 был лучше, чем сценарий 3. Понятно? Чтобы блочное кодирование помогало. Искали-искали.
Но смотрите, здесь-то вы в сценарии 3 кодируете в индивидуальные состояния, а в сценарии 4 вы
можете кодировать в перепутанные состояния. Вопрос заключается в том, помогает ли это перепутанность
на входе для того, чтобы передавать больше информации. Искали-искали. Сначала поняли,
что не помогает для определенного класса каналов, которые называются там деполяризующие каналы
произвольного размера. Потом сказали, а у нас же компьютер есть, давайте в машине загрузим, пусть она
решает эту оптимизационную задачу. Нам не нужно вот этот прям предел, давайте какое-то n возьмем,
ну например вот эту двойку. Если покажем, что c2 бесконечность больше, чем c1 бесконечность,
ухи, то уже хорошо. То есть перепутанное из двух частиц помогает. Искали на компьютере, не нашли.
Для кубитных каналов. И возникла гипотеза. Гипотеза агитивности.
Заключается она в том, что c, phi, tensor, другой любой канал возьмите, phi. Так, давайте теперь
индекс поставим. Значит 1 бесконечность равно c1 бесконечность, phi, плюс c1 бесконечность, phi.
Si это канал. Если в качестве этого psi вставите phi, то получите просто здесь 2 c1 бесконечность,
и вот эту формулу. Там будет раньше тогда. Если она верна, то тогда у вас получится,
что cn бесконечность, phi, tensor на n будет 1 делить на n, c1 бесконечность, phi, tensor на n,
и это будет просто c1 бесконечность, phi. Вот. Значит, сейчас посмотрю еще время. Там интересная
история. Вот мало времени. Про те каналы, для которых она выполняется, поговорим в следующий раз.
Но что самое интересное? Самое интересное заключается в том, что в 2009 году Hastings все-таки
нашел Hastings. Нашел конструкцию, для которой c2 бесконечность, phi, больше, чем c1 бесконечность,
то есть перепутанность на входе помогает передавать больше информации. Про эту конструкцию стоит
поговорить отдельно. Вот сколько успеем до звонка, столько поговорим. Как она была сделана? Эта
конструкция вероятностная. То есть с вероятностью 1 существует канал phi такой что. Но сам канал
представлен трудящимся небом. Значит, где существуют такие каналы, вернее, какие конструкции?
Размерность пространства должна быть большой. Число операторов Крауса должно быть большим,
но приблизительно в корень, если я правильно помню, из размерности пространства. Дальше вы
пользуетесь некоторым аппаратом теории вероятности. Этот канал он random unitary. Сейчас напишу формулу
для него. То есть вид этого канала, как он строится, он известен. То есть это 1 делить на n, сумма по i от
доводу n, u, i, t, rho, u, i, t, крестик, где u, i, t это унитарные преобразования. Как вы понимаете,
количество этих уиток будет определять ранг Крауса. n это ранг Крауса, а вот размерность самого
пространства Гильбертова это другая величина d. Вам нужно и d делать большим, и n делать большим.
То есть и n во много раз больше единицы, и d во много раз больше единицы. Но между ними есть
определенная связь. И вот используя такую конструкцию, Хассингс смог показать, что с вероятностью
один существует такой канал, для которого это нарушается. Если кто-то представит канал в явном
виде, то он будет молодец, конечно. Хорошая статья будет. Это была в nature physics. В общем,
гипотеза аддитивности опровергнута, но в явном виде написать канал phi я вам не могу. Задачи
для будущих поколений. Все, спасибо, встретимся через неделю.

Здравствуйте, друзья. Как меня слышно? Слышно.
Отлично. Так, ну давайте начинать.
Ну мы в прошлый раз закончили с вами разговор по условным от ожиданиям, и сегодня таки перейдем к новой теме.
Значит, у нас с вами впереди по плану всякие предельные законы, закон больших чисел, Центральная предельная теория.
Ну короче говоря, сходимости, сходимости последовательств случайных величин.
И давайте прежде чем приступить к вот этому завершающему блоку, поговорим вообще в принципе о последовательности случайных величин.
То есть когда мы говорим про сходимость, у нас значит есть последовательство случайных величин.
И последовательство случайных величин – это по сути случайный процесс. Что вообще такое случайный процесс?
Это когда у вас есть прямое время, оно может быть дискретное, то есть время может быть дексировано натуральными числами, целыми числами.
А может быть действительными числами. И в каждый момент времени у вас задана своя случайная величина.
То есть по сути это некоторая динамика изменения случайной величины.
Ну и вот простейший случай – это когда у вас время дискретно, и вы рассматриваете так называемое случайное блуждание.
Причем самый простой случай – это простейшее симметричное случайное блуждание.
Когда вы начинаете, ну скажем, из нуля, и в каждый момент времени вы шагаете с вероятности 1,2 вправо или влево, то есть на плюс единицу или на минус единицу.
И в момент времени N у вас получается, что ваша случайная величина – это есть ваша позиция.
То есть сумма N независимо случайных величин, которая принимает равновероятное значение 1 и минус 1.
Вот. Давайте это дело формализуем, попробуем поизучать свойства такой последовательности.
Ну и в частности поймом, как оно устроено в пределе.
Будет такой естественный шаг, прежде чем поговорить в общем случае о пределах последовательств случайных величин.
Так, значит, случайное блуждание.
Ну, когда мы говорим про случайное блуждание, в принципе, это не обязательно случайное блуждание напрямой, оно не обязательно симметричное.
То есть в общем случае у вас есть какое-то векторное пространство, допустим N, и вы в каждый момент времени сдвигаетесь на некоторый случайный вектор.
Ну вот будем рассматривать только простейший случай, когда случайное блуждание напрямой, и мы сдвигаемся на единичку или на минус единичку.
Значит пусть у нас есть независимый случайный величин, X1, X2 и так далее, независимый знак по распределённой случайной величины.
И даже для простоты давайте читать, что они симметричны, то есть вероятность единички 1 на 2 и вероятность минус единички тоже вероятность равна 1 на 2.
Ну рассмотрим такой процесс, такую последовательность случайной величины Sn, начнём с 0, то есть с 0 равно 0, константа, а дальше для любого N Sn это сумма первых N случайных величин.
Такая последовательность называется простейшим симметричным случайным блужданием напрямой.
В принципе дальнейшее, многое из того, что я буду говорить, можно будет применять для несимметричного случая, то есть когда у вас единичка минус единичка с разной вероятностью, ну или в принципе там можно шагать на разные значения вправо и влево, там вправо на 3, вправо на 2 и так далее.
Но чтобы разобраться с методологией, давайте рассмотрим симметричный случай, гораздо больше всего интересного для этого случая можно доказать.
Ну во-первых, давайте поймём какое распределение такого случайного блуждания, такого случайного процесса.
И с этими словами зададимся вот к этому вопросу, пусть у нас есть какой-то момент времени N, чему равна вероятность того, что в момент времени N мы оказались в точке K?
Ну понятно, что N плюс K должно быть чётным. Нельзя оказаться в момент времени N в такой точке K, что N плюс K нечётный.
Если у вас нечётное количество шагов, то вы оказываетесь в нечётной точке. Если у вас чётное количество шагов, то вы оказываетесь в чётной точке.
Это во-первых. Во-вторых, понятно, что вы не можете выйти выше за N и ниже за минус N. То есть по модулю K должно быть в пределах от N до минус N.
Ну вот пусть K подходящий, то есть во-первых, N плюс K чётно, а во-вторых, по модулю K находится в пределах от минус N до N.
Чему-то равна эта вероятность. Ну, смотрите, какая бы у вас ни была реализация, вот какую бы вы ни взяли конкретную последовательность шагов длины N,
ну зафиксировали, в какие-то моменты вы сходили на единичку, в какие-то моменты на минус единичку. И вот пришли в K в конце.
Понятно, что все эти шаги будут равномерноятными. Но у вас так как вероятность пойти вправо 1,2, вероятность пойти влево 1,2, то какой бы вы ни разом смотрели последовательность шагов,
у него будет вероятность 1,2 в степени N. То есть у вас конкретный набор единичка минус единичек, и для конкретного набора его вероятность это 1,2 в степени N.
Поэтому все способы прийти в момент k имеют одну и ту же вероятность 1-2 степени n.
Поэтому чтобы посчитать эту вероятность, нужно умножить количество способов прийти в точку k на
вероятность одной конкретной реализации, то есть на 1-2 степени n. Сколько всего есть способов прийти
за время n в точку k? Ну понятно, сколько вам нужно сделать n плюс k пополам шагов вправо,
и n минус k пополам шагов влево. То есть на самом деле вы решаете такую систему равней. Если у вас
есть x шагов вправо и y шагов влево, то, во-первых, x плюс y должно быть равно n. Это
общее количество шагов. А во-вторых, x минус y должно быть равно k. То есть значение, в которое
вы пришли, это в точности x минус y. Количество шагов вправо минус количество шагов влево.
Ну и решая его, получается, что x это n плюс k пополам.
Поэтому вы придете в точку k тогда и только тогда, когда вы совершили ровно n плюс k пополам шагов
вправо. А поэтому вероятность, поэтому количество способов это c из n по n плюс k пополам.
И умножить на одну вторую степень n. Это если k хорошее. То есть, во-первых, модуль k меньше
0, чем n. А во-вторых, k плюс n делится на два. Да, иначе вероятность просто ноль.
Ну вот такое получается интересное распределение у наш ВСН. Ну и давайте, значит, когда мы говорим
про случайный процесс, нам не только интересно распределение в каждый момент времени, нам
интересно вот вся динамика. То есть смотреть на все распределения одновременно. То есть
распределение случайного процесса целиком. Вот, давайте подумаем о том, как вообще устроены
типичные траектории этого случайного процесса. Что вообще такое траектория? Значит, траектория
это следующая вещь. Вы можете нарисовать такую графику. Давайте сфиксируем какой-то элементарный
исход. У вас есть же вероятность на пространстве. Есть множество элементарных исходов. Вы можете
взять какой-то совершенно конкретный элементарный исход, посчитать, посмотреть на значение ваших
случайных величин на этом элементарном исходе. Вы получите детерминированную последовательность.
С0 от Омега, с которой 0, конечно, равно. С1 от Омега это либо единичка, либо минус единичка. С2 от
Омега это либо минус два, либо ноль, либо два и так далее. Да, вот получите как конкретную
совершенно последовательность чисел. И вот она и называется траекторией. Траектория вашего
случайного буждана. Вы можете нарисовать, грубо говоря, график этой траектории. Представить себе,
как выглядит для вашего конкретного элементарного исхода, как выглядит траектория. Вы начинаете
с нуля, потом вы идете, скажем, к единичку, потом вы можете пойти в два, потом можете спуститься,
снова пойти вверх, несколько раз спуститься, нынче, ну и так далее. Да, вот любой такой график,
это будет график траектории вашего случайного буждания. Понятно, что если мы берем какое-то
маленькое количество шагов, способов нарисовать такую картину будет не так много. Но хочется понять,
как выглядят траектории, когда шагов очень много. Как угодно, в принципе, могут выглядеть,
но, наверное, есть какие-то типичные траектории, то есть те траектории, которые более вероятны,
чем какие-то другие. Ну, скажем, слабо верится, что вы за н шагов пришли в точку n. Наверное,
вы где-то будете пониже, чем n и повыше, чем минус n. Ну и вот насколько пониже, чем n,
и насколько поменьше, чем минус n. Вообще, можно нарисовать какую-то область,
оптимальную, внутри которой находятся почти все траектории вашего случайного буждания. Ну,
что значит оптимальную? В том смысле, что если вы пытаетесь сузить, то у вас не получится. Ваше
случайное буждание начнет находиться за пределами трудностей. Вот, и эту задачу можно решить. Мы
сегодня об этом поговорим. Прежде чем поговорить, давайте решим вот такое вот полезное упражнение.
В общем-то, это упражнение тоже важно для того, чтобы понять, какая у нас есть оптимальная область,
в которой попадают все траектории, почти все траектории. Значит, упражнение. Спрашивается,
какая вероятность того, что мы попали в точку k в момент времени n, и при этом ни разу ноль не
пересекали. Ну, пусть k скажем положительно. Нас интересует вероятность того, что s1 больше 0,
s2 больше 0 и так далее, sn – s1 больше 0, а в момент времени sn мы оказались в точке k.
Ну, вот иными словами, раз у нас симметричный случай, мы хотим посмотреть, сколько бывает
траекторий, которые приходят в точку k, но не пересекают. Наверное, многие знают, что речь идет
практически с каталана. Давайте задачу быстренько решим. Как ее решить проще всего? Ну, смотрите,
есть вот такой вот трюк. Во-первых, чтобы в момент времени 1 мы оказались в точке положительной,
у нас есть только одна такая возможность – мы должны попасть в единицу. Так как мы в момент
времени 1 можем оказаться либо в единицу, либо в минус единицу, но делать нечего, мы должны попасть
в единицу. Ну, дальше вот есть у нас какой-то момент времени n, и мы, скажем, должны попасть в какую-то точку k.
Воль не пересекай ни разу, но, кстати, понятно, что в момент времени 2 мы должны попасть, конечно,
в точку 2. Но что нам здесь важно? Важно вот что, давайте задачу обратную рассмотрим, а сколько
траекторий, которые пересекают 0? Да, то есть понятно, что количество траекторий,
который не пересекает 0 – это количество всех траекторий, минус количество траекторий,
которые пересекают 0, поэтому достаточно решить задачу для количества пересекающих траекторий.
Вот. Ну 같 washing траекторий, который пересекает 0, может быть несколько раз. Вообще-то у нас 0.
Давайте отметим момент первого пересечения с тулём и отразим нашу траекторию симметрично,
начиная с этого момента относительно осепсис. То есть получим какую-то отражённую траекторию.
Куда она пришла? Ну конечно она пришла в точку минус K.
Понятно, что между всеми траекториями, которые пересекают 0 приходят в точку K,
и всеми траекториями, которые ходят в точку минус K... FREE versa
Раз уж пришла в точку минус, я имею в виду вот отсюда.
это старцевая точка наших всех траекторий. Из единицы вышли. Так вот, значит, количество траекторий,
которые пересекают ноль, приходит в точку k, и количество траекторий, которые приходят в точку
минус k, оно просто одинаковое. Просто потому что, чтобы прийти из единицы в точку минус k, вам
надо хотя бы один раз ноль пересечь. А значит, если вы отображаете относительно первого момента
пересечения с нолем, вы получаете биекцию. У вас в обе стороны отображение работает. Поэтому
количество траекторий, которые... Давайте это как-то обозначим. Пусть, скажем, n с индексами
AB от XY, это количество траекторий,
или давайте AB здесь уберем, n маленько напишем. Это количество траекторий,
которые за время n приходят из точки x, точку y.
Приходит из x выпрек. Ну и, скажем, n с волной то же самое, но еще есть дополнительное
условие, что ни разу нельзя пересечь ноль. Пусть n с волной или, наоборот, можно пересечь сейчас.
То есть, наоборот, можно пересечь. Значит, n с волной, n от XY, это количество траекторий,
количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают
ноль. Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
Мы смотрим на количество траекторий, которые за время n-1.
Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
С другой стороны, вот эта n с волной – это количество всех траекторий, минус количество тех траекторий, которые ноль не пересекают.
Теперь смотрите, наша искомая вероятность. Мы ее теперь можем посчитать.
Понятно, что нужно одну вторую степень n умножить просто на количество траекторий.
Но количество траекторий я посчитал.
Это количество траекторий за время n-1 из единиц в k, минус c из n-1, потом n-k пополам минус 1.
Но не это надо умножить на 2.
Тут надо внимательно помнить о том, что мы в первый момент времени шагнули в точку 1.
Мы могли здесь в знаменателе написать одну вторую степень n-1, но это было неправильно,
потому что у нас еще есть первый шаг, который мы делали в связи с 1-2.
Давайте это приведем к какому-то более приятному виду.
Количество траекторий из единиц в точку k мы умеем считать.
c из n-1 по n-1 плюс k-1 пополам, минус c из n-1 по n-k пополам минус 1.
Можно еще от минус единички избавиться и переписать это в виде c из n-1 по n-k пополам,
минус c из n-1 по n-k пополам.
Почему мы написали, где c из n-1 на степени n-1 плюс k-1?
Это вот эта величина, это количество траекторий за время n-1 из точки единицы в точку k.
То есть мы поднялись вверх на k-1. Мы к количеству шагов должны прибавить то, насколько мы вверх поднялись.
Мы поднялись вверх на k-1, поэтому здесь n-1 плюс k-1 пополам.
В предыдущем случае мы опускались вниз на k-1, и поэтому у нас было минус k-1.
Это разные n и n-s-1.
Но вот это...
Просто там вот раньше стоит...
Одно, вот это другое.
Я чтобы найти количество непересекающихся путей, я вот из вот этой штуки вычел предыдущую и сюда просто переписал.
Из количества траекторий из единиц в k я вычел количество траекторий из единиц в n-1.
Так, хорошо.
Есть ли какие-то вопросы?
То есть просто вроде все-таки мы как-то одинаково обозначаем, да?
И это разная вещь.
Это кто?
Ну, n и n-s-1 от единицы k.
Просто это же одно и то же, да?
Одно и то же, как и от единицы минус k?
Ну, там просто окраина стоит, не знаю.
Нет, не стоит, тут еще минус.
А, все понятно. Все, спасибо. Я понял. Я думал, это тире. Спасибо.
Я из этого вычислил как раз количество непересекающихся, непересекающих ноль траекторий, как правое выражение, минус левое.
Откуда у меня здесь эта разность появилась, да? Я вычел из этой штуки вот эту штуку.
Еще вопросы?
Так, окей. Хорошо.
Значит, из этого можно получить важное следствие, которое как раз вот там применяется в так называемом законе повторного логарифма.
Это то само утверждение про то, в какой области лежат почти все траектории.
Давайте возьмем, обозначим за m большой от m максимум.
Обозначим за m большой от m максимум по всем k от нуля до m sk.
Ну и посмотрим на распределение, на распределение этой случайной величины, а именно найдем вероятность того, что mn хотя бы х.
Предлагается gch это событие с другим и с его дополнением.
Представьте вероятность в виде сумм по деятельности.
А именно вероятность того, что mn больше уночим х и sn больше уночим х.
Плюс вероятность того, что mn больше уночим х и sn больше уночим х.
Значит первая вероятность совместная, в ней одно условие сильнее, чем другое.
Понятно, что если sn больше уночим х, если в момент времени вы оказались выше х, то и максимум под up.
Может где-то там было еще больше.
Поэтому условие sn больше уночим х сильнее, чем условие mn больше уночим х.
А по так называемому принципу отражения, вот смотрите, что мы только что с вами на самом деле не явно доказали, когда мы вот эту формулу получили про вероятность того, что mn больше уночим х и sn больше уночим х.
А по так называемому принципу отражения, вот смотрите, что мы только что с вами на самом деле не явно доказали, когда мы вот эту формулу получили про вероятность того, что мы в момент времени n оказались в точке k, не пересекая ноль.
Мы с вами обратили внимание на то, что количество траекторий, которые ведут в точку k, пересекая ноль.
И количество траекторий, которые ведут в точку минус k, не пересекая ноль, одинаковое.
И по аналогичному соображению, вот эта вторая вероятность, она в точности равна вероятности того, что sn больше чем х.
Я сейчас это еще раз поясню.
Смотрите, еще раз навесим траекторию.
Вот представьте, что есть какой-то уровень х.
И есть там момент времени n.
Мы смотрим на вероятность того, что sn меньше чем х.
А при этом максимум больше. То есть вы как-то там шли.
И в какой-то момент вы были выше х, но в момент времени n оказались ниже.
Давайте возьмем первый момент пересечения с х и отразим нашу траекторию, начиная с этого момента относительно прямой параллельной оси абсциссы, проходящей через точку х.
Продолжение траектории будет вот таким.
Продолжение траектории...
А нет, я же должен пересечь здесь.
Вот так вот я должен сюда прийти, а потом прийти сюда.
Это симметричная траектория. Мы попадаем в точку симметричной траектории.
И мы видим, что у нас есть вот эта точка.
Я должен сюда прийти, а потом прийти сюда.
Это симметричная траектория. Мы попадаем в точку симметричную относительно точки х.
То есть она лежит относительно прямой х.
То есть она лежит выше этого уровня х.
Из этого поярное дело следует, что количество траекторий, которые идут выше чем х, то есть таких, что в момент времени n оказались выше х,
совпадает с количеством траекторий, которые ниже чем х, но при этом в какой-то момент пересекают х.
То есть, раз траектория одинакова, то вероятность совместная того, что m ≥ x и Sn ≥ x,
она просто совпадает с вероятностью того, что Sn ≥ x.
Ну и для удобства давайте это перепишем как две вероятности того, что Sn ≥ x
минус вероятность того, что Sn ≥ x.
Вот. Назовем это дело утверждение 1. Это утверждение, которое будет использоваться для доказательства
закона по второму логарифму. Мы его явно доказывать не будем, но я объясню идею.
Значит, вероятность того, что m ≥ x равна две вероятности того, что Sn ≥ x.
Ну или на самом деле, что то же самое в силу симметрии, это то же самое, что вероятность того, что m ≥ x.
В силу симметрии вероятность того, что Sn ≥ x и вероятность того, что Sn ≤ x совпадает.
Поэтому я здесь могу убрать двойку и написать вместо этого m.
Минус вероятность того, что Sn ≥ x.
Для каких-то больших n, какое бы ни было x, вероятность того, что Sn ≥ x очень маленькая.
то есть в пределе распределение максимума оно такое же, как распределение модуля Sn.
Если мы хотим в какой-то причине смотреть на распределение максимума, то мы можем при
достаточно больших m заменить его на распределение модуля Sn. Вот это вот первая мысль,
которая будет использоваться для доказательств закона повторного логарифма, который я сейчас
сформулирую. Что же такое закон повторного логарифма? Это утверждение о том закон повторного
логарифма, ZPL сокращённый. Закон повторного логарифма. Это очень довольно тяжело доказывающиеся
утверждение о том, как раз в какой области лежат почти все траектории случайного блуждания.
А ещё он звучит вот так. Вероятность того, что верхний предел при n стремящемся бесконечности
Sn поделить на корень из 2n log-log-n равна единице, равна единице. Давайте попробуем
проинтерпретировать, понять, что означает это утверждение. Верхний предел некоторой последовательности
равна единице с вероятностью 1. Ну то есть это означает, что что значит вероятность 1? То есть
для почти всех траекторий. Вероятельностная мера тех траекторий, для которых это свойство верно,
она равна единице. Для почти всех траекторий это верно. Что же верно? А верно следующее, что вот если
вы возьмёте траекторию и для неё рассмотрите вот эту вот последовательность Sn поделить на корень из
2n log-log-n, то есть отномируете некоторым правильным образом траекторию, то вы получите, что верхний
предел такой последовательности равен единице. То есть иными словами, что значит верхний предел равен
единице? Это значит, что ваша последовательность, если вы чуть-чуть отойдёте от единицы, возьмёте
1+, то значит ваша последовательность, она может быть больше, чем 1+, лишь конечное количество раз. Это
во-первых, а во-вторых, если спустите чуть-чуть вниз, то есть рассмотрите 1-эпсилон, то это будет
означать, что бесконечное количество раз ваша последовательность больше, чем 1-эпсилон. То есть
это очень точное утверждение, которое говорит о том, что траектории должны быть следующей области.
Давайте ещё раз начисуем картинку.
Ну вот, можно нарисовать такую кривую.
Это будет 1+, умножить на корень из 2n лог-лог-н.
И можно ещё нарисовать то же самое, только 1-эпсилон.
1-эпсилон на корень из 2n лог-лог-н.
Ну, траектория симметричная в смысле своих свойств. Вот, поэтому снизу будет такая же картинка.
Это 1-эпсилон, это минус 1+, на корень из 2n лог-лог-н.
И чуть выше минус 1-эпсилон.
Вот, и что нам говорит закон по второму логарику? Мы говорим, что если возьмём траекторию типичную,
то есть те траектории, которые попадают в множество мира 1, то она будет бесконечно много раз
пересекать чёрную кривую и только конечное количество раз пересекать синюю.
Вот чёрную кривую она может пересекать бесконечно много раз, то есть чем дальше вы идёте по времени и вы
всё будете находить всё новые и новые точки пересечения с чёрной кривой, а синюю кривой
в какой-то момент она перестанет пересекаться. Вот что говорит закон по второму логарику.
Мы его оставим без доказательств, но схема доказательств я поистину, то есть там тяжёлая
техника, поэтому давайте ограничимся основными утверждениями, которые используются для его
доказательств и методами доказательств. Доказательство основывается на лиме Борреля
Контель. И что такое лима Борреля Контель?
Давайте рассмотрим произвольную последовательность событий. Пусть 1, 2 и так далее, это какая-то
бесконечная, счётная последовательность событий из нашего множества событий.
Давайте рассмотрим событие, которое будет обозначаться вот так АНБЧ, и произносится это
будет как АН бесконечно часто. То есть это есть множество тех элементарных исходов, которые
попадают в бесконечно много событий. То есть как это определить в термах пересечений и объединений
множеств? Как это определить в термах пересечений и объединений множеств? Но это надо, значит,
всё объединить, а потом по одному выбрасывать. Если у вас элементарное событие содержится в
бесконечном многих, то оно в таком множестве будет содержаться. То есть это есть пересечение по
всем АН от единицы до бесконечности, объединений по всем К больше ноль, чем АН, а КАТы.
Вот, значит, мы обозначаем, рассмотрим вот такое событие.
Значит, тогда, первое, если сумма вероятностей АН меньше бесконечности,
то вероятность того, что АН бесконечно часто, равна нулю.
И второе, если сумма равна бесконечности и к тому же событие независимое в совокупности,
то вероятность АН бесконечно часто равна единице.
Так, ну это просто утверждение. Давайте быстренько докажем.
Что такое вероятность АН бесконечно часто? Ну, это вероятность вот этого нашего пересечения объединений.
Давайте посмотрим на события внутри первого пересечения. Давайте посмотрим на эти вот события.
Они вложены. Да, это что такое? Вот мы взяли Н равно единице, тогда мы объединили вообще все А, все события 1, а 2 и так далее.
Потом взяли Н равно 2, и значит новое объединение будет отличаться от предыдущего тем, что мы не объединяем с А1.
Вот то А1, которое не содержится во всем объединении, мы его выкинули. Потом А2 выкинули, то МА3.
То есть сужается вот это объединение, оно сужается. То есть это система вложенных событий.
И по непрерывности вероятностной меры эта штука равна пределу приенстремящимся бесконечности, вероятности объединения пока больше Н чем А, окатых.
А в свою очередь эта вероятность, как мы знаем, не превосходит сумма вероятностей.
Ну то есть это остаточный член нашего ряда. А раз ряд сходится, то остаточный член стремится к нулю с Ростомэн, что и требовалось.
А там вероятность, Окадович. Ой, конечно, спасибо большое.
Хорошо, теперь второй пункт.
Теперь второй пункт.
Ну давайте перейдем к дополнению. То есть возьмем вероятность АН бесконечности часто.
И напишем, что это единица минус вероятность дополнения.
Ну то есть на самом деле вероятность объединения, вероятности единицкой бесконечности, пересечений, пока больше Н чем А, пока с чертвой.
Ну опять, если я посмотрю на эти внутренние события, то эти события, наоборот, расширяются.
Значит, я увеличиваю N и, соответственно, уменьшаю количество множеств, которые пересекаются. Чем меньше у меня множество пересекается, тем больше события.
Значит, это система расширяющихся событий вложенных. И поэтому, опять, потеряемая непрерывность вероятностной меры, это есть просто предел вероятности, понимаете?
Ну и давайте посмотрим на эту вероятность по знакам предела.
Ну здесь нам, конечно, помогает независимость. По независимости это просто произведение про всем К больше Н чем N.
Вероятности окатая с чертвой. Ну то есть единицы минус вероятности окатая.
Ну дальше стандартный трюк. Когда вы знаете, что сумма равна бесконечности, и вам нужно посмотреть на произведение, вам нужно, конечно, произведение таких вероятностей это ноль,
но вы можете использовать следующий стандартный трюк. Вот вы знаете про сумму вероятностей, а вам нужно посмотреть произведение единиц, единиц минус вероятностей, как от этого произведения перейти к сумме.
Очень просто. Нужно взять экспонент от логарифма, то есть это будет экспонент от суммы логарифмов.
Ну и заметьте, что логарифм единицы минус х меньше, чем минус х.
Значит, по монотонности экспонента логарифма мы получаем экспоненту от суммы минус вероятности окатых.
Так как весь ряд расходится, то и его часть, его остаточный счет тоже расходится, то есть сумма этих вероятностей равна минус бесконечности, а значит экспонента равна нулю.
То есть это ноль, и значит наша исходная вероятность равна единице.
Хочу обратить ваше внимание на то, что даже к пределу переходить не надо, какой бы вы ни взяли n, вероятность такого бесконечного пресечения равна нулю.
Есть ли какие-то вопросы?
Ну, значит, теперь вопрос у меня. Как применять? Я сказал, что эту лему можно применить для доказательств законоповторного логарифма.
Как это сделать? В законоповторном логарифме написан верхний предел ранее единицы с вероятностью 1.
Причем тут лему бороли контель. Но на самом деле вот это утверждение о том, что верхний предел чему-то равен, это как раз про то, что какое-то событие выполнено бесконечно часто.
Помните же, я говорил, что если прибавим к единичке epsilon, то окажется, что не бесконечно часто мы пересекаем этот уровень.
А если вычтем из единички epsilon, то окажется, что бесконечно часто пересекаем этот уровень.
И давайте для доказательства законоповторного логарифма, точнее для демонстрации того, как можно доказать законоповторного логарифма, мы решим такую задачу.
Она будет гораздо проще, чем законоповторный логарифм с силой независимости.
Вот когда вы смотрите законоповторный логарифм, у вас там стоит под пределом, случайно, личная Sn.
Если вы меняете n, Sn, конечно, зависимы. Причем зависимы довольно сильно.
Например, Sn от Sn плюс 1 отличается всего на единицу.
Поэтому в явном виде применять второй пункт лему бороли контеля у вас не получится.
Но, тем не менее, применяется именно лему бороли контеля, только перед тем, как ее применить, немножко пострадать.
Но давайте просто чтобы вы увидели, что это в точности условия лему бороли контеля, не считая независимости.
Я решу немного другую задачу, но в таких же термах сформулированную.
Пусть у вас есть последовательность независимых случайных величин, которые имеют экспоненциальное распределение с параметром 1.
Необходимо доказать, что вероятность того, что верхний предел при этом стремящемся бесконечности
Xn поделить на лог n равен единице. С вероятностью 1.
Ну то есть, вот вы, да, интересны такое наблюдения. Если вы смотрите на то, как ведет себя последовательность экспоненциальных случайных величин,
то вот траектория этой последовательности, она лежит на том, что у вас есть последовательность,
Ну, то есть, вот вы, да, интересно такое наблюдение, если вы смотрите на то, как ведет себя последовательность
экспоненциальных случайных величин, то вот траектория этой последовательности, она лежит в области
между logn и –logn, да, и она оптимальна в смысле, что если чуть уменьшите, то уже за эту область будет выходить.
Давайте это докажем.
Ну, мы хотим доказать две вещи.
Первая вещь – это то, что с вероятностью 1 верхний предел меньше, если брать на 1.
Ну, а второе – это то, что он больше, если брать на 1.
Что значит верхний предел меньше, если брать на 1?
Это значит, что для любого епсилон, верхний предел какой-то последовательности, ну, в нашем случае такой,
меньше, если брать на 1, это то же самое, что для любого епсилона больше 0.
Неверно, что кси n поделить на logn больше, чем 1 плюс епсилон, бесконечно часто.
Да, какой бы ни взяли епсилон, лишь конечное количество раз кси n поделить на logn будет больше, чем 1 плюс епсилон.
Но давайте обозначим это событие за m.
Найдем его вероятность.
Но это в точности вероятность того, что кси n больше, чем logn умножить на 1 плюс епсилон.
Это интеграл от плотности.
Плотность экспоненциального распределения с проявлением 1, да, если это минус x dx.
Понятно, что это есть n в степени минус 1 минус епсилон.
Поэтому сумма вероятностей i n конечна.
И значит вероятность того, что i n бесконечно часто равно 0.
Формально мы еще не доделали задачу, потому что здесь для любого епсилона еще стоит.
Возвращаемся к тому, что мы хотели. Мы хотели показать, что вероятность того, что верхний предел кси n поделить на logn меньше, чем 1, равна 1.
Это событие, это в точности, как мы с вами выяснили, вероятность того, что для любого епсилона больше 0, не выполнена i n bч.
А i n от епсилона зависит. Я просто опустил зависимость от епсилона, но i n зависит от епсилона.
На самом деле это событие монотонно по епсилону, в том смысле, что если вы епсилон будете уменьшать, то событие от этого при этом будет вложено.
И на самом деле можно рассматривать не континуальное множество епсилон, а счетное.
То есть это все равно, что для любого епсилона вида 1mt, где n натуральное число.
То есть вы можете рассматривать пересечение по всем натуральным m.
Давайте я все-таки здесь напишу для удобства, что i n зависит от m.
Почему так? Потому что мне важно, что я могу выбрать епсилон сколько угодно маленьким.
Если у меня m сколько угодно большое, то я таким образом могу выбрать епсилон сколько угодно маленьким.
Ну и пофигу, что он определенный вид имеет, это равносильные вещи.
А это есть вероятность пересечения по всем m.
p от m бесконечно часто отрицание.
p не должно быть.
Вот. Ну понятно, что если у вас есть счетное пересечение событий, которые имеют меру 1, то вся вероятность тоже будет 1.
Но к тому же здесь еще и вложенные.
Здесь с увлечением m события вложенные, и поэтому это по непрерывности есть предел вероятности.
Преимущество бесконечности. Вероятность того, что i n от m бесконечно часто.
Можно было бы этим не пользоваться в принципе, потому что у вас не в пределе даже единица.
У вас каждое событие имеет вероятность 1.
Поэтому и пересечение таких событий тоже будет иметь вероятность 1.
Здесь даже еще более сильная вещь. Здесь можно в пределы перейти.
Короче, единица, что и требуется.
Вот. Ну и аналогично второй пункт.
Когда вы смотрите на вероятность того, что верхний предел больше m единицы.
Вы что говорите? Вы говорите, что верхний предел больше m единицы.
Тогда и только тогда, когда для любого epsilon больше нуля.
Кси n поделить на лог n больше равно чем 1 минус epsilon бесконечно часто.
Ну и вы точно так же можете перейти к одной m вместо epsilon.
Тогда и только тогда, когда для любого m натурального.
Кси n поделить на лог n больше равно чем 1 минус 1 m бесконечно часто.
Ну дальше вы обозначаете как выше это событие за n от m.
И говорите, что сумма этих n вероятностей от n от m, она теперь равна сумме n в степени минус 1 плюс epsilon.
Она бесконечная.
Вместо epsilon должна быть одна m.
Не суть.
Она бесконечная, а значит, по Лемми-Боррели-Кантели вероятность того, что n от m бесконечно часто, они у вас независимые события.
Вот в этом месте мы независимый используем.
Равно единицы.
Ну и все.
И дальше вы снова обозначаете как выше это событие за n от m.
Вот, ну и все.
И дальше вы снова этим пользуетесь.
Для доказательства исходного утверждения.
Говорить, что...
Ну опять событие вложено, поэтому это есть предел при м с применяющимся бесконечности.
Что, в общем-то, на самом деле опять же не нужно, потому что событие не просто, вероятность события не просто не соединится, а для всех m она равна.
Поэтому просто пересекаете счетное множество событий, вероятность которых она деится, получается событие вероятности 1.
Но можно пределы перейти без разницы.
И в общем получаем 1, что это у вас.
Есть ли вопросы?
Вот, ну прекрасно.
Теперь, если вы посмотрите на это доказательства, вы увидите, что здесь важно распределение.
То есть вы явно считали вероятность этих i n.
Так как вы знали, что распределение экспоненциально, это в одном случае у вас получилось n степень минус 1 минус epsilon.
А в другом случае n степень минус 1 плюс epsilon.
Если мы вернемся к закону повторного логарифма, то мы увидим, что нам нужно знать распределение s n.
Ну, мы его знаем.
Я увижу, что он написал.
Вероятность того, что s n равняется k, это есть c из n по n плюс k пополам на 2 степень минус n.
Но вот этот вот биномиальный коэффициент, c из n по n плюс k пополам, непонятно, как его анализировать.
Вам же в итоге нужно, чтобы вот этот порог, он был в виду корень из 2n log n.
То есть когда вы вместо k подставляете корень из 2n log n, то вы получаете, что там если на epsilon сдвинется, то там будет бесконечность.
Если вправо, если на epsilon влево, там будет минус бесконечность.
И вам для этого нужно понимать асимптотику, как асимптотически ведет себя закон распределения s n.
Ну и еще там технический момент, который в общем-то позволяет Lema-Barrelli-Cantelli применить.
У вас все-таки, как я уже говорил, не независимая случайная величина.
В общем, там нужно к максимуму перейти.
И вот это вот утверждение 1.
Но вот это давайте оставим за кадром.
Подумаем о том, как понять, какое асимптотическое распределение s n.
Ну вот есть тиремма уавролапласа про предельное поведение биномиального распределения.
Я думаю, что Иван Генрихович вам ее формулировал.
Надеюсь.
Если нет, я в любом случае формулирую.
Но видимо, не доказывал.
Если я не прав, скажите, что доказывал.
Значит, я ее сейчас докажу.
Сформулирую и докажу.
Но я ее сформулирую и докажу в более общем виде.
Даже если вам Иван Генрихович формулировал, я докажу в более общем виде, именно который нужен здесь.
Здесь важно, что нам нужно понимать распределение s n для правильно растущих k.
То есть k должно быть порядка корень из двух n лог лог m.
Вот.
И вот это есть некоторая тиремма уавролапласа, которая позволяет для таких k понимать, какое предельное распределение.
Это ее некоторое обобщение, которое я сейчас сформулирую.
В общем, она будет состоять из двух частей.
Из локального утверждения и интегрального утверждения.
И будет сейчас два пункта.
Первый я аккуратно докажу, а второй я просто прокомментирую.
В общем, он практически явно следует из первого.
Я думаю, что каждый из вас способен с таким упражнением справиться.
Ну, давайте для удобства, для некоторой общности, будем формулировать ее для биномиального распределения.
Потом я поясню, как мы от биномиального переходим к случайному блужданию.
Вот пусть у нас есть x n, биномиальная случайная величина с параметром n и p.
И пусть есть какая-то последняя параметра, которая у нас есть.
И пусть есть какая-то последовательность phi от n, которая равна o мало и от n в степени 2,3.
Тогда первое.
Значит, supremum, по всем k, целым и отрицательным.
Таким, что может быть, что может быть.
Таким, что модуль k-np меньше броно, чем phi от n.
От модуля разности вероятность того, что sn равняется k,
уделить на 1, уделить на корень из 2 p, np, 1-p.
Испонента от минус k, минус np в квадрате.
Уделить на 2 np, 1-p.
Эта дробь, она отстымит с единицей.
Равномерно.
То есть, если я возьму supremum этой дроби, минус 1, по всем возможным k,
из моей окрестности, от ожидания.
По этому от ожиданию случается.
А здесь вместо sn должно быть xn.
То вероятность попасть в точку k, она будет равномерно близка к вот этому выражению,
к этому выражению, которое у знания цитатиста.
Вторая часть – это интегральный вариант, который просто из первого выводится.
Имея вот эту формулу, можно вывести следующее.
Что supremum по всем, опять таким же точно k,
а модуля разности между…
Сейчас, секунду.
Сейчас я подумаю, как это аккуратно писается.
Ну, хорошо.
Модули разности сейчас нет.
Прошу прощения, давайте по-другому напишем.
Ну, хорошо.
Модули разности сейчас нет.
Прошу прощения, давайте по-другому напишем.
Значит, равномерно по всем k.
Сейчас объясню, что значит равномерно.
Целым не отрицательным.
Таким, что модуль k-np меньше, чем ferret m.
Таким, что модуль k-np меньше, чем ferret m.
Таким, что модуль k-np меньше, чем ferret m.
Таким, что модуль k-np меньше, чем ferret m.
Вероятность того, что xn меньше, чем k…
Разница с локальным вариантом.
Потому что локальным вероятность того, что xn равняется k,
а здесь вероятность того, что xn меньше, чем k.
Тут даже не обязательно требовать того, чтобы k было целым,
сделать его вообще произвольным, действительным. Давайте кадаем. Вероятность того, что xn
между прочим k равна интегралу от минус бесконечности, от минус бесконечности,
до k-np поделить на корень из np1-p, от 1 поделить на корень из 2p e в степени минус
квадрате пополам dx. Это интеграл умножить на 1 плюс умало от единицы. Когда я говорю равномерно
по всем k, я имею ввиду, что это равномерно спрятано в умало от единицы. То есть можно найти умало от
единицы, которая будет не зависеть от k. Есть какие-то вопросы о формулировке теремоопроплоса?
Если мы докажем первую часть, то на самом деле вторая часть из этого следует. Почему? Потому
что вы просто берете вероятность того, что xn меньше 1чмk и представляете ее в виде суммы
вероятностей. Каждую из вероятностей внутри суммы вы умеете приближать с помощью локального
варианта, локального варианта теремомопроплоса и получите такую большую сумму вот этих экспонентов.
Ну и приближаете после этого эту сумму интегралом. Это все выглядит абсолютно непонятно. Зачем? Как
этим пользоваться? А еще формула очень страшная. У этого есть мотивация какая-то или это просто
красивая теорема? Не переживайте. Во-первых, я уже частично объяснил, какая мотивация. Да,
нам нужно, чтобы доказать закон по второму логарифу, нам нужно понимать какое в пределе распределение
у см. И вот теорема овролоплоса это дает. Во-первых, во-вторых, на самом деле это не просто красивая
форма, это центральная теорема теории вероятности, которая обобщается до так называемой центральной
предельной теоремы. Ну это утверждение очень сильное. Мы тут некоторую равномерность утверждаем
по всем К, который довольно сильно мог отличаться от марта ожидания. В общем случае, когда у нас
есть произвольное распределение, здесь распределение биномиальное, аналогичное утверждение
можно доказывать для произвольных распределений. Это называется центральная предельная теорема.
И физический смысл я прямо сейчас пояснил. Я доказать уже не успею, но может быть я начну
доказывать. Посмотрим, хотя бы напомню, что такое форма стилинга, который здесь надо использовать.
А сейчас давайте я поясню физический смысл этой теоремы. Давайте решим такую задачу.
Предположим, мы доказали. Решим вот такую вот задачу.
Ну, не знаю, первое, что приходит в голову. Про монетку первое, что в голову приходит.
Смотрите, вот мы с вами уже много раз говорили про монетку, про частотность выпадения орла или
решки, которая одна-вторая. Что это значит? Это значит, что если вы будете очень много раз
подбрасывать монетку и посчитаете, сколько раз у вас выпала решка, поделите на количество
подбрасываний, то если это количество очень большое, то будет что-то близкое к одной-второй
и чем больше это vardır тем ближе этоimming wyter. Crowbar мы с вами уже формально доказали и
называют закон больших чисел. Закон больших чисел утверждает, что если, сейчас пр sung 강ines
разделить, в частности, он утверждает, что если разделить количество решек на количество подбрасывания,
то в пределе будет одна вторая. Он утверждает на самом деле нечто большее.
Смотрите, вот что говорит закон больших честь. Он говорит, ну хорошо,
пусть количество подбрасывания, давайте я напишу, ладно, пусть у нас есть монетка,
xn это количество решек прием подбрасывания к симметричной монетке. Симметричная,
значит убирает на срежке орла, одна вторая. Вот, что говорит ЗБЧ. ЗБЧ говорит вот что,
пусть у вас есть какая-то растущая последовательность wn, которая стремится к бесконечности,
тогда xn-n пополам поделить на корень из n wn должно быть близко к одной второй. Что
значит близко к одной второй? Близко к чему? К нулю должно быть близко. Я из xn вычел средний,
поделил на корень из n на w. То есть с одним словом вероятность того, что эта штука по модулю больше
чем x, стремится к нулю. Это ЗБЧ в точности, который мы с вами уже формулировали. В частности,
если я в знаменателе возьму hn, то есть если wn это корень из n, то здесь написано ничто иное,
как среднее количество решек минус одна вторая мало. Чем больше n, тем меньше эта штука. Тем
меньше отличается это усоединенное количество решек от одной второй. Ну а это еще более сильное
утверждение, что в знаменателе можно написать не n, а вплоть до корни из n. Вопрос, а что будет
если корень из n написать? Чему равна вероятность того, что xn-n пополам поделить на корень из n по модулю
больше чем x? Будет стремиться к нулю или нет? Более практический вопрос. Пусть n очень большой,
но я не знаю, скажем, миллион. Пусть n миллион. Ну что значит n миллион? Это значит,
что решек должно быть примерно примерно 500 тысяч. Ну вот чему можем ли мы с какой-то большой
точностью сказать, чему равна вероятность того, что xn больше чем, не знаю, 500-1000.
Ну вы скажете, да не бойся, одна вторая. Понятно, что вероятность того, что xn больше
чем 500 тысяч, это одна вторая. Почти. Потому что это как раз половина, это как раз ровно половина
миллиона. Но я не сильно от 500 тысяч отошел, я там не 10 тысяч прибавил, а всего одну тысячу прибавил.
Наверное, вероятность того, что xn больше чем 500-1000, тоже примерно одна вторая. Ничего подобного,
она меньше чем одна вторая значительно. Если вы там вместо 501 тысячи напишете 505 тысяч,
если вы посмотрите, наверное, что xn больше чем 505 тысяч, это будет примерно 0. То есть,
иными словами из вот этой теоремы мавролапласа следует, что вот не просто возле 500 тысяч должно
быть количество решек, а очень близко. На самом деле порядок, на который ваше среднее может
отличаться от имперического среднего, может отличаться от теоретического среднего, оно там порядка
корень из z. То есть, почему я здесь написал 1000, потому что 1000 это корень из миллиона. Давайте
увидим, что я прав, применив теорему, которую я вверху сформулировал. Ну вот, прям вот этот
частный случай будем решать. Вероятность того, что xn больше чем 501 тысячи. Это все в терминах
более общей задачи, которую я вышли сформулировал. Вот если я возьму n равно миллион, вот здесь будет
у меня стоять 1000. И тогда epsilon это будет типа единица. Это просто частный случай, задача,
которую я сформулировал выше. И раз я говорю, что нифига к нулю не стремится, это вероятность. Это
означает, что ответ нет. Вот здесь вот ответ нет. Не стремится к нулю. То есть, в законе больших
чисел корень из n это оптимальная граница. Я могу корень из n умножать на любую растущую функцию,
сколь угодно медленную, но убрать wn или написать констант вместо wn я не могу. Закон больших
чисел именно в этом месте совершенно оптимальный. Но давайте в частном случае вот для этих конкретных
чисел увидим, что мы можем применить наш теорим. Значит, xn имеет биномиальное распределение с
параметрами 10 в шестое, 1 в второе. Вероятность того, что xn, ну давайте посмотрим, что такое np.
np это 500 тысяч. Зачем мне нужна np? Потому что оно вот здесь вот стоит. Да, вот здесь есть np,
еще есть корень из np, 1-p. Давайте сначала поймем, что такое np это 500 тысяч. Теперь
поймем, что такое корень из np, 1-p. Ну это сколько? Это корень из миллиона умноженного до корня
значения. Да, то есть это 1000 пополам или 500? 500. Значит, вероятность того, что xn больше чем 500-1000,
это в точности вероятность того, что xn минус np больше чем 1000.
А это в свою очередь в точности вероятность того, что xn минус np, хоть зрительно корень из np,
1-p больше чем 2. Ну то есть если я из правой части не равен, вот это мое k.
Это мое k. Если я из него вычту np и поделю на корень из np 1-p, я получу 2. Вот, ну применяю
теорему муавролапласа. Да, я понимаю, что вероятность того, что xn больше чем k. Это
есть 1 минус вероятность того, что xn меньше оно чем k. Это примерно равно, я сейчас это прогоментирую.
Примерное равенство. Здесь есть нарушенная некая строгость. Значит, вот здесь есть мало от
единицы. То есть иными словами, левая часть стремится к правой, если я мало от единицы уберу.
Поэтому я, конечно, при фектированном n не могу сказать, что точно равно, но это n,
вот этот миллион, он настолько большой, что ошибка будет очень маленькая. Я сейчас отдельно
об этом скажу. Давайте сначала посчитаем, что тут получится. Это примерно единица минус
интеграл от минус бесконечности до 2. От 1 поделить на корень из 2p, e в степени минус x
Это табличный интеграл, то есть вы там можете написать функция Laplace в интернете или нормальное
распределение или в квантире нормального распределения. Не суть, в общем, как к воду
Найдете табличку значения вот этого интеграла. Открою табличку и посмотрю,
чему равен вот этот интеграл от минус бесконечности до 2. Вот чему он равен.
Он равен 0.9772, ну примерно. Он примерно равен 0.9772.
Да, значит наша вероятность, это примерно 0.228. Ну то есть очень маленькая вероятность.
Вероятность того, что их цен больше чем 501 тысяч, это 0.02. А если у вас стояло 505 тысяч,
то там было после запятощего куча нулей. Вот, теперь по поводу вот этого примерного равенства.
Есть теорема, которая называется Теорема Берри и Сейна. О том, с какой скоростью в интегральной
теореме мавролапласта левая часть сходится с правой. Да, но то есть иными словами оценивает вот
этого мало от единицы. Вот этого мало от единицы можно оценить там констант и поделить на коре низе.
То есть в нашем случае ошибка будет в тысячных. Ошибка будет в тысячных, то есть вот ну может
быть причем причем там будут какие-то там может быть на единицу в тысячных или даже в десяти
тысячных. То есть ну максимум здесь будет тройка стоять после двойки. То есть вот в этом знаке
может быть ошибка. Короче говоря, мы эту вероятность можем записать довольно точно.
Есть ли какие-то вопросы?
Ну я надеюсь, что я достаточно подробно пояснил, как применяется теорема мавролапласта и почему
она действительно такое очень значимое дополнение к закону больших чисел. На самом деле она даже
точнее, чем закон больших чисел. Из нее закон больших чисел можно вывести. То есть из того, что вы
знаете, что когда в знаменателе коре низе, то эта вероятность не тривиальная в пределе. Из этого на
самом деле следует, что если вы чуть-чуть увеличите знаменатель и сделать его растущим, то вероятность
сразу будет в деле ноль. Поэтому закон больших чисел на самом деле из теорем мавролапласта вывозит.
Теорем мавролапласта гораздо более сильного утверждения. Ну для бенмяльного распределения, но у нас
в будущем еще будет центральная предельная теорема, которая работает вообще для всех распределений,
а не только для бенмяльных. Хорошо, тогда докажем мы эту теорему в следующий раз. На сегодня все.
Если есть какие-то вопросы, задавайте.
Если вопросов нет, то всем до следующей субботы. Спасибо, до свидания.

вот соответственно сегодня продолжаем разговор про то что у нас происходит методах оптимизации
прошлый раз мы немного в некотором смысле поразвлекались что ли какие-то совсем простенькие
примеры посмотрели сегодня будем уже погружаться в сам предмет те в те в ту терминологию которая
нам понадобится в рамках курса вот соответственно да так окей напоминаю что мы с вами решаем вот
такую задачу оптимизации и в самом простом случае она у нас безусловная то есть то множество по
которым по которым мы хотим найти минимум оно просто соответствует всему вашему пространству
но никак не ограничено ну и соответственно водится следующее определение вот точка
их у нас называется локальный минимум функции f на на rd если существует некоторый шарик в данном
случае вот шарик определенно так вот у нас вокруг этой точке радиус этого шарика r
и следует на любой точке из этого шарика f от x звездой меньше чем f от y соответственно
понятную в этом шарике на иксе достигается минимальное значение вот мы соответственно
глобальный минимум точке их что глобально глобальный минимум на rd если у нас соответственно вот это
вот неравенство выполняется для любой точке и из всего множество то есть какой бы мы
играть не взяли у нас я по 30 но тут все понятно вот здорово ну то есть принципе вот это вот эти
все определения которые даны здесь можно обобщить и на случай когда у нас рассматривается не
безусловная задача условно на каком-то множестве x вот вам достаточно соответственно пересечь
вот здесь вот написать x вот и вот здесь вот этот шарик пересесть множеством
шарики и для всех игр которые содержатся ваше множество x по которым вы оптимизируете в
шарике у вас соответственно должно быть выполняться условие ну и во втором случае
аналогично здесь у вас становится множество x ну и здесь соответственно множество x вот тут
все более-менее просто какие-то такие совсем базовые определения ok давайте соответственно
докажем такой факт который вы скорее всего знаете уже с математического анализа что
соответственно в точке которая является локальным минимумом у вас дифференцируемая функция
принимает значение градиента данных в случае так как у нас она зависит от вертера аргументов
вот окей давайте докажем давайте пойдем от противного и предположим что в оптиме
назначение не 0 ну и разложим ряд в нашей окрестности локального минимума вот этой
точке x звездой соответственно ряд здесь выписан то есть нулевой член член первого порядка
вот и соответственно о малая которая выписана вот здесь то есть при стремлении x
x звездой у вас о малое вот это вот расстояние x минус x звездой нормировано на реальный
расстояние x звездой стремиться к нулю ну то есть это по факту просто векторное
определение малого объективного нормы вот ну и что соответственно мы делаем давайте попробуем
доказать что действительно от противного что если у нас градиент в оптиме локальный минимум не
равен нулю то это неверно и для этого давайте вот рассмотрим точку вот такого вида x волной
которая представляет собой x звездой минус какой-то коэффициент лямбда скалярный умножить на наш
градиент который не нулевой я напоминаю вот первая цель которую хочется поставить по-первых
попасть нашей точке и точка икса звездой в окрестность где у нас точка икса звездой
является локальный минимум вот икс икс тильда при достаточно малом лямбда понятно попадет в
окрестность потому что вектор градиента у нас зафиксирован вот шарик мы попадем такое лямбда
понятно подберется вот окей тогда соответственно что тогда с одной стороны у нас значения в ф с тильдой
должно быть больше либо равно чем в ф икса звездой просто по определению локального минимума мы
подобрали как раз окрестность так чтобы мы попали туда и в итоге нашли какое-то противоречие плюс
должна быть выполнена вот эта вот опроксимация которую мы записали в терминах о малая вот ну
здесь соответственно во второй строчке просто подставляю явный вид x стильдой вот здесь вот я
его просто выписываю здесь соответственно я уж подставляю что у нас икс стильдой имеет конкретный
вид икса звездой минус лямбда градиенты ф икса звездой ну вот здесь соответственно появляется вот
этот член плюс вот этот член вот понятно что в данном случае уже норма градиента только тут
да двоечку забыл вот норма градиент в данном случае это просто константа некоторая и умалые терминах
умала мы говорим уже с точки зрения лямбда вот окей ну и соответственно да лямбда мы можем
подобрать так поварировать так посмотреть посмотреть в том числе на константы градиента что у нас
умалое от лямбда вот это ведет себя там меньше либо равно чем лямбда пополам и множество норма
градиентов в квадрате вот просто по определению мало что у нас умала от лямбда и оно и при
достаточно малых лямбда будет меньше либо равно чем лямба пополам и множество какую-то константу вот
все ну и тогда я подставляю что вот у меня вот здесь вот эта вещь ограничена вот таким вот значением
лямбда пополам на норму градиента в квадрате и получаешь вот следующее выражение вот в силу
того что я подбирал лямбда достаточно маленьким но больше нуля в силу того что я предположил что
у меня норма градиента не равна нулю я получаю что значение в экстильдой меньше строго меньше
чем значение в точке икса звездой мы уж получаем соответственно насчет противоречия вот с тем что
у нас и икса звездой это локальный получили соответственно условия оптимальностей самое
одно из самых базовых условий оптимальности в оптимизации вот но вообще наша цель ведь находить
какие-то глобальные минимумы ну застряли мы в каком-то локальном минимуме это же не особо решение
задачки которая нам нужна вот цель находить какие-то хорошие решения глобальные минимумы вот и на
прошлой лекции в принципе стало понятно что эти глобальные минимумы вот довольно сложно находить
даже добавляя какие-то предположения на функцию все равно получается что лучше чем какой-то перебор
у нас ничего и не работает ну что довольно плохо поэтому как я и обещал сегодня мы будем разбираться
с предположениями которые нам помогут обойти эти проблемы вот и первый такой из таких предположений
это выпуклость выпуклость функции соответственно выпуклость функции в rd вот пусть нам до нас
соответственно неперерывно дифференцируема функция f будем говорить что она является выпуклой если
для любых на всем rd вот если у нас для любых x и y из rd выполнено следующее соотношение вот
некоторое соотношение которое нам пока не понятно что вообще значит вот часто пройдем дальше и
посмотрим на картинку вот но смысл такой то что на самом деле вот тут я сразу записал
определение опираюсь на то что функция у нас дифференцируема на самом деле как вы понимаете
выпуклые функции могут быть не обязательно дифференцируемы мы сейчас это поймем потому
что есть еще одно определение выпуклых функций и вы его будете разбирать на семинарах точнее
более подробно разбирать как первое так и второе определение даже скорее начнете со второго на
семинарах посмотрите примеры выпуклых функций посмотрите как можно проверять функции на выпуклость
как по первому определению как по второму так и по каким-то дополнительным фактам вот но второе
определение гласит оно вообще уже не требует никакой дифференцируемости оно просто гласит то
что у нас значение функции f вот такой вот точке лямбда x плюс 1 минус лямбда y что называется
выпуклой комбинации двух точек потому что лямбда берется от нуля до единицы ну получается у нас
есть две точки x и y и мы берем какую-то из этих точек на отрезке соединяющих эти две точки вот и
оказывается что для любых значений вот это и в точке значение функции в точке на отрезке вот
такая вот комбинация двух значений тоже выпуклая комбинация двух значений должна лежать выше вот
ну давайте посмотрим немного на физику обоих определений в лекциях мне скорее будет нужна
физика первого определения вот ну и соответственно картинка я вам ее нарисовал вот это пока не нужно
это мы дальше обсудим а смысл соответственно выпуклости означает в том что вот у вас есть
ваша целевая функция здесь она соответственно выделена такой линии без пунктирчиков непрерывистой
линии вот а выпуклость означает что вы проводите в некотором смысле линейную опроксимацию этой
функции по градиенту то есть вы берете значение в точке x данном случае просто f от x и линейно
ее опроксимируете исходя из того как выглядит градиент вот если вот функция лежит я реально
целевая функция лежит выше вот этой линейной опроксимации то тогда она выпукла вот это
свойство нам соответственно понадобится больше на лекции второе свойство которое я выписал оно
в принципе тоже хорошее и интересно и часто оно пригодается для проверки функции на выпуклость
полуопределения вот оно означает что вот у вас есть два значения f от x и 2 и 2 значение f от y и
вам говорится о том что отрезочек которые соединяют вот эти две точки соответственно
точку x и f от x и точку y и f от y он лежит выше чем сама функция ваша выпукла вот тоже хорошее в
принципе физическое свойство вот соответственно да вот но в случае когда у нас функция дифференцируема
вот эти два определения эквивалентны соответственно вторая понятно более общая в связи с тем что оно
справедливо не только для дифференцируемых функций например какие вы знаете сами просты
недифференцируемых функций модуль понятно вот он выглядит вот так вот является ли он выпуклым да
является просто по вот этому физическому смыслу который и второму определению который я вам сказал
вот какие бы вы отрезки не соединяли они будут лежать выше чем график функции вот об этом вы
соответственно подробнее уже на семинарах поговорите вот сейчас мы вам главное вот это
физическое свойство которое то что у вас линейная опроксимация всегда подпирает снизу вашу целевую
функцию вот хорошо аналогичное определение только сильная выпуклость сильная выпуклость теперь
у вас добавляется еще вот такой вот дополнительный дополнительный член ми пополам х минус
игрек в квадрате вот и аналогично можно вести ровно такое же определение для как раз случае
когда у вас уже не дифференцируема функции вот соответственно вот так вот два определения чуть
она стала в некотором смысле не знаю сложнее но на самом деле вот вот это выпуклость сильная выпуклась
дает более хорошие свойства для функции а именно на картиночке это выглядит следующим образом то
есть теперь у вас подпирается не вот этой линейной просто опроксимации не вот этой линейной
опроксимации а теперь вы говорите то что у вас функция растет но и растет быстрее может расти
быстрее чем квадрат то есть вот тут вот я уже подпираю чем-то более качественные то есть
квадратичные функции дополнительно то есть до этого у меня было просто линейная ну и она как-то
себя ведет рядом с этой линейной функции то есть она может там и вообще и вести себя как линейная
функция потому что там если мы реально рассматриваем а здесь вот вас еще подпирается дополнительно
квадратом вот что как мы увидим дает лучшее свойство как и для каких-то гарантий нахождения
решения так и работы методов вот вот такие два свойства выпуклости сильная выпуклость вот
хорошо тогда давайте докажем следующую теоремку довольно простую пусть она выпукла непрерывно
дифференцирована функция вот и у нас есть некая точка икса звездой что соответственно там у
нас значение градиента равно нулю не дописал значение градиента равно нулю тогда у нас
икса звездой это не просто какая-то точка это глобальный минимум выпуклых функций что довольно
хорошо то есть это уже какие-то гарантии что если я нашел стационарную точку то я сразу же нашел
глобальный минимум вот окей тут все просто просто делаем по определению записываем определение
выпуклой функции и все то есть тут просто нужно немного подставить точки которые вам нужны то есть
и точку икс ну и соответственно силу того что это у этого скалярно произведение обращается в
0 потому что градиент равен нулю тогда у вас это просто значение от икса звездой и получается
что ехал тикс больше либо равно ф от икса звездой что по определению вам сразу же говорит о том
что для любой точке икс у вас ф от икса звезды меньше либо равно и значение это хорошо классное
свойства, приближаемся уже к чему-то явно хорошему. В обратную сторону,
понятно, мы и доказывали, соответственно, получается у вас критерий. Глобально минимум
значения градиента равно нулю. Значение градиента равно нулю. Это глобально минимум
для выпуклой функции. Отлично. Теперь определение выпуклого множества,
потому что, в принципе, мы будем решать не только условные задачи, но и
безусловные. Сегодня мы, конечно, больше сконцентрируемся на безусловных
задачах, то есть на задачах, которые нужно, где нужно оптимизировать функцию на
Rd. Ну и, в принципе, безусловные задачи, мы дальше уже их будем касаться,
то есть через две лекции уже пойдут безусловные задачи. Ну и, соответственно,
чтобы их тоже уметь решать, нужно вводить множество, на которых мы будем пытаться
решать. В данном случае, опять же, вводится выпукло множество. Вводится оно
следующим образом. У нас множество выпукла. Если мы берем две точки и отрезок,
соединяющие эти две точки, тоже будет лежать в этом множестве. Подробно вы про
выпукло множество будете говорить уже на следующем семинаре, то есть не на этой
неделе, а на следующем. Сейчас вы закончите обучаться взять у производных,
там и градиентов, десианов, а потом, соответственно, перейдете как раз уже к
понятию выпуклостью, выпуклым множеством, выпуклым функциям. Вот. Слайд назад, пожалуйста.
Если что, слайды есть в чате. Еще один слайд назад. Да, да, смотрите, я же прям в
теореме прописываю, непрерывно дифференцируемо. Да, да, да, я же здесь тоже написал, что
равно нулю, описался здесь вот, вот. Равно нулю, равно нулю, тут не дописал. Вот. Вот.
Если что, слайды я выложил в чат, чтобы вы в формулировке теореме, если что, видели. Вот.
Мы сразу быстренькие вопросы. Какое множество выпуклое, какое множество не выпуклое?
Да, первое, третье у нас, соответственно, понятно выпуклое, первое, третье вот, а второе нет. Второе
нет, ну понятно в силу того, что ровно по физическому смыслу определения выпуклого
множества две точки, соединяя лежащие во множестве, должны содержать и отрезок,
который тоже должен принадлежать этому множеству. Вот. Тут какой-то бак, вот. Можно, в принципе,
рассмотреть и более этот. Сложный вариант, но это скорее так, примеры. Вот. Можно, можно поиграться,
там где-то убирать границы, где-то не убирать границы. Нас сейчас нет, там главное просто понять
суть, что у нас есть отрезок, он должен лежать там. Поиграть и с границами на семинарах. Вот.
Окей. Смотрите, на самом деле, определение выпуклой функции и сильно выпуклой функции можно обобщить
и не просто на R, ну а сказать, что она выпукла просто на каком-то множестве. Но при этом почему-то
нужно всегда добавлять, что множество, на котором вы хотите задать выпуклость, оно выпукло. Вот. Как вы
думаете, зачем? Ну да, то есть помните, в чем сам суть и того, и другого определения выпуклости,
особенно вот того, который более такой наглядный, что у вас отрезок, соединяющий точки x, f от x,
и соответственно y, f от y, он должен лежать выше, чем график функции f и y. Вы же здесь проверяете
любые точки вот этого множества, да? Вот. В принципе, вы должны уметь проверять любые точки. И если вы
рассматриваете уже не выпуклое множество, вы можете взять и вырезать какой-то кусок отсюда. И здесь
с функцией может происходить все, что угодно. Вот. Как вы при этом что-то там собрались минимизировать,
вот. Как вы будете там методами в том числе обходить вот эти ямы, ну никто не знает. Соответственно,
понятно, что хочется вот такие эффекты выбирать, и поэтому если мы задаем выпуклость уже не на rd,
а rd, как вы понимаете, это выпуклое множество, просто потому что оно содержит все векторы,
соответственно, и любые отрезки. Вот. То, соответственно, да, можно обобщить при понятии
на какое-то любое выпуклое множество. Окей. Здесь я соответственно ввожу еще одно оптимальное
условие, когда мы минимизируем функцию не на всем r, а на каком-то выпуклом множестве x. Вот. Ну и,
соответственно, условие оптимальности выглядит следующим образом. Доказывать я его сегодня не
буду, потому что, в принципе, оно нам не понадобится на сегодняшней лекции. Доказательство его уже будет
в пособии. Вот. Давайте просто посмотрим на физический смысл. Вот. Физический смысл по факту означает то,
что у вас градиент указывает внутри вашего выпуклого множества. Вот. То есть любой вектор
градиента и как бы вектор, соединяющий какую-то точку множества и ту оптимальную точку,
которая является решением, они должны между собой иметь острый угол, потому что, в принципе,
как раз вам градиент указывает на рост функции, антиградиент вам указывает на то, куда функция
убывает. Вот. И понятно, что если вам направление убывания, оно вдруг окажется внутри множества,
вы туда и будете убывать. Вот. Значит, вы не нашли ни оптимум. Вот. А так, ну, соответственно, да. То
есть, получается, вы лежите на границе и как бы не посмотрели, куда градиент, в какую бы сторону вы
там этот вектор не прочертили, который с точкой x, у вас оказывается, что угол между градиентом и
этим вектором, он острый. Вот. Значит, что в направлении множества, у вас идут только увеличения.
Более формально доказательство, соответственно, будет в пособии и, соответственно, возможно,
мы докажем, оно довольно простое, не уверен, что нам понадобится через две лекции даже, возможно,
просто в пособии будет. Вот. Окей. Давайте еще чуть-чуть поговорим о том, что мы можем сказать о
минимумах выпуклых функций, то есть мы поняли, что если мы нашли значение градиента района нулю,
то сразу же у нас глобальный минимум. А вообще вопрос,
существует ли локальный минимум выпуклых функций, вот? Ответ — нет. Ответ — нет. Локальных
минимумов выпуклых функций не существует. Вот. Если, соответственно, локальный минимум,
если мы нашли локальный минимум выпуклых функций, он сразу же является глобальным,
что значительно лучше. Докажем это следующим образом. Пусть мы нашли
какой-то локальный минимум, рассматриваем вот такую выпуклую комбинацию нашего
локального минимума и какой-то точке x из нашего множества x. Что вы можете
сказать о точке x, которая здесь у вас написана, x лямбда, что вы можете о ней
сказать в силу того, что как мы ее определили.
Супер! Почему? Потому что это выпуклая комбинация, x у нас в x, x с звездой тоже в x,
просто потому что это решение в этом множестве. Да, мы знаем, что у нас x
со звездой, или здесь надо приписать звездочку, она у нас лежит в x.
Соответственно, давайте подберем лямбда достаточно малым, чтобы у нас x лямбда
попадало не просто во множество x, а попадало еще в ту окрестность, где у нас x
со звездой является локальным минимумом. Опять же, в силу того, что у вас вектор x
какой-то фиксированный в данном случае, несмотря на то, что вы это брали как
произвольную точку для любого, x вы такую лямбду подберете, чтобы вы взяли и
попали в окрестность, где у вас x со звездой является локальным минимумом.
А не должно быть наоборот, что лямбда уходит в единицу, а он уходит в единицу?
А, да, тут скорее лямбда, наоборот, близким к единице. Если бы наоборот написал вот так,
вот сюда лямбда, тут один минус лямбда, тогда мало, да, спасибо. То есть понятно,
все равно суть, варьируем лямбда так, чтобы у нас в итоге эти две функции,
точка x лямбда попала в окрестность, где у нас x со звездой экстрем. Дальше что
делаю? Пользуюсь определением, во-первых, локального минимума, то есть у нас
значение в x со звездой меньше, чем значение в x лямбда. А дальше пользуюсь
выпуклостью, потому что x лямбда это выпуклая комбинация точек. И соответственно
что у меня получается? Лямбда f от x и 1 минус лямбда f от x со звездой, так? Вот. Ну и что я
из этого могу? Какой вывод сделать? Какой я могу отсюда сделать вывод? Все видят, да, то есть я
просто из правой, из левой части вычту 1 минус лямбда f от x со звездой, да. И что у меня
останется тогда там с коэффициентом лямбда, но по факту останется то, что f от x меньше либо
равно f от x со звездой. Согласны? Супер. Вот. Но в силу того, что мы x брали произвольным,
получается, что для любой точки x у нас это выполнено. Значит x со звездой это просто глобальный
минимум. Окей. Как мы с вами уже обсуждали, в случае выпуклых функций у вас множество может
как содержать решение. Вот. Может оно быть, и решение может быть не единственным. Мы смотрели
на функцию x минус y, ну там x1 минус x2 в квадрате. Вот. А может вообще не содержать решение. Помним
тоже линейную функцию, которая является, как вы понимаете, выпуклой. В силу опять же определения,
понятно, что линейную функцию снизу подпирает линейная функция. Вот. Линейная функция у вас
выпуклая, но она всем r, она у вас решение не содержит. Вот. Поэтому выпуклые функции,
могут как иметь нулевое множество решений пусто, как так, и в соответствии далеко не
уникальное решение. Вот. Но можно доказать, что множество решений для выпуклой задачи,
то есть с выпуклой множеством, с выпуклой функцией, тоже является выпуклым множеством.
По определению, пустое множество и множество из одной точки
понятно выпукло.
Первое, это скорее так договоренно считать, что пустое множество
выпукло.
А второе, понятно, у вас одна точка, вторую точку
просто оттуда не возьмете, отрезок содержит одну
точку.
Интереснее более случаи, когда у вас две точки в
этом множестве.
Ну и соответственно, что нужно сделать, рассмотреть
правильно выпуклую комбинацию этих двух точек.
Ну и посмотрим, что там происходит с этими двумя
точками в этой выпуклой комбинации.
Опять же, в силу выпуклости я знаю, что у меня вот это
значение, в этом значении оно меньше, чем оптимальное
значение.
Просто в силу определения того, что у меня х1 со звездой,
х2 со звездой, это какие-то решения.
Соответственно, на отрезке значения, либо больше, либо
равно.
А дальше я расписываю по выпуклости и получаю
вот такую комбинацию двух значений.
Я же знаю, что у меня значение в х1 со звездой и в х2 со звездой
это просто оптимальное значение f со звездой, ну и f со звездой.
Ну все, получаю f со звездой.
Ну и что получается?
Что у меня вот это значение f от х лямбда со звездой
подперто с двух сторон.
F со звездой.
Получается, что оно равно f со звездой.
Получается, что любая точечка, любая выпуклая комбинация двух решений также
лежит во множестве решений. Получается, что множество решений у нас действительно
выпукло. Хорошо. Ну и соответственно, хороший факт то, что в сильно выпуклом
случае у вас множество решений состоит ровно из одной точки.
Нет, тут в данном случае я пока утверждаю, что если оно вообще решение есть, то
оно состоит из одной точки. Ну и соответственно, доказываем от противного.
Пусть у нас два решения, которые не равны друг другу. Рассматриваем какую-то
выпуклую комбинацию. Причем эта выпуклая комбинация берется так, чтобы у вас
она не совпадала ни с одним из крайних значений, просто какая-то точка на
отрезке. Вот. И рассматриваю, что происходит с этой функцией нашей функции f в этой
точке. То есть она, понятно, больше либо равна, чем значение оптимальное. Дальше я
использую определение сильной выпуклости. Видите, тут добавился вот этот кусочек,
который из определения сильной выпуклости. Ну а дальше что? Дальше что? Я говорю то,
что у меня значение f от x со звездой 11 со звездой, f от x со звездой 2 со звездой,
а не эквивалент на f со звездой. Выписал. Остался вот этот кусочек, который, как вы понимаете,
какой? Вот этот кусочек что про него стоит? Ну с минусом он будет отрицательным. То есть лямбда
мы взяли неравную единицу нулю. Взяли специально так, чтобы были точки. И точки по предположению x
11 со звездой и x 2 со звездой между собой не равны. Мы предположили, что решение не единственное.
Поэтому вот то, что я выделил, оно отрицательное. Получается вы из оптимального значения вычислили
отрицательное, и у вас получается противоречие f от x со звездой меньше, чем f от x со звездой,
потому что вычтено что-то явно отрицательное. Все, окей, тогда получили противоречие. Значит,
точка всего одна, если она вообще существует. Оказывается, можно и доказать, опять же,
будет в конспекте, что такая точка вообще единственна. То есть не просто она одна,
она обязательно еще и существует. Доказательства тут довольно тоже интуитивные. В силу того,
что у вас, как мы помним в определении сильно выпуклая функция, она у вас подпирается внизу
параболы. Давайте я верну на картинку. В выпуклом случае у вас может постоянно вот эта линейная
функция опускаться, а в сильно выпуклом случае у вас это парабола, хорошая парабола, которая вас
ограничивает. И вы знаете, что у вас функция ограничена этой параболой. То есть там есть
ограничение снизу, а значит и будет достижение. Более формально будет в конспекте. Все мы сегодня
реально не успеем, потому что нам нужно успеть еще и самый первый метод рассмотреть оптимизации.
Окей, так здесь посмотрели, доказали. Хорошо. Дальше еще два факта про сильную выпуклость.
Первый нам скорее понадобится для доказательства, и доказывается он довольно тривиально из
определения. Раз факт, то есть дифференцируемая функция у нас выпукла, если выполнено вот такое
вот соотношение. Это эквалентное определение тому, что у нас было до этого. Можно использовать такое
определение, если удобно. Вот мы и в доказательстве в том числе и воспользуемся в дальнейшем. Опять же
доказательства в конспекте. Второй кусочек, это вот тоже довольно важный факт, так называемый
критерий сильного выпуклости. Для дважды непрерывно дифференцируемая функция. Функция у вас
соответственно является сильно выпуклой, если ее гессиан является положительно определенным.
Ой, ой, ой, ой, ой, ой, ой, ой, ой, ой, ой, ой, ой, ой. Да, да, да, я извиняюсь.
Дозы петто, ой да, Господи, что-то у меня, я поплыл в это определение конкретно.
вот так оно должно выглядеть вот я поплыл да здесь в определении на всякий
случай зафиксируйте себе или там что вот я понятно конспект как я это поправлю
вот вот мю вот оно мю вот я же говорю я тут это конкретно поплыл в двух местах
смотреть это положительная определенность положительная
определенность то есть у вас матрица гесса должна быть положительно
определена вот это критерий и он вам понадобится в домашнем задании вот опять
же сегодня уже не успеем его доказать пособие будет он вам скорее вот нужен
для проверки некоторых свойств функций в домашнем задании вот я думаю там будет
понятно где вам этот критерий может пригодиться вот поэтому мы сегодня на
него и смотрим вот чтобы вы его уже знали и могли использовать в своих
целях вот окей так все могу перелистывать
супер вот так тут как раз про это говорится продезе 2 определение которое
нам понадобится это гладкость это гладкость выпуклость это вот по одну
сторону то что мы поговорили то что у нас функция ограничена а гладкость это как
то не странно будет в другую сторону но сначала это нужно будет понять вот пусть
дана опять же у нас непрерывно дифференцируема функция вот давайте
пусть будет сразу тоже r вот тогда у нас функция является гладкой
л гладкой либо можно эквивалентно говорить именит и липшицев градиент если
выполнено вот следующие условия вот то есть вот такого условия некоторые на
резкость изменения градиента вот у нас было в прошлый раз условия на
резкость изменения функции липшица функции теперь у нас липшицевость
непрерывность градиента вот непрерывность градиента вот давайте
попробуем разобраться связи с физическим смыслом тут я просто даю
некоторую ремарку то что гладкость на самом деле можно может определяться не
только во второй норме но и там в любой другой вот нам это сейчас долго еще не
понадобится но вот можно про это знать вот давайте докажем вот такое
замечательное свойство для гладких функций вот почему я в том числе не
доказывал эти там положительная определенность гессиана просто потому что
доказательство тех фактов похоже вот на то что я сейчас буду делать поэтому это
особо и то есть как бы технику мы поймем потом уже это как бы по аналогии в
некотором смысле доделывается ну то есть там главное сейчас вот этот один факт
вот окей я что делаю я беру формулу ньютона лебница вы на нее сейчас
внимательно смотрите на верхнюю строчку и говорите понимаете вы ее или нет что
я написал тут можно сказать что вы не понимаете понимаете ли вот верхнюю
строчку да вы что все понимают производная по направлению все правильно то
есть смотрите что там что произошло верхней формуле это скорее это ваш третий
семестр третий семестр когда вам соответственно определяли определяли
интеграл по кривой а б и это соответственно чему равнялась просто
значению f в точке rb минус значение в точке r в точке a где r это некоторая кривая
вдоль которой вы интегрировали ее можно как-то запереметризовать в данном случае
она запереметризована параметром того так вот вот какая у меня кривая у меня
кривая довольно простая точка x плюс соответственно направляющий вектор y
минус x который как раз и отвечает за то в какую точку я попаду то есть там я
варьирую от нуля до единицы соответственно в нуле я в иксе в единице я в игреке то есть я
запереметризовал мою кривую как раз вот начальная точка x и вектором направления y минус
x все у меня есть кривая r по которой вдоль которой я интегрирую справедливо формула ньютона
лебница также вроде как вы скорее всего мотонализе должны были доказывать то что такого рода
интеграл и когда у вас есть функция потенциал так называемый есть ее градиент то есть вот то что
у вас записано это есть просто градиент некоторого потенциала то такой интеграл
вас вообще не зависит вдоль какой кривой вы там интегрируете вот такой факт у вас вроде как был
вот но нам он в принципе даже особо и не важен сейчас главное что вот эта формула ньютона лебница
она справедливо справедливо вот теперь понятно откуда взялась но я надеюсь туда вот дальше
дальше вот во второй строчке что я делаю нет смотрите кривая так как задается если я
поделю на модуле у меня будет единичный вектор а мне нужно попасть из точки x точку y
на единичный вектор не смотрите у вас же вот в этой формуле вот так вот если и дальше вы еще
будете раскрывать у вас так ну давайте выпишем производная так так выписали
у
y минус x вот так вот детал все так а ну давайте рассуждать нужно ли поделить на y минус x вот
у вас же кривая как задается вот с этой формулы вы согласны вот с этой формулы согласны
нижний смотреть а хорошо тогда у нас есть кривая в точке соответственно rb и ра но в данном
случае rb это просто точка y ра это точка x так правильно вот ну и соответственно вот у
меня получается f от x минус f от y ой f от y минус f от x вот ну и я задаю как вот этот вектор вы
хотите его отнормировать еще то есть у меня как раз вот здесь точка y здесь точка x я задаю как
x tau y минус x вот что будет когда я продиференсирую возьму как раз дифференциал rt у меня чему
равен дифференциал rt чему равен вот здесь вот если на это смотрю y минус x надо tau вроде
все правильно то есть нормировки никакой не вижу нормировка нас какой-то единичный вектор
переведет то есть я не попаду в точку y я попаду на расстояние 1 от точки x направление точки y так
окей или нет смотрите то есть у нас же что у нас есть кривая точка x точка y задается вот
таким вот уравнением точка условно b которая там вот в этой общей формуле это y точка a это x так
я соответственно вот выписываю f от y минус f от x соответственно если я вот здесь вот отнормирую
на x минус y по модулю я же не попаду в точку y просто если от tau буду варьировать от нуля до
единицы вы можете это нормировку вот у меня вот здесь вот эта нормировка она как бы зашита сюда
вы можете вот разделить на x минус y но тогда вот здесь вот и родить вот этот x минус y чтобы
вы как раз попали в игре окей всем понятно я надеюсь туда вот так дальше вторая строчка
ничего сложного просто вытащил вот это которая в принципе не зависит от tau и здесь я про
интегрировал по tau потому что тут никакой зависимости от tau в этом скалярном произведении
нет поэтому интеграл от нуля до единицы просто будет давать единицу умножить это скалярное
произведение дальше дальше ну я счет получил раз два вот это я сгруппировал поставил сюда
модули еще сгруппировал и поставил модуле вот дальше просто равенство и модуль от интеграла
модуль от интеграла дальше что интеграл же то получается мне что модуль суммы ну понятно что
что сумма модулей больше, больше либо равна, чем модуль суммы. Согласны? Вот. Оценили. Занесли,
соответственно, модуль под интеграл. Хорошо. Это прям хорошо. Вот. Занесли модуль под интеграл.
Дальше, что я делаю? У меня есть скалярное произведение. Кстати, я надеюсь, все понимают,
что вот эти скобочки – это скалярное произведение. Вот. Просто для меня шок был,
оказывается. Раньше же вы обозначали типа круглыми скобочками. Я уже привык вот к этим. Вот.
И кто-то спрашивает у меня иногда. Вот. Окей. Смотрите. Модуль. У меня стоит как бы разность
градиентов. И еще стоит разность аргументов. Ну, вот разность градиентов она как бы как раз
намекает на то, чтобы хотелось бы воспользоваться гладкостью по определению. Вот. Для этого я применяю
КБШ. Вот. Коши-Бунюковский-Шварц. В векторном виде. Все знают, все помнят. Или хотя бы слышали.
Аб меньше либо равно, чем… Просто аб. Согласны, да? Все. Коши-Бунюковский-Шварц. Ничего тут опять
сверхъестественного нету. Вот. Вытащилась у меня норма разности градиентов. Вытащилась разность
аргументов. Супер. Вот. Пользуюсь гладкостью для вот этого безобразия. И выношу коэффициент.
Тут как раз выскочит у меня L у-х в квадрате, потому что вот здесь еще есть у-х. Так? Вот. А под интегралом
останется только tau. Вот. Но вот с этим интегралом мы, конечно, справимся уже. Вот. И успешно
справляемся. Вот. Все. Вопросы? В принципе, вот те свойства, которые я не доказывал,
доказываются похожим образом. Вот. Поэтому я показал только вот это. Вот. Хорошо. Получили
замечательное свойство, которое мы с вами доказали. Анонсировали до этого. Вот. Оказывается,
для выпуклой функции справедливо оно же, но еще и вот этот модуль можно раскрыть просто по
определению выпуклости. Вот. До этого я поставил модуль, но по определению выпуклости подмодульное
выражение больше нуля. Вы модуль раскрываете и просто записываете. Опять же, ничего такого
сверхъестественного нету. Первое свойство. На самом деле оно хорошо отражает физику. Вы уже на
предыдущих картинках это все, в принципе, видели. Сейчас давайте вам картинку покажу. Это заранее.
Первое свойство отражает физику. Гладкости. Вот. Выпуклость. Ограничение снизу линейной функции.
Сильно выпуклость. Ограничение снизу не просто линейной функции, а какой-то еще и дополнительно
подпертый квадратичной. Вот. Гладкость. Это уже ограничение сверху. Тоже квадратичной функции. Вот.
Ну и получается, что вот как раз в сильно выпуклом случае у вас по факту функция как бы в некотором
смысле подпирается двумя параболами и как-то между ними себя ведет. Вот это все. Вся физика того,
какого рода класс задач рассматривается. В выпуклом случае, соответственно, подпирается не параболой,
но у него там коэффициент квадратичный равен нулю. Вот. Второе свойство. Оно нам скорее понадобится для...
Так, пара 12. 10, да? Вроде нормально. Вот. Второе свойство. Оно нам понадобится как раз больше для
доказательств, чтобы бороться как раз с разностью градиентов. Первое оно скорее про физику,
про физику того, что происходит. Второе свойство про, скорее, какое-то техническое свойство,
которое понадобится больше для доказательств. Вот. Хорошо. Тут просто про первый факт сказано,
мы это обсудили. Вот. Давайте про второй. Смотрите, чтобы доказать второй факт, рассмотрим вот такую
вот не совсем обычную функцию. Какую вспомогательную функцию phi, которая представляет собой f от y,
и вот такой вот градиент умножить на вектор y. Градиент при этом фиксированный,
точка x фиксирована. Вопрос. А вот является ли это вообще функция гладкой, с какой-то константой
липшица. Вот. И выпуклой. Как вы думаете? Я вот ее рассмотрел зачем-то. Вообще она выпуклая и гладкая или нет?
Ну что нужно делать, чтобы проверить? У нас не так много вариантов. Вот. Давайте по
определению лупанем. Вот. Так. Что там в градиенте у phi будет стоять? Давайте только тут точки,
пусть будут, чтобы x не употреблять, я везде y поставлю. Что там в градиенте у phi будет? Расскажите
мне. Люди, которые сегодня будут писать. Отлично. Что еще? А у скалярного произведения
какой градиент? При фиксированном векторе. У какого скалярного произведения будет градиент
при фиксированном f от x? Градиент f от x. Сколько у скалярного произведения будет? Вот у этой
просто функции. Давайте продиференцируем функцию a от x одномерно. Сколько будет градиент? Сколько? А. А здесь сколько?
Левая часть. Все правильно. Я f от x. По-моему, это базовая вещь, которую вам должны были
упомянуть на семинаре. Градиент, условно, линейные функции, это просто вот кусочек градиента. Вот.
Это же по определению просто показывается. В разности расписывать, у вас градиент остается. Вот.
Что? И вот здесь у вас что будет? Соответственно, минус f от x и плюс f от x. Один у вас вылез
с минусом, когда брали градиент по phi y1, а второй с плюсом вылез, когда брали по y.
Потому что он не меняется, он сокращается. Правда, он сокращается. Вот. Он сокращается. И у вас остается
просто что? Что константа липчется, эквивалент на какой константе липчется? Для функции f.
В первом? Тут можно и равенство поставить, я согласен. Все. Ну в принципе было же верно. А в
первом взяли градиент. Взяли градиент. Градиент phi от y равен чему? Градиент f от y минус
градиент f от x. Все. Мы это подставили туда. Все. Хорошо. То есть мы нашли константу липчется.
Нашли константу липчется. Вот. Является ли эта функция выпуклой? Да, является. Я уж не буду это
проверять. Тоже проверяется по определению. Вот. Градиент вы уже научились брать. Вот вы знаете
вот этот градиент. Его можно подставить в определение функций. И все будет нормально. Вот. Все. Тут я
пишу как раз константа липчеца совпадает. Просто проверяем по определению. Я вот тогда уж надеюсь,
что вы это будете проверять по определению как упражнение. Потому что если вот тут уже какие-то
вопросы с градиентом возникли у довольно простой функции. Вот. То нужно что-то дома тоже попробовать
порешать. Вот. Тем более вас сегодня что-то спросят. Как посчитать градиент? Вот дадут вам линейную
функцию, а вы так о. Также что можно заметить? Смотрите. А y со звездой равный x, это минимум,
оказывается, нашей функции phi. А почему? Почему? Градиент равен нулю. Все правильно. Градиент
в этой точке y со звездой равный, соответственно, f от y со звездой минус f от x равен нулю. Если
я действительно подставлю сюда x, будет 0. Все получается хорошо. Вот. Как раз все правильно.
Градиент равен нулю. Воспользуемся первым утверждением. Раз мы его с вами доказали. Давайте им
воспользуемся. Вот оно первое утверждение про гладкость. Вот. Но тут я подставляю довольно хитрое.
То есть вот тут y это вот для первого утверждения. Здесь x это тоже для первого утверждения.
Подставляю вот соответственно y для первого утверждения вот таким, x вот таким. Ну и соответственно
вместо функции f я подставляю теперь уже мою функцию phi. Вот. Ну и все. То есть вот раз f от y это,
вот это f от x. Дальше скалярное произведение выписывается. Разность между y и x она понятна.
Она будет просто минус 1 делить на l градиентов от y. Вот. Ну и здесь все. Вот этот кусочек. Зная
константу липшица. Я его здесь выписываю. Окей? Все понятно вроде, да? Не должно быть.
Так. Вопрос есть или нет здесь? Вот. А теперь смотрите. Я вот в этом скалярном произведении
что могу найти? Вот в этом скалярном произведении. Что это? Один и тот же вектор на себя. Это модуль,
да, конечно. Только с коэффициентом 1 делить на l. Так. Вот. Поэтому в правой части тоже что-то
стоит похожее. Коэффициентики можно поубирать. Ну там соответственно 1 делить на 2 l. Тут будет l,
остается 1 делить на 2 l. Все. Все. Просто перестановочка. Получилась перестановочка. Я
выписал то, что получается. Здесь вот это значение в моей точке y, которое я взял. Это мы то,
что получили на предыдущем слайде. Но я говорю то, что это меньше, чем значение в оптимуме.
Понятно. Потому что какая тут произвольная точка, а тут значение в целом оптимуме. А здесь вот x это
значение. Поняли, что оптимум это x. Все. Супер. Тогда можно просто подставить вместо phi вот в эти два
неравенства реальное значение. Чему же это phi равно? И получить то, что в принципе мы и хотели.
То, что мы и хотели. Вот. Супер. Вот такое вот свойство. Я тут переписываю уже в том виде,
в котором я его сформулировал в теореме. Как им пользоваться, мы уже увидим дальше. Вот.
Сейчас? Ну если у вас получается. Супер. Вот. Никто не успевает. Помедленнее.
Хорошо. Хорошо. Давайте помедленнее пойдем. Вот. Так. Ну я вроде комментирую то, что там происходит,
куда мы что передвигаем. Вот. Вот можете проверить вот этот переход, что я все переставил тут верно. Вот.
Я могу хоть эти выкладки делать сам рукой, потом они у вас просто окажутся в готовом виде. Давайте
делать так в следующий раз. Я буду рукой писать. Вот. Давайте так тогда. В следующий раз давайте так.
Вот. Хорошо. Соответственно получилось свойство, которое понадобится нам в дальнейшем. Вот. Тут у
меня конечно был вопрос, а где мы воспользовались выпуклостью? Потому что вроде как вот в этом
неравенстве, когда я делал переход, я пользовался только тем, что мы в принципе доказывали в
произвольном случае. Когда доказывал вот это для гладкости, я ставил модуль и там не было,
никак не использовалось то, что функция выпукла. Это потом мы его раскрыли положительно. Все. Супер. Мы
пользовались тем, что градиент равен нулю, и поэтому мы говорили, что это минимум. Глобальный
минимум функций. Вот. Вот здесь мы и пользовались выпуклостью. Вот. Хорошо. Так. Вот здесь есть ответик
на этот вопросчик. Физический смысл, который мы с вами обсудили. То, что у нас гладкость
ограничивает сверху. Вот. Ну и здесь соответственно сильная выпуклость и то же самое. То есть у нас
есть ограничение снизу, сильная выпуклость и ограничение сверху через это. Вот. Окей. Переходим.
Наверное главной ключевой темой сегодня это градиентный спуск. Метод, который в принципе зародился.
Первый метод оптимизации, который вообще появился, придумал его Коши для решения
соответственно системы линейных равнений. Это мы немного с вами обсуждали в прошлый раз. Вот.
Рассматривается. Безусловная задача оптимизации множество оптимизации РД. Нет никаких ограничений
всего пространства. Вот. В чем идея? В чем идея? Как мы знаем, что у нас производный или градиент
просто указывают в направлении возрастания функции. Соответственно антипроизводный или
антиградиент указывают, будьте здоровы, в направлении убывания функции. Вот. Ну и соответственно
возникает мысль. Если нам градиент локально говорит, куда функция убывает, ну давайте я пойду вдоль
градиента. Ну вот соответственно это и есть реализация. То есть у нас есть какая-то текущая
точка, мы считаем в ней градиент, делаем какой-то шаг вдоль этого градиента, получаем новую точку.
Вся идея. Очень просто, очень понятно. Вот. А теперь нужно понять немного смысл. Давайте
на всякий случай вы мне скажете, куда указывает градиент в точке x1. Перпендикулярно это прямой.
Сюда или сюда? Вот здесь оптимум у нас. Наружу. То есть функция увеличивается сюда. Вот. Здесь как
раз изображены линии уровня, что вот здесь функция меньше, здесь больше становится. Увеличение
функции идет вот сюда. Вот. Соответственно градиент у нас указывает наружу. Вот. А антиградиент как
раз наоборот внутрь. Вот вся и суть. Используя какие-то локальные свойства толкаться к решению. Вот.
Вот направление роста. Соответственно в направлении убывания мы и пойдем. Вот. Возникает вопрос, а
зачем вообще в градиентном спуске нужен шаг? Вот у нас есть какая-то функция, есть стартовая точка.
Я зачем-то в градиентном спуске добавлял шаг. Например, функция квадратичная x2. Понятно,
у меня там производная будет просто x. Ну давайте по ней будем ходить. Там x-гамма kx. Вот. Конечно,
понятно, что если взять гамму равной единичке, мы сразу в оптимум придем. Ну что будет,
если мы будем менять гамму от маленьких до больших значений? Да. Если гамма маленькая,
мы будем как-то медленно-медленно вот так потихонечку шаг за сожжечком спускаться. Это вроде как долго.
Вот. Вроде как долго. Если мы возьмем гамма довольно большим, то есть мы можем как-то за
несколько шагов, а то и за один шаг дойти до решения. Вот. Мы можем взять гамма довольно большим,
что мы будем в некотором смысле скакать вот так вот из разных кусков параболу. Так. А может
произойти вообще ужасная ситуация, что мы настолько доверились градиентному спуску,
мы настолько доверились градиенту, что он нам верно отражает какие-то локальные свойства функций.
Ну и предположили, что эти локальные свойства хорошо апроксимируют всю функцию, что взяли точку,
взяли огромный шаг, ну и начали вот так вот шагать. Понятно, вы разойдетесь. Вот. Видите,
в чем суть? То есть суть градиентного спуска, что вы по факту рассматриваете эту апроксимацию,
рассматриваете апроксимацию первого порядка вокруг вот точки. Вот. Соответственно,
у вас здесь градиент будет стоять, здесь будет стоять расстояние. Вот. Апроксимация первого
порядка может быть нехорошей. То есть да, для каких-то функций она может быть близка к реальности,
для каких-то, ну вы понимаете, что функция ведет себя лучше. Вот. Или наоборот, хуже, чем вот эта
апроксимация. Поэтому нужно уже учитывать, что вот это локальные свойства градиента, они имеют
свойства меняться вообще с точкой. Поэтому большие шаги могут вас ввести совсем далеко. Вот. Ну и давайте
попробуем доказать сходимость градиентного спуска. Вот. Ну и давайте я вот тут уже буду руками
попробуем доказывать. Тут что-то я уже выписал. Вот. Но тут в принципе все понятно будет. Я не
показываю результат, потому что мы пока, ну я вам в прошлый раз показывал теоремы, где мы по факту
там оценки какие-то получали уже. Зачем их сразу показывать, если по факту результат становится
понятен уже после того, как мы что-то доказали. Здесь что хочется сделать? Давайте рассматривать
гладкие и мюсельно выпуклые задачи. Знаем, что оптимум уникален. Так. Ну и что? Посмотрим, насколько
мы к нему приближаемся. Вот. Каждую итерацию. Что я делаю? Я подставляю шаг градиентного спуска.
Окей? Все. Пока здесь ничего такого. Дальше я раскрываю этот квадрат. Раскрываю этот квадрат.
Раз-два. Уходит в одну скобку. Вот этот кусочек уходит в другую скобку. Плюс удвоенное произведение.
Все. Тут ничего такого сверхъестественного. И у меня к вам вопрос. А что мы будем делать дальше?
Что мы будем делать дальше? Вот. Пока я сделал какие-то самые простые манипуляции. Что у нас есть,
чтобы можно было что-то пооценивать. Я же не зря предположил гладкость и сильную выпуклость.
Вот. Давайте как-то попробуем их использовать. Конечно. Давайте попробуем неравенство применить.
Вот. Смотрите. Норма градиента. Как оценим? Что у нас было в гладкости? Какие может быть свойства
в гладкости? Какие может быть свойства в выпуклости? Сильные выпуклости были, чтобы это можно было
оценить. Норма градиента. Где мы встречали с вами норму градиента в квадрате? Через константу
сильной выпуклости. Где мы встречали норму градиента, чтобы можно было сверху оценить? Мы же
хотим сверху оценить, насколько мы вообще хорошо приближаемся. Где мы в гладкости встречали?
Помните, как раз у нас было вот так. Вот. Но здесь у нас точка одна. А вторую где взять?
Ноль. Правильно. Не зря же мы это условие оптимальности сегодня обговорили. Вот.
Давайте добавим этот умный ноль сюда. Так. Хорошо. И с этим мы уже понимаем, как действовать. Так.
Я взял и расписал по определению. L гладкости. Согласны? Вот. С этим кусочком что-то тоже нужно
делать. А что? Вот L гладкость мы уже пользовали. Надо бы сильную выпуклость использовать. Давайте я
вот здесь делаю так. Я тоже добавлю умный ноль. Вот. Так. Умный ноль добавлен. И помните, у нас было
такое свойство. Я вам упоминал в дыхе того, что нам оно понадобится в доказательствах. Не доказывал.
Из определения следует. Вот. Что здесь это можно оценить как mu x k-x со звездой. У кого там открыты
слайды на компьютере, можете как раз отмотать на те свойства, где как раз была положительная
определенность десиана. Вот. И посмотреть, что в таком виде у вас там есть. Игры? Все норм. Шаг?
Ой, да-да-да. Все. Шаг появился. Супер, да? Все вроде норм. Сейчас улавливаете, что происходит. Все. Так.
Давайте это все сделаем красиво, на слайде соберем. Вот. Я здесь это все выписываю, конечно. Те
свойства, которые вам нужны. А условия оптимальности, которые мы с вами добавили. И соответственно
начинаю с этим всем играться. Вот они, выписанные вещи, которые мы только что с вами получили. И
дальше их просто группирую. Понятно, что у меня везде теперь x-каты, x-каты, x-каты. Минус x со
звездой. Все. В одну скобочку. Получили вот такую вот скобочку, как здесь. Что дальше? Что хочется?
Предположение индукция. Ну, а чего вам вообще от этой скобочки хочется? Зачем? Супер. Видно,
что мы по факту получили рекуррент в зависимости нового расстояния от предыдущего. Для любой
или гладкой сильно выпуклых функций. То есть, какую бы мы функцию не взяли, вот это будет верно,
что градиентный спуск вот такое гарантирует. Так? Верхняя оценка. Вот. Хочется приближаться к
решению. Вы согласны, чтобы расстояние уменьшалось? Поэтому вот мы говорим, что вот эта скобочка должна
быть строго меньше единицы. Согласны? Супер. Вот. Мы хотим соответственно меньше единицы. Подбираем.
Что будем делать? Как подобрать, чтобы она была меньше единицы? Вот. Как подобрать? Парабола такая.
Что это у нас вообще с точки зрения гамма? Квадратная неравенствия. Это парабола с ветвями.
Куда? Вверх. Вот. У нее есть где-то минимум. У нее есть где-то минимум. Вот. Как минимум в нуле это равно
единицы, и скорее всего есть минимум, который даст значение меньше единицы. Ну сколько у нее там
минимума? Какой точкой у нее минимума будет? Сейчас. Берем производную, получаем тут двоечка еще
вылезет, это уйдет, это уйдет. Получится mu делить на l в квадрат. Правильно посчитал производную? Я
уже старый, вы сейчас мне подскажете. Правильно, да? Скорее всего, да. Вот. Ответик. Соответственно,
да. mu делить на l в квадрате. Ну дальше я это все подставляю просто в шаг. И получаю, что вот
эта скобочка у меня отображается как 1 минус mu в квадрате l в квадрате. Согласны? Супер. То есть
получается все, мы уже начинаем выедать, мы начинаем приближаться, у нас расстояние до решения
меняется, причем уменьшается, гарантированно уменьшается. Вот. Записано это все? Безобразие. Что дальше?
Ну можно просто запустить рекурсию. Сказать, что я теперь знаю, что у меня x-каты там, ну зачем
уже предполагать индукция, по факту она все доказана. Вот. Ну да, это можно называть индукцией,
но я могу сейчас уже запускать рекурсию и вот здесь вот соответственно вытаскивать сначала,
например, x-ката минус 1 x звездой. Тут у меня соответственно степень 2 возникнет. И так далее.
Дальше-дальше-дальше-дальше запускаю. Вот. И дохожу до вот такого. Вот. За ка шагов я вот выеду
столько. А давайте вы мне расскажете. Я вчера как раз посмотрел, как вы написали тесты с точки
зрения про лекционные вопросы, какая это скорость сходимости? Линейная. То есть скорость геометрической
прогрессии, когда вы приближаетесь к решению, выедая показатель геометрической прогрессии. Это
линейная скорость сходимости. На графике я напоминаю, где у вас логарифмический масштаб по оси x,
ой, по оси y, вы как раз имеете линию, поэтому она и называется линейной. Вот. Это линейная скорость
сходимости. Супер. Вот. Дальше, соответственно, что? Дальше хочется получить из этого всего
безобразия число итерации. Сколько мне нужно сделать итерации, чтобы гарантированно дойти до
решения? Вот. Хочу я, например, получить какую-то точность epsilon. Ну вот. Хочу я гарантировать,
чтобы у меня xкат-x звездой было меньше epsilon. Меньше либо равно от точности epsilon. Вот. Сколько
мне нужно сделать итерации тогда? Да. То есть смотрите, в чем суть. Я говорю то, что... я хочу,
чтобы вот это гарантированно. Значит, я хочу, чтобы вот моя оценка, которую я получил, я подбирал k
исходя из того, что оценка будет обязательно меньше epsilon. Тогда k подберется всегда так,
что epsilon будет достигаться до точности epsilon. Вот. Отлично. Тут единственная проблема в том,
что... вот это логарифмировать сложно. Ну, какой-то показатель, какая-то степень непонятная. Я предлагаю
тут перейти к экспоненте. Вот. Ну, таким образом, что у вас 1-x меньше либо равно, чем экспонента
в степени минус x. Кто понимает, откуда это свойство берется? Для x, которыми там от нуля до единицы.
Это же просто разложение экспонента. У вас как там 1, дальше как раз первая степень минус x,
дальше уже будет квадрат. Вот здесь вот будет возникать 1-x. Дальше x квадрате пополам. Вот. А
здесь x квадрата уже нету пополам. Поэтому правой части нам больше. Вот. Простое свойство из Тейлора.
Вот. Такое. Техническое. Здесь все норм? Все. Тогда вот уже здесь прологарифмировать значительно
проще. Логарифмировать значительно проще. Можно взять натуральный логарифм. Я как раз требую вот
то, что мы с вами обговорили, степень меньше epsilon. Но я там, соответственно, писал без этих квадратов.
Здесь квадраты появились, я их теперь добавил. Вот. Прологарифмировал и получил, что у меня k должно
быть больше, чем l в квадрат, делить на mu в квадрат. Ну там можно этот логарифм. Вот. Такое число
итераций, которое мне необходимо до достижения. Первая наша оценка на метод, который мы с вами
получили. Окей? Супер. Вот. Но вообще проблема в том, что это не очень хорошая оценка. Вот. Я вас
повел не туда изначально. Просто чтобы показать, что в принципе вот эти верхние оценки, как нижние,
можно получать довольно грубо. Ну то есть сейчас я загрубил. Оценка нехорошая. Можно лучше,
мы сейчас сделаем лучше. Вот. Вот. Ну и получается, что так, что можно иногда реально грубо оценить,
получить какую-то скорость сходимости, расстроиться, что метод работает так на практике
значительно лучше, чем здесь. Вот. Ну соответственно с этим надо как-то бороться и делать более тонкий
анализ. Сейчас давайте попробуем этот тонкий анализ и сделать. Вот. Хорошо. Вернулись ровно туда,
откуда начали. Все, что я расписывал, вот здесь я просто раскрыл квадраты, как я напоминаю. И здесь
я в одном месте добавил f от x градиент в f от x звездой. Наш 0. Так. Супер. Давайте чуть-чуть
вспоминать другие свойства, которые были. Я же не зря для гладкости тоже свойства доказывал.
Вот. Специально потратил время на свойства, которые на нормы градиента. Отмотайте себе и
расскажите, как мне оценить вот эту. Вот здесь вот. В конспекте найдите, как вот это оценить. И я
даже его выписал. А, вот. Даже выписал. Ну ладно, давайте вы мне расскажете. Вот. Как вот его оценить.
Давайте, давайте. Не смотать. Не, так не интересно. Давайте вы, наоборот, верхов натайте. Вот. Мы ж
как раз договорились, что будете сейчас это не сами подсказывать, рассказывать, и мы будем в режиме
онлайн все выводить. Что с подсказками-то делать? Как оценить вот эту норму разности? Вот. Было
свойство, где у нас как раз была норма разности градиента для l-гладкой выпуклой функции.
Нет, вот определение мы пользовались и получилось плохо. А там были свойства еще. Были свойства,
как раз мы на них потратили время, где нормы градиентов есть. Что там? Рассказывайте. Много букв.
Я запишу, давайте. Как оценить норму разности градиентов? Так, ну что, не нашли, что ли?
Дальше нашли? Ну, продиктуйте уж дальше, как это будет. Ну, это скалярное произведение. Давайте
норму разности градиента. Я уж. Давайте сразу. Что там? Ну, найдите это свойство. Мы же ее там
доказывали. Где-то в районе как раз после l-гладкости, где я доказал сначала для произвольной
гладкой, а потом для выпуклой. Там в теореме было две строчки. 2l, f от x, kt, минус f от x со звездой.
Плюс что там еще? Плюс что-то есть? Все? Ну, давайте диктуйте плюс.
x кт и минус x со звездой? Вот так? Вроде норм, да? Вот. А почему не продиктовал человек,
который посмотрел по ответу? Вот за ответ написан. Ну, давайте кто-нибудь тогда без ответа знает,
почему вот только это останется? Ну, вот градиент же равен нулю, да? Поэтому вот этого всего нет.
Хорошо. Вот. Осталось еще поиграться вот с этим скалярным произведением. Ну, давайте,
что там уж? По выпуклости, сильной выпуклости будем оценивать. Вот. Что там будет? Два гамма к,
mu пополам, xk минус x со звездой. Я просто по определению пишу. Можете прямо проверять,
что это ровно то, что написано в определении. Плюс fxk минус f от x со звездой. Верно все?
Все. Первое. Для скалярного произведения расписали просто по определению, самому первому
базовому определению сильной выпуклости, а вот для норм градиента чуть-чуть поигрались с свойством.
Вот. Получили вот такое вот безобразие. Вот. Выписано оно у меня раз и выписано оно у меня два.
Дальше я группирую. То есть видно, что у меня есть... Zoom закончился что ли? Так, беда какая. Почти
дошли до конца. Интернет может закончился у меня, потому что надо было к Миктон Гэп подключиться,
а я к мобильному своему подключился. Вот. Сейчас посмотрим. Это видимо Zoom. Вот. Так. Это вот мы
с вами только что поняли, как получили. Теперь я просто сгруппировал xk минус x со звездой и f от x минус f от x со звездой.
Ничего сложного, просто манипуляции с алгеброй. Вот. А теперь к вам вопросик. Что хотим-то? Вроде бы
опять к чему-то хорошему идёт. Опять появилась вот этот хороший коэффициентик, который даёт вот
эту сходимость геометрическую. Что-то проблема есть одна. Где? Второе слагаемое, оно может быть как
положительным, так в принципе отрицательным. Вот. Хотелось бы, чтобы оно было отрицательным,
чтобы мы его как раз сверху просто нулём оценили. Как его сделать отрицательным? Гамма-катой подобрать,
причём здесь прямо очень видно, как его нужно подбирать. Как? Гамма-катой меньше, чем один
делительный. И тогда второе слагаемое будет отрицательным, ну, неположительным. И тогда мы
сверху его просто берём и убьём. Всё. Вот. Получили вот такую вот рекурренту. Опять очень похожа
на ту, что у нас была в принципе. Вот. Запускаем. Запускаем. Тут я прям пишу в виде гамма-иты. Но в
итоге подставляю гамма равна 1 делит на л, потому что это максимальный шаг, который мне разрешает
взять теорема. Так. Вот. И получаю вот такую вот рекурренту. Лучше или хуже, чем была? Не рекурренту,
уже результат. Я уже всё раскрыл. Вот. Лучше или хуже, чем была? Вот этот? Вот это, да? Смотрите,
вот эта скобочка, вот это больше либо равно 0, согласно, потому что f от x каты, а это оптимум. Вот.
Это больше либо равно 0. Скобку делаем меньше 0 и убиваем. Вот. Результатик. Лучше или хуже? Выглядит
лучше, потому что там были μ в квадрате или в квадрате. Да? Когда, соответственно, мы сейчас всё
пролагарифмируем. Вот. Здесь я уже всё пролагарифмировал. Ну, посмотрели уже, как это делается. Экспоненты
нужно сделать. Потом у вас получается там этот лагарифм. Вот. Всё. Теперь уже теорема о сходимости,
потому что мы её получили. Вот. Вот такая вот сходимость. И, соответственно, оценка на число
итерации можно записать в виде вот в таком виде. L на μ, лагарифм, или вот там L делить просто на μ.
Здесь я как раз сразу с вас знакомлю с той нотацией, которая принята в численных методах
оптимизации. Это о-нотация, как раз когда мы хотим убивать константы, численные, не вот эти,
параметрически, которые там от свойств функции зависит L, μ, точность решения епсилон. Вот.
А константы нам двоечку, потому что вот здесь вот лагарифм, мне так-то выскакивало двойка,
потому что был квадрат. Я здесь должен был вытащить двойку, она выскочила из лагарифма,
но я её убил о-нотации. Вот. А можно вообще убивать и все лагарифмы. Тоже часто принятая вещь.
Тут нужно добавить ещё тильдочку сверху на до. Если вы видите ось тильдой, то скорее всего убиваются
ещё не только какие-то константы, численные, не параметры функции и точность решения, а убиваются
ещё и лагарифм лишним. Вот такой вот результат L делить на μ. Итерацией нам нужно, чтобы достичь
точности епсилон. Хороший результат. Где, где, где максимум? Ось тильдой, а какой максимум?
Эпсилон. Смотрите, я ещё раз говорю, ось тильдой убирает лагарифмические факторы, в том числе те,
которые зависят от параметров. Потому что лагарифм часто... Сейчас мы увидим по оценкам, что
лагарифм это вещь хорошая. Вот. Часто ситуация хуже. Ось тильдой? Вот. То есть точное определение
в данном случае это просто реальная оценка минус вот эти все константы. То есть это скорее
не совсем так, как у вас в алгоритмах даже определяются. Ось тильдой у вас просто оценка сверху была.
Константы числены? Числены. Ну, значит это даже так же. Потому что, видите, здесь мы чиселки убиваем.
Ось тильдой, и вот вы убиваете. Там ещё получаются любые лагарифмические факторы, которые, соответственно,
ваша реальная оценка. Вот эта оценка умножит на какие-то лагарифмические факторы, любые
лагарифмические факторы. Вот. Как более формально это записать, надо подумать. Просто вот так принято,
что мы убиваем вот стильдой нотации все лагарифмы. Нет, смотрите, просто лагарифм от экспонента,
это вот он уже у вас как раскрывается, как что-то, что аргумент экспонента. Понятно, предполагается,
о чем мы зашевелились, я пока никуда не разрешал, мы еще разговариваем с человеком. Так. Вот. Ну,
понятно, предполагается, что мы убиваем так, что вот у нас уже там есть зависимость духи,
как вот здесь, вот один делить на е или в таком. Вот. Формально я подумал, как это определить,
потому что я справедливости ради в литературе по оптимизации тоже не видел, там всегда пишут,
ну мы убиваем лагарифм. Вот. Окей, я подумаю, потому что самому стало даже интересно,
как это формально определить. Но смысл понятен. Можно, в принципе, лагарифм и оставлять. Вот.
Так. Немного интуиции, доказательства, быстренько. Вот. Смотрите, что на самом деле происходит
в случае градиентного спуска. Когда мы, помните, подбираем шаг, мы подбираем шаг исходя из верхней
аппроксимации, чтобы не сделать довольно большой шаг, потому что, видите, верхняя
аппроксимация довольно резко растет. Вот. И как мы знаем, если функция резко растет,
сделаем большой шаг и улетим. Вот. Поэтому шаг мы подбираем исходя из аппроксимации верхней,
1 на L, чтобы не было вот этой резкости. Но в худшем случае она себя может вести как нижняя граница,
поэтому подбирается шаг как верхняя. Но из-за того, что шаг такой большой, в итоге он может быть
довольно маленький, потому что сам градиент может меняться довольно слабо, потому что как бы
физика может быть и как ни в нижней границе. В итоге вот у нас и получается, кто играет 1 на L,
что как бы шаг исходя из верхней границы, а сходимость в худшем случае исходя из нижней
границы, то есть скорость того, насколько там большой градиент и так далее. Вот. Вот это физика. То
есть вы выедаете как бы поверхней, но сходитесь довольно медленно, потому что mu может быть сильно
меньше, чем L. Вот. Так-так-так-так-так. Здесь я вот как раз это пишу вам. И здесь быстренько оценки
сходимости, чтобы просто понимать картину. Это все будут способии. Мы вывели в одном случае. Теперь
нужно понимать, что происходит в других. Вот это мы с вами вывели. И это оценка, как ни странно,
не улучшаемая. То есть градиент, испуск. Таки вот анализ у нас теперь оптимален. До этого мы получили L в
квадрате делить на mu в квадрате. Это плохо. Вот это уже не улучшаемая для градиентного
спуска. В выпуклом случае вот такая вот оценка. 1 делить на epsilon. Вот. Какая скорее всего эта
сходимость? Хуже это и лучше или лучше, чем у нас было? В сильно выпуклом случае. Хуже. Хуже. Это
сублинейная сходимость. Это сублинейная сходимость 1 делить на k. Вот. Она отсюда пришла. Как раз вы как раз 1
делить на k сказали, что это порядка epsilon, и у вас появилась эта epsilon в знаменателе. Поменяли k и
epsilon местами. Соответственно, вот если у вас так, то тогда у вас k порядка 1 делить на epsilon. Вот.
Сублинейная сходимость в выпуклом случае все хуже. Причем еще и сходимость у вас будет по
аргументу. По функции. То есть здесь вы по аргументу, потому что решение уникально. А там по функции,
потому что решение не уникально. У вас может быть их много, и вы просто сошлись к какому-то из
решений. Но по функции сошлись. По значению функции. Также есть оценки и в невыпуклом случае. Тут
вообще 1 делить на epsilon в квадрате еще хуже. Вот. Но в невыпуклый случай у вас сходимость к
стационарной точке не лучше. Что это за стационарная точка? Ну непонятно. Ближайшая ли она вообще была
к решению? Ну не к решению, а к тому место, где вы стартовали. Непонятно. Хорошая ли она с точки
зрения там глобального, локального минимума? Непонятно. Потому что в невыпуклом случае мы
не можем ничего доказать. Вот. В случае липшицевой функции, когда у вас не липшицев градиент, а липшицевая
функция, тоже вот есть оценки 1 и 2. Вот. Они здесь выписаны. Тоже сублинейной скорости сходимости.
Когда у нас невыпуклая липшицевая функция, мы с вами разбирали все ужасно. Первая лекция, там вот эти
примерчики оттуда. Вот. Смотрите. То есть вот такая ситуация. И что вообще, как дела еще у градиентного
спуска среди методов первого порядка? На самом деле довольно хорошо. То есть градиентный спуск вот
в этом случае. Ой, не вот в этом. Вот в этом случае, вот в этом и вот в этом. Здесь выписаны оценки
только для градиентного спуска, кроме первой лекции. В трех случаях из пяти он оптимален. То есть
среди методов, которые вызывают информацию, градиенты, градиенты. Вот. Лучше не придумать, чем
градиентный спуск. Вот. Вот в случае, соответственно, невыпуклой оптимизации, в случае м липшицевой оптимизации,
то есть для липшицевых функций. Но есть еще два случая, которые мы сегодня как раз разбирали.
Гладкий сильно выпуклый и гладкий выпуклый. И вот там градиентный спуск не оптимален. Его результаты
можно улучшать. Не анализ, но наш анализ оптимален. То есть для градиентного спуска результат уже
не улучшаем. Но есть другие методы, которые работают быстрее. И мы вот уже покажем, что какие-то
методы и насколько они работают быстрее, и почему они оптимальны на следующей лекции. Все. Заканчиваем.

Хорошо, тогда давайте начинать, в общем, мы с вами две лекции
посвятили MapReducer и как вы наверно убедились на семинарах
писать код на чистом MapReduce неудобно, даже для простой
задачки типа WordCount надо написать что, mapper, reducer,
два файла на Python, надо написать bash script, а если мы хотим
сделать что-нибудь дополнительное типа сортировки, то надо
еще два файла на Python и опять bash script, в общем, это все
долго и еще в середине двухтысячных сообществу разработчиков
стало понятно, что MapReduce задачи хорошо ложатся на
SQL, давайте проверим, как они хорошо ложатся, небольшое
задание для вас, попробуйте вот эти SQL конструкции
представить в виде MapReducer, как бы вы решали задачу,
например, селектова Reducer или фильтрации в виде VA,
давайте.
Хорошо, а что мы почему сортируем, то есть что является ключом?
То есть у нас получается два ключа, один или ноль,
один берем, ноль не берем, так, и получается, что все,
что мы берем, оно пойдет на один Reducer, потому что
ключ один, и тебе не кажется, что это как-то не очень
хорошо, потому что, ну, просто твое решение задачи
это всегда один Reducer, даже если у тебя 5 терабайт табличка
и как бы слишком много.
Можно как-то еще сделать, напомню, MapReduce это не обязательно
всегда Map и Reduce, может быть что-нибудь одно.
На семинарах еще не делал я никаких фильтраций.
Ну, в общем, что мы делаем на мапе вообще, вот на каком-то
рандомном мапе?
К нам приходят строки, и мы их опроватываем, и мы
принимаем решение вообще печатать что-то на выход
или нет, и поэтому на мапе можно вполне себе сделать
фильтрацию.
Потому что уже на мапе можно какие-то данные выкинуть,
и если, например, это таблица, то у нас приходят строки,
и мы эти строки обрабатываем, выбрасываем лишние поля.
То есть уже на мапе весь, всю логику селекта можно
сделать.
Чего?
Почему условный файлик, да, но, а почему файлик не
в HDFS?
Да, ну, то есть еще раз, если у нас в мап-редьюсе нет редьюсера,
то то, что нам нагенерил маппер, это является результатом,
и оно переносится в HDFS, ходуб его сам переносит.
Поэтому селекта можно реализовать на обычном мапе, УЭА тоже
можно реализовать на обычном мапе, просто мы сортируем,
точнее мы фильтруем не по вертикали, а по горизонтали,
то есть мы принимаем решение, какие строки оставлять,
какие не оставлять.
Вот, что касается OrderBy, ну, здесь понятно, нужна
сортировка, нужен редьюсер, что касается GRUBY, тут тоже
нужен редьюсер и нужна сортировка, джойн мы с вами
разбирали, и вы, наверное, помните, что там, бывает
конечно мап-джойн, когда редьюсер не надо, но в большинстве
случаев, если брать общий случай джойна, редьюсер
там надо, то скажите про хевинг.
Хевинг это, по сути, УЭА, но поверх агрегации, то есть
когда ты пишешь там селект, сумма чего-нибудь, фром
чего-нибудь, и потом ты хочешь отфильтровать уже по
этой сумме, не по полю, а по агрегату, вот тогда
нужен хевинг.
Ну если ты делаешь какую-то агрегацию, то нужно редьюсер.
Чего?
Можно цепочку, а точнее мап-редьюс полноценный
плюс маппер и маппер, да, а можно прямо на редьюсе
это делать, то есть когда ты посчитал агрегат, ты посчитал
сумму какую-нибудь допустим, и дальше у тебя написано
хевинг сумма больше пяти, и прямо на редьюсере можно
вот по этому агрегату пофильтровать, и в больше пяти, значит выводим,
иначе не выводим.
Вот получается вот такая штука, то есть действительно
на селект ве это мап, остальное это мап-редьюс, ну и бывает,
что мап-редьюс еще с чем-нибудь.
И получается, что действительно задачи мап-редьюса они
хорошо ложатся на SQL, и если сделать SQL фреймворк, который
будет это все реализовывать, нам будет намного проще,
потому что, как вам наверное сказали на семинарах, для
того чтобы писать на ходу, надо обладать большим количеством
разных компетенций, то есть нужно знать сам мап-редьюс,
нужно знать многопоточку, нужно знать джаву, питон,
и хочется делать то же самое, но на SQL, когда вот кроме
SQL по сути ничего знать не надо.
И действительно в 2002 году появился фреймворк, не единственный,
их появилось сразу несколько, вот если успеем, мы с вами
рассмотрим совсем обзорно еще пару штук.
Ну вот, самый известный это был Hive, Hive это SQL обертка
поверх мап-редьюса, вот не нужно путать с UBD, у вас
во всех был курс по базам данных, так, в прошлом году.
Хорошо, напомните определение база данных, вот с UBD, что
это вообще такое, это же не просто SQL.
Да, как именно управлять, то есть какие действия должно
делать это приложение, чтобы мы сказали, оно с UBD, вот просто
какая-то программка, которая ходит в базу, дергает по
селекту что-то, это же не с UBD, хотя она управляет базой.
Чего?
Свойства сохраняются, хотя это не обязательно, а что еще,
то есть мы должны предоставлять все возможности для хранения
и для обработки, и для оптимизации еще к тому же.
Если мы посмотрим на Hive, то он ничего для хранения
и для обработки нам не предоставляет, как мы увидим позже, Hive просто
транслирует SQL в набор задач на Hadoop, и данные как хранились
в HDFS, так и хранятся в HDFS, обрабатывали мы, как в Hadoop
писали код, только мы сами писали кучу кода, а теперь
он генерится, и все, поэтому Hive это не с UBD, это просто обертка.
Но с приходом этой обертки появляется не только удобство
в плане того, что две строчки SQL у тебя все готово, а
появляется еще и проблема, которая обозначена на этом
слайде, это схема, то есть когда у нас был чистый Hadoop, мы
могли в этот Hadoop запихать что угодно, все равно мы сами
код пишем, и мы сами с этим как-то разберемся, а теперь
у нас появляется схема, и нужно думать, как нам эту схему,
ну то есть как с этой схемой вообще работать.
Что такое схема on read, схема on write, на курсе UBD объясняли?
Ну вот если мы берем обычную базу данных, Oracle, PostgreU, там
SQL, неважно, когда мы пишем данные, то сразу на этапе
инсерта обычно проверяется, все ли у нас правильно записалось
записалось и все ли соответствует нашей табличке, то есть если в
табличке 5 полей, там int, а мы сделали insert 10 строковых
полей, то наверное не сработает и будет ошибка, это конечно
хорошо, но что делать, если данных много, и например мы
пишем в эту нашу хайвовскую базу данных какие-то логи, мы
их пишем, а читать мы может их вообще не будем, мы может
будем читать самый последний какой-то кусочек head этих логов,
то есть большая часть строк нам может быть не понадобится,
тогда какой смысл делать вот эту проверку при каждом
инсерте? При каждом инсерте мы проверку не делаем,
то есть схема on write это когда проверка делается при записи,
при инсертах, а схема on read это когда проверка делается при
чтении, и вы тоже посмотрите на это на семинарах, это очень
необычно, когда вы создали табличку, вот сделали create
table, подали в эту табличку какую-нибудь большую папку,
там гигов 100 размером, и у вас эта команда выполнилась
за десятую часть секунды, все очень быстро, в ответе
OK, все хорошо, и кажется табличка готова, мы можем с ней
работать, потом мы начинаем данные читать, делаем первый
селект, и у нас этот селект валится с ошибкой, вот это
и есть схема on read, вот так работает Hive, давайте теперь
посмотрим как он в плане архитектуры работает, тут большая сложная
схема, но ее можно разбить на две части, и вот правая
часть это знакомый вам ходуб, левая часть это то, что нам
приносит Hive, что нам приносит Hive? Он нам приносит UI,
UI может быть разным, на этом курсе мы будем работать
с UI Apache Huey, и мы будем работать с Hive Shell, еще бывает
Apache Zeppelin, еще бывают разные интерфейсы, вообще можно
Hive даже к Юпитеру прицепить, в общем у нас есть интерфейс,
в этом интерфейсе мы пишем запросы, и у нас работает
драйвер, драйвер логично он все это как бы обрабатывает
и передает дальше, дальше у нас идет компиляция, то
есть здесь мы проверяем синтаксис, мы превращаем запрос,
то есть мы превращаем SQL текст в план запроса, мы сегодня
тоже посмотрим, что это за планы запроса такие, и этот
план запроса дальше можно уже выполнить, и мы передаем
этот план запроса на Execution Engine, и Execution Engine уже разбирается
как этот запрос будет выполнять, здесь возможны варианты,
смотрите какие возможны варианты, чтобы понять какие
возможны варианты давайте вспомним язык SQL, все запросы
которые мы пишем на языке SQL, на какие типы их можно
разбить, ну там вот эти типы они обычно тремя буквами
называются, DDL, да, что такое DDL, это Data Definition Language,
то есть что это, это Create Table, например, там Create Database,
дальше Manipulation, что делает Manipulation, ну да, что-то с этими
данными делает, там Update и Insert, еще их 5 если я не ошибаюсь,
хотя в литературе иногда то 4, то 5 пишут, Select как его
называют, Data Query Language, ну то есть DDL это definition это мы
определяем какая у нас будет схема, Manipulation это мы как-то
с ней взаимодействуем, изменяем, а Select он ничего не изменяет,
и это отдельно Data Query Language, еще два осталось, Transaction, да
DCL, то есть это работа с транзакциями, это всякие там
Commit, Rollback и еще, да, ну там смотря какое удаление,
то есть есть удаление всей схемы данных, это DDL, а есть
удаление просто данных, это DML, то есть Drop Table это будет
DDL, а Delete по условию это будет DML, вот, какой пятый
остался класс, он не сильно относится к Hive, в Hive он
представлен достаточно бедно, Data Control Language, то есть работа
с правами, вот, и что мы тут видим, у нас есть DDL, DDL здесь
отдельная стрелочка, потому что когда мы пишем какой-нибудь
DDL запрос, он не взаимодействует с самими данными, он взаимодействует
со схемой данных, схема у нас хранится в MetaStore, MetaStore
это вот тут написано, это специальная база данных, и она сохраняет
метаинформацию про таблицу, какие поля, какие типы, кто создал,
какие файлы, вот все, что мы знаем про базу данных, это
MetaStore, и DDL он взаимодействует с MetaStore, дальше
здесь Hive, вот он умеет взаимодействовать с Hadoop'ом
тремя способами, большая часть запросов, которые мы будем
делать, это запросы, которые порождают джобы, то есть мы написали
SQL, она превратилась в десяток цепочку MapReduce.Job, но бывает такое,
что мы данные откуда-то перемещаем, например, из одной папки
в другую, и делается это без MapReduce, и тогда нам MapReduce
не нужен, мы ходим напрямую в HDFS, и еще бывает отдельный тип
запросов, это селекты без агрегации, то есть просто селект
набор полей from таблица, без агрегации, без фильтрации,
без ничего, чистый селект, Hive его делает без применения
MapReduce, тоже отдельная стрелочка называется fetchResult.
А теперь давайте посмотрим, как происходит преобразование
SQL текста в MapReduce задачи, оно происходит в 4 шага,
первый шаг это парсер, то есть просто проверка синтакса,
за что мы написали действительно SQL, дальше идет
семантический анализ, то есть проверка типов данных,
например, если мы сравниваем в разделе where 2 поля,
одно из них int, другое из них big integer, то какой будет
в итоге тип. Да, big integer, то есть мы просто приведем
они явно к большему типу. Ну и тут вообще мы проверяем
типы данных, чтобы мы не сравнивали число со строкой,
допустим. Раскрытие звездочки, вот с раскрытия звездочки
тут я остановлюсь отдельно, только мне нужен маркер.
Так.
Вот как вы думаете, какой из вот таких запросов будет
работать быстрее? Ну давайте посмотрим, то есть звездочка,
вот как у меня написано на слайде, происходит раскрытие
звездочки. Что такое звездочка? Это список всех полей,
то есть у нас звездочка превращается в field1, field2,
field3 и так далее. Здесь просто field1. Вот, если мы имеем дело
с текстовым форматом данных, когда как только мы читаем
какую-то запись, мы читаем всю строку, то здесь особой
разницы нет. Но в хайве, ну не только в хайве, есть такое
понятие, как колончные форматы хранения. Это всякие там,
может быть слышали вот такие форматы. Вот, часто применяются
такие форматы в работе с хайвом и со спарком, и так как
у нас обычно колонок много, то здесь имеет значение,
сколько колонок мы прочитаем. Мы можем прочитать не всю
строку, а набор каких-то колонок. Поэтому вот это, это
сразу будет работать очень долго, потому что нам приходится
читать все колонки. Дальше вот это. Здесь мы читаем одну
колонку. А что мы делаем с единичкой? Единичка это не
номер колонки. Единичка это просто константа 1. То есть
по сути в этом случае мы не читаем ничего, мы просто
создаем еще одно поле эффективное, которое заполнено
единичками. То есть самый лучший вариант это вот это.
Потому что мы тут не читаем даже первое поле. Мы просто
увидели, что строка есть, добавили единичку. Увидели,
что строка есть, еще добавили единичку. Вот и следующий
этап после семантического анализа это построение
логического плана запроса. То есть здесь уже SQL окончательно
превращается в набор джабов, в набор мапперов и редьюсеров.
И последняя стадия это физический план запроса. То есть мы
учитываем уже, как именно, на каких нодах хранятся
данные. И в этом случае Hive при выполнении он будет знать
откуда тот или иной кусочек данных читать, на какой машинке
он есть, откуда его ближе достать и так далее.
Вот давайте посмотрим на вот этот план запроса и
попробуем по нему что-нибудь понять. Вот если здесь нормально
видно, что вы можете сейчас сказать по вот этому вот
набору такого текста, можно ли как-то восстановить запрос,
понять, что там происходило.
Ну что мы видим? Мы видим tablescan, причем два раза. То есть
во-первых, мы видим mapReduce. Значит, у нас есть джаба,
как минимум одна. Дальше мы видим mapOperator3, то есть
вот это маппер, вот это редьюсер. На маппере у нас
происходит tablescan, то есть, видимо, тут какой-то селед
был по табличке books. Дальше filterOperator, значит, у нас
тут было где-то where и было polyTimeStamp, которое мы вот
так вот сравнивали. Второй tablescan, табличка users, и здесь
фильтра никакого не было. Вот ReduceOperator, здесь у нас
join. То есть, получается, мы поджойнили две таблички,
причем одна, видимо, была в подзапросе, и мы там что-то
пофильтровали. И поверх всего этого следующий stage это
conditionalOperator, то есть вот этот весь наш join, он обернут
еще в один запрос, в котором есть where.
Ну и давайте посмотрим, какие таблички вообще у нас
бывают. Что касается времени жизни,
таблицы бывают постоянные и временные. Здесь, я думаю,
все понятно. Что касается влияния на данные, у нас
бывают два типа таблиц, и мы с вами на курсе будем
работать в основном с таблицами external. Они вообще более
популярны в том случае, если есть какие-нибудь ценные
данные. Вот представьте, есть какая-нибудь жутко дорогая
установка физическая, ну или какая-нибудь еще. Ее
запустили, провели эксперимент, сохранили данные, и дальше
разные команды аналитиков данных начинают с ними
работать, с этими файлами. Может быть такое, что у
разных команд им нужны разные данные, и у них будут
разные таблицы. Но при этом источник данных, он будет
общий для всех. И нам надо сделать так, чтобы никто
ни в коем случае не сломал вот этот источник данных,
чтобы он там не перезаписал ничего случайно, не удавил,
не дропнул. Поэтому в таком случае мы делаем таблицу
external, это значит, что мы берем источник данных,
создаем свою схему, и эта схема накладывается как
трафарет на этот источник данных. То есть мы читаем
сам источник, но проводим его через эту схему, если
мы что-то хотим дропнуть, удалить, изменить, то у нас
происходит следующее, у нас или меняется сама схема,
или используется такая штука, как Data Warehouse.
То есть когда мы создаем вот такую external таблицу,
у нее есть Data Warehouse, и если мы хотим данные перегруппировать,
добавить, что-то поменять в них, исходник мы не меняем
ни в коем случае. Все, что мы доделываем, все добавляется
вот сюда, и мы потом работаем с исходником плюс DWH,
ну или просто с DWH работаем.
И даже если мы такую external таблицу полностью дропнем,
у нас дропнется только схема данных, а сами данные
у нас останутся.
Вот, теперь давайте посмотрим в сторону вот такой задачки.
И пока мы не знаем еще хайва, не умеем на нем писать,
буквально через 5 минут мы начнем смотреть на код,
и будем вместе с вами, у кого есть ноутбуки,
то можете сразу подключиться и что-нибудь пробовать написать.
Но пока мы до этого не дошли, давайте попробуем
на MapReduce вот эту задачу решить.
Вот, посмотрите на условия, посмотрите на данные,
и ответьте на первый вопрос.
Сколько будет MapReduce Job в решении такой задачи?
Пока вот сюда мы не смотрим 3 пункта, создать базу данных
и так далее, это мы с вами все сейчас пройдем.
Пока мы просто на MapReduce.
И потом мы сравним, как это делается на MapReduce и как
это делается на хайве за 5 строчек.
Ну и что у нас будет на первом маппере?
Результат Map задачи будет какой?
Так, хорошо, нам нужно среднее количество адресов
по подсетям, но сами адреса нам не нужны,
поэтому мы колонку адресов можем превратить в единички.
И тут будет Subnet, единичка.
Так, а здесь что будет?
Subnet и Summa.
Так, ну вы сказали, что здесь одна джоба, но вроде как
у нас одна джоба кончилась на этом.
Чего?
А как мы будем это среднее вычислять?
То есть получается, нужна все-таки еще одна джоба,
и нам надо подумать, что будет здесь.
Все, а все это что значит?
То есть ты имеешь в виду взять все эти Subnet
и сконкатенировать их между собой или что?
Таких может много быть очень.
Никак не меняем, хорошо.
Дальше что будет тут?
Это какое-то одно число, то есть в среднем
сколько у нас адресов в среднем по всем нашим подсетям.
Тогда с вот этим мы что-то делаем, потому что
количество подсетей мы теперь в рамках редьюса не посчитаем.
У нас просто разные подсети пойдут на разные редьюсеры.
А еще раз, а как мы нашли числитель, где он у нас?
N это же не числитель, это только одно число,
а числитель это общая сумма.
То есть нам нужно посчитать общую сумму, поделить на count.
О, можем конечно.
Тут может один быть, тут может быть просто какая-то
константа или null, то есть какой-то один ключ.
А дальше у нас получается кортеж, и тут будет N и 1.
А здесь будет потом K.
Все, вот так мы и посчитаем.
И вот это будет у нас ответ, одно число.
Ну и теперь давайте подумаем, сколько нам всякого кода
нужно тут написать.
Это у нас будет 4 питон скрипта и 2 bash.
Вот, а теперь мы с вами попробуем это же самое,
точно эту же задачку решить на SQL, на хайве.
Так, сейчас мне нужно зайти на наш Hadoop cluster.
Я захожу на другой, чтобы не грузить ресурсы, но он
в большинстве похож на тот, который МИПТ.
Вот, и так для начала давайте поймем, как вообще нам
работать с хайвом.
С хайвом можно работать несколькими способами.
Первый, это просто запустили хайв.
Да, вот, не обращайте внимания на этот permission-deny,
то у вас на кластере их не будет.
Так виден код или увеличите.
Вот, и у нас есть интерактивный интерфейс хайв.
И, например, мы можем посмотреть, какие базы данных у нас
у нас тут есть.
Show databases.
Вот список баз данных.
Ну и, в общем, можно в этом хайв-шеле выполнять разные SQL-команды.
Можно прямо тут же обращаться к файловой системе
через восхитительный знак.
Вот, например, так. Ну, как в Юпитер ноутбуке.
Можно обращаться к файловой системе HDFS.
Причем не надо писать HDFS в начале.
Да, не забываем ставить точку с запятой в конце.
Вот, в общем, такой хайв-шел.
Это первый вариант, как можно работать с хайвом.
Второй вариант. Это можно выполнять однострочники.
Например, хайв-database-example-e.
Да, minus database. Это мы подключились, приконнектились к базе.
И теперь мы выполняем команду show tables.
Вот, ну мы видим, что у нас есть табличка subnets.
Вот, ну а если мы хотим выполнить какой-нибудь большой файл,
потому что в хайв-шеле писать большой SQL трудно.
Потом через однострочник тоже большое что-то не напишешь.
У нас остается третий способ. Это хайв-f.
Вот, например, давайте посмотрим на вот этот файл.
Так, не топ, наверное, файл.
Вот, мы подключились к базе данных.
Посмотрели какие таблицы есть.
Описали таблицу describe и выполнили простой селект.
Вот такой вот целый набор запросов. Давайте его выполним.
Выполняется все просто.
Hive-f. Test-hive.
Вот, мы все видим. У нас сначала подключились к базе данных.
Получили OK.
Потом вывели список таблиц.
Потом описали таблицу.
Вот у нас два поля IP-маска.
И вот наш селект. Все как на слайде.
И есть еще четвертый способ.
Для этого мне нужно пробросить порт.
Вот, есть вот эта вот система. Ее называют Apache Huey.
Вот это вот ругательное слово, оно расшифровывается как Hadoop User Experience.
То есть здесь не только вещи, касающиеся хайва, здесь, в принципе, такая оболочка,
которая делает работу с Hadoop более красивой и приятной.
И, например, здесь можно тоже работать с какой-нибудь базой данных.
Давайте найдем ее Example.
Где у нас Example? Вот у нас Example.
Можно открыть табличку, можно посмотреть, как она устроена.
Ну и вот с автоподстановкой в более таком приятном интерфейсе можно выполнять какой-нибудь код.
Вот такая вот штука.
Теперь мы немножко поняли, как работать с оболочкой хайва.
И давайте вернемся к нашему запросу.
Как решать задачу? Мы поняли.
Но теперь, чтобы ее все-таки решить на хайве, нам надо немного подготовиться.
Нам надо создать базу данных, создать таблицу и уже на ней писать запрос.
Давайте создадим базу данных.
В принципе, вы можете делать все то же самое вместе со мной.
Я думаю, это будет полезно.
Создаем базу данных. Вот такой код.
Что мы делаем? Вот название нашей базы, а вот location.
Тест DVH. DVH это тот самый Date Warehouse, куда будет складываться все то, что должно попасть в нашу таблицу, а таблица external.
То есть все то, что мы должны изменить.
Поэтому давайте создадим такую базу данных.
Сейчас я еще раз зайду в папку.
Сначала мы базу данных удаляем, если она была, и создаем новую.
Пока она создается, идем дальше.
А дальше мы создаем таблицу.
Первое мы подключаемся к базе данных, пишем use.
Потом удаляем таблицу, если она есть, и дальше ее создаем.
Вот create external table.
Здесь, как в любом create table, прописываем типы полей, названия полей.
И потом вот raw format delimited.
Что вот это значит?
Ну, это можно прописать.
Обычным человеческим языком.
И понятно, что поля у нас разделены табами.
С точки зрения хайва здесь используется такая вещь, как Simple Serde.
Serde это сериализация, диссериализация.
В хайве есть много библиотек, которые называются Serde.
И они не имеют никакого значения.
В хайве есть много библиотек, которые называются Serde.
И они отвечают за чтение данных из Hadoop, и за запись данных в Hadoop.
Потому что в Hadoop может быть один формат хранения.
Там могут быть какие-нибудь строки.
И вот часто встречающийся кейс, если мы в Hadoop, например, имеем джейсоны.
Как хранить джейсоны в базе данных?
Что собой представляет джейсон?
С точки зрения таблицы SQL.
Обычно это какой-нибудь вложенный набор массивов и словарей.
То есть словарь внутри массив, внутри еще словарь.
Если распаковать, получится джейсон.
И вот в хайве есть разные библиотеки, которые, например, могут превратить джейсон в такую таблицу.
Или другой кейс, который вы будете разбирать на семинарах.
Когда у нас в Hadoop хранится какая-то помойка текста, совершенно не распаршенная, с кучей какого-то мусора.
И вы к этому всему применяете Regex Serde.
То есть пишете специальную регулярку, запихиваете ее в Hive.
И Hive, на лету при создании таблицы, вычленяет колонки из этого мусорника, и у вас создается табличка.
Но это такие более сложные кейсы, вы будете их разбирать на семинаре.
Здесь у нас самый простой кейс, когда у нас просто идет разделение по табам.
Дальше последние две строчки, это store test.xfile, то есть как хранится, ну и где хранится путь к данным.
Давайте снова выполним, мы видим, что база данных у нас создалась.
Теперь табличка, табличка у нас вот такая.
У кого-нибудь есть возможность за мной повторять или пока вы этого не делаете?
А что не получается?
Что, порт пробросить?
Порт пробросить или что?
Ну дело в том, что да, у нас на кластере не очень новый клиент Hive.
То есть в Hive есть сейчас два клиента, есть Hive и есть Beeline.
И чем они отличаются?
Hive более толстый клиент, он при запуске стартует виртуальную машину Java,
подключается к Hive сервер, а Beeline в этом смысле проще.
Для него Java не нужна, и он запускается быстрее.
Можно mcedit, можно vim, можно touch, ну в общем, как угодно в Linux.
Ну а теперь, когда мы уже создали файлы, когда у нас есть табличка база данных и колонки,
кто-нибудь может написать SQL-код, который получится.
Вы все проходили базы данных в прошлом семестре.
Наверное, написали коды посложнее. Что?
Ну хотя бы те, кто проходил.
Тут много людей, которые кивали, когда я спросил, были ли базы данных у вас.
Ну можете просто рассказать, что у нас будет, какие будут запросы.
Чего?
Таблица есть, которая называется subnets.
Вот, а задачка у нас вот такая.
В этой таблице лежит два поля. Вот, собственно, сэмпл таблички, он прямо на слайде есть.
Ну что?
Так, губай по маске, дальше sumo по маске, и потом усоединяем.
Ну вот, например, вот так.
Правда, нам еще с вами не нужны пока что три поля.
Вот так.
И вот.
Вот, вот так.
Вот так.
Вот, и вот так.
Ну, вот.
Ну вот, и вот так.
Вот так.
Ну вот.
А, вот так.
Ну вот.
Правда, нам еще с вами не нужны пока что тейбл-сэмплы.
Вот, то есть можно сумму, можно каунд, то есть сколько
у нас вообще айпишников в каждой подсети и потом
усредняем.
Заметьте, что все настройки, которые мы делали для
ходупа, то есть название джабы, количество редьюсеров,
все, что мы указывали в ходупе, все можно указывать
и здесь, то есть вот через сет, сет и дальше конфиг.
Давайте теперь посмотрим на план запроса, который
у нас получился.
Чтобы посмотреть план запроса, мы просто перед запросом
вставим ключевое слово explain и дальше, как обычно,
запускаем код.
Тогда у нас вместо выполнения запроса мы изгенерим планы
и на этом остановимся.
Вот, что мы видим?
Во-первых, мы видим, что у нас как бы две джабы было,
мы с вами тут обсуждали на доске, что две джабы,
а стейджа почему-то три.
На самом деле три стейджа это как раз и значит две
джабы, потому что последний стейдж, вот этот, он отвечает
просто за вывод.
Это вот та самая стрелочка fetch, которая в архитектуре
хайва шла вот сюда вниз, fetch results, она вот отдельно
и здесь никакого map reducer нету.
Она отвечает за то, что мы, например, поставим лимит
вывода какой-нибудь, в общем, вот это последний стейдж.
Перед этим идет у нас вот такой стейдж map reduce, что
мы здесь видим?
У нас есть group by, у нас есть aggregation count 1 и вот мы
на стадии reducer видим, что у нас работает count.
И здесь, кстати, видим lazy binaries rd, то есть как происходит
обмен данными между хайвом и ходупом, потому что раз
у нас разные стейджи, значит это разные джобы.
Между джобами, где сохраняются промежуточные данные в
и потом опять из ходу почитаются.
Вот, ну а здесь у нас на этапе reducer происходит average.
Так.
Вот, теперь давайте этот код наконец-то выполним
и посмотрим, что он нам выдаст.
Сейчас мы впервые увидим, как выполняется код на хайве
и при этом получается map reduce задача.
Вот видите, total jobs равно два.
Number of reduced tasks, вот мы установили через config, что это 10, вот
у нас 10.
А кто может сказать, во второй джобе сколько будет
reducer?
Вот та джоба, которая средние берет.
Один, потому что один ключ.
Здесь ключей много, в первой джобе нам пришлось ограничить
десяткой.
Как вы думаете, какая джоба будет работать быстрее,
первая или вторая?
Почему?
Да, меньше данных.
7 гигов, ну кластер поменьше, чем тот, на котором вы домашки
делаете.
Это такой резервный кластер.
Ну и скорее всего, тут кто-нибудь что-нибудь сейчас тоже считает.
Вот, первая джоба отработала.
И начинается вторая джоба, и тут написано number of reduced
tasks determined at compile time 1, то есть мы уже тут ничего
не устанавливали, а мы просто, у нас один ключ.
Ну все, у нас все посчиталось.
Запомним вот это время, total map reduced time spent, мы будем
оптимизировать код, или мы с вами сейчас, или вы
на семинарах.
В общем, это такое максимальное время, оно будет меньше.
Ну и сейчас 8 с половиной минут.
Вот такой вот код.
Так.
А теперь давайте подумаем, как можно оптимизировать
этот код.
Да.
Само выполнение на map reduced оно было бы примерно
таким же, потому что это все равно код выполняется
на ходупе.
Возможно, мы немного сэкономили бы на преобразовании, на
генерации этого map reduced.
Да, не сильно отличался.
А сейчас мы будем с вами придумывать оптимизации,
которые помогут нашему коду сильно отличаться в
лучшую сторону.
Давайте представим такой кейс, что у нас есть таблица
со всеми IT-специалистами России, допустим, или даже
не России, а мира, и мы знаем про этих IT-специалистов
Некоторый набор данных, например, country, city,
ну допустим, 4 поля нам хватит, и нам нужно по этой таблице
проводить регулярную аналитику.
Например, искать среднюю зарплату специалистов
в России, сравнивать ее с другими странами, считать
среднюю зарплату по странам, максимальную там за какой-нибудь
период, если мы тут дату добавим.
В общем, мы постоянно взаимодействуем с вот этим полем в сканторе,
и получается, что нам нужно, даже если нам нужна одна
страна, нам нужно делать full table scan по всей таблице.
Чтобы такого не было, чтобы это как-то оптимизировать,
используется такая вещь, как партиционирование.
Потому что и в случае, когда мы находим среднее, нам
нужно отдельно друг друга страны обсчитывать, и когда
мы находим какую-то одну страну, нам нужно тоже ее
искать по всей таблице, и нам бы хорошо иметь заранее
данные разложенные, чтобы мы каждую страну хранили
где-то отдельно.
Этот механизм и называется партиционирование.
Как он работает?
Берется вот эта таблица, и у нас готовятся данные
заранее, по каждому ключу создается отдельная папка,
и в эту папку складываются данные для этого ключа.
То есть вот у нас отдельная будет папка для России,
отдельная папка будет там для Америки, ну и так далее.
И в эту папку у нас попадут три поля конкретно для этого
ключа.
И тогда, если мы будем искать, например, делать аналитику
по одной стороне, мы сначала найдем нужную страну, нужную
папку, и внутри этой папки будем уже ходить.
Так, сейчас мы с вами посмотрим тоже, как это делается в коде.
Таблица будет новая, я говорю это о том, с точки зрения
синтаксиса и как данные хранятся.
В одной папке одна таблица, но в этой папке появится
подпапки.
Сейчас мы это увидим физически, как это все происходит.
Вот с точки зрения синтаксиса мы создаем новую таблицу
и пишем сюда одно поле.
Мы сейчас вернемся от этой предметной области обратно
к нашим подсетям.
Мы создаем таблицу, в ней одно поле, а второе поле
у нас идет сюда, partition by.
Почему его нету даже в create table, мы тоже сейчас поймем,
когда у нас код это работает и когда мы посмотрим живьем
на папке, которая получится.
Мы создаем таблицу, переносим данные из старой таблицы
в новую таблицу.
Осталось понять вот это, первая строчка, dynamic partition
mode non-strict.
Что это значит, на самом деле в хайве может быть
партиционирование строгое и нестрогое, по умолчанию
оно строгое.
Строгое это значит, что мы заранее знаем сколько
будет у нас папок, сколько будет ключей.
То есть мы заранее знаем сколько у нас стран и готовим
эти папки.
Но это не всегда удобно, иногда мы просто не знаем
сколько их будет.
Вот, например, в случае с подсетями мы понятия не
имеем заранее, сколько у нас подсетей в этой табличке
есть.
Поэтому нам хочется иметь возможность создавать
папки на лету, пришла новая подсеть, мы создали новую
папку и туда складируем данные для этой подсети.
Ещё одна подсеть новая пришла, опять новую папку
создаём и так далее.
Давайте этот код выполним, посмотрим как создаются
папки, куда переносятся данные и у вас наверно сразу
возник вопрос, таблица видик стёрнул, о каком вообще
переносе данных, рассортировке по каким-то папкам может
идти речь, она же не изменяемая.
А как маски приведут к тому, что у нас папки появятся
разные и в них данные появятся?
Хорошо, но когда мы создаём таблицу партиционированную,
мы не знаем, нам понадобится Россия или Польша или ещё
какая-то страна.
Как оно отражается в твоей частине, как оно формируется
в таблицах или в метаинформациях?
Метаинформация у нас хранится в базе данных, как я в начале
сказал, у нас есть база, там есть специальные таблички
и в этих табличках хранятся данные про то, какие поля
в таблице, кто создатель, какие права доступа, какие
данные, где эти данные лежат, вся информация.
Да, они-то не трогаются, но здесь как раз тот случай,
когда нам их нужно потрогать, потому что нам нужно, чтобы
у нас появилась другая структура папок.
Как раз, чтобы не делать full table scan при поиске нужной
стороны, допустим, надо, чтобы по странам мы их разложили,
а данные изменить мы не можем, поэтому что нам приходит
на помощь?
Нам приходит на помощь Data Warehouse, вот этот самый
location, который вот тут, где у нас создание базы данных,
вот этот location, который мы создали при создании
BD, вот это у нас Data Warehouse и сейчас нам придется скопировать
данные вот сюда, они вот тут будут разложены по папкам
и мы потом будем при работе с этой таблицей обращаться
уже к Data Warehouse, в данном случае, да, бывают разные
кейсы, бывает, когда мы делаем инсерты и в Data Warehouse помещаются
только инсерты, а здесь нам надо пересортировать
все, поэтому придется копировать тоже все.
Пока работает наш код, ну, заметьте, тут редьюсера
у нас не будет, а будут только мапперы.
Вот, наш Data Warehouse, давайте посмотрим.
А что именно мы запоминаем?
Так, во-первых, маска будет, да, весить много, во-вторых,
тебе придется обойти всю таблицу, ну, то есть, у нас,
так нам нужна одна страна Россия, а так у нас там 289
стран, тебе придется в 289 раз больше данных каждый
раз читать и тут как бы неизвестно, что лучше, раз скопировать
или на каждом запросе читать в 200 раз больше данных.
Вот, так, где у нас Data Warehouse?
А, тут немного у меня неправильное название, в названии базы данных.
Вот, в нашем Data Warehouse появилась табличка Subnet Part.
В этой таблице появились папки.
У каждой папки, видите, название маска равно чему-то.
Давайте посмотрим на эту папку, на любую.
Вот, эта папка соответствует партиции.
В этой папке у нас есть файлы, давайте откроем
какой-нибудь файл на выбор.
Что мы видим?
Мы видим одну колонку, но у нас всего было две, осталась одна.
Именно поэтому, вот здесь, мы в разделе Create Tables оставляем одну колонку,
потому что и в нашей таблице, по сути, будет одна колонка.
Вторая колонка полностью перекочевала в название папок.
Давайте теперь посмотрим, как наш код будет выполняться на этой нашей новой таблице.
Все, что нам надо сделать, это заменить Subnet на Subnet Part и выполнить код еще раз.
А пока давайте посмотрим, какие тут возможны варианты с этими партициями.
Вложенные партиции в Hive делать нельзя, поэтому там будет другой механизм.
Если мы хотим сделать партиционирование сначала по стране, потом по городу, допустим,
то вложенные партиции сделать не получится, а вот партиции по двум ключам спокойно можно сделать.
То есть, можно будет, чтобы был там Part 1.
Вот, то есть, мы разбиваем и по стране, и по городу. Чем это плохо, как вы думаете?
Очень много маленьких папок, поэтому вот так обычно не делают.
Да, там вопрос какой-то. Так обычно не делают, а вместо этого делают следующий уровень,
который называется бакетирование или кластеризация.
То есть, если партиции у нас делаются тогда, когда ключей точно немного,
то стран у нас сколько? 289, если я правильно помню, примерно так. Ну, не 5 миллионов.
А городу у нас намного больше, там уже счет идет на сотни тысяч, по-моему.
Поэтому в этом случае, для полей, у которых много уникальных ключей,
мы используем не партиционирование, а бакетирование. Чем оно отличается?
Физически один бакет это не одна партиция, а один файл. То есть, это следующий уровень вложенности.
Потом, как происходит разбиение на бакеты. Если в партициях мы имеем столько партиций,
сколько ключей, то что касается бакетов, ключей может быть много.
Поэтому там используется любимая нами формула еще из MapReduce, вот такая.
HashAdcap по модулю на B, B это количество бакетов. То есть, количество бакетов,
как и количество редьюсеров мы задаем сами, потому что если мы это все оставим на откуп
самого хайву, он нагенерит очень много файлов маленьких.
Пока что вернемся к нашим партизиям и давайте смотреть. Вместо 8,5 минут мы имеем 5 минут.
Вот, хорошее такое ускорение получилось примерно на треть.
Что касается бакетов, давайте посмотрим подробнее здесь, как это происходит.
Вам есть сейчас пример кода, показывать про бакеты не буду, потому что у нас все равно
два поля. Тут уже с нашими данными бакетировать нечего.
Но в принципе мы можем использовать бакетирование.
Вот, бакет-тейпл.
Хороший пример здесь. То есть, мы указываем clusterBy по какому полю или по каким полям.
И дальше int сколько бакетов. То есть, мы явно указываем вот это число B.
Ну и дальше у нас следующий уровень, если не помогает не вот это, не вот это.
Допустим, данные были собраны как-то не очень правильно и получилось, что больше всего
записей из России и больше всего записей здесь, из Москвы.
И когда мы делали это разбиение, у нас все равно 90% записей оказалось вот здесь,
у нас получился самый большой такой бакет. Что мы в этом случае делаем?
Мы в этом случае используем ключевое слово skewedBy.
Это такой последний шанс все-таки еще раз разбить данные, еще раз оптимизировать.
SkewedBy он уже вручную с конкретным бакетом можно поработать и как-нибудь разбить вот этот
один большой бакет по какому-нибудь условию.
Если мы его разбили и у нас оказалось, что в основном RuMSK 90%, значит надо разбить как-то еще,
потому что вот здесь допустим у нас будет не name salary, а будет больше полей.
И нам надо учиться как-то и по этим полям разбивать.
Ну просто потому что разбиение по вот этим ничего не дало.
Вот в этом случае у нас используется skewedBy.
Где еще можно использовать бакетирование? Можно его использовать в сэмплинге.
То есть вот есть такая утилита в хайве как tableSample.
Она используется в том случае, если вам, например, нужно что-то очень быстро, но не точно посчитать.
В этом случае мы делаем sample. Делается это все в одну строчку.
То есть мы указываем таблицу и указываем условия, соответственно с которыми мы будем формировать выборку.
Есть два типа сэмплинга. BucketSampling и BlockSampling.
Если у нас есть бакеты, если таблица разбита на бакеты, мы можем просто указать,
какие бакеты или сколько бакетов мы берем, например, каждый третий из 32.
То есть разбили все бакеты по 32 и берем каждый третий.
Как именно, вот, рандомно. А можно не рандомно, можно по какому-то правилу.
Сделали такую выборку и дальше можем с ней работать.
Если бакетов нет, то нам на помощь приходит BlockSampling.
Это, собственно, разбиение по блокам.
И здесь мы можем указать не только блоки, здесь мы можем указать количество процентов,
количество байт, вот в мегабайтах, в гигабайтах и количество строк.
И здесь надо понимать такую вещь. У вас, по-моему, даже в домашке будет такое,
что вот эти три способа, если данных мало, то реально работает только один по строкам.
Потому что два остальных, они округляют сэмпл до блока.
И вот представьте, если у вас данные небольшие и там всего, допустим, 5 блоков,
то все равно, что вы скажете, один процент, что пять процентов, что десять процентов,
у вас все равно это будет один блок. То есть от одного до двадцати процентов,
у вас будет одна пятая, будет один блок. И поэтому размер такого сэмпла будет одинаковый.
Поэтому в домашке, когда у вас будет задачка, насколько я помню, она есть,
где надо будет что-то просэмплировать, берете сразу, ну или бакеты создаете,
или берете последний способ сэмплирования, где указываете строки.
Вот здесь округления до блока не будет. Там, где вы будете задавать размер
и будете задавать проценты, будет округление до ближайшего целого блока и будет все нарушено.
Если сейчас какие-то вопросы, нам, в принципе, осталось разобрать два небольших кусочка,
тогда map join. Вы, наверное, помните, насколько в Hadoop'е нам было проще делать map join,
но, правда, он не всегда корректно работал, там были исключения. Сейчас попробуем это на Hive.
Сейчас я зайду все-таки на наш с вами MIPS-кластер, потому что он просто больше.
Вот, и так у нас будет сейчас две таблицы, с которыми мы будем делать join.
Для того, чтобы понять, что это за таблица, давайте выполним вот такое простое исследование,
то есть опишем таблицы, посчитаем, сколько строк и посмотрим на структуру.
У вас какие-то вопросы?
Так, вот у нас семь мапперов, один редьюсер. Ну и вот мы видим, какие поля в наших табличках.
Первая таблица – это логи работы какого-нибудь веб-сервиса, там, какой запрос, в какое время,
из какого IP-шника. И вторая таблица – это маппинг IP и регион России. У вас примерно такие таблицы будут в домашках.
Вот, и так у нас получается первая таблица. Где тут ее число? Вот у нас получается сколько?
Десять миллионов строк. И вид этих строк вот такой. И вторая табличка – она у нас маленькая,
у нее всего восемь тысяч, и сами строки тоже маленькие, всего по две колонки. То есть понятно,
что у нас как раз готовая картина для мапджойна. Теперь мы с вами будем джойнить эти таблицы и смотреть,
как это все работает. Сначала у нас будет такой полноценный взрослый джойн,
без всяких оптимизаций, без мапджойна. И чтобы это точно было так, я отключил опцию автоконверт,
потому что Хайф умный, если он видит, что одна таблица маленькая, он может сделать мапджойн автоматически.
Поэтому я это все отключил и делаем просто обычный вот такой лэфт джойн.
Сейчас пока весь этот код выполняется, можете задать какие-нибудь вопросы.
Я понял. То есть ты имеешь в виду блок-сэмплинг вот это по процентам, по строчкам?
И третий способ – по мегабайтам.
Вот именно из-за того, чтобы такого не было, Хайф округляет сэмпл до ближайшего блока.
То есть ты возьмешь, например, 10 мегабайт, а у тебя размер блока 128 мегабайт.
У тебя будет 128 мегабайт в сэмпле. Считается весь блок.
Поэтому просто в домашней нашей я не рекомендую использовать мегабайты. У нас маленькие данные, маленькие блоки.
И просто когда в задаче надо будет поисследовать что-нибудь в зависимости от размера сэмпла,
у вас там прямая будет просто, не будет зависимости. Сэмпл будет всегда одинаковый.
Вот у нас сработал джойн. И время его работает 3 минуты 19 секунд.
Теперь второй вариант этого джойна. Второй вариант мап-джойн.
Чтобы у нас был мап-джойн, что надо сделать? Первое, мы включаем автоконверт.
Второе, мы подсказываем Хайфу, что нужно сделать мап-джойн.
Есть специальный такой хинт. Это не комментарии. Комментарии в Хайве ставятся двумя дефисами.
А это специальный такой, как бы, комментарий с плюсиком.
Специальный такой синтаксис для того, чтобы мы указали, вот здесь мы хотим сделать мап-джойн.
И вот эта таблица у нас идет в роли маленькой.
На самом деле Хайф позволяет делать и тройной, и четверной мап-джойн.
То есть здесь через запятую можно указать целый список маленьких таблиц.
Они все добавятся в distributed cache и будет мап-джойн сразу от нескольких таблиц.
Но на самом деле не вот это, не вот это не гарантирует нам того, что мап-джойн реально будет.
Это все только подсказки. Если мы вот такой код используем для двух равных больших таблиц,
то Хайф никакой мап-джойн делать не будет.
Выполняем код. Пока выполняем, давайте посмотрим, что у нас тут было.
А тут у нас было 8 мапперов. Почему 8 мапперов? Потому что 8 блоков.
И почему 15 редьюсеров? Потому что мы так указали.
Что здесь?
Здесь, во-первых, мы видим number of reducers равно 0.
И во-вторых, мы видим, что наша задача работала 13 секунд, ну 14 секунд хорошо.
То есть здесь было 3 минуты чем-то, а тут было 14 секунд.
И в результате мы получили то же самое.
Вот, это что касается того, насколько мап-джойн нам может помочь ускорить работу.
Есть ли какие-нибудь вопросы сейчас?
Смотрите, не совсем так. На самом деле вот часть этих ускорений я вам рассказал только теоретически,
а практически мы сделали только два. То есть как бы алгоритм был такой.
Сначала мы сделали партиции, разбросали большие ключи, которых мало по папкам.
И это мы сделали на практике.
Потом мы уже без практики, мы сделали бакетирование.
То есть те ключи, которых больше, мы разбросали по файлам. Следующий уровень оптимизации.
Потом мы взяли один большой ключ, который не получилось разбить ни партиционированием, ни бакетированием.
И разбили его уже в ручном режиме как-то с помощью скьюет бай.
Нет, через скьюет бай. Тут пока нет джойнов. Это другое совсем.
Это просто разбиение данных. То есть мы могли оптимизировать вот так в три этапа.
Следующие вообще не связанные никак. Это можно использовать отдельно.
Можно делать партиции или делать только мап-джойны. Это вообще разницы нету.
То есть вот эта оптимизация мап-джойн связана с тем, что мы маленькую таблицу добавляем в distributed cache
и реализуем на ней тот алгоритм мап-джойн, который мы в прошлый раз разобрали.
И это никак не связано. Если заметишь, у нас таблицы даже были разные.
Но в оставшееся время давайте сделаем небольшой обзор того, что у нас существует, кроме хайва, для аналогичных задач.
Есть пиг. Точнее он уже скорее не есть, а скорее был, потому что пиг, он в принципе у него такая же логика работы, как у хайва.
Поэтому мы имеем высокоуровневый язык, который транслируется в MapReduce.
Но если в хайве это SQL, точнее HiveQL, это все-таки диалект SQL, он не совсем, не на 100% SQL.
То есть здесь это свой язык, пиг латин. Вот такая вот помесь питона с SQL, которую надо отдельно учить.
И поэтому пиг сейчас практически не используется.
Дальше импала. В отличие от хайва, это полноценная база данных.
То есть у нее есть и своя логика хранения, и своя логика вычислений. Она может быть не связана даже вообще с MapReduce.
Часть данных она хранит в памяти, из-за этого она работает намного быстрее.
Минус импалы в том, что тут нет поддержки сложных типов.
Вообще какие в хайве существуют типы данных?
Давайте посмотрим. Вот Hive Data Types.
Что мы тут видим? Мы видим большое количество численных типов данных.
И тоже обращайте на это внимание, когда будете делать домашку.
Например, если вам указана колонка возраст, то возраст это TinyInt.
Потому что, наверное, странно, чтобы у человека был возраст 32 тысячи.
Поэтому осмысленно выбирайте численные типы данных.
Потом у нас есть разные строковые, буллевские типы данных, есть разные таймстемпы.
И есть комплекс Data Types. Где-то тут я это видел.
Вот. Четыре типа данных. Массивы, словари, структуры и юнионы.
Вот ничего этого в импале нет.
Поэтому в импале сложнее работать, например, с XML, с JSON, с Ямлами.
То есть когда у нас есть какая-то вложенность.
Надо больше парсить руками, чтобы превращать это в плоскую табличку.
Ну или в набор плоских табличек.
Дальше Apache Presto. Это тоже еще одна база данных.
Тоже полноценная база данных. Написана на джаве.
Сейчас она продвигается всякими большими big data гигантами.
Типа Data Stacks.
Ну по сути она чем-то похожа на импалу.
Правда сложные типы здесь есть.
Дальше Apache Presto.

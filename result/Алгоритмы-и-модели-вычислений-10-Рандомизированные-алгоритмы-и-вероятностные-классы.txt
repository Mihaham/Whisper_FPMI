Сегодня мы поговорим про рандомизированные алгоритмы и
вероятностные, вероятностные, сложностные классы.
Ну, вообще рандомизированные, вероятностные, это более-менее синонимы.
Но вот традиционно в русском языке алгоритмы тоже могут называть вероятностными,
но как-то чаще рандомизированными называют, а вот сложностные классы наоборот.
Встречались ли вам где-нибудь такие алгоритмы в каких-нибудь других курсах там?
Тестную простоту, да, очень хорошо.
Тестную простоту, но на самом деле еще быстрая сортировка есть из таких совсем базовых примеров.
Но еще есть такой известный метод, называется метод Монте-Карло.
Что еще?
Но на самом деле метод Монте-Карло есть, есть Лас-Вегас, есть даже Атлантик-Сити,
но на самом деле метод Монте-Карло это с одной стороны некоторые конкретные методы,
а с другой стороны некоторые классы методов.
А Лас-Вегас это в основном класс методов.
В общем, давайте поговорим про метод Монте-Карло.
Идея рандомизированных алгоритмов стоит в том, что они позволяют сделать что-то,
чего не позволяют сделать обычные детерминированные алгоритмы.
Вот метод Монте-Карло, который классический, так сказать, the Монте-Карло метод,
конкретный метод связан с подсчетом интегралов, то есть площадей, объемов и так далее.
Вот пусть у нас есть там какая-то фигура,
и мы хотим посчитать ее площадь или ее объем.
Вот если фигура двумерная, значит, тогда есть такой школьный метод.
Школьный метод заключается в том, что разделить на какие-то клеточки маленькие.
Ну и школьный метод стоит в том, чтобы посчитать, сколько клеток целиком попало в фигуру,
и еще добавить половину кисла клеток, которые частично попали в фигуру.
Ну и вот это, если можно оплочить клетки, то это будет приближение к площади фигуры.
Ну и понятное дело, что чем мельче разбивать на клетке, тем ближе будет это приближение.
Ну, конечно, предполагает, что фигура какая-то нормальная, с какой-нибудь гладкой границей и так далее.
С задним каким-то уравнением, например.
Тогда да, можно так считать.
Ну, конечно, предполагается, что мы как-то умеем понимать, лежит ли клетка внутри или в ней, или на границе.
Ну, например, если мы по одной точке умеем понимать, лежит она внутри или вне, то хорошим приближением будет просто про четыре угла.
Посмотреть, лежат они внутри или вне.
Ну и понятно, что можно как-то залезть.
Вот даже на картинке такого и нету, да.
Залезать какую-то клетку.
А, ну вот эта вот клетка так похожа.
А, нет, неправильно, да, у нее тоже с разных сторон.
Да, в общем, я так абстрактно писал, даже нет такого примера.
Но понятное дело, что можно придумать так, что она там как-нибудь вот так вот вылезает.
Нет, не так.
Сейчас.
Что нужно сделать?
А, вот так, наоборот.
Да, тогда вот у этой клетки четыре угла внутри, но она не целиком внутри.
Вот.
Но опять же ясно, что чем, да, значит, чем меньше разбить, тем меньше такого будет происходить.
И в пределе все равно все нормально будет.
Соответственно, можно достаточно мелко разбить.
По всем углам посчитать, лежат они внутри или вне.
Те, где углы лежат внутри, считать, что они целиком внутри.
Те, где углы лежат вне, считать, что они целиком вне.
А те, где часть так, часть так, считать наполовину.
Или можно даже считать, что если у меня там три угла внутри и один вне, то она там на три части лежит внутри.
Вот.
И вот это вот будет каким-то приближением.
Вот.
И это даже будет довольно неплохо работать, если фигура двумерная.
Но если фигура многомерная, то возникает проблема, что если вы там на n части делите по измерению, а у вас d измерений,
то всего у вас будет n в степени d частей.
То есть получается, что как бы ваша сложность...
Ну, а n как-то связана с точностью приближения.
То есть сложность будет вроде бы полиномиальная на степени d.
Но она будет полиномиальная по точности приближения.
А вот по размерности она будет экспоненциальная.
И, соответственно, если d достаточно большое, там 10-мерное, там какое-нибудь 10-мерное, то уже никаким образом это считать нельзя.
Будет слишком...
Для приемлемой точности будет слишком большое число ячеек, которые все нужно обработать.
Вот.
Поэтому в чем заключается метод Monte Carlo?
Он заключается вот в чем.
То вот у нас есть там какая-то такая же фигура.
И мы вместо того, чтобы как-то ее целиком охватывать, мы просто занимаемся тем, что случайным образом...
Значит, случайным образом кидаем какие-то точки.
Да.
В общем, каким-то образом случайно кидаем точки.
Ну и считаем, какая доля точек попала внутрь фигуры.
То есть площадь фигуры примерно равна доля случайно выбранных точек...
Значит, попавших внутрь.
Ну с номеров, конечно, да, то есть нужно эту...
Либо вместо площади считать, что это доля площади внутриобъемлющего прямоугольника,
либо, наоборот, вот здесь вот умножить на эту самую площадь объемлющего прямоугольника.
Вот.
Ну вот это есть метод Monte Carlo.
То есть кинем случайно точки, посчитаем, какая доля попала внутрь, нормируем, и это будет ответ.
А это почему работает?
Ну это работает из предельной сиаремы.
Что, смотрите, вероятность того, что точка попала внутрь фигуры...
Ну опять же, если нормальная там измеримая фигура, то вероятность равняется доля внутри площади.
А у вас уже какой-нибудь вервер был?
Да, в общем, почти ничем не отличается.
В общем, это и то, и другое это мера.
Да, ну вы что-то уже знаете оттуда?
Ничего не знаете?
Не, ну смотрите, да, у вероятности есть...
Ну вот, что там в самом начале изучаю, да, или в школе, что была комбинаторная вероятность,
то у нас есть какое-то конечное множество вариантов, и, например, они все равновероятны.
Или, может быть, каждому конкретному варианту приписано какое-то число там от меня до единицы, так что они суммируются к единице.
И вот это число, это вероятность отдельного варианта для различных элементарных событий.
И тогда, действительно, вероятность это...
При равномерном распределении вероятности это просто доля множества хороших исходов, множества всех исходов.
Ну а при не равномерном там эту долю с весами нужно взять.
Вот, а еще будет геометрическая вероятность, когда у вас какое-то там пространство геометрическое,
с мерой.
И, соответственно, там нечетное число точек, поэтому попасть в конкретную точку можно только сравнению ноль.
Но тогда говорят, опять при равномерном распределении говорят, что вероятность попасть в какую-то фигуру
действительно пропорциональна площади этой фигуры.
Ну а при не равномерном там как раз возникает общий понятие мер, там всякая сигма, алгебра и так далее.
И мер тоже были. А, ну тогда вы, наверное, все знаете, что...
Вероятность это просто такая мера, у которой по всему пространству мера единица.
И, собственно, больше особо ничего.
Вот.
Ну вот, соответственно, да, действительно можно сказать, что вероятность, если мы берем случайную точку, то вероятность попасть внутрь фигуры
действительно более-менее по определению равняется доле площади этой фигуры, в доле площади объявляющих пространства.
Вот, а дальше есть предельная теорема.
Там они бывают разные, там есть центральная предельная теорема, разные другие предельная теорема.
Вот, и это как раз, видимо, будет на Тёрвере, пока особо не было.
Но суть в том, что, если вы много раз проводите испытания,
то у вас вот эта вот комбинаторная вероятность в том, что вы провели, да, то есть число успехов,
поделительное число испытаний,
стремится к настоящей вероятности, да, то есть к доле площади, в объявляющей площади.
И стремится довольно быстро.
Вот. Но при этом, там, как бы, в среднем стремится, да, а так-то, конечно, может быть, вам не повезло, да, и все ваши точки попали внутрь.
Да, такое может произойти редко, но может.
Вот. Ну, в таком случае вы сильно ошибетесь, да, подсчитывая долю.
Вот. Может, наоборот, там все наружу попали.
Но, скорее всего, да, там, с очень большой вероятностью,
подсчитанная доля будет в небольшом интервале центром в настоящей вероятности, да, в настоящей доле.
Вот. Ну, а там разные бывают, там, 95% на верительный интервал, там, 99%, да, это означает, что если у вас,
ну, давайте я картинку нарисую, там есть такое колоколообразное распределение, да, оно уже распределение гауссом, да, что вот есть настоящая,
настоящая тестота, настоящая вероятность.
Вот. И есть то, что у вас получилось.
Вот. И подлесно распределение, оно как-то вот так вот выглядит, да, вот такая вот колоколообразная функция.
Вот. Ну, то есть тут, как бы, чтобы, если не вдаваться там в детали, прям в тюрвер, то идея такая.
Пусть мы вот наши испытания проводим и n раз.
И еще вот эти вот n раз, да, как бы, эти n раз тоже еще много раз повторяем.
Вот. И вот здесь вот, как бы, по вертикали написано, насколько часто возникает такой вариант.
Да, то есть чаще всего будет прямо ровно в настоящем значении.
Да, немножко направо, немножко налево будет пореже.
А если сильно направо или сильно налево, это будет еще реже, и вот эти вот хвосты, как говорят, легкие.
Да, это легкие хвосты, то есть, они экспедиционно маленькие.
Да, то есть, они экспедиционно маленькие.
Вот. Ну, вот это вот по-простому, да, значит, что означает.
Есть такой эксперимент, знаете, можно прямо изготовить, изготовить такую штуку, что, как бы, вот такая вот пирамида, да, из каких-то таких штучек.
Да, значит, так что там сверху, ну, и так далее, да, значит, как бы сверху падает шарик, и он равновероятно падает сюда или сюда.
Вот. Потом, соответственно, вот эти вот, как бы, глотики так поставлены, что, если он падает сюда, то после этого он тоже еще равновероятно, вот сюда вот или вот сюда, вот.
И здесь тоже равновероятно, сюда и сюда.
Вот. Ну, и так, вот если сверху много шариков начать сыпать, да, а здесь, в самом низу еще будут как бы такие ловушки для них.
то как раз в центральной ловушке будет больше всего шариков, там левее-правее, немножко поменьше,
еще левее-правее, еще немножко поменьше, а если сильно, там в самой правой и самой левой почти ничего не будет.
И форма будет прямо вот примерно такая.
Ну, типа того, да.
Да-да-да, да, цешки асимпатические будут вот так вот.
Вот, но на самом деле, да, значит вот
центральная предельная теорема говорит, что даже не обязательно, чтобы было равновероятно.
То есть, например, если мы сделаем так, что у каждого гвоздика, скажем, средство 1 треть падает налево, а 2 третьи направо.
Да, вот так как-то немножко подвинем, но так и тут тоже 1 треть налево, там 2 третьи направо, да, и так у каждого гвоздика.
Вот, тогда как бы центр сместится,
вот самый высокий столбик будет не на середине, а как раз на 2 третьях.
Вот, но вокруг этого столбика все равно будет картина прямо такая вот в пределе.
Да, то есть он там никуда не будет скошено, а все равно будет вот так вот распределено.
Вот, там уже будут не цешки, но там будут цешки с какими-то степенями, да, по биному ньютона, вот, но все равно симпатически это будет вот так вот.
Вот,
ну и
значит, что еще нужно знать про
центрально предельную теорему, да, что если у нас n испытаний, да, то тогда как бы ширина
типичная ширина вот этого колокола будет порядка 1 делить на корень из m.
Вот, но ширина еще есть такое слово дисперсия,
насколько, ну дисперсия это как бы разброс,
насколько сильно, соответственно, вот чем больше испытаний, тем уже будет этот колокол, да, и тем точнее будет предсказание.
Вот, но это очень важно, да, что если мы хотим тут получить точность епсилом,
то это означает, что число испытаний должно быть порядка 1 делить на епсилон в квадрате.
Вот, а и это не зависит от размерности, да, то есть нам главное было,
значит, нам главное уметь
подхитать,
лежит наша точка внутри фигуры или не лежит,
да, то есть, чтобы понять, как бы испытание удалось или не удалось.
Вот, и тогда размер не важна, размер не важна, все равно будет полинамиальное число испытаний, чтобы получить
точность
порядка и епсилон.
Да, да, по вертикали насколько часто возникает такая доля, да, по горизонтали сама доля.
Вот, но и это гораздо лучше, чем метод разбивания на ячейке,
потому что в методе разбивания на ячейке, чтобы получить точность епсилон, нужно порядка 1 делить на епсилон в степени D,
испытаний, где D это размерность.
А тут порядка 1 делить на епсилон в квадрате,
независимо от размерности.
Вот, но тем не менее возможны ошибки, да, то есть какой бы тут легкий хвост не был, все равно можно попасть вот сюда.
Таким образом, да, значит, это вот конкретный метод Монте-Карло для подсчета объема многомерного.
Ну а вообще,
в общем случае, методами Монте-Карло
называют
рандомизированные алгоритмы,
значит, которые всегда работают за, ну у которых есть какая-то оценка на время работы
гарантированная, да, значит, имеющая
гарантированную оценку
на время работы,
но иногда ошибающаяся,
значит, но
могущее
ошибат.
Ну, тут, соответственно, ну и может быть какое-то соотношение между временем работы и вероятностью ошибки, да, что здесь происходит.
Так, а теперь действительно есть алгоритмы Лас-Вегаса.
Значит, алгоритмы Лас-Вегаса
наоборот
никогда не ошибаются, но могут работать очень долго иногда.
Никогда
не ошибаются,
но могут
работать долго.
Вот, ну давайте я так не,
приведу пример,
значит, он формально не совсем подходит, потому что он
не может быть
там, что вот есть вход и есть выход,
но концептуально будет именно так.
Значит, это пример,
ну, точнее можно сказать, что там
выход есть, но он сам по себе случайный.
Значит, пример, как сгенерировать
вещество,
как сгенерировать
вещество,
как сгенерировать
как сгенерировать
бит
равный
единице со вероятностью
1 треть.
Ну и, соответственно, и нулю, в принципе, 2 третья.
Нулю с
вероятностью.
Вот, ну а чем это может быть?
Ну, может быть так, что это непосредственно где-то
используется.
Например, вам нужно
кинуть кубик.
Да, до какой-то игры нужно кинуть кубик.
Значит, у кубика 6 вариантов.
Ну, вот, соответственно,
можно считать, что
что получить 11 вариантов
нужно
кинуть монетку
и еще кинуть
ну,
да, может быть, начиная с такого
может быть немножко сложнее, да, но
в общем, какую-то монетку
с тремя исходами равновероятно.
Вот. Ну, вот это не даже более
вроде как более простой вариант,
что просто единица 171 треть, но и 172 треть.
Как такая вкошенная монетка.
Ну, или может быть действительно
может быть прям такое нужно
например, вы, скажем,
пишете какую-нибудь компьютерную игру,
где там сражаются какие-то силы
и соответственно
вероятность победы зависит
от соотношения сил.
И соотношение сил такое, что вероятность
как раз вот такая вот.
И вам нужно сгенерировать такой бит.
Вот. А вы, например, умеете
генерировать просто бит.
Это вообще, на самом деле, интересный вопрос.
Где вообще брать случайные биты?
Сейчас, может, обсудим немножко.
Вот. Допустим, мы умеем кидать
честную монетку, да, и у нас есть
источник битов, которые
равновероятно нуле единицы, да,
и независимы друг от друга.
Так. Это вообще все понятные слова
говорю, что независимы?
Ну да. Да, да, да.
Формальное определение,
что вероятность
пересечения событий
это произведение вероятностей.
Вот. Ну, на самом деле, можно сказать так, что
независимые события
означают, что
исход одного не влияет
на исход другого.
Ну, в общем-то,
мы можем сказать так, что
независимые события
означают, что исход одного
другого.
Да, то есть, вот если у вас есть
там две монетки,
да, вот мы кинули одну,
значит, не важно,
как кинули другую.
Вот. Ну, например,
например, пусть мы там кидаем монетку,
монетка, то какой-то ветер дует,
и этот ветер как-то так делает,
что они чаще там орлом выпадают.
Или, например,
ветер как-то так
дует, что они чаще выпадают
вот это одинаково.
Что мы не знаем точно,
как именно он дует,
но он на них как-то одинаково влияет.
Вот. И тогда, значит, если мы одну
монетку кинули, она выпала орлом,
то тогда вторая тоже с большей
верности орлом выпадет,
потому что он как-то на них одинаково влияет.
Вот.
Вот это зависимость.
Ну, а независимость означает,
что ничего такого не происходит, да,
и то, что там одна монета как-то выпала,
никак не влияет на то, как все остальные выпадут.
Вот.
Хорошо, значит, пусть у нас такие биты есть,
и мы хотим вот так вот делать.
А что, может, кто-нибудь
знает, как это делать?
Да.
Ага.
Угу.
Угу.
Да, совершенно верно.
Значит, вот предлагается
такой алгоритм
генерации этого бита.
Соответственно, если
два нуля,
то...
Так, мы что хотим?
А, мы хотим, чтобы единица была в том числе на треть.
Так, давайте тогда...
Если две единицы,
то тогда
исход один.
Значит, если
ноль один или один ноль,
то тогда исход ноль.
Значит, а если два нуля,
то тогда повтор.
То есть кидаем еще,
еще раз две монеты кидаем, и все то же самое.
Ну и, соответственно, видно, что
такой алгоритм
вообще может никогда не закончиться.
Потому что, в принципе, может быть так,
что он все время кидает, да,
нули, нули, нули получаются.
Да, и вообще ничего другого никогда больше не выпадет.
Вот. Ну, такая, конечно,
рст ноль бывает.
Вот.
Ну, а теперь в среднем,
да, в среднем это очень быстро закончится.
Да, можно там
посчитать
среднее число бросков, да, то есть у нас
в среднем три четрати будет вообще один бросок.
Ну, точнее два.
С 73 будет два броска,
с 73-16-ых будет четыре броска и так далее.
В среднем
ну,
4 третьих умножить на два, да, то есть
4 третьих раунда будет.
Да, 4 третьих раунда или 8 третьих бросков.
Вот. То есть в среднем это быстро,
но никакой конкретной
верхней оценки нет.
Ну и вообще понятно,
что если у нас ограничено
число бросков,
то одну треть и две третьи
мы не можем получить. Будут получаться
только какие-то дроби
со вспоминатием степени у двойки.
Да, потому что у нас всегда два степени
максимально числа бросков в вариантах.
Да, и из них
какое-то слово подходит.
Соответственно, если мы на каком-то
этапе это остановим,
и вместо того, чтобы повторять, выдадим там
что-то по умолчанию, то у нас вместо
одной трети и двух третьей будут какие-то
дроби приближенные
к одной трети и двум третьям
со взнаменателями степень двойки.
Ну а симпатически, если
нужно никогда не заканчиваться,
то тогда будет
одна треть и две треть.
И в этом смысле он никогда не ошибется.
То есть всегда будет
точно будет вероятность одна треть и две третьи.
Но, значит,
оно действительно может работать долго.
Так, хорошо.
Ну раз как я же говорил,
как раз пять минут до перерыва,
давайте поговорим про то,
откуда брать
случайные биты.
Конечно, если мы говорим про модель,
то можно сказать,
что они просто там откуда-то берутся.
Да, можно по-разному говорить,
что у машины есть команда
получить случайный бит.
Или есть заранее, где-то они
на ленте уже записаны,
заранее мы их только читаем.
Или она сразу
получает аргумент, который считается
случайным.
В общем, в модели можно
разными способами делать,
но это не представляет большой проблемы.
А откуда на практике их брать?
Ну, из каких-то процессов,
которые мы считаем случайными.
Значит, вот там есть
каких-то
хаотических процессов.
Ну, я бы источники
разделил на природные,
технически и социальные.
То есть природные
это какие-нибудь там
как облака движутся,
или какие-нибудь там космические лучи,
или какой-нибудь радиоактивный распад.
В общем, какие-то такие
природные процессы,
которые либо очень сложные,
типа как гидродинамика
и движение облаков,
либо вообще
случайные,
как нам согласно теории,
как с радиоактивным распадом.
Ну, техники,
это можно питать там
температуру процессора,
или точное времяобращение
к функции,
что-нибудь такое можно брать.
Время обращения компилятора
к данной функции, например.
Ну, это считается, что
есть операционная система,
и в какой конкретный момент
будет выполнена эта программа.
Это вроде как случайная вещь.
Случайная какая-то вещь.
Если говорить
про социальные процессы,
то можно какие-нибудь там биржевые индексы
смотреть, какие-нибудь точные значения
котировок.
Можно смотреть, не знаю, как там люди
через транкету в метро проходят,
в какой точный момент.
Ну и так далее.
Есть много
процессов,
разной степени случайности,
которые можно использовать для получения битов.
Но дальше возникает вопрос.
Возникает вопрос, насколько
действительно случайная
и насколько случайный бит получится.
То есть, если это какое-нибудь движение
облаков, то оно же вообще не совсем случайное.
Там есть какие-то
закономерности.
И не повлияют ли эти закономерности
на то, что мы будем получать?
Если речь идет
о квантовых процессах,
и мы верим, что они
по-настоящему случайные,
то все равно стоит вопрос об их измерении.
И нет ли какой-то
систематической ошибки в приборе,
который их измеряет.
То есть, так или иначе
есть какое-то
отклонение,
могут быть какие-то неравномерности,
но дальше возникает вопрос,
что с ними делать.
В общем, вопрос,
зачем они
используются.
Если, например,
в методе Монте-Карло используется,
то нам совершенно не важно,
какие они на самом деле,
вот эти точки не на самом деле случайные,
или у них какая-то такая
хактическая закономерность,
что ее очень трудно предсказать.
Нам важно, чтобы их использование давало правильный ответ,
или почти правильный.
Чтобы, по крайней мере,
ошибка была не сильно больше, чем при естественно случайных.
Ну, тогда, на самом деле, если мы так хотим,
то можно
вообще
не особо заботиться,
на самом деле, об обычной случайности.
То есть, вообще
генераторы случайности,
генераторы случайности
делятся на генераторы
истинно случайных битов.
Это как раз то, про что я сказал.
Какие-то природные
или технические
процессы.
И также будут генераторы
псевдослучайных битов.
Вот.
И вот эти вот псевдослучайные биты,
они вообще не случайные.
Да, они генерируются
сложным, но детерминированным процессом.
Но, как раз, если псевдослучайные
работают так же хорошо, как
по-настоящему случайные
мысли вычисления,
то вообще было бы очень хорошо именно их использовать.
Но дальше возникает вопрос
может так сделать или нет.
Ну, вот, соответственно,
с истинно случайными
возникает вопрос
о об улучшении
случайности.
У нас может быть какой-то первичный генератор
из случайного,
из котического процесса,
а мы его выход как-то улучшаем,
чтобы там распределение было
получше.
Таким-то образом.
Ну, а про псевдослучайные,
их улучшение
это
какая-то модификация
генератора, так, чтобы
он обманывал побольше тестов
случайности.
Так что, то, что это псевдослучайные,
а не истинно случайные, никто бы не заметил.
Так, ну ладно, сейчас
прижим сделаем.
Потом
перейдем, значит.
Так.
Так, значит, теперь поговорим про
некоторые конкретные
рандомизированные алгоритмы.
Вот, ну и
прям такой классический пример.
Это тесты
про статуи.
Что-то в конце 70-х годов
был прям взрыв интерес
к этой теме.
Связанный, прежде всего, с открытием
с открытым ключом
криптосистемы RSA.
Значит, которая для своей работы
требовала простых чисел,
довольно больших.
И, соответственно,
поэтому требовалось
проверять простоту.
Вот. И
детерминированный алгоритм
только через 25 лет, начале
21-го века
игровал Каял и Саксена.
Вот.
Хотя, ну, в принципе, конечно, математики
с 18-го века интересовались
проекты простоты,
а так еще и в Древней Греции идет всякая
решетовая ротосфена и так далее.
Вот. Но
прям так вот реально понадобилось
именно в конце 70-х годов.
Вот.
Ну и есть
такое высказывание,
не помню, правда, чье,
то если бы тогда в конце 70-х
был бы известен
полиномиальный алгоритм
проверки простоты, детерминированный,
то есть как раз
АКС-алгоритм, то
тогда область
верензовых вычислений
развивалась бы вовсе не так быстро.
Потому что если можно
прям точно узнать, то
какая разница, что там можно
посчитать вероятностью.
Вот.
Значит,
ну тогда вот это не знали,
уже нужно было
длинный число проверять
на простоту.
Ну вот придумали
несколько разных подходов
как посчитать
вероятность.
Вот. Ну, самый простой,
но не совсем корректный
на самом деле, значит,
это тест фирма.
Значит, есть такая
малая теория на фирмах,
которая говорит,
значит,
малая теория на фирмах
говорит, что
если P простое,
вот,
а,
а и P, да,
именно просто,
да, то есть, ну попросту говоря,
они делятся на P.
А то тогда, а в степени P-1
значит,
сравнимо с единицей
у моделю P.
Вот.
И идея заключается в том,
давайте брать случайно а и просто
проверять.
Значит, тест
соответственно выбирает
случайно а
ну и проверяет
Так, ну дайте себе, чтобы была другая
буква,
что а в степени x-1 сравнимо с единицей у моделю x.
Вот. Тогда, если оказалось,
что неверно,
и при этом а на x не делится,
то это означает,
что точно не простое.
То есть, если неверно,
то точно,
точно не простое.
Вот. Однако есть проблема.
Знаете ли вы, в чем стоит эта проблема?
Да, что есть числа кармаекла.
Значит, существует
числа кармаекла,
значит, для которых
тоже
всегда в степени x-1
сравнимо с единицей
по моделю x.
Ну, там вот есть теорема Эйлера
про то, что а в степени
функции Эйлера от x-а
сравнимо с единицей.
А вот числа кармаекла, это как раз те, для которых
вот x-1 делятся на афии от x.
Вот. Ну и тогда получается,
что мы единицу в какую-то степень возводим, да,
остается единицей. Вот.
Числа кармаекла не очень много,
они довольно редко распределены.
Ну и, соответственно,
вот. Но, значит, проблема
вот в чем, да, что можно было бы сказать,
что, ну, ничего страшного, да, у нас
просто иногда ошибки возникают,
числа кармаекла немного,
вот. Но проблема в том, что это ошибки,
в смысле, что для некоторых x,
независимо от выбора случайного, у нас
случается неправильный ответ.
Вот. А мы хотеть другую ошибку,
что там всегда ответ получается,
скорее всего, правильный,
но с некоторыми,
ну, каким бы ни было x, да,
ответ всегда будет правильный,
но с небольшой вероятностью неправильный.
Вот.
Но вот если нас это устроит,
да, то есть мы как будем тестировать
простоту или принадлежность
к числам кармаекла,
да, то это будет хороший тест, да,
потому что можно сказать,
что если, если вот
этот раз не всегда верно,
то верно не больше, чем в половине случаев.
Вот. Ну, и тогда,
соответственно, можно,
например, не одно случайное a взять,
скажем, десять случайных a взять,
и тогда, если это всегда
верно, то все a пройдут.
А если это не всегда верно,
тогда, каждая пройдетaca tapi одна вторая,
а все a вместе
пройдутся по 324,
ну, и вот это можно считать маленькой ошибкой
и считать, что это вы, скорее всего, правы.
Вот. Ну, то есть как бы вот так на коленке такой тест тоже пройдёт.
Вот. Но надо понимать природу ошибки, да, что она хоть и редкая,
потому что число кармайкла мало или меньше, чем простых.
Вот. Но тем не менее уж если число кармайкла попался, там всегда будет ошибка.
Вот.
Так. Ну ладно, соответственно.
Что ж тогда делать?
Ну, тест Миллера Рабина на самом деле, так сказать, исправляет ошибку.
Значит, тест Миллера Рабина проверяет следующее, да, что, во-первых,
а в степени х-1 сравним с единиц по моделю х.
Вот. Дальше, но если х было чётным, то оно почти никогда не простое, да,
то задача триреальная получается. Вот. А если х нечётная, то тогда
должно быть ещё и а в степени х-1 пополам, да, значит, сравнимо с плюс-минус единицей.
Да, значит, потому что вот по простому модулю, да, всегда корень двумя способами извлекается,
а вот по непростому, может быть, и больше в том способу, да, то есть вот,
ну, всегда а в степени х-1 пополам в квадрате равно вот этому, да, то есть равно единице.
Вот. По простому модулю будет всего два варианта, плюс-1 и минус-1.
По непростому будет какие-нибудь ещё, да, например, модулю 8 и 1 и 3 и 5 и 7 в квадрате
даёт остаток 1. Вот. Значит, тоже по модулю х. Вот. Ну, а дальше, если тут плюс-1,
да, значит, если тут 1, то тогда, соответственно, а х ещё и, ну, х-1 ещё и на 4 делится, да,
да, тогда будет а в степени х-1 на 4 тоже должно быть сравнивать с плюс-минус единицей.
Вот. Ну и так далее, так далее, пока не поделена максимальная степень Лойкина,
которая надеется, да, значит, и так далее, пока х-1 делить на 2 в степени х не станет
ничьётным, ну, или пока в какой-то момент не будет минус единицей, значит, или
пока не появится минус единицей. Да, значит, тогда получается, что вот для простого
числа ряд из таких остатков будет сначала единицей, а потом возникнет минус единицей,
ну, либо до самого конца будет единицей. То есть получается, что для простого числа
будет, соответственно, 1, 1, 1 и так далее, потом минус 1, ну, либо уже до конца единицей.
Вот. Так вот теорема, которую доказали Миллер и Рабин, мы не будем доказать,
она довольно такая сложная теоретика числовая, но, в общем, теорема такая,
то если их непростое, то ряд такого вот, такого вот, такого вида будет не более
чем для четверти, для четверти иксов, в том числе и для чисел Кармайкла, там всё равно
всё будет вот так. Вот. Ну, это получается, что да, и понятно, что это всё можно
проверять за полинамиальное время, в том числе и за полинамиальное, конечно же,
от еклобитов, да, то есть за логеремическое формовое яйца. Вот. Тут, ну, понятное дело,
что мы можем определить максимальную степень двойки, на которой х-1 делится,
посчитать все вот эти вот степени, ну, и там ещё нужно алкерить быстрое возведение степени.
Да, это вы знаете. Вот. Ну, и тогда получается, что если х-ростое, то проверка пройдётся всегда.
Вот. Если х-непростое, то всё равно не больше 1 четверти. Вот. Ну, вот. Так этот тест работает.
Вот. Значит, есть разные тесты. Да, значит, есть тест Соловея-Штрассона, который
на другом принципе работает, да, он связан с квадратичными вычетами и не вычетами.
Вот. Есть ещё более сложные тесты. Например, есть такой тест Эйдл Мана-Хуана.
Для нас сейчас важно следующее. Смотрите. Какова природа ошибки? Вот здесь вот.
Давайте я прям, наверное, запишу, что если х-ростое, то тогда и тест говорит,
то простое. Вот. А если х-составное, то тогда тест, скорее всего, говорит,
так, давайте я прям тут допишу, тест точно говорит, что простое. А если х-составное,
то тест, скорее всего, говорит, его говорит, что составное, но может сказать,
и что простое. Вот. То есть тут ошибка с одной стороны. Ещё важно понимать,
следующий, да, это вот, что тест говорит, а ещё можно с другой стороны посмотреть,
как интерпретировать результат теста. То есть тогда, если тест говорит,
что число составное, тогда оно точно составное. Наоборот получается. Если число
простое, то тест точно говорит, что оно простое, а если он говорит, что это
составное, значит оно точно составное. А если тест сказал, что простое,
тогда может быть простое, может быть составное. Вот. Там ещё нужно пересчитывать
вероятности учётом формы bias, да, поскольку простых чисел вообще нет.
Ну скажем, если рассмотреть там от одного до там миллиона, да, там один
деятель на логариф men, натуральный, да, примерно такая частота. Ну миллион
это у нас, ну, в общем-то у нас, в общем-то у нас, в общем-то у нас, в общем-то у нас
натуральный, да, примерно такая частота. Ну миллион это у нас E в какой степени,
да, это два в двадцатый, да, E в какой-то меньшей, да, там, ну не знаю, в пятнадцатый,
например. Вот. То есть, грубо говоря, там одна пятнадцатая числа от одного до
миллиона будет простой. Ну так, пример, да, в общем, можно посмотреть, сколько
именно, и тогда, смотрите, если мы берём случайное число и запускаем вот прям вот
этот вот тест один раз, и тест кажется, что оно простое. Тогда как будет вероятнее,
что оно простое или оно составное? На самом деле, всё равно будет, по вкассе
может быть вероятность, что оно, да, считается, что одна семнадцатая, да, то есть
на одно простое приходится шестнадцать составных. Тогда, как бы, вот этот вот
тест, да, он, скажем, для четверти, да, пусть ровно для четверти работает.
Вот. А из этих шестнадцати он двенадцать отбросил, а четыре оставил. И теперь у нас
на те случаи, когда тест сказал да, у нас получается одно простое и четыре составных.
Вот. Но это не беда, потому что можно ещё раз тест запустить заново, тогда из этих
четырёх составных уже только одно останется, также будет пятьдесят на пятьдесят.
Вот. Но если ещё пару раз запустить, и он все раз и сказал, что простое, тогда уж,
наверное, простое. Да, это вот процесс, который называется амплификацией, да,
значит, когда уменьшается ошибка. Если тест провести несколько раз, да, то тогда
это будет более надежный. Так.
Так вот. У теста Эйдл Манахуана устроена ошибка ровно наоборот.
Значит, сам тест я не буду рассказывать, да, он довольно сложный.
Тест Эйдл Манахуана устроен ровно наоборот. То есть, если число составное,
то тогда тест точно говорит, что оно составное, а если число простое, то,
соответственно, он большой вероятностью говорит, что оно простое.
Да, и там случается ошибка с другой стороны.
Ну и, соответственно, такие два теста можно, на самом деле, использовать совместно.
Да, то есть, можно один применять, потом другой применять.
И рано или поздно, да, рано или поздно один из тестов даст такой результат,
который можно говорить безговорочно.
Да, то есть, либо первый тест скажет, что число составное, тогда оно точно составное,
либо второй тест скажет, что оно простое, тогда оно точно простое.
Вот. И вот это, на самом деле, более явный пример алгоритма Лас-Вегаса,
который я тут как раз стер.
Вот, хорошо.
Так, дайте еще один пример задачи.
Значит, она, на самом деле, больше важна для теории,
но, тем не менее, она довольно поучительна сама по себе.
Значит, чем она важна для теории?
Потому что, в отличие от редкой простоты, вот для той задачи, которую я сейчас опишу,
неизвестна никакого детерминированного алгоритма полиномиального,
но известна вероятностью.
И вообще, в целом, тут такой же ответный вопрос, как и про PNP.
Значит верно ли, что рандомизированный алгоритм мог сделать больше
чем детерминированный.
Появ Ist, что если вероятность есть, то ее можно не использовать.
И все обычные будут так же и тривиально рандомизированными.
Поэтому все задачи, которые решаются обычными, решаются рандомизированными.
Но вопрос, есть ли такие задачи, которые решаются рандомизированными,
... не решаются обычными.
На первый взгляд, ситуация точно такая же, как и с NP, да, есть ряд задач,
ну, не такой большой, как для NP, но, тем не менее, есть ряд задач, которые
не решаются известными детерминированными алгоритмами, но решаются известными рандомизированными.
Вот. И никто там не умеет для них ничего придумывать.
Вот. И вот вопрос, должны ли мы на этом основании полагать, что, действительно,
рандомизированные могут больше?
Ну, вот, оказывается, если поглубже в теорию погрузиться, то, на самом деле,
скорее всего, всё-таки, рандомизированные ничего не могут дополнительно.
Это просто учёные пока не придумали алгоритмы для вот тех задач.
В том числе, для той, которую я сейчас расскажу.
Вот. Там довольно глубокая есть теория, в том числе, зависимость между двумя этими фактами,
про PNP и силу случайности.
Даже там была статья, где это описывалась, очень важная статья,
с таким сброским названием Hardness versus Randomness.
То есть, оказывается, что не может быть и того, и другого.
Либо есть Hardness, то есть, есть сложные задачи, которые решаются только перебором.
И тогда, в общем, на самом деле, там Hardness – это не только то, что PNP, несколько более сильное утверждение.
В общем, грубо говоря, что если есть достаточно сложные задачи,
то тогда, на самом деле, его можно дерандомизировать.
То есть, превратить вероятность алгоритма в детерминированное.
Вот. Ну, либо есть Randomness, то есть, случайность действительно добавляет вычислительную силу.
Ну, вот. Ну, не знаю, может быть, мы немножко подробнее успеем обсудить, но это реально сложная вещь.
Я это бывает на спецкурсах читаю.
Вот. Ну, значит, что это за задача, которую я несколько раз анонсировал?
Значит, это задача о равенстве многочленов.
Вот. Ну, на первый взгляд, она какая-то совершенно очевидная.
Что вот даны многочлены, значит, даны многочлены P и Q.
Вот. И вопрос – это один многочлен и разное.
Значит, они одинаковые или нет?
Вот. Ну, на первый взгляд, кажется, что вообще в чем вопрос?
Просто раним коэффициент, они либо одинаковые, либо нет.
Но это верно только при канонической записи.
Каноническая записи означает, что мы предчисляем многочлены как сумму одночленов.
Конечно, если многочлены – это сумма одночленов, то даже если там в разном порядке их записывать,
да, и одночлены, там есть от нескольких переменных, то тоже можно по-разному писать.
Тогда мы это, конечно, сможем проверить. Никой задачи нет.
Но может многочлен быть задан не как сумму одночленов, а как какое-то арифитическое выражение.
Арифитическое выражение, где там перемножаются какие-то многочлены, складываются, квадраты возводятся.
Вот. И тогда, на первый взгляд, тоже нет проблем.
Можно просто раскрыть все скобки. Да, и после этого их все равно.
Но на самом деле тут уже есть проблема, потому что если вы раскрываете все скобки,
то у вас слагаемых может быть экспоненциально много.
Если у вас, например, произведение n скобок, и в каждой скобке 2 слагаемых,
то тогда может получиться экспоненциально много слагаемых в результате.
И поэтому так просто вы их уже не сравните.
Ну а в общем случае, еще на шаг дальше,
дело продвигается,
а именно говорит, что даже может быть не выражение, а тоже называются арифитические схемы.
Да, значит, тут важно, что вот эти многочлены заданы арифитическими схемами.
Но это аналог логических схем, которые мы изучали,
только там вместо конъюнкции, дезюнкции, отрицания будет сложение и умножение.
То есть это аналог булевых схем,
но со сложением и умножением.
То есть получается, что у нас есть какие-то переменные.
Это, может быть, отращено от нескольких переменных.
Мы эти переменные как-то там складываем, перемножаем,
результаты тоже можем складывать, перемножать
и, соответственно, переиспользовать.
То есть одно и то же.
Все схемы, в общем, отличаются схемой просто от формулы.
Тем, что одно и то же сложные выражения, которые в разные части формулы входят,
мы можем сократить и один раз только использовать.
И это еще дополнительно уменьшает размер.
Соответственно, эти схемы должны быть полинарного размера.
Что такое размер схемы? Мы уже изучали?
Это тоже вопрос, вот чего?
Ну, можно сказать так, что как раз размер схемы мы будем считать параметром,
от которого будет пленом браться, который уже во времени работает.
Не, значит, нам, смотрите, что нам нужно?
Есть два подхода.
Один подход, что есть размер входа.
Именно от него все должно быть поленоимеально.
Поленоим arriving с
общий параметр, и от него измерiają все палиномы,
то есть, и размер схемы должен быть палиномиальным от этого
параметра, и время работы должно быть палиномиальным
от этого параметра, и все, что там еще есть, будет
палиномиальным от этого параметра.
Ну, более-менее, одно и то же, да,
потому что полином от полинома, это полином,
вот. Но, бывает так, как удобно,
ну, например, можно считать, что у вас, скажем,
скажем, схема одного размера, n — это размер одной схемы, а не двух.
Или еще там что-нибудь.
Или, например, можно считать, что n — это число элементов в схеме,
а не полной длины записи еще со всеми связями.
В общем, вот такая задача.
Ну что, понятна ли формулировка?
Ну теперь я хочу рассказать, как ее решать вероятно.
Так, идея решения.
Идея решения, что если
п... так, вот я тут тоже три черты использую, но здесь это имеется
тождественно равно.
То это как бы один и тот же многащение. В том смысле, если мы раскроем все скобки
и упорядочим каноническим образом, то будет один и тот же многащение для p и q.
Значит, если p и q — это одно и то же, то, соответственно, для любого x
p от x равно q от x.
Это, на самом деле, не совсем одно и то же,
потому что это зависит от того, над каким полем вы это вычисляете.
То есть, например, если у вас только 0 и единица, то x и x² — это разные
многочлены, но, тем не менее, одна и та же функция.
И, соответственно, вопрос, который здесь ставится, — это именно равны ли
p и q как многочлены? То есть, там есть какие-то переменные,
и если раскрыть все скобки, то будет ли одна и та же сумма
значеных или не одна и та же?
Изначально, не, изначально над натуральными числами.
Не над полем, да. Изначально...
Нет, можно сказать, что там вообще никакого поля нету, а у нас есть просто переменные, есть
сложение и умножение. Но если мы перемену саму собой складываем
много раз, то у нас автоматически получается натуральный коэффициент.
И, соответственно, больше никак коэффициент не получится.
Если у нас нет константа, есть только сложение и умножение,
мы начинаем с переменных, можем их складывать, будут...
но не вычитать и не делить, тем более, мы их можем только складывать и умножать.
От этого у нас будет получаться натуральный коэффициент.
А у нас только сложение и умножение.
Если у нас есть сложение и умножение, то вычитать не получится.
Не, можно рассмотреть другой базер, минус тоже добавить,
будут как бы другие схемы, другая постановка, и решаться будет прямо так же все равно.
Действительно, значит, если p и q одно и то же, то для любого x, p от x равно q от x.
А если p и q это не одно и то же, то для малого,
для малого числа x будет верно, что p от x равно q от x.
Значит, почему? Потому что тогда разность, да, значит, разность p минус q.
Значит, это нетривиальный, то есть не нулевой, не тождественно нулевой многочлен.
И тогда у этой разности не слишком много корней.
Соответственно, у p минус q не может быть слишком много корней.
Вот. Ну, я сейчас покажу, что это значит.
Не может быть слишком много корней.
Ну, например, если там всего одна переменная, то тогда вы, наверное, хорошо понимаете, да,
что многочлены степени D не больше, чем D корней.
Значит, если многочлены вот одной переменной, то тогда, соответственно,
многочлена степени D будет не больше, чем D корней.
Вот. Ну, это какое-то очень простое утверждение, да, но, правда, опять же вопрос над каким?
Ну, над полем. Для поля это верно, да, не важно над каким, над действительными,
над комплексными, над конечным полем, над любым полем.
Вот. А если это не поле, а кольцо, тогда, конечно, может быть и больше, чем D корней.
Вот. Ну, если это кольцо целых чисел или натуральных, то все равно не больше D, да,
потому что над, даже над комплексными будет не больше, чем D корней.
А если мы от комплексных оставили только вообще целые, да, действительные целые,
то тогда, соответственно, их больше не может стать.
Вот. Ну, и, соответственно, идея такая, да, значит, если от многих переменных,
то там несколько сложнее, значит, от нескольких переменных.
Давайте я только ключевые слова скажу. Да, значит, тут Лемма Шварцзиппеля.
Лемма Шварцзиппеля. Да, у вас ее не было?
А, ну, уже здесь на семинарах. Да. Ну, в общем, либо еще будет, либо там.
Поищите где-нибудь. В общем, суть в том, что, суть ее все равно, что не может быть слишком много корней,
но, опять же, да, на самом деле вот тут мы уже переходим, да, значит, переходим к конечному полю,
потому что вообще-то, ну ладно, пусть у нас даже одна переменная, да,
Ради чего мы все вот это изучаем? Ради метода проверки, так что взять случайный х и проверить, равны или нет.
Да, то есть тест получается, тест получается в том, чтобы проверить, что P от х равно Q от х для случайного х.
Соответственно, если не равно, то тогда многочлены точно разные. Вот. Если равно, то, скорее всего,
скорее всего одинаково. Вот. Но есть, с таким применением подходом есть некоторая проблема.
Во-первых, если у нас в цикле любое натуральное число, то нельзя взять случайно натуральное число.
Да, это как, это понятно, наверное, да, что, ну я бы, если мы берем равновероятно, да, то вероятности должны быть сюда одинаковые,
но если она равна нулю, то тогда и вообще сумма равна нулю, а если она больше нуля, то сумма равна бесконечности, да,
поэтому не может быть равноверного распределения на натуральных числах. Вот. Может быть только там какое-то.
Вот. Это первая проблема. А еще есть вторая проблема, немножко более тонкая, в том, что сами значения P от х и Q от х могут быть слишком большими.
Потому что, смотрите, у нас может быть, например, итерированное возведение в квадрат.
Что мы берем там, скажем, двойку, ее возводим в квадрат, результат тоже возводим в квадрат, и так n раз.
И тогда у нас получается дважды экспоненциальное число, которое занимает экспоненциальное число bit.
Вот. Соответственно, просто записать результат тоже не получится.
Вот. Ну вот, и чтобы от этих обеих проблем уйти, да еще и получить лему Шварцзиппеля для случаев нескольких переменных,
от натуральных чисел переходят к остаткам по простому модулю.
Вот. Соответственно, так, в общем, все это нужно, значит, нужно по модулю P.
Для какого-то простого P.
Ну и тогда там можно доказать, что можно взять как раз простое число, не слишком большое,
чтобы можно было работать с остатками по этому модулю.
Но не слишком маленькое, чтобы, тем не менее, слишком много корней не появилось.
То есть вот это простое число должно быть сильно больше, чем степень.
Но при этом экспоненциальное от m.
Но, в общем, это можно все сделать.
Ну, от размера схемы.
Не, смотрите, само число экспоненциальное, а длина его записи полиномиальная.
И поскольку сложение и умножение по модулю это эффективная операция, то это нам подойдет.
То есть проблема была в том, что значение получалось дважды экспоненциальным.
За счет интерированного задения в квадрат.
Ну вот, соответственно, если все эти оговорки сделать, то все сработает.
То есть вот это все равно останется.
Если не равно, то точно не равны.
То есть вот сюда, если мы теперь по модулю считаем, то вот это точно верно.
А вот это вот останется.
Если у нас малое число, будет малой долей в этом поле из поэлементов.
Ну вот, значит, это будет работать.
Ну еще есть вопрос, откуда простое число взять такого размера.
У нас же есть тест простоты.
А еще есть теория о том, что простых чисел довольно много.
Можно просто брать случайно через каком-то интервале, проверять простоту
и в какой-то момент наткнемся на подходящее.
Так, ну вот. Вот такое вот рассуждение.
Ну что ж, у нас в следующий раз будет подробный разговор про веренцевные классы.
Так что всем спасибо за внимание.

Ну, не помню точно, чем мы закончили, но в целом идея была в том, что мы смотрели на то, как в целом записывается это невозможно так сейчас секунду, то есть решить вопрос по так тут эта штука и тут появился ра.
Да, мы закончили на том, как учитывать следующая точка в нашем стандартном методе, то есть у нас есть текущее приближение так и вот так, наверное, сейчас правильно будет сделать.
Да, у нас есть текущее приближение, и мы его обновляем с помощью найденного направления ашкатова и шага альфа.
Посмотрели два разных способа. Линейный поиск и методовитных областей.
Обсудили то, чем они похожи, чем отличаются. Вот и дальше перешли к тому, что мы закончили.
Вот и на этом, кажется, остановились. Я правильно помню?
Кто-нибудь вообще помнит? Отлично, спасибо.
Вот так надо сейчас заранее подготовить доску.
Значит, обычно используются три таких основных методовитных областей.
Ну, основные как бы критерии сравнения можно так назвать.
Вот можно сравнивать их по сложностям.
Тут есть две взаимозависимые и связанные истории.
Первая называется аналитической сложной, вторая называется ажематической.
То есть под аналитической сложностью подразумевается, что мы просто считаем число областей, а не областей.
И считаем, что, условно, вызов оракула это константное время занимает.
Поэтому мы не будем особо напрягаться, если там...
Ну, в общем, все это делается по модулю того, что...
Так, какая у нас лекция? Десять. Отлично.
По модуле того, что посчитать оракулу, мы не будем напрягаться.
По модуле того, что вычислить ответ оракулы — это просто.
Это, конечно, не всегда так.
И мы увидим, когда это не так и к чему это может приводить.
Но в нашей текущей концепции черного ящика и вот этой оракуленной абстракции
мы вполне себе можем предположить, что нам важно только то, сколько раз мы должны посчитать оракулы.
вполне себе можем предположить, что нам важно только то, сколько раз мы должны посчитать
амусол на градиен. Ну и арифметическое, это почти то же самое, только считаем не количество вызова
аракулы, а общий число вычислений, в смысле, сколько там флопсов надо сделать. То есть понятно,
что они друг с другом связаны в виде того, что арифметическая сложность просто аналитическая
умножить на стоимость одной итерации, на стоимость вызова аракулы. Поэтому тут зависимость
от контекста удобнее либо одно, либо другое определение сложности использовать. Второй способ
это сравнить скорость сходимости. Обычно вот здесь всякий термин доказан. Вот про то, какой
метод как сходится, в смысле какого из критериев, которые мы перечисляли, вот здесь вот. То есть
что на каждой следующей итерации меньше? Норма ошибки, модуль разности между f и f
со звездочкой. Или же там как-то градиент правильно образом уменьшается. Это все можно
как бы оценить в зависимости от числа k, то есть какой будет прогресс на каждой итерации. Отсюда
получаем скорость сходимости. Сейчас будут примеры того, что это скорость сходимости у тебя не
бывает. И что и как оно соотносится друг с другом. Ну и третий путь, самый такой дуболомный,
это просто эксперименты. То есть просто можно запустить на вашей конкретной задачи набор методов,
посмотреть какой из них работает лучше, какой хуже. Понятно, что вот это наиболее трудозатратная,
наверное, вычислительная трудозатратная операция, потому что надо, ну, во-первых,
надо что-то запускать и кодить, возможно. Вот, не всегда это просто. Во-вторых, надо,
возможно, если очень большая, то довольно вычислительно затратно и много времени надо
ждать, когда же у вас что-то считается, чтобы сравнить порядок сходимости. Почему надо дожидать
достаточно долго, будет понятно уже скоро. Какие скорости сходимости бывают? Я, кажется,
на этом не стал сильно торопиться и решил начать именно с того, какие бывают скорости сходимости.
Первый, это сублиней, имеет вот такой замечательный вид, имеет вот такой вот вид, то есть мы нашу норму
х на каждой итерации следующей приближаем к х со звездочкой, как вот такая вот оценка сверху.
Вот, то есть тут, заметьте, стоит к, это тоже самое число, которое здесь, вот, и альф меньше единицы.
Вот, то есть это c делить на к, c делить на к квадрат, c делить на корень, с к, все вот это вот
подходит под сублинею сходить. Следующий, это линейная или еще называют геометрической прогрессией.
В этом типе сходимости зависимость уже другая, то есть левая часть остается такой же, вот, а правая
оценивать сверху, как c умножить на q в степени k, то есть q это число от нуля до единицы, вот, и, соответственно,
k это тоже самое, это к, которое здесь по аналогии с тем, что было. То есть, видите, правая часть существенно образом изменилась.
Значит, сверх линейная, это все то же самое, только здесь было к, просто к, ну и тут, возможно, какая-то линейная функция от k, вот,
а здесь у нас стоит k в степени p, то есть некоторая полиномиальная зависимость от номера итерации, вот, и самая, наверное,
быстрая, с которой мы будем иметь дело, это квадратичная, вот, в которой вот этот вот полином изменяется на экспоненциальную зависимость,
ну и в частности два в степени k, вот. Альтернатива к такой формулировке есть не что иное, как вот такая вот формулировка,
то есть мы, наша норма в квадрат раз, ну, то есть вы умножаете текущую норму в квадрат и получается, ну, у нас следующая норма, вот.
То есть каждый раз число значих цифр, по сути дела, удваивается, удваивается или возводится в квадрат? Возводится в квадрат, да.
То есть было две значих цифры, стало четыре, было четыре, стало шестнадцать, ну, в общем, и так далее, вот. То есть все оно, это все довольно быстро сходится.
Ну и картинка, которая это все дело иллюстрирует, значит, синий это сублинейная сходимость, и тут видно, что, видно, что для нескольких первых итераций сублинейная сходимость может оказаться быстрее, ну, как бы давайте ошибку меньше, чем линейная, вот.
Но в дальнейшем, если мы как бы тут немножко продолжим, тогда вот эта штука улетит в десять минус, там, условно, двадцатый какой-нибудь, вот, да, а вот эта штука как раз-таки сублинейная останется вот примерно на таком же уровне, а линейная будет идти, прямой будет график вниз.
Ну, сверхлинейная, понятно, идет поначалу сильно быстрее, но потом все равно разница между вот этим квадратиками, вот этим треугольничками, вот этим квадратиками, вот этим треугольничками, она уже достаточно существенна для того, чтобы, ну, сверхлинейная сильно существенно симпатически медленнее, чем квадратичная.
Понятно ли, что это за график, за понятия, как они отсортированы и почему, условно говоря, они так называют?
Вроде да.
Значит, важный момент про тярем исходимости. Вот сейчас на этих двух слайдах будет приведено некоторое summary вот этого вот параграфа из книжки, в общем, рекомендую с ней ознакомиться, вот, она довольно, но она не очень новая, то есть это, наверное, из самых первых книжек, вот, но, кстати, может быть, я даже показать могу сейчас, секунду, ой, да, плюсы, ну, я не знаю, видно, не видно, вот она вот так вот выглядит, сейчас.
Да, вот заблюдивается, да, окей, в общем, такая вот зеленая книжка, вот, вот такой вот толщины издана в 83 году, вот, ну, это, по-моему, переиздание уже, наверное, не написано, ну, ладно, не так это важно, вот, идея в том, что рекомендую с ней ознакомиться, в ней содержится, наверное, все основополагающие идеи, которые мы будем
использовать и развивать в рамках наших дальнейших лекций, вот, и вот там в самом начале как раз-таки автор рассуждает о том и приводит аргументы о том, что какую роль, значит, играют теоремы сходимости, что из них можно, как их можно использовать в положительном ключе и как их не нужно использовать, чтобы не случайно не нарваться на какую-то неэффективность при решении каких-то прикладных задач. Вот, значит, сначала начнем с хороших новостей, что эти самые теоремы нам дают.
Во-первых, они нам дают класс задач, для которых применяется мета. Вот, то есть чаще всего в подобного рода теоремах фигурируют условия на то, какой должна быть функция, например, то есть должна быть, должна быть, должна ли она быть там дифференцируемой или необязательной, должна ли она быть дваждиференцируемой, должен ли быть кисян или не должен быть. В общем, все эти вопросы, они здесь обсуждаются обычно в теоремах сходимости.
И поэтому просто глядя на них, вы можете сразу понять, ага, у меня вот задача такая-то, поэтому металл такой использовать не надо, потому что он для нее, например, неприбеги, или его поведение для такого рода задач неизвестно.
Это довольно полезно.
В общем, да, какие бывают классы задач? Бывают выпуклые, гладкие, например, стандартные. То есть нужна ли выпуклость или не нужна. Если нужна, то если, условно говоря, если нужна то, что она дает хорошего, в общем, ты уже знаешь, что она хорошего дает, вот, если она не нужна, то насколько будет результат хуже, например, и будет ли, в общем, этот работать и куда он будет сходиться.
А теория могут дать качественное поведение методов. То есть они могут дать информацию о том, существенно ли та точка, с которой вы начинаете, и, собственно, по какому функционалу есть сходимость. То есть сходится ли она по функции, сходится ли она по аргументу или еще как-то.
Понятно, что несложно привести, наверное, пример функции, которая сходимость по функции, грубо говоря, будет, а по аргументу не будет.
Это там, не знаю, сейчас показать. Простой пример, типа, даже не буду, наверное, рисовать, такой гипер, который вот и бывает.
Вы запускаете метод, он там по f сходится, по x не сходится, потому что разница по f маленькая, а по x может быть достаточно большой.
Поэтому тут вот такие вот всякие странные функции. Ну, если вас смущает то, что он там неограничен, ну, то есть если что-то безкачное бывает, можно как бы делать то же самое, потом по выпустости определить, чтобы было, ну, короче, такое парабола с очень маленьким коэффициентом.
Вот, когда по f с каждым шагом разница мала, а по f мала, а по x большая. Вот, это тоже может сообщить вам теорию сходимости.
Значит, скорость сходимости тоже можно получить некоторую оценку. Вот, значит, что эта штука вам дает?
Она дает некоторую теоретическую оценку на то, сколько это раз потребуется, чтобы получить одной точность.
Ну, вот мы сегодня уже, наверное, посмотрим, как это может работать. Вот, она может дать определение факторов, которые влияют на сходимость.
То есть отчего, например, вот здесь вот, да, давайте посмотрим.
Например, если вот этот q, вот отчего зависит q? Вот q может быть 0,99999 и будет безумно долго, или q может быть там 0,1 и будет достаточно быстро.
То же самое здесь. Вот здесь, в принципе, это не так важно, потому что два степеника достаточно быстро сбивает любую там, наверное.
Гораздо быстрее, в общем, сбивает любую, достаточно близко к единице q, но тем не менее.
Вот так, факторы. Да, ну и еще иногда можно заранее вручить твой итераций, может быть оно будет немного завышенным.
Вот, но используя оценку из теоремы, можно прийти к выводу и получить результат того, насколько.
Сколько надо в общем запускать итерации, чтобы получить оценку за одной точности по заранее определенному функционалу, что важно.
Понятны ли плюсы? И то, что из теоремы сходимости можно выводить?
Да, понятно. Окей. Значит, теперь чего нельзя выводить?
Значит, если теорема говорит о том, что метод сходится, это еще ничего не говорит о том, что надо его применять или не надо, потому что он может сходиться бесконечно долго.
Об этом важно помнить. Оценки сходимости, которые с одной стороны вот здесь упоминались, чаще всего зависит от некоторых констант, которые определяются из самой функции.
И вот определение самых констант, это не самая простая задача.
Поэтому вы можете примерно оценить порядок, но дойти до абсолютных точных значений не всегда возможно.
Ну и все теоремы, они, понятное дело, работают в точной арифметике, когда у вас действительное число, все в порядке.
Но на компьютере у вас там не действительное число, у вас там плавающая точка.
Поэтому ошибки округления они зачастую не учитывают, и это может приводить к некоторым более не таким оптимистичным результатам, о которых могут говорить теоремы.
Но надо иметь в виду, что всегда есть некоторый эпсилон, который дает некоторую погрешность, который, в свою очередь, может некоторым образом портить сам процесс вычисления.
Понятно ли о чем теоремы сходимости, о чем надо помнить и беспокоиться в процессе их использования?
Да.
Окей.
Значит, теперь некоторый зоопарк методов, которые будем как-то каким-то образом классифицировать на более понятные классы, чтобы сравнивать, не сравнивать, короче говоря, там, методы из разных классов, чтобы не получилось так, что вы сравниваетесь с заранее плохим методом просто потому, что он использует меньшую информацию.
Значит, первое. У нас есть порядок метода. Нулевой порядок – это аракул возвращения только функцию. Первый порядок возвращает функцию градиан. Второй порядок возвращает еще и гися на 32-х перегонах.
Вопрос, существуют ли методы более высокого порядка, был довольно достаточно длительное время открытым.
Ну, в общем, не так давно, 2019 год, статья Юрия Евгеньевича Нестерова про то, как сделать реализуемыми. Можете знакомиться с математикой программинг, один из самых классных журналов по оптимизации.
Ну, там, правда, снова, наверное, акцент на всякую теорию про оптимизацию, но тем не менее.
В общем, это пока все в стадии разработки, то есть нет какого-то пакета, который, вроде, пока еще нет, который можно было бы запустить, как CVXPy, вот в прошлый раз мы смотрели.
Раз он бы вам все решил, не думал с какого порядка. Пока это все некоторая наука, которая еще не дошла до стадии технологии.
Вот, значит, второй аспект, по которому можно классифицировать методы, это то, сколько точек и как именно они используют историю.
То есть есть одношаговые методы, которые, ну, там, подавляющее большинство, наверное, одношаговых, вот, которые берут некоторую начальную точку, ой, не начальную, некоторую текущую точку и генерируют по ней следующую точку.
Вот есть многошаговые методы, которые генерируют следующую точку по не текущей, а еще и предыдущей.
Вот они зачастую работают лучше, чем вот такие методы, потому что просто у них больше информации о том, о том, какой была траектория и, соответственно, больше информации локальной в разных точках.
И из этой локальной информации они могут более качественным образом генерируют некоторую глобальную информацию, собственно, то, о чем я говорил в прошлый раз.
Ну и, собственно, главные выводы про это самое введение, это то, что какая у нас общая схема работы, как мы сравниваем и какие у нас есть методы и задачи, для которых мы в той или иной степени можем эти самые методы применить.
Вот сейчас потихоньку, так, есть какие-нибудь вопросы, может быть, по этой самой первой части, чтобы не сильно торопиться.
Вот если что-то непонятно, то спросите.
Так, вроде все понятно, я надеюсь, переходим тогда к градиентному, к методу спуска, в частности, градиентному спуску.
Я надеюсь, что мы сегодня благополучно закончим.
Значит, методы спуска так и методы, которые на каждую
итерацию уменьшают значение функций. Довольно очевидное
определение, исходя из названия. Спускаемся к минимуму.
Значит, если это выполнено, то направление шката
называется направлением убывания, и поначалу мы будем
как раз стремиться получать направление убывания.
Чуть позже мы от этого условия откажемся
и получим более быструю сходимость.
Как бы странным это не казалось.
Методы, которые не требуют монотонности,
оказывается, могут сходиться
более быстро.
Выглядит обычно это вот так.
Так.
Раз.
Два.
Вот.
Ну и картинка обычно выглядит вот так.
Что вот тут у вас
число итераций. Ой.
Здесь
какой-то там ошибка, например,
по функции.
И у вас один метод сходится как-нибудь вот так.
Монотонно.
А другой метод сходится как-нибудь вот так.
Не монотонно, но быстрее.
И, наверное, уже в следующий раз мы с таким методом познакомимся.
Понятно ли в чем
разница?
У красного метода мы не требуем того, чтобы накажется
итерация.
То есть разрешается некоторые подъемы, но они в итоге
оказываются.
Некоторые позитивные эффекты оказываются.
Локально мы их возрастаем, но глобально мы сходимся быстрее.
Понятно.
Окей.
Так. Возвращаемся.
Да.
И переходим, собственно, к... Да, еще пара определений.
Мы будем чаще всего использовать L-гладкие функции.
Собственно, L-гладкость нам нужна будет для того, чтобы
там некоторые оценки получать.
Значит, L-гладкая функция — функция, у которой градиент лепшится.
Тут вот дано достаточно общее определение
с сопряженной нормой. Чаще будем жить в мире,
где у нас будет есть... Есть будет вторая норма, и соответственно,
сопряженная ей тоже будет вторая норма.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Т.е., что L-гладкое — то для любых двух точек x, y выполнено все еще неравесно.
Вот.
Т.е., мы функцию в некоторые точки q можем оценить сверх
в некоторые квадратические функции для...
Ну, т.е. просто квадратичная функция,
которая в некотором смысле напоминает разложения
в ряд Тейлера до 2-го члена.
Только здесь вместо Gisiano фигурируются, собственно,
константы лепшицы. Вот. Щас потом покажем, почему это работает.
У нас есть формула унитоналибинца, может быть, не совсем привычным во виде, но я надеюсь, что ее не сложно здесь распознать.
То есть что такое? Скалярное произведение y на x это константа, а t это от 0 до 1.
То есть мы интегрируем грубо говоря градиент от x до y на самом деле.
Ну то есть замена переменной можно привести это все к интегрированию градиента от x до y, получить соответственно f от y минус f от x.
Понятно ли как это делается и почему это работает?
Или не очень? Не очень привычная запись.
Смотрите, обозначим вот эту штуку за что-нибудь, за z, например.
Тогда dz это y минус x на dt.
А это ровно то, что зайдут здесь.
Стало ли получше сейчас?
Наверное, да, понятно.
Пока не буду устраивать опрос, понятно нет, но пожалуйста более активно проявляйте себе, если есть какие-то вопросы.
Тогда можно расписать таким вот образом.
Это самая разность.
Прибавить и вычесть скалярное произведение градиента в точке x на y минус x.
Получим, то есть тут эта штука от t не зависит, можно вынести.
А эта штука тоже не зависит, но мы тем не менее оставим внутри и внесем в скалярное произведение.
Далее пользуемся тем, что у нас определение есть.
Поэтому вся эта история, которая есть f от y минус f от x минус вот это вот скалярное произведение,
оцениваем сверху как интеграл от нормы.
Интеграл от нормы меньше либо равен, ну то есть норма от скалярного произведения,
пониравясь к Шубоняковскому, меньше либо равно, чем произведение норм.
Норма разности градиентов и норма разности аргументов, то есть y, y и x.
Далее вот эта штука заменяется на l умножить на разность аргументов.
То есть аргумент здесь какие x, t на y минус x, а тут x. То есть остается только норма,
ну то есть l на норму t минус норма, еще раз, l умножить на норму t на y минус x.
Вот t выносится, вот y минус x тут еще один, поэтому появляется квадрат.
Но это все дело выносится, остается только интеграл от t по dt, что есть 1 вторая,
поэтому получаем здесь l по 1. Вот, собственно, доказательства.
Все ли шаги были понятны, что откуда берется.
Сейчас можно еще раз последнюю часть просто...
Да, давайте, смотрите. Вот есть понятно, откуда это взялось?
Да, вот до этого момента, да.
Шикарно. Вот теперь у нас, видите, здесь есть разность градиентов.
Да.
Вот она.
И мы ее сверху ограничим вот так.
То есть у нас l, дальше вот из этой штуки вычитаем x.
Получаем t на y минус x по норме.
И еще умножаем на y минус x.
Получаем t на y минус x в квадрате.
И все константы выносим, вот эти вот, которые тут подсвечены.
Остается интеграл от t по dt, понятно, на вторая.
Так, стало получше.
Да, все понятно.
Ага, шикарно.
Ну, собственно, один из критерий для l-гладкости следующий.
Функция l-гладкая, если она дважды пред дифференцируемая,
тогда для l выполнено одновременно, что функция является l-гладкой,
и норма DC на сверху ограничена самой константой для всех x.
Доказательство довольно, кажется, прямолинейное.
Пустель, гладкая функция.
Тогда для любого направления d и альфа больше нуля у нас есть вот наше определение.
Что сделаем дальше?
Дальше мы, тут хитрый предельный переход, смотрите, делим на альфу.
Да, то есть рассматриваем вот такую штуку,
которая есть не что иное, как эта штука называется.
У меня есть специальное название.
Кто помнит что-нибудь?
Хорошо, на что она похожа?
Призводной по направлению?
Вот, производной по направлению, конечно.
Значит эта штука производной по направлению d.
Мы знаем, что производной по направлению равна 무엇у скалярному произведению
градиента функции на это самое направление.
Поскольку у нас тут сама функция, то градиент и градиент это гисян,
И скалянное произведение – просто умножение гессиана на это самое направление.
Ну и вторая норма остается.
В силу непрерывности, наверное.
Да.
Понятно ли это равенство теперь?
То есть мы помним этот факт, да?
Прекрасно.
Ну вот.
А то, что мы получили здесь –
это ровно то же самое.
Следовательно, в силу вот этого
у нас выполнено, что эта штука
меньше, чем L на норму D.
Ну и поскольку это выполнено для любого D,
то по определению матричной нормы
как супревому отношению
действия матрицы на вектор делить
на норму вектора,
мы получаем, что вторая норма F
ограничена сверху от самой L.
Вторая норма
гессиана F.
Понятен ли переход?
То есть получается вы делите на норму D
и делите супревому?
Да, вот здесь.
Окей, теперь наоборот.
Пусть вторая норма гессиана меньше любого L
или любого X.
Опять с помощью формулы Newton-Levenson
в том же самом виде практически.
Только теперь у нас она применяется
для градиентов.
Разность градиентов – это интеграл
от гессиана, умноженного на Y-X.
При этом гессиан считается
на отрезке от 0 до 1 по T.
То же самое, что у нас было здесь.
Только теперь у нас не скалярное произведение,
потому что у нас не функция
а стоит градиент, а просто умножение
гессиана на вектор.
Ограничиваем эту штуку сверху,
берем норму.
И что произошло?
У нас норма здесь
меньше либо не интеграл от нормы.
Интеграл от нормы
почему стоит?
Произведение матрицы на вектор.
Соответственно,
норма этого произведения меньше
либо она, чем произведение норм.
Норма Y-X выносится.
Под интегралом остается
норма гессиана.
Поскольку гессиана в любой точке меньше
либо на L,
мы получаем единицу,
получаем L на норму
разности Y-X.
Получаем определение
L гладкой функции.
Тут даже попроще вот в эту сторону.
То есть тут три хода,
тут всего два.
Но этот вход, наверное, не самый очевидный.
Вот дальше уже попроще.
Все ли понятно?
Да.
Шикарно.
Да, ну и, собственно, гридентный спуск
очень легко ему получить,
используя нашу лему о спуске.
Не зря же она так называется.
Значит, у нас была F от Y,
мы ее оценили сверху.
И эта функция же от Y.
Мы можем взять,
ну да, и, собственно, вот это работа,
то, что было следствием
следствия того,
что было казна ранее,
того, что максимальное значение
для гессиана меньше уровня L,
потому что, собственно, вот это, по сути, определение.
Ну, для симметричной матрицы, конечно,
поэтому будет лямбда максимальная.
Не симметричная у меня немножко другая история,
но у нас для гессиана все симметрично.
Поэтому берем, просто минимизируем
вот эту квадратичную оценку
сверху по Y.
То есть минимизируем просто оценку
сверху на нашу функцию.
Поскольку эта штука выпукла,
то достаточно общать градиент.
Считаем градиент в точке Y со звездочкой.
Остается,
из линейного члена остается
градиент.
Отсюда остается L на Y минус
XК.
Отсюда получаем, что Y со звездочкой
это XК минус единица на L
F штрих на XК.
Это наша новая точка XК плюс 1.
Отсюда получили градиентный спуск
с шагом единицы на L.
Сейчас еще будем понимать,
почему этот шаг так хорош
и что будет, если его пытаться изменить.
Понятен ли вывод?
Он очень простой, опираясь на то,
что мы минимизируем не саму функцию,
а квадратичную оценку сверху на нее.
Градиент, почему такой? Понятно, да?
Почему у нас что-то со знаком,
где это нет?
У нас Y штрих минус XК
со знаком...
А, нет, все правильно.
Это хорошо, что все правильно.
Я расстроился, что
градиентный спуск сломался.
Как шаг выбирать?
Еще пока мы не пришли к тому,
что его надо выбирать.
Эта штука единицы на L
это некоторый шаг,
который соответствует...
Хороший важный момент.
Эта штука в точности,
то, что у нас было в самом начале,
направление минус градиент,
шаг альфа К равен единиц на L.
В точности подпадает
под все наши изначальные построения.
Как шаг выбирать?
Можно взять постоянно меньше, чем 2 на L.
Почему надо брать меньше, чем 2 на L,
через несколько слайдов.
Можно брать убывающую последовательность,
расходящуюся, 1 на K и так далее.
Есть куча разных способов адаптивного подбора
для того, чтобы функция убывала.
Но есть нескорейший спуск,
который решает дополнительную задачу
по минимизации функции
в новой точке, которая зависит от альфы.
Тут надо будет мне добавить что-то такое.
Это простая формула, которую несложно добавить.
Сейчас я ее напишу.
Я думаю, что из названия
она плюс-минус очевидна.
Мы выбрали уже направление.
Наше направление hk это минус
f' от xk.
Нам нужно найти альфа со звездочки.
Такой что функция
в новой точке.
А новая точка это что такое?
Минус альфа
на f'
от xk.
Будет минимально при не отрицательном альфе.
Эта функция
это уже скалярная функция.
Это некоторые же от альфы,
которые из rvr.
Потому что все остальные компоненты уже заданы.
Поэтому
можно одномерно минимизации какую-то
пытаться искать этот минимум.
В общем,
печальная новость заключается в том,
что, во-первых, вы тратите кучу времени
на то, чтобы найти эту альфу,
и вторая печальная новость в том,
что с синтетической скорой сходимости у вас не улучшится.
Там еще про это будут некоторые комментарии.
То есть по времени, может быть, вы что-то и выиграете,
но, условно, из линейности
получить какую-то тратичную сходимость
у вас не получится.
Понятно ли, что такое нескорейший спуск?
То есть правило выбора шага
под названием наискорейший спуск?
Да.
Окей.
Хорошо.
Да.
В общем, это я уже поговорил.
Теперь по сходимости.
Посмотрим
на то, как
наш метод
будет сходиться.
Как это сделать?
Посмотрим, что происходит
в точке xk плюс 1.
f от xk,
f'
xk плюс 1 минус xk
просто подставили вот сюда.
Подставили сюда
xk плюс 1.
Мы знаем, что у нас xk плюс 1
было получено с помощью градиентного спуска,
поэтому разница
xk плюс 1 минус xk, это что такое?
xk минус 1 минус xk
это минус
альфа на градиент.
Подставляем,
получаем здесь минус альфа
на норму градиента в квадрате.
Здесь получается
l альфа в квадрат нормы градиента в квадрате.
Понятно,
вот это равенство.
Да.
Шикарно.
Вносим благополучно
квадрат нормы градиента из вот этого
слагаемого и вот этого слагаемого.
Получаем вот такое выражение.
Теперь смотрим,
что произошло.
Произошло то, что мы из
некоторого нашего значения текущего функции f от xk
вычитаем что-то,
некоторый коэффициент, умножить на строго
отрицательное число, ну положительно,
потому что если эта штука ноль, мы уже в решении.
Ну, в решении или там,
где-то в сценарной точке, например.
Поэтому чтобы у нас было убывание,
надо потребовать, чтобы вот этот коэффициент был больше 0,
чтобы мы что-то вычитали,
чтобы наш значение функции уменьшалось.
Ну, из вот этого неравенства очевидно следует,
что alpha k
не должен превышать 2,2
делит на это.
Если она эту штуку превышает,
то у нас условие убывания нарушается.
Поэтому у нас есть вот такая вот оценка
на постоянный шаг.
Далее.
Если мы промаксимизируем эту штуку,
то есть мы пытаемся найти такой коэффициент,
такой шаг, чтобы убывание
было как можно более сильным,
то мы получим в точность
единицу на l, вы не поверите.
То есть ровно то, что у нас было вот здесь.
Ну и после
подстановки единицы на l вот сюда,
мы что получим?
Мы получим вот такую вот оценку
на f от x
минус f от x k плюс 1,
как единица на 2 l
на вторую норму градиента в текущей точке.
Это нам еще сейчас понадобится,
для того чтобы вывести некоторые дополнительные факты.
Соответственно, если мы теперь
вот в этом неразе
просуммируем слева и справа
по k от 0 до t,
то здесь будет телескопическая сумма,
из которой останется только
f от x 0 и f от x
на t плюс первой
итерации, а здесь останется
единица на 2 l сумма всех норм.
Ну как бы,
если функция ограничена снизу,
то вот эта штука меньше,
больше, чем f со звездочкой.
Поэтому это все меньше, чем
f от x 0 минус f со звездочкой.
Это значит, что
при тесте стремящейся к бесконечности
эта штука сходится.
Да, ряд сходится.
Если ряд сходится, значит
его n-ый член
стремится к нулю.
Необходимые условия сходимости рядов.
Поэтому
норма градиента будет стремиться к нулю
при
номере
в номере...
Короче, приказ стремиться
к плюс бесконечности текает номер итерации.
Да. Отсюда
мы получаем, что градиентный спуск
всегда сойдется к сценарной точке.
Причем сценарная точка
будет еще и той, в которой
последовательно мы будем уменьшать значение
функции при стремлении к ней.
Понятен ли вывод?
Понятен ли вывод
как вывод и как процедура его получения?
Вроде да.
Хорошо.
Вот.
Это первый результат.
Если вы запустите метод
для функций x-куб,
то он сойдется к нулю благополучно.
При этом
в нуле не минимум.
Теперь
более
интересная
и информативная теорема
говорит нам о том, что...
Сейчас я посмотрю на план.
Сейчас я про теоремы проговорю,
будет 5 минут вперед.
О том, что
если функция выпукла,
градиент липшицов и шаг 1 на L,
то сходимость градиентного спуска
она сублинейная.
То есть сходится довольно медленно
для просто выпуклой функции
с липшицем градиента.
Доказательств этой теоремы, к сожалению,
я здесь не привожу,
потому что оно довольно неприятное.
И обозначений довольно много.
Я понял, что в процессе подготовки
лучше показать результат,
чем утопить вас в процедуре
его получения.
Для просто выпуклых функций такая история.
Для сильного выпуклых функций история
более простая
и более быстрая.
Через 5 минут давайте там...
В 37 давайте продолжим.
Посмотрим, что будет происходить
для сильно выпуклых функций.
Все, опять наперев.
Продолжаем разговор.
Сильная выпуклая функция.
Напоминаю, что у нас было
следствие сильной выпуклости
о том, что
если функция сильной выпуклой,
то ее можно снизу оценить тоже параболой.
То есть тут мы
в случае просто L-гладкой
оценивали сверху параболой.
В случае сильной выпуклой можем оценить снизу параболой.
Причем коэффициент
у этой параболы будет
постоянно сильной выпуклости mu.
Тогда
мы можем минимизировать
в обе части позад.
То есть до этого мы минимизировали
оценку сверху,
получили градиентный спуск,
а сейчас будем минимизировать
одновременно и левую правую часть,
то есть оценку снизу.
Что мы получим?
Здесь мы получим f-адыка со звездочкой,
понятное дело.
А здесь, проделав все те же
самыми манипуляциями,
мы получим
вот такое выражение.
То есть у нас было вот здесь
единица на 2L,
а тут станет единица на 2 mu.
Тогда норма градиента
оценивается сверху как
2 mu умножить на
разность между f-адыка
и f со звездочкой.
Понятен ли первый шаг
к тому, чтобы получить нужный нам результат?
Вообще не очень понятна
минимизация.
Не очень понятна минимизация.
Ну, смотрите, мы до
этого
смотрели на то... То есть мы хотели
получить, найти
минимум этой функции,
и мы ее сверху оцениваем,
находясь в текущей
точке.
Давайте лучше я нарисую картинку.
Наверное, это будет полегче.
Ну, то есть
было что-то,
ну, какая-то график там,
была какая-то там
странная выпуклая
функция.
Давайте кривовато получилось.
Несимметрично
я нарисовал.
И вот мы находимся
в текущей точке.
Вот наш xk.
И мы вот,
будучи в этой точке, вот эту красную
функцию можем оценить
глобально сверху некоторые параболы.
Вот так.
Это в одном случае.
Это если просто была l
гладкая.
Если же она у нас
не сильно выпукла,
то мы в этой же точке можем оценить ее
другой параболой.
Ну, какой-нибудь вот такой.
Ну, как-то вот так.
Не очень
художеством.
Важно следующее, что
в случае просто
l гладкости мы находим минимум
вот этой вот синенькой
штуки и получаем вот здесь вот условно xk
плюс один.
Вот. А в случае
нет, собственно, даже гридентной спуск.
А в случае зеленой мы просто
ищем минимум и смотрим,
насколько мы долетим
в данной точке
от минимума.
Насколько минимум нашей исходной функции больше,
чем минимум ее оценки снизу.
То есть, грубо говоря, минимум
у зеленой условно где-нибудь здесь,
а минимум у красной он
где-то тут.
И мы смотрим вот этот вот зазор.
Сейчас. И пытаемся оценить,
насколько зазор велик
с помощью знания нормы гридиента
в текущей точке.
Вот. Это нужно будет нам
сейчас увидеть, зачем.
То есть умение оценивать
и сверху, и снизу является ключевым
для получения оценок исходимости
для синевых функций.
Да. Получили норму гридиента.
Насколько она велика.
Окей. Теперь у нас есть информация
о том, что наш шаг
это единица на L.
Поэтому у нас есть оценка сверху.
Ну, собственно, вот это
переписано то, что мы получили
вот здесь.
Только на минус один умножили,
короче, преобразовали.
Что f со звездочкой меньше
лимбровной, чем f от x ка плюс один,
меньше, чем f от x минус в общем.
Вот.
То есть это просто по определению.
Далее. Получаем
линию исходимости. Каким образом мы это будем делать?
Тут вот, да,
надо будет что-то дописать еще, чтобы было совсем очевидно.
Смотрите.
Нам нужно оценить сверху f от x
ка плюс один минус f со звездочкой.
Значит, f от x ка плюс один
минус f со звездочкой, это
что это есть такое?
Это в точности вот то, что у нас
стоит вот здесь.
В общем,
из этого неравенства
вычли f от x
со звездочкой.
Получили слева f от x ка плюс один
минус f со звездочкой,
а тут у нас f от x ка минус f со звездочкой.
А мы знаем, что у нас f от x
минус f со звездочкой
меньше, чем норма градиента
делить на 2 на мил.
Вот.
Таким образом,
мы получили
вот.
Таким образом, с одной стороны...
А, это же не так.
Тут у нас f от x минус f со звездочкой остается,
а норма градиента заменяется
на выражение, связанное с
вот этим вот множителем.
И получается, что
если вынести вот этот общий множитель,
то получится коэффициент равный
едице минус едице на каппо,
где каппо это l на мю.
Вот 2х кратятся,
которые здесь.
Понятно ли, что произошло,
или лучше расписать более подробно?
Честно говоря, получается, вы
действительно... Ну да, правильно ли,
что вы просто вычислили f со звездочкой,
а потом как бы вы читаете
меньше, потому что неравенство
то, что норма градиента
больше, чем вот это.
Так, я понял, что сейчас надо лучше все это
переписать. Сейчас секундочку, я это покажу
на доске. Да, тут я
не докрутил чуть-чуть слайд,
но, в общем, сейчас все будет.
Так, минус единица
на 2l, норма градиента,
да.
Так, норма градиента
у нас больше, поэтому
сейчас у нас есть 1 и 2.
Значит, чтобы получиться
нку сверху,
нам надо вычитать
нам надо вычитать
что-то меньшее, да?
Да, у вас как раз неравенство,
что эта штука меньше.
Так, сейчас я пойму, секунду.
Что сломалось?
Ага.
Так, а что у меня есть? У меня есть то, что
норма градиента
больше, чем 2μ
на f от xk,
fk, минусов со звездочкой, да?
И мне нужно
теперь получить
нку сверху на эту самую разность.
Так, чтобы у нее получить,
надо сделать так, чтобы там был
чтобы там был минус.
Чтобы там был минус.
Да, возможно, здесь какие-то проблемы
со знакомыми в некотором месте возникли.
Все же нормально, у вас
норма градиента больше, чем какое-то
число. А вы норму
как бы вычитаете. Значит, если вы будете
вычитать меньшее число, вы получите
оценку сверху.
Да, но я же вроде как больше вычитаю,
или нет? Нет, вы наоборот, ну как бы
здесь вы читаете больше, да?
А, я вычитаю меньшее, да-да-да-да, я вычитаю уже
2μ. Все, спасибо.
Да, что-то я запутался. Да, то есть
что-то получается, что...
Да, сейчас надо показать картинку.
Да, я градиента же
отказываюсь, наоборот.
Поэтому все в порядке.
Да, то есть было вот так
вычли, а потом
вычитаем меньшее
вот это вот равное 2μ на разность
f. И получаем, что
давайте я вот это вот перенесу лучше вот
сюда, так эту рамочку надо ввести.
Вот, а тут
меньше либо равно, чем
fk-то минус f со звездочкой,
минус
2μ на 2l
на fk
минус f со звездочкой.
Да, ну вот оно и получается.
1 минус 1
на kappa на
fk-то минус f со звездочкой.
Вот, kappa чудо
l на μ.
Вот, понятно, наверное, что
kappa это
то есть kappa это
некоторое число, которое лежит в интервале
от 1 до плюс бесконечности.
Вот, если плюс бесконечности, если оно
очень большое,
kappa растет.
В общем,
чем больше kappa,
я неправильно говорю,
чем меньше
μ, то есть μ меньше,
следовательно, kappa больше.
Поэтому чем наша функция
будет сказать,
все хуже
и хуже,
то есть это значит, что у нее константа
7,5, чем меньше и меньше,
тем обусловленность выше
и тем медленнее будет
сходимость. То есть
отсюда же что следует, что
fk-то минус f со звездочкой
поможет, да,
меньше либо равно, чем 1
минус 1 на k по степени k
на f0
минус 0 минус f со звездочкой.
То есть вот мы получили
линейную сходимость
с коэффициентом, который равен
единице минус единице
на k. Отсюда
если kappa очень большое,
то эта штука стремится к единице,
что очень грустно.
При этом, если kappa
равно единице,
то мы сойдем из одной итерации.
Понятен ли этот пример?
Ну, не пример, а как бы интерпретация.
Очень хорошо.
Так.
Да, линейную сходимость получили. Отлично.
Теперь
некоторая теорема
тоже про сильную клей-функцию,
которая может быть чуть более общей
связывать как раз таки
по функции с...
А, тут уже все аккуратно. Видите, тут со звездочкой стоит.
То есть, по сути,
некоторая переформулировка того, что мы
уже использовали, только с учетом того,
что шаг еще дополнительно подбирается так,
чтобы с учетом константа есть сильный выпуск.
Мы пока что подбирали шаг
равный единице на l, никак не
использовали mu. А тут mu используют,
получают немножко другой коэффициент,
но зависимость от kappa
остается прежней.
То есть здесь, по сути, kappa минус 1 на kappa плюс 1.
И это все работает.
Значит, теперь
самое интересное, что
на эту самую линию сходимость влияет.
На нее влияет
значение для куса звездочки,
который мы...
Вот это куса звездочки у нас.
Или вот здесь, это просто вот эта величина.
Выражаясь через kappa,
числа бусловности для матрицы
это отношение максимального
к минимальному симметричному,
то это отношение максимального
к минимальному собственному значению,
поэтому здесь очень прямая аналогия
с
в общем,
с бусловностью
гисианы и константами
сильной выпусти и липшести градиента,
которые у нас есть в функции.
Соответственно, при kappa очень больших,
да, в общем, я это уже в сути показал,
у нас есть тремление к единице и очень медленно сходим.
То есть если взять kappa равное 100,
то мы получим уже q равное
0.98, что будет
безумно долго сходить. Если там kappa порядка единицы,
ну, например,
для 0.6,
для 4 получим q равное 0.6,
что достаточно быстро
приведет нас к
достаточно маленькому значению.
Геометрию этого требования,
тут я, да, тут я, наверное,
надо написать картинку,
геометрию этого требования очень простая,
мы просто хотим, чтобы,
да, можно сейчас, например,
быстренько нарисовать
довольно простой.
Мы хотим, чтобы наша функция,
например, если в 2D мы живем,
она была, как можно более,
ее линии и уровни были как можно более сферическими.
То есть
в условиях близка к единице
линии и уровня вот так выглядит,
kappa порядка единицы,
kappa существенно больше
единицы линии и уровня выглядит вот так.
Вот, и поэтому если мы, например,
стартуем отсюда, то мы,
давайте я нарисую, будем вот так вот
зигзагообразно сходиться очень-очень
долго. Вот, а здесь мы за одну
итерацию придем к решению.
Ну и простой пример, когда
это легко увидеть, это функция там, не знаю,
x-транспонированная x,
то есть если у нас функция f от
x,
одна вторая x-транспонированная
x,
давайте без одной и второй даже.
Вот.
Тогда градиент
f от x это 2x,
гисиан,
ой,
гисиан
f2 штиха от
x это
две единичные матрицы,
отсюда
кастанта липшица равна 2.
Вот.
И тогда возьмем у них 3x0,
тогда x1 у нас, ну понятно,
что f2 штиха равна 0.
Вот.
Тогда x1 это x0 минус
единица на l
умножить на 2x0
равно 0.
Что не так?
А, да, mu тоже
равно 2. Вот.
Все-таки kappa равно единицу.
Понятно, почему все это работает
в этом примере или не очень?
Или я слишком быстро все
рассказал?
Да, можно еще раз как вы определили
l, mu?
Да, смотрите. Ну, l как определим l?
Мы уже знаем, что у нас l это
константа липшица для градиента,
слишком долго не буду писать,
это равносильно тому, что у нас
норма ограничена
сверху.
Это у нас был такой критерий, помните?
Оказывали?
Да.
Вот. Наш
десиан в этом случае это 2 и 2 единичные матрицы.
Норма равна 2.
Понятно, почему?
Да. Прекрасно.
Вот константа l равно 2.
Да, хорошо, а mu?
Вот. Теперь про mu давайте.
Mu это...
То есть, поскольку у нас про mu есть
оценка снизу,
я, конечно, это сегодня не доказываю, но
давайте сейчас я на пальцах объясню.
Про mu у нас есть вот это.
Вот. А это значит,
что, короче говоря,
проводя все те же самые
рассуждения, которые были проведены вот здесь,
можно показать,
что
константа mu будет...
Константа сильной выпуски для дважды и дни
дифференцируемой функции,
это равносильно тому, что вторая норма снизу
ограничена mu.
То есть, вот тут те же самые
рассуждения будут работать.
Про оценку сверху квадратичную.
Ой, сори, не так.
Ну, в общем...
Сейчас я неправильно
объясняю, секунду.
Короче говоря, вот
поскольку у нас
есть вот это,
тут надо поаккуратнее, то это значит,
что мы в каком-то смысле взяли
и оценили
норму нашего
гисяна, который стоял бы...
квадратичная форма, с которым стояла бы тут,
она
заменили mu умножить на единичную матрицу.
А раз мы это сделали,
то мы и подразумеваем, что
норма нашего гисяна больше
либо равна mu.
То есть, мы как бы снизу подперли
эту норму.
Ну, а мы уже получили,
что у нас норма просто равна 2.
То есть, там для любого х на 1 этаже.
Поэтому mu тоже будет
равно 2.
Вот.
Ну, то есть, как бы
вот эта вот картинка,
она по сути говорит
о том, в каких пределах
может варьироваться
кривизна нашей функции
исходной. То есть, она ограничена
сверху l и ограничена
снизу mu, если
функция является сильной
выпухой.
Насколько убедителен
такие объяснения?
Я в следующий раз
что-то поподробнее расскажу.
Наверное, то, что
вы просто ограничиваете кривизну,
это нормально.
Нормально, да? Ну, хорошо, да.
Просто можно все это формально
расписывать аккуратно и
строже, но это типа еще там
15. Вот. Поэтому
mu равно тоже 2.
Консонта липчется равна 1,
и мы за одну литерацию сходимся.
Вот. Теперь
если, например,
рассмотрим функцию вида
x-транспонированная qx,
то и, например, в случае
если n равно 2,
матрица q, например, будет иметь
диагональный вид 1,100.
Все будет гораздо
хуже.
Понятно, что
ответ все еще в 0.
Понятно, я надеюсь, да?
Да.
Да. Хорошо.
Вот. Но тут уже так
еще дело не пройдет.
l у нас здесь какой будет?
l будет равно 100,
по-моему, да?
Ну да,
lambda max для q
это 100.
Ну а lambda min
равно 1.
Поэтому, когда мы будем делать
1 на l, у нас будет
1,100.
Вот.
Ну и наш
благополучный x1 будет равен
x0 минус
две
ой, две
там градиент
два qx.
Две сотых
qx 0.
Соответственно,
это равно
денечной матрице
минус две сотых q
на x 0.
Вот.
Ну и понятно, что тут довольно долго
надо будет страдать
тем, чтобы прийти к 0.
Потому что
можно оценить, как это будет выглядеть,
на самом деле, что
1 минус
две сотых
и что?
Минус 1 на x 0.
Минус 1, что ли?
Да.
Тут уже что-то странное происходит.
Точно две сотых?
Да, вроде две сотых.
А, нет.
И правильно.
Два q же, поэтому
200 и 2.
Да, поэтому
будет 1 2 сотый,
тут будет 1 сотый. Да, все правильно было.
1 сотый, тут 1 сотый
и
единичный минус
ноль.
Окей.
Пусть будет...
Пусть будет...
Тут пусть будет ноль, да,
а тут будет что-то странное.
И это что-то странное будет.
То есть, по одной компоненте мы к 0 сойдемся,
а по другой не сойдемся.
И дальше будут
мучительные попытки сойтись
и по первой компоненте тоже.
Эта штука будет довольно долго сходиться,
сценировать к 0.
Это к тому, что большая обусловенность
приводит к очень медленной сходимости.
Что я дальше хотел сказать?
Дальше важный момент.
Про геометрию я рассказал.
Понятно про картинки,
что это было?
Почему что ломается?
Вроде да.
Хорошо.
Дальше возникает вопрос,
можно ли мы
сходиться лучше?
Мы знаем, что для уплакывающих функций
липшим и градиентам, градиенты спускаются
как единица нога,
а для сильных функций с липшим и градиентам
сходит слияние скорости сходимости
такого вида.
Вопрос не праздный.
Можно ли сходиться быстрее?
И что значит быстрее?
И как это определить?
Допустим, мы предъявим метод,
который сходится быстрее.
Вопрос, как понять, что лучше сойтись
для данного класса функций нельзя?
Но этот вопрос отвечает
на такая история, концепция некоторая,
название в нижней оценке сходимости.
Ее с первого раза, мне кажется,
понять не очень просто.
Давайте я попробую объяснить так,
чтобы было
плюс-минус
ясно, зачем это надо,
и какая конструкция для этого строится.
У нас есть два класса
функций,
и в каждом из них
мы можем найти
такие функции,
что выполнены следующей оценке снизу.
Для выпуклых функций
с липшим и градиентом вот так вот,
а для сильно выпуклых функций вот так вот.
При ограничении
на класс методов такой,
что каждая следующая точка
это x0 плюс некоторые
линии и комбинации градиентов,
полученных во всех предыдущих точках.
Что предлагается сделать?
У нас есть семейство методов,
и у нас есть класс функций.
Мы в каждом из этих классов
функций строим некоторым
хитрым образом такую функцию,
что какой бы метод
из вот этого семейства
мы не взяли,
у нас получится, что
любой метод из этого семейства
на этой функции
будет сходиться не лучше,
чем вот так.
То есть если, например,
для градиентного спуска мы находили
оценки сверху,
то тут мы наоборот находим оценку снизу,
чтобы показать, что
лучше, чем вот так,
живя в данном классе методов,
сойтись на данном классе функций нельзя.
Соответственно, методы,
которые достигают этих нижних оценок,
называются оптимальными.
В том плане, что
в смысле теоретических оценок
сойтись быстрее
на заданном классе задач
в заданном классе методов
нельзя, потому что вот оценка.
Понятно ли,
что произошло?
Нет, не очень.
Давайте я тогда попробую еще картинку нарисовать.
Может быть, после картинки станет полегче.
Вот, смотрите.
Так.
Сейчас, погоди, давайте я нарисую картинку сначала.
Сначала нарисую картинку,
потом покажу еще раз формулу.
Значит, вот у нас есть
некоторый класс функций.
Назовем его...
Зовем его f,
фигурной такой вот.
И есть класс методов m,
фигурной.
Это функция.
Это метод.
И это наш типичный график
в зависимости
некоторой метрики от числа и террации.
Так, модуля
вообще тут не нужна.
Вот.
Что мы только что
получили для градиентного спуска?
Ну, например, мы получили, что
взяли какую-то функцию
из...
допустим, рассматривали
класс выпуклых функций.
И мы получили, что вот
так сходится.
Вот.
Значит, вопрос. Почему
нельзя сходить, например, вот так?
Или как-нибудь вот так?
Или
как-нибудь...
Так, сейчас я другой цвет выберу, секунду.
Или как-нибудь, может быть, вот так.
Может быть, существуют методы
каждой из этих линий.
То есть,
вот это,
вот это
и
вот это.
Это все.
Так, нужно еще один цвет.
Вот так.
Это все разные методы.
Возможные.
Или невозможные. Не знаю пока.
Так, понятна гипотеза?
Дайте по кусочкам двигаться.
Да.
Да, хорошо. Значит, что предлагается
сделать? Давайте теперь найдем
некоторую функцию.
Пусть существует некоторая функция
там G со звездочкой,
которая лежит в нашем классе.
Вот. И для этой функции
мы получим, что
вот на этом же графике, который я, видимо,
заново перепишу...
А, давайте не заново. Давайте просто
еще один цвет
какой-нибудь. Давайте вот такой.
Вот для этой функции.
Сейчас ее надо как-нибудь... Вот пусть она будет
вот такой вот.
Да. Вот для нее вот этот
цвет.
Там как-то можно
красиво его заштриховать,
но я не умею.
Ладно.
Вот.
Для этой функции мы знаем, что
оценка снизу имеет вот такой вид.
То есть,
надо... То есть, еще раз.
Мы зафиксировали функцию
и взяли
любой m
из методов.
И любой метод взяли.
Вот. И показали, что
все, что мы можем добиться, лежит только вот здесь.
То есть, линии могут идти
где-то выше. Быстрее
нельзя.
То есть, чтобы это показать, надо было
ровно показать, что уже со звездочки там
x, k-t-g,
ой, звездочка
плохое название тут, да, наверное.
Давайте g с волной.
Минус g со звездочки больше
либо равно, блаблабла. Чего-то так.
Для любого метода.
Какой бы
мы ни взяли из этого семейства.
Стало ли
полегче?
В общем, да. Плюс непонятно,
как вы эти оценки берете.
Да, это
хороший вопрос.
Там просто
эти функции строятся
и показываются
две страницы выкладок.
Ну хорошо, давайте
я в следующий раз, когда сейчас уже конец должен
пройти лекции, попробую
вкратце хотя бы объяснить, откуда они берутся
и каким образом строятся эти самые оценки.
Возможно, тогда
сейчас есть вопрос, то надо его обязательно
удовлетворить.
Про то, почему вот этот знак
начинает работать.
Почему больше либо равно, почему не меньше.
Значит,
в итоге мы имеем
пока что
умозрительные некоторые величины.
1 на k квадрат и
линейную сходимость вида
с коэффициентом, в котором
входит корень искапа.
И в следующий раз мы, во-первых,
начнем с того, что посмотрим, собственно, откуда
это взялось.
На самом деле, самое
неприятное вот здесь,
а вот получить вот это уже не так сложно
из вот этого. Там они плюс-минус
похожи функции.
И предъявим методы, которые
эти нижние оценки достигают.
После чего мы скажем, окей, вот методы,
которые в теории, согласно
теорооценкам, быстрее них сходиться
нельзя на данном классе задач.
Это метод спрошен ингредиентов
с кратичной положительно определенной матрицы.
Метод тяжелого шарика
для сильного пылы функции,
и, собственно, градиентный метод кестерова, который
для призвольного пылы функции достигает
вот этой оценки.
Вот это план на следующий раз, плюс
то, что я сказал про вывод.
В итоге
19.09, идеальный тайминг.
Мы сегодня
обсудили скорость сходимости,
градиентный спуск, его
свойства и сходимость.
Немножко начали обсуждать нижние оценки
и то, какой вывод
и какой посыл
они могут, они несут
и какой профит могут дать
для
анализа и сравнения метод.
Есть еще какие-то вопросы, кроме того,
откуда взялись вот эти
вот странные штуки.
Про него поговорим в следующий раз.
Так, ну, тишина.
Надеюсь, что всем все понятно.
Давайте закончим.
И в следующий раз, видимо, снова будет все в зуме,
я так понимаю.
На более-менее постоянном режиме.
Так что ссылку я пришлю,
как обычно.
Надеюсь, что
пояснение про вывод
этих оценок будет
данным.
Все тогда. Спасибо за внимание.
До следующего раза.
Ссылочку на видео я, наверное,
пришлю заинтересованным.
Уже спрашиваю про запись.
Я выложен, наверное, просто в чат.
И
будет возможность у ваших
коллег посмотреть.
Все. Все спасибо.
И до следующего раза. До свидания.

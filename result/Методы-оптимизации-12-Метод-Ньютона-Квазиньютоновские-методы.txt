Сегодня у нас лекция про метод Ньютона, первая часть,
и потом успеем поговорим про Юнтонский метод.
Мы по-прежнему пока что живем в мире, где есть только
безусловные задачи, в следующей неделе будем смотреть
на условные задачи, и таким образом там еще 2-3, скорее
наверное 3, чем 2, лекции.
Мы посвятим тому, что происходит, когда у вас есть какие-то
ограничения, выбирать метод в зависимости от вида
этих ограничений, и что есть с промежуточного между
солверами общего вида, которые решают предвольные, выпуклые,
ну не предвольные, задачи выпуклого, оптимизации
в конической форме с некоторыми предопределенными конусами,
и некоторые выпуклые задачи, у которых структура ограничений
чуть проще, чем что-то совсем произвольное.
Пока живем в таком мире, почему нам будет важно
посмотреть на метод Ньютона именно сейчас, и почему он
еще важен для решения задач общего вида, мы тоже,
я думаю, через неделю, через две.
На следующей лекции будет метод проекции градиента,
проэксимальный метод, метод Франко Вульф, также известный
как метод солвого градиента, и соответственно на следующей
очередь, неделю, уже я надеюсь поговорить про методы внутренней
точки, и то, как они работают в контексте именно задач
общего вида с коническими ограничениями.
Ну и последнее будет про солверы, пакеты, то, как это
все реализовано в практических каких-то штуках.
Итак, метод Ньютона, что происходит?
Все у нас атерационно, x0 переходит в x1, переходит в x2.
Правила этих переходов мы будем строить, основываясь
на том, что мы вместо исходной задачи минимизируем вот
такую квадратичную оппроксимацию исходной функции.
Я бы даже сказал вот такую.
Плюс одна вторая, транспонированная, гессиан появляется на h.
Вот минимум по h.
Вот таким образом мы получаем наше направление.
Эта функция является выпуклой, и условие, что гессиан у
нас положительно определен, понятно ли этот момент,
или нужно пояснить.
Если понятно, поставьте, пожалуйста, в чате плюс.
Один вижу, второй, все вот это установили, прекрасно.
Ну и соответственно условие первого порядка на поиск
минимума будет записываться вот так, просто градиент
в нашей точке h будет ноль, и поэтому надо будет наше
направление h со звездочкой, это решение линейной системы.
Вот.
Важно понимать, что условно говоря, вызов функции, которые
решает линейную систему, не тождественен вызову
функции вида, ну давайте, раз еще я начал Питоновской
нодации писать.
Сейчас, секунду, вот так, умножить на там минус, да,
какой-то.
Минус f' на x.
Вот это разные вещи с точки зрения численноустойчивости
и алгоритмов, которые используются.
Поэтому вот то, что обведено желтеньким, оно предпочтительнее.
Вот это лучше, чем то, что вычисляется через NP-Linal
Inf.
Вот, потому что вычисление обратной матрицы, в принципе,
очень устойчивая операция с точки, ну вот, в арифметике
с плавающей точкой.
Вот, поэтому, в общем, так делать не надо.
Good.
Значит, в итоге наш метод будет записываться таким
вот образом.
xкт-, я напишу сейчас аналитическое решение, но все, что касается
его численно реализации уже выше было сказано.
Это называется мет Ньютона.
То есть понятно, что его можно применять, только
когда вы знаете, что у вас в любой точке есть гисян,
потому что иначе будет что-то странное.
Вот.
Значит, что про него можно сказать?
Ну, уже глядя на эту штуку, свойства какие.
Первое, то требует N квадрат памяти.
Второе, требует N куб времени на итерацию.
При этом мы помним, что все методы, которые оперировали
только градиентами, они были тому от N.
На одну итерацию просто надо было сложить что-то.
Сложить какие-то, ну, линейные комбинаты векторов какие-нибудь,
тут, наверное, очень кривые цифры, ничего не понятно.
Это N квадрат, потому что хранение гисяна.
То есть видно, что в целом метод гораздо более ресурсно
затратен, нежели метод первого порядка, с которым мы обсуждали ранее.
Понятно ли утверждение, понятно ли, почему так происходит?
Можно ли идти дальше?
Вполне.
Прекрасно.
Вот.
Значит, раз мы усложнили себе жизнь на каждой итерации,
то, наверное, хочется что-то получить взамен.
И вот взамен мы получаем некоторые более продвинутые резорты по сходимости.
Сходимость, как обычно.
Тут будет два пункта.
Первый пункт будет нам говорить, что в целом она сверхлинейная,
а второй пункт будет говорить, что если мы там еще кое-что докрутим,
то она станет квадратичной.
Вот.
Так, сейчас.
Бла-бла-бла.
Наверное, так не надо делать.
Секунду.
Что можно сказать про эту самую сверхлинейную сходимость?
Вот, ну, давайте я так сделаю.
Пусть у нас х-звездочка – это локальный минимум.
Вот.
Ну, мы знаем тогда, что в нем там градиент ноль,
и гессиан, ну, у нас, видимо, положить-то придет.
Вот.
Можем вокруг нуля градиента представить его,
разложить рядом Тейлора относительно некоторые точки хк.
Ну да.
Собственно, гессиан на что?
На х-звездочка минус хк.
Плюс или мало от нормы.
Вот.
Ну, в общем, понятное.
Понятное преобразование.
То есть, мы находим точки хк.
Хотим смотреть, что у нас будет в х-звездочке.
Мы знаем значение градиента.
Равенство точное, потому что есть добавочные.
Добавочная слагаемая умалая.
Вот.
Восответственно, что у нас из этого будет следовать?
Можем это все умножить.
Поскольку у нас вот это вот все выполнено,
то мы можем обе части умножить на обратный гессиан формально.
Ну, тут все объекты, которые у нас тут используются,
они существуют.
Поэтому, в общем, можно с ними спокойно оперировать.
Вот.
Давайте я подробно попытаюсь расписать.
f' хк.
Тут будет плюс х-звездочка минус хк.
То же самое малое, потому что норма обратного гессиана
также будет ограничена, поскольку, ну,
может быть, она будет ограничена какой-то большой константой
в случае каких-то проблем с обусловленностью.
Но, тем не менее, ничего выраженного мы тут не получим.
Вот.
Что мы видим?
Мы видим, что у нас есть хк.
И выражение, которое очень напоминает то направление,
которое у нас фигурировало в процессе обновления
нашего хк в методе Ньютона,
в котором мы получили вот буквально только что.
Вот это выражение.
Видно?
Видна указка, да?
Я надеюсь.
Да, видно.
Да, все прекрасно.
Вот.
Что это нам, в общем-то, дает?
Дает то, что мы можем переупорядочить
все наши замечательные выкладки.
Именно сказать, что хк минус,
ну, собственно, направление f'-1 хк f' хк.
Вот.
Ну и, собственно, из этого еще вычесть х со звездочкой.
Вот.
И сказать, что...
Да, а это у нас ноль.
Мы пропускаем этот этап и просто сравним с нолью.
Вот.
И эта штука у нас умалая от хк-х со звездочкой.
Ну, это, по сути дела, что получается?
Что хк плюс 1 минус х со звездочкой
это умалая от нормы хк минус х со звездочкой.
Вот.
Ну и значит, что если мы теперь перейдем
к пределу приказ стремящегося к бесконечности
между нормами хк плюс 1 минус х со звездочкой
норме хк минус х со звездочкой, то
это же, по сути дела, что будет?
Это будет предело отношения между умалым.
Вот.
И тем аргументом, который стоит под умалым.
Что ж такое, хочется написать, минус в нижнем регистре.
А это ноль по определению.
И значит, сходимость будет сверхлинийная.
Потому что если бы это было
число как какой-то меньший единиц,
то это было означало, что
сходимость линейная и мы каждый раз
хк раз уменьшаем эту самую норму.
А поскольку мы уменьшаем быстрее, чем
чем в константное число раз,
вот, следовательно, получается сверхлинийную сходимость.
Вот.
Это было, видите, достаточно общий такой
некий результат, который, тем не менее...
Сейчас скажу что-то.
Сейчас, секунду.
Да.
Важно подчеркнуть, что
все это
должно выполняться в случае, когда у нас
хк достаточно близко
ну, близко находится
со звездочкой.
Вот.
То есть это, насколько я понимаю,
обусловлено ровно тем, чтобы был выполнен дотаральство.
Вот.
Что у нас...
Вот, надо день написать со звездочкой.
Ну вот.
То есть все вот это, вот сверхлиния сходимость,
сколько напишу, локальная.
Вот.
Это важно, потому что
пример ситуации, когда у нас все разваливается,
типа, функция fiat t, например, равна вот такому вот выражению.
Я привожу этот пример.
Проиллюстрировать, зачем, в принципе,
нужно отдельно оговаривать про локальность.
Потому что, ну, наверное, нужно небольшое отступление сделать
и сказать, что вот есть метод Newton для решения
задачи вот такой, что обсудили.
Есть метод Newton для решения задачи вот такой.
И вот они по сути дела эквивалентны,
потому что вот эта задача в случае выпуклой функции
сводится к задаче вот такой,
в которой можно сказать, что это g от x.
Поэтому можем посмотреть вопрос
решения нелинейного уравнения
очевидным решением 0.
Вот.
И, ну, здесь
для такой штуки метод Newton, понятно, как будет записываться.
Это будет xk минус матрицы якобе
минус 1 на g от x.
Нужно ли пояснять, почему здесь вот такое выражение?
Или очевидно, или вы уже знаете это все?
Ставьте плюс, если можно идти дальше,
и минус, если нужно подробнее пояснить.
Сейчас к чему относится вот этот комментарий,
то есть плюс-минус?
Этот комментарий относится к тому, что...
Так, надо пояснить, все вижу.
К тому, что мы сейчас покажем
локальное устройство метода Newton
на примере не решения задачи минимизации,
а решения системы, ну, решения нелинейного уравнения.
Допустим, что сводим одно и другое.
Да, заодно же показать, действительно.
Ну, то есть смотрите, что происходит.
Вот мы хотим вот такое найти.
Это мы вместо x подставляем xкат
плюс линейную опроксимацию xкат на x минус xкат.
Ну и отсюда x, наш новый, каплю в первый получается, да?
Выражается понятно, что xкат и минус g' минус 1 от xкат
вон g от xкат.
Все векторы, потому что g это функция из rn в rn,
система нелинейных уравнений.
В общем, получаем, что методы совпадают,
потому что если наш g это f',
то g' это будет f' ровно тот самый гессиан,
о котором мы до этого говорили.
Понятно ли, почему так выглядит метод Ньютон
для решения системы нелинейных уравнений теперь?
А, гуд.
Хорошо.
Ну, давайте найдем производную.
Я думаю, тут все стилисты как-то делаются.
Э, что, одно и второе, да?
Минус t.
Сейчас опять не хватит места, но ничего.
Конечная бумага, это.
Добно.
Так, бом-бом-бом-бом-бом.
Одно второе на 2t.
И, собственно, 1 плюс t квадрат
минус одно второе.
Вот, что-то такое.
Это все, значит, будет у нас равно.
Двойки уходят, t квадрат остается,
и можно вынести, в общем, 1 плюс t квадрат.
Так, давайте не будем, не будем торопиться.
Это выносить, получается, единица
минус квадрат умножить на 1 плюс t квадрат.
Извините, а мы минус при показателе 1 плюс t квадрат забыли?
Да, я...
Сейчас, тут все неправильно.
Я хотел сделать 1 плюс t квадрат, вот так, правильно?
И минус t квадрат, вот.
Делится на 1 плюс t квадрат.
Это, так говорить, корректно.
Все, отлично.
Это сокращается,
и мы получаем, что наше производное
это 1 плюс t квадрат в степени минус 3 вторых.
Вот.
Ну и теперь, собственно, t к плюс 1
это t к минус 1 плюс t к в квадрате
в степени 3 вторых, потому что там минус стоит.
И это все умножается на что?
На t к и 1 плюс t к в квадрате минус 1 вторая.
t к минус t к умножить на вот, ну понятно,
на 1 плюс t к в квадрате минус t к в кубе.
Вот такая это рация.
То есть, ну, сходимость-то вообще-то почти кубическая, да, получается?
Вот.
Но это, но важно другое.
Важно, что при норме t 0 меньше единицы мы будем сходиться.
При норме t 0 равным единице
будет, будет асцелировать все.
Третье, если t 0 больше единицы, то расходится.
Поэтому важно, чтобы мы попали именно вот сюда.
Локальность важна. Понятен ли пример? Хорошо.
Это мы про, был первый пункт,
про сверхъянеисходимость.
Теперь второй пункт, про квадратичную исходимость,
собственно, ради чего все это задевается.
Вот. И тут потребуется несколько условий.
Именно три штуки.
Пусть первый, гессиан-липшицов, уже не так нехило.
Нормы, соответственно, там матричная вторая норма и подчинен,
точнее, тут вот вторая норма Евкрида,
вот тут вторая норма матричная, типа, максимальная.
В данном случае собственное значение,
потому что гессианы симметричные,
в общем случае старше и сингулярное значение.
Вот. Дальше x 0 достаточно...
Дальше правильно сказать, что f от x сильно выпуклась
в константы μ. Вот. И третье, что мы достаточно близки,
а именно вот так. Вот. То есть, вот.
И тогда из этого всего замечательных,
из всех этих замечательных условий следует,
что у нас x плюс 1 минус x со звездочкой
будет меньше либо равно, чем m на норму x ка минус x со звездочкой
вот тут вот самый квадрат появляется,
на 2 μ минус m нормы x ка минус x со звездочкой.
Вот. То есть, вот это вот наличие вот этого квадрата
обеспечивает нам то, что сходимость будет квадратична.
Вот так. Здесь двоечка. И здесь тоже. Вот.
Теперь, то есть, мы будем...
То есть, график сходимости будет выглядеть вот так.
Сейчас я, наверное, его отдельно покажу.
Типа, оп. И вот здесь вот было, грубо говоря, 10 минус 1.
Дальше оценки какие? 10 минус 1, 10 минус 2, 10 минус 4 и 10 минус 8.
Ну, короче, понятно, что дальше идет 10 минус 16.
Вот. И мы сошлись за раз, два, три, четыре, четыре террация.
То есть, это не тот, это не градиентный спуск,
который там будет сходиться миллион лет
до точно здесь минус там 10.
Тут опять террация к 10 минус 16 сходится.
Понятно ли?
Формулу скорости сходимости я не успел выписать.
Спасибо.
Не выбирайте.
Выбирайте, я тоже не успел.
Да-да-да. Все.
И я сейчас картину хотел показать, на которой это все делаю и старирую.
Вот. Так.
Для этого надо делать вот так и вот так.
То есть, для вот такой вот задачи,
которая, по сути дела, смысл которой в том,
чтобы найти центр аналитический для многоугольника,
это ли почему это связано как-то с центром многоугольника или нет,
поставьте плюс, если понятно, почему связано,
и минус, если непонятно.
Непонятно, да?
Ну ладно, давайте это вернем.
Ну, хорошо, давайте пока запомним, как она выглядит.
Я сейчас про метод скажу, потом вернемся и я немного смысл поясню.
Вот. Ну и вот если запустить метод Newton и градиентный спуск,
то они будут сходиться вот следующим образом.
То есть, вот здесь вот будет некоторая область,
в которой метод Newton будет сходиться линейно.
Вот. Почему и как это будет?
Почему так происходит, я чуть позже скажу, когда мы сейчас время докажем.
Начиная с этой точки, он получается раз, два, три за три итерации,
как вот было примерно, ну примерно оценка, которая у меня и была.
Вот. Он благополучно сошелся к там 10 минус 8-ой.
Блестяще, быстро, легко и без особо, ну, относительно легко.
Без каких-либо проблем, именно связанных с симпточкой сходимости.
Вот. Это вот, в общем, типа, если вы что-то такое видите,
когда вы хотели получить метод Newton, значит, что вы все правильно сделали
и, в принципе, все хорошо так и должно быть.
То есть, если у вас метод Newton начинает медленно сходиться вот в стиле вот таком,
вот это значит, что вы где-то там что-то неправильно, неправильно учили.
Вот. Сейчас в процессе, я надеюсь, как-то отдельно упомянуть
тем, какие потенциальные ошибки вы можете случайно сделать.
Вот. Так. Теперь, собственно, про задачу, откуда она в целом может взяться.
Ну, вот взяться она может быть... И следующая история.
Надо решить, система не равен. А х меньше, либо равно единице,
в условии, что модуль Са меньше, либо равен единице.
Вот. Хотим найти какой-то х. Вот. При этом вот эта штука,
я надеюсь, геометрия ясна, это пересечение гиперплоскостей.
По сути дела, это многоугольник.
Любой х, который лежит внутри, нам потом, нас устраивает.
Вот это, вот это, вот это, вот это. Ну, плюс вот это. Вот.
А как выбрать какой-то один? Вот. Ну, и отсюда...
Парам-пам-пам. Да.
Отсюда следует простая штука, что давайте сделаем так,
чтобы он этот наш х отстоял от наших граней, как можно дальше.
Нет ли логика? Окей. Вроде, ну, вроде осмысленно, да?
Ну, вот. И чтобы это сделать, мы рассмотрим вот такую вот,
типа, такое выражение, которое, ну, просто, то, насколько мы лекуатнули,
ну, насколько хорошо у нас всё выполняется. Вот.
И насколько мы хотим, чтобы оно стало каким? Побольше, больше или нуля,
то мы возьмём и вот логариф от него сделаем.
Потому что логариф-то только на положительных определён,
поэтому как только мы будем приближаться к нулю,
у нас всё будет разлетаться. Ну, и поскольку логариф –
мы это ещё и такая штука, мы поставим сюда минус,
чтобы он стал вот таким. Скажем, что мы хотим проминемизировать
эту всю историю, ну, и по всем понятно, по всем неравенствам.
Эм. Вот. Тем самым мы будем как бы балансировать
принадлежность ИКСа нашему многоугольнику так, чтобы,
ну, грубо говоря, все неравенства были выполнены
максимальным зазором от границы. Понятно ли, откуда взялась установка?
Прекрасно. Так, графики я показал, всё прекрасно.
Давайте теперь, собственно, докажем это утверждение.
Доказательство будет, ну, не очень быстро,
но и в то же время максимально как бы инструментальным
и понятным, я надеюсь. Для начала обозначим всё то,
что нам нужно использовать, а именно ведём величину
РК плюс один, которая будет равняться, собственно,
нашей невязке. Вот. И после подстановки выражения
для метода Ньютона мы получим, что эта вся история
есть не что иное, как РКТ минус это самое направление.
Вот. Дальше сделаем такой трюк. Распишем f'xk вот таким вот образом.
f'xk-f'x' поскольку это ноль. Вот. И скажем, что это интеграл
от 0 до 1 от гессиана на x' плюс tRK dt. То есть мы возьмём
как подтресочек, соединяющий x' и xk и проинтегрируем
второй производной вдоль этого отрезка. Я тут забыл написать
ещё РК, чтобы размеры не совпали. То есть формула
Ньютон-Лейбенца переписаны в периметрическом виде,
так сказать. Понятно ли, откуда взялось?
Да. Прекрасно. Ну, это наш f'xk и теперь мы можем его
подставить вот сюда. Получим, что РК плюс один это РК
минус гессиан, умноженный на интеграл. Вот. После
чего мы благополучно выносим за знак, выносим множитель
обратно гессиан. Получаем, что тут у нас… Сейчас, секунду.
Так, рано. Потом, конечно, это сделаем. Сначала надо
сказать, что, в сути дела, у нас теперь… Бла-бла-бла.
Вот так dt ещё умножается на РК. Вот. Вынесли, в общем,
то, что не зависит от t. После чего понимаем, что теперь
наша норма РК плюс один. Меньше ли бравна норме,
ну, обозначим… Так, не хватает немножко места. Обозначим
вот эту штуку за g. Вот. Что это ну, норма g на норму
g. Вот. И поймём теперь, что на самом деле мы почти
уцели, потому что вот этот вот РК-1 уже есть. Теперь
нам нужно оценить норму g так, чтобы второй множитель
нормы РК появился, тогда у нас появится квадрат.
Мы будем, собственно, дойдём до того, что мы изначально
хотели получить. Давай теперь оценивать. Значит, что такое
же kt? Ну, да, она, конечно, от ка зависит. Почему оно
равно? Можно вынести теперь обратно гессиан, то, что
хотел, что всё мы начали сделать. И внести единичную,
получившуюся статус от единичной матрицы внутрь
интеграла, чтобы делать хитрый трюк. Вот. А трюк,
собственно, очень простой, что норма g у нас, это меньше
либо равно, чем норма вот этой штуки в первом множителе.
И потом интеграл от нормы разности. Интеграл от нормы
разности у нас ограничен в силу того, что гессиан
липшится. Вот. Поэтому это меньше либо равно, чем
вот это. Здесь будет интеграл для одного m на норму разности
аргумента. xk минус x звёздочка минус t r dt. Понятно ли,
что происходило до этого момента? Ставьте, пожалуйста,
плюс, если понятно, и минус, если в какой-то момент были
непонятные преобразования или мотивации их приведения.
Вижу два плюса. А вижу три. Отлично. Вот. А что получается-то?
Вот эта штука, это же, ой. Вот эта штука, это же, на
самом деле, rk. t у нас с 0 до 1 интегрируется. Поэтому
можно переписать, что наш интеграл, который вот здесь,
минус t r dt, по сути, норма rk, собственно, то, что вы хотели.
0,1 m, а 1 минус t, потому что положительное число dt. Теперь
надо посчитать интеграл. Давайте, давайте посчитайте
этот интеграл, пожалуйста, и скажите мне, чему он будет
равен. Вот. Немножко взбодримся с утра пораньше. У меня
уже как эти штуки вычисляются. Ответ можно писать в чате.
Ну, это t минус t квадрата пополам плюс какая-то констанция.
У вас определённый интеграл для одного числа должно
получиться. А, ой, точно, первообразно. Одна вторая, по идее.
Ну да, вроде тоже должно получиться одна вторая. То есть, смотрите,
по сути дела, эта оценка интеграл дала нам необходимый
дополнительный множитель. Вот этот. Вот. И теперь осталось,
как бы, вот эту штуку оценить. Вот. Но для того, чтобы ее
оценить, нам потребуется вспомнить о том, что у нас из
липчатогости следует следующая оценка на просто
гессиан в точке xk. Вот. Через гессиан в точке x звёдочка минус
m на норму rk и на единичную матрицу. То есть, это просто
вывод того, вывод из того, как у нас. Ну что, вот если
у нас вторая норма xk минус 2 минус x со звездочкой.
Вот здесь вот вторая норма чего-то меньше. Это значит,
что лямбда макс у этой штуки, он меньше, чем вот эта величина.
А это значит, что если мы рассмотрим матрицу f2' xk
минус f2' x со звездочкой минус вот то, что вот здесь стоит.
Вот. То мы получим отрицательно определенную матрицу. Вот.
Ну а раз так, то у нас, ну там, да, тут надо бы, наверное,
было написать какая-нибудь ерунда со знаками опять начнется.
А, ерунда со знаками начнется, да. Ну да, тут типа вот так
надо поставить. В общем, от этого ничего не, свойство
липчатогость от этого не поменяется, короче говоря.
Вот. И теперь, когда мы переносим вот это вот сюда, то мы получаем
ровно то, что стоит вот тут. Получилось ли уследить за этими
стрелочками тем, что происходило? И так себе. А, вот это умножить
на единичную матрицу. А, нет. Или лучше все-таки писать
все подробнее. Лучше подробней. Окей, ладно. Давайте.
Вот эта штука меньше, чем m на x минус xk минус x со звездочкой.
Это наши условия липчатогости. Вот. Ну, то есть вот здесь стоит
ровно то же самое. Вот. А здесь стоит уже вот это то же самое
умножить на единичную матрицу. Да, и это отрицательно определено.
Ну, потому что мы из максимального множителя вычли что-то, вычли
матрицу, сдвинули спектр, грубо говоря. Вот. И раз
была отрицательная величина, то вся матрица будет отрицательно
определена. Понятно ли это преобразование? Да, да, да.
А, все, отлично. Вот. Ну, собственно, да, это было
пояснение вот к этому, к этой строчке. Теперь мы знаем, что у нас
функция сильно выпукла, поэтому в выполнении к этой
штуке мы имеем, что f' xk f' x
со звездочкой m rk. Единичную матрицу мы сверху можем вот
эту теперь величину подпереть mu на единичную матрицу в силу
критерия второго порядка сильной выпуклости. То есть теперь у нас
вот образуется mu минус m норма rk на единичную матрицу. Все, отлично.
Почти победа. Вот раз такие соотношения справедливы для
самих матриц, то для обратных будет справедливо ровно обратное.
Неудивительно. Потому что, ну, лямбда макс, лямбда мин меняется
местами при взять обратной. Вот. Ну и поэтому, значит, f'-1xk
по норме меньше либо равно, чем 1 делить на mu минус m
на норму r. Все. Значит, теперь объединяя вот это...
Таль, давайте к ней. Объединяя вот это...
Крупнее. А, это самое крупное, оказывается. И вот это,
вставляя все это вместе, мы получим ровно нужный новый результат.
Откуда берется квадрат справа? Ну, с правой части вроде, надеюсь, понятно.
Вот. Теперь что? Теперь надо пояснить, зачем нужно вот это.
Вот это нужно затем, что если мы вот эту величину распишем таким вот образом,
что мы mxk минус x' делим на 2 mu минус mxk минус x' в квадрате
и умножаем теперь на xk минус x'
Надо быть уверенным, чтобы в начале у нас вот эта величина
будет меньше 1. Ну, чтобы норма действительно была.
И если мы как раз-таки потребуем, чтобы это было меньше 1 для равной 0,
то мы получим вот эту вот оценку.
То есть, тут все достаточно прямолинейно, без каких-либо хитрых штук.
Нет ли доказательства и основные поводы из него? Вроде да.
Хорошо. А теперь, значит, пара слов про то, как бороться с этой локальностью.
Ну, наверное, не очевидно, но поскольку единственная штука,
в которой мы еще не внедрили метод Ньютона, это размер шага,
именно его настройка позволяет сделать сходимость глобальной.
Называемый демпфированный метод Ньютона, я не очень люблю это название,
но, в общем, записывается он понятным образом,
с точностью как мы до этого обсуждали, когда у нас есть шаг, у нас есть направление.
Ну, давайте шаг выделим желтеньким, направление выделим красненьким.
И, значит, использование этого шага позволяет, во-первых, сделать сходимость глобальной,
и, во-вторых, из, грубо говоря, области линейной сходимости мы,
когда мы придем из этой области в область, где у нас квадратичная сходимость,
то поэтому надо начинать с альфа ноль в бэктрекинге равным единице в адаптивном...
Блин, плохо.
То есть при адаптивном поиске альфа, ну, я вот так сделаю, ноль должно быть равно единице.
Потому что если вы попали в нужную область, то взятие альфа ноль равно единице,
и если для этого шага сходимость уже получается, что функцию убывает,
скорее всего, это значит, что вы попали в эту самую область,
и дальше, взяв этот шаг равный единице,
вы получите уже квадратичную сходимость вместо линейной,
которая присутствует, если делать шаг меньше единицы.
Нет ли основная мотивация и то, как делается подбор шага в этом методе?
Очень хорошо.
Окей, так, 9.45, слушайте, хорошо, хороший темп.
Теперь, собственно, еще важный момент про мета Ньютона,
который уже был проиллюстрирован на картинке,
это то, что метод Ньютона дает решение очень высокой точности.
То есть если вам не нужна такая точность,
то, возможно, вам лучше остановиться на методах первого порядка,
которые, там, 10 мил в шестой вам дадут по норме градиентов какой-нибудь задачи,
и на том спасибо.
Вот, а если вам нужно только 10 мил в двенадцатый, там, из каких-то ображений,
то только метод Ньютона вам сможет дать такие-такие-такие точности за разумное время.
Это, наверное, финальная ремарка.
В целом, то в целом,
да, мы вроде бы уже обсудили более-менее все достойные недостатки,
то есть память, высокая стоимость одной итерации,
но за счет этого мы получаем квадратичную сходимость и высокую точность.
Теперь, так, есть какие-то вопросы по первой половине?
Конец.
Конец, ну замечательно, очень хорошо.
Надеюсь, что все действительно достаточно понятно.
Теперь перейдем к квазинтуловским методам Ньютона,
идея которых очень простая.
Вот у нас был, так,
вот у нас был градиентный спуск, который мы получали из вот следующей оценки.
Вы помните?
Сверху оценивали квадратично.
Понимаете такое? Прекрасно.
Это был градиентный спуск.
Методы Ньютона у нас примерно такая же ситуация.
Только там, ну, не оценка сверху, но, в общем, так, да?
И они, эти методы, в своих свойствах, достоинствах и недостатках,
являются комплементарными друг к другу.
То есть то, что хорошо у одного, то плохо у другого.
В частности, у градиентного спуска сходимость только линейная,
у метод Ньютона квадратичная.
С другой стороны, градиентный спуск требует всего лишь линейного числа,
линейную сложность по памяти имеет,
метод Ньютона имеет и квадратичную сложность по памяти,
и кубическую по итерации, ну, по стоимость одной итерации.
То есть вот они как бы друг друга дополняют.
Поэтому возникла идея сказать, что давайте мы сделаем что-то промежуточное
и будем нашу функцию приближать, вот, почти что точно так же, как метод Ньютон,
он только вместо гессиана возьмем и будем его приближать
к некоторой матрице, к другой, не уже не гессианам.
То есть метод перестает быть методом второго порядка
и возвращается семейство методов, которые используют только градиенты.
Вот, и сейчас будем разбираться, как именно эту матрицу B нам куда мы выбрать.
И для этого, ну, то есть понятно, что отсюда
xk плюс 1 будет пересчитывать по тем же самым формулам,
ну, тут вот 2 альфа к и все заработало, фей.
Вот, то есть на самом деле нам как бы даже не матрица B нужна, а матрица B в минус 1.
То есть что надо сделать?
Надо чтобы сложность одной итерации стала хотя бы сложной меньше, чем N куб.
Первое, второе, сходимость осталась хотя бы сверхлинейной,
чтобы, ну, не сваливаться в линию сходимости градиентного метода.
Вот, ну, вот сюда же как бы два пункта.
Первый – это пересчет xk плюс 1, пересчет из xk xk плюс 1,
а второй – это пересчет из Bk в Bk плюс 1.
То есть пересчет матрицы и пересчет x должны быть суммарно недороже, чем N квадрат.
Понятно ли, почему это надо?
Вкратце, можно пояснить?
Мы хотим, чтобы наш метод работал быстрее, чем метод Ньютона.
Да.
В методе Ньютона у нас стоимость решения линии системы N куб.
Мы хотим, чтобы стоимость одной итерации была меньше, чем N куб.
У нас в эту одну итерацию помещается не только вычисление пересчета xk плюс 1,
но и обновление матрицы B.
Поэтому вот это надо, ну, очень надо следить и про вот это, и про вот это.
А что такое сверхлинейное? Можете напомнить?
А, сверхлиния скорость сходимости, да, могу.
То есть мы доказывали только, что метод Ньютон сходится квадратично.
Теперь мы его как бы немножко огрубляем,
но хотим огрубить не так сильно, чтобы, настолько плохо, чтобы стало типа линейное.
Логично, согласитесь?
Да, согласен.
Хорошо.
Так, и третий пункт со звездочкой, обычно.
Хотим хранить B, то есть убрать, давайте, компактное хранение B.
Вот, это типа, чтобы уйти от N квадрат памяти.
Как это делается, вообще-то пока сход очень неочевидно.
Вот, и я надеюсь, что ближе к концу мы все-таки успеем это разобрать.
Вот, вот такие у нас требования.
Значит, теперь, собственно, что мы к этому матрицу B можем,
что называется, извлечь из каких-то соображений.
Ну, вот тут хитрое некоторое место.
Смотрите, вот когда у нас появилась вот эта вот оценка,
давайте назовем FQ.
Q у нас будет зависеть от направления H, и, собственно, так вот.
Но здесь, в общем-то, когда мы все посчитали, нам все известно, кроме матрицы B.
Давайте подумаем, что мы можем сказать о свойствах этой оценки,
когда у нас есть некоторые вот куксы такие.
Ну, то есть нету, что надо как-то B0 проинциализировать.
Слушай, не вопрос.
Инициализация.
Ну, типа, лидичные матрицы можем проинциализировать, в общем-то, не беда.
Если мы так сделаем, то у нас получится такое выражение.
x1 равно x0 минус алифа 0 на градиент.
В общем, получим просто градиентный спуск, это все понятно.
Далее, точка, то есть это вот мы типа, вот у нас был тут x0, мы вот переехали в x1.
Далее нам нужно из x1 каким-то образом перейти в x2.
Как это должно быть, должно выглядеть?
x2 это x1 минус альфа 1, b1 в минус 1, f' от x1.
Известно все, кроме вот этой штуки.
Понятно ли на каком мы сейчас этапе?
Окей, я вижу 1+.
Я не понял вопрос.
Вопрос в том, понятно ли, что мы сейчас пытаемся получить
и на каком этапе мы сейчас находимся в процессе этого получения.
То есть мы сейчас пытаемся, исходя из того, как мы интеррируемся с одной точки в другой,
понять, какие требования на матрицу бы наложить, чтобы ее можно было бы пересчитать.
Да, понятно, хорошо.
Все, хорошо.
Смотрите, мы сейчас находимся в точке x1.
И в точке x1 у нас есть вот эта вот замечательная модель f'
Давайте я, может быть, даже запишу.
f, ой, f' и есть вот единичку.
То есть это f от x1 плюс колярное произведение на…
Ну и плюс соответственно коэффициентик h транспонировано b1 h.
Вот, это наша модель.
Что мы можем потребовать от этой модели, чтобы в некотором смысле догнать ее точность?
Как вы думаете?
Знаете, что у нас помимо f' есть еще версус просто f от x.
Честная наша функция.
Которую мы можем по точкам оценить в плане модели черного ящика.
Какие будут варианты?
Мы можем по конкретному интерту h посмотреть на реальное значение x плюс h
и посмотреть на h транспонированное b…
И сказать, что мы хотим, чтобы вот это было значением h t b h.
Смотрите, если я правильно понял, то вы, в общем-то, на правильном пути…
Смотрите, мы находим сейчас точки x1.
И куда из этой точки мы можем перейти, чтобы можно было сравниваться…
Сравнив получившееся значение нашей модели точным значением функции.
Или с точным значением чего-то, связанным с функцией.
Первое направление, которое направится, это ноль.
Не поверите.
Надо бы…
Хочется, чтобы наш градиент нашей функции в нуле был бы равен честному градиенту в x1.
И, как вы можете убедиться, это автоматически выполняется.
Тем ли это видно?
Или нужно написать еще одну строчку с градиентом нашей модели?
Видно.
Кострин взяла a, гуд, Дмитрий.
Нет ли вам, что если возьмете градиент и подставите ноль, то получите просто градиент с точки x1?
Ага, вижу.
Хорошо.
Это первое.
Второе направление, какое напрашивается?
Единичный вектор.
Что мы не знаем, что происходит в направлении единичного вектора?
А, тогда то же самое, что в прошлый раз.
Какое?
h с предыдущего шага.
Да, но только не h, а мы перейдем вот из этой точки теперь вот в эту точку.
А это направление называется x0-x1.
Вот.
Тогда мы получим что?
Что градиент f' точки x1 плюс там 1 на 2 альф, понятно?
Сейчас.
Нет, подожди.
Да, понятно.
1 на 2 альфа.
Просто 1 альф, простите.
Когда будет что здесь?
B1 на x0-x1 должно выровняться f' от x0.
Во.
Внимание, вопрос.
Сейчас, секунду.
У меня куда-то потерялся.
Сейчас какая-то проблема возникла со знаком.
Ой, не со знаком, а с альфой.
Но я верю, что сейчас я пойму, как ее правильно разрешить.
А h у нас там точно...
Ну да, x0-x1, да.
Да, видимо правильнее смотреть пока что без шага.
Да.
Вот.
Да, давайте пока без шага будем смотреть.
То есть полная аналогия с тем, как метод у меня классический
уводился без демпирования.
И тогда с учетом этой поправки мы получим условия на матрицу
B1 вида...
B1 умножается на x0-x1, а тут будет разность градиентов.
Вот этот вектор обычно в литературе обозначается s0.
Ну, это соответственно y0.
Вот.
В принципе, уравнение, получающееся в результате B1 s0 равняется
y0, называется в англоязычной теме аналогии second equation.
Ну, на русский как хотите можно переводить более-менее
там квазюмтовское уравнение, еще что-нибудь там.
Вот.
Отсюда надо найти B1.
Кто понимает, какие тут возникают проблемы?
Так, ну что, какие проблемы?
Всегда ли можно решить?
Сколько уравнений, сколько неизвестных?
Единственное ли решение?
Все как обычно.
Думаете?
Кажется, что сильно не единственное, потому что мы...
Да, это правда.
Прекрасно, да.
Решение единственное, потому что переменные в порядке
n2, хотя там немножко меньше, потому что симметрия есть,
они единственные.
Прекрасно.
Всегда ли оно есть?
Считывая, что матрицу B мы вот тут вот, что хотим?
Чтобы...
Нет.
Чтобы B было равно Bt, и матрица B была бы положительно
полуопределена, потому что это как бы, ну, гися
нам приближает, поэтому очень хочется, чтобы этот
свойств также отнаследовалось.
Всегда ли будет решение?
Решение.
Все у таких вот неявных ограничений, я сказал.
Вот хорошо.
Что может пойти не так?
Давайте исходить из того, что мы видим, и что может
сломаться.
Вижу, что идей особо нет.
Ну, смотрите, что может сломаться.
Сломаться может следующий момент, что если мы слева
и справа умножим на S0, мы получим S0, B1, S0, а слева
будет Y0, S0.
И вот почему вот эта штука будет больше нуля, никто
нам не сказал.
То есть мы можем так промахнуться с выбором шага
в 1, в 0, то вот это вот скалярное определение может стать
отрицательным.
Да, наша матрица B, она не положительно определена,
потому что на векторе S0 квадратичная форма соответствующая
принимает отрицательные значения.
Нет ли проблем?
Нет проблем.
Хорошо.
Чинится аккуратной настройкой размера шага вот на этапе
обновления предыдущей точки.
То есть на этом этапе мы альфа 0 подбираем не только
так, чтобы значение функции уменьшилось, но и так, чтобы
скалярное определение стало было положительным.
Наши упражнения, проверьте, что при альфа на стоимящем
сек нулю это будет выполнено.
Или не будет выполнено, тогда можно будет обсудить
в следующий раз, что ломает.
Вот.
Значит теперь, для того, чтобы зафорсить и заставить
это решение, это уравнение иметь единственное решение,
делают следующий шлюк.
Говорят, что давайте мы будем решать вот такую задачу.
То есть мы будем искать такую матрицу B из всех матриц,
для которых наше уравнение выполнено, то она будет
ближе всего к прошлой матрице, которая у нас уже была.
Тем самым мы пытаемся сказать, что вот на этом графике у
нас оценка гисяна в этой точке, оценка гисяна в этой
точке, они не очень сильно начинают.
Нет ли мотивации?
Good.
Значит, о чудо это все добро имеет аналитическое решение,
вы не поверите.
Я его сейчас запишу и мы немножко обсудим.
Ну давайте даже, нет, давайте я сейчас не буду его
записывать.
Имеет аналитическое решение, пока так напишу.
Вот.
Значит мы к этому еще вернемся.
Но перед тем, как кружаться в дебри, хочется простой
метод рассказать.
Называется метод Бразилая Бурвина.
Вот.
И очень все изящно, я надеюсь вы оцените.
Посмотрите, когда мы рассматривали градиентный
спуск, у нас было вот такое направление.
Мы его можем переписать вот таким вот образом.
И можем сказать, что это примерно равно нашему оценке
на гисян минус первое ф штрих от х.
То есть вот эта штука будет выполнять наш модель,
выполнять роль модели нашего гисяна.
Наша матрица B.
Раз это матрица B, то для нее мы хотим получить квазинтонское
уравнение.
Ну вот check antiquation тут самый.
То есть B на s ката равняется y катуум.
Давайте подставим.
Что такое?
1 на альфа s ката равняется y катуум.
То есть нам надо найти такую альфу, что x s ката был примерно
равняется альфа на y ката.
Как это делать?
Это уравнение, ну в смысле уравнение, да, уравнение
на альфа больше нуля.
То есть это скалярное уравнение вообще.
Как найти такой альфа?
Ну и лучший.
Есть ли понимание?
У нас всегда были менее одинаковые подходы во всех
таких случаях.
В любой непонятной ситуации надо писать более менее
одно и то же.
Но нет, интуиция еще недостаточно развилась.
Если нужно 1 вектор s данной приблизить к некоторым
другим вектору, умноженным на неизвестное число.
Такое число, чтобы приближение было максимально точно,
то надо сделать.
Ну частная норма, например.
Чего-чего?
Еще раз?
Норму y1 поделить на норму sn.
А почему так?
Почему это будет наилучшая?
Я скажу y10.
То есть давайте я картинку нарисую.
Вот у нас 1 вектор, s1, sn.
И мы хотим приблизить его числом, умноженным на
другой вектор.
y1 и так далее yn.
Как найти альфу?
В каком ноль вы хотите лучшее приближение?
Вот чтобы вот это равенство было как можно измерить.
Давайте это измерим.
Это прекрасный вопрос.
Но можно измерить нормы разности.
Отлично.
Можно измерить нормы разности.
Что дальше надо сделать?
Решить задачу оптимизации.
Ну гениально, конечно.
В любой непонятной ситуации мы ставим какую-нибудь задачу
оптимизации, которую благополучно можем решить.
Эта штука минимизируется по не отрицательным альфу,
квадраты.
Давайте найдем градиент этой функции по альфу.
Диктуйте.
Это выпуклая задача, потому что у нас…
Мне кажется минус 2 на то, что внутри скобка.
Так, давайте одну вторую домножим, чтобы не страдать.
Подожди.
Сейчас.
Минус ykt на то, что в скобках.
Градиент должен быть числом, потому что аргумент
числов.
Так, вспоминаем, как считать градиент.
В общем, полезным.
Минус ykt транспонированное на то, что в скобках.
Минус ykt транспонированное на то, что в скобках.
Да, это похоже на правду.
Это 0 при альфе, равный альфе со звездочкой.
Давайте найдем альфу.
Это же несложно.
Получается, ykt транспонированное s-катой делить на норму
ykt в квадрате.
Чудо.
Мы только что получили новый метод выбора шагов
в градиентном спуске.
Потому что мы начинали ровно с этого.
Воспользовавшись методологией квадринтонских методов,
подставив в квадринтонское уравнение нашу модель
Дисяна, мы получили способ, который требует УАТН вообще
вычислений.
Все классно.
И вот оказывается, что эта штука работает достаточно
неплохо.
Сейчас мы через некоторое время, я надеюсь, как раз
успеем посмотреть, как именно она работает.
Понятно ли, как получался метод?
Куда он взялся?
Каких соображений?
Вот Андрею понятно, как там дела у остальных.
Так, Дмитрий Гуд.
Окей.
Так, хорошо.
Это была как бы простая история.
Более сложная история, собственно, вот если немножко назад.
Вот если идти по пути, в которой мы до этого как бы
были.
Говорили, что мы ищем такую B, которая близка, потом получаем
из B, получаем H.
То есть B-1.
Там тоже все получается аналитически.
И вот этот метод называется метод AFP по премиатурам,
ну, по первым буквам имен автора.
Вместо этого метод BFGLS по именам Бройден, Флейчер,
Гольфар, Пшано, авторы, они предложили следующий трюк.
Они говорят, а давайте мы вместо B будем минимизировать
норму для сразу обратной матрицы.
Ну, у нас задача перепишется как SKT равняется HYKT.
Ну, и там понятно все симметрично.
И вот у этой задачи тоже есть, ну, понятное, аналитическое
решение, которое я сейчас попытаюсь записать.
Мы с вами обсудим, как это все дело правильно считается.
Это не самое, наверное, очевидное.
Хотя, может быть, для вас это будет очевидно.
Тут такое жуткое выражение.
Сразу предупреждаю, чтобы вы не пугались.
Это все выглядит жутко, но у существа ничего страшного
и трофического, ну, ничего страшного и чего-либо,
что предотвращало бы эффективное использование этого метода,
тут как бы ничего не написано.
И РОКАТ это вот такая штука.
То есть, смотрите, мы получаем.
То есть, у нас есть и S0, Y0 и H0.
Мы эти штуки подставляем в эту формулу, получаем H1.
Вопрос, как посчитать H1 за O от N квадрат?
И можно ли это сделать?
Это же была наша цель.
Просто если делать все в лоб, то кажется, что тут,
типа, умножение матрицы от N куб, все пропало.
Мы с этим как-то побороться.
Какие будут предложения?
И вообще, видите ли вы эту проблему?
Проблему побольше сказать?
Да, нет.
Что делать?
Проблема видна в том, что можно H1 выразить, но,
как вы уже сказали, это уже матрическое произведение.
Хорошо.
Да, отлично.
Я рад, что видна проблема.
Давайте думать и предлагайте идеи, как ее преодолеть.
Значит, ответ кроме двух словах.
Задача минимизации?
Не тех двух словах.
Нет, погодите.
Тут уже мы ее решили.
Понимаете, в чем проблема?
Задача минимизации решается один раз.
То есть, если мы тут уже получили некоторые решения,
то дальше уже поздно.
Что-то другое должно произойти.
Ладно, один вопросов называется раскрыть скобки.
Потому что если мы перепишем hk плюс 1 как k минус ro k s k y h
минус ro k...
на брат.
Плюс ro k квадрат.
Проверьте, правильно ли я раскрыл скобки?
Да, нет.
Выглядит правильно.
Давайте теперь смотреть, что происходит.
Понятно ли почему посчитать вот эту штуку?
Это уатн квадрат.
Умножить столбец на строку.
При изведении двух матриц, одна из размеров из которых единица.
Понятно.
Да, нет.
Вижу плюс, отлично.
Едем дальше.
Давайте теперь вот этим посмотрим.
Значит, тут ro это число вообще.
Это неважно.
Мы его считаем за уатн.
Скалярное произведение.
Это легко.
Дальше мы умножаем h на y за n квадрат.
Умножаем вектор, столбец.
Умножаем его же на sk транспонированное, что снова дает n квадрат.
Тут как бы дальше надо скобки расставить правильно.
У нас есть оптимизации.
Первые два слова.
Вторые два слова это раскрыть скобки.
И еще два слова это расставить скобки.
Чтобы все правильно быстро посчиталось.
Ну и здесь та же самая история.
Вот эта штука n квадрат.
Это уатн.
Число вообще получается.
И вот это потом снова n квадрат.
В итоге после раскрытия скобок у нас все операции происходят
за уатн квадрат операцией.
Ну за уатн квадрат все преобразования.
Понятно ли каким образом нам удалось избежать сложности.
Кубической сложности.
Ну фокус мы с перемножением векторов в бестоматриц.
Ну фокус в том, чтобы без формирования явного матрицы вида единицы плюс плюс там или минус ранг один.
Вот который вот здесь вот.
После раскрытия и последовательного перемножения получается, что мы просто умножаем матрицу на вектор.
А потом матрицу ранг один явно формируем пока что.
Потому что у нас в памяти вроде пока что мы можем все позвольте n квадрат.
Боремся только с сложностью одной итерации.
Ну все. Все или не все. Все ли понятно по этому методу.
То есть общая логика такая.
Вот мы здесь на этом этапе.
Давайте еще раз напишу наверное.
То есть у нас есть x0. Есть. Есть x0.
Мы считаем x1.
Потом считаем f' от x1.
Потом считаем h1.
Потом считаем x2.
Потом считаем f' от x2.
То есть тут последовательность выполнения шагов она немножко отличается от привычной.
Потому что есть дополнительный шажочек со звездочкой, которое в отчислении h1.
Шажочек плюс.
Это вычление градиента не перед вычислением точки.
А после вычления предыдущей точки.
И в общем градиент посчитали.
Посчитали h2.
Ну и x3 соответственно потом пошел дальше.
То есть тут немножко последовательность так хитро перекручивается.
Понятно ли как это реализуется?
То есть я псевдокод сейчас так набросал.
Но я надеюсь что после там уже трех-пяти методов, которые мы изучили.
Талант понятно как это кодится?
Или есть еще не до конца понятно?
Суть происходящего.
Да, нет.
Понятно.
Понятно. Очень хорошо.
Так здорово.
Значит смотрите сейчас уже 10.23.
Значит мы с вами разобрали какие методы сегодня.
Ну DFP вкратце.
ФГС чуть более подробно с анализом сложностей.
И Бразилая Бурвейна тоже достаточно подробно.
В следующий раз мы рассмотрим LBFGS.
LBFGS собственно самый главный метод решения в большой размерности.
Один из главных наряду с сопряженными градиентами.
Вот.
И пойдем уже там в максимальный градиентный метод.
Там что у нас еще будет?
И Франк Вольп.
Это будет в следующий раз.
Я надеюсь как раз вот эта штука не займет слишком много времени.
Хотя там есть очень красивая идея.
Но я постараюсь ее максимально непонятно и подробно прояснить.
Так.
Тогда на сегодня все.
Большое спасибо за внимание и участие.
В следующий раз переходим наконец-то к задачам с ограничениями.
И посмотрим для каких задач.
Какие важные операции типа проецирования.
Типа вычисления минимума линейных функций на множестве.
Как они могут помочь.
Какие скорости сходимости будут у соответствующих методов.
И почему это может быть быстрее.
Ну то есть прям на примере посмотрим.
Быстрее чем использование солвера.
Который не знает ничего про структуру задач.
И является таким General Purpose.
Для общих целей.
Все тогда.
Большое спасибо.
И до следующей недели.

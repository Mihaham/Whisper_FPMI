Так, сегодня, соответственно, продолжаем разговор с методами
в духе градиентного спуска, соответственно, на чем
мы остановились в прошлый раз, остановились мы на
следующем.
Смотрим оценку для градиентного спуска, какая оракульная
сложность.
Соответственно, у него для решения L гладкая mu сильно
выпукло в задачке.
Ну и соответственно, сколько нужно там итерации у него
сделать, вызвать оракулу, чтобы мы, соответственно,
нашли там точность решения Эпсилон.
Кто помнит, кто посмотрел это, так, да, это правда,
соответственно, вот такая примерно оракульная сложность
у нас у градиентного спуска.
То есть, столько раз нам нужно посчитать градиент,
чтобы найти решение с точки зрения чего, почему мы
там искали решение для сильно выпуклой задачи, по аргументу,
по функции, по норме градиента, по аргументу.
То есть, мы находили прямо точное решение в связи
с тем, что для сильно выпуклой задачи у нас решение единственное
и существует.
Вот.
Окей.
Это хорошо.
Так.
Тогда, соответственно, у нас возник такой вопрос,
а вообще, вот это то, что мы получили, улучшаем
или нет?
То есть, можно ли придумать какой-то метод, который
будет работать быстрее, чем вот эта оценка?
Вот.
Оказывается, да, можно придумать.
Вот.
И идеи об этом, на самом деле, появились довольно
давно.
Вот.
И, соответственно, здесь, 1964 год, наш соотечественник
недавно скончался, Борис Теодорович Поляк, предложил
следующую схему.
То есть, тот же самый градиентный спуск, который мы изучали
в прошлый раз, плюс, соответственно, к этому методу добавляется
так называемое моментумно-ислагаемое.
Вот.
Физика у этого безобразия более чем понятна.
Вы говорите, что теперь у вас у метода в некотором
смысле есть не просто какие-то точки, где он находится,
а есть еще и в некотором смысле масса.
То есть, вы находитесь в точке, и метод как бы двигается
от точки к точке, и вот у шарика, пусть бы, раз, потому
что метод называется метод тяжелого шарика, у шарика,
который как бы иллюстрирует ваше движение метода, есть
инерция.
Вот.
Есть некоторая инерция, и когда вы его хотите, например,
толкнуть по градиенту в текущей точке, вот, эта
инерция будет играть роль.
Ну и, соответственно, эта инерционно-ислагаемая
в некотором смысле будет продлевать вашу траекторию.
Вот.
И вы будете двигаться, а, частично по градиенту, по
антиградиенту, как вы и сказали, вот, как вы и попросили
у метода.
Плюс, у вас будет сохраняться инерция от предыдущих
шагов.
Вот.
Вся идея очень естественна, очень понятна, и на самом
деле она довольно-таки неплохо работает.
Вот.
На первой картинке, соответственно, у вас сходимость обычного
градиентного спуска, находите градиент в текущей точке
и шагайте.
Вот.
А на второй картинке у вас, соответственно, изображен
метод тяжелого шарика.
То есть, что здесь происходит?
Смотрите, вот это у нас градиент, как и на предыдущей картинке,
вот это градиент.
Вот.
А вот эта инерция, которая у нас осталась, вот эта инерция.
Вот.
И видно, что если мы как раз будем учитывать и инерцию,
и, соответственно, градиент, у нас метод будет двигаться
лучше и быстрее сходиться к решению.
Вот.
И аналогично у всех остальных точек.
Вот такая простая, в принципе, идея лежала в основе метода
тяжелого шарика.
Вот.
А, окей.
Почему?
Так.
Мы же знаем, что наши шаги при этом довольно маленькие
градиентного спуска, потому что мы шаг сделать довольно
большим не можем.
Потому что мы просто вылетим.
Ну, в некотором смысле, да.
Вот.
Но, как вы видите, у вас, как бы, траектория становится...
Смотрите, нет, смотрите, тут какая идея.
У вас прошлый градиент, если вы взяли довольно большой
шаг, он супер устарел.
То есть вы вот по этому направлению, по которому
вы шли, вот по этому направлению, вы уже минимизировались.
Ну, или очень хорошо минимизировались.
Вот.
То есть вы нашли вот эту точку, и вот это направление,
оно в некотором смысле уже плохое.
Вот.
А вот новый градиент, скорее всего, указывает в какое-то
довольно противоположное направление.
Вот.
И видите, что на диагонали может находиться что-то очень
хорошее при этом.
Вот.
Да, да, конечно, конечно.
Интуиция вот такая.
То есть здесь вы минимизировались хорошо, новый градиент
у вас указывает, скорее всего, в другое направление,
но при этом что-то среднее между вот этим.
Оно еще указывает в более классное направление.
В данном случае, прямо на картинке видно, что это указывает
более сильно в оптимум.
Вот.
На презентном презентажке, соответственно, есть ссылочка.
Можете ее тыкнуть и немного поиграться с методом тяжелого
шарика.
Вот.
Там, соответственно, можно менять шаг, можно менять
моментум.
Вот.
И посмотреть, как он сходится для разных задач.
Вот.
Да.
А что там такое?
Так.
Сейчас спустить надо.
Сейчас.
Так.
Смотрите.
То есть, в принципе, идея кажется естественной.
Вот.
Это может сработать.
Эта реальность и сработает.
Мы докажем, что этот метод реально работает классно.
Вот.
Исходится лучше.
Точнее, не этот метод, а его уже модификация.
Вот.
Хорошо.
Вот.
Так.
Ну, давайте тогда чуть-чуть порассуждаем.
Какие вообще видите плюсы и минусы в этот момент?
Проградиентный спуск мы в принципе не обсудили.
Вот.
Давайте про это хотя бы обсудим.
Вот.
Какие плюсы, какие минусы?
Вот.
Поверники, вы видите?
Вот.
Поверники.
Что вы видите?
Ну, хорошо.
Отлично.
Проверники.
Видите?
Вы видите?
В jeito, как мы видим?
В那 attainment.
плюсы, какие минусы у такого метода, который вы только что увидели. В том числе, может быть,
минусы какой-то градиентного спуска, на котором он по факту базируется.
Да, смотрите, то есть тут тоже хорошая идея. Если у вас градиент действительно сильно меняется,
или вы хорошо довольно выбираете шаг, так что у вас градиент, как вот на этой картинке,
сильно поворачивается. У вас, соответственно, видите, здесь могут быть довольно сильные такие
зигзаги. При этом, если мы посмотрим на траекторию, которая создает тяжелый шарик,
эта траектория становится более плавной. По той же ссылке можете пройти и поиграться. Например,
задать какой-то довольно большой шаг у градиентного спуска и поварировать моментум. Моментум маленький,
асцеляции большие. Моментум становится больше, асцеляции становится меньше. Да, это в некотором
смысле тоже игра. Моментум не самый очевидный параметр по подбору. Кажется, что нужно брать
довольно большим, но иногда слишком большой становится плохо. И вот это как раз минус метода,
то есть в градиентном спуске нам нужно было подбирать шаг. Это мы с вами не обсудили,
это в некотором смысле вопрос семинарских занятий, как подбирать шаг в методе градиентного спуска.
Ну и в том числе домашнего задания, где вам тоже немного с этим надо поиграться. А в тяжелом
шарике есть теперь не только шаг, теперь есть еще моментум, который нужно подбирать. И,
как мы понимаем, просто взять его нулевым не интересно, а достаточно большим уже плохо.
И это тоже в некотором смысле игра. Окей, что еще? Что еще можете выделить хорошего, плохого?
Да, то есть в градиентном спуске мы хранили точку, считали по ней градиент и обновляли х. А
здесь нужно хранить две точки, чтобы у нас был моментум. Вот, ну здесь есть какие еще минусы,
плюсы выделяю соответственно. Какая-то у него есть физика и интуиция, его довольно легко
имплементировать, то есть это не сложнее, чем градиентный спуск и довольно дешево вычислять,
если мы считаем, что градиент действительно вычисляется спокойно для данной задачи. Два
параметра, то что обсудили, ну и такой что ли ключевой вопрос. Это конечно здорово, мы придумали
новый метод, который может работать лучше. Вы в домашних зданиях в том числе посмотрите,
что он работает часто действительно хорошо. Вот, ну мы же как-то нацеливались на то, чтобы улучшить
оценки для градиентного спуска. Вот, а даст ли метод тяжелого шарика эти улучшения? Да.
Ну смотрите, давайте вот просто посмотрим, что там происходит. Вот, то есть здесь мы по факту
берем градиент в точке k-1. Ну это было гамма минус 1. Да, что-то такое? Правда или нет? Или там
что-то со знаком напутал еще? Тут минус еще, да? Минус. Вот, ну и смотрите, то есть кажется,
что во-первых, если бы вы сделали шаг больше, то вот эта точка у вас бы х поменялась. То есть это
была бы другая точка. Здесь мы как бы считаем ту точку, в которой мы как бы использовали просто
предыдущий шаг. Вот. Плюс соответственно, плюс соответственно, здесь у вас есть этот момент на
слагамме, который вам в том числе не дает сделать этот предыдущий шаг довольно большим. Вот. Тоже
некоторые отличия. Но главное отличие, конечно, вот в этой точке. И на самом деле точка, в которой
считается градиент, вот что в градиентном спуске, что в методе тяжелого шарика, что следующий метод,
который мы увидим, она является ключевой. Потому что вот ответ на тот вопрос, который я задал,
а вообще метод тяжелого шарика улучшает оценки в градиентном спуске или нет, ответ нет. Вот. То есть
в общем случае метод тяжелого шарика может быть не лучше, чем градиентный спуск. Вот. Ну и Борис
Теодорович придумал это в 1964 году и, соответственно, долго думал над этой задачей. А как тогда вообще
получить метод, который работает лучше? И можно ли вообще доказать что-то для метода тяжелого шарика,
что он в каких-то случаях работает лучше? Ну и до конца жизни на самом деле он в некотором смысле
интересовался этим вопросом. Даже после того, как показали, что в общем случае метод тяжелого шарика
не улучшает работу градиентного спуска и не является оптимальным. Вот. Все равно в каких-то
частных случаях Борис Теодорович это было интересно. Вот. Но был у Бориса Теодоровича ученик Юрий
Евгеньевич Нестеров, который, соответственно, через 20 лет изобрел следующий метод, так называемый
ускоренный градиентный спуск, часто называют методом Нестерева. Вот. Выглядит он следующим образом.
Кратенько, то есть это мой полный листинг алгоритма, кратенько его итерацию я вот изложил здесь. Вот.
Нестеров, соответственно, вот тяжелый шарик. Давайте посмотрим на отличия. Чем они между собой
отличаются? Вообще есть разница или нет между ними? Может тут просто Нестерева две точки берется и все.
Ну это новый шаг, который вот мы делаем, соответственно. Вот этот шаг. Как мне обновить
x и как мне обновить y? Вот. Кто понимает, в чем разница? Ну вот. Она вообще есть или нет между этими двумя методами?
Ну, индекс tau там тоже вроде как был. Супер. Смотрите. Я же как могу сделать? Я могу записать значение
для yk. То есть как у меня yk на предыдущей итерации обновлялся? Это xk плюс, соответственно, tau kt xk-xk-1.
Так? Вот. И теперь подставить вот сюда. Tau k-1. Что здесь тогда получится? Tau k-1 поставим. Вот. И здесь тоже.
Видно, что идея по факту очень похожа на тяжелый шарик. Вот это чисто как раз xk и плюс моментум,
который у нас уже был. Но вот здесь вот как раз точка в градиенте стала ключевой. Вот. Нестеров
добавляет моментум еще и в точку. Вот. То есть он просит пройти чуть дальше точки этой. Вот.
Насколько сильно? Вопрос. То есть в tau kt нужно еще подбирать. Вот. Tau kt-1. Вот. То есть да,
вот как раз в тяжелом шарике мы просто брали точку, по ней брали градиент и просто там моментом
использовали. А Нестеров эту точку еще проталкивает чуть дальше. То есть это в некотором смысле какой-то
взгляд в будущее. Как будто в том числе вы сделали чуть больший шаг методом градиентного спуска в прошлый
раз. Ну и точнее, в предыдущей итерации Нестерева. Вот. И нашли вот эту точечку. Вот. И эта идея,
что ли, стала реально очень крутой в плане оценок сходимости. И у, соответственно, Юрия Евгеньевича
получилось доказать то, что метод Нестера работает в общем случае лучше. Вот. Да. Да, давайте.
В плане xk-1 меньше, чем xk-1. Что значит меньше? Ну она может быть отрицательной. Ну это же
точки просто. Это какие-то точки, где у вас минимум вы не знаете. Что значит отрицательная?
Просто не понимаю тут. С какой точки зрения отрицательная? Вот. На самом деле вопрос довольно
интересный. То есть на самом деле вы увидите это в экспериментах, что метод тяжелого шарика,
что метод Нестерева, они не монотонны. Я не знаю, кто-то уже наверное делал домашнее задание и вы
видели, что градиентный спуск у вас, ну, линейно, как мы доказывали, сходится к решению. Вот. А метод
Нестерева, соответственно, и метод тяжелого шарика, ведет, например, себя по иксу не монотонно. То
есть он может увеличивать расстояние, а может уменьшать. Вот. Но в среднем как бы тренд ведет
сильно лучше, чем для метода градиентного спуска. Вот это как раз тот эффект, который
может наблюдаться, который, в принципе, правильно подметили. Вот. Что по факту вы же все равно в некотором
смысле делаете градиентным спуском, тоже об этом как раз тоже девушка говорила, чтобы шаг чуть-чуть
дальше. Это может быть нехорошо. Вот. Локально. Но глобально, оказывается, это хороший тренд. Вот.
Сделать чуть больше и как бы взглянуть чуть-чуть в будущее. Вот. Ну и сейчас мы это как раз подоказываем.
Метод Нестерева довольно, ну, по мне так, не то чтобы суперсложно, но по прошлым итерациям,
которые когда я его показывал, доказывается немного мутурным. Вот. И долго. Вот. Я поэтому решил взять
другой метод тоже, который достигает нужных оценок и показать его. Ну понятно, что в силу того,
что метод Нестерева стал суперпопулярен. Придумали кучу разных методов, которые также достигают
его оценок. Вот. И делают в некотором смысле такое вот ускорение. Вот. Ну и на самом деле довольно
забавная история связана, может не забавная такая, типичная для науки история связана с методом
Нестерева. То, что, в принципе, вот он его придумал в 83-м году и где-то до начала десятых про него
особо не вспоминали. Вот. Была эта классная теория, был этот метод, было доказано, что он классный. Вот.
А потом случился бум машинного обучения. Ну и так оказалось, что эти все сеточки классно обучаются
всякими методами с моментумом. Вот. HDD с моментумом, Adam это тоже метод с моментумом. Вот. И оказывается,
все эти моментумы работают классно. Тут все вспомнили, что оказывается, эти моментумы были
придуманы там 50 лет назад Поликом. Вот. Усовершенствованы там 30 лет, через 30 лет Нестеровым. Вот.
Здорово. Вот. Так в некотором смысле вот это все воскресилось и реинкарнировало. Вот. И сейчас эти
работы супер популярны. Вот. Именно вот эти все моментумы и ускорение. Просто потому, что это
выстрелило на практике. Для тех задач, которые еще не существовали в том виде, в котором они
существуют сейчас. И в тот момент, когда создавались методы, этих задач просто не было. Вот. А я, соответственно,
рассматриваю вот такой вот метод, который называется линейный каплинг. Вот. И суть этого метода такая,
то есть вы строите две последовательности. Одна у вас как бы в некотором смысле это последовательность
обычного градиентного спуска. Вот. Ну вот. Авторы это называют как бы ordinary step, ordinary step. Вот. Ну
мы увидим, что тут на самом деле шаг довольно будет стандарт для градиентного спуска. А это называется
в некотором смысле агрессивный step. Агрессивный step. Вот. И вы связываете вот эти две последовательности
между собой. Y и Z через точку X. И как раз эту точку X подаете в градиент. Вот. Интуиция метода
Нестерова более, что ли, понятна. Там эти моментумы здесь. Но она не совсем понятна. Но метод тоже даст
нужный результат. И чем он хорош, ну вот конкретно почему я его выбрал для лекции, у него довольно
простое доказательство. Которое мы сейчас попробуем и воспроизвести. Выход, да, да, да. Выход, то есть в качестве
выхода вы возвращаете среднюю точку. Среднюю. Сейчас поймем почему. То есть пока не последнюю. То есть раньше
обычно была последняя и мы доказывали, что по последней точке все хорошо. Вот. А сейчас поймем почему среднюю.
Ну вот видите, тут еще как бы в некотором смысле проблема, что по средней точке на самом деле вот, я не знаю,
смотрели конспект или нет, там можно доказать в принципе сходимость градиентного вспуска по
средней точке. Она будет симпатически такая же, как и для последней точки. Вот. Более того, в случае,
видите, ускоренных методов, как я говорю, там сходимость, она не монотонная. Вот. И часто средняя
точка, она даже лучше, чем какая-то последняя, которая могла быть у вас на пике. То есть там сходимость
вот такая иногда. Вот. И взять вот эту точку, ну часто не так хорошо, как взять какую-то среднюю,
которая находится, ну вот, где значение функции может быть ниже даже. Вот. Но главное тут понимать,
что асимпатически средняя точка не хуже, чем, ну именно с точки зрения О большого. Она даже
для градиентного вспуска не хуже, чем последняя. Можно часто имирать минимум, это правда. Вот. Но нам
понадобится средняя точка, для нее как раз с доказательством просто все хорошо схлопнется. Да.
Да, на самом деле вот современным моментом они основаны на этой идее, что вы берете инерцию,
домножаете ее на коэффициент старую инерцию. У вас как бы старая инерция убывает по геометрической
прогрессии. Вот. Сейчас рассматриваем такие методы просто потому, что а. это были первые методы,
б. хочется для них что-то доказать. Вот. Для ваших методов тоже можно доказать, но чуть посложнее. Вот.
К сути опять же вот этого метода, почему я его выбрал, для него получится что-то доказать. Вот.
Для Нестера тоже конечно все получается, просто доказательство будет чуть длиннее. Вот.
Да. Так. Так. Среди лит на к, это вы в каком случае или что?
Ну смотрите, давайте вот просто порассуждаем, что например, видели оценку в выпуклом случае,
какая будет для метода Нестера? Что там будет соответственно у него? Вот. Ну не для Нестера,
для градиентного спуска. Вот. Один делить на к, да? Это соответственно номер итерации. Вот.
Если вы, то есть для каждой точки вы там можете гарантировать, что у вас соответственно она
на один делить на к ближе к решению. Так. Если вы соответственно это все усредните по к, от одного до
к большого, один делить на к. Вот. Сколько при этом получится? Ну сколько там примерно получится?
Логарифм на к, да? Ну не сильно страшно. Вот. То есть лишний логарифм, а на самом деле его даже можно
не убрать. То есть в оценке, которую мы получали для последней точки, можно получить и для средней
точки ровно такую же оценку, какая у вас приведена в теореме. У вас помнится вот так вот было, и кто
опять же читал конспекты, там соответственно вот что-то в таком духе у вас вылезало. Да? Вот.
Когда мы доказывали для сходимости в выпуклом случае. Ну и мы говорили, что у нас соответственно методы
убывают, соответственно у нас f от x ката постоянно уменьшается. Но можно уже воспользоваться
Янсоном, кто знает неравенство Янсона, когда у нас функция выпукла и есть среднее значение точек.
Да, поэтому вот это меньше чем f от средней точки. И поэтому сходимость по средней
точке она примерно такая же симпатическая. Вот. То есть в принципе проблем в этом особо нету. Вот.
Вот даже такая грубая оценка показывает, что логарифм вылезет и не страшно. Вот. А более точно
можно вообще показать, что и логарифм не вылезет. Вот. Ну смотрите, во-первых, у вас просто есть
градиентный спуск. Вот. А есть какой-то метод, который тут по факту тот же похожий на градиентный
спуск. Средняя? Еще раз, средняя точка она не плохая, не хорошая. То есть мы только что показали,
что для градиентного спуска по ней тоже есть сходимость. Вот. И сейчас мы тоже увидим, что средняя
точка будет более чем норм. Вот. Более того, на самом деле на практике часто какие-то средние
взвешивания и экспоненциальные взвешивания дают даже лучший результат, чем последняя точка.
Особенно в стахистических методах, когда у вас непонятно, что происходит. Вот. Средняя,
последняя точка может быть какая-то просто плохая, вас просто выбило куда-то не туда. Вот. А средняя,
она более устойчивая. Вот. И часто брать среднюю довольно хорошо оказывается. Окей. Смотрите,
давайте тогда вот этот слайд мы фиксанем. Метод и предположение, которое нам по факту понадобится.
L-гладкость и mu-сильная выпуклость. Вот. Вы их можете у себя открыть. Ну и дальше давайте
поехали доказывать сходимость. Вот. Что там происходит, соответственно? Ну давайте просто
вспоминать, как мы это делали для градиентного спуска. Что мы там делали с вами? Кто помнит,
что там для доказательства градиентного спуска нужно было делать? Ну как мы вообще начинали? Что
мы хотели оценить, во-первых? Расстояние до чего? До решения, да? Ка плюс первой точки до оптимума.
Здесь то же самое. Давайте попробуем это сделать. Я возьму точку z. Вот. Как для тори,
для которой я хочу померить расстояние. Посмотрю, как оно меняется. То есть в некотором смысле часто
доказательства, да и вообще придумывание метод, это творчество. Вот. Ну и здесь давайте возьмем точку z
и посмотрим, как оно меняется. Подставляем итерацию. Давайте вы мне поможете. Что там с итерацией?
Что там? Как z обновляется? f от x-ката. Давайте я сразу гаммы сделаю постоянными. Нам потому,
что они каты не нужны. Там можно их не варьировать со временем. То есть у нас получится результат и
так. Вот. Чтобы эти индексы постоянно не писать. Вот. Я их сразу постоянными сделаю. Вот. Окей. Дальше
мы что раскрывали? Чтобы у нас вылезла z-ката минус x звездой. Вот. Дальше вылезало соответственно
скалярное произведение. Помнится, да? Вот. И соответственно z-ката минус x звездой. Вот. Плюс гамма в
квадрате fx-ката. Согласны? Супер. Вот. Смотрите, тут возникают какие-то вопросы. Помнится в
градиентном спуске, когда мы доказывали, у нас вот тут точки совпадали. Дальше мы пользовались
выпуклостью и все вроде как было хорошо. Сейчас мы это все поменяли из-за того, что мы хотим там
какие-то моменты мы накручивать и так далее. Брать эти выпуклые комбинации. Ну тогда давайте
здесь это тоже пофиксим, чтобы точка совпала и как бы там мы уже были ближе к выпуклости. Вот.
Я здесь сделаю одинаковую точку. Вот. Но за это, понятно, мне нужно будет расплатиться дополнительным
скалярным произведением. Вот. Окей. Дополнительное скалярное произведение. Будет выглядеть
следующим образом. То есть вы два эти скалярных произведения сложите, у вас получится нужный
результат. Так? Ну давайте посмотрим еще раз. Вот. Я презентацию в чат скинул, вы можете на
телефонах там открыть, смотреть. Вот вам нужен по факту вот этот слайд. Алгоритм и два предположения
по факту, которым мы будем пользоваться. Вот. Окей. Выглядит так, что вот это в принципе нормально.
Это нормально. То есть тут как раз связь какая-то. Тут точка другая, конечно, скалярное произведение,
но скорее всего будет все хорошо с этим безобразием. Вот. Вот с этим проблемой нужно как-то оценивать.
Ровно такие же проблемы, которые у нас по факту возникали и в градиентном спуске. Как-то оценить
норму градиента, как-то оценить скалярное произведение. Вот. Что мы для этого применяли? Давайте
вспомним. Как там норму градиента нам, например, оценить? Что там мы делали, чтобы оценить норму
градиента? Эль-гладкость. Вот. Соответственно для нормы градиента часто как раз применяется
эль-гладкость для того, чтобы ее оценить как-то. Мы с вами в градиентном спуске как делали? Мы
оценивали с помощью этого. Добавляли ноль, умный, и соответственно оценивали. Вот. Здесь я чуть
сделаю по-другому, похитрее. Воспользуюсь только вот тем свойством, которое я выписал до этого.
И для точки Y, потому что в принципе для точки Z мы уже много чего вытащили. Вот. А здесь я хочу как
раз эль-гладкость, плюс использовать то, что в YK там тоже используется апдейт с помощью градиента. Вот.
Я хочу записать свойства гладкости в двух точках. YK плюс 1 и соответственно, какой второй? YK.
Второй. Вот. Давайте попробуем. Или, точнее, лучше. Нет, XK. То есть, чтобы вот у меня вылезла разность
вот этих безобразий. Вот. Давайте попробуем просто записать гладкость в точке YK плюс 1 и XK. Как
вот это будет выглядеть в этом виде? Вот это не равенство, если я буду подставлять вместо Y,
YK, а вместо X и XK. Как будет выглядеть?
Сюда? От первой строчки ко второй. Я просто раскрыл квадрат.
Ну, A плюс B в квадрате равно A в квадрате. Ну, плюс 2AB плюс B в квадрате. Все. Так,
как будет гладкость выглядеть? Так. Дальше, соответственно, какая точка тут будет? XK или какая?
YK плюс 1 минус XK. Вот. Чем вот такое неравенство хорошо? Вроде бы я что-то просто записал гладкость
в двух точках. А хорошо на тем, что когда мы вот сюда начнем подставлять градиент, ну, точнее,
как мы обновляем YK, здесь вылезет градиент с нужным коэффициентом и с минусом. И здесь тоже. Они
повылазят. Так. Тогда здесь все получится. YK плюс 1, XK. Ну, эту технику вы, в принципе,
видели и в доказательстве выпуклого случая, кто смотрел конспект. Вот. Так. И здесь, соответственно,
от XK в квадрате. Окей? Да. Здорово. Просто в гладкости
то же самое. Вот оно. Ну, смотрите, главное помнить физический смысл. Сильная выпуклость вам снизу
функцию ограничивает параболой. Вот. Ваша функция может как-то реально вести себя как-то вот так.
Ну, я просто рисую. Там у вас были пунктирные линии до этого. А гладкость у вас ограничивает ее
сверху. Тоже параболой. Параболойдом. Вот. Поэтому вас видите в два направления неравенства. Внизу
как бы параболой от сильной выпуклости, а сверху параболой от гладкости. Вот. Соответственно, да,
расписали. Здесь это все можно сгруппировать и получить следующее выражение. Вот. И отсюда я
хочу вытянуть норму градиента. Оценку на норму градиента. Вот. Ну, значит, если мне, а причем
оценку сверху, потому что нам вот здесь его нужно было оценить сверху. В верхней линии. Так. Вот. Как
это соответственно вытащить? Как это вытащить? Вот. Как это вытащить? Ну, давайте перенесем в правую
часть. Вот. И разделим. Разделим на тот коэффициент, который у меня стоит перед нормой градиента.
Могу ли я всегда делить на коэффициент перед нормой градиента? Ну вот, перед нормой градиента у
меня стоит вот этот коэффициентик. Вот. А вот будет ли вот это неравенство всегда правдивым,
если я буду делить на этот коэффициент?
Нужно потребовать что? Чтобы этот коэффициент был положительный. Все понимают, да?
Если у вас коэффициент будет отрицательный, у вас неравенство просто
поменяет знак. Тогда что нам нужно потребовать? Какие у нас должны быть это,
чтобы неравенство было хорошим? Ну вот это соответственно. Вот это, чтобы этот
коэффициентик у меня был положительный. Сколько? От нуля до двух делить на L, так?
Вот. Все. Вроде бы получили оценку на норму градиента, которая успешно может
встать вот в это неравенство, которое я до этого сделал. Так, давайте это спустим
тогда вниз.
Так, что нам здесь? Это нам уже не нужно. Это мы уже все подоказывали. А вот это
как раз мы поднимем. Вот. И подставим соответственно сюда вот эту норму
градиента. Здесь у нас получится что? Соответственно разность 2 делить на
это 2-nL. Так, и здесь будет разность функций f от xk минус f от yk плюс 1. Вот.
Окей. Вроде бы все норм. То есть с первым кусочком разобрались. Пока лучше как так-то
и не выглядит. Какие-то точки x непонятные, точки y, они здесь разные. Вот.
Ну давайте попробуем дальше разбираться тогда со скалярным произведением.
А, да. Здесь соответственно уже становится неравенство. Всего того, что мы сложили
два неравенства, мы уже использовали неравенство. Второе это неравенство, которое мы
здесь получили. То теперь вот так вот. Вот. Здесь все понятно? Тогда двигаемся к
скалярному произведению, к скалярному произведению. Вот. Скалярное произведение f
xk, что там, zk, минус xk. Вот. Ну и смотрите, я вот когда-то что смотрю, вот мне здесь
реально вот в этом то, что потихоньку получается, есть x, есть y. Вот здесь везде
z-ки уже стоят, здесь x. Вот. Точка y выглядит лишней. То есть либо ее нужно использовать,
то есть тут как у два варианта. Вот. Кажется, что вот здесь вот, когда мы все равно сейчас
будем что-то вытаскивать, а как-то оценивать через гладкость или выпуклость, ну точка z
вылезут значения функции. Вылезут значения функции, и значение функции в точке z, оно
будет лишним, потому что здесь у нас есть x и здесь есть y. Опять же, это не какая-то строгая
интуиция, почему нужно доказывать так. Вот. Потому что часто, ну это доказывается в некотором
смысле путем пробы ошибок. Вот. Вы просто пробуете какое-то неравенство, получается с помощью него
что-то оценить или нет. Вот. Ну вот здесь, соответственно, давайте попробуем это скалярное
произведение с минусом. Сделать так, чтобы у нас вылезло после применения там гладкости
либо выпуклости y. Вылезли y. Вот. Как это сделать? Как вот избавиться от z вот здесь?
Что у нас есть, чтобы избавиться от z? Смотрите, мы уже использовали одну строку алгоритма,
использовали вторую строку алгоритма. Есть еще одна строка, которая связывает z, x и y.
Супер. А как будет выражаться z-каты? Через y и, соответственно, x. Вот.
Ну давайте tau просто буду оставлять, опять же, умножить на что? Один неделительный tau.
x-каты. Плюс. Минус, да, здесь? Минус. Один минус tau или что? Один минус tau на y-каты.
Вот так, да? Вот так. Правда ведь или нет? Вот. Я надеюсь, что правда. Если не правда, мы не дойдем.
Вот. Хорошо. Вот. Тогда я подставляю сюда вот это выражение, которое продиктовали, и получается здесь
что? x-k минус один минус tau y-k минус tau x-k. Так. Ну я просто tau вынес за скобки. Вот. Один
делительный tau за скалярное произведение. Поэтому здесь у меня tau еще возникло. Вот. И вроде бы,
кстати, сейчас хорошо получится, потому что вылезет как раз еще дополнительный коэффициент вида один
минус tau. И здесь будет что? x-k минус y-k. Так. Вроде все норм. Вот. Так. Ну теперь давайте,
раз уж что-то хорошее появилось, скалярное произведение, которое кажется, ровно которое
нам и нужно, давайте попробуем как-то оценить. Например, с помощью выпуклости. Как вот такое
скалярное произведение со знаком минус оценить? Давайте выпуклость. У вас сильно выпуклость даже
выписано. А? Сколько будет? f от x минус y-k. Я даже mu писать не буду, и она будет как бы большим
просто каким-то. Ну а с минусом она будет. Да? Вот. У вас она там с минусом вылезет mu, ну то есть
вот вылезет у вас минус mu пополам y-k минус x-k. Так? В квадрате. Вот. Ну я это просто скажу,
что это все равно меньше нуля, поэтому в оценке это можно даже не писать. Вот. Ну мне сейчас она
просто не нужна, не понадобится. Вот. Окей. Оценили скалярное произведение. Оценили скалярное
произведение. Так? А, да, неравенство, конечно. Неравенство. Вот. Получили довольно хороший кусочек,
ну что-то оценивали. Вот. Пора его подставлять. Вот в то, что мы уже наполучали до этого.
Так. Надо это все в страничке. Вот. Это скалярное произведение. Тут еще было два гамма. Два гамма.
Вот. И что у нас там наполучалось? 1-tau делить на tau f от x-k минус f от y-k. Так? Вот. И смотрите,
уже что-то вырисовывается довольно красиво. Я увеличу. Вот. Сейчас. Так. Оценка скалярного
произведения. Точно правильно выписали точки? Минус должно быть, да, здесь быть? Вот. Тут
должен быть минус. Соответственно, здесь этот минус тоже есть. Вот. И видно, что хорошо получается.
Я вот от этого x-а смогу избавиться. У меня будет разница y стоять просто. Y предыдущий и минус y
следующий. Да? Но при условии, что я подберу правильно вот эти коэффициенты. Я же могу
настраивать метод так, чтобы у меня эти коэффициенты были равны. Но я в частности это и запрошу,
чтобы у меня вот эти два коэффициента при настройке метода, ну, были между собой равны.
Тау. Вот. Скалярное произведение тогда давайте я подсотру уже. Вот. Мы потихонечку как раз будем
переходить к подкатним кусочкам. Здесь у нас что будет? Здесь у нас будет… Мы потребовали,
чтобы у нас все совпало. Так мы настроили параметры. Например, выбрали тау. Так,
чтобы было выполнено соотношение. И тогда сверху у меня что будет? zk минус x звездой минус 2 гамма f
xk. xk минус x звездой. А здесь соответственно в скобочках ну давайте запишу в виде 2 гамма это 2
минус это l. Вот. А здесь у меня будет соответственно f от yk минус f от yk плюс 1. Окей? Вот. А теперь
смотрите. Дальше уже довольно просто. Переносим в левую часть 2 гамма f от xk. xk минус x звездой.
Такое вы уже видели, когда мы доказали для выпуклого случая. То есть влево переносится
скалярное произведение. Вот. А вправо остаются соответственно разности.
Так. Вот. Что дальше делаем? Дальше просто суммируем по всем k. Так. От нуля до последней
итерации. Согласны? Угу. Супер. Так. Я сразу усредню еще. xk k минус x звездой. И здесь что у меня будет
в виде 2 гамма f от yk минус f от yk минус xk минус xk минус xk минус xk минус xk минус xk минус xk
минус xk.
Как раз у х нулевое минус x звездой меньше либо равно чем что-то. f х нулевое минус f от x звездой.
Там представляемое произведение, но почему я его не пишу сразу же? Кто понимает, почему я не писал
соответственно, да, я оценил разность функций и получил следующее выражение. Ну вообще выглядит как
сублинейная сходимость, потому что 1 делить на k все кажется плохо, но не все так печально. Почему?
Потому что, во-первых, я что дальше подбираю? Подбираю правильно параметры это и гамма, чтобы вот это выражение,
которое у меня здесь стоит, минимизировать. Правильно подобрать это и гамма, соответственно, оптимально это в данном случае
это просто 1 делить на l, чтобы вот вторая дробь, которая написана была минимальна. Вот. Я ее подставляю.
Сейчас чисто технические вещи идут. Вот. А дальше гамма подставляю, чтобы вот подобрать вот насколько вот, чтобы у меня
теперь сумма двух дробей была минимальная. Гамма, соответственно, нужно подставить равное,
гамма делить на 1 делить на корень из двух mu l. Это просто через производную выводится
найти минимум функций по гамма и найти там минимум функций по это. Вот. Тут ничего сложного не должно быть.
Я подставляю и получаю вот такое вот выражение.
Где, где, где?
Сократилось? Да, мы пользовались. Это не равенство. Было у нас...
Что мы предполагали? Мы предполагали, что это у нас от 0 до 2 делить на l, так? Это раз.
А что мы еще предполагали? Это правильный вопрос. Мы предполагали вот это. Да?
Ну, смотрите, на самом деле, что это. Это получается 2 гамма, это 2-nl, 1-tau делить на tau, так?
По факту это... так мы задаем tau. Видите? Здесь мы можем задать tau так, чтобы выполнилось вот это значение.
Зная гамма, зная это, можно решить это уравнение и найти tau. Вот.
Все. То есть вот это было на самом деле ограничение на tau. Вот. То есть тут проблем с этим нет.
Здесь мы все это прогоняем, прогоняем, прогоняем. Вот.
И получаем вот такое вот выражение, которое я подчеркну. Вроде бы кажется все плохо,
сублинейная сходимость, значит, мы скорее всего проиграли, но на самом деле нет.
Вот. Давайте возьмем k вот таким, чтобы у меня просто вот тот коэффициент, который стоял здесь,
но он стал в итоге 1 и 2. Ну, k подберем. Понятно, он подбирается l, делить на mu, корень,
и там восьмерка еще встает, чтобы одна вторая встала. Так?
Это что означает? Что получается, что мы его запустили, вот этот алгоритм наш линейного каплинга,
на k итераций, и можем гарантировать, что вот эта точка средняя, которую мы выдали как выход,
она в два раза нас приблизила к решению. Так? Вот.
Но мы же тогда этот каплинг можем перезапускать, брать как новую стартовую точку вот эту точку.
И тогда вот эта точка у нас перекочует влево, вправо, и мы тогда еще в два раза сократим расстояние.
Вот. Получается в некотором смысле такая рестартовая процедура. Нашли новую точку,
ее взяли как стартовую, заново запустили каплинг.
Вот. Получается, что вот если вы запустите этот каплинг на t итераций,
то вы получите, что через t вот этих больших как бы эпох, t раз на k итераций вы его запустите,
вы расстояние до решения приблизитесь там на 1 делить на 2t.
Вот. Ну а дальше вы что, найдете t, потому что вам просто нужно там подобрать точность епсилон,
вы по функции должны сойти с точностью епсилон, соответственно t равно вот такому значению.
Просто пролагарифмировать. На логарифм двойке. Угу. Вот.
А дальше все. Вы каждую итерацию вызываете градиент один раз.
Каждую большую эту эпоху вы делаете k итераций, то есть k вызыва градиента,
плюс эпох вы делаете t, и того вызыва градиента k умножить на t.
Так? Окей?
Вот. Получается, что оракульная сложность именно количества вызова градиента метода,
который мы получили, будет корень из l делить нами.
Вот. А у градиентного спуска просто l делить нами.
Так? Вот. Это, в принципе, то, к чему стремились, получить что-то лучше.
Вот. Соответственно, оценочка уже в виде теоремы все это выписано, параметры подобраны, k выбрано,
tau здесь еще, на tau забыл ограничение написать, ну вот то, которое мы добавляли, 1-tau, равно чему-то там.
Вот. Ну, в общем, получается вот такая оценка, которая лучшего градиентного спуска.
Вот. Но между тем и даже к этой оценке остаются вопросы.
Потому что, да, она лучше, но можно ли еще улучшить?
Взять не корень, чтобы было из l делить нами, а корень четвертой степени, например.
Вот. Как нам вообще понять то, что полученная оценка, например, не улучшается?
Что нужно делать?
Да. Нужно получить нижние оценки. Вот. Нужно получить нижние оценки.
А в чем суть нижних оценок?
Кто помнит, в чем суть нижних оценок?
Наоборот, самый худший пример. Самый худший пример, что любой алгоритм работает плохо на нем. Да?
Вот. Ну, давайте попробуем придумать такую задачу, на которой любой алгоритм будет работать плохо.
Вот. Но вообще вопрос такой. А что значит любой метод?
Ну, вот я сказал, любой метод. Что значит любой метод?
Ну, вот мы же с вами уже выводили в некотором смысле нижние оценки.
По методу мы строим задачу.
Мы же как-то описывали, что такое любой метод. Мы явно не брали не все методы, а какие-то ограниченные.
По оракулам мы их ограничивали.
Помните, мы тогда рассматривали методы, которые оперируют информацию нулевого порядка.
И здесь то же самое. То есть хочется ограничить метод как-то по оракулам.
Первое условие довольно простое, что у нас у метода есть в некотором смысле какая-то начальная точка.
То, откуда мы стартуем, соответственно она формирует то множество достижимости, в которые мы пришли.
То есть точки, в которые мы умеем доходить методом.
Как формируется? Что мы можем спрашивать в соответственном функции?
Мы можем спрашивать у оракула, можем спрашивать градиент.
Ну, как раз мы рассматриваем методы, которые умеют работать с градиентом.
Спрашивать мы можем только в тех точках, которые мы уже достигли.
Что в принципе логично, как работает градиентный спуск.
Но самое интересное, как мы считаем множество достижимости.
Это линейная комбинация из того, где мы уже были, и градиентов в этих точках.
То есть в принципе это предполагает, что мы можем делать любую шагу градиентного спуска, использовать любые моменты.
Главное просто считать линейную комбинацию из градиентов, точек, и, соответственно, иксов, которые мы уже достигли.
Просто линейная комбинация, не важно с каким коэффициентом.
Понятно, что это в некотором смысле вещь ограничивающая.
Ну и в качестве выхода у нас просто какая-то точка из МК берется.
После К вызова оракула мы, соответственно, говорим, что вот то, что лежит в МК,
мы выбираем отсюда какую-то точку, из линейной оболочки градиентов, иксов, и выдаем ее как ответ.
Подходят ли методы, которые мы изучали под это определение?
Да, подходит. Не зря же мы его рассматриваем.
То есть мы как раз берем градиенты в точки, которые мы уже достигли, считаем в этих точках градиент,
и дальше следующую точку строим как некоторую комбинацию из старых точек и градиентов.
Да, подходят, но, соответственно, все ли методы здесь учтены? На самом деле нет.
То есть есть операции, когда мы можем вызывать градиент, но делать что-то кроме линейных комбинаций.
Например, брать какие-то скалярные произведения градиентов и иксов.
Вроде бы кажется какая-то непривычная комбинация, действие.
Но оказывается, что такое действие, если его разрешить, нам оно сейчас запрещено.
Может ускорить метод и достигать более высоких скоростей сходимости.
Но мы сейчас работаем в таком что ли сетапе.
Мы ограничили методы не просто тем, что мы можем вызывать градиенты.
Мы можем считать только линейные комбинации градиентов и точек.
Вот так вот ограничили. И в этом классе будем работать.
Плохая проблема. Тоже результат не из сервиса, соответственно.
Выглядится плохая проблема следующим образом.
Квадратичная задача, как ни странно. Даже на квадратичной задаче может быть все плохо.
Квадратичный кусочек плюс еще один квадратичный кусочек, который завязан на константу mu.
Плюс вот такой вот линейный кусочек. Поймем, зачем он потом будет нужен.
Смотрите, я не буду проверять, что эта задача является...
l гладко сильно выпукла. Она действительно такой является.
Она l гладкая, mu сильно выпукла.
У вас в домашнем задании в том числе есть задача, как для квадратичной задачи
оценить константу липшица l и константу сильно выпуклости mu.
Для этой задачи ровно то же самое делается. Как оценить l и как оценить mu.
Единственное, что может быть нетривиально доказать, что у матрицы A она ограничена четверкой.
Вот такая матрица, 4i и минуса, будет положительно определена.
А снизу у вас положительно полуопределена эта матрица.
Этот значок обозначает, что у вас матрица положительно полуопределена.
А такой значок обозначает, что у вас матрица такого вида будет положительно полуопределена.
Получается вот такая матрица, пока не понятно, что с ней делать, даже не понятно, какой она размерности.
То есть Нестер говорит, давайте не париться, возьмем размерность равной бесконечности.
Можно взять и похитрее, взять конечную размерность.
На самом деле, суть не меняется, бесконечность даже получше будет в плане сути, а не каких-то тонкостей и доказательств.
Окей, давайте думать, что происходит, почему вообще вот такую проблему хочется рассматривать.
Смотрите, исходим из предположения, что у нас начальная точка это ноль.
Это в некотором стандартном ограничении, что мы стартуем из нуля для нижних оценок.
Понятно, можно взять какую-то другую точку, но у вас задача, можно тогда сместить по оси координат, что вы будете всегда стартовать из нуля.
Стартуем из точки ноль-ноль.
Скажите мне, а чему вообще равен градиент этой функции?
Вы опять же можете открыть себе слайд с этой функции и продиктовать, чему равен градиент этой функции.
Совсем простая квадратичная задача.
Чему там равен градиент у нее?
Кто был на семинарах, где вы дифференцировали квадратичные задачи?
Сколько там будет?
Коэффициент.
Плюс, минус, там минус.
Е. Согласны? Это градиент.
А давайте посмотрим, что будет, если мы в этот градиент, но мы же посмотрим, что происходит.
У нас изначально дана просто нулевая точка, мы в ней по факту и только можем считать градиент.
А что происходит с градиентом? Чему он равен, если точка ноль-ноль подставлена?
Если я ноль подставлю, чему вот равен градиент?
Последнему просто вот этому вещь.
Я просто давайте выпишу, градиент пропорционален E1.
То есть в градиенте будет только первая не нулевая координата, так?
Согласны?
Получается, когда мы возьмем линейную комбинацию х0 и градиента,
у нас в этой линейной комбинации будет сколько не нулевых координат?
Только одна, так?
Только одна не нулевая координата.
Причем первая, да?
Согласны?
Идем дальше.
А теперь посчитаем градиент в точке x1, например.
Где x1 принадлежит множеству...
Давайте я так и запишу.
Линейная оболочка, где первая координата не нулевая.
Мы же как раз ее и достигли.
Мы могли после первого вызова получить любую точку, где первая координата не нулевая.
Все остальные нулевые.
По-другому никак.
Посмотрите на вид матрицы.
И подумайте, а что получится тогда?
Что можно сказать про не нулевость координат следующего градиента?
Матрица, вот она.
Она не зря у вас такой имеет странный вид, такой диагональный.
Что будет, если я возьму вектор, где только первая координата не нулевая и домножим эту матрицу?
Где могут быть не нулевые координаты только?
Смотрите, я домножаю на вектор, где у меня вот эта координата не нулевая.
Получается, что первая может быть не нулевой и вторая.
Получается, что в новом градиенте только первая и вторая координаты могут быть не нулевые.
А значит, я апдейт, который получу, взяв там х1, изм1 и градиент х1, стиль, например.
Получается, что вот это максимум только две не нулевых координаты.
Согласны?
И так далее.
Дальше вы будете размораживать одну координату и видеть, что у вас теперь максимум три не нулевые координата.
Получается, задача имеет такой вид, что вы за один вызов оракула в выходе алгоритма добавляете одну не нулевую координату.
Супер. Это важное замечание, которое мы могли найти.
В лучшем случае.
Когда вы говорите про решение, то ваш алгоритм может угадать первые координаты.
Если вы вызвали оракула х1, то первые координаты он может угадать в точности.
А все остальные он не угадает. Согласны?
Просто потому, что у него там ноль.
У него там ноль, и он не может никуда их двигать.
Поэтому давайте поймем, как выглядят решения этой задачи.
Чтобы понять, а что там происходит в координатах.
Где у нас выход будет ноль, а координаты реально не ноль.
Насколько там много мы упустим, если мы вызовем оракулу всего к раз.
То есть первый вывод мы сделали.
К вызовов оракула равно к не нулевых координатах.
Теперь давайте находить решение х звездой нашей задачи.
Как будем находить?
Градиент 0, правильно. Почему это рабочая схема?
Задача у нас сильно выпуклая.
Значит решение единственное и уникальное.
Хорошо, градиент равен 0.
Поищем решение, поищем решение.
Давайте выпишем это все безобразие, потренируемся.
Градиент равен 0. Градиент я уже вверху вот выписал.
Вот у меня градиент.
Кажется, что в силу вида матрицы уникальное что-то будет в первой строчке, в последней.
А во всех центральных будет одинаковая какая-то зависимость.
Давайте выписываем просто.
Для первой строчки что будет?
Когда я буду брать градиент по первой строчке?
У чего у меня здесь будет L mu делить на 8 матрица A умножить на...
Что будет стоять в первой координате у градиента?
Вот здесь у этого градиента в первой координате что будет стоять?
2х1-х2 плюс mu пополам на что?
На х1.
Точно так?
Или что-то все же теряю?
Все нормально, да?
Вообще в градиенте мы на самом деле потерялись,
потому что здесь же когда мы квадратичную задачу дифференцируем,
двоечка-то вылезает еще.
Поэтому здесь вот этих нету двоечек.
Здесь еще вот такая добавочка.
Согласны?
Единичка.
Просто единичка от вот этого единичного вектора.
Получается вот такая вот зависимость.
А теперь давайте выпишем все остальные строки.
Что там будет получаться?
Минус х...
Ну давайте я буду вот так.
И минус один писать.
Плюс х и т.
Два х и т.
Минус х и т плюс один.
Можно даже без скобочек.
И так понятно, что это координата,
потому что индекс мы вверху пишем.
Итерация.
Здесь, соответственно, это уже не первая строка,
поэтому вот этого кусочка там не будет.
Останется только кусочек от mu.
mu х и t равно нулю.
Ну и там еще будет последняя строчка.
Чуть поподробнее я на слайде уже покажу,
чтобы долго с этим не возиться.
Вот, что это такое?
Как мне, например, находить х и т отсюда?
Кто понимает, что это?
Ну смотрите, а сверху вниз тоже непонятно.
Вот я вот тут вот, видите, застрял.
У меня х и т, х1 и х2 как-то завязаны.
Там еще последняя строчка просто есть.
Как вот найти связь, например,
исходя только вот из этой строчки
между всеми х?
Вот, кто понимает?
х2 можно через 1?
Ну можно, можно.
Вообще я намекаю на то,
что вот то, что у вас записано,
это же рекуррента просто.
Есть х и т, х и т плюс первая,
которая выражена через линейные коэффициенты,
х и т плюс бета х и минус первая.
Так?
Такие рекурренты вы не умеете решать?
Умеете.
Что нужно сделать?
Надо написать характеристический многочлен.
В духе лямбда в квадрате равно
альфа лямбда плюс бета.
Характеристический многочлен.
Находите его корни.
Ну и, соответственно, решение у вас
будет выписываться в виде
c какой-то константы 1,
корень 1 в степени
и плюс c2 лямбда 2 в степени.
Соответственно, так решается эта рекуррента.
Одно начальное у нас условие
будет здесь болтаться.
Второе начальное условие у нас
будет болтаться вот здесь,
потому что, видите, мы тут
замыкаем на вот эту z,
потому что, я как сказал,
все линии будут совпадать,
кроме последней и первой.
Решается эта рекуррента.
Давайте я ее не буду решать.
Я просто покажу ответик.
Пожалуйста, рекуррент выписал.
Соответственно, это для последней строчки
значения.
Можно, соответственно, подобрать
вот этот z, который я здесь поставил,
так, чтобы у вас в итоге в рекурренте
вылезло одно из значений лямда,
лямда катых, причем еще с коэффициентом c
равным единице.
Вот так вот z это подбирается.
И при этом q будет равно,
это один из корней многочлена
характеристического,
который меньше единицы.
Соответственно, можно подобрать z так,
чтобы у вас в итоге, по факту,
все вот эти x каты будут выражаться
вот в таком вот виде.
Это можно проделать как упражнение.
Линейную рекурренту вы уж сами решите.
Тут ничего сложного нет.
Согласен, это мы уж делать не будем.
Я думаю, с этим вы справитесь.
Там проходят, где вы проходите,
на комбинаторике или на...
Вот.
Так что...
Соответственно, справитесь,
это рекуррент,
и там ничего сложного нет.
Поняли, как примерно выглядит.
Дальше, соответственно,
вот это то, что мы с вами обсудили,
как меняется решение,
что мы приближаемся только на k-координат.
А дальше смотрите, какой трюк.
Давайте теперь я размерность подберу
и скажу, что у меня размерность равна 2k.
То есть, зная, например,
на период количества вызова фаракула,
я скажу, что у меня размерность
2 раза больше.
А зачем я так сделал?
Правильно.
Чтобы у меня k-координат
гарантированно были нулевыми в выходе.
То есть, k-координат первых
я в лучшем случае подберу,
а вот k последних координат точно не подберу,
потому что там гарантированно 0,
как мы с вами и обсудили.
Вот.
Хорошо.
Соответственно, да.
Тогда у меня начальное приближение к x.
То есть, это у меня просто 0.
Поэтому мне нужно просто посчитать
вот эти q и t.
То есть, ну, тут квадрат стоит,
поэтому здесь двоечка.
Ну, я выразил вот так вот.
Согласны, что это верно?
Я просто разбил на две суммы.
То есть, сумма была от 1 до 2k,
а осталось две суммы от 1 до k.
Так?
Ну, зачем я это сделал?
Просто удобно.
Здесь я что делаю?
Я хочу как раз оценить расстояние до решения.
Я предполагаю, что в первых k-координатах я попал,
а в остальных я уж точно не попал,
поэтому я суммирую все мои непопадания.
Вот.
Выношу коэффициентик, который тут общий будет, q в степени 2k.
Получается вот такая вот сумма.
Так?
Я вот к этой сумме просто и хотел привести,
поэтому я здесь ее расписал чуть похитрее.
Вот.
И тогда вот у меня расстояние до решения
можно оценить вот в следующем образе.
Угу.
Ну, тут простая алгебра у вас,
вот эту сумму выражается вот отсюда
и подставляется вот сюда.
Вот.
Но так как у меня q – это число меньше единицы,
я когда возвожу степень, это становится чем-то
все меньше, меньше, меньше единицы.
Поэтому я просто это могу оценить единицей
и сказать, что в худшем случае я здесь на двоечку просто разделю.
Так?
Вот.
Получилась вот такая вот оценка.
Дальше я подставляю q
и получаю уже итоговый результат,
мою вот эту верхнюю оценку,
мою нижнюю оценку.
То есть получается, что вот снизу
расстояние до моего решения ограничивается вот такой вот формулой.
Так?
Видно, что здесь возникают как раз нужны нам корни.
Корень из l делить на корень из mu.
До этого у нас что-то возникало в духе 1 минус
корень из mu делить на корень из l.
Ну, именно по оракульной сложности.
А здесь у нас как раз то же самое,
но подпирает нас уже снизу.
Да, это вот то, что у нас возникало,
оно нас подпирало сверху.
Вот.
А это у нас подпирает снизу.
Да, коэффициенты может быть какие-то другие.
Двойка какая-то возникла.
Здесь еще эта двойка.
Вот.
Но глобально это получается то же самое.
Вот.
Соответственно, получается, что мы с двух сторон подперли.
Единственное, что я еще отмечу,
что вот здесь вот критерии сходимости по x,
там у нас был по функции,
но опять же между ними можно переходить
в сильно выпуклую задачу,
у вас там будет просто появляться
дополнительный фактор в духе там mu на l,
или там будет появляться
дополнительный фактор mu на l.
Вот.
Логарифма-то не самое главное,
что нас волнует.
Нас волнует вот этот коэффициентик l делить на mu.
Корень из l делить на mu.
Получается вот такой вот схемой,
ну вот, идеи на самом деле довольно нетривиальные.
Вот.
Можно добиться того,
что как раз по координатам не сходитесь,
сколько это координат неправильных,
и там они соответственно оцениваются,
что вот l делить на mu
это то количество,
которое вам нужно вызвать.
Корень из l делить на mu.
Получается, что та оценка,
которую мы получили для линейного каплинга
с рестартами,
она правильная,
то есть она не улучшаемая.
Ну, с точки зрения там o,
до константа получается,
до константа она не улучшаема.
Какие-то численные константы могут быть в нижней оценке лучше.
Вот.
Вот к такому выводу пришли
довольно такой
хороший и жесткий вывод.
На самом деле можно повторить
то же самое для выпуклых задач.
То есть мы сейчас все это проделывали
для сильно выпуклых задач.
Исходимость и нижнюю оценку
можно повторить для выпуклых задач.
Ровно те же самые похожие примеры,
похожий анализ в пособии все будет.
Метод Нестерова
также имеет
такие же верхние оценки,
как и линейный каплинг.
Но в отличии, например, от линейного каплинга
его не нужно рестартовать,
что довольно хорошо.
Ну и, соответственно, да,
получается, что метод Нестерова
именно с точки зрения количества вызовов аракула
у нас является оптимальным.
Вот.
Это фундаментальный результат,
который, соответственно, Юрий Евгеньевич получил.
Очень крутой результат.
И, как ни странно, видите,
он всплыл в нейросетях
через 20-30 лет
после того, как
это все было получено
для выпуклых задач.
Ну и все.
На сегодня, соответственно, у меня все.

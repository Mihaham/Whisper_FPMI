Так, будем начинать, да, Колея? Совсем у нас немного, ну ладно. Значит, поздравляю всех с
прошедшим праздником, Днём Победы. Символично, что 79 дней назад победоносно завершилась
Великодейственная война, и мы сегодня с вами победоносно, надеюсь, завершаем курс теории
вероятности. Напомню вкратце, на чем остановились. В прошлый раз мы неким специальным образом ввели
вот такую нехитрую с виду функцию, которую назвали имперической функцией распределения. То есть
алгоритм такой, это важно как бы, ну, помнить, понимать. Сначала мы омеги поставили в соответствие
случайную последовательность, а потом на этой случайной последовательности при фиксированном
n, при любом n, ну, построили вот такую функцию. То есть мы к элементарному исходу поставили
соответствие функцию. Ну, кстати, если мы ставим числу, точнее говоря, элементарному исходу в
соответствие числу мы это называем случайной величиной. Если ставим вектор, называем случайным
вектором. Если ставим последовательность, называем случайной последовательностью. А если мы
элементарному исходу ставим соответствие в функцию, то это новый для вас объект называется
случайным процессом, который вы будете изучать в следующем семестре. Поэтому в
некотором смысле, но не в некотором, а, собственно, по определению, империческая
функция распределения – это случайный процесс, ну, специфичный, конечно. Вот. И мы с вами
сформулировали и начали доказывать теорему Гливенко, которая выглядит следующим
образом. В равномерной метрике для, мы оговорились, непрерывных функций
распределения, в равномерной метрике империческая функция сходится к так
называемой имперической, она называется почти-наверное, то есть с
вероятностью единицы. Значит, и еще мы успели в прошлый раз понять, что вот этот
супремум, ну, во-первых, это случайная величина. Почему это случайная величина?
Потому что вот этот супремум, решение задачи, так сказать, нахождения супремума
эквелинтно решению задачи нахождения того же супремума только навсюду плотно
множестве рациональных точек. Ну, и там вкратце повторюсь, значит, поскольку любые
функции распределения, в том числе и fn и fx в плюс и минус бесконечности равны нулю
единицы, соответственно, то точка супремума, ну, грубо говоря, где-то посередине.
Поскольку у нас есть, по крайней мере, непрерывность слева, то мы к точке
супремума можем построить сходящуюся последовательность. И уж тем более, не
тем более, и понятно, что мы ее можем построить из рациональных чисел всю
доплотную множество. Поэтому вот этот супремум, это вот этот вот супремум. А
здесь это уже супремум по счетному множеству, то есть dn случайно
не влечена. Вот на этом мы с вами остановились. Ну, дальше, в общем, не
сложно. Давайте вот такое введем множество или большое от k. Это в
пространстве элементарных исходов. Туда входят такие омега, что fn от xk
сходится к fx от xk. xk у нас это рациональные числа. Мы в прошлый раз тоже об этом
сказали. На самом деле, fn это среднеаррифметическая одинаково
распределенных случайных величин. Поэтому по теореме, второй теореме
Колмогорова, она для каждого x сходится почти, наверное, к своему
отожиданию. А мот ожидания fn от x, мы с вами это тоже получили. Это, на самом
деле, fx от x. То есть, вот эта сходимость имеет место, почти, наверное, по второй
теореме Колмогорова. То есть, это означает, что f от lkt равно единице для
любого xk, принадлежащей q. Следующий шаг. Введем множество l, которое
есть пересечение всех lкт. xкт принадлежит q. Счетное пересечение множества
единичной меры. Какова мера этого множества? Счетное пересечение множества
единичной меры. Единица. Поэтому мы получаем, что вероятность l, которая на
самом деле есть вероятность пересечения омега таких, что fn от xкт
сходится к fx от xкт, равно единице. А что значит это? Какие сюда
омега входят? Такие, на которых на всех рациональных числах имеет место вот такая
сходимость. На счетном множестве. Ну и давайте возьмем какое-нибудь омега из
вот этого множества z. Конкретно для этого омега мы получим последовательность уже
не случайных возрастающих функций ограниченных, которые сходятся навсюду
плотно множестве к некой функции fx от x. Но отсюда следует, что конечно и для этого
выбранного омега supremum по x принадлежащих xk принадлежащих q fn от xk
минус fx от xk тоже стремится к нулю. Принадлежащих к бесконечности. Это вот
для конкретного омега из z. Но омег здесь в z вероятностной меры единица.
Поэтому вот это равно единице, что и означает вот это утверждение. А следовательно
и это. Вот собственно доказательства теоремы Гливенко. Здесь нам от некой
мороки нас избавило то, что мы считаем функцию fx от x непрерывную. Собственно это и есть то
утверждение, как я в прошлый раз говорил, которое доказал Гливенко. Результат Кантелли в общем
относится к функциям распределения произвольного вида. Идеологически доказательства то же самое,
но только надо повозиться вот в точках разрыва. То есть с идеологической точки зрения это обобщение
ну как бы такое естественное вполне. Вот и если говорить, что fx от x произвольная функция
распределения, тогда этот результат называется теоремой Гливенко-Кантелли. Предысторию вопроса
усиления приоритетов я на прошлой лекции сказал. Вот. Ну оказывается, что это не все свойства вот
этой, еще раз повторюсь, очень простенькой на вид функции. Давайте еще одно полезное свойство
этой функции найдем. Я ее тут еще раз выпишу. Dn вот это вот такая случайная величина. Ну собственно,
кратко говоря, Dn сходится к нулю почти на верное. Вот что мы знаем. Но хотелось бы понять, что-то
о распределении этой случайной величины. Смотрите, вот индекс n здесь есть, ну потому что понятно,
что от n зависит вот этот супремум. А вот индекса, связанного со случайной величиной,
которая относится, здесь в обозначении нет. И это не случайно. Оказывается, что распределение
Dn не зависит от случайной величины x. То есть является универсальным. И именно это в дальнейшем в курсе
статистики позволяет активно использовать вот эту случайную величину в задачи так называемой
проверки гипотез. В статистике этот критерий называется критерием Колмогорова или Колмогорова
Смирнова. Ну немножко забегаю вперед, так сказать. Но еще раз просто скажу, что именно тот, ну вообще-то
говоря, удивительный факт. С чего бы это вдруг? Тот факт, что распределение Dn не зависит от x,
ну в общем вызывает некое удивление с моей точки зрения. Значит, мы давайте этот факт докажем. Но
для довольно широкого класса функции распределения, у которых существует обратное. Это небольшое
ограничение, потому что в принципе любую функцию распределения мы можем построить сходящуюся к ней
по распределению последовательность, каждая из которых будет обладать обратной. Ну чтобы обратная
существовала, она и так не убывающая, просто надо, чтобы она константе, да, нигде не равнялась. Вот,
значит, то есть это не сильно большое ограничение, которое не принципиально, еще раз повторюсь,
построив сходящуюся последность из функции такого класса, мы придем как бы к общему результату.
Значит, но перед тем, как доказать это утверждение, обращу внимание на следующий факт. Если он вам знаком,
вы мне скажите, я не буду тратить время. Если нет, то значит его докажу. Скажите, пожалуйста, вот если
я возьму, ну пусть вот такую функцию с обратной, у которой существует обратное, и функцию распределения
подставлю саму случайную величину, что я получу в результате? Ну, думал, может быть, как-то на
семинарах касались. Тогда давайте, это, кстати, тоже на вид очень непростой, но крайне важный факт.
Значит, ну давайте, это же какая-то случайная величина получилась, функция Борелевская,
подставили случайную величину, получилась случайная величина. Чтобы найти функцию распределения,
надо вот такую вероятность посчитать. f кси от кси меньше х. Ну, кстати, х теперь уже от 0,1,
да? 0,1. Ну, функция распределения сама от 0,1. Так, ну чего, есть обратное, поэтому это вероятность того,
что кси меньше f-1 кси от х. А вероятность того, что случайная величина меньше какого-то числа,
это же функция распределения по определению, то есть f кси, взятая в точке f-1 кси от х, то есть
в точности х. Ну, вот это вот функция распределения вот этой случайной величины. А что это за случайная
величина, функция распределения которой равна х от нуля до единицы? Это равномерная на 0,1 случайная
величина. То есть мы получаем вот такой, ну, удивительный факт. Если функцию распределения,
ну, вот с этим ограничением небольшим подставить саму случайную величину, то мы получим равномерную
на 0,1 случайную величину. На 0,1 случайную величину. И, собственно, вот этот вот замечательный факт
позволит нам с этой dn, что называется, разобраться. Так, ну, вот давайте теперь чуть
поподробнее это распишем dn. Значит, supremum по х. Вместо f-n от х напишу ее определение.
Ксикатая мы помним все независимо одинаково распределенные и имеющие функцию распределения
f кси. Вот так. И давайте сделаем замену вот такую. х равно f в минус 1 от u, где u уже будет
естественно от 0 до 1. Тогда мы получим, что это supremum. Ну, теперь уже по u будет supremum.
Пока 1n сумма k от 1 до n. А вот к этому условию применим монотонное преобразование. f кси от
х меньше f кси от х. Минус f кси, а вместо х f-1 от u равно supremum по u. Смотрите,
f кси от кси это равномерная на 0.1. Мы только что с вами показали. А f кси от х с учетом того,
что х равно f-1 у, это получится такое выражение. И вот так напишу u ка т. Это равномерная на 0.1
случайная величина. Меньше u. Вот замену произвел. Минус, но это u. Ну и смотрим внимательно,
что такое получилось. По определению. Это империческая функция распределения равномерная
на 0.1 случайной величины. Вот это империческая функция распределения равномерная на 0.1 случайной
величины. А вот это значение функции распределения. То есть на самом деле это мы получили ровно вот такое
выражение только в качестве кси равномерная на 0.1. Таким образом, какая бы ни была бы случайная
величина кси, случайная величина ДН представляет из себя вот это уклонение в равномерной метрике
имперической функции распределения, случайных величин, имеющих равномерное 0.1 распределение,
от гипотетической функции, которая в данном случае равна u для равномерного 0.1 распределения. Вот такой
замечательный факт. Вот такой замечательный факт. С другой стороны, ну что нам теория Могливенко
гласит? Что вот эта ДН сходится почти, наверное, к нулю. Поэтому какое бы ни было распределение
этой ДН, ее предельное распределение выглядит вот так. 0. Вот это, так сказать, предел, напишу, f ДН от x.
Предел по распределению. Поэтому о распределении асимпатическим случайной величины ДН от нам
ничего не говорит, хотя при каждых конкретных Н, пожалуйста, вот она есть, ее можно там посчитать,
и в целом можно даже для каждого конкретного Н это распределение построить. Но для того, чтобы
результат был более интересен, нужно эту случайную величину ДН промасштабировать соответствующим
образом или, как модно сейчас говорить, проскалировать. Оказывается, что если взять корень из Н ДН,
то эта уже случайная величина сходится по распределению к некой довольно сложной случайной
величине, обозначаемой К Большое, это случайная величина Калмогорова называется, поскольку
распределение корень из Н ДН было получено Калмогоровым. Не представляется в виде аналитической
функции, представляет из себя довольно хитрый ряд. Но, тем не менее, значит, если вот ДН просто
неинтересно ее распределение самой ДН, потому что она все равно сходится к нулю по распределению,
теорема Гливенко нам об этом говорит. Но вот если ее вот таким образом отмасштабировать,
корень из Н ДН, то получится, так сказать, случайная величина Калмогорова, которая, собственно,
и лежит в основе большого количества критериев проверки гипотез. Эта величина затабулирована,
в общем, такая важная случайная величина. Вот такие замечательные свойства у этой ну совсем
простой на вид функции, имперической функции распределения. И вы с ней еще там главным
образом в математической статистике столкнетесь неоднократно. А мы, значит, закончим
собственности аремы Гливенко и со свойствами имперической функции. Значит, сейчас мы перейдем,
в общем, к последней теме, которая вот с какой проблемой связана. Ну, мы с вами получили достаточно
много результатов сходимости, по вероятности, почти наверное, выполнения закона больших чисел,
усиленного закона больших чисел и так далее. Но вы знаете, как я уже в общем говорил, как и при
использовании каких-то таксотитерационных алгоритмов, нам недостаточно факта сходимости к решению. Ну,
точнее говоря, это тоже полезный и важный факт, но с практической точки зрения нам нужно что-то
знать о скорости сходимости. Иначе мы не понимаем, где мы находимся. Мы уже в пределе, или если мы
N еще увеличим, у нас опять все разойдется и потом где-то сойдется. Ну, кстати, в вычислительной
математике такие эффекты бывают, не знаю, знаете. Ну, может быть, даже на эту тему там даже есть
один совсем канонический случай. Ну ладно. Вот, связанный, кстати, с именем Академикой Белоцерковского.
Вот, значит, поэтому мы сейчас попытаемся чего-то сказать про скорости сходимости тех объектов,
средних там, нормальных 8-тики, которые мы с вами изучали. И напомню, что с какими-то там первичными,
если можно так выразиться результатами, мы в этой области сталкиваемся.
Значит, напомню вам неравенство Чебышева. Если у нас есть последовательность
независимых одинаково распределенных величин, то вероятность того, что кси N средняя отклонится от
своего от ожиданий на величину больше, чем ε, меньше равна чего? Дисперсия. Кси делить на ε квадрат,
но только еще умножить на N. Средняя. Ну, вообще говоря, вот, пожалуйста, пример скорости сходимости.
То есть сходимость по вероятности среднего значения к своему от ожиданию имеет порядок 1 на N.
И еще мы с вами, когда изучали асимпатическую нормальность, рассматривали неравенство Берри Эссона,
только там уже невероятность. Значит, если там выполнено условие того, что некая правильным
образом отнормированная случайная величина сходится к стандартному нормальному закону,
например, стремясь к бесконечности, то функция распределения... Ну, собственно,
сходимость по распределению означает, что поточечно функция распределения сходится к функции
распределения нормального закона. Ну, как бы поточечно, на несчетном множестве, как бы не очень,
так сказать, понятно. Поэтому на этот счет есть неравенство Берри Эссона, которое говорит нам,
что в такой знакомом стандартное обозначение, стандартная нормальная случайная... функция
распределения стандартной нормальной случайной величины, греческая в большое. Вот эта вот величина
будет меньше математического ожидания, кси центрирована в третьей степени, делить на корень из n
сигма в кубе. Сигма с средней квадратичной отклонения. Ну, вот еще как бы один пример оценки
скорости сходимости. Это мы вот с вами уже как бы сталкивались. Вот. Но сейчас мы двинемся дальше
в результаты, которые, ну, во-первых, имеют существенно более высокие скорости сходимости и,
как следствие, ну, более, более, так сказать, с практической точки зрения более важны. Вот.
Чтобы к этому перейти, я вам, что называется для затравки,
напишу такое неравенство. Значит, dn это вот оно,
то есть, уклонение равномерной метрики, вероятность того, что dn будет больше или равно
epsilon, меньше или равно 2 на e в степени минус 2n epsilon квадрат. Вот такое неравенство имеет
место. То есть, уклонение вот в равномерной метрике имперической функции от гипотетической,
это случайно и влечена, которая, естественно, по вероятности в том числе сходится, так сказать,
к нулю и имеет место вот такая скорость сходимости. Обратите внимание, что из этого
результата следует теорема Гливенко, потому что вот это ничто иное, как быстрая сходимость по
вероятности. То есть, вот этот ряд по n сходится, а это достаточно условие того, что dn сходится
к нулю почти наверно. Данное неравенство называется неравенство дворецкого Кифера Вольфовица,
такое тройное у него название, тройное название. И появилось оно существенно позже теорема Гливенко,
естественно, типа лет 30 назад, ну не знаю, 30-40 лет назад. И здесь мы опять видим вот эту штуку,
видите, n эпсилон квадрат. Вот она нас будет преследовать в целом по понятным причинам,
но давайте, что называется, разберемся по порядку. Так, приведу еще одно неравенство такого же
типа. Вот здесь его запишу. Значит, пусть у нас есть серия случайных величин xнкта, они независимые,
и с вероятностью единицы все ограничены. То есть, вероятность того, что xнкта по модулю меньше
или равно c, равно единице для любого k от единицы до n. Если бы c была маленькая, то мы бы сказали бы,
что это как пренебрежима малый в наших терминах. Вот, но здесь она как раз немаленькая. И, значит,
стандартное у нас обозначение xн, сумма xнкта крат единицы до n, то есть сумма случайных величин в
серии. Ну и здесь это как, видите, как в тензорном учителении. Если индекса нет, значит, по нему
просуммировали. Вот. И тогда имеет место вот такое неравенство. xн, только центрированное,
больше или равно епсилон, меньше или равно экспонента, минус епсилон квадрат, делить на два дисперсия
xн и плюс такое неочевидное слагаемое, епсилон c делить на три. Епсилон c делить на три. Значит,
вот это неравенство называется неравенством Бельштейна. И привел я его в связи с тем,
что это, пожалуй, первое неравенство из череды вот этих вот неравенств, я не знаю, вот такого
типа, если можно так выразиться, с экспоненциальной исходимостью. Значит, это неравенство где-то
тридцатые годы, ну, грубо говоря, сто лет, там, может, чуть меньше сто лет, когда эти неравенства
были получены Бельштейном. Значит, Бельштейн Сергей Натанович, академик академии наук СССР,
ученик Гильберта, кстати, ученик Гильберта. Вот, значит, потом подобные неравенства лет через
тридцать-пятьдесят посыпались как из рога изобилия, можно сказать, но все-таки первые, по-видимому,
было неравенство Бельштейна. Ну, кстати, если считать, что все хн-каты одинаково распределены,
тогда здесь просто n умножить на дисперсию, если здесь вместо хн среднего поставить единица на n,
то есть отклонение среднего значения от своего отожидания, то здесь появится тот же самый
множитель epsilon квадрат n. Еще раз повторю, который нас будет преследовать, и в целом так
физически понятно почему. Вот, значит, ну а теперь давайте чего-то и сами попробуем получить,
не все как бы ссылаться на классиков. Так, ну, я тогда вот это напоминание просто было сотру.
Так, давайте рассмотрим вот некую случайную величину Xi, про которую мы знаем следующее,
что она принадлежит минус 1,1 с вероятностью единицы, и второе ограничение, которое мы наложим,
мотожидание Xi равно нулю. Вот, и давайте получим относительно такой случайной величины,
там некое неравенство, достаточно универсальное. Так, давайте рассмотрим математическое ожидание
е в степени t Xi, t больше нуля будем считать. А, извините, еще до этого еще одно неравенство,
прошу прощения, которым мы будем пользоваться. Значит, давайте рассмотрим такую несложную цепочку
равенств. Вероятность того, что некая случайная величина Xi больше или равна
Эпсилон, равна вероятности того, что Xi минус Эпсилон больше или равно нулю,
равно для любого t больше нуля вероятности того, что t умножить на Xi минус Эпсилон
больше равно нулю, равно вероятности того, что е в степени t Xi минус Эпсилон больше или равно
единице. Ну а теперь смотрите, вот эта величина уже не отрицательная, и к ней можно применить
неравенство Маркова и написать меньше или равно математического ожидания е в степени t Xi минус
Эпсилон или так написать е в степени минус Эпсилон t на математическое ожидание е в степени t Xi.
Но поскольку мы для любого t это делали, то можем написать и чуть по-другому. Вот так. Вероятность
того, что Xi больше равна Эпсилон меньше или равно минимуму по t больше нуля е в степени минус
Эпсилон t на математическое ожидание е в степени t Xi. Вот это неравенство, ну как бы может быть одна из
форм, ну по сути вот это неравенство, называется неравенством Чернова или границы Чернова. Само
по себе может быть ничего не дает, но как мы увидим, по крайней мере на одном несложном примере,
это удобный инструмент для оценки уклонений. Значит Чернов это американский математик,
значит у него не очень американская фамилия по простой причине. Он и получил от своих родителей,
которые в начале прошлого века эмигрировали в США из России. Вот, значит вот будем иметь
в виду. Ну и давайте теперь как бы дальше двинемся. Я хочу найти математическое ожидание е в степени
t Xi. Ну сначала как-то так давайте, может промежуточно. Е в степени t некая х. Это выпуклая функция,
да. Поэтому давайте я представлю в таком виде. Е в степени t. Е в степени t. Одна,
вторая, один плюс х. Одна, вторая, один плюс х. Минус одна, вторая, один минус х. Правильно,
да. Это е в степени t Xi. И в виду выпуклости запишу это следующим образом. Меньше ли равно одна,
вторая, е в степени t. Плюс одна, вторая, е в степени t. Правильно, да. Вот. Значит теперь
воспользуюсь таким представлением. Вот здесь, ну вместо х я поставлю Xi. И поскольку взял мат
ожидания, то давайте возьму мат ожидания здесь, если вместо х поставить Xi. Ну мы помним, что мы
рассматриваем случаи, когда мат ожидания Xi равно нулю. Поэтому мы получаем е в степени t Xi. Меньше
или равно е в степени t, плюс е в степени минус t делить пополам. Xi пропало, да. Вот. Ну это понятно
не для всех все-таки Xi, а для с вероятностью единицы ограниченных. Ну может единицы это частный
случай, но в принципе ограниченных. Вот. Ну и давайте дальше. Одна, вторая. Ряд напишу. Т в степени
к на к факториал. К от нуля до бесконечности. Плюс минус т в степени к на к факториал. Также
к от нуля до бесконечности. Так. Все нечетные степени сократятся, четные встретятся дважды,
поэтому это равно сумма t в степени 2k на 2k факториал. К от нуля до бесконечности. Правильно, да.
Ну теперь что такое 2k факториал? Это к факториал умножить на k плюс 1, на k плюс 2, на k плюс 3 и так
далее. Вот я эти k плюс 1, k плюс 2 и k плюс 3 заменю на двойку. Таким образом я знаменатель уменьшу,
дробь величу и поэтому это будет меньше или равно. Т в квадрате в степени k делить на 2 в степени
k на k факториал. Ну и нетрудно убедиться, что это е в степени t квадрат пополам. Правильно, да.
Ну вот здесь давайте в уголке напишу, будем на него часто ссылаться. Математическое ожидание
е в степени t кс меньше или равно е в степени t квадрат пополам. Ну легко мы получили это неравенство,
но правда из-за эту легкость мы заплатили там точностью, как станет ясно в дальнейшем. Но тем
не менее для того, чтобы получить правильные порядки везде, этого достаточно. Так вот такой имеет место факт.
Так, ну а теперь давайте вот что сделаем. Давайте рассмотрим серию, как мы любим,
xnk. Это будут независимые случайные величины. Все с вероятностью единица ограничены,
xnk меньше или равно единица по модулю равна единице. И математическое ожидание xnk равно 0, k равно 1n.
Пока пусть будет так. Вот и давайте рассмотрим. Вот что давайте рассмотрим вероятность того,
что xn больше некого ексила. Ну xn это вот оно свернуто по серии. Это звонок, да? Ну давайте
отдыхайте с этого момента и продолжим понеравенство Чернова. Ну что продолжим наш труд, коллеги. Итак,
вероятность xn больше равно епсилон, xn как бы произвольная. В общем, можем воспользоваться
неравенством Чернова и написать меньше или равно. Тут поставлю для любого t, e в степени минус
епсилон t, на математическое ожидание e в степени t, а вместо xn поставлю сумму xn катова.
xn каты независимые случайные величины, мат ожидания распадается в произведение мат
ожиданий, а каждое из со множителей к каждому применю вот эту вот формулу и получу, что это меньше или равно
e в степени минус епсилон t, плюс t квадрат пополам, t квадрат пополам, умножить на n.
Ну и разумно это взять, поскольку для любого t, давайте это возьмем в точке минимума, минимум
этой функции это гипербола ветвями вверх, значит tn минус епсилон равно 0, t равно епсилон делить на n.
Правильно, да? Ну и в точке t равно епсилон делить на n, что у нас получается? e в степени
минус епсилон квадрат делить на n, плюс t квадрат это епсилон квадрат делить на n квадрат,
двойка и умножить на n. Так n1 сокращается и получается e в степени минус епсилон квадрат
делить на 2n. Вот такое вот мы с вами получили неравенство. Значит, если у нас есть вот такие
величины с вероятностью единиц, ограниченные единицей, с нулевым от ожиданием, то тогда вероятность
того, что их сумма будет больше епсилон, ограничится вот такой величиной, e в степени минус
епсилон квадрат делить на 2n. Ну а теперь давайте еще одну полезную получим неравенство. 1n xn,
то есть средне арифметическое случайный величин к серии, больше или равно епсилон,
больше или равно епсилон, равна вероятности того, что xn больше или равно епсилон умножить на n и,
подставив вместо епсилон, епсилон умножить на n, получим, что это меньше или равно e в степени
минус епсилон квадрат n делить пополам. То есть для таких случайных величин среднее значение
отклоняется от мат ожидания, поскольку здесь мат ожидания нулю, в терминах закона больших чисел,
среднее значение отклоняется от мат ожидания на величину большую, чем епсилон, с вероятностью
меньше, чем e в степени минус епсилон квадрат n пополам. Ну конечно по сравнению с неравенством
Чебушева принципиально другое, там скорость ходимости единицы делить на епсилон квадрат n,
а здесь e в степени минус епсилон квадрат n. Но это все эффекты, полученные благодаря тому,
что мы рассматриваем класс случайных величин, ограниченных с вероятностью единицы. Если это
требование убрать, то все, ну так сказать, в целом гораздо хуже. Так, ну и на что еще хочу обратить
внимание, прям не знаю, выписать или выписывать, не выписывать. Мы еще можем рассмотреть вероятность
того, что xn по модулю меньше или равно минус епсилон. Ну давайте вот здесь вот напишу,
чтобы не на слух воспринимать. xn вот это же меньше или равно минус епсилон. Ну понятно,
зачем я это делаю, да, чтобы модуль потом поставить. Вот это равно вероятности того,
что xn минус xn больше или равна епсилон. Если вы посмотрите на вывод, то это приведет к тому,
что здесь будет стоять минус xn. Обращаю ваше внимание, что если xn удовлетворяет вот этим
свойствам, то и минус xn удовлетворяет. Поэтому, собственно, все то же самое, получим тот же
результат. И в конце концов получаем, давайте вот здесь его запишу, он как бы так будем на него
ссылаться. Напишу одностороннее неравенство и двухстороннее для среднего.
Попала. Ну понятно, да. Так, значит, вот такую получили штуку скорой сходимости как бы довольно
общего вида. Ну отсюда в частности следует, что все последние случайных величин, которые
с вероятностью единицы ограничены, их среднее всегда сходится почти на верное, потому что,
еще раз повторю, вот такое ограничение гарантирует быструю сходимость по вероятности. Вот. Так,
дальше теперь некое обобщение. Некое обобщение. Так, вот это позволю себе стереть. Хотя вот тут,
тут уже можно стереть на самом деле. Давайте теперь рассмотрим случайные величины,
другой их буквы обозначим, yкт. Пусть их будет н штук. Вот. И, собственно, вот что потребуем,
чтобы yкт центрированная, то есть yкт, а хотя, извините, нет, более слабо можем потребовать пока,
потребуем, чтобы они тоже были все равномерно ограничены с вероятностью единицы. Вероятность
того, что yкт по модулю меньше или равно c, ну пусть будет c, равна единице, k равно 1n.
Так, я сейчас, извините, просто, чтобы потом не вводить новых каких-то обозначений, думаю,
давайте, ладно, пусть c. Вот, значит, ну и давайте рассмотрим вероятность того, что сумма yкт будет
больше или равна некого Эпсилона. Это то же самое yкт центрированных, извините, вот тут уже
центрированный, да. Значит, больше или равна Эпсилон. Давайте представим в виде вероятность того,
что сумма yкт центрированная делить на c, 1n, больше или равна Эпсилон делить на c. И обратим
внимание, что yкт центрированная делить на c обладает тем свойством, что она по модулю меньше
единицы, и мат ожидания равно 0. Поэтому вот к этому выражению можно применить полученный нами
результат, вот этот вот, и написать, что это меньше или равно е в степени минус Эпсилон квадрат
делить на 2nc квадрат, на 2nc квадрат. Вот, или для среднего yкт центрированная средняя, мы еще такое
используемое значение, больше или равно Эпсилон, это меньше или равно, чем е в степени минус n Эпсилон
квадрат делить на 2c квадрат, на 2c квадрат. Вот. Давайте запишу сюда. Ну, для центрированных запишу.
И давайте сразу модуль поставлю. Вот, еще такое мы получили нераненство. Ну,
теперь давайте еще чуть-чуть порассуждаем.
Давайте теперь вот вместо такого симметричного ограничения по модулю меньше с, давайте считать,
что вероятность того, что yкт принадлежит некому отрезку Акт, Бкт равно единице. То есть чуть-чуть
так от симметричного перейдем к несимметричному, хотя, так сказать, этот. Все-таки введем такое
свойство, что Бкт минус Акт ограничено некой константой С, К равно 1n. То есть распределение может
быть на любом интервальчике с концентрированной вероятностью единицы, но длинные этих интервалов
все-таки все ограничены некой общей константой. Еще раз повторю, от этого условия не избавиться,
в общем случае, если мы не хотим потерять, так сказать, вот такую экспоненциальную скорость
Вот так. Теперь такое утверждение. Значит, если yкт принадлежит Акт и Бкт с вероятностью единицы,
то мотожидание yкт тоже находится в этих же интервалах. Это понятно, да? Ну,
просто интеграл Либега. Вот функция ограничена снизу Акатом, сверху Бктом. Интеграл Либега
тоже будет так же выглядеть. И тогда yкт средняя тоже будет принадлежать, в общем случае, этому же
интервалу, да? yкт средняя. Так, значит, yкт средняя это yкт минус мотожидание yкт. Значит,
что? yкт центрированная. А есть средняя? Прошу прощения. Значит так, yкт, мотожидание yкт,
вот это может быть равно A, а вот это может быть равно B, да? Вот, значит, ну в худшем случае,
или наоборот, вот это B, а вот это A. Значит, вот это вот по модулю принадлежит Бкт минус Акт.
Правильно, да? Нигде не ошибся. Вот. И все это меньше c для любого k. Поэтому мы можем написать.
Теперь так. Теперь можем написать так. Собственно, никак особо не можем написать,
если не введем. Так, это мы получили. Ладно, давайте тогда будем считать, что все они принадлежат
одному и тому же интервалу. Прошу прощения. Прошу прощения. Вот. И тогда мы можем написать
следующее, что, вон последнее неравенство, вероятность того, что yкт центрированная,
а тут, кстати, я средний это не поставил. Прошу прощения. yкт центрированная средняя по модулю
больше или равно ε, меньше или равно 2c, уже меньше или равно B минус A пополам. Правильно, да? Ну, смотрите,
значит, в отожидании yкт B плюс A пополам, если у максимальное значение B получается так,
и если минимальное значение A получается то же самое с другим знаком. То есть по модулю меньше
вот так. И тогда, воспользуясь опять же тем неравенством, мы получим вот такой результат.
yкт центрированная средняя больше равно ε, меньше равно 2e, B минус A в квадрате пополам пополам,
минус 2n ε квадрат делить на B минус A в квадрате. Тут двойка. Ну, смотрите, по сравнению с этим,
как бы скорой сходимости в четвертой степени, да? То есть очень существенная разница. Если здесь
какой-нибудь там 10 минус первый, здесь 10 минус первый, то здесь уже будет 10 минус четвертый. То есть
очень существенная как бы подвижка. Так вот оказывается, что вот этот результат верен не
только для симметричных вот таких случайных величин, а для любых. И вот для любых случайных
величин. Эту же именная теорема называется теоремой Хёфтинга. Иногда одну, иногда две буквы пишут.
Теорема Хёфтинга. То есть если у вас есть n штук, ну на самом деле одна из, так сказать,
интерпретации. Если у вас есть n штук случайных величин, которые от A до B, то уклонение среднего
от его мат ожидания на величину больше чем ε ограничивается вот такой штукой. Значит Хёфтинг
это финн, финский ученый, но звать его Василий. Почему? Потому что когда Хёфтинг родился,
Финляндия была в составе Российской империи. Вот, значит, ну тем не менее он там ни в Советском
Союзе, ни в Российской империи не работал, в Америке по-моему работал, но такой Хёфтинг
Василии такой имеется в теории вероятности. Значит, итак, это теорема Хёфтинга. Ну, я уже,
когда поминал неравенство Бернштейна, говорил, что потом через там 30-50 лет, вообще говоря,
появились как изрога изобилия. Вот мы с вами рассмотрели неравенство Хёфтинга, есть еще неравенство
Азумы, ну как бы в несколько специфичной области там с мартингалами связано. И такое неравенство
из этой же серии, неравенство, такая сложная фамилия Магди Армида, неравенство Магди Армида. Вот,
но еще раз повторюсь, все они крутятся вокруг вот этого n-эпсилон квадрат. Ну,
наверное, настал момент сказать, почему, как вы думаете, почему. Потому что мы имеем дело
со средними величинами, тем более финитными, значит, они всегда симпатически нормальны,
а епсилон квадрат n – это, ну, грубая вероятность отклонения нормального распределения на величину
больше чем епсилон стандартно нормального, среднего именно значения, да. Вот, поэтому все крутится
вокруг вот этого n-эпсилон квадрат, который, собственно, еще раз повторюсь, был Бернштейном,
так сказать, там первый раз, так сказать, показано, показано наличие вот таких неравенств. Так,
что еще можно сказать по этому поводу? Ну, вот эти вот неравенства, интерес к ним возник, ну,
относительно недавно. Почему? Потому что до появления вот этих big data, ну, они носили в
значительной степени все-таки теоретический характер. Ну, например, позволяли сделать вывод о
сходимости, почти, наверное, так сказать, быстрая сходимость, да. Но так, вообще-то говоря, до появления,
еще раз повторю, вот этой революции, да, ну, сколько, какими объемами, там, не знаю, измерений,
вычислениями, измерений, да, там, естествоиспытатели, так сказать, оперировали. Ну, тысячи там, условно
говорят, тысячи, это много, а то несколько сотен. Ну, и, так сказать, в этих объемах вот эта величина
все-таки достаточно большая. Чтобы вот эта величина была, ну, какая-то маломальски человеческая,
да, надо, чтобы n-эпсилон квадрат там было, так сказать, ну, не знаю, там порядка единиц хотя бы,
единиц. А это значит, что если епсилон взять какую-нибудь там, там, 10 минус второй, то это уже
n-10 в четвертый, десять тысяч, как бы, измерений. Так сказать, до информационной эпохи с такими
объемами данных, в общем-то, ну, не работали. Вот, значит, поэтому носило вот это вот такой
теоретический, в общем-то, характер. А когда вот появились, так сказать, большие данные,
и там какие-то там, не знаю, там миллионы измерений, вот тогда вот эти вот границы уже стали вполне
рабочие. И вот одна из таких задач, которая, ну, практическую там ценность имеет, да, состоит в
оценке параметра распределения Бернули. Вот когда вы рассматриваете там графы и ребрами
приписываете некие вероятности перехода, там такая модель популярная, да, ну, возникает вопрос,
а как оценить на основании опытных данных, на основании обучающей выборки, надо как-то оценить
вот эту вот вероятность перехода. Ну, вероятность перехода, собственно, как бы вот и оценивается,
исходя вот из неравности такого типа. Что мы здесь получаем, если будем считать, что у нас
yкт там имеет, вот yкт принадлежит распределению Бернули с параметром t. Тогда вероятность того,
что ук средний отклонится от своего мотожидания на величину больше, чем епсилон. Меньше или равна
для распределения Бернули b-а равно единице, да, значит, меньше или равно, чем 2 на е в степени,
вот как раз вот это 2 е квадрат n. То есть вполне рабочая граница, то есть как бы вот можно с этим
работать, уже получать какие-то, при том объеме данных, которые, так сказать, сейчас обрабатываются,
уже можно получать достаточно точные оценки вот этих вероятностей перехода по ребрам графов.
Вот и, собственно, как вот интерес возобновился. А так, еще раз повторюсь, там, 50 лет назад,
образно говоря, значит, ну так было это там, ну много из этого, но носил такое, в общем,
скорее теоретическое значение, не переходило в практическую плоскость. Ну, как я говорил,
сам в начале курса такое в науке бывает, да, так сказать, какие-то наработки, теории, они могут
ждать, там, не знаю, десятилетиями своего часа, да, пока, так сказать, они понадобятся. Ну вот,
эти, этот раздел, так сказать, вот он как бы понадобился при наличии, так сказать, больших данных,
вот. И вот это вот еще раз просто, как такая частная, но очень важная задача, прямо обеду. Вот как
можно строить, получать довольно, так сказать, точные качественные оценки вероятности успеха
в испытании Бернулий, которые интерпретируются как вероятность перехода по ребру графа. Вот. Так.
Что еще по этой теме? Так, вроде ничего не забыл. Единственно, акцентирую еще раз, вот на чем внимание.
Значит, смотрите, мы с вами получили вот этот результат теория Мехёвтинга для частного случая.
Почему мы были вынуждены, так сказать, к частному случаю, так сказать, обратиться? Ну потому что мы
слишком просто, слишком грубо вот это, вот это неравенство получили. Оно очень грубое. Из этого мы
потеряли вот эту, точнее говоря, двойка у нас оказалась вместо числителя, а оказалась знаменателя.
Если бы мы провели более тщательный анализ, воспользовались бы выпуклостью функций, но только
на отрезке АБ, собственно, как это и делается в теории Мехёвтинга, то мы бы как бы аккуратно вот
такой результат получили. Но мы, так сказать, схалтурили немножко вот здесь, поэтому получили
более слабый результат, хотя с точки зрения, как вот еще раз повторюсь, теории вероятности как
естественной научной дисциплины довольно понятно, что так должно быть. Почему? Когда у нас довольно
большие n, вот средние значения, мы же можем их разбить на группы по слагаемым. Каждая из этих групп
будет нормально распределена, это стремится к нормальному распределению и финитно, а нормальное
распределение, оно симметрично, поэтому, так сказать, попадаем в класс симметричных распределений,
в общем, как бы понятно, что так должно быть. Осталось это доказать, вот что Хёвтинг, собственно, и сделал,
да, вот, ну в том числе, так сказать, там как бы не он один, на этом поприще трудился. Вот, значит,
тогда по этой теме, вроде, вроде ничего не забыл, да, вроде ничего не забыл, тогда по этой теме все,
еще раз повторюсь, что это не единственного типа неравенства, их там, ну, несколько,
относящиеся там к разным ситуациям, вот, довольно, как бы, развитые вот эти вот
задачи или неравенства на концентрацию меры. Ну, еще раз повторю, так сказать, порядок у них
у всех один, это n епсилон квадрат. Так, ну, тогда, позволю себе, давайте, вот здесь сотру.
Так, ну, и там хотел бы, наверное, завершить наш курс вот таким
вопросом и соответствующим результатом. Значит, смотрите, мы, когда с вами центральную пределу и
предельную теорему рассматривали, мы получили, что, что, вот, множество такого вида, точнее,
если центральная, предельная, то мы считали, что, не считали, а по условию они все одинаково распределены.
К единице до n делить на n сигма квадрат, корень квадратный, вот так напишу. Вот, вот,
такие вот величины сходятся по распределению к стандартному нормальному закону. Вот,
это центральная предельная теорема. Давайте чуть-чуть, ну, я говорил, что, на самом деле,
есть много результатов, связанных с различными модификациями. Могут они как-то зависеть
специальным образом эти ксикаты и тоже может иметь место, так сказать, сходимости к стандартному
нормальному закону. Еще какие-то вводится, так сказать, специфические там виды. На эту тему много
результатов. И вот одна, как бы, группа результатов относится к случаю, когда количество вот этих
слагаемых случайно, количество слагаемых случайно. То есть вы имеете сумму независимых случайных
величин в случайном количестве. Ну, и вот оказывается, что при разных условиях тоже имеет место
асимптотическая нормальность. Этот, так сказать, при разных схемах эксперимента. Точнее говоря,
при разных способах нормировки. Да, при разных способах нормировки тоже имеют место асимптотическая
нормальность и один из результатов из этой области ну частный случай просто для
понимания значит я вам расскажу это теорема принадлежит Натану Андрею Иксановичу значит
там в прошлом заведующему нашей кафедрой тоже кстати так свете участнику войны значит
полковник в отставке он по моему был да вот значит в применении или в терминах
центральной предельной теоремы давайте вот рассмотрим такую такую постановку вот у нас
есть случайная последовательность в общем случае до бесконечности пусть будет вот значит независимые
случайные величины независимые случайные ну пусть даже раз мы в терминах этой постановки
в частный случай центральной предельной теоремы обобщаем независимо одинаково распределенные
случайные величины дисперсия кси ну сигма квадрата и обозначим и есть у нас дискретная
случайная величина n с функцией распределения которая зависит от некоторого параметра лямбда
от некоторого параметра лямбда вот который обладает единственным свойством требуется чтобы для
любого аргумента для любого n малого функция распределения стремилась к нулю когда лямбда
стремится к бесконечности все дискретные распределения к с которым имели место удовлетворяет
этому свойству только надо правильно параметра лямбда подобрать например если он имеет
биномиальное распределение то таким параметром является количество экспериментов а самой
случайной величиной ну является от количества успехов серии из инструментов как только вы
количестве экспериментов будете стремить бесконечности функции распределения будут
удовлетворять вот этому свойству в общем это не диковинное которое какое-то свойство которое
там надо еще поискать наоборот очень распространенное ну вот и если такое свойство выполнено такой
простой единственное свойство то вот такая сумма ксикатая как единицы до n большого минус а
делить на корень квадратный из n большое sigma квадрат будет стрелиться к стандартному
нормальному распределению но только понятно что не по случайной влечении а вот по этому параметру
ну с практической точки зрения если вы будете если n у вас это биномиальное распределение то если
вы будете увеличивать количество экспериментов или набирать статистику все больше и больше то даже
несмотря на то что n случайная вот эта штука будет сходиться к стандартному нормальному распределению
это вот частный случай теоремы натана вот применённый к обобщению центральной предельной теоремы значит
доказательства особенно в данном случае не сложная но оно идейно как бы и на самом
деле не сложная вот значит давайте рассмотрим вероятность того что вот это наша специальная
случайная влечина будет меньше некоторого x и вычтем из нее вот стандартную функцию распределения
стандартно нормальную значит и представлю этот в таком виде вот эту вероятность по формуле
полной вероятности я выпишу так
меньше x при условии что n большое равно некому n малому и на вероятности n малого
плюс
точнее говоря минус минус f от x и добавлю сюда единицу сумму всех вероятностей что n большое
равно и мало сумма по н единица да теперь смотрим дальше значит смотрите вот это некая
условная вероятность при вот таком событии не нулевой вероятностной меры то есть ну как бы
все корректно да как мы помним условная вероятность это обычная вероятность только взятая на под
множестве событий условия то есть вот наше условие значит это на самом деле можно представить
вот в таком виде секат и минус а ка уже от единицы до n малая то есть вот этого условия делить на
корень n малая сигма квадрат меньше x умножить на вероятность того что
сэкономлю одну строчку напишу здесь минус f от x
на вероятность того что n большое равно n малая ну и дальше просто
дальше просто тогда модуль вот этой вот
еще может быть разобью на сумму значит равно пишу равно
суммы
сумму возьму по n меньше некого n с волной пока не пояснил попозже пояснил и точно такую сумму
по n больше и он с волной вот ну может я там это что-то там скажу не не все запишу тут как бы
достаточно все очевидно смотрите значит для понятности наоборот ту которую не написал она
по очевидней это вот так ну сейчас еще буквально так седьминуту вот смотрите значит
предостаточно больших n вот эта разность мало потому что при не случайно мэн вот эта штука
стремится к нулю при любом фиксированном x да то есть это грубо говоря епсилон можно вынести за
скобки здесь останется вероятность вот такого события которые можно иметь на единицу а вот
в этой сумме это же разность вероятности она по модулю меньше двух двойку можно вынести а то
что здесь останется это функция распределения n большого в точке n с волной да которая стремится
как мы знаем к нулю когда лямба стремится к бесконечности то есть выбрав n с волной такое чтобы
вот эта вероятность была достаточно мало вот а да выбрав н с волной чтобы вот эта вероятность
была достаточно мало но вот эту сумму сделаем маленькой а вот это при достаточно большом n то
есть для n больше n с волной грубо говоря каждый сам множественный будет меньше некоторого
епсилона я понятно объяснил ну так это в принципе не сложно для данного случая все довольно просто
это обобщение центральной предельной теоремы на случай случайного количества слагаемых хочу
заметить что не совсем тут все тривиально видите мы какой выбрали порядок ну или или величину как
отнормировали для сходимости а если бы мы здесь например поставили дисперсию вот этой суммы
случайной ксикатая кат единицы дн большое не и случайно лично на множестве квадрат а дисперсию
вот этой суммы то там вообще бы уже как бы ничего не получилось бы то есть там нормировка должна
выглядеть или или в этой теореме выглядит так и приводит к успеху другая нормировка уже к успеху
может не привести так коллеги ну что значит все мы с вами закончили курс теории вероятности надеюсь
что так сказать там как может быть не сразу но постепенно это у вас перейдет такой активную как бы
фазу довольно длинные истории развития предмета как я вам сказал там порядка 500 лет но на самом
деле я вам хочу сказать все достаточно близко что я имею в виду как это время время на самом
деле гораздо более сжатое чем нам кажется я в свое время имел так сказать честь там несколько
раз здороваться за руку с академиком никольским сергей михалычем который должен до 107 до 107 лет
кстати вот читал мне математический анализ академик никольский естественно ручкался с академиком
бюрнштейн а поскольку они стали в одной академии академик бюрнштейн естественно ручкался с
гильбертом который был научным руководителем так что коллеги я в трех рукопожатиях от гильберта вот
так что все довольно так сказать в этом мире спрессовано и теория вероятности которая там
500 лет развивалась мы с вами видите вот за там сколько там 12-13 лет с этим вот как-то ну не
сказать что так уж всю но прошли все спасибо тем кто доходи дослушал доходил вот потому что так
сказать наверное перед пустой аудитории мне было бы не сильно приятно читать хочу поблагодарить
нашу фею которая весь курс у нас вот добросовестно здесь так сказать отработал оператором все коллеги
значит успехов теперь возможность видеться с вами на экзаменах по случайным процессам или
математической статистике

Александр, я заведую кафедру мат. основ управления и работаю на ней с 2005 года. И вот уже где-то лет
15, может быть больше, но с момента как вот мы стали делать от кафедры всякие факультативы типа
100-х анализ задач, была какая-то такая идея, что было бы здорово сделать объединяющий курс,
который бы рассказывал о основных принципах, которые используются в анализе данных и в целом
в такой математике, изучение каких-то больших задач. Эти задачи большие, они не обязательно
связаны с анализом данных, это в том числе мат. моделирование, это стат. физика, я не знаю,
это теория чисел, это что угодно, это например моделирование ковида, распространение эпидемии,
это изучение биологических моделей, хищник, жертва. То есть, как бы сказать, когда мы пытаемся
описать какое-то явление природы, мы часто делаем какие-то допущения предельные переходы. Как
правило, эти предельные переходы связаны с тем, что агентов много, время достаточно большое,
система живет. И вот эти предельные переходы, они приводят к так называемым эргодическим
теоремам, к законам больших чисел, к нелинейным законам больших чисел, к явлению концентрации
меры. И как побочный продукт, естественно, вся эта математика используется замечательным образом и
в анализе данных. Но надо сказать, что эта математика, она же используется, например, в каком-то
виде в перечислительной комбинаторике, она используется в биологии, математической естественной биологии.
И как бы обозреть это студенту практически нереально, потому что ни одна программа,
которая предусмотрена то или иной магистратурой, не предполагает, что как бы в основу будет
положено какое-то единство в математике. Потому что вот вы изучаете какой-то тот или иной курс,
и вам рассказывают, например, какой-нибудь из TFKP метод перевала или метод лоплассы,
стационарной фазы, что-то такое очень специфическое. А потом оказывается, что все задачи
симпатической комбинаторики, аналитической комбинаторики, ну не все, но большая заметная
часть решается с помощью TFKP. Казалось бы, вот как это, естественно, в курсе TFKP вам это не
расскажут. Да я больше того скажу, что в курсе TFKP, скорее всего, вам и метод перевала не рассказывали,
но тем не менее. Или вы изучаете теории вероятности, вам рассказывают центральную предельную теорему,
а в курсе оптимизации вам рассказывают всякие результаты о сходимости констат хастического
градиентного спуска. И у вас это не совсем вяжется, потому что, ну как бы, где центральная
предельная теорема и где вот эти результаты о сходимости SGD, стокастического градиентного спуска,
на самом деле, собственно, чтобы аккуратно получить оценки скорости сходимости стокастического
градиентного спуска, нужен неосимпатический вариант центральной предельной теоремы,
который называется неравенство азума хевдинга. Для мартингала разности. Вот какие-то такие умные
термины, но это настолько просто и настолько органично, это проще, чем CPT, доказывается. И это же
неравенство азума хевдинга используется много где, в том числе, например, в изучениях
романтических чисел графа, допустим, или еще что-нибудь. И вот этих связей огромное количество,
и я не знаю, насколько у меня получится реализовать ту программу, которую я задумал, а программа
связана с некой, так сказать, демонстрацией единства в математике на примере разных задач,
прежде всего анализа данных, но не только. Моделирование интернета, это не совсем анализ данных,
решение каких-то задач компьютер-сайенс, где невозможно чисто просто взять матрицу и умножить на
другую матрицу, там надо как-то рандомизировать. И вроде как это все в каких-то отдельных курсах
вам встречаться будет, безусловно. Например, на шестом курсе для студентов ФУПМА, ну это школа
ПМИ, направление ФУПМ, там есть курс Кузюрина Фамина, ну сейчас его Фомин читает этот курс,
там очень много алгоритмических вещей, связанных так называемые рандомизированными алгоритмами,
вероятностным анализом алгоритмов, то есть какое-то пересечение будет. Естественно,
с курсом случайных процессов будет пересечение по части эргодических там всяких теорем.
Вот сегодня мы будем говорить о методе Markov Chain Monte Carlo, то есть популярный метод решения
задач больших размеров, но как бы еще раз хочу подчеркнуть, что смотреть на это надо не как на
набор примеров, а как на попытку продемонстрировать, прежде всего расставить акценты важность,
каких-то, а более-менее одних и тех же математических концепций. Вот что такое конструкция Markov Chain Monte Carlo?
Вот мы сегодня узнаем, что это на самом деле просто эргодическая теорема для Markov-х цепей и явление
концентрации меры. Вот как бы и это же самое явление концентрации меры используется много где,
и в основу вот конкретно из Markov Chain Monte Carlo положен часто принцип максимум правдоподобия
еще дополнительно. Я какие-то сейчас слова говорю, которые в общем может все не все из вас знают,
но эти слова будут многократно повторяться по ходу курса, и когда три-четыре раза вам в разных
контекстах встретится одна и та же математическая конструкция, вы совершенно по-другому на нее
начнете смотреть. Она не просто вам запомнится, она станет для вас родной, и мне очень важно показать,
что в математике, и вот вообще в том, что вы изучаете на физтехе, есть десяток таких результатов,
которые разбросаны по разным курсам, и если вам и рассказывались, то скорее всего в каком-то таком
обрезанном виде, не очень акцентируя на этом внимание, а на них сидит вся современная наука,
то есть вот многократно вы это же самое будете использовать практически чем бы вы не занимались,
вот если вы идете в науку. И вот начать бы мне хотелось сегодня, ну да сразу скажу, что курс
будет состоять из в основном очных лекций, но в какой-то момент возможно мы перейдем на онлайн
формат, если потребуется привлечь специалистов, которые не смогут приехать. Ну вот у меня есть
желание привлечь очень известного специалиста по теории информации, Григория Антоновича
Кабатянского, который расскажет, надеюсь компактно, как теория информации и big data связаны,
вот все основные принципы, которые вот совершили революцию в теории информации, и там в основе
лежит явление концентрации меры, и часто это связано с шарами, с такими простыми достаточно
явлениями, и таким простым фактом, что если шары трехмерные пересекаются, взять два центра,
вот так вот, два шара-шара, и они пересекаются, этот объем достаточно заметен, то когда эта
картинка будет рисоваться в Rn, и Rn стремится к бесконечности, то объем пересечения будет,
ну можно чуть-чуть еще отдалить их, он будет стремиться к нулю, причем экспоненциально быстро,
и вот такого типа какие-то простые наблюдения, ну аля там, что весь объем шара сосредоточен
у корки, или если мы проведем экватор, то вся площадь многомерного шара сосредоточена около
этой полоски, вот такого рода эффекты, довольно простые геометрически, мы их естественно выведем,
они вот по сути лежат в основе там теории информации, и все, вот по сути вот эти принципы,
и вот как бы это компактно рассказать и закрепить не только в теории информации, а много где еще,
чтобы вы посмотрели, как более-менее одна и та же математика решает кучу разных задач,
и более того, эта математика синонимична, то есть я могу что-то доказывать используя TFKP,
могу что-то доказывать используя ну какую-то другую технику, например,
ну теория мукаша вычетых и метод перевала, а могу доказывать, например, используя метод
большого канонического ансамбля, то есть как стат физики там исследуют, то есть аппарат
производящих функции, и уже немножко куда-то уходя, хотя тоже там в конечном итоге TFKP возникает,
вот мы попробуем специально продемонстрировать разные техники, разную математику, но многократно,
то есть не один раз, и начнем сегодня мы с задачи оранжирования веб-страниц, и на этой
задачи мы продемонстрируем три замечательных результаты, торгодическая теорема, это всякие
законы больших чисел, концентрация меры, ну и в принципе максимум правдоподобия, если получится,
хотел бы это рассказать, еще немножко окрестности демонстрируя. Замечу, что в курс активно был
вовлечен Максим Рахуба, это очень такой хороший специалист по матричному анализу, он ученик Ивана
Валерьевича Селеца, он защитился, вот получил, прошел пост-дог, по-моему даже защитился в Швейцарии,
может быть он там пост-дог делал, но сейчас он вернулся в Россию, и есть замечательная
возможность его вовлечь, он очень хороший специалист по всяким разложениям матричным,
и матричные разложения это вторая такая большая линия, что многие современные задачи, они требуют
хранения данных в виде матриц, и те матрицы, которые получаются оказываться каких-то колоссальных
размеров, надо как-то это в общем их представлять малорангово, то есть а-ля СВД, вот вокруг
singular value decomposition, это очень важный сюжет, он как-то скомканно рассказывается в целом у нас в
курсах всяких вот вокруг вычислительной математики, а это важная тема, то есть она рассказывается,
но вот она очень часто в жизни современно используется, и мы тоже решили про это отдельно
рассказать, там будет меньше всяких концентрационных вещей, но тоже будут тоже всякие такие эффекты
больших размеров и будут проявляться, но давайте начнем с задачи пейдж ранг, в чем идет речь,
я прям так и начну, гугл проблем, гугл проблем, это никак не связано с русском надзором, гугл
проблем, ну почему гугл проблем, потому что в общем в конце 90-х бриллиный пейдж как раз положили в
основу то, что я сейчас буду рассказывать, ну каких-то первых, даже не в конце 90-х, а в середине,
вот идеи ранжирования веб-страниц, значит давайте представим себе такую ситуацию, что у нас есть
граф, какой-то граф, ну вообще говоря большой, очень большой, это граф интернета, и каждое,
ну по-хорошему конечно здесь надо рисовать ориентированный граф, каждая вот эта вот стрелочка
это гиперсылка, а каждая вот эта вот вершинка, это давайте считать веб-страница, ну можно еще
больше упростить считать, что это сайт, но конечно лучше точнее считать, что это веб-страница,
естественно могут быть такие стрелки, ну в общем можете себе представить какой угодно граф,
главное свойство этого графа, чтобы, ну я так упрощаю жизнь, чтобы если это понимать как
систему дорог, то по этой системе дорог вы могли бы легко, ну не легко, а хоть как-то в принципе
добраться из одной вершины в другую, то есть чтобы была полная доступность старту из любой
точки, вы могли оказаться в любой другой, на самом деле вся, все что я буду рассказывать верно
и при более слабых предположениях, а именно когда есть красная площадь, то есть такая точка,
которую из любой точки можно приехать, ну вот где бы вы ни жили, вы на красную площадь всегда
можете попасть, а уже, так сказать, дальше, что с красной площади вы куда-то можете попасть,
это никто не утверждает, просто что вы можете добраться до красной площади, то есть до
достаточно существования вершины, в которую можно свалиться, но вот нам сейчас будет удобнее
считать, что все связано, это называется неразложимый граф, дальше мы просто, значит,
ведем человечка, вот то, что я сейчас рассказываю, это, ну в общем, базовые вещи в курсе случайных
процессов, мы увидим, что отчасти это вот сегодняшняя лекция, это будет повторять, но я постараюсь
сделать так, чтобы вы что-то новое узнали, даже если вы третий курс, хорошо отучились,
и знаете, что такое там марковский процесс и все такое. Так, у нас есть человечек, который
блуждает по этому графу, ну а на графе какие-то, естественно, видены вероятности, с которыми
человечек переходит со страницы на страницу, пускай эта страница и эта страница, допустим,
g, ну и вообще говоря, у него есть какие-то разные варианты, куда там идти, ну и вот этот граф задан,
матрица переходных вероятностей задана, поезжитое, вот, стрелка рисуется в том случае,
если поезжитое больше нуля, поезжитое больше нуля, вот, матрицу переходных вероятностей мы
называем p, и это индекс строки, g это индекс столбца, поезжитое, вот, и мы считаем, что строк и столбцов
очень много, например, миллиард, 10 миллиардов, 100 миллиардов, ну какое-то очень большое число,
давайте считать, что в начальный момент времени человек находится в каких-то состояне, в каком-то
можно просто считать состояния, и вот начальное распределение вероятностей p от 0, это, например,
просто есть там 0, 0, 1, 0, ну все, остальные нули, вот это вот начальное распределение вероятностей,
это может быть какое-то распределение вероятностей реально, то есть может быть с вероятностью
вероятностью одна вторая он там вон там но это сейчас неважно возникает следующий вопрос хорошо
значит вот мы взяли этого человечка и дальше что что как бы естественно было бы спросить ну что
будет происходить со временем если он будет блуждать согласно тем вероятностям которые
нарисованы это вот абсолютно классическая постановка задачи она в чистом виде соответствует
самой простой марковской цепи однородной дискретно неразложимый то есть означает что в общем
просто однотипная ситуация из итерации в итерации повторяется просто надо следить за эволюцией
вероятностной меры то есть если я обозначаю закон распределения этого человечка по вершинкам
пат то чтобы написать эволюцию этого закона распределения я должен написать такую рекурренту
давайте поймем откуда она появилась на п наверное удобнее было бы воспринимать это как-то не векторно
а скалярно допустим для состояния 1 вот у нас есть какое-то состояние давайте не один а же же и
вот значит тут состояние какие-то из которых он может сюда прийти вот и и же и же вероятность того
что человечек окажется в состоянии же и в момент времени t плюс 1 это что такое это по формуле
полной вероятности это есть сумма вероятности того что в момент времени и момент времени t он
находился в состоянии и вообще говоря и может совпадать же и скакнул в состояние же вот так
и сумма естественно идет по всем и вы согласны или я что-то не так написал не надо это подробнее
пояснять или это очевидно это хорошо это очевидно да все отлично это называется формула полной вероятности
то есть это группа полная группа событий что все пространство вероятностное разбивается на
состояние это это это то есть где-то он находился и это не пересекающие события вот это полная
группа событий а это вероятность перехода то есть формула по-хорошему выглядит вот так п ну я не
переход переход переход вред при условии что мы находились вHer и значит при условии что
мы находились и умножить на вероятность нахождение и в момент времени то есть вот
просто важно что вот это вот есть как раз эту условная вероятность а это есть вероятность
перехода то есть по-хорошему эту формула надо было писать в обратном порядке тогда это
была классическая форма записи хорошо если я вектор на это соберу то я получу
ровно вот это соотношение это все ну в принципе да то есть на самом деле самый
главный шаг сделан я написал динамику я написал динамику эволюции вероятностных
мер следующий вопрос который можно задать замечательно ну и что дальше то
будет вот этот вопрос мобитурентов спрашиваем ну спрашивали потом меня
перестали приглашать вот на собеседование на самом деле просто как
бы сказать уже перестал быть замдекан и поэтому естественно как бы не стал
участвовать собеседник а так когда участвовал в собеседованиях для
абитурентов я их вот это почти спрашивал жестко но но я по-другому
спрашивал сейчас поймете что все было не так жестко смотрите абитурентам
которых действительно судьба решалась вот ну спорный бал и надо было понять
человек может как там сообразить что-то я им давал такую задачку
последовательность xn плюс один равняется xn плюс два и я говорил что
предел есть то есть предел есть не надо это доказывать лимит xn при
стремляющимся к бесконечности равен а считайте что это вы уже как бы знаете
более того x0 выбирается как-нибудь так что там предел вполне как бы понятно
какой будет то есть например 100 единицы и предел есть вопрос как к чему равен
предел идея понятна вот этот простой трюк который я сейчас скажу он лежит в
основе вообще почти ну вот всех каких-то результатов связанных с большими данными
этот результатов и в общем связан с тем что есть есть есть есть какая-то
динамика которая потенциально это надо доказывать к чему-то сходится то совсем
не сложно бывает определить то к чему она сходится для этого просто надо
найти так называемую вариантную меру а в данном случае эта мера просто число
то есть надо найти такое число которое при подстановке в эту динамику это
неподвижная точка ну то есть в нашем случае это а равняется корень из а плюс
два как я это уравнение получил что нет это я с потолка взял ответил на вопрос
вот я так умею не так еще брать вот значит смотрите им просто какая я ну
как бы я не знаю откуда я взял из фехтангольца есть вот такой пример что
надо найти предел как его найти но вообще говоря изучать что он существует
это отдельная такая кропотливая работа а вот если я верю что он уже есть то я
могу его само число найти число 2 как найти что этот предел равен именно
двум что это именно два получается подстановка сюда потому что корень из
двух вот x плюс два если x больше чем 0 даже больше чем минус два то это
непрерывная функция что могу менять местами пределы корень и получается а
равняется корень из а плюс два переходя к пределу вот здесь это простое
наблюдение нам нужно для того чтобы то же самое сделать вот здесь
но если то же самое нужно сделать вот здесь давайте это сделаем и получим что
значит вектор p со звездой которая называется стационарное распределение
удовлетворяет вот этому уравнению и этот вектор называется пейдж ранг почему
называется пейдж ранг почему он как-то вот связан с транжированием страниц
очень просто потому что если значит посмотреть на как бы сказать ну так
содержательно на всю эту ситуацию то страница которая посещается человеком
где-то на а симпатически с большей вероятностью она более популярна пока
это еще не совсем хорошие объяснения но так интуитивно на самом деле вот этот
вектор p со звездой он одновременно отвечает на вопрос как часто человек
был в той или иной вершинке то есть можно задать вопрос вопрос это как бы на
самом деле простое упражнение как часто за бесконечный горизонт времени ну в
пределе человек посещал ту или иную вершинку и вот это p со звездой тоже
самый дает ответ на этот вопрос то есть вообще говоря это уже содержатни то
есть один и тот же человек проводил время в этих вершинках согласно этому
вектору а вот это уже объяснение почему это разумно называть пейдж ранг ну
хорошо а теперь возник вопрос ну брина пейджа соответственно кстати сказать
если я правильно понимаю сергей брин это сын значит брина который был
учеником сергей петровича новикова то есть это в общем как бы его отец
заканчивал вместе новикова сергей петровича там по аспирантуре учился
довольно такое интересное наблюдение то есть сказать идея положена в основе
гугла в общем это идея частично вот как-то с россии связано может показать
неожиданно да ну значит неважно мы возвращаемся сюда
хорошо а теперь такой вопрос как это реально считать когда матрица миллиард
ну можно конечно запустить человека но вообще говоря какая будет скорость вот
вот у этого процесса и в этом смысле конечно здесь помогает некое понимание
что на самом деле если матрица более-менее неплохая то вся эта динамика вот эта
динамика она довольно быстро сходится то есть количество итерации которая
требуется чтобы например п а т стало близко к п со звездой ну это по векторно
да там норма векторная естественная два норма стала меньше эпсилон то вот
число этих итераций ты большое значит оно порядка там некоторая константа дайте
о большое о большое я напишу что такое альфа ну там на логарифм значит ну сюда
вообще говоря может входить раз число вершины это n на эпсилон вот что-то
такое может быть вот то есть что такое альфа это хороший вопрос и чтобы на него
ответить давайте подумаем вообще говоря в чем геометрия вот этого всего
происходящего значит верно ли что если п это вектор из симплекс а это
распределение вероятности то после преобразования мы получим вектор из
simplex а я могу это доказать но мне кажется это интуитивно понятно так ведь
ну было бы странно если бы имели распределение вероятности
все это имеет физический смысл и мы как бы сделав шаг оказались не в
распределение врачи то есть�� будут доказывать это ну интуитивно понятно это
значит что динамика имеет инвариант на вектор этот вектор собственно понятно
что раз у нас здесь неподвижная точка то есть это уравнение имеет решение
значит есть собственное значение единичка но это значит что матрица уже как бы не очень хорошая потому что она не сжимает то есть если бы это был
оператор у которого спектр лежит в единичном круге трога то это было бы сжимающее отображение мы могли бы альфа под альфа понимать
максимальное собственное значение и это был бы спектральный радиус это называется ну и тогда бы это все в общем работала тут не так
тут не так потому что уп максимальное собственное значение единицы я пытаюсь пояснить почему
собственно значение единица потому что у матрицы левые собственные значении правый совпадают то есть
просто можно говорить о значит спектре матрицы собственных значениях не привязываясь к левому
собственному вектору и правому так вот очевидно что поскольку есть вариантные вот это вот как бы
элемент симплекса переводится элемент симплекса значит можно быть уверенных что есть правый
собственный вектор, состоящий из единичек, и он переходит в правый собственный вектор, то есть в тот же единичек, то есть мы просто
явно можем предъявить. Поэтому не удивительно, что и левый собственный вектор такой есть, то есть вопрос
существования решения не стоит, это очевидно, что оно есть. А собственно, неразложимость вот этой вот цепи,
что из любого состояния можно прийти в любое другое, означает, что единственно, то есть состояние в классе
распределения вероятности такой вектор единственный. Вопрос, почему мы к нему сходимся, и что, ну,
линейный, и что такое альфа? Ответ, а просто потому, что если перейти в пространство, которое связано
с, соответственно, с вот с лучами на нетрицательном артанте, то вот эта динамика, она будет сжимать по
направлению вот как раз к этому собственному вектору специальной метрики, метрики Бергофа-Гильберта.
Я не буду сейчас про это подробно говорить, но идея в том, что максимальное собственное значение
матрицы и правда единица, а вот сжимаемость определяется не им, оно определяет инвариант,
вот это оно определяет, а определяется скорой сходимости следующим по величине модуля собственным
значением. Вот это альфа, это так называемый spectral gap. Spectral gap это что такое? Это расстояние
между максимальным собственным значением матрицы P. Я не случайно сейчас все эти вещи говорю,
потому что они многократно нам будут встречаться в разных контекстах, то есть всегда есть у матрицы
единица, а вот из-за того, что она неразложима, все остальные собственные значения, потому что этот
граф такой, что из любой вершины можно в любую другую прийти, все остальные собственные значения
лежат вот как-то тут и соответственно максимальный из них определяет то самое альфа, вот это
называется spectral gap и он входит в оценку скорости сходимости, да.
Значит задача сводится к поиску левого собственного вектора матрицы P, ну или если угодно,
правого матрицы P транспонируем, но не просто собственного вектора, говоря по научному,
собственного вектора фрабениуса перона, то есть собственного вектора, отвечающего максимальному
собственному значению, вот такой вот ответ.crew-едет. Все остальные собственные вектора нам не
так интересны. Ну и, в частности, правый собственный вектор матрицы P, отвечающий
собственному значению единицы, нам не интересен, он тривиальный, он из 1 состоит.
У нас динамика через левые векторы записанные, поэтому нам нужно искать левый собственно вектор.
Они отличаются, когда матрица несимметрична, то левая и правая могут отличаться. А вопрос как бы
следующий. Ну замечательно я все чем-то рассказываю, но пока не очень понятно где вот это вот какая-то
это изюминка, потому что пока была только вот одна идея, но этого мало,
потому что хочется какой-то больше концентрации, и вроде как анонсировано было
сильно больше. Но давайте для начала мы поймем, что уже даже вот как бы в
таком варианте, который я написал, есть на самом деле некоторая такая интересная
математика, связанная с, грубо говоря, с тем, как вообще отсюда что-то
вытаскивать. То есть, ясно, что мы можем подождать вот такое время и делать
эволюцию просто с этими матрицами, и получается найти решение, если подождем
вот столько времени. Но в таком случае, если матрица имеет число ненулевых
элементов n, n, z от p, нам придется потратить вот такое время. То есть, число
ненулевых элементов матрицы, ну я логарифмы опускаю, давайте напишу все-таки там
логарифм, логарифм n на эпсилон. Ну ясно, что от эпсилона зависимость не такая
чувствительная, но в общем проблема вот в этом. Если наша матрица миллиард на
миллиард, и где-то из каждой веб-страницы в среднем там сотенка
выходит в ссылок, то это 10 в 12, там может быть. 10, не знаю, в 10, в 11.
Но это так себе удовольствие, если учесть, что альфа типично может быть там еще
там 0,1, да, то у вас уже как получается число арифметических операций, 10 там
не знаю, в 12, в 13. Ну так себе удовольствие. Самое главное, что вообще говоря, вот
это все действие, ну конечно, можно хранить матрицу в виде списка смежности, но все
равно это так себе удовольствие. А где ее хранить? В оперативной памяти, ну
оперативная память, ну не знаю, 8 гигабайт. 8 гигабайт она не поместится.
Ничего делать.
Альфа и спектрал ГЭП это расстояние между, давайте так напишу, это единица
минус лямбда 2 от П. Лямбда 2, второе по величине значение, собственное значение
матрицы, увеличение модуля, то есть вот так. Вот что такое спектрал ГЭП, то есть
максимальное собственное значение это единица, а следующее лямбда 2 и вот альфа
это единица минус лямбда 2 П. Это вопрос, я так понял был. Хорошо, значит в принципе вот
сама жизнь подсказывает, что надо делать. А давайте пустим человечка реальный, будем
за ним следить. Но это уже, извините, другой вопрос, то есть если мы пустим,
это не страшно, если лямбда кратный, тут просто берется, ну там возникают некоторые
эффекты следующего порядка, но как бы в первом приближении все так же остается,
это неважно. Меняет как бы синтетическое разложение, но первый
член синтетического разложения в тех категориях, в которых я пишу, это
ничего не меняет, но при условии что альфа больше нуля. Есть на самом деле замечательные
результаты о том, что как бы я действительно могу, ну вот в идеале, представьте себе,
что у меня на каждой итерации, вот как бы совсем в идеале, вот это неправда, но допустим,
допустим я на каждой итерации, вот реально человечка помещаю в одну из вершин, вот с
этим вектором вероятности, то есть это не марковский процесс, вот сейчас я скажу очень
важные вещи, постарайтесь ее понять, потому что она постоянно будет использоваться, то есть еще
раз, нам надо найти p со звездой, а я не знаю, чему равняется p со звездой, но грубо говоря,
я могу в каком-то смысле сэмплить, это вот то, что используется в анализе, то есть я могу
разыграть, я как бы могу разыграть, где находится человек, проблема только в том, что я это делаю,
как бы с некоторой предысторией, но давайте вот идеализируем картину, давайте будем считать,
что мой человек магическим образом на новую итерацию просто скачет согласно этому вектору
распределения вероятности, то есть у меня есть соответственно вектор ню случайный, который
имеет мультинамиальное распределение, то есть на итерации t ню t равняется k, k ты вершиню и давайте,
и это равняется просто p, че там у меня снизу, блин, плохо, ладно, p со звездой от i,
и t компонента, то есть понятно, о чем я говорю, то есть я предполагаю, что мой человек какой-то
монстр, который вообще ему эти марковские блуждания не нужны, он просто сразу берет и скачет в одну из
вершин согласно вектору page rank, ясно, что настоящий человек, настоящий человек, он как бы вот этого
может добиться только в осимпотике, то есть если он будет достаточно долго гулять, и мы зафиксируем
момент времени, то это будет верно, то есть вот если я здесь напишу лимит по t, то это будет верно,
но тогда будет другая проблема, если я зафиксирую достаточно большой t, а потом возьму t плюс 1,
то уже это будет неверно, то есть одновременно у меня там ну или там я не знаю, не ажи, то есть я
не смогу вот так написать, это будет заведомо неверно, если t большое, то мне, конечно, хотелось,
чтобы каждый новый sample давал бы мне независимую реализацию вот этого вектора, а потом бы я взял
и оценивал настоящий вектор частот просто как средне арифметическое по траектории вот этих
вот new t, это было бы как бы идеально, да, вот это как бы то, что хотелось бы сделать, то есть хотелось
бы, чтобы этот человек, который вот так вот живет, просто вот я бы суммировал векторы, которые
состоят ну соответственно из нулей единиц, в основном в смысле из нулей и одной единицы, если
точнее говорить, и каждый раз эта единица где-то находится, но это случайный вектор, вот такие векторы
я суммирую, позиция единицы определяется тем, где конкретно человек находится, ну подождите,
давайте я, так сказать, неправильно написал, new t равняется значит единице new t и вот так вот,
сейчас я понял, вот так я перепишу, это будет корректнее, вот new t равняется единице с
вероятностью p, вот все, а иначе ноль, вот, ну соответственно я напишу иначе ноль, если,
ну с вероятностью давайте так, p значит new t и равняется ноль с вероятностью соответственно
единицы минус p со звездой и вот так сейчас корректнее, вот, и вот и в идеале было бы,
конечно, суммировать вот такую сумму и тогда бы я получил, что да, здесь тоже лучше переписать,
new t плюс первое, вы правы, new t плюс первое и g равняется единице, вот, и вот я бы что тогда
мог сказать, ну по всяким неравенствам концентрации я бы мог, сейчас я про это скажу подробнее,
написать, что если бы взять число шагов, ну значит один на eq, то замечательным образом я бы
получил, я объясню откуда eq, я бы получил то, что надо, ну то есть что надо, ну я получил
такой результат, то вот этот вот вектор единица на t частоты, ну вот эти new t, значит t от единицы
до t, минус соответственно p со звездой, 0.2 нормия, вероятность того, что это больше,
чем значит, ну аккуратно это так выглядит, там c1, это числовая константа, по-моему,
она там условно 4 плюс там 2 логарифма, sigma минус 1, на соответственно корень из t, значит вот
эта вся штука меньше либо равняется, меньше либо равняется sigma, вот такого типа результат я
мог бы получить, вот что это такое, как такие результаты получаются, мы с вами будем многократно
говорить, я к этому сейчас скоро вернусь, но пока просто придется поверить на слово, что вот как-то
так, ну для тех, кто знает, что такое центральная предельная теорема, я думаю, что вот этот
результат не есть, а так сказать, какое-то открытие, почему, потому что если у меня две вершины,
давайте для простоты возьмем две вершины, то есть всего две вершины, тогда мне достаточно написать это
неравенство для первой компоненты, то есть у меня есть случайная величина nu, nu соответственно первая
компонента, вот и вот эта величина, она равна, если повторю, независимо они разыгрываются,
значит она равна с вероятностью, с вероятностью p со звездой единица, вот ну и получается, что я могу
написать для вот этой вот по сути схемы испытаний Бернули, для первой компоненты, никаких норм мне
здесь ставить не надо, центральная предельная теорема, она формулируется таким образом,
nu, t первая компонента, значит сумма, значит поделить на корень с t, на корень с t дисперсия
здесь должна стоять, сейчас не буду ее выписывать, это не очень интересно, ну потом это мы сделаем,
t от единицы до t большого минус, мат ожидания вот этой штуки умножить на t, ну уж мат ожидания,
думаю, можем в ходу оценить, это будет p со звездой первая компонента на число итерации, вот это
мат ожидания всей этой суммы, не вот эта штука должна стремиться к чему соответственно,
ну к нормальному стандартному нормальному распределению, то есть быть как конечной,
то есть это означает, что масштаб поведения вот этой вот штуки как раз действительно более-менее
один на корень с t, что собственно здесь и написано, то есть если посмотреть на то,
что здесь написано, здесь в общем-то ровно то же самое и получается, вот то есть мы имеем как бы
по сути центральную предельную теорему, но в чем принципиальное отличие вот этих вот марков,
этих марковских цепей, я сейчас просто написал, что было бы если человечек каждый раз независимо
выбирал бы свою вершинку согласно p со звездой, но это же не так в нашем случае, это же неправда и
вот к сожалению, поскольку у нас есть марковская цепь, то ну грубо говоря, вот это время альфа,
один альфа, это есть время, между которыми, которое требуется, чтобы сечения стали независимы,
то есть чтобы то, откуда я стартовал, полностью вымылось, то есть чтобы уже через ток и итерации
человек в каком-то смысле забыл, где он, с чего он стартовал и можно было говорить о
практической независимости сечений и вот в этом смысле качество вот этого результата падает как
раз в альфа раз, то есть вот этот масштабный коэффициент альфа-то маленькая и мы теперь не можем
так точно говорить о том, что эта вот величина именно меньше t, ну давайте я наверное напишу
более понятно, вот я так напишу, чтобы это было значит правильную сторону, то есть с большой
вероятностью, с большой вероятностью мы можем локализовать, где находится вот это вот вектор,
то есть вот как бы резюме, мы по сути, по сути вот эту всю марковскую такую вот специфику,
связанную с тем, что сечения зависимы, человечек блуждает, но нас это не сильно останавливает,
то есть мы в каком-то смысле считаем, что сечение независимо просто с фактором альфа,
который отражает mixing time время смешивания, вот процесс требует большей итерации,
он требует итерации как раз больше такое количество раз, то есть грубо говоря,
мы пропускаем такое количество итераций, делаем первую итерацию, потом ждем такое
количество итераций и снова берем реально вектор, который там распределение потом
снова пропускаем вот столько итераций снова берем понятно что все это в каком
смысле делает сходимость медленнее но но что важно в конечном итоге мы тоже
можем получить что число блужданий человечка вот этого число итераций
необходимых чтобы найти вектор pageRank с точностью epsilon вот в этом смысле
то есть если это epsilon приравнять то мы получим что t есть o большое давайте
с точностью до логарифов я логариф мы опущу 1 на альфа множить 1 на epsilon
квадрать да логариф мы опускаем и получается что вообще говоря размерность
пространства удивительным образом вообще не входит я напомню с чем мы
сравниваемся ты станте значит метод простой итерации ну на английский это
пауэр метод я поэтому пауэр напишу значит это есть вот так 1 на альфа ну те
же логарифмы я опустил но но извините ннз да ннз ннз вот это ннз в этой
матрице п то есть понятно вот как бы big data это вот о чем если вот это много
если вот это много то давайте мы будем использовать какой-нибудь
рандомизированный алгоритм что это будет с какой-то большой вероятностью я
естественно вот эту вероятность сейчас не пишу она под логарифом и
соответственно не то не точность хуже будет входить то есть мы не будем так
точно решать задачу с логарифмической точностью мы будем не точно решать но
за счет этого мы существенно выиграем вот здесь это такая идея вот и
соответственно расплата будет да в том что придется решать не так точно но
зато без каких-то сильных заморочек связанных с большой размерностью это
вроде выход но
ты павуру естественно зависит от эпсилон но это вся зависимость от эпсилон
воз волной ну хорошо если хотите я тут могу такие писать и на тепсилон и на
тепсилон вот здесь здесь это все не так сейчас
существенно там на самом деле еще появляются логарифмы я сейчас их не
буду то есть там больше логарифмов там из-за из-за того что здесь немножко как
бы тоже надо поаккуратнее писать там с логарифмами я сейчас не буду про это
говорит то есть вот этот результат когда я добавляю к сожалению это
происходит не бесплатно и здесь тоже появляются логарифма тен но это все
абсолютно такие мелочи которые сейчас не хочу вас грузить важно сама идея вот
идея фундаментальная если вы работаете с марковской цепью то миксинг тайм род
один альфа замедляет сходимость это вот запомните это реально всегда спасает
но практически всегда это работает и многие из этих результатов еще даже не
получены то есть это на самом деле сложно получать но это практически всегда так
работает а теперь самое интересное то есть я вам рассказал вообще говоря некий
способ ухода от того что называется ну как бы детерминированный алгоритм и
переход к рандомизируем но вопрос это есть марков чейн монтекарло то есть чтобы
найти стационарное распределение давайте просто запустим допустим
случайный процесс и он сам нам выдаст вот это стационарное распределение ну
как бы и да и нет потому что на самом деле на самом деле марков чейн монтекарло
имеет и другую цель он имеет цель не просто выйти на стационарное распределение
он имеет свои цели еще как бы дополнительно среди всех состояний
найти наиболее вероятное потому что представьте себе что у аспеса звездой
каким-то магическим образом вырождается саврино так вот что вырождается что почти
все компоненты ноль одна единичка и где это ничка находится вы в принципе чтобы
найти вам надо все пересмотреть все что есть пессо звездой а это экспоненциально
много состояний и вот когда вы запускаете марков чейн монтекарло вы не
думаете сколько состояний человечек блуждает ему все равно насколько большой
граф ведь так вы можете реализовать его блуждание абсолютно не задумываясь как
это вообще ну сколько там вершин более того если у вас есть симметричная
монетка то чтобы сгенерировать движение вот равновероятно ну скажем по
трем направлениям в общем-то не надо не надо сильно много затрачивать сил
значит смотрите значит вот у вас есть симметричная монетка вы хотите
сгенерировать случайную величину которая принимает равновероятно значение 1 2 и
3 равновероятно как вы можете это сделать ответ возьмите значит начальное
состояние кидайте значит сейчас неправильно нарисовал конечно вот сейчас
и нарисую правильно поторопиться значит вот так вот так это сюда уйдет значит это
вот сюда уйдет а это вот туда уйдет вот вот у меня есть три состояния и я их
1 2 3 я бросаю симметричную монетку значит решка орел резко орел в зависимости у того
что выпала я дальше и ду либо сюда решка орел sponge либо сюда и да LA tu stitches
орел режка орел вот если два раза выпал орел я беру и возвращаюсь вот
сюда это повторяю а если соответственно выпала
кинорешка решка орел орел решка то я уже все окончательно выбрал состояние но
но то если вы проларел орел то я возвращаюсь и повторяю можно показать что
ну в общем-то вероятность того что я зациклюсь она в общем экспоненциально
убывает и вы получаете алгоритм который за логарифмическое время от числа
состоянии с помощью такого количества подкидывания симметричной монетки дает
вам любое распределение вероятностей поэтому человечку не так сложно будет
естественно логарифмическое число раз затрачивая даже если экспоненциально
много соседей ему не так сложно будет с помощью алгоритма кнута яу который
описал поймать понять какому соседу пойти это если вы используете симметричную
монетку как источник первозданной случайности если вы используете
генератор случайных чисел равномерно распределенных на отрезке 0 1 вы просто
берете разбиваете вот у вас есть распределение вероятности вы пишете п
1 п 1 плюс п 2 и так далее и вот вот эти отрезки они будут соответственно длиной
п 2 п 1 и вы мы приготовите таким образом память готовить таким образом память и
дальше кидая случайно вот величину один раз от н при процессинге сделайте
дальше кидая случайную величину равномерно распределенную вы будете каждый
раз ну в общем определять в какой отрезок она попала единственная тут
сложность что если эти числа очень близки то есть очень много засечек на
отрезке 0 1 то с точность вот этого генератора должна быть столько же битов
сколько сколько битов отличаются вот эти это требует дополнительных мы тоже
сегодня об этом поговорим требует дополнительных усилий чтобы генератор
равномерных случайных чисел позволял вам отличать ну то есть достаточно точно
определять какой интервал попало ну еще раз это я только о том что не надо
сильно беспокоиться что сделать шаг алгоритма вот этого это что-то сложное
это не сложно потому что сгенерировать какой-то конечное дискретное
распределение это в общем-то логарифмические проблемы единственное что
там может быть сложно что возможно потребуется при процессе вот это все это
конец истории или что-то еще можно сказать а вот давайте теперь сделаем
такое наблюдение оно наверное будет как бы еще более жизненно ведь в жизни то
не так вот я вам что-то рассказываю но в жизни ты не один человек блуждает по
графу это же очевидно и в жизни все по-другому по-другому конечно но все-таки
что-то похоже есть давайте представим себе реальный интернет он же состоит не
из одного человека а из огромного количества людей n большое которые все
живут на этом графе абсолютно тот же самый граф но они живут и блуждают по
нему все вместе одновременно так ведь теперь такой момент а давайте возьмем
количество человечков вот такое это число людей то есть ну так по порядку n
то есть число людей у меня будет равняться вот этому самому 1 на x квадрате и
каждый человечек будет жить жизнь длиною 1 на альфа но повторю что здесь на
самом деле два логарифма один логарифм связан с mixing time там второй еще вот ну
неважно в общем это сейчас на логарифм мы не смотрим я просто с точностью до
логарифма беру 1 на x квадрате человечков эти человечки как-то
распределены на графе транспортной сеть во и на графе вот этом вот пейдж
ранга и они начинают блуждать независимо друг от друга вы даже можете
пофантазировать кто-то со временем может делиться от пачковаться кто-то от него
и начинать продолжать путь то есть человек блуждает блуждает а в какой-то
момент он порождает знаю кого-то и тот еще тоже начинает независимо будет то
есть вам не обязательно начинать не знаю с большого количества людей вы можете
начать с трех человек они ну с трех как-то не естественно с двух человек
значит они вот скажет поживут какое-то время их станет больше и вот они так и
будут плодиться но дело в том что они уже когда будут плодиться они как бы
жизнь какую-то часть прожили и они уже более-менее равномерно распределены и
точка старта случайно выбирается но это это на самом деле какие-то поправки я
сейчас не хочу в это вдаваться в общем идея в том что если каждый человек вот
что что мы имеем если каждый человек проживет жизнь вот такую а их столько то
верно лишь что я тот же самый результат получу если просто соберу
статистику где кто находится но смысл и сколько человечка в той вершине в
другой вершине улавливаете идею да подождите кто кто сказал что они зависимым
образом блуждают это правда если я от пачков они конечно как бы
как конкретно тот кто отпочковаться зависимым образом но во-первых у нас
увидите марковская цепь нас не сильно смущает зависимость это как бы ну как бы
минус а плюс том что хорошая точка старта но по факту да если говорить
аккуратно давайте не будем заниматься отпочкованием они все независимо блуждают
вот в этом случае
так так ну хорошо они блуждают 1 альфа это что означает что каждый из них вот
давайте как просто подумаем каждый из них что в итоге будет он же
независимо блуждают что каждый из них выйдет на некий вектор состояния ну
грубо говоря п со звездой правильно
ну давайте вот вообще максимально дальше я упрощу ситуацию давайте у нас две
вершины то есть у нас куча людей две вершины чего нам как бы заниматься
какими-то общими формулами если можно ситуацию выродить и почувствовать что
происходит вот у нас граф ну вот самая общая ситуация но из двух вершин 1 2 и
здесь огромное количество человечков находятся они как-то блуждают эти люди
туда-сюда но согласно тем вероятностям которые здесь нарисованы вот они
блуждают блуждают и собственно у каждого человечка не у как мы там
обозначить будем обозначать давайте теперь индекс будем использовать как не
ука значит не ука это случайный вектор который ну давайте значит пускай не ука
это равняется единице если давайте один если если катый человек
катый человек так ну давайте зафиксируем момент времени t который
есть один альфа там по большое что там у нас ну давайте не будем писать
логарифмы на на эти вот зафиксировали момент времени на самом деле здесь 7
он конечно нужно брать поточнее чем то которое мы в итоге хотим но это сейчас
неважно и так если чк ты человек давайте напишу чтобы уника человек а
катый человек человек в момент времени ты в момент и большое находится
находится состояние 1 состояние 1 ну в вершине 1 вот ну соответственно не у
катая 1 равняется 0 иначе ну то есть он находится в состоянии 2 ну что тут
кубочках состояние 2 состояние 2 так хорошо что теперь ну теперь я могу
сказать то же самое про каждого человека ката произвольный и просто
задать вопросом вот у меня есть сумма не укатая тут именно в этот момент
времени сумма не укатая от 1 к от 1 до ну чего там n большого так ведь да то
вы можете сказать про эту сумму вот уже теперь вопрос чисто к вам вот ну просто
чтобы проверить чтобы это не было голословно давайте вот подумаем значит
что мы можем сказать мы близки к цели но еще это не цель еще ничего не
обосновал итак у нас есть сумма случайных величин не укатая 1 ну даже
как бы это случайная величина мы можем сказать какой у него закон распределения
какой у него закон распределения ну приблизительно то есть вероятность того
что не укатая от 1 равняется 1 чему равно ну приближенно чему это равно
ну наверно п со звездой единица так ведь да ну это не очевидно потому что
ну это просто результат о сходимости вот той динамике который я говорил ну
то есть с точностью epsilon штрих с точностью epsilon штрих это так но если
говорить совсем строго хорошо вы можете написать что по модулю это значит
меньше чем epsilon штрих я могу epsilon штрих взять настолько маленький насколько хочу
потому что он стоит под логорифом и поэтому я не морочусь такими вещами ну вот
так сейчас согласны
хорошо на самом деле я понял что смущает сейчас я тогда поясню это таким
образом значит вас смущает что марковская динамика которая вот ну
такая органическая марковская динамика что ей для того чтобы сходится требуется
время 1 альфа так ведь или нет в это смущает откуда это один альф правильно
ну хорошо ладно ну да да сейчас смотрите это все с одной стороны правда с
другой стороны делал в деталях и надо количественно все это говорить значит
в каком-то смысле у вас сомнения что муфосалин мудрец который проживет тысячу
лет он в конце своей жизни будет знать больше чем 10 обычных людей которые живут
по 100 лет неплохие обычные люди но неважно 10 людей по 100 лет проживут и они
возьмут каждый в конце своей жизни соберутся вместе на съезде и скажут как
надо жить среднем арифметическим вот об этом идет речь что 10 людей проживших
по 100 лет и качество их прогноза это мы поняли что параллельно делается и
качество прогноза сделанного муфосалином который прожил тысячи лет
приблизительно одинаково ответ да это так точностью до логарифм в которые я
опускаю вот логарифмы в случае муфосалина будут лучше а в случае вот
этих вот обычных людей они будут их больше степень больше но это вопрос
логарифмический а вот как ни странно это так то есть реально то самое время
которое вот один альфа его и достаток им не надо больше каждый проживет столько
времени там единственная проблема что я писал штрих надо брать точнее я потому
написал аксен штрих ну как сказать понимаете вот совсем недавно то же самое
к этому приду на одной из лекций произошло всток оптимизации я застал
время общаясь с нестеровым когда просто не знаю это вот некоторые революции сейчас
такая когда ну вот естественно было сказать что мы берем значит траекторию
одного процесса достаточно длинную и по качеству но решение задачи сток
оптимизации и по качеству это естественно должно быть лучше чем
запустить параллельно траектории решение той же задачи независимый потом
остановиться и взять средне арифметическая и то же самое сделать то
есть прожить условно 100 итераций но 10 процессорами или одну большую
траекторию из тысячи итераций но одну одну одни процессор и вот оказывается
что ну естественно какими-то оговорками что в общем-то на самом деле это так ну
то есть первым приближение это так и там дальше уже вопрос возникает ну с
какими оговорками и да там бывают сказать вещи такие тонкие ситуации
когда это не так но в целом это в первом приближении так это немного странно но
этим надо пользоваться раз это так ну но но повторю что делал в деталях вот
здесь этих оговорок меньше просто но они не одинаковые то есть там логарифмы
разные ну хорошо коллеги я хотел бы просто еще чего-то успеть рассказать
поэтому давайте мы уже как бы перейдем к тому что вы мне сами подскажете как
вот это все получить как как вообще получить аналогичный результат но уже
вот из этого подхода вот откуда он вытаскивается но давайте допустим цпт
вот что нам говорит цпт что я должен куда написать вот эта сумма а нам
интересно не столько сумма сколько вот эта штука так 1 на n давайте сразу
писать в варианте который нам интересен что из этого я должен вычислить мат
ожидания так замечательно мат ожидания чему равняется
с звездой 1 так дальше я должен на что-то поделить ну у дисперсионер нужна
чему она равна ну давайте давайте это я могу писать константа вам надо
отвечать прям точно ну ну чему ровня дисперсия ну хорошо она равняется ps
и звездой единица единица минус пресса звездой единицы вообще говоря если
пресса звезды маленькая этим нельзя пренебрегать только там надо не дисперсия
средне квадратичное отклонение по моему так ведь да ну еще на коре низен ну
давайте разделим на коре низен так и эта штука приближенно равняется 0 1 так
ведь ну по распределению вот что-то такое естественный слен большое
естественный слен большое замечательно теперь я говорю следующее что значит вот
эта разница вот эта разница если просто совершу некую махинацию перенесу
знаменатель вот сюда вот эта разница она должна равняться должна равняться ну
приближенно соответственно вот величине такого масштабы ps звездой от
единички единица минус пресса звездой от единички на коре низен на соответственно
стандартная нормальная случайная величина но это более-менее то чем мы тут
занимаемся только тут было т альфа t а здесь было соответственно n большое то
есть мы как бы перенесли то есть это означает что если я хочу сделать вот
эту штуку масштаба эпсилон масштабы эпсилон я хочу сделать я должен
поскольку эта величина более-менее порядка единицы но это и стандартная
нормальная случайная величина она с большой вероятностью лежит диапазоне
минус 3 3 и мне там неважно что она там может быть 10 это вероятность
практически ничтожно поэтому поэтому чё
ну вот я получаю что это равняется ипсим отсюда я получаю что n пропорционально
1 на ипсин квадрат я правильно выбрал один на ипсин квадрате то есть вот
откуда это и я как бы сейчас понятия я могу говорить по умному говорить что
это приближенно на самом деле неправильный надо писать неравенство
концентрации и мы будем этим заниматься но пока этого достаточно пока интуиция
нас на как бы она о том что почему этот результат верен и в параллельной
архитектуре а это время выхода то есть я тот же самый результат получил уже в
варианте совсем практичном то есть я как бы запускаю параллельно человечков
каждый живет вот такое время потом я смотрю где они находятся беру
частотно где кто находится и соответственно определяю пропорции уже
прям чисто ну как как реально в жизни и это тоже пейдж ранг неплохо правда
то есть вообще говоря мы этот вектор пейдж ранг получили вполне уже
практической процедурой возникает здесь огромное количество на самом деле
вопросов связанных с тем что блин это как-то все очень слишком просто ведь на
самом деле число моделей и вообще как бы ну вот то что нас интересует их как
бы многообразие намного больше вот как это например связано с сетями массового
обслуживания как например это связано со всякими сток химики на этика и как
это связано с какими-то моделями они знают социодинамики как это связано
например со моделированием распространения коронавируса и вообще
эпидемии это все есть какая-то общая на общее начало ответ да вот это важно
то есть вот на самом деле вы смотрите на некую математику и как бы замечательно
что она лежит в основе еще много чего и вот обратите внимание чем я здесь
пользовался вот вы как бы уйдете с лекции будете там дома о чем-то думать но
если вы запомните два принципа пока только два и вы вы вам точно это ну как
бы сказать пригодится еще уверен многократно вы можете забыть какие
детали но запомните что мы пользовались органической теоремой и по сути
центральной предельной теоремой по факту мы пользовались двумя пределами с
одной стороны мы дали системе пожить достаточно долго и поэтому появилось
понятие мера появился понятие понятие стационарное распределение вариантная
мера а потом мы взяли скейлинг или то сказать чесну некий предел по числу
агентов и мы стали эту меру как бы выделять как то-то что характеризует
концентрацию вот вот уже как бы на каком многоагентной системы то есть то что
нам надо было получить мы получили как два предела как предел числа агентов и
предел на саму предел времени я мог в другом порядке это сделать то есть я
мог например сначала сделать предел по числу агентов и и в этом случае и в
этом случае если бы я сделал предел по числу агентов я бы получил ту самую
динамику которая была вот это вот полноценная динамика которая ну как
называется пауэр метод вот это то есть если бы обратный порядок предела сделал
я сначала бы сделать бесконечно много агентов то у меня бы вот это эволюция
полу функции плотности она бы в точности превратилась эволюции этого
вектора и было бы не интересно а вот за счет того что я в обратном порядке
предельные переходы сделать то есть сначала я время устремил бесконечности а
потом число людей это вот получилось достаточно интересно интерпретируемо
так вот что я хочу сказать это же самая конструкция используется многократно
много где и вот эти эффекты как эргодическая теорема по сути принцип
неподвижной точки и концентрации меры види цпт чего-то еще это те два
атрибута которые будут в течение курса встречаться в совершенно разном
контексте но они будут более-менее вот объяснять как из явления которое
сложное вытащить что-то вот такое вот конечное этим пользоваться есть ли
вопросы потому что я сейчас рассказал мы сейчас будем закреплять эту тему вот
нет сейчас вот как бы представьте себе что помните здесь рисовал монетку как
ну как бы кнутая алгоритм ведь когда мы кидаем монетку связанную с одним
человеком и другим человеком просто постулируется что эти монетки разные
независимые и они блуждают независимо точки старта выбираются независимо и
все что как бы мы чем пользуемся это фактом что по прошествии вот этого времени
распределение вероятности которая связана вот с тем или иным человеком оно
значит близко кого-то к этому вектору пессо звездой и все независимость тут
появляется вот этих случайных величин просто по построению то есть это как раз
не сложно здесь как бы есть зависимость точки старта и текущего состояния в
этот момент времени но нет зависимости между ними а зависимость от точки старта
не страшная потому что мы пользуемся здесь когда пользуемся центральной
предельной теоремой независимостью вот этих вот случайных величин то что это
случайная величина частично определяется точкой старта так это не страшно это просто вопрос
качество опроксимации вот этой штуки то есть ну будет не совсем точно ну и тут байс какой-то
надо будет учитывать но поскольку это все под логорифом в той оценке то я могу все вот эти
вопросы загнать ну с запасиком то есть это надо аккуратно делать но когда что-то под логорифом
можно об этом сильно не думать этот крас тот случай так теперь я расскажу довольно странную
историю насколько я понимаю реальная история была был перехват письма из тюрьмы тысяча символов
из американской тюрьмы насколько я понимаю давно значит в этом письме люди видимо увлекающиеся
перлаком холмсом подобно тому как пляшущие человечки начата код был ставил соответствия
буквам английского языка и символом разных человечков вот так в этом же письме тоже были
нарисованы человечки их было 10 тысяч ну так я приблизительно говорю и возник вопрос как бы
что написано в этом письме ну потому что вообще говоря да кстати нет вопросов чате отлично
вообще не отлично ну значит у меня есть время и дальше еще раз ссылку начать начать в общую
группу за ведь так это как бы как противоречиво получается то есть если там и кинут ссылку начать
ну ладно это не ко мне да хорошо значит м-7-7 то есть тема которую мы сейчас развиваем это
продолжение вот этого называется марков чейн монте карл так вот было письмо 10 тысяч символов 10
символов и функция которая сопоставляет буквы там английского алфавита человечка в общем таких
функций столько сколько перестановок то есть у нас есть массив из 40 символов ну буквы плюс всякие
символы типа запятая и человечки ну сколько уникальных там человечков было вот ну давайте
для простоты считать 30 это сейчас не так важно вот один в один да ну то есть да да да да да да да
действительно если взять все все правильно вы говорите сейчас все будет вопрос не в этом то есть
я говорю у нас цель не столько решить задачу сколько продемонстрирует некое единобразие
математики это действительно важно потому что ну очень как бы так классно получается что одна
и та же математика решает огромное количество задач и действительно есть какие-то свои особенности
у каждой задачи они нам важны но это второстепенно потому что главная математика вот в нашем курсе
действительно предварительно можно просто посчитать вот мы берем обучаем например какую-нибудь
ну программу которая считает частотно текстов война и мир на английском сколько в войне и
мир на я не знаю на весь объем на условно миллион символов букв сколько буквы а раз встречается и
вот частотным образом мы обучаем такую программу и соответственно просто смотрим сопоставление
частот сортируем массив частот сортируем массив частот встречаемости человечков и получаем
предварительно некую функцию f которая собственно и есть этот самый изоморфизм изоморфизм который
вот здесь строится то есть у нас есть предварительный кандидат предварительный кандидат есть но дальше
возникает следующий как бы этап понимание что качество такого ну это просто реально есть статья
персидиаконис в которой прямо написано что будет если если так сделать то есть вот если вы
сделаете 2009 года если вы сделаете такую процедуру там была реально письма письма с человечками и
то что получается получается честно сказать но не очень хорошо я такое не могу понять я просто по
английски не то чтобы классно читаю но это прям вообще непонятно и поэтому было решено сделать
следующее что вообще говоря есть не просто частотная закономерность букв а есть закономерность с
какой вероятностью после одной буквы идет другая буква то есть надо проходить по войне и миру не
просто считая частоту встречаемости а а как бы определяя вероятность того что после буквы
давайте последовательно все это получается при постановке человечка в букву английского
алфавита и вот собственно x к f от x к плюс первая вот эту вероятность можно оценить в смысле не
можно оценить она есть то есть для любых двух букв я не знаю па б да мы можем оценить чему
равняется вероятность того что за букву а пойдет буква б и таким образом мы можем написать
вероятность любого текста которые выявляется то есть мы просто берем первую букву по частотному
закону пишем то есть п от f от x согласно частотной вот этой таблице потом мы берем п но и ф от
x 0 потом мы берем п от f от x 0 на f от x 1 потом мы берем вероятность того что f от x 1 на f от
x 2 и так далее и вот вот так вот посчитанная вероятность это есть вероятность того что ну
значит как бы вероятность функции f то есть мы можем приписать конкретному способу дешифрования
вероятность правильно то есть я уже ничего такого хитрого сейчас не сказал а какая разница нам
же эти не нам же важно ни к чему равняется это вероятность а как они между собой соотносятся
понятно что она будет не не ну понятно что она будет вы можете логарифм считать этой вероятности
какая разница то есть это вы можете умножать на 10 десятые это же не очень важно вот ну больше
того она сильно того ноль не улетит все-таки число пар оно не то чтобы колоссально большое
ну 40 на 40 да ну сколько это будет значит ну 10 в третье ну будет значит соответственно
каждый такая штучка 1 на 10 третьей но значит вы просто можете ну текстом 10 тысяч вы просто
можете смотреть на этот порядок который одинаковый его просто не учитывать это для нас неважно то есть
для нас эта функция определена как бы с точностью до множителя нам этот множитель абсолютно не
важен для нас важно что они друг от друга вот отличаться будут ну сильно но не так чтобы
очень вот мы на этом сказать будем играть ну да они кстати между собой могут сильно отличаться
хорошо какой какая стратегия действия какой бы f вы могли бы выбрать то есть как бы как что
вам кажется правильно выбрать вот в этой задачи максимум по f взять правильно ну то есть вы решить
задачу максимизации то есть перебирать каким-то образом но это же переборная задача правильно
понимаю ну и кто-то знает какой не переборно решать это максимум правдоподобия но а как
ее решать то есть градиент не посчитайте а теперь такой вопрос я это зафиксировал а теперь вы давайте
сюда вернемся вот почему ни у кого не возник вопрос что я написал средне арифметическая почему
все как-то приняли это как ну на веру и никто не спросил а почему средне арифметическое почему
я не взял какую-то другую напрямую не смещенную оценку ну то есть это настолько естественно что
мы не задумываемся на самом деле но это не то чтобы прям ну как сказать всегда очевидно но еще
раз вот почему почему имея значит такую случайную величину вот не укатнил давайте
катая до 1 и зная что ем от ожидания равняется неизвестному параметру ps 2 1 почему я использовал
именно средне арифметическое но замечательно представьте себе что я сумасшедший лучник беру
так стрелу так бац и стенку так вот ну вот туда вот так вот с равной вероятностью любой угол
выбираю ну и потом я прошу вас определить расстояние на котором я нахожусь от стенки чтобы
вы делали правильно там распределение каши это значит что средне арифметическое будет
иметь такой же закон как отдельное слагаемое то есть о том что возьмете средне арифметическое
у вас закон распыли не поменяется там надо брать тогда что-то типа медианы или не знаю как-то
обрезать там ну что-то хитрее действовать то есть еще раз ничего и ничего особо умного есть
так он распределение как я кидая стрелы накидываю но как бы этот закон зависит от расстояния если я
вот так стою то стрелы будут значит одним образом ложиться если я стою вот далеко то стрелы будут
другим образом ложиться получается что этот параметр неизвестный можно как раз попытаться
восстановить из того что там ну статистика моих попаданий куда-то а я равновероятно кидаю так вот
и не очевидно что из этого закона надо бы именно вот то есть это не может быть не лучший способ
возникает вопрос а как при каких условиях почему именно такая оценка возникла вот давайте
попробуем сопоставить вот с этим подходом чтобы почувствовали что на самом деле об одном и том же
а на самом деле все очень просто если у вас есть вероятностная модель она у вас есть параметрическая
модель вот она у вас собственно есть что вы имеете совокупность простую выборку независимых
одинаково распределенных случайных величин вот она вот ну дальше и возникает вопрос какова
вероятность того что выпадет то что выпадет ну если истинная вероятность п давайте так назовем
выпадение орла или выпадение вот состояние 1 то я могу написать что вероятность того что
выпадет конкретная последовательность это есть вот такая вот сумма нюкаты от единицы кат одного
да сколько у нас там было n большое на единицы минус п но эти п с извездой 1 буду писать
п с извездой 1 на значит соответственно n большое минус сумма нюката согласны что это так или есть
какие-то другие гипотезы как это должно выглядеть может биноминальный коэффициент я опустил или не
правильно почему я не опустил почему мне должно здесь быть потому что те действительно я
сформулировал задачу так какова вероятность того что выпадет конкретная последовательность но
если конкретное последовательства мне не надо учитывать сколько в ней было успехов если я
учитывал чтоmania успехи могут быть в разных местах я бы тогда здесь поставить биноминальный
коэффициент а мне не интересно где как бы разные учитывsc underwear? конкретной послед utilities я
вот я считаю сколько раз было орел это вот число успехов и сколько раз решка число неудач
но это конкретная последовательность поэтому мне не надо учитывать кратность враждения теперь такой
вопрос вот это вот что такое это есть правдоподобие это есть аналог того что здесь написано только
здесь я подбираю f я работаю в пространстве функции устанавливающих азаморфиз между буквами
человечками а здесь я работаю в пространстве ну как бы однопараметрическом вот этот параметр
на самом деле я упростил ситуацию до двух вершин если бы я не упрощал до двух вершин естественно
мне бы надо было здесь писать многопараметрическое все это дело и тогда бы здесь было мультинамиальное
распределение и было бы немножко посложнее но физика та же самая то есть получились бы те же
самые оценки только по компонентно и дальше что я должен делать что о чем говорит принцип максимум
правдоподобия правильно то есть п оценка вот это вот оптимальная оценка некотором смысле п с
крышкой она должна равняться ну как бы арг максимум арг максимум вот этого выражения арг максимум по
п по п со звездой единичка это вот п с звездой единичка но возникает вопрос хорошо я вот такую
оценку найду собственно я ее нашел это вот и есть такая оценка доказывается очень просто арг
максимум от этого выражения есть арг максимум от логарифма этого выражения когда я возьму логарифм
меня эта штука перейдет в числитель логарифм п будут здесь будет логарифма 1 минус по я
могу продиференцировать по п минимум достигает максимум достигается в этой точке то есть вот
это и есть та самая оценка а дальше работает такой результат фундаментальный результат который
называется теоремы фишера мне хотелось бы это рассказать потому что мы сейчас этим будем
пользоваться результат заключается в том что вот то что я здесь говорил про
центральную предельную теорему это собственно общий результат для таких
оценок то есть вот такая такого типа штука то есть вот эта оценка оценка
f с крышкой f с крышкой да значит f с крышкой со звездой 1 причем это
может быть вектор на минус п с звездой 1 значит будет меньше либо равняться
некая константа ц давайте я напишу ц и от соответственно ц от настоящего
значения она зависит п с звездой 1 но его здесь будет стоять логарифм сигма
минус первый на н вот такая вот штука и все это верно с вероятностью с
вероятностью 1 минут сигма вот это верно возникает вопрос но и чего я
такого написал особо неожиданного ну то есть во первых во первых вот чтобы
понять за что идет борьба вот сразу скажу что вот это дело оно
получается при любых разумных подходах и вы можете брать не обязательно подход
максимум правдоподобия какой-то еще и вот вот вот это будет более-менее так
но если это разумно борьба идет за вот эту штуку то есть оценка максимального
правдоподобия давайте я здесь явно напишу вот она вообще говоря
естественно зависит от объема выборки который у нас и поэтому это
коэффициентик вообще говоря зависит от н но теорема фишера утверждает что вот
это вот тн цен то есть предел приен стремящимся к бесконечности вот этого цена
тета равняется некий но не тета п с звездой в нашем случае
п значит со звездой один с крышкой он равняется просто ц от п со звездой один
с крышкой и вот эта штука она минимально среди всех возможных оценок то есть вы
можете как угодно оценивать но в пределе когда число объем выборки в
нашем случае число человечков стремиться к бесконечности вот вы не
сможете предложить такой способ оценивания который был имел константу
лучше чем подход максимум правдоподобия понятно о чем речь или не совсем
предел смысле числовой последовательности это это все числовые числа
вероятность здесь как бы она сидит в том что вот только это случайно это
случайно и поэтому я говорю с вероятностью все остальное правая
часть естественно какие-то числа оценка это с крышкой то есть вот это и есть вот
это средне арифметическое не у меня к сожалению с обозначениями какая-то
проблема я чувствую что обозначение меня плохие я прям это чувствую ничего
сделать с этим не могу я поясню почему потому что я как бы частично импровизирую
и вот мне кажется что мне сейчас они удачно выбрал обозначение да
константа зависит все-таки от настоящего по значения параметра и
истинного вот вопрос правильный я сейчас мы до этого дойдем а как на
практике вычислять мы же не знаем это значение вот тут будет ответ на ваш
вопрос то есть проблема в том что ну я не знаю то есть на самом деле мы эту
константу в данном случае можем явно написать ну смыться я ее сейчас напишу
cn от п со звездой единица с крышкой ну без крышки было но это в нашем случае
равняется корень из п со звездой 1 на 1 минус п со звездой 1 мы же знаем чему
это константа в нашем случае равна ну и тут какое-то число я ну просто число
просто число там не знаю церк ну двое двойка там я не помню чему она равна и
мы знаем чему она равна в нашем случае вот это константа и возникает проблема
что а как использовать эту оценку на практике и вот тут подсказка как раз так
и звучит что если вы хотите пользоваться это оценкой возьмите это
неравенство и подставьте разверните его правильным образом и подставьте
оценку сверху п со звездой от 1 то есть вы получите вместо п со звездой а тут
оценку снизу то есть подставьте сюда оценку сверху вот отсюда
п со звездой от 1 и отсюда то есть это будет п со звездой от 1 плюс вот это
поправка но возникает в каком-то смысле порочный круг потому что в этой
поправке в свою очередь сидит п со звездой и ясно что когда вы будете
заниматься вот этим вот я вообще не советую этим заниматься но если так
будет желание то в какой-то момент вам надо просто остановиться и уже не
заниматься поправкой то есть вам надо это называется философия построения
симпатического ряда то есть вы столкнулись с проблемой вы не знаете как
сюда подставить п со звездой но вы имеете формулу которая оценивает п со
звездой и сверху и снизу проблема в том что в этой формуле в свою очередь
сидит п со звездой но она уже сидит как бы на уровень ниже то есть влияние
вот этого п со звездой меньше будет потому что там еще корень добавится и
вот вы значит оцениваете здесь п со звездой от 1 вот этим вот и следующие
слагами будут уточняющие уже с дополнительным фактором один на корень
из н и вы можете разложить в ряд по один на корень из н а симпатический ряд
красиво правда ну ладно не удержался это не было в планах но раз вопрос
возник так вот еще раз зачем я сейчас вам это рассказываю затем что понимаете
вот это все об одном и том же вот здесь максимум правдоподобия там максимум
правдоподобия но если объем выборки большой то это не просто какой-то
принцип оценивания а это принцип оценивания который приводит к наименьшим
дисперсии наименьшим ну вот это все-таки не очень строго ну как бы на
меньшему доверительному интервалу доверительному множеству то есть вы был
получается из данных вычленяете ну самый точный как бы вы самым точным
образом оцениваете неизвестный параметр то есть заданной вероятностью вы на
и более компактно локализуйте неизвестные параметры не с точки
зрения зависимости от н а с точки зрения вот того как будет входить этот
нождель вот ну а симпатически проблема в том что это результат а симпатически и
на самом деле к сожалению хоть не а симпатической теории есть но она
принципиально не не исправляет оговорку что это только на бесконечных
временах то есть при конечных временах при конечных n это может быть не самый
лучший способ то есть метод максимального продоподобия он
гарантированно и лучше только для экспоненциального семейства которому
принадлежит схемы испытаний бернули то есть это это гарантированно для любого
n будет наилучшая но это не всегда так ну и когда много параметров понятия что
такое лучшее не совсем понятно потому что это один компонент а у нас их много
лучше по одной компоненте но вот оказывается что равномерно на и лучше по
попа то сказать всем направлением мы еще скажу что когда мы говорим о лучшее не
лучше это тоже сложно говорить потому что здесь есть п со звездой и в пункции
может быть самая маленькая при то есть при заднем п а при другом п другая
другой способ оценивания лучше вот что примечательно метод максимального
правдоподобия он именно говорит о том что вся функция для всех своих
аргументов имеет значение соответственно лучше чем любой другой способ не для
конкретного для всех вот это не тривиально это действительно такой очень
мощный принцип как оценивать что-то что вы не знаете вот ну согласитесь это
просто вы пишите вероятность того что запараметризуйте запараметризуйте то
что вы не знаете по это параметр и найдите оценку максимального
правдоподобия а симпатически это оценка на и лучше ну при некоторых
условиях регулярности вот ну и вы пользуетесь этим ну к слову насчет
условий регулярности это вот такое хорошее упражнение я раньше когда в
москве надо было часто бывать ну просто мне стало интересно сколько
ступенек на эскалаторе на водонах я ездил через водонах я каждый раз вставал
на ступеньку они пронумерованы ну там они на пронумерон через каждый 10
ступенек и ты видишь только более менее ступеньку которая в твоей
окрестности то есть я встаю на ступеньку вижу ага там 3 150 160 в какой-то день я
увидел по моему 180 ступеньку и мне было вопрос сколько вообще ступенек вот я
месяц так ездил а ступенек понятно что больше чем 30 и значит я наблюдал давайте
для простоты считать что ступеньки все пронумерованы и вот я каждый день
наблюдал ну словно каждый день ступеньку с номером возникает вопрос как
мне оценить ступеньку максимальное число ступенек то есть у меня есть
выборка из равномерного распределения на отрезке 1 и неизвестное мне число n
давайте я назову это параметром это не что я отвлекаюсь но просто это реально
как бы случай жизни по моим интересам а значит это вы не знаете как бы вы
оценивали если у вас есть выборка x и t каждые случайно величайки каждый
имеет равномерное распределение на отрезке 1 это вот чтобы вы сделали
мат ожидания к ситово чему равно вот это пополам ну значит если я возьму
средне арифметическая 1 на n и сумма иксит их ну или кат их у нас кайт
обозначается сумма икс кат их кат одного до n то мат ожидания чему будет равно
ну мат ожидания будет равняться это пополам так ведь но если я соответственно
поставлю здесь двойку у меня мат ожидания будет это правильно то есть я
могу оценивать неизвестный параметр это тем что взять средне арифметическая
чесну вот каждой ступени которые мне встречались умножить на два и получить
какую-то теорию которая приведет более-менее такой оценки так как ни
странно так но это неверно с точки зрения теорема фишера то есть теорема
фишера говорит об какой-то оптимальности только в случае когда носитель
распределение не зависит от параметра обратите внимание на то что здесь
написано у меня распределение от единицы до тета то есть у меня неизвестный
параметр тета как бы входит носитель я не могу дифференцировать например по
знаком каком-нибудь интеграл а то есть это не есть регулярное семейство у меня
то множество мера где значит она функция отличная от нуля функции
распределения зависит в свою очередь от параметра вот в предыдущем модели
такого не было и как ни странно вот здесь оказывается что есть оценка
которая намного лучше а именно я беру x максимальный то есть я беру это
называется ну n-то порядковая статистика то есть максимальный элемент выборки я
беру максимальный элемент выборки x ката кат одного дн и умножаю его на n плюс
один поделить на n вот такую оценку беру то есть чуть-чуть корректирую и
оказывается что эта оценка имеет дисперсию которая пропорционально 1 на n
они 1 на n в квадрате и эта оценка естественно сильно лучше она вообще из
другой лиги она просто но это противоречия с
теоремой фишер но противоречия нет потому что потому что это сюда входит не
совсем так как надо но это тонкости это я вам сейчас просто привел как бы некий
результат что вы понимали что ну вообще не все что сказать ну не как сейчас
модно говорить это другое это как бы немножко другое это не то же самое что
вот потому что носитель зависит от распределения и не надо это как бы вот
делать тут аналогии то есть все-таки мы пытаемся как-то единобразно смотреть но
где-то надо проводить черту и говорить что извините не так не работает хорошо
возвращаемся сюда значит что мы сейчас поняли то мы вообще узнали уже как бы
кульминация мы узнали что есть вот органическая теорема есть всякие
неравенство концентрации но мы пока пользуясь простейшими вариантами
предельных теорем но и есть принцип максимума правдоподобия понятно что все
это как-то уживается давайте посмотрим как это уживается марков чейн монте
карло и это будет кульминация вот на этой задачи как это все вместе поженить
смотрите значит все на самом деле достаточно просто если мы придумаем
марковскую цепь какую-то динамику которая бы имела равномерное распределение
стационарная неравномерно стационарная инвариантная вот это вот как бы это
задача то есть если нам удается придумать такую динамику то дальше нам
не обязательно я не знаю заниматься тем что просто реально сравнивать какие-то
функции мы можем сделать очень простой процесс мы можем запустить как бы этот
марковский процесс запустить марковский процесс и просто посмотреть куда он
сойдется вот я сейчас опишу следуя той самой статье диакониса алгоритм и мы
за оставшееся время попробуем понять почему он работает у нас как раз
остается час если хотите можно сделать перерыв немножко снизить и следующей
лекции они будут более конкретные то есть мне важно было и на первой лекции
телом как показать немного что что у нас дальше будет и много из того что мы
сейчас с корреговоркой проговаривали оно будет действительно нормально
рассказано частично то есть вот то что я сейчас говорит с цпт там все как-то
не аккуратно махал руками много из этого в другом контексте нам еще раз
встретиться и там я докажу уже не цпт а не равенство концентрации и но в
другом контексте но вы можете это не равенство применить вот просто чтобы
здесь более аккуратно получить и вот это вся движуха она просто позволит ну
где-то вы с корреговоркой это все как бы мы проговорим а где-то прям будет
честно если посмотреть но я так хочу сделать на курс в целом то в конечном
итоге почти все основные такие математические трюки будут рассказаны в
деталях вот и у вас не должно остаться так впечатление что а вот это мы
проскочили ну будет почти все покрыто такая цель я не знаю в какой степени это
удастся ну давайте действительно подумаем что у нас пока есть на данный
момент у нас есть граф которым состояние которым есть состояние то есть давайте
этот граф так и опишем то есть вершина графа это различные функции f1 f2 f3 и так
далее ну какие-то функции это ничего хитрого я сейчас не говорю вообще все
что сейчас буду говорить она настолько тривиально что как бы сказать это
доступно школьникам даже не то что школьникам а школьникам не знаю 9 классы
18 но если вот как бы потом это все посмотреть в совокупности то получится
что блин а ведь это совершенно универсальная штука которая классно
работает но математика элементарно ну во всяком случае чтобы это запустить
чтобы это заработала понять почему это не элементарно а вот как бы чистое
описание будет элементарно и вот не удивляйтесь что сейчас все очень просто
в этом есть некий подвох вот это простота она она специально так сделано
что вы сейчас почувствовали на самом деле все это конечно может работать куда
как более сложных контекстах но вот я хочу вас предупредить что простота она
как бы немножко здесь как сказать опасно опасно в том смысле что ну
имейте в виду что на самом деле это очень крутые идеи и они очень классно
работают и несмотря на простоту это реально очень мощная вещь и так что я
сейчас делаю я сказал что есть всякие способы кодирования теперь мне ходит
идея я хочу составить граф в котором каждой вершине отвечает свой способ
кодирования дальше на этом графе я хочу ввести такое распределение
вероятностей которая бы отвечала вот этой функции то есть стационарное
распределение вероятностей пи от f было бы вот это ну с точностью до какого-нибудь
номирующего множителя чтобы это было адекватное выражение то есть у меня
экспоненциально много ну 40 факториал состояний эти состояния я как-то
пронумеровал мне неважно что их я не могу описать нигде хранить мне это
неважно мне неважно что я реально не могу задать эту функцию это все сейчас
неважно важно то что я могу придумать какую-то реальную динамику блуждания
человечка который блуждает по этому графу достаточно долго а потом он
соответственно после какого-то количества итерации выходит на
стационарное распределение которое дается вот этой формулой потому что я
подбираю способ блуждания ровно так чтобы стационарное распределение было
вот это вот и дальше я пользуюсь таким фактом то если это вот есть распределение
вероятностей то проявляется эффект эффект большой выборки то есть если
дополнительно вот здесь x который максимальный xn там я не знаю он большой
там x4 и так далее xn большое вот если n большое большое то у меня приблизительно
то же самое наблюдается что и здесь когда n большое то есть я выписываю
некую вероятность видите здесь тоже вероятность я просто говорю что давайте
найдем максимум этой вероятности я вообще ничего хитрого не сказал я сказал
давайте найдем максимум этой вероятности и качество этого максимума у нас один на
корень из n это уже неплохо то есть один на корень из n это n объем выборки здесь
объем выборки число букв число букв это каждая буква дает новую какую то есть
это как бы аналог аналог числа числа человечков число букв есть у меня текст
достаточно большой но 10 4 это вполне себе текст это больше страницы то ну это там
как раз где-то страница была по моему то это вполне есть вот аналог что я
надеюсь к чему я отклоню что я надеюсь что просто максимум этого выражения это
будет некоторая формула но если я идеально найду этот максимум которые
а симпатически ну в нашем случае это симпатика выполняется как бы она будет
близко к настоящему так сказать ну как сказать к настоящему значению к истинному
f то есть если я верю в то что текст сгенерирован согласно вот такой модели
то я реально получу с большой ну с близкую близкое значение вы конечно
можете сказать что блин но это же не так никто из заключенных не знает
марковские процессы во всяком случае скорее всего и не занимается тем что
генерирует случайный текст согласно марковской модели генерации текста
предварительно прочитав войну и мир и сделав частот на анализ но это же бред но
это бред он во всем анализе данных то есть когда вы пытаетесь что-то значит
определить вы занимаетесь тем что проектируете реальность на вашу картину
этой реальности ваша картинная реальности обычно параметрическая или
какая-то которая как-то ну вот как вы видите мир и вы в этой картине мира
пытаетесь подобрать наилучшие ну как бы так такое такие параметры такое
мировосприятие сформировать которая на вашей картине мира наилучшим образом
этот мир описывает это очень естественно и собственно даже если
эта модель так называемо misspecified то есть она не как бы не настоящая вы хотя
бы найдете проекцию реальности на эту модель наилучшую проекцию реальности то
есть я как бы сейчас говорю вещи которые может быть реальны но это важно
понимать потому что это постоянно нам встречается в жизни в анализе данных то
есть не надо буквально понимать что жизнь так устроена вы просто ищите
проекцию жизни на эту картину мира и вот если объем выборки достаточно большой
число слагаемых число сомножить или тут большое тот тот же самый эффект мы в
праве ожидать по принципу максимум правдоподобия ну повторю что максимум
правдоподобия выборка простая здесь выборка марковская то есть у вас текст
генерируется марковским процессом здесь есть проблемы что как бы в плане
независимости скорости сходимости может быть медленнее но есть книжка
брагимова хасминского там все это как бы не так существенно то есть принцип
максимум правдоподобия вы можете вообще брать стационарный процесс или там
необязательно и то же самое то есть вот это вот результат о том что если
траектория достаточно длинная не обязательно независимой реализации просто
траектория наблюдения достаточно длинная то это дает возможность оценивать
параметры мы об этом будем говорить когда будем на эргодичность смотреть с
другой стороны это все будет то есть это я вам точно обещаю этого будет много
многократно это будем проверять про то как датчик случайных чисел работает
вообще про эргодичность теории динамических систем как это связано со
стахасик это небольшой сюжет но очень яркие я думаю все это ну оценить и ну
пока пока идем дальше и пока мы поясняем следующее что у нас есть
значит состояние у их 40 факториал столько сколько символов и дальше нам
надо ввести правила блуждания по этому графу давайте считать что самым
естественным правилом является транспозиция потому что с помощью
транспозиции можно по любую перестановку получить что такое
транспозиция заключается в том что буква а кодируется значит соответственно
этого человечка а букву z кодирует вот этого
человечка да ну я условно так транспозиция заключается в том что вот
так я делаю то есть преобразование заключается в том что ну давайте напишу
значит здесь как бы не знаю как это красиво написать но давайте то есть то
есть смотрите если у нас здесь допустим f2 оно значит xк переводит в
значит вот такого человечка а xп переводит в вот такого человечка
ну вот вот такого два разных человечка два разных человечка xк и x
какой-то петой то я рисую здесь стрелку подразумеваю что вот здесь xк
переходит вот в такого человечка а соответственно xп буквы xp ну согласно
этой функции переходит вот в такого человечка все остальное одинаково ну и
обратно то есть я считаю что есть и обратное преобразование то есть значит
f2 от f3 отличается тем куда переходит буква xк xп я понятно сейчас говорю или
не очень то то есть у меня правило такое что вершинка соединена с другой
вершинкой если она отличается на одну транспозицию только сколько транспозиции
столько соседей ну то есть не то чтобы их очень много то есть только сколько
там пар букв час сейчас правильно но свое время следующий вопрос возникает
такой но хорошо я вел граф по которому вполне реально блуждать так ведь ну
давайте сначала это поймем реально блуждать по такому графу или нереально ну
с точки зрения практики вы можете как бы бросить монетку если есть тут
вероятности и соответственно сделать эту транспозицию ну в чем проблема давайте
я сейчас сначала просто опишу процедуру а потом соответственно мы
поймем почему она такая и вот не удивляетесь про статья процедура такая
у меня я допустим нахожусь здесь у меня есть соседи я равновероятно выбираю
любого соседа равновероятно соответственно выбираю соседа и дальше
смотрю следующее вот значит он сюда вот значит тут какая-то перестановка там
f не знаю 4 и не важно как я пронумеровал и дальше я смотрю следующее если пи от
f4 больше чем пи ну даже да больше либо равно чем пи от f
чего тут 2 тогда я что делаю так вам кажется что я должен делать ну из
логики как бы сказать ну я надо должен там и остаться ну потому что это лучка
я же максимум еще значит это с вероятностью единица то есть ну как бы
получается не с вероятностью единицы а с вероятностью 1 на м то есть если пи от
f4 больше чем пи от f2 то тут стоит 1 на столько сколько соседей м число
соседей и сло соседей так ведь еще раз смотрите а что я в чем и в чем как бы
вообще идея я хочу найти перестановку это вообще задача комбинаторной
оптимизации то есть я хочу найти перестановку которая по принципу максимума
правдоподобия дает максимум правдоподобию а то есть я могу выписать
для любой для любого способа кодирования могу выписать вероятность один способ
имеет одну вероятность другой способ имеет другую вот эта матрица мне дана то
есть я сначала прочитал льва толстого и частотным образом определил как много
пар а б как много в тексте толстого на английском языке раз встречается пара
а б я взял число встречаемости а б поделил на общий число пар буб и нашел
чистоту то же самое я сделал любой другой пары букв у меня появилась матрица
теперь я эту матрицу подставляю в зависимости того как я выбираю функцию это
как а ракуле и к скает человечки я значит беру ставлю вот для двух человечков
ставлю в зависимости от f ну вот такую матрицу переходных вероятность если
другая функция здесь другая вероятность будет но одна но по матрице п а б ну а
б пробегают все буквы я могу восстановить матрицу отвечающую любой
функции дальше по этой матрице отвечающий функции я могу просто взять
произведение их и просчитать вероятность текста какова вероятность
того что заключенный написал текст вот такой если функция кодирования f то есть
если я понимаю его то сказать и человечка вот так то какая вероятность
что это вот вот вот вот текстом такой написал а если функция другая то текст
будет другой расшифровывается по другому вероятность будет другая я хочу
найти такое и при котором это вероятность наибольшая по аналогии с
тем как я хочу найти здесь такое п при котором вот эта вероятность наибольшая
роль п роль п здесь играет функция f только здесь п единичка и мне вот это все
как бы сказать тривиально тут все получается а вот здесь это огромное
пространство состоянии 40 факториал перестановок и собственно чтобы найти
максимальный f которая здесь доставит максимум что я хочу сделать я хочу
придумать хочу придумать такое такие веса рёбер чтобы блуждая человечком по
этому графу я через какое-то время равное mixing time вот это 1 альфа я был в
той или иной вершине с вероятностью вот такой то есть еще раз f это и есть
состояние то есть вероятность оказаться в состоянии f2 стационарном
режиме на 8 тотики это есть вот то что я сюда ставлю f2 а вероятность оказаться
в состоянии f4 но надо сделать транспозицию пересчитать эту
вероятность но замещу что мне это реально не надо делать по всем вершинам
это я как бы гипотетически говорю где я живу и блуждаю по факту я должен делать
что я должен находясь в текущей точке старта которая получается частотным
образом просто по частотам определяют стартово ф и дальше начинаю блуждаешь
как я начинаю блуждать ищу равновероятно соседа что такое ищу равновероятно
соседа я фиксирую случайно выбираю 1 букву например я хочу заменить ну что
я фиксируя первую букву я например беру человечка этого ну и случайно
выбираю второго человечка смотрю какая букву отвечает вот этому человечку и
второму но их случайно выбрал и просто меняю местами перей moonsky и
и, соответственно, перехожу туда, куда, что я выбрал.
Столько, сколько соседей, столько, сколько транспозиций,
столько, ну их будет сколько.
Соответственно, первую букву я могу выбрать количество раз n-1,
а вторую, ну сейчас, первую букву выбирается n раз,
вторая n-1. Ну то есть, если у меня число букв n,
то это приблизительно n в квадрате.
То есть, это m, это приблизительно n в квадрате.
Ну вот, я, это не очень много.
Ну, это и важно, что все вероятности одинаковы.
Да, то есть, эти вероятности одинаковы.
Значит, я выбираю одну из соседей, дальше я проверяю.
Если ты мой сосед, и ты лучше меня,
то я принимаю твою, как бы сказать, сторону,
и я такой же, как ты становлюсь.
В смысле, я просто, я сюда перехожу точно.
А вот если мой сосед, ну то есть, если это первый же режим,
давайте это как-то четко напишем, чтобы это закрепилось.
То есть, первый режим, первый режим, что
p от f4, допустим, больше, чем p от f2.
Тогда соответствующая вероятность перехода f2, f4,
вероятность перехода, она что?
Она равняется 1 на m, то есть, равняется 1 на n, на n-1.
Вот. Ну, как бы понятно, что это можно сделать для любой вершины.
И по факту нам не надо заранее эту матрицу хранить.
Вы это можете по факту определить. То есть, вы сначала
генерируете соседа, это делается, ну как бы, без всякой математики.
Просто равновероятно выбираете две буквы, два человечка,
меняете местами f, а потом уже проверяете это условие.
Если это условие выполнено, то вы туда переходите.
А вот если оно не выполнено, если оно не выполнено,
то, то есть, как бы, переходим, переходим, переходим, переходим.
Я так проще напишу. А вот если оно не выполнено, то есть,
π от f4 будет, соответственно, меньше, чем, ну давайте,
меньше либо равно, чем π от f2. Ну, тогда, тогда с вероятностью,
соответственно, π от f4 на π от f2, я все равно перехожу.
То есть, я разыгрываю, значит, монетку с вероятностью
π от f4 на π от f2. Это число меньше либо равняется единице.
И вот с такой, с такой вероятностью все равно перехожу,
с такой вероятностью перехожу, перехожу.
Еще раз, я гарантированно перехожу, если мне туда,
как бы сказать, если там лучше, оно и нормально,
там лучше, я туда сразу пошел. Но если там хуже, я как бы,
так сказать, исследователь, мне интересно, а что будет дальше?
Я все равно туда перейду, но с какой-то вероятностью.
И эта вероятность тем меньше, чем там хуже. Логично, да?
Но пока это количественно я не объяснил.
Результат. Если я буду так блуждать, то по прошествии
некоторого времени я действительно выйду вот на такое
стационарное распределение. Это Марковская цепь.
Это Марковская цепь для вот такого графа.
Естественно, я блуждаю на таком графе, выйду на
стационарное распределение, которое будет даваться,
я сейчас это буду обосновывать, будет даваться таким распределением.
Но это еще не все. Я не просто выйду на это распределение,
а я как бы по факту, я же один человечек сейчас блуждаю,
я в итоге окажусь в каком-то состоянии.
Но дальше возникает вопрос, о каком состоянии я окажусь?
Ведь я буду распределен согласно этой мере.
В разных состояниях я окажусь разной вероятностью, но
поскольку у меня есть наиболее вероятное состояние,
и вокруг него концентрируется мера, это и есть предельная теорема.
Только здесь это центральная предельная теорема для Марковских процессов.
И там все будет замедлено в альфа раз, тот самый фактор.
То есть придется ждать в альфа раз дольше, в один альфа раз дольше,
в отличие от независимых испытаний. Но Марковская цепь тоже хороша тем,
что она как бы в каком-то смысле независима, только на масштабе 1 альфа.
И вот придется здесь подождать чуть подольше, но эффект тот же самый.
В конечном итоге, что вот объем текста позволит сконцентрироваться
вот так же, как здесь 1 на n, только здесь еще будет фактор стоять,
который связан с марковостью процесса. Здесь-то все независимо.
Ну и замечательно. То есть я не просто, еще раз, я не просто выйду на состояние,
я еще окажусь в наиболее вероятном состоянии, но мне ровно это и надо.
То есть, короче говоря, если я буду блуждать по этому графу,
и в какой-то момент достаточно большой, я скажу, все, хватит.
Я, естественно, могу находиться не в оптимальном состоянии.
Звучит как-то немножко странно, но вы поняли.
Так вот, я буду находиться не в оптимальном состоянии, но рядом с оптимальным.
И это получается, процедура дает возможность найти максимум,
вообще говоря, просто занимаясь тем, что я блуждаю по графу,
только за счет того, что я подобрал динамику переходов так,
что стационарная мера такая, какая я хотел.
И возникает естественный вопрос. Это магия какая-то.
Я угадал такую простую схему, которая дала мне возможность
понаперед заданной стационарной мере найти матрицы переходных вероятностей,
за счет которых реально можно организовать блуждание,
не думая о том, сколько у нас там состояние, не думая о 30-40 факториал.
Вот это меня не интересовало. Мне неважно, сколько всего вершин.
Большинство из этих вершин абсолютно дурацкие.
Я никогда там не окажусь. Я уже стартовал с нормальной позиции за счет частотного анализа,
а потом еще и каждый раз в каком-то смысле улучшаю состояние,
но если не улучшаю, то с небольшой вероятностью в целом стараюсь
только на повышение ставок играть.
И вот это блуждание так организовано, оно по прошествии какого-то количества шагов
выйдет вот сюда и получится классно.
Возникает вопрос, вообще как все это быстро происходит?
Вот проверим интуицию.
Первый вопрос такой, вот насчет карт.
Вот у меня колоды из 52 карт.
Какие есть способы тасования колодокарт, чтобы все карты были,
чтобы все 52 факториал способов перестановки были равновероятны?
52 это много? 52 факториал это много?
Это очень много.
Получается, что если у нас пространство состояния марковской цепи 52 факториал,
то кажется, что ждать надо очень долго.
Интуиция такая, чем больше граф, тем, соответственно,
дольше надо ждать время выхода на вот...
Да, пожалуйста.
Ну вот конкретно...
Подождите, подождите.
Ну я же это использовал в том, что мы переходим именно из f2 в f4.
Я обосную это, то есть не торопитесь, просто это все будет.
Я это обосную, то есть мы поймем, что это общий рецепт.
Что?
Мне бы хотелось это рассказать не как частный случай, а как общая схема.
Это будет следствием некой общей схемы,
которая называется алгоритм метрополиса Хастингса.
Или просто метрополиса, но это будет чуть позже.
Да, пожалуйста.
Смотрите, здесь как бы последовательность событий.
Сначала я разыгрываю соседа.
Это уже 1 на m, то есть вероятность того, что будет выбран конкретный сосед,
это вероятность 1 на число соседей.
Стало быть, я не соединиться и перехожу сюда,
с вероятностью 1 на m при условии, что f4 больше, чем f2.
p от f4 больше, чем p от f2.
Мы можем для любых пар вершин сравнить и заранее все это прописать, если хочется.
А с другой стороны, если p от f4 меньше, чем p от f2,
то вот это 1 на m, оно в свою очередь шкалируется на фактор p от f4 на p от f2.
Поскольку p от f заданная функция в каждой вершинке,
то при желании мы можем весь этот граф записать явно.
Но я имею в виду не граф, а переходные вероятности.
В этом нет проблемы.
Просто нам это не нужно, нам это нужно по факту здесь и сейчас.
Поэтому нам разумно это каждый раз делать, как бы с нуля,
а не хранить это где-то в памяти.
Я не знаю, ответил на вопрос или нет.
Да, пожалуйста.
Нет, нет, нет.
Я имел в виду именно, что f, отвечающая максимальному вот этому значению,
оно будет близко к f, которое оптимально.
Найденное f будет близко к оптимальному.
То есть дело в том, что...
Я понимаю, о чем вы говорите.
Мы решаем задачу оптимизации.
И как бы естественно, если задача вырожденная...
Например, попробуйте на питоне градиентным методом отминемизировать х в тысячной степени.
Вы не найдете точку ноль.
Градиентный метод становится уже где-нибудь на 0,9.
Почему? Потому что вырожденная функция очень сильно становится на отрезке минус 1,1.
И вроде как понятно, что минимум в нуле.
Но как бы заранее магическим образом не знать, что это за функция,
а просто использовать градиентный оракул, то градиентный оракул видит ноль.
И вот вы на огромном поле должны найти самую низкую, ну как бы сказать, ямку в поле.
Но как вы эту ямку найдете в низинку?
Если поле практически плоскость, а смотреть вы только под ноги, как я лекции читаю.
То есть вы вот так вот все делаете.
И вам надо найти точку нынешнюю в этой, так сказать...
Как вы это сделаете?
Надо все поле обойти.
Ну хорошо, поскольку функция выпукла, вы будете идти по антиградиенту.
Но это тоже очень долго.
И не в одномерном случае там непонятно, насколько это может.
Ну короче так, сколько угодно долго может быть.
Поэтому о сходимости по аргументу речь идти не может.
Но дело в том, что специфика задач, которые возникают при оценке максимума правдоподобия,
она заключается в том, что так называемая матрица Фишера, то есть матрица вторых производных,
которая характеризует вот этот функционал с точки зрения логарифма.
То есть логарифма от этого функционала.
То есть гисиан логарифма целевой функции.
В минимуме, что естественно, или в максимуме.
У нас как бы этот гисиан положить на определенный.
То есть у нас как бы естественно должно быть, если нет выраженности в параметрах,
что он отделим от нуля, это дает сильную выпуклость, это дает сходимость по аргументу.
Но в каком-то смысле наименьшая константа сильной выпуклости,
то есть там матрица обращается, и это будет входить вот в эти оценки.
То есть та проблема, о которой вы говорите, она сидит вот здесь.
То есть в принципе вот здесь могут быть какие-то тонкости,
связанные с тем, что эта константа может быть достаточно большая.
Но все равно это сходимость по аргументу.
Это сходимость по аргументу, равно как и там сходимость по аргументу.
Так, еще раз, расстояние перестановками, и что?
Нет, ну понятно, но у метриках эминга используется стандартная метрика.
То есть если я понимаю вопрос, то рядом это значит, что из там сколько позиций,
из большого количества позиций, вот у вас будет отличие в одной-двух позициях.
То есть это и наблюдалось на практике.
То есть текст был правильно угадан за исключением двух-трех букв.
То есть вы можете посмотреть статью Диакониса.
То есть действительно там из сорока символов неугадно было там меньше пяти.
Вот об этом идет речь, если речь о практике.
Понятно, что на таком категорном языке говорить о epsilon, delta невозможно,
потому что у нас и правда понятие окрестности.
То есть надо тогда симпатически еще делать по числу символов, но это уже проблема.
То есть да, я понимаю, о чем вы говорите, но...
Еще раз, значит конкретно.
Еще раз, давайте я все-таки закончу мысль с картами, и тогда это немножко прояснит ситуацию.
Пример с картами, чем он примечательен? Я же так и не договорил мысли.
Она простая, что для того, чтобы более-менее перемешать колоду с 52 карт тасованием,
снять верхнюю карту и засунуть ее случайно равновероятно в колоду,
то таких действий достаточно где-то 200-300.
И с колодой 52 карты вы ее перемешаете с очень хорошей точностью.
Будет равновероятно все варианты. 200-300 раз.
А если вы делаете так, снимаете, привязите на половинку и вот так...
Это 8 раз достаточно.
Удивительно, да, то есть это очень быстро.
И надо сказать, что вообще говоря, вот эти вот mixing-таймы, время выхода марковской цепи на час,
это время, характерно, что-то похожее на диаметр графа.
То есть сколько надо сделать итераций, чтобы из одной точки дойти до самой плохой другой точки.
То есть вот у вас есть город, и вот это вот время перемешивания, оно, естественно, не может быть меньше,
чем диаметр графа, это логично, правильно?
Но с другой стороны, для многих раздумных сетей это просто приблизительно с точностью до логарифма равно диаметру.
Ну, как бы в том-то и дело, что да, по диаметру мы, чтобы идти, это надо идти, прям узнать, куда идти.
Но как часто бывает, что с точностью до логарифма это одно и то же.
Ну да, оно и понятно.
А что такое диаметр графа?
Ну просто за счет транспозиции вы из одной перестановки можете получить другую перестановку.
Значит вам надо сделать таких перестановок приблизительно столько, сколько символов.
Ну и по факту чуть больше, конечно, было итерации, но это не очень аккуратно.
Я могу привести пример графа, например, с пинсне.
Вот возьмите здесь какая-то клика, мост и клика.
Ну и такой пинсне очки получились.
Вот если вот такое сделаете, ничего хорошего не будет, потому что пока вы, так сказать, здесь будете перемешиваться,
здесь откажетесь, потом сюда скакнете, это может уйти.
Можно так подобрать вероятности, что вероятность сюда попасть экспоненциально мала по времени, по числу вершин.
То есть вы просто здесь будете гулять.
Если вероятность здесь оказаться маленькая, вероятно, что вы здесь окажетесь, а потом еще сюда скакнете,
ну вот вы сами себя загнали в такую ситуацию.
Это специально я придумал.
А на самом деле, если такая типичная ситуация, то диаметр и есть хорошая оценка того, почему тут все работает.
Понимаете, это такая интуиция, которая реально на самом деле в большинстве жизненных ситуаций помогает.
А вот вместо того, чтобы абстрактно себе мыслить, всякие айфы, еще вот, пожалуйста, диаметр, все сразу понятно становится.
Ну мне, во всяком случае.
Ну хорошо, давайте все-таки еще раз.
Есть какие-то вопросы по тому, почему так все происходит, коллеги, не стесняйтесь.
Мы никуда не торопимся, и я могу меньше просто...
Все, переходим тогда к следующему вопросу.
Как подобрать матрицу переходных вероятностей так, чтобы понаперед заданной мере p от f, давайте абстрактно.
Я хочу понаперед заданной p от f подобрать переходной вероятности так в графе, который задан, чтобы у меня стационарное распределение было p от f.
И тогда я пишу общую процедуру, которая совершенно аналогичным образом вы можете ее использовать.
Если я не буду привязываться конкретно к этой специфике, я буду привязываться только к абстрактной формуле p от f.
Давайте это сделаем.
Это распределение на вершинах, совершенно верно.
Еще раз, p от f это вероятность того, что марковский процесс, который вот связан с этим графом,
и с какими-то переходными вероятностями будет в симптотике находиться ту-ту-ту-ту согласно вот этому распределению вероятости.
Как подобрать эти переходные вероятности?
То есть по сути вопрос формулируется так.
Если задана p со звездой, если задана структура матрицы p, в которой не нулевые элементы стоят там, где стоят, то есть у нас вот такая структура в таком виде записана,
то, соответственно, как заполнить пустоты?
Желательно как-то конструктивно и просто, чтобы это было верно.
То есть все, что мне дано, это p со звездой и, соответственно, есть места, куда надо что-то вставить.
Ну, естественно, по правилам того, чтобы это была матрица переходных вероятностей.
Ну, здесь работает...
Нет, ну, точки это там, где больше нуля.
Это любая вершина, которая здесь нарисована, потому что я везде перехожу, безусловно.
Если я что-то рисую, это общий правило вообще в теории марксских цепей.
Если нарисована стрелка, значит, перехода по ней больше нуля.
Если она ноль, то она не рисуется.
И от f4 вы не поверите, это вот что такое. f4, f4, f4, f4, f4.
Я просто четверку дорисовал. Вот что такое p.
Вот и все. f4 – это конкретная перестановка, и я ее конкретную здесь и пишу.
Вообще ничего хитрого.
В смысле, вот правда, то есть не ищите подвоха.
То есть вы понимаете, что это какая-то интересная математика, но схема максимально простая.
То есть подвоха здесь нет.
Смотрите, я сделал эту затравочную матрицу просто для некой общности.
Вообще я мог ее взять как-то, чтобы это была стахастическая матрица.
Я еще могу вести p00i, это оставаться в этом состоянии.
То есть –p0i житое сумма пожи.
Это, повторюсь, затравочная матрица.
Она мне, в общем, я могу ее по-разному выбирать.
Нет, это какой-то конкретный способ.
Можно другим способом просто затравочная матрица,
которая, в моем случае она, на самом деле, поскольку у меня симметричная картинка,
то в моем случае эта матрица можно ее брать один на число соседей.
Она будет одинакова по и житое.
Но во всех вершинках она будет одинакова.
От и жи она зависеть не будет, но это в моем случае.
Я повторю, что это в моем случае.
А в общем случае она может как-то более хитро выбираться.
Я сразу хотел бы в общем случае вам рассказывать.
Идем дальше.
А дальше я хочу подобрать такие b и житое.
То b и житое на p.
Если я возьму матрицу переходных вероятностей p и житое,
вот с такой вот, так определенную,
то хочу, чтобы у меня действительно была вот эта мера инварианта.
Я сейчас, значит, немножко подсмотрю, значит,
что для этого надо, значит, хорошо.
Значит, я хочу подобрать b и житое так, чтобы вот эта мера p была стационарной.
Какое достаточное условие я могу выписать, чтобы прямо сходу сказать,
что решение вот этого уравнения p со звездой транспонированной равняется p со звездой транспонированной p,
чтобы это действительно, вот p, я сюда пишу, чтобы это было верно.
Какое достаточное условие?
Оказывается, что по физике процессы очень простые условия.
Если у меня p и житое на p, соответственно,
если у меня п и житое равняется p и житое на p, то у меня как бы будет баланс.
То есть я гарантированно имею так называемые условия детального баланса,
и вот это будет выполняться автоматически.
Как это я и не знаю.
Как это я и не знаю, если это то, с чем мы пляшем.
То есть мне не обязательно пытаться подобрать какое-то условие такое,
что оно настолько общее, что прям это выполняется.
Мне достаточно будет найти такое p и житое, чтобы выполнялось вот это условие, естественно, для всех и житое.
Почему этого достаточно?
Ну, по физике просто, если у меня...
Вообще, что такое как бы стационарное распределение?
Значит, что человечек, находясь в состоянии И, в стационаре,
вероятность того, что он находясь в состоянии И, перейдет в состояние жи,
на следующем шаге, это есть вот это вот.
То есть как бы даже не так.
Вероятность того, что он именно находится в состоянии Я, потом оказывается в жи, равняется вот этому.
Но то же самое в обратную сторону.
То есть если вы как бы предположите, что пиита это доля людей, которые находятся в состоянии И,
а это можно так вот с человечками также интерпретировать,
а p и жи в состоянии жи это как бы доля людей,
то доля людей, которые перейдут из состояния И в состояние жи, будет вот такая.
То есть сколько людей в единицу времени, поток людей из И в жи.
Это вот такой поток.
Но ему будет рай встречный поток из жи в И.
То есть вы не просто как бы приравниваете поток, сколько уходит во все состояния,
поток у сколько входит.
Это как бы в общем случае.
Я сейчас говорю, что у меня поток, уходящий в конкретную вершину, в любую другую,
равен потоку, приходящему из нее в обратную сторону.
Это частный случай равновесия.
То есть это как бы не просто какая-то абстрактная такая вот...
Я сузил класс, я сузил класс матриц на то, чтобы вот еще дополнительное условие детального баланса
выполнялось, это вот Колмогоров.
Условие детального баланса.
В языке, конечно, оно хорошо известно, детального баланса.
Нет, это не критерий, потому что существует много примеров, когда у вас вот это верно,
а это неверно. Возьмите практически любой граф в системе PageRank, и это будет не так.
Просто случайно нарисуйте граф, то есть это скорее исключение, чем правило.
В типичной ситуации это не так.
Но давайте теперь, как бы раз я сузил класс, раз я конструктивно описал под класс того, что мне надо,
давайте попробуем подобрать B и JT так, чтобы это было верно.
Что для этого надо?
Для этого надо, чтобы P и JT на P и JT равнялось, ну, соответственно, чему?
P значит A и JT на P и JT, правильно?
Но у меня P и JT с этой затравочной матрицей, P и 0 и JT, поэтому я могу сразу говорить про B.
B и JT на, соответственно, B и JT, ну, соответственно, равняется P и JT на P и JT.
И мне надо подобрать набор вот этих вот функций B и JT, B и JT, так что это верно.
Давайте в этой связи обозначим эту величину Z.
И, соответственно, будем говорить, что B и JT, это есть некоторая функция F от Z.
Ну, то есть это есть некая функция от P и JT на P и JT.
Каким свойством должна эта функция удовлетворять?
Я вообще ничего хитрого не делаю.
То есть, на самом деле, это элементарная математика, я поставил себе цель.
Давайте подберем такую, такие матрицы переходных вероятности, чтобы вот было вот это выполнялось.
Как я это делаю? Дело десятое, но просто я максимально упрощаю себе жизнь.
То есть стараюсь упростить сначала это условие, потом вот как-то, ну, сохраняя некую общность все-таки за счет выбора матрицы P.
Но я сразу сказал, что она симметричная, она сокращается в этих выкладках.
Поэтому вся как бы содержательная часть, она вот здесь.
И теперь мне надо понять, какая функция F от Z, чтобы это было верно.
Но здесь сразу получается без вариантов, что если у меня должно выполняться вот это соотношение, то у меня должно быть так.
F от Z делить на F1 на Z, и это должно равняться Z.
Вот так вот.
Ну, потому что, вот, я, смотрите, определил B и JT как F от Z.
Ну, значит, тогда B JT это P JT на P JT, а тогда будет P JT на P JT.
Вот, 1 на Z.
Вот, то есть, я надеюсь, это понятно, да, что я...
Вот, а теперь давайте найдем какие-то конкретные примеры функций, которые это удовлетворяют.
Жалко, я стер, это то, что не надо было стирать.
Ну, например, F от Z равняется
минимум из Z на 1.
Сейчас я проверю, это будет выполняться.
Да, это как раз Хастингс метрополис.
Вот это есть функция Хастингс метрополис.
Именно она использовалась метрополис нами в этом алгоритме.
Ну, а если я возьму F от Z, значит, что там у нас еще какая-то функция?
Значит, Z...
Корень тоже, наверное, подходит.
Корень, подождите, корень...
С точки зрения практики, то есть, вот наиболее эффективна вот эта функция.
Она потому что в каком-то смысле, в каком-то смысле...
Она потому что в каком-то смысле мажорирует все остальные,
и получается как бы наибольшая движуха.
То есть, с этой функцией вы как бы наиболее активны,
а значит, миксинг тайм наиболее, ну, наименьше.
Поэтому, в общем, можно доказать, что все функции, которые вы будете подбирать, их много.
Но они, во всяком случае, в теории уступают вот этой.
Вот, и поэтому, как бы, в общем-то, выбирают в основном вот эту функцию.
Ну, и теперь получается, что, что надо делать?
Ну, надо сначала сделать пристрелочно, как бы, выбрать, ну, вообще, соседа в каком-то смысле.
Когда вы выбрали соседа, ну, вот тут, в нашем случае, вы дальше уже принимаете решение.
Если этот сосед, ну, лучше вас, по тому критерию, что вероятность у него лучше,
вы прям туда и переходите, но переходите, как бы, не с вероятностью единица,
не с вероятностью единицы, а вот с этой вероятностью, которая связана с тем,
ну, сколько там в целом соседей.
Ну, а если он хуже вас, то вы все равно берете отношение,
ну, z это значит отношение вероятностей, сравниваете и тоже переходите с какой-то вероятностью.
Обратите внимание, что мы, мы, как бы, сделали некий общий трюк,
который, вообще говоря, работает всегда в разных контекстах,
и вы можете так решать совершенно разные задачи, которые, ну, вот, на первый взгляд,
вообще никак не, вот, еще раз, вообще, при чем здесь все, что я рассказывал?
Ну, кроме того, что я и там рисовал человечков, и здесь я рисовал человечков,
больше прямой аналогии нет, если так чисто визуально,
потому что там были человечки, они блуждали по графу, а здесь человечки, это реально элементы шифра.
Ну, на самом деле, больше и такой связи содержательной нет,
а содержательная связь возникает на этапе принципа максимум правдоподобия,
на этапе эргодической теоремы для марковских процессов
и на этапе в каком-то смысле связи принципа максимум правдоподобия с центральной предельной теоремой,
потому что оценка принципа максимум правдоподобия, это оценка, в которой как-то резюмируется,
ну, то есть, это максимум, как бы, логарифма некой функции,
логарифм этой функции, это же функция вероятность,
эта вероятность составлена из произведений вероятности того, что каждый элемент выборки как-то выпадет.
Когда возьмете логарифм за счет простоты выборки,
у вас это будет логарифм произведения, потому что независимые исходы,
ну, в случае марковского процесса посложнее,
и у вас получится надо минимизировать сумму,
а за счет того, что это сумма случайных независимых величин,
у вас наблюдается явление концентрации меры,
то есть, это не чудеса, чудес здесь нет,
потому что, еще раз, когда вы максимизируете правдоподобие,
а правдоподобие получается по принципу независимости,
то есть, независимая разыгрывание,
то максимум этой функции то же самое, что максимум логарифма,
а логарифм от произведения равняется сумме логарифмов слагаемых,
и уже эти случайные вот слагаемые независимые,
вы изучаете поведение вот этой суммы,
поэтому и есть концентрация, это же сумма независимых величин,
с марковской цепью посложнее,
там тоже сумма будет, естественно,
а независимая, но дело в том, что,
помните, я говорил, что можно взять частоту,
то же самое там, частота пребывания человечка в той или иной вершинке,
так вот, ровно потому, что частоты,
если брать траекторию одного человека,
как часто он бывал в той или иной вершинке,
вот ровно потому, что это тоже сходится вот к этому распределению,
сходится приблизительно так, как в законе больших чисел,
просто медленнее, то и тут вот такой эффект проявляется,
и вот мы поигрались на больших всяких вот таких эффектах,
на эффектах больших чисел,
энергодичность, то есть время к бесконечности,
ну в нашем случае число итерации этого буждания,
ну и вот то, что в нашем случае много букв,
а там много агентов буждает,
на этом мы дополнительно получили концентрацию,
то есть не просто вышли на стационарную меру,
но так еще и эта стационарная мера вокруг чего-то сконцентрировалась,
теперь их, собственно, объявления,
и они, мне кажется, достаточно важные,
значит, наверняка у многих возникает вопрос,
вообще, что будет происходить дальше,
и вообще, что это такое,
потому что это не формат в некотором смысле,
значит, первая лекция была такая водная,
я на ней постарался, ну грубо говоря, рассказать много чего,
дальше я буду делать немножко по-другому,
потому что мне важно было вас в каком смысле увлечь
и показать спектр, некоторый спектр,
ну вот теперь давайте вспомним,
а что я не рассказал,
ну я, например, вообще ничего толком не рассказывал
про вот эти варианты CPT концентрации,
ну то есть что они асимпатические,
а нам надо не асимпатические,
не рассказывал про, вообще говоря,
что реально в PageRange там надо как бы смотреть,
когда агентов много,
как вообще эффект, что надо не просто
а там мультинамиальную схему брать,
может быть, это кажется мелочью,
но ведь на самом деле,
когда мы переходим к серьезным задачам,
там же все это тоже всплывает,
и хочется какую-то единую математику,
которая полезна много где,
в разных совершенно контекстах,
ну и вот мы это будем, собственно, дальше закреплять,
то есть какие-то не только это,
как вот, значит, там какие-то не разница концентрации помогают жить,
как они помогают жить там, например, в сток оптимизации,
как они помогают там в каких-то матричных вопросах,
как они помогают в моделировании,
я не знаю, макросистем, например,
того же самого, ну там не столько не разница концентрации,
сколько эффекты больших чисел,
но можно и концентрацию,
как, например, неравенство Санова,
такое классическое неравенство,
вот оно помогает вообще и в теории информации,
и одновременно в сток химкинетики,
и вот, например, в изучении всяких там моделей,
я не знаю, взаимодействия там,
животных друг с другом, там людей,
всякие обменные модели в статфизике,
и очень много-много чего такого,
будут разные техники рассказаны,
геометрия будет рассказана,
например, в следующий раз я постараюсь вас увлечь
явлением концентрации меры на сфере
применения к безградиентным методам оптимизации,
то есть, вообще говоря, я вам расскажу то,
чем Пуан Каре занимался в конце жизни,
это результаты, связанные там из формулы Гауса,
почему из Гауса формула нормальное распределение так часто возникает,
не только из-за счет CPT, а за счет геометрии,
там будет теория динамических систем,
там будет очень много связей,
которые, я надеюсь, покажут,
что математика едина с одной стороны,
а с другой стороны, вот эта математика,
она сейчас и в Big Data,
именно в машинном обучении еще популярна,
это будет не сразу,
и вот такими москами мы будем потихоньку, значит, идти,
и в частности, то, что я сегодня не успел нормально рассказать,
это про эргодические всякие теоремы,
вот у нас будет приложение финансовой математики,
мы будем, как вот реально оценивают всякие дрифты,
ну не знаю, снос или как это,
за счет эргодичности уже случайных процессов,
как, почему возникает вот эта вот мартингальная вероятность,
то есть мы будем пытаться вскрыть не просто Big Data,
будем пытаться понять, почему в жизни возникают те или иные законы,
какие-то степенные законы,
и вообще, как бы сказать, что является первой причиной?
Стиль, в котором это будет происходить, приблизительно такой,
но я, наверное, буду меньше рассказывать,
но буду больше сосредотачиваться на каких-то проработке деталей,
приблизительно, как было в конце вот здесь.
В частности, я очень надеюсь рассказать метод перевала,
метод стационарной фазы, метод Ла Пласса,
это асимпатические методы, анализы, например,
вот как вывести формулу стирлинга,
я почти уверен, что никто не знает,
хотя это несложно.
Я это сделаю в три строчки с помощью элементарной замены в интеграле.
Эта замена многократно используется,
это отчасти объясняет появление энтропии,
потому что везде, где есть равноправие, там есть факториалы.
Везде, где есть факториалы по формуле стирлинга,
есть N делить на E в степени N.
Логарифм от этого – это энтропия.
Вот почему энтропия так привилегирована,
почему она в жизни так часто возникает.
Мы будем это изучать, она будет повсеместно встречаться,
мы будем встречать разные законы расстреливания,
не только нормальные,
и будем пытаться понять, почему одно и то же многократно встречается.
И вообще, насколько важна первозданная случайность,
может быть, достаточно случайность получать как результат динамической системы.
В общем, я очень надеюсь, что курс будет интересный.
И, собственно, в какой-то момент, где-то через месяц,
мы будем вам предлагать реально какие-то сюжеты,
то вы можете взять, например, то, что я рассказываю,
взять парочку статей и попробовать получить что-то новое.
Это может быть связано с программированием,
может быть, с математикой, что вам ближе.
И приблизительно это можно, например, объяснить так.
То есть вам реально дадим какой-нибудь такой жизненный пример.
Например, тор, на нем животные.
Ну да, не очень жизненный.
Просто нам важно, чтобы лес был...
Ну, нам важно, чтобы животные не уходили из леса.
Его, значит, модель хищник-жертва.
Представьте себе, что они реально блуждают по решетке,
если хищник встречает жертву, он ее кушает.
И вы то, что я вам буду доказывать, вы можете это проверить.
Вы можете проверить, какие скелинги должны быть.
Вы можете проверить mixing time.
Это все довольно просто делается,
но вы реально можете скрывать какие-то законы.
То же самое можно сделать в перестановках.
Ну, и это все как бы подкрепляется математикой.
И мы будем... Вы можете сами вот эти макросистемы
или какие-то большие статистики там,
что вам будет ближе.
И где-то к середине октября, я думаю, я уже окончательно
сформирую набор задач.
Более того, скорее всего, Максим Рахуба подготовит
даже Юпитер ноутбук-презентации.
Возможно, какие-то и вам уже по готовому коду,
по там уже реализованному какому-то методу,
связанному с кем-то может матричным,
вы можете просто что-то доделать и получить новый результат.
Я со своей стороны смогу такое представить
по всяким задачам вокруг там многорувких бандитов.
Это тоже будет всякие вот безградиентные методы.
И сток... Ну, то есть такая современная сток-оптимизация.
И там будет много сюжетов, где вы просто можете взять
существующую статью, что-то доработать
и получить новый результат.
Все, вы курс дали.
То есть если вы в состоянии разобраться
с новым материалом там в виде двух статей,
понять, как, например, поженить эти статьи,
получить новый результат, доказать что-то,
годится отлично все.
Или вы в состоянии закодить то, что в статьях написано,
проверить, открыть экспериментально
какой-то новый закон.
Я в конце приведу один пример,
о котором мы рассказывали президенту.
Совершенно случайно.
И все, на этом закончу.
Была программа школьников в Сириусе,
там талантливый молодежь, привозят.
И, значит, преподавателям что-то рассказывают.
Это я первый раз поехал в 2016 году.
Просто чтобы вы сейчас почувствовали,
насколько все это рядом и какой проект можно сделать.
Ну вот, значит, мы делали проект такой,
что я рассказал школьникам вот этот пейджранк,
а потом сказал, что, ребят, на самом деле,
вот эта матрица переходных вероятностей,
она хитро формируется,
потому что интернет растет по принципу
деньги к деньгам.
Если у вас уже есть какой-то веб-граф
и тут есть какое-то количество вершин,
то, соответственно, новый появившийся сайт,
он будет делать ссылки,
ну, как бы сказать, наиболее активно
на то, что и так цитируется.
И вы видите модели роста.
Тут возникают степенные законы вершин.
Это можно, мы будем это изучать.
Все это будет.
Будут скелинги, теорема курса,
будет очень красиво.
Но это потом.
Это все потом.
Как бы школьникам я это рассказал,
я сказал, что доказать степенной закон
для вершин, как бы, ну, не изи,
но я им буквально это доказал.
И, соответственно, они как бы
классно вообще.
Вот тут макросистема,
тут еще степенной закон.
А дальше-то что?
Степенной закон вершин есть,
а можно ли то же самое сказать про PageRank?
Потому что самое интересное,
это степенной, ну, как бы, это хорошо,
что у этого есть.
Но верно ли, что если отранжировать
компоненты вектора PageRank,
то тоже будет степенной закон?
Открытая задача.
То есть для степеней вершин.
Вот в такой модели Buckley-Откуса
Preferential Attachment это можно доказать.
Ну, это вот Андрей Михайлович любит Рогородский.
А для PageRank нет.
Я сказал, ребят, ну, чего, собственно,
давайте, значит, проверим.
То есть они реально брали какие-то фрагменты
интернета, скачивали,
они доказывали, они проверяли
гипотезу, что если здесь есть
степенной закон роста,
а он есть, если модель роста графа
Preferential Attachment, то она будет и здесь.
То есть матрица P порождена,
она не любая.
Она порождена определенным законом роста.
То есть у нас есть динамика, которая порождает
матрицу P. Матрица P в итоге
сформировалась. И она сформировалась
тоже стахастическим образом, но
согласно неким правилам. Деньги к деньгам.
И, соответственно, они потом
должны были исследовать численно,
насколько вот вектор PageRank
тоже имеет степенной закон, вообще говоря,
с другим параметром. Что значит степенной закон?
Отсортируйте компоненты вектора
PageRank и нарисуйте их. То есть самая большая,
следующая. И нарисуйте
в масштабе lock-lock. В масштабе lock-lock
если получается прямая, то наклон
прямой отвечает... Вот они это проверяли
на большом количестве
для большого,
для того, что известно,
то, что доказано, это для индекса вершины.
То есть вот для такого. А это не было
известно. Вот они это проверили, установили
закон и даже получили формулу аналитическую,
потому что можно здесь
менять параметры. И в зависимости
от этих параметров меняются параметры
вот здесь, и можно угадать какой-то закон.
Вот есть что доказывать. Если бы они еще
доказали, было бы совсем круто, но они
тогда, 2016 год, чисто экспериментально
это установили, и вот значит мы так
уже заканчивается смена, сидим,
все это обсуждаем, а я их там вот
отучил деньги к деньгам. Ну значит приходит
сначала, боюсь соврать вам песков,
потом еще кто-то. Это потихоньку
потом входит президент. Почему занимаетесь?
А мы как бы вроде не планировали
ничего такого, ну просто
было обычное. Ну его школьники начали
ему рассказывать деньги к деньгам,
все такое.
Да, да, да, да, деньги к деньгам, да.
Вот так там,
все такие кивают.
Ну довольно
интересно, ну и реально,
насколько проект понравился, и
потом вышла статья,
в общем, вы можете найти ее
в интернете, там, PageRank,
и посмотрите, что там написано, как раз это
вот такой типичный пример проекта.
Можете по моей фамилии и, соответственно,
кстати, в основном эти школьники пошли
на ФКН. Моей руки тут
нет. То есть, по-моему, половина из них
сейчас на ФКН не учится. Я-то в основном
на физ.тех агитирую, но вот так получились.
Вот.
Ну кого-то из них даже вот Варвара знает,
это Лаунов,
по-моему, и Сергей Ким.
Может кого-то вы знаете. Ладно,
сейчас не так важно.
Значит,
собственно, это все, вот это просто пример,
что такого будет много,
и отчасти вы можете управлять процессом,
это удобно. То есть, в каком-то смысле
это не просто фильм, а это как бы
вы можете кликать пультом и говорить
замедлитесь, ускорьтесь, а да расскажите
вот это, и я могу это корректировать.
Пока, во всяком случае, время есть,
и, в общем, жду обратную связь.
Постараюсь со временем
рассказывать более строго,
потому что сегодня была такая вводная
лекция.
Так, спасибо.

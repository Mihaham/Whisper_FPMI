Отлично. Тогда мы готовы начать. Грустно, что вас мало, конечно. И смотрите, что я прочитаю в этом
курсе оставшиеся лекции, я расскажу вам про разные системы, про Кавку, про Зукипер,
про Кассандру, про Спанер, не знаю, успеем мы или нет поговорить, но сейчас будет отлично.
Но сегодня такая вводная лекция, и она предназначена для широкого круга лиц, в том числе
людей, которые не слушают мой спецкурс по определенным системам, там где я довольно подробно про все
рассказываю сегодня, по замыслу такая обзорная лекция про то, как вообще можно думать про
распределенные системы и что о них нужно знать. В смысле, такой теоретико-архитектурный
точки зрения совмещенный, если вы ими пользуетесь, но все же вы хотите понимать примерно, что под
капотом происходит, с какими гарантиями все работает, какими свойствами системы могут или
не могут обладать и как правильно про них говорить. Так вот, в какой степени подробности
мне об этом рассказывают, потому что я не совсем понимаю аудиторию свою сегодня.
Вопрос непонятен, да? Я хочу узнать, насколько... Слушали ли вы первые лекции моего спецкурса или нет?
Да. Вот. По замыслу поет лекция для тех, кто не слушал. Но если вы слушали, то... Я все равно что-то
повторю, потому что, не знаю, если запись будет, ее будут смотреть, то будут смотреть не только
другие люди, но если у вас есть в принципе желание поговорить о чем-то более детально,
я вам не рассказал, или у вас какие-то вопросы с тех пор остались, то будет разумно.
Чтобы вы побольше их задавали, чтобы я получал больше обратной связи и понимал,
в какие вещи можно было бы углубиться и что-то подробнее проговорить. Вообще сегодня я хотел бы
рассказать вам в итоге про, совсем коротко, про результаты, про основные теоретические результаты,
про ФЛП, про каптиорема, про то, какими словами люди говорят про определенные системы. Но для
начала нужно поговорить, напомнить или рассказать, как вам больше нравится, про то, что мы вообще
понимаем под распределенной системой и что нужно любому человеку, который их администрирует и их
использует и из них строит инфраструктуру, что про все это нужно знать. Во-первых, нужно конечно
же понимать, даже если вы работаете с облаками, даже если вы не ставите физические машины,
даже если вы не занимаетесь заказом оборудования, все равно нужно понимать, из каких физических
компонентов все в итоге будет выстроено, пусть даже облачного провайдера. Если мы говорим про
теорию, и сегодня я хочу рассказать немного про теорию, то теория всегда должна быть согласована
с практикой, и поэтому очень важно в самом начале, прежде чем говорить про Зукиппер или про Кавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкав
к теме, из каких блоков она состоит. Ну, она состоит, понятно, из компьютеров, которые соединены проводами, которым доставляется сообщение, но, как правило, важно, как именно эти провода и эти компьютеры организованы в физическом мире.
Ну, вот вы понимаете, что, конечно же, эти компьютеры устанавливаются где-то в датацентрах, и в датацентрах они стоят, во-первых, не по одному, конечно, они стоят в стойках.
Эти стойки, ну, это просто такой базовый элемент инфраструктуры в датацентрах, это способ организации машин. Ну, вот, видите, здесь много серверов, они вложены так рядами в такие вот большие шкафы, и между ними тянется очень много проводов, которые соединяют эти компьютеры с коммутатором, который, в свою очередь, соединяет эти компьютеры в одну большую сеть.
Казалось бы, это вещь, которую вы не наблюдаете, когда вы работаете со определенными системами, и даже когда вы, не знаю, изучаете какую-то систему, вам правило говорят, что есть дата нода и нейм нода, и вот какая разница, как они стоят в датацентрах.
Вот это важно, потому что с понятиями, там, стоек, датацентров, регионов, кластеров, связано понятие фейлордомейна. Фейлордомейн – это компонент физической инфраструктуры, который может полностью выйти из строя из-за какого-то одного правила аппаратного сбоя.
Вот вы думаете, что вы обеспечиваете отказоустойчивость, например, в HDFS, храняя реплики одних и тех же данных, одних и тех же фрагментов файлов на разных машинах.
Но вот понятно, что этого недостаточно, в общем случае, потому что эти реплики могут находиться в одной серверной стойке, и да, наверное, отказ дисков, особенно крутящихся дисков, можно считать некрелированным сбоем, они происходят независимо.
Но если, скажем, у вас есть такой шкаф, где вам стоят две реплики, и вы на них положили копии данных, а выйдет из строя вот этот коммутатор, то вы разом лишитесь нескольких ваших реплик, возникнет так называемый крелированный отказ, и пользователь, возможно, может лишиться своих данных, по крайней мере, на некоторое время.
Вот пример того, как знания про физический мир помогают строить более надежные, более доступные распределенные системы, а именно, любая распределенная система, с которой вы работали, и наверняка вы против DFS это говорили, обеспечивает вам гарантию recoverance.
То есть, сама система должна знать про то, что не просто компьютеры соединены приводами, а еще они организованы в стойке, если система данные реплицирует, она должна хранить копии в машинах из разных стоек.
Ну, тут можно даже, смотрите как, можно даже делать еще чуть-чуть сложнее, не просто размазывать по стойкам, но еще и заботиться о доступности на уровне самой стойки, на аппаратном уровне, а именно, ставить в стойку там не один коммутатор, который связывает эту машину с другим, эту стойку с другими, а сразу несколько, просто потому что коммутаторы, это, как и любые компанентные инфраструктуры, вещь не отказывает, стойчивая.
Из-за какого-то аппаратного сбоя или даже бага мы этот компонент можем потерять, и будет замечательно, если мы сможем пережить отказ даже одного коммутатора в стойке, если мы там разместим их пар.
Желательно независимых производителей, потому что, понятно, у них могут быть разные сценарии отказов и там какие-то разные баги внутри.
Ну а дальше эти стойки в кластере, распределенные системы связываются, эти стойки в этот центре связываются в понятии кластера, ну и когда-то можно было бы думать о распределенной системе, в общем случае, примерно вот так вот.
У нас есть отдельные машины, они укладываются в эти стойки, реки, их там десятки, и все это связывается одним свечом, одним коммутатором в один большой кластер.
Вот люди так долгое время и жили, но столкнулись с той проблемой, что вот такой дизайн, он не масштабируется.
Масштабирование – это еще одна распределенная система, данные наши не помещаются ни в одну, ни в 80, ни в 100, может быть даже ни в тысячу машин, потому что мы храним пятабайты или там сейчас тот же масштаб экзобайты.
И очень важно, чтобы сами системы могли расти не только алгоритмически, чтобы в смысле в них не было препятствий к масштабированию.
Вот если вы изучали HDFS, то вы знаете, что эта система с некоторыми поправками не может горизонтально масштабироваться, потому что объем методанных ограничен емкостью одной машины.
То есть есть препятствия такие архитектурные, которые возникают на уровне дизайна распределенных систем, а есть препятствия архитектурные, которые возникают уровнем ниже, на уровне дизайна кластеров и центров.
И вот такой дизайн, он когда-то годился, потому что, во-первых, данных было еще относительно немного, а во-вторых, люди жили как-то изолированно.
В смысле, даже если у вас какая-то большая компания и выходит много-много данных, то этим данным хранятся часто, ну так скажем, децентрализовано.
В смысле, что у вас много проектов, много сервисов, и каждый сервис наказывает свои машины, разворачивает свои кластера относительно небольшие.
Ну и вот нигде в одном месте не возникает необходимости построить какой-то гигантский кластер с большой пропускной способностью любого разреза.
Ну вот так было некоторое время, и скажем, так было в Яндексе, если вам интересно, но кажется, где-то с десятых годов, может быть середины, возник такой тренд, что очень неэффективно встроить большие компании, в которых очень много разрозненных сервисов, которые используют разрозненные отдельные кластера для деплоя своих систем.
Но ровно потому же, почему неэффективно, скажем, каждому условному бизнесу, каждому стартапу строить свои кластеры и покупать свои физические машины.
Гораздо разумнее переиспользовать что-то общее, ну как бы декомпозировать, разделить бизнес-логику какой-то, ну, ваши сервисы от инфраструктуры хранения данных и от инфраструктуры сетевой аппаратной.
И люди пришли к дизайну, где они строят очень большие кластера, и машины в этих кластерах просто разделяются между разными сервисами, между разными условными пользователями.
У вас может быть огромный кластер, и часть машин его занимают машины, которые обслуживают поисковую систему, а часть машин обслуживают операции в Мэпридьюсе.
Ну и там в зависимости от нагрузки в данный момент на поиск или на вычисления, задачи машины можно перебалансировать, между двумя пользователями.
Ну вот такая вот фундаментальная очень важная идея, и сейчас везде все это так организовано, ну и, конечно же, вы понимаете, что облака ровно так устроены, облачный провайдер, будь там Amazon, Google или Yandex, дает вам большие кластера и позволяют вам разворачивать свои предложения, свои сервисы поверх этих машин, и все это там гибко очень ракистировать.
Ну вот для того, чтобы так делать, в итоге нужно строить вот эти самые большие кластера, и вот просто так взять серверные стойки и объединить их в один большой кластер невозможно, нужно делать что-то сложнее.
Ну вот скажем пример дизайна настоящего центра, где не просто там стойки соединены одним свечом, а где используется довольно сложная конфигурация, которая обеспечивает с одной стороны высокую отказоустойчивость, то есть тут нет никакой сетевой коробки, никакого свеча, который бы на себе завязывал всю коммуникацию между любыми машинами.
А с другой стороны, у такого дизайна очень высокая выпускная способность.
Короче говоря, между любой парой машин довольно много маршрутов.
И устроено такие центры, например, на следующем образом. Мы берем много-много стоек, ну вот в данном случае 48, и связываем их между собой вот четырьмя коммутаторами.
То есть у нас здесь есть возможность четырьмя способами у любых двух машин из разных стоек четырьмя способами друг с другом поговорить.
И вот такой модуль, из которого мы строим дальше большой кластер.
Вот если нам хочется расширить кластер, мы добавляем в него такой модуль, а дальше мы соединяем их перпендикулярными плоскостями.
Вот у нас есть такие вот поды, в каждом из которых 48 стоек, то есть 48 умножить там на, скажем, 50 машин.
И четыре коммутатора, которые связывают эти стойки внутри пода, мы еще перпендикулярными плоскостями ровно такой же вот конструкции связываем между собой.
То есть если мы хотим добраться из этой стойки, мы должны попасть сначала в один из этих коммутаторов, то есть выбрать из этих плоскостей,
и в этих плоскостях выбрать один из этих коммутаторов для того, чтобы перейти в другую плоскость.
Ну вот тут возможно разные на самом деле варианты дизайна, но это такая более-менее стандартная.
И она хороша тем, что здесь не требуется никакого специфичного оборудования, то есть здесь используется вполне себе доступный на рынке коммутатор,
и не требуется изобретать какую-то волшебную коробку, которая позволит объединить очень-очень-очень много машин.
И такую коробку строила бы, я не знаю, какая-нибудь одна компания, и вы бы завязали всю свою инфраструктуру на нее.
Вот нет, такой цели нет. Ровно противоположно тому, чего мы хотим.
Мы берем более-менее производительные, но доступные, наверное, коммутаторы,
и строим из них сеть, которая может расширяться и расширяться более-менее ограниченно.
Опять же, это все нужно понимать, потому что мы работаем с...
Ну, потому что система сейчас работает в общем пуле ресурсов, на общем пуле машин,
и никто сейчас не пытается как-то изолировать свою систему от других.
Еще одна причина так делать — это утилизация.
Вот если у вас есть отдельные кластера, то понятно, что ваша система не может быть нагружена все время.
Как вы находитесь на пике нагрузки, вы должны иметь некоторый запас в случае роста числа пользователей.
Поэтому, как правило, ваша система, ваши аппаратные ресурсы не утилизируются полностью.
Вы не загружаете до конца процессора, вы не загружаете сеть и так далее.
Если вы живете в отдельных кластерах, то получается, что у каждого проекта, у каждого сервиса есть полмашин,
и эти машины используются не до конца.
Вот если мы их складываем в один большой кластер и с помощью какого-то планировщика,
позволяете эти ресурсы большого кула машин разделить между сервисами, то вы можете добиться более высокой утилизации.
То есть вы можете балансировать эти ресурсы между теми,
отдавать их приоритетизировать тем, у кого сейчас нагрузка есть, а тем, кому она не нужна, у них выбирать.
Но если у вас какие-то вопросы есть, то спрашивайте, потому что если вы вдруг это слышали,
то, не знаю, может быть вам какие-то детали интересны или какие-то, ну не знаю, что-нибудь еще, что я не говорю, а мог бы.
Вот, ну и такая конструкция, она помещается в дата-центр, может быть даже не в один,
и на уровне дата-центра нам тоже важна и отказоустойчивость, и масштабируемость.
Вот мы хотим расширяться, поэтому мы строим несколько зданий, разумеется,
но мы хотим отказоустойчивость, поэтому мы должны позаботиться о резервировании тех компонентов, которые могут отказать.
Ну скажем, могут порваться магистральные кайбери, которые соединяют дата-центр с другими дата-центрами,
потому что, как правило, реплики вашей системы, если система является высокодоступной,
должна размещаться в разных отказах, то есть в разных зданиях, может быть даже на разных континентах.
И вот эти здания должны соединять провода в магистральной кайбере,
и для того, чтобы ваша система, ваша инфраструктура была устойчива, не знаю, к отказу кайберю,
в смысле, к какому-то экскаватору, который вы раскопали, вы соединяете ваш дата-центр несколькими кайберями.
И вы, конечно, заботитесь о резервном питании, то есть вы питаетесь от подстанции,
скорее всего, у вас есть в вашем здании еще и дизельные генераторы, которые питают,
если мы спустимся на уровень вниз, дизельные генераторы, которые служат резервным источникам питания.
Ну, не знаю, тут еще на этой картинке витрины мельницы, но это совсем про запас.
Вот так примерно устроен дата-центр, и вот в нем мы собираемся деплоить все наши системы,
будь-то кавказу, кипер, что угодно, ТФС.
И узлы этих систем общаются между собой.
Общаются они, как правило, по стандартным протоколам, по протоколу TCP, скажем.
И вот почему нужно думать и об этом, если вы пишете распределенный код,
потому что нужно понимать, какие гарантии вам эти провода дают.
То есть вот вы отправляете сообщение с одного узла до другого узла.
Вот что с ним может случиться?
Ну, могут случиться очень разные вещи.
Могут случиться, не знаю, у вас может поломаться чехсума,
потому что где-то в проводе перевернулись какие-то битики,
потому что ну просто там не дискретный сигнал передается.
Или у вас соединение может порваться,
ну конечно сложно, почему это может произойти.
Ну а в общем, какие гарантии у вас есть в случае отправки сообщения?
Эти гарантии могут, вот без этих гарантий,
строить какие-то распределенные, без понимания таких гарантий,
строить распределенные протоколы очень сложно.
В теории, как правило, говорят про два типа гарантий.
Говорят про надежные и ненадежные каналы.
Вот ненадежный канал – это канал, в котором вы отправляете сообщения,
и оно может доставиться, а может нет.
Но если вы будете ретраить отправку бесконечно долго,
то вы бесконечно много раз отправите одно и то же сообщение,
то оно гарантированно дойдет бесконечно много раз.
И имея такую абстракцию, теоретическую абсолютно,
можно построить надежный канал, такой, что вы в него отправляете данные,
он гарантированно доставляет сообщение адресата,
причем доставляет ровно один раз.
Такая интуиция, что почему мы считаем, что такой канал можно построить?
Потому что у нас есть уникальный идентификатор,
который можно генерирует на каждой машине.
У нас есть фейерлоз-канал, ненадежный канал,
который гарантирует доставку.
И вот ретраем мы можем добиться семантики at least once,
то есть мы доставим сообщение по крайней мере один раз,
а с помощью уникальных идентификаторов получатель может отфильтровать дубли
и получить семантику at least once.
Ну а дальше в теории алгоритмы строят поверх таких каналов,
что на самом деле не очень практично,
потому что в реальности вы такой канал построить не можете,
потому что вы работаете в физическом мире,
и вы работаете с уже существующими слоями инфраструктуры
и набором абстракций,
и у вас есть TCP-соединения, которые,
с одной стороны, являются ненадежным каналом,
с другой стороны,
беспечить семантику at least once, конечно, не могут,
потому что если вы отправляете сообщение,
то внутри одного TCP-соединения
собственно сам TCP гарантирует, что оно будет доставлено ровно один раз,
какой-то фрейм, который вы в TCP-соединении упаковали,
а с другой стороны, если TCP-соединение рвется,
то никаких гарантий вы тут уже не получаете.
Разумно было бы думать об этом так,
то есть он настоящий гарантий отправки по сети.
Когда вы отправляете сообщение в сеть,
то с другой стороны, либо получатель узнает об этом сообщении,
либо он его получит,
прочитает байты из своего сокета,
либо же на стороне отправителя
случится разрыв соединения,
и отправитель может это как-то обработать.
Но гарантировать 10 exactly once невозможно.
Если не рвется, то вы не понимаете,
получил ли адрес от сообщения или нет.
Вы можете его перетраить,
а дальше вы можете на этом уровне рассчитывать
семантику at least once.
То есть каждое сообщение будет доставлено,
по крайней мере, один раз.
Это уже более разумные допущения,
и на уровне сетевой коммуникации
стоит пользоваться, конечно, ими.
Примерно так организованы вот эти коробки
дата-центра и провода.
Пара слов про TCP.
TCP – это довольно сложная конструкция.
С одной стороны, это такая абстракция надежного провода,
которая гарантирует доставку сообщения
и ровно и без дублей, и без потерь.
А с другой стороны,
это самостоятельная распределенная система.
И когда вы думаете про коммуникацию по TCP,
то вы должны учитывать, что, скажем,
у одной машины, то есть у вас, у клиента
и у севера могут быть разные представления
о том, в каком состоянии сейчас соединение.
Может быть, вы думаете, что соединение у вас открыто,
а север будет думать, что его уже нет,
потому что он взял и резко перезагрузился,
потому что у него пропало питание.
Пока вы не пойдете и не отправите пакет TCP-сегмент
на север, вы не узнаете о том,
что соединение первое на самом деле.
Потому что TCP в проводах не реализован,
во всей промежуточной системе инфраструктуры
никакого TCP нет, там работают только IP
и протоколы более низкого уровня.
Поэтому, в общем, я рекомендую статью прочесть,
если вы ее не знаете, если я еще не рассказывал,
она про то, какие конфигурации могут возникать
просто в пределах одного TCP-соединения,
как клиент и север могут расходиться в своем представлении
о том, как узлы общаются между собой.
Это шутка в зуме.
Он хочет сказать на просто или просто шумит?
Непонятно.
Окей.
Таким образом, система состоит из узлов,
которые объединены в реке, которые объединены сложной,
вот это называется, я не сказал,
коммутационной фабрикой.
Центры бесконечно большие, и в них живут узлы,
которые обмениваются сообщениями с помощью
относительно надежного транспорта.
Ну, а дальше мы собираемся на эти узлы поместить
какие-то наши программы, которые будут заниматься,
которые будут реализовывать ту или иную распределённую систему,
ну, скажем, файловую систему или кивалию хранилища
или базу данных или что угодно.
И прежде чем мы перейдем к разговору о самих системах,
полезно добавить в нашу картинку,
которую мы сейчас так медленно рисуем,
ещё и клиента.
Клиенты – это тоже полноценные участники распределённой системы.
И почему важно говорить про клиентов, хотя, казалось бы,
они просто вызывают какие-то методы.
Во-первых, клиент – это сущность, которая тоже подвержена отказу.
И, разумеется, вы внутри системы должны бороться с отказами,
у вас не должно быть единой точки отказа,
то есть единого компонента, или, не дай бог,
даже отдельной машины точно не должно быть,
отказ, который приводит к тому,
что вся система становится недоступной.
Ну, скажем, вот лидер,
name-node HDFS является точкой отказа.
У нас могут быть тысячи машин, которые хранят данные,
но без дела, который менеджит методанно этой системы,
хранит списки чанков файлов,
вся эта система недоступна для пользователя.
Но вот, помимо того, что в системе,
в дизайне системы не должно быть такого компонента,
на которого все было бы завязано,
и он бы не выдерживал отказанной машины.
Также, вообще говоря, нужно думать и про отказы клиентов.
Вот, скажем, вы пишете сервис блокировок.
Сервис блокировок – это такой стандартный компонент в инфраструктуре.
Вот эти слайды Google довольно старые,
но на самом деле они уже давно сделали что-то разумное.
Вот их архитектура, архитектура кейвалюхранилища
и тут есть какие-то вспомогательные сервисы.
Ну вот, система, которая занимается,
которая планирует узлы программы на кластере,
есть распределенная файловая система,
которая отвечает за хранение данных,
и есть сервис блокировок, который выполняет,
который предназначен для координации узлов распределенной системы.
Но вот эту задачу тоже можно делегировать отдельному компоненту.
Так вот, приходит клиент,
он берет в этом сервисе блокировок,
как можно догадаться, блокировку,
и после этого отказывает.
Вот если мы можем требовать от того,
что от там GFS или от там другой,
от самой Bigtable, чтобы в нем отказывать,
чтобы он был тейрайсинг отказан,
от клиента мы, конечно, отказоустойчивость требовать не можем.
Поэтому мы должны позаботиться о том,
а как же наша система будет восстанавливаться после сбоя клиента.
И там нужны какие-то таймауты,
и это там каскадом влечет за собой разные интересные последствия.
А именно, если коротко говорить,
что распределенные блокировки – это довольно сломанный,
довольно странный паттерн,
довольно странный механизм координации распределенных систем.
Его многие используют, но при этом, что самое нерепое,
гарантировать взаимного исключения он не может.
Это гарантировать ваш локальный статем юток с вашим многополучным приложением.
В общем, про клиентов нужно тоже думать, потому что они отказывают.
Про клиентов нужно еще думать,
потому что они на самом деле являются
полноценными участниками распределенной системы иногда.
Потому что они тоже исполняют код вашей системы на самом деле.
То есть в простейшем случае клиент просто посылает системе запрос,
там по HTTP скажем,
но в сложных случаях и у нас будет скоро система ZooKeeper,
сервис координации.
Это как раз опенсорс-альтернатива вот такому сервису блокировок,
который в Google назывался Chabi.
Так вот, если вы используете ZooKeeper,
то в ZooKeeper вы тоже можете брать условно какие-то блокировки,
сделать их сами, но смысл этого не меняется.
И в случае смерти клиента, отказа клиента,
система должна каким-то образом уметь откатить его действия,
освободить его блокировки.
И в этом случае от клиента требуется выполнять некоторый протокол,
согласованный с протоколом,
который работает на узлах самого ZooKeeper.
В общем, клиент является здесь полноценным участником системы.
Ну и наконец, про клиентов нужно говорить,
потому что именно на уровне клиентов,
на уровне наблюдаемого поведения,
формулируются очень важные свойства систем,
а именно в адресу согласованности.
Вот для разработчиков распределенной системы эта система –
это набор узлов, которые размещаются на кластере
и обмениваются сообщениями.
Для клиентов распределенная система – это некоторая точка входа,
некоторый сетевой адрес,
по которому клиент соединяется с какой-то машиной системы
и задает у него запросы.
И вообще говоря, в хорошей распределенной системе,
конечно же, клиент не знает про какие-то нюансы реализации.
То есть клиент как клиентская библиотека может про это что-то знать,
но клиент всего лишь вызывает методы клиентской библиотеки,
говорит, я вот хочу добавить что-то в файл,
я хочу записать что-то по ключу,
я хочу создать узел в дереве ZooKeeper.
Вот такие вот простые операции,
и клиент в таком случае.
Клиент взаимодействует с системой, по сути,
как с некоторым объектом, там, не знаю, таблицей, деревом,
файловой системой, который конкурентный в том смысле,
что с ним могут работать одновременно разные клиенты.
Я отказываю устойчивое,
то есть мы не ожидаем, что сбой путьной машины сделает наше дерево ZooKeeper
или наша файловая система целиком недоступной.
Но при этом клиент не думает, как именно его запросы обслуживаются,
он не хочет чем-то думать.
Идеально было бы, чтобы для него система представлялась таким вот
атомарным, отказоустойчивым, надежным объектом,
в который помещается еще неограниченное количество данных,
то есть система умеет масштабироваться.
Но вот не любая система, не о любой системе можно так думать.
И тут возникает понятие, которое носит название модель согласованности,
ну или в случае систем с транзакцией модель изоляции.
Пользователи, вернее, работают с распределенной системой,
как с объектом в разделаемой памяти, условно говоря.
Вот адрес системы буквально ими переменный,
ну а разные клиенты – это буквально разные потоки.
То есть так об этом можно думать, и это будет разумно соответствовать реальности.
И, как правило, от разделаемого объекта мы ждем какой-то понятной семантики,
как он будет работать в случае конкурентного доступа.
Если мы говорим про какие-то объекты внутри многопоточной программы,
то мы их защищаем юдоксом, и все понятно.
А если мы говорим про распределенную систему,
то становится уже менее понятно, потому что…
Ну представьте себе, вы клиент…
Давайте я покажу вам такие, не то чтобы слайды,
но вот я воспользуюсь картинкой, которую я рисовал в другом курсе,
потому что тут уже все есть.
Вот представьте, что у вас есть определенная система,
у нас есть каких-то реплик, они каким-то образом общаются друг с другом.
Это клиенту не важно.
А клиент просто отправляет сначала запрос на запись в эту систему,
получает подтверждение о записи.
Он там записал в ячейку памяти единицу,
в каком-то ключувке в хранилище единицу.
Получил подтверждение от системы, потом пошел к своему соседу,
рассказал об этом, и этот сосед отправил систему запрос
на чтение и тоже получил ответ.
И конечно, второй клиент ожидает, что чтение вернет результат,
который сделает клиент.
Вот такие естественные ожидания у пользователей от системы.
Ну потому что пользователи всю эту распределенность не наблюдают.
Но при этом внутри системы обеспечить такую гарантию довольно сложно.
Почему сложная?
Ну потому что система распределенная, и она должна каким-то образом
упорядочивать все записи, все чтения, которые с ней происходят.
Каким образом это можно делать?
Ну вот можно делать это по-разному.
Например, можно упорядочивать все записи, которые с системой происходят,
с помощью временных меток.
Вот узел получил запись, посмотрел на локальные часы,
увидел, что сейчас 13.01, и вот назначил запись в такую временную нетку.
Если на другой узел придет другая запись,
и эта запись получит другой временную нетку,
то вот система поймет, что была запись первая, была запись вторая,
они как-то упорядочились, и в системе нужно оставить более свежую запись.
Ну и могут возникать какие-то неприятные сценарии,
а именно, что пользователи пришли в одном порядке,
а часы там на узлах расходились, они были не синхронизированы.
И синхронизация часов – это отдельная история, отдельная сложная история,
и, к сожалению, история, которая не имеет, скажем, идеального решения.
Вы не можете идеально синхронизировать часы.
И ваши записи могут переупорядочиться.
Или, скажем, вот вы сделали эту запись,
эта запись, разумеется, была обработана не одной машиной,
каким-то набором машин, каким-то кворумом,
и, скажем, вот пользователь записал данные вот сюда.
А после этого он сообщил своей записи другому пользователю,
а пользователь прочитал, и прочитал их там с другой машины.
И запись не увидел.
Вот пользователь не может, не знает про то,
как система устроена внутри, не знает про то,
что она там временные мерки выбирает, или что какие-то собирают кворумы.
Пользователь лишь наблюдает,
лишь выполняет операции, порождает вот такие вот отрезки,
начало операции, завершение операции, может обмениваться сообщениями
с другими клиентами, с другими пользователями,
и отправлять новые запросы. То есть он порождает вот такие вот,
что называется, конкурентными историями системой.
И он хочет, чтобы эти конкурентные истории были разумны в некотором смысле.
То есть понятно ему клиенту, который не знает про то,
как система реплицирована там, устроена изнутри.
И вот ровно в этом месте появляется очень естественное принятие модели согласованности.
Модель согласованности говорит, а как же пользователь может думать
об исполнении своих операций внутри системы.
Это понятие не про то, как система устроена внутри.
Это система про то, как пользователь наблюдает поведение этой системы.
То есть мы здесь декомпозируем клиентов, гарантии клиента
от децентрализации.
И самая сильная и самая естественная гарантия,
которую вы как правило хотите получить, называется
либо линьеризуемость, либо строгая сириализуемость.
Вот это две главные гарантии, которые вы хотите иметь,
когда вы работаете с определенными системами.
Линьеризуемость означает, что несмотря на то, что все системы
можно работать конкурентно, любое исполнение операций над этой системой
можно представить себе в виде последовательного исполнения,
где все операции происходят в некотором порядке друг за другом
и при этом сохраняется относительный порядок неконкурирующих операций.
То есть если пользователь выполнил операцию О1
и после этого, возможно, другой пользователь выполнил операцию О2,
то система исполнила их как будто бы в некотором глобальном порядке,
но обязательно О1 исполнила до О2.
Почему это важно?
Почему мы хотим такого атомарного наблюдаемого поведения
или, строго говоря, линьеризуемого поведения?
Потому что очень удобно пользователю не думать про конкурентность.
А почему мы хотим, чтобы система соблюдала порядок операции в реальном времени?
То есть если вы сделали операцию О1, она завершилась до начала другой операции,
то эти записи будут системой в таком же порядке.
Потому что пользователь все-таки имеет какие-то ожидания
про порядок выполнения операции внутри системы.
В данном случае пользователь сделал запись, отправил сообщение другому клиенту,
второй клиент сделал чтение.
Разумеется, второй клиент ожидает, что он увидит результаты этой записи.
Почему?
Потому что между этим чтением и этой записью есть причинность.
Ну или это называется отношение happens before.
Мы говорили о нем в прошлом, это и весной.
Конечно же, пользователь не знает про предшествование операции в реальном времени,
потому что, опять же, я говорю, задача синхронизации часов не решается идеально.
Но пользователь знает, что вот это чтение случилось, потому что случилась эта запись.
Но беда в том, что система не знает про то, что между этим чтением и этой записью есть такая зависимость.
Причинность реализуется, коммуникация возникает за пределами системы.
Но при этом сама система должна эту коммуникацию, эту причинность учесть.
Поэтому она дает гарантию не в терминах после причинности, которая ей неизвестна, а в терминах реального времени.
То есть система вам говорит, что если вы завершили запись, а после этого начали чтение,
то ваше чтение увидит, по крайней мере, сделанную запись, а может быть, что-то более свежее.
Вот это такая фундаментальная гарантия.
И, скажем, для Key Value хранилищ или для систем, которые реализуют отдельные объекты, это гарантия линиализуемости.
А в случае, если мы говорим про систему с транзакциями, про базу данных, про систему,
где можно, скажем, потрогать сразу несколько строчек таблицы, то это называется строгая сериализуемость.
То есть реализуемость и сериализуемость – это гарантия атомарного поведения нашей системы.
Ну и плюс дополнительная гарантия про соблюдение предшествования аберраций клиентов,
про соблюдение happens before через соблюдение порядка аберраций в реальном времени.
Ну вот, это такая фундаментальная диаграмма всех возможных гарантий.
Тут находятся более славые гарантии.
Но они нам, наверное, не очень интересны, потому что все-таки современные системы, хорошие системы,
чаще всего линиализуемые.
Вот правая ветка – она про гарантии относительно операций над отдельными объектами.
Правая ветка и левая ветка – про гарантии зарядции транзакций.
Как правило, вы работаете либо с транзакциями, либо с отдельными операциями.
И вот вы используете либо эту ветку гарантии, либо эту ветку гарантии.
Вопрос. Как же такие гарантии достигаются?
То есть как строятся линиализуемые системы, которые, несмотря на то, что внутри реплицированы,
достигают вот такого атомарного наблюдаемого поведения?
Для этого есть некоторый фундаментальный примитив, который называется atomic broadcast.
И в следующий раз у нас будет система зукипер.
Так вот, она вот на этом примитиве и построена.
Атомик broadcast – это такое фундаментальное понятие.
Это примитив коммуникации в распространенной системе.
Примитив коммуникации между ее узлами.
Вот с одной стороны между каждой парой узлов есть TCP-соединение.
И оно там дает какие-то гарантии.
Гарантию можно считать, что TCP-соединение строго говоря даёт гарантию exactly once,
но поскольку соединение может порваться и вы должны будете его переустанавливать,
то эта гарантия превращается в at least once.
Так вот, для того чтобы строить линиализуемые системы,
то есть системы, которые ведут себя как одно атомарное целое,
нужно поверх такой ненадежной сети, которая не гарантирует exactly once доставку,
построить систему, которая… построить примитив коммуникации,
который дает больше гарантий.
Вы можете отправить сообщения другим узлам.
Два разных узла могут отправить два разных сообщения.
Но вот примитив коммуникации Atomic или Total Reordered Broadcast
гарантирует, что если два сообщения были отправлены,
то отправлены всем узлам разом,
но если два сообщения были отправлены разом,
то узлы, которые доживут до получения этих сообщений,
получат их в одном и том же порядке.
То есть, это примитив, который позволяет вам сделать такую широкомещательную рассылку
с гарантией порядка доставки сообщений.
И если у вас такой примитив есть,
если вы можете сделать такую так иначе,
И если у вас такой примитив есть,
если вы можете независимо с разных узлов
отправлять всем другим узлам сообщение,
эти сообщения доставляются в одном и том же порядке,
вы можете на самом деле решать задачу репликации,
то есть вы можете хранить копии состояния вашей системы,
которые будут согласовано меняться,
согласовано переживать одну и ту же серию апдейтов.
Вот приходит клиент вашу систему,
он скажем, в ней что-то пишет,
переписывает какую-то строчку вашей таблице,
своей таблице,
для этого отправляется команду в систему,
какая-то реплика ее получает,
не важно какая,
она иницирует бродкаст,
и когда этот бродкаст доставит эту команду
на эту самую реплику,
то она применяется к данным пользователя
и пользователь получает ответ.
Другой клиент может прийти на другую реплику
с другой командой, с другой записью,
и эти записи вообще конкурируют во времени,
но за счет того, что на каждую реплику
эти две записи доставятся в одном и том же порядке
просто силами этого бродкаста,
мы получим гарантию,
что все реплики нашей системы двигаются
через одну и ту же серию апдейтов,
проходит через одну и ту же серию апдейтов,
а значит, пребывает в согласованном состоянии,
и мы таким образом получаем линьеризуемость.
Если мы хотим сделать линьеризуемость,
то вот такой фундаментальный принцип
мы, как правило, используем в нашей реализации
приметив под названием Atomic Broadcast.
А каким образом этот Atomic Broadcast
можно построить?
Вот тут возникает, наверное,
главная задача, которая в теории распределенных систем
существует, это задача консенсуса.
Задача консенсуса звучит очень просто.
У вас есть n узлов,
и они должны договориться об общем выборе.
Просто каждый узел получает сообщение, запускается,
и должен поговорить с другими узлами,
и завершиться, и выбрать некоторые значения.
Все, что нужно сделать,
выбрать одно из предложенных значений,
все узлы, которые доживут до выбора,
должны выбрать одно и то же,
ну и алгоритм должен завершаться.
Вот такая совершенно примитивная,
тривиальная постановка задача
оказывается эквивалентна
по сложности задачи реализации Atomic Broadcast.
А Atomic Broadcast – это вот приметив,
с помощью которого достигается главная гарантия,
главная модель согласованности линьеризуемости.
Вот если вы умеете решить такую простую задачу,
то вы можете добиться линьеризуемого поведения вашей системы
и таким образом от пользователя скрыть всю распределенность.
Это очень важный, очень фундаментальный факт,
что мы всю сложность распределенных систем,
ну фактически не всю половину сложности,
свели к одной простой задаче.
И вот тут уже не нужно думать про клиентов,
тут уже не нужно думать про какие-то Happens Before,
которые там возникают.
Если вы просто можете научить Enz Love
выбирать общее значение и завершаться при этом,
то вы можете более-менее добиться любого,
вы можете добиться линьеризуемого
или строгости реализуемого поведения сверху.
Ну и про задачу Consensus,
это конечно долгая история,
ее невозможно там за одну лекцию, за полчаса рассказать,
но с задачей Consensus связаны два фундаментальных результата.
И результаты они про то,
что задача решается довольно тяжело.
Первый результат состоит в том,
что для того, чтобы задача решилась,
для того, чтобы вы могли завершаться,
сделать выбор и при этом не нарушать Agreement,
то есть не расходиться во мнениях,
не расходиться в выборах,
вам требуется довольно много узлов.
То есть если вы готовы переживать отказы при этом,
то вам требуется довольно много узлов.
А именно, если вы хотите пережить
f отказов задачи Consensus, то есть в вашей системе,
то вам требуется, по крайней мере,
два f узлов, два f плюс один узол.
То есть если вы хотите хранить сообщения,
хранить данные пользователя,
какую-нибудь его запись, строчку в таблице
и переживать отказ любого диска,
то вам нужны три копии.
Если вы хотите хранить данные,
переживать отказ двух дисков,
то вам нужно уже пять копий данных и так далее.
Сразу замечу, что задача серьезно упрощается,
и вам не нужен никакой Consensus,
если ваши данные мутабельны.
И это такое очень важное замечание,
вы с самого начала должны понять,
у вас данные мутируются или нет.
Если мы говорим про кивалию хранилища
или про базу данных, то вы, конечно, там делаете апдейты.
А если мы говорим про файл, про файловую систему,
то, может быть, в API файловой системы у вас нет апдейтов.
Может быть, вы просто добавляете данные к файлу,
делаете апенды.
Это довольно разумное ограничение,
потому что оно с системой снимает огромную уголовную боль,
а именно необходимость решать задачу Consensus,
в большом масштабе.
Но если у вас данные мутабельны,
если вы пишете кивалию хранилища,
или если вы пишете базу данных,
что одно и то же, то Consensus вам необходим,
потому что иначе это единственный способ,
в котором вы можете добиться линейализуемого поведения,
то есть вскрыть распределенность от пользователя
и дать ему очень понятную модель поведения вашей системы.
Первое ограничение, которое касается задачи Consensus,
в том, что требуется довольно большая избыточность.
То есть для двух реплик, 5 узлов,
для трех реплик, 7 узлов и так далее.
Это первое ограничение.
Второе ограничение.
Оно еще более...
Это ограничение, в общем-то, понятно,
если вы знаете про систему кворумов.
В любом случае, если вы даже не знаете,
поговорим об этом,
когда будем разбирать систему Кассандра,
то сбил смысле.
По-другому.
Есть первое ограничение про то,
что если система должна переживать f отказов
и завершаться, и не нарушать общего выбора,
то требуется 2 f плюс 1 реплика, по крайней мере.
Второе ограничение.
Оно звучит еще более шокирующе.
Это, наверное, самое известное ограничение,
которое существует в распределенных системах.
Это FLP-тиарема.
FLP-тиарема говорит о том,
что задачи Consensus,
ее невозможно гарантированно решить,
даже если у вас, в принципе,
возможен только один сбой.
То есть даже если вам пообещали,
что в системе откажет не более одного узла,
что, конечно, в реальности.
Невозможно.
В смысле, получить такое обещание.
То даже в таких условиях
все равно задача не решается.
В том смысле, что
если ваш алгоритм Consensus
не нарушает agreement,
то есть никогда его ответы не расходятся,
выборы узлов не расходятся,
то в этом алгоритме непременно существует LifeLog.
То есть сценарии,
где узлы бесконечно долго
не могут сделать выбор.
Все это, конечно, справедливо
не в любой теоретической модели,
а справедливо в модели,
где мы предполагаем
асинхронность
или частичную синхронность.
То есть мы не делаем предположений
о скорости наставки сообщений.
Мы не требуем, чтобы они все
гарантированно доставляли
за некоторое ограниченное время.
Как правило, конечно же,
сеть в датацентрах
устроена довольно эффективно.
То есть если мы говорим про тайминги,
то внутри датацентра можно ожидать
латентности отправки сообщений
в пределах миллисекунд
или сотен микросекунд,
что довольно быстро.
Но это вероятностная гарантия.
Вам не гарантируют,
что всегда 100% сообщений
будут доставляться за ограниченное время.
Если вы это допускаете,
а любая промышленная система
работает именно в таких предположениях,
то вы подвергаете себя
эффекту FLP-тиаремы.
И таким образом
ваш консенсус может не завершаться.
То есть ваш Atomic Broadcast
может не доставлять сообщения.
И для вашей распиленной системы
это означает,
что ваша распиленная система
может не отвечать пользователю.
Она не нарушает...
Если консенсус не нарушает
требования общего выбора,
то это в свою очередь
на уровне Atomic Broadcast
не приводит к нарушению
общего порядка доставки.
Это в свою очередь не приводит к тому,
что не приводит к нарушению
линьализуемого поведения.
Но при этом это недоступна система.
Вот такой вот фундаментальный факт,
что если система линьализуема,
то она использует Atomic Broadcast,
то есть решает задачу консенсуса
или серию задач консенсуса
для того, чтобы с помощью этой серии
договориться о порядке
доставки сообщений.
Но поскольку все эти задачи
решаются по синхронной модели,
то на нас действует теарема FLP.
И теарема FLP говорит,
что в нашей реализации
обязан быть лайфлог,
обязан быть сценарий,
где из-за неудачного порядка
доставки сообщений в сети
алгоритм никак не может завершиться,
никак не может обработать
запрос пользователей.
Это первый способ
говорить про какие-то
теоретические гарантии.
Второй способ – он гораздо
более популярный, потому что он
не требует знаний про
какие-то внутренние детали,
про теорию задачи консенсуса,
про бродкасты и все такое.
Это
засуждение в духе
каптеаремы.
Вот есть
такой подход
к классификации распределенных систем,
который в принципе
про то же самое,
но совершенно другими словами.
Когда мы говорим
про распределенную систему, мы должны сказать,
какие гарантии надают пользователю,
как она переживает отказы
и
как она ведет себя, когда пользователи
с ней работают конкурентно.
Особенно интересно, как система
переживает один специальный вид отказов,
а именно партишены.
Вот у вас могут
строить отдельные узлы – это одна проблема.
Еще у вас может
ломаться сеть. Ломаться таким образом,
что она разваливается на два сегмента
и
внутри одного сегмента
узлы могут общаться друг с другом
и могут работать с клиентами.
А в другом сегменте,
внутри одного сегмента, а между сегментами
коммуникации нет.
Скажем прямо,
внутри DC с такой топологией
довольно сложно получить партишен,
но если у вас
есть несколько DC, которые соединены
к магистральными каверами,
то такая ситуация уже вполне естественна.
И довольно любопытно,
как система может
на такое поведение сети
реагировать.
Есть специальный сценарий
для этого.
Есть специальное имя для этого
Speedbrain.
Для поведения системы,
когда система разваливаясь
на два сегмента,
начинает в этих сегментах действовать независимо.
И если
вы можете
в одну часть
один клиент,
работающий с одним сегментом,
в него данные записать,
а с другой стороны
другой клиент из другого сегмента
может данные прочитать, то понятно,
что второй клиент запись первого увидеть
не может, потому что просто коммуникации
между полушариями системы нет.
Вот такой сценарий
называется Speedbrain.
И понятно, что вот здесь есть
некоторые фундаментальные трейдов,
как система себя ведет.
Вот интуиции по поводу
поведения системы в данном случае
выражают в правилах в виде каптиоремы.
Говорят, что есть
consistency – это
гарантия,
это, собственно, поведение системы,
как оно описано пользователю.
Вот линейризуемость –
это модель согласованности.
Consistency model.
Есть availability –
это свойство, что система отвечает
на запросы, которые вы в нее посылаете.
И третья буква
в каптиореме – это
partition tolerance, то есть
устойчивость к отказу, способность системы
каким-то образом реагировать на partition в сети.
И вот говорят, что
можно классифицировать
системы следующим образом,
что системы не могут достигать
всех трех свойств, то есть они не могут
переживать партишены,
оставаясь абсолютно доступными
и сохраняя согласованность.
Поэтому, когда вы говорите про конкретные системы,
вы должны охарактеризовать,
какой выбор они делают,
какие буквы они выбирают.
Грубо говоря, есть системы,
которые
разумно предположить,
что есть три типа системы.
Все три свойства недоступны,
потому что понятно, что если у вас
система разделилась на две части,
и они не общались между собой,
то нельзя одновременно быть согласованным
и обслуживать, отвечать на каждый запрос.
Но поэтому нужно выбрать
какие-то две буквы из трех,
и вот скажем, у нас
будут системы,
которые выбирают consistency
и которые выбирают доступность.
То есть у вас есть такой фундаментальный тредов
между доступностью системы и согласованным поведением.
Но в такой формулировке
это довольно странная теория,
но это вообще не теоремы, конечно же,
в этом собственно и проблема
этой самой CAP.
Такая просто интуиция,
которая следует из очевидного дизайна
от определенных систем.
И вообще формулировать ее нужно немного иначе.
Сказать, что...
Ну не то чтобы вы выбираете любые две буквы,
потому что partition...
Если consistency и доступность
это свойство вашей системы,
то partition это просто некоторая данность
в физическом мире.
Partition могут возникать.
Поэтому когда говорят про CAPTIOREMO,
говорят, что в системе бывают partition
неизбежно.
Поэтому ваша система должна выбирать,
как она себя в случае partition ведет.
Либо она способна обслуживать пользователей
с обеих сторон partition,
и тогда она объявляется доступной.
Или же она
должна быть согласованной,
то есть линеризуемой.
И это означает,
что, видимо,
в какой-то части partition
она обслуживать пользователей не может.
И тут нет никакого общего рецепта,
в смысле, как делать правильно.
Решение, то есть выбор
между consistency и доступностью
зависит от
просто вашей задачи,
от ваших требований.
Мы поговорим про Cassandra.
Cassandra — это open-source реализация
системы Dynamo,
киволюхранилище,
с помощью которого Amazon строил свой интернет-магазин.
Так вот,
Amazon не было необходимости
строить систему с высокой доступностью.
В системе Dynamo
хранились корзины с товарами.
И, ну, корзины с товарами —
это все-таки не банковский счет.
Если в случае partition
из этой корзины
проходит какой-то товар, потом снова там появится,
ну, это не смертельно.
Главное, чтобы пользователь понимал,
что он покупает, когда он нажимает на «покупить».
С другой стороны, есть система,
при которой выбор
очевиден в другую сторону.
Если мы, скажем,
храним данные пользователей,
какие-то, не знаю, его письма, или счета,
или что-то подобное, то мы не можем позволить
себе потерять эти письма
или сделать деньги недоступными.
В смысле, сделать...
Нарушить изоляцию транзакций над
счетами пользователя.
Мы не можем себе позволить
испортить данные,
которые он пишет в свою,
в нашу базу, в наше облако,
пусть даже ценой
некоторой недоступности.
Вот если мы облачный провайдер,
то мы не можем предоставить ему...
Мы можем предоставить два типа систем,
но мы не можем за него
выбрать в любом случае.
Вот если вам нужна доступность,
то вы, как правило, не используете консенсус.
Если вам нужна доступность,
то вы должны строить систему без консенсуса,
потому что консенсус еще раз
ограничивает вас в доступности, когда возникает партишка.
Я говорил, что
для того, чтобы решить задачу консенсуса,
вам нужно иметь
доступными, по крайней мере, половину узлов.
Но вот партишка наставляет
систему
с большей частью и меньшей частью.
И в меньшей части консенсус не решается,
а вместе с ним не решается задача
на тумбе Quest,
а вместе с ним не решается репликация,
и система блокируется.
Так что если вы работаете
с какой-то системой,
то вы должны понимать
во-первых, на совсем грубом
высоком уровне,
как она позиционирует
себя в смысле каптиоремы,
и если она позиционирует себя
как система доступная,
то, видимо, из этого следует,
что она внутри не использует консенсус,
не использует Atomic Broadcast,
и это означает,
что она не может вам
предоставить высокой модели согласованности.
А это значит, что эта система
неизбежно заставляет вас
думать про свою реализацию.
Вот мы будем говорить про
Кассандру и Динамо,
и для того, чтобы понять,
какие гарантии вы получаете,
вы обязаны разобраться,
как внутри система
реплицирует данные.
Если же вы видите,
что ваша система...
Если вы видите в документации,
что ваша система позиционирует себя
не как AP,
а как consistency
плюс partition tolerance,
то есть она выбирает случай partition
в consistency, то вы понимаете,
что эта система использует консенсус,
она видимо предоставляет вам
высокую модель изоляции
или высокую модель согласованности,
но в то же время
эта система в случае partition
в одной части
становится полностью недоступной,
ну и в принципе
в ней возможны лайфлоги,
то есть какие-то периоды,
когда система недоступна, хотя все ее узлы
могут быть даже живы,
просто в силу асинхронности сети
и в силу FOPTR.
Такой вот фундаментальный трейдов,
который заложен в любой системе
и фундаментальный трейдов
в смысле между consistency
и availability
и на уровне реализации,
на уровне уже чего-то конкретного
этот трейдов заключается
в выборе или в отказе
от использования
atomic broadcast и задачи консенсуса.
То есть если
cap-теорема
еще раз правильно
спозиционирую ваше
понимание, cap-теорема
это всего лишь такая интуиция,
какое предпочтение
делать система в своем дизайне.
А под капотом уже
действуют совсем другие
теоремы
и другие задачи,
и под капотом мы говорим про задачу
atomic broadcast,
то есть упраточную доставку сообщений
про то, что эта задача
atomic broadcast решается
с помощью задачи консенсуса,
а задача консенсуса в некоторых
ограничениях просто не решается
и решается
без каких-то очень сильных гарантий,
то есть без гарантий завершения.
И вот все это связано
напрямую с моделью согласованности,
которую система вам предоставляет.
То есть как вы можете думать
о ее поведении?
Вот если вы все это в голове свяжете,
то станет примерно понятно,
как можно из гарантии
системы сразу понимать какие-то
фундаментальные принципы ее дизайна
и наоборот понимать, что какие-то
гарантии система вам дать не может.
Если она является
высокодоступной, то это означает, что
она не может использовать консенсус
и это значит, что она будет
рано или поздно
и будет для вас неатомарной.
Ну вот это такой
совсем короткий экскурс
в
основные базворды,
которые нужно знать,
при работе с определенными системами.
И я бы еще
к этому у нас осталось немного времени,
я бы еще раз здесь сказал про то,
как примерно
о системе можно
думать не в смысле
задач, модели согласованности
и возможных отказов
и отказоустойчивости
в смысле архитектуры.
Вот это такие два взгляда, которые
требуются для того, чтобы
системой просто аккуратно
пользоваться. Какие гарантии она дает,
какие задачи она внутри решает с одной
стороны, а с другой
какие
слои выделены
в ее архитектуре.
Смотрите,
как бы система ваша не была устроена
и какой бы модель данных
она вам не предоставляла,
будь то key value хранилищ,
будь то база данных,
будь то распределенная файловая система,
в какой-то момент
вы должны
где-то хранить данные.
То есть система распределенная, конечно же,
но рано или поздно
вы спускаетесь в отдельные жесткие диски
и должны там
хранить и модифицировать
большое количество данных.
Причем на этих конкретных дисках
уже эти данные будут
обновляться точечно
по-другому скажу,
что вот в конце концов
на самом низком уровне архитектуры вам нужно
будет решать задачу хранения данных
и сложность в этой задаче
в том, что
чтобы позволить
точечный апдай данных
скажем, если вы храните key value хранилищ
то вот вы храните это key value хранилищ на диске
и при этом
делать это эффективно
с учетом того, что диски
не умеют
случайный доступ.
Вот диски, которые вращаются
они в принципе не умеют прыгать
быстро между отдельными секторами.
Для этого диску нужно повернуться
и передвинуть считывающую головку
на другой трак.
Это вот единицы миллисекунд.
Вот на этом уровне возникает такая задача
как делать это эффективно
и как делать это надежно.
Как переживать, скажем,
рестартную машину в любой момент времени.
Это вот самый низкий слой
архитектуры любой системы.
Абсолютно любой.
Дальше вы поднимаетесь на следующий
уровень. Это уровень репликации.
И вот уровень репликации
это ровно тот уровень,
на котором возникают
все гарантии, в смысле модели
согласованности.
И на котором
появляется
трейдов между доступностью
и согласованностью.
И вот это тот уровень,
на котором фиксируется отказово-устойчивость вашей системы.
То есть сколько отказов
она способна переживать.
Ну ладно, не совсем, но и на этом уровне тоже.
Вот
вся
все те базворды,
которые я произносил,
вот у меня был откаст, консенсус, модели согласованности.
Это все вот этот уровень репликации.
Его задача
взять отдельные жесткие диски
и отдельные хранилища данных
на этих жестких дисках
и сделать
из этих нескольких дисков
нескольких машин,
которые сами по себе
не отказово-устойчивые, разумеется.
Сделать некоторую отказово-устойчивую
сущность.
Сделать одну машину,
которая как будто бы не отказывается
с помощью томикбродкаста.
То есть решить задачу консенсуса
и решить задачу репликации.
И вот эта задача,
решение этой задачи более-менее
называется
реплицированным автоматом.
Реплицированный автомат позволяет вам
реплицировать
некоторое состояние совершенно
произвольное между несколькими машинами.
Для этого там решается задача.
Реплицируется блок изменений.
Он реплицируется с помощью томикбродкаста.
Потом несколький модель
решает задачу консенсуса.
И вот если вы используете такой дизайн,
то на этом уровне у вас появляется как раз
с одной стороны согласованность,
а с другой стороны вы теряете
доступность в случае,
в меньшей части partition в случае
разделения сетей на два сегмента.
Если же вы Cassandra или Dynamo,
то вы используете другие протоколы,
более слабые протоколы для репликации,
и в этом случае вы теряете
согласованность.
Но при этом вы можете обслуживать записи и чтения
даже в случае partition.
Следующий уровень репликации любой системы
это слой
шардирования.
Вот у вас может быть
очень много таблиц
в системе, очень много данных
в таблицах, очень большие файлы,
очень
много файлов.
И все это, конечно же, не помещается
в отдельную машину. Вы берете
много машин,
делите ваши данные
таблице
или файловую систему каким-то образом
на части.
С файлами системы это сложно делать, но вот
когда мы говорим про таблицы, то есть про кивали
у хранилища и про базу данных,
то шардировать довольно естественно.
Можно по диапазонам строчек.
Вот мы эти данные шардируем,
раскладываем их по разным машинам
и таким образом
мы позволяем системе
потенциально горизонтально
масштабироваться.
Если мы пытаемся соотнести это
с предыдущими уровнями, то вот
картинка примерно это устроена.
Если мы пишем базу данных,
то под капотом это будет
кивали у хранилища. Это кивали у хранилища
делится на диапазоны
ключей.
Эти диапазоны ключей реплицируются
на разных машинах и реплицируются
с помощью Turing Broadcast консенсусом.
При этом каждая физическая машина в кластере
она отвечает за
репликацию
сразу многих диапазонов
ваших данных.
После этого уровня
идут уже транзакции
и идет слой распыленных запросов.
Так же как CAPTIOREMA
в фундаментальном смысле
CAPTIOREMA в фундаментальном смысле
что она заставляет вас делать выбор
между
согласованностью и доступностью
то есть между применением консенсуса
и отказом от консенсуса
так же
вот эта диаграмма
вот эти слои, этот дизайн
он достаточно фундаментален
и так или иначе прослеживается в любой распределенной системе.
Вот из таких кубиков вы в конечном итоге
ее собираете.
Тут конечно есть
разные нюансы, разные
разные способы
в этом дизайне что-то оптимизировать.
Это отдельная большая история.
Но в целом
если вы говорите про
Кавку
или вы говорите про Кассандру
или вы берете спанер
то в любой подобной системе
все равно будут
вот такие вот слои.
Где-то транзакций не будет, не будет SQL
но по крайней мере вот такие подзадачи
решаются довольно изолировано
и о них нужно и думать
изолировано, потому что нужно
декомпозировать разные компоненты системы
по задачам, которые они решают.
Вот задачи принципиально такие
и одна из них выстраивается над другой.
Ну вот примерно так
можно думать про распределенную
систему в целом.
То есть брать любую систему
и вот раскладывать ее по слоям
архитектуры, по гарантиям, которые
она доставляет, по задачам
которые внутри нее решаются или нет
и таким образом
получать довольно неплохое представление
об отказу устойчивости
и о поведении системы.
Просто потому что вот такой дизайн
и такие ограничения
они касаются любой системы
как бы она не была устроена
какую бы задачу она не решала.
И уже вот на понимание
этой общей архитектуры
и общих ограничений
нужно конкретную систему отображать.
Мы собственно в последующих занятиях
так и будем делать. Если мы будем
говорить про Кассандру, то мы будем
разбираться, а как именно
она не используя
консенсус
пытается все-таки обеспечить
согласованность и как это позволяет
достичь доступности.
И если мы говорим про
Spanner, то каким образом
он обеспечивая согласованность
все же пытается достичь
доступности.
В общем, это разговор
про следующие занятия, там где
мы будем говорить про конкретные системы
разбирать конкретный дизайн, свойства.
Ну а пока
с обводной частью, наверное, все.

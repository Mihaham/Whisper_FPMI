Давайте продолжать и начинать нашу вторую лекцию.
Я напомню, что в прошлый раз мы вели понятие случайного
процесса, его важнейшую характеристику, вероятностное
это семейство конечномерных распределений ввел теория
Калмогорова о том, что нам достаточно сдавать процесс
через некое семейство функций, которое согласовано в определенном
смысле, также как согласовано семейство конечномерных
распределений.
Ну и некоторые другие понятия.
Сегодня мы будем с вами углублять наш взгляд на
случайные процессы и начнем мы, пожалуй, с важнейших
численных характеристик после функции распределения
это мотождание, дисперсия и так далее.
Первое, я не буду подробно писать, здесь все очень просто,
что мы будем называть математическим ожиданием случайного процесса.
Мотождание случайного процесса это функция времени, которая
в каждый момент времени равна математическому ожиданию
соответствующего сечения.
Все тривиально.
То есть это значит, что мы Т зафиксировали, то, что
мы получим, это случайная величина, вот ее математическое
ожидание в этот момент Т.
Так что если нам нужно посчитать мат ожидания какого-то
процесса, мы просто проходимся по всем Т, вычисляем мат ожидания
соответствующей случайной величины, ну и получаем
функции этого Т.
То, что мотождание при разных Т может быть разным.
Вот эта функция называется математическим ожиданием
случайного процесса.
Она не обязана быть определена для всех Т, потому что мотождание
не обязательно быть определенным всегда.
У некоторых случайных ключин есть мотождание, у каких-то
нет мотождания.
Так что вот такая функция называется мотожданием.
Вот так и все, где она определена.
Точно также можно вести понятие дисперсии.
значит дисперсия это функция времени которая в каждый момент времени равна дисперсии соответствующего
сечения случайного процесса точно также как мат ожидания вот ну здесь я ничего нового
он практически не сообщил а вот новое для вас понятие это корреляционная функция я ее буду
обозначать буква r это функция двух моментов времени давайте так напишем t и s это функция
двух моментов времени которая для каждой пары t и s времен равна корреляционному моменту
соответствующих сечений корреляционный момент у него синоним кавариация я сейчас все распишу
кавариация от кси т запятая кси с а что это такое это есть математическое ожидание по
определению кавариации это кси от t минус мат ожидания кси от t умножить на кси от с минус
мат ожидания и от с вот по определению что такое кавариация двух случайных величин вот здесь мы
ее написали и если раскрыть скобки вспомнить свойства математического ожидания то тогда
можно переписать это в эквивалентном виде более простом математическое
ожидание кси от т на кси от с минус математ
могут быть коррелированы, могут быть зависимыми даже, так что вот эта функция
показывает корреляцию между двумя различными ее сечениями.
Что мы взяли t зафиксировали, взяли s зафиксировали, это у нас какие-то
случайные величины уже вполне определенные, и здесь мы вычисляем
корреляционный момент или к вариацию. Вот по вот этим формулам и получается
функция rcts. Эта функция, она наследует свойства кавриаций, ну например, она не
чувствительно к аддитивным постоянным, то есть если вы сюда добавите какую-то
константу, не случайную величину a, сюда добавите b, тогда кавриация не изменится.
Вы вольны для упрощения себе жизни в вычислении добавлять сюда и сюда любые
константы. Вот если вы умножите здесь на a, вы эту a можете вынести линейно за
кавриацию. Вот такие вот примитивные свойства можно назвать. Какие еще можно
свойства назвать? Корреляционный момент связан с дисперсией. Каким образом?
Корреляционный момент случайной величины сама собой дает дисперсию. В нашем
случае в наших терминах rtt есть кавриация xiat, это есть дисперсия, то есть d с индексом
xiat. Так что если мы знаем корреляционную функцию, то мы можем вычислить дисперсию,
просто взяв одинаковые аргументы у функции r. Еще есть свойство симметричность, есть свойство
вот такое. Если мы поменяем их местами, то это равносильно тому, что мы поменяем местами
множители в произведении, но понятно, что ничего не изменится. Так что вот есть такое свойство
симметричность. Есть еще одно свойство очень полезное, значит rx ts. Так, t и s, по-моему там
в квадрате. Сейчас я сначала напишу, меньше либо равно rxtt на rxss. Да, вот так вот. Это еще называется
неравенством Коши Банюковского для корреляционной функции. Ну, в общем-то, это даже не то,
что называется, это как бы следствие неравенства Коши Банюковского в теории вероятностей для
корреляционного момента. Потому что корреляционный момент определяет скалярные произведения, это
свойство скалярного произведения. Вот, хорошо. Вот такие свойства есть. Потом у нас будут лекции,
где мы гораздо глубже уйдем в изучение свойств корреляционной функции. А пока вот я напишу самые-самые
такие примитивные, которые нам вот сейчас понадобятся. Кроме корреляционной функции у нас в курсе еще будет
ковариационная функция. Мы ее будем обозначать буквой k. Ковариационная функция по определению,
это есть математическое ожидание и от t на кси от s. То есть это корреляционная функция ts,
корреляционная функция, плюс, получается плюс мат ожидания кси от t на кси, мат ожидания кси от s.
То есть она определена для процессов, не только у которых мат ожидания равны нулю, но если мат
ожидания равно нулю у процесса всюду, то тогда ковариационная функция совпадает с корреляционной,
потому что это просто 0 и это равный. В каких-то случаях удобно вычислять корреляционную, в каких-то
случаях ковариационную. У нас будут теоремы, которые формулируются гораздо проще, компактнее и
доказываются проще для ковариационной функции, а не для корреляционной. Ну в общем, мы будем
одинаковое количество времени иметь из той и другой функции. Обращаю внимание здесь на некий
разлад в терминологии, в некоторых книжках, например, Миллер Панков, они называют вот это
корреляционной функцией, а вот это ковариационной. То есть тут есть вот такая вот, обратите на это
внимание, я рассказываю так, как у меня в моих пдфках, так как вот в классических книжках авторов
на тангус Горбачёв, там еще много где. Вот это у нас будет корреляционной, а это ковариационной.
Так, ну вот, вот такие вот понятия. Что еще можно здесь вести? Говоря о корреляционных моментах,
мы можем вводить, кстати говоря, еще и взаимные корреляционные функции. Вот есть у нас два
процесса, кси и это. Тогда взаимной корреляционной функцией будем называть вот такую функцию. Видите
здесь, какой индекс кси-это, потому что он для двух процессов пишется. Это есть просто ковариация для
кси-ат на это атес. Вот, это взаимная корреляционная функция двух процессов. Так, ну что еще можно сказать?
Какие еще есть характеристики? А, характеристическая функция. Так, сейчас я сначала напишу, так сходу по
памяти не очень помню, она нам редко пригождается, фи. Значит, для кси-ат атес, это есть математическое
ожидание экспонента и на эс на кси-ат, вот так. Вот, то есть характеристической функцией процесса
называется вот такая функция, кстати, двух переменных, с и t, такая, что каждый момент времени t, она равна
характеристической функции, видите, зависит от s, соответствующего течения, вот так. Это характеристическая
функция процесса. Здесь, кстати, сразу но возникает. Вот, мы помним в теории вероятности, что
характеристическая функция однозначно определяет распределение случайной величины. Вот, в процессах это не так.
Если мы задаем хар-функцию вот таким образом, то хар-функция однозначно определит только одномерное
распределение процесса, то есть только распределение кси-ат. Она ничего не говорит о том, как связаны
между собой сечения в разные моменты времени. То есть вот, если у вас где-то спросят, то имейте в виду,
что хар-функция случайного процесса однозначно определяет только одномерное распределение. Она
не может восстановить распределение всего процесса, из нее нельзя вывести в общем случае семейство
конечномерных распределений. Хотя есть такие процессы, уникальные. Мы, кстати, будем с ними иметь дело и,
как правило, только с ними мы им будем иметь дело, у которых семейство конечномерных распределений
определяется одним только одномерным распределением. Вот, какие процессы есть, сегодня полосоновский
процесс будем рассматривать. Там достаточно задать некоторые его свойства и одномерные распределения,
и тогда из них можно однозначно получить свойства все остальные,
то есть семейство конечномерных распределений тоже из одного одномерного можно восстановить.
Ну и для таких процессов, естественно, харфункция процесса однозначно определяет не только одномерное,
но и всё, ну просто потому что из одноверного можно вывести любое другое,
просто такие вот особенные процессы, а в общем случае нет, харфункция не определяет всё.
Так, что ещё могу здесь назвать? С таких понятий вроде бы всё ввёл.
Ещё хочу пояснить по поводу вот этих вот всех вещей.
Обратите внимание, что математическое ожидание – это мат ожидания ксиатэ.
Сюда входит только одно сечение ксиатэ, сюда не входят никакие другие сечения,
только одно – т зафиксировали, вычислили мат ожидания.
Это означает, что математическое ожидание, оно определяется только одномерным распределением процесса.
Неважно, какое у него там двумерное, трюмерное распределение,
вот ожидание определено только одномерным распределением.
Вот это можно явно показать, записав это математическое ожидание
в терминах интеграла Риммана с Тель-Тиеса или Лебега с Тель-Тиеса.
Смотрите, как интеграл от x на dfc от x точкой запятой t.
То есть мы пишем интеграл, от чего мы берём, то есть это всё заменяем на x,
потом пишем дифференциал, и дальше идёт функция распределения этого чего-то.
Но оно зависит от t, поэтому мы вот таким вот обозначением пользуемся.
Так что вот эта запись, она просто явно нам показывает,
что мат ожидания является функцией только одномерного распределения.
И дисперсия тоже.
Её можно написать в интеграл x минус, получается вот так вот,
mxt в квадрате на dfxt.
Ну если вы не очень помните, что эта запись, значит это ничего страшного,
это просто для понимания я написал.
Если эта функция распределения имеет плотность,
тогда на этот дифференциал вы просто смотрите, как она плотность умножить на dx.
Плотность зависит от x и t вообще говоря, плотность умножить на dx.
Если это дискретное распределение, тогда вместо интеграла пишете сумму,
а вместо df вы пишете вероятность того, что xt равняется x.
Вот и всё.
И вот этих как бы двух случаев нам в принципе достаточно.
А вот эта запись, она вот самая общая, она в себя всё включает,
в том числе когда и смешанное какое-то распределение, не дискретное, не непрерывное.
Мы можем написать похожие вещи и для корреляционной функции.
Вот корреляционная функция, это будет тоже уже двумерный интеграл получается.
Давайте я напишу здесь.
Значит rcts можно тоже выразить как двумерный интеграл от чего?
x минус мат ожидания, xi от t на y минус мат ожидания, это от t.
Вот.
И умножить на d2f.
Значит xi x запятая y, t запятая s.
И вот здесь будет стоять уже двумерное распределение.
Ну а здесь уже двукратный интеграл или бегость Ильтьеса.
Ну это такие формулы абстрактные, но мы не будем ими пользоваться.
Я просто говорю, чтобы вы понимали, что вот эти величины, они зависят от одномерных распределений.
Вот эта штука не определяется одномерным распределением.
Эта штука определяется двумерным распределением.
То есть для её вычисления важно знать, как распределены сечения xi от t и xi от s.
И одного только распределения xi от t, хоть и во все моменты времени t, в том числе и s,
недостаточно знать для вычисления этой характеристики.
Нужно двумерное распределение знать.
Да.
Так, ой.
Нет, нет, нет, здесь xi.
Слушайте, тут что-то всё вообще неправильно.
Тут xi, а здесь s.
О, да, спасибо.
Это у нас было там, когда взаимное корреляционное, а здесь нет.
Значит, x, y, m, xi, t, s.
Сейчас всё правильно.
Ну вот.
Всё, все вот эти вот понятия мы ввели.
Есть ли пока какие-то вопросы?
Ну, пока всё достаточно тривиально.
И по большому счёту мало чего нового пока мы узнаём.
Вот пока только корреляционная функция нового для вас понятия.
Так, ну хорошо.
Основные численные характеристики мы ввели.
И будем потом с ними всё время работать.
Теперь переходим к большой теме новой, которую мы сегодня рассмотрим.
Поассоновский процесс.
Это первый процесс, такой содержательный, который мы с вами изучим, рассмотрим.
И он очень важен, потому что он встречается в приложениях, в многочисленных приложениях.
И мы будем очень много к нему возвращаться, о нём говорить.
И мы его будем изучать и на первом задании сейчас, первые лекции.
И на втором задании у нас он ещё будет.
У него огромное количество свойств.
Он очень много где встречается.
Поассоновский процесс.
Но сначала я начну с некой вводной.
Поассоновский процесс.
Начну с некой вводной.
Давайте мы представим себе ось времени и какие-то события, которые могут наступать.
И нас интересует, сколько событий произошло на участке времени.
Вот допустим у нас есть ось времени t, здесь у нас ноль, здесь у нас момент t.
И нас интересует, сколько событий произошло на этом интервале времени.
То есть время идёт в какой-то момент.
В какой-то момент раз произошло событие, потом снова пустота, раз снова произошло событие, снова пустота и так далее.
И вот этот счётчик событий мы здесь обозначим за kt.
Так что этот счётчик событий, у него график его траектории, он следующий.
Сначала, допустим, событий не было в нулевой момент времени.
Поэтому до следующего события, пока оно произойдёт, здесь будет ноль событий.
Потом событие какое-то произошло, неважно какое произошло, и счётчик наш скакнул.
Ну, допустим, на единицу скакнул.
Потом снова пустота, снова ничего.
Потом снова что-то произошло в момент t, он снова скакнул.
Ну, допустим, тоже на единицу.
Так что его траектории это какие-то ступеньки.
Кусочно постоянные не убывающие функции.
Возвращающие.
Вот счётчик.
На него можно смотреть, как на счётчик событий на некотором интервале.
Теперь давайте представим себе следующее.
Что мы не просто какой-то произвольный счётчик будем рассматривать, не просто произвольные события.
Давайте мы некоторые предположения ведём о ситуации, с которой мы имеем дело.
Давайте мы предположим, что сколько событий произошло здесь, не зависит от того, сколько событий произошло здесь.
Вот сколько событий произошло, это случайная величина.
И мы будем считать, что на непересекающихся интервалах времени вот это число событий произошедших, они как случайные величины независимы.
Ну и вообще, если мы возьмём много интервалов непересекающихся по времени и получим множество случайных величин,
которые равны сколько произошло событий на этих интервалах времени, пусть все эти случайные величины независимы в совокупности.
То есть самую сильную такую, возьмём независимость.
Вот, то есть будем считать, что k от там t и плюс один минус k от t и для всех i, для любых t итых,
такие, что вот эти вот интервалы не пересекаются, пусть все они независимы в совокупности.
Вот это множество независимо в совокупности.
k начинается в нуле, как я сказал до этого.
Ещё давайте предположим, что то, сколько событий произошло на интервале,
ну допустим t плюс h минус kt, зависит только от длины этого интервала.
То есть сколько событий произошло здесь, как случайная величина, её распределение,
не зависит от того, где вы этот интервал возьмёте, лишь бы он был одинаковым.
То есть если вы возьмёте интервал и возьмёте его фиксированную длину и рассмотрите вот этот участок времени,
то случайная величина, сколько здесь произошло событий, она имеет такое же распределение, как здесь.
Это всё наше предположение, давайте так предполагать.
То есть это однородность.
То есть пусть вот эта вещь зависит только от h.
Это то, что называется однородностью процесса, то есть не важно, где мы его рассматриваем.
Итак, мы рассматриваем счётчик каких-то событий, происходящих в разные случайные моменты времени.
Вот такой, что он начинается из нуля, он однороден, не важно, где вы его рассматриваете,
сколько произошло событий, не зависит от участка на этом интервале времени.
И пусть сколько событий произошло здесь, не зависит от числа событий, произошедших здесь, здесь и так далее.
Вот мы рассмотрим такую модель.
Такие предположения очень естественные, обратите внимание.
То есть такие простейшие, естественные предположения о потоке событий, которые происходят, которые разворачиваются во времени.
И мы ведём с вами ещё одно предположение, которое значительно сузит наше рассмотрение.
С одной стороны, а с другой стороны оно всё равно будет оставаться достаточно общим.
Мы будем считать, что вот это приращение, это не просто какая-то случайная величина,
а то, что она является полосоновской случайной величиной.
То есть пусть это полосоновская случайная величина.
Давайте я вам сразу напомню, что Xi имеет распределение полосона с параметром лямбда.
Это означает, что она принимает значение 0, 1, 2 и так далее до бесконечности.
И вероятность вот этого события, это есть E в степени минус лямбда на лямбда в степени k, на k факториал.
k равняется 0, 1, 2 и так далее.
Где лямбда это параметр, лямбда больше нуля, который называется ещё интенсивностью.
Вот мы будем с вами предполагать, что вот эта случайная величина имеет полосоновское распределение.
Поэтому предположение зависит только от h.
Так что вообще говоря, её аргумент, а лямбда, который там стоит, зависит от этого h.
Вот, да.
Что именно будет зависеть? Её распределение будет зависеть от разности t и плюс первого и t итого.
То есть, все вот эти приращения, эти случайные величины со своими какими-то распределениями, которые зависит от этой разности,
если разности разные, то их распределение тоже разное.
Ну а это свойство говорит о том, что они независимы.
Оно не говорит о том, как они именно распределены, о том что все они в совокупности независимы.
Вот. О том, как именно распределены мы говорим вот здесь, когда я говорю, что вот эта вещь,
во-первых, зависит только от аж, а во-вторых, давайте предполагать, что это поассоновская
случайная величина. Вот где у нас, кстати, впервые появляется распределение. Потому что без
распределения, если мы его нигде не ведем, мы ничего не получим. Где-то оно должно быть
введено. Вот это первое место, где оно появляется. Почему вдруг мы говорим о поассоновской случайной
величине? Ведь много есть распределений. В общем-то, вот этот выбор поассоновского распределения
для приращения, он естественным образом возникает в приложениях. И это основано на так называемом
законе редких событий и теореме поассона, которая у вас должна была быть по теории вероятности.
Она, грубо говоря, говорит о том, что если у вас есть очень много событий, которые могут
происходить, а могут не происходить, и вероятность того, что они произойдут, очень маленькая, но этих
событий очень много, тогда вот эта случайная величина, которая равна тому, сколько событий
произошло, вот она имеет распределение близкое к поассону. Формально говоря, это означает, что если
у вас есть набор бернулевских случайных величин, n штук, и они имеют распределение
вернули p,n, причем с ростом n, их вероятности того, что они происходят, вот эти p,n, они стремятся
к нулю, но стремятся к нулю так, что n, p,n стремится к некоей лямде больше нуля, то есть у вас количество
случайных величин растет, вероятности того, что они станут равными единицами падает, и вот это n
рост и падение p связаны вот таким образом, то тогда вот эта случайная величина, сумма, то есть сколько
произошло, ведь это нолики единички, их сумма, это сумма всех единичек, то есть сколько всего
произошло событий, вот эта вещь, она сходится к случайной величине, которая имеет распределение
поассон с вот этим параметром лямда, который вот здесь фигурирует, который является пределом n, p,n,
вот эта теорема поассона и закон редких событий, то что еще называется, вот эта ситуация, когда у вас
чего-то много и это происходит редко, а вас интересует, сколько произошло всего этого чего-то, вот это
очень частая ситуация, ну начинают обычно примеры с физики, когда там радиоактивный распад, там
какой-то датчик ловит частицы, и частицы редко попадают на экран, и там что-то загорается, но
частиц очень много, и поэтому там сколько частиц там на каком-то интервале вы зафиксировали,
это поассоновская случайная величина, как показывают эксперименты, там раньше там значит число
телефонных вызовов из данного района, люди звонят куда-то редко, ну скажем так, ну не каждую секунду
вы куда-то звоните, да, вы звоните редко, но людей много, и поэтому сколько за заданный интервал времени
произошло звонков, это тоже поассоновская случайная величина, ну близка к ней, то есть это такая полезная
модель, там где-нибудь в IT технологиях, что это может быть такое, какой-нибудь сайт, люди на него
заходят редко, но людей много, опять же, и поэтому сколько людей зашло на сайт, это снова поассоновская
величина, то есть удобно приближать, мы сейчас не будем говорить о том, насколько вообще адекватна
такая модель в каждой конкретной ситуации, но я просто говорю, что вот такое часто бывает, можно много
таких вот ситуаций себе вообразить, когда чего-то много, это происходит редко, и тогда, если опираться
вот на этот закон редких событий, выходит, что сколько событий произошло, это поассоновская
случайная величина, вот поэтому и естественно вот здесь вот эту вещь рассмотреть как поассоновскую
случайную величину, то есть рассмотреть конкретно такую модель, итак, мы с вами имеем дело с потоком каких-то
событий, которые происходят в разные моменты времени, в случайные, и мы рассматриваем счетчик
числа событий на интервале 0t, начинаем его из нуля, пусть у него будут независимые приращения, пусть он
будет однородный, и пусть эти вещи распределены по поассонам, тогда то, с чем мы имеем дело,
называется поассоновским процессом, вот этот счетчик называется поассоновским процессом,
давайте я теперь все, о чем я здесь поговорил и рассказал, соберу в одном месте и дам уже
формальное определение, процесс k от t, k больше либо равно нулю, называется поассоновским, если первое
k от 0 равно нулю, ну кстати говоря, если мы подставим сюда 0, мы получим сечение, а сечение это
случайная величина, а как это случайная величина равна нулю, ведь случайная величина это функция,
она зависит от исхода, вот она что, для всех исходов равна нулю или для каких-то, вот для почти всех,
почти наверное, вот так вот, второе k от t это процесс с независимыми приращениями, то есть для
любых n там больше либо равных, не знаю чего сейчас подумаю, для любых t1 меньше либо равно t2 меньше
либо равно и так далее, меньше либо равно tn, значит k от t1, запятая k от t2 минус k от t1 и так далее,
k от tn минус k от tn минус 1, ну наверное n больше либо равно 2, логично взять, вот эти вещи независимы
в совокупности, вот так и третье, значит для любых t больше s больше либо равных 0, k от t минус k от s есть
полосоновская случайная величина, ну и мы с вами рассмотрим простейший вид полосоновского процесса,
когда здесь получается некая лямбда на t минус s, лямбда больше нуля это свободный параметр,
вот который однозначно определяет процесс, вот такую вот ситуацию рассмотрим, видите,
здесь распределение зависит только от разности между t и s, это полосоновская случайная величина,
так что это однородный, процесс однородный, вот это мы будем называть полосоновским процессом,
ну и, сколько там времени-то, сложно сказать, да, а нет, еще время есть, допережило, ну давайте
изучать свойства этого процесса, сегодня все оставшееся время будем изучать его свойства,
свойств у него огромное количество, на самом деле мы будем чуть ли не весь семестр изучать его
свойства одного только этого процесса, но оно стоит того, потому что это действительно вещь,
так, я вот может быть только вот эту вещь оставлю, пусть она тут где-нибудь написана, вот, то есть это
когда x имеет полосоновское распределение с параметром лямбда, ну какие свойства можно назвать,
значит, смотрите, которые следуют только из вот этих трех, трех свойств, трех вот этих вещей,
ну с чего начать, ну, во-первых, смотрите, ну, давайте так, свойства, свойства, свойства
полосоновского процесса, ну, k от t, это есть k от t минус k от нуля, да, потому что k от нуля ноль,
почти всюду, а вот эта разность, она имеет распределение полосона, вон какое, это полосон с параметром
лямбда t, t минус ноль, это t, вот, так что мы нашли распределение сечения, вот, а раз мы нашли
распределение сечения, одномерную функцию распределения можно записать для этой вещи,
мы можем найти все численные характеристики, которые связаны с одномерным распределением,
например, математическое ожидание k от t, если мы t зафиксируем, мы получаем случайную
величину вот с таким вот распределением, ну, какие у нее там свойства, давайте,
я тут напишу, математическое ожидание вот такой кси, это есть вот эта лямбда, и она, кстати,
дисперсия равна, вот, так что и здесь получается, что это есть то, что стоит под полосоном, у нас
под полосоном стоит лямбда t, обратите внимание, так что это лямбда t, и она равна дисперсии,
дисперсия k от t, значит, в среднем значения процесса растут, дисперсия тоже растет, вот,
дальше k от t, это по ассоциативной величине, по ассоциативной величине принимает значение 0,
1, 2 и так далее, значит, k от t принимает значение 0, 1, 2 и так далее, вот, более того, для любых t
больше s, вот это приращение тоже принимает, получается, значение 0, 1, 2 и так далее,
потому что она является по ассоциативной величиной, то есть, они все не отрицательны,
вот эти приращения, а это значит, что процесс всюду не убывает, каков бы вы интервал не рассмотрели,
приращение этого процесса на этом интервале будет либо 0, либо 1, либо 2 и так далее, но не может
быть отрицательным, значит, все траектории этого процесса растут, и так как они дискретны,
0, 1, 2 и так далее, не может быть промежуточных значений, но это значит, что траектории процесса это
какие-то кусочно постоянные, оно сюда идет, потом в какой-то случайный момент времени происходит
скачок, например, на единицу, потом постоянно какое-то случайное время, потом снова скачок,
допустим, тоже на единицу и так далее, вы, кстати, обратите внимание, что траектории у процесса это
кусочно постоянные функции, не убывающие, а математическое ожидание, непрерывная функция,
то есть аналог какая-то такая, вот математическое ожидание лямбда t, а сами траектории вот они
такие вот дискретные, то есть надо сразу же себе запомнить, что вид траекторий никак не связан
с видом математического ожидания там дисперсии, то есть по-всякому может быть, здесь мы видим,
что здесь ступенька, какие-то разрывные функции, какая-то разрывная функция траектория,
а мат. ожидания вполне себе хорошая, непрерывная, даже гладкая, сколько угодно дифференцируемая
функция, вот она в данном случае возрастает. Так, ну вот, что еще можно здесь сказать,
давайте попробуем вычислить корреляционную функцию этого процесса, корреляционную
функцию, значит, я вот здесь начну, корреляционная функция плацсонского процесса, то есть что мы
должны сделать, мы просто пишем определение, ну а что, мы же больше ничего не знаем, пишем
определение, мат. ожидания КАТ на КАТС минус мат. ожидания КАТС, ну давайте перерыв,
я потом досчитаю, я все равно не начал. Так, ребята, давайте продолжать,
возвращаемся к расчету, мы с вами вычисляем корреляционную функцию плацсоновского процесса,
вот это есть ковариация его сечений КТ и КС, которые мы будем сейчас считать, ну мы уже
мат. ожидания посчитали, вот это есть лямбда Т, это есть лямбда С, так что в принципе это мат.
ожидания КТ на КАТС и минус лямбда в квадрате ТС, вот, нам нужно посчитать только вот это,
мат. ожидания вот этого произведения, ну как мы будем это делать, в каждом случае это как бы
своя ситуация, хотя если процесс имеет независимые приращения, то тут подход он
единый, значит, давайте мы рассмотрим сначала ситуацию, когда Т больше С, тогда мат. ожидания КАТ КАТС,
мы можем записать так, мы можем вот сюда, где КАТ добавить минус КАТС, то есть минус КАТС плюс
КАТС вот к этому добавить, такой умный ноль, да, мат. ожидания КАТ минус КАТС КАТС и получается плюс
мат. ожидания К в квадрате от С, вот, поняли, что я сделал, потому что как зависит между собой
сечение, мы сейчас не понимаем, но мы зато знаем, что приращения независимы, поэтому для них
мат. ожидания произведения приращений равно произведению мат. ожиданий этих приращений, вот,
так что очень логично и удобно переходить к приращениям, вот здесь мы перешли, вот я перешел
к приращению КАТС, это на самом деле тоже приращение, это есть КАТС минус КАТ0 и получается,
что у нас есть интервал 0С и СТ, вот они не пересекаются, так что это приращения на
не пересекающихся интервалах, мы смотрим на пункт 2, они независимы, значит, мат. ожидания
произведения приращения равно произведению мат. ожиданий, вот, мат. ожидания вот этого
приращения, ну а что это за приращение, это же есть원 пла són с параметром лямбда t
минус С, а его мат. ожидания это лямбда на t минус S. умножить на мат. ожидания вот этого
приращения, а это есть по осон с параметром лямбда С, его мат. ожидание и лямбда С.
умножить на лямбдс плюс здесь мотожидание квадрата а мотожидание квадрата это есть дисперсия
плюс квадрат мотожидания правильно дисперсия ну так как это по ассо над лямбдс дисперсию мы
знаем это лямбдс вот это снова по ассо лямбдс мотожидание лямбдс и в квадрат возводим лямбда
в квадрате с в квадрате все вычислили так что получается плюс плюс лямбдс и плюс лямда в
квадрате с в квадрате вот это мы что вычислили это мы вот это мотожидание вычислили произведение
вот потом ну вот здесь у нас даже что-то сокращается уже здесь да лямбда в квадрате
с в квадрате сократиться с тем то есть мы можем сократить вот это здесь получится лямбда тс
лямда в квадрате тс плюс лямбдс когда мы подставим это сюда и вычтем лямда в квадрате тс то
мы получим что р к тс равняется лямбдс вот это лямбдс она единственная выживет здесь мы рассмотрели
случай т больше с но у нас здесь полная симметрия если мы рассмотрим с если с больше
т будет то тогда мы получим лямбдс то есть лямбдс когда у нас т больше с лямбдс когда у нас
т меньше с вот но если мы рассмотрим где-то равенство то в общем-то это тоже ничему не будет
противоречить хотя там у нас по сонот нуля будет нехорошо да ну отдельно можно рассмотреть
случай t равняется с давайте отдельно рассмотрим то есть а что такое отдельно смотрите р к т т а
мы же знаем что дисперсия правильно у нас было такое свойство для колоритационной функции а это
есть лямбда т так что на самом деле здесь можно где-то даже равенство написать как-нибудь вот так
вот ну и это можно написать в компактном виде смотрите вот какая буквы здесь стоит та которая
меньше здесь с меньше значит она стоит здесь ты меньше значит она стоит когда равенство неважно
так что это есть лямбда на минимум т и с это есть корреляционная функция процесса по осуна вот
рк т с это лямбда на минимум т и с эта функция она определена всюду на вот этом множестве 0
плюс бесконечность в квадрате вот то есть она если я нарисую здесь тест вот при всех вот этих вот
она определена во всех точках она рана лямбда минимум т с если график ее посмотреть трехмерный
получается до каждой точке составляется число пирамида такая будет вот пирамида у которой ребро
идет вот здесь максимально пирамида то что это пирамида кстати важно на какой-то какой-то
задачи это полезное наблюдение потому что когда считаешь допустим интеграл двукратный от этой
штуки то двукратный интеграл такой функции это объем объем пирамида легко посчитать это там вот
это на это и там на высоту тривиально считается просто обратите на это внимание через такое
практическое практически техническое замечание по поводу этой функции так ну вот мы вычислили
корреляционную функцию плацонского процесса вот значит ну едем тогда дальше вот эти свойства
которые здесь написаны они на самом деле однозначно определяют семейство конечномерных
распределений то есть пользуясь этими тремя свойствами можно найти функцию вероятность или
функцию распределения для вектора составленного из сечений этого процесса вот но я не буду это
делать в общем-то это технически такая большая работа я просто скажу как это вообще делается
смотрите тут все на самом деле тривиально мы не знаем как распределенные сечения между собой
собственно это наша задача это узнать находя вот это семейство распределений но мы знаем что вот
эти независимы и мы знаем как они распределены значит вектор составленный из сечений процесса
мы знаем его распределения потому что вероятность того что этот равен чему-то этот
равен чему-то и так далее равно произведению вероятности что это прав一 чему то это прав
на чему ты так далее потому что они независимы вот а чему равные вероятности для каждого
из них мы знаем потому что мы знаем это распределение так что мы знаем как распределяли
векторы из приращений. Ну а просто сечение процесса, это некое линейное
преобразование вот такого вектора. Ну и в общем-то перейти от одного к другому,
это чисто технический момент, он не принципиален. Главное, что тут надо
понимать, что если у вас задано распределение вот этого вектора для
произвольных n, произвольных t, то вы можете, воспользуясь этим преобразованием
линейным, перейти уже к распределению вектора. Эта операция, она вполне себе
однозначная, формула есть, все дела. Вот, то есть вот эти свойства, они действительно
однозначно определяют семейство конечномерных распределений. Я говорил
на прошлой лекции, что мы не будем задавать процессы как функции исхода и
времени, мы будем их определять через конечномерные, семейство конечномерных
распределений. Ну конечно, это не значит, что мы будем это семейство явно
выписывать каждый раз. Вот в данном случае достаточно задать такие вот
аксиоматические свойства, то есть задать процесс аксиоматическим образом. И
получается для него, что семейство конечномерных распределений по нему
восстанавливается однозначно. Вот, окей. Дальше едем.
Какие еще свойства мы можем получить и вообще какие свойства нам интересны
для этого процесса? Вот мы знаем, что он скачет. Он сначала постоянен, потом
скакнул. Постоянен, скакнул. А на сколько он скачет? На 1, на 2? Может ли он
скакать больше, чем на 1? На сколько он в среднем скачет? Дальше. Я говорю о
том, что скачки происходят в случайный момент времени. А как распределены эти
моменты времени? Их распределение каково? А вот эти интервалы между скачками, вот
эти длинные интервалы, они никакое распределение имеют? Являются ли они
зависимыми или независимыми эти интервалы? В общем-то много вопросов возникает
про этот процесс. И в принципе, только исходя из этих определений, можно дать
ответа на все эти вопросы. Рассматриваю их, правда, по отдельности. Отдельно
рассмотрим вопросы о том, насколько скачет процесс. Отдельно рассмотрим вопрос
о том, в какие моменты происходят скачки и так далее.
Но вот мне нравится больше другой подход. На самом деле, мы сейчас с вами
рассмотрим и докажем одну теорему, из которой вот эти все свойства, они сразу
одним махом следуют. То есть мы сразу же ответим на все эти вопросы, одним махом
доказав всего одну теорему. Вот. Эта теорема называется теоремой о явной
конструкции Буасоновского процесса.
Это первая, по-моему, наша с вами, да, первая с вами теорема, которую мы будем
доказывать. Она большая, мы будем ее доказывать все оставшееся время, наверное,
но
там много и технических деталей, о которых полезно вспомнить теорию вероятности тоже.
Так, теорема. Явная конструкция Буасоновского процесса. Значит, так, пусть
кси n это независимые случайные величины с распределением показательным параметром
лямбда. Вот. Ну, то есть, функция плотности f кси n, у них у всех одинаково, она
равняется лямбда на е в степени минус лямбда х, при х больше либо равных ноль. Вот.
Пусть дана последовательность независимых, ну, в совокупности, естественно, в
совокупности. Ну, я, если не говорю, как именно независимые, то в совокупности у
нас никогда с вами не будет попарных независимости, если мы говорим о
независимости. И вообще не только у меня, а в любых других книжках, учебниках, если не говорят,
о какой независимости идет речь, всегда подразумевают независимость в совокупности.
Сразу запомните это. Независимость в совокупности случайные величины. Вот. И обозначим, обозначим
s, n, sumo их до n. Вот. Ну и пусть для определенности s0 равняется нулю, потому что здесь у нас n от единицы,
нам будет удобно s0 задать как ноль. Вот. Тогда, тогда процесс вот такой, x от t, который равен
supremo при n больше либо равных единице n таких, что sumo k равно от единицы до n в секатах меньше
либо равна t, ой, а, ну да, то есть s, n меньше, меньше либо равно t, можно так, можно всяко написать.
Да, вот этот процесс, то есть тут вот s, n является по осоновским процессом с параметром лямда,
тем самым лямда, которая вот здесь в показательном распределении находится,
процессом с параметром лямда, вот, то есть его семейство конечномерных распределений совпадает
семейством конечномерных распределений процесса, которые определяется вот таким образом, раз
семейство конечномерных распределений совпадает, нам неважно с чем мы имеем дело, с этой конструкцией
или с той конструкцией, это одно и то же. С точки зрения нашего интереса вероятностными
свойствами. А я вам напоминаю, что нас только это интересует. Вот, мы будем доказывать эту теорему
доказательства. Значит, смотрите, скетч доказательства следующий. Как мы будем это делать?
Для того, чтобы доказать, что это полуслуженский процесс, а у нас кроме этого определения больше
ничего нет. Значит, нам надо что сделать? Нам нужно взять вот эту конструкцию x от t и просто
проверить все пункты в этом определении. Если они все будут выполнены, то тогда это полуслуженский
процесс. Как я говорил, вот эта вещь однозначно определяет семейство конечномерных распределений.
Все. Так что, если этот процесс удовлетворяет этим свойствам, значит, у них одинаковый
семейство конечномерных распределений. Значит, это полуслуженский процесс. Вот. Так что мы будем
для этого процесса проверять вот эти свойства. Так. Ну, там, в общем-то, тривиальные. Напустим,
если мы возьмем t равное нулю, x от нуля рассмотрим, здесь будет 0. Ну, тогда Sn должно быть равна нулю.
То есть, значит, самый наибольший n, когда это возможно, n равняется нулю. Значит, x от нуля
равняется нулю. То есть, вот этот первый случай, он тривиально следует из просто из определения вот
этой вещи. Вот. Так что с первым пунктом так-то понятно. Но самое сложное — это доказать вот эти
два пункта. То есть, проверить, что у такого процесса с супремумом каким-то. То есть, посмотрите, x от
t — это супремум, а нам нужно приращение того процесса. Приращение с супремумом, в общем, такая вещь,
это нетривиальная. Вот. Нам надо доказать, что они независимы в совокупности. И еще определить,
что приращение имеет вот это распределение. Значит, как мы с вами поступим? Как будем доказывать эту
теорему? Первое, что мы сделаем — это мы определим, как распределены snt. Я имею в виду не каждое snt
по отдельности, а вектор из snt сначала определим. Это будет первый шаг в доказательстве. А второе — мы
выпишем приращение вот этого процесса, составим вектор из этих приращений и найдем это распределение,
вектор из приращений, в терминах sn. То есть, выразим приращение x через sn. И, зная распределение
для sn, мы найдем распределение векторов приращения x. И мы увидим, что это распределение расщепится.
Вот эта большая вероятность, она расщепится на n произведений, как и должно быть. Потому что мы
доказываем независимость в совокупности. И каждый множитель там имеет ровно такое распределение,
какое надо. Пуассоновское с вот этим вот параметром. То есть, вот наш будет подход. Итак, ищем,
как распределен вектор из snt. Пишем вероятность, функцию вероятности для приращения x и выражаем
ее в терминах sn. И пользуемся вот этой нашей конкретикой. То, что sn это не какая-то произвольная
случайная величина, она получена в результате сумм вот этих величин. То есть, мы этой конкретикой
воспользуемся. И путем некоторых преобразований там, короче, сначала будет огромное выражение на всю
доску, потом она просто схлопнется и получится красивое короткое выражение в самом конце. Вот и все.
Вот это вся идея. То есть, идея сама по себе очень простая. Ну, нужно вот напрячься немножечко,
чтобы нигде не запутаться и провести от начала до конца, чисто технически. Итак, ну давайте по
порядку. Давайте мы не будем бросаться сразу в омут. Здесь много чего предстоит делать. Давайте
разберемся со snt. Значит, смотрите, кси, независимые в совокупности, имеют показательное распределение.
Сразу сумма, говорим, она по ирлангу распределена. Помним, да, ирланг, n лямбда — это распределение
суммы для вот этих кси. Окей. Как связаны ски с кси? Смотрите, ведь мы знаем, что они независимо
распределены и знаем их распределение. Поэтому, чтобы найти распределение ски, то есть, вектора из
с, нам хочется как-то связать их распределение с распределением кси. То есть, это все, что мы знаем.
Вот смотрите, если мы напишем s1, s2 и так далее, sn, вот этот вектор осмотрим, то он связан кси
линейным образом. Это будет единичка, нолик, нолик и так далее, единичка, единичек, нолик и так
далее, единичка, единичка, единичка, нолик и так далее, и так далее, единица, так далее,
единица и так далее, единица в самом конце. Вот. Согласны? Это то, как связан вектор
с вектором кси. Так. Отлично. Теперь смотрите. Вот этот вектор. Мы знаем плотности каждой компоненты.
Вон она там выписана. Они независимы. Значит, плотность всего этого вектора равна произведению
плотностей всех их. Так? Здесь осуществляется линейное преобразование. В теории вероятностей
была формула, по которой мы можем вычислить плотность этого вектора, если осуществляется
некоторое преобразование здесь. Кстати, не обязательно линейное. Вот. Ну, в нашем случае даже
еще проще. Здесь линейное преобразование. То есть, я напоминаю вот в таких вот квадратных скобках,
что, значит, если у нас... Сейчас. Тут мне самому не запутаться. Значит, если у нас есть вектор y
и он равнеется a на x, вот, то тогда плотность вектора y в точке y это что такое? Это плотность
x в точке в какой? Мы бы хотели написать x, но мы хотим выразить все равных y. Но x равен a в минус 1
на y, поэтому мы пишем a в минус 1 на y. И здесь единственное нужно умножить на модуль определителя
вот этого. Не терминант, а в минус 1. Вот. Мы будем пользоваться вот этим способом, вот этой формулой.
Так что... Давайте я там сотру. Особо не пригодится. Чего? Ну, теорема такая.
Что надо на модуль умножать. Ну, не быть модуля не может, потому что детерминант может
отрицательно наставить, а функция плотности не может быть отрицательной. Ну, это, конечно,
не объяснение, почему там модуль, но просто как бы модуль должен быть. Вот. Теорема такая,
можно посмотреть, как она доказывается. Там стоит модуль. Так, хорошо. Ну и давайте теперь найдем плотность
s. Нам же это надо найти. Плотность s в точках, допустим, x1 и так далее, xn. Вот. Это что такое? Это
есть плотность вот этих векторок си. Где? А в минус 1, в данном случае, а в минус 1 умножится
вот на этот вектор. Ну, то есть нам нужно выразить си через s, получается. Здесь у нас будет x1. Ну,
обратная матрица, она же какая? 1, все нули, минус 1, 1, все нули, 0, минус 1, 1, все нули и так далее. Вот.
То есть x2-x1. Смотрите на x как на, как на эти, как на наши s, как на наши суммы. Поэтому,
чтобы получить x2, надо x2 вычислить s1. Вот. Ну, хорошо. Вот. xn-xn-1. Вот. Это мы написали вот это.
Ну, детерминат этой матрицы равен единице. Детерминат обратной матрицы это единица разделит
на детерминат исходной матрицы. То есть это единица. Поэтому модуль детерминат а в минус 1 это единица.
И вот так вот связаны плотности с икси. Вот таким вот образом. Мы знаем, какова плотность икси. Это
есть произведение по всем и от единицы до n. Значит, чего? Лямда умножить на e в степени минус лямда.
Умножить на то, что тут стоит. Это x и минус x и минус первое. Вот. И нам не нужно забывать,
что наша плотность определена, когда этот документ больше либо равен нуля. Это равносильно тому,
что мы запишем вот это экспоненты умножить на индикаторную функцию того, что x больше либо равен нуля.
Если x отрицательный там ноль, x положительно это будет лямда экспонента. Поэтому мы вот тут
напишем индикаторную функцию x и t больше x и t минус первое. Вот. Таким вот компактным образом,
чтобы без этих фигурных скобок, если то там. Вот. Можно компактно записать это вот таким образом.
Вот. Ну а теперь что? Лямду выносим как лямду в степени n. Здесь у нас,
смотрите, произведение экспонента. Это экспонента суммы. Здесь телескопическая сумма. И выживают все,
точнее, умирают все кроме последнего, x и n. Получается e в степени минус лямда x и n. Вот. А здесь
произведение индикаторов, то есть все вот эти условия должны быть выполнены. И их можно
заменить на один индикатор. Индикатор того, что, давайте так напишем, ноль меньше x1,
меньше x2, меньше и так далее, меньше xn. Вот. Все вот эти индикаторы можно записать как один
индикатор. Вот. Вот мы нашли функцию плотности для s. Она имеет вот такой вид. Ну выглядит,
мягко скажем, не очень. Ну что поделать? Зато это правильно. Выглядит сложно, зато это правильно.
Так что нам предстоит с этим работать. Окей. Следующий шаг делаем. Теперь мы рассмотрим вот
такую вероятность. Мы рассмотрим вероятность. Значит, а, ну, кстати говоря, тут мы для удобства,
видите, тут у меня и от единицы, тут есть x0, а кто такой x0? x0 у меня здесь не было. Ну,
просто мы взяли x0, равный нулю. Просто для того, чтобы вот эти обозначения, они все были
совпадающими. Согласованными, скажем так. Чтобы или симметричными, чтобы одинаковая, одинаковая
запись была. Теперь мы рассмотрим вероятность того, что x от t1 равняется, так, вот я не помню,
как тут проще. Нет, тут, наверное, через k. Да, вот так мы запишем k1. x от t2 минус x от t1
равняется k2 минус k1 и так далее, до x от tn минус x от tn минус 1 равняется kn минус kn минус 1.
Вот мы рассмотрим вот такую вероятность. На самом деле, мы могли здесь написать какой-нибудь k1,
k2, k3 и так далее, до kn, где они произвольные, принимают значение 0, 1, 2 и так далее. Но,
по-моему, нам будет удобно, если мы введем вот эти значения сюда именно таким образом. Но
только потребуем, чтобы больше a был больше, чем меньше k. То есть, чтобы было 0, меньше либо
равно k1, меньше либо равно k2, меньше либо равно и так далее, меньше либо равно kn. И пусть 0 это
будет наш k0. Нам тоже будет удобно такого обозначения вести. И мы рассмотрим вот эту вещь
для произвольных k, произвольных n, вот таких. И нам нужно показать, что вот эта вероятность равна
произведению вероятностей вот этих вот равенств, которые написаны здесь. То есть, что оно расщепится
все. И более того, нам надо показать, что вероятность приращения, она там такая какая надо,
чтобы было поасоновское распределение. Вот. И давайте мы вот что сделаем. Мы возьмем вот это
событие, которое здесь написано, и выразим его в терминах s. Сум. Вот этих именно s. Ну,
как мы это сделаем? Я вот тут сотру. Это нам уже не нужно абсолютно. Ну, смотрите, тут все просто.
Начинаем от нуля. Теперь смотрите, что означает, что xt1 равняется k1. Это значит,
что вот этот supremum равен k1. Это значит, что максимальный номер в этой сумме, чтобы оно еще
не превосходило t, это k1. Если мы возьмем k1 плюс 1, вот эта сумма станет больше t. Понятно? И мы уже
выйдем за нее. Так что вот это событие xt1 равняется k1. Оно означает вот что. Что у нас здесь есть t1,
и что у нас сумма, которая отвечает k1, она меньше t1. А вот следующее за ней sk1 плюс 1. Плюс 1 к k1
прибавляется. Не к индексу единицы прибавляется, а к этому k прибавляется. Вот она идет дальше. Вот.
Обратите внимание. Вот если вы этот момент поймете, то все нормально. Обратите на это внимание.
Супремум берется. А вот тут стоят оценки. Вот это sn. Здесь максимальный номер, что он не
превосходит t. Значит, а мы говорим, что xt1 равняется t1 равняется k1. Значит sk1 меньше ли бы равен t,
а вот следующий за ним s уже будет больше равен. Значит, наибольший номер равен k1. xt1 равен k1.
Вот. Значит, у нас все вот эти sk первые, они лежат вот в этом интервале, а следующий за ними
лежит здесь. Дальше. Вот на это посмотрим. xt2 минус xt1. Он равен k2 минус k1. Вот. Это означает,
что у нас все sk до sk2 лежат до t2, а следующие за ним это уже k2 плюс 1. Вот. Это уже событие,
которое заключается в том, что xt1 равен k1, а xt2 минус xt1 равен k2 минус k1. Вот. Но это еще
равносильно тому, что, видите, если xt1 равен k1, мы можем вместо него сюда подставить, сократить,
и получить, что xt2 равняется k2. И логика та же самая, что до k2 они все сюда входят, а вот следующий
за ним уже в этот интервал не входит. Он лежит правее от t2. Ну и так далее. И так далее до конца.
Так что вот эта вся вероятность, вот эта вся вероятность равно. Смотрите, чему это равно все.
Это есть вероятность того, что все вот эти s попали вот сюда. Я вот так в фигурных скобках
нарисую. s1 и так далее, sk1 принадлежат интервалу 0, t1. Они имеют непрерывное распределение. Это
неважно. Принадлежит, точка не принадлежит, это неважно. Это все непрерывное распределение имеет.
Поэтому я не обращаю внимания здесь, какую надо квадратную скобку делать или круглую. Это вообще
неважно. Вот. Запятая следующие за ними sk1 плюс 1 и так далее до sk2 принадлежит t1, t2. Запятая
и так далее. Ну вот тут, кстати, очень сложно. sk с индексом n-1 плюс 1 и так далее до skn принадлежит
от tn-1 до tn. Вот как я и обещал, мы выразили вероятность, связанную с превращениями
x в терминах s. А у s мы распределение знаем. У вектора s мы распределение знаем. Значит,
мы знаем распределение s1, s2 и так далее до skn. А здесь нужно вычислить вероятность,
связанную с этими s-ками. Так что понятно, что раз мы знаем распределение s-ок и нам
нужно найти вероятность, связанную с ними, ну как-то это ее посчитать можно. Все. Дальше дело
техники. Вычисления вот этой вероятности. Все самое идейное мы уже сделали. Давайте теперь
вычислять вот эту вероятность. Это я могу стирать? Так. Ну что, давайте мы будем вычислять эту
штуку. Ну, вероятность. Вот такая вот. Что такое вероятность? От того, что некий случайный
вектор s1, s2, skn попал в какую-то область. Это есть интеграл по этой области от плотности. Так?
Интеграл от плотности по области. Так. Ну вот будет такой большой-большой интеграл. Многократный.
От чего? От плотности s в точках x1 и так далее, xn. А, смотрите, у нас сколько тут? Kn штук. Kn. О как. Здесь
у нас будет стоять dx1 и так далее, dxkn. И по какой области? А вот тут она написана области. Значит,
у нас теперь переменными являются иксы. Что x1 и так далее xk1 попал вот сюда. Что xk1
плюс 1 и так далее xk2 попал вот сюда и так далее xkn минус 1 плюс 1 xkn попал вот сюда. Во! Вот мы
вот по этой области интегрируем вот эту плотность. Ну что, страшно? Мне тоже, потому что я не помню,
что делать дальше, но мы сейчас сообразим. Я помню, что только там все на самом деле не сложно. Так. Ну
что нам, что нам, собственно, надо делать? Надо воспользоваться, наконец-то, конкретикой. Конкретика
у нас для PS. А, я ее стер, да? Так. То есть нам нужно взять вместо... Ну давайте так, я напишу интегралы.
Вот тут вот переписывается вот это. Вот. И какая у нас там была плотность? Вот черт. Значит там лямбда.
Колька элементов это kn на e в степени минус лямбда. Последний из х, да? Последний из х. Так. И на
индикатор того, что 0 меньше x1 меньше и так далее меньше xkn. Вот. dx1 и так далее dxkn. Так. Вот. Ну вы
там проверьте. Я же писал формулу для плотности, по-моему, такая. Так. Ну вот. Хорошо. Только сейчас,
я, по-моему, все-таки что-то забыл. Ща-ща-ща-ща. Да. Я еще одну вещь забыл в область дописать. Смотрите.
Это касается tn tknt момента. Вот у нас там xtn равен xknt. Так. Это означает, что вот
сюда попал xknt, а вот сюда попал xkn плюс один. Для того, чтобы это supremum был. То есть это
должен быть максимальным. Значит следующий за ним должен превышать. Поэтому вот сюда нужно
будет еще кое-что добавить. Что skn плюс один превышает tn во. Вот тогда xtn будет равен xkn. Вот так. Так что
вот здесь нужно будет добавить xkn плюс один больше чем tn во. И вот теперь будет нам гораздо проще
вычислять. Все вот это дело. Так. Ну хорошо. Так. Ну все. Начинаем упрощать. Хватит. А то у нас тут
что-то все растет и растет. Да. И не видно этому ни конца ни края. Давайте упрощать. Значит вот это все
равно. Это равно. Я вот тут вот подальше начну. Вот тут вот. Теперь смотрите. Как упрощать будем?
Ну. Во-первых. А кстати. А у меня что-то нету. xk. Сейчас-сейчас-сейчас-сейчас. xk. n плюс один у меня
еще нету. Да. Видите я сюда добавил skn плюс один. skn плюс один. Значит здесь надо идти dkn плюс один.
И здесь идти dkn плюс один. Сколько у нас этих s, столько нам нужно x и всего. Так. И вот здесь давайте
сделаем плюс один. И здесь сделаем плюс один. Так. Вот. Вот теперь точно правильно. Мысли. Уже все.
Ух ты мой. Так. Ладно. Тогда ускоряемся. Да. Значит смотрите в чем суть. Ладно. Я тут не успел. Но
смотрите в чем суть. Это уже чисто технические детали. Во-первых. Вот этот. Когда мы интегрируем
вот по этой области. Обратите внимание. Мы производим интегрирование по этой области. Вот это
больше tnt. Значит xn плюс один точно больше всех остальных x. Потому что эти лежат здесь. А эти
и подавно меньше. Эти меньше tnt. А он больше. И раз мы интегрируем по такой хитрой области. Это
означает что знак меньше здесь нам на самом деле не нужен. Мы его можем просто выкинуть. Потому
что интегрируя по этой области. Уже заведомо у нас будет так что это больше всего остального. Так
что мы просто можем взять и убрать это отсюда. Из этого индикатора. И смотрите тогда что получается.
Вот эта вещь зависит только от xkn плюс один. И вот у нас дифференциал. Мы можем этот интеграл
вытащить вовнутрь. И проинтегрировать. И у нас останется что-то внутри. Интеграл от чего-то.
Вот давайте я напишу. kn плюс один. Значит там получается у нас интеграл. От чего? От tn да
плюс бесконечности. Это вот эта область наша. Вот e в степени минус лямбда xkn плюс один. Вот dxkn
плюс один. И умножить на интеграл от всего остального. И индикатор. Ноль меньше x1 меньше
и так далее. Меньше xkn. Вот так dx1 и так далее dxkn. Ну вот вытащили. Ну вот это выражение там
можно преобразовать. Посмотрите в пдфках. Тут интеграл экспоненты простейший. Теперь вот эта вещь.
Теперь смотрите. Для расчета вот этой вещи мы можем воспользоваться той же самой идеей. Какой
мы воспользовались, когда мы от этого избавились. Если вот эти иксы больше tn минус один, то они
точно больше, чем все остальные иксы. Поэтому из этого индикатора соответствующий знак меньше можно
выкинуть. Мы уже интегрируем по нужной нам траектории. И вот этот индикатор с этим меньше лишним,
он нас не ограничивает. Так что вот весь этот набор иксов можно поделить на несколько частей вот
этих вот. Вот эти иксы потом. То есть индикатор то, что эти иксы упорядочены. Умножить на индикатор,
что эти иксы упорядочены и так далее. И никак эти индикаторы между собой уже не взаимодействуют.
Знак меньше между ними мы убрали, потому что мы уже интегрируем по нужной нам области. Так,
вот этот индикатор распадется на произведение индикаторов с иксами, которые не взаимодействуют. Так
что весь этот интеграл является произведением интегралов по этой области, по этой, по той,
по той и так далее. Вот это первая идея здесь. То есть вот этот индикатор, он расщепляется на
произведение индикаторов, на группы. Дальше, дальше-то что? Мы должны будем проинтегрировать вот
по такой области, где все иксы упорядочены. Если бы они не были упорядочены, тогда интеграл по
этой области от единицы, это просто объем этой области, а это параллелепипед многомерный. Так
что это будет t1-0 в степени k1. Объем этого множества t2-t1 в степени k2-k1-1. Видите,
вот они наши разности появляются, которые потом будут у нас в поассоновском распределении находиться
и так далее. Это если бы не было индикатора. А у нас индикаторы с упорядоченными иксами. Это значит,
что мы рассматриваем не параллелепипед, а симплекс в нем. И объем этого симплекса равен объему
параллелепипеда разделить на факториал пространства. То есть получается, что интеграл по вот этой области
с упорядоченными иксами. Это есть объем этого пространства как есть параллелепипеда, то есть t1 в
степени k1 и разделить на факториал пространства k1 факториал. Чувствуете t1 в степени k1 разделить на k1
факториал. Вот оно. Вот оно. Откуда вот эти факториалы в знаменателе потом для поассоновского
распределения возникают. Все. Это вся идея. То есть мы отщепили вот этот. Мы пришли сюда,
расщепили этот индикатор на произведение индикаторов, потому что у нас область такая. И
дальше заметили, что то, что мы получили, это просто объемы симплексов вот в этих вот параллелепипедах.
Объем, формула объема мы знаем. Так что получается так, что мы получим некоторое выражение, вот уже
конечное, которое зависит от t1, t2 и так далее и k1, k2 и так далее. И вот это выражение, оно будет
из себя представлять не что иное, как произведение вероятностей приращения для каждого x в отдельности
и для k. Так что это будет как бы доказывать вот этот второй пункт, что они действительно независимы
в совокупности. Вот. Ну и так как там вот вылезут вот эти вот факториалы и так далее, выражение,
которое мы получим, это будет поассоновское распределение. Так что мы автоматически докажем
на самом деле и третий пункт тоже. Вот. Ну, пожалуй, на этом все. Да, вот я немножко не успел, но
дальше идут чисто детали технического плана. Мне кажется, что вы способны посмотреть просто мои
ПДФки, где очень аккуратно вот эти все интегралы выписаны. В общем, там двух действий только не
хватает. Это вы посмотрите. А идейно, что потом с этим делать, мы поговорим уже на следующей лекции.

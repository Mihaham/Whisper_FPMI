Невязки вот в такой норме бывают, вот, из корень
из эм большой, да, минус корень из эм маленькая, в степени к, норма начальника невязкая, вот, и что такое маленькое, это границы спектра
все собственные значения матрицы
эм большой
прекрасный результат, видно как, что управляет сходимость
распределение собственных значений, эту оценку можно уточнять если ее сформулировать в терминах индивидуальных собственных значений
вот, но тут, оценка сделана более грубой, но зато для целого класса матрицы
тут не важно какая эрбитовая матрица, лишь бы вот эти маленькое и эм большое были фиксированы, и тогда вот эта оценка имеет место на любой такой эрбитовой матрице, для любой начальной точки
ну в конце лекции просто я еще и задачку вам предложил, на самом деле число итераций зависит от начальной невязки
вот, если начальная невязка раскладывается лишь по, ну скажем, с собственным векторам, то за эсатерацией точно и решение будет
просил я вас это доказать, ну надеюсь что вы сделаете, вот вам задачка
ну можно и дальше еще сказать, а если начальная невязка раскладывается по таким собственным векторам, что собственные значения лежат в интервале от эм маленького до эм большого
ну где эм маленький может быть больше, а эм большое меньше, но тогда эта оценка тоже будет верна, то есть на самом деле, значит все здесь, извините, смотрю
да, Наташа, сейчас буду занят, добрый день, нет, сегодня ни в коем случае не приеду, нет, я по четверкам не бываю
ну можно там нарисовать, ладно, хорошо, спасибо, я прошу прощения
вот, для начальной невязки, начальная невязка просто, вот начальная невязка вы разложили ее по собственным векторам и смотрите какие там собственные значения
и там взять эм маленькое, большое, и эта оценка будет справедлива, да или нет, ну просто понятность с самого хода доказательства, будет именно такая оценка получаться
то есть есть еще такая зависимость принципиальная, конечно, от выбора начального вектора, как фишка легла, что он знает, как он будет выбран
ну здесь возникает вот какой вопрос важный, вообще-таки он для вычислительной линейной алгебы и для теории итерационных методов может быть один из главных вопросов
значит, ну наше общее желание такое, чтобы число итерации было поменьше, число итерации, нужной для достижения нужной точности, было поменьше, понятно?
и этого добиваться можно в принципе двумя способами, можно придумывать новые итерационные методы, а можно неким образом модифицировать уже известные итерационные методы
а можно преобразовать матрицу и применять уже известные методы, вот два пути
ну вы давайте, честно говоря, мне больше нравится второй путь, не плодить новых итерационных методов, хотя эти тоже можно менять
а вот имея некий универсальный, хороший итерационный метод, например, для ермитовых матриц, метод с упражненным ингредиентом, пытаться преобразовать матрицу, ну то есть преобразовать систему, перейти к эквивалентной системе фактически
но так, чтобы для этой новой системы метод сходился быстрее, о чем речь?
ну что такое эквивалентная система?
вот, понятно, ну тут, опять же, кому больше нравится, есть две возможности
я могу вот так вот написать
ну, Б, слишком много букв Б, давайте
напишу букву П
ну П должна быть невыраженым
любую невыраженную взяли, ну получились они
значит это одна возможность, но есть и вторая возможность
написали вот так вот
ну тут понятно, если вы эту систему решили, то чтоб найти Х, надо
и найдено Y может быть
вот две матрицы, вместо матрица возникает матрица либо П умножить на А, либо А умножить на С
значит вот этот переход, переход от системы с матрицей А
системы с матрицей П умножить на А, или А умножить на П, принято называть предвуславливаемым
предвуславливаемым
значит почему, происхождение этого термина
ну вот если вы на эту оценку посмотрите, то ее можно переписать
вот этот умножить можно переписать вот в таком виде
вот то, что я обвел в кружочке, это есть число обусловленности нашей элементной матрицы
правильное число обусловленности нашей элементной матрицы
ну если считать, что М маленькое, это в точности самое большое, самое маленькое
то есть в оценке, эта оценка зависит на самом деле от отношения большого и маленького
то есть от оценки числа обусловленности
ну как шлом обусловленности новому матрицу, новой формы ромобра
оно у нас возникло, когда мы обсуждали влияние малых возмущений на решение элементной системы
то есть предусловить, как бы изменить число обусловленности
за счет умножения на матрицу П с левой или с правой
ну матрицу П надо выбрать получше
самый идеальный выбор, какой?
аминус 1
конечно, и личной матрице число обусловленности равно единицы меньше не бывает
одна это рад
но этот выбор практически невозможен
потому что если мы знаем аминус 1, то мы вообще-то и решение системы легко находим
если мы умеем его назвать на аминус 1
то нужноять какой-то компромисс
в каком-то смысле правильно думать, что предусловиватель есть некое приближение к обратной матрице
но в каком-то смысле
смысл добросет последний punto
в некотором смысле предусловиватель есть приближение к обратной матрице
еще раз подчеркиваю, этот смысл может быть очень широким, но нам бы хотелось, если исходить из вот
этой оценки, нам бы хотелось, чтобы поменьше новые числа выслушать. Но этого мало. Что еще нужно? Нужно,
если мы по первому пути идем и решаем такую систему, нужно, чтобы на матрицу П была
процедура умножения. Значит, А по определению мы имеем эффективную процедуру умножения.
Вот, чтобы применять метод сопряженных градиентов, нужна эффективная процедура умножения на П.
Ну и здесь тоже. Ну тут надо опять же, ну да. Ну, понятно, что я не вкладываю
математический смысл. Ну, приемлемо-быстрее. Матрица может быть остановически большой,
но есть какая-то задана программа, которая умножает матрицу на вектор. А сама матрица,
в виде массива своих элементов, отсутствует. Вся информация о матрице зашита в этой процедуре
умножения. Вот, для А задана некая процедура. Ну и желательно, чтобы аналогичная процедура
для П была, была бы еще более быстрая, чем процедура умножения на А желательно. То есть,
желательно, чтобы была вообще еще быстрее. Ну а с тем сравнивать. Ну, цену одну операцию вы знаете,
это у Атемовича, вместе с Громеджемином. Вот, конечно, идеальнее, если будет порядка,
порядка. Эна Пираса это идеальней, конечно. Хотя, довольно редко сама процедура умножения на матрицу
имеет такую сложность. Обычно она сложнее. Но есть еще один подводный камень, который, возможно,
что-то из вас уже заметил. А для каких матриц применяется метод попрыженного объема?
Для ермитовых положительных определенных. Вот смотрите, вот мы имели матрицу А. Ермитову
положительную определенную. И вот умножили, ну скажем, я не знаю. Вот перешли к матрице вот такой.
А что стало с ермитовскими? Говорю уж о положительных. Ну вот, допустим, мы П тоже,
будем тоже выбирать П. Ну, логично, потому что если это приближение к обратной матрице в каком-то
смысле, то обратная матрица тоже ермитовая положительная. Вот когда мы П умножим на А, ермитовость обычно теряет.
Если вот весь метричный матрик спрямнулся, результат будет вообще не симметричный. А вот здесь, давайте,
вот здесь полезен некоторый уровень абстракции, который, мне кажется, в наших построениях тоже был.
Да мы-то как говорили, вот матрица и есть естественные скалярные произведения. Ну, может быть,
вас учили на первом курсе или не знаю, что вот можно ввести совершенно произвольные скалярные
произведения. Можно определять самые разные скалярные произведения. Ну вот, и определить
для линейного оператора А понятие сопряженного оператора, он вот так вот обозначается, вот таким
равенством. Точно на пространстве ввели скалярные произведения, и вот линейный оператор со звездой,
который обеспечивает такую равенство для любых электрофон, называется сопряженным. Ну,
теперь сопряженный оператор будет зависеть от выбора скалярного произведения. Понимаете?
Изменим скалярные произведения. Сопряженный оператор изменится. Ну и можно говорить, значит,
если, как будет здесь пониматься, эрмитовость относительно скалярного произведения теперь
рвутова. То есть если сопряженный оператор совпадает с исходным оператором, в данном скалярном
произведении, то он называется эрмитовым. В данном скалярном произведении появляется степень свободы,
выбор скалярного произведения. А теперь смотрите, какое бы мы скалярное произведение не выбрали,
если в этом скалярном произведении у нас эрмитовый оператор, то мы можем запускать метод
сопряженного граммета. Ну при условии, что он еще положится определённо. Всё. Значит, теперь задача
предобуславливания. Вот я утверждаю, что оператор вида П умножить на А, он будет эрмитовым и положить
на определённом в некотором скалярном произведении. Его можно выбрать. А задача для вас, придумайте
такое скалярное произведение. Ну для его придумывания надо знать матрицы П и матрицы А. То есть вот
матрица П умножить на А является эрмитовой положительно определённой в некотором скалярном
произведении. А мой вопрос к вам в каком? Не, не, не. В каком скалярном произведении вот этот
линейный оператор умножение на матрицу П будет эрмитовым. Ну если это выполняется,
если линейный оператор, значит вот это и есть некий линейный оператор. Значит это и сопряжённый
линейный оператор. Относительно вот этого скалярного произведения. Вот, если звездой совпадается,
то есть на самом деле тот же самый оператор, то тогда такой оператор называется сам сопряжённый.
То есть теперь А это линейный оператор. Ну можно думать, что это оператор умножения на
матрицу А. Ну а со звездой здесь не будет сопряжённой матрицы. Это некоторый линейный оператор.
Вот. Распланировали и брали комплексное сопряжение. Но скалярное произведение было,
оно было естественное скалярное произведение. Оно было. Ну вот здесь вот можно на более
общем уровне эти понятия ввести. И это помогает. Потому что вот задачку вы всё-таки решите. Вот вы
можете в качестве предуславления брать любую эрмитовую положительную определенную матрицу. И при
этом имеется скалярное произведение, в котором умножение на матрицу П будет самосопряжённым
положительно определенным оператором. В некотором скалярном произведении. А вот в каком? Вот это
мой вопрос к вам. Ну а мы давайте продолжим дальше. Продолжим дальше. Значит, мое намерение сегодня
рассмотреть методы для другой задачи. Тоже весьма важной. Почисление собственных значений,
соответственно, второго. Есть такая задачка? Нет. Значит, попрошу с доски встретить.
Некоторые классические методы для вот этой задачи. Спектральные. Такие задачи нужны спектральными.
Здесь много постановок. Понимаете, если матрица астрономических размеров, то, конечно, а зачем
все собственные значения? То их много. Обычно нужна какая-то информация собственных значений. Ну,
например, какая-то локализация. Либо самое большое по модулю, или просто самое большое
собственное значение. Либо самое маленькое, либо ближайшее к заданному числу. Много постановок.
Ну, конечно, для матриц умеренных размеров и постановка такая обычная. Найти все собственные
значения тоже имеют быть право рассмотрено. Ну, и собственные вектораны можно интересоваться.
Значит, с чего начать? Вот есть одна идея. На самом деле, чрезвычайно такая распространенная.
Ее следы можно, если присмотреться внимательно, можно разглядеть в самых изощренных алгоритмах.
Вот что-то удивительно сложное. Смотрите на это слово и видите. А там, оказывается, вот эта простенькая идея обеспечивает.
Значит, давайте рассмотрим матрицу А. Пусть даже произвольную, но предположим, что ее собственные
значения такие, что лямбда 1 самая большая по модулю и оно отделено от остальных.
Ну, бывает так. И нам бы хотелось найти лямбда 1.
Ну, вот предложение. Давайте еще одно. Предположим, что все собственные значения просты.
Ну, предположим, что есть базис собственных значений, извините, собственных векторов.
Ну, это, конечно, ограничение, но с вероятностью единица комплексная матрица обладает.
Это правда не помогает в жизни. Это замечательно.
Ну, теоретически, значит, А запитая, лямбда запитая, она запитая.
Возьмем некий вектор Х, произвольный, и разложим его по базису.
Ну, плюс, я специально выделю вот этот самый первый вектор и его координаты.
Значит, Х можно взять любой, но, тем не менее, пусть координаты по первому собственного вектора не нулевая.
С вероятностью единицы тоже нет.
Давайте поддействуем на вектор Х к этой степени матрицы А.
Значит, что будет? Будет лямбда 1, степень К, а вот то, что здесь я напишу, я напишу так, это у маленькая лямбда 1, степень К.
Стоит некий вектор, но его элементы это у малая от лямбда 1, степень К.
Ну, ровно по вот этой причине всего отделенности, остальные что-то значение от самого большого.
Ну, а что такое? Вот такое шкалярное произведение.
Это лямбда 1, степень К, на К1, на Z1, на Л, плюс у малой от лямбда 1, степень К.
Ну, а теперь что? А теперь давайте вот такое отношение рассмотрим.
Ну, понятно, что... Ну, я приглашаю вас замечать, что это с лямбда 1, не с у малой от лямбда 1.
И сразу возникает идея метода, да?
Ну, умножайте, выберите вектор Х, умножайте его до матрицы А в степень, и вот последните на новый маттер.
И вот частные такие вычисляющиеся в пределе получатся с лямбда 1.
Даже скорость сходимости понятна какая, она поделится отношением 2 к лямбда 1.
Маленькая, вот быстро будет сходиться.
Вот такой метод простой, если угодно, ну, степенной метод, да, этот степенной метод.
Но, кстати, при его реализации, вот если вы прям так будете реализовывать, ну, так не получится,
вы не можете умножать на степенька, тут у вас может быть рост, быть цисел, да?
То есть надо как-то нормировать.
Ну, ясно, что можно... Нужно нормировать на каждой, на каждой итерации, вот, результат.
Но, тем не менее, вот довольно естественный простой метод, который можно применять,
можно придумывать обобщение, когда группа собственных значений возникает,
можно превратить вот эту идею в метод итерации подпроскратства,
тогда вы выбираете некую подпроскратство, нажать на матрицу А, подпросованную А,
и вот на этом подпроскратстве вы строите маленькую матрицу,
выходит ее собственные значения, вы берете их как приближение к собственному значению,
исходной большой матрице.
Важно, чтобы к нулю уходило.
Где?
К минус один. К минус один. Важно, чтобы прямо до один осталось, и остальное уходило.
Так и будет. Идея эта простая.
Вот эта идея возведения в степень так, чтобы на фоне чего-то все остальное уходило к нулю.
Да.
Нулямда один констант.
Ну вот теперь такое нетривиальное развитие этой идеи.
Вот авторы вот этого нетривиального развития даже категорически не согласны
с тем, что это есть нетривиальное развитие простой идеи.
Они склонны были утверждать, что это у них принципиально другой метод.
Это, конечно, правда.
Но как за сложными конструкциями иногда можно разглядеть следы
уже известной простой идеи.
Но это никак не умоляет значение более сложной конструкции.
И она действительно необходима более сложной конструкции,
потому что эта простая идея в таком виде, именно это ограничено.
Я хочу познакомить вас с одним из совершенно знаменитых
знаменитых классических методов вычислительного изменения.
В речь пойдет о QR-алгоритме.
Значит, этот алгоритм предназначен для вычисления
собственных значений и собственных векторов произвольной комплекции
матрицы умеренного размера.
Не слишком, но не астрономически.
Так, чтобы в память она помещалась.
Потому что она будет преобразовываться.
Сегодня ни одной девочки нет.
Почему я об этом подумал?
Можно было бы не спрашивать, почему я об этом подумал.
Но в связи с QR-алгоритмом есть причина.
Значит, один из авторов вот этого знаменитого метода
это Вера Николаевна Кувлановская.
К сожалению, ушедший уже.
Ленинградский математик, вычислитель.
Вот она.
Во всем мире известна была.
Во всем мире.
Значит, как один из авторов.
Ну, она и другими вещами занималась.
Как один из авторов QR-алгоритма,
о котором я сейчас вам хочу рассказать.
Ну, были и западные коллеги независимо действующие.
Но, тем не менее, ее вклад здесь признан.
А что представляет QR-алгоритм?
Ну, цель уже обозначена.
Что он должен делать?
Находить собственные значения и собственные векторы.
Вот берется исходная матрица.
Позже будет она.
Ну, то, что я сделаю на первом шаге,
будет выполняться потом и на втором, и на третьем.
Совершенно аналогичным образом.
И до матрицы А0 находится QR-разложение.
То есть, матрица Q1 унитарная,
а матрица R1 верхняя треугольная.
То есть, до любой комплексной матрицы
можно найти QR-разложение.
То есть, представьте, в виде произведения
витарной матрицы и верхней треугольной.
Каким образом это сделать?
Если хотите, можно провести организацию станций
методом грамошмита.
Но можно использовать вращение или отражение
на полуципе прообразований
и с их помощью преобразовать матрицу
к верхней треугольной.
Ну, так или иначе.
После того, как найдено QR-разложение,
вот эти множители ставятся в обратном порядке.
И это будет новая матрица A1.
Вот основной шаг QR-разложения.
Ну, давайте напишу еще второй шаг.
Какие-то чудеса, да?
И так далее.
Зачем делать такие преобразования?
И какое отношение эти преобразования
имеют к поиску собственных значений матрицы A?
Имеют, на самом деле.
Давайте на первый шаг посмотрим.
Значит, A1
это R1
на Q1.
Но я же могу вот так написать Q1,
а здесь поставить Q1.
Вот, скобочки здесь поставлю.
И в скобочках появилась матрица 0.
Правильно?
Ну, дальше мы продолжим аналогично.
Другая интересная формула.
Катой матрица
имеет вид такой.
Ну, этот матрицы обозначен через Z-кат.
З-кат и сопряженная,
Z-ката.
Ну, подождите.
Значит, что понятно, что матрица Z-ката
есть произведение унитарных матриц,
поэтому сама будет унитарная.
Значит, звездочка то же самое,
что для унитарной матрицы сопряжение
равносильно обращение.
Вот эти матрицы Акката,
которые мы будем производить в ходе коэр-алгоритма,
они все подобны исходной матрице.
Более того, они унитарно подобны.
Это значит, никакой неприятности связанной
с возможным ростом элементов нет.
Это будет неплохо вычислять.
Хороший вопрос, зачем мы это будем вычислять?
Вычислять-то можно.
Ну, наблюдение такое. Мы интересуемся собственными значениями.
Значит, накатом шаги мы получаем матрицу Акката,
а все остальное можем забыть,
потому что матрица Акката имеет те же собственные значения,
что и исходная матрица.
Ну, если мы для Акката и найдем собственные векторы,
то можно с помощью умножения на матрицу З-ката пересчитать
и получить собственные векторы исходной.
То есть мы на каждом шаге в коэр-алгоритм
делаем аналогичные задачи с матрицей Акката.
Но редукция задачи имеет смысл
только в тех случаях, когда мы думаем,
что новые задачи будут проще решаться.
Ну а для какой матрицы
собственные значения очень легко найти?
Ну, это уж мечта, конечно.
Если еще пошире классы.
Для треугольной.
Что там?
Вне главной диагонали все равно
на главной диагонали в треугольной матрице будет собственное значение.
Ну, у нас есть теория возмущений.
То есть можно так сказать, если матрица
слабо отличается от верхней треугольной,
то на главной диагонали стоят приближения их собственных значений.
Таким образом,
чтобы объяснить полезность
вот этих итераций,
нам надо понять, почему
вот эти матрицы Аккаты
по форме стремятся к верхней треугольной.
Ну, в данном случае к верхней треугольной.
Вот эти матрицы Аккаты
по форме стремятся к верхней треугольной.
Вот теория будет ровно такой.
Матрица Аккаты по форме стремятся к верхней треугольной.
Ну, что значит по форме стремятся?
Значит, все поддиагональные элементы стремятся к нулю.
Значит, вот
еще раз. Матрица Аккаты
все поддиагональные элементы стремятся к нулю.
А значит, на главной диагонали мы
с вами будем получать приближение их собственных значений.
Остановиться надо тогда,
когда вот эти поддиагональные элементы стремятся к нулю.
Ну, об основании тем не менее
сказано, но надо и сделать это.
Как-то, так сказать, увидеть, почему то, что я сказал верно.
Давайте попробуем.
Ну,
но
некий есть простой.
Ну, вообще-то говоря, когда это все создавалось
многостраничной даже работы писали, чтобы
объяснить то, о чем я сейчас говорю,
не сразу же приходит простой способ
обсуждения.
Но вот достаточно простой способ обсуждения придумал
классик в числительной линейной алгебе.
Это Уилкинсон.
Значит, у него есть замечательный труд
под названием «Алгебраическая проблема сотни значений».
Вот как, может быть, Гантмахер и так.
Вот такого же формата.
Это вот была долгое время.
Да, может быть, остается Библии классической
в числительной линейной алгебе.
Посвящено, кстати, анализу численно-устойчивости
алгоритма того,
как влияют ошибки округления.
Ну, у нас совершенно фундаментальная книга.
Но вот он
предложил
некий простой набор условий,
ограничений, при которых то, что я сказал
верно. То есть никто не говорит, что это в общем
случае верно.
Значит, теорема будет вот такая.
Значит, та
диагонализована.
Это уже ограничение.
Уже ограничение.
Значит, лямбда это матрица
сотни значений.
И они считаются
попарно разными.
Ну, давайте я это допишу
на отдельной строчке.
Ну, с лямбда
один самый большой.
Значит, это уже целых два предположения.
Целых два предположения.
Да я еще давайте предположу, что нулевого нет.
То есть матрица комплексная,
диагонализуемая. Все собственные значения
попарно различны.
Значит, можно их вот так и понять.
Вот третье предположение
кажется фантастическим.
Кстати, х
это матрица из собственных векторов.
Так вот обратная
к матрице из собственных векторов
имеет Элю разложение.
Это является строго регулярной.
Эль здесь унитриугольная матрица, а уни выражена
вверх.
Ну, унитриугольная
треугольная с единичками
на главной диагональ.
Значит, Эль нижняя унитриугольная
с единичками.
Ну, мы обсуждали с вами строго регулярно
необходимые достаточные, что Элю разложена.
Вот все три условия.
И вот если эти три условия выполнены,
то
в матрице Аккаде
любой элемент, для которого И
больше, чем Ж, то есть это подглавный
вот он
соединится к нулю, при карте
и начнется вот так.
И это мы попробуем доказать.
Это будет обоснование.
Такой раз горит, правда же?
Да?
Так.
Ну давайте, попробуем.
Ну давайте, у меня тут есть тоже вклад
в теорию куэрелгоритма.
К этому статью написал,
дал ей такой заголовок, который потом
по совету рецензента
изменил. Значит, статью я так
дал. Одно бесполезное замечание куэрелгоритма.
Бесполезное, я имел в виду, что оно практически
не имеет никакого значения.
А пафос того, о чем я тогда
подумал, заключается в этом третьем
условии.
А что если обратная к матрице
собственных вторых не является
строго регулярной? Тогда иллюразложений нет.
Ну я, значит, что сделал?
Вот была одна из задачек, которые я вам
предлагал. Ну какая бы не была
невыраженная матрица, мы же можем
лпо-разложение сделать, где p будет
определенно однозначно.
Кто-то из вас решал эту задачу.
Я в этой заметке в статье
доказал утверждение о том, что
вот в предельной матрице
собственные значения на главной
диагонали будут расположены,
их порядок будет определяться
при установке p.
То есть все то же самое, но будет
в нем порядок.
Ну ладно, эта лирика совершенно
бесполезна, потому что с вероятностью
единица условия строго регулярности
выполнена. То есть при этом вы даже
численно не проверите вот это мое
утверждение, потому что будут
возникать ошибки округления и все.
Ну давайте попробуем
посмотреть.
Ну и может быть разглядеть
предыдущие много методов.
Так.
Ну у нас есть
замечательная форма для матрицы
окатая.
Вот если мы сразу вот о следах
степенного метода.
Возведем нашу исходную матрицу
в степень k.
Ну это есть
q1 на r1.
q1 на r1 кора.
Или
это есть q1
на а1 в степени k-1
на r1.
Ну отсюда
потом мы для а1 в степень k
методам то самое повторим.
Значит возникает формула такая для матрицы
от степени k.
Вот такая q1
у катая, а здесь r1
и rk.
Ну вот эта матрица уже имела обозначение
z-катая, правильно?
А эту матрицу я обозначу через u-катая.
z-катая, как мы уже говорили,
это матрица унитарная,
поскольку является произведением унитарных матриц.
А вот эта матрица u-катая
наверхне треугольная. Почему?
Потому что она есть произведение
в треугольных матрицах. Что?
А в степени k.
То есть это есть
унитарная матрица z-катая
наверхне треугольная матрица u-катая.
Ну вот они следы степенного метода
в этой формуле.
То есть матрица в степени k
возникает
в другом порядке.
Ну как, взяли вот эту форму.
Дальше, а1 в степени k-1
написали аналогичную.
Возникло здесь q2 и r2.
В другом порядке, правильно?
Ну в другом порядке.
Ошибка все-таки была.
Ошибка была.
Для последней будет q2.
Ну, значит
Ну, заниматься надо матрице-катой.
Она нас волнует.
Вот матрица-катая.
Это есть
z-катая с звездой.
Заткатая с звездой.
Заткатая с звездой.
Заткатая с звездой.
Заткатая с звездой.
На а.
На заткатая.
Так, а для матрицы А есть у нас
вот такая вот форма.
Значит, это есть
z-катая с звездой.
x
х
х
х
х
х
х
х
х
Лямба это матрица диагональная.
Но, значит,
здесь x-1 на заткатая,
здесь обратная к ней.
Здесь обратная к ней.
Значит, вот эта матрица нас волнует.
Если верно то, что нам надо доказать,
то вот эта матрица,
x-1 на заткатая,
она стремится по форме
при сделанных нам предположениях.
При сделанных нам предположениях.
Вот надо только
сообразить, почему.
Ну, давайте вот этой матрице будем заниматься.
x-1
на заткатая.
Давайте заметим, что
когда заткаты унитарные матрицы,
поэтому вот вся эта пассета
равномерно ограничена.
Поскольку заткаты унитарные.
Согласны, да?
То есть все элементы этих матриц
ограничены по модуле с таким числём.
Большой, но ограничен.
Так, теперь
теперь
давайте
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
Это х-1 азоткатая. Куда смотреть то надо. А вот сюда. Азоткатая это есть афстепелика на укатой министерии.
Нигде не обманулся. Смотрите внимательно. Иногда не хочешь обманешь.
А вот теперь для катая воспользуемся вот матрица. Какой вид имеет афстепеликатая? Это х на лямбда степелика.
Вот так и напишем. Х на лямбда степелика на х минус первой укатая.
А вот здесь х-1 на х тоже сократится.
А вот теперь смело подставим. Вот теперь на иллю мы сюда поставим.
Ну неплохо. Тут уже следы верхних треугольных матриц имеются.
Вот здесь л нижняя треугольная. Но она умножается на верхнюю треугольную. Правда, если мы нижнюю треугольную умножим на верхнюю треугольную, результат будет...
Ну и теперь небольшой трюк. Совсем маленький.
Вот здесь мне понадобилось, чтобы нулевого значения не было.
Ну раз я здесь лямбда степелика написал, я здесь должен написать лямбда степелика.
Я обманул?
Да вон она есть, стоит между лямбда степеляка и лямбда степеля минуска.
Здесь ничего не исчезло, кое-что появилось дополнительно.
Появилось, что вот это лямбда степеля минуска и лямбда степеляка, чтобы они сократились.
Вот эта матрица вторая, она при каждом к верхняя треугольная.
Ну первая матрица нижняя треугольная, по-прежнему все печально.
Но печаль все уменьшается и уменьшается.
Значит я утверждаю следующее.
Вот эта вот матрица стремится к единичному.
Просто стремится к единичному.
Печаль исчезает.
Почему да?
Давайте посмотрим.
Давайте я нарисую.
Допустим третьего порядка.
Ну только здесь.
Давайте посмотрим, что получится в результате.
Здесь единичка.
Здесь лямбда два.
Удалить на лямбда один.
Это не к, а л два один.
Здесь единичка.
Верно?
Здесь лямбда три.
Нет.
Лямбда.
Лямбда три поделить на лямбда один.
Это не к, а л три один.
Здесь лямбда два.
На лямбда один.
Это не к, а л два один.
Здесь единичка.
Ну видно, что под дегональными элементами числа меньше единиц поможет.
Это реальность.
12 нулевых.
Ну я и утверждаю.
Ну а матрица не стремится к единице.
Нет.
Не верхняя треугольная.
Матрица нижняя треугольная.
На главный диагональ всегда единицы.
А все поддегональные элементы не стремятся к ней.
То есть верно, что эти матрицы сходятся к единичной матрице.
Таким образом, вот эта матрица,
х минус первый.
Она имеет вот такой единичка.
Плюс умалые от единицы.
Да вот такую вот матрицу.
Ну естественно можно переписать.
Вот такая вот матрица.
Ну и хотелось бы написать вот так.
Да?
Ну нужно понять, что нам дает право это сделать.
А?
Нет, пока не может.
Значит вот вопрос к вам.
Почему нет?
Ну вопрос о том, вот здесь матрица,
элементы которой сходятся к нулю,
умножается на вот такую вот матрицу.
Почему у нас так получится матрица,
элементы которой сходятся к нулю?
Правильно.
Потому что вот эта матрица равномерно ограничена.
Потому что вот эта матрица равномерно ограничена.
А почему она равномерно ограничена?
Давайте запишем.
Сейчас я напишу.
Нет, не потому что она имеет вид какой.
Вот эта матрица, которая равномерно ограничена,
умножить на обратную вот такой вот матрицу,
для которой можно ряд неймана написать.
Видно, что это будет равномерно ограничена.
Потому что ЗК унитарная.
ЗК унитарная.
Ее элементы по модуле на происходе видны.
А Х-минус фиксированная.
Так мы же все доказали.
Мы все доказали.
Потому что, раз вот эта матрица стремится к верхней треугольне,
обратная к ней тоже стремится к верхней треугольне.
Здесь, вот смотрите, вот эта к верхней треугольне стремится,
а эта обратная к этой матрице тоже стремится к верхней треугольной.
«Теорема о исходимости QR-агритма»
в предположениях у Ексензона эта goblet-коп door написана.
Доказано.
Ну и, конечно, с вероятностью единицы все эти предположения
выполнены.
На практике, если вы наугад берете матрицу, у меня все
собственные значения разные.
Ну потому что вы x можете выбрать всегда так, чтобы
обеспечить нужный порядок.
Несмотря на то, что теоретически это не ясное ограбление,
но практически, конечно же, если нарушены эти условия,
мы имеем неприятность.
Ну если лямбда 2 и лямбда 1 близки, там скорость сходимости
никакая.
На практике мы можем наблюдать медленную сходимость, и
это может быть неприятно, и реально это может быть.
Это правильно.
Так вот, придумано было, как коэр-алгоритм можно
ускорять.
Это существенный часть коэр-алгоритма.
Придуман способ его ускорять.
Сейчас я вам расскажу, это такая классика нашей науки.
Прежде всего, тут есть две вещи, конечно, есть скорость
сходимости, а есть еще просто цена одной итерации.
И то, и то важно.
Давайте вначале обсудим цену одной итерации.
Ну какая она, если порядок маты ЦН, то какова цена
одной итерации?
А?
Н куб, правильно.
Коэр-разложение будет стоить н кубе действия.
Потом надо коэр в обратном политике переносить.
Еще н кубе действия.
Значит, вот так как написано, цена одной коэр-этерации
есть н куб.
Но я вот люблю слово ортодоксально, то есть то, что я вам рассказал,
это есть ортодоксальный коэр-алгоритм.
И в таком виде он никогда не применяет.
Значит, можно снизить цену одной итерации.
Вот мне кажется, что это англичанин по фамилии Фрэнсис заметил.
Тоже интересные судьбы бывают.
Вот несколько лет тому назад, те классики, которые еще были живы,
они пытались этого Фрэнсиса найти,
потому что он отметился статьей, которую все на проповую цитировали
о коэр-алгоритме.
Значит, он там предложил, как снизить цену одной итерации.
Сейчас я вам расскажу, это вначале просто оказывается.
Ну а кроме того, еще одна идея.
Я обсуждал, как можно уменьшить само число итерации за счет таких манипуляций.
Вот эти две вещи.
Ну вначале, как можно снизить цену одной итерации.
Наблюдение очень простое.
Любую матрицу комплекции с помощью умножений слева и справа
на некоторые просто вычисляемые унитарные матрицы специального вида.
Ну, например, мог быть просто матрица отражения или перестановки.
Больше ничего не надо.
Можно привести любую матрицу, любая матрица унитарно подобна верхней треугольной матрице.
Знаете, какая она такая?
Любая матрица комплекции унитарно подобна верхней треугольной матрице.
Это знаменитая теорема Шура.
Вы не знаете?
Ну это плохо.
Любую матрицу можно найти в унитарной матрице КОТАК, чтобы получить верхнюю треугольную матрицу.
Она, кстати, довольно легко по индукции доказывается.
Можете испытать свои темы.
Ну начать с собственного вектора к.
нормированного.
Вы потом его достроите, свести задачу к аналогичности для матрицы.
И вот так вот собрать унитарную матрицу, ту, которую нужно получить верхнюю треугольную матрицу.
Вот здесь, конечно, собственное значение.
На счет теоремы Шура.
Который вы должны уметь доказывать.
Уверен.
Ну в Жардановом базе оно-то имеет некоторые отношения.
Но матрица кой-то унитарная матрица, а здесь всего лишь верхнюю треугольную.
Если вы напишете Жарданову матрицу, там вот эта матрица, там примерно неподобная.
Ну и матрица из собственных треугольных матриц, а тем не обязательно унитарная.
Может она такой в принципе не будет.
Это простая идея, это намного более простой результат.
Элементарно доказывают.
Элементарно доказывают.
Но с вычислительной точки зрения это все-таки как результат существования.
Но не способ вычисления.
Потому что если мы найдем вот эту верхнюю треугольную матрицу, то мы уже собственное значение получим.
Ну верно, но так не делают.
Ну в принципе вы правы.
Найдете в начале одно собственное значение, потом называется это словечком дефляция.
Вот сведение к задаче меньшего размера можно.
То есть это действительно метод некоторого права.
Но тем не менее, да, индуктивные доказательства Теоремы Шуров в принципе конечно дает метод некоторого.
Но я хочу вот какое есть наблюдение другое.
Значит можно, давайте вот кротчурку забудем, а вот здесь добавим еще под догонат.
Вот можно совершенно элементарно без всякой индукции доказать, что любую комплексную матрицу можно привести к почти треугольной матрице.
С помощью последовательности умножения на унитарную матрицу.
Сейчас я вам покажу как.
Ну пять хорошего числа.
Значит вот пять на пять.
И здесь будут элементарные преобразования.
Ну если там отражение, может быть корень надо будет извлекать.
В длину выходить вектора.
Больше ничего не надо.
Значит что делаем?
С помощью вот этого элемента.
Ну зря его в кружочках было.
Ну скажем, может быть вам проще, проще объяснить когда все вещественное.
Давайте объясню когда все вещественное.
В комплексном случае лучше преобразование отражения использовать.
А вот в вещественном случае можно и вращение.
Значит вот можно с помощью этого элемента получить здесь ноль.
Повращав, умножив на матрицу вращение.
Ну и аналогично и здесь можно получить и здесь.
Но мы должны выполнять преобразование конкурентно.
То есть мы должны справа выполнить те же преобразования со столбцами.
Ну давайте заметим.
Вот здесь меняются вторая и третья строчки.
Будут меняться вторая и третья столбцы.
Второй четвертый, второй пятый.
Вот эти нольные нули сохранятся.
Вот это преобразование подобия или унитарного подобия на самом деле.
Ну взамен можно скажем было бы использовать матрицу отражения.
Чтобы сразу здесь эти ноли получить.
Здесь в комплексном случае прекрасно.
И это элементарная операция.
Практически арифметическая операция.
Но можно надо его часто длиной вверх.
Дальше закрепляем успех.
Теперь с помощью этого элемента здесь вращение получаем.
А вот эти нули сохранятся.
Ну почти, но не совсем.
Сейчас.
Что-то нужно сделать.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Что-то мне не нравится.
Не в гауссе.
В гауссе мы с этого элемента начинаем.
А здесь мы не трогаем первую стручку.
Так.
Вот у нас.
Вот такая матрица.
И теперь мы с помощью этого элемента здесь ноль получаем.
Ну а здесь два нуля комбинируется.
С ним ничего не произойдет.
Получаем здесь два нуля.
Справа аналогичное преобразование.
И один из полученных нулей не испортит.
Ну наконец.
Вот.
Аналогично здесь получаем ноль.
Справа преобразуем.
Эти нули в соответствии.
Вот и все.
На диагональ никаких нулей ничего не будет.
На диагональ никаких нулей.
Ну а сейчас как у нас?
Там элементы другие.
Здесь семетра и не предполагается.
Здесь семетра и не предполагается.
Но вы нащупали одну интересную вещь.
Значит смотрите.
Значит что мы сейчас с вами увидели.
Вот можно такую
сэкономентальную фразу.
После того, что мы увидели.
При применении коэр-алгоритма
не ограничивая вводчивость.
Можно считать, что исходная матрица
и ясная и точная диагональ.
Мы можем вот эти преобразования выполнить.
Нужно всего им в кубе действовать.
И вот мы их выполним.
Получим матрицу, которая имеет
те же собственные значения.
Но она будет верхней почти треугольной.
И мы коэр-алгоритм
начнем применять к верхней треугольной матрице.
Вот так.
Вот так.
Вот так.
Вот так.
Вот так.
Вот так.
Вот теперь
как применять
коэр-алгоритм к верхней треугольной матрице.
Сейчас покажу.
К верхней почти треугольной.
К верхней почти треугольной.
Итак, вот эта матрица
А или А0
вот теперь она в верхней треугольной.
У меня какое-то такое предварительное преобразование.
Что б на это Фрэнсисе предложил
делать предварительное преобразование.
Но дальше еще кое-что надо заметить.
Значит, вот надо коэр-разложение
для этой матрицы выполнить.
Сколько надо действовать на это?
Чтобы вот этот ноль получить
в порядке nb.
Не n куба, а n квадрат.
Правильно.
n квадрат.
А потом в обратном порядке перемножить.
Но это будет равносильно тому, что вот эти
нули опять станут нигулями.
Значит, наблюдение
на одной итерации коэр-разгоритма
верхний почти треугольный вид
сохраняется.
То есть, если матрица А была,
а ноль была в верхней почти треугольной,
то А1 тоже останется в верхней почти треугольной.
Есть реализация коэр-этерации.
И такая, что
на итерациях
будет поддерживаться
верхний почти треугольный вид.
То есть, этот вид
инвариантен относительно коэр-этерации.
И как вы справедливо заметили,
цена n квадрата.
Значит, было n куб.
Н куб, да, стало n квадрат.
Чудесно.
Чудесно.
А теперь вот я
хочу переосмыслить
замечание, которое вы сделали.
Вы это неправильно заметили, конечно.
А если матрица Ермитова,
то вот тогда
то, что вы сказали, будет правдой,
тогда и сверху будут нули получаться.
То есть, если вот
то же самое преобразование
мы будем применять к Ермитовой матрице,
то в результате получится матрица
не только верхней почти треугольной,
но еще и нижней одновременно
почти треугольной.
То есть, она будет
трех диагонами.
То есть, Ермитова почти
треугольная матрица
она вот приводится
к Ермитовой треугольной матрице.
к Ермитовой треугольной матрице.
Что?
Ну, ну лиспу.
Ну, лиспу.
То есть, в Ермитовом случае нет.
Ну, там нет подобия,
нет цыгана.
Здесь преобразование должно быть
преобразованием подобия.
Ну, даже унитарного подобия.
Ну, что вы хотите сказать?
Что вы хотите сказать?
Нет, что вы хотите сказать?
Не сможете.
Ну, посидите,
посмотрите.
То есть, если у вас матрица не Ермитова,
не Ермитова,
и вы хотите делать
преобразование
унитарное,
унитарного подобия,
то вы
ну, к верхнему треугольному
виду можете привести,
как я рассказал.
Ну, кто-то большую сделать.
Вот, в случае Ермитовой,
ну, просто понятно, что
вот эта верхняя почти треугольная матрица, она же должна
Ермитовой.
А что такое Ермитова
верхняя почти треугольная матрица?
Она трагедионная.
Правда?
Она трагедионная.
Вот. А теперь
представьте себе, что вы все то же самое
применяете QR-авгоритм
Ермитовой.
Да, спасибо.
Что в диагональной матрице?
Значит, вот эти нули получаете,
сколько действий надо?
И кто-то из вас замечательный матч,
ну, сколько будет?
Сколько действий будет?
N, уже не N в квадрате,
а N.
И когда вы в обратном порядке
перемножите,
уже R и Q, да,
у вас
снова появится уже трех
диагональной матрицы Ермитова.
Правда?
Ну, и это будет стоить порядка N.
То есть, в
Ермитовом случае
никакая ретерация будет стоить
по большой от N.
Что совсем прекрасно.
То есть, QR-авгоритм
применяемый к Ермитовой
матрице вообще быстрее.
Но
после предварительного преобразования.
Значит, в общем случае
предварительное преобразование
дает верхнюю
почти треугольную матрицу.
А в Ермитовом случае
трехдиагональная матрица.
Никакой ретерации
становится, либо
сложности N в квадрат,
либо сложности N.
Значит, вот первая цена
одной итерации может быть уменьшена
за счет предварительного
преобразования, как в один раз таком.
N в кубе
потратили,
а после этого у вас, в общем случае,
N в квадрате умножит
на число итерации.
Ну, а теперь займемся вопросом
о числе итераций, их бы тоже
хотелось бы как-то уметь уменьшать.
Но здесь идея такая,
это идея сдвигов.
Значит, вот вместо
исходной матрицы
рассмотрим сдвинутую матрицу.
Выберем сдвиг альфа.
И для вот этой сдвинутой
матрицы найдем QR-разложение.
Вот, а новую матрицу
построим.
Перемножим эти множители
в обратном порядке и добавим
то, что вы читали.
Вот QR
итерации со
сдвигом на альфа.
Ну, опять таки,
давайте заметим, что такое
есть А1.
Здесь я могу написать Q1 со
звездой Q1
R1 и Q1
минус альфа и, правильно?
Ну, здесь вот, в единичной матрице,
где в скобочках
увидели,
что увидели в скобочках?
Это есть
А
минус
альфа
плюс альфа и.
То есть, как и раньше
было,
матрица Q1
имитарно подобна исходной матрице.
Бесмотря на то, что мы какое разложение
делали не для матрица, а для
сдвинутой матрицы.
Ну, вот и в результате
вместо вот степеней
возникает
вот, можно даже дальше
посмотреть.
Значит, это будет иметь
разложение,
как мы раньше
писали
и так далее.
То есть, в результате
выбор сдвигов
значит,
сходимостью
будут управлять
не собственные значения матрицы,
а сдвинутые собственные значения.
То есть, собственные значения вот этой
матрицы.
И это есть, да, да, да, да, да, да, да.
Вот такие сдвинутые.
То есть, это есть некий полином определенный
сдвигами от матрицы А.
И вот отношения вот этих
новых собственных значений
могут быть поменьше.
Сдвиги выбираются именно с такой целью,
чтобы сделать это отношение поменьше.
Ну, вот общая идея сдвигов такая.
Значит, вот QR-алгоритм
обычно применяется
с предварительным преобразованием
к почти треугольной матрице
либо к треугольгагональной в этом случае.
И обычно со сдвигами.
Ну, вот в случае сдвигов
такой вот теория
глобальной исходимости вроде бы нет.
Но при разных стратегиях
сдвига доказывается
очень хорошей локальной исходимостью
квадратичной
или даже выше в ровном случае.
Ну,
это как раз и вышло.
Вот, я должен сделать объявление,
но кто-то может быть
обрадуется, а кто-то огорчится.
Следующий четверг, лет впереди.
Вот подгадал как-то.
А вот через две недели будет.
Смотрите.
Как-то мы один день пережили без вас.
Очень красиво.
Ну ладно,
на сегодня все.
Спасибо.

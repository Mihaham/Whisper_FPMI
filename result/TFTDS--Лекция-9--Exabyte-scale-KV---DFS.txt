Ну что, сегодня состоится лекция, которой мне самому не хватало долгое время, и вот она появилась наконец. Я ее очень рад. Надеюсь, что я вас тоже порадую.
Почему ее раньше не было? Потому что она сложная. Не то чтобы она сложная алгоритмически, а как-то очень сложная. Нет, ничего такого не будет.
Но в ней много очень больших идей, которые скапливались, скапливались, наконец оформились. И мне кажется, очень хорошо.
Помните, когда-то на самой первой лекции я вас спрашивал, зачем нам вообще нужны распределенные системы? Почему они распределенные?
Зачем быть такими? И первая причина, наверное, самая очевидная, это отказа устойчивости.
Ну, мы не можем доверять конкретной машине. Конечно, она всегда может пересгрузиться, поэтому все, что мы в ней храним, все, что мы делаем, мы должны хранить, видимо, персистентно на жестком диске.
И мы такую задачу решали с вами, когда говорили про LSM, про Lockstruct 4.3. Как сделать упорядоченное key value хранилище поверх жесткого диска.
Причем хранилище должно быть с произволенным доступом, а жесткий диск умеет только не спеша вращаться и умеет только последовательно доступ эффективно.
Вот мы эту задачу решали с помощью LSM.
Мы могли пережить рестарту узла. Кроме того, вместе с рестартом любая машина может не перезагнула, выключится, но не включится потом, она может отказать.
Поэтому мы надеемся, что если она вдруг поднимется, то на жестком диске данные останутся, но если она не поднимется, то, видимо, нам нужно задублировать ее состояние на другой машине.
И для этого мы решали задачу репликации.
Давайте я чуть аккуратнее напишу.
Задача хранения данных и задача репликации.
Здесь у нас был LSM или B plus деревья, а на уровне репликации у нас multipax или RAF, другой ваш любимый алгоритм консенсуса.
Но это же не единственная причина делать распределенные системы.
Вторая причина, про которую мы сегодня будем говорить, это масштабируемость.
Может быть, скорее всего, наши данные просто не помещаются в одну машину, даже если мы считаем, что она надежная, потому что мы ее зареплицировали.
Может быть, у нас настолько много данных, что они не помещаются даже в 100 машинах, в 1000 и, может быть, в 10000 там помещаются.
Сегодня мы хотим решать такие задачи. Мы хотим говорить про масштабируемость распределенных систем.
И в качестве примера у нас будут два класса систем. Это key value хранилища и это будет распределенная файловая система.
Казалось бы, мы уже распределенную файловую систему делали, key value хранилища мы, может быть, интуитивно представляем, как делать распределенным.
Мы с этого начнем сегодня. Но я хочу вам сразу сказать, объяснить, в чем сложность здесь.
Вот эти задачи в каком-то смысле вам знакомы. В смысле, сложность, которая там возникает, она должна быть хорошо знакома.
Ну, скажем, задача реализации упорядоченного хранилища, упорядоченного контейнера с произвольным доступом поверх жесткого диска.
Это вполне себе алгоритмическая задача. Мы зафиксировали модель стоимости и сказали, что у нас нет произвольного доступа к жесткому диску,
но есть доступ к последовательно большими блоками, и он эффективен.
И вот мы в этой модели стоимости, где мы хотим минимизировать количество секов, делаем упорядоченное хранилища,
хранилища, который умеет путагетомо-произвольному ключу.
Для этого мы что делали? Ну, мы там использовали SSTable, какие-то BloomFilter, в общем, какая-то алгоритмическая идея была за этим всем.
Задача репликации это вообще чистая алгоритмическая задача.
Она целиком про конкарнси. У нас есть узлы, какие-то распределенные акторы, они как-то неупорядочно действуют, как-то конкурируют друг с другом,
в общем, сообщения летают по сети в произвольное время, ну и в итоге складываются какие-то конфигурации,
какие-то исполнения, и в любом таком исполнении наш алгоритм должен вести себя корректно.
Это вот более-менее прошлый семестр, когда мы занимались многопоточностью, те же самые проблемы.
Ну, строго говоря, в storage же есть еще одна проблема инженерная, про которую мы еще не поговорили,
но об этом будет отдельный семинар, про то, как жить поверх файловой системы жесткого диска, какие там есть проблемы.
Ну, потому что задача-то не только в том, чтобы эффективно обращаться к данным по произвольному ключу,
а еще и в том, чтобы пережить restart машины в произвольный момент времени.
Вот вы пишете что-то в лог, добавляете там какую-то большую запись о том, что вы вставляете по ключу K значение V,
а где-то в середине вас перезагружают. И вот какие-то ваши данные успели записаться в лог,
какие-то не успели записаться в лог, система в каком-то неожиданном состоянии завершилась.
Ну и после этого оно должно перезагрузиться и начать работать корректно.
Так что нужно здесь учесть какие-то аспекты, какие-то нюансы работы файловой системы и жесткого диска.
Про это мы поговорим, но все же такие локальные, понятные задачи.
Вот задача масштабируемости, у нее сложность совсем другого рода.
Тут не нужно, скажем, оптимизировать число фаса или число раунд трипов, чего-то такое делать.
Задача масштабируемости, она про то, как взять какую-то систему, которая, не знаю,
может быть, хорошо живет на одной машине или на 100 машинах и масштабировать ее в тысячу раз.
И решение этой задачи, оно не про алгоритмы и не про инженерию, а про то, чтобы найти в системе узкие места
и вот эти узкие места научить масштабировать.
Вот спроектировать такой дизайн, выделить такие абстракции, такие слои, так их скомпоновать,
чтобы даже не складеть что-то еще сложнее.
И так скомпоновать все это, чтобы вот в системе не было узкого места,
чтобы она могла расти горизонтально бесконечно.
Мы могли бы добавлять в нее машины, и она бы эти машины как-то утиризировала,
то есть хранила там больше данных и обслуживала больше запросов.
Вот эта задача для нас новая, мы еще ее не решали толком.
Ну, мы пробовали делать с файловыми системами и к какому-то решению там пришли,
но вот сегодня мы хотим его еще большему масштабируем, вспомнить,
на чем мы остановились на каком-то прошлом семинаре
и довести задачу до логического конца.
Почему мы сегодня говорим про масштабируемость,
например, двух классов систем киевареохранилища и файловые системы?
Ну, тут есть два объяснения, одно историческое, другое, более современное.
Историческое такое, что в издревле, вот последние два десятилетия,
люди строили в больших компаниях, как правило, было два параллельных пайплайна работы с данными.
Была бач-обработка и была реалтайма-обработка.
Ну, скажем, вы индексируете интернет, у вас есть какой-то гигантский граф,
его нужно как-то обходить и что-то с ним делать, не знаю, поджарамку словно считать.
И вы пишете системы, которые, скажем, умеют в фоне, не спеша, вот эти огромные массивы данных перемалывать.
Ну, тут можно представить себе какой-нибудь MapReduce.
Нам не важна здесь аутентная степерация, нам важно, чтобы мы отказоустойчиво
хранили и обрабатывали огромные массивы данных, петабайты данных.
Ну и где такие данные хранить? Конечно же, это распределенная файловая система,
где у вас помещаются огромные бесконечного размера файлы.
Киеварю хранили еще исторически, были про интерактивную обработку.
Ну, не знаю, представьте себе Amazon, который продает вам какие-то книги и девайсы,
вы приходите к нему, добавляете товары в корзину, нажимаете купить.
Это интерактивная работа и, конечно же, Amazon под капотом у себя сохраняет все ваши покупки
в какую-то базу, покупки вашей корзине в какую-то базу,
потом, когда вы нажимаете купить, эта транзакция куда-то дальше улетает.
То есть два таких кейса разных. Сейчас немного не так, но и киеварю хранилища,
и распределенная файловая система все еще нужны, потому что, грубо говоря,
если вы Google, то у вас есть распределенная файловая система, которая называется Colossus.
Это следующая интерация, масштабируемая файловая система GFS,
которую мы обсуждали как-то. И более-менее все, что вы Google храните у себя
на ваших машинах, а сколько их у вас, кстати, у вас оказывается миллионы,
ну, единицы миллионов. Вот все, что вы на этих единицах миллионов машин храните,
хранится, в конце концов, не на локальных жестких дисках, в смысле, конечно же, там, где еще,
но логически оно хранится не на конкретной машине, а в распределенной файловой системе.
И вот все эти огромные массивы данных хранятся в итоге в DFS,
и вот эта DFS должна масштабироваться до таких объемов.
Что касается кейварю хранилищ, то они сами по себе, конечно, еще актуальны,
но сейчас поверх них научились делать, поверх масштабируемого кейварю хранилища
с транзакциями, научились делать таблицы и запросы, и в конце концов научились делать базы данных.
Поэтому, да, вы, наверное, работаете с базами данных, но в конце концов под вами кейварю хранилищ
и под вами распределенная файловая система, даже если вы напрямую с ними не работаете,
и вот все ваши данные хранятся там. Поэтому задача масштабирования кейварю и DFS
это наша главная задача. Ну и давайте мы с чего-нибудь начнем.
Начнем мы сегодня с кейварю хранилищ.
Что это такое, вы, наверное, хорошо уже помните. У нас есть операция put, у нас есть операция get, delete,
у нас есть чтение диапазонов, потому что так можно делать snapshot и так можно делать транзакции.
Можно было бы сказать, что у нас есть вот такая большая-большая таблица.
Вот здесь написан ключ, здесь написано значение.
Но мы такую задачу пока не умеем решать, но мы умеем решать задачу в пределах одной машины.
Мы умеем строить lsm. Дальше мы умеем этот lsm реплицировать с помощью алгоритма multipaxos или raft.
Так что мы задачу умеем решать, но вот до тех пор...
Только если объем данных, которые мы храним в кейваре, умещается в одну машину, в один жесткий диск.
Ну а теперь у нас таблица большая. Давайте назовем ее bigtable.
В смысле система bigtable? Просто большая таблица.
Она настолько большая, что она не помещается, конечно, в одну машину.
Как мы поступим с ней?
Мы ее протиционируем горизонтально. То есть мы возьмем и эту большую таблицу поделим на какие-то разумные размеры части.
И каждую такую часть мы назовем как?
Мы назовем каждую такую часть таблет.
В самом деле ключи в этой таблице независимые. Пока мы не говорим про транзакции, они независимые.
Поэтому мы можем разделить таблицу на части и обеспечить отказоустойчивость каждой части независимо от остальных.
Каким образом? Ну, просто сделать RSM.
Вот у нас есть эта огромная таблица, и у нас рядом с ней есть pull машин.
Ну вот давайте за хранение каждого таблета будут отвечать какие-то машины.
Вот у нас есть красный таблет, и у него будут три реплики здесь.
Эти три реплики образуют, видимо, RSM. Они реализуют multipaxos или raft для того, чтобы каждый из этих реплик локально хранит данные таблета в локальном хранилище, в roxdb или leveldb,
и упорядочивает апдейты в это локальное хранилище на этих репликах с помощью алгоритма multipaxos или raft.
Понятная идея, да? Это мы с вами делать умеем.
Ну, давайте подумаем про какие-то нюансы.
Ну, например, какого размера эти таблеты?
Она бесконечная, она может расти и расти, и вот неограничена. Мы, по крайней мере, к такому стремимся.
А таблет какого размера?
Фиксированного размера. Но очень трудно, когда у тебя ключи появляются, удаляются, что-то фиксированного размера иметь.
Нам нужно какую-то верхнюю границу, видимо, выбрать.
Ну и вроде бы, естественно, верхняя граница у нас есть.
Мы не можем в таблете хранить больше, чем помещается в одну машину просто потому, что это такое ограничение RSM.
Все, что реплицирует RSM, должно помещаться в одну машину в один диск.
Ну, у машины, наверное, какие-то, не знаю, терабайтные диски.
Вот верно ли, что нужно делить эту большую таблицу на таблеты на шарды размером терабайт?
Ну, что значит слишком много? Их будет много.
Подожди, очень странное замечание, потому что мы себе крупнее делать таблеты не можем просто.
А ты говоришь, что их уже много получается. Нет, дело не в этом.
Ну как бы, разом-то их зачем читать? У нас приходит пользователь, говорит PUT.
По ключу значения. Мы находим каким-то образом таблет.
Он приходит в какую-то строчку со своим PUT.
Вот мы должны понять, на каком таблете эта строчка лежит.
Ну, вот, допустим, на этом.
Ну, это правильно до запроса. Там это обычный PUT, который попадает в RSM, там в multipax,
среплицируется и в конце концов применяется к локальному состоянию, то есть помещается в levelDB.
Ну, вот такой как-то ответственный получится, потому что если у нас один таблет имеет
веточную границу по размеру диска машины, то нам нужно давать гарантию по диску этого таблета.
И тогда мы не можем размещать по машине больше одного таблета.
Так и зачем нам размещать по машине больше одного таблета?
Нет, на ход мысли правильный. Есть таблица, она делится на таблеты как-то произвольно по ключу,
но в конце концов пользователь же неравномерно к этой таблице обращается.
Может быть, там есть какие-то горячие ключи, горячие таблеты, есть какие-то менее активные,
где нагрузка меньше, данных меньше.
Поэтому мы можем только запустить систему и смотреть, как пользователь с ней работает.
Он может в какой-то диапазон писать больше, делать большую операцию над каким-то диапазоном,
над каким-то меньше. И в итоге какой-то таблет, он же разделен по ключам, условно говоря.
У него есть какой-то стартовый ключ.
И может быть, между этими ключами будет очень много записей, и таблет начнет расти, расти, расти.
И с одной стороны, он может переполнить машину, которая его хранит, но машины, реплики.
А с другой стороны, просто эти машины могут не справляться.
Но точнее, это же RSM, и там все записи обслуживают один лидер.
И вот он может стать узким местом для этого таблета, потому что запросов слишком много.
В этом случае что мы захотим сделать? Мы захотим этот таблет разделить.
И тогда нам нужно как-то будет их перебалансировать в нашем кластере.
Сказать, что вот часть данных уезжает на другие машины, например.
Ну или так получится, да.
Ну а если мы начнем эти таблеты двигать, то если у нас таблет размером терабайта,
то их двигать будет очень тяжело, эта система будет очень не гибкой.
Поэтому мы, видимо, хотим небольшие таблеты.
И в промышленных системах их размер измеряется, не знаю, десятками, сотнями мегабайт.
У нас будет еще в будущем система Google Spanner, из которой был TrueTime.
У нее есть Open Source Clon, Open Source реализации по мотивам, как ROGDB.
Это база данных с транзакциями и таблицами под капотом Key Value.
И в этом Key Value таблеты размером 64 мегабайта.
Ровно для того, чтобы их можно было легко двигать по кластеру.
Ну и тогда, разумеется, на каждой машине у вас может быть не один таблет, а несколько.
То есть у вас одна машина является репликой красного, является репликой зеленого.
И если вдруг этой машине станет слишком тяжело, потому что очень много запросов,
какой-то таблет или таблеты станут очень большими,
то их можно разделить на части и размазать по кластеру.
Хорошо, тогда следующий шаг.
Видимо, эти таблеты нужно балансировать.
Кто этим занимается?
Ну какая-то нода. Если мы возьмем какой-то узел, который будет этим всем заниматься,
и выделим его, то он может умереть.
Тогда система, видимо, останавливается, потому что никто не сможет координировать действие.
Ну вот мы выделим такую роль.
Shard Manager.
Название довольно условное.
Это в данный момент какой-то конкретный узел из кластера, который понимает,
где какие таблеты находятся, то есть какими машинами обслуживаются какие-то таблеты.
Он следит за тем, чтобы все были живы, следит за нагрузкой
и умеет таблеты двигать между этими узлами.
Разумеется, будет плохо, если это будет конкретная машина, потому что она откажет.
Ну что мы можем сделать?
Сделать ее отказоустойчивой тоже.
Давайте я вам покажу какие-то картинки.
Shard Manager это, конечно, одна машина, просто реплицированная.
То есть логически это один актор.
Мы сегодня говорим про киевое хранилище, у нас будет несколько примеров.
Bigtable, про которую вы могли читать статью, ZPDB это киевое хранилище в Facebook.
Ну и слой Key Value, который реализован как RoachDB, это Open Source-система по мотивам Google Spanner.
Ну вот, пожалуйста, как RoachDB, вот его дизайн.
Вы строите SQL поверх какого-то большого монолитного Key Value хранилища.
И в этом хранилище что у вас есть?
Отдельные машины. На каждой машине есть SSD-диск с RoachDB.
Ну и вот каждая машина является репликой для отдельных таблетов.
Но тут они называются range-диапазоны.
Пожалуйста, одна машина хранит сразу несколько.
Ну и разумеется, каждый таблет принадлежит, каждый range реплицирован на трех нодах.
По поводу Shard Manager я хотел показать.
Вот ZPDB картинки, вот еще раз та же самая конструкция.
Это, наверное, очень хорошо видно.
У нас есть пять машин.
Каждый shard в трех репликах.
Ну и вот они как-то по этим машинам распределены.
Вот в ZPDB, в Key Value хранилище Facebook, что еще делает Shard Manager?
Вот помните, мы говорили про multiprocess и пришли к тому построить алгоритм,
но в нем кое-чего не хватало.
Мы не сказали, как именно в multiprocess выбирается лидер.
Ну то есть понятно, что его можно выбрать со старшим ID,
но это вроде бы не очень эффективно, потому что у меня может быть пустой лог.
А еще мы не сказали, как в multiprocess новый лидер выбирает себе n, эпоху, в которой он будет жить.
В принципе, RAF то обе эти проблемы решал.
Там процедура выбора лидера была разумной, и этот лидер принадлежал некоторому терму,
то есть у него это n свое было тоже.
Так вот, ZPDB используется в multiprocess для репликации каждого таблета.
И shard manager что делает?
Он следит за лидерами каждого таблета, и если лидер умирает,
то shard manager это понимает, выбирает нового лидера для данного таблета
и назначает ему новую эпоху.
Ну то есть вот эта задача, выбор нового лидера и назначение ему новой эпохи
происходит централизовано shard manager.
Эта задача решается разом для всех таблета, для всех RSM в одном месте.
Понятная идея?
Хорошо. А теперь представим, что вы клиент,
и у вас есть какой-то пут по ключу значения.
И вы должны прийти на этот кластер и попасть в какой-то нужный вам таблет к лидеру,
который его обслуживает. Как вы это сделаете?
Как вы в этом большом пуле машину, не знаю, тысячи или десятки тысяч машин,
как вы найдете ваш таблет и лидера, который его обслуживает?
Ну вот идем сюда.
Что мы можем заметить? Что во-первых, shard manager страдает,
потому что вы к нему ходите по такому пустящему поводу, как маленький пут.
Это все-таки координатор, то есть он не хочет находиться на пути записей
или чтений у клиентов.
Он начинает стать туским местом.
Кроме того, сколько у нас здесь таблетов?
Порядочно.
Справедливое замечание. Порядочно.
До каких вообще объемов мы хотим штаблироваться?
До каких величин? До какой емкости?
Это правильно. До бесконечности хорошо масштабировать.
Если мы масштабируемся до бесконечности, то нам на любую задачу хватает запас.
Но вот бесконечность чему сейчас равна на данном этапе развития человечества?
А сколько сейчас данных нужно хранить? Вот если у тебя миллион машин?
Ты не хочешь умножать, я понял.
Ну мы не знаем, у тебя миллионы машин, а не какие-то тарабайта дискового пространства.
Но тут, конечно, нужно учесть, что это все реплицировано еще, то есть
логический размер данных меньше, чем они занимают физически на дисках.
Нет, петабайт это уже прошлый век.
Вот мы хотим масштабироваться до экзобайтов. Больше человечество сейчас не умеет.
Ну в смысле, не нужно ему. Ну вот до каких масштабов мы хотим?
И тут возникают некоторые технические проблемы, а именно, что вот этих таблетов много,
прям вот очень много. И, конечно же, вот этому менеджеру, который следит за кластером,
ему нужно знать, за какие таблеты какие машины отвечают.
Ему нужно просто хранить такое отображение. Вот у тебя есть вот эта большая таблица,
и тебе нужно знать, ну она поделена, одна таблица, ну или много таблиц, скорее всего.
Вот в системе Bigtable много больших таблиц. И тебе нужно для каждого таблета
знать, кто сейчас его обслуживает. Ну давайте назовем его таблет-сервер.
Тут есть проблема в том, что вот это отображение, оно большое.
Ну просто много метаданных у этого хранилища, потому что таблеты маленькие, а таблицы большие.
Вот, и ну просто вот этот шард-менеджер, он отказаустойчивый, потому что он реплицирован.
Но с другой стороны, вот просто такой объем данных, у него с трудом помещается.
Нет, мы, конечно, можем его шардировать, в смысле, в конце концов, разные таблицы,
разные таблеты, они более-менее независимы, поэтому можно взять и как-то поделить,
взять несколько шард-менеджеров, каждый отвечает за свой набор таблетов,
статически поделить по какому-нибудь хэшу. Ну и вот сделать много шард-менеджеров.
Вполне себе разумное решение. В смысле, оно будет работать, видимо.
Вот, но можно подумать и сделать несколько элегантнее.
Вот у нас bigtable, любая большая таблица, это отображение исключение значения.
Вот, и чтобы поддерживать эти таблицы с отображением исключения значения,
нужно хранить еще одно отображение исключения значения. Ну правда, служебное.
Вот, ну вот давайте мы сделаем для вот этого отображения еще одну таблицу.
Тут уже можно говорить про саму систему Google Bigtable. Вот Google Bigtable так и устроен.
То есть там есть пользовательские таблицы, и для каждого таблета,
мы должны для каждого таблета хранить точку обслуживания этого таблета.
И это еще одна служебная большая таблица. То есть Bigtable хранит свои методанные в Bigtable.
Это довольно удобно, потому что сам Bigtable масштабируется вроде как.
Ну точнее, мы делаем его масштабированным. Если он будет масштабироваться,
то и методанные будут масштабироваться. Поэтому не нужно каким-то специальным образом
решать вот такую задачу, как шардировать менеджера всех таблетов.
Мы можем методанные положить отдельно. Но смотрите, какая беда.
В конце концов, шард менеджер должен быть. Он будет с этой таблицей работать.
Но чтобы работать с этой таблицей, ему нужно знать, кто ее обслуживает.
Понятно почему, да? То есть он хочет что-то записать,
чтобы знать, что какой-то таблет обслуживается такой-то машиной.
А для этого ему нужно пойти в какой-то таблет, который обслуживает таблеты этих методанных
и к нему обратиться с этим запросом. А кто обслуживает эти таблеты? Как это узнать?
Ну вот. Ну смотри, у тебя есть вот эти таблеты, а есть вот эти методанные таблеты.
И для них нужно решить такую же задачу.
Видимо, там нужна еще одна таблица методанных и методанных.
Но чем она хороша? Тем, что она меньше становится.
Потому что для методанных нужна была большая таблица.
А для методанных и методанных мы делим еще на сколько-то, на 64 мегабайта.
И эти донны становятся еще меньше.
И вот может быть, здесь уже будет один таблет,
который знает, кто обслуживает каждый методанный таблет здесь.
Но для этого таблета же тоже нужно знать, кто его обслуживает, чтобы в него записать.
Ну а этот таблет обслуживает, в конце концов, одна же машина.
Вот куда мы эти данные положим? Ну, если мы в Bigtable, мы Google.
У нас есть Bigtable, там есть таблицы пользователей, там есть Bigtable,
там есть таблица уже совсем небольшая с методанными и методанными.
И в конце концов все упирается в какую-то одну машину, которая обслуживает вот эту.
И вот знание про эту машину нужно хранить отказоустойчиво.
И вот знание про машину, которая обслуживает этот таблет, хранится в Google Chabi.
Это сервис координации дерева, в котором мы можем брать локи
и можем в его узлы писать небольшие данные размером килобайта.
Вот мы можем туда записать одну эту машину.
Так что когда мы клиент приходим с путом, мы идем не в Shard Manager, потому что мы не хотим выгрузить.
Мы поначалу идем в Chabi первый раз, потом идем к этому таблету, узнаем из какого же таблета методанных,
какая машина обслуживает таблет методанных для нашей таблицы и нашего ключа.
И отсюда узнаем уже машину, которая нам нужна, которая хранит и обслуживает наш конкретный таблет,
в котором мы хотим что-то записать.
Понятная идея?
То есть мы строим этот Bigtable и методанный Bigtable кладем в сам Bigtable, потому что он хорошо масштабируется.
Эта идея, которая прижилась и ее использует как сама система Bigtable, так и более-менее все Open Source аналоги.
Так делает и ROGDB, который я показывал вам, и так делает, скажем, HBase.
Вот у вас на параллельном курсе должна быть рано или поздно лекция про HBase.
Это аналог Bigtable Open Source, написанный на джаве.
Ну и вот там есть такая же модель с таблицами, и есть специальная таблица с методанными.
И вот в ней ключ выглядит так, имя вашей таблицы с вашими данными,
запятая ключ, с которого начинается таблет этой таблицы, ну там что-нибудь еще,
и вот по этому ключу вы находите точку обслуживания, машину, которая отвечает от этих данных на запрос.
И вы приходите со своей, ну вы хотите что-то записать или что-то прочесть из вашей таблицы по вашему ключу,
и вы вот в такой таблице ищете первый ключ, ну последний ключ не больше, чем ваш,
и таким образом через служебную таблицу вы находите точку обслуживания и уже идете в ваш целевой таблет.
Ну а все начнется с Google Chabi.
Ну разумеется, вы пишете по своему ключу, видимо, много раз, поэтому вы можете закашировать.
Конечно, вы закашируете, кто обслуживает корневой таблет, но чтобы его часто не грузить, вы, конечно,
закашируете, кто обслуживает каждый таблет метод данных, ну или большинство таблетов метод данных,
с которыми вы работаете. Поэтому вот на Google Chabi, конечно, не то чтобы все клиенты на каждую запись
сходят в Google Chabi, это было безумием, она потом нарывалась.
Ну вот такая идея, и она в этом состоит мастерство дизайна архитектуры распределенных систем.
Как это придумать? А как это вообще придумать можно?
Ну то есть это можно придумать просто так, как бы без всякого бэгграунда,
а на самом деле эта же конструкция хорошо вам знакома.
Не то чтобы люди выдумали что-то совершенно принципиально новое.
Ну очень удаленно, но виртуально.
Она не то что очень удаленная, это одно и то же.
То есть у тебя есть в операционных системах механизмы в виртуальной памяти.
Ты процессор должен на каждое обращение транслировать виртуальный адрес физический.
У тебя есть ключ, ты должен транслировать его в точку обслуживания таблета.
Что у тебя для этого есть? У тебя есть память, она поделена на страницы.
И у нас здесь есть таблицы, они поделены на таблеты.
В этих страницах есть твои данные, вот они.
А еще есть специальные страницы, в которых лежат адреса других страниц,
чтобы ты нашел по виртуальным адресу свою физическую.
Это таблица PageTable, это страница PageTable.
И вот страницы PageTable, они же образуют такой бор.
Ну и здесь то же самое. У тебя есть корень, ты попадаешь сюда,
выбираешь один из этих таблет, попадаешь дальше.
Вот так же, как у тебя фиксировано количество уровней таблиц и страниц,
так же у тебя зафиксировано количество уровней здесь.
У тебя есть одна большая таблица с методанными, у тебя есть одна маленькая таблица из одного таблета,
которая помогает найти тебе нужный таблет в методанных.
Ну а вообще, как процессоры находят таблицу страниц для процесса текущего?
А где он лежит?
За хардкожем? Ну видишь, за хардкожем.
Ну у тебя же процессы меняются, может быть, за хардкожем.
Он лежит в процессоре, в регистре.
Ну вот Chabi, это как раз очень маленькая память,
и там есть специальный регистр, который хранит корень всего этого.
А дальше ты от него начинаешь идти вверх и находишь свою физическую страницу.
Вот та же самая идея.
Здесь, кажется, границ никаких нет.
Ну вот, мне кажется, что супер изящная идея.
Ну а теперь можно, это еще не все, что нужно сказать про киеварю хранилища,
но пока мы остановимся и поговорим про другой класс систем,
про распределенные файловые системы.
Их же тоже нужно масштабировать.
Ну и давайте вспомним наш прогресс, что мы успели сделать на каком-то прошлом семинаре.
Мы говорили, что файловая система, она из себя что представляет?
Ну во-первых, она хранит файлы.
И видимо, распределенная файловая система должна хранить сколь годно большие файлы.
Поэтому мы делим их на блоки.
Ну а помимо этого у файловой системы есть иерархия имен, некоторое дерево.
И для каждого узла этого дерева, для каждого листа этого дерева нам нужно помнить информацию,
из каких блоков этот файл состоит.
Вот так что у нас есть это дерево.
Для каждого файла у нас есть структура, которая называется inode.
И там лежат какие-то атрибуты.
Ну скажем, там размер.
И список блоков.
И вот по таким идентификаторам блоков и дальше где-то можно на диске найти,
на блоке устройств.
Значит, нам нужно поддерживать дерево.
Нам нужно хранить inode, списки блоков.
Ну и нам нужно хранить сами блоки где-то.
Так вот, что мы сделали с вами тогда?
Мы сказали, что есть в файловой системе логически два уровня.
Данные и методанные.
И когда мы делаем распределенную файловую систему,
то мы хотим вот эти два уровня выделить в две подсистемы.
Уровень... вот методанные будут находиться в подсистеме,
в котором мы назвали методанную стор,
а данные будут находиться в системе под названием chunkStore.
Ну chunk потому что мы блоки переименовали в чанки.
Ну и давайте вспомним, как мы chunkStore и metastore строили.
Ну во-первых, мы сказали, что давайте не будем пытаться
создать API настоящей локальной файловой системы.
Мы вместо этого ограничимся какими-то более простыми операциями.
Скажем, мы не будем делать перезаписи файла.
Будем делать только append.
И append сразу большими блоками, большими порциями.
То есть на каждый append порождается новый chunk.
И нужно его сначала положить в chunkStore,
а потом прийти в metastore сказать, что мы создали новый chunk.
Это как списку чанков в inode для данного файла.
В чем было преимущество такого дизайна?
Ну он был прост, потому что чанки становились имутабельными,
они просто один раз добавлялись и все.
И вот chunkStore у нас имел очень простой API.
Мы могли положить в него какие-то данные.
И нам от метаса chunkStore отдавал ID.
А дальше по этому ID можно было этот chunk потом прочитать.
Metastore хранил все остальное.
То есть он хранил дерево, и он хранил inode файлов.
Из каких чанков они состоят?
Вот эти самые идентификаторы, которые генерировал chunkStore.
Вспоминаем, да?
Как мы делали chunkStore и как мы делали metastore?
Помните?
Вы должны что-то сказать мне.
Как мы делали chunkStore?
Ну, брали pool-машин для начала.
Давайте я коротко напомню, что мы там достигли.
Брали pool-машин.
Брали какую-то выделенную машину, которая всем этим управляла.
ChunkMaster.
И когда мы делали pool, мы приходили к этой машине.
А она выбирала из этого всего pool-а свободных машин для хранения данных.
Выбирала какие-то, допустим, три и писала реплики на них.
Генерировал идентификатор, конечно.
Вот сюда.
Сюда.
И сюда.
Вот здесь появлялся наш chunk теперь.
Когда мы хотели его прочитать, то мы приходили к этому chunk-мастеру.
Чанк-сервер.
Чанк-мастеру давайте.
И он, поскольку помнил, где что хранится,
направлял нас в одну из этих машин и мы с нее читали данные.
Это не работало. Почему?
Потому что машина умирает, и мы всю информацию теряем.
Прежде чем решать эту задачу, мы сказали следующее.
Вообще говоря, вот эта машина, этот контроллер этого абстрактного диска
не обязан помнить, где что лежит.
Почему?
Пусть он просто поднимается пустой, абсолютно ничего не знает.
Но каждая машина из этого кластера будет ему периодически сообщать,
какие чанки на ней лежат, с какими идентификаторами.
Так что эта машина может подняться пустой и подождать некоторое время
и постепенно в себе восстановить все состояние.
Пока не понятно, чему это помогает.
Это помогает, когда мы хотим направить реплики у этого chunk-мастера.
Почему?
Потому что репликации между этими chunk-мастерами никакой не нужно.
Пусть теперь каждая машина сообщает о своих chunk-ах не только ему периодически,
а вот всем chunk-мастерам.
Так что мы можем прийти на какую-то конкретную из них,
на какой-то конкретный мастер, сделать пут через него.
Он положит этот chunk сюда, сюда и сюда.
И чуть позже вот эти три машины расскажут о новом chunk-е всем остальным chunk-мастерам.
Они, конечно, не синхронны, то есть тут eventual consistency,
но в конце концов все не сойдутся.
И вот такой дизайн тем хорош, что тут легко повышать козоустойчивость.
И тут не нужна никакой сложной координации, не нужна задача консенсуса.
Нам не нужно ничего упорядочивать.
Мы реплицируем монотонно растущее множество.
Вот так мы получали chunk-store, который вроде бы легко обеспечивал козоустойчивость.
И он рос до тех пор, пока эти машины справлялись с нагрузкой.
Они могут ее делить между собой.
Но даже если они перестанут справляться,
потому что просто очень-очень много chunk-ов, и они про все помнить не могут,
то мы эту конструкцию шардировали.
То есть мы говорили, будем делить теперь chunk-ы по хэшу.
То есть возьмем эту картинку, удвоим ее здесь рядом,
и когда нам приходит пользователь с путом, мы берем хэш от его chunk-а,
от его данных, и идем статически либо в первый chunk-store, либо во второй chunk-store.
И этот chunk-store нам генерирует такой идентификатор, в котором будет зашифровано,
а где потом эту chunk искать в первом или втором chunk-store.
Так что этот слой довольно тривиальным образом горизонтально масштабируется бесконечно,
без задачи консенсуса.
То есть вот здесь масштабируемость достигалась.
А вот с metastore были проблемы.
Здесь данные имутабельные, и реплицировать их одно удовольствие.
В metastore данные имутабельные.
Мы постоянно, не знаю, переименовываем, копируем, создаем, удаляем,
дописываем новые chunk-ы в iNode.
И, конечно же, в этой задаче порядок модификации важен.
Нам важно, в каком порядке происходят create-файлы и delete-файлы.
Поэтому chunk-у metastore мы делали RSL.
То есть пусть у нас теперь три машины хранят дерево и iNode
и реплицируют все операции добавления chunk-ов,
или переименование файлов, создание файлов с помощью multipax или raft.
И эта конструкция в целом работает.
То есть в ней единой точки отказа нет, как было в GFS.
В GFS iNode, дерево и вообще расположение chunk-ов на дисках
хранила всего одна машина, один выделенный мастер,
единая точка отказа.
Но вот здесь мы расположение chunk-ов на дисках перенесли в chunk-store,
а все оставшиеся вместо данные мы реплицируем с помощью multipax.
Так что отказоустойчивость есть, и масштабируемость в определенных границах тоже есть.
Это не бесконечный вопрос.
У нас не умеет, что мы придем к chunk-мастеру с лутом,
и потом почти сразу придем к соседнему.
Но нас это не беспокоит, потому что если мы получили от chunk-store ID,
а потом с ним приходим, то мы уверены, что в нем данные есть.
Правда ведь? Но он же нам этот ID сгенерировал.
Поэтому если мы приходим в другой chunk-сервер,
и он нам говорит, что пока не может прочесть,
то видимо это означает только одно.
То, что он потерял данные, что совсем плохо,
либо что он просто не успел про них узнать еще.
Поэтому мы просто подождем некоторое время, притраемся, и рано или поздно он нам ответит.
То есть это не проблема. Мы уверены, что в нем данные есть.
Ну и если мы знаем, с какой периодичностью каждый chunk-сервер шлет свои chunk-мастерам,
мы знаем, через какое время примерно сделать ретро и данные прочесть.
Так что здесь, оказывается, устойчивость штаммировости есть бесконечная,
а вот здесь мы упираемся в RSM.
Упираемся в RSM почему? Потому что, во-первых, у нас может быть слишком много файлов в файловой системе,
а во-вторых, потому что у нас может быть слишком много chunk-ов в файловой системе.
Даже chunk-и большие.
О чем пишет нам Facebook в своей свежей статье, кажется, 21 года,
про свою файловую систему. Что если вы делаете такой дизайн,
то не то чтобы система не будет работать,
вы, конечно, поставите какие-то квоты, что пользователь не может заводить там
больше какого-то количества файлов, больше какого-то количества chunk-ов,
и система способна будет жить.
Но она не способна будет бесконечно масштабироваться.
И вот с таким дизайном Facebook пишет, что вы можете промасштабироваться
до десятков петабайт данных.
Потому что, когда вы достигнете такого объема данных в chunk-сторе,
вы просто упретесь в количество файлов и количество chunk-ов в мета-сторе.
То есть у вас узкое место, оно вот здесь.
И вы упираетесь в RSM.
Потому что все, что реплицируется на RSM, должно помещаться в одну машину.
Ну, вот тут можно было сказать, шардируем.
Потому что есть таблиты, они как-то независимые.
А вот здесь дерево, и оно логически цельное.
Его сложно шардировать.
Ну, не то что невозможно, но сложно да возможно.
То есть можно хранить iNode на разных машинах.
Ну, то есть можно придумать какую-то схему, которая позволит мета-стор шардировать.
Другое дело, что это сложно и неудобно.
Так вот, как же в этом месте справиться с узким местом, с мета-стором,
и масштабироваться за десятки петабайт?
А мы хотим экзобайты.
То есть это сотни петабайт, тысячи петабайт.
Мы хотим на несколько порядков больше данных.
В одну машину это не поместится уже никак.
Понятна проблема?
То есть у нас вот здесь узкое место.
Но оно было здесь узким местом, здесь оно шардировалось проще.
А тут теперь сложнее.
У вас же начался HDFS?
Наконец. Здорово.
HDFS умеет так шардироваться, это называется, кажется, федерация.
Но это костыль.
То есть вы говорите, что вот у меня есть одно поддерево, есть другое поддерево, я с ним никогда не работаю.
Но это не совсем правда.
Мы хотим какого-то...
Все, что происходит сегодня, оно должно быть в какой-то степени астроумно.
Вот эта идея была астроумной, правда? Сложности поспорить.
Но мне так кажется, что оно астроумное.
Положить методанные Bigtable и Bigtable.
Вот здесь тоже нужно что-то придумать похожее.
Нет, не понимаете?
У нас, смотрите, есть методанные.
Чем они отличаются от чанков?
Тем, что чанки мутабельны, а здесь все очень мутабельно, наоборот.
И порядок апдейтов важен.
Мы вот точечно меняем файлы, точечно меняем записи в айнодах.
Отходим в произвольные места и что-то там меняем.
Порядок важен.
Нет?
Не понимаем, да?
У нас имутабельные чанки, извиняюсь.
У нас единица имутабельности, единица хранения очень большая.
Это там гигабайты, например, могут быть.
Ты хочешь сделать Copy-on-Write на объектах, например, гигабайтами?
Гигабайты это не сработает.
А у нас же с лева были мутабельные данные.
С лева? В смысле, с лева где?
Вот на левой части.
Да, вот здесь были мутабельные данные.
Блестящие идеи.
Давайте положим не только методанные Bigtable и Bigtable,
давайте положим методанные файловые системы,
которые в ее хранилище тоже.
Вот ровно так Facebook и поступает.
Вот, у них есть файловая система под названием Tectonic.
И нам будет, конечно, сложно разглядеть.
Но они хранятся, смотрите, сейчас структура их.
Вот, у них есть ChunkStore, который бесконечно масштабируется,
он хранит данные файлов.
Блоки, чанки, как вы их назовете.
Там есть некоторые нюансы.
У вас есть методанные.
Это дерево-файловые системы,
это inode-файлов и это еще один вспомогательный уровень,
сейчас в нем не хочу.
Так вот, эти данные помещаются в KeyValue-хранилище.
И переложить их в KeyValue-хранилище можно довольно понятным образом.
Сложно, не знаю, что нужно сделать.
Давайте, может быть, если сделать поярче, это не работает, да?
Ну ладно, я не хочу так делать часто просто.
Экран опускается.
Давай по такому поводу подождем его.
Вот, невероятно.
Остановись.
Сейчас, давайте еще немного.
Вот, это схема укладки методанных в KeyValue.
Как в KeyValue представлена директория?
Директория — это набор ключей, составных,
где составляют стандарты.
В KeyValue представлена директория.
Директория — это набор ключей, составных,
где сначала идет ID-директория, а потом идет имя файла.
Откуда появляется ID-директория, тут, видимо, такой же слой, неважно.
Вы уже узнали ID-директории по другому запросу,
по вспомогательному ключу, из директории в ID-директории.
А теперь вы хотите, скажем, пролистать файлы директории.
Для этого вы делаете итерацию по префиксу dir-id.
И вот вы итерируетесь по ключам, где второй компонент ключа, служебный, — это имя файла.
Если вы хотите узнать, из каких блоков состоит файл, чтобы его прочитать,
вы сначала получаете директорию, потом вы обращаетесь по такому ключу и узнаете файл ID.
А дальше вы листите хранилище по ключу с таким префиксом.
И вот в тех ключах, которые вы перечисляете итератором,
вы из второго компонента извлекаете идентификатор-блок.
Точнее, вы знаете, что блоки вашего файла перечислены в отдельных ключах,
и вот задав итерацию по такому префиксу, вы можете перечислить все его блоки
и узнать в конце концов про их расположение.
Ну, короче, понятно, что происходит.
То есть у вас каждая директория выложена как набор ключей с одинаковым префиксом,
и каждый файл, список чанков каждого файла, их же может быть очень много,
тоже выложен как набор ключей.
Мы не можем положить их в одну запись, потому что она может быть очень большой.
Мы хотим ограничить размер записи, поэтому мы делаем для каждого файла много служебных ключей,
где второй компонент ключа — это идентификатор-блока, который...
блока, в который составляют этот файл.
Понятно? Но есть проблемы.
Ну вот, смотрите, k-value мы масштабировать умеем.
Поэтому мы умеем масштабировать метод данной файловой системы более-менее бесконечно.
Так что мы умеем бесконечно масштабировать файловую систему теперь.
Но есть неудобства, которые возникают из-за такого дизайна, связанные именно с k-value.
Вот мы кое-что потеряли относительно дизайна с R7, который у нас был раньше.
Понятно или нет, что мы потеряли?
Ну вот, раньше мы могли сделать атомарные операции, потому что RSM — это же...
вам хочется, не знаю, взять и переименовать 100 файлов разом.
Вы берете и в RSM отправляете служебную команду, которая говорит, вот атомарно переименуем 100 файлов.
И эта вот служебная команда, такая транзакция, она через лог упрощается с другими командами, как единое целое.
И RSM применяется все эти 100 переименований подряд, как будто бы атомарно.
Потому что это одна машина, в конце концов.
Вот здесь уже разные файлы и разные директории, они находятся потенциально в разных таблетах k-value хранилища.
И атомарной работы с ними уже нет.
Ну точнее нет, если у вас нет транзакций.
В Bigtable транзакций нет, в k-value хранилища Facebook, вот этот самый ZPDB,
а вот это k-value хранилища, это ZPDB, про который я говорил раньше, там тоже транзакций нет.
Короче говоря, вы не можете сделать ренейм из произвольного места в произвольное место файловой системы.
Ну там move сделать.
Но вы все-таки хотите какую-то атомарность иметь.
Тут все понятно, да? Я могу это убрать.
Хотя нет, рано.
Но все-таки какую-то атомарность вы хотите иметь, скажем, вы хотите, я не знаю, иметь атомарность в пределах одной директории.
Как Facebook этого достигает?
Ну у вас есть k-value хранилища, там есть таблицы, в ней есть таблеты.
И вот каждый таблет это RSM.
Поэтому внутри таблета вы можете работать с данными атомарно.
Вы можете там транзакции делать.
Вообще говоря, вот в Bigtable, если вы читали статью, там нет транзакций между разными строчками,
потому что границы таблетов неизвестны.
Но вот Facebook делает так.
Он раскладывает данные так, чтобы ключи с одинаковым префиксом вот таким вот попадали строго в один шарт, в один таблет.
Поэтому Facebook уверен, что вот так расположив данные в k-value хранилище,
любая операция над одним файлом будет атомарной.
Но у вас нет атомарности на совершенно произвольных местах в дереве файловой системы.
Ну и еще один нюанс.
Вы, скажем, не можете легко посчитать, сколько у вас занимает под дерево,
потому что это много запросов, нужно вот так вот рекурсивно обходить.
Что? Нет, в директорию можно положить хоть миллионы файлов.
Ну как? Я думаю, что миллионы файлов в директории помещаются все-таки в таблетах размером 100 мегабайт.
У тебя же записи очень маленькие, это важно здесь.
Про метод данные говорим.
Нет, а данные, они имутабельные из чанков.
Нет, мы говорим про метод данные.
Директория одна, но файлы, конечно, не могут поместиться в один таблет,
но как бы данные-то и не в таблетах хранятся в конце концов, они хранятся в чанк-сторе.
А метод данные помещаются в один таблет, поэтому можно с этим жить.
Ну и плюс Фейсбук пишет, что файлы одной директории в конце концов размазываются по разным таблетам,
потому что у них там разные файлы ID, поэтому когда они читают даже параллельно файлы из одной директории,
они хорошо распределяют нагрузку по всему киеварю-хранилищу.
Ну в общем, такая вот идея.
Понятно?
Что мы научились масштабировать киеварю-хранилищу,
положив метод данные и киеварю-хранилищу в киеварю-хранилищу,
и мы научились масштабировать файловую систему,
положив и метод данные тоже в киеварю-хранилищу, но правда аккуратно положив.
Следующий шаг.
А вот теперь мы поговорим про бектейбл.
Смотрите, мы сделали DFS через киеварю.
Теперь забудем во всем этом, в смысле, что мы масштабировали DFS, вернемся киеварю-хранилищу.
И подумаем, насколько разумен этот дизайн, который у нас был придуман.
Вот что мы делали?
Где мой карандаш?
Как выглядела вся конструкция для киеварю-хранилища?
Мы брали узлы с дисками и на каждый узел помещали локальное хранилище.
LSM. LevelDB и ROXDB.
Для того, чтобы если машина перезагрузится, она данные не потеряла.
Но машина могла совсем отказать, поэтому мы эти LSM-ы реплицировали между машинами.
Мы решали задачу репликации.
Этот уровень, мультипаксис, он упорядочивал на разных репликах одного таблета в записи в копии LevelDB.
А поверх этого мы сделали еще шардирование.
Посмотрим сюда, насколько эта конструкция разумна.
Она понятна, да?
Но зачем нам здесь репликация?
Потому что мы не доверяем конкретной машине с конкретным LevelDB.
Она рестарта переживет, а вот если она совсем сломается, то мы потеряем ее диск целиком и LevelDB вместе с ним.
Поэтому мы берем этот LevelDB и реплицируем, потому что он живет поверхненадежной файловой системы.
Я бы сказал, что мы это уже сделали.
У нас уже есть файловая система понадежнее.
Она распределенная и отказаустойчивая.
Она реплицирует методанные, она реплицирует данные.
Почему бы нам...
Мы здесь делаем LSM, потом его реплицируем.
Почему бы нам не перевернуть эти два уровня?
Почему бы нам не сделать LSM поверх файловой системы, которая не может потеряться?
Почему бы нам не сделать LSM поверх распределенной файловой системы?
Ну вот ровно так и делает Google.
Ну а что поделать? Придется немного подождать.
Вот устройство их таблета.
Вот это буквально один LSM.
Но вот этот LSM из чего состоит?
Он состоит из лога, из s-tables и из m-tables.
И мы реплицируем всю эту конструкцию вот здесь под проектором,
потому что мы боимся, что мы потеряем диск, на котором лежат лог и s-tables.
Мы берем три диска.
Ну вот здесь мы берем просто файловую систему, которая под капотом все реплицирует.
Поэтому мы кладем в нее один лог.
Ну для каждого таблета один лог и один набор s-tables.
То есть фактически мы переворачиваем два уровня.
У нас теперь репликация не над LSM находится, а под LSM.
Ну у тебя были машинки с levelDB, чтобы надежно хранить большие данные внутри себя.
Но ты и боялся, что ты каждую машину можешь потерять.
Поэтому ты реплицировал эти levelDB через multipax.
Мы говорим, реплицировать автомат.
Автомат в данном случае это levelDB, это локальное хранилище.
Ну метод данных плюс данных.
Метод данных реплицируется через QVAL, мы только что это сделали.
Данные реплицируются через chunk store.
Когда мы писали каждый chunk, мы писали его в три копии в chunk store.
Вот, так что мы теперь не боимся это потерять.
Файловая система отвечает за надежное хранение этих данных.
Единственное, что нам остается сделать, выбрать точку обслуживания.
Потому что непонятно, у кого в голове мем тейбл и кто в этот лог делает записи.
Почему это разумно? Почему такой дизайн лучше?
Понятно ли вам?
Это же очень глубокая идея.
Это важнее, чем алгоритмы, чем оптимизация мультипаксиса.
То есть это тоже важно, но сложность она вот здесь примерно, в этой идее.
Давайте перевернуть. Не делать репликацию поверх LSM, а LSM поверх репликации.
Каковы преимущества такого дизайна?
Блестящее замечание. У нас был слой репликации над слоем хранилища.
И вот хранилище было очень мутабельное. Мы туда писали и читали постоянно.
И репликация была сложная, это был консенсус.
Потому что он упорядочивал конкурирующие апдейты.
Если мы перевернем всю конструкцию, сделаем LSM поверх DFS,
то в файловую систему будут спускаться уже имутабельные чанки.
И репликация будет имутабельных данных.
А реплицировать имутабельные данные намного проще, чем мутабельные.
Ну и намного эффективнее.
У нас была репликация вот здесь.
Она была тройной. У нас были три LSM на трех машинах.
И мы через Multipax их кормили одинаковыми последовательствами команд.
То есть в три раза у нас есть гигабайт ваших данных.
Они превращались в три гигабайта данных на трех машинах.
Когда мы говорили про определенную файловую систему, я вам рассказывал,
что имутабельные чанки не обязательно хранить в трех копиях.
Для того, чтобы получить отказу устойчивости две машины, два диска.
Вы можете использовать erasure coding.
То есть избыточное кодирование, которое будет экономнее.
Вы делите ваш чанк на шесть блоков.
Добавляете к нему еще, к этим шести блокам, три чек суммы специальные
с помощью кода Фридес Ламона.
И вот вы переживаете два отказа, но при этом у вас overhead по диску в полтора,
а не в три раза.
Ровно потому, что у вас данные теперь имутабельные.
Ну да, это не совсем бесплатно, потому что там нужно уметь их восстанавливать,
еще эти данные при потере каких-то блоков чанка.
Но все же вы можете сэкономить себе в два раза больше дисков.
Ну представьте, у вас миллион дисков, а теперь у вас 500 тысяч дисков.
Это очень много.
В смысле денег очень много, дисков много и денег очень много.
И вот просто переставивая два уровня, вы получаете, вы экономите 500 тысяч дисков.
Неплохо, да?
Ну можно еще меньше хранить.
Там коэффициент избыточности не один, не полтора, а там 1.33.
С репликацией через multipax ты такого не получишь, потому что там данные имутабельные.
Второй бонус, что ты логически разделяешь хранение данных и обслуживание данных.
Вот раньше тебе нужно было три реплики, которые там в памяти держат эти memtable в себе,
ну, воспроизводят этот LSM.
У тебя здесь для каждого таблета должна быть только одна машина,
которая хранит у себя в памяти memtable и обслуживает путы и геты.
И при этом эта машина может легко умереть.
Она умрет, не страшно, мы выберем другую, потому что данные не потеряются, данные не нужно двигать.
Вот гораздо легче балансировать таблеты.
Раньше, чтобы передвинуть таблет на другую машину, нужно было данные двигать, как-то тяжело.
Вот сейчас мы можем просто подвинуть, то есть сказать, что эта машина перестанет обслуживать запросы,
живая и другая, а данные все равно в DFS лежат уже, их двигать не нужно.
Ну, или их можно дури прецентрировать под капотом DFS, но это вот уже гораздо проще.
Это вообще очень фундаментально важная идея про разделение точки обслуживания и отказа устойчивого хранения.
Вот вы приходите в облако, заказываете себе виртуалку, я уже говорил вам об этом.
Вы получаете себе конкретную машину с процессором, который исполняет ваш код.
Вот эта машина может умереть.
Но диск под ней не умрет, потому что диск это некоторая абстракция.
Ваша файловая система, ваша виртуалка работает поверх бочного устройства сетевого,
но и в случае Google ваш диск на самом деле хранится в системе Colossus DFS.
Вот здесь та же самая идея, и это все приводит к более гибкому дизайну.
Давайте я еще один пример вам покажу.
У нас в будущем, не таком ударёком, кстати, уже, будет система Spanner.
Это будет разговор про транзакции, про TrueTime, про транзакции, про все вместе.
Но вот смотрите, как устроены шарды в Spanner.
Там тоже есть таблеты, у таблета есть реплика в каждом DC,
и в каждом DC этот таблет, это RSM, все, мультипакс здесь работает.
Но при этом каждая машина, каждая реплика, эта RSM хранит данные не на своем диске,
а хранит данные в Colossus, в файловой системе распределенной.
То есть Google нигде про диски не думает, нигде с дисками конкретными не работает,
потому что они ломаются.
А вот файловая система не ломается, то есть она скрывает все эти сбои отдельных дисков.
Ну что, осилили ли идею?
Сделали K-Value поверх RSM.
Сейчас сделали, соберусь, сделали RSM поверх файловой системы.
Так нет, в этом же идее и есть.
Ну то есть в Google никакая система, кажется, не должна использовать
локальную файловую систему, локальный диск.
Она должна прямо либо косвенно, через другие системы, хранить свои данные в Colossus.
Вот ровно поэтому важно уметь масштабировать распределенную файловую систему.
Блестящее замечание.
Мы продили некоторый цикл.
Это довольно неудобно, да?
Ясно?
Проблема понятна? Повторить?
Мы, смотрите, научились масштабировать.
Мы можем построить просто отдельные K-Value.
Вот сделать, завести много машин, поставить на каждые там быстрые диски,
на каждые машины поставить LSM в виде RocksDB,
реплицировать где-то наши данные, протеционировать их там вот горизонтально,
сделать каждый таблет отдельным RSM,
реплицировать эти LLDB с помощью MultiPax из пределов таблета,
ну вот построить такое автономное K-Value хранилище.
Вот Facebook так и сделала.
Они построили автономное K-Value хранилище без зависимости.
И через него сделали масштабируемую файловую систему.
Вот у них там миллиарды файлов, и там экзобайты данных в них хранятся.
И DFS, Tectonic, файловая система Facebook, они используют ZPDB и K-Value хранилище
для хранения метод данных.
Тут проблемы никакой нет у них.
Но если мы гугл, то мы говорим, что вот K-Value хранилище можно делать эффективнее,
если делать наоборот, не DFS делать через K-Value хранилище,
а K-Value хранилище через DFS.
Потому что мы хотим хранить LSM каждого таблета прямо в DFS сразу.
Тогда получается, что мы не умеем масштабировать DFS.
Потому что мы не можем использовать в Bigtable DFS, а в DFS Bigtable.
Нам нужно что-то выбрать.
На самом деле как? На самом деле можем.
Смотрите, вот у вас есть уже DFS, вы его написали.
Он правда плохой, отказаустойчивый, отказа неустойчивый.
То есть у него есть одна точка отказа мастер.
Но при этом у вас он уже есть.
Почему бы им не воспользоваться?
Сейчас пишите GFS версии 2, который хочет масштабироваться.
В нем может быть очень много данных.
Но как это сделать?
Нужно поделить систему на мета-store и chunk-store.
И зря стерозите эту картинку, chunk-store у нас уже какой-то есть.
И он там горизонтально масштабируется.
Мета-store у нас нет.
В смысле, у нас есть пока отдельная, в GFS версии 1 это отдельная машина.
Ну неприятно, мы бы хотели заменить эту отдельную машину на key-value.
И в это key-value уложить каким-то образом дерево нашей файловой системы.
Ну у нас же есть Bigtable в Google.
Ну вот давайте положим данные мета-store в Bigtable.
Правда Bigtable нужна файловая система ведь все равно.
Но что приятно, что она нужная, а она правда поменьше файловой системы.
Ну то есть вот здесь мы хотим построить систему, которая масштабируется очень широко.
Но у нас есть GFS, которая не может масштабироваться широко,
потому что он опирается в объеме данных в мастере.
Но с другой стороны, мы же что в этом Bigtable собираемся хранить?
Мета-данные большой файловой системы.
А сколько их будет по сравнению с объемом данных?
Ну вот Google говорит, что когда вы строите систему для хранения данных,
то мета-данные у нее это примерно одна десятитысячная от объема данных.
Так вот, вот нам в этом Bigtable для хранения мета-данных GFS версии 2
нужно в десять тысяч раз меньше данных хранить, чем хранится мета-данных, чем данных.
Так что что мы можем сделать здесь?
Как мы можем сделать этот Bigtable?
Сделать его поверх GFS 1.
То есть он не масштабируется, в нем есть точка отказа,
но по крайней мере с масштабируемостью проблемы нет.
Потому что масштабируемости GFS версии 1 достаточно для того,
чтобы вместить все мета-данные для GFS версии 2.
Вот этот GFS хранит мета-данные, этот GFS через Bigtable косвенно.
Но правда есть точка отказа.
Что с ней делать?
То есть вся эта конструкция держится где-то на одной машине.
Вот если она умрет, то...
Не то чтобы система остановится, это неправда, но мы проще поговорим еще.
Но как-то некомфортно.
Либо придумать что-то, либо укреплять.
Давайте переименуем GFS 2 в Colossus, чтобы было масштабнее.
Мы делаем Colossus.
Такой же идеей.
Что?
Можем это?
Ну сейчас.
Ну я про это расскажу.
У них нет статьи про Colossus, но у них есть маленькая презентация,
слайды без записи даже.
Но по ним, мне кажется, я понимаю, что происходит сейчас.
Так вот, что делать-то?
Мы переименовали, пока лучше не стало.
Не всегда помогает.
Вот мы сейчас решаем главную задачу нашей лекции.
Нам нужно промасштабировать файловую систему
через кейвельную хранилищику, которая требует зависимости файловую систему.
Не понимаете, что нужно сделать?
Нужно сделать взаимную рекурсию.
Нужно здесь написать Colossus.
Вот.
Вот.
И это более-менее, ну почти решение задачи.
Что?
Ну ты писал к ним взаимную рекурсию.
Вот функция A вызывает функцию B, функция B вызывает функцию A.
У нас система Colossus зависит от Bigtable,
Bigtable зависит от Colossus.
Ну понимаешь, не всякая рекурсия плохая.
Рекурсия плохая, когда она не сходится никуда.
Если сходится, то нормальная.
Жить можно.
Так вот, смотри.
Что мы сделали?
Мы вот перейдя от этого Colossus к этому Colossus,
что сделали, по сути?
Мы уменьшили объем данных в 10 тысяч раз.
Вот.
И если у нас было 150 байт данных,
ну которые мы хотели хранить вот здесь, вот в этом Colossus,
то в этом Colossus, который хранит метаданные этого Colossus через Bigtable,
сколько данных уже будет?
10 терабайт.
Ну а что мы сделаем дальше?
Ну как бы дальше у нас снова есть Metastore здесь
и Chunkstore.
Ну Colossus он как бы и тут, и тут,
поэтому почему бы Chunkstore не сделать у них общий?
Вот, а Metastore, ну у него уже свой.
Это снова Bigtable.
А Bigtable снова нужен Colossus для того, чтобы работать,
чтобы данные хранить.
А сколько в этом Colossus будет данных?
Вот один гигабайт.
Вот.
Ну давайте еще один раз, еще один хоп сделаем.
Сейчас, ну как бы дело тут не в том, что в оперативке отказу истойчиво.
Мы получим 100, сколько?
1 гигабайт в 10 тысяч.
100 килобайт, да?
Вот.
Ну то есть пам-пам-пам.
И когда у нас останется 100 килобайт данных, мы положим в чабе.
Ну как-то так, да.
Ну то есть в конце концов мы храним вот 100 килобайт этих метаданных,
метаданных, метаданных, метаданных.
Ну вот вы понимаете, что получается.
Ну тут как бы матрешка из Colossus.
Такой каскад.
Ну так нет, базовый Colossus, ну то есть базовый рекурс – это чабе.
Ну то есть нам нужно же от Colossus Frenary просто отказаться,
потому что мы не можем вот таким вот уже кое-что делать.
Ну не нужно просто.
Вот.
Ну а дальше мы вот как бы вытягиваем себя за волосы.
Мы берем Colossus с помощью него, строим Colossus побольше,
с помощью него строим Colossus еще побольше.
Вот.
Это же потрясающе красивые идеи,
и она вот не сводится к тому, чтобы просто добавить много машин
и поделиться все на равной части.
Это вот такой странный каскад, такая рекурсия.
Ну вот bootstrapping.
И вот откуда придумать эту идею?
Понимаете ли вы?
Вот, да, отличное замечание.
Вот представьте, что вы пишете новый язык программирования.
Вам нужно писать комператор.
В смысле не так.
Вы пишете новый язык программирования,
и что вам хочется сделать в первую очередь на этом языке?
Ну, проверить, что он вообще разумен в этом языке,
что на нем можно программу писать.
Вы выбираете программу Compereator.
Пишете на вашем любимом новом языке программирования
комператор для этого языка.
Это довольно сложно, потому что у вас еще нет комператора для этого языка.
Так что вам нечем скомпедиировать ваш комператор.
Поэтому вы что делаете?
Берете C++ и пишете комператор на нем.
Ну, как бы не для вашего языка,
потому что он сложный какой-то, вы его даже не придумали.
Вы пишете комператор на C++
для некоторого маленького,
полного подможества вашего нового языка.
То есть теперь вы научились комперировать
очень простые программы вашего нового языка.
И вот с помощью этого комператора
вы пишете на вашем уже новом языке
с простыми конструкциями комператор
для более сложной версии вашего языка.
Ну и так можно продолжать дальше.
То есть вы каждый следующий комператор пишете
с помощью комператора предыдущей версии.
Обычное дело, так вы постоянно делаете,
если вы пишете комператор.
Только так все и поступают.
Нет, еще раз.
Ты хочешь написать комператор языка Go
на языке Go, а у тебя нет как бы...
Ну, потому что это же все равно что признать поражение.
Ты написал язык, на котором даже комператор
для себя невозможно написать.
Это же провал, это позор.
Нет, это такая как бы проверка
на вообще разумность твоего языка.
Если на нем комператор пишется,
то, наверное, все хорошо с ним.
Ты хочешь сразу Bigtable писать на новом языке, да?
Пишет комператор, и вот так же
ты себя за волосы вытягиваешь.
Вот здесь та же самая конструкция, такой же каскад.
Правда, смотрите, задача еще не решена на самом деле.
Вот представьте, что я
пользователь, прихожу
и говорю, хочу
добавить chunk в колоссус.
Что происходит?
Что происходит,
если мы делаем все неаккуратно?
Помните, как мы делали chunkStore и metastore?
Мы говорили, что там все данные
мутабельные, поэтому нужно добавить новый chunk,
а потом записать его в metastore.
Вот мы создаем здесь новый chunk в chunkStore,
и должны добавить
этот chunk в metastore,
в inode файла.
Запись в metastore — это запись в Bigtable.
Запись в Bigtable —
это append в log
lsm, который хранится
в GIF в новом колоссусе.
Append в конец
этого лога — это запись
еще одного chunk в этот колоссус.
Ну и в итоге мы так начнем
спускаться по этой
рекурсии взаимной вниз,
пока не запишем что-то в чабе.
Кажется, что это не будет работать.
Мы хотим, чтобы записи,
аппенды, которые случаются много,
не трогали вот эти колоссусы.
Поэтому
на самом деле chunkStore должен быть более
сложным, и chunkStore должен
уметь мутабельные чанки.
То есть у нас есть chunk,
и мы должны уметь
в него добавлять
новую порцию
и не добавляем
это данных,
потому что иначе мы провалимся вниз
до самого чаба.
Вот GFS пробовал так делать.
Там были перезаписи chunk, там были аппенды chunk
и никаких хороших гарантий у них не было.
Но с перезаписями chunk там была проблема,
потому что ваша перезапись могла
попасть на границу chunk, и в итоге
два
primary или два таблиты здесь
будут упридачивать их произвольным образом.
Вот эти две записи на границе.
Colossus
запрещает вам перезаписи.
В нем нет опеды для перезаписей.
В нем есть только аппенды.
У GFS
и с аппендами были проблемы.
Потому что вы
опендили, опендили, потом primary
перевыбирался, который отвечал
за упорядочивание аппендов,
и в итоге новый primary не знал про старый, а какие-то
другие клиенты приходили и делали повторно свои аппенды.
Короче, ничего не понятно.
Как делает GFS
это делает любая разумная файловая система,
но подозреваю, что и Colossus.
Так делает Facebook уж точно.
Они говорят, что у каждого chunk
должен быть только один писатель.
Но это разумно.
То есть, если вы
киевольное хранилище с LSM
и вы пишете в этот LSM
влог новой записи,
то только вы
один в этот лог и пишете, больше никто.
А упорядочивать
записи одного
клиента очень легко.
То есть, не нужно думать, что
кто-то сломается. Просто chunk закрывается.
То есть, если вдруг машина,
которая обслуживала аппендов chunk,
поломается, то chunk просто выявляется законченным.
Если
изменится писатель, то тоже chunk считается законченным.
Поэтому очень легко
делать аппендов chunk
можно делать аппендов chunk
без
взаимодействия с метастором.
И при этом
сохранится голосованность.
Потому что мы ограничили single writer.
Черт возьми, я забыл кусок
лекции рассказать.
Почти все готово, думал. На самом деле,
уже нет.
Давайте я с этим закончу.
Мы пропустили огромную у нас дыра
в понимании.
Но, во-первых, колоссус,
это понятно, как работает.
Про него статьи на самом деле нет.
Но,
смотрите, что есть.
То есть,
можно...
Один раз Google
в этой системе что-то рассказал.
В виде таких скромных
слайдов, и там
информация очень компактная.
Вот мы хотим
масштабировать это данные.
Но при этом для Bigtable нужна своя файловая система.
Поэтому они говорят, ну, где же
разместить эти данные?
В GFS нельзя, потому что
не подходят в модели данных.
MySQL
не вариант.
Keywall storage,
локальное хранилище не масштабируется.
Вот есть Bigtable, давайте класть Bigtable.
Ну и рисуем вот такую картинку.
У них есть общий
чанг-стор.
На каждой машине кластера у них
работает сервис, который называется D.
Такой диск, потому что
отвечает за хранение данных локально.
И поверх этих дисков вы можете собирать
вот этот чанг-стор.
И вот вы можете взять этот...
Немного странная
картинка, но в итоге мы сводим все
к GFS, а дальше GFS можно
заменить на...
Ну как бы сделать рекурсию теперь.
Ну и вот
промасштабировав
несколько раз, мы
уместим данные
в чане.
Ну то есть
не то чтобы
по этим слайдам дизайн легко читается,
но если некоторое время подумать и
почитать какие-то другие статьи, это становится понятно,
что они имели в виду.
Но, к сожалению, нигде они об этом хорошо
не написали.
Ну а теперь дыра в понимании.
Смотрите, у нас есть Bigtable.
И
мы отделили хранение
данных LSM.
Данные LSM-а от точки
обслуживания. Вот машина, которая получает
команды и пишет в лог, хранит
меня на table у себя в память.
Ну это вот одна машина.
Что если она умрет?
Что?
Нет, ну в смысле
не то чтобы новая машина не нашлась,
она найдется.
Но у себя теперь, ну смотри,
если эта машина умерла,
то на смену ей пришла другая.
Каким образом?
Распределенные блокировки.
Ну вот,
для этого используется Chabi.
В Chabi таблет
Server берет распределенную блокировку.
Но мы знаем, что распределенные блокировки
это фундаментально сломанный механизм координации.
И распределенные блокировки
могут владеть 2 узла.
Ну, в смысле, по мнению Chabi
только один узел, но сами узлы могут думать иначе.
То есть у вас уже отобрали
блокировку, но вы все считаете, что она у вас есть.
Вот.
И это вот под задачу,
которую нужно решать. В принципе,
ее решал консенсус. Как избавиться от конкуренции
лидеров.
Ну вот, в таком
бай-дизайне вам нужно решать ее на уровне выше.
То есть у вас была машина,
которая обслуживала данный таблет
с данными в GFS.
Она почему-то залипла.
Ну, кстати, почему она залипла, легко объяснить.
Потому что у вас еще не было
HBase. HBase
это open-source реализация Bigtable.
Она написана на Chabi. В Chabi
сборка мусора. Вот.
Ну и сценарий залипания
мастеров, таблет серверов,
это просто бай-дизайн, то, что ожидается.
Вот какой-то тикет
из HBase.
Ну вот они пишут, что
нормально, тут у них
RS это Region Server, то есть это машина,
которая обслуживает регион таблицы,
регион этот таблет. Вот. Ну и
разработчики ожидают, что машина
может уснуть на ГЦ, потом проснуться и
все еще думать, что она владеет блокировкой.
Ну вот,
для этого
для этого
ну,
это проект Apache, HBase.
Так,
большая картинка не стала, да?
Ну вот, нужно делать
fencing, то есть нужно,
чтобы каждый новый
таблет сервера, который обслуживает
таблет, понимал, в какой он и пофиг
находится относительно других.
Вот. Ну и...
Супер мелко, конечно, не видно ни черта.
Но проблема решается
на уровне файловой системы.
То есть, смотрите,
это важная деталь, мы ее
упустили, к сожалению, в середине реакции, очень жаль.
Вы берете блокировку
в чабе.
Вы таблет-сервер, который
обслуживает таблет большой таблицы.
Вот.
А пишете данные файловую систему
во внешнюю систему.
Так что чабе
может знать уже, что блокировку у вас забрал.
Но
пишете вы не в чабе после этого файловую систему.
И вот уже на уровне файловой системы вам
нужен какой-то механизм, который позволит
файловой системе защититься от старого лидера.
Ну вот, это
механизм lease. То есть,
если у вас был старый лидер, он открыл файл
в HDFS.
В HDFS есть только один писатель.
Ну и сейчас я такую виртуальную схему
рассказываю. Необязательно так сделано в HBase.
В HBase сделано немного не так.
Но можно себе представить такой механизм,
что тот, кто владеет логом,
владеет LSL.
Вот вы открыли, вы получили,
вы взяли блокировку в чабе,
вы стали лидером таблета,
открыли лог на запись, пишете в него.
Если вдруг
вы залипли
и в системе выбился другой лидер,
то он переоткрывает
лог в DFS.
И тем самым он
инвалидирует дескриптер у
первого лидера.
И первый лидер все еще думает, что он
лидер, пытается в лог
что-то записать, какую-то новую мутацию.
Но система HDFS
говорит ему, что все, ты опоздал,
у тебя уже отобрана риза.
То есть, кто-то другой уже открыл файл
после тебя.
И у тебя право на запись отобрали.
Вот такую задачу нужно решать, если вы
используете такой дизайн в целом с PowerDFS.
То есть, вам нужно на уровне чуть выше
решать задачу
защищаться от старого лидера,
от их конкуренции, как-то его
нейтрализовать.
В HBase, кажется, это
решается переименованием директории.
Это важная деталь, потому что
Facebook файловая система умеет
отомарность на одной директории.
Это может быть важно, если вы строите такие протоколы.
Ну что ж,
какие уроки мы извлекли
из этой лекции?
Уроки не
тривиальные.
Первый урок простой,
что нужно всегда искать
узкое место в системе.
И узкое место в наших системах
это всегда методанные.
А как вы собираетесь
сделать это?
Ну что ж,
уроки не тривиальные.
А как вы собираетесь масштабировать
это узкое место?
Тут уже есть варианты.
Вы можете в обоих случаях
как-то наивно делить,
шардировать это методосостояние, придумывать какие-то
не то чтобы костыли, но что-то неудобное.
А вы можете проявить некоторую
изобретательность вместо этого.
И положить данные Bigtable и Bigtable, и придумать идею,
которая похожа на
виртуальную память в операционных системах.
Или придумать вот такой вот bootstrapping,
как в компеляторах.
Не первый и не второй, кажется, тривиальный не является.
И не сводится к тому,
что мы просто добавляем машину для масштабируемости.
Нет, мы делаем что-то иногда очень хитрое.
Ну и
такая сложная мораль,
что
трудно
смотреть
на историю последних 20 лет
и на Google,
очень сложно представить, что вы в конце концов придумаете такое.
То есть как бы тут одна ошибка,
одно неправильное движение,
и вы построите совсем другие системы,
и вот такая конструкция уже не сработает.
Как это сделать, не совсем понятно.
Это сложно.
То есть как выстроить
такую глобальную декомпозицию
на сервис-координации,
на кивалию хранилища поверх файловой системы,
на файловую систему через кивалию
хранилища, через файловую систему.
Вот как бы вся эта конструкция,
одна большая целая, она сама по себе
очень нетривиальна.
А люди придумали ее впервые,
только они ее сделали, по сути.
Повторить ее сложно, но и кажется,
что на самом деле неразумно, потому что даже
вот Facebook не пытается.
Проще делать систему, которая не имеет внешних зависимости.
А тут мы взяли
две очень большие сложные системы и научились
делать их друг через друга.
Наверное, потому что они уже были, но все же,
то, что это получилось так сделать,
это, по-моему, поразительно.
Ну и это такой совершенно
отдельный навык, в смысле
масштабирование, проектирование.
То есть он не сводится к тому, что мы там
оптимизируем какие-то сложности, какие-то
маленькие задачки решаем.
Вот тут какой-то совершенно
другой способ мышления
подходит к задаче.
Ну что, мы решили
теперь, мы умеем делать
локальные хранилища с вами.
Мы умеем реплицировать, ну почти умеем
остаться написать.
И мы умеем
строить распределенные системы
из этих вот отдельных
отказоустойчивых единиц
RSM-ов.
Вот таким способом, вот таким способом.
Что нам осталось? Нам осталось
делать транзакции и получить Google Spanner.
Вот этим мы займемся
в ближайшее.
Ну, через неделю
мы начнем этим заниматься, через две недели, вернее.
Что?
Я думаю, что...
Ну,
не то чтобы каждый байт,
который хранится, хранится в колоссусе.
Я думаю, что это, ну может быть и не так, конечно,
но это почти так.
Ну, это разумно. Ты как бы избавился
от... Ты не думаешь нигде
про сбойные диски.
Ты везде работаешь с отказоустойчивой файловой системой.
В чем мотивация
не использовать ее?
Она позволяет себе отказоустойчивой
и очень экономно хранить данные.
Но она, конечно, оптимизирована
для того, чтобы все быстро работало.
Она может работать даже
быстрее, чем твой жесткий диск, который может затупить.
Ну, потому что там
один жесткий диск может затупить, а вот там,
не знаю, много жестких дисков разом,
им затупить сложнее.
Ну да, тут еще нужно учесть
такой нюанс инженерный, что
с одной стороны,
мы писали локально раньше в свой диск,
а теперь мы ходим по сети, пишем в какой-то другой диск,
в какой-то другой системе.
Но на самом деле у тебя система
Bigtable и система Colossus,
как бы их узлы могут располагаться
на одной и той же машине физически.
То есть ты можешь буквально писать
на свой диск просто через такую цепочку абстракций.
Если система это учитывает,
если там менеджер, который
оставляет эти сервисы по машинам,
это учитывает, а он это учитывает,
то ты можешь оверхедить большого и не
получить.
Вот так что разумно все, что
ты хранишь, хранить в такой вот файловой системе.
Ну и кажется, что вот любая,
если ты не используешь Colossus прямо, то ты используешь
Colossus косвенно, потому что вот Spanner
база данных,
ты хочешь использовать ее, потому что тебе
удобно с таблицами работать,
но ты кладешь данные в Spanner, Spanner
ты кладешь данные в Colossus все равно,
ты не можешь его избежать.
У Black Colossus он
еще не один, да?
Он свой на каждый датацентр.
То есть Spanner это обычный RSM,
то есть каждый кусочек
этого Spanner, каждый кусочек твоей таблицы
это три реплики, они находятся в разных дец,
но при этом каждая реплика работает не со своим диском,
а хранить данные в своем Colossus.
То есть у тебя коэффициент репликации не три, а три умноженные
на коэффициент репликации внутри Colossus,
а он может быть там не знаю, полтора.
Когда мы читаем откуда?
Ну нет, конечно.
Вот смотри, мы спускаемся в Colossus ниже,
раз там, грубо говоря, в 10 тысяч операций.
Сейчас, ну подожди, если у тебя данные имутабельные,
подожди,
давай в порядку, если мы говорим про key value,
то в кэше хранились
адреса
узлов, которые обслуживают
таблет служебные,
но и таблеты твои.
Это просто адреса узлов, которые обслуживают твои запросы.
Если они вдруг перестали
обслуживать твои запросы, они об этом скажут тебе,
и ты попробуешь заново, пройдешь через чай,
через всю таблицу страниц заново.
То есть тут ничего, никакой проблемы нет.
Если у тебя данные вообще имутабельные,
то тоже проблем нет, если у тебя данные там немного
имутабельные, то
проблема тоже решается.
Ну кэше используется, конечно,
без кэше это все
не работает. Ты к тому, что кашировать нужно аккуратно? Да, нужно аккуратно кашировать.
Ну, смотри, вот от того, что у тебя здесь получился такой странный каскад сложный,
это же не означает, что ты вглубь его ходишь каждый раз.
Ты его чаще всего не покидаешь в первый уровень, то есть ты работаешь как с обычной файловой системой.
Просто иногда ты спускаешься на уровень Рижа. В Чабе ты спускаешься супер редко.
Да, забыл сказать, что в принципе система может лишиться Google-Чабе,
он отказаустойчивый, но можно потом, не знаю, 5 репель взорвутся.
Так вот, это не остановит конструкцию все.
Потому что, в принципе, пока ты пишешь в Чанке и не создаешь новые,
тебе даже не обязательно с Меда-Стором разговаривать.
Так что система может жить без Чаби вот на этом уровне очень долго,
пока кто-нибудь заметит и починит.
То есть спускаться в нее часто не нужно,
потому что она не сможет выдержать такую нагрузку,
как выдержит колоссус все равно.
Чего?
Нет, подожди, Чаби он в основании лежит, он про все, что выше, ничего не знает.
Чаби это RSM.
Данные, которые лежат в Чабе, они, слава богу, хранятся на его дисках.
Нужно остановиться когда-то.
Это просто обычный RSM, ты берешь дерево с локами,
с какими-то маленькими записями размером 100 килобайт-мегабайт.
И ты реприцируешь его на семи машинах, в семи регионах.
Ты надеешься, что они разом не откажут.
Точнее, большинство будет доступно.
В чабе есть кэши, я предлагал тебе статью почитать для этого.
Там кэши есть, чтобы к чабе часто не ходить.
Они там сделаны определенным образом, довольно странно.
Там некий протокол когеренности, если ты помнишь, что это.
Если ты идешь в чабе, что ты пишешь,
то чабе берет и инвалидирует кэши у всех клиентов.
Довольно странный дизайн.
Ну что, тогда перерыв.
Кажется, вопросы кончились.

Метод Ньютона и квазинтонские методы. Метод Ньютона сходится очень быстро, но требует больших ресурсов.
Квазинтонские методы сходятся тоже чуть-чуть медленнее, но требует существенно меньше ресурсов.
Сейчас будем понимать, как это все дело строится, в чем там специфика работы и как это все можно вызывать из стандартных пакетов
и каких-то задач, которые могут возникнуть на практике.
Метод Ньютона. Смотрим на задачи минимизации выпуклой функции. Идея в чем. Метод является методом второго порядка.
То есть для его получения, для заказа и выражения, который будет давать пересчет в следующей точке, надо знать дисян.
То есть функция должна быть дважды деференцируемой.
Это важно. Если это так, то мы можем построить в данной точке x квадратичную аппроксимацию с крышкой.
Такой вид имеет. Просто второй порядок.
Если дисян строго положительно определен, то есть функция является строго выпуклой,
тогда мы можем записать вот такую задачу минимизации, то есть заменить функцию исходную на квадратичную аппроксимацию
и сказать, что давайте мы будем в данной точке минимизировать эту функцию по h, тем самым надеясь получить направление,
по которому надо двигаться для ее уменьшения.
Ну и соответственно, поскольку это все выпукло, то условие первого порядка дает нам, что оно нам дает.
То есть он дает то, что в точке минимума градиент равен нулю.
И это также является достаточным условием.
То есть мы записываем градиент вот этой функции, приравним его к нулю и выражаем аж на звездочку.
Отсюда аж на звездочку имеет такой вид, что это в общем эта штука является процедурой решения систем линейных уравнений.
И метод Ньютона записывается вот таким вот образом в рамочке, что мы из текущей точки перемещаемся в следующую,
по направлению, которая является решением линейной системы с матрицы Гесиана и минус градиент правой части.
Понятно ли это?
Этот слайд. Или есть какие-то вопросы?
Прекрасно.
Важный момент, что вот это направление нужно считать не как вычисление обратной матрицы и умножение ее на вектор.
Для того, чтобы решать линейную систему, не нужно считать обратную матрицу.
Нужно сделать иллю-разложение, так называемое, которое факторизует матрицу на нижней верхней треугольной.
И потом решает набор систем с такого рода матрицы.
Это делается за 1 ватт на квадрат.
Но понятно, что само разложение будет стоить 1 куб.
Как и стандартная процедура решения линейной системы.
Это канонический метод Ньютона, про который будут следующие несколько слайдов.
После чего мы перейдем к его модификациям и, соответственно, к Возюмтовским методам.
После того, как поймем, в чем проблема метод Ньютон.
Метод Ньютон также вы могли встретить в теме про решение системы нелинейных уравнений.
Например, у вас есть вот такая вот система, и вам надо найти X.
Тут была немножко другая история. Вы считали приближение линейное.
То есть тут G штрих этой матрице якобы.
И получали delta X. И по таким же ровным правилам.
Только вместо решения системы с градиентом, тут было решение системы с якобы.
И соответственно, метод Ньютона был таким.
Какая у этого метода связь с нашей задачей?
Очень простая. Если у нас срива функция выпукла, то условия оптимальности записываются вот таким вот образом.
А это есть не что иное, как нелинейных уравнений.
Поэтому, когда мы ищем направление, чтобы получить направление движения метод Ньютона,
то это то же самое, что и направление для поиска корня функции G.
Тем не менее, несложно заметить, что метод Ньютона для решения уравнений является более общим.
Как вы думаете, почему? То есть это более общая штука, чем метод Ньютона для решения задачи оптимизации.
Возглядя на этот слайд, можно внимательно на него посмотреть и увидеть, в чем там разница.
Есть идеи? Не очень.
Окей. Смотрите, в чем дело.
Когда здесь мы записываем условно икебян, то в случае, когда у нас система, то этот икебян вырождается в гессианах.
Однако никто не сказал, что... Ну и про гессиан мы знаем, что это симметричная матрица, как минимум.
Положить на определенный, в случае выпуковом случае, все понятно.
Но когда мы рассматриваем эту задачу, то, что вот тут матрицейкой будет быть такими свойствами, никто не сказал.
То есть, в общем случае, тут может быть какая-то несимметричная матрица, которая будет довольно хитро устроена.
Поэтому анализ метода Ньютон для решения нелинейной системы уравнений более сложная задача, чем его анализ для решения задачи оптимизации.
В общем случае, сходимость нетривиальна исследования.
И можно... Тут есть ссылочка на фракталы Ньютона.
Это такая штука, которая возникает в процессе интеррирования методом Ньютона для самых простых уравнений.
Если интересно, можете посмотреть, тут довольно красивая картина.
Что известно про сходимость метода Ньютона?
Первое результат, который мы будем получать, это предположение, что у нас положительный определенный кисян, строго.
Если это неправда, то метод перестает работать, имеется в виду, что надо его как-то подкручивать так, чтобы он не...
Ну, в случае, если функция не выпукла, он вообще может оказаться в...
Ну, не выпукла и не ограничена снизу, он может вообще улететь без бесконечности, и ничто его не спасет.
Если же функция просто выпукла, и там есть некоторые критические точки, где кисян 0,
то там можно его некоторым образом регулиризовать так, чтобы двигаться в epsilon-окрестности этой точки,
где кисян всегда положительно определен, но довольно плохо обусловлен, скажем так.
Значит, сходимость локальная, то есть в зависимости от выбора x0, метод может сходиться, расходиться или эсценировать.
То есть тут такой довольно капризный метод, который не для любого x0 будет сходиться,
в отличие от градиентного спуска, который в случае выпуклой функции у нас сходился из любой точки.
Для того, чтобы заставить его сходиться из любой точки, его специально подправляют, а именно добавляют шаг,
который также подбирается некоторыми адаптивными техниками, и это все называют демпфированный мета-нютн.
Но создемпфированный, потому что направление немножко шкалируется на этот самый альфа-кат.
И выбирать этот шаг, выбирать его по наложенному спуску, чтобы там было достаточно существенного убывания,
правил армии, все эти истории, они в этом случае также начинают работать.
Введение шага существенно расширяет обысходимости, то есть он начинает сходиться из любой точки,
но скорая исходимость начинает страдать. Сейчас посмотрим, как именно это происходит.
Во-первых, при всех этих несильно обремительных условиях можно получить локальную сверхлинную исходимость.
Как это сделать? Пусть в локальном минимуме градиент 0, 10 положительно определен,
тогда, расписав ряд Тейлора в точке их со звездочкой, мы получим вот такое приближение для функции, которая равна градиенту.
Понятно, откуда это взялось? Почему это так выглядит?
Вот тогда, поскольку у нас есть строгая положительная определенность,
то умножим обе части на обратный гессиан, мы получим слева, перенеся все необходимое в левую часть,
мы получим здесь разность xk и x со звездочкой, а здесь получим решение системы матрицы гессиана и минус градиент правой части.
И эта штука равна умалому от нормы разности x со звездочкой и xк.
Поскольку мы знаем, что тут у нас по сути дела записана одна итерация метода Ньютона.
Таким образом, мы видим, что у нас справедливо следующее равенство, что xk плюс 1 минус x со звездочкой это умалое от нормы разности x со звездочкой и предыдущей точки.
Это значит, что предел отношения равен нулю.
Это значит, что сходимость сверхлинейная, потому что если бы она была линейная, то предел был бы равен Константику, которая фигурировала в линейной сходимости.
Это понятно? Почему так произошло?
Хорошо. Получили пока что сверхлинейную сходимость при его несильно обремительных условиях.
Теперь давайте получим локальную квадратичную сходимость, более сильный результат.
Пусть тут уже поскольку результат более сильный, то и условий надо побольше.
Требуется локальная сильная выпуклость с константы мю, то есть чтобы в x со звездочкой гессиан был отделен, его спектр был отделен от нуля на константу мю.
Также нужно липшить его с гессианом с константы м, и близость x0 к x со звездочкой вот настолько.
Тогда есть квадратичная сходимость, то есть норма между x ка плюс 1 и x со звездочкой меньше либо равна, чем констант, умноженный на норму в квадрате между x ка и x со звездочкой.
То есть вот этот квадрат как раз таки гарантирует квадратичную сходимость и удвоение числозначих цифр в записи на каждой итерации.
На графике скоро, буквально здесь, можно увидеть как именно отличается сходимость градиентного спуска и метода Ньютона.
То есть градиентный спуск сходится линейно, вот она линейная сходимость, метод Ньютона сходится квадратично, локально и линейно в удалении от точки мимо.
То есть смотрите, что происходит.
Тут график для демпфированного метода Ньютона, поэтому здесь мы шаг подбираем, и он не равен единице, то есть он меньше единицы.
Поскольку мы его начинаем итерировать, начинаем подбирать единицы.
И потихоньку, если у нас наше направление приводит к возрастанию значений, то мы делим шаг на два условия, на три.
Умножаем на какое-то число меньше единицы.
Что дальше? Дальше мы пришли в некоторую окрестность, и уже в этой окрестности мы начинаем сходиться квадратично.
То есть видите, вот уже начиная отсюда, тут было условно 10 в первой, тут стало 10 в минус первой, здесь стало 10 в минус третий, четвертый, и здесь стало 10 в минус восьмой.
То есть каждый раз норма градиента становится меньше в квадрат раз.
В отличие от того, как выглядела сходимость для градиентного спуска, когда мы просто эту норму умножали на какое-то число меньше единицы.
Понятно ли в чем разница линии неквадратической сходимости?
Теперь доказательства. Доказательства 9 шагов.
Значит первое, введем необходимое определение.
rk плюс первое это будет наша ошибка, которая будет связана с rk вот таким вот образом, потому что если подставить, то станет ясно, что здесь стоит просто rk.
А тут остается выражение для направлений.
Теория Манюттона Лейблица. Напоминаю про интегралы производную и значение.
Ну в общем то, как интеграл от функции на отрезке можно выразить через значение на краях.
То же самое можно записать для градиентов.
Градиент в точке xk это градиент в точке xk минус ноль, то есть градиент в точке x звездочкой.
И это интеграл от f2' вот это вот ровно то же самое, что и вот это.
То есть тут теперь f2' точка где-то между xk и x звездочкой, и это умножается на rk, то есть на xk минус x звездочка.
Вот там общение формулы Манюттона Лейблица на многомерный случай.
Отставляем вот сюда. Вот и получаем следующий результат.
Что rk плюс первое это идентичная матрица, минус обратный гисян, умноженный на вот такую штуку интеграл и умноженный на rk.
В силу того, что такое матричная векторная норма, у нас есть свойство публикативности.
То есть норма rk плюс первого ограничена сверхпроведением норм ЖКТового и РКТового.
Все ли понятно, что было на этом слайде?
Да.
Прекрасно. Теперь нам осталось разобраться с тем, как норму ЖКТ оценить.
Тут нам пригодится Лейблица с гисяном. ЖКТ у нас вот такая вот штука.
Поэтому, что мы делаем?
Мы вот эту идентичную матрицу переписываем как произведение гисяна и обратного.
То есть обратного и гисяна. После вынесения обратного гисяна мы получаем интеграл такой разности,
что, соответственно, после взятия нормы превратится в произведение нормы вот этого на интеграл от нормы разности.
А эта штука фигурирует у нас в липчество из гисяна.
То есть это m на норму разности аргументов.
А норма разности аргументов это по сути rk минус t на rk.
Отсюда получаем оценку.
Тут, по-моему, квадрат где-то потерялся.
Нет, квадрат не потерялся, все в порядке.
Получаем вот такую вот оценку.
Смотрите, что получилось.
Тут фигурирует норма rk, а ЖК мы оценили сверху вот так.
Оценили, по крайней мере, вот тот кусочек.
То есть тут еще одна rk появилась.
Теперь осталась норма гисяна в минус 1 оценить снизу и все будет хорошо.
Это нам позволяет сделать липчество или гисяна и сильный выпуск.
Поскольку у нас справедлива следующая оценка на гисян,
то это следует напрямую из-за липчества с гисяном.
Вот что, раз у нас норма разности меньше либо равна, чем m на rk t,
значит сами гисяны в тех точках, которые рассматриваются, связаны вот таким вот образом.
Что в свою очередь к силу сильной выпуклости можно записать вот так.
То есть мы вот это вот выражение заменяем на mu умножить на единичную матрицу.
Соответственно, когда мы будем оценивать норму,
нам нужно оценить норму обратной матрицы к вот этой,
то мы получим вот такой вот неразь, просто единицы делить на норму вот этой штуки.
Ну, собственно, вот все, что нам нужно было получить.
То есть складывая все вместе, мы получаем итоговую оценку,
что тут у нас m на r квадрат и тут деление на 2 умножить на вот это выражение.
Собственно, вот эта вот оценка, она откуда берется?
Она берется из требований, чтобы вот в начальной точке у нас
величина, на которую умножается сама норма ошибки,
так это надо написать лучше на доске.
Сейчас я напишу.
Оп.
Так, это не то, это не то.
Так, так, так, так, так, так.
Ой.
Так, все получилось.
Share screen.
Так, сейчас плагин надо установить.
Я надеюсь, не придется сейчас никуда переходить,
перевыходить и все заработать.
О, все заработало, ура.
Ой, что такое?
Так, да, я забыл, что тут никогда ничего не работает.
Так, сейчас секундочку, я в другой части подключусь.
Так, ага, вот так.
И...
Вот так.
Сейчас, я надеюсь, может, немножко пропаду, но надеюсь,
немного долго.
Если даже и пропаду.
О, все, сейчас все точно будет хорошо.
Так.
Ага.
Убирайся, ничего не работает.
Убирайся.
Убирайся, ничего не работает.
О, ура, все заработало.
В итоге, наше исходное выражение имело вот такой вид,
Сейчас, если вы ничего не видите, ничего страшного, я сейчас пишу с слайда, mu-m, вот так, вот, так, тут стереть, конечно, вот, и нам надо, чтобы, вот, в итоге-то, вот эта штука, это что такое, это m на норму xk-x'
умножается на норму xk-x'
вот, вот эта штука должна быть меньше единицы, вот, если мы явным образом расписываем это требование
вот, и потом сделаем необходимое преобразование, ну, то есть, тут как бы вместо k поставим x0, например, да, что на первой итерации мы хотим, чтобы у нас x1-x' было меньше, либо равно, чем там что-то умножить на x0-x'
и вот это что-то должно быть меньше единицы, это довольно естественное желание, чтобы расстояние все-таки уменьшилось к x'
вот, тогда, перенеся влево-вправо, получим, что x0-x' меньше, либо равно, чем 2 mu делить на 3 m
понятно? откуда это взялось? да
хорошо, возвращаемся к слайдам тогда
так, это была теорема про сходимость метода Ньютона
значит, какие у метода Ньютона достоинства и недостатки?
достоинства, трагичная сходимость, высокая точность решения, и, ну, вот, афина и неврядность перечисленная, но, в общем, это история, которая говорит о том, что метод не зависит, не меняется при афином преобразовании координат
то есть, в градиентном спуске это не выполняется, вот, в метод Ньютона это выполняется
вот, что плохого?
довольно много всего плохого
во-первых, надо гессен хранить, это инклады по памяти
надо системы линейных уравнений решать, это n-куп
ну и, там, гессен может быть вырожденным, но это вроде с какими-то некоторыми специальными решениями лечится
но все равно нельзя сказать, что это прям легко и просто
как это чинить?
чинить надо следующим образом
поскольку мы знаем метод, который лишен всех этих недостатков, а именно этой градиентной спуску
то давайте посмотрим, что у них общего
и попробуем найти некоторый баланс между достоинствами и недостатками этих двух методов
значит, когда мы получали градиентный спуск, то
мы основывались на следующей ассоциации сверху
тут вместо единицы налихо была L
ну и мы получали, что максимальный постоянный шаг это единица налихо
то есть тут, по сути дела, тоже была использована квадратичная оценка
но с единичной матрицы
тогда у нас, с нашей звездочки, это просто минус альфа на градиент, получаем градиентность
метод единицы был все то же самое, только вместо единичной матрицы стоял 10
и мы получали метод налихо
можно налиф поправить
и нам нужно что-то, что будет по скорости сходимости лучше, чем градиентный спуск
то есть будет больше, чем линейная сходимость
но по времени одной итерации будет быстрее, чем мета ньютона
понятно ли идея и то, на чем она будет основана?
ну, на чем, собственно говоря, как мы будем строить?
непонятно то, что мы хотим, а как строить, пока непонятно
ну, погодите, да, сейчас как только понятно, что хотим, то и как строить станет, я скажу
возьмем толстые методы, общая схема
оцениваем нашу функцию квадратично, но подставляемся на некоторую матрицу B
которая что-то среднее между единичной и 10
чтобы быть лучше, чем градиентный спуск
но быстрее, чем метод ньютона, поскольку матрица B будет обладать специальными свойствами, которыми мы сейчас получим
значит, соответственно, минимум достигается в той же самой точке, минус Bk, минус первый градиент
и квазинцовский метод – это вот такая вот штука
то есть как смартфон BkT, собственно, будет посвящена оставшейся несколько слайдов
ну, там, не оставшиеся, но следующие несколько слайдов, скажем так
обозначение, которое тут используется
BkT – это оценка на градиент, hkT – это оценка на гессиан
hkT – это оценка на обратный гессиан
далее будем это использовать
знаете, какие требования к BkT мы предъявляем?
нужно уметь быстро пересчитывать Bk плюс первое на основе BkT
при условии, что у нас есть только градиенты, то есть мы хотим ограничиться первым порядком
то есть вычислять только градиенты, никаких гессианов нам не надо
но на основании градиентов строить оценку на гессиан
это первое
второе – быстрый поиск направления, то есть вот эта вот система должна решаться быстрее, чем за n-куп
тогда мы удовлетворим нашим требованиям, что
наш квазинтулский метод будет работать быстрее, чем метод Ньютона
по скорости одной итерации
и надеяться, что
и с верхней линии сходить еще должно быть
ну и BkT еще желательно как-то компактно хранить, но про это будет ближе к концу лекции
отдельная модификация про компактное хранение
ну давайте потихоньку эти требования думать, как их удовлетворить
да, ну тут сайт про немного истории, я обычно рассказываю про то, что
сначала его предложили физики, потому что у них ничего не работало
потом хотели вопубликовать, ничего не опубликовалось
30 лет было припринтом, все уже стали активно использовать, кучу модификаций получили
в итоге только в 1991 году опубликовали в первом номере, вот я вам скажу, журнал по оптимизации эту работу
в общем, такая вот хитрая история этих методов
значит, обновление Bk
правила двух градиентов
звучит так, что, во-первых, градиент нашей квадратической оценки
вот этой, в нуле, должен быть равен точному градиенту в точке xk
ну и это так получается просто по построению
потому что градиент fq равен вот этот градиент плюс Bk на h
ну h равен нулю, Bk на h да нуляется, это просто градиент, это по построению квадрата
но, поскольку мы знаем предыдущую точку
то мы можем сказать, что если мы, имея эту функцию, шагнем по направлению
которая приведет нас к предыдущую точку
то мы должны получить точный градиент в этой предыдущей точке
то есть, если мы шагнем назад
то нам надо получить значение в точке xk
значение градиента в точке xk
отсюда, после подстановки
получаем, что разность градиентов
ну, получаем такое уравнение, короче говоря
или, ну и в общем-то, квазинтонским уравнением
которое записывается с помощью введения следующих обозначений
с
это разность х
двух соседних
а y это разность градиентов в соответствующих точках
поэтому
уравнение на B вот такое
то есть Bk плюс 1 на h должно быть равно y
вопрос
софт 2
всегда ли оно имеет решение
и всегда ли оно единственное?
как вы думаете?
ну, это зависит от матрицы B
давайте еще раз посмотрим, что это уравнение задает
ну, что вот здесь известно, а что неизвестно?
без каты неизвестно
почему?
мы же знаем текущую и следующую точку
точнее, текущую и предыдущую
а, оно относительно B, что ли, получается?
ну да, это уравнение на B, совершенно верно
ну, хорошо, теперь давайте на вопрос ответим
ну, оно всегда имеет решение, но не единственное может быть
ну, смотрите, про всегда этот вопрос
поскольку он на матрицу B
помните, было еще такое вот условие?
а, точно
поэтому давайте, когда все может сломаться?
ну, когда там s-каты на y-каты, если там меньше нуля будет получаться?
да, именно так
то есть вот это вот требование того, чтобы угол между s-катом и y-катом был тупым, да?
то есть острым
то есть если вдруг эта штука меньше нуля, это произведение
то все плохо
поэтому нам нужно специально шаг альфа подбирать так, чтобы вот это вот скалярное произведение было правильного знака
это отдельная история
да, допустим, это выполнено
значит, теперь про единственность
да, нет?
алло?
вы тут?
да
хорошо
что думаете про единственность?
как вообще понять, будет ли решение единство или нет?
что надо посмотреть?
если решение не единственное, то выражение будет получаться
кто выраженный?
я забыл, что мы решаем
вы еще не проснулись, что ли? У вас же перед этим пока лекция еще должна быть
да
давайте, соберитесь
уже почти 12 часов
ну
на что надо посмотреть, чтобы понять единственность или нет?
что при этой единственности с решением?
допустим, у вас обычность тем или иным уравнения
а х равно b
когда решение единственное?
относительно х?
да
когда?
ну, когда они выражены
а если выражено, то что?
если выражено, если есть хоть одно решение, то там их бесконечно много будет
ага
а теперь давайте, то есть
тут как бы, наверное, поскольку у нас-то задача на b
что там? b
bs равно y
но считайте, что b это х
то есть эта штука
n на n
это n, это n
какие системы линиевых уравнений имеют единственное решение?
а какие нет?
ну тут я не понимаю, это относительно b, это непонятно
ну а что непонятного?
давайте распишем
что b11s1
плюс там b12s2
там b13s3
например, да?
ну вот тут у вас было a11x1
плюс a12x2
плюс a13x3
а, ну если vs0 есть, то, кажется, не единственное
чего-чего?
если vs0 есть, то не единственное
потому что мы можем соответствующие элементы на диагонали увеличивать там, сколько хотим
кажется, никакой положительной определенности там не испортим
ну или что-нибудь такое
ну, на самом деле, ответ более...
опираюсь на более общий факт
о том, что если у вас число уравнений меньше, чем число неизвестных
ну, типа, ax равно b, но матрица у вас вот такая вот
то есть число уравнений, условно, там 10
а матрица 100 на 100
ой, тут я криво нарисовал, вот так надо нарисовать
то, что тут будет, единственное или не единственное?
ну не единственное
ну не единственное, конечно
ну вот так вот
конечно, конечно
но вот это наша ситуация
у нас n квадрат, ну не n квадрат, а n-1,5, по-моему, да?
или плюс один?
минус один, помню
почему?
ну, от одного до n
от одного до... да, плюс один, правда?
неизвестных сил симметричности
и n уравнений
ладно, я понял, кажется
как бы, вот абсолютно вот этот случай
вот, в итоге
решение единственное, действительно
чтобы сделать его единственным, надо немножко подкрутить
как обычно
этот подход, а именно сказать, что
новая цена 50 должна быть как текущей
то есть у нас
получается
дополнительная задача оптимизации
которая минимизирует
норму B-BKT
и требует, чтобы новая матрица
удовлетворяла вот этому уравнению
вот, сейчас будем потихонечку это формализовывать
значит, необходим B0 задать нам
обычно B0 задать как гамма на единичную матрицу
параметр в процедуре поиска шага
значит, все вычисления, которые мы будем проводить
надо сделать так, чтобы они имели сложность
меньше, чем n-1
потому что мы хотим быть быстрее
на каждой итерации метода ньютона
ассигнатической
ну и, собственно, примеры, которые мы сейчас
пытаемся успеть разобрать
это Бразилай Бурвейн, ДФП и БФГС
и его версия заграничной памяти
метод Бразилая Бурвейн
очень простой метод
опроксимируем гессиан диагональной матрицей
направление нашего гридетом методе было вот таким
его можно записать как
альфа, кан и единичную матрицу на гридиан
вот эта штука, в свою очередь, есть не что иное
как обратная матрица к вот такой матрице
вот, а это так, тут опять лишняя скобка, я все никак не поправлю
18 слайд
ну, значит, не лишняя, не хватает вот этой стороны скобки
то есть, это примерно обратный гессиан на гридиан
ну, значит, давайте подставим вот это вот условие
вот это вот модель нашего гессиана
в квазинтуловском уравнении
мы получим вот такое вот уравнение на альфу
то есть, нам надо, по сути дела, минимизировать вот норму такой разности
эта задача решается аналитически, она вообще-то ну сколяр
то есть, она просто в квазинтуловский метод
выродился в метод поиска шага
представляете, какая прелесть
что
строили-строили
в простейшем случае
оказывается, это все то же самое, что и шаг подобрать правильный
вот, и
он имеет аналитическое решение
вот такое
то есть, шаг
надо ОЛАТН операции сделать
вообще, конечное число, все детерминировано
никаких там параметров в процедуре подбора шага
ничего этого нет
и все вполне себе замечательно работает
понятен ли подход?
да
отлично
да, значит, храническая модификация, статья на ней 2016 года
на ней все, в общем, рекомендую посмотреть, если интересно
метод DFP, собственно говоря, явно образом решает задачу минимизации
то есть, поиску ближайшей матрицы
текущей
для которой выполняется квазинтуловское уравнение
обычно тут стоит пробедение своего нормы, но это, в общем, как это решается
оставим за скобками
вот, и запишем явное решение
вот оно, оно общеизвестное в литературе
приводится
вот, и, значит
замечательно то, что такая структура матрицы
то есть, смотрите, тут матрица B с левой и с правой обкладывается
типа проекторами
в каком-то смысле
вот, то есть они
выглядят как единичный минус РО на некоторую матрицу РАНГ-1
плюс матрица РАНГ-1
и эту матрицу такой структуры можно обратить
аналитически
по формуле Шерману Моррисона Вудбеля
вот она тут приведена
точнее приведен ответ
то есть, смотрите, чтобы
допустим, у нас есть матрица B0
что нам надо сделать?
нам надо посчитать B0-1
ну, учитывая, что B0 это единичная матрица на константу
обратно считается просто
вот, это наш H0
дальше наш H1 высчитывается вот так вот
давайте посчитаем, сколько операций нужно сделать, чтобы посчитать H1
значит, во-первых
тут идут какие-то сложения вычитания
это все ОАТН квадрат
далее
знаменатель считается за ОАТН
тут матрица РАНГ-1 ОАТН квадрат
посчитать
здесь тоже ОАТН квадрат
потому что сложится самая сложная операция
это умножение обратного гессиана на вектор H0
здесь произведение тоже матрица РАНГ-1
потому что вот эта штука равна вот этой штуке транспонированной
поэтому мы один раз считаем Hk на yk
и берем матрицу РАНГ-1 здесь
умножаем вот этот вектор на yk
и после этого проведения сложения получаем новое приближение для H1
и все это за ОАТН квадрат
успели?
да
хорошо
собственно, модификация метода DFP
метод BFGS, вот почему он так называется
Бройден, Флэчер, Гордмар, Пшанно
и идея в том, что давайте мы не будем приближать B
давайте мы сразу H приблизим
у нас квадринуск уровень уже поменяется
но решение задачи будет записываться точно так же
только где-то поменяется местами y и s
почти теоремы
на основании ущереджих соображений можно показать, что
если функция сильного последующего гессиана
то BFGS будет сходить сверх линей
более точный анализ ходимости довольно затруднительным
в силу сложности формул, которые
довольно их трудоемкости для какого-то исследования
и получения оценок
основные комментарии про BFGS
очень хорошо работают на практике
и бывает свойством самой коррекции
то есть если он на какой-то итерации
приближение матрицы H оказывается достаточно плохим
то на следующих итерациях это плохое качество будет исправляться
сейчас на примере посмотрим, как это работает
это я, наверное, сильно комментировать не буду
следующий этап
это квазинтонский мест с ограниченной памяти
у нас пока что осталась проблема, что
сложность хранения обновления гессиана это n квадрат
при этом мы понимаем, что на самом деле
нам вся матрица не нужна
нам нужно всего лишь эффективно ее навестору умножить
и даже грандианам
и еще мы понимаем, что значения векторов y и s
получены на первых итерациях
они по-прежнему остаются включенными
в значение матрицы H на сотой итерации
потому что она просто обновляется
через сумму и вычитание
но при этом они могут портить эту самую оценку
поэтому давайте мы будем хранить
только последние m значений
корректировать hm0 на каждой итерации
и умножать на такое приближение гессиана рекурсивно
тогда сложность будет mn
и для хранения последних m пар
нам нужно будет списание статуру данных,
которая называется deck
метод лучше всего работает на практике
заранее определяем m и обновляем рекурсивно
у нас были такие формулы
если расписать m шагов, то мы получим
что здесь присутствует набор из умножений
из скалярных произведений
и потом умножение на hm0
в итоге мы умножаем вот эту матрицу
в виде на вектор
за o от n
потому что все, что здесь используется
оно представляется таким вот образом
а умножить на матрицу v вектор
это всего лишь o от n
потому что нам надо взять этот вектор
посчитать скалярное произведение с s
и вычесть из него вектор y умноженный на row
и на вот это скалярное произведение
это нам дает быстрый способ
получения результата умножения матрицы
обратного гессиана на вектор
без формирования самой матрицы
мы просто храним набор векторов, с которых
эта матрица складывается
понятно ли идею?
окей, эксперименты
смотреть, что происходит
опять рассмотрим эту же самую задачу
видим, что метод dfpm
вот в этом месте сломался
и перестал сходиться
вместе с тем
да, тут немножко кряя картинка
ладно, я ее потом обновлю
вместе с тем метод bfgs
в этом месте тоже перестал сходиться
но в силу своей самой коррекции
через несколько итераций он выправился
и продолжил сходиться в таком же стиле
видно, что bfgs сошел быстрее всех
при этом бразилай и бурвин
примерно до столько же итераций
при этом bfgs и sci-fi сходятся достаточно долго
потому что там хитрый процедуровый барышан
который не является оптимальным для этой задачи
в итоге выводы по сегодняшней лекции
сложность одной итерации
по ньютоновской методу от n квадрат
в строении санкубом в методе ньютона
для метода bfgs требуется линейное количество памяти
у bfgs есть самая коррекция
и сверхленность сходилась
проблема этого сомнительства метода в том, что
на классический случай
все начнет ломаться
если у вас градиент не точный, то
ничего не работает
начальное приближение b0 или h0 это некая пристика
с теорией сходимости все не очень хорошо
это пока что в стадии разработки все находится
и не любой способ хорошего шага будет гарантировать вам
положительность скалярного произведения y на s
для задач большой размерности
точный градиент известен
bfgs является стандартом
все прикладные работы, которые что-то минимизируют
и задача является, безусловно, чаще всего
использовать именно его
во многих пакетах он реализован
скорее всего, на любом языке
на котором вы что-то пишете
и который маломальски приспособлен
решение каких-либо вычислительных задач
там это все будет
следующий раз у нас будут максимальные методы
последней уже лекции
посмотрим на то, как решать задач с ограничениями
когда эти ограничения достаточно просты
есть ли какие-то вопросы? Нет
очень хорошо, надеюсь, что все было понятно
тогда спасибо за внимание, и на этом мы заканчиваем
всем большое спасибо
и до следующего раза

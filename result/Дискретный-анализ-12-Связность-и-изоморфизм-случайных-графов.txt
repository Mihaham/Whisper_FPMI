Так, а лекции-то у нас по-прежнему пафосные. Все очень содержательно, интересно, и сегодня мы
продолжим разбираться с компонентами связанности, причем вы находитесь в таком удивительном
моменте, когда я впервые читаю доказательства одной из частей некоторой теоремы, которую раньше
я не доказывал совсем. Но благодаря тому, что мы часть теории графов перенесли в первый
семестр, мне удается туда включить еще это. Это важно, это красиво очень, но просто до сих пор
не успевалось. Значит, что мы доказали в прошлый раз? Давайте я напомню. Мы в прошлый раз доказали,
что если вероятность ребра случайного графа имеет вид t логариф men поделить на n, то при t
большем единице асимптатически почти, наверное, g от np связан, а при t меньшем единице асимптатически
почти, наверное, g от np не связан. Вот так. Это мы доказали в прошлый раз. Но на самом деле это даже
не самый, может быть, главный факт о случайных графах и их связанности, хотя очень важно. Я
пытался продемонстрировать практическую составляющую этого дела, но вот есть еще один очень
важный факт, который я обычно оставлял полностью без доказательства, а сегодня постараюсь доказать
наполовину. Значит, пусть p теперь это c поделить на n, то есть выскакивает, не выскакивает, а наоборот
уходит, выпадает это рассмотрение с множества логариф men. Ну то есть эта вероятность ребра совсем
как бы маленькая, и понятно, что в этом случае случайный граф, конечно, связан не будет. Это
понятно? Тут даже не c меньше единицы, это вообще в логариф men раз меньше. Конечно, связанности не
будет. Тем не менее, опять имеет место фазовый переход через единичку. c меньше единицы и c
больше единицы это принципиально разные вещи. Оказывается, что если c больше единицы, то
асимптотически почти, наверное, не так, то существует такая... Нет, давайте, наверное, я все-таки с c меньше
единицы начну, а то слишком громоздко будет виноват. Давайте, если c меньше единицы здесь сначала
будет, значит, если c меньше единицы, то существует такая константа бета, большая нуля, то асимптотически
почти, наверное, число вершин каждой связанной компоненте, ну случайного графа, конечно же,
от np не больше, чем бета помножить на логариф men. То есть не просто граф разваливается, но он,
конечно, разваливается, потому что мы находимся в таком режиме относительно первой теоремы,
но он разваливается на крошечные компоненточки. Если всего n вершин, то в каждой компоненте не
больше, чем бета логариф men. Очень существенная вещь, он совсем развален. Так, а второй пункт — это
что происходит при c большем единице. Тут чуть более громоздко. Если c больше единицы, то существует,
давайте, снова бета больше нуля, и существует гамма из интервала от нуля до единицы. Тут важно,
что строгий интервал такие, что асимпатически, почти, наверное, g от np есть ровно одна компонента,
не менее чем гамма n вершинами, а все остальные компоненты имеют не больше, чем бета лог n вершин.
Остальные компоненты имеют не больше, чем бета логариф men вершин. В смысле,
каждая из них имеет не больше, чем бета логариф men вершин. Каждая из них. Есть ровно одна гигантская
компонента, в которой вершин порядка общего числа вершин в графе. Вот эта вот ровно одна
компонента, она действительно официально совершенно называется гигантской компонентой случайного
графа. Понятно говорю, да? Но это сильный ход. То есть, если в пункте один традиционная сарделька,
обозначающая множество вершин, заполняется уже не как сарделька, а как огурец, такими крошечными
компонентами размер, каждый из которых не превосходит логарифма по порядку, то здесь в пункте
два возникает гигантская компонента, но я ее вот как-нибудь так нарисую, вы должны понимать,
что вот эта гамма, она, конечно, может быть очень близка к нулю, но слава богу, она единая для
всех n. То есть, когда растет число вершин, размер компонента растет пропорционально, но я не утверждаю,
что гамма близка к единице. Чем ближе c к единице, тем, конечно, ближе гамма к нулю. Это, я надеюсь,
интуитивно хотя бы понятно. Чем дальше c от единицы, тем жирнее становится гигантская компонента.
Математически она гигантская в том смысле, что ее размер линейен по числу вершин, а это крошечные
компоненты, а не логарифмические. Кстати, друзья, вы помните, когда мы занимались
хроматическим числом случайного графа, вот этот режим тоже был в некотором смысле нами изучен,
правда, я там ничего не доказал, но вы не помните. Я говорил так, что если c меньше единицы, а p равняется
c поделительно, то это в скобках, это я напоминаю то, что было когда-то, то асимпатически почти
наверное хроматическое число случайного графа не превосходит тройке, даже равно тройке.
И пояснением к этому было, что все связанные компоненты этого графа это деревья или унициклические
графы. Сейчас вспоминаете, нет? Нет, не помните, кто-то помнит. Ну было такое. Сейчас выясняется
дополнительная вещь, вот эти деревья, унициклические графы, они крошечные. Возможно, знание, которое мы
сегодня создадим, я это докажу, вот пункт один я докажу, поможет вам решить вот это как упражнение.
Доказывать я это все равно не буду, но можете попробовать. Что говорите? Ну это да, это некая задача со
звездочкой, да, да. А вам уже это выдали? Ну хорошо, да, это задача со звездочкой, а я вам подсказываю,
что можно попробовать ее решить, например, зная вот этот результат. Ну может быть как-то по-другому,
то есть не обязательно так, но этот результат может помочь. Вот, но сам по себе результат очень
содержательный. Есть такая, понимаете, как бы эволюция случайного графа, эволюция прямо в смысле,
как мировая история эволюционирует. Я люблю в этом месте рисовать такую как бы развлекательную
картинку. Вот у нас есть два порога, один поделить на n и логариф men поделить на n, которые определяют
как бы качественный скачок в истории мира. Ну мир, это как формируется граф, конечно. Значит,
если мы находимся сильно ниже вот этого порога, первого про который мы только сегодня узнали,
то у нас фактически имеет вместо феодализм, потому что граф распадается на крошечные вот такие
вот феодики. Соответственно, то, что происходит вот в этой части, от одной n и до логариф men
поделить на n, можно квалифицировать как появление империи. Гигантская компонента,
это своего рода империя, вокруг которой все остальное, это, извините, не биполярный мир там
никакой, там одна империя, а вокруг вот это логарифмическое охвосте, которое захватывает
империя, захватывает мировое господство после перехода через вот этот, соответственно, порог.
Ну потому что все, не остается никаких охвостей, а империя захватывает весь граф, он становится
связным. Вот так вот это я обычно интерпретирую. На самом деле, в теории случайных графов, которые
описывают какие-то более, может быть, даже реальные процессы, идущие в мире с какие-нибудь там сети
интернет или экономических взаимодействий между банками, наличием транзакций и так далее. Вот такие
вот фазовые переходы, они прям вот играют роль при оценке рисков, например, возникновения каких-нибудь
финансовых кризисов. Добрый день. Я не знаю, я пришел, он тут стоит. Я пришел, да, было открыто.
Нет, ну как никого нет, там были студенты еще, я уже вторую лекцию читаю подряд. Я потом сдам, конечно.
Так, друзья, понятно все, да? То есть, да, это играет роль вообще во всей теории, очень существенная. Ну хорошо,
я нацелился за сегодня, может быть, не только это сделать, но за сегодня уж точно доказать вот эту
часть. А вторая останется без доказательства в курсе, ну раньше все оставалось, а сейчас вот так.
Там метод как бы новый, поэтому я точно хочу это рассказать и всегда хотел новый по сравнению с
тем, что мы делали. То есть, это не неравенство Маркова, там не неравенство Чебышова, а некий,
ну в каком-то смысле ветвящийся процесс. У вас, конечно, никаких процессов еще не было, у вас
только вероятность началась. Процессы будут ровно через год, Даша, больше они начнутся через год,
весной третьего курса, но ничего из них не надо знать, не переживайте, я вам сейчас все расскажу,
все будет понятно. Вот давайте для начала, прежде чем формально прописывать доказательства, возьмем
просто какой-нибудь граф, какой-нибудь не обязательно случайный, просто конкретный граф,
пока на нем нет никакого распределения. Я не знаю, нарисуем граф. Пусть в нем есть несколько
связанных компонентов, вот пример графа, такой граф. Возьмем какую-нибудь его вершину, любую
совершенно, и будем вести такой процесс, будем считать, что эта вершина живая, а все остальные
вершины, ну вообще все остальные вершины, но не мертвые нет, пока не мертвые, нейтральные,
да. Дайте считать, что она живая, а все остальные нейтральные. Вот из множества нейтральных вершин
вершин, мы выбираем всех соседей вершины В в графе.
На этом примере видно, что соседей два выбрали, обозвали
их живыми, а вот эту таки кокнули.
Но это такой процесс размножения гибели, ветвящийся процесс.
Ну знаете, классический ветвящийся процесс, это
просто, у вас есть организм в начале всех времен, он
порождает какое-то количество потомков, а сам умирает,
кто-то из его потомков порождает еще какое-то количество
потомков, сам умирает, и так вот процесс идет, он
может выродиться.
Я понятно говорю, но я не хочу подробно рассказывать
про классические процессы, это вам расскажут через
больше чем год, но смысл совершенно понятен.
Вот теперь у нас две живых, одна мертвая вершина, а все
остальные нейтральные, вот все остальные пускай нейтральные.
Теперь мы выбираем любую из живых, ну например выбираем
вот эту, и смотрим ее соседей, ну это правда да, смотрим
ее соседей среди нейтральных, но знаете тут какой-то организм
родился и никого не породил, ну бывает к сожалению такое
в природе, ну а в графе это не так трагично смотрится,
у нее просто нет соседей, но важно, что я говорю про
среди нейтральных, это важно для описания уже случайного
процесса, с которым мы будем работать, среди нейтральных
нет, ну кокаем ее, напрасно прожила свою жизнь, что
поделать, у нас остается вот эта живая вершина,
ну у нее слава богу среди тех вершин, которыми по-прежнему
мы считаем нейтральными, есть целых две соседки,
вот ее кокаем, эти все три мертвые, видите они уже
мертвые стали, а эти две живые да, пока еще живые,
пока еще живые, ну у этой живой вершины есть хоть
одна соседка среди нейтральных, нейтральных, а ну нейтральные
есть, но туда не попасть, соседок нет, да, то есть
у нее среди нейтральных соседок нет, кок мертвой
стала, кок мертвой стала, и все, что случилось на выходе,
мы умерщвили все вершины в одной компоненте связанности,
а именно в той, к которой относится выбранная нами
исходная вершина В, согласны, ну то есть это такой возможный
процесс обхода всей компоненты, ну можно по-другому как-то
было определить, например, не из нейтральных, выбирать
там из живых, но вот мы выбираем из нейтральных, так, друзья,
всем понятно, что происходит?
Теперь давайте считать, что граф случайный, все вот
мы переходим прямо к доказательству, я объяснил, как процесс
устроен на конкретном примере, теперь у нас случайный граф,
и мы тем не менее фиксировали какую-то вершину В из нашей
знаменитой сардельки, вон там целых два рисунка
этой сардельки, В это вершины, сарделька, вот, берем какую-то
вершину и запускаем такой же процесс, ну какой, давайте
Y с индексом T будем обозначать число живых вершин, Z с индексом
T, нет, давайте Z с индексом T чуть позже, нейтральные
обозначим N с индексом T, так, T это время, то есть есть
начальный момент времени потом, следующий шаг, ну
вы видели эти шаги, T это время, сейчас я напишу и
начало и рекурсию, N, T это число нейтральных, нейтральных
вершин, ну и давайте действительно Z, T, это не число мертвых,
это число потомков, ну то есть соседей, конечно, выбранной
живой вершины, то есть процесс прямо в точности пойдет так,
как я его описал, Z, T это число потомков выбранной
на шаге с номером T выбранной живой вершины, ну на самом
деле проще всего сказать, ну давайте я все-таки начну
с того, что Y0 конечно равно единице, потому что мы стартовали
с вершины V, вот она живая в начальный момент времени,
ну и еще просто сказать, просто сказать, что YT, количество
живых вершин, это сколько их было на предыдущем шаге,
плюс сколько породила выбранная нами живая вершина, и минус
это самая вершина, которая перестала быть живой, так
и еще можно вот так сказать, Z с индексом T имеет биномиальное
распределение с параметрами N с индексом T минус 1 и P,
ну NT минус 1 это случайная величина, то есть такое
условное конечно распределение, но если зафиксировано
количество нейтральных вершин на предыдущем шаге,
то конечно число потомков любой живой вершины имеет
биномиальное распределение вот с такими параметрами,
обозначение понятно или не очень, ну то есть это
случайная величина, которая может принимать любое значение
от нуля до вот этого числа, а P, ну это вероятность успеха
как всегда, ну то есть вероятность того, что ZT равняется какому-то
K, давайте я напишу аккуратно, что ZT равняется какому-то
K, это C из NT по K на P вкатый на Q в степени NT, ой NT минус
1, NT минус 1 минус K, немножко не лезет, понятно, но обычное
биномиальное распределение, как у нас жизнь устроена,
вот мы зафиксировали какую-то живую вершину, есть какое-то
количество нейтральных вершин, и на случайном
графе число потомков, это надо каждое потенциальное
ребро, которое могло бы из этой вершины ввести в множество
нейтральных, протестировать на то, ведет она туда или
не ведет, ведет с вероятностью P, не ведет с вероятностью
Q, 1 минус P, поэтому получается биномиальное распределение
с такими параметрами, сейчас все понятно?
Так.
Слушайте, а вы меня, между прочим, не спросили, что
будет при C равном единице, и в прошлый раз из вас это
пришлось клещами тянуть.
Нет, но на самом деле при C равном единице там люди
очень много чего, но в данном случае это настолько действительно
громоздко и непонятно, что я решил это вообще не комментировать.
В этой теории я давал комментарии в прошлый раз, там при C равном
единице все понятно.
Так, ну ладно, давайте докажем вот такую лему, она простая,
как Y с индексом A.
Сейчас, подождите, прежде чем лемма, забыл сказать
кое-что.
Давайте считать, что вот этот процесс Y, T, количество
живых вершин, он в какой-то момент ведь принимает значение
ноль.
Впервые.
Что в этом случае происходит, как вы считаете?
Все останавливается, мы выделили компоненту связности,
правильно?
Содержащую исходную вершину V, правильно?
Но давайте будем считать, что этот процесс формально
продолжается дальше, но просто для удобства будем
определять и дальше Y, T просто через такие случайные
величины.
Это будет полезно технически.
То есть, когда процесс вырождается, все, первый
раз выразился, отлично, мы нашли компоненту.
Но сами вот эти случайные величины мы определим и
дальше тоже.
Ну так же, да, вот этой рекурсией, да, то есть, там был ноль,
к нему прибавляется какой-то случайное Z, T, там минус
один.
Какое Z, T?
Ну вот такое.
Не хотите?
Ну я вот хочу так.
Это перестает описывать, да, это перестает описывать,
на всякий случай просто я ее доопределю, так это
будет технически полезно.
Ну хорошо.
Я утверждаю, что Y, T имеет вот такое распределение,
это бином параметрами n-1, 1, минус 1, минус p в степени
t, дальше к этому прибавляется единица и вычитается t.
Ну то есть, мы берем вот эту константу, в каком-то
смысле константу, если t зафиксирована, то это константа.
И к этой константе прибавляем вот такую биномиальную случайную
величину.
Я утверждаю, что это и есть случайная величина Y, T.
Так, ну смотрите.
Вообще, очевидно, что n, T, это я, кстати, не написал,
но напишу здесь, это n минус t минус Y, T.
Давайте подумаем, очевидно это или нет.
Вдруг вообще неверно.
Да, если t равно 0 нам, например, очень хорошо подставлять,
здесь будет 0, а здесь будет 1, n минус 1, когда у нас дана
только одна исходная вершина нейтральных, n минус 1 штука.
Правильно?
Ну вообще говоря, достаточно тогда, так, с этим согласились,
тогда доказать вот это утверждение, это то же самое,
что доказать вот такое утверждение, что вот это есть n минус 1,
1 минус p в степени t, просто вот так, что число нейтральных
вершин, это просто бином от n минус 1, 1 минус p в степени
t.
Сейчас, почему это так, давайте, я еще сам не понял.
Я знаю, что так должно быть, я просто это понимаю хорошо.
Теперь давайте осознаем, почему это так.
Ну надо аккуратно осознать, это просто на самом деле,
но надо осознать.
Значит, смотрите, допустим, мы доказали, что n с индексом
t, это вот такой бином, тогда, тогда, Y с индексом t, это же
n минус t минус наоборот nt, ну так перекувырнем, почему
бы нет.
Мы доказали, что nt, это вот такой бином, но давайте
напишем так, это n минус 1 минус nt, плюс 1 минус t.
Согласны, что пока я сделал совершенно стандартное,
понятное преобразование.
1, тут минус 1, все сходится, то есть 1 минус t уже вот оно,
очень удачно.
Ну а тут-то тоже все понятно, смотрите, вы из количества
испытаний вычитаете количество успехов вот в такой схеме
Бернули.
Потому что я понятно выражаю, схема Бернули, испытания,
были эти слова все?
Не нужно пояснять.
Если мы верим, что нейтральные вершины имеют вот такое
распределение, это значит, что фактически число нейтральных
вершин, это число успехов в схеме из n минус 1 испытания
Бернули вот с такой вероятностью успеха.
Если из числа испытаний вычесть это количество,
но это будет число неудач, но вероятность неудачи,
соответственно, это 1 минус вот эта величина.
Ну и в другую сторону, естественно, все так же получается.
Я вывел из того, что nt такая вот биномиальная, то что
yt такая как нам нужна, но обратная, очевидно, верно,
потому что все действия, которые мы совершали, они
обратимые.
Мертвые не входят, и что?
Вот они вычитаются, вот это t это как раз вычитаются
мертвые вершины.
Есть еще вопросы?
Вроде все хорошо.
Вопрос правильный, да, вот это вычитание t это как
раз вычитание мертвых вершин.
Так, вроде получилось.
Ну давайте докажем этот факт, подчеркнутый факт,
скажем просто по индукции.
n0 это мы знаем, n-1, и о чудо, это правда бином от n-1 и 1-p в
нулевой степени.
Ну что такое 1-p в нулевой степени, это единица.
Бином от n-1 с вероятностью единицы это просто n-1.
Если мы каждый объект выбираем с вероятностью 1, всякий раз
успех случается с вероятностью 1, ну успехов и будет с определенностью
просто n-1 штука, то есть это база индукции t равно 0.
Ну там t равно единица также точно просто устроена.
Ну а дальше давайте сделаем шаг.
Вот у нас nt, это еще раз перепишем, n-t-yt, а дальше к y применим
ту рекурсию, которая у нас вот здесь записана, вот эту
рекурсию.
Значит у нас, я поднимусь наверх, так у нас получится
n-t-yt-1, да, минус zt, да, минус zt и плюс 1, так да, и плюс 1.
Давайте так это перепишем, n-t-1-y с индексом t-1 и минус
zt, по-моему все правильно, да, минус t, плюс 1, вот так я загнал,
плюс 1, да, минус t-1, но это что такое?
Это nt-1, к которому можно применять предположение
индукции, но давайте не сразу, это nt-1, а вот это, смотрим
еще раз сюда, это бином от nt-1 и p, то есть вот тут
вот бином от того же nt-1 и p, то есть мы из количества
испытаний nt-1 вычитаем число успехов на этом количестве
испытаний, но это есть естественно бином от nt-1 и 1-p, правильно?
Мы из количества всех испытаний выкидываем количество
успехов в этих испытаниях, у нас получается количество
неудач, правильно?
Из всех испытаний выкидываем количество успехов, получается
количество неудач, но вероятность неудач это 1-p, ну да, лучше
писать, конечно, тильно, но знаете, вообще совсем
хорошо писать не так, а, наверное, хорошо писать
равняется дальше какую-нибудь букву, там, это или что-нибудь
такое, где это и дальше тильда вот это, ну, наверное,
так будет более корректно, но это чересчур, тильда,
тильда, хорошо, тильда, но смотрите, что такое бином
от nt-1 и 1-p, если мы знаем, что само nt-1 по предположению
индукции это бином от n-1 и 1-p в t-1 степени, то есть
мы берем n-1 испытание и ищем в нем успехи вот с такой
вероятностью, а потом на множестве этих успехов
снова вот с такой же вероятностью, с такой же, с такой вероятностью
производим снова поиск успехов, для этого они были
неудачами, теперь они стали успехами с вероятностью
1-p, будто или понятно, вот здесь мы говорим, есть nt-1
испытание и вероятность, ну пусть успеха, 1-p, мы сжимаем
в 1-p раз как-то, ну в каком-то смысле, среднее значение
брать, а мы знаем уже, что найти само nt-1, это значит
произвести испытание Бернули, просто на фиксированном
количестве объектов, вот с такой вероятностью успеха,
но эти действия независимы от этих, значит вероятности
надо перемножить, сейчас понятно сказал, нет, но вот
у нас есть сколько-то испытаний, само их количество, это число
успехов вот в такой схеме, теперь мы к этому количеству
снова применяем схему, но действия в рамках этой
схемы от действий, которые вот здесь осуществлялись
не зависят, поэтому вероятности успеха просто перемножаются,
выбрать отсюда сколько-то успехов с такой вероятностью,
это все равно, что выбрать отсюда сколько-то успехов
с такой вероятностью, умноженной на вот эту, понятно, ну все,
мы получили то, что хотели, то есть вот это N-1 и 1-p в тепени
t, индукция завершена, индукция получилась, сейчас звонок
будет, дайте нам тогда устроим перерыв, заметьте, что здесь
то уже никаких случайных величин нет, мы аккуратненько
по индукции убедились в том, что нейтральные вершины
имеют вполне себе куда оно делось, понятное распределение,
вот оно, тут все зафиксировано, при фиксированном N, фиксированном
T, все зафиксировано, то есть пользуясь вот этими индуктивными
действиями, мы в конце концов получили неусловное распределение,
распределение при условии того, какое значение примет
предыдущая случайная величина, а безусловное, именно, что
как бы мы не зафиксировали эту величину, в итоге получится
вот такое вот распределение, для этого и была применена
индукция, ну да, продумаете это еще, потом можем еще
подискутировать, я думаю все будет понятно, ничего
такого страшного нет, в итоге у нас получились вполне
недетерминированные распределения, то есть yt не опирается больше
ни на каких предшественниц, оно имеет вот именно такое
распределение с понятными параметами, и вот если мы
сейчас это до какой-то степени поняли, дальше поймем, лучше
когда будем глубже разбираться, то этим можно очень хорошо
и красиво воспользоваться.
Так, сейчас воспользуемся, вот у нас какая-то вершина
v, я еще раз напоминаю, она уже была где-то здесь написана,
вот она, ну я еще раз напоминаю, что мы стартовали с какой-то
вершины нашего графа, вероятность того, что мощность компонента
содержащей эту вершину больше какого-то t, ну то есть
что процесс остановится впервые позже, чем в момент
времени t, t от v это компонента связанности, содержащая
вершину v, вот вероятность того, что количество вершин
в этой компоненте больше чем t, вероятность того,
что мощность этой компоненты больше чем t, она не больше
чем вероятность того, что yt больше нуля, вот в этом
месте я пользуюсь тем, что процесс продлил формально,
потому что понимаете, что значит yt больше нуля, я
нигде здесь не пишу, что t это первый момент времени,
в который yt больше нуля или последний момент времени,
когда он еще больше нуля, yt может оказаться больше
нуля в более широком множестве случаев, когда мы продлеваем
этот процесс, он тоже может потом случайно оказаться
больше нуля, поэтому я пишу меньше либо равно, я этим
пользуюсь явно, так, теперь что значит, что yt, смотрите
на лему, больше нуля, это с какой величине у нас одно
вероятностное пространство, это множество графов на
n вершинах, а я понял, да, пусть вот дано какое-то
t, мы просто оценим сейчас вероятность, а потом подберем
это t так, как нам нужно, да-да-да, сейчас вот t это
просто параметр оценки, вот ввели какое-то t и посмотрим,
с какой вероятностью количество вершин в компоненте связанности
содержащей v больше, чем вот это t, ну какое это t,
любое t, с какой вероятностью размер компонента больше,
чем это t, с вероятностью точно не больше, чем вероятность,
нуля, а это есть вероятность того, что binom от n-1 1-1-p в степени
t больше, чем t-1, я вот это 1-t, которое прибавляется
там, перекинул вправо, так, ну сейчас, наверное, все понятно,
можно я вот так, больше либо равно t, тоже самое, забыл скобочку, да, вот эту скобочку забыл,
так, ну быть больше строго, чем t-1 для целого числа, это то же самое,
чтобы быть не меньше, чем t, у нас тут целое число с t, целое число больше,
чем t-1 тогда и только тогда, когда оно больше либо равняется t, теперь, ну давайте так,
это меньше либо равно, тут я перепишу, вероятность того, что binom от n и 1-1-1-p в степени t больше
либо равняется t, хр, ну почему хр, потому что одно дело посчитать успехи на n-1 испытании и
сравнить их вот так именно с t, а другое дело на n, но, наверное, вероятность того,
что за n бросаний станет больше либо равно t, опять скобку не закрыл, станет больше либо равно t,
это вероятность больше, вообще говоря, чем вероятность того же самого за n-1 бросаний,
монетки. Так, и то же самое касается вероятности, если мы вот эту вероятность оцениваем сверху,
то и всю большую вероятность мы тоже оценим сверху, чем больше вероятность успехов в каждом
испытании, тем больше, конечно, вероятность того, что успехов будет много, значит мы это оценим как,
но я думаю все присутствующие знают, как можно оценить 1-p в степени t снизу.
1-pt, да? Ну то есть, в общем, все это не больше, вот давайте так, это не больше, чем pt. Это не
больше, чем pt, поэтому все вместе не больше, чем вероятность, с которой binom от npt больше
не выровняется t. Так, дорогие друзья, сейчас я в очередной раз у вас спрошу, вы теорему
муавролаплас или центральную предельную теорему знаете? Центральную предельную знаете? Я знаю,
я могу напомнить. Ну, потому что ей хотелось бы воспользоваться, в принципе, можно здесь
сделать такое же исхищрение, как я делал, помните, когда оценивал уклонение пьяницы от кабака.
Можно не применять cpt, но я не могу непосредственно сослаться на то ухищрение, то есть нужно тоже
передоказывать. Давайте я вам для разнообразия расскажу вот это не через уклонение, которое там
с чосинусами какими-то хитрыми, а через cpt. Заодно лучше заиграет теорема, которую часть из вас знает,
часть еще нет, но я ее напомню, она простая. Классический факт, без него вы никогда не
останетесь. Нет, она простая в том смысле, что без нее вообще никак, то есть это совершенно
классический факт, который играет огромную роль в приложениях и теории. Доказательства ее довольно
сложные, но здесь-то она будет применяться к биномиальным величинам. Могу, в общем, для биномиальной
ее написать. Там это более просто доказывается, но тоже требует выкладок в духе всяких формул
стирлинга и так далее. Обычно в наших курсах, насколько я понимаю, отдельно для биномиальных
не доказывают, выводят прям как следствие с cpt. Ну давайте я напомню, что такая центральная
предельная теорема в самом простом своем обличии. У нас есть последовательность случайных величин,
бесконечная, которые независимая и одинаково распределены. Независимая и одинаково распределена.
Ну еще, конечно, надо сказать, что мат ожидания каждой из них это какой-нибудь а, дисперсия
каждой из них это какой-нибудь сигма квадрат. Конечно, большая нуля, то есть это невырожденные
случайные величины с конечной дисперсией. Вот она обозначена сигма квадрат. Значит,
тогда утверждается, что вероятность, с которой кси1 плюс и так далее плюс кси n минус на поделить
на корень из n сигма квадрат. Можно вот так написать просто. Ну, например, меньше либо
равняется x при n стремящемся к бесконечности сходится к 1 поделить на корень из 2p. Так,
от минус бесконечности до x не в степени минус t квадрат полам dp. Это называется плотность
стандартного нормального распределения. Сейчас, я кокнул? Нормально, точно? Нормальное распределение
должно быть нормально. Так, ну на самом деле, то мы делаем, вот здесь у нас ведь тоже сумма
независимых случайных величин. Что такое бинон? Какими-то там параметрами скажем n и pt. Что это
за случайная величина? Это сумма кси1 плюс и так далее плюс кси n, где вот эти случайные величины
независимые и очень просто одинаково распределены. А именно, они принимают значение 0 с вероятностью pt,
1 с вероятностью pt и 0 с вероятностью 1 минус pt. Успех неудача. Так, это, я надеюсь, понятно,
вроде мы это сегодня однократно говорили. Но успехов в схеме из-за испытаний вернули с вероятностью
успеха pt. То есть, мы находимся вот в этой ситуации, в ситуации центральной предельной теории. Так,
мат ожидания какое? pt, да, вот здесь pt. Мат ожидания, это просто pt такой случайной величины,
это очевидно. Ну, а здесь будет npt соответственно вычитаться. То есть, чтобы нам привести наше
выражение к виду, который вот здесь фигурирует, надо слева и справа вычесть npt. Ну, надо вычесть
n раз математическое ожидание каждого из слагаемых, чтобы хотя бы числитель был похож на то,
что фигурирует в центральной предельной теории. То есть, я пишу вот так. Это вероятность,
с которой binom от n и pt минус npt больше либо равняется t минус npt. Просто сделала тождественное
преобразование, слева и справа вычил npt, чтобы подогнать левую часть к тому виду,
который фигурирует в cpt, в центральной предельной теории. Но надо еще и поделить
на корень из n sigma квадрат. Так, ну сейчас поделим. Так, какая у нас дисперсия у каждой из ксиитов?
Кокнул, нет? Да, понятно. Но вот какая дисперсия у ксиитов? Как посчитать дисперсию? Дисперсия
это вот так минус вот так. Вот это дисперсия, правильно? Одна из формул, которая кажется здесь
удобнее. Если ксиитов возводить в квадрат, что-то поменяется в сравнении с ксиитом? Нет. Поэтому
это pt минус pt в квадрате. Это pt минус pt в квадрате. Ну или как более общепринято писать pt на 1
минус pt? Ну более общепринято писать p на q. Но у нас сейчас роль p играет pt, а q, соответственно,
это 1 минус pt. Я внятно выражаюсь? Вот. Ну то есть надо как написать? Господи, как бы мне от этого
бинового проклятого избавиться? Может какую-нибудь букву его обозначить? Это, например. Уже надоело
его переписывать каждый раз. Это минус npt поделить на корень. Так, на корень из какой же бяки
мы делим? Вот из этой. Но только на n еще надо умножить и будет нам полный катапсис. 1 минус pt.
Так, больше либо равняется ну совсем поганой бяки t минус np. Да никакой поганой бяки. Вообще смысл
то теоремы центральной предельной чувствуете, понимаете? Что очень простой и естественный. Мы
взяли случайную величину, складывающуюся как сумму независимых одинаковых факторов,
отцентрировали ее, как говорят, то есть догнали ее среднее в ноль, и отнормировали так, чтобы
дисперсия получилась единица. То есть у нас стоит здесь случайная величина со среднем 0 и дисперсией 1.
И теорема утверждает просто, что распределение этой случайной величины, функция распределения
этой случайной величины очень похожа на функцию распределения стандартного нормального, тоже со
среднем 0 и дисперсией 1. В этом же смысл. Ну здесь вот они так выразились чуть-чуть громоздко, но ничего
в этом страшного нет. Здесь тоже npt под корнем 1 минус pt. Так, и большая скобка закрывается. Теперь
настает некоторый момент истины, потому что как сходится? Я думаю, что в том варианте центральной
предельной теоремы, которые вам доказывают в стандартном курсе, именно в общем виде сходится как-то
и сходится. Но если здесь стоит тема испытаний Бернули, то есть биномиальная случайная величина,
то сходимость равномерная. И на самом деле сходимость равномерная и в общей CPT тоже, в некоторых
дополнительных условиях. Может быть вам в курсе это и говорят, я не знаю. Но в общем, вот эта
сходимость в нашем случае равномерная. Это очень важно. Почему это важно? Потому что у нас справа
стоит не какой-то фиксированный x, как здесь, а x, который зависит от n. И я вообще не могу,
строго говоря, писать, что это стремится и дальше сюда ставить функцию, зависящую тоже от n. Я
могу писать вот так, имея в виду, что тот интеграл, который появится справа, будучи поделенным на вот
это выражение, стремится к единице. И на самом деле даже будучи вычтенным из этого, тоже стремится
к единице, потому что имеет место равномерная сходимость. Вот эта сходимость в нашем случае
равномерная. Сейчас понятно? Я пишу тильда и рисую вот такой интеграл. Ну, я надеюсь, что вы
понимаете, здесь у нас было неравенство такое, поэтому мы интегрировали dx. Здесь неравенство в
другую сторону, поэтому интегрировать мы, естественно, будем от правого вот этого предела. Здесь будет
1 на корене с 2p интеграл. Вопрос какой-то. t-npt. Громоздко выглядит, смысл очень простой. Вот я
переживаю, вроде много слов произношу, а на самом деле все, что происходит, очень просто. npt 1-pt,
здесь плюс бесконечность наоборот, ну и е в степени минус наоборот. x давайте писать,
квадрат dx, чтобы не путать с нижним пределом, который сейчас от буквы t зависит. Ладно, друзья,
вот если вы хотите, ну не катарсис, так хотя бы просто понимание того, что произошло. В чем суть,
а может и катарсис? Вот у вас сейчас прояснится, почему нам важно, что c меньше единицы, может уже
у кого-то прояснилось. У нас же c меньше единицы. Я утверждаю, что это принципиально важно для того,
чтобы дальше что-то получилось. Давайте я напишу вот это t-npt поделить на корень,
вот там npt на 1-pt. Господи, помилуй, кажется какая-то бяка. Но у нас p это c поделить на n. Вот это вы
помните еще, что p это у нас c поделить на n, поэтому npt это ct х, то есть вот здесь написано t-ct и
делится это на корень из ct, ну умножить на какую-то фигню. Умножить на 1-ct деленное на n. Вот так,
то есть вот эта штука, стремящаяся к единице под корнем, она ни на что не влияет, это фигня.
Ну в принципе я, наверное, даже могу как-нибудь оценить. Сейчас давайте подумаем, это меньше
единицы, да? Больше единицы? Плохо, да? Это больше, чем если... А, так нет, это же хорошо.
Так, друзья, я кокнул вас вот этим своим размышлением или ничего? Я очень хочу от этой
бяки избавиться, чтобы вам приятнее на все это было глядеть, но могу не избавляться. Я хочу
что сказать? Я хочу сказать, что мы интегрируем вот от этой величины, а если бы мы интегрировали
не от такой, а вот от такой, то что ближе к плюс бесконечности исходное или вот такая?
Исходное ближе к плюс бесконечности. Ну значит соответствующий интеграл меньше,
чем интеграл, в который подставлена вот такая штучка, правда? Ну приятнее глядеть. Давайте
я так и напишу. Это не превосходит интеграла от t-ct на корень просто из ct до плюс бесконечности
1 на корень из 2p, е в степени минус х квадрат пополам dx. Ну вот тут принципиально важно,
я уже пытался это начать говорить, что c меньше 1. Именно потому что c меньше 1,
t-ct, куда стремиться при t растущем? Чем больше t, тем куда это ближе к плюс бесконечности. То
есть это функция, стремящаяся к нулю. Успели, да, уследили. Какой скорости? Вот это вы понимаете
или нет? Какой примерно скоростью, я не прошу прям вот точно оценить, но какой примерно скоростью,
вот эта величина, в которой сейчас идет интегрирование, это величина, которая имеет вид,
ну давайте я как-нибудь константу обозначу. Тут 1 минус c поделить на корень из c. Вот давайте
это 1 минус c, t за скобку, поделить на корень из c, обозначим как-нибудь. Ну альфа, например,
хорошо? Так, это будет альфа корень из t. Альфа положительная, потому что c меньше 1. Значит,
тут стоит альфа корень из t и идет интегрирование до плюс бесконечности очень-очень быстро убывающей
функции. Я утверждаю, что это стремится к нулю, но это я не знаю, просто он совсем такой аналитический,
простой вроде факт, примерно как e в степени минус, ну какую-нибудь еще букву надо написать,
альфа штрих умножить на t. Весь этот интеграл не превосходит e в степени минус альфа штрих на t,
где альфа штрих тоже положительная. Но я просто фактически подставляю альфа корней из t вот сюда и
получаю вот этот альфа штрих. То есть весь интеграл доминируется своим первым, ну как сказать,
слагаемым. Интеграл это же сумма такая, мы берем самое большое слагаемое, а дальше они убывают,
убывают с бешеной скоростью, и вся эта сумма доминируется своим первым слагаемым. Вот я все,
что утверждаю. Давайте расценим это как простое упражнение, которое я не буду сейчас решать,
что мне лень просто, а вам, может быть, будет полезно. Нормально так? Ну попробуйте,
по крайней мере, если не будет получаться, будем обсуждать. По-моему, должно получиться. Я вроде
объяснил интуицию, функция очень быстро убывает, интеграл это сумма вот этих площадей. Мы берем
значение в самом первом кусочке, а дальше сумма она доминируется вот этим первым слагаемым кусочком.
Фуф! Что у нас получилось? У нас получилась вероятность того, что компонента связности,
содержащая данную вершину, имеет больше чем t вершин сама. Эта вероятность маленькая,
она не превосходит e в степени минус альфа штрих на t, где альфа штрих как-то зависит от исходного c.
Понятно? Ну хорошо, остался последний, извините за тавтологию, штрих, но не над альфой, а в доказательстве.
Сейчас будет подлинный катарсис. Сейчас мы полетим на крыльях части.
У вас, наверное, еще какая-то легкая, но у меня тоже. Так, смотрите, вероятность того,
что существует v такая, что модуль c от v больше чем t, ну это очевидно не больше чем n на e в степени
минус альфа штрих t, правильно? Потому что вершина n штук, а существование, это как всегда объединение
событий. Ну так пусть или положим пусть t равняется, как нам, например, 2 поделить на альфа штрих
логариф натуральный n. Ну, например, 2 поделить на альфа штрих логариф натуральный n. Тогда пишем меньше
либо равно, или равно даже, равно, почему меньше? Но n умножить на e в степени минус альфа штрих
сокращается, то остается 2 логарифмен. Ну это, извините, 1 поделить на n. Это стремится к нулю.
Но значит с вероятностью стремящейся к единице для любого v размер компонента связности,
которая содержит эту вершину, не превосходит 2 поделить на альфа штрих логарифмен. Обозначаем
вот эту штуку буквой бета, которая фигурировала в формулировке теоремы, и получаем ее результат.
Существует бета, зависящая только от c. Вот, например, такая 2 поделить на альфа штрих,
что все компоненты связанности, с какой бы вершины мы ни стартовали, имеют количество вершин,
не превосходящее бета лог n. Ну в данном случае логариф натуральный. Я вроде так и писал.
Кажись, доказал. А, она стремится к единице, конечно. Я сказал, но не дописал. Это стремится к нулю,
а это ее отрицание. Ну, стремится к единице. А симпатически почти, наверное, каждая компонента
случайного графа имеет не больше, чем бета логарифмен вершин. Так, друзья, тут было две тонкости
в этом доказательстве. Одна, это то, что вам немножко было тяжело интуитивно и, казалось,
не совсем формально жонглировать случайными величинами, которые имеют биномиальное распределение,
зависящее от предшественниц. И вот с этим, наверное, надо вам тоже подразобраться, подумать,
насколько это действительно осознается, корректно и так далее. Вот, это первый момент. А второй
момент, ну не вот этот второй момент, а второй момент доказательств. Я сюда просто показываю и
вижу, что это второй момент. А второй момент, это то, что я немножко вас замучил, наверное, вот этими
выглотками. Но если вы на них внимательно посмотрите, вы поймете, что это я вас мучил,
потому что вам понятно было, вам непривычно, а так-то они же очень простые. Подогнал просто
классическую теорию. В принципе, я мог сделать здесь рассуждение в духе вот того большого уклонения
пьяницы от кабака тоже бы сработало, и тогда не нужно было бы ни асимптотик неравномерной исходимости,
просто там тяп-ляп, чосинусы какие-то оценили. Ну, я почему-то решил продемонстрировать на таком
варианте. Кажется, что это тоже неплохо. Два тонких момента, которые могли вас чуть смущать
в доказательствах, но продумайте их до конца, и все будет хорошо. Так, у меня есть еще несколько минут.
Я хочу, наверное, начать рассказывать про последнюю тему о случайном графе, которая есть в этом
семестре. Но я, собственно, обещал, что я расскажу впервые за всю историю вот это и расскажу то,
что уже рассказывал, это изоморфизм случайных графов. Тоже мне кажется, очень хорошая иллюстрация
того, как в случае, когда поступающие вам на вход графы не имеют какой-то наперед заданной
известной вам структуры, здорово пользоваться понятием случайности. Ну, то есть, помните,
я говорил про жадный алгоритм, например, раскраски графов? Конечно, если вы знаете,
что тот граф, который вам предстоит раскрашивать, имеет какую-то достаточно специфическую структуру,
то пользоваться жадным алгоритмом преступно. Но если вы действительно заранее не знаете,
какой граф из общей тучки графов на вас капнет, но почему бы не воспользоваться жадным алгоритмом?
Он же ошибается почти всегда не больше, чем вдвое. Это здорово. Вот с изоморфизмом ровно такая же
ситуация. Есть великая проблема изоморфизма графа, которая близко даже не решена.
Пt больше единиц, но мы же его посчитали. Пt поделить на n. t и t фиксировано, но, естественно,
мы считаем, что оно настолько велико, чтобы это было меньше единиц. Да, конечно, это правильный
вопрос. Конечно, наверное, если бы я рассказывал прямо до запятых, как некоторые делают,
да, это, наверное, стоило бы сказать. Спасибо, да, это правильно. t и t фиксировали. В итоге оказалось,
что t зависит от n, но, слава богу, это логариф men, поэтому все равно все в порядке. Еще раз надо
проверить, что в итоге t это логариф men, но все равно это корректно определенная величина,
у всех достаточно больших. Так, ну давайте про изоморфизм несколько слов скажу начально.
Кто такое граф изоморфный? Это, я надеюсь, все понимают или надо пояснять? Существует
биекция между множеством вершин, которая ребра переводит в ребра, а не ребра в неребра. Казалось
бы, ну, елки-палки, на входе вам даются два графа. Ну, естественно, с общим множеством вершин. Ну,
или отождествляем, то есть биекция может быть. Мощность одинаковая. Неужели трудно проверить,
они одинаковой формы или не одинаковые? Ну, вот трудно, да, никто не знает. Я не знаю,
насколько вас учили этому в каком-то виде на алгоритмах, я как-то не слежу. Не учили,
но это, да, это другая история, и я не собираюсь учить. Ну, можете, я не знаю, какие-то вещи,
которые совсем просты, попробовать сделать самостоятельно. Например, легко придумать,
относительно легко придумать алгоритм, который за линейное по числу ребер время проверяет и
заморзнули два дерева. Ну, да, может быть, потому что я его знаю, мне кажется, что просто. Наверное,
да, да, наверное, в этом дело, да, действительно, может быть, даже для деревьев не так просто. Даже
для деревьев не так просто. Согласен. Вот, есть на данном, на данном этапе разные классы графов.
Ну, вот я уже сказал, деревья, удается быстро проверить на изоморфизм. Удается. Там бывают,
например, планарные графы, про которые, кажется, у вас какие-то семинары были, да. На семинарах
что-то про планарность было, а на лекциях я не рассказывал. Но, тем не менее, если есть два
планарных графа, это сложный алгоритм реально, вот, он умеет проверять их на изоморфизм. Так,
ну, еще какой пример я помню. Ну, скажем, если, так вот, существует к, такое, что для любого в из
в дек в не превосходит к, тогда за у от n в к можно проверить. Да, к сожалению, так. Но это
грустно, да, я согласен. Я рассказываю, что, вот, видите, есть какое-то ограниченное количество случаев,
и даже асимптотика ужасная, потому что, если вы хотите ограничить степень каждой вершины десяткой,
ну, извините, n в десятой, это, конечно, все равно, что 2 в степени n. Ну, почти что. Или там n в сотой,
но это уж точно 2 в степени n. Насколько я помню, да, то есть, в общем случае, вроде бы, да,
может быть, были какие-то прорывы, которых я не отследил, но вроде бы так. Еще, например, я помню
такой класс, я не претендую на исчерпывающее знание всех классов, этим занимаются люди,
я просто не настолько уж прям специалист. Есть так называемые интервальные графы, которые с вами
можно проверять на интервальность достаточно быстро, там за линейное время все делается,
вот. Ну, и между собой сравнивать. Интервальные – это графы, у которых вершины отрезки прямой,
а ребра, если отрезки пересекаются. Такие графы называются интервальными. Тут за линейное время
делают. Нет, ну есть масса других классов, я просто могу не знать. Конечно, какие возникают идеи,
надо придумать какой-нибудь вариант. Число вершин, число ребер. Ну, к сожалению, не помогает. Там
собственные числа графов какие-нибудь можно брать. Ну ладно, просто степень вершин те же самые. К
сожалению, два графа с одинаковым набором степеней вершин могут быть неизоморфны, это легко построить
примеры такие. Я понятно говорю, да? То есть проблема в итоге оказывается дико трудной и,
в общем, никто не знает, как ее решить, вообще имеется ли там полинамиальный алгоритм,
не полинамиальный, она даже классу ни к какому сейчас не отнесена. Ну, вернее, ни к какому из
таких основных, это NP трудная задача или нет. Вот этого мы не знаем. Конечно, там классов тоже
великое множество. Вам когда будут сложность вычислений рассказывать, там Мусатов и компания,
вы узнаете такие классы, о которых и не подозревают. В этом смысле изоморфизм,
конечно, к некоторым из них относится. Но вот NP труден он или нет, это неизвестно.
Был сравнительно недавно результат товарища Ласло Бабайи венгерского происхождения,
живущего в Америке, классика абсолютного, который очень много в этой области придумал,
и часть этих результатов принадлежит тоже ему и сделана еще в 80-е, если не 70-е годы. Ну вот он
занимался, занимался, занимался и да занимался до того, что придумал вот такой сложности алгоритм
логарифом N, а здесь в степени гамма, ну где гамма, к сожалению, больше единицы. То есть он придумал
алгоритм, который умеет проверять изоморфизм произвольных графов на N вершинах за вот столько
операций. Ну это весьма круто, потому что это не E в степени N, а это почти E в степени логариф МН,
но правда логариф МН все-таки в степени больше единицы. То есть полиномиальный алгоритм,
это алгоритм, который вот так работает, это алгоритм МН, а тут вот эта C или гамма там в
показателе степени. Но все равно это безумно круто, там доказательства на 100 страниц. У нас когда-то
тут был целый семинар, Мусатов и Миш Тихомиров вместе пытались со студентами разбирать это
доказательство. Год разбирали, но что-то разобрали. Вот такая вот потрясающая совершенно история.
Вот, а я хочу в следующий раз рассказать такую теорему, которую в начале 80-х же
годов двадцатого века доказали три человека. Первый из которых по алфавиту это Бабаи,
но тот же самый. Второй это Эрдаш, но я уже начал писать латиницей. Давайте так же и буду писать,
но Эрдаша мы знаем хорошо. Вот, и товарищ которого я не знаю и, к сожалению, не знаю как читать
правил. Ну то есть я не знаю какого он происхождения, поэтому я боюсь наврать в прочтении его фамилии.
Ну вроде не Венгер по крайней мере, поэтому вообще если Венгер, то вот это эс должно читаться как
Шей, тогда у него очень неплохая фамилия Шелков. Ну вот, ну не знаю, в общем, это может быть немец,
кто угодно. Я, к сожалению, просто не знаю. Вот, они доказали такую замечательную совершенно теорему,
что если GH это случайные графы, ну я вот так напишу, взяли два случайных графа, то вот их
эзоморфизом можно проверить за линейное по числу ребер время, то есть за O от N в квадрате.
Ну это, конечно, не очень четко. Давайте я скажу так. Существует такое множество графов K, N,
это под множество множества тех графов на N вершинах, графов на N вершинах, то мощность K,
N поделить на 2 в степени C из N по 2 стремиться к единице при N, стремящемся к бесконечности,
то проверить принадлежит ли G, K, N можно за O от N в квадрате. И если G и H принадлежат
к N, то проверить их эзоморфизом тоже можно за O от N в квадрате. То проверить G изоморф на
ли H можно за O от N в квадрате. То есть нетремиальность теоремы состоит в том, чтобы придумать,
как описать это множество, чтобы проверка принадлежности к нему графа не занимала
большого времени, а дальше еще так его описать, чтобы вероятность попадания в него, то есть вот
эта дробь стремилась к единице. Но это вероятность просто того же случайный граф принадлежит K, N,
я зачем-то так записал. Понятен пафос, да? Если опять, если вы считаете, что графы на вас
капают равномерно, не с какой-то наперед-задной структурой, одинаково, вероятно, все, то легко
проверить их на изоморфизм, если вы готовы ошибиться в O маленьком от общего числа случаев.
Всего два в степени C из N по два графов, а ошибаетесь вы в такой доле графов,
которая стремится к нулю. Ну типа того, да, можно и так выразиться, конечно, да. Ну естественно,
да, конечно, конечно. То есть вот на случайных графах удается эту пыль под ковер заметать,
а на всех графах 100 страниц, классификация конечных групп и так далее, там жесть полная.
Ну слушайте, как точно-то я отговорил. Все, в следующий раз докажем теорию.

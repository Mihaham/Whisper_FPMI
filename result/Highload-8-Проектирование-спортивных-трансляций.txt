мы с вами остановились на мы мучились с вами с миграцией с диспетчерами и прочее прочее прочее
байдой да у нас осталось несколько паттернов и потом собственно самое главное
так давайте вспомним базы данных
что мы умеем делать с базами данных как мы масштабируем
это понятно спасибо как как именно
паттерны окей основной шардирование дальше какие еще
шардирование вы его делите на весь объем данных делите на кусочек шардирование у
нас есть как бы под историях называемая центральный диспетчер экспорт с разделения
наш арт это раз второй это некая функция от чего там от кого-то и дишника из этих
от этих данных дальше что еще есть
партиционирование да вспоминайте в чем разница
нет здесь у нас есть еще тоже такой паттерн виртуальные шарды экспорт с разделения
виртуальные шарды виртуальные шарды короче с протеша вернее все просто у вас есть блог
данных он не влезает на одну машину по какой-то причине либо не успевает машина обрабатывать
и так далее вы его режете на какие-то кусочки первый принцип разрезки некая функция например
от айдишник этих данных вторая от некого центрального диспетчера эта функция возвращает
вам что возвращает вам шахт где вы храните тот или иной кусочек данных второй центральный
диспетчер ты у него быстренько спрашиваешь этот айдишник где хранится она тебе говорит куда
хранится виртуальные шарды это мы вот сюда вставляем некий виртуальный слой то есть мы
делим все наши данные изначально на очень большое количество кусочек то есть например у нас всего
два сервера нам и все равно делим на 10 тысяч виртуальный шар а потом говорим что вот этот идет
сюда вот этот набор виртуальных шартов идет на один а вот этот набор виртуальный шартов идет
на другой это мы сделали для того чтобы нам потом вот легче было переносить и тусовать
партизонирование что смысл да это мы делили на горячие холодные короче немножко другой
принцип разделения данных то есть грубо говоря здесь вы режете вертикально здесь вы режете
горизонтально ну то есть например вы берете новости и там горячие новости на одну машинку
весь архив на другую машинку похоже немножко немножко разная немножко разная логика что
ли да то есть как бы вот этого подхода по какому принципу вы разбиваете данные еще
что начало репликация конечно репликация история простая вы пишете нам мастер отчитайте
так называемый слоев пишите господина читайте с рабов так не толерантно да мастер слей
репликация бывает мастер мастер репликация еще какие-то виды не суть важно важно то что у
вас есть некий процесс база данных внутри организует этот процесс в том что она копируется
с одной с некой главной машинке на второй зачем нам это нужно потому что у нас обычно
чтение больше чем цепь так еще баздан что еще мы придумали давайте давайте давайте давайте
проспорить меня получше сождаем тоже наспомнил функциональное разделение да ну это очень
простая штука панельное разделение йогойка заключается в следующем вас разная данная почему
бы не хранить разные данные в разных может быть даже в разных базах какие-то в ноутску или какие-то
зашаргировать какие-то реплицировать и так далее зависит от их назначения еще
и нормализация все верно
все верно и просто не не нормализуешь данные классики бы вам сказали о том что ребята
давайте нормализовывать и все будет хорошо хранить их правильно и так далее где нормализация ты
их специально хранишь неправильно ты их не делаешь какие какие изменивает
если ты их не нормализовал то это означает что тебе когда ты данные меняешь ты придется
совершить больше работы зато тебе придется меньше работы совершать когда ты данные извлекаешь
здесь же то есть называйте как угодно динормализация или не нормализация да то есть
как бы без нормы или даже введение избыточности есть такой еще термин пример и избыточности то
есть вы некоторые данные храните просто в нескольких и цепляем матрики база данных вы база
данных исквельны вообще по чуть-чуть изучали проблога что-нибудь знаете обычно вы скольные
таблички у вас данные фиксированного размера правильно но есть иногда такие такие блоки
которые размер не фиксируем строка какая-нибудь не знаю заголовка вот она может быть от нуля до
256 байк или тело статью вообще-то не знаю нуля до 64 килобайка вот так решил наш редактор
может быть еще кое-что имя автора от нуля до 32 байка поехали да то есть как бы с числами все
просто не знаю 2 8 10 тогда как вот такую строку хранит база в большинстве случаев
что вы делаете да обычно она поступает очень тупо она просто берет считает максимум
максимум и вот этот максимум выделяет под каждую строчку вы вот так вот все хранить
либо какие-то значения еще что-то еще что-то с плавающей строкой либо выносит какие-то
особенно длинные значения куда-то поддельное храняющие в любом случае это строка может
быть длинной как баз данных работает диск два слова вы что-нибудь слышали про это знаете
нет не было база данных общается с диском так называемым за туплыми просто некий блок
по 8 килобайт например да позгас работает по 8 килобайт это означает что в 8 килобайт то
есть когда и нужно считать одно запись она не считает одно запись она читает 8 килобайт всегда
то есть когда читать эти минимум а дальше например если вы делаете какую-нибудь сложную выворку
большую да там не знаю и не все у вас допустим в индексов с какими-нибудь фильтрасты и так далее
она будет читать по 8 килобайт и даже если из каждых 8 килобайтного блока нужен один толпец
для того чтобы фильтр сделать она будет читать все 8 килобайт там одна запись поднимется одна
запись и в этой одной записи один столбик но все равно будет читать по 8 килобайт как с этим
бороться да то есть это вызывает довольно негативные последствия в работе с базы
данных тогда начнется работа базы данных если даже если вы программист все равно должны
будете вместе читать так называемый план выполнения запроса база данных можно спросить вот я
сконструировал запрос как собираешься выполнять и нам распишет как он собирается выполнить и нам
могут быть такие опасные строчки что я типа собираюсь просканировать всю таблицу дорогой
друг для того чтобы ответить на твой запрос это означает ну как бы это сам плохой вариант самый
медленный вариант выполнения короче я к чему беду я беду провести пример введения избыточности
ну например вам нужно делать какую-то систему фильтрации то есть у вас есть какое-то количество
фильтров и куча различной информации что вы можете сделать ну вы можете повесить
индекс конечно на каждый из кавказов которые будете искать по которым будете
фильтров а можете поступить в трее можете взять и эту информацию по которую
которая нужна для фильтров вынести в отдельную таблицу
выносите подельную таблицу дублируйте и здесь потому что нужна информация полная по объекту
и здесь и в таком случае здесь у вас неизвестная длина от не знаю записи килобайт а здесь у вас
точно сфиксировано здесь только числовые значения которые нужны только для фильтров и здесь
например-то мне не знаю 60 байт 60 байт влезает один тупл дофига и баз данных такой табличка
быстрее чем с такой
дожди ты не управляешь тем что читает она будет читать все 8 килобайт она будет читать все 8
килобайта брать нужно кусочек вы читать блокс для работы так так устроена не только баз
данных так операционка устроена и и работы с диском устроена и не будьте вычитывать кусочек
они здесь читает блок сразу памяти все распакуют отдадут тебе кусочек с памяти поэтому вы как
архитектор как программист разработчик высоконагружу системы вы должны читать и знать
как работает внутри и операционка ваша если вы вот так вот сделаете да нам это все просто
закширует у нее же есть какие-то буферы у баз данных вы когда ее настраивать и вы их говорить два
мегабайта на такой это буфер два мегабайта такой два мегабайта такой просто взяли посчитали
например у всего объема новостей если взять только цифрки какой бы объем данных получится
быть небольшой потому что цифр их мало если вы к ней часто к этой таблице будете обращаться база
данных это естественно понимает это естественно киширует вот это пример введения избыточности
это такая не совершенно не рациональная логика вы берете и одни и те же данные храните два
казалось бы у вас есть из коэль таблицы все с них хорошо ну нифига вот так будет работать быстрее
ну не конечно она не выделяет под максим она скорее всего вынесет это вообще поддельное
в отдельный этот так сделать указатель выложили поддельный кусочек с блобами и так
далее все зависит от конкретно баз данных я не знаю как принцип такой дальше то есть варчара
это плохо то что не позволяет работать с этой штукой нормально так окей хорошо с этим закончили
вот баз данных 1 2 3 4 5 6 7 8 что у нас осталось у нас осталось совсем чуть-чуть
нет почему у тебя для если ты знаешь что у тебя есть какой-то класс запрос нужно быстро
вычислять ну типа вот вот идеальный пример фильтрации ты берешь все что касается фильтров
например делаешь до некой длины если у тебя эти данные вот как раз большим количеством варчара
да а потом ты уже для тех кого-то отфильтровал пойдешь вот в эту таблицу и по
идишнику вытащишь полную строчку так будет быстрее чем фильтровать вытаскивает сразу
понял принцип ты намеренно портишь структуру своей базы данных для того чтобы вытаскивать
данные оттуда быстрее по идишнику выбор очень быстро всегда да по идишнику всегда выбор очень
быстро здесь очень просто но все равно она тебя будет читать если у тебя айдишники если у тебя
большая большой пайл из базы данных и ты вычисляешь ты берешь четыре значения по идишнику даже а
эти вытаскивать будет не по одному а именно по 8 по 8 она будет находить тот тупол где лежит
нужно тебе айдишник читать по 8 килобайт и поднимать но это все равно будет быстрее так
блок нет blob это это что типа но это варча blob это просто
кусок данных неограниченной длины что-то типа такого картинки можно хранить в базе данных при
желании так то есть мы здесь с вами из широкой таблицы так называемая широкая таблица делаем
узкую что у нас еще осталось как работает яндекс ребята из яндекса расскажите как поисковая
я задаю вопрос запрос путин как будет выполняться мой запрос
распарсили дальше
чего построить
с поисковым движком нет не так ну может быть и так детали но там неба
вопрос под охом путину вообще сказать не будет путин 100 процентов закашировались
путин и достанет из каша а ну например там я не знаю пт и главный корпус как
будет хорошо сентябре возможно сейчас сейчас вряд ли да как будет выполняться запрос как
вообще как вы думаете как вы предполагаете как устроен поисковый индекс январь
какие есть предположения как выполняется этот запрос
нет ну может быть я сейчас не про немножко не про это и видно что весь индекс на одну машину
не влезает он влезает не знаю на 10 тысяч машин допустим
ну да
да примерно так но чуть посложнее то есть у вас есть и он огромного размера
индекс некая обратная структура которая говорит о том что слова
мть ии встречается в таких-то страницах на просторе интернета пола главный встречается
в таких страницах на просторе интернет слово корм встречать на вас как тогда перс today
overnight такой странички невозможно яндекции индексируют то есть scriptures
содержимая страница, пытается понять в его семантику смысл, пытается понять, какие слова,
что означают, к чему они привязаны и так далее, и строит обратный индекс. Слово страниц.
Собственно говоря, в базе данных то же самое. Вы индексы что ли не проходили даже? Ну да,
вы индексы проходили в базе данных. Точно такая же логика. То есть у вас есть ID-шник. И индекс
позволяет быстро по этому ID-шнику, по некоторому дереву найти в листочке этого дерева конкретный
адрес, физический адрес, где в файле базы данных лежит вот этот самый ID-шник. Индекс позволяет
не перебирать все подряд, а найти очень быстро, чаще всего по бинарному дереву найти, где находится
этот самый ID-шник физически и достать оттуда строку. Вот то же самое здесь. Только здесь слово,
а в качестве значения все потенциальные места, где находится это слово. Соответственно, для того,
чтобы сделать вот этот МФТИ главный корпус, нужно найти, где находится МФТИ, где лежит главный,
где лежит корпус и попытаться это пересечь, а потом отранжировать. Но проблема в том,
что индекс колоссальных размеров. Он просто гигантский. Его ни на один диск не записать.
И к нему огромное количество запросов. Как с этим бороться? Скорее всего, я не знаю детально,
но скорее всего сделано так. Он побит на какие-то кусочки. Отшаргировано. Каждый из этих кусочков
вы не можете положить просто на один шар, потому что МФТИ главный корпус, а также любой другой
запрос нужно искать в каждом из этих кусочков. То есть он как-то порезан. То есть это слово нужно
искать везде. То есть нужно выполнить параллельное выполнение. Когда вы берете один запрос,
выполняете его сразу на этих машинах. Потом склеиваете результат. Это одна машинка не справится
со всем потоком запросов. Скорее всего, каждый кусочек индекса в Яндексе лежит на нескольких
серверах. Абсолютно одинаково. И это какая-нибудь структура. Здесь я рисую 3, здесь может быть сколько
угодно машинок и так далее. И она случайна. Вот это один кусочек индекса. Это другой кусочек
индекса. Она ищет МФТИ во всех какой-то одной из этих машинок. Здесь, например. Ищет это слово.
Вот как-то так. Потом это все соберется, склеится и вернет в пользу. Отранжируется и вернет в пользу.
Вот какая-то такая конструкция. Паттерн называется параллельное выполнение.
Думаю, что может не хватить ее мощности. Вот здесь мы решали проблему о том, что у нас слишком много
данных. А вот здесь мы решаем проблему о том, что у нас одна машинка может не справиться.
Примерно так же, например, хранит свои данные YouTube. Он не может одно видео хранить в одном
видео. Скорее всего, видео хранится на каком-то кластере. Называются бинарные кластеры очень часто.
Бинарные кластеры. Но здесь, в этом кластере несколько машин. И это видео хранится на всех.
Это отказустойчивость тоже самая. Что произойдет, если вот эта машинка умрет?
Две другие продолжат свою работу. То есть это видео не пропадет?
Но они не страницы передают. Они наверняка передают какие-то следки страниц или какие-то индексы страниц.
Да, я думаю, что таких машин не одна. То есть скорее всего, запрос в Яндексе выполняется очень
сложно. Вот ты забираешь слово ты и главный корпус и понеслась. Сначала там, не знаю, первый слой,
который занимается сборкой. Вернее, даже скорее так. Сначала, например, там первый слой, который
занимается сборкой страницы. Он идет отдельно в индекс, отдельно он идет в рекламный кабинет,
отдельно он идет в новостной кабинет. Ищет по этому новости. Другой ищет по этому рекламу.
Третий ищет там, я не знаю, по этому погоду. Этот пошел в индекс считать. Здесь какой-то очередной
слой машин. Взяла работу эту машину, пошла в нужный индекс и так далее. И вот как-то это все
собирается и отдает использовать. И анекс мега сложная. Это мега сложная история.
Я не знаю, но можно посчитать. Но много, я думаю, что много. Это внутренняя сеть. Я тоже не знаю. Я думаю,
что мегабайты. Но это нормально. Это внутренняя сеть, она не внешняя. Ты за этот трафик не
платишь. Это как бы машины в соседних, грубо говоря, этих стойках стоят. Десяти гигабитные,
наверняка какие-нибудь между ними линки. Короче, это очень симпатичный паттерн.
Тогда вам нужно решить какую-то сложную задачу. Но надо иметь в виду, что на это способное
ограниченное количество программистов. Если вы такую штуку выбираете, лучше этого не делать.
Потому что для этого вам придется набирать команду типа яндексовскую, платить ребятам
совершенно бешеные деньги. И они вам да напишут. За пять лет разработки они вам напишут такую штуку.
Поэтому в большинстве случаев стараются использовать что-нибудь такое наколеночное.
Дальше еще один паттерн, который мы сейчас быстренько посмотрим, называется специализированные
сервера. В большинстве случаев у вас мы с вами рассматривали какую-то стандартную задачу.
Например, новостной сайт или что-нибудь подобное. То есть какие-то такие проекты,
которые ничего сложного из себя не представляют. Но у вас есть целый класс проектов,
которые стандартным способом не решишь. Стандартной трезвенной структурой не решишь.
Например, игра, дота или варкрафт или чем угодно. Короче, есть паттерн,
который называется специализированные сервера. Или, например, раздача видео,
потокового видео. Для этого нужна некая специальная логика работы вот этого сервера.
У нас, помните, один из паттернов назывался трехзвенной структурой. Мы с него начинали.
Трехзвенная структура. Общая логика, frontend, backend и bazdan. Ну или система хранили. Вот для
всего этого есть некие стандартные сервера. И это некая стандартная, самая простая логика.
Но она все-таки довольно медленная. У нас есть некий оверхед на передачу данных от
одного блока к другому. Вот как раз это то, что тебе волной попало внутри яндекса.
Для некоторых проектов это не подходит. Мы придумали такую штуку, что, ребята,
давайте для тех проектов, для которых это не подходит, мы напишем, что это специализированные.
Специализированный сервер. Сюда идут все игры. То есть вот в этом случае,
вы когда используете стандартное решение, да, то есть как выглядит ваша программа внутри
backend. Опять же, я вам рисовал в самом начале нашей истории. Есть некий стандартный сервис,
веб-сервер. Который вызывает вашу программу и передает ей данные. Но ваша программа может
быть на чем угодно. Я не знаю, на руль, на PHP, на чем угодно, на джаве. Но все заботу про
установление серверного соединения, там я не знаю, какие-то там, не знаю,
алоцирование какой-то памяти, там еще чего-нибудь, да, то есть вот работа сейца,
работа, такая техническая работа, она на нем, на веб-сервере лежит. Специализированный сервер
прекрасное решение, но только все на вас. Да, он будет работать быстрее. Именно за счет того,
что он будет очень узко заточен под конкретную вашу задачу. Игры, не знаю, какой-нибудь чат,
процессор, видеотрансляция, видеосайты и все прочее прочее. Именно когда трансляция идет,
не когда ты в файл в YouTube ты по большей части файл скачиваешь, в большом счете. Просто скачиваешь
конкретный файл, его плеер проигрывает. А есть истории, где ты не просто должен скачать файл,
где ты вот принимаешь, например, из одного источника и транслируешь в 10 других. При этом
еще проверяешь, что здесь что-нибудь какой-то перекодировкой занимаешься. Ну, например, с камеры.
Вот есть же ребята, например, вот вы меня сейчас снимаете, ты на файл на диск запишешь,
что-то такое. А есть, например, мы на конференциях делаем, снимаешь и в прямом эфире транслируешь.
Ну, относительно в прямом эфире, здесь все-таки некоторая задержка происходит на этом процессе,
не знаю несколько секунд, может быть секунд два, пять, но все равно в реальном
времени происходит. Это принципиально другой процесс. Для этого нужно специально писать вот
эту саптину. Таких саптинок не очень много, например, связанных с видео их штук пять в
всем мире. Которые умеют это делать хорошо. В чем одна из них написана в России. Что еще?
Что еще может нам всем потребоваться?
Игры, видео.
Не проходит первая схема, да, где она настолько,
где из-за ее универсальности она становится прям совсем неподходящей и неудобно.
Чаты, да. То есть, короче, вещи, где нам нужно поддерживать постоянное соединение. Вот эта вот
штука у нас работает на запрос-ответ. На запрос-ответ она работает идеально.
А иногда нам нужны постоянные соединения из клима. И вот здесь вот эта штука рассыпает. Здесь
везде тайм-ауты, здесь все разрывается и так далее. Здесь постоянные соединения, возможно, только в виде
имитации. В чатах есть такой сервер, называется Комет-сервер. Класс серверов,
подход, да. Установление постоянного соединения. Вот это как раз все сюда.
Комет-сервера, чат и так далее. Вот. На этом, собственно говоря, все.
Давайте вспоминать еще раз. Значит, базы данных мы проговорили.
С чем мы начинали? С трехзвеннего архитектура.
Еще что назвал? Какие паттерны? Каширование.
Чего? Так, это ты мне перескакиваешь. Это сервисно-ориентированная архитектура,
когда мы хотим. Или микросервис. И к ним как средство обмена информации между ними братишь
сообщений. Так, еще. Толстый клиент. По сути означает, что мы переносим в браузер,
но на сторону клиента мы переносим часть вычислений. Вы знаете, толстый клиент идеально
коннектится, например, с Комет-сервером. Вот чат, как писать. Да, блин, переложить максимум
на стороны толстого клиента, а на серверной стороне поставить как раз Комет-сервер,
который будет с ним держать постоянные соединения с этим толстым клиентом и передавать ему какие-то
команды. Вот типа у вас чат получится. А вот так будет сложно. Ну, потому что нужно будет каждую
секунду запрашивать, не пришло ли новое сообщение, не пришло ли новое что-нибудь. Так, что еще?
Давай, какое? Настабирование. Горизонтальное. Запускаем несколько бэкендер, запускаем несколько
фронтендер. Вертикально. Память докупаем и процессор помощен. Что еще? Аналогично.
Функциональное разделение там, функциональное разделение здесь. Точно так же бэкендер можно
поделить. Разделение. Форум на одну страничку, новость на другую. Остатки на счетах на третью. Еще.
И все. Отлично. Отложенное вычисление тоже целый блок. Мы используем то факт, что нам не
всегда нужно все сразу. Это раз, а во-вторых, иногда для того, чтобы обработать запрос пользователей,
нужно очень много поработать. Видео перекодировать, фотографию обрезать, обработать, повернуть и так
далее. Поэтому мы разделяем работу. Часть делаем сразу, часть ставим задачей. И сюда же у нас,
куда мы ставим задачи, мы ставим задачи в очереди. Что еще здесь? Отложенное вычисление. У нас там были
еще различные варианты. У нас был еще здесь конвейер. Когда мы строим цепочку, данные передаются
из одного к другую, третью и так далее. Или, например, через очереди. Помните, куда мы обрабатывали?
Я не знаю, статистику обрабатывали. То есть, некая конвейерная обработка данных. Еще у нас была
ассинхронная обработка, но это в очереди. Согласен, да, это не совсем правильно. Стрелочки не верны.
Ассинхронная обработка, а здесь же очереди. Еще у нас с вами была деградация функциональности,
как некий подход. Когда мы намеренно снижаем функциональность нашей системы, зависит из-за
роста, например, нагрузки. И вот эти два. Специализированный сервер и параллельное вычисление.
Параллельные вычисления и специализированные сервера.
Все? Пока. Ладно, сейчас еще осталось делать за мало. Осталось научиться этим пользоваться.
Это, конечно же, не все. Они периодически возникают новые. Их периодически кто-то придумывает,
и так далее. Я не знаю, давно появилась сага, еще что-то, еще что-то. Но принцип я вам по-большому
счету все обрисовал. То есть вы сталкиваетесь, то есть у каждого из этих паттернов, если вы
вспомните, есть логика. Их можно даже не запоминать. Он возникает в ответ на какую-то
особенность нашего с вами программного кода. То есть то, что нам задача ставил менеджер продукта.
Заказчик. Итак, алгоритм проектирования. Собственно говоря, о чем мы здесь собрались. Как понять,
какой из, что и когда использовать. Итак, к вам приходит заказчик и говорит, хочу Facebook. Ну ладно,
для начала хочу, я не знаю, Tinder. Ну нет, Tinder слишком просто. Сейчас придумаю,
что хочу. Ну, допустим, я хочу, знаете чего, я хочу под Сочи 2014 или под какую, я не знаю,
FIFA 2018 сделать спортивные трансляции. Вот у меня есть комментатор. Вот я хочу,
чтобы его куча народу слушала. Давайте придумаем алгоритм. Или смотрела, или читала. Читала.
Алгоритм проектирования. Что вы сделаете первым? Вот я ваш заказчик. Давайте, первый шаг ваш.
Понеслась. Да, вы начинаете ему задавать вопрос. Помните нашу матрицу? У нас такая была
матрица. Мы спрашиваем про функциональные, про функции будущего проекта. Потом про различные
цифры этого проекта. При этом про все это мы спрашиваем, как в прошлом, ну в смысле,
как в реальности, как сейчас будет, потом как будет в будущем. Не только про цифры мы спрашиваем,
как будет в будущем, но и про функциональность спрашиваем, как выйдет в будущем. А про все цифры мы
с вами спрашиваем опять же в трех разрезах. Средняя и максимум. Вот мы с вами вот это все,
вот этот корпус информации собираем. То есть первый шаг. Сбор информации.
Первый шаг алгоритм. Допустим, вы все собрали. Шаг второй. У вас есть полное описание всех функций,
всей бизнес логики. Сбор информации, сбор бизнес логики и так далее. Всего,
чего вы хотите знать про этот проект, вы все узнаете.
Ну оно простое, но дорогое. Ну понятно, как именно. Вот с чего вы подступитесь? Вот как,
вот архитектуру спроектировать нужно. Как ты будешь выбирать инструмент? Ну не совсем. Хорошо.
На основе чего? Какие ресурсы? Для того, чтобы оценить ресурсы, тебе нужно сначала понять,
какие ресурсы потребуются. Короче, второй шаг. Мы с вами пытаемся понять. Нам описали бизнес
логику. Нам теперь нужно понять, какие потребуются ресурсы. Какие данные есть в системе? Как они
с друг с другом взаимодействуют? Как эти данные обрабатываются? Нам нужно с вами нащупать хоть
что-нибудь, на основе чего мы с вами архитектуру-то и построим. То есть второй шаг. Анализ того,
что потребуется. Строим тему движения данных внутри проекта. Это не совсем бизнес логика.
Бизнес логика тебе информацию предоставляет в виде того, что вот эта страничка должна себя вести
вот так. Например, пользователь туда вбивает, нажимает на кнопочку купить акции, должно
происходить покупка акции. Тебе нужно из бизнес логики вытащить некую модельку. Как эта штука
будет у тебя работать? Пока без серверов, без всего и так далее. То есть у тебя есть акция,
есть пользователь, они взаимодействуют так-то. То-то, то-то, то-то. То есть некая модель. Мы эту
модель строим для нескольких целей. Модель данных внутри системы. Я даже сказал так. Модель движения
данных внутри вашей системы. Нам нужно. Из этого мы с вами сделаем несколько выводов. Первый
вывод, который мы сделаем, мы оценим ресурсы, которые нам нужны. Второе самое важное, мы с вами
поймем, где у нас критический путь так называемый. То есть, за что мы упрёмся? То есть, вокруг чего
нам плясать, понимаете? То есть, что нам потребуется, да? То есть, это очень связанная вещь. Оценка ресурсов
тоже нам придет вот туда, в критическом пути. Ну, например, мы с вами проектируем. Вот давайте
попробуем на лету. Мы с вами проектируем YouTube. Вот нам рассказали, что могут делать пользователи
на YouTube. Вот мы прикинули какое там, я не знаю, среднее количество роликов. Еще что-то, еще что-то,
еще что-то. У нас с вами появляется представление о том, что нам нужно, во-первых, дохрена места
для того, чтобы это все хранить. Это раз. Во-вторых, дохрена там, я не знаю, каналов для того, чтобы это все
отдавать. И вот у вас уже начинают вырисовываться некие критические точки, вот так называемые.
Критический путь, критические точки. То есть, где, во что вы будете упираться? То есть, во-первых,
в диск. То есть, явно нужно придумывать что-то, как это все хранить. Во-вторых, вы выпьетесь в bot.
То есть, явно нужно придумывать, как это все раздавать. Еще вы упретеся в перекодирование,
потому что данные готовятся долго. То есть, как-то опять же придумывать, как это все перекодировать.
Возможно, для каждого из этих проблем у вас будет отдельное решение. Например, другое. Facebook.
Лент, лента новостей. Нет, не согласен. Лента новостей.
Ну вот вы начинаете строить модельку. Вот у вас есть пользователь, у него есть пост,
у него есть друг, друзья. Друзей есть там, я не знаю, свои посты. И вам нужно построить вот эту
френд-ленту. В случае с фейсбуком вы упрятите то, что вот дохрена дамы. У каждого пользователя там,
я не знаю, 100 по 100 и, например, я не знаю, 100 друзей. Вот у вас уже 10 тысяч объектов,
которые вам нужно будет отсортировать под каждого пользователя. В случае с фейсбуком вы упрятите
то, что у каждого пользователя все индивидуально. Для того, чтобы он посмотрел свою ленту, вам
нужно будет архи огромное количество действий сделать и попытаться это как-то построить. И
вообще непонятно как это построить. Фейсбучную ленту. Знаете почему? Да потому что у вас вот
это все еще с этими, с постами, оно еще, блин, на одну машину не влезает. Так бы вы могли какую-нибудь
SQL табличку сделать и пытаться по ней как-то ее оптимизировать и пытаться по ней делать выборки
друзья посты. Не, нифига не получится. А какой-нибудь пользователь, у которого друзей не 100,
например, не знаю, а 5 тысяч или он подписан, не знаю, на 10 тысяч друзей, на 10 тысяч человек,
вообще кладется всю эту систему. Это что нужно? Для того, чтобы построить, нужно у каждого из этих
10 тысяч вытащить их последние посты. Причем, по всей видимости, не за последние несколько недель.
Потом это все как-то пересортировать, а все эти посты лежат на разных серверах. Ну то есть это как-бы как это делать вообще?
Ну да, вывод мы пойдем скорее всего по пути какой-то избыточности, то есть мы как-то будем эти данные,
не знаю, дублировать в 10 мест и так далее, чтобы хоть как-то эту ленту выдавать.
То есть, короче, вот вторая задача. Вам нужно ответить на вопрос по поводу критического пути.
Что у вас, что не решается в лоб, что закончится быстрее? Во что вы упрятесь?
То есть, некая как раз вот эта вот проблема, которую нам, архитектурой, и надо будет решить.
Дальше. Ну а дальше, собственно говоря, продолжение этой проблемы.
Вот здесь, конечно, магия произойдет. Ну то есть мы с вами, перебирая некие особенности данных,
которые у нас получились, я бы еще знаете, что добавил перед решением? Я бы еще добавил,
что очень часто особенности решения находятся не только из критического пути. Вот так бы я бы сделал.
Я бы еще добавил сюда допустимую деградацию. Допустимая деградация системы. То есть от чего мы можем отказаться?
Например, в нашем фейсбоке. Они у нас обязаны, да, можем ли мы что-то потерять? Или можем ли мы не в хронологическом порядке их выстраивать?
Если вы заметите, в социальных сетях никогда хронология не выставлена в хронологическом порядке.
Она выставлена в примерно хронологическом порядке. Но у вас может быть сначала двухчасовой пост,
потом пост, который был две минуты назад, вот, а потом пост трехдневной данности, а потом трехчасовой пост.
Не-не-не, даже ВК, она не всегда строится именно вот так. Да, можно сейчас открыть любую ленту и посмотреть.
Прямо увидите, наверняка. То есть короче, иногда выход лежит здесь. И вот уже четвертым пунктом решение.
Ну что, давайте попробуем. То есть в решении мы с вами как раз на основе всего того, что собрали.
На основе бизнес-логики, на основе критического пути, на основе каких-то особенностей вот этих вот самых данных.
Да, оцениваем ресурсы, что закончится быстрее, то некие особенности.
Например, какая ведь королевская чета в Великобритании выходит замуж. Женец. И оттуда идет трансляция.
Вот в чем особенность? Все будут смотреть, да. Данных колоссальное количество.
Еще какая особенность? Не только, не про это. Один сочек единственный.
Может быть мы с этим что-то сыграть сможем?
У нас не сайт трансляции. Понимаете, эта штука будет, даже если у нее трафик будет в 20 раз больше, чем на Ютубе,
я не знаю, в 100 раз больше на Ютубе, она будет более простой. Почему? Потому что источник один.
Нам не нужно думать ни о чем. Источник один и источник нами полностью контролируем.
То есть они все идут с одной точки. Нам отдают готовую картинку, а мы ее распространяем.
Это гораздо проще сделать, чем, например, Ютуб, где источников множество и где они могут обновляться кем угодно и в какое угодно время.
То есть как бы за что-то, вот все, что выше, это пояс того, за что мы с вами зацепимся для того, чтобы спроектировать наше решение.
Можно чуть ли не перебором. То есть на что нужно смотреть?
Критический путь, что закончится быстрее, некую потенциальную деградацию, о чем мы можем отказаться, некие особенности данных.
Ну и ресурсы. Прямо смотрим прицельно, что у нас, где у нас есть какая-то фишечка, которую мы сами можем использовать.
И потом пытаемся спроектировать.
Давайте попробуем.
Например, первый сайт.
Первый, что мы с вами будем проектировать.
Спортивная трансляция.
Погнали.
Спрашивайте.
Проектируйте мне спортивную трансляцию, парни, пожалуйста.
Нет, пока не нужны комментарии.
Значит, логика такая.
Есть три редактора.
Один отвечает за фотки, два отвечают за текст.
Они смотрят футбольный матч в реальном времени.
Что-то происходит.
Они пишут маленький текстик, небольшой.
Не знаю.
В среднем там байт-стон.
Публикуют его.
Все остальные это читают и смотрят.
Ну, читают.
Трансляция текстов.
Текстов и иногда картинки.
Ну давай так, давай первую часть сначала решим.
Трансляция только тексты.
Как только пользователь...
Как только...
Первый хороший вопрос.
Давайте так.
Задержки до 10 секунд.
Нормально.
Ты все равно комментируешь.
Это комментатор пишет.
Через какое-то время.
Нормально.
До 10 секунд.
Нет, мы хотим это у себя.
Рекламу, пользователи и так далее.
Нет, это сайт наш.
Но вопрос хороший.
Да.
Product Manager тебя возненавидит.
Ну, допустим, ладно.
Так.
Что еще?
Что еще вы меня спросите?
Пользователи, которые смотрят трансляцию.
Ну, допустим, миллион.
Они не должны обновлять страничку?
Нет, они не должны обновлять страничку.
Они открывают браузер.
И страничка сама обновляется.
Миллион человек.
Не, по всему миру разбросано.
Давай, пока без картинок.
Первая часть без картинок.
Первая часть без картинок.
Да.
То есть, если я зашел только что,
я должен иметь возможность прочитать
с самого начала.
Да.
Я не понимаю тебя.
Какие Пуши?
Я Product Manager.
Ты вебе Пуши не отправишь?
Это как бы...
Браузеры умеют Пуши?
Да.
И что происходит?
Как реагирует браузерная?
Не, ребята.
Это немножко не то.
Этот Пуш...
Ну, ладно.
Я понял, про что вы имеете в виду.
Нет, таких штук не надо делать.
Я смотрю сайт.
У меня открыто браузерное окно.
У меня там все периодически обновляется.
И у нас таких миллион.
Параллельно давай одно.
Да нет, одно.
Итак, у нас есть трансляция.
У нас есть миллион читателей.
У нас есть время обновления
максимум 10 секунд.
И у нас есть два редактора, которые пишут.
Пока только читать.
Ну, все. Спросили, в принципе, все.
Нам достаточно.
В среднем одно сообщение.
100 байт, например.
Ну, один обзор.
Как часто сообщения?
Ну, несколько.
Давай два раза в минуту.
Да.
Чего?
Да, потом будет больше.
Но вы про это не думайте пока, реально.
Давайте мы вот пробуем
первое, первое, первое
про экспорт делать.
Так.
Так. Давай
попробуем
пройтись по второму
пункту чуть более подробно.
Оценка ресурсов. Чего нам нужно прикинуть?
Какие у нас ресурсы?
Давайте так. Что вообще прикидывать
то надо?
У нас по большому счету
нам нужно прикинуть диск.
Как это все место занимает.
Нам нужно прикинуть канал.
Нам нужно прикинуть
процессор.
Ну, память еще можно.
Если есть какие-то особенности.
Какие у нас еще ресурсы?
Ну, все.
Этим будут пользоваться постоянно.
Предыдущий, да, нужно хранить,
но вечно хранить.
Но она закончилась, она больше не меняется.
Это к вам вопрос. Я ничего не знаю.
Технически это делается
как раз с толстым клиентом.
Сейчас спроектируем.
Так. С диском у нас
здесь есть какие-то проблемы?
Сколько у нас матча?
90 минут, да?
90 минут.
Это у нас с вами
180 комментов. Давайте 200
с круглого счета.
По 100 килобайт.
То есть по 100 байт.
20 килобайт.
Театрансляция максимум.
На диск вылезает.
Канал.
1 миллион человек.
1 миллион человек,
и у нас еще есть 10 секунд.
Как?
Соответственно раз в 10 секунд,
то есть грубо говоря,
этот миллион человек делает к нам
6 миллионов запросов в минуту.
Да, или
100...
Короче, 1 миллион деленное на 6.
Правильно?
Правильно. 1 миллион деленное на 6
это у нас сколько запросов в секунду?
Ну, 1200, да?
200 к запросов в секунду.
Потому что 10...
Раз в 10 секунд
это у нас 6 запросов в минуту.
Правильно?
А подожди, раз в 10 секунд.
Да, ты прав, 100.
Конечно.
100 запросов в секунду.
Это много, мало?
Так, не мало.
Какой нам потребуется
для этого канал?
Если мы будем
отдавать ему всю страничку,
то
в конце
мача
каждые 100 секунд...
Каждый из этих запросов будет скачивать у нас
все 20 килобайт.
Это сейчас...
Ну, подожди, это уникальное.
Это уникальное.
Они сидят постоянно.
Мы должны разработать систему,
в которой они сидят постоянно.
Это не 2 мегабайта. Почему это 2 мегабайта?
2 гигабайта?
2 гигабайта в секунду.
Пока мы...
Если в лобешник решать, то да.
Значит, мы будем
начать думать над этим.
2 гигабайта в секунду — это у нас
18 гигабит.
Это не мало.
То есть мы
потенциально в этом
не будем упереться с вами.
Потенциально.
Процессор можем упереться?
Ну...
А?
Нет, вряд ли.
Согласен с тобой, да?
Память тоже. Ну, что у нас там 20 килобайт?
То есть как бы здесь
потенциально у нас проблема только с каналом.
Что еще?
Допустим, мы деградацию. 10 секунд.
Ну, все нам известно.
Какие-нибудь особенности данных
здесь есть?
Маленькие. Относительно редко обновляются.
Так, еще.
Все обновления?
Ну да.
Редко обновляются.
Я бы еще сказал, что они обновляются
из одного источника.
Короче, они все одинаковые.
Так.
Они одинаковые.
Ты имеешь в виду, что они одинаковые для всех, да?
Или что?
Они одинаковые для всех.
И здесь у нас
что еще было самое первое?
Маленькие.
Согласен.
А теперь попробуйте
совершать эту самую магию.
И исходя из всего вот этого
предположить, как бы мы могли
самым простым способом это все решить.
На что мы будем опираться?
Вот что с вашей точки зрения
здесь самое-самое-самое ключевое?
Так. Редко обновляются.
Это позволяет нам их кашировать.
Ну, допустим, так.
Еще.
Один источник и позволяет нам очень сильно
упростить
тему обновления этого блока, да?
То есть у нас ни Рейска Эйдишнов
там нет, ничего нет, мы просто тупо
похороняем и все.
Что еще?
Думаете, Леша?
А?
Одинаковые данные. Как бы ты это использовал?
Слушайте, одинаковые данные.
Ну да, где-то там, да.
Все совершенно верно, ребят.
То есть как бы здесь ключевая...
Смотрите.
Вот это все себе разрисовали.
Вот все это расписали.
Из того, что это один источник,
означает, что вы не паритесь по поводу обновления.
Нету Рейска Эйдишнов ничего.
То, что у вас одинаковое
все для всех,
означает, что можно использовать прям вообще
самую примитивную схему хранения.
Что бы вы использовали?
Файл. Тупо файл.
Бастданных нафиг не нужна.
Тупо файлик, который
не знаю, текстовый, HTML и так далее.
Файл.
Он для всех одинаковый.
Он для всех одинаковый,
все очень просто. И один источник
позволяет нам с этим файлом очень легко и просто работать.
А то, что мало данных,
означает, что он будет очень маленький, этот файл.
Так, хорошо.
С файлом, допустим, вы решили.
Так, быть с вот этой вот фигней.
С 18 гигабитами.
То есть толстый клиент.
Я понял.
Окей, хорошо. То есть
первое предложение.
Каждая запись,
отдельный файл.
А как они, как браузер
на стороне толстого...
Ну, как бы первое предложение
толстый клиент.
Не, подожди.
Браузер
по умолчанию
ты ничего отправить не можешь.
Он тебе запрос делает.
Вот он тебе какой запрос должен сделать?
Вот я захожу на эту страничку дальше.
Хорошо. Он регулярно,
раз в 10 секунд, вот у нас есть
вот это ограничение, делает тебе запрос.
Как выглядит этот запрос?
Это конкретная страница?
И что?
Чего он до какого момента?
Ну, это вы уходите
от нашей простейшей схемы
с файлом. Что значит до какого момента?
Получается у каждого индивидуальная история
в таком случае.
У каждого индивидуальная история
в таком случае.
Окей. То есть алгоритм у нас такой.
Так, у нас здесь
ристосный клиент, который заходит
на страничку в первый раз.
Получает там какой-то джаваскрипт.
В этом джаваскрипте
записано, сколько у нас
уже выпущено сообщений.
Вот, например,
n штучек.
Да, и эти сообщения, допустим,
да, там тоже есть n штучек
плюс содержимое этих самых
n сообщений.
Потом этот джаваскрипт
раз в 10 секунд делает запрос
на что? На n плюс один
ну, на содержимость
n плюс один точка
Ну, типа того. То есть он пытается скачать
следующие сообщения.
Окей, хорошо. Допустим.
Почему?
Ну, как
ты будешь там
наркотик
Ну, а
как
А как
выглядит тогда запрос?
Они появляются редко, но есть
проблемы. Они появляются нерегулярно.
У тебя может
в течение двух минут ничего не быть,
а потом какой-нибудь гол, и у тебя
а нефигач со скоростью-то, я не знаю.
Пять сообщений за 10 секунд.
Что у вас серверы ссылают? У вас серверы
туповат. Он только на запрос
отвечает. На какой?
А?
Ну, здрасьте. Вот ты нам
вопрос написок взял и усложнил жизнь.
Сразу.
Ну, окей.
Я понял.
Пачка html.
Запрашивайте следующие сообщения.
А
тогда как быть, когда у нас
девятое сообщение,
или, например, десятое сообщение,
к нам пришла эта пачка, мне 10 сообщений.
Ну, а мы же не знаем, может там уже есть 11-12
за это время.
А если гол произошел именно
десятым сообщением,
и следующая пачка уже готова
эмоциональных сообщений,
вы как-то себе жизнь усложняете.
Давайте проще.
То есть направление мысли правильно,
то есть запрашивать не все.
Давайте какое-то более простое.
Поняли предложение?
То есть, как бы,
под вариант такой.
Точно такой же JavaScript,
который
возвращает содержимое всего того,
что было до этого, ну, в смысле, получает
вместе с этим JavaScriptом содержимое.
И последнюю n.
И у нас есть,
грубо говоря,
вот у нас 200 файлов.
К концу игры у нас будет
200 файлов, правильно?
То есть,
один HTML
у нас, допустим, хранит
все данные, все
сообщения,
начиная с первого.
Два HTML
хранит все сообщения, начиная со второго.
Три HTML хранит все сообщения,
начиная с третьего и так далее.
В чем минус?
Когда нам нужно будет записать
сообщение номер 200,
мы должны будем сделать 200 записей.
Правильно?
Каждый из них добавить.
Так, хорошо.
А еще попроще.
Вот у нас сразу
это означает, что нам нужно
сто тысяч раз в секунду
этот скриптик выполнить.
Это не проблема,
но мы не хотим. Мы хотим максимально
тупую систему, простую.
Хорошо. Где у тебя будет
эта информация храниться, исходная
для этого скрипта?
Исходная.
Скрипт на вход
получает X, правильно?
Откуда он берет сообщение, начиная
с номера X?
Открывает файлы,
скобилирует
и отдает.
То есть, мы вместо того, чтобы
один раз записать в 200 файлов,
собираемся сто тысяч
раз в секунду
читать из 200 файлов?
Можно?
Хорошо.
Да, есть такой вариант,
но тем не менее, ты собираешься
даже если там будет page cache и все прочее,
ты собираешься совершить
кучу, не знаю,
системных вызовов.
Это не очень хорошо.
Этот путь
я даже согласен с тем,
мне не нравится вариант 200
файлов обновлять, но даже
это лучше, чем то, что ты предлагаешь.
Потому что хорошо, к диску
мы ходить не будем.
Но
к диску мы ходить не будем.
Но ты все равно будешь вызывать
операционную систему с запросом
открой этот файл и отдай мне
его содержимое.
Там ты один файл читаешь. Нет, там ты и делаешь,
каждый запрос читает один файл.
А здесь ты будешь читать
энное количество файлов.
Ну хорошо, но больше
чем один.
Он предлагает делать скриптик,
в котором
содержимое этих файлов собирать
на лету.
Да, ребят, я понимаю, но вы
нарушаете нашу, вы не используете
по большому счету,
вам нужен будет процессор.
Давай даже
просто тупо посчитаем.
Проверить, есть ли такой файл.
Раз.
Собрать его.
Два, если его нет.
Отдать его. Три. То есть у тебя
два системных вызова
на одну историю вместо одного.
Ну посчитайте.
Вот сколько у вас будет.
Вот у вас за каждый 10 секунд.
За каждый 10 секунд у вас
произойдет 1 миллион запросов.
У вас будет 1 миллион запросов.
Каждом из них
два системных вызова. У вас будет 2 миллиона
системных вызова.
В случае, если вы сделаете вот так
и у вас раз в 10 секунд что-то обновляется,
у вас будет 1 миллион
плюс 200 системных вызовов.
Ну как лучше? Как легче сделать?
Ну конечно в втором варианте лучше.
Не плохо.
А
Н-ная строка у тебя
фиксированной длины.
Как ты вычисляешь адрес?
Считать файл с 15-ой строки
это невозможно.
Можно не читать файл с
офсета такого-то.
И этот офсет жестко задан в байтах.
Мне нужно фиксированно
делить и все прочее.
Нет?
Это в среднем?
Я не помню, но
те же самые раз 10 секунд.
9 раз вычитать файл с 15-ой строки.
А 9 раз
прочитать это все.
Все какое-то извращение. Придумайте проще вариант.
Помнить о
офсете
отверстия в этом огромном массиве
Вот.
Ты предлагаешь просто
положить в какую-то структуру памяти.
И написать собственный сервер
для этого.
Можно,
но это уже сложнее.
Антенны.
Тебе нужно писать собственный сервер.
Нафиг нужно?
А?
Кто будет память хотеть?
Вообще да.
Inginx умеет читать
и ходить в кэш напрямую.
Но если ты захочешь,
чтобы у тебя Inginx читал из памяти,
ты начнешь писать модуль для Inginx.
Это уже сокращает в 2-3 раза
количество программистов, которых ты можешь
привлечь к проекту.
Вот в этой нашей схеме,
даже вот в этой схеме,
у нас на сервере нет
программного обеспечения вообще.
У нас есть стандартный фронтенд,
стандартный Inginx, который файлы сдает в диск.
Все.
Мы ничего не программируем. Мы программируем
это.
Да, и то, что мы в конце пишем
200 раз.
Я согласен.
Пока это самый простой способ,
который вы придумали.
Вот что происходит с клиентами,
которые
приходят в первый раз
на 89 минут.
Уже лучше.
Еще упростили систему.
То есть мы поддерживаем сами
точно такой же
JavaScript.
У нас есть файл, который
у нас есть.
AllHTML
и nf, конечно, последний.
Так.
А вот теперь, допустим,
меня...
Как мне понять,
собственно говоря?
Вот я JavaScript.
У меня пропало соединение.
А потом через 10 минут восстановилось.
Мне куда идти?
А?
А какая логика у меня?
Почему я должен запросить все?
А, допустим, у меня
тайм-аут на минуту произошел.
А?
Не, у меня на минуту
пропало интернет. Это не то, чтобы тайм-аут.
Я же каждый раз запрос делаю.
Лишний запрос.
Понятно.
Вместо него отдать пола
что-нибудь.
Потом я должен буду там JavaScript-ом разобраться.
Ну, можно символическую ссылку.
Хотя не очень это...
Работать будет,
но не очень красиво.
Вы зацепились
для идеи
вот эту вот.
Потому что JavaScript у вас запрашивает
nhtml.
А можно как-то без этого?
А что
в update.html лежит?
А там последние
n-постов или там последние
все посты за последнюю минуту?
А как лучше?
За минуту лучше, потому что у вас
на JavaScript-е вы про минуту
знаете, что у вас прошла минута
на стороне толстого клиента.
А сколько за это время
постов было сделано, вы не знаете.
То есть, грубо говоря,
если мы с вами договариваемся, что у нас
есть два файла.
Первый файл
это все посты, а второй
это, я не знаю,
update.
И мы точно знаем, что в update
лежит последняя минута.
Все посты
за последние 60 секунд.
То тогда наш JavaScript
в зависимости
того, например, он отвалился.
Он может вычислить, куда его находить.
Потому что он знает, насколько он отвалился.
Он знает, сколько времени он не получал
информации.
У него есть последняя, то есть у каждого
JavaScript-а есть некая переменная.
Последнее обновление
он последнее обновление знает.
И он знает текущее время.
Он всегда может вычислить, куда ему
ходить, сюда или сюда.
То есть, мы избавились от лишнего запроса
к серверу на выяснение
деталей.
Если ты заходишь в первый раз, ты всегда идешь
вот сюда.
А дальше
ты каждый раз проверяешь, сколько времени
произошло с момента последнего обновления.
Если больше минуты, идешь сюда.
Если меньше минуты, идешь сюда.
И обновлять мы будем каждый раз.
При каждом обновлении два файла.
А он не стирается, он только обновляется.
Update
хранит обновление
за последние 60
календарных секунд.
Нет.
У тебя сразу пишется все в all
и сразу модифицируется update.
В update
у тебя каждый раз все старые сообщения,
которые не нужны в нем уже, они
стираются.
Мы можем поделить какое-то соотношение
с того, что у нас
вот эти трениры, когда
поделится.
Проблема в чем?
У JavaScript
59 секунд
с момента последнего запроса.
Он идет в updates
и идет дольше секунды.
Как обойти
эту историю?
Ну,
думайте.
Понятно.
Первый вариант.
Update содержит реально
120 секунд, но клиент
считает, что он содержит
только 60.
Этого у нас появляются минуты
на вот эти вот расхождения.
А еще варианты.
Time out на клиенте.
Значит, все равно может быть.
А если я параллельно Adobe Photoshop
запускаю, понимаешь?
Ну-ка, ну подожди.
Этот вариант хороший,
но не 100% надежный.
Потому что все равно могут быть
задержки даже больше чем на минуты.
Еще вариант.
Ответ
Time out
на клиенте.
Давайте еще украдем
количество постов, которые
выглядели.
Давайте просто действительно
внутри
updates.html и all.html
будем хранить некую, то есть как бы
пост, это не просто
текстовая строка.
Например, у поста есть какой-нибудь, не знаю,
номер.
Фиксумы давайте считать,
тупо говоря.
И вот JavaScript знает, что у него
последнее обновление было, там я не знаю,
на 109 минуте.
И при этом он
вывел, там я не знаю,
74 поста.
Последний номер поста, который у него есть,
74, он запрашивает updates.
И все вроде бы хорошо.
Но в апдейте, например, первый пост
начинается с 76.
Нам же очень
просто понять, что что-то не так.
Что-то не так, все тупо.
Идем к all.html, пересобираем
все заново. И начинается все
сначала. То есть мы просто вводим некой
простейший способ проверить
целостность того, что к нам пришло.
Например, через номер поста.
Или через время
последнего поста. Или что-нибудь еще.
Ну короче, какую-то неважную, какую-то
простую штуку, которую вы JavaScript'ом
можете проверить. И JavaScript
в таком случае у вас хранит две цифры.
Первый хранит время, второй
хранит как раз значение этой самой
чек-суммы.
Какого-то параметра,
через который мы все проверяем.
Почему?
Два запроса у нас получается только
в случае, если мы получили что-то
неволидное. Мы получили,
посчитали неволидно, идем сюда.
Не всегда это происходит.
Это как раз да, это самый редкий кисть.
То есть заходим в первый раз, сразу
идем сюда.
Дальше пытаемся читать апдейтс.
Если что-то ведет не так,
идем сюда. Так, хорошо, а в апдейтсы
у нас...
Мы в канал-то будем влезать?
Какого размера у нас апдейтс?
Ну, допустим, сколько?
Ну, давай.
20 сообщений за две минуты.
Это у нас не 20 килобайтов,
это у нас 2 килобайта.
То есть это означает, что у нас
не 18 гигабит нам нужно,
а 1,8.
Потому что я неправильно считаю, да.
16.
1,6.
Ну, 1,6 гигабита
тоже плохо, конечно.
Многовато.
Что делать?
Нормально.
Ну, 1,6 гигабит
это гарантированно, что тебе нужно
1,6 гигабита.
Ну, 1,6 гигабита
это гарантированно, что тебе нужно
1,6 гигабита.
Это гарантированно, что тебе нужно
10 гигабитный интерфейс.
А 2 гигабита есть?
Ну, допустим.
Ну, хорошо. Но все равно не круто.
Мы очень жестко его используем.
А если вдруг что не так?
На нас ссылку опубликовали
где-нибудь.
Раз у нас все идут
и идет махом
поток новых пользователей.
Не знаю.
На 89-й минуте
происходит какое-то чудо.
Россия выходит в финал чемпионата мира.
Все про это узнали, все побежали смотреть.
У нас бат сфига за минуту.
Пришло еще там, не знаю, еще
полмиллиона человек.
И все они качают в первый раз
олах это имейт. И все. У нас все легло.
Уперлась в канал.
Что делать?
У нас пока да.
А как вы сделаете 2?
Так, то есть вы просто
тупо ставите 2 сервера.
На оба из них кладете оба файла, так?
И тогда пишет админка.
Ну, хорошо.
Лады.
Так, что еще здесь может сломаться?
Хороший вопрос. Что будем делать?
Итак, мы оставляем пока вот этот вариант.
Это пятая стадия.
То, что я сейчас делаю.
Это называется попытаемся ломаем.
Попытаемся сломать.
Что будем делать,
если не?
У нас 2 сервера.
На обоих из них
лежат оба файла.
Как писать?
У нас есть некая админка.
У нас есть редакторская интерфейс.
Который записывает
оба файла.
Нет, погоди. Есть проблема?
Атомарные записи.
Представляете, что такое?
Грубо говоря, файловая система
не совсем атомарна.
Если вы пишете что-то в файл,
файловая система не гарантирует вам,
что вы в этот же момент
можете что-то читать.
У файловой системы есть
атомарные записи.
Атомарные функции.
А есть не атомарные.
Вот если вы просто делаете
file upon,
и вы туда добавляете,
это не хига не атомарная история.
Окей, согласен.
А move – атомарная.
Операция переименовывания файлов
она атомарная.
Понимаете, у вас файлик,
вы добавляете в него вот этот кусочек.
Если вы тут делаете upon,
то кто-то читает, может считать
вот такой кусок от файлов.
У нас не чек-сумма,
у нас ID-шник.
Это у нас такая чек-сумма,
просто для проверки.
Некий ID-шник, который мы проверяем,
что все идет по следователю.
Мы это обходим как?
Мы пишем не в эти файлы,
мы сначала пишем
в файл, например,
аа1 уа1,
а потом вызываем
процедуру move.
аа1 в а.
А move – атомарная?
Move меняет, да,
move – атомарная,
если она при этом не копирует
данные из разных томов.
То есть, если у вас лежат эти файлики рядышком,
в рамках одной, не знаю,
что это говорит.
Move – атомарная, да,
она переводит переименование
в таблице адресации.
Ну, типа того, да.
Не суть важна, нам важно,
что она атомарна.
Выглядит как план?
Что делать, если вот эта вот френь сломалась?
Не, подожди.
Ааа...
Ааа...
Ааа...
Ааа...
Ааа...
Ааа...
Ааа...
Ааа...
Ааа...
Ааа...
Подожди.
Ааа...
Клиенты у нас теперь подсоединятся к этой,
начнут с ней работать,
мы это в толстой клиент запрограммируем.
А редакцию к моему интерфейсу что делать?
Ну, смотри.
Хорошо.
Через какое-то время вот эта штука восстановилась.
Что тогда происходит?
Тогда у нас
новые клиентики,
которые...
То есть тогда редакторский интерфейс
ее обновит только при очередной записи.
Так?
А до этого они у нас...
Ааа...
Вот когда у вас возникает такой вопрос,
вы параллелькуете к продукту и спрашиваете,
а что делать?
У нас там два сервера.
Вероятность того, что он выйдет из строя
именно в момент трансляции,
примерно такая.
Надо ли нам про это что-то думать?
Если он скажет надо,
то тогда вы начинаете думать о том,
как один сервер, что означает восстановиться
и так далее.
Какой самый простой способ
их синхронизировать?
...
...
...
...
...
...
Вот мы как...
В какой момент и как мы будем это реплицировать?
Не, ну подожди, руками.
Вот руками нельзя ничего делать.
Вот ты перезагружаешь его.
Вот он перезагрузился, поднялся.
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
можно но не надо да блин ставьте сюда процесс загрузки машины синхронизировать все с другой
и все уговорил да короче при загрузке машины просто-напросто возьмите и садите в не знаю
сингом каким-нибудь за папочку
команда юниксовая которая синхронизирует две папочки ну или там две машинки
ну блин а вас вы юникс не изучали ни в каком виде
в любого по большому счету в любой операционной системе есть некие некие инструкции которые ты
можешь задать как разработчик программист как сис админ которые должны будет выполняться в
процессе загрузки ну в этом как она называется виндус там даже там просто бот файл ботничек
короче любым способом это что напоминает помните у нас с вами была история с кишом со стартом
с холодным кишом когда у нас машина с кишом перезагружалась она загружалась и в кэше было
ничего очень простое решение точно также вставить процесс загрузки обновления этого киша самыми
простыми самыми часто встречающимся объектами вот тоже самое так точно такой подход все ребят
в принципе мы с вами спроектировали очень простую систему редакторский интерфейс пишет на две
машины в два файла в одном всегда все в другом только последние там не знаю 120 секунд плюс
javascript которого есть две перед внутренние переменные одна время второе это айдишник
последнего сообщения при первом запросе мы запрашиваем пол хт мл при всех последующих мы
смотрим сколько времени прошло и прошло меньше минуты запрашиваем updates смотрим
что у нас вернулась в апдейте проверяем его целостность проверяем что все номера идут
последовательно после чего идем в если не последовательно идем запрашиваем пол если
последовательно то выводим содержима updates как хранить данные вот здесь в каком виде
лучше джейсончик чем хт мл можно хт мл но лучше джейсончик почему джейсончик потому что
ты оперирует то что здесь javascript соответственно ему проще всего оперировать собственные массивы все
вот вся архитектура а теперь добавить сюда картинки
куда так
да сначала ты поспрашиваешь про то что потом купишь cdn да посчитаешь сколько тебе нужно
картинок и так далее а как их сюда то
короче либо cdn либо простой хранитель который будет балансировать между картинками как выбрать
с разных концов свет короче по счету по почитать умножить средний размер картинки на количество
запросов на то как и часто не появляется и так далее cdn эта история просто кто-то за вас
сделает поставить frontend и его настроить просто в нескольких различных точках земного шара и вы
себе нам общаетесь как с некой единой сущностью вы и говорите выложи к себе вот эту картинку она
ее выкладывает возвращает себе адрес и этот адрес в чем плюс cdn в том что он по большому счету
подстраивается под пользователях да то есть например пользователи с бразилии cdn состоит
из многих серверов которые по всему миру разбросаны пользователь из бразилии не пойдет в
россию пойдет сервер бразилии стоит за этой картинкой так далее cdn тебе вернет некую
универсальную некий универсальный урок который ты вставишь куда внутрь вот этих вот самых
сообщений все точно так же
load balancer это громко сказано у нас в этом случае просто нам нужно мы посчитаем например что с
нашим трафиком с такой с нашими каналами с такой нагрузкой справится например 5 отдающих серверов
картины но еще мы ставим просто на просто 5 серверов картинок на них простой frontend на них
engine x который тупо отдает эти картинки и все да а их ты можешь да а их и round robin на уровне dns
балансируешь но смысл это и есть как бы ну в нашей терминологии франк смеешь в виду что здесь нет
и не стоит какой-нибудь циски которые занимается балансировкой не нужно можно но не надо
так все в следующий раз будем проектировать сами все-таки frontend и facebook

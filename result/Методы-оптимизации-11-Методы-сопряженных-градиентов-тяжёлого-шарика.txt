Сегодня у нас сложная лекция про методы, которые работают лучше, чем градиентный спуск, но и на практике в честь и тоже.
Начнем мы с того, чем был запрос в прошлый раз. Начнем мы с напоминания о том, что было в прошлый раз.
В прошлый раз мы рассмотрели градиентный спуск и то, как он работает.
То есть показали, какая у него будет скорость сходимости в случае выпуклой функции и сильной выпуклой.
А также я привел как просто некоторый факт то, какие существуют нижние оценки точности на этом классе функций.
Был вопрос, откуда они взялись, и сейчас некоторое время хочется посвятить.
То есть откуда они берутся?
Чтобы показать, что выполнила некоторая нижняя оценка на скорость сходимости, нам нужно предъявить такую выпуклую функцию.
В случае выпуклой просто выпуклой функции, не сильной выпуклой.
Что будет происходить в случае сильной выпуклой, я в конце скажу.
Надо предъявить такую функцию, что если мы итерируемся с помощью такого метода, то заранее известно, что наша отличия сходимости
между значением функции в текущей точке xк плюс один и оптимальным значением будет не меньше, чем вот некоторые выражения, которые известны с номер итерации.
И вот сейчас в ближайших двух-три слайдах покажем, что действительно будет этот самый как квадрат, который не совпадает с тем порядком, с которым сходится обычный грядетный спуск.
И вот как раз таки за счет наличия этого зазора и возникает необходимость получения более быстрых метод.
Вот такая вот красивая иллюстрация.
Для того, чтобы построить такую функцию замечательную, точнее не очень, рассмотрим вот такую матрицу.
Она трехтиагональная, и ее размерность равна 2k плюс один.
То есть 0k это число итераций, которые мы будем делать.
И мы хотим построить такую функцию, для которой при заданном числе итераций сходимость будет точно не лучше, чем.
В общем, вот такая вот матрица трехтиагональная, имеется денечки над и под диагоналями, двойки на диагонали стоят.
Заметим, что для нее выполнена, если мы возьмем квадратичную форму с этой матрицей, то у нас образуется вот такое вот выражение.
Это легко проверить, просто если вы это распишете, то вы увидите, что у вас x и t будет всегда, и квадрат x и t будет встречаться два раза.
Это, собственно, двойки на диагонали.
А попарные произведения, которые тут фигурируют как минус 2 на x и икс икс плюс один, они возникнут за счет того, что у вас вот эти минус единички над и под диагоналями, они в сумме дадут минус 2.
Поэтому тут будет минус 2, все в порядке, последние иксы.
Понятно ли, откуда это взялось? Вообще слышно ли меня?
Так, кто-нибудь есть тут, кто готов что-то сказать? Слышно, хорошо, спасибо.
Окей, значит, поскольку это сумма квадратов, то это значит, что матрица положительно полуопределена.
Поскольку сумма квадратов не может быть отрицательной, поэтому у нас есть определение положительной определенности.
Более того, мы можем заметить, что если мы вычтем из диагонали этой матрицы четверку, то тут образуется минус двойки.
И если мы посмотрим на квадратичную форму, образованную вот этой матрицей, то она будет ровно такой же за вычетом того, что она будет практически точно такой же.
Единственная разница будет в том, что перед всеми квадратами будут стоять минусы.
А вот здесь минус поменяется на плюс.
То есть увидим, что функция стала принимать только отрицательно неположительные значения.
Ну, значит, сверху оценка вот такая.
Таким образом, мы можем определить L гладкую функцию с помощью задания этой матрицы.
И задав некоторую константу L, мы получим, что это L на 8, вот это X транспонирует на X минус L на 4.
И вот здесь вот единичный ОРД будет принимать участие, который нам сильно поможет в дальнейшем.
Значит, градиент можем посчитать. Матрица симметрична, поэтому градиент это L на 4, а X минус L на 4 ОРД единичный.
И поскольку мы хотим найти ее минимум, чтобы получить вот эту величину, то нам нужно найти X азвездочка такое что, а X азвездочка равно E1.
Рекурсивно можно расписать, что для такой матрицы решения системы с правой частью, где единица стоит только в первом индексе, а все остальные нули,
записывается как вот такое вот выражение. Вот это буквально надо там пару строчек написать, чтобы понять, что на самом деле,
идя снизу вверх, из последней строчки вы выражаете Xn-1, ну и в дальнейшем Xn-2, Xn-3 и так далее.
И в конце образуется, что вот, собственно, 2X1-X2 равно единице. Вот, собственно, X1 и X2 выражается через Xn.
Вот получается, чему равно Xn, и потом вы снова поднимаетесь и получаете вот такую вот рекурсию.
Вот это, ну можно считать довольно несложным упражнением. Не хочу сейчас это расписывать, может быть слишком долго будет.
Так, понятно, да, что здесь происходит? И зачем нам это надо?
Так, понятно, окей. Едем дальше.
Тогда мы можем просто подставить это самое значение в нашу функцию, получить, что f со звездочкой равняется минус L на 8, вот на эту штуку.
Вот, это, то есть тут просто арифметики. Я надеюсь, что тут у своего проблемы не должно возникнуть понимание того, откуда что училось.
Вот это, это линейный член, вот этот квадратичный. Раскрыли скобки, сократили, получили.
Короче говоря, здесь простые, простые выпутки.
Вот теперь пусть у нас X0 равно нулю.
Вот, то есть начинаем мы с нулевого вектора. Если начнем мы с нулевого, то там будет всего лишь движка, как сдвижка на этот самый вектор.
Ничего особо практикально не поменяется.
Вот, ну то есть вот здесь вот в семействе методов можно считать, что X0 это 0, потому что все равно здесь сдвигаемся.
Вот, тогда у нас X1 будет равен альфа на E1.
Вот, а X2 будет равен вот такому выражению.
Теперь если мы внимательно посмотрим на то, что это за вектор X1, X2, то станет понятно, что в векторе X1 только одна компонента, не 0, это первая.
Векторе X2 уже две компоненты, не 0, потому что вот у этого вектора два первых.
Собственно, когда вы будете, ну да, два первых элемента не 0.
И в силу структуры матрица, дальнейшее умножение на X2, X3 и так далее будет просто давать плюс одну не нулевую компоненту, начиная с первой.
Вот, поэтому после K-итерации такого метода у нас все элементы с индексами больше K будут нулями.
То есть после первой итерации у нас все индексы начинают со второго нуля.
После второй итерации у нас все индексы начинают с третьего нуля и так далее.
Вот, вот такой вот получается вектор, который потихонечку накапливает не нулевые значения.
Теперь, если мы рассмотрим задачу, что мы хотим найти арг-миниум нашей функции при условии, что все х после K-той позиции нули,
то это, по сути дела, означает, что мы просто взяли и сузили нашу задачу до размерности K.
Изначально она у нас размерности 2K плюс 1.
А мы берем только K на K, вот эту под матрицу первую, из первых строк и столбцов.
Поэтому по аналогии можно получить решение для вот этой задачи.
И это будет вот такое вот, ну то есть все то же самое, только здесь было N у нас вот здесь вот.
Тут станет K. Все остальное нули в силу того, что мы тут написали.
Теперь, собственно, самое главное. Подставляем это решение, получаем значение в этой точке.
Соответственно, справедливо следующая оценка, что f от xK минус f от x со звездочкой.
Это точно больше, чем f от xK со звездочкой, то есть оптимальное значение такое, что K компонент решений не нули первые.
Вот, минус f от x со звездочкой. А вот это все мы посчитали уже.
И если это все дело подставить, то мы получим вот ровно вот это выражение.
Но сейчас у нас тут фигурирует K, но не фигурирует нигде норма разности между x0 и x со звездочкой.
Давайте сейчас это добавим.
Понятно, поскольку у нас x0 это 0, то норма, квадрат нормы x со звездочкой есть не что иное, как сумма квадратов координат.
Вспоминаем классические формулы. Тут расписываем, тут у нас архиметическая прогрессия, тут сумма квадратов.
И одна сумма и другая сумма имеет аналитическое выражение.
Вот они тут приведены. Я надеюсь, что все понимают, как это выводится.
Достаточно классическая техника.
Вот тогда, подставляя вот эти выражения вот сюда, мы получаем, что наша норма x со звездочкой ограничена сверху выражением вида 2K плюс 1 на 3.
То есть это значит, что K плюс 1 больше, чем 3 вторых норма, квадрат нормы разности x со звездочкой x0.
Теперь все это дело вместе, если соединим. Здесь мы умножим на K плюс 1.
И K плюс 1 у нас сверху ограничено. Поэтому в итоге получаем вот такое замечательное выражение, которое и совпадает с нашей нижней оценкой.
Вот такая вот буквально 3-4 слайда на то, чтобы получить то, что для метода, который линьирует следующую точку, как некоторые начальные точки,
плюс линейная комбинация градиентов, выполняется вот такое вот выражение.
Теперь что делать с сильным опуклым случаем?
Для сильного опуклого случая функция, на которой достигается, для которой выполнена тоже нижняя оценка, она примерно такая же.
Только тут некоторая нормировка происходит на константу липшицы, на числобусловленности.
И константа сильной опуклости мют добавляется поправочками в полам на квадраты второй нормы.
И для нее можно проделать все то же самое и получит оценку через корень из числобусловленности.
То есть вот то, что у нас есть. У нас есть оценка снизу на выпуклую функцию с лучшим градиентом.
Есть оценка снизу на сильную опуклую функцию с лучшим градиентом. Вот это вот корень из капы здесь ниспялен.
В то же время сходимость градиентного спуска всего лишь вот такая.
То есть у него сходится разность между значение функции в точке cap plus 1 и оптимального значения функции меньше, чем cap plus 4.
1 делить на cap plus 4. А здесь сходимость линейная, но коэффициент линейной сходимости cap minus 1 на cap plus 1.
А здесь корень.
Этот зазор, который тут есть, вроде как быстрее, чем вот так нельзя, но градиентный спуск сходится всего лишь вот так.
Значит, наверное, можно предъявить какой-то метод, который будет сходиться в точности вот так.
Сейчас секунду.
Так, собственно отсюда мы и берем мотивацию и некоторую интуицию того, откуда берется, собственно,
ускоренный градиентный метод, который, собственно, сейчас везде по большей части в больших задачах используется.
Значит, для того чтобы подвести, откуда это все берется, рассмотрим сначала простую задачу,
а именно квадратичная целевая функция, симметричную строго положить определенной матрицей,
и для нее построим ответственный градиент.
Из необходимых условий следует напрямую, что ax равно b, и нам надо всего лишь решить линейную систему.
Дальше нам будет нужно обозначение, что градиент будет равен нас просто невязким.
И мы, по сути дела, свели задачу оптимизации к задаче решения систем линейных уровней.
Вот, это понятная, хорошо изученная задача.
И сейчас посмотрим, каким образом метод скруженного градиента будет соотноситься с тем, что мы хотим получить, а именно с вот этим.
Так, давайте историю я, наверное, пропущу, чтобы время сэкономить немножко.
В случае, когда мы пытаемся решить задачу квадратичную и получить решение градиентным методом каким-то,
то мы понимаем, что, в принципе, метод может сходиться безумно долго, если условия большие.
Но поскольку у нас задача квадратичная, то это значит, что мы можем, грубо говоря, разложить решение по базису и за одну итерацию найти просто компоненту,
ну, скаляр, который состоит перед соответствующим базисным вектором.
Вот этот скаляр, то есть по большому счету, мы можем сойтись за энтерацией, в точной арифметике никаких проблем здесь не будет.
Это не работает в случае для градиентного спуска, потому что он сходится, может бесконечно долго сходиться, если условия большие.
И, собственно, хочется сделать как раз-таки такой метод, который для квадратичной задачи точно за энтерацией сойдется.
Потому что решение, по сути, разложение по базису системы не уравнивает.
Понятно ли вот эта мотивация? То есть, какую проблему хочется решить, которая бы наличествует в градиентном спуске в случае квадратичной задачи?
То есть перейти от сходимости, которая может быть сколько угодно долгой, до, ну, там понятно будет какая-то линейная, но тем не менее,
в принципе, при плохом, при достаточно вытянутых линейных уровнях, это может быть сильно больше, чем размерность пространства.
Что не есть хорошо и выглядит как некоторые неестественные ограничения. Ясно, отлично. Едем дальше.
Значит, как его делать? Для этого нам потребуется определение сопряженных относительно матрицы положительно определенных векторов,
которые, собственно, их сопряженность означает, что они попарно, в случае различных индексов, зануляют соответствующий квадратичный упор.
Если и не равно же, то это ноль. Ну, свойства, понятно, линейно-независимые.
И если мы возьмем сопряженное направление шаг по наискорейшему спуску, то получим изначально тратство, которое будет сходиться.
То есть мы как бы будем строить байлис относительно скалярного произведения порожденного матрицы A.
Вот такая вот замечательная история. Соответственно, шаг будет определяться таким вот образом.
Если мы вычтем из обеих частей по... Ну, в общем, переведем это все на язык невязок, то получим ровно вот это.
Тут у нас будет стоять AX минус B. Получаем RK плюс 1 равно RK плюс альфа КПК.
Ну, собственно, вопрос, как получать сопряженное направление, он довольно кажется очевидным в плане своей постановки.
То есть понятно, что мы можем сгенерировать случайные вектора, но как сгенерировать сопряженные направления, чтобы было вот так?
Есть у нас какие-то идеи, что в этом случае делать можно?
Если мы знаем, что сопряженность это по сути дела артегональность относительно матрицы системы,
которую положить на пределе нас строгом.
На самом деле делать можно разное. Например, можно взять просто байлис собственных векторов для матрицы, и для него это будет работать.
Можно запустить артегонализацию грамма Шмита только с скалярным произведением, которое этой матрице порождено, и все будет работать тоже.
То есть путь не один, можно по-разному делать.
Теория на сходимости, что если последовательно XK генерируется методом сопряженных направлений, то у нас есть артегональность направления к невязке,
и XK есть не что иное, как армин функции на пространстве, которое совпадает с пространством X0 плюс линейной комбинацией всех наших направлений.
Докажем сначала, что эти два условия эквиваленты.
Если мы рассмотрим функцию phi-gamma, которая есть не что иное, как значение функции в точке, которая просто вот это.
Рассмотрим и параметризуем эту задачу через вектор гамма, который есть вектор-компонент, с которыми эти вектора образуют X.
Тогда она будет строго выпукла, потому что у нас функция положить на определенные матрицы.
Следовательно, существует гамма-созвездочка, в которой достигается решение.
Берем производную критерию первого порядка.
Производная от гаммы есть не что иное, как гамма – это у нас вектор.
Поэтому градиент берем по гаммам, получаем, что каждый раз,
поскольку это у нас градиент, это его ИТ-компонента, эта ИТ-компонента равна вот такому скалярному произведению.
И она должна равна.
Но вот эта штука есть не что иное, как невязка.
Поэтому это в точности соответствует тому, что невязка у нас артегональна направлению.
Поэтому эти два условия эквивалентны.
Теперь по индукции докажем первое.
База индукции у нас просто ноль по построению.
У нас R1 – это направление, которое…
Это AX1-B.
Если мы поставим в качестве, что такое у нас X1, то в силу того, как определялся шаг, мы получим тут ноль.
Шаг определяется по правилам низкорейшего спуска, поэтому это будет работать.
Гиппози, за что для Rk-1 и всех по ИТ-ых соответствующие скалярные произведения также зановляются.
Давайте распишем, что такое Rk.
Rk – это просто Rk-1 плюс вот это выражение.
Смотрим скалярные произведения Pk-1, фиксированные по направлению, и Rk.
Вставляем.
И опять получаем ноль, потому что у нас Rk-1 так строится.
В случае скорейшего спуска для квадратичной формы это просто ноль.
Теперь посмотрим, что происходит для всех предыдущих по ИТ-ых.
От 1 до k-2 теперь.
Отставляем снова.
И у нас что образуется?
Вот эта штука сидит в точности вот здесь.
А вот эта штука есть не что иное, как ноль, потому что у нас направление острижено.
И никогда не равно, всегда меньше k-1.
Поскольку направления являются спряженными, то эта штука равна ноль.
Все ли понятно в выводе?
Еще раз.
Мы показали, что нет спряженных направлений генерирует нам последовательность,
которая на каждой итерации, во-первых, минимизирует функцию на таком пространстве,
во-вторых, дает артагональность, невязки и соответствующих направлений.
Есть ли какие-то вопросы по этой части?
Нет вопросов.
Хорошо.
Давайте дальше пойдем.
Это мы показали.
Теперь стопстандхуд берут спряженные градиенты.
И что это такое?
Мы будем генерировать спряженные направления хитрым образом.
Ранее мы обсуждали, как их можно генерировать.
Первое – это либо байсовство векторов, либо артагонализация специальная.
Но и то, и то довольно затратно, потому что требует артагонализации
ко всему предыдущему набору векторов, что долго и дорого.
Метод спряженных градиентов позволяет генерировать
спряженные направления только используя предыдущие направления.
Это такая хитрая процедура, которая для устроения полного набора
спряженных направлений не требует знания в текущий момент
всех предыдущих направлений, только одного предыдущего.
Это существенно экономит и память, и время.
Как это работает?
Сначала задаем направление по 0 равный минусу 0.
Следующий направлений генерируется как предыдущий направлений
с некоторым коэффициентом, и плюс, точнее, минус градиент.
РК плюс 1.
Бета К плюс 1 выбирается так, чтобы направления были спряжены.
Удивительно образом, но то, что ПК спряжено ПК плюс 1,
автоматически будет соответствовать тому, что ПК плюс 1
будет спряжено и всему остальному набору направлений.
Легко показать, как бы Бета К плюс 1.
Просто условие спряженности, вот оно.
Вот оно, вставляем, получаем 0.
Отсюда Бета К плюс 1 равно замечательному выражению вот такому отношению,
которое есть не что иное, как значение затратичной формы для разных векторов.
То есть тут по сути два раза на одном матрицу умножить
и посчитать скалярное произведение.
Вседокод довольно прозрачный.
Считаем невязку. Вначале П, это минус R.
Пока у нас торм больше эпсил, мы считаем альфа по правилам нескорейшего спуска.
Потом у нас Х обновляется в соответствии с этим самым направлением.
Считаем невязку, считаем Бета по формуле вот этой вот.
И обновляем П.
Вот, собственно, буквально пять строчек получили мета спряженно-градиеновый для квадратичной формы.
Можно его немножко ускорить, если переписать правила вычисления альфа через норму.
И через... Да, тут знаменательный поменялся.
То есть альфа и бета можно переписать так, чтобы было...
Тут для бета было, то есть не было умножения матрицы на эктора,
было просто вычисление норм для двух последовательных невязок.
Тут следствие напрямую того, что у нас направление ортогональное,
и невязки ортогональные, направления спряжены, а невязки ортогональные направления.
То есть это просто пересчет формул, более быстрая версия, которая состоит в реализации.
Теперь, почему спряженный градиент оказывается спряженными направлениями?
Оказывается, что вот эту теорему я доказывать сейчас не буду,
но просто она довольно... Четыре пункта, и они все равносильны.
Это довольно длительная процедура с доказательством этого факта.
Но, значит, идея в том, что после коэтерации, если мы не сошлись,
то у нас ортогональные наши невязки будут,
и пространство, которое натянута на невязки и на направления,
есть не что иное, как пространство, которое натянута на вот такие вот векторы.
То есть одно и то же пространство.
И более того, эти самые направления оказываются действительно спряженными относительно этой матрицы.
То есть на самом деле все сводится к тому, что вот это за пространство такое.
И это пространство имеет специальное название.
Это Крыловское пространство порядка K.
И оно... То есть это определение.
Это пространство натянута на вот такие векторы.
Пространство Крылова.
Вот. Да.
Значит, основное свойство, которое...
Ну, может быть, оно плюс-минус очевидно, тем не менее.
То, что решение нашей системы лежит в пространстве Крылова этого порядка.
Это несложно показать.
У нас есть теория Магамильтона-Келли о том,
что матрица является корнем своего характеристического многочлена.
То есть если вы в качестве П возьмете вот этот самый дотерминат,
в который характеристическое уравнение вставляете вместо лямбда А,
получаете полинома от матрицы, и он занулился.
Поэтому когда мы полинома от матрицы умножаем на B,
мы получаем вот такую штуку.
Да, и она равна нулю, понятной причины.
Теперь умножим слева и справа на минус первый.
У нас тут степень понизится,
и останется последнее слагаемое вида AN на A-1B.
Ну и, собственно, это A-1B мы тут благополучно можем выразить.
Внимание, вопрос. Почему мы можем поделить на AN?
Есть ли у кого-то понимание?
Надо немножко вспомнить теорию многочленов.
Как у них коэффициенты разложения связаны с корнями, например?
Ну, коэффициенты, да, коэффициенты,
ну не разложения только, а вот коэффициенты записи.
А как связано A-1AN с корнем?
С корнями многочлена, чтобы ответить на вот этот вопрос.
Ну, нам еще один переход будет делать, но он посмелся очевиден.
Есть ли понимание и помнит ли кто-то о том, как это работает?
Почти, только, по-моему, след, это как раз-таки A-1, по-моему, след.
А вот здесь это там произведение,
произведение вроде как этих собственных значений.
То есть последний врунутость, ремьет, короче, говорят, что у вас там C это произведение корней,
нам делены, понятно, на правильную вещь.
А второй там сумма.
В случае больших размерностей у вас вроде последние множители произведения,
поэтому если тут будет хотя бы один ноль,
то матрица перестает быть положительно полуопределенной,
и становится положительно определенной строго,
становится выраженной, и все грустно.
Поэтому очень важно делить.
Ну и смотрим просто на выражение и понимаем, что на самом деле тут в точности записано вот это.
То есть действительно тут есть константный вектор,
ну есть вектор B, ну и так далее до A n-1.
То есть тут вроде все хорошо, но понятно, что доводить это все до n нам не хочется,
хочется чуть-чуть сократить вычисление.
То есть на самом деле мы пытаемся найти наилучшее приближение
на пространство крылого катового порядка, что ли.
При этом направления, которые мы используем, они не равны тем направлениям, которые генерируют это пространство.
Как вы думаете, почему?
Почему нельзя использовать вот эти векторы, например,
для того чтобы в медленность пространных направлений просто по ним идти?
Что не так может случиться, что может сломаться вот с этими вот направлениями, как вы думаете?
Вот на самом деле несложно увидеть, что если вот эта штука вы будете ее генерировать,
то векторы, которые получаются умножением одного и того же векторного матрица много раз,
они получаются все более и более коллинеарными.
И в конце концов они, кстати, если их еще немножко нормировать,
будут сходиться к собственному вектору, соответствующему максимальному по модулю собственному значению.
Вот такой замечательный факт.
Поэтому, когда мы будем пытаться строить на их основе решения,
то у нас будут коэффициенты очень большими, потому что они будут...
Короче, мы будем пытаться разложить решения по байсу из почти коллинеарных векторов.
Это не очень хорошая идея с точки зрения вычислений.
Поэтому направления делаются по-другому.
А именно вот этот байс, он в процессе итерации, он артагонализуется.
То есть мы генерируем наше крыловское пространство с помощью артагонального базисы.
Вот это.
И по сути, весь мед. сп. градиентов мы ищем решение в артагональном, в артонормированном крыловском базисе.
Теперь посмотрим, как он будет сходиться.
Понятен ли посыл вот здесь? Это важное место для понимания того,
как мед. сп. градиентов классически работает.
Что на самом деле мы просто пытаемся найти решение линии системы,
но это самое решение мы ищем в специальном пространстве крыловского.
Оно хорошо тем, что для его артагонализации нужно знать всего лишь...
Для получения следующего вектора надо знать всего лишь текущую точку и предыдущий вектор.
И все остальное будет получаться по явным формам.
Это, собственно, основная идея, которая сейчас будет активно использоваться
для получения утверждения сходимости этого меда.
Есть ли тут вопросы какие-то?
Пауза, чтобы немножко выдохнуть и осмыслить произошедшее.
Вроде в чате вопросов нет.
Ну хорошо, давайте тогда пойдем дальше.
Сейчас будет немного несколько таких не очень легких слайдов,
но я думаю, мы справимся.
Значит, смотрите, решение понятно как записывается.
При этом минимум функции, если мы просто это дело подставим,
то мы получим, что тут все сократится,
останется вот это выражение.
А это выражение, если что иное, как их созвелось в а-норме.
При этом а-норма, это вот так.
Надо расширить другой экран.
Что такое а-норма?
Полезно знать.
А-норма, это такая норма, которая порождена матрицей А.
Соответственно, наше решение, которое мы можем записать в а-норме,
это что такое?
И к соответствии, у нас а-1 на b транспонированное,
а на а-1b.
Ну и мы в точности получим, что b транспонированное а-1b.
Поскольку матрица симметрична, то транспонированный тоже будет симметричный.
И там справедливо, что так, что так.
В общем одно и то же.
Распонирование.
Поэтому иногда это еще записывать вот таким вот знаком,
типа минус транспонирование.
Так.
Распонирование.
Так.
Возвращаемся к слайду.
Вот ровно оно тут и получилось.
Далее.
По функции сходимость, естественно, будет такой.
Мы из f от x вычитаем их с созвездочкой.
А дальше понимаем, что вот здесь у нас стоит а-норма x.
Вот здесь стоит на самом деле x транспонированный на ах-созвездочке,
потому что это просто b.
Ах-созвездочка это b.
Следовательно, это все можно как полный квадрат выделить
и получить, что сходимость по функции это то же самое,
что и сходимость по аргументу, только в анорм.
Вот.
Вот такая вот связь между этими двумя типами сходимости существует.
Соответственно, нам далее достаточно определить,
что будет происходить вот с этим вот выражением в процессе итерирования.
Вот.
И это же нам сразу же даст сходимость по функции.
Вот.
Ну и теперь, собственно, давайте сделаем там 5 минут перерыв
на то, чтобы, перед тем как приступить к анализу сходимости метод
сформированных градиентов, он здесь не очень простым будет.
Все.
5 минут перерыв продолжаем в 11.36.
Так.
Ну, давай продолжим.
Собственно, аналии сходимости сформированных градиентов,
не очень простая штука, но, в принципе, довольно понятно,
если немножко подумать.
Значит, поскольку у нас экската лежит в каосском пространстве,
то мы можем его представить в виде вот такой вот линейной комбинации векторов,
которые в процессе итерировали.
Вот.
Это есть не что иное, как матричный полином, умноженный на вектор.
Вот.
При этом, ну, то есть это некоторый полином в степени не вышедший,
при этом, ну, то есть это некоторый полином в степени не выше k-1,
потому что у нас, понятно, есть ограничение на размер,
ну, на количество итераций.
При этом мы знаем, что xk минимизирует нашу функцию в каосском пространстве,
то есть, учитывая вот это вот соотношение о том,
что 2 на fx-f1 равняется второй норме, а-норме.
Это значит, что вот происходит вот это вот, выполнена вот эта равенство.
То есть мы минимизируем нашу а-норму на Крыловском пространстве.
Это значит, что мы ищем такой полином в степени не выше k.
Ну, тут вот не выше k-1, вот.
Ну, понятно, что степень меньше k, то есть k-1 меньше.
Вот.
Вот, вот такого вот выражения.
То есть мы свели задачу поиска x,
задачу поиска полинома, которым будем действовать на правую часть.
Вот.
Ну, дальше немножко математики.
Что называется?
Поскольку у нас матрица симметричная, положительно строго определенная,
то, да, тут вот на самом деле транспонирование,
а не звездочку, потому что у нас действительно случай.
Ну, неважно.
Короче говоря, тут транспонирование, потому что это ортогональная матрица.
Вот, никаких комплексных чисел у нас тут нет,
поэтому избыточная степень общности, скажем так.
Вот.
Получаем, что вот эта штука есть не что иное, как...
На самом деле, если расписать, что такое а норма,
и а у нас это вот такое вот имеет разложение,
то это будет полинома диагональной матрицы,
минус обратной к диагональной,
и еще и норма становится диагональной.
Вот.
То есть полная диагонализация вот этого вот выражения.
Вот.
Если его расписать, то окажется, что это на самом деле...
Тут еще квадрат стоит вот у этой нормы.
Поэтому это будет не что иное, как сумма.
Будет сумма отношений, где вот это деление на лямбдаиты
вот происходит ровно потому, что здесь один делить на лямбдаиты
элементы обратной матрицы диагональной.
Вот.
Тепень полинома по-прежнему сохранился.
Теперь, если немножко...
Ну, понятно, вот этот новый некоторый полином
в степени уже может потенциально равный к,
потому что мы тут на лямбда еще домножили.
Вот.
И в нуле эта штука единичка.
Вот мы получили.
Новое наше выражение.
Вот.
Для некоторого полинома.
Вот.
Понятно ли, может быть, не совсем подробно
детальный переход отсюда сюда,
но вот если по модуле этого перехода,
понятно ли все, что происходило далее?
Или есть какие-то вопросы?
Что откуда взялось?
Слойка следующая.
Мы ищем наш х в крыловском пространстве.
В крыловском пространстве выражается через
минимум 100 х.
Это полинома от матрицы.
Вот.
И поиск ха сведен к поиску полинома.
Вот.
Дальше происходит некая манипуляция для того,
чтобы упростить выражение, которое нам надо минимизировать.
Вот.
И уточнить требования на полином.
Вот все, что здесь происходит.
В чате по выражению нет вопросов.
Ладно.
Тогда давайте дальше.
Надеюсь, все понятно.
Теперь можно это дело оценить.
Каким образом?
Вот здесь у нас есть что?
Есть значение q от лямбитого.
Что здесь зависит от полинома?
Давайте поймем.
По полиному здесь зависит только q.
А вот выражение dt на лямбдт от q не зависит.
Вот.
Поэтому давайте заменим.
Вынесем эту сумму за скобки.
А q от лямбдитого, сумма pi,
заменим просто на максимум по всем лямбдитам.
Таким образом, наша неравенство...
Ну, мы получим неравенство,
где некоторая константа умножается на инфиво.
Вот.
Вот эта штука, если внимательно ее расписать,
есть не что иное, как
квадрата нормы для х со звездочкой.
Вот.
А здесь у нас этот самый инфимум остался.
Теперь.
То есть что это значит?
Если мы...
Нам надо найти инфимум
по всем полиномам.
Вот так, чтобы...
А, инфимум по всем полиномам от максимума q от лямбды.
От лямбдитов в квадрате.
Теперь.
Пусть у нас есть у матрицы А m различные собственные значения.
Рассмотрим вот такой полином.
То есть ионно зависит от y.
Здесь стоит произведение y минус лямбда и t.
Делить на произведение всех лямбд.
И умножим еще на минус единичку в степени m.
Вот.
Что мы при нем можем сказать?
Во-первых, в нуле он единица.
Собственно, вот эта вот минус единичка в степени m
это поправка на знак вот этого выражения в нуле.
И степень равна m...
И степень равна m, потому что их всего m разных штук.
И вот здесь вот фигурирует y в степени m.
Вот.
Что же мы при нем можем сказать?
Что, поскольку здесь мы берем инфимум по всем полиномам,
но мы предъявили конкретный полином,
который удовлетворит всем нашим ограничениям.
Поэтому если мы подставим его сюда,
то мы получим еще просто оценку сверху.
В итоге, наша оценка на f, она больше ли оранули,
по понятным причинам, просто по определению.
И сверху ограничено 1 и 2 на норму,
а-норму х звездочек в квадрате,
на максимум по r от λi.
Но λi это спектр,
а в каждом из λi наши значения равны 0.
Поэтому здесь мы тоже получаем 0.
В итоге, что выяснилось?
Выяснилось, что если у матрицы A есть m различных собственных значений,
то наше отличие между fk и fm,
тут на самом деле не k,
надо написать ам,
степень m,
тут степень k,
поэтому тут fm.
Минус f со звездочкой в точности совпадает.
То есть мы сошли за m-итерацией.
Таким образом, какой бы огромной ни была наша задача,
сколько бы миллионов там не было переменных,
если спектр матрицы такой,
что там всего лишь m различных собственных значений,
то мы с помощью метода сопряженных градиентов
сойдемся ровно за m-итерацией в точной ритуции.
Понятен ли этот вывод?
Он тут довольно существенный.
Окей, идем дальше.
Значит да, за м-итерацией нашлись.
Например, n равно 100,
и спектр сойдет из четырех значений.
1, 10, 100 тысяч.
Число обусловленности, обращая ваше внимание, равно 1000.
Это довольно много.
Ну и собственно спектр вот так, естественно.
Значит как будет сходиться метод сопряженных градиентов и градиентный спуск?
Ну вот метод сопряженных градиентов, эта синяя линия,
сходит за 4 итерации.
По норме градиента к 10-8 и ниже.
А метод градиентного спуска сходится медленно, медленно,
застой ты рад с значением минус 6 по функции.
И, ну, про норму градиента я вообще молчу.
Тут даже несколько единиц, я так понимаю, значений.
То есть даже меньше единиц он не стал.
И это все в случае использования правил на искрежа спуска.
То есть это лучшее, что можно взять.
То есть шаг подбирается наилучшим образом локально,
как это может быть в методе градиентного спуска.
Вот разительная, существенная разница в поведении этих двух методов.
Вот, так, тут номер итерации.
Да, я надеюсь все графики легенды довольно хорошо видны и понятно.
Вот, значит что дальше?
Дальше можно, если брать другие полиномы,
типа Чебушовского, то можно получить, собственно,
наши оценки через корень искапы.
Вот, который и...
К чему все это, собственно, было рассказано?
К тому, что в случае, если у нас квадратичная целевая функция,
то медленно-градиентом дает оценку линейной сходимости,
ровно такую, которая соответствует оценке снизу.
Вот, но это только для квадратичной целевой функции пока что.
Вот корень искапы, тут вот, да, принципиально
неулучшаемая оценка для методов, которые генерируют решение
как в пространстве натянутым на градиент.
Значит, вот, это важный момент здесь.
Теперь, значит, понятно, что можно общить этот метод
на некую дратичную функцию.
Для этого вместо скорейшего спуска по альфу надо подбирать его
просто адаптивно на основе тех правил, которые мы ранее обсуждали,
там типа правила армии и так далее, убывания, вот эта вся история.
Коэффициент мы ищем с помощью градиентов
на основе либо методов Летчер-Ривса отношения,
либо Полак-Рибьер, либо Хестен-Счтифель,
вот такие вот различные выражения для бета подбираются.
Вот, а невязка, все остальное остается прежним.
То есть невязка заменяется на градиент,
бета считается через градиенты, альф подбирается адаптивно.
Все, получили зоопарк методов спяженных градиентов
для неквадратичной целевой функции.
Откуда это все берется, я как бы в детали вдаваться сейчас не буду,
вот это берется просто из того, что мы когда записывали
наше решение для квадратичной целевой функции,
то у нас был вид вот такой вот.
Но это что такое? Это просто градиенты.
Градиенты на ка плюс первые итерации на ка.
Все, вот как бы заменили просто по аналогии,
по формулы и результаты для которых потом дополнительно выводится.
Вот это берется из, наверное, из каких-то других соображений.
Но в общем сейчас без подробностей, просто что вот есть такие
три разных метода, они реализованы в большинстве пакетов,
их можно спокойно вызвать и использовать.
Проблема неквадратичного случая в том, что с ростом
числа итерации направления могут становиться уже коллинеарными.
То есть в квадратичном случае у нас что было?
У нас была там артагональность, все хорошо.
Вот как только мы переходим к неквадратичному случаю,
у нас все эти свойства благополучно пропадают.
И все может пойти не так, как хотелось бы.
Поэтому надо делать restart.
Мы интервируемся.
Поняли, что наши направления стали артагональными.
Забываем всю историю и начинаем двигаться с нуля.
Это помогает немного как бы освежить процесс
и позволит ему не тащить за собой хвост
слишком старых итераций, которые уже совсем не релевантны
в данной локальной точке.
Это к тому, если возвращаться к обсуждению общей схемы работы методов,
что какая общая информация задачи у нас есть.
Вот здесь у нас информация задачи, мы ее генерируем
это направление градиентов и сопряженные направления,
которые используют знание градиента с предыдущей точки.
И актуализация этой информации
выражается в том, что
мы не можем делать restart.
Это довольно перспективная техника,
которая много где может вам встретиться.
Получается направление убывания.
Не всегда при адаптивном поиске у нас будет направление убывания.
Там есть некоторые специальные условия.
Это вы можете довольно легко получить,
что должно выполняться для направления убывания
для P и для Альфа,
чтобы их произведение было направлением убывания,
и вы не ушли слишком далеко
от текущей точки,
чтобы аппроксимация вашей функции
все еще работала.
Это будет через две недели.
Возможно, через две недели, наверное,
или на следующей неделе.
Тоже довольно интересно.
Еще раз вспомним про этот метод,
когда будем клоизинцовский метод обсуждать.
У них есть интересная интерпретация тоже через спрешенный градиент.
Это все, что я хотел сказать про метод спрешенного градиента.
У нас осталось 20 минут ровно на два метода,
которые самые интересные, общие,
которые будут отчасти закрывать вопрос
об оптимальности градиентных методов,
которые лежат в том классе,
который я несколько раз показывал.
То есть когда у нас х плюс 1, это х0 плюс
линейное пространство, натянутое на градиенты во всех предыдущих точках.
Итак, метод тяжелого шарика.
Борис Тодорович Полик, 1964 год.
Идея в следующем.
Давайте мы возьмем произвольную функцию здесь,
и будем подправлять наш шаг по антиградиенту
вот такой поправочкой.
Будем учитывать еще и направление,
по которому мы пришли в текущую точку.
Из х к минус 5.
Тогда, если у нас был
градиентный спуск вот такими вот осцилляциями,
то метод тяжелого шарика эти осцилляции сглаживает.
То есть он вместо того, чтобы брать
вот это направление,
берет их неотрицательную комбинацию,
и получает вот такой вот более сглаженный.
Это метод двухшаговый.
Он использует и х к, и х к минус 1.
Почему тяжелый шарик?
Потому что связь этого метода
с дифференциальными уравнениями в том,
что если вы дискретизуете дифференциальное уравнение с трением,
то у шарика появляется некоторая инерция у точки.
И учет этой инерции как раз приводит к тому,
что мы делаем.
Но запоставив просто формулу,
можно показать, что метод сопряженных градиентов
это просто частный случай этого метода для квадратичной телевой функции,
когда у бета есть специальное выражение,
и вот эта штука это просто предыдущее направление.
То есть тут все довольно близко и похоже.
Поскольку мы хотим получить некоторые оптимальные оценки,
то для этого метда есть вот такая
теория исходимости.
Тут некоторый вывод.
Теория о том, что если функция сильно выпукла,
тогда если взять шаги альфы вот такие,
а бета вот такие,
то мы получим оптимальную оценку скорости исходимости
для такого класса функций.
Проблема этой оценки в том,
что, во-первых, она зависит от констанции,
от таких зависимостей,
о которых я говорил ранее,
когда мы обсуждали,
что нам дает и что не дает теория исходимости.
Точно получаем, что это быстрее градиентного спуска,
и в каком-то смысле является аналогом метода
сопряженных градиентов для выпуклой квадратичной функции.
Сильно выпуклой квадратичной функции.
Вот так.
Что хорошего?
Хорошо, что есть оценка.
Хорошо, что она быстрее, чем градиентный спуск.
Что плохо?
Плохо, что у параметров есть зависимость от L и mu.
Теорияма не говорит о том, что будет,
если альфа и бета выбирать не по этим формам.
Но на практике бета обычно выбирает порядка единицы.
Я имею в виду, что где-то 0,7-0,9.
Это дает вполне себе хорошие результаты.
Альфу можно выбирать как единица L, например,
или как адаптивно тоже.
Это вполне себе рабочая схема.
Типичный пример работы для квадратичной задачи.
Спряженный градиент сходится очень быстро.
Тут есть некоторая не монотонность,
но это, видимо, задача не очень...
Получайная задача.
Спектр не очень понятен, как распределенный.
Возможно, с той итерацией как раз таки и произошло.
Градиентный спуск с фиксированным шагом сходится очень медленно.
Но тяжелый шарик с оптимальными параметрами,
поскольку их можно получить для квадратичной задачи,
он сходится вот так.
Хуже, конечно, чем спряженные градиенты,
потому что для них это все было придумано.
Но это лучше, чем метод градиентного спуску.
И понятно, что обобщаемость у него
будет более высокая,
чем у спряженных градиентов.
Тут хотя бы есть какие-то теории.
Есть какие-то вопросы про метод тяжелого шарика?
Нет вопросов. Прекрасно.
Тогда давайте пойдем дальше.
Вот этот шарик, который сегодня хочется охватить,
это, собственно, ускоренный градиентный метод
Юрия Ивановича Нейстера в 1983 год.
Один из основных и главных прорывов
в конце XX века.
Один из вариантов его постановки он вот такой вот.
У нас есть теперь две последовательности,
х и у.
При этом х делает по-прежнему шаг
градиентного спуска.
Но хитрый заключается в том, что в отличие от...
Тут я обычно об этом спрашиваю.
Нет, тут не спрашиваю.
Отличие от тяжелого шарика в том,
что тут мы для того, чтобы посчитать
Xк плюс первое, считаем градиент в Xкатом
и шагаем по антиградиенту относительно точки Xк.
А тут чтобы получить Xкат,
мы берем некоторую точку Yкат и другую,
не равную Xк.
И более того, градиент считаем тоже не в этой точке.
То есть здесь мы,
как можно сказать, что здесь мы тоже берем
Xк плюс бета, то есть тоже какая-то другая точка.
Но градиент считается только в Xк.
А здесь и точка другая относительно которой старт.
И градиент в другой точке считается.
И потом эта самая точка пересчитывается
через линии некоторую комбинацию
векторов Xк и Xк плюс 1 минус Xк.
Вот этот коэффициент,
он тут немного магия,
но есть более сложные формулы,
это просто самая простейшая, которая плюс минус работает.
И тут важно, что тройка.
Там есть работы, в которой доказывается,
что здесь тройки нельзя.
Это важно и связано с,
как бы это удивительно ни казалось,
дифференциальными уравнениями и устойчивостью их решений.
Если будут кому-то интересны,
я могу ссылки прислать в чат,
где про это подробно расписывается.
Он тоже не монотонный.
Не монотонность проявляется, ее тут плохо видно.
Вот здесь он немного акцилирует,
и норма градиента может возрастать.
То есть он накапливает некоторую историю,
чтобы потом начать убывать.
Визуализация этого меда такая вот.
У нас есть Xк и Yк.
Чтобы получить Xк плюс 1,
мы переходим из точки Y к Xк плюс 1.
Потом берем прямую,
которая соединяет точки Xк и Xк плюс 1.
Получаем точку Yк плюс 1.
Из нее шагаем по градиенту,
получаем точку Xк и так далее.
Вот такая схема работы,
которая приводит к замечательным результатам.
Сходимость по функции становится равна 1 на как квадрат
для обычной пуклы функции.
А для сильной пуклы функции она действительно становится равна
1 на корень Xк.
То, как это было показано и требовалось
нашими нижними оценками.
То есть вот этот метод в теории
оказывается очень выигрышным
по сравнению с градиентным спуском.
На практике есть некоторые проблемы.
Вот они тут уже нарисованы.
Это, насколько я понимаю, та же самая квадратичная задача.
Здесь у метода Нистерова
такие рифовые поведения.
То есть он опять же не монотонный.
Выбор параметров для него
это немного отдельная история.
Это не всегда просто и не всегда очевидно, какие нужно подбирать.
Какие тут параметры?
В этой схеме это шаг.
Понятно, что если вы в качестве вот этого коэффициента возьмете
то будет еще один параметр, который тоже надо настраивать.
Это не всегда легко.
Обычный шаг берут...
Типично я бы сказал, что лучше брать шаг поменьше.
Если взять шаг поменьше, то он будет сходить
точно быстрее, чем градиентный спуск таким же шагом.
За счет именно того, что история учитывается хитрым образом.
И левочисление следующей точки
используется не вспомогательно последовательно.
Для начальных итераций видно, насколько здесь существенное ускорение.
Даже по сравнению с методом
тяжелого шага.
Это метод, который
сходится к не самой высокой точности, но очень быстро.
Я бы так его описал вкратце.
В целом, градиентные методы
сделаны для того, чтобы сделать много тяжелых итераций
с сайтики точности 10-12, 10-13.
Поэтому можно начать стопориться,
потому что у них просто порядка точности не хватает.
Давайте некоторое общение проведем
того, что мы знаем.
Есть ли какие-то вопросы про метод ускоренного
как он работает, и что для него надо
использовать, и какие основные моменты здесь могут
случиться?
Вижу, что написали, что нет вопросов.
Окей.
Тогда небольшой обзор того, что мы только что изучили.
Что мы знаем? Мы знаем, что сходиме с градиентом с куском
можем улучшить. Мы знаем, что
метод с упруженными градиентами надо использовать, если у вас
целевая функция квадратично сильно выпукла.
Что ускоренный метод является оптимальным для выпуклых,
для всех тех двух классов функций, на которых мы
все это дело рассматривали.
Какие у нас остались вопросы?
Которые мы в следующий раз, наверное, или в следующий,
или в следующий раз будем пытаться решать.
Что делать, если градиент точно не посчитать?
Если функция настолько сложная, настолько многомерная,
что посчитать градиент нельзя, но можно посчитать его отца.
Это так называемые стахотические методы.
Мы про них будем говорить, наверное, в следующий раз.
Что делать?
Везде зависит от константа, который непонятно как подбирать.
Это тоже обсудим
в контексте того, что изменится
в наших результатах про ускорение
и его наличие для модификации градиентного спуска
в случае, когда у нас появляется стахастика.
Если у нас есть стахастика,
то насколько у нас скорость сходимости начнет портиться?
Мы же не можем быть уверенными в том, что
мы делаем шаг по неточному градиенту
и при этом у нас скорость сходимости
не ломается ни разу.
Это было бы, наверное, слишком странно, чтобы быть правдой.
Поэтому в следующий раз мы посмотрим
на то, как это все будет портиться
и насколько сильно.
Это анонс
на следующий раз.
Я буду там более подробно рассказать про всю эту стахастику.
Это важный кусочек,
который, наверное,
наиболее активно будет использоваться, мне кажется.
Резюме сегодняшней лекции.
Мецпежонных градиентов для квадратичной
целевой функции сильной выпуклы. Его сходимость.
Какова обобщается это все на неквадратичную задачу
и какие там возможные нюансы?
Что такое метод тяжелого шарика?
Какие там параметры? Как он сходится?
И ускоренный градиентный метод как некоторое завершение
обсуждения того, какие методы первого порядка
из класса методов, которые генерируют решение
как линейную комбинацию градиентов в начальной точке.
Куда это все дело пришло
и какие дальнейшие расширения в плане появления
стахастики, появление нелинейных
функций для генерации следующей точки.
Сейчас мы жили в мире,
где у нас... Так, надо нарисовать.
Где у нас
xCAD плюс 1 это x0 плюс
линейная комбинация градиентов.
Вот, а что будет,
если, например, здесь будет что-то нелинейное,
типа x0 плюс какая-то нелинейная функция чего-то там.
Вот, для этого класса все понятно,
для этого класса непонятно, и на практике оказывается,
что к этому классу относятся квазинью туннельские методы,
квазинью туннельские методы,
которые сходятся
очень хорошо.
Для задач, где вы можете
посчитать полный градиент точный,
ну, то есть это там десятки-сотни тысяч переменных,
эта штука работает очень хорошо.
И квазинью туннельские методы
и сопряженные градиенты для нелинейных задач,
не квадратичные, то есть простите,
это основные методы, которыми надо их решать
в случае, когда у вас большая размерность.
Потому что они хранят только векторы,
то есть никаких матриц ничего не надо.
И единственный вопрос в том,
как правильно эффективно посчитать градиент, как его хранить,
и в каком виде с ним обращаться.
Про это мы будем говорить про квазинью туннельские методы
и их связь с сопряженными градиентами, которые были анонсированы,
будет, наверное, через раз.
И в целом, насколько я понимаю,
сейчас все это дело, почему-нибудь, движется к концу.
Анонс на ближайшее время такой.
В следующий раз будет стокастика,
через раз будут квазинью туннельские методы,
6 декабря будет про максимальные методы
и про решение сдачи на простых множеству.
Будет ли что-то 13-го, не уверен, но посмотрим.
Посмотрим, во-первых, по тому,
сколько людей будет приходить отключаться.
Дальше я расскажу о темах, которые я уже анонсировал.
Потому что они являются основополагающими для всего остального.
Поэтому, я думаю, если у вас будет понимание что-то про них,
то дальше вам будет не так сложно
разобрать, что происходит.
Лейна программирования или еще чем-то,
что немножко не успевается.
12.08, две минуты на вопросы.
Все ли понятно, что будет происходить, какой у нас план,
и как все будет завершаться.
Очень хорошо, что все ясно.
Тогда давайте на этом сегодня закончим. Спасибо за внимание.
До следующего раза видео я выложу так же, как обычно,
в папочку на Google Диске.
Надеюсь, что, возможно, те, кто не смогли подключиться,
что-то там посмотрят и для себя полезно узнают.
Все, спасибо и до следующей недели.

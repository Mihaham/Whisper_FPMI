Ну чего, нам осталось добить в каком-то смысле результат, но там еще так порядком. Давайте я
вот здесь напомню лемму, которую мы доказываем, она называлась лемма в лемме. Тверждение было
такое. Математическое ожидание y-катого больше либо равняется асимпатически. Вечины
m в квадрате поделить на 2k в четвертой степени, а y-катый это размер самой большой цепочки,
ну так сказать, порёберно не пересекающихся, не пересекающихся, ка вершинных независимых множеств.
Ну вот, товарищи, если вы следите за лекциями, то вы понимаете, что такое y-катый. Ну ладно,
я всё равно выпишу, бог с ним, это пригодится. Y-катый, для восприятия просто доказательства,
это, наверное, всё-таки полезно. Максимальное, не помню, что я там какую букву писал, t что-ли,
t писал. Максимальное t, такое, что существует. Давайте я их сегодня буду не s обозначать,
а k. Так хочется, k большое, 1 и так далее. k большое t такие, что для любого i мощность
каитова равняется k маленькому, мощность каитова пересечённого скажи там, то что любых и и же,
конечно, не больше единицы. Можно вот здесь написать, для любых и и же, вот так, чтобы было
корректно. Мощность каита пересечённого скажит не больше, чем единица, и каитое, к большое,
итое независимое множество. Вершин в графе g. Вот так. Пусть это будет, это полезно. Вот,
теперь доказываем лему. Ну, граф у нас, вообще говоря, случайный, но давайте просто зафиксируем
какой-то граф, всё равно, когда он случайный, это просто значит, что мы ему вероятность какую-то
присвоили, а так-то он граф и граф. Так, пока фиксируем. Граф g, в нашем случае me вершинах,
я напоминаю, что у нас графы на me вершинах сейчас все. Фиксируем граф g на me вершинах. Пуф,
у него есть какие-то независимые множества. Независимые множества вершин, мощность ка.
Они же нас интересуют, вообще говоря, нам потом усреднять по всем графам, но пока зафиксировали
граф. Давайте как-нибудь их тоже обзовем. Ну, как мы их обзовем? Да, тоже к большое. К большое 1,
к большое 2. А сколько их всего? Ну, у нас есть обозначение для этого, и даже не одно,
а целых два уже. Сегодня будет третье. Вот, их x с индексом k от g. Вот так. Ну, конечно,
очень громоздкая запись, но важно, чтобы вы просто вспоминали. x с индексом k маленькая от g,
это k маленькая. Это количество независимых множеств мощности k в графе g. Вот в графе g есть
какие-то независимые их множества, их, конечно, столько. Вот мы их как-то перечислили в определенном
порядке. Все. Ну, просто ввел обозначение, так? Я обещал в прошлый раз, что в доказательстве
этой замечательной леммы будет присутствовать дополнительная рандомизация. То есть мы еще
дополнительно почему-то усредним, чтобы получить итоговую оценку. Но нас-то интересует, понимаете,
не сами независимое множество, а гирлянды вот эти, которые здесь выписаны, гирлянды из не сильно
пересекающихся независимых множеств. Поэтому мы сделаем такую дополнительную рандомизацию,
которая позволит двойной счет или усреднение какое-то организовать. Мы возьмем пока непонятно,
какую чиселку, назовем ее Q со звездочкой. Ну, чтобы она не путалась с буквой P, которая воспринимается
как вероятность ребра случайного графа, у нас-то, конечно, P равно 1 и 2. Я бы мог здесь просто P
написать, но может возникнуть путаница. Если я напишу Q, вы подумаете, что это 1 минус P. Поэтому
я пишу Q со звездочкой. Но это будет тоже некая вероятность. А именно это число должно быть,
конечно, из отрезка 0 и 1, как и всякая вероятность. И мы это будем интерпретировать как вероятность того,
что мы выберем k и t и от единицы до х маленькое от g. То есть мы зафиксировали пока что граф,
перечислили все его независимые множества. А теперь как бы над каждым из этих независимых множеств
подбрасываем монетку, у которой вот такая вот вероятность отвечает за одну из сторон. И вот
если одна из сторон, которая отвечает, вот эта вероятность выпадает, то мы выбираем соответствующая
k и t. Подбросили первый раз монетку, реализовалась нам условная решка с вероятностью Q со звездочкой,
выбираем k1. Подбросили еще раз монетку, с той же самой вероятностью выбираем k2. Или не выбираем,
с противоположной вероятностью. Мы запускаем схему испытаний Бернули, в которой вот столько
испытаний, а вероятность успеха вот такая. Можно так? Прореживаем в некотором смысле вот эти
независимые множества. Вот третье обозначение сейчас будет. Давайте сейчас совсем временно до
конца доказательства этой леммы через mu обозначим как раз xkt от g. Не, не так, не xkt,
а mat ожидания xkt. Mu это mat ожидания xkt. Ну, то есть это, конечно, то, что мы раньше обозначали fkt от m.
Вот так мы это обозначали на прошлой лекции. Mat ожидания xkt это c из m по k на 1 вторая в степени
c из k по 2. Если по максимуму брать, то есть четыре варианта записать одно и то же. Можно
писать явную формулу, можно написать обозначение прошлого раза, можно просто как это определяется
чисто с вероятностной точки зрения. Ну а я хочу совсем одну буковку сейчас mu, но виноват. Так
это удобнее, сию секунду mu и все. Ведем просто такое обозначение. Так, сейчас я соображу, чего я
теперь хочу сделать. Так, товарищи, с той стороны кто-нибудь есть? Если что-то вдруг непонятно,
не стесняйтесь, пожалуйста, задавайте вопросы. Я только не понимаю, мы сейчас в зуме работаем или
в ютубе? Первокурсники работали в зуме. У меня сейчас виден зум, но там пока видны только, а в зуме
никого. Но у меня виден чат зума, в котором очень много вопросов от первокурсников и никого из
второкурсников. Я понял, ну хорошо. Ну а тогда в ютуб, если вы задаете вопросы, мне просто оператор
соответственно транслирует, что непонятно. Так, сейчас я хочу ввести некоторое множество.
Давайте так, как бы мне его обозначить. Смотрите, идея очень простая. Сейчас обозначение я придумаю,
конечно. Идея очень простая. Нас интересуют такие независимые множества, которые мало
пересекаются. Давайте, наоборот, попробуем посчитать те пары независимых множеств,
которые вредят вот этому нашему желанию, то есть в которых элементы, то есть независимое
множество пересекаются не меньше, чем по двум элементам. Вот как-то так. Так, я, кажется,
понял. Давайте я еще введу вот тут вот обозначение. Я добавлю сюда фигурные скобки и для этого
множества введу обозначение k красивое от g. Ну это просто на несколько минут нам, наверное,
поможет. А именно сейчас я буду определять вот эти вредоносные пары, которые сильно пересекаются
вопреки нашему желанию, и мне будет удобно отсылать вас просто к кокалиографическому.
Так. Ладно, давайте притормаживаю w. Ну давайте w от g. Я просто думаю,
как мне обозначить, чтобы это коррелировалось тем, как я это говорил в прошлом году,
но это может быть не так уж важно. В конце концов, какая разница. Вот w от g. Это будет
количество таких пар k и t, k житая, что k и t, k житая принадлежит k красивому от g, и мощность
пересечения k и t и k житого больше либо равняется двойке. Ну как я обещал. Меня чуть-чуть смущает
обозначение w от g. Мне чуть-чуть казалось, что я его раньше как-то по-другому писал. Ну какая
в чем разница. Ну w и w. w от g это количество пар или может быть не количество лучше, а просто множество,
а мощность мы потом сюда навесим. Это неважно. Пусть это будет просто множество таких пар,
не количество, а множество. Множество пар, которые нехороши с нашей точки зрения. Так,
действительно мат ожидания мощности. Нет, это вы не поймете сходу. Сейчас я пытаюсь понять,
чего я хочу сделать. Смотрите, вот здесь был фиксированный граф. Я хочу, чтобы было понятно.
Я хочу сделать, чтобы было понятно. Сейчас был фиксированный граф. Мы выделили в нем все его
независимое множество. Вот так обозначили их совокупность. А дальше устроили прореживание.
Дальше устроили прореживание. Давайте знаете, что через c от g обозначим вот это вот прореженное
множество после того, как мы провели испытание Бернули. То есть это случайное множество,
являющееся под множеством k от g, коль скоро g фиксирован. Понятно? Ну можно написать распределение
вероятностей, конечно. То есть можно написать, с какой вероятностью это множество состоит из
конкретного набора k больших с какими-то индексами. Ну понятно, с какой. q со звездочкой в степени
количество этих k больших умножить на 1 минус q со звездочкой в степени xk от g минус
количество этих k больших. Ну это понятно. Вот. А дальше мы еще навешиваем случайность обратно
на граф. То есть если угодно у нас получается такое вероятностное пространство, в котором пары
возникают граф, он уже становится случайным, и его вот эта вот случайная совокупность. То есть
вероятность этой пары мы воспринимаем просто как произведение вероятности этого графа. Ну она равна
1 поделить на 2 в степени c из m под 2, в ней вообще ничего умного нет, на вероятность вот этого c от g,
которую я сейчас озвучил. Я понятно сейчас сказал? То есть у нас двойная случайность. У нас и граф
случайный, но и вот это множество, которое является под множеством k красивого, оно тоже случайное.
А вероятность пары это просто произведение вероятности графа на вероятность вот этого
случайного под множество. Так, ну и тогда, черт, мне бы какую-нибудь другую букву что-ли использовать,
да. Давайте w штрих o от g запятая c от g. Это будет множество совершенно такое же к аитах, кожитах,
но здесь будут к аиты и кожиты, принадлежащие c от g. Ну и то же самое вот это вот условие. Не лезет,
но оно то же самое. Так, интересное сообщение появилось или что? Там кто-то есть, да? Это круто.
Это товарища задержка из-за того, что мне стали все-таки показывать непосредственно
ютуб. Вернее, а в ютубе 7 человек. Я даже вижу сколько человек в ютубе. Ну если есть какие-то
вопросы в чат, то можете их задавать уже теперь мне непосредственно, но не суть. То есть два
множества отличаются, вот два этих множества, да, это случайные множества, но в первом случае
случайен только граф, и мы смотрим на те его независимые множества, которые плохо пересекаются,
сильно пересекаются, то есть одновременно не могут присутствовать в той гирлянде, которая
вычисляется вот этим y-катом. А во втором случае у нас другое вот это вот вероятностное пространство,
и w штрих вычисляет количество вредных пар, уже попавших в прорежанное, дополнительно рандомизированное
множество. Понятно сказал, нет? Ну если понятно, если понятно, а единственно, что я еще вот чего
хочу сказать, важный момент, может быть пары надо было рисовать не в круглых скобках, а в фигурных,
потому что я хочу подчеркнуть, что эти пары у нас будут неупорядоченными.
Неважно в каком порядке смотреть на два множества. Подчеркиваю, рисую не кортежи, так сказать,
а множество просто. Так, если C,E понятно, то давайте введем обозначение, господи помилуй,
сейчас для математического ожидания мощности w обозначим его дельта пополам. Пополам это вот
указывает на то, что нам все равно в каком порядке считать пары, а дельта это как раз когда не все
равно. Ну так, немножко замудренно сказал, но мне так удобнее просто обозначать и привычнее. Ладно,
но давайте начнем с более простого. Вот математическое ожидание мощности множества C. Может с этого
и надо было начать, а потом уже w всякие вводить, но неважно. Вот математическое ожидание мощности
множества C. У нас есть все обозначения для того, чтобы написать, чему оно равняется. Если вы
понимаете, как устроен процесс прореживания, он не зависит от того, как мы выбираем граф. Вы
выбрали граф, а дальше уже прорежем. Последовательное усреднение идет и по графам, по всем,
и по прореживаниям на множествах, независимых множеств этих графов. Посмотрите, давайте начнем
с более простого. Математическое ожидание мощности k. Сейчас вы меня убьете. Что такое
математическое ожидание мощности k? Конечно, это мю. То есть это шестое, что ли, или какое там
подсчет? Пятое, да? Пятый вариант того, как можно написать любой элемент из этой строчки. Это
мат ожидание мощности k, товарищи. Мощность k это xkt от g, значит его математическое ожидание это мю,
да, это любая из этих штуковин. Ну виноват. Что ж такого-то? Пятое, да? Может быть с этой
подсказкой понятно, каково мат ожидание мощности c. Если мы каждый объект из k красивого выбираем
с вероятностью куса звездочкой и не выбираем с вероятностью 1 минус куса звездочкой. Схема
Бернули. Что здесь надо написать? Я хочу от вас добиться. Это вроде очевидно. Как считается
математическое ожидание числа успехов в схеме Бернули? То, что написано в YouTube,
разложить по индикаторам, это доказывать можно и так, конечно. Я бы, по-моему, тут
очевидную вещь написал. То есть представьте себе, что само вот это k красивое не случайно,
граф g фиксированный. Вот представьте, что граф g фиксированный, с чего мы и начинали. Ну тогда
математическое ожидание мощности c. Это что такое? Это очевидно куса звездочкой умножить на вот это,
на xk, tg. А дальше мы усредняем по всем графам. Ну куса звездочка это общее для них. Усредняем
по всем графам, мю получаем, мю на куса звездочка. Все, что я хочу сказать. Ну да, конечно, это из
линейности получается, правда? Сначала одно усреднение проводим, потом другое. Знаете,
как представляют порядки суммирования? В сущности оно и есть. Нет вопросов. Более сложный вопрос,
но, по-моему, тоже понятный. Мат ожидания мощности w штрих. Коль скоро мат ожидания мощности w такое.
Но сейчас уже у вас опыт есть, я думаю, вы должны правильно ответить сразу. Мы делаем то же самое.
Мы здесь берем просто граф конкретный, а здесь к этому конкретному графу добавляем еще случайное
прореживание. Но при этом мы выбираем множество не по одному, а парами. То есть нам нужно, чтобы
каждое множество попало в c от g. Вероятность того, что каитое попадает в c от g, это куса звездочкой.
И вероятность того, что кожитое сюда попадает, это тоже куса звездочкой. То, что они оба одновременно
попадают сюда, это куса звездочкой в квадрате. Поэтому ответ, ладно уж, дельта пополам умножить на
куса звездочкой в квадрате. Понятно? Представляете себе именно так. Мы сначала фиксируем граф,
потом из графа выбираем вот эти вот случайные под множество прореживания. Так, чтобы получилось c от g.
Усредняя по ним, мы получаем куса звездочкой в квадрате, а затем усредняем по всем графам и
просто по определению получаем дельта пополам. Потому что вот этот квадрат, он общий для всех.
Ну, я надеюсь, что я понятно объяснил. Прекрасно. Теперь делаем еще одно последнее множество c со
звездочкой от g. Мы его получаем так, мы удаляем, это очень стандартная для нас идея, удалить что-то,
что нам вредит. У нас такое было, например, когда мы доказывали теорему Эрдаша про хроматическое
число и обхват. Помните? Вот мы тоже там чего-то случайно набрали, а потом подправили. Вот здесь
также удаляем из c от g по одному множеству из каждой пары, вот здесь видно, почему надо пары считать
неупорядоченными. Из каждой пары принадлежащий w' от g. Нам надо разорвать плохие пары. Зачем нам
их брать упорядоченными? Не нужно, конечно. Берем их без учета порядка и выкидываем любое множество
из такой вот неупорядоченной пары. Одно. И так из каждой пары. Ну, может, мы лишнее повыкидываем,
потому что вполне может случиться, что выкидывая одно множество из какой-то пары, мы уничтожим
сразу десяток пар, например. Может же такое случиться, что мы лишнее выкинем. Но понятно,
что уж во всяком случае мощность c со звездочкой от g точно не меньше, чем мощность c от g минус
мощность w' от g c от g. Вот так. Мощность совсем не лезет. Так, друзья, успеваете за мыслью,
да? Почему больше либо равняется? Мы выкинули отсюда, может быть, больше множеств. Нет,
почему бы сейчас? А, ну да, да, да, потому что выкинув одно множество, мы могли разорвать несколько пар.
Так, прекрасно. Ну, давайте усредним все это и по g, и по c от g. То есть у нас получится,
что, на самом деле, ну ладно, не важно, мощность c со звездочкой, а мат ожидания мощности c со звездочкой
больше либо равняется, нежели мат ожидания мощности c минус мат ожидания мощности w'. Ну, то есть это
у нас есть mu на q со звездочкой минус дельта пополам на q со звездочкой в квадрате. Ну, просто
подставил то, что мы с вами обсуждали. Так, подставил то, что мы с вами обсуждали. А зачем я это
сделал? Так, мат ожидания у Катова, оно-то не меньше, чем мат ожидания c со звездочкой. Ну, у Катой от g просто не меньше,
чем c со звездочкой от g. Потому что в c со звездочкой нет ни одной вредной пары, а у Катой это размер
самой длинной гирлянды без вредных пар. А c со звездочкой это некий алгоритмический такой вероятностный
способ построить конкретную гирлянду. Ну, значит, средняя от максимума не меньше, чем средняя от этой
конкретной конструкции. Это и есть наша нижняя оценка, которую мы хотели получить. Но она пока вот в таком
виде. Это что-то пока ничего, да? Не, ну, дайте я напомню. Нам-то нужно доказать, я не буду переходить к той
доске, что ейк больше либо равняется один плюс умалое от единицы. Это в первую очередь нужно тем,
кто сидит в ютубе, на 2k в четвертой. Но, елки-палки, вы мне не сказали, наверное, что там k1 вообще писать надо,
да? Давайте я тогда вернусь к той доске. Я забыл. Вот здесь надо писать вот так же. Давайте всюду
здесь считаем, что k это то же самое, что k1. Ну, виноват. У нас в прошлый раз было обозначение для параметра k1.
А сегодня я чуть-чуть про это забыл. Ну, давайте считать, что все буковки k, которые здесь присутствуют,
просто более короткая запись для того параметра k1, который у нас был на прошлой лекции. Естественно,
потому что мы доказываем лему в лемме именно для этого случая. Все, а то иначе я должен везде
сейчас переписывать, добавлять какой-то лишний индекс, это скучно. Конечно, и вообще этот индекс
действительно не смотрелся бы. Ну, в общем, короче, мы вот это хотим доказать, а доказали вот это.
Вообще, как вы думаете, ведь кусать звездочкой это параметр, который в нашей власти. Это параметр
прореживания, который просто какая-то чиселка из отрезка 0,1. Мы же, наверное, можем его сейчас
выбрать по оптимальнее, по возможности. А как его выбрать по оптимальнее, чтобы оценка
дальше была еще получше? По параметру q со звездочкой, по этому параметру, то, что мы видим, это парабола.
Парабола? Значит, она такая, да? Но это и хорошо, да, потому что... Сейчас. Нет, что-то я...
Дождите. Что-то меня заскокло. Парабола, что за смотр? Что-то я не понял. Правда же? А? Я хотел...
А, все, да, у меня заскок совершенно тупого характера. Конечно, надо выбрать q со звездочки так,
чтобы достигался вот этот максимум, но нельзя писать больше либо равно. Я написал больше либо равно,
и стою туплю как идиот. Ну, потому что это не больше либо равно максимума, а просто наилучшим вот
это значение будет, когда мы возьмем q со звездочки, где максимум достигается. Ну, конечно, да. То есть
надо взять q со звездочкой max, которая равняется минус b поделить на 2a, 2a это минус дельта,
минус b это минус mu, то есть mu поделить на дельта. Вот такое вот. Ну, давайте возьмем mu поделить на
дельта. Тогда у нас, конечно, здесь получится mu квадрат поделить на 2 дельта вроде бы. Правильно?
mu квадрат поделить на дельта, да, минус mu квадрат поделить на 2 дельта. То есть разность будет как
раз mu квадрат поделить на 2 дельта. Короче говоря, нам нужно вот это, а мы написали вот так.
Прекрасно. Так, ну и что? А все не так плохо. Я вот сюда вот двигаюсь так потихонечку. Я
же не зря вспомнил, что это k1. Давайте посмотрим на то, как мы выбирали параметры в самом начале
доказательства теоремы в прошлый раз. В прошлый раз это копилось там на какой-то доске, ну а в
этот раз оно уже пропало, потому что аудитория другая. В общем, что у нас получалось за счет того,
что k это k1? Не, не помните уже. У нас вообще вот про эту штуку кое-что говорилось. Это вот
сейчас ружье выстрелит, это будет полный катарсис. А если дельта ноль? Ну, Александр, находящийся в
ютубе, ну как дельта может равняться нулю? Не, ну скажите, дельта это математическое ожидание
очевидно положительной величины. Александр меня спрашивает, а если дельта ноль? Это, мне кажется,
не очень содержательное замечание, потому что дельта, конечно, не может равняться нулю. Это мат
ожидания случайной величины, которая не тождественно нулевая и уж точно принимает
неотрицательные значения. Дельта это удвоенное мат ожидания мощности некоего множества пар,
но понятно, что в очень многих графах это множество пар не пустое, поэтому мат ожидания точно не ноль.
На случайном графе, который принимает с положительной вероятностью любое свое
конкретное значение. Нет-нет, дельта, конечно, не ноль, там есть другой более тонкий вопрос,
и вот этот более тонкий вопрос нам сейчас предстоит обсудить. Там параметр вот этот k1 был
с одной стороны выбран таким образом, чтобы оказаться асимпатически равным, но здесь не важно,
что писать m или n, m такое было, что это неважно, 2 лог 2 ич на n, но это сейчас тоже не так существенно,
скорее вот в этом месте мы писали m в степени 3 плюс о малой от единицы, и это не шестое обозначение
для одного и того же, это просто его выражение в более грубом варианте записи. Мы так подобрали,
но все-таки возвращаясь вот сюда, нам хочется получить такое неравенство, мы имеем неравенство
такого вида, но, наверное, из этого следует как должно выглядеть дельта. Смотрите, если мы знаем,
что mu это... сейчас, что-то мне как-то... да-да-да-да, я понимаю, сейчас я, по-моему, немножко не в том
порядке излагаю логику, это все то, что я сказал правильно, то есть вот тоже на той доске написано,
это все правильно, конечно, было выбрано таким образом, но это сейчас, секунду, что-то я чуть-чуть
притормаживаю... а, да не, ну что я действительно притормаживаю? Вот если мне нужна такая оценка,
а я написал вот так, то, наверное, это означает, что дельта асимпатически ведет себя каким-то очень
понятным образом. Ну что ж я так притормаживаю, действительно, каким образом? То есть мне нужно
сказать, что mu2 поделить на 2 дельта асимпатически равно m2 поделить на 2k в четвертой, если я хочу
завершить доказательство Лемма в Лене. Я должен убедиться в том, что вот выполнено такое асимпатическое
равенство. mu2 на 2 дельта sim tilde m2 поделить на 2k в четвертой. Ну то есть, что отсюда следует
дельта асимпатически равно, так это сокращается, mu2 на k в четвертой на m2. Вот давайте временно
поверим, что дельта действительно такое. Это последнее, что надо будет проверять, самое последнее,
что останется. Помните, я обещал Лемма, Лемма в Лене, потом еще отсылка, вот это еще отсылка. Ну вот
сейчас последнее, что мы проверим, что это правда. Но пока это непонятно, почему так, это надо считать
честно. Давайте поверим, что это так, тогда все получилось. Что мы доказали Лемму в Лене или нет?
Это провокационный вопрос. Если поверить вот в это, это останется проверить, как Лемму в Лене в
Лене. Но допустим, мы эту Лемму в Лене в Лене доказали. Замечание Александра про то, что дельта
может оказаться нулем, оно не очень состоятельное, потому что, ну не может оно оказаться нулем,
хотя это была хорошая попытка. Я предлагаю гораздо более прозрачную и забавную попытку предпринять сейчас.
Вот, правильно, да. Кто нам сказал, что вот эта вот штука меньше единицы? Почему это меньше единицы,
с какой радости? А вдруг это не так, тогда все наши вероятностные рассуждения мук леш. Смотрите,
но мы в это по-прежнему верим, сейчас нам это предстоит как-то проверять, но мы в это верим.
μ поделить на дельта асимптатически равно μ поделить на μ квадрат к в четвертый умножить на
м в квадрате, шлеп-шлеп, получается м в квадрате поделить на к в четвертый мю. И вот тут, наконец,
возвращаемся опять к первой доске, выстреливает вот это. Мы подобрали параметры так, чтобы мю было
м в кубе, ну там с какими-то логарифмами, но неважно с какими логарифмами, к это тоже логарифм,
но тут м в кубе, то есть это м в квадрате поделить на к в четвертый на м в кубе, но опять же с точностью
до логарифмов, все равно это стремится к нулю, а нам достаточно, чтобы при больших значениях
m все было меньше единицы, как и получается. Осталось как-то убедиться вот в этом. Сейчас я
напишу некую жуткую сумму, которая выражает дельту и постараюсь убедить вас в том, что да,
получится так, но кусок доказательства оставлю за рамками, то есть не буду давать его экзамени,
конечно, тоже, и сейчас не буду, потому что просто скучные выкладки будут. Так, давайте, наверное,
где-нибудь на левой, что ли, доске на самой, вот здесь вот напишем.
Так, мат ожидания мощности w, но это, конечно, по линейности считается. Мощность w это количество
этих неупорядоченных пар независимых множеств, то есть мы суммируем по всем. Так, тут длинный-длинный
вопрос. А почему, если e y k t больше либо равняется константа от x, а константа от x tilde x,
то e y k t больше либо равняется f от x? Не понял, а к чему это? Я, к сожалению, стер e y k t,
я чуть не понял. Не понял в чем вопрос. Не понял. Но это не константа никакая, дельта совершенно не
константа. Это некая функция, которая ведет себя асимпатически вот так. Дельта себя ведет
асимпатически вот так, но это в точности означает просто, что дельта это 1 плюс о малое от единицы
на μ квадрат k в четвертой на m в квадрате. Это просто одна и та же запись. Вернее, разные
записи одного и того же явления. Дельта тильда и дельта равно 1 плюс о малое от единицы.
Ну, я просто подставляю вот это все вот сюда вот, и я получаю ровно то, чего хочу. Ну,
там будет 1 плюс о малое от единицы в знаменателе, но 1 поделить на 1 плюс о малое от единицы это
тоже, конечно, 1 плюс о малое от единицы с какой-то другой конкретной функцией, которая тут стремится
к нулю. В общем, я не очень понимаю, в чем этот вопрос. Ну, вот я вроде пояснил. Ренес,
я сумел пояснить? На правой доске мы заменили. Да, вот я сейчас это вот как раз и пояснял.
Тильда это и есть 1 плюс о маленькой от единицы умножить на. И если это 1 плюс о маленькой от
единицы стоит в знаменателе, но это все равно, что написать 1 плюс о маленькой от единицы в
числителе. Я надеюсь, что это стало понятно. Вот, я возвращаюсь сюда. Ну, как по линейности это
посчитать? Надо просуммировать, сейчас туда-сюда бегаю, надо просуммировать вообще по всем парам
множеств мощности K, которые вот так пересекаются, а потом умножить на вероятность того, что оба
этих множества являются независимыми в граффиже. То есть картина такая. Есть мэ вершин. Дальше мы
фиксируем среди них K вершин какого-то множества K большое и Т. Потом мы фиксируем сколько-то общих
вершин, которых будет не меньше двух. К этим общим добавляем еще сколько-то, ну пусть здесь
Т вершин, тогда здесь будет K минус Т. Вот эти две сардельки вместе дают К же Т, а вот это вот множество
К и Т. А две сардельки вместе это К же Т. И вот надо перебрать все такие пары множеств и для каждого
из них проверить оба ли они являются независимыми в случайном графе. То есть просуммировать надо
так одну вторую мы тут не пишем или пишем одну вторую, а ну мы же Дельта считаем, тогда одну вторую
не пишем. Надо просуммировать по Т от двойки до К без единицы С из М по К на С из К по Т на С из
М минус К по К минус Т. И дальше одна вторая в степени 2 С из К по 2 минус С из Т по 2. Вот какая хрень получается.
Абсолютная. Не, ну еще раз Т вот это вот количество общих элементов Каитова-Кожитова, поскольку мы
сейчас в определении Дельта считаем, что пары все-таки упорядоченные, то на двойку делить не нужно.
Вот мы сначала выбираем К вершин, вот эти К вершин для множества Каитова, потом последовательно
выбираем из вот этих К вершин Т для кусочка множества Кожитова, который как раз находится в пересечении,
потом из оставшихся М минус К вершин выбираем вот эти К минус Т. После чего отсутствующих в обоих множествах
суммарно ребер должно быть ровно столько, как я написал. 2 С из К по 2 это если бы они не пересекались,
но они имеют Т общих вершин, поэтому надо вычесть еще С из Т по 2. У меня почему-то опять
зум появился на экране. Он мигал-мигал и появился, да? Еще раз? Мат ожидания, да, ой, елки-палки,
а мощность В, да, это дважды мат ожидания, то есть это Дельта, да, мы Дельта считаем. Ну, я хочу
доказать, что Дельта ведет себя симпатически вот так, как здесь подчеркнутым написано. Да, да, да,
это мы Дельта считаем. Вот она. Так, все согласны, что Дельта такая? Это понятно? Мерзость абсолютная,
понимаете? Значит, что я сейчас сделаю? Я вам покажу, что при Т равном двойке, именно двойке,
при начальном слагаемом, вот эта вся хрень, она попроще выглядит, но вот именно она и равна симпатически
вот тому, что здесь написано. А больше ничего доказывать не буду. Ну, то есть дальше я скажу,
что в принципе надо действовать примерно так же, как мы делали, когда, если помните, доказывали С
больше единицы в теореме про связанность случайного графа. Там тоже была жуткая сумма, в которой все
было симпатически сконцентрировано в начале. Ну, там была какая-то сумма по К от единицы до N пополам,
С из N по К на единицы минус П в степени К на N минус К. Ну, была там какая-то такая сумма. Неважно,
можете не вспоминать. Но мы там разбивали суммирование на несколько частей, в одном оценивали
геометрической прогрессии, в другом еще там как-то как хвост. В принципе, здесь можно сделать так же,
но здесь еще сложнее. Если бы здесь было так же, я бы сделал, но здесь сложнее. Останется просто
поверить уже вот в этом месте, останется поверить, что все остальные слагаемые тонут вот в этом начальном.
Но у нас такое бывало, поэтому поверить легче. То есть какое-то время дальше вот в этом суммировании
все будет геометрически убывать, как геометрическая прогрессия, экспоненциально убывать,
потом оно начнет снова возрастать вот к этому К-1, но настолько вырасти не сможет, чтобы
достигнуть значения при t равном двойке. Все равно будет бесконечно мало. Но это очень мучительно
считать асимптотически, я не буду это делать, потому что это не помогает увеличить сумму ваших знаний.
Вот все, что вы понимаете, этим вполне могло бы ограничиться. Это кажется, что уже есть полное
доказательство того, что нам нужно. Сумму ваших знаний увеличило бы, знаете, что, если бы я
доказал неравенство азумы для липшицевых функций, вот это бы увеличило. Но это еще три лекции,
но я же не спецкурс читаю про хроматические числа случайных графов, поэтому давайте все-таки
соразмеряться. Ну вот, при t равном двойке мы получаем c из m по k на c из k по 2 на c из m
минус k по k минус 2 на одну вторую в степени 2 c из k по 2 минус 1. И это надо поделить,
ну поделить в том смысле, что мы же доказываем асимптотическое равенство, давайте поделим на вот
эту дробь и убедимся, что в пределе будет единица. Поделим на вот эту дробь и получим
предель единица, μ квадрат. Ну ладно, давайте я напишу честно μ квадрат, а потом раскрою,
μ квадрат, k в четвертой, а здесь m в квадрате. Так, напоминаю, что μ это просто c из m по k на
одна вторая в степени c из k по 2 математическое ожидание x катова. Нет, Александр, двойку перед
суммой уже не нужно, потому что если бы вот этой двойки не было, то перед суммой была бы одна
вторая, а когда мы на двойку домножили, одна вторая пропала. Дельта это количество упорядоченных пар,
а дельта пополам это количество неупорядоченных пар. Я виноват, я немножко запутал, но смысл очень
простой. Двойка перед суммой уже не нужна. Так, м квадрат на k в четвертой, а м в квадрате все
отлично. Так, и вот оно такое, поэтому давайте еще раз перепишем. Это равняется c из m по k на c из k по 2
на c из m минус k по k минус 2. Одна вторая 2 в степени c из k по 2 минус 1. Поделить на c из m по k
в квадрате на одна вторая в степени 2 c из k по 2 и на k в четвертой, а потом еще умножить на
м квадрат. Вот так вот. Следите только, пожалуйста, очень внимательно, что вот тут я где-нибудь могу
наложать и после этого будет ой, не сошлось. Ужас какой, но вроде пока правильно. И главное тут есть
элемент радости. Смотрите, шлёп-шлёп, хлоп-хлоп. Видите, как хорошо. Кое-что сокращается. В общем,
довольно пакостное сокращается как раз. Сейчас я сотру на центральной доске, себе место освобожу.
Так, что нам делать? Давайте я еще на всякий случай напомню, что k это тильда 2 лог 2-ичный
m. Я не думаю, что в таком виде нам это понадобится. Важно только, чтобы вы понимали, что k само
стремится к бесконечности. У нас вся асимптотика по м, м стремится к бесконечности, но вот k тоже вместе
с ним растет, поэтому кое-что тут можно наверняка упростить, например, сказать вот так. Это тильда,
так, k квадрат пополам. Я заменил c из k по 2. Но мне асимптотика нужна. Да, и тут еще одна вторая,
но в минус первой степени, то есть умножить на 2. Это вот эта одна вторая в минус первый. И дальше
умножаем на c из m минус k по k минус 2. Я пока это трогать не буду. Делим на c из m по k. И дальше
еще вот это, m в квадрате. Ой, я написал на k в четвертый. Вроде все, да? Все остальное кокнули. Вот
эти двойки пропали. Так, k квадрат тоже сокращается вот здесь. В итоге у нас остается
c из m минус k по k минус 2 поделить на c из m по k, m в квадрате на k в квадрате. Ну, кажется,
я сейчас должен победить. Очень надеюсь на это, потому что все, что мне осталось доказать,
это что вот эта первая дробь а симпатически равна k квадрата на m в квадрате. Ну, давайте я ее
перепишу. c из m минус k по k минус 2 на c из m по k. Ну, конечно, она довольно противная. Что
говорите? Нет, по стирлингу не будем. По стирлингу не будем. И, кстати, воспользуемся все-таки тем,
что k логеретмическая. По стирлингу не будем. Мы факториалы честно напишем. m минус k факториал,
часть из них сократим. k минус 2 факториал, m минус 2 k плюс 2 факториал. Да, так ведь. Так,
умножить на... Тут m факториал, тут k факториал. Так, k минус 2 факториал, m минус k факториал.
Снова что-ли? Ой, какая прелесть. Не, ну, на самом деле ничего страшного тут нет. Значит,
это равняется, смотрите, k факториал поделить на k минус 2 факториал, это k умножить на k минус 1.
Поэтому я напишу, не равняется отиль, да? Мне все с точностью да симптотики достаточно. Я напишу k квадрат.
Так, k квадрат нарисовалось. Это очень приятно осознавать, товарищи. k квадрат нарисовалось. А тут
остается m минус k факториал. Еще раз, m минус k факториал поделить на m факториал и на
m минус 2 k плюс 2 факториал. Ой, боже ж ты ж мой. Ну, что делать? Что делать? Надо сокращать факториалы.
Не, ну давайте сокращать, иначе я вас запутаю. Что поделать? k квадрат, оно осталось по-прежнему,
ну а факториалы тут, по-моему, все равно какие сокращать. Я бы, честно, m квадрат
какой-нибудь отщепил сразу, тогда лучше тильду нарисовать, поскольку мы знаем,
что оно должно отщепиться. Вот. Ну, откуда я его отщепил? Например, отсюда. То есть,
здесь осталось m минус 2 факториал. Так, чего у меня получается после того, как я сокращаю m минус
k факториал и m минус 2 факториал? Ну, в знаменателе остается m минус 2, m минус 3 и так до m минус k
плюс 1. Противно. m минус 2, m минус 3, m минус k плюс 1. Следите, следите, могу где-то наврать,
но я стараюсь так рассказывать, чтобы за этим уследить было можно. То есть, я отщепил m в квадрате
так, чтобы получилось ровно то, чего хочется. У меня тут осталось m минус 2 факториал. Ну, вообще
не важно, какой факториал, пусть будет m минус 2 факториал. А тут m минус k факториал. Ну, вот, значит,
выживают m минус 2, m минус 3, m минус k плюс 1. Это я вот этих сократил. m минус k плюс 1. А этих вот
прям так, как они написаны, сокращаем. Тут сверху остается m минус k, m минус k минус 1, m минус 2 k
плюс 3 что ли? Ну да, конечно. Согласны? Так, в ютубе вроде вопросов тоже нет. Так, давайте
посчитаем, сколько совмножителей внизу и сколько сверху. Значит, здесь m минус k минус 0, потом m
минус k минус 1. То есть, от нуля считаем 1. А это m минус k минус сколько? Минус k вычесть 3, да? То
есть, k минус 2. Вот здесь k минус 2 совмножителя. А здесь сколько? Здесь вычитается 2, 3, потом k минус
1 тоже k минус 2. Согласны? И тут сверху k минус 2 совмножителя, и тут снизу k минус 2
совмножителя. Давайте сделаем гениальный ход. Вытащим просто и сверху m в k минус 2, и снизу m в
k минус 2. Сверху останется 1 минус k на m, 1 минус k плюс 1 на m и так далее. 1 минус 2 k минус 3 на m.
Ну и снизу такая же фигня. 1 минус 2 на m, 1 минус 3 на m. Я очень подробно объясняю, вы в принципе должны
были научиться этому в начале курса, но наверняка же забыли. 1 минус k минус 1 на m. Согласны?
Просто из каждой скобки вытащил m. Естественно, вот это вот долой. И вот тут важно, что k очень
маленькое. Если б k, например, уже было порядка корня из m или больше, не дай бог, то ничего бы не
получилось. Мы с вами, в принципе, в асимптотической части курса проходили с самого начала. У нас такое уже
бывало. Ну, я не знаю, поприятнее смотреть на нижнюю часть, а верхнюю сделаете аналогично. То есть нижняя
часть, я переписываю 1 минус 2 на m, 1 минус 3 на m, так далее. 1 минус k минус 1 на m. Это e в степени
логарифм от 1 минус 2 на m. Абсолютно стандартная вещь. Мы с такой неоднократно сталкивались. Плюс,
и так далее. То есть тут набор очень несложных на самом деле инструментов, которым достаточно
привыкнуть разок. И все. 1 минус k минус 1 на m. И вы это уже будете в уме как бы считать. Ничего
сложного. И дальше по Тейлору раскрываем. E в степени минус 2 на m, минус 3 на m, минус k минус 1 на m.
Плюс, ну, господи, можно написать о малой от той же самой суммы и вообще не заморачиваться здесь.
Многоточие это вот эта же самая сумма. Просто о малой. Не о большой. Квадраты ненужные.
Просто о малой. Вот получится E в степени минус тут m. А тут чего? Ну, сумма не от 1 до k минус 1,
а от 2. Но это уже смешно. Я лучше так напишу k квадрат на 2m на 1 плюс о малой от 1. И все.
Как квадрат пополам, это сумма числителей с точностью до асимптотики. Ну, и вот эта асимптотика
тоже, конечно, отправляется вот сюда, вот этого маленькой от единицы. Ну, k это логарифмическая
величина. То есть все, что здесь стоит с огромным запасом, стремится к нулю, а вся экспонента
стремится к единице. И точно так же абсолютно устроен числитель. Но только здесь суммирование
начнется не от двойки и закончится не k минус 1, а от k и до 2k минус 3. Ну, какая разница?
Оно все равно будет порядка k квадрат. А k квадрат это фигня. Все. То есть и числитель стремится к
единице, и знаменатель стремится к единице. Значит асимптотика вот ровно такая, как нам
хотелось. Все сократилось, и это действительно единица. Ну так вот. Доказал в общем. Ну что,
я готов переходить к новой теме, если вопросов по этой истории нету. Я готов переходить к
гиперграфам, рассказывать какие-то экстремальные характеристики их, всякие пересечения множества
и так далее. Это уже другая тема. Я собираюсь стирать. Да. Но там последнее, что было про двойку
перед суммой. Все вроде мы обсудили. Все хорошо. Время еще есть где-то 20 минут. Я сейчас порассуждаю
про новую тему. Скорее всего ничего доказывать не буду, просто ввиду объекты, какие-то экстремальные
характеристики, которые нам будут интересны. Давайте сразу центральную туску тоже сотру.
Так, давайте новое. Значит, гиперграфы и некоторые их тоже экстремальные характеристики. Ну просто
характеристики на самом деле экстремальные в том смысле, что как и для графов, нас в основном будут
интересовать максимумы или минимумы чего-то. Как число независимости, это максимум множества какого-то.
Но гиперграф, что это такое? Мне подарили тут магнитную наклейку на холодильник, на котором
изображен я как ведущий этой самой, как называется, игры. Господи, Якубович, как она называется?
Стол они крутят. Поле чудес, да, как ведущий поле чудес. Написано, значит, там в клеточках,
как положено. Почти все слово только пропущено там, где гиперграф. А я стою с таким, знаете,
кабачком цукини в руке. Почему цукини? Потому что гиперграф это практически то же самое,
что граф. Странно, что, кстати, не с сарделькой, а именно с кабачком. Это то же самое, что граф,
но ребра у него это не двухвершинное множество, не двухэлементное множество, а сколько-то,
неважно сколько, двух, трех, пяти. То есть это такой же объект, есть множество вершин,
есть множество ребер, но если вершины это просто какое-то множество, обычно конечное,
то ребра, можно вот так написать, это подмножество в множестве всех подмножеств. 2 в степени В это
множество всех подмножеств, а если я пишу именно такое включение, это значит, что E является
набором каких-то объектов из вот этого множества, то есть совокупностью подмножеств. Иными словами,
для любого A из E, A это какое-то подмножество, может быть, совпадающее даже с В. Все множество
вершин тоже может быть ребром гиперграфа. Давайте только считать для дальнейшего, что мощность A никогда
не принадлежать у нас будет 0 и единица. То есть мы не будем с вами рассматривать гиперграфы,
у которых бывают пустые ребра или ребра, состоящие только из одной вершины. Каждое
ребро в нашем понимании будет состоять хотя бы из двух вершин. Это не значит, что гиперграф
с такими условиями, без перечеркиваний, нельзя определять, можно, но мы такие не будем рассматривать.
Мы такие рассматривать не будем. Мы будем считать, что мощность A не принадлежит 0 и 1. Больше того,
мы будем говорить с вами всюду только про K однородные гиперграфы. Ну то есть для любого A из E
мощность A будет равняться K. Все ребра будут одной и той же мощности и, конечно, два однородные
гиперграфы это просто обычный граф. Мы будем рассматривать гиперграфы всегда в таком же
сугубо обыкновенном смысле, как и графы. То есть мы будем считать, что ребра неупорядочены. Это
просто сочетание каких-то объектов из V. Рёбра неупорядочены, вершины в них, конечно,
не повторяются. Кратных ребер нет, это тоже видно прямо отсюда. Вершины не повторяются,
значит как бы аналогов петель не бывает. Ну вот такой совсем обыкновенный будет гиперграф.
Понятно, что если графы было удобно изображать на плоскости, рисуя какие-то отрезочки или дуги,
соединяющие вершины точки, то с гиперграфом все не так здорово. Но серьезно, либо кабачок цукини,
либо сарделька. Мне кажется, сарделька это прекрасный способ изобразить ребро.
Три однородных гиперграфа. Какая сарделька. Вот эти три вершины, например, тоже образуют вот
такой вот. Как же это нарисовать? Вот так. Вот тоже сарделька. Я согласен, что рисовать такую штуку
на самом деле, конечно, совершенно невозможно. Поэтому сардельки, это скорее для меня лично
приятный, просто чисто на вкус приятный, наверное, способ представить себе, что происходит. Я
действительно представляю себе такое множество объектов и там какие-то подмножества, как сардельки
такие вот, извиваются и все. Вы можете представлять так, как вам это удобнее. Понятно, что люди,
которые среди вас, например, изучают топологию, они наверняка или почти наверняка слышали некий
вариант истории про гиперграф, который называется симплециальный комплекс, но может и не слышали.
Не было симплециальных комплексов, ну услышите когда-нибудь, точно будут. Вот это тоже гиперграф,
но с таким он не однородный, он со свойством наследования. То есть если там есть какое-то
ребро мощности 3, то и все его подмножества тоже являются ребрами. Ладно, это проехали,
это никто не знает, поэтому я замолчал. Никаких гиперграфов, симплециальных комплексов не будет.
Гиперграф и гиперграф. Такая вот набор сарделек в кастрюке. Вот, давайте, наверное, сразу ведем
три величины, изучением которых мы будем заниматься для гиперграфов, потому что они важные. Они
связаны с теорией кодирования, они связаны с разными задачами чистой математики и ее приложений,
поэтому я считаю крайне важным все их три обсудить в том или ином виде. Давайте начнем
с величины, ну давайте f от nrs. Я здесь буду писать буквы rs, ну сейчас вы увидите, что это такое,
три параметра. Параметры очень простые. Значит, на самом деле это вот что такое,
сейчас я напишу формально, потом аккуратно прокомментирую. Это максимальное такое k,
что существует r однородный гиперграф на n вершинах.
Сейчас я просто думаю, ну ладно, давайте так. Такое, что мощность e равняется k и для любых ab,
принадлежащих e, мощность a пересеченного с b не меньше, чем с. Букв много, сейчас поясню,
все на самом деле правда несложно. Значит, что говорится? Говорится, давайте считать,
что у гиперграфа сейчас n вершин, вершин n штук. Ребра каждая состоят из r вершин,
r однородный, значит, каждое ребро это просто под множеством мощности r. Ну вот какое-то под
множество условной мощности r, вот там какое-то, ой, не попал, под множество условной мощности r,
ну тут тройки для примера нарисованы, ну и так далее. Вот такие вот r-элементные сардели. Но
от них дополнительно требуется, чтобы они попарно имели не менее, чем s общих элементов,
пересекались достаточно сильно, каждые два множества. Но вот на этом рисунке s равно единице,
или нулю, как хотите, но нулю бессмысленные ограничения, а единицу уже какой-то смысл имеет.
Можно вот такую еще сардельку добавить, такую, будет последовательный такой цикл из трех сарделек,
которые попарно пересекаются ровно по одному элементу. Ничего не напоминает? Только что было,
ук. Вот это про то самое. Но в ук требовалось, чтобы они попарно пересекались не больше,
чем по единице, а здесь мы говорим наоборот не меньше, чем, ну и уже не по единице,
а там по какому-то абстрактному s, которое зафиксировано заранее. Ну на самом деле,
чтобы совсем было действительно хорошо, давайте я сразу скажу h от nrs, то же самое,
только вот здесь меньше либо равно s. А вот это уже в точности ук, прям вот один в один.
Ну просто ук, там вот это r, оно же k, было каким-то конкретным, а здесь або каким является.
Опять же s там равнялась единице, а у нас оно або какое теперь? То есть вот это задача теории
кодирования, прямо в чистом виде. Я могу не повторять эту историю, потому что это в прошлый раз
обсуждалось. И там и там максимумы, то есть нас интересует, насколько много можно построить ребер,
ну мало-то понятно, вообще ни одного ребра не возьмете, условия все формально будут выполнены,
одно ребро возьмете, все условия формально будут выполнены. То есть хочется построить как можно
больше ребер данной мощности, на множестве вершин тоже данной мощности, чтобы эти ребра попарно
как-то вот друг относительно друга располагались. В этом случае сильно попарно друг друга цепляли,
а в этом случае наоборот слабо. И еще есть величина m от nrs, которая нас очень будет интересовать,
это вот такая. Ну они там как-то еще между собой хитро связаны, конечно. Так, первый вопрос,
все-таки нужно ли, наверное, пояснить? Я вот сейчас подумал, я про кодирование-то может не говорил,
почему вот эта задача про h, она задача теории кодирования, или того, что было когда-то на
окотече, а я это рассказывал, вам достаточно? Наверное, нет, да, забыли уже все. Почему это,
да, типичная задача теории кодирования? Наверное, это все, что я успею сегодня сделать за оставшееся
время. Ну и хорошо, что перегружать-то информации? Почему h от nrs это про кодирование?
Потому что фактически, я объясняю, почему h от nrs это задача теории кодирования. Писать на
доске я не буду, слова мне несколько раз повторю. Так, потому что фактически можно
интерпретировать всю эту историю следующим образом. У вас ребро а это не что иное, как вектор
из единиц и нулей, вектора из единиц и нулей, у которого r единиц, ну и все, и n-r нулей. Можно
так интерпретировать, но понятно, что имеется в виду. Если у нас а состоял из элементов 1, 2 и
так далее r из вершин 1, 2, давайте так, v равняется 1, 2 и так далее n, это множество вершин.
Если а у вас состоял из первых, например, r элементов множества v, то здесь будет первые r
единицы, а дальше n-r нулей. Стандартная история, так у меня появился вопрос. Да-да-да, все,
рассказываю, как это связано с теорией кодирования, просто чертова задержка в этом
ютубе. Вопросы приходят позже, чем я начинаю на них отвечать. Я уже рассказываю, как это
связано с теорией кодирования. Все поняли, да, как векторы строятся, но мы такое делали уже на
этих лекциях. Строятся векторы, такие характеристические, что ли, соответствующие этим множеству. Векторы из
нулей единиц. Ну что такое теория кодирования? Надо передавать каждое конкретное слово по
каналу связи, кодируя его чем-то вот таким. Вы берете слово мама и ставите ему в соответствие,
например, вот такой вектор r единиц, а дальше n-r нулей. Вам берете слово папа, а ему ставите
в соответствие n-r нулей, а дальше единица. Ну, например, то есть как-то. И вот у вас есть передатчик,
вот у вас есть приемник, вот есть канал связи. Вы по каналу связи последовательно передаете вот
эти единички и нолики. Так? Передали единичку, а что получит приемник? Единичку? Ну на самом деле
черт его знает. Может тут есть какие-то помехи на канале связи? Могут быть помехи. И мы можем знать,
ну я точно рассказывал задачу теории кодирования в окотече вам, потому что я же рассказывал границу
Плоткина. Вот это вот как раз про вот это. Мы можем знать, сколько ошибок допускается на одно слово,
то есть сколько единиц и нулей суммарно могут преобразиться во что-то противоположное. Мы знаем,
что есть какие-то там D-искажения. Искажения это превращение единицы в ноль не более чем D-искажения.
Единица может превратиться в ноль, ноль может превратиться в единицу. Вот когда вы передаете
очередное слово как последовательность из единицы нулей, вы точно знаете, что не более чем D
ошибок будет допущено на этом канале. Вопрос состоит в том, как построить максимально большое
количество вот таких вот кодовых слов последовательности из нулей единиц, чтобы на выходе можно
было однозначно восстановить информацию. Ну что это значит? Вы рассматриваете какое-то кодовое
слово, например вот такое. Рассматриваете какое-то другое кодовое слово, например вот такое. А дальше
смотрите все слова, которые могут получиться из этого слова за счет не более чем D-искажений.
Ну одно искажение, это значит какое-то здесь может быть нолик появился. Короче,
вы рассматриваете такой круг. Помните, я говорил хемминговое расстояние, это все было в АКТЧ.
Рассматриваете такой круг, у которого хеммингов радиус, это как раз вот это вот D. Хеммингов
радиус, это как раз вот это D. Если два таких круга пересекаются, это значит,
вы плохие слова взяли в свой словарь. Потому что представим себе, что это слово превратилось
в какое-то вот такое, и это тоже может, в неможе превратиться. Все, вы не восстановите информацию
никак. Но здесь как раз взят пример таких двух слов, которыми это, наверное, не случится,
когда R фиксировано, а N большое. То есть я плохую картину нарисовал. Тут будет скорее вот так как раз.
Они не будут эти круги пересекаться, а раз они не будут пересекаться, то мы всегда точно можем
сказать, какое бы ни было здесь слово, оно точно не из этого слова получено. Какое бы ни было здесь
слово, оно точно не из этого слова получено. Все заранее знают словарь. Все все заранее знают.
Это не криптография, это все я говорил в АКТЧ. Это способ исправлять ошибки при передаче по
зашумленному каналу связи, когда известно, какое сверху ограничение на
количество искажений на каждое слово. Так, ну хорошо, а при чем здесь все-таки задачи
про h от nrs еще раз? Это мы не договорились. Это мы вроде как не договорились.
Но вот что отвечает за хеминговое расстояние между вот такими двумя словами?
Фактически мы же хотим чего сделать? Мы хотим сделать так, чтобы каждые два
слова в нашем коде отстояли друг от друга на расстоянии строго больше, чем 2d.
Это понятно? Вот у нас есть какое-то слово, вот есть какое-то другое слово, у них
сколько-то общих единиц, сколько-то общих нулей. Вот есть какие-то два слова.
Когда расстояние по хемингу, то есть количество отличающихся координат,
между ними больше какого-то 2d, например. Вот когда оно больше, чем 2d? При каких
условиях? Что это значит? Это значит, что и таких много, и таких много. Но у нас r
единиц тут, и r единиц тут. Значит, вот это равно вот этому, очевидно. Ну то есть сумма вот этих
величин больше, чем 2d. Если каждая из них больше, чем d, а это в точности то же самое,
как посмотреть на пересечение и сказать, что оно меньше, чем r-d. Вот это пересечение должно
быть меньше, чем r-d. Вот обозначите это за s, наверное, минус 1, и вы придете в точности к
задаче отыскания максимального количества таких кодовых слов. Максимальный размер словаря,
который можно составить для передачи информации по этому каналу связи, если мы хотим с гарантией
восстанавливать, какое слово передавали. Не знаю, я понятно объяснил? Ну с учетом того,
что это было в прошлом году, я надеюсь, что тем более понятно. Границу-то плотки, наверное,
действительно давал с помощью матрицы Адамара. Так что кое-что у нас про это было. Но вот кроме
матрицы Адамара, оказывается, есть еще такая гиперграфовская история, которая легко транслируется
в историю с теорией кодирования. Я еще одну вещь успею сказать про вот это. Это у нас тоже было.
У нас с вами было m от n3,1, товарищи, в некотором контексте. А именно у нас был, если помните,
такой замечательный граф. Он дважды был. Сначала он был как иллюстрация к Гамильтоновости по
Эрдешу и Хайнеллу, а потом он был как пример, если кто помнит, рамсейского графа. Хотя слово
рамсей там скользко произносилось. В общем, как пример ситуации, когда альфы и омега одновременно
маленькие. Помните такую историю? В общем, это был граф, у которого вершины это всевозможные
тройки элементов вот отсюда. Ну или что то же самое, векторы как раз из нулей единиц,
с тремя единиц, мен минус тремя нулями. Мы его и так и так интерпретировали. А ребрами мы называли
любую пару вершин, мощность пересечения которых в точности равнялась единице. Помните такой граф?
Дважды использовался. Так вот, альфа от этого графа, а ровно его мы как раз читали, причем мы даже
использовали, если кто-то еще помнит, красивый линейно-алгебрайический метод. В общем, альфа от этого
графа это в точности м от n3-1. Потому что нас как раз интересует самое большое количество троек,
никакие две из которых не пересекаются ровно по единице. Альфа от этого графа. И вообще,
можно ввести граф именно g от nrs, у которого вот здесь будет вместо тройки r, а здесь вместо
единицы будет s. И альфа уже вот от этого графа это в точности м от nrs. Вот такая связь.
Понятно? Ну все на сегодня. Если вопросы какие-то остались, спрашивайте, а так все.
Ну в следующий раз займемся уже изучением этих величин, то есть как-то оценивать будем и так далее.
Ну не очень понятно, что вы имеете в виду под этим. То есть, видите, мы сопоставили
гиперграфовской задачи, теоретикографовую, но информация другая. То есть, здесь нам нужно
просто альфа найти, но это то же самое, что найти на гиперграфе такую характеристику. Конечно, да,
вот именно эту характеристику гиперграфа экстремальную можно выразить вот в таких
графовских терминах. Но вообще говоря, информация, если с практической точки зрения смотреть,
которая содержится в гиперграфе, она больше в любом случае, чем та, которая в графе. Например,
ну какая практическая точка зрения. Знаете, вот любят люди изучать, и это бывает очень полезно,
графы каких-нибудь соавторств. То есть, есть большое количество математиков, например, и
некоторые из них пишут совместные статьи. Ну обычно какой граф строят? Вершины это математики,
а ребрами соединяют двух математиков, если у них есть общая совместная работа. Ну можно мультиграф
делать, если совместных работ много. Но, конечно, в этом графе, мультиграф, то есть с кратными
ребрами, в этом графе информация о структуре взаимодействия между авторами точно беднее,
чем если вы составите гиперграф, в котором вот эти гиперрёбра, это будет прямо множество соавторов
конкретной статьи. Потому что в конкретной статье может быть не два автора, а десять, там,
пятнадцать. Бывают мегаколлаборации по тысячи соавторов, по пять тысяч даже. Там всякие
экспериментальные работы по физике так пишутся. В ЦЕРНе сидит огромное количество экспериментаторов,
в МФТИ тоже, и вот получается гигантское количество соавторов, гиперрёбро колоссальное. Так,
Александр задал прекрасный вопрос тем временем в Ютубе. GATNRS действительно называется графом
Джонсона, и это терминология, которая как раз идет из теории кодирования. Разумеется,
для теории кодирования эти графы тоже нужны. Тут мы запрещаем все маленькие расстояния,
а тут мы запрещаем конкретное расстояние Хемминга. Понятно, что величины в итоге окажутся так или
иначе друг на друга влияющими, поэтому специалисты по теории кодирования GATNRS тоже изучают. И вот в
теории кодирования такие графы называются графами Джонсона. Есть и другие науки типа
комбинаторной геометрии, где эта терминология не так прижилась, но тем не менее это правильное
действительно название, а у нас звенит звонок. Так что, наверное, на этом всё.

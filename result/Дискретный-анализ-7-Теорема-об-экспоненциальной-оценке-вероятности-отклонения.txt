Редактор субтитров Е.Воинова Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
конкретный пример. Возьмите n равное 10 в шестой, чтобы было как-то более впечатляюще, так всем
рассказываю. Возьмите a равное 10, например, в четвертый, то есть он делает миллион шагов,
пьяница делает миллион шагов и хочет, надеется, удалиться от центра притяжения на расстояние
всего лишь 10 тысяч или больше. Ну то, что здесь стоит именно такой знак или такой, я надеюсь,
вы понимаете, без разницы. Больше, больше, либо равно это не имеет никакого значения. Значит,
вот он хочет за миллион шагов удалиться хотя бы на 10 тысяч вправо. Вероятность этого это 10 в шестой
поделить на 10 в восьмой, а на меньше одной сотой. То есть, если он 100 раз попробует сходить на
расстояние миллион, то скорее всего все равно ничего не получится. Ну, 100 раз делает миллион шагов,
100 миллионов шагов. Ну, такое нет, понимаете, но программа. Программа-то может делать много шагов,
а вот человек реальный, квадриллион километров во мраке. Но, понимаете, то есть это скорее даже
не то, что пить вредно, а то, что такое объяснение алкоголизма. Человек не может уйти далеко от
кабака. Ну, вредно, конечно, алкоголизм даже плохо. Ну вот, ладно. Значит, к чему я клоню? Клоню
я всегда к следующей теореме, которая забивает крышку, последний гвоздь в крышку гроба этого
несчастного пьянице. Да. Нет, но она же меньше. Ну, зачем делить пополам? Она меньше. Ну, то есть,
не то, что если формально писать, а если пытаться оценить лучше, то да, конечно, можно поделить пополам.
Да. Ну, тем лучше. Я же оцениваю это сверху. Но если пополам разделить, это же лучше будет. Ну,
то есть, мое неравенство правильное. Просто может быть завышенное. Но я вот сейчас хочу сформулировать
теорему, которая говорит, что оно не в два раза завышенное, а в дофигищий раз. Вот, извините меня
за грубое выражение. Значит, утверждается, что можно вот такую штуку доказать. Вероятность
не линейна или там квадратична, а экспоненциально мала. Вот смотрите на тот же самый пример. А квадрат
поделить на n, это перевернутая n поделить на а квадрат. То есть, это 100. Но вот здесь как раз
пополам делится это 100, получается 50. Но это, конечно, хорошо. Но согласитесь, что e в минус 50 и
гораздо меньше, чем 1 сотая. Это означает, что уже действительно хуже, чем квадриллион километров
пресловутой. У меня каждый год такое исследование про квадриллион километров. Это цитата, на самом
деле. Она кому-то заходит или нет? Кто-то понимает, откуда эта цитата про квадриллион километров
во мраке. Ладно. Раз все молчат, значит, никто не понимает. Ну ничего странного. Может, там кто-нибудь
понимает. Просто там, в смысле, вот там. За камерой. Не оператор, а те, кто будут смотреть потом. Нет,
нет. Хуже. Ну, узл. Не хуже, конечно. Больше. 20 тысяч льи, это всего 80 тысяч километров. А тут,
понимаете, квадриллион. Квадриллион это сколько? Это 10 в 15, да? Триллион это 10 в 12, квадриллион
10 в 15. Ну ладно, это все не важно. В общем, совершенно обалденная вероятность. Значит, дорогие друзья,
поскольку у вас действительно еще мало было теории вероятностей, дайте я чуть-чуть вам буквально
пару слов скажу по поводу того, с чем вот это связано. С тому функцию вы, наверное, узнаете,
да? Она похожа на что-то, да? Встречали вот такую функцию? Это нормальное распределение называется,
но если не встречали, вы ее встретите. Все равно обязательно. Это прям очень важная функция,
которая есть в теории вероятностей. Называется плотность нормального распределения. Там надо
еще на корень из 2p поделить, чтобы интеграл по всей прямой равнялся единицей. Тогда это действительно
будет плотность. Но если вы не знаете, что это, забейте. Я сейчас вот эту теорему докажу,
не ссылаясь ни на какие интегралы, ни на какие ваши знания из теории вероятностей. Я сейчас
докажу, честно докажу, и вам надо будет уметь воспроизводить. Это гениальное доказательство.
Ну а это вы потом меня вспомните. У вас будет предельная теорема, там какая-нибудь типа
центральная. Можете просто вот так где-то отложить у себя в памяти, что когда-то в теории вероятностей
будет центральная предельная теорема. Там будут вот эти интегралы. Но из нее, строго говоря,
вот это не будет следовать, потому что она предельная. Там какой-то предельный переход. А здесь
вот прям вот совершенно явное неравенство, верное для всех n и для всех а. Поехали гениальное
доказательство. Ну, нас интересует вот такая вероятность. Во-первых, давайте вот в этом
неравенстве слева и справа произведем умножение на одну и ту же положительную константу.
Ну, зачем-то. Константу обозначим лямбда. Лямбда больше нуля. Мы ее потом подберем. Я могу
сразу сказать, чему она в оптимальном случае равна, но мы ее подберем, чтобы вас порадовать.
Ну, можем так сделать. Это не самый гениальный ход. Самый гениальный ход — это как бы,
зная ответ, подогнать под него. То есть давайте вот в этом неравенстве возьмем от левой и правой
части экспоненту. И е возведем в степень левой части, правой части. Экспонента — функция
монотонная, правильно? Поэтому это тоже верное будет неравенство. Больше либо равняется е в
степени лямбда. Теперь применим банальное неравенство Маркова, от которого я уворачивался,
помните, довольно усиленно. Я сказал, что не хочу вот здесь применять неравенство Маркова,
хочу применить его именно в форме ЧБШ. А вот здесь применим прям неравенство Маркова. Видите,
вот это же положительное число, а это функция, которая принимает только положительные значения.
Поэтому можно применять неравенство Маркова. Я пишу нм неравенства Маркова, согласно которому
будет вот так. Мат ожидания экспоненты. Ну поделить на е в степени лямбда, ну давайте я напишу
умножить на е в степени минус лямбда. Поделить на е в степени лямбда, поделить на правую часть
мат ожидания левой части, поделить на правую часть. Это неравенство Маркова. Ну поделил вот так,
написал. Так, все успевают? Нормально? Так, смотрите, экспоненты от суммы, это произведение экспонент.
Сами величины независимы, поэтому экспоненты от них, очевидно, тоже независимы. А значит,
мат ожидания их произведения, это произведение их мат ожидания. Если вдруг вы не знаете такого
свойства, уверяю вас, оно точно будет, я его не буду доказывать. Еще раз, если случайные величины
независимы, то мат ожидания их произведения – это произведение их мат ожидания. Но тут важно,
чтобы они были независимы. Линейность верна всегда, мат ожидания суммы – это всегда сумма.
мат ожидания. А вот мат ожидания произведения равно произведению мат
ожидания только в случае, если величины независимы. Так, ну пишу. Мат ожидания
е в степени лямбда х1 и так далее. Мат ожидания е в степени лямбда хn.
Ну и вот этот прилипало, который никуда не девается, е в степени минус лямбда. Но это
хорошо. Нам надо оценить сверху. Отрицательные экспоненты нам явно в
этом деле только помощницы. Наша цель оценить вероятность сверху. У нас уже есть
отрицательные экспоненты. Как здорово. Так, давайте я пойду справа налево.
О-хо-хо.
Ну, беру любое из этих мат ожиданий. Поскольку иксы-то и все одинаково распределены,
они все принимают вот два значения, с вероятностью одна вторая, то какая разница,
какое брать. Они все одинаковые. С чему они равны?
Е в степени лямбда плюс е в степени минус лямбда пополам. Ну, это даже называется
чосинус там какой-то. Косинус гиперболический. Вот, но я лично не помню
формулу Тейлора для косинуса гиперболического. Может, вы помните?
Ну, я лично помню вообще только для экспонента. Вы меня извините. Я вот помню
для экспонента, а дальше я могу через экспонента что угодно записать. Поэтому,
с вашего позволения, я традиционно для себя просто напишу ряд Тейлора для
каждой из экспонента, а потом их сложу. Одну-вторую я пока вынесу за скобку. Тут
будет сумма пока от нуля до бесконечности лямбда вкатой поделить на к факториал.
Плюс сумма пока от нуля до бесконечности минус лямбда вкатой поделить на к
факториал. Так, ну вот, я легко соображаю, что если k нечетная, но одно и то же в
двух суммах и при этом нечетная, то они друг друга кокают. А если k четная, то они
наоборот складываются, потом делятся пополам и дают то, что было в исходном.
Так, я не очень быстро говорю. Нечетные аннигилируют, а четные складывают, потом
делятся пополам. То есть у меня получается вот так. Сумма пока от нуля до
бесконечности лямбда в степени 2k поделить на 2k факториал. Я надеюсь, что мне
не надо делать замену переменных. Ну, k и k, какая разница? Только четкие выжили, но с коэффициентом 1,
потому что двое здесь поделились пополам. Так, теперь я замечаю глупость, но она... Как глупость?
Она очень простая, конечно, но сильно лучше и не сделать. k факториал на 2 в степени k.
Ну, это очевидно. Ну, вроде смешно, а при k равном нулю это точное равенство. То есть улучшить нельзя,
например. Поэтому при k равном единице, кстати, тоже. Поэтому в каком-то смысле это не
улучшаемая история. Значит, у меня получается меньше либо равно сумма. Ну, а дальше они уже
маленькие становятся. Сумма пока от нуля до бесконечности лямбда в степени 2k поделить
на 2k факториал и на 2 в степени k. А теперь я замечаю, что это опять ряд Тейлора. Ой, на k,
конечно. Да, извините. Конечно, на k, иначе это не будет ряд Тейла. На k факториал. Да-да-да,
вот я подставляю вот это неравенство. Потому что вот так. О, называется это орешек арахис.
Вот видите, какой арахис. Ладно, шутка глупая. Но он все время у меня рисуется, как такой арахис.
Не, ну короче говоря, видите, тут лямбда квадрат пополам и все вкатый. Я специально вот этот
лямбда квадрат пополам в этот арахис обвел. Лямбда квадрат пополам вкатый и поделить на k
факториал. Что, плохо видно? Лямбда квадрат пополам и все это вкатый. То есть это снова ряд для
экспонента только лямбда квадрат пополам. Вот так. А теперь я здесь продолжаю. У меня получается вот
что. У меня получается е в степени лямбда квадрат н пополам. Что? Да-да-да, вот мне уже сказали,
я зачеркнул, но зачеркнул плохо. Вы не увидели. Ну надо было просто стереть, да, но это долго стирать,
надо скобки тогда стирать. Ну как-то, в общем, я решил. Ну, впрочем, и сейчас их надо как бы стирать,
поскольку я двойку зачеркнул. Можно не стирать, да, в скобках k, но мне скажут, что в скобках k это
наверно какое-то произведение. Ладно, друзья, я шучу, но понятно. k факториал, правильно. Именно
поэтому получается такая экспонента. Вы видели, да, что там е в степени лямбда квадрат пополам?
Видели? Но это вот n таких одинаковых экспонент получается. Ну то есть надо умножить на n в
показатели, потому что они перемножаются, в показатели складываются, ну получается n вот раз,
один и тот же показатель. И минус лямбда а, товарищи. Ну я думаю, что уже многие поняли,
что произошло. У меня в показателе экспоненты находится квадратный, ну, трехчлен, двучлен,
я уже не знаю. Парабола, короче говоря. Парабола усами вверх. У нее есть точка минимума по лямбда.
Помните, я сказал, что я в принципе могу сказать, чему равняется лямбда, но мы его подберем. Нам же
нужна оценка сверху. Мы подберем лямбда таким образом, чтобы она была самой лучшей. Чтобы вот
это выражение было минимально, то есть чтобы парабола минимизировалась. Ну в какой точке
минимизируется? Ну вы понимаете, там производная равна нулю, да? Ну, со школы еще многие изобрят
минус b поделить на 2a. Тут минус b это a, а 2a это n. Ну то есть надо взять лямбда равная a поделить на n.
Это точка минимума, где производная ноль. И подставить сюда будет a квадрат поделить на 2n
квадрат и умножить на n. А тут будет a квадрат поделить на n с минусом. То есть тут на 2n,
а с минусом просто на n. Ну вот и будет вот это. Прямо в точность.
А? В каком-то смысле это, знаете, как доказательство из книги. То есть,
пойди додумайся до такого. Три строчки, по сути, короткие. Никаких вам там центральных
предельных теорем ничего. И эта оценка, по сути, не улучшаемая. То есть вот лучше сделать нельзя.
Ну там, точностью, да, мизер. Принципиально это не улучшаемая оценка. Вот такое вот. Называется
неравенство большого уклонения. Друзья, вы только осознайте, значит, мы сожрали с потрохами неравенство
Чебышова, но только в этом специальном случае. То есть важно, что суммируются независимые величины,
которые еще там какие-то очень специфические значения принимают плюс-минус один с вероятностью
одна-вторая. Мы на этом очень существенно сыграли. Конечно, я не утверждаю, что неравенство Чебышова
всегда можно вот так вот улучшить. То есть здесь неравенство Чебышова стер, да, давало что-то
типа n поделить на а в квадрате, а тут вот такая вот экспонентность перевернутая величиной в показатель.
Я не говорю, что в любом случае можно сделать такое улучшение, но вот для такого набора
случайных величин можно. И в этом некий пафос, о котором мы еще будем говорить на протяжении
этого курса. Так, понятно я сказал? Так, теперь для вот этой теоремы нужно некое обобщение,
которое похоже придется давать после перерыва. Давайте сейчас перерыв тогда 6-5 минут. Так, ну давайте
я обобщу. Все, я продолжаю. Давайте я обобщу немножко или ну как обобщу, ну просто перенесу на некий
другой случай вот этот результат. Наверное без доказательства, потому что там оно немножко
более техничное, но в общем суть та же самая. Так, вот такая вот случайная величина немножко
отличающаяся на самом деле от пришественницы. Сейчас скажу какая. Вот такая вот случайная
величина, на самом деле вот она и называется биномиальная. Называется биномиальная
случайная величина с параметрами n и p. Ну понятно, что такое параметры, а смысл очень простой. Вместо
нет, можно говорить про пьяницу, конечно. В каком смысле можно говорить и про пьяницу,
которая по наклонной двигается, которая на горе. Да, но тут видите я немножко по-другому, 0 и 1,
но можно из этого что-нибудь по вычитать. Нам все равно сейчас вычитать придется. Среднее у него
не в кабаке, понимаете. Вот если вот такую величину брать, то среднее в точке n умножить на p,
правильно? Среднее в точке n умножить на p. Вот, ну можно вычесть это n умноженное на p,
тогда будет пьяница как раз со среднем в кабаке. То есть вот если из это n-ого вычесть np,
то это фактически пьяница, которая двигается по наклону. Давай. np это мат ожидания, это n.
А мат ожидания вот такой величины это 0, то есть мы возвращаемся к истории про пьяницу. Но сама
исходная величина называется биномиальная, я просто расскажу как бы заново то, что вам и так расскажут,
конечно, но вот применительно к нашей ситуации. Обозначается это вот так, достаточно традиционно,
ну может вам будут писать просто b от np или bin от np, но я пишу bin, чтобы было прям видно,
так друзья, почему она так называется? Какие значения принимает такая величина на каком-то
там своем омега? Какие она значения принимает? Ну понятно 0, 1 и так далее n. Это все значения,
какие она может принимать. Ну вот, чтобы это был пьяница, нужно писать 1-p, а тут
minus p. Вот здесь писать 1-p, а тут minus p. Тогда в сумме как раз получится вот то, что здесь написано.
Ну не важно, в общем, вот есть такая случайная величина. Она принимает значения от 0 до
какой вероятностью она принимает свое конкретное значение k? Ну надо, чтобы k раз выпало 1,
cn-k раз выпало 0. Ну конечно, да, c и cn пока на p вкатый на q в n-катый. Я просто поясняю,
почему она называется биномиальной. Binom это сумма всех вероятностей и ее значений. Ну binom
p-q в n-й степени равны единице. Что конечно и должно быть, но должна сумма вероятностей равняться
единице. Это понятно. Ну вот. Значит, аналог теоремы, какую мы там доказали, это утверждение о том,
что вероятность, с которой это n-np больше либо равняется а, не превосходит е в степени
минус 2а квадратная. Двойка прыгнула из знаменателя в числитель. Вот эта вот двойка,
тут она в знаменателе, а тут она в числитель. Ну, не зависит. От p не зависит.
Что будет? Чем плохо p1? Пока мне не очевидно. Ну, в общем, это можно доказать, но это довольно
такая рутинная деятельность. Ну, я бы оставил это без доказательства, это просто аналог вот
этого. Чуть более сложными аналитическими выкладками. То есть если тут красиво очень,
прям вот вся идея видна на трех строчках, то там надо повозиться подольше, там строчек шесть.
Надо еще какие-то мерзкие аналитические неравенства писать. На поля не уместятся,
а мне кажется, ну что я вас буду перегружать техникой? Смысл-то уже понятен.
Похожая величина тоже уклоняется от среднего с очень маленькой вероятностью. Такой вот хороший
результат. Так, ну все, я готов, товарищи, доказывать вот эту теорию, наконец-то.
Ну, я надеюсь, вы тоже готовы воспринимать. Никого не кокнул, там биномбат, все понятно. Ничего
сложного. Так, сейчас нам надо кое-что придумать. Доказательство пошло вот той теории.
Сейчас мы кое-что придумаем. Вот пусть нам дан какой-то конкретный граф G.
Ну, у нас множество вершин, это будет, конечно, набор чисел от единицы до N,
что мы работаем с обычным случайным графом, наверное, на вершинах. Ну, давайте запустим
на этом графе некий процесс. Я очень стараюсь, во-первых, сделать так, чтобы не запутаться,
потому что я не повторил, но я вроде помню. А во-вторых, чтобы вас не кокнуть. Ну вот,
но я вроде помню. Рассуждение, оно не очень сложное. Оно очень красивое, на самом деле. Надо
некий процесс запустить на графе, и тогда все-все получится. Ну, давайте я нарисую какой-нибудь
конкретный пример графа, неважно. Вот есть какой-то граф. Заметьте, я его нарисовал несвязанным,
потому что у нас, как правило, несвязанные графы будут получаться в рамках нашего пункта. Поэтому
я для примера нарисовал несвязанный граф. Теперь я беру какую-то его вершину в качестве стартовой.
Вот какая-то его вершина. Я в каком-то смысле запускаю такой процесс рождения и гибели. То есть
я интерпретирую эту вершину как прародительницу некоего рода. Я понятно говорю, да? Ну, наверное,
записывать невозможно, но вот это не надо записывать, потому что если вы все это будете
записывать в тетрадку, ну у вас трактат получится. Сейчас вы увидите, я совершенно формально опишу
этот процесс. Вам главное понять, чего происходит, ну и мне тоже не запутаться. Вот есть прародительница
рода. Она интерпретируется как единственная живая в начале всех времен. Вот давайте в каждый
момент времени через у с индексом t, t это дискретное время, обозначать число живых вершин.
Число живых вершин. Ну, то есть вот y0 равно единице, потому что у нас в начале всех времен есть вот
эта вершина v, и она единственная живая. Все остальные вершины мы называем нейтральными.
Пока что. Вот когда у нас есть только одна живая вершина, все остальные, давайте,
число нейтральных вершин обозначать n с индексом t. Число нейтральных вершин в момент времени t,
ну то есть n1, ну или n0. Давайте, пусть будет nt плюс один, что ли. Мне почему-то так хочется.
n1 равняется единице. Тут время стартуется единице, тут время стартуется нуля. Н1, ну или с нуля
все стартовать. Ладно, попробуем с нуля, вот не запутаюсь. Н0, ой, только не единица, а n-1, конечно.
Да, вы, наверное, на это смотрите. Нет, тогда я n1 напишу. Все, хочу n1. Н0, ну ладно. Ладно,
время одинаковое. Все, все, давайте время одинаковое. Н0 равняется n-1, а вот все получится. Давайте,
н0 равняется n-1. Ну или я nt плюс один напишу, тогда все будет хорошо. Если я тут nt плюс один
напишу, тогда все будет хорошо. t равно нулю будет n1. Ладно, все, все, пусть будет так. Не хочу никого путать, пусть
будет так. Значит, y равно 1, n0 равно n-1. Вот, теперь вот эта вот единственная живая вершина порождает
потомков. Ну, товарищи, я думаю, совершенно понятно, кого она порождает. Она порождает
множество своих соседей. То есть потомки этой вершины – это те вершины, которые соединены с
ней ребрами. Так, вот я прямо на этом рисунке изображаю. Она порождает потомков и умирает.
Да, и умирает. Теперь смотрите, у нас у2. Так, внимание, товарищи, чтобы было просто все понятно.
у2 равняется 2. Ой, у2, у1. Так, друзья, нам бы, конечно, успеть времени не так много,
не хотелось бы переносить на следующую лекцию завершения доказательств. Давайте
сосредоточимся. Согласны, да, живых – это вот ее потомки. Она померла. Да, да, да, да, да. То есть,
смотрите, у меня пока граф не случайный. Я нарисовал совершенно конкретный граф, чтобы было абсолютно
понятно, как устроен процесс на конкретном графе. Нет, воскресать она не умеет, к сожалению. Все, вот
она умерла и умерла. Значит, теперь у1 у нас равняется 2, а n1 чему равняется? 1, конечно. Дальше мы
совершенно наугад, без разницы, обзываем, например, вот эту вершину живой. Вернее, не обзываем, вот они
две живые. Берем вот эту вершину и смотрим на ее потомство. Не есть потомство? Нету, потому что
потомство выбирается, внимание, товарищи, только из нейтральных всегда. Потомство выбирается из
нейтральных. Это живая, поэтому никак. Ну, да, виноват. n1 равно 4. Согласен, да, да, да. Забыли мы про
эту компоненту, но мы-то ее видим, а компьютер не видит. То есть, вот компьютер на вход поступил
и он действительно не понимает. Это отдельная компонента, не отдельная. Пока просто 4. Все правильно,
извините. Конечно, 4. Тем не менее, если мы выбрали вот эту вершину как продолжательницу рода, то она
умирает, к сожалению, ничего не породив. Зря появилась на свет. Так, идем дальше, друзья. Кто у
нас остался в живых? Вот эта подруга. Она порождает. Ну, можно я уже Y писать не буду? Дальше все понятно.
Она порождает, помирает, остается живая. Да, у нее там нейтральных 3, но фиг она до них дотянется,
поэтому она тоже помирает. И в этот момент Y с каким-то индексом становится равным нулю. Ну,
понятно, с каким индексом. Если Y0 равно 1, там Y1 равно 2, Y2 равно тоже 2, Y3 равно 1.
Y4 равно 0. Все. Род выродился. В момент времени 4. Что такое, товарищи, 4? Это количество вершин
вот в этой компоненте связности. Да, одну из живых, любую совершенно, неважно в каком порядке их
выбирать, в конце концов процесс остановится, и остановится он в тот момент времени, когда мы
наберем вот ровно эту компоненту связности, в которую входила исходная вершина. Так, друзья,
но мне казалось, что вот на этом примере очень хорошо. Не на этом конкретном, я его сейчас выдумал
из головы, но если вы берете какой-то конкретный пример, то совершенно понятно, как идет процесс.
Теперь мы можем считать, друзья, внимание, вот самый главный момент, что граф случайен, и этот
процесс тоже состоит не из конкретных чисел, а из случайных величин. Вот эти все Y, N, это
случайные величины. Конечно, зависимые, да, да, но вот мы сейчас разберемся, на самом деле все очень
неплохо. То, что зависимые, безусловно. Значит, давайте Y с индексом T. Во-первых, можно представить
вот так. Это Y с индексом T минус 1. Это сколько было живых на предыдущий момент времени. Может,
придется переносить. Посмотрим. Так, что надо прибавить? Ну, надо прибавить число потомков вершины,
которую мы выбрали как живую. То есть число ее соседей среди нейтральных. Давайте вот это
число обозначим просто Z с индексом T. То есть это число соседей очередной живой вершины среди
нейтральных на этот момент времени. Можно я не буду писать, а просто скажу. Так, ну и надо
вычислить единицу, потому что умерла вот эта вот вершина, которая столько вершин породила.
Вообще, в науке в теории вероятности в случайных процессах называется ветвящимся процессом,
когда каждая очередная частица что-то порождает, сама, допустим, умирает, и дальше идет вот этот
процесс размножения. Такой ветвящийся процесс размножения. Так, что еще мы можем сказать? Как
ZT, ну скажем, так распределена. Вот, смотрите, вот, допустим, у нас была в начале одна вершина.
Выражение случайная величина имеет распределение. Вас не смущает? Ну, случайная
величина имеет распределение, это значит, мы знаем с какими вероятностями она принимает
свои значения. Вот, например, она имеет биномиальное распределение, если вероятности всех ее значений
вот такие. То есть, если значения это целые числа от нуля до n, и их вероятности выражают с какой-то
такой формы. Вот, может, конечно, да, но я утверждаю, что ZT тоже биномиальный.
Но с каким параметром? С какими параметрами? Я же не зря старался, бином, писал вам, товарищи.
Сейчас он появится во всей красе. Я утверждаю, что она имеет биномиальное распределение.
С какими параметрами? Ну, давайте посмотрим, Z1, она на ком распределена? Она распределена на n0,
то есть, тут надо писать тогда nt-1 и p. Но, смотрите, вот, если здесь единичка, то тут n0,
которая равно n-1. Ну, почему я так пишу? Потому что вот у меня одна живая вершина в случайном
графе, в случайном, не вот в этом конкретном, а в случайном графе. Она присоединяется к каждой из
оставшихся n-1 вершин с вероятностью p и не присоединяется с вероятностью q. То есть, мы
n-1 раз бросаем вот такую монетку. 1 проводим ребро, 0 не проводим ребро.
Друзья, давайте вот это осознаем, понятно или не очень? Ну, конечно, это при условии,
что мы знаем, чему равняется nt-1, то есть, конечно, это зависимые величины. Если нам nt-1 до
no уже, на каком-то графе оно реализовалось, то zt имеет биномиальное распределение вот с такими
параметрами. Очередное число потомков, мы этих потомков выбираем только из нейтральных вершин,
которые были на предыдущем шаге. И каждое из них мы выбираем с вероятностью p, потому что граф
случайный, не конкретный, а случайный. Каждое ребро в нем возникает с вероятностью p. Сейчас,
друзья, вот это точно понятно? Умел я как-то объяснить? Хорошо, теперь давайте еще одну рекурсию
напишем последнюю. Значит, как выражается n с индексом t? Ну, наверное, надо что сделать? Надо
из общего числа вершин вычесть количество живых в тот же самый момент времени и вычесть еще что-то.
Что еще надо вычесть? Количество мертвых, правильно. Ну, то есть t. Например, в нулевой
момент времени это n-1, n-y0, ну и минус 0. Первый момент времени это n-y1-1, вроде,
правильно. Вот, я думаю, что этого нам хватит. Черт, наверное, все-таки не успеем. Ну, ничего страшного,
значит, закончим в следующий раз. Главное, чтобы вы все осознали, потому что, видите, вы немножко
не знаете еще вот этих всех вещей, связанных со распределением, мне приходится немножко напоминать.
Ну, ничего страшного, тут все просто. Так, значит, ну давайте леммой, что ли, это обзовем.
Я утверждаю, что yt имеет вот такое замечательное распределение. О, я придумал, как сделать так,
чтобы успеть хотя бы такую содержательную часть, сейчас увидите, катартическую, катарсисную.
Катартическую это как-то неоднозначно звучит.
Во, во, во. Ну, лучше не тильду, наверное, рисовать, лучше написать вот так, чтобы понять не было.
Имеется в виду, что yt вкладывается как сумма фиксированной части, ну, конечно, от t зависящей,
но при данном t фиксированной, и случайной. А вот эта случайная часть имеет вот такое
биномиальное распределение. То есть n-1 раз бросаем монетку, и вот такая хитрая вероятность успеха.
Само утверждение чисто формально понятно. Я утверждаю, что число живых вершин в момент времени t,
это случайная величина, конечно, она зависит от графа, это случайное число, но его можно получить
так. Взять вот это вот фиксированное число и к нему прибавить число раз, когда вот эта
монетка выпадает единицей кверху. Монетка, это вот мы бросаем монетку, либо она падает решкой,
с вероятностью p добавляется 1, либо она падает орлом, с вероятностью q добавляется 0.
Ну, наверное, вы знаете эту интерпретацию с монетками, да? Монетку бросаем n раз.
Если в очередном бросании решка, добавляем единичку. Нет? Ну, не добавляем ничего.
Поэтому я говорю вот в терминах монеток. Это просто утверждение, мы его докажем,
но похоже, что мы его докажем в следующий раз. Ну, просто потому что, если я сейчас в это закопаюсь,
вы катарсис сегодня не испытаете и в следующий раз вы уже вообще забудете. Поэтому я предлагаю
пока отложить доказательства этого факта и воспользоваться ими. Хорошо? Сейчас мы применим
эту лимму для завершения доказательства теории. Итак, смотрите, вероятность того, что y,
t больше нуля, ну, то есть процесс еще не прервался. Так, помните, что когда y,
t обратится в ноль, это мы исчерпали всю компоненту для вершины, с которой стартовали. Хорошо, да? Да,
друзья, заметьте, вершину мы зафиксировали. Когда мы вот в общем случае вот это все писали,
мы зафиксировали какую-то вершину, например, один. Давайте считать, что мы зафиксировали один.
Дальше пошел вот этот процесс. Вероятность того, что y, t больше нуля, это, конечно, вероятность того,
что binom от этих параметров, да что ж такое, n минус 1, 1 минус больше, чем t минус 1. Ну,
перенес сразу вправо вот эту 1 минус t. Понимаете? Тут вроде все просто. Если поверить в справедливость
леммы, то вот так. Ну, лемму еще надо будет доказать, но это в следующий раз. Значит,
дайте я не буду переписывать, сварить какая ловкость рук. Ну, было вот так, а стало вот так. Ну,
это правда, потому что t это целое число. Быть строго больше, чем t минус 1, это то же самое,
чтобы быть больше либо равном t. Ну, тут такая ловкость рук невеликая. Но это, в общем,
ладно, это не очень интересно. Теперь смотрите. 1 минус 1 минус p в степени t меньше либо равняется
pt, кажись. Ну, 1 минус p в степени t больше либо равняется 1 минус pt. Знаем такое неравенство?
Ну, bernoulli, наверное, оно называется, но вроде бы это ясно просто. Ну, из binomo сразу следует.
Вот. Меньше либо равняется pt. Ну, а n минус 1 меньше, чем n. Это как-то совсем очевидно,
вот то, что n минус 1 меньше, чем n. Я утверждаю, что из этого, конечно, следует следующее неравенство.
Давайте вот так сотру. Вот так. Меньше либо равно, интересующая нас вероятность, не больше,
если я такую врезку сделаю, просто пояснение, не больше, чем вероятность того, что binom от n
pt больше либо равняется t. Но я заменил число бросаний монетки на большее и вероятность заменил
на большую величину вероятность единицы. Но, разумеется, суммарное число единиц большое в
этом случае с большей вероятностью, чем в этом. Если я и бросаний увеличил количество, и вероятность
успеха в каждом бросании тоже увеличил вероятность единички. Согласны? Но я могу строго меньше даже
написать. Тоже будет правда, но неважно. Так, друзья, слушайте, мы, по-моему, пришли, наконец,
вот к этой теории. Смотрите, это с индексом n. Это вот она прямо, только тут p, а не ppt. Вот тут
у нас p. Где? Где? Вот тут p. Вот тут p. А тут у нас вместо p, p умножить на t. Ну какая разница? Сейчас
применим. Ну, надо переписать. Надо переписать вот так. Значит binom от npt минус npt больше либо
равняется t минус npt. Я просто лево и справа вычил npt, чтобы подогнать под ту теорему,
которая вот там нарисована. Видите, да, друзья? Видно. Теперь давайте вот на это посмотрим. У нас
пет какое. Вон там оно. C поделить на n. Сейчас катарсис будет. Значит, что такое npt? Вот давайте
я так подчеркну. Что такое npt? Это n. C делить на n на t. Это ct, правильно? А c у нас, туда,
видите, да, меньше единицы. То есть вот так число t минус ct, оно больше нуля. Вот можно применить то,
что написано в теореме. Что это не превосходит e в какой степени минус, а не влезет сейчас.
Ну давайте вот сюда. Не превосходит e в степени минус. Так, 2a квадрат. Это t минус ct квадрате
поделить на n. Что такое n? Плохо. Сейчас. Я понял, где я ошибся. Я хотел вот в эту теорему подставить,
черт, я вот здесь наврал. Я понял. Наврал. Вот вы правильно меня спросили, а я все-таки
наврал. Я так уперся, говорю, что от p не зависит, а от p зависит. Ах ты Господи. Так, сейчас, или это неважно.
Сейчас. Значит, секунду. А, так это все равно. А, все-все-все, да, это правильно. Ну ладно,
тогда может ничего страшного. Не, правильная теорема. Уперся-то нормально. Значит, смотрите,
давайте я чуть прокомментирую, особенно для тех, кто там в записи будет слушать. Я сначала подумал,
что все-таки здесь стоит сделать зависимость справа от p. Содержательная зависимость должна
быть устроена чисто интуитивно. Это понятно так. Вот здесь вместо n написать дисперсию вот этой
вот величины. Ну дисперсия это n, это известно, она равна npq. Я думаю, ну дай я отсюда запишу
npq. Ну можно, конечно, написать npq, но дело в том, что npq, очевидно, меньше, чем n, дробь больше,
чем если я сюда подставлю n, а со знаком минус, ну получается как раз то, что я здесь написал.
Причем p, умноженное на q еще можно заменить на одну четверть. Вот у вас тут будет четверка,
поделенная пополам, как в той теореме, которую мы доказали, и будет как раз 2a2n. Я заодно вспомнил,
как доказывать эту теорему, потому что ну я как бы не собирался вам ее доказывать и не вспоминал,
как ее доказывать. А доказывать ее по сути надо именно так. То есть вот здесь будет e в степени
минус a2 на npq и на 2. Ну а поскольку p и q у нас одна-вторая в худшем случае, то будет как раз
два квадрата подережная. Короче, это правильный результат. Короче, это правильный результат,
ничего я тут вам не наврал, это я просто зарапортовался немножко. А оно вот здесь важно,
на самом деле, потому что больше чего-то вот, ну слушайте, давайте вот это надо написать для
любого а больше нуля, конечно. Это иначе просто очевидно неверно, посмотрите, потому что если вы
возьмете отрицательное какое-то а, то это у вас получается просто некая величина лежит на правой
половине прямой. Это вероятность высокая. Надо уйти вправо от среднего, ну там больше либо равняется,
наверное, ну да, да, да, да, да, да, да, да, да, да, да, да, то есть вероятность вот это превратится
просто в единицу, если тут брать отрицательные числа, а в конце концов превратится в единицу. Ну
слушайте, ну уйти от, вот если мы находимся не в этой ситуации, а в простейшей, когда кабак в нуле,
уйти вправо на расстояние отрицательное, ну вообще невозможно, а наоборот оказаться больше,
чем минус один, ну это вероятность один. В этом смысл, понимаете, вот уйти от кабака так,
чтобы оказаться правее точки минус один, вот поэтому тут важно, конечно, чтобы она была больше
нуля, вот здесь мы используем очень по существу 100с меньше единицы, ну я могу здесь написать на n,
давайте сообразим тогда, что у меня получится, значит, неужели я все равно не успею, ну что ж такое,
очень долго рассуждаю, так, ой, черт возьми, все-таки мне хочется как-то это, почему,
что ж такое-то, я хочу, почему-то мне все время хочется вот здесь это npq написать,
вот в этом месте все-таки я запутался, черт возьми, npq хочется написать, чтобы катарсис-то получился,
времени осталось мало, черт, я не знаю, что ж такое, то есть я хотел, чтобы здесь получилась величина
порядка t, тогда вот это t квадрат с t сократится до t, и это будет то, что мне нужно, я вот что-то
сейчас не соображу, почему я могу n заменить на t, или это очевидно, подождите, наверное очевидно,
подождите, подождите, а? чего хорошего-то, не, ну подождите, n больше чем t это понятно,
но знак неравенства будет не в ту сторону, что-что, что такое сейчас, ай-яй-яй-яй-яй-яй,
ой-яй-яй, как нехорошо, ой, что ж ты будешь делать, ай-яй-яй, почти все получилось-то, так, смотрите,
вот еще раз, я-то чего хочу сказать, что это вот такая вот величина на два npq, ну, конечно,
npq не превосходит, ой, n поделить на 4, поэтому тут верно, что отсюда следует вот это, да,
поскольку npq не превосходит n поделить на 4, p у нас, в худшем случае, одна вторая, то, конечно,
вот это следует, применить я все-таки хочу вот это, знаете почему, потому что p у меня сейчас это
не константа, как одна вторая, а c поделить на n, и это, конечно, мне очень сильно сейчас улучшит жизнь,
понимаете, одно дело грубо оценить p, умноженное на q, как 1 четверть сверху и получить вот это
общее неравенство, которое я пытался применить, и совсем другое, при нашем конкретном p равном
c поделить на n, применить вот это неравенство, оно гораздо сильнее, чем то, что сверху, следствие
это правильное, я его держал в голове, вот что значит не повторить, готовьтесь к лекциям, товарищи,
и к экзамену, вот, понимаете, это правильное утверждение, но мне нужно вот это, оно тоже правильное,
вот мне нужно вот это, к сожалению, для меня, что я затянул, вот, поэтому я здесь пишу не n,
а все-таки 2pq, что такое pq, 2npq, значит, np это c,
да что такое-то, а, все-все-все, вот у меня p, это же pt, да-да-да, простите, пожалуйста, да,
вот pt, конечно, вероятность успеха-то это p умножить на t, то есть это опять вот это npt,
вот это npq, которое здесь, ой, позорище, это 2npt, это np я написал, и еще умноженное на q,
ну, то есть на 1-pt, на 1-pt, вот так вот, это все в знаменателе, тут вот, да, вот эту двойку,
вот эту двойку, конечно, надо убрать, да, двойка теперь она в знаменателе, то есть вот это np,
потому что p у меня, p умножить на t, а q это 1-pt, вот это вот n2npq, вот так вот, npq, ну, я надеюсь,
что сейчас я выправился, ну, в том смысле, что я не запутал окончательно, нормально,
понятно, что происходит, да, ой, какое счастье, слушайте, я еще кое-чем похваст, то есть 1-pt можно
убрать, знаете почему, потому что он меньше единицы, в дроби он больше, а со знаком минус снова меньше,
поэтому я его зачеркну, он мне не принесет никакой пользы, вот, короче говоря, у меня получается вот
так, ну, чуть-чуть на пару минут задержу, извините, сейчас доведем до результата, значит, у меня
получается вот так, равняется е, в какой степени, вот смотрите сюда, тут t квадрат, тут, так, npt это
ct, вот npt это ct, вот так, npt это ct, в нашем специальном случае, поэтому мы t квадрат делим на t,
у нас получается минус t, и это минус t, на что умножается, на 1 минус c в квадрате, на 1 минус
c в квадрате, и делится просто пополам, да, и на c, да, на 2c, а теперь смотрите, берем t в виде бета на
логориф men, ну, бета можете считать не констант, а там какая-то функция, так, чтобы это получилось
целое число, или рисуете тут целую часть, это уже не важно, вот берете t в виде бета на логориф men,
можно же так подобрать бета, чтобы это было меньше, чем 1 поделить на n в квадрате, ну как, тут е в
степени минус логориф men, тут какая-то конкретная константа, она еще умножается на бета, просто
компенсируем эту константу так, чтобы получилось 1 на n в квадрате, очевидно можно, да, добрать
положительное бета, или не очевидно, но тут логориф men, е в степени минус логориф men, это 1n,
но возводим ее так, чтобы получилось 1 на n в квадрате, так, теперь смотрите, это вероятность того,
что стартуя, внимание, вот сейчас будет катарс, вероятность того, что стартуя с конкретной вершины,
вы остановитесь позже, чем в момент времени бета логориф men, правильно, ну то есть, что в этот
момент времени yt все еще будет больше нуля, все еще будет живая популяция, что момент остановки будет
дальше, чем вот эта величина, вот вероятность настолько маленькая, теперь вероятность того,
что существует вершина, существует вершина такая, что вот это вот выполнено, вот это вот yt,
уй, yt больше нуля, вот, что существует такая вершина, но она меньше либо равна n поделить на n в
квадрате, потому что вершина всего n, это объединение событий, нет, зависимые, зависимые,
конечно, но это объединение, вероятность объединения не больше, чем сумма вероятностей,
нет, нет, мы вообще запускаем независимо от каждой вершины, мы берем объединение просто событий,
смотрите, мы берем одну вершину и смотрим множество графов, для которых стартуя с этой вершины,
мы остановимся позже вот этого момента, потом все забываем, берем другую вершину,
может быть даже из той же компоненты и снова все запускаем заново, то есть мы сильно мажорируем
интересующую нас вероятность, мы на это не смотрим вообще, мы отдельно смотрим,
что будет если запустить процесс отсюда, мы вообще про все забываем, еще раз, что такое событие,
это множество графов, вот множество графов, для которых стартуя отсюда, мы остановимся
позже, чем вот в этот момент времени, оно имеет вероятность меньше, чем один на n в квадрате,
точно также множество графов, для которых стартуя от двойки и делая все то же самое,
опять остановимся позже, чем в этот момент времени, оно естественно снова имеет вероятность
меньше, чем один на n в квадрате, ну какая разница с какой вершиной начать,
Вот, но я и говорю, что вероятность того, что тортуя хотя бы с одной вершины мы
остановимся поздно, она маленькая, она меньше, чем n поделитель на n в квадрате, она стремится к нулю.
Ну а значит, с вероятностью стремящейся к единице нет ни одной компоненты размера больше, чем
battle-agorifmen. Всё. Вот это, что существует компонент размера больше, и эта вероятность стремится к нулю.
Так, друзья, я вот в ту сторону не смотрю, понятно или не очень?
Ну я готов ещё раз пояснить, да. Всё. Слушайте, я на 5 минут задержал,
то свинство с моей стороны, прошу прощения.

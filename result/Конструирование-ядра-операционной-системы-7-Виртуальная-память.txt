Я еще раз поздравляю всех с завершением первого блока, и сегодня мы начинаем
новый этап нашего курса, то есть этап, где мы погрузимся в виртуальную память.
План на сегодня у нас следующий. Перед тем, как мы погрузимся в саму виртуальную
память и посмотрим, как она работает, мы все-таки вспомним, как мы эту виртуальную
память можем настроить в операционной системе. Для этого поймем, что в общем-то
виртуальная память это не единственный способ преобразования. То есть мы уже
помним про сегментную адресацию, про которую мы разбирали на второй
лабораторной работе. Соответственно, еще вспомним про то, как работает Memory
Controller. После этого мы поговорим про ММУ на x86, то есть посмотрим на
реализацию таблиц и страниц, и немножко разберем, как, собственно, этот ММУ можно
перенастраивать в операционной системе. Перед тем, как мы к этому перейдем, у меня
такой простой вопрос. У вас на компьютере установлено 4 гигабайта оперативки, ну
допустим. Соответственно, когда работает операционная система, она использует,
например, там адрес такой, как, например, 4 гигабайта, там плюс 100 мегабайт. Там
действительно оперативная память операционной системы не падает. Как такое
возможно? Нет, виртуальная память еще не используется. Тоже еще не используется, 1 к 1.
Нет, адресная шина 48-битная, это обычная x86-64. Виртуальная память используется, но в режиме 1 к 1.
Нет, все пишется. Нет, это обычная оперативка. Нет, никакого пэйчфолдта, просто пишем. Нет,
плашка честная, 4 гигабайта. Нет, эта память не имеет, скажем,
еще одного адреса ниже, чем вот конкретно этот. То есть, грубо говоря, если перебирать адреса от нуля до...
Нет, кэши выключены. Нет. Ну ладно, на самом деле все очень просто и разгадка кроется в следующем.
То есть, у вас... Да. Ну, не так важно, на самом деле. Ну, в данном случае просто это x86, и на x86 это относительно
актуально, но в принципе это не обязательно может быть x86. Дело в том, что контроллер оперативной
памяти, с которым вы работаете, видит оперативную память одним способом, а вы, как код, исполняющийся на
центральном процессоре, видите оперативную память другим образом. Соответственно, если у вас, допустим,
есть условно DDR-память, которую вы каким-то образом используете, совершенно не означает, что эта DDR-память
будет отображаться с нулевого адреса по размеру этой DDR-памяти. Такая особенность хорошо объясняет,
почему нужно было в какой-то момент прийти к мемори-мапам, то есть, некоторым картам памяти,
которые определяют... Сейчас стандарт UFI. До этого был, соответственно, ABIOS, был вызов функции E820,
который вам говорит, что вот на самом деле ваша оперативная память находится вот там-то, там-то и
там-то, она имеет вот такие-то типы. Потому что, если бы вы просто смотрели, как вы делали там раньше,
например, через регистр того же самого RTC, сколько действительно там у вас установлена оперативной
памяти, то вот вы узнали, что у вас 4 гигабайта, и в какой-то момент вы писали совершенно не туда,
куда надо. Почему? Ну, так исторически сложилось, что конкретно на x86 реально там вот память,
которая чуть ниже 4 гигабайт, она используется под устройство. Ну, в частности, мы помним,
наверное, первую лекцию. Кто вспомнил, возможно, бы ответил на вопрос. У процессора есть reset
vector. Соответственно, reset vector на x86 находится чуть-чуть пониже, чем 4 гигабайта. Соответственно,
чуть под 4 гигабайтами у процессора замаплена флэш-память, и, соответственно, вот та часть,
вот которая, грубо говоря, вот эта вот под флэшку, она тратится, ее потом просто перемещают в
более высокие адреса. Таким образом у вас получается, что у вас там от нуля до, грубо говоря,
4 гигабайта минус 20 мегабайт идет условно обычная память, потом у вас идет флэш-память,
потом у вас снова идет обычная память, которую мы вернули, потому что здесь было замаплено
устройство. Соответственно, вот такие хитрые преобразования, они делаются там не только для
флэш-памяти. Ну вот, например, у вас у всех есть видеокарточка. Вы знаете, как видеокарточка
общается с вашим компьютером? Да, есть шина PCI, PCI express в настоящий момент. Соответственно,
мы помним из третьей лабораторной работы, вот вывод, что для того, чтобы добраться до PCI
express, у нас есть некоторые порты ввода-вывода, но в принципе обычно адресное пространство устройств
еще и отображается через оперативную память, то есть через MMIO. Вот через MMIO стандартно
отображается конфигурационное адресное пространство, и это отображение плюс-минус фиксировано,
то есть настраивается специфичным для вашей платформы образом и плохо там, скажем, меняется.
А если говорить о именно самой процедуре общения, то есть там, как, например, текстуры попадают в
видеопамять, то для этого используется такая штука, называется бары или base address register. Вот у PCI,
у него может быть 6 таких баров. У PCI может быть 6 таких баров и оно вот выглядит примерно вот так,
где каждый, собственно, бар это 32-битное значение. Таким образом, у вас, соответственно,
вот все бары, которые там были со старым PCI, они должны были находиться в 32-битном адресном
пространстве, потому что те люди, которые писали, соответственно, стандарт PCI, они не считали,
что возникнет такая вероятность, что у вас там будет больше, например, 2 гигабайт или 3 гигабайт
оперативной памяти. То есть можно все разместить в 4 гигабайтах, там все равно шинта 22-битная,
никогда туда не долезет. Соответственно, в реальности размер бара может быть там по вот такой вот
штуке не больше 256 мегабайт, просто по простой причине, что часть битов в этом баре отведена
на служебные данные, то есть там указывается тип этого бара и некоторые другие вещи. Как вы
понимаете, вот с такими ограничениями жить достаточно неприятно, то есть, во-первых,
размер 32 бита сильно ограничивает в адресах и, во-вторых, соответственно, там размер самого
бара всего в 200, там, 56 мегабайт не более, он нас еще больше ограничивает. Кстати, не сказал,
как прочесть размер, собственно, бара, вы просто пишете в текущее значение все f, то есть все биты
устанавливаете, после этого считываете и смотрите, какие биты оказались нулями. То есть те биты,
которые оказались нулями, они соответственно декодируются, можно узнать таким образом размер.
Вот, поэтому сейчас в современных стандартах PCI у вас бар, он находится как бы в двух половинках,
то есть, соответственно, первые там, грубо говоря, 32 бита, это младшие значения бара,
следующие там 32 бита, это у вас старшие, соответственно, биты, соответствующие бару.
Таким образом можно получить либо 6 32-убитных баров, либо 3 64-убитных, можно, в принципе,
в перемешку. Вот, и как результат, например, там в биосах относительно давно появилась там опция
above 4G encoding, она как раз вот про особенность расположения PCI памяти у вас в оперативной памяти.
То есть, когда раньше, собственно, для старых операционных систем можно было расположить PCI
память только здесь, ну то есть ниже 4 гигабайт, то соответственно с приходом вот этой опции above
4G decoding encoding вы можете использовать 64-убитные бары и можете, соответственно, записать адрес
бара там выше 4 гигабайта, соответственно, вот такой вот ерунды с потерей адресов. Вот здесь вот
обычно там терялось несколько сотен мегабайт и их потом пришлось здесь искать. Ее больше сейчас нет.
Кроме этого, кстати, вот возможность записать, соответственно, бар в два, соответственно,
32-убитных регистр она еще привела к возможности увеличить сам размер бара. Ну то есть представьте,
что у вас графической карточке, ну там допустим, сколько сейчас там, ну 32 гигабайта оперативной
памяти, например, соответственно, вам хочется туда передать какую-нибудь текстуру большую или там
не текстура, а какой-нибудь там вычислительный набор данных, которые вам надо обработать для
вашей нейросетки. Если у вас размер бара там 256 мегабайт, то вам придется последовательно маппить
там вот кусками по 256 мегабайт видеопамять в вашем драйвере и загружать вот эту текстуру вот
так вот по кусочкам. Это не очень удобно, там гораздо проще, соответственно, отобразить всю
видеопамять в оперативную память, то есть установить размер бара там, скажем, максимальный,
и там современные видеокарты, ну где-то так, начиная с 2008 года, ну чуть-чуть попозже, 2008 год это
стандарт появился PCIe. Они позволяют это делать, поэтому мало того, что у вас, соответственно,
сами бары переехали в высокую память, так они еще и увеличились. Ну, увеличились там опять же
по опции в биосе, потому что конфигурационным адресным пространством PCIe управляет биос в x86
системах, так сложилось, но вполне себе позволяет вам оптимизировать процедуру общения с PCIe.
Но это что касается работы с физической памятью, соответственно, на деле мы понимаем, что кроме
виртуальной памяти, которую мы с вами имеем, на более ниже лежащих уровнях у вас все равно есть
какие-то способы преобразования адресов, и вот даже когда вы говорите, что у вас физический адрес,
он на самом деле не физический с точки зрения оперативной памяти. Причем, я вам даже больше
скажу, он на самом деле, даже когда вы перевычисляете адрес, ну допустим, нулевой адрес у вас
соответствует нулевому адресу в memory-контроллере, на самом деле этот нулевой адрес вовсе не будет
находиться в нулевой адресе, не будет являться нулевым адресом в вашей DDR-памяти. Догадываетесь почему?
Нет, я имею в виду, что вот в смысле, ну да, тут что-то лежит, но вот тут вот ничего не лежит,
вот соответственно, вот это вот маппится, допустим, один к одному. Ну допустим, скажем так, не так важно.
Да нет, там все проще. Про атаку вида Callboot слышали? Ну когда там страшные дяди, собственно,
в погонов приходят, собственно, жидким азотом заливают ваш компьютер, собственно, после этого
вынимают оперативную память и дампят ее содержимое. Ну слышали про такие страшилки. Знаете,
почему она-то не работает? Правильно. Соответственно, есть, во-первых, скрэмблинг, который значительно
усложняет поиск соответствий между адресами и соответственно заполненными битами в контроллере
оперативной памяти. То есть фактически данные перемешиваются, вам придется использовать
статистический анализ для того, чтобы понять вот какому реальному адресу соответствует байт.
Ну сложно, но в целом можно на самом деле извлекают. А на современных процессорах
к этому еще добавляется шифрование аппаратное, чтобы на всякий случай. При этом, соответственно,
кэш, ключ шифрования находится в регистрах процессора и его так просто извлечь не получится.
Ну это что касается вот самой низкоуровневой стадии работы с оперативной памятью, с памятью
разных устройств. Но нас, как разработчиков операционных систем, все-таки больше беспокоят
следующие две стадии. Ну про сегментную адресацию мы помним. То есть сейчас действительно сегментная
адресация используется в 4-битном режиме только для thread-local storage. Что такое thread-local storage, помните?
Да, это, соответственно, локальные для потока переменные. Вот, соответственно, у вас есть
фактически вот эти самые сегментные регистры и с помощью одного из там мср регистров, ну типа там
js-base или fs-base, можно установить текущее значение вот этого самого сегментного регистра и тем
самым адресовать оперативную память локально для конкретного ядра ЦПУ. Что касается страничной
адресации, то это дополнительный уровень индирекции, который нам доступен с целью, собственно,
изолировать адресные пространства других процессов и позволить процессам жить как бы так, чтобы они
не думали о том, что работают в адресном пространстве с кем-то еще. То есть, с одной стороны,
процесс считает, что вся оперативная память принадлежит ему, с другой стороны, на деле это
обеспечивается там ядром операционной системы, которая эту память просто так настраивает и снимает
с вашего приложения необходимость думать о как вот использовать физическую память, чтобы вам ее
хватило и чтобы соответственно все ваши данные в ней расположились. Как ядро выбирает ту физическую
память, которую он может использовать для построения карт виртуальной памяти? Мы все помним,
что есть UEFI Memory Map и мы только что выяснили, собственно, для чего он нужен, потому что на
самом деле физическое адресное пространство, оно во-первых, нелинейное, чаще всего еще и non
contiguous, то есть непоследовательное. И UEFI Memory Map это некоторый массив, состоящий из таких вещей,
как называются там дискрипторы. В дискрипторах у вас есть вот такая вот структурка, она у вас
есть в стандарте UEFI и вам придется по ней походить в как раз шестой лабораторной работе,
чтобы настроить себе аллокатор физической памяти. То есть на структурке есть такие поля,
как тип памяти, ну соответственно там conventional memory, например, это свободная память,
которая никем не используется. Какая-нибудь boot services, код или дейта, это память,
которая используется для сервисов того же UEFI, которая не используется в момент старта
операционной системы. Runtime Services может использоваться, потому что это память,
которая обеспечивает функционирование UEFI Runtime Services в процессе работы операционной системы.
Соответственно есть информация о том, где физически данная там память находится,
то есть там, грубо говоря, физический старт, то есть начало участка памяти, соответствующие
там физическому адресу. Есть количество страниц, причем в UEFI есть жесткое требование,
что размер страниц в UEFI равен 4 килобайта, но это не значит, что UEFI не поддерживает системы,
на которых размер страниц не равен 4 килобайта. То есть просто UEFI определил для себя,
что есть вот некоторая гранулярность, с которой он работает. Понятно, что на системах,
где размер страниц меньше, либо больше, просто придется дополнительно пересчитывать при
разработке самой прошивки. Не очень понял вопроса. У вас конфигурация ММУ будет задаваться в тех
страницах, которые у вас поддерживает железка. Соответственно вы не сможете,
грубо говоря, отобразить участок памяти меньшей гранулярности, чем размер страниц,
которые поддерживает ваша железка. Просто не сможете так сформировать регистры или
иные структуры, о которых мы сейчас поговорим, когда будем обсуждать таблицу страниц на x86.
У вас там места подбитой не хватит. Соответственно, есть количество страниц,
которые выделены в данном регионе. Есть некоторые дополнительные атрибуты, такие как кэширование,
права доступа, которые могут быть использованы вашей операционной системой для правильной
настройки этой самой памяти. Простой пример. Когда я вам рассказывал только что о барах,
чтобы узнать размер бара, вам нужно сначала записать какое-то значение, а потом прочитать то,
что получилось на выходе. При этом вы прочитаете совершенно не то, что вы записали. Если вы
установите неправильные биты кэширования, например write-through кэширование, то да,
оно действительно вполне успешно запишет все в самоустройство. Но когда вы будете читать данные,
то они, например, прочитаются из кэша, и вы прочитаете совершенно не то, что вы хотели.
Поэтому ufi, собственно, вам делает жизнь несколько проще. Он сразу говорит, что вот тут, например,
у меня находится MMIO-память, то есть memory map.io, то есть там есть некоторые устройства, и там,
допустим, эта память не кэшируемая. То есть он упрощает вам жизнь. Кроме физического адреса,
у вас здесь еще есть виртуальный адрес. Виртуальный адрес соответствует адресу вот этого
вот самого дискриптора в то время, когда работает операционная система. То есть фактически он
говорит, что вот этот вот самый дискриптор будет доступен по вот этому виртуальному адресу из
гидра, когда операционная система запустится. То есть в тот момент, когда вы будете, например,
вызывать runtime-сервисы, такие как перезагрузка, получение какой-нибудь переменной из ufi,
variable storage, получение времени, какие-то другие операции, в том числе, например,
конфигурационной таблицы и так далее. Есть мысли, почему вот это вот самое поле virtual start,
его нужно указывать именно в процессе работы вашего загрузчика операционной системы и
передавать в метод setVirtualAddressMap, который выполняется у вас прямо вот перед стартом
операционной системы. То есть почему просто не могла бы операционная система потом сама как-нибудь
решить, куда ей хочется разместить, то есть как ей хочется замаппить вот эти вот регионы памяти,
если ей действительно хочется. Ну вот почему, собственно, операционная система, которая
определяет эти виртуальные адреса, то есть у вас загрузчик операционной системы эти адреса
назначает. То есть он их просто берет, какие хочет, и выставляет. То есть причина там,
почему он это должен делать еще в тот момент, когда работает прошивка ufi, то есть работают
бутсервисы. Ну вот зачем, то есть в принципе, почему нельзя было просто операционной системе не
иметь вот это вот поле вообще в рамках спецификации ufi и не просто отобразить так, как ей удобно. Ну я
вас уверяю, люди, которые там делали спецификацию ufi, они, конечно, сделали, возможно, удобнее,
но причины были вовсе не в том, чтобы сделать для пользователей что-то удобнее, просто по-другому
бы не заработало. Есть мысли? Ну допустим, я вам могу простой пример кода перевести. Вот представьте,
что у вас записана вот такая конструкция. Это некоторые две глобальные переменные в,
соответственно, C программе. Первая переменная это integer a, соответственно, вторая переменная
указатель на integer b, которая инициализируется значением указатель на переменную a.
Вы правильно, соответственно, мыслите, что у вас возможно две ситуации. Первая
ситуация, это когда, грубо говоря, вот та программа, которая содержит в себе скомпилированный,
соответственно, этот код загружается по фиксированному адресу, соответственно, мы тогда на стадии
компиляции можем вычислить вот этот вот адрес, его сюда подставить и ничего делать не нужно будет.
Здесь будет просто лежать абсолютное число, которое, соответственно, мы сможем использовать. Второй
вариант, если у нас есть ISLR, соответственно, address space layout optimization, то есть у вас программа
может загрузиться по случайному там базовому адресу, причем, возможно, даже не только базовому,
в зависимости от того, как ISLR реализован, то есть у вас гранулярность там может быть разная
достаточно. Вот у вас, соответственно, есть ISLR и это означает, что ваша программа будет загружена по
случайному адресу. И вот этот вот случайный адрес, он фактически добавит смещение,
он добавит смещение вот к этому адресу относительному, который был вычислен на
стадии компиляции, который нужно прибавить, чтобы получить реальный адрес переменной А.
Вот эту процедуру, соответственно, преобразование того адреса, который лежит здесь в компилированной
программе и, соответственно, добавление к нему вот этого оффсета выполняет загрузчик вот этой
вот программы и, соответственно, механизм, который обеспечивает вот это самое преобразование,
называется relocation. То есть в программе специальным образом зашит некоторые там вот набор инструкций,
которые говорят, что нужно сделать преобразование и, соответственно, там загрузчик вот этого вот
образа программы смотрит на этот список relocation и делает вот эти преобразования в момент, соответственно,
там загрузки данного файла. Райперовый тиф адресинг работает, но в данном случае как вы сделаете
райперовый тиф адресинг? У вас просто хранится адрес. Здесь глобальная переменная, соответственно,
этот код не существует, это некоторая статически инициализированная память. То есть, да,
действительно, можно было бы сгенировать конструктор, но зачем? В целом, да, но как бы
смучиться придется с конструкторами, потому что вы никогда не задумывались, как у вас работают
конструкторы. Вот простой вопрос, у вас есть функция main, где выполняются конструкторы?
Правильно, они действительно выполняются до main, и, соответственно, если у вас программа embedded,
то вам придется выполнить конструкторы самому. Соответственно, там в джосе есть специальный код,
который выполняет конструкторы, которые сгенировал компилятор. Для этого нужно
делать дополнительные усилия. Уже есть или будет? Уже есть. Скорее, сейчас уже должен быть,
потому что у вас там поддерживаются, по крайней мере, конструкторы, которые для санитайзеров
используются. Соответственно, это дополнительное усилие и, соответственно, это дополнительное
замедление вашего кода, как говорится, зачем. Поэтому гораздо проще использовать эту технику.
Так вот, с этого объяснения стало понятно, зачем вот сюда написать виртуальный адрес?
Так он уже релоцировал. Это не V2 код. Так он уже их применил.
Так ядро работает по физическим адресам сначала.
Вот этот вот адрес не равен вот этому. По крайней мере, может быть, не равен. Поэтому они остаться
корректными плюс-минус не могут, потому что, возможно... Что?
Про ядро не беспокойтесь. Не беспокойтесь про ядро. Ядро находится в виде незагрузчика
операционной системы. Загрузчик операционной системы работает с ядром так, как он хочет.
Ему не важно абсолютно, как работает. Ладно, это на самом деле не столь такая тривиальная мысль.
Дам еще одну подсказку. Вот у вас есть рантайм-сервисы. Рантайм-сервисы операционная система будет
называть по виртуальным адресам, не по физическим. Да, вот представьте, что у вас в коде рантайм-сервиса
был вот такой код. Соответственно, в какой-то момент у вас в глобальных переменах хранились
указатели, содержащие физические адреса каких-то участков памяти, которые были валидные в момент
работы прошивки. В какой-то момент мы захотели работать с виртуальной памятью. То есть мы
захотели запустить операционную систему, и мы назначили вот эти самые виртуальные адреса тем
регионам, которые будут исполняться в процессе работы нашей операционной системы. В UEFI есть
такой метод, называется convert pointer. Этот метод как раз занимается тем, что он преобразовывает
физический адрес в виртуальный в соответствии с той таблицей memory map, которую вы в него
передадите. Соответственно, вот у вас есть актуальная таблица memory map. Он находит,
грубо говоря, регион, в котором у вас лежит данный участок памяти, то есть он фактически
находит физический адрес. Соответственно, далее вычитает physical start из значения вашей
собственной адресопеременной и прибавляет virtual start. В результате он получает новый адрес,
который будет уже являться виртуальным. Здесь есть тонкость, что вот такая конструкция,
мы ее воспроизвели сейчас в C, она может возникать и в сгенерированном компилятором коде. То есть
там в некоторых ситуациях компилятор может сгенировать релокацию, причем не обязательно
такую, но какую-то, которую нужно будет преобразовать. Поэтому здесь решение не
столь элегантное, но кроме вот ручного вызова вот этого метода ConvertPointer, загрузчик,
который есть в UEFI, он еще дополнительно проходит по соответственно всем вот этим вот релокациям и
их заново обновляет. То есть проводит ту же самую процедуру, но еще и для релокации. Но как
результат, по этим двум причинам необходимо сообщать в прошивке UEFI, по каким виртуальным
адресам у вас в дальнейшем планируется ее исполнение. При этом здесь есть важный момент,
что операционная система или там прошивка в момент назначения виртуальных адресов ничего
по этим виртуальным адресам не выполняет. То есть у вас вот эти виртуальные адреса есть,
все необходимые структуры в момент, то есть к концу вызова setVirtualAddressMap, они будут
преобразованы, но при этом никто отображение виртуальных адресов на соответствующий физический
не настраивает. Это особенно, кстати, было неочевидно многим разработчикам прошивок,
поэтому нет ничего удивительного, что многие прошивки там на заре развития UEFI, они просто
падали, когда виртуальный адрес не был равен физическому. Вот, собственно, такой относительно
грустный опыт. Да, не странички, обратите внимание, у вас тут есть количество
страниц, но да, действительно все верно, адрес может быть свой для каждого дискриптора, то есть чаще
всего операционная система назначает адреса последовательно друг за другом, но вот в целом.
А когда вы требуете несколько дискрипторов? То есть один дискриптор за одно что отвечает?
За один тип памяти. Да, вот у вас типы здесь перечислены. Ну, не все, но знаете, вот есть тип,
например, вот вывод MMIO, есть, соответственно, там тип, например, там runtime services код, есть тип
runtime services дейта, но вот, соответственно, если вы смотрите на эти типы, то скорее всего вам придет
ложное ощущение, что вот этот вот тип данных код, который он предназначен для кодового сегмента
UFI runtime driver, а вот этот вот тип предназначен для сегмента данных UFI runtime driver. Ну, ведь
похоже, да? Соответственно, вот у нас сегмент кода, вот у нас сегмент данной, давайте им дадим два
разных типа. На самом деле это совершенно не так, и вы только что ответили на вопрос почему.
Значнее как, оно было так, пока люди не поняли, что они на самом деле сделали неправильно и они
начали все это чинить. Есть мысли почему? Ну, вы буквально только что ответили на этот вопрос.
Нет, один и тот же там не надо, тут вопрос в том, что если virtual start этим сегментом будет выдан
непоследовательно, то есть разница между virtual start конкретного кода и данных и, соответственно,
physical start между кодом и данным принадлежащих одному исполняемому драйверу будет разной,
то, соответственно, у вас просто упадет программа. У драйвера внутри себя есть обращение по
относительным адресам, то есть Reproative адресация к переменным данным, и, соответственно, оно вот
таким образом просто падало. Как результат, у нас сейчас есть две таблицы, соответственно,
есть ufi-memory-map, а есть, соответственно, ufi-memory-attributes-table, соответственно,
есть так называемая mat. Вот есть map, есть mat. Вот, соответственно, mat это табличка вот
ровно с такими же дескрипторами, но в этой табличке присутствуют только те дескрипторы,
которые получают виртуальный адрес, и, соответственно, эти дескрипторы, они, грубо говоря,
делят каждый дескриптор memory-map на один или, соответственно, более поддескрипторов,
в котором уже указываются права доступа. То есть, грубо говоря, там все драйвера получают,
соответственно, права, получают тип ufi-runtime-services-code в memory-map, а, соответственно,
в memory-attributes-table они получают два поддескриптора, в котором, соответственно,
в первом там написано, грубо говоря, допустим, write-protect права доступа в атрибутах,
а, соответственно, во втором, соответственно, там execute-protect, то есть, соответственно,
там изолируется права доступа на исполнение. Соответственно, сегмент, тип дескриптора
runtime-data используется для локации куч, то есть память в куче. Но, в любом случае,
типов, собственно, много, у вас, например, код загрузчика, это loader-code, loader-data будет использовать.
Да, она это должна сделать, если она хочет использовать ufi-runtime-services. Если она не хочет
их использовать, то, в принципе, она может даже не назначать виртуальные адреса. Но, как бы,
в целом, там стандартная практика, что что-то все равно надо назначить, а дальше уже можно
маппить, можно не маппить в зависимости от потребностей. То есть, например, там в джосе
runtime-сервисы они не замаплены, хотя им адреса назначены. Но это не из-за того, что, там,
грубо говоря, так нельзя было сделать, а просто в джосе есть индивидуальное задание добавить
поддержку runtime-сервисов. Да, задание, на самом деле, очень интересное, я его всячески рекомендую,
потому что у вас есть еще 32-битная прошивка в дополнении к 64-битной. И очень интересно делать
преобразованные прыжки из 64-битного кода в 32-битный, чтобы вызвать сервисы в 32-битной
прошивке, после этого прыгнуть в 64-битный обратно. Поэтому настоятельно рекомендую попробовать,
кто хочет практиковаться при переключениях режима или кому, соответственно, второй лабораторный было
мало. Поэтому, наверное, все. Единственное, что можно сказать еще о казусах. Вот представьте,
что у вас был вот такой вот код, и, соответственно, там прошивка, ну, вот в момент загрузки вашего
бинарника, она сделала там преобразование вот этого адреса, то есть она добавила ему смещение
там ISLR. После этого вы взяли и написали в своей какой-нибудь программе там код, ну, там допустим,
там B, там ALOG, там, ну, не важно, там 4. То есть взяли и изменили значение B. Как я уже вам сказал,
у вас есть два механизма. Вот, соответственно, механизм 1 — это конверт-пойнтер, который вы
руками делаете, а, соответственно, механизм 2 — это вот то, что делает прошивка, когда добавляет
базовое смещение. То есть вопрос, что здесь должна выполняться? То есть если бы вы не изменили
значение B, то вроде бы должна была сама прошивка это починить. А вы вроде изменили значение B,
но прошивка никак не может узнать, изменилось оно или нет, потому что у вас память просто
перезаписалась. Это set virtual address map. А в момент, когда вы работаете с прошивкой там до еще загрузки
операционной системы. Что? Не совсем, потому что вот эта вот процедура, прибавление вот этого
SLR offset, она вам вычистит вот этот вот SLR offset и добавит новое значение. То есть у вас будет
просто неволидное значение переменной, потому что выделенный адрес вот здесь, он может не
соответствовать тому дескриптуру, который был вот здесь. На самом деле, это одна из дыр просто
очередная в UEFI спецификации и так, соответственно, писать нельзя в runtime драйверах. Те, кто,
соответственно, так делали, они отлавливали очень интересные ошибки при работе операционной
системе со своими прошивками. И вот там в линуксе есть файлик, называется efikworks.
Вот вы его можете почитать, там увидите, сколько хороших прошивок существует в мире и какими
костылями их приходится там либо отключать, либо чинить, чтобы они каким-то образом продолжали
работать. Легко, но в целом уязвимости там в основном в других местах. Но мы целом заканчиваем с
нашими таблицами memory map и memory attributes table. То есть мы получили всю карту физической памяти,
которую у нас только есть и, соответственно, настроили механизм сегментации, чтобы он отображал
все один к одному. И сейчас наша задача настроить виртуальную память. То есть,
так как это 64-битный режим, то нам нужно настроить виртуальную память даже в ядре.
И как, собственно, это у вас выглядит? В процессоре есть специальный регистр. В x86 этот
регистр называется CR3. В этот регистр у вас записывается некоторый физический адрес,
ну вот как раз на слайде, соответственно, есть регистр CR3, в котором содержится адрес
некоторой структуры в оперативной памяти. Данная структура называется PML4, но в целом для
понимания нужно просто понять, что это самый, грубо говоря, корневой элемент таблиц трансляции,
который у вас будет. В x86 у вас, соответственно, 48-битная шина, и мы виртуальный адрес можем
представить вот таким образом. Все помнят, что странички в x86 4k, то есть вот эта вот штучка,
это 4 килобайта, и она же называется offset. А в дальнейшем ваш виртуальный адрес делится
на, собственно, 4 сегмента. Вот, собственно, 9 бит 1, 9 бит 2, 9 бит 3, 9 бит 4. Когда процессор видит,
соответственно, ваш виртуальный адрес, он его рассматривает как, грубо говоря, 4 индекса.
Соответственно, 4 индекса в некоторых массивах, которые вот как раз являются таблицами страниц.
В каждой таблице страниц, которая также там размером 4 килобайта, есть 512 записей размером
8 байт каждая. Вот в каждой записи, которую мы как раз индексируем вот этим вот числом,
соответственно, 2 в 9 и 512. Назначение вот тут от 0 до 511 является индексом массива
сначала самой старшей таблицы. Она индексирует вот эту вот самую таблицу и получает некоторый
фактически адрес, который может быть двух видов. Соответственно, вид первый, она получит адрес
следующего уровня таблиц. То есть она фактически получит вот этот вот адрес. То есть адрес начала
следующего уровня таблиц, в частности, для x86 это pdp. По поводу названий, соответственно,
pml4, pdp, pde, pte. Это название, которое использует Intel, но с счетом того, что эти аббреватуры
достаточно неудобно запоминать, чаще всего просто нумируют в обратном порядке. То есть вот это у вас
там p4, это соответственно там p3, это там p2, это соответственно p1. Так гораздо проще запомнить,
чем пытаться вот. Да, pte и соответственно pde. Ну, соответственно, pd это соответственно
page directory, а e это entry. То есть вот entry, которая вот тут, она соответственно вот pde, соответственно,
это pte и так далее. То есть на это pml4e, это соответственно pdpe. Сама табличка pdpt. Запись,
соответственно, добавляется e. Так вот, вы получаете, вот здесь у вас хранится адрес на
следующую табличку, и вы берете следующий бит. Вот следующие, грубо говоря, 9 бит. В сумме вот с
этим они вам дают следующую запись. Здесь вы переходите к следующей адрес памяти и снова
вычисляете адрес следующей ячейки. Вот здесь хранятся соответственно физические адреса каждого
там последующего уровня трансляции. Пока вы не дойдете до последнего уровня трансляции, который
будет указывать уже непосредственно на физическую память, непосредственно на 4 килобайта, и вам
останется лишь прибавить последние 12 бит, то есть последний offset к этому адресу, чтобы получить
вот реальное расположение вашего соответственно кода в оперативной памяти.
Да, вот соответственно я так раз поэтому и сказал, что есть два пути. Первый путь, это вот
собственно пройти по всем вот этим вот уровням, а соответственно второй путь, это соответственно
остановиться на каком-то из этих уровней и сказать, что, например, там этот уровень является
конечным. Ну вот, например, если вот этот уровень последний, то у вас остается там страничка 4
килобайта, а если я, например, остановлюсь вот здесь, то у вас будет страничка 1 мегабайт. Вот
здесь можно выставить флажок специально, то есть в записи вот в этой вот там будет флажок, который вам
скажет, что после вот этой вот записи идет не следующая таблица трансляции, а уже оперативная
память, и соответственно процессор будет делать меньше хопов. Относительно быстрее, потому что
дьявол кроется в деталях, то есть если вся вот эта вот штука обращения, грубо говоря, к четырем
уровням, грубо говоря, там таблиц, она достаточно дорогая, и если бы просто вот вам подсунули
такой механизм трансляции, который каждый раз выходил в оперативную память, то было бы очень
дорого. Но на деле в процессорах есть такая штука, называется TLB или там Translation Locosight Buffer,
который запоминает фактически, какой виртуальный адрес соответствует какому физическому там в
формате LRU. Ну то есть он просто запоминает последние несколько штук, наиболее там часто используемых,
и тем самым соответственно обращение к памяти может не происходить в процедуре трансляции.
Это просто обычный кэш. Соответственно вот за счет этого у вас может оптимизироваться
соответственно процедура обращения к вот этим вот самым адресам. При этом ну конкретно у
интела там TLB бывает нескольких видов в зависимости от размерности страниц. То есть грубо говоря там
например там ну на 4k у вас там допустим ну например там 64 записи там на 1 мегабайт там допустим там
4 записи соответственно там на 2 гигабайта у вас там допустим там ноль или там одна
запись. И вот таким образом соответственно от увеличения как бы размерности таблиц вы не
всегда можете выбирать, потому что у вас процессор может иметь разные соответственно там TLB. Соответственно
одна из таких интересных задач в real-time системах это расположить трансляцию таким образом, чтобы
она как раз помещалась в этот TLB и не ходила в оперативную память вообще. По памяти я вам не
скажу. То есть на самом деле сейчас за счет конвейера за счет всех вот этих вот кэшей в
среднем разницы никакой. Более того у вас сейчас x86 64 режим в принципе не позволяет не использовать
таблицу трансляции. То есть у вас в любом случае все как минимум один к одному отображается. Поэтому
эти цифры получить по крайней мере на x86 стало еще сложнее. Да ну можете сами попробовать и
соответственно расскажете. Но сейчас эти цифры пренебрежимы малые, никто без виртуальной памяти не
использует соответственно там современный процессор общего назначения. Даже в embedded
системах просто потому что выигрыш, который дает виртуальная память он перевешивает значительно
те накладные расходы, которые приходится учитывать при построении систем. Ну и более того тут
надо еще понимать, что в некоторых процессорах, в том числе которые используются в специализированных
системах типа Hard-Real-Time, вот такая табличная конфигурация виртуальной памяти, она встречается
далеко не всегда. То есть это не единственный способ как можно программировать ММУ. Вот если
там представите там какой-нибудь Airbus, там условно 320, который в воздухе летает, вы наверняка летали,
то соответственно там будет использоваться архитектура PowerPC, с высокой долей вероятности
там ядро, по крайней мере на некоторых подсистемах будет вот такое. Это соответственно там PowerPC,
и соответственно вот у этого ядра ММУ работает вот без всего вот этого, там только ТЛБ. Причем
ТЛБ программируется вручную. То есть вы можете сами как разработчик операционной системы
занести ТЛБ записи и вот они будут вам обеспечивать трансляцию. Есть PowerPC, которые IBM, на нем
работали там всякие Power Mac и так далее, то есть компьютеры общего назначения. Есть PowerPC,
который соответственно там Fayscale, он же соответственно сейчас NXP, и вот эти ребята
занимаются automotive, avionics, космонавтика и так далее. То есть вот есть два направления
расприменения PowerPC, соответственно PowerPC всякие там компьютеры типа Power Mac, IBM всякие там
серверы, которые IBM Power, Power 8, Power 9 и вот это все. Это вот в одну сторону, там как раз таблица
страниц обычно. А соответственно вот это, это там всякий embedded, где нужны там гарантии времени,
то есть на их лучшего времени работы предсказуемой. То есть понятно, что там для вот такого ММУ
предсказательное и худшее время, там вполне себе реально в отличие вот этого вот. Но соответственно
в такой конфигурации не удастся отобразить всю память или удастся, но там с большими ограничениями.
Примиссии тяжело поддерживать, то есть конкретно там у нас приходится использовать целый интересный
математический аппарат, чтобы разложить операционку вот в такое вот подобное. Ну в разных модулях по
разному, но в принципе там обычно вот на таких вот штуках в районе 2 гигабайт. Ну это там скажем
скорее предел. То есть обычно там чуть-чуть поменьше. У вас здесь есть signExtend, вот он как раз подписан.
Соответственно если вот здесь единичка, то соответственно вот тут будут тоже все единички,
вот так вот так далее. Если у вас соответственно тот нолик, то соответственно и тут будут нолики.
Ну они не отрицательные, они безнаклые. Обычно по-моему в Винде вообще в userspace используются низкие адреса.
Не эксперт на Винде, не буду врать. Ну упадет пользователь, но земля ему кухом.
Ну у вас в какой-то момент просто вычисления с адресами будут приводить вот ко всяким различным
интересным вещам. То есть можно использовать в качестве wraparound и всякие там подобные операции.
Вот так просто архитектуру сделали. В каких-то вещах это не так, в каких-то это так. Ну вот у
Intel это так, а там у другой системы там обычно размерность шины она там соответствует. Или в
warmup будет просто tech использоваться, если у вас там эти, как его называют, аутентифицированные
указатели. То есть у вас вот эта память, она будет использоваться под криптиграфическую
подпись вашего адреса. То есть ну с какой железом далее, собственно, с таким железом придется
жить. Хорошо ли, плохо ли, ну как повезет. Так, ну это понятно, соответственно, как происходит
преобразование адреса. Или все-таки нужно какие-то вещи повторить. Смотрите, вот здесь вот у вас в
записи лежит адрес начала таблицы следующего уровня, а соответственно смещение вот в этой
таблице оно лежит вот здесь. То есть, грубо говоря, вот это, это, грубо говоря, плюс вот это,
дает вот это. А, понятно, да, я по привычке нарисовал, соответственно, что у меня нулевой
адрес вот здесь, а на картинке он нулевой адрес вот здесь. Да, старая привычка, я всегда нулевой
адрес вижу, соответственно, там, где он реально находится, потому что я в экзампе в основном
работаю. Рисовать нулевой адрес внизу в литературе такое часто встречается. Кому-то нравится, кому-то
не очень. Так, соответственно, вот в этих вот записях существуют различные флаги, но, соответственно,
вот пример таких флагов, это вот, например, флаг S, который как раз нам говорит, что является, грубо
говоря, данной, там, данная запись, является оконечной, например, это будет там запись, там,
двухмегабайтовая или одна гигабайтовая. Далее, там, соответственно, есть флаги типа, соответственно,
там, access или там обращение. Ну, точно так же, как и с сегментными записями, там, gdntree, то есть
флаги в таблице страницы очень сильно повторяют то, что есть в gdnt, ldt-записях. Ну, соответственно,
user говорит вам о том, что память доступна пользователю. То есть довольно часто при работе,
соответственно, в режиме вот ring3, ring0, в одной таблице страниц у вас отображаются и страницы
ядра и страницы пользователя. Соответственно, у страниц пользователя есть bit.u, который говорит,
что это user и, соответственно, пользователь может их исполнять с непривилегированного режима.
У страниц ядра этого бита нет и, соответственно, даже если пользователь обратится к, собственно,
памяти ядра, то, собственно, он упадет с нарушением прав доступа. С этим, соответственно, есть множество
проблем, связанных со спекулятивными атаками, и поэтому в большинстве современных систем на самом
деле, если они работают, по крайней мере, на intel или на некоторых армах, схема с отображением
ядерного и пользовательского пространства целиком в одной таблице страниц, она не используется,
просто потому что спекулятивные атаки. Но конкретно, как это все работает, мы поговорим
отдельно на следующей лекции и сейчас можете просто считать, что у вас хороший процессор,
поэтому можно делать себе удобно. Хороших процессоров уже не осталось, но мы же учимся.
Вот можно вы делаете свой хороший процессор. Вот, соответственно, есть каширование. Каширование вам
позволяет, соответственно, запоминать, то есть как раз использовать это некоторое, там, кашировать,
сами эти отображения с виртуальных адресов на физические. Если вы запретили каширование,
то следующий уровень таблицы он не будет запомнить. Вот как раз то, что вы говорили.
То есть он не будет запомнен процессором и, соответственно, можно будет при изменении вот этой
вот самой таблицы страниц сбросить только там tailback.
Хотя, а когда происходит изменение какой-то таблицы виртуальных адресов, как это об этом процессору сообщить?
CR3 перечисать. Перезаписать и перечисить. И он очистит, конечно, свой процессор.
То есть, по крайней мере, это такой самый простой, надежный способ. То есть сочетали значение CR3,
записали его обратно. Вот у вас там процессор, он сбросил каши.
Ну, есть, соответственно, BITIC PRESENT. Если BITICA PRESENT нет, то у вас может
какое-то и быть отображение, но, соответственно, в данный момент оно недоступно.
Это может быть удобно в тех ситуациях, когда вы что-то хотите размаппить, но при этом не хотите потерять данные.
То есть, вот вы сняли BIT PRESENT и, как бы, у вас данные все как бы есть, но пока вы его не поставите,
память не будет работать.
Сейчас, а то, что вы говорили, что CR3 сбрасывает, конечно, BITIC PRESENT. То есть, получается,
если в видеоходе вызывается какой-нибудь исколок для облагации допущенных страниц,
то это сделано, соответственно, очень сильно замедленно для программ, потому что даже нельзя
сочетать.
Нет, можно более мягкими способами сбросить конкретно эти. Есть, типа, сброс кашей по кашелиниям,
есть всякие экстендопиды. То есть, в свое время, почему всякие хасвиллы меньше замедлились,
потому что у них есть такая штука, которая, типа, тегированный TLB. То есть, когда у конкретного,
грубо говоря, процесса есть некоторый тег в той памяти, который он использует в TLB,
соответственно, можно зачистить только память для этого тега. Ну, там, в общем, много всяких разных техник.
Конкретно как там x86, я даже все, наверное, сейчас не вспомню, поэтому можете открыть Intel SDM
и там много всего хорошего написано про этот счет. Ну, то есть, грубо говоря, есть такой жесткий способ,
который работает плюс-минус всегда, но есть, понятно, способа попроще.
Соответственно, на ниже лежащих уровнях тоже, соответственно, есть различные биты конфигурации памяти.
Ну, вот, например, есть nxbit, который запрещает исполнение этой памяти.
Ну, соответственно, все хорошо, выставили nxbit, и после этого, как вы пытаетесь выполнить память,
то сгнивается соответствующее исключение, что patch hold нельзя выполнять ту страницу, которая не является исполненной.
Причем, что интересно, кроме nxbit, сейчас там в современных Intel есть еще такие штуки,
называемые SMAP и SMAP System Memory Access Prevention и System Memory Execution Prevention.
Идея заключается в следующем. То есть, вот, если у пользователя есть какие-то исполняемые страницы,
то их совершенно не обязательно исполнять в ядре. Ну, потому что зачем в ядре исполнять память,
которую может исполнять пользователь? Может он ее еще даже перезаписать может?
То есть, ну, просто небезопасно. Поэтому дополнительный как бы механизм, который изолирует исполняемые страницы пользователя от ядра.
То есть, если ядро пытается исполнять исполняемую память пользователя, он получит исключение.
SMAP, это соответственно Access Prevention, он запрещает доступ из ядра к памяти пользователя вовсе.
То есть, если ядро захочет почитать или подписать в память пользователя,
ему придется его замапить в себе, в адресное пространство, этот кусочек памяти,
или, соответственно, там просто отключить SMAP на время.
То есть, когда вы выполните индивидуальное задание, вот тогда будет нельзя.
То есть, на одно из индивидуальных заданий это вот NX, SMAP, SMAP. Вот.
Ну, как бы поставить заграничный лук и сделать это временно системной страницей?
Да, надо будет обязательно выдать это индивидуальное задание вместе сразу с какими-нибудь SMP, чтобы локи было поинтереснее захватывать.
Вот, допустим, у нас есть бит листовой в странице записи, а, допустим, этого бита нет, он более стартный в записи.
А его здесь и нет. А его здесь и нет.
А если мы ее назначим как листовую?
Ну, там будет, то есть в зависимости от типа таблицы, он по-разному, соответственно, бит интерпретируется.
Там формат записи один и тот же, но, соответственно, используемые биты отличаются от, соответственно, установленных битов.
Точно так же, как и в GDT, установили часть битов, соответственно, у вас другие биты интерпретируются по-другому.
Ну, соответственно, если у вас оконечная страничка, то там будет NX bit. Если она не оконечная, то, соответственно...
То есть, в зависимости от установленного сайта?
Да, да, да.
Ну, соответственно, можете вот, опять же, посчитать SDM, тут я просто некоторые там интересные биты привел,
а так их там чуть больше и, соответственно, можно делать чуть более интересные вещи.
То есть, как раз в конце лекции есть ссылки на страницы Intel SDM, которые стоит почитать,
даже если вы уверены, что вы все поняли, как работает виртуальная память в X86.
Мы как раз вот здесь пришли к ситуации, что мы время от времени хотим все-таки менять наше отображение.
То есть, мы его менять хотим не всегда и не на всех операционных системах.
То есть, как вы думаете, есть ли операционные системы, где не нужно менять таблицу трансляции?
Ну да, hard real-time системы, да, в некоторых случаях они могут действительно иметь статическую конфигурацию памяти,
но на самом деле не обязательно hard real-time.
Hard real-time – это лишь один из примеров операционных систем определенного класса.
То есть, у вас может быть класс операционных систем embedded, то есть, встраиваемый,
при этом это может быть не hard real-time операционная система, может быть просто какой-то soft real-time или вообще не real-time.
Но тем не менее, действительно, в некоторых операционных системах, строемых специального назначения,
используется принцип статической конфигурации памяти, когда мы выделяем все ресурсы на момент сборки образа операционной системы.
Это актуально, потому что вам позволяет обеспечить защиту от таких ситуаций, когда вы начали работать и вам памяти не хватило.
То есть, факт того, что у вас операционная система собралась, означает то, что вам хватило памяти на целевой вычислитель.
Поэтому в некоторых системах действительно можно не позволять менять таблицы страницы,
более того, их можно даже не отображать в виртуальной оперативной памяти.
То есть, если нам не надо менять таблицы страницы, то их можно просто расположить в какой-то момент в физической памяти,
а потом никому к этому доступа к этой физической памяти не давать.
Что еще дополнительно повышает надежность вашей операционной системы, потому что ее можно меньшими способами обрушить, поломать и так далее.
Но тем не менее, кроме систем, где нет возможности изменить конфигурацию виртуальной памяти,
есть большая часть систем, где это сделать можно.
И обычно существует два способа, как это можно сделать.
То есть, один способ – это использование некоторого дерева.
Соответственно, у вас в Джоси используется именно такой способ, и он в целом является основным для большинства систем.
Есть такая структура, типа range дерева, которая…
Алгоритмы структуры данных, я думаю, все проходили.
Кто не проходил, может вспомнить, условно, первый курс.
Range дерева, когда у вас, грубо говоря, отображается какой-то диапазон,
на что-то там, ну, грубо говоря, 2, 3, 5.
В данном случае диапазоны должны быть равные.
И у вас, соответственно, есть несколько вот таких пар.
И в зависимости, если там это дерево, то оно может быть как-то отсортировано.
Соответственно, может быть это не дерево, а какая-нибудь хэш-таблица.
Но не так важно. На обычных процессунных системах это именно так используется.
В Джос у вас как раз будет использоваться одно из таких деревьев.
Ну, в качестве домашнего задания можете посмотреть, как именно оно работает.
Второй способ – это self-referencing page labels.
Или самосцелающиеся таблицы, но сноявшегося русского термина как такового нет.
И такой механизм используется в Windows.
И в целом эта иллюстрация как делать не надо.
Windows, на самом деле, очень долго от этого страдал.
И до сих пор страдает.
Потом я объясню, почему так делать не надо.
Ну, по крайней мере, для определённого класса систем.
Например, тот же Касперский свой iOS этот механизм тоже использует.
И в принципе у них последствия не столь плачевные.
Но по другой причине, потому что у них операционки кое-чего нет.
Да, смотрите.
У вас возникает простая задачка.
Я всё-таки на следующий случай напомню, что вы хотите что-то отобразить.
То есть вы хотите что-то изменить, перестроить в вашей карте памяти.
Чтобы это сделать, вам нужно модифицировать таблицу страниц.
Ну, активную в текущий момент, то есть неактивную в текущий момент.
Но если это какие-нибудь деревья, то можно модифицировать любую таблицу страниц.
И это как раз, наоборот, очень-очень удобно.
И это одно из преимуществ управления памятью на основе деревьев.
Потому что вы можете в любой момент перестроить любую таблицу страниц.
Соответственно, если у вас используется система с self-referencing page tables,
то можно изменять только текущую активную в данный момент, лёгким способом.
Но вот как раз этот лёгкий способ, он тем самым, собственно, и интересен.
То есть представьте, что в самой старшей табличке,
то есть PML4, она же, соответственно, там P4, у вас есть запись,
которая ссылается на саму себя.
В качестве здесь примера используется 511-я запись.
Соответственно, 511-я запись имеет адрес, соответствующий вот этой вот самой PML4.
То есть, грубо говоря, если у вас попадётся 511-я запись в адресе,
то это означает, что на самом деле вы будете использовать таблицу P4
не только как таблицу трансляции четвёртого уровня, но и как третьего.
То есть вот эта вот запись, она вам сделает цикл.
Соответственно, она войдёт вот сюда.
Дальше мы, соответственно, вернёмся обратно.
И вот эта штука, соответственно, вот этот вот адрес, он снова пойдёт сюда же.
Понятна идея или не очень?
Ну, как-то будет понятно, почему.
Может быть, теперь вы куда-то интерпретируете, да, здесь P4,
да, все же.
Соответственно, идея в том, что после того, как вы один раз прокрутились вот тут,
у вас осталось только три операции разыменования по индексу.
Соответственно, грубо говоря, вот тут будет первая, вот тут будет вторая, вот тут будет третья.
И соответственно, смещение вот это вот, которое у вас останется, оно будет указывать на таблицу P1.
То есть у вас таблица P1, вот это, она будет интерпретироваться не как таблица следующего уровня,
которая состоит из каких-то, собственно, записей, индексов и так далее.
Она будет интерпретироваться как обычная память.
Потому что я напомню, что всё вот это вот, это четыре килобайтновые страницы.
Да, это все, чтобы быстро получить доступ по физическому адресу к таблице.
То есть вам процессор аппаратно вычислет адрес, соответствующий вот этой самой табличке.
Что?
Чтобы ее изменить.
Да, или прочитать. Чтобы ее прочитать, изменить и так далее.
Только там не будет проблем с битом size.
Там же, тот бит, который size, ну, если, например, правильно брать, не вы взяли это,
то он у BDE расстоянияется как бит dirty, который ставится...
Да, то есть там на самом деле с битом size там будут только проблемы в том, что если вы...
Чиселку большую будете использовать.
То есть если у вас offset будет слишком большой, то проблемы с size будут.
Но вы учите...
Что?
Ну, вы можете прокрутиться не только один раз, вы можете вот здесь еще прокрутиться.
То есть вот тут вот еще выставить единички.
То есть тогда два раза прокрутитесь.
Соответственно, получите доступ к P2.
Так можно к P3.
То есть вы можете к любую табличку посмотреть таким образом,
и тем самым узнать, есть ли у вас там bit size установлено или нет.
Так нет, я даже жду, что у нас, когда мы вот так вот получим,
даже один раз прокрутившим, получим доступ к P1 по адресу,
у нас, ну, то есть процессор уже не знает, не будет ничего прокрутились,
или он отдельно иногда его обрабатывает.
А зачем ему отдельно что-то обрабатывать?
Вот именно он. То есть он не обрабатывает, это отдельно.
А это значит, что он будет считать, чтобы таблица P2, это на самом деле,
он будет расценивать как P1.
И тогда он будет ее флаги в ней расценить как флаги в PTE.
А в флагах PTE у них тот bit, который у остальных...
Не, биты совпадают.
Вот именно, что bit, который будет как size, он в PTE и используется для чего-то другого.
Если вы правильно проявите, то это биты дёрги.
Нет, там порядок...
Нет, порядок там не соответствует порядку записи.
Там порядок битов в entry совпадает независимо от, соответственно, уровня.
Сейчас, то есть в PTE bit size...
Находится в том же месте, что и, соответственно, да, он там должен быть и до 0.
То есть все уровни могут выполнять...
Да, да.
Да, они полностью эквивалентны по геометрическому расположению битов.
В тем стране, что для них разные названия, с разными сокращениями.
Ну, так просто было удобно, когда было два уровня.
В 300-битном x86, соответственно, было только, соответственно, PDE, PTE.
Ну, page directory и, соответственно, page tape.
А потом вот стало 4, сейчас уже 5.
Нет, не понимаю, интересно.
Ну, и на ещё один уровень, чтобы шина стала больше.
Да.
Это же получается обратно, но скорее, не в чтении вот этой добычи.
Да.
Вроде, когда требуют обратную скринину,
играть пришло такое вот, что будет тяжелое, если я сейчас использую.
Тогда-то надо использовать обратную скринину.
А это же чтение таблицы.
Скажем, в какой-нибудь тририальной таблице.
Это не искрение, это, брат, чем почтение.
Это просто кодпись прочитала.
Эту штуку, как бы, открыли.
То есть Intel такой, вот, так можно было.
Это, ну, пяточный ремонт даблейки реализации.
У тебя же, например, просто пишут адрес.
Да.
Процессор менять никак не нужно было, чтобы это заработало.
Это просто так, можно посмотреть пристальным взглядом и увидеть, что так можно.
Есть ли у меня никакого фишки,
типа, сделать там, ну, 1.1.1.1,
типа, ну, будет вот такая вот цифра на земля, да,
а 1.1.1.0, там, цифра, она входит на другую страницу,
чтобы там, так, не ассоциироваться.
Ну, то есть, нет, ну, другого варианта,
что он не проигрывается, что-то ещё не это полезно.
Ну, может быть, вы найдёте.
У меня так в голову, наверное, не приходит.
То есть, ты просто, у тебя вот этот вот,
ты ссылаешься, короче, на b4 entry,
на котором, короче, адреса b3 таблицы,
ну, соответственно, ему стоит адрес самой b4 таблицы.
Вот в этом entry.
Вот 1.1.1.1, дописано для примера.
Да, там любое значение может быть.
Чтобы оно указывало на b4 таблицу обратно.
Ааа, опуск.
Почему не пострадать?
Вот вопрос аудитории.
Почему так делать плохо?
Ну, потому что пользовательская программа
может прочитать, кто вывез.
Как она может её прочитать?
Ей сюда юзер не поставит,
она ничего не сможет прочитать.
Ну, она может сам поставить и по себе.
Что?
Ну, упадёт с page fault, там, земля и пуха.
Ну, если она знает, как бы,
находится,
если она знает, на каком b4 entry находится фляг,
то она может, по сути, просто узнать содержимые таблицы.
Ну, вы близко, но кое-что не учитываете.
То есть, на самом деле, идея, действительно, в похоже.
То есть, проблема в чём?
Что когда вы выполняете код execution в ядре,
вам неплохо бы понимать,
примерно, где у вас что находится.
Потому что есть такая штука,
называется ASLR и, соответственно, KSLR.
Вот, соответственно,
зная,
где именно находится вот этот механизм
self-referencing page tables,
вы можете понять всю карту
отображения текущего виртуальной памяти
на физическую в вашем адресном пространстве.
Да, вам придётся получить привилегии ядра,
но, соответственно, самое первое,
что нужно сделать,
после того, как вы получили привилегии
при исполнении ядра,
это его не уронить.
Соответственно, чтобы его не уронить,
можно использовать эту технику
как раз для того, чтобы победить ASLR в ядре.
Ну, тут как раз пример,
как соответственно у вас
адреса смещаются
при этих вот циклах.
Соответственно,
винда теперь использует случайную запись
для self-referencing page tables.
Раньше они всегда использовали 1ED,
и это было очень радостно.
Для тех людей, которые
ломали ядерный ASLR в ядре.
Соответственно,
того же Касперский Ост
это не страшно,
потому что они в ядре
не используют ASLR
по причинам дополнительной надежности.
Вообще это звучит как довольно
странная юзимость,
что если вдруг у завышника
уже было исполнение кода в ядре,
то так он сможет обойти ASLR,
нет, смотрите,
получить исполнение
и, скажем так,
иметь возможность его проэксплуатировать
это разные вещи.
Это если у вас исполнение,
в смысле вы код исполняете,
никто уже, благодаря нашим стараниям,
не может так легко
получить просто прямое исполнение кода в ядре.
Есть такие штуки, как
Return Oriented программа,
поэтому вам
как бы легко
вы не получите
исполнение в ядре,
только в очень простых
операционных системах.
Чаще всего у вас какой-нибудь
Memory Corruption
и достаточно непрямое
исполнение кода.
У меня произвольно читать память.
У вас уязвимость
позволяет произвольно читать память,
но не более, но не исполнять ее.
У вас уязвимость позволяет раскрывать, например,
память ядра.
Так вы будете читать все,
вы прочитаете с некорректного адреса,
и, соответственно, ваш эксплуатит вместе с ним.
Соответственно, служба безопасности
после этого увидит,
что у вас ядро упало,
отключит компьютер от сети.
Все.
Заиграли в хакера и хватит.
Ладно.
На этом, соответственно,
у меня обещанные ссылки
на Intel SDM, которые я настоятельно
советую посмотреть.
Они вам пригодятся
на шестую, седьмую, восьмую лабораторные работы,
где у вас будет
как раз интересный процесс
реализации
поэтапной
виртуальной памяти
в Джосе.
И в целом
на этом у нас
на сегодня все.

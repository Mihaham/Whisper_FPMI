Итак, что такое SQL-Learning, я думаю, все помнят, правильно?
Мы из него, как обычно, сейчас вытащим только собственно
датасет, ну, функцию, которая генерирует нам синтетический датасет раз, и вытащим функцию, которая считает качественную классификацию.
Пока что мы с вами явно метрики, в кавычках метрики, не в смысле, не на алгебре, классификацию не разбирали, но accuracy, она же,
скажем так, точность, но со звездочкой, потому что есть еще precision recall, где precision тоже точность на русский перевели, поэтому обычно и называют accuracy.
Потому что на русском у нас появляется коллизия типа precision и accuracy переводится одинаково.
Вот, короче-ка, это доля правильных ответов, все, тут ничего думать не надо.
И разбиение на тройный тест. Ну, давайте генерируем себе маленький датасетик, смотрите, на самом деле, для всяких
игрушечных вещей крайне полезная вещь, что чем руками генератор данных писать, более того, многие из вас, я думаю, будут писать
диплом в этом году или в следующем.
Пожалуйста, не пишите диплом за последний месяц.
Так вот, на всяких синтетических данных поработать бывает очень удобно, чтобы свои гипотезы проверить.
Лезоскалерно дергать генератор синтетических данных сплошное удовольствие, не пренебрегайте им, пожалуйста.
Вот. Ну и, собственно, вот наш с вами какой-то простенький датсет, с которым мы с вами можем поработать.
Она порождает вам выборку просто-напросто из нескольких классов
случайным образом. Ну вот, если вы почитаете...
Не, смотрите, в данном случае мы с вами просто сгенерировали себе выборку из явно как бы случайных точек. Короче, что у нас здесь на самом деле есть?
По факту у нас здесь с вами нормальное распределение.
Несколько нормальных распределений случайным образом инициализируются, из них сэмплируются точки.
Двумерное, понятное дело, нормальное распределение.
Да, да.
Мы сейчас с вами как раз таки разобьем, собственно, нашу выборку на две части. Одна будет нам доступна, вторая будет отложенная.
Смотрите еще раз. Вот это вся выборка, которая нам доступна.
Мы пока что с вами работаем, грубо говоря, на оффлайн-данных, поэтому у нас есть выборка. Мы от нее часть отрубаем.
Это будет отложенная выборка, на ней будем проверять качество на отложенных данных. Все остальное используем как обучающую выборку.
Окей?
То есть, по факту отложенная выборка это то же самое, что могло бы к нам прийти потом.
Единственное, что мы сейчас с вами их уже видели, но так как учебные цели пока нормальные. По-хорошему, конечно, мы не должны вообще ничего там видеть.
Вот.
Ну, отложенная выборка симулирует то, что мы не видели на этапе обучения, но она по факту таковой и является.
Смотрите, у вас могут данные приходить или в режиме реального времени, например, там биржевые всякие данные, вот биржи работы, данные падают.
У вас могут быть данные отложенные, например, на том же самом кегле, на соревновании по машинному обучению.
У вас есть выборка, которую вам дали, чтобы учиться, и вам дали признаково описание, допустим, объектов с тестовой выборки, для них ответов вы не знаете.
Вы ответы генерируете, отправляете на платформу Kegel, она вам, соответственно, говорит, угадал, не угадал, молодец, не молодец.
И причем она опять говорит, на самом деле, по подвыборке, чтобы вы не знали на части данных вообще своих результатов, потому что именно по этой части данных будет потом утоговый лидерборд оттениваться.
То есть вы не знаете до последнего, как ваша модель себя ведет, но как в реальной жизни.
Вы сделали предсказание для клиентов, вы про них что-то знаете, потом клиенты либо ушли, либо остались, либо повысилась прибыль, либо понизилась. Все так же. Понятно?
Собственно, вот разбиение наших данных, тут вроде комментарии излишни.
Ну и собственно, вот наш классификатор, просто кнн-очка, так, там зеленая штука прыгает, супер, просто кнн-очка, берем 30 процентов, откладываем на тест, остальные 70, соответственно, останутся на тройне.
Вот наш классификатор, породить его абсолютно просто, инстанцируем k-nervous classifier.
Заметьте, гиперпараметр k мы явно не указывали, это не значит, что его нет, просто по умолчанию он равен 5, то есть пока что мы будем работать на 5 соседях.
Собственно, обучаем мы его, ну что значит обучаем, по факту кнн никак не обучается, там параметров нет, он просто запоминает обучающую выборку, он говорит, понял, ходите вот сюда, все.
И соответственно, для предсказания он точно так же делает предсказания для всех. Мы видим, что 90 процентов точность классификации для нашего классификатора.
Да, по дефолт, по умолчанию метрика, скорее всего, Евклидова, ну почему, почти наверно, скорее всего, потому что я, если честно сам, далеко не всегда помню все доки наизусть, давайте посмотрим.
Да, вот до этого мы с вами как раз помните, я говорил, что в некоторых условиях она хорошо подходит, сейчас для этого как раз доберемся.
Так, ну ты что-нибудь покажешь мне или нет, дай доку.
Во, дал доку.
Так, веса соответственно для всех одинаковые, алгоритм построения, вот, собственно, изначально у нас метрика Именковского, собственно, степенью 2, ну короче, да, Евклидова метрика.
Вот, хорошо, ну и здесь мы с вами можем на самом деле посмотреть на предсказания и сразу, чтобы это было недалеко от того, что я вам говорил, мы с вами можем и на вероятности посмотреть.
Вот, смотрите, предикт, проба, у всех почти методов классификации из колерна оно есть, то есть, предскажи мне, пожалуйста, вероятность, вы можете их замечательно тоже увидеть.
Вот, заметьте, некоторые объекты прям получили вероятность аля 100%, некоторые получили поменьше, вам что-нибудь видно вообще?
Хорошо.
Вот.
Ну и давайте, соответственно, посмотрим, кому он что предскажет.
Вот как будет наш классикатор выглядеть для различных, скажем так, как его называют.
Давайте его пока заставлю построить, для различного количества соседей.
Допустим, вот по ближайшему соседу он работает, по сути, как построена данная картинка.
У вас, у нас нарисована просто картинка изначальных точек, точки тестовые, в кавычках, положенные в, разбито все это по сетке, в, как это называется, у нас есть сетка, у нее есть вот эти ноды, как это называется, углы, спасибо.
В узлах сетки стоят типа точки, каждый из них, каждый узел сетки классифицируется, получаем цвет.
После этого раскрашиваем картинку, видим, где, грубо говоря, границы для различных классов по мнению нашей модели.
Видим, что для одного соседа границы какие-то немножко рваные, но это логично.
Один сосед оказался в ближайшем, и это сразу повлияло.
Для двух соседей все еще есть вот эти штуки, то есть тут несколько желтых точек.
Для трех, четырех и так далее эти дыры начинают постепенно исчезать.
Видите, вот у нас есть желтые точки на границе.
Видите, для тридцати соседей уже у нас достаточно плавная грынь.
В принципе, логично, чем больше у вас плотность точек, грубо говоря, в вашем пространстве,
и чем больше соседей вы учитываете, тем более четкая картина у вас будет.
На самом деле, если вы заместите, замощаете все свое линейное пространство, признаков пространства точками,
условно, берете какой-нибудь тепсилон сеть, строите, во всех узлах есть по точке,
то все в порядке, у вас метрический алгоритм будет работать замечательно.
Просто другое дело, что чем больше размеров пространства,
тем больше объектов вам понадобится, чтобы все работало замечательно.
Только и всего.
Ну и заметьте, что у нас качество классификации, в принципе,
разница на трене и на тесте.
Допустим, для двух соседей на тесте у нас 80%, на трене 93%,
здесь один на 80%,
но логично, что если у нас один ближайший сосед,
на обучающей выборке это будет всегда 100% точность,
у нас каждый объект и так присутствует в обучающей выборке.
Догласны?
По двум соседям видим, что у нас есть ситуация,
где объекты двух классов находятся очень близко друг к другу,
потому что иначе бы по двум соседям у нас тоже была точка близка к сотне.
У нас, ну, к единице, у нас она не близка к единице уже.
И так далее.
Смотрите, а теперь, внимание, вопрос,
я пока его оставлю немножко подумать,
внимание, вопрос,
а всегда ли мы можем использовать
КНН с Евклидовой метрикой абсолютно спокойно?
Или нам надо что-то явно понимать про свои данные?
Тут данные мы сгенерировали из какого-то нормального распределения,
все нормально.
0.1, да, ну, все-таки это все равно чиселки,
но они уже какие-то странные.
Верное замечание, а?
А размер небольшой?
Ну, про переобучение тоже.
Смотрите, мы с вами, когда считаем Евклиды в расстоянии,
мы что делаем?
Корень извлекаем, а еще мы с вами, по сути,
берем вектор, грубо говоря, разностей и считаем его нормой.
Правильно?
А у нас каждая компонента, вообще говоря, развивляется?
У нас разве компоненты однородны?
Ну, все признаки разве одной и той же природы?
Нет.
Ну, величитая невероятность, какая-то фигня еще...
Да, есть такая проблема.
Вот.
Как бы то, что у нас категориальные переменные,
плохо, грубо говоря, вы считаете друг из друга, да, вы правы,
но с этим еще можно, худо-бедно, как-то мириться.
А вот если у нас с вами признаки в разных шкалах,
то КНН, применяйтесь в коробке, ну, это очень плохая затея.
Почему?
Предположим, у нас с вами, опять же, есть условно,
вот дурацкая ситуация, выгрузили откуда-нибудь из ОДНСа
какую-нибудь табличку, и там какие-нибудь начисления есть
в рублях и в копейках.
И у вас как бы все начисления как раз какие-то,
порядка сотен рублей.
Тогда у вас получается, что для КННа, так это два отдельных
признака, разница в копейках и эквалентно разница в рублях.
Они одинаковый вклад в расстояние дают.
Хотя по смыслу это абсолютно бессмысленно.
Или что то же самое, у вас там может быть, я не знаю,
рублях, а цена чего-нибудь в тугрифах.
Все.
Получается, что у вас разница в шкалах
влияет на то, какой вклад делают ваши признаки
в расстояние исходное.
Поэтому, если вы, скажем так, не обладаете четким
пониманием, что у вас шкалы указывают реально на значимость
признаков, и чем больше шкала признаков, тем более
он важный, для КННа лучше признаки нормировать.
Вообще говоря, почти всегда лучше нормировать ваши данные.
В разных шкалах лучше для каждого признака посчитать
среднюю дисперсию, минус среднюю поделить на корень
с дисперсией.
Стандартизовать данные будет целее и работать будет более
устойчиво. Кроме случаев, когда у вас реально каждый
признак, скажем так, чем более значимый признак,
тем больше у него шкала и наоборот.
Окей? Договорились?
И, кстати, как раз таки вот с тем, что мы с вами считаем
расстояние, это нам тоже поможет, так у нас по крайней
части отклонения там синего от зеленого тоже будет
гораздо ближе ко всему остальному.
Окей?
Капейки это полная разница?
Не, не, вот смотрите, у вас условно
есть сумма, она у вас просто записана в табличке
70 рублей, 24 копейки.
У другого там 25 рублей, 31 копейка.
Тогда у вас получается отклонение по рублям и по копейкам
и, по сути, одинаково порядка вклады, хотя это принципиально
неверно. И я привел этот пример, именно чтобы было
понятно, потому что если такой пример, можно там
еще как-то данные предобработать, их совместить и так далее.
Представь себе, что у вас, не знаю, там, это на самом
деле даже нормировкой не починится, более того, это
плохо. Но в общем случае, представь себе, что у вас
там один признак, грубо говоря, не знаю, у вас там
площадь в квадратных метрах, средняя, не знаю, вот,
средняя площадь жилья, допустим, в Москве, это
один признак. А другой признак, это у вас население
города, например, просто в количестве человек.
И тогда у вас для одного города, там, для маленьких
городов, там, 60 тысяч, для Москвы, это миллионы,
там, для миллионников, а средняя площадь жилья
все равно будет там варьироваться от 25 до, я не знаю,
там, 100, условно. Получается, когда мы будем считать
своими расстояниями между точками, у вас средняя
площадь жилья просто улетит куда-то, потому что у вас тут
очень большая разница в шкалах, и это будет гораздо
больше вклад в норму, чем разница там на единицу.
Услов, да что угодно на самом деле, мы же расстояние
считаем, нам не важно какая-то силовая переменная.
То есть, если у вас очень большая разница в шкалах,
то у вас тот признак, который в большей шкале, просто
будет доминировать все остальные. Ничего хорошего
в этом нет. Так что, пожалуйста, не забывайте
нормировать ваши данные или четко понимать, почему
не надо нормировать ваши данные. Я могу придумать
норму, с которой мы доминируем эти фильмы?
Ну, можно ввести, да, такую норму, это нормально.
Технически, да, вы можете туда свою норму ввести,
вы, в принципе, можете очень сильно переписывать
все, что есть, это нормально. Вот. Ну и коллеги,
смотрите, последнее, на самом деле, что хочется сказать
перед тем, как я вас отпущу. Мы с вами только что
брали простенький датсет, с ним все работало.
Но, по-моему, у нас есть датсет, у нас есть
датсет, мы с вами только что брали простенький
датсет, с ним все работало. Но, по факту, мы можем
сделать более сложный датсет, где у нас и данные
уже более зашумленные, и количество информативных
признаков уже не все, а, допустим, только половина.
Половина, какой-то мусор, так бывает. И можем увидеть,
что у нас с вами классификатор тогда будет выдавать
качество классификации порядка 60%, что все еще
лучше, чем слепое угадывание, на трех классах
это было бы, сколько? 33%. Но, тем не менее, про
то, как настроить вашу модель так, чтобы не сидеть
вручную и все перебирать, мы поговорим на грядущих
занятиях. А пока, добро пожаловать, скажем так,
в мир машинного обучения, где у вас уже есть два
метода и база по линейной алгебре. Спасибо, все.

У нас все равны. Соответственно, всем плохо. А мы поехали к лекции 14.
Собственно, у нас лекция сегодня посвящена одному из приложений Фурье.
Ну, преобразование Фурье – это свертки. Мы поговорим какое-то время о свертках,
потом придем к одному классному применению, и в конце нас ждет небольшое путешествие в прошлое.
Свертки, которые по-английски называются Convolutions, потому что еще есть свертка,
которая называется Fold. Вот Fold – это не про это. У нас все-таки Convolution.
Для одномерного случая мы с вами предъявили на какой-то лекции, но там было очень размазано,
поэтому давайте четко проговорим. Определение. Пусть у нас есть две последовательности, два вектора.
Вот тогда мы определим свертку от C и T. Это будет тоже вектор,
который будет… Далее мы свертку обозначаем звездочкой, и компонент этой свертки определяется так.
A и J, B и J. У вас это и на семинаре было, и все такое. Однако это есть в экзаменационной программе,
поэтому я, опять же, лучше расскажу подробнее. Тут вроде все фиксировано, все окей должно быть.
Отклинивание очень простое. Давайте научимся считать. А именно что мы будем делать?
Понятное дело, что можно наивно за OOTM посчитать, но это как-то глупо. Потом,
когда мы перейдем к многомерным сверткам, будет понятно, почему это очень глупо.
А рассмотрим многочлен A от X и B от X. Ну что мы умеем делать с многочленами?
Мы уже научились их делить. Складывать вы читаете так умели. Давайте перемножим.
При перемножении может что-то интересное получится. Так обозначим C от X.
А от X на B от X. И теперь нужно всего лишь раскрыть скобочки. Их немного, всего лишь mn-штук.
Поехали. Я думаю, паттерн понятен. То, что у вас при X в катой степени берутся все попарные
произведения, у которых сумма индекса дает k. Вот здесь вот 0.2, 1.1, 2.0. Все дает в сумме 2.
Так, теперь что мы с вами сделаем? Рассмотрим с вами вот такую интересную операцию, как взятие
хв-1, коэффициент АС. Что это такое? Ну это а0 bm-1. Так, плюс останется здесь b1 a m-2 b1,
плюс a-1 b0. Вроде нигде не напутал. Что это такое? Давайте поймем, что это на самом деле
скалярное произведение таких вот векторов. B-reversing. Скалярное произведение этих двух векторов.
B-reversing имеется в виду просто разворачивать этот вектор задом наперед и уберете а0 bm-1,
а1 bm-2 и так далее. Можно выписать при следующей степени коэффициент. Почему нет? Доска большая,
времени много. Ну или же скалярное произведение. Ну и так далее. Можно продолжать и выписать их
все коэффициенты вплоть до при n-1. По сути дела, мы нашли все, что нам нужно с точностью того,
что b надо развернуть. То есть, если бы b у нас изначально был бы развернутый, то мы бы нашли
бы все вот эти вот значения явно, и все было бы прекрасно. Поэтому, чтобы найти свертку, вам
нужно b развернуть, дальше перемножить и взять коэффициент от n-1 до n-1 степени, соответственно.
Нет, почему? У вас всего лишь m компонент может быть. У вас длина вектора b, b0, b1, bm-1,
и в сумме у вас должно m давать. У вас не существует bm. Поэтому у вас не получится ничего сделать здесь.
Вот. Собственно, это то, как свертки считать в одномерном случае. То есть, первый шаг – разворачивайте
екту, второй – перемножайте многочисленно, третий – берете нужный коэффициент. А зачем это нужно?
Вот. Я, в общем-то, давным-давно еще, когда у нас потоки заканчивались, на последней
лекции про потоки, анонсировал программу по строкам. И я говорю с вами о том, что мы будем с вами искать
вхождение с опечатками. И давайте введем такую метрику.
С и Т – это строки. В модуле – равно модуль Т. Возьмем две строки равной длины. Тогда расстояние
хемминга между С и Т определяется как вот такая вот штука. То есть, это число позиций, где символы не совпали.
Ну, не знаю, вместо abobe написали abibo. Один символ различается, поэтому расстояние хемминга – один.
Вот. С существенным ограничением здесь является то, что у нас длины строк равны. Это, конечно, унылая вещь.
Вот. Есть более обобщенная метрика. Давай здесь напишем, что это расстояние хемминга. Вот.
Есть расстояние левинштейна. Вы, скорее всего, с ним сталкивались, когда вы проходили динамическое
программирование. В семинарии была задача на него. Я напомню, что расстояние левинштейна между двумя строками
уже произвольных длин. Это то, сколько можно сделать модификаций одной строки, чтобы перевести ее в другую.
Где модификация – это замена символа, вставка в произвольное место, удаление из произвольного места.
Вот. Здесь же мы таким не оперируем. Мы только считаем число несовпадений. То есть, в некотором плане это число замен букв,
можно сказать. Такое очень-очень суженное расстояние левинштейна. Вот. В жизни мы живем, где у нас строки не равны по длине,
вообще-то. И в целом как-то хочется оценивать похожесть одного вектора на другой вектор. Зачем-то нужно.
Вот. Чуть позже будет пример. Вот. А пока что давайте определим вот что. Обобщенное.
Хотя нет, давайте не будем его выводить, в принципе, нам не понадобится. Так. Какую задачу будем решать с вами?
Внезапно найти все вхождения паттернов текст с точностью до к ошибок. Вау.
Найти все вхождения п в т с точностью до к расхождения.
Решение, в общем-то, будет максимально незамысловатым и немножко использовать свертки.
Чем свертки хороши? Ну, тем, что они дают нам скалярные произведения.
А вот если у нас два бинарных вектора одинаково длинным, возьмем скалярное произведение, то что будет означать его значение?
Ну да, то есть то, насколько они похожи на самом деле. В некотором плане мы можем так измерить похожесть.
Давайте воспользуемся этим свойством. Таким наивным свойством мы решим эту задачу.
Наверное, сразу примерно понадобится.
И текст.
Вот.
И что мы будем делать?
Ну, давайте к зафиксируем.
Давайте так, как а.
Ну, думать зафиксируем. То есть мы будем все две ошибки допустить.
Окей.
Какой у нас план? Построим вот такой вот интересный вектор.
Так, давайте покрупнее.
Так, vs sigma.
Вот еще it.
Это индикатор того, что s it равен sigma.
Sigma это букву алфавита.
Давайте sigma zh sigma алфавит.
Ну да, здесь у нас алфавит из двух букв, а b.
Все очень приятно, потому что мне лень выписывать кучу векторов.
Поэтому пример такой.
Ну, давайте просто напишем эти вектора.
И все.
Так, v текст по букве a.
0, 1, 0, 0, 0, 1.
Соответственно, v текст.
Так, а по букве b буду писать правее.
Букве b выписываем 1, 0.
Букве b выписываем 1, 0.
И аналогично поступим с паттерном.
Окей.
Свернем.
Давайте сворачивать.
Так, ну этот накладываем сюда, получаем 0.
Этот накладываем сюда, получаем 1.
Этот накладываем сюда, получаем 0.
Да, 6-4+, получили нужный нам вектор.
Теперь здесь.
Текст по букве b.
Так.
Этот накладываем сюда, получаем 1.
Дальше накладываем сюда 2.
Сюда 1.
Окей.
Что это значит?
Что эти векторы свертки нам дают?
Это значит, что если мы приложим наш паттерн,
начиная с первой позиции, ну с нулевой,
то есть вот сюда вот.
Это значит, что по букве a у нас ноль совпадений.
Ну, действительно, у нас здесь все буквы a, тут совпадений нет.
А по букве v ровно 1.
А, вот оно.
Ну и так мы можем просуммировать по всем буквам.
Так, наверное, здесь еще будет видно.
Поэтому я построю вектор u,
который будет просто сумма
по всем сигмам.
Vp sigma.
Vt sigma.
В нашем случае u равен 1, 3, 1.
И он обозначает суммарное число совпадений.
Как раз таки с этой позиции.
То есть вот с первой позиции у меня совпадений лишь одно.
Тогда как со следующей позиции.
Тогда как со следующей позиции.
У меня совпадений внезапно 3.
Раз, два, три.
Ну понятно, как извлечь число расхождения.
Это нужно отнятие длины паттерна, значение этого вектора.
Ну и в итоге вот ваша позиция, которая вам дает ответ.
Надо оценить время работы этой штуки.
Ну и давайте мы скажем, что мы уверены, что длина паттерна меньше, чем длина текста.
Для сокращения вычислений.
Ну выражений точнее.
Так, окей.
Так, это на вычислении одной вот этой сверточки.
T log t будет.
Ну пройдем в том, что у нас сверточек sigma.
И здесь можно сказать, что можно применить хитрую технику под названием бинаризация алфавита.
А именно, что мы можем сделать?
Можем каждую букву закодировать двоичными символами.
0, 1.
А, это нулевая буква, у меня все 0.
Б, это первая буква, у меня там 0, 0, 0, 0, 1.
Вот.
Тогда что произойдет?
У нас алфавит станет бинарным 0, 1.
То есть здесь будет 0, 0, 0, 0, 0, 1.
Вот.
Тогда что произойдет?
Бинарным 0, 1.
То есть здесь будет log sigma.
Но еще у нас тогда все длинные строки увеличатся в log sigma раз.
То есть здесь будет t log sigma log t log sigma на log sigma.
То есть в некотором плане...
Ой, фу ты, здесь не будет log sigma, у нас будет вообще бинарный.
0, 1.
Здесь будет.
То есть после бинаризации вот этого штука константа,
а вот module t превратится в module t log module sigma.
То есть мы просто sigma превратили в ее логарифм.
И это уже круто, потому что не классно зависеть линейно от размера алфавита.
Все-таки некоторые люди обожают писать с майликами в чате,
а с майлики это далеко не буквы.
Вот.
У них там уже большие какие-то значения кодировок,
поэтому в общем-то удобно использовать бинаризацию.
Вы скажете, как тогда искать по-хорошему,
потому что все-таки там будут удлиненные какие-то штуки,
там будут блоки биты не совпадать, биты совпадать.
На что я скажу?
Постройте блоки, потом сожмите их просто и все.
То есть построите этот длинный вектор,
потом сожмите его отдельно по блокам.
И причем блок сжимает только все единички.
Вот.
Окей.
Самым простым закончили.
А давайте применение найдем этой штуке какой-нибудь.
Ну, как вы думаете,
где мы чаще всего в жизни сталкиваемся с какими-то непрерывными функциями?
Ну, исключаем от Ан, конечно же.
Окей, за пределами физтеха.
Ну, внезапно это музыка.
Вот.
И в чем суть?
Ну, музыка это какой-то сигнал.
Внезапно, да.
Мы все передаем как эти сигналы.
И можно этот сигнал дискретизировать,
просто какие-то там точки на графике разметить.
И мы внезапно хотим понять, насколько
использовали у нас там...
Вот.
Ну, я не знаю, кто-нибудь у вас делал видео на YouTube там.
Вот.
Если использовать какую-то музыку там, не свою,
то можно получить стаик за использование авторских прав.
Вот.
Ну окей.
Как искать вообще и банить видосы?
Вы думаете, там сидит там, не знаю,
тысяча человек и мониторит все видео, которые загружают на YouTube?
Я думаю, нет.
Вот.
Хотя бы миллионных надо по-хорошему.
Это значит, что и надо платить зарплаты,
и это очень грустно.
Ну, все-таки.
Мы живем в мире, где все честно.
Вот.
Соответственно, а как понимать вообще,
есть ли в музыке... есть ли в видео
похожий фрагмент по звучанию чем-то,
что нужно забанить?
Давайте свернем просто.
И все.
У нас же дискретизированный сигнал.
Давайте свернем.
Все. Окей, хорошо.
А как в шазаме искать...
Ну, в шазаме искать музыку все-таки сложнее.
Вот.
Хотя в целом все то же самое.
Вы просто перебираете банк вариантов
и просто их сверточкой прогоняете
и смотрите.
Подходит, не подходит, плюс-минус.
Вот.
Вот это применение одномерных свер.
Давайте выходить в 2D.
Да.
Это картинки.
А именно...
Давайте обсудим такую вещь.
Ну, у вас еще машинного обучения не было,
скорее всего. Ну, почти у всех.
Вот.
Там есть такая штука, как задача компьютерного зрения.
Это мы хотим картинки распознавать.
Если котик на ней,
в какую сторону он смотрит.
Там очень много всяких задач,
типа задач сегментации.
Хотим выделить на подорожной картине
все автомобили, отдельно знаки светофора,
отдельно какие-то другие знаки.
Хотим квалифицировать по изображению
все объекты нужных нам классов.
Это сегментация.
Можно также работать с последовательностью
картинок. Называется видео,
если вы не в курсе.
По задаче примерно те же самые там стоят.
Что там было?
Не знаю, это какое-нибудь веселое видео
или токсичное видео, которое надо забанить.
Можно такие классификаторы строить.
Соответственно, как работать с картинками
не очень понятно,
потому что это что-то очень-очень странное.
Картинка матрицы
какого-то размера.
Давайте считать пока что и просто матрица.
В общем случае, это тензор трехмерный.
И что с этим нужно делать?
С этим вообще не понятно, что делать.
Как это анализировать?
Потому что значение какого-то пикселя
зависит от соседних все-таки.
Это раз.
Два.
С картинками неудобно работать,
потому что непонятно,
как перевести в машине,
что тут есть кот, а тут собака.
И все. Сидишь и не понимаешь.
А ответ на это
дали сначала до появления
нейросетей всяких разных.
Были классические всякие алгоритмы компьютерного зрения.
Выделяем какие-нибудь контуры
картинок, дальше что-нибудь по ним классифицируем.
То есть упрощаем задачу.
И до появления
таких нужных вычислительных мощностей
нельзя было применять нейросети,
только они появились
в таком плюс-минус классном применении
с точки зрения масштабности.
2012 год был.
Есть такое соревнование классное.
Что-то ImageNet
как называется, или как-то так.
Его суть в том, что там просто огромный банк
изображений с разной одеждой.
Там нужно отличить это куртка,
это свитшот, пуловер, что-то такое.
И вот первый раз,
когда нейросеть показала плюс-минус
удобоворимый результат, это был 2012 год.
А его основой
был сверточный слой,
так называемый.
Первое слово, которое
в своем сочетании будем использовать
это convolutional layer.
Convolutional сверточный
layer-слой.
Вот почему слой?
Потому что нейросеть
на очень попном уровне это лего.
Лего, которое
вы просто ему выстраиваете граф вычислений,
вот у вас вход,
тут какой-то f,
тут какое-то промежуточное состояние x1,
какой-то там f2,
какой-то x2.
Вот такой выход
какой-то.
Там либо пометка, что там есть
котик или нет, или там что-то более интеллектуальное.
Вот.
Например, там по изображению
сгенерировать текст, что на нем написано.
Пожалуйста, это вот входит все вот
в эту прекрасную
абстрактную идею в нейросете.
Вот.
Собственно, вот эти вот f1, f2 и так далее,
они могут быть самыми разными,
и их есть несколько видов.
Вот в этой краске свертка.
И что такое свертка?
Давайте определим.
Так, определение.
Ну и здесь давайте перейду сразу в 3D.
Почему в 3D?
Потому что на самом деле картинка,
вы ее раскладываете обычно по rgb,
к спектру.
Ну, красный, синий, зеленый, то, что у вас там в глаза видит.
Вот.
В каких-то пропорциях смешивая,
вы получаете там цвета всевозможные.
Вот.
Поэтому давайте определим так,
что картинка у нас будет вообще
трехмерная штука.
Мы напишем вот так вот
результаты в свертке.
Изображение
a
размеров
w на h
на c. c это число каналов.
Если у вас черно-белая картинка,
то канал 1.
Просто степень
от того черного к белому, сколько вы переходите.
Вот.
0,5 это полусерый какой-то.
Вот. А в нашем случае rgb
это будет их 3.
Но на самом деле у Нью-Йорксетти много больше каналов есть.
Потому что там какие-то каналы будут отвечать
за отдельные кусочки картинки, на самом деле.
То есть там,
если мы хотим распознать транспорт,
то один канал
будет там, условно, отвечать за то,
чтобы вычинять оттуда турбину самолета.
Другой канал будет отвечать за то,
что мы можем вычинять колеса велосипеда.
Вот.
И ядра
фильтра
по-английски kernel
ядра b
размерами
вот таких вот.
Так, давайте,
чтобы было c. Да, у них
число каналов одинаковые, но соотношения
различны будут.
А именно
w1 меньше, чем w,
h1 меньше, чем h.
Вот.
Давайте
представлять это картинкой.
Так, окей.
То есть наше изображение
это что-то вот такое.
Вот.
Аккуратней чуть-чуть.
Давайте его на пиксели
разобьём с ядрой.
Вот.
Аккуратней чуть-чуть.
Аккуратней чуть-чуть.
Аккуратней.
Аккуратней.
Это такая картинка.
Я здесь вглубь не рисую,
здесь будет число каналов
в третьем измерении.
В некотором плане можно отметить, что это ось w, а это ось h.
Есть ядро
ну, вот маленьким давайте.
Три на три.
И что получается?
Берут и прикладывают это вот ядро
во всех возможных местах.
То есть прикладывают в левый верхний угол картинку
три на три.
Дальше, что у вас получается?
Это значит, что вы приложили какой-то вот кубик
к кубику,
берёте скалярные произведения вдоль
третьей компоненты
и суммируете их все.
Это получается значение на выходе
в этом пикселе соответствующем.
Поэтому давайте запишем здесь так, что
а-па-па-пам
давайте d
и ты житый.
Это будет что такое?
Это сумма по всем u
от 0 до
w1-1.
Сумма по всем v
от 0 до h1-1.
Скалярное произведение вот таких вот векторов.
a
i плюс u
плюс v
b и
buv.
Вот такая вот страшная конструкция.
Казалось бы, всего лишь приложили
прямоугольничек к прямоугольничку, посчитали,
снова к прямоугольничку, посчитали
и так далее ко всем возможным местам прикладываем.
То есть вот у нас первое место приложения,
дальше на 1 сдвинулись второе место
приложения,
теперь на 1 вниз. Третье место приложения,
четвертое,
еще на 1. Пятое,
шестое.
Ну давайте
вот
я сказал сразу, что это именно
что вектора. Это как раз произведение векторов.
То есть у вас вектора берутся в этом измерении.
Здесь, на самом деле, еще есть c.
Здесь вот тоже есть такая штука c.
То есть да, на самом деле трехмерные штуки
и это очень страшно.
Давайте распишем до конца.
Нет, здесь у нас места не хватит. Давайте я
перейду на эту доску.
Так.
Сумма по u.
W1-1.
Сумма по v.
H1-1.
Сумма по g.
От 0 до c.
A
i плюс u.
G плюс v.
А, g у нас занята.
Блин.
А, g у нас занята.
А, g у нас занята.
А, g у нас занята.
Букла k у нас еще вроде не занята.
B у ты
в этой
катой.
Ну,
выглядит громоздко.
Давайте допустим,
что мы бы это считали наивным
путем. То есть явно бы все
форы прописали.
То есть получилось бы, что для вычисления
одного пикселя результирующей картинки
нам пришлось бы
4, 4, 4.
А этих пикселей
много. То есть сразу
получилось что-то типа
w1
на w-w1
на h1
h-h1 на c.
Ну, давайте оценим.
Скажем, что
w1 оширен порядка половины,
чтобы
огрубить вычисления.
Ну, какого у нас качества
фотографии современной?
Много.
Я не очень знаю числа, но окей.
Там картинка anti-full hd, да?
1080 на 720,
наверное. Ну, плюс-минус.
То есть
Full HD всего лишь просто,
чтобы вычислить одну свертку.
Это число каналов
умножить на
1080 квадрат
на 720 квадрат.
Ну, будто бы не очень
много, да? Ну, тут порядка миллиона,
тут чуть меньше миллиона.
Просто hd.
Вообще прикиньте, насколько
мир отстал. Ну, или я отстал,
я не знаю. Одно из двух.
Вот. Ну, выглядит
плюс-минус приемлемо, да?
Ну, сколько тут? Ну, давайте скажем, что
700 тысяч, да?
700 тысяч это
49 на 10
десятый.
Ну, ни о чем, да? У вас в контесте
примерно такие задачи залетают.
Вот.
Учитывая то, что у нас там порядка
не знаю, там сколько гигагерцов
процессоры в современных там телефонах, например,
или что-нибудь таком.
Ну, давайте даже поделим на 10
девятый. Получим, что что-то типа
3 на 49. Ну, еще на 10
поделим, с запасом, чтобы было.
Получим порядка 15 секунд.
А проблема
в том, что это всего лишь одна свертка.
На самом деле сверточный
слой, он выдает вам чуть больше
информации. А именно он выдает
еще f каналов
на самом деле. То есть запускается
f сверток разных на одном
изображении с f разными ядрами.
Поэтому сюда еще f надо
добавить на самом деле. В этим
точку. И это вычтение лишь
одного куска свертки.
Вот миросетка, которая в 2013
году работала, она называется AlexNet.
И там у нее глубина
порядка 7 вот таких вот сверточных слоев.
Но вот как вы думаете,
если подать одну картинку,
наверное, это не круто, что она будет там
с точностью хуже, чем человек, там
через несколько минут выдавать тебе
эта куртка. И те такие, нет, это пиджак.
Все, технология бесполезна.
Ну вот, казалось бы,
надо что-то другое делать.
И теперь вам нужно посмотреть
внимательно на выражение вот этих вот трех сум
и кое-что заметить.
Ой, не тот маркер.
Так, ну я накрою мне
определение с начала лекции.
У нас была такая вот штука интересная.
Давайте по u от 0
до m-1.
a и
plus j, b и j.
Это одномерная
свертка.
Ой, и плюс j. Какой и плюс j? И плюс u.
И плюс u,
j, plus v, v.
Ну и k в придачу.
То есть это свертка,
это свертка, и по этой штуке свертка.
Трехмерная свертка перед вами. Вау.
Вы думали, это нигде не нужно?
Вот, оказывается, это очень даже нужно.
Ну и что?
k и k плюс 0.
Сумма по одному слагаемому.
От 0 до 0,
нормально же.
Вот. Ну давайте выпишем,
тогда время работает.
Давайте скажем, что это
трехмерная свертка.
И выпишем
симпточку. Тогда это будет
от
w h
лог w h
на c.
w h,
у нас что это такое?
18720,
порядка 700000.
Умножаем на лог 700000,
это что-то, укладывающееся в одну секунду,
уже круто, да?
У нас это укладывалось
в разы больше, да.
Ну еще цешечка здесь.
Понятное дело, что это все распараллеливается,
там всякие вычления эффективные
на ГПУ и так далее.
Это вот на третьем курсе будете писать такое.
Ну и не такое, там попроще,
там будут матрицы перемножить,
на ГПУ распараллелить.
И там получается, что здесь сильно падает
констант из таких грубых вычислений.
Но, как видите, уже мы сократили на порядок.
И это круто,
очень.
И в целом позволяет нам хоть как-то
обрабатывать изображение.
Чего-то круче вам предложить
пока что не могу, потому что, по сути,
мы достигли в некотором плане
вершины эволюции с точки зрения симптотики.
С точки зрения константа, конечно же,
но у нас всегда там надо все нормально оптимизировать,
чтобы все работало быстро.
Но мы не этим занимаемся,
потому что про всякие вычтения на ГПУ
это вообще отдельная наука,
которой надо прям уметь.
В некотором плане, потому что у вас все равно
вот это останется циклом
пока, все равно, так или иначе.
Вы могли бы написать
только ц сюда.
Вы могли бы сюда под логарифм еще ц закинуть.
Но зачем? У вас же ц
выполнен лишь один раз.
То есть у вас, по сути,
в некотором плане хоть и трехмерная свертка,
и писать-то надо как трехмерную свертку хорошую.
Можно писать как двумерная свертка,
просто ц раз.
Ну и все.
То есть двумерная свертка по каждому каналу,
потом суммируете. Ну потому что
у них есть прикол в нейросетях,
потому что там не нули денечки живут.
Там живут какие-то вещественные числа,
которые отображают интенсивность
того, что мы видим
в некотором плане.
Интерпретация этих компьютерных
все это вообще отдельная наука.
То есть здесь в этом плане
нельзя сказать, что это нули и один,
потому что это не бинарный фактор.
Это какой-то фактор, который отображает степень того, насколько
в данном канале изображение
похоже на то, что мы хотим.
Грубо говоря так.
Но опять же мы опустились
до оверноучпопного уровня,
потому что можем. Все-таки мы про алгоритмы
разговариваем.
Оказалось бы, свертки это
конечно круто, но иногда нужно
уметь и другие преобразования делать.
Но самое простое это банально зашумить картинку.
Вы хотите там размытый фонт сзади в зуме?
Явно никто не будет какие-то
свертки строить. Там другие преобразования.
Вот. Это второй тип слоев.
Это pulling layer, так называемый.
Их два вида.
Больше на самом деле,
но в основном нас будут интересовать два.
MaxPulling и
averagePulling.
Average это среднее.
Max это максимум.
Соответственно, окей.
Давайте вернемся
в далекие-далекие времена.
Вспомним то, с чего начинался весь наш курс.
Прям в самом сентябре.
Помните, про что первая лекция была?
Еще.
Во, префиксная сумма.
Это хорошо.
Что на второй лекции было?
Во, контейнер.
Классно.
Сейчас я буду выписывать
формулы кокстрен
average и maxpulling и вы будете говорить, как решать.
Эффективно.
Начнем с простого.
Окей.
AveragePulling.
Он принимает
на вход в себя
изображение
w на h на c.
Это изображение
w на h на c.
Это изображение
w на h на c.
Это изображение
h на c.
Это изображение.
И еще он принимает в себя
две чиселки.
Давайте ab.
Это будет a.
Тогда результат b
будет вот такая вот штука.
b и j.
Определяется, как
это будет.
Давайте чуть-чуть.
Еще запишу третье измерение.
Но опять же в виде питомской нотации
оставлю.
Кому?
Не, там в каждом канале
отдельно будет браться.
Так.
А, ну я молодец.
Я круто обозвал
букву.
Здесь будет a
и plus u.
J plus V.
И по третьему измерению
берется по всем каналам.
Это формула для усоединенного
пулинга.
Как считать будем эффективно?
Да, это двумерные
префиксные суммы.
Просто это потом будет поделить на ab.
Все.
Ну и давайте последний компонент,
чтобы мы могли построить
игрушечную нейросетку.
Это MaxPulling.
Пулинг оно
что-то, грубо говоря,
фильтрует или вытаскивает
какие-то фичи.
Изображения.
AveragePulling он усредняет
в плане того, что мы там
не сильно теряем информацию
со всех каналов,
но при этом уменьшаем размер изображения,
чтобы наши алгоритмы быстрее работали.
Потому что опять же в изображении,
когда оно очень высокого качества,
очень много лишнего.
Вот, например,
если я сфоткаю аудиторию
и мне нужно будет сказать, правда ли,
что это учебный класс, мне не нужно будет
процентов 99 информации.
Вот, окей.
Я думаю,
тогда понятно, что такое MaxPulling.
Он строится очень идеально просто.
Ну, здесь давайте определим
его по каждому каналу.
Потому что если, к сожалению, уже не помню,
как это называется,
по каждому каналу или нет.
A
I plus U
J plus V
Вот так вот. Это как считать?
ДО не предлагать.
Сразу.
Не считается.
Эффективнее надо.
Нет.
Там строить долго.
Строить долго.
Я не зря напомнил вам про вторую лекцию.
Вспоминайте, что у нас там было.
Нет.
Вы сами сказали, стэки, очереди были,
контейнеры.
И теперь вы отошли от этого.
Нет.
Помните, у нас была очередь с максимумом.
У нас был стэк с максимумом
и очередь на двух стэках.
Там как бы нужно было раз-два
хоба и сдать задачу в контест.
Примерно.
Вот. Это тоже самое, по сути.
Только это двумерный вариант,
и он делается вообще не сложнее одномерного.
Поэтому здесь мы получаем
асимпатическую сложность на самом деле
от параметров.
Аж на С.
То есть просто
именно сколько нам дали ресурсов,
столько мы затратили по времени.
Аналогично здесь.
Ну все, мои начинающие
исследователи в компьютерном зрении,
вы построили игрушечную нейронку.
Я вас поздравляю.
Конечно нет, но почему?
Потому что у третьего курса уже было машинное обучение.
И они должны знать,
что нейросети по-хорошему...
Вот тут была цепочка входов.
Х0, х1, х2.
Аут.
И там проблема в том,
что вам нужно уметь вычислять...
Тут функция ошибки задается какая-то.
То есть то, насколько мы не похожи
на ожидаемый результат.
Называемый L
от XN.
L loss, потому что.
И типичным методом обучения нейросетей
это нужно брать градиенты этой штуки
по всем вот этим вот слоям.
Проблема в том, что
ну вот, первое, здесь нужно брать производную сложную функцию.
Это очевидно.
Но это не главная проблема. Проблема в том, что
не очевидно, что свертка дифференцируема
на самом деле.
Это первое. Хорошо, окей.
На самом деле она очевидно дифференцируема,
потому что просто какая-то линейная комбинация чего-то.
И все линейные преобразования дифференцируемы.
Ну, вы просто берете
и берете вот по каждому B
УВК, частную производную,
и считаете градиент. Ну и там как-то изменяете.
Вот это называется процесс обучения
нейросети.
Ну, казалось бы, тогда нам нужно
там C, W,
аж квадрат, вот там как вот на
самого верху написано операции сделать по-хорошему,
чтобы их вычислить. Оказывается, что нет.
И градиент свертки выражается через свертку.
Вот. Это забавный факт.
Выводить я его не буду, потому что
все-таки у нас курс не про
машинное обучение.
Но кому хочется, в конспекте есть ссылочки на все статьи
с выводами. Можете почитать.
Вот.
То есть смотрите, что мы с вами сделали
по-хорошему.
У нас сегодня с вами последняя лекция.
Мы с вами узнали качество
трехмерной свертки.
И начали строить нейросети.
Потом мы вернулись к самым, к самым, к самым
базовым истокам.
И закончили строить нейросети.
Вау, курс замкнулся.
Я поздравляю вас с этим.
Можно, конечно, вспомнить все, с чем мы начинали
и чем мы закончили. Однако это
оставлю все-таки вам поностальгировать в чате.
Перечитать условия первых задач.
Как вы долго потели над код-стайлом.
Но почему-то у вас-то до сих пор происходит.
Вы до сих пор
не научились настроить линтеры сами.
В целом все. Можно расходиться.

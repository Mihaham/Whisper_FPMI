Так, добрый день. Сегодня у нас предпоследняя уже лекция получается.
Мы поговорим подробно про верояностные вычисления.
Ну а на последний, видимо, будет какое-то окончание про вероятность.
Да, и еще про некоторые темы поговорим.
Вот так, вкратце напомню, что мы изучали в прошлый раз.
Мы изучали в целом понятие вероятных вычислений и несколько примеров.
В случае, когда ответ бинарный, да или нет, то верояностные вычисления означают, что есть некоторая вероятность и ошибки.
То есть может быть так, что настоящий ответ нет, а алгоритм отвечает да, ну или наоборот настоящий ответ да, а алгоритм отвечает нет.
Соответственно, мы изучили несколько примеров.
Пример проверка числа на простоту и проверка многочленов на равенство.
Лему Шварца-Зиппеля, ну ладно, я думаю, можно пропустить.
Интуитивно, по крайней мере, должно быть понятно, что два многочлена редко совпадают в большом числе точек.
Ну и сейчас начнем с того, что поговорим про верояностные классы.
Мы немножко поговорили про верояностные алгоритмы, про верояностные задачи.
Теперь поговорим про верояностные классы.
Ну и начать на самом деле с того, что такая модель используется.
Модель вычислений – это верояностная машина тьюринга.
Ее можно по-разному определять.
Формально сама по себе машина как математический объект.
Это то же самое, что нетерминированная машина.
Формально как математический объект.
Верояностная машина – это то же самое, что недетерминированная.
То есть там тоже есть всеобычные множества, там множество символов алфавита, множество состояний,
какие-то выделенные состояния и функции перехода, которые являются многозначными.
Функция перехода является многозначной.
Но отличие следующее.
Недетерминированная машина магически угадывает правильную ветвь вычислений.
Правильный переход в каждый момент.
А верояностная просто идет по случайной ветве.
Соответственно ответ верояностной машины – это про случайная величина.
Удобнее всего предполагать, что каждый раз у нас есть два возможных перехода.
Удобнее всего предполагать, что функция двузначная.
То есть каждый раз есть два допустимых следующих шага.
То есть в каждом состоянии есть два допустимых шага.
Ну и тогда не нужно возиться с тем, как это работает.
То есть в каждом состоянии есть два допустимых следующих шага.
Ну и тогда не нужно возиться с тем, какое распределение вероятностей.
Если каждый раз всего два варианта, то просто каждый из них будет равновероятен независимо от других.
Либо альтернативно, как и с недетерминированной машиной можно говорить терминных сертификатов.
Альтернативно есть просто два аргумента.
М от XR.
X это собственно аргумент.
А R это случайные биты.
Ну а уж откуда эти случайные биты берутся, это не очень важно.
Может быть они в самом начале сгенерированы, может они генерируются по ходу дела по одному.
Может быть не в самом начале, но в бесконечном количестве.
Вот эти вариации чуть-чуть к разным вариантам приводят, особенно когда мы считаем память, особенно когда она логарифмическая.
Когда речь идет только о времени, то это все неважно.
Теперь все классы построены вокруг двух вероятностей.
Говорит, что если X лежит в A, то тогда вероятность по случайному R, что машина от XR выдаст единицу.
Ну какая это сейчас я попозже напишу, потому что это для разных классов по-разному.
Ну и соответственно, если X лежит у вас чертой, то тогда получается, что вероятность по R,
что M от XR равно единице, тоже какая-то.
Соответственно, идея в первом случае должна быть большая, а вероятность в втором случае маленькая.
Ну и теперь посмотрим на конкретные классы.
Ну тут начнем с самого важного класса.
Самый важный класс это класс BPP.
Так, значит, если BPP, когда здесь будет больше чем 2 треть, а здесь будет меньше, чем 1 треть.
То есть, соответственно, вероятность, получается, близко к единице, какие-то пороги должны быть.
Значит, этот класс с двухсторонней ошибкой.
Вообще BPP расшифровывается как bounded error probabilistic polynomial.
Соответственно, bounded error означает, что ограниченная ошибка, то есть вероятность того, что ответ неправильный, будет меньше, чем 1 треть.
При этом, что важно, это ошибка двусторонняя. Здесь больше 2 третьей, здесь меньше 1 треть.
Так, дальше. На самом деле, если вспомните, что мы вот тут проходили, и в случае многочленов, и в случае простых чисел,
получалось, что, соответственно, в одном случае ошибки точно не было.
То есть тут, в случае теста Милля Рабина, простое число точно признавалось простым, а составное, с ней сравнится, тоже было простым.
В случае многочленов, тут получалось, что если многочлены одинаковы, они точно будут признаны одинаковыми.
Если они не разные, то с небольшой вероятностью могут тоже быть признаны одинаковыми.
И с этим связаны два класса с затронившей ошибкой.
Один называется RP, и это означает randomised polynomial.
В данном контексте так сложилось, что randomised и probabilistic – это разные слова.
Так, если RP, то тут будет больше, чем 1 вторая, а тут просто равно 0.
То есть получается, что если реальный ответ «да», то и машина скажет «да» с хотя бы 1 вторая.
Если реальный ответ «нет», то машина точно скажет «нет», и ни за что не скажет «да».
Есть симметричные случаи – это QRP.
Это означает, что здесь равно единице.
Это как раз случай простых чисел. Если число простое, то точно скажут, что оно простое.
Если составное, то непонятно.
Тут, соответственно, будет меньше, чем 1 вторая.
Это QRP.
Теперь есть класс просто PP без буквы B.
Без буквы B – это с неограниченной ошибкой.
Но если разрешить ошибку 1 вторая и там и там, то тогда у нас что угодно сюда подойдет.
Мы всегда можем просто кидать монетку и такой ответ давать.
Поэтому так не подойдет.
Например, можно написать больше или правдно в 1 и 2.
Тут, соответственно, строго меньше, чем 1 вторая.
Чем PP лучше, чем BPP?
Больше он тем, что какая бы ни была машина, она какой-то язык определит.
Потому что здесь проблема в том, что машина может выдавать не больше 2-х третьей, не меньше 1-х третьей,
а что-то промежуточное, тогда такая машина вообще никакого языка не определит.
А вот в случае PP получается, что любая машина какой-то язык определяет.
Дальше еще есть важный язык ZPP.
ZPP это zero error, нулевая ошибка.
Пробавилистик polynomial.
Здесь, соответственно, тут будет равно 1, тут будет равно 0.
Почему это не просто P?
А дело в том, что в этом случае zero error получается только ожидаемое время работы.
Полиномиально.
Да, в худшем случае может быть экспоненциально.
Или даже в принципе больше.
Ну и вообще-то сюда же можно еще отнести NP и QNP.
Да, можно писать, что NP.
В случае NP тут будет здесь больше 0, а здесь равно 0.
Ну и в случае SQNP тут будет равно 1, а тут будет меньше 1.
Вот такие классы.
Дальше определение у всех одинаковое.
То есть язык A лежит в классе одном из этих.
Есть существует такая машина M, вычисляемое заполиномиальное время,
что, если X лежит в A, то вероятность того, что машина вы достанется,
нужно брать сверху соответствующие условия,
а если X не принудится, то вероятность, соответственно, нужно брать снизу условия.
Ну, соответственно, понятно, почему NP и QNP сюда подходят.
Потому что в NP, если ответ да, значит такой R есть.
И значит, можно его случайно взять на расположительную вероятность.
Если такого R нет, то его случайно взять нельзя.
Одну секунду.
Тут можно теперь поговорить про то, как эти классы соотносятся друг с другом
и с разными другими классами, которые мы уже изучили,
которые связаны со схемами, с полиномиальной иерархией,
с полиномиальной памятью и так далее.
Но давайте нарисую некоторую диаграмму.
Ну, в самом низу у нас будет класс P.
Значит, класс P, которого тут, конечно, нигде нет.
Значит, дальше будет класс ZPP.
И он не просто будет, а у нас будет равняться пересечению.
И у нас будет равняться RP в пересечении с QRP.
Получается, P, конечно, вложено в ZPP,
потому что можно случайно это вообще игнорировать,
просто вычислить ответ.
Дальше чуть выше будут самети.
Я буду соблюдать цветовое кодирование.
Самети RP и QRP.
Так, 4P.
И тут QRP.
So, QRP.
Так.
4RP.
И QRP.
Значит, дальше будет получаться, что здесь...
Здесь bpp, bpp, np, np co-np, и на самом верху pp.
Вот, соответственно, тут получается вот так вот.
Значит, тут получается вот так вот.
Значит, вот так вот, вот так вот.
Так, и, соответственно, вот так вот.
Вот, ну и самое вверху можно еще, скажем, pspace.
Значит, вот здесь можно написать pspace, который тут точно над всем этим будет.
Так, ну вот, в общем, вот такая вот картинка.
Некоторые из этих ложений совершенно очевидны.
Значит, сначала очевидные обсудим, потом будем доказывать неочевидные.
Так, ну а то, что p вложено в zpp.
Ну, мы вроде уже обсудили, что можно просто взять тот же самый поэполиментальный алгрит, который дает точный ответ.
И, соответственно, просто игнорируя случайные биты, тот же самый ответ выдавать.
В общем, мы понимаем, что время будет и в худшем случае, да, время будет и в худшем случае пальномиальное, и ожидаемо будет пальномиальное.
И, в общем, вот это вот более-менее понятно.
Так, теперь то, что zpp...
Ну, не, значит, это вообще терминес, немножко сложнее, наверное, попозже докажем.
Ну, давайте я, наверное, буду все вложения кратко описывать.
Они, в общем-то, не то, что совсем очевидно.
Так, значит, p вложено в zpp.
Значит, тут просто игнорируем r, используем старый алгоритм.
Так, значит, rp вложено в np.
Ну, это как раз очевидно.
На что, если вероятность больше, чем одна вторая, то она больше нуля.
Если внимательно посмотреть на то, что у нас вот здесь вот, то есть тут у нас равно нулю, равно нулю, тут больше одной второй, тут больше нуля.
Конечно, ясно, что из того, что больше одна вторая следует.
Ну и на самом деле аналогично то, что qrp вложено в qnp.
Да, то есть тут qrp вложено в qnp, это аналогично.
Ну и на самом деле то, что bpp вложено в pp, тоже аналогично.
Вот, а именно, значит, если больше двух третьей,
Следователь, да, больше одной второй.
Ну, а меньше одной третьей, значит, меньше одной второй.
Меньше одной третьей.
Меньше одной третьей.
Следовательно, меньше одной второй.
Так, теперь.
Ну вот, например, rp вложено в bpp.
Значит, rp вложено в bpp.
Значит, это происходит, потому что, ну а тут уже так просто не получится.
Потому что, смотрите, конечно, если равно нулю, то это меньше одной третьей.
Но если больше одной второй, то еще не знаю, что больше двух третьей.
Но можно амблифицировать, то есть можно запустить два раза.
Ну, допустим, алгоритм дважды.
И в качестве результата возьмем
Вот, тогда получается, что если x, значит, если x лежит у вас чертой,
если если лежит у вас чертой, то тогда получается, что вероятность, значит, вероятность того, что m от xr равно нулю
и m от xs равно нулю.
Значит, это будет произведение вероятности, значит, вероятность того, что m от xr равно нулю.
Умножить на вероятность.
Тут m от xs равно нулю.
В общем, это ноль. Да, потому что 2 нуля умножаем.
А, не, подождите, тут же дизью мы писали.
Так, тогда лучше, нет, не так надо писать, надо писать, что не равно умножить, а меньше либо равно, чем сумма, которая все равно ноль.
Так, меньше либо равно, чем сумма, которая равна нулю.
Ну и понятно, что если у нас вероятность m, значит, она ноль.
Так, если x лежит ва, значит, тогда вероятность...
Ой, подождите, извините, я все правильно писал. Извините, простая вещь.
Потому что тут же у нас что мы считаем? Мы считаем вероятность...
Ой, нет, извините, что я запутался.
Надо вот тут равно единице, дизьюнце.
Вот, да, теперь правильно. То, что на первом запуске равно единице или на втором запуске равно единице.
Вот, значит, теперь если x лежит ва, если x лежит ва, тогда вероятность того, что m...
То же самое, да.
m от xr равно единице или m от xs равно единице.
И вот тут нужно сказать, что это 1 минус вероятность того, что m от xr равно нулю.
И m от xs равно нулю.
И вот это равно, значит, 1 минус. Вот тут уже можно взять произведение, потому что это независимая реализация.
m от xr равно нулю.
Множить на вероятность того, что m от xs равно нулю.
И вот это получается больше.
Значит, вот это получается меньше 1 и 2. Это меньше 1 и 2. Мы это перемножаем, берём с минусом.
Это будет больше, чем 1 минус.
Одна вторая, умножить на одну вторую.
Вот это 3 четверти.
Вот, и это больше, чем 2 третьи.
Вот, да, ну, в общем, немножко круто получилось, на самом деле очень просто.
Просто если много раз запускаем один тот же тест, то результат получает всё более-более надёжный.
Так, начинается, получается qrp.
qrp вложено в bpp.
Просто аналогично.
Так, это мы вот это вот.
Ещё пять вот этих связок остаётся.
Почему np вложено в pp?
np вложено в pp.
А тут это вот почему.
Тут получается, что мы берём такую штуку v от x.
Пусть у нас v для np, а m мы для pp строим.
m от x и один бит, и оставшийся r.
Это будет равняться следующему.
Единица. Единица, если sigma равно нулю.
Вот, значит, ar не равно слову из всех нулей.
Дальше 0, если sigma равно нулю.
ar тоже из всех нулей.
И дальше будет v от xr, если sigma равно единице.
Теперь смотрите, тут ровно половина.
Половина без одной штуки единиц возьмётся вот отсюда.
Теперь смотрите, получается, что количество таких пар sigma r,
что m от x и sigma r равно единице.
Это будет там 2 в степени q минус 1.
И плюс количество таких r, что v от xr равно единице.
Соответственно, если у нас x лежит 2,
то вот эта штука больше единицы, больше нуля.
Ну, больше равно единицы получается.
Больше либо равно единицы.
Ну и получается, что такая штука будет больше равно, чем 2 в степени q.
А это равно 1 и 2 умножить на 2 в степени q плюс 1.
Ну а соответственно, хотя половина получается.
Если x лежит 2 с чертой, то эта штука равняется 2 в степени q минус 1.
Это будет меньше, чем 1 и 2 умножить на 2 в степени q плюс 1.
Ну вот, поэтому NP вложена в PP.
Ну и CoNP вложена в PP аналогично.
Вообще в этой картине все симметрично.
Все симметричные вложения делаются точно так же, как исходные.
Ну что у нас осталось? PP вложена в PSPACE.
Ну это очень легко, на самом деле.
Что просто на памяти, которая излина r, можно перебирать все r.
Значит, запускать m от xr, подсчитывать долю единиц и сравнивать с половиной.
Долю сравнивать с одной-второй.
Вот, поэтому PP вложена в PSPACE.
Единственное, что у нас осталось, это ZPP равняется rp пересечения square p.
Так, ну это из двух частей.
То есть лево-направо есть вложение, и справа-налево есть вложение.
Так, ну почему ZPP вложена в rp?
Значит, смотрите, пусть среднее время работы равно какому-то TAT.
Тогда, поскольку время от величины не отрицательное, то можно применить неравенство Маркова.
По неравенству Маркова, вероятность того, что время работы больше, чем два TAT, будет меньше одной-второй.
Вот так получается алгоритм такой.
Значит, запустим алгоритм на два TAT шагов.
Если остановился, то выдадем тот же ответ.
Если не остановился, то выдадем ноль.
Смотрите, что получается.
Смотрим на определение.
Если ответ у нас вытянут, то он точно правильный.
Дальше что получается?
Если мы запускаем на два TAT, то, скорее всего, время успеет завершиться.
Причем тут важно это ожидаемое время.
Это означает, что для любого фиксированного X среднее время усредненное по r будет пальномиальным.
Это не усреднение между X.
Это именно для каждого X усреднение по случайным битам.
Но получается, что если ответ один, то сериальность больше, чем одна-вторая, успеет завершиться.
И выдаст один.
Поэтому тут больше одна-вторая будет.
Если ответ ноль, то в любом случае будет ответ ноль.
Либо потому что ZPP завершился, либо потому что мы не нашли ответа.
Тогда мы выведем ноль.
Дальше ZPP вложено в QRP.
Значено.
Но выдаем в конце единицу.
И тогда неопределивший перерастывает на другую сторону.
Это же все правильно.
Поэтому слева направо вложено.
И остается в другую сторону.
Почему QRP вложено в ZPP?
Тут дело вот с чем.
Смотрите.
Настройной ошибкой нужно быть аккуратным.
Потому что тут немножко раньше нулевую с другой стороны.
Если не лежит ва, то точно машина выдаст ноль.
Но если она выдала ноль, то это не значит, что она точно лежит ва.
Наоборот, если она выдала один, то она точно лежит ва.
А если она выдала ноль, то она может быть и отсюда, и отсюда.
Получается такая двойственность, что если не лежит ва, то точно выдаст ноль.
Но если выдала ноль, то не значит, что точно не лежит.
А наоборот, если выдала один, то точно лежит.
Хорошо, пусть у нас язык лежит в пересечении.
РП в пересечении QRP.
Пусть А лежит в РП в пересечении QRP.
Что это значит?
Значит, что есть два алгоритма V и W.
Со следующими свойствами.
Если у нас X лежит ва, то, следовательно, вероятность того, что V от XR равно единице,
значит, это будет больше, чем 1 вторая.
Если X лежит ва с чертой, то тогда вероятность того, что V от XR равно единице, это равно нулю.
И, соответственно, также, если X лежит ва, то тогда вероятность того, что W от XS равно единице, это равно единице.
Значит, если X лежит ва с чертой, то тогда получается, что вероятность того, что W от XS равно единице, это будет меньше, чем 1 вторая.
Можно на это самое еще посмотреть.
Значит, если W от XR равно единице, то тогда получается, что X лежит ва с чертой.
Нет, тут, наоборот, равно нулю.
Их лежит ва с чертой.
Ну и получаем такой алгоритм.
Значит, запускаем V от XR и W от XS.
Ну и получаем такой алгоритм.
Так, да, прошу прощения.
Хорошо, значит, запускаем V от XR и W от XS.
Вот, соответственно, если V от XR равно единице, то возвращаем единицу.
Значит, если W от XS равно нулю, то возвращаем ноль.
Вот, а если не то, не другое, то просто запускаем еще раунд.
Вот, что же получается?
Получается, что вероятность в любом случае, независимо от того, лежит ли X ва,
будет верно следующее, что вероятность того,
что V от XR равно нулю и W от XS равно единице, значит, это будет меньше, чем 1 вторая.
Вот, то есть в каждом раунде вероятность завершения
будет больше 1 второй.
Ну а то есть стандартная задача, значит, тогда среднее число совершенных раундов
будет меньше двух.
Так, значит, среднее число совершенных раундов будет меньше двух.
Ну а тогда, поскольку каждый раунд полинамиален,
то среднее число шагов тоже полинамиально.
Ну вот, поэтому, значит, получилось, что в ZWP это будет лежать.
Вот, но в принципе есть такая вариация определения, что здесь у нас, в принципе,
возможно бесконечная ветвь, то есть может быть так, что адрес вообще никогда не закончит работу.
Ну а на самом деле можно принудительно обрезать, да, если нам очень долго не везет,
то мы можем просто полный перебор экспоненциально организовать какой-то момент.
И если у нас с экспоненциальной малой вероятностью происходит экспоненциально большой перебор,
то это в среднем ничего страшного.
Так, ну ладно, есть какие-нибудь вопросы.
Получается, что мы изучили вот такую картину из классов.
Так, что еще надо сказать?
Ну, значит, есть теперь общая идея амплификации.
Идея амплификации, значит, уменьшение ошибки.
Значит, например, если рассмотреть класс языков А,
для которых при х лежит ВА, верно, что вероятность,
значит, равна единице будет больше ноль, чем какая-то там альфа.
А при х лежащем ВА с чертой верно, что вероятность того, что м от х равно единице, равно нулю,
то при любой константе альфа, но только строго от нуля до единицы,
то при любой константе альфа от нуля до единицы получится также РП.
Вот, более того, альфа может зависеть, значит, может быть функцией от n равного длине х.
И при любом альфа, который будет от единицы делить на полином от n до единицы минус один делить на два в степени полином от n, будет все еще РП.
Вот, значит, для БПП будет аналогично.
Значит, для БПП будет так.
Да, значит, пусть у нас при х лежит ВА, при х лежащем ВА верно, что вероятность того, что м от х равно единице, значит, это будет больше, чем бета.
Больше, чем гамма, чтобы гамма была больше бета.
Значит, при х лежащем ВА с чертой, при х лежащем ВА с чертой верно, что вероятность того, что м от х равно единицы будет меньше, чем бета.
Вот тогда получается, что и при любых константах ноль меньше бета, меньше гамма и меньше единицы получится БПП.
Да, и также при любых функциях, когда у нас бета больше, чем один делить на два в степени полинома от n,
да, значит, дальше гамма минус бета больше, чем один делить на просто полинома от n, а, соответственно, гамма больше, чем один минус один делить на два в степени полинома от n.
Значит, тоже получится БПП.
Вот, но идея очень простая. Идея, на самом деле, та же самая, которую мы вот здесь уже использовали.
Только мы вместо двух раз запустим полиномиальное число раз.
Да, то есть тут будет получаться, что будем запускать алгоритм полиномиальное число раз.
Соответственно, в случае РП, в случае РП брать дезъюнцию.
Вот, а в случае БПП брать по большинству.
Значит, в случае РП брать дезъюнцию, в случае БПП брать по большинству.
Ну и тогда, значит, в одном случае просто будет довольно простое соображение.
В общем, из тех же соображений, что и раньше.
То есть мы просто умножаем. И идея такая, что...
Да, смотрите, у нас, значит, в случае РП получается, что вероятность того, что вот эта дезъюнция равна нулю.
То есть что для любого I, M от X рытое равно нулю.
То есть это у нас будет 1 минус альфа в степени число повторений, в степени Q.
Соответственно, если у нас альфа это 1 делить на полином, значит, если альфа это 1 делить на P от N,
А Q от N будет равно P от N на какой-нибудь еще...
Ну, на T от N давайте.
Вот тогда, соответственно, вот это вот 1 минус альфа вкутай будет равняться 1 минус 1 делить на P от N в степени P от N, T от N.
И это примерно равняется экспоненту в степени минус T от N.
Ну, а соответственно, вероятность того, что это единица, как раз то, что нас интересует, будет примерно равно 1 минус E в степени минус T от N,
Ну, что, собственно, у нас вот здесь вот написано.
Ну, для расчета по большинству и оценки ошибки
Нужно использовать неравенство больших уклонений.
Например, так называемое неравенство черного.
Интуиция такая...
А, ну да, по большинству это если...
Это если симметрично относительно 1 и 2, тогда по большинству.
Да, соответственно, если не симметрично, то тогда где-то там середины нужно сравнивать.
Значит, большинству...
Так, а дать я вот тут.
То есть тут точнее нужно сравнивать с
бета плюс гамма пополам.
Так, интуиция. Ну, интуиция такая, что
мат ожидания
отличается от вот этого бета плюс гамма пополам.
Ожидание чего? Ожидаемой долей единиц.
Ожидание доли единиц отличается от бета плюс гамма пополам.
Хотя бы на единицу делить на...
Ну, давайте я так и пишу два полинома от n.
Дай мне суду, что вот одна вторая вот от этого полинома.
Да, соответственно, при этом вероятность
отклонения
на, соответственно, k-сильма, значит, на k-стандартных...
14 отклонений на k-стандартных отклонениях
экспоненциально убывает
по k. Вот. И это называется CPT.
CPT. В этом и заключается, что
хвосты, что все стремится к гауссовскому распределению,
а хвосты у гауссовского распределения экспоненциально убывают.
Ну и, соответственно, стандартное отклонение
имеет порядок...
Так, значит, 1 делить на корень
из числа повторений.
Q это у нас получается.
Значит, где Q, это число повторений.
Да, и стандартное отклонение имеет порядок 1 делить на корень из Q.
Дальше, где Q, это число повторений.
Вот. Ну и, соответственно, нужно
сделать столько повторений, чтобы
вот это вот было бы сильно меньше,
чем, соответственно, вот это вот.
Ну и это полиномиальное число повторений.
Вот это полиномиальное число повторений.
Ну а зачем нужно конкретно нераз черного?
Ну, идея просто в том, что CPT же предельная теорема.
То есть это в пределе стремиться к гауссияне.
А нам нужно не в пределе, а через какое-то конкретное число шагов.
И чтобы вот это как бы вес этого хвоста оценивать
для конкретного числа шагов, нужно иметь...
Значит, нужно уметь оценивать уже этот размер хвоста
на каком-то конечном этапе.
Но для этого вот такие неравенства и больших уклонений нужны.
Вот так.
Ну и, наверное, последнее, что я сегодня расскажу,
значит, это теорема Эйдлмана.
Теорема Эйдлмана, которая связывает
два наших последних раздела.
Конечно, что BPP тоже вложена в P slash Poli.
Так, и это, доказывается, так.
Значит, смотрите, сделаем столько повторов,
что вероятности ошибки станет меньше...
Ну, один делить на два в степени m.
Значит, меньше, чем один делить на два в степени m.
То есть меньше, чем один делить на два в степени m.
Делить на два в степени m.
Значит, то есть меньше, чем число слов х длины n.
Вот, тогда получается...
Значит, тогда каждому из х не подходит доля r.
Ну, меньше, значит, меньше, чем вот это вот один делить на два в степени m.
Значит, всего х в степени n штук.
Поэтому найдется такое r подходящее для всех х.
Но после этого для всех х данная длина.
Значит, длина n.
Значит, после этого такое r можно зафиксировать.
Такое r можно зафиксировать как часть подсказки
и затем переделать машину Turing m в схему стандартным образом.
Можно сказать, что вот эта r будет подсказка
и дальше использовать уже то, что мы доказывали,
что машина с подсказкой, это есть то же самое.
То есть можно сказать, что r будет вся подсказка
и пользоваться старой теоремой.
Можно сказать, что это часть подсказки,
а другая часть это описание машины Turing m.
В схему запаиваем.
Ну вот, собственно, это вся теорема.
Ладно, так, ну что, не скиньте вопросы.
Хорошо, спасибо, что слушали.
В следующий раз последняя лекция.

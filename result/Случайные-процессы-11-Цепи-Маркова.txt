Вот такая его форма. Она довольно громоздкая, и вот это считается с конца. То есть фиксируется так, что будет понятно, как всегда у нас.
Это маленькая, это самая большая, и начинается с конца интегрирования. Все переменные фиксируются, кроме последней.
По последней проинтегрировали. Вот по этой мере, как только эти все фиксированы, то эта мера, можно сказать, аргумента Xn.
И эта функция тоже от Xn зависит. Ее проинтегрировали.
Но если множество простого вида... То есть видите, тут формула, из-за чего она немножко громоздкая, из-за того, что множество общего вида.
Если множество распалось, как мы помним для того, чтобы, конечно, мерное распределение, однозначно определить, достаточно знать меру на произведение.
Из этого всех она там устанавливается. Но вот если множество распалось в произведение, то здесь будет просто... Что будет?
Здесь будет мера этого последнего. Но эта мера последнего, она еще зависит от предыдущих.
И поэтому, когда мы начнем следующее интегрирование делать, то тут уже не будет так просто не общепиться мера еще одного.
А эта мера, вот эта следующая, будет считаться так. А по этому множеству вот эту функцию нужно будет проинтегрировать.
По предыдущим примерам. Вот ее проинтегрируем, ну и так далее.
Поэтому на самом деле уже на втором ходу все это усложнится, и не будет такого расщепления.
Потому что особенного смысла нет. Я предполагаю, что это произведение оценит некое множество.
И вот когда мы так с конца интегрируем, то когда мы доберемся до последнего переменного, то предыдущее нам выдаст нечто,
зависящее от X1. Потому что все остальные переменные уже окажутся связанными, интегрированы, их уже не будет.
Ну форма вот такая довольно громоздкая, но в чем ее смысл?
С помощью этой формы, ну X1 это там где, ну это фазовое пространство, у нас в процессе принимается значение.
Ну, как я говорю, в большинстве случаев можно считать, что это R, D или R, но в принципе это формула, что такая, универсальна для такого угода.
Где процесс принимается значение.
Значит смотрите, в чем смысл еще этой формы?
Ну, с помощью этой формы, с помощью этой формы, по переходам, вот таким F, вот таким аргументам, I, M, U, T, можно задать вот эту формулу,
вот эта формула выписана в предположении, что процесс есть, но в правой части никакого процесса нет.
Значит можно задать, можно задать конечку мерки и распределение, а значит проверить, их согласовывается.
Ну, вот такая формула, которая в теории Голмогорода требуется.
И, по теории Голмогорода, получить процесс с такими, ну, с такими конечными мерками распределений.
Так вот, значит смотрите, теория Голмогорода дает какой-то процесс, но она не говорит, что это марковский процесс.
Так вот, теория марковского процесса, полученный процесс, будет марковский.
Это просто полезная фактка сведения, мы его не будем доказывать, это будет довольно длинная техническая проверка.
А вот что мы докажем, это мы докажем вариант этой теории, мы сейчас для марковских цепей.
Для процессов у нас это как факт без доказательства.
Ну, казалось бы, раз мы принимаем такой факт для процессов без доказательства, что-то домучится с марковскими цепями, зачем для них отдельно доказывать частный случай.
Но это из-за того, что он доказывается сравнительно неиспознанно, ну и там хотя бы в этом частном случае видно, что происходит.
Эту теорию тоже, конечно, при желании можно было доказать, но такое доказательство, ну, по крайней мере на экзамене довольно неприятное, считаю, было воспроизводить такое доказательство.
А для цепей довольно всё кроме.
Значит, теперь ещё одна теория без доказательства.
То же она у меня в конспекте есть, то же, то же доказательство, то же как факт.
А у неё доказательство, не бог вещь какое, сложное.
Вот, например, в учебнике Буинского и Ширяева можно его прочитать в странице с небольшим.
Ну, тоже такое, знаете, довольно забористое теоретическое множественное обознавание.
Значит, теория.
Теория.
Процесс.
А с независимой мониторией вращения.
Так, Маркочки относительно порождённой инфитрации.
Значит, смотрите, когда есть процесс с независимыми вращениями, то никакой фильтрации, может, и нет.
Но, помним, всякий процесс устраивает свою собственную фильтрацию.
Относительно его собственной фильтрации, такой процесс будет Маркочкой.
То есть смотрите, что у нас, что получается.
Значит, вот у нас был важный класс процессов.
Это процессы с независимой мониторией вращения.
Значит, ещё был важный класс процессов галлов.
Ещё был важный класс процессов мардингаллы.
И вот четвёртый класс процессов это маркерс.
Ну и вот всегда полезно выяснять, какие между ними соотношения.
Ну, гауссовство это очень специалистическая, конечно, вещь.
Ну и мы видели, что, скажем, гауссовские процессы с независимыми вращениями,
ну там, при некоторых дополнительных широких предположениях,
они вообще превращаются в винарские процессы.
То есть у некоторых, получается, что у некоторых классов пересечение такие очень прочные.
Значит, некоторые классы друг в друга вкладываются.
Ну вот, вот этот пример.
Вот этот пример.
Процесс с независимыми вращениями – он маркерс.
Значит, у нас про процессы с независимыми вращениями ещё одно было другое вложение.
А если процесс с независимыми вращениями квадратично интегрируемый, то он мардингалл.
Так, значит, ещё такие соотношения.
Ну и, естественно, возникает вопрос, как связаны маркерские процессы с мардингаллом.
Но здесь уже таких простых связей нет.
Ну, оно и понятно, потому что в качестве упражнения можно доказать,
если вы взяли маркерский процесс и подставили его в гомеоморфизм,
ну скажем, взяли маркерский процесс на прямой и подставили его в гомеоморфизм,
то все там нужные соотношения измеримости сохранятся, и новый процесс останется маркером.
Ну, например, если вы взяли напрямой марковский процесс и взяли куб от него, то на что он останется марковским.
Но маркингальность так не согласуется с отображениями.
Вот в качестве упражнения можете проверить, что куб винарского процесса...
Ну, просто в этой границах и науке головной руками основеет, что куб винарского процесса это марковский процесс относительно его интеракции, но не маркингаль.
Вот маркингальность, она не очень дружит с нелинейными преобразованиями.
То есть по большому счету можно сказать так, что когда преобразование реально нелинейное, то вот практически никогда маркингал не переводит маркингал.
На самом исключении, можно между людьми, постоянно можно говорить, но в сколько-нибудь невырубленных ситуациях так не будет.
Поэтому все-таки совсем уж такой матережечной системы нет между этими основными классами процессов, которые мы рассматриваем.
Но вот какие-то интересные соотношения есть.
Значит, теперь цепи маркового.
Цепь маркового, значит, это марковский процесс
с конечным или счетным числом состояний.
У нас иногда даже требуют, чтобы еще и время было дискретное.
Значит, у нас будет еще более специально конечные цепи маркового с дискретным временем.
У нас время будет просто 1, 2, 3 и так далее.
И конечное фазовое пространство.
Ну, можно считать, что просто точки 1, 2, m.
Значит, более того, у нас будут однородные цепи маркового.
Ну, я напомню, что однородный марковский процесс это когда, значит, однородный марковский процесс общий.
Значит, переходная функция зависит только от разности времен.
Ну, то есть вот такая вот функция, она становится функцией четырех аргументов.
Она становится функцией трех аргументов.
Ну и мнимонический смысл этого обозначения, что это вероятность попасть в множество e за это время, находясь в нулевой момент в x.
Но для марковской цепи, но тут все равно, видите, все равно переменных-то хватает, их меньше стало, конечно, их 3, но тоже будь здоров.
Но для марковской цепи все это еще сильно сокращается, для однородной марковской цепи это все еще дальше сокращается.
И, значит, сейчас давайте я, чтобы у меня обозначения не разошлись с конспектом.
Значит, для однородной цепи все определяется переходными вероятностями за один шаг.
Это числа такие p и g, это вероятность перехода из xe в xg за один шаг.
Ну а, значит, раз время у нас дискретное, то вероятность за n шагов, это будет определяться степенью этой матрицы.
Значит, вот возникает такая матрица, p, вот с такими элементами, значит, m на m, значит, матрица вероятностей перехода за один шаг.
Ну что можно хорошего сказать про эту матрицу? У нее, естественно, элементы неотрицательные, и когда суммируем по g, то сумма вот этих вероятностей единица для каждого i.
Это отвечает, значит, смысл очень простой, что из каждого i можно перейти за один шаг в одно из этих состояний, и поэтому сумма вероятностей должна быть единица.
Ну, кстати сказать, в этом месте пока еще не видно, зачем нужно, чтобы пространство было конечным, это пока что годится и для счетного.
Так что, в принципе, до сих пор пока было не очень существенно, что конечное, для счетного то же самое, но только матрица будет, естественно, бесконечной.
Но сейчас довольно быстро, ну, все-таки станет понятно, что лучше считать, что у нас конечное пространство.
Значит, смотрите, что будет... Так, сейчас давайте вот это я сотру.
Значит, пусть P0 начальное распределение цепи в момент n равный нулю.
Ну, я сказал, что у нас время натуральные числа, а тут вот еще n равное нулю появилось, но давайте добавим 0.
Это удобнее все-таки считать, что в начальный момент времени не первый, а нулевой.
Ну, некоторые, правда, считают, что ноль и является натуральным числом, но вот я помню, Арнольд это всегда горячо оспаривал.
Ну, это, в общем, дело вкуса, считать ноль натуральным числом или нет, то есть, как говорят в школе, натуральные числа это те, которые при счете предметов,
ну, вроде как предметов может не быть, тогда ноль вроде нужно включать, но большинство все-таки вроде склоняется к тому, что натуральные числа это от единицы начинается.
Ну, в общем, во всяком случае давайте n равное нулю добавим.
И, значит, что же это за, ну, начальное распределение?
Это, значит, π0 это вероятностная мера на, ну, вот на этом множестве.
Так, значит, чем она, ну, чем она задается?
Она задается числами π0, ну, вот от этих икситых.
Ну, здесь пока еще тоже не очень важно, что их, конечно, для счетных тоже самое, конечно, будет.
И, значит, это вероятность того, что процесс, ну, вот принял такое значение, значит, в нулевой момент времени.
Так, значит, что дальше происходит с процессом?
Ну, пусть pn распределение кси n.
Так.
А?
У кси, вот это?
Ну, ноль.
Значит, как происходит дальнейшая динамика?
Так.
Ну, скажем, π1, это будет π0 умножить на матрицу переходов.
Значит, умножение справа.
Ну, да.
Множество состоений у нас, s1, sm.
Ну, можно считать, что это 1m.
Ну, это иногда удобно, иногда нет.
Потому что иногда неудобно, что эти 1m могут путаться с моментами времени.
Так.
Вот поэтому, чтобы эти состояния не путать с моментами времени, давайте мы их будем пока, ну, пока можно вот этими.
Ну, то есть, слушайте, не s1, какое s1, у меня x1 было, x1.
Слушайте, это я вот, давайте я исправлю, как было в конспекте.
Ну, кстати, конспект я на днях окончательно и пришлю, потому что кроме вот этих марковских цепей еще два вопроса осталось.
Ну, сами понимаете, что это дело вкуса, как обозначать точки пространства.
Ну, давайте, чтобы было согласование с конспектом, давайте я вот тут тоже исправлю.
Значит, у меня осталось, собственно, конспект уже практически готов, два вопроса еще осталось.
Это, значит, про ветвящийся процесс, значит, там дописать и про, значит, модель страхования.
Так, вот два вопроса.
Это там еще несколько парочка, там тройка страниц будет.
Это я, в общем, на днях допишу и пришлю.
Вот, значит, смотрите.
Значит, почему умножение справа?
Так, умножение справа.
Это довольно необычно, так, это довольно необычно.
Это если мы пишем, это если мы пишем, ну, вероятности, вероятности это строки.
Так, и поэтому, чтобы, ну, вот, как умножают строку на столбец, так, ну, в смысле, строку на матрицу.
Ну, вот, вот умножают это на первый столбец там и так далее.
Помните, это с матрицами довольно не безобидная штука, как их умножать на векторы.
Если векторы столбцы, так, то матрицу обычно пишут слева, ну, и умножают, ну, там, по соответствующему правилу.
Ну, откуда здесь берется, ну, почему так странно, почему такое странное правило?
Почему справа-то надо умножать?
Так, ну, это вот почему.
Давайте посмотрим, значит, что такое, значит, ну, значит, π1, ну, скажем, от х1.
Так, это вероятность попасть в х1 в момент времени 1.
Ну, как можно попасть в х1?
В х1 можно попасть, находясь в нуле, но в одном из состояний, так.
И поэтому нужно, значит, откуда можно было в х1 попасть?
В х1 можно из х1 попасть, из х2, из хм, так.
Значит, нужно просуммировать.
По всем g от 1 до m, так.
Вероятности перехода, значит, мы смотрим в, значит, в g, сейчас, ну, не в g, в i давайте.
Значит, можно из разных i попасть в 1, так.
Но при этом еще надо учесть, учесть на, ну, учесть вот эту, значит, вероятность того, что в момент 0 были в i, так.
Значит, вот, видите, получается, вот такая формула получается.
Так, а, значит, а, ну, что, что, что это происходит, так?
Значит, ну, вот, вот, вот эта строка умножается, так, пиноль там 1 и так далее, так.
Значит, это мы, это мы заполняем первый элемент, так.
У нас вот эта новая вероятность, это опять строчка, так.
Вот вы заполняете первый элемент этой строчки.
Ну и что вы делаете? Вы берете строчку, исход нулевых, нулевых состояний, так.
И на что умножаете? Ну, скалярно умножаете.
Ну, вот на такой, как бы, вектор с компонентами, значит, p1, значит, 1, p2, 1 и так далее.
Но, значит, и это кто? И, значит, это номер строки, так, в матрице.
И поэтому получается, что вы идете по первому столбцу, так, значит, по первому столбцу.
И вот, ну, у вас была строка, и вы, значит, взяли, значит, ну, строки этой матрицы.
Вот так вот вы делаете. Вот, значит, вот такой вы выделили столбец.
Этого умножили скалярно на это, получили тут элемент, так.
Потом перешли к следующему столбцу, так.
То есть, ну, такой получается довольно непривычный вид умножения,
потому что все-таки большинство людей, которые...
Ну, когда приходится матрицу умножать, как все привыкли умножать их,
ну, так сказать, с левой на столбцы.
Но почему так... Вот в этой науке так почему-то традиция делать так,
ну, и я даже объясню, в чем такая традиция.
Значит, как перевести в обычную традицию?
Ну, казалось бы, очень просто. Пиши состояние, пиши столбцами.
Кто тебя вынуждает в состояние писать строками? Пиши их столбцами.
Ну, хорошо, будем писать столбцами.
Но, увы, когда мы будем их писать столбцами,
то для того, чтобы слева умножать, матрицу надо транспонировать, так.
Но это не очень удобно, так.
Вот как-то все привыкли, что лучше иметь дело не с матрицей,
значит, ну, транспонированной, а с самой матрицей переходов.
Это как-то вроде удобнее и нагляднее.
Поэтому вот цена за это такая запись, так.
Значит, обратите внимание, что матрица не обязательно симметричная.
Конечно, если бы матрица была симметричная, то тут вообще никаких проблем нет.
Можно сразу было писать столбцами и умножать слева.
Но в том-то все и дело, что транспонированная матрица не обязательно совпадает с исходом.
Потому что вероятность перейти из И в Ж совсем не та же самая, что из Ж в И.
Поэтому это совсем не предполагается, так.
Ну и у нее по строкам и столбцам, у нее, видите, разные свойства.
Все элементы не отрицательные, так.
Но сумма единиц получается именно по строкам, так.
Значит, ну и в общем случае, значит, Pn, в общем случае Pn.
Это P0 умножить на n, ну, по индукции из этого получается вот такая вот формула.
То есть видите, получается, что если вам дана матрица перехода, так,
то вот эти вот распределения в моменты n, они однозначно определены исходным нулевым распределением.
Вот в той теореме, которую я выписывал без доказательства,
там смотрите, как было, что если вам дана переходная функция,
а здесь аналог переходной функции, это вот эта переходная матрица.
Ну, вы скажете, переходная функция зависела от трех переменных, так.
А куда они делись, эти переменные, так.
Значит, вот у нас есть только, так сказать, вот эти два параметра, куда остальное это делось.
Ну, значит, смотрите, как, значит, связь какая, ну, вот для однородного процесса.
Значит, связь была вот такая, так.
Ну, в смысле, вот функция была переходная такая, а здесь вот такая, так.
Значит, кто кому соответствует, так.
Вот, ну, так как-то можно ожидать, что вот этот, это, наверное, кто-то из этих, так.
Но спрашивается, кто.
Но с точки зрения вот этого, так, с точки зрения вот этого, этот, это вот кто такой.
Это, значит, это х и вот этот стоит, так.
Тут стоит единица, так.
А тут стоит х ж.
И видите, тут стояла вероятность из точки х за время t перейти в множество e.
Но тут у нас для того, чтобы говорить о том, какая вероятность попасть во множество e,
достаточно сказать, какая вероятность из точки х и попасть в одну точку.
Не во множество, а в одну точку, причем за один шаг.
Потому что если вы знаете, какая вероятность попасть в точку, то множество e тут состоит из точек.
для общего e это просто будет сумма вот этих по этим, так, видите,
тут получается такая связь вот этой общей функции, значит, переходной,
получается такая связь, p значит x, t, e, получается так, x это один из этих,
ну вот давайте сначала за один шаг, значит, вероятность попасть из точки x во множество e,
значит, за один шаг, ну x это одна из x итых, так, и значит здесь будет, ну здесь будет сумма,
значит, здесь будет сумма по j, по j таким, что xjt попало в e, так, будет вот это,
p значит x и, ну вот тут тоже и это точка одна из этих, так, попасть в xj, так, ну а это как раз вот эти
наши переходные вероятности, и поэтому, значит, тут нужно просуммировать по всем точкам из e,
а вот эти вот p и j, так, вот, значит, вот это у нас получится за один шаг, но тут t не обязательно единица,
а тут t, ну какое-то натуральное, но, соответственно, нужно что сделать, нужно, если тут t не единица,
а какое-то натуральное t, то нужно здесь ставить элементы матрицы в степени t, так,
все будет так же, как за один шаг, но только, значит, матрица перехода за n шагов, а матрица
перехода за n шагов это просто степень исходной матрицы, так, связь вот такая получается, так,
и теперь, значит, получается здесь аналог этой теоремы, которую я без доказательства сформулировал,
аналог получается вот какой, но он более простой и по формулировке, и по доказательству, поэтому,
поэтому его и можно доказать, значит, аналог получается вот какой. Значит, теорема, сейчас
Давайте только, чтобы у меня хуже всего отойти обозначение конспекта.
Теорема такая, пусть даны п и вот матрица п из этих вот элементов, м на м матрица.
И начальное распределение п0 на х, значит, вот из м элементов, так? Значит, это дано.
Тогда существует цепь Маркова.
С пространством вот этим вот конечным х, значит, для которой п0 это распределение
ксиноль, так? А вот эти вот переходные вероятности это условные, значит, меры того, что переходов за один шаг.
То есть условные вероятности быть в момент n плюс 1 в xj при условии
нахождения в момент, в предыдущий момент в и, в x и. Ну, поскольку мы интересуемся однородными цепями,
то это, видите, это не зависит от n, видите? Вот однородность, значит, это как раз то, что это от n не зависит.
Ну, давайте, давайте это докажем. Ну, естественно, доказывается с помощью теоремы Колмогорова.
И это частный случай вот этой общей теоремы, поэтому, ну, как я говорил, если из этих переходных вероятностей
сворганить, вот так, как я объяснял, переходные функции, то можно было бы это получить из той недоказанной теоремы для общих процессов.
Но поскольку тут все довольно элементарно доказывается, ну, давайте докажем. Это, в общем, несложно, потому что все-таки хоть что-то надо с доказательством,
потому что вот когда придут эти 150 человек, ну, невозможно же только формулировки у всех спрашивать.
Ну, естественно, все формулировки там спишут откуда-то. Ну, конечно, можно сказать, что и доказательства можно списать,
но это, вы же знаете, когда списано доказательства, то это еще вопрос, поняты ли оно, его можно обсуждать.
Вот про формулировку трудно обсуждать, понята она или нет. Если она написана правильно, то что тут обсуждать, так?
Поэтому вот, значит, в этом, значит, некоторый смысл, чтобы кое-что было с доказательством.
Значит, доказательства. Ну, можно считать, применим теорему Колмогорова,
значит, считая, что х лежит в R, ну, например, что х и ты, это просто и.
Значит, теперь давайте, значит, берем, ну, вместо, значит, у нас распределение, распределение конечномерное,
значит, в моменты времени 1n, значит, определим вот по такой формуле.
Значит, смотрите, что это за распределение, так?
Значит, мы хотим, значит, мы хотим задать совместные распределения.
Сейчас только не em, а en должно быть здесь, здесь должно быть en, значит, en должно быть.
Ну, эти i1 и n, они вот, значит, среди первых m, так?
Значит, смотрите, что это за распределение?
Ну, я напомню, для теоремы Колмогорова нам нужно задавать совместные распределения, так?
Вот у нас еще случайных величин нет, а мы, значит, хотим задать совместные распределения, так?
А, значит, поэтому, значит, если так задать, значит, если так задать, то получится следующее.
Проекция, проекция вот этого на rn есть, ну, предыдущая, так?
Но я напомню, что нам нужно задавать совместные распределения не только подряд в моменты времени, так?
А нужно еще в какие-то моменты с пропусками, так?
Поэтому, значит, остальные, нужно еще вот, значит, вот такие вот нужно задавать, так?
Значит, там в какие-то моменты t1, значит, там tk, так, где t1 меньше tk.
Значит, зададим, ну, ну, где они, значит, где они не подряд, ну, вот эти вот 1k, значит, зададим как проекции.
Ну, например, значит, например, значит, p, значит, p2, 3, это проекция p1, p2, 3 на, значит, r2.
Нам же еще вот такие нужно задавать, так?
Но при этом, при этом, ну, вот это я не проверил, почему так будет согласование.
Но это из условия, значит, давайте я напишу, это из условия, что сумма поезжитых пожи равняется единице.
Вот из этого условия непосредственно проверяется, значит, когда сдаешь по такой формуле, что будут согласованы.
Но нам, смотрите, для теоремы Колмогорова, там ведь надо проверять не только в подряд идущие моменты, так, согласованность, но и с пропусками.
Их задавать тоже нужно, но мы их задаем так, ну, в принципе, можно формулой, конечно, написать, но формула будет еще более громоздкой, так?
А проще словами сказать, что когда эти не подряд, так, то мы просто как проекции их зададим.
Но при желании можно было по аналогичной, но такой более громоздкой формулой это и записать, так?
Но из-за того, что вот эти согласованы, то, ну, проекции тоже получится согласованы, так?
Поэтому у нас получилась согласованная система, так?
И что дает теорема Колмогорова?
Значит, теорема Колмогорова дает процесс, дает процесс, ну, ксиен, ну, с такими, с такими распределениями, так?
Ну, но почему две вещи надо проверять?
Ну, почему ксиен Марковский, так?
Значит, вот это первое, что не очевидно, потому что Колмогоров ничего там про Марковский не обещал, но дает процесс.
Вот задали вы конечное мерное распределение, получили процесс, а мы обещали процесс Марковский.
Почему он получился Марковский, вот это надо проверять.
Ну, и второе, что нужно проверять, значит, сейчас это второе у меня или первое.
Ну, еще, ну, это у меня второе, но проверим моего первое.
А вот еще, почему, значит, почему у нас будет, ну, значит, это хорошо, это Марковский процесс, конечно,
но почему он будет, вот, ну, так сказать, с такой переходной функцией?
Ну, вот, лучше сказать, почему с такими условными вероятностями?
Вот это еще тоже, значит, у нас, ну, получился какой-то процесс, ну, хорошо.
А почему он Марковский?
Ну, Колмогоров всего-навсего обещает, что у него, конечно, мерные распределения такие, какие выписаны.
Но, а, с какой стати он Марковский?
Б, мы же хотели, чтобы это была Марковская цепь с такими переходами, а Колмогоров на этот счет ничего, так сказать, не обещает.
Поэтому и то, и другое нужно проверить.
Ну, вот, собственно, в той теореме, которую я написал без доказательства, там тоже, конечно, все это надо проверять,
и там эта проверка, ну, она более техническая.
Но давайте мы хотя бы, значит, в случае конечной цепи увидим, что, ну, все это руками проверяется, в общем, довольно-таки несложно.
Значит, смотрите, проверяем последнее.
Вот это, значит, последнее просто.
Последнее сводится вот к чему.
Вот это, вот к чему сводится последнее.
Ну, давайте, ну, последнее, последнее очевидно.
Последнее очевидно.
Ну, смотрите, что такое вот эта штука?
Значит, видите, это, смотрите, что это такое?
Это двумерное распределение, но у нас была формула для конечномерных распределений.
Там такого не было.
Это как раз тот случай, когда надо проекции убрать.
Потому что у нас, смотрите, видите, только последние две позиции заданы.
Вот у нас там была формула, когда все от 1 до n позиции заданы.
А тут, видите, не так.
Значит, из чего складывается эта штука?
Ну, она складывается из заданий по всем позициям, но только первые у вас какие угодно.
Поэтому это, смотрите, что это будет за сумма?
Значит, это сумма вот таких вот произведений.
Значит, вот по всем, значит, по всем и1, и так далее, и n-2.
Так, вот, значит, но, значит, давайте посмотрим.
Значит, давайте посмотрим на эту сумму без последнего множителя.
Так, значит, сумма без последнего множителя вот эта.
Ну, вот, вот этот последний.
Ну, вот этот, вот этот, вот он.
Ну, это вот тот же самый, который здесь стоит.
Ну, это переход из и в жи.
Значит, сумма без вот этого последнего.
Есть как раз...
Меньше чего? Нет, почему? Они какие угодно.
Нет, эти какие угодно.
Нет, эти какие угодно.
Тут же, когда у нас пишутся эти вероятности переходов, они же не упорядочены по большему, меньшему.
Они всевозможные.
Значит, смотрите, это будет как раз, будет как раз вероятность вот этого.
Так, почему?
Потому что это у нас считается...
Ну, значит, это у нас, это у нас охвачено нашей формулой было.
Ну, собственно, из нашей формулы, из нашей формулы вытекает, что вот такая вероятность...
Обратите внимание, эта вероятность, она тоже не есть частный, она тоже не есть частный случай формулы,
потому что у нас формулы всегда от единицы до этого.
Но какова вероятность того, что в этот момент это равно вот этому?
Это вам нужно сложить всевозможные, выстраиваете, что могло быть в предыдущие моменты.
И в предыдущие моменты могло быть что угодно.
И вот вы складываете все эти возможности по предыдущим моментам, выстраиваете.
Вот тут нет предыдущих времен, а вы их искусственно добавляете, и в них могло быть что угодно.
Но из нашей формулы как раз получается, когда вы выстраиваете эти предыдущие моменты по всем этим возможностям, то у вас как раз вот это получается.
В этом как раз и состоит то, как мы получаем проекцию.
У нас исходная формула для мер на Rn, а тут проекция такой меры на Rn-1, на последний сомножитель.
Вот она считается по такой формуле.
Поэтому с этим равенством OK.
А еще нам нужно для марковости.
Эта вещь совсем безобидная.
Больше возни с марковостью.
С марковостью несколько больше возни.
Марковость.
Марковость – это условная независимость прошлого и будущего, прификсированном настоящим.
И для общего марковского процесса все эти проверки довольно гнусные, теоретико множественные, всякие выкладки.
Но здесь нам нужно из-за того, что все дискретно и конечно, нам нужно вот что проверить.
Что P от XiM равняется I, а XiN равняется J, при условии, что XiT равняется L.
Что это распадается в произведение, вот что означает.
И это когда? Это когда M меньше T и меньше N.
Вот видите, это момент из прошлого, это момент из будущего, это прификсированном настоящем.
И вот они должны так распадаться.
В общем определении марковости там стоит похуже, там стоят множество.
Но у нас тут все конечно, поэтому множество составляется из точек.
И поэтому если для точечных значений, видите, у нас множество очень простые.
Общее множество это конечное объединение таких.
Но если мы вот так сможем для точечных множества раздрабливать, то для общих конечных также будет.
Вот эту формулу нужно проверить.
Калмагуров ее так не обещал.
Он сказал, вот вам будет процесс, а уж что там с этим процессом, это уж там дальше сами разбираетесь с ним.
Ну вот мы сейчас с этим процессом и разбираемся.
Смотрите, как это можно преобразовать.
Естественно, мы хотим это проверить.
Для проверки заметим, ну это еще из предыдущего вытекает, что вероятность того, что вот это равно j,
при том, что xk равно i, это будет вот что.
Это считается с помощью степени матрицы.
Вот это из предыдущего вытекает.
Ну, в общем, это то, что мы уже доказали.
Это дает такую формулу.
Смотрите, это вероятность попадания из i в j за n шагов.
Ну и причем неважно, с какого момента, с какого k мы начали.
У нас так сразу процесс получается однородным, просто по построению.
Это следствие предыдущей формулы.
А это индекс, это у нас матрица вот эта, и ее элементы, это вот эти числа.
Ну то есть это, конечно, не степени этих чисел.
Ну вот выписать, кто такие, это, конечно же, невозможно.
Потому что у вас даже когда матрица дана, но попробуйте-ка ее возвести в здоровую степень.
Это понятно, что эти числа, они конкретные какие-то, но физически их найти обычно нельзя, конечно.
И тогда смотрите, что нам нужно, к чему сводится наша проверка.
Значит, нам надо.
Смотрите, что нам надо.
А кто такие эти условные вероятности?
Это же тут все дискретно, и поэтому условная вероятность, это очень банально.
Это вероятность пересечения делить на вероятность условия.
Они стали тут школьными.
В предыдущей общей теореме там все не так банально.
Там всякие условные вероятности, они не считаются, как чего-то там, что-то на что-то делить.
Но здесь считаются, и это, конечно, здорово упрощает дело.
Именно поэтому тут можно добраться до конца.
Значит, нам нужно, смотрите, что.
Нам нужно, что вероятность вот такая тройственного такого события, тройственного пересечения.
Умножить на вероятность вот этого, есть произведение двух.
Чем это отличается от того, что нужно?
Я просто внес всюду эти условия сюда, и еще поделил на эту вероятность.
Но видите, здесь она дважды, вот эта вот вероятность, видите, здесь она дважды.
Тут два раза.
А тут только один.
И поэтому нужно, чтобы получилось равенство после перехода к пересечениям, нужно левую часть еще на нее домножить.
Вот такая формула получается.
Вы скажете, формула, конечно, довольно гнусная.
Потому что, видите, эта штука, она даже еще и не охватывается нашей формулой.
Потому что наша формула для конечномерных распределений, она была только для, когда у вас подряд моменты идут времени.
А здесь моменты идут с возрастанием.
Сейчас только тут, тут М.
Тут опечатка, тут М.
Ну да, тут тот же самый М.
Значит, М самый маленький, Т побольше, Н еще больше.
И вот если бы это было 1, 2, 3, то это было бы то, что у нас написано.
Но это, к сожалению, не 1, 2, 3.
А эта штука с точки зрения нашей общей формулы, это вот что такое.
Это нужно взять все-таки всех 1 до N и эти зафиксировать, а потом суммировать по всем прочим, которых тут нет.
Так что для этого формулу можно записать, но она довольно гнусная будет формула.
Я даже в конспекте ее не стал писать.
Вот, и поэтому, для того чтобы получить то, что нам нужно, нам...
Значит, смотрите, вот тут, вот тут уже произошли некоторые улучшения с учетом вот этой формулы.
Значит, смотрите, что получается.
Из предыдущей формулы получается следующее.
Значит...
Значит, это будет...
Значит, мы ее опять вот так вот запишем.
И это мы запишем по предыдущей формуле.
Вот так получается такая формула.
Значит, что нам надо?
Значит, надо вот что нам проверить.
Что вот это тройственное пересечение...
Ну да, да.
Нет, ну не совсем обратно, потому что у нас сейчас кое-что сократится.
Значит, смотрите, что нам в итоге надо.
Значит, вот что нам надо доказать.
О, вот у нас к чему свелось.
Это у нас буквально на мгновение мы перешли опять к условной вероятности для того, чтобы у нас кое-что сократилось.
И итоговый...
Итог, что нам нужно проверять.
Пока что мы так немножко улучшаем свою позицию потихонечку.
Но пока что у нас основная головная боль, это вот это тройственные пересечения.
Все эти предыдущие вещи показывают, что двойные пересечения, это вещь довольно безобидная.
С ними мы довольно лихо разделываемся.
А вот основная головная боль, это тройственные пересечения.
Ну, значит, давайте докажем эту формулу.
Значит, давайте докажем эту формулу.
Значит, смотрите, что...
Эта формула, на самом деле, смысл ее более-менее понятен.
Видите, как она устроена, эта формула.
На самом деле, тут хочется объявить, что это очевидно.
Но это было бы очевидно, если бы это уже была Марковская цепь.
А вот смотрите, почему это очевидно.
Потому что, смотрите, о чем говорит...
Что это за событие?
Это событие говорит, что у вас вы в какой-то момент в прошлом были в И, в настоящем оказались в Л, а в будущий момент оказались в Ж.
А что правая часть говорит?
Она говорит, что нужно зафиксировать вот этот кусок, прошлое и настоящее,
и умножить это, что вот эта вероятность получается умножением того, что вот у вас такая предыстория,
на вероятность перейти из Л в Ж за N-1 шагов.
Тут как раз вроде это и написано, что вы в этот момент в И, в момент Т в Л,
а через время N-T вы попали в Ж.
Ну и вроде бы у вас ровно это и написано.
Это обозначение формулы.
Это обозначение уже не к формуле относится.
Формула закончилась.
Ну давайте я подальше это унесу.
Это формула звездочка.
Значит, смотрите, эта формула так и интуитивно кажется понятной.
Но нам нужно как-то все-таки исходя...
Ведь у нас, понимаете как, у нас вот это-то определяется некой конкретной формулой,
от нас независящей.
И нам нужно как-то использовать эту формулу.
Ведь пока что надо признать, что мы нашу создающую формулу для распределений,
что мы по-честному для тройственных пересечений, мы ее не использовали.
Мы ее использовали только для одинарных и для двойных.
А для тройственных не использовали.
И поэтому ясно, что тут было бы, так сказать, не очень честно сказать, что теперь все очевидно.
Значит, смотрите, при n равном t плюс 1 это вот что.
Это вот какая формула.
Так.
Значит, это...
Значит, что можно сказать про эту формулу?
Откуда эта формула взялась?
Ну, она вытекает...
Значит, это верно.
Значит, это верно из задающей формулы.
M.
Ну, тот же, который здесь, M.
Индексы не меняются. У нас вот есть эти три момента.
M, значит, меньше t настоящий и следующий.
Значит, вот смотрите, когда следующий он буквально следующий,
то это получается, это следует из нашей формулы задающей.
Ну, вот если посмотреть, какая будет.
Вот действительно так оно и будет.
Теперь, пусть верно при каком-то n больше t.
Тогда верно при n плюс 1.
Значит, давайте посмотрим, что будет при n плюс 1.
Значит, при n плюс 1 будет следующее.
Значит, это по определению будет вот что.
Это будет вот такая сумма.
Значит, в момент времени n это может быть какое-то k.
Ну и вот мы пока это просуммируем.
Ну, давайте я повыше напишу.
Значит, смотрите, это по предположению индукции.
Это есть сумма по k.
Потому что сумма по k дает как раз следующую степень матрицы.
Поэтому, тем самым получается верно для n плюс 1.
Ну и вот тем самым по индукции получается, что все проверено.
Индекс k, тут k и тут k.
Смотрите, свертка вот этого пока дает следующую степень матрицы.
Сейчас, суммы нет.
Тут уже нет суммы, это я механически переписал.
Тут никакой суммы уже нет.
Тут просто уже ответ, который мы хотели получить.
Тут никакой суммы уже нет, конечно.
Значит, смотрите, если посмотреть, как доказывается вот та теорема, которую я привел без доказательства,
то там нечто похожее исполняется на уровне измеримых функций.
Аналогично и проверяются тождества, но там, к сожалению, не будет сумм.
И там к точкам все не сводится.
И там поэтому как проверяются всякие равенства для условных вероятностей.
Там равенства для интегралов.
Ну они немножко более громоздкие получаются.
У нас даже и здесь нельзя сказать, что как-то все очень элегантно.
Естественно, если вместо суммы у вас интегралы по множеству,
ну несколько более громоздко будет.
Ну и как мне кажется, там несколько труднее следить за ходом вычислений.
Но в принципе идея такая же.
Точно так же проверяются, там нужно тоже две вещи проверить.
Там согласованность получается легко, тут особой роли нет в дискретности.
А вот дальше нужно проверить.
Калмагоров дал некий процесс.
И вот почему он марковский, и почему у него получилась нужная переходная функция,
ну вот там проверяется похоже, но технически я бы сказал немножко более муторно.
Здесь конечно тоже нельзя сказать, что совсем не муторно,
но по крайней мере тут никаких хитростей нет.
Просто прямолинейно проверяем, так сказать, и все.
Так, теперь последнее, что я скажу, а следующий раз мы это уже докажем.
Значит, определение.
Вероятностная мера π на х инвариантна для цепи.
Ну или стационарна.
А если она не меняется, ну если она, так сказать, собственный вектор в этой матрице.
То есть тогда получается, что тогда получается?
Значит, тогда получается, что распределение, если вы начали, значит получается,
если вы начали с инвариантного распределения, то оно не меняется с ходом времени.
Ну и вот последний факт, очень простой, на котором мы сейчас кончим.
Значит, теорема.
Значит, существует инвариантная вероятностная мера π.
Значит, доказательство очень простое.
А симплекс, ну скажем, П, ну там или С, симплекс С всех вероятностных мер,
мер на х, отображается в С, ну посредством, ну оператором П.
Ну и смотрите, какая ситуация.
Значит, у вас есть конечномерный выпуклый компакт, он, значит, отображается в себя.
Значит, существует неподвижная точка, неподвижная точка.
Ну, например, это вытекает из теоремы Боля-Брауэра.
Эта теорема, конечно, гораздо мощнее, чем нам нужно.
Значит, эта теорема для каких угодно непрывных отображений симплексов себя.
А у нас отображение линейное.
Поэтому, конечно, это, так сказать, некоторая такая стрельба из пушки по веробьям.
Но с другой стороны, если можно, так сказать, подстрелить одним выстрелом, то чего тогда мучиться, так сказать, и ходить с рогатками.
Значит, в следующий раз будет еще одно утверждение о специфических инвариантных мерах.
Ну и на этом цепи Маркова кончатся.
И будут еще две очень специальных задачи.
Это задача о ветвящемся процессе Гальтына Ватсена и конкретная задача о разорении в модели страхования.
Вот это уже, так сказать, как примеры, что ли, будут.
Но поскольку здесь, как я понял, в курсах случайных процессов на физтехе как-то принято эти две вещи рассказывать.
Ну я тоже решил, так сказать, по традиции это включить.
Ну и вот смотрите, остается следующая лекция.
И еще одна между майскими праздниками.
Ну и вот я, естественно, буду прилагать усилия, чтобы оставшиеся за эти две лекции рассказать.
И чтобы, ну, значит, 5-го числа, чтобы закончить.
Вот, в общем, так сказать, план такой.
Но, в принципе, теоретически у вас еще 12-го есть лекция.
А потом чуть ли даже и не 19-го.
Но это уже какие-то крайности, в общем, я думаю.
Так, все, тогда давайте на этом приостановимся.

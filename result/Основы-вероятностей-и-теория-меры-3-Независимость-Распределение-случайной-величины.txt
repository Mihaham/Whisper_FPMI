Чем мы с вами занимались? Так, это у нас третья лекция. Мы с вами в прошлый раз занимались формулой
метеоровероятности. Первое, что мы с вами забрали эту форму, это условная вероятность.
Условная вероятность события А при условии события В. В случае, если событие В не нулевой
вероятности, оно определяется как частная вероятность пересечения события А и В и вероятности
условий. Тогда же мы с вами поговорили о том, что на самом деле это есть некоторые проблемы в
этом определении, потому что если у нас событие В нулевой вероятности, тогда интуитивно понятно,
что вот этот объект, но так его определить мы не можем. Примеры я приводил. Вы научитесь строить
условную вероятность ситуации, когда вероятность условия будет нулевая, но это будет где-то в
середине курса математической статистики. То есть даже в курсе тервера это сделать пока не будете уметь.
Второе, что мы с вами разобрали, это формула полной вероятности. Работает она в ситуации,
если у вас есть разбиение, причем неважно конечное или бесконечное разбиение Омега.
Мы с вами обсуждали такое разбиение Омега, это набор непересекающихся событий БКТ, то есть все они
должны быть элементами Ф. Причем такие, что тогда для любого события А верно следующую форму.
Вероятность события А можно расписать как сумму, ну в зависимости от того наше разбиение конечное
или бесконечное, таких произведений. Условных вероятностей А по условии БКТ и на вероятность БКТ.
Пример этой формулы мы с вами не успели разобрать, давайте быстро его разберем. Есть такое народное
творчество, что если у нас с вами есть, ну предположим, три студента, которых, ну то есть есть
преподаватели, которые только этих трех студентов выспрашивают на семинарах. Условных назовем умный,
веселый, с большой буквы, без уважения, ну и старост. Но имя старости преподаватель всегда
знает, поэтому проще вызывать старость. Ну вот, и что такое БКТ? То есть это кого вызовет семинарист?
Ну предположим, что семинарист хороший, то есть он старается научить людей, поэтому веселого он.
Давайте у нас первый будет умный, второй веселый, а третий старост. То есть вероятность того,
что он вызывает веселого, ну веселые они всегда не очень умные, ну вот, поэтому давайте вот его
он вызывает с вероятностью одна вторая, умную он вызывает с вероятностью одна шестая, ну это когда
совсем сложная задача, надо, чтобы ее все-таки человек пришел. Ну вот, ну а старосту во всех
оставшихся ситуациях это вроде одна треть. Там получается единицы, да? Ну понятно, что БКТ всегда
сумма их вероятности равна единице, потому что они в объединении дают всю ОМЕГ. Что тогда такое
условная вероятность А при условии БКТ? То есть это вероятность того А. Какое нас интересует
событие А? Ответ был дан. То есть какой-то там вопрос преподаватель задает, нас интересует,
как бы он получил ответ на свой вопрос правильно, естественно, или нет. Что такое А при условии БКТ?
Какова вероятность того, что ответ будет дан, если отвечает умный? Какая тут вероятность у нас?
Ну обычно. Как смешно. Единицы. Ну мы считаем то, что как бы у нас же математическая модель,
идеальные истории. Поэтому мы считаем то, что умный всегда отвечает на вопрос. Весело отвечает
на них редко. Вот сейчас у нас идеальная модель же, да? Ну вот. Старосты... Ну ладно, ладно, так хорошо.
Не, ну старосты, эти люди ответственны и никак не связаны с учебой. То есть его преподаватель часто
вызывает просто потому, что знает, как зовут человека, поэтому чтобы не было неловко, то вызывают
его. Ну и погнали. Считать вероятность события А по этой формуле. Я не буду формулу расписывать,
да, вот именно буквенно. То есть получается у нас сумма из трёх слагаемых, где вероятности условий
домножаются на условную вероятность. То есть у нас Б1 это что такое? 1 шестая умножить на 1,
но это редко происходит. Дальше Б2, 1 вторая на 1 десятую. И Б3 у нас получается 1 треть на 1 треть.
Так, ну сейчас самое сложное, надо это сложить, господи боже мой. Это 1 шестая, плюс 1 двадцатая,
плюс 1 девятая. Лучше я не смог подобрать чисел, я прошу за это прощение. Вот, ну по-моему,
всё плохо, да? Нет, ну давайте досчитаем всё-таки. Это тяжело. То есть у нас что будет 2 тройки,
2 двойки, да, господи, 1 пятёрка. 180. Ну 2 тройки, 2 двойки, 1 пятёрка, да, 180.
Здесь мы умножаем на сколько? На 30, да, тут мы умножаем на 9, да, а тут на 20. Так, и сумма у
нас получается 59, 180. Ну то есть это почти, ну поскольку 59 это почти 60, да, то есть это почти 1 треть.
Могли ли мы ожидать такое? То есть смотрите, умный отвечает всегда, но вызывает его редко. Конечно,
я тут нехорошо сделал. Умный отвечает всегда, но вызывает его редко. Весёлый, его вызывают в
половине случаев, отвечает он вообще крайне редко. Ну вот, ну здесь у нас соответственно по 1 треть.
То есть ответ, то что он будет близок 1 треть, можно было предугадать именно, потому что вот здесь
у нас эта единичка занижается за счёт 1 шестой, а здесь и 1 десятая, она сильно влияет за счёт вот
этой 1 второй. Ну и получилась в итоге 1 треть. История ясна? Ну мы же всегда чем-то пренебрегаем,
когда... Ну слушай, ну преподаватель старенький может быть. Так, погнали дальше. Следующая,
третья формула, формула Байс. Это так называемая формула на нахождение пастериозной вероятности,
пожалуйста, поднимите руки, кто различает вот эти два понятия, философских, априорные и
пастериозные. Это философские термины, ну вот вы с ними пересечётесь, ну получается там на четвёртом
курсе, но вообще на самом деле надо знать, это как-то неприлично. То есть это вещи такие известные. Это
латинские термины, соответственно априори это доопытные, ну как бы дословно по-латыни это
доопыта, а пастериорные это послеопыта. Соответственно знания подразумевается априорные,
те которые вложены в вас, а пастериорные это те которые получены вследствие опыта.
Соответственно там философии, там постоянные шли, раньше что сейчас там происходит я не знаю,
но когда нам преподавали философию, я поступал в госпитальнатуру, мне надо было её выучить,
так что я в этом разбирался, они всё время спорили о том, что какие знания у нас априорные,
и вообще существует ли априорные знания изначально. Это тоже вот наподумать потом,
если вы будете в электричке ехать, делать будет нечего. Когда Ньютон, там непонятно Ньютон или
Лейбенис, в выпуске Ньютона они соответственно дифференциальные и интегральные исчисления
как бы написали, никто не понял вообще, что было сделано. Потом постепенно, постепенно, поскольку
это всё применялось активно, человечество начало постигать всё то, что было сделано. Иронично,
что сейчас для вас вообще вопрос интегрального и дифференциального исчисления это какая-то
банальность, согласитесь, покивайте. Боже мой, вы это делали в 10-м, кто-то в 10-м, кто-то в 11-м
классе, в зависимости от той программы, по которой вы учились. Это что вопрос о том, что люди раньше
были тупее, если они с таким трудом понимали то, что сделан Ньютон. Ну, предположим, там,
сколько-то лет после того, как были опубликованы работы. Казалось бы, нет, да, но то есть многие даже
считают из старшего поколения то, что раньше дети были умнее. Иронично, я сейчас пришёл с
предпосветом, мы как раз именно это обсуждали, да, со старшеками, да, то, что студенты пошли не те.
Но с другой стороны, ведь очевидно то, что вы воспринимаете эти сложные действительно вещи куда
легче, чем поколение до этого. Почему это так происходит? Ну вот именно потому, что как бы у вас
на генетическом уровне вот это вот закладывается. Ну то есть есть какие-то апостериорные знания,
которые ну постепенно выкапываются, да. Ну вот, я честно не шарю в этой теме, вот вот честно,
вот я тогда философию прошёл, но просто вот эта тема с апостериорным знанием, она очень,
с апостериорным, извините, с апостериорным знанием, потому что с апостериорным всё понятно,
чего, у вас был опыт, вы не знали, вы узнали, вообще неинтересно. А априорное знание, пишется там,
идёт априори, там по-моему как слышится, так и пишется, априори и апостериори. Я вот не помню
окончания какие, но это латынь, поэтому кто-нибудь посмотрите, пожалуйста. Ну вот, то есть это после
опыта, это до опыта. И соответственно, одна из обязательных частей любого курса философии — это
критика чистого разума Канта, ну слышали такой мануэл Канта, ну слышали, ну вот. Там вот тоже
вот рассуждения идут о том, что какие знания в нас заложены, то есть представление о пространстве
и времени, они тоже. Вот ребёнок рождается, новорождённый, вот пространство и время,
оно в него заложено, представление о том, как это всё происходит, или оно тоже постепенно у него
складывается в процессе роста. На философии всё пройдёте, я не специалист. Хорошо,
почему я сейчас вот это, почему апостериорное знание? Вернёмся вот к той истории. Предположим,
вопрос в том, что преподаватель ну совсем старенький, он не отметил, кто отвечал. И у него
просто напротив задач стоит плюсик и минусик, но эту задачу справились, разобрали, здесь не
справились, не разобрали, ну и так далее. И вы задаёте следующим вопросом, а какова вероятность
того, что с учётом того, что как бы задача была решена, её рассказывал, ну давайте старость.
То есть ещё раз, случайность осталась из-за чего? Из-за того, что вы не знаете в точности результат
эксперимента, да, то есть как бы вы знаете, только бывает или не бывает решена задача,
но какие-то тонкости от вас ушли, ну как в покеры, помните я объяснял, да, то есть как бы вы часть кар
знаете, а что там закрыто и не знаете. Здесь тоже самое, вы знаете результат, кончательный такой
результат эксперимента, но как вы к нему пришли неизвестно. Это понятно? Понятно, почему называется
опространение. И вопрос, как искать вот такие вероятности, если нам известны, но те же вещи,
которые были объявлены до этого. Доказательство формулы короче, чем её формулировка, поэтому я
буду формулировать и доказывать сразу, хорошо? Это условная вероятность, ну условная вероятность,
пока про неё немного знаем, поэтому пишем просто по определению. То есть это вероятность pour
сечение делить на вероятность условия. Соответственно, здесь у нас есть ещё
замечание, что вероятность А больше нуля, но всегда, когда мы пишем условный вероятность,
сейчас у нас вероятность условия всегда положительна. Так, вероятность пересечение в
точке того, что мы знаем, только вы inflict условия и условные вероятности, вероятность
мы можем расписать как произведение вероятности
условия на условную вероятность.
Это есть?
Хорошо.
Вероятные события мы можем расписать по форму ли полной
вероятности.
То есть это сумма PoCat единички, ну там либо до n, либо до
бесконечности вероятности А при условии BeCat на вероятности
BeCat.
Ну то есть если вот мы возьмем наш пример, какова вероятность
того, что вызвали старость, если ответ на вопрос все-таки
был дан.
Ну погнали считать.
Так, примеры тот же самый, B3 при условии A, что у нас
получается в числителе 1 3 1 3, знаменатели 1 3 1, а
если 1 3, нет 59, 180, извините, что у нас получилось, 20, 59,
да?
Есть да?
Вору, не вору?
Вроде не вору.
Ну то есть примерно 1 3 вышло, история ясна, какие-то
комментарии будут по поводу эту?
Ну вот у меня, когда я как бы слушал курс тезуеды,
у меня были комментарии по поводу эту формулу.
Значение похоже, потому что я вместо того, чтобы
сесть и подобрать хорошие коэффициенты, выдумываю
их на ходу, смотря на разных старост.
И плохие получились, ну плохое сочетание чисел
не видно, ну плохо.
Ну давайте сделаем попроще, как вы думаете, какова будет
вероятность того, что вызвали, ну кого, веселого, давайте
возьмем этого, умного, лучше умного, какова вероятность
того, что вызвали умные, если ответ был дан?
Эта вероятность должна быть какая, большая, маленькая?
Давайте посчитаем.
Ну ответ же был дан.
Веселые ответы практически не дают, а старости их дают
редко.
То есть кажется, что вероятность все-таки должна быть велика,
ведь у нас условная, потому что ответ был дан.
Давайте просто найдем честно, то есть здесь у нас получается,
что вероятность, то есть 1 шестая на 1, а тут у нас,
ну по-прежнему наша почти 1 треть, 59, 180.
Если мы 180 закинем наверх, у нас там что получается?
Мы на 6 делим, 30, да?
30, 59, почти 1 вторая.
То есть если ответ был дан, вероятность того, что его
дал умный, 1 вторая, хотя его вызывают крайне редко.
Почему 1 вторая?
Потому что старости редко отвечают, а веселого практически
не отвечают.
Да, это бывает, но редко.
Есть ли еще какие-то замечания по поводу вот этого?
Ну мы же идеализируем модель, когда строим.
Еще раз смотрите, мы живем в рамках какой-то идеальной
модели.
Когда вы будете решать задачки на подбрасывание
монеток, у вас не будет элементарных исходов в соответствующей
ситуации, то что монетка застряла между половицами
паркета на полу, потому что это бывает редко.
Иногда вы идеализируете модель, потому что вы посчитать
иначе не можете.
У меня было, мы же как, у нас на Мехмате, так же как
и у вас, мы распределяемся по кафетам после второго
курса, и я сначала пошел к мужику, который занимается
провожением теории вероятности в химии-биологии.
А у моих родителей доктора химических наук, я думаю,
ха, у меня такие консультанты шикарные.
И дальше была беда, он мне говорит, что скорость реакции
пропорциональна концентрации.
Я говорю, это не так.
Ну то есть, как бы, часто это так, но есть куча реакций,
в которых это не так.
Он говорит, забей, потому что мы работаем с математической
моделью, которая там ля-ля-ля-ля-ля-ля-ля.
Ну вот, я тогда, будучи третий курсник, вообще не понял,
что мне говорят.
Ну потому что у нас тервер начинался с того, что вот
вам как бы Омега ФП, и пошли, погнали математикой.
Это была чисто математическая дисциплина.
И вот эта связка между экспериментом и мат-моделью, у нас она
не прострадалась, но потому что она нам в принципе не
нужна, мы математики.
Я вам толкал речь о том, что вы обделенные, ну в смысле
не вы, а помышленники.
Это правда.
Ну то есть, я даже тогда не понял, кстати, я как бы
ушел, занялся, я как бы сменил после третьего курса и диплом
писал у другого научного руководителя.
Я не смог.
Теперь я понимаю, потому что, то есть, это как бы не
он не молодец, а я не молодец.
Вот, это в тему мат-моделей еще раз сказали.
Еще есть какие-то замечания по поводу этого?
На самом деле есть.
У вас нет ощущения, что что-то фигня какая-то?
Вот у меня на самом деле, то есть, нам рассказали эту
форуму Байлса, окей, доказали, решили пару задач, доказали
на экзамене.
После этого я с ней не пересекался вообще никогда.
То есть, я же потом кандидатский минимум сдавал, диссертацию
писал, защищал и так далее.
И ни разу не пересекался с форуму Байлса.
Хотя все остальные вещи, которые мы с вами будем
проходить, они очень прикладные, они вылезают везде и всюду.
Потом я о ней вспомнил, только когда начал вести семинары
на Мехмате, потом здесь читать лекции и так далее.
Плюс в математической статистике есть такой Байлсовский
подход к оценению, вы будете его проходить, и он в середине
прошлого века, по-моему там 60-е, 70-е годы, там аккуратно
описали этот способ построения оценок, все, ура, и сказали
эти оценки, Найфки никому не нужны, потому что у нас
есть оценки, которые строятся методом максимального
предподобия, там наименьшее дисперсии, эти оценки
бесполезные.
Ну, в принципе, построили, люди диссертации защитили,
все хорошо, живем дальше, так в науке часто бывает.
То есть, проработали какую-то тему, там люди диссертации
защитили, там сели на должность профессора, и дальше спокойно
живут.
А потом, что оказалось, когда начался весь этот
бум с машинным обучением, оказалось, что вот этот
Байлсовский подход к оцениванию, в машинном обучении вероятностные
подходы, они там, на них все зиждется, и оказались
то, что эти Байлсовские методы вообще безумно применимы.
Я в силу своей убогости не знаю, как применимы.
Ну, то есть, как бы я в машинке не шарю, вообще никак.
Ну, это все идет отсюда, то есть, чтобы вы прониклись
вот этой историей, то есть, потом вы с ней пересечетесь,
на мат-статистике у вас будет Байлсовский подход
к оцениванию, там вообще шаманство полнейшее.
Вот реально, там у вас, как бы, вы подбираете распределение
параметра, чтобы там что-то с оценкой было, ну, то есть,
шаманство полное, а потом это вылезает в машинке,
ну, вот, знаешь, и люди даже знают.
Просто запомните, как бы, то, что вот это вот, потом
будет вылезать отовсюду, хотя сейчас это кажется
какой-то фигней.
История ясна?
Запомните.
И четвертая, это формула умножения вероятности,
которую мы сегодня разберем.
Форма умножения вероятности.
Сразу начнем с задачки.
Так, нам нужен мышка какая-нибудь, ну, в смысле, таком будем
экспериментировать.
Давайте на вас, хорошо.
Берем простую задачку.
У нас есть уровня, в которой у нас есть сколько белых
и сколько красных шаров.
Семь белых и, хотите, семь, ну, черт тогда, я сегодня
не попадаю ни в числа, ни в людей, в суть, ну ладно.
Так, семь белых и семь красных.
И мы вытаскиваем три штуки.
Издается вопрос о том, какова вероятность вытащить
такую последовательность шаров.
Белый, вслед за ним красный, вслед за ним белый.
Дальше, как говорил профессор Терелони из Гарри Путера,
освободите свой разум и скажите, какой будет ответ.
Ну, то есть, что мне нужно сделать, чтобы посчитать
вот эту вероятность.
Давайте все-таки не семь, давайте вот здесь девять.
Ну, просто то, что было.
Да, извини, пожалуйста, без возвращения, то есть,
мы последовательно вынули, положили и вот нам нужно
получить последовательность трех шариков.
А можно диктовать числа?
Ну, я свожу, ладно.
Ну вот, поднимите, пожалуйста, руки те, кто сказал бы то
же самое, что сказал докладчик.
Ну, если бы я вот выбрал не его, а вас, то есть, практически
все.
А теперь мы пытаемся понять, почему это вообще правда.
То есть, мы умножаем вероятность.
А почему мы, кстати, умножаем?
А как вас зовут?
Гера.
Гера.
Гера.
Почему мы умножаем?
Потому что способ выбрать второй шар не зависит от
самого, как он от первой.
Это верно?
Это, кстати, вот это говорят практически все.
Ну, я короче скажу.
Ну, это же независимая вещь.
Почему умножаем вероятность?
Мы независимость еще не брали, но все же знают то, что
если событие независимое, мы вероятности умножаем.
Ну, вы это знаете.
Я думаю, в школе, курсе, террере, все проходили.
Все проходили в школе, курсе, террере.
Приду в голосу прямо, да?
Хорошо.
И практически все говорят, как и Гера.
Почему мы умножаем?
Ну, они же независимые.
С другой стороны, если вы задумаетесь хотя бы на
три секунды, вы также как Тихон скажете, ну нет, конечно.
Ну, потому что ведь, смотрите, вот когда мы пишем вот эту
Понятно то, что мы учитываем информацию о том, что уже
белый в уровне пропал один.
Мы же как бы девять из пятнадцати.
То есть у нас теперь в уровне не шестнадцать шаров, а пятнадцать,
причем пропавший именно белый.
Так что сказать то, что вот эта вещь не зависит от этой,
да нифига.
Вот мы же зависим.
Тогда почему умножаем?
И всегда вот в этот момент повисает тишина.
То есть люди не понимают, почему умножаем.
При этом каждый, каждый человек, которому вы зададите
этот вопрос, дает такое решение, и оно правильное.
Мы сейчас с вами докажем, почему это действительно
работает.
Вот это вещь, которую я понять не могу, если честно.
То есть это тоже какой-то как бы память на уровне
генома.
Ну, как бы, да, действительно, вероятность не надо считать
так, причем почему, никто объяснить не может.
Ну, если только вы не сильно задумаетесь и не докажете
то, что мы сейчас с вами докажем.
Вот так с полточка Фика объяснишь.
А объяснение стандартной независимости, естественно,
бред.
Слушай, можно делать все, что угодно.
Вопрос откуда?
Почему вы все даете правильный ответ, хотя, когда вы пытаетесь
обосновать, вы говорите очевидную неправду.
Теорем, погнали, теорем.
Если а1 и так далее аn события, то есть это есть элемент
f, при этом они все положительная вероятность для любого i,
то тогда для того, чтобы посчитать вероятность пересечения
их всех, все ли помнят о том, что когда между множествами
никакая теоретика множественной операции не написана, это
значит, что там пересечение.
Ну, это так, если множество не написано, так же, как
с умножением, если ничего нет между там 2х, это значит
2 умножить на х событие с множествами точно так
же.
Если написано а, а, б, это значит, а пересекаем
с б.
Хорошо?
Ну, вот как это считается?
Мы берем вероятность первого, которую потом домножаем
на вероятность второго при условии первого, которую
потом домножаем на вероятность третьего при условии и
первого, и второго, и это значит пересечение и так
далее.
Последний множитель это вероятность последнего
при условии всех предыдущих.
Где что-нибудь не так, я не понимаю?
Это было а1, может, не очень внятно, было а1, ну ладно.
Доказательства, ну, примерно, тут как бы очевидно, смотри,
да.
Если мы сейчас вот начнем расписывать вот это вот
произведение, соответственно, первый множитель это вероятность
а1, второй множитель как условная вероятность
мы расписываем как отношение пересечения к вероятности
условия.
Третью вероятность тоже расписываем как вероятность
пересечения на вероятность условия и так далее.
и так далее. Последний множитель в числителе у нас получается пересечение всех
делить на вероятность пересечения всех, кроме последнего.
Легко видеть то, что знаменатель каждой последующей дроби будет сокращаться с числителем предыдущей,
и в результате у нас останется только числитель последней дроби.
То есть это то, что мы и хотели.
То есть доказательство очевидно и при этом понятно, что
герой делал именно вот это. Что это? Вероятность того, что 1 выточенный будет белым.
Это вероятность того, что выточенный будет красный при условии, что предыдущий выточенный был белый.
Ну и так далее. А это то, что 3-й будет вытащенный белый при условии, что до этого мы выточили белый и красный.
История ясна?
Вот это, на самом деле, для меня какая-то локальная
тайна.
Я не могу сказать, что я сильно пытался найти ответ
на этот вопрос.
Если ты поймаешь человека на улице, он тебе даст точно
такой же ответ.
Ну, не совсем бабушку, но какой-нибудь, предположим,
возьмете школьника совсем среднего, он скажет то же
самое.
А можно отнестись к этому как и у шепсту.
Ну вот, вы можете поэкспериментировать на самом деле.
Возьмите, не знаю, родители, если у вас родители, люди
не близкие к математике, попробуйте задать им этот
вопрос.
Хотя, скорее всего, родители пошлют, потому что надо
задумываться хотя бы над сюжетной линией, что
там происходит.
Ладно, с формулами это все, то есть это те вещи, которыми
вам нужно пользоваться.
Причем я напомню присутствующим то, что мы с вами все, что
мы делали, мы делали в двух случаях.
Либо у нас модель, которую мы с вами построили, была
дискретно.
Причем дискретно у нас там два случая, либо она у
вас конечная, и тогда событие это любое подмножество
и вероятность, ну там всего два свойства, вероятность
всего единичка и вероятность объединений двух непересекающихся
сумма.
Либо у вас дискретно в том плане то, что у вас счетное
множество, счетное Омега большое, но тогда у вас еще
добавляется требование вот этой вот счетной аддитивности
P.
Ну то есть, если вероятность счетного объединения, то
там тоже это будет сумма вероятности.
И последнее, мы когда делали с геометрической вероятностью
то есть вот только для этих трех типов моделей мы выводим
то, что мы сейчас выводим, просто потому что других
моделей мы не знаем.
Это ясно?
Все, что мы делаем, оно будет верно всегда, да, конечно.
Просто делать мы это можем только для таких, потому
что большего матапарата у нас нет.
Так, следующий объект – независимость.
По сути главный объект вероятности, почему сейчас объясню.
Начинаем мы с независимости событий.
Еще раз, что подозревается под событиями?
У вас есть математическая модель.
Омега, пространство элементарных исходов f, это событие, которое
под множество омега большое, и p – это функция, определенная
на f со значениями в 0,1.
Пока у нас только омега может быть, то, что я проговорил,
счетная или под множество rn, f – это множество всех
под множество случаев дискретно, и p мы определяем просто
поточечным в случае дискретных моделей.
Если геометрическая вероятность, тогда тоже там обсудили
как определять.
И сейчас нам нужно вести понятие независимости.
Что значит то, что событие А независимо с событием
Б?
С понятием независимости возникает из-за того, что
очень большое… Ну вот, на памяти.
Про независимость вы слышали, начиная какого-то с раннего
возраста.
Во взрослой жизни тебе нужно быть независимым, зарабатывать
много денег, и у вас как бы независимость въелась
в мозг именно с чистой человеческой точки зрения.
И поэтому в те веры бывает тяжело, когда выводите понятие
независимости, потому что все время вот этот хвост
жизненного опыта, особенно вот подросткового опыта,
когда тема независимости особенно остро воспринимается,
он вас тянет назад.
Но сначала давайте попробуем вообще сообразить, о чем
речь.
То есть вот у вас есть два события А и Б, и мы говорим
о том, что события А независимы от события Б.
Что бы это могло быть?
Вот у вас есть омега большая, вот есть события А и события
Б.
Вот если они не пересекаются, можно говорить о том, что
они независимы?
Ну, идейно, почему нет?
И что?
Ну конечно.
Ну то есть если у нас случилось событие А, что значит случилось
событие А?
Было реализовано омега маленький, который из А.
Что мы тогда можем сказать про Б?
Ну его точно нет.
Поэтому если они не пересекаются, называть их независимыми
вообще странно.
Потому что если произошло А, то Б точно не произошло.
То есть мы можем сделать какой-то вывод А, Б, по тому
произошло или не произошло А.
Хорошо.
То есть кажется, что они должны пересекаться.
Ну хорошо, а если они будут пересекаться, то какое
естественное определение независимости А и Б?
Ну тут только можно знать правильный ответ, как бы
дойти до этой мысли на самом деле тяжело.
То есть если мы рассмотрим недавно введенный нами
объект, условную вероятность А при условии Б, ну будем
считать, что вероятность события Б больше нуля.
Вероятность условная А относительно Б.
То есть какова вероятность А, если мы знаем, что произошло
событие Б?
Если мы считаем, что А и Б независимы, то что мы
ожидаем от этой вероятности?
Что она будет равна вероятности А.
То есть информация о том, произошло или не произошло
событие Б, не должно повлиять на вероятность А.
Идейно понятно?
Минус этого определения очевиден, потому что независимость
событий должно быть очевидна симметричным отношением.
Понятно, что я сказал, да?
Да, то есть почему мы сейчас будем раскрывать?
Потому что здесь у нас, во-первых, есть требования
на Б, во-вторых, это как бы условие, что А не зависит
от Б.
А нам хочется, ну мы же сейчас понимаем, что мы хотим,
нам хочется, чтобы понятие независимости событий было
симметричным.
Поэтому мы распишем условную вероятность, ну и умножим
на знаменатель.
И именно вот это вот равенство становится определением
независимости события А и Б, то есть определением.
События А и Б называются независимыми, если вероятность
их пересечения равна произведению их вероятности.
Обозначается, ну вот так, как я писал, ну это как
антагональность, только с двумя вертикальным палочками.
Обычно тут никакой эмоциональной реакции у людей не возникает,
потому что мы как бы вводим новые объекты и исследуем
их свойства.
Это просто какое-то новое понятие в науке, ладно,
хорошо.
Но в самом деле это центральное понятие тервера, потому
что все, чем мы будем заниматься потом, это и есть вот тот
действительный анализ, о котором я говорю, потому
что мы будем работать с вами со случайными величинами,
это будут измеримые функции.
Математическое ожидание, всякие дисперсии и так
далее – это интеграл либег, случайные величины – это
будут измеримые функции, я уже говорил об этом, ну
вот, извините.
Ну вот, вероятность – это просто там будут сигмоаддитивные
меры напрямую.
Что это понятие действительного анализа, который является
частью функционального анализа?
И вот понятие, которого нету в этих науках, это понятие
независимости множеств.
Оно вообще не логично с точки зрения действительного
анализа, потому что посмотрите вот на эту запись.
Мы с вами когда будем заниматься?
Ведь идейная мера – это продолжение понятия площади,
ну, идейно, это понятно?
Ну вот, а давайте посмотрим, что здесь получается, то
есть как бы площадь пересечения множества равна произведению
площадей.
То есть это какой-то бред с точки зрения именно
вот и функционального действительного анализа, они даже такого
не изучали.
И, собственно говоря, вот мой семинарист, он же ваш
лектор на продвинутом потоке, ну вот, мы когда действительного
анализа занимались, он сказал то, что, ну в принципе
этот ваш тер веры, он как бы это под множество нашего
великого функционального анализа, но блин, да, там
есть вот понятие независимости.
Вот то, чего нет.
Вот, чтобы вы поняли, что это центральная вещь.
И если вы смотрите там какие-то публикации современной
тер веры, ну современной я вру, когда я занимался
тер верой, было очень много, игрались с понятием независимости,
то есть самая простая ситуация, когда там случайная величина,
мы попозже разберем, независимая.
Хорошо, а если они зависимы, то как они зависимы?
Водится понятие слабой зависимости, сильной зависимости,
там такие метрики специальные есть.
А что происходит, когда слабая зависимость, когда
сильная зависимость, и играются именно вот с этой
вещью.
Так что это центральное понятие в тер веры, чтобы
вы осознали.
Так.
С чем?
Ну то есть, что мы получили благодаря такому определению?
Мы получили то, что мы можем сейчас рассматривать ситуации,
когда а и б события нулевой вероятности, и при этом все
хорошо.
Давайте посмотрим, если у вас событие b нулевой вероятности,
вот это хрень обнуляется, а что происходит с этим?
Она тоже обнуляется, то есть мы получили свойство
о том, что у вас события нулевой вероятности независимы
с любым событием.
Это плохо, если так глубоко задуматься.
Ну да, оно же никогда не происходит, пока оно не произойдет.
Поэтому именно в качестве определения зависимости
берется это, а не это, потому что включают в себя вот
эти неприятные случаи, когда событие у вас вероятности
ноль.
Пример.
И что такое схема испытаний b нулевой?
Это математическая модель, соответствующая эксперименту,
когда вы проводите n независимых испытаний, результат каждого
испытания у вас 0 или e1, при этом вероятность единички
у вас p.
Помните, как мы строили?
Ну вот, и как у нас там было?
Элементальный исход – это был набор из нулей единиц.
Соответственно, там мощность омега у вас была 2 в степени
n.
Моя мощность омега определялась, вероятность определялась
по точке, поскольку это дискретная модель, конечную
вероятность определяем в каждой точке.
Она определялась как p в степени сумма ежитых пожирит
единички до n на единичку минус p в степени n минус
суммы ежитых.
p соответственно – это параметр нашей математической модели,
а эта чиселка от 0 до 1.
Это ясно?
Но вы же помните, что когда мы эту модель строили, мы
начали с классической модели, потом увидели, что так сложно
не надо, и можно работать с такой моделью.
Согласны?
Ну то есть при построении модели, вот здесь, мы вроде
как вот эту независимость не учитывали.
Давайте сейчас проверим, она сохранилась или нет.
Ну то есть, я сейчас осматриваю два события.
Событие А.
Оно состоит в чем?
Мы рассматриваем все такие омега, что и первое равно
одному.
Ну то есть, первый был успех, а событие Б, мы рассмотрим
это такие элементарные сходы, что последний был
успех.
Ну вообще, идейно, как бы независимость должна
быть.
Но блин, у нас как бы независимость, есть сейчас математическое
понятие, для того чтобы проверить, нужно просто тупо
проверить равенство.
Погнали проверять.
То есть для этого нам нужно посчитать три вероятности
и проверить это.
Как посчитать вероятность события А?
Мы ее считали, это было, какие-то мучения были, складывали.
Она у нас получилась П, вероятность события Б тоже получилась
П.
Ну а теперь давайте посчитаем вероятность А пересеченной
с Б.
Ну то есть это что?
Мы должны просуммировать по всем омегамальникам
таким, что у них на первой позиции единица и на последней
позиции единица вероятности этих омег маленьких, то
есть сумма по ежитых по ж от единички до n, n минус
сумма по ж от единички до n и житых.
Ну теперь смотрим, мы прекрасно понимаем, что вот в этой сумме
первое слагаемое, последнее слагаемое, это у нас единички.
Ну то есть для каждого слагаемого, вот первое и последнее
слагаемое вот в этих суммах кривых мы знаем.
Окей, получается, я могу вынести.
Вот здесь вот у нас будет, то есть получается П в квадрате,
сумма у нас получается П в степени сумма по ж от
двойки до n минус 1 из житых на единичка минус П в степени.
Ну вот здесь вот у нас первое и последнее двойка.
Черт, как неудачно.
Можно я сейчас вот так сделаю.
На единичка минус П в степени n минус 2 минус сумма по ж
от двойки до n минус 1 и житых.
Так, вы видите то, что пишется, да?
Ну если не видите, то догадывайтесь.
Ну ладно, видно, вот последнее вообще видно.
А теперь давайте посмотрим, почему идет суммирование.
Суммирование идет по всем наборам i2 и так далее i n
минус 1.
По всем возможным наборам от i2 до i n минус 1, согласны?
А мы с вами разбирали то, что суммы вот такого вида
какие, чему будут равны суммы такого вида.
Еще раз смотрите, вы суммируете по всем наборам длины n минус
2, П в степени сумма и житых в этом наборе на единицу
минус П в степени n минус 2 на сумму и житых в этом
наборе.
Ну там через бином мы получаем, что это будет единичка.
Это еще не бином, тут ц никаких нету, но через
бином мы с вами доказывали, что вот такая сумма будет
равна одному.
Кивайте, что было.
Окей.
Ну то есть в результате у нас получилась П квадрат.
Ура!
Ну то есть действительно мы получили то, что вероятность
пересечения равна произведению вероятности.
Так что в ответ на вопрос Тихона, как мы это делаем,
сидим и проверяем.
А разве это не делает процесс бессмысленности?
В смысле?
Ну мы же хотели, например, задача была не то чтобы проверить
независимые события, а посчитать вероятность того, что первые
и последние единицы.
Я бы такой, ну события независимые значат П умножить на П.
Ну вот теперь ты знаешь, что они независимы.
Ну теперь смотри, то есть как обычно, то есть если
ты математик и ты работаешь сразу в математической
модели, тебе там уже сразу говорится о том, кто с кем
там независим.
Ну такие условия задач.
Если только одно из них не пуст.
Ну да, очень важное замечание, спасибо.
Если только кто-то из них не является нулевой вероятностью.
Ну, идейно да, но у нас есть вот этот вот выраженный
случай с вероятностью, с событием нулевой вероятности,
но окей, мы будем считать, что событие нулевой вероятности
независимо с любым.
То есть в нашем виде это ничему не противоречит.
Именно поэтому мы взяли в качестве определения
не вот это, а вот это.
Так и следующая история, это когда у вас событий
больше.
Давайте возьмем n событий.
То есть расширим немножко понятие независимости
событий на большее количество событий.
А теперь второй случай.
Когда у вас n событий.
Да, а вон видишь, там было написано единичка событий.
Я буду стараться писать аккуратнее.
Тут возникает два понятия.
Независимость попарная.
Так, события a1 и так далее an, называются независимыми
попарно.
Но тут прост.
Если для любых и j, ну и неравно j конечно, они независимы
между собой.
Потому что когда у вас событий 2, мы знаем, что такое независимость.
И второе определение события, называются независимыми
в совокупности, независимыми в совокупности, называются
независимыми в совокупности, если для любого их количества
и любого их набора.
Понятно, то есть i1 и k зажаты между единичкой и n, и они
между собой неравны.
Будет выполна следующее, что вероятность пересечения
ашек с этими индексами, равно произведению вероятности
этих ашек.
Вопрос, какое понятие сильнее.
Из того, что следует.
Какая непрерывность сильнее, равномерная или обычная?
Если у вас функция непрерывная равномерно, то она непрерывно
прост.
Давайте мы обозначим это определение 1, вот это
определение 2.
Из кого что следует?
Из второго следует первое.
Из второго следует первое, почему?
Конечно, можно просто брать k равной 2, но как бы для
любого их количества, то есть частности для 2, для
любых двух будет выполнено вот это.
В обратную сторону неверно, вы будете в этом убеждаться
постоянно, есть классические примеры, называются пример
Бернштейна.
Их можно много построить, но это такой стандартный
и он визуализируется хорошо.
Давайте возьмем с вами тетрайдер, чьи вершины мы покрасим
в три цвета.
Давайте мы возьмем серый, какие вы унылые на самом
деле, только сейчас это понял.
Паярчина, оранжевый и синий, нет, черный, хорошо, а третью
вершину покрасим в три цвета, будет серый, черный и оранжевый.
Элементарный исход это вершинка, вероятность каждой
вершинки у нас одна четвертая, то есть такая равновероятность
И мы будем насматривать в качестве событий, какого
цвета вершина, то есть А это вершина у нас оранжевая,
Б вершина черная и С это у нас вершина синяя.
Я на следующей лекции приду в красном бомбере, в ярко
красном, чтобы было веселее.
Погнали, вероятности А, Б и С чему равны?
Ну то, что вероятность оранжевая, одна вторая, либо это, либо
это вершина.
Соответственно, вероятности всех этих событий у нас
получаются одна вторая.
Вероятность их пересечения, всех троих, это одна четвертая,
потому что только одна вершинка обладает всеми
тремя свойствами.
При этом, если мы будем брать пересечение любых двух,
вероятность А и Б, то есть это вероятность и оранжевая,
и черная.
Такая только одна, вот эта.
Эта вероятность получается тоже одна четвертая.
А, С, А, Б и Б, С.
И что у нас с вами получается?
Получается, что попарная независимость у вас выполнена,
потому что для любой пары вероятность пересечения
будет равна произведению вероятности.
А вероятности независимости в совокупности нет, потому
что если мы возьмем в качестве К тройку, тогда мы у нас,
если мы берем все три события, то вероятность пересечения
всех трёх не равна произведению вероятности этих трёх.
Ну всё.
Ну модель просто непрозрачная, ты не понимаешь какая там
была сюжетная линия раскраски этих вершин.
Ну, кстати, такое бывает, действительно, если вы
вам просто описывают мат-модель, вот именно то, что вы берёте,
предположим, там отрезок прямой, берёте такие множества
и такие множества, считаете, они получаются у вас почему-то
независимыми, хотя вы вообще не видите, почему они независимы,
это связано с чем?
С тем, что вы вот этой реальной задачи не видели, которая
привела к этой математической модели.
Понятно?
Ну то есть мысль в том, что вот это понятие независимости,
оно естественно соответствует тому понятию независимости,
которое из жизни пришло, потому что всё-таки тервер
это у нас, да, это идеальная модель, но всё-таки это
идеальная модель случайного эксперимента, который мы
видим всё время вокруг себя.
Это ясно?
Просто если вы начинаете докапываться вот тут, становится
очень тяжело.
Вы же можете, ну так же, как я докопался там до своего
научерка с тем, что вы мне модели строите, которые
не соответствуют реальности, так и там в геометре вы можете
докопаться, у нас не бывает линий нулевой толщиной,
но работает вся эта наука, потом, когда вы начинаете
применять в жизни, она работает.
Это всё в тему того, вот то, что я про ПМФ вам говорил.
Пока всё, то есть вот понятие вели, ну вот будете на семинарах
с ним работать.
Следующее понятие, с которым мы с вами сталкиваемся, это
понятие случайной величины.
Да, случайной величины.
У независимости будет третий пункт, но там будет независимость
случайных величин, для этого сначала нужно ввести, что
такое случайная величина.
Цель полагания очень простая, то есть случайная величина
это какая-то числовая характеристика случайного эксперимента,
который у вас есть.
Потому что одна из проблем, с которыми мы с вами столкнулись,
вообще в чём?
То, что мы с вами работаем с вероятностным пространством,
с математической моделью случайного эксперимента.
И в этой модели омега маленькая, ну элемент омега большого
это кто?
Элемент омега большого, это всё, что мы можем о нём
сказать.
То есть какой-то элемент множества, природа которого
неизвестна.
То есть ожидать то, что омега маленькая это число, да
нифига, у нас было сколько задач, где омега это там
кортежи или это часть прямой или ещё что-то, это понятно
сейчас?
И это основная проблема, потому что вся та математика,
которую вы наработали к данному моменту, она плюс-минус
бесполезна.
Ну потому что главный предмет предыдущего года, да, не
катечен, не ваш, Господи, кому это надо, а именно математический
анализ.
И там построен огромный математический аппарат,
который здесь оказался бесполезным.
Это же обидно.
Это во-первых.
Во-вторых, почему всё-таки именно матан – самая важная
дисциплина?
Потому что все всегда работают с числовыми, с числами, то
есть если даже у вас какая-то жизненная история, всё равно
вы стараетесь какую-то там метрику придумать, чтобы
у вас числа были, потому что с числами работать хорошо
удобно.
И здесь та же самая история, да, у вас случайный эксперимент,
он соответствует каждому результату случайного эксперимента
соответствует какому-то омега маленькому, какой-то
хренью какой-то быть, но вам обычно всё-таки нужны
числовые характеристики.
Ну, предположим, вы кидаете кости, вас интересует сумма
или схемы испытаний Бернуля, вас интересует количество
успехов.
Идея ясна?
Поэтому, определение, если омега не более чем счётно,
и любое под множество омега большое является событием,
то случайной величиной мы назовём любую функцию
числовую, определённую на пространстве элементарных
исходов.
Ну, пример стандартный, я его проговорил, если мы
рассмотрим схему испытаний Бернуля, ну, то есть когда
у вас омега маленькая, это набор из 0 единиц, на самом
деле, нас ведь обычно не интересует, на каком шаге
был успех, на какое бывание удачи, нас интересует количество.
То есть вот, пожалуйста, у вас числовая характеристика,
это будет число, ну, то есть даже вот так, вот это вот
яркий пример случайной величины.
С какими проблемами мы здесь сталкиваемся?
Что напрягает в том определении, которое я написал?
Любая.
Не, область определения, вот написано.
То есть это функция, которая каждому элементарному исходу
ставит в соответствии чиселку.
То есть хорошие примеры, некорректные, но хорошие,
то есть, ну да, вот эта оговорка что-то нас раздражает,
да, то есть мы же хотим всё-таки давать определение, как
вводить понятие, которое потом будут продолжаться
на общий случай, а что будет, когда оно не такое.
Потом опять же, мы же с вами работали не только с дискретными
моделями, у нас была модель геометрической вероятности,
помните?
Когда Омега это под множество РН, я его сюда не включил.
Казалось бы, с моделью работали, а как бы случайную величину
для неё не определяете.
А я, я и Иван Генрихич нехорошо.
Чем вызваны проблемы?
Смотрите, ещё раз, значение числовой характеристики,
но мы всё равно знать не будем, у нас же случайный
эксперимент, мы не знаем Омега маленького.
Ну то есть, если вы знаете результат случайного эксперимента,
он перестал быть случайным, всё, вот вы там, предположим,
вы ездите, вот вы попали в ДТП, вот вы уже в этом
ДТП сидите, вас в этот момент вообще эти вероятности попадания
в ДТП не интересуют, вас интересует, когда приедет
эти, ну мужчины эти, подними, плохо звучало, извините.
Ну вот, то есть нас интересует до проведения опыта, то есть
какая вероятность того, что мы попадём в этот ДТП,
то же самое с значениями к себе от Омега, вы не знаете
значения.
А тогда что вас будет интересовать?
Интересуют обычно вероятности про случайную величину.
Ну давайте рассмотрим, вот предположим, Омега маленькая
это у вас, например, некорректный, но хороший для понимания
результат вашей сессии, эмоционально близко, поэтому
хорошо воспринимается.
То есть, ну предположим, сколько у нас оценок обычно?
10?
Ну за сессию.
Ну там, дивзачёты, экзамены порядка 10, ну то есть вот
10 оценок с порядком, ну не совсем корректно, давайте
пример.
То есть Омега маленькая это набор И1 и так далее
и Н, и 10, где соответственно и житые это оценка за житые
экзамены.
Ну вот, соответственно, какие числовые характеристики
от Омега у нас бывают, которые вас интересуют?
Например, средняя, когда нам нужно средняя?
Когда?
Для обрамовки.
Правильно?
Там же сколько сейчас надо?
9, 8, 9, ну сколько было в предыдущем смеси?
8,6, 8,5.
8,6?
Фистеф уже не тот просто, ладно, ну вот, раньше 9 было,
ну вот.
Например, это, ну вот, кто может предположить ещё
одну как бы, ну это для отличников, а для максимум, когда максимум
кого интересовал?
Ну честно, ну нет, минимум интересует, минимум да,
почему минимум интересует?
Ну какой меркантильный, ужас просто.
Ну минимум на что влияет?
Ну это всё-таки как бы, мы сессию берём в окончании
сессии.
Передачи есть, передачи нет.
Ну вот.
Ну вот, например, для обрамовки ответ на какой вопрос нас
интересует.
То есть, кси от омега, ну сколько говорите, 8,6, да?
Ну то есть, какова вероятность того, что кси от омега, это
я перебозначу, кси от омега больше либо равно 8,6.
Других будет интересовать вероятность того, что это
от омега строго больше двух.
То есть, обычно нас интересуют ответы вот на такие вопросы.
Согласны?
Теперь давайте как бы всё-таки вернёмся как бы к математике.
Что такое п?
П – это функция, определённая для событий.
А здесь я ненавинство пишу.
У нас есть такое событие…
Сейчас, извините, там…
Так лучше?
Но это же не средний, это минимум.
У нас не бывает, у нас же целые оценки ставят в зачётку.
И зачёток больше нет.
Так, ещё раз.
Кто понимает, что тут написано «событие» на самом деле?
То есть, это вероятность всех таких омег маленьких,
что кси от омега больше либо равно 8,6.
То есть, вот эта вот хрень является под множеством омега
большого.
Ну да, это набор каких-то омег маленьких, на которых
кси от омега принимает такие значения, что выполнять не нравится.
Окей?
А теперь вопрос.
Если вот это не выполнено, то есть, если не любое под
множество омега большого является событием, то кто
вам сказал, что это элемент f?
Почему важно быть элементом f?
Потому что p определена на f.
Давайте вспомним геометрическую вероятность.
Помните, почему мы упёрлись вот в эту проблему с этой
несчастной f и p?
Потому что если у вас геометрическая вероятность,
то там вероятность считается как отношение этих
либеговских мер множеств.
Но у нас же не все множества измеримы по либегу.
Поэтому в качестве f мы можем брать множество всех
подмножеств.
Это понятно?
А теперь вот здесь вот кто вам сказал, что конкретно
вот это подмножество омега большого будет являться
элементом f?
Никто не обещал.
И поэтому определение случайной величины потом у вас
будет выглядеть как?
В теории вероятности.
То есть кси, которая является какой-то функцией изомегавер,
будет называться случайной величиной, если выполна
следующее требование.
Простите меня за то, что я сейчас скажу.
Для любого баррелевского множества напрямой будет
выполнено вот это требование.
Кси в минус первое от b будет являться элементом f.
Но почему кси в минус первое?
Смотрите, вот это вот множество можно сделать вот так.
Кси в минус первое от множества от 8,6 до 10.
Ну потому что наш средний балл не может быть больше
10.
10 бывало, знал людей.
Недолго правда, но бывало.
Так вы понимаете, что вот это равенство верно?
Я одно и то же записал.
Еще раз.
Прообраз чисел больше либо равных 8,6.
Это и есть те омега маленькие, на которых выполнено вот
это неравенство.
Вы это понимаете?
Окей.
То есть вот прообраз какого-то отрезка.
Вот он должен быть элементом f большое для того, чтобы
вот эта вероятность была определена.
Дальше там возникает законный вопрос, а для каких множеств
мы будем это требовать?
Ответ на это для баррелевских множеств.
А что такое баррелевские множества?
Узнайте через месяц.
Ну кто-то быстрее.
Это множество наименьшей сигма алгебры на прямой,
которое содержит все хорошие множества.
Хорошие множества у нас на прямой какие?
Очевидные, нужные для нас.
Ну это совсем хорошее.
То есть баррелевское поменьше.
Одно из определений баррелевского множества – это наименьшее
сигма алгебра, содержащее все открытые множества.
Но это сложно, потому что открытые множества, их
что-то там много и тяжело.
Это наименьшие сигма алгебры, содержащие все отрезки
или все интервалы, или все лучи.
Тяжко.
Вот именно для этого нужен этот курс, потому что я
говорю, что когда на третьей паре теории вероятности
людям выкатывает такое определение, они запоминают
только вот это.
То есть да, они потом к экзамену выучивают.
Нет, ну реально, что это просто…
Ну это истинная суть случайной личности.
Это числовая характеристика эксперимента.
Да, окей.
И они это понимают.
А вот этот вот доресок, они вообще не понимают.
Это что за хрень?
Зачем это надо?
Зачем это надо?
Потому что у нас есть глобальные проблемы с ФП.
И из-за этого может получиться так, что мы возьмем такую
УКСИ, что вот эта вещь вообще не определена.
А если она не определена, то нафиг нам вообще этот
весь тервер нужен, если мы не сможем отвечать на
вопрос о том, какая вероятность того, что у нас будет обрамовка.
Идея ясна?
Хорошо.
Я напомню еще раз, зачем нужен вот этот вот кусок
с тервером.
Мы как бы обозначаем проблемы, которые мы будем решать.
И один из больших кусков нашей второй части курса
– это будут измеримые отображения.
Вот эта вот вещь – это требование измеримости отображения
УКСИ.
Измеримые отображения, какие там есть свойства,
там то, что есть, вы складываете измеримый, получается
измеримый и так далее.
Теперь следующие объекты, которые у нас связаны
со случайной величиной.
Это определение, оно выглядит довольно случайно.
Вы выбрали какое-то множество, которое мы считаем приятным
и красивым.
Для всех?
Нет.
Б.
Бетатер.
Это просто что, с чем мы такие…
Это то, что нам нужно.
Это еще раз.
Берелевская сигма алгебра – это сигма алгебры, которая
содержит все множества, которые нам в принципе
могут понадобиться.
Ну то есть, еще раз, мы не можем определить для всех
под множество прямой, потому что, как мы выяснили
в случае неизмеримого полебегу, есть множество, которым
даже представить себе не множим, но они хреновы.
У них даже там полебегу не измерим.
Так и не надо с ними работать.
Не нужны они.
Мы же все-таки для провожений делаем.
А людям нужно отвечать вот на такие вопросы.
Больше, меньше чисел, принадлежат отрезку.
Этого достаточно.
То есть часто у меня вот там…
Ну вот мы читали у химиков теории вероятности.
На химфаке у нас была предпрактика.
И там, знаете, как определяли Берелевскую сигму алгебру?
Сигма алгебра состоящая из хороших множеств.
Это прямо определение было написано.
Не, ну слушайте, им вот это вообще нереально рассказать.
При этом человек, который читал, он как бы ученый,
профессор и так далее, он не мог ахинею нести.
Ну, в плане написать.
Поэтому он писал Берелевскую сигму алгебру,
когда у него спрашивали, что это такое, он говорил,
ну это все хорошие множества напрямую.
И достаточно.
Вот у вас будет отдельный семинар, посвященный к тому,
что вы разобрались с ней,
потому что поняли, из чего она состоит.
Хорошо?
Так, следующее это распределение случайной величины.
Распределение случайной величины.
Я вернусь к мысли о том, что нас не очень интересует
значение,
конкретное значение кси от омега.
Почему мы омегу не знаем?
Тут что возникает?
На том, что если у вас есть случайная величина,
то вслед за ней получается множество значений.
Случайной величины.
Поскольку мы сейчас работаем только лишь в этой ситуации,
когда у нас омега это дискретное
вероятностное пространство,
то и значение у нас тоже не более чем счетное количество.
Это понятно?
Кивните.
Я сейчас бы сказал простую вещь.
Область определения функций не более чем счетный,
значит множество значений функций тоже не более чем счетно.
И тогда что получается?
Каждому значению
к каждому значению
случайной величины
мы можем поставить в соответствии
вероятность, с которой принимается это значение.
Что такое по ката?
Это вероятность того,
что кси от омега
равняется х ката.
То есть значение случайной величины
и вероятность этих значений.
Давайте по примерам пойдем.
Примеры.
Первая это Бернулевская случайная величина.
Не путайте с Берномиальной.
Бернулевская.
Извините, Бернулевская распределение.
Ой, ей плохо. Извините.
Бернулевская распределение.
То есть у вас два значения.
То есть это подбрасывание монетки кривой.
Х ката у вас
извините
состоит из двух значений
нолика и единичка.
При этом вероятность нолика
у вас единичка минус п.
Вероятность единички у вас п.
П это какая-то чиселка от 0 до 1.
Х ката.
Еще раз.
У вас к пробегает
либо конечное, либо счетное множество значений
которые соответствуют номеру значения
своей случайной величины.
А п ката это вероятность
этого значения.
Хорошо, с тем же самым индексом.
Первый это Бернулевское распределение.
Будет иногда в задачах
вылезать.
conquer это биномеальное распределение.
Биномеальное распределение
у каждого распределения
естественно своей физический смысл
это распределение числа успеха
в схеме испытаний Бернуля.
Бернулиское распределение здесь,
а здесь биномеальное это
число успеха в схеме испытаний Бернуля.
Постарайтесь не путаться пожалуйста,
сложное, я понимаю, распределение числа успехов в схеме испытаний
Берноль.
Ну давайте его как бы найдем, хотя вы его и так уже знаете,
мы его несколько раз выводили, то есть у вас омега это есть и 1 и так далее и n,
и кси от омега это есть сумма и жидкость, то есть количество успехов, успех у нас это единичка.
Ну то есть смотрим кси от омега какие значения принимает от 0 до n, ну то есть n плюс одно значение,
то есть получается хкт у вас равно просто k, где k принадлежит множеству от 0 до n. Теперь погнали
вероятности, то есть тогда у нас пкт это будет вероятность того, что кси от омега равно ну просто
k. Мы с вами это считали, а на чему там было равно?
Был такой, да? Это мы с вами выводили даже.
Так, третье, полосоновское распределение, нет давайте геометрическое, геометрическое распределение.
Сумма пкт всегда единица. Ну давайте посмотрим, хкты это все значения случайной величины,
так? Если мы просуммируем все пкты, что мы получим? То есть это сумма по всем хкт принадлежащим х,
вероятности того, что кси равняется хкт. То есть получается эта вероятность получается какого события?
Они же все не пересекаются между собой, кси одномоменно не может принимать два значения,
то есть эти все события между собой не пересекаются, это видно? Хорошо, и если, то есть мы их можем
вместе собрать, эти события, поскольку мы складываем эти вероятности, вот если мы их вместе соберем,
мы получим какое событие? А чему равна вероятность того, что что-то произошло? Да, мы говорили,
что вероятность всего Омега у нас 1 обязательно. Так, геометрическое распределение, помните,
мы с вами строили модельку, в которой Тихон Безобразничий играл, пока не выиграет, было такое.
Ну то есть опять же тут можно случайно начинать, это что такое? Это номер эксперимента, на котором
Тихон выиграет, да? Ура! То есть получается у нас хкт, это что? Номер, на котором он выиграет,
это либо первый бросок, либо второй, и так далее до бесконечности. Ну то есть это первое распределение,
где у нас бесконечное число значений. Какие пкт получались? То есть пкт это вероятность того,
что кси равно ка, то есть Тихон выиграл на катом шаге. Чему она там была равна? 1-p в степени k-1 умножить
на p. Всё. Тут есть небольшие, так, во-первых, как бы два замечания. Первое, геометрическая
вероятность и геометрическое распределение, это вообще разные вещи, никак с собой не связаны.
Геометрическая вероятность это такое короткое название вот той вот модели, когда мы точку бросали
на под множество rn, конечные меры. Помните, да? Вот это вот как бы геометрическая вероятность,
а геометрическое распределение, это вот такое вот дискретное распределение, у которого такие
значения с такими вероятностями. Это понятно? И четвертый пункт, это Пуассоновское распределение.
Мы его напишем, разойдемся. Что такое Пуассоновское распределение? Я думаю, на семинарах вы
с ними уже пересекались? Нет? Ну, то есть у нас здесь x катор, это есть 0, 1, извините,
принадлежит. 0, 1, 2 и так далее. Что-то я, слушайте, если я какую-то дичь пишу, правьте меня, пожалуйста, ладно?
То есть значения у нас вот такие, а вероятности у нас вот такие. Лямда в степени k делить на k
факториал на e в степени минус лямда. Лямда это какая-то чиселка больше нуля. Вот это самая
правильная реакция, которая должна быть на эту строчку. То есть если вы откроете как бы вот любую
там книжку по терверу, там вот эта вот вещь, рассмотрим Пуассоновское распределение. Пуассоновское
распределение это вот такое. Можно, кстати, проверить, что это действительно распределение,
то есть сумма действительно будет 1. Давайте это просуммируем быстро. Сумма пока от нуля до плюс
бесконечности, лямда в степени k на k факториал на e в степени минус лямда. Кто в состоянии
просуммировать этот ряд? Ваши предшественники очень плохо, кстати, справлялись? Ну типа мы знаем
ответ. Да, это хорошо. А как единицу из этого получить? Кто узнал экспоненту? Ну то есть, ну почти.
Вот если вот эту хрень вынести, она же от k не зависит. Это нормирующий множитель. Если вынести за
знак суммирования, это в чистом виде экспонента, то есть e в степени лямда. А вот это e в степени
минус лямда это просто нормирующий множитель. И действительно получается 1. То есть это действительно
распределение. То есть вот значение, вот их вероятности, вероятности больше нуля, сумма равна
единице, все хорошо. Но как бы реакция какая должна быть? Откуда эта дичь взялась? Ну ладно,
хорошо, мы взяли распределение экспонента, но у нас много каких распределений есть. Причем
тут вообще нафиг экспонент? А это мы узнаем в следующей лекции. Все приходите.

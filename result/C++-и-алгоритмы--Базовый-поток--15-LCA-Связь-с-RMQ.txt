Сегодня мы говорим про новую задачу, точнее новую постановку задачи, хотя на самом деле
окажется, что она очень тесно связана с постановкой, которую мы уже знаем.
Сегодня говорим про задачу LCA или lowest common star. Самый нижний общий предок.
В чём заключается задача? Дано. Некоторое дерево, ну или давайте так напишем, корневое дерево.
Ну что такое корневое дерево? В прошлом семестре, я думаю, обсуждали, это просто некоторый граф,
у которого есть некоторая выделенная вершина, из которой исходят какие-то другие вершины,
из этих вершин в свою очередь исходят другие вершины, ну и так далее.
Значит, вот дано некоторое корневое дерево, и соответственно, что нужно?
Нужно для любой пары XY, которые являются вершинами дерева, найти самого низкого общего предка.
Ну, что это означает? Это означает, что вот, допустим, мы рассматриваем вершины вот эту X и вот эту вершину Y.
Вот. Какие у них есть общие предки? Ну, понятное дело, что вот эта вершина является общим предком,
и корень является общим предком, вообще говоря, любой пары вершин. Вот.
Так вот, задача LCA, или точнее запрос LCA от XY, просит вас найти самый нижний среди всех предков.
То есть, понятное дело, что если вы рассматриваете произвольную пару вершин XY, то как выглядит путь до них?
То есть, есть некоторый общий путь, и дальше он в какой-то момент вот разветляется.
То есть, один путь ведет в X, другой путь идет в Y.
Вот. Как раз задача заключается в том, чтобы найти вот этот самый момент, при котором у вас XY идут по абсолютно разным путям.
То есть, найти минимального или самого низкого общего предка.
Задача понятна? Вот.
Ну, вообще говоря, наверное, сразу вот отсюда можно уже сделать некоторый вывод о том, что, наверное,
эта задача будет как-то похожа на задачу RMQ. Вот. Почему?
Ну, потому что задача RMQ, что у нас было? У нас был некоторый массив.
Если мы условно так изобразим массив, вот, ну где по оси Y мы отложим, собственно, сами значения, значения этого массива,
то понятное дело, что задача поиска минимума на этом отрезке, она сводится к тому, чтобы найти самый минимальный элемент,
ну или, грубо говоря, найти некоторый условно выделенный элемент, который является меньше всех остальных.
То же самое, только задача, по сути, сводится к тому, чтобы вот на данном интервале узлов найти узел, у которого наименьшая глубина.
Ну, об этом чуть позже поговорим. Давайте пока рассмотрим непосредственно задачу LCA и как ее можно решать.
Начну. Классическим решением здесь является решение с помощью так называемого метода двоичных подъемов.
Метод двоичных подъемов. Так, в чем этот метод заключается?
Ну, смотрите, давайте снова, нам дано некоторое дерево, нам дана вершина X, нам дана какая-то вершина Y,
вот, тут есть какой-то путь, тут есть какой-то путь, и вот есть некоторый общий путь, нужно найти вот эту вершину.
Значит, в чем будет заключаться идея? Давайте для каждой вершины будем дополнительно хранить глубину, на которой она находится.
То есть первый шаг для каждой вершины, посчитаем ее глубину.
На самом деле это сделать несложно, я думаю, ну, то есть это может делать рекурсивно.
Понимаете, просто спускаемся от корня до детей, у детей делаем глубину на один больше, чем у родителей, ну и так далее.
Ну, то есть формулы пересчета, я думаю, давайте напишем D от X, это просто есть D от P от X плюс 1,
ну, где P от X это родитель X, ну и D от корня будем полагать равным нулю.
Ну, хорошо, для каждой вершины посчитали ее глубину, то есть вот у нас есть вершина X находится на глубине D от X,
есть вершина Y, которая находится на глубине D от Y.
Ну и метод двоичных подъемов на самом деле очень похож на такой, на метод двоичного поиска, то есть на бинарный поиск,
бинарные подъемы и бинарный поиск. Что говорит бинарный поиск? Давайте возьмем и прыгнем на какую-то степень двойки,
нарежем наш общий мастив пополам и попробуем посмотреть туда, попали мы куда нужно или нет.
Ну и вместо того, попали мы куда нужно или нет, мы будем там идти либо влево, либо вправо.
Вот давайте попробуем что-то подобное придумать здесь. Вот давайте предположим, пусть D от X совпадает D от Y, ну для простоты.
Вот есть вершины Y, есть вершины X, они находятся на одной глубине. Что можно придумать?
Ну вот давайте допустим мы каким-то образом, пока неизвестно каким, но умеем из какой-то вершины подниматься сразу на несколько шагов наверх, за единицу.
Вот допустим мы из X-а можем прыгнуть сразу сюда и из Y тоже можем прыгнуть сразу сюда.
И мы попали в какую-то такую вершину, то есть вершину, в которой прыжок из X и прыжок из Y совпадает.
Что можно сказать про эту вершину? Является она ответом или нет?
Ну в данном случае нет, но вообще говоря, может являться ответом.
Но вообще говоря, тот факт, что мы попали в одну и ту же вершину из X-а и из Y, не дает никакой информации о том, является ответом или нет.
То есть это может являться ответом, а может не являться ответом.
А что если я прыгну, скажем, вот куда-то сюда? Я прыгнул сюда и прыгнул сюда. Есть у меня какая-то определенность или нет?
Да, вот тут уже есть некоторая определенность, то есть я точно знаю, что тут ответа нет.
Поэтому идея будет заключаться в следующем.
Давайте будем искать самый длинный прыжок такой, что, грубо говоря, у меня вот эти родители, родители X-а и родители Y, не совпадают.
Потому что если они совпадают, то у меня нет никакой информации о том, попал я куда нужно или нет.
Дальше я постараюсь сделать как можно более длинный прыжок отсюда, по родителям, так, чтобы у меня снова эти родители не совпадали.
Ну и так далее. И вот в тот момент, когда я не смогу сделать такой прыжок, допустим, здесь, что это будет означать?
Это означает, что я нашел самых первых родителей, которые не совпадают.
Это значит, что их родители это и есть общий предок. Согласны?
Ну план такой.
Теперь как мы будем реализовывать? И вообще, как мы можем прыгать на произвольное расстояние, скажем, не за линейное время?
Ну давайте вторым пунктом посчитаем. Для любой вершины V посчитаем следующую характеристику.
Это будет родитель, который находится на расстоянии 2 степняка шагов вверх.
Нам будет достаточно хранить не всевозможные прыжки, то есть прыжок на 1, прыжок на 2, прыжок на 3, то есть это слишком дорого.
То есть у нас получится квадратичное сложение, если мы для каждой вершины хранили всевозможные прыжки.
Так вот идея заключается в том, что давайте для каждой вершины хранить всего лишь логарифмическое количество прыжков,
ровно так, как мы это делали в разреженной таблице.
Так что для каждой вершины будем знать, куда мы попадем, если мы прыгнем на 1 шаг, на 2 шага, на 4 шага и так далее.
Как посчитать эту характеристику? На самом деле тоже очень просто, по сути как вот эту рекуррентную.
Как будем считать? Ну, во-первых, понятно, что для любой вершины v, v.up от 0, чему будет равно?
Чему равен прыжок на 2 в степень 0 из любой вершины? Да, это просто родители этой вершины.
Если v.parent. То есть это такая некоторая база индукции. Для 0 мы знаем, как все посчитать.
Теперь, если мне хочется посчитать, куда я прыгну, если я буду...
То есть характеристику v.up от k. То есть куда я попаду, если я буду прыгать на 2 степень k шагов вверх.
Ну, допустим, я посчитал все это для всех k штрих меньше, чем k.
Прилагается взять v.up от k-1 и parent. Похоже на правду?
Но кажется, что нет. Что вот это означает? То, что здесь написано.
Вот мы находимся в вершине v. То есть мы хотим попасть вот куда-то сюда. То есть 2 в степень k.
v.up от k-1 мы попадаем вот сюда. Если мы берем parent, то мы падаем вот только сюда.
Что нужно сделать? Да, еще раз вызвать v.up от k-1.
То есть если у нас v.up от k-1 уже посчитано, то нам вот эта штука вернет некоторую вершину.
Из этой вершины нам нужно снова прыгнуть на 2 степень k-1 шаг наверх.
То есть точка v.up от k-1.
Ну и понятно, что это тоже можно посчитать рекурсивно, так как если вы находитесь на каком-то пути до вершины v,
если вы рекурсивно запускаетесь от каждой вершины, то в тот момент, когда вы будете вызываться от вершины v,
все остальные характеристики.up на этом пути будут уже посчитаны.
Поэтому мы корректно можем вызывать как up от k-1 для этой вершины, так как по предположению индукции мы для всех меньших k уже все посчитали.
То есть мы корректно сможем вот для этой вершины, в которой мы пойдем тоже посчитать, точнее тоже вызвать,
а под k-1 он тоже будет посчитан.
То есть это мы научились прыгать из произвольной вершины.
Давайте тут напишем, что это тоже занимает уатен, но это естественно тоже занимает линейное время.
То есть за линейное время мы научились, во-первых, считать глубину каждого элемента,
во-вторых, прыгать на степень двойки из каждой вершины.
Так, давайте сюда.
Ну и третий шаг.
Давайте уже алгоритм напишем.
LCA от x, y.
Так, ну смотрите, я писал некоторым образом алгоритм или идею алгоритма в случае, когда у меня глубина вершины x и глубина вершины y совпадает.
Но жизнь устроена немного сложнее, и вообще говоря, глубина y и глубина x могут отличаться.
Что можно сделать в этом случае? Как будем действовать?
Да, поднимем y на высоту x, отлично.
Ну вы согласны, что на какой бы глубине у меня находился y здесь,
я всегда могу залогрифмическое количество шагов поднять до вот той же самой глубины?
Согласны?
Поэтому давайте напишем следующую вещь.
Давайте напишем так.
Давайте так, если d от x больше, чем d от y,
то сделаем swap x, y, чтобы в y всегда находилась наиболее глубокая вершина.
Ну а далее, давайте я так напишу,
up y равно up от y на d от y минус dx.
Это поднять y на dy минус dx шагов вверх.
Все понимают, что эта строчка может быть выполнена залогрифмическое время, или расписать.
Верите ли вы, что если вы знаете, на какое количество шагов вам нужно прыгнуть,
имея прыжки два степенника, имея прыжки размера степени двойки,
вы всегда залогрифмическое количество шагов сможете тогда прыгнуть.
Ну просто как бы раскладываете вот эти числа по степеням двойки и вот прыгаете на эти шаги.
Таким образом, вот здесь мы уже гарантировали, вот на этом уровне,
мы гарантировали, что d от y обязательно совпадает с d от x.
Ну отлично, осталось теперь реализовать вон ту идею, про которую мы говорили, что мы делали.
Ну во-первых, если x уже совпадает с y, то в этом случае мы уже можем вернуть x или y.
То есть если вершины совпадают, то в принципе самый низкий общий предок это есть одна из этих вершин.
Вот, а далее сделаем следующую вещь.
Давайте сделаем так, for k целая часть логарифма n до нуля.
Если x.up от k не равно y.up от k, то я перехожу в эти самые вершины.
Вот. Что здесь происходит?
Ну по сути здесь происходит перебор, давайте здесь нарисую.
По сути здесь как раз происходит перебор прыжков, на которые я могу прыгнуть.
Ну то есть снова есть некоторый путь. Есть x, есть y.
И я пытаюсь прыгнуть, начиная с наиболее возможного прыжка.
Понятно, что если у вас в дереве всего n элементов, то самый длинный прыжок, который вы можете сделать, это вот k равное логарифму двоичного n.
То есть мы пытаемся сделать самый длинный прыжок.
Ну и дальше уменьшаем этот самый прыжок до тех пор, пока мы не встретим различающиеся вершины.
Ну вот ровно здесь это и написано. То есть мы берем самый длинный прыжок.
Если то, куда мы попали из x и из y не совпадает, то это значит, что этот уровень нас устраивает, и мы идем дальше.
Возникает вопрос, что у меня тут цикл вроде как один.
То есть допустим я в какой-то момент для какого-то k' нашел хороший прыжок для x и хороший прыжок для y.
Что произойдет у меня на следующей террации? На следующей террации я буду рассматривать k равное k'-1.
Вопрос, а не нужно ли мне заново начинать этот цикл?
Ну в принципе теоретически я, наверное, могу взять какой-то вот такой прыжок, у которого k больше, чем k', и попасть в какую-то хорошую точку.
Или нет?
Да, смотрите. То есть смотрите, если у меня какой-то k', вот тут устраивает, то значит на следующей террации k' и все большие элементы k' меня устраивать не будут.
Поэтому следующий как, который мне нужно рассматривать, это k'-1. Почему это так?
Ну, смотрите.
Вот смотрите, что означает, что меня k' в прошлый раз устроил?
Это значит, что когда я прыгал на k', это был первый прыжок, первый по величине прыжок, ну то есть наибольший прыжок, на который я могу прыгнуть,
так что я не выхожу за пределы вот этого неравенства.
То есть это наибольший прыжок, который меня не выводит вот на этот путь.
Ну это значит, что какой-то прыжок больше длины k'-1 попадал куда-то сюда.
Зафиксировали. Теперь я попал сюда.
Почему на следующей террации мне не имеет смысла рассматривать прыжок на k', на k' и больше?
Ну потому что если я отсюда прыгну на k', то я тоже попаду вот сюда.
А эту точку я уже рассматривал.
Поэтому следующий как, который я должен рассматривать отсюда, это, как минимум, k'-1.
Понятно?
Поэтому я обхожусь одним циклом.
То есть я одним циклом просто спускаю k от логарифма двоечного n до 0
и обновляю x и y при необходимости.
Ну и что у меня получится в конце?
Что у меня получится в конце, когда я закончил этот цикл?
Что мне нужно вернуть?
Ну вот, я что делал?
Я пропрыгал всевозможными прыжками
и дошел до какой-то точки.
Слева и справа.
Вот, при этом последний прыжок у меня был наименее возможный,
так, чтобы у меня все вершины не совпадали.
То есть, что я рассматривал в качестве последнего шага?
В качестве последнего шага у меня был k'0.
То есть, точнее так, у меня были последние шаги k'1, k'2 и так далее.
Что это означает?
Это означает, что я наиболее близким образом подошел к наименьшему общему предку.
То есть, если бы я мог еще ближе подойти,
то я бы смог прыгнуть на какое-то большее количество шагов и так далее.
Но так как я все k рассмотрел,
то это означает, что я вот это расстояние,
по сути, вот это расстояние d,
я как раз разбил по степеням двойки.
Но так как любое число я могу покрыть степенями двойки,
то это означает, что я как раз дошел до нужного места.
Тогда что мне нужно вернуть в качестве ответа?
Ну, x parent.
Да?
Ну, или x parent, или y parent.
Ну почему? Потому что этот цикл мне всегда приводит в вершину,
в которую у меня x не совпадает с y.
Ну и плюс вот таким вот двоичным подъемом я дошел до точки,
точнее, до наиболее высокой точки,
ну или точнее так, наиболее низкой точки,
в которую у меня родитель x и родитель y расходятся.
Но раз это наиболее низкая точка,
в которой x и y расходятся,
это значит, что точка выше
обязательно склеивает x и y вместе.
Согласны?
Разумно?
Ну вот.
В этом заключается алгоритм двоичного подъема.
Ну и понятно, что
сам вот этот цикл
работает за алгоритмическое время,
плюс из вершины y
на произвольное количество шагов
мы поднимаемся тоже
за алгоритмическое время,
поэтому
в общее время запроса
есть алгоритм n.
Вот.
Да.
Ну да, на самом деле достаточно алгоритма,
но давайте предполагать, что если такого прыжка нет,
то там
ну там либо 0 птр,
либо просто корень и так далее.
Ну да, ну в принципе понятно, что
вот тут можно начинать
с алгоритма d,
где d это вот глубина x.
Ну это неважно, на симпатику не влияет.
Вот.
Еще вопросы?
А как это может быть?
Давайте рассмотрим, ну смотрите.
Давайте
ну вот общий предок,
и вот тут какой-то путь до x,
и вот тут какой-то путь
до y.
Ну вот пусть
длина от x до
вот этого элемента,
не знаю какая,
ну d пусть будет.
И тут тоже d.
Ну то есть мы это гарантировали
вот этой строчкой.
Что по сути делает этот цикл?
Ну этот цикл по сути раскладывает
d по степням двойки.
То есть мы сначала ищем
наибольшую степень двойки,
которая меньше, чем d.
То есть если d имеет какое-то там
битовое представление,
то мы сначала найдем вот эту единицу.
Потом мы спускаемся по k,
находим вот эту единицу.
То есть находим минимальную степень двойки,
которая вот нас приводит в d.
Ну и так далее.
Ну так как у нас есть 2 степени k,
давайте ка 0,
плюс 2 степень k1 и так далее,
вот все эти ка мы находим.
Но это означает, что в конце мы в любом случае придем вот сюда.
Ну собственно цикл так устроен.
Поэтому для любого d
мы попадем туда, куда надо.
Еще.
Ну окей.
Так.
Ну в общем-то,
что касается классического
решения задачи LCA,
мне сказать вам больше нечего.
В принципе есть и другие алгоритмы,
но я думаю этого достаточно.
То есть он линейный,
он делает запрос в логографическое время и так далее.
Возможно в следующем симметре,
когда мы будем говорить про графы,
ну с некоторыми из вас,
мы поговорим про какие-то другие алгоритмы,
использовать другие идеи,
о которых мы как раз в следующем симметре поговорим.
А сейчас давайте перейдем
к вот какому вопросу.
Поговорим о тесной связи задачи LCA
и задачи RMQ.
Значит пункт
связь
RMQ
и LCA.
Да, я кстати не сделал анонс.
Вообще говоря, по идее,
если мы успеем,
хочется прийти к тому,
что я обещал на первой лекции посещенное RMQ,
а именно решение сдачи RMQ
статической за линейное время
и за единичный запрос.
То есть если вы помните, когда мы рассматривали разреженную таблицу,
там, ну во-первых, классическая разреженная таблица,
она работает за n log n и
с запросом за единицу,
мы смогли ее улучшить там до, давайте,
у нас было n log n,
и вот единица потом,
нам удалось ее улучшить
за O от n,
и вот логарифма n,
и в конце
у нас был
log log n,
и вот единица.
Вот.
Ну вот сегодня
я стремлюсь к тому, чтобы
рассказать некоторый алгоритм, который работает
вот так, то есть
наиболее эффективным образом.
Понятное дело, что быстрее нельзя.
Если у вас есть массив размера n, то быстрее, чем за n,
вы его не сможете обработать, ну и плюс запросы быстрее, чем за константа
и права это нельзя.
Удивительным образом оказывается, что это можно сделать,
если усмотреть некоторую связь
с задачей RMQ и задачей LCA.
Вот, давайте о ней поговорим.
Так.
Давайте для начала,
не знаю, давайте, например, для начала
покажем, что
задачу RMQ
можно свести к задачей LCA.
Ну то есть, допустим, вы пропустили
все прошлые лекции,
вот, и только сейчас узнали про LCA.
Вот.
И а у вас в контесте задача только на RMQ.
Так вот, любую задачу RMQ
вы можете решить с помощью алгоритма,
который решает LCA. Вот.
А как это можно сделать?
Ну, смотрите, что мне дано в задаче RMQ?
Ну, в задаче RMQ мне дано некоторый массив,
там, не знаю, а0,
1,
а2 и так далее, аi-1.
Вот.
Мне нужно отвечать на запрос,
вида, найти минимум
на отрезке от L до R.
Вот.
Как умею решать LCA?
Решить эту задачу.
Так вот,
внезапно нам потребуется
декартово дерево.
Значит, давайте возьмем
и
построим
декартово дерево
по неявному ключу.
Ну, или,
если вас не устраивает неявный ключ,
то вам можно просто взять
или с ключами
индексами массива.
А
с приоритетами
yi
равными ai.
Ну, то есть, возьмем и построим декартово дерево
на следующих парах.
Ну, то есть, на парах 0,
а0, на парах 1,
а1, ну и так далее.
n-1,
an-1.
Ну, так как ключи у нас индексы,
то в принципе индексы можем опустить.
В прошлый раз обсуждали, что эта идея называется
декартово дерево по неявному ключу.
Давайте построим такое декартово дерево.
Что у нас получится?
Получится какое-то
дерево.
Вот.
То есть, спроецируем
на ось
на ось x.
То есть, это нулевой элемент.
Это первый, второй, третий,
четвертый, пятый, шестой,
седьмой.
Ну, а здесь написаны, собственно, значения нашего массива.
Значения вот этого массива.
Ну, и эти значения массива
мы полагаем,
точнее, эти значения массива
мы используем в качестве приоритетов.
В качестве значений, а именно в качестве приоритетов.
Например, один, два,
три, четыре,
пять, шесть, семь,
восемь. Это приоритеты.
Вот.
И на самом деле уже
видна некоторая структура.
Что мы знаем про декартово дерево?
Что у нас находится в корне любого поддерева?
Нам дано некоторое поддерево.
Например, вот это.
Что мы знаем про его корень?
Нет, не в ту степь.
Это произвольное декартово дерево без RMQ, RSQ.
То есть, это декартово дерево, которое хранит...
Ну, что такое декартово дерево? Давайте.
Это структура данных,
которая хранит ключи.
В данном случае у нас декартово дерево по дневному ключу
и хранит приоритеты.
По приоритетам оно является чем?
Бинарной пирамидой
с минимумом в корне.
В корне любого поддерева хранится минимум.
Все, то есть декартово дерево уже, по сути,
обладает некоторой структурой,
которая хороша для нашей задачи RMQ.
То есть с задачи RMQ мы хотим находить минимумы.
Соответственно, декартово дерево в корнях поддеревьев
уже хранит минимальные элементы.
Давайте разбираться как...
Точнее, все, тут разбираться нечего.
Мы, по сути, решили задачу.
Утверждение заключается в следующем.
Вот если мы так сделали,
то утверждение такое...
RMQ LR
это то же самое, что LCA LR.
То есть если я хочу найти минимум
на отрезке от 0 до...
от 1 до 3,
то что мне нужно сделать?
Мне нужно взять вершину
с индексом 1,
вершину с индексом 3
и найти их LCA.
Понятно?
То есть запрос RMQ просто-напросто
сводится к поиску LCA
в декартовом дереве.
Ну, давайте
докажем, почему так.
Ну, во-первых,
что можно сказать?
Давайте обозначим LCA LR
какой-то буквой,
не знаю, L, L маленькой.
Вот пусть L маленькая,
это результат запроса LR.
Это L большого.
Что можно сказать про L?
Так как L
корень
под дерево,
так как L корень под дерево,
то понятное дело,
что L меньше равно, чем L маленькая,
и это меньше равно, чем R большое.
Ну, что это означает?
Это означает, что, смотрите,
буквально следующая вещь.
Вот у вас есть LCA,
у вас есть L, у вас есть R.
При этом L большое находится влево
и R большое находится в правом по дереве.
Ну, а так у вас декартовое дерево
является бинарным деревом поиска,
то это означает, что L как раз таки зажата
между этими двумя индексами.
То есть этот элемент лежит на этом отрезке.
В чем заключается корректность задачи RMQ?
RMQ должен вам найти элемент,
который находится именно в этом отрезке,
и он должен быть минимальным
среди всех элементов этого отрезка.
Первый пункт мы уже гарантировали.
То есть если мы нашли LCA,
то этот узел обязательно находится
в этом отрезке. Согласны?
Ну так, это корень под дерево,
в котором находятся элементы LR,
и при этом LR находятся,
вообще говоря, в разных под деревьях.
Так, ну и теперь надо показать,
что в L действительно хранится минимум
на отрезке вот LDR.
Ну, как выглядит L?
Значит, вот есть где-то L большое,
есть где-то R большое.
Вот.
И выглядит как-то все так.
Значит,
так как
L корень
по L точка Y
минимален
во всем под деревя L.
А под дерева L
содержит
отрезок
LDR.
То есть понятно, что если у вас есть
некоторое под дерево,
и под дерево при этом содержит ключи L и R,
то, понятное дело, что все промежуточные ключи
тоже содержат под дерева L,
а под дерева L содержат
под дерева L и R.
То, понятное дело, что все промежуточные ключи
тоже лежат здесь.
То есть не может быть такое, что элемент,
который находится между L и R, находится где-то вот здесь
или где-то вот здесь, ну где-то за пределами этого под дерева.
Ну, это просто свой стабинарный дерево поиска.
Поэтому L содержит полностью
отрезок LR, ну и плюс, возможно, еще что-то.
И при этом L, вот на этом всем
большом отрезке, является минимум.
Ну, соответственно, L является минимум и на этом отрезке.
Согласны?
Ну, все.
Из этого следует.
L
является
минимумом
и на
L и R.
Ну, все.
То есть, если мы рассмотрим LCA
для L и R,
то про этот LCA мы знаем,
что он лежит в пределах L до R,
так это корень этого под дерево,
то есть вот этот элемент,
ну, это элемент массива, лежит между L и R.
Это, во-вторых.
А, во-вторых, мы точно знаем, что это элемент минимальный,
минимальный на этом отрезке. Почему?
Потому что он минимальный во всем под дереве, а L и R
целиком лежат в этом под дереве.
L минимален и на этом отрезке.
Вот так. Есть вопросы? Давайте еще раз поговорим. Что мы сделали? Мы свели задачу RMQ с задачей LCA.
Как это делается? Во-первых, мы время еще не обсудили, давайте обсудим тоже параллельно.
Что мы делаем? Во-первых, мы строим декартовое дерево по неявному ключу, ну или используя в качестве ключей индексы массива.
Строить декартовое дерево мы умеем за линейное время, то есть в случае, когда у нас ключи отсортированы.
Поэтому первый шаг занимает OT. На самом деле все. Весь припроцессник сводится к тому, что мы на этом массиве строим некоторое декартовое дерево.
Дальше запрос. Ну а запрос показан здесь. За какое время он работает?
За какое время работает этот запрос? Ну LCA, с самой задачей LCA мы умеем решать как минимум за долгорифмическое время.
Ну поэтому запрос RMQ тоже будет работать за долгорифмическое время.
То есть как выглядит запрос RMQ на массиве? Ну то есть если вам нужно найти минимум на массиве от LDR,
то вы ищете узлы соответствующими индексами, ну допустим вот L и R.
Дальше запускаете поиск LCA, ну например с помощью двоичных подъемов.
Вот, находите эту вершину и смотрите, ну не знаю, либо приоритет этой вершины, если вам нужно само значение вернуть,
либо индекс этой вершины, то есть какое увеличение вершины она является.
Вот, соответственно, ну тогда находите индекс. Вот.
Значит, вот так выглядит сведение задачи RMQ с задачей LCA.
Следующий пункт, это сведение задачи LCA к задачи RMQ.
Значит, допустим, вы пропустили пять минут первой лекции, а потом, в общем, наоборот, не пропустили первые пять минут, а потом все пропустили.
Значит, как решить задачи LCA, если вы умеете решать задачу RMQ? Все тоже довольно просто.
Смотрите, значит, в чем заключается постановка задачи LCA?
Значит, у вас есть какое-то дерево. Вот какое-то дерево.
И вам нужно для любой пары вершин уметь определять самого низкого общего предка.
Как мы поступим? Поступим мы следующим образом.
Во-первых, давайте снова, как и раньше, для каждой вершины посчитаем ее глубину.
Давайте в качестве первого пункта посчитаем глубину вершин за линейное время.
А дальше мы выполним... Вы же проходили всякий in-order, pre-order, post-order обходы графа?
А вот тут мы выполним все order обход графа, то есть и post-order, и pre-order, и in-order.
Давайте напомним краткое напоминание, что если у вас есть pre-order обход,
то это означает, что вы условно делаете принт вершины, а потом рекурсивно обходите левую часть и pre-order.
Обходите правую часть.
Post-order это то же самое, но только принт вы делаете в конце.
Сначала запускаете от левого сына, от правого сына, и потом вершину.
Ну и in-order. Сначала обходите левого сына, потом печатаете саму вершину, а потом заходите в правого сына.
Ну просто проходим в порядке. Есть какой-то естественный порядок на вершинах.
Если у вас больше двух детей, то скорее всего вы их храните в отдельных полях или в массиве.
Каким-то образом проходите.
Ну короче говоря, в pre-order что происходит? Вы сначала обходите саму вершину, а потом в каком-то порядке обходите детей.
В post-order вы сначала обходите детей, а потом обходите саму вершину.
Это просто напоминание. Мы будем делать другую вещь.
Мы будем выполнять сразу все обходы. В том смысле, что после каждого посещения ребенка мы будем посещать себя еще раз.
Вообще этот обход называется либо Эйлеров обход, либо обход DFS.
Давайте его называть Эйлеровым.
Я напишу в случае бинарного дерева. В случае, когда у вас больше детей, вы просто вставите соответствующий обход между каждым вызовом от детей.
То есть мы делаем print node, коротко напишу Эйлер от node left,
потом print node, потом снова заходим уже в правого сына, ну и потом возвращаемся в себя же.
Ну давайте схематично изобразим, как выглядит этот обход.
И этот обход выглядит следующим образом.
Ну сначала мы идем и посещаем вершину 0, рекурсивно запускаемся от единицы, ну точнее от левого сына.
Печатаем единицу, рекурсивно запускаемся от левой части.
Так, только тут надо, давайте пронумеруем вершины, это нам нужно, нам уже нужны номера вершин.
Давайте так, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9.
Вот еще раз, как выглядит обход?
Ну смотрим, сначала стартуем с корня, пишем 0.
Дальше спускаемся в левого сына, печатаем единицу, спускаемся еще раз в левого сына, печатаем четверку.
Все, рекурсивный вызов закончился, выходим.
Запускаемся от единицы, а точнее вот вернулись единицу снова.
После того как мы закончили рекурсивный вызов, мы снова печатаем ту же самую вершину.
То есть снова печатаем единицу.
Идем вправо, печатаем двойку, то есть обход выглядит как-то вот так.
То есть спустились двойку, грубо говоря мы будем дерево обходить вот так по контуру.
То есть Эллеров...
Что?
Да, пятерку надо печатать.
То есть Эллеров обход, он грубо говоря обходит дерево вот так вот по контуру.
То есть вы сначала спускаетесь по левым детям, потом идете вправо, возвращаетесь снова в родителя.
Давайте допишем.
Давайте здесь будем писать.
0, 1, 4, 1, 5.
0, 1, 4, 1, 5.
Дальше снова 1, снова 0.
Что-то я слишком большое дерево нарисовал, давайте...
Чуть уменьшим.
Ну нет, если нет детей, то мы как бы заходим и сразу выходим.
Ну там дополнительно еще должны быть вставлены проверки, что если ребенок есть, то...
Ну я это пропустил просто, понятно, что...
Понятно, что если у вас нет левого ребенка или правого ребенка, то обходить рекурсивно в него не нужно.
Поэтому только один раз печатаем.
0, 1, 4, 1, 5.
Дальше снова 1, дальше 0.
Дальше заходим в центральных детей, то есть 2, 6.
Ну и дальше поднимаемся обратно, 2, 0.
То есть грубо говоря просто обходим там 0, 1, 4, 1, 5, 1, 0, 2, 6, 2, 0.
И дальше 3, 7, 3, 0.
Вот.
Вот такой Эйлеров обход.
То есть второй пункт.
Запускаем Эйлеров обход.
Печатаем вершины, и самое важное сейчас, печатаем...
Вершины и их глубины.
То есть дополнительно помимо того, что мы напечатали каждую вершину, мы еще рядом с ней напечатали ее глубину.
То есть для нуля это 0, для единицы это 1.
Дальше для двойки-тройки тоже 1.
Вот. Ну для всех остальных 4, 5, 6, 7 это 2.
Вот.
Ну, понятно, что Эйлеров обход работает за сколько?
Он тоже работает за линейное время.
Хотя, казалось бы, он там печатает слишком много всего.
То есть мы каждую вершину печатаем по несколько раз.
Но при этом, заметьте, что каждую вершину мы напечатаем ну сколько?
Ну, не более чем ее степень.
Когда мы печатаем вершину, когда мы только в нее приходим, и когда мы возвращаемся в нее из детей.
То есть вот мы в нее вошли, дальше вернулись единицы, вернулись из двойки, вернулись из тройки.
А что будет, если мы просуммируем все степени всех вершин?
У нас будет просто общее количество ребер, умноженное на 2.
А общее количество ребер у нас не более чем 2 от количества всех вершин.
Давайте напишем.
Сделали этот шаг.
Ну и теперь запрос.
То есть, по сути, вот мы построили такой массив.
Значит, утверждение звучит в том, что чтобы найти LCA для любых двух вершин,
например, для четверки и тройки, мне достаточно найти какую-то четверку вот здесь,
ну, допустим, вот эту, и какую-то тройку здесь, вот эту.
А дальше вот на этом отрезке найти RMQ.
Что? По глубинам, да, естественно.
В данном случае, например, это ноль.
Давайте докажем, что это действительно так.
А LCA от XY, это есть RMQ от IDXX и, давайте просто, и от X, и от Y.
Ну, это некоторые позиции XY в построенном, ну, давайте RMQD.
Ну, в том смысле, что мы берем минимум по глубинам.
То есть, ну, в качестве назначения массива используем вот эти значения.
Так.
Ну, почему это так?
Ну, во-первых, давайте поймем следующую вещь.
Вот я взял, в данном конкретном примере я взял четверку и тройку, да?
Я взял какую-то четверку и какую-то тройку.
Вот если я рассмотрю последовательность 4, 1, 5, 1, 0, 2, 6, 2, 0, 3 и так далее,
вот что это будет?
Вот просто там без относительной задачи, вот просто смотрим на это дерево,
и я вам говорю, 4, 1, 5, 1, 0, 2, 6, вот.
Что это будет с точки зрения дерева?
Ну, не совсем под дерево.
Это просто будет некоторый путь из четверки в тройку.
Согласны?
Вот давайте это напишем.
Что отрезок и от х, и от у задает путь из х в у.
А что можно сказать про любой путь из х в у, вот вообще про любой?
Вот я беру произвольные вершины в дереве, и вот я иду из х в у.
Да. Согласны ли вы, что он обязательно должен пройти по наименьшему общему предку?
Ну, потому что наименьший общий предок это по сути та вершина, которая связывает х с у.
Ну или можно воспользоваться утверждением, что в дереве между любыми двумя вершинами есть только один простой путь.
Вот. Но этот простой путь, естественно, проходит через общего предка.
Вот. Ну, а любой путь в дереве проходит через LCA xy.
Ну, любой путь, давайте путь, из х в у.
Любой путь из xy в дереве проходит через LCA xy. Всё. То есть что мы знаем?
Мы знаем, что вот на этом отрезке, на котором мы ищем RMQ, у нас гарантированно есть LCA.
Что осталось показать?
Осталось показать, что у него будет наименьшая глубина среди всех вершин, которые здесь перечислены.
То есть мы сказали, что тут есть какой-то LCA, но при этом пока непонятно, почему у этой вершины обязательно будет минимальная глубина.
Да, вдруг мы поднимались куда-то выше. На самом деле выше мы не поднимемся.
Почему?
Почему мы не можем подняться выше LCA?
Есть догадки?
Ну, это да. А поподробней?
Ну, в целом да. То есть можно проще.
Можно сказать, что если наш рекурсивный вызов от какой-либо вершины закончился, то мы в эту вершину больше никогда не вернемся.
Согласны?
Давайте напишем следующую вещь.
Во-первых, наблюдение. Вот то, что я сказал.
Если вызов...
Ну короче, давайте так. Не так.
Эллеров обход не вызывается два раза от одной и той же вершины.
Эллеров обход два раза не вызывает одной и той же вершины.
Ну, откуда это следует? Ну, это следует вот из самого алгоритма.
Если я запускаюсь от вершины node, то есть я печатаю вершину, запускаюсь от левого сына, и вот от левого сына я больше никогда не запущусь.
Все, если этот вызов у меня закончен, то я в эту потерю больше никогда не вернусь.
Понятно?
Если я запускаюсь от левого сына, то я всего левого сына обошел и возвращаюсь обратно.
Все, от него я больше никогда вызываться не буду.
И вот после того, как вот этот вот рекурсивный вызов у меня закончится, от вершины node я тоже никогда не буду вызываться.
Ну, ровно по этой причине. Понятно?
То есть вот эта сама функция, то есть не функция print.
То есть print это просто печатать, это не обход.
Обход это вот эта функция, error.
А вот функция error никогда два раза от одной и той же вершины не вызывается.
То есть error обход не вызывается два раза от одной и той же вершины.
Ну а что это значит?
Допустим, на этом пути, который мы рассматриваем, ну я напомню, мы взяли какую-то вершину x,
в массиве взяли какую-то вершину y в массиве, и рассмотрели путь от x в y.
Вот допустим, на этом пути есть вершина выше LCA.
То есть что это значит?
Вот у меня здесь LCA от x и y, и вот есть вершина выше нее.
Здесь где-то x, здесь где-то y.
Что это означает?
Как я мог попасть в эту вершину?
Это означает, что я от x как-то попал в эту вершину, да?
А потом спустился в y.
Давайте, давайте здесь.
То есть error обход поднялся в родителя LCA.
Еще раз повторю.
Я стартовал свой путь из x.
Дальше, где-то посередине, этот путь забрел в родителей LCA.
Ну что это значит?
Значит, что я зашел в LCA, а потом из LCA поднялся в родителя.
А что это означает?
То есть в каком случае я поднимаюсь от потомка к родителю?
Вот что означает вот этот подъем в терминах error обхода?
Да, это означает, что у меня завершился рекурсивный вызов.
То есть по пути поднимаюсь я вверх только в случае, когда у меня завершается рекурсивный вызов.
То есть рекурсивный вызов error от LCA был завершен.
Вот.
То есть еще раз останавливаемся, понимаем, что происходит.
Мы были в вершине x.
Из вершины x мы каким-то образом добрались до родителей LCA.
Так мы добрались до родителей LCA, это означает, что у нас рекурсивный вызов от этой вершины был завершен.
То есть все, мы это дерево больше не обходим.
Все видят противоречие.
Но противоречие заключается в том, что после того, как я из x пошел в родителя,
у меня оказывается, что я из этого родителя потом как-то еще попал в y.
Ну а как я мог попасть в y, если я не могу снова возвращаться по этому самому ребру?
Понятно?
То есть у меня error в путь два раза в одну и ту же вершину не заходит.
Ну все, противоречие.
То есть рекурсивный вызов error от LCA x, y был завершен.
А потом
вновь
был
запущен.
Ну давайте напишем, что так как
так как после
был
напечатан y.
Все, ну противоречие.
Противоречие с наблюдением, да?
Наблюдение.
Все.
Давайте еще раз проговорим доказать.
На самом деле простой, тут много слов получилось, но идея простая.
Значит мы находим...
То есть у нас есть некоторый массив.
Массив error обхода.
А вы берем какое-то вхождение x туда, какое-то вхождение y.
Вот эта последовательность
последовательность вершин, которые мы посетили, задает некоторый путь.
Некоторый путь изy.
Значит первый пункт очевидный, этот путь обязательно проходит через LCA.
Теперь второй пункт нужно доказать, почему этот путь
никогда не проходит
по вершинам, которые находятся выше LCA.
Ну очень просто.
Значит допустим он проходит через вершину, который находится выше ЛCA.
Это значит что мы из x поднялись вот сюда.
Но если мы поднялись вот сюда, то есть если мы в какую-то вершину поднялись обратно, то по этому же пути, то есть вот по этому ребру мы никогда не пройдем.
Ну вот так устроен Euler's обход.
Ну то есть если вот этот вызов завершился, то все, сюда мы больше никогда не попадем.
Ну все, а это значит, что отсюда мы никак в игры попасть снова не могли.
Да? Но это противоречие с тем, что у нас путь устроен так.
Х поднимается сюда, а потом каким-то образом падает сюда.
Вот, противоречие.
Все, следовательно, LCA – это вершина с минимальной глубиной, которую мы посещали.
Ну все, это значит, что мы можем свести задачу LCA к поиску просто RMQ вот на этом массиве.
Точнее, на этом подотреске.
Окей?
Отлично.
Ну, по сути, вот.
То есть по сути мы доказали что? Мы доказали, что задачи LCA и RMQ по сути эквалентны.
Да, то есть если вы умеете решать одну задачу, то вы обязательно умеете решать и другую задачу.
Вот.
Теперь давайте начнем наш путь.
Все, последний пункт.
Начнем наш путь к тому, что попробуем решить задачу RMQ за линейное время.
И тут нам внезапно потребуется сведение к LCA.
То есть небольшой спойлер.
Как мы будем решать задачу RMQ за линейное время?
Мы возьмем задачу RMQ, сведем ее к LCA,
потом LCA сведем к RMQ, и вот его мы решим за линейное время.
Сложно.
Давайте...
Давайте поясню.
Значит, на самом деле...
Я утверждаю, что вот это сведение LCA к RMQ на самом деле сведение не к RMQ.
На самом деле мы свели задачу к, наверное, чуть более простой.
Что можете сказать про этот массив?
Вот я утверждаю, что вот этот массив устроен несколько особенным образом.
Во.
Все понимают, что соседние элементы, обязательно, элементы, которые находятся в этом массиве,
они либо спускаются вниз, либо поднимаются наверх.
И вот такая познавка задач называется плюс-минус один RMQ.
Давайте замечание.
Задача, то есть давайте здесь напишем RMQ плюс-минус один.
Задача...
Задача...
Задача,
которой
свелась
LCA
называется
RMQ плюс-минус один.
Так как
соседние элементы
отличаются на плюс-минус один.
Вот.
Так, и вот
задачу RMQ плюс-минус один
можно решить эффективно
за линейное время.
Вот давайте этим
займемся.
Еще один пункт
RMQ
плюс-минус один.
Значит, алгоритм, с помощью которого мы будем решать эту задачу,
вот эту задачу мы сейчас решим за линейное время,
ну, в смысле, за построение за линейное время и
ответный запрос за единицу.
Вот.
Значит, алгоритм, который мы рассмотрим,
называется алгоритмом
Farah, Colton и Bender.
Вот.
На всякий случай.
Значит, как работает этот алгоритм?
Алгоритм работает
ну, очень похожим образом, как мы обсуждали на первой лекции.
Значит, на первой лекции мы обсуждали SQRT декомпозицию.
Помните?
Что мы делали?
Вот у нас был массив.
Далее мы его разбивали на
кусочки размера B.
B.
Тут B.
Ну, этот кусочек, возможно,
тоже B.
Этот кусочек, возможно, меньше равен, чем B.
Что мы делали?
Ну, мы просто брали и считали минимум
в каждом из этих кусочков.
То есть размер этого массива N.
Размер этого массива
N делённый на B.
Ну, давайте отдельно как-то
ещё раз поясним.
Разбиваем
на подмассивы
размера B.
На каждом
подмассиве
разбиваем
на подмассивы
ищем минимум.
Ну и третий пункт.
Вот давайте вот на этом массиве, как и раньше,
вот на этом массиве
построим разреженную таблицу.
Строим
на полученном
массиве
sports table.
Так.
Начну и
сколько времени это занимает?
Давайте T при процессинга
от N и B.
Значит, как и раньше, это нас занимало,
понятное дело, там, линейное время
на то, чтобы посчитать всевозможные минимумы
на каждом из этих подотресков.
Ну и плюс мы строим разреженную таблицу
на массиве размера N делённый на B,
но разреженную таблицу мы строим
за вот такое время.
Так.
И, ну, весь основной вопрос у нас заключался в том,
что мы делаем с этими небольшими кусочками.
Да?
То есть в классе, в простом варианте
мы просто считаем там банальным образом,
то есть примитивным образом,
то есть просто если нам нужно,
точнее так, давайте напомним,
что если вам нужно найти минимум на каком-то отрезке,
то вы этот запрос вводите к поиску минимума
на вот этих подотресках,
то есть, по сути, вот на вот этих элементах,
на маленьких элементах.
На вот этих вот остатках,
на этих остатках вы считаете минимум
каким-то другим образом,
ну, например, там, ну, просто линейно проходитесь,
ну, либо на вот этих маленьких кусочках
в свою очередь строите свою Spark таблицу.
Вот это то, что было раньше.
Значит, Farah, Colton и Bender
предлагают другую вещь.
Давайте, наконец, воспользуемся тем фактом,
что у нас задача RMQ плюс-минус один.
Значит, ключевой вопрос следующий.
Что делать с массивами размера B?
То есть, что делать с вот этими друзьями?
Значит, тут давайте воспользуемся тем фактом,
что у нас задача плюс-минус один.
То есть, все соседние элементы отличаются
от плюс-минус один или минус один.
Пример.
Чем отличаются эти массивы?
Ну, то есть, согласны, что с точки зрения,
что вообще говоря, с точки зрения задачи RMQ плюс-минус один,
эти массивы ничем не отличаются.
Ну, то есть, и этот, и этот массив
можно закодировать следующим образом.
Плюс один, минус один, минус один, минус один,
минус один, плюс один.
Ну, то есть и этот, и этот массив можно закодировать следующим образом.
Плюс один, минус один, минус один, минус один, минус один, плюс один.
Согласны?
Что и этот массив, и этот массив имеют вот такой вид.
То есть нам не важно, чтобы найти минимум на произвольном отрезке,
нам вообще говоря не важен начальный элемент.
То есть нам важно то, как ведут себя эти элементы.
Какой у них график условный.
И это нас наталкивает на следующую мысль.
Что в принципе, количество таких различных последовательностей,
вот такого вида, наверное, не очень много.
Согласны?
То есть разнообразие массивов у вас не очень большое.
То есть в случае, когда вы решаете обычную задачу RMQ,
то соседние элементы могут отличаться на плюс пять, на плюс четыре и так далее.
А если у вас массив отличается на плюс-минус один,
то вы любой массив можете закодировать вот таким образом.
И ответ не будет зависеть от конкретного вида.
То есть ответ будет зависеть лишь от такой маски.
Плюс-минус один. Согласны?
Есть контакт?
Давайте напишем.
Любой массив
можно закодировать
последовательностью
из b-1
последовательностью плюс-минус один.
Как это математически пишется?
В степени b-1. Похоже на правду?
Последовательностью из b-1 элемента
принадлежащего множеству минус один и плюс один.
Согласны?
И вы, наверное, сейчас ждете чего-то умного.
Есть массивы плюс-минус один,
и мы сейчас как-то очень круто на них сможем построить запросы.
Ну вот фиг вам.
Мы сейчас возьмем, переберем все такие последовательности,
переберем всевозможные пары LR
и посчитаем на них ответы.
Тупым перебором.
Переберем.
Прямо подчеркну, втупую.
Все последовательности,
все эти последовательности
и все пары LR.
Все эти последовательности, все пары LR.
И предподсчитаем ответы.
Все, то есть условно, если у вас b равно, не знаю там,
b равно 5,
то что мы делаем?
Мы перебираем 2 в степени 4,
16 возможных массивов,
и для каждого массива,
для каждого из этих 16 массивов
перебираем всевозможные пары.
То есть как мы на самом первом лекции говорили,
самый тупой алгоритм.
Вот мы делаем самый тупой алгоритм.
Но только мы сохраняем все результаты в отдельную таблицу.
Окей?
Сколько памяти на это уйдет?
Ну и сколько времени?
Ну вообще говоря, это занимает сколько?
Ну сколько всего таких последовательностей размера b-1?
Ну почему b-1, кстати?
Все понимают.
У нас же последовательство размера b.
Ну потому что у нас первый элемент,
он ни на что не влияет.
То есть нас интересует только вот этот хост,
плюс-минус один.
Сколько всего таких последовательностей размера b-1?
Два в степени b-1.
Ну в терминах онотации давайте просто напишем
2 в степени b.
Ну у большой 2 в степени b.
На минус 1 забьем.
А сколько всего возможных пар LR нам нужно рассмотреть?
Ну b квадрат.
На константы забиваем b квадрат.
То есть вот такой предпочет.
То есть для каждой возможной
последовательности всего таких последовательностей
2 в степени b,
ну 2 в степени b-1, ну пишем 2 в степени b,
мы перебираем всевозможные пары LR
и сохраняем ответ.
То есть нам нужно вот столько памяти и вот столько времени,
естественно.
То есть вот сюда добавляется еще член
2 в степени b на b в квадрате.
Алгоритм закончен.
То есть вот алгоритм такой.
Давайте еще раз проговорим.
Берем массив размера n, делим его
на куски размера b.
Значит внутри каждого куска, точнее
по каждому куску считаем минимум, сохраняем в отдельный массив.
Вот поэтому получившемуся массиву
строим спарс таблицу.
Теперь вопрос, как отвечать на запросы
внутри вот этих подотресков?
То есть когда у меня получаются вот такие остатки?
Очень просто.
Фара Колтенбендер предлагает просто-напросто
взять всевозможные такие последовательности
и предпочитать ответ
на них.
То есть ответ на запрос вот на такой маленьком
участке будет занимать
от единицы. Это что? Ну то есть почему так?
Ну я просто буду смотреть, что это за
массив такой.
То есть какую массу он имеет.
То есть как он закодирован в терминах плюс-минус единицы.
И дальше обращаться к соответствующей
ячейке lr.
То есть запрос
работает всегда
за от единицы.
Ну почему? Потому что это одно обращение к спарс таблице
и максимум два обращения
к вот этой предпочитной
таблице размера 2 в степени b на b квадрат.
Все.
Запрос за единицу проговорили. Теперь
в качестве задания на после перерыва
попробуем понять
почему вот эта штука,
ну как эту штуку можно сделать линейной?
Тут в перерыве задали
вопрос. Я это не проговорил, но
давайте скажем. Ну смотрите,
был такой вопрос.
Ну смотрите, у меня есть...
То есть я хочу отвечать на запросы вот на таких маленьких отрезках.
Но чтобы ответить на запрос
на этой маленькой отрезке, мне нужно сначала понять
какому типу он принадлежит.
То есть по сути потратить время равное b.
А потом только обратиться в таблицу.
На самом деле тут тоже можно
все сделать за единицу, точнее нужно делать за единицу.
В том смысле, что вы можете заранее
для каждого этого подотреска понять
какому типу он относится.
Окей?
Это отрезок относится к типу m1,
этот относится к типу m2, m3 и так далее.
То есть заранее предпочитать для каждого
такого отрезка
его маску.
И дальше уже, когда вам поступает конкретный запрос,
скажем, вот на таком отрезке,
на таком подотреске, вы просто
уже знаете маску этого отрезка и просто обращаете
к соответствующим элементам lr.
Поэтому тут тоже все за единицу.
Поэтому
операция запроса
действительно выполняется за единицу, так как
к вот такому массиву у нас
sparse таблица обращение за единицу.
И вот к этим элементам мы тоже, так как все предпочитали,
это просто одно обращение
к трехмерному массиву.
Осталась непонятная следующая вещь.
Вот смотрите.
Время построения.
Мы
строим sparse таблицу за время nb
на алгорифм n деленное на b.
А после этого
на таких маленьких отрезках,
и точнее не на них, а вот просто перебираем
всевозможные маленькие отрезки,
всего их 2 в степени b.
И для каждой пары lr считаем на них ответ.
То есть 2 в степени b на b квадрат.
Вопрос. Что нужно взять в качестве b,
чтобы у нас все получилось?
Ну смотрите, как бы в прошлый раз,
когда мы обсуждали такую технику,
мы говорили, что в принципе в качестве b
неплохо бы взять
алгорифм n, да?
Ну почему? Какая логика была?
Ну вот если мы возьмем b равное алгорифму n,
то просто получится. У нас тут получится n
деленное на алгорифм n,
умноженное на алгорифм n
деленное на алгорифм n.
Так, можно тише, пожалуйста?
Так.
И что у нас получалось? У нас вот этот алгорифм сокращался вот этим алгорифмом,
да, и получалась просто
линейная симптотика.
Но проблема вот в чем. Если я возьму алгорифм,
да, и вот здесь, что будет, если
возьму что-то большее, чем алгорифм?
Ну по симптотике. Ну, например, там
какой-нибудь, не знаю,
алгорифм в квадрате.
Ну b станет
b большим, да?
Там у нас на самом деле был другой член,
в общем, ладно. Раньше мы брали в качестве b алгорифм,
у нас все устраивало. Значит, утверждается,
что взять алгорифм сейчас
не получится.
Почему? Потому что если я в качестве b
возьму алгорифм, то тут у меня возникнет что?
Два степени алгорифма это n,
и еще алгорифм в квадрате n.
Беда.
То есть алгорифм сейчас
оказывается слишком большой.
Ну, давайте попробуем взять
что-то меньше, чем алгорифм.
Если возьмем что-то меньше, чем алгорифм,
то...
Ну, давайте здесь.
Если я возьму что-то меньше, чем алгорифм,
то что у меня получится здесь?
Если b по асимптотике меньше, чем алгорифм,
то вот этого сокращения у меня не получится.
И вот тут возникнет
нечто, что больше, чем n.
Понятно?
То есть возникает такой затык.
С одной стороны,
брать алгорифм хорошо, потому что
здесь тогда будет n,
но тут будет n лог в квадрат n.
Если я захочу уменьшить вот этот член,
то есть возьму что-то меньше, чем алгорифм,
то у меня вот этот член вырастет,
он будет больше, чем линейный.
Что делать?
Не зря же написана эта формула.
Как ее победить?
Как все свести к линейному случаю?
Предлагаю алгорифм в степени меньше единицы.
Давайте попробуем взять алгорифм...
Опять же, меньше, чем алгорифм нельзя.
Допустим, корень алгорифма.
Тут будет n деленное на корень из лог n.
Тут будет лог n,
минус, не важно там,
n деленное на корень из лог n.
Вот этот алгорифм сократится
с этим корнем,
то есть этот алгорифм сократится с этим корнем
из лог n, и тут будет член...
Точнее, тут будет замножитель вида корня из лог n.
Само n степени меньше единицы.
О, а это уже хорошо.
Идея такая.
Идея взять n в степени, ну скажем,
1 вторая.
Давайте не так, давайте сделаем так.
Давайте возьмем b равная 1 вторая
от лог n.
Вроде как предыдущая интуиция,
не знаю, как у вас,
наверное, на основании предыдущих курсов,
предыдущих алгоритмов кажется, что константа ни на что не влияет.
Ну действительно, если мы посмотрим сюда,
то какая разница?
Возникнет константа, тут возникнет константа.
У нас в терминах онотации везде
константы убиваются.
Но не для экспонент.
Вот для экспонент константы крайне важны.
Смотрите, добавив всего лишь 1 вторую,
то есть заменив лог n на 1 вторую лог n,
что я получу здесь?
Здесь я получу все то же самое o от n.
Да, потому что тут будет логарифм,
и логарифм с логарифм сократится, получится просто n.
А что будет здесь?
Тут будет 2 в степени 1 вторая лог n,
умноженная на 1 вторая лог n в квадрате.
Чему равно 2 в степени логарифм n пополам?
Корень из n.
Корень из n на логарифм в квадрате n.
А что можно сказать про эту функцию?
Смотрим.
Да, любая степень логарифма растет медленнее,
чем любая степень n.
Понятно?
То есть можно сказать, что это асимптатически
растет медленней,
чем, ну скажем, корень из n, умноженный на корень из n.
Еще раз. Любая степень логарифма растет медленней,
чем любая степень n.
То есть это означает, что вот эту штуку
и мы победили, все. То есть как выглядит полный алгоритм? Мы уберем изначально, то есть мы хотим
решить задачу RMQ плюс-минус один, то есть у нас массиве все элементы соседние отличаются на плюс
один или минус один. Что мы делаем? Весь массив разбиваем на подмассивы размера b, значит
внутри каждого подмассива считаем минимум, строим на получившейся массиве спарс таблицу,
разреженную таблицу. Дальше для всевозможных подмассивов b, ну точнее для их маск,
предпочтим ответ. Предпочтен ответ занимает 2 в степень b умножить на b квадрат. Таким образом,
полное построение занимает вот такое время. Если я в качестве b возьму одну вторую на алгоритм n,
то у меня второй член будет n и третий член будет оцениваться сверху как n. В итоге суммарное
построение есть от n. Вот. Таким образом, удалось решить задачу rmq плюс-минус один за
линейное время. Ну почти как обещал. Остался последний пункт, вот действительно самый последний,
это понять как решить произвольную задачу rmq за линейное время при процессинге, точнее статик
rmq за линейное время при процессинге и за единицу запроса. Ну я уже немного проспойлерил, но давайте
напишем. Значит пункт последний. Решение статик rmq
за при процессинг от n и запрос от единицы. Все очень просто. Была у нас задача rmq. Сведем ее к
задаче lca. Мы умеем сводить задачу rmq к задаче lca за линейное время. Ну помните, да? Просто строим
декартовое дерево. Дальше. Что мы умеем делать с задачей lca? Да, мы умеем ее сводить к задаче rmq
плюс минус один. Тоже за линейное время. Рмq плюс минус один. А что мы умеем делать с задачей rmq
плюс минус один? Да, мы умеем ее решать с помощью алгоритма Farah-Colton-Bender. Все.
Farah-Colton-Bender. Все. Вот такая история. Ну, естественно, на практике так никто не
делает. Вот. Поэтому я как бы сделал анонс, что лекция больше теоретической. То есть с точки
зрения теории можно решить задачу rmq за максимально оптимальное время. То есть за линейное время
процессинга и за единицу. То есть, в принципе, если покопаться в различных математических,
алгоритмических статьях, то есть много предлагаемых решений, той же самой задачи rmq и так далее.
Ну это, наверное, самое классическое. Оно, наверное, встречается во многих курсах. Ну и плюс, наверное,
одно из самых простых. Ну, по крайней мере, из того, что я видел. Вот. Есть еще один алгоритм,
который описан на Codeforces. Я могу после лекции скинуть в чат. Если интересно,
почитайте. Ну, там какие-то народные умельцы, в общем, описали алгоритм, который может решать
ту же самую задачу rmq за линейное время и за вот единицы запроса. Вот. Но там, нам, кстати,
используется точно такая же идея, но уже без сведения к LCA и тому подобное. Но как бы первоисточников
я не нашел, поэтому рассказывать не стал. Вот. Ну, что еще можно сказать в заключении? Ну все,
мы полностью, по сути, рассмотрели задачу rmq, рассмотрели задачу LCA. Да, поняли, что, на самом
деле, с задачей LCA ничем не отличается с задачей rmq, то есть они взаимно друг другу сводятся. Вот.
Ну и по поводу этого метода тоже, наверное, стоит сказать, что это довольно распространенный метод
решения различных алгоритмических и других задач, который называется алгоритмом четырех
русских. Вот. Ну, то есть в чем идея? Идея стоит в том, что если у вас есть какая-то большая задача на
массиве, на матрице и так далее, то вы ее разбиваете на какие-то мелкие подмассивы, подматрицы. Дальше
каждую отдельную подзадачу решаете в тупую, то есть просто предпочитываете там всевозможные ответы
и так далее. И за счет этого каким-то образом достигаете ускорения результата. Вот. То есть,
можно сказать, вот алгоритм Faraholten-Bender является там применением метода четырех русских вот к
задаче rmq плюс-минус один. Вот. Поэтому, когда вы встретите там какой-нибудь алгоритм, то, может,
какие-то аналогии найдете. Вот. Ну а на этом все, лекционная часть закончена. В общем, все,
что я планировал так или иначе рассказать вот в этом семестре, я рассказал. Ну все, всем спасибо.
Аплодисменты.

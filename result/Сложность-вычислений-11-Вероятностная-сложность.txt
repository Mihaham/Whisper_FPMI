В прошлый раз мы изучали схемы. У нас осталась небольшая тема про НСИ и иерархию и всякие
быстрые сумматры. Я думаю, что ее можно пропустить, либо она будет на семинарах,
ну или просто пропустим. А вот подробнее поговорим про вероятные вычисления. Сегодня и через неделю
поговорим, с одной стороны, про то, что можно вычислить при помощи случайных битов,
но и, с другой стороны, можно от этого, наоборот, избавиться. То есть можно заменить рандомизированные
алгоритмы на детерминированные. Есть одна очень известная история про то,
как это было сделано. Это вопрос о проверке простоты. Есть известная задача.
Праймс. То есть нужно по числу понять, что оно простое.
Соответственно, если тут расписывать подробно, то на вход дана двоечный запись числа натурального,
то нужно понять, что оно простое. При этом, если мы говорим о полинамиальном времени,
то стандартный школьный алгоритм, перебирать все делители до корня и пытаться поделить,
он не будет полинамиальным, потому что мы же смотрим на длину записи, то есть на число знаков.
А число знаков – это алгоритм от самого числа. То есть полинамиальный алгоритм для этой задачи
– это на самом деле полилогарифмический от самого числа. Ну и, конечно, если мы перебираем
все числа до корня, то никаким полилогарифом тут не пахнет. Это долгое время было интересной
задачей. Ну и сейчас остается интересной задачей. На самом деле, задача решена. Есть алгоритм
Агревала Кояло-Соксене 2004 года, который как раз решает за полилогарифмическое время от П,
но там степень довольно высокая. Там степень примерно 6 плюс Эпсилон. Есть там всякие вариации,
если там какая-то гипотеза верна, там будет ни шестая, четвертая степень и так далее.
Вот. Не знаю, вам, вроде, Андрей Михайлович не рассказывал, да, НОКОТЧа? Не собирается. Не,
НОКОТЧа закончилась уже. Не, был какой-то год, когда этот алгоритм входил прямо в основную
программу, да, и все его учили и сдавали. Ну, кто на 5 хотел сдать? Ну, не на хор 5, а на отлично.
Хорошо, мы его тоже не будем изучать, но вот о чем Андрей Михайлович тогда не рассказывал,
это то, что на самом деле этот алгоритм был получен в результате дерандомизации. То есть
сначала Агревал придумал некоторый вероятностный алгоритм, а потом уже вместе с Кояло-Соксеной они
придумали, как его заменить на детерминированный, который по сути делает то же самое. Вот. Ну и вот это
на самом деле один вообще из базовых вопросов в сложке вычислений верно лишь, что все можно
дерандомизировать. То есть что можно заменить вероятностные вычисления на детерминированные.
Причем это вообще важная задача для криптографии. Как вы, наверное, понимаете, что многие
криптографические протоколы основаны на том, что вам нужны большие простые числа, потом вы их там
перемножаете, и это получается ключ и так далее. Есть очень много разных протоколов,
которых нужны большие простые числа. Соответственно, если у вас есть большое и простое число, нужно
проверить, что оно простое. Ну и соответственно, если оно большое, то тогда простые алгоритмы не будут
работать. Будут слишком долго работать. И поэтому нужны какие-то хитрые алгоритмы. На самом деле и
даже АКС-алгоритм на практике все равно слишком долгий, потому что там шестая степень от числа
знаков. А вероятностные тесты работают за вторую степень от числа знаков и уже дают приемлемую
точность. Начну я с того, что мы поговорим немножко о конкретных вероятностных алгоритмах. Или вас
это учили где-нибудь? Ну скажем, Миллера Рабина или Славея Штрасна, знаете, что такое? Сейчас,
вы все знаете откуда-то еще. Ну какие-то другие были алгоритмы,
да? Не, вот Монте-Карло это вообще правильные слова. Можно сначала вот это, кстати, обсудить. Но
Монте-Карло и Лас-Вегас, значит, Монте-Карло есть в узком смысле некоторые конкретные алгоритмы,
да, и в широком смысле некоторые классы алгоритмов. Вот. А Лас-Вегас, насколько я знаю,
это только класс. Или у вас тоже был класс. Да, ну давайте сначала тогда это обсудим быстренько.
Про Монте-Карло это еще в середине XX века придумали, как быстро считать многомерные интегралы.
Если у вас какая-нибудь плоская картинка, да, напишу, метод Монте-Карло. Так, ну это напоминание,
как я понимаю, то есть можно быстро вспоминать. Значит, если у вас плоская картинка, да, то есть
вообще такой метод в школе проходит. Нужно взять палетку, вот так вот разбить на ячейке,
и посчитать, сколько у вас клеточек целиком попало внутрь, сколько частично, значит,
взять количество тех, которые целиком попали, плюс половину тех, которые частично, это будет
примерно площадь. Вот, и это довольно точный метод на плоскости. А если у вас будет
десятимерное пространство, то будет проблема. Здесь как бы нет проблемы, скажем, на 100 частей
разбить здесь, на 100 частей разбить здесь и 10 тысяч клеточек проанализировать. Но если у вас
десятимерное пространство, то будет не 10 тысяч, а 100 в десятый, то есть 10 в двадцатый. Это уже
очень много, и все эти клеточки просто так не проанализировать. Соответственно, что придумали
Метрополис и Улам? Что придумали Метрополис и Улам? Они придумали, что когда у вас много
измерений, то не нужно вообще делать некую сетку, а нужно просто случайно кидать точки. Кидаем
точку. В общем, как-то вот случайно накидываем точки в достаточно большом количестве. Ну,
в общем, что-то такое там получается. Значит, накидали точки в большом количестве, и дальше просто
считаем, какая доля попала внутрь фигуры. И тут уже от размеров не зависит, это зависит только того,
умеем ли мы понимать, лежит ли точка внутри фигуры. Просто по всяким вероятствам-тиоремам,
независимо от размерности, одно и то же число точек нужно взять, чтобы разброс был какой нужно.
Вот, хорошо. Значит, таким образом, что это в узком смысле метод Монте-Карло под счет объема. В широком
смысле, можно сказать следующее, здесь гарантированно время работы, но не гарантирована точность,
то есть, возможно, ошибка. Значит, гарантированно время, но, возможно, ошибка. Вот, Лас-Вегас на
оборот. Лас-Вегас точно правильно, но не гарантированно время, может быть, долго, только
в среднем гарантированное время. Значит, точно верно, но, возможно, задержка. Ну, простой пример
алгоритма такого духа. Это как при помощи обычной монетки сгенерировать случайную величину 1 3 2 3.
Да, значит, понятно, что если вы какое-то фиксируемое число раз кидаете монетку, то у вас
точно вероятности будут со становящим степень двойки, а 1 3 не является такой дробью. Но можно
делать следующее, кидать монетку два раза, и, например, если выпало два орла, то говорить да,
если выпало разное, орел-решка в любом порядке, то говорить нет, а если выпало две решки, то еще два
раза бросать. Тогда получается, что одна четверть это ответ да, две четверти ответ нет, и еще одна
четверть повтор. Ну и там ряды просуммируются, будут как раз одна треть и две трети для ответов да и нет.
Но, конечно, никакого гарантированного времени нет. Любое ограничение на время сразу сваливает все
в большинственные числа. Но среднее время будет не очень большое. Среднее число бросков будет
вообще 4 третьих, видимо, да. Вот, то есть даже меньше двух. Ой, в смысле 4 третьих умножить на два.
Средний число пар бросков 4 третьих. Да, то есть это получается 8 третьих, да, если одиночные
броски считать, но меньше трех. Так, хорошо, еще бывает алгоритм Атлантик-Сити, в которых и ошибка
возможна, и задержка возможна, но как бы ничего лучше не получается. Не, это правда, да, что можно
превращать одно в другое. Ну сейчас, метод Монте-Карло, как бы если ответ это число, то можно
увеличивать время и уменьшать ошибку. Так, ну хорошо. Ладно, теперь давайте про простые числа поговорим.
Значит, начнем совсем простой вещи, который называется тест Ферма. Так, что нам говорит
малотерема Ферма? Что любой остаток в степени p сравним с единицей по моделю p, ну, кроме нуля.
Так, это что? Это малотерема Ферма. Значит, малотерема Ферма, что если наибольший общудитель от a и p
равен единице, то тогда в степени p сравним с единицей. Так, что там p-1, да? А, давайте так,
а в степени p сравним с a по моделю p, тогда это вообще всегда верно. Так, сейчас, ну ладно, пусть так
останется. Ну и ладно, давайте я напишу. Все-таки с минус 1. Потому что тест Ферма устроен следующим образом.
Тест Ферма устроен так, что мы берем случайное a, ну, например, от единицы до p-1. Тут нам как
раз пригодится вот это бросание монеток с повторами. На самом деле, да, совсем случайно у нас не
получится, если только p-1 не степень двойки. Но если даже и не степень двойки, то там какие-то
небольшие отличия от равномерного распределения нам ничего не испортит. Что? Да нет, ну зачем два раза?
У нас пусть будет отличие от... Ну мы что можем сделать? Мы можем сделать, чтобы отличие от честной 1
длить на p-1 было каким-то экспоненциально маленьким. Ну да, то есть если p-1, если степень двойки,
то нам нужно логарифм p монеток кинуть. Если не степень двойки, нужно следующую степень двойки
делать как вот это самое, с одной третью. Соответственно, можно еще порядка того же
логарифма это сделать. И вероятность того, что каждый раз будет ошибка, будет как раз порядка 1
длить на p, что будет экспоненциально маленьким от логарифма, то есть от числа знаков. Короче говоря,
это все не очень важно. Давайте считаем, что мы умеем выбирать прямо случайное число с таким
распределением, как надо. Так, выбираем случайно от 1 до p-1. После этого, во-первых, если так
получилось, что наибольший общеделитель от a и p не равен единице, то тогда число точно непростого.
Ответ нет. Второе. Иначе проверяем, что a в степени p-1 сравнивается с единицей.
И соответственно, как есть, так и отвечаем. Если да, то мы отмечаем, что, скажем так, возможно,
простое. Вот. А если нет, тогда уж точно составное. Точно составное. Так.
Тут, кстати, еще нужно упомянуть о быстром возведении степени, чтобы это был полиномиальный алгоритм.
Это вы знаете, наверное. Посчитать там квадрат, 4 степени, 8, 16 и так далее. Потом сложить,
какие нужно, чтобы получилось a в степени p-1. А тут алгоритм Евклида, так что это полиномиальный
в целом алгоритм получается. Вот. Тогда что же получается? В чем его плюс, в чем его минусы?
Слышишь, что первый не обязательно проверять, да? Ну, вообще, наверное, да.
Да, то есть этот нотас будет оставаться деятелем. Ну, может, конечно. На самом деле, просто
алгоритм Евклида, по идее, попроще будет. Ну, неважно. Правильно. Можно и не проверять.
Вот. Проблема тут в другом. Проблема в том, что некоторые составные числа проходят этот тест
Renste-1. Знаете ли вы такие числа? Кармаекла, да. Вот. Их, конечно, мало. Да, их, конечно, мало.
Так, но вроде, насколько я помню, доказали, что их бесконечно много. По-моему, это какой-то недавний
результат, но доказали, что их бесконечно много. Что-то, я помню, мы это выясняли. Какой-то типа,
десятилетней давности или что-то такое. Нет, критерий-то простой. Критерий, что p-1 делится
на функциялер от p. Критерий простой. Вопрос верный, что он выполнен для бесконечного числа чисел.
Вот. Проблема в том, что числа Кармаекла составные, но проходят тест всегда.
С точки зрения теории, но не практики, можно было сказать следующее, что если бы оказалось,
что их конечное число, то можно было бы это конечное число исключения прямо прописать в
текст алгоритма, и тогда этот тест работал бы. Если у нас число входит в конечный список чисел
Кармаекла, то тогда оно составное, а иначе мы делаем вот это. И там есть еще теорема, что если
p не простой и не число Кармаекла, то тогда вот здесь ответ да будет с вероятностью максимум половина
для случайного а. Половина. А тогда, смотрите, половина это уже хорошо, потому что можно взять
не одно случайное а, а десять случайных а. И тогда, чтобы все десять раз сказали да, а на самом деле не
всегда было да, нужно, чтобы мы все десять раз попали в эту нужную половину, а у этого вероятности уже
будет одна тысяча двадцать четвертая. Ну, если мало, возьмите двадцать раз, будет одна миллионная с лишним.
Вот, это называется амплификация, это позволяет уменьшать ошибку. Вот, ну хорошо, значит, но даже в
теории оказалось, что чисел Кармаекла бесконечно много, вот, поэтому этот тест он хоть и для
малого, да, для редких входов дает ошибку, но тем не менее для этих редких ходов он дает
всегда ошибку с рентстью один. И за пункт один не всегда. Да, хорошо, за пункт один не всегда, но все
равно там мало будет делителей. Вот, а мы хотим не это, мы хотим, чтобы для каждого входа был ответ
правильный, но, возможно, была ошибка за счет неудачного выбора случайных битов. Вот, хорошо,
ну и, значит, на этом основание Миллер и Рабин придумали, но в некотором смысле, обобщение теста
ферма. Вот, и вот это уже прямо на практике используется и до сих пор, да, то есть если нужно
проверить на простоту и допустимо ошибка, да, то вот можно тест Миллера Рабина использовать. Так,
он действительно довольно простой. Значит, смотрите, вот пока пусть п простое, значит,
п простое, тогда мало-то время ферма выполнена, да, и, соответственно, а в степени p-1 равняется единице.
Да, ну правильно, сейчас, мы это уже обсудили, да, что есть небольшой шанс, что мы прямо попадем
в делитель, выбирая а, но это очень редко бывает. Ну числа кармайкла, там обычно не маленький делитель.
Так, хорошо, ладно, я буду писать сравнимо, но не буду писать модуль. Значит, также, конечно,
в случае p-2 мы как-нибудь разберем, соответственно, p будет нечетная. Тогда в этом случае получается,
что а в степени p-1 пополам должно быть сравнимо с плюс-минус единицей. Вот, а если а в степени
p-1 пополам сравнимо с плюс-минус единицей, и при этом еще p-1 делится не только на 2, но и на 4,
значит, то можно, соответственно, еще и дальше сказать, что а в степени p-1 пополам на 4 тоже
сравнимо с плюс-минус единицей. Вот, ну и так далее. Вот, и так далее, да, то есть рассматривается ряд
вот a в степени p-1, p-1 пополам, p-1 на 4 и так далее, пока будет делиться. Да, то есть вот это уже как, в чем тенд заключается.
Ну, можно и так сказать.
Вот, значит, рассмотрим а в степени p-1, а в степени p-1 пополам, а в степени p-1 на 4 и так далее,
значит, все поможет для p. Значит, соответственно, если p простое, то этот ряд,
то в этом ряду либо только единица, либо сначала единица, потом минус единица. Значит, в этом ряду
либо все единицы, значит, либо префикс. Это сначала обязательно единица, значит,
потом еще сколько-то единиц, и потом минус единица. Здесь хотя бы одна единица.
Вот, но для простых это понятно, потому что по простому модулю есть только два корня из каждого
квадратичного вычета, и из единицы только единица и минус единица. Вот, но вот Миллер и Рабин
доказали следующее. Значит, теорема, что если p не простое, значит, если p составное,
то вот это верно с вероятностью не больше одной четверти. Ну, можно сказать так, да.
Пока либо не будет минус единицы, а если вместо, если были единицы, а потом не минус единицы,
а что-нибудь еще, то значит, точно не простое. Вот, ну тоже получается так, что если число
простое, то это точно верно, и соответственно, если это условие нарушено, то число точно составное.
Ну получается, что так, да.
Ой, ну такие детали уже зависит от точной модели. Вот это и называется точная модель. Ну, в принципе,
да. Ну, типа того, да. Ну, хорошо, доказывать я это не буду. Теория чисел какая-то хитрая.
Но дальше идея такая, что опять же, вот эта вот одну четверть, это вообще еще лучше, чем одна вторая,
потому что тут надо в два раза меньше повторять, чтобы такую же маленькую вероятность ошибки оставить.
Вот так, ну хорошо, значит, Славея Штрассена, значит, я думаю, можно пропустить.
Славея Штрассена основана на квадратичных вычетах. Так, а вот квадратичный закон взаимности у вас был?
Или не у всех, да. Хорошо, ну ладно, значит.
Ага, так у вас второй квадратич был по выбору, да, и там не все брали, да. А вообще не было, да.
Понятно. Вот. А что такое символ Якови, знаете? Символ Якови, знаете, что такое? Вот есть символ
Лежандра, есть символ Якови, который тоже самое, но по непростому модулю. Так, не, ну ладно, тогда это пропустим.
Это слишком долго рассказывать. Вот, значит, я лучше про другое расскажу, значит, для чего окотуче не нужно.
Есть такая очень важная задача, называется polynomial identity testing, значит, PIT, задача о равенстве многочленов по-русски.
Задача о равенстве многочленов. Значит, ну, как бы, на первый взгляд, звучит очень просто, да, есть два многочлена,
надо проверить это один и тот же многочлен или разный. Вот. Ну, дальше начинается сразу много вопросов.
Значит, во-первых, что мы вообще понимаем под многочленом? Потому что у многочленов есть две принципиально разные понимания.
Это формальная запись, там сумма с коэффициентами, и это функция на каком-то множестве, который эту запись задает.
Если это множество, это конечное поле, то это получается не одно и то же. Вот у нас были многочлены Джигалкина,
которые основаны на том, что x² и x это разные записи, но один тоже многочлен, то есть одна и та же функция.
Разные многочлены как записи, но один тоже многочлен как функция.
Значит, на этот вопрос мы рассматриваем многочлены как записи.
Ну, дальше вопрос, а как именно мы их записываем? Потому что ясно, что если у нас просто сумма с коэффициентами,
то там вопрос только в порядке слагаемых и порядке множителей, это мы как-нибудь проверим, что можно так сортировать.
Ну, можно как-то сортировать оба многочлена, единообразные, проверить, что получилось одно и то же.
Тут тоже нет проблемы. Но проблема начинается, если эти многочлены заданы какими-то выражениями со скобками.
Конечно, мы все умеем раскрывать скобки, но если скобок много, их все раскрыть, то может получиться очень длинное выражение.
Если у вас есть ян скобок и в каждом два слагаемых вы все раскрыли, то будет два степени ян слагаемых,
и потом еще приводить подобное. Ну, два степени ян, да, и каждый из ян скобок, либо первый слагаемый, либо второй.
Классическая постановка даже еще более хитрая. Не только формулы разрешаются, но и схемы.
Вот рассматривается понятие алгебраическая схема.
Алгебраическая схема – это аналог логической, но там будут сложения и умножения в качестве базовых операций.
Аналог логической с сложением и умножением в качестве базовых операций.
Тоже правильный вопрос.
Переменных у нас будет много, то есть тут, смотрите, я как бы формулирую как можно более общую задачу, которую можно решать вероятностно.
Даже с одной переменной могут быть некоторые трудности за счет итерированного умножения.
То есть мы можем взять х и потом его как бы взять и умножить сам на себя,
а потом результат еще раз умножить сам на себя, а потом еще раз умножить сам на себя и так далее.
Что это я рисую? Я имею в виду, что результат умножения поддаю и как первый множественный, и как второй множественный на следующий элемент умножения.
Да, тогда, соответственно, в итоге тут получится х в степени 2 в степени n.
То есть даже степень получается экспедиенциальной, даже для одной переменной может быть какая-то слишком длинная запись.
Вот, и это такая прямолинейная схема.
Если как-то витвиться будет и так далее, то может там и слагаемых будет тоже экспедиенциально много, а не только степень.
Ну, собственно, будет, если перенажать х плюс 1 на х квадрате, плюс 1 на х4, плюс 1 на х8, плюс 1 и так далее, все раскрыть, то там будут все степени.
Вот, ну хорошо, соответственно, получается, что задача такая.
Значит, даны две вот таких алгебрических схемы, схемы по IQ.
Значит, и нужно проверить, что они задают один и тот же многочлен от многих переменных.
Один и тот же многочлен.
Вот, вот это называется PIT.
Значит, как ее решать за полинальное время никто не знает.
Но он знает, как решать вероятностно.
Значит, вероятность алгорита такой.
Но нужно взять какую-нибудь модуль, большой, но простой, и взять два случайных остатка по этому модулю,
взять один случайный остаток по этому модулю и вычислить на нем его многочлену.
Конечно, не раскрывая скобки, а идя просто сверху вниз и вычисляя значение всех этих штук.
Если получилось разное, то мы решим точно разное.
Если получилось одно и то же, то мы решим, скорее всего, один и тот же.
Тут, значит, вероятностный тест.
Значит, вычислить PQ
по модулю
по модулю P на случайном входе.
Значит, это будет быстро, если P не слишком большое.
Если P имеет размер порядка экспонента от вообще размера входа.
Значит, размер входа – это число элементов в этих схемах.
Вот это правильное замечание.
То есть возникает вопрос, откуда брать вот это вот P.
Откуда брать P.
Вообще проблема еще может быть,
если вдруг получилось, что именно по этому модулю P многочлен равен нулю, а вообще не равен.
Ну, точнее, в смысле, эти многочлены два совпадают по модулю P, а вообще не совпадают.
Если так вдруг получилось, то будет проблема.
Ну, их конечно конечное количество, но важно еще не только, что оно конечное,
но еще не слишком большое.
То есть, на самом деле, вот это вот P тоже надо брать случайным, но в некотором диапазоне.
Значит, случайный в некотором диапазоне.
Значит, а в каком?
Ну, все-таки, на чем основана работа этого теста?
В принципе, что у многочлена корней не больше, чем его степени.
И это, на самом деле, верно и для нескольких переменных.
Значит, там эта более хитрая штука называется Лемма-Шварца-Зиппеля.
Ну, не знаю, сейчас можно обсудить, наверное.
И это вам не говорили, да, таких слов?
Лемма-Шварца-Зиппеля тоже не говорили.
Там что-то похожее было.
Там тоже были какие-то корни.
Ну, там были тоже две корни у многочлены многих переменных.
Тоже помню.
Нет, это вроде в первом семестре.
Там, значит, была без доказательств, я и сам не помню.
А...
Так, ну ладно, сейчас, может, обсудим.
В общем, глобально картина такая,
что мы хотим взять P достаточно большим,
чтобы все-таки по этому модулю,
если по этому модулю многочлены тоже оказались разными,
чтобы у них было мало общих корней.
Ну, не общих корней, в смысле.
Точи, где они равны, то есть корни их разности.
Поэтому P должно быть достаточно большим.
С другой стороны, P должно быть достаточно маленьким,
чтобы по этому модулю все-таки все можно было вычислять.
И с третьей стороны, нужно, чтобы P не было делителем
всех коэффициентов разности этих многочленов.
Но оказывается, что все-таки все это можно одновременно выполнить.
Получается, что чтобы...
Ну, идея такая, что чтобы можно было вычислять по этому модулю,
нужно, чтобы P было экспоненциальным,
но более-менее любым экспоненциальным.
А чтобы P...
Сейчас, чтобы было не слишком много корней,
P должно быть больше некоторых конкретных экспонентов,
которые возникают из размера этих схем.
И там получается зазор.
То есть зазор между той экспонентой,
которая уже дает мало корней,
и той экспонентой, которая еще позволяет все быстро вычислить.
И более того, можно показать, что в этом зазоре достаточно много простых чисел,
чтобы, если мы берем случайное простое число в этом зазоре,
то мы попадаем в то, по которому...
Скорее всего, попадаем в то, по которому многочлены,
если были разные изначально, то останутся разными.
И по этому модулю тоже.
Это следует из того, что простых чисел довольно много,
если эта разница на них на всех делится,
то это такой маленькой схемы уже просто не получится сделать.
Вот. Ну вот.
Так, ну я не знаю, надо ли в эти детали лезть,
или так примерно поняли, и можно дальше идти.
Да, хорошо, потому что дальше я хочу поговорить про
уже сложностные классы,
их соотношения.
Так, а перерыв надо делать?
Ну давайте на три минутки сделаем.
Я в прерыве уже начали спрашивать про формальную модель,
действительно я про нее ничего не сказал.
Значит, давайте тогда с этого начнем.
Значит, что такое вероятностная машина тюринга?
Вероятностная машина тюринга.
Значит, вероятностная машина тюринга.
На самом деле есть разные определения,
и они не всегда эквивалентны,
особенно если речь идет о лагерфнической памяти.
Значит, если речь идет о поленальном времени,
там более-менее неважно, что происходит.
С лагерфнической памятью есть некоторые тонкости,
про них мы, наверное, почти не будем говорить.
Простейший вариант – это просто машина с двумя аргументами,
где второй аргумент случайный.
Простейшее определение – это машина с двумя аргументами M от X, R.
Где X – это вход, а R – случайные биты.
Значит, это похоже на сертификатное определение NP.
Там у нас тоже был вход и сертификат.
Отличие в том, что сертификат нам там один подходил,
а здесь мы не можем выбирать,
просто он выбирается случайно.
На первый взгляд, недетерминированные случайные – это синонимы,
но не в смысле сложить вычислений.
В смысле сложить вычислений получается, что R случайно,
и все вместе M от X, R получается случайной величиной.
Вернульевской, которая либо 0, либо 1,
и, соответственно, будет какая-то вероятность для каждого экзотика,
вероятность того, что это выдаст единицу.
Точно так же можно сделать вариант
и через недетерминированного шинонтюринга.
Можно сказать так, что вероятная машина –
это точно такой же объект, как недетерминированная машина,
и у нее там есть дерево вычислений,
и у нее там есть дерево вычислений,
но вместо того, чтобы выискивать ветку,
которая ведет к ответу «да»,
мы берем просто среднее по всем ветвям.
То есть просто в каждой точке ветвления у нас, пусть там равновероятно,
по всем ветвям уходят, получается распределение на листьях,
но и мы берем среднее, это и будет как бы ответ.
Так.
Сколько случайных битов?
Сколько случайных битов?
Значит время работы должно быть, если мы там говорим,
ну вообще все ресурсы меряются как функции от длины х.
То есть по лином от длины х?
Да, то есть случайных битов по лином от длины х.
То есть на самом деле это больше, чем она работает?
Ну типа того, да.
В принципе есть еще другой подход, например,
на одной ленте у вас есть х,
а другая лента вообще бесконечная,
она вся целиком заполнена случайными битами.
Но это я с временем не ограничивать.
Ну в плане вот с этой модели у нас нет точности смоделирования.
Ну да.
Да, если это ограничено, то так Лас-Вегас не смоделирует.
Да, я согласен.
Вот.
Можно еще моделировать через специальное состояние,
которое выдает просто случайный бит.
То есть мы переходим в специальное состояние кинуть монетку,
и в этот момент в той клетке, куда мы смотрим,
в следующий момент получается случайный бит.
И мы его дальше как-то используем.
Вот.
Значит вот если есть логографическая память,
то это все не совсем одно и то же.
Потому что одно дело, когда у вас уже есть лента
с случайными битами, вы их можете там туда и сюда ходить.
Вот.
А другое дело, если вы все, что можете хранить,
только на логографической памяти,
да и соответственно не можете хранить те биты, которые уже были.
А на случайные биты не односторонние ленты?
Может быть односторонние.
Но все равно, нет, не одно и то же.
Потому что если у вас есть любая память,
даже если есть случайный бит односторонняя,
если у вас сколько угодно памяти,
то вы можете все, что получилось, уже себе записывать и хранить.
Вот. Если у вас логографическая память, то все вы не можете хранить.
Вы только логариф можете хранить выпавших битов.
Вопрос.
Если мы предъявим через него, как они через медитерминированную машину,
то у нас одинаковое распределение по листам?
Или мы считаем, что каждый по вашему?
По листам, по ветвелениям.
Ну опять, там тоже есть разные варианты.
Есть вообще такой подход и к недотмирной тоже,
что у вас просто есть две функции перехода.
Там дельта 0 и дельта 1.
И недотмирно у нас получается либо недотмирно одно и то же,
поэтому просто переход, либо у вас получается ветвление.
Вероятность получается, что просто вы кидаете монетку,
по какой функции перехода переходить.
Вот, хорошо.
Ладно, в общем получается, что для каждого х есть вероятность,
для каждого х задана вероятность того, что m от xr равно 1.
Вот, и дальше есть такой маленький зоопарк веренственных классов.
И они все определяются так, что если х лежит в множестве,
то какое-то одно условие должно быть выполнено на эту вероятность.
Если х не лежит в множестве, то какое-то другое условие на эту вероятность.
Значит так, что эти условия конечны.
Сейчас изучим.
Изучим, какие есть классы и как они соотносятся.
Изучим, какие есть классы и как они соотносятся.
так но дать табличку
тут будет табличка определений, а здесь я нарисую диаграмму, как эти классы друг у друга относятся
тут будет класс, какая должна быть вероятность того, что м от хр равно 1 при х лежащем в а
вот и дальше вероятность того, что м от хр равно 1 при х не лежащем в а
ну с чего начнем? начнем с класса rp, rp расшифровывается как randomised polynomial
тут всюду будет полиномиальное время, то есть что вот м от хр работает за полином от длины х
значит в случае с рп тут будет 0, а тут будет больше либо равно 1 и 2
вот и к нему есть парный qrp, так равно 0 давайте напишу
парный qrp тут будет равно 1, а тут будет меньше либо равно 1 и 2
как раз простые числа и вот эта задача равенстве многочленов они все будут в qrp
если утверждение выполнено, если число простое или если многочлены равны, то наш алгоритм всегда выдавал единицу
если соответственно неверно было, то тогда вероятность была меньше чем 1 вторая, но там у Миллера равен 1 четверть
здесь надо аккуратно показать какие параметры, может и меньше 1 и 2
это класс с односторонней ошибкой, дальше есть класс с двусторонней ошибкой bpp
буква kb означает bounded error, bounded error пробабилистик полиномиал
тут рандомайст, а тут пробабилистик, и это разное означает, что так сложилось
соответственно здесь будет больше либо равно чем 2 третья, здесь будет меньше либо равно чем 1 третья
еще есть без буквки b, просто pp
тут будет больше либо равно 1 и 2, а тут будет строго меньше 1 и 2
тут решается по большинству, а если ничья, то в пользу ответа да
и такое решение должно дать правильный ответ, тогда это будет pp
еще есть, но можно делать просто p, значит если здесь равно 1, если здесь равно 0
почему это будет просто p?
потому что можно просто какой угодно r написать, например из всех нулей
и вычислить m от x и этого r, и это будет правильный ответ
если здесь всегда дают 1, здесь всегда дают 0, значит с любым r всегда дают правильный ответ
то есть это просто p
а есть вот как раз к вопросу о Лас-Вегасе есть zpp
значит тут тоже zero error zpp, тоже тут равно 1, тут равно 0
но только среднее время полиномиальная
среднее время работы полиномиальная
ну и на самом деле можно в этих же терминах и np и co-np тоже сформулировать
значит np тут у нас будет больше 0, а тут равно 0
ну а co-np соответственно тут будет 1, а тут будет меньше 1
но проблема в том, что это большая нуля, это может быть что-то экспоненциально маленькое
так, хорошо, вот такое местоопределение
правда, это сейчас обсудим
и в bpp тоже
так, сейчас я нарисую большую диаграмму, как все эти классы друг к другу соотносятся
значит здесь будет p, дальше будет zpp
и оно тут не просто будет, а можно про него доказать, что он равняется rp в пересечении co-rp
соответственно тут будет
что еще раз?
ну это я диаграмму хасса рисую, кто в кого вложен
значит здесь rp, здесь co-rp
так, значит здесь bpp
значит здесь np
значит здесь co-np
и вот здесь на самом верху будет pp
вроде все, да? 8 и там 8
некоторые положения тут очевидны
например, почему p в zpp вложено?
потому что если время в худшем случае полиномиально, то в среднем оно тоже полиномиально
а ошибки там те же самые
так, дайте я буду жирным делать то, что мы обсудили
так, вот это очевидно
еще очевидно вот это вот
потому что если меньше чем 1 на треть, то меньше, чем 1 на вторая, если больше, чем 1 на треть, то больше, чем 1 на вторая
поэтому если выполним условия для вот этого, то выполним для вот этого тоже
и по тем же самым причинам rp вложено в np
значит co-rp вложено в co-np тоже просто из этой таблички
если больше 1 и 2, то больше 0
то есть можно сказать так, что случайные биты, которые дают ответ 1 в rp
они живут в сертификатном смысле np
остальные уже не такие очевидные
я, наверное, буду уже записывать
в общем, про то, почему rp вложено в bpp вы на самом деле уже сказали
это называется амплификация
нужно два раза повторить
и взять
в случае с rp нужно взять дизьюнкцию
запустить два раза
два раза взять дизьюнкцию
и так получается, что 0 останется нулем
а вот больше ли бравно 1 и 2 превратится в больше ли бравно, чем 3 четверти
ну понятно, да?
что получился 0, ну чтобы там и там получился 0, а там, вероятно, не больше 1 и 2
и запуски независимые, и поэтому будет не больше, чем в 1 четверти
ну а 3 четверти уже больше, чем в 2 третьи, да, и мы получили условие на bpp
так, ну чего, понятно, да?
так
ну тут понятно, что аналогично, да, только наоборот
так, почему вот это вот верно, что zpp вложено в rp
так, чего, почему, говорите?
а, из np в pp, нет, это немножко посложнее
давайте вот это сначала обсудим
значит, zpp почему вложено в rp?
ну смотрите, пусть у нас есть алгоритм, который работает в среднем t от n
и дает правильный ответ
да, значит, пусть m от xr работает в среднем
значит, t от n дает верный ответ
так, тогда есть вероятность Маркова
так, вы это проходили?
да
хорошо
чего?
8 раз проходили
а, 8 раз уже проходили вероятность Маркова, ну хорошо
тогда получается, что вероятность того, что время работая
ой, time m от xr будет больше, чем 2t от n
значит, тут вероятность будет меньше, чем 1 вторая
вот, и тогда получается, что алгоритм такой
значит, rp алгоритм
значит, вот запустить m от xr на 2t от n шагов
значит, соответственно, если остановилось, то такой ответ и дать
значит, остановилось, дать ответы, дать ответы, дать ответы
Значит, соответственно, если остановилось, то такой ответ и дать.
Значит, остановилось, значит, дать такой же ответ.
Значит, не остановилось, но надо дать такой ответ, чтобы не ошибиться в нужную сторону.
Так, значит, смотрите, нам нельзя давать ответ 1, если х не лишь ва.
Ну, раз мы не можем давать ответ 1, не знаем, какой на самом деле, но дадим ответ 0.
Значит, соответственно, если не останавливается, то дадим ответ 0.
Вот, тогда что же получается?
Ну, получается, что если, значит, если на самом деле не лежит ва, то мы в любом случае дадим ответ 0.
Либо потому, что мы успели почитать правильный ответ, либо потому, что мы по умолчанию дали ответ 0.
Вот, если лежит ва, то тогда получается, что с вероятностью 1-2 хотя бы мы успели почитать правильный ответ,
значит, и соответственно, да, значит, и сказали верно 1.
Ну, а с оставшейся вероятностью сказали 0, ошиблись, но эта ошибка нам разрешена.
Ну, понятно, да?
То есть мы говорим, что P&R равно 1 при условии, что их служит A больше, чем просто P&R равно 1.
И мы берем вот этот P&R и играем, что P&R равно 1, если оно работает хотя бы на 2.
Ну, да. Ну, типа того, да.
Вот, хорошо. Значит, ложно в QRP аналогично.
То, что обратное, значит, пересечение RP в QRP вложено в ZPP, это давайте я оставлю там на семинар или на упражнения.
Так, и чего еще был вопрос вот про эту часть, да? Почему NP вложено в PPP?
Так, а в чем вопрос?
Ну, у нас же там ровно 0, а не меньше, чем Epsilon.
Ровно 0, да.
Ну, как бы, а там, типа, то есть моделирование сразу отпадает?
Нет, значит, смотрите.
Значит, смотрите, идея такая, что нам нужно, как бы у нас по духу и там, и там нет разрыва.
Значит, смотрите, когда я вот здесь пишу меньше, чем 1 вторая,
тут на самом деле меньше, чем 1 вторая, минус 1 делить на 2 в степень длина R.
Давайте я это прям тут напишу.
То есть это меньше либо равно, чем 1 вторая, минус 1 делить на 2 в степень длина R.
Соответственно, здесь тоже, когда я пишу больше нуля,
то тут тоже на самом деле будет больше либо равно, чем 1 делить на 2 в степень длина R.
Соответственно, вот этот экспоненциальный разрыв у нас есть.
То есть тут, хотя все выглядит непрерывно, на самом деле все дискретно.
И, соответственно, есть какой-то маленький разрыв.
Дело в том, что нужно этот маленький разрыв передвинуть как бы с нуля на 1 вторую.
Да.
Да, да, да, совершенно верно.
Совершенно верно.
Для фиксированного X, то есть для любого фиксированного X,
среднее время по случайным битам должно быть меньше, опять же, фиксированного полинома длины этого X.
Вот.
Ну вот, соответственно, вот это вот NP вложено в PP.
Значит, это делается так.
Ну а смотрите, я тут еще, ну не знаю, специально или не специально,
но я сделал так, что здесь как бы в другую сторону получается,
что здесь у меня одна вторая вот тут вот, а здесь получается ноль вот здесь вот.
Но это неважно.
Сейчас я сразу покажу, как можно как бы сюда и сюда перекидывать.
Вот.
Значит, давайте пусть там скажем V от XS, значит, это машина из NP.
Машина для языка A из NP.
Вот.
Тогда можно сделать вот что.
Значит, M от XR, значит, длина R будет длина S плюс 1.
Так, значит, это будет вот что.
Значит, это будет ноль, если R состоит из всех нулей.
Значит, это будет один, если R начинается с нуля, но содержит единицу.
Давайте я так условно напишу, это ноль, значит, а здесь где-то есть один.
Вот.
И это будет V от XS, значит, если, ой, если R, это 1S.
То есть, если R начинается с единицы, то мы эту единицу откусываем, все что осталось, значит, подставляем в качестве сертификата в машину.
Вот.
А что у нас получается?
У нас получается, что у нас почти одна вторая единица уже получилась за счет вот этих вот.
За счет этих получилась еще не совсем одна вторая.
Одна вторая минус одно исключение.
Вот.
И если вот здесь есть единица, то она с этой исключения добавит, да, заменит как бы вместо этого исключения будет единица вот отсюда.
И уже будет больше либо вот одной второй.
Вот.
Если же все S не подходят, да, то у нас здесь будут нули, еще здесь один ноль, поэтому единица строго меньше, чем одна вторая.
Нет?
Ну давайте разберемся уже, видимо, больше ничего не успеем, все равно.
Ну, значит, смотрите, у нас S было случайным.
Ну, машина не терминирована, но, значит, смотрите, если...
Давайте подробно, что, значит, если...
Если X лежит в A, то тогда существует S, значит, такое, что V от XS равно единице.
Тогда давайте местную вероятность посчитаем количество.
Значит, тогда количество таких R, что M от XR равно единице, будет больше либо равно, значит, больше, чем 2 в степени длина S минус 1.
И это которые вот отсюда берутся.
Да, если первый бит ноль, а дальше где-то есть единица.
Таких будет 2 в степени длина S минус 1.
И еще плюс один.
Да, плюс один, который из вот этого S.
То есть вот это это R, который из всех нулей, значит, а это будет R, который 1S.
Ну а это будет равняться 2 в степени длина S, то есть доля будет хотя бы одна вторая.
Может, длина R будет в два раза больше.
Длина R на один больше, а 2 в степени будет в два раза больше.
Да, поэтому так одна вторая получилась.
Вот, это если X лежит в A, а если X не лежит в A, то вот этой вот добавки точно нет.
И поэтому будет строго меньше, чем одна вторая.
И вот и получилось, что мы как бы порог сдвинули с нуля на одну вторую.
Дальше есть, на самом деле, еще более хитрое дело.
Избавиться от асимметрии.
Вот здесь вот.
Сделал так, чтобы здесь было строго больше одной второй, а здесь было строго меньше одной второй.
Ну это примерно так же делается.
Но прямо так не получится.
Там это немножко по-другому нужно.
Нужно добавить там минимум два новых бита.
Ну и там дальше немножко там поперекидывать, чтобы мы точно как бы сдвинули на градацию.
Вот.
И это позволяет доказать, что PP замкнута относительно дополнения.
Если здесь будет строго больше, здесь строго меньше, то от того, что мы перевернем ответ, то как бы и здесь большинство тоже перевернется.
Вот так. Ну вот.
Значит, хорошо.
Что мы в следующий раз изучать?
Ну, во-первых, все-таки обсудим про амплификацию.
Что здесь можно вместо одной второй, что еще можно написать.
На самом деле, не только любое число от 0 до единицы, но еще и некоторые функции.
Но не любые.
Вот.
И второе, что мы обсудим, это, значит, где вот это BPP находится относительно других классов.
То есть, довольно легко понять, что даже PP будет в PSP вложено.
Просто потому что на полинамиальной памяти можно просто перебирать все R и прямо посчитать вероятность и сравнить, что с чем нужно.
Вот.
Ну а сам BPP будет ниже лежать.
А именно оно будет и в PSP лежать.
То есть это можно заменить на схемы.
И будет еще лежать в полинамиальной иерархии на втором уровне.
Вот.
Ну, наверное, этого нам хватит на следующий раз.
А через две недели мы поговорим про то, нельзя ли тут все схлопнуть, вот эту вот нижнюю часть.
Вообще есть гипотеза о том, что P равняется BPP.
Да, и нижняя часть, вот это вот сейчас схлопывается.
Ну, это верхняя оценка.
Это не нижняя оценка, это верхняя оценка.
Вот.
Ну вот.
Соответственно, есть основания полагать, что можно все дорандомизировать.
И что на самом деле P равняется BPP.
Вот.
Так, хорошо.
Ладно, это все.
Спасибо за внимание.
До встречи.

Всем доброго вечера! Мы наконец-таки с вами запустились спустя 15 минут после начала пары,
поэтому давайте продолжим изучение АКУДА, и сегодня мы с вами будем говорить про две
темы. Первая тема — это сумма чисел массива, мы с вами посчитаем, а вторая задача, которую мы
сегодня с вами разберем, — это сумма на префиксе. Задачи интересные, задачи тяжелые. Более того,
я скажу, что если их правильно решать, а не теми алгоритмами, которые использовались до определенного
момента, можно получить очень сильный импакт, профит в вырешении данных задач. Мы сегодня будем
разбирать эти две задачи независимо друг от друга, но в целом, я думаю, мы с вами их успешно разберем.
Значит, что мы с вами уже умеем делать? Мы умеем складывать с вами массивы. Согласны? То есть с этим
вообще никаких проблем нет. Но в чем проблема будет заключаться задачи подсчета суммы чисел массива?
Во-первых, ломается кэш. Во-вторых, это массивное взаимодействие между всеми элементами массива.
Вы прекрасно понимаете, что нам нужно делать огромное количество взаимодействия между соседними
элементами, потому что нам необходимо сложить все элементы в массивы. Вот, и, значит, ну, собственно,
нам надо понять, как решать эту задачу. Значит, в чем заключаются задачи редакшн? Представьте себе,
что у нас есть задача массива А, и есть некоторая коммутативно-ассоциативная операция типа плюс с
нейтральным элементом. Вот, и нам необходимо посчитать значение всех элементов, значение операции,
применимо ко всем элементам нашего массива. В данном случае мы будем считать сумму чисел массива.
Значит, давайте разберем классическое решение. Классическое решение это у нас есть n операции
сложения и один поток. Да, кажется, ничего сложного в этом нет.
Кориком пробежались. Как бы мы решали задачу, если бы у нас был бы MPI?
Да, смотрите, мы бы разделили наш элемент массива на отрезки и запустили бы
вычисления элементов на отрез. То есть у нас получается, наберем наш массив,
ой, черным по черному, наберем наш массив и его раз, два, три, четыре, на ц кусков бьем.
В каждом куске складываем элементы, потом складываем все элементы между собой. Спрашиваются,
мы получаем асимптотику n делить на ц плюс ц.
То есть мы каждый из элементов вычисляем параллельно, плюс еще ц раз вычисляем последовательно.
Ну, может быть, там еще каким-то образом. Вопрос, который хочется задать. В куде будет ц равно 4352?
Или есть какой-то другой подвох, который нам надо будет решать?
А, это количество кудоядер в RTX 2080i, который у нас стоит на кластере. Вряд ли. Почему?
Да, смотрите, здесь у нас будет огромное количество элементов взаимодействия. То есть здесь у нас
с вами как разбиты элементы между собой. Каждый кусок, как считается, параллельно или последовательно
элементы считают? Последовательно, да. Помните, мы в прошлый раз с вами говорили про проблему кэш-линий,
что на видеокарте так считать нельзя. Да, мы, кстати, по-моему, на семинарах с нашей группы сделали
сложение по 8 элементов массива. Замедление было в 4 раза. По 16 элементов начали складывать последовательно.
У нас, по-моему, замедление получилось в 16 раз. Нет, чем просто нормальным лаяутом это все сделать?
Здесь не нормальный лаяут. Какой у нас нормальный лаяут в элементах мы должны соблюдать? Мы должны
складывать элементы в диапазоне с чем? Размер блока. И давайте попробуем сделать вот такую первую
идею нашего решения и обсудить ее. Значит, у нас с вами будет с потоков. С потоков. И каждый поток
возьмет свои собственные элементы. 0, c, 2, c, 1, c минус 1, 2, ой, c плюс 1, 2, c плюс 1, 3, c плюс 1 и
так далее. Ну и остальные элементы будут складывать между собой. И параллельно мы будем складывать
элементы вот здесь. В итоге у нас получится выходной массив размера c, в котором мы получим
результат. То есть здесь одна сумма, сумма 0, здесь сумма 1, здесь сумма 2. Кажется, складывать элементы
будем с вами последовательно. В чем проблема?
Ну да, здесь есть еще одна проблема. Не знаю, увидите вы или нет. А вы знаете c для каждой
видеокарты? Нет, c неизвестно для каждой видеокарты. И в каких-то видеокартах у нас c имеет, значит,
кратно 64 в каких-то видеокартах, да, кратно 128. Потому что здесь у нас используется 4 варпшедуллера на 1 см,
а здесь у нас используется 2 варпшедуллера на 1 стриминг мультипроцессор. И ладно, вы подгадаете
даже константу c, но при этом вам нужно будет дергать код на видеокарте, чтобы узнавать эту
константу c. Это неудобно. Раз. А второе, что вам нужно будет подбирать, даже если вы подбираете
элементы не c, а чуть больше, чтобы выглядеть код грамотно видеокарте, то вы по факту сделаете
следующее. Вы сделаете паддинг даже до размера блока. И тут математика процесса сильно усложняется,
потому что невозможно правильно посчитать, сколько вам этих строчек алоцировать. То есть,
если у вас 68 стриминг мультипроцессоров по 64 элемента, все равно, скорее всего,
вам придется добивать это до размера блока порядка 256. То есть, получается 256 на 68 ячеек
массива вы должны обрабатывать параллельно. То есть, физическая абстракция, это, казалось,
выглядит хорошо, но когда мы переходим в логическую абстракцию, размер блока оптимальный должен
быть как можно больше, поэтому здесь уже получается в параллель у вас получается массив не из 64 на 68
элементов, а уже 256 на 68 элементов. Чтобы это логически выглядело в коде видеокарте.
Ну вот, а симптотика-то, кажется, хорошая, но и, в принципе, так можно делать. И первое как раз
одно из заданий, это будет как раз посчитать сумму чисел вот таким образом. То есть, вы берете,
говорите, что считаете до размеров блока, а потом их агрегируете. Вот здесь используете
сумму. Вот это первый способ, который можно использовать, но он может выстреливать в ногу,
и он является неэффективным. Вот это понятно. Что, вопрос какой-то?
Вот. То есть, агрегируем по элементам, потом складываем все элементы между собой. Поэтому давайте
мы немножечко отойдем от этой задачи и решим другую задачу. Мы можем, второй способ,
которым мы можем с вами решить задачу, это параллелизация этого процесса. Давайте я расскажу
его тоже на слайде. Значит, смотрите, что вы делаете. Вы бьете массив на набор блоков.
В каждом блоке считаете сумму. После этого вы считаете сумму сумм блоков. После этого считаете
сумму сумм блоков. И повторяете это до победного. Ну да, то есть, смотрите, здесь у нас размер
массива N, здесь у нас размер массива N делить на блок size, здесь у нас получается размер массива N
делить на блок size в квадрате и так далее. Вот относительно этого, оно отличается тем, что,
во-первых, порядок вычисления элементов другой. Во-вторых, для того, чтобы layout аккуратно работал,
C должен быть кратным размером блока, иначе это работать не будет. И для этого нужно как раз
сделать padding. То есть, мы берем не C, а берем по факту, так сказать, здесь вот SM размер на блок size.
Да? Каждый столбец. Да.
Вот там и суть.
В смысле? А потому что, я не знаю, но в семинарах рассматривали этот пример, что когда вы берете и
одному потоку, закидываете несколько последних элементов подряд для сложения, то второму несколько
подряд, третьему несколько подряд и так далее. У вас скорость программы уменьшается в количество раз,
пропорциональное тому, сколько элементов вы закинули. То есть, вы считаете по четыре элемента подряд,
у вас время программы замедляется в два раза. Восемь подряд начинаете складывать, у вас время,
по сравнению с этим layout уменьшается в восемь, в четыре раза. В шестнадцать последних у вас размер
программы уменьшается в 16 раз, замедляется. Кошли не ломаются. Наоборот, если мы считаем последним,
то у нас кошли не ломаются. Не-не-не, смотрите, еще раз. У нас есть элементы от нуля до 32. Мы говорим
следующее, что если нулевой элемент взял нулевой поток, то шестнадцатый поток получит шестнадцатый
элемент. Двадцать восьмой поток получит двадцать восьмой элемент и так далее. Не, ну вот так. То есть,
это оптимально. Если мы берем последовательно, то у нас происходит боль, потому что у нас с вами
есть нулевой элемент, первый, второй. И вот у нас нулевой поток начинает складывать. У него идет
сначала сложение с нулевым, потом с первым, потом со вторым, потом с третьим и так далее. У нас
получается, что нулевой поток берет значение у первого элемента массива, в то время, как у первого
потока, он должен взять элемент под номером, получается пятый. И в итоге нулевой элемент берет
первый, первый берет пятый, а первый, по идее, с учетом того, как у нас кашление выстроено,
должна взять второй элемент массива. Кашление ломается. Да, да, он пошел в другую линию и у него
да, да, да. Что? Значит, мы говорим, что давайте разобьем нашу матрицу на количество элементов
массива, на вот такое количество элементов. То есть, количество стримов мультипроцессоров на
размер нашего блока. И тогда говорим следующее, что давайте это назовем P. Тогда мы вот сюда,
по первой строке, располагаем. Каждый поток будет нумеровать вот эти все элементы. То есть, у нас
получается нулевой поток берет нулевой элемент, первый, первый и так далее, до P-1. Потоки именно,
некуда ядро. Потом следующие возьмет элементы с P, P плюс один, P плюс два, и так далее. Потом
два P, два P плюс один, и так далее. То есть, у нас получается нулевой поток будет складывать
элементы 0, P, два P и так далее, первый поток будет складывать элементы один P плюс один, два P плюс один,
три P плюс один и так далее. Четвертый параллельно, параллельно- параллельно, то есть они
вот будут идти параллельно сверху-вниз и складывать строки. Точнее, складывать себя, столбцы,
склапывать. Вот это один способ. Второй из способов — это
площное перемножение, точнее, площное сложение элементов.
Есть еще третий способ, и, кстати, как ни странно,
он работает очень быстро. Вы не поверите, как он выглядит.
Еще один способ есть. Вы не поверите. Офигенный способ.
Вы вычисляете тит, и дальше пишется следующее, особенно
когда вы считаете сумму чисел в блоке. И дальше вы пишете
следующее, и птит меньше, чем n, аут блок idxx. Так, стоп.
А? Не-не-не, мы хотим выходным массивом аут иметь сумму
чисел в блоке, образно говоря. Вы говорите следующее.
Да, вообще можно даже проще написать. Вы пишете просто
так. Вы выделяете какой-нибудь массив аут и пишете следующее.
Можете, кстати, замерить по бычмаркам. Мы просто начали
сгружать все в один элемент. Вот, оказывается, что если
у вас есть элемент для каждого блока отдельно, вот здесь
вот, то есть у вас аут от блока idxx, то это будет работать
намного быстрее, чем все первые реализации, которые
мы с вами сегодня посмотрим. TIT – это номер потока внутри
в нашей сетке. N – это количество элементов в вашей массиве.
Atomic Ed – это атомарная операция сложения элементов, объявления
элементов. А? Ну нет, не один. То есть каждый поток
у нас в видеокарте – это отдельная сущность. Правда,
один поток, одно куда ядро может выполнять несколько
поток. Так, сейчас зарядник подключу. Вот, и мы с вами
сейчас будем решать интересную задачу, а именно считать
сумму чисел внутри блока. Чего? Скип. А это означает,
что мы выделили, что размер массива наш не делится
на размер блока. То есть у нас в массиве допустим
258 элементов, а всего мы выделили 512 потоков. То есть у нас
получается потоки с 259 под 512 просто ничего не делают.
Потому что если они начнут что-то делать, у них будет
undefined behavior. В хорошем случае и секвел в плохом случае.
Ну тогда мы регламентируем количество блока. Приношу
извини, зарядник. То есть у нас 2000 элементов в массиве,
тогда мы говорим следующее. 2000 элементов. Не, количество
потоков мы сами выделяем. Мы можем сказать, что размер
блока 512. Тогда сколько блоков нам надо выделить? 4000
или 4000. Тогда у нас trade ID, вот этот тит может принимать
значение элементов от 0 до 2047. То есть это мы сами
задаем. Входим. Там есть максимальный размер. 2х32 хватит? 2х31.
Наверное хватит. Про вот этот, что он на самом деле
быстрее, чем те способы, которые мы сначала посмотрели.
Впрочем, смотря на то, что мы пишем в один элемент.
Ну да, просто синхронизация будет выполнена эффективно.
Так, решение задачи. Давайте рассмотрим первое решение
задачи и мы докажем, что каждый блок можно обработать
за время равной логарифму от блок size. У нас будет вот
такое вот пирамидальное сложение. Здесь нулевой
элемент отвечает за сложение нулевого элемента и первого.
Второй за сложение второго и третьего. Четвертый за
сложение четвертого и пятого и так далее. И всего у нас
таких циклов будет n поделить на блок size. Ой, логарифм
от блок size. То есть у нас асимптотика будет с вами в следующее.
Вот это вот все. Ох, это получается. Смотрите, за логарифм блок
size мы с вами умеем превращать это все из n в n поделить на
блок size. То есть уменьшать количество действий вот
таким образом. Вот таким вот способом. А количество
операций здесь какое? Здесь надо считать на самом деле.
Это сложно. Но можно здесь достигнуть n на c, но в общем
возникает здесь есть какие-то проблемы. Давайте как раз мы их
разберем и возьмем следующую вещь, что положим, что у нас
размер 32 бита, 32 элемента, это нам сейчас будет важно.
Значит, понятно ли вот картинка, так как складываем элемента?
Н это общий массив. Нет, в этом массиве блок size
элементов. То есть наша задача сейчас ввелась в то, что вместо
того, чтобы считать сумму всех чисел в массиве, давайте
посчитаем сумму чисел в блоке. Каждый блок мы будем
запускать параллельно. Вот. И утверждение, что мы вот
эту вот штуку можем посчитать за n делить, за от логарифм
блок size и террации. Количество действий, которое мы при этом
осуществим, тут считается очень сложно. Ну, не сложно,
но не поверьте, оно будет порядка от n по всем элементам
массива. Не-не-не, тут фишка в том, что здесь асимптотика
будет порядка именно n на логарифм блок size для всего.
То есть вам не сильно поможет, что вы это распараллели.
То есть эффекта от распараллели почти не будет. А? 512. 2000,
да. Ну, можно округлить верхнюю сторону. Да, можно считать,
что она 2048. Почему это так? Давайте разберем. Здесь
нам нужна как раз операция простаивания варпа. Напомним,
что мы выполняем операцию в варпе только в том случае,
если в этом варпе есть хотя бы один элемент. Да, один
trade ID, который работает внутри этого варпа. А теперь давайте
посчитаем, что у нас проходит с варпами. Смотрите,
на первом шаге у нас взаимодействуют все варпы, потому
что у нас сложение идет по четным индексам. То есть у
нас нулевой элемент массива, первый элемент массива,
второй, ой, нулевой, второй, четвертый занимается
сложением. В итоге у нас 8 варпов всего, и в каждом
из них есть четный элемент. Раз, раз, раз, раз, раз, раз.
Раз, раз, раз, раз, раз. То есть у нас все варпы задействованы.
Но при этом сколько у нас по факту элементов складывается?
Только половина. У нас складывается только половина
элементов, хотя несмотря на это мы простаиваем все варпы.
То есть у нас уже этим точку увеличивается два раза
от оптимальной. Повторяем дальше. Теперь складывается
каждый четвертый элемент, и все равно количество варпов
у нас восемь. Еще раз, восемь, еще раз, восемь, восемь,
дальше сколько будет? Четыре, да, и один. Всего у
нас, смотрите, на сложение 256 элементов было дернуто
47 варпов. Оптимально? Нет, не оптимально. Почему?
Потому что у нас как минимум здесь у нас простаивало половина,
здесь у нас простаивало три четверти, здесь семь восьмых
и так далее. Мы практически потеряли все преимущество.
Вопрос, как вернуть это преимущество?
Да, как?
Ну да, смотрите, что у нас получается, что у нас эта
структура не сконденсирована, если объяснить. То есть
неплохо было бы, чтобы каждый варп, то есть все потоки
внутри одного варпа вычисляли сумму. То есть у нас получается
следующее, что у нас нулевой элемент складывал сумму
двух, дальше второй элемент массива складывал сумму
вот этих двух, четвертый элемент массива складывал
вот эти два и так далее. Вопрос, вот эту вот сумму
какой элемент мог начать складывать?
А вот эту сумму какой элемент мог начать складывать?
Второй. Так, берем, вот эту всю гармошку сжимаем.
Смотрите, что происходит. У нас меняется нумерация
поток. То есть теперь нулевой поток складывает первые
два элемента, первый, второй, третий, третий, четвертый
и так далее. Вопрос, как вернуть это преимущество?
Нулевой поток складывает первые два элемента, первый
поток складывает дальше элементы, второй поток складывает
еще два элемента и так далее.
Так, что у нас происходит с количеством операций?
Так, смотрите, опять же разбираемся, что с варпами.
На первом шаге теперь взаимодействует только четыре
варпа, потому что половина потоков у нас просто
простаивает. На втором шаге сколько будет?
Два. То есть у нас варп с 64 по 96 поток отключается.
Дальше сколько?
Один. Все, мы находимся в пределах одного варпа.
И дальше один, один, один, один, один.
Ура!
А числа в кружочках?
Там числа в кружочках были 0, 2, 4.
Здесь числа в кружочках 0, 1, 2.
Так что у нас вплюснулось все внутри одного потока
и кажется все замечательно.
Количество варпов, которые используется.
Что такое варп?
Это по соответственности 32 потоков.
Говорим, что у нас варп работает,
если хотя бы один поток внутри варпа
задействуется при вычислении.
Вот. Значит, смотрите, какие четыре варпа
задействуются в первом шаге.
Первый варп, это от 0 до 31.
Второй от 32 до 63.
Третий от 64 до 96.
То есть первый варп 0, 31.
Потом 32, 63.
54, 95.
И 96, 127.
Все, четыре варпа.
Не, блок-сайз 256 у нас.
Это количество элементов,
которые задействуют первые ячейки при сложении.
Да, мы каждый второй складываем.
Да, то есть мы 128 потоков сгруппировали вместе.
Это четыре варпа.
Да, да.
Но главное, чтобы они были расположены последовательно.
Потому что если они не расположены последовательно,
то у нас получается такое решето.
Где половина варпов работает только.
И вот.
И вот.
И вот.
И вот.
И вот.
Да ладно, меньшее количество варпов задействуется
для вычисления операций.
То есть наоборот, хорошо.
Меньше тактов процессора будут.
То есть мы утверждаем, что нам нужно
задействовать 12 варпов
для того, чтобы посчитать сумму
всех чисел в блоке.
Там было 48 варп-операций.
Четыре раза лучше стало.
Это никто не гарантирует.
Внутри одного потока, скорее всего, даже нет.
Понятно?
Ну, математика просто.
Это был наивный алгоритм.
Совсем наивный.
Вот такой.
Не, на самом деле у этого алгоритма есть одно преимущество,
которое мы сломали
во второй итерации нашего алгоритма.
Давайте узнаем.
То есть смотрите.
Кажется, что этот алгоритм сильно хуже,
но вот этот алгоритм, кажется, что
лучше.
Да?
А теперь смотрите.
Ну, это не очень тоже реализация.
Смотрите, какой подвох.
Рассмотрим.
На какой-то, на четвертой стадии,
у нас возникнет следующая вещь,
что у нас берет нулевой поток,
складывает элементы номер ноль и шестнадцать
и кладет их в элемент.
То есть здесь у нас получается сумма
с нулевого по пятнадцатый,
здесь у него сумма
шестнадцатого по тридцать первый.
Тогда первый поток,
вы смотрите, складывает,
какой элемент.
Он возьмет значение элемента
из тридцать второго варпа
и из сорок восьмого варпа,
ой, с сорок восьмого потока.
Потому что здесь у нас в вами
элементы суммы с тридцать второго по сорок седьмой
и с сорок восьмого по шестьдесят третий
элементы.
Причем, если посмотреть
на операцию записи,
куда они будут писать,
то вот этот вот товарищ
будет записывать все,
что он будет писать.
Вот этот вот товарищ будет записывать
все в тридцать второй элемент массива,
а вот этот будет записывать
все в нулевой элемент массива.
А теперь смотрите, в чем особенность.
Это особенность заключается в том,
что все локальные операции мы осуществляем
с разделяемой памятью,
которой у нас модификатор shared был.
То есть это все происходит
на уровне или одного кша.
И здесь происходит
немножко другой фактор,
что у нас два потока,
раньше они пытались написать в разные линии
и ломали кашлению,
а здесь происходит еще хуже,
потому что у нас два потока
в одном варпе, подчеркну,
они пытаются написать
один и тот же элемент
по модулю 32.
То есть в итоге у нас происходит
вот такой интересный кейс.
Смотрите, у нас
нулевой поток
пытается записать информацию
в кашлению,
в нулевой элемент кашлении.
Что пытается делать первый поток?
Первый поток пишет, типа, кажется,
в 32 элемент,
но по факту
он пишет
в нулевой элемент кашлении
тоже.
Вот это вот понятие,
которое у нас здесь возникло,
это понятие банк.
То есть это по факту источник данных,
через который идет запись
в разделяемую память.
И смотрите, что происходит,
у нас вот этот элемент,
он начинает писать тот же самый поток,
тот же самый банк,
то есть ту же самую исходную точку.
В итоге у нас получается
две операции записи,
в одном и том же варпе,
записывают в одну и ту же банку.
В одном ассемблерном такте.
Ну, ассемблерная операция.
32, но
это особенность видеокарты,
что когда мы пишем в разделяемую память,
мы пишем на самом деле не
в элементы отдельности по массиву,
а мы делаем это через такую абстракцию
под названием банк.
То есть они записываются
через кэш-линию.
А в кэш-линии у нас
наполнено 32 элемента.
Да, по итогу будет записываться 32,
но по факту это идет...
Давайте я немножко перерисую картинку,
чтобы было понятно.
Значит, смотрите, вот у нас
нулевой элемент массива, вот у нас
они оба друг другу пишут, значит,
здесь они пишут нулевой элемент массива,
а здесь они пишут 32 элемента.
Кажется, все честно.
Но, поскольку мы работаем
с вами в выразделяемой памяти,
то здесь у нас возникает вот такая вот страшная
красная штука, называется банка.
И запись происходит через банк.
Внутри одного варпа,
почеркну.
Значит, что у нас происходит? У нас нулевой элемент
собирается записать вот сюда, в нулевой элемент.
В банке 32 элемента.
И записывает это сюда просто.
Что же делает первый элемент?
Первый элемент тоже должен записать именно
через вот эту вот абстракцию.
Он идет вот сюда
и записывает
это в 32 элемент.
Ручка полетела.
То есть у нас два потока
в одной ассеблерной инструкции
проходят через один и тот же
ну и точку синхронизации.
По факту можно сказать, что это Mutex
на запись.
Я знакомый с понятием Mutex.
Это блокировка.
То есть по факту в этот элемент
может зайти только один элемент
из варпа.
Но такого не случилось.
Хотя там тоже есть такое.
Там, например, если мы берем
да, то есть смотрите,
если у нас идет нулевой элемент
и 16, то история
ровно повторяется.
Потому что они опять же будут писать
в нулевую банку. Нулевой будет писать сюда
и вот здесь.
И вот здесь.
И вот здесь.
И вот здесь.
Нулевой банку. Нулевой будет писать сюда.
Нулевой.
А 16 тоже обратится вот сюда
и будет писать 32 элемент массива.
Не-не-не. У нас, смотрите, операция
как происходит.
У нас, типа, говорится, что
значит, что делает
нулевой поток. Он берет, считает
0 плюс 1 и записывает нулевой поток.
16 поток считает
32 и 33 элемента
и записывает результат 32 поток.
Это если мы берем два последних
элемента, складываем.
Та же самая проблема.
То есть по факту у нас половине случаев
наш параллельный доступ
превращается в последованный.
Несмотря на то, что даже этот способ будет быстрее,
чем предыдущий.
А?
А почему здесь фигура числа 32?
Потому что так устроено кашление
на видеокарте. Размер кашления равен
размер варпа.
Для эффективной записи.
А?
Да. Ну, пока они придумали
видеокарту, где размер варпа 64
элемента.
Вот.
В итоге у нас возникает каш...
так называем
банконфликт, извините.
Это поведение, мы разделяем
память, когда два потока внутри одного варпа
записывают данные внутри разных кашлений
по одному индоксу.
Вот. Я как раз нарисовал картинку,
как это выглядит.
Цель программистов на видеокарте
разрешить как можно больше банконфликтов.
То есть написать алгоритм,
который не зависит от банконфликтов.
Далеко не для всех алгоритмов это возможно.
Вот.
Собственно, вот он конфликт.
Как решать?
Да. Ну, вот смотрите.
Мы хотим, чтобы с вами...
Давайте немножко подумаем. Мы хотим, чтобы
у нас с вами два элемента вычисляли
т.е. у нас
с вами
нулевой подход. Вот.
Представим себе, что вот у нас с вами есть
секунду.
Вот у нас есть с вами
массив из 256 элементов.
Мы хотим, чтобы
на первую операцию считало половину
элементов суммы. Первая половина.
Вопрос. Как реализовать операцию
сложения, чтобы это работало?
Чтобы нулевой элемент занимался сложением,
при этом
по факту он записал в себе
сумму именно нулевого элемента.
Смотрите, фишка.
Давайте считать, что
каждый поток будет складывать
в себе свой собственный элемент.
Тогда какой второй элемент он будет
складывать?
Да.
Раз.
Два.
Минус один.
И вот это уже будет
эффективной реализацией.
То есть мы массив бьем пополам
и по факту параллельно складываем.
Да.
Да, любой.
Да.
Нет, не будет, потому что каждый элемент
в массиве записывает,
каждый поток записывает именно свою ячейку.
Все чисто.
Там ифы ставятся и все.
Там ифы ставятся.
Ну если не делаем,
то не складываем. Вот.
Вот такая вот красивая реализация получается.
Хорошо.
Значит,
как решить итоговую задачу здесь?
Здесь говорится, что можно посчитать сумму
в блоке, посчитать сумму между
блоками и повторить
пока не останется один блок.
Но знаете,
что я вам скажу?
Этот алгоритм устарел.
Вот этот алгоритм устарел.
То есть он был,
он работал хорошо,
но там до десятых годов.
Сейчас уже
есть такая другая интересная особенность,
которая заключается в том,
что, а давайте-ка подумаем
с вами о том, а что же такое
видеокарта?
То есть мы с вами уже много раз сказали,
что 32 элемента
в варпе
работают с одной
ассемблерной инструкцией.
Мы пока что работали
с ассемблерными инструкциями
такими параллельными,
что у нас все потоки
внутри варпан вычисляют свой собственный элемент массива.
Ну и работают с ним.
Но вопрос,
можно ли создать инструкции в ассемблере
видеокарты такие,
что они будут каким-то образом
вычислять взаимодействие между собой,
между элементами внутри одного варпа.
То есть сказать, допустим, возьми,
вот ты пятый поток,
пожалуйста, сложи его
с девятым элементом этого варпа.
Ты сложи пятый элемент варпа
с девятым элементом варпа.
Да!
Сейчас мы откроем презентацию с вами.
Так, шуфлы, инструкция называется
это все дело.
Инструкция называется shuffledown.
Давайте как раз статью
от NVIDIA рассмотрим.
Смотрите какая красота.
Давайте сделаем следующее.
Выполним вот такой вот код.
Что такое shuffled XOR?
Здесь есть разная операция.
Вот смотрите.
Вот пример на слайде.
То есть у нас есть 8 элементов,
считаем, в варпе.
И тогда мы можем сказать,
пожалуйста, а какие элементы
ты хочешь сложить?
Что делает функция shuffledown?
Она берет
индекс потока,
который у нас есть
и указывает,
что, допустим,
четвертый поток, который у нас есть в варпе,
значение,
которое у нас здесь получается,
оно должно быть в нулевом элементе.
Когда мы запрашиваем shuffledown
у нулевого элемента варпа,
он возвращает значение,
которое находится в четвертом элементе варпа.
У первого элемента
из пятого.
Удает пятый.
Да, мы это только что делали,
но мы раньше делали это
на уровне кода.
То есть мы там написали,
возьми элемент массива,
сделай плюс равно чему-то.
А здесь есть особенная инструкция,
которая делает это внутри варпа.
Классно, да?
А теперь магия.
И мы повторяем это
до тех пор, пока мы находимся
внутри одного варпа.
Теперь математика процесса,
которая заключается в том,
что, вы не поверите,
максимальный размер блока у нас 1024.
MaxBlockSize
MaxBlockSize
Чем число 1024
известно?
Это степень двойки какая?
А можно ли это еще
как-то по-другому представить?
Да, это 32 в квадрате.
Интересно.
А что это значит,
что у нас 32 в квадрате?
Это значит,
что в нашем блоке 32 варпа.
Поэтому давайте предположим,
что у нас с вами есть 32 варпа.
В каждом из них
внутри варпа мы запустим
вот эту пирамидку,
которую сверху запустили.
Это на уровне варпа.
Потом мы все эти элементы
сагрегируем в один элемент массива
и сделаем еще одну такую же пирамидку.
Но мы сделаем это уже
на инструкциях именно внутри варпа.
Есть разница на самом деле.
На обычном коде можно сильно обжечься.
Осемблеры инструкция быстрее.
Мы явно указываем,
что используем инструкцию осемблера для этого.
Но мы не будем использовать
инструкцию осемблера.
Мы будем использовать
инструкцию осемблера для этого.
Она берет указатель
у элемента down
и запрашивает значение
по этому указателю
на 4 потока в варпе выше.
То есть у нас получается,
смотрите, допустим,
у нас были в варпе значения
в регистрах.
В0, В1, В2, В3.
В4, В5.
В6, В7.
В8, В9.
В10, В11.
В2, В31.
Значит, что сделает shuffledown?
Отshuffledown
если мы его запросим
из В2 и 4.
Он нам вернет значение,
которое находится в регистре В6.
Мы сделали бы то же самое,
но суть в том,
что, помните, я говорил,
что как только мы используем
операцию внутри какого-то
shared памяти,
нам нужно писать синхронизацию.
То есть нам нужно
указывать везде sync threads.
На каждую операцию
записи вы разделяем мы память,
иначе у нас возникнет гонка данных.
То есть у нас
это бы выглядело следующим образом,
что, пожалуйста,
s и t
плюс равно
s и плюс четыре
образно говоря.
И тут бы мы ставили всегда sync threads,
чтобы у нас операции были синхронизированы.
Вот, здесь мы можем отключить
эту операцию синхронизации.
Потому что это все
делается внутри одного варпа.
То есть количество синхронизации уменьшится на 5.
Да, да.
Потому что это делается внутри одного варпа,
это одна инструкция сэмблера.
Да.
Да, ну каждый размером с ворп, да.
Да, да, да.
Вот такая интересная штука.
Какая интересная штука.
Какой интересный алгоритм.
Понятно суть этого алгоритма?
Хорошо.
Собственно, это то,
к чему люди пришли.
Используя вот этот элемент.
Точнее, вот эту операцию.
Вот, а теперь перейдем ко второй части,
если вы не против.
Это вычление суммы на префексе.
Суть этой штуки заключается в том,
что нам нужно посчитать
следующую вещь.
У нас есть элемент A0, A1.
A enter.
И нам нужно посчитать элементы вот такие.
Обычно две разных задачи бывают.
Бывает inclusive scan,
бывает задача exclusive scan.
Значит, в задачи inclusive
наша цель посчитать
A0,
A0 плюс A1,
A0 плюс A1 плюс A2,
ну и сумму
A it,
A it от 0 до n.
Значит, что делает задача exclusive?
Я, честно, могу перепутать
эти термина.
0, A0,
A0 плюс A1 и то же самое.
Нравится или нет?
Нравится или нет?
За заключением одного элемента.
Просто про реализацию поговорить.
Нам надо решить обратную задачу.
Если задачу вычления сумм чисел массива
мы можем считать каким-то образом
параллельно,
но то с этой задачей
как дела обстоят?
Вы когда-нибудь писали эту задачу параллельно?
Не, но ее можно написать параллельно
и не поверите за сколько.
Вы ее можете записать параллельно
за n лог n.
Приблизительно симптотика у вас будет.
Но с учетом того, что мы
здесь будем приблизительно делить
на C,
то это окажется быстрее,
и считать это за n.
Поэтому давайте рассмотрим
решение задачи.
Вопрос, который я хочу задать,
знаете ли вы такую структуру данных,
как дерево Фенвика?
Дерево Фенвика.
Оно здесь используется.
Да, да, да, да.
Значит смотрите, есть массив,
значит мы хотим
посчитать такие суммы.
Первый алгоритм,
который является такой
на самом деле достаточно эффективным,
и сейчас он используется, и на семинарах
вы это тоже посмотрите,
мы считаем скан на блоке.
Смотрите, что мы делаем.
Мы складываем элементы с 0 по 7,
ну и дальше складываем себя
со следующим, потом себя
через двойки, себя через 4,
себя через 8.
То есть такая пирамидка.
Она заключается в том,
что образно говоря,
чтобы посчитать сумму
с 0 по 6 элемент,
нам нужно посчитать сумму
со 2 по 6, плюс сумму
с 0 по 1.
То есть мы разбиваем
наше число, которое необходимо,
на степени двойки, ну и дальше
раскладываем его в двоичной системе.
То есть у нас 6 в десятичной,
это 1, 1, 0 в двоичной.
Берем четыре последних
и потом еще два предпоследних.
То есть видно, как это все дело складывается.
Этот алгоритм хороший,
но сколько в нем операций осуществляется?
Отлагаем.
И несмотря на это,
этот алгоритм является оптимальным
из-за своей константа.
То есть константа у него низкая.
Но давайте попробуем
уменьшить эту константу.
И оказывается следующее,
что...
А, да, у нас еще n лог n ворп
операции, потому что мы с вами
понимаем, что каждый элемент будет затрагивать
свой ворп.
То есть видите, тут прямо массивные
вычисления идут.
Необходимая синхронизация,
еще дополнительно, потому что
когда вы записываете элементы,
вам нужно всегда ставить sync-tracks в конце,
чтобы это все работало.
Знаете, как проверить,
почему нужен sync-tracks?
Попробуйте в кодах, которые у нас есть к семинарам
закомментировать строчки sync-tracks.
Посмотрите, что получится.
А? Будет весело.
Может у кого-то были семинары,
те увидели.
Поэтому мы делаем следующую вещь.
Сделаем две стадии.
Первая мы складываем элемент 0 и 1.
Закрываем нитку вот такую, видите?
Складываем 2 и 3.
4 и 5.
Что и что 7.
И вот смотрите, на самом деле, здесь у нас получается,
если мы посмотрим внимательно на сумму чисел,
которые у нас получаются, здесь получается дерев fendvik.
Да.
И давайте как раз это посмотрим,
как это работает.
А теперь научимся складывать элементы,
имея это дерево fendvik.
То есть, а какую задачу решает дерево fendvik?
Напомните мне, пожалуйста.
Да.
Собственно, сумму чисел на припексе,
на любом подотреске.
Да, оффлайн.
Ну, нам нужна оффлайн сумма.
Ой, извините.
Вот.
Сумму чисел на любом подотреске.
Ну, понятно, что мы можем решать сумму задачи
на префиксе.
А?
Нет, это вот как раз наверху.
А нам надо...
Это то, что мы хотим получить.
А вот это то, что мы хотим получить.
А вот это то, что мы хотим получить.
Мы хотим получить пустое множество,
ноль,
эксклюзивный скан решаем.
Так.
Давайте действовать.
Так, понятно.
Понятно.
Усломалось.
Так.
Что мы делаем?
Заметьте, пожалуйста,
одну закономерность.
А она здесь есть.
Что такое 0-6?
Давайте разложим это.
0-3 плюс 4-5 плюс 6.
Вот это у нас 0-3 плюс 4-5.
Вот это 0-3 плюс 4.
Вопрос. Где у нас 0-3 находится?
Жух.
0-3 находится справа в верхнем углу,
находится справа в верхнем углу,
в левой части.
И нам его нужно распространить в правую часть.
Видно, да?
Давайте мы его отправим сюда.
А 0-7 нам нужен?
Не особо.
Поэтому мы сделаем следующее.
Мы его закроем,
точнее мы его запомним,
а здесь напишем пустое множество.
Вот.
И пока что эфемерно сложим это с нулем.
А вот это пустое множество,
оно здесь находится.
Выставим его сюда.
В итоге у нас получаются элементы.
Смотрите, пустое множество.
Ой, нет, подождите.
Словно что здесь еще нет.
0.
0-1.
2.
Пустое множество.
Здесь у нас 4.
4-5.
6.
Пустое множество.
Ой, не пустое множество, извините.
0-3.
Вот так, да?
Так, теперь давайте еще одну
штуку заметим.
Разобьем вот этот массив еще пополам.
Смотрите, фишка в чем.
Пустое множество нам нужно здесь.
0-1 нам нужно здесь.
0-1.
2.
Здесь пустое множество, 0.
Теперь смотрите, опять же то же самое
перекрестное сложение.
Здесь получаем 0-1,
здесь получаем 0-2.
Перекрестное сложение.
Пустое 0,
пустое множество, 0.
А здесь что у нас происходит?
Давайте посмотрим здесь, что происходит.
Делаем перекрестное сложение
и получаем сумму элементов 0-5.
А сюда отправляем 0-3.
Еще одно перекрестное
сложение, получаем такую вещь.
Видите, пирамидка такая
получилась красивая.
Кажется, все красиво.
Как вы думаете, в чем проблема этого алгоритма?
Он выполняется
за линию.
Сложный. А если алгоритм
сложный, то что у него большое?
Константа.
Если алгоритм сложный,
у него большая константа.
А если у него большая константа, то
скорее всего эта константа настолько большая,
что она ломает, что она работает хуже
алгоритма, который работает для логин.
Здесь это
на практике и происходит.
Вот она, вторая
стадия еще разок.
Более того, у этого алгоритма есть огромное
количество банк конфликтов.
Те самые, которые
у нас были.
Смотрите, вот даже
вот здесь есть банк конфликт.
0-й поток складывает 0-й и 1-й
и записывает результат в 1-й.
1-й поток складывает,
0-й поток потом складывает элементы 1-й и 3-й.
3-й – 7-й, 7-й – 15-й,
15-й – 31-й.
При этом 16-й поток
параллельно будет складывать 32-й
и 33-й элементы массива.
Банк конфликт,
вот он.
Как его решать?
Идея гениальная.
Не-не-не, помогает.
Так, вы верите в
эзотерику?
Нет, я серьезно говорю,
верите в всякие
эзотерика,
оккультные мистические
всякие вещи?
Хорошо.
Что эзотерического есть
в телеке?
Ну,
что-то есть в телеке.
Нет, почему говорят,
что телевизор плохо смотреть?
Вот там говорят голову
промывает мозги.
Да, бинго, 25-й кадр.
Смотрите, идея состоит в следующем.
Давайте сделаем 25-й кадр,
именно 32-й элемент каждого ворпа будем пропускать.
Ворпу,
ворпу,
ворпу,
ворпу,
мы будем пропускать.
Мы будем говорить следующее,
что у нас элементы массива,
то есть наша память существует,
но мы будем работать с этими элементами массива
следующим образом.
Вот мы записываем с вами
элементы массива с нулевого по 31-й,
потом тот элемент массива,
который по идее должен быть
на 32-м элементе, мы пропускаем,
тут самый 25-й кадр,
дальше элементы массива идут
с 32-го по 63-й,
потом опять пропуск
и так далее.
Ну не, мы просто говорим,
что когда мы обращаемся, смотрите,
в чем фишка, мы обратились
с вами допустим к первому элементу массива,
вот здесь вот.
То есть у нас операция идет следующим образом.
Раз, раз, мы сложили.
А дальше мы обратились,
элемент какой там у нас был?
16-й поток, да, у нас складывал
32-й и 33-й элемент массива.
Это элементы массива,
но если мы посмотрим индексы,
то здесь у нас будет нулевой,
здесь у нас будет первый,
здесь у нас уже будет 33-й элемент массива
и 33-й элемент массива.
Ой, наоборот.
Так, стоп, стоп, стоп, я кажется,
кажется нам уд... А, нет.
Нормально. То есть смотрите, у нас обращение
идет к 33-й и 34-й.
Если здесь мы делали запись первого элемента массива,
то здесь мы делаем запись 34-й элемент массива.
Который не находится в одной банке.
Они развелись по разным индексам
по модулю.
Все, конфликт исчерпан.
Клевая идея.
Не-не, это вот тот алгоритм,
который вот до этого
был наиболее оптимальным.
Но у него константа большая.
Вот.
То есть это модификация вот этого алгоритма,
который позволяет вот это решать
линейным способом. На семинаре посмотрите
код для эстетического удовольствия.
Но давайте вернемся к современным реалиям.
И поэтому я сейчас вам покажу
еще одну картинку.
Так, 32-ю кадр мы его
с вами обсудили.
Это так, так же, как все это складывается.
То есть мы сканируем, потом делаем
скан сканов, а дальше нам нужно
распространение суммы элементов.
Сделать обратное.
Эх, во!
Современный скан.
Современный скан.
Напоминаю, что 1024 это 32
в квадрате.
Поэтому мы можем внутри каждого скана
сделать
shuffle up sync операцию.
Ну, отсложить элементы, получаем
такую красивую. Потом, на уровне
варпов мы ставим барьер.
Перемещаем все
наши элементы в необходимые
суммы и начинаем дальше
повторять операцию. То есть первое
считает у нас сумму на префиксов внутри
каждого варпа, второе уже делает
синхронизацию между всеми варпами.
То есть вот этот элемент массива достается
всем остальным, вот этот элемент массива
складывается со всеми
дальнейшими
и так далее.
Вот он сверху, вот они
индексы.
Да, одна линия.
Где?
А!
А, это редукшн.
То есть это, допустим, мы хотели
посчитать какую-то сумму.
Ну, можно считать, это, на самом деле, картинку
просто со слайда взял.
Можно считать, что, типа, здесь
все кроме одной строки можно обрезать.
То есть вот тут провести вот
такую разделительную линию и считать, что вот это
наш элемент массива.
Не, в остальных уже не по одному.
А, подождите,
здесь секунду, сейчас.
Сейчас, математику процесса
надо понять.
Смотрите, тут идея такая,
вот про верхнюю строку точно не могу
ничего сказать. Идея такая, что
мы сначала агрегируем сумму в каждом скане.
Ну, в каждом варпе получаем
сумму на префиксах.
Ну да, варп это вот эта вертикальная полоса,
которая у нас.
Из четырех.
Да, это один варп.
Дальше мы ставим
барьер,
распределяем значение в варпах
для следующих элементов массива.
То есть говорим, что здесь у нас
идет сложение с этим,
здесь идет сложение с этим и так далее.
Распространяем их.
То есть получаем сумму на префиксах,
которые дальше можно синхронизировать
между собой
для того, чтобы посчитать сумму чисел массиве.
Ну, сумму чисел на префиксе.
То есть алгоритм трехстадейный.
Вот, я предлагаю, поскольку мы сейчас
уже немножко не бум-бум не варим,
да, попытаться
это перенести на следующий раз и
с цифриками там
разрисовать это.
Но это современная реализация, которая
бьет все остальные.
Выглядит весело.
Но работает,
кстати, реализовывать вот эту штуку
лучше, чем вот эту.
Ну, да.
А, ну, здесь для того,
чтобы пар конфликтов,
банк конфликтов избавиться.
Я же говорил, что мы брали
нулевой и первый элемент складывали,
получали первый элемент массива.
Дальше 32 и 33
складываем,
складываем это в 16 потоке.
При этом, смотрите, здесь у нас
запись идет в первый элемент массива.
То есть здесь запись у нас шла в первый элемент массива,
в райд.
А до этого мы
делали в райд в 33 элемент массива.
И в итоге нулевой и 16 поток
писали в два одинаковых потока
в барпу.
Сделали сдвиг.
Теперь вот этот 33 поток,
33 элемент массива, на самом деле,
оказался в 34.
Ну, что сдвиг на один произошел.
Вот, и в итоге
здесь мы делали запись в первый,
а здесь уже окажется запись по второй,
пишем по модулю.
Да,
даже боль.
Да, размер памяти уменьшился на 1,
32.
Ну да.
Да, ну там константы больше
для вычления этих операций возникают.

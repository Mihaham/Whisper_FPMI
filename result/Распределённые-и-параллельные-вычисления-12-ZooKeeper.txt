Сегодня наша цель — зукиппер, и нужно как-то мотивировать его появление.
В прошлый раз мы с вами говорили о том, что я рассказывал про то, что существует такая каптиария,
которая говорит, что если вы пишете распределённую систему, то вы должны выбрать три буквы из трёх,
вы должны выбрать не все возможные варианты, конечно, но вы должны, если у вас есть партишны,
они у вас, разумеется, есть. Просто потому что такова жизнь, потому что такой сбой может
случиться в сети, то вы должны выбирать, чему ваша система даёт приоритет. Она остаётся согласованной
или остаётся доступной. Не то чтобы это какая-то теорема, разумеется, это скорее просто инженерная
интуиция. Вы должны выбрать один из двух вариантов дизайна. Как ваша система будет вести себя,
когда сеть расколется на два сегмента и между сегментами связи не будет, а сами сегменты будут
рабочими внутри себя и даже могут обслуживать клиентов, которые с этими сегментами могут
поддерживать связь. Один вариант, если ваша система в таком случае готова обслуживать запросы,
готова отвечать на них и готова менять состояние, то есть система остаётся высокодоступной.
И вторая альтернатива, когда система говорит, что я не могу работать в таких условиях, я не
могу независимо принимать решения в двух сегментах сети, я должна в одной части заблокироваться,
а в другой продолжить обслуживать пользователей и давать им уже согласованный ответ. Вот оба
варианта, обе альтернативы, они по-своему разумны. Но вот если мы говорим про согласованность,
то в общем тут никаких сомнений нет. Система не может обеспечить согласованность с двух сторон
partition, поэтому выбирает одну половину, а в другой блокируется. В случае с доступностью это тоже
в некоторых случаях разумно, потому что не во всех задачах, не во всех приложениях нам вот эта самая
согласованность нужна. У нас, кажется, еще будет лекция про Кассандру и про Amazon Dynamo. Так вот,
там мотивирующий пример, который, мотивирующая задача, которая стояла перед инженером Amazon,
состояла в том, чтобы сделать корзину товаров для их сайта. Так вот, для такой задачи согласованность
не очень-то важна. В такой задачи можно вполне себе добавить товар в корзину, потом он оттуда
исчезнет, потом снова появится. Это не очень большая беда, можно было бы это пережить. А поскольку
две эти альтернативы взаимоисключающие, ну то есть понятно, что если у вас есть два сегмента сети,
между ними нет коммуникации, то вы не можете записать с одной стороны и потом прочистить с другой
половины сегмента, из другого сегмента. Так вот, если вы должны выбирать, то вот в некоторых условиях
вам разумно выбрать высокую доступность системы. То есть дизайн, в котором система доступна на
запись, при котором система доступна на запись даже в меньшей части partition. Ну просто потому,
что там тоже есть клиенты. Но это будет в следующий раз, в другой раз, а сегодня мы поговорим все же
про системы, которые выбирают согласованность. Ну и что значит согласованность? Они, видимо, выбирают,
как правило, линьализуемость. Линьализуемость — это самая высокая степень согласованности,
которая система может предоставить. Линьализуемость — это гарантия про то,
что пользователь, клиент, вообще говоря, не наблюдает распределенности системы. То есть
система, конечно, состоит из разных узлов, которые связаны проводами и которые не могут быть
полностью синхронные, но при этом пользователь может думать о системе как о таком одном отказоустойчивом,
высокодоступном компьютере. Пока в системе остается большинство узлов, пользователь эту систему,
может работать как просто с одной надежной машиной. Это очень сильная гарантия, она очень удобна
пользователю, но понятно, что есть более слабые гарантии, мы сейчас про них не очень хотим
говорить. Мы под согласованностью понимаем, ну, по крайней мере, CAP-теорема в своей строгой
интерпретации под согласованностью принимает линьализуемость. Это вот еще один повод попинать
CAP-теорема, потому что сама теорема в кавычках, она плохо формализует, что такое согласованность,
а вариантов согласованности довольно много. Вот мы под согласованностью будем понимать
линьализуемость и в прошлый раз и с субботом уже давно мы обсуждали, что вот эта самая
линьализуемость, которая говорит о том, что система ведет себя атомарно и выполняет как будто бы все
операции в некотором глобальном порядке, причем с соблюдением предшествования операции в реальном
времени, то есть с соблюдением причинности для пользователя. Так вот, система обеспечивает эту
гарантию с помощью вспомогательного примитива, который называется Atomic Broadcast. Если вы умеете
строить Atomic Broadcast, то вы можете реплицировать произвольное состояние линьализуемое, и вот
это означает согласованность для пользователей. Atomic Broadcast, напомню, это примитив коммуникации,
которая позволяет узлам в определенной системе коммуницировать друг с другом и при этом иметь
некоторый общий глобальный порядок доставки сообщений. Разумеется, сеть вам такого не
позволяет. В сети это сложная компутационная фабрика, которая связывает разные машины
огромным количеством разнообразных маршрутов, обеспечивает высокую пропускную способность
любого разреза и никакого порядка, конечно, не гарантирует. Но вот вам порядок нужен,
когда вы собираетесь решать задачу репликации. Вам удобно, если бы все команды от всех
пользователей, которые выступают в вашу систему, были бы каким-то образом упорядочен на всех
репликах. Давайте я, для тех, кто не слушал меня по субботам, быстро нарисую картинку и мы вернемся
к звукитеру. Примитив Atomic Broadcast или Total Reorder Broadcast. Это инструмент, с помощью которого
распространённую систему достигается согласованности. Если мы в CAPC-ареме выбираем
Google C, то мы неизбежно должны использовать в своем дизайне Atomic Broadcast. У вас есть,
допустим, три узла и у нас есть два клиента, которые приходят в систему со своими запросами.
Вот клиент 1 с какой-то командой 1 и клиент 2. И вот реплики системы, они должны быть у
возможности синхронны. И вот мы сейчас этого с помощью Atomic Broadcast пытаемся достичь.
Разумеется, система может быть сложнее, чем просто три реплики, в ней может быть очень много узлов,
но как правило происходит следующее. Либо ваша система это гигантская база данных или гигантская
таблица, тогда у вас просто много-много компьютеров делится на шарды. Каждый шар включает
сохранение части данных и каждый шард сам по себе является набором реплик. То есть,
можно вот опуститься до такого уровня. Ну а в некоторых системах все не так. Там есть
некоторая иерархия в смысле, ну не знаю, можно представить себе распределенную файловую систему,
с которыми вы уже работали, и вот там есть уровень данных и уровень метаданных. Ну вот уровень данных
может быть устроен одним образом, а уровень метаданных может быть устроен как раз с помощью
репликации через Atomic Broadcast, потому что там есть сложное состояние, которое мутируется
комплектно разными клиентами. Вот так или иначе согласованность где-то в таком месте достигается,
она достигается с помощью этого инструмента. А именно, когда реплика получает команду от клиента,
то она становится координатором этой команды и выполняет процедуру, которую мы назовем Atomic Broadcast.
Она иницирует широковещательную рассылку команды C2 на вот этот набор реплик. Клиент
один делает то же самое. Он иницирует Atomic Broadcast для своей команды. И Atomic Broadcast,
вот этот протокол, неизвестный нам, гарантирует, что если у нас реплика будет жить достаточно долго,
то она получит обе эти команды, причем каждая реплика получит эти команды в одном и том же
порядке. Но какая-то реплика может получить не все команды, потому что просто она откажет,
но если она будет работать достаточно долго, то она получит красную и синюю команду в одном и
том же порядке. И каждая реплика будет поддерживать копию состояния некоторого,
ну не знаю, дерево, файловая система, кусочек, таблица, и будет в одном и том же порядке
применять одни и те же апдейты. Разумеется, реплики не синхронные, но при этом они проходят
через общую серию, через одну и ту же серию изменений, серию мутаций, и этого достаточно,
чтобы поверх вот такого протокола, чтобы поверх Atomic Broadcast получили линиаризуемость. Как
только координатор отправляет, как только координатор отправив, поставок подотправил
команду пользователя, сам получит Atomic Broadcast, а это, конечно, мгновенная операция, потому что
между репликами должна быть какая-то координация довольно сложная, чтобы говорится о порядке. Так вот,
в основном, он получит эту команду сам, он применит ее к своему состоянию и может вернуть
это пользователю. Вот такой протокол по модулю того, что мы не знаем, как Atomic Broadcast реализован,
обеспечивает линиаризуемость, обеспечивает согласованность. Ну а чтобы этот Atomic Broadcast
реализовать, я в прошлый раз говорил, нам нужно решать задачу консенсуса. Задача консенсуса — это
очень простая задача, где у нас есть узлы, у них есть на входе какие-то значения, и задача этих
узлов — просто договориться об общем выборе и завершиться, сделать этот выбор. Если узел завершается,
то он должен выбирать одно из предложенных значений и должен, и все завершившиеся, сделавшие
выбор узлы должны выбирать одно и то же. Вот эти две задачи, не то чтобы эта задача сводится к
этой, они вообще позволяют по сложности, это одна и та же по сложности задача. Вот, и на практике
обе эти задачи решаются с помощью, ну как правило, промышленных системах решаются с помощью двух
алгоритмов. Это алгоритм Multipaxas и алгоритм Round. Это два похожих алгоритма, можно сказать, что это
примерно один и тот же алгоритм. Но вот они существуют, реализация деталей этих алгоритмов,
конечно, сегодня в лекцию они не могут пронести, это не наша цель. Но важно, что если вы пишете
распределённую систему, то, как правило, вы уже знаете, что вы уже выбрали в CAP теориями букву C,
вы знаете, что вы хотите достичь согласованности, то есть линия резуемости, вы это делаете по общему
рецепту с помощью реализации Atomic Broadcast, и Atomic Broadcast, как правило, вы пишете даже не голыми
руками, потому что он уже написан. Но вот есть протокол RAFT, и это, наверное, сейчас такой
стандартный протокол для большинства open source распределённых систем. Вот если вы open source,
если вы пишете open source распределённую систему в 2021 году, то, скорее всего, вы начнёте с RAFTA.
Ну, точнее, если вы будете использовать consensus в своём коде, то, скорее всего,
вы возьмёте RAFT просто потому, что протокол достаточно хорошо описан, у него очень много,
есть, во-первых, очень много спецификаций, документаций, там какие-то ториллов, но,
а кроме того, что, наверное, важно в первую очередь для разработчиков, есть open source реализации
очень высокого уровня. Причём есть вот для самых разных языков, для Go, для C++, для Rastia,
вот эти реализации, они используются вот в промышленных распределённых системах. То есть,
это не просто какой-то такой академический проект, это вот именно код, который работает,
в конце концов, в существующих системах под большой нагрузкой, и вы можете его переиспользовать.
Но, тем не менее, это не единственный способ подходить к реализации распределённых систем,
которые выбирают C и используют внутри себя только broadcast, протокол consensus. Альтернативный подход
состоит в том, чтобы, один подход, это взять библиотеку, которую этот consensus реализует,
которая решает реализовать приметивный broadcast, и встроить её в свой код. А есть совершенно
альтернативный дизайн, который был предложен, который выбрали в Google ещё в 2006 году. Идея состояла
в том, чтобы вместо того, чтобы consensus помещать, решение consensus в виде библиотеки помещать в код
вашей системы, в код ваших узлов, вместо этого можно вынести задачу консенсуса, решение задачи
консенсуса, приметив atomic broadcast в отдельный сервис. То есть, сделать consensus как сервис.
Консенсус необходим, если вы хотите достичь буквы C в каптеореме, если вы хотите получить
линеризуемость в качестве модели согласованности. Так вот, этот consensus можно вынести за пределы
ваших узлов, за пределы вашей системы, и пользоваться им как сервисом. И этот сервис с точки
зрения Google, с точки зрения Google в 2006 году, выглядит как сервис блокировок. То есть, это сервис,
в котором вы можете прийти и взять блокировку, и что ты под этой блокировкой что-то делаешь. Ну,
то есть, это такой распределённый, отказоостойчивый mutex. Насколько такая модель данных вообще
адекватна. Что такое распределённые блокировки, это отдельная история. Мы к ней сегодня придём,
довольно завысоватым путём. Но, просто сам point, что вместо того, чтобы встраивать консенсус в
код ваших узлов, вы можете использовать консенсус в виде внешнего сервиса. Ну,
аргументы Google были довольно странные, что там блокировки более привычные пользователям. На
самом деле, конечно же, это не так, потому что распределённые блокировки, как мы сегодня увидим,
они сильно отличаются от блокировок, которые вы используете по точным коде. Но, тем не менее,
вот появилась такая система. Она появилась в 2006 году, если мне память не изменяет. И спустя
несколько лет в Yahoo написали другую систему по мотивам Google-чаба. Ну, то есть, замысел был тот
же реализовать консенсус как сервис, вынести его в внешнюю систему, но реализация, точнее,
не реализация, а модель данных была выбрана другой. Ну, вот мы сейчас посмотрим на то,
какая модель данных была выбрана в этой системе. Она называется ZooKeeper. И поговорим немного про то,
как она внутри устроена, потому что внутри и на границе с клиентом, потому что это всё важно.
ZooKeeper — это система, которая принадлежит классу систем, которая называется сервис координации.
Вот в этом курсе вы обычно говорите про системы, которые хранят данные пользователя или обрабатывают
данные пользователя, и которые там масштабируются, которые отказоустойчивы. Так вот, ZooKeeper не
предназначен для того, чтобы с ним работали непосредственно пользователи. ZooKeeper не хранит
большой объем данных, он хранит маленький объем данных, и он не предназначен там для
какой-то обработки. Короче, пользователи с ним взаимодействовать не должны. ZooKeeper — это
система, которая служит таким вот кубиком, с помощью которого строятся другие распределенные
системы. То есть, если вы пишете там, не знаю, очередь сообщений, если вы пишете там очередь
задач, если вы пишете там условные какие-то мы-предьюстеры, что-то подобное, то вы в дизайне
своей системы для того, чтобы добиться согласованности, для того, чтобы решать разные задачи, которые
в системах возникают, можете использовать вот этот самый ZooKeeper. Какую модель данных он вам
предоставляет? ZooKeeper хранит в себе дерево. Дерево с узлами, и у вас есть, ну давайте на опись
сразу посмотрим, операции, которые выглядят так. Create — вы создаете путь в некотором узел по пути
в этом дереве. Вы можете этот узел удалить, вы можете проверить, существует ли он, вы можете
перечислить потомков какого-то узла. Вы можете, ну давайте пока остановимся на этом и сразу скажем,
сразу оговоримся, что несмотря на такой опи, у нас есть дерево, там есть пути, узлы, их можно создавать,
удалять и проверять, существует ли они. Так вот, это ни в коем случае не файловая система. Ну
файловая система, про файловая система вы и мы уже говорили в разных, по разным причинам. Так вот,
это не файловая система, это всего лишь иерархическая организация данных. Сами узлы в этом дереве — это
не файлы. Вообще, все данные, которые хранит зукипер, должны умещаться в одну машину. То есть,
очень небольшие данные. И отдельный узел тоже очень маленький. Размер отдельного узла может быть,
ну вот, ограничен там какими-то единицами в мегабайт. То есть, вы там можете хранить только небольшой
блок с данными. Сам зукипер никак не интерпретирует эту задачу вашего приложения. И у нас нет
операций типа append. То есть, мы не можем это точечно модифицировать узлы. У нас в API есть только
операции getData и setData, которые просто читают содержимое узла и перезаписывают содержимое узла.
Я бы сказал, что вот на зукипер, на данные в зукипере стоит смотреть, как на набор атомиков. Но вот
зукипер выполняет в распределенных системах ту же функцию, которую выполняет в многопроточном
приложении атомики. Зукипер нужен для синхронизации узлов в распределенной системе. И так же,
как у атомиков, у зукипера есть более сложные операции, чем get и set, то есть, чем просто
чтение и запись. Но об этом мы чуть позже поговорим. Главное, что не стоит думать об этом как в файловой
системе, стоит думать об этом как просто об иерархическом наборе атомиков, через которые
можно синхронизировать узлы распределенной системы или даже какие-то сервисы распределенной системы.
Но, значит, это первое приближение. Но есть у зукипера API интереснее, чем просто создать узел,
прочесть данные из узла, записать узел. Когда вы создаете узел, у вас есть разные режимы. Как
именно вы можете это сделать? Если мы посмотрим на команду create, то, смотрите, вы создаете по
такому пути узел вот с такими данными, и у вас еще есть create mode. Вот посмотрим на create mode.
Вы можете создать, во-первых, персистентный узел, то есть узел, который появится в дереве и будет
жить там вечно до тех пор, пока его явно не удалят операции delete. Но у вас есть еще и другие
варианты. У вас есть два флага, которые можно комбинировать между собой. А именно, вы можете
создать узел, который называется sequential. Смысл такой, когда вы создаете sequential узлы в зукипере,
то в некоторой директории, то сам зукипер для этой директории поддерживает счетчик,
и каждый новый узел получает себе порядковый номер. При создании узла в директории в режиме
sequential к имени узла, который вы приложили в операции create, добавляется еще один компонент,
а именно порядковый номер этого узла. Вот, и не помню, где-то здесь есть хороший пример.
Возможно, он есть.
Вот. Если вы создаете узел по какому-то пути, то внутри директории накременится счетчик,
и создается узел с таким номером. Пока вы не создаете узел, вы, конечно, не знаете,
какой номер он получит, но вот, создав его, вы можете это потом узнать. Это первая возможность.
Если думать в сторону, если пытаться дальше проводить аналогию с атомиками, что мне кажется
очень разумным, то это вот некоторый аналог атомарной операции FetchEd. Если вы когда-то изучали
атомики, то вы, наверное, знаете, что просто операции store и load для координации узлов,
для координации потоков мало. Нужны еще какие-то более сложные операции, но вот одна из них – это
FetchEd, и sequential узлы эту функцию выполняет. На самом деле, в зукипере есть даже операция CAS,
хотя она вот так довольно незаметно выглядит, потому что она называется всего лишь setData,
как будто бы она просто перезаписывает данные в узле. Но при этом у нее есть аргумент версия,
и семантика этого setData такая. Каждый узел, когда вы в него что-то пишете, каждая запись в зукипере на
узле, каждая перезапись узла меняет его версию, увеличивает, представляет новую версию. Сам
зукипер отвечает за то, чтобы эти версии венатонно росли. Так вот, если вы пишете данные в узел,
вы можете поставить себе, поставить зукиперу ограничение, что я хочу записать в узел по этому
пути вот эти данные, только если сейчас версия узла равна вот этой. Это вот буквально compare
exchange в атомиках, только даже более мощный, потому что вот тут нет ABA, потому что вы сравниваете
несодержимое, а версию. Ну вот такой API. Но даже это еще не все, потому что чем должен отличаться
зукипер от атомиков? Зукипер предоставляет вам атомики вам клиентам, но при этом сами клиенты,
они отличаются от потоков в многопоточной программе, потому что поток он запустился и работает,
а клиент в распределенной системе, он подвержен отказам, он может просто взорваться и больше
ничего не делать. Поэтому это не просто набор атомиков, организованных иерархически, а набор
атомиков, которые еще учитывают возможные отказы клиента. И по этой причине, когда вы создаете
узел, у вас есть дополнительная опция. Сейчас мы вернемся в... Давайте закроем лишнее. Вернемся
в режимы создания узла. У вас есть еще один режим, который называется ephemeral. И для того,
чтобы объяснить, что это такое, нужно немного поговорить про то, как сам зукипер реализован
и как с ним работает клиент. Зукипер это... Разумеется, зукипер это система надежная. Зукипер
это набор реплик, которые общаются между собой и реплицируют некоторое состояние. И реплицируют
это состояние, а не как раз с помощью протокола atomic broadcast. Но это не совсем RSM, если вы помните,
знаете, что это такое. Там не протокол RAFT используется, потому что зукипер был написан до того,
как придумали RAFT. Там используется не multiplex, там схема немного другая, по смыслу похожая,
но все-таки другая. Когда мы говорим про multiplex или RAFT, то у нас есть клиент, он коммитит
команду в реплицированный лог, копия которого есть на каждой реплике, и потом эта команда
применяется к реплицированному состоянию. В зукипере используется подход, который называется
primary backup, а именно, у вас есть лидер в кластере, он получает команду от клиента, он применяет
ее к своему состоянию, скажем, increment, и после этого реплицируют уже такие вот идомпатентные
апдейты на другие реплики. Схема похожая, но все же другая, и там у нее есть некоторые интересные
особенности, например, гораздо легче делать снапшоты, но это уже глубина реализации, нам сейчас не
очень важно. Важно, что зукипер внутри себя, зукипер предоставляет вам модель согласованности,
гарантирует, что он упорядочивает все апдейты, упорядочивает их с помощью примитива Atomic
Broadcast, и для этого использует собственный протокол, который называется зукипер Atomic Broadcast.
То есть это буквально протокол, который был придуман по мотивам мультипаксиса для зукипера,
то есть это тоже протокол, который оптимизирован, который выполняет одну фазу на быстром пути для
комита команды пользователя, простите, немножко простужен, тоже выбирает лидера, но в общем это
некоторый рафт, который был придуман специально вот для этой системы, и разумеется, чтобы протокол
работал, чтобы система оставалась доступна, в этой системе должны оставаться живыми большинство
реплик. Если у нас зукипер, инсталляция зукипера из трех узлов, то зукипер будет доступен и будет
обслуживать ваши команды, ваши операции до сих пор, пока в нем доступны два узла. Вот, как правило,
зукипер инсталирует не на трех узла, но есть разные варианты. Зукипер можно размещать, если в одном
датацентре можно взять и пять узлов, пять или семь узлов, такие стандартные цифры, если вы
размещаете зукипер, реплики зукипера в разных датацентрах, то вот три датацентра, три узла этого
будет кажется достаточно для того, чтобы повысить доступность этого компонента. Но сам зукипер
является, разумеется, CP-системой, то есть он использует внутри Atomic Broadcast, решает задачу
консенсуса, поэтому в случае partition в части сети, где осталось меньшинство реплик зукипера, эта
система окажется недоступной для пользователей. Так реализован зукипер, там внутри некоторый
Atomic Broadcast, некоторые условные паксы с оптимизированной, с выбором лидера и с
однофазным комитом, и дальше клиент в эту систему, в этот Atomic Broadcast отправляет свои команды create,
delete, вот те команды setdata, которые что-то в состоянии зукипера меняют. Поэтому на всех репликах
зукипера это состояние меняется согласованно, меняется, проходит через одну и ту же серию
update. Что еще нужно знать про устройство зукипера, про то, как с ним взаимодействует клиент.
В зукипере вы можете помимо, вот есть, скажем, три узла этого самого зукипера, есть клиент и есть
дерево, которое сейчас хранит каждый узел зукипера. Ну, во-первых, немного нотации. Для того,
чтобы не путать вот эти кружочки и вот эти, то есть узлы зукипера и узлы дерева, в зукипере узлы
называются Zeno. Ну вот, у вас здесь есть корень. Вот здесь какая-то директория, ну, условная
директория, тут нет такого понятия именно узлы. То есть про директорию нужно, потому что директория
это что-то про файловую систему, это не файловая система. Так вот, вы клиент, вы помимо того,
что можете создать просто персистентный узел по 200 по другому узлу, или вы можете построить узел
с автоматической нумерацией, с некоторым аналогичным, вы еще можете создать эфемерный узел.
Давайте это по-особенному нарисуем. Вот у нас есть синий клиент, и он может создать синий
эфемерный узел. Смысл такой, вот есть клиент и он может... Почему я обо всем этом говорю? Потому
что, то есть почему мы добавляем еще один тип узлов? Потому что клиенты могут отказывать. В
атомиках потоки не отказывались. Здесь клиенты могут отказывать, поэтому для отказывающих
клиентов, для того чтобы оборватывать такие сценарии тоже, мы заводим новый тип узлов
эфемерного. Если клиент создает этот эфемерный узел, и вот он в принципе такой же узел,
как и все остальные, но с некоторой поправкой в нем нельзя создавать потомков. Вот чем этот
узел отличается от других, тем своей судьбой. Вот если клиент умирает, то если клиент успел
создать персистентный узел в этом дереве, то этот узел остается жить. А вот эфемерный узел
исчезает вместе с потерей клиента. Каким образом это реализуется? Для того чтобы создавать
эфемерный узел, для того чтобы работать с зукипером, мало просто иметь, скажем,
такой тип API, по которому вы можете посылать какие-то запросы. Нет, клиент зукипера это
полноценный участник всей этой системы. Клиент зукипера использует довольно сложную
клиентскую библиотеку. Когда он работает с системой, он устанавливает с ней логическую сессию.
Он открывает сессию и в рамках этой сессии, сессия идентифицируется некоторой уникальной
строчкой. Вот в рамках этой сессии клиент выполняет все свои операции, в том числе создание эфемерного
узла. Сессия считается живой, пока жив клиент, то есть пока он управляет регулярно в зукеперы
такие свои операции. Либо, если ему нечего делать прямо сейчас, он сам библиотеку клиентской
отправляет в этой сессии узлу системы, узлу зукипера Harbit. Зукиперы называются pings. То есть
клиентская библиотека говорит зукиперу, что клиент все еще жив. Вот если сессия протухла,
то есть клиент показал, допустим, и перестал посоветовать pings эти Harbit, то зукипера понимает
и автоматически стирает эфемерный узел, который был создан этим клиентом. Ну и флаг эфемерности и
флаг sequential можно комбинировать. То есть вы можете создать узел с автоматической нумерацией,
который исчезнет, если исчез клиент. То есть смотрите аналогия с автоматиками еще раз. Вот у нас
есть SerData плюс версии. Это просто CAS. У нас есть sequential узлы. Это
перчерк. И на случай умирающих узлов у нас еще есть плюс к этому всему эфемерность.
Ну вот здесь на самом деле ситуация хитрее, чем я рассказываю. Сложнее устроено. Гарантии тут
более... короче, нужно быть аккуратным. Смотрите, что такое сессия? Ну сессия такое логическое
соединение с системой. Разумеется, внутри реализации, внутри клиентской библиотеки написан какой-то
код, который работает с обычными соединениями. И в конце концов сессия, это некоторая логическая
сессия, реализуется в виде некоторого DCP соединения с системой, через которое клиент
отправляет свои операции в систему и клиентская библиотека отправляет пилинги. Так вот, разрыв
этого соединения это не разрыв сессии. Ну скажем, у вас какой-то узел самого зукипера может оказаться.
Это нормальная ситуация, потому что зукипер внутри себя использует протокол atomic broadcast,
а этому протоколу atomic broadcast для того, чтобы работать, достаточно собирать хворумы,
пока живо большинство узлов зукипер может обслуживать пользователями. Но при этом вот это
соединение разрывалось, и клиент переконнектится к другому узлу. Но при этом сам зукипер сессию не
потеряет, потому что сессия, но зукипер с помощью atomic broadcast реплицирует с одной стороны дерево,
а с другой стороны сессия реплицирует. То есть, если вы знаете про мультипансис прорав, то сессия это
часть персистентного состояния. Оно реплицируется вместе с данными, которые в этом деле живут.
Вот это не беда, но может быть другая беда, а именно, что клиент, ну допустим, залит на сборке
мусора, мы видим, что вот нативный API для зукипер написан на джаве. В джаве есть сборщик мусора,
он может остановить вашу программу в любом момент времени, в том числе клиентскую библиотеку,
которая отправляет инги, поэтому сессия протухнет уже по-честному, и потому что у нее есть таймал,
и клиент должен, разумеется, об этом получить уведомление. Вот протухание сессии — это самая
большая неприятность, которая может случиться с клиентом, и это такая не unrecoverable error. Из этой
ошибки нет способа адекватным образом восстановиться, потому что вы не понимаете,
теперь в каком системе состояния. Вы утратили сессию, у вас исчезли все эфемерные узлы, вы должны,
ну не знаю, начать сначала с чистого листа, забыть, как будто бы вы перезагрузились, можно об этом,
можно к этому так относиться. Пока все понятно, скажите. Понятно. Если примерно понятно,
как зукипер внутри устроен, какую модель данных он предоставляет, с какими гарантиями.
С гарантиями у зукипера довольно сложно, потому что, с одной стороны, все запросы на запись
упорядочиваются через atomic broadcast, а вот с чтениями чуть хитрее. На самом деле,
зукипер и чтение могут читать немного старые данные, но в принципе для большинства приложений
это не очень важно, но это очень сложная история, если хотите, можно потом углубиться в документацию.
Окей, вот если все это понятно, то теперь можно подумать, а почему нам зукипер полезен,
то есть какие задачи мы собираемся решать с помощью этого зукипера. Итак, какие задачи мы хотим
решать в первую очередь? Прежде чем решать задачи, маленькое замечание важное. Чем зукипер
отличается от протокола консенсуса, то есть такое забавное наблюдение и правильный подход к этой
системе. Если вы используете какую-то библиотеку для консенсуса для atomic broadcast в своем коде,
то она вырешает консенсус внутри вашей системы. Для каждой системы, которая использует консенсус,
будут свои узлы, свои алгоритмы, своя библиотека, свои независимые консенсусы. Так вот, зукипер
предназначен для того, чтобы он был общим. Вот если вы пишете одну систему, потом другие
люди вашей компании большой пишут другую систему, и вам и им нужен сервис координации, то вы можете
переиспользоваться на тот же зукипер. Иерархия нужна для того, чтобы разные проекты разнести по
разным узлам, по разным поддеревьям. Или у вас есть одна система, просто разные инсталляции
на разных машинах, на разных кластерах. Опять, эти системы могут использовать один общий зукипер,
просто разместив свои данные в разных поддеревьях. У каждой системы будет свой
собственный путь, и там будут храниться все метаданные этой системы. Итак, первая задача,
которую мы будем сегодня решать. Они все будут довольно абстрактными, но, в принципе,
вы, надеюсь, поймете, что они неизбежно возникают более-менее в любой системе,
в любой распределенной системе, чем бы она ни занималась. Итак, представим, что, ну, я не знаю,
вы пишете каком-то условно, и у вас есть много машин, например, 9, которые готовы выполнить что-то
полезное. Там запускаются узлы вашей системы, и, разумеется, каждому узлу, чтобы запуститься,
нужно иметь некоторую конфигурацию. Так, я не знаю, какие настройки кэшей, настройки rate limit,
но все, что вы можете, какие-то порты, по которым нужно слушать сообщения из сети, все,
что вы можете представить. То есть, на каждом узле должен быть какой-то файл с конфигурацией. И есть
проблема теперь. Вот есть администратор, и он подготовил, тут на каждом узле лежит какой-то файл,
он называется, ну так вот, очень условно, config.json. И вот есть первая версия, которая лежит, нулевая
версия, то есть, которая была на старте каждого узла. А теперь у администратора есть
config.json версии 2, версии 1. И он хочет этот конфиг положить на все узлы, обновить его на каждом
узле. Понятно, что он не может так сделать, потому что все узлы, не все узлы сейчас доступны. Но его
цель, чтобы конфигурация родается на каждом узле. Вот такая очень простая распределённая задача.
Узнается, если у вас пластер большой, то нет никакой надежды, что все узлы будут живы и все
узлы будут, можно будет собрать все узлы в каком-то временном промежутке и положить на них config.
Если у вас там тысячи машин, то каждый день у вас сломаются какие-то там единицы, десятки
дисков, вы постоянно что-то меняете, что-то отключено, поэтому нельзя просто так пройтись
по всем машинам и обновить там конфигурацию. Как же быть? Как поддерживать узла в конфиге так,
чтобы их можно было легко обновлять? Легко и надежно обновлять. Есть идеи,
потому что вопрос простой, у Zookieper у нас есть. Вот вместо того, чтобы хранить на каждом узле
конфигурацию какую-то сложную, мы на каждом узле будем хранить конфигурацию, которая называется
bootstrap.json и все, что вот в этом файле будет написано, это адрес Zookieper, то есть реплики и порты,
на которых работают узлы Zookieper, так чтобы просто каждый узел системы мог к этому Zookieper прийти.
Зачем? Затем, что именно в Zookieper мы положим конфигурацию для каждого узла. Ну, давайте
пропишем такую условную домашнюю, свою собственную распределенную систему абстрактную и вот в Zookieper мы
заведем для нее узел. И здесь у нас будет узел config. И просто узлы знают, что это можно положить
вот сюда. Этот путь меняться в будущем не планирует. Когда администратор хочет обновить конфигурацию
каждого узла, он не пытается это сделать буквально на каждом узле, потому что он не знает, когда каждый
отдельный узел станет доступным. Он пишет эту конфигурацию, обновляет ее здесь. Он вместо
нулевой конфигурации записывает здесь первую конфигурацию. И эту операцию выполнить легко,
потому что сам Zookieper является доступным, он может переживать отказа. Так что администратору
скорее всего это удастся. И пусть каждый узел вместо того, чтобы администратор обновления конфигурации,
он просто пользует узел Zookieper. Простой, тупой вариант изначальный. Пусть там каждый узел раз в
30 секунд перечитывает эту конфигурацию. Ну и смотрите, я уже говорил, что когда мы обновляем данные
Zookieper, когда мы говорим set data, то Zookieper бампает версию узла. Поэтому узлу легко понять,
что конфигурация изменилась. Ему не нужно что-то там перечитывать и сравнивать. Он читает версию
этого узла, и если она увеличилась, то значит администратор обновил конфигурацию и нужно сам файл перечитать.
Идея понятна? То есть мы сосредоточили наше глобальное знание, вот некоторую глобальную истину,
какова сейчас конфигурация в одной отказоустойчивой системе, а все узлы просто к ней обращаются. То есть мы
вот эту коммуникацию Admin общается со всеми, заменили на Admin общается только с Zookieper,
и узлы общаются с Zookieper. То есть мы прямую связь между ними разорвали, заменили ее на
коммуникацию с общим отказоустойчивым компонентом. И для того, чтобы эту конструкцию
запустить, достаточно просто иметь bootstrap-конфиг, который позволит узлу вообще стартовать. То есть
он стартует, из этого конфига получает адрес Zookieper, получает путь конфига в Zookieper, и дальше из этого
Zookieper, по этому пути вычитывает уже свой конфиг и поднимается. В чем проблема такого подхода?
Узлы могут с разными версиями в один момент работать.
Но я бы сказал, что это неизбежно.
Если ты прям хочешь сделать все синхронно, то, боюсь, у тебя один способ – все выключить, а потом включить.
Тут задача не в том, чтобы они все как-то синхронно переключились, это недостижимо все равно,
а в том, чтобы, если конфиг меняется, то все узлы его рано или поздно перечитали и начали работать с ним.
Тут не то, чтобы что-то плохое произойдет, просто конструкция очень неэффективна, а именно каждый узел полит Zookieper.
А если у вас их там, не знаю, тысячи или десять тысяч, не дай бог, то они будут каждый раз в 10 секунд идти в Zookieper,
то это просто большая нагрузка на сам Zookieper. Можно ли ее оптимизировать?
Конечно же, мы этого хотим, потому что конфиг не будет обновляться каждый 10 секунд.
Ну не знаю, раз в неделю может будет обновляться или раз в месяц вообще, а нагрузка будет постоянная у нас.
Поэтому в апизу Zookieper есть еще одна фича очень важная, которая позволяет такие сценарии оптимизировать.
Давайте вернемся на экран.
На API у нас есть метод, который называется Exist.
Мы туда передаем путь к узлу и получаем в ответ структуру, которая называется Stat.
В общем, информация про то, в каком состоянии сейчас узел, и тут можно у него версию узнать.
Много разных версий, много нюансов, но короче, сейчас не суть.
То есть это мета информация про Zenoat внутри Zookieper.
Но смотрите, в этом API есть еще один параметр.
В этом вызове. Ой, не здесь, это структура.
Вызов Exist. Вы передаете путь, а еще флажок Watch.
Смотрите, когда вы создаете клиента Zookieper, вы помимо того, что указываете строчку, в которой перечислены реплики с портами, чтобы соединяться с ними,
и помимо указания тайм-аута для вашей сессии, сколько секунд без кинга Zookieper будет считать вас живым.
Вы передаете еще реализацию Watcher.
Watcher – это интерфейс.
Ключ оставите внизу.
Хорошо, спасибо.
Когда вы подключаетесь к кластеру, вы передаете еще Watcher.
Watcher нужен для того, чтобы не полить, а заменить полных нотификаций.
Вот у Watcher есть всего лишь один метод.
Watcher реализует один метод процесс.
И ему сваливается вот такое вот событие, что по некоторому пути случилось что-то.
Ну, например, по некоторому пути был создан узел.
В смысле, в некоторой директории был создан узел.
Или удален узел.
Или модифицирован узел.
И вместо того, чтобы вот в этой конструкции, которая сзади меня была нарисована,
каждый узел полил конфигурацию.
Нет, каждый узел перечитывает конфигурацию и вешает Watch.
Но нужно сделать это аккуратно.
Нельзя просто прочесть конфигурацию, а потом сделать Exist с Watch.
Понятно ли почему?
Потому что между этими вызовами могло случиться изменение.
Так что, смотрите, любая читающая операция может этот Watch поставить.
И протокол такой.
Вы стартуете на месте узла.
Вы читаете с помощью GetData.
Вы читаете ваш Bootstrap-конфиг статический, который не меняется.
Там, где написан адрес зукипера.
Вы из этого конфига получаете путь до вашего конфига в зукипере.
Вы с помощью GetData этот конфиг читаете.
И вместе с этим вы ставите Watch в рамках вашей сессии.
То есть вы клиент.
Вот права зукипера знают, что это не просто отдельный запрос,
а что вы именно клиент сессии.
И вы в рамках этой сессии вешаете Watch.
И когда администратор обновит конфигурацию...
Давайте я это нарисую сейчас на доске.
У нас есть с одной стороны зукипер.
Есть какие-то узлы вашей системы.
И есть администратор.
Вот на старте узлы системы обращаются к зукиперу
с помощью с операцией GetData.
Они читают себе конфиг.
После этого, что у них есть сессия 1, сессия 2.
И у зукипера теперь есть два Watch.
То есть эти узлы теперь...
Ну давайте такие вот очки нарисуем маленькие.
Не знаю, видите, вы клиент.
Они ждут изменений.
Потом появляется администратор, и он будет разноцветным.
Он обновляет конфиг с помощью SetData.
После чего зукипер, понимая, что данные изменились,
за которыми наблюдали два узла,
сообщит им об этом с помощью Watch.
И после этого, вот эти два узла пойдут и еще раз перечитают конфиги.
Но при этом, что могло случиться?
Пока они получат уведомление, пересчитают,
в этом интервале вполне могло случиться еще одно обновление конфиг.
Вот сам по себе Watch не говорит, что данные изменились на такие.
Это просто событие, что что-то поменялось.
И вот между этой нотификацией, когда Watch одноразовый,
вы повесили его, вы получили один notification,
и после этого он отменяется.
Больше уже узел зачем не следит.
Так вот, вы в такой схеме пропустите одну версию конфига.
Но для задачей конфигурации это не страшно.
То есть вы здесь перечитаете уже не первую версию, а вторую версию.
Понятно?
Понятно.
Окей, тогда идем.
Это такой метод замечания.
Вот Watch в зукепере, это еще одна важная фича.
Немерные узлы и Watch.
Вот Watch это механизм каширования на самом деле.
И вот эти нотификации это как протокол инвалидации кашей.
Клиенты системы не хотят постоянно по любому поводу
в зукепер ходить и перечитывать данные.
Они хотят поддерживать локальную копию этих данных.
Но для того, чтобы она была согласована,
нужно, чтобы в случае перезаписи в зукепере ваша копия инвалидировалась.
Ну вот это буквально как в протоколе конгерентности кашей.
Если кто-то пишет систему, то он посылает другим запрос на инвалидацию.
Вот в GoogleChab прямо так в зукепере немного по-другому.
Но смысл такой же.
Если кто-то подписался, а потом кто-то другой перезаписал данные,
то эта перезапись приводит к триггеру нотификации
у вас на клиенте вызывается процесс адвента какого-то
и вы можете инвалидировать уже свой каш.
То есть смысл ровно такой же.
Это еще одна аналогия с атомиками и с процессорами.
Это протокол конгерентности кашей.
И не то, чтобы это было удивительно.
Это как раз очень разумно, потому что мы решаем задачу координации.
И поэтому инструменты такие же.
Хорошо, с обновлением конфигурации мы разобрались.
Давайте решим еще одну задачу, которая на системах возникает неизбежно.
Это задача обнаружения спойных узлов.
Как с ней поможет ZooKeeper?
Представьте себе, мы снова пишем наш собственный маленький
reproduce и у нас есть pool машин.
И вот на этих машинах должны запускаться какие-то джабы,
которые перемалывают часть каких-нибудь данных, файлов, таблиц, чего угодно.
Мы считаем, что каждая машина – это некоторые вычислительные ресурсы.
Это набор процессоров, которые могут выполнять reproduce и джабы.
Но нам, конечно, нужно понимать, какие сейчас машины доступны, какие нет.
Это еще один способ ZooKeeper поменять.
Еще один очень простой способ.
Если у нас есть наша система и там есть узлы Worker,
то мы в этой конструкции, в нашей поддереве ZooKeeper,
для нашей системы заведем еще один узел,
который будет называться Workers.
И каждый узел, когда он стартует, он просто приходит в ZooKeeper,
потому что у него есть адрес ZooKeeper,
и он в нем создает эфемерный узел со своим FQD.
И теперь, если у нас есть такой условный компонент планировщик,
который получает задачи от пользователя,
делит данные на части, отправляет эти части с этими задачами на узлы,
то вот этот планировщик будет что делать?
Он должен будет следить за вот этой директорией.
Он вешает на нее Watch,
и если какой-то узел умирает, то у него протухает сессия,
эфемерный узел исчезает,
триггерится нацификация на этой директории у планировщика,
и планировщик получит уведомление от системы,
что директория изменилась,
перечидывает ее и обновляет список живых узлов,
список узлов в своей собственной памяти,
которые он считает живыми.
Ну и дальше, если он успел на какой-то узел задачу назначить,
он ее переназначает на другой узел.
То есть тут снова Watch,
и снова эфемерные узлы,
впервые эфемерные узлы, и снова Watch.
Ну и снова этот планировщик кэширует на себя список живых машин.
То есть если мы используем Watch, это означает,
что мы видимо поддерживаем копию этого списка у себя,
и если этот список меняется здесь, то он меняется и через донатификацию, и у нас.
Если мы его перечитываем, обновляем после донатификации от ZooKeeper.
Окей.
Ну да, я забыл сказать про самое базовое применение ZooKeeper,
а это сервис Discovery.
То есть если у вас в системе довольно много разных сервисов,
много компонентов, микросервисов,
то для того, чтобы они просто могли находить друг друга,
вы вместо установленного DNS можете использовать,
ну вы можете использовать ZooKeeper как DNS,
вы можете в нем создавать узлы,
которые отвечают,
имя которых это имя сервиса,
и уже в рамках этих узлов записывать данные о том,
какие из каких машин состоит, к кому можно приходить.
Но вам нужен некоторая статическая для нас запись,
а именно вот этот конфиг для того,
чтобы просто найти сам ZooKeeper, потом же через него находится все остальное.
Итак, идем дальше.
Следующая очень естественная задача,
которая возникает в распределенных системах,
это задача выбора координатора.
Вот если мы надеюсь,
будем успеем поговорить с вами про капку,
это все мести вроде должны,
если я все правильно понимаю,
то, ну это точно,
тоже распределенная система,
там есть какие-то узлы,
которые хранят в копии логов,
копии партий, что бы это ни значило пока,
и выбирается среди всех узлов
некоторый узел, который управляет другими узлами,
который следит за тем, что в системе происходит,
кто что хранит, кто жив, кто умер,
и разумеется, этот узел должен быть в системе 1.
Вот если у нас такая задача возникает,
то очень разумно подумать в сторону взаимного исключения.
Мы хотим сделать,
мы хотим выбрать узел,
который будет координатором,
просто сделав распределенную блокировку,
то есть пусть узел логит блокировку,
и дальше будет руководить
остальными узлами,
потому что он один.
Вот с такой мыслью,
в Google придумали Chubby,
именно поэтому Chubby был
сервисом блокировок.
В ZooKeeper от модели блокировок
на уровне API отказались,
то есть у нас нет методов
лог, анлок, подобного,
acquired risk. У нас есть операция
атомиков, потому что понятно,
что из атомиков можно сделать блокировку,
то есть можно сделать спинлог.
Так вот, мало шансов,
что вы где-нибудь в своей жизни
будете писать, ну не то,
что мало шансов, но
вряд ли вы в своей жизни будете писать
спинлог для процессора.
Скорее всего, он уже написан за вас,
или вы просто используете.
Но вот знание про спинлог
внезапно полезно, если вы используете ZooKeeper,
потому что в ZooKeeper нет
в чистом виде блокировок,
но через модель данных
вы можете эти блокировки реализовать.
Вот давайте представим,
что мы хотим сделать,
решить взаимные исключения
с помощью ZooKeeper. У нас есть разные узлы,
и каждый из них готов
стать главным координатором,
лидером, мастером
всей нашей системы.
Как бы мы могли такую задачу
решить с помощью ZooKeeper?
Давайте у нас опять будет
директория с лучшим,
и там в ней один
эфемерный узел.
У нас будет директория с лучшим,
а в ней эфемерный узел.
Вот.
Когда он умирает,
всем посылается сигнал,
и всем вы пытаетесь делать касс.
Вот у нас есть,
давай скажем, что у нас есть
персистентный узел
slash, моя распределенная система
slash master.
И вот в этом,
на этом узле мы создаем,
в этой директории
условной мы создаем потомка
slash log.
Вот этот узел эфемерный.
Каждый узел,
который хочет стать мастером системы,
хочет стать координатором,
пытается вот по этому пути
создать потомку slash log.
ZooKeeper, напомню, упорядочивает
все операции, которые к нему приходят.
В том числе все креаты.
Поэтому среди креатов будет
первый успешный
и остальные неуспешные
которые получат исключение,
что узел уже существует.
И вот когда мы создаем узел,
мы вот это,
мы, во-первых, создаем его
атомарно, если его еще не было.
Это, ну вот, буквально
create это
операция
exchange в атомике,
правда?
Записать, если там пуст.
Ну вот, create это вот создать,
если ничего не было еще.
И в этом create мы сразу
положим внутрь этого узла
в качестве данных
просто свой адрес,
чтобы другие узлы, которые
провалят свой create,
после этого понимали,
что, ну, кто сейчас
мастером выбрал.
Если они пытались создать узел,
а он уже существует, они его перечитают
и узнают мастера.
Ну, если вдруг это им нужно вообще.
Хорошо, если у этого, да,
мы создали этот узел,
стали мастером, а что делают остальные,
которые проиграли?
Они, видимо, с помощью
Exist вешают Watch
и этот Exist может
вернуться либо с,
ну, либо этот файл все еще существует
и они подвисят Watch.
Либо вдруг,
пока они вешали Watch, то есть между create
и Exist файл был убален,
потому что узел умер.
И тогда они об этом опять же узнают
и попробуют снова.
Но если они успели создать Watch,
если файл все еще существует
и они подвисели Watch,
то они просто ждут, пока не получат
нотификацию о том, что
узел slash log исчез из диктории
мастер и попробуют снова.
Ну, вот такой простой протокол.
В чем его проблема?
А мы Watch вешаем на мастер
или на лог?
Мы Watch вешаем на мастера
или на нот?
Тут, мне кажется, не важно уже.
Как угодно можно сниматься.
Ну, опять, наверное,
неаккуратно вопрос задаю.
В чем нерефективность
такого решения?
Но эта нефективность называется
Sundering Hair.
Проблема следующая.
Вот это вот,
вот это вот,
вот это вот,
вот это вот,
вот это вот,
вот это вот,
вот это вот,
вот это вот,
проблема следующая.
Вот у вас мастер был, а потом
он отказал.
И в этом случае
узел исчез.
У многих подписчиков, которые готовы были
его заменить, сработала
нотификация, пришло сообщение
о том, что узел удален.
И каждый из них посылает
create-систему после этого.
Выглядит не очень
страшно. Выглядит
не очень страшно, потому что
вряд ли мастер будет умирать часто.
Но с другой стороны, зависит
от того, какой узел кипера используете.
Во-первых, узел кипера могут пользоваться
много систем. Во-вторых,
в вашей системе просто могут быть
много компонентов, которые выбирают
среди себя какого-то главного.
Это можно оптимизировать в кавке.
Я покажу, как это сделано.
Через неделю, видимо, или через две.
Но потенциально
это может стать неэффективным.
Что сразу много клиентов приходят
и начинают ломиться в
зу кипер, создавать у него
тот же узел. Хотя удастся
только одному.
Можно ли этот сценарий оптимизировать?
Разумеется, может.
И даже вы знаете как.
Для этого вы
в прошлом семестре
писали тикет-лог.
Было такое?
Да, было такое.
Но я не знаю,
что такое тикет-лог.
Я не знаю, что такое тикет-лог.
Я не знаю, что такое тикет-лог.
Было такое?
Вот тикет-лог можно сделать
в зу кипере. Причем
я бы сказал, что это такой
рекомендуемый способ
изготовления блокировки в зу кипере.
Вообще в зу кипере можно делать самые
разные вещи.
И поскольку
зу кипер, у него AP слишком низкоуровневая,
то есть он не решает некую конкретную задачу.
Он скорее дает инструменты для того, чтобы это все сделать.
То есть так же, как в C++, у вас есть атомики,
а дальше крутитесь, как хотите.
Так вот, зу кипере тоже обливает атомики
и говорят, что вы с помощью атомиков
можете с помощью атомиков и вочей
обвочить такой аналог фьютекса,
где вы можете заснуть.
Но тут все очень
сходство совершенно прямое.
Вы можете из всего этого делать
разные примитивы коммуникации.
Вы можете делать разные локи, можно делать барьеры,
можно делать очереди.
Но, наверное, мы не успеем уже про очереди поговорить.
Но давайте хотя бы с локами разберемся.
Так вот, предлагается сделать
немного пооптимальный протокол,
чтобы, если
кто-то лог отпускает,
то после этого не все ломились
в зу кипер, а только один узел, который
стоит в очереди.
Для этого, когда мы пытаемся
захватить лог, то мы берем
директорию мьютекс,
директорию лока
и создаем там
эфемерный sequential узел
logDefice.
Напомню, что sequential означает, что
к этому имени прибавится еще
некоторый порядковый номер, который будет присвоен
самой системой.
Ну и когда
вы вызываете create, то
смотрим на API,
вы получаете путь, который вы
создали. Ну потому что если вы создали персистентный
узел, просто персистентный, то это будет
ваш пас. Если вы создали sequential
узел, то вам вернется порядковый номер
его.
Вот, создали
эфемерный sequential узел.
То есть вы таким образом получили
тикет в тикетлоке, заняли
какой-то порядковый номер, заняли
свою очередь. Но
чем отличается наш протокол от тикетлока
тем, что у нас клиенты могут отказывать
и
поэтому из этой очереди
тикетов какие-то номера могут
выпадать. Но может быть вы
уже выиграли, на самом деле, непонятно. Поэтому
первое, что вы делаете, вы говорите getChildren.
И таким образом узнаете
как вы упорядочились относительно
других. То есть либо вы
первый в директории, и тогда уже
новый узел, который может
быть создан, получит
порядковый номер больше, чем вы.
Так что если вы видите
с помощью getChildren, что вы
наименьший узел в директории,
графически, то
вы можете быть уверенными, что вы
первой в очереди на блокировку.
В этом случае
вы уходите и вы довольны.
Что если вы узнали,
что вы не первой? Ну вы должны
ждать. Ждать кого?
Вы можете повесить watch
на всю директорию.
И когда какой-то узел
будет появляться, или какой-то узел
будет освобождать
блокировку, даже если перед вами другие
узлы еще есть, вы будете
получать нотификацию и перечитывать всю
директорию. Это будет неэффективно.
Поэтому вы вешаете
watch на
узел перед вами.
Вы говорите exists
на предшествующем
лексикографическом порядке узле.
Этот exists
мог вернуть вам false, потому что
пока вы все это делали, ваша очередь
прям подошла.
Ну тогда ваше представление
об очереди устарели, и вы
откатываетесь в пункту 2.
Но если вы подписались,
вот здесь вот, успешно,
с помощью exists
повесили watch
на предыдущий файл,
то вы просто ждете, пока он
не исчезнет.
Непонятно, почему он исчезнет, потому что
узел умер и выпал из очереди.
Клиент умер и выпал из очереди.
Или потому что ваша очередь просто
подошла. Но в обоих случаях
вы
откатываетесь в пункту 2
и перечитываете
список узлов и
обновляете свое
понимание происходящее.
То есть это вот буквально ticket lock
с блокирующим
ожиданием.
И здесь вы делаете get children
тогда, когда
умирает узел в очереди
перед вами.
То есть если
допустим все живые берут блокировку в
каком-то порядке, то в принципе ничего страшного
не происходит. Каждый
то есть стирание вот этого вашего узла
приводит к всего лишь к пробуждению,
то есть к иноцификации
только одного ждущего узла,
одного ждущего клиента.
Я не знаю,
на самом деле вот мы
минутка забавных
фактов.
Я кажется весной
перестал это рассказывать, потому что все не помещается,
но вообще в
линуксе,
вообще в жизни
есть разные спинлоки, и вот есть
спинлоки, которые оптимизированы
для протокололога
генетности кэшей, чтобы запись не инвалидировала
многих других узлов.
Так вот в линуксе есть вот примерно
такой спинлок, который я вам рассказал, только он
реализован с помощью там weight-free
очередей.
Это такая сложная реализация, есть там попроще
по-моему.
Если я найду его за 5 секунд,
то хорошо, не найду, то сдамся.
А нет, вот же он.
Это называется
mcs-spinlock,
очень простой, очень истроумный,
и вот
идея похожая.
Но
если мы поговорим, то то есть
вот так вот можно делать распределенные
блокировки, можно выбирать мастер,
короче, чего мы научились делать?
Мы научились поддерживать конфигурации
мы научились делать
failure detection и
просто
понимать, в каком состоянии сейчас кластер находится,
с каких узлов он состоит.
Мы научились выбирать
мастер с помощью блокировок.
Ну вот про блокировки
разговор еще не закончен, потому что
сами распределенные блокировки
это довольно хрупкая вещь, потому что
они, в отличие от блокировок
многопоточных, не
гарантируют взаимного исключения.
Вот про это есть
замечательная статья Мартина Клепмана,
вот она.
Мартин Клепман, как говорит этот картинка справа,
автор всем известной
книжки с Кабаном,
в которой, вот такая огромная
толстая книга, в которой в перемешку,
мне кажется, без всякой системы
описаны разные идеи
алгоритмические, инженерные,
которые возникают при проектировании
больших систем обработки
и хранения данных.
Но мне кажется, что там
книга не самым лучшим образом
декомпозирована, там нет отдельно
не отделены инженерные
аспекты от какой-то теории,
очень сложно по ней какое-то понимание
построить. Но
тем не менее, там очень много
всего, и книгу
я определенно рекомендую.
Так вот, этот человек написал замечательный пост
про распределенные блокировки, про то, что
идея вообще довольно сомнительная, потому что
если вы пишете такой код, я иду
в сервис блокировок, это Чаби в Google
или это ZooKeeper, где я сделал свой тикет-лог
и беру там блокировку по какому-то пути,
а потом
под этой блокировкой
я работаю
с каким-то внешним состоянием,
а потом лог отпускаю,
то в многопоточном варианте
этот код, конечно, будет правильным, а
в распределенном он не будет работать, потому
что, ну опять,
у вас есть вы
клиент системы, и вот вы думаете, что
вы владеете блокировкой, а есть система,
которая вам эту блокировку
выдала, и
ваше понимание о том, владеете вы блокировкой
или нет, оно может
с системой расходиться. Вот тут есть
замечательная диаграмма, можно ее проиллюстрировать,
наверное, это то, что нам нужно.
Вы взяли
блокировку, она называется Лиза,
а не блокировка, у нас, к сожалению,
нет сейчас силы разбираться в тонкостях,
точнее я потом расскажу. Взяли
блокировку, получили ответ,
и эта блокировка
привязана, ну эта блокировка,
это эфемерный узел, как мы уже выяснили,
и жизнь этого эфемерного узла
привязана к жизни сессии.
Вот, блок-сервис поддерживает вашу сессию,
у него есть тайм-аут.
Если этот тайм-аут истекает,
а вы не посылаете пинги системе, то она считает,
что вы, разумно считает, что вы отказали.
После этого блокировку
вас отнимает, поэтому это не блокировка,
называется Лиза. То есть вы
арендуете блокировку, а не захватываете его
владение.
У вас блокировку отнимает,
но вы на самом деле живы, потому что
вы просто заснули
на паузе сборки мусора.
И появляется другой клиент,
и он эту блокировку перехватывает.
И после этого он под этой
блокировкой пишет что-то вот сюда.
Какое-то внешнее хранилище.
А потом просыпаетесь вы,
и вы заснули просто вот между этой
строчкой,
после этой строчки и до вот этой.
Вы все еще думаете,
что вы владеете блокировкой.
И вы пишете в третью
систему, и вот тут-то вы
нарушили.
Это не то чтобы до-после,
а вот у вас есть
два конкурентных вызова, и они
неопорядочны блокировкой, получается, теперь.
Это же та проблема, которая у нас была
в кундуаре,
которую мы решали.
На кундуар похоже.
Нам нужно проверять каждый раз,
что мы все еще...
Ну да,
если мы что делаем
во внешней системе, то мы должны
убедиться, что мы все еще владеем
блокировкой.
Беда в том, что у нас здесь две системы,
они друг про друга работают,
а мы все еще не владеем блокировкой.
Да, здесь две системы, они друг про друга
ничего не знают, но и не могут знать.
Поэтому тут есть
разные варианты, как можно поступать.
Вариантов
много.
Для этой лекции важно, скорее,
что
ZooKeeper сам по себе
такую проблему допускает.
Вот отдельный сервис блокировок
такую проблему допускает.
Как можно эту проблему решить?
Можно решить ее,
поместив данные,
с которыми вы работаете,
и блокировку в одну и ту же систему,
то есть хранить в ZooKeeper и данные,
и блокировку.
Конечно, в ZooKeeper много данных не поместится,
но какое-то количество данных все-таки поместится.
И вы можете аккуратно
обновлять эти данные,
только если вы все еще владеете блокировкой.
Если у вас
отнимут блокировку, значит у вас протухло сессия,
значит вы и данные обновлять не можете.
Но это удобный
удобный дизайн системы,
когда у вас такие...
Когда у вас можно
часть состояния поместить ZooKeeper.
Но это не всегда возможно,
поэтому в общем случае у вас проблемы.
Ну а некоторые... Простите,
он простудился.
Некоторые сложные системы, ну скажем,
для тех,
кто слушал меня по субботам,
я рассказывал про YandexDB.
Это
большая распределенная база данных
с детерминированными транзакциями,
которые написаны в Yandex для Yandex облака,
но не только для Yandex облака.
Так вот, там есть такой же...
Там похожий дизайн.
В смысле, у вас есть подсистема,
в которой можно хранить и блокировки,
то есть и...
Я остановлюсь,
отмотаю себя в прошлое
на 30 секунд.
И скажу, что вот как такие проблемы
хотя бы можно замечать?
В этом хранилище.
Можно просто локи версионировать.
То есть вы не просто владеете локом,
вы владеете локом,
я не знаю, пятым по очереди,
или шестым по очереди.
То есть вы с блокировками связываете
еще и поколения.
Поэтому,
если мы говорим про Google Chabi,
это снова не он,
это вот Google Chabi,
то в API Google Chabi у вас есть
помимо acquire и release,
то есть взять себе отпускание блокировки,
у вас есть метод getSequencer,
который возвращает вам
некоторый порядковый номер ваш.
Ваш порядковый номер относительно других
критических секций того же Mutex.
Ну а в случае с зукипером
все еще проще,
потому что у вас есть
потому что
у вас есть
стат
и через этот стат
из узла можно получить
его версию, его глобальный порядковый номер
за xid.
В общем, вы с самого зукипера можете
порядковый номер блокировки изъять
и пользоваться им для того,
чтобы сообщить вот этой самой
третьей системе
о том, что вы
первой
обладателем Mutex.
Если система поймет, что к ней уже приходил второй,
то она ваш запрос проигнорирует.
Вот такое упорядочивание
этих самых блокировок,
это называется fencing.
Ну вот зря я картинку
не промотал, она здесь была.
Вот это называется здесь токен. То есть вы берете блокировку
и получаете токен. И каждый
следующий клиент,
который получил блокировку, получает себе
больше токен. И система,
просто запоминая, наибольший токен, который она видела, может
отвергнуть
более старый токен.
Но если вы вдруг знаете Rafter, Mutex,
Axis или что-нибудь про консенсоры,
то идея знакомая. И в общем, тут попытка
вынести эту логику
из вашего кода в
внешнюю систему.
И такая идея она используется и в большем
масштабе. Все-таки в зукипер
данные и блокировки
положить нельзя, потому что данных много.
Но вот если вы пишете систему с чистого
перистата, вот в Янекс есть ЯнексDB,
и там архитектура такая.
Есть подсистема
хранения, называется там Distribute Storage,
Block Storage. И у нее
API такое. С одной стороны, она хранит
неимоверное количество данных,
любое количество. Она там десятки,
сотни петабайт. А с другой стороны, она
внутри реализует консенсус и
выдает узлам
поколения.
Так что, если вы заранее
позаботились об этой проблеме,
то вы можете
избежать,
аккуратно побороть
проблему
отсутствия взаимного исключения
в распределенных блокировках.
Да, действительно,
два узла могут
в определенной системе думать, что не владеют блокировкой,
это неизбежно.
Но в таком случае можно
по крайней мере защитить,
можно по крайней мере заблокировать
старый узел, который на самом деле,
блокировка которого уже устарела,
не дать ему модифицировать
какое-то состояние.
То есть, локальное понимание,
локальную веру, что я узел, владею блокировкой,
мы победить не можем,
это просто
свойство распределенности,
с которым мы должны смириться.
Но мы можем заблокировать модификацию
мутации вот такого узла
с помощью токенов поколений,
как они там называются, по-разному.
Ну что,
это, наверное,
все, что можно за ограниченное время
про зукипер же сказать.
Если у вас вопросы есть,
то давайте мы их обсудим.
Маленький анонс,
когда мы будем говорить про кавку,
то в кавке зукипер
является как раз
вот тем самым консенсусом, как сервис.
То есть, это компонент, который
гарантирует, что данные
обновляются в кавке согласованно,
что они упорядочиваются
и там ничего не теряется.
Вот когда мы будем говорить про кавку, мы увидим,
как зукипер там используется.
Применение зукипера
по назначению ПиМО, то есть
консенсус как сервис.
Попросы?
Пожелания?
Что-нибудь?
Я не знаю, будет ли у вас
семинар по зукеперу. Если
будет, то там можно
обсудить очень простую задачу,
которая внутри себя комбинирует
все, что мы сегодня
обсудили. Вот задача,
где можно собрать все вместе
и детектор сбоев,
и конфигурации, наверное, можно
собрать и взаимные
исключения, и еще что-нибудь. А именно
задача такая – сделать сервис, который
очередь задач,
тредпул распределенный,
у вас есть пул машин,
они могут, разумеется, отказывать,
и есть клиенты,
которые в этот пул бросают
задачи. Ну, то есть вот какие-то, не знаю,
маленькие башискрипты,
которые должны запуститься
на любой машине, что-то сделать, и вот
неизвестно что, просто вот
какие-то команды, которые мы можем системам
помещать. Вот задача в том, чтобы
эти задачи
исполнялись распределенно параллельно
на разных машинах, и чтобы
каждая задача, которую клиент отправил в систему,
исполнилось, ну, по крайней мере, один раз.
Тут уже можно разные дизайны придумывать,
более или менее эффективные, но вот
хороший дизайн, он сочетает
примерно все техники, которые мы сегодня
обсудили.
Ну что, если вопросов больше нет,
то спасибо, что зашли.
Заходите в следующий раз, мы поговорим
либо про Кассандру, либо про Кавку.

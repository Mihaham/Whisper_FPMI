Мы продолжаем тему численно-медленно решение краевых задач для обыкновенных дифференциальных
уравнений. Я напомню, на чем мы остановились в прошлый раз и продолжим. Мы начали говорить о
задаче штурма Нью Вилле, которая имеет в дифференциальном варианте вот такой вид. Это уравнение
второго порядка. С точки зрения физики она описывает либо диффузионные процессы, либо
типопроводные, либо вообще процессы расстостранения тех или иных факторов. Оно используется, например,
и в экологии, и в социологии, и даже для описания расстостранения эпидемии. Здесь, правда,
социональный вариант. Обычно чаще используется в несоциональном варианте это уравнение.
Сначала начнем с обыкновенного дифференциального уравнения.
Х у нас меняется от 0 до L. Краевые условия, давайте еще раз повторим, важный момент,
ну вот так вот такие у нас будут. У альфа-1 у штрих по х от 0 плюс бета-1 от 0 равняется гамма-1.
Здесь х равняется 0. Левые ограниченные условия и правые. Альфа-2 у штрих по х от L плюс бета-2
у от L равняется гамма-2. Х равняется L. То есть у нас вот такая краевая задача рассматривается.
Что мы делаем далее? Далее мы вводим, как обычно, расчетную сетку,
омега-n. Ну она имеет такой вид. mH равняется x, n. n это 0, 1 и так далее до n большого.
h это шаг интегрирования L, то есть длино отрезка интегрирования делим на n.
Ну x0 у нас 0 и x с индексом n большой у нас есть L. Вот это наша расчетная сетка.
В данном случае она равномерная, но я вам говорил, что вообще говоря,
сетка совсем не обязательно должна быть равномерной. Более того, чаще ее делает
неравномерной в зависимости от разных причин. Ну одна из причин, например,
сетку можно сгущать в области градиентов решений. Ну и кроме того, не всегда область интегрирования
бывает такой хорошей, как в нашем случае отрезок. В многомерном случае это могут быть такие довольно
сложные области интегрирования, и так равномерные сетки там просто принципиально невозможны.
Так, и далее мы апроксимируем наши дифференциальные уравнения разностными соотношениями.
Давайте это сделаем.
Для внутренней точки у нас что получится?
1 на h, коэффициент диффузии или типа проводности в зависимости от физики задачи.
1 вторая, здесь у нас уn плюс 1 минус уn делим на h. Здесь пусть будет kn минус 1 вторая,
уn минус 1 делим на h и плюс k1n. Давайте первую производную тоже довольно простым соотношением приблизим.
Это не единственный способ дискотизации и апроксимации. Я пока довольно простой вариант выбрал.
Равняется g от n, где n меняется от 1 до n. Это дискотизация первого дифференциального уравнения.
Здесь можно задать вопрос, почему я первую слагаемую, это вторая производная решила апроксимировать следующим образом.
Представим себе, что у нас это узлы сетки n, n минус 1, n плюс 1.
Положим, что в точке n у нас есть разрыв каких-то механических свойств.
Например, биометаллическая пластинка нагревается, алюминий, какой-нибудь быстрый нагреваемый металл, трудно нагреваемый материал.
Как нам выбирать тогда коэффициенты типа проводности? Лучше всего выбирать в центре очерек.
То есть здесь n минус 1, n минус 1 и n минус 2. То есть в центре выбираем коэффициенты в центре очерек.
Тогда ничего страшного, если в точке n будет разрыв механических свойств.
Это внутренняя точка, теперь краевые условия.
Ну их давайте тоже я пока довольно простыми соотношениями апроксимирую.
Для примера можно их апроксимировать и более сложными соотношениями.
Мы об этом попозже будем говорить.
Так здесь у нас x равняется 0. Это левая граница и правая граница.
Альфа 2 у n минус 1, у n большое это крайняя точка, делим наш и плюс бета 2 у n большое, гамма n, здесь x равняется l.
То есть правая граница и так мы получили следующее разное соотношение.
Если вы повнимательнее поглядитесь, вы увидите, что то, что я выписал, это есть не что иное, как система линейных алгебраических уравнений.
Хотя есть весьма специфическая матрица, есть матрица такого специфического вида.
Давайте эту систему представим в более понятном виде для ее решения.
Уравнение первое я представлю в следующем виде.
Аn у n минус 1 минус bn у n плюс cn у n плюс 1 равняется dn.
Здесь n у нас будет меняться от 1 до n большого. Это внутренняя точка.
Левая граница. Условенно левая граница давайте мы вы пишем так.
b0 у 0 плюс c0 у 1 равняется d0. Здесь n равняется 0.
И условия на правой границе представим в таком удобном для нас виде.
a н большое у n минус 1 минус b н большое у n большое равняется dn.
То есть здесь n равняется n.
Вот такая система уравнения. Разумеется между коэффициентами a, b, c и k, k1, k2 имеется связь.
Ее можно вычислить. В этом уголке мы эту связь выпишем.
Она нам потребуется чуть попозже.
Ну, например, а н. Как ее можно представить через коэффициенты исходного дифференциального уравнения?
Следующим образом, а n это будет k малое, n минус 1 второе делим на h квадрат и минус k1n делим на 2h.
Ну, h всегда мы считаем малым параметром. То есть шаг сетки это всегда малый параметр.
Мы считаем он, разумеется, всегда положительный, но он всегда много меньше единицы.
Это малый параметр. Поэтому первое слагаемое много больше второго.
Поэтому а н мы можем представить как... То есть второе слагаемое просто мы можем пренебречь.
k здесь что у нас? xn минус h пополам на h квадрат.
Это вот а n. Точно так же мы поступаем с коэффициентом cn.
Он будет тоже приближенный равен k малое xn минус h пополам делим на h квадрат.
И, что касается правой части dn, это просто будет fn. Она практически не поменялась.
В дальнейшем нам потребуется еще оценка отношений вот этих двух коэффициентов а n и cn.
Скоро она нам потребуется, поэтому я ее выпишу заранее, чтобы два раза не делать еще одну и ту же работу.
Значит а н делим на cn. Приблизительно это есть k малое xn минус h пополам на k малое тоже xn плюс h пополам.
И если эти выражения раскрыть по формуле Тейлора до первого члена линейного, то их можно представить как единица плюс ch.
Ch тоже приблизительно, где ch много меньше единицы и больше нуля.
c это обычный коэффициент, мы его считаем о большой вот единице, а ch всегда это малое число.
Вот это сообношение нам чуть позже потребуется.
Так, это мы представили нашу систему в таком...
k малое, сейчас секундочку, k малое это вот этот коэффициент, а k малое вы это имеете в виду?
Это этот и есть этот коэффициент.
То есть здесь kn плюс 1 вторая, это есть xn минус h пополам, это есть n минус 1 вторая.
Да, давайте, нет-нет, если вопрос задан, значит он имеет...
Давайте его напишем тогда.
kn плюс 1 вторая, это есть k от x, если помните, в официальном уравнении k зависит от x.
Значит, это есть xn плюс h пополам, то есть это значение k в промежуточной точке.
То есть если я пишу вот такой коэффициент, то мы это просто писали, надо напомнить, в прошлой семестре.
Это означает, что значение функции берется в промежуточной точке между узлами.
Центральной точке между узлами m и n плюс 1.
Тогда это будет точка 1 плюс 1 вторая.
Это довольно часто используется в численных методах, когда точка берется в промежуточной.
Так, еще посмотрите, что еще непонятно.
Так, ладно, думайте, какой вопрос задать.
Так, теперь, значит, мы уже пришли к данной системе уравнений.
Если ее представить уже в нормальной матричной форме, то она будет иметь следующий вид.
Это некая матрица A, давайте A с волной, без волны потом потребуется, равняется D.
Значит, матрица A с волной имеет вот такой вид.
Значит, только три главные диагонали у нее не нулевые.
Остальные все элементы нулевые.
Значит, B0, C0 это левое ограниченное условие фактически.
Далее A1 минус B1, C1 многоточек.
Далее AN минус BN, CN это некая N строка.
Ну и нижняя часть AN минус 1, минус BN минус 1, CN минус 1.
Ну и правый ограниченный слой нам дает так, что AN большой минус BN большой.
То есть матрица вот такого вида.
В основном она нулевая.
В основном ее элемент равен нулю, но три главные диагонали не нулевые.
Что касается...
Ой, D это векторы.
Это по своему смыслу векторы.
Так, давайте я это затру.
Так, U это вектор U0 до Un.
Это как раз вектор русского функции.
Это наши коэффициенты.
И D это вектор совмест правой частей.
Вот D1 до Dn большого.
То есть имеем такую вот систему уравнений.
Такой специфической матрицы.
Так, как мы ее решаем?
Решаем ее следующим образом.
Так, ну пусть эта система слева у нас пока останется.
Пока останется.
Решение этой системы вот в таком виде.
Un-1 равняется Pn на Un плюс Qn.
Где Pn и Qn это так называемые прогоночные коэффициенты, которые нам нужно найти.
Ну и мы их естественно найдем.
Опираясь на краевые условия.
Ну а само это соотношение называется традиционно прогоночным соотношением.
Если мы это соотношение поставим в уравнение для внутренней точки,
то получим немного другое уравнение.
Уn равняется Pn плюс 1 на Un плюс 1 плюс Qn плюс 1.
Причем из этих двух уравнений мы можем вычислить значения коэффициентов.
Pn плюс 1 и Qn плюс 1 так называемые прогоночные коэффициенты.
Ну, они легко вычисляются.
Pn плюс 1 это есть Cn делим на Bn минус An Pn.
Qn плюс 1 это есть An на Cn Bn минус An Pn.
Это вот вид прогоночных коэффициентов.
Обратите внимание, что они вычисляются,
потому что только в правых частях стоят коэффициенты, которые нам известны.
То есть A it и B it и C it нам известны.
Следовательно, мы можем пройтись от n равного 0 до n равного n
и вычислить все прогоночные коэффициенты.
Вот это вычисление носит название прямой прогонки.
Это вычисление прогоночных коэффициентов.
Далее, если мы их вычислили, мы их вычисляем, естественно.
Идем с левой верхней части матрицы вправую нижнюю,
то дальше нам необходимо вычислить само решение.
Само решение мы, конечно же, будем вычислять, исходя из прогоночного соотношения.
Давайте возьмем последнее уравнение.
Уn-1 равняется pn на u, n плюс qn.
Последнее уравнение.
Уn мы считаем заданным в правом краевом условии.
То есть мы можем найти решение в точке ун-1.
Точно так же мы поступаем и с решением в точке ун-2.
Это будет pn-1 на u, n-1 и плюс qn-1.
Ну и так далее.
Дальше ун-3, ун-4 и так до u1.
То есть мы находим таким образом решение нашей задачи.
Обратите внимание, что сам алгоритм простой.
Два цикла.
Слева направо и справа налево.
И что особо важно, он имеет два важнейших свойства.
Первое.
В отличие от методогауса, легко оценить количество аризметических действий.
Это всего-то обольшое от n.
Когда мы имеем такую оценку количества аризметических действий,
такие алгоритмы принято называть эффективными.
Если n², n³, в математике начинают думать,
как бы сделать так, чтобы не было обольшого от n³ или n².
Нам нужно уменьшить порядок.
Ну и в основном это получается.
Есть даже оценки обольшого от логарифма n.
И второе, что алгоритм оказывается действительно удивительно устойчив.
Правда, для линейных задач и задач с переменными коэффициентами,
то есть с коэффициентами, зависящей от независимой переменной.
Для нелинейных задач в явном виде алгоритм прогонки не работает.
Но можно не линейные задачи�ry и мы об этом будем говорить редуцировать к алгоритмам прогонки,
к нескольким алгоритмам прогонки.
об этом мы будем говорили.
Это уже так называют итерационные процессы в функциональных пространствах.
Мы знаем, что есть иерационные процессы для нахождения.
Корни нелинейных уравнений, системы линейных уравнениях.
Но оказывается и в функциональных пространствах
можно с помощью итерационных процессов искать целые функции
и одномерные, и двумерные, и многомерные и так далее
об этом сейчас мы и будем говорить
а теперь вот какую я еще хотел сделать
немаловажную ремарку по поводу устойчивости
давайте все-таки поговорим об устойчивости этого процесса
об устойчивости процесса прогонки
так, ну вот давайте сделаем так
докажем такую теорию
по поводу устойчивости процесса прогонки
алгоритм прогонки устойчив при выполнении следующих условий
во-первых, это условия диагонального преумладания
с которыми мы с вами хорошо знакомы
и во-вторых, условия следующие
ну, p1, который вычисляется из краевых условий
он должен быть от 0 до 1
но на самом деле, если это так, то
в очень простом методоматематической индукции
доказать, что если это так, то и все pn будут лежать в интервале от 0 до 1
я это простую не буду доказывать
вы ее докажете без проблем без меня
мы будем на это опираться
разумеется, это так, если выполним условия диагонального преумладания
условия диагонального преумладения здесь необходимы
так, теперь давайте вот что сделаем
докажем вот эту теорию
давайте рассмотрим пока какой-то один коэффициент
то есть прямой процесс прямой прогонки
потом рассмотрим и обратную прогонку
да, давайте
сейчас мы говорим про алгоритм прогонки
вопрос в чем?
сейчас мы говорим о устойчивости метод прогонки
мы рассмотрим его на примере вычисления коэффициента pn плюс 1
как я выписывал, это выражение относится к pn плюс 1
это теоретически
в реальном компьютере, разумеется, есть погрешности
поэтому в реальном компьютере нужно написать так
pn плюс 1 плюс некая погрешность
delta n плюс 1
равняется cn
а здесь bn
минус am
на pn плюс delta n
и плюс некая погрешность epsilon n
что здесь что?
epsilon n это все ошибки, которые накапливаются при вычислении pn плюс 1 на одном шаге
то есть все есть ошибки в cн, bn, am
все мы их сюда вот вносим
в epsilon n
что касается delta n плюс 1
delta n это так называемая наследственная погрешность
которая накапливается при вычислении коэффициента pn
то есть при прямой прогонке
идем дальше
нас интересует как соотносятся pn плюс 1
delta n плюс 1 и delta n
нас интересует эволюция погрешности
будет ли погрешность возрастать
будет ли она убывать
либо она будет находиться на каком-то таком уровне
который нам не мешает решать задачу
это важнейший момент
если она будет возрастать экспоненциально
то надо нам выбирать другой алгоритм, другой метод
иногда говорят даже в крайних случаях другую модель
по процессу
давайте первое слагаемое разложим
в вариант Эвера до первого линейного члена
это будет cн
здесь у нас будет
bn минус
an pn
далее плюс
второй слагаемо
an cn
делим на
bn минус
an pn
в квадрате
здесь у нас будет малая величина delta n
и плюс
epsilon n
разложили до первого члена
теперь смотрим
давайте я сотру сверху
смотрим, что у нас на самом деле получилось
получилось на самом деле хорошее выражение
которое очень быстро упрощается
что такое первое слагаемое?
это есть не что ни было как pn плюс 1
вот оно у нас pn плюс 1
то есть мы их просто сокращаем
в левой и в правой части
у нас получается следующее
у нас получается следующее
delta n плюс 1
это наша погрешность на n плюс первом шаге
равна
an cn
здесь у нас bn минус
an pn в квадрате
на delta n и плюс
epsilon n
плюс epsilon n
здесь трудно не рассмотреть
если здесь знаменатель в квадрате
тот же, что и выражение для pn плюс 1
нужно возвести cn в квадрат
и это будет просто pn плюс 1 в квадрате
то есть это будет что у нас?
an делим на cn
а здесь cn в квадрате
и наш знаменатель
bn минус an
и pn в квадрате
ну плюс
давайте вместо epsilon n
оставим просто epsilon
имея ввиду, что epsilon n
меньше равняется epsilon и больше 0
то есть некий максимальный такой погрешность
оставим epsilon
то есть индекс m нам не очень принципиальный
для этой погрешности
ну а что это такое?
здесь очень просто разглядеть
выражение очень сильно упрощается
для эволюции погрешности вычислений
вот эта эволюция погрешности вычислений
она очень важна в любом вычислительном процессе
даже если вы работаете с дабл пресиженной
с какой угодноточностью
вы обязательно делаете ошибки
в многократных вычлениях
разумеется, если вы там 10 раз одно и то же
действия подряд делать, ничего страшного
чаще всего приходится
одну и ту же группу в аэспетических действиях
делать миллионы раз
и вот здесь вы можете
наступить на
острый подводный камень
то есть на большие погрешности
это означает, что dn плюс 1
меньше равняется, чем у нас
давайте я в скобках отмечу
an на cn, поскольку мы оценивали его
а здесь pn
плюс 1 в квадрате
здесь вот я
дельта n упустил
на тоже
погрешность на n
на n-м слой и плюс эпсилон
ну вот
теперь, посмотрите внимательно на это выражение
мы его, оказывается, можем очень быстро
и очень резко упростить
каким образом?
pn плюс 1 в квадрате
это величина, которая меняется от 0 до 1
an к cn
если вы помните, я совсем недавно
проводил оценки этой величины
и, как видимо, не зря
тогда мы можем
написать, что это есть
единица плюс ch
на дельта n
и плюс
и плюс эпсилон
дельта n и плюс эпсилон
и вот так мы упростили наше выражение
для эволюции погрешности
ну теперь давайте вот что сделать
нам бы хотелось, конечно, знать
как зависит погрешность на n-м шаге
от нуевой погрешности
от дельта 0
это тоже можно сделать
совсем несложно
ну давайте я уже сотру
исходные расслабления
мы их уже знаем наизусть, наверное
так
что у нас получится
ну давайте вот такую
лестнику сделаем
дельта 1 меньше не равняется
единице плюс ch
ch, допоминаю
величина много меньше единицы
и больше 0
значит, на дельта n
на дельта n
плюс
ну здесь уже дельта 0 будет
и плюс эпсилон
идем дальше
дельта 2
меньше не равняется единице плюс
ch
в квадрате на дельта 0
здесь будет эпсилон
единица плюс
ch
ну дельта 0
то есть я поставил дельта 2
меньше равняется единица плюс ch
дельта 1
ну а вместо дельта 1 я дал верхнюю цинку
тогда у меня получается здесь квадрат
а здесь вот накапливается некая сумма
ну давайте
чтобы было до конца все понятно
еще дельта 3 допишу
дельта 3 меньше не равно
единица плюс ch
в кубе на дельта 0
плюс эпсилон
а здесь у нас накапливается вот такая сумма
единица
единица плюс ch
единица плюс
ch в квадрате
так ну и так далее
и так далее до
дельта n
значит дельта n будет оцениваться следующим образом
единица плюс
ch
в степени n
на дельта 0
плюс эпсилон
и здесь накапливается как вы видите
геометрической
прогрессии
и он прекрасно известен
единица плюс
ch в степени n
минус 1
ну если есть
геометрическая прогрессия
то ее всегда можно просуммировать
как вас учили в школе
и получить
окончательную оценку
которая нам очень важна
с точки зрения
оценки устойчивости
процесса прогонки
или как
в американской терминологии
алгоритмом Томпсона
так значит что это будет
ну давайте я сюда
принесу наверх
значит
единица плюс
ch в степени n
на дельта 0
ну а здесь эпсилон
на сумме геометрической прогрессии
ch в степени n
минус 1
и внизу единица плюс ch
минус 1
меньшее равне с единичкой
мы здесь предъебрежем
поскольку здесь величина положительная
получим единица плюс ch
в степени n
на дельта 0
ну и здесь видно
эпсилон на ch
на единица плюс ch
в степени n
что это такое
это есть
давайте единица плюс ch
вынесем за скобки
в степени n
в скобках остается дельта 0
плюс эпсилон
на ch
ну и поскольку ch величина малая
мы просто эту скобку
степенной функцию можем представить в виде экспонента
е в степени ch
n
а здесь дельта 0
плюс эпсилон
хорошо
вот это окончательный
итог наших всех
рассуждений
что отсюда мы видим
то есть как эволюционирует
наша погрешность
так здесь у нас давайте
напишем дельта
n плюс 1
чтобы было понятнее
ну в скобках дельта 0
величина малая
эпсилон это величина порядка машинной ошибки
ch всегда много больше
величины
машинной ошибки
по крайней мере h нужно выбирать так чтобы
эти величины были много больше машинной ошибки
ну если вы помните еще где-то
в сентябре мы с вами говорили
и до семинарх видимо вы в лабораторной делали
что например если мы
чисто дифференцируем какую-то функцию
ну например в простейшем образом
с первым порядком процемации
то шаг выбирать близко
к машинной точности
ошибка будет накапливаться
здесь аналогичная ситуация
то есть всегда нужно чтобы шаг
был много больше
машинной погрешности
то есть нельзя убирать
там скажем так бесконечно мало
ну бесконечно мало
теперь экспонента
да экспонента всегда смущает
потому что экспонент означает
накопление погрешности
но здесь у нас какая ситуация
c h n
c x n
c это величина всегда порядка
у большого от единицы
x n ну для краевых задач
в отличие от задач каши
все-таки всегда-всегда этот интеграл
интервал интегрирования ограничен
обычно тоже в больших случаях
порядка у большого от единицы
но не всегда
когда я говорю о жестких краевых задачах
некорректных то это вот как раз
задачи да их не очень много
они не часто встречаются
задачи вычислительно некорректные
то есть такие когда вот эта
экспонента может
расти может расти
таких задач немного но они
встречаются подавляющее в большинстве
вот оценка эволюции
погрешности имеет такой вид
ну в общем она где-то
порядка у большого от единицы
то есть это вот нормальная погрешность
она растет
не сильно либо вообще
не растет
есть доказательства
прямой прогонки разумеется то же самое
я могу написать
для второго коэффициента прогоночного
это q
n плюс 1
это a n
на q m
на q m минус
d n
делим на знаменатель
b n минус a n
b n
те же самые
в рассуждении можно проявить и тоже
оценить что
будет вот какая
погрешность для этого коэффициента
но это не буду делать это все аналогично
то есть прямая прогонка в общем-то
мы видим процесс устойчивый
что касается обратной
прогонки
что касается обратной прогонки
каким будет
этот процесс
будет ли он устойчив
ну это просто следует
из прогоночного соотношения
еще не закончили
я сказал
про устойчивость прямой прогонки
это половина
теоремы но очень важная
что касается обратной прогонки
то ее устойчивость следует
следует прямо из прогоночного соотношения
я его напишу и сразу будет понятно
у
n это что у нас
это есть p n плюс 1
на дельта
n плюс 1
и плюс q
n плюс 1
это у нас
так описывается обратная прогонка
в реальном компьютере это u n плюс
delta n
равняется
p n плюс 1
а здесь delta n
плюс 1
так
простите
здесь у нас u n плюс 1
u n плюс 1
давайте
последнее будем делать
u n плюс 1
плюс delta
n плюс 1
плюс q n плюс 1
и
суммарная погрешность
я описывал
n малая
суммарная погрешность тех вычислений
на n шаге
ну а delta n это у нас
как я говорил у нас следственная погрешность
опять смотрим
u n это есть
p n плюс 1
на u n плюс 1 и плюс q n плюс 1
сразу все сокращается практически
остается
delta n
равняется
p n плюс 1
на delta n плюс
1 и плюс
q n
вот что у нас остается
при этом вы хорошо знаете что
p n плюс 1 лежит на интервале
от 0 до 1
все доказательства закончены
вы конечно скажете
а как влияет q n
и t слова
мы только что показывали
предоказательств устойчивости
по είому прогонки как она влияет
то есть прагонка также
будут устойчивы
прогонка устойчива
при выполнении
naw discretion
зачем я говорю
address
только трехточечные и не только вот такого вида, о котором я говорил. Сейчас я
говорил о прогулке, которая идет как бы слева направо, с левой межней части
матрицы в правую нижнюю, на самом деле можно делать обратную прогулку, то есть
вычислять коэффициенты прогуточные справа-налево. Это тоже будет
совершенно законная прогулка. Более того, можно делать процесс встречных прогулок,
то есть одновременно вычислять коэффициенты и слева сверху, и справа внизу, и в
Тайчаке где-то в центре матрицы. Называется процесс встречных прогулок.
Другой вопрос и очень важный, что и когда нужно делать. В основном, конечно,
делается именно прогулка в таком виде, в котором я вам ее представил.
Это суммарная погрешность, которая допускается при всех анимитических вычислениях на ином шаге.
То есть у нас же погрешность везде есть. И когда у n, и по n, и коэн вычисляем, везде у нас есть погрешность.
Сумма в сумме мы берем эту погрешность и объединяем в epsilon n, чтобы не возиться...
Нет, это вот правильный коэффициент, а погрешность, которую мы допустили при его вычислении, вот здесь сидит.
Мы все эти погрешности для при вычлении qn плюс 1 и pn плюс 1 загнали вот сюда, в epsilon n,
чтобы не возиться с лишними обозначениями.
Все понятно? Просто это обличение нашей жизни.
Как вы предполагали, написать qn плюс 1 и pn плюс 1 плюс epsilon, здесь можно написать и так далее.
В принципе, мы получим точно тот же результат, но здесь я просто пишу гораздо меньше зачков, чем в первом случае, это экономит время.
Могу сказать, что прогонка может быть прямая, обратная, может быть встречная, но могут быть прогонки не трехточечные, но, например, пятиточечные.
При решении дифференциально уравнений четвертого порядка... Нет, не точность, четвертого порядка.
Об этом чуть позже. Сейчас важный момент следующий.
Теперь перейдем к задачам нелинейным.
Прогонка прекрасного алгоритма хороша для нелинейных задач либо для задач с переменными коэффициентами.
Конечно, есть большое желание свести решение нелинейных задач к этим простым и устойчивым алгоритмам.
Это, в общем-то, удается сделать.
Давайте посмотрим, как.
Давайте я упрощу немного наше уравнение.
Напишу так, у, вторая производная, неизвестная переменная, равняется f у, где у это уже правая нелинейная часть.
Это, в принципе, она тут же нам портит всю жизнь.
Нелинейность, но самый интересный, как я говорил, процессы, и физики, и механики, и медицине, и в экономике, как правило, нелинейные.
Ну, не как правило, есть, конечно, линейный процесс, но, в основном, интерес представляет нелинейный процесс.
Ну, и условия на границах пусть будет у от 0, пусть, например, равно d малое, а у от l пусть будет d большое.
Давайте аппроцимируем нашу производную вторую.
Мы научились это сделать в прошлом семестре.
И будет следующий вид иметь, om-1-2, om-n-1 делим на h квадрат, и это равняется fn.
fn меняется от единицы до m-1.
Так, и у 0 внизу, это есть у нас d малое, у l внизу, это есть d большое.
Вот так.
Ну, и в операторном виде, вот этот оператор, аппроцимирующий вторую производную,
обычно такое уже каноническое представление, это лямбда xx, а у лямбда равняется fn.
То есть, этот оператор, разница, представляется, обычно обозначается следующим образом,
лямбда xx.
Это стандартное такое обозначение принятое.
И вот что мы хотим сделать.
Давайте сначала рассмотрим первый метод стрельбы.
Он использовался активно, где-то еще 100 назад, например, в задачах в баллистике.
Есть такая задача, сейчас, конечно, все гораздо более точнее, и проще стрельба по закрытым мишеням.
То есть, есть пушка, перед ней гора, и нужно попасть в мишень, которая находится за горой.
Эта задача решалась как-нибудь странным методом стрельбы.
Правда, конечно, это не только это.
Но вот как это делается?
Как нам свести задачу краевую к решению f задачи каши?
Ну, а как решается задача каши, мы знаем.
Например, медленный ангел кота.
Это уже, так сказать, проблем не вызывает решение такой задачи.
Однако у нас чего не хватает, чтобы мы задачу каши решили.
У нас вторая производная, то есть три неизвестных, и только одно начальное данное.
Поэтому нам нужно добавить еще какое-то данное, но мы его не знаем.
То есть, если мы вот такую графику нарисуем, здесь будет у нас x,
здесь у нас скажем 0, здесь у нас 0, где-то здесь и правая граница.
Пусть у нас здесь вот начальное условие на левой границе, а здесь направое.
И да малое, и да большое.
Положим, что вот такая кривая, это наше точное решение, которое мы не знаем, которое нам нужно найти.
Нам хотелось бы начать эту задачу, решение задачи, задачи каши.
Оттолкнуться от точки до 0 и попасть в какую-то другую точку.
И потом таких кривых нарисовать много.
А потом из них уже получить наше решение.
У нас это не получается.
Поэтому мы вводим такую функцию, как α1, например,
которая равняется ау1-у0 и 9 а.
То есть, это аппроксимация первой производной с первым порядком точности.
Но ау1 мы не знаем, поэтому а1 мы тоже не знаем.
Мы просто берем какое-то значение а1.
Но, разумеется, как правило, когда вы решаете задачу, то в ней есть какой-то физический смысл.
Поэтому а1 с какой-то степенью точности всегда можно определить ее пределы.
Вот мы берем, если а1, то мы можем уже эту задачу начать решать.
То есть, у нас есть у0, есть у1.
Можно решать задачу каши.
Ну, положим, мы ее решили и попали вот сюда.
У, давайте так обозначаем, у эльфа, у эльфа вот а1.
Вот а1 в эту точку опали.
Это задача каши.
Куда опадаем?
Далее берем какое-то другое значение а2.
Из того диапазода значение а, которое мы оценили, априорно до решения задачи.
Всегда перед решением задачи делаются априорные оценки и решения, и параметров, и так далее,
чтобы не попасть в трудную ситуацию.
До а2.
Вот положим, мы с помощью а2 попали сюда.
Это будет у эль, вот а2.
Ну, дальше и так далее до некого ак.
Вот у эль, вот ак.
И так мы достреляли, например, ну и так далее.
Последний давайте сделаем, ак.
Ак.
Вот мы достреляли, ка.
Задачу каши до решали и получили ка.
Нет, это...
Ну, положим, вы, смотрите, оценили, что все альфа-каты у вас лежат в диапазоне от 0 до 1, да?
Ну, вы берете а1, например, там 0,1, а2, 0,2, ну и так далее.
А там, скажем, там 9, да?
Ну, 0,9 и так далее.
То есть вы выбираете вот альфа с каким-то шагом в вашем диапазоне.
Диапазон, ну, как правило, в реальных задачах вы можете оценивать.
То есть он, как правило, от минус до плюс без кодичности не лежит в диапазоне.
Как правило, он все-таки ограниченный, вы можете его хоть грубо, но оценить.
Поэтому а1, а2, ак, это не зависит друг от друга.
Это просто числа, которые вот в вашем диапазоне лежат.
Дальше мы что делаем?
Что мы получили, на самом деле?
Так, сейчас пундочку, где у нас?
А вот три, да?
Так, что мы получили?
Что мы делаем дальше?
Из этих n-кривых.
Из этих n-кривых.
Вот что мы получили.
Мы получили функцию ul от αк.
И нам бы хотелось, чтобы, если мы из нее вычитаем правое условие d,
чтобы оно было с какой-то точностью равно 0.
Ну, разумеется, с какой-то точностью.
С какой-то задной точностью, не обязательно с машиной.
Точность вы задаете сами.
Но я здесь, конечно, очень смело написал знак ранчества.
Правильнее написать знак приближенного ранницы,
поскольку всегда будет эта ранница выполняться приближенно.
Нам бы этого хотелось.
Ну, либо можно написать даже так.
Функция от α, это есть ul от α минус d.
Нам бы хотелось, чтобы она была равна 0.
А что я написал, на самом деле, вот здесь?
Я просто написал нелинейное уравнение.
Нелинейное уравнение вы решать умеете.
Например, какие методы есть для секущей?
Есть такой метод еще?
Ну, очень знаменитый метод.
Как?
Newton, конечно.
Поразительный метод.
Это один из самых поразительных методов,
наверное, во всей математике.
Может, не только в математике.
Как, например, Newton есть в механике,
поразительная вещь.
Когда Newton его предложил в конце XVII века,
примерно тогда же он предложил свой метод решения
нелинейных уравнений.
И он до сих пор не просто живой,
он активно живой, этот метод.
Сколько ему лет, там уже юбилей нужно праздновать,
ставь, много столетний,
он до сих пор живой.
Его совершенствуют, его ускоряют.
Но он остается методом Ньютона.
Как и второй закон Ньютона.
Поразительная вещь совершенно.
Но если вы забыли, я просто напомню,
как он выглядит.
В данном случае, да.
Сейчас будет тряпка более мокрая.
Альфа К плюс один.
Это есть альфа К минус
f от альфа К делено
f' по альфа К.
Это метод Ньютона.
Разумеется, вот вы сказали правильно,
метасекущий как раз означает,
что производная у нас
не анонтически вычисляется численно.
Поскольку альфа К у нас задана
по точке этой точки.
Поэтому здесь нужно вычислять ее численно.
Производную.
Например, вот по такому простому
соотношению.
Это вот что касается метод Ньютона.
Но можно подойти к этой же даче
и немного по-другому.
Из другого, как говорится,
до конца.
Что мы имеем в краевой задаче?
Мы имеем, например,
ряд прецедентов.
У ноль,
это левое граничное условие,
и у ль вот альфа К.
Это то, что мы настреляли
на правой границе получили.
С К меняется от единицы там
до К большого.
Одна вот такая пара
называется прецедентом.
Вот совокупность пар
таких, как называется,
не помните, я как-то вам говорил?
В терминологии машинного обучения
это называется
обучающая выборка.
Совокупность вот таких прецедентов.
Сама машинное обучение,
если говорить о более
правильных названиях,
это есть обучение
по прецедентам.
то есть у нас есть много прецедентов
и на основании их мы находим решение, разумеется, с некой
степенью точности
ну а если мы имеем обучающую выборку, то нам нужно составить целевую функцию
целевая функция, естественно, просится
мы с вами об этом говорили
в виде f квадрат альфа
почему f квадрат?
потому что минимум этой функции 0
и мы хотим
найти альфа, при котором эта функция
минимальна
то есть 0
для этого мы можем использовать
ну любой есть метод оптимизации, о которых мы с вами говорили
альфа-к в данном случае это есть
аргумент
минимума
функции f квадрат по альфу
ну если вы помните эти названия
метод наискорейшего спуска, метод градетного спуска
и так далее
мы с вами говорили о методах оптимизации
аппарат
машинного обучения, это есть
два аппарата, о которых мы с вами проходили. Первый аппарат это теория
приближения функций
мы с вами говорили, второй- это метод оптимизации, о которых мы
с вами тоже говорили. То есть, я свел нашу задачу краевую
к задаче машинного обучения
на самом деле мы с ним периодически будем встречаться
всё новое хорошо забыто с той стороны
интерполяция и экстраполяция
мет на имеющих квадратах это все было переименовано в методы машинного обучения
на самом деле и даже краевые задачи тоже можно представить как метод машинного обучения
фактически это метод простой итерации
который можно представить как метод машинного обучения
но есть конечно и более продвинутые по скорости методы мы обращаемся опять же к великому дилтону
вот здесь у нас будет тоже уравнение второго порядка нелинейного
в котором мы начинали и когда я рассказал о методе стрельбы
лямбда хх ун равняется f от ун
давайте распишем производную ун минус единица минус 2 ун плюс
этот оператор это уже не производный это уже оператор разосный ун плюс 1 делим наш квадрат
и равняется f под ун ну разумеется начальные данные нам известны но смотрите здесь
три неизвестных да то есть нам очень хотелось бы очень свободительно использовать метод прогулки
но действительно три неизвестных да то есть трех диагональный матрица все здорово что нам
правая часть она нелинейная поэтому не можем применить метод прогулки но несколько секунд до
размышления что можно придумать как все-таки можно использовать алгоритм прогулки для решения этой задачи
итерационные методы молодец конечно конечно методы последовательных
приближения то есть нам нужно задать некую начальную функцию которая назовем начальное
приближение и от нее отталкиваться я рисовал эту картинку да я не буду ее повторять вот скажем
наше точное решение и положим мы из каких-то соображений физических оценили начальное приближение
этой функции ну в данном случае конечно напрашивается прямая самая простая начальное приближение вот
то есть мы уже переходим чему и в медле стрельбы и в этом методе о котором сейчас будем говорить это
же мы находим в функциональных пространствах функцию функцию ну разумеется в точную виде то есть
мы уже работаем функциональных пространствах ну поэтому вообще говоря вот один из главных
математических аппаратов в учительнике математики это функциональный анализ вот ну я конечно упрощает
его определение чтобы было проще ну положим вот взяли некую начальную функцию давайте обозначим
ее фи от о ну скажем вот ну от у 0 вот пусть она будет такая и мы хотим найти начальное приближение
как мы это будем делать вот давайте это разнос со соотношения напишем так вверху будем вставить
значок один у н минус один один где один это будет следующее приближение так далее
у н плюс один один на аш квадрат а здесь будет у нас что ф от у н 0 а у н 0 это наша функция фи от
другой против прощения я написал x и это это функция фи от икс азумеется
фи от икс так это наше приближение первое пожалуйста здесь мы можем использовать прогулку для
нахождения первого приближения у первого все нам здесь все известно поскольку первое приближение
это функ? и дальше мы тоже самое делаем зная первое приближение здесь мы уже ставим
давайте я вам перператором видно пишу у 2 второе приближение находим здесь будет f от у 1 ну и так далее
и в конечном итоге у нас получается такой-то рационный процесс лямбда x икс у и плюс один
н равняется f от у и который заканчивается например за одну точность например у и плюс один минус у и по
норме меньше н равняется некой заданной точностью т.е. мы решаем эту задачу
а что чтобы стал линейным мы всего лишь задали начальное приближение некую функцию например
линейную я сказал например можно в любую функцию узнаете известную в данном случае
она напрашивает само собой и тогда вы решаете значит альгаритм прогонки доходит и первое
приближение ну например оно будет каким-то таким таким-то таким
в первом приближении мы пишем уже вот эти самые краевые точки которые нам заданы в
краевых условиях все мы уже не задачу каше решаем мы решаем задачу на основе алгоритма прогонки
welder
вот мы написали эту функцию нам известно вот эта функция у 0 от x это наша fluorite
agon function ciyat x то есть правая часть нам известно это приближLink это
начальное приближение оно неправильное это не точное решение это начальное приближение
но мы нам известно мы его задача а здесь у нас три известных это что у нас получается
Это система уравнений линейных с трехдевинальной матрицей, то есть та же прогулка.
А это правая часть прогулки. То есть алгоритм прогулки, который мы сейчас подробно разбирали,
мы просто здесь используем для нахождения первого приближения.
Да, это та функция, которую вы задали в качестве начального приближения нашего исковного решения.
Потом уже у1 вы поставляете правую часть, находите у2, это второе приближение,
и дальше вы получаете такой трорационный процесс.
То есть вы делаете n прогонок. Н прогонок пока не выполнится в условиях вашей точности.
Это будет метод простой итерации в функциональных пространствах.
Вот так сейчас. Но его, конечно, нужно ускорить. Его можно ускорить.
Вот здесь я уже напишу этот итерационный процесс в явную форму.
Вот здесь поставим у1, а здесь у2. То, что вы как раз писали.
Ну и задается обязательно начальное данное, начальное приближение.
Вот это и есть итерационный процесс в функциональном пространстве.
То есть мы итерационным образом и в медном последовательном приближении ищем уже функцию.
Посмотрите еще раз, это понятно или нет?
Вообще процесс аналогичен процессу поиска корня нелинейного равнения, но не совсем.
Здесь мы находим n точек. Не одну точку, а n точек.
Ну или там ка большой точек я обозначил.
Мы находим функцию по точкам.
Дальше мы ее можем делать непрерывно, например, с помощью оператора интерполирования.
То есть любая точная функция делается непрерывно с помощью оператора интерполирования.
Которую вы выбираете тоже с необходимой вам точностью.
1, 2, 3 порядка и так далее.
Лучше всего с планом интерполирования.
Оно самое надежное.
Хорошо.
Теперь смотрите.
На самом деле есть некоторые мелкие недостатки.
Помните условия, которые нужно хорошо помнить?
Выбора шара интегрирования для функции средней или правой части.
То есть мы будем ограничены.
Если, например, у нас будет норма этой мальцы с якоби большая, то шар по времени тоже будет ограничен.
Как с этим справиться?
В общем, на самом деле так же, как мы справлялись и ранее.
Смотрите, что мы можем сделать на самом деле.
Здесь я поставил значок i.
Вот если бы я поставил значок i плюс 1, у нас была бы явная схема.
И проблемы с выбором шара h не было.
Но что у нас получилось?
Я здесь написал ничто иное, как функцию нелинейного равнения.
То есть нам придется решать итерационным образом нелинейного равнения на каждом шане.
Это усложнит вычисление, то есть увеличит количество алимптических действий.
Как можно сделать так, чтобы количество алимптических действий все-таки уменьшить, а не увеличить?
Три секунды для размышления.
Какие идеи будут?
Погромче.
Вопрос, как вот это нелинейное уравнение решить так, чтобы не увеличивать количество алимптических действий, а уменьшить?
Еще раз?
Хорошая идея, но пока подождем.
Пусть будет пока сетка неравномерная.
Помните, как называется метод Ньютона?
По-другому.
Касательных или метод линейно-оризации?
Собственно говоря, Ньютон всего лишь линейно-оризовал функцию и получил совершенно гениальный метод по скорости сходимости.
Что мы можем делать?
Мы же эту функцию тоже можем линейно-оризовать.
Как f от u и плюс 1 мы представим, давайте оставим не только линейный член,
f от u и t плюс f' по u от u и t на u и плюс 1 минус u и.
Вот я линейно-оризовал эту функцию.
И смотрите, если я эту линейно-оризацию функцию ставлю в правую часть, у меня i плюс 1.
Где остается? Вот она остается.
Но это уже слагаемо, мы можем перенести в левую часть.
Оно нам уже не испортит прогонкой, это линейное слагаемое.
И мы спокойно делаем прогонку.
То есть найти первую террацию.
Но уже в правой части у нас будет стоять линейно-оризованная функция.
Этот метод называется метод Ньютона или метод квази-линиоризации в функциональных пространствах.
Идея Ньютона используется, но тот-то метод Ньютон точно сделал.
Предложили он этот метод, я не берусь утверждать, вполне возможно.
Но идея Ньютона, идея линиоризации не в линейной правой части, она оказалась очень плавотворной.
Она используется до сих пор очень активно при решении очень новых задач.
А в ненейности в подавляющей большинстве задач присутствует.
Так что вот Витя Ньютон, он не только физик, он...
...эф-штрих?
Эф-штрих?
Ну, эф-штрих тоже нам известна на этой террации.
Мы считаем, что она нам известна.
Она может быть известна не аналитически, а по точкам ее может приблизить.
Но она нам известна, эф-штрих.
То есть она не будет там какая-нибудь волшебная функция?
Ну, она нам известна.
Поэтому мы вот решаем.
Вот этот...
Почему эф-штрих по У будет проще, чем...
Почему эф-штрих по У будет проще, чем решение вот этого ненейного уравнения?
Этот вопрос?
Ну, вопрос правильный.
Но я хотел бы его ополирать вам.
Возьмите какую-нибудь простую задачу с кадровым уравнением и решите.
Вот методом простой итерации левое уравнение и методом Ньютона его же.
И сколько итераций будет в первом случае и во втором?
Вы увидите, что в случае метода Ньютона, ну, если функция не какая-нибудь очень хитрая, экзотическая,
то вы увидите, что метод Ньютона, конечно, очень экономичный метод.
Но он тоже, как правило, вот забирает буквально несколько итераций.
В несколько итераций вы находите решение вашей нелинейной задачи.
Вот, то есть, этот метод линеризации.
Так.
Теперь сколько у нас осталось времени?
Время осталось немного.
Так, ну и вот тогда остался время.
Ещё один метод вам интересный расскажу.
Его вообще-то относится к методам приближённым.
Нечисленно приближённым.
Но это не очень важно в термологии.
Метод в хуне.
У нас есть вот аппроксимация нашего уравнения второго порядка.
Ну, давайте для простоты UL возим в равном UL.
Понял?
И вот какая интересная штука получается.
Оказывается, вот этот оператор λх имеет собственные значения, собственные функции.
Я сейчас не буду делать примерно то, что вы получаете.
Что это означает?
Это означает, что λх можно на некоторый диаметр ω.
Это есть λ.
Тоже на этот диаметр ω.
Прекрасное вам известное соотношение с первого курса.
Оказывается, имеет этот оператор собственные числа.
И они известны.
Ну, как это можно поверить?
Прямо поставим непосредственную постановку в это уравнение.
4 на h квадрат на sin квадрат π к h на 2L.
Это собственные значения и собственные функции.
Кроме квадрата k от xl, это будет у нас в нормированном виде 2d9 квадрат плюс 2dL.
А здесь sin pknh на L.
Ну вот.
Ну а что мы делаем, если у нас есть базис из собственных векторов?
Зачем нам нужен вообще базис из собственных векторов?
Это мы говорили в конце прошлого семеста.
Для того, чтобы приближать функции.
И получать приближенные решения в виде разложений по базе собственных функций.
Вот. Мы это и можем сделать.
И что у нас получится?
Получится вот что у нас.
Решение мы представляем у КТ в виде разложений по базису из собственных функций.
Коэффициенты СКТ нам пока не известны.
Их мы называем коэффициентами Fourier.
Ну к меняется от 1 до n-1.
То же самое мы сделаем с правильной частью.
fKT мы тоже разложаем в таковый вариант.
Ну коэффициенты Fourier давайте так ободначим по-другому.
f с крышкой.
Чтобы fKT.
Ну а это ωKT это наши собственные векторы от xn.
Так если это мы так делаем.
Подставляем эти уn и правую часть вот в это уравнение.
Подставляем. Что получаем?
Так. Подставляем. И что мы получаем?
Получаем следующее.
λх.
Здесь у нас что будет?
Сумма СКТ на ω.
Я уже немного сокращу и допишу.
Что?
А, собственные функции?
Пока давайте так. Вместе они берутся откуда-то.
Просто если я уйду в Волгебург, мы с вами будем заниматься немного другой наукой.
Значит смотрите, я написал решение задачи собственные значения и собственные функции.
Вы можете их поставить в это уравнение и проверить.
Они будут сопроводять.
Пока будем считать так, что я их угадал и можно проверить.
Разумеется, они находятся.
Это просто бы выдел в другой область немного.
Поэтому пока поверьте и можете проверить.
И справа у нас будут что?
Правая часть FКТ на ω.
Теперь давайте этот знак оператора я внесу под знак SUM.
Под знак SUM.
Вообще-то этот прием в математике существует.
Когда угадывается решение, потом ставится в уравнение оператора
и тем самым доказывается его справедливость.
Хотя это не часто применяется.
Под знак SUM что получится?
Получится СКТ здесь λхх на ω КТ.
А здесь SUM FКТ на ω КТ.
Что такое λхх на ω КТ?
Говорите мне.
Я уже пишу.
Это есть λКТ на ω КТ.
Просто значение на собственные функции.
И равняется FКТ на ω КТ.
Отсюда мы находим коэффициент фурье.
СКТ это будет FКТ на λКТ.
Коэффициент фурье мы решаем задачу.
Но разумеется, я не слышал вопроса, а сколько же чайных ряда нужно брать.
Это вопрос важный.
Зависит от вашей точности.
Может оказаться так, что количество чайных ряда нужно брать большое или даже очень большое.
Но этот метод, его обычно приближенным, нечисленным, очень популярен во многих областях.
Особенно физических.
На сегодня давайте закончим.
До свидания.

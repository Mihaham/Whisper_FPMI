Начнем. Что сегодня, соответственно, про что сегодня разговариваем. Во-первых,
продолжаем обсуждать стахастическую оптимизацию. Снова вернемся к методам редукции дисперсии,
потому что в прошлый раз посмотрели только на них с точки зрения каких-то идей и интуиций. Сегодня
уже строго покажем, что это действительно все безобразие работает. На второй части лекции
поговорим про координатный спуск. Здесь я вам просто напоминаю, что у нас было в прошлый раз.
Рассматривали с вами постановку вида конечной суммы. Рассматривали, на самом деле, не только ее,
но более общие постановки стахастические. Но как раз во второй части лекции, когда говорили
про методы редукции дисперсии, остановились именно на вот такой вот постановке, когда у нас
целевая функция имеет вид конечной суммы. Полный градиент функций считать дорого,
поэтому мы считаем только градиент по какому-то слагаемому. Эта слагаемая часто берется просто
случайно и равномерно. То есть, генерируются какой-то индексы, который как раз нам и говорит,
какой мы слагаемый выберем на данный момент. И в среднем, как вы понимаете, как мы с вами уже
показывали, это будет действительно просто такой вот честный градиент. Но опять же, с дисперсией этого
всего безобразия никто вам не гарантирует, что с ней будет все хорошо. Она может быть довольно большой,
довольно большой. В связи с этим именно для задач конечной суммы придумали методы редукции
дисперсии, редукции дисперсии, о которых мы уже с вами поговорили. Идея у них была такая,
что нам нужно использовать какой-то стахистический градиент g. И суть этого стахистического градиента
такая, что он стремится к честному значению функции f в точке х со звездой, которая стремится к нулю,
если у нас, соответственно, процесс решения задачи стремится к х со звездой. И тогда все хорошо,
тогда проблем с этим стахистическим градиентом не должно быть. В частности, мы с вами привели
пример, это метод сага. Напоминаю его интуицию, то, что здесь мы запоминаем градиенты по бочам,
которые посчитали. То есть посчитали градиент f и того, сохранили его в переменной y, y и t. И вот
тогда в итоге у нас в этих y, точнее в усредненном значении, будет копиться такой усредненный
градиент. Это все обсуждали, почему это хорошо. Сегодня, соответственно, докажем, почему это
действительно хорошо с точки зрения теории. Но интуиция была понятна уже в прошлый раз. Говорили
про это, то, что мы сохраняем то, что у нас есть. Здесь у нас в y в итоге сохранится такой запаздывающий
градиент. В среднем это будет честный градиент и, соответственно, будет как раз вот это замечательное
свойство. Так как у нас в итоге, если x ката стремится к созвездой, то в y у нас значение
будет стремиться к значению градиента конкретного f житого в точке x и звездой. И тогда вот средний
y будет стремиться к градиенту в точке x и звездой, что стремится к нулю. Тогда стахистический градиент,
который мы используем, тоже будет стремиться к нулю, что хорошо и у нас пропадут асталляции.
Проблема с памятью мы тоже обсуждали, но давайте теперь подоказываем. Все это безобразие, потому что
пока мы все это делали в некотором смысле голословно, а хочется сделать это по-честному. Мы предполагали
то, что у нас есть сходимость x как со звездой, но кто же знает, потому что метод настолько плохой,
что он этого не гарантирует, поэтому вся его физика идет крахом. Ну давайте посмотрим,
что там получается. Опять же начинаем супер уже знакомо. Расписываем, как у нас ведет себя
расстояние до решения. Потом подставляем сам метод и получаем что-то вот такое.
Дальше вы мне подсказываете, что мы делаем, чтобы как-то анализировать стахастику. Что делаем,
чтобы сейчас со стахастикой работать. Пока непонятно, что с стахастикой делать. Что надо
сделать-то? От ожидания. Берем условное математическое ожидание. Можно было полное,
но по-хорошему лучше условное, которое как раз у нас зафиксирует всю предысторию,
которая происходила до текущей итерации. Как мы с вами это уже помним, на случайной величины
в данном случае будет xкат и плюс один, поэтому на него это условное мат. ожидание будет действовать
дальше. Вот эта скобочка у нас первая. С xкат случайной с точки зрения этого математического
ожидания не будет, поэтому я его сразу не накидываю, просто ее переписываю. Дальше xкат это
тоже случайная величина, потому что она как раз генерируется на текущей итерации, поэтому на нее
условное мат. ожидания действует. А то, что стоит второе в скалярном произведении, это как раз не
случайная величина, поэтому я сразу условное мат. ожидание занес под скалярное произведение.
Дальше соответственно так. Опять же xкат у нас случайная величина, поэтому норма xкат в
квадрате тоже нужно промат. ожидать. Окей, ну давайте я здесь еще сразу же занесу вот этот нашумный 0,
f от x звездой. Градиент f от x звездой. Ну понятно, в оптимуме наша вся целевая функция дает 0,
так как мы решаем с вами безусловную задачу. Окей, с чем там надо обычно разбираться? Первое,
это нужно разобраться с мат. ожиданием xкат условным. Ну я на самом деле ответ уже написал на предыдущем
слайде. Ну давайте как раз покажем, что это будет несмещённой вещью. Окей, давайте разбираться,
просто подставляем xкат, что там у нас было. Кстати, ну давайте тогда, раз уж заговорили
при этом мат. ожидания, предлагается тогда немного посмотреть вообще на сам алгоритм. И вроде как,
когда мы говорили про алгоритм, кажется, что вот как раз в yкат у нас в этой сумме yкат
усреднённых и в некотором смысле копится запаздывающий градиент. Поэтому такая вот более-менее
натуральная идея это просто менять yкат. То есть менять yкат и вот эту сумму yкат использовать
как градиент, по которому я буду шагать. То есть делать не вот так, как здесь, а конкретно,
ну например, вот так. То есть такая самая банальная идея, то есть не банальная, но приходящая на ум
после того, как я вот воспользовался этой идеей с сагой, что сохранять мои запаздывающие градиенты,
которые мне я считал на прошлой террации. Вот давайте я здесь выпишу. Идея, что, возможно,
я как раз в качестве jkат буду просто использовать что-то в духе вот такого.
yk plus 1 g, то есть суммировать по всем g и усреднять. Ну или по-другому это можно переписать вот так вот.
Это вроде как y из прошлой террации, просто один из y у меня обновился, конкретно тот,
который у меня является случайным. Поэтому вот здесь можно выписать что-то вот такое. То есть
я вычитаю y k и k и вместо него добавляю честный градиент по вот этому сэмплу. Так? Понятная идея,
да? То есть разница на самом деле только в том, что вот я вот здесь, вот этого нет в сага. Вот этого
нет в сага. И вообще изначально мне казалось, когда я сам просто пытался разобраться в доказательствах,
почему не берутся jkato в качестве, ну вот такое jkato, которое я здесь написал, в качестве градиента.
Вот. И на самом деле это, честного ответа на этот вопрос нет. Я на него отвечу сейчас просто потому,
что давайте посмотрим, что вот с таким jkato, давайте я стильдой обозначу, jkato стильдой,
что у него происходит по математическому ожиданию. Вот. Ну давайте проверим,
что у него происходит по математическому ожиданию. Давайте jkato с тильдой, xkato.
Условное математическое ожидание. Ну давайте мы отожидаем. Понятно, что здесь случайно
величиной будет только ikt с точки зрения этого условного отожидания, поэтому я по ней и буду
отожидать. f ikt xkato – y kato ikt. Ну и сразу за пределы условного отожидания вынесу вот эту сумму y.
Вот. Ну и чему будет равна математическое ожидание, кто понимает вот это вот условное,
когда мы пробежимся по всем возможным значениям ик?
y мы обновляем следующим образом. Вот мы выбираем, ну тут на самом деле уже y вот,
ну а я вот здесь и вот так вот выпишу. То есть вот по факту здесь я вот так вот просто добавляю
вот этот алгоритм n-ку везде. Вот. Чтобы по факту не пересчитывать заново как-то g по-другому,
а просто средний y – это у меня то, что есть. То есть я вот когда-то посчитал y, обновил его,
а дальше просто делаю вот так вот. Следующий y у меня опять же обновится, я просто вот средний
буду использовать. Без вот этого дополнительного, без этой дополнительной манипуляции, которая
написана здесь. Вот. То есть jkato – это будет просто средний y. Понятно, y-ки обновляются на этой же
итерации и на следующей итерации получается у меня будет что-то вот такое уже новое. Единственное,
что я почему добавил сюда вот это вот в таком виде. Ну я типа y обновил заранее. То есть я вот так
вот сделал, чтобы у меня в текущем y хотя бы была итерация о текущей точке k. Потому что в таком
виде это будет запаздывающая какая-то вещь. Потому что в jkato, вот если я так без этой
стрелочки сделаю, то в y-ках будет храниться градиент максимум в точке xk-1. Вот. Ну в том виде,
в котором изначально было понятно все хорошо, потому что там был градиент в точке xk. Ну вот
если я так сделаю, я поэтому немного это теряю и поэтому лучше y-ки обновить сначала, а потом
посчитать средний. Вот так вот он обновляется. Ну в той версии, которую я сейчас хочу предложить.
Вот. Чему будет равно математическое ожидание? Тут на самом деле ничего сложного нет. Нужно просто
посмотреть, как ведет себя и. Какие она значения принимает? От 1 до n с вероятностью 1 на n, так?
Поэтому здесь вот это 1 на n она вытащится, потому что она здесь была. Вот. А здесь она и останется.
Еще вот эта вероятность. А это просто коэффициент, который был. Первое 1 на n это коэффициент,
который внутри был. И тогда здесь будет fj-yjkato. Вот. Сумма по j. Вот. И что получается-то? Что-то хорошее
или нет с точки зрения, если мы сейчас все это схлопнем? Да на самом деле нет. Потому что, видите,
в чем проблема? Тут выскочит тоже средний y только с дополнительным множителем 1 делить на n. 1 делить
на n. Он неприятный. И вот поэтому вот это по условному от ожидания не равно f от xk. Вот. Опять же,
я не говорю, что это плохо. Вот. Просто нам до этого вроде как нравилось, что по условному от ожидания
градиент бы хотелось, чтобы он был не смещенным. Но в том же методе сара не так. Вот. Там нет такого
свойства. И за счет этого, кстати, он работает даже лучше. Но здесь, соответственно, понятно,
что авторы метода, да, и мы с вами сейчас делаем так, что просто подгоним. Это все безобразие. И вот эту
1 на n отсюда и уберем. 1 на n отсюда и уберем. Поэтому она испарится здесь, испарится здесь. И тогда у вас
как раз по условному от ожидания будет все хорошо. У вас y схлопнутся между собой и взаимуничтожится,
останется просто средний градиент и все будет хорошо. Вот. Поэтому здесь по условному от ожидания
действительно получается честный градиент. Вот. Но для этого, соответственно, нужна вот такая вот
манипуляция. То есть чуть-чуть метод нужно все же было модифицировать. И здесь вот не просто считать
средний y, а вот так вот. То есть с еще домножением тогда вот будет несмещенность. Тогда будет
несмещенность, при этом сами y обновляются просто градиентами. Вот. Понятная идея, как получается
несмещенность здесь. Как раз обсудили это, не просто голословно посмотрели, а обсудили, как можно это
чуть поварировать. Чуть поварировать, и никто на самом деле не говорит, что вот этот вариант
лучший. Потому что вы в некотором смысле таким образом все равно чуть-чуть увеличиваете дисперсию.
То есть средний y, возможно, был бы лучше. Потому что так у вас есть какой-то fy sample, вы его берете
с множителем больше, чем вроде как 1 на n, и значит увеличивать и вклад его дисперсии. Вот. Никто
вам не гарантирует, что этот вклад будет хороший. Но, опять же, с точки зрения теоретического анализа,
который мы сделаем сейчас, хотелось бы иметь что-то среднее. Ну, чтобы по мотожданию, условному,
все было хорошо. Поэтому мы останавливаемся на таком варианте. Окей. Так, теперь надо разобраться
теперь уже со второй нормой, где я сразу, как я напоминаю, добавил градиент со звездой. Вот. Вот тут
как раз начнется более интересная вещь. Ну, поэтому давайте разбираться. Вот. Опять же, выписываю просто
значение ж, как я ее обновляю. У меня здесь градиент f и k, x и k. Так, y, k и k плюс средний y.
Так. Ну, давайте здесь я сразу же занесу в это все безобразие умный ноль. Умный ноль. Следующим
образом. Следующим образом. Разнесу чуть-чуть. Здесь вычту f и k, t, x со звездой и добавлю сразу
же его. Понятно, что мне, как мы с вами уже много раз обсуждали, градиент по какому-то бачу в оптимуме
меня обязательно равен нулю. Но опять же, я же его вычитаю и добавляю, поэтому ничего страшного не
произойдет. Вот. Окей. Дальше. Кошибуниковский шварц. Кабаша. И получается что-то вот такое. У меня
в одной скобочке будет f и k, t, x и k, t минус f и k, t, x со звездой. Что в принципе выглядит уже
адекватно, потому что как раз разница градиентов по гладкости и выпуклости мы это можем расписать.
Вот. Через разные функции. Но меня больше не будет сейчас интересовать вторая скобка. Вот. И вы мне
подскажите, а что вы в ней такого интересного увидите. Я еще выпишу и k, t, x со звездой минус y и k, t, k
минус вот этот давайте вот так вот 1 на n плюс y, k, g и вот здесь давайте вот градиент в x со звездой я
вот тоже распишу через сумму. Вот. Это в некотором смысле такая небольшая подсказка. Интересно,
что вот происходит с этой скобкой. Давайте вот быстренько на нее глянем. Кто что видит в ней? Что-то
интересное в ней есть? Как это связано? Может в ней вот эти два слагаемых. То есть вот этот,
вот это слагаемое и вот это. Есть ли какая-то между ними взаимосвязь?
У них мотождания совпадают, но второе это вообще не случайная величина. Вот это вообще не случайная
величина с точки зрения условного математического ожидания, которое мы ввели. Вот. Да, это дисперсия
на самом деле записана. То есть вот это случайная величина относительно того математического
ожидания, которое мы с вами записываем. То есть тут вот есть это и kt, которое по факту, ну давайте
я вот так вот так и запишу. Xi случайная величина, ну случайный вектор в данном случае. А это получается
как раз математическое ожидание этого случайного вектора здесь записано. И получается, что здесь
у меня записано выражение. Xi минус математическое ожидание Xi. Ну условное, понятно, математическое
ожидание в квадрате. Вот так вот. Ну и получается, что это действительно дисперсия. Дисперсия.
Поэтому это все безобразие. Можно оценить как второй момент, потому что второй момент у нас,
понятно, больше либо равен, чем дисперсия. Вот. Отлично. То есть здесь мы более-менее разобрались,
что произошло. Так вот, давайте тогда перепишем это все. А, ну по второму тоже двойка. КБШ-то вот
оно. Там у меня сумма вот здесь. Я вот это. Сейчас давайте. Вот это. Это у меня типа A. Вот это у меня B.
Ну я A плюс B в квадрате расписываю как 2A в квадрате плюс 2B в квадрате. Вторая вот эту двоечку я
забыл. Вот. Все, а вот так вот получается. Понятно, да? Ну все. Супер. Так, тогда у меня здесь остается
разница градиентов. Xкт и kt со звездой. Математическое ожидание. И здесь опять же это двоечка,
только уже стало полегче, потому что грохнули этот еще дополнительный хвост. Вот. И тут, в принципе,
все хорошо. Осталось только, как раз, опять же, условно проматожидать. Потому что, опять же, у нас
единственная случайная величина, которая осталась здесь, это и kt. Вот. И на самом деле только она
является случайной. Поэтому, когда я вот это все условно теперь проматожидаю, у меня просто вылезет
сумма. Вот. 1 на n в данном случае это, опять же, вероятность того, что у меня выражение, которое
написано под мотожиданием, это, ну, например, градиент f итого xкатого минус градиент f итого
х со звездой в квадрате в квадрате. Вот. И внизу то же самое. 1 на n сумма yk итый минус f итый х со звездой.
Согласны? Вот. То есть, вот такое вот выражение, вот такая вот получилась оценка. В принципе,
выглядит неплохо. Почему? Потому что вот это мы с этим мы с вами умеем работать. Это будет что-то
в духе. Сейчас мы по гладкости это распишем. А это, ну, сейчас чуть-чуть обсудим, что это за безобразие.
То есть, давайте бам-пам-пам. Это все сделали. Вот. К этому, а нет, я даже еще не проматожидал. Вот
здесь я проматожидал. А, и здесь я сразу же расписал это все безобразие по гладкости. По гладкости. Вот.
Ну, там у меня проматожидали еще, а здесь я еще расписываю это все по гладкости. То есть,
у меня разница градиентов теперь расписывается вот так вот. Через дивергенцию Брегмана. И кто
понимает, почему вот дальше вылезает просто разница самих функций? Есть понимание, почему?
Вроде как еще было скалярное произведение. Где скалярное произведение? Ой, я описался здесь,
поэтому. Ну, где скалярное произведение? Вот. Если там Х со звездой должно быть,
Х со звездой там должно быть. Ну, оно ноль, потому что у вас суммирование по И идет, вы вносите
сумму под скалярное произведение, суммируете эти градиенты. Градиент f правильную нулю,
поэтому у вас останется просто сама функция. Вот. И получается следующее. Вот. Тут собрали
результаты. И вот это по мотожиданию все хорошо. Это просто честный градиент. А вот эта дисперсия
вот эта оценивается следующим образом. Вот это как, ну, то, что у нас обычно возникало в градиентном
спуске. Только у нас обычно возникало с двоечкой. Но тоже разница по функции была. Вот. Окей,
это хорошо. Ну, четверкой. Ладно, не страшно, разберемся четверкой. Вот. Шаг просто нужно будет
взять поменьше, потому что только на это влияет. А второй кусочек, ну, вот он такой как раз непонятный,
но на самом деле это ровно то, что мы хотели. Помните, когда мы обсуждали интуицию, получается,
что если у меня y, k, t, it будут стремиться к градиенту f, f, y со звездой, то все хорошо. Как
раз у меня дисперсия будет стремиться к нулю. Вот. И это мы здесь как бы в некотором смысле
физически получили. То есть более формально, получили вот это замечательное свойство. То,
что у нас стремление y, k, t к it будет действительно нам уменьшать дисперсию стахастического градиента.
У нас же не дисперсию, а второй момент вот этот. Второй момент. Ну, на самом деле вот просто вот это
отвечает за честный градиент, поэтому вот это скорее отвечает действительно за дисперсию. То есть
это бы вылезло, если бы там g, k, t расписали вот так вот. Вот. Это бы вот это. Вот это скорее
бы ушло в дисперсию. Вот это бы ушло в дисперсию. Вот. А тот первый кусочек,
который бы мы там расписали вот так вот, он бы ушел вот сюда. Поэтому вот это как раз у нас дисперсия.
И здесь, как вы понимаете, она действительно может стремиться к нулю, если y ведут себя правильно.
Если y ведут себя правильно, ну давайте понимать. То есть пока опять же мы до конца не дошли,
но какую-то физику уже получили. Вот. Собрали вместе, получили вот такое безобразие. Здесь,
как опять же правильный подбор шага, как вы знаете, нам может дать классный результат,
но все еще непонятно, что делать вот с этой фигней, потому что физика хорошая, но уничтожить
формально не можем. Чтобы с ней бороться, нужно будет доказать еще один дополнительный факт. Еще
один дополнительный факт. Давайте попробуем разобраться, как ведет себя вот тогда вот эта
последовательность от итерации к итерации. Приближается ли она к градиентам х звездой или нет.
Вот. Ну надо разбираться, надо разбираться. Поэтому буду вести, буду разбираться, как вот себя ведет
вот это безобразие. То есть накаплюсь первой итерации, смотрю на вот то, что у меня вылезло там в каты
итерации. Давайте попробуем оценить. Опять же, возьму условно-математическое ожидание по каты
итерации. По каты итерации. Занесу это условно-математическое ожидание под знак суммы, потому что есть
линейность. Вот она всегда есть, она никуда не пропадет. Вот. Теперь у меня к вам вопрос. Теперь
у меня к вам вопрос, потому что сейчас, до этого мы вообще никак не пользовались тем, как меняется
от итерации к итерации у. Вот. А теперь этим придется воспользоваться, потому что до этого у нас
был просто y-кат, и там у него просто индекс был случайный. Там от ожидания по индексу, и оно, на
самом деле, хоть y, хоть z, напиши. Неважно, это все равно получится что-то там среднее. А теперь важно,
как мы им меняем y. И это сыграет роль. y-кат плюс 1. Какие значения он может принимать? И с какой
вероятностью? Сколько у него вообще вариантов? Все правильно. То есть у y-кат плюс 1 и всего два варианта.
Либо градиент, если нам повезло. И если не повезло, тогда просто с предыдущей итерацией мы его забираем.
Поэтому здесь, когда мы условно-математическое ожидание, у меня что выскочит? С вероятностью 1-1
делить на n. Это просто y-кат, который заберется с прошлой итерацией. Давайте вот так поставлю скобочки.
А с вероятностью 1 на n, у меня ровно то, что нам уже подсказали, выскочит очень хорошая вещь. Это вот
как раз разница f и t, x и kt, f и t, x и звездой. Вот. Отлично. Отлично. Тогда смотрите, что получается.
Выписываю следующим образом. 1-1 делить на n. Здесь у меня будет тумма y и x и звездой.
А здесь у меня как раз останется 1 делить на n, умножить на 1 делить на n, f и t, x и kt, минус f и t, градиент f и t, x и звездой.
Как расписывать вот это? Мы только что с вами разобрались вот эту сумму. Поэтому я здесь сразу напишу, давайте
2l, f от xk, минус f от x и звездой. По гладкости и выпуклости, дальше скалярное произведение убьется.
Окей. Смотрите, что теперь классно получилось. Вот это у меня в некотором смысле какая-то последовательность sigma k плюс 1.
За которой мы как бы понимаем, какая у нее физика. Если она будет сходиться к нулю, то все с ней хорошо.
Значит там дисперсия будет сходиться к нулю, а y стремятся к f от x звездой. Вот. Так это же она у меня тоже, только на кат и итерации.
На кат и итерации получается, что я получил что-то в духе sigma k плюс 1, по условному математическому ожиданию, меньше либо равна, чем sigma k, плюс какая-то добавка.
2r делить на n, f, xk, минус f от x звездой. Согласны? Вот. Смотрите, получается такое что-ли занимательное свойство.
Здесь это доказано. Вот. Смотрите, что в итоге-то. У меня есть сходимость по x. Здесь я сразу накидываю полным от ожидания.
Сходимость по x, вот она. Вот. С этим мы вроде как умеем бороться. Правильный гамма меньше чего-то уничтожит это.
Здесь, опять же, с дисперсией не знали, что делать. Не знали. Вот. Но про эту дисперсию я знаю то, что она мне сходится, то есть вот тут тоже линейная сходимость.
Линейная сходимость. И здесь линейная сходимость. Вот. Ну и здесь опять же возникает неприятный член, который вот здесь, мы вроде как уже умели уничтожать.
Надо получается как-то вот эти две рекурренты, которые по факту у меня вроде как есть сходимость.
По x линейная, просто чуть-чуть что-то ей мешает до полного счастья.
Плюс у меня есть опять же сходимость линейная по вот этим y. По вот этим y тоже что-то мешает до полного счастья ее сделать прям по-настоящему линейной.
Вот. Но видно, что они друг другу могут в некотором смысле помочь. То есть вот это хотелось бы уничтожить, а это умеет уничтожать вот здесь, вот правильным подбором шага, потому что там есть вот это выражение с минусом.
Ну а это хотелось бы тоже как-то поуничтожать здесь, но вот это возникает здесь. И вот здесь как бы есть знак минус, который нам может помочь это уничтожить.
Поэтому вот эти две рекурренты предлагается просто сложить между собой, сложить между собой с правильным коэффициентом.
Вот. Коэффициент m гамма в квадрате. m гамма в квадрате, и я складываю две рекурренты, которые у меня получились.
Вот. Вот это линейный кусочек у меня сходимости был. Вот это у меня вылезло из второй рекурренты на y.
Вот. А это у меня вылезло из первой рекурренты, где у меня как раз болтался неприкаядный кусочек гамма в квадрате, два гамма в квадрате.
Вот. m-ка тут как раз у него сокращается. Вот. Он у меня вылез из той рекурренты.
Плюс как раз то, что я хотел. Вот сюда у меня залетело в скобочку вот это значение fx ката минус f от x звездой,
который ровно как мы знаем у нас больше либо равно нуля. В скобочку залетел дополнительный кусочек, который мы как раз сейчас с вами и за нули.
Который мы как раз с вами сейчас и за нули. Ну точнее вот эту скобочку просто сделаем меньше либо равной, больше либо равной нуля.
Вот. Окей. Какую m-ку взять, ваше предположение? Какую m-ку взять? Вообще имеет смысл какую брать m-ку?
Какие мысли? Что там?
Ну там мы же это с помощью шага еще сможем сделать. То есть скорее там вот шаг подбирается исходя из m-ки.
Вот. Вот интересно скорее с точки зрения m, потому что вот только вот это слагаемое зависит только от m.
Потому что m мы можем варьировать, но вот это слагаемое зависит только от m. Что мы от него хотим?
Что мы от него хотим?
Больше нуля это хорошо, но оно в любом случае будет больше скорее всего нуля. То есть тут единица, n-ка это понятно больше единицы, поэтому она будет больше нуля.
Вот. Что от него-то хочется? Мы же вроде обсуждали, то есть у нас есть линейная сходимость по x, есть линейная сходимость по y.
Хороший вариант, хороший вариант, но подобрать будет довольно сложно. Это правда, это лучший вариант, это сделать так, чтобы вот эти как бы линейные сходимости вот это и вот это вообще совпали.
То есть подобрать m-ку так, чтобы прям вот это можно было сделать, но вообще это даже не всегда возможно.
Хотя вроде как, если m-ку брать довольно, ну ладно, может и возможно всегда, можно проще просто сделать. Я хочу, чтобы у меня линейная сходимость сохранилась, так?
Вот. Поэтому я вот хочу, чтобы у меня вот здесь вот, вот из этой скобки вылезло что-то, что меньше единицы. Что меньше единицы.
Вот. И за счет m-ки я понятно это могу сделать. Беру довольно большую m, и ну она не сильно повлияет на то, что произойдет.
Давайте я возьму m равную 4n. 4n. Пока непонятно, почему я такую беру.
Вот. Ну все довольно просто будет, во-первых, потому что у меня линейная сходимость сохранится. Здесь вылезет 1 девять на 2n.
Вот. Ну почему, например, нельзя взять больше, обсудим чуть попозже. Потому что кажется, что чем больше m я возьму вот здесь вот, тем лучше у меня будет линейная сходимость по вот этому кусочку.
Но посмотрим, почему нельзя. Почему я взял такой, и этого достаточно.
Тогда у меня вот здесь вот там все сократится, вылезет конкретно 6 гамма л. Мы возьмем гамма меньше, чем 1 делить на 6 л.
И это все у нас уйдет. Останется только линейная сходимость. Две линейные сходимости.
Но я могу их вот так вот склопнуть через максимум. Понятно, что я могу сверху оценить вот эти два множителя как максимум из этих двух множителей.
И тогда смотрите, что у меня получилось. У меня получилась вот такая вот функция в k плюс первой.
В k плюс первой точке. И эта же функция в k этой точке.
Ну и вот так получился такой вот хитрый критерий сходимости, состоящий из двух кусочков. Из двух кусочков.
И по нему можно измерять сходимость, запускать рекурсию и получать результат.
Это здесь у меня и отражено. Как, соответственно, получить сходимость для метода saga.
Здесь, соответственно, да. У меня берется ката-итерация.
Ну, запускается рекурсия. Здесь остается, соответственно, вот эта функция, которая у нас используется в качестве сходимости в нулевой точке.
Здесь k в k этой точке. И все хорошо. Все хорошо. То есть вот вообще эта функция довольно хорошо отражает физику.
То есть она дает нам понимание, что у нас есть сходимость и по x, и по y.
То есть и y стремятся вот к этому значению. К f и тому x звездой. Причем линейно. Вот.
И x. А почему так? Потому что на самом деле у меня v-ката состоит из двух неотрицательных частей.
И если у меня v-ката стремится к нулю, то у меня и любая его из его частей, которая по факту меньше либо равна этому v-кату, тоже будет стремиться к нулю.
Вот. Как и расстояние по x, так и расстояние по вот этим y до градиентов.
Вот такой вот результат для метода saga.
Единственный вот вопрос, опять же, возвращаясь к тому, почему я не взял довольно большую m, потому что вроде как мог.
Вот. А почему не взял?
Да. Потому что на самом деле вот тут вот, вот тут вот в оценке сходимости, в оценке сходимости, вот это и есть m.
Вот это и есть m. И как вы видите, правая часть от этого всего безобразия зависит.
В левой части я, да, могу написать что-то в духе, что у меня здесь всегда будет x-ката минус x звездой в квадрате.
Вот. И вроде как зависимости от m-ки нет.
Но вот здесь зависимость от m-ки выскочит.
Зависимость от m-ки выскочит. И как вы понимаете, тут как раз, если вы y инициализируете нулями, то есть изначально память для текущего градиента там,
по sample e, вы зададите нулем, то у вас как раз будет m-ка увеличивать множитель, который зависит от как раз вот этих норм градиента x звездой.
Вот. Берете большую m-ку, соответственно, у вас сходимость в этом плане портится.
Но m-ка, видите, как видите, такой инструмент. Инструмент именно подбора результатов, чтобы сходимость была красивая.
При этом, как вы видите, ну там m-ку порядка n брать и стоит.
Потому что вот здесь вот по линейной сходимости она у вас была изначально 1 минус n, 1 делить на n.
Ну, понятно, что это какая-то линейная сходимость, там, зависящая от n-ки.
И то, что здесь появилась двойка, в принципе, с точки зрения онотации, в которой мы обычно записываем результаты, не особо важно.
Поэтому чуть-чуть попортили, но не страшно. То есть m-ку главное не взяли особо большой.
Вот. Получается здесь вот такая вот игра. Взять m-ку небольшой, но при этом еще не испортить эту линейную сходимость.
Окей. Из этого всего безобразия можно получить, понятно, сходимость метода,
которую мы с вами, в принципе, уже видели. Это итерационная сложность.
Она, понятно, зависит от обоих множителей. Если вы там подставите шаг 6L, который максимальный, и у вас останется вот так вот.
Какой-то из этих множителей, понятно, преобладает. И в зависимости от этого у вас получается результат.
То есть по-хорошему здесь нужно бы написать максимум от 2n 6L делить на mu.
6L делить на mu, logarithm 1 делить на epsilon.
Ну так как мы записываем результат с точки зрения онотации, то у вас здесь получается,
что вы максимум можете раскрыть просто как сумму, и от констант, которые численные, можете просто избавиться.
Понятно, что вот это вот так вот.
На двойку, ладно, припишу, а потом, когда в онотацию запихивать, вообще все пропадет.
Как-то так, как-то так. То есть вот такой вот результат.
Видели уже в прошлый раз, обсуждали то, что этот результат на самом деле лучше, чем сходимость градиентного спуска,
потому что для градиентного спуска итерационная сложность на самом деле вот L делить на mu.
Итерационная сложность, но, как вы понимаете, в градиентном спуске нам нужно считать полный градиент по всем бочам.
По всем бочам, то есть он будет в N раз тяжеловеснее, чем подсчет градиента по одному сэмплу.
По одному сэмплу, ну либо там если делаем бочирование, то будет чуть больше, но не страшно.
Вот, то есть вот эта N, N часто не играет роль, потому что градиентного спуска с точки зрения вычислений,
именно градиентов здесь будет N, а там получается у нас N плюс L делить на mu,
потому что для градиентного спуска можно даже вот так оценку записать.
И понятно, что какая из них будет лучше.
Окей, окей. Все, получили опять же результат, напомнили то, что у нас это все действительно хорошо работает.
Для метода SVRG, для метода SAR доказательства очень похожие, то есть главное суть этой техники в том,
что сконструировать критерий сходимости не из одного кусочка, а сразу из нескольких.
Такое мы в принципе видели, когда, а, ну, метод Нестеров я по-другому доказывал.
Ну, то есть в методе Нестеру на самом деле также критерий сходимости, он состоит из двух кусочков.
Исходимости по функции и исходимости по аргументу, вот, с правильными коэффициентами подобранной.
Вот, и здесь то же самое, то есть тут вводится две последовательности, которые обе как бы сходятся,
но с небольшими помехами, но они друг друга в этом плане компенсируют,
и в итоге получается хорошая линейная сходимость в отличие от обычного HDD.
Вот, окей, тогда в первой части все, перерыв, ну что, продолжаем, продолжаем.
Вот, теперь чуть-чуть опять же сдвинемся в другую сторону.
Ну, продолжаем как бы рассуждать про статистические методы.
И вот, когда мы говорили про методы редукции дисперсии, ну, там, не только про них конкретно,
мы сейчас говорили о том, что мы просто вместо подсчета полного градиента по всей обучающей выборке,
например, считаем градиент по отдельным сэмплам, по отдельным сэмплам, вот.
Вот, а если с точки зрения какой-то матрицы, говорить с точки зрения матрицы A,
которая по факту у нас из этих сэмплов и состоит, то есть они вот тут в строке записаны эти сэмплы,
A1, там A2 и так далее, вот, что это означает?
Это означает, что мы по факту выбираем какие-то строки из этой матрицы
и используем только их при подсчете градиента, вот.
И как раз стахастика состоит в том, что мы выбираем нужные строки, вот,
именно в матрице данных.
Ну а какой более-менее противоположный, как бы, стахастический смысл можно вложить в метод?
Вот, какие еще могут быть идеи, если не строки, то что?
Столбцы, понятно. Вот, а за что тогда отвечают столбцы?
Если строка это просто у нас сэмпл, вот, то столбец с матрицей данными, то что?
Координаты это признак, да, то есть
конкретная координата оптимизационной перемены – это конкретный признак.
То есть получается, что…
с одной стороны, когда мы говорим про задачу
стахастической оптимизации до этого, мы просто выбирали сами строки,
выбирали конкретные сэмплы, вот,
противоположный вариант, давайте брать не сэмплы,
давайте брать конкретные признаки, брать конкретные координаты.
Ну и на самом деле с точки зрения вообще квадратичной задачи все эти методы
редукции дисперсии часто ну не особо эффективны в плане вычислений, потому что
опять же у вас есть матрица A, вы записываете вот эту задачу в виде вот такой вот
вот такой вот и что вы делаете? Вы можете на самом деле переписать ее даже вот так
x транспонирована, A транспонирована, A x минус A транспонирована, B
целевую функцию и на самом деле вам достаточно один раз только возвести мат...
умножить матрицу на матрицу, вот, а дальше умножать вектор x для подсчета
градиента уже на вот эту матрицу размера D на D, вот. Понятно, что в более сложных
задачах кипологистической регрессии так вам просто не проканает, вот. Там уже нужно
считать каждый раз что-то отдельно, вот. Поэтому как бы здесь сразу же возникает
такая сразу же естественная идея, как мы говорили про признаки, то есть выбор
каких-то признаков. А если у меня есть задача, вида, ну давайте вот так вот
запишу, градиент у меня какой-то моей там целевой функции g равен какой-то
матрице B на x, B на x там, ну, минус C, например, B на x минус C, вот. Как мы поняли,
мы должны выбрать какой-то отдельный признак, но в данном случае даже проще,
даже проще, давайте будем брать просто координату, координату этого вектора,
координату этого вектора, то есть считать не полный градиент, не там перемножать
полную матрицу на вектор x, а перемножать получается опять же строку, ну, потому что
здесь мы уже рассматриваем матрицу B, которая в некотором смысле с матрицей A
связана не тождественно, вот, вы вместо полной матрицы B, вместо полной матрицы B хотите
посчитать градиент вот этой вашей функции по какой-то случайной координате, по какой-то
случайной координате, и тогда вам нужно просто вот эту строку и катую строку перемножить на
вектор x. Это значительно дешевле, потому что перемножая всю матрицу на вектор x, вам нужно
сделать D в квадрате операций, то в случае, когда вы считаете только одну координату этого
градиента, вам нужно сделать D операции, то есть такая процедура в D может быть и две раз дешевле,
когда вы вместо полного градиента считаете градиент по координате. Как это в том числе связано
с задачей машинного обучения? Для квадратичных задач, для логистической регрессии там
может быть довольно просто, то есть считать градиент по конкретному признаку, то есть производную
просто по этому признаку. Там может быть явная формула, она может быть действительно дешевле по
сравнению с тем, что вы вычисляете полный градиент. Часто в реальных задачах вы не просто не можете
посчитать градиент, не производный по координатам, вы просто имеете доступ только к информации
нулевого порядка. Используя информацию нулевого порядка, вы на самом деле можете просто
апроксимировать производную по направлению. Производную по направлению, только здесь,
я вот так напишу, производную по направлению. Какое-то направление E у вас случайно задано,
но это вы из мотонализа еще знаете, что через конечную разность его можно просто апроксимировать.
Понятно, если вы стремите tau к нулю, то у вас может быть что-то действительно хорошее в этой
апроксимации и получится. То есть вот такого рода координатные стахастические методы,
они возникают не только из-за того, что вам хочется что-то дешевле считать, но из-за того,
что вам хочется просто выжить как-то из информации нулевого порядка, информацию типа градиента,
ну через конечной разности, например, это рабочая схема. Подробно про вот именно
оптимизацию нулевого порядка, когда у вас градиент производно по направлению апроксимируется
через конечные разности, вам расскажет Маргарита на семинаре. Сегодня мы этого касаться не будем,
ну тут скорее мы просто сегодня поговорим про координатный метод, когда вы вместо подсчета
полного градиента считаете одну из его координат. Одну из его координат, по каким причинам так
происходит? Ради удешевления информации, ради удешевления, ой только тут я видимо из саги это все
переписал, тут память не нужна, вот ради удешевления вычислений или просто потому что не до жира быть бы
живым, вот нам сейчас не важно. Окей, считаем градиент по координате, посчитали вот производную,
чтобы у меня получился градиент я здесь домножаю еще на вектор, ну точнее вектор размерности D,
я здесь домножаю еще на базисный вектор соответствующий, на самом деле можно было не
домножать, потому что по факту я же обдейчу только одну координату и мог бы здесь как раз
апдэйтить её, и поэтому в том числе координатные методы и дешевле, в том числе даже в виде финального
апдэйта мне нужно поменять только одну координату, а не полный вектор. Но я здесь записал так,
потому что это вещь такая более формальная, мне скорее для доказательства так удобнее.
Вот, а я зачем-то здесь еще приписал вот эту D, вот этот множитель D,
который у меня соответствует размерности задачи. Зачем?
Кто может уже сразу понимает, зачем здесь эта D? Я вроде выбрал случайную координату и все.
Каким бы чем?
Ну, на самом деле мы, опять же, боремся за не смещенными.
Ну, давайте сейчас мы на это посмотрим. Вот, понятно, что вы физику уже более-менее понимаете,
зачем это все нужно, что как бы в скалярном произведении дата возникает одна координата
вместо всех. Вот, и это нужно как бы ее чуть-чуть подправить.
Окей, делаем все то же самое, как делали раньше. Делаем шаг градиентного спуска,
смотрим, как меняется расстояние. Ну, тут уже подставляем то, что мы используем в качестве
нашего стахастического градиента. И понятно, нам нужно будет по условному
математическому ожиданию это все безобразие оценить. Ну, давайте попробуем оценить это
безобразие по условному математическому ожиданию.
Так, к чему равно? К чему равно? Кто понимает, как посчитать такое условно
математическое ожидание, если координаты у меня выбираются равномерно.
Чему равно это условно-математическое ожидание?
Давайте, я D вынесу сразу за него.
Какие у меня значения принимает E?
От 1 до D, поэтому я так и запишу.
G пробегает значение от 1 до D.
Вероятность выбора из координатов равномерна,
так как у нас все это выбирается,
мы просто один делим на D.
И здесь, соответственно, пробегают значения F,
K, T, G, E, G.
И чему равна эта безобразие?
Кто понимает?
Кто понимает?
Давайте, я D сокращу и выпишу вот это.
Все правильно, это просто градиент.
Мы каждый из координат домножаем
на соответствующий базисный вектор.
И в итоге получается просто градиент.
Все правильно.
И как вы видите, без домножения на D,
которое я изначально здесь приписал,
которое здесь возникло, понятно, и здесь возникло,
у вас здесь был бы градиент, но только деленный на D.
Вот и вся история.
Почему это домножение нужно?
Опять же, для несмещенности,
просто потому что мы уже привыкли делать несмещенный анализ.
На самом деле это не обязательно.
Можно было, опять же, оставить без этой D.
В некотором смысле сейчас мы поймем,
что мы перегоняем ее из одного места в другое.
Здесь мы ее добавили,
но придется ее в некотором смысле
и компенсировать в другое место.
Окей.
С первым моментом разобрались.
Теперь второй момент.
Вот так.
Здесь что получается?
Здесь что получается?
Так.
Так, кто понимает?
Опять же, в условном от ожидания
и kt пробегает значение от 1 до D,
все аккуратненько выписываем.
D квадрат выскакивает из-под нормы.
Вероятность – это 1 делить на D.
Опять же.
И значение, по которому пробегаем,
это у нас просто f,
xk,
и…
ну, здесь давайте g поставлю.
g от 1 до D,
e g,
в квадрате в квадрате.
Окей, давайте я D вынесу.
У меня одна D сократится.
Получается, останется что-то вот
в таком виде.
Окей, сейчас давайте выпишем.
Так.
f g e g
в квадрате.
Чему равна эта норма?
Внимает.
Точнее, даже сумма этих норм.
Просто норм и градиент.
Потому что, опять же,
когда вы берете норму в квадрате
от вектора,
то есть за норму вообще можно вынести
вот эту координату.
Вот эту координату можно вынести за норму,
потому что это скаляр,
производная по координате.
Потому что скаляр можно вынести
за пределы нормы в квадрате.
Норма базисного вектора
равна просто единице.
Поэтому здесь у вас будет просто
сумма квадратов координата.
Это есть просто норма.
d
норма f от xk
в квадрате.
Вот. Получилось так, что
что у нас здесь?
Там-пам-пам.
По мотожданию все хорошо.
В дисперсе получили
увеличение
градиента. Увеличение градиента
в d раз.
Увеличение нормы градиента
в d раз.
Поэтому, когда мы это все безобразие
подставляем, у нас получается то, что
мы обычно имели в градиентном спуске
расстояние до решения предыдущей точки,
скалярное произведение, плюс
норма градиента в квадрате
на гамма в квадрате. Единственное, что здесь
добавилась еще это дополнительная d,
которая
сейчас увидим, где она дает
свой импакт фактор.
Расписываем по выпуклости, по гладкости.
Все получается ровно, как обычно.
Здесь линейная сходимость.
Здесь тоже у вас сначала
вылезает с минусом разность функций,
которая из скалярного произведения
выскочила из сильной выпуклости.
А дальше вы по гладкости расписываете
норму градиента,
добавляя туда f и x звездой,
и у вас выскакивает, вроде бы,
все хорошо, гамма, l.
Но здесь выскакивает еще дополнительная d,
потому что она у нас возникла
из-за того, что
брали мы от ожидания.
Выскочила эта d.
Ну и, как вы понимаете,
шаг теперь какой?
Он не 1 делить на l, он теперь должен быть
меньше, чем d делить на l.
В некотором смысле у вас
произошла компенсация.
Вы домножили
градиент на d,
чтобы он был не смещенным.
Но в шаге
вы в итоге
на d разделили.
То есть в итоге у вас что получилось?
У вас
итоговая сумма
получилась такая, что у вас
умножение на d, деление d на d
никакого фактора от этой дешки нет.
А почему так? Почему так вообще происходит?
Потому что в некоторых крайних
случаях может так
случиться, что
ну
вы
координатным методом по факту
вы цепляете
полный градиент.
Пусть у вас есть какая-то совсем простенькая задача.
x1 в квадрате
плюс x2 в квадрате вы минимизируете.
И вдруг вам задали
стартовую точку, например, 0.1.
Стартовую точку 0.1.
И, как вы понимаете,
то, что по второй переменной вы по факту
все уже минимизировали.
И у вас
то, что сейчас будет записано,
градиент такой вот,
и вы из него начинаете
выдергивать координаты.
Вы начинаете из него выдергивать координаты.
Выбирай иногда 0 и
иногда полную координату.
Если, опять же, по этой стратегии,
которую мы с вами обговаривали, вам нужно домножать
на d, ну можем даже
для наглядности задачу
супер увеличить до размерности d,
и здесь стартовать из всех
нулей, кроме дешки, тогда вы
умножаете на d.
Вы же что делаете, когда вы выбираете
нулевые координаты?
У вас нулевой градиент и ничего не происходит.
А когда вы выбираете
первую координату, просто случайно,
вы на самом деле посчитали
честный градиент. Честный градиент
в текущей точке.
Потому что вот так у вас устроена
стартовая точка, так вам
в некотором смысле
подфартило, с другой стороны, не подфартило.
То есть вы реальный
градиент умножили на d,
но этого же нельзя делать,
ну просто так, за бесплатно.
Мы знаем, что в градиентном спуске,
по факту вы сейчас будете пытаться сделать
шаг обычного градиентного спуска, так делать
нельзя. Увеличение шага,
воспределенный шаг 2
делить на l, а вы тут
пытаетесь увеличить
размер градиента в d
раз, то есть запихать в шаг
дополнительную дешку, ее нужно компенсировать.
И она, понятно, будет у вас компенсирована
в шаге.
Вот. Ровно из-за
этой ситуации.
Из-за этой ситуации, потому что
в некоторых частных случаях
вызов координаты, выбор просто
координат эквивалентен, по факту подсчета
полного градиента.
И поэтому
увеличение градиента в d раз
сразу же увлечет уменьшение
шага в d раз.
Вся идея. Поэтому вот здесь
появляется компенсация
в виде d делить на l.
Окей. Ну, тогда, раз
появилась компенсация, мы можем убить
вот это все безобразие, получить
линейную сходимость, которую, опять же,
мы любим, знаем, и
все. Дальше
опять берем полным от ожидания, пользуемся
tower property, получаем честную линейную
сходимость. Единственно, что выглядит
вообще как будто для градиентного спуска.
Но, опять же, разница только в том,
что теперь в шаг закинуто константа
d. И когда вы будете вычислять
реально уже результат для сходимости,
больше чем 1 делить на d вы подставить
не сможете. И поэтому здесь у вас
возникнет что-то вот такое.
Вот такой вот множитель.
И когда вы будете это переводить уже
в итерационную сложность,
в итерационную сложность, в итерационную сложность
у вас будет d делить на mu l.
logarithm
1 делить на epsilon.
1 делить на epsilon.
То есть получается, как она соотносится
со сложностью градиентного спуска.
В d раз больше, в d раз хуже.
Опять же, вот этот пример,
который мы с вами рассматривали,
x1 в квадрате, x2 в квадрате
и так далее,
xd в квадрате
и стартовая точка
1000.
По факту же вам, чтобы
хорошо минимизировать эту функцию
градиентным спуском, нужно
один раз посчитать градиент
по первой компоненте.
Ну или несколько раз в зависимости от того,
какой вы возьмете шаг, но если правильно
подберете шаг, то вообще один раз и вы сразу же
упадете в оптимум.
Но возникает проблема
в силу того, что во-первых, одну проблему
мы решили. Мы компенсировали
эту d, которая дает несмещенность
в шаге, но это не страшно.
То есть теперь у нас будет получаться по факту шаг
градиентного спуска, когда мы будем выбирать
нужную координату в градиенте.
Но эта координата
теперь, к сожалению, будет выпадать нам
необязательно
сразу, потому что мы выбираем одну из d координат
и во всех остальных
координатах у вас просто ноль
градиента, вы никуда не двигаетесь.
Ну и как вы понимаете,
это что? Это просто
геометрическая случайная величина.
Вам нужно выбрать
то есть вы подбрасываете монетку, вероятность
выпадения успеха 1 на d.
Понятно, что количество
подбросов, чтобы у вас появился
успех, чтобы вы выбили
нужную вам координату, он будет
пропорционален d.
И вот, к сожалению,
эта d она здесь и возникает.
То есть даже вот такая совсем простая задача
вам говорит о том, что
вот эта проблема
координатного метода,
увеличение количества итераций на d,
она не решается.
Она не решается, если вы не введете
какие-то дополнительные знания о задачи.
Понятно, что
если вы знаете, что во всех точках
с оптиум кроме нужной точки
вы сразу же в нее ударите,
в некотором смысле сместите вероятность
просто в нужное место
и все будет хорошо.
Но вы же этого изначально не знаете,
вы делаете какой-то solver, который более-менее
универсально работает.
Поэтому в общем случае ничего хорошего.
Ничего хорошего.
Окей.
Что еще?
А в градиентном слуске это как соотносится
в сложности?
Будет то же самое.
Если мы предполагаем, что вычисление
не обязательно,
что может произойти,
что градиентный спуск почисляет полный градиент.
Здесь мы считаем одну из производных
по направлению и кажется, что
в хорошем случае это в d раз дешевле.
Понятно, в общем случае это не совсем так.
Часто бывает это
не в d раз дешевле,
чуть меньше, чем в d раз дешевле.
В хорошем случае это в d раз дешевле,
но как мы видим количество итераций
в d раз возрастает.
Здесь в d раз выиграли
с точки зрения вычисления градиента,
но в d раз проиграли с точки зрения
итераций.
К сожалению, в отличие от методов редукции дисперсии,
координатные методы
с точки зрения аракулной сложности
почета производных
по направлению, по координате,
они не улучшаемы.
Как мы видим, на простой задаче это
подтверждается.
Вот такая вот беда.
Но опять же можно
в некотором смысле эти свойства
чуть улучшить,
когда вы знаете какую-то
специфику задачи.
Знаете, например, что
константа липшицы отличается
по разным из направлений.
Но это опять же такая специфика,
которая, как мы знаем, может улучшить
оценку, просто потому что
задача в некотором классе сужается,
поэтому исходимость может стать быстрее.
Это ровно о том, что
когда мы с вами говорили, что
есть, например,
выпуклые задачи, для них есть
одна оценка, сублинейная исходимость для
градиентного спуска. Если вы говорите,
что задача не просто выпуклая, а сильно выпуклая,
у вас появляется уже линейная исходимость,
но вы сузили класс задач,
который вы умеете решать.
Здесь то же самое, добавляете какие-то дополнительные
хорошие свойства, понятно, могут быть какие-то
и улучшения.
На самом деле это все, что я хотел
сегодня рассказать, но время у нас осталось,
поэтому давайте чуть-чуть еще
пообсуждаем тот
подход, а почему у меня, кстати,
слайды не пролистываются.
Тут я просто говорю, что на самом деле
координатный метод хорошо себя проявляет на практике,
потому что уменьшение
шага не обязательно делать
в ДРС.
Понятно, можно
это все обобщить на так называемый
блочно-координатный случай, когда
вы считаете производную
шагу не по одному направлению, а по сразу
нескольким направлениям по бачу направлений.
Для задач большой
размерности это
такой хороший рабочий способ.
И понятно, что это все безобразие, можно
ускорить с помощью моментума.
Метод чуть более хитрый будет, там нужно добавить
второй моментум.
Но, в принципе, это все ускоряется
и получается тоже на самом деле
оценка такая, что по сравнению с
Нестером вы ничего не выигрываете,
то есть там тоже в ДРС больше по количеству
вот такая вот проблема, к сожалению,
у координатных методов она
есть.
На самом деле у вас был проект про координатный
метод и редукцию дисперсии в координатных
методах.
Но раз у нас время осталось, давайте про нее и поговорим.
Потому что, опять же,
координатный метод,
он похож на SGD.
В SGD выбрали бач, пошли
по бачу, здесь выбрали координат
и пошли по координате.
И возникает такая же естественная
идея, как
в методе SEGA.
В методе SEGA, раз
вы уж пошли
по какой-то координате,
посчитали производной по этой координате,
давайте сохраним ее.
Давайте ее сохраним.
И тогда что получается?
Давайте опять же введу
некоторый вектор y, в данном случае он
будет просто размерности d.
Размерности d.
И что я буду y сохранять?
Я специализирую этот вектор нулями.
Просто нулевой вектор
какой-то у меня будет.
И что будет происходить? Давайте я
выбираю какую-то одну из координат
равномерно и
независимо
от предыдущих итераций.
Считаю производную по этой координате.
Давайте вот так. Я ее записываю.
И вектор y
обновляюсь соответствующим образом.
Я беру y.
Давайте
k плюс первый
равен y к этому.
И
из старого y я забираю
ту координату, которая у меня была.
Та координата, которая у меня хранилась.
И записываю вместо нее ту координату,
которую я посчитал.
Ровно та же самая похожая идея
на сагу.
Метод называется SEGA.
Потому что
идея
довольно эквивалентна.
Понятно, что там были бы
координаты.
И вы векторе y
сохраняете те координаты,
которые вы когда-то посчитали.
Дальше опять же
тут два варианта.
Два варианта оказывается,
что можно, например,
так и делать шаг
по y.
Это, кстати, на самом деле
не SEGA.
Это такого рода метод
казался
довольно плохим.
То, что я вам говорил про SEGA.
Средний y,
который хранится в SEGA,
не обязательно плохой.
Потому что
в методе
мне сделал студент Андрей.
Он попробовал
в классической SEGA
взять
вот этот y, который мы получили.
Назвал этот метод Ягуар.
И оказывается, что в таком варианте
он работает даже лучше
на практике для некоторых задач.
И более того,
в теории для невыпуклых задач,
для методов типа
Франко Вульфа тоже дают лучше оценки, чем SEGA.
Проблема только в том,
что вот здесь вот этот y,
как вы понимаете,
по математическому ожиданию,
не дает честный градиент.
Не дает честный градиент – не дает.
Но, как оказалось,
это не страшно.
Это то, про что я говорил,
в том числе, когда мы говорили про SEGA.
Не обязательно вот тот y,
который, та самая банальная идея
использовать просто
средний y как градиент,
она не обязательно страшная.
И никто не говорит о том,
что нужно использовать
обязательно какую-то
несмещенную оценку градиента.
Исходный алгоритм SEGA действительно
использует несмещенную оценку градиента.
Для этого там делается
следующая манипуляция.
Используется домножение на D.
Это ровно то,
что мы видели и в Саге.
Домножаете на дополнительный
множитель, там был N,
здесь, соответственно, у нас D.
Вот это SEGA.
Это SEGA.
И там, соответственно, делается шаг
xk плюс 1
равно xk
минус гамма zhk.
Давайте вот так.
Запишем.
Это две вещи, это SEGA.
В Ugoar такого не делается,
метод даже становится проще,
не нужно вычислять какое-то дополнительное zhkt.
И оказывается,
что в некоторых случаях
он еще и оценки исходимости дает лучше.
Поэтому ровно то, что я говорил про SEGA.
Не обязательно там использовать
несмещенную оценку.
Никто не говорит, что несмещенная оценка
это всегда хорошо.
Опять же, на статистике, я думаю, вам это тоже рассказали,
что несмещенность
это, конечно, хорошее свойство,
но это не панацея.
Давайте можно на всякий случай
проверить, что у SEGA это будет
несмещенная оценка.
Понятно, у Ugoar тогда это будет
уже смещенная оценка, но проблем в этом нет.
Проверяем.
Берем, опять же, условное математическое ожидание.
И грек в этом условном математическом ожидании
никак не фигурирует, потому что
он взят с прошлой террации,
поэтому это условное математическое ожидание действует
только на IKT, на наш выбор координаты.
Действуем тогда
ею.
Условно математическим ожиданием.
Вот.
Ну и когда подействуем условно математическим
ожиданием, что у нас вылезет, кто понимает
быстренько, из условного математического
ожидания.
Опять же, сумма.
Ну давайте
я вот так буду записывать, так как мы каждую координату
с вероятностью 1 на D эта вероятность
вылезает. Понятно, она сейчас
сократится сразу же.
И здесь у нас что будет вылезать?
y, it, j,
j, индекс,
d, e, j,
базисный вектор, или давайте базисный вектор
я даже поставлю.
За пределы я вот так вот сделаю.
f, x, k, t,
j,
e, j.
Вот.
Все сократилось. Понятно, что когда вы сейчас
просумируете вот эти все y,
y, j, t умножить на e, j, t,
это будет просто полный вектор
y, и тогда у вас здесь останется
честный градиент f от x, k.
Вот. Это не выполняется
для Егора, как вы понимаете, в силу того,
что там нет этого домножения на d.
Ну, не страшно, не страшно.
Вот. И аналогичным
образом здесь делается
техника редукции дисперсии.
То есть, также в доказательстве вы
используете
дополнительную
последовательность, дополнительную
последовательность. Единственное, что
здесь это вроде как
техника редукции дисперсии сильно и не нужна.
Потому что, когда вы выбираете координаты,
как мы поняли,
это не страшно,
потому что у вас
градиент
стремится к нулю.
Но даже если вы выберете у него какую-то случайную координату,
она тоже стремится к нулю, потому что весь вектор
стремится к нулю, значит и координата стремится к нулю.
Координаты стремятся к нулю.
Поэтому дисперсия
того, что происходит вообще
в координатном методе, она не так страшна.
То есть, она действительно
стремится к нулю, поэтому variance reduction
в координатном методе не нужен.
Но почему бы его не сделать?
Просто потому, что это какие-то хорошие свойства
памяти, которые у вас есть.
Вы запоминаете старые координаты
и говорите, что их переиспользуете.
Значит, возможно вносите
что-то хорошее в метод, который у вас есть.
Оценки сходимости
у Сеги ровно такие же,
как у обычного координатного спуска.
В LZD никакой панацеи тут не происходит.
Опять же, потому что
такая вот задача
можно рассмотреть в нашу неприятную
квадратичную задачу с неприятной стартовой точкой.
То, что вы запомнили,
что происходит в других координатах,
где и так были все нули,
ничего хорошего, ничего плохого вам это не даст.
Поэтому вам нужна
конкретная координата.
Все вырубилось, да?
Ну ладно, это, наверное, знак.
Это, наверное, знак.
Сега, соответственно,
не лучше, не хуже.
Но, опять же,
выбор координат, он, возможно, не только
в нераспределенном случае.
На следующей лекции, на последней лекции
мы с вами поговорим про
так называемые
распределенные задачи обучения.
И там нам тоже во многом
будут пригождаться методы, которые
выбирают координаты. Но там у них физика уже другая.
Там борьба идет не за подсчет
градиентов.
Включился?
Что с ним происходит, непонятно.
Там идет борьба не за подсчет
градиентов, ну и за них, на самом деле,
тоже нет. Там больше идет борьба за то,
чтобы пересылать меньше, поэтому вы выбираете
координат. То есть вы считаете
полный градиент, но используете
координатный метод просто потому, что хотите
переслать меньше информации,
вместо полного вектора переслать
1% координат.
И вот там уже
возникнет своя специфика,
которая будет неприятной,
с которой мы с вами, соответственно, будем бороться.
И там уже какие-то техники редукции
дисперсии для координатных методов.
Ну там мы уже будем называть их методами
сжатия.
Там они
понадобятся, там они понадобятся.
Что с ним?
Там они понадобятся,
поэтому жду на следующую
лекцию. Она нам будет такая, скорее, уже обзорная.
Доказательств я там
уже показывать не буду, просто скорее
пройдусь по идеям распределенных методов,
которые не
затронуты будут, например,
вот в пятницу-субботу.
Просто так пройдемся, поговорим.
Понятно, что в программе этого всего в колокве
и монету. Познакомимся,
ну и на этом закончим курс лекции.
А на сегодня все, всем всем спасибо.

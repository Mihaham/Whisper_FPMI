Всем доброго вечера! Мы с вами продолжаем изучение CUDA. Сегодня у нас вторая лекция из нашего курса,
и сегодня мы будем говорить, наконец-таки, уже про более глубокие вещи, а именно про
лимитирующие факторы, которые есть на видеокарте. Сегодня мы рассмотрим, собственно, немножко более
глубокое устройство видеокарты, поймем, что такое варпы, и поймем, какие кошлинии существуют на
видеокартах. Значит, наверное, мы с вами в прошлый раз, значит, когда мы говорили уже на семинарах,
я спросил, что вы использовали код на семинарах, да, и каким образом вы замеряли время, которое
выполняет программа? Замеряли как-нибудь? Наверное, вы использовали утилиту под названием nvprof. Была
такая? Сейчас я напишу. Было такое? Что? Можете включить? Прошу маним.
Нет. Все, хорошо. Знаете, в чем проблема эта утилита? Она не работает на новых видеокартах.
Да, это поколение 383090. Да, то есть она работает на поколениях не более 20xx. На самом деле выполняется
ассинхронно. То есть, если мы говорим про исполнение кода, вот это момент времени t, у нас с вами есть два
потока. Один поток исполнения на CPU, а второй поток исполнения на GPU. И здесь нам понадобится
небольшое знание диаграмм UML, чтобы вы поняли, как это работает. Совсем небольшое. Значит,
когда вы вызываете функцию на потоке GPU, то вы по факту выполняете ее с получением какого-то
результата. То есть, у вас функция на самом деле является синхронной. Синхронной. Когда вы выполняете
какую-то еще операцию на CPU, если вы не создаете отдельный поток, то вы работаете в синхронном
режиме. Важно, что когда вы вызываете функцию kernel с угловыми скобками, и эти угловые скобки
что означает? Вот это означает количество блоков, он же еще grid-dim, и есть у нас количество
трэдов. Это называется блок-dim. То у вас вот этот кернов будет исполняться ассинхронно.
Что такое ассинхронная функция?
Да, это функция, которую мы отправляем, но результат которой мы не дожидаемся. То есть,
мы отправляем код на видеокарту. Значит, каким образом достичь синхронизации вот этого всего
процесса? А вот это у нас, кстати, таймлайн для исполнения гидра. Каким образом дождаться
результату выполнения функции? Здесь нужно использовать один из двух механизмов. Механизм
номер раз, плохой. Вызвать функцию. Не пугайте, это завтра будет. Первая функция, это барьерная
функция, которая называется kuda-device-synchronize. Но это ставит глобальный барьер на выполнение кода
на цпу и на гпу. Kuda-device-synchronize функция, которая выполняет ядро. То есть, она блокирует все
потоки исполнения до тех пор, пока и не на цпу, и на гпу не выполнится наш код. Это вариант плохой,
потому что это глобальная блокировка. Поэтому давайте сделаем чуть хитрее. Как этого избежать?
Значит, для того, чтобы замерить время работы одного ядра, ну, это глупо делать, заставить
общую блокировку. Поэтому давайте сделаем хитрую вещь. Значит, мы с вами воспользуемся такой функцией,
которая позволит нам замерить. Вот, смотрите внимательно. Поставить точку, отсечку, отсечку на гпу таймлайне.
Дальше, когда мы делаем здесь, выполняем наш код, который у нас идет, синхронный или асинхронный,
после вызова асинхронного кода, мы поставим отсечку в коде, который стоит после запуска ядра. То
есть мы ставим отсечку, просто говорим, что окей, в ветке исполнения у нас появляется
здесь наш старт, а здесь у нас появляется наш конец. При этом, вот когда мы ставим отсечку,
это событие реально еще может быть не выполнено. Это Netify на будущее. Что нужно сделать для того,
чтобы мы получили результат времени? Вот тогда мы уже ставим блокировку на то, что мы хотим получить
именно вот этот результат. То есть ставим какой-нибудь синг по временной линии,
какой-то момент времени. И тогда, после того, как мы поставим синг на то, что у нас событие n закончено,
мы получим результат вычисления нашей функции. Теперь нам нужно понять, когда нам надо ставить
точку синхронизации. Желательно ее ставить до синхронной операции. То есть смотрите, у нас
выполняется ядро, после этого мы выполняем какую-то синхронную операцию после, а после того, как мы
выполнили эту синхронную операцию, вызывать дожидание, вызывать ожидание вот это, наступления вот
этого события. Это событие уже выполнено, поэтому функция синхронизации сработает за вот единиц.
То есть ставим это все за момент времени выполнения ядра. Какие синхронные функции бывают?
Наверное, первая функция, но она глобальная. Какая еще функция? Какие функции вообще посмотрели два семинар?
Не, CUDA Event Create, а то вот как раз засечка точки. Вам даже про CUDA Event Create уже рассказали. Я
как раз про них рассказываю. Какие еще функции были? Ну, как обычно называется.
Так, ладно. Да, бинго. CUDA Memspy. Значит, смотрите, у функции CUDA Memspy, что она делает? Она копирует
память с одного источника на другой, либо с девайса на хост, либо с девайса на девайс,
либо с хоста на девайс. Так вот, если функция CUDA Memspy вызывается с параметрами h2d и d2h,
и функциями d2h, то эта функция является синхронной, потому что нам нужно дождаться события того,
что и там, и там появляются источники данные. То есть обычно происходит так, что вы засекаете
точку времени, ставите end, после этого ставите какой-нибудь CUDA Memspy и после этого получаете
информацию о том, а действительно за какое время у вас получено событие. Просто если вы так не
сделаете, то у вас в итоге ноль получится, потому что у вас вот эта delta t будет равной нулю. То есть
она будет обновлять событие только после того, как вот событие по факту выполнется. Понятно вот
этот концепция. Вывод из всех этих рассуждений состоит в том, что нам помимо точки замера
времени, нужно еще точка синхронизации с часами, которые идут на видеокарте. И для этого как раз есть
некоторые функции. CUDA Event T это событие времени, которое мы можем с вами использовать. CUDA Event Create
это функция, которая создает событие. CUDA Event Record это записывать это событие. CUDA Event Create
это конструктор класса, точнее инициализатор структуры. CUDA Event Synchronize это ожидание
исполнения события. То есть вот как раз вот этот розовый sync, который я написал, это как раз вызов
CUDA Event Synchronize. Да? Не, не нужно. Да, то есть вы вызываете CUDA Memcpi, CUDA Event Synchronize
после Memcpi и у вас эта синхронизация пройдет вообще мгновенно, потому что это синхронное
событие уже наступило. Вот. Зачем ждать события? Собственно, вот экран у нас. Да, и тут говорится,
что ядро, еще раз давайте почистим, ядро выполняется ассинхронно. Более того, там ассинхронность
намного более круче. Мы это рассмотрим на четвертой нашей лекции. Да, если не вызвать CUDA Event
Synchronize, то вы получите 0 по времени. Вот. И это можно сравнить с трекером прохождения дистанции.
То есть вы когда, не знаю, жите, вам важно именно только ваше время, а не время того,
которое произошло от начала старта. Вот. Собственно, вот такой код. Давайте его разберем. Возможно,
вы его видели. Уже. То есть у нас происходит замера точки CUDA Event Create от старта. Потом у нас
происходит замер события Stop. Потом мы вызываем с вами CUDA Event Record от старта. Дальше функцию Add
мы вызываем от количества блоков и размера блока. Ставим события Stop. После этого делаем CUDA
Memcpi и только после этого ставим CUDA Event Synchronize. Вот Stop. Вот. После этого мы можем
дернуть события CUDA Event Tail Up Time, которая замерит нам количество миллисекунд по чужну,
которая исполняется наше ядро. Давайте, а? Вопрос. Да. Зачем здесь стоит Synchronize? Ну,
специально стоит, чтобы, типа, убедиться, что это событие действительно произошло.
Можно попробовать, кстати, запустить код с синхронизом и без синхрониза. Посмотреть,
что произойдет. Что? Да, да, да. Они гарантируют, что те трекеры, которые ставятся на CPU,
они идут в порядке. То есть они линиаризованы. Ну, получаем вот такой вот результат. А теперь,
так, давайте вопрос по примеру. Это откуда, куда мы копируем память? То есть, смотрите,
мы вычислили код на ядре. Мы создали dx, dy, а дальше нам нужно скопировать результат на
наше устройство. То есть у нас же есть две памяти. У нас есть память хоста, есть память девайса.
Вот, мы берем наш память девайса, в котором мы записали сумму двух массивов, и нам надо
его скопировать на операционную систему. Вот. И это указывает направление, откуда, куда мы копируем.
Так, а теперь, смотрите, тонкий момент, который необходимо посчитать. Нам нужно посчитать
количество операций, которые мы с вами выполняем. Значит, для этого я скажу, какая функция там считалась.
Функция, которая считалась там, она была такая. х плюс равно y-векторно. То есть там for int и равно нулю,
и меньше n, х и то равно х и то плюс y и то. Давайте посчитаем количество операций,
которые мы с вами можем делать. Значит, все замеры производились на видеокартах,
которые у нас есть на кластере. У нас на кластере 4352 ядра. 4352 ядра. Многовато.
Будет. Значит, частота ядра при этом самого полтора гигагерца. Гигагерца. Мощный такой процессор,
но два раза слабее, чем обычный. Более того, мы, смотрите, еще можем прогонять две операции за цикл.
Это связано как раз с информацией о рендеринге, которая у нас была. То есть по факту видеокарта
устроена так, что она умеет гонять две операции за цикл. То есть заменять тот источник, который у
нас имеется. В итоге мы получаем с вами вот такую прекрасную вещь. 4352 умножить на 1545 умножить на 2.
Это количество операций в секунду, которое мы можем делать в мегагерцах. То есть 13,5 миллионов
мегагерц. Это 13,5 тысяч гигагерц. Получаем 13,5 терра флопс. Терра это 10 в какой-то большой степени.
А это расшифровывается как floating operation per seconds.
Многовато будет.
То есть можно посчитать много чего. Вопрос. Кажется, мощность большая, все замечательно.
Но чем приходится платить? Это раз.
Ну есть контекст такое. Давайте вспомним пример, который вам показывали на семинарах уже. А что с
памятью, собственно? Тут проблема с памятью. Смотрите в чем. Просто. Насколько я помню,
в примере складывается 2.28 элементов. Проверим сейчас. Я могу, в принципе,
по SSH на сервер зайти и сделать сейчас прям эксперимент.
Так, пожалуйста, тут нужно это. Так, значит, по real computer. Так, clear делаем. Так, значит,
куда. Смотрите, у нас тут 2.28 элементов. Выделяется память, замеряется время.
Да, тут, к сожалению, Vim. Вот. И у нас получается количество миллисекунд.
nvcc mainq-o3 main. Смотрим NVIDIA SME. Какая видеокарта свободная. Ну, пятая.
Девайс с пять. Вам рассказали вот про такую штуку? Про переменное окружение. Ну, хорошо. Она
позволяет установить определенную видеокарту. Так. Так, сейчас. А, ну, смотрите, пример. Видите,
CUDA Event Record Stop. Что происходит, если мы поставим CUDA Event Lab, ставим до CUDA WMCPI.
Логично, что мы получим ноль. Возвращаем сюда. Заодно проверим, нужно ли нам событие.
Сейчас. А потому что у нас CUDA Event Record, CUDA замер вот этого события произошел до того,
как это событие реально произошло по времени. То есть пока у нас ядро выполнялось на видеокарте,
мы сразу взяли дельту. Хотя там еще должно 5 миллисекунд протечь по времени. Так,
еще один пример. Ну, кстати, видно, что CUDA Event Synchronize можно не использовать.
Вот спросили пример, нужно ли его использовать или нет. Так, ну, смотрите,
мы получаем на 2,28 элементов сложение за 6 миллисекунд. Так, ну, давайте посчитаем,
сколько это у нас. 2,28 делить на 0, 0 сколько? 6 делить на миллиард.
44. Да. Да, и как говорится, и как тут поется в одной известной песне Сакнаэль,
не будем ее цитировать, благо у нас тут учебное заведение. Что-то здесь не так.
Да, собственно, что-то у нас происходит не то. Давайте разбираться, где подвох.
Процессе взаимодействия в памяти. Именно так. А давайте посчитаем пропускную способность
канала. То есть по факту, что у нас происходит? У нас происходит 2,28 элементов. У нас с вами
происходит операция чтений из памяти, видового элемента массива и операция записи видового
элемента массива. Поэтому посчитаем пропускную способность, которая у нас есть. Значит, сейчас,
я не знаю, на курсе по операционным системам, вам рассказали, как работает RM внутри?
Random Access Memory. Как вот эта вот платка работает? Ну, в целом, да, но просто
как считается пропускная способность канала? Просто сейчас понадобится. В общем,
пропускная способность вычисляется из следующих трех коэффициентов. Во-первых,
это частота, на которой работает оперативная память. Сколько тактов мы можем прогонять. Дальше
пропускная способность самого канала, шина. И еще есть такой коэффициент под названием Efficiency,
эффективность канала пропускных данных. Наверное, вы слышали, что есть DDR2, DDR3, DDR4. А? Уже 5
появился. Это специальное, так сказать, поколение памяти DDR. И каждое это поколение самостоятельно
собственно улучшается. И вот этот коэффициент эффективности как раз зависит от архитектуры DDR.
Сколько по факту, там сказать, сколько раз мы можем прогнать наш сигнал за счет того,
что у нас современная оперативная память. Итак, частота оперативной памяти для той
видеокарты, которая у нас была 1750 МГц. 1,7 ГГц. Пропускная способность шины
352 бита, подчеркну внимание, а не байты. А эффективность равняется восьмерке.
Спрашивается, как это посчитать? Ну, я не ожидаю от вас ответа. Знаешь, это, точнее, давайте,
все-таки одну часть ответа я хочу дождаться. Я хочу дождаться, почему здесь есть двойка точно.
Ну, двойку из восьми, по крайней мере, мы сможем с вами объяснить. Потому что D,
как расшифровывается? Double data rate. Вот одна двойка идет отсюда. Осталось четыре.
Почему? Ну да, осталось две двойки. Не-не-не. Вообще архитектура,
которая стоит в 28 ста, это GDDR6. Graphical double data rate 6. Вот. И если почитать спецификации
именно этой архитектуры, то окажется, там четыре передачи за цикл. Потом, по-моему,
следующий появился тоже GDDR6X, и у нее уже 8 передач за цикл было. То есть,
вам цикл передает двойной кусок данных, и у вас еще есть четыре передач за цикл. В итоге 8.
Ну как это? С чем бы это сравнить? А, ну смотрите, пример, канонический пример.
Вот вы хотите пройти через турникет в электричке. Да, ну, на станции
Долгопрудная или Станция Новодачная. Хотя на Ставодачной там этот, как это называется?
Валидатор, да, на Долгопрудной. Допустим, валидаторов нет. Ну и как можно пройти? Пока охранник не видит,
можно пройти не одному человеку за один билет, а нескольким человеком за один билет. Ну вот,
приблизительно так же здесь и происходит. Да. Но только это здесь легально происходит за все
специфики архитектуры. Ну да. Не, ну я просто про суть, типа как это может быть, рассказал. Вот.
В итоге получается, смотрите, 616 гигабайт в секунду, если перемножить все эти четыре коэффициента.
То есть 1750 на 352. Почему я не перемножил на 8? Да, потому что мы уме поделили на 8.
Что? А потому что в битах измерялись, а не в байтах. 616 гигабайт в секунду. А теперь давайте посчитаем,
сколько у нас с вами в нашей задаче операция чтения и памяти. У нас два в 28 раз. Мы делаем
следующее. Выполняем задачу у равно плюс равно х. Сколько операции чтения на один элемент массива?
И записи. Да. Read x, read y, write y. В итоге получается, сколько у нас элементов задействовано.
Сколько памяти нам надо перегнать? Два в 28 на три на четыре байта. Два в 28 на 12 байт.
Вот столько памяти нам надо перегнать. Давайте посчитаем, сколько два в 28 на 12 байт.
Сколько? 3, 2, 21. Это у нас гигабайта. А теперь поделим вот это число на 616.
Ой.
Мы 5,2 мс из наших шести потратили просто на взаимодействие с памятью. То есть память
является лимитирующим фактором. Ну если теперь честно, честно вот эти вот которые у нас были
два в 28 делить на 0, 0, 0 сколько там? 8? Ну уже лучше по крайней мере. А это знаете,
что я посчитал? Это сколько терафопсов на самом деле выдает видеокарта на сами
вычисления? Ну там зависит от. Тут можно еще пятерку накинуть. Ну в общем уже под терафопс можно
посчитать. То есть операции на самом деле столько. Но мы еще не учитываем, что у нас есть операция
загрузки, загрузки выгрузки регистров и всякое такое. Еще есть цикл 4, по которому нам надо ходить.
Чтобы террироваться, плюс еще переключать контекст. То есть у нас получается, смотрите,
большую часть времени мы просто занимаемся копированием памяти, чтением записью памяти.
87 процентов времени. Надо как-то это решать. Логично? Погнали решать. Да, мы вот это как раз посчитали.
Типа количество операций, которые необходимы для чтения записи. То есть мы получаем три
операции. Всего 12 n-байт. Ну вот. И мы таким образом можем посчитать эффективную пропускную
способность. Если мы посчитаем количество операций, поделим на время исполнения программы. То есть это
каждая операция взаимодействует с четырьмя байтами, три операции на каждый элемент. Это
тоже самый пример. Вот. КПД у меня получалось 90,5 процентов времени у нас было, но тут даже еще
меньше получилось. Как можно ускорить? Давайте подумаем. Вот вы, разработчики видеокарт,
не поможет. Кэши надо добавить. Нам нужно добавить кэш-линия. Вот. Ну, можно еще сделать
еще другим способом. Значит, если вы хотите быстрее копировать участки памяти, это немножко
другая тема. Если вы замеряли код, вы видели, что КУДУМ-25 девайс-ту-хост очень долго времени
использует. Вот. Поэтому, если вам нужно просто эффективно перекидывать память с хоста на девайс,
то вы можете алоцировать память, которая не является записываемой памятью. То есть вы в нее не
можете записывать память, но можете передавать между разными ядрами. Это называется Pint Memory.
Функция, которая делает это, называется КУДА-хост-лог. То есть вы алоцируете память,
именно прибиваете ее к земле для того, чтобы в дальнейшем можно было копировать. Но это
продвинутые совсем вещи. Преимущественно обычно нет. По-моему, у меня есть тоже пример,
в который можно увидеть этот КУДА-хост-лог. А вот он, в принципе. То есть, смотрите, тоже еще одна
особенность нискоренных систем. Значит, у нас память на хосте, на самом деле, когда
отправляется на видеокарту, она происходит следующее. Ее прикрепляют, Pint Memory, и потом мы копируем
это все в оперативную память устройства. Вот. При этом, если у нас, то есть нам нужен дополнительный
участок копирования памяти осуществить. Pageable памяти, в которой вы сможете прочитать. Вот это
память вы не сможете прочитать, у вас сиквел включится. Вот. Поэтому, если вам не нужно записывать
память на хосте, то лучше делать так. Как сделано справа. То есть, один раз пригвоздили, а дальше
копируете, пересылаете и так далее. Вот. И мы наконец-таки достигаем вот этой пирамиды,
которая является пирамидой памяти. Ой, это быстро. Наверное, вы даже делали вот такой вот эксперимент,
что ходили и смотрели, сколько занимает память, сколько денег стоит тот или иной участок памяти.
Значит, здесь что касается иерархии памяти. Значит, на самом низком уровне здесь находятся самый
медленный источник памяти. Чем мы идем выше, тем у нас быстрее источник памяти, но тем дороже нам
приходится платить. Ага. Кто-нибудь знаком с самым низким уровнем хранения данных в этой пирамиде,
которая у нас здесь есть? Да, магнитная лента. А? Не знаю. Ну, до сих пор, если есть какие-то данные в
архивах, они записываются на вот эти вот магнитные ленты с бобиной, из которых можно достать. Кстати,
для специально хранения данных на ленте, чтобы данные быстро восстанавливались и не особо бились,
придумали специальный формат хранения данных. Знаете, какой?
Тар. Тейп-архив расшифровывается. Так что теперь вы знаете, что такое. Почему исторически в
линуксе используется TAR Gazette архивы? Потому что они еще и в UNIX были для записи на ленты.
Ну, наверное, с лентами вы вряд ли столкнетесь. Следующий уровень – это диск. Это hard drive. Давайте
посчитаем скорость доступа к hard drive. Какая скорость доступа к hard drive обычно у нас? Сейчас.
Ну ладно, вы что, фильмы не копируете или что? Сейчас в интернете смотрим. Ну, давайте посчитаем
где-то 100 мегабайт в секунду. Скорость копирования данных на hard drive, которую реально можно достичь.
Так, сколько стоит hard? Так, рубрика эксперимента. Так, извините. Так, давайте.
Так, идем покупать HDD. Так, ну 4 терабайта возьмем.
Что получается? Две сто. Ой, извините, рублей рисовать не умею. Две сто рублей на терабайт.
Ну вот. Так, поднимаемся по иерархии выше. Что у нас в иерархии выше?
Нет, подождите, какой рам? SSD у нас дальше. Так, покупаем SSD. Вот такой возьмем, да?
Так, что у нас получается? Скорость доступа у нас 500 мегабит. Такая же будет?
Где? А есть тут категория?
SSD M2, да? А, вот они. Это NVMe. Вот он. 12 тысяч, да? 12 тысяч 7800, да?
Так, сколько скорость у него доступа? Три с половиной, да? Три с половиной терабайта. Три с половиной гигабайт в секунду.
Смотрите, что произошло. Здесь, в принципе, не сильный фазовый переход случился. Дальше мы поднимаемся в рам.
Здесь нужно понять, сколько мы... Какая скорость ощущения в оперативной памяти?
Купаем оперативку. Нет, DDR4. Хорошо. Значит, скорость доступа 2,6 гигагерца.
2,6 гигагерца стоит 4000 на 16 гигабайт. 4000 рублей уже на 16 гигабайт.
Получается, на терабайт это будет много. Надо 4000 поделить на 16 умножить на... На сколько? На тысяч. 250 тысяч рублей на терабайт.
Хорошо. Сколько мы за эту скорость платим? Мы получаем 2,6 гигагерца в секунду.
2,6 гигагерца. Тут шину памяти нужно смотреть. В порядке тут тоже возрастут.
Гигагерц. Надо поспать машину. Давайте посмотрим. Рубрика сегодня. Покупаем видеокарты.
Давайте DDR4 Bandwidth... А, вот они. 90 гигабайт в секунду.
Смотрите, что произошло. У нас пропуская способность выросла в 20 раз. 25 раз. Но при этом что произошло у нас с вами?
А цена не в 25 раз выросла. Под 40 где-то. Дальше вычислять все сложнее, потому что в нашей иерархии возникает кэш.
Да, ну теперь... А все остальное находится в процессорах на самом деле. Что, какие последние видеокарты? О, процессор Ryzen там какой-нибудь.
А откуда тут DDR4, кстати? Тут в комплекте что ли продается? Ну вот. А, это максимальная поддержка. Все.
Ну, смотрите. Давайте смотреть. У нас объем кэша L2 4 мегабайта, 32 мегабайта. То есть у нас объем 4 мегабайта, 32 мегабайта.
Скорость доступа у нас здесь будет, по-моему, если не ошибаюсь, где-то в 20 раз быстрее, чем в оперативной по кэшам. То есть это приблизительно 1800 гигабайт в секунду.
Ну, вы можете прогнать информацию. Там будет следующий слайд. А стоимость этого чудо-дела будет уже... Ну, сколько вы отводите времени на то, что у вас есть кэш в процессоре?
Давайте тысячу рублей оставим. Ну, не тысячу, ну ладно, 500 рублей даже. На сколько? На 32 мегабайта. Ну, получаете 2-3 миллиона рублей на терабайт.
А? Ну да. Напишем просто много. Можно посчитать. То есть видно, как у нас хлопывается пирамида. И, кстати, что на самом верхнем уровне этой пирамиды? Регистры.
Регистры получаем за один такт. То есть работаем по частоте оперативной памяти, ой, по частоте нашего процессора. Регистров всего сколько? Ну, 32.
Вы можете посчитать, сколько стоит один... Да, тут будет бесконечность. Где? Вот тут? Ой, блин, надо считать. Не, скорость увеличится... Смотрите, стоимость уменьшилась в 8 раз по прикидкам, а скорость уменьшилась в 500 раз.
То есть 500 делить на 8. Ну, смотрите, тут у нас деление на 8, а здесь у нас идет умножение на 500. Ну, то есть 120... А, на 5 тысяч. Не, на 500, на половинку от этой суммы. То есть вот это поделить на вот это это 500.
16 тысяч на 32. 500. То есть здесь у нас получается 500 делить на 8. 60, да. Сколько это будет? 15 миллионов рублей, кажется.
Ну да, 15 миллионов рублей на трабайт мы получаем где-то. Это по прикидкам, что мы приняли, что кэш у нас стоит 500 рублей всего лишь от всего процессора.
Ну да, все прекрасно. Теперь вы понимаете, собственно, почему эта иерархия выглядит именно таким образом. Ты прох, так сказать. Вот это понятно, да?
Если мы поднимаемся на уровень кэша регистров и оперативной памяти, здесь происходит следующая вещь. Что у нас есть L1 и L2 кэш. L1 и L2 кэш отдельно для каждого ядра. И дальше мы обращаемся к L3 кэшу, если у нас получается cache miss.
В итоге у нас получается до L1 и L2 кэша мы доходим за 5-12 циклов процессора. Потом мы до L3 кэша заходим за 35 циклов процессора. А если мы поднимаемся дальше в оперативную память, то это 200 плюс циклов процессора.
Ну, кстати, по цифрам мы с вами совпали. То есть вы понимаете, насколько у нас отличаются результаты. В иерархии GPU на самом деле все очень похоже, но есть некоторые детали. Вы помните, что такое стриминг-мултипроцессор?
Это по факту аналог ядра в нашем тетральном процессоре. Здесь есть две уровни кэша. Это L1 кэш, который доступен на каждом стриминг-мултипроцессоре, и L2 кэш, который является общим между всеми стриминг-мултипроцессорами.
Обычно L1 кэш, кстати, здесь имеет некоторые особенности. И смотрите внезапно. Ширина кэшлини 128 байт.
Здесь должен произойти инсайт. Сколько элементов у нас поместится в одну кэшлинию?
А базовый тип, сколько весит в байтах? 4,4.
Что у нас имеет параметр 32? Количество куда ядер в одном ворпе. То есть сделано все максимально так, чтобы у вас кэшлиня подгружалась на ворп.
То есть вот он у вас ворп. И к нему прямо одновременно прилетает участок памяти, который идет последовательно. То есть если вы к участку памяти обращаетесь не последовательно, у вас получается кэшвиз.
То есть в нулевом левете массив надо обращаться желательно к нулевому, к пятому, к пятому, к девятому, к девятому. А они делают это сикосинакоси.
Вот уже пошли приколы, связанные с видеокартой. Хорошо. Так, вот это понятно про кэшлиню.
Теперь отличия от ЦПУ и ГПУ. В ЦПУ у нас есть L3 кэш, здесь у нас есть L2 кэш. При этом смотрите, насколько сильно замедляется скорость доступа к L1 кэшу.
За 80 тактов приблизительно это делается. А если мы говорим про L2 кэш, то скорость доступа к нему приблизительно такая же, как к оперативной памяти видеокарт.
Ну чуть быстрее, но в целом сойдет. А теперь, так, вот это тоже понятно.
Особенностью, значит, как выглядит это теперь в логическом устройстве. Значит, здесь есть огромное количество памяти, которое мы с вами еще не смотрели.
Значит, у нас с вами, напоминаю, что grid, вот это вот набор блоков, он делится у нас с вами на блоке, а в блоке есть потоки.
Вот, и оказывается, что есть глобальная память, есть константная память, константная память лимитирована, куда вы можете сложить все свои константы.
Есть текстурная память, откуда вы можете подгрузить текстуры. Дальше, у каждого потока есть, у каждого блока есть shared memory, которая доступна на блок.
Текстура? Смотрите, представьте себе, вы меня, вы решили отрисовать вот эту вот стенку, которая здесь есть в игре.
И решили вместо вот этого вот прозрачного фона наложить какой-нибудь орнамент. Вы берете png-шку и накладываете эту png-шку как орнамент. Вот эта вот png-шка называется текстурой, которую вы накладываете.
Ну как раз то, что мы хотим отрисовать, то есть то, что мы хотим положить на какой-то поверхность для того, чтобы отрисовать ее на экране.
Напоминаю, раньше это тоже было текстурной памятью, но мы, когда перешли в концепцию куды, мы поняли, что нам нужно использовать видеокарту не только как задачу отрисовки графики, но и как задачу общего назначения.
Поэтому отделили эти два вида памяти.
Смотрите, у нас важно, shared memory находится на блоке. Регистры при этом привязаны к каждому потоку, и у каждого потока существует локальная память по аналогии с хипом.
Ой, по аналогии не с хипом, извините, со стэком. То есть внутри ядра тоже можно использовать свой стэк, но тут есть некоторые ограничения.
И замечательная особенность, что разделяемая память доступна на блоке.
А теперь сюрприз-сюрприз. Можете ли вы управлять кэшом в процессорах общего назначения?
Вы когда-нибудь видели инструкцию, в которой вы управляете кэшом?
Ну да. А здесь это является особенностью в видеокартах.
На самом деле, когда вы используете подчеркивание подчеркивания shared или используете разделяемую память, вы как раз пользуетесь L1 кэшом.
Уникальная особенность видеокарты стоит в том, что мы можем на уровне кода использовать L1 кэш.
При этом ключевое слово, которое объявляется, это shared. То есть вы объявляете, допустим, массив внутри блока.
Это будет выглядеть так. shared int, допустим, dx.
Ну, 256 элементов.
И каждый поток будет видеть этот участок памяти внутри одного блока.
То есть внутри одного блока у вас будет один участок памяти, в другом блоке выделится такой же участок памяти.
Вот. И при этом скорость доступа будет на уровне L1 кэша.
Данные распространяются между всеми потоками в одном блоке.
И, собственно, мы работаем с shared памяти уже на скорости несколько терабайт в секунду.
Мне кажется, супер. Да, правда, есть ограничения.
Значит, размер этой памяти внутри блока ограничен сверху обычно каким-то параметром, вида 48 килобайт.
То есть много слишком памяти вы не выделите.
Вам рассказывали на семинарах, какое количество потоков может вместиться в одном блоке сверху?
Ну да, причем степень двойки.
Не больше, чем 1024 потока вы можете выделить в современных архитектурах видеокарт на один блок.
Ну вот. Ну, в общем, вот так вот.
Вот этот участок возникает такой замечательный.
И теперь вы можете делать всякие локальные операции.
Допустим, считать производные.
Ну что, что такое производные?
Ну, предел. Не, если мы говорим в терминах вычислительной математики.
f от x плюс дельта х минус f от x поделить на дельта х.
Где дельта х, это по факту следующий элемент массива.
То есть вам нужно эффективно брать соседей из определенного элемента.
Вот. Так же будет работать блок на перемножение матриц, который вам придется реализовать.
То есть вы берете блок, принимаете как шерсть памяти.
И в нем делаете перемножение.
Строки на ставца.
Вот это будет работать намного больше.
Взяли текущий элемент массива.
Это момент времени x плюс x.
Взяли следующий элемент массива, который x плюс дельта х.
Посчитали между ними разность.
Да, тоже с точки.
Значение нашей функции.
Кто? Значение функции?
Ну, мы их подаем на вход.
Они у нас посчитаны.
Ну, то есть это вычислительная задача.
Нам нужно просчитать производную функцию.
Ну и берем, считаем.
Так. Как определить размер шерсть памяти?
Первый способ. Он выглядит вот так.
То есть вы явно объявляете размер массива.
Второй способ. Это вот такой вот.
Вы указываете следующую конструкцию.
Extern.
Shared.
Int.
x. И оставляете квадратные скулки.
Не указываете размер.
Как указать размер?
Extern. Что означает ключевое слово?
Где-то там будет. А где это там?
Ну, где-то там, на самом деле, это в параметре функции вызова ядра.
Третьим параметром в функции вызова ядра,
в операторе вызова ядра,
это количество оперативной памяти,
которое вы делаете на блок.
В байтах сразу предупреждаю.
А не в количестве элементов массива.
Собственно, здесь уже начинается прикол.
Почему?
Потому что до текущего момента мы вычисляли все элементы со соседним.
То есть мы вычитали векторные операции, которые могли считаться параллельно.
Здесь же нам нужна синхронизация.
Потому что мы берем, допустим, значение элементов в текущем массиве,
на текущем элементе в следующем элементе массива
и записываем значение в и текущий элемент массива.
В чем проблема вот этого кода?
Ну, у нас дата рейс.
Более того, у нас потоки могут выполняться следующим образом.
Что у нас есть один поток, у нас есть второй поток,
а потом у нас вычисляется код.
Если мы не поставим барьер, то мы можем сильно поплатиться.
Из-за того, что код выполняется параллельно.
Допустим, здесь элемент массива еще не посчитался.
То есть нам нужно приметить синхронизации,
когда мы работаем со соседним элементом массива.
И на уровне блоков мы можем делать все эти операции.
Когда мы работаем со соседним элементом массива.
И на уровне блока есть такой оператор,
ну или одного каша.
Это некоторый пример кода,
который считает там какой-то дельта х.
И какое здесь есть слово, которое вы еще ни разу не видели.
Сингтрец. Что делает сингтрец?
Сингтрец это барьер внутри одного блока.
То есть у вас два блока между собой могут не синхронизироваться.
Но внутри они должны синхронизироваться.
То есть после каждой записи в оперативную память,
нужно писать сингтрец.
Но это барьер.
То есть все потоки должны пройти друг в друга.
Должны пройти через этот барьер.
Так.
Сейчас помню.
Вопрос. Как дедлог получить?
В цель неаккуратным мы использовали сингтрец.
Да.
Ура, мы получили сингтрец.
Ну что?
Ура, мы получили дедлог.
Ну не писать такой фот.
А?
Все?
Ну все, программа завечна.
Ну понятно?
Да.
Ну все.
Ну все.
Ну все.
Еще drive, да?
Да, CTRL-C.
Или kill-9, в зависимости от того, что вам больше нравится.
Так, про доступ данно.
Тоже поговорим.
Тоже еще один тезис про pipeline cache
Размер блока 256
Хотим одним потоком обрабатывать два элемента.
И здесь два примера.
Способ один.
Это обрабатывать элементы вот таким способом.
То есть Первый поток обрабатывает элементы 01,
обрабатывать элементы 2, 3 и так далее, либо второй способ это обрабатывать потоки под
номером 0, 256, 1, 257 и так далее. Что, как вы думаете, более натуральный для видеокарты?
А? Конечно, второй способ, потому что первый способ просто ломает все кашлинии. Вот такой
вот код есть. Вы даже его на семинарах посмотрите. Одна функция называется add,
другая называется stupid add. Вторая функция высчитает элементы последовательно. Видите,
тут в цикле кефор мы берем последний элемент. Либо мы пропускаем его через разбер блока.
Результаты. Хотите узнать? Первый способ, только не в секундах, в миллисекундах. Первый способ
18,6 миллисекунды, которая stupid add, а который add тот самый, который мы с вами видели. 5,8, 5,9 миллисекунды.
А? В 4 раза. И чем больше элементов у вас вычисляется на поток, тем больше у вас эта разница будет.
Ага, да. Еще у нас кашмисы везде мы получаем. То есть вы представляете, вы еще в два раза
увеличиваете количество вычислений из-за того, что у вас кашмис. Пропуск такой быстрее и этот
пропуск равный размеру блока. Все, хорошо. Так, я посмотрел, сколько времени у нас есть. Собственно,
как можно сломать кашлению? Рубрика слева у нас call-esque доступ, справа это uncall-esque доступ.
Значит, я специально сегодня почитал Stack Overflow, ну и всякие другие источники, и решил изучить
информацию, типа, насколько больнее будет uncall-esque доступ. На самом деле сейчас в современных
архитектурах уже разница не сильно высокая будет, но учитывайте в то, что если вы попадаете в одну
кашлению, то если вы попадаете в одну кашлению, в принципе порядок, в котором вы считываете
элементы внутри одной кашлении, не сильно проблемный, как сказать. На старых версиях видеокарты это
доставляло бы огромную проблему. То есть в первых версиях CUDA код, который у нас запускается вот
с красным, с, так сказать, бардаком, намного был бы медленнее, чем код слева. В современных
архитектурах не совсем так. Ну, вот исправили это все дело. Ну и, соответственно, чем больше
непосредственность доступов, тем больше время работы, потому что мы сильнее ломаем кашлению нашу.
Так, ну тоже можете попробовать функцию рандома какого-нибудь взять и посчитывать всякие элементы
массива. То сделать random permutation внутри 32 элементов. Так, хорошо. Если здесь тоже хорошо,
давайте поговорим про поток управления. В видеокарте работает концепция предсказательного
вычисления. Я не знаю, слышали ли вы про такое в стандартных процессорах. Собственно,
за счет чего современные процессоры ускоряются. У вас есть IF, какое-нибудь, вы заходите
одновременно, прежде чем вычислить условия внутри IF, из-за конвейерности вы идете в ветку,
которая внутри ZEN и внутри LSA и считаете ее параллельно. После того, как у вас эти ветки
вычислись, возможно вычисляется вперед, это все дело, вы проверяете условия и делаете
джамп по определенной маске. То есть в какую ветку вычисления вам нужно забирать результат.
То есть это называется предсказательное вычисление. То есть вы по факту заходите в тело условия до того,
как вы заходите в обычный IF. Вы проверяете флаг, который там стоит.
А потому что это операция jump. Операция jump тяжелая с точки зрения ассемблера.
Conditional jump. Ну да, jump легкий, conditional jump тяжелый. А тем более на видеокарте.
С учетом того, что у вас половина потоков в одном варпе, то есть у вас есть в ворп,
у вас половина потоков в варпе прыгает в ZEN, а половина потоку прыгает в LSA.
Вот проще не дождаться этих двух коопераций, а сразу пресчитать вперед, а потом уже.
Поэтому вы, если вы увидите там библиотеку Torch, PyTorch знакомый с такой, то увидите то,
что там нету функции типа IF, ZEN и так далее. Там есть функция when. То есть вы говорите,
если у вас условие на какой-то тензор выполняется, вы идете в одну ветку. Если условие на элементы
тензора не выполняется, вы идете в другую ветку. Представляете, другой элемент массива.
Тем самым вы по факту максимально оптимально используете концепцию предсказательных учислений.
Вот это понятно, да? И, значит, вычисляется ветка Syphilis cells,
потом вычисляется значение в условии на варпе и дальше делается уже, собственно, переход.
Так, скажите, в чем проблема в этом козе?
Происходит следующее. Человек решил в третьем потоке посчитать сумму элементов.
Ничего. То есть, смотрите, у нас получается, если номер потока третий,
то мы тысячу раз элементы складываем иначе, мы записываем значение элементов. В итоге у нас
разветвляется ветка и в итоге вместо того, чтобы посчитать этот код на других ветках,
мы по факту замедляем нашу программу 31 раз. Поэтому не надо внутри EFA делать тяжелую операцию.
Делайте так, чтобы у вас количество элементов внутри EFA было максимальным. Внутри одного варпа.
Кстати, это нам поможет в следующем занятии.
Когда мы будем оптимизировать с вами сложение чисел массива.
Да, но они будут строить быстрее.
Так, понятно, что не нужно выполнять операцию. Внутри 3dx равно равно какой-то константе.
Хорошо. Следующая история. Особенности национальной синхронизации, как я говорю.
Значит, счетчики. Нам нужно посчитать какую-то информацию внутри счетчика,
который должен быть расшарен. Ну, статистику какого-то посчитать.
Простенькую. А что в данном случае можно сделать?
Понятно, что плюс равно использовать не вариант. На один int какой-нибудь. Потому что у вас из кучи
потоков сыпется операция плюс равно и в итоге у вас получается боль и страдание, связанная с тем,
что у вас очень много датарейсов и практически половину значений при сложении вы пропустите.
Вам показывали пример того, как выглядит операция плюс равно в ассемблере?
Я не помню. Ассемблер.
Начал? Так. Или не вот это нужно?
А!
Похоже, у меня болт.
Не, lanes are down.
Так, а может какое-нибудь другое компиля?
Ладно.
То есть, видите, плюс равно это две операции.
Ну да, но все равно, вот видите, две операции, в итоге у вас синхронизация может произойти
в разные моменты времени.
То есть, у вас один поток начнет выполнять одну операцию, а второй поток влезет
и сделает перемещение этого элемента.
А?
Да, в итоге половину элементов вы просто пропустите.
Да, в общем, да, да, да, поэтому так не нужно делать, и для этого есть атомарная
операция.
Атомарная операция, которая выполняется за не так время.
Atomic add и есть compare and set, atomic exchange, то есть это синхронизация
локальна.
Теперь, как осуществлять синхронизацию между блоками?
Ну да, либо, смотрите, можем сделать, берем вот такую блокировку, пишем.
А?
А?
Да, собственно, лок, всем известные локи.
Либо, мы можем с вами делать вот такую функцию threadfence.
Значит, что делает runfence, он блокирует независимые операции
между собой.
То есть, ошеляет барьер на независимых операциях.
Вот, при этом shared используется внутри блока, для threadfence, значит, блокировка
как идет?
Для shared памяти, если вы обращаетесь к ней, то она работает внутри
блока.
Если мы, для девайса, для элементов массива, которые на девайсе, это ошеляет
глобальную блокировку.
Вот это ключевое слово threadfence.
То есть, есть подчеркивание, подчеркивание sync threads,
есть подчеркивание, подчеркивание threadfence, а есть подчеркивание,
подчеркивание threadfence system, которое делает полную блокировку
нашего ядра.
Так, ага, тут понятно, да?
Ну, еще посмотрим, сейчас, если время остается, threadfence
system.
Теперь важная информация про регистры.
Регистров, смотрите, есть ограничения некоторые.
Количество регистров на блок равняется 2 в 16.
Немало.
Ну, теперь, смотрите, все зависит от того, какой размер
блока вы поставите.
Если вы поставите размер блока 1024, у вас регистров
будет меньше.
Если вы поставите размер блока 256, то у вас регистров
будет больше.
Да, но есть еще одно ограничение, до 255 регистров на поток.
Максимальное количество поток на shared stream процессоре
1024.
Размер блока 1024, если, то смотрите, если мы используем
1024 регистров на поток, то у нас будет 64 регистров
на поток, если размер блока 1024.
Если размер блока 32, то количество регистров будет
255.
Куда вы от этого не прыгнете.
Поэтому нам всегда необходим компромисс в вычтении этих
ресурсов.
А?
Ну, то есть, выбирайте размер блока оптимальным
образом.
Так, чтобы у нас не произошло memory spilling, то, что у нас
информация, которая была в переменной, вдруг свалилась
в память на stack, а находилась в регистре, чем больше
memory spilling, тем хуже.
Потому что вы идете уже в память, несмотря на то,
что эта память может быть в L1 Cache находиться.
Но здесь уже использование L1 Cache будет не явным.
Так, ну, наверное, давайте подытожим, что у нас сегодня
было.
У нас сегодня была такая лекция, связанная именно
с синхронизацией всего и вся.
Мы с вами поняли, что главным лимитирующим фактором в
видеокартах является оперативная память.
Разобрали иерархию оперативной памяти, поискали информацию
в магазинах, поняли, что у нас с вами все не так просто.
Где это?
Так, не это.
Во!
Вот она наша первая вида памяти с вычислениями.
После этого мы разобрали виды памяти и поняли, что
у нас есть L1 Cache, которым мы можем пользоваться через
ключевое слово shared, и вы с ним будете смотреть
на семинарах.
Дальше мы поняли, что синхронизация начинается тогда, когда
нам нужно взаимодействовать с соседними элементами.
И у нас есть локальная синхронизация при помощи
sync threads, с которой очень легко выстрелить себе в
ноги.
Есть глобальная синхронизация, которую лучше не использовать,
если вы не хотите замедлить свою программу.
Вот.
И в конце немножко поговорили с информацией по регистрам.
Значит, в следующий раз мы с вами будем рассматривать
интересную вещь.
Мы решим с вами две больших задачи.
Задача 1.
Мы научимся считать сумму всех чисел массива.
Это на самом деле не сложная операция на видеокарте,
но в целом надо поработать с shared памятью очень сильно.
Хотя можно и посчитать сумму в Atomic Ad, это я так по секрету
скажу.
Это будет операция не очень медленная.
То есть вы берете просто, пишете, вы заходите в код
кедра и пишете просто Atomic Ad вот в это число.
И не поверьте, на видеокарте это работает быстро.
То есть атомарная блокировка работает очень быстро.
Тут надо учитывать, что внутри варпа можно выполнять
разные операции.
То есть посмотрим некоторые махинации, которые есть.
И вторая операция – это подсчет суммы чисел на префиксе.
Вот.
Префиксную сумму научимся считать, причем научимся
считать параллельно.
Вам, наверное, еще никто не рассказал, как сумму
на префиксе можно параллельно считать.
Вот.
Так, на этом все.
Если есть вопросы по материалу, то задавайте в каком плане
экогерентности кэши.
Есть такой механизм, да, спасибо за вопрос.
Там это на уровне варпа происходит даже.
То есть там получается, что нельзя делать так, чтобы
два потока внутри в одного варпа писали в один и тот
же элемент.
В один и тот же индекс варпа.
Это называется банк конфликтом.
Да, это спойлер к следующей лекции.
Мы как раз будем решать банк конфликта в следующий
раз.
Вот.
Ну, что-то похожее есть.
Вы делаете 0,1,2,3, значит 256,257, ну и там дальше.
Смотрите.
Значит, когда вы складываете ноль у нулевого потока,
взял 0-ой элемент, и 256,п deaf element, п是我ude element это
257.
И получается, что вот кэш линия подгружается 연ordu
и Ocean nom seating, то есть значит simultaneously V 0,1,1.
Что происходит, когда мы spill over . summarize thismumbling
а первый элемент... а первый поток получает второй элемент массива.
Хотя мы ждали,
что этому потоку получится первый элемент массива дать.
То есть мы ожидаем, что этому потоку отгрузится первый элемент массива, а он уйдет и берет второй.
Получается кошмис.
А?
Да, а последствия чтения?
Все, приплыли.
Угу.
Который последовательно вот этот вот, то есть нулевой, что нулевой поток будет заниматься сложением этих элементов массива,
первый будет заниматься сложением этих и так далее.
Не-не-не, они по факту должны считать параллельно, типа, что нулевой параллельно подгрузит нулевой,
а первый подгрузит второй. Но из-за того, что они,
так сказать,
ломают кэш-линию,
да, то в видеокарте существует все-таки механизм, чтобы это работало, а не ломалось, а они переключаются на последний доступ в память.
Мы будем их вставить.
Ну, потому что организация идет параллельно. У нас вот эти потоки стоят в одном варпе, то есть это у нас одна ассемлерная инструкция по факту.
И поскольку это одна особенная инструкция, архитектура видеокарты сделана таким, это на уровне архитектуры видеокарты сделана таким образом, чтобы в одной особенной инструкции подгружался просто блок памяти.
То есть получается, на одну инструкцию мы выдаем один блок.
Да, подними. Да, да, да. А тут приходит первый поток и говорит, мне что-то другое дайте.
Ну да, ну видели, что когда мы сделали 8 потоков, то есть каждый поток начал перезаписывать 8 раз, но мы замедлились в 3 раза.
Ну да, то есть у нас вот она одна кашление сразу складывается, тут вторая кашление сразу подгружается.

Это у нас четвертая, по-моему, да?
Быстро вспоминаем, на чем мы остановились.
Базовое понятие теории вероятности распределения.
Я напомню, что в дискретном случае распределением,
дискретным распределением мы называем соответствием
между значениями и вероятностями этих значений, здесь у нас
соответственно ПК это число больше или равное нулю и сумма
по всем возможным К у нас должна быть одна единица.
Хк это просто какие-то значения.
Соответственно, когда мы говорим о распределении
случайной величины, распределении случайной величины, у нас
возникает понятие, ну, иногда называют фазовым пространством,
то есть множество значений случайной величины кси.
Соответственно, если у нас дискретная теория вероятностей,
а мы пока понимаем, что работать мы можем только
с ней, потому что для других вариантов у нас математического
аппарата не хватает, то соответственно у вас множество значений
случайной величины кси не более чем счетно.
И тогда у нас соответственно Хк по всем К это и есть множество
значений случайной величины кси, то есть это те значения,
которые принимает случайная величина, а Пк это есть вероятность
того, что случайная величина кси принимает значение
хк.
То есть я к чему?
К тому, что понятие распределения не привязано к случайной
величине, оно может быть само по себе.
Ну или в контексте, что же мы берем распределение
к какой-то конкретной случайной величине.
Теперь быстро пройдемся по тем распределениям,
с которыми вы работаете сейчас на семинарах.
Заодно вспомним, какие у них обозначения.
Так, ну погнали.
Соответственно первое распределение Бернулевское.
Бернулевское распределение.
Совершенно беспонтовое, ну потому что у нас здесь
к принимает всего два значения, 0 и 1, то есть к бывает либо
ноликом, либо единичкой.
Соответственно П нулевой это у нас единичка минус
П, П первой это просто П, где П это чисел К от 0 до
1.
Соответственно обозначение для этого распределения
Берн П, потому что понятно, что параметр этого распределения
только 1.
Это вот это П, физический смысл этой чиселки это
вероятность успеха.
То есть у нас один эксперимент, соответственно ему соответствуют
два значения 0 или 1, вероятность единички П, вероятность
0 к единичке минус П.
Ну неинтересно.
Два значения, две вероятности.
Второе было веселее, это биномиальное распределение.
Теперь смотрите, фамилий тут немного, они постоянно
перекрываются, поэтому главное смотрите на существительное,
которое идет.
Биномиальное распределение тут уже повеселее, тут значения
могут быть от 0 до n, хк это у нас тоже к, а Пк это вероятность
того, что в схеме испытаний вернули, у нас будет ровно
к успеху.
То есть опять у вас есть значения и вероятности
этих значений.
Обозначается оно традиционно, причем вот прям вот традиционно
в том плане, что в литературе это частое обозначение,
биномиальное nP.
Потому что понятно, что у этого распределения у
вас два параметра, это n, число испытаний и P это вероятность
успеха.
Следующее распределение геометрическое.
Распределение номера первого успеха в схеме испытаний
вернули, если мы не ограничиваем число испытаний.
Соответственно k здесь у нас 1, 2, может быть любое
далее число натуральное.
Я говорю, тут есть расхождение, даже если в Википедию залезть,
там как бы два варианта для геометрического распределения,
это номер первого успеха или число неудач перед
первым успехом.
Вещи связаны, но тогда если это число неудач перед
первым успехом, у вас тут от нуля будет начинаться.
Просто каждый раз, если там будет что-то, вы уточняете
это, которое геометрическое.
Но поскольку все знают эту путаницу, всегда в условиях
задачи там уточняется.
Ну вот, соответственно, хк у нас снова к, а вероятности
у нас, мы с вами их считали, там было единичка минус
п в степени k минус 1, то есть у вас k минус 1 было неудача
и потом первый успех.
Во всех этих случаях мы проверяли вот эту вот,
то что сумма всех пк, по всему возможному k будет равна
единичке, все хорошо.
И потом неожиданный отряд, мы ввели Пуасоновская,
да здесь, тут геомпе, тоже там литература такой встречается.
Параметра у нас только один.
И четвертое это Пуасоновская, Пуасоновская, Пуас, господи,
как подхреново, извините, низко пишу, Пуасоновская.
Которая, соответственно, к у нас здесь опять целая
от нуля, хк это снова к, а вероятности задаются
таким выражением, лямда в степени k делить на кофактериона
и е в степени минус лямда.
Лямда это какая-то положительная чиселка, которая и будет
в параметрах распределения, обозначаться оно будет вот
так.
Пуасоновская с параметром лямда.
Законный вопрос, который в этот момент возникает,
что это за бред, то есть мы так смотрим.
Ну, здесь действительно все хорошо, это действительно
распределение, у вас есть зависимость между значениями
и вероятностями, вероятности все не отрицательные, ну
лямда больше нуля все хорошо, сумма действительно равна
единице, потому что видно то, что это и есть ретриоризовожение
экспонента, просто нормирующий множеством ле в степени
минус лямда.
Это же видно?
Да.
Хорошо.
Ну а дальше возникает вопрос, откуда вообще эта фигня
взялась.
Соответственно, разобраться в этом вопросе нам помогает
теория М.Пуасон.
Теория М.Пуасон.
Ну погнали.
Пускай у нас случайно лечена ксиентая, распределена
биномиальна с параметрами N и PmT.
Соответственно, когда вы будете задавать в тексте
или условиям будет сказано, что случайная величина
имеет какое-то распределение, это будет знак эквивалентности
и дальше то обозначение распределения, которое я
выписал.
Подозревается то, что у меня N бегает по натуральным
числам, то есть для каждого натурального числа у меня
есть своя случайная величина ксиенты, которая имеет биномиальное
распределение с такими параметрами.
При этом вот этот индекс, что значит, то есть у вас
P как-то зависит от N, при этом известно то, что N умножить
на Pm будет стремиться к лямду, которая больше 0 при N, стремящемся
к бесконечности.
То есть последовательность Pm, это последовательность
чисел, которая удовлетворяет нашим требованиям, то есть
это от 0 до 1, да, вот отсюда, это числа от 0 до 1, при этом
видно, что они стремятся к 0, причем так стремятся
к 0, что вот это произведение стремится к положительной
чиселке лямды.
Ага, что утверждается в этом случае, что вероятность
того, что ксиента будет равна K, ну, любому натуральному
К, ну вот, у нас 0 натуральный, ну ладно, давайте я так
напишу, натуральный источник с 0, может быть 0 входит,
тогда всё равно не важно.
В общем, это будет стремиться при стремящемся к бесконечности
к лямду в степени K, делить на K factorial на E в степени
минус лямда.
Теория очень простая, доказывается быстро.
Давайте просто честно выпишем вот эту вот вероятность.
Ведь это что?
Это наша вот эта вот ПКТ, да, вот отсюда, то есть это
вероятность того, что наша случайная личина принимает
значение K, да, где, вот оно, наша случайная личина
принимает вот это вот наше значение ХК, которое у нас
сейчас просто есть К.
Ну, то есть это просто С из Н по К, ПНТ в степени
К, на единичку минус ПНТ в степени N-K.
Я могу врать, поэтому аккуратно следите, что я тут не обманываю
вас.
Распишем вермиальный коэффициент, что у нас там, N factorial, K factorial
на N-K factorial, ну, N factorial и N-K factorial сократим, подразумевается
что?
K у вас сейчас фиксированная чиселка, N стремится к бесконечности.
Поэтому там K factorial оставим в покое, вот здесь внизу, да,
у нас останется что?
Там получается произведение N на N-1 и так далее, последний
множитель будет N-K плюс 1.
Не водил?
Не водил.
Так, соответственность теперь вот здесь.
Давайте посмотрим.
Причем, что я знаю про ПНТ, я знаю вот эту вот вещь,
то есть ПНТ хороша в связке с N, ну окей, давайте это
и сделаем.
То есть ПНТ умножить на N в степени K, поскольку
я давно добавил N в степени K, я на него поделю.
Так, а здесь мы оставляем единичку минус ПНТ в степени
N-K.
Хорошо, мне не нравится вот это N-K, поэтому давайте
перепишем вот так вот.
Сейчас все нормально, да?
Давайте еще перепишем, чтобы стало совсем хорошо.
У меня здесь в числителе K множителей, в знаменателе
K множителей, ну в эту степень, я сейчас как бы эту степень
раздербаню на множители по одному множителю в каждую
скобку запихну, причем скобка конечное число, у меня K
фиксированная чиселка, она к N никак не привязана,
и у меня получится конечное число множителей, поэтому
арифметика проделывается, тут будет работать.
У меня что получится, единичка, единичка минус 1 НТ, единичка
минус 2 НТ, последнее будет единичка минус K минус 1 НТ.
Не наврал?
Не наврал.
Дальше у меня идет PНТ умножить на N в степени K, единичка
минус PНТ в степени K.
Числитель я пока писать не буду, давайте сейчас посмотрим,
что у нас происходит.
Это чиселка, все нормально, вот это конечное произведение
скобок, каждый из которых сходится к единице, это
видно, то есть это к одному.
Смотрим сюда, здесь у нас конечная степень, ну конечная
в смысле она от N не зависит, не растет, а в основании
степени у нас последовательность, которая сходится к лямбде,
значит вся вот эта дырень сойдется к лямбде в степени
K.
Так, теперь смотрим на знаменатель, на это, что такое PНТ?
Это последовательность, которая стремится к нулю,
это очевидно.
Если это произведение стремится к конечной чиселке, N бесконечно
большая, значит PНТ должна быть бесконечно малой.
Единичка минус бесконечно малая, в конечной степени
все нормально, это у нас сойдет к единичке.
То есть единственный вопрос, который остался, который
сложный для нас, это что будет происходить с вот
этой вот дыренью?
На что это похоже?
Который?
Ну они там первый, второй, ну с Ешкой, это похоже на
замечательный предел с Ешкой.
Давайте подгоним так, чтобы было совсем похоже, что
у нас там было?
У нас там было N знаменатели, правильно?
А тут мы сделаем PНТ умножить на N.
Чтобы стало совсем похоже на замечательный предел,
мы показатель сделаем, какой нам надо.
N разделить на PНТ умножить на N.
Соответственно, вот такие вещи, ну вы нам это не делали,
там типа сложные функции для последовательности.
Вы же знаете, что единица минус х в степени х, у вас
будет стремиться, когда х стремится куда?
Вот эта стремится к нулю, значит, первенутая стремится
к бесконечности.
Ну что, вот эта вот вещь у вас будет стремиться
к Е в минус первый.
Было такое, ну это делали.
А еще вот эту вот хрень, ну чтобы равенство было
верно, мне нужно возрести в какую степень, у меня же
возводилось в степень N, и я добавил вот эту вот
дрянь.
А вот эта дрянь стремится к лямбде.
На мотоне вы должны аккуратно были доказывать, что так
переходить к пределам можно.
Было такое.
Ну было, поэтому.
Ну вот, теперь давайте собирать, что в итоге у нас получилось.
Это к единице неважно, это к единице неважно.
У нас каф факториал в знаменателе, лямбда в степени каф в
числителе.
И вот этот числитель, е в минус первый и еще лямбда,
и того е в степени минус лямбда.
Все.
В чем ценность данного результата?
Пока я стираю, вы пытаетесь придумать ответ на этот
вопрос.
Да.
То есть, у вас есть вот ксиенты, последовательность
случайных величин, которые так распределены, при этом
вот этот вот параметр, который от N зависит, ведет себя
так, что вот это.
Запитая тогда, будет везать вот следующую историю.
В чем ценность, она вообще есть?
Мы объяснили, откуда взялись вот эти непонятные штуки.
И почему мы именно экспоненту начали раскладывать в
ретрибер, а не логарифм, например.
Я думаю, многим логарифм нравится, это же такая субъективная
история.
Кому что нравится.
Вот.
Есть ли еще вариантов?
Наверное, вариантов нет.
Смотрите.
Тервер ведь как бы история очень древняя, то есть,
как бы вопрос о ксерматизации, он был решен недавно.
То есть, как бы Гильберт это сформулировал, проблему
как бы Комагоров ее зафиксировал.
Нельзя сказать, что Комагоров решил, там было много шагов
к этому решению, просто он как бы все оформил окончательно.
Но тервер, наука древняя и вот все теми вещами, которые
мы занимаемся, занимались давно.
И в частности, вопрос с бенемиальным распределением
он достаточно простой.
То есть, предположим, у вас есть какое-то производство
или ловля, я не знаю, что-нибудь.
По следовательности испытаний, предположим, вот станок
у вас работает, форд какой-нибудь древний.
Станок хороший, но все-таки он иногда сбои дает.
Иногда у вас проскакивает бракованный товар.
Будем считать, что брак – это успех у нас.
И в этом случае у вас вероятность, она предположим, какая-нибудь
вот такая.
То есть, поя маленькая, это где-то 1,2.
И дальше возникает вопрос о том, какая вероятность
то, что в пароте, состоящей из тысячи изделий, брака
будет, предположим, меньше или равно трех.
Вроде как простая же история.
То есть, у вас что получается?
Случайно чинок С имеет бенемиальное распределение, где у вас
тысячи было испытаний, а вероятность успеха – 1,2.
И вы просто смотрите вероятность того, что кси меньше или
равно, чем 3.
Вам что для этого нужно сделать?
Вам нужно просуммировать пока от нуля до трех, что
С из тысячи по три на 1,2 в степени 3 на 1,2.
Потом 1,2 в степени 3 на 199,2.
Так, тут к, а тут тысячи минус к.
Так, сейчас проверяем, я вру или не вру.
Ну да, но это невозможно посчитать объективно.
Нам с машинами как бы легко, для машины это достаточно
простая история.
Вы представляете, вот это, если у вас k равно единичке,
то есть, возводить такое число в 999 степени – это
как?
То есть, как получить вот эту вот чиселку?
Дальше приходит на помощь терраэмплассон, который
говорит о чем?
То, что вот эти наши вероятности, они близки к тем штукам.
То есть, вместо того, чтобы считать вот эту сумму, можно
посчитать вот такую сумму.
Сумма по k от 0 до 3, лямбда в степени k делить на k факториально
и е в степени минус лямбда.
Вопрос только, какую лямбду взять.
Ну, n умножить на pn, то есть, n это у нас тысячи, а pn
это у нас 1,2.
Ну, то есть, лямбда у нас получается, сколько, 5?
Потому что посчитать вот эти вот вещи по таблицам
радиса, ну, как бы, е в степени минус 5, 5 я думаю мы как-то
возведем в степень, там сколько, от 0 до 3, факториально
посчитаем, ну, е по таблицам радиса мы найдем.
Ну, по крайней мере, сумму из этих четырех слагаемых
мы посчитаем хорошо.
Понятно, что для как бы исследователей, которые
занимались этими вещами до как бы появления числительных
машин, это было некоторое спасение.
Вместо бенемиального распределения использовать плацоновское
распределение.
То есть, идея плацоновского распределения простая,
это приближение бенемиального распределения, которое позволяет
вот эти вот хрень хоть как-то читать, оценивать.
Какая реакция присутствующих на мой текст, который я
произнес, пока я стираю, вы можете его произнести.
Че, никакой, или вы подождете, пока я к вам повернусь.
Даже не знаю, в какой своей части.
Я верю, что лучший.
Какая вам радость от этого, но это первая история раз,
но предположим то, что у нас нет вычислительных
машин.
Все-таки какие комментарии по поводу текста, который
я произнес до этого.
Кажется, что как бы в рете вы, Иван Генрихович, потому
что, еще раз, это предельный переход.
То есть, да, премьер стремящимся к бесконечности, который
на фига не стремится к бесконечности, у нас n фиксированный.
Большое, да, но что такое большое вот в этих вопросах.
Как бы вы через Маттан проходили, вы понимаете, что как бы
существует n большое, начиная с которого мы попадаем
в обсвал на крестность.
О том, ну существует номер, начиная с которого мы попадаем
в обсвал на крестность.
Но насколько большой этот номер, никто не знает.
Все знают, что он есть.
И поэтому применять вот этот результат для этого
значения n, ну как-то странно.
По меньшей мере.
Это то, что вы должны были сказать.
Иван Генрихович, это за фигня.
Это ясно?
Ну вот.
Конечно же, все интересуют скорость сходимости в теореме
Пуассона и существует уточнение про скорость сходимости
в теореме Пуассона.
Я сейчас формулирую.
Ну, результат самый точный, какой я нашел на данный
момент.
То есть, он доказан Прохоровым.
Ну, Прохоров – это 20 век.
Вот.
Чтобы просто вы оценили степень.
Так, уточнение.
Понятно то, что когда они пользовались в 19 веке вот
этими результатами, у них тоже было уточнение в теореме
Пуассона.
Это просто я машу руками для того, чтобы обосновать,
почему я вот эту вещь вообще ввожу, чтобы вы увидели
хотя бы, что это правда.
Понятно то, что математики в 19 веке доказывали очень
кондовые результаты, просто у них там с мотопаратом
было сложно.
Эти результаты доказывали очень мощные.
И понятно то, что у них были оценки скорости сходимости
вот здесь.
Так, результат Прохорова, который я нашел.
Смотрите.
Пускай у нас случайная величина КСи принимает биномеральное
распределение с параметрами NP.
Принимает биномеральное распределение… Извините.
Случайная величина КСи с биномеральным распределением
с параметрами NP.
Это Пуассоновское распределение.
Это имеет Пуассоновское распределение с параметром
лямбда.
При этом известно, что они связаны вот этим соотношением.
Никто никуда не стремится, просто тупо равно.
Вот эти три чиселки связаны равенством.
Тогда утверждается следующее, что сумма по К от 0 до плюс
бесконечности, вероятность того, что КСи равно К, минус
вероятность того, что это равно К… Понятно, сумма
по всем К.
Да, смотрите.
Понятно, что биномеральное принимает значение от
нуля до n.
Ну как бы подозумевается то, что просто обнуляется
вот эта хрень.
Она будет меньше либо равна, чем 2 лямбда делить на n,
на минимум из 2 лямбда.
То есть давайте посмотрим в нашей истории.
В нашей истории у нас n было 1000, p была 1,200, ну соответственно
лямбда это была пятерка.
То есть вот эта вот штука у нас оказалась какая?
10 делить на 1000 и умножить на 2.
То есть 20 тысячных – это сколько?
1,50.
Я сейчас не вру, нет.
Ну вроде да.
Вроде 1,50.
Ну это хорошо.
То есть это у нас 2%.
Ну как бы для тех времен точность вполне себе хороша.
Ну вот.
Ну вот самый точный результат, который я нашел.
То есть действительно по крайней мере вот эта вещь
показывает то, что я вот там руками махал, это законно
вполне и точность вот она прям вычисляется по этой
форме.
Ну вот это те дискретные распределения, с которыми
вы будете работать на семинарах.
Вопрос, который у нас, давайте про плацоновский небольшой
текст.
Сейчас я прошу прощения, я не отвлечен, наверное
это будет последний мое отвлечение за весь курс.
Что такое е?
Как все знают, начиная наверное кто с девятого, кто с десятого
класса.
Это вот такой предел.
В пределе с Тремяющимся к Бесконечной такой последовательность.
Ну на этом мы отрабатывали термин, вы отрабатывали
термин Верштарас, но доказали, что это последовательность
там монотонно, ограниченно, значимый есть предел, предел
какой-то странный, обозначим его за е.
А что потом на чем происходит с е?
Е у числа?
Прости, но по-моему, чтобы посчитать производную
вот эту функцию можно было сделать и так.
И тоже вполне спокойно посчитать производную.
Ну то есть как бы использование е вот здесь, вот оно было.
Ну просто я хочу обратить ваше внимание на момент,
который должен был быть у вас напряг.
Число, которое появилось откуда-то, ну вообще непонятно.
Мы отрабатывали термин Верштарас и получили какую-то
левую последовательность, предел, который не знаем,
обозначили ok.
А потом оказалось то, что это число вылезает вообще
отовсюду.
Это понятно?
Особенный момент, который должен был вас впечатлить,
когда вы считали первообразную обратную пропорциональность
и вдруг неожиданно оказывалось вот так.
Это что за бред, господи, эта функция проще только
линейная, но казалось бы, да?
Причем тут е, вот в этой истории, которая определялась
вот так.
Вы, наверное, не задумывались над этим, да?
Ну да.
Какой ответ на мой вопрос, который я не задал, наверное?
То есть понятно, что е является пределом этой последовательности,
но так получилось.
Но истинный смысл е другой какой?
Кто может дать нормальное определение числа е?
Производная что?
Это такое основание показательной функции, что ее производная
равна самой себе.
Много ли функции улетворяют вот этому-то требованию?
Две.
И это?
И тождественный ноль.
Ну и линейное преобразование, да, конечно.
Это понятно?
Да?
С коэффициентом масштаба.
Ну вот, то есть е это то основание показательной
функции, что ее производная равна самой себе.
И это истинное, то есть как решение дифференциального
уравнения, там f' равняется f.
Та же самая история, то есть почему, как бы вот, просто
проблема в чем?
Проблема в педагогике.
То есть впервые, ну просто идя по естественным стадиям
своего математического развития, вы столкнулись,
могли столкнуться с е вот в этот момент, когда вы
считали пределу последовательности.
И это очень хороший повод, потому что термин бешенаса
и так далее.
Ну вот.
Но это не значит, что истинная природа числа е заложена
бетом.
То же самое и здесь.
Ну вот.
Да, плацоновское распределение у вас встречается как приближение
биномиального распределения.
Типа мы здесь не можем считать, поэтому будем считать вот
это теорему, которая это обосновывает, но это не
значит, что это истинная природа плацоновского
распределения.
Ид militia?
Дальше вы будете с ним встречаться, все время встречаться,
встречаться, а по кеям эта история у вас будет плацоновский
процесс, на случайных процессах, и вот там вы приблизитесь к пониманию того, чем прекрасно это
распределение. Сейчас хорошо? Такая интрига. Вот, это первая мысль. И вторая мысль, которая
хочется сказать о том, что вы должны проникнуться идеей распределения. Еще раз, когда вы работаете в
теории вероятности, история в чем заключается? Вы не работаете с омега маленьким, как с результатом,
потому что если у вас есть результат эксперимента, то случайность ушла, уже все, уже эксперимент
случился, вы уже подписали. Чего, не надо делать, я буду так акцентировать на этом момент, на это ваше
внимание. А мы же с вами, у вас было собрание со мной на первом курсе? Да, я же говорил об этом, не
подписывать или нет. Я обычно всем этом говорю на собрании 31 августа с первым курсом. Нет? Вы не
помните? Плохо. Ну так вот, поэтому распределение случайных величин, это то, что нас действительно
интересует от случайной величины, потому что не конкретное значение, а то, какие значения, с
какими вероятностями случайная величина принимает. Как задаются распределения в дискретном случае,
это просто, это соответствие между значениями и вероятностями. История простая. А что делать,
если у нас значение становится бесконечное число? То есть, как в этом случае нам задавать
распределение? Соответственно, мы с вами позже построим всю теорию с баррельской сигма алгеброй,
и дальше будет тема о том, как задавать меру на баррельских сигма алгебрах. Это будет решаться
через функции распределения. Кто-то из семинаристов уже сейчас вам даст функции распределения,
вы начнётесь с ними работать сейчас, но это будет основной объект для работы на тервере. Я к этому
тексту вернусь, когда мы будем с мерами работать, когда мы будем строить меры, я вернусь к этому
обсуждению, к функции распределения. Извините, зря я это начал, мне тут написано, но зря. Всё,
второй объект, который у нас есть, это вопрос независимости случайных величин. Мы с вами
определили независимость для событий. Что нам делать, если у нас две случайных величины?
Я напомню, что сейчас всё, что мы делаем, мы делаем только для дискретных вероятностных
пространств, потому что для других у нас просто нет, потому что для других мы пока мотопарад не
построили. Ну погнали, то есть если у нас есть с вами две случайные величины, у них соответственно
есть их фазовое пространство и их значения, то есть хи, кси и хи. Это у нас множество значений
случайных величин и кси. Т.е. мы будем говорить, что кси независима с этой, если любого значения
случайных величин и кси, и любого значения не случайных это, будет верно, что два события,
кси равно х ката и это равно y ката, будут независимы. Все понимают, что вот это вот написано
событие. Т.е. это как бы все омега такие, что кси от омега стало равно х ката. И здесь то же самое,
все омега такие, что этот омега равно y ката. То есть это означает, что вероятность того, что кси
равно х ката и это равняется y ката, будет равно произведению вероятности кси равно х ката на
вероятность, это равняется y ката. В чем самые важные здесь истории? Вот это. Для любых.
Обычная реакция вот на такое определение у людей какая?
А кто у вас? А, хорошо. Ну вообще обычно напрягаются по
поводу того, что вот это для любых. Потому что кажется, то есть смотрите, если у нас у кси,
у это бесконечное множество значений, то есть что получается? Бесконечное число равности должно
быть выполнено. То есть тут возникает сомнение о том, что такое в принципе бывает. Ну в этот момент
у людей. На семинарах мы специально решаем задачку о том, что вам нужно доказать, что существует две
независимые пулассоновские случайные величины. Ну то есть прямо предъявить вероятностное
пространство случайной величины на них пулассоновские, которые оказываются независимыми. Так что на семинарах
эту задачку решить сейчас не будем тратить на это время. Хорошо? Так, давайте одну задачку сделаем, чтобы...
Давайте возьмем две случайные величины. Кси имеет пулассоновское распределение с параметром,
ну давайте лямбда 1, это имеет пулассоновское распределение с параметрами лямбда 2. И они независимы.
Ну и вопрос, что мы можем сказать про сумму этих случайных величин. Когда вы спрашивают,
что вы можете сказать, спрашивают, какое у нее распределение. Еще раз, от случайных величин всегда
интересно, какое у нее распределение. Потому что по распределению можно понять, какие значения
она принимает, с каким вероятностью, какие более вероятные, какие менее вероятные. Ну погнали.
То есть что нам нужно сделать? Во-первых, понятно то, что множество значений случайной величины кси
это от нуля и дальше, это от нуля и дальше. Поэтому кси плюс это она будет принимать вот те самые
значения, не отрицательные и целые. Окей, то есть вот это вот будет ее фазовое множество
состояний. Погнали. Получается, что для любого к мы сейчас будем считать вероятность того,
что кси плюс это равно к. То есть как бы значение мы понимаем, какие она будет принимать. Сейчас нам
для того, чтобы задать распределение, нужно найти вероятности, с которыми она принимает эти значения.
Соответственно, как мы это будем делать? Смотрим, у нас тут две случайные величины. Одновременно мы
про них ничего не можем сказать, а отдельно про каждую можем сказать. Это прямая история
про формулу полной вероятности. Помните такую? То есть мы суммируем по всевозможным значениям
какой-нибудь одной случайной величины, но предположим, пускай это будет кси. И по формуле полной
вероятности вот здесь у нас будет кси плюс это равно к при условии того, что кси равно и. То есть
произведение условной вероятности на вероятность условия. Это в точности форму полной вероятности.
Теперь смотрим здесь. Поскольку мы рассматриваем вот это вот событие только при тех Омега,
когда выполнено вот это, я получается вот здесь, могу заменить на и. Кси же у меня равно и.
Получается, я могу там написать и. Кстати, я получу следующую вероятность того, что это равняется k
минус и при условии того, что кси равно и, умножить на вероятность того, что кси равно и. Теперь
смотрим вот здесь. Вероятность вот этого события при условии вот этого события. Что про эти два
события можно сказать? Это равна чиселке и кси равна чиселке. Они независимы. Это из-за
определения независимости случайных величин. Случайная величина независима, если для всех
значений, для любой пары значений этих двух случайных величин, вот такие два события будут
независимы. Получается вот эти два события у вас независимы. А если они независимы, то что
можно сказать по условному вероятности? Она равна безусловной. То есть вот это можно выкинуть.
Еще заметим такую штуку. Вот видите, здесь написано это равняется k минус и. А и пробегает куча
значений. То есть понятно, что как только вот эта чиселка у вас становится отрицательной,
эта вероятность обнуляется. Поэтому вот тут вот можно закончить на каком значении? Накатом,
да? Потому что после этого у нас уже тут отрицательные числа и вероятность обнуляется. Что у нас
получается? Сумма по и от 0 до k. Здесь у нас вероятность того, что это равняется k минус и.
Это умеет пуласоновское распределение. То есть там тоже пуласоновское принимает конкретное
значение. Это что такое? Лямбда 2 в степени k минус и делить на k минус и факториал. Ну на
e в степени минус лямбда 2. Тут же все понятно, да? Здесь проще писать, потому что у нас получается
лямбда 1 в степени и и факториал на e в степени минус лямбда 1. Соответственно мы выносим за
скобку все, что у нас не под индексом и. То есть e в степени минус лямбда 1 плюс лямбда 2.
Что у нас тут еще есть? Здесь прямо не хватает нам для бенемиального коэффициента. Смотрите,
вот тут вот и факториал, тут k минус и факториал. Не хватает еще k факториал в
числителе, чтобы получился бенемиальный коэффициент. Давайте я его сделаю. То есть я до него
домножу, значит я до него разделю. Получится вот так вот. И у меня получится тогда вот здесь что?
c из k по i. То есть если я на k, то я его домножу. Лямбда 1 в степени и на лямбда 2 в степени k
Что нам эта хрень напоминает? Бином она напоминает. То есть вот эта хрень у нас получается лямбда 1
плюс лямбда 2 в степени k. То есть вероятность того, что случайная личина равна k, равно вот этому.
То есть получается кси плюс это. Какое имеет распределение? Плассоновское,
но с другим параметром. У нас получилось лямбда 1 плюс лямбда 2. Так, это все видят. Если есть
вопрос, можно сейчас задать. На самом деле непонятный и нетреальный результат. То есть если мы
складываем случайную величину, которая независима, потому что если мы кси сложим, то есть если у вас
была плассоновская случайная начинава, если вы ее сложите самой собой, вы получите два
умножить на kси. Этоrier2 Умножить на кси.
Если кси имеет плассоновское распределение, нет не так. Я не спрашиваю, почему не работает это решение,
Я спрашиваю, почему точно не плацомовское распедление?
Может быть, как-то по-другому там получать?
Да.
Два.
Да, ну то есть, если я вместо этого беру ту же самую кси,
получается кси плюс кси.
Это просто 2 умножить на кси.
У этой случайночной плацомовское распедление или нет?
Нет.
Нет.
Почему?
Это зависит от событий, но это кси и кси.
То есть, не работает это решение, это хорошо.
Может быть, как-то по-другому там получается плацомовское
распедление.
Это точно, то есть, 2 умножить на кси.
Если кси имеет плацомовское распедление, имеет распедление
точно не плацомовское распедление.
Объяснение очень простое.
Какое?
Я вам стукну сейчас.
Кого-нибудь.
До кого дотянусь.
Ура!
Она принимает только четные значения.
Еще раз, плацомовская случайная величина принимает
все целые неотрицательные значения с такими вероятностями,
которые там выписаны были.
Лянда, все пеникаде, линкофакталерога и так далее.
То есть, она каждое неотрицательное целое значение принимает
с положительной вероятностью.
Это ясно?
Кивайте.
2 умножить на кси принимает только четное значение.
Она равна единице с нулевой вероятностью, у нее не может
быть плацомовского распедления.
Да?
Да.
Ладно.
Поэтому, мысль в чем?
Если вы складываете какие-то зависимые плацомовские случайные
величины, вы получаете что-то.
Например, 2 умножить на кси – это какая-то новая
случайная величина, которая понимает только что-то
неотрицательное целое значение.
А вот когда мы складываем независимые, почему-то распределение
сохраняется, причем достаточно мило, то есть параметры
складываются.
Эта фигня будет выполнена очень часто.
Если вы складываете две биномиальные, у которых
одинаковая п, разные н, но одинаковая п, там тоже
будет биномиальная, там будет n1 плюс n2 и p.
Потом, когда вы откроете для себя непрерывные случайные
величины, там будет та же самая история, то есть нормальная
случайная величина, которую скорее всего вы слышали,
вот эти колокола.
Если вы складываете независимые нормальные случайные величины,
вы снова получаете нормальную случайную величину, только
там параметры будут другие.
Вот если они независимые, там все что угодно может
получиться, а если независимые, то получается то же самое
распределение.
Так что мой текст о том, что это понятие центральной
теории вероятности, я надеюсь, я смог вас сейчас убедить,
что и для случайных величин это тоже важно.
Хорошо?
Окей.
Следующее понятие, второе по степени центральность,
это математическое ожидание.
Случайная величина.
Соответственно обозначение, было принято два обозначения
для математического ожидания, мат ожидания кси или вот
мат ожидания кси.
Соответственно и это из expectation, ну как ожидания,
да?
Ну вот м это, ну это просто, это мин, ну как средний.
Ну вот, в советской, в зарубежной литературе практически
вся использовала вот это обозначение советское.
Вообще будем честным, в двадцатом веке как бы теория
вероятности делался советской наукой, ну то есть вот такие
самые яркие результаты были получены именно советскими
учёными, там Комагоров, Прохоров.
Ну вот.
А и тут было два варианта, либо expectation, либо мин.
Ну вот.
Соответственно после развала Союза было две группировки,
которые между собой соперничали, соответственно это кафе
теории вероятности на михмате МГУ, ну вот.
И кафе математической статистики случайных процессов на
михмате МГУ.
Соответственно я был с кафе теории вероятности, а Рыгородский
был с кафе математической статистики.
Рыгородский просто упорствовал долгие годы и писал вот так,
соответственно мы все писали вот так.
Ну вот.
В общем, он сломался, он уже года три пишет как все.
Ну вот.
То есть вот это обозначение уходит в прошлое, ну вот.
Постепенно как-то вымывается, потому что редакторы, когда
отправляете статью на рецензию, они поправляют то, что давайте
все-таки как все писать будем.
Поэтому мы с вами будем писать только вот так, хотя встречается
и эммочка.
Но говорят Рыгородский уже года три.
Ну вот.
Раньше это было смешно, потому что он писал М, и мой вот
этот текст прям заходил, потому что у вас дискретное
анализ сейчас идет, и там встречается это.
Ну вот.
В чем идея математического ожидания?
В суть, это просто понятие среднего от ваших наблюдений.
То есть давайте рассмотрим историю, то, что у вас есть
случайный эксперимент, и вы проводите серию из
Н большой экспериментов.
Ну то есть Н большое – это прям много.
Эксперимент.
Я напомню, как бы, терверус продиктован нам жизнью, поэтому
все эти понятия, которые мы тут вводим, они все-таки
из жизни приходят.
И те определения, которые проявляются, они продиктованы
им.
Ну вот.
И у вас есть какая-то числовая характеристика.
У вас есть какая-то числовая характеристика.
Я сейчас не говорю про случайную величину, потому что случайная
величина – это промат-модель, а сейчас мы как бы опустились
назад на случайный эксперимент.
Ну то есть вы снимаете вот при этих Н экспериментах
эту числовую характеристику, и у вас получается наблюдение
кси1 и так далее кси Н большое.
Ну это те числа, которые вы сняли.
И среднее ваших наблюдений, очевидно, это просто среднее
арифметическое.
Ну то есть вы провели много-много измерений, а потом просто
берете среднее асимметическое от этой истории.
Теперь давайте соображать.
Вот любая из этих ктишечек, она и является, я значение
тут подбирал, кси1 это есть как бы числовая характеристика
результата эксперимента, то есть Омега с индексом
И – это результат нашего случайного эксперимента,
который получился при Итым испытании, то есть у нас
тут И меняется от единички до Н большого.
При этом мы что понимаем?
Мы понимаем то, что у нас каждому результату эксперимента
поставлен соответствующий элементарный исход, число
которых, ну пускай у нас будет Н маленькое.
В общем, идея в чем?
То, что у нас обычно Н маленькое у нас меньше, чем Н большое.
Обозначение понятное, верхний индекс – это результат
случайного эксперимента, а нижний индекс – это просто
номер моего случайного, номер элементарного исхода
просто в моем Омега Большом.
В связи с этим, что мы с вами получаем?
Мы получаем то, что среди вот этих Омега с верхними
индексами часто встречаются вот эти вот наши Омега.
И тогда мы можем вести такие вещи как А и Т, это что будет
такое?
Это будет число тех Омегаитов, что, извините, это будет
число тех Омега-житых, что Омега-житая равняется
Омегаитовым.
Ну, то есть это сколько раз вот в наших вот этих
вот экспериментах выпало конкретный элементарный
исход?
Все понятно?
Все, да?
Ну, например, сумма всех аитов будет равна чему
получается?
А?
Н большое.
Н большое, конечно.
При этом и у нас меняется от единички до н маленького.
Теперь давайте пойдем вот сюда, вернемся.
Вот здесь вот хочется собрать вместе те как бы ксишки,
которые соответствуют одной Омеге.
Число, мы их знаем, сколько их будет аит, и получается,
что у нас вот здесь вот, в числителе будет что происходить?
Сумма по и от единички до н маленького, аит умножить
на кси от Омегаит с нижним индексом.
Ну, просто сколько раз Омегаиты у нас вот тут вот попалось,
вот мы их все вместе и собрали, и количество их будет вот
такое.
И все это мы делим на Н большое.
Причем я могу переписать это еще вот так вот.
Причем я говорю, сейчас мы как бы говорим о том,
просто человек сидел, провел очень много испытаний,
снял показания и работает с третьим объектом.
Мы просто этот объект немножко преобразовали.
То есть сейчас мы говорим про реальную жизнь.
Теперь давайте посмотрим, что вот это такое.
Что такое аиты?
Это количество выпавших у нас Омегой.
Сколько раз у нас выпало Омега маленькая, и ты деленный
на Н большое.
Какая идеализация вот этой дроби?
Это что такое?
Еще раз.
Вероятность чего?
Омегаит.
Идейно.
Равно я, наверное, не хорошо пишу.
Давайте я стрелочку напишу.
То есть когда мы строили с вами математическую модель
случайного эксперимента, мы говорили, что вероятность
это что?
Это идеализация частоты, которой у нас там есть статистическая
устойчивость частот.
А сейчас получается, что вот этот новый объект, который
мы хотим вести, если мы его будем идеализировать,
его нужно вот так.
То есть приходим к определению, к математическому ожиданию
случайной величины х называется, я напоминаю, что у нас сейчас
все в дискретном случае, то есть мы работаем только
с дискретной случайной величиной, называется сумма
по всем элементарным исходам, вероятность этих элементарных
исходов на значение случайной величины в этом элементарном
исходе.
Почему мат ожидания определяется именно так, я надеюсь, я обосновал.
Это определение в математике, но оно продиктовано такими
простейшими соображениями.
Хорошо?
Теперь пошли свойства математического ожидания.
Так, первое, если случайная величина кси имеет распределение,
имеет вот такое распределение, хк, пк, пк, то математическое
ожидание случайной величины кси можно посчитать как
сумма по значению случайной величины значений, домноженных
на вероятности этих значений.
Ну, доказательство очевидно, доказательство этой истории
очевидно, потому что мат ожидания это есть сумма
по всем омегам маленьким, п от омега, на кси от омега,
при этом мы понимаем, что вот этот множитель у нас
часто повторяется, то есть значения случайной величины
от разных случайных экспериментов часто одинаковые.
И мысль в чем?
То, что мы можем вот эту сумму перегруппировать.
Перегруппировать так, то есть в рамках одной группы
мы соберем такие, что у нас будут одинаковые вот
эти множители.
То есть в результате мы будем суммировать по значениям
случайной величины, внутри у нас будет по таким омега,
что кси от омега равняется хк, здесь у нас будет п от
омега умножить на хк, где мы бегаем по всем значениям
случайной величины кси.
Здесь, соответственно, хк от омега, независимо
от того, можно вынести за знак суммы, получается
просто суммированная вероятность элементарных исходов, на
которых кси от омега принимает значение хк.
Ну так это и есть наша вероятность, вероятность того, что кси
равняется хк, а это в наших обозначениях и есть пк.
Идея ясна?
Какой важный вывод мы делаем вот из этой истории?
Какой важный вывод мы делаем из этой истории?
Там такая боль на лице.
Чего вы от нас хотите?
Читайте давайте, мы пишем.
Чего еще думать что-ли надо?
У нас тут был созвон, ой нет, там нельзя, это внутренняя
информация 1С, ладно, не буду говорить, что я тут
вспомню.
Зачем подзапись?
Ну так чего, кто-нибудь родит какой-нибудь вывод?
Это мы молодцы, да, тяжелая была, употели.
Смотрите, как бы, идея, в чем мы делаем, в те веры,
вообще в любой науке, когда вы занимаетесь, то есть
у вас есть дисциплина, в чем смысл, то есть вам
водятся объекты, а дальше вы изучаете свойства этих
объектов.
Ну согласитесь, то есть там вам вели линейное пространство
и дальше вы это линейное пространство сбоку в профиль
там в разрезе изучаете, это понятно, любая дисциплина
так строится.
Теперь вера, то есть как бы у нас ну типа там случайный
эксперимент, но на самом деле мы всегда работаем
с математической моделью, у нас математическая дисциплина
есть, вот у нас есть мат-модель, дальше мы там вводим объекты
и с ними работаем, изучаем их свойства.
Соответственно, у нас что получается в те веры, мы
ввели новый объект с случайной величины, дальше рассматриваем
различные характеристики этого нового объекта, что
у нас там есть, у нас есть распределение случайно
величины, это какая-то ее характеристика, теперь
новая возникла история с математическим ожиданием,
но указана характеристика случайной величины.
Это понятно?
А вот эта теорема говорит о чем?
То, что на самом деле математическое ожидание является характеристикой
не случайной величины, а чего?
Ее распределение, то есть мы сейчас показали, что
важно какое распределение случайной величины.
И эта характеристика именно распределением самой
случайной величины.
Еще раз, случайная величина как функция от случайного
эксперимент а от элементарного исхода она нам не очень интересно нам интересно
распределение и вот мы сейчас выяснили то тот объект который мы водим вводим
является характеристикой именно распределение это ясно хорошо два
замечания первое замечание вот тут вот добавим еще один результат который я
доказывать не буду что если я хочу посчитать если я хочу посчитать
математическое ожидание функции от случайной величины
ну какой тут функция от случайной величины это будет новый случайно
величина так отяснайте вы меня пугайте еще раз ну как бы вот это вот у вас phi
как это числовая функция числовая функция отчесу функции это binsten
функция это понятно поэтому это новый объект это какая-то новая случайная
Ну, например, мат ожидания кси в квадрате, соответственно, как считать вот эту вещь?
Ну, либо по определению, бегая по всем омегам маленьким, по всему вероятностному пространству,
и суммирую p от омега на phi кси от омега, либо, ну непонятно как, здесь мы уже этим пользоваться не можем.
А, либо, да, извините, либо искать распределение вот этой новой случайной величины.
Ну, например, кси у нас была полосоновская, значит 2 умножить на кси надо писать,
то есть она принимает вот такие четные значения вот с такими вот вероятностями.
Прописывать это распределение, потом выписывать вот этот вот ряд.
Оказывается, что можно так не делать. Можно для распределения случайной величины кси,
но просто суммировать вот такие вещи. То есть мы по-прежнему отталкиваемся от распределения случайной величины кси,
но суммируем как бы не ее значение, а phi от ее значения. Можно я не буду это доказывать, это делается точно так же.
Да, просто вот здесь вот мы собираем как бы не по значениям phi, а от кси, а по значениям кси все еще.
Мы же суммировать можем как хотим, ну сумму нашу большую разбивать.
Так, это первое замечание, которое есть. Второе замечание, вот тут вот я наврал.
Кто может придумать, в чем я здесь наврал? В определении математического вождания.
Вот докопайтесь до этого определения. Я напомню, что живем мы все еще в рамках дискретного вероятностного пространства,
то есть у нас омега не более чем счетно, а если счетно, то там мера еще есть, как бы счетные суммы тоже можно составлять.
Кто сказал что? Да, да. То есть беда состоит в чем? То, что вот эта сумма, видите как я пишу, по всем омегам маленьким.
Если у вас омега большое счетное множество, то как я суммирую эту сумму?
В этот момент у нас еще всплывает в мозгу факт из математического анализа, я надеюсь он у вас был.
Вы понимаете к чему я апеллирую? Что условно сходящийся ряд можно так перенумеровать, что он сойдется к чему угодно.
Кивайте, чтобы помнить этот результат.
Очень плохо киваете вообще. Нет, в смысле то, что есть люди, которые этого не знают.
Саша, это было?
Нет, возможно, что такого не было.
Но факт в том, что если ряд условно сходящийся, его можно перенумеровать так, он сойдется куда угодно.
А тут вообще нумерации нет.
Поэтому вот здесь, если ряд сходится абсолютно.
Теперь смотрите.
Когда конец нашего курса будет состоять в том, что мы будем строить интеграбль либега. Зачем нам это нужно?
Зачем нам это нужно?
Если у вас ω перестает быть счетным множеством,
то понятно, что сумма должна стать интегралом, и это
не интеграл Риммана, потому что интеграл Риммана строится
на rn.
У нас сейчас пространство, и непонятно, какой природы.
И там мы будем строить интеграл Риммана.
И главное отличие интеграла Риммана от интеграла Риммана
заключается в том, что для интеграла Риммана в принципе
и нет понятия условной сходимости.
И сейчас должно быть понятно, почему.
Потому что у нас идет суммирование по всему, неважно какая
последовательность, а получается условную сходимость в принципе
не может быть.
Ладно, это и есть.
Замечу то, что практически всегда, когда вы будете
считать какое-то математическое ожидание, вы будете пользоваться
именно этой формулой.
Внимание, это формула для подсчета мат ожидания.
Поскольку вы будете пользоваться только ей, то к концу курса
у вас будет ощущение, что это и есть определение.
Это не так.
Определение оно вот и продиктовано оно вот этими соображениями.
А в общем случае, для произвольного пространства, это мы к концу
курса построим.
Это будет интеграл Олибек.
Теперь пошли по таким арифметическим свойствам математического
ожидания.
Так, второе.
Мат ожидания – это есть линейный оператор на множестве
случайных величин.
Тут оговорка на множестве случайных величин, для
которых в принципе мат ожидания существует.
То есть, что это значит?
Это значит, что для любых случайных величин кси,
и любых чисел а и b, тут уточнение таких, что существует
мат ожидания кси и мат ожидания это.
Будет верно, что мат ожидания а кси плюс b это, есть а
мат ожидания кси плюс b мат ожидания это.
Показательство очевидно.
То есть, посмотрели вот туда на определение.
Вы понимаете, если вы тот ряд, который в правом верхнем
углу выпишете вот для этой хрени, он у вас абсолютно
сходится, он спокойно распадается на два ряда, чиселки вы
выносите за знак суммы и получается мат ожидания
этого и мат ожидания этого.
Все.
Третье.
Различные варианты для неравенства.
Соответственно, если у вас кси больше либо равно
нуля, то мат ожидания кси тоже будет больше либо равно
нуля.
Если кси у вас больше либо равно, чем это, то мат ожидания
кси будет больше либо равно, чем мат ожидания это.
Так, но это очевидно, опять же, исходя из правого верхнего
угла.
Это видно?
Ну потому что там все слогами у вас не отрицательны, значит
сумма не отрицательна.
Ну вот, здесь что мы с вами получаем, ну отсюда, это перенесли
влево и свели к этому, видно?
Ну и используем еще линейность, чтобы там мат ожидания на
конце.
Так, и следующая вещь, которая нам нужна, то что модуль
мат ожидания кси будет меньше либо равно, мат ожидания
модуль кси.
Ну в предположении, что все математические ожидания
существуют из тех, которые выписаны, но это следует
отсюда.
Угу, хорошо.
Так, четвертое.
Мы до двадцати минут, да?
Ну давайте, самое важное я скажу, хорошо.
Если две случайные величины независимы, то мат ожиданий
и их произведение равняются произведению мат ожидания.
И вот тут всякий раз, то есть когда мы с вами сталкиваемся
с понятием независимости, у нас стандартно все ломается.
Вот здесь снова все сломалось, потому что у нас что?
Ну кси это просто какой-то оператор на множестве случайных
величин, линейный, который себя хорошо ведет.
Окей, да.
Получается что, что если мы умножаем случайные величины,
и мы сделаем вещь такую нестандартную для линейного
оператора, он себя начинает вести очень странно.
Почему-то мы получаем произведение математических ожиданий
этих случайных величин.
Доказательство этой истории очень простое.
Ну вы просто честно расписываете вот то, что у вас есть.
То есть когда мы расписываем мат ожидания кси умножить
то есть опять у нас что?
У нас есть множество значений случайночной кси, множество значений
случайночной это.
Это у нас просто сумма по всем омега, кси от омега,
это от омега, п от омега.
Ну соответственно пускай у нас вот это есть хк, а множество
значений случайночной это, это будет так.
Давайте здесь мы сделаем и, а здесь мы сделаем ж.
Вот так.
То есть у нас что получается?
Это сумма по и, сумма по ж, сумма по омега таким, что
кси от омега это есть хит, это от омега это есть ужит,
п от омега хит ужит.
Я делаю то же самое, что и раньше, я просто перегруппировал
мою сумму.
Дальше я вижу, что вот эти два множителя они не зависят
от индекса суммирования.
Я его могу вынести.
Соответственно получается я суммирую вероятность
элементарных исходов, для которых выполнены вот
эти два требования, ну и получаю результат.
У меня будет хит на у ж, да, на вероятность, ну того,
что выполнено это и это.
То есть кси равняется хит, это равняется у жит.
Ну то есть это вероятность всех таких омег маленьких,
что выполнено и это и это.
Ну то, что у меня здесь было записано.
Понятно же, да?
Теперь давайте посмотрим.
То есть фактически под вероятностью, что написано.
Это пересечение двух событий.
Через точку запятой это логическое и, но логическое
и в теоретическом множественном виде это пересечение двух
событий.
А мне было сказано то, что случайные величины у меня
были независимы.
То есть для любых и и ж, вот эти два события независимы.
То есть их можно расписать как произведение вероятностей.
Кси равняется хит, это равняется у ж.
Дальше я что делаю?
Я вот этот объединяю с этим множителем, ну а у ж
объединяю со своим.
Дальше.
Вот эта хрень от и не зависит, поэтому я ее могу вынести
вот сюда.
От ж, извините, от ж.
Вот это от ж не зависит, я могу вынести за вот этот
знак суммы.
У меня получится хит на вероятность того, что кси
равняется хит умножить на сумму по ж.
То есть у ж на вероятность того, что это равняется
у ж.
В результате вот эта вещь у меня получается, мата
ожидания это, она выносится вот за эту сумму, а вот эта
вещь у меня получается мата ожидания кси.
Я получил то, что я и хотел доказать.
То есть получается мата ожидания, которое изначально
было каким-то простым таким, простой вещью объективно.
То есть это просто линейный оператор на множестве
наших функций.
Все.
Ну то есть как бы в линале это часто встречалось, в
функане будет сейчас встречаться.
Когда возникает понятие независимости, он очень
странно будет себя вести.
А дальше будет вообще класс.
Потому что после этого мы ведем понятие дисперсии,
которая будет играть роль такой квадратичной формы.
То есть это линейная форма, там будет квадратичная
форма.
В случае независимых случайных величин она себя начинает
вести как линейную форму.
То есть дисперсия суммы независимых будет суммой
дисперсии.
Хотя она с точки зрения функана квадратичная форма.
Ну линавая функана.
В общем, будет очень прикольно.
Все.
А мы же закончили в 20 минут?
А, в 30?
Черт.
Слушайте, а я почему тут, я не знаю.
Так, все, значит продолжаем.
Нет, давайте я еще одно свойство тогда мата ожидания
сформулирую, мы тогда разойдемся.
Слушайте, почему вы молчите все время?
Иван Георгиевич, вы в приоте.
Иван Георгиевич, вы медленный.
Иван Георгиевич, вы перепутали опять все.
Нужна обратная связь от аудитории, иначе невозможно.
Давайте.
И последнее, пятое свойство.
Мне нравится Лукаши Буниковского, известный всем присутствующим.
Соответственно, как оно будет выглядеть для
Тервера, то есть мата ожидания модуля ксиет в квадрате
будет меньше, чем вот это.
Ну, как обычно в предположении, то, что все эти вещи существуют.
Для решения вот этой истории мы что делаем?
Мы вводим две новые случайные величины.
Да, ну, как бы первый случай.
То, что пускай у нас
ни одна из этих штук не равна нулю.
Что тогда?
Мы введем новые случайные величины.
Кси, чертой которой есть, кси, деленное на
корень из мата ожидания, кси в квадрате.
Да.
И мы вводим новые случайные величины.
И мы вводим новые случайные величины.
Мата ожидания кси в квадрате.
Да.
И это чертой, который есть, это делить на корень из
мата ожидания, это в квадрате.
Ну, такая нормировка.
Потому что вот это у нас что?
Это случайная величина, а это просто чиселка.
Я разделил случайную величину на чиселку.
Ничего интересного.
Все хорошо.
Теперь стало то, что я могу сказать.
Да.
Теперь смотрите.
Какое неравенство верно для вот этих вот двух случайных
величин?
Неравенство каши.
Мы его все знаем с какого-нибудь седьмого класса, наверное.
То есть вот такой вот модуль, ну два вот таких вот модуля
у нас меньше либо равные, чем сумму таких квадратов.
Ну, то есть эти что?
Да.
Ну, просто.
Теперь, поскольку как бы вот в это мы верим, то есть
можно навешивать математическое ожидание на наше неравенство,
я его навешу.
Двойку у меня вынесу.
У меня получится два мата ожидания модуля кси с чертой
и это с чертой.
Окей.
Так, теперь давайте смотреть.
Чему это равно мата ожидания квадрата вот этой дыряни?
То есть если я вот это возведу в квадрат, ну давайте.
Мата ожидания кси с чертой в квадрате.
Это у нас что?
Мата ожидания кси в квадрате делить на мата ожидания
кси в квадрате.
О, Господи, что это за хрень?
Давайте смотреть.
Вот это кто?
А?
Ну, мат-объект.
Кто это?
Это случайная величина, ну возведенная в квадрат,
ну это новая случайная величина.
Это кто?
Ну мат-объект какой?
Это чиселка, это вот то А из второго свойства, это чиселка.
А мы выясняли, мат ожидания obligations к olsunQU seiner
амперат�а, потому чиселка просто выносится из-под
знака мат ожидания.
То есть у нас получается единицы делить на мата ожидания
к си в квадрате, умножить на мата ожидания к си в квадрате.
Ну то есть зачем я делил вот на эту хрень? Я добился того, что вот мат ожидания квадрата
этой случайночной было 1. Ну и тут тоже будет 1. То есть у меня получается 1 плюс 1. Ну то есть у меня
получается, что в результате мат ожидания вот моего модуля меньше либо равно единичке. Я 2 скоротил.
Хорошо. А давайте теперь вот эту дырянью распишем. Это что такое? Это у меня кси делить на корень из
мат ожидания кси в квадрате. Это делить на мат ожидания это в квадрате из этого корень. Тут
модуля и это меньше либо равно единицы. Можно я напишу 4d? Ну это чиселки, это мат ожидания.
Они выносятся по знакам от ожидания. Потом у меня просто числовое неравенство, которое
я домножаю на эти чиселки и получаю вот это. Все, 4d. Тут на самом деле очень важная история, что если
вдруг достигается равенства, помните там, вы же, у вас где было неравенство к ошибке? Это матан. И
когда вы с евклидовыми пространствами работали, это как неравенство для скалярного
произведения? Может какая боль написана на ваших лицах? Так, подождите, у вас евклидового
пространства был термин такой? Где? На Угеме. Хорошо. И там вы доказывали общий вид неравенства к
ошибке Николаевского для скалярного произведения. Было такое? Да. Хорошо. Ладно. И там было как бы,
что было сказано? Что если у вас неравенство к ошибке Николаевского оказывается равенство,
то тогда что? Там было что-то хорошее. Равенство между кем и кем? Ладно, вернемся к этому обсудку. Я
в общем понял. Надо начать неравенство к ошибке Николаевского для евклидовых пространств в следующий
раз. Напомним это. Все, спасибо, что подошли.

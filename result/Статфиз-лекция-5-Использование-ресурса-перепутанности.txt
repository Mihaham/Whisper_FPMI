Теперь С, что там, АС. Это мне нужно взять след по B. Тета, это квантовый канал, сохраняет след.
Значит, след от вот этой части, это все равно что след по КСИ. КСИ матрицы плотность, след единичка.
Значит, просто выпадает от отмножитель, получается вот такая вот сумма. Так, теперь СА. Это вам нужно
взять просто след по этой части, по этой части, там единички, получается вот такое рожание. И энтропия
всего в целом. Теперь зачем мы эту формулу использовали? Мы вот эту вот формулу, видите,
использовали, чтобы S, A, B, C ограничить сверху. Сверху ограничили S, A, B, C. Вот у вас формула для
пропускной способности. В случае оптимального ансамбля, видите, сюда входят вот эти вот объекты.
Энтропия вот этого F, тензорно, тета. Вот оно у нас записано. То есть, если я сверху ограничиваю вот
это A, B, C, то это означает, что я здесь могу поставить меньше либо равно, потому что здесь знак минус.
Просто сумму по K, P, K, T надо будет написать ко всем членам. Так, уже, уже, уже. Значит, так,
как вам это выделить-то? F, T, T, R, K, T написано внизу? А, оно у нас, сейчас, сейчас могу неправильно вам
сказать. Да, поэтому, да, правильно. Владимир возмущается. Снизу ограничу, правильно. Снизу
ограничу вот это выражение. Вот что я сделал. Вот это вот ограничение у меня будет снизу. И тогда
это будет ограничение сверху на вот это выражение с минус. Вот теперь я тоже сам понял. А первое
выражение мы с вами как ограничили сверху. Энтропия составной системы не превосходит суммы
энтропии под системы. Вот с первым выражением мы так обошлись с вами. Так что давайте теперь
это запишем. Вот чем, как будем продолжать? Будем писать меньше либо равно. Мы здесь продолжим.
И продолжим здесь на центральной доске писать, что у нас там получается.
Значит, c1 бесконечность phi, это меньше либо равно. Чего? Здесь мы как с вами сделали? Энтропия
s phi действует на подсистему 1. Первая подсистема это у нас что такое? Вот это вот с буквами
p. Значит, у вас еще такое начало было в прошлый раз. Сумма по k по l, p, k, q, k. А, энтропия была.
Вот так. Энтропия. Сумма по k по l, p, k, q, k, l. Чего там у нас? z k l плюс s. Второй подсистемы.
Сумма по k по l, p, k, θ. А q еще. q k l, θ. x k l. И еще что? Минус. Продолжаем.
Минус сумма по k, p, k, t. И теперь вместо этой энтропии верхнюю оценку на нее напишем,
которая здесь возникает. Сейчас я тогда перепишу в квадратных скобочках. Так, это будет s
сумма по l, q k l. Чего там? z k l. Прибавить энтропии всего в целом. Сейчас тоже напишем.
Ну ладно. Так, тут мы сейчас может запутаемся. Ну ладно, я напишу a, b, c. Буду пробовать распутываться.
Минус сумма, s сумма по l, q k l, z k l, l, l. Так, понятно, откуда все члены возникли.
Первые два из обычного свойства субодитивности. Вот это вот выражение написали, что энтропия
составной системы не превосходит сумму энтропии под системой. А последнее из вот этого неравенства.
Ну давайте с этим последним неравенством как раз поработаем. Тут на правой доске поработаем.
Смотрите, какие объекты у нас в этом выражении входят. Вот есть энтропия от суммы, где вот такое
выражение стоит с этими вспомогательными l. Вот давайте про него подумаем. Вы видите,
что здесь l это грубо говоря нумерация блоков в матрице плотности. То есть если я напишу вот
эту сумму по l, q k l, z k l, l, l. Энтропия этого выражения совпадает с энтропией выражения,
если я поменяю их местами. То есть напишу порядок исследования Гильвертова в пространство поменяя.
Сейчас скажу зачем я так пишу, чтобы вам было понятно блокчено-диагональная структура. Вот
смотрите энтропии у вас у какой матрицы получается? У блокчено-диагональной матрицы какие...
Вот здесь? Ну это числа. Поменяем местами под систему. Это все равно, что вы применяете
унитарный оператор обмена. Q числа, да. Вот смотрите, в каждом блоке теперь что стоит?
Вот сумма, а нет не сумма. Тут стоит просто q k l, z k l. l равное, например, единице. Потом следующий
блок какой q k l равно двойке, z k l равно двойке и так далее. Это понятно или нет? Мне главное,
чтобы вы вот это поняли. l, ортонормированный байс, определяет положение вот этих вот блоков.
Энтропия получается сумме энтропии блоков. Вот это понятно? Это будет сумма по l. Видите,
сумма наружу выходит в этом случае. s q k z k l. Вот. Здесь теперь у вас энтропия берется не от матриц
плотности, а от субнормированных матриц. Их след меньше единиц. Это тоже можно упростить дальше.
Значит, как это упростить? Вспоминаем, что такое энтропия. Энтропия это у нас минус след вот
этого q k l, z k l. Так, что-то сейчас напишу. Логарифм. Логарифм от q k l, z k l. Такое выражение.
Теперь логарифм от произведения числа на матрицу. На семинаре с вами тренировались. Логарифм этого
числа на единичную матрицу. Прибавить это число на логарифм. А нет, вы прибавите просто логарифм этой
матрицы. Сейчас запишу. Логарифм числа на единичную матрицу. Прибавить просто логарифм этой матрицы z k l.
Это понятно? Просто матричная форма вашего правила. Логарифм произведения равнеется сумму логарифм.
Так, и что тогда здесь у нас получается после упрощения? Так, получается, поскольку z k l
матриц плотности, их след равне единице, получается просто минус сумма по l q k l логарифм q k l. И что там еще?
И, и, и. И получается плюс q, так, сумма по l q k l, энтропия вот этих вот z k l.
Так, вот здесь вот понятно? Ну, то есть, вот это выражение, которое здесь встретилось, оно у нас упрощается.
Так, теперь, где у нас еще похожее выражение с этими l-ками? Вот здесь вот, с, а, б, с.
Тут вот. Значит, в чем отличие вот этого выражения от того, которое мы только что делали? Тут еще будет вот эта часть сидеть.
Так, давайте я напишу, тогда, с, а, б, с, вот здесь вот. Значит, рассуждения там абсолютно такие же,
абсолютно такие же. Значит, что мы с вами тогда получаем? Минус, а, да, сумму по l q k l логарифм q k l.
Так, прибавить сумму по l q k l, вот это вот выражение. А тут стоит, будет стоять энтропия чего?
Будет стоять энтропия вот этого тензорного произведения.
Так, теперь у меня к вам вопрос. Энтропия от тензорного произведения.
Энтропия от тензорного произведения двух матриц. Антон, что это такое будет? Энтропия от тензорного произведения.
Не произведение. Сумма, просто сумма энтропии без минусов. Просто сумма энтропии. Помните это или нет?
Сумма энтропии.
Вот, что получается. Так, давайте теперь посмотрим, какие выражения у нас уйдут, когда мы подставим их вот в эту форму.
Что у нас там получится тогда? Вот эти вот суммы по l q k l, логарифм q k l, оно появилось и в ABC, вот здесь вот, в энтропии ABC, и оно же у нас вот в этом члене, в последнем присутствует.
Видите? Так, тут другого цвета нет, поэтому я просто обведу, вот это вот уйдет вместе с вот этим вот выражением, они уйдут, когда мы их подставим вот в эту форму.
Так. Это понятно? Хорошо. Так, теперь что у нас еще? Еще что-нибудь уйдет или нет?
Вот, смотрите, видите? Сумма по l q k l s z t k l, и тут тоже сумма по l q k l энтропия z t k l.
Значит, их я тоже выделю теперь двумя линиями. Они тоже уйдут.
Тоже уйдут.
Значит, что тогда у нас останется?
У нас тогда останется от вот этой вот разницы, от вот этой вот разницы, у нас останется только вот эта энтропия.
Так, но не забывайте, что еще вот на это q да множество, тут я написал. Вот это произведение-то осталось на скобочку общую.
Значит, вот так, куда мне написать, куда мне написать? Давайте я правую часть сотру, уже она не нужна.
Все вспомогательные объекты мы посмотрели.
Значит, мы тогда напишем равно вот здесь, вот в конце этой формулы, и продолжу я его здесь.
Это равно писать.
Что у нас тогда получается? Переписываем первые два члена.
s sum q k l q k
Так, дальше что мы с вами переписываем? Отнять.
Вот это первое выражение переписываю.
sum q k
p k
s
sum q l
q k
l
z k l
Так, и что у нас еще осталось?
Осталось у нас вот это вот выражение.
О, есть.
Так, с каким знаком-то оно будет у нас?
Со знаком?
Плюс, это что-то мне не очень нравится.
А, вот здесь вот минус стоит.
Значит, если я напишу minus sum q k p k
Спасибо, что подсказали.
minus sum q k p k
А дальше здесь будет еще sum q l
Видите, двойная сумма получается.
q k l
и энтропия
Все, мы уже с вами на финишной прямой, кажется.
Упростили до такого выражения.
Так, теперь давайте разделять.
Вот там, где у нас z, это будут те выражения,
которые мы будем сейчас смотреть.
А то, что у нас содержит кси,
это будут другие два выражения.
Так, вот, теперь давайте интерпретировать,
что у нас получилось.
Я напишу таким образом.
Это будет энтропия.
Вот смотрите, соединю сейчас первую
и вторую в одну запись.
Ну, как бы в одну строчку напишем
и увидим что-то такое.
sum b k p k
Теперь смотрите.
Вот то, что стоит внутри,
sum q l q k l z k l
Это тоже какая-то матрица плотности.
Почему? Потому что q k l
распределение вероятностей q l
и z k l тоже набор матриц плотности.
Это какая-то матрица плотности,
которую нужно бы описать индекс k.
Вам не нравятся мои эти обозначения?
Давайте свою букву
для этого всего.
Давайте греческую букву
для матрицы плотности.
Давайте y.
Как маленькую y писать,
я что-то не помню.
Давайте большую.
Давайте большую.
y k
Это матрица плотности.
Вот это понятно?
Это матрица плотности.
И вот смотрите,
если я здесь двойную сумму смотрю,
сумма по l q k l z k l
как раз мне дает эту матрицу плотности.
Значит, я вот здесь пишу
только сумму по k теперь,
l не пишу сумму.
И пишу здесь вот это вот y k.
Отнять сумму по k
А так,
p k
энтропия y k
Причем эта матрица плотности
не абы какая.
Смотрите, почему я говорю,
что она непроизбольная.
Давайте посмотрим
на это выражение,
которое у нас здесь есть.
Возьмем q k
Возьмем частичный след
по второй подсистеме.
Тогда у нас как раз и получится.
Это сумма по l q k l z k l
Здесь будет единичка
просто частичный след.
Это будет как раз ипсилонка.
Если я в правой части возьму
частичный след по второй подсистеме,
я получу ипсилонку.
Что получается в левой части,
если я беру след по второй подсистеме?
θ сохраняющий след отображения.
Поэтому это взятие
частичного следа.
Смотрите, сейчас напишу,
а вы скажете, понятно или нет.
θ от взятия частичного следа
по второй подсистеме
от этого ρ k
Давайте говорите.
Мы берем частичный след
по второй подсистеме,
чтобы посмотреть,
что такое ипсилонка.
Я сейчас просто допишу
дальше, что такое ипсилонка.
Ипсилонка
это есть phi
действующая на
взятие частичного следа
по второй подсистеме от ρ k
Тут нам то, что
здесь стоит взятие частичного следа
по второй подсистеме от ρ k, неважно.
Это просто действие phi
на какую-то матрицу плотности.
То есть этот y k
матрица будет в образе phi.
Если у вас было
множество всех матриц плотности,
на которые действовала phi,
то вот эти y будут лежать
в образе этого phi.
Так что по-хорошему
мне надо было
вас попросить
вот эти частичные следы
по второй подсистеме ρ k
как-то обозначить.
Но я думаю, что обозначений уже очень много.
Но если вы
стерпите еще одно,
то сделаем.
Сейчас объясню,
почему я это хочу.
Здесь phi действует на что-то.
Давайте я так и напишу
след по второй подсистеме ρ k.
А здесь напишу
phi то же самое.
Хорошо, с первым членом разобрались.
Теперь вы видите,
что это есть
граница Холлива
для канала phi.
Вот здесь что важно.
Почему важно, что эти y
принадлежат образу phi?
Потому что здесь
не произвольная матрица плотности,
а те, которые получаются
действием phi на что-то.
То есть они уже зашумлены
этой phi.
И поэтому сама
эта разность
не будет превышать
х пропускной способности.
Это будет следующая строчка.
С первым выражением разобрались.
Теперь вторая часть.
Вторая часть проще.
Идеологически.
Здесь у вас стоит энтропия
суммы по k по l
p k t q k l
θ
x k l
отнять
сумму по k по l
p k q k l
s
θ
x k l
Здесь у вас произведение
p k t q k l
задает новое распределение
по мультииндексу q l.
Это само распределение
вероятностей.
Распределение вероятностей
по мультииндексу
q l.
То есть смотрите, у вас есть
какой-то мультииндекс q l.
Распределение вероятностей.
А здесь матрица плотности тоже
с двойной нумерацией q l.
То есть это какой-то ансамбль.
Понимаете?
То, что мы написали сейчас,
это сумма двух х величин.
х величина
Для какого ансамбля?
Для ансамбля
p k t
и вот этого phi,
действуя на взятие следа
по второй подсистеме
от рокатова,
плюс х величина
для какого распределения?
p k t q k l
и вот это вот phi
действует
q l.
Так, пишу мелко.
Надо, наверное, продублировать крупнее.
Вот, что у нас получилось.
То, с чего мы начинали, не превосходит.
Перепишу просто те величины,
которые там написаны.
х величина для ансамбля
phi действует
на след по второй подсистеме
от рокатова,
плюс х величина
для ансамбля
p k t q k l
и вот это действует
на что там?
x k l.
А сами х величины
не превосходят
их супремумов по ансамблям.
Это конкретные ансамбли,
которые получились
из оптимального ансамбля
для тензорного произведения.
Здесь мы взяли конкретный ансамбль,
на котором достигается равенство.
Потом выразили,
через неравенство
получили здесь какие-то
ансамбли, которые
для указанных каналов phi и theta
могут быть оптимальными,
могут не быть оптимальными.
Поэтому я пишу
меньше либо равно.
Меньше, если не оптимальное,
равно, если оптимальное.
Чего? c 1 в бесконечности
для канала phi
плюс c 1 в бесконечность
для канала theta.
Все, вот здесь мы и закончили
с вами доказательства.
Но это по-латински,
это требовалось доказать.
Саму фразу я вам не скажу.
В книжке что там?
Квадратик ставят?
Или что делают?
Вопросы я не спрашиваю.
Вопросы я не спрашиваю.
Вопросы я не спрашиваю.
Ой, что-то я начал стирать,
не спросил у вас есть ли вопросы.
Ну, не самое простое доказательство,
согласен с вами.
Тут скорее надо
как вам сказать,
когда знаешь, что хочешь получить,
тогда следишь за этими
отображениями phi и theta.
Если не знаешь, что хочешь получить,
то можешь запутаться.
Сейчас я здесь восстановлю
и скажу следствие.
Утверждение мы с вами доказали.
Какое будет следствие из него?
Если phi
разрушает перепутанность,
то
вот истинная пропускная
способность c
бесконечность-бесконечность phi
равняется его
пропускной способности
c1-бесконечность.
Вот такая вот история.
Доказательство простое.
Вспоминаем, что такое
истинная пропускная способность.
Это когда вы разрешаете
использовать блокчные кодирования
неограниченной длины.
То есть n длина блока,
и вы тогда
используете эти
блокчные кодирования,
посылая уже
большие
коллективные
состояния
через n копии канала.
Вот эту формулу
помните или напомните?
Смысл заключается в том, что вы теперь
кодируете в длинные такие
слова, которые могут
быть перепутанными.
Мы знаем, благодаря хастингсу,
что в общем случае это помогает
больше информации передавать.
Если же канал разрушает перепутанность,
то пользы от этого нет.
Почему?
Потому что вы вот это phi tensor на n
можете представить вот в таком
виде. Давайте я перепишу этот предел.
Тут у вас стоит c1 бесконечность
phi
tensor умножить на phi
в степени n-1
тензорной.
Вот это phi в степени
тензорной n-1
играет роль вашего канала
тетто из предыдущего
утверждения.
Понимаете?
Теперь вы что? Давайте сделаем
в уме. c1 бесконечность phi
прибавить c1 бесконечность
phi тензорно n-1.
Снова его раскладываете
как phi
тензорно умножить на phi
тензорно n-2.
И так по индукции.
Значит у вас здесь что получится?
Меньше либо равно,
чем предел при n
стремящемся к бесконечности.
Чего? 1 делить на n, n-1,
c1 бесконечность phi.
Будьте здоровы.
Правильно.
n пропадает,
остается просто
c1 бесконечность phi.
То есть мы с вами доказали,
что c бесконечность бесконечность
не превосходит
этой c1 бесконечность phi.
Однобуквенная пропускная
способность еще говорят.
Однобуквенная, потому что
в нобуквенной пропускной
способности еще говорят.
Однобуквенная, потому что здесь единичка стоит.
А почему равно?
Потому что не меньше.
Все, доказали.
Некоторые резюме.
В прошлый раз мы с вами
каналы разрушающей перепутанность
в отделе.
И вот сейчас мы видим, что передавать классическую
информацию через такие
каналы,
каким свойством обладают,
что блокчное кодирование
не поможет вам.
То есть не стоит мучиться,
просто однобуквенное кодирование
можно использовать для передачи информации
в этом случае.
То есть никакого выигрыша от перепутанности
на входе нет.
В этом смысл
информационных этих каналов.
Сейчас сделаем перерыв,
а после перерыва
посмотрим другую ситуацию,
где перепутанность будет
использоваться как ресурс
для передачи квантовой информации.
Все, делаем перерыв.
Я анонсировал перед перерывом,
что теперь мы будем использовать
перепутанность
как ресурс
для передачи информации.
Для этого мне нужно сначала напомнить,
что сама перепутанность
не позволяет передавать информацию.
Вспоминаем это
эксперименты АСПО.
Инштейна Падульского-Розана.
Парадокс ОПР
с прошлого семестра.
И эксперименты АСПО.
В чем была идея?
У вас был некоторый источник
от английского сорса,
перепутанных там частиц,
например, фотонов.
Что вы дальше могли делать?
Тут у вас
некоторое перепутанное состояние.
Один фотон летит влево,
другой фотон летит вправо.
Тут вы могли
выбирать,
какое измерение делать,
то есть выбирать базис,
в котором делать измерение.
И тут тоже.
Там три параметра записываются в виде вектора.
Были два детектора.
Тут было плюсик,
минусик.
Это D1.
Тут также было
плюсик,
минусик.
Это было D2.
Помните такую картинку?
Да, да, да.
Напомню.
Пусть это перепутанное состояние,
такого вида.
1 делить на корень из 2,
0, 0, плюс 1, 1.
Вот такого вида, например.
А измерение у вас
в базисе собственных векторов sigma z.
То есть
измерение в стандартном базисе.
То есть вот измерение
в стандартном базисе для кубита,
измерение в стандартном базисе для кубита.
Тогда что у вас будет получаться?
Если слева вы получили 0,
то и справа
вы со 100% вероятностью получите 0.
Правильно?
Если слева получили 1,
справа получите 1.
Корреляции 100%.
То есть полностью скоррелированные
выходы слева, справа.
Почему нет передачи информации?
Да, вероятность появления
этих ноликов и единичек
это одна-вторая.
Если бы, например,
из одной лаборатории в другую
хотел передать какое-то сообщение вида
0, 0, 0, 1, 1,
то не факт, что при генерации
при измерениях
получилась бы именно такая последовательность.
Правильно?
Вероятность 1 делить на 2 в 5.
Поэтому мне нужно
звонить по обычному телефонному кабелю
и посмотреть, вот сейчас
то, что я хотел
передать, и так далее.
То есть смотрите,
просто перепутанность,
использование перепутанности просто
не позволяет передавать информацию.
Давайте эту схему теперь
модифицируем
на центральной доске.
Покажу, как можно модифицировать эту схему.
Модифицируем.
Вот пусть у вас есть источник,
он генерирует
перепутанной пары.
Одна пара в лабораторию А попадает,
ой, один кусочек этой пары,
другой кусочек этой пары в лабораторию
В попадает.
Дальше смотрите, что вы можете сделать.
Вы можете
в зависимости от той
буквы, которую вы хотите
передать,
подействовать на свой
кусочек из этой пары
некоторым преобразованием,
которое будет зависеть
от этой буквы.
Поскольку здесь мы
с чистыми состояниями работаем,
давайте применим унитарную
операцию, которая
зависит от этой буквы х.
Ух мне надо писать.
Унитарное преобразование
зависище
от буквы х.
Если я хочу, например, передать нолик,
я буду одно преобразование использовать.
Если хочу передать единичку, то другое.
Что тогда получается?
Получается, что
я модифицирую свою часть
перепутанного состояния
в зависимости от того,
какую букву хочу переслать.
Что я могу использовать?
Я могу использовать, например, оптоволокно
или другой канал связи,
атмосферу просто.
Вот здесь появляется
ваш канал связи.
Квантовый канал.
Вполне положительно сохраняется
этот отображение.
В итоге сигнал
от этой первой половинки
приходит в лабораторию B.
Что в лаборатории B можно сделать?
Погодите.
Я хочу послать, например,
вот эту последовательность.
0, 0, 0, 1, 1.
Значит, у меня источник
должен сгенерировать 5 пар
фотонов.
Первая пара пришла,
вернее кусочек ее проходит,
я применяю унитарную операцию
у 0.
Дальше этот фотон летит
через канал связи
и попадает в лабораторию B.
Вшло второй кусочек.
Второй раз источник
сгенерировал пару.
Я снова у 0 применил,
отправил. Еще раз у 0
применил, отправил. На четвертый раз
буду уже у 1 применять,
чтобы то сообщение
отправить.
Понятно?
И потом буду применять
снова 1.
Что хочу передать,
то и применяю.
Но источник каждый раз должен
новый парон соорудить,
перепутанный фотон.
Теперь смотрите, что в лаборатории B
можно сделать.
В лаборатории B можно измерить
эти два фотона
одновременно,
то есть коллективное измерение
двух фотонов.
Измерение
двух
фотонов.
У вас будет какой-то исход измерения,
правильно? Классический.
Это будет ваш Y.
Взаимная информация
между X и Y.
Взаимная информация
между X и Y.
Это будет ваша передача
классической информации
через квантовый канал
с использованием ресурсоперепутанности.
Эта передача
показывает количество
классической информации
классической,
количество классической информации,
которую
можно передать
через квантовый канал
квантовый канал
при использовании
ресурсоперепутанности.
Перепутанность сама по себе
не позволяет передавать квантовую информацию.
Если вы ее используете
как некоторый ресурс
с последующими операциями
кодирования и декодирования,
то вы можете передавать.
Но видите, должна осуществиться
через A в B, через этот канал.
Схема понятна?
Давайте посмотрим
самый простой случай.
Пример.
Кубиты.
То есть A и B это кубитные системы.
S производит у вас
состояние
вот то самое, которое
с вами вспоминали.
Плюс один-один.
Одно из состояний было.
А phi пусть у вас тождественный канал.
То есть шума нет.
Что тогда будет происходить?
Здесь у вас перепутанный
пара фотонов
и нам нужно что-то сказать про вот эти кодирования.
Вот смотрите, что я сейчас буду делать.
Я в качестве х возьму 0,1,2,3
а в качестве ух
у меня будет выступать
вот что.
1 если х равно 0
sigma x
ну или sigma 1
если х равно 1
sigma 2
если х равно 2
sigma 3 если х равно 3.
Можно было написать просто
что ух равняется sigma x.
Это матрица Пауля.
Они унитарны?
Да.
Они соответствуют некоторому
унитарному преобразованию,
которое вы можете над одним из кубитов
в лаборатории А совершить.
Смотрите, что тогда получается.
Вот здесь вот в лабораторию B
будет
перепутанная
пара фотонов.
Вот так вот.
Вот здесь вот в лабораторию B
будет
перепутанные фотоны
входить в нее уже.
Изначально были перепутаны.
Одну часть
мы изменили с помощью
вот этих унитарных вентилей.
Тут никакого преобразования не произошло.
И вот они пришли.
Какие у нас получатся с вами
состояния?
Это я psi обозначу.
На вторую часть
ничем не действует.
Но если
х равно 0,
то я сам знаю,
что получится.
Это будет то же самое
состояние, что изначально.
1 делить на корень из 2
0, 0, плюс 1, 1.
А вот если
х равняется 1,
Викторию,
что я получу?
1 делить на корень из 2,
а какое состояние придет
Викторию?
Тогда.
1, 0, плюс 0, 1.
Согласны?
Как мы так сделали?
Мы просто применили
унитарный оператор
к первой пути системе.
Если х равно 2,
Эдуард,
что получу?
Надо вспомнить
sigma 2,
0,
минус i и 0,
в стандартном байесе.
i куда-то вылезет,
общее.
Плюс 0, 1.
Правильно.
И если х равно 3,
то что мы получим?
Святослав, что мы получим?
Минус будет.
Эти все состояния вам что-то напоминают?
Состояние Белла
вы получаете.
Вы можете сказать, состояние Белла
не было и
было 1, 0, минус 0, 1.
Но это общая глобальная фаза.
Правильно вы все говорите.
Она не влияет на все физические свойства
этого объекта.
Это все состояние Белла.
Это состояние Белла.
Они артагональны.
В качестве измерения
я могу провести
проективное измерение
в базисе состояния Белла.
В качестве детектора
вот этого
будем использовать
измерение в базисе Белла.
У него сколько исходов
у этого измерения в базисе Белла?
Четыре.
Четыре исхода.
Так, где мне это нарисовать?
Давайте на центральной доске нарисую.
Значит, у вас
четыре исхода.
Значит, Y может
принимать то же значение
либо 0, либо 1, либо 2, либо 3.
Теперь давайте с вами составим
некоторую табличку истинности,
как вы делали в школе
на информатике.
Какие X, какие Y будут получаться?
Так.
Или это не так называется?
Наверное, я что-то не то сказал.
Так, сейчас, погодите секундочку.
Так, Байс Белл у нас что?
Ну, давайте вот прям
в таком порядке и пойдем,
как здесь написано.
Да, да, да.
Значит, правильно
сказал Екатерина, что
если мы
используем
вот такую нумерацию
для Y, то у вас получится
с вероятностью 1, вы будете
0,10 получать.
1, все остальные будут 0.
Но это на самом деле, в чем мы рисуем
эту английскую матрицу переходов?
X-ы переходят в какие Y?
С какими вероятностями?
Да, если я X,
вот смотрите, это надо осознать.
Если я в качестве X-а использую, например,
двойку,
то это что означает?
Пришла ко мне
часть перепутанного состояния.
Я применяю здесь sigma2,
отправляю дальше.
Здесь проводится измерение
в Байси Белла.
Какой исход я там получу?
Получу с определенностью
вот это вот
проективное измерение
в Байси Белла дело.
Получается, что я могу
идеально различать
в этом примере
четыре возможной конфигурации.
При этом сколько раз я посылаю
фотон через канал?
Один раз.
В итоге, вывод какой?
Могу переслать
два витоинформации
за пересылку одного
фотона.
Это надежная передача
двух витоинформаций.
При пересылке
через
phi
одного
фотона
или кубита
в этом случае.
Это что означает?
Что я не один бит информации
извлекаю, а два бита
информации извлекаю.
Поэтому люди называют это еще
сверхплотное кодирование.
Superdense coding по-английски.
Какой вопрос?
Ну,
не знаю.
Видимо, там, где достигается
границы Холлива.
Если бы у вас
этого ресурса перепутанности
не было, сколько бы вы могли
бит информации максимально пересылать
при пересылке одного кубита?
Один.
Когда ресурс перепутанности
появился,
при отсутствии шума,
когда phi идеальный канал,
при отсутствии шума, можно пересылать два.
Я показал, как.
Самый
тонкий момент здесь,
почему на практике
это сложно реализовать?
Потому что шум не должен действовать
на вот этой части.
Тут нет шума
в этой части, где
вы создаете вашу перепутанную сталь.
Пример понятен?
Теперь возникает вопрос.
А если мы теперь phi
сделаем не тождественным каналом,
а каналом с шумом?
Сколько информации можно будет передавать?
Мы понимаем, если шум маленький,
мы чуть-чуть от тождественного
отличаемся, то будет чуть меньше
двух бит.
Так, а еще что у сходницы получается?
Что-что?
Вот смотрите, теперь
какое-то определение сделаем.
Что в этой схеме ИХY понятно?
Вы можете
теперь сформулировать
вот так вот.
Вы можете сформулировать
понятие
Intel Note Assist от КПСТ.
Пропускной способностью
канала phi
с использованием
ресурсоперепутанности
называется
вот такая величина.
Обозначим ее C-EA.
EA от английского Intel Note Assist.
Это будет у вас супрем
вот этим вот ИХY
в схеме, которую мы обсуждали.
Что вы можете менять?
Вы можете менять
вероятность
появления этих букв PX.
Вот это вы можете делать.
Вы можете менять
унитарные
операторы у Х.
В принципе, вы и
источник перепутанных пар
можете менять.
Но не буду его сюда
описать, потому что...
Ну, давайте напишем С.
Так, все?
Вроде все.
А, и еще
измерительные устройства
можете менять.
То есть, можете EY
ваш POVM
на выходе тоже
менять.
POVM.
Понятно?
Сейчас.
Разве нет
противоречия какого,
что вот
значит, если мы считать
время, за которое мы теали
два библии информации,
это вот время половины пути
плюс еще половину пути.
Правильно?
Тут
считается по количеству
входов в канал
FI. Вот.
Провайдер, который вам дает
этот канал, связь,
он не знает,
используете вы ресурс перепутанности,
не используете. Он просто знает,
что к нему что-то на вход пришло,
что-то на выход вышло.
На выходе вышло.
Вот сколько раз вы обращаетесь к этому
провайдеру,
в расчете на количество этих
обращений количество информации
считается. То есть, смотрите, как вы могли
гипотетически поступить.
Вы могли приготовить много
перепутанных состояний,
положить
кусочки вот этих перепутанных пар
в один чемодан
и куда-то уехать.
А потом
пересылать эти
фотоны через канал,
зная, что
получатель сообщения
будет брать кусочки
своих вот этих вот перепутанных пар
и измерять их совместно
с тем, что пришло от вас.
То есть, если бы у нас существовал
чемодан, в котором мы могли бы
хранить вот
эти вот
кусочки перепутанных
состояний,
а потом, при необходимости
использовать, пересылая через
обычный канал связи,
тогда было бы нам счастье.
Мы могли бы
пересылая один бит,
получать там вплоть
до двух бит информацию.
Ну, вы начали считать время.
Я говорю, не надо считать время,
надо считать число использования
канала ФИ.
То есть,
пропускная способность,
вот какой у нее смысл?
Она показывает вам количество
бит информации,
деленное на что?
На число,
ну, на количество использований
канала ФИ.
Вот, такой схема.
Теперь наша задача,
наша задача
состоит в том, чтобы понять, как выглядит
формула для этого ЦЕАФИ.
Анонсирую сразу
результат, чтобы вы могли
на семинарах что-то посчитать.
Пока без доказательства.
Вот, оказывается, что здесь
можно
написать равенство
вот такому выражению
супремум
по одному
состоянию РО.
Это будет среднее состояние ансамбля.
Вот такой вот величины.
Энтропия РО
плюс энтропия
ФИ, действуя на РО.
Отнять
энтропия
комплементарного канала,
действуя на РО.
Что такое комплементарный канал,
будем обсуждать.
Где ФИ с волной,
это комплементарный
канал
по отношению
к ФИ.
Здесь
стоит
что?
Энтропия состояния на входе,
энтропия состояния на выходе,
минус энтропия выхода
комплементарного канала.
Это будет показывать,
сколько информации
в окружении растворяется.
Сейчас буду объяснять,
что такое
комплементарный канал.
Что такое ФИ с волной?
Вы помните,
что квантовый канал
можно записать
в виде представления
Stein Spring.
То есть можно записать
в виде следа
по вспомогательному пространству
K
W
РО W
Крестик. Где?
Вот это W
это отображение
из вашего H
в HK.
РО – это у вас
матрица плотности,
ну, оператор плотности на H.
Вот здесь.
Напомню,
конструкцию,
как мы это делали.
ФИ от РО можно записать
в виде представления
Крауса, Сума, Пока,
АК, ТРО, АК, Крест.
Это представление Крауса.
Погромче скажите.
Оператор плотности.
Так.
Вот у нас есть представление Крауса.
Помните такое для квантового канала?
W в этом случае,
мы с вами как делали
эту изометрию?
Мы просто брали операторы Крауса
и их записывали с вами в столбец.
А Р
Значит, не знаю,
помните вы или не помните,
вот такое вот свойство есть,
что
W
Эрмитово-сопряженное
умноженное на W равняется
единичному оператору.
Это результат сохранения следа
вот этим отображением
ФИ.
А если в обратную сторону умножить,
то в общем случае вы не получите
единичный оператор.
W называется изометрией.
Вспомнили
представление Крауса?
Начинается
прошлый семестр.
Теперь
представление Крауса.
Прошлый семестр.
Представление станцпринга тоже
прошлый семестр.
Теперь какая картинка у нас с вами
может быть
для представления станцпринга?
На левой доске я сейчас
диаграмму нарисую.
Вот есть у вас состояние РО.
Вы что можете сделать?
Вы можете
его через вашу
изометрию
изменить.
Вы можете
его изменить
через вашу изометрию
пропустить.
На выходе вы получите
какое состояние?
W РО W крестик.
И это будет
состояние
составной системы.
Состояние
составной системы.
Какой?
Здесь одна линеечка
играет роль системы на выходе.
А другая
линия играет роль эффективного
окружения.
K это как раз
пространство эффективного
окружения. Почему
эффективное?
Потому что реальное может быть какое-то другое.
Но математическое
описание
можно сделать с этим
эффективным окружением конечной размерности.
Теперь
когда мы с вами делаем представление
SteinSpring для канала,
мы берем частичный след
по вот этому вспомогательной подсистеме.
Частичный след.
И тогда у нас здесь
получается phi действует на РО.
Мы еще с вами рисовали таким образом.
РО здесь у вас
есть некто риперное состояние.
E мы обозначали в прошлый раз.
Тут какое-то
унитарное.
Здесь вы берете
частичный след.
Такую картинку вы рисовали?
Теперь смотрите.
Времени у меня мало, чтобы новую рисовать.
Я сейчас буду модифицировать эту.
Теперь мы
с вами сделаем по-другому.
Мы будем
частичный след
брать по первой подсистеме.
Вот здесь будем брать
частичный след.
А здесь
на выходе у нас будет что-то получаться.
Как это интерпретировать
на нижней картинке?
Если я беру частичный след по первой подсистеме,
то это означает, что мне не важно,
как выглядит система
после действия канала.
Мне важно,
как выглядит окружение.
То есть нам важно, как
наше состояние РО,
которое мы на вход подали канала,
повлияло
на окружение.
И это задается в виде
комплементарного канала.
Вы видите, отображение РО
на выход сюда
будет линейным отображением.
И это линейное отображение
и будет комплементарным каналом.
Тогда как запишется формула?
Формула для комплементарного канала
запишется тогда так.
Фи комплементарное РО
равняется след
уже по h,
по вашему исходному
Гильбертовому пространству, по вот этому.
Вот чего
W РО W.
То есть смотрите,
формулы очень похожи
для Фи комплементарного и для Фи прямого.
Просто частичный след берется
по разным частям.
Давайте теперь найдем
в явном виде
действия комплементарного канала.
Если нам известно
представление Крауз.
Вот это W
это изометрия
составленная из операторов Крауз.
Давайте вспомним
как записать W РО W крестик.
Я должен взять вот этот столбец
из операторов,
потом написать РО,
потом написать строчку
из операторов.
Можно перемножать? Можно.
Что получу?
Получу матрицу
состоящую из блоков.
Первый блок
A1 РО A1 крест,
второй блок
A1 РО A2 крест
и так далее.
Соответственно вторую строчку
начну писать A2 РО A1 крестик.
A2 РО A2 крестик
и так далее.
Понятно структуру матрицы?
Каждый из этих блоков
имеет размерность системы
на выходе.
Тут может быть у вас
некоторый когнитивный диссонанс.
Смотрите, чтобы взять след
по K
W РО W крестик
в наших обозначениях
в смысле этой блоковой матрицы
составной системы.
Это будет вторая подсистема
или первая?
Диагональки.
Давайте исходя из этого делать.
Сумма диагональных матриц
Блоков диагональных
АК РО АК крест.
Сумма диагональных блоков
БЛА, правильно?
Но это чему соответствует?
Помните, когда мы берем
сумму блоков на диагонали?
Для этой матрицы
это как бы по первой подсистеме
взять след.
Поэтому я говорю, что будет сейчас
когнитивный диссонанс.
Хорошо, теперь
если я возьму след
по другой, а их всего две
подсистеме
то как мне записать результат?
Это значит, нужно взять след
внутри каждого из блоков, правильно?
То есть это будет некоторая матрица
Тут уже будут стоять числа
На пересечении
ИТ из строки
и житого столца
будет какое число написано?
След
от АИТОВО
РО АЖ крестик
И это есть действие
комплементарного канала
комплементарного канала
на состояние РО
Задавайте вопросы
Еще такую формулу пишут
ФИ комплементарная
действуя на РО
ИТ житый элемент
Вот так вот
ИТ житый элемент
вот этого выражения
Это есть просто след
АИТО РО АЖ крестик
Вот такую формулу можете увидеть
Так можно вычислить
и задать комплементарный канал
действие комплементарного канала
Подробнее в следующий раз
Откуда берется
вот это СРО
прибавить СФИ РО
отнять СФИ комплементарный РО
в следующий раз
Это взаимная информация
У нее есть свойство такое
Видите, почему я здесь не пишу
бесконечности всякие
Потому что у нее есть свойство аддитивности
В этом случае
мы знаем, что
использование больших перепутанных
не к чему не приведет
Это результат конкретный
В следующий раз подробнее
про то, как берется эта величина
Но вы можете уже решать задачки
из задания
с расчетом ее

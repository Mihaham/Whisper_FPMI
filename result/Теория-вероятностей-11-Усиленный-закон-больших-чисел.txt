Да, ну в общем, мы запись начали с некоторым познанием, но вот всё, что произошло, написано
на доске. Мы пытаемся доказать, что исходимость по вероятности следует исходимость по распределению,
и для этого будем использовать критерий исходимости по распределению, который говорит, что последний
исходец по распределению, тогда это когда он сходится слабо, а слабая исходимость
определение написано здесь. И вот мы будем доказывать им на это, то есть мы говорим,
что пусть у нас есть исходящиеся по вероятности последовательность, будем доказывать,
что какой бы мы ни взяли непрерывно ограниченную функцию, справили вот это. Начиная с некоторого
момента модуль разости, модуль ожидания f от x и f от x меньше, чем epsilon для какого-то epsilon,
который мы зафиксировали. Итак, давайте сделаем следующий трюк. Давайте рассмотрим последовательность
это n и обозначим вот таких вот случайных величин x умножить на индикатор того, что модуль x меньше
чем n. Понятно, что почти наверное такая последовательность стремится к си. То есть
если я увеличиваю n, то начиная с некоторого омега, у меня просто в этом омега значение
совпадет с кси. Вот из этого, конечно, следует исходимость по вероятности, это мы уже умеем доказывать.
В частности, что у нас означает исходимость вероятности? В частности, это означает,
что вероятность того, что модуль это n минус кси больше чем 1 вторая, стремится к нулю. А эта
вероятность это на самом деле в точности вероятность того, что модуль кси больше чем n.
Но что это означает? Это означает, что мы сможем найти какой-то n1, что для любого n больше
либо равного чем n1 вероятность того, что модуль кси больше чем n меньше либо равна чем
Эпсилон поделить на 3c.
Вот. Хорошо. Это первая вещь, которую мы будем использовать. Теперь, собственно, равномерная непрерывность,
о которой я говорил. Вот давайте возьмем отрезок ну скажем от минус 2n1 до 2n1.
Значит скажем, что f равномерно непрерывно на нём. То есть, если f непрерывно на cmr, то на
любом отрезке она будет равномерно непрерывна. Что это значит?
Ну это значит, что найдется такое дельта. Давайте выберем его меньше чем, меньше чем n1.
Понятно, что мы можем брать его. То есть, если мы какой-то дельта нашли, то любое меньшее дельта тоже подходит.
Это значит, что для любых x и y, модуль разности между которыми меньше чем дельта, модуль f от x минус f от y меньше чем f сам поделить на 3.
Так, это вторая вещь, которую мы будем использовать. И третья вещь, которую мы будем использовать, то, что нам доносится.
Сходимость по вероятности. То есть, мы знаем, что ксиен стремится по вероятности кси. Это значит, что, что нам, сколько нам нужно?
Так, дельта нам нужно. Вероятность того, что модуль ксиен минус кси больше чем дельта, стремится к нулю.
Да, это вот то самое дельта, которое мы здесь взяли, вот это дельта.
Ну, в частности, это означает, что найдется такое n2.
Что от любого n больше нула, чем n2. Вероятность того, что модуль ксиен минус кси больше чем дельта,
меньше равна, чем f сам поделить на 3.
А теперь, мы давайте все это дело будем использовать для нахождения.
Возьмем, выберем n больше чем максимум из n1 и n2, тогда мы все это смело сможем использовать.
И, значит, для него посмотрим на модуль разности между от ожиданиям f от кси от n и от ожиданиям f от кси.
Ну, в силу линейности, это есть, конечно, модуль от ожидания от разности f от кси от n минус f от кси.
Что меньше не равно, чем от ожидания модуля разности.
Вот, а дальше давайте посмотрим на отдельную ситуацию, когда модуль кси больше чем n1,
и модуль кси меньше оно, чем n1.
То есть, представим это как от ожидания от модуля f от кси от кси n минус f от кси
на индикатор того, что модуль кси больше, чем n1,
плюс от ожидания модуля f от кси n минус f от кси
на индикатор того, что модуль кси меньше, чем n1.
Ага, да, давайте здесь будем не на 3c делить, я прошу прощения, а на 6c.
Здесь будем делить на 6c, да, мы тут имеем право любую константу написать, вот давайте 6c напишем.
Тогда, так как значение нашей функции f на любом аргументе меньше 0, чем c,
то вот этот модуль разности всегда не превосходит 2c,
да, вообще какие бы ни были аргументы, эта штука не превосходит 2c,
поэтому мы этим 2c этому от ожидания можем ограничить, умножить на вероятность вот этого события,
да, потому что в ней этого события будет просто 0, потому что это индикатор,
умножить на вероятность того, что модуль кси больше, чем n1.
Но эта вероятность у нас вот тут вот ограничена, как кси поделить на 6c,
поэтому, господи, f сам поделить на 6c, мы перемножим, получим f сам поделить на 3,
да, то есть вот с этим у нас все просто.
Что делать с вторым слагаем? Второе слагаем, давайте еще разобьем,
у нас вот есть еще два условия,
сейчас второй трейс, да, вот эти два, вот мы в соответствии с ними это дело разобьем.
Я могу еще раз, а почему мы так можем ограничить вот 90 почти 2c вот на p на xn?
Вот это или что?
Да, да, ну смотрите, да, вот эта штука, она просто,
эта штука, она просто не превосходит 2c,
да, потому что значение функции f не превосходит c по модуле,
вот, а значит вы всю эту случайную величину, вот все это произведение можете ограничить
как 2c умножить на индикатор,
а мод ожидания от 2c умноженного на индикатор, это в точность вот эта штука.
Спасибо.
Вот, значит во втором слагаемом мы индикаторы еще разобьем на две части,
мод ожидания модуль f от xn минус f от xi умножить на индикатор того,
что модуль xi больше меньше, меньше 0 чем n1,
это значит первое условие,
и второе условие вот это, да,
которое у нас в третьем возникает в пункте,
модуль xi минус xi n больше, чем delta,
ну и соответственно то же самое, только меньше либо равно,
так, меньше либо равно, да, давайте вот здесь вот напишем меньше либо равно,
имеем право вот здесь меньше либо равно написать, чтобы все было строго, секунду.
Значит умножить на мод ожидания модуль f от xi n минус f от xi
на индикатор того, что модуль xi меньше 0 чем n1
и модуль xi минус xi n меньше 0 чем delta.
Вот, но со вторым слагаемом поступим точно так же, как с первым поступили,
то есть опять же мы понимаем с вами, что вот эта штука меньше 0 чем 2c,
поэтому можно написать плюс 2c на вероятность второго события,
второе событие даже можно увеличить,
написать просто вероятность того, что модуль xi минус xi n больше, чем delta,
благо мы знаем, что она, ага, тоже 6c здесь,
здесь тоже 6c мы пишем, а не 3c,
вот, то есть точно так же получим epsilon positive на 3, как с первым слагаемом,
а наконец с третьим слагаемом мы будем использовать второе условие,
да, мы знаем, что если между аргументами расстояние не больше, чем delta,
и они лежат вот в этом отрезке,
то тогда модуль разности
извините, вот в третьей строчке снизу вы написали модуль xi меньше n1,
а в нижней модуль xi больше, чем n1,
или это не к тому относится
сейчас, вот это, это вот это
вот второе слагаемое, вот
и сейчас я третьим слагаемым занимаюсь
да, значит еще раз, если у меня два аргумента лежат вот в этом отрезке
и между ними расстояние меньше, чем delta,
тогда я могу
модуль разности значений
в моей функции ограничить вот так
у меня как раз эта ситуация, вот у меня два аргумента, xi на xi
между ними расстояние меньше, чем delta
и один из них меньше, чем n1
но это значит, что второй меньше, чем n2
так как у меня delta меньше, чем n1
то значит оба аргумента попадают в этот отрезок
значит я теперь могу второе условие применить
и получить, что
при условии выполнения вот этого индикатора
то есть эта штука строго меньше, чем epsilon поделить на 3
а значит в моем ожидании меньше, чем epsilon поделить на 3
умножить там на вероятность, но
я даже вероятность просто уберу
понятно, что у меня от этого получится оценка сверху
просто напишу epsilon поделить на 3
но все это вместе даст epsilon
все это вместе даст epsilon
что я требуюсь
доказали, что исходимость по вероятности
следует исходимость по распределению
есть какие-то вопросы
теперь давайте еще немного поговорим
можно вопрос, а вот как мы это получили
диагномерно-непрерывности
мы просто взяли какой-то epsilon
такой вот, ну в смысле
ну просто мы, короче
взяли epsilon на 3, да, и все
и мы больше ничем не пользовались
не совсем понял вопрос
в смысле, мы просто
как бы записали намерно-непрерывности
и выбрали epsilon поделить на 3
и все, то есть мы ни с каких фактов это не уводили
мы просто взяли epsilon
и мы просто взяли epsilon
и мы просто взяли epsilon
и не уводили
вот никаких фактов не предшествовали
merits, никаких фактов это не предшествовал
вот это написано просто по сути
я определение равномерно непрерывности
больше ничего здесь не используем
нам доношено функции непрерывно на всем
значит мы делаем Secco
whatever operation we take, the last of our function will be equal
это написано, определенная равномерно неперев올ность
я выбрал для удобства, чтобы потом у меня все просуммировалось. я мог здесь просто эпсилу написать, окончательный вывод от этого бы не изменился.
ну хорошо, значит мы с вами в процессе доказательства этого утверждения уже сформулировали критерии слабой сходимости, критерии сходимости по распределению, так называемой теориям Александрова.
давайте теперь еще сформулируем еще пару полезных критериев, но для сходимости по распределению мы сформулировали, давайте еще сформулируем для сходимости почти наверное и для сходимости по вероятности.
вот это в общем-то аналогия так называемого критерия каши. для сходимости почти наверное вообще все понятно. сходимость почти наверное по сути обычная, сходимость последовательности.
вы говорите, что для каждого аргумента, почти для каждого аргумента есть сходимость значений функции на этом аргументе.
ну и тем самым можно говорить про сходимость, про фундаментальность последовательности 1.
то есть что это значит? будем говорить, что ксен фундаментально почти наверное или с вероятностью 1, если вероятность того, что ксен фундаментально она единица.
ну и соответственно критерий каши, тогда будет как выглядеть.
существует кси такая, что ксен стремится кси почти наверное, тогда и только тогда, когда ксен фундаментально почти наверное.
ну это более-менее очевидно, давайте короткое доказательство напишем.
ну пусть сперва ксен стремится кси почти наверное.
ну в эту сторону совсем просто, тогда рассмотрим множество, давайте вы значим его а таких, что ксен от амега стремится кси от амега.
ну у нас есть критерий каши, который говорит, что последовательность сходится тогда и только тогда, когда она фундаментальна.
да, поэтому из этого условия следует, что ксен фундаментально.
да, значит вероятность b равна единице, это и есть собственно определение фундаментальности последовательности случайных величин.
по обратную сторону, пусть ксен фундаментально с вероятностью 1.
давайте рассмотрим событие а, на котором наш последователь фундаментально.
понятно, что для любого амега из а существует предел при стремящемся к бесконечности ксен от амега, обозначим его кси от амега.
ну а вне, положим ну не знаю, ноль.
ну нам только понятно, что кси это случайная величина, но это правда, поскольку мы знаем, что предел последовательности случайных величин это снова случайная величина.
вот можно взять ксен с волной равная ксен умножить на индикатор события а, и тогда вот эта посредность будет вообще поточечна.
то есть вообще для любого амега из амега большое ксен с волной от амега будет стремиться ксетами.
да, то есть здесь не почти наверная сходимость, а сходимость вообще для всех амег.
и мы знаем, что предел последовательств случайных величин является случайной величиной, след на кси с волной это случайная величина.
ну и на множестве а, у которого вероятность равна единице, есть сходимость, а значит есть сходимость почти наверно.
что и требовалось.
по аналогии можно сформулировать критерий каши сходимости по вероятности, который чуть будет более сложен доказать.
давайте сначала скажем, что такое фундаментальность по вероятности.
будем говорить, что ксен фундаментально по вероятности.
если для любого epsilon больше 0, предел при NAM стремящемся к бесконечности, вероятность того, что модуль ксен минус ксен больше либо равно, чем epsilon, стремится к 0.
ну и критерий каши говорит, что есть сходимость по вероятности, когда опасность фундаментально по вероятности.
существует такая случайная величина кси, что ксен стремится кси по вероятности, тогда и только тогда, когда ксен фундаментально по вероятности.
так ну опять в одну сторону несложно из сходимости по вероятности фундаментальность доказать.
пусть ксен стремится по вероятности кси.
вот что сделаем, давайте посмотрим на модуль ксен минус ксен.
понятно, что эта штука по неранде с треугольника меньше чем сумма модулей ксен минус кси плюс ксен минус кси.
да, значит вероятность того, что модуль ксен минус ксен больше оно чем epsilon.
ну если оказалось, что модуль ксен минус ксен больше оно чем epsilon, то из этого следует, что хотя бы одна из вот этих штук должна быть больше чем epsilon пополам.
то есть вероятность это будет меньше чем вероятность того, что модуль ксен минус ксен больше繼 с逆.
плюс вероятность того, что модуль ксен минус ксен больше да, если еще раз из того, что вот это событие влечет объединение вот этих двух событий.
из вот этого события объединение этих двух событий следует
а вероятность введения событий превосходит в сумму вероятностей, но так как каждый из них стремится к нулю, то эта вероятность тоже стремится к нулю, что требуется.
Есть ли какие-то вопросы?
Хорошо, теперь в другую сторону доказываем.
Пусть теперь ксен фундаментально по вероятностям.
Для того, чтобы доказать, что у нас есть сходимость по вероятности, мне потребуется одна очень важная лемма, которую мы будем в дальнейшем использовать не только здесь, но еще и несколько раз.
Доказательств других полезных вещей.
Ну, кстати, это тоже известная теорема из теории меры, которую Иван Гейн, наверное, рассказывал.
О том, что если последность сходится по вероятности, из нее можно извлечь под последность, которая сходится почти на верное.
Но это простое упражнение, простое следствие лемма-барреля кантеля. Давайте я его быстренько докажу.
Если ксен стремится по вероятности кси, если ксен стремится по вероятности... Ой, господи, нет, не так я буду это формулировать.
Если, значит, межфундаментальность по вероятности нужна. Если ксен фундаментально по вероятности, то из нее можно извлечь под последность сходящуюся почти на верное.
Значит, существует под последовательность, ну, последовательность чисел НК такая, что ксен-к стремится, и какая-то случайная врачность на кси такая, что ксен-к стремится почти на верное кси.
Приказ стремящимся бесконечности.
Вот, ну, сперва эту лему докажем. У нее простое доказательство, которое к тому же использует лемма-барреля кантеля, которое мы с вами недавно изучили. Давайте быстренько ее с помощью леммы докажем.
Значит, ну, прям построим эту последовательность на самом деле. Возьмем N1 равной единице от любого к, начиная с двойки, положим Nk.
Nk это минимальное такое g больше либо равное, чем, строго больше, чем Nk-1.
Таким, что для любых R и S больше либо равных, чем g, вероятность того, что модуль кси R минус кси S больше, чем 2 в степени минус k, это вероятность меньше, чем 2 в степени минус k.
Ну, то есть понятно, что такое Nk просто найдется за счет того, что, на что я здесь делал, зафиксировал некоторые ексилом, ну, в каком-то странном виде 2 в степени минус k.
Так как последовательность фундаментально по вероятности это значит, что предел при R и S стремящимся бесконечности такой вероятности равен 0.
То есть вероятность имеется к 0 при R и S стремящимся бесконечности. Ну, значит, при достаточно больших R и S она будет меньше в частности, чем вот то, что эти штуки совпадают.
И они совпадают, это совпадение. Я мог и разными сделать, но мне удобно их выбрать такими. То есть вот это дельта, а вот это ексилом.
Вероятность того, что модуль кси R минус кси S больше, чем дельт стремится к 0, ну, значит, с некоторым моментом это вероятность меньше, чем такой ексил.
Вот, поэтому такие тахлопастинцы я смогу построить. Вот, и дальше надо заметить, что...
Что надо заметить? Надо заметить, что вероятность того, что
модуль кси Nk-1
больше, чем 2 в степени минус k-1
бесконечно часто
равна 0, так как... Ну, применяем млем барреля контель. Да, вот такая вероятность бесконечно часто равна 0, если мы знаем, что сумма
таких вероятностей конечна.
Она действительно конечна, так как мы так выбирали Nk-1.
Сумма таких вот вероятностей она
меньше, чем сумма 2 в степени минус k-1, что конечна.
Да, вот зачем мне нужна здесь, вот зачем мне вот в этом месте нужна геометрическая прогрессия, чтобы сумма сошлась.
Ну, я там мог 1,9k в квадрате написать, тоже бы сошлось, конечно.
Вот.
Значит, раз сумма ограничена, то
вероятность того, что модуль кси Nk-1 больше, чем эта штука бесконечно часто равна 0.
Теперь зачем мне вот здесь нужна тоже геометрическая прогрессия?
Последующей причине.
Значит,
рассмотрим событие
A
таких Омега,
что модуль кси Nk-1
больше, чем 2 в степени минус k-1 бесконечно часто.
Понятно, что вне этого события, то есть в любом Омега, который А не принадлежит,
вот это вот дело выполнено лишь конечное число раз, там 5, 17, 28, миллион, неважно сколько, но конечное число раз.
А значит, начиная с некоторого момента, будет меньше, чем равно.
А значит, сумма модулей
кси Nk-1
по всем k, да, 2 до бесконечности?
Конечно.
Раз начиная с некоторого момента, мы можем ограничить нашу последовательность геометрической прогрессии,
значит сумма конечна.
То есть вот такой вот ряд сходится абсолютно
а значит, если я модуль сниму,
а именно рассмотрю кси Nt,
кси Nt, mt скажем,
равная сумме по всем
равной чему?
равной кси N1
плюс сумма по всем k,
от 2 до m
кси Nk-1
вот эта штука будет сходиться прием стремящейся бесконечности.
Да, так как если я вот здесь вот бесконечность напишу,
то этот ряд будет сходиться до всего того, что он сходится абсолютно.
Значит, эта штука стремится к нектому кси
на каждой омеге.
Да, то есть вот я для любого омега и за счертой вот это могу утверждать.
Что найдется некоторое кси.
Такое, что вот эта штука стремится кси.
Ну все, это есть в общем-то искомая случайная величина.
То есть теперь положим для всех омегой из A
кси от омега равна нулю
и получим, что кси Nt стремится почти наверно кси, что требуется.
Таким образом, если есть какие-то вопросы,
то вероятность события равна нулю.
Можете наполнить?
Полеем баррель кантели.
Да, это в точности вероятность события.
Она равна нулю, так как полеем баррель кантели
мы можем это выяснить на основе того, что мы знаем, что вот такой ряд сходится.
Раз такой ряд сходится, то полеем баррель кантели
вероятность того же события выполнена бесконечно сейчас равна нулю.
Понятно, спасибо.
Окей.
Теперь мы из этой леммы должны выяснить, что это такое.
Теперь мы из этой леммы должны выяснить то, что нам нужно,
а именно почему из фундаментальности вероятности следует сходимость вероятности.
Ну вот мы уже предположили фундаментальность вероятности.
Значит, по нашей лемме мы теперь можем выделить подпосредственно сходящиеся почти наверно.
И вот эта кси, это и будет наша интересующая нас случайная верещина,
к которой по вероятности будет сходиться кси.
То есть осталось доказать, что ксиан сходится по вероятности кси.
Ну, давайте, это, наверное, уже просто.
Значит, давайте...
Сейчас.
Во-первых, исходимости, почти наверно, следует сходимость вероятности.
То есть мы знаем, что ксиан ката сходится и по вероятности кси.
Вот, поэтому давайте сделаем две вещи.
Во-первых, возьмем такое...
Каноль.
Каноль.
Каноль.
Каноль.
Что для любого к больше равно, чем каноль.
Модуль ксиан ката.
Вероятность того, что модуль ксиан ката минус кси.
Сейчас. Нет. Давайте знать, как сделаем.
Еще немножко сложнее.
Давайте напишем просто, что модуль ксиан ката и минус...
Вероятность того, что модуль ксиан ката и минус кси.
Больше равно, чем эпсалон пополам стремится к нулю.
Приказ стремящейся бесконечности.
Вот так вот напишем. Это с одной стороны.
А с другой стороны,
В силу...
Фундаментальности по вероятностям.
Мы знаем, что предел при n стремящейся бесконечности и n катом стремящейся бесконечности.
Ну, точнее k.
Правильно писать.
Предел при n стремящейся бесконечности и k стремящейся бесконечности.
Вероятность того, что модуль ксиан минус кси н ката, тоже больше Dust .
Да, первая сила лемма, вторая сила фундаментальности, вероятность.
вероятность и вероятность. Ну и все, а теперь возьмем и, как выше, обратим внимание, чтобы не
равнился треугольника, вероятность того, что mod xn – xi больше значим epsilon, не превосходит
вероятность того, что mod xn – xnkt больше значим epsilon пополам, плюс вероятность того, что mod xnkt
– xi больше значим epsilon пополам, так как обе вероятности стремятся к нулю, то и сумма тоже стремится к нулю, что и требуется.
Так, есть ли какие-то вопросы?
Хорошо, мы наконец-то с вами, в общем, поговорили всякие общие факты про сходимости, и можем
теперь непосредственно перейти к предельным теоримам. И первое, с чего мы начнем, это так
называемые усиленные законы больших чисел, сокращенно УЗБЧ, усиленный закон больших чисел.
Значит, у вас уже был закон больших чисел, и в нем, я напомню, фигурировала сходимость по вероятности.
Вот, значит, чем усиленный закон больших чисел означается от обычного закона больших чисел,
тем, что сходимость в нем почти наверно не по вероятности, тем, что сходимость в нем почти
наверно не по вероятности. Вот, ну давайте для начала сформулируем. У нас будет два усиленных
закон больших чисел. Усиленные закон больших чисел, они будут немного отличаться условиями.
То есть нельзя будет сказать, что один сильнее, чем другой, в одном будут одни условия, которые в каком
смысле будут сильнее, чем в другом, в другом будут другие условия, которые будут сильнее, чем условия
в первом. И вот начнем, назовем его УЗБЧ-1. Так называемый усиленный закон больших чисел для случайных
величин с ограниченной дисперсией. То есть здесь утверждается, что нужно использовать некоторую
ограниченность дисперсии в отличие от второго усиленного закона больших чисел, в котором вообще дисперсия
даже не обязана существовать. Итак, пусть ксиен независимые случайные величины.
И пусть у них есть дисперсия. Ну, есть конечно второй момент.
Они одинаково распределены? Нет, одинаковой распределенности не нужно.
Значит, пусть, кроме того, существует некоторая последовательность Бэнте, не отрицая положительных чисел,
которая не убывает. И в пределе равна бесконечности. Такая, что сумма по всем n дисперсии ксиен
соответственно Бэнт в квадрате, конечно.
Сходится вот такой ряд суммы дисперсии ксиен в квадрате.
Тогда, Sn минус мат ожидания Sn. Значит, Sn это сумма первых n.
Как обычно. Значит, Sn минус мат ожидания Sn поделительно Bn стремится почти, наверное, к нулю.
Давайте соотнесем это с обычным законом больших чисел, поймем, какие вообще Бэнты можно выбирать и что вообще происходит.
Для начала я напомню, что в обычном законе больших чисел совсем не обязательно нужна независимость.
Там нужна попарная независимость или более того, более слабые условия попарная некоррелируемость.
То есть, чтобы к вариации всех пар случайных причин были равны нулю. Здесь этого недостаточно, здесь нужна независимость.
В этом смысле он слабее, но этот закон силен именно с сходимостью почти, наверное, поэтому он называется усилием.
Теперь давайте попробуем соотнести знаменатели.
Вы помните в предположении, что все дисперсии ограничены? Мы и так формулировали усиленный вариант, так называемый.
Обычный закон больших чисел, там все одинаково распределены, в знаменателе n стоит.
Но мы потом с вами выясним, что если все дисперсии просто ограничены одним и тем же цепь, то в знаменателе можно поставить все что угодно больше, чем корень из n.
Ну вот здесь чуть хуже. Во-первых, если я в знаменателе корень из n поставлю, то здесь у меня это будет n.
Если я дисперсии просто меняю на константу, у меня получается сумма цепь 9 и трет расходится. То есть корень из n нельзя.
Ну и прямо чуть-чуть больше тоже нельзя. То есть если напишу корень из n log log log n, то такая штука тоже будет расходиться.
И вот она будет расходиться вплоть до того самого повторного логарифма и закона повторного логарифма, что неудивительно.
То есть если вы здесь напишете под корень повторный логарифм, то это будет как раз место, в котором у вас все еще будет расходиться.
Но если вы возьмете уже чуть больше, если тут будет n log в квадрате log n, то уже будет сходиться.
То есть как раз это log log n, это та самая граница, в которой закон больших чисел начинает или перестает переходить в усиленный закон больших чисел.
Есть лишь небольшой зазор, в котором закон больших чисел в этом смысле сильнее.
Сходимость по вероятности есть в знаменателе, когда есть что угодно растущее быстрее, чем корень из n, вплоть до корень из n log log n.
То есть для такого диапазона знаменателей у нас есть сходимость по вероятности, но не сходимость почти наверно.
А для более сильно растущих знаменателей уже будет сходимость почти наверно.
Ну вот давайте я для того, чтобы это было записано, напишу здесь какое-нибудь простое замечание.
Значит в частности, если все случайные величины одинаково распределены.
То, ну, например, sn минус от ожидания sn поделить на n, стремится почти наверно к нулю.
Ну и вообще здесь в знаменателе можно там, скажем, написать n в степени 1 вторая плюс дельта, для любого дельта больше нуля.
Напарено же можно еще сильнее уменьшать знаменатель, не только писать степень n, но, как я уже говорил, там можно писать корень из n умножить там на логариф, например, тоже будет правдой.
А одинаково распределенность я здесь написал просто для того, чтобы все дисперсии стали одинаковыми.
То есть вот эти числа становятся одинаковыми, они на расходимость этого ряда перестают влиять.
Что еще раз?
Дисперсии должны существовать, да?
Да, ну и когда я пишу в частности, я имею в виду, что все это выполнено. То есть мы в теории уже написали, что есть второй момент.
Конечно, да, если дисперсии нет, то мы не сможем применить теорию.
Да, существование дисперсии здесь по существу и все доказательства опирается именно на переход отсюда-сюда.
То есть вот из этого нужно сделать вот этот вывод. То есть от сходимости дисперсии нужно перейти к сходимости случайных величин.
То есть в каком смысле сходимости V2 нужно перейти к сходимости почти наверно.
И мы сегодня не успеем этот теориям доказать, но я начну рассказывать всякие утверждения, которые будут для этой теории применяться.
Еще раз ключевой момент. Нам нужно научиться переходить от сходимости вторых моментов к сходимости случайных величин.
Ну вот непосредственно этот переход мы, наверное, в следующий раз осуществим.
А сейчас я начну доказывать всякие вспомогательные утверждения для того, чтобы этот переход осуществить.
Вот этот переход от сходимости дисперсии к сходимости почти наверно называется теориям Калмогорова и Хинччина о сходимости рядов.
И мы его с вами докажем в следующий раз. Сегодня хотя бы надо сформулировать.
Успеем, но может даже сформулировать не успеем, там еще перед ним нужно 2 леммы доказать.
Значит, сперва докажем вот такую лему.
Ну тоже такого своего рода криперисходимости почти наверно.
Значит, ксиен фундаментально почти наверно.
Тогда и только тогда, когда для любого эпсилум больше нуля предел при n-стримящемся бесконечности,
вероятности того, что супремум по всем k больше чем равным единице модуль кси n плюс kt
минус кси nt больше чем эпсилум с триммерным пределами нуля.
Но вот это некоторый очень полезный способ записывать сходимость почти наверно,
которую мы будем использовать, когда как раз будем переходить от сходимости дисперсии к сходимости почти наверно стученных величин.
Так, давайте эту лему докажем.
Значит, во-первых, переформулируем ее немного.
Доказывать вот явно вот это не очень понятно как, но сейчас мы напишем равносильную вещь.
Ее будет доказывать проще. Значит, смотрите.
При делу такой вероятности равен нулю,
тогда и только тогда, когда предел при n- и m-стримящемся бесконечности,
то есть предел при н- и м-стримящемся бесконечности,
то есть предел при н- и м-стримящемся бесконечности,
то есть предел при н- и м-стримящемся бесконечности,
то есть предел при n- и m-стримящемся бесконечности,
нет, мы написали предел при n-стримящемся бесконечности, а m внутрь, извините.
Значит, предел при n-стримящемся бесконечности вероятность того, что suprem по k и l
больше равном чем n, модули того, что xk-xl больше чем ε, равна нулю.
Почему это одно и то же? Ну понятно, что понятно.
Понятно, что сейчас, если вот это равно нулю, то и это равно нулю.
То есть вот справа налево понятно,
потому что вот этот suprem он меньше либо равен, чем вот этот suprem.
Здесь suprem вообще по всем k и l больше чем n, здесь каким-то специальным.
То есть в роли l и n зафиксировали, а вот k уже всевозможны.
Но на самом деле есть пределы одинаковые, просто потому что не раньше треугольник выполнен.
Да, если вы знаете, что модуль xn плюс k1 минус xn больше чем,
меньше либо равно, чем ε пополам, и модуль xn плюс k2 минус xn меньше либо равно, чем ε пополам,
то из этого следует по нераде с треугольника, что модуль xn плюс k1 минус xn плюс k2 меньше равно, чем ε.
Да, поэтому вот вы взяли какие-то k и l больше чем n.
Вот здесь поставили кс и ка и посмотрели на такую вероятность.
Поставили кс и l пополам, и потом кс и l и посмотрели тоже на такую вероятность.
Обе стремятся к нулю, а значит такая вероятность будет тоже стремиться к нулю.
Да, то есть поэтому слева направо тоже верно по нераде с треугольника.
Вот, поэтому эта вещь равносильная.
А значит вместо того, чтобы вот это доказывать, вместо того, чтобы первую вещь доказывать, будем доказывать вторую вещь.
Это просто. Итак, поехали.
Значит, кс и n фундаментально почти наверное.
Это что значит по определению?
Это значит, что вероятность того, что кс и n фундаментально равна единице.
То есть вероятность того, что для любого x больше нуля.
Существует такое n0, что для любых k и l, чтобы у него там k и l, существует такое пусть n, что для любых k и l больше равных чем n.
Модуль кс и k минус кс и l меньше n, чем epsilon равна единице.
Ну давайте возьмем вероятность отрицания этого события, который будет равнулю.
То есть вероятность того, что существует epsilon больше нуля.
Такое, что для любого n найдутся k и l больше равные чем n.
Такие, что модуль кс и k и t минус кс и l и t больше чем epsilon равна нулю.
Дальше мы можем перейти, так как вот эти события вложены по n, вот эти события вложены по epsilon.
То есть чем меньше epsilon, тем меньше события.
Чем больше событий.
Чем меньше epsilon, тем больше событий.
То есть они расширяются.
То мы можем вот это вот существование заменить не на квантор по континуальному множеству, а на квантор по счетному множеству.
И написать это как существует такое m натуральное.
И потом вместо epsilon написать 1 m.
Стандартный трюк.
И это уже вероятность объединения счетного количества множеств.
Вероятность объединения счетного количества множества равна нулю, тогда это тогда, когда вероятность каждого из них равна нулю.
То есть это означает, что для любого n.
Вероятность того, что для любого n существует k и l.
Хотя бы n, такие что модуль kс и k и t минус kс и l и t больше чем 1 m, то это равна нулю.
это мог проделывать и для изначальной задачи, то есть для такого вида супремума, проблема
возникла на следующем шаге. значит смотрите, следующий шаг вот какой. вот теперь я смотрю
вот на эти события, которые зависит от n, давайте это обозначим. и это события вложены. чем больше
n, тем шире события, тем уже события вложены. чем больше n, тем меньше диапазон для поиска этих
каэль, тем уже события. и если бы я взял изначальный вариант, я бы этого не мог убеждать,
потому что у меня бы одно было бы любое k, а второе было бы n, и поэтому они были бы уже не вложены,
то есть диапазон для поиска второго аргумента у меня бы каждый раз просто менялся, не уменьшался,
а менялся. то есть у меня каждый раз n был бы фиксирован, поэтому я не мог говорить про вложенность
событий. а эти события вложены, и поэтому я могу перейти к пределу. потеремие непрерывности
вероятности меры это верно тогда и только тогда, когда предел при отстремляющемся бесконечности,
вероятность того, что существует k и l больше либо равной чем n, модуль кси kt и минус кси lt
больше чем 1mt равен нулю. но самое время вернуть epsilon. да, опять же, так как чем больше m,
тем больше это вероятность, тем, соответственно, больше это предел, то, раз я могу это делать,
эту величину сколь угодно малой, то я могу брать произвольно epsilon, и это в общем подтверждение
будет разносить. тогда и только тогда, когда любого epsilon больше нуля, предел при отстремляющемся
бесконечности, вероятность того, что найдется k и l хотя бы n такие, что модуль кси kt и минус
кси lt больше чем epsilon равен нулю. вот, а это ровно то, что нужно доказать, ну точнее вот это.
да, то есть вероятность того, что найдется k и l такие, что модуль кси kt и минус кси lt больше
чем epsilon, это в точности вероятность того, что supremo больше чем epsilon. ну и давайте
перепишем это в окончательном виде, на чем и закончим заказать стемлеммы.
так, тогда и только когда любого epsilon больше нуля, предел вероятности того,
что supremo пока и l больше равным чем n, модуль кси kt и минус кси lt больше чем epsilon равен нулю.
Важный вопрос, а получается мы тем, что события вложены для того, чтобы пределить эти?
да, да, именно так, вот для того, чтобы воспользоваться тремой непеременностью вероятности мира.
вот у нас есть пересечение, для любого этого пересечения, у нас есть пересечение событий.
если они не вложены, то говорить о том, что вероятность пересечений равна нулю, тогда
только на пределы равны нули, неправильно, но возьмем, пусть одно из них в общественной
не пересекает. вот есть куча событий пересекающихся, одна вообще не пересекать. ну и тогда, конечно,
вероятность пересечений равна нулю, хотя у них у всех может быть какая-то большая вероятность,
и она может с этим не сходиться. чтоб возьмем все события одинаковые, они имеют вероятность
одна вторая, и другое тоже одна вторая, который является
дополнением до них. Вероятность перечечения ноль, а предел
вероятности одна вторая. Вот, поэтому такой переход
можно сделать именно за счет теремы непрерывности,
то есть за счет того же события вложения.
Ну вот, значит, следующая лемма, так называемая неравенство
Калмогорова. После того, как мы докажем эту лему,
мы сможем наконец формулировать терему Калмогорова и Хинчина
сходимость рядов, с которой уже споследует усиленный
Неравенство Калмогорова. Значит, пусть есть последовательность
независимых, не обязательно одинаково распиленных,
просто независимых случайных величин. И для удобства мы
всегда будем центрировать, то есть вот у нас в усиленном
законе больших чисел вычтена сумма матожедания. То есть
мы центрировали случайные величины, то есть это было
бы равносильно такой формулировке, что сразу предполагали,
что пусть у нас есть независимые случайные величины с матожеданием
равным нулю. Да, вот мы могли это в условиях предположить,
а здесь бы не вычитать. Это было бы равносильно, потому
что мы любую случайную величину, у которых есть матжедание,
вот таким вот образом можем центрировать. И для удобства
мы давайте будем сразу везде центрировать. То есть это
не меняет общность утверждения, но просто сокращает запись.
Независимые случайные величины с матожеданием к ситам
равным нулю. И пусть у них тоже конечные
вторые моменты, как и в формулировке усиленного
закон больших чисел, тогда, во-первых, для любого
Эпсилона больше нуля, вероятность того, что Супрэмун, пока от
единицы до Н, модуля скатая, вероятность того, что модуля
скатая, так вероятно, Супрэмун модуля скатала
больше Н, чем Эпсилон, меньше равна, чем матожедание
модуля снт в квадрате 9m в квадрате.
Но я хочу, кстати, отметить некоторую похожесть данной
неравенства Маркова. То есть если бы вы тут убрали
Супрэмун, то это была бы точность неравенства Маркова.
Когда вы возводите обе части в квадрате, берете матжедание,
у вас получается неравенство Маркова. Или неравенство
Чебышова, что одно и то же. И именно его вы применяете
для доказательств закона больших чисел. Для доказательств
закона больших чисел вам достаточно этого неравенства.
То есть вы берете в момент N и смотрите на вероятность
того, что ваш модуль снт Эпсилон превосходит. А здесь, помните
отличие сходимости по вероятности от сходимости почти наверно.
Вот пример с бегающим отрезком. Мера отрезка уменьшается,
но так он бегает, то поточной сходимости нет. Через любую
омегу он будет пробегать бесконечно много раз. Но
чтобы этого избежать, вам нужно смотреть не на сходимость,
а на равномерную сходимость. То есть вам для этого этот
Супрем здесь и нужен. То есть чтобы перейти от сходимости
по вероятности к сходимости почти наверно, совершенно
естественно написать вот этот Супрем. И оказывается
все то же самое будет верно. Аналог неравенства Маркова,
только его более сильная версия, когда мы максимум
написали, тоже будет верно. Хорошо, значит и второе.
Нам второе не понадобится непосредственно для доказательства
усиленного закона больших чисел, но вот нам понадобится
это для доказательства теоремы Колмогорова и Хинчна
сходимости рядов в более общем виде, которая интересна
сама по себе в общем-то. Поэтому вторую часть тоже
сформулируем. Значит если найдется такой Эпсилон больше
нуля, что для любого И модуль ксии та меньше, чем
С, то есть если последность случайных величин равномерно
ограничен некоторой константой, то тогда вероятность того,
что этот максимум больше равна чем С, больше равна
чем Эпсилон, больше либо равна чем 1 минус С плюс
Эпсилон в квадрате в пределительном от ожидании модуля СН в квадрате.
Обратите внимание, что первое неравенство, интересно
когда вот это моджедание маленькое. И тогда можно
утверждать, что это вероятность тоже маленькая. Если вот
эта штука стремится к нулю, то и вот эта вероятность
тоже стремится к нулю. Наоборот второе, интересно, когда
моджедание большое. С одной стороны если моджедание
маленькое, то ну и супраемом типа не может быть большим.
С другой стороны, если моджедание большое, то
супраемом В 것 может быть большим. То есть если
здесь structures стремится к бесконечности, то тогда
вся эта разность стремится к единице, и значит тогда
вероятность это тоже стремится к единице.
Разные утверждения, которые могут помогать доказать
Exhale с одной стороны, что когда моджедание большое
случайная величина большая, с другой стороны когда
моджедание маленькое, случайное величина маленькая, то
То есть вполне себе может быть полезно для доказательства перехода от сходимости дисперсии
к сходимости случайных величин. Хорошо, давайте доказывать.
Сперва первое. Ну давайте вот наше событие, которое нас интересует, обозначим за. То есть это
множество таких омега, что супремум, то есть для всех k от единицы до n. Найдется k,
извините. Найдется k от единицы до n, что модуль s-кат и больше, чем осьмум. Понятно,
что это есть дизъюнкное объединение событий аккаты, где аккат единицы до n. Дизъюнкное
значит, что это множество не пересекаются. Где аккаты это множество таких омега, что вплоть до
катова меньше, чем эпсилум, то есть с1, модуль с1 это омега меньше, чем эпсилум, и так далее.
Модуль sk-1 это омега меньше, чем эпсилум, а вот модуль s-кат это омега уже больше,
чем осьмум. То есть события аккаты показывают, что k это первый момент, когда больше, чем эпсилум.
Ну, если мы предполагаем, что такой момент существует, то тогда мы получаем что-то событие
от в точности объединения таких событий. Вот, хорошо. Ну, кстати, эти определения будут общие
для пунктов 1 и 2, поэтому давайте сначала их дадим, а теперь перейдем к пункту 1.
Мат ожидания sn в квадрате, конечно, больше броно, чем мат ожидания sn в квадрате на индикатор a.
И, значит, так как a это дизъюнкное объединение, то индикатор a это сумма индикаторов аккат.
Дальше давайте представим sn как, во-первых, занесем его под знак суммы и вычтем и прибавим
sk, то есть напишем sn-sk плюс sk в квадрате. Индикатора k, сумма pk. Вот, дальше квадрат суммы запишем.
sn-sk в квадрате плюс 2 sn-sk на sk плюс sk в квадрате.
Теперь по линейностям. Это есть сумма pk, мат ожидания sn-sk в квадрате на индикатора k,
плюс сумма pk, 2, да, мат ожидания от sn-sk на sk на индикатора k и плюс сумма pk,
мат ожидания sk в квадрате на индикатора k. Вот, значит, и это мы хотим ценить ограничить снизу.
Значит, вот эта штука неотрицательна. Понятно. Ну, потому что это в квадрата, это тоже неотрицательно.
Поэтому отжижение от неотрицательности членовеличины неотрицательны. Просто выкинем его.
Значит, дальше в силу независимости случайных величин, вот это вот sn-sk, это же сумма ксишек с k
плюс 1 до n. Они от первых k не зависят. Да, то есть вот эта штука и вот эта штука независимы.
А поэтому можно представить, как произведение мат ожиданий. Мат ожидания от sn-sk умножить
на мат ожидания от sk на индикатора k. Вот, то, что касается последнего слагаемого,
мы знаем, что на аккатом, если выполнено событие аккаты, то sk по модулю, хотя бы эпсилон,
а значит снизу можно в последней сумме написать вместо мат ожидания эпсилон в квадрате на
вероятность акката. Да, так как у меня на акката эти случайные величины хотя бы эпсилон,
то соответствующим образом я могу этому ожидания ограничить. Вот это вот место, где мы применяем
такой же абсолютно, такую же стратегию, как предоказательство нерайства Маркова. Там мы для
одного sn вот это сделали, а тут мы, ну похитрее, мы sn представили в виде вот таких вот сумм, sn
представили в виде вот таких вот частей трех. И вот только для третьей части мы это сделали,
мы sk ограничили этим эпсилоном. Можно вопрос? Да. Вот там где вы фиолетово написали во второй
снизу строчки. Почему sn-sk независимо с sk на индикатор аккаты? Ну индикатор аккаты определяется
первыми к случайным величинами. То есть это по определению вот такое вот событие. Это событие,
которое зависит от первых к случайных величин, а от остальных n-k независит. Это есть функция от
ks1 до ksk. Окей? Да, понятно. Вот. Значит, так как у меня случайно величины центрированы,
то есть когда так у них мат ожидания ноль, вот эта полинейность, это сумма мат ожиданий. Мат ожиданий
ksk плюс первая, плюс тогда ли мат ожиданий ksn, то они все равны нулю, значит эта штука ноль. И остается
просто в точности epsilon в квадрате на сумму по k вероятностей аккатых. Но это вероятность a. У меня
a представляется как дизюмбная сумма вот этих вот аккатов. Получаем epsilon в квадрате на вероятность
a. Откуда требует, откуда следует непосредственно требуемая вероятность a к меньшему ночью мат
ожидания sin в квадрате. Так, есть ли вопросы? Ну хорошо, есть еще четыре минуты. Успеем,
но может быть на пару минут задержимся второй пункт доказать и на этом закончим. Теорему о
сходимости рядов уже в следующий раз. Значит, теперь хотим мат ожидания sin в квадрате ограничивать
снизу, сверху и сделаем это так. Значит, меньше броно, чем мат ожидания см в квадрате на индикатор
a. И плюс, ну давайте напишем сперва вот такое точное равенство, плюс мат ожидания см в квадрате
на индикатор a с чертой. Но на множестве a с чертой мы точно знаем, что у нас максимум меньше,
чем эпсилон, а значит в частности см в квадрате тоже меньше, чем эпсилон. Поэтому мы можем это
ограничить как мат ожидания см в квадрате на индикатор a и плюс эпсилон в квадрате на вероятность
a с чертой, то есть на 1 минус вероятность a. Вот, прекрасно, теперь снова работаем с мат
ожидания см в квадрате на индикатор a. Ну, как и выше, представляем a как сумму индикаторов
окатых. Заносим снова см внутрь, представляем его как см минус ск плюс ск, делаем в точности то
же самое и получаем, значит, просто перепишем, мат ожидания см минус ск в квадрате на индикатор
окатые, сумма пока, плюс мат ожидания, два мат ожидания от суммы пока на см минус ск на ск на
индикатор окатые и плюс сумма пока на мат ожидания ск в квадрате на индикатор окатые.
Вот, значит, как и выше, вот эта штука просто новая.
Значит, и у нас остается сумма пока мат ожидания от, я напомню, что см минус ск это ск
плюс 1, плюс и так далее, плюс ск в квадрате на индикатор окатые и плюс сумма пока мат
ожидания ск в квадрате на индикатор окатые. Значит, смотрите, дальше такая мысль,
модуль ск меньше либо равен чем модуль ск-1 и плюс модуль ск. Вот, да, модуль суммы не превосходит
сумму модули, а на событии окатом, я напомню по определению, все случайные величины вплоть до
к-1 меньше чем эпсилон. То есть, если мы предполагаем, что окатая верно, то вот эта штука меньше
чем эпсилон. Еще во втором пункте мы предполагали, что все случайные величины меньше чем с. То есть,
вот эту штуку мы можем оценить как эпсилон, а вот эту мы можем оценить как с. Все в сумме мы
получаем меньше либо равно чем эпсилон плюс с на окатом. Да, то есть, когда мы будем смотреть
вот на эту штуку, мы можем от ожидания ск в квадрате ограничить как эпсилон плюс с в квадрате.
Итак, собираем все вместе. Мат ожидания ск в квадрате меньше либо равно. Сперва вот эта
штука. Значит, вот это слагаемое у нас еще осталось. Но здесь силу независимости получаем сумму по к.
Мат ожидания вот этого ск плюс 1, плюс так далее, плюс ск в квадрате на вероятность окатая.
Квадрат не туда поставил. Вероятность окатая. Дальше вот эту штуку мы оцениваем как с плюс
эпсилон в квадрате на сумму вероятности окатая. Еще остается плюс эпсилон в квадрате на 1
минус вероятность окатая. Ну вот еще вот это у нас есть. Ну это понятное дело, это понятное
дело меньше 0, чем от ожидания ск в квадрате. Смотрите, мат ожидания от ск плюс 1, плюс так далее,
плюс ск в квадрате. Если вы это возведете в квадрат, то получите там всякие удвоенные
произведения, но они все нулю равны. Мат ожидания от удвоенных произведений равны нулю в силу
независимости в силу того, что мат ожидания каждой случайной величины это 0. Поэтому это просто
мат ожидания, сумма мат ожидания квадратов. А это меньше темра 0, чем сумма мат ожидания квадратов
вообще всех случайных величин, начиная с 1. Что в точности мат ожидания см квадрат?
да, значит окончательно мы приходим вот к такой формуле, с которой нам просто вероятность а
останется вытащить. мат ожидания с н квадрат меньше чем равно, чем мат ожидания с н квадрат на
вероятность а, плюс с и плюс епсимум в квадрате на вероятность а, плюс епсимум в квадрат на 1
минус вероятность а. ну теперь отсюда выцепляем вероятность а и получаем что нам больше либо равна,
чем мат ожидания с н в квадрате минус епсимум в квадрате и делить на мат ожидания с н в квадрате
плюс ц плюс р в квадрате и минус епсимум в квадрате что в точности есть 1 минус ц плюс
witnessing в квадрате под
так как вот эта штука положительна, можем это еще чуть-чуть ослабить и получить искомую
оценку 1-c++ в квадрате по зрителям ожидания c++ в период.
Ну, собственно, все. Есть какие-то вопросы.
Где-то вообще использовали, что в окатом окатый элемент больше, чем эпсилон?
Нет, здесь мы этого не использовали, нам уже не равенство другой стороны.
Да, мы здесь это мы используем в предыдущем, а здесь мы пользуемся вот этим.
Предыдущем где? В пункте 1.
Да, где? В пункте 1 мы этим пользуемся вот здесь.
Видите, вот больше либра, но здесь эпсилон возник.
Это из-за того, что на окатом скат в квадрате больше, чем в окатом квадрате.
Спасибо, спасибо.
Да.
Так, есть еще какие-то вопросы.
Если вопросов больше нет, то на этом все. Всем спасибо. До встречи в следующую субботу. До свидания.

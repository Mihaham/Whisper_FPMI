Значит, было теорема, существование процесса кси-тэ
с независимыми превращениями и начальным распределением q0
и данными распределениями qst
значит разностей qt-qs
равносильно вот такому тождеству
вот такое тождество отвечает за то, что есть процесс с данными распределениями
смотрите, кто у них даны
даны изначально меры вот эти
и начальное распределение
и хотим чтобы был процесс с независимыми превращениями
у которого разности
смотрите, не распределение кси-тэ самих даны
а даны распределение разностей
и когда такой такой процесс есть
вот в терминах характеристических функционалов
вот этих вот мер
а если хотеть без функционалов, а в терминах самих мер
то это свертка
в терминах мер
это можно переписать вот так
вот так можно переписать
смотрите, это дело вкуса
если кто-то не любит характеристические функционалы
ну вот, пожалуйста, в терминах мер
если кто-то считает, что свертка это что-то подозрительное
и было когда-то давно
а вот проще перемножать функции, то вот так
но, в общем, это одно и то же
доказательства
в одну сторону все очень просто
смотрите, как будет в одну сторону
давайте, чтобы не отходить от обозначений конспекта
в особенности с этими...
но как обозначать моменты времени, конечно, совершенно безразлично
а тут какое правило?
этот первый, этот второй
этот второй, этот третий
вот, значит, правило простое
так, значит, смотрите
значит, можно вот так написать
вот так можно написать разность
значит, вот эти две случайные величины независимые
у независимых распределения есть свертка
или что-то то же самое, что вот то, что написано
в одну сторону совсем просто
нужно вспомнить, что у независимых случайных величин
когда складываете, то преобразование фурье
характеристическая функция распадается в произведение их характеристических функций
в другую сторону чуть менее очевидно требуются некоторые вычисления
в другую сторону
значит, пусть тождество выполнено
значит, ищем
значит, ищем процесс с помощью следствия теоремы Колмогорова
значит, теорема Колмогорова
использует преобразование фурье не двухточечных распределений
а всевозможных конечных
значит, нам нужно из двухточечных произвести
значит, давайте напишем
нужно из двухточечных распределений
устроить те, которые требуются в теореме Колмогорова
то есть их характеристические функции нужно устроить
значит, как я их тут обозначал
пусть сначала начальное распределение
дирак в нули
то есть мы в начале строим процесс в нулевым
значит, сначала ищем
ищем с нулевой стартующей точкой
как мы это делаем?
ну давайте сделаем так
значит, при выбранных точках
вот это Pt1tn возьмем как образ произведения
значит, вот такого вот
Qt1 на Qt2
ну Qt1 это 0t1
t1t2
и так далее
Qtn-1tn
значит, вот это распределение у нас есть
при линейном отображении
линейное отображение вот какое
y1 есть x1
значит, y2x1
плюс x2
значит, и так далее
yn есть x1
и так далее
плюс xn
ну почему это естественно?
ну потому что
если бы у нас уже было
вот такое распределение нмерное
то как бы мы его могли получить?
ну мы посмотрели бы
вот давайте это я просто в качестве пояснения
тут напишу как бы в скобках
вот представьте себе, что у вас есть
ну что вы хотите найти вот такое распределение
но компоненты не являются независимыми
у процесса превращения независимы
но не сами компоненты
поэтому вот нельзя просто взять
чтобы получить вот это нмерное распределение
распределение нельзя просто перемножить
вот эти распределения
вот другое дело
когда у нас вот такое распределение
из разностей
вот эти компоненты независимы
а у этого вектора распределение считается очень просто
нужно перемножать распределение компонент
но теперь мы смотрим
как получается распределение этого вектора
из распределения вот этого вектора
ну смотрим как этот вектор получается из этого
он получается линиейным преобразованием
вот тем
поэтому его распределение
есть распределение
есть образ вот этого распределения
при том линии нам
операторы, а сейчас у нас пока распределений нет, но мы, так сказать, имитируем вот это
построение, как будто бы они у нас были, если бы они у нас были, то мы должны были бы вот так
поступить, а распределений пока нет, но образы, мер у нас есть, вот у нас получились образы
этих мер, значит, мы из двухточечных распределений путем из перемножений и линейных
преобразований настроили вот такие вот распределения, но теперь нам надо проверить,
да, это сейчас еще небольшое пояснение, что будет, когда t1, значит, смотрите, тут у нас t положительные,
но ведь мы должны строить еще такие распределения, когда t1 еще и нулю может быть равен,
значит, если t1 равняется нулю, а дальше они, значит, идут как-то, то в качестве вот такого,
так берем уже построенную меру, берем меру для точек на одну меньше, так на rn,
так минус один, значит, вложенную в rn, считаемую как мера на rn, рассматриваемую как мера на rn,
ну мы как бы игнорируем, вот это, поскольку мы сейчас в ситуации находимся, когда x0,0, то ну вот
мы как бы вот эту первую нулевую игнорируем, так, значит, таким образом у нас есть вот эти вот меры,
так, значит, что должно быть, значит, какое у нас нужное условие согласования,
значит, нужное условие согласования,
значит, давайте я его выпишу, нам нужно следующее, давайте сначала посмотрим,
что у нас получается с характеристическими функционалами, значит, характеристический
функционал, значит, образы меры при линейном отображении, отображение l считается так,
значит, фи-мю от у есть фи-мю, простите, образ, вот этот образ при l в минус 1,
значит, это будет фи-мю, а, значит, исходный характеристический функционал, но в него надо
подставить вместо y сопряженный, ну почему так, ну это просто из определения, значит, смотрите,
значит, по определению, по определению вот так считается фурье, значит, по определению образа
меры, значит, по формуле замены переменных это получается вот это, так, ну и остается,
остается теперь вот это переписать вот так, значит, ну здесь перекинут l сюда со звездочкой,
так, вот, вот откуда, значит, вот откуда появилась эта звездочка, так, значит,
поэтому мы можем найти фурье, вот эти, ну вот характеристические функционалы этих вот мер,
значит, для, ну еще, еще, значит, замечаем, для той меры, которая еще не преобразовывалась,
значит, характеристический функционал вот такой вот,
значит,
на векторе y1, yn есть произведение, так, поскольку, значит, это произведение мер, то, значит, смотрите,
что получается, значит, характеристических функционал, значит, получается вот такой,
значит, вот так он будет записываться, так, а значит, что будет с образом,
так, что будет с образом, значит, характеристический функционал образа, так, значит, какой будет,
значит, вот сюда, значит, видите, видите, вот эта штука функция от y1, yn, так, и нам теперь вместо
y1, yn надо подставлять, значит, мы вместо y вот там справа, видите, должны подставить l звездочка от y,
так, значит, как выглядел l, мы видели, так, значит, как l звездочка получится, ну, или звездочка нужно,
ну, там матрицу транспонировать, так, и поэтому, значит, ну, вот ответ будет такой, это будет,
значит, давайте я выпишу порядки возрастания числа, ну, то есть, как бы, с конца,
значит, это откуда так получилось, ну, это получилось проверкой, значит, во что превратится вот этот,
значит, вот получился такой ответ, а теперь, значит, что нужно проверять в терминах,
значит, условия согласования, обратите внимание, поскольку мы условия согласования проверяем
ситуации, когда упорядоченные по времени наборы t-шек, так, то у нас остается одно согласование по проекциям,
оно пришло из вот этого условия Колмогорова, что когда вы rn проектируете на там rk, так, то у вас,
ну, образ должен быть какой нужно, значит, как мы помним, это сводится к проектированию в пространство на единицу
меньшей размерности, так, то есть, значит, что нужно, какие проекции нужно рассматривать, вот у вас есть rn,
но rn нужно проектировать не только на rn-1, ну, вот такое буквальное, которое отбрасыванием последней координаты получается,
но на все rn-1, которые получаются отбрасыванием любой из координаций, первый по n, по последнюю, так,
значит, что, ну, вот когда у нас там было условие в терминах характеристических функций, значит, вот что нужно,
значит, вот что нам нужно, значит, вот этот ответ я пока сохраню, так,
значит, нам нужно следующее, значит, ограничение, ограничение, значит, этого
на подпространство, где м-тое координата 0 должно совпасть, значит, смотрите с чем, должно совпасть,
значит, с характеристической функцией вот такой вот, где вот этой, ну, где вот этой нет, давайте я вот так напишу,
вот так, ну, давайте смотреть, значит, давайте их слечим, значит, что будет,
значит, что будет, вот пусть эта звездочка будет, так, значит, подставим ym равное нулю в звездочку,
значит, смотрите, что получится, получится следующее, ну, вот, ну, я, естественно, считаю, что m оно не обязательно крайнее,
ну, где-то там в середине m, значит, если m, если m это первое или последнее, ну, все будет, в общем, очень похоже,
но давайте не будем разбирать эти случаи, а такой вот разберем, так сказать, наиболее характерный случай, когда m не крайнее,
то тогда смотрите, значит, что получится, значит, шли, шли, шли, значит, с yn, так, значит, дошли до, смотрите, докуда дошли,
значит, когда появится первый раз здесь, ну, вот это вот m, давайте посмотрим, в какой первый раз оно тут появится, так,
значит, ну, в какой, так сказать, в какой комбинации оно появится, ну, вот, смотрите, значит, будет, давайте я напишу, y будет n,
значит, плюс y, n, минус 1, значит, плюс и так далее, а y, m, плюс 1, плюс y, m, так, вот так оно появится, да, вот оно первый раз появилось, значит, какое это будет момент?
значит, здесь будет t, m, минус первое, t, m, вот так, так, вот он этот первый момент, да, значит, вот мы его локализовали,
но его на самом деле нет, это я написал, просто чтобы было видно, куда мы ноль подставляем, так, а значит, ну, а дальше что будет?
значит, следующий, ну, вот оно появилось, значит, первый раз оно появилось, так, а значит, и что дальше, какой будет следующий, ну, следующий будет вот какой, t, m, t, m, плюс 1, так, а?
сейчас, сейчас, сейчас, сейчас, м, да, не так, не так, не так, не так, t, m, минус 2, t, m, минус 1, вот так, да, вот так будет,
и, значит, смотрите, что здесь будет, здесь будет, ну, y, m, значит, плюс и так далее, плюс y, m, вот этот же, да, ну, которого на самом деле нет, и плюс y, m, минус 1, так, ну, и пошли какие-то дальше, да.
нет, нет, почему же она такая сильная, она такая совсем не диагональная, она такая сильно треугольная, ну, ну, вот у нас, смотрите, какое было преобразование,
y1 равен x1, y2 равняется x1 плюс x2, ну, и так далее, то есть там половина, ну, так сказать, она такая целая треугольная, там много единиц, она диагональная довольно далека, ну, там, грубо говоря, половина единицы, половина нулей, так.
значит, смотрите, тут какая была структура, шел последний, так, вот здесь, потом последний плюс предпоследний, так, ну, и потом, ну, и тут начинают добавляться, добавляться, и последний выстраиваются все, так, вот такая была структура.
теперь мы вот в эти, вот в эти, подставляем ym равное 0, так, и он задействован, ну, он задействован, вот первый раз он задействован вот здесь, так, значит, вот тут, значит, стоят какие-то произведения, значит, вот он первый раз здесь задействован, да, а потом дальше он все время, ну, дальше он опять все время задействован, так, значит, вот это все время задействован, да, а потом дальше он все время задействован, так, значит, вот это все время задействован, так, значит, вот это все время задействован, так, значит, вот это все время задействован, так, значит, вот это все время задействован, так, значит, вот это все время задействован, так, значит, вот это все время задействован, так, значит, вот это все время задействован, так, значит, вот это все время задействован, так, значит, вот это все время
вот что получилось, так, а теперь, значит, теперь, значит, а теперь, значит, смотрите, значит, смотрите, что, что, что получилось,
значит, тут получилось вот я еще зря, зря я не выписал, значит, зря я еще не выписал вот этого предыдущего,
значит, смотрите вот тут появилось вот такое вот, значит, вот тут появилось вот такое вот слагаемое,
Вот тут появилась вот такая вот слагаемая, ну не слагаемая, сомножитель такой появился, но давайте напишем, кто перед ним стоял.
Перед ним стоял вот кто, t, m, т, м. Сейчас, только я нехорошо тут написал, у меня наоборот должно быть.
У меня сначала меньше стоит, потом больше.
Значит, здесь, смотрите, кто стоит перед этим.
Значит, перед этим стоит, значит, вот кто, t, m, t, m плюс один, вот так вот, y, m и так далее, плюс y, m плюс один.
А и он умножился вот на этого, значит, y, m и так далее, плюс y, m плюс один, потому что вот этот исчез.
Значит, вот это у них одинаковые. Вот эти у них из-за того, что у этого y, m не исчез, вот эта точка стала общая у этих двух сомножителей.
А у них у каждого, ну какая-то своя сумма, они вообще говоря разные.
Но когда y, m занулился, оказалось, что у них стоит одинаковый аргумент, но у нас есть тождество, значит, по нашему тождеству.
А как только что-то вычисляется вот такого рода, видите, то смотрите, что происходит.
Значит, происходит сокращение сомножителя.
Видите, вот этот t, m промежуточный, он пропал, и это оказалось равно вот чему.
Значит, было, тут стояло два сомножителя, но когда мы подставили y, m равное нулю, эти два сомножителя свернулись, и получился один сомножитель.
И, значит, поэтому это равно тому произведению, которое соответствует...
y, m, значит, и так далее до последнего вот этого t,
значит, какое там у нас последнее, 0, t1, значит, от суммы вот этой y, m, значит, и так далее, без вот этого.
То есть это как раз получается характеристическая функция вот этого вот набора с исключенным m.
Видите, для набора с исключенным m вот будет ну такого же сорта произведения, но множители на единицу меньше.
Точки, вот эти все те же самые, у них, смотрите, когда y, m равно нулю, у них все множители просто одинаковые,
значит, у них все множители одинаковые, кроме того, который зацеплял y, m, потому что у того, у этого такого не было с этой точкой.
Для него, видите, сразу вот в этом наборе такой точки не было. В этом наборе как бы сразу перескок отсюда-сюда.
А здесь, среди этих точка t, m зацеплялась, но вот из-за этого совпадения, вот из-за этого равенства, эта пара свернулась, и точка t, m пропала.
После свертки точка t, m пропала. Оказался как раз ровно тот множитель, который стоит вот здесь.
Ну, значит, они совпадают, значит, все по теореме получается, что все доказано для нулевого начального значения.
Ну, теперь в общем случае, теперь нам нужен общий случай, как в общем случае найти процесс.
Значит, в общем случае строим, строим ксеноль, равная нулю, процесс.
И затем, затем добавляем независимую случайную величину ксеноль с распределением q0.
Ну, то есть вот так делаем.
Ну, на уровне, без процессов, на уровне вот этих вот характеристических функций, независимую с этим процессом.
Но можно не запариваться, ну, конечно, когда говорят независимую с этим процессом, спрашивается, а почему такая есть?
Ну, чтобы это не обосновывать, это с помощью теоремы Колмогорова тоже обосновывается, но это можно не обосновывать,
а просто сказать, что мы сделаем на уровне, что мы сделаем на уровне характеристических функций.
Значит, на уровне характеристических функций делаем так, просто умножаем, умножаем все на q0,
простите, на финоль, характеристическую функцию q0.
Значит, сначала построили все с целью сделать нулевое начальное значение, а потом, когда мы все это сделали,
то еще берем и умножаем на, ну, это как раз равносильно тому, что мы ко всем добавляем, ко всем построенным добавляем одну и ту же случайную величину
с нужным распределением начального, ну, от них независящую. Но вот при такой процедуре выгода в том, что не надо объяснять, почему так есть.
Но на самом деле вот в этом и состоит объяснение, как такое сделать. Ну, вот так, умножить характеристические функции на общую вот этого q0.
Значит, обратите внимание, когда мы добавляем к ним что-то, неважно, что мы добавили, чтобы мы к ним не добавили, чтобы мы к ним не добавили общее, разности этого не заметят.
Поэтому для разности ничего не изменилось. Ну, а для начального условия, конечно, изменилось.
Но, собственно говоря, можно было бы, конечно, так не делать. Мне кажется, просто так немножко наглядней, когда с нулевым начальным условием.
Значит, вот таким образом эта теорема доказана. С помощью этой теоремы строятся два важных процесса с независимыми перерощениями.
Значит, вот у нас почти все процессы, которые встречаются, будут построены вот с помощью теории Холмогоровой и ее следствия.
Значит, вот давайте рассмотрим два основных процесса, Винеровский и Пуассоновский, строятся с помощью этой теоремы.
Давайте я сначала скажу, сразу скажу, что это за процессы, а потом мы обоими чуть-чуть подробнее займемся.
Значит, Винеровский, это вот кто такой. Дубль ВТ. У него сразу начальное значение нулевое.
И вот эти вот перерощения независимы, независимы при упорядоченных по времени ТН-ных.
И каждое перерощение, значит, вот давайте здесь напишу, это центрированная гауссовская с дисперсией Т-С.
Ну, когда С меньше Т. А кто у нее фурье, у такой гауссовской?
Значит, то есть характеристическая функция такой гауссовской, значит, это экспонента минус у квадрат пополам умножить на Т-С.
Значит, вот кто у нее у разности, видите, у разности.
Ну, давайте смотрим условия согласования. С меньше У меньше Т.
Значит, смотрите, что должно быть. Должно быть, что вот это должно оказаться равно вот чему У минус С на вот это Т минус У.
Значит, ну сравниваем, есть равенство между ними, но есть равенство, равны, значит, они равны.
Все, значит, по доказанной теореме такой процесс есть.
Значит, ну мы в следующий раз поговорим про гауссовские процессы и еще иным способом это же проверим.
Мы вообще то, что есть Винеровский процесс, мы проверим тремя способами, но это не потому, что каждый ненадежен.
То есть это не будет та ситуация, про которую Коломогоров вспоминал, что он в старших классах увлекался историей и даже подумывал, чтобы стать историком.
И ходил уже школьником на всякие семинары такие уже, так сказать, студентов, но там ему очень не понравилось, что в истории каждый факт требует нескольких подтверждений.
Он решил, что в математике как-то лучше одного достаточно.
Так что это у нас не потому будет три обоснования, что каждого недостаточно, а просто каждая основана на каких-то интересных аспектах Винеровского процесса.
Значит, смотрите, первый аспект, он выступил у нас как процесс с независимыми превращениями.
Второй аспект этого Винеровского процесса, он выступит у нас как гауссовский процесс.
И третья его сущность это случайный ряд, он у нас появится как случайный ряд.
Каждый из трех этих граней может быть использован как доказательство его существования.
Так, теперь давайте по асоновски процесс, но только давайте, чтобы я не путал.
Я вот, кстати, в конспекте, я вам скоро пришлю ревизованный конспект, и я там некоторые обозначения исправлю, чтобы соответствовало тому, что я на лекциях писал.
Ну вот, в частности, характеристические функции, у нас аргумент характеристической функции, согласно проведенному в прошлый раз референдуму,
будет через Y обозначаться, а не через лямбда, как это делают. Но почему? Потому что лямбда у нас сейчас будет задействована особо.
Значит, определение. Теперь давайте по асоновски процесс, это будет отдельная история.
Это тоже очень важный процесс, и он тоже у нас двумя способами появится.
Вот, определение. Процесс NT на ноль бесконечности, по асоновски, с параметром лямбда,
или еще его называют с интенсивностью лямбда.
Значит, если приращение независимое, так же, как Винровский, в начальный момент ноль,
и при S-T приращение вот это, это вот что такое, имеет распределение по асона с параметром лямбда.
Это вот что значит. Простите, не лямбда, а лямбда на T-S.
Это значит вот что. Давайте я вот, чтобы какое-то время это повисело здесь, напишу.
Это значит, что вероятность того, что XI от T минус N от S есть K, есть вот что такое.
Так, значит, и это нужно поделить на K факториал. Значит, K здесь 0, 1, ну и так далее.
Значит, ноль и натуральные числа. Ну вот, это дело вкуса, ноль считать натуральным числом или нет.
Ну вот, Арнольд, например, считал, что ноль ни в коем случае нельзя считать натуральным числом.
Значит, смотрите, это известное классическое распределение.
Для того, чтобы убедиться, что оно есть, нужно сложить ряд из них и убедиться, что это единица.
Какие значения принимает пуассоновская случайная величина? Она принимает значения 0, 1 и так далее, вот с такими вероятностями.
Тут еще замешан параметр. И вот каждое приращение, оно со своим параметром, у них общий этот множитель λ общий, но при нем еще, видите, вот эта разность.
Ну, теперь спрашивается, почему он существует? Ну, давайте сосчитаем характеристический функционал.
Значит, давайте сосчитаем характеристический функционал этого процесса. Ну, не процесса, а приращение.
Значит, фи, значит, с, т, значит, от у. Значит, как нужно считать?
Ну, как? Ну, нужно брать, умножать вот это дело вот на это. Вот такое мат ожидания.
Но тут никакого интеграла не возникает. Возникает ряд по k. Почему? Потому что вот эта штука принимает, какие она значения принимает?
Значит, она принимает значения k. Значит, это принимает только вот такие значения. Но нужно еще умножить на вероятности вот те.
Вот такая, вот такой получается, видите, такой ряд получился.
Так, ну и что же это? Такое что-то напоминает такое с первого курса. Значит, какой-то ряд, а какие-то и еще ужасные.
Значит, вот с комплексного анализа, не к вечеру будь помянут. Но он тут такой вполне себе безобидный, этот i.
Получается, смотрите, что, во-первых, с радостью замечаем, что этот множитель от k не зависит, его можно из ряда исключить.
И смотрите, что тут будет стоять. Тут будет такой вот стоять ряд с обратными факториалами, а при них будут как раз стоять степени.
Так, значит, какие-то. И поэтому, значит, смотрите тут, что в итоге получится.
Получится вот такой, вот этот множитель, он как выносился, так и выносится. А здесь смотрите, что получится.
Здесь получится экспонента. Так, значит, экспонента, ну давайте я ее, чтобы меньше этажность была.
Значит, смотрите, кто здесь, экспонента кого. Тут стоит e в степени, ну вот мы когда это объединим, так тут будет вот что стоять.
E в степени i, y, так. E в степени i, y. И при нем вот этот множитель.
О, видите. Значит, вот такая есть получилась.
Ну такое, в общем, ну ничего себе выражение все такое.
Естественно, это можно еще загнать под общую экспоненту.
Это же тоже экспоненты, поэтому можно из этой экспоненты вычесть еще вот этот множитель.
Ну давайте это проделаем, не поленимся. Давайте это проделаем, и будет вот такой ответ.
Значит, смотрите, что тут получится. Значит, тут получится лямда t-s, так.
А тут будет e в степени i, y, так, минус 1. Вот такой получился ответ.
Вот эта функция аргумента y, вот она эта функция аргумента y, так.
Ну и опять, значит, опять нужное условие опять выполнено.
Значит, опять, опять получилось, что вот это есть, это равенство опять есть у нас.
Значит, очень похожим образом, очень похожим образом на предыдущее, так.
Значит, таким образом, значит, процесс, значит, процесс n-t существует.
Значит, видите, вот два важнейших для нас процесса, вот они оба следствия этой теории.
Но сейчас мы, знаете, что сейчас вот давайте сделаем, сейчас, или вот что,
мне нужно где-то через пару минут сделать небольшой технический перерыв,
потому что я одновременно еще на диссертационном совете присутствую.
Я вот так сказать сочетаю, как говорится, приятное с полезным.
Значит, вот мне в три нужно, значит, в три мне нужно туда подключиться.
Он, к счастью, он, к счастью, удаленно в режиме происходит.
Значит, давайте я сформулирую теорему.
Значит, другой способ, другой способ получить пуассоновский процесс.
Ну, кстати, понятно, я это забыл отметить, но сами догадываетесь,
что пуассон это, конечно же, должен быть француз.
Ну, или вот любители французской кухни знают, что пуассон это просто рыба по-французски.
У нас так, так сказать, серьезно звучит пуассон.
А вообще-то это рыба.
Значит, да, так вот, значит, это такой был известный математик,
он и вероятностью занимался, и анализом, и уравнениями,
ну, и, в общем-то, да и алгеброй тоже.
Его имени много чего названо.
Значит, помимо распределения пуассона, значит, есть уравления пуассона,
значит, есть всякие скобки пуассона.
Ну, в общем, он много где серьезно отметился.
Да, так вот, значит, теорема.
Значит, пусть это Н, процесс восстановления,
из независимых случайных величин, ну, ксиитая,
с экспоненциальным распределением.
Тогда, тогда это пуассоновский процесс.
Ну, вот сейчас после перерыва технического я напомню,
что такое и процесс восстановления, и что такое показательное,
ну, что такое экспоненциальное распределение.
Так, значит, восстановление, это вот что такое.
Значит, берутся, складываются эти независимые случайные величины.
И, значит, для каждого t мы берем максимальное N,
покуда сумма не превысила t.
Теперь, что такое распределение, ну, вот это с параметром экспоненциальное.
Вот давайте я напишу, значит, ксииты распределены экспоненциально
с параметром лямбда больше нуля.
Что это значит? Это значит, что имеют плотность,
значит, имеют плотность вот такую вот.
Лямбда на e в степени минус лямбда х, значит, умножить на индикатор,
значит, ноль плюс бесконечность от х.
Значит, вот такая вот плотность.
Слева от нуля ноль, значит, слева от нуля ноль,
и потом в нуле лямбда, ну, и потом вот так вот убывает.
Значит, это довольно интересное распределение.
Любопытно, что у него среднее совпадает с дисперсией.
Значит, таким образом, видите, у Плассоновского процесса
есть совершенно явная формула его сдающая.
Видите, тут никаких распределений нет.
Он задан не с помощью теоремы Колмогорова,
а, можно сказать, явной формулой.
Ну, вы спросите, а зачем?
Ну, это полезно по нескольким причинам.
Во-первых, всегда полезно, когда какой-то процесс появился
в каком-то таком первозданном виде.
Ведь у классиков задолго до Колмогорова он появился.
Этот процесс действительно в прикладном отношении...
Вообще процессы восстановления довольно важны в прикладном отношении процесса.
Я напомню, у нас такие самые элементарные процессы восстановления.
Есть такие усложненные, но у нас самые элементарные.
Но даже в нашем элементарном обращении
процесс восстановления зависит от распределения
вот этих независимых, одинаково распределенных.
Значит, кто у нас параметр процесса восстановления?
Ну, вот это распределение каждого из них.
И Пуассоновский процесс получается при очень специфическом выборе
вот этого распределения, а именно вот при таком.
При других видах распределения.
Другие процессы, не Пуассоновские.
И вообще надо сказать, что вроде тут все очень элементарно,
но оказывается, что это довольно нетривиальные процессы.
Вот как только вы начинаете брать общее распределение,
не какой-то простого вида конкретного, а более общего,
то весьма нетривиальные вопросы возникают про эти процессы восстановления.
Так что это такой классический объект.
Вот с Виннеровским процессом у нас будет похожая история.
У нас будет явная формула, вот прямо явный процесс,
будет записан явной формулой, в виде ряда.
Вот здесь в виде Супрему какого-то, а там будет записано в виде ряда.
Но там у нас не будет доказательства,
потому что оно очень трудно доказать, что вот та формула,
которая у нас будет, что она и впрямь задает то, что надо.
А вот здесь можно, ну нельзя сказать, что это такое совсем, так сказать, легкое занятие,
но тут можно это явно голыми руками проверить.
Ну вот давайте мы это попробуем проверить.
Ну, собственно как попробуем.
Я вот один раз проверил, у меня вроде бы получилось.
Так, давайте попробуем на доске это воспроизвести.
Ну, методика тут, на худой конец, методика тут такая.
Это объявляется задачей.
Ну, если мы ее делаем, то значит вот мы молодцы.
А если мы сейчас быстро не сделаем, то перейдем к Гауссовским процессам,
ну и будет вам задача.
Доказательства.
Ну, я надеюсь, что сделаем, но тут некие вычисления.
На самом деле меня больше всего тревожит вычисление,
но вы же недавно сдавали мотан, ну и вообще считается,
что физтехи могут какие угодно интегралы считать.
Вот сейчас и посмотрим.
Значит, найдем распределение вот такой штуки.
Значит, вот такого вот вектора.
Значит, оно получается...
Значит, оно получается из распределения кси1 ксиn.
Значит, вот линейным преобразованием с определителем равным единице.
Давайте посмотрим, какое будет линейное преобразование.
Значит, у него матрица такая.
Значит, матрица такая.
В строке, ну, в катой строке, в катой строке,
стоят 1, 1, 1 до позиции к, а потом нули.
Так, значит, вот такая матрица.
Ну, почему у нее определитель 1?
Ну, так всегда было.
Ну, вот, так сказать, физтехи должны это уметь проверять.
Значит, поэтому плотность...
Значит, смотрите, какая плотность распределения.
Значит, вспоминаем, что мы ушли сейчас от этих независимых превращений.
У нас сейчас эти сами стали независимы.
Поэтому распределение вектора просто произведение распределений.
А у них плотности распределения.
Значит, это будет просто очень простая плотность.
И поэтому распределение, плотность распределения,
это будет вот что.
Ну, то есть, это будет плотность вот этого.
Ну, то есть, это будет плотность вот этого.
Вот от чего.
И итог будет вот какой.
чтобы меньше было писанины на доске, кое-какие пропускаю вычисления, но желательно их проверить.
Значит, поэтому смотрите, какой будет итог.
Значит, будет вот такой итог.
Значит, итог будет вот какой. Лямбда в степени n на e в степени минус лямбда xn и на индикатор,
вот от чего xn больше xn минус 1 больше и так далее x1. Вот такая вот плотность.
Ну почему? Потому что смотрите, какая плотность у каждой из этих. У каждой из этих плотность вот
лямбда умножить на экспоненту. А на экспоненту чего? Вот этого с минусом. Но так только,
когда аргумент положительный. А если аргумент ноль или отрицательный, то ноль получается.
Вот откуда выплыло вот это ограничение. Вот это ограничение выплыло из-за этого,
что каждая плотность на отрицательной полуаси равна нулю. Теперь лямбда в степени n,
ну вот оно и есть, лямбда в степени n, это общий множитель. Но куда подевались все эти разности?
Ну а разности посокращались, потому что, видите, тут стоят не сами аргументы, а их разности.
Ну там с этими множителями лямбда. Но тем не менее они все благополучно посокращались,
кроме остался только последний. Вот последнего никто не сократил. Вот получилась, смотрите,
получилась такая плотность суммы. Но это еще далеко, это пока еще довольно-таки далеко
результатом. Зачем-то мы сосчитали плотность распределения суммы, но наш процесс-то это
вовсе не сумма. Зачем мы сосчитали такой ответ? Такой ответ мы вот зачем сосчитали.
Значит, смотрите, что нам надо найти. Нам надо вычислить, ну для того, что убедиться,
что наш процесс пулассоновский, нам нужно вычислить вот такие вот вероятности.
При различных числах. Зачем нам нужно вычислить такие вероятности? И почему мы их так странно
записали? Ну нам в действительности нужно, что нам нужно убедиться? Нам нужно убедиться,
что у процесса независимые перерощения. Вот в чем надо убедиться. Поэтому мы вынуждены
рассматривать вот такие разности. Это по определению, процесс с независимыми перерощениями.
Дальше. Мы хотим увидеть, что при рощении независимой это значит, что эта штука распалась,
вот эта вероятность распалась. Но это нужно делать для всевозможных значений. Ну там c1, c2, cn.
Но мы, так сказать, авансом, чтобы упростить себе вычисления, мы вот эти числа, тут надо было
бы писать это равно c1, это равно c2, это равно там, ну и так далее. Но мы, то есть ну наоборот это cn,
там c2, c1. Но мы, значит, хитро написали. Ну а что было бы, если бы мы, так сказать,
как бесхитростные ребята, ну просто поставили сюда тут cn и c1. Ну ничего особенного, ужасного
не было бы. Ну просто дальше формулы сложнее станут. Сейчас мы увидим, что сейчас нам предстоят
такие вычисления. Они и без того, ну такие не слабые, а если еще и константы плохо подобранные,
то они будут очень длинными. Вот собственно финт в этом. Теперь, если мы убеждаемся, что эта
вероятность распадается в произведение всегда, то мы получаем, что процесс с независимыми
приращениями. Дальше, когда для одного приращения, когда для одного приращения, ну вот только, скажем,
для этого, значит, мы находим, что эта вероятность равна тому, чему полагается быть для распределения
уасона, ну, значит, все дело в шляпе. Поэтому, как только мы убеждаемся, ну, вернее, как только мы
получаем ответ, то все. Ну давайте попробуем этот ответ получить. Значит, основное, так,
значит, основное. Значит, это три воскресаятельных знака, но это не междометия, а это, так сказать,
знак того, что это очень важно. Значит, это есть вероятность события. Значит, вот какого события.
С1 и так далее, СК минус один принадлежат ноль Т1, СК один плюс один, значит, и так далее,
СК, СК два принадлежат Т1, Т2, значит, и так далее. И последнее, и последнее, С, значит, КН минус один плюс один,
значит, и так далее, СК, Н принадлежат ТН минус один, ТН. Вот. Значит, вот так. Да, да, да, еще нет,
еще я не дописал, еще я не дописал самое важное, еще я самый важный кусок не дописал. Значит, СКН,
сейчас, не-не-не, вот я здесь нехорошо написал, а, да, нет-нет, все, здесь я написал хорошо,
СКН плюс один больше ТН. Значит, вот, вот основное, так сказать, вот так мы, значит, эту вероятность
интерпретировали, так. Ну, почему это так? Ну, вот это надо, вот это надо разобраться, но так вот,
если некоторое время на это посмотреть, то, ну, в общем, вот можно это понять. Ну, в особенности это
можно понять, когда уже сказан ответ, чему это равно. Вот если вам сразу сказали, найдите эту
вероятность. Ну, это, в общем, не очень просто, но тут вот, так сказать, классики, значит, догадались,
и вот, значит, ответ сообщили. Ну, вот это надо осмыслить. Я пытался, я пытался этот ответ как-то
себе объяснить, но, ну, не знаю, но мне кажется, это невозможно объяснить, вот, мне кажется,
что если несколько минут смотришь на это и думаешь, и сравниваешь ответы, то понимаешь,
почему это так. А вот так вот просто сказать, значит, как говорится, просто и ясно, почему это так.
Ну, в общем, надо вот на это внимательно посмотреть и увидеть, значит, почему это так. Но, собственно,
интуитивно это понятно, потому что, ну, вот речь идет, так сказать, о перескоках этих сумм, ну,
так сказать, между уровнями, так. Ну, вот надо проанализировать, что за перескоки происходят,
и вот что происходит, сравнить эти два события и увидеть, что, ну, вот так оно и есть, так.
Но теперь получается, что, значит, смотрите, что получается. Сейчас, но на самом деле получается,
что надо заканчивать, между прочим. Значит, вот давайте прикинем, значит, сколько времени еще есть?
А, минус шесть уже, ой. Слушайте, не, слушайте, а чего же вы молчите, вы должны были протестовать.
Все, тогда точно заканчиваем, все. Значит, в следующий раз продолжим, в следующий раз продолжим.

Десятое занятие. И сегодня, соответственно, мы говорим про мета штрафов. Вот, и связанный с ним метод,
один из самых популярных тоже методов оптимизации, ADMM. Так, хорошо, хорошо. Ну что, мы все еще продолжаем
с вами решать задачу оптимизации. Все еще у этой задачи есть какие-то ограничения. Вот. В данном случае
я пока ограничиваюсь только ограничением вида равенств. Ну и, соответственно, сразу же у меня
возникает к вам такой вопрос. Давайте я возьму некоторую константу RO, просто какое-то число,
и немного модифицирую мою целевую функцию. Ограничения я не трогаю. Соответственно,
в целевую функцию я вписываю вот такой вот дополнительный член. Такую вот добавку. Вот.
Что можно сказать? Изменилась ли у меня задача? Как тогда поменяется решение, если я вот добавил
вот такой вот кусок туда? Так, минимум мог увеличиться. Не увеличится? Именно значение
минимума или точка значения. Так, у кого еще какие версии?
Да, то есть на множестве, на котором мы решаем эту задачку, а оно вот оно, оно у нас задается с
помощью ограничений. На этом множестве вот эта добавка чему равна? Нулю. Поэтому я добавил за
бесплатно считать. То есть она мне вообще никак не поменяла задачу. Просто на том множестве,
на котором я решаю задачку, там ничего не происходит. Там, соответственно, эта добавка ноль, и она на нее
никак не влияет. Вот. Поэтому вот то, что сейчас написано здесь, это две эквивалентные задачи,
верхняя и нижняя. Соответственно, да. Но. Где была, соответственно, нижняя задача? Хочется ее теперь
сделать более дружелюбной. И, соответственно, уже в данном случае мы убираем ограничения. Убираем
ограничения и оставляем уже задачу безусловной оптимизации. Мы решаем ее на Rd. Но вот, соответственно,
функция целевая, которую я теперь хочу минимизировать, она выглядит следующим образом. Это не просто моя
исходная целевая функция, ну плюс вот эта добавка, которую, соответственно, называют штрафом. Штрафом за
то, что вы не удовлетворяете ограничениям. Соответственно, удовлетворяете ограничениям, штраф равен нулю.
Не удовлетворяете. Штраф, соответственно, начинает расти в зависимости, понятно, от 3, ну и в зависимости от того,
насколько вы не удовлетворяете ограничениям. В итоге мы получаем вот такую вот целевую функцию, с которой и
хотим взаимодействовать. Ну а сейчас что думаете, сильно ухудшилась задача или нет? Какие вот первые
мысли, когда вы смотрите вот на что-то вот такое и вспоминаете исходную задачу? Не эквалентно, да. Как это
можно понять? Какие вот первые мысли есть? Будут ли выполняться ограничения или нет?
Да, соответственно, тут, если мы, например, ров возьмем не особо большим, то, например, значение штрафа,
ну оно может быть вообще не особо влияет на целевую функцию. И в принципе, то есть под штрафной
функцией может стоять, ну вот значение, которое стоит здесь, оно может быть не нулевым, вот, и даже
довольно большим, если рост совсем маленький, вот. В связи с этим как бы возникают какие-то первые
наблюдения. Да, соответственно, хорошо, что мы перешли от задач с ограничением к задаче без
ограничений. Такую задачу приятнее решать, методы мы для нее знаем, но, соответственно, предполагается,
что вот здесь мы уже за пределы наших ограничений можем спокойно выходить, то есть вот такой штраф,
он не запрещает нам выходить за ограничения, ну пошли мы куда-то в бок, ну штраф начал
потихоньку расти, вот. Ну, соответственно, не проблема, возможно, при этом там у меня целевая
функция уменьшается, ну, то есть вот функция f. Мы же для этого и решаем на задачу на множестве,
потому что, скорее всего, глобальный минимум у него находится где-то в другом месте. Ну вот уходим мы
от штрафа. Штраф увеличивается, а функция при этом уменьшается и непонятно, что уменьшается
быстрее. Соответственно, это в некотором смысле тоже борьба и баланс здесь. Единственное,
что вот хорошая интуиция, когда мы устремим бесконечность, и что у меня будет, когда x
удовлетворяет ограничениям. Чему это равно? Только что обговаривали. Это просто f, да, это просто моя
целевая функция, а если x не удовлетворяет ограничениям, то чему это равно, если rho? Это плюс
бесконечность. То есть вот в такой интуиции это действительно работает хорошо, rho большое, прям
огромное, ну и тогда реально мы по факту работаем на нашем целевом множестве, которое у нас было
изначально задано с помощью ограничений, а соответственно, вне этого множества у нас просто
функция равна плюс бесконечности, и мы туда не заходим. Вот. Это хорошо, это хорошо, вот. Поэтому
все же появляется какая-то надежда, что взяв достаточно большой rho, можно вот эту задачу,
которая у нас написана, отрешать так, что действительно получено здесь решение будет
неплохо, так, ипроксимировать то решение исходной задачи, которая у нас была. Окей. Почему нет? Давайте
разбираться, насколько вообще мы хорошо это все можем сделать. Вот. Мы прикоснемся только к каким-то
основным результатам, но сначала покажу, как, например, это можно сделать для задачи с ограничением
типа неравенства, как их можно запихать штраф, вот, аналогичным образом, что нужно сделать мысли и
идеи. Неравенство с неотрицательными коэффициентами. Как это записать? Ну вот,
добавляем, например, к штрафу неравенства, вот, какой-нибудь, вот так вот просто добавляем,
но это вроде не совсем правильно, потому что кажется, что... Вот. Вот. Это хорошо. То есть,
вот так вот, например, нормально. То есть, как раз, если у меня g меньше нуля, вот, у меня срабатывает
в максиме ноль, и, соответственно, к штрафу прибавляется ничего. Вот. Если, соответственно,
g начинает расти и становится больше нуля, срабатывает, соответственно, уже штраф. Ну, здесь
можно еще квадратик и квадратик дописать, чтобы все четко совсем было. Вот. Понятная идея, да? Как,
соответственно, сюда можно еще запихать что-то вида неравенства.
Квадрат, квадрат, квадрат. А какая разница, я снаружи или внутри поставлю? Вот. Ну, все равно у
меня функция положительная. Ну, можно, наверное, и без квадратов. Да, это правда. Вот. Ну, можно и
с квадратом. То есть, здесь квадрат необязательный. Штрафуем и штрафуем. Функция положительная.
Окей. Так. Хорошо. Хорошо. Идем дальше. Давайте попробуем потихонечку,
по чуть-чуть вообще поразбираться за свойствами, которые у нас есть у этой задачки. Вот. Чтобы
понять, насколько вообще мы делаем что-то адекватное. Первое свойство довольно простое.
Первое свойство довольно простое. Пусть мы нашли, у нас есть решение х со звездой,
наша исходная задача с ограничениями. Вот. И есть х ро со звездой. Это решение нашей
новой безусловной задачи оптимизации, но, соответственно, с дополнительным штрафом. Нашей
вот штрафной задачи. Вот. Ну, хочется показать, что будет выполнено следующее отношение. Оказывается,
значение в точке, вот, которую мы нашли с помощью штрафной задачи, будет меньше либо равно,
чем значение в реальной целевой точке, которую мы ищем в исходной задаче. Фак довольно простое,
что мы делаем. Давайте посмотрим. Скажем, что если у нас х со звездой, это решение нашей
исходной задачи. Почему выполнено вот это равенство, как думаете? Да. Потому что х со звездой это решение.
Задача. Значит, он удовлетворяет всем ограничениям. Значит, штрафная функция для нее равна нулю. Дальше,
соответственно, что я говорю? Ну, давайте я разморожу точку, потому что х со звездой это теперь
не минимум. Возможно, не глобальный минимум моей функции штрафной. И здесь будет какая-то, может быть,
другая точка стоять, которая дает минимум штрафной функции. Ну, вот я ее и подставляю. Понятно,
что здесь будет меньше либо равно. Ну, а дальше, как я рассуждаю? Говорю, что у меня штраф,
эта вещь не отрицательная. Поэтому, если я уберу из функции штраф, то, соответственно, функция станет
только меньше. Ну, не больше, не увеличится. Окей? Все довольно просто. Но главное, какую интуицию это
нам дает? То есть, то, что точку мы нашли, в ней функция будет обязательно меньше. Ну, не больше,
но в случае, когда меньше. Что это значит, если у нас вот так вот выполнилось? Что это значит?
Ну, может ли такое быть, если мы, например, находимся в пределах наших ограничений?
Да, мы попали в множество за пределами, потому что просто по определению глобального минимума
задачи. X звездой — это решение, глобальный минимум, когда он, соответственно, меньше либо равен
значению в нем, меньше либо равно любому значению в пределах этого множества. А здесь, соответственно,
мы получаем то, что у нас значение в нашей точке, которое мы нашли, может быть, вообще меньше.
Вот получается, мы точно за пределами находимся. Ну, либо, если у нас равенство, то мы прям идеально
попали. Чаще реализуется как раз второй случай. И действительно, когда вы применяете метод со
штрафами, вы по факту выходите за пределы множества. Вот, плохо это ли хорошо, зависит от конкретной задачки,
потому что где-то это позволительно, вот, где-то непозволительно, потому что в любом случае у вас
есть вычислительные какие-то огрехи, и даже решая точную задачу, например, исходную какую-то
другими методами, вы все равно получите решение, которое, скорее всего, из-за каких-то небольших
вычислительных огрехов на какие-то малые порядки отличается от, ну, неудовлетворяя от
ограничений. Ну, в пределах погрешности какой-то машины точности. Ну, и здесь то же самое, понятно,
что подбирая большое РО, можно вытянуть. Ну, вот это мы сейчас как раз и попробуем понять. Вот,
большое РО, соответственно, кажется, что это может помочь. Ну, давайте, соответственно, попробуем это
подоказывать. Первое свойство, которое связано с подбором РО, это то, что увеличение РО
влечет за собой вот такое вот соотношение. То есть, значение штрафа с увеличением РО уменьшается.
Ну, не увеличивается. Не увеличивается. То есть, берем больше РО, значение штрафной функции в том
решении, которое мы найдем, минимизируя, безусловно, задачу со штрафом, станет меньше. То есть,
чем больше РО, тем, соответственно, меньше значение штрафной функции, тем меньше вы выходите за
пределы ограничений. Вот. Давайте, на отдельном листике черкану доказательства здесь. Давайте,
тут на самом деле не сложно, просто нужно в две стороны записать. Пусть у меня есть сначала одна
задачка, которая со штрафом, например, РО2, которая меньше. И есть, соответственно, вот эти ограничения.
Вот. У этой задачки есть свое решение. Например, обзовем его XRO2 со звездой. Это решение.
Вот. Ну, тогда по свойству решения, так как мы решаем задачу безусловную, да, и если бы даже
условная была, у нас что бы справедливо. Здесь РО1 со звездой. Вот это будет решение задачи с РО1.
Плюс РО2 сумма штрафов. Здесь XRO1 со звездой. Ну, это все понятно, будет меньше либо равно,
чем значение в точке РО2. Так. Ну, это просто из определения того, что я работаю с минимумом моей
штрафной задачи. Окей? Здесь вроде ничего сложного. Аналогичную вещь я могу записать и для задачи,
когда я поменяю РО2 на РО1 и в обратную сторону. То есть подставил теперь РО1 и здесь будет
X со звездой РО2 плюс РО1, аж и в квадрате X со звездой РО2 меньше либо равно, чем f X со звездой РО1 плюс РО1 и штрафы.
Здесь согласны? Так. Теперь я суммирую эти две строчки. Вот. Видно, что будут по красоте уходить
вот какие-то такие слагаемые, которые завязаны на функцию f, и соответственно вылезет что-то
довольно хорошее. Давайте сразу я подобные приведу вот раз-два и раз-два-раз-два. Сейчас там чем-то с рожкой
будет. РО2 минус РО1 сумма ошитых X со звездой РО1 меньше либо равно, чем РО2 РО1 сумма ошитых
в квадрате X со звездой РО2. Вот. Ну а все, дальше я говорю то, что у меня по предположению РО2 меньше,
чем РО, или наоборот, или как-то как это предполагал. Что у меня тогда получится? Ну тут неважно,
давайте пусть будет РО2 больше, чем РО1, тогда я когда разделю обе части неравенства на РО2
делить на РО1, минус РО1, знак не поменяется, и тогда у меня окажется, что значение штрафа
точки в решении задачи с РО1 будет больше либо равно, чем значение точки в решении задачи с РО2.
Окей? В формулировке может быть по-другому было, там РО1 было больше, ну это неважно,
поменять местами. Все, здесь простой факт, который в принципе нам дает хорошую интуицию в плане того,
что мы хотя бы понимаем, что увеличение РО может привести к каким-то вещам в душе того,
что действительно у нас штраф хотя бы не будет увеличиваться, штраф хотя бы не будет увеличиваться.
Так, сейчас более такая теорема существенная, выглядит она следующим образом, немного так
замудренно формулируется, пусть у нас опять же есть функции вот эти, наша задачка с ограничениями,
эти функции являются непрерывными. Есть у нас множество х звезды, это множество решений
исходной задачи, предполагается, что оно не пусто. Дальше соответственно что? Дальше мы
рассматриваем вот такого рода множество. Это просто соответственно значение, в этом множество U
входят те точки, где соответственно у меня f'х меньше, чем f'х со звездой. f'х со звездой это у меня
значение соответственно на моем множестве, которое задано ограничениями, а f'х это соответственно
тут уже в ВУ могут вводить точки, которые и ограничения не удовлетворяют. И предполагается,
что это множество ограничено. Ограничено, сейчас мы как раз, ну давайте можно сразу обсудить зачем
вообще это нужно. Потому что мы сейчас будем как раз работать с штрафной функцией, как-то говорить
про ее решения, где они лежат относительно решений нашей исходной задачи и зачем я вообще
ограничу вот это множество U. В пределах самого множества, на котором мы решаем нашу исходную
задачу с ограничениями, с функцией может быть все хорошо. Она действительно там принимает какое-то
конечное значение, возможно даже уникальное, если повезет. Это минимально ее значение. Что
может происходить за пределами этого множества никто не знает. Ушли за пределы, вышли, наткнулись
например на такую ситуацию, когда даже выпукла функция, просто уходит минус бесконечности.
Соответственно да, чтобы этого избежать, работать с штрафными функциями в том числе этого хотим
избежать, потому что добавили какую-то линейную функцию и она у вас сразу минус бесконечность
поползла. Этого хочется избежать. Соответственно предполагается, что вот у нас функция f, ну вот те
значения, которые меньше, чем fх звездой, ариол обитания ограничен. Ариол обитания ограничен,
чтобы как раз не уползти в минус бесконечность и работать с штрафными функциями, хотя бы какие-то
гарантии иметь. Иметь то, что мы не решаем какую-то бесполезную задачу. Ну то есть бессмысленно
решать задачу, которая там минимум где-то в минус бесконечности болтается. Вот. А про что вообще
теорема? А теорема про то, что вот если у нас есть какое-то Эпсилон произвольное, тогда существует
некоторый подбор параметра РО и оказывается, что вот множество решений штрафной задачи,
как раз на самом деле вот это условие гарантирует, что это множество не пусто. Вот если исходное
множество х звезды было не пусто. Для любых РО, которые больше подобранного нашего РО Эпсилон,
окажется так, что вот множество решений задачи штрафной будет лежать в некоторой Эпсилон окрестности
исходного множества решений. Понятна суть, да? То есть берем нашим исходное множество,
чуть-чуть его раздуваем на Эпсилон. Ну вот здесь вот как раз чуть-чуть отступаем от него на Эпсилон.
Был шарик, например, стал шарик плюс Эпсилон. Немного раздули и оказывается, что вот можно всегда
подобрать такое РО, что множество решений уже задач со штрафом будет лежать в пределах вот этого
раздутого шарика. Понятно, что если хочется взять довольно маленькое Эпсилон, вот видимо берется
большой РО и получается, что вы почти совпали с точки зрения именно вот этих множеств решений,
где они относительно друг от друга лежат. Окей? Окей. Ну давайте попробуем это подоказывать.
Давайте попробуем это подоказывать. Вот. Пойдем от противного. Пойдем от противного и соответственно
будем предполагать, что у нас ну вот так случится, что множество не лежит. Ну и соответственно,
что там предполагалось у нас существование РО Эпсилон и для него следовало, что для любого РО больше,
РО Эпсилон у вас соответственно х РО со звездой кладывалось в это. А теперь я хочу пойти от противного
и говорю то, что у меня существует некоторая последовательность РО ИТ, который устремляется
в бесконечность. Ну вот здесь понятно РО. Оно просто больше, чем никакого либо РО Эпсилон и понятно,
что если я не устремлю эту последовательность плохую в бесконечность, я просто РО Эпсилон могу чуть-чуть
поднять и тогда вот у меня все, которые не попали, они зажуются и у меня все РО будут нормальными. Вот. Я
ввожу последовательность РО ИТ, который устремляю к бесконечности и вот для этой последовательности
у меня х ИТ со звездой не вкладывается в х Эпсилон. Вот. Второе утверждение я могу, кстати, переписать
чуть даже по-другому. Я могу сказать, что существует некоторый х ИТ со звездой. Такой,
что он не лежит в х Эпсилон. Так, ну это по факту как раз то, что у меня не вкладывается. Существует
элемент, который где-то лежит за пределами. Окей. Понятно суть, да? Что произошло? Как мы идем от противного?
Вот. Хорошо. Так. Это мы с вами предположили. Давайте, соответственно, подумаем. Подумаем,
что мы можем сказать про значение в точке и х И. Х И я все еще напишу давайте из РО ИТ. Он в РО ИТ лежит,
а в РО Эпсилон не лежит. Что мы можем сказать, например, как соотносятся вот эти два, две вещи.
Значение в точке х И со звездой и значение в точке х со звездой, где х со звездой это решение.
Исходная задача с ограничением. Что мы можем сказать? Давайте без стремлений. Просто вот есть одно
значение, есть другое. Что можно? Меньше либо равно. Почему? Потому что мы только что с вами это
доказали. Х ИТ со звездой это же у меня решение задач со штрафом, где штрафы мне определяются РО ИТ.
Так? А мы поняли то, что эти решения они меньше либо равны чем значение функции в этих решениях,
меньше либо равно, чем значение функции в исходной, в решении исходной задачи. Согласны? Вот. Допишу,
чтобы это было понятно. Мы это вроде как только что с вами доказали и довольно просто доказали,
там в одну строчку это все делалось. Хорошо. Поняли, что у нас вот так вот. Но, и как раз тут
тут срабатывает то предположение, которое я делал насчет множества У. Что множество У у меня
ограничено вот тех значений. Получается, что вот х, отсюда получается, что у меня х ИТ вот этот,
лежит Ву, которая ограничена. Так? Хорошо. Понятно, что мы вот ввели какую-то последовательность и х ИТ,
х ИТ со звездой это тоже некоторая последовательность. Но про х ИТ со звездой мы узнали,
что она вот эта последовательность лежит некоторым ограниченным множестве. Вот. Что мы
тогда можем сказать про такую последовательность? Какие факты? Да, это первый курс. Вот она теорема
Бальцана-Вирштрасы. Бальцана-Вирштрасы, что у вас существует, у ограниченной последовательности
обязательно существует один частичный предел. Ну, вот одна сходящаяся под последовательность.
Дальше, соответственно, Бальцана-Вирштрасы теорема. Вот. И мы рассматриваем уже вот эти точечки х
ИТ со звездой, которые сходятся к некоторой х ИТ со звездой. Это мы нашли как раз сходящуюся
под последовательность. Вот. Окей? Вот. Сходящаяся под последовательность. Хорошо. Ну, я на самом деле не зря
брал еще и непрерывные функции. Что у меня функция f непрерывна, что у меня функция h непрерывна.
Что мы можем сказать теперь про значение функций? Вот так. Как мы можем, например, связать значение
функции вот это, вот это и, соответственно, ну давайте их пока вот эти два значения свяжем. Что мы
можем сказать, если у меня функция f непрерывна, если у меня х ита стильдой стремится к х стильдой,
что будет? Ну, тоже стремится, потому что есть непрерывность. Это эквивалентное определение
непрерывности. Согласны? Вот. То есть вот здесь в пределе и стремящемся к бесконечности будет
просто равенство. Это одно из эквивалентных определений непрерывности. Вот. Функции.
То есть под последовательность точки стремится. Окей. Супер. Хорошо. Мы поняли то, что у меня
под последовательность вот значение вот это будет стремиться сюда. Вот. А это что значит? Это
что значит? То, что у меня значение х звездой. Я же знаю, что у меня вот эти все f иты, они меньше
либо равны, меньше либо равны, меньше либо равны, чем f от х звездой. Так? Но тогда f стильдой у меня
будет меньше либо равна, чем f от х звездой. Так? Хорошо. Так. Запомним этот факт. Это первый факт.
То есть пока он нам ни о чем не говорит. То есть это понятное свойство, которое в принципе следовало
из того, что мы берем x иты. X иты это вот решение нашей задачи со штрафами. Ну и окей. Возможно
просто x стильдой со звездой. Ну тоже лежит за пределами множества. Ну это просто какое-то
более классное решение, которое еще глубже опускается. Ну тоже лежит за пределами множества. Просто
ближе к честному глобальному минимуму безусловной задачи оптимизации, если бы у нас в исходной
задачи не было бы ограничений. Вот. Но на самом деле сейчас мы покажем, что x стильдой оно еще и
лежит в самом множестве. Лежит в самом множестве исходном с ограничениями. А значит x стильдой
будет являться решением. Потому что вот это как раз определение глобального минимума.
Потому что x со звездой у нас это что? Это глобальный минимум. Если другая точка,
которая тоже принадлежит множеству, дает значение функции меньше либо равно, то понятно,
что она тоже глобальный минимум на таком множестве. Вот. Давайте подоказываем. Сейчас,
если я себе освобожу. Так, бла-бла-бла, это мы с вами доказали. Так, это мы с вами обсудили. Так,
теперь, соответственно, доказываем то, что у нас выполняются ограничения. То есть хочу доказать,
то, что у меня x стильдой со звездой удовлетворяет ограничения. Ну, еще раз суду от противного.
Получается такая вложенность у нас от противного. Мы начали исходно доказывать от противного. Сейчас
еще этот факт докажем от противного. Ну и выруливаем в итоге к исходному. Так, ну от противного
что тут? Говорим то, что раз хотим от противного, поэтому предположим, что какое-то есть ограничение,
которому вот хотя бы единственное ограничение наше x стильдочкой не удовлетворяет. Вот. Ну и посмотрим,
что соответственно будет. Опять же, в силу того, что у меня функции h непрерывные,
функции h непрерывные. Что я могу сказать? Я могу сказать, что вот значение h Катова в точках x
и t со звездой с тильдой. То есть наши вот эти последовательности, исходящие к x тильдой. Они
будут больше либо равны, чем 1 вторая h, давайте по модулю возьму, по модулю, h с тильдой со звездой,
которая соответственно не равно нулю, которая там по модулю соответственно будет прям больше нуля.
Вот. Почему так? Ну это понятно для достаточно больших и. Так? Опять же, все это происходит из-за
непрерывности. Согласны? Потому что у меня соответственно вот это должно стремиться к h Катому x
с тильдой. Вот. И понятно, что в какой-то момент мы просто действительно окажемся в епсилонокрестности
вот этой точки, вот этой точке. Ну соответственно выйдем за пределы 1 вторая этой точки. То есть
просто значение станет ближе. Ближе для какого-то епсилона. Ну и соответственно для таких и это
будет выполняться. Окей? Хорошо. Так. Ну тогда, что я могу теперь сказать про мою, про мою вот эту
функцию. Давайте я прям, для каждой x с тильдой я выпишу его функцию, подставлю сюда x и t со звездочкой.
Ну и что там будет получаться? С тильдой, со звездочкой, плюс ρ и тильда ограничения
h и t в квадрате x и t. Вот так вот. Окей. Выписал. Что будет происходить, когда я вот это все безобразие
устремлю по и к бесконечности. Куда она будет стремиться? Бесконечности. Почему? Потому что
первый член у нас стремится просто к f x с тильдой, что в принципе просто какое-то конечное число.
Но когда я еще вот это начинаю устремлять, то есть одно из ограничений у меня точно не срабатывает.
Ограничение h с тильдой у меня не срабатывает. По предположению наше мы идем от противного.
Вот оно не срабатывает. Тут появляется какое-то значение. Тут начинаются значения вот эти h и t,
x и t, быть больше нуля конкретно. Вот даже больше, чем 1 вторая h с тильдой. Вот это. Сейчас 1 вторая.
h x со звездочкой с какого-то момента. Но при этом еще и ρ улетает в бесконечность,
потому что это же тоже последовательность, которая зависит от и. Вот. Ну и когда, соответственно,
ρ умножится на какое-то неотрицательное число, которое строго больше, чем 1 вторая h с тильдой,
то соответственно у меня это все в бесконечности и улетит. Так.
Согласны? Хорошо. Так. Ну и в принципе на самом деле на этом мы с вами и заканчиваем.
Так как почему? Потому что у нас мы с вами знаем то, что у меня есть значение в точке x со звездой
и значение для любой функции вот здесь вот. Вот здесь вот. Должно быть меньше, чем значение f
от x со звездой. Вот. А получается так, что у меня, я какое бы вот и не взял, вот какое бы я,
какому бы у меня не равнялось f от x со звездой в силу того, что вот это выражение у меня стремится
к бесконечности, я всегда буду подбирать номеры так, чтобы у меня значение функции просто уходило
выше. Вот. Получили противоречие? Противоречие пока только тому, что у меня ограничения не
выполнены. А это значит, что вот на данный момент мы поняли что? То что у меня x с тильдой со звездой
удовлетворяет ограничениям. Раз. Вот. Плюс f в точке x с тильдой меньше, чем f от x со звездой. Меньше ли
баронотчимов от x звездой. Это значит, что у меня x tilde со звездой это решение
исходной задачи. Просто по определению.
Вот. Так. Хорошо. Ну и осталось дойти до финального противоречия.
В силу того, что у меня последовательность x tilde и t стремится к x tilde,
то когда-то я всегда буду оказываться в эпсилонокрестности точки x tilde,
которая лежит в пределах x звездой. Согласны? Ну и получается, что я буду
оказываться и вот в этом множестве. А мы сказали, что вроде как мы строили
последовательность изначально вот эту x situ, так что вот они не лежат в
эпсилонокрестности множества x звездой. Окей? Понятно здесь, да? То есть получилось,
что действительно для любого эпсила мы подбираем ρ и у нас все это лежит
компактненько. Вот эти два множества, они можно считать, там, эпсилон совпадают.
Хорошо? Хорошо. Так. Чуть-чуть быстренько обсуждаем, что мы поняли про метод
штрафов. Супер-плюс это то, что условная задача становится безусловной. Поняли то,
что с увеличением ρ приближаемся к исходной задаче тоже понятное хорошее
свойство. Вот. Но это же это же свойство в некотором смысле и нехорошее, потому
что, во-первых, а мы выходим за пределы множества и это происходит ну вот для
любого, чаще всего происходит для любого ρ неравного плюс бесконечности, а на
практике это всегда так. Вот. И, возможно, для каких-то задач просто так нельзя
делать, ну, в силу постановки. Просто какие-то у вас ограничения задают, например,
ресурсы, которые просто нельзя менять в плане того, что выходить за их
пределы. Вот. Ну и, соответственно, с точки зрения вообще решения задачи вот
этой штрафной, нашу штрафную функцию, увеличение ρ это вообще ни разу не
хорошо. Как вы понимаете, с увеличением ρ у вас просто растут некоторые
свойства данной задачи, например, константа липщицы градиента. Даже несмотря на то,
что, например, в ашке у вас может, аш у вас может представлять собой что-то в духе
ax-b, когда вы это возведете в квадрат, у вас там выскочит вот это ρ перед ним, ну и,
соответственно, вы начнете раздувать эту задачу, увеличивая ρ. При этом все еще
будет болтаться исходная задача. То есть, которая, например, вот эта моя задача может
быть у вас μ сильно выпуклой и l гладкой. Вот. Для этой задачи вы там с помощью
оценки спектра можете вычислить, что, например, она там лямбда гладкая и,
например, не сильно выпуклая. Вот. Не сильно выпуклая, просто потому что матрица не
очень хорошая. Вот. И тогда что, у вас константа просто липшится, будет расти
градиента. Вот. А константа μ при этом меняться не будет. Потому что она тянется из
исходной функции, которую вы не меняете. Вот. А тогда будет расти сложность там
градиентного спуска, которой вы будете решать штрафную задачу, ну или там
ускоренного градиентного спуска. Вот. С увеличением ρ. Ну а как мы поняли, ρ
увеличивать вроде как надо, чтобы получать более качественное решение. И вот здесь
в некотором смысле это борьба. Что мне делать? Брать большое ρ и, соответственно,
обузить мою исходную задачу, ну вот, страфную задачу. Вот. Либо брать маленькое ρ. Вот.
Получать хорошую задачку с точки зрения вычислительных вопросов. Но вопрос,
тогда возникают уже вопросы к качеству решения. Насколько она
удовлетворяет ограничениям. Так. В этой части все. Вопросы.
Если нет, тогда перерыв. Так. Ну что, давайте продолжать. Давайте продолжать. И сейчас
как раз разговор пойдет про второй метод, который сегодня хотелось бы
рассмотреть. Он супер популярный, все еще используется во множестве оптимизационных
библиотек. Ну давайте, во-первых, поймем откуда вообще берется он и, соответственно,
его интуиция. Смотрим на вот такую задачку. Тут я даже упростил. До этого у нас были
произвольные ограничения. Теперь у нас ограничения типа равенств. Хорошо. Так. Пишем
лагранжан. Лагранжан. Вот. Из лагранжана, как вы знаете, можно получить двойственную
функцию. Вот. И, опять же, если у вас есть выполнено условие слейтера, пожалуйста,
решение двойственной задачи более чем... Точнее, максимизация двойственной
функции, решение двойственной задачи действительно вам даст какие-то хорошие
результаты. Ну и, соответственно, да. Давайте попробуем сделать что-то в духе градиентного
метода, но не для исходной задачки или для экстраградиентной методы, как решили лагранжан,
а что-нибудь для двойственной задачи, для двойственной функции. Определение двойственной
функции, соответственно, выписано. Дальше что? Раз так как мне двойственные функции нужно
максимизировать, я делаю тоже как бы градиентный метод, только вместо спуска я делаю подъем. Иду вверх.
Шаг альфа. Ну и, соответственно, градиент, он тут берется по лямдо. По лямдо понятно,
потому что единственная переменная функции g это лямдо. Но просто функция g выглядит не совсем
очевидно, то есть это вот минимум какой-то от лагранжана. Ну давайте чуть-чуть это попробуем
попереписывать. Это с прошлого слайда. А теперь я сделаю вот так. Я скажу, что вот xкт это просто
вот arg-минимум, то есть то значение x, на котором достигается вот этот минимум. Вот, я этот arg-минимум
нахожу и, соответственно, могу его подставить теперь вот в то выражение, которое у меня было
здесь. Вот. И тут уже минимум пропадает. Согласны? Вот. Просто чуть-чуть переписал, дополнительным шагом
сделал эту минимизацию в явном виде. Нашел значение xкт плюс один, на котором это все безобразие
достигается. Вот. И подставил его в нашу задачку уже в шаг градиентного метода по двойственной
функции. Все. Вся идея. Ну, соответственно, такой метод называется, соответственно, у нас двойственный
подъем. Двойственный подъем. Популярный, понятный метод. Тут еще чуть-чуть переписал, потому что видно,
что вот я этот градиент беру по лямбда. Вот это у меня от лямбда не зависит. Ну, а все, что осталось,
соответственно, оно и влетело в градиент. Вот оно. Окей. Все. Двойственный подъем. Двойственный подъем.
Во. Это вопрос важный. И тут, соответственно, два варианта. То есть в ADMM это тоже вопрос возникнет.
Первый вариант. То, что у вас функция f является дружественность с точки зрения вычисления
аргминиума. Ну, что это значит? Это то, что вы этот аргминиум просто считаете бесплатно. Вот. Такой
концепция мы сегодня в доказательствах и будем придерживаться. Ну, в единственном доказательстве,
которое осталось в сходимости ADMM, что вот функция просто, когда мы считаем этот аргминиум, в явном
виде дает вам ответ. Вы можете его там выразить. Функция f простая просто-напросто. Так. Второй
вариант. Второй вариант. То, что вы этот аргминиум честно считаете каким-то методом оптимизации. И
тогда при анализе уже всего метода, который у нас здесь есть, двойственного подъема, вам нужно
учитывать, что вот этот XCAD у вас выплевывается с некоторой ошибкой. Что это нечестный аргминиум,
а это некоторая посчитанная с помощью численного метода версия этого аргминиума. Ну, и у вас есть
какие-то гарантии, что XCAD и плюс один отличаются от реального аргминиума не сильно. Вот. На
какой-нибудь там эпсилон. Ну и, соответственно, эта ошибка у вас тянется и, понятно, будут влиятельны
итоговый результат. Мы пока предполагаем, что вот все дружественно, аргминиум мы считаем честно,
в технические детали, как учесть эту ошибку, мы уже не полезем. Вот. Чуть-чуть модифицирую исходную
задачу. Поняли мы уже с вами то, что вот такого рода модификации делаются у нас за бесплатно. Она
никак не влияет на нашу задачу. Вот. Всего того, что на множестве ограничений просто такая добавочка,
это ноль. Вот. Пожалуйста. Градиент, вот у этого аргминиума мы градиент не считаем. Градиент
вот этого, вот это. Смотрите, мы решаем задачу минимума. Вот. У меня есть какой-то X, который
просто дает этот минимум. Ну, это же задача оптимизации. Задача оптимизации. Есть значение
минимум, а есть значение аргминиум, которое дает это минимум. Ну, пусть это... Я это обзываю
xкт плюс один. Вот. Ну и, соответственно, что? Если я знаю значение... Вектор, на котором минимум
достигается, я его могу подставить вот туда. Но я его подставляю. Вот. И все. И получаю как раз
минимум. Вот. Значение g в точке λкт, который... А почему должно быть по-другому? Потому что это
же какое-то значение. Каким оператором я действую потом, мне абсолютно неважно. Я вот вычислил значение
функции здесь. Так. Ну и все. Зависит от лямбды. Да, может быть, тут надо чего-нибудь еще выдумать. Да.
Скорее всего, тут все корректно, потому что там есть какие-нибудь теоремы в духе Демья
Новоданскина, где можно менять градиенты минимум местами. Вот. Ну давайте будем считать,
что вот так нам это просто нужно для некоторой интуиции. Вот. Я подумаю, что там нужно четко,
чтобы четко это все выражалось. Вот. Скорее всего, нужно какие-то действительно условия докинуть. Ну
смысл вот такой вот. Находим x ката, делаем шаг. Так. Хорошо. Про добавочку. Добавили вот
эту трошку. Ни на что она не влияет. Лагранжан чуть-чуть поменялся в связи с этим. Добавилась вот
эта вот дополнительная... дополнительный кусочек. На самом деле, его добавляют обычно не для того,
чтобы как-то в теории что-то получить. Это скорее вот выполняют функцию регулизатора, чтобы задача,
например, стала иметь более классные свойства в плане сходимости. Вот. Чтобы обеспечить
устойчивость сходимости, потому что исходная задача, она просто выпукла вогнуто, если у вас
функция f выпукла. Вот. И какие-то свойства сходимости действительно могут, ну, быть не самые классные в связи
с этим. Добавляю вот этот регулизатор, чтобы просто улучшить качество сходимости на практике. Вот.
В точке зрения теории, это не особо нужная вещь. То есть, в первую очередь, вот такое
добавление, которое называется агментация, это практический трюк. Вот. Ну и, соответственно,
для него тоже можно записать вот двойственный метод типа двойственного подъема. Только здесь
единственное, что меняется a. Lagrangian на уже аугментированный, вот с этой добавочкой. Плюс
специально, специально меняют шаг. Вот. Был какой-то шаг альфа, берут его равным ro, где ro это вот ровно
параметры агментации. Ну, просто так делают, чтобы избавиться от двух параметров. Потому что ro это
в некотором смысле параметр вашего метода, насколько нужно его взять большим, маленьким и так далее. Вот.
Плюс шаг еще, это параметры метода. Два параметра подбирать, в принципе, уже не так приятно,
поэтому почему бы их не объединить и решают сделать вот так. Ну, вот такой вот метод, двойственный подъем.
Нам, в принципе, он особо и не нужен сейчас, просто чтобы понимать идею, откуда это все безобразие
берется. Вот. А то, что нам нужно сейчас, это вот такая вот задача. Видно, что она становится сложнее. То
есть, тут появляется вторая группа переменных. Это y. То есть, теперь есть не только x, по которым мы
минимизируем, есть еще перемены y, по которым мы также минимизируем. И ограничения становятся более
сложными. Это уже сумма двух. Вот. axby равно c. То есть, вот такой вот длинный вектор у меня появляется.
На самом деле, если покопаться в применениях как раз ADMM метода, то становится понятно, откуда
берутся вторые функции здесь. Ну, например, давайте я просто какую-нибудь так на скидку сделаю задачку,
чтобы было понятно. Не знаю. Скорее всего, вы на семинарах даже сталкивались с чем-то таким в духе того,
что возникала какая-то вторая группа переменных для минимизации, по которой нужно было
минимизировать. Она возникает чаще всего искусственным образом. Ну, вот какая-нибудь классика
машинного обучения. Функция потерь, линейная модель плюс регулиризатор. Я думаю, уже в некотором смысле
вы с таким знакомы. Ну, что у нас тут? Действует просто. Data set умножается на x, получается линейная
комбинация. Дальше штрафуете с реальными лейбами. Как вот на такую задачу можно посмотреть
чуть по-другому. То есть, вообще, это, безусловно, задача. Вы ее, в том числе, в домашнем задании
решали. Безусловно. Но что я могу сделать? Я могу сделать вот так вот. Я могу ввести дополнительные
ограничения. Ну, давайте я вот сюда y напишу. Вот y, x у меня остается из Rd, y вводится новый,
не обязательно, даже из Rd, давайте пусть будет Rn. Вот. И добавляется ограничение, что у меня y равен
ax. Вот. Видно, да? Ну и, соответственно, появляется вот такая вот задача, которая написана здесь по
факту из исходной задачи машинного обучения, что довольно просто. На самом деле, вот такого рода
задачи, они возникают очень много где. То есть, там прямо целая книжка есть про то, как вот можно
разные задачи переформулировать под вот такую, чтобы решать АДММ. Это один из примерчиков. И
часто, действительно, y это просто вспомогательная группа переменных. Ну и тут, кстати, видно то,
что у меня вот эти функции f и g, действительно, могут быть довольно дружественными. Потому что l и
r, ну это какие-то, l это loss функция, r регулиризатор. Чаще всего это простые функции. Вот. Когда вот,
соответственно, l уже идет в придачу вместе с матрицей A, там не так все хорошо. Вот. Понятно,
что там за матрица A меняются свойства, а теперь мы как бы вытащили матрицу A в ограничение. Ну и,
соответственно, у меня теперь l болтается одна, и я могу все ее хорошие свойства, она, например,
тоже может быть обычная квадратичная там какая-то функция в духе там y-b в квадрате. Вот. Я
ее могу использовать. Она действительно будет дружественной с точки зрения вычисления каких-то
аргминемов по ней. Так. Это, соответственно, раз. Дальше агментацию добавляем. Пишем Lagrangian. Вот.
Легко проверить, что такой Lagrangian порождает выпукловогнутую задачу, если у вас, давайте допишу,
f и g выпуклые. f и g выпуклые. Вот. Получается выпукловогнутая седловая задача. Ну окей. Хорошо.
Это, в принципе, мы с вами видели и для более простой задачи без агментации и без. Это проверять
не будем. Можно как упражнение это сделать. Так. Так. И, соответственно, вот так вот выглядит метод
ADMM. Та же самая идея, что в двойственном подъеме. По переменным минимизации как бы делаем минимум,
вот. А потом делаем шаг спуска по переменной максимизации. Вот. Только вот здесь вот ключевая
особенность, почему он и называется ADMM, что у вас здесь Alternating Direction. Alternating Direction
методов multipliers. Multipliers понятно, потому что у вас двойственные множители. А Alternating Direction,
потому что вот в двойственном методе у нас была минимизация по x, здесь у нас минимизация по x,
y. Вот. И по факту, если переносить двойственный метод прям дословно, то вы должны вот вместо вот
этих двух строчек, которые я вот здесь вот написал, вот этих двух строчек заменить их на одну строчку
поиска минимума по x и по y одновременно. Вот. Но здесь поступают в некотором смысле и 3,
упрощая задачу, вы сначала минимизируете по x при фиксированном y и лямбде, а потом минимизируете
по y. Вот. И получается, вот это как-то называется альтернированная минимизация, когда у вас там
функция зависит от двух групп переменных, и вы сначала по одной минимизируете, потом по другой,
потом еще раз по одной, по другой. Но здесь просто сначала по одной, потом по другой,
поэтому он и называется Alternating Direction. Потому что вот есть у вас два этих шага сначала по x,
потом по y. Окей. Вот. Так вот выглядит АДММ. Немного страшная идея. Такая с точки зрения практики
понятная. Вот. И действительно это классно работает. Сейчас попробуем доказать, как это все безобразие
сходится. Вот. Буду доказывать чуть-чуть другую версию. Что-то у меня не схлапывалось вчера
от доказательства предыдущего алгоритма. Я чуть-чуть поменял строчки. У меня до этого в предыдущем
алгоритме x вот здесь вот был. Вот. Я сначала по x делал минимизацию, потом по y, потом делал шаг по
лямбде. Здесь, ну на самом деле ничего особо и не меняется. Я сначала по y делал минимизацию,
потом шаг по лямбде. Вот. Потом минимизацию по x. Но это же по факту то же самое. Если я цикл запущу,
вот у меня что прошло? y, лямбда, x. Окей. Но я же могу как говорить? Вот так. Сейчас, например,
нахожусь на минимизации x, перехожу на следующую итерацию, буду уже минимизировать по y и потом
по лямбде. Но это ровно то же самое, что было. Это только в начальных итерациях будет работать
чуть по-другому, потому что вы сначала обновляете y, потом лямбда, потом x. Но по факту, если правильно
инициализировать там предыдущий алгоритм или этот по-другому инициализировать, то они совпадут.
Я просто чуть-чуть поменял порядок, но суть-то осталась та же. Вот. Какие действия, как цикли
выполнять, тут особо это не важно. Что у меня просто не получалось с предыдущим, здесь получилось
полегче. Вот. Покрасивше доказательства. Давайте я вам вот это оставляю. В принципе,
это в презентации у вас есть, и мы сейчас пойдем доказывать. То есть, метод видите,
Lagrangian видите. Потому что на слайдах у меня этого не доказательства нет. Будем, соответственно,
все это доказывать сейчас в режиме реального времени. Поехали. Давайте что-нибудь поделаем.
Давайте вспомним вообще, что происходило в доказательстве, ну вот, например, какого
текстра градиентного метода или градиентного спуска. Но у нас же вот что-то вылезало в духе f от x,
ну там kt, например, xkt минус x звездой, ну или здесь просто x стояло, а справа было что-то в духе
xkt минус x в квадрате минус xkt плюс 1 минус x в квадрате. Так? Ну, в качестве оператора f у вас там
либо был просто градиент f от xkt, либо градиент по x, антиградиент по y, например. Ну вот,
что-то вот такое, да? Было. Теперь вот хочется получить что-то вот то же самое. Вот такого рода
результат, похожий вот на вот это, но для DMM метода. Ну, соответственно, что? Если не знаете,
что делать, в принципе, для градиентного спуска это тоже в стратегии работает. Берете строчку и
пишете для нее условия оптимальности. Просто для градиентного спуска там условия оптимальности
считаете записаны за вас, а тут, видите, вы аргуминиум должны искать. Должны искать аргуминиум.
Вот, у нас в первом случае по y, да. По y давайте писать условия оптимальности для вот строчки
с y. Эта строчка номер два. Как она будет выглядеть, кто понимает условия оптимальности? Градиент равен нулю, алгорит
перед вами. Так. Так, на обложе от y, дальше что? Там еще РО болтается, вроде
как? Нет? Или не болтается? А, там с лямбдой? Да-да-да-да-да. На обложе, да, все правильно. Сначала,
дальше что у нас там? B транспонированная лямбда, все правильно. Дальше, дальше с РО поехали, да. РО. Что
там будет? С РО. Кто понимает? Там квадратичная просто задачка, как из нее градиент быстро взять.
B транспонирована бы скакивать из скобки, как множитель просто перед y. А дальше скобка
просто остается. А в скобке у нас, соответственно, ax плюс by минус c. Это просто условия оптимальности
записано, только тут единственное, что нужно выставить, это написать альфа каты, x каты. Вот. Но, и оно
все равно нулю. Все понимают, что произошло. Я просто для строчки, где выбираем argmin по y,
написал условия оптимальности для этой задачи, приравнял градиент к нулю. Но это же условия
эквивалента тому, что как раз оно выполняется для y катова плюс один, так как он же argmin у нас.
Согласны? Так. Дальше по лямди. По лямди там довольно просто, потому что там,
видите, там все записано, я просто вот так и оставлю. Давайте я выпишу вот так вот. Лямда к плюс
один минус лямда к равно ro axk плюс by к плюс один минус c. Так. И аналогичную вещь мне
надо еще записать для x. Там по x мы тоже argmin берем, поэтому и записываю для него f xk
плюс один. Так. Дальше соответственно что? Будет выскакивать а транспонированная лямда
к плюс ro а транспонированная axk плюс бета. Так, только тут надо не ошибиться. Лямда к плюс один там.
Y к плюс один минус c равно 0. Все? Понятно здесь, да? Просто я выписал для каждой строчки условия
оптимальности. Для строчки с лямдой я просто вообще там ничего не выписал, приравнял. Аргумен?
А, да, да, да. Вот. Здесь окей? Так. И теперь вот хочу чуть-чуть поиграться. Давайте мне для
этой задачки, чтобы привести к тому виду, который у меня наверху, нужно найти вот оператор f.
Оператор f, который там, ну давайте я от z буду обозначать, z у меня пусть будет это вектор x,
y, лямда. Вот. Оператор f в данном случае это просто градиент по x, так градиент по y и
антиградиент по лямду. Это ровно то же самое, как мы с вами делали для экстраградиентного метода. Вот.
Там потом, когда вы эти градиенты подставите, вот вы для этого потом просто выпуклостью воспользуетесь,
или выпуклостью-вогнутостью, у вас там все красиво будет схлопываться, будете получать значение
просто разности функций. Вот. Окей. Здесь я выражаю, что градиент по x это будет у меня,
что градиент f от x плюс a транспонированная лямда. Хорошо. Так. И что-то еще. А, давайте я вот так
еще. Давайте тут без rho. Вот давайте пусть будет обычный Lagrangian, без rho. Вот так. По лямдию
у меня будет чему равен градиент? Это будет просто ax плюс bi, а нет, это по y, по y у меня надо
сначала по y, там будет nabla g от y плюс b транспонированная лямда. И по лямдии антиградиент это
минус ax плюс bi минус c. Вот. Важная деталь, что здесь я убрал вот эту рожку и смотрю на Lagrangian
без рожки. Потому что по факту рожка, как мы понимаем, она в реальности на решение исходной
задачи не влияет. А мы когда делаем именно, решаем этот Lagrangian, мы ведь не штраф вводим,
то есть мы реально решаем исходную задачу и хотим получить решение исходной задачи. Вот. Я
убрал здесь рожку. Ну и, соответственно, да, мне хочется теперь, видимо, получить,
так как вот здесь вот у меня возникают f от xкат и градиент, g yкат и плюс один. Но я хочу,
видимо, получить f от zкат и плюс один. Вот. Вот его скалярное произведение на что-то. Лучше вот
на что-то вот такое. Так. Хорошо. Хорошо. Давайте попытаемся это сделать как-то по группиру,
перегруппируем. Вот. В принципе, вот для x уже что-то вырисовывается. Такое вот хорошее. f
xкат плюс один плюс а транспонированная лямбда ка плюс один. Так вот равно. Так. Равно
ро а транспонированные а икс один плюс б у ка плюс один минус ц. Так.
Дальше мне нужно получить по y. По y у меня g y ка плюс один. Но вот тут с b не совпадает. Тут
не лямбда ка плюс один стоит. Вот. Поэтому я добавлю вычту. Вот здесь поставлю b транспонированная
лямбда ка плюс один. Но вот тогда справа мне нужно будет это компенсировать. Я все еще напишу
ро. Ро у меня точно это остается. Ро b транспонированная а икс ка плюс б у ка плюс один минус ц. Вот. И
тут нужно добавить и вычесть. То есть вот b транспонированная лямбда ка плюс один минус лямбда ка. Вот.
Это компенсация за то, что я вот здесь вот поменял точку. Справа да да справа давайте вот здесь
поставлю тогда вот минус вот сюда. Вот. Хорошо. Так. И мне осталось чуть-чуть опять же поиграться вот
с этим. С градиентом по лямбда. Чтобы у меня был антиградиент по лямбда. А это должно у меня
вылезти соответственно минус а икс ка плюс один. Плюс в скобочках это все так. Ага. И теперь это
так так так так так. Что там вылезет один делить на ро. Лямбда ка плюс один минус лямбда ка.
Так вроде все норм и соответственно из-за того что я вот здесь менял точку мне ее нужно компенсировать.
Вроде бы. Правильно. Так. Так. Хорошо. Вот такого. Какое-то вот такое вот у меня выражение получилось.
И смотрите какую вот тут замечается такая особенность. Такая особенность которая на
самом деле возникает в градиентном спуске. Мы просто это так не расписывали. Тут вылезают разности
соседних значений. Вылезают разности соседних значений. Вот единственное что вот вот с этими
выражениями до конца не понятно что делать. Потому что там пока вот такое вот такая красота не
вылезла. Но на самом деле ее можно сейчас повытаскивать. Этим мы и займемся. Ну давайте
сейчас как мы это сделаем. Как мы это сделаем. Споргалку свою гляну. Все понял. Так. Делается
следующее. Делается следующее. Смотрите. У нас мы знаем что лямбда к плюс один минус лямбда к это
РО АХК плюс БЙК плюс один минус С. Вот. А здесь то по факту это же и возникает только ущел
умноженное на B транспонированное. Поэтому здесь меняемся просто. Так да меняем вот этот выражение на
лямбда к плюс один минус лямбда к. Вот. Так. На РО. Так так так. А убрать РО да. Убрать РО. Все. Поменял.
Осталось верхнее выражение. Там чуть чуть более проблемно конечно. Там опять нужно чуть-чуть
точку поменять. ХК плюс один плюс БЙК плюс один минус С. И тут из-за того что я точку поменял у меня РО АХК
минус ХК плюс один. Вот. Вот такое вот выражение. По факту у меня вот это там стоит умножить на
транспонированные. А мне нужно будет соответственно это все поменять на
А транспонированные лямбда к плюс один минус лямбда к минус А транспонированные РО АХ минус ХК плюс один.
Согласны?
Ну вот что-то вот такое. Что-то вот такое. Вся соль теперь вы видите у меня везде выскакивают вот эти
разности. Вот эти разности. Давайте я это перенесу на следующую страничку. И мы с этим красиво обойдемся.
Красиво обойдемся.
Вот. Я хочу вот это все безобразие записать в виде матрицы. Я хочу это все записать в виде матрицы.
Так. Ну давайте попробуем это сделать. Так. Матрица П и атона умножается на вектор ХК плюс один
минус ХК на вектор YК плюс один минус YК на вектор лямбда к плюс один минус лямбда к. Вот. Надо
найти сейчас вот эту матрицу. Найти вот эту матрицу. Ну давайте искать. Так. Ну что у меня тут?
Как у меня все безобразие умножается? В первой строчке у меня вылезает лямбда и вылезает разница
по Х. С Х она вылезает положительная, а транспонированная АРО. Так. Дальше по Y ничего там не вылезает,
по лямбдом вылезает отрицательная. А транспонированная. Во второй строчке у меня лезет только разница по
лямбдом. Да? По лямбдом. Я правильно сейчас записал? Там за нуляется. Ну ладно, пока напишем,
как есть. Ноль, ноль, по лямбдом. Там получается два бета транспонированное с минусом. И в последней
строчке у меня опять вылезает А, ноль, один делить на РО и на единичную матрицу. Так. Вот что-то типа такого,
что-то типа такого. Кажется, я где-то вот накосипорил, вот здесь должен быть ноль. Вот где-то я со
знаком напутал. Где-то я чуть-чуть напутал со знаком тут. Так. Это я откуда убрал? Для Y я записывал,
подставил. Здесь, зря стер, конечно. Так, подставил. Ну вот здесь, видимо, где-то минус должен быть.
Потерял этот минус. Вот этого множителя не должно быть. Так, ладно. Поверим нас, что тут ноль.
Получается вот такая матрица. Получается вот такая матрица. И на самом деле это уже,
считайте, финал доказательства. Вот. Потому что что у нас получилось? У нас получилось, что f от x
ка плюс один. Ну это очень похоже на то, что мы всегда получали так. Равно p на разницу. Давайте только
не x, а z. Тут z у меня. Полный этот вектор. Это вектор из х, у и лямб. Вот. Как раз у меня вот это f
z ка плюс один. Он равняется, как мы поняли, матрицу умножить на просто на превращение z ки.
При а, вот здесь, да? Во, правильно. Это правильно. Вот. Супер. Так. И здесь я еще должен скалярно
умножить на z ка минус z. Вот. Хорошо. Хорошо. Так. Дальше что делается? Дальше что делаем? Дальше вот
это вот то, что у меня написано, расписываем чуть похитрее. Ну то есть это вот на самом деле скалярное
произведение, вот которое второе, оно просто выражается через нормы. Ну точнее, такие немного
хитрые нормы. Вот. Давайте я запишу выражение, и мы просто увидим, что оно действительно выполняется.
z ка минус z на p. Так. Минус одна вторая, z ка плюс один минус z, p, z ка плюс один минус z,
и здесь соответственно минус одна вторая, да? Все правильно. z ка минус z ка плюс один,
p, z ка минус z ка плюс один. Вот. Договорились? Договорились. Давайте посмотрим, что это
действительно так, что тут я вам нигде сильно не наврал. Так. Какого рода тут вообще слагаемые могут
встречаться? z на p на z. Вот. Было ли оно у нас до этого? Такого у нас до этого не было. Но оно и
уничтожится. Вот оно соответственно уничтожается. Вот здесь вот. Так. Вот. Времени просто не так
много остается. Вот давайте вот это утверждение, оно просто в качестве упражнения. Тут действительно
ничего такого сверхъестественного, это нужно просто раскрыть аккуратненько. Вот. Смотрите,
смотрите, что еще хорошо, вот эта матрица положительно-определенная. Вот. Тоже можно
спокойно проверить, что она действительно является положительно-определенной. Ну что у вас там,
если эти нули убрать, вот этот крест нулевой, у вас останется матрица 4 на 4, а транспонированная а
минус а транспонированная а. Ну этого ноль, да, в детерминате. Минора у нее положительная. Вот. Она
действительно положительно-определенная, а значит с помощью вот этой матрицы можно индуцировать
норму. Вот. И она на самом деле здесь индуцирована. Вот. Потому что, видите, вот здесь вот. Вот это
выражение, которое написано. Вот это. Я же могу, например, взять корень из матрицы и вот так вот ее
раскидать. Она еще и симметрична у меня. Вот. И сделать что-то типа такого. То есть получается,
что это просто необычная евкридовая норма, а такая норма, которая порождена матрицей. Вот. Обозначается
это вот так вот, это ее просто определение. Матричная норма, например, вот. Которая порождена матрицей
а. Норма порожденной матрицы. Это вот просто ax. Так вот. Вот. На эту матрицу мы просто умножаем. Вот.
это оно и есть. И здесь соответственно в чем мне вылезает zk минус z по норме p в квадрате
минус zk плюс один минус z по норме p в квадрате и минус zk плюс один минус zk в квадрате по
норме p. Окей? Вот. Ну вот такое вот выражение получается. То есть видно, что уже получили
что-то очень такое прям то, что хотелось. Вот это оператор, столярное произведение оператора на
точке, на нужденном разности, а здесь разность норм, разность норм, то есть там kt минус k плюс
первая и что какая-то вообще неположительная, которую мы можем просто убрать и сказать,
что нет. Потому что оценка же сверху пишется. Вот. И я могу здесь просто поставить знак меньше
либо равно. Вот. Все. То есть получили то, что мы обычно получаем. Так. Дальше. Дальше делаются
стандартные шаги. Кто помнит, как там вот f превращалась для седловых задач в разности? Кто
помнит? Что там нужно было сделать, когда мы экстраградиент доказывали? Вот как мы дальше
расписывали вот этот для седловой задачи вот эту f? Да. По выпукловогнутости. То есть мы говорили,
что у нас по x было выпукло, поэтому там возникнет разность нужная. Так. Ну давайте я здесь тогда
напишу l, x. Давайте я вот так вот, для вот этой я напомню, как делается. Вот для нашего случая
там аналогичность. Все это проделывается. x, x. Давайте kt и лямбда, kt, kt, kt, kt, x. Здесь лямбда,
kt, лямбда. Вот у нас вот что-то вот такое. И мы, соответственно, это снизу. Думаю, просто. И мы снизу
это оцениваем по выпуклости. По x у нас функция выпукло. Как мы это можем оценить?
Как l, x, kt, лямбда, kt, минус лямбда, x, лямбда, k. Так, это просто выпуклось по x. Я первую строчку
расписал. Так. Вогнутость по y, вогнутость по, ой, не по y, а по, соответственно, лямбде. По лямбде
она что даст мне? x, kt, лямбда. Так, минус x, kt, лямбда, k. Это вогнутость. Вот здесь вот. Вогнутость.
Выпуклость. Вот. И что в итоге это дает? Это дает то, что у вас уничтожаются вот эти кусочки. Остается l,
x, лямбда, k, с минусом плюс l, x, k, лямбда. Вот. А дальше мы как? Максимизировали по лямбде,
минимизировали по x и получали критерий gap. Да? Зазор. Окей? Согласны? Кто помнит, что мы действительно
так делали, когда доказывали там для экстра градиента? Вот. И здесь то же самое. У вас возникает вот
это же выражение здесь. Вот оно. Вы его расписываете. У вас получается здесь l по переменной
максимизации здесь x, kt, y, kt. Только с плюсом. Плюс один. Плюс один. Лямбда. И, соответственно,
здесь l, x, y, лямбда, k, плюс один. Вот. Это слева, справа. Что вы, соответственно, дальше делаете?
Суммируете. Суммируете. По всем k. От нуля до k минус 1. Внизу все уничтожается, кроме нулевого и
последнего. Последний с отрицательным знаком. Соответственно, его просто убираем. Остается вот
такая вот разность z нулевого минус z в этой норме. Раскрываете, соответственно, по эту норму,
по определению. Все. И все красиво получается. Получается сходимость. Здесь у вас тоже все
суммируется. По янсону, по выпуклости, вогнутости вы приходите к средней точке. Вот. Показываю теорему,
как выглядит. Вот она средняя точка. Вот она. Z квадрате. Матрица та же самая. Матрица,
я знал, как выглядит. У меня был этой беты. Lagrangian нулевой, то есть без рожки. Понятно, да? Ну и
кратенько про ADMM. Это действительно метод, который является ключевым во многих солверах, в том числе,
который вы будете проходить в рамках этого курса. Более того, во многих солверах он является
дефолтным. То есть, если вы не указываете конкретный, он использует ADMM. Связано это с тем, что вот та постановка,
которая вроде кажется не особо козистой, с X, с Y, она на самом деле действительно включает
в себя очень много частных случаев. Вот. И поэтому ADMM реально покрывает большой спектр задачек,
которые необходимо решать на практике. Именно вот такой вот выпуклой оптимизации с ограничениями.
Вот. Ну и такая, что ли, ключевая его особенность, то что вот здесь вот штраф-то он все же не как
вот метод из штрихных функций. Это прям так необходимая вещь, которую нужно устремлять в
бесконечности и делать большим. Здесь это вспомогательная вещь для ускорения сходимости.
Потому что по факту вы решаете задачу Lagrange, вы решаете седловую задачу и вы могли бы ее решить
и без вот этого дополнительного штрафа. Штраф это регулиризатор здесь, бесплатный регулиризатор,
который не портит саму задачу, но улучшает именно сходимость на практике. Вот. Вот это вот такая
ключевая особенность и отличие от тех штрафов, которые мы заработали с вами в начале лекции.
Все, спасибо. В конце немного сумбурненько, ну да ладно. Все.

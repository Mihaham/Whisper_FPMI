Так. На чем мы остановились? Мы жадный алгоритм обсуждали. Что сделали? Жадники же обсуждали.
Да, то есть вот мы остановились просто полностью завершив историю про, как называется, жадный алгоритм.
Про жадный алгоритм, про то, как можно считать сравнительно быстро хроматически число на случайном графе.
Но потом я сформулировал теорему, которая говорит о том, что если вам вложены те графы, которые не попали в почти все, то будет плохо.
Там бывают очень скверные графы. Да, на этом я остановился.
Но я хочу продолжить на самом деле исследование всех этих характеристик случайного графа и давайте я, наверное, начну все-таки с каких-то вещей, которые с одной стороны относятся к нашей науке, к дискретному анализу, то есть случайный граф, например, что такое, а с другой стороны, немножко напомню вероятность, потому что я все-таки понимаю, что у нас есть разные учебные планы и, например, люди, которые позиционируются как математики,
изучают сейчас в большей степени сигма-алгебры, нежели, собственно, вероятность, поэтому надо быть на каком-то общем базисе. Я правильно понимаю ситуацию?
Если вы считаете, что это автоматично.
Что говорите? Или все знают, например, что такое неранейство Чебышова?
Нет, все еще сигма-алгебра.
Все еще сигма-алгебра, ну вот я так себе это и представлял. Ничего страшного, мне совершенно не трудно.
Давайте так. Что такое случайный граф?
Ну, в обычном смысле слова, как мы его до сих пор употребляли, это понимают вроде бы как все.
Мы поняли, что это очень полезный объект, потому что вот, например, он работает с точки зрения подсчета каких-то сложно вычислимых характеристик.
Работает в том смысле, что демонстрирует возможность вычисления этих характеристик на почти всех графах.
Ну давайте чуть более отчет. Я уже не помню, давал я это определение или нет, я вот сейчас напишу.
Если оно здесь было в каких-то лекциях, ну я быстро напомню и все, иначе можно чуть подробнее.
Значит, модель, с которой мы будем работать, в общем виде называется моделью Эрзо Шейрэнии.
Вот скажите мне, такие две фамилии здесь в связке между собой звучали уже или нет?
Нет, был потом.
Мне кажется тоже, что не звучали пока, что, может где-то в другом месте я их говорил, а на этих лекциях нет.
Значит, эта модель, которая появилась на самом деле раньше, чем Эрзо Шейрэнии написали свои знаменитые статьи,
ну вот статьи стали знаменитыми, поэтому модель называется так.
Физики, между прочим, очень сильно обижаются и говорят, да их там рядом не стояло, мы это придумали за много лет так.
И в общем они в какой-то мере правы.
По-другому эта модель называется биномиальная и это всех примеряет.
Ну и еще иногда говорят, что эта модель классическая в том смысле, что она действительно такая уже старая, наиболее изученная
и очень важная с точки зрения в том числе и приложений тоже.
Модель очень простая, вот слово бином указывает на ее происхождение очень хорошо.
Давайте, так, ну все присутствующие знают, что вероятность определяется так.
Есть три объекта, один из которых Омега, это пространство элементарных событий, другой сигма алгебра событий и третья это вероятностная мера.
Вот это все ведь присутствующие знают?
Да.
Правда?
Да.
Да, ну отлично.
То есть в данном случае, ну давайте я пишу, Омега, f и p, как это принято писать, ну вы не пугайтесь.
Вот на самом деле все предельно просто можно и без этих, конечно, изысков, без колмогорова скажем так.
Значит Омега это множество всех обычных, то есть обыкновенных простых графов на заданном множестве из n вершин.
Давайте считать, что я на вершину, у нас просто числа от единицы до n, а вот ребра могут быть какими угодно.
Ну то есть мощность Омега, это как много раз у нас уже говорилось, это просто 2 в степени c и z подв.
Обобщение по сравнению с тем, что мы делали, скажем, на прошлой лекции, состоит в том, как мы определяем вероятностную меру.
f это, конечно, просто 2 в степени Омега, тут нет ничего умного, у нас конечное вероятностное пространство, поэтому кажется нелепым ограничивать себя в рассмотрении событий.
Любое множество графов мы считаем событием.
Не гоню?
Все отлично.
Вероятностная мера задается на элементарных событиях, то есть на графах, можно воспринимать ее, конечно, как вероятность того, что g окажется равным.
Тут все зафиксировано, вершины не случайные, но вот именно с такими репушками.
Но это слишком длинно писать, да в общем и в теории героятностей принято брать просто p от Омега маленького и все.
Поэтому вот это я зачеркну, чтобы вас не смущало, и напишу просто p, некоторое p.
Давайте его здесь зафиксируем число из отрезка 0.1 в степени мощность E умножить на 1 минус p в степени c из n по 2 минус мощность E.
Вот я так по-колмогоровски формально задал вероятностное пространство.
Так, дорогие друзья, вот вы, присутствующие здесь, понимаете, что это действительно схема Бернули просто, то есть биномиальная модель.
По сути мы что делаем? Мы берем полный граф на n вершинах, я условно нарисую k4, но в общем случае берем полный граф на n вершинах.
У нас есть вот эта чиселка p, это условная вероятность успеха, или вероятность того, что монетка ляжет кверху какой-то своей конкретной стороной, скажем, решкой.
p это вероятность решки, и если монетка при очередном бросании падает кверху решкой, то мы сохраняем ребро, а если падает кверху орлон, то стираем.
Согласны, что получится ровно вот такая вероятность каждого конкретного графа?
Можно больше не разжевывать, правильно?
Вот это корректно определенное вероятностное пространство, очевидно, что сумма всех таких штук это единица, это просто в чистом виде бином.
Ну и частный случай, когда p маленькое равняется 1 и 2, это частный случай, с которым мы до сих пор работали.
А вот это вот выражение, то есть что оно выражает?
Вот это.
Мы таким образом зададем вероятность каждого графа.
А p маленькое?
p маленькое это вероятность каждого отдельно взятого ребра, если хотите, но это уже пояснение, почему модель называется биномиальная или где там схема испытаний Бернули.
Вот это можете считать просто определением, то есть есть модель, которую принято вот так вот обозначать.
У нее два параметра, как у любой схемы испытаний Бернули.
Один параметр это n, но в данном случае n это не число испытаний, а n это количество вершин графа.
Число испытаний здесь c из n по 2.
Так, я честно говоря проглядел, кто задавал вопрос, чтобы смотреть на него.
Я задавал.
А вы задавали.
Ну сейчас все стало понятно?
Да.
То есть вот это вот формальное определение, можно забыть про то, что бросали монетку.
А дальше я пояснял, что ну конечно это надо воспринимать так.
Очень естественная модель.
Знаете, представьте себе такую сеть из серверов, разбросанных по всей стране или по всему миру.
И вот каждые два сервера между собой соединены в какой-то линии связи.
Тогда вот это p, можно считать даже не p, а 1-p.
Можно считать вероятностью возникновения помехи на линии связи.
Вот случайный граф, который образуется, это граф связей между серверами,
если реализовалось какое-то количество помех.
Мы считаем, что реализуются они взаимно независимо.
То есть практически смысл этой модели абсолютно по-моему понятен.
Больше того, вот эту историю про связность полученного графа,
про какие-то компоненты связности, мы обязательно будем рано или поздно обсуждать.
Это очень важная вещь.
Так, друзья.
Вот сейчас больше нет вопросов по модели?
Еще один.
Да.
У нас формы гениального распределения еще перед вот этими p, а 1-p еще ц-шка стоит.
Ну естественно, но мы же определяем вероятность конкретного графа.
То есть если у вас четыре вершины, и вам дан совершенно конкретный граф,
например, вот такой вот цикл, который я сейчас нарисовал,
то его вероятность будет просто p в четвертой на 1-p в квадрате.
Откуда тут ц-шка возьмется?
Другое дело, что если вы будете считать вероятность возникновения цикла,
то вам придется умножить на количество циклов,
которые можно построить на четырех вершин.
Потому что вы сложите все вероятности.
А, ну конечно, да, надо знаете что сказать,
что если А принадлежит вот этой сигма-алгебре,
то, конечно, вероятность определяется, как всегда,
как сумма по g принадлежащим А, вероятность этих g.
Ну это, я надеюсь, и так понятно.
У нас такая дискретная сигма-алгебра, на ней вероятность всегда так определяется.
Так.
Ну вот раньше я как-то в этом месте начинал рассказывать именно про связность,
но вы же услышали, что это практически значимая вещь,
связность там какая-то и прочая, серверы соединены,
или какие-нибудь, может быть, дороги проходят,
которые по какой-то причине могут быть перекрыты случайным образом, и так далее.
То есть это, безусловно, важная для приложений вещь.
Эту модель очень глубоко изучают,
и мы много чего про нее изучим.
Но сейчас я хочу сконцентрироваться все-таки на хроматических числах,
числах независимости и прочих подобных объектах,
потому что мы увидели, что модели случайных графов для них работают,
я это уже говорил в начале лекции,
и в прошлый раз действительно мы это демонстрируем.
Я хочу глубже разобраться в том,
как себя ведут вот эти характеристики на случайных графах.
Хорошо, товарищи? Будем разбираться.
Прежде чем разобраться, я вам расскажу
два замечательных неравенства из теории и вероятности,
или даже три,
которые у вас, ну, два из них точно возникнут в ближайшее время,
просто в соответствующем курсе.
Мне не кажется, что это дублирование информации,
потому что вам расскажут по-другому, я в этом уверен, совершенно,
ибо я их буду доказывать только в самом простом дискретном случае.
Может быть, это вам даже поможет воспринять то,
что я вам рассказываю на общих лекциях.
Хорошо?
Мне не жалко времени,
мне главное, чтобы вы умели этими инструментами
действительно пользоваться.
Ну, что такое случайная величина, все присутствующие знают, да?
Случайная величина в нашем понимании,
ну, какая там, х,
это просто любая функция из omega w.
Наверное, вас учили,
что случайная величина это не любая функция,
любая измеримая функция.
Правильно?
Нет, не учили?
А, ну, дискретный случай,
вот, случай, когда сигмал гипрополная,
то есть состоит из всех, вот здесь написано, вот здесь,
из всех возможных подмножеств,
это любая функция.
Можно не заморачиваться никакими страстями.
Если вас учили, что она измеримая,
не пугайтесь, в данном случае это просто любая.
Если вас не учили,
что я это сказал, вас еще научат.
Нормально?
Не кокнул пока что.
Так, ну, вот, любая функция называется случайной величиной.
То есть она понимается как случайная,
не в том смысле, что вот я вызвал случайного человека к доске,
измерил его рост,
и он получился случайным.
Если я уже вызвал человека к доске,
рост совершенно определенный,
но человека случайно вызывал,
поэтому и функция называется случайной.
Вот и все.
То есть тут такая есть удвоственность, что ли,
некоторое...
...заметание пыли под ковер, что ли.
Ну, хорошо.
Так, любая функция.
Теперь, у этой функции есть разные замечательные характеристики.
Например, есть вероятность,
с которой эта функция принимает какое-нибудь конкретное свое значение,
х маленькое, то есть х маленькое это вещественное число.
Если у нас ω имеет мощность меньше бесконечности,
а мы работаем ровно с такой ситуацией,
нам больше ничего не нужно,
у нас конечное вероятностное пространство,
ну, тогда, конечно, мы должны иметь посчитать вероятность,
с которой х принимает любое конкретное свое значение.
Это тоже никого не пугает?
Все нормально.
Я просто действительно не знаю, до чего вам дочитали в разных потоках.
Мне трудно судить.
Главное, чтобы было понятно, что происходит.
Вот, друзья, зачастую вот эту вероятность посчитать крайне трудно для конкретных ситуаций.
Чтобы вы почувствовали немножко, насколько это трудно,
я вам задам задачку,
которая звучит очень просто.
Нет, давайте сначала решим простое упражнение.
Вот мы работаем со случайным графом.
Сейчас давайте считать, что мы живем только в этом мире,
вот в том, который там описан.
Я вам предложу сначала такую величину.
х от g равняется число ребер в графе g.
Ну, то есть то, что мы и писали, собственно, как мощность е.
Но это же случайная величина, правильно?
Это функция от графа.
С какой вероятностью х равняется конкретному числу k,
где k какое-то целое число?
В каких вообще пределах может меняться k?
Правильно.
k это число от 0 до c из n под 2.
Чтобы точно всем было понятно, это число от 0 до c из n под 2.
Ну, и с какой же вероятностью х равно k?
Вот это как раз будет ответ на вопрос про ц.
По-моему, это отсылка к задаче.
Ну, ужасно то, что действительно надо написать c из c из n под 2 по k.
Почему-то даже наших продвинутых студентов пугает,
что из c можно взять тоже c.
c из c.
Можно с n записать c из n под 2.
Ну, можно, конечно.
Товарищ, дайте еще скобки!
Ну, слушайте!
Как можно таким кокнуть?
У нас просто испытаний.
c из n под 2.
Вот их столько.
Надо выбрать какие-то k конкретно, в которых реализовался успех.
То есть сохранилось ребро.
конечно, умножить на 1-p в степени c из n по 2 минус k. Я надеюсь, это совершенно
понятно, очень простое упражнение. Вот как раз это и появилось, про которое меня
спрашивали, где она? Вот она появилась. Вот, а теперь задачка. Пусть х от g это число
треугольников в графе g, ну то есть полных подграфов на трёхмершину.
У нас такой недавно был на семинаре. Прекрасно, ну я не знаю. И что, решили что-ли? Вроде да.
Я думаю, что вы под ожиданием. А я-то вам чего хочу сказать, вот почитайте
например, вероятность того, что x равняется 0. Ну то, что в графе нет треугольников.
Нет, ну смотрите, вот может быть пустой граф, в нем нет треугольников, да? А может
быть, например, двудольный граф, причем любой, не обязательно полный. В нем ведь
тоже нет треугольников. Но согласитесь, что с нашей точки зрения, у этих разных
графов совершенно разные вероятности. У двудольного куча ребер, а у пустого не
одного ребра, поэтому вероятность, если только p не равно 1 и 2, все разные, правда?
А их же потом складывать надо. Вот и подумайте, чтобы почувствовать, насколько все хитро.
Ну подумайте, подумайте. Я вас уверяю, что это совсем не просто, но вы это должны
сами почувствовать. Вы это должны почувствовать сами. Чуть-чуть я вам скажу, почему это
трудно, ладно? Чуть-чуть. Мне обычно говорят х, а давайте это так посчитаем. Это 1 минус
вероятность того, что x больше либо равно 1. Почему-то людям кажется, что вот это
посчитать проще. Почему людям кажется, что это посчитать проще? Вы, может, зря
смеетесь. Что такое x больше либо равно 1? Что значит, что x больше либо равно 1?
Это значит, в графе есть треугольники, правда? Люди думают... Сейчас, извините, пожалуйста.
Ой, сейчас буквально, секунду. Я могу перезвонить через полчаса? Ага. Так.
Но это вероятность объединения, то есть можно считать по параметру, что ничего не случилось.
Вот, правильно совершенно. Вот вы совершенно правильно понимаете сложность вопроса. То есть людям
почему-то кажется, что вот эта вероятность равна c из n по 3 умножить на p в кубе. Но это чушь.
Она этого не превосходит. Но не больше того. Почему кажется? Потому что здесь, на самом деле, вот в этой
вероятности, давайте я стрелочку поставлю, вот в ней стоит объединение по i, объединиться до c из n по 3,
ну каких-то событий a с индексом i. Каких? Мы нумируем все возможные тройки вершин графа,
и it состоит в том, что it по счету тройка образует треугольник. Друзья, такой темп успеваете? Все успевают?
Мы еще раз, мы нумируем все тройки вершин нашего графа. Вот у нас вершина от единички до n.
Мы их в каком-то произвольном порядке нумируем от единицы до c из n по 3. Событие it состоит в том,
что it по счету тройка образует треугольник. Понятно сейчас? Эти события очевидно пересекаются,
ну потому что бывают графы, в которых находятся несколько треугольников, товарищи. В чем проблема?
Поэтому говорить, что эта вероятность просто равна сумме вероятности этих событий нельзя. Сумма-то,
конечно, c из n по 3 на p в кубе, но это всего лишь верхняя оценка. Это не превосходит c из n по 3 на p в кубе,
что вероятность каждого из этих товарищей, безусловно, на p в кубе, 3 вебра присутствует и все.
Друзья, точно я не очень быстро говорю? Точно? Нормально? Это сложно, тут формулу включения-исключения
и фиг напишешь, но попробуйте, может быть у вас получится без. Ну в некоторых ситуациях асимптотику,
конечно, точно можно найти. А асимптотику вот этой вероятности при n стремящемся к бесконечности,
n число вершины, можно найти. Например, если p константа, можно найти. Ну это я вам сейчас порешаю все.
Вот и задачка на 5 минут. А я что сказал, что на 5 минут, я сказал вам напонять, что тут все сложно.
То есть если мы хотим найти вероятность того, что какая-то конкретная величина принимает некоторое свое значение,
ну это бывает сложно. Вот вы подумайте, увидите, что сложно. Больше я от вас ничего не требую.
Это не задача на экзамен, слушайте. Вы меня, наверное, так поняли, что вы сейчас ее порешаете,
я на экзамен вам за десятку поставлю. Нет, ну если порешаете, поставлю, может я не знаю.
Нет, ну просто понимаете, там можно скачать что-то из интернета, как я проверю, что вы сами решаете.
Ну такое не найдешь в интернете. Нет, нет, я просто хочу, чтобы вы задумались и поняли,
что искать распределение конкретных случайных величин в комбинатуре бывает совсем непросто.
Так, поэтому возникают удобные инструменты типа всяких вероятностных неравенств.
Я хочу, чтобы вот до вас просто вся кухня этого дела дошла по максимуму.
Вам это обязательно расскажут по бою курсов, которые сейчас читаются по основам вероятностей теории мер.
Но я думаю, что могут рассказать достаточно формат.
А я сейчас вот для конечного случая расскажу максимально аккуратно именно с точки зрения приложений.
Ну вот прямо вот эта вот ситуация про формулу включения и исключения, которая здесь возникла,
это и называется по большому счету неравенство Маркова.
То есть неравенство Маркова это простая оценка формы включения и исключения и ничего больше.
Но оно очень удобное, это мы сегодня увидим.
Что такое неравенство Маркова?
Я его докажу не только для случайных графов.
Давайте считать, что мощность Омега меньше бесконечности, вот это важно.
А уж случайный это граф или еще какое-то конечное пространство, это не важно.
Так, пусть х из Омега принимает только не отрицательные значения.
Р плюс я понимаю так, что там и ноль может быть, но отрицательных там нет.
Тогда для любого А большего нуля, вероятность того, что х больше либо равняется А.
И тут я вспоминаю, слушайте, а мат ожидания-то все проходили что такое или тоже не все знают?
А, ну давайте я быстро напомню.
Да, да, что-то мне в голову пришло.
Ну так вам построили курс, ничего страшного.
Так, дорогие друзья, поднимите руки, кто не знает, что такое мат ожидания случайной величины, формально не знает.
Ну, я сейчас быстро вам объясню, я думаю, что вы быстро воспримете.
Для конечного случая это совсем очевидно.
Если у вас есть случайная величина х на конечном, вероятностном пространстве,
то мат ожидания это просто сумма по всем Омега маленьким из Омега большое.
Х от Омега умножить на вероятность этого Омега.
Проще ничего быть не может, просто такое среднее взвешенное.
Может быть, кстати, и другим слушателям будет полезно как-то в этом контексте посмотреть на ситуацию,
а то ведь могли дать более общий определение сразу.
Ну, наверное, давали такое тоже.
Так, ну понятно, просто средне взвешенное.
Если все вероятности одинаковые равны 1 по делительной мощности Омега,
ну это просто среднеархметическое.
Ну, они же могут быть разными.
Знаете, я вызываю из этой аудитории случайного человека, но у меня есть любимчики.
То есть не каждый вызывается с вероятностью одна, там, условно, 50 или сколько тут сидит народу,
а кто-то вызывается с большей вероятностью, кто-то с меньшей.
И дальше я у каждого из вызванных измеряю рост.
Ну, наверное, средний рост это все-таки надо не на 50 поделить, а вот с этими весами еще взять.
Друзья, понятно?
Даже тем, кто не знал определение мат ожидания.
Его можно переписать по-другому.
Сгруппировав слагаемые, на которых Х принимает одно и то же значение,
и так тоже определение обычно дают.
То есть представьте себе, что Х на своем омега принимает какие-то конкретные различные значения y1 и так далее yk.
Это разные действительные числа.
y1 и так далее yk.
Тогда здесь получится вот так.
Сумма по i от 1 до k, yi t умножить на вероятность того, что x равняется yi t.
Я тоже надеюсь, что всем, кто поднял руки, понятно, почему так.
Перегруппировал просто слагаемые, и все.
Совершенно понятно.
Вот, ничего больше про мат ожидания знать не нужно.
Доказательства абсолютно очевидные.
Обычно пишется справа налево.
Вот, мы знаем с вами определение мат ожидания.
Эта сумма по i от 1 до k, yi t на вероятность того, что x равно yi t.
У нас а, не так.
Давайте разобьем эту сумму на две части.
В одной будут такие i, на которых yi t больше либо равняется а,
а в другой будут такие i, на которых yi t меньше, чем а.
Успеваете?
Разбил сумму на две очевидные части.
После того, как я бил суммы на части, которые начинались с n в степени 0,6,
такое разбиение кажется детским.
Помните n в степени 0,6?
А тут-то все понятно.
Смотрите, какую я фигню сейчас сделаю.
Я замечу, что, поскольку их принимало только положительные, не отрицательные значения,
то вот эти все y, которые во второй сумме, они положительные, не отрицательные, правильно?
Слушайте, ну вероятность-то это точно не отрицательное число?
Я скажу, что вот это все больше либо равно 0.
Так, это больше либо равно сумме по i таким, что yi t больше либо равняется а,
yi t и t на вероятность того, что x равно yi t.
yi t больше либо равно, а тут тоже yi t.
Значит, это все больше либо равно, правильно?
Больше либо равно.
Общее a выносим за скобку.
Сумма остается все по тем же i, на которых yi t больше либо равняется а.
А тут вероятность того, что x равно yi.
Надеюсь, товарищи, вы понимаете, что сумма, которая здесь написана,
это и есть в аккурат вероятность того, что x больше либо равняется а.
То есть у нас в итоге получается a умножить на вероятность того, что x больше либо равняется а.
И читаем это справа-налево, получаем нужное нерадство.
Очевидно все?
Нужно вопрос.
Да.
Мы здесь не доказываем для случая, когда мощность не имеет граничности,
или это просто неправда, когда мощность не имеет граничности?
Это правда.
Естественно, нужно, наверное, говорить о том, что мат ожидания должно существовать,
иначе просто непонятно, что написано справа.
Мат ожидания для бесконечных не всегда определено корректно.
Так это верно всегда.
То есть это я просто рассказываю совсем понятное такое доказательство,
но оно понятное в общем случае тоже.
Что вдруг такой интеграл вылезет, а тут сумма.
Друзья, понятно все, да?
То есть в частном случае, если здесь вместо a поставить единичку,
то мы аккурат оказываемся вот в этой ситуации.
А если кто-то знает, что математическое ожидание линий,
то он и понимает, что вот это вот как раз математическое ожидание
числа треугольников в случайном графе.
Фактически я здесь написал частный случай неравенства Марков.
Но он совсем очевиден.
Ну, линейный смарт ожидания-то надо знать.
Давайте я напомню, это нам точно пригодится,
если мы берем с какими-то коэффициентами две случайные величины складываем,
то математическое ожидание можно посчитать вот так.
Это свойство линейности, которое, я думаю,
знает все, кто знает, что такое мат ожидания.
Знаете, да?
Линейная комбинация случайных величин в среднем
безотносительно того зависима и независима эти величины не важна.
Всегда будет комбинация их математических ожиданий.
Что очень важно, что никакого значения не имеет зависима
или независима эта случайная величина.
Так, я пока не произношу никаких слов, которые выходят за рамки общего понимания.
Точно? Все хорошо.
Все, вот неравенство Маркова и еще неравенство Чебышова.
Нужно, а третье это будет на вырост.
Оно сразу не понадобится, но будет очень классно.
Я его сразу расскажу.
Так.
Ну, чтобы напомнить неравенство Чебышова, давайте.
Я должен сказать, что такое дисперсия случайной величины.
Поднимите руки, кто знает, что такое дисперсия.
Ну, видимо, опять все, кроме тех, кто намотал.
Ну, напоминаю, что такое дисперсия.
Это математическое ожидание вот такого вот квадрата разности.
То есть, формально уже понятно.
А по сути, это среднее квадратичное уклонение случайной величины от своего среднего.
Среднее квадратичное это корень дисперсии.
Ну, уклонение корень дисперсии, да, строго говоря, корень дисперсии.
Ну, я по сути говорю, а не по тем терминам, которые обычно приняты.
Да, среднее уклонение это корень дисперсии обычно, да.
Ну, не важно.
Вот, если раскрыть скобки и воспользоваться вот этой линейностью,
но это все вам в курсе вероятности рассказывать,
то вы сами можете раскрыть скобки.
Это простое упражнение.
У вас получится еще вот так.
Бывает полезность.
Это называется второй момент, а это квадрат мат ожидания.
Я не думаю, что надо больше говорить про дисперсию.
Ну, дисперсия.
Может быть, дурацкий вопрос, почему здесь квадрат?
Ну, если снять квадрат, будет 0.
Ну, просто по линейности, если квадрат убрать, будет 0.
Очевидно, потому что мат ожидания, мат ожидания, это мат ожидания.
Мат ожидания – это константа.
Значит, его среднее значение – это анаш.
Если у всех одинаковый рост, то средний рост будет все равно равен вот этому общему значению.
Можно глупого вопроса?
Да.
А где стоит квадрат?
Над ешкой или над скобочкой?
Вот так, конечно, да.
Конечно, вот так.
И здесь тоже вот так.
Но обычно просто эти скобки не рисуют.
Но смысл именно такой.
Да, конечно, конечно.
То есть вы смотрите значение случайно лично.
Где-то находится мат ожидания.
Оно может не совпасть ни с одним значением, конечно же.
Мат ожидания может же не совпасть ни с одним значением.
А дальше вы измеряете, какой разброс средний возникает.
Вот дисперсия, она меряет в каком смысле разброс.
Собственно, слово дисперсия означает разброс.
Вот, неравенство Чебышова тоже верное всегда.
Поэтому я даже не буду писать, что амера конечна, действительно.
И больше того, я напишу пуст Икс любая случайная величина.
Как бы вы ее не определяли.
Тогда не обязательно положительно значить.
Тогда вероятность того, что Икс уклонится от своего математического ожидания не меньше, чем на а.
Здесь тоже надо написать, для любого а большего нуля.
Я что-то забыл.
Для любого а большего нуля вероятность вот такого уклонения не превосходит дисперсии.
Икс и Икс поделены на а в квадрате.
Я утверждаю, что это очевидное следствие из неравенства Маркова.
Так, у меня совсем высокая вот есть кусочек.
Почему это очевидное следствие из неравенства Маркова?
Ну, потому что пишите вот так.
И воспользуйтесь неравенством Маркова.
Друзья, понятно, эта величина принимает только положительные, не отрицательные значения.
А квадрат, естественно, по-прежнему положительно.
Значит, по неравенству Маркова мы получаем математическое ожидание вот этой штуки.
Икс минус Екс в квадрате.
Поделить просто на а квадрат.
Ну, так это же есть дисперсия.
Вот и все.
Любопытно, что в наших комбинаторных исследованиях,
несмотря на то, что неравенство Чебышова это следствие неравенства Маркова,
они друг друга дополняют.
Очень естественным образом.
Обычно неравенство Маркова доказывает, что
скорее всего, какого-то объекта в случайном графе не существует,
а неравенство Чебышова доказывает, что какой-то объект существует.
Страстно звучит, да?
Или абстрактно слишком?
Так, ну оба неравенства понятны?
Нет, ну если нормально, давайте я про треугольники немножко расскажу,
просто иначе вам будет не очень понятно.
Бог с ним, что я в прошлые годы этого не делал,
а в этом году сделаю.
Меня ж записывают.
Все хорошо, не зря значит записывают.
Я чуть-чуть все-таки по-разному читаю,
особенно в этом году с учетом того, что началось все не так, как в прежние годы.
Я же часть прочитал Лукатече.
Ты хорошо.
Я обещаю, мы перейдем к хроматическим числам,
но пока давайте разберемся с сутью происходящего, ладно?
А то я вас научу микроскопом гвозди забивать, и это будет не очень хорошо.
Вот про треугольники, смотрите.
Давайте так.
Теорема, с позволения сказать, очень простая.
Номер один.
Теорема номер один.
Пусть в модели эрдоширении
В зависит от Н.
Ну так, может быть?
Это более общий случай.
Ну нормально, да, вполне с ростом числа вершин
вероятность того, что отдельно взятая связка пропадет,
ну, например, возрастает, а почему нет?
Может же такое случиться.
Чем больше компьютеров в сети, тем легче развалить отдельно взятой ребро этой сети.
Такое может быть.
Ну вот П зависит от Н.
Ну так, что если мы Н умножим на вот это П, на П от Н,
ну, хотите я на П от Н здесь напишу,
то в пределе получится ноль при Н, стремящемся к бесконечности.
Тогда вероятность того, что g от НП, сейчас я допишу и потом устроим перерыв,
есть треугольники,
ну, это вероятность того, что x больше либо равно единице,
ведь x число треугольников.
Я утверждаю, что стремится к нулю при Н, стремящемся к бесконечности.
Знаете, как это словами называется?
Я думаю, не знаете.
Ну, может кто-то знает, кому я в коле рассказывал когда-то.
Это называется асимпатически, почти, наверное,
в случайном графе нет треугольников.
Ну, вот такая терминология, я просто о диаскопках и кавычках напишу.
А, П, Н, нет треугольников.
Вероятность того, что они есть, стремится к нулю,
вероятность того, что их нет, стремится к единице,
и вот если вероятность какой-то последовательности событий
с ростом числа вершин стремится к единице,
то мы говорим, что это выполнено асимпатически, почти наверх.
Так, можно целиком не писать, уследили, асимпатически, почти наверх.
Давайте я докажу эту теорию, мы потом сделаем 5-минутный перерыв.
Она настолько очевидная, смотрите,
вероятность того, что x больше либо равно единице,
не превосходит мат ожиданий x,
которые есть в цейзанпотре на П в кубе ввиду линейности.
Все-таки все понимают, почему это цейзанпотре на П в кубе?
Ну, я не знаю, давайте я поясню, ладно.
Это тоже совсем просто, это пригодится на после перерыва.
Давайте я вот так представлю x.
Товарищ, x это число треугольников.
Нумеруем все тройки вершин от единицы до цейзанпотре, как и раньше.
Ну там вот было, но уже стерто.
Нумеруем и берем в качестве x вот здесь, напишу,
на графе то, что называется индикатор единицу,
если вот эта итое тройка вершин образует треугольник и ноль иначе,
то есть сама скапает случайный граф, все он реализовался,
вышел к доске, так сказать, попал сюда,
и если на конкретной итое по счету тройки вершин в этом конкретном графе треугольник,
мы пишем единичку, а иначе ноль.
Такой, знаете, совершенно идиотский программистский подход.
Ну умный программистский подход это какой-нибудь красивый алгоритм,
который быстро считает число треугольников в графе.
А у нас идиотский подход.
Давайте переберем все тройки вершин в таком длинном цикле цейзанпотре длиной,
и на каждом шаге будем добавлять очередную единичку,
если очередная тройка вершин вот в этом графе образует треугольник.
Друзья, все понятно?
Поскольку мат ожидания линейна, то мат ожидания х,
которая нас интересует, это сумма мат ожидания этих товарищей.
Но мат ожидания товарищей это просто единицу может на вероятность треугольника,
то есть на П в кубе.
Вот это последнее понятно?
По второму определению в мат ожидания.
Ну все вот получилось.
Это ведет себя как N в кубе поделить на 6 на П в кубе,
а у нас по условиям пострелится к нулю.
Ну значит оно стремится к нулю.
Все, я теорем доказал?
Доказал?
Все, перерыв.
Так, смотрите, как я уже говорил,
неравенство Маркова успело нам ответить на вопрос о том,
что каких-то объектов почти наверняка нету.
Вот давайте я теперь сформулирую комплементарную,
то есть дополнительную теорему по отношению к этой,
когда будет видно, что Чебышов доказывает прямо противоположное.
Только вы понимали, что действительно неравенство Маркова и неравенство Чебышова
дополняют друг друга,
и кое-что еще вы поймете, это я вам произнесу обязательно.
Может вам это произнесут и в лекциях по теории вероятностей?
Не знаю.
Но это очень важно.
Два. Пусть опять-таки в g от n, p,
p равняется p от n,
причем n умножить на p от n
стремится к плюс бесконечности.
При н-то стремящемся к этой самой плюс бесконечности.
Бесконечности, понятно.
Я не всегда плюс рисовал, но смысл тот же самый.
Согласитесь, это такая комплементарная ситуация, да?
Ну, конечно, есть еще такая очень тонкая промежуточная ситуация,
когда n, p стремится к константе,
ну или вообще никуда не стремится.
Давайте ее пока не обсуждать.
Ну, как бы прямо противоположно.
Да, n, p стремится к нулю, а тут n, p стремится к бесконечности.
Я утверждаю, что тогда,
и могу уже писать вот так, а симпатически почти наверное
g от n, p есть треугольники, хотя бы один.
Так, согласитесь, что это прямо противоположная ситуация,
то есть там а симпатически почти наверное их не было,
здесь они а симпатически почти наверное их есть.
Все видят, да? Прямо прямую противоположность.
Так, как это доказывать?
Ну, давайте возьмем x, это опять будет число треугольников,
то же самое, которое здесь обсуждалось,
число треугольников в случайном графе g, то же самое.
Чего нам нужно доказать?
Нам нужно доказать, что вероятность,
с которой x больше или равно 1, стремится к одному.
Правильно?
Есть треугольники, x больше или равно 1.
Вот хотим доказать, что она стремится к одному.
У нас есть только верхняя оценка, вот эта.
И из того, что эта верхняя оценка сейчас стремится к плюсу бесконечности,
ничего не следует, правда же?
Это же верхняя оценка, что с того, что она дурацкая?
Может, образ дурацкий?
Вот, поэтому мы будем действовать так.
Мы напишем, что это 1.
Сейчас, подождите, или так оставить?
Нет, правильно, да.
Минус вероятность того, что x не превосходит нуля?
Ну, это вот прямо ход, знаете, тот самый.
Помните, как я вам говорил, что люди думают,
что проще посчитать вероятность отсутствия треугольника,
если из единицы вычислить вероятность его присутствия?
Сейчас я сделал ровно то же самое.
Но это поможет, потому что я просто хочу подогнать под неравенство Чебышова.
Значит, смотрите, я сейчас...
А, что еще может вас смущать?
x не больше нуля, но оно может быть меньше нуля.
Нет, а я в самом деле так оставлю, чтобы неравенство Чебышова было похожим, формальным, правильно?
Послаться.
Ну да, да, то есть, друзья, вот кто-нибудь потом начнет перечитывать конспект
или пересматривать лекции, если я это не скажу,
обязательно может возникнуть упытливого, по крайней мере, человека вопрос,
о чем я здесь не написал ровно нуля.
Я что, считаю, что x может быть отрицательным?
Нет, конечно, я так не считаю,
но просто я хочу подогнать под вид неравенского Чебышова.
Я его специально нестил.
Значит, давайте еще одно смешное преобразование сделаем.
Вот такое вот.
Ха-ха.
Видите, людей проканал. Смешно, правда?
Хе-хе.
Сейчас еще одно такое же смешное.
Один минус, но последнее.
Вот так напишу.
Ну я просто слева справа добавил неравенство константу, равную мотажеданию х.
ex это конкретное число, я его просто слева и справа добавил.
Но смотрите как хорошо, тут x-ex, тут ex-x,
но если сейчас модуль нарисовать, то какая разница, правда?
Ну и, конечно, я хочу сказать, что это больше либо равно,
один минус, как раз вероятность, под которой уже стоит модуль,
ну я могу также оставить, не меняя знак внутри,
больше либо равно единице.
Так, понятно, почему получилось неравенство в нужную нам сторону.
Мы хотим доказать, что вероятность стремится к единице,
поэтому оценивать ее надо снизу.
Ну, ясно, что вероятность, которая вычитается,
если тут нарисован модуль, больше, чем вероятность, которая вычитается тут,
по крайней мере не менее.
Модуль чаще бывает больше либо равен чего-то,
чем это происходит с подмодульной величиной.
В общем, это очевидно.
Все, применяем неравенство чьи бы шоума, оно что-то меньше либо равно,
но со знаком минус, поэтому неравенство в нужную сторону,
включаем больше либо равно 1 минус дисперсия х
поделить на квадрат математического ожидания.
Заметьте, дорогие товарищи, что все выкладки, которые мы проводили,
справедливы не только для числа треугольников,
а для любой случайной величины,
которая на Омега принимает значение 0, 1, 2 и так далее.
То есть множество натуральных чисел.
Согласны?
Нигде в этих выкладках не использовалось ничего,
кроме того, что х это целочисто значная, причем неотрицательная случайная величина.
Вот здесь использовалось, что она целочисто значная,
ну и, собственно говоря, все.
То есть эту идею можно использовать не только для числа треугольников,
но для любой какой-нибудь счетчика, для любого счетчика на графе.
Можно взять число К4, можно взять число независимых множеств какого-то размера,
можно взять число цепей или циклов или еще чего-то,
и для них и для всех это будет верное неравенство.
Поняли, да?
Ну что осталось доказать для завершения доказательства теории?
Что дисперсия мала по сравнению с квадратом математического ожидания.
Верно?
Да.
Если мы докажем, что эта дробь стремится к нулю,
то мы получим стремление к единице.
Так, прекрасно.
Все, я стираю неравенство Чебышова.
Поскольку я про хроматические числа сегодня уже ничего не успею,
мне пришлось читать такую лекцию вводную по вероятностным вещам,
но случайные графы тоже присутствуют, все хорошо.
Давайте я докажу это до конца.
Заодно вы поймете очень важную вещь, которую я пока не произнес.
И тогда у вас откроются глаза.
Это даже не катарсис, это прозрение.
Так, ладно.
Слушайте, мат ожидания мы знаем.
Что такое мат ожидания?
Ой, зачем скопки?
Мат ожидания это С из Н по 3 на П в кубе.
Вот оно еще тут сохранилось.
Хорошо?
А вот дисперсию надо посчитать.
Ну, для дисперсии я писал сегодня удобную формулу.
Это е х квадрат минус квадрат ех.
То есть фактически нас интересует вот это.
Ну видите, я не зря, как я уже говорил, не зря вот это написал.
Кому-то было непонятно.
Я линейности объяснил.
А заодно это сейчас пригодится.
Давайте просто вот эту сумму возведем в квадрат.
Сейчас вот возведем в квадрат.
То есть я считаю второй момент ех квадрат.
Беру ех1 плюс и т.д.
Плюс х С из Н по 3.
Вот эти самые индикаторы, которые там написаны.
И возвожу в квадрат.
Ну, люди, которые учили в свое время полинамиальные коэффициенты,
в квадрат сумму многих величин могут возвести.
Правильно?
Так, что же у нас получится?
Во-первых, будет сумма квадрата.
А, будет?
А, во-вторых, будет сумма по и неравном ж.
х и т на и ж и т.
Ну, имеется в виду, что здесь и ж и ж и это разные вещи, поэтому я двойку не рисую.
То есть здесь n умножить на n-1 слагаемых, а не С из Н по 2.
Раскрываем по линейности и кое-что еще замечаем.
Знаете почему?
Потому что это 0 и 1.
Что в квадрат-то возводить, если 0 и 1?
Успеваете?
Понятно, это 0 или 1, поэтому в квадрате то же самое, что без квадрата.
Тогда вот эта часть по линейности это снова мата ожидания, которая мы знаем.
У нас получается мата ожидания, вот оно, я его не буду переписывать.
Плюс сумма по и неравно ж, мата ожидания х и т умножить на и ж и т.
Ну, бог вас знает до каких высот вы добрались в вашем нематематическом плане.
Математическом до этого вы точно не добрались.
Но может быть вы знаете, что если бы эти величины были независимы, то мата ожидания было бы равно произведению.
Но они независимы, к сожалению.
Ну, потому что и т по счету, и т по счету тройка точек может образовывать треугольник, и ж и т по счету, вот смотрите, и т по счету тройка точек.
А вот ж и т по счету тройка точек, вершин, да?
И т по счету тройка вершин, скажем, один, два, три, и ж и т по счету тройка вершин, скажем, один, три, четыре.
Наличие треугольника на этой тройке, наверное, что-то говорит о появлении его здесь.
То есть, величины эти зависимы.
Ни в коем случае, конечно, нельзя считать, что мата ожидания произведения это произведение мата ожидания.
Не гоню?
Да, нормально?
Ну, так все на картинке дорисовано, как посчитать?
Мне придется перерисовать и икс.
Надо просто рассмотреть три случая.
Может быть, вот такая картина, одна тройка, и другая тройка.
Может быть, вот такая картина, одна тройка, и другая тройка, и вот такая.
Первая, вторая.
Вторая, третий не отличается с точки зрения того, в какую степень П возводить,
но с точки зрения подсчета удобно их разделить.
Я понятно сказал для всех, да?
То есть, что у нас получается?
Давайте я вот эти конструкции посчитаю.
Совсем просто.
Это C из N по три, количество способов выбрать первые три вершины, умножить на C из N минус три по три.
Это оставший за оставшихся N минус трех выбрать еще три вершины.
Я не делю пополам, потому что пары тройок у меня упорядоченные.
Я вот здесь не рисовал коэффициент двойка, соответственно, тут я не делю пополам.
Знаю, что это всегда напрягает, но вот так.
Так, умножить на P в шестой, но это вероятность того, что и тут треугольник, и тут треугольник.
Шесть треугольников, значит, P в шестой.
Так, вот здесь тоже будет P в шестой, но количество считается вот так.
Так, вот так, на P в шестой.
Еще раз, почему так?
Три вершины выбираем, C из N по три способами.
Общую вершину выбираем, любым из трех.
После того, как она выбрана, еще две из N минус трех оставшихся, да выберем.
Ну и плюс вот эта как раз ситуация, в которой единственная есть зависимость,
поэтому получается не в шестой, а в пятый.
Тут пять ребер, это общая, ребер пять.
Так, C из N по три, снова на три, потому что вот эти две общие вершины выбираются тремя способами.
Так, и на C, ну можно не на C, конечно, просто на N минус три.
На C из N минус трех по одному.
Вот эту одну четвертую вершину надо выбрать откуда-то из N минус трех оставшихся.
И здесь на P в пятой степени.
Так, друзья, все понятно?
Нужно ли проводить противную выкладку или можно оставить ее вам как упражнение?
Какую?
Какую?
Надо же разделить посчитанную только что дисперсию на квадрат математического ожидания.
Провести, наверное, да, ленитесь.
Проведите сами эту выкладку, мы все посчитали, смотрите, мы посчитали только что второй момент.
Вот он, вот эта вот правая часть, омерзительное длинное выражение, которое полностью вычисляет второй момент.
Из него надо вычесть квадрат мат ожидания и поделить на квадрат мат ожидания.
И нужно убедиться, что, да, в тех условиях, в которых мы живем, получается в пределе ноль.
Можно оставлю как упражнение?
Давайте, потому что это как-то не совсем затевая вещь.
Но здесь реально важно следующее.
Вы понимаете, почему самого факта, что мат ожидания все-таки стремится к бесконечности, недостаточно?
Вот есть случайная величина, она принимает какие-то значения.
Средняя растет, но разброс может быть настолько большим,
что вероятность, с которой случайная величина принимает хоть какое-нибудь разумное значение,
ну скажем, больше либо равные единицы, не стремится к одному, можете привести такие примеры.
Нам очень важно, что дисперсия маленькая.
Вот мы ее там как-то посчитали и убедились, что она маленькая.
За счет того, что она маленькая, и получился результат теории М2.
А теперь, товарищи, обещанное прозрение.
Сейчас, могу вот эту часть стереть?
Наверное, да.
Ну я не буду считать, поэтому, мне кажется, я могу ее стереть.
Так-то все понятно?
Вот.
Прозрение.
Хе-хе.
Помните, вероятность х больше либо равно единицы,
х от числа треугольника, от задачки, помните, была задачка?
Так.
Мы сказали, что это вероятность объединения по И от единицы до С из Н по три аито.
Аито, товарищи, оно вот здесь присутствует.
Вот этот мод, это и есть АИТ, на самом деле.
Согласны?
Это очень важно. Согласны, что это АИТ?
ИТ по счету тройка образует треугольник.
Вы мне говорили, что это про формулу включения и исключения, но мы-то с вами ее знаем.
Мы даже ее выводили из обобщенного обращения мебиоса.
Мы ведь много еще выводили.
Нет, ну конечно, да.
То есть формула включения и исключения для вас это прямо такое фундаментальное знание.
Но она начинается с чего?
Ой, Господи, что я нарисовал.
Вероятность А1, плюс и так далее, плюс вероятность АС из Н по три.
Это что такое?
Это С из Н по три умножить на П в кубе, правда?
То есть это математическое ожидание Х.
Я сейчас ничего не доказываю.
Я пытаюсь вам рассказать некую суть, чтобы у вас возникло понимание картины мира.
Постарайтесь в этом слушаться.
Дальше. Чем она продолжается, эта формула?
Надо вычесть всевозможные попарные пересечения.
А1, А2, минус АС из Н по три, минус 1, АС из Н по три.
Так, правильно написал, да?
А теперь смотрите вот сюда.
Что такое х и т умножить на х ж и т?
Ну даже не сюда, это я чуть-чуть подстер, а вот сюда.
Уже когда мат ожидания занесено внутрь, у нас получается вот так.
Нет? Х и т х ж и т.
Это вот то, что мы посчитали и потом вот тут стерли.
Согласны?
Ну это как раз прояснится того, что...
Друзья, вы понимаете, что это и есть вот эта сумма?
Вот эта сумма.
Все понимают?
Что значит х и т х ж и т равно единице?
Это значит одновременно х и т равно единице, то есть выполнено аи и т,
и х ж и т равно единице, то есть выполнено ажи и т.
То есть это пересечение аи и т в ажи и т.
То есть вот эта штуковина, вычисленная нами в процессе доказательства теоремы 2,
это вычитаемая формуле включения и исключения.
Таким образом, неравенство Маркова, в котором участвует мат ожидания,
отвечает за первую часть формулы включения и исключения,
а неравенство Чебышова, в котором присутствует дисперсия,
выражающаяся фактически через это, если забыть про уже вычисленное мат ожидания.
Это отвечает за вторую часть формулы включения и исключения.
Вы почувствовали смысл, да?
Что смысл?
То есть в принципе можно там дальше еще что-то написать,
и это будут третий момент, четвертый момент,
потому что там будут возникать пересечения трех, четырех событий,
соответствующие произведениям трех, четырех и так далее случайных величин,
то есть возведению в куб, в четвертую степень и так далее исходного количества треугольников.
Вот сейчас по темпу нормально?
Понятен смысл, да?
Это очень важно понимать, что тут ничего такого шумного,
просто формула включения и исключения,
и конечно, если мы ее обрываем здесь, то получается неравенство в одну сторону,
а если мы добавляем минусы, то в другую,
потом добавляем еще какие-то плюсы,
опять будет в одну сторону, вычитаем?
А почему это, конечно?
Почему это очевидно?
Ну может быть и не очевидно, но вот так получилось.
Это очевидно, но...
Не-не, ну смотрите, давайте так, я же сейчас ничего не доказываю,
а просто пытаюсь объяснить суть происходящего.
Вот до второго момента мы это доказали,
а то, что так же будет с третьим и с четвертым,
конечно это надо доказывать, но интуитивно понятно, что так и будет.
Другое дело, что обычно в комбинаторике третьи и четвертые моменты возникают,
на этом мы сейчас закончим лекцию,
в том случае, когда NP не стремится ни к нулю, ни к бесконечности,
потому что вы же все-таки интересуетесь,
а что же будет вот на этой тонкой грани?
У меня очень длитонная красновидность.
Толстая, да?
Все конечные.
Не, ну слушайте, она куда тоньше, чем многообразие функций с этим свойством или противоположным?
Все-таки.
Все-таки.
Ну, посмотрите, NP стремится к нулю, это П может каким угодно,
хоть E в минус N, хоть 1 поделить на N лог N, там, каким хотите.
А если NP стремится к константе,
то вы ограничиваетесь уже П-шками, которые ведут себя как это константа,
поделенная на N, грубо говоря.
А вы, кстати, да.
Это не такое уж жирное множество, это множество всех константов,
а тут множество всех совершенно разнообразных функций.
Ну и так же.
Еще раз, я не рассматриваю случаи,
когда предела нет вообще,
потому что непонятно, что с ним делать,
он мне очень интересен.
Теорема 3.
Я ее не буду доказывать.
В этом курсе,
в этом курсе я ее доказывать не буду,
но это полезная очень вещь,
понимать, что происходит в теореме 3.
Значит, пусть у нас опять
уже от NP,
так я просто напишу, NP стремится к C,
но, естественно, большие нуля.
То есть,
P ведет себя симпатически как C поделенная.
Запомните этот режим,
он еще будет пригождаться нам в дальнейшем.
Это очень важная ситуация,
в которой кое-что происходит,
но это не скоро.
Пойдем.
Попробуем однопропорционально
количество вершин в графе.
Тогда
вероятность того,
что в случайном графе нет треугольников,
х равно нулю, это нет треугольников,
стремится
к Е
в степени,
ага,
нет треугольников, да,
все хорошо.
Е в степени
минус C в кубе на 6.
Нет, ну тут, на самом деле,
для людей, которые смотрят
на левую часть доски,
ничего страшного нет,
потому что, ну давайте, я пройдусь к левой части доски,
смотрите, мат ожидания
х, это вот такая величина всегда,
и в текущей ситуации
она ведет себя симпатически в аккурат
как C в кубе поделить на 6.
Подставьте сюда
P равное C поделить на N.
У вас N в кубе сократится,
а C в кубе на 6 останется.
То есть, вот этот вот показатель
экспоненты со знаком минус,
это просто асимптотика математического ожидания х.
Вот она каким-то боком здесь вылезает.
Ну, я не расскажу, каким.
Я ж не доказываю тебе.
Задача со звездочкой.
Нет, ну это сложная задача,
сами не докажете.
Нет-нет, скорее всего, сами не докажете,
потому что тут надо какое-то,
ну надо посмотреть, как ведет себя
вот эта формула включения-исключения
на всех вообще слаганях.
Вот честно ее вычислить.
Ну это можно.
В данной ситуации это можно сделать.
Друзья, еще раз, это ни в коем случае
не задача на десятку на экзамене.
Это...
Ну нет.
Не надо.
Это просто, чтобы вы понимали,
как мир устроен.
Ну вот я не могу все рассказать в этом курсе,
хоть он и большой.
И вас кокнет, и времени просто не хватит,
даже несмотря на то, что курс большой.
Я думаю, что этот метод мы разбирать
все-таки не станем.
Называется он метод моментов,
что вполне ожидаемо.
Это вот как раз к вопросу Тихона,
ну как доказать?
Вот если доказывать эту теорему,
я думаю, что вот доказано
отвечающими за то, что я сказал.
Так.
Ну, смотрите, мне кажется...
А!
Ну да.
Мне кажется, что я все сказал
для того, чтобы в следующий раз перейти
наконец к раскраскам графов,
как я и обещал.
Мы используем там и неравенство Маркова,
и неравенство Чебышова,
и еще одно неравенство,
и в следующий раз
в следующий раз займемся раскрасками.
Хорошо?
А неравенство, которое я говорил,
помните, говорил третье неравенство,
третье, там люди будут смотреть
запись, а где третье?
А я его сейчас не успел, товарищи.
Это не страшно, потому что
для следующей лекции оно не нужно.
Вот на следующей лекции я расскажу
красивые вещи про хроматические числа,
а потом, если останется время,
вернусь к третьему неравенству.
Всем спасибо.
АПЛОДИСМЕНТЫ

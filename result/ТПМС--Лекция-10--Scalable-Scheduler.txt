Ну что, давайте начнем. Добрый день, доброе утро. Давайте, прежде чем перейти к теме лекции,
она сегодня про планировщик, я небольшое замечание сделаю про текущую домашнюю работу,
про задачу Fiberny Mutex, где вы пишете Mutex, Condvar, Weight Group, Произвольный примитив
синхронизации. Вот некоторые люди уже заметили, что там есть некоторые сложности, скажем так. То есть
у вас все работает, кроме одного небольшого теста, который иллюстрирует очень неприятную ситуацию,
очень тонкую ситуацию, которую нужно сначала увидеть своими глазами, разобрать механику
детальную, как там что происходит, и, в-третьих, причинить. Вот, пожалуйста, обратите на это внимание,
там есть очень тонкая хитрость с лайфтаймами и с разрушением объектов. Вот, есть Mutex, а есть
Weight Group, и применение у них несколько разное. Когда мы используем, мы используем Weight Group для
того, чтобы дождаться завершения других Fiber'ов. И в одном потоке Fiber сигнализирует последнее,
что вот он завершается, говоря Dan, а в другом потоке другой Fiber выходит из Weight'а,
завершает свою процедуру, и вот разрушается стековый фрейм, вызывается деструктор этого самого
Weight Group. И если неаккуратно один поток, завершая Dan, все еще пишет что-то в разрушаемый объект,
то у вас разламывается адрес Sanitizer, он говорит вам, что вы пишете уже разрушенную память. Ну там,
в тесте это делается специальным таким маленьким костылем видеолокацией на кучу, чтобы адрес
Sanitizer лучше обнаружил. Ну вот, пожалуйста, обратите на это внимание, разберите, и потом
попытайтесь починить проблему. Ну и смотрите, она чинится разными способами. Вот есть мнение
мое пока на данный момент, что следует исправить эту проблему в Weight Group, потому что именно она
про Join Fiber'ов, и именно она про разрушение, в конце концов. Но если вы вдруг решили бонусный
уровень, то есть вы решили задачу из экзекутеров Lock-Free Strand, а потом асинхронный Mutex,
а потом вы разобрались, где же здесь связь с Mutex, и в задаче Mutex решили бонусный уровень,
который про реализацию Lock-Free Mutex, то у вас там такой проблемы не будет. То есть вот прям хорошая
реализация Mutex, ее благополучно избегает. Так что вам даже не нужно будет думать про вот эту
этот сценарий с разрушением. Ладно, в общем, обращаю ваше внимание, что есть сложное место,
и, к сожалению, вот с ним придется столкнуться, и его придется разобрать. Вот я вам пока рекомендую
делать это в Weight Group, а чуть позже мы узнаем разные способы, как можно более надежно эту проблему
решить. Ну вот так или иначе, о чем задача вот про Mutex? Про то, чтобы придумать приметивы
синхронизации, точнее, про то, чтобы реализовать Mutex для начала, это такой первый очевидный ее уровень,
разобраться, как именно он устроен в ядре, потому что, по сути, мы пишем то же самое,
что и в ядре, и нужно это именно так воспринимать. Мы не пишем что-то другое, мы пишем тот же самый
клуб, потому что и под ядром, и под нами один и тот же, в конце концов, компьютер с одними этими же
операциями. Следующий уровень – это придумать дизайн хороший для саспенда, то есть увидеть,
что Mutex – это совсем не что-то фундаментальное с точки зрения синхронизации. Mutex – это всего лишь
такой частный случай вот этого самого общего саспенда, который задача требуется придумать.
Просто мы ограничены некоторой изоляцией между ядром операционной системы и пространственным
пользователя, а вот в Fiber'ах у нас такого ограничения нет, поэтому мы можем расширить
семантику Futex'а и поддержать там произвольную стратегию засыпания и планирования возобновления,
которая называется Avator. Вот это следующая очень важная часть задачи – она про дизайн,
ну и про API для блокировки, каким оно должно быть. Дальше идет Lock Frame Mutex,
который сложный, очень красивый бонусный уровень, и в конце концов, когда вы все это
сделаете и вот почините эту проблему с разрушением, вообще заметите ее, у вас получатся довольно
хорошие файберы. Ну вот если вы, правда, будете соблюдать все требования и задачи, избавитесь
от аллокаций, и в планировщики избавитесь от аллокаций, и в Mutex'ах, в общем, у вас получатся такие
аккуратные файберы. А дальше вопрос – насколько они соблюли хорошие? Как бы это проверить?
Вот мы их пишем, у нас нет там аллокаций, есть там интрузивность в Mutex. Верно ли,
что мы написали хороший, эффективный код? Ну что вы скажете по этому поводу? Мы же, не знаю,
может быть, вы взяли и запустили там профайлер с ним, посмотрели, насколько быстро он работает,
где он проводит время? Не было у вас такого желания? Нет, разумеется, стеклос будет быстрее,
он поэтому и существует, у него мы стеклос платим за одно и выигрываем другое, это разумеется,
это очевидно. Но мы не используем стеклос подход, мы используем стеклоподход и делаем это намеренно,
так что в рамках этого подхода у нас хорошие файберы получились, они быстро исполняются.
Ну вообще-то, как они исполняются, зависит от еще одного компонента, который называется
планировщик, правда? Чтобы вообще что-то исполнять, нам нужен шеддлер. Этот шеддлер,
планировщик исполняет некоторые задачи, которые в случае с файберами являются такими служебными
задачками, которые запускают крутину, делают шаг файбера, потом останавливаются и перепланируют
возобновление этого файбера. Так вот, что же можно сказать про наш планировщик? Насколько он будет
хорош? Ну потому что все же зависит не только от реализации самих файберов, но и от того,
кто их исполняет. Ну и давайте вот, прежде чем говорить, хороший наш планировщик или плохой наш
планировщик, что мы вообще понимаем под хорошим? Чего мы ожидаем от хорошего планировщика?
Мы ожидаем этого от планировщика, а что мы ожидаем от хорошего планировщика?
Ну он же не знает будущего, он же не Кассандра.
Ну честность опять, да, честность это хорошее требование, но смотри,
я спрашиваю не про то, как должен вести себя там. Понятно, что у планировщика есть какие-то
разумные гарантии, которые он обязан соблюдать. Он должен запустить каждую запланированную в него
задачу, наверное, да. Я сейчас спрашиваю про то, что такое быстрый планировщик. Вот, это очень
хорошее замечание. Вот вы изучаете операционные системы, вы знаете, что там вот есть много
компонентов, какая-то сложная логика. Вот вопрос, когда исполняется операционная система в компьютере?
Ну если, ну если бы она исполнялась всегда на компьютере с одним ядром, то я боюсь, что такая
система была бы бесполезна, потому что исполнялась бы только она. А нам нужно, чтобы исполнялась
программа пользователя, а не ядро. Вот, ну то есть есть какие-то ситуации, когда программа,
ваша вот полезная нагрузка компьютера обращается к планировщику. Всё остальное время планировщик,
ну в смысле планировщик операционной системы, должна не работать. Вот почти всё время должна
занимать ваша программа, ваша какая-то полезная нагрузка. Вот это очень разумные ожидания от
планировщика. И мы ожидаем от RedPool'а тоже, что вот если мы посмотрим, где эта
программа проводит свое время, то мы будем надеяться, что она большую часть
времени проводит внутри кода пользователя. Вот то есть, вот здесь. И поменьше внутри
методах планировщика. Чем меньше работает планировщик, тем лучше. Согласны? Вот. Ну,
потому что работать самого планировщика – это вот как бы обслуживание запуска задачи. Он
должен распределять задачи между ядрами процессора. То есть, он должен распределять
вычислительные ресурсы. Вот чем меньше его будет, тем, кажется, наш планировщик будет эффективнее.
Ну, давайте посмотрим, насколько эффективен наш планировщик. На самом деле, я сейчас покажу не
наш планировщик, а мой планировщик, который написан намеренно так же, как и ваш, но он чуть
аккуратнее. Там, скажем, нет лишних элокаций, поэтому у вас будет еще медленнее. Ну, в любом случае,
у вас будет возможность это исправить еще. Что я запускаю в этом планировщике? Я хочу запустить
вот такой код. Ну, какая-то нагрузка. Я беру и запускаю в планировщике 13 групп файберов,
которые, где каждая группа в количестве 100 файберов вот такое количество раз захватывает по очереди
два мютокса. То есть, у каждой группы по 2 мютокса по 100 файберов, и вот они хватают эти мютоксы,
выполняют какие-то простые критические секции. Код, разумеется, он не слишком практичный,
вряд ли настоящая программа вот так вот работает. Но вот по этой программе уже можно посмотреть,
насколько много времени она занимается планированием, насколько много времени она
собственно исполняется. Ну, вот давайте мы и посмотрим.
Ну, вот мы собрали профиль, можно посмотреть на него. Я не знаю, видите ли вы что-нибудь на
проекторе? Ну, ничего не поделать боюсь, что. Ну, я расскажу. Вот это Flame Graph. Ну, то есть,
вот горизонтальные такие колбаски — это время, проведенное в каком-то из вызовов. И вот я смотрю
на какой-то thread-threadpool. И вот смотрите, половину времени, 44%, он проводит в лямде пользователя.
А где он проводит еще половину времени? Проводит он ее в функции worker-routing. Звучит,
выглядит довольно плохо. А что же он делает в worker-routing? Ну, вот он берет mutex. В смысле,
он прямо в вызове mutex-lock проводит 8% времени. Еще, ну, примерно столько же в mutex-unlock. А потом
еще раз mutex-lock и еще раз mutex-unlock. Ну, вот видите, он половину времени рабочего вашей программы
занимается тем, что она вот захватывает и освобождает mutex. В смысле, вот не ваш файберный mutex,
который, кстати, написан здесь хорошо. Вот в этом примере он написан как lock-free mutex. Кстати,
вот можно и посмотреть, как это влияет. Вот, например, мы запускаем программу threadpool
с таким наивным планировщиком. А потом и с lock-free mutex. Потом поменяем на реализацию,
которую вы пишете. Посмотрим, что случится. Ну, стало хуже тут, как бы, да, тут заметно,
что стало хуже. Одна из причин написать хороший lock-free mutex. Он уже на десятки процентов ускоряет
такой, ну, синтетический, но все же пример. Ну, вот mutex мы улучшили даже. Я про это и говорю,
что, собственно, вот возьмем, напишем хороший файбер, но под ними вот планировщик, который
половину времени проводит в mutex-lock. Давайте теперь я по-другому скажу, что такое хороший или плохой,
ну, быстрый или эффективный или неэффективный планировщик. Я бы сказал, что эффективный
планировщик – это планировщик, который хорошо масштабируется. Вот наш планировщик пока плохо
масштабируется. Я сейчас поясню, что это значит. Под масштабированием вообще глобально система любых
распределенных, вот многопоточных понимает такое свойство, что если вы добавляете вашу систему
новые вычислительные ресурсы, скажем, для примера вычислительные, то система начинает
обслуживать больше запросов в единицу времени пропорционально. Ну, вот вы пишете какой-нибудь,
у вас распределенный Mopreduce, который какие-то вычисления проводит, у вас было 100 машин,
а вы сделали 200 машин. И вот у вас теперь пропускная способность вашей системы, полезные
работы выполняются в два раза больше в единицу времени. Мы, наверное, от планировщика такое хотим.
С ростом количества ядер в процессоре, с ростом количества потоков, которые запускают на этих
ядрах задачи, кажется, планировщик должен выполнять в два раза больше задачи в единицу времени.
Ну, так бы вел себя идеальный планировщик, который бы вот просто линейно масштабировался. А что можно
сказать про нашу реализацию нашего планировщика? Является ли она масштабируемой или нет?
Верно ли, что мы, добавляя в машину ядра, вот в таком тесте получаем там кратное ускорение?
Он не масштабируем. Ну, потому что... Что мы видим в профайле-то? Еще раз. Мы видим, что планировщик
половину времени тратит на локи-анлоке-мьютакса. Почему? Ну, потому что в планировщике есть точка
contention. Есть вот этот глобальная очередь общая для всех потоков воркеров. И каждый поток,
чтобы взять себе задачу, берет и идет к этой общей очереди и захватывает блокировку.
А разве количество локов и анлоков в единицу времени зависит от количества ядер? Разве оно
растет с ростом числа ядер? Ну, вряд ли, да? Скорее, оно уменьшается с ростом числа ядер. То есть,
чем больше нагрузка, тем больше нагрузка на там, как протокол когеретности кэшей, на вот то,
чтобы подвинуть одну ячейку из кэша там, кэшлинию из одного кэша в другое, в другое, конечно,
но вот вы помните все это, да? Зачем нам нужна была лекция про кэши в этом курсе? Чтобы сказать,
что для синхронизации, что при записи в ячейку памяти ядру нужно получить кэшлинию в монопольное
владение. Нужно поговорить с другими ядрами, инвалидировать в них копии, получить себе кэшлинию
в состоянии modified, эксклюзивный доступ к ней иметь. Поэтому, если у вас есть ядра, и они работают
с одними и теми же ячейками памяти, ну, скажем, с одним и тем же флажком в одном и том же мютоксе,
то, кажется, все эти ядра работают с ним последовательно. При этом, чем больше ядер,
тем больше просто дополнительная нагрузка на кладные расходы, на синхронизацию, на пересылку
сообщений, на их обработку. Ну, то есть, мы бы, конечно, хотели, чтобы планировщик работал быстрее
с ростом количества потоков, ядер потоков, соответственно, но пропускная способность
критической мютокса, она просто ограничена. Вот. И, скажем, помогло бы нам, если бы мы взяли и заменили
вот этот самый мютокс, который защищает очередь, на лог-фри очередь, которую мы делали в прошлый раз.
Но точно так же это не помогло бы, потому что, смотрите, лог-фри очередь — это снова ячейка,
в которой хранится поинтер на голову, и мы там этот поинтер меняли кассом, читали голову, потом
пытались перекинуть ее дальше, делали compare exchange. Ну, compare exchange — это снова операция, которая
пишет в ячейку памяти, даже если она, даже если неуспешный касс. И снова мы вот, все ядра будут
сериализовываться вот на дно этой ячейки, будет contention, будет нагрузка. Понимаете проблемы, да? Вот.
Чтобы построить хороший планировщик, эффективный планировщик, который будет масштабироваться,
нам нужно применить какой-то другой подход. А это единственное, в смысле, это и есть плата
за синхронизацию. Тут совершенно неважно, тратим мы время в сисколах, потому что мы засыпаем,
или тратим мы время в преобращении к ячейкам памяти. У нас все равно есть точка contention,
у нас все равно есть место, где потоки очень часто сталкиваются друг с другом и упорядочиваются,
действуют последовательно. Неважно, какой именно механизм к этому приводит. Тут специфика,
смотри, какая. Вот наш тредпул, он, в принципе, хороший тредпул, он нормальный, не нужно про него
ничего плохого говорить, просто он не подходит нашей задаче. Вот он подходит для задач, которые
выполняют какие-то вычисления тяжелые. Вот задача запустилась, некоторое время поработала,
что-то полезное поделала. А у нас здесь задачи очень короткие. И вот почему мы видим такой профиль?
Потому что время работы задачи, оно сопостоимое с временем захвата блокировки. Это просто
величина одного порядка. И вот ровно поэтому мы на блокировку, именно поэтому здесь уже нужно
тратить время на оптимизацию вот этого самого Mutex, а точнее не Mutex оптимизировать, а пытаться его
с пути планировщика убрать, чтобы он запускал задачи и при этом не пытался захватывать постоянно
для каждой задачи один и тот же разделяемый Mutex. Но вот, кстати, мы сегодня будем разбирать,
не кстати, сегодня будем разбирать, как написать такой хороший эффективный планировщик и будем
делать это на примере языка Go. Так вот, в языке Go вот до 2012 года был такой вот наивный планировщик,
который просто работал, но работал неэффективно, не масштабировался. А потом появился Дмитрий
Вьюков. Это такой очень известный человек-операход, который придумал огромное количество всего в
синхронизации, придумал огромное количество разных паттернов, трюков. И на аватарке то ли
канала, то ли чат у нас Дмитрий Вьюков, вы должны его запомнить. И он написал вот половину runtime
Go, которая связана с запуском Go routine. Вот написал планировщик, масштабируемый планировщик. И на всякий
случай, ну не на всякий случай, просто посоветую вам, посмотрите его доклад чудесный, 2019 года он
выступал на конференции в Петербурге, гидра называется, рассказывал, как этот планировщик устроен. Ну вот,
мы сегодня про это и поговорим. И я буду иногда пользоваться его слайдами. Ну о чем он говорит,
что вот есть наивный планировщик, когда у вас потоки для того, чтобы исполнять горутины,
но горутины, файберы, задачи в нашем случае, в случае Goet задачи это горутины конкретно, в случае нашего
планировщика это абстрактные задачи. Вот эти потоки, они сталкиваются на Mutex и вот там упорядочиваются,
никакого масштабирования не происходит. Ну lock free, в общем, та же самая история, все равно у нас
есть contention на ячейке памяти. Как же побороть проблему с масштабированием? Как же избавиться,
ну как же масштабировать, когда у нас есть некоторая точка contention, ну избавиться от нее.
Сделать систему распределенной, сделать так, чтобы, ну говорят, шардировать состояние. Так,
чтобы каждый поток, каждый forker в threadpool, каждый поток планировщика работал независимо со
своим состоянием, со своими ячейками памяти и с другими потоками у него не было необходимости,
ну по крайней мере, часто синхронизироваться. Что именно предлагается? Ну давайте просто сделаем
вот этот самый распределенный планировщик, шардированный планировщик, где у каждого потока
воркера будет своя локальная очередь задачи и он будет работать в первую очередь с ней. Когда
он планирует, когда мы планируем задачу и планируем ее из планировщика, ну скажем, мы
анлочим mutex в файбере и это приводит к возобновлению другого файбера, который ждал этого mutex.
Ну давайте его запланируем не в какую-то глобальную очередь, для которой нам придется
брать блокировку или какой-нибудь лог-приписать, но все равно contention будет. Давайте положим ее
в локальную очередь. Когда мы будем забирать задачу, посмотрим на локальную очередь. Если
там что-то есть, то вот берем и исполняем. Кажется, что тогда вот на быстром пути,
когда у всех много работы и она равноверно распределена, то воркеры не будут вообще
общаться друг с другом и у них не будет синхронизации и они могут работать из
этого физически параллельно. Вот общие ячейки памяти убивают параллелизм. Здесь у нас их
может просто не быть. Мы не предсказываем, какие задачи легче, какие тяжелее. В файберах
они все не длинные. Если мы в задачах в файберах делаем много работы, то это не… то файберы
используются не по назначению. Да.
Мало. Ну смотри, да, я не показал. Давай ускорим предпул и посмотрим, как он начнет работать.
Ну не все так могут. У вас будет возможность это сделать. Ну там было что-то три секунды,
да, отработало все. Три или что-то такое. Ну вот этот планировщик распределенный,
шардированный, в котором нет. Ну ладно, я говорю, там есть глобальная очередь,
и я сейчас про нее поговорю. Но на быстром пути ее нет. Ну мы же понимаем, как компьютер устроен.
Мы послушали лекцию про кэши, мы посмотрели в профайлере, мы видим, мы понимаем, что в нашем
планировщике есть контеншн. Мы понимаем, что время захвата задачи в планировщике сопоставим
с временем работы самой задачи. Ну вот, все это указывает на то, что нужно планировщик
шардировать. Мы это сделали, и вот теперь процедура воркера занимает там 15 процентов. Ну вру,
тут еще внутри задачи выполняется код рантайма немного. Ну вот, кстати, здесь видно, что на
переключение контекста мы тратим два процента времени. Вот это наша цена за стэкфалка рутины.
К этому мы в конце еще вернемся. Ну вот, почему наш код работает быстрее? Потому что вот воркеров
стало меньше. Полезная нагрузка была, так и осталась. А планировщик теперь работает незаметнее,
потому что он работает на... потому что он не общается... потому что воркеры в планировщике
реже обращаются к раздряемым ячейкам памяти. Ну вот, это все довольно естественные вещи,
что мы такого ожидали, и так на самом деле и происходит. И сейчас мы будем обсуждать,
а как именно такой планировщик сделать. Локальные очереди завести их, наверное, не сложно. В чем
сложность такой реализации? В том, что избавляясь от централизованного состояния, от одной общей
очереди, мы теряем многие приятные свойства. Ну, например, нам будет сложнее засыпать в
ожидании работы, потому что раньше мы посмотрели под мьютоксом на очередь, видим, что она пустая
и заснули, значит. А теперь мы не можем так заснуть, потому что у нас состояние децентрализовано,
и мы не можем атомарно проверить, что нигде нет задач. Ну, к этому мы придем еще. Другая сложность
в том, что нужно балансировать нагрузку аккуратно. Вот пока в таком дизайне совершенно непонятно,
почему не получится какой-нибудь перекос, почему на каком-нибудь потоке воркере будет
грутина, которая много запускает других грутин, а на другом таких грутин не будет, и в итоге у
одного воркера будет больше работы, будет длиннее очередь, и этот воркер, ну, и некоторые грутины не
будут исполняться, хотя могли бы. В одной общей очереди все было просто. Если в очереди общие были
задачи, значит, все воркеры в тредкуле уже нагружены работой. Здесь это уже не так. Ну, в общем,
здесь появляется сложность с балансировкой нагрузки. Ну, я сразу скажу, что у нас все-таки будет
общая очередь, которая, в частности, эту проблему решает, и она будет защищена мютоксом, но просто к ней
мы будем обращаться редко. Будем, но все-таки редко. Она будет помогать все-таки балансировать
нагрузку между этими локальными очередями. Вот. В принципе, можно было бы дальше код писать,
но все-таки мы будем говорить про планировщик Go сегодня, а у него есть еще одна неприятная
особенность, но не то что неприятная, специфичная для Go особенность в том, что в Go не то чтобы
файбер, там есть грутины, и это только грутины. То есть никаких потоков у вас нет, есть только
легковесные потоки в виде грутин. И поэтому возникает такая вот проблема, что...
секунду, я найду подходящую картинку. Идея следующая. Но проблема следующая, что у вас есть поток
планировщика, он исполняет некоторую грутину, некоторые файбер, и эта грутина может, ну,
как-то синхронизироваться с другими через специальные примитивы, про которые язык знает,
runtime языка знает, там mutex, канал, а может быть у нас сделается SQL и заблокируется.
Вот. Проблема в том, что если у нас в планировщике фиксированное число потоков,
то мы избавимся от... мы лишимся ядра целого. Поэтому в планировщике Go нужно отдельно обрабатывать
случаи, когда поток, когда грутина, которая исполняется в потоке, хочет сделать блокирующий
системный вызов. Поэтому в планировщике Go выделяется еще одна сущность. Там есть
грутины, там есть потоки, которые эти грутины исполняют, и есть еще сущность, которая называется
процессор. Вот, процессор – это очередь задач. Это некоторое состояние, где лежит локальная очередь.
И чтобы поток мог исполнять грутины, он должен завладеть некоторым процессором. А если же
поток планировщика исполняет грутину, и эта грутина делается SQL, то runtime процессор
отпускает и блокируется в системном вызове. А другой поток планировщика подбирает процессор
и исполняет дальше его задачи. Нам это не очень актуально, потому что мы используем файберы намеленно,
мы понимаем, как их правильно использовать, и не должны делать SQL внутри файберов. Ну, блокирующий
SQL, блокирующий поток. В ГОА у вас нет такого выбора потоки, потоки грутины, есть только грутины,
поэтому появляется такая дополнительная сложность. Она не по существу, то есть для нас это неважно,
для нас можно думать про такую картинку. Просто формально в коде, который мы будем читать,
есть несколько сущностей. Вот они называются грутины, процессоры. Грутина – это просто
исполняемая задача, по сути. Процессоры – это очередь задач. И 3D, которые называются машинами.
М. Ну, смотрите, мы будем читать код на ГО сейчас. И вот в runtime коду на ГО, смотрите,
что есть. Есть классы, которые называются… Давайте сейчас мы их найдем. Runtime2, все правильно.
Есть грутина. Тут у ГО довольно лаконичный style-гайд. Мы не должны разделять восторги по
поводу этого style-гайда, но вот нам приходится сегодня с ними жить. В принципе, вы даже ГО,
вы можете не знать, как мы будем читать код, но он похож на C. Если вы все их знаете примерно,
то вот в целом считайте, что это такое, что-то подобное. Вот есть структура грутины, есть структура
процессора. И смотрите, в процессоре есть очередь. Это локальная очередь. Это массив,
в котором лежит 256 пойнтеров, но может лежать 256 пойнтеров на грутины. Вот у каждого потока,
который исполняет грутины в планировщике ГО, есть локальная очередь на 256 слотов. И есть два
пойнтера Head и Tail, которые вот бегают по ней. Циклический буфер. На семинарах мы разбирали,
надалим память, циклический буфер. На лекции мы разбирали циклический буфер. В кышах сегодня
он пригодится. Вот, это локальная очередь. Есть еще структура, которая называется М-машина. И есть
разделяемое состояние планировщика, которое описывается структурой ShedT, и там есть глобальная
очередь. Ну вот, ровно тот дизайн, который описан на этом слайде. Вот это грутины, это процессор,
это машина, это разделяемое состояние с глобальной очередью. Здесь 256 слотов,
здесь неограниченное количество элементов. Пока понятно? Ну а теперь давайте разбираться,
как работает планировщик. Ну вернее, как он работает. А чтобы про это говорить, нужно подумать,
а как вообще планировщик вступает в работу, когда он запускается. Во-первых, программа запускает
код планировщика, когда она планирует новую работу. Ну скажем, мы отпустили Mutex и разбудили
другую грутину. То есть когда мы добавляем работу в планировщик, нужно выполнить код планировщика.
А во-вторых, когда текущая грутина по каким-то причинам останавливается, и планировщику нужно
выбрать новую грутину для исполнения. То есть добавление работы и поиск новой работы. Вот давайте
сначала про добавление работы. Когда мы в планировщик добавляем новую работу,
новую грутину или новые файберы для запуска? Перечислите варианты. Запускаем новую грутину,
запускаем новые файберы. На всякий случай, я буду сегодня говорить то про ГО, то про наши файберы,
в принципе разницы не делаете. Синонимы сегодня нет. Мы запускаем новую грутину. Значит,
в планировщике появляется работа, планировщик должен куда-то ее положить, эту работу. Другой
пример какой-нибудь. Кажется, работа тогда исчезает и не появляется. Ну мы YIL делаем,
например. У нас как бы текущая грутина останавливается, а потом снова добавляется
в планировщик. Снова нужно куда-то ее положить. Ну или в общем случае мы будем кого-то. Мы
отправили по каналу сообщение о другой грутине. Отправили, и вот эта грутина теперь разбудилась
и может выполняться. Нам нужно куда-то в планировщику ее запланировать. Или мы
отпустили Mutex, и какая-то грутина сможет его теперь захватить, мы ее разбудили. Ну вот,
давайте в рантайме посмотрим, как рантайм себя собственно ведет в таких случаях.
Ну давайте начнем с GoShedImpl. GoShed — это функция в рантайме Go, которая эквивалентна YIL. Ну то есть,
она перепланирует текущую грутину. Текущая грутина говорит рантайму среди исполнения
планировщику, что ей не очень-то нужно сейчас исполняться. Она готова уступить место другим.
Вот наш Fiber Yield. Смотрите, что мы делаем в таком случае. Мы говорим glob rank you put.
Привыкайте к именам, они похожи на C. glob — глобальная очередь, rank you, put — добавить.
glob rank you put — смотрим. Мы под локом планировщика добавляем грутину в глобальную очередь. То есть,
смотрите, у нас есть локальные очереди, есть глобальная очередь, и Yield — он очень пессимистичный.
Он бросает грутину прямо вот сюда. Не в локальную очередь, а прямо далеко-далеко. А что, если мы
посмотрим на новую грутину? Это уже функция rank you put. Не glob rank you put, а просто rank you put.
Эта функция, ну пока пропустим эту забавную строчку, и это пропустим, и много всего пропустим. Ну в общем,
даже пока я код не показываю, разберёмся с этим чуть позже. Эта функция добавляет грутину в
локальную очередь. То есть, в конец локальной очереди. Вот сюда. Когда мы будем другую грутину,
то опять мы добавляем её в локальную очередь. Разумно? Разумно. Теперь смотрим, как этот самый
rank you put реализован. Ну как реализован глобал rank you put, мы уже посмотрели. Мы под мьютоксом
добавляем задачу в очередь, увеличиваем размер очереди. Как реализован этот rank you, чуть позже
к этому вернёмся, это важно, кстати. Но пока смотрим на rank you put. Rank you put добавляет задачи в
локальную очередь. Локальную очередь, я напомню, это циклический буфер. Это циклический буфер из
256 элементов. Что? А как сделать буфер ограниченного размера в ограниченном, в массиве ограниченной
ёмкости? Ну, потому что будет странно, если в одной грутине, если у одного воркера в одной его
локальной очереди будут скапливаться и скапливаются новые грутины, а у других этих грутин не будет.
То есть, мы хотим всё-таки нагрузку балансировать. Ну вот ограничение на локальную очередь, это такое
необходимое следствие. Ну, необходимость, потому что нам всё-таки нужно как-то распределять задачу
равномерно между воркерами, между машинами. Не должно быть большого перекоса. Вот это ограничение,
оно вот про перекос максимально. Окей. Значит, мы должны добавить задачу в циклическую очередь,
в конец циклической очереди. Что нужно сделать? Прочитать tail, записать туда в слот по этому индексу
pointer на грутину, потом сдвинуть tail вперёд. Да? Ну вот мы так и делаем. Мы читаем head, мы читаем tail,
вспоминаете лекцию про кыши и вспоминаете семинар про расстановку memory-order в слабых в циклическом буфере.
Если у вас его не было ещё, то требуйте срочно, потому что вот же нам это нужно прямо сейчас
планировщики. Мы читаем head, читаем tail. Обращаем внимание, здесь мы читаем tail с relax
статомиком, потому что только мы меняем tail. Ну, кто был на семинаре, по крайней мере, на моём
последнем, это знают. Смотрим, если в буфере ещё есть элементы, слоты доступные, то хорошо,
мы помещаем в индекс tail по модулю длины очереди, длины в смысле капасти, размера очереди,
к грутину и двигаем tail вперёд. Ну, я понимаю, что вы, может быть, первый раз видите код на го,
особенно вот так, так написанный, но тут не должно быть ничего особенного, удивительного, да? Вроде
всё понятно. Вот, ну, смотрите, но если в очереди уже не было слотов, если она переполнилась, там
256 элементов, то мы проваливаемся в rank you put slow. И что мы там делаем? Ну, смотрите, у нас
переполнилась локальная очередь. Что бы вы сделали в таком случае? Мы бы передвинули задачи в глобальную
очередь. Сколько задач? Все? Это было странно, у нас было много работы, теперь у нас стало мало работы,
нам ведь теперь не хватает, что же нам делать? Нет. Ну, не нужно двигать мало, в смысле одну,
потому что очередь снова быстро переполнится, видимо. Ну, то есть, смотрите, rank you put slow
выгружает часть локальной работы в общую очередь, чтобы работа балансировалась. Но эта общая
очередь — это общий mutex, точка contention. Нам нужно минимизировать работу с этим mutex. Поэтому
мы хотим амортизировать эти расходы на синхронизацию. Мы хотим, в данном случае, передвинуть половину
нашей очереди, чтобы и у нас что-то осталось, а с другой стороны, чтобы у других много появилось.
То есть, чтобы у нас и много осталось, и был ещё зазор по слотам. Поэтому мы берём, вычисляем
размер нашей очереди, берём половину и формируем такую вот пачку. То есть, читаем из нашего локального
массива rank you, из вот rank you нашего процессора, которым владеем мы данная машина, поток,
worker, и пытаемся эту пачку выгрузить в глобальную очередь. Но мы делаем там это всё довольно хитро,
потому что, смотрите, ну, во-первых, нельзя в глобальную очередь класть задачи по одной,
потому что мы там много раз возьмём mutex. Нужно положить в rank you сразу пачку,
но сделано ещё хитрее. Я забыл, как функция называлась, чёрт возьми.
Смотрите, она берёт mutex и работает там за константу, потому что очередь, это на самом деле
интрузивная. То есть, смотрите, мы достали из локальной очереди пачку задач groutine,
которую мы выгрузим наверх в глобальную очередь. Мы связали их поинтерами, и потом уже мы за
константу под mutex добавили сразу много задач в разделяемую очередь. То есть, мы минимизируем
время критической секции, которая нам всё-таки тогда необходима. Мы к ней обращаемся редко,
когда очередь переполняется, а когда обращаемся, делаем это аккуратно. То есть, уж точно не зовём
пут на каждую groutine, и ещё оптимизируем интрузивностью. Но у нас в курсе есть
интрузивные контейнеры, вы с ними уже работали, а вот там есть тоже списки, и вы можете
конкатинировать их за констант. Это важно. Понятно пока, да? Только смотрите, ещё какой нюанс есть.
Когда мы двигаем tail вперёд в ru, когда мы двигаем head вперёд, то есть мы пытаемся забрать из
локальной очереди половину элементов. Вот когда мы их забираем, мы почему-то двигаем head вперёд не
просто стором, а почему-то кассом. Хотя довольно странно, потому что очередь локальная, и работаем
же с ней только мы. Ну, к этому я вернусь ещё, тем более я вам даже не рассказал пока почему,
что делает этот код. Я его как-то пропустил. Но пока и ладно, пока неважно. Когда мы добавляем в
планировщик новую работу, мы пытаемся добавить эту новую groutine запланированную в конец локальной
очереди. Если же локальная очередь переполнилась, если там 256 элементов, то мы выбираем половину
этих элементов из локальной очереди, провязываем их ссылками, захватываем минуток с общей очереди
и перекладываем половину туда. Вот, значит, половина планировщика рассказана, как планируется новая
работа. А теперь самое главное в планировщике, как он новую работу себе выбирает. Вот, планировщик
выбирает себе новую работу, когда текущая groutine решила заснуть. И для этого у планировщика есть
функция, она называется shadow. Вот, функция. Вот, смотрите, мы вызываем go shed. Мы вызываем
go shed. Плохой, необыченный. Вот, я вам всё-таки его покажу. Вот, go shed это функция, которая
перепланирует текущую groutine. Вот, мы её вызываем, и что происходит? Мы кладём себя в глобальную очередь,
далеко-далеко, а потом вызываем функцию shadow, потому что сейчас нам нечего делать. Нам, в смысле,
текущей машине, текущему потоку воркера. Функция shadow, ну, тут много всего происходит,
потому что runtime это сложная штука, тут сборка мусора, вытеснения. В конце концов вызывается
выбранная groutine. Какая-то groutine выбирается и вызывается, а перед этим она выбирается. И для этого
есть функция, которая называется, давайте мы её найдём, она называется find runnable. Нужно найти
очередную groutine для исполнения. Ну, давайте думать, что мы делаем. Сначала, разумеется,
мы пробуем достать groutine из локальной очереди текущего потока, текущей машины. Ну, зачем нам
синхронизироваться с другими? Runqget. И тут снова мы читаем head, читаем tail, читаем groutine из
хеда, и почему-то двигаем хед опять кассом. А если не получилось, то ретравимся. Опять непонятно,
почему мы так делаем, потому что в циклическом буфере вроде бы, ну, потому что вроде бы один,
только один поток работает с этой очередь. Ну, ладно, непонятно, но оставим это на будущее. Вот
может быть, да. Вот мы сейчас это и увидим, собственно. Сначала мы идём в локальную очередь,
пытаемся брать задачу оттуда. Если получилось, нашлось, то вот здорово, мы сэкономили себе
синхронизацию. Вот именно за счёт этого быстрого пути планировщик быстро и работает. Именно поэтому
в профиле мы видим мало планировщика и много полезной работы. Но если не получилось, то мы идём в
глобальную очередь. И опять, оттуда мы забираем, видимо, не одну задачу, а много. Половину мы
берём runQsize, дерем её на число потоков и пытаемся забрать столько. Вот, одну катую, где какая-то число ядер.
Хорошо, попробовали, не получилось. Что делать дальше? Ну, дальше нужно учесть, что мы пишем всё-таки
runtime язык ОГО. ОГО есть, кроме запуска грутин, есть там таймеры, есть сеть. И вот прямо здесь
можно сделать епол. Ну, то есть, какую-то работу, которая может породить какие-то события внешние,
в смысле, реагировать на внешние события и запланировать новые задачи. То есть, мы, смотрите,
мы поток планировщика. У нас runQ пустая, глобальная runQ тоже пустая. Работы не так много, видимо.
Вот почему бы не отвлечься и не нагенерировать событий внешних? Ну вот, если этого не получилось,
то смотрите, что нужно делать. Ну, предлагается вуровать у других потоков. Ну, потому что, может быть,
ни у кого прям больших излишков нет, но прямо скажем, мы сейчас недоунтируем процессор. Может
быть, у кого-то в очереди 100 грутин, а у нас сейчас 0 грутин. Поэтому что мы делаем? Мы...
Давайте найдем код. Мы вызываем функцию stillwork. А там мы вот некоторое количество раз берем и
обходим другие процессоры, то есть, другие очереди в каком-то случайном порядке, и пытаемся
у них захватить задачку. RunQ still. И вот ровно поэтому у нас локальная очередь неоднопоточная. В нее
добавляет только один поток, который владеет этой очередью. Поэтому в RunQ put здесь просто store.
А вот извлекает из этой очереди несколько потоков. Во-первых, кто этой очереди владеет,
а во-вторых, тот, кто из нее ворует. Ну и опять, если уж мы воруем, то это же синхронизация. Лучше
делать это пореже. Лучше про запас себе набрать грутин. Поэтому мы пытаемся захватить сразу какое-то
количество. Ну половину прям пытаемся своровать. Мы читаем head, чужое читаем tail. Мы передаем
буфер, куда нужно переложить сворованные грутины. А дальше мы пытаемся... Смотрите, мы читаем
в этот локальный буфер сворованные грутины, а потом берем и пытаемся кассами передвинуть head
вперед на n. Если получилось, то мы своровали. Если не получилось, то, видимо, кто-то другой
с этой очередью работал, тоже воровал, или просто поток, который владел этой очереди, доставал
задачи, грутины. Так что, может быть, нужно претравиться заново. Идея ясна? А вот теперь,
смотрите, много тонкостей начинается. Во-первых, обратите внимание, что у нас индексы head и tail,
они не закругляются. Они монотонно растут. И мы каждый раз берем по модулю. Понятно ли зачем?
Вот тут нельзя написать закругление индексов. Тут важно, что они монотонно растут. Да, потому
смотрите, что может получиться. У нас была грутина, которая воровала задачи. Она пытается
передвинуть head с нуля до двух, с нуля до десяти. Сорова десять задач. Она прочитала их себе в буфер
поэнтеры, а потом пытается передвинуть tail, head кассам. Но между вот этим чтением в буфер и
кассам очередь переполнилась, потом пошла по второму кругу, и в итоге head теперь снова стал ноль,
и у нас случилось ABA. Мы подумали, что у нас касс успешный, хотя на самом деле очередь
сильно изменилась. Так что мы используем здесь монотонные индексы, и ABA у нас не бывает. Понятно?
Ну вообще, тут почему-то в runtime я не знаю ответа. Head и tail 32. Они могут переполниться,
разумеется. Ну, я не знаю, можно было бы просто 64 сделать, и они бы не переполнились никогда в жизни.
Ну, порядочные люди избегают даже такого. Ты прав, Ром, что такого скорее всего не будет,
но какая-то… Ну, не знаю, виртуалку поставили на… Ладно, не подойдет. Короче, сборка мусора началась.
Там… Смотри, если исправить на 64, то просто станет безопасно всегда. Ну,
да ладно, я не спорю. Я не спорю, но в домашней работе напишите, пожалуйста, 64. Вот, а теперь
второй очень важный момент. Переполнение будет больным. Ну, видимо, они считают, что не…
Теперь следующий важный момент, который касается этого простого кода. Открываем задачу,
задачу, которая называется файберы карутина. И там говорят, что с тредпулом нашим… Ну,
мы пишем тредпул, используем его для запуска файберов. И при этом говорят, что вот пользователи
тредпула, которые не будут запускать файберы, будут использовать функцию submit вместо того,
чтобы бросать задачу прямо через мета тредпула. Почему? Потому что тогда можно сделать
интрузивность. И про эту интрузивность повторяют в каждой задаче. Вот в экзекьюторах повторяют,
говорят… Ну, умоляют уже, можно сказать. Сделайте, наконец, интрузивность, говорят, задача,
потому что тогда… Ну, потому что добавление любой задачи в тредпул – это добавление в контейнер
некоторой внутренней функции, да, объекта function. Объект function – это type rager. У него
фиксированный размер, а лямбда в нем может быть любого размера произвольного типа. Нужно стереть
ее тип, нужно положить его на кучу, повесить на нее pointer, сделать динамическую локацию. Так вот,
файбером, фьючем, корутином, которые будут у нас в stackless C++ эти динамические локации не
нужны. У нас один раз файбер алоцируется сам по себе на куче, и вот он сам и может быть задачей.
Вот в хорошем коде… В хорошем коде задача… Задача – это pointer. Это pointer прямо на файбер,
который служит задачей. Или это pointer на такую… Или это pointer, который построила функция submit.
Она взяла лямбду, алоцировала на куче, поставила pointer туда. Короче говоря, если… Но аккуратная
реализация файберов не делает лишних локаций в самом планировщике. Так вот, это было до текущего
момента, это была просто аккуратность. А сейчас это необходимость. Посмотрите на этот код в го.
Мы здесь читаем из буфера при воровстве. И вот мы читаем из ячейки буфера pointer на
грутина. А другие потоки тоже могут воровать же, да? А поток, который владеет этой очередью,
может туда писать прямо сейчас. Мы не можем уже в планировщике хранить сложные объекты задачи.
Мы не можем функционы хранить, потому что мы не можем их атомарно в одном потоке читать,
а в другом писать. Сознали проблему? Здесь важно, что в массиве хранятся просто поэнтеры.
И один поток их читает, другой пишет. Ну и мы бы все плюс-плюс написали здесь relaxed atomic.
Relaxed atomic от поэнтера. Вот ровно поэтому для того, чтобы такой код эффективно написать,
чтобы написать эффективный work steering, нам нужно в планировщике избавиться от STD-фанкшенов
и перейти на аллокации задач снаружи пула, перейти на инклюзивные задачи. Пусть пользователь
решает, где должна жить задача. На стеке, на куче, каждый раз виллоцировать и переиспользовать,
потому что fiber и переиспользуемая задача. В планировщик попадают уже просто поэнтеры.
Это важно. Вот тогда можно в планировщике сделать work steering эффективно. Без этого вы просто не
напишете вот этот код. Вы эту строчку не сможете написать. Хорошо, я объяснил вам все это. Да,
еще не все. Мы попытались, в любом случае у нас будет задача про то, чтобы написать work steering
очередь, потом сделать work steering планировщика. Поэтому постепенно мы это еще раз поймем. Итак,
мы были find runnable. Мы попробовали взять локальную задачу из локальной. Где мы вообще? Что это? Где
я нахожусь? Find runnable, да. Мы пробовали локальную очередь, потом глобальную очередь, потом полить
события внешние, потом пытались воровать. Только смотрите, воровать всеми потоками бесполезно,
но плохая идея. Представьте, что у вас мало задач и у вас все потоки начинают воровать друг
у друга. Это бессмысленно какая-то. Они будут этим заниматься без сно. Life lock получится какое-то.
Поэтому в планировщике ограничение, что воруют не больше, чем половина потоков. Если вы
переполняете этот лимит, то вы не воруете задачу, вы сдаетесь. И что вы сейчас сдаетесь? Вы
пытаетесь уснуть. То есть вы попробовали взять задачу, а очередь у вас пустая. Ну, в смысле,
очередь не какая-то локальная, а вот глобальная. Ну, в смысле, в планировщике как будто бы нет
задач для вас. И вы готовы уснуть. В чем сложность? В том, что легко было писать такой код, когда у
вас были мутокс и кундвары. Мы в блокирующей очереди брали лок, смотрели, если очередь
пустая, то кундвар вейт. Да? Очень просто. Проверили пустоту одним вызовом эмпти на контейнере,
а потом уснули, отпустили лок, и вы уверены, что пока вы засыпали, ничего не изменилось. Вот тут все
намного сложнее в этом планировщике, потому что у вас больше нет одной очереди, нет одного мютокса.
У вас состояние сильно децентрализовано, сильно размазано. Вы проверили локальную очередь,
потом проверили глобальную очередь, потом проверили другие очереди. Пока вы проверяли,
ваши знания тут же устаревают. Поэтому, когда вы решили заснуть, может быть, работа уже появилась,
и вам нужно очень аккуратно заснуть. И вот на самом деле это засыпание, это, я бы сказал,
самая сложная часть планировщика. Вот это все было очень легко, а вот засыпание там нужно
помучиться. Нужно сначала объявить, что мы засыпаем, потом перепроверить, что ничего не появилось нового,
а потом все-таки заснуть. И если с момента объявления засыпания задача появилась,
то она нас сможет разбудить. Там есть много тонкостей, давайте оставим это на какое-нибудь
светлое будущее. Может быть, мы это напишем кто-нибудь из вас, но вообще это сложно. Короче,
засыпаем. В принципе, я вам рассказал про планировщика. Добавляем мы сюда,
если переполнилось, выгружаем половину сюда. Когда мы забираем работу, мы смотрим сначала
сюда. Если здесь пусто, идем сюда, забираем часть задач. Если и здесь пусто, то пытаемся воровать
задача. Если нигде ничего нет, засыпаем. При добавлении мы добавляем задачу. Зачем смотреть?
Зачем? Глобальная очередь это точка, которая, как раз, балансирует нагрузку более-менее равномерно.
К ней все обращаются. Так, наверное, тоже можно было бы пробовать. Кажется, что тут проще.
Ну, это немного разные вещи. Это, наверное, самый аккуратный ответ. Ну, мы знаем, потому что если
бы так было лучше, Дмитрий Бюков бы написал именно так. Плохой аргумент, но он, мне кажется,
близко к истине. Итак, алгоритм планировщика понятен, а теперь он утверждается, что он эффективный.
В смысле, тут мало contention. Мы берем mutex на очень короткое время и очень редко. Но утверждается,
что мы построили плохой планировщик, потому что он не то чтобы не эффективный, он нечестный.
В него можно положить задачу, и она никогда не исполнится. Это, наверное, не то, что мы хотим
от планировщика. Причем я даже рассказывал вам сценарий, когда такое получится. Ну, если вы
внимательно следите за происходящим, то можете такой сценарий сконструировать. Вот в планировщике
есть грутина, она где-то лежит. Ну, и программа так написана, что до нее не доходит дела никогда.
Ну, смотрите, у нас грутина сделала yield, и Go ее пессимизировала, отправила глобальную очередь.
Но так получается, что во всех локальных очередях есть работа. Там много грутина обмениваются
данными по каналам и вот постоянно там перепланируются, у каждого ядра есть, чем заниматься. Локальные
очереди не пустеют. А у нас в функции find runnable есть, есть приоритет у локальной очереди.
Поэтому задача, которая попала в глобальную очередь, может никогда и не исполнится. Нужно
это починить. Но вот для этого нужно вернуться в функцию shadow, потому что в функции shadow вызывается
find runnable, а перед этим выполняется еще некоторый код. И смотрите, что тут происходит. Вот мы берем...
Мы берем текущую машину, ее процессор. Shed tick — это количество итераций планировщика. Вот. И если
количество итераций планировщика кратно 61, то мы сначала достаем из глобальной очереди задачу одну.
Вот. А потом уже локальная очередь, потом уже find runnable. В мире есть фундаментальные константы,
физика на них как-то опирается. Вот насколько фундаментальная константа 61. Отвечает
разработчик этого кода. Ну, ответ сложный, конечно. Ответ сложный. Ну, это какое-то число.
Ну, оно простое. Смотрите, вообще магические константы в коде довольно сложно. Вот знаете
вы про click-хаус, самую быструю на свете энергетическую базу данных, которую Алексей Миловидов
в Яндексе написал. Ну вот. Он рассказывал там в одном из своих докладов про интересные штуки.
Там базы данных, мы кладем туда данные, они шардируются между разными машинами, между разными
шардами по хэшу от ключа. А внутри шарда есть хэштаблица какая-то. И вот если хэш-функция,
которая раскладывает данные по шардам, скоррелирует с хэш-функцией в хэштаблице, то ваша хэштаблица
перестанет быть константной. Ее сильно перекоист. Понимаете проблему? Вот. Это такие забавные штуки
возникают в жизни. Так вот. 61 для того, чтобы вот такое странное число, чтобы не коррелировать
с какими-то другими магическими константами в коде. Вот не использовать на всякий случай 61 в своем
коде. Это может быть опасно. Насколько это константы хорошие? Ну вот. Давайте я вам покажу
планировщик Tokyo, framework Tokyo в языке Rust. Вот там тоже есть приоритет. Мы берем сначала из
локальной очереди, а если там нет, из глобальной очереди. Но иногда наоборот. Иногда это когда? Вот.
Ну утверждается, что Дмитрий Юков написал пока лучший известный планировщик и другие языки,
что Kotlin, что Rust. Они его более-менее переписывают. Ну там с какими-то техническими отличиями,
но фундаментальный алгоритм тот же самый. Таким образом, мы немного отсюдим честность. А теперь
вопрос. Вот хороший, но важный вопрос. Насколько нам вообще хочется честности? Насколько нам
хочется FIFO? Потому что это же явно не FIFO, да? Мы там раз в 61 итерацию что-то там попросим.
Вот разумно ли вообще FIFO от планировщика требовать? Чем раньше задача была запланирована,
тем быстрее она, тем раньше она исполнится. Ну давай операция на наши задачи.
Совсем неразумно. Вот FIFO подходит для задач независимых. А наши файберы, они не независимые,
они коммуницируют друг с другом. Они отпускают, там берут общие мютексы, они отправляют друг другу
сообщения по каналу. И вот представим себе, что одна грутина ждала на канале, а другая грутина
отправила сообщения в канал и разбудила эту. И вот мы при добавлении, при отправке сообщения в
канал должны запланировать в планировщик вот эту спящую грутину. Она теперь готова исполнится.
Мы её положим в локальную очередь, в конец. Но зачем мы её туда положим? Чтобы она проснулась и
прочитала сообщения из памяти. Когда мы пишем что-то в память, куда мы пишем? В кэш. И хорошо
мы потом из кэша прочитать. Но мы отправляем сообщения, кладём грутину в конец локальной очереди,
и пока она там маринуется, пока до неё дойдёт очередь, вероятно, данные для этой грутины,
которые она хотела получить, собственно, из кэша уже вытеснет, пока она там ждёт своей очереди,
пока перед ней там ещё 100 других грутин запустится. Вот если мы отправили сообщение
грутине, то выгодно разбудить её прям вот в следующей очереди. Сделать не фифо-планирование,
а лифо. Запустить последнюю добавленную грутину. Потому что мы знаем, что так будет эффективнее по
кэшу, потому что мы ради этого своей планировщикой пишем. Мы знаем, что у нас задачи коммуницируют
друг с другом, они не независимые. Поэтому нужно сделать, видимо, лифо-планирование. Вот это
делается очень забавно. Смотрите, у нас в функции runQput есть флажок next, оно говорит, нужно положить
грутину в локальную очередь, но как бы вот next это означает, что вот она хочет запуститься следующей.
Поэтому, смотрите, в структуре P рядом с runQ есть ещё вот runNext. Такой лифо-слот с длинным
комментарием. Это такой маленький стэк из одного элемента перед очередью. И когда мы планируем
грутину, если у нас стоит runNext, мы добавляем новую грутину в этот слот. Когда мы берем
грутину из локальной очереди, мы сначала смотрим в этот runNext. Если там оказался не null,
то мы берем оттуда и только иначе берем из очереди. Когда мы добавляем грутину в локальную очередь,
то мы, если ее запланировали запуск следующей, то мы пишем в runNext наш новый pointer, и если в
runNext уже что-то было, то мы эту вытесненную следующую грутину помещаем уже в конец runQ.
Понятная идея? Ну, то есть такой маленький лифо-слот, маленький костылик.
Кстати, можно проверить, дает ли это выигрыш. Давайте проверим. У меня есть другой workload,
который про каналы. Тут запускается много пар грутин. Некоторые грутины отправляют
сообщения в канал, а другие селектом дожидаются сообщений из канала. Ну, и я сейчас запущу код.
Нет, я не хочу. Так а зачем мне медленный планировщик? Зачем кому-то медленный планировщик?
Я хочу сделать сейчас вот, вот что. Так, вот код с каналами, с нормальным планировщиком,
вот с этими локальными очередями. Но когда я отправляю сообщения в канал, я в этом коде
бужу fiber, отправляю его в конец локальной очереди. А сейчас давайте я код немного изменю. Вот у меня
executors, они стали немного сложнее, к сожалению, с последней лекции. Я теперь могу сказать
планировщику некоторый хинт. Как ему планировать задачу очередную? Может быть,
а бы как, а может быть, вот, постарайся следующим. Если ты умеешь так, то запланируй ее как
следующую. Это подсказка, это не директива. И я могу в селекте сказать, что я хочу следующим
запустить. Ну, если я бужу грутину на селекте, то пусть она запустится следующей.
Вот это вот такая оптимизация, она вот уже на десятки процентов ускоряет,
просто за счет того, что с кошами лучше работает. Это важно. Но есть проблема. Мы снова сломали
планировщик. Смотрите. Ну, сначала... Нет, у меня планировщик не сломан, но сейчас я его сломаю.
И смотрите вот сюда. Вот такой вот тест. Ну, не тест, а пример. Я беру планировщик с одним
потоком. Запускаю там грутину с двумя каналами. Беру два канала, запускаю первую грутину. Она,
пока не проставлен флажок, ждет из первого, ждет из Y сообщение, кладет в X. Другая грутина ждет
из X кладет в Y. А третья грутина 10 раз делает Y, а потом говорит хватит. Вот. И если мы этот код запустим,
то чем он закончится? Ну, это правда, иначе зачем бы я стал его показывать?
Ну, он не завершается. Почему? Потому что у нас есть один поток, и там выполняется,
допустим, эта грутина. Она кладет сообщение в Y и ждет от X. Блокируется. Но когда она кладет в Y,
она планирует на запуск следующий, вот эту грутину. А она берет значение, перекладывает его в X,
и планирует на запуск эту грутину следующей. И вот они друг друга так вот через лифо-слот
этот по кругу друг друга запускают. Получается такой вот цикл. Как его разорвать?
Ну, предлагается сделать так, что если вы уж зачасти, ну то есть если вы подряд планируете много
итераций через лифо-слот, то остановитесь. Просто поставьте себе какую-то константу,
что вы не хотите слишком часто. 17. Хорошо, когда есть инициативный человек.
Код стал завершаться.
Ну что ж, дважды мы сломали честность. Ну вот как честность. Гарантию, что каждая
запланированная грутина исполнится. И дважды мы ее вот так вот починили. Кстати, можно посмотреть
теперь в этом коде, как все это работает. Ну, в смысле, посмотреть на статистику. Сейчас,
это не этот код. Там по времени вообще смотрится. Я не помню даже константа там или нет.
Вот видите, у меня какое-то количество задач в каналах. Ну там миллионы какие-то. И почти все
они выполняются через лифо, оставшиеся через локальные задачи. Воровства не так уж и много.
А если запустить хороший мютекс, серийный мютекс, как пишут в условии, чтобы это пока не значило,
то смотрите, там лифо вообще не будет, потому что мютекса столько хорош, что ему даже не нужно
лифо. Вот это одна из причин написать хороший мютекс, ну то есть разобраться с этими логфри,
со стрендами и потом вот это все написать. Ну что, давайте какой-то итог подведем. У нас
не осталось времени. Файбером нужен хороший планировщик. Обычный тредпул с разделяемой
очередью подходит для больших независимых задач, но совершенно не подходит для маленьких,
быстрых, зависимых друг от друга задач файберов. Чтобы сделать планировщик быстрым, мы пытаемся
избавиться от contention, от ячеек памяти, с которым мы работаем из разных потоков. Мы шардируем
состояние, делаем локальные очереди, делаем глобальные очереди для балансировки нагрузки,
делаем FIFO-слот для оптимизации по кышам, воруем задачи для балансировки нагрузки. И в общем,
вся эта конструкция начинает работать. Почему это точка оптимума, такой дизайн? Ну вот,
обращусь вот к авторитету. В смысле, люди пробовали и придумали вот такой дизайн,
и кажется, лучше не получается пока сделать. Что для нас нужно, что важно? Аккуратно. Во-первых,
я надеюсь, что эта лекция, она для вас послужит мотивацией, чтобы дальше заняться домашками
про оптимизацию планировщика и вообще пооптимизировать производительность, потому
что кажется, что это интересно. И обращаю ваше внимание, что вот мы здесь оптимизируем
производительность нигде не оптимизируя какие-то там асимптотики или что-то подобное. Вот все
ускорения, они появляются из-за того, что мы знаем, как работает компьютер, как устроена кыши,
как устроена синхронизация. Вот просто знание про кыши, мы дважды сильно ускорили код, мы разбили
состояние планировщика на независимой очереди, и мы вот добавили лифо, и вот этот лифо, маленькая
странная оптимизация, берет и ускоряет код на десятки процентов. Волшебство же. Вот, чтобы
компьютер, чтобы ваша программа быстро работала, вам нужно хорошо понимать ваш многопоточный
компьютер. Ну и последняя статья расхода, про которую в начале мы уже сказали, давайте я еще
раз ее покажу. Это же следующая лекция. Да, следующая лекция возможна. Смотрим в профиль. Вот там есть
разные штуки, и в частности есть переключение контекста. Ну вот тут два процента получилось на вот
эту процедуру с ассемблером. Вот это тот overhead, который можно... вот эти два процента тоже можно
устранить. Это уже не свойство планировщика, это уже свойство наших файберов. Ну, нашей реализации
собственно нашего инструмента для выражения конкурентности файберов. Вот есть другой инструмент
стеклоскорутины в C++, там где вот эти кое вейт, слава кое, return, вы это, возможно, уже видели,
я показывал. Так вот, этот механизм ценой некоторого, скажем так, когнитивного синтактического
overheadа позволяет от переключения контекста избавиться и сэкономить вот эти два процента,
которые здесь неустранимы в этом коде. Ну, видимо, в следующий раз мы этим займемся.
Что ты имеешь в виду? Ну, это я имел в виду как когнитивный синтактический overhead. Ну,
в смысле, что у тебя функции в красе в два цвета, я это имел в виду, если что. Видимо,
я непонятно говорю. Я с тобой согласен, что это фундаментальная вещь. Ну вот, в общем,
есть плюс, есть минус. Мы поговорим об этом, вероятно, через неделю. На сегодня все, спасибо.

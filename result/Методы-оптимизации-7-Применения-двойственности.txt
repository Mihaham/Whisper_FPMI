Сегодня такая, в каком-то смысле, выдаваляющая некоторые
предварительные итоги лекции про то, о чём мы обсуждали
последние, типа, полтора месяца, и здесь мы придётся
вспомнить всё то, с чём мы начинали, там, в частности,
там, сопряжённые конусы, вот это всё, сейчас оно
на счёт неистовы использоваться, вот, и я надеюсь, станет
более-менее понятно, зачем все эти теоретические
построения были необходимы.
Вот.
В следующий раз будет промежуточная лекция между теорией
и методами, когда мы посмотрим на то, как автоматически
унифицировать форму задачи, которую вы можете записывать
помощью формул.
Вот.
То есть, как они преобразуются к коническому виду, про
который сейчас ещё будет сказано дополнительно,
вот, и как процедура построения, приведения задачи к стандартной
форме, связана с процедурой проверки, то, что вы написали
в упаковку задачи, а не то, что пришло вам в голову,
просто так.
Вот.
Это всё будет в следующий раз.
И через раз, соответственно, уже начнём методы.
Вот.
Так.
Но прежде чем перейти, собственно, к тому, что, да, к тому,
что сегодня про двойственность продолжим говорить, и,
в частности, про коническую двойственность, что это
такое откуда-то наберётся, и чем она примечательна.
Почему есть просто двойственность ещё конической двойственности,
чем она, как бы, почему её выделяет в отдельный термин.
Так.
В прошлый раз мы обсудили, закончили обсуждать условия
оптимальности, двойственность, условия слейтера, условия
ККТ.
Вот.
Сейчас я их немножко повторю, чтобы ещё раз вам напомнить
о том, как они выглядят.
Вот.
Это просто слайды, summary слайды с прошлой недели.
Вот.
Значит, если у вас есть тройка значений, которые
являются решением прямой двойственной задачи и сильной
двойственности выполняется, то есть оптимальные значения
на х и на лямбда и м со звёздочками совпадают, то справедливо
следующие пятёрка условий.
Вот.
Довольно очевидно.
Тут вот мы показывали, что если выполнена сильная
двойственность, то у нас есть соотношение между
тем, что минимизация лагранжана по х совпадает с лагранжаном
в точке х со звёздочкой, которая есть решение исходной
задачи прямой.
Вот.
И, собственно, вот это вот отсюда как раз-таки и
берётся.
Значит, допустимость х со звёздочкой в исходной
задаче, допустимость μ в двойственной, дополняющая
нежесткость в условии дополняющей нежесткости и сационарность
лагранжана по прямым переменам.
То есть равен с нулю градиента лагранжана по х в точке
х со звёздочкой при условии, что мы этот лагранжан считаем
в лямбду со звёздочкой и миду со звёздочкой.
Это надо выучить.
Вот.
Значит, в случае выпуклой задачи у нас было два утверждения.
Первое утверждение в одну сторону.
Оно нам говорит о том, что если есть выпуклая задача
и есть три вектора, для которых выполнены условия ККТ,
тогда у нас есть сильная двойственность и эти три
вектора есть решение прямой двойственной задачи.
Вот.
То есть в чём принципиальное отличие вот этого условия
от вот этого условия?
То есть второе условие нам говорит о том, что есть
из-за чего выпуклые выполнены условия слейтера, тогда
х решение прямой задачи, это равносильно тому, что
существуют лямбда и мю такие, что для них, для всех этих
трёх векторов выполнены ККТ.
Вот.
Здесь не утверждается никаких, то есть не оговариваются
никакие условия, при которых эти лямбда и мю будут существовать.
То есть тут мы говорим просто.
Если каких-то трёх, три мы взяли, подставили, получили
это ответ.
Вот.
Здесь же утверждается, что существование х будет
решением только в том случае, когда найдутся такие лямбда
и мю.
Вот.
То есть тут немножко более как бы общий и инструментальный
факт, что мы всего лишь добавив свой слейтер, гарантируем
существование таких векторов.
Вот.
Это отличие, которое я ещё раз хочу проговорить.
Значит, то, что, по-моему, в прошлый раз, я не помню,
честно говоря, насколько подробно обсудили, о том,
что мы можем, мы уже вроде-то обсуждали про то, как мы
можем переформулировать задачи в эквивалентном
виде, через над-график, через вот, новых равенств,
неравенств, добавление новых переменных.
Вот.
При этом задачи будут эквивалентные, но построенные для них
двойственные задачи могут быть не эквивалентные.
То есть они будут просто по-разному записываться.
Вот.
И в зависимости от этого вы можете получить как более,
то есть вы можете переписать исходную задачу некоторым
образом и получить соответствующие двойственные задачи, которые
будут решаться проще, например.
Вот.
Например, типа, стандартный приём, вы хотим написать
двойственную задачу для вот такой задачи.
Но тут есть некоторые проблемы, эта задача без ограничений.
Тут непонятно, как эта двойственная будет выглядеть.
Вот.
Чтобы было, процедура построения двойственной была более
содержательной, предлагается ввести новую перемену y
и сказать, что ах-b просто равно y.
Тогда у нас мы расширяем множество переменных,
на которых мы хотим что-то решить.
Теперь у нас это типа m плюс n, x и y.
Вот.
Но зато появляется ограничение типа равенства, и двойственную
задача уже понятно, как строить.
Понятно ли это преобразование?
Прекрасно.
Второй приём более хитрый, потому что он напрямую опирается
на тот факт, что когда мы строили двойственную функцию,
то мы искали инфиумы Лагранжана по всему множеству допустимых
х, не используя в поиске инфиума никакие ограничения.
То есть взяли ограничения, которые у нас были, отправили
их в Лагранжан и забыли про них в поиске инфиума.
Все об этом помнят, да?
Окей.
Альтернативный способ, ну не альтернативный способ,
а как бы некоторая вариация на тему заключается в том,
что вот если у нас была какая-нибудь такая задача,
то мы вот это вот условие можем из функциональных
ограничений, которые у нас записывались бы в Лагранжане,
отправлялись, включить их в это вот неявное условие,
что у нас там х лежал в множестве d, если вы помните, такое
обозначение было.
Вот.
И соответственно, когда будем писать Лагранжан, то
в него уйдет только вот это ограничение, но инфиум
будет искаться по вот этому множеству.
То есть у вас есть как бы такой вот баланс между
тем, насколько легко вы будете считать инфиум из
Лагранжана, то есть если у вас, например, тут х из
rn, и вы ищете минимум просто функции на rn, ну типа берем
производный, подставляем, все получается.
Вот.
Но это добавляет вам ограничений в двойственную задачу, потому
что наличие таких ограничений приводит к наличию ограничений
типа неравенств на соответствующий множитель Лагранжа.
Понятно, да, Лойко?
Но, как альтернатива?
Давайте вот мы видим, что эти ограничения довольно
просты, всего лишь там х от а до б.
Называется так называемым коробочного типа ограничения.
Вот.
Давайте мы их отправим просто вот в это множество, при
этом что у нас будет?
Если мы будем решать двойственную задачу чуть сложнее, то
есть теперь нам инфиум Лагранжана искать не на всем пространстве,
а только на тех х, которые от минус одного до одного.
Это, наверное, чуть более сложно, чем для любых х.
Вот.
Зато что получаем?
Получаем, что когда мы будем выписывать двойственную
задачу, у нас никаких ограничений типа неравенств не будет,
потому что у нас только равенства остались.
Вот.
То есть нам стало сложнее найти двойственную функцию,
и стало проще стало выглядеть двойственная задача.
Понятие на это как бы переливание из одного ведра в другое,
грубо говоря, сложно, проблема.
То есть как-то ноу фриланч, он здесь заключается в том,
что в каком-то месте вам становится труднее.
Либо вам труднее в том, что у вас двойственная задача
сложнее, либо вам труднее в том, что ее двойственную
функцию найти не так просто.
Понятна логика.
Прекрасно.
Все это сейчас и в следующий раз будет активно нужно,
потому что мы будем каким-то некоторым образом стандартизировать
получаемые задачи.
Так, ну пример.
Наверное, на семинарах всем про это должны были рассказывать,
в общем-то, я надеюсь, по крайней мере.
Если кому-то не рассказывали, то я сейчас расскажу как
раз таки, лекции для этого нужны, чтобы как-то унифицировать
в целом то, что происходит в курсе.
Значит, если у нас вот такая вот задача, ограничение
типа неравенств, больше-больше либо равно, повторяю, вроде
я уже про это говорил, что поскольку мы весь наш
формализм строили в предположении, что у нас ограничение типа
неравенств меньше либо равно, то надо умножить на
минус в динечку сначала, перед тем как записывать
лагранжа, вот поэтому тут вот этот минус начинает
фигурировать.
Вот лагранжа у нас выглядит вот так, мы на нее внимательно
смотрим и с удивлением обнаружим, что наша функция лагранж
есть не что иное, как линейная функция по х.
Ну, вроде бы всем, надеюсь, это видно, всем это видно,
просто вынесли, расписали, взяли, ну, то есть линейный
алгебр там, а b транспонированный, это b транспонированный
на A транспонированный, в общем-то здесь пока ничего
более сложного нет.
Нам надо найти минимум по всем х, поскольку мы все
ограничения, которые у нас были, отправили в лагранжа,
но линейная функция при всех х это минус бесконечность,
она не ограничена снизу, вот, почти всегда за вычетом
случая, когда у нас вот это ноль, когда она просто
равна константе, вот, поэтому двойственная функция будет
иметь такой вид, что при условии, что у нас выполнено
такое равенство, у нас она равна минус лямин транспонированный.
Суммируя это все в двойственную задачу, мы получим следующий
не совсем очевидный ее вид, возможно, откуда он взялся,
первое, мы должны максимизировать двойственную функцию, но
тут стоит минус, поэтому давайте мы заменим максимум
на минимум и уберем этот минус, осталась вот эта
история.
Далее, у нас есть ограничение, при котором эта двойственная
функция конечна, это вот это равенство, но по мимо
этого равенства у нас еще есть ограничение на μ,
потому что μ это множество лагранжа для ограничений
типа неравенств, и μ должно быть больше либо равно нуля,
ну и соответственно мы отсюда можем μ выразить, это просто
c плюс а транспонированная лямба, и μ больше либо равно
нуля, значит, что а транспонированная лямба плюс c больше либо
равно нуля, записали, все ли преобразования понятны?
Поднимите руки, кому все понятно?
Вот это да, можно, то есть мы получим лямбду, потом
сюда поставим лямбду, получим мю, мю равно вот этой штуке,
все должно выполняться по построению.
Вот, значит, это простейший случай, и на программирование
все, то есть тут порой, ну то есть есть путь, по которому
говорят, типа давайте мы посмотрим на c, а, b и неравенство,
и запомним, куда надо что подставлять, что b уходит
в линейную функцию сюда, а ограничение типа ну там
равенств, неравенств, оно просто комбинируется
из а транспонированной c, ну в общем, я не люблю так
делать, в общем, приятно понимать, откуда что берется,
и вывод к тому же здесь, типа на три строчки, вот,
очень рекомендую разобраться, откуда это берется, и сильно
не запоминать конкретные формулы, а уметь их уводить,
к тому же здесь это несложно, вот, значит, соответственно,
устоя оптимальности для линейного программирования
будут иметь следующий вид, довольно несложный,
у нас будет, а х отверточка равно b, х больше нуля, это
просто допустимость в исходной задаче, вот эта штука, это
что такое?
И там еще что-то появится, да, вот эта штука, это что
такое?
Откуда она взялась?
Ну а что с предыдущим слайдом, мы же к ККТ пишем, к какому
условию из ККТ соответствует это равенство, да, это градиент
Лагранжаана, вот он, и вот в оптимальном лямбда-имю
созвелось, куда оно уже бы травянули, вот эта линейная
функция, градиент равен просто константному вектору,
от которой это к сане зависит, а то зависит от лямбда-имю,
наполняющая не жесткость и допустимость двойственной
задачи.
Личный вопрос, возможно, там отпечатка, да, там
Да, значит, вопрос, который возникает, допустим, мы решили
вот эту задачу, как нам по найденной лямбда со звездочкой
найти х со звездочкой, имея под рукой условия оптимальности,
да, какие будут идеи, так, ну мы за звездочками найдем
отсюда, да, это хорошо, дальше что делаем, есть четвертое
что она нам говорит?
Ну давайте, заканчивайте мысли, вы все правильно говорите,
не стесняйтесь.
Четвертое условие, произведение мю на х равно нулю, вы получили
мю, там какие-то чиселки стоят.
Сейчас мы живем в мире, где у нас есть только лямбда
со звездочкой и мю со звездочкой, нам надо х со звездочка получить
каким-то образом, так, прекрасно, мю не 0, х 0, значит, какая-то
часть х мы сразу понимаем, как к чему равна, вопрос,
как понять, чему равна оставшаяся, да, вы правы, первая нам
помогает, и тут как бы пока что неочевидный факт, который
будет доказан, я надеюсь, в свое время, примерно через
месяц, может быть через полтора, вот о том, что вот
эта штука, это что на самом деле такое, это мы берем
столбцы матрицы А и взвешиваем их с коэффициентами из
вектора Х, все помнят эту интерпретацию, умножение
матрицы на вектор, я надеюсь, прекрасно, матрица в нашей
задаче исходной, она чаще всего типа длинная, то есть
там столбцов больше, чем строк, соответственно,
вот это вот операция выпиливания, которая, в относении к тому,
что мы часть х за нуляем, это значит, что мы часть столбцов
матрица выкидываем из этой линейной комбинации, ну
на ноль умножаются, они как бы не участвуют в процедуре
получения вектора В, так вот, то, что у нас останется
в некотором регулярном случае, сейчас пока не буду
вдаваться подробностей, что это значит, вот, будет,
то, что у нас останется, будет содержать невырожденную
под матрицу матрица, то есть у нас останется ровно
м не нулевых элементов, и в результате, ну и у матрицы
а м строк, и то, что мы получим под матрицу из некоторых
м столбцов и м строк, будет невырожденным, которое,
что позволит нам, собственно, решить получившуюся квадратную
систему, получить соответствующие сиксы. Понятно ли тонкое
место в этом выводе, и как его, как оно используется
в общем. Так, по диаметрии, кто все понял? Так, окей,
ну вот почему она будет квадратной, будет показана
и рассказана на лекции определения программирования,
то есть пока, пока верим, вот, и места, в которых она
будет не совсем квадратной, мы их тоже обговорим в соответствующий
момент. Вот, значит, утверждение такое, что если допустимое
множество не пусто, то будет выполнена сильная двойственность,
вот, и оно, это утверждение следует напрямую из того,
о чем мы говорили про условия слейтера, поскольку мы,
вот здесь вот у нас, давайте посмотрим на допустимое
множество, это что такое, это вот по сути подпространство,
если матрица длинная, вот, и в этом подпространстве
мы берем только те иксы, которые больше либо равны
нуля, вот, ну поскольку тут, если мы, то есть если оно
не пусто, то мы можем, ну, как бы, и это подпространство,
которое мы можем бесконечно в обе стороны двигать, вот,
то коэффициент больше нуля точно найдется, поскольку
мы можем там почти любое число получить в этом
подпространстве, вот, поэтому, значит, для исследения
программирования проблем с необходимостью проверять
условия слейтера нет, потому что по виду этого множества
просто из явного вида следует, что условие, что найдется
икской с строго положительными весами будет работать, вот,
поэтому возникает вопрос, что будет, если допусти
множество пусто, вот, как это диагностируется, ну,
вот это диагностируется с помощью следующего утверждения
о том, что если допустима множество пусто, то двойственная
задача окажется неограниченной, вот, то есть если вот в исходной
задачи у вас получилось, что таких иксов вообще
нет, не то что он там один, который только ноль, но
вообще их нет, то вот в этой задаче, в которой я так
быстро пролистал, вы получите неограниченное, то есть можно
выбирать лямбду так, что вот эта штука цельевая
функция будет неограничено уменьшаться, и сейчас попытаемся
понять, как так получается. Тут нам на помощь придет
лемма-фаркаша, о которой мы говорили некоторое время
назад. Наше допустимое множество исходной задачи имеет такой
вид. Значит, если оно пусто, значит у нас по лемме-фаркаше
существует вектор, такой, что скалярное произведение
с правой части отрицательно, а результат умножения слева
на матрицу А по компоненту на больше ли браве нуля.
Все ли помнят лемма-фаркаш? В общем, она тут приведена.
Это вот протеорема отделимости, когда мы строили, ну мы говорили
что у нас есть вектор В, и он либо лежит в конусе
натянутой на столбце матрицы, а либо не лежит. Поскольку
конус натянутый на столбце матрицы является выпуклым
и замкнутым, то факт не принадлежности этому конусу
вылечает за собой существование разделяющей дверь плоскости.
Я надеюсь, что представить себе картинку довольно
несложно или сложно. Надо ли рисовать картинку
про лемма-фаркаш еще раз? Так, в понятном обосновании.
Супер. Значит, нашли такой вектор P. Ура. Чедвоздная
сдача имеет такой вид, только что я ее показывал.
Пусть у нас лямда с крышкой этот это умножить на P где-то
больше нуля. Тогда если мы подставим лямду с крышкой
в целевую функцию, получим вот такое выражение. Поскольку
P транспонировано на B меньше нуля, то при достаточно
большом тете получим стремление к минимуму бесконечности.
Вместе с тем, поскольку P транспонировано на A больше
нуля, то увеличение P ведет к тому, что вот эта штука
будет достаточно большой, чтобы получившаяся лямда
с крышкой стала допустима. Поэтому с одной стороны,
увеличивая T, вы все сильнее становитесь в нужном полупространстве
и вместе с тем целевая функция становится бесконечно маленькой.
Да, получили неограничность. Понятно ли, что произошло?
Если непонятно, то спросите.
Поднимите руки, кто разобрался.
Вы не разобрались? Нужно, давайте, конечно.
Нет, нет, нет, смотрите за стрелочкой. Это больше нуля, поэтому вы как бы это
увеличиваете. Чтобы векторица не было, какие-то отрицательные числа, вы все
равно без качества уходите, и это будет выполняться.
Так, смотрите, я понял вопрос про граничные всякие случаи, да? Сейчас еще не
придумаю, секунду. А, то есть, если паттерн спонированное на равну нулю, что будет?
То есть, если это ноль, это ноль, а, ну тут, кажется, какие-то проблемы с рангом матрицы
получаются, да? Ну, матрица полного ранга, поэтому не существует ни нулевого вектора,
перемножение на который дает нулевой вектор. Вот, наверное, так. Можно это как-то объяснить.
А все поняли вопрос? Не беспокойствуй вашего коллеги.
Хотя, сейчас, вот да, я тоже сейчас об этом подумал, что частично нули. Так, что делать
с частичными нулями? Часть ноль, часть не ноль. Что сломается? Частично ноль. Так, хорошо,
давайте я сейчас не хочу сильно долго думать, я тут напишу некоторые пояснения по поводу частичных
нулей. Вот, так, восьмой слайд. Вот, так, это значит, мы уже нашли одну опечатку и
одно недостаточно полное обоснование. Хорошо, лекция проходит не зря. Так, окей, да, за вычетом
вот этого небольшого нюанса, в принципе, вроде как все работает. Теперь переходим, то есть,
вы разобрали только, что мы с вами двойство для линейного программирования, которое, если вы
помните, было простейшим примером конической оптимизации, когда у нас их лежал в конусе
положительный актант под названием. Тут не зря, это красненьким выделено, хотя, наверное,
на экране. Теперь, собственно, перейдем от простейшего нетресетного актанта к произвольному
коническому множеству, которое, типа, обобщенным неравенцем представляется. Так, вопрос
все ли помню, что это за значок? Перед тем, как идти дальше. Так, вижу, что не все помнят. Этот
значок означает, что минус hgt от x лежит в конусе. То есть, у нас была там матрица положительно
полуопределена, там норма вторая, было некоторое соотношение, это мы все записывали как n+, то
либо матрица лежит в конусе положительных полуопределенных матриц, либо вектор из n
плюс одной компонента лежит в соответствующем конусе, порожденной нормой нектом. Удалось ли
вспомнить? Вот эта вот величина минус h, ну, тут вот это меньше либо равно k, значит,
при умножении на минус единицу у нас получается, что минус hgt от x больше либо равно 0 в смысле
конуса, это значит, что минус hgt лежит в конусе. В конусе k. Написано k, это конус некоторый,
ну, там, правильный, там замкнутый, содержит 0, ну, 0 всегда содержит, неправильно говорю,
замкнутый, выпуклый, и ему не принадлежит прямая, чтобы там были выполнены все эти соотношения
порядка, про которые вам, наверное, много раз уже рассказывали, там, транзитивность,
рефлексивность, вся эта история, она следует напрямую из геометрии выпуклого стихонуса.
Запишем Lagrangian для этой задачи, может быть, не совсем привычная форма, но я надеюсь, вы сейчас
немного присмотритесь и увидите, что это ровно тот же самый Lagrangian, который был ранее, только
теперь g и h это типа вектор функций, которые типа g жирный это вектор из gитых, h жирный это вектор из
h житых. А так, суммирование с коэффициентом лямбда и tµ, это просто, конечно, произведение записано.
Ну, и отчуда двойственная задача будет записываться точно так же, только теперь у нас
μ станет необходимым, чтобы он принадлежал спряженному конусу. Понятно ли почему?
Смотрите, что мы хотели, чтобы у нас, когда мы выводили двойственные задачи, у нас была
некоторая надобность в том, чтобы, ну, то есть вот это все еще по-прежнему, независимо от того,
что происходит вот здесь, вот является, чем является выпуклой функции, ну, линейной функции при
фиксированном иксе. Тут ничего не поменялось. Но мы хотели, чтобы, когда до этого обсуждали,
что у нас ажитое было без вот этого конуса просто меньше либо равно нуля. И когда мы здесь записывали,
мы требовали, чтобы μ было больше либо равно нуля. Помните, да, что мы там оценку снизу получали,
потом и максимизировали. И это нас приводило к такому виду задачи. Только здесь был не ка со
звездочкой, был не ка со звездочкой, а просто было больше нуля без ка со звездочкой. Спойлер просто
потому, что сопряженный к положительному актанту это он сам. Тут, в общем-то, все абсолютно согласуется
с тем частным случаем, который был сначала разобран. Что нам надо теперь? Теперь нам надо, чтобы вот
эта штука была какой? Чтобы она была меньше либо равно нуля. Чтобы все те предположения и
выводы, которые мы делали ранее, выполнялись, надо потребовать, чтобы вот эта штука была меньше либо
равно нуля. Но это означает, что у нас же для аж мы знаем, что минус аж лежит в конусе. А раз у нас
минус аж лежит в конусе, то все те вектора, для которых это отрицательное, это просто элементы
сопряженного конуса. Так, хорошо. Все ли помнишь, что такое сопряженный конус? Зачем с этого? Нет.
Окей, давайте я напишу тогда. Да, наверное, здесь нужно добавить слайд про напоминание основных
определений. Наверное, тяжеловато так сходу напоминать. Смотрите, у нас был конус. Так,
тут надо какой-то цвет. Был конус, это конус. У нас был сопряженный конус. Это набор таких
у, что скаллярный произведение у на х было больше либо равно для любого вектора из конуса. Вот.
Ну и, соответственно, поскольку вот здесь вот у нас есть некоторая история с тем, что у нас
минус аж жито лежит в конусе, и нам нужно, чтобы это было меньше нуля в скаллярном произведении,
а не больше нуля, как было. Ну, то есть вот это вот больше нуля, оно становится меньше нуля,
потому что у нас минус аж жито лежит в конусе. Поэтому соответствующий множественный лагранж
должны лежать в сопряженном конусе. Удалось ли разобраться в нагромождении формул,
который приведен на слайде? Так, поднимите руки, кто понял? На что?
Вот эта штука, она... Сейчас, я, наверное, понял вопрос. Сейчас скажу секунду.
Ну, тут типа для любого х, который лежит, допустим, в области определения, минус аж жито лежит в конусе,
вот так. Ну, типа все образы лежат в конусе. Поэтому для всех образов надо потребовать,
иначе когда мы будем считать инфимум по х, мы не сможем сказать, что это любые х.
Смотрите, да, это вопрос решается следующим образом. Просто вот эта штука, то есть, видите,
здесь, я понял, что здесь не хватает. Тут не хватает индекса j, наверное, для полной картины. То есть,
типа у вас для каждого нерайса может быть свой конус. Понятно, да? Вот, и, соответственно,
вы вот этот кусочек, который хотите, вы его в качестве пересечения можете взять. В пересечении
конусов, и у вас все получится тогда. Просто это сопряжение будет считаться чуть более хитрой,
чем если бы это был декартом произведения. Ну, на самом деле, может быть, тут индекс j-то и не
нужен, а нужно просто сказать, что этот конус может быть декартом произведения конусов,
часть из которых будет константа какая-нибудь. Ну, типа, единицы, все остальное мы спроецируемся на
подпространство. И тем самым вот этот кусочек, который нам нужен, мы его как бы вырезаем за счет
того, что смотрим просто срез. Типа конус вот такой, а мы вот так вот его режем и получаем то,
что нам нужно. Вот, да. Так, еще какие-нибудь вопросы есть? Зачем все это надо? Да, ну, в общем,
сейчас я скажу, сейчас сначала приведу к ККТ, потом скажу, зачем все это надо. ККТ записывается
точно так же, только теперь все, что у нас касалось конуса, меняется на его сопряжение. Все
остальное не меняется. Вот. Этого легко достаточно показать, просто взяв выкладские не сильно
многочисленные с прошлой лекции и увидеть, что разница между ними только в том, только вот
здесь вот. А все остальное, что у нас было, оно как было, так и станет. Там никакого существенного
использования того, что у нас неравенство, это неравенство в обычном привычном нам смысле,
просто мы чисто с нулем сравниваем, нигде не использовалось на самом деле. Поэтому замена
обычного неравенства на обобщенное приводит только к появлению вот этой вот модификации. А все
остальное будет работать точно так же. Вот. В общем, поэтому все утверждения, которые мы делали для
все утверждения, которые мы делали для произвольных неравенств, будут работать и для обобщенных там
парусовисейтеров и все такое, только вот да, усовисейтер будет вот так вот выглядеть теперь. То есть
нам будет важно, чтобы было строго, то есть чтобы он лежал именно внутри конуса, короче говоря,
минус FIT от X с крышкой. Вот. Так. Ну да, 20 минут, нормально. Теперь давайте посмотрим, собственно,
на то, как будет выглядеть двойственная задача для задачи оптимизации в конической форме.
Напоминаю, как это выглядит. У нас линейная целевая функция, линейное ограничение равенства и
ограничение на X, его принадлежность к конусу. Минус X, больше нуля, да, X лежит в конусе. Все
то же самое, что было в линейном программировании, только теперь кассоведочка появилась. И сейчас,
наверное, как раз-таки будет правильным показать, почему это так. Я думаю, за 5 минут мы его вырву.
То есть что было, ой, была вот такая задача. Правильно хоть написал? Да, вроде.
Что происходит? Строим для него лагранжаан. C транспонированный X плюс лямбда транспонированный,
AX минус B. И наша двойственная функция же от лямбда. Ой, это инфимум. Ни у кого пока вопросов нет.
Че надо еще добавить? Вопрос на понимание. Да, надо добавить, что здесь мы берем инфим только
по тем иксам, которые уже лежат в нашем конусе, поскольку мы их не включили в
лагранжаан. Понятно, откуда это взялось? То есть ровно то, о чем я говорил несколько слайдов назад,
когда про разные формы задачи мы обсуждали. Прекрасно. Смотрим дальше. Это что такое?
Во-первых, тут есть часть, которая не зависит от икса. Мы ее сразу давайте выкинем. И у нас останется C плюс,
так вроде. Добавляйся. Всем понятно, что произошло? Просто вынесли внимание и смотрим,
что получается. Получается, что наша двойственная функция – это инфимум такого замечательного
скалярного произведения, которое есть линейная функция по иксу. Что мы знаем про это инфимум?
Громче. Не стесняйтесь. Не совсем. Ну, почти не совсем так. Достаточно аккуратно. Смотрите,
мы видим, что нам нужно, чтобы этот инфимум… его как-то получить надо. Что получить инфимум,
что надо сделать? Как искать инфимум? Тревожная тишина.
Любую непонятную ситуацию дифференцируй. Смотрите, что мы знаем. Мы уже знаем, что у нас,
если мы смотрим на такие выражения, что-то видит скалярное произведение нашего элемента с каким-то
вектором, то мы знаем, что есть вот такая вот оценка, что эта штука всегда больше левра нуля,
если наш вектор будет из конуса лежать. Так? Почему рано инфимум? Ну да, хочется сказать,
конечно, что мы вот эту вот инфимум, говорим, что это ноль при условии, что это лежит в нашем
сопряженном конусе. Потому что для всех иксов из этого конуса это выражение больше левра нуля.
Понятно, я надеюсь. Теперь давайте поймём почему… да, давайте теперь сделаем перерыв и вопрос на
перерыв понять, почему, если эта штука не лежит в конусе, то тут будут какие-то проблемы.
Вопрос понятен? Да, нет. Потерялись на предыдущем шаге, да?
Да, то, что сломается. Да, вот, собственно, это действительно проблема, на которую надо подумать.
Так, всё, пять минут, да, пять минут вроде. Да, короче, в 32 минуты продолжим. Работает,
давайте расскажите всем. Ага. Да, то есть если эта штука не лежит,
то внукс получается неограничен, и мы получаем минус бесконечности. Давайте ещё раз. Смотрите,
если эта штука не в сопряженном конусе, это значит, что существует икс из конуса,
на котором это скалярное произведение отрицательно. Это значит, что умножив этот самый икс на положительное
число, мы не выводим его из конуса, но делаем значение бесконечно маленьким. Бесконечно маленьким,
в смысле, минус бесконечности, а не веським к нулю. Вот. Получаем минус бесконечности,
получаем неограничность. Не понял и громче. Это никак не мешает, поскольку всё то же самое можно
проделать для, ну, короче говоря, в результате у вас тоже получится конус, и для того же самого
причине конусов конус, в общем. Да, так что тут все эти оговорки, они технические в каком-то смысле.
Так, вопросы, предложения, замечания. Всё понятно? Таким образом, получаем вот такую вот, а ну да,
я же не договорил. Значит, получаем, что у нас двойственная функция, минус лямба транспонированная
b, будьте здоровы, мы её максимизируем, при условии каком, что c плюс a t лямбда лежит в сопряженном конусе.
Окей? Но, ну мы можем как бы поменять максим на минимум, и возможно тут еще как бы некоторая
замена переменных типа лямбда на минус лямбда, но по-моему это вот тут не, а нет, это ровно тут,
это и тут и сделано, да? Что, ну смотрите, это что значит? c плюс a t лямбда больше либо равна нулю,
в смысле ка со звездочкой, ну и c больше либо равна, в смысле ка со звездочкой, чем a t лямбда. Вот,
ну и что получилось то, что написано на слайде? a t лямбда меньше либо равно. Так, там какая-то
проблема с минусом возникла, надо будет перепроверить. Ладно, в общем, здесь с точностью до, тут с точностью
до минуса или вот здесь вот какая-то проблема со знаком. Вот, в общем, идея в том, что зная ка и то,
что он, если он самый сопряженный, то вот это ка со звездочкой вы знаете сразу. То есть, что произошло?
Вы, допустим, у вас есть функция, которая строит, то есть у вас есть функция, которая принимает на
вход c, a, b и конус в каком-то виде. Вот, имея эту информацию, вы автоматически можете построить двойственную
задачу, просто сказав, что целевая функция линейная, тут b стоит, тут стоит a транспонированная, тут c,
тут ка со звездочкой. То есть, вы ничего не делаете, вы просто привели задачу к такому виду и сказали,
ага, ну а теперь я просто подставляю и получаю правильный ответ. То есть, уже никакой там
инфим благранжа, но все вот это сидит внутри ка со звездочкой, ничего не надо делать. Понятно ли
сила этого аппарата? Или не очень? То есть, зная таблицу конус его сопряженный, вы сразу получаете
способ записи двойственной задачи. Окей, значит, вот именно для того, чтобы вот это все можно было
настолько автоматически записывать, мы и изучали конусы сопряженные, конусы и двойственные,
все эти постановки. Потому что в дальнейшем мы будем решать задачи понятное дело как. Мы будем
записывать, у нас будет прямая задача, у нас будет двойственная задача, мы будем обновлять
одновременно и прямые перемены, и двойственные. И таким образом контролирует зазор двойственности,
который здесь как бы автоматически вы можете посчитать, поскольку у вас все функции линейные,
вы знаете, как они выглядят. Вы можете контролировать сходимость вашего метода. Вот такое чудо природы.
Понятно ли перспектива такого подхода? Да, нет. Непонятно. Давайте еще раз. Вы
сейчас я подумаю, может быть стоит лучше перенести это обсуждение на следующий раз,
когда мы более конкретно про это поговорим. То есть смотрите, когда вы решаете исходную задачу,
вы можете ее как-то решать. Но для того, чтобы проверить, что ваш метод, который ее решает,
который, ну то есть метод, который решает, это такая некоторая штука, функция, которая генерирует
по следованию иксов, которые сходятся к решению. То есть x1, x2, x3, x5, x10, x100. И мы хотим,
чтобы в конце мы пришли как со звездочкой, которая будет решением. Вот. Чтобы проверить,
что ваш текущий x близок, далек от решения, где вы сейчас находитесь, надо вам еще что-то
итерироваться или уже нет, надо, ну стандартный и самый качественный функционал сходимости,
это взор двойственности. Это понятно. Мы это проходили прошлого, или надо напомнить. Было,
отлично. Он всегда больше либо ровне нуля. Можно. Это разница между вот этим значением и вот этим
значением. Вот. Соответственно, если вы имеете одновременно и прямую задачу, и двойственную,
то вы можете итерировать здесь x, чтобы это стало поменьше, а здесь итерировать лямду,
чтобы это стало побольше. И измеряя разницу между получаемыми значениями целевых функций,
получить x и лямда такие, что вот это отличается от вот этого на 10-6, условно говоря, и это будет
индикатором того, что вы пришли к решению прямой двойственной. Вот. Но в чем магия-то? В том,
что вот эту задачу вы не записываете руками, вы передаете солверу вот эти параметры,
конус C, A, B, и он внутри себя автоматически все это делает. Поскольку вот для этой формы эта
форма автоматически строится. То есть не надо ничего делать, по сути дела. Все за вас сделано.
Понятно ли почему теперь и что происходит? Окей. В следующий раз про то, как именно это
происходит и что делать, когда у вас вот такая вот, когда у вас не вот коническая форма, а вот
такая вот, и тут просто допустим стоит меньше либо равно нуля, тут линейная, тут выпукла,
тут выпукла. Как произвольно выпуклую задачу сводить коническую, в следующий раз будет подробно
рассказано. Потому что пока это некоторая дыра между нашим предложением, то есть мы
обсудили, что происходит с выпуклым задачей, мы обсудили, что происходит с коническими,
а как по-одно привести к другому пока непонятно. Вот. Так, да, теперь, ой, ну вот, мое время пропало,
один слов. Теперь посмотрим, что происходит, как строить двойственную задачу для задачи
по определенной оптимизации. Исходная задача у нас была в таком виде, и мы обсуждали, что это то
же самое, что и линейное программирование, только теперь у нас вместо строк с транспонированной и
аитых полупоявились матрицы. А так, следы, это все еще скалярные произведения. Только не для
векторов, а для матриц. Помните, было такое? Двойственная задача ставится, внимание,
абсолютно так же, как и для, то есть смотрите, что происходит, здесь линейная функция,
тут тоже линейная функция, и это наш конус. Вот что мы имеем, и теперь мы можем явно написать
аналогичную формулу только с учетом специфики того, что это матрица, и что тут у нас следы
фигурируют. Давайте поймем, как это делается. Так, сейчас я перепишу исходную формулу. Ой, так.
Так, и тут, в общем-то, все уже написано, да, плюс-минус. Так.
Так вроде получше, да? Так, ну давайте смотреть, что у нас имеется. То есть первое, что мы замечаем,
что вот это условие то же самое, что у нас х лежит в Sn+. Конус самосопряженный, поэтому то,
что у нас будет там какое-то неравенство, у нас все еще будет Sn плюс сопряжение, это все еще
Sn плюс, это мы получали. Ну вот. Теперь давайте смотреть, что нам надо. Нам надо, чтобы целевая
функция была лямдо-транспонированная на B. Так, что у нас тут B? У нас была исходная задача вот
такого вида, B это был вектор, тут вектор B тоже присутствует, просто он записан по-компонентно.
B это наши элементы вектора B. То есть наша целевая функция в двойственной задаче, это лямдо-транспонированная
B, а тут все хорошо. Двойственная функция. Интереснее, что будет происходить в плане ограничений.
В ограничениях у нас фигурирует выражение вида C плюс транспонированная лямдо. Что это такое?
Значит, C это тот вектор, который фигурировал целевой функцией исходной задачи.
Вот. В нашем случае это матрица C. Это выражение будет конвертироваться в некотором смысле. Так,
давайте я напишу так. C плюс АТ лямдо. Тут такую стрелочку большую. Это что такое? Это C плюс. Теперь
давайте поймем, что такое АТ лямдо. АТ лямдо это... Что это? Точнее так, не что это, а какая у этого
интерпретация? Так, а вот если вы скажете громче, уберете руку от рта, будет совсем отлично. Еще раз,
еще раз. Нет. Взвешенная сумма столбцов матрица транспонированная, хотя бы. Почему? Именно так.
Меняете строки столбцы и получаете, что это взвешенная сумма строк.
Непонятно. Давайте я распишу. Что было? Было изначально АХ равно B. Вот это выражение это сумма АИТХИТ,
где АИТ это столбец.
Так? Да нет. Да почему? X. Смотрите еще раз. Вот у вас матрица. Умножите на вектор.
Вы строку умножаете на столбец. Следующую строку на столбец. Это значит, что вы каждый столбец
умножаете на соответствующий индекс и складываете. Получаете вектор. То есть думаете об этом никак,
что вы получаете компоненты вектора. А как вы получаете весь вектор? Все, разобрались. Все, шикарно.
Так. Возвращаемся к нашей записи. Считат транспонированной лямбда. Это мы взвешиваем строки
матрицы А с коэффициентами лямбда ИТ. Осознали? Шикарно. А здесь у нас были строки матрицы А умножались
на вектор Х. Получались бы АИТы. Теперь в нашей задаче вместо строк матрицы А у нас матрицы АИТы,
которые также умножаются скалярно на матрицу Х, которая нам неизвестна. Поэтому, чтобы получить
аналог А транспонированная лямбда, нам нужно посчитать взвешенную сумму АИТых.
Потому что они играют роль в строк матрица, который изначально фигурировали в исходной
задаче. Кто осознал аналогию? Поднимите руки. А кто не осознал аналогию? Поднимите руки.
Не очень, да? Ну смотрите, как бы еще объяснить. Когда мы записывали двойственную задачу,
вот тут понятно, что происходит. Вот А транспонированная лямбда, это мы складываем
строки матрицы А с весами лямбда ИТ. Это понятно. Отлично. Теперь смотрим на вот эту задачу. Вот здесь у
нас след. Это скалярное произведение матрицы АИТы на Х. В случае исходной задачи у нас было
скалярные произведения строки матрицы АИТы на вектор Х. Каждое такое скалярное произведение у нас
заменилось на произведение матрицы. Поэтому теперь в исходной задаче строка матрицы АИТы
превратилась в полноценную отдельную матрицу АИТую. Так, вот этот момент понятен или не очень?
Это понятен. Хорошо. Теперь, когда мы смотрим на вот это вот выражение, то это линейная комбинация
строк АИТых. Вот она. Линейная комбинация строк с теми же самыми коэффициентами лямбда.
Лучше стало. Окей. Замечательно. Получили вот это выражение, которое теперь должно лежать в сопряженном конусе.
Так, ну как-то так. Давайте посмотрим, что получается в исходном. Да, там вот с точностью до знака,
как я уже сказал. Тут, кстати, более понятно, откуда что берется. То есть мы изначально должны
были максимизировать минус лямбда транспонированная В. Сейчас, я тут что-то...
Будто бы что-то сломалось по пути. Надо понять, что именно сломалось. Лежит в конусе, да.
Да, в общем понятно, что произошло. Вот. При условии, что C плюс вот эта самая сумма лежит в сопряженном конусе.
Да, могу-могу, но тут немножко надо будет повычитывать формулы на предмет лишнего минуса,
который записался в слайдах, похоже. Или если не записался, то хотя бы написать пояснее,
откуда это взялось. Пока это немного не соответствует друг другу. Так, больше-меньше линейная комбинация,
плюс C максимум минус лямбда ТБ. Да, то есть от сюда получаем, что минус лямбда ТБ и тут вот
замечательное ограничение. Заметьте, что исходная задача у нас была на матрицу, а двойственная задача
стала на вектор. Видно разницу? Вот, то есть вроде как стало полегче, но ограничение типа
неположительной определенности, положительной полуопределенности, оно все еще осталось. Вот, так.
Да, значит, хитрый момент. Условие слейтера для этой задачи формулируется так, что теперь нам
нужна матрица Х такая, что она будет строго положительно определена, то есть лежать именно
внутри конуса и выполнена соответственно равенством. Важный момент, который, собственно, пока мы
активно пользовались тем, что это примерно линейное программирование, там все то же самое,
оказывается, что здесь такой, что называется, такого легкого, легкой процедуры решения уже не
будет, и не всегда, если множество не пусто, то для него будет выполнен условие слейтера. Вот,
и тут оно становится существенным. Вот, и сейчас посмотрим конкретный пример, в котором это будет
себя ярко проявлять. Рассмотрим вот такую вот простую задачу. Она является, значит, полуопределенной
оптимизацией, хотя кажется, что нет. Всем ли понятно, в каком виде что здесь представляется. То
есть, вот эта задача, это есть не что иное, как вот эта задача на самом деле. Ну да,
устоишь до знака. То есть, линейная функция здесь, и линейная комбинация матриц с коэффициентами
положительно полуопределенна.
Все осознали, все увидели. Отлично, давайте, что такое матрица С? Да, матрица С единица вот здесь,
все остальные нолики. Вот, давайте сейчас распишем эту задачу. Так, сейчас я сначала продублирую, чтобы.
Да, конечно.
Так, тонала первых всего лишь двумерная. На плоскости можно нарисовать при желании. То есть,
вот эта штука, это что такое? Это единица плюс x1, на что? На 0, 1, 0, все нули, большие нули,
плюс x2, на что? 1, 0, 1, 1, 0, и тут вот так. И вот так. Давайте найдем решение. Чтобы найти решение,
надо сначала допустимо множество оценить. Ну, как-то проанализировать. Какое здесь допустимо множество?
Как это все на самом деле расписывается?
Ну да, критерис или вестра, конечно. То есть, что у нас есть? У нас есть, я буду писать, x2
плюс 1 больше либо равно 0, x1 больше либо равно 0, и самое важное, кажется, что x2 в квадрате,
в чем больше либо равно 0, да? Ой, что-то не то. А, минус x2 в квадрате больше либо равно 0, да,
вот так. Но отсюда уже следует, что x2 равно 0. Понятно ли, откуда я это взял? Поделитесь,
если кто понял, откуда я это взял. Так, отлично, ура. Нашли ответ. Это 0, да? Всем понятно. Супер.
Теперь, смотрите, важный момент. Когда это все решается на компьютере, то никто критерий
севестра не проверяет никогда, потому что это масштаб проблем, я думаю, понятен, да? Типа,
у вас матрица N на N, и у вас там неравенства этих нелинейных будет довольно много. Поэтому,
когда вы вот так вот пишете, то solver автоматически приводит задачу к такому виду и строит двойственную
для нее. Понимая, какие матрицы A где... Понимая, какие матрицы C, A и T, где тут вектор B, и для задачи
в таком виде есть автоматический инструмент, который строит вот такую вот. Ну, то есть, как бы,
я еще не сказал, что если вы возьмете двойственную для вот этой, вы получите вот эту. Они как бы друг
в друга переходят. Это несложно показать достаточно. Давайте же проделаем эту манипуляцию. Тут,
кстати, по-моему, ответ уже написан. Да, ну так и допустим 0, что написали, оптимальное значение 0.
Да, давайте не будем подглядывать слайды и выведем, как будет выглядеть двойственная руками, чтобы
показать, как это все записывается. Значит, нам нужно... Что? Мы ищем матрицу. Давайте я ее у обозначу.
Тут, значит, C у, A и T, у равно 0, и у вот такой. Значит, мы уже поняли, что C у нас это вот это,
это у нас A1, это у нас A2. Ой, тут B и T. И осталось понять, вектор B у нас какой.
Почему равен вектор B?
Тут остался так много саундных переменных, которые мы и не использовали.
0,1, да, конечно. Потому что у нас вектор B сидит вот здесь. Это, на самом деле,
наша B-транспонированная Х. Х из двух компонентов, там сфигурирует Х2, поэтому 0,1. Прекрасно
записываем, как это выглядит. Смотрите, какой у нас... То есть теперь у нас было две переменных,
стало N на N-1 пополам переменных. Потому что нам целую матрицу теперь надо определить.
След матрицы C на некоторую матрицу. Чему равен? След произведения. Матрица C имеет довольно
специфичный вид. Но Y1,1, конечно. То есть нам надо фактически определить Y1,1 при условии,
что Y у нас вот такой, и выполнено еще два равенства. Давайте их поймем, какие это будут равенства.
А1 мы знаем, и А2 тоже знаем, и B1, B2 тоже знаем. Давайте продиктуйте, что получится.
Y2,2,0 и еще раз, что-то не то. То есть тут же есть довольно хитрые единички,
которые начнут играть свою роль немаловаженную. Ну, след от произведения матрицы на вот такую
матрицу. Чему равен? Тяжело как-то идет. Громче, не поэлементное. Но если вы вытягиваете,
просто мне проще вытягивать три произведения вектора 3 на 3, чем вытягивать вектор 9 на 9,
умножать его, а потом склад. Параллельно три штуки 3 на 3 проще удержать в голове,
чем одну большую 9-мерный вектор. Да, это правда. Так, давайте проверим. Вроде правда, да? Минус
Y1 на 1 почему-то. Почему? А, потому что там стоит что-нибудь в знаке, да? Ага, понятно. Ладно, сейчас.
То есть, да, с равенством, со следами все было в порядке. Вот. Матрица C.
Я скажу, в чем было дело. Давайте мы посмотрим сейчас. Да, тут вот, видите, все-таки вот эта штука,
она сыграла какую-то роль. Так, исходное значение у нас было минус X2, да, и мы от этого минуса избавились,
сказав, что у нас вектор минус ушел из этого и минус остался здесь. Так, давайте сейчас я поставлю
здесь минус, чтобы не сломалась двойственность. Сейчас увидите, что произойдет, если там минус
не будет. Вот, и да, надо будет до конца починить знаки в слайдах. Так, окей, по модулю вот этого
минуса, я надеюсь, понятно, откуда это взялось. Супер. Теперь давайте решим эту задачу. Она,
оказывается, не такой сложной, как может показаться на первый взгляд. Смотрите, вот эта штука нам
дает к систему неравенств на элементы матрицы Y, который у нас фигурирует вон там и вот здесь.
Давайте сейчас, я даже не полюнюсь, ее аккуратно распишу. Хотя половину писать не надо, ну ладно.
Вот так. И мы знаем про ее положительную полуопределенность. Что здесь будет нас интересовать?
То есть Y1,1 у нас тут уже заведомо больше или бравее нуля. А, Y2,2,0, отлично. Круто, да? Это
значит, что вот это вот minor, если мы на него посмотрим, что он нам скажет? Он нам скажет,
что Y2,3 на Y3,2 меньше или бравее нуля. Что теперь надо вспомнить? О чем я немножечко забыл,
записывая матрицу. В точку. Она симметрична. Поэтому вот здесь вот на самом деле фигурирует Y2,3 в
квадрате. И поэтому вот это вот первое, второе равенство замечательное, оно оказывается,
что Y1,1 в точности равен 1. А и наш ответ минус 1. Теперь смотрите, то есть тут D со звездочкой
минус 1. Если бы тут минуса не было, то мы бы сломали двойственность, потому что получили бы,
что значение двойственной функции больше, чем оптимальное значение прямой задачи,
которая равна нулю. Вот поэтому должен быть минус и в знаках, которые фигурируют в слайдах
вкралась какая-то неточность. Отсюда следствие. Да, ответ минус 1. Смотрите, что получилось. У нас
есть одна задача, есть другая задача. И там, и там допустим множество непустых. И зазор двойственности
положителен, равен 1. В чем проблема? В чем проблема? Да, именно так. Условие сеттера не выполняет.
Это значит, что для, вот здесь, точнее, наверное, правильно показывать, вот для, вот допустим,
не существует строго положительно определенной матрицы, для которой было бы выполнено вот эти
ограничения равенства. Ну да, для которой были бы выполнены эти ограничения равенства. Вот таким
образом это существенно отличает задачу полуопределенной оптимизации от линейного
программирования, в котором, значит, если не пусто, то точная сильная двойственность будет выполнена,
все будет работать замечательно. Вот здесь возможны вот такие вот патологические
эффекты, которые будут в свою очередь влиять на то, как solver будут
оперировать и что они вам будут дать, потому что у вас вот двойственные
переменные, которые будут соответствовать этой задаче, просто бесконечно будут
улетать и вам будут говорить, что что-нибудь там infeasible или unbounded,
наверное unbounded. При этом решения существуют, все в порядке, и там и там решения
существуют, просто они разные, не совпадают. Так, вопрос есть по этому
примеру и по, в целом, по двойственности и, в частности, конической двойственности,
которая существенным образом упрощает получение двойственной функции, двойственной
задачи.
Так, всем все понятно? Красота. Так, ну тогда давайте заканчиваем. Значит, сегодня мы
обсудили, что, обсудили, какие преобразования можно делать с задачами, чтобы
получать двойственные задачи различного вида, посмотрели на двойственные задачи
для линейного программирования, для общенных неравенств и для конической
двойственности. Посмотрели, в чем разница между полупределенной оптимизацией
и линейным программированием. В следующий раз, собственно, как я уже анонсировал,
будем смотреть на то, как из произвольной задачи выпукла оптимизации собрать
коническую задачу. То есть, откуда берутся конусы, почему они, ну то есть, и почему
зачастую бывает так, что они там самосопряженные, и что большинство
задачи из них так или иначе записываются. Ну там плюс еще экспоненциальный есть, вот.
И как запись задачи в конической форме, ну то есть, как запись задачи двойственной в конической
форме, можно обратно преобразовать в задачу в обычной форме виде неравенств, например. Это все
довольно делается прямолинейно. Ну, просто, у вас был условно, у вас было в исходной, значит,
ограничение вида там ax-b первая норма меньше либо равно чему-то, вы говорите, окей, это значит,
что у меня вектор ax-b, то, что стоит справа, лежит в конусе, порожденном первой нормой, вот. Раз
эти перемены лежат в этом конусе, значит, спряженные будут лежать в конусе, порожденном
бесконечной нормой. И, соответственно, просто выписывается то, что набор переменных лежит в
таком конусе виде неравенств. И это, кстати, еще один момент, о котором тут в этих слайдах я не
говорю, но буду говорить в следующих, что такой подход позволяет строить двойственные к задачам,
ну, упрощает построение двойственных к задачам, которых целевые функции или ограничения не
являются дифференцируемыми. То есть мы существенным образом в процессе построения опирались на то,
что мы там можем градиент посчитать, что у f0, что и у ограничений. Вот, при этом никто не обещал,
никаких ограничений при именно получении этих самых величин использовано не было. Поэтому,
если у вас там идет что-то не дифференцируемое, то это не значит, что двойственные задачи не
существуют. Просто, чтобы ее найти, надо немножко там, ну, либо пользоваться техникой для негладких
задач. Я ее специально не рассказываю, потому что, чтобы ее нормально рассказать, надо там,
типа, две-три лекции потратить, а выхлопать от нее на практике не очень много, потому что все в
итоге сводится конической двойственности, которая записывается автоматически. Поэтому я решил
лучше про это поподробнее поговорить, чем рассказывать про обобщение градиентов на негладкие функции.
В общем, следующий раз будет, как я уже сказал, небольшое введение в пакет решения задач
оптимизации, на каких принципах они строятся, и посмотрим, в чем разница в их использовании,
в частности, удобство versus обобщаемость некоторая. Ну, не обобщаемость, а широта класса задач,
на которых вы можете тот или иной инструмент использовать. Так, на этом все удивительно вовремя,
даже все успели. В общем, 14 слайдов, видимо, это оптимальное количество, более-менее, в окрестности того,
сколько можно успеть рассказать. Всем спасибо, в следующий раз, надеюсь, продолжим.

Сегодня я хочу продолжить рассказ про вероянстое вычисление. Надеюсь, я немножко успею поговорить
про дарандомизацию, то есть как вообще избавиться от вероятностей, по крайней мере в некоторых случаях.
Что про вероятность надо сказать? Во-первых, нужно поговорить про амплификацию.
Амплификация или уменьшение ошибки.
Проще всего поговорить для класса RP. Я напомню класс RP с односторонней ошибкой.
Это означает следующее, что существует пальномиальный алгоритм такой, что если х лежит в А,
то тогда вероятность того, что этот алгоритм, скажем, м от хр, выдаст единицу,
значит, эта вероятность больше либо равна 1 и 2, значит, если х не лежит в А, то тогда вероятность того, что м от хр
равно 1, значит, это будет равно нулю. Значит, вот это односторонняя ошибка. То есть если сказали да, то точно ответ да.
Если сказали нет, то непонятно. Может быть, на самом деле нет, а может быть, так ошибка произошла.
Так вот утверждение следующее, что вместо 1 и 2 можно поставить любой порог, вместо 1 и 2 можно поставить любую границу на некоторых диапазонах.
Можно поставить любую границу от единицы делить на n в степени c, где n это длина х.
Значит, до 1 минус, а тут будет 1 делить на 2 в степени n, степени d.
Вот, то есть вот этот разрыв может быть от обратного полинома до 1 минус обратного экспонента.
Ну, значит, сильнее расширить уже нельзя, значит, если здесь мы доведем до единицы, то фактически можно любой r взять,
если я его подставить, то это будет p, а если мы здесь сузим и сделаем, скажем, обратную экспоненту, то будет np.
Если тут будет 1 длить на 2 в степени длина сертификата, в степени длины r, то это будет в точности np.
Но в этом диапазоне все будет одно и то же. Здесь в том числе будут любые константы между 0 и единицей.
Так, значит, как это делается? Ну, нужно повторять много раз доказательства.
Можно повторять много раз и брать дизьюнцию ответов.
Значит, смотрите, если на самом деле ответ нет, то это будет дизьюнция нулей.
Если х не принадлежит А, то будет дизьюнция нулей. Ну, то есть ноль получится.
Ну, в смысле вероятность единицы будет нулем.
Вот, если х лежит в А, то при к независимых запусках...
Значит, вероятность нуля будет 1 минус епсилон в катой степени.
Ну, диапазон как раз то, что мы вместо 1 и 2 подставляем.
Тут, чтобы дизьюнция была нулем, нужно, чтобы каждый раз был ноль.
Соответственно, вероятность единицы больше либо рано, чем вот эта епсилон, значит нуля не больше, чем 1 минус епсилон.
Ну, тут а тоже не больше, чем 1 минус епсилон.
Ну, а теперь вспоминаем матан, первый семестр, пределы.
Если епсилон это 1 делить на n степени c, а k это n степени c плюс d, то тогда как раз и получится.
Тогда как раз вот это 1 минус епсилон в катой степени будет примерно равно, чем e в степени минус n степени d.
Ей это больше 2, соответственно, это будет больше, чем 2 в степени минус n в d, а это как раз то, что нам нужно.
То есть меньше, наоборот, нам и нужно меньше, чтобы вот эта вероятность нуля была маленькая.
Ну вот, так и получается. Так и получается, что вот от такого епсилона пришли к вот такому.
Так, ничего, понятно?
Это для односторонней ошибки это простое рассуждение, для двусторонней немножко посложнее.
Так, давайте я тут напишу, напомню. Двусторонняя ошибка это BPP.
То есть тут получается, что если x лежит в A, то тогда вероятность того, что m от xr равно единице будет больше, чем 2 третьих.
Вот, соответственно, если x не лежит в A, то тогда вероятность того, что m от xr равно единице тут будет меньше либо равно, чем 1 треть.
Вот, ну и вот тут, если тут альфа, а тут бета, то тогда утверждение такое, что класс не поменяется для всех альфа и бета.
Класс. Один и тот же.
Значит, для всех альфа, бета, таких что?
Значит, тут соответственно альфа будет больше, чем 1 делить на 2 в степени D.
Значит, бета будет больше, чем альфа плюс 1 делить на 1 в степени C, и бета меньше, чем 1 минус 1 делить на 2 в степени D.
Да, то есть как бы по краям между альфа и бета и краями должна быть обратная экспонента, а вот между альфа и бета должен быть обратный полином.
Так, ну что, утверждение понятно?
Значит, идея доказательства такая.
Идея доказательства заключается в том, что мы точно так же запускаем много раз, и если это 1 треть и 2 треть, то голосуем по большинству,
а если произвольна альфа и бета, то сравниваем выборочные средние со средним арисметическим.
Значит, запускаем K раз и сравниваем долю единиц с альфа плюс бета пополам.
Дальше есть некоторая вероятностная выкладка, которую я хотел бы пропустить.
Так, а у вас прям червьер был с CPT?
Да.
А слов типа не раз в Черново вам не говорили или говорили?
У нас на семинаре было.
О, на семинаре было.
Хорошо, сейчас, на семинаре по вероятности или по сложности?
По вероятности.
По сложности, а то мы общались, что будет использоваться.
Будет использоваться, да.
Ой, наоборот, альфа и бета наоборот нужно.
Альфа нижняя граница, да, а бета верхняя.
Вот так.
Но, в общем, я не буду прям выкладку делать, я нарисую картинку.
Знаете, смотрите, чему нас учит CPT?
К тому, что если мы много раз вот такую вот бернуливскую случайную величину независимо запускаем,
да, и берем средняя, то это будет колоколообразная функция.
Соответственно, есть как бы пороги вот альфа и бета,
а еще есть настоящая средняя, которая либо левее альфы, либо правее бета.
То есть вот какое-то, вот здесь вот будет какое-то настоящее среднее.
Соответственно, тут будет какой-то колокол, который как-то вот так вот будет идти.
Ну а вот здесь вот альфа плюс бета пополам.
Соответственно, если мы попали вот в этот правый хвост, то тут будет ошибка.
И нам нужно доказать, что эта ошибка может быть сделана экспедициально маленькой.
Но тут дело в следующем, что ширина этого колокола будет порядка 1 делить на корень из числа повторов.
На число повторов у нас k, то есть вот тут будет порядка 1 делить на корень из k.
А вот эта вот штука от альфы до альфы плюс бета пополам, она у нас хотя бы 1 длительная степень С.
Ну и, соответственно, ошибка, поскольку это нормальное распределение,
то ошибка будет экспедициально малой от того, сколько раз ширина этой сигны уложится.
Но если она вот настолько экспедициально малой должна быть, то, соответственно, должно порядка n в степени D раз уложиться.
Ну как раз получается корень из k типа n в степени С.
Так. Ну вообще там какое-то многочлено получается.
Наверное должно быть 2C плюс D, что ли.
Ну ладно, в общем, короче говоря, вот это k должно быть полиномом от m.
А каким именно это как раз получается из точных выкладок с неразрешением Чернова?
Да, собственно, почему нам CPT недостаточно?
Потому что она именно предельная теорема, а нам нужно конкретное полиномиальное число повторов.
И вот поэтому нужно неразрешение Чернова, что именно такого числа повторов хватит, чтобы этот график был достаточно близок,
чтобы можно было сделать вывод о том, что ошибка маленькая.
Так, ну что, интуиция понятна?
Вот, хорошо. Ну, значит, выкладки либо для самостоятельного изучения, либо насчет интуиции.
Вот, потому что дальше я хочу успеть поприменять это.
Теперь какая идея?
Идея, что ошибка может быть сделана экспоненциально маленькой,
и в том числе это может быть экспонента больше, чем размер входа.
То есть знаменатель в экспоненте не показатель, в смысле.
Нет, показатель, да, показатель в знаменателе в экспоненте может быть больше, чем длина входа.
Вот, это очень важное соображение.
Это очень важное соображение.
Сейчас на его основе я расскажу две теоремы,
которых, в принципе, можно считать дарандомизацией.
То есть как превратить вероятностные алгоритмы во что-то детерминированное,
но, правда, не в П, но во что-то похожее.
Значит, одна теорема связывает BPP и неравномерную сложность,
а вторая теорема связывает BPP и полинарную рафхю.
Сейчас, значит, с этим разберемся.
Первая называется теорема Эйдлмана.
Значит, если вы знаете про криптосистему RSA,
то Эйдлман как раз буква A в RSA,
он на букву A начинается.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Он на букву A начинается.
Вот.
Так.
Значит, теорема такая, что BPP вложена в П слеш поле.
То есть можно заменить неравномерный вероятностный алгоритм
на неравномерный детерминированный.
Равномерный вероятностный алгоритм можно заменить на неравномерный детерминированный.
Так.
Значит, как мы это делаем?
Ну, первым делом сделаем ошибку меньше, чем 2 в степени −m.
Так.
Доказательства.
Сделаем вероятность ошибки меньше, чем 1 делить на 2 в степени −m.
Значит, здесь n — это длина x.
Ну, что значит вероятность ошибки?
Ну, значит, дали неправильный ответ.
То есть это как бы альфа или 1-бета.
Значит, если правильный ответ 0, то альфа будет вероятность дать ответ 1.
Если правильный ответ 1, то 1-бета — это будет дать вероятность ответ 0.
Вот.
Значит, соответственно, ошибку мы сделаем вот такую.
Что это значит?
Значит, это означает, что существует такое r, что для любого x, значит, если длина x равно n,
то тогда этот алгоритт дает правильный ответ.
Значит, тогда получается, что v от xr равно 1.
Тогда и только тогда, значит, когда там x лежит va.
Да, то есть есть один конкретный, случайно, r, который дает правильный ответ ко всем x.
Почему?
Ну, это простая вероятность наценка, что каждый конкретный x меньше, чем вот столько забраковывает разных r.
Всего x как раз столько, соответственно, суммарно, все x меньше единицы.
Забракуют, значит, будет не забракованный.
Но и вот этот не забракованный мы и возьмем.
Так.
Ничего, понятно соображение?
Вот. Ну а после этого мы пользуемся тем, что это не равномерно.
Получается, что v от xr преобразуем в схему.
V от xr преобразуется в схему.
Это аналогично тому, как было p вложено в p-slash-поле.
Вот.
Но это будет схема от xr.
Дальше r просто можно, как говорят, запаять в схему.
А r можно зафиксировать вот таким образом.
Значит, соответственно, получится схема от x.
Ну и размер будет пальномиален, потому что v пальномиально.
Но от фиксации вообще размер не меняется.
Вот.
Так.
Ну чего?
Понятно?
Так, в общем, не очень сложное рассуждение.
Сейчас следующая теремия посложнее будет.
Кинуть, может, вопросы.
Так.
Значит, следующая теорема про вложенность в пальномиальную
иерархию.
Вот.
Это, в общем, тоже некоторое избавление от вероятности,
но ценой недотерминизма.
Тут, в принципе, даже не умеет доказывать, что
p меньше, чем pn exp даже, или что-то даже большее.
Хотя верят, что, наверное, bpp равно p, но не умеет отделять
от очень больших классов, да, еще больше, чем exp.
Вот.
Ну, зато если окажется, что bpp равно exp, тогда будет,
что p не равно np.
Да, это еще один такой способ.
Иронический способ, конечно, p не равно np, но, скорее всего,
так не получится, просто потому что утверждение.
Ну, и утверждение, из которого следует p не равно np, скорее
всего, неверно.
Так.
Хорошо.
Давайте напишу теорема.
Так.
Я ее атрибутирую тремя именами gacha, cipsa, relautemana.
Значит, bpp выложено в σ2-пальномиальное, в пересечение
p2-пальномиальное.
Вот.
Там такая была история, cipsa делал доклад на конференции,
и он доказал, что bpp вложено в ph на каком-то более высоком
уровне.
Ну, там, наверное, можно посмотреть, на каком.
Вот.
А gacha был среди слушателей, и он сказал, что да, это все
нормально, но можно упростить и показать, что на втором
уровне.
Вот.
И там что-то cipsa ему предлагал совместную статью, а gacha
говорит, да нет, вообще все очевидно.
И типа, ну, с другой стороны, это там ваш результат.
В общем, в итоге lautemana там как-то тоже все послушал
и еще упростил доказательства.
В общем, тот конец, который я буду рассказывать, придумал
lautemana по мотивам достижения gacha и cipsa.
О, это давно, это какие-то...
конец 70-х, наверное.
Так, нет, ну я могу в книжке посмотреть, там есть ссылка.
Ну ладно, давайте будем доказывать.
Так, сначала я нарисую метафорическую картинку.
Значит, сначала мы, конечно же, произведем амплификацию,
то есть у нас будет ошибка маленькая.
Соответственно, я буду там рисовать множество, скажем s,
это множество таких r, что m от xr равно 1.
Соответственно, вот будет две ситуации.
Одна ситуация, это если x лежит ва,
значит, тогда это множество, оно какое-то вот такое вот большое будет.
Почти всего пространства занимает.
Квадратик это все r, а вот это множество кривое,
это, соответственно, те r, которые, в общем, s.
Вот это вот s.
Если x лежит ва, значит, если x не лежит ва,
то тут, соответственно, это s, это будет какая-то крошечная часть всего пространства.
Дальше геометрическая интуиция, про которую мы дальше будем доказывать,
что она работает и в нашем пространстве тоже, потому что в нашем пространстве были в куб,
а вовсе не в квадрат, как я здесь нарисовал.
Тем не менее, я наглядно нарисую, что происходит.
Попробую, по крайней мере.
Идея такая, что если мы вот это большое множество немножко подвигаем во все стороны,
то оно там будет каким-то вот таким вот, вот так вот сдвинется.
Этот угол закроет, потом мы его сдвинем сюда,
да, но там как-то вот, соответственно,
ой, нет, так, это я не то нарисовал, вот так оно должно быть.
Этот угол закроет.
Ну и, соответственно, здесь тоже, я думаю, что это уже не очень будет видно,
но как-то вот так вот я части нарисую, и, соответственно, вот сюда вот.
В общем, таким небольшим числом сдвигов этого множества можно прямо все пространство покрыть.
А вот эту вот штучку, если вы сдвинете, то она даже если все копии будут не перекрываться,
то все равно оно все не покроет, потому что она маленькая.
Ну и тогда утверждается, что вот это и будет разделяющее свойство.
Значит, существует набор сдвигов такой, что любая точка будет покрыта.
Значит, существует набор сдвигов такой, что любая точка покрыта одним из двигов.
Ну ладно, не совсем грамотно.
Одним из образов, образом при одном из двигов исходного множества, если полностью.
Вот так.
Ну хорошо, значит, это то, что мы хотим.
Ну а теперь нужно доказать, что это действительно можно сделать,
что в нашем булевом кубе интуиция из Евклидова квадрата сработает.
Вы написали топор сигнала.
Это правда, да.
Но дело в том, что BPP за счет симметрии между 1 и 3, 2 и 3, 7,
BPP будет замкнут относительно дополнения.
Так что если мы вложили в sigma2, то тогда его дополнение в ложится в p2,
дополнение к BPP, то есть множество, не дополнение, а co-класс,
множество дополнений к языкам с BPP, это будет тоже BPP,
и с другой стороны ложится в p2.
Так, понятно, да?
Вот.
Так, ну хорошо, значит, как будем доказывать, что в булевом кубе все работает?
Так, а слышали ли вы про вероянностный метод в комбинаторике?
Дискретно в анализе в первом семестре, да?
Или сейчас вы тоже ходите?
Мы не ходим.
В третьем семестре был, да, очень хорошо.
Вообщем, здесь будет как раз он.
Вероянностный метод заключается в том, что мы возьмем случайные сдвиги,
и докажем, что вероятность отрицания будет меньше единицы.
Ну, следовательно, существуют сдвиги, для которых утверждение верно.
Так, и нужно еще оценить, чтобы их количество было не очень большим,
а инпульны мякоть.
Ну, в общем-то, это очень важно.
Так, и нужно еще оценить, чтобы их количество было не очень большим,
а инпульны мякоть.
Ну, а сейчас это само все получится.
Так, смотрите, пусть этих сдвигов К штук.
Пусть сдвигов К.
Значит, тогда чего мы хотим?
Мы оценим вероятность того, что существует такой х.
Так, давайте их как-нибудь обозначим.
Значит, эти сдвиги будут shift.
С1, С2 и так далее, СКТ.
Это просто случайные вектора такой же длины, как R.
Значит, случайные вектора длины R.
Ну, давайте закончим рассуждение, и потом сделаем перерыв.
Так, что мы хотим?
Мы хотим, что какой-то...
Так, нет, не х, а R.
Существует такой плохой R, что мы его как не сдвинем,
потому что все попали вне того множества, которое дает единицу.
Значит, существует R.
А тут будет, что V от х и R плюс S1 равно 0.
И так далее.
И V от х и R плюс СКТ.
СКТ равно 0.
Так, при том, что вообще этих R будет...
Ну, почти все.
Значит, при этом...
Количество таких R, что V от х и R равно единице,
поделить на 2 в степени длина R,
это у нас будет больше, чем 1 делить на 2 в степене.
Наверное, столько хватит, где на длина х.
Мы хотим подобрать параметры, чтобы это было меньше единицы.
Как мы это будем оценивать?
Значит, смотрите.
Во-первых, тут квантов существования по R,
то есть это объединение событий,
это будет меньше либо равно, чем просто количество разных R.
Значит, 2 в степени R умножить на вероятность для конкретного R.
Значит, это на самом деле одинаковая будет вероятность.
Умножить на вероятность того же самого,
только без квантов существования.
Тут V от х, R плюс S1 равно 0, и так далее.
И V от х, R плюс S ка т равно 0.
А здесь у меня уже конъюнция независимых событий.
Давайте напишем.
Они случайные, равномерные, независимые.
Соответственно, это уже будет в точности равняться.
Тут будет 2 в степени длина R умножить на вероятность для конкретного V от х, R плюс.
R плюс S и T равно 0 в ка т степени.
Ну а какая вероятность?
R тут фиксирована, S и T случайные.
Соответственно, нужно попасть в дополнение к этим.
Понятно, что если R фиксирована, то R ксс и T тоже равномерно распределено.
И нужно попасть не в это множество.
То есть это будет у нас меньше, чем 2 в степени длина R умножить на 1 делить на 2 в степени n в ка т степени.
И это должно быть меньше единицы.
Я напоминаю, это должно быть.
Ну, вроде ясно, какой ка нужно взять.
Должно быть, что ка на n у нас должно быть больше, чем длина R.
Ну, то есть ка больше, чем длина R делить на n.
Но это полином.
Это полином, потому что длина R сама полином.
Ну и в принципе все. На этом все схлопывается.
Ка мы выбираем.
Получаем, что вероятность меньше единицы.
То есть это то, что нам нужно, чтобы было последнее верно.
Значит, мы получили вероятность.
Значит, мы можем выбрать так, чтобы такого R не существовало.
А это означает, что любое R покрыто.
И, соответственно, вот эта формула верна.
Давайте перепишу теперь в этих обозначениях.
Значит, в итоге существует такой набор s1 и т. д. s ка т.
Для любого R получается V от X R плюс s1, что равно единице.
Или, и т. д., или V от X R сор с ка т. равно единице.
Да, ну и последнее, что еще нужно сказать, что если на самом деле ответ нет,
то этих сдвигов не хватит.
Но это с большим запасом выполнено.
Если настоящий ответ нет, то число вот таких R, таких, для которых вот это верно,
будет меньше либо равно, чем ка умножить на 1 делить на 2 в степени n.
Но это, конечно, если n не слишком маленькая,
а k это полином, то это будет меньше единицы.
То есть все R покрыть не получится.
Получается, что для любого R это не верно.
Так, ну теперь уж мне кажется точно все.
Вот эту формулу для σ2 мы доказали.
π2 получается приходом к дополнению.
Значит, соответственно, если настоящий ответ да, то эта формула верна.
Если настоящий ответ то не верна.
Вроде все получилось.
Так, ну что, киньте вопросы.
Ну вот, отсюда получается ироническое следствие.
Давайте я прямо напишу.
Такое ироническое следствие, что если вдруг bpp равно exp,
то тогда p не равно np.
Да, потому что если еще и p равно np, то p равно ph,
а bpp в ph, и тогда exp в ph, и exp равно p,
а это претворяет четыре время оба иерархия.
Ну, опять же, маловероятно, что именно таким образом
будет доказано, что p не равно np.
Скорее всего, посылка не верна.
Так, ну хорошо, давайте сейчас перерыв небольшой сделаем.
И поговорим после перерыва про дерандомизацию.
Значит, по крайней мере, в каких случаях можно избавиться от вероятности вообще.
Значит, дерандомизация.
Значит, дерандомизация.
Так, ну, давайте сейчас на простом примере покажу минимум один способ.
А так, не знаю, может быть, мы и второй успеем.
Значит, первый способ называется метод условных мата ожиданий.
Значит, метод условных мата ожиданий.
Но пример будет такой.
Значит, задача MaxCut.
Задача MaxCut.
Максимальный MaxCut.
Да, не MinCut, а MaxCut.
Ну, например, есть группа людей, некоторые из них по какой-то причине
не хотят находиться друг с другом в одном автобусе.
И вопрос, как их рассадить на дварных автобусах,
чтобы как можно больше этих нежелательных связей было разорвано.
Вот эта задача MaxCut.
Значит, терна задачи поиска это следующее.
Значит, по графу G, который есть набор вершины-рёбер,
найти представление...
Найти представление V, которое есть S, не связанное с T.
Значит, такое, что количество рёбер у V из E, таких, что у в S,
а В в T, это чтобы было максимально.
Значит, если мы ставим вопрос о том, можно ли найти такой разрез,
чтобы это количество было больше липрона, чем K,
то это будет NP-полная задача.
Вот, значит, это NP-полная задача.
Однако её можно апроксимировать вероятностно.
Значит, вероятностная апроксимация.
Просто каждая вершина равновероятно Vs и Vt.
Значит, каждая вершина равновероятно Vs и Vt.
Тогда у нас каждое ребро разрежется с вероятностью 1-2.
Каждое ребро разрежется с вероятностью 1-2.
Таким образом, средний размер разреза – это одна вторая от всех рёбер,
что больше либо равно 1-2 от максимума.
Средний размер разреза будет больше либо равен 1-2 от общего числа рёбер,
что, конечно, будет больше, чем 1-2 от максимального.
Ну а это, получается, решение в среднем.
А вопрос, можно ли с такой же точностью апроксимировать детерминированно?
Я не знаю, может, кто-нибудь из вас уже в уме сообразил, как.
Я это буду рассказывать для демонстрации метода.
Вопрос, как апроксимировать детерминированно с той же точностью.
Смотрите, идея такая, что пусть у нас есть случайная величина,
значит, кси ит – это будет, скажем, единица, если v иt лежит в s,
значит, и ноль, если v иt лежит в t.
И мы будем определять, да, ну и скажем, что пусть там cat,
значит, от кси1 и так далее ксиенная, значит, это размер разреза.
Что мы выясним? Мы выясним, что мат ожидания этого размера – это 1-2.
Ну а дальше смотрите, значит, мат ожидания
разреза, размера разреза, это с одной стороны равняется 1-2 просто,
а с другой стороны – это полусумма, ну давайте я вот так напишу подробно,
значит, это почему условные мат ожидания.
Значит, cat от кси1 и так далее кси n при условии, что кси1 равно 0,
значит, и плюс мат ожидания cat от кси1 и так далее кси n при условии,
что кси1 равно единица, и пополам.
Так, эта запись понятна?
Ну как бы мы разбили границу пространства на две равномерно половины,
в середине сначала по каждой из них, а потом взяли середине от всего.
Но что это означает? Это означает, что одна из этих штук будет больше
либо равно 1-2.
Следовательно, одна из этих оценок будет больше либо равно 1-2.
Ну и вот так мы кси1 зафиксируем, и дальше будем продолжать по индукции.
То есть каждый раз у нас какое-то количество кси и так зафиксировано,
и мы среднее выбираем так, чтобы условного мат ожидания не уменьшилось.
Значит, зафиксируем кси1 и продолжим аналогично.
То есть каждая кси и плюс первая фиксируется так, чтобы условный мат ожидания не уменьшилось.
И это очень правильный вопрос.
Действительно, чтобы это работало, нужно уметь вычислять.
И это, собственно, ограничение метода.
Это работает, если мы умеем вычислять все эти условно-мат ожидания.
Это работает, если можно вычислить условно-мат ожидания.
Но в данном случае это действительно можно.
Значит, как это вычислять?
Между какими фиксированными?
Ну да, совершенно верно.
Да, пересчитывается быстро.
Так, ну что, можно не записывать?
А теперь скажите, если это теперь все раскрутить, то что это за алгоритм получится?
Да, это получается жадный алгоритм, когда мы просто каждую новую вершину отправляем так, чтобы разрезать хотя бы половину ребер, идущих от нее, в уже разминченные.
Ну и там как бы очевидно, что одну вторую мы так разрежем.
Это не самый лучший алгоритм из известных, там есть более хитрый.
Там какое-то рациональное число, которое примерно, что-то типа 0.878 или что-то такое.
Можно более умно действовать и большую долю гарантированно разрезать.
То есть это будет доля от всех ребер исследовать на приближении к максимуму.
Но еще сильнее приближать не умеют, и для какого-то уровня приближения уже доказано, что это НП трудное.
Но есть другой пример, в котором так просто уже не обойдешься.
Простой же алгоритм дает худшую оценку.
Это пример МАКС-3-САД.
МАКС-3-САД это дана 3 KNF.
Дана 3 KNF, причем по-честному 3 KNF, то есть там в каждой скобке ровно 3 литерала с разными переменными.
В каждой скобке 3 литерала с разными переменными.
И нужно максимизировать число истинных скобок.
Нужно максимизировать число истинных скобок.
Смотрите, совсем наивный же один алгоритм дает приближение тоже 1-2.
Мы рассматриваем какую-нибудь переменную P1 и рассматриваем все скобки, которые ее содержат.
Смотрим, что больше с P1 или с отрицанием P1, берем соответствующие значения.
И вот эти скобки с P1 вообще больше не рассматриваем, мы половину из них выполнили.
Дальше если немножко подумать, то ясно, что это совсем не очевидно, что те, которые не выполнены, зачем выкидывать.
Можно их попробовать еще выполнить на следующем.
Чтобы одна вторая, мы прям выкидываем.
Давайте не будем выкидывать, а просто эту переменную только вычеркиваем из скобки, а само ее оставляем.
И если есть следующие переменные, то мы их тоже рассматриваем.
Так что получится?
Точно это не хуже.
Не, не хуже-то это да, но лучше на самом деле получится.
Получится, просто гарантия будет выше, так мало ли, может там всюду есть P1, и она сразу на первом этаге, на первом шаге все выполнены станут.
Так строго лучше не получится.
Вообщем гарантия будет вместо 1 и 2 будет 3 четверти.
Потому что смотрите, чтобы у нас скобка стала ложной, нужно, чтобы все три литерала стали ложными.
И когда у нас один литерал становится ложным, то какая-то другая скобка нова становится истинной.
И так на каждую ложную у нас приходится хотя бы 3 истинных.
Но это значит, что истинные хотя бы 3 четверти.
Так, да?
Во, да, совершенно верно.
Если вот этим методом делать, то выяснилось, что можно еще лучше.
Потому что если взять случайные значения всех переменных, то сколько у нас скобок-то выполнится в среднем?
7 восьмых.
В среднем 7 восьмых, что больше, чем 3 четверти, которые у нас получились.
Действительно, чтобы получить 7 восьмых детерминированно, нужно увеличивать веса.
То есть нужно не только оставлять скобки, но еще увеличивать их вес.
Сейчас, это в среднем для конкретного трясата?
Для любого конкретного трясата, да.
Для любого конкретного трясата, но только вот с этим условием, что там в каждой скобке ровно 3 разных литерала.
И от разных переменных.
Тогда в среднем будет точно 7 восьмых.
Мне не нравится, что там же события зависимые у нас.
Так мы же суммируем, не перемножаем.
Мотождание суммы равно сумму от ожидания даже для зависимых.
То есть у нас у каждой одной скобки 7 восьмых, и мы эти 7 восьмых складываем.
Они, конечно, зависим, но это нам не важно.
Хорошо.
В общем, соответственно, такие наивные подходы нам 7 восьмых не дали.
А что получилось 7 восьмых, нужно именно вот этим условным от ожидания считать.
И тут получается, что если у нас какая-то дизъюнкт, то тут получается вероятность истинного.
Это будет 1, если есть истина.
Соответственно, 7 восьмых, если 3 неопределенных.
Дальше 3 четверти, если 2 неопределенных, одна ложная.
Одна вторая, если одна неопределенная, и две ложных.
И, соответственно, ноль, если три ложных.
И отсюда получается, что у такого должен быть вес в два раза больше, а у такого в четыре раза больше.
Почему? Потому что это 7 восьмых, оно перейдет либо в один, либо в три четвертых.
Потом добавится или вычис ts одна восьмая, а три четвертых придет в один или в одну вторую, то есть тут добавится или вычисцerte одна четвертая.
Одна вторая перейдет в единицу или в ноль, то есть добавится или вычисто reflectingventa одна вторая.
Ну вот поэтому соответственно, эта фанада в 4 раза больше чем в одно восьмое.
поэтому вес 4, а здесь вес 2. Дотабинированный алгоритм получается как наш второй,
только когда мы сравниваем, кого больше п или отрицание п, то мы берем скобки,
в которых еще 3 с весом 1, скобки, в которых 2 с весом 2, скобки, в которых 1 с весом 4.
Вот здесь уже теорема, что лучше нельзя, если 7 восьмых плюс епсилон и епсилон константы,
то это уже NP трудное. Но это сложная теорема. Это, кстати, в следующем году на спецкурсе
я буду рассказывать, почему 7 восьмых плюс епсилон уже NP трудное.
Ну что, у нас еще 10 минут, можно еще один сюжет рассмотреть. Давайте на том же самом
примере про MaxCAD другой подход, который, кстати, этот же подход позволяет еще на один вопрос
ответить, связанный с драндомизацией. Ну это как бы такая драндомизация по максимуму,
когда мы хотим вообще от вероятности избавиться. А может быть, ну как бы такая более эффективная,
чтобы алгоритм был все еще вероятностный, но использовал меньше случайных битов.
Вот, и можно поставить вопрос. Ну, например...
Сейчас, чего-чего, в чем вопрос?
А, ну это правда, да. Не-не-не, сейчас, подождите, мы хотим 7 восьмых плюс епсилон не от числа скобок,
а от максимально возможного числа одновременно истинных. Да, это вы, конечно, правы, что больше
7 восьмих числа скобок может не получиться, но мы хотим 7 восьмых плюс епсилон от максимального
количества одновременно истинных. Так, в общем, смотрите, вот если вспомнить, что у нас было в самом
начале лекции, то мы уменьшали ошибку и делали это за счет повторения того же самого алгоритма
каждый раз со свежими случайными битами. И у нас получалось, что ошибка становилась экспоненциально
маленькой, но при этом число случайных битов умножалось на число запусков, то есть как раз
на то, что у нас было в этом самом, в показателе экспонентов знаменателя. Ну и вопрос,
можно ли сделать как-то лучше? Вот, оказывается, можно, и один из способов это использовать
попарную независимость. И вот дарандомизацию МАКСК тоже можно сделать через попарную независимость.
Значит, дарандомизация через попарную независимость.
Ну, значит, смотрите, вот в этом рассуждении, в рассуждении про то, что случайный разрез имеет
размер 1 на 2, мы используем только попарную независимость вот этих ксиитах. Нам достаточно
нам достаточно попарной независимости ксиитах для утверждения о том, что мат ожидания
того, что размер разреза будет равен 1 и 2. Ну понятно почему, потому что у ребра только два конца,
и, соответственно, если для концов ребра ксиита и ксижита и независимая, то они совпадут
всё равно всегда на 1 и и2, и нет всё равно еще на 1 и 2. Вот, а дальше есть, смотрите, есть такой
способ за счет маленького числа истинно-случайных величин получить большое число попарной
из маленького числа не зависимых случайных величин, то есть совокупностей независимых,
получить большое число попарно независимых. Значит, а именно, смотрите, а вот такой подход,
Пусть, скажем, у1, у2 и так далее, уLt это у нас независимые в совокупности случайная величина.
Тогда мы рассмотрим вот такой набор. Во-первых, их всех оставим. Дальше возьмем, да, они из 0,1.
Значит, рассмотрим их всех, независимые в совокупности и равномерно распределенные.
Так, значит, возьмем их всех, дальше возьмем все попарные суммы по модулю 2, значит, у1 к
4, у2, у1 к 4, у3 и так далее, у1 к 4, уL, у2 к 4, у3 и так далее, вплоть до уL-1 к 4, уL.
Значит, дальше все потроечные суммы у1 к 4, у2 к 4, у3 и так далее, и вплоть до к 4 их всех.
Значит, это будут попарно независимые и тоже равномерно распределенные случайная величина.
Так, ну почему равномерно распределенные? Понятно, да, как бы мы их ссорим, они были независимы,
значит, будут равномерные. Ну а попарно независимые, вот почему, значит, ясно, что уже по тройкам они не будут независимы,
у1, у2 и у1 к 4, у2 вполне себе зависимы. Но пока мы берем только две, то, смотрите, есть какая-то одна сумма,
есть какая-то другая сумма. И тогда есть хотя бы одна переменная такая, что она в одной сумме есть,
а тут, соответственно, нет у ИТ. И тогда, смотрите, одно из определений независимости,
что условное распределение одной величины условно на вторую совпадает с безусловным.
Тогда получается, что если мы зафиксируем все переменные кроме уИТ, значит, зафиксируем все переменные кроме уИТ,
значит, тогда тут получается фиксированное значение, а тут все еще равномерно-случайное.
Значит, за счет одной уИТ аксор будет случайным, равномерно-случайная.
Но это и означает, условное распределение будет равномерно-случайным, безусловное тоже равномерно-случайным, значит, они попарно независимы.
При условии, что вторая фиксирована. Но тут еще более дробное разбиение, то есть то, что вторая фиксирована,
я разбиваю на то, из чего конкретно она сложилась, и все равно получается, что у ИТ будет равномерно-случайная,
даже на более мелких событиях, условно, тоже будет равномерно-случайная.
Согласны, что они все попарно независимы?
Получается, что у нас получилось, что l независимых совокупностей мы превратили в 2 в степени l минус 1 попарно независимых.
Хорошо. Таким образом, получилось, что это ожидание по так построенным, именно для таких,
кси1 и т.д., и ксиn, которые как раз вот эти, кси1 вот это, ксиn вот это. У нас будет равно 1 и 2.
Дальше еще одна мысль осталась, что вот это мат ожидания можно прямо явно посчитать за понимание время.
Да, значит, это можно не его посчитать, а это будет как бы усреднение, то, что оно усредняет, можно посчитать.
А, ну ладно, я тебя так оставлю, значит, можно посчитать все значения для всех, точнее сейчас, значения для всех наборов у1 и т.д. уlt.
Ну и выбрать из них максимальное.
Да, почему? Потому что всего этих наборов, всего наборов 2 в степени l.
Но 2 в степени l, это у нас примерно то же самое, что и n.
Что получается? Получается, что мы перебираем все наборы вот такой длины, из них вот по этому правилу составляем кси и т.
По кси и т.д. считаем размер разреза и находим самую большую из того, что посчитали.
И поскольку средняя не больше максимума, то этот максимум будет хотя бы 1 вторая.
Не самый простолкрито для этой задачи, но это опять же демонстрация метода.
Ну чего, понятно? Да, перебор по подможеству, при том, что по этому подможеству средняя такое же, как по всему множеству.
И получается, что этот перебор мы можем сделать.
Значит, у нас есть исходный короткий вектор длины l, он булливый, мы перебираем все возможные булливые векторы длины l.
L это логарифмен, грубо говоря.
Потому что для конкретного вектора у мы делаем вектор кси вот по этим вот формулам.
То есть по всем ненулевым маскам мы берем под сумму, и это будет соответствующий кси и т.
И потом вот это кси и т.д. понимаем, в какую часть графа мы поместили эту вершину.
И вот у нас получается два в степени разреза, в которых примерно m.
И из этих разрезов средняя такая же, как средняя по всем, то есть в среднем половина.
И мы вот по этому подможеству можем выбрать максимальный.
Ну типа того, да.
А зато она какой-то другой там гарантированно больше резит, а в среднем это все сократится.
Ну вот, да, действительно такая вот идея сокращения перебора.
Так, ну вот.
Так, ну идем все. Спасибо.

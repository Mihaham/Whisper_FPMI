Как ваши продвижения по сегодняшней вероятности?
Ну, я понимаю, да.
Но я имею в виду, что в прошлый раз я чё-то начал рассказывать,
и вы мне сказали, что почти ничего не знаете.
Но, может быть, за неделю, которая прошла с тех пор,
вы много чего нового узнали?
Нет?
А, это не относится к тому, что я рассказывал.
Ладно.
Меня это не напрягает.
Буду рассказывать сам.
Хорошо, а то, что я рассказывал в прошлый раз,
вы как-то осознали, помните?
Про графы?
Нет, там были мат ожидания каких-то случайных величин.
Инейность.
Чё, не было, что ли?
Как не было? Ну, я же в прошлый раз рассказывал.
А, не было на теории вероятности. Там не было, да.
А у меня-то было.
Круто.
Понятно, понятно.
Не, но, друзья, мой темп изложения понятен, нормален.
Вы за мной успеваете?
Или что-то надо напомнить?
Так, с точки зрения дискретного анализа вы помните,
чего я хотел доказать?
Тоже не помните.
Ладно, смотрите, это мне нарисовали мёртвый угол.
Здесь я писать не буду.
А здесь я напишу тому,
которому я очень подробно в прошлый раз комментировал,
формуировал так-сяк.
Ну, как вам сказать-то?
Так-сяк прямо её со всех сторон рассматривал
из практической точки зрения.
Содержательный счёт какой-то.
Давайте я напомню.
Пусть...
П это вероятность ребра случайного графа,
и она зависит от того, сколько вершин у этого графа.
Причём вот таким вот
кажется пока ещё загадочным полосом.
Это константа, большая нуля,
тогда имеет место,
что бы я назвал, квазовым переходом.
Если c больше 1,
то асинтетически почти, наверное,
случайный граф связан.
А если c
меньше 1,
то асинтетически почти, наверное,
g, n, b не является
связан.
Ну, я так говорил, что будет в лицеравном единице,
говорил какую-то убойную совершенно теорему про
е в степени минус, е в степени минус гамма.
Понимаете, был такой просто
абсолютный восторг, по-моему.
Мне кажется, это абсолютный восторг.
Но я чётко говорил в прошлый раз,
что доказывать буду только это.
Я всё ещё не готов к вам это доказывать,
потому что, если я правильно помню,
чем я завершил прошлую лекцию,
после которой я успел прочитать ещё 8 штук
других лекций.
Поэтому голос по-прежнему не совсем нормальный,
но уже так полерьми.
Значит, что я хочу сказать?
Я помню, что я закончил линейностью мат ожидания,
и мы посчитали среднее число треугольников
в случайном графе.
И я вам задал домашнюю задачу –
посчитать вероятность того,
что треугольников в случайном графе нет.
Но, естественно, вы не пытались её решать.
Я прав, товарищ?
Вы даже забыли, что она была.
Ты красный.
Ну ладно, мы к ней тоже вернёмся.
Давайте я напомню, что я обозначал
вот таким образом мат ожидания
случайной величины х,
а ещё я вам введу вот такой вот объект,
называется дисперсия случайной величины х.
Вообще мне кажется, что в этом году
я читаю чуть более подробно и аккуратно,
чем обычно.
Может быть, с этим связано, как бы это сказать,
большее количество народа в аудитории
на третьей лекции.
Четвёртая, ну вообще круто.
Так, давайте я напомню, что такое дисперсия.
Напомню, скажу, с нуля.
Это математическое ожидание
вот такого вот выражения.
Мы берём случайную величину,
вот эту х,
вычитаем из неё
её среднее значение, мат ожидания.
Всё это дело
возводим в квадрат
и потом снова у средня.
Можно вот так ещё скобки нарисовать,
к чему применяется оператор Е.
Мат ожидания.
Так, что-нибудь понятно?
Ну, это такое вот, если словами говорить,
среднее квадратичное уклонение
случайной величины от своего среднего значения.
То есть, вот она
принимает какие-то значения,
вот эта случайная величина х,
мат ожидания,
а вот этот вот разброс,
он меряется дисперсией.
Друзья, нужно подробно или понятно?
Можно ещё скобки раскрыть.
Так тоже делаю.
Получается полезная, удобная другая формула.
Ну как? Скобки раскрываются в квадрат,
умеем возводить.
2х Ех
плюс Ех
в квадрате.
Я просто тупо возбил в квадрат разность.
А теперь я применяю Е
вот к этому поражению.
То есть, могу воспользоваться линейностью.
Раскрыть скобки, занося Е внутрь.
Линейностью пользуюсь.
Друзья,
если вдруг всё понятно,
но вы тем не менее не успеваете записывать,
вы меня притормаживайте, пожалуйста.
Я хочу, чтобы вы всё успевали понимать
и фиксировать для себя то, что вы поняли.
Что это значит?
Это значит, что я беру квадрат из значений
случайной величины,
кроме квадрата до усреднения.
Очень интересно, что произойдёт,
когда я применю мат ожидания к этому поражению.
Смотрите, товарищи,
2 это, очевидно, константа.
Правда же, она вынесет за скобки при усреднении?
А Ех тоже константа.
Она тоже вынесет за скобки, правда?
Поэтому получится вот так.
Минус 2Ex.
Это я вынес двойку и мат ожидания,
который является константой, за скобку.
А в скобках осталось только мат ожидания х.
То есть я снова это умножаю на мат ожидания х.
Это тоже константа.
Мат ожидания константа, это она же сама.
То есть мы прибавляем
Ех в квадрате,
потому что усредняя константу мы получаем её же саму.
Так, дорогие друзья,
но вот это тоже Ех в квадрате,
для вашего позволения.
Он со знаком минус из двойкой,
а этот со знаком плюс.
Поэтому получается вот такая штука.
Такая вот удобная альтернативная формула
для подсчёта дисперсии.
Вот она.
Вот это вот называется второй момент.
Слушайте, друзья,
я, конечно, буду издеваться,
но вдруг среди вас есть люди,
которые любят писем.
Смеются, значит, не любят, похоже.
Ну хорошо.
А может быть, среди вас есть люди,
которые знают физику.
Да, ну нет, это уже истерика начала.
Выполн в осадок.
Значит, если вдруг всё-таки кто-то не смеётся,
я вот скажу, может, туда скажу.
Вот физики, физики,
есть такое понятие, называется момент инерции.
Никто не знает, да?
Знаете, да?
Вот практически то же самое.
Дисперсия в математике это момент инерции физики.
Ну так вот, там стойте сюда, нормировки.
Ну ладно, это так просто.
Так, друзья, ну я думаю, что я не буду
больше комментировать, что такое дисперсия.
А единственное, что можно сказать,
такой вопрос, а зачем здесь возводить в квадрат?
Иначе ноль получится.
Ты квадрат убрать,
то по линейности, очевидно, получится ноль.
Другое дело, вы скажете, ну хорошо, можно
рисовать модуль, да?
Ну вот если рисовать модуль, то считать неудобно.
Просто считать неудобно, и те неравенства,
которые я сейчас приведу,
надо помогать доказывать вот такие теоремы,
они просто будут очень неудобно записываться.
Так.
Вот, я хочу привести
два классических неравенства
в теории вероятностей. Называются они
неравенство Маркова и неравенство Чебышова.
Слышали такие слова сочетания, нет?
Ну кто-то слышал, кто-то нет,
а сейчас я все подробно, спокойно расскажу.
Так, неравенство
Маркова.
Друзья,
чтобы вы точно понимали, что происходит,
я еще раз повторю. В курсе
основ теории меры и теории вероятностей,
вам, конечно, это все расскажут.
Потом, в большом курсе теории вероятностей,
вам еще раз это расскажут.
Это очень важные вещи, их надо прям вот как
центральные фарты теории вероятностей воспринимать,
потому что они работают, они нужны,
вот они используются здесь, они в массе других
ситуаций используются. Это надо знать.
Я это доказываю, просто чтобы вам все было понятно.
Я доказываю в частном случае,
когда пространство элементарного события
у нас граф в конечное число.
Итак,
пусть случайная величина Х
на своем пространстве Омега, ну, например,
на множестве графов, которые имеют там вершины,
ну, или на каком-то конечном пространстве
принимает вот такое вот какое-то конечное множество значений.
Давайте еще
дополнительно
предположим, что каждая из этих значений
не отрицательна.
Случайная величина принимает только не отрицательные значения.
Ну, например,
число треугольников в случайном графе,
это вот прекрасная величина, которая принимает
только не отрицательные значения, правда?
Сто треугольников.
Ну, есть какая-то случайная величина,
все ее значения не отрицательные.
Для любого
А больше нуля,
А это константа, фиксированное число,
любое действительное число положительное,
вероятность того,
что Х
больше либо равняется А,
и вверху
в математическом опыдании,
мы в зеленом на А.
Что, что говорите?
А оно будет следовать отсюда моментально.
Значит, и Чебырков,
значит, и Чебырков, это Марков, да?
Оно не так выглядело, кстати?
Нет, оно выглядело с моего роста.
А, оно называлось Чебырковым, да?
Ну, это некая культура, знаете, можно это назвать Чебырковым,
можно то, что я обычно называю.
Но как-то вот люди, которые занимаются именно
специфической теорией вероятности, они почему-то
разделяют, говорят, что вот это Марков,
называть можете, как хотите, в принципе, и то, и другое Чебырков.
Естественно, Чебырков, это знал и следствие знал.
Просто так называю.
Так, чего, друзья, знаете, как это доказывать, да?
Ладно, сейчас докажем.
В одну строчку.
Все?
Нет, ну давайте, мат ожидания Х.
Я справа-налево буду смотреть на это неравенство.
Давайте посмотрим на мат ожидания Х, это что-то.
По определению.
Это сумма.
Вот И от единицы до Н.
Игрехы значение, да,
умножить на вероятность того,
что Х равняется Y.
Это второй вариант определения, который мы обсуждали в прошлый раз.
Просто определение мат ожидания.
Так.
Теперь я просто распеваю эту сумму на две части.
Я вообще, наверное, вас замучил
с распиениями сумм на две части.
Но, слава богу, это не тот случай, как при подсчете
в унициплических графах.
Помните, там была эта степень 0,6, какая-то жуткая.
Тут все гораздо проще.
Давайте отдельно просуммируем по таким И вот от единицы до Н,
для которых Yt больше ли равняется А.
Мы осуществляем все то же самое.
Yt умножит на вероятность того,
что Х равняется Y.
И отдельно все остальные.
Те, для которых Yt, соответственно, меньше,
строго, чем. Так, плюс.
Сумма по всем.
И таким, что Yt меньше, чем А.
5Yt на вероятность того,
что Х равняется Y.
Так, друзья, я думаю,
что это не составляет таких вопросов.
Очевидно, да?
Так, смотрите, что еще отчего.
Издевательская оценка. У нас все Yt не отрицательные.
И вероятность это тоже не отрицательное число.
Или вы считаете, что вероятность может быть отрицательной?
А комплексной может быть?
Ладно.
Короче, я вот эту всю бяку
оценю с и до нулем. Всю.
Давай.
Тогда сумма двух сумм оценится
только первым слагаемым, первой суммой.
Давайте ее оцениваем.
Сразу.
Смотрите, каждая Yt в этой сумме
не меньше, чем А.
Вынесем А за скобку.
А в скобках останется сумма по тем И,
для которых Yt не меньше, чем А.
Вероятность того,
что Х равняется Y.
Поэтому...
Так, этот перекод понять?
Слушайте, ну вот,
мы суммируем вероятности событий,
которые очевидно не пересекаются, да?
И суммируем по всем тиглях, которых Yt,
то есть значение Ха,
больше либо равняется А.
Но вроде вся эта сумма и есть вот эта вероятность.
Когда мы получаем А,
мы умножим на вероятность того,
что Х больше либо равняется А.
Просто вот это событие разбили на кусочки
и сложили вероятность.
Друзья, если я вдруг подержу Y,
я могу разогнаться.
Не, нормально?
Или еще медленнее?
Так, ну вроде вы должны были понять.
Так что теперь справа налево,
вот эта вероятность не больше,
чем мат ожидания поделить на А.
Вот получили неравенство Марков.
Так, давайте неравенство Чебыршова.
Вот то, что я называю,
неравенство Чебыршова.
А, вас еще наверное учили Чебыршов говорить, да?
Или Чебыршов?
А, не учили?
Не, есть две научных школы.
Одна считает, что надо говорить Чебыршов,
а другая, что Чебыршов.
Вот нет ни одной школы,
которая считала, что надо говорить Марков.
Все говорят Марков.
Но ее в русском языке не пишут.
Вернее, знаете как?
Когда надо писать, не пишут.
А когда не надо, пишут.
Вот помните, кто доказал это теорему,
которую мы никак не докажем?
Не помните?
Я специально рисую ударение,
просто подчеркиваю,
что надо ударять на «и», не на «и».
А Эрдов?
Ничего не могу сделать.
Во всех моих прошуток, которые изданы в МЦНМУ,
хоть тресть, они пишут так.
Естественно, Эрдов.
Если ее в русском языке, она всегда ударная.
А он вот так пишется.
Мне отделили мертвый угол,
я пишу вне мертвого угла.
Мертвый угол вырос.
Каждый год расширяется.
В общем, в оригинале он пишется вот так.
Но в кангерском языке,
«с» читается как «ш».
Вот эти две палки, это даже не две точки.
Ну, ударение там все-таки на «е».
А это такое «ы».
Вот оно читается как «Эрдовж».
В конце концов, я их поломал.
Я ей говорю, ну вы не ставьте это ее,
я ей говорю, ну вы не ставьте это ее,
нет, нельзя, надо ставить «е».
Но они писали вот так.
Эрдовж, ну Эрдовж, это нормально,
просто пойди догадайся.
Ладно, сейчас я вот здесь верну фокулу,
которую мы получили, она может пригодиться.
Выкладку я убрал, а саму формулу сохранил.
Так, неравенство Маркова,
это неравенство Чебыфова.
Пусть
х, любая
случайная личина, вообще любая.
Ну, я имею в виду любая,
в том смысле, что здесь она обязана была
не отрицательные значения, а там вот какая вот,
какая вот.
Ну, вас в курсе теории вероятности
будут грузить какими-то условиями сходимости, мат ожидания,
дисперсии, но у меня напоминает всегда
конечное множество значений,
поэтому там мат ожидания всегда есть, дисперсия всегда есть,
меня эти вопросы не беспокоят.
Пусть их любая случайная величина,
тогда
для любого А больше нуля
вероятность,
которая выполнена
вот такое вот неравенство, то есть х уклоняется
от своего среднего не меньше, чем назаданную величину,
достаточно сильно как бы уклоняется от центра мира,
вот эта вероятность не большая,
чем дисперсия х
поделенная на а в квадрате.
Я утверждаю, что это очевидное следствие
неравенства марков.
Давайте рассмотрим такую случайную величину,
пусть у, например, это будет х
минус ех
квадрате.
Согласитесь,
эта случайная величина
принимает не отрицательные значения,
потому что является полным квадратом.
То есть к ней можно применить
неравенство маркова.
Давайте так применим.
Игре больше не бы равняется а в квадрате,
а вот это, которое в чебышу.
Ну, это же в точности тоже самое, правда?
Как корень извлекается в неравенстве слева,
появляется модуль,
а справа то самое а, которое было положено.
Нормально?
А чего это не превосходит,
согласно неравенству маркова?
Это не превосходит ей
поделить на а в квадрате,
но что такое ей?
Это в точности дисперсии.
Все.
Надеюсь, это не было сложным прогоном.
Очевидное следствие из неравенства маркова
это вот такой формат неравенства чебышова.
Но можно, конечно, говорить первое неравенство
чебышова, второе неравенство чебышова.
Послушайте меня сейчас внимательно,
это очень важный момент.
Хотя неравенство чебышова
является следствием из неравенства маркова,
вот здесь в таких теоремах,
которые возникают в теории графа,
они друг друга дополняют.
А именно первый пункт мы будем доказывать
с помощью неравенства маркова,
с помощью неравенства чебышова.
То есть в комбинаторике,
ну я не знаю как, можете себе это пометить,
можете просто послушать, вы увидите это потом.
В комбинаторике неравенство маркова отвечает за то,
что чего-то почти наверняка нет.
А неравенство чебышова,
что нечто почти наверняка есть.
Но можно это на слуху воспринять,
объясню это уже на конкретных примерах.
Так, живые все.
Давайте я, наверное, тогда
вас не томить,
под эту теорему начну доказать.
У нас все инструменты готовы.
Так.
Ого.
Ну готовы они, конечно, готовы,
только что-то они много места занимают.
Знаете, почему?
Технически чуть более просто доказать пункт номер два.
Поэтому я с него начну.
Вы будете смеяться или плакать,
но сложность техники пункта номер один
будет состоять в том,
что там будет сумма,
и ее придется виноват разбивать на две части.
Опять?
Нет, хуже!
Еще хуже.
Слушайте, давайте вот как сделаем.
Можно я даже сначала
попробую неформально объяснить,
откуда пошла вот эта функция.
А потом уже буду аккуратно все доказывать.
Мне кажется, так будет лучше.
Вот сейчас можете ничего не записывать,
просто послушать,
потом я все равно аккуратно все напишу.
Давайте обозначим через х.
Хотите запишите, как вам удобно.
Это пригодится.
Случайную величину на множестве графов,
которая принимает значение,
равное количеству
изолированных вершин
в этом графе же.
Ну, изолированная вершина,
иными словами, это вершина степени ноль.
То есть просто отдельно стоящая вершина,
которая не примыкает, не является вершиной
никакого ребра.
Вот бывают такие вершины в граф, да?
Ну, бывает.
Пришла на лекцию к Андрею Михайловичу
куча народов,
и есть в этой куче один человек,
изолированная вершина графа знакомства.
Ну и так далее.
Наверное, мне надо еще чуть-чуть назад
отмотать, знаете, зачем я вел такую величину?
Чисто интуитивно.
Смотрите, у нас вопрос о том, связи в графе или нет.
Вот давайте подумаем.
Друзья, давайте подумаем вместе прямо подумаем.
Если связи разрушаются независимо
с одной и той же вероятностью,
скорее всего граф разломится пополам
или от него отскочит какая-нибудь компонента
не знаю, логарифмического размера по числу вершин
или от него просто отвалится изолированная вершина одна.
Что более вероятно?
Что легче сделать? Разломать граф на две компоненты
размером пополам или отчепить от него изолированную вершину?
Очевидно что второе?
Или не очевидно?
Сейчас, друзья, вот смотрите, вот был граф,
у него N вершины.
Так, сколько здесь 6?
Расломать его вот на два таких куска изолированных,
это значит у какого-то 9 ребят.
А оторвать одну вершину,
это у какого-то всего 5 ребят.
И в общем случае, если у вас тут N пополам
то купить все связи между ними
это надо вклокнуть
N в квадрате на 4 ребра.
Я понятно говорил или нет?
А отвалилась одна вершинка,
это только N минус одно ребро.
Пока давлю только на интуицию.
Только на интуицию, больше ничего.
Поэтому раз легче всего отчипывать изолированную вершину,
вот давайте рассмотрим количество изолированных вершин.
Вопрос на то, как вы поняли
инейность математического ожидания.
Ну скажите мне, чему равно Eх?
Кто может сходу?
Сумма от ожиданий, то вот бы каждый конкретный вершин.
Ну ответ.
Правильная сумма.
Давайте, раз это вызывает уже такие вопросы.
Правильно-правильно, это важно написать.
Давайте напишем вот так.
x1, плюс и так далее, плюс xn,
dx и da, g.
Это как в прошлый раз индикатор.
То есть единица,
и изолированная вершина.
И ноль иначе.
N множество 1 минус 1.
Почему? Ещё раз.
Потому что мы складываем N одинаково к слагаемому.
А мат ожидания одного, любого из них,
это вероятность того, что E изолировалось.
То есть пропали N минус 1.
Надо писать подробнее.
Ну вот вершинка E.
Вот остальные.
Изначально было N минус 1 ребро.
Мат ожидания индикатора
это 1 умножить на вероятность.
То есть это вероятность вот этого события.
Ну а какая вероятность,
что пропали все вот эти N минус 1 ребро?
Вот ровно такая.
Что, гоним, что ли?
Так, друзья, поднимите, пожалуйста,
чтобы кто сейчас понимает, что происходит.
Сейчас будет
махонький такой аналитический катарсис.
Смотрите.
N я оставлю.
А тут напишу в любимом стиле.
Что такое 1 минус P?
Это E в степени логарифм от 1 минус P.
Так, N минус 1 я нанесу.
А тут будет логарифм от 1 минус P.
Ну, мы уже знаем с вами,
что P стрелится к дулю.
Но в общем это можно и так сообразить,
что, наверное, оно будет стремиться к нулю.
То есть если бы мы не знали ответ,
все равно пусть оно стремится к нулю.
Тогда что у нас тут получается?
N на E в степени вот так.
1 плюс 1 минус.
И тут N.
Я написал просто, что логарифм от 1 минус P
асимпатически равен минус P.
Даже не пишу никакого ряда Тейлора,
только вот первое из логарифм.
Логарифм это минус P с точностью до
стремящегося единицы со множителя.
Ну, и N минус 1 точно так же
асимпатически поменял на это.
Все вас надели?
Сейчас будет катарсис, смотрите.
Вместо P то, что написано там в теории.
N на E в степени минус.
1 плюс 1, 5.
N я могу умножить на P сразу.
Так, понимаете, что будет целый логарифм?
Нормально, да?
Вот этим N.
Так, E в степени логарифм N это что?
N, правильно.
И тут N. Давайте напишем вот так.
N в первой степени.
Минус 1 плюс малая от единицы.
Вот это.
Умножить на C.
Ха-ха.
Понимаете, если C больше единицы,
то это стремится к нулю.
А если C меньше единицы, то это стремится к бесконечности.
Формально говоря, из этого пока, к сожалению,
ничего не следует.
Но интуиция, понятно, да?
Если C больше единицы, то как минимум
мало изолированных вершин.
Если C больше единицы, то, наверное, их будет много.
Правда, почему? Это еще тоже вопрос.
Ну, все-таки к бесконечности стремится мата ожидания.
Сейчас, друзья, вы поняли, да, какое здесь переключение?
Все, вот теперь я формулизую.
Знаете, прежде чем я формулизую, еще...
Проблема-то в чем?
Вот вы еще мало имели дело с мата ожиданиями,
с случайными величинами и так далее.
Послушайте, послушайте, это больно.
Вот, например, мата ожидания стремится к бесконечности.
Следует ли из этого,
что почти наверняка случайная величина
принимает большое значение?
Нет, конечно.
Потому что представьте себе, что половина значений
случайной величины ноль,
а половина все больше и больше.
Мата ожидания будет стремиться к бесконечности,
а доля значений, которые большие,
будет все время одна втора.
Понятно сказал, нет?
То есть просто из стремления к бесконечности
мата ожидания, конечно, ничего не следует.
Еще раз повторяю, это только интуиция,
откуда взялся базовый переход.
Был катарсис, а теперь начнется уука.
Так, уука.
Мне еще не нравится, что Чебышова панит.
История.
Где же мне писать?
Так хочу в мертвый угол пойти, знаете,
прям подмывать.
Походишь мне.
Ну ладно.
Сюда пойду.
Строго доказываем пункт 2 теории.
Сейчас будем строго доказывать пункт 2.
Кажется, меня прервет звонок,
но давайте я еще раз напишу,
с которыми мы разобрались.
Мы уже знаем, что мата ожидания х
это n на 1 минус 1,
и мы знаем, что в рамках 2 пункта
это стремится к плюс бесконечности,
потому что c меньше единицы.
Мы находимся во 2 пункте теории.
Так, теперь смотрите.
Мы что хотим доказать?
Что граф не связан
с вероятностью стремящейся к единице.
Все ж помнят, да, pn это значит
вероятность стремиться к единице.
Граф не связан.
Но если в графе будет хоть одна изолированная вершина,
из этого же будет следовать, что он не связан,
но это та самая интуиция, с которой мы стартовали.
Давайте напишем вероятность того,
что это единица.
Мы хотим доказать,
что эта вероятность стремится к 1.
Если мы это докажем, пункт 2 у нас в кармане.
Согласны?
Вспели за мыслью?
Так, конечно, первое,
что хочется сделать, это применить
правительство Маркова, но, к сожалению,
так.
А нам-то нужно доказывать стремление к единице.
Нам нужно неравенство в другую сторону, понимаете?
Перерыв 5 минут.
Так, возвращаемся в строй.
Нам нужно это оценить снизу.
Сейчас сделаем фокус, который подгонит нас
под неравенство Чебышова.
Ну, конечно, я могу сразу написать ответ,
но занудствовать.
Я пишу вот так.
Это равно...
Так, товарищи, понятно,
почему верен такой переход?
Это называется вероятность отрицания равна единице
минус вероятность события.
Что должно быть?
Меньше строго, чем один,
но Х принимает только целые значения.
Ну да, товарищи,
я могу здесь равно написать.
Это будет то же самое.
Все понимают, Даша, если я напишу равно, так будет то же самое.
А я не хочу. Мне так нравится.
Но я хочу подогнать подгиб неравенства Чебышова,
поэтому я вот так пишу, хотя вот право
написать точное равенство.
Дальше. Продолжаю издеваться.
Минус Х больше либо равен нуля.
Я умножу лево-справо на минус один.
Один минус вероятность
ех минус х
больше либо равно ех.
Но это я добавил константу равную мат ожидания
лево-справу.
Так, а вот теперь я пишу
неравенство в нужную сторону.
Один минус вероятность того, что модуль
х минус ех
больше либо равняется ех.
Ну, почему это так?
Потому что вот эта вероятность
принимает большое значение,
уж точно не реже, чем это делает
исходная величина.
Не реже. То есть вот эта вычитаемая вероятность,
она скорее всего больше,
чем вот эта вычитаемая.
Ну, поэтому неравенство в нужную сторону.
Вот здесь вычитается скорее всего больше,
чем правое.
Так, неравенство Чебышова
применяется вот сюда.
Оно про то, что это не больше чего-то,
но оно же идет со знаком минус,
поэтому мы продолжаем именно в ту сторону,
в которую нам надо.
Больше либо равно, чем один
плюс дисперсия х
поделить
на квадрат мат ожидания.
Смотрите, мы знаем, что мат ожидания
стремится к бесконечности, это обязательное условие,
но мы не знаем, почему бы вот эта дробь стремилась к нулю.
Вот если она стремится к нулю,
если она стремится к нулю,
то мы, конечно, получим стремление к единице,
которое нам нужно.
Так, а вы помните, что мы хотим, чтобы это стремилось к единице, да, ведь?
Вот нам для этого достаточно и, в общем, необходимо,
чтобы обведенная штука стремилась к нулю.
Очень хорошо, что знаменатель стремится к бесконечности,
но вдруг дисперсия еще быстрее.
Это вот то, о чем я говорил.
Если разброс относительно среднего очень большой,
то стремление к бесконечности, вообще говоря,
еще ничего не означает.
Ну, давайте считать дисперсию.
Так.
Помните?
Я еще не успел это стереть.
Дисперс x равняется второму моменту
минус квадрат математического ожидания.
Ну, квадрат мат ожидания,
по-моему, понятно, чему равен.
Да?
Давайте только второй момент посчитаем,
ну а потом подставим все, конечно.
Так.
Ну, друзья, мы же помним, что x это сумма,
x1 и так далее, xn.
Давайте так и напишем.
x1, так далее, xn,
все это в квадрате.
Ну, давайте так и напишем.
x1, так далее, xn,
все это в квадрате.
Умеете возводить квадрат сумму
многих слагаемых?
Но тут очень простые полинамиальные коэффициенты,
они все равны двойке или единице, да?
Это равно
x1 квадрат,
что так далее, плюс xn в квадрате,
а дальше я вот так напишу.
Сумма по i не равно j,
xt умножить на xjt.
Ну да, как бы удвоенное,
но, друзья, я буду считать,
что и j и j и суть разные вещи,
тогда не надо удвоивать.
То есть тут отдельно присутствует
x1 умножить на x2 и x2 умножить на xjt.
Упорядоченные слагаемые,
а так, да, конечно,
xn2, если приводить подобные.
Так, это я еще раз напишу,
это единица или ноль,
вот если
и изолированная,
а тут если, ну иначе надо было написать,
если i не является изолированной.
Изолированной.
Дорогие товарищи,
что такое xt в квадрате?
Это xt,
1 в квадрате это 1,
0 в квадрате это 0.
Я вот так вот, можно сделать?
Ну какая разница возводить квадрат
и не возводить значение?
Равно.
Линейность применяем.
Слушайте, ну вот эта часть чему равна?
Конечно, вот этому, правильно?
Давайте я так и напишу,
ex.
Мы знаем, чему равняется ex,
но вот я вот эту часть напишу просто как ex.
Так, дальше будет сумма
по i неравном g,
мат ожиданий вот этих произведений.
Теперь давайте просто сообразим,
а произведение, по сути, это что такое?
Оно тоже равно единице или нулю, правда?
Единицы если обе вершины
изолированы и ноль иначе.
Смотрите, вот это вот
равно единице,
если обе вершины и ежи
изолированы,
ну и ноль, если хотя бы одна из них
таковой не является.
То есть чему равно мат ожидания?
Давайте я картинку нарисую, вот тут.
Две вершины и ежи.
И еще есть
н-2 оставшихся.
Так, все понимают, что такое н-2 оставшихся, да?
Вот что значит и ежи изолированы?
Это значит отсутствуют
и н-2 ребра, да?
Такие н-2.
И еще вот это.
Сколько суммарно? Дважды н-2 плюс один.
Два н-3.
Хорошо в уме считаете, молодцы.
То есть чему равно это мат ожидания?
Оно равно 1-p
в степени 2n-3.
Ну а общее значение получается вот такое.
Это мат ожидания х я переписал.
Слагаем их n умножить на n-1,
а это н по 2, потому что они упорядоченные.
И на 1-p
и на 1-p
в 2н-3.
Ну и все, сейчас у нас получится стремление к нулю.
Сейчас я напишу я.
Да, друзья, знаете, чего я забыл сказать?
Все логично, все правильно, никаких ошибок нет.
Я забыл сделать важное замечание.
Не относящееся к доказательству,
относящееся просто к сути процессора.
Вероятность того, что х больше либо равно 1,
не меньше чем 1 минус вот эта дробь.
Вдумайтесь,
мы где-то использовали то, что х
это именно количество изолированных вершин.
Что мы использовали?
Единственное, что мы использовали,
это что х принимает целые значения, больше ничего.
Понимаете?
Вот здесь.
Когда отрицанием того, что х больше либо равняется 1,
оказалось то, что х не больше нуля.
То есть вот такое вот
неравенство,
оно верно для любой случайной величины
принимающей целые значения.
Например, для числа треугольников.
Помните про число треугольников?
Ну вы не решали это, Дом, вы забыли.
Ну число треугольников.
Вот это просто вам по жизни, чтобы вы понимали,
это удобное неравенство, которое можно использовать
для решения разных задач.
Так, возвращаюсь к подсчету.
Давайте теперь я это рассуждение сотру.
Так, пишем дисперсия,
и х поделить на квадрат
математического ожидания.
Равняется ех
плюс n,
n минус 1,
на 1 минус п в 2, n минус 3.
А видите, как я длинно нарисовал?
Знаете почему?
Кто догадывается, почему я такую длинную дробь нарисовал?
Нет,
потому что это не дисперсия,
а это, товарищи, второй момент.
Чтобы получилась дисперсия,
надо еще вот здесь
вычислить знаменатель.
Вот это уже будет действительно дисперсия в числителе.
Ну а знаменатель, знаменатель-то что?
Вот он знаменатель, все.
Он не длинный, он короткий.
Так, смотрите.
Во-первых,
имеем дробь 1 поделить на ех,
но ех стремится к плюсу бесконечно,
поэтому эта дробь стремится к нулю.
Какая удача, уже кое-что стремится к нулю.
Но нам надо, чтобы все стремилось к нулю.
Вот, смотрите, там вот это.
Так, прибавить.
Так, n, n-1,
1-p,
2n-3, это я просто переписал,
и в знаменатель ставлю
квадрат математического ожидания.
Прям явно его пишу.
1, 1-p,
2n-2.
Так, все успевают.
Квадрат возвел.
Явно написал квадрат мат ожидания.
Так, и минус 1.
Это вот ех квадрат поделить на ех квадрат.
Все.
Внимание, товарищи, как ведет себя средняя дробь?
Стремиться к единице.
Я надеюсь, что все видят.
Я еще знаете, как могу написать?
Она равна 1 плюсо малое от единицы.
Ну, стремиться к единице
это то же самое, что равна 1 плюсо малое.
Привыкли, нет?
Нормально?
Такое жонглирование просто мат-аналитическими обозначениями.
Чпок-чпок.
Тут стремиться к нулю, тут стремиться к нулю.
Значит, все стремиться к нулю.
И второй пункт теоремы у нас в короли.
Доказал вроде.
Все.
Дробь стремится к нулю.
Значит, вероятность наличия изолированных вершин стремится к единице.
Ну, значит, граф, скорее всего, не является связанным.
По причине того, что мы отщепили отдельные вершины.
За этим была интуиция.
Понятно.
Так, друзья, есть какие-нибудь вопросы по доказанному?
Отлично.
Я надеюсь, что это означает не то, что я восклокнул,
а наоборот, что я молодец.
Помните, я говорил, что на примере этой теоремы
мы увидели, что неравенство Маркова в комбинаторике
только дополняет неравенство Чебышова.
С помощью неравенства Чебышова мы только что доказали,
что почти надерное что-то есть в графе.
А именно изолированная вершина.
Хватит за мысли?
Ну, да.
Ну, да.
Хватит за мысли?
Давайте теперь, доказывая пункт 1,
применим неравенство Маркова,
чтобы убедиться в том, что в случайном графе
почти надерное нет не только изолированных вершин
и ситуации, когда С больше единиц,
но вообще никаких отдельных компонентов связанности.
То есть давайте доказательства пункта 1
еще раз, Чебышов,
почти надерное что-то есть,
а Марков сейчас скажет, что почти надерное ничего нет.
Доказательства пункта 2.
Давайте возьмем теперь через X
обозначим другую величину, не ту, которая была раньше,
не число изолированных вершин.
X это будет число нетригиальных вершин.
И это реально компонент связанности.
Ну, то есть тех, у которых меньше строка, чем n-вершины.
Компонент связанности.
Еще раз, компонент,
который не совпадает совсем графу.
Компонент, у которых одна вершина изолированных,
две вершины можно, 3n-1 можно,
но только нет.
То есть, если
же
связан,
то X равняется 0 и наоборот.
Надо было сказать тогда и только тогда.
Но могу и так.
Еще раз, граф связан тогда и только тогда,
когда X равняется 0.
Просто по определению.
В графе нет ни деревянных компонентов, значит он связан.
Но это такой поясник.
То есть, наша цель
доказать,
что
асинтонически почти, наверное,
X таки равняется 0.
Граф связан, то есть X равняется 0.
Ну давай посмотрим,
в какой вероятности X больше либо равен единице.
Помните мой стат?
Я сказал, хочется применить неравенство Маркова, но оно ничего не дает.
А вот здесь как раз именно оно и дает.
Согласно неравенству Маркова,
неравенство Маркова
это не больше, чем Y.
Интересно, кто-то понимает, почему, да?
Ну надо на единицу делить, там поделить на А,
но А у нас равно 1.
Поэтому и X просто и останется.
Так.
Как бы нам теперь посчитать
или оценить мотоожидание XA?
Давайте прежде всего напишем вот так.
X1
плюс и так далее,
и X и так далее,
Догадываетесь ли вы, что такое X и T тут?
Я специально торможу, чтобы вы могли задуматься и записать.
Просто количество компонентов на Y вершинах, правильно?
Количество компонентов X и T это количество компонентов
на Y вершинах.
Пользуемся линейностью.
Получаем сумму
в поединке единицы до N.
Ой, да, это Y1, извините.
Мотоожидание X.
Так что мотоожидание X этого посчитать,
сколько в среднем компонентов,
каждый из которых имеет ровные вершины.
Придется
вам подумать, а мне постирать.
У вас умственное упражнение, а у меня физическое.
Пока я стираю, подумайте.
Наверное, ничего не придумали, да?
Я запою на F степени Y.
Но не на F степени, нет.
И не один, ну один минус P, но не в степени Y.
Значит, маленький, давайте я нарисую картину.
Пресловутая сортелька
символизирует собой множество всех вершин случайного графа.
Нас интересует
под множество мощности Y.
Давайте я под сортельку нарисую, напишу Y.
Хорошо?
Теперь давайте я так сделаю.
Я все такие подсортельки перечислю
за номеру.
Вот там A1
и так далее A с индексом C из N по Y.
Это все возможные множество вершин мощности Y.
Каждый из них теоретически может оказаться чем?
Компонентой связанности, да?
А может не оказаться.
Ну, линейность, это такой истиматик, станут алгоритмами.
Я вам уже говорил в прошлый раз.
То есть мы пишем так, как бы это обозначить.
Давайте обозначим X
и T житое.
Мы зафиксировали. И
ажи будут меняться в пределах от теницы до C из N по Y.
Значит, X и T житое
от графа.
Это есть один.
Если
аи-те
ой, ажи-те
ажи-те, извините, конечно, жи у нас
отвечает за номер подсортельки.
Если ажи-те
является компонентой
в графе G
и ноль иначе.
Вот первое, что вы должны ососнать, самое главное.
Вы понимаете, что X и T ажи
это количество компонентной вершины.
Это в точности сумма пожи
от 1 до C из N по Y
X и T житое ажи.
Ну, вот этот
мега алгоритмический подход.
Тупо перебираем все подможества
и каждое проверяем.
Вы мне скажете, Боже мой,
это почти издевательство.
Если вот это и является компонентой,
то, наверное, вот это уже не является.
Согласны, да?
Если вот такая подсортелька
в каком-то конкретном графе
служит компонентой связности,
может вот такая служить,
то как компоненты могут пересекаться?
Так, друзья, нормально?
Я пытаюсь давить на то,
что вот эти величины
дикозавидимые.
И с точки зрения алгоритмов,
конечно, глупо складывать все.
А с точки зрения подсчета
от ожидания самое то.
Ну, правильно же?
Мало ли чтобы там автоматом с нами,
а мы компьютер заставим
просто все вот это считать.
Все.
Так, друзья, поднимите руки,
чтобы вы понимали.
Ну, хорошо.
Так, тогда
мат ожидания иксытова
это сумма пошли от 1СН
до ЦИЦН по И.
Ну, можно я не буду писать
промежуточный шаг.
Мат ожидания иксытова-житого, конечно.
Но оно равно вот этой вероятности.
Мат ожидания иксытова-житого,
которое сюда надо рисовать,
оно равно вот этой вероятности.
То есть надо писать вот так.
Вероятность того, что ошибка
является компонентной гранижи.
Согласны?
Мат ожидания идикатора
это вероятность того,
что он равен единице.
И вот тут наступает некий кирпик.
Почему, собственно, вы не смогли
сходу ответить на вопрос?
Дело в том, что непонятно,
как посчитать значение этой вероятности.
Ну, они же каждый имеет
разные вероятности, понимаете?
Можно доказывать, что есть
основное дерево, там еще какие-то
страшные вещи говорить,
но мы не будем этого делать.
Все равно вот эту вероятность
явно вы не посчитаете,
только на компьютере.
Ну, оценим сверху, конечно.
Нам же нужно вот так оценить.
Мы сейчас эту вероятность
тоже оценим сверху.
Так, зря я нарисовал вторую сорту.
Она влитит.
Вернемся к исходной картине.
Это вот у нас ожитое нарисовка.
Оно размерами.
Вот событие ожитое
является компонентом.
В чем оно состоит?
Во-первых,
в том, что вот этот кусок
графа Ж
связанный.
А во-вторых,
никаких ребер
вот такого вида уже нет.
Правда?
Наружу из этого множества
никакие ребра не ведут.
Потому что это компонента связанности.
И ничего добавить нельзя.
Это класс эквивалентности максимально.
Все понимают, да?
Вот таких ребер быть не может.
Ну, давайте просто тупо
оценим
вот так.
Вероятность того, что
из ожитого
в В минус ожитое
В это вот это.
Это все.
Множество вершин
это В.
В минус ожитое не идут ребра.
Ну, меньше, наверное, строго, конечно.
Но я уже написал.
Поняли, да? Тут стоит пересечение двух условий.
Одно состоит в связанности,
а второе вот оно написано.
Но я вероятность пересечения
оценил вероятностью одного из событий.
Забил вообще
на связанность.
Следили?
Так.
А вот это вероятно считается.
Чему она равна?
Правильно.
Сумма пожи
от единицы
до цели из н по i
1 минус
p вот в такой вот
степени.
i умножить на n минус
i.
Ну почему? Потому что тут у нас
и вершин, вот тут вот у нас
и вершин, а тут у нас
n минус i вершин.
И нам нужно, чтобы любая из этих
с любой из этих не соединялась
с ребром.
Вероятность отсутствия ребра это 1 минус
p, а количество отсутствующих
как раз и умножить на n минус
Так. Возвращаемся сюда.
И чтобы это не больше
чем сумма по i от
единицы на n минус 1.
Возвращаемся назад.
Слушайте, они слагаемые
это у нас, не знайте старши.
Ну то есть надо просто вот эту
штуку умножить на
c из n по i.
c из n по i
1 минус p
в степени i
на n минус
Так.
Что нам осталось доказать?
Осталось доказать
под вопрос.
Это стремится к нулю, правда?
Вероятность
того, что есть хотя бы одна
компонента
должна стремиться к нулю.
Скоро мы хотим, чтобы отсутствие компонента
было выполнено почти наверх.
Согласны, да?
Да.
Где?
Вот тут?
Нет. Смотрите, shi
это номер и элементного
множества.
А количество элементов, как раз i
не важно, какой shi
какой номер
не важно, какую именно сардельку размера
i мы берем.
Количество отсутствующих ребер,
то есть таких, что одна вершина снаружи, одна внутри
это именно i умножить на n минус i.
Сколько вершин тут
их и втуг
умножить на сколько вершин снаружи их n минус i.
Пара отсюда и отсюда
i умножить на n минус i.
Shi
вот оно. И здесь никакой
зависимости от shi нет.
Именно поэтому я всю сумму
просто превратил
c из n по i на 1 минус
p в слепени i на
n минус i. И вот сюда
вот переписал.
Ну, времени-то мало осталось.
Давайте, знаете, по чему я это скажу?
В опену есть время еще какое-то.
В опену. Смотрите, что будет
если i равно 1.
По-моему, это как раз мат ожидания
числа изолированных вершин, но это понятно
и равно 1
компоненты размера 1.
То есть как раз изолированные вершины
будет 0.
Смотрите, если i равно 1
я напишу, то слагаемое
представляет собой n на
1 минус p в n минус 1.
Мы уже знаем, что в нашей ситуации
в рамках пункта 1
это стремится к 0,
это мы знаем, это не вопрос, это известно.
Помните, да?
Поскольку c больше 1,
это стремится к 0.
В этом есть смысл.
Но слушайте из того, что одно лишь
слагаемое стремится к 0,
к сожалению, не следует, что вся сумма
верхний предел, который тоже зависит
от данного, стремится к 0.
Понимаете, да?
Какая крупная неприятность.
Причем, если вы посмотрим
внимательно, чем ближе
c под единицей,
тем медленнее стремится к 0.
Вот это первое слагаемое.
Прям оно может быть совсем на грани,
еле-еле-еле.
Там как 1 поделить на корень
миллионной степени z, что-нибудь такое.
Если c это
1,1 миллионная, то
там будет 1 поделить на корень
миллионной степени z. Все очень медленно
стремится к 0. Что делать?
Какой ужас.
На первых давайте заметим,
мы сегодня не закончим,
но самая противная аналитика пойдет
в следующий раз. Давайте заметим,
что
достаточно
доказать,
что вот такая сумма стремится к 0,
то как бы половинка.
Почему?
Потому что все, что идет
после n пополам,
оно симметрично.
Т-шка симметричная,
и тут стоит симметричная пункция.
Если мы докажем,
что при всех i не больше,
чем n пополам,
сумма слагаемых стремится к 0,
то как бы половинка
идет после n пополам,
то как бы половинка
идет после n пополам,
сумма слагаемых стремится к 0,
то при большем будет та же самая сумма,
она тем, что тоже будет стремиться к 0.
Нормально объясню?
Достаточно доказать вот это.
Так, что я еще
успею сказать?
Давайте вот это
вот, ну
каждое слагание
обозначим a,
i, t,
a, t.
Везем такое обозначение.
i, t слагаемое, но оно является
пункциатом.
Попробуем
i плюс
1 от r
разделить на i, t от r.
Этому в следующий раз
аккуратно сделаем.
Но давайте
вы просто интуицию поймаете.
Достаточно, если там 5 минут
или сколько осталось.
Допустим,
это в следующий раз станет понятно,
окажется, что это не больше,
чем некоторое q от n,
которое от i вообще не зависит.
Видите, вот тут есть i,
а тут и нет.
Причем q от n не або какое, а стремящееся к 0.
Вот вдруг такое получится.
Как бы можно было тогда действовать?
Смотрите.
Смотрите.
Это a1 от n.
Выносим за скобку.
Самое первое слагаемое.
В скобках остается
1.
Правильно?
Потом a2 от n.
Поделить на a1
от n. Я надеюсь, вы успеваете,
товарищи.
Следующая как?
a3 от n.
Поделить
на 1.
Поделим на 2.
Множим на 2.
И поделим снова на 1.
Вот так.
Что у нас вырисовывается,
товарищи?
Необидительно.
Правильно.
Яндрическое прогрессие.
То есть,
это меньше,
чем a1 от n.
Умножить на 1.
Плюс q от n.
Плюс q2 от n.
И так до бесконечности.
Ну, до бесконечности,
на самом деле, не до бесконечности.
Ну, неважно. Меньше же.
Можно оценить бесконечно.
Меньше не было равно, понимаешь?
Ну, тут конечная числа слагаемых,
а тут бесконечная.
Ну ладно.
Бесконечно, правда?
Умножь.
То есть, будет a1 от n.
Но я люблю суммировать бесконечную прогрессию.
Помните, мы с вами столбик делили когда-то.
Это проще записывается просто.
Короче, запись для суммы бесконечная.
Слушайте, ну q от n стремится к 0.
a1 от n.
Вот оно. Оно тоже стремится к 0.
Куда стремится это выражение?
К 0.
Дорогие товарищи,
к сожалению,
к сожалению,
нам не удастся доказать
вот это.
Но не совсем.
Дослушайте до конца.
Слышно очень звучит? Да.
Нам не удастся это доказать при всех ине,
превосходящих a1 пополам.
Любое у маленькой от n все хорошо,
но порядка, она же,
уже плохо.
Поэтому нам придется эту сумму распить
на две части.
Поняли, да?
Одна часть будет касаться
вот именно этого пополам.
Вот он там будет реализован.
А другая настолько уже будет маленьким
хвостиком, что мы вот тяпля
поценим, он тоже будет стремиться к 0.
Я объяснил это длительно.
Понятно?
Следующий раз мы это аккуратно реализуем.
Но это тяжелый шанс.

Окей, давайте начинать. Тогда у нас сегодня вторая
презентация по MapReduce. Напомню, на чем мы закончили
в прошлый раз. Мы с вами разобрали, что вообще такое
парадигма MapReduce, из каких частей она состоит, то есть
MapSortReduce. Мы с вами разобрали несколько примеров. Например,
что MapReduce можно реализовать вообще без всякого ходупа,
можно на обычном питоне, на самом деле не только
на питоне, можно на плюсах реализовать. И мы с вами
разобрали типичную жадачку для MapReduce, то есть wordCount.
То есть если нам надо посчитать в каком-то тексте, сколько
раз встречается каждое слово, что мы делаем. Мы каждому
слову в соответствии ставим единичку, потом на стадии
сортировки группируем, и на стадии редюса суммируем
все единички для каждого слова. То есть вот такая вот
штука получается. Ну и мы с вами разобрали, как это
работает в ходупе. На семинарах, насколько я понимаю, вам
уже объяснили, как это работает детально, вы поразбирали
какие-то примерчики, позапускали на кластере. Было у всех
такое. Хорошо. А теперь давайте разберем еще несколько
примеров. Например, как решить задачу grep с помощью
MapReduce, как вы думаете. Вот дан какой-нибудь документ
в такого вида как на рисунке, и нам надо его погрэпать,
то есть пофильтровать по какому-то ключу. Вот все
описываем, опять же, с точки зрения Map делает то-то,
Sort делает то-то, Reduce делает то-то. Sort можно опустить,
в принципе. Обычно и так понятно, что он делает.
Уже лучше. То есть у нас осталось одно слово, мы сделали
бинарный ключик 0 или 1, а дальше вопрос, а зачем нам
вообще тащить за собой слова, у которых 0? То есть наша
задача сделать grep, то есть выкинуть вообще те строчки,
у которых нет совпадений. Поэтому зачем нам ставить
в соответствие 0, когда мы просто можем на маппере
проверить. Если слово соответствует тому, что мы хотим, мы его
печатаем. Если не соответствует, мы его не печатаем. Все.
И этого достаточно. То есть на маппере мы делаем такую
проверку. Слово у нас, если оно одно, его можно запихать,
например, DistributedCache. На семинарах вам показали
DistributedCache. Кому-то показали, кому-то нет. Но если не показали,
напомните семинаристам, видимо, в следующий раз покажут.
И reuse нам не нужен, а значит sort нам тоже не нужен. Вторая
задачка groupBy. GroupBy на самом деле можно рассматривать
как в SQL. В SQL groupBy сам по себе не бывает. Он обычно
идет с какой-то аналитикой. Мы сгруппировали, и поэтому
groupBy посчитали, например, сумму или какой-нибудь там
квантиль, допустим. Тогда эта аналитика будет у нас
считаться на reduce. А если задача просто создать группы,
сгруппировать, то нам тогда reducer не нужен, но он все
равно будет. Почему reducer все равно будет? Потому что
в MapReduce вот эти три стадии есть некая связь. Мы не
можем запустить, например, только sort. Если нам нужен
sort, то нам нужен reduce, хоть какой-нибудь, который, например,
просто печатает то, что получил на вход. То есть map сам
по себе может быть. Reduce сам по себе быть не может.
Ему нужен sort, и ему нужен map. И sort сам по себе тоже
не может. Ему нужен map, и нужен reduce. Вот. А теперь попробуем
оптимизировать то, что мы с вами обсуждали прошлый
раз. Вот та же задачка wordCount. Мы считаем слово единичка.
На маппере сделали слово единичка, на редюсере сложили
слово единичка. И возникает такая идея. Если мы видим,
что на маппере эти слова единичка уже совпадают.
Вот тут есть, если вы посмотрите на эту красную область,
вы увидите don't 1, don't 1, need 1, need 1. Уже совпадают.
Возникает желание, давайте мы его схлопнем прямо здесь.
Зачем тащить кучу пар с собой на редюсер, когда можно
здесь получить don't 2, там need 4 и так далее. У нас будет
пара одна. Это будет меньше данных, и нам будет быстрее
и проще передавать на редюс. Такая оптимизация есть,
она называется комбайнер. Вот работает она вот так.
Когда у нас отработал маппер, он результаты своей работы
сбрасывает, если вы помните, сначала в буфер, потом на
диск. Потом происходит стадия локальной группировки
и локальной сортировки. Вот то, что здесь написано.
Когда мы данные на локальном кусочке, на локальном маппере
сгруппировали, мы можем запустить комбайнер. Можем
запустить, можем не запускать, как захотим. Кстати, вот
эти три стадии, что они напоминают из прошлого
раза? Ну да, они напоминают обычный мапредьюс. То есть
сортировка просто локальная. Комбайн это тоже по сути
такой редьюсер с урезанным функционалом. И запускается
он вот здесь, где указано красное стрелочке. То есть
когда мы создали вот этот вот файл и готовы его разбрасывать
по редьюсерам, мы сначала его утрамбовываем. Вот,
у комбайнера по сравнению с редьюсером есть несколько
особенностей. Давайте рассмотрим их особенно. Обратим внимание
на последнее, я его расскажу отдельно. То, что до последнего
в принципе понятно. Hadoop сортирует результаты работы
мапа. То есть это небольшой редьюсер, который сортирует
и группирует все и считает все. А это работа с маленьким
кусочком данных. Поэтому данных мало, сеть вообще
не используется и такой комбайнер он обычно работает
быстро. Но так как мы комбайнером что-то сагрегировали, то
редьюсеру тоже работать проще. То есть по сути комбайнер
это такая локальная помощь для редьюсера. А вот дальше
вот эта вот последняя часть, что комбайнер может применяться
несколько раз, а может не одного раза, а может один
и контролировать мы это в принципе не можем. Ну как
бы можем, но это надо глубоко залазить в Hadoop и обычно
так не делают. Мы сейчас рассмотрим, как так получается.
Давайте смотреть. Вот эта вот первая часть это работа
мапера. И вот эти вот, вот это вот круг разбитый на кусочки,
эти кусочки это как раз файлы. То есть отработал мапер,
записал в буфер, буфер скинулся на диск, появился один
такой файлик, потом второй, потом третий и так далее.
Дальше на этих файликах у нас запускаются комбайнеры.
Давайте посмотрим как они запускаются, что при этом
происходит. Вот у нас запустился один файл, но допустим этот
файл большой или там очень много ключей и мы запустили
несколько комбайнеров. И также мы проделали для
нескольких еще файлов. После этого когда у нас получился
результат, у нас получается несколько уже схлопнутых,
уже частично обработанных файлов. А потом мы видим,
ну ходу как бы анализирует эти файлы и видит, что у нас
опять ключи совпадают. То есть да, у нас уже нету
слова единичка, слова единичка. У нас есть, например, need
4, need 1, need 6. Ключи совпадают, значения разные, их можно
еще раз сложить. Что происходит? Запускается еще раз комбайнер.
То есть вот эти вот маленькие результаты комбайнеров,
комбайнеров первого этапа, вот этих вот синих, они
соединяются вместе, происходит операция merge и запускается
еще один комбайнер. После этой второй стадии мы видим,
что все равно у нас, там где прошли вторые стадии,
у нас все равно есть совпадение. И мы делаем еще раз ту же
операцию, то есть опять схлопываем и запускаем еще
раз комбайнер. Так можем проделать несколько раз
и в самый последний раз это мы замечаем следующее,
мы замечаем то, что у нас на одной ноде работало
несколько мапперов этой джобы. И мы можем вот эти
результаты, уже покомбайненные, еще раз схлопнуть, еще
раз объединить и запустить еще раз комбайнер. Ну потому
что все равно сеть не используется, все равно это все в рамках
одной машинки, почему бы и нет. Мы это сделаем, это
называется у нас merge final, то есть объединили уже с
нескольких маптасков результаты в один, еще раз скомбайнили
и получили уже полностью максимально схлопнутый
результат для вот этой ноды. Это я вам рассказал,
почему комбайнер может запускаться несколько раз.
То есть тут несколько стадий, причем комбайнер запускается
на результатах самого себя. А когда может не быть
ни одного раза? Вот здесь как раз на схемке указано
if a single record is too big to fit in memory buffer. Но это на самом
деле не обязательно чтобы был именно single record, может
быть например две-три записи. То есть в ходу тоже есть
настройки, которые отключат комбайнер, если записей
мало. Если у нас допустим одна запись занимает там
несколько мегабайт, и этих записей помещается в файл
мало, нет смысла запускать комбайнер. Почему комбайнер
нет смысла запускать, потому что комбайнер это процесс.
Для того, чтобы запустился комбайнер надо пойти в ресурс-менеджер,
попросить ресурсы, дождаться этих ресурсов, стартовать
виртуальную машину джавы, стартовать в нем вот этот
процесс. Потом это все обсчитается и нужно это все завершить.
Ну долго, иногда это вообще делать не надо. Поэтому иногда
комбайнера просто нету. И даже если мы в коде прописали,
что мы хотим комбайнер добавить, он у нас не будет запущен.
Это все настраивается на уровне опции, вот можете
посмотреть, открыть эту схему, и здесь много вот этих
тонких настроек для комбайнера. Обычно в них особо не лезут.
Ну по крайней мере, когда мы пишем код для комбайнера,
мы должны быть уверены, что он будет на любых настройках
работать, то есть задачка не упадет, если комбайнер
будет запускаться несколько раз, если он запустится
один раз, если он вообще не запустится. Во всех этих
случаях задачка не упадет. Чтоб так было, нам и нужно
выполнять вот это условие. Комбайнер не может изменять
ключ и значение, то есть тип ключа и значения. Ну и
если говорить в общем, то он вообще не может менять
структуру данных. То есть не может в комбайнере
получиться вместо строка число, строка tuple или там
строка array. Такого быть не должно.
Давайте проверим, как мы это поняли, и попробуем
составить комбайнер, ну просто скажите, какую функцию
комбайнер будет выполнять для каждого из этих трех
кейсов.
Пока вы думаете, вопрос, на семинарах уже проходили
сортировку с помощью добавления девяток.
Ну давайте тогда думать, каким будет комбайнер вот
для суммы, например.
Так, ну будет просто сумма для одинакового ключа.
Окей, для среднего, что? Тут немного сложнее, потому
что среднее от среднего, оно неправильно будет считаться.
А сумма и количество у нас с структурой меняется.
Так, где маркер? То есть у нас было, например, там
А, нельзя пальцем рисовать.
Не работает.
Ну, ну, ну, ну, ну.
Ну, не работает.
Не работает, я тогда открою блокнот.
И буду показывать здесь.
А второе параллельное считается суммой.
А потом ты делаешь следующую джобу, которая это все будет
объединять.
Работать такое, конечно, будет, но наша цель, конечно,
не будет.
Ну, да.
Работать такое, конечно, будет, но наша цель, комбайнер,
зачем мы делаем?
Чтобы оптимизировать, чтобы быстрее работало все.
А ты вместо одной джобы сделал две.
Поэтому не очень хороший вариант.
На самом деле, можно подумать просто немного по-другому.
Мы не вот эту структуру пытаемся запихать в одно
число.
А мы просто здесь структуру поправим.
Ну, сделаем вот так.
Кто нам мешает на мапере вместо одной единички поставить
две единички?
И все, у нас мы по одной единичке посчитаем сумму,
по другой единичке посчитаем каунд.
И структура будет сохраняться.
Окей, а что делаем с медианой?
Для среднего мы, так как мы не можем посчитать средний
от среднего, то есть вот такая штука.
работать не будет.
Значит нам нужно считать отдельно сумму, отдельно каунт, а это два элемента.
На маппере у нас была
ключ и значение, где значение это одно число,
а на комбайнере мы хотим сделать два элемента, в этом и проблема, и нам пришлось
маппер раздувать, то есть менять его структуру, чтобы было два числа.
Числа.
Количество чего?
Так, и что мы дальше с этим будем делать?
А количество мы можем посчитать? У нас же комбайнер,
он считает что-либо только для локального кусочка, он не может посчитать все количество.
И дальше что?
То есть мы считаем на комбайнере локальную медиану, да?
Не получится.
Вот, то есть для того, чтобы посчитать медиану вообще в целом, нам нужно,
чтобы наш ряд был отсортирован глобально, то есть весь ряд, чтобы был отсортирован, и мы ищем
там центральные значения или два центральных значения поделить на два.
Ключевое, что для того, чтобы посчитать медиану, нам надо отсортировать все.
Что у нас идет перед комбайнером? Local sort и перед ним маппер.
Нигде там отсортировать все, возможностей нету, поэтому
получается вот так.
То есть для медианы мы не можем придумать комбайнер, потому что медиана требует полной сортировки.
Вот мы с вами такие три кейса разобрали.
Есть еще in-mapper-комбайнер.
Это такой хак, который, с одной стороны, иногда полезный, иногда вредный.
Почему я сейчас расскажу.
Мы с вами уже обсудили то, что сам по себе комбайнер, он требует не сильно много, но все-таки накладные расходы,
то есть не очень много.
Мы уже обсудили то, что сам по себе комбайнер, он требует не сильно много, но все-таки накладные расходы на работу с ресурсами, на работу с джавой.
Иногда этого делать не хочется.
In-mapper-комбайнер, это когда мы прямо в коде делаем логику комбайнера.
Ну вот взять опять же тот же word count.
Что там делает маппер?
Он берет слово, пишет единичку, которую потом мы когда-то к чем-то сложим.
Давайте мы не будем ждать чего-то когда-то, а просто на маппере, на обычном будем хранить словарик, вида слова, число.
Пришло новое слово, мы или добавили в словарик новый ключ, или добавили единичку к текущему ключу.
То есть мы будем вот такой вот словарь накапливать.
И вот этот word count у нас будет локально считаться.
Это будет хорошо и быстро до того момента, пока наша маппа не займет весь наш контейнер.
То есть всю память, которую нам выдали, и потом мы скажем out of memory, все пока.
Поэтому in-mapper-комбайнер, он хорош тогда, когда мы точно знаем, что значений будет немного.
То есть условно мы, например, считаем всех студентов МФТИ, у которых делим студентов МФТИ на классы.
Там хорошисты, отличники, те, у кого удовлетворительно и так далее.
Четыре класса.
И вот мы считаем, сколько студентов в каждом классе.
Мы точно знаем, что у нас количество классов никогда не вырастет.
Или там, например, считаем количество студентов на каждой фистех-школе тоже.
Фистех-школа количество какое-то константное.
В какой-то момент может случиться плюс один какой-нибудь, но плюс тысяча фистех-школ явно не появится.
Вот в таких случаях можно использовать in-mapper-комбайнер.
А вот для обычного word count, где слов постоянно может быть больше, больше и больше,
in-mapper-комбайнер не годится.
Вот есть ли какие-то по-комбайнеру вопросы?
Тогда, компаратор, на семинаре вам расскажут, как вообще делать сортировку,
если нам нужно, если нам нужно, если нам нужно, если нам нужно.
И вам расскажут, как вообще делать сортировку,
если нам нужно отсортировать данные как-нибудь необычно.
Ну, чтобы понять, что значит необычно для ходупа, надо понять, что значит обычно для ходупа.
А обычно это значит, что вот стандартный сорт в ходупе, на который мы никак не влияем,
вот он как есть, так и есть, что он делает, как именно он сортирует.
Он сортирует по возрастанию, это первое.
То есть если нам надо найти топы и отсортировать по убыванию, уже не годится.
Дальше он сортирует все объекты, даже если это числа, он сортирует их лексикографически, как числа, то есть как текст.
Сейчас попробую.
Вот, то есть что значит отсортировать числа лексикографически?
Это значит, что числа типа, вот такие числа, они будут отсортированы именно в таком порядке.
Потому что мы их сравниваем не как числа, сравниваем по символю.
То есть, надо как-то с этим справиться, как бы вы справлялись и как бы вы справлялись с такими числами.
Так, вот этот вот.
То есть надо как-то с этим справиться, вот как бы
вы справлялись с этим, имея на руках только маппер
и редьюсер, как мы можем преобразовать наш ключ значения
на маппере, чтобы сортировка была правильная, да и третий
момент, мы сортируем только по первому полю.
Вот это то, что делает стандартная сортировка в
Hadoop.
Давайте подумаем, как можно с ней бороться штатными
средствами.
Мы знаем маппер, мы знаем редьюсер, внутри сортировки
мы пока не лезем, потому что не знаем как.
Да, тут добавить, умножить на минус единичку, хорошо.
Что делать с вот этим, когда оно по символе сравнивает.
А хэш почему?
У нас есть набор значений, мы хотим отсортировать,
а что суммировать?
Ну, я так понял, ты имеешь в виду длину выровнять
как-то или что?
Да, вот я не совсем понял, куда туда можно приплести
хэш, который тоже может разную длину давать.
Что с проектором?
Чтобы сделать одинаковую длину, нам надо сначала
найти самое длинное число, а в данном случае это 4000.
И можно, как вы сказали, дописать слева нули, а можно сделать
проще и с одной стороны не проще, с другой стороны
просто убить двух зайцев.
И вот эту проблему починить, и вот эту.
Сделать 10 в степени n-1-k, где k это вот это число.
Но проблема была в том, что данные сортируются
только по возрастанию.
А здесь нам надо длину выровнять.
Если мы сделаем вот эту вот формулу применим, то
у нас получится.
Давайте считать, что n у нас равно 4, вот такие у нас
будут получаться числа, и мы видим, что у них длина
одинаковая.
Да, почему я здесь сделал 10 в степени n-1, потому что
вдруг у нас тут 0 где-то стоит.
Если я сделал 10 в степени n-0, то у нас будет где-то
тут стоять 10 тысяч, и оно нам весь порядок испортит.
Вот, и таким образом мы справились за одной из
сортировок по возрастанию, потому что тут уже сортировка
будет инвертирована.
Вот, то есть вот такую штуку надо сделать, и с третьей
проблемой как справиться, ну поменять местами поля
на мапере.
Ну хорошо, как бы все это можно сделать, но есть две
проблемы.
Первая проблема в том, что когда мы стадию сортировки
пройдем, на редьюсере нам надо будет все это раскрутить
обратно, ну потому что ответ все-таки должен быть
какой-то вот такой, более адекватный, потому что мы
потом будем как-нибудь диаграммы строить по этому результату,
и с вот такими вот девятками непонятно что делать.
Ну и потом для того, чтобы вот эту формулу применить,
нам надо знать n.
Для этого нам надо уже заранее оценить, какой у нас может
быть максимальный каунт.
Вот, поэтому на семинарах вам покажут, это все конечно
костыли, так и делать не надо, а как надо делать?
Как надо делать компаратор?
То есть вот мы поняли, что вот это конечно не вариант
дописывания кучи девяток.
Компаратор, зачем он вообще тут надо и откуда он взялся?
Ну раз есть сортировка, для любой сортировки нужно
сравнение.
И в компараторе мы просто прописываем, как именно
мы будем сравнивать, потому что если это числа, то ходуб
их как-то может сравнить, а если это стулья парты
и крокодилы, то как бы непонятно, как их сравнивать.
Мы пишем свой класс, или используем существующий,
на самом деле на нашем курсе во всех домашках и во всех
задачках хватит использования существующего компаратора,
причем одного, который называется K-field based компаратор,
то есть как бы он так и расшифровывается как компаратор,
который базируется на ключах.
Если вдруг вам в жизни понадобится использовать какой-то
другой компаратор или написать самому, то придется взять
вот этот класс, отнаследоваться от него и реализовать функцию
compare.
Здесь есть Javista?
Если Javista нет, то функция compare напишется на Javi достаточно
легко.
Это просто функция, которая получает на вход два аргумента
и возвращает возможные три значения.
Один, если один аргумент больше другого, минус один
в обратном случае и ноль, если равный.
Вот такую штуку вам надо реализовать, обернуть
в класс и добавить в ходуб, как здесь на слайде написано.
В случае с K-field based компаратор, вам на семинаре покажут,
вам нужно будет задать количество полей, с которыми
вы работаете и набор ключиков, как вы эти поля будете
сортировать.
Кто пользовался командой sort в линуксе?
Да, в шеле.
Когда будет возможность, зайдите в мануал команды
sort.
То, что там написано, все эти ключики, их можно будет
использовать и в компараторе.
Дальше мы с вами разобрали комбайнер, разобрали компаратор.
Есть еще одна вещь, на которую мы смотрели, как на черный
ящик.
Имейте в виду partitioner, это тот самый процесс, который
отвечает за разбиение данных по редьюсерам, то есть он
решает вот эта конкретная пара ключ значения, на
какой она редьюсер пойдет.
По умолчанию, это решается вот так, hash от K по модуле
R.
Кому R мы задаем руками.
Еще в прошлый раз я вам показывал, что мы можем
задать количество редьюсеров руками, а количество мапперов
напрямую не задается, оно зависит от размера сприта.
Поэтому вот это мы знаем.
Ну и опять же иногда нам этого не хватает.
Например, в задаче может быть такое, что какие-то
ключи, хотя они разные, но они должны обязательно
попасть на один редьюсер, потому что мы хотим какую-то
общую статистику посчитать по ним.
Тогда мы это прописываем в условии вот здесь, в partitioner.
То есть для того, чтобы сделать свой partitioner, нам нужно
опять-таки писать на джаве.
Вот в случае с комбайнером, кстати, нам можно не писать
на джаве.
Маппер, комбайнер и редьюсер можно реализовать на питоне.
Компаратор уже на питоне реализовать не получится,
partitioner тоже.
Нам придется все-таки взять джаву, взять класс partitioner
и реализовать там вот эту вот функцию, она тоже довольно
простая.
То есть получаем на вход пару ключ значения, константу
редьюсеров подаем сюда, сколько их штук и получаем
int.
Вот этот int – это номер редьюсера от 0 до r.
Про свой partitioner мы прямо сегодня поговорим на ближайшем
примере.
Если это сейчас непонятно, то не страшно.
Вопросы по комбайнерам-компараторам появились какие-нибудь?
Окей, тогда давайте разберем такую задачку.
Вот, называется она обратный индекс, то есть это в таком
даже не первом, а в нулевом приближении то, что делают
поисковые системы.
Там, конечно, все сильно сложнее, но вот в таком
самом упрощенном варианте у нас есть какие-то документы,
какие-то веб-страницы, которые представлены вот в таком
виде.
ID-шник и содержимое.
Есть какой-то терм, то есть шаблон, по которому мы
ищем, и нам надо сделать вот такой вот output, то есть
шаблон и набор ID-шников, которые этот шаблон содержит.
Давайте подумаем, как это делается на MapReduce.
Да, контент – это слова какие-то, набор, да.
Зачем так сложно?
Ну, можно и так, но это как-то сложно.
То есть ты хочешь достать массивы ID-шников, посчитать
каждое слово, в скольких документах оно встречается,
а потом понять, где терм, ну, как бы можно, но вообще
проще.
А что мы дальше с этим будем делать?
Надо дальше потом проверить, где у нас встретился терм.
А если ты отделил слово, то есть если ты разбил контент
по частям, как ты будешь проверять?
Вот у тебя здесь терм встретился, а здесь не встретился, ID-шник
один, что с этим делать?
Да, докрутить сюда.
Может, ну, ты их только в тюбл объедини, не может.
А что еще раз будет в значении?
Слово в контенте, терм или нет.
А что нам мешает взять контент весь и проверить, в нем терм
встречается или нет?
То есть вот это вот разбиение его можно не делать, потому
что мы в итоге все равно принимаем решение по всему
контенту.
Или там есть терм, или там нет терма.
Но это получается такая же штука, как грэп, только
нам потом по-другому нужно данные выдать.
Если в грэпе мы это все в столбе выдавали, то здесь
надо это все еще в строку объединить.
То есть на маппере мы сделали, по сути, грэп, один в один,
но правда еще убрали контент.
То есть как только мы проверили, есть ли терм в контенте
или нет, контента нам больше не нужен.
Ну а потом нам остается это все скомпоновать, то есть
терм и повытаскивать ID-шники с результатов.
Это был такой самый простой вариант даже не обратного
индекса, такого скорее поиска.
Давайте теперь усложним и заодно посмотрим на тот
пример, где нужно писать свой партишнер.
Вот теперь мы должны вот эту выдачу из кучи ID-шников
как-то проранжировать.
Ну опять же, то же, что делают наши поисковые системы.
Мы что-то загуглили, у нас выдалось там 10 тысяч,
10 тысяч элементов, но в топе мы обычно находим что-то
более релевантное, а дальше уже менее релевантное.
Как это отранжировать?
Ну опять же, самый простой вариант в реальной жизни
делается сильно сложнее, потому что вот эту метрику
терм фриквенсии, вы наверное знаете, что ее можно легко
обмануть, ну ее часто обманывали, делая страницы, где просто
писали какой-то рандомный текст и повторяли ключевые
слова 10 тысяч раз, видели наверное такие страницы.
Не видели?
Ну сейчас может такого и нету, но вот где-то года
три назад я встречал, что там например, ты ищешь
какую-нибудь книжку по хадупу, допустим, и у тебя выскакивает
страница, где будет написано хадуп-хадуп-хадуп, потом
какой-то рандомный текст, потом опять хадуп-хадуп
10 раз, ты открываешь, и там кроме рекламы ничего,
и вот этого вот бреда ничего нету.
Помогает уменьшать шум, но не до конца, собственно,
чтобы обмануть TF и DF, стали делать странички, которые
содержат не только кучу упоминаний ключевого слова,
а еще между ними какой-то рандомный текст пишут, и
тогда уже эта метрика проще обманывается, ну поэтому
используются еще всякие другие метрики.
Ну а вот TF, она самая простая, она просто считает, сколько
раз наш терм встретился, то есть не просто есть нету,
а сколько раз встретился.
Давайте эту задачку тоже решать, вот решение здесь
написано, что мы делаем.
На мапе мы делаем тот же самый грэп, но одновременно
с грэпом мы еще делаем и ворд-каунт, то есть вот
мы в контенте теперь считаем не просто встретился терм
или нет, а сколько раз он встретился, и выводим единички.
Дальше на редьюсе мы суммируем, но нам нужно сформировать
вот такие вот пары, то есть редьюз у нас выдает вот
такие пары, терм docid это ключ, вот терм docid это ключ,
задача редьюсера просуммировать единички.
Хорошо, но нам надо, чтобы был не терм docid это ключ,
а чтобы терм был ключом, и вот тут как раз случай
использования своего партишнера, то есть после того, как мы
вот эти вот тройки сделали, терм docid и сумма, нам остается
их перегруппировать вот в такую запись, для того,
чтобы такая запись была правильная, надо чтобы
все термы одинаковые, независимо от их docid, попали на один
редьюзер.
Вот, то есть это более сильное условие, чем обычный партишнер.
Обычный партишнер нам бы требовал вот этого, хэш
от… вот, обычный партишнер нам бы требовал вот этого,
а мы сделаем партишнер, который будет делать вот
это, то есть более сильное условие, вот когда нам нужно
какое-то более сильное условие сделать для партишнера,
мы пишем свой.
Есть ли какие-нибудь вопросы сейчас?
То, что в обычном апредьюсе нам главное добиться, чтобы
записи с одинаковыми парами терм docid попали на один редьюзер,
а теперь мы делаем более сильное условие, нам надо
чтобы не только терм docid был одинаковый, а чтобы…
достаточно того, чтобы терм был одинаковый, то
есть значений в одном редьюзере становиться больше.
Вот, ну и мы получаем вот такую вещь в итоге.
Эта вещь – это длинное значение, то есть ключ
он маленький, а значение тут может быть просто огромная
строка.
И давайте подумаем, как ее хранить, и вернемся опять
же к тому, о чем я рассказывал в начале, вот комбайнер,
когда он не запускается, когда запись очень длинная.
Вот как раз пример очень длинной записи, что вообще
можно с ними делать, как Hadoop умеет их хранить.
Можно их хранить двумя как бы подходами.
Подход Stripes – это когда мы храним вот в таком виде,
как мы выявили, то есть есть ключ, есть огромная-огромная
строка со значения.
И подход Payers, когда мы разбиваем эту большую строку, у нас
получается дублирование ключа, но зато значение
получается маленькое.
То есть мы вот этот вот лист большой разбили на тюплые,
и каждый тюплый вводим отдельно.
Скажите, какие плюсы и минусы вы видите в каждом подходе?
Ну, с точки зрения хранения, с точки зрения того, как
оно накружает ресурсы – диски, память, сеть.
Что во втором?
Да, то есть больше данных.
Мы просто нагенерили больше данных, что значит грузим
диск больше.
А в первом?
Да.
То есть мы получается меньше передаем данных в один
момент, но зато мы суммарно получаем больше данных,
то есть больше диск грузим.
А в первом случае диск мы грузим меньше, но зато
вот эта вот огромная строчка, она должна вся уместиться
в память.
Потому что у нас, да, есть там всякие в ходупе инструменты,
которые делают вот этот буфер.
Буфер переполняется, мы можем скинуть на диск, но
у нас нет никакой возможности разбить одну запись на кусочки,
какая бы она огромная ни была.
Одна запись, а если она не помещается в оперативку,
то на этом все мы падаем.
А что именно непонятно, а то, что, а ты имеешь, что
почему здесь нету индексов, да, docid1, ну вообще они тут
должны быть.
То есть мы просто взяли вот этот лист, понятно,
что там тюплы, они отличаются, и мы их просто отдельно
записали.
То есть в случае спэш, да, больше нагрузка на диски,
больше нагрузка на сети размыто, все должны передать,
но меньше нагрузка на память, потому что записи маленькие.
В случае страйпс, наоборот, меньше нагрузка на диски
сеть мы передаем реже, но уже если передаем, то передаем,
и памяти может тупо не хватить.
Вот, а теперь переходим к самому страшному, что есть
в MapReduce, это джойны.
Напомню, что джойны бывают вот такие, ну на самом деле
их там больше видов, но самые базовые, вот их четыре,
inner, full, left, right.
Скажите вообще, как вы думаете, какая самая большая боль
при реализации джойна в MapReduce?
Вот вы до этого какие-то задачки на семинарах разбирали
по MapReduce, что-то получалось, а вот джойн, чем он отличается
от всего того, что мы с вами видели раньше?
Почему inner, а остальных чего там, что отличается?
То есть узнать, откуда именно пришли данные, с какой таблички,
с первой или со второй, да?
То есть да, у нас просто есть две таблицы, две разных
структуры, а наш MapReduce, с которым мы до этого работали,
у него есть один input, то есть минус input равно и вот папка,
и маппер тоже один, ну да, их там много процессов,
много копий, но сама реализация маппера, сам код, он один
и тот же.
Получается, что нам нужно написать такой маппер, который
умеет работать и с первой таблицей, и со второй таблицей,
и еще он умеет понимать, откуда данные пришли.
Сейчас мы это будем с вами делать, потому что опять
же с точки зрения ходу по ходу из коробки не поймет,
что есть таблица A, есть таблица B, он просто понимает, что
вот ему приходят какие-то пары, иногда они приходят
разные.
Поэтому логику джойна, саму как бы реализацию джойна
мы вынесем в Reduce, а в маппе мы просто сделаем предобработку
и выясним, какие данные откуда пришли.
То есть когда у нас есть разные строчки, на маппе
мы просто их парсим по какому-то шаблону и ставим
какой-нибудь тег.
То есть вот теги A и B, если таблица A, если таблица
B.
Потом мы делаем сортировку, группировку и partitioner только
по ключу, без тегов, и получается, что данные с одинаковыми
ключами и для таблицы A, и для таблицы B придут в
один Reducer.
И уже тут мы сможем спокойно сделать джойн, потому что
в рамках одного Reducer имеем одинаковые ключи и для A,
и для B, нам просто остается их объединить.
Давайте это разберем на примеры.
Сейчас я зайду на наш GitLab.
Итак, я сейчас зайду на наш Github, и я хочу, чтобы вы
видели, как мы делаем это, как мы делаем это, как мы
делаем это, как мы делаем это, как мы делаем это, как
мы делаем это, как мы делаем это, как мы делаем это,
как мы делаем это, как мы делаем это, как мы делаем
это, как мы делаем это, как мы делаем это, как мы
делаем это, как мы делаем это, как мы делаем это,
как мы делаем это, как мы делаем это, как мы делаем
это, как мы делаем это, как мы делаем это, как мы
делаем это, как мы делаем это, как мы делаем это, как
мы делаем это, как мы делаем это.
Вот видно сейчас, что на экране написано или увеличить?
Увеличить, да?
Так, это мы уберем.
Вот давайте подумаем, как решать такую задачу.
То есть нам надо проанализировать сайт Stack Overflow и построить
гистограмму количества вопросов, ответов в зависимости
от возраста пользователя.
Но беда в том, что возраст пользователя у нас в одной
таблице, вот в этой маленькой users, а данные про посты
для этих пользователей в другой таблице, и нам
надо обязательно сделать join.
Вот давайте разбираться, как мы его будем делать.
На мапе что мы будем делать?
Ну первое, мы всегда чистим те данные, которые нам не нужны.
То есть сначала подумаем, а что нам вообще нужно?
Нам надо оставить PostTypeId, и в принципе все, то есть
ownerUserId и PostTypeId.
Что здесь будет?
А здесь будет IDH.
Вот, что делаем дальше на Reducer?
Тут решение есть, но оно с тегами, то есть мы с вами
можем придумать даже без тегов.
А здесь у нас нету IDH, что делать?
Заметь IDH, то есть мы это сделаем в Reducer.
А тогда как мы будем считать количество?
То есть нам придется как-то отдельно суммировать двойки,
отдельно суммировать единички, как-то это может проще сделать.
Давайте сделаем join, то есть у нас будет из двух таблиц
по две колонки, у нас будет одна таблица из нескольких
колонок.
Здесь, заметьте, мы теги не делаем.
Почему?
Потому что мы можем с помощью каких-нибудь проверок
отличить первую строку от второй строки.
Как это отличить?
Дело в том, что пост typeID бывает 1 или 2, а 2-летние дети
вряд ли сидят на строковерфлоу, h будет обычно больше.
Поэтому у нас будет такая строчка.
Пост typeID, он у нас тут или 1 или 2, и h.
Что нам осталось делать?
А сможем ли мы это посчитать в редьюсере?
Или надо что-то еще сделать?
Нам вообще нужен ownerUserID после того, как мы это склопнули
уже?
Да, по h агрегировать и здесь можно сразу вот эту структуру
немного изменить.
То есть, пост typeID 1 или 2, он нам не очень удобный.
Это можно сделать даже на мапере, будет еще быстрее.
То есть, вот здесь пост typeID мы превратим в такую структуру.
У нас будет булевское значение здесь и булевское значение
здесь.
Точнее, наоборот, 1, 0.
То есть, пост typeID, который 1 или 2, мы его превратили
в два флага.
И если у нас это вопрос, то тут будет, допустим, 0,
а здесь будет 1.
Если ответ, то наоборот, тут будет 1, тут будет 0.
Вот, а тут поэтому у нас получится такая структура.
Опять же, 0, 1.
1, 0.
А вот что делать, если к нам пришла строка с age?
То есть, id age.
Мы же ее не можем просто выкинуть, когда у нас нет ни
вопроса, ни ответа, есть id age.
Что тогда мы здесь сделаем?
0, 0.
То есть, тогда у нас получится тут или 1 или 0, тут или 1
или 0, а тут или age или, допустим, тоже 0.
То есть, если мы имеем дело со строчками вот такими
и мы хотим дописать сюда age, то будет age 0.
То есть наша задача с точки вот этого вида и вот этого
вида превратить в одну структуру.
А дальше мы суммируем.
Прямо здесь же на этом редьюсере мы можем просуммировать
для owner user id.
Вот так.
Вот до этого момента пока понятно или есть вопросы
и ничего не понятно?
А потому что как ты будешь суммировать отдельно двойки,
отдельно единички?
Когда у тебя есть одна колонка, там написано 1, 1, 1, 1,
потом 2, 2, 2, потом опять 1, 1.
Как ты будешь это считать?
Тебе все равно придется разносить это в два разных поля.
То есть отдельно считать количество двоек, отдельно
считать количество единиц.
А сделали ли мы сейчас все сразу?
Можем ли сказать, что это ответ?
Посмотрите еще раз, что спрашивается в условии.
Вроде, а, тут не видно.
Сейчас я поверну тогда выше.
Чего тут еще не хватает?
Структура похожая.
Мы можем выкинуть owner user id,
но нам все равно уже не надо,
и у нас останется три колонки, как мы и хотели.
Но все ли это или не все?
Ну а до сортировки?
Мы просуммировали, по какому ключу?
То есть какой у нас группой был сейчас?
По ключу owner user id.
Мы же по нему сгруппировали на стадии sort.
И по нему сджойнили.
То есть на редюсер у нас пришли пары с одинаковыми
owner user id.
Поэтому мы просуммировали вопросы и ответы
не для возраста, а для одного пользователя.
То есть это количество вопросов и ответов у одного пользователя.
Поэтому что нам еще надо сделать?
Просуммировать по возрасту.
А что это будет?
Может я неправильно понял,
но просуммировать возраст.
Это будет явно что-то не то.
Сгруппировать по возрасту.
То есть нам нужно былоığı пересчитывать.
Дело не было.
И если мы мы лечим вот это вот другое количество вопросов,
то мы и не сможем их сгруппировать.
То есть нам нужна еще одна джоба.
Опять map. На мапе мы оставляем h.
Вот эта сумма, ну давайте ее запишем быстрее.
Some answer, some question.
Вот такая вот у нас тройка.
Но в этих тройках у нас h пока еще может повторяться.
Поэтому на reducer мы делаем h.
Уже по h мы группируем.
Делаем sum от some answer и sum от some question.
Вот такая у нас штука получилась.
Но это только один из способов джойна.
Это такой самый лобовой джойн, который сложный работает.
Он конечно всегда, но сложно.
Есть вот такой джойн. Он называется map.join.
Когда одна из табличек маленькая, мы можем ее зачитать в distributed cache.
Что такое distributed cache, вам расскажут подробнее на семинарах.
То есть это такая область на нодах, которая не относится к HDFS.
Поэтому в нее быстрее писать, с нее быстрее читать.
И при этом она выделяется на всех нодах, где запускается задача.
То есть запустили задачу.
Если для нее нам нужен distributed cache, то на всех нодах,
где будут работать ее мапперы, reducers и комбайнеры,
везде будет создана вот эта папочка, и там будет лежать cache.
Обычно в этот cache кладут какой-то маленький файл.
И вот у нас есть возможность разбить только одну таблицу.
У нас будет джойн, кусочка большой таблицы со всей маленькой таблицей.
То есть по сути мы делаем параллельно маленькие джойны,
которые потом остается только объединить с помощью уже обычного юниона
и получить таблицу.
Вопрос всегда ли будет этот джойн работать.
Вот если мы возьмем четыре джойна, которые были вот тут,
двумя слайдами раньше,
для всех вот этих случаев нам джойн будет работать.
Если кто-то вдруг делал наперед домашку по хайву и читал документацию,
там прям явно документации по хайву написан ответ на вопрос.
Не все, иначе бы не спрашивал.
Давайте вы просто проверьте каждый из этих джойнов,
где будут какие-нибудь проблемы.
А почему с Индер будет проблема?
Ну и что, если у нас что-то не совпадает,
то у нас будет где-то нал.
В случае Энерджойна мы вот эти наловые пары отбрасываем.
Но у нас же эта же таблица скопирована и на втором кусочке, и на третьем кусочке.
То есть те пары, которые вот здесь вот тут вот не поместились,
отбросились, они обязательно будут здесь или здесь.
То есть Индерджойн тут как раз отработает,
потому что все, где нет соответствия, мы выбрасываем.
А почему Фонджейн не будет работать?
А можно более конкретно, что именно мы выбрасываем?
То есть да, то, что тут будет дублирование, это правильно.
А откуда оно возьмется?
А что ты имел в виду?
Про Фонджейна сейчас говорим.
Так, но ведь кусочки 1 и 2 они же друг друга не дублируют.
То есть синие кусочки это одна таблица.
Мы просто ее разбили на куски, но она от этого не перестала быть одной таблицей.
Дублирование будет немного не там.
Вот давайте представим, например, Left Join.
Вот как раз типичный, вообще самый часто используемый джойн в дата инжиниринге,
это именно Left Join, потому что мы берем какую-то таблицу и джоиним ее с каким-то маленьким справочником.
Когда у нас есть большой какой-то датасет,
а нам нужно выдать отчет по какому-то фильтру.
По фильтру времени, по фильтру компании, еще какому-то признаку.
Поэтому Left Join.
И как раз рассмотрим Left Join здесь.
Что у нас будет происходить?
Давайте даже на примере вам покажу.
На таком простом примере просто таблица с числами.
Весь у нас таблица.
Это у нас таблица B такая.
То есть она большая.
У нее два кусочка, три не будем рисовать.
И таблица, ну точнее это таблица A.
У нее два кусочка.
Таблица B.
И таблица A.
У нее два кусочка.
Таблица B.
Таблица B.
Вот, например, вот так.
Вот так можно сделать.
Сейчас я подберу числа, чтобы было понятнее.
Вот так можно сделать.
У нас будет дублироваться Left Join и Full Join.
У нас будет дублироваться Left Join и Full Join.
У нас будет дублироваться Left Join и Full Join.
У нас будет дублироваться Left Join и Full Join.
У нас будет дублироваться Left Join и Full Join.
У нас будет дублироваться Left Join и Full Join.
У нас будет дублироваться Left Join и Full Join.
У нас будет дублироваться Left Join и Full Join.
Иường на ^^
У нас будет дублироваться Left Join и Full Join.
У нас будет дублироваться Left Join и Full Join.
У нас будет дублироваться Left Join и Full Join.
Сейчас проверю точно.
Вот даже здесь мы видим дублирование, что у нас дублировались вот эти вот строчки.
Ещё бывает такое, что дублируются нулы. Вот это даже бывает чаще, что в первом кусочке есть
нал, а во втором как раз мы находим соответствие.
Дублирование хорошо, мы с ним можем справиться, но мы не можем справиться с таким
кейсом, когда у нас одна строчка выдает нал, а другая с таким же
ключом она не выдает нал. Сейчас попробую такой пример сделать.
Здесь, наверное, на этом примере будет проще всего показать на райджойне.
Вот если мы хотим райджойн сделать, смотрите. 1, 1, 1, 4, 1, nal с первым кусочком.
Второй кусочек. 1, 1, 1, 4, 1, 1. То есть в первом случае у нас четвёрка не нашлась,
а во втором случае у нас четвёрка нашлась. А, ну здесь, кстати, я ошибся. Вот так.
То есть смотрите, у нас получаются строчки. Часть ключей встречается с налом,
часть ключей встречается без нала. И чтобы нам завершить джойн, нам надо вот этот весь
результат пройти ещё раз, выкинуть те строчки, которые встречаются с налом, в то время как с этим
же ключом мы встречаемся без нала. То есть вот эти вот 4, 1, nal должны пропасть, 1, 1, nal должна
пропасть. Но это не значит, что все налы нужно удалить, потому что, например, если мы добавим
сюда 5, 1, то 5, 1 нет нигде. Будет 5, 1, nal, и здесь будет 5, 1, nal. И вот этот 5, 1, nal, он должен остаться,
потому что это честный нал, он действительно нал, он нигде не встречается. Вот, а вот от этих, скажем так,
нечестных налов, которые встречаются, но в другом кусочке, мы должны избавиться. А это, ну это редьюсер,
по сути. То есть нам надо вот этот весь датасет сгруппировать ещё раз по ключу, и для каждого
ключа проверить, если там не нал, если там есть не нал, то мы его только и оставляем. Если там есть
только налы, то мы берём один нал, одну запись с налом, все остальные выбрасываем. Поэтому, да, вот в
данном случае, write join не будет работать. Тут зависит от того, как поставить таблицы, но один из
джойнов у нас не работает. И последний кейс джойнов, это такой средний случай между map join
и reduce join, это bucket join. Это когда у нас табличка, вот эта красная, табличка B, она не настолько большая,
чтобы её класть в HDFS, но и не настолько маленькая, чтобы она поместилась в distributed cache. Нам
её приходится разбивать на части, и каждую часть класть в свой distributed cache. Скажите тогда,
какой джойн здесь не будет работать? Вот, учитывая то, что мы сейчас разбирались с дублирующимися вот
этими налами,
а еще, а еще, все никакой джойн не будет работать, поэтому вопрос, зачем нам вообще такая штука нужна?
Такая штука нужна нам для того, если мы имеем возможность заранее вот эти вот кусочки разбить,
то есть не просто как это делает HDFS, разбить по объему, по каким-то частям, а если мы имеем
возможность заранее подготовить данные так, чтобы, например, вот первый кусочек, он соответствовал
интервалу от одного до A, второй кусочек от A до B, третий кусочек там от B до C. Если мы соблюдаем
эти интервалы, то есть они совпадают и в таблице A, и в таблице B, то тогда все будет работать. Ну,
то есть, по сути, нам нужно заранее сделать предобработку. Ну что, мы в принципе закончили,
но можем разобрать еще yarn, чтобы не оставлять время на следующем занятии, у нас будет хайф. Если
у вас есть еще минут семь, можем еще поговорить про yarn. Давайте, то есть что вообще такое yarn?
Это yeti-NAS-ресурс негашейтер, то есть еще один распределитель ресурсов. Собственно,
он распределяет ресурсы, выделяет их на маперы, на редьюсеры, на комбайнеры, на всю эту штуку. Как
он это делает? Он взаимодействует не напрямую с оперативкой, с ядрами процессора, а он оперирует
такой абстракцией, как контейнер. Только это не docker-контейнеры, не виртуальные машины, а контейнер,
он представляет собой какой-то набор выделенных ресурсов, например, два гига оперативки и одно
ядро процессора. На этом контейнере у нас стартует виртуальная машина джавы, стартует процесс
какого-нибудь мапера, и вот в нем мы живем. Вот это самая простая реализация такого планировщика.
Очередь. Пришло приложение, кластер свободен, оно заняло все ресурсы. Пришло второе приложение,
ресурсов нету, оно ждет. Какие плюсы и минусы у такого подхода? Ну плюс понятно, а просто сделать,
то есть обычная очередь. А минус в том, что у разных программ разные приоритеты, разные требования. То есть
может быть application номер два, она супер срочная, да еще и очень маленькая, и она бы быстро отработала,
если бы вот этот ап один жирный не занял бы кучу ресурсов. Поэтому второй тип планировщика,
когда мы на каждое приложение выделяем capacity. Заранее в конфиге прописываем, и получается,
что вот у приложения один capacity 75%. Кластер свободен, допустим, даже ресурсы есть, но он все
равно больше чем 75 никогда не займет. Точно так же, как второе. Вот у него 25. Все, как бы оно не
хотело, оно больше никогда не займет. Какие здесь минусы вы видите у такого планировщика?
Да, ресурс простаивает. То есть такой планировщик, он хорошо подходит для real-time приложений.
Вот когда у вас, мы в принципе в этом курсе почти не будем затрагивать такие приложения,
может быть на семинаре вам расскажут чуть-чуть. Это real-time приложение, когда есть, например,
какой-то сервис на Spark, он слушает порт, и туда постоянно прилетают какие-то данные. В принципе,
с одинаковой периодичностью, с одинаковой нагрузкой, и мы знаем, что в принципе такое приложение
всегда будет требовать примерно одинакового количества ресурсов. Мы на него запланировали
capacity, и оно себе вот как вот на этой желтой части диаграммы, оно будет постоянно эти ресурсы занимать.
Но с другой стороны, даже у таких приложений бывают перепады в том смысле, что вот есть map reduce.
Например, наша программа так написана, что мапперам надо много, потому что много сплитов,
много данных прилетает, но на мапперах мы много чего фильтруем по какому-нибудь критерию,
и к reduce приходит очень мало данных, и у нас остается один reducer, и получается нам надо то очень
много, и мы ждем, то очень мало, и мы простаиваем. Поэтому такой планировщик, он тоже не сильно
подходит, ну иногда не сильно подходит. И третий, самый такой продвинутый, называется честный
планировщик. Но здесь все честно, если ресурсы свободны, то почему бы нам их не дать? Если их
кто-то другой хочет занять, то мы в зависимости от приоритета, вот здесь пополам, application 2
отжирает половину, но в зависимости от приоритета, от настроек, оно может съесть 25%, 10% или 80%.
Что на этой картинке вас смущает? Ну зорчик это то, что приложение начало работать не сразу. А что еще?
Там учитывается два параметра, сколько нужно реально ресурсов, и какой приоритет.
То есть то, каким образом планировщик принимает решение, что нужно отдать 50%, да?
Может, а почему оно должно меняться ступеньками? Ну кроме того, что так написано на следующем слайде,
почему оно еще должно меняться ступеньками? А если ему все время нужно много ресурсов,
тогда вот типа все нормально или какие-то проблемы все-таки есть? В зависимости от приоритета,
если приоритет равный, то пополам, если у кого-то больше, то там уже умножаем на приоритет. Здесь
главная проблема не в том, что сколько ресурсов мы даем, а как быстро мы даем. То есть вот смотрите,
Ярн здесь выдал ресурсы резко, пришло второе приложение, сказала мне 50, вот тебе 50. Что
произошло с вот этими джебами, которые здесь работали, вот в этой части? Их убили. То есть получается,
что вот тут что-то работало, оно доработало до 99,9%, потом их взяли и убили. Если какая-то
польза от того, что здесь вообще работало, пользы никакой нет, потому что раз оно не доработало,
его килинули, значит ходуб его перезапустит еще раз с нуля. Поэтому вот эта вот желтая часть,
она по большей части оказалась бесполезной, такая уж какая-то белая. Поэтому в реальной жизни
ресурсы выделяются постепенно. Вот как вы сказали ступеньками, то есть мы ресурсы не отбираем,
мы их просто не выдаем. Пока приложение работает, оно себе работает. Как только оно начинает,
какой-то маппер завершился, в норме если ресурсы есть, то сразу на его место становится следующий
маппер или редьюсер. А здесь мы не будем выдавать ресурсы, мы просто отберем их в пользу АП2. Иногда
можем с помощью преэппшена отобрать наперед. Даже если приложению 2 сейчас столько не нужно,
мы их отобрали наперед и начинаем их как бы осваивать, занимать их нашими джобами.
Но это еще не все. Все, что осталось сказать, что контейнер это тоже штука достаточно динамичная.
То есть то, что мы взяли, выделили контейнер, два гига оперативки и одно ядро процессора,
такого контейнера может просто не хватить по каким-то причинам. Вот у нас маппер там сильно
разросся. Чаще всего в Spark такое бывает, потому что в Hadoop'е с этим проще. Не хватило памяти,
мы скипклили все на диск. В Spark с этим сложнее, потому что там немного не так построена логика работы
с диском. Там все максимально хранится в памяти. Если памяти не хватает, то часто бывает out of
memory просто. Поэтому контейнеры в Ярне задаются не одной парой значений, а у них три пары. У них
minimum, maximum и step. То есть в начале мы выделили минимальный контейнер. Дальше в Ярне есть две
роли. Есть ресурс-менеджер и нод-менеджер. То есть ресурс-менеджер это глобальная роль,
которая решает сколько контейнеров кому выделить. Нод-менеджер решает сколько контейнеров выделить
в рамках одной ноды и стоит ли растить какой-то контейнер. То есть мы можем, если наша джоба просит
ресурсы, мы можем удовлетворить ее просьбой двумя способами. Дать больше контейнеров или вырастить
те контейнеры, которые есть. А растятся они вот так. То есть есть минимум, например два гига и одно
ядро. И есть step, например там пол гига, но и тоже одно ядро, потому что мы не можем ядра пополам
выдавать. И вот если мы хотим наш контейнер вырастить, мы его начинаем на вот этот step растить,
растить, растить, растить, пока он не достигнет максимума. Если он достиг максимума, это обычно
какое-то там большое значение вроде десяти ядер и тридцати гигов. Если он достигло максимума и
хочет еще, то мы его удаляем. Тоже часто с таким столкнетесь в спарке, когда будете писать код.
Что вот джоба работает, работает, работает, потом написано not enough physical memory. Где это имеется
в виду physical memory? Не на всем же кластере. В одном контейнере не хватило памяти, мы его кельнули и
перезапустили еще раз. И скорее всего повторится та же история, потому что вина в этом не контейнеры,
а вашего кода. И так он будет по циклу перезапускаться несколько раз. Ну и сами контейнеры,
они тоже бывают разные. То есть вот этот минимум, он не для всех контейнеров одинаковый, а, например,
можно указать, что для мапперов у нас будут одни контейнеры, например, один-один, один гига
для редьюсера будет полтора один. Ну и в последних версиях Hadoop он еще умеет работать с видеокартами,
поэтому туда еще добавляется третье значение, еще и видюхами мы управляем.
Ну в принципе, на этом все. В конце, на последнем слайде можете посмотреть
tutorial, как писать Джобы на Hadoop стриминге. Здесь очень подробное объяснение,
несмотря на то, что статья древняя ей 10 лет и там второй питон, но объяснение там
очень подробно написано что и зачем. Вот на этом все, у нас с вами осталось еще одно занятие,
оно будет по хайву, а дальше будет читать роман Леповсту. Контрольное будет,
скорее всего, в следующий раз, но мы еще с семинаристами обсудим или в следующий раз или через раз.
Все, всем спасибо.

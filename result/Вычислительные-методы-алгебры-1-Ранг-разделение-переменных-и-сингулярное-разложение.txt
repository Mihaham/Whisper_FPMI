Значит, вот наша функция от I и V, элемент этой самой матрицы, которая нас интересует,
она есть сумма разложимых функций, и число этих разложимых функций равно 2023.
Что можно сказать о ранге нашей матрицы? Уже довольно много подсказок было сделано,
в этом некоторые лихости, но смысла мало. Лихость есть, а умности нет.
Нет, это вы не чувствуете, понимаете, вот то, что вы говорите, вы совершенно не видите ситуацию.
Причем здесь разделение примерно? Ну ладно, давайте дальше продвигаться.
Вы определительно считаете второго порядка, так я понимаю.
Давайте я запишу вот такой вектор столбец, укатая от 1, укатая от 2 и так далее, укатая от 0.
Такой вектор столбец умножим на строчку.
Из-за вида элементов матрицы следует, что это матрица, есть произведение матрицы столбца.
Если вы вспомните какое явление ранга, наверное вам говорили, что ранг это наивысший порядок,
а матрица не нулевается.
Значит смотрите, что же получается, а наша матрица есть сумма, вот таких вот матриц.
Значит наша сумма, есть сумма, наша матрица, это сумма 2023, матрица ранга единица.
Значит ранг суммы матрицы не превосходит суммы ранга.
Те, кто понимают существо понятия ранга, сразу увидят вот эту формулу, должны бы рассуждать таким образом.
Значит можно функцию представить в виде суммы разложенных функций, число этих разложенных функций будет оценкой ранга.
Потому что каждая разложенная функция соответствует матрице ранга 1, а сумма матрицы ранга 1, ранг ее не больше чем число разложенных.
И еще у нас получается оценка на ранг.
И ответ, который я, собственно, от вас ожидал, вот опять это обучение.
Короче говоря, если кто-то решит проблему то с кем?
Автомат.
Вот у вас смотрите какая матрица.
Давайте я так вот напишу, альфа 1, альфа 2, числа.
Тут понимаете какое-то химическое средство, нужно какое-то решение, с этими инструментами тяжело.
Вот матрица, почему равен юра?
Ну давайте возьмем любой определитель.
Значит это будет альфа-катая на бета-катая, альфа-катая на бета-эль, альфа-эль на бета-катая и альфа-эль на бета-эль.
Ну посчитайте, почему равен?
Нулю, да?
Да.
Нулю, значит ранг не больше двух.
Но если есть хотя бы один, то он равен.
Если у него не до 2 порядка, то он равен.
Что и выше порядка не больше двух.
Вот, но вот сам вопрос означает, вернее подтверждает, уже складывающееся мнение о том,
что линейную алгебру изучили поверхностно.
К сожалению.
Понятие ранга, оно на самом-то деле одно из ключевых, ну как понятие размерности линейного пространства,
понятие ранга бахлится в линейных, выскученных понятиях.
И вот, что самое важное, что компания должна знать об этом понятии,
это то, что оно связано с идеей представления функций двух переменных в виде суммы отложенных функций.
Ровно то, что мы наблюдали в этом примере.
Ну мы оценку с вами получили.
Ну вот, оказать, что я все не придумал, можете поразмышлять.
Я тут, как гипотеза, думаю, что если большое больше чем 2023, то он будет с точности равен 2023.
Но, честно говоря, доказывать это я даже не буду.
Думаю, что это так, но моя-то задача сейчас обратить на связь понятия ранга.
На понятие об разложимой функции, с возможностью представлять функции от двух переменных в виде суммы,
в разложимых в виде конечных сумм.
Значит, ну давайте я теорему сформулировал, а то, которую вы должны бы знать.
Ну, все-таки да, непонятно, сноровка.
Не думал, что это такая тяжелая работа.
Значит, это теорема из помощного вашего.
Значит, ранг матрицы равен наименьшему числу матриц ранга 1,
который в сумме дают в матрице.
Другими словами, если мы посмотрим на элемент матрицы как на функцию индексов иежи,
то наименьшее число разложимых функций, которые в сумме дают в данную функцию, будет ранга.
Вот такая теория.
Я предлагаю задачу, эту теорему обдумать самостоятельно.
Вы ее знали. Вы должны были ее знать.
Если не знаете, то вы ее должны были знать.
Ну, если вам не сказали, значит, вы очень хорошо учили.
Вот это главное, что надо знать по ранг матрице.
То, чего вы знаете, возможно, это тоже полезно знать.
Но это очень существенная вещь.
Связанная с идеей разделения переменных при изучении функций от нескольких переменных.
Ну, наверное, вы уже имеете некое ощущение,
возникает ощущение, что вот эта идея – это разделение переменных, идея – это богатых.
Не знаю, вы в равнении математической физики еще не изучали, наверное.
В следующем семестре.
А там вы увидите, что методы построения решений,
в тех случаях, когда можно аналитически это сделать,
они почти начинаются все с использованием идеи разделения переменных.
И, в общем, не так далеко от нее и уходит.
Это мощнейшая идея.
Мощнейшая идея не тут.
Но в реальной области, оказывается, у вас эта идея познакомая,
но вы не знали, что это такое.
В матрице.
Ну, естественно, вычислительные линейные…
Ну, конечно, объект основной – это матрица.
Матрица или систему линейных алгоритмических уравнений
или какие-то задачи, связанные с матрицей,
задачи типа переменчивых квадратов.
Иногда интересуется, вот, пристучение матрицы, что-то еще.
Ну, нормально.
Вот, применяется, конечно, вычислительная линейная алгоритма в жизни много,
очень много.
Даже, вы знаете, есть ситуации совершенно неожиданные,
где применяется линейная алгоритма.
Совершенно неожиданные.
Даже, вот, если вы подходите…
Я не знаю, раньше можно подходить к некому матрицу,
получать…
Вот.
Или, вот, допустим, ну, наверняка вы со своего компьютера
заходите на какой-то далекий компьютер.
Ну, естественно, проходит.
Говорит, передается в зашифрованном виде.
Вот здесь, оказывается, место линейной, место алгебре,
безусловно, есть линейное место,
весьма существенное место.
Это протокол кшифрования.
Вот.
Еще одна ситуация, где уж точно теория матриц,
и то, именно, почти то, что очень близко к тому,
что мы сейчас тоже с вами все ждали,
начали все ждать.
Вот вы…
А это уж каждый делал, да?
Хотите что-то узнать?
Мы набрали какие-то слова поисковики,
и он дал кучу.
Все расскажет.
Нас куча документов, ссылок.
И в этот момент
происходит применение
интересной линейной,
существенной линейной,
связанной с понятием…
Ну, не только это.
То есть, не надо буквально…
Это какая-то очень серьезная задача,
что, надо сказать, решается.
Но есть и фундаментальный метод,
который позволяет эту задачу
тоже достаточно хорошо
решать методы с наук
в некоторой модели
связанной с понятием…
Ну, и, конечно, ситуации
такие многообразные,
когда море каких-то измерений…
Ну, вы понимаете, вот жуткое вещь,
какие-то измерения…
Что с ними делать?
Каждую секунду что-то меряется.
И вот эти данные.
Это где угодно, в физике.
В химии, в геологии.
Вот эта куча данных.
А законы еще пошел.
Ну, в общем, понятно,
как-то их надо откуда-то…
То есть, чего-то в этих данных
фундаментальных законах.
И что?
Руки никто не поднимает.
С помощью методов, которые
в этих данных на лизе.
Вот здесь.
Для понятия ранга матрицы
или для вот этого…
Именно для…
Для идеи представления матрицы
в виде суммы матрицы ранга 1.
Вот здесь.
Такое просторное употребление
этих понятий.
Ну, а…
Более точно.
Более точно.
Вот представьте себе,
что есть огромная матрица.
Да.
Громная матрица.
Да, как-то мне надо еще…
Тут, вовремя рисуем.
Это данные каких-то измерений.
Вот.
Ну, что мы понимаем?
При измерениях всегда есть
и погрешность, и есть еще
то, что называют шум.
То, что там неважно,
но там присутствует.
Значит, наша задача такая.
Вытащить
существенную информацию
и, по возможности,
убрать шум.
Ну, естественно, для этого
нужна какая-то модель
для интересующих нас дам.
Вот, оказывается,
мир устроен удивительным образом.
У меня объяснения такого
готового нет.
Но это факт, тем не менее.
И объяснения, безусловно,
существуют.
Тут их изучение, кстати,
тоже есть.
Одна из проблем.
А что за факт медицинский?
Вот оказывается,
что надо эту матрицу записать
в виде Б,
но я напишу про шум.
А ей это шум.
Погрешность.
А вот эта матрица Б,
как правило,
это матрица небольшого ранга.
То есть, вот она,
приорная информация,
вот это качество.
Маленькая, единица, два,
в общем, пять, десять.
Размер матрицы огромный.
И вот эта вот модель.
Модель представления.
Значит, на самом деле,
вот эта матрица,
это есть матрица небольшого ранга
или малого ранга,
плюс что-то,
что надо выбросить.
Ну и с точки зрения математики,
здесь возникает
ясная математическая задача.
Вроде бы ясная.
Сейчас не разберемся с ясностью.
Для заданной матрицы
найти приближение.
А именно, матрицу ранга,
которая не выше заданного числа.
Ну и в каком-то смысле,
скажем так,
давайте искать бы и лучше.
В этот смысл.
Вот задача.
Вот задача.
На множестве матриц
с границей ранга.
И задача вот чрезвычайно важна.
И чрезвычайно распространенная.
Ну, вот те, кто
заинтересуется анализом данных,
а это уже, я не знаю, сейчас у Масова сходится.
Может, такая же новая деятельность,
но
не для этой деятельности.
Не для этой деятельности.
Ну, я думаю.
Значит, вот это.
А как такое задачу,
что об этом известно?
Что-нибудь вам известно из курса
линейного, который вы прослушали?
Синабулярное отношение?
Да.
Все-таки, молодцы, что-то было.
Все-таки, что-то было.
Вот.
Один из основных инструментов
теоретических
и в определенной степени
и практических тоже.
Это
симулярное разложение матриц.
Прозвучало пойду.
Давайте запишем это вот.
У вас было это в курсе, да?
Да.
Хорошо об этом помните.
Но, тем не менее,
это настолько важно,
что напомнить необходимо.
И мы сейчас вот этим займемся.
Ну,
симулярное разложение матрицы.
Что это такое?
А что это такое?
И как это
симулярное разложение матрицы
задачей приближения матрицы
матрицы
малого ранга.
Это то, что я не знаю.
Это существенная вещь.
Если вы себя с этой задачей
не видите, то зачем вам симулярное?
Ну, подождите.
Подождите.
Давайте начнем потихонечку.
Вот когда вы только начинали
изучать,
что можно сказать?
Что мы с вами знаем?
Любая матрица
может быть записана
как сумма матрицы ранга 1.
Любая матрица
минимальное число
матрицы ранга 1.
Что разное?
Ранга 1.
Вот это.
Ну, давайте я
напишу,
пока это еще не симулярное.
Так, может быть,
как
был другой какой-нибудь
пиндекс использован.
Почему бы
пояснились?
Ну,
Р.
Р это ранга матрицы, да?
И тогда, а что я здесь
суммирую? Вот вектор.
Пальша.
Вектор.
Обозначение понятное.
Вектор столбец.
Теперь, вот так.
Давайте вспомним немножко
как это можно записать.
Это можно записать вот так.
Матрица У
играет на матрицу В
транспонированную.
Где матрица У, это матрица
составленная из столбцов
В1, В2,
В3.
А матрица В
матрица составлена из столбцов
В.
Видите?
Все видят.
Ну, здесь надо вспомнить
как умножается матрица.
Может,
надеюсь, это может быть.
Да?
Вот.
Ну, иногда говорят, что
вот это есть скелет.
Ну, это еще не есть
сингулятор.
Это вот скелет.
То есть, можно смотреть,
как и на ранг смотреть.
Это наименьшее число
столбцов матрицы У
и В тоже.
Такое, что
А можно записать в виде У
на матрице В.
Вот это наименьшее число.
Это тоже есть рамка матрицы У.
Вот.
Это тоже есть рамка матрицы.
Ну, потому что если вы написали
такое разложение,
дальше вы и такое разложение пишете.
Сумма матрицы В.
Сумма матрицы В.
Вот.
А вот
как подойти теперь к сингулярному разложению?
А сингулятор
вот такая.
Давайте вот эти
столбцы У и В
потребуем, чтобы они были
ортонормированы.
А вот этой текст
я прошу тебя, значит.
Артагональными
и
единичной длины.
Ну, надо вспомнить
немножко определений,
которое
будет необходимо
сделать. Ну, вот
такой выход
принято обозначать
исчезнуть через В.
Я привык
это еще.
Не буду обозначать.
Вот такой выход принято обозначать.
Комплекс.
Вот. Теперь
вот это что такое?
Это множество
столбцов.
Лекторов С.
Ну, мы будем обычно
делать что-то с столбцами.
Ну, может быть что-то
иногда я уже это засчитаю физически.
Сейчас у нас часто будет
столбцы.
Значит, как-то элементы
где-то интересные. А вот если артагональными
то это элементы комплекс.
Ладно.
Теперь вот давайте я возьму
два столбца.
Х
Х
С.
Что мы можем понимать
по столярной прогрессии?
Вот.
Вот число.
Пятьдесят
два.
Пятьдесят попрыжок.
Вот здесь
число элементов
этих лекторов.
Столько.
Сколько элементов
у этих лекторов.
Значит, вот это есть
часто говорят естественное
столярное произведение
нервных лекторов.
Комплекс.
Вещество то же самое, но для вещества
вот этот же столк,
который столбцов теряет.
Вот здесь
склярное произведение.
Какие векторы называются артагональными?
Склярное произведение, которое
вновь.
Ну и система.
Что такое артагональная система
векторов? Вот система векторов, которые любые
два вектора
артагональными.
Вот.
Теперь, если есть склярное произведение
можно купить длину.
Длину
давайте вот так вот.
Длина это что такое?
Полная квадрата.
Склярное.
Длина. Сейчас вы решите
их длину.
Вы понимаете, что есть склярное
произведение, и это склярное произведение
длину.
А это корень комплексного числа может быть?
А вот посмотрите, что такое
если у и х, какое число полной квадраты?
А, действительно.
Потому что
тишинная квадрата
это модуль квадрата.
Это будет сумма квадратов
в модуле.
Вот.
Значит, вот некая напоминающая.
Но, вероятно, вы знакомы
и с некоторыми обобщениями
вероятно, вы знаете, что склярное
произведение можно вести
и на абстрактный уровень.
С помощью аксиона.
Да.
Ну и соответствующую
искренному длину
тоже можно в другую.
Видите, что я говорю?
Вот это
конкретно вот такое
склярное произведение
всегда будет в естестве.
Ну точно.
Но
иногда важно
использовать некоторые
неестественные.
Иногда естественные используете
неестественные произведения.
Есть такие ситуации.
Не надумаю.
Значит, еще
есть
еще
вообще есть понятие нормы.
Знакомостные или нет?
Вот в векторном пространстве
или в интернете
можно
векторам приписать
некоторое не отрицательное
вещественное число.
И в веке
это понятие тоже
с помощью аксиона.
Вот.
И бывает так.
Вот если мы рассмотрим
вот эту длину, то это пример был.
Но бывает нормы, которые
не пораждаются
неразборчиво.
Ну не знаю.
Примерно.
Что-то поразбышлять.
Вот это
некоторое
напоминание.
Нам, кстати,
такие нормы тоже будут нужны.
Да?
А вот если бы я возьму матовую
матовую зуба
вот можно так писать.
Вот тем
отверткой.
Вот.
И еще одну матовую зубу.
У нас здесь возникает
линейное каскадство матрицы
с комплексными элементами
размером матрицы фиксированной
м на п.
Это линейное каскадство.
И в этом пространстве тоже
можно ввести скалятное произведение.
И в некотором смысле, естественно,
скалятное произведение.
Ну как это сделать?
Я думаю, вот так.
Ну, пускай
есть своя жизнь
а вы
ездите на матрицу
ну, тогда
скалятное произведение
наверху у нас будет
как сумма
вот такая. И по всем
видишь.
Значит, вы видите, что есть основание
почитать это
произведение естественно.
Вот, ну, можно подумать
о каких-то
какого
записи
это я так могу
писать, что я могу.
А!
В прошлом видео
запрещен
эта сумма диагональна для меня.
Проверь!
В порту!
В порту!
В порту!
В порту!
И вот это скалятное произведение
для множества матриц
порождает длину.
Длина эта называется
ну, понятно, что это можно прогреть.
Хотя
в русских книжках
называли
ту же самую норму и в Крит.
Ну, и в Крит по норму
наверняка ничего не знал.
А в Западной
чертуре говорят, что это можно прогреть.
Ну, как это положено
поделим вот так.
Если написать норму
вот так вот
то есть вот так можно
вычислять
обмениваться
в норму матрицы.
Это норма порожденная, я тут
скажу.
Но!
Да и ведь!
Ну, вот давайте
вспомним задачу.
Ничего зря мы сейчас
делаем.
Задача какая?
Для западной матрицы
найти матрицу малого ранга,
которая приближается к этой матрице.
При лучшем области.
А что значит при лучшем?
Давайте напишем норму матрицы.
Вспомним, что она
имеет возможность
для всевозможных матриц B
ранга по параллельному участнику.
Вот, что значит постановка задачи
стала четкой.
А что означает матрица
приближается к матрице?
Ну, хорошо. А что значит функции
приближается?
То есть мы хотим
матрицу A
заменить на матрицу B.
И наш интересует
норму матрицы A-B.
То есть норма,
как B отличается от A,
вот эта норма отличается от B.
То есть вам можно сказать, что
любой объект приближает любой объект,
но с какой погрешности?
Вот надо просто стояние
между этими объектами уметь измерить.
То есть приближение
необязательно хорошее приближается.
Когда мы интересуемся,
насколько близки
у объекта,
вполне удобно
и оценивать погрешность.
Вот есть еще одно
попоминание.
Вы мне скажете, было это у вас
или не было?
Хочется верить, что что было.
Вот одна норма есть.
Но для матриц
очень популярны
некоторые другие нормы.
Вот есть
норма, которая
вот здесь
касается девочек.
Норма называется спектраль.
Почему она так называется?
Естественно,
смех связан с тем,
что возникла порция черного призыва.
Объясним.
Возвращение
ну что
вот только
не прямо там.
Связь есть суть.
А сейчас вообще
я поделяю,
является это вот так.
Пишется страшное слово Украина.
Вот, а здесь исходится
ивкрида-взрыв.
Вот, и говорят,
что вот эта норма
поражена
ну двоечка знаете почему?
Потому что ивкрида-взрыв
тоже часто так обозначают.
Есть такие гейдеровые норы.
Вот, два нормы гейдера
это как раз та самая
ивкрида-взрыв.
Ну а много интерес нет
почему это можно было
в Украине.
Значит, вот такое определилось.
Встречалось?
Встречалось.
Вот, ну как бы предмет
и алгебры, и тленения,
и вода.
Вот с Украиной могут
не достигаться.
А в данном случае
не придают не достигаться.
В смысле какого-то
можно, кстати, это определить
и написать.
Значит, можно написать
миграми, да?
По сколько достигается.
Максимум может быть.
Вот правильно, максимум.
Максимум может быть.
А можно написать это
максимум
по всем
иксам единичный диагонал.
А здесь
без всякого знаменателя
просто нет.
Ну это вот видите, что это
этот пример.
Значит, вот
для нас наиболее
интересные нормы
вот эти две
в случае матрицы.
Вот норма фобениуса,
которая порождена скалянно,
и вот эта норма,
которая порождена
объектом.
Часто говорят, что это
оператор.
То есть оператор наиболее
и фобениус.
Вот две нормы.
Вот если, если вы
любите решать интересные задачи,
вот вам задача.
А не что удовольствие.
Ну он так
не самое простое,
но решаемо.
Покажите, что
фобениусовая норма
не является
матрицей.
Ну в каком смысле?
То есть никакие вы не можете
заплатить в некоторое время.
В некоторое время
можно заменить какие-то
конкретные нормы.
Думаю параметры.
Вот оказывается,
если вы имеете дело
с нормой фобениуса
матрицы,
то вы не можете.
Вот эта формовость
не имеет места.
Не имеет места.
Ну если
совсем просто
вы не имеете.
Дескополья такая
изощрённая задача.
Не при каком выйти.
Но нельзя получить
такую реакцию.
Ну ладно, это так.
Так.
Итак, вот
спектральная норма
матрицы
будет объяснить.
Спектральная
норма фобениуса.
Ну а теперь
давайте вернёмся
к симулятору отложиться.
Что это такое?
Что это такое?
Вот.
Вот
есть у нас уже
скелетное разложение.
Станкулярное разложение
это тоже некоторое скелетное разложение.
Только с дополнительной силой.
Что это за свойство?
А вот
главное, чтобы вот эти векторы были
артагональны.
Чтобы вот эти векторы у1 уr
были артагональны.
И векторы у1 вr
были артагональны.
Две артагональных систем.
Дальше можно, то есть вот
теорема.
Существует такое скелетное разложение
в теореме.
Очевидно.
Следующаяimy.
Существует такое скелетное разложение
в котором система у
и система...
Значит, существует
скелетное разложение
в котором система
и система у
и система у
ст Der emple Kazakhstan
Это есть сингулярное?
Почти.
Почти.
Одну только вещь надо сделать.
Ну давайте я тебя втрощу и нормирую.
Если мы их нормируем, то в этой сумме надо написать коэффициент.
Сейчас я пишу, что такое сингулярное разложение.
Здесь я пишу коэффициент.
Здесь это бывает.
Значит вот условия.
О, один коэффициент.
Артонормированная система.
Второе условие.
Система В1.
Р.
Артонормированная тоже.
Относительно естественно.
И третье условие.
Поскольку сигма это нормировочный множитель.
Сигма альфа.
Ну вот на самом деле нулевые можно выбросить.
Поэтому можно всегда считать.
Ну иногда и нулевые.
Что?
Сигмаль.
Чисто.
Вот это есть сингулярное.
Вот это есть сингулярное разложение.
А число.
Значит если сигма альфа положительная и все.
То значит означает, что Р есть ранг матрицы.
И это есть сингулярное разложение матрицы.
Ну не знаю, видели вы в таком виде сингулярное разложение матрицы или нет.
Смотрите.
В сингулярном разложении есть несколько ликов.
Один из них вот такой.
А какой еще?
Давайте.
Давайте.
Давайте.
Ну правда принято это писать.
В комплексном случае здесь принято писать конечно вот так.
Нарто-гонально, на диагонально, на арто-гонально.
Значит правильно.
И вот ту самую матрицу, которая у нас была.
А здесь вести матрицу вот такую.
Ну давайте я напишу слово.
Сигма.
А здесь я напишу E.
Здесь напишу сигма R.
Здесь напишу вот R.
А сигма R, что это такое?
Сигма R.
Это будет диагональный матрица.
Вот такие числа.
Так.
То есть вот один лик сингулярного разложения.
Вот еще один лик.
А есть еще.
А теперь смотрите.
Вот давайте вот эти векторы.
Вот эти векторы.
Вот эти векторы.
Настрой.
Дворка нормирована на оба.
Ну сколько?
Кстати, сейчас я смотрю.
И вообще-то.
Начал писать.
Ну ладно.
Пускай буду пока так писать.
Иногда часто возникает желание
в боевые места.
Ну ладно.
Как написал, так обыскать.
Значит, сколько там векторов фута должно быть?
Думаю, наверное.
Размер матрицы.
Нет.
Ремга.
Вот чтобы это была ортонормированная система.
Вот это будет матрица.
Ну понятно, что
У R есть ее чарт.
Понятно?
Ну здесь, ну давайте ладно.
Ну что ж я.
Давайте я тут напишу.
Уздор.
То, что
содержат те столбцы,
которые дополнили
имеющуюся систему.
Ну вот, первый столбцов
это Уя.
А у струбников, ну остались вот эти вот столбцы.
Уя.
А вот эти вот.
Вот теперь матрицу В.
И тоже достроим
за ортонормированную организацию.
Вот здесь.
Вот тоже можно это написать.
В Р.
В.
Ну и теперь я ж хочу
вот вместо той столбцов,
которая есть, по бокам уметь матрицу.
Уй В.
Уй В.
Ну смотри.
Чтобы было понятно.
Здесь матрица
Сигма.
Здесь Сигма В.
А здесь В.
Ну и здесь у нас.
А размеры матрицы У и В
получаются раз.
Ну поскольку матрица в общем случае прямоугольная.
Вот эта формула проверяется
в лоб.
Это проверяем.
Ну смотрите.
Вот мы
умножаем
вот эту блочную строку
на блочную матрицу.
То есть на блочную столбцов.
При этом
вот эта уздомика
вообще никакого значения
не имеет для получения результата.
То сколько элементов
уздомика будет умножаться.
Вот и бездомика
тоже на результат никак не умеет.
В результате действительно получается
видно.
Вот три лика сингулярного разложения матрицы.
Которые надо в голове
иметь.
И все три полезны.
Первый,
второй и третий.
Третий содержит
избыточную информацию.
Поскольку уздомиков
для того чтобы
восстановить матрицу не нужны.
Но тем не менее
какая польза от матрицы
матрицу квадратной
ее стал цепкой.
Как такие матрицы
называются?
Артагональные.
Унитарные.
Унитарные.
Молодцы.
Вот здесь вот эти матрицы умые
это унитарные матрицы.
Молодцы.
Молодцы.
А тем они замечательные.
Их легко обращают.
Для артагональной матрицы
достаточно транспонировать.
Для унитарной матрицы
достаточно транспонировать.
М мы не выбираем.
М это
число строк исходных.
А исходных?
Эм на эм.
Эм на эм.
Значит вот это вот напоминание
о том что есть такое
сингулярное разложение.
А как оно вообще получается?
Давайте я это уже вспомню.
Теория.
Вот тебе у нас есть.
Да, но я не написал
слово теория.
А фактически сейчас речь о теории.
Что это за теория?
Любой комплексный цитат.
Существует сингулярное разложение.
То есть любой матриц
можно в таком виде записать.
Ну а значит и в таком и в таком.
Сигмы называется сингулярными числами.
Вот эти положительные
сингулярные числа одно одно.
Ну вот
то есть вы сейчас
правильно говорите
что вот сейчас вот
это знание полезно.
Вы понимаете?
Вот только что
товарищи заметили, что
если мы
допустим
вот так вот
а я не знаю
а это или нет?
Ну давайте посмотрим
что это за матрица-то будет.
И здесь как раз уже в примешке полезна
вот эта запись сингулярная.
Сингулярная.
Вообще-то есть.
Берем
Тру
Сигмагрой со звездой
со звездой
Тру
Сигмагрой со звездой
Тру
Сигма
Сигма
Усо
Тру
Сигма
Скобочки здесь встанут что с скобочками?
Единичными
Так?
Единичными
Вот видим что получилось.
Сигма два раза значит
Сигма три раза встретилась.
А вот здесь Сигма
Вот так вот отникнула
Сигмагрой со звездой
Сигмагрой со звездой
Ну а это В
А здесь можно написать вместо ВС
ВС1
В любом образе
Матрица с звездой
А подобные диаметрии
Матрица С
Числа ВС
Матрица С
Положительное сингулярное число
Это корни квагратные
Из положительных собственных значений
Матрица с звездой
Ну отдельные вопросы
А почему для любой матрицы
Матрица с звездой
Нет?
Как это?
Потому что вот эта матрица
Теперь как же получить
Как доказать
Что сингулярное разложение
Существует?
Вот один из путей
Один из путей
Значит как доказать
Существует сингулярное разложение
Вот один из путей
Вот давайте я
Посмотрим
Вот эту пробу
Которую мы называем спектральной
Ну несколько
Замегает вперед
Можно доказать
Что вот эта спектральная
Матрица сингулярная
Она на самом деле равна
Самому большому сингулярному числу
Сингулярность
Корни квагратные
Из положительных собственных значений
Матрицы с звездой
Связаны с спектральной
Но
Но это не спектр матрицы А
Это спектр матрицы С
С звездой
Вот такая она
И она достигается
В чем единичная тюни
Да?
В моде
По которой достигается
Лично
Ну хорошо
Ну хорошо
Давайте
Вот этот вверх так
Перезвоним
Ну давайте я не буду сейчас
Губочек
Чебузрить
Это будет
А это будет
Сигма
Не то что это сигма
А это
Сигма
Матрица
И тетми
И
Так
Сигма это самая норма
Спектральная норма матрица
Существует два таких вектора
Х и В
Нет
Существует два таких вектора
Существует два таких вектора
Сигма это сигма и Игорь
А почему?
Не достигается
Сигма это обращается
И на Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Ну до Х
Теперь
То есть что такое
У нас
Значит
Вода
Вот это
если норма х есть единицы, то сигма это есть норма ах, или другими словами это
есть, ну, вектор ах нормируем, обозначаем через у и получаем вот то, что это.
у, у это есть, единицы на сигма, ах.
один и тот же, еще раз, достигается сигма, при этом норму х можно взять равной единице.
ну, такой, к сути, и через у обозначаем вот такой вектор.
чему равна длина такого вектора? единицы.
да, и получаем вот такую формулу, да.
ну, можно на единичной сфере искать всю формулу.
можно по всем не нулевым эксам, то же самое, что искать по эксам длины единицы.
то есть я могу здесь вместо вот этого условия написать вот такую.
значит вот самое интересное вот это, самое интересное вот это.
вот мы имеем такое равенство. что можно сделать дальше?
можно построить унитарную матрицу х и унитарную матрицу y.
ну, вектор единичной длины можно достроить до нормированной убави.
понятно? множеством способов.
есть много способов. то есть эти х и у высшести это не однозначно.
вот, но надо смотреть, что мы таким образом можем... ах, сейчас посмотрим.
смотри, что мы дальше делаем.
теперь вот это равенство, я хочу так. отсюда я хочу вывести, вот как написать.
а умножить на х равняется у умножить на что-то.
какая-то матрица. но эта матрица полностью определена матрицами х и у, правильно?
ну понятно, что я могу на у-1 умножить и определится вот эта матрица.
но какой будет первый столбец этой матрицы?
здесь будет сигма, а здесь нули, правильно?
матрица, давайте, это однозначно определена.
а первый столбец с моим домом такой, то это будет правильное равенство.
потому что а на х маленькая это сигма на у маленькая.
у маленькая это первый столбец матрицы у большое, вот он на сигму здесь умножится.
да, это нормированных базисов.
так, чтобы матрица х была унитарной и матрица х тоже унитарная.
вот, а здесь, ну вот эта строчка, ну давайте я как-то вознащу через это спонированное.
давайте через домик его возначим.
а что дальше можно заметить?
что вот эта строчка с, транспонированная, на самом деле нулевая.
ну а откуда это увидеть-то?
осмотрите, осмотрите.
значит, вот эта вот матрица, давайте я ее буквой b что ли обозначу пока.
b? да не важно, ну вот это b, что такое b?
это унитарная матрица х с звездой на а, на х.
то есть а и b связаны умножением на унитарные матрицы.
что совсем легко можно доказать?
что спектральная норма, вот та самая операторная норма, которая стерна определение, она не изменится.
если вы матрицу умножите на унитарные матрицы слева и справа, то спектральная норма не изменится.
а это знаете, с каким свойством, замечательным совершенно связано.
вот если вы вектор умножаете на унитарную матрицу, то его длина не меняется.
знакомый, да?
вот отсюда вытекает то, что спектральная норма матрицы не меняется при умножении этой матрицы на унитарную матрицу.
слева или справа, можно с обеих сторон.
то есть вот здесь, то есть вот эта норма тоже сигма.
матрица b тоже имеет норму сигма.
а теперь давайте-ка вот эту матрицу b умножим на вот такой вот вектор в столбец.
значит, что это будет?
ну первая компонента будет вот такая, правильно?
а вторая компонента будет, конечно, вот.
ну остальные это вот такой вот вектор.
ну а теперь давайте посмотрим на нормы.
посчитаем, просто посчитаем.
ну понятно, что здесь вот так можно написать, а здесь вот так вот.
в знаменателе в точности такой, в числитель не в точности такой еще надо написать, но я напишу вот так.
очевидно, да?
то есть
плюс
вот так вот.
довольно корень, да?
или не корень?
а должно быть, вот здесь вот, должно быть, но ведь супремом по всем векторам таким, по всем отношениям это сигма.
еще отсюда и следует, что норма c, длина c нулевая.
то есть на самом деле вот эта первая строчка нулевая.
да?
но это если она действительная?
да, действительная.
может еще?
не обязательно.
ну давайте я это со звездой напишу, ну какая разница.
нет, конечно.
ну длина это корень квадрата и сумма модулей в квадрате.
может еще?
конечно.
значит, вот смотрите, на векторе таком сигма c мы получаем значение отношений не меньше, чем сигма.
сигма квадрат, вот, вот оно, оценочка.
но ведь максимально возможные значения это сигма.
мы имеем неравенство, сигма больше или равно в корне квадрата сигма квадрат плюс норма c в квадрате.
корня квадрат, ну и сигма квадрат плюс норма c в квадрате.
значит, норма c равняется нулевой.
а что произошло?
великая вещь, редукция размера.
редукция размера.
то есть теперь мы задачу у построения или осуществования сингулярного разложения свели к аналогичной задаче для матрицы размер, который меньше.
и можно вспомнить об индукции математической.
то есть если мы предположим по индукции, что для матрицы аздомиком сингулярное разложение существует,
то мы можем его написать и вывести из него, ну с исполнением вот этой конструкции, которая у нас уже есть,
сингулярное разложение исходной матрицы.
а, додумайте, пожалуйста.
вот вам один из таких коротких достаточно путей к получению сингулярного разложения матрицы.
да.
но вот с этой, мы сейчас должны, да, все, можно стираться, должны конечно потихоньку завершать.
вот немножко медленнее у меня получилось, чем я предполагал.
но, может быть, не надо уж сильно спешить.
знаете, как в анекдоте про машинистку, которая печатала 100 знаков в секунду,
что получается ерунда получается.
значит, вот теперь мы понимаем, как, это один из способов, есть и другие пути получить сингулярное разложение,
есть и другие.
ну, есть и такое, которое я вам рассказал, с использованием индукции, с использованием спектральной нормы.
задачка.
а вот теперь задачка, которую, думаю, вы можете решить.
докажите, пожалуйста, вот к следующей лекции, уж точно вот посмотрите, поразмышляйте над тем, что происходило,
докажите, что спектральная норма матрицы равна старшему сингулярному числу.
вот теперь есть все.
то есть, если вы видите сингулярное разложение, то есть вы можете...
самое главное, это то, что, понимаете, что спектральная норма, вот та самая диагональная матрица сигма,
будет совпадать со спектральной нормой матрицы А.
то есть, все, что надо сделать, надо посмотреть, а как вычислять спектральную норму диагональной матрицы.
ну, довольно легкая задачка.
и тогда вы докажите, что...
докажите, что спектральная норма матрицы равна ее старшему сингулярному числу.
спектральная норма диагональной матрицы, это наибольшая...
ну, это уж, просто рассуждайте, что это такое.
наибольший модуль наибольшего элемента.
ну, у нас там матрица с положительными числами.
вот, а следующая вещь, вот важность...
для чего это самое сингулярное разложение мы получали?
почему оно так распространено?
распространено ровно потому, что имеет связь с задачей о малоранговом приближении матрицы.
вот эта связь.
и вот то, что нам дальше надо будет изучить, вот такие вот задачи.
а минус b.
ну, давайте вот спектральной норме, да?
минимизировать по b.
ну, как каким матрицам b?
ранг которых ограничен заданным числом a.
что это такое будет?
вот одна задача, еще одна задача.
давайте теперь другую возьмем норму.
ну, с таким же условием.
во всем...
значит, что здесь будет?
вот эти два ответа.
ну, кстати, попробуйте, может быть, получится.
вот эти две теоремы.
теоремы я должен еще вопрос стереть, так чтобы получилась теорема.
догадываетесь, что есть?
здесь будет сигма каплюса мин.
если считать, что сингулярные числа
занумерованы так, что идет самое большое, потом поменьше, и так далее.
а здесь, на самом деле, вот здесь.
а здесь идет вот такая сумма.
есть такой хвостик.
значит, вот эти две формулы
составляют, ну, такой стержень, что ли, теории
применения сингулярного разложения.
именно из-за этих формул, замечательных, это разложение так распространено.
то есть оно дает полное, гарантированное решение задачи
о малоранговом приближении матрицы
в спектральной норме и в норме Фрабениуса.
с оценкой погрешности.
то есть, если вы знаете сингулярные числа, вы точно знаете, с какой погрешностью.
возможность приближения.
смотрите на сингулярные числа.
понимаете, картина здесь вот очень ясная.
но время вышло, насколько я понимаю.
ну, давайте так вот.
есть книжка методы численного анализа.
методы численного анализа.
ну, автор.
я и автор.
картишник Евгений Евгеньевич.
издательство, по-моему, академия.
там уже лет десять назад это было.
вы бы не читали?
нет, нет, нет, нет.
на конспекте какие-нибудь есть?
ну, значит, я не уверен, что я по какой-то книжке вам в точности что-то читаю.
но эта книжка, она может помочь.
будет список вопросов задач, которые надо будет решить.
что?

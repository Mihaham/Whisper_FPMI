Так, всем здравствуйте, продолжаем разговор, небольшое
исправление того, что было в прошлый раз, вот в этом
утверждении про линейное программирование надо
было дописать, что требуется еще дополнительно, что допустим
множество, ну в общем, да, напоминаю, мы изучали двойственность
на примере вот такой вот простейшей задачи, вот
было линейное программирование, целевая функция линейно-ограниченная,
равенство и неравенство такие.
Мы получали двойственно задачу и потом было утверждение
о том, что допустимое множество в прямой задаче пусто, то
же самое, что и двоясная задача имеет, неограничена
целевая функция на допустимое множество и тут важно было
добавить, что еще и допустимое множество в самой двойственной
задаче не пусто, потому что оно может быть пустым
вместе с допустимым множеством в прямой задаче.
И тогда все работает точно так же, как было, только
ту лямбду, которую мы будем подставлять, строим как
лямбда с чертой плюс тета на p, где p — это вектор из
лемма-фаркаша.
И тогда все работает, потому что для лямбды с чертой оно
уже допустимо, поэтому вот эта штука, то есть вот эта
плюс вот эта уже больше либо равна нуля, и мы добавляем
то, что от tp больше либо равно нуля, если будет выполняться.
И, соответственно, здесь также будет минус бесконечность,
потому что лямбда с чертой на b — это константа некоторая,
а p, транспонированная на b, у нас строго меньше нуля,
вот это вот стремление к бесконечности, получаем
бесконечно маленькое значение.
Бесконечно маленькое, в смысле, стремляющийся к
минусу бесконечности.
Теперь пример того, в чем может быть проблема за допустимым
множеством.
Довольно простой пример, берем вот такую вот допустимую
множество задачи линейного программирования, то есть
у нас не отрицательные чиселки, но одна из них
меньше минус единицы, а одна компонента меньше
минус единицы, что заведом противоречит этому условию.
Но формально ничего не ломается, потому что мы
его можем переписать вот так вот, то есть вот формальная
матрица A, вектор B и ограничение на новый блочный вектор.
было вот здесь.
Вот это наше допустимое множество.
Вот, которое пусто, вот, тогда соответственно по тем
правилам, которые мы вывели вот здесь, вот-вот, у нас
тут допустимое множество такое вот.
И заметьте, что оно внезапно, неожиданно зависит от вектора
C еще дополнительно.
То есть, если тут мы выбрали A и B, вот, матрица A, вот,
вектор B, то поставив, ну, по строке допустимое множество
для, соответственно, C плюс A транспонированное на
лямбда, лямбда, соответственно, двумерный, вот, мы получаем,
что наше допустимое множество в двое, значит, вот такое
вот.
Вот, ну и давайте подберем, можно подобрать очевидным
образом, C2 и C4 так, чтобы вот эта штука не выполнялась.
Вот, ну, например, C2 и C4 равны минус единице.
Вот.
Получим, что и здесь тоже получается пустое множество
за счет того, что у нас допустимое множество зависит от целевой,
от вектора C, который в целевой функции, который никак
не участвует в допустимом множестве исходной задачи.
Понятно, да, откуда взялась еще одна степень свободы,
которой можем получить пустое множество.
Так, да, нет, руки поднимите, кто понял.
Так, один, два, так, давайте еще раз, было вот так, это
допустимое, понятно, что это пустое множество?
Так, тут понятно, понятно ли, как мы его переписали
таким образом?
Добавили к X1 некоторое положительное число, чтобы получить равенство
минус единице, это, соответственно, S1, и к минус X2 добавили тоже
положительное число, это наше S2, чтобы получить равенство
минус единице.
Это, соответственно, S1 и S2, и получаем, что X1 плюс
S1 равно минус единице, и минус X2 плюс S2 равно минус единице.
Получили еще две дополнительные переменные.
Так, вот это понятен.
Это наше допустимое множество, которое в точности совпадает
с тем, что было вот здесь.
Вот наша матрица, вот наш вектор B, вот не отрицательность
по компоненту нашего целевого вектора.
Далее, мы тут страдали долгую порну, получали, допустим,
множество двойственного значения, вот такое.
Вот наша матрица A, вот наш вектор B, значит, допустим,
что будет записываться как C плюс A транспонированное
на лямбду.
A транспонированная, соответственно, матрица 4 на 2, где строки
– это столбсы, я надеюсь, тут понятно.
Поэтому получается, что первая строка становится
первым столбсом, тут лямбда 1, тут минус лямбда 2, тут
плюс лямбда 1 и плюс лямбда 2 умножили.
Из этих четырех нерайс мы выбираем 2, а именно нерайс
на лямбду, что у нас отсюда лямбда 2 меньше либо равно
C2, вот из второго, и лямбда 2 больше либо равно, чем минус
C4.
Понятно, откуда что взялось?
Отлично, говорим, что если у нас вектор C оказывается
вот таким вот, например, то это пустое множество,
заведомо.
Отсюда и тут пустое множество, и тут пустое множество.
То есть условия того, что допустимое множество в прямой
издаче пусто, ничего не означает о том, что допустимая
издача обязательно будет разрешима.
Поэтому эта вот оговорка здесь существенная, чтобы
было вот это лямбда с чертой, которую мы здесь используем
для того, чтобы у нас все сработало.
Так, понятно, так, подеватируйте, кому понятно, так, окей,
хорошо.
Едем дальше.
Так, все, это тут с прошлой лекции вроде я все долги
вернул.
Там еще были некоторые поправки, но они вроде плюс-минус
очевидны.
Это ладно, они мне будут не возвращаться.
Так, переходим к сегодняшней непосредственной лекции,
а именно про то, как, собственно, строятся солверы для выпуклых
задач.
Ну, для в целом задач и выпуклых в частности.
Так что сегодня будет немного примеров, немного каких-то
ссылок на ранее рассмотренный материал.
Так, это было в прошлый раз, двойственность, обученное
неравенство, коническая двойственность, нам сегодня
понадобятся конические представления задач, двойственность
для СДП и условия солвера для СДП почему это существенные.
Напоминаю, как выглядит в общем виде наши задачи.
Мы минимизируем некоторую целевую функцию f0 при условии,
что у нас есть неравенство и равенство.
То, насколько мы можем такие задачи решать, напрямую
зависит от свойств вот этих самых функций, которые
сюда входят, собственно, в такие задачи.
Значит, если у нас есть, если все афинно, то это
линия программирования, о которой мы до этого, о которой
некоторое время уже обсуждали.
И такие задачи могут быть крайне эффективны и быстро
решены, там куча методов, мы про них поговорим чуть
позже.
С другой стороны, если что-то нелинейное, то даже очень
простое, то это напрямую может существенным образом
повлиять на сложность решения и привести к тому, что задача
станет по полной, по сложной и все эти вот слова о том,
что только переборные методы экспоненциальной сложности
будут работать.
К счастью, для многих нелинейных, нелинейных функций, целевых
и в ограничениях, которые выглядят, может быть, сильно
нелинейно, оказывается, что при наличии выпуклости
на них все разрешимо также за полинамиальное время,
то есть достаточно быстро.
Да, это напоминание того, как выглядит задача выпукло-оптимизации,
в общем, видим.
Какие у нее свойства?
Линейное программирование – частный случай, то здесь
разделение, как я уже говорил на одной из первых лекций,
что разделение происходит не по линейности и нелинейности,
а по выпуклости и невыпуклости.
Удивительно, но нелинейные целевые функции с нелинейными
ограничениями, но выпуклыми оказываются почти так же
быстро решаемы, как и линейное программирование, когда
все линейно.
Значит, может выглядеть довольно сложно, страшно,
непонятно, но при должном внимании и аккуратном переписывании
все получается переписать так, что окажется, что функции
выпуклые, и поэтому можем решить быстро.
Встречается очень много приложений, где это все появляется,
поэтому это достаточно интересно, разумно и полезно
рассматривать подобного рода задачи.
Да, значит, как мы знание о том, что или ожидание о
том, что наши функции выпуклые, можем использовать?
Тут есть много вариантов того, что мы можем делать,
для того, чтобы ничего не делать и просто верить в
то, что нам пришло на вход, является выпуклым и исходя
из этих предпосылок, что-то с этим делать.
Это не очень понятное дело, перспективное направление,
поскольку у нас много информации, но мы не всегда можем быть
уверены, что оно действительно работает, потому что мы никак
не проверили то, что нам пришло, действительно выпукло.
Вы написали в Питоне какую-то функцию и отправили ее
в оптимизатор, и он может начать с ней делать, при
этом непонятно, почему оно будет работать, если вы
никак не проверяете это все.
Это, по-моему, дело просто, ничего делать не надо, но
преимущества теряются, поскольку вы не используете в явном
виде, вы не понимаете заранее или никак не выводите то,
как это выпукло, если непосредственно строится, то есть почему
то, что вам пришло на вход, выпукло или не выпукло.
Понятно, что альтернативный способ этого можно проверять,
то есть вам что-то пришло, вы сказали, давайте мы сначала
проверим, выпукло или не выпукло, а потом только
будем что-то запускать.
Тут не очень понятно, как это делать, поскольку вам
пришел черный ящик, который вы подаёте х, он вам возвращает
значение, и вам надо как-то проверить, что вот эта вся
штука, она выпукла.
Но не очень понятно, как это делать, поскольку либо
вы будете по каким-то критериям это проверять, но во всех
критериях фигурирует страшный квантор для любого, и когда
вам нужно для всех в мерном пространстве проверить,
вы дольше проверять будете, чем решать.
Компромисты, некоторые вариант вот этих двух крайностей
являются в том, что мы предполагаем, что пришедшая функция
на вход будет выпуклой, но наше предположение опирается
на то, что мы заранее заставляем пользователей строить целевую
функцию так, чтобы в процессе, перед тем как решать, вот
эта вот проверка была достаточно быстро осуществима.
То есть у нас есть некоторое правило композиции некоторых
элементарных блоков FIT, исходя из знания о том, из каких
блоков мы строим нашу функцию, и того, как допустимое
преобразование меняет свойства функции, мы можем
вывести свойства в целом результата.
То есть как проверять выпуклость, например?
Можно проверять по критериям, например, критерия второго
порядка.
Грустно, что во-первых, нужна определенность гессиана,
во-вторых, нужна, что эта определенность была для
любого вектора.
Это грустно, это долго и непонятно, как это для
любого исследовать, особенно это страшно, когда у вас
какой-нибудь максимум сидит, который не дифференцируем.
Там вообще это работать не будет, потому что только
непрерывная дифференцируемость требуется.
Вот.
Вот проверка выпуклости через зачисление выпуклых
функций более перспективный способ, который заключается
в том, что я уже частично проговорил, что есть набор
простых функций, выпуклость которых вы показали руками,
на основе как раз-таки, например, вот этого.
Вот.
Есть некоторые преобразования, которые выпуклость не
портит.
Можно максимум брать, например, можно там афинные
преобразования от аргумента делать, еще что-то можно
там складывать, на константы положительные домножать.
Вот.
И вы на основе этих правил генерируете новую выпуклую
функцию просто по этим самым правилам.
Вот.
Тогда, соответственно, solver надо будет разобрать
вашу функцию на составляющие и проверить, что ингредиенты,
которые вы туда засунули и правила, которые вы использовали,
в результате дают выпуклую функцию.
Какие функции считаются простыми?
Так, а понятно, что вообще происходит-то?
Я так, может, слишком быстро говорю?
Нормально, да?
Прекрасно.
Ну, понятно, у нас есть степенные функции, у нас есть всякие
алгорифмы, экспоненты, есть линейные функции.
Я надеюсь, все понимают, как все дело проверяется.
Все понимают.
И как же?
Громче.
Ну да, в случае, когда у вас скаляры, то просто
берете вторую производную, смотрите на ее знак, тут
линейность по определению можно проверить, норма
по определению проверяется, максимум проверяется,
ну, по определению, наверное, алгорифм проверяется через
которые второго порядка.
То есть тут надо гессианом проанализировать.
Вот.
Вот эта штука проверяется более хитрыми техниками
через сведение к функции от скалярного аргумента.
Вот.
То есть вот эти функции проверяются руками.
Ну, наверное, еще какой-то набор функций можно проверить.
Это, в общем, не так, чтобы очень сложно и страшно.
Вот.
Теперь, как бы, наши элементарно некоторые штуки, которые
мы можем использовать для того, чтобы генерировать
новые выпуклые функции с помощью некоторых довольно
несложных правил.
Вот.
Ну, например, как бы, на константе вот можно домножать,
на неотрицательном.
Вот.
Это по определению проверяется.
Вот.
Можно складывать.
Это проверяется, ну, да, тоже по определению, наверное.
Вот.
Можно вот с афиной функцией брать композицию, это проверяется
тоже по определению.
Тут, в принципе, все по определению проверяется.
А, вот максимум проверяется, не обязательно по определению,
можно через над-график это все дело сформулировать,
сказать, что...
То есть вот это все, я в каком-то смысле повторяю то, что мы
уже обсуждали на занятии про соответственно выпуклые
функции.
Вот.
Чтобы как бы актуализировать, что ли, ваше представление,
ну, как это работает.
Значит, максимум, да, можно брать, ну, и там, понятно,
можно брать композицию.
Вот тут приведен один пример, их, понятное дело, довольно
много, что если у вас функция от числа, то если она выпуклая
и возрастает, то если вы на вход подадите результат
действия выпуклой функции, то в результате у вас тоже
будет выпуклая функция.
Вот.
Показывается это либо по определению, либо в случае,
если вы накладываете еще какие-то ограничения
на дифференцированность, то можно в критерии второго
порядка воспользоваться.
Возможно, нет.
Там меньше либо равно же должно стоять.
Поэтому...
Ну, не убывает, да, как минимум.
То есть линейная функция подходит, вот.
Простой пример, чтобы себя проверить, как бы.
Вот.
Ну и да, в общем, их довольно много, тут можно, помимо скалярных
композиций, можно изучать векторные композиции.
То есть когда у нас результат одного...
То есть, грубо говоря, когда вот такая вот ситуация,
что у вас, как бы, одна функция переводит один вектор
n-мерный в другой, там, m-мерный, потом этот m-мерный
переходит какой-то в p-мерный, вот, и потом это все еще
в какую-то функцию, которая скалярно возвращает.
То есть, такая вот литвистая сложная композиция.
Вот.
Ну, пример можно посмотреть, типа, максимум линейной
функций.
Довольно выразительное семейство, потому что, ну,
потому что вы можете любую функцию, как бы, приблизить
набором подпирающих ломаных.
Вот.
И, в общем, тем самым приблизить, в некотором смысле, с помощью
вот такого вот семейства.
Вот.
Ну, что у нас здесь есть?
У нас есть...
Да, давайте.
У нас есть линейная функция, набор линейных функций,
у нас есть максимум.
Вот.
Ну, линейная функция выпуклась, максимум сохранять
выпуклась.
По этому результате будет выпуклая функция.
Вот.
То есть тут проверка несложная.
Вот.
Если вы смотрите вот такую вот L1 регуляризацию линейнейших
квадратов, то тут что у нас?
Первая это сумма.
Какие функции здесь стоят?
Тут функция, константа положительная умножить на что-то выпуклое.
Тут константа положительная умножить на квадрат от нормы,
от афинного преобразования аргумента.
Вот.
Квадрат...
Важный момент, что квадрат на положительных аргументах
выпуклый и возрастает, поэтому вот это вот правило
вот это вот работает.
Вот.
То есть если бы у вас был вот здесь вот была не норма,
а что-то, знак чего вы не могли бы определить, то
все бы перестало работать.
Вот.
Это, кстати, можно будет сейчас проверить.
Вот.
Ну вот еще такой более общий пример, что если
у вас есть выпуклая функция f it, то взяв вот такую вот
функцию, минус сумма логарифм от минуса f it, вы получите
тоже выпуклую функцию, рекомендую это проверить
с помощью критерии второго порядка, например, взяв
явно десятку счетов.
Это хорошее упражнение для того, чтобы вспомнить
или потренироваться, дополнить на том, как это все
дело происходит.
Вот.
Это полезно для того, чтобы когда у вас вот такое вот
ограничение и ваш допустимое множество по сути дела
ограничивается вот такими вот эксами, то вы можете
как бы вот это ограничение перенести в целевую функцию
с помощью вот такого вот дополнительного слагаемого,
которое будет предотвращать при решении выход за пределы
допустимого множества.
Ну, просто потому что при f от x близких к нулю эта
штука будет плюс бесконечностью, и вы заведомо
попытаетесь не сильно близко к границе подходить.
Понятны ли примеры?
Окей.
Да, ну и последний пример.
Тут максимальное собственное значение у симметричной
матрицы.
Понятны ли, почему это выпуклая функция?
Тут более сложная история.
Ну, Suprema Maximum.
Так, неплохо.
А вот эта функция какая?
Ну, гисян по матрице тяжело.
Четырехмерный.
А как вы будете определенноспроверять?
Вам надо там сворачивать с любыми двумя матрицами.
Тяжело.
Можно проще.
Когда мы обсуждали максимум, у нас было что?
Нам надо было, чтобы для каждого индекса вот эта
функция была выпуклая.
Вот написано, да?
Это понятно.
Для Suprema все то же самое, только тут нужно, чтобы
для каждого икса эта штука была выпуклая.
А для каждого икса, это что за функция?
Какие врядды?
То есть вот это вот x фиксирован, и эта функция от матрицы.
Каким свойством она обладает?
Не, не, функция от матрицы еще раз.
А переменная, да.
В этом-то вопрос как бы.
То есть все привыкли обычно, что вот когда идет
такое, то матрица фиксирована, а x переменная.
Поэтому эта штука квадратичная.
Все сразу типа квадратичная, квадратичная.
А у нас ситуация меняется, потому что мы хотим
проанализировать вот это выражение на выпуклость.
Но она линейная, конечно.
Она линейная.
Просто вы в силу того, что сюда поставьте
αа плюс, что там, 1 минус αb,
и у вас все распадется просто в сумму.
Всем это видно.
Окей.
Поэтому эта штука линейная,
линейная значит выпуклая,
поэтому взять Suprema выпуклость не портит.
А у нас, конечно, множество максимума.
Ну он не максимум.
Множество тут такое, ну типа.
Да, потому что проблема в том,
что мы все вместе стоим.
Нет, проблем никаких нет.
Да, но тут проблем никаких нет,
потому что вот когда мы,
если вот это вот показывать не совсем по определению,
а через надграфики.
Вам в многих семинарах про это рассказывали,
что-нибудь про надграфики?
Тишин? Да?
Нет? Окей, я тогда расскажу.
Ну вот как вот так показать
через надграфики?
Понятно или нет?
А что надо рисовать?
Вот эти футы, да?
Так, а что дальше?
Так, и что получится?
Какое?
Какое?
Почему?
Нет, может быть да, может быть нет,
а аргументы-то какие?
То есть тут важен не ответ, тут важен Путин, как бы.
Да, через так,
чтобы потом можно было обобщить на Suprema.
Ну вот да, вот вопрос
через надграфик, как это делает.
Да, вот, правильно.
То есть когда мы берем максимум,
то мы пересекаем над графики вот этих функций.
Это как раз то, что вы хотели закрашивать.
Вот, а поскольку надграфик
у каждого из вот этих вот ингредиентов
выпуклый, поскольку сами функции
выпуклые, то их пересечение будет
выпуклым. А когда мы берем Suprema,
то мы пересекаем бесконечную множество над графиков
для каждого элемента.
Ну и бесконечную множество
мы можем пересекать, и выпуклость от этого
не портится. Поэтому тут
никаких проблем с Suprema нет.
Это к тому, что иногда доказывать по определению
может быть не всегда хорошо. Ну не всегда
продуктивно для каких-то дальнейших
общений. Вот.
А есть ли понимание,
как эту штуку представить в
конической форме?
В конической.
Когда у вас минимизируются линейные функции
при условии, что
в каком условии-то?
Ну там, линейное равенство и то,
что искомый аргумент
принадлежит некоторому конусу.
То есть хотим условно, например,
что сделать?
Suprema был функционал?
Это типа целевая функция,
мы хотим записать минимум
lambda max от некоторой
матрицы.
Так, наверное, это надо написать.
Где тут моя доска?
То есть хотим вот что сделать?
Там A лежит в некоторой множестве,
например, ну неважно,
где-то лежит.
Как вот это в конической форме записать?
Давайте пока игнорировать вот это множество,
я не хочу сейчас сильно заморачиваться,
хочу просто вот этот значение записать в конической
форме.
Так, чтобы
Ой.
Было минимум, там
zeta x
a x равно b
Вот так.
То есть как первый шаг
я сделал так, чтобы целевая функция
стала линейной?
Уже несколько раз обсуждали,
что надо заменить
и каким образом.
То есть мы можем
вкатить с паттерном
и вкатить на паттерн x и z
и на z и z.
Ну там просто значение нет.
То есть
x и z и y, потому что
a
Ну сейчас мы у нас
как?
Ликтуйте.
Сейчас погодите,
минимум y, так,
как что?
Так.
Ну почти.
Ну я бы сказал
даже не выпукло,
потому что у нас с x
некоторые проблемы есть.
В частности, ну
понятно, да?
Ну типа
норма x равна единице, это типа шар какой-то.
Не выпукло множество,
как минимум.
Давайте чинить.
Хороший вопрос.
Куда делся максимум по x?
Ну, видимо, хотели сделать следующее.
Хотели сказать, что
вот то, что вот здесь, вот это можно записать
как минимум t
при условии, что лямбда
max от a меньше
либо равно t.
Понятно, что это одно и то же?
Или нет?
Да, нет.
Руки подьмите, кому понятно.
Громче?
А, хорошо,
давайте обсудим еще раз этот момент.
Смотрите, когда мы ищем
минимум функции,
я, мне кажется, зарисовал эту картинку, но нарисую еще раз.
Например, вот такой.
То это наш x,
это наш f от x.
И мы ищем
только по оси x минимум.
Если мы делаем вот так,
то мы ищем одновременно и x
и значение t,
чтобы t было как можно меньше,
то есть f от x было как можно меньше,
но x все еще был допустим.
То есть мы как бы
ищем и x и значения
на всей плоскости,
а не только на оси x.
Поэтому мы делаем неравенство.
Понятно?
Ура.
Ну смотрите, у нас что получилось,
здесь уже что-то линейное, это прекрасно.
Все видят, что это что-то линейное.
Замечательно.
Теперь давайте разберемся,
как расписать вот это в виде
некоторого конического ограничения.
То есть эта штука какая-то
нелинейная, существенно.
Так?
Все видят, что это нелинейная штука.
Это максимум какая-то,
страшное дело.
Что делать?
Что значит, что максимальное собственное значение
матрицы А меньше либо равно, чем t?
Вспомнить надо ли имя алгебру теперь?
И какой конус у нас связан
с матрицами там как-то?
Ну,
ну,
ну,
ну,
ну,
Хорошо. Какой конус связан с матрицами?
Отлично. Положить на определенный симметричный матриц.
Шикарно.
Какая матрица должна быть симметричной
положительно полуопределенной, чтобы это было
выполнено? Которая будет зависеть
от A и T?
Громче?
T на I минус A.
Хорошая гипотеза. Давайте проверим.
Вроде похоже на правду.
Давайте проверим.
Все увидели, почему это правда.
Поднимите же, кто увидел.
Два. Ну, три, наверное.
Смотрите, что происходит со спектром,
если мы умножаем, вычитаем из матрицы
диагональную матрицу с
одинаковым элементом на диагонали.
Да, они смещаются на одно и то же
число. Соответственно, если у нас
максимальное собственное значение меньше
T, то когда мы
ну, там, условно A
минус T берем,
то у нас точно будет все отрицательное
значение будут все.
Поэтому эта штука будет отрицательно
полуопределена.
Ну, мы на минуту намножим, получим вот это.
Так, получилась картинка?
Или лучше расписать?
Поднимите руки, кто хочет, чтобы я это расписал.
Так, поднимите руки, кому все понятно.
Так, ура.
Вот, ну и соответственно, вот наша
то есть T
и минус A. Это что такое?
Это лежит в нашем конусе.
А сама по себе вот эта штука
это, ну, понятно, что
это, наверное, да?
Или нет?
Ну, вот это линейно-матричное
не нравится, по сути дела, когда мы
записываем, типа,
наши константы умножить на некоторые
константные матрицы и больше либо равно нуля.
Как вот двойственная задача в прошлой лекции.
Давайте, я, наверное, не зря ее
открыл,
быстренько сейчас промотаю,
вот так вот. Ну вот, ровно вот то,
что мы здесь получали, вот.
Понятно, да?
Откуда взялось?
То есть тут у нас T, а потом
все остальные, ой, все остальные матрицы, которые были,
они
да,
они образовались вот здесь.
Что это?
Они образовались, тут минус Y, как раз стоит,
и это все конечная задача.
То есть мы минимизируем T,
то есть C транспонирует
на некоторый большой вектор,
длинный, при вот этом условии.
При этом T у нас здесь как
число, а матрица A
у нас как матрица.
Если вам
не очень понятно, почему тут,
как это, почему вот это
похоже на вот это, хотя тут вроде одна
и та же размерность, но вот это
вот выражение, его же можно вытянуть в один большой
вектор.
Понятно, да?
Или нет?
Вот это.
То есть это T
на единичную матрицу, это типа
N квадрат, вектор длины
N квадрат, и вы из него вычитаете
тоже вектор длины N квадрат.
И на него просто хитрые
ограничения наложены, которые
сильно нелинейные, но записываются таким вот
образом.
А здесь в C сидит тоже
N квадрат плюс одно число, которое
соответственно
нули там, где матрица A
и единицы там, где T.
Получается транспонированный
на некоторый большой вектор.
Поделитесь, кто понял такое
объяснение.
Так, не очень.
Ну, смотрите,
тут, видите, один конус, тут
одна и та же переменная должна фигурировать.
А тут у нас фигурирует
T, а тут фигурирует какая-то матрица еще,
которую мы ищем.
Вот как это связать?
Связывается это очень просто. Вот эта
штука рассматривается как
операция, условно, над длинными
векторизованными матрицами.
И тогда для вектора C у нас есть
просто N квадрат нулевых компонент,
чтобы убрать всю зависимость от
матрицы A и оставить
единичку только там, где элемент
для числа T, который мы
также ищем.
Стало ли полегче?
Стало полегче.
Ну, хорошо. Так, ладно, это было некоторое
отступление, не знаю, насколько оно было нужно,
но, наверное, полезно. Так,
где слайд? Вот.
Да, короче говоря,
это к тому, каким образом
подобного рода задачи можно переписать в
кодической форме. Сейчас далее будет
данный общий рецепт того, как это
делается и почему это можно делать
почти на лету и автоматически.
Да, но
сначала, перед тем, как
переходить к тому, как это все делать
на лету и автоматически, надо сказать пару слов
о том, в общем, в целом, как решать задачу
к оптимизации, если она у вас
если она у вас есть. Значит,
первый путь, самый, наверное,
понятный, использовать некоторые
стандартные солверы для определенного
типа задач. То есть, условно говоря,
вот вы, у вас есть какая-то задача,
вы понимаете, ага, это задача линопрограммирования
или это задача квадратичного
программирования. Или у нас что-то полуопределенное
в одном из видов,
вот таком вот
или вот таком вот, например.
Вот тогда,
окей, есть написанное, просто
гуглите lpsolver,
у вас выпадет
огромное количество реализации различных
языков программирования библиотек,
вы говорите, ага, вот мне нужно
условно на плюсах,
на питоне, на скале,
на чем хотите, вот, на джули,
берете,
скачиваете
и отправляете ваши данные,
ваши задачи в этот солвер, он выдает ответ.
Проблема, а, ну точнее, пока не проблема,
пока плюсы довольно
легко.
Не надо ничего писать, все за вас уже написали,
вам просто надо правильно запустить.
Это, возможно, тоже не всегда легко,
но, по крайней мере, это исключительно
работа с тем, как правильно представить,
да, ну, в общем, да, это вот
собственно второй пункт, что
солвер жестко фиксирует форму задачи.
То есть, если у вас
линейное программирование в форме
ограничений AX равно B
и X больше либо равно нуля,
а солвер требует задачу
в виде AX меньше либо равно B,
то все, всю работа
по превращению задачи
из одного вида в другой вы берете на себя.
Вот, это некоторая
проблема, потому что, в общем,
в случае, вот, все, что мы видели
вот здесь,
вот,
оно, ну, сложно сказать,
что это стандартный вид некоторый,
то есть, это пока что и не линейное
программирование, и не квадратичное,
в общем, в случае, то есть, это что-то странное
пока что, вот, поэтому есть некоторые
зазоры между той общей формой, о которой мы
говорили, и теми стандартными солверами, которые,
ну, для тех задач, которые мы
до этого обсуждали, как пример из задач
выпуска оптимизации и
смотрели на то, каким классом они относятся.
Вот, значит,
понятно, что
вот это все дело
сделано довольно долго
и упорно большим количеством разработчиков,
вот, поэтому, значит,
считается, что
то, как долго
и упорно вот это разрабатывать,
окупается тем, чтобы много людей этим может воспользоваться.
Вот, то есть, некоторая
огромная махина,
которую вы, скорее всего, смотреть не будете,
но вам надо как бы научиться
грамотно этим пользоваться.
Вот, второй подход
это вы увидите задачу
и начинаете думать, так, как же мне ее решить?
Вот, придумываете какой-то свой метод,
реализуете, проверяете корректность
проблемы, трудоемко,
вы можете получить
эффективность для одной конкретной задачи,
при этом, если у вас там что-то поменяет, все может
сломаться. Вот.
Опять же, компромиссный вариант между этими
двумя, то есть, один это
использовать уже что-то написанное,
но оно написано не для вашей задачи, а для
другой, ну, в другой форме, как минимум.
Вот, и вам нужно как-то
что-то там, как-то ее подкрутить так, чтобы она
стала, чтобы те параметры, которые принимают
функции solve, они бы
соответствовали тому, что вам надо.
Вот. И тому,
что, типа, все писать с нуля, ничего
и ни на кого не смотрим, причисали статью,
реализовали кое-как
с какими-то ошибками, не ошибками,
все проблемы, как на все грабли
по дороге наступили, которые вот здесь вот
уже за вас решили. Вот.
И в конце что-то получили, но
потратили кучу времени на разработку, на
отлаживание, одна задача решена,
другая задача чуть-чуть другая пришла,
я не понял, что делать. Вот.
Компромиссный
вариант это преобразовывать
вот задачу, которая вам пришла вот здесь,
вот, к некоторому стандартному виду,
который бы решался вот этими solver'ами.
Вот.
Как это делается? Да,
перед тем, как это делается. Значит, множество задач,
которые мы можем решить вот этими штуками,
расширяется, преобразование,
которое вы, в принципе, можете делать,
может быть довольно сложным,
неприятным, громоздким, сейчас посмотрим,
к чему это может привести. Вот.
И, в принципе, этом
не всегда, то есть
это может быть не совсем тривиальное
действие, которое,
то есть, есть методы, которые автоматически это
делают, за счет того,
что они делают это автоматически,
приходится платить, возможно, некоторой избуточностью.
Вот. Но, как бы, вот эта вот
громоздкость не всегда
является большой проблемой.
Так, ну давайте пять минут перерыв,
и потом продолжим. Тут как раз там,
ну да, примерно половина. Немножко
похоже
на вот это,
вот, но, в общем,
как бы, они не заточены
под конкретный вид задачи,
но заточены под ее,
как бы, класс. То есть, словно там,
с ограничениями задач или без ограничений,
там, какие, какого рода
ограничения. Про часть из них мы поговорим
в дальнейших, в дальнейших лекциях.
Вот. Но, в общем,
вот это про вот это и вот это говорить
не будем, про максимальные методы поговорим.
Вот. В основном, они были разработаны
в шестидесятых-семидесятых годах
в Советском Союзе. Подробнее вот здесь
написано довольно хорошо.
Вот. Можете по слуге пройти, там будет
некоторый документ. Вот.
Значит, универсальные методы
для задачи
оптимизации, есть ли какие-то функции
недиференцируемые, ничего страшного, они с этим
кое-как, как-то там справятся.
Вот. Значит, металлипсоид
знаменит тем, что с помощью
него показали полинамиальность
доза членов программирования.
Вот. Впервые, там,
семидесятый какой-то год.
Про это еще, там, когда будет лекция про линейное
программирование, более подробно поговорим.
Вот. Но на практике не очень эффективен.
В общем, они, вот эти два, они на практике
так. Есть некоторые
недостатки. Вот.
Значит, более продвинутые
и полезные с практической точки
зрения методы называются методами внутренней точки.
Вот. Они работают
значит, с ограничениями. Вот.
Вот тут вот есть книжка,
девятьдесят четвертый год. Есть Рев Немировский.
Тоже советские математики.
Юрий Евгеньевич сейчас
профессор в
Левине, в Бельгии.
Немировский в Джорджи
Атехе в США.
Вот. Девятьдесят четвертый год. Можете посмотреть.
Там куча цитирований. Все
ими очень сильно пользуются.
Вот. Обзор более понятный.
И вот я не уверен, доступно ли
вот это для скачивания. Вот это точно
доступно.
Вот. Важные отличия этих методов в том,
что они применяются для гладких задач.
Вот. Для гладких функций.
И задач в конической форме.
То есть тех задач, про которые мы
до этого, которые мы до этого
обсуждали. То есть линейное программирование,
конусы второго порядка, полупределенная
итерация. Вот.
Очень эффективный метод.
Удивительно, но в зависимости...
То есть нужно всего лишь несколько
десятков итераций обычно.
И это количество
итераций растет
в зависимости
от размера, с ростом
размерся задачи, чуть ли не там, типа, ну, плюс пять.
Не знаю. Размер нас увеличил
в десять раз, он там, типа, плюс
десять итераций. Вот. Ну, то есть она
очень... Методы очень
не очень, точнее, чувствительны
к тому, какая разбирательная вход приходит.
Вот. Я надеюсь, что мы успеем
все-таки поговорить о том, как они строятся
и какие у них у этих методов есть свойства.
Вот. Идейно
они довольно понятные.
Когда начинает... Если начинает
разбираться в том, как это
эффективно реализовывать, там как бы отдельный
курс можно про это, наверное, прочитать. Вот.
Какие все толкости реализации вот этих методов
для вот такого рода задач.
Вот. К счастью, если вам нужно просто этим
воспользоваться, есть готовые пакеты, где все это реализовано,
вы просто нажимаете кнопку solve
для правильной записи, и все у вас все получается.
Вот. Каждая итерация
требует решения некоторой блокчины
или линейной системы. Откуда наберется,
тоже поговорим. Вот.
Значит, в чем проблема
применения подобного рода методов в том, что
например, вот у вас задача, типа, негладкая.
Потому что есть первая норма, которая
не является гладкой функцией.
Поскольку равна сумме
модулей входного
вектора.
Что предлагается сделать?
Предлагается, да, все выпукло, но F негладкая.
Предлагается поменять задачу
так, чтобы она стала гладкой.
Каким образом
это делается?
Как сделать так, чтобы
задача стала гладкой,
а никакой негладкости не было?
Тут, кстати, тяжело на X.
Sorry за опечатку.
Прости.
Так, а понятно,
что было сказано до этого-то?
Везде.
Что в квадрат возвести?
И что у вас будет?
У вас будет там произведение модулей.
И поздравления.
Не, погодите, ну, задач уметь нельзя.
Надо получить решение вот этой задачи.
Но какие-то эквивалентные преобразования
провести так, чтобы
стала гладкой.
Что делать?
Ну да, на самом деле
вводим переменную T.
Говорим, что первая норма X
меньше либо равна T, ну а дальше
все компоненты
по значению
каждой компоненты от −T
до T.
Получаем, тут все выпукло,
тут выпукло.
Что оказалось?
Оказалось, что количество переменных
увеличилось в два раза
и ничего ограничения появились.
Но задача стала гладкой.
Важно, что
решив одну задачу, мы получаем решение
о следующей задаче.
И вспоминаем, что для медно-внутренней точки
количества переменных
в большой зависимости нет в числе
итераций.
Поэтому что хоть так,
что N, что 2,
в целом будем считать, что
некритично.
Зато можем решить
плюс-минус,
можем сделать это все
почти что автоматически.
То есть, как это работает?
Есть некоторые выпуклые задачи.
Далее делаем некоторые
преобразования этой задачи
квалентные.
ПК является такой задачей,
для которой применимы методы внутренней точки.
Решаем
эту задачу медно-внутренней точки,
который зашит где-то
вот здесь.
Где-то вот здесь, в стандартном солдере.
И получив решение,
поскольку мы знаем,
какие здесь были преобразования,
мы просто из решения задачи ПК
получаем решение по ноль.
Вот здесь, понятное дело,
может быть больше ограничений,
переменных, все что хотите,
но важный момент,
что вот правила преобразования,
которые мы здесь делаем,
мы знаем, как их реализовывать
эффективно.
Вот здесь у нас появилась единичная матрица
или что-нибудь еще там,
мы знаем, что там будет единичная матрица
для этого блока переменных,
и мы ее явно не храним.
Мы просто говорим, что действие этой матрицы на вектор
это он сам,
и когда нам нужно решать какую-то систему
с блочной матрицей, с соответствующими блоками,
то мы не формируем ее явно,
мы просто, грубо говоря,
индексацию правильно прописываем,
где что у нас находится.
И все довольно эффективно делается.
Значит,
соответственно,
важный момент, что
правила преобразования для выпуклых функций
у нас порождает преобразование
для задач. То есть то,
что мы делали для функций,
автоматически приводит к преобразованию
из П0 в ПК.
Был у нас максимум,
то есть, условно, видим максимум,
вводим новую переменную, добавляем ограничения.
Все, условно, алгоритмически делается
абсолютно понятно.
Увидели композицию,
вводим новую переменную,
добавляем ограничения.
У нас стало h от t
и добавилась переменная.
То есть разносим композицию.
Ну, а для h от t
у нас, во-первых, скалярная функция,
для нее можно сделать то же самое только для t.
Еще одну переменную ввести, условно, s
и сказать, что у нас тут вместо h от t
s и h от t меньше любовной s.
Вот.
Да, давайте.
Да, давайте.
Да, давайте.
Так это то же самое, что было до этого,
когда мы обсуждали.
Но вот, во-первых, не отрицательная штука
это нас спасает.
И мы просто говорим, что вот это, при условии,
что вот эта норма меньше либо равна t.
Сейчас, погодите, вру, не так.
Лямбда умножить на сумму модулей.
Так?
Каждый модуль меньше либо равен t-итому.
Если он, ну, еще раз, идея та же самая,
то мы минимизируем функцию,
заменяем функцию
на число
и говорим, что функция меньше либо равна числа.
Вот это оно и сделано.
Да, но то, что у нас
этот минимум зависит и от x, и от t,
компенсируется как раз
тем, что у нас есть здесь
вектор t, у каждого x
свой
ограничитель сверху.
Нет, проблема в том, что
x равна t не выпукло ограничение.
И поэтому нам нужно
сделать вот так.
Ну,
то есть
равенство будет достигаться?
Да.
Но если,
грубо говоря,
чтобы обеспечить выпуклость, мы искусно расширяем область значений.
Да, но вы скажите,
получите ли мы его, можем ли мы ответить,
который не на исходных значениях?
Получим ли мы его, давайте посмотрим.
Можем ли мы его получить, в принципе?
То есть, если у нас x, грубо говоря,
вы хотите сказать, лежит, да, между t и
t? То есть, если у нас
x лежит между t и t,
что получится?
Можем ли мы его пододвинуть куда-то,
чтобы он
условно сделал эту штуку поменьше?
Наверное, можем.
От этой тоже зависит,
вы правы, да.
Как раз таки вы можете,
не согласовывая на скорость
набира x и t,
у вас получится, может получиться,
в зависимости от пункта этого дела,
что можно ли с ним дать?
Так, давайте я пока
обновлю слайды, чтобы было
понятнее. Я понял вопрос.
Давайте
я тут допишу, почему так
можно делать. Окей, принято.
О чем мы говорили?
Если мы видим
подобного рода функции ограничения,
то мы их автоматически изменим на такие.
Это я тоже проговорил.
Теперь, да, важный вопрос.
Вот эта штука, если мы ее
получили, то почему,
как такое неравенство конвертируется
в коничность?
Конвертируется очень хитрым образом, на самом деле.
Если есть такое ограничение
для упеклой функции,
то мы можем рассматривать такой хитрый конус.
Конус состоит
из точек в Rn плюс 2.
И
эти точки
удовлетворяют вот такому неравенству.
То есть упражнение
проверьте, что это будет
выпеклый конус для выпеклой функции.
Сейчас будет дан некоторый дополнительный
инструмент, который позволит
это легко сделать.
И тогда вот это неравенство
и эквилент тому, что у нас
некоторому конусу
принадлежит вот такая вот тройка чисел.
То есть если вот эту тройку поставить вот сюда,
z равно t,
y равно 1, получим в точности f от x
меньше уровня t.
Да, при выпеклой.
То есть выпеклое
ограничение такого вида
эквивалентно тому, что тройка чисел
лежит в конусе.
То есть мы каждой выпеклой функции
можем поставить соответствие некоторый конус,
который тоже будет выпуклым.
И более того,
для него можно получить сопряженный,
что нам открывает дорогу конической
двойственности, о которой мы говорили
в прошлый раз.
Теперь, почему это будет работать?
Почему эта штука, то есть как инструмент,
который позволит показывать
выпуклость этого конуса?
Для этого нам поможет перспективное
преобразование,
так называемое. Пусть у нас есть функция
выпеклая функция,
тогда функция от n плюс 1 переменной
вида вот такого
для положительных t
также будет выпуклой.
Выглядит, я понимаю, может быть довольно страшно
и непривычно,
но на самом деле логика очень простая.
Мы расширяем область определения
на положительные числа
и в аргумент
подставляем x делить на t,
а потом результат умножаем на t.
Понятно, непонятно?
Давайте пример.
Пусть у нас была функция
в квадрате эвклидовой нормы.
Мы знаем, что нам выпукло?
Знаем?
Знаем.
Давайте вместо нашего вектора
подставим x делить на t.
Что мы получим?
Получится посчитать?
Да.
Вторая, вторая, квадрат эвклидовой нормы,
да, все просто.
Что получится?
Нет.
Потому что еще умножение на t есть.
Просто посчитать,
будет делить на t квадрат, умножаем на t,
получаем, что функция
нормы эвклидовой
от x делить на t,
выпуклая функция по x и t.
А теперь если мы вспомним,
что афинное преобразование
аргументов не портит выпуклости,
то функции вида...
Угадайте какого.
Да, наверное это странный вопрос.
Функция вида
вот такого
тоже будет выпуклой.
Что называется?
Попробуйте проверить каким-то другим способом.
Вы будете долго и упорно страдать.
Да, потому что у нас была норма
x делить на t.
А мы взяли...
Что мы взяли? Афинное преобразование от каждого аргумента.
Представили его в виде
некоторой афины.
Да, подставили линейную...
Просто линейное преобразование
аргумента сделали.
Вот, ну там понятно,
эта штука больше нуля.
Должно быть.
Да, кстати,
может быть, я немножко вас обманываю,
в том плане, что
может быть, надо лучше сказать наоборот.
Но если мы взяли
афинное преобразование аргумента,
то мы получили
афинное преобразование аргумента.
Может быть, надо лучше сказать наоборот.
Что вы там...
Ну есть понятно, преобразование вперед
и преобразование назад.
Непонятно, чем я сейчас говорю, да?
Да, да, да.
Что это t, а это вот это.
И наоборот будет то же самое.
Вот.
Число знаменателей.
Число знаменателей.
Не переживайте.
Здесь плюс D, все, тут все честно.
Вот, это я к тому, что вот
условно вот такого вида выражения,
вы их выпуклась
условно гессиан считать здесь довольно
бессмысленную.
Вот, ну вы получите вот такое какое-то огромное
штуковина, вы ее на положительную полуопределенность
будете проверять очень долго и упорно.
Да, поэтому я и сказал,
что это типа, наверное, можно думать о том,
что в обратном преобразовании.
То есть это была функция условно от
X, вот.
А мы говорим, что t у нас
это вот это,
вот, и типа,
а вот это это типа обратное преобразование
вот здесь вот. И нам нужно
получить из выпукл...
Из какой-то функции выпуклую, при этом мы знаем,
что обратное преобразование сохранит выпуклость.
Непонятно.
Ага.
От X и от...
Ну, то есть, короче, вопрос, почему здесь было
t, а тут t нет? Я правильно понял вопрос?
А...
Да, правда.
Ничего не портит, потому что мы берем
линейные преобразования, то есть у
нашего преобразования,
которое было, каким, афинным,
оно может нам размерности
понизить. Поэтому можем наложить
дополнительные ограничения на связь
от двух аргументов.
Окей.
Подходит такое объяснение?
Хорошо.
Так, окей, это я
показал пример, так, а теперь
собственно, да,
небольшой доказательств того, почему это работает,
а... Какое?
Доказательств с продолжением, я бы сказал.
Пусть у нас
есть точка, которая лежит на графике
функции g вот этой.
То есть xts над график g
это значит, что
g больше...
Ой, не g больше, а s больше, чем
g.
Это значит, что f меньше,
чем s на t.
Ну, поделили на t, получили вот такое соотношение.
Значит, вот эта точка
x делить на t и s делить на t
лежит на графике f.
Понятны две строчки?
Поднимите руки кому?
Понятно.
Так, good.
Соответственно, получили, что над график
для g
это
некоторое обратное преобразование от вот такого вот
для над графика f.
Ну, было вот так,
мы там сделали какое-то преобразование,
вернулись сюда.
Вот. Над график f выпукло и в множество,
мы это знаем, потому что функция выпукла.
Теперь осталось понять, почему
вот это преобразование будет сохранять выпуклость.
Видите оно какое хитрое?
Мы берем
вектор,
берем последнюю координату,
делим все координаты
на последнюю
и отбрасываем последнюю координату.
Вот какая механика
за этим стоит.
Это называется перспективным отображением,
что довольно логично.
То есть было
было множество c
из rn
декарта в произведении r++
и у нас вот такая функция, которая
делит на положительную
последнюю компоненту и оббрасывает ее.
После применения
такого преобразования выпуклость
в множество сохраняется.
Доказательство очень простое,
будет небольшое упражнение,
которое несложно проделать,
надо расписать
на полстраничке формулы,
пусть у нас были
две такие точки
с положительными числами
в конце.
Наше преобразование
к их
выпуклой комбинации
это вот такая вот точка.
То есть подставили, поделили,
я не знаю, тут
наверное понятно.
Поднимите руки, кто понял,
что происходит.
Да, да, да,
вот, вот взяли вот эту точку
и применяем p, p это вот это.
Соответственно,
первый n тут,
последний здесь, поделили.
Что теперь надо доказать?
Точнее, что теперь надо найти?
То есть это пример того, что мы
напоминаем, мы доказывали, что под действием
некоторого преобразования выпуклость сохраняется.
На одной из первых лекций
мы доказывали, что действие
афинной функции
сохраняет выпуклость. Там в одну строчку
все расписывалось.
Здесь в две
будет расписываться.
Что надо показать?
То есть это некоторый образ
нашей точки,
который лежит в том множестве
выпуклость которого мы хотим найти.
Какой следующий шаг?
Почти.
Надо найти такие бета,
чтобы образ лежал
внутри
образов x и y.
Понятно.
Почему так?
Не понятно.
Давайте я распишу тогда подробнее.
Это наше множество
C.
Мы взяли две точки
и берем
какую-то точку между ними.
И у нас есть отображение
P, которое переводит
это все в C с волной.
Чтобы показать, что при этом
отображении множество C с волной
будет выпуклым,
нам надо показать, что вот эта
точка, которая была здесь,
она отображается куда-то еще,
будет лежать в отрезке, который
есть
в конце которого
образы исходных точек.
Понятно, да, теперь?
Это, собственно, и будет знать,
что множество C с волной выпуклым.
Да, нет, понятно.
Поднимите рыбка, вам понятно.
Так, почти.
Так, разбрались
или нет?
Скажите, что вас смущает.
Все?
Отлично.
Так, это наша цель,
которая благополучно
решается
с помощью коэффициента
β, который надо найти,
просто подставив,
то есть у нас есть вот эта левая часть,
а правая часть,
это β умножить на х
с крышкой делить на хн плюс 1
плюс 1 минус β на y с крышкой
делить на yн плюс 1.
Да, и вы просто приводите
к общему знаменателю и выражаете
β. Я верю,
что вы справитесь
с этим несложным заданием
и убедитесь, что оно действительно будет лежать
от 0 до 1. Там
несложные выражения.
Значит, образ также будет выпуклым множеством.
Соответственно,
вот то, что пропущено
в этих слайдах
пока что, это то, что
в определенном уровне доказывается результат
в том, что обратное отображение перспективного
тоже сохранит выпуклость.
То есть если у вас было множество вот такое,
а дальше вы
подставляете, ну понятно,
что он сделал, подставить просто
множество вот такое,
то из него
вернуть в
rn плюс 1 с полным вектором.
Так, понятно, как выглядит обратное отображение?
Или нет?
Ну, например, да.
Так, good.
Собственно, вот это, некоторый
путь к тому, как показать, что это
все, ну, то есть вот это, видите, просто
именно отображение самой функции.
Понятно, да?
То есть вот это будет по-прежнему выпукло,
ну и дальше вроде все более-менее очевидно
показывается вот про выпуклость этого конуса.
То есть то, что это конус, это совсем понятно.
Умножаете x, y, z
на одно и то же число, тут все сокращается,
получаете то же самое выражение.
Вот, объединяете это дело с нулем,
ну ноль будет лежать. Осталось
выпуклость проверить, но выпуклость как раз-таки показывается
через перспективность.
Понятно ли основные ингредиенты?
Непонятно.
Что непонятно?
Смотрите, утверждение было такое, пусть у нас
f от x выпукловая функция.
Тогда этот конус будет
выпуклым.
То, что это конус, это понятно.
Отлично. Теперь про выпуклость.
Вот эта штука, это
перспективное отображение, тут плюсика не хватает,
перспективное отображение функции f.
Вот оно.
Нет, стоп, стоп, стоп. Это множество.
А это функция.
Функция умножается еще на это самое число.
То есть умножение
на это число
надо для того, чтобы
получить обратное перспективное
отображение для ноб графика.
То есть если бы мы просто делили,
то тут был бы s и мы такие
непонятно, что делать, потому что это какое-то странное преобразование.
Это понятно этот шаг?
Давайте еще раз.
Давайте еще раз. Смотрите.
Хотим показать, что этот конус
выпуклое множество.
Смотрим на это выражение.
Это перспективное преобразование функции f
по вот этому определению.
Значит это некоторая выпуклая функция.
Ну а дальше у нас тут есть
выпуклая функция от x и y,
которая меньше либо равна z.
И вот эта вот множество таких точек,
что g от x и y меньше либо равно z
задает выпуклое множество.
Проверяется по определению.
Берете две точки,
подставляете серединку
или точку из отрезка функцию g,
пользуетесь выпуклостью
и у вас получается, что точка
из середины отрезка тоже будет меньше z.
Точка из отрезка будет меньше z.
Если в гранических точках
у вас выполнено это неравенство.
Вот.
Выучили выпуклость этого конуса.
То есть тут ключевой момент
это вот это преобразование.
Выпуклость которого
вот такая вот через все
доказывается.
То есть
я поэтому
всю эту историю не рассказывал,
когда мы обсуждали выпуклые функции множества,
потому что когда
если бы я вам это рассказал тогда,
вы бы наверное удивились, зачем это надо.
Вот. Но вот сейчас это надо вот этим вот.
Чтобы обосновать, как от неравенств
перейти к конусам.
Вот. Окей. Теперь собственно переходим
к тому,
как
трансформируется точка
методов внутренней точки.
Так, да, даже наверное что-то я еще успею
рассказать.
Значит,
когда мы строим CFITF0
из линейтарных функций правил,
то мы проверяем выпуклость.
Можем проверить автоматически выпуклость.
И о чудо,
такой же разбор на ингредиенты
позволит привести задачу
к стандартной форме.
То есть он позволит записать задачу
в виде, здесь что-то линейное,
а тут конуса.
Потому что
вот.
Если мы будем рассматривать элементарные функции,
которые будут нам собирать
что-то гладкое,
то мы в процессе
выпуклости задачи
получим преобразование,
которое нам позволит
переписать эту задачу так,
чтобы можно было запустить метод
внутренней точки.
То есть одновременная проверка выпуклости
приводит к преобразованию
задачи, которые делают ее гладкой.
Ну и конической.
Понятно ли это?
Так, теперь пример вроде тут
скоро должен появиться, который мне очень нравится.
Все это называется Discipline Convex Programming,
DCP.
Идея в чем? Сдаются перемены, сдаются параметры.
Целевую функцию
и ограничение строите из элементарных
функций, которые
как-то переопределены
и оснащены чем-то еще дополнительно
внутри пакета, внутри библиотеки.
И правила композиции тоже заданы.
Вы получили выпуклость задачи
по построению.
Вы ее потом,
ну не вы, а когда вы ее отправляете
в библиотеку, она там внутри разбирается
на элементы.
И приводится к форме,
для которой подлежит запуск одного из
стандартных солверов.
Решаете пакетом,
восстанавливаете решение.
Пример разбора. Давайте тут
может не очень крупно.
Допустим,
у вас, ой,
видно, да?
Жуткая функция.
Что это такое?
Агариф сумма экспонент
это единица делить
на аргумент,
при том, что аргумент положительный.
Ну, грубо говоря, единица на х, при х больше нуля.
И модуль 364 умножить
на z.
Переменами является z и v.
Как проверить ее
на выпуклость? Выглядит
жутко, не правда ли?
Очень просто.
Мы говорим, у нас
основная функция – алгорифма сумма экспонент.
Какие у нее аргументы?
Функция 1 на х
и модуль.
Значит, это выпукло,
это выпуклые знаки положительные.
Дальше. Функция
1 на х. Вот она.
А какой у нее аргумент?
Линейный.
Линейная функция может быть как отрицательная,
так и положительная.
Это что такое? Это операция плюс,
которая делится,
которая сочетает в себе
что-то зависщее от z, что зависщее от v,
все линейно. Знак плюс-минус.
Здесь все аналогично.
Далее. Когда все дело собирается,
то получается, что это все
выпукло.
Вот тут анализ знака, анализ выпуклости.
Вот тот разбор,
который происходит для проверки выпуклости.
В соответствии с правилами.
Давайте теперь
вместе поиграем в...
В общем, пример взят
вот с этого сайта. Я надеюсь, он сейчас откроется.
Видно, да?
Давайте я вот так делаю.
Вот. И тут есть
типа квиз,
где можно поиграться.
Собственно, quad over lin
это то, что я вот недавно писал про
нормы х в квадрате делить на t.
В одномерном случае
это х в квадрат делить на t.
Х в квадрат
делится на t.
Как вы думаете, будет ли это выпукло?
Что-что?
Кто?
Не-не, смотрите, мы когда
обсуждали, то мы пришли к тому, что
у нас
было вот так.
Было вот так.
Была...
Да, у нас она оказалась
выпуклой. Давайте теперь
поймем, будет ли выпукло то, что
написано вот тут.
Тут есть два варианта.
А финна выпукла вовнута или не подходит
под правила DisciplineConvex Programming?
Я знаю.
Почти.
Вроде как.
Давайте проголосуем.
Кто за афинность?
Никого. Кто за выпуклость?
Руки поднимите.
Два человека. Три.
Кто за вовнутость?
Нет таких.
Кто за вот это? Один.
Кто воздержался?
А вы почему воздержались?
Я не понимаю.
Какая функция?
Смотрите, вот функция. Я попытался
объяснить, что это за функция.
Вы поняли, что это за функция?
Да.
Это 34 на z в квадрате
делить на w.
Надо проверить, будет ли она
такой, такой, такой или такой.
То есть, это вот эта функция.
x в квадрат на t.
Вместо x у нас 34 на z.
Это афинное преобразование.
Делим на w.
Что?
Почему вы не знаете?
Мы знаем, что если мы...
Смотрите, я тут только что показываю.
Что эта штука будет выпуклой,
потому что это перспективное отображение
для выпуклой функции.
Похоже.
Но не та же самая.
Потому что там еще
афинное преобразование вот здесь какое-то.
Сохраняет.
Это правда.
Что да?
Ну, квад оверлин, это вот
x в квадрат делить на t.
Хорошо. Разбрались.
Вот вопрос.
Давайте проголосуем. Я как раз таки призываю.
То есть вы за выпуклость?
Хорошо.
Так. В итоге
большинство за выпуклость.
Так. Давайте проверим.
И вы правы.
Давайте поймем почему.
Умножение
и w здесь уходит
все как вроде получается.
Давайте повысим уровень
до hard.
Так. Но это еще не hard.
Ну ладно. Давайте быстро.
Это что такое?
Логарифм от корня
x пополам минус 1 на u.
Переменная x и u.
То, что u больше 0,
это просто область определения
этой функции записана.
Вроде нет.
Еще раз. Область определения
им в пост только для положительных
аргументов.
1 на u, да.
Да.
То есть тут разность.
Удозрительно, да?
Логарифм от корня.
Страшное дело.
Корень вот так,
логарифм как-то вот так.
И еще что-то вычитается, да?
Давайте, типа, кто за
афинность? Руки поднимите.
Никто.
Кто за выпуклость?
2. Кто за могнутость?
1.
Кто знает, что нельзя этого проверить?
Я вижу опять много
воздержавшихся. Что вас останавливает
от того, чтобы принять решение?
Что непонятно.
Ну, смотрите,
мы знаем о том, как,
какие композиции функции, какие функции нам дают.
Ну вот,
это надо применить здесь.
Так, ну ладно, большинство
проголосовавших вроде за выпуклость.
А вот и нет.
А эта штука вогнута на самом деле.
Да, тут даже вот вам
написали почему.
Потому что инфпоз это
выпукло, минус это вогнуто.
Вот, а вот логарифм от корня
это вогнуто. И поэтому
вы как бы складываете две вогнутые функции,
получаете вогнутую функцию.
Ну вы на минус да множество,
вы учтите, что вогнутая
это минус выпуклое.
И это правило, оно как бы
конвертируется в плюс-минус.
Ну то есть, что там если
бывает и вогнутая, и композиция
с вогнутой, то будет вогнутая скорее всего.
Что такое?
Короче говоря, ссылочка в презентации есть,
можете поиграться.
Это мне кажется довольно забавно.
Что?
Сейчас, давайте посмотрим,
по-моему можно.
Вот она лазит.
Короче, экспрешу сюда пишите в нужном виде,
он вам его разбирает.
Ну тут надо как бы сначала прочитать,
какие функции он поддерживает.
Типа вот.
Ну и довольно много, как видно, да.
Вот quadoverlink,
который я пытался
рассказать сегодня.
pos,
нормы разные,
нормы разные,
логарифма, сумма, экспонент.
Вот он в pos,
кстати, да, вот видите, их больше нуля.
Ну, короче говоря, вроде как
выпуклость или вогнутость почти всего,
что тут есть, мы с вами обсудили в той или иной
степени подробности.
Единственное, что было не рассмотрено до сегодняшней
лекции, это вот это. Ну вот я вроде попытался
объяснить, почему это работает.
Так что,
короче говоря, опять я ничего не успел,
но в следующий раз тогда продолжим.
Так, на чем мы остановились? Вот здесь.
И тогда, да, тут немножко
истории, DCP и solver.
Да, короче, остался чуть-чуть,
но ладно, не буду вас задерживать.
Надо делать перерывы тоже.
Так, все, всем спасибо
и до следующего раза. А, в следующий раз
он уже наверное будет в зуме, да?
Ну все, тогда как раз в зуме вроде обычно
проходит.
Может быть больше народу подключится.

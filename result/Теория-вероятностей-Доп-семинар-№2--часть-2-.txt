Гауссовские вектора.
Смотрите, нормальная случайная величина в одномерном случае
— это очень важное распределение на самом деле, потому что
оно в CPT встречается, ну а CPT — это очень часто используемый
объект.
И в целом оно такое хорошее, гауссовское, и захотелось
его каким-то образом обобщить на многомерный случай.
Вот.
Ну и сейчас мы, наверное, будем какую-то аналогию
между одномерным и многомерным случаем проводить.
То есть случайные величины и вот случайные вектора
гауссовские.
Значит, в одномерном случае у нас просто нормальная
случайная величина параметризовалась средним и дисперсией.
Таким вот образом.
Мы с вами в одномерном случае нормальную случайную
величину определяли как случайную величину с
такой вот плотностью.
Но у такого определения была небольшая проблема.
Ведь константы — это тоже, по сути, нормальная случайная
величина просто с нулевой дисперсией, а плотность
вот уже существовать не будет, потому что на ноль
делить плохо.
Собственно, поэтому мы придумали другое определение
для нормальных случайных величин в одномерном случае
через характеристическую функцию.
То есть мы можем называть случайную величину нормальной,
если у нее характеристическая функция имеет вот тот вид,
который мы с вами посчитали в начале семинара.
Вот такое.
И вот такое определение конвенционально, потому что
если у вас дисперсия нулевая, то у вас остается вот, получается,
Е в степени ИАТ, а это и есть просто характеристическая
функция константы.
То есть вот определение через характеристическую
функцию согласуется с константным случаем, то есть со случаем
нулевой дисперсии.
Вот.
А давайте попробуем понять, чем вот случай с константой
он как бы отличается.
Ну, по сути, он отличается тем, что у вас просто все
распределение теперь находится на множестве меры ноль,
то есть в одной точке.
У одной точки мера ноль.
А когда вы рассматриваете уже какой-нибудь многомерный
у вас уже нульмерных подпространств очень много.
Допустим, если вы рассматриваете плоскость, то вот такая
прямая, это тоже нульмерное пространство, но в ней,
по идее, какой-то гауссовский одномерный вектор может
быть определён.
Поэтому какое-то определение через плотность также бы
столкнуть с проблемами, и даже этих проблем было бы
больше.
Потому что здесь, по сути, нас только константы напрягало,
а здесь нас уже будут напрягать все нульмерные, ну, получается
все гауссовские вектора, которые определены на
множестве нулевой меры в нашем пространстве.
Поэтому нужно какое-то более вот конвенциональное
и общее определение.
И вот если мы в одномерном случае определили в итоге
хорошо нормальную случайную величину через характеристическую
функцию, в многомерном случае сразу сказали гауссовский
вектор.
Это такой вектор, у которого характеристическая функция
имеет следующий вид.
Здесь давайте небольшое замечание, мы до этого только
характеристические функции случайных величин рассматривали,
а теперь у нас ещё появляются характеристические функции
случайных векторов, определяются они вот таким вот образом.
Вот, и гауссовский вектор, это такой вектор, у которого
характеристическая функция имеет вот такой вид.
Вот это, это скалярное произведение, я так пишу.
Это, это вот здесь?
Это, ну это просто типа е.
Ну это, типа жирная буква.
Вот.
И вот, на самом деле, каноничное определение, гауссовский
вектор, это такой вектор, у которого характеристическая
функция вот такая.
Теперь давайте подумаем, что здесь ещё написано.
В одномерном случае мы с вами определяли случайную
величину, по сути, вот над ожиданием и дисперсией.
В многомерном случае примерно всё аналогично задаётся,
но теперь у вас вектор средних и матрица ковариаций.
Ну, для чего нам вектор средних?
То есть, по сути, смотрите, в одномерном случае у вас
плотность выглядела вот так.
Вот у вас получается центр распределения, а дисперсия
определяла, насколько у вас как бы разброс большой
относительно этого центра.
А когда вы в многомерном случае, вот теперь мы на
плоскость сверху смотрим, вам нужно задать точку
центра опять вашего распределения, поэтому теперь у вас
мотожидание, это как бы вектор мотожиданий, то есть,
какая-то точка, по сути, какой-то вектор.
Вот.
А дисперсия, в общем-то, это как бы вектор мотожиданий,
то есть, какая-то точка, по сути, какой-то вектор.
Вот.
А дисперсия у нас поменялась на матрицу ковариаций.
Матрица ковариаций – это такая матрица квадратная,
в которой на и懂 житом месте стоит ковариация
и, и жите компоненты вектора.
Получается, что на диагональ будет стоять дисперсии,
потому что ковариация и, и, ну, типа
и, и компоненты, и, и и компоненты – это просто
дисперсии и, и компоненты.
Вот.
Ну и по сути, вот эти 2 штуки также как и в одномерном
случае, вот эти 2 задают однозначно гауссовский
Ну, то есть, по сути, вектор А говорит, где у вас центр колокола будет, а матрица
кавариации говорит, ну, будет ли у вас этот колокол, допустим, вдоль какой-то компоненты вытянуть.
То есть, если у вас почти линейная зависимость, у вас как бы колокол будет вот такой вот приплюснутый
и вытянутый вот так вдоль вот этой оси. Понятно? Это вид сверху, если что. Вот, супер.
Что можно сказать про вектор средних? Ну, вектор средних — это просто произвольный
вектор из пространства Rn. Ну, давайте теперь у нас гауссовский вектор.
Си принадлежит Rn. Ну, в отмерном пространстве. Что такое в матрице кавариации? Ну, мы уже более-менее
определили. Это такая матричка, у которой на item-житом месте стоит кавариация кси и кси ж.
Вот. Ну, более-менее какое-то понятное определение. Что можно сказать про вектор средних? Ну, вот он
любой, а матрица кавариации — не любая. Она, понятно, что должна быть симметричная в силу того,
что у вас кавариация симметрична, но еще также она должна быть неотрицательно определена.
Неотрицательно определена — это значит, что для любого вектора х, х-транспонированная σх,
у вас больше либо равно 0. Такая штука. Ну, как это доказать? Вот это можно расписать как сумму.
У вас будет по сути сумма х и х-ж кавариация кси и кси ж. Вот. А дальше вы суммируете сначала
по первой компоненте, потом по второй. То есть пользуетесь просто белинейностью. Так, надо
что-то стереть. Это не так важно, поэтому я не на новой доске пишу. Вот. Ну, вот это можно по
белинейности собрать вот в такую штуку. То есть это будет просто кавариация суммы кси и ксы кси и сумма
х-ж кси ж. Ну, а это по сути дисперсия. Ну, а дисперсия больше либо равно 0. Ну, все,
мы с вами доказали, что вот эта сигма — неотрицательно определенная матричка. Вот. То есть
в одномерном случае у нас были какие-то похожие ограничения, то есть у нас среднее могло быть
любым, а дисперсия должна была быть больше 0. Ну, больше либо равно 0. Так и вот в многомерном
случае у нас вот этот вектор любой, а вот эта матричка должна быть неотрицательно
определенная. То есть тоже какое-то обобщение одномерного случая на многомерный случай.
Окей. Так, это я сотру. Не так интересно. Дальше. Задали мы, значит, с вами через х функции,
поняли, что такое H, что такое сигма. У гауссовского вектора на самом деле есть
и эквивалентное определение, потому что вот такое определение через характеристическую функцию на
самом деле мало каких-то полезных свойств может нам дать о гауссовском векторе. Поэтому у гауссовского
вектора есть еще два эквивалентных определения. Первое, значит, смотрите, самое главное определение
вот такое. То есть вектор называется гауссовским, если его характеристическая функция вот такая.
Соответственно, эквивалентное определение, давайте, эквивалентное определение два. Получается,
если с какими-то параметрами. То есть линейная комбинация компонент гауссовского вектора,
вот здесь мы взяли скалярное произведение, то есть по сути мы с вами взяли просто линейную
комбинацию компонентов вот этого вектора с какими-то константами, с какими-то коэффициентами.
А если вот любая линейная комбинация имеет нормальное распределение, ну вот здесь константа
тоже считается нормальным распределением. И второе эквивалентное определение через
преобразование стандартных независимых нормальных случайных величин. Сейчас мы это аккуратно
запишем. То есть, если существует матричка А, размера получается м на м, и вектор В,
который просто будет из РН такой, что кси представима вот в таком вот виде. Вот. Где тета,
тета М. Независимая, так здесь надо стереть, иначе видно не будет. Независимая стандартная
случайная величина нормальная. То есть, как бы это какое-то линейное преобразование независимых
стандартных нормальных случайных величин. То есть, еще раз, гауссский вектор это такой вектор,
у которого характеристическая функция вот такая первое определение. Второе, любая линейная комбинация
его компонент имеет нормальное распределение. И третье определение, что вот ваш вектор на самом
деле был получен путем того, что вот взяли независимые нормальные случайные величины
стандартные, домножили на матричку какую-то слева и добавили какой-то вектор констант. Вот. Тогда
это тоже тогда ксигаусский вектор. Три эквивалентных определения. Супер. Продолжаем аналогию какую-то.
Вот здесь у нас появилось линейное преобразование какого-то вида, и хочется продолжить аналогию
с одномерным случаем. В одномерном случае, что мы с вами имели? В одномерном случае у нас было
следующее. Так, только здесь, наверное, писать будет неудобно. Ну ладно. Нет. Не пошло. Вот в одномерном
случае, если кси имела нормальное распределение с параметрами асимма квадрат, то альфа кси плюс
бета будет иметь нормальное распределение с параметрами альфа а плюс бета, запятая альфа квадрат
сигма квадрат. Вот. Ну вот. Это утверждение вызвало какие-то вопросы, но вот мы его можем... Мы сейчас
докажем в многомерном случае, а это просто частный случай одномерного. Одномерный просто частный
случай многомерного будет. И хочется понять, если у нас был теперь гауссовский вектор с
вектором средних а и с матрицей кавариации, то что можно сказать про распределение вот
линейного преобразования гауссовского вектора? Как что он будет распределен? Ну вот, на самом деле,
опять какая-то наследственность от одномерного случая будет. Ну давайте так же докажем. А утверждение.
Утверждение такое, что вот распределение будет... так, аа плюс б, а второй параметр у нас будет
ксиа-транспонировано. Вот. То есть давайте посмотрим на одномерный случай. Так же домножается вот
на эту... тут на константу, там на матричку добавляется вот этот вот сдвиг, параметр сдвига. И вот здесь
потом на а в квадрате домножали. Здесь мы домножаем как бы нашу матричку на а и на транспонируемую.
Какая-то аналогия есть. Вот. Ну давайте это докажем. Это вот утверждение. Доказательство. Давайте так же,
как и в одномерном случае, вспомним, как у нас выглядит характеристическое преобразование,
характеристическая функция линейного преобразования. Вот. По определению для случайных векторов,
это получается E в степени и акси плюс b. Вот. То есть мы просто взяли с вами определение характеристической
функции для случайного вектора, и вот здесь просто воспользовались определением для линейного
преобразования случайного вектора. Получили с вами вот такое выражение. Вот. Ну и нетрудно заметить,
что в силу линейности к вариации, которая у нас есть, у нас E в степени и bt у нас вылазит из-под
математического ожидания, и остается по сути характеристическая функция вектора акси в какой
точке? Ну, в точке A транспонированной. Это понятно? Ну, потому что, смотрите, вот у нас мы вот эту
b-щику вынесли уже за мотож, ну, за х-функцию. Осталось у нас вот такое выражение. Да? Это
эквивалентно тому, что акси транспонировано на A транспонировано на T. Ну, просто вот такое
перемножение. Ну, потому что здесь у вас что получится? Эта матричка какого-то, ну, вот,
квадратного размера умножайте на вектор. У вектора размер на стену на 1. То есть у вас здесь будет,
по сути, одна строчка. Окей? Сейчас. Значит, вот у этой штучки у нас размерность. Н1? Да, я,
по сути, A перекинул вот сюда. Да, ну, это вот делается вот таким вот нехитрым преобразованием.
Отлично. Вот. Получили вот похожее выражение, как в одномерном случае, если вспомните,
что мы делали с характеристическими функциями. Вот, и теперь мы можем с вами
посчитать характеристическую функцию линейного преобразования. Так, я пока
эквивалентное определение тогда сотру, или мне вот отсюда стереть? Давайте я отсюда сотру тогда.
Давайте с вами посчитаем характеристическую функцию. Теперь вот пусть кси, гауссовский вектор
с параметрами асима. Давайте найдем с вами характеристическую функцию вот такого вот
преобразования. Здравствуйте, Дмитрий. Вы вовремя? Вот, ну, вот просто по доказанному утверждению мы с
вами домножаем вот это на характеристическую функцию. А характеристическая функция у нас это
Е в степени ИА. В точке получается а транспонированная Т, минус пополам. Ну и что у нас здесь было? Сигма, а транспонированная Т, а транспонированная Т.
Просто применили утверждение о характеристической функции линейного преобразования вектора. Ну вот,
в гауссовском случае. Ну и теперь, если мы с вами параметры все соберем вместе, у нас получится
характеристическая функция вот такая. У нас здесь будет всего просто линейности. Так, так, так, так, так, так.
А перебрасываем вот сюда. Получается А плюс В. И вычитаем. Ну, А тоже перебрасываем. Что завершает доказательство утверждения
о том, что линейное преобразование гауссовского вектора имеет тоже гауссовские, гауссовское распределение вот с такими вот параметрами.
Вот. Ну, в одномерном случае мы примерно тоже самое проделывали. Вопросы? Ответы? Ну ладно. Так, это мы с
вами выяснили. Вопрос еще может вот такой возникнуть. Но все-таки нам иногда может
хотеться плотность гауссовского вектора получить. Ну вот, вопрос такой. А когда существует плотность
гауссовского вектора? Ну, на самом деле плотность гауссовского вектора идейна. Давайте подумаем просто.
По сути, ваше распределение должно быть такой же размерности, как и то пространство,
которое вы рассматриваете. Если это правда, то тогда у вас матрица к вариации должна быть
полного ранга. Так вот, утверждение. Утверждение. Если матрица к вариации полноранговая,
тогда существует плотность гауссовского вектора. И эта плотность имеет следующий вид.
Вот. Ну то есть, вот эта плотность на самом деле очень похожа на одномерную. У нас там тоже был
один, а корень из двух Пи сигма квадрат в знаменателе. Было? На Е в степени Х минус А в квадрате. Ну то
есть, в одномерном случае у нас плотность как выглядит? Один на корень из два Пи сигма квадрат,
Е в степени Х минус А в квадрате на два сигма в квадрате. Ну и вот опять можно какую-то
наследственность заметить. Что как бы вот у вас что-то за дисперсию, отвечающую знаменателе,
стоит. Здесь в степени одна вторая? Нет. Ну, в общем, да. На самом деле, в степени одна вторая,
если сигма квадрат положить как исходный констант. Определитель матрицы. Вот. Здесь вот как раз тоже
видите обратная матрица какая-то стоит. Вот здесь мы делили с вами на сигма в квадрате. Здесь вот как
раз-таки мы делить на матрицу не можем, поэтому мы по сути здесь домножаем на вот обратную матрицу.
И вот как видите, как бы вот одномерный случай очень как бы похож на многомерный. Где?
Сигма полноранговая. Ну, значит, у нее положительный определитель. Ну, ранг равен размеру матрицы,
значит, все столбцы линии независимы, значит, определитель не нулевой. Ну, не выраженная, да.
Ну, можно сказать полноранговая, можно сказать не выраженная. Да ладно, хорошо. Я просто люблю
уговорить полноранговая. Супер. Но плотности обычно не нужны, хотя может казаться, что нужны. На
самом деле все задачи на гауссовские виктора решаются без плотностей, по крайней мере из тех,
что у нас в программе есть. Что дальше? Так, это мы все обсудили. Про плотность мы с вами обсудили.
Вот. Ну, как бы плотность я написал, показал, что она похожа на одномерный случай, и все,
мы про это забываем. Мы больше плотности сегодня пользоваться не будем. Это нам не интересно.
А интересно нам следующее.
А интересно нам следующее. А давайте подумаем. Вот у нас есть гауссовский вектор.
И компоненты. Компоненты кси. Давайте кси катай вот так. Гауссовская. И вопрос в том,
как они связаны между собой. То есть то, что вектор гауссовский, и то, что компоненты нормальные. Ну,
нормальные, по сути. Так, товарищи, какая тут связь? Кто-нибудь может вызнает?
Ну, это неправда. Верно в правую сторону. Если вектор гауссовский, то, ну вот, давайте так.
В право верно, в лево неверно. А в общем случае, но в частном верно. То есть, смотрите, если вектор
гауссовский, почему компоненты гауссовские? Ну, потому что у нас есть вот это определение, что любая
линейная комбинация имеет нормальное распределение. Ну, тогда если вы возьмете вот этот вектор с
единичкой на том месте, которое вас интересует, то у вас, по сути, вот в этой линейной комбинации
останется только кси катая компонента. И по вот этому определению эквивалентному у вас будет как
раз-таки нормальная случайно величина. Поэтому из того, что вектор гауссовский, следует, что, ну,
каждая его компонента гауссовская. Да, это следует, это следует вот из третьего определения. Так. Хотел
что-то написать? Справа, ну да. Каждая компонента гауссовская, гауссовская, просто в определении 2,
λ равняется много нольков, потом единичка на катом месте, и потом еще много нольков.
А в обратную сторону. Вот тут надо два случая рассмотреть. Ну, давайте в общем случае сначала.
В общем случае это неправда. Контр-пример. Рассмотрим случайную величину кси, имеющую распределение
нормальное с параметами 0,1, и случайную величину это, которая просто принимает значение плюс-минус 1,
с вероятностью одна вторая. Утверждение. Случайная величина кси, это имеет нормальное распределение с
параметами 0,1. Ну, они независимы. Утверждение. Кси, это имеет нормальное распределение с параметами
Доказательство. Доказательство не сложное, вероятность того, что кси эта меньше либо равно x,
Просто по формуле полной вероятности это одна вторая на вероятность того, что кси меньше либо равно x,
Плюс 1 вторая на вероятность того что, кси больше либо равно-минус x. В силу того, что распределение
асимметрично, эти штуки равны, и это у нас просто вероятность того, что
кси меньше либо равно х.
Таким образом, функция распределения ксен совпадает с функцией распределения исходной случайно-вечной.
Значит, если у кси было распределение n01, значит и у кси тет такое же распределение n01.
Ну, этот значок рассмотрим. Это глаз, масоны.
Вот.
Собственно, короче, ничего содержательного.
Так.
Смотрите, с учетом этого, давайте теперь рассмотрим с вами вектор.
Рассмотрим тогда вектор. Давайте вот сюда перейду.
Вот такой. Из кси и кси это.
Что мы можем сказать про его компоненты?
Ну, каждый компонент имеет гауссовское распределение. Ну, то есть нормальное распределение. Потому что кси по условиям n01, кси это тоже имеет
распределение n01 по
повод доказанному утверждению.
Но это не гауссовский вектор. Ну, давайте просто получим противоречие с каким-нибудь
определением. Давайте поломаем определение номер два.
Любая линейная комбинация должна иметь нормальное распределение. Ну, давайте просто возьмем сумму.
Компонент.
Ну, то есть, по сути, линейную комбинацию с 1,1.
А кси вынестица у нас будет 1 плюс 1.
Вот. Ну, и в силу того, что они независимы, это с вероятностью 1,2
ноль.
И с вероятностью 1,2
это нормальная случайная величина с какими-то параметрами. Ну, видимо,
0,2.
Сейчас мы на двоечку домножаем.
Значит, ну, 0,4.
Потому что 2 в квадрате домножается. Ну а понятно, что это, ну, не нормальная случайная величина, потому что есть точка, у которой положительная мера.
У нас это либо константа. Ну, то есть, на нормальной случайной величины у нас либо константа, ну, тогда вся вероятность должна быть в одной точке
сконцентрирована. Либо, это уже, честно, непрерывная, абсолютно непрерывная случайная величина, тогда она вся должна быть
Нормальная. А вот это какая-то смесь дискретной части непрерывной, так что это нам не подходит, и это ненормальная случайная величина
следовательна из того, что
компоненты гауссовские не следуют, в общем случае, что вектор гауссовский. Но, вот давайте так замечание напишу.
Так, соответственно, это мы с вами доказали.
Но нужно сделать
замечание.
Но если ксика
независимая, нормальная компоненты, то вектор.
Гауссовский.
Ну, это более-менее очевидно, потому что, вот, у нас есть третье определение, где по сути просто гауссовский вектор определяется как линейное преобразование
независимых стандартных нормальных случайных величин. Если вы составите вектор из независимых случайных величин, но с другими параметрами,
ну, вы с такими параметрами можете просто получить
гауссовский вектор путем того, что возьмете сначала
вектор, составленный из независимых стандартных нормальных случайных величин, а затем просто их подомножаете на что-то, чтобы у вас получились вот те
независимые нормальные случайные величины, которые у вас были, ну, с какими-то другими коэффициентами. Понятно?
То есть, если вектор гауссовский, то каждая компонента нормальная, но если каждая компонента нормальная, в общем случае, не следует, что вектор гауссовский.
Вот, но если они независимые,
если они независимы компоненты, тогда, если вы из них составите вектор, этот вектор будет гауссовским.
Вот, это важно понимать.
Ну, и вот, собственно, если мы вот,
тот пример еще вот, такая небольшая философия,
давайте еще небольшая философия сейчас будет,
минутная.
Философия заключается в следующем.
То есть, смотрите, мы с вами вот тут рассмотрели вектор кси, кси это, да, и можно рассмотреть,
допустим, вектор вот такой, кси-кси, ну, где кси у нас
N01, а
это, это, ну, вот,
такая случайная величина, что она равна плюс-минус емничке,
с вероятностью 1,2.
Вот, можем рассмотреть вот такой вектор и вот такой вектор.
То есть, вот этот вектор,
он нормальный с вероятностью 1,2 и с вероятностью 1,2, он сконцентрирован в одной точке.
То есть,
по сути, с вероятностью 1,2 вы находитесь в точке ноль.
Сейчас, как бы это аккуратно сказать. Нет, если без преобразований, то смотрите, у нас одна координата всегда нормальная, а вторая нормальная,
но со знаком. То есть, либо
то же самое, либо с минусом. То есть, получается, что все ваше распределение, оно вот на этих двух осях, как бы
находится. То есть, если вам так повезло, что этот знак
единичка просто, ну, то есть, положительный, тогда у вас, по сути, вектор кси-кси, и у вас все значения вдоль вот этой оси находятся.
Ну, то есть, у вас плотность, можно вот так нарисовать, если на нее сбоку смотреть.
Если у вас получилось так, что это выпала минус единичка, тогда у вас вектор лежит вот на этой прямой.
Вот. А если мы с вами вот такой вот вектор нарисуем,
то он всегда будет просто лежать на
вот такой прямой. И
смотрите, вот этот вектор гауссовский, а вот этот вектор не гауссовский.
То, что вот этот вектор не гауссовский, мы с вами показали. А то, что вот этот вектор гауссовский, понять несложно.
Можно просто проверить вот это вот определение
на
и вот на линиейную комбинацию, например.
любая линейная комбинация здесь будет либо 0, либо какая-то вот нормальная
сущность влечена с киндекоэффициентами.
И вот как бы в чем суть?
Во-первых, понятно, что плотности у такого вектора не будет, потому что, ну, это выраженное
распределение, в смысле оно на множестве меры ноль.
И матрица к вариации
будет как раз таки не полноранговая, то есть выраженная,
она будет вот такая.
Ну, 1, 1, 1, 1.
Поэтому у вас и плотности нет,
поэтому и все ваше распределение лежит как бы на прямой,
а не на плоскости.
Если вы сядете вот в эту точку, вот в эту точку,
и будете смотреть как бы на график плотности, виден АFl, то
либо вы просто будете рассматривать только эту прямую,
то у вас на ней как бы будет обычное одномерное нормальное распределение,
справедливо?
Вот, то есть как бы
даже если у вас матрица к вариации выраженная,
на самом деле это просто нормальное распределение, но в подпроспранвсе
меньше размерности.
ну просто в подпроспрансе каком-то.
Это все еще нормальное распределение. Все еще гал officialsский вектор, но меньше размерности.
А вот в чем здесь противоречие?
В тем, что вот как вы не смотрите, но у вас никогда не будет гауссовский вектор, даже вот на меньшем подпространстве,
который вот таким вот крестиком как бы располагается.
Идеи непонятны в чем отличие?
Между, ну, неполноранговым вектором и вообще не гауссовским вектором.
Или вы меня пугаете?
А в суть в том, что мы тихо не можем бы к пространству вот это нормально?
Ну то есть смотрите вот это случайная величина, но вот этот вектор,
он нормальный, просто если вы вот вторую лишнюю координату берете,
это просто продублирование координации, на самом деле это просто вот...
как бы здесь избыточная информация какая-то есть,
но это все еще нормальная случайная величина, просто меньше подпространства.
А вот это не в каком меньшем подпространстве не является нормальной случайной величиной,
потому что ваша нормальная случайная личность не может как бы вот на таких вот подпроссансовых располагаться никогда.
То есть вот здесь вы садитесь в эту точку и смотрите вот на график плотности и увидите перед собой такой красивый график плотности, да?
А здесь что-то дурацкое получится.
Ну, у вас как бы плотность и вот вдоль этой прямой, и вдоль вот этой прямой, да?
Да, да, да, да.
Потому что вот это подпространство, которое мы рассматриваем, оно действительно, оно не является подпространством.
То есть если мы рассматриваем с вами область из вот этих двух прямых, оно не является линиейным подпространством.
Понятно?
Вот это справедливо замечание.
То есть да, оно как бы меньше размерности, но оно не является подпространством, поэтому это не нормальный вектор.
Ну, не гауссовский вектор.
Так, мы близимся к задачам.
Самое важное, то, что нужно сказать перед задачей, это самое главное свойство гауссовских векторов.
За счет которого, по сути, все задачи решаются.
А...
Свойство такое...
Давай так.
Диаметр.
Диаметр.
Диаметр.
Давай так.
Теорема.
Ну, хотя это утверждение, ладно, пусть будет теорема.
А...
Вот пусть ксигауссовский вектор.
Тогда...
Компоненты независимы его...
То есть ксига.
Если они не коррелированы.
Ну, на самом деле, тогда и только тогда, когда они не коррелированы.
Ну, очевидно, что из независимости следует некоррелированность.
То есть в одну сторону очевидно, и мы даже доказывать не будем.
Гораздо более интересно то, что, смотрите, чтобы компоненты гауссовского вектора были независимы, достаточно нулевой к вариации.
То есть, как бы в общем случае, стрелочка отсюда-сюда не верна.
Но вот когда мы рассматриваем именно гауссовский вектор, именно его компоненты,
если они не коррелированы, то они уже независимы.
Доказательства.
А... доказать, на самом деле, очень просто.
Нужно просто еще одну теорему вспомнить.
Давайте так. Критерии независимости в терминах характеристических функций.
Критерии независимости в терминах характеристических функций выглядят следующим образом.
Случайные величины независимы тогда и только тогда, когда...
совместная характеристическая функция распадается в произведение характеристических функций.
То есть, что-то похожее у нас было с функциями распределений.
И еще вот такое же утверждение есть на самом деле и в терминах характеристических функций.
Доказывается вот на лекциях.
Но давайте с учетом этого докажем вот этот факт.
На самом деле, все очень просто.
Смотрите, если все ваши компоненты не коррелированы,
то у вас, на самом деле, матрица кавариаций
имеет диагональный вид.
то у вас на самом деле тогда матрица ковариаций
имеет диагональный вид.
А вот здесь везде 0. Ну просто компоненты все не коррелированы.
Значит ковариация между ними равна 0.
Почему из этого будет следовать независимость компонентов?
Ну вот воспользуемся критериями независимости.
Вспомним, как у нас выглядит характеристическая функция гауссовского вектора.
Характеристическая функция гауссовского вектора это у нас на самом деле E в степени.
Вот сейчас эскалярное произведение здесь как сумму перепишу.
А минус, так, вот так.
Да, давай здесь G сделаем.
G от 0 до N. Вот смотрите, а так как вот эта штучка диагональная,
то по сути это же тоже теперь просто сумма, а ты E в квадрате с коэффициентом sigma E в квадрате.
Ну потому что вы просто, когда вот этот векторочек будете на маточку умножать,
у вас каждая компонента просто по отдельности умножится, а потом вы просто ее еще раз умножите
на себя и по сути вот это можно переписать как сумму sigma, давайте так, GG на TG в квадрате по G от единички до N.
И в силу этого, что можно сказать? Ну дальше просто заметим, что это теперь все на самом деле
распадается в произведениях характеристических функций компонентов, потому что это все на самом деле
теперь просто произведение таких вот характеристических функций.
Ну здесь можно там для однозначности квадратики расставить, допустим, но это не обязательно.
И теперь заметьте, что вот эта эта же характеристическая функция, вот эта эта
характеристическая функция нормального распределения каждой компоненты.
То есть, у вас совместная характеристическая функция, распалась в произведениях характеристических функций.
Теперь вспоминаем, что вот мы с вами только что обсудили критерии независимости,
что случайная величина независимы тогда и только тогда, когда совместная
характеристического функция
распадается в произведение характеристических функций.
Ну и все, мы это с вами получили, что если матрица диагональная,
вот эта штучка просто распадается на сумму, и мы распадаемся в произведение.
Получили, что компоненты независимы
тогда и только тогда,
когда они не коррелированы.
Справедливо?
Справедливо.
Вот, верно, на самом деле и даже более строгое утверждение, что, смотрите, если
ваша матрица кавариаций
представима в блочно-диагональном виде,
а вот все остальное нули,
то вот эти подвиктора будут друг от друга не зависеть.
То есть мы с вами рассмотрели случаи, когда у вас просто блочно-диагональный вид,
в смысле, все элементы просто диагональные,
но если здесь у вас будут
блоки на диагонале, которые не нулевые, а все остальное нули,
то у вас вот эти подвиктора будут независимы между собой.
Ну, доказательства абсолютно аналогичны.
просто там с индексами чуть посложнее
их попереставлять, но
то же самое будет выполнено
ну и все, мы готовы решать задачу
собственно давайте ее решать
мы сначала вспомогательную задачу еще решим
небольшую
но она важна для понимания того как
третью решать
я все отправлю потом
ну да
ну а что
потом запись посмотрите если умираете
можете домой пойти отдохнуть
мы тут с Ильей и у нас это
тусовка по ходу намечается сегодня
автопати
да, отдыхаем
так, ну давайте вот такое замечание, смотрите
вот пусть у нас есть гауссовский вектор
из двух компонентов состоящий
на самом деле
кси можно представить в таком виде
кси можно представить в виде кси1 плюс кси2
такое что кси1
ну давайте так, кси2
это какая-то функция от это
а кси1
независимо с этим
утверждение
если вам дан гауссовский вектор
то вы любую его компоненту можете
представить как сумму двух компонентов
одна из которых
это просто какая-то функция от другой
компоненты
а второе, ну вот первое слагаемое, а второе
слагаемое это что-то что независит от вот
этой второй компоненты
доказательства
давайте попробуем наш
вектор кси это
давайте поймем какого вида мы будем
искать с вами кси1 и кси2
смотрите кси2 мы будем искать
в виде лямбда это
почему в таком виде потому что на
самом деле все что мы хорошо умеем делать
с гауссовскими векторами это линейные
преобразования
вот и соответственно кси2 мы будем искать
в таком виде
а кси1, ну понятно как
кси1 у нас будет
кси-кси2
просто из вот того определения
ну и вот теперь утверждается что
существует лямбда при котором кси1 и кси2
будут независимы
это первое
ну почему это так
потому что по сути все что мы сейчас с
вами делаем
это мы исходный вектор
домножаем на матричку вот такого вида
гауссовские вектора замкнуты относительно
линейных преобразований
мы это обсуждали
соответственно то что получится
во-первых получится кси1 и кси2
а во-вторых
ну на самом деле это будет
опять гауссовский вектор
и если вот у исходного гауссовского
вектора у вас
параметры были
ну какие-нибудь я не знаю
пусть будет вектор средних ноль
а матрица кавариаций
сигма
то вот у этого этот вектор тоже будет
гауссовский
ну видимо с вектором средних ноль
а
матрица кавариаций
у него вот по тому правилу который мы
обсудили поменяется то есть если вот это
матрица А
то новая матрица кавариаций у него будет
сигма отранспонирована
и утверждается что можно подобрать такой
коэффициент лямда
что вот эта матрица будет диагональна
то есть компоненты будут независимы по
предыдущему утверждению
то есть мы сделаем некоррелированные
компоненты кси1 кси2
вот и соответственно они будут
независимы по предыдущему утверждению
который мы с вами доказали
ну супер
а можно искать
можно решать какие-то уравнения искать из
них как бы
какие лямда нам нужны
но самый простой способ это
решение уравнения некоррелированности
что мы по сути хотим
мы хотим чтобы после этого
преобразования
кавариация кси1 кси2
была равна нулю
кавариация кси1
кси1 у нас это
по сути давайте перепишем это
через определение кси1 кси2
кси1 это у нас кси минус лямда это
а кси2 у нас это просто лямда это
теперь пользуясь биллинейностью кавариации
мы с вами поймем например следующее
что это кавариация, что это лямда
кавариация
кси это
а минус
лямда квадрат
кавариация это это
ну и у нас по уравнению вот это должно
все равняться нулю
отсюда либо лямда 0
но лямда 0 на самом деле нам не подходит
либо лямда
равняется кавариация
кси это
поделительно кавариация это это
ну и утверждение
что действительно мы с вами только что
взяли кси
разложили
в сумму двух компонент
одна из компонент кси1 это будет просто
кси минус лямда мы с вами подобрали
вот такое лямда
минус лямда это а вторая компонента это
просто будет лямда это
откуда брать вот эти штуки
вот то что в числе не то что в знаменателе
ну да из исходной матрицы кавариации
то есть у вас была исходная матрица
кавариации
и здесь у вас на самом деле есть и
дисперсия обеих случайных величин и их
кавариация попарная
поэтому если вот вам дана эта матрица
вы знаете это это значит находите лямда
значит умеете раскладывать любую
компоненту в сумму
зависящие от другой компоненты и
независимая вот с другой
все и это сейчас выстроили у нас задачу
собственно
собственно третью задачу
так задача 3
задача 3 следующее
где у меня условия
пусть x, y, z
гауссовский вектор
а имеющие вот такое вот распределение
вектор средних 0 и матрица кавариации
сигма
а матрица кавариации сигма нам дана
и она выглядит следующим образом 3
минус 1
1
минус 1 2 минус 1
1 минус 1 4
вот такая матрица кавариации
и нас просят найти математическое
ожидание
синус
y
а е в степени x плюс 2 z
вот здесь 2
вот задача на самом деле страшная
если ее так сходу решать
то есть первое что может прийти в голову
это
ну матрица невыраженная
можно видимо плотность найти и видимо
что-то там тройной интеграл еще
но видимо можно как-то проще
и действительно можно проще
вот при помощи той задачи которую мы
только что с вами решили
мы сейчас эту задачу в принципе
достаточно быстро решим
в чем идея
идея в том
чтобы разложить
ну вот получается когда у вас какая-то
сложная функция вот
грубо говоря здесь синус от чего-то
и
ешка от чего-то
идея в том чтобы разложить одну из вот
этих компонентов либо это либо вот эту
вот по предыдущей задачи
на что-то что зависит
ну мы будем сами раскладывать вот это
на что-то что зависит от y
и что не зависит от y
ну для чего это нужно давайте вот сейчас
план
я тогда план напишу
мы хотим с вами
x плюс 2 z
представить как что-то
зависище от y
плюс что-то не зависище от y
вот если мы так сделаем
вдруг нам получится
вдруг у нас так получится
сейчас
я так же раскладывал
да
все я так же решал просто чтобы
а
а
а
а
вот
соответственно
если мы вот так с вами разложим
вот эту сумму
то что у нас получится
у нас получится что математическое ожидание
синус
y
вот можно равенство будет написать вот здесь вот такое
это будет математическое ожидание синус y
на е в степени
и вот x плюс 2 z мы с вами разложили под
такому вот правилу
плюс что-то
и вот это
мы разложили под такому вот правилу
плюс у
и причем у у нас
независимый с y
ну на самом деле тогда
это можно разложить в произведение математических ожиданий
вот таким вот образом
ну почему
потому что у вас вот это не зависит от y
и вот это не зависит от вот этого y
соответственно вот это вот выражение e
оно не зависит от é в степени u
и соответственно мы можем тогда
воспользоваться вот таким вот свойством что
мотождание произведения равно произведению мотожданий в силу независимости
ну и все, на самом деле вот это легко считать
там есть небольшая техника, мы сейчас обсудим, какая
и вот это на самом деле легко считать
тоже небольшая техника, но
и считается несложно
так или я понятна
супер
вот
В каком виде мы будем U искать?
Ну, очевидно, в таком же виде, как и в предыдущей задаче, то есть U у нас это
x-αy
плюс 2z
Мы хотим, чтобы
U не зависело от
y
Ну, видимо, нужно опять решать уравнение несмещенности
Уравнение несмещенности такое
кавариация y
и U
равно 0
Ну, U у нас вот так определено
Ну и все, по сути нам осталось отсюда альфа найти
Опять-таки, по белинейности это все можно пораскрывать
Это, по сути, будет
кавариация
yx
минус альфа кавариация
yy
плюс 2
плюс 2 кавариации yz
И все это равно 0
Вот это
Вот это, и вот это мы с вами знаем из вот этой матрики кавариации, которая нам дана
по условию
Ну и все, отсюда альфа находится
Ну вот, я дома считал, у меня альфа получилась вот такая
Там, если что, решение будет
можно будет проверить меня
Если я нигде не наложал, то у меня альфа получилась равная минус 3 вторых
Возможно, там пересчитать нужно, где-то я может что-то
Ну смотри, кавариация yx, ну а что?
Да, там еще на двоечку
А ты теперь решаешь параллельно, да?
Спасибо
Супер
Ну вот, у меня получилась альфа минус 3 вторых
Все, соответственно, мы можем вернуться вот к этому шагу решения
И теперь, по сути, нам нужно посчитать математическое ожидание вот такое
и математическое ожидание вот такое
То есть, по сути, два интеграла по отдельности посчитать
Это, на самом деле, уже считать достаточно легко
Но нужно знать как
То есть, если не знать, как это считать, то это сложно
А вот если знать, то это легко
Давайте начнем с какого интеграла? Ну, давайте начнем с первого интеграла
Как найти математическое ожидание
sin y
на e в степени минус
Что у нас там? 3 вторых?
Как вот такое мы должны посчитать?
Есть проще метод
Рассказываю
Смотрите, во-первых, давайте
Запишем с вами
Вот мы тоже раскроем и запишем как интегральчик
с плотностью
То есть, у нас здесь будет sin y
на e в степени
минус 3 вторых y
и на плотность
плотность это у нас 1 поделить на корень
из двух π
Так, у y дисперсию можно взять из матрицы к вариации
Это 2
e в степени
Да, минус 3 вторых y, спасибо
А здесь будет
y минус
Так, ну, средняя ноль
Это все, если что, из матрицы к вариации беру
на 2 сигма квадрата, то есть тоже на 4
То есть, просто плотность случайно величины y записали
Ну и вот
от какой функции мы с вами берем мотаж?
Что можно заметить?
На самом деле, вот это можно сюда занести
И это, по сути, у нас будет просто
смещение, на самом деле, вот этой вот случайно величины y
Это будет сдвиг
Ну, почему это будет какой-то сдвиг?
Потому что на самом деле это будет просто
интеграл
по r
sin y
Там собирать можно полный квадрат какой-нибудь
А здесь у нас будет e в степени y минус 3
Это с минусом, значит, там с плюсом будет плюс 3 в квадрате
на 4
Ну, ему тут еще лишнее слагаемое добавили, поэтому нужно еще на него поделить
Ну, нужно на него еще намножить
Так, а добавили мы с вами 9 четвертых
Сейчас, подожду секундочку
Почему плюс 3?
Потому что здесь минус был
Мы вот 3 вторых y сюда переносим
И, получается, здесь будет y в квадрат плюс 6y
Ну, здесь вот минус
И вот здесь вот минус
Тогда здесь будет y плюс 3 плюс 9 четвертых
9 четвертых мы с вами здесь вычили, значит, надо намножить
Ну, вынесли ее так как константа
Вот такая штучка получилась
А это у нас что такое? Это, на самом деле,
9 четвертых математических ожиданий
Нормальные случайные величины с параметрами 0
Мат ожидания 3, минус 3
Дисперсия 2
Да, дисперсия 2
А вот, ну вот
Случайные величины вот такое распределение
От функции sin y
Вот, и теперь вопрос
Как найти им от ожидания sin y
Ну вот, если y имеет вот такое распределение?
Такой интеграл, по-честному, считать не очень приятно
Можно проще
А проще, на самом деле, это просто вспомнить характеристические функции
Мы с вами знаем, наверное, что sin y
По формуле Эйлера
Это e в степени
И получается y
Минус e в степени минус i y
На 2e
Справедливо?
Ну, наверное, мы можем все этому тоже навесить
Пока что никаких интегралов с комплексными числами у нас нет
Ну, там можно, в общем, да, ладно, будем считать
Короче, здесь мы трюками пользуемся
Вот, здесь у нас будет математическое ожидание e в степени i y
Минус математическое ожидание e в степени минус y
Теперь, заметьте, что вот это у вас, по сути, характеристические функции
Ну, похоже
В качестве t здесь нужно взять единичку, а здесь в качестве t нужно взять минус единичку
То есть, вот это на 1
А вот это еще, по сути, с минус единичка идет
И вот это это хар функция
А хар функцию вы легко можете восстановить, если вы знаете параметры
нормального распределения
Справедливо?
Ну и все
Парни, вы что, издеваетесь?
Я даю вам способ, как не считать интеграл
Вот это получается у нас характеристическая функция
Вот этой вот случайной величины в точке
Ну, 1 получается
Минус характеристическая функция
В точке минус 1
И поделить это все на 2e
Справедливо?
Ну, давайте вот это вот y штих у нас будет какое-то, видимо
Чтобы для красоты, наверное, здесь лучше вот y штих уже везде писать
Потому что мы немножко у него распределение, конечно, попортим
Это вот такой трюк, что, типа, если вам нужно таким от ожиданий считать
Чаще всего проще хар функциям обратиться
И вот хар функцию y штих вы, наверное, знаете
Хар функции y штих
Ну, вот здесь такую сношечку напишу
Характеристическая функция y штих
В точке t
Ну, это просто характеристическая функция нормальной случайной величины
Вот с такими параметрами
То есть это е в степени минус i at
Ну, то есть, видимо, 3t
Потому что средняя i at минус sigma квадрат
Ну, то есть 2t квадрат пополам
Ну, то есть, минус t квадрат
Сейчас я проверю, что у него то же самое было
Вроде да
Ну и все, и соответственно, вот ваша характеристическая функция
Берете ее в точке 1
В точке 1 минус t квадрат
Ну и давайте вот здесь равенство продолжу писать
А я уже продолжил
Так, идем дальше
Значит, вот константа, которую мы вынесли
И теперь пользуемся вот этим вот утверждением
Так, характеристическая функция в точке 1 у нас будет
Ну, видимо, так, мы делим это все на двои
Характеристическая функция в точке 1 это будет е в степени
i3 минус 1
Сейчас одну секунду, как-то у меня по-другому
Сейчас, сейчас, сейчас решаем
Минус, в точке минус 1
Здесь будет минус 3i минус 1
Да, справедливо
Дальше, что я сделал
Да, е в минус 1 мы вынесем
У нас в скобочках останется е в степени 3 минус е в степени 3
Вот, ну, заметим следующее
Что у нас на самом деле опять через косинус и синус и распишем
Е в степени 3 и вот это е в степени минус 3
Косинус у нас сократится и останется только синус трех
Ну, ок
У меня получилось, что это синус в точке а будет
Я просто в общем виде дорешивал
Сейчас давайте поймем, правильно это или неправильно
Так, так, так, так, так, так, так
Может быть и синус трех
Ну, давай посмотрим быстро
Е в степени минус 1 мы вынесли 3 минус 3
То есть у нас будет косинус 3 минус косинус 3
А здесь будет потом синус 3
Синус 3, прошу прощения, плюс синус 3
Ну, кажется, да, синус трех будет
Короче
Да, справедливо
Правда, у меня почему-то получилось, что здесь синуса
должен получиться, то есть вот этого параметра
Ничего странного, подожди
Можешь сейчас быстренько дорешать тогда и вот понять,
что здесь минусом
Но в целом этот нотограмм мы почти досчитали
А ответ получился
Ну, видимо, 5 четвертых
Ну, вот, на синус 3 или синус минус 3
У меня почему-то синус минус 3 получилось
Так, кто-нибудь может проверить решение, пожалуйста
Так, кто-нибудь может проверить решение, пожалуйста
А, одну секунду
В общем, здесь ашки вылазят, здесь минус, я вот здесь потерял
Вот в определении характеристической функции здесь вот минус 3 будет
Потому что здесь же i, a, t будет
Понятно?
Да
Вот здесь вот там же i, a, t, а это как раз минус а будет
Ну, а это минус 3, прошу прощения, значит будет минус i, 3
Да, и здесь значит будет плюс, здесь минус
И итоговый ответ с минус троечка будет
Вот такой
Все, первый интеграл мы с вами посчитали
Ответ вот такой
А как считать второй интеграл, понятно?
Так, я сейчас сразу пометку сделаю, потому что у меня
Я на четверть ошибся немножко там с коэффициентом
А это у нас задача 5 четвертых получилось
Все остальное вроде верно
Так, теперь давайте на второй множественный посмотрим
То есть на второй мотож, который нам надо посчитать
А, как посчитать его?
Ну, на самом деле здесь уже должно быть не очень сложно
Просто по определению u, мы с вами u нашли вот в таком
вот виде, то есть это математическое ожидание
E в степени x минус альфа y плюс глаза
Понятно, что вот это так, во-первых, давайте такое вот замечание
Если вас просят найти математическое ожидание E в степени случайно величина
Как это можно посчитать через то, с чем мы работали сегодня?
Ну да, кажется, что это просто характеристическая функция x
Вопрос только в какой точке?
Если на E будет минус и, видимо
Вот, то есть осталось только понять какое распределение
имеет вот эта случайная величина, которая у вас
стоит в показателе экспонента Но любая линейная комбинация
ну альфа у нас, кстати, 3 вторых, так что здесь надо
переписать Да, это нормальное распределение
Осталось только понять с какими параметрами
Сейчас найдем математическое ожидание x плюс 3 вторых, y плюс 2z
Ну так как у нас исходные мы тоже все были нулевые
по линейности это будет просто сумма нулей, то есть 0
Нужно найти дисперсию Ну самый верный способ найти
дисперсию этих штук это просто посчитать к вариации
этой штуки самой собой
Все вот это у нас, если что, есть Типа кавариация xx, кавариация
xy, все это в материке есть И я дома это посчитал
и у меня получилось, что дисперсия равна 37 вторых
В общем меня можно проверить Но идея здесь такая, раскрываем
это просто по линейности и все кавариации у вас известны
Ну не будем дальше считать, я дома не зря это считал
Вот, супер, а характеристическая функция
Не, нет, кавариация случайной величины самой собой это
дисперсия Ну просто посмотрите на определение
кавариации и на определение дисперсии
Так, сейчас справедливо все вроде бы Значит, чтобы
найти вот это математическое ожидание, значит нам нужно
характеристическую функцию Понять как выглядит характеристическая
функция вот эта случайная величина x плюс 3 вторых y
плюс 2z в точке t пока в точке t Вот, так как это имеет нормальное
распределение с параметрами 0 37 вторых, если я не ошибся
в подсчете дисперсии, тогда характеристическая функция
у нас имеет следующий вид Е в степени, ну вот, средняя
ноль, поэтому то, что с минимой частью отпадает,
минус 37 вторых в квадрате, а нет, в квадрат не надо
возводить, просто минус 37 вторых, потому что это у
нас так уже дисперсия пополам Короче, на 4, 37 четвертый,
да Вот еще одну багу у себя нашел
37 четвертых Вот
И, соответственно, мы второй интеграл с вами посчитали
t квадрат, ну мы поставляем ему, да, хорошо, давай t квадрат
И дальше мы вот эту характеристическую функцию рассматриваем в точке минус и
Ну и у нас получается, да, получается Е в степени 37 четвертых
Ну и тогда итоговый ответ
Это есть просто произведение вот тех двух интегралов,
которые мы с вами посчитали У первого интеграла у нас
получился синус минус 3 на Е в степени 5 четвертых,
а у второго у нас математическое ожидание получилось Е в
степени 37 четвертых Вот, ну это, видимо, еще как-то
даже сокращается Итого ответ синус от минус трех
умножить на Е в степени 42 четвертых, да, 21 вторая,
ну вот ответ То есть идея такая, у вас сначала какая-то
сложная функция, вы одно из слагаемых разваливаете
на независимую-зависимую компоненту с тем, что осталось,
и затем пользуетесь независимостью этих компонентов, чтобы
матожи по отдельности считать А по отдельности матожи чаще
всего какие-нибудь вот такие через хард-функции считаются
Это вот как один из способов подсчетов
Окей?
Так, ну все С характеристическими функциями мы тоже закончили
Обсудили, в принципе, все, что хотели Так, а вот здесь
у меня еще, я помню, я балгу не заметил
Теница здесь должна быть Ну и дальше все поправить
Так, ну и у нас последняя тема, которую час не подскажете
Нормально, к 12 закончим Может быть, кому-то это пригодится
Так, это все, это все Да, ну мы все более-менее разберем
сегодня, я надеюсь Ну там в зависимости, у меня 6 задач,
допустим, на контрольных обычно Ну я там придумываю
Обычно что-то, чтобы скучно не было Там, чтобы что-то
порешать было, а то Если все все решат, то не интересно
будет Так, ну и у нас последняя тема
На самом деле она очень большая Давайте мы вот ограничимся
40 минутами на ее обсуждение Поэтому детально мы, наверное,
ее прям очень рассмотреть не сможем Но как решать
задачу мы поймем однозначно Все остальное мы в принципе
достаточно подробно рассмотрели Так, а во сколько начала
у нас было?
19-20?
19-15?
Смотри, еще час Не, мы там 6 сидели тогда
Я сегодня против бить рекорды Сегодня хочу спать
Так, условные математические ожидания
Последняя тема, и у нас там будет 2 маленьких задачки
и одна вот прям хорошая задачка Прям настоящая
Как?
3 задачи?
3 задачи?
Ну 2 очень маленькие, очень короткие Держись здесь
Надо размяться
Не, а вот статистика будет у вас Вот почему по статистике
семинар шел 6 часов?
Потому что на самом деле там уже просто очень много
материала Я на самом деле даже не все рассказываю
сегодня что хотел В планах было гораздо больше
Вот, но если все рассказывать нормально, то типа курсы
становятся все сложнее И типа за час уже не расскажешь
пол семестра Такие дела
Это семинар века, он будет, я думаю до моего выпуска
это произойдет 24 часа я думаю мы будем
тервер рассказывать как-нибудь Не, надо, надо, надо как-то
это Терверство, ты там все
Так, сейчас я еще выдохну и продолжим
Ну поехали Так, условное математическое ожидание
Значит, если вы помните, то в начале мы с вами, когда
вот только с какими-то условностями знакомились, мы рассматривали
объекты вигда, то есть просто условные вероятности одного
события относительно другого Ну вот, здесь что угодно может
быть, допустим какое-нибудь событие А
Вот, ну и вы помните, что это определялось каким-нибудь
вот таким вот образом Если вероятность B больше нуля,
а если она равна нулю, тогда это определялось как угодно
Ну мы нулем обычно полагали Вот, и на самом деле если
рассмотреть вот такую функцию, у которой вот сюда любое множество
принимает, то это ведь частная мера будет
Ну потому что, грубо говоря, у вас есть нож, что все элементарных
исходов Теперь вы знаете, что у вас
произошло событие B, и вы вот здесь по сути
нормируете на вероятность этого события B
То есть теперь вы знаете, что все, что происходит, происходит
внутри вот этого вот события И на самом деле это теперь
мера, мы ее как бы вот отсюда все перенесли вот на этом
Оно чтобы Вот, такие объекты мы рассматривали
Что дальше можно быть логично рассматривать?
Ну вот, как бы, что кажется логичным Логичным, например,
может быть следующее Во-первых, вот здесь рассматривать
событие вида х и равняется к в качестве событий А
Просто какую-то дискретную случайную величину рассмотреть
И ввести условное математическое ожидание следующим образом
Вот так Ну, то есть, теперь мы здесь, вот у нас
есть какое-то событие Рассмотрим какую-нибудь случайную величину
дискретную Теперь все события,
которые мы вот сюда будем подставлять будут вот такого
вида Вот То есть на самом деле
мы теперь получили какую-то новую дискретную случайную
величину И вот у этой новой дискретной
случайной величины, которую мы вот так обозначили необычно
Можно определить математическое ожидание так же, как мы определяли
математическое ожидание до этого Только по сути у нас
у обычной случайной величины это х на вероятность того,
что все равняется х А здесь у нас при условии b
Вот И на самом деле уже в тот момент,
когда мы с вами вот такие штуки рассматривали
можно было перейти к условным математическим ожиданиям
Но вот по программе мы так не делаем обычно
Дальше Смотрите, здесь у нас вот это событие
b оно фиксировано На самом деле давайте попробуем
с вами рассмотреть в качестве множества b
событие вот такого вида Какая-то другая дискретная
случайная величина равна какой-то константе
Ну, почему не множество b?
Ну, множество b может быть любым Вот, тогда у нас на самом
деле уже появляется условное математическое ожидание
вот такого вида Получается, что все равняются х
И так, получается, это равняется х Поделить на вероятность
того, что это равняется х Вот Ну и это уже какое-то
условное математическое ожидание в дискретном случае
одной случайной величины относительно другой случайной
величины То есть если мы знаем, что там
В общем, если мы знаем, что какая-то случайная величина
приняла какое-то значение и она как-то связана вот
с этой случайной величиной То как бы вот эта вероятность
каким-то образом учитывается вот в этой случайной величине
Ну ладно, сейчас это мы аккуратно скажем чуть
позже Вот А какая мораль?
Какая у всего этого мораль?
Ну, на самом деле условное математическое ожидание
это случайная величина Понятно ли это?
Хорошо, допустим, это понятно Здравствуйте
Дальше давайте следующий переход сделаем
Ладно, про случайную величину будем чуть позже поговорим
Давайте следующий переход сделаем Заметим, что вот если
у нас в условии стоит какая-то случайная величина
Случайная величина на самом деле порождает сигма-алгебру
Просто сигма-алгебра, порожденная случайной величиной, мы
ее определяем таким вот образом Это просто про образы
всех баррельских множеств Поэтому, на самом деле, в общем
случае гораздо лучше рассматривать не условное
от ожидания относительно случайных величин А сразу
условное математическое ожидание относительно
сигма-алгебра Потому что, если это относительно
случайная величина То это относительно вот такой
сигма-алгебры, а если у вас какая-то другая сигма-алгебра
То как бы это и тот случай захватится
То есть давайте теперь рассматривать УМО относительно Сигма-Алгебры.
То есть это не то чтобы более общее, но вот какой-то более широкий взгляд на условные математические ожидания.
Просто если у вас в условии стоит другая случайная величина, она порождает Сигма-Алгебру,
и соответственно это будет просто каким-то частным случаем того, что мы сейчас будем делать.
Вот, и давайте определение дадим уже.
Давайте дадим определение.
Итак, пусть у нас есть кси, это случайная величина, на вероятностном пространстве Омега ФП.
Пусть мы с вами рассматриваем какую-то под Сигма-Алгебру В, то есть она меньше.
Тогда УМО, тогда случайная величина ЭТО, которая обычно еще обозначается вот таким вот образом.
То есть как бы это разное обозначение, мы ее обозначим ЭТО, она вот обычно обозначается вот так.
Называется УМО, если выполнено два свойства.
Первая. Вот эта штучка является измеримой относительно вот этой меньше Сигма-Алгебры, это важно, мы скоро поймем почему.
И второе выполнено интегральное свойство.
Интегральное свойство заключается в следующем, что если мы с вами считаем математическое ожидание кси на индикатор А, вот что за события А мы сейчас поймем, они должны совпадать с математическими ожиданиями ЭТО на индикатор А, где А принадлежит меньше Сигма-Алгебры.
Так, сейчас, про оба свойства мы поговорим.
Сначала ответим на вопрос, когда существует УМО.
Ну вот на самом деле, если существует конечный абсолютный момент, то есть утверждение, если математическое ожидание модуля кси меньше бесконечного, то это значит, что у нас есть конечный абсолютный момент.
Модуля кси меньше бесконечности, тогда УМО существует и почти, наверное, единственно. То есть оно существует и единственно, почти, наверное.
То есть мы разобрались, что такие объекты действительно существуют чаще всего, если с мат. ожиданиями все хорошо.
А во-вторых, что УМО, ну вот это объект, который должен вылитворять УМО таким свойством.
Дальше я предлагаю посмотреть на примерчик.
И на этом примере понять, в чем суть.
Да, потому что это на самом деле Сигма-Алгебра, порожденная случайно-величной.
Это в принципе то же самое.
Давайте рассмотрим с вами вот такой пример.
Пусть у нас Омега, это просто четыре точки.
Ну все равновероятны пусть будут, то есть Сигма-Алгебра это у нас просто будет в два степени Омега.
А мера, ну равновероятна. То есть мера всех точек.
Равна одной четверти.
И собственно предлагается рассмотреть под Сигма-Алгебру вот такую.
Здесь у вас восемь, так сколько здесь у вас элементов получается?
Здесь у вас 16 элементов, а вот здесь у нас будет 4 элемента.
будет 4 элемента всего. И вот предлагается вот такую
подсигма-алгебру, в этой сигма-алгебре рассмотреть.
Вот. И нужно еще какую-нибудь случайную величину кси
задать. Ну давайте зададим с вами кси от омегаитова
ну просто равно 1. То есть от омега 1 это будет 1, от омега
2 это будет 2, от омега 3 это будет 3, от омега 4 это будет
4. Давайте рисуночку сделаем. Рисуночка такой. То есть
получается наша омега 1, вот наша омега 2, вот наша
омега 3, вот наша омега 4. Четыре точки. Ну и получается
наша кси их переводит, то есть они все равно вероятны
между собой, на них вероятность одинаково задана. И наша
кси переводит их в 3, в 2, в 1 и в 4 соответственно.
И теперь вот наша под сигasst沒關係 по сути какие
под множество здесь рассматривает. То есть в исходной сиг vér
алгебре у нас здесь были множество всех этих 4-х точек.
А вот в этой под сигassed алгебре есть множество,
омега1, омега4. Такое множество есть. Ну пустое множество.
Вот такое множество. Ну и все. Ну в смысле, и все 4 точки.
Окей? И теперь, в чем суть?
Что такое условное математическое ожидание?
X относительно
вот данной сигма-алгебры.
Это какая-то случайная величина, которая как бы усредняет вот на этих...
на этих множествах, которые на самом деле...
Ну вот эта сигма-алгебра, она более крупная, в ней меньше мелких элементов, ну вот это видно. То есть здесь у вас просто 16 элементов, там и
одна точка есть, и всевозможные... В общем, здесь все комбинации, точка есть, а здесь вот только
сигма-алгебра более крупная, она меньше элементов в себе содержит. Вот. Она как бы должна
с точки зрения определения, ну, во-первых, быть измеримой, но проблемы с измеримостью в дискретном случае практически не бывает, там это чуть позже обсудим,
то есть пока что измеримость не совсем существенна,
но она должна как бы с точки зрения математического ожидания
совпадать с исходной случайной величиной. То есть давайте посмотрим на интегральное свойство. О чем говорит интегральное свойство?
Интегральное свойство говорит о том, что математическое ожидание
кси
на индикатор А должно равняться математическому ожиданию это на индикатор А.
Любого А
принадлежащая вот эта вот более крупная сигма-алгебра, ну, вот эта.
То есть, грубо говоря,
с точки зрения средних значений каких-то
они совпадают.
То есть, грубо говоря, если у вас есть, вот пример можно какой-нибудь другой еще придумать, что если у вас вот выпадает кубик,
а под сигма-алгебр можно рассмотреть, например, четные числа и отдельно нечетные, то средние значения у вас будут отличаться.
Как бы, ну ладно, я думаю это понятно.
То есть, если вы знаете, что у вас выпали четные значения, то у вас
средняя, как бы это, во-первых, у вас средняя это будет теперь случайная величина, то есть у мой это случайная величина, потому что
если вы, допустим, предположили, что выпали четные числа, то тогда у вас
математическое ожидание равно, ну сколько там, два
плюс четыре, плюс шесть?
Да, четыре. А если выпало один, три, пять?
Три. Вот. Соответственно, почему это случайная величина?
Потому что если у вас выпадут четные числа, то у вас, грубо говоря, значение вашего ума это
четыре, а если выпадает нечетное число, ну то есть вы знаете, что выпало нечетное число, то у вас, как бы, значение три.
Вот. И сейчас вот, что такое интегральное свойство, чуть-чуть позже скажем.
Но идея в чем? Идея в том, что у вас есть информация не вся. То есть, на самом деле, сигма-алгебры
это, на самом деле, какой-то аналог той информации, которую вы владеете об эксперименте.
То есть, вот исходная сигма-алгебра, она все говорила. Она говорила, вот, выпала
единичка,
или выпала двойка, или выпала двойка, или тройка. То есть, она очень точно могла информацию дать вам об эксперименте.
Вот эта более бедная сигма-алгебра, она может вам только более грубые предсказания дать. То есть, она может сказать, что выпало либо
единичка, либо четверка, либо двойка, либо тройка. То есть, выпало только четное число.
Вот. И с этой точки зрения,
условное математическое ожидание, это как бы
лучший предсказатель значения
с точки зрения того, что у вас вот информации меньше, то есть, чем вы хотели, у вас информация как бы ограниченное количество,
не вся. И вот, УМО, это на самом деле
наилучший предсказатель в средне квадратичном подходе, но это на статистике будет сказано. То есть, наилучшие предсказания,
если вы знаете, вот, не всю информацию об эксперименте.
И давайте вот на этом примере попробуем понять, какое распределение у нас будет у условного математического ожидания.
То есть, мы уже с вами поняли, что на самом деле это у вас определено, почти наверно, единственное.
То есть, на самом деле, за счет вот интегрального свойства измеримости, это сдается однозначно.
Во-первых, это у нас
Это у нас это случайная величина в нашем случае, действует изомегабар.
Вот, вот, вот, вот.
И вот, вот, вот, вот, вот.
И вот, вот, вот.
Вот, вот, вот.
Вот, вот, вот.
Вот, вот.
Случайная величина в нашем случае действует из-за мегаметра.
Давайте поймем, какое распределение у нее будет.
То есть у нас у кси было равномерное распределение на этих четырех точках,
то есть это было равномерное распределение на значениях 1, 2, 3, 4.
Почему это действует из-за мегаметра, а не из-за...
Случайная величина – это всегда отображение измеримое.
Это не ограничено на сульную систему?
Нет.
Во-первых, просто потому что у вас всегда случайная величина –
это измеримое отображение из пространства элементарных исходов В.
Просто грубо говоря, оно у вас не умеет отличать вот это значение от вот этого.
А, если вот вы его понимаете, у него в сульной сигналке в ренте нет...
А, так может быть, у нас все есть пробилы.
Потому что в аппаратке там дополнение не может привести туда.
Понятно?
Вот, ну и давайте поймем просто, какие значения она принимает.
То есть нам нужно... Вот утверждается, что с учетом интегрального свойства
и определения исходной случайной величины х, мы можем с вами распределение это
однозначно восстановить.
Ну, распределение это – это нам, по сути, вероятности омег мы с вами знаем.
Нам нужно понять, какие значения у них будут.
То есть смотрите, какие значения у нас будут.
То есть смотрите, у омег у всех одинаковая вероятность 1 четверть.
Вопрос, какие значения ваша случайная величина на этих омегах будет принимать.
Ну вот, ответ на этот вопрос помогает найти интегральное свойство.
Ну, смотрите, у нас какие множества а есть вот в этой мелкой сигма-алгебре?
Пустое множество, наверное, нельзя рассматривать.
Ну, здесь будет просто ноль.
У нас есть множество, например, мы там рассматривали с вами омега 1, омега 4.
С точки зрения интегрального свойства, математическое ожидание ψ
на индикатор давайте вот это вот событие А обозначим.
Должно совпадать с математическим ожиданием это на индикатор А.
Ну, это просто по определению условного математического ожидания.
Давайте поймем, чему равно то, что слева написано.
То, что слева написано, чему равно?
Ну, это просто сумма значений с вероятностями А.
Ну, и только те омеги нам подходят, которые попадают сюда.
То есть значение это у нас будет в силу того, что мы с вами определили случайно величину,
что она омега и тому сопоставляет ита.
То есть это будет единичка умножить на его вероятность на 1 четверть плюс А4 умножить на 1 четверть.
Понятно?
Вот так.
Что у нас вот здесь записано?
Здесь у нас записано с вами, ну, вот какое-то неизвестное.
Это омега 1 на 1 четверть.
Плюс это омега 4 тоже на 1 четверть.
Ну, и как видите, они как бы должны быть равны.
То есть вот здесь тоже можно на крайность поставить.
Ну, и на 1 четверту, наверное, сейчас мы можем сократить.
Вот.
Одну секундочку.
5 равняется это омега 1 плюс это омега 2.
Окей?
То есть смотрите, еще раз.
У нас вот здесь с вами, так, где, где, где, где, так, сейчас.
Вот 1,4.
В среднем оно что здесь выпадает?
2,5?
В силу равновероятности у вас вот этот исход и вот этот они как бы равновероятны.
Ну, отсюда на самом деле у вас это омега 1 совпадает с это омега 2.
И равняется 2,5.
Почему?
А, они омега 4?
Да, да, прошу прощения, они омега 4.
Да.
Я, я, я описался.
Идеи непонятно?
Почему они непонятны?
Потому что они равновероятны и с точки зрения нашей сигма алгебры они неотличимы.
То есть это по сути один и тот же элементарный исход.
Сейчас, а как у нас определяется это вообще?
Вообще, то есть, да.
А предъявление у них есть?
Еще раз.
Это, это просто условно-математическое ожидание и оно должно удовлетворять двум условиям.
Оно является сигмой измеримой?
Так.
А что оно означает, что там вот этот равенство, что оно равнов, там оно ожидание?
Да, сейчас, сейчас я скажу.
Вот это равенство?
Нет, что оно равнов, там оно ожидание, кси при условных условиях.
Это просто обозначение.
Уму вот так обычно обозначают, но мы просто, чтобы не писать вот такую конструкцию, мы обозначили вот эту случайную величину через это.
Окей.
А почему это случайная величина?
Мы пообщаем, как играют.
Чуть-чуть позже, хорошо скажу.
Идеально, потому что, смотри, у тебя значение ее, ну то есть это как бы среднее значение при условии того, что что-то произошло.
Справедливо?
Да.
То есть у тебя среднее значение меняется от исхода.
Видимо, там есть какая-то зависимость от исхода, следовательно, ну это грубо говоря случайная величина.
Более детально чуть позже скажу.
Вот.
А почему у вас значение должно быть равное?
То есть как бы такой вопрос может возникнуть.
Ну смотрите, вот ладно, здесь как раз таки стреляет измеримость.
Предположим, что они не равны.
Предположим, что θ от ω1 равняется a,
и это не равняется b, которому равняется θ от ω4.
Да?
По первому условию у нас функция должна быть измерима относительно меньше бедной фигмы алгебры.
Но тогда возьми прообраз
от a.
Прообраз от a это будет только ω1.
Потому что вот это принимает другое значение.
А ω1 у нас уже не лежит вот в той фигма алгебре, которую мы с вами определили.
Понятно?
Непонятно.
Ну то есть она не в субъектах значениях.
Да, то есть они у вас, грубо говоря, на одном и том же множестве разбиения, они должны принимать одно и то же значение.
Потому что если они будут принимать у вас разные значения,
то у вас, грубо говоря, прообраз должен еще побиться,
а у вас и так уже как бы мы разбиение с вами взяли.
Мы сейчас это чуть-чуть аккуратнее докажем,
подзадачка будет небольшая, мы это аккуратнее покажем.
Но на самом деле они должны принимать одинаковое значение на областях разбиения.
Да.
Поэтому на самом деле вот это они равны
двум с половиной.
И что в принципе соотносится с тем, что мы с вами ожидали.
Потому что, смотрите, если выпадает Омега-1, у нас случайная величина возвращает единичку.
Если выпадает Омега-4, то наша случайная величина возвращает четверку.
Значит, в среднем, если мы знаем, что произошло либо это, либо это,
лучшее предсказание это два с половиной.
Потому что оно ровно посередине между вот этим и вот этим.
И, соответственно, оно будет как бы с наименьшей ошибкой
вне зависимости от того, какой на самом деле Омега-1 или Омега-4 выпал.
Окей?
Ну и здесь на самом деле несложно понять, что здесь у нас 2 и 3.
Здесь тоже будет 2 с половиной.
Вот ровно по аналогичной, все если аналогично проделать, вы поймете это.
Можно еще там пример какой-нибудь, ну сейчас не будем рассматривать.
Например, вот с игральной костью тоже можно рассмотреть.
Какие-нибудь дискретные примеры очень хорошо заходят.
То есть, грубо говоря, за счет измеримости, то есть, ладно, измеримость на самом деле даже в дискретном случае существенна.
Это первое.
А второе, за счет интегрального свойства мы с вами восстановили значение,
ну то есть вот здесь мы уже можем написать, что это 2 с половиной.
И вот здесь, что это 2 с половиной.
Мы с вами восстановили распределение вот этой случайной величины.
То есть, чисто за счет вот тех двух условий, которые в определении даны.
Ну и на самом деле, условно-математическое ожидание существует единственно почти наверно.
Там.
То есть, это единственная случайная величина, которая сюда подходит на такую роль.
В том числе для недискретных.
В том числе для недискретных, то есть, да, это в общем случае верно.
Так, ну теперь мотивация понятна, да?
То есть, грубо говоря, условно-мат ожидание это какое-то, это предсказание,
при условии того, что у вас не полная информация о результатах эксперимента, а ограниченная.
Вот.
Ну и вроде нормальный пример рассмотрели.
Собственно все, давайте теперь, какой-то гуманитарную часть мы закончили.
Теперь можно более содержательные вещи какие-то порешать.
Прям совсем содержательные.
Да, мы сейчас этой докажем.
Это как раз наша первая задача.
Как раз таки наша первая задача вот такая.
Вот пусть у нас...
Так, задача.
Пусть у нас вот эта сигма алгебра, которая в условии стоит,
является сигма алгебры, порожденной разбиением.
То есть, сигма алгебра, вот, порожденная разбиением,
это вот ровно на самом деле какое-то более общее описание того случая, которое у нас было.
Потому что у нас по сути какое разбиение порождало?
Ну вот эти две точки и вот эти две точки.
Это разбиение всего пространства.
Ну потому что объединение их всех дает все пространство, и они не пересекаются.
Давайте вот теперь с вами что-то более общее скажем.
И пусть, теперь пусть у нас вот эта вот условная сигма алгебра,
которая в условии стоит, порождена разбиением.
Ну оно может не счетное, оно может быть не счетное.
Прошу прощения, оно может быть счетное.
Не счетное это плохо, а вот счетное оно может быть.
Ну и вероятность каждого такого множества разбиения больше нуля.
Ну это просто такое техническое требование, чтобы у нас там с условной вероятностью вопроса не возникло.
И вот утверждается, что в дискретном случае, когда мы строим УМО относительно разбиения,
мы можем с вами вот явно задать УМО.
Ну то есть явно задать ту случайную величину, которую мы с вами рассматривали.
В прошлой задаче мы ее вот это обозначали.
Вот, ну теперь можно так записать.
То есть это неважно, это в моем повествовании это абсолютно эквивалентные записи.
Ну вот здесь n это как бы какой-то индекс, это не натуральные числа, а какой-то индекс.
Давайте тогда n от единички до n, так будет лучше.
Вот.
То есть смотрите, в случае разбиения мы можем с вами понять, как вот явным образом выглядит наше условное математическое ожидание.
Это мы сейчас докажем.
То есть идея в чем?
То есть ну вот это функция ОТОМИГА, то есть у нас какая-то случайная величина,
а у нас есть вот эта функция ОТОМИГА,
а у нас только одно слагаемое из этой суммы выживает, то в которое наше ОМИГА попало.
И в нем у нас значение константное, вот такое, ну и как Илья сказал, это усреднение, просто еще нормированное на меру этого множества.
Доказательства?
Первое.
В силу того, что, давайте просто проверим, давайте попробуем найти условное математическое ожидание, чтобы оно удовлетворяло определению условного математического ожидания.
То есть вот это у нас есть.
В силу того, что, давайте просто проверим, давайте попробуем найти условное математическое ожидание, чтобы оно удовлетворяло определение условного математического ожидания.
То есть очевидно, что это представимо вот в таком виде.
Какая-то константа.
Мы не знаем какая константа.
Но точно оно представимо вот в таком виде.
Да, g порождена, давайте вот так, в сигму алгебру, порожденное вот таким разбиением.
Спасибо за замечание.
Час позднее уже.
Час позднее уже.
Шарики за ролики закатываются.
Сейчас мы скажем, сейчас мы это скажем.
Смотрите, то есть утверждается, что у mo, какое бы оно ни было, оно на самом деле это константа на индикатор множестве из разбиения.
Как раз таки просто такая же логика, как у нас была до этого.
Ну пусть не так.
То есть пусть существует, доказательство,
а неравная b,
ну такое, что вот на одном dn у вас ваше это
вещество.
То есть вот пусть у вас есть какие-то элементарные исходы,
которые принадлежат одному множеству из разбиения,
значения. То есть она не константа, она ADN. То есть принимает два разных значения. Но у нас же
должна быть измеримость. А что такое измеримость? Это значит, что прообраз
должен принадлежать сигма-алгебре, порожденный... Короче, измеримость должна быть относительно
вот такого чего-то. А значит, самые маленькие элементы у нас это здесь разбиение, ну вот,
элементы разбиения. То есть к какому-то разбиению он должен принадлежать. То есть вот это множество,
оно должно быть объединением элементов разбиения, вот этих вот. Но если у нас так получилось,
что у нас ADN, грубо говоря, нужно еще поделить на ту часть, где у тебя принимает оно значение A и
где принимает значение B, ну, значит, вот этим разбиением оно у тебя уже не порождается,
это сигнал-алгебра. Проблема. Всё. Противоречие. С измеримостью.
Ну оно содержит какое-то под... Смотри, у тебя получается DN распадается на D1 вот так сверху,
и DN2, то есть вот здесь у тебя он принимает значение A, здесь принимает значение B. Ну и соответственно,
про образ Ашки у тебя это вот это множество. Значит, оно у тебя должно лежать вот в этой сигнал-алгебре,
а в этой сигнал-алгебре у тебя нет, потому что у тебя только вот это большое множество там лежит.
Ну всё. Ну да, окей. По рукам. С измеримостью. То есть всё, что мы сейчас делаем, это просто
проверяем определение умо. А теперь, по сути, что нам нужно сделать? Нам нужно подобрать такую
константу, чтобы выполнялось интегральное свойство в условно-математическом ожидании. Ну вот,
ровно то же самое, что мы с вами делали в том примере руками, сейчас мы более в общем виде сделаем.
То есть должно выполняться интегральное свойство. Давайте просто запишем это интегральное свойство.
Вот так. Смотрите, это мы уже поняли, нужно вот в такой форме искать. Значит, мы можем это
переписать как сумма cн-ых на индикатор дн-ых на индикатор. Так, ну здесь. Замечание. Смотрите,
вот здесь А у нас должно быть из вот этой сигнал-алгебры. Все элементы этой сигнал-алгебры
это просто объединение каких-то вот этих элементов разбиения. Значит, справедливо,
что вот интегральное свойство можно проверить только для какого-то декатова. Думаю,
что справедливо. Вот. Ну и теперь заметим, что вот здесь у нас произведение индикаторов,
соответственно выживет только один индикатор. Так, давайте я перейду. Да-да-да-да. Как раз и выживет
у нас получается цк, констант вылезет, а там будет просто мотождание индикатора, то есть вероятность
dk. Вот. Итого отсюда мы нашли с вами константу, которая нас интересует. Вот. Здесь равенство.
Следовательно, если мы с вами вспомним, что мы начинали с вами вот отсюда и пришли с вами вот
к этому, можно выразить цк. Цк, это у нас есть не что иное, как мотождание кси на индикатор
dk поделить на вероятность dk. Ну и все. Мы с вами доказали. И на самом деле вот там сейчас свойства
немножко обсудим условного математического ожидания. Там всегда все проверяется так. Если вы хотите
доказать, что что-то является условным, ну вот, что условное математическое ожидание представимо
в каком-то вот конкретном виде, то нужно проверить, что выполнено два свойства. Что выполнена первая
измеримость, а второе, чтобы интегральное свойство выполнялось. Если это выполнено,
то вы нашли умо, оно там единственное почти наверно. Окей? Супер. Так. Друзья, немного осталось.
Это мы обсудили. Это мы обсудили. Теперь немножко про основные свойства. Их 10.
Нет, доказывать не будем. Идея доказательств везде одинаковая. То есть вы проверяете просто вот
то же, что мы сейчас сделали, измеримость и... Ну и это, наверное, можно будет найти где-нибудь в
конспектах. По-любому это должно быть. Так что это не очень интересно. Но просто мы сейчас
некоторыми свойствами будем пользоваться, поэтому, наверное, хотелось бы хотя бы их
выписать. И примерно интуитивно хотя бы понимать, почему они верны, потому что на самом деле за ними
интуиция стоит какая-то. А сколько сейчас времени? О, блин. Идем на рекорд. Нормально, ничего. Идем
дальше. Так. С вашего позволения, уже час поздний, я просто выпишу свойства, буду писать, и мы будем
их обсуждать. Основные свойства условного математического ожидания. Ну, поехали. Первое. Если
кси измеримо относительно вот той вот более мелкой сигма-алгебры, которая у нас стоит в условии,
то математическое ожидание кси при условии, это просто кси. Ну там почти, наверное. Ну идея такая,
что если вам вот информация, которая в этой сигма-алгебре говорит, даёт всё как бы об
эксперименте, который кси описывает, ну получается всё. То есть если вы берёте какую-то полную сигму-алгебру,
которая измеримость, это как бы говорит о том, что они связаны между собой, и как бы вся информация,
которая содержится в этой сигму-алгебре, позволяет восстановить вот эту случайную величину, например.
И соответственно, вот в принципе логично это свойство выглядит. То есть у вас лучший предсказатель,
если вы знаете всё о случайной величине, сама случайная величина. А второе. Ну это просто линейность.
Я опускаю какие-то технические требования, но то, чтобы моторы по отдельности существовали,
вот. Но это более-менее понятно, что это просто будет какая-то вот такая штука.
Так, линейность обсудили. Ну тут обсуждать нечего. Дальше есть монотонность, что если у вас одна
случайная величина меньше либо равна другой случайной величине, то тогда и их уможки,
относительно одной и той же сигму-алгебре, будут связаны вот так. Ну тоже более-менее идейно всё
понятно? Какой-то переход с неравенствами. Так, вот дальше интересное условие, интересное
свойство. Так, это третье было. Четвертое. А если кси не зависит от вот той сигмы-алгебры,
которая стоит в условии, то у нас математическое ожидание кси при условии сигмы-алгебры вырождается
в математическое ожидание кси. Ну то есть идея такая. Информация, которая вот здесь содержится,
ничего не говорит вам о вот этой случайной величине. Тогда лучше предсказания, лучше прогноз,
это просто среднее. Ну какое есть. Вы ничего дополнительного не имеете, значит,
если вы там будете минимизировать квадратичное отклонение вашего предсказания относительно
истинного значения, то ничего лучше вот среднего нет. Ну смотри. Событие ксиприн
принадлежит какому-то баррелевскому множеству, для любого баррелевского множества.
Сиприн принадлежит баррелевскому множеству независимо с любым событием из этой сигмы-алгебры.
Да, да. Ну короче, для любого, для любого. Да. А из вот этой сигмы-алгебры? Ну то есть независимо
из двух сигмалегебров легко определяется? То есть это просто независимо из всех,
парная независимость всех элементов оттуда и оттуда. Вот. А как по случайной величине построить
сигма, ну событие какое-то? Можно вот такие рассмотреть? Да, ну это по сути прообразы,
потому что здесь вы когда омеги вычисляете, так и раз. Вот. Это более-менее понятно. Следующее
свойство. Пятое. Телескопическое свойство. Вот им мы сегодня пользоваться не будем,
оно скорее такое теоретическое, что если у вас есть вот сигма-алгебра F, в нее может быть вложено
две сигма-алгебры. Вот, допустим, одна вложена вот так, а в нее вложена сигма-алгебра вот так.
То есть вот это это вообще самая мелкая, ну наоборот самая крупная, она меньше всего
элементов содержит, относительно вот этой. Это самая мелкая, потому что она много мелких
элементов содержит, а это самая крупная. Тогда телескопическое свойство заключается в следующем.
И второе телескопическое свойство.
Выглядит точно так же. Вот. То есть грубо говоря, самая крупная сигма-алгебра,
которая меньше всего элементов содержит, то есть там только самые большие множества,
маленьких там нет, она ожирает более мелкие сигма-алгебры.
Так, ну и там еще буквально пару свойств осталось.
Так, формула полной вероятности. Если вы возьмете математическое ожидание
от условного математического ожидания, то это будет просто математическое ожидание
исходной случайной величины. Но это вроде более-менее понятно. То есть на каждом
куске мы грубо говоря предсказываем как надо, если мы их усердним, то мы получим мотож.
Вот. И осталось, ну давайте наверное вот последнее свойство, седьмое.
Которое утверждает следующее, что измеримый множитель можно вытаскивать из-под условия.
То есть если у вас из-под, вернее вот, если у вас кси в данном примере, если у вас кси
измеримо относительно условия, тогда кси можно выносить. Ну то есть про кси вы грубо
говоря все знаете, поэтому, ну поверьте мне, о ней можно не усерднять, мы просто берем ее значение.
А про это мы знаем меньше, поэтому вот ее давайте как-то усерднять и искать
А про кси мы все знаем, вытащим ее и все. Возьмем значение. Вот такие вот 7 свойств. Давайте задачка.
Дальше сейчас первая задачка будет просто поприменять эти свойства. Это самая банальная
и базовая задача. Я бы наверное отдельно отметил, что вот самым, ну сегодня нам будет больше всего
приглашаться вот это свойство, а вот это свойство. То есть относительно измеримый, независимый и вот
это свойство. И линейность нам еще пригодится. То есть вот эти четыре свойства, их как бы,
особенно сейчас на них внимание уделить, пока я не стер доски. Сейчас отру с доски и начнем решать.
То есть линейность, вот это, вот это и вот это. Так-с.
Задачка номер один. Это уже не номер один, это вторая в данном разделе.
Кси независимый сакка. Кси имеет такое же распределение как это и распределены как нормальная
случайная величина с параметрами 34. Нас просят найти условное математическое ожидание,
кси минус это в квадрате при условии. Решение. Как будем решать?
Ну это все просто на свойства. То есть смотрите, давайте сначала вот это просто раскроем то,
что внутри. Это будет кси в квадрате минус два кси, это плюс это в квадрате при условии это.
Дальше я сказал обратить внимание на линейность. То есть это у нас будет несколько
математических условных математических ожиданий. Вот таких вот. Минус два условных
математических ожидания кси это при условии это. Плюс условное математическое ожидание это в квадрат
при условии это. Справедливо? Первое раскрываем по независимости, то есть это просто будет
математическое ожидание кси в квадрате. Да, последнее по измеримости у нас раскроется.
Со вторым что делаем? Да, выносим измеримый множитель. Измеримый множитель это, его мы вынесли,
и осталось мотождание кси при условии это. Кси не зависит от это, поэтому это мотождание кси.
А последнее по измеримости, да. Ну, халява, да. Ну, можно досчитать это быстро. То есть смотрите,
у нас кси и это имеют вот такое распределение. Тогда математическое ожидание кси это 3. Это нам
сейчас пригодится. Так, и нам нужно мотождание квадрата. Ну, давайте так. Дисперсия кси это 4,
а это то же самое, что и мотождание кси в квадрате минус мотождание кси в квадрате. Вот это у нас 9.
Следовательно, математическое ожидание кси в квадрате это у нас 4 плюс 9. 13.
Вроде проверили мы все. Так, ну и все. Мы получили, что ответ это в квадрате. Минус,
так, 3. Ну, минус, видимо, 6 это. Плюс 13. Все. Ну, и вот как бы у нас получилась какая-то случайная
величина. Вот. И тут вот, наверное, интересно заметить. Следующее. Обратите внимание,
что у нас условное математическое ожидание по сути получилось функцией от условия. Если в
условии стоит случайная величина, то уможка это функция от этой, функции от условия. Потому
что тяремка есть такая. Пусть у вас есть две случайные величины. Кси является этой измеримой
тогда и только тогда, когда существует f-баррельская, такая что кси равняется f от
этой. То есть, грубо говоря, одна случайная величина измерима относительно другой,
только тогда, когда она является функцией баррельской от условия. Ну, вот так. Поэтому,
когда мы будем искать умо относительно именно случайных величин, то у нас будет получаться
функция от условия. Функция от тех случайных величин, которые стоят в условии. Так, супер,
с этим разобрались. Дальше. Почти перешли уже к последней задаче. Этот листочек рассмотрели.
Так, тот листочек вроде тоже. Супер. Вот, теперь давайте немножко про гауссовские вектора поговорим
в контексте условных математических ожиданий. Потому что, смотрите, как бы те свойства, которые
мы изучили у гауссовского вектора, а именно вот то, то свойство самое главное, которое было
связано с независимостью, на самом деле помогает решать вот задачи на умо, потому что вы вот
раскладываете составляющую на зависимую и независимую часть. Ту, которая зависит, вы по измеримости,
грубо говоря, считаете, а ту, что не зависит, вы просто в от ожиданий превращаете. Вот. То есть,
опять-таки, идея ровно та же, что и просто в гауссовских векторах изначально. Задача.
Привет, последняя. Так, задача. Пусть у нас есть гауссовский вектор. Гауссовский вектор с
параметрами. Ну, короче, вектор средний вот такой, а матрица к вариации вот такая. И у нас проще найти
условные математические ожидания вот такого вида. Так, как будем решать?
Не, матрицу подбирать не надо. Ну, надо, но не надо. Ну, смотрите, давайте так. Утверждение.
Ну, вернее так, план решения. План решения. Вот пусть у вас x,y это гауссовский вектор.
Понятно, что вот, если вы из вот этих двух компонентов смотрите вектор, то есть, первая
компонента будет x minus это, а вторая x minus это, это тоже будет гауссовский вектор, потому что это
просто линейное образование исходного гауссовского вектора. Да, идея абсолютно такая же. То есть,
если x,y у нас гауссовский вектор, и вас просят посчитать математическое ожидание x при условии y,
то делается, да, это действительно таким вот образом, чтобы вы разбиваете x на что-то, что зависит от y,
плюс что-то, что не зависит от y. Ну, вот ровно так же, как мы делали с вами до этого. Ну и все. И это,
на самом деле, очень просто дальше дорешивается, потому что это будет альфа-ю по измеримости,
а второе слагаемое, это просто математическое ожидание вот этой вот независимой составляющей.
Теперь осталось применить вот этот план к нашей задаче. И решить я, собственно. Так, решение.
Мы, видимо, хотим с вами то, что у нас вот x минус это представить в следующем виде. Альфа на
то, что в условии, то, что в условии у нас x плюс это, плюс у. У должно быть независимо с x плюс это.
Да, да. Да, да, да. Так, ну понятно, в каком виде мы будем искать. У этого у нас будет просто x
минус это, минус альфа, x плюс это. Такого вида будем искать. Вот, ну и все. И дальше,
если мы хотим независимости, нам достаточно потребовать, чтобы кавариация x плюс это у у нас
определенно вот таким вот образом была равна нулю. Ну и давайте, наверное, досчитывать не будем.
Вот понятно, что делать, да? Если мы вот это проскрываем просто по белинейности,
то мы с вами все кавари... Не, ладно, давайте засчитаем. Точно? Не, матричку никуда не надо. Все,
решаем дальше. Так, кавариация. В смысле, мы же матричку не искали. Просто вот это уравнение
решаем и находим альфа. Да, хорошо. Отсюда можно альфа найти. Вот какой-то альф отсюда получится.
Супер. И дальше, ну вот, переходим к плану решения, который у нас вот здесь был. То есть,
мы с вами рассматриваем. А, ну просто мы пишем дальше. Вот это равняется то, что нас интересует.
Х мы разложили вот так. То есть, мы теперь это считаем как математическое ожидание,
условное. Вот с тем коэффициентом, который мы нашли. Альфа на условия. То есть, здесь будет кс
плюс это. Плюс у. Ну, у мы вот в такой форме искали. Си минус это. Минус альфа на кс плюс это.
При условии. Так, а в условии у нас было кс плюс это. Ну супер. Дальше по линейности. Вот это будет
измеримо. Оно просто превратится в альфа кс плюс это. А вот это независимо, поэтому это просто будет
математическое ожидание этой штуки. Ну и дальше останется только от ожидания этой штуки посчитать,
но это понятно как делать. Это просто сумма компонент гауссского вектора. Она тоже будет
иметь нормальное какое-то распределение с каким-то параметром. Нужно будет этот параметр найти,
но если альфа знаем, то вот в от ожидания диспер все это штуки можно найти. Ну и все. А там
только мот ожидания достаточно будет. Нам же это мот ожидания выродится. Мот ожидания легко находится.
Ну вроде все, да? Не будем тогда дорешивать. Хорошо, что вы поняли как такие задачи решать.
Так, давайте ответ сейчас скажу. Что-то там на 7 делить надо было. 2 минус кс плюс это – ответ.
2 минус кс плюс это на 7. Ну вот и опять то, что мы с вами обсуждали. Ответ – это функция от
условия какая-то. Ну все замечательно. Задачу решили. Вот теперь еще немножко теории и задачи.
Вот. Но вряд ли вам вот такую задачку дадут на контрольной. Скорее всего на контрольной нужно
будет вот там что-то с плотностями поделать. С условными плотностями. Наверное, у меня нет
времени мотивировать, что такое условная плотность и для чего она нужна. Но зато я
могу быстренько аналогию какую-то провести несложно, чтобы было чуть понятнее. То есть,
помните мы в начале семинара с вами рассматривали? Ну вот мы в начале семинара рассматривали
условные математические ожидания. Вот эти обе дискретные были. А это равняется y,
как сумма, ну по сути, x. Так, на вероятность x при условии это равняется y. Мы там как-то
это мотивировали, что вот существует такой объект. И на самом деле вот мы могли с вами
умо, по сути, зная эти условные вероятности, мы могли с вами посчитать умо в дискретном случае
просто по определению условного математического ожидания. Хочется в абсолютно непрерывном случае
иметь что-то похожее, какой-то похожий объект, и вот похожий объект есть, и этот объект называется
условная плотность. То есть так же, как и в дискретном случае у нас были вероятности в непрерывном плотности,
когда обе случайные величины были дискретные, у нас здесь были вероятности, а теперь
мы с вами перейдем к плотностям условно.
Так, сейчас, наверное, нужно условную плотность все-таки аккуратно как-то ввести.
Ладно, давайте так ведем ее тогда. Условная плотность, то есть сейчас пока что вот это
немножко в сторонке стоит, условная плотность обозначается вот так. Это такая функция,
что вероятность того, что ксикрина держит b при условии того, что вторая случайная величина равна
y, это просто интегральчик по множеству b, это условие плотности.
Вот так. Ну, то есть идея какая? Грубо говоря, условная плотность — это плотность вот этой
случайной величины при условии того, что с это происходит что-то, это приняло какое-то конкретное
значение. То есть, грубо говоря, если у вас есть какое-нибудь, допустим, двумерное распределение,
и вот, допустим, одна случайная величина у вас по х, а вторая по y, тогда если вы знаете,
что вот та, что по y приняла какое-то конкретное значение, то теперь, по сути, у вас вот здесь
какая-то новая плотность. Ну, допустим, у вас какой-нибудь такой, в общем, какая плотность у вас,
поверхность в двухмерном пространстве, в трехмерном пространстве, поверхность такая,
которая удовлетворяет условиям плотности, то есть она интегрируется в единичку и не отрицательно.
Вот. И вы знаете, допустим, что у вас вот одна случайная величина приняла значение вот такое.
Тогда вы как бы делаете такой срез этой плотности, и вот, по сути, ваша новая плотность, она вот,
вот это срез, вот это вот, сейчас, вот эта поверхность двумерная. То есть эта двумерная
поверхность заметает под собой какую-то область, и вот если вы знаете, что вторая случайная величина
приняла какое-то значение, то вы, грубо говоря, режете и смотрите на этот срез, и эта плотность,
по сути, ваша при условии того, что вторая случайная величина принимает такое значение. Вот.
То есть если какое-нибудь распределение в круге посмотреть, предположим, что вы
рассматриваете какую-нибудь случайную точку в четверть окружности. Тогда, если вы знаете,
что у вас y-ковая координата приняла какое-то вот такое конкретное значение, то ваше распределение
находится теперь, значит, вот здесь. Ваша плотность, это, по сути, вот какое-то равномерное распределение
вот на этой штучке. К сечению, да? Ну да, да, да, по сути, сечение. То есть это плотность, вот,
ну да. Это теперь равномерное распределение вот на этой вот прямой. Такой вот объект. Для чего
его, собственно, вводится? Ну, чтобы, на самом деле, в непрерывном случае у нас был аналог вот такой
формулы. Ну, мы в начале ее, на самом деле, мы руками помахали в самом начале. Да, это обе случайно
величины дискретны. Да, ну, примерно до этого можно дойти, это не очень сложно. Вот. Ну, более-менее
понятно, да, что вот какое-то было условное распределение, условная случайная величина. Вот. И мы
дальше просто по определению мы тоже в дискретном случае вот так вот определили ее. Вот,
на самом деле, хочется тоже самое сделать что-то и в непрерывном случае, но в непрерывном случае
вот, вероятность как-то плохо определяет для конкретных точек. Нужно определять какие-то
плотности. Ну и поэтому, вот, вот такие объекты определяются, при каким-то офиксированном значении
просто определение плотности на самом деле. Супер! Тогда первое утверждение, которое хочется
сказать. Как искать условную плотность? Ну, очевидно, что нам условная плотность понадобится дальше,
чтобы считать условные мотожи. А если существует совместная плотность,
тогда условная плотность
есть просто совместная плотность подъедет на плотность в точке y, если плотность в точке y
больше 0, ну и ноль иначе. Ну тоже похоже очень, потому что у нас условные вероятности также
определялись по совместной вероятности поделить на вероятность условия. Вот, и предположим, что мы
с вами знаем теперь, предположим, что мы с вами теперь знаем условную плотность, как тогда искать
условное математическое ожидание? Ну вот, ответ на этот вопрос дает тоже теорема, что на самом деле,
если вот у нас есть плотность вот такая условная, то чтобы найти математическое ожидание вот такой
штучки, нужно просто ее проинтегрировать по условной плотности. Ну и это вот как раз, это ничего не
доказывает, но это завершает аналогию вот этого примера, ну вот непрерывного случая, дискретного
случая. Потому что здесь также можно функции навесить на это и на это, ну и вот здесь вот то
же самое записано. Какую-то аналогию это завершает. Ну а собственно этого я и хотел. Так, я сейчас
проверю, что я нигде не соврал. Фкс при условии да, да, кс при, так, х, у, окей, так, а то, что выше,
я говорю тоже верно. Вот, ну и все, и теперь собственно мы можем уже решать задачу. Давайте сначала мы
с вами обсудим план решения задачи вот такого вида, который у нас будет четвертым, и затем
быстренько решим последнюю задачу, и на этом все. План решения. Там будут скорее всего непрерывные
случайные величины, поэтому план решения такой. Первое. Найти совместную плотность. То есть вас
просят посчитать условное математическое ожидание f от х при условии у. Найти, значит первый шаг,
это найти совместную плотность. Вот такую. Так, совместно. Если случайные величины независимы,
то это просто произведение. Второй шаг. Найти условную плотность по вот тому первому пункту,
который написан на этой доске. Вот по вот этому. Нашли условную плотность. Третий пункт по
вот второму, вот по второму утверждению на этой доске. Необходимо взять интегральчик. Так,
это у нас будет интегральчик по f от х при условии у. Вот. И последний шаг. У нас вот здесь
было фиксированное значение, но вместо него теперь можно подставить саму случайную величину.
То есть, на примере задачи посмотрим. Замена у мало на у больше. Это все корректно работает,
если что. Ну вот такой план. План решения. Ну и давайте его применять.
А который час? Так, сейчас я подумаю. Я потом поделюсь материалами, где это можно найти.
Так, текст, текст, текст, текст, текст. Ну и собственно вот. Какие обычно задачи бывают?
Вот последняя задача, она обычно какая? Вам сначала нужно каким-то образом поупрощать
умо, а потом посчитать умо через вот условную плотность. Ну и давайте посмотрим на такую
задачу. Задача номер четыре. Ну и последняя сегодня. Так. Хорошо, что я ее решил. Звучит у нее условие так.
Найти условную плотность p от x при условии y поделить на x. Если x имеет равномерное распределение
на отрезке 0,1, а y имеет плотность 4y в кубе на индикатор. y принадлежит от 0 до 1.
Сейчас. Первый раз слышу эту песню.
Давайте решать. Осталось немного. Значит, смотрите. Первое, что у нас просят в этой
задаче, это найти вот такую условную плотность. А второе, при помощи нее найти условное
математическое ожидание xy при условии y поделить на x. Вот. Ну, заметьте, что вот в лоб такая вот
условная плотность вам не поможет найти вот то, что здесь нужно. Поэтому, на самом деле, решение.
Первое. Давайте с конца пойдем. Это же. Тут будет сложновато. Тут будет сложновато, потому что
здесь произведение, линейностью воспользоваться не можем. Можем измеримый множитель вынести. Давай
это сделаем. Понимаешь, да, как это сделать? Вот смотри. На самом деле, подсказка условий есть. Мы
должны оставить функцию от x слева, а справа должен быть y поделить на x. То есть нам нужно и от y избавиться.
То есть первое. Давайте я вот здесь равенство такое напишу. Это, на самом деле, y на x вынести и
останется математическое ожидание x в квадрате при условии y поделить на x. Да, да. Ну, понятно,
что y на x измерим относительно y на x. Да? Все, собственно. А теперь, если у нас будет условная
плотность, то мы можем ее x в квадрате помножено на нее проинтегрировать, и тогда мы найдем вот эту УМО.
Собственно, для этого нас в первом пункте просят найти сначала условную плотность. Это как бы первый
шаг. Вы ее найдете. Второй шаг. Это вы вот это вот упростите до такого состояния, чтобы можно было
воспользоваться вот тем утверждением по вычислению УМО через условную плотность. Понятно? Супер. Вроде не
сложно. Ну и, соответственно, давайте тогда первый пункт решать. Нам нужно найти с вами вот такую
условную плотность. Что нам для этого нужно сделать? Сейчас я решение свое возьму. Да, нам нужно найти
совместную плотность. То есть, в силу теоремы, которую мы обсудили, условная плотность x при условии
Ну дела. Ну дела. Я нашел ошибку в самом начале решения. Я, короче, сегодня всю ночь считал вот такую
штуку. Ну и на самом деле она сложнее получается. О, это значит у нас сейчас гораздо проще все получится.
Мы справимся. Спасибо большое. Да, независимо. То есть нам нужно найти условную плотность по вот
той теоремке, которую мы обсудили. Условная плотность через совместную плотность выражается. То
есть на самом деле в числителе у нас будет, здесь будет что у нас получается плотность x запятая y поделить на
x в точке s поделить на плотность, видимо, y поделить на x в точке t. Если плотность, ну давайте так,
на индикатор того, что p от y поделить на x в точке t больше 0 иначе 0.
Окей? Так, ну смотрите. Это был первый шаг. Второй шаг. Видимо, нам нужно найти совместную плотность.
А совместную плотность как нам найти вот таких вот двух случайных величин? Ну, во-первых,
мы с вами знаем, что совместная плотность, сейчас я посмотрю как я. Мы, во-первых, знаем с вами,
что совместная плотность исходных случайных величин, это просто произведение плотностей,
потому что они независимы. Тогда, что мы можем с вами сказать? Тогда мы можем сделать с
вами преобразование. То есть дальше вспоминаем первую половину семестра и пользуемся теоремой
о плотности преобразования. Ну, то есть нам нужно намножить на екобиан обратного
преобразования и там что-то выражать. Давайте это аккуратно проделаем. То есть мы вводим с
вами новые случайные величины у и в. У мы с вами оставляем в x, в мы кладем с вами y поделить на x.
Спасибо большое.
Дальше ищем обратное преобразование. Ну, обратное преобразование здесь очень легко ищется.
Вот, ну вы, если что, проверяйте меня, пожалуйста. Дальше мы ищем екобиан обратного преобразования.
Екобиан на обратном преобразовании это будет единичка по v0, по uv, по vu. То есть просто u будет.
Вроде справедливо. Я не соврал. Вот, ну и тогда совместная плотность
x и y поделить на x в точке st, ну в новой точке uv. Это у нас екобиан, то есть u умножить на плотность
x, x у нас это u в точке u умножить на плотность y, а y у нас это uv. Блин, ну все по-человечески
получается. У нас сейчас решение в два раза короче получится, чем у меня дома. Потому что я там перепутал
и x на y читал. Ладно, не страшно, не страшно. Так, ну вот здесь план решения, я его сейчас
сотру. В принципе мы уже все что нам нужно было практически взяли отсюда. Сейчас вот
за используем плотности вот эти, когда будем считать вот эту плотность совместную.
Так, ситуация. Давайте как-нибудь плотность совместную теперь выразим там, подставим
значение из условия. То есть у нас x при условии y поделить на x, совместная плотность,
прошу прощения, в точке uv. Так, что у нас такое u? u это у нас x, x у нас имеет какое распределение по
условию? Короче, оно от 0 до 1 лежит, поэтому модуль спокойно снимаем, потому что u будет
положительным. Дальше плотность x. Плотность x это 4x в кубе, подставляем u. То есть у нас будет
4u в кубе на индикатор того, что u принадлежит от 0 до 1. Отлично. И теперь плотность y. Плотность
y это у нас индикатор того, что u принадлежит от 0 до 1. Ну по сути, если у нас u от 0 до 1,
v от 0 до 1. Ну ладно, u принадлежит от 0 до 1. Вот. Согласны? Вроде как не наложал. Дальше следующий шаг у нас
какой? Смотрите, совместную плотность мы с вами нашли, нам нужно найти с вами плотность y поделить
на x. Для этого нам нужно, наверное, убрать лишнюю переменную, короче, выинтегрировать ее. Дальше,
так, это получается был второй шаг, третий шаг. А третий шаг следующий. Мы берем с вами и интегрируем
вот это чудо по первой координате. Так, давайте соберем все вместе. Это будет 4u в четвертой. На
индикатор u от 0 до 1. Так, а v это у нас что такое? Отношение двух положительных случайных величин.
Оно тоже положительно. Ну ладно, значит здесь будет у нас интеграл от 0 до, отсюда у нас следует,
что u меньше либо равно, чем 1 на v. До минимума из единички и 1 на v. Так, согласны?
Вроде как правда. Так, дальше, наверное, нужно случаи разобрать аккуратно. Давайте аккуратно
эти случаи рассмотрим. Если что, это мы считаем, ого, вот это экспрессия. Это если что у нас плотность
у поделить на х в точке v. Ну вот этот интегральчик у нас вроде как очевидно, что равен 5, 4 пятых в пятый
в подстановке от 0 до минимума из единички и 1 до 9. Вот, ну и соответственно плотность в точке v.
Это у нас вот два случая надо рассмотреть. Если v меньше единицы, то есть v принадлежит от 0 до 1,
тогда получается у нас вот это будет больше единички. Значит у нас верхний предел это единичка,
тогда это будет 4 пятых минус 0. Так, а если v больше единички будет, тогда здесь у нас получится,
получается 4 пятых, а 1 поделить на v в пятый. Вроде как справедливо. Так, отлично. Так, ну если что у
нас вот, кстати, сейчас здесь небольшие коллизии, вот здесь у нас st было, там uv. Давайте здесь тоже на uv поменяем,
чтобы все красиво было. Это безрадница, короче. А, ну просто условная плотность, если плотность
в знаменателе больше 0, то и так, а если она равна 0, тогда здесь 0 просто определяется. То есть по сути
это же то же самое, что на индикатор домножить. Ну то есть там просто такая системка у нас была,
если положительно, тогда так, а если 0, тогда ну 0. Ну я просто на индикатор домножил и все. Так,
ну теперь давайте попробуем с вами вот эту условную плотность уже представить в требуемом
виде. В каком-нибудь виде, в котором с ней уже можно будет работать хорошо. Ну да. Итак,
p от x при условии y поделить на x в точке uv, это у нас что такое? Это у нас что такое? А соответственно,
если v меньше 0, то это 0. А если v, видимо даже меньше и равна 0, если v принадлежит от 0 до 1,
и последнее, если у нас v будет больше либо равной 1. Нет, все красиво будет, я обещаю. Я тоже думал,
что дома не очень приятно, а потом дорешал. Вот, в общем, если от 0 до 1, то у нас в знаменателе
4 пятых просто будет. Ну то есть домножаем на 5 четвертых. А вот здесь у нас нужна совместная
плотность. А совместную плотность мы где выписывали? Вот это чудо, да, у нас? Что-то, ну да. Не густо. Так,
значит там uv четвертый, да, у нас будет. Ну давай смотреть на индикаторы. Кажется,
что половина индикаторов у нас упростится. Ну смотри, а v от 0 до 1 у нас здесь лежит,
значит, вот смотри, v от 0 до 1, значит, вот это больше 1, значит, у нас только этот индикатор
выживает. Значит, индикатор того, что u принадлежит от 0 до 1. Ну там, короче, может,
где-то скобочки нужны замкнутые включать, ну типа, то есть включать концы. Не совсем важно,
потому что множество меры 0 ничего не значит в небеременном случае, но вообще аккуратно
нужно, конечно, все расставить. Так, и если v больше или в равной 1, то у нас что получится?
Так, это у нас, на этом мы делим. Вv пятый будет в числителе, uv четвертый, ого. Так,
здесь v больше или в равно 1, значит, у нас вот этот индикатор теперь работает, то есть на самом
деле индикатор того, что u принадлежит от 0 до 1 в этой. Вот так. Ну, потому что у нас
есть 2 индикатора на u, по сути. Вот. Ну вот наша условная плотность. Ну, мы почти уже уфинишим,
да-да, это считается спокойно. Дальше. Мы хотим, наверное, посчитать. Мы помним,
что мы там упростили, и в итоге нам нужно x в квадрате считать при условии y поделить на x,
так ведь? По теореме, вот эта штука, это не что иное, как интеграл x в квадрате,
ну ладно, если x это у, то это получается будет u в квадрате, умножить на условную плотность.
Вот так. Да? В точке v. Да, прошу прощения, вот здесь нужно зафиксировать, там теоремки,
вот это значение фиксировано. Вот так. А потом мы вместо v подставляем вот ту случайную величину,
которую у нас. Да. Вот такой интегралчик нам надо посчитать. Ну давайте аккуратно посчитаем.
Да, вроде все корректно. Ладно, нормально. Давайте считать. Тут опять будет пару условий. Видимо,
если плотность нулевая, нас ничего не интересует. Вот этот случай можно не рассматривать. Теперь,
если v принадлежит от 0 до 1, ну да, там будет нулевая, то есть нас это не особо интересует. Так,
если v будет принадлежать от 0 до 1, то у нас плотность вот такая. Еще нужно u в квадрате
намножить. То есть будет 5 u в 6, то здесь будет интеграл 5 u в 6 на индикатор. Какой у нас там
индикатор выжил? А ну, это все интегрируем от 0 до 1. Так, и потом у нас еще второй интегралчик будет.
Если v больше либо равно 1, 5 u в 6. В какой вы выносим? 5. В 5 выносим. Ну, 5 в 5, видимо, выносим.
Здесь тоже можно константу вынести. 5 в 5 выносим. У нас остается u в 6. Только вопрос в пределах
интегрирования здесь будет от 1 до 1 на v. Да, при v больше либо равно 1. Соответственно,
ну, это кажется, что не сложно считается. 5 седьмых. Давайте запишем еще ду для красоты,
чтобы не подумали, что мы не образованные. В принадлежит от 0 до 1. А вот здесь у нас получится
тоже 5 седьмых. Но еще мы интегрируем это будет u в 7 на 7. Ну да, и делить на v квадрат.
Если v принадлежит. Ну, короче, если v больше нички. Вот. И теперь самое интересное. По сути ответ
надо записать. Так, это я могу уже спокойно стирать. Так, так, так, так, так, так, так, так, так.
Так. Ага. Значит, у нас по условию спрашивали, математическое ожидание x и y при условии,
при условии чего? y поделить на x? Да, мы с вами это упростили до вот такого состояния,
что это y поделить на x на условное математическое ожидание x квадрате при условии y поделить на
x. Вот. А вот это мы с вами только что посчитали. Ну, мы посчитали с вами прификсированным v. Вот.
Теперь, по сути, мы вместо v в этом ответе просто y поделить на x подставляем. Ну. И все получается.
Это теоремка есть. Ну, типа мы, ну, на самом деле это вопрос в том, что мы, смотри.
Да, мы посчитали для v, то есть мы прификсированном значении посчитали условное
математическое ожидание. То есть, если у тебя v, ну, то есть, если у тебя условия зафиксированы,
то у тебя умок какая-то константа. Ну, в данном случае. Ну, какое-то значение принимает. А теперь,
ну, как бы, у нас же должно получиться функция от условия, а в условии у нас, по идее, случайная
величина. И мы, по сути, теперь начинаем вот эту вот v фиксированную варьировать, ну, за счет того,
что мы считаем, что теперь v не фиксированная, а вот просто случайная y поделить на x. Вот. Ну,
это как бы, это показательство, это не доказательство, ничего. Там, ну, доказательство можно аккуратно,
что так можно делать. Так. Ну, и ответ. Пять седьмых, коль скоро y на x. Так, еще нужно вот на
это намножить. y поделить на x. Пять седьмых y поделить на x. Коль скоро у нас y поделить на x.
От 0 до 1 лежит. И 5 поделить на 7 на v в квадрате. Так, y поделить на x. Нужно еще,
на условии у нас y поделить на x, то есть это будет y в квадрате, x в квадрате. Да, x поделить на y.
Спасибо. x поделить на y. Коль скоро y поделить на x. Нет, здесь в квадрате, но у нас еще y на x отсюда.
И мы делим на него в квадрате, поэтому у нас все сокращается. Вот. Ну, и собственно, это вот ответ.
Нормально? Что еще за дачку? Так, а сейчас сколько 0,53? Ну ладно, нормальный такой средне...
Среднестатистический семинар получился. Нет, все, хватит. Расходимся.

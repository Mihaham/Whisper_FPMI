Всем доброго дня! Мы с вами продолжаем наш курс лекций. Прошлая лекция была у нас
достаточно короткой, потому что у нас были проблемы с экраном. В общем, мы все поправили,
здесь настройки стали лучше. Я купил новый переходник, поэтому теперь все зашибись. Это
значит, что мы с вами можем посвятить большее время компиляторам. Значит, первой половиной
лекции у нас сегодня будет, наверное, сильно знакома всем тем, кто проходил курс по формальным
языкам в ФПМИ. Точнее, в ФПМИ информатика. Потому что мы будем с вами говорить про
восходящие парсеры. А именно поговорим снова про LR, но это будет именно больше напоминание того,
что именно такой LR и каким образом его можно использовать. Если мы сегодня успеем, мы с
вами еще начнем разбирать такую интересную тему, как семантический анализ. Смотрите,
у нас будут восходящие парсеры. Это те парсеры, которые разбирают слово снизу вверх. То есть,
у нас есть входное слово, и мы будем собирать с вами наборы токенов в программатические правила.
Значит, начнем с алгоритма LR. Как он расшифровывается? LRK это алгоритм, который умеет,
во-первых, парсить слева направо слово. То есть, он и пытается выводить именно правосторонний
вывод. Почему? Потому что, если мы с вами посмотрим, каким образом у нас с вами раскрываются токены,
неужели новый мел привезли, хороший мел привезли, ура, все улучшения у нас. У нас есть входной набор
токенов, и дальше мы начинаем делать разбор. То есть, у нас возникает какое-то правило,
у нас терминал раскрывается в последствии терминала LR. Потом у нас есть второе правило,
и после этого, предположим себе, у нас строится домик. Каким образом у нас это дерево строится?
Это действие №1, это действие №2, это действие №3. Соответственно, если мы с вами будем это
делать все в обратном порядке, то вот это действие у нас будет первым, вот это действие у нас будет
вторым, а вот это действие будет третьим. Поэтому, среди раскрытия вот этого не терминала, первое
правила будет вот этим, а не вот этим. Я надеюсь это всем понятно. Хорошо, и значит у нас есть такое
понятие как kTokenLugahead, то есть мы можем посмотреть на kTokenов вперед. Зачастую нам будет
достаточно просмотра именно на один токен вперед. Хорошо, значит давайте вспомним, что такое алгоритм
LLK. Это рекурсивный разбор, который допускает просмотр на kToken вперед. Мы с вами его в
прошлый раз рассматривали. То есть мы в LLK пытаемся сразу раскрыть правила и достигнуть того,
чтобы у нас конфликтов нет. В LLK на самом деле мы откладываем раскрытие правила как можно дальше,
то есть мы стараемся сначала раскрыть самое правое правило. И здесь у нас возникает интересный
алгоритм. Давайте его еще раз вспомним. Это алгоритм перенос свертка. Сразу скажу, что мы
должны создать стартовое правило, сделать пополненную грамматику. То есть у нас с
вами из не терминала s' должно выводиться не терминал s и символ доллар. Здесь я не буду
использовать символ доллар. Как вы думаете почему? Он может быть частью языка, то есть это может
быть символ, это может быть любой оператор. Допустим, если мы говорим в Варе про замечательный
язык AR, то в нем доллар это как оператор точка. Видно везде все по-разному. Поэтому здесь мы будем
использовать специальные символы под названием EndoFort. И мы будем хранить указатель на первые
ВК токенов предложения. И дальше у нас будут четыре вида поведения в зависимости от того стека,
который у нас есть. Напомню еще раз в чем заключается алгоритм. У нас есть стэк,
в котором мы можем сделать два действия. Первое действие это прочитать букву.
Допустим, у нас есть слово ABC и есть грамматика. Так, из SAB получается, ну и давайте из BFBC. Вот у нас
есть три правила грамматики. Мы берем это у нас входное слово и давайте параллельно мы начнем
строить стэк. Мы считаем наше слово, смотрим какая следующая буква. И здесь у нас возможно
одно из двух действий. Если у нас на стэке лежит правая часть какого-то правила, то мы можем взять
правую часть этого правила заменить на левую часть этого правила. Это называется reduce. Если у нас
следующая буква хорошая, то мы можем делать операцию, то есть положить ее на стэк. Давайте как раз
для этого алгоритма проделаем эти действия. Значит первая буква A делаем shift. Дальше вторая
буква B делаем shift. Третью букву тоже считываем. Буква C получается. Мы кладем ее на стэк. А дальше
видим, что у нас на самом деле BC это правая часть правила из BFBC. А там чем можно сделать с вами?
Да, мы можем сделать reduce. То есть мы стираем правую часть правила и получаем здесь B, здесь A.
Опять видим правую часть правила, кладем на стэк S. Ну и дальше у нас следующий кривой конец
слова. По идее нам нужно разбирать эти варианты. Мы кладем конец слова и здесь мы можем сказать,
что наш слово принадлежит нашей грамматике. Это так работает алгоритм. Давайте вспомним,
собственно, те, кто проходил курс по параметрам языка, что есть проблемы этого алгоритма.
Оно неоднозначно. Мы не понимаем, в какой момент времени нам нужно делать shift операцию, а в какой
момент времени нам нужно делать операцию reduce. И из-за этого у нас могут быть конфликты. Какие
типы конфликтов у нас могут быть? У нас может быть shift-reduce-conflex. Это когда у нас с вами
возможен либо снятие следующей буквы, либо чтение, либо снятие стэка правой части, либо
reduce-reduce-conflex. Когда оно может быть? Когда у нас два правила, которые имеются, они заканчиваются
прямо в одном месте. Здесь сразу скажу, что мы на самом деле здесь рассматриваем стэк. Стэка
недостаточно. Поэтому, чтобы уйти от этой проблемы, нам нужно построить поверх всего этому такое
понятие, как ситуация. То есть мы не понимаем, где сейчас мы находимся, поэтому нам нужно по факту
хранить состояние парсинга, в котором мы находимся. Здесь если мы говорим следующую вещь, что вот эти
вот правила, которые у нас имеются, мы можем указать для каждого правила позицию, в которой
мы сейчас находимся в этой правиле. То есть когда мы находимся здесь, мы можем считать, что мы
находимся в начале разбора. Это означает, что мы можем поставить точку здесь, а если мы спускаем,
можем спуститься по вот этому не терминалу s, то точка будет стоять здесь. Мы говорим, что на самом
деле мы можем в этом автомате сказать, что у нас есть состояние s$ и s.ab. Когда мы с вами считаем
букву a, мы по факту понимаем с вами, что в этом правиле мы можем сдвинуть точку на da.
Bollar, a.b. Этого правила здесь нет. И следующее правило, которое у нас здесь есть, мы видим,
что точка b, поэтому b мы можем с вами раскрыть. Bc. Здесь дальше у нас идет переход по букве b
и переход по букве c. То есть та позиция, в которой мы с вами находимся в автомате,
показывает, где мы гипотетически можем находиться в текущем аэропорте. Вот смотрите,
интересный момент, какой возникает. Когда мы в этом алгоритме прощем слово abc, вот дошли
до сюда, то мы можем сказать, что мы стартуем из этого состояния, проходим a, b, c, находимся здесь.
Дальше, каким образом мы с вами понимаем, что мы можем сделать с вами операцию reduce в данный
момент? Да, у нас есть правило и точка, в котором находится в конце. Это означает, что мы можем
взять правую часть правила, снять ее, отмотаться по пути, которая у нас есть в автомате, пойти сюда,
сюда. Путь мы можем с вами хранить и сделать переход по букве b, по большой букве. То есть,
то где мы с вами находимся? Получаем ab. Потом мы находимся в этом правиле. У нас есть правая
часть правила, из s выводит ab. Либо, если здесь был бы переход по букве, отсюда, по любой,
то у нас был бы shift-reduce-conflict. То есть, если бы здесь был переход по букве c, мы бы не смогли
понять, что мы делаем. То есть, нам надо либо идти по букве c, либо отмотываться назад. Но,
здесь у нас нет ни однозначности, поэтому мы идем, отматываемся по букве b, по вот этому пути.
Значит, отматываемся по b, отматываемся по a, идем по s. Вот, и находимся теперь в этой ситуации.
Ну, и здесь можно сказать, что мы находимся в конце слова, да, нам ничего не надо прочитывать.
И поэтому мы можем сказать, что наше слово принимается. Это как раз так называемый алгоритм lr0.
То есть, смотрите, изначально мы сразу строим с вами таблицу. Вот эту lr. Это можно представить как
dk, либо можно представить как таблицу. И по ней делаем переходы. Значит, было доказано, что этот
алгоритм работает за линейное время на курсе формальных языков. Сейчас мы это повторять не
будем. Это не смысл нашего занятия. Вот, собственно, это будет вот такая вот таблица. То есть,
мы можем в принципе по каждому состоянию определить, какое действие у нас будет. Либо shift,
либо reduce, либо accept. Вот это состояние accept. Либо error, если мы не сможем по этому автомату куда-то
двигаться дальше. Хорошо. Вот пример lr-таблицы для грамматики. Значит, мы такие строили на курсе
на предыдущем. Скажите, кто нас сразу смущает в этой таблице? Вот что-нибудь вас смущает?
Во-первых, она разряженная сильно. Во-вторых, в ней есть дырки. Ну, это, собственно, одно и то же.
А еще? Что такое цифра? Это вот эти состояния в нашем автомате, которые занумерованы. Здесь
состояния идут от одного до двадцати трех. Собственно, что означает s? Это означает,
что мы делаем shift в определенную состояние нашего автомата. То есть, допустим, здесь у нас указано
23s16. Это значит, что если в двадцать шестом, двадцать третьем состоянии здесь используется еще
следующий символ. Если следующий символ плюс, то мы идем в 16 состоянии. Если reduce, то мы должны
откатиться по какому-то конкретному правилу. А если у нас есть пересечение в какой-то ячейке таблицы
shift и reduce, то что мы говорили с вами на курсе формальных языков? Делаем МК больше. То есть,
у нас грамматика не LR 1, LR 0 и так далее. То есть, мы можем попытаться разрешить конфликт за счет того,
что мы будем увеличивать букву. Как вы думаете, что происходит в реальных парстерах?
Когда мы строим вот эту таблицу, у нас обнаружится конфликт.
Еще. Ну, потянет, конечно. LR 2 строится, там еще более огромная таблица получится. На самом деле
вариант такой. Как действует Bizon? Он делает следующее. Если он в какой-то ячейке обнаруживает
конфликт, вы при компиляции получите warning о том, что я нашел такой конфликт. Если указать опцию
Conterexamples, которая появилась в версии Bizon, начиная с 3.6, по-моему, или с 3.8. Нет, там есть
специальная опция при компиляции. Какие-то старые версии Bizon оно не умело показывать. Ну, из-за этого
компиляция, конечно, сильно увеличивается. А runtime error у вас возникнет, когда вы реально придете в эту
ситуацию и реально у вас возникнет этот конфликт. Да-да-да, именно так, что если мы не попадаем в эту
ситуацию, ну и ладно. Да, то есть это можно считать, что в компилятор это runtime behavior.
Там LR2 нельзя заставить, там можно заставить в Bizon'е, по крайней мере, использовать JLR, Generalized LR.
То есть он попытается наоборот сделать его недетерминированным, этот автомат. А что если?
Да-да-да. Вот такое пример LR таблицы. И здесь вот это вот отдельное состояние по переходам,
по нетерминалам. Здесь есть GE, то есть это как раз те переходы по нетерминалам, которые мы делаем при
откате назад. Так, а, посмотрел все-таки, да, параметры, да. Хорошо, понятно ли вот эта вещь? Значит,
в чем еще здесь огромная проблема LR алгоритма? Почему хейтят? Ну, собственно, за размер этой таблицы.
Кажется, арифметика. Ну, не сложная грамматика в целом, да. А у нас, извините, уже 23 состояния. И таблица
23 на сколько? Ну, порядка 10. То есть вы представляете теперь, что мы находимся сейчас не в 2024 году,
да. А в те моменты времени, когда говорили, что 600 килобайт памяти хватит всем,
тогда проблема была с компиляцией этих программ. И на самом деле, если так исторически отматывать,
то как раз линковка появилась в тот момент времени, когда поняли, что, типа, все стадии
компиляции в оперативную память просто не поместить. Да, некоторые строки можно
смёржить и про это мы поговорим немного позже. Ну, это да. Да, тут есть пример работы алгоритма,
можно, значит, поставить на паузу, либо посмотреть презентация, на каким образом разбирается вот такое
выражение. A равно 7, B равно C плюс D. Господи, какая-то сложная грамматика. Ну, видимо,
оно такое позволяет. А, здесь оператор присваивания равно есть, да, получается. A7B равно C плюс,
собственно, D, которая равна A5 плюс 6, а потом ещё и D используется. Да, да, да.
Да, ну вот, да. Вот, вот она, грамматика. То есть, а что она делает? Она говорит,
identity это E, expression это либо ID, либо NUM, либо E плюс E, либо SE. То есть, видите, тут набор
стейтментов есть, и после этого мы возвращаем ещё какое-то выражение. Не поверите, если мы будем
рассматривать промежуточное представление кода, то там такие конструкции прямо явно возникнут.
То есть, если строить не ER, который LVM-овский, а вот ER, который обычно строят, там есть такой
оператор, называется ESEC. В общем, что он означает? Он означает следующее, что вы сначала вычисляете то,
что находится здесь, а потом возвращаете вот это выражение. Вот, это обычно нужно, допустим,
если вам нужен какой-то булливый флаг, который нужно посчитать после каких-то выражений. Обычно
таким образом каст от була к антуиду идёт, или наоборот. Я точно не помню. В общем, вот такая
интересная вещь. Вот он разбор. Давайте поговорим про парсер LR0. Что такое LR0 парсер? Это наивный
алгоритм, который не смотрит на следующую букву. То есть, здесь мы не опирались на следующие
буквы, которые у нас есть, и мы просто к каждому правилу добавляем маркер позиции, в которой мы
находимся. Это называется ситуацией. То есть, мы должны начать в нулевой ситуации. Из S штрих
выводится точка S, конец файла. Или, а заканчивать мы должны из S штрих перейти в S, точка конец
слова. При этом у нас на входе ничего не должно быть. Если у нас на входе что-то есть,
наверное, мы не разобрали всё правило далеко, а какой-то префикс. Здесь есть пример. Ещё раз
строим автомат. Каким образом мы его строим? Если мы делаем переход по букве, то пытаемся
продвинуть точку по правилу. Если точку продвинуть при этом не получается, мы находимся перед
не терминалом, то мы пытаемся раскрыть этот не терминал, добавить новое правило. Здесь это
тоже как раз указано. В итоге, для простой грамматики получается вот такая таблица. Давайте
посмотрим на неё внимательно. Здесь, допустим, что у нас получается? Вот здесь у нас есть переход.
Допустим, видите, здесь, возможно, зацикливание. Вот это очень важно. То есть, у нас есть переход
по открывающейся скобке. Давайте посмотрим, что у нас происходит при переходе при открывающейся
скобке. У нас здесь есть правило. f стрелочка точка, сколка открывается, l скобка закрывается.
В итоге мы продвигаем эту точку. У нас получается правило — f стрелочка ковычка открывается,
точка l. Как ни странно, это вот это правило. А дальше это правило при раскрытии порождает
все остальные правила, которые были в этом состоянии.
Поэтому, чтобы не дублировать эти состояния в бесконечном
цикле, мы как раз их и, получается, подставляем.
То есть делаем переход в одно и то же состояние.
В итоге, табличное представление этой грамматики вот такое.
То есть здесь у нас конфликтов с вами никаких не будет.
Так, если что, останавливайте, потому что я про...
Вот это?
Точка S...
Что из него раскрывается?
А из него вот эти два?
Ну, нет, они как раз вроде как по порядку.
То есть у нас из S стрелочка точка L,
из L раскрываются вот эти два правила,
а из S потом...
Да, то есть здесь конфликтов, кстати, нет.
Это интересно.
Но вот, однако, не всякая грамматика является LR0.
Даже больше скажу, больше через грамматика
не является LR0 грамматикой.
То есть у нас, получается, из S выводится E с крышки, E кавычки.
И вот смотрите, здесь важный момент, что
если мы посмотрим на третью ситуацию,
то по плюсу мы можем с вами либо
двинуться в четвертую ситуацию,
либо
развернуться по вот этому правилу.
То есть мы не понимаем с вами, что именно делать.
В данном случае. Скорее всего, если мы добавим
еще один символ на просмотр вперед,
вполне возможно мы с вами эту
проблему решим.
Скорее всего, здесь визуально можно увидеть, что
если у нас следующий символ плюс,
то скорее всего мы пойдем по шифту.
Если следующий символ доллар, то есть конец
нашего файла, то мы откатимся обратно.
Это визуально видно.
Поэтому хотелось бы это каким-то образом разрешать.
Значит, первая вещь, первая оптимизация,
которая в данном случае поможет нам,
это алгоритм SLR.
Значит, мы с вами про него вообще никогда не говорили.
Идея такая, что давайте воспользуемся тем,
что у нас было в LL-алгоритме.
У нас что происходит?
У нас с вами для каждого унитерминала есть
такое понятие как follow.
Кто его помнит?
Да.
Переки следующий алгоритм.
Follow от нитерминала A.
Это первое, что может вывестись
после раскрытия этого нитерминала.
Так, сейчас вспомню.
То, что после.
Да, первое, что может после.
Ну да, если ничего не может, то йо.
Это я, кажется, про first plus сказал.
Да, то есть это первое, что может вывестись
после раскрытия нитерминала A.
Ну и давайте мы это сделаем.
Мы будем говорить, что давайте
мы...
Сейчас скажу, что мы сделаем.
Вот, мы с вами в LR0 таблицам
оставляем reduce
только для тех терминалов X,
которые могут
то есть то, что может раскрываться
после reduce правила. То есть, смотрите.
Я не знаю. А, тут нету примера.
Давайте как раз его разберем. Вот видите?
Вот ему правило. Есть стрелочка T.
Вот у нас есть конфликт.
Как мы решаем? Нам нужно посчитать
follow от е.
Почему тут будет равен, кстати, follow от е?
Давайте посчитаем его.
Так.
А?
А, ну да.
Хорошо. Так, что у нас получается?
Мы хотим посчитать follow от е.
Ага.
Да, то есть у нас
е равно t плюс е.
То есть, нам нужно посмотреть на правила,
в которых есть е.
В правой части.
Да.
Причем е не должна быть последней.
Здесь е последняя, поэтому мы будем
смотреть на то, что выводится сверху этого правила.
А здесь следующий символ относительно е. Какой?
Доллар. То есть, у нас получается, что
follow от е это доллар.
И как это разрешать? То есть, мы с вами понимаем, что
вот, к сожалению, здесь нету разметки,
то, что вот здесь у нас не должно быть reduce по второму правилу.
И здесь не должно быть reduce по второму правилу.
Reduce по второму правилу должен оставаться только здесь.
Некоторая ивристика, которая помогает на лету не смотреть
на следующую букву, а посмотреть на множество follow.
Понятно, что это не фанацея от всех болезней.
И далеко это все быстро не вправляется.
Вот. Для этого у нас есть LR1-алгоритм.
То есть, позиция, это у нас
вот такая вот ситуация.
То есть, у нас с вами, это не позиция, это ситуация,
а стрелочка альфа точка бета запитается,
где а стрелочка альфа бета это правила грамматики.
То есть, как подсчитать? Мы находимся в текущей позиции
ситуации, и первая буква, которую мы можем вывести после того,
как мы выйдем из этого правила, это буква С.
Вот. И именно по таким вещам мы можем пересчитывать
наши правила.
Вот наши грамматики. Так, ну,
я не хочу сейчас еще раз детальнее вглубляться
все это, как это все делается.
Здесь прямо математический вывод есть.
То есть, опять же, нам нужно набрать
как раз
что мы на самом деле делаем? Мы пытаемся
набить в наше множество автоматик, как можно больше
ситуаций. И здесь как раз говорится, что
если у нас есть точка перед ней терминалом,
мы хотим вывести какое-то правило, то мы можем
посчитать, какое правило из этого может вывести.
И правильно считать первую букву.
То есть, давайте я как раз попробую нарисовать еще раз
на картинке.
Получаем,
допустим, у нас было правило A выводит
α, х, β.
Здесь у нас в этом выводился первая буква, символ Z.
А дальше у нас выводится γ.
Какую букву можем вывести?
Вот смотрите, мы из х выводим γ,
и дальше смотрим на первый символ, который у нас выводится из βZ.
То есть, смотрим на первый символ, который выводится из βZ,
и в ту же самую ситуацию добавляем
какой там символ?
У, где У это первая буква, которая выходит
из βZ.
И дальше с учетом этого и пересчитываем правила грамматики.
Соответственно, reduce мы оставляем только
по тем символам, то есть, если у нас
получается какой-нибудь вывод аля
я не знаю,
а, стрелочка, гамма, точка, бета,
гамма, точка, б,
то reduce оставляем только в том месте, где у нас находится
b, потому что мы выходим из правила, и следующий символ
должен быть b.
Другие символы не подходят.
Опять же понятно, что это будет решать далеко не все конфликты,
но опять же это можно решать.
Так, пример.
LR таблицы.
Ну да.
Ну да, нужно будет обработать аккуратно EOF.
Значит, пример, кстати,
такой о грамматике. Это переменные
C, переменные и указатели C.
То есть, что у нас есть? У нас есть S,
это S$. Дальше
что у нас может быть? Из S у нас может
написаться, что V равно E, это выражение
у нас,
мы можем statement указать как определенное expression,
а в чему у нас может быть expression быть?
E это может быть V,
причем V это либо у нас значение,
либо значение под указателем.
Да, кажется так.
Если я не накосячил нигде.
Вроде нет.
В общем, смотрите,
главный кейс, который здесь есть, у нас либо есть
присвоение по выражению, либо по указателю.
Возможно, что здесь нужно наоборот,
что не V выводит E, а
из E может выводиться этот звездочка
либо апперсант.
Именование и разменование указателя.
Посмотрим внимательно на эту грамматику.
Ага, да.
А что у нас подходит в таком случае?
Допустим, у нас подходит
звездочка X равно
E, E, E, E, E,
E это V.
Допустим, может такое подходить.
Вроде...
А?
Да, это подходит под это описание.
Вроде такое можно даже скомпилировать.
А Y это еще одна переменная.
Мы говорили...
А, у вас не было в прошлые разы.
Мы говорили, что в качестве X здесь уже находятся
не переменные, а токен.
Да, соответственно, Y это просто лексема,
которая скрывает за этим токен.
Ну, это у нас получается разыминование
того, что находится под этим.
Так, сейчас.
Ну, да.
То есть, насколько я понимаю, в левой части
это будет взять дополнительное адреса, да?
А здесь именно получается разыминование указателя.
Это да.
Здесь это надо различать, к сожалению, здесь этого не сделано.
Да, да, да.
Поэтому в грамматике зачастую, чтобы
не париться, обычно даже на уровне самой грамматики
делают отдельный не терминал для L-value
и отдельный не терминал для R-value.
Чтобы развести их поведение.
Давайте посмотрим на эту грамматику.
Скажите, пожалуйста, что здесь...
Не кажется вам ли эта таблица достаточно большой?
Давайте внимательно подумаем,
что мы можем с этим сделать.
Да, здесь много одинаковых ситуаций,
которые при этом переводятся в достаточно похожие.
Вот, смотрите, давайте посмотрим
к примеру ситуации 8 и 11.
В принципе, что мы здесь видим?
У нас конфликтов никаких нету, а ситуации 8 и 11
между собой сильно похожи.
Почему бы их не объединить?
Тем более тут пиковые состояния.
Ну вот, утверждается, что если
мы можем с вами нашу грамматику,
она изначально у нас лежит в классе LR1.
Почему бы мы не можем сузить наш класс грамматик,
при этом таблицу сделать меньше?
При этом грамматика при этом будет адекватной
для этого класса.
Да, Михаил, мы тут пытаемся смёржить состояние
в нашей таблице.
То есть 8 и 11, в принципе, почему бы их не смёржить,
если это никаким конфликтом не приведёт?
То есть мы можем посмотреть на две строки,
смотрим, куда они переходят.
Если, значит, у нас переходы,
можно слить, окей, и смотрим
для всех прародителей, можем ли мы эти переходы слить.
Вот у нас есть переход в восьмое состояние здесь,
из X восьмое, переход по X восьмое, здесь переход
по X в одиннадцатое.
Доллар запятая равно это следующий символ,
то есть это на самом деле две ситуации.
У нас первая V стрелочка X точка доллара,
вторая V стрелочка X точка равно.
Ну да, типа того.
Ну и, собственно, мы их можем слить.
Какие ещё тут сливаются ситуации?
Чего?
Ничего, ни одна стрелка не выходит.
Это означает, что если мы доходим до них,
то мы получаем тупик,
и нам нужно откатываться назад.
Проблема с откатыванием назад нет,
потому что мы храним весь...
Чего-чего, ещё раз?
Нет, символ доллар равно необходимо хранить,
потому что представим себе, что у нас есть правило,
мы находимся в разборе какого-то
предложения, какой-то программы,
а следующий символ не доллар и не равно.
Что мы должны сделать?
Скажешь всё плохо, выдать ошибку компиляции.
Вот, поэтому это дополнительные проверки,
которые нам нужны.
Ну да, так и надо.
Да, только надо по определённым символам это делать.
То есть когда мы находимся здесь,
то есть следующий символ доллар,
либо следующий символ равно.
Смотрите, аккуратнее, чтобы при merge
эти правила объединяются,
и если здесь в 11 ситуации следующий символ равно
даёт ошибку компиляции, то при объединении
следующий символ равно не будет давать ошибку компиляции.
С этим надо быть аккуратнее.
Ну вот, какие ещё ситуации можно объединить здесь?
7, 12, согласен.
Ещё 10, 14.
6, 13 можно ещё объединить.
На самом деле это немного напоминает алгоритм
и автомизация автоматов.
То есть когда мы переходим в одни и те же классы эквалентности
и пытаемся их, опять, только там мы их разъединяли,
а здесь наоборот, пытаемся соединить.
Вот, и вот эта вот таблица, которая мы получим,
она называется уже LIL1.
То есть объединяем те состояния,
в которых состояния, закрывая символ локохедсов, падают.
Собственно, мы их уже с вами нашли.
Это 6, 13, 7, 12, 10, 14 и 8, 11.
Главное, чтобы два разных редюсса не попали в одну ячейку.
Вот, в итоге у нас получается более простая грамматика,
и вы не поверите, Бизон строит LIL1-грамматику.
То есть класс корректно распознаваемый программ
в нём на самом деле будет меньше,
потому что в некоторых грамматиках, которые являются LIL1,
они будут конфликтовать в чьи в них могут быть дополнительные конкуренты.
То есть при мерже ячеек у нас возникнут неоднозначности.
Вот, и в итоге у нас получается вот такая большая картинка.
Значит, сразу скажу, здесь один тонкий момент.
Значит, у нас есть LIL1-грамматики,
это всё однозначные грамматики.
То есть класс однозначный грамматик,
он сужается до класса LIL1-грамматик и LIL1-грамматик.
Собственно, LIL1 уже чем LIL1 при подставлении,
потому что там правило намного проще для проверки.
При этом у нас важное утверждение следующее,
что здесь именно, внимание обращу,
говорится именно про грамматики,
здесь не говорится про языки, задаваемые этими грамматиками.
Да, потому что есть такое утверждение,
что любой язык, который задаётся некоторой LRK-грамматикой,
можно задать языком, который задаётся LR1-грамматикой.
Это прям утверждение, которое мы, правда, не доказывали
на курсе формальных языков, но оно есть.
То есть, если мы говорим по классам языков,
то эти языки будут сильно схлопываться.
Значит, у нас, смотрите, будет хватать либо LIL1,
если мы идём по пути Бизона,
если мы идём по пути Антелера,
то у нас с вами будет алгоритм LIL со звездой.
Собственно, LIL со звездой он даже шире
в каких-то моментах, чем LRK.
То есть, если... Да, изменив грамматику.
Чё?
Нет, наоборот. Наша цель как раз будет
состоять в том, чтобы задать грамматику так,
чтобы она свелась в класс LR1.
Да, главное язык, чтобы был нормальный.
В смысле, я за одной грамматикой в другую?
А, чтобы пересобирать её?
Надо посмотреть доказательства этого факта.
Ну, я думаю, что это всё.
Я думаю, что это всё.
Я думаю, что это всё.
Я думаю, что это всё.
Ну, да.
Честно, сам доказательства не помню.
Ладно. Мы всё-таки про компиляторы тут пришли
говорить, а не про грамматики.
Поэтому нам нужно, как обычно, найти как можно больше ошибок компиляции.
Собственно, давайте вспомним,
как мы в LIL решали ошибки компиляции.
Мы удаляли символы до тех пор,
когда не дойдём до того момента,
когда мы можем спуститься.
И, точнее, не спуститься, а продвинуться по определённому символу.
То есть, мы работали с вами с делецами.
Здесь же суть наоборот.
Здесь очень удобно, что в LR-алгоритме есть
так называемые ERROR токены.
Собственно, это токены, так называемые wildcard токены,
которые между собой могут представлять
следующую вещь. Это точка со звёздочкой.
То есть, вы можете по этому символу двигаться
сколько угодно раз,
до тех пор, пока вы не дойдёте до определённого значимого символа.
То есть, смотрите,
мы можем вставить ERROR токен.
Видите, EXPRESSION — это что такое?
Скобочка открывается, скобочка закрывается,
а внутри него ERROR.
То есть, если у нас ничто никаким образом
не разбирается какое-то правило, то всегда мы можем
скатиться именно в это правило, правило с ERROR токеном,
и начинать разбор символов до тех пор, пока мы не дойдём
до символа, закрывающего скобки.
Даже не терминал, это
регулярное выражение, я бы так сказал.
То есть, мы можем, грубо говоря, написать какую-нибудь
ерунду,
даже несколько символов ерунды, и вот пока мы не дойдём
до закрытия этого символа, мы...
Он попытается по первой закрывающей, скорее всего,
закрыть.
Он пытается посмотреть на вид грамматики.
Да, да, да.
Ну, то есть наша цель, напоминаю, когда мы пишем компилятор,
находить ещё как можно больше ошибок в компиляции.
Что мы делаем? Так, смотрите,
мы обрабатываем стэк, пока мы не сможем сделать
shift по ERROR.
И пропускаем символы
алфавита, пока мы не сможем сделать
shift дальше. То есть, по факту,
либо мы говорим, что ERROR принимает всё, кроме
конкретных символов, да, и мы
делаем, по факту, пропуск символов
до тех пор, пока мы не можем
делать shift дальше. То есть, как бы, продвинулись, остановились,
ждём следующего символа.
Тут спрашивается, почему в третьем шаге не reduce?
Ну, потому что...
Ну, потому что обычно, когда мы задаём ошибки
синтаксиса, которые есть, мы задаём их
как конкретные симантические инструкции.
То есть, мы явно считаем скобочный баланс,
считаем множество выражений, типы выражений,
типы не считаем. Любые симантические, синтактические
конструкции должны быть учинили.
Наша цель, как разработчиков компилятора,
сделать такую вещь, чтобы
учесть как можно больше
ошибок, которые может
совершить пользователь, и внести их
в нашу грамматику.
Да, то есть, допустим, баланс
посчитать, какие-нибудь скобочки фигурные,
типа скобочка фигурная открывается, значит, где-то закрывается
Мы считаем, что ещё не выйдет у нас,
как мы можем сказать, что ошибка
это вся программа, так и
какой-то небольшой день.
Ошибка светлая.
Ну, на самом деле, как мы можем здесь
понять? Мы просто можем отсечь позиции,
когда мы пришли в ерор, и отсечь позицию, когда мы
вышли из этого ерора. Это как раз этот участок, и
будет ошибка в компиляции.
А зависит от того,
где мы эти ерор-токены будем вставлять?
Нет, конечно же.
Ну вот, два примера.
В смысле, давайте разбирать,
значит, что такое expression, да?
Нет, не PSP, а грамматика для арифметики,
предположим. У нас есть последствия
операторов пресваивания,
а равно b плюс c и так далее, и там, допустим, есть
а равно скобка открывается.
Нет, это вот, если мы говорим про
бизон, внутри него есть прямо ключевое слово
ерор в грамматике.
Да, можно быстрее. То есть, тут как раз, если
говорится про обработку ерор-токенов, то сначала мы можем
пытаемся, то есть, если мы пришли в какое-то
место, пытаемся
размотать стэк, который вот у нас был, пока мы не
можем сделать shift по ерор-токену. То есть, грубо
говоря, идем, идем, идем, потом понимаем, что мы
зашли в какую-то ситуацию, причем в этой ситуации
мы сделали shift по ерору. Наша цель будет
выйти как можно быстрее и пройти дальше.
Ну, смотри.
Давайте посмотрим. Значит, у нас есть правило, допустим,
х, вот.
Вот такое вот. А у нас правило
вот такое.
Ну, то есть, вот у нас слово такое.
Да, да, нам дали.
Плюс.
Угу.
Хорошо. Тогда что у нас будет с грамматикой?
Ну, смотрите, давайте мы
допустим,
e id плюс, вот такое
вот, или e
ерор
закрывается. У нас два таких правила.
Да, переменная.
Стандартная нотация. То есть, а здесь у нас
получается, какая грамматика?
Это плюс-плюс-плюс закрывается.
Скопка открывается, плюс-плюс-плюс это три
токена у нас. То есть, плюс может обозначаться как
отдельный токен.
А у нас его нету.
Вот это наше входное выражение.
Вот нам нужно в нем найти ошибку компиляции.
Да, да, да.
А вот,
предпочтительнее он
начинает разбирать то, что без ошибки.
По факту мы можем сказать, что, значит,
как мы можем работать с ерор токеном?
Вот давайте подумаем. Мы можем сказать, что у нас
в нашей грамматике, помимо переходов к
грамматике, мы можем сказать, что
у нас есть
в нашей грамматике, помимо переходов
по определенному символу, которые были
классические, у нас дополнительно будут добавляться
переходы по ерору.
То есть, которые будут нас возвращать туда же.
То есть, образно говоря,
у нас если было...
Вот, допустим, мы здесь находились.
ID, да, или
ерор вот такой вот.
То есть, ну что мы можем сделать? Если по ID мы идем,
то идем сюда. Если по какому-то другому токену,
то мы это...
Если, значит, по фигурной скобке,
ой, по закрывающей скобке мы можем двигаться дальше.
То есть, причитав этот ерор токен,
давайте вот так.
То есть, тут, понимаете,
нужно некоторое запоминание в уме. То есть, у нас скобка
закрывается, тогда мы едем дальше.
А по всему остальному мы делаем следующее. Мы
берем и этот.
Делаем переход по пустому слову, ждем, пока мы
не по пустому слову, а по слову нашей грамматики.
То есть, здесь будет переход по плюсу,
который сделает шифт нашего символа плюс.
Да, но при этом, по факту,
его в путь на стеке не положат.
То есть, как бы получается эфемерный путь,
который мы не запоминаем.
Не-не-не, здесь важно, что
именно по вот этим правилам мы строим. То есть, сначала
грамматику строим для правил, а потом уже дополнительно
переходы по RTOX.
Да.
Да, такого правила не дали.
Да.
А, понял, понял, понял.
Видимо, согласен здесь. Здесь есть несколько
подходов. Допустим, мы можем сделать следующее. Сразу
при первом переходе сделать шифт по символу
еррор. То есть, встретили плюс, встретили символ,
которого у нас нет, мы делаем шифт сразу по символу
еррор.
Да, да, да. То есть, мы по факту
можем сказать, что мы создаем дубликат этого состояния,
в котором у нас здесь идет еррор,
точка здесь.
Нет,
после еррора идти не терминал не может, потому что
здесь мы обрабатываем с вами только входное слово.
То есть, мы здесь переходим в режим, что у нас
идут слова из входного языка.
Да.
Надо, да. То есть, ниже мы раскрываться
не можем для того, чтобы не делать дополнительного
выклеивания.
Да, да, да, да.
Вот.
Значит, то есть, вот такая вот вещь, она
есть, и она реально может пригождаться на практике,
главное не забывать считать всякие скобочные балансы,
собственно, там, чтобы у нас
не ломать семантическую сущность разбора.
То есть, желательно как раз задавать как можно больше
вот таких вот семантических инструкций.
Так, ну, смотрите, ладно, мы все-таки
компилятор пишем, и причем мы пишем достаточно
современный компилятор.
А что помимо того, что у нас есть ошибки в компиляторе?
Что у нас еще есть?
Что? Вормиги? Нет.
Багги.
Багги-то, естественно, есть. Нет.
Вот мы напишем с вами
может быть даже какую-нибудь командную строку откроем.
Так.
Так, у меня какой Python?
Так.
Эх, был плохой компилятор.
Да я понимаю, я специально
вызываю эту штуку.
Если вы хотите показать
в подсказке выдаст.
Импорт.
Не, не, не, понятно, то есть
Господи.
Ой, есть у меня.
О, ура!
Получилось.
А кстати, в Python есть
еще такая вещь, как Syntax Forming.
Например, можно пробовать строку, умножить
сло и взять срез, и все это проделать
без другого слова.
Он выдаст вам Syntax Forming, что вы берете средства
не от строки.
Есть разные вещи.
Собственно, к чему я клоню? К
восстановлению ошибок в компиляции.
Собственно, здесь есть вот такая вещь.
Это метод Бурке Фишера, он достаточно известный.
Значит, что мы делаем? Он как раз хорошо
работает для лир алгоритма.
И чем он заключается? Мы держим очередь из
к токенов, которые могут быть исправлены,
которые необходимы для возрождения коррекции.
И между с ними, то есть представьте себе,
что у вас есть два запуска лир алгоритма.
Один, который едет вперед, а другой, который
отстает на ка большое токенов для возможной
Сначала первый проход, потом идет второй проход.
Как только мы делаем по токену Shift,
мы добавляем этот символ в очередь
и обновляем стэки.
Когда у нас происходят ошибки в компиляции,
то мы пытаемся сделать возможную либо инсерцию,
либо даляцию, либо замену.
То есть как бы подсказываем
следующему старому стэку.
То есть у нас получается из настоящего,
мы говорим в прошлом, может быть ты исправишь
то, что происходит?
И начинаем перебирать варианты.
Понятно, что здесь уже возможен разный
статистический анализ, вот это важно.
Что мы понимаем, грубо говоря, собираем кодовую базу людей
и вычисляем вероятности того,
какую ошибку именно допустили.
Марковские цепи, знаем, что такое?
Марковские цепи, ну...
А, у вас только начались слупы.
Вот представьте себе, что у вас есть какой-то процесс
во времени, причем важно, что этот процесс
во времени, он зависит только от предыдущего события времени,
но никаким образом.
То есть у нас есть, получается, события...
Да, случайный. То есть у нас есть процесс
и он зависит только от того, что мы наблюдали
в момент времени t-1,
но не наблюдали в момент времени ранее.
А, что формально это математически можно описать.
В итоге у нас вот этот весь процесс можно задать
в матрице фазового перехода от момента времени t-1
до момента времени t.
Ну и дальше, какая наша цель? Наша цель состоит в том,
чтобы собрать статистику переходов, вот мы, допустим, были
в наборе токенов, собственно, в какой набор токен
мы дальше можем перейти.
И в зависимости от этого предлагаю самый вероятный токен для починки.
Как вариант.
То есть видите, здесь уже идут всякие статистические анализы.
То есть, как говорится, некоторые науки
у нас синергируют.
Хорошо, значит...
Да, опять же, нужно учитывать семантические особенности.
Поэтому либо мы используем
семантический анализ,
либо, допустим, для какой-то переменной, которая у нас, допустим,
не была объявлена, мы можем предложить.
Давайте этой переменной предъявим значение по умолчанию.
0, 1 и так далее.
Вот, собственно, это такая дополнительная вещь.
Ну и последнее, что можно предлагать,
это...
предлагать дополнительный набор правил, из которых
пользователь мог ошибиться.
Их можно предлагать очень много.
Скопку забыл, фигурную скопку забыл
поставить еще, закрыл и так далее.
Главное, чтобы сами вот эти наборы правил, которые мы дополнительно добавляем,
не добавляли определенных конфликтов.
Вот, это будет важно.
Так, собственно, это все, что касается теми
синтоксического анализа.
Так, давайте по ней вопросы.
У нас еще очень...
Значит, смотрите,
была статья, по-моему,
AntLR1
is not needed.
Архив, по-моему.
Да вот, да. Сейчас я найду.
Сейчас, подождите, найду статью эту.
Так, статья называется...
статья 2010 года.
Мы ее найдем в поисковике.
А, во!
А он не про LR, он про этот.
Вот.
Вот.
Собственно,
YAK SD это называется статья.
Значит, YAK это именно Бизон в современных интерпретациях.
Вот такая вот статья.
Типа, и говорится, что
top-down motivation, пора это.
Заканчивать каргокульт парсинга, товарищ пишет здесь.
Вот.
И говорит, что, типа, изначально
все это затачивалось
по то, чтобы, если мы что-то парсим,
то это прямо то, что мы видим,
то мы и получаем на выходе. Визивик, так называемый.
И как раз вот и sees, вот и get.
И как раз, если мы говорим про регулярные выражения, они такие есть.
То есть, мы с вами с регулярными выражениями видим, что происходит.
А что происходит с автоматами, мы не видим.
Ну, что происходит. Вот.
И поэтому здесь товарищ очень долго прямо рассказывает.
Прямо про то, что, типа, в чем проблема
иллера, алгоритма.
Вот.
Ну, собственно, и здесь предлагает это.
Ну, тут прямо математика, математика, математика.
Вот. И, в общем, тут
предлагается обойти
этот подход.
В общем, видно, что здесь уже какие-то
это...
интересные вещи. Ну, и тут если почитать...
А?
Не, не критикуйте алгоритм.
Он добавляет свои собственные механизмы, так сказать,
предиктивного парсинга и так далее. И говорит здесь, что
это...
типа, что для некоторых библиотек они не очень
хорошо подходят, так сказать.
Вот. Поэтому лучше
использовать методы, которые позволяют,
так сказать,
те библиотеки, которые легко, во-первых, пишутся
и легко понимаются. Ну, то есть даже если мы
сделаем сравнение, то что намного быстрее
понимается? Иллель алгоритм или лерн алгоритм?
Иллель, конечно. То есть для разбора иллеля нам достаточно
пол пары. Для того, чтобы разобрать иллель, нам нужно там пары
три. И тем более, особенно дебагать
иллель алгоритм намного проще, чем иллер алгоритм.
Потому что вот у вас есть таблица, вот сидите, разбирайтесь
с ней. Типа, таблица обычно, если там
ее сконвертировать в png, то там 10 тысяч на 10 тысяч
пикселей. Вот такая вот простыня.
И сиди, разбирайся, где это все происходит.
Вот, поэтому
на практике сейчас все-таки используют
эти не сходящие парсеры,
а не восходящие. Ну, это видно даже
по популярности того, что
происходит.
Ну, да, мы говорили, что
антеллер работает за квадрат в худшем случае,
но там худший случай это
N умножить на какую-то константу K.
N на константу K. K это
как раз максимальная длина контекста, которую нужно
предсказывать. Так вот, проводили статистические
анализы, оказывается, что там в большом проценте
случаев там K равняется двойке-тройке.
То есть за двойку-тройку уходить не надо. То есть получается, что
мы не тратим время на то, чтобы строить вот эту
огромную таблицу, хранить большое количество оперативной памяти,
а на лету пытаемся прямо
строить контекст, и этого вполне достаточно.
То есть у нас получается и по времени, и
по памяти, и по времени.
Ну, не LL1, LL звездочка
все-таки.
Ну, ошибок много. То есть у вас, допустим,
грамматика, в которой есть неявная рекурсия.
Вот мы сидим, то есть одна из проблем
как раз нисходящих парсерий, это неявная
рекурсия. Неявная левостроение
рекурсия.
Ну да, ее убирать мы умеем,
но вот, типа, образно говоря, там
в каком моменте это может возникнуть? Там какая-нибудь
сложная математически-кабинаторная лямбда функция,
которую фиг пойми, как разбирать.
Где неявные рекурсии, неявные рекурсии
погоняют.
Ну, что? Да.
Зачастую именно так сейчас.
LR не работает?
Можно взять и переключиться на JLR.
JLR, он любит...
Да, он уже с всеми однозначными работает,
но нужно сразу заложить, что
асимптотика может быть больше.
А более того, если рассказывать
совсем веселые моменты, то, как не странно,
вот те люди, которые еще занимаются всякими
диффузионными моделями, там
красивые картинки генерят,
я дебажил один раз код,
как раз для этих диффузионных моделей, смотрю,
что-то не работает. Читаю в отладчике,
читаю stacktrace. Написано early.py.
Ну,
прям в отладчике.
Стактрейс, и там early.py написано.
То есть люди вообще не парятся, короче, для коротких слов,
а там как раз механизм парсинга такой,
какому-то слову меньше тона дать и так далее.
Просто не паримся, пишем early.
За квадрат работает.
Почему бы и нет? Ну, за квадрат для однозначных
агроматик работает.
Ну вот, да, зачем придумывать еще что-то?
Ну, понятно, что когда у нас квадрат, собственно,
на код в 10 миллионов строк, то это уже будет
проблема.
Ну, может быть, 1С компилировать из исходников.
Ну, или любую другую библиотеку из исходников,
это будет тяжело.
Так, это что касается синтоксического анализа.
Давайте тогда я сегодня сделаю некоторое
предварительное знакомство с тем, куда мы будем двигаться дальше.
10 минут осталось.
Это семантический анализ. То есть следующие несколько лекций
мы с вами посвятим тому, что мы будем с вами детально
рассматривать, что такое семантический анализ.
То есть, где мы находимся с вами на стадии компилятора?
То есть, мы с вами написали сканер, у нас есть парсер,
у нас есть дерево вывода, но что у нас теперь есть?
Нам нужно что-то с этим деревом вывода сделать.
Для того, чтобы мы с вами пришли к промежуточному
представлению. Здесь будет несколько шагов.
И вот смотрите, важный момент, вот в этом слайде
видите, вот это вот поле инфрастракция.
Инфраструктуры написания компилятора.
Сейчас мы с вами видим на самом деле, что у нас
эта инфраструктура никак не была завязана между
стадиями компиляции. То есть, мы взяли
сканер, запустили его, взяли парсер, его
запустили. А вот на этой стадии мы как раз начнем
пользоваться инфраструктурой. То есть, выход
каждой стадии, то есть выхода из предыдущей стадии
нам вполне может быть недостаточно.
Нам нужно будет использовать это еще каким-то
образом. Вот, давайте поймем.
Вот у нас есть токен,
который вышел после стадии
сканера, ну и после стадии парсера он
привязался к какому-то правилу.
Что нам нужно знать о токене?
В каком правиле он находится?
В каком правиле он находится?
Какой тип он имеет?
Его значение, да.
Вот смотрите, у нас есть токен, а-ля
идент.
Идентификатор.
Вот. Но что у нас может быть?
Этот идентификатор, в принципе, может быть типом
в какой-то момент времени.
То есть, обозначением самого типа. То есть, у нас есть
класс full, допустим, да. Это у нас
класс, идентификатор будет.
Сейчас дайте я доски с утру.
То есть, у нас есть класс full.
То есть, это название класса, это какой-нибудь
typeIdent.
Его даже можно попробовать различить. Но потом
в какой-то момент времени мы, допустим, указываем.
Вот такую вещь пишем.
Разный контекст. То есть, здесь это объявление типа
здесь это вызов конструктора.
Здесь это
определение типа.
И надо это каким-то образом понять.
Ну а?
Да, мы сможем определять свои собственные типы.
Ну да.
На машине Тюринга будет сложно
программировать.
Ну да.
Итак, смотрите. Нам нужно будет понять о токене.
Какого он типа, если это токен переменная?
Вторая важная вещь. Какое количество памяти он
занимает, если это переменная? То есть, это массив.
Или это переменная примитивного типа.
Либо это указатель на какой-то класс.
С учетом того, что мы понимаем, что в каком-нибудь
C, C++, Rust, у нас прямо есть явные типи ЗАЗ,
в котором мы можем указать указатель или нет.
То вот в джаве, в питоне, вот это уже какая-то
сложная история.
Потому что в джаве у нас везде обращение идет
по указателю, точнее по ссылке.
В питоне вы пишете x равно 1.
Ну что такое x тогда?
Если бы мы говорили в терминах C++,
то это была бы переменная типа int,
которая занимает 4 байта.
А в питоне это, извините, переменная типа
cpytonint,
которая занимает не 4 байта.
А?
Вот.
То есть вообще непонятно.
То есть питон просто и простой,
но вот он оказывается такой.
Но возможно, что если мы это скомпилируем
при помощи какого-нибудь сайта,
то это возможно и будет int x.
То есть нам нужно понимать,
влезет он в регистр или нет.
У нас токен может быть названием функции,
и тогда нам нужно понимать, какое количество
аргументов он принимает.
Следующий важный момент, который нужно понять,
это область видимости этого токена.
То есть сколько нам нужно хранить значение
этого токена, то есть область видимости.
И вопрос еще следующий, кто ответственный
за локацию и удаление?
Что делать мы, и переходим на си язык,
в котором есть управление памятью?
Либо мы будем реализовывать абстракцию REE.
Либо мы будем писать сборщик мусора.
Либо мы вставляем Borrow Checker.
В общем, много есть таких вещей,
и вот из чего будут состоять стадии симматического анализа.
Важный момент.
Как раз мы пройдемся по абстрактному дереву.
Будем построить токены,
и для каждого токена поймем, что это за переменная,
какой тип и так далее.
Дальше у нас будет идти проверка типов.
Возможно, что мы будем готовить фреймы.
Функциональные фреймы для каждой функции
мы заготовим по факту аналог стека.
Дальше у нас будет идти проверка типов
и промежуточный код.
Сразу скажу, что таблицу символов
можно строить несколькими способами,
и мы построим ее несколькими способами.
Один из них достаточно тупой,
а второй этот способ достаточно умный, но богованный.
Николай, который сидит здесь, нам это подтвердит.
Здесь возникает такая важная вещь,
что нам нужно будет перейти
из грамматики к так называемой атрибутной грамматике.
Для каждого не терминала
мы будем хранить с вами некоторую
осмысленную сущность.
Для каждой сущности, возможно, нам нужно будет
определять набор этих атрибутов.
Если мы говорим вообще очень-очень грубо,
то для каждого компилятора мы можем сказать,
что каждому терминалу и не терминалу
можно соотнести некоторый класс.
Допустим, у нас есть
a равно id плюс id.
Что такое a?
Expression. Причем какой expression?
Expression для сложения.
Мы можем с вами приписать, что, допустим,
атрибут не терминалу a
будет внутри него храниться контекст,
что это именно атрибут для сложения.
И параллельно мы с вами как раз
для каждого не терминала
будем стараться написать объект,
который он содержит.
И он может конвертироваться в ad expression.
Другие стейтменты тоже мы будем обозначать.
То есть, если у нас есть какой-то стейтмент,
вот когда мы встречаем не терминал a
в какой-то момент времени,
как только мы его разбираем, мы строим объект
под названием ad expression,
в котором внутри будет еще, который будет
ad expression, который здесь будет парситься.
Да, уже абстрактную сущность.
Да, да, да.
Причем не все сущности могут выражаться.
Некоторые сущности могут добавляться или убираться.
Дополнительно, если вы хотите что-то делать
с этими сущностями, то в принципе
каждому не терминалу можно посчитать атрибут.
То есть, если у вас есть какой-нибудь простой expression,
образно говоря, смотрите,
x это нам плюс нам.
То есть два числа.
Вы складывайте два числа. Вопрос.
Нужно ли вам создавать ad expression
от вот этих двух значений?
И их посчитать на стадии разбора.
Да, конечно же. То есть мы можем...
Да, да, да. То есть мы можем явно
плескать, что x точка вал
нам плюс нам.
Получилось, да?
Обычно это делается на уровнях листьев деревьев.
Потому что в других местах сложно.
И вот как раз вот эти вот вещи называются атрибутными грамматиками.
Наша цель будет как раз от атрибутных грамматик
перейти... от классической грамматики
перейти к такому понятию, как атрибутная грамматика.
Мы в принципе можем понимать, как делать.
А через него как раз перейти к понятию абстрактной синтоксической дерево разбора.
Да, а когда именно числа пишут?
Четыре плюс пять, допустим.
Да, да, да.
Потому что когда здесь будет identity, мы это уже сделать не сможем.
Потому что там в runtime могут быть проблемы.
Типа переменная, где ее взять и так далее.
И вот как раз вот такие просто случаи можно прямо обчитывать атрибутами.
А вот сложные варианты. Нам нужно уже дерево разбора.
Да, мы заканчиваем все сейчас.
И они будут построить абстрактные синтоксические дерево разбора
и научиться их обходить.
На семинарах мы уже начнем это рассматривать сегодня.
А на лекциях чуть позже это рассмотрим.
Так, ну что?
Видимо у нас следующий лекция через две недели, да?
Да.
В общем, спасибо большое.
Увидимся в следующий раз.

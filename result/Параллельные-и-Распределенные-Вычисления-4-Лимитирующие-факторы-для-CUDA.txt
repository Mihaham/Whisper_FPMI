Всем доброго дня! Мы с вами опять настраивали технику. Точнее, сегодня мы не настраивали технику,
а дожидались, пока эта техника доедет. У нас в процессе съемок присутствует некоторая конвейерность.
Раз мы говорим про конвейерность, то давайте поговорим сегодня про то,
какие лимитирующие факторы присутствуют в написании приложений на Куте.
Мы с вами в прошлый раз ввели логические и физические абстракции, при помощи которых мы с вами можем
замерять, так сказать, некоторую производительность. И, по большей части, на семинарах вам, по идее,
должны были показать работу профилировщика. Я на своем семинаре показывал. Я запускал Antelita NvProv,
в которой мы смотрели скорость работы программы. Но, как известно, начиная с видеокарты NVIDIA 30
поколений вида ампер, возникла есть такая история, что этот профилировщик вырубили из стандартной поставки.
То есть классической профилировки нет, но нам все-таки необходимо замерить время работы нашей программы.
И сразу скажу, что как раз с временем замера программы возникают первые проблемы, потому что мы с вами
выясним, что код ядра, который у нас есть, обычно запускается именно ассихронно.
Как замерить скорость работы программы на видеокарте? Для этого есть специальный счетчик под названием
KudoEventT. Создание события времени – это KudoEventCreate. Оно принимает объект на вход KudoEventT.
Дальше у нас с вами есть событие KudoEventRecord. Мы с вами записываем определенное событие.
А дальше есть еще одна команда, которая называется KudoEventSynchronize, которая ожидает исполнения события,
то есть ставит блокировку на события. Почему это важно? KudoEventRecord и KudoEventSynchronize.
Вот смотрите, есть два именно способа замеры, точнее две единицы замера.
Почему нам не хватит замерить точку старта, точку конца, вычесть из разницы начала и кажется все хорошо?
У кого-то есть какие-нибудь мысли по этому поводу? Почему нельзя взять две точки и замерить разницу между ними?
У нас ядро на самом деле выполняется асинхронно, вот это надо понимать.
И вот давайте, чтобы вы понимали, как запускается код на видеокарту.
Минутка компилятора KudoEventRecord.
Значит, смотрите, когда мы с вами говорим про код ядра, то вы видели...
Так, а есть здесь те, кто не на мысль многое? Ага, вам просматривали код ядра на KudoEventRecord?
Нет, у вас еще не было, да? Ну давайте тогда я расскажу.
Значит, у нас есть код функции IntMain.
И дальше вам нужно объявить некоторую функцию, которая называется GlobalVoid.
Значит, это код ядра.
Что-то одно, допустим, принимает количество элементов.
А, в этом смысле, да.
Вот, вот эта штука называется ядром.
И когда вы запускаете код, то на самом деле происходит следующее.
Вы вызываете функцию это с некоторыми параметрами.
Значит, это параметры конфигурации ядра.
Значит, здесь мы указываем число блоков.
Вот, число потоков.
Это у нас называется блок DIMM.
А вот это у нас количество этих блоков.
Гринди.
Значит, теперь как происходит процесс компиляции в Kudo программу.
На самом деле, он заключается в следующем.
Что здесь код по факту делится на две основных части.
Одна часть едет в код компиляции под видеокарту.
Вторая часть едет в код компиляции под CPU.
То есть, на самом деле, что у нас происходит под капотом.
То, что помещается в модификатор Global и Local, это на самом деле
суммарный модификатор device plus host.
Просто модификатор host обычно опускается.
То, что помещено идентификатором device, едет на само устройство.
И компилируется именно внутри видеокарты.
То есть, для запуска на видеокарту.
То, что помещено ключевым словом host, появляется как доступ кода на ЦПУ.
Причем, когда мы указываем код именно самого ядра,
по факту у вас происходит что-то похожее на процесс линковки.
Вспоминаем курс по технологиям программирования.
Как происходит линковка, берегляйте.
Тут статическая все-таки линковка.
Что у нас происходит?
Мы с вами продолжаем. У нас сегодня день технических проблем.
Но ничего страшного.
Смотрите еще раз. Что у нас с вами происходит?
Когда мы запускаем код на девайсе, он компилируется под девайс.
То есть, именно код будет именно самой библиотеки.
Здесь у нас просто будет declaration функции add, что она у нас есть и она есть на видеокарте.
И по факту, когда мы запускаем функцию add, мы создаем объект типа future.
Только этот объект типа future неявный.
То есть, мы с вами не сможем явно контролировать результат работы этого ядра.
То есть, явно получить результат.
То есть, мы сможем получить результат только в том случае,
когда мы с вами поставим какую-то точку синхронизации или какой-то барьер.
Это необходимо понимать.
Поэтому нам необходимо всегда ждать события.
Либо после точки установки события делать некоторую синхронную операцию.
Давайте код посмотрим еще раз.
Ядро у нас выполняется синхронно.
И смотрите, если вы напишите, образно говоря, здесь создадите рекорд.
Здесь сделайте рекорд.
Если вы сразу возьмете delta равно t-end-t-start, то он вам покажет время 0.
Потому что истинные моменты t-start и t-end у нас здесь не появятся.
Давайте как раз чтобы вот это все визуально было, я нарисую картинку.
Которая покажет всю магию нашего процесса.
То есть, у нас с вами...
Это у нас хост.
Это у нас девайс.
Нарисуем диаграмму последовательности.
У нас с вами, когда мы запускаем какое-то ядро,
в нашем случае функция add, это функция,
в нашем случае функция add.
Эта функция является синхронной.
То есть, у нас не будет никакого результата.
После этого, в какой-то момент времени, мы можем вызвать еще одну функцию add2.
И она именно станет именно в этот момент времени.
То есть, у нас получается время на cpu, время на gpu идет не таким образом, как обычно.
Соответственно, если мы ставим здесь рекорд,
а после этого ставим здесь рекорд,
то есть, это у нас t1, а вот это t2,
то понятно, что у нас t1 и t2 фактически на видеокарте еще не были исполнены.
То есть, момент t1, здесь он будет t1 со звездочкой,
а вот здесь момент t2 будет здесь, находится здесь.
То есть, этот момент t2 со звездочкой.
То есть, он у нас по факту не наступил.
Оно будет обновляться только в тот момент времени, когда вы на видеокарте получите событие.
То есть, вначале оно заполняется текущим временем, но после этого оно заполняется актуальным временем.
Соответственно, какая из операций является блокирующей, которую мы с вами прошли?
Первое, это CUDA event synchronize.
Это раз.
Тогда вы дождетесь завершения события, поставите блокировку.
Но лучше, опять же, эту блокировку ставить после выполнения всяких ядер,
потому что иначе это нарушает очередь исполнения.
А второй способ, есть еще полный синхронайз, называется CUDA синхронайз.
Я не советую его делать.
В какой-то момент времени из-за этого очень сильно страдал библиотека PyTorch.
Потому что после вызова ядра там была функция .cpu, то есть тендер вернуть на цепу.
И что там, фактически, происходило?
Там просто CUDA синхронайз на всю видеокарту.
Ставился глобальный блокировщик и скидывал спавнную цепу.
Из-за этого конвертация была очень тяжелой.
А в TensorFlow в тот момент времени все было написано достаточно аккуратно через механизм стримов.
Вот как раз механизм стримов мы через лекцию посмотрим.
И оказалось следующее, что если мы поставим после этого какую-то синхронную операцию,
то, в принципе, синхронайз делать не надо.
И, как ни странно, одной из такой синхронных операций это MemCopy.
То есть после MemCopy, если вы обращаетесь с каким-то элементом массива,
а там именно вот эти вот события будут записаны на массивы.
То есть у вас по факту построится граф вычислений под капотом видеокарты, потому что компилятор другой.
И, собственно, когда вы запросите MemCopy определенного куска массива,
с которым вы производили операцию, то для этого массива будет как раз блокирующая операция.
То есть у нас получается, допустим, у нас есть массив dA, мы делаем MemCopy,
дальше kernel, еще kernel, и дальше MemCopy.
Вот, тогда после этого MemCopy у нас возникнет синхронная операция,
при помощи которой мы уже получим завершение всех событий, которые были до этого момент.
Так, понятен ли вот этот тезис?
Ну, что, типа, если умеряем время, то нужно именно использовать копирование времени.
Вот, то есть мы ставим трекер в прохождении дистанции, а потом на финише уже сверяем часы.
Вот приблизительно вот такой вот код мы с вами увидим, если мы напишем на видеокарте.
То есть мы ставим куды event start, куды event start, куды event start.
Значит, создаем два события, записываем точку старта, записываем точку остановки,
после этого делаем куды MemCopy, потом делаем куды event synchronize,
и дальше есть точка за мерами времени, куды event elapsed time.
Скажите, пожалуйста, какая строчка на самом деле здесь является лишней?
В этом коде.
Ну вот, оно на самом деле не нужно.
То есть зачем нам синхронизировать события stop?
Если у нас копирование массива dy уже произойдет после выполнения гидра.
То, что dy у нас как раз завязано на копирование, и мы в результате тоже скопируем dy.
Так, понятно ли вот это вот код?
Вам скорее всего очень много раз придется им пользоваться.
А?
Какая строка?
Cuda event record stop.
Cuda event record stop.
А затем, что все-таки нам нужно поставить отсечку времени.
То есть мы выполняем гидро, нам нужно отставить момент времени, когда оно закончилось.
То есть мы здесь замеряем время гидра под названием edit.
Или я иначе не понял?
А?
Хорошо.
А теперь давайте посчитаем количество операций, которые у нас используются.
И здесь будет еще один интересный факт.
Вам по идее на семинарах должны были показать, что код сложения двух массивов работает приблизительно за 10 с количеством итераций, равняемым 10 десятой.
То есть 10 десятой, а мощность видеокарты все-таки не 10 десятой, а 10 двенадцатой.
Вот наша цель как раз разобраться с тем, а где здесь возникает проблема.
Итак, смотрите, на видеокарте RTX 2080 Ti, которая стоит на нашем кластере, 4352 ядра.
Частота ядра 1,5 ГГц.
На самом деле видеокарты настроены таким образом, что в целом они могут прогонять две операции за один цикл.
За счет дублирования.
И в итоге мы получаем вот такую мощность видеокарты.
То есть мы можем количество ядер умножить на частоту нашего одного ядра, умножить на количество тактов.
А операции за один такт.
Получаем 13,5 Tera floating operations per second.
Flops.
Это сколько получается?
Tera, да.
10 двенадцатый.
13 на 10 двенадцатый, мы должны получить 10 тринадцатой операции в секунду.
В смысле переполнить?
В смысле переполнить?
10 тринадцатый, 14 битный, счетчика не ждет.
10 восемнадцатый.
Ну да, достаточно быстро.
Не, я говорю к тому, что у нас все равно 10 десятый.
Получилось на замерах.
Чем приходится платить?
Памяти необходимо платить.
Скорость доступа к памяти.
То есть скорость доступа к памяти начинает играть в лимитирующий фактор.
А что с памяти?
Давайте посчитаем такую величину, как пропускная способность.
Значит, у нас с вами пример с AXP.
Значит, это прямо...
Расшифровываться как S равно AX плюс Y.
Итак, мы с вами векторно выкопали.
Итак, мы с вами векторно выполняем операцию AX плюс Y.
Давайте посчитаем количество операций, которые у нас с вами здесь происходят.
X, Y и S это массивы.
Сколько операций чтения массива у нас произойдет здесь?
Во-первых, нам нужно будет прочитать...
Первое, что нам нужно будет прочитать, X.
Да.
Read Y, Write S.
Что мы с вами получаем?
Значит, здесь у нас получается N, здесь N, здесь N.
В сумме у нас получается 3 N операции.
Но тут важно не количество операций, а количество байт.
Которые мы используем.
Сразу говорю, что мы работаем с вами с 32-битными числами.
С 34-битными числами не надо работать, потому что они все ломают.
В итоге мы получаем с вами 12 N операций.
12 N байт, которые необходимо прогнать через код нашей видеокарты.
12 N байт, которые необходимо прогнать через код нашей видеокарты.
Логично?
Давайте посчитаем, сколько это будет.
Мы с вами получаем следующую величину...
А да, это сколько мы на практике будем с вами должны прочитать операции, 12 N байт, зафиксировать.
Теперь посчитаем пропускную способность видеокарты.
Это на самом деле максимальная величина по объему памяти,
которая видеокарта может прогонять за единицу времени.
Точнее, за секунду.
Значит, как она измеряется?
Она измеряет следующим образом.
Она измеряет частота оперативной памяти на ширину шины данных.
То есть у нас данные в оперативную память гоняются по шине.
Попробуем положить на эффективность этой памяти.
Первая. Частота оперативной памяти видеокарты где-то 1600-1700 ГГц.
На ЦПУ мощность частота памяти где-то 2,6-3 ГГц.
Здесь меньше, но сравнимо с частотой процессора.
В раме 2,6 ГГц.
Тут говорят 1750 ГГц.
В раме частоту 1,75 ГГц.
Вот. То есть видно, что мощность.
Давайте теперь сравним еще по одному фактору.
Какая мощность частота ЦПУ?
Где-то 3 ГГц сейчас стабильно можно выжимать.
На десктопах версии.
Здесь у нас мощность ГПУ это 1,5 ГГц.
А теперь посчитаем количество.
Сколько ядер там? Справа на ЦПУ.
Ну 8-24.
Да нет.
Сейчас можно AMD.
Да-да-да. Мы именно такие трэду считаем.
Значит здесь у нас получается 4352.
Ну вот.
Если у нас на центральных компьютерах ЦПУ и RAM
могут достаточно быстро друг друга и обеспечивать
по ресурсам, потому что пропускной способности хватает,
потому что они могут кормить эти 8 ядер,
то здесь кормить с такой же скоростью 4000 ядер
это большой-большой вопрос.
То есть вы не успеете просто с такой же частотой
обеспечивать ресурсами.
А главное это подносить ресурсы для вычислений.
Вот. Поэтому здесь возникает проблема.
И как раз пропускная способность
она собирается таким образом.
Я показываю специфику видеокарты 2080Т
и можно посчитать.
Значит частота данных битной шины это 352,
но при этом бита.
А эффективность видеокарты равняется 8.
И здесь нужно лезть именно в железки компьютера
и понимать, что мы говорим про память.
Что такое DDR, я хочу вас спросить.
Знаете ли вы?
Да, DDR2, DDR3, DDR4.
А?
Как расшифровывается DDR?
Так, там три.
Последний это rate.
Double data rate.
То есть за один такт прогона
мы можем доставить память по факту по шине данных.
Мы можем проставить две операции.
Итак, смотрите.
На самом деле здесь получается,
что в современных видеокартах 2080Т
используется видеокарт памяти DDR6.
О, Господи.
Сейчас я отвечу на звонок.
Да вы задолбали, ладно.
DDR6, 4 передачи за цикл у нас получаются.
Вот.
И double data rate у нас возникает.
Особенности DDR передач мы еще получаем,
две передачи дополнительно.
То есть шестая, шестой поколение DDR
нам дает на один цикл, так сказать, 4 передачи,
а double data rate нам дает еще две передачи.
В итоге получаем 4 на 2, 8.
То есть за один такт на самом деле мы можем
перегнать в 8 раз что-то по шине данных.
Опять же, где это читать, это нужно читать
в спецификах архитектуры видеокарт.
Точнее, оперативной памяти видеокарт.
И в итоге мы с вами получаем следующую величину.
616 ГБ в секунду.
Давайте, чтобы вы меня не считали арманщиком.
1750 на 352.
616, это в гигабайт.
Ну что, кажется ли вам эта скорость большой?
А?
Вообще да, но для видеокарты это мало.
А теперь давайте посчитаем следующую вещь.
Очень простую.
Эффективная пропускная способность.
Это количество байт на единицу времени,
которое мы с вами перегоняем.
И получается, что нам нужно посчитать количество байт,
которые необходимы для всех операций
чтения и записи.
И здесь мы получаем следующую основную часть.
Мы читаем X, читаем Y, пишем Y.
Получаем 3n операции, 12n байт.
А теперь смотрите, 228 байт.
28 на 3 на 4.
Сколько это у нас получается?
Это у нас в байтах.
Делим на 1024, делим на 1024.
Получаем, нам надо перегнать 3 гигабайта.
Для того, чтобы вычислить эту задачу.
3 гигабайта.
Мощность этой оперативной памяти,
частота, с которой мы гоним эту оперативную память,
это количество элементов массива.
Здесь примеры будут для N228.
И примеры семинаров тоже.
Все для N228.
А теперь давайте посмотрим,
что у нас получается.
Это количество элементов массива.
Здесь примеры будут для N228.
И примеры семинаров тоже.
Все для N228.
То есть мы складываем 2,28,
вектор размера 2,28,
вот таким вот способом.
Ну и что? Давайте посчитаем.
Теперь следующая вещь.
Мы берем 3, делим на 616.
Сколько мы получаем?
Мы получаем с вами 4,8 мс.
А время работы этого ядра
по замерам?
Смотрите, сколько.
5,7 мс.
То есть какую долю времени
мы тратим на то, чтобы посчитать
просто брать элементы из памяти?
Ну, что нам нужно сделать?
Одно на второе поделить?
Да, то есть смотрите, 83% времени
во время вычислений ядра
мы занимаемся тем,
что читаем элементы из памяти.
То есть это очень медленная операция.
То есть как минимум там
можно еще быстрее сделать,
и в итоге у нас получается,
что вот по факту перещелкиванием
элементов, складыванием, перемножением
мы занимаемся на самом деле
от силы там 1 мс.
То есть как минимум вот то 10,10
можно легко превратить 10 в 11,
а то и даже в 10,12.
Потому что мы владеем
немедленностью.
То есть смотрите, сейчас что произойдет.
Если вы на видеокарте запустите
тот же самый код,
ну а просто сделаете фор,
эффективно, так сказать,
запустите всю ту же сетку,
то работать это будет в разы быстрее,
а не 10,10 это раза в секунду.
То есть память у нас является
лимитирующим фактором для работы
с видеокартой.
Так, это понятно?
А значит, что если у нас память
является лимитирующим фактором,
то что нам нужно сделать с памятью?
Нет, детально изучить.
Поэтому следующий фактор, который у нас
будет, это следующий
способ. Значит, первый способ,
который позволяет это все сделать,
это заблокировать память
на хосте. Что это означает?
Это означает, что мы с вами можем
по факту сказать, что вот
та память, которая используется
там для трансферинга данных
видеокарт, она у нас
блокируется,
и мы выполняем ядро. Это каким
образом можно ускорить
кудомимцпай операцию?
Потому что операция кудомимцпай тоже работает очень долго.
Во-первых, чтобы не делать синхронизацию,
да, именно для того,
чтобы не делать синхронизацию на кудомимцпай,
использовать операцию кудомимцпай оси.
Но
как работает классическая
модель в памяти?
Вы знаете, наверное, из курса
операционных систем, что вся память
оперативная, она бьется на куски страниц.
Эта память называется пейджибл.
Так вот, видеокарта позволяет сделать так,
чтобы те куски памяти,
которые у нас есть, были не пейджибл,
а были пинт памятью.
Что это позволяет делать?
Это аналог функции mlock.
Я знаю,
рассказывали вам про нее
на курсе операционных систем.
Прямо вызов mlock функции,
memorylock.
По факту, что позволяет сделать
mlock? Она говорит вам следующее,
что ваша память не может быть выгружена
в swap.
То есть у вас, по факту, память
не модифицируется
вне механизма
чтения.
И тогда вы можете сделать следующее.
Вы можете создать
классическую пейджибл память,
далее вы делаете
пинт память параллельно с ней
и делаете копирование
из пинт памяти
в память видеокарты.
Пинт память — это память,
которая не подвержена
механизму
работы со страницами.
То есть вы ее фиксируете
в определенном месте
и по факту отдаете ее
на откуп управления
не уже операционной системой.
Как это выглядит?
Да, ее прям многому не выделить.
Но это не может быть
измена,
измена,
измена,
измена,
измена,
измена,
измена,
измена,
ее прям многому не выделить.
Потому что у нас по факту
мы уходим из механизма виртуальной памяти.
Вот.
Это недостаток этой штуки.
Их надо использовать только в тех местах, которые...
Значит, смотрите, у вас появляется
пейджибл память
и появляется пинт память.
А дальше у вас
есть этот в рам.
Собственно, вот эта память
можно выделить при помощи механизма
kudo-host-lock.
Причем ее можно запустить
с похожим параметром, как mmap.
Вот.
А здесь вы копируете
при помощи memcpy.
А здесь вы копируете при помощи
kudo-memcpy-i-sync.
При этом пинт память
можно напрямую писать тоже элементы.
Я вот как раз вчера сидел, разбирался
и были такие примеры.
Это ускоряет программу
в том случае, если вам
лимитирует именно скорость переброски данных
из CPU-памяти
с GPU-памяти.
Такое бывает чтение.
Потому что, несмотря на то, что код
нашего ядра, который мы с вами
запускали, работает 5.77 мс.,
а копирование памяти
работает 300-400 мс.
Зачем копировать память?
Зачем выполнять ядро, когда
копирование памяти просто сжирает
все время работы видеокарты?
Вот это понятно?
Хорошо.
Вот она картинка, кстати.
Все равно
дататрансферы он как раз все делает
через пейджел памяти,
а здесь у нас как раз все работает
через пинт памяти.
А теперь давайте разберемся наконец-таки
с иерархией наших
учтительных устройств.
Не знаю, видели ли вы эту картинку
или нет. Скорее всего, где-то ее
рассказывали, что на самом деле скорость
доступа к памяти, которая у нас
имеется, она
на самом деле
сначала она является слишком медленной,
объем у нас достаточно большой.
После этого, чем
больше скорость доступа
хотим, тем
стоимость наших ресурсов
экспоненциально растет,
но при этом получается
объем памяти
становится сильно меньше.
И здесь как раз можно попытаться
пересчитать количество ресурсов,
которые нам приходят сюда секунду.
То есть объем памяти на секунду,
получается,
сколько мы платим денег за 1 гигабайт памяти
и сколько
мы скорости платим
в гигабайтах на секунду.
Значит,
самым медленным здесь является лента.
Вы когда-нибудь
видели ленты магнитные?
Вы не поверите,
до сих пор бэкапы хранят на магнитных лентах.
Да, в том же самом
Амазоне.
Вот они.
Так, магнитные ленты
для хранения данных.
Вот так они выглядят.
Ну, это старые какие-то,
но есть достаточно много
из них.
Это именно система хранения данных.
Это самый тяжелый бэкап.
Но почему работать с лентами
очень неудобно?
Потому что совсем
непроизвольный доступ к памяти.
Он линейный, да.
Но при этом, как ни странно,
Linux разработчики
часто работают
с форматом хранения данных
именно для этих лент.
Все очень просто.
Обычно, где хранят
разные данные?
В каком смысле разные данные?
Данные, которые слишком много места
обычно занимают.
В каком формате хранят?
В каком формате хранят?
В каком формате хранят?
В каком формате хранят?
Ну, пример.
У нас есть какой-нибудь
набор текстовых файлов.
Они занимают слишком много места.
Архивируем.
Хорошо.
Ну, теперь смотрите.
Пишем R.
Архив.
Ленточный формат.
Ой, я кажется...
Что сломал?
Делаю экран.
Ленточный
формат
называется Tape.
Tape-архив.
Да, то есть самый первый
формат архивирования, который появился,
это формат Tar.
И он был предназначен на том, чтобы хранить
архивы именно
на магнитных лентах.
Да-да-да.
Это не ваши зипы, рары и так далее.
Именно в Tar-формате
ленточные штуки позволяют хранить данные.
Дальше, если мы с вами поднимаемся
по ленте, то у нас появляются
разные диски.
У нас появляются диски жесткие
и диски Solid State Drive.
То есть диски, которые...
Как это называется?
SSD, да.
Flash-накопители.
Обычно твердотельные накопители.
И мы с вами
можем даже посмотреть
в магазинах, сколько
твердотельный накопитель стоит за терабайт
и сколько обычный накопитель стоит за терабайт.
Хотите эксперимент?
Так, какой магазин
вы полюбите?
Хорошо.
Так, хорошо.
Магазин CityLink.
Господи.
Товары, товары.
Да.
SSD-накопители, давайте смотреть,
сколько он стоит.
Да,
нативную рекламу.
Вот, давайте NVMe возьмем.
У нас получается, смотрите,
семь с половиной тысяч
за терабайт.
А?
Да.
При этом скорость доступа,
какая у нас получается?
Три с половиной гигабайта в секунду,
да, где-то.
Ну, два гигабайта в секунду.
Так.
Хорошо.
Теперь пойдем смотреть жесткие диски.
Давайте первое.
Что?
Вэдэблю?
Вэдэблю?
Ну, блю.
Тут ценное качество, конечно.
Лучше вот это. Давайте возьмем.
А?
Сколько?
Восемь терабайт.
Нормально.
У меня дома восьми терабайт на ДИК ставит.
И тут заканчивается.
Так, значит, HDD.
Сколько тысяч рублей за терабайт
получается?
22 делить
на 8, это кажется 3,75.
О, Господи, да.
Три раза дешевле.
А?
Ну, слушайте, да.
Ну, на самом деле,
тут мощность хранения данных есть.
Так, а какая скорость у него доступа?
Вряд ли.
Я думаю,
SDD будет под сотню стоить.
Все характеристики.
Максимальная скорость интерфейса.
600 мегабайт в секунду.
То есть мы видим с вами цену ресурсов.
Давайте теперь поднимемся выше.
Сильно дешевле.
На терабайт, но и сильно медленнее
по скорости.
Так, дальше у нас идет кто?
Оперативная память.
Так, давайте
оперативная память.
Оперативная память.
Какую возьмем?
Давайте DDR5.
Капец они мощные стали.
О!
Пойдет, короче говоря.
Итак, что мы здесь видим?
Объем памяти 32 гигабайта.
На 10, сколько?
На 11 тысяч.
Чтобы получить твердую память.
На 11 тысяч.
Чтобы получить стоимость на терабайт,
что нам нужно сделать?
Нам нужно эту 10 тысяч бумаж на 32.
Получаем 352 тысячи рублей за терабайт.
Потому что у нас 32 гигабайта
объем оперативной памяти.
Это ну за терабайт, конечно.
То есть, видите, порядок
возрос 40 раз.
Но при этом, что у нас
происходит со скоростью?
Битность шины.
Здесь есть...
О, пропускная способность.
Сколько это получается?
38,4 гигабайта в секунду.
Ну, видно разницу.
То есть, получается,
у нас и здесь выросло в 20 раз,
и здесь приблизительно в 40 раз
ценник поднялся.
Дальше что идет?
Кэш.
Значит, как оценивать кэш?
А?
Не знаю.
Не знаю.
Не знаю.
Не знаю.
Да.
Он там есть многоуровневый.
И нам просто нужно, на самом деле,
зашить некоторую величину относительно
скорости центрального процессора.
То есть, относительно стоимости центрального процессора.
Давайте возьмем какой-нибудь
центральный процессор.
Какой мы с вами возьмем?
Вот этот, допустим, да?
Так.
У нас мощность 3,9 гигагерца.
Число ядер потока
столько.
Кэш.
Ну, сколько кэш у нас получается?
20 мегабайт?
Плюс-минус, да?
А?
Ну,
это некорректно.
Я согласен, что это некорректно сравнивать.
Нам нужно плюс-минус прикидочное значение
получить. А?
Ну, 20...
Так. L2 кэш у нас на каждое ядро
получается.
16 плюс 12 на 3...
Так.
Плюс 3 на 6. 34
получается, да?
34 мегабайта.
Нам нужно 1024
на 1024 поделить на 132.
1024 на 1024
поделить на 34.
Ну, вот.
Получаем 30 тысяч
и вот эту величину,
которую нам нужно
умножить на 14 тысяч.
Давайте не на 14 тысяч.
Предположим, что, как вы думаете,
кэш сколько стоит
относительно цены процессора?
Ну, давайте даже скажем
5 процентов.
Ну, сколько?
15. Хорошо.
Так.
Получаем с вами 65 миллионов рублей
на турабайт информации.
Скорая доступа.
Ну, давайте тут напишем порядка
гигабайта в секунду.
То есть, за очень быструю скорость
нам действительно нужно очень много платить.
Ну, и самый верхний уровень
этой всей пирамиды
это регистры.
Сколько у нас регистров?
Ну, штук 20.
Соответственно, здесь уже
почти под бесконечно все будет.
Ну, да, потому что сколько?
6 регистров это, образно говоря,
даже на каждое ядро,
если у нас там 20 регистров,
то 20 на 12 получается 240,
там на 4,
ну, где-то получается
тысячу байтов
получается где-то.
1 килобайт получается где-то.
1 килобайт, собственно,
здесь будет сложно считать,
а скорость доступа, ну,
по турабайт, наверное, можно получить.
Вот такая пирамида.
Это пирамида именно на ЦПУ,
как она у нас работает.
То есть, на классическом компьютере.
Так, понятно ли вот это?
Вот это вот разъяснение.
То есть, мы с вами даже провели эксперимент
в том, как это считается.
Мне кажется, просто это
никто не показал.
Вот, если мы говорим про РАФИС
и НОЦПУ, то
здесь, наверное, немножко ошибся
в какой-то степени.
У нас с вами есть регистры,
из которых, если у нас данных нет,
то мы доходим до уровня L1-L2 cache,
за 5-12 тактов процессора
получаем память.
Дальше, если у нас чего-то нет в L1-L2 cache,
мы идем в L3 cache, который уже общий
и получаем информацию
о том,
уже со скоростью где-то
200-400 гигабайт в секунду.
А дальше, если
нам не повезло с Cache,
то мы уже идем в оперативную память
и скорость доступа падает
достаточно сильно.
Ну да, где-то мы по оценкам, кстати,
не ошиблись.
То есть, чем дальше,
чем ближе мы делаем запрос
в оперативной памяти,
тем меньше объем мы с вами можем перегонять.
Что касается иерархии
самой видеокарты,
здесь у нас есть 3-ми мультипроцессоры,
к каждому из 3-ми мультипроцессоров
подключена L1-Cache,
он имеет определенный объем,
и дальше между ними есть L2-Cache,
между ними всеми,
общий L2-Cache.
И мы с вами
в какой-то момент времени
должны сказать, что у центрального
процессора всегда есть
линей, в которых мы можем
загружать последовательно, и дальше
все к ней могут обращаться.
Если мы говорим про ГПУ,
то ширина окрашения ГПУ 128 байт.
Нет, это для НЛИДИСКИ.
Мы тут говорим именно про...
Да, для других
надо смотреть спецификацию.
Да, они зафиксировали это
под свою архитектуру.
Так, а теперь магия.
Значит, заключается следующее.
Вмещается ровно 32 флота.
Да, совпадение недуманное.
Вообще, смотрите,
cache-линия подгружается на один ворб.
А это означает следующее,
что все элементы, которые обращаются
внутри одного ворпа,
могут обращаться как раз
внутри одной кашлини.
Если вы вдруг пойдете
в другой объем памяти,
то, увы, у вас
кашлиня сломается.
В другой кусок памяти.
Кстати, там будет про это еще слайд.
Теперь про ГПУ.
В чем заключается?
Особенность в том, что здесь L1 L2 cache
и доступ к L1 cache
намного медленнее, чем доступ
к оперативной памяти.
При этом видно, что пропускная способность
сильно больше, и наша цель
как разработчиков видеокарты
на самом деле уметь
эффективно обращаться
либо к L1 cache
либо к
участку памяти, который очень сильно
собой напоминает L1 cache.
То,
что именно у нас
может находиться на чипе.
То есть у видеокартов,
у ГПУ, на самом деле,
есть несколько типов памяти.
Значит, есть общая память, глобальная.
Есть память, которая внутри находится
на одном чипе.
Внутри одного стриминга мультипроцессора.
А теперь
смотрите интересную картинку,
которая у нас здесь есть.
У нас с вами есть, так сказать,
он-чип-мемори.
Это shared память,
local памяти и регистр.
То есть она находится на самом чипе,
и размер ее не очень большой.
При этом мы видим с вами, что shared память
выделяется на блок,
локальная память выделяется на один поток,
регистры тоже выделяются на один поток.
А глобальная память,
константная память и текстурная память,
они выделяются
целиком.
Текстурная память это та, в которой можно
использовать текстуры,
это отрисовки компьютерной графики.
Константная память
это память, в которой вы можете хранить константы.
Размер ее тоже не очень большой.
И есть глобальная память, в которую мы обычно
закидываем массивы.
И что такое
разделяемая память?
Это особенность видеокарты.
Мы можем использовать память
на уровне чипа.
Если мы посмотрим
про центральный процессор, у нас такой возможности нет.
Единственное, что нам позволяет
каким-то образом процессор управлять,
это использовать ASCII и AVX инструкции.
Более ничего.
Здесь же мы можем управлять
внутри чипа.
И для того, чтобы такую память объявить,
нам нужно объявить ключевое слово shared.
При этом данные
между всеми потоками
распространяются именно в одном блоке.
И появляются
локальные массивы
с быстрой скоростью доступа, несколько
секунд. Единственная проблема
shared памяти заключается в том, что сначала память
не нужно скопировать.
У вас есть глобальный массив, у вас внутри каждого
блока выделяется маленький локальный
участок, в который вы можете обращаться.
То есть у вас появляется
свойство локальности данных.
Правда, удовольствие ограниченное.
Всего на архитектуре
видеокарты можно выделить порядка
48 килобайт на блок.
Но смотрите, давайте посчитаем.
Размер блока у нас обычно какой?
В видеокарте.
Обычно, я не знаю, проводили ли вам
замеры, максимальный размер блока
это 1024 байта.
1024 элемента на 4 байта.
То есть у нас получается
4096 байта.
4 килобайта.
То есть по факту, если у вас размер
блока 4 килобайта,
то вы в разделяемой памяти
можете хранить
X12 от этого значения.
То есть у вас получается на каждый элемент
вы можете хранить дополнительно
12 вы разделяемой памяти.
Если вы делаете меньше размер блока,
то вы можете большее количество элементов
хранить на одном блоке
в разделяемой памяти.
То есть это по факту такая буферная зона, в которой
вы можете обращаться.
Как объявлять размер
shared памяти?
Вот это тоже важный момент. Ее можно объявлять либо статически, либо динамически.
Статически
это мы делаем следующее.
Мы объявляем вот в этом моменте времени
shared какой-нибудь int
x
и дальше пишете размер массива.
Там, допустим, 128.
То есть вы определяете это
статическим образом.
Второй способ, который нам может быть
полезен, это
следующий.
Вы объявляете extern
и в квадратных скобках указываете
то, что вам необходимо.
А как это определить размер массива?
У кого-нибудь есть мысли?
У кого есть мысли, куда
ее можно засунуть?
Она должна объявляться динамически.
Кто у нас вызывает
ядро?
Где у нас
в этом моменте времени
вызывается ядро?
Видите?
Это оператор тройной скобки.
Это
операторы параметров ядра.
У параметров вызовов ядра есть
третий параметр.
Это количество байтов,
которое вы делаете в shared memory
на каждый блок.
Именно количество байтов.
Таким образом, когда вы
обращаетесь к разделаемой памяти здесь,
вам нужно будет автоматически
отсчитывать указатели на следующий элемент.
Потому что глобальная shared память общая,
если вы хотите передать туда
два массива в shared памяти,
правильно настраивать индексацию,
потому что это будет последний блок.
Это можно делать
третьим параметром вызов ядра
передать количество байтов.
Примеры с разделяемой памяти
будут на семинарах.
Теперь давайте
поговорим про то,
где располагаются разные памяти.
Вид памяти у нас
есть типы памяти
Register, Local, Shared, Global
и Constant.
И дальше указывается,
где эта память сохраняется.
Register и Shared находятся
на самом чипе видеокарты.
Local, Global и Constant
находятся вне чипа.
При этом та память,
которая не находится на чипе,
она может быть закэширована.
Вот это важно.
Остальная память не кэширована.
Смысл ее кэшировать,
если она и так уже находится на чипе.
Про доступ.
Все они работают в режиме read-write,
кроме Constant.
Вы туда можете числа передавать
установленное какое-то значение
и его использовать.
Тут разные уровни кэша.
Constant хранится
на уровне L1 кэша.
Зачастую локальные переменные
обычно хранятся на уровне L1.
На уровне L1 кэша
глобальные все-таки скидываются
на размер L2 кэша.
В какой области видимости
они находятся?
Регистр и локальные переменные
можно будет прочитать
на уровне одного потока.
Shared видит все потоки
внутри блока.
Глобальная память видит Device и Host.
И Constant тоже видит Device и Host.
Как CPU, так и GPU их видят.
При этом жительный цикл,
где они определяются.
Регистр и локальные переменные
определяются внутри потока.
Shared определяются внутри блока.
Константные глобальные памяти
определяются на Host.
И дальше функциями куда
перекидываются либо на блок,
либо перекидываются
внутри гидрата.
Вот такая сводная таблица
по типу памяти,
которая у нас существует.
Давайте остановлю здесь
и спрошу, понятно ли это табличка.
Хорошо.
И теперь надо рассказать
про синхронизацию внутри
shared памяти.
Почему это важно?
У нас есть такой код,
и здесь появляется
одна важная инструкция.
Кто ее может найти?
Синк тредс.
Что делает синк тредс?
Синк тредс – это
барьер внутри одного блока.
Это означает,
что все операции,
которые вы сделали внутри одного блока,
доходят до этого момента.
И ставится глобально в блокировку.
Зачем это необходимо?
Это необходимо для того,
чтобы избежать датарейса.
Когда у вас
часть потоков, допустим,
прошла в следующее вычисление,
тут, видите,
в коде уаттит,
это ut от treddx
минус ut от предыдущего treddx,
у нас значения какие-то могли
еще не записаться,
и с этим возникли проблемы.
В какой-то ячейке запись у нас есть
внутри блока, какой-то нет.
Здесь ставится глобальный барьер
на блок.
Это делается всегда
на случай работы
с разделяемой памятью.
Вопрос.
Каким образом можно легко
сделать
дедлог
на видеогарте?
У кого есть мысли
по этому поводу?
Какой код вызовет нам
дедлог?
Это типичная ошибка,
которая иногда
допускает люди
смотреть.
Пример очень простой.
Вот такой код вызовет
дедлог.
Да,
не все потоки
внутри блока его вызовут.
Потому что только один поток
внутри этого блока вызовет
сингтресс.
Да, это барьер.
Ну вот.
То есть аккуратнее.
Хочется только на эффективную
операцию ставить,
а не только на эффективную.
Хочется только на эффективную операцию ставить
сингтресс, нет.
Внутри ифоксингтресс, пожалуйста, не ставьте.
По уровню доступа к данным,
давайте тоже поймем
по регистрам следующее.
Как раз в прокашлине.
Представьте себе, что у нас есть размер блока
256 и мы хотим одним потоком
обрабатывать два элемента.
Этот пример я уже на семинаре показывал.
Сейчас покажу еще всем.
Способ первый.
Пусть у нас каждый поток
будет обрабатывать элементы
в последнем порядке.
То есть 0.1, потом 2.3, потом 4.5,
потом 6.7 и так далее.
Второй способ, который у нас
будет, это следующий.
Обрабатывать элементы вот таким образом.
То есть делать перескок через размер блока.
То есть первый поток будет обрабатывать
0.256,
первый 1.257,
третий, то второй 2.258
и так далее.
То есть со сдвигом на размер блока.
Вопрос.
Как вы думаете,
какой способ будет работать
на CPU быстрее?
Какой?
Первый.
Потому что у нас элементы подряд,
каждый поток считывает последним.
Да, почему на GPU работает
второй быстрее?
Давайте посмотрим код, кстати,
который это показывает.
То есть есть функция add,
которая как раз перескакивает
через размер блока и получает
итоговые результаты.
Есть функция stupid add,
которая выделяет и складывает по
8 элементов последовательно.
А теперь
вспоминаем, что я говорил.
Я говорил про то, что размер кэшлини
равен размеру варпа.
То есть у нас получается,
если у нас с вами есть
какой-то нулевой элемент массива,
у нас с вами есть нулевой элемент массива
и он запросил кэшлини,
то он весь варп
запросил кэшлини
с элементом 0.31.
Соответственно, элементы внутри
кэшлини могут общаться как угодно,
но главное не выходить за эти пределы.
Когда мы складываем 0.256,
то у нас
для вот этих ребят подгружается
свое собственное кэшлини,
и все эффективно считают,
потому что нулевой с нулевым,
первый с первым и так далее.
А 256 и 257 и так далее.
Для них
подгрузится свое кэшлини.
То есть на каждую операцию у нас идет
кэшлини. А теперь смотрите
альтернативный способ.
У нас будет нулевой элемент,
который будет подгружать значение.
Нулевой элемент подгрузит
себе нулевое значение,
потом в одно и то же кэшлини первое значение
и так далее. 31.
Отлично, но в такой концепции
четвертый поток,
в то время как
напоминаю, что это происходит
в первом уровне, то есть все операции будут
отшлины в один такт,
четвертый поток
внутри ворпа
пойдет и запросит значение
из 32-го элемента.
Вот в самый момент времени.
То есть он выйдет
за пределы кэшлини,
которого подгрузили на этот ворп.
И у нас возникнет кэшмисс.
И нам придется лезть
в глобальную память для того,
чтобы подгрузить эти значения.
Понятен тезис.
Почему это
лучше писать
с пропуском черед размер блока?
Естественно, если размер блока делится
на 32.
В прошлом раз говорили, что размер блока
должен делиться на размер ворпа.
И тот самый пример,
который у нас был.
Нормальный способ
сложения будет работать за 5,7 мс.
То есть у нас
никаким образом не меняется результат.
А медленный способ начинает работать
в 3-4 раза медленнее.
То есть на семинарах
до 10 раз
замедление получалось.
То есть этим нужно быть аккуратнее.
Способ складывать элементы таким
образом быстрее.
И я про это хотел
сказать.
Все данные, которые
обращаются к ипотокам
могут обращаться
внутри одного ворпа, чтобы не сломать
кашливую.
Чем больше выгрузок в ней кашлять,
тем больше время работает наша программа.
Это важно учить.
И давайте
последний тезис, который мы сегодня успеем
разобрать. В следующий раз мы тогда про синхронизацию
будем детально говорить.
В видеокарте есть механина
предсказательного вычисления.
Что это означает?
Это означает следующее.
Если вы хотите вычислить какое-то значение
и вы вычисляете ветку
с if и с else,
дальше вы вычисляете
значение в
то, что у вас
находится внутри
if.
То есть у вас получается следующее.
if x,
дальше else.
Вы стараетесь вычислить ветку,
которая находится внутри
здесь и здесь, параллельно.
Потому что скорее всего у вас
распределение произойдет по потокам.
То есть у вас запускается одновременно две ветки вычисления
и
после того, как вы посчитали значение в
x, вы подставляете маску
по тем результатам, которые у вас
получаются здесь.
То есть те ветки, в которых x был равен true,
подставляет результат,
который здесь.
Если с else, то подставляет результат,
который здесь.
Поэтому вы на самом деле
не увидите где-то функции
if или еще что-то там,
даже вычисления инструментов.
То есть у вас скорее всего будет функция
под названием tf.v
или там pytorch.v,
где у вас пишется функция, которую вы проверяете
и дальше одновременно два массива,
которые вы должны вернуть.
Массив, который выполняет значение true
при условии того,
что у вас значение верно,
у вас значение false,
там, где у вас значение неверно.
То есть вы сразу готовите два массива
и подставляете результаты по матке.
И за счет этого
ускоряется время работы на видеокарте сильно.
И вот такая вот
финальная картинка,
которая у нас возникает.
Не пишите, пожалуйста, такой код.
Он нужен только для отладки.
То есть у вас получается один поток внутри варпа
выполняет все операции,
а остальные простаивают в это время.
То есть у нас получается
время варпа,
время работы процесса
внутри варпа.
Это как раз, так сказать,
самое долгое время работы потока
внутри этого варпа.
Вроде латничное.
Все равно
видеокарта не может вам
сделать undefined behavior,
поэтому с этим будут проблемы.
Все, про особенности
организации тогда мы, наверное,
будем говорить в следующий раз.
И, наверное,
в следующий раз я еще
принесу примеры для отладки
программ. Посмотрим, собственно, как отлаживать
программу, потому что
с этим тоже бывают некоторые проблемы.
Потому что
вот какое поведение
у нас будет в случае
того, если ядро не выполняется?
Да, ошибка в ГПУ ядре.
Паника?
В каком смысле паника?
Нет, нет, нет.
Там возникнет следующее.
У вас ядро такое скажет
и вам ничего, там не кинется
никакой exception, ничего не кинется.
У вас просто получается, вы замеряете
время работы ядра, у вас получается ноль
по времени ядра.
Ну или какие-то там миллисекунды.
То есть у вас никаким образом не уведомляется
напрямую о том, что у вас
происходит. А по факту
в результате вызова ядра
хранится флаг Ярно.
И именно его надо доставать,
потому что видеокарты это больше сишные
инструкции.
Вот, давайте
наверное вопрос. Сегодня мы с вами как раз
посмотрели на особенности
работы с памяти
на видеокарт и научились
замерять время работы программы.
Но поняли, какие виды памяти
существуют.
Все, надеюсь, что
в следующий раз у нас будет все.
К меньшим количествам косяков.

Ну что, поехали? Последние три лекции мы с вами говорили про то, как писать код, который запускает
потоки, который в потоках что-то делает, который конструирует сами потоки. В общем,
мы говорили про пространство пользователей, про то, как поверх процессора описывать какие-то
вычисления конкурентные. На два ближайших занятия жизнь у нас серьезно поменяется,
потому что мы перестанем говорить о том, как писать код. Ну ладно, это неправда,
будем говорить сегодня, как писать код. Но в первую очередь у нас сегодня и на следующей
неделе задача будет другой. Мы поговорим с вами о том, что мы принимали с вами с самого начала
курса как данность, а именно понятие разделяемой памяти. Ну да, мы себе представляем процессор,
представляем, наверное, даже ядра себе. Эти ядра работают с общими ячейками памяти. Ну и для нас
такой вот планка памяти, там есть ячейки, мы их читаем и пишем. Вот я покажу сегодня и особенно
через неделю о том, что вот такого представления, конечно же, недостаточно, чтобы делать что-то
полезное эффективно. Ну и давайте мы вспомним, с чего мы когда-то начали говорить про вообще
многопроцессорные системы, про разделяемую память, мы вспомнили, поговорили про закон Мура,
который говорит, что там каждые два года примерно количество транзисторов в процессоре удваивается.
Почему? Потому что они становятся меньше, это продолжается до сих пор, и раньше они становились
быстрее еще вместе с этим, они быстрее переключались. Но в какой-то момент эта история закончилась,
то есть процесс транзисторов все больше и больше, но быстрее переключаться они не стали,
тактовая частота, рост тактовой частоты остановился, и это толкнуло людей к тому,
чтобы строить многоядерные процессоры, которые вычисление производит параллельно. То есть мы не
можем в единицу времени больше их проводить, ну в смысле на одном вычислительном ядре, на одном
юнити вычислительном, но мы можем реплицировать этот юнит и теперь делать там четыре раза больше
вычислений параллельно, если, конечно, они при определенных условиях четыре раза быстрее.
Ну вот об этом мы сегодня и хотим поговорить, собственно. Так вот, с одной стороны, то, что вот
закон Мура выполняется, но только для количества транзисторов, но уже не для частоты, это вроде бы
печальный факт. Ну то есть, как у вас там почти 20 лет назад были процессоры с частотой в несколько
гигагерц, так и сейчас они остаются и быстрее не становятся. Но должно ли нас это очень сильно
печалить? Ну вас это печалит, например? Очень печалит. А вот не должно на самом деле, потому что
проблема не в ядрах и не в процессорах, проблема в том, что память медленная. Вот обращение к
памяти процессора стоит, ну, примерно в сто раз дороже, чем манипуляция с регистрами. То есть,
вы, конечно, можете ускорить процессор, но узкое место сейчас в компьютере – это не процессор для
вас, это хранилище, но в частности память. Этот разговор можно как бы и дальше распространить там
на внешнее хранение на диске, но вот память уже в сто раз медленнее, чем процессор. Поэтому хочется
в этом месте что-то ускорить, чтобы программы становились быстрее. Ну и в таком случае,
когда у нас есть очень медленные хранилища и очень быстрые клиенты, условно, есть для такой задачи
общее решение, которое называется кэширование. Ну, тут картинка не про память совсем, конечно,
не про процессор, а про какое-то внешнее большое хранилище, какую-то холодную базу данных,
которая хранит все данные на диске, и к ней обращаются клиенты. Обращаются, видимо, часто. И
мы хотели бы, чтобы этих клиентов быстро обслуживали. Для этого мы между клиентами и большим хранилищем
помещаем компонент, который называется кэш. Ребят, потише немножко, вы меня сбиваете.
Помещаем кэш. Кэш гораздо меньше по объему, чем хранилища, разумеется. В него все данные не
могут поместиться, но он старается обслуживать чтения, которые выполняют клиенты, которые
обращают клиента к хранилищу. Значит, клиент сначала идет за данными в кэш. Если он там
находит ответ, в ситуации называется кэш хит, попадание по кэшу, то в хранилищ медленный мы
не ходим. Если же мы в кэше не находим ничего, то мы, так уж и быть, идем в медленное хранилище,
далеко. Получаем от него ответ и поселяем этот ответ в кэше, чтобы в следующем обращении мы
попали в этот кэш, и ответ был быстрее. Эта ситуация называется промах по кэшу, кэш мисс.
Конструкция понятная? Вот, конечно, очень много всего неизвестного, что это за хранилищ,
какой у него интерфейс, как именно работает кэш, какой там механизм вытеснения, но нам сейчас это
не очень важно и вообще сегодня не очень важно. И давайте лучше перейдем к конкретному примеру,
а именно к нашему примеру, к процессору и памяти. У нас в качестве медленного хранилища есть
оперативная память. Там лежит какая-то ячейка, какая-то переменная X, и мы собираемся из нее
прочесть значение в регистр, для того чтобы что-то с ним сделать. Сначала мы эту ячейку ищем в кэше,
который мы устанавливаем между ядром и памятью. Кэш гораздо меньше, чем память, кэш гораздо
быстрее, чем память, и если ячейка находится в кэше, то мы в память вообще не входим в этой
инструкции. То есть, если случается кэш хит, то это положительный сценарий, кэш мисс. Если кэш мисс,
то мы проваливаемся в память, идем и читаем данные оттуда. Но что важно, мы читаем оттуда не одну
ячейку памяти, это было бы очень неэффективно. Мы оттуда читаем целый блок памяти, который мы
назовем линией. И эта линия в типичном современном процессоре составляет 64 байта, 8 машинных слов.
Вот когда вы читаете что-то из памяти, вы читаете не одно машинное слово, а сразу 8. И эти 8 машинных
слов эту линию помещаете в кэш. Ну вот такая запись в кэше называется кэш-линей. Ну а дальше уже вы
оттуда берете данные. Конструкция понятна, ну пока терминология, которая возникает, понятна. Если
понятно, то сразу нужно уточнить, что кэш между ядром и памятью разумеется не один, их целая
иерархия. У нас есть кэш первого уровня, кэш второго уровня, кэш третьего уровня, и каждый последующий
кэш больше, чем предыдущий, дальше от ядра, чем предыдущий, медленнее, чем предыдущий, дешевле,
чем предыдущий. Ну и вообще поддерживается такое свойство, что если у тебя есть, если кэш-линия
находится в кэше более младшем, то она есть и во всех старших кэша. Ну типичные размеры тут
указаны. Для кэша первого уровня это 1632 килобайта, для кэша второго уровня это там 256 килобайт,
512, 2 мегабайта, кэш третьего уровня 8 мегабайт. А размер памяти, ну не знаю, 100 гигабайт. Довольно
серьезный зазор, да? Вот. Но тем не менее утверждается, что даже такие крошечные кэши,
выстроенные перед памятью, существенно ускоряют исполнение вашей программы. Да, еще маленькое
замечание, но не знаю, насколько полезно нам сегодня, что кэш первого уровня, как правило,
разделяется на кэш для инструкции и кэш для данных. Для инструкции тоже нужен кэш,
нужен кэш, чтобы программа быстрее исполнялась, чтобы просто быстрее вычитывать ее код. Так вот,
прежде чем объяснить, почему такие маленькие кэши оказываются полезными, еще некоторые
технические комментарии, насколько быстро они работают. Вот если мы говорим про регистры,
то обращение, если мы говорим про кэш первого уровня, а что я, собственно, говорю вам? Давайте
я вам покажу лучше хороший референс. Это такое известное место в интернете из одного старого
доклада, и если вы гуглите, то гуглите вот latency numbers every programmer should know. Так оно и есть. Вы
все должны знать эти числа, ну по крайней мере порядок этих чисел. Тут говорится, что вот L1 кэш,
референс в него стоит 0,5 на на секунду, ну то есть вы можете миллиарды раз обращаться к L1 кэшу
в секунду. Ну вот более-менее так же быстро, как вы молотите инструкции. L2 кэш, ну вот он уже
медленнее в 10 раз. Референс обращения к памяти 100 на на секунду, то есть 100-200 раз медленнее,
чем обращение к L1 кэшу. Ну если говорить про диск, то с диском тут совсем все печально,
потому что обращение к диску, ну если у вас диск вращающийся магнитный, то там пока он
повернется, пока рука с головкой считывающей станет на нужную дорожку, это занимает 5-10 миллисекунд
миллисекунд. А у нас были на на секунды. Это вот, ну сколько, шесть порядков разница, да? В миллион
раз медленнее. Но вот эти числа, они из доклада 12 года, прошло уже 10 лет, но не то чтобы кэши и
память драматически ускорились за это время, нет, не ускорились, но все-таки числа могут меняться,
особенно там SSD, тут наверное устаревшие данные. Но можно даже эти числа не помнить, потому что
можно помнить скорее их отношения. И для этого есть довольно удачная аналогия, которая называется
пивная иерархия. Смысл такой, что, ну когда вот вы пьете пиво уже из бутылки, лимонад, я пью
лимонад вместо пива, то это вот обращение к регистру. Когда у вас вот бутылка стоит рядом с вами,
это L1 кэш, вы протягиваете руку, берете ее. L2 кэш, когда у вас стоит холодильник рядом с креслом,
вам нужно его открыть и достать. L3 кэш, когда вам нужно пойти к холодильнику, на кухню. Основная
память, это примерно как спуститься вниз к магазину у подъезда. А если говорить про обращение к диску,
если вдруг, не знаю, вашу страницу памяти вытеснили на диск, потому что вот физическая память
закончилась, то это примерно как за лимонадом отправляться в Австралию на яхте. Вот это очень
долго. Ну, то есть, если вы так себе это все представите, то получите, ну, будет понятно,
с какой скоростью все работает, даже если вы не помните отдельные числа. Ну вот, значит, кэши,
они довольно близко по скорости к процессору, но они замедляются, конечно, по мере удаления от
процессора, но все же существенно быстрее, чем память. Но они очень маленькие по сравнению с
памятью. И возникает вопрос, а почему же вообще они помогают? Почему же они что-то ускоряют? Ответ
следующий. Дело в том, что программы, которые вы пишете, они обращаются с памятью не совсем
произвольно. В них есть некоторые паттерны. А именно, программы соблюдают, как правило,
программы ваши соблюдают два типа локальности. Ну, один из двух типов локальности. Первая – это
пространственная локальность, вторая – это временная локальность. Пространственная локальность,
она говорит о том, что если ваша программа обращается к ячейке памяти, к какому-то
машинному слову, то, вероятно, она в ближайшем будущем обратится к какому-то соседнему
машинному слову. Но это совсем простой сценарий, совсем такой очевидный, естественный. Вот вы,
не знаю, пишете Quick Sort или Merge Sort, а вы там ходите по массивам справа-налево, слева-направо. И тут
довольно кстати приходится то, что вы сохраняете в кэше не одну ячейку памяти, а целую кэшлинию.
Вот вы прочитали очередной элемент массива, а дальше следующие восемь вы читаете бесплатно,
практически. То есть паттерн очень естественный. Это пространственная локальность. И есть
временная локальность. Временная локальность говорит о том, что если вы обратились к ячейке
памяти, то, скорее всего, в ближайшем будущем вы обратитесь к ней же еще раз. Но опять,
представьте себе, что у вас какое-нибудь дерево поиска, и чтобы вы там не искали,
как правило, вы проходите через какую-то верхушку этого дерева чаще всего. Вот эта верхушка дерева
закашируется. Ну, может быть, для деревьев поиск, где у вас R-2, это не слишком крутая оптимизация. Но
когда, скажем, мы работаем с там B-деревьями или с деревьями вообще во внешней памяти, то это
становится уже очень важно. Ну вот, два типа локальности. Понятно, да? Понятно сценарий.
Зачем нам об этом знать? Ну, потому что пока мы не говорим про конкаренность и про многопоточность
вообще, а пока мы говорим просто про какую-то оптимизацию, которая есть в процессоре. Но в
любом случае она полезна вам, потому что вам разумно ее учитывать, когда вы пишете свой код. Вам
разумно с помощью знания о кышах, об иерархии памяти уточнить, детализировать свою модель
стоимости, которую вы используете при программировании. Ну вот, чему вас учат на алгоритмах? Что у вас есть,
что у вас единица стоимости, это, грубо говоря, обращение к ячейке памяти. Ну, что-то подобное. Но это
же воронье, потому что если вы обращаетесь к соседним ячейкам памяти, это одна история. А к далеким
ячейкам памяти совсем другая история. Это стоит сто раз дороже. Вот разумно учитывать локальность
в моделистой масте, когда вы оцениваете эффективность своего алгоритма. Ну, скажем,
вы приходите на собеседование после учебы и вас спрашивают, ну, там, что быстрее, итерация по
вектору или итерация по эстаделист? И вы отвечаете, ну, разумеется, итерация по вектору, потому что вы
сканируете там кэшлини одно за другой. А что происходит, когда вы итерируетесь по списку? Вы
каждый раз прыгаете по какому-то неизвестному адресу, какую-то неизвестную память и, скорее всего,
по кэшу промахиваетесь. И вот вы вместо того, чтобы промахиваться по кэшу раз в восемь обращений,
вы промахиваетесь на каждом обращении. Кажется, это не очень эффективно. Ну, или не знаю. Вы
увлекаетесь машинным обучением и вот любите умножать матрицу. Ну, что вы знаете про умножение
матрицы? Что его можно как-то оптимизировать? Какие-то рекурсивные сложные алгоритмы? Я предлагаю
ничего не оптимизировать. Я предлагаю написать тупой код. Надеюсь, я не ошибся в умножении
матрицы. Ну вот, просто двойной цикл. В нем еще один цикл. Считаем каждый римен. Запускаем. Вот
матрица размера тысяча. Некоторые генерируются случайным образом. Тут не очень интересно. Запускаем
и перемножили за одну секунду. А как это ускорить, если мы знаем про генериантность кэшей? Ну вот у нас
по первой матрице мы ходим построчно. И это разумно. А по второй матрице мы ходим по столбцам. И вот
каждый новый индекс K – это же прыжок в памяти, и это промах по кэшу. Вот мы давайте транспонируем
матрицу сначала B, а потом перемножим две матрицы построчно. И что у нас получится? Стало в пять раз
быстрее. Довольно приятно. Вот. И дело тут совсем не в алгоритмических каких-то оптимизациях. Это
нам даром не нужно. Вот мы просто транспонировав матрицу, получили производительность в пять раз
быстрее для однопоточного кода. Тут еще, конечно, огромное пространство для оптимизации, но вот уже
такое базовое знание про кэши дает нам возможность программу ускорять. Вот. Есть очень красивые
способы. Это, конечно, совсем не относится к нашему курсу. Это, скорее, такое балавство алгоритма.
Вот скажем, представьте себе бинарный поиск на каком-то огромном массиве. Вот как можно
оценить этот алгоритм с точки зрения промахов по кэшу? Насколько он эффективен? Но он очень плох. Мы
берем середину, потом четверть, потом… Вот мы каждый раз прыгаем далеко. В какой-то момент, конечно,
мы вместимся в одну кэшлинию, но там уже можно и линейный поиск запустить, уже неважно. Так вот,
оказывается, что люди этим озаботились и придумывают, как можно уложить данные для бинарного
поиска в памяти так, чтобы промахов по кэшу было как можно меньше, ну, просто минимальное
возможное количество. Причем для этого даже не нужно знать размер кэшлинии. Это такая очень
красивая рекурсивная укладка ВНМД БОСа. И если, скажем, вы пойдете в шат учиться, что довольно
разумно, то Максим Бабенко вам там расскажет на курсе по алгоритмам о внешней памяти к
такие штуки сделать. Ну, то есть, вы изучаете одни алгоритмы, а потом можно детализировать
модель памяти, детализировать модель стоимости, модель вашего компьютера и придумывать новые
алгоритмы, которые учитывают уже больше нюансов, которые больше параметров. И получать более
разумное приближение к вот настоящей железке. Ну что, есть ли вопросы пока? Если нет, то нужно
поговорить теперь о том, зачем вообще мы говорим про кэши в нашем курсе, потому что пока, ну, это
все полезно, конечно, знать, но пока не относится к конкаранции, к параллельности, вообще к
многоядерности. Ну, дело в том, что вот эта картинка, вот эта и вот эта картинка, она
усложняется, когда мы начинаем говорить про многоядерные процессоры, потому что оказывается,
что у каждого ядра свой собственный L1 кэш, ну, или L2 кэш тоже может быть общий, и они
соединены вот в такой интерконнект, через который вся эта конструкция общается с памятью. То есть,
смотрите, у каждого ядра есть собственный кэш, и дальше эти кэши связываются вот в одну систему
вместе с памятью. Почему это добавляет нам сильно сложности? Ну, посмотрите, в чем дело. Вот раньше
вы жили в мире, где у вас была память, планка памяти, там были ячейки, которые вы читали и
писали. А теперь каждая ячейка может быть в памяти, а еще может быть в кэше. Ну, то есть,
у вас теперь логическая ячейка, одна единственная, имеет потенциально несколько физических
воплощений. Одно непосредственно в памяти, а другие в кэшах. То есть, у вас появляются копии ячейки,
у вас теперь нет одной ячейки, у вас теперь много копий одного и того же. И эти копии, хуже того,
могут расходиться друг с другом, они могут показывать разные значения. Ну, смотрите,
как это выглядит. Вот пример ячейки памяти. У нас есть большой процессор, там 16 ядер. И мы
делаем следующее. Мы берем ячейку памяти и на каждом ядре пишем туда свой номер. Вот третье
ядро пишет в ячейку значение 3, а потом читает то, что написано в ячейке. И вот, что читают разные
ядра. Ячейка одна. Ну, то есть, ячейка одна, а значение у нее много одновременно, у нее много
значений при этом. Выглядит довольно странно, да? Вот, довольно сложно жить в таком мире,
где у ячейки много одновременно значений. На самом деле, эта картинка иллюстрирует чуть больше,
чем я говорю, потому что здесь есть некоторая структура заметная. Тут какие-то пары соседних
ядер похожи почему-то. У этих всех историй наблюдения за ячейкой есть какая-то общая
структура. Ну, про это мы поговорим через неделю. Это уже история больше, наверное, про модели
памяти. Но, тем не менее, о чем я говорю? О том, что кэши в процессоре и память образуют... Ну,
то есть, о чем я говорю? Что разделяемая память, которую мы принимаем за данность, это на самом
деле абстракция. В компьютере никакой разделяемой памяти одной и вот единственной экономической нет.
Вместо нее есть распределенная система, где есть ядра с отдельными кэшами и общая память.
И, ну, видимо, содержимое этих кэшей и памяти должно как-то синхронизироваться. И вот это история
про распределенные системы. А распределенные системы это очень сложно, оказывается. Ну,
я про это читаю в отдельных спецкурсах, я там рассказываю очень долго про то, что очень сложно
научить вот три компьютера в сети, которые хранят одни и те же данные, вести себя вот как
единое целое. Вот вести себя так, чтобы пользователь снаружи не понял, что это три копии одного и того
же, потому что копии расходятся, машины перезагружаются или отказывают. Это сложная история, но, по счастью,
в процессоре решение все же становится проще, потому что, в отличие от распределенных систем,
где машины могут умирать незаметно для других, все-таки в процессоре отдельные ядра сами по
себе не отказывают. Это немного упрощает дело, поэтому все же процессору удается поддерживать
некоторые простые варианты относительно копий одних и тех же данных в разных кэшах. Итак,
проблема понятна, про которую я рассказываю, что у нас есть несколько копий и ядра с ними
работают вроде бы независимо, но их нужно как-то синхронизировать между собой. Ну вот,
эта задача называется протоколка гириантности. Как синхронизировать содержимое одних и тех же
линий в разных ядрах с памятью. И этот протокол к гириантности поддерживает для вас очень
простые варианты. Они выглядят так. Смотрите, если у вас какая-то линия памяти находится
сразу в нескольких кэшах, то протокол к гириантности гарантирует вам, что все эти копии этой линии в
разных кэшах одинаковы и совпадают с памятью. Гарантия понятна? Если линия есть в двух кэшах,
как минимум, то гарантируется, что значение ее актуальной совпадает с памятью. Ну и все эти
копии одинаковы. Вот мы в этом случае будем говорить, что кэш-линия в этих кэшах находится
в состоянии Sherrod. Вот у каждой линии в каждом кэше будет отдельное состояние. Состояния будет
четыре. Вот поэтому протокол к гириантности называется Мессия. Вот мы сейчас говорим про
состояние S-Sherrod. Первая гарантия. Вторая гарантия. Следующая. Если кэш-линия была изменена текущим
ядром, но изменения еще не попали в память, а разумеется, когда процессор, я не сказал об этом,
пишет в кэш, вообще пишет в память что-то, то он пишет сначала в кэш. Он не пишет сразу в память.
Он пишет в кэш, а потом эвенчивали данные из кэша, сбрасываются в память. Так вот, если какой-то
процессор, какое-то ядро записало свежие данные в какую-то кэш-линию, но содержимое этой кэш-линии
еще не уехало в память, то гарантируется, что в других кэшах копии этой линии нет. То есть эта
линия, если у нее состояние грязное, как говорят, то она находится только в одном кэше. То есть ядро
владеет этой линией эксклюзивно. Вот если эксклюзивное владение обозначается буквой M,
состояние кэш-линии, когда она отсутствует в кэше, обозначается буквой I, но invalid. То есть отсутствует
или нет одно и то же для кэша. Вообще, кстати, не поговорил, извините, я вернусь в прошлое. Секунду.
Потерялась картинка. Что такое вообще кэш? Я как-то упустил момент. Это на самом деле железная
хэштаблица. Она состоит из какого-то фиксированного числа бакетов с фиксированной емкостью. То есть это
аппаратная хэштаблица, которая реализована прямо в процессоре. Вот так же, как, не знаю,
page table – это аппаратное префиксное дерево, которое тоже реализовано прямо на схеме процесса.
Вот. Так что кэш-линия, она вот... кэш-линия может находиться, вообще говоря, в разных... линия
памяти может находиться в разных строчках кэша, но гарантируется, что если она грязная, то она
находится только... только в одном кэше. Ну и есть некоторое специальное состояние,
которое называется exclusive, которое вроде бы про то, что кэш-линия совпадает с памятью,
как и в случае shared. Но при этом состояние exclusive означает, что кэш-линия находится
гарантированно только в одном кэше. В других кэшах этой линии нет. Вообще говоря,
и shared не требует, чтобы кэш-линия была одновременно в двух кэшах. Может быть,
в одном кэше... ну, две кэш-линии были в состоянии shared в двух кэшах, а потом одну кэш-линию вытеснили,
потому что, ну, какая-то запись... какой-то записи потребовалось места, и кэш-линия выпала из кэша.
Но вот состояние shared не понимает, что линия находится только в одном кэше, а exclusive
понимает. Картинка неправильности должна быть invalid, конечно. Простите, небольшой баг. То есть,
если состояние exclusive, то оно эксклюзивное, взаимно исключающее. Modify – это то же самое,
взаимно исключающее. В других кэшах значений быть не может. Идея понятна? Тогда я хочу вам показать,
как это будет выглядеть. Ну вот, такой симулятор протокола когериантности.
У нас есть память из четырех ячеек, у нас есть четыре... три, почему-то. У нас и раньше было
три, это странно. Три ядра. У каждого из них есть кэш из двух кэш-линий. Ассоциативность кэша – один.
То есть, ячейки 0 и 2 попадают в первую линию, 1 и 3 попадают во вторую линию, ну, в нулевую,
в первую, соответственно. Ну, давайте что-нибудь прочитаю. Вот я читаю данные,
мне отвечает память, и вот у меня данные в кэше в состоянии exclusive. Вот я, ядро, знаю, что других
ядер этого состояния нет. Ну, а дальше вопрос. А как вообще, собственно, эти состояния поддерживаются,
вот эти гарантии, про которые я говорю вот здесь? Как гарантируются вот эти варианты в кэше?
Они гарантируются с помощью коммуникации. Можно, прежде чем я к ней перейду, я скажу,
что вот состояния shared и exclusive, они же на самом деле очень, очень естественные, потому что, ну,
если вы что-то знаете про Mutex, то, возможно, вы знаете про штуку, которая называется «где же она»,
shared Mutex. Слышали про него? Какие у него любопытные. Удивительно. Shared Mutex это, ну, или RV Mutex,
или Reader-Writer Mutex. Он про то, чтобы немного обобщить интерфейс Mutex и к методам log и
unlock добавить методы log shared и unlock shared. Ну, идея такая. У вас есть некоторое состояние и,
может быть, его хотят менять. Для этого у вас есть log-unlock для критических секций. Вот какой-то
поток хочет получить эксклюзивный доступ к данным. А может быть, у вас сразу несколько потоков
хотят читать данные. Вот они могли бы делать это параллельно. Вот читать и одновременно писать
нельзя. Это data race, это undefined behavior стандарта C++. А вот параллельно читать можно. Поэтому
мы к log и unlock добавляем log shared и unlock shared. И гарантия следующая, что между log и unlock может
находиться только один поток. А между log shared и unlock shared могут находиться несколько поток
разом. Но секции log-unlock и lock shared-unlock shared не пересекаются. Ну, то есть, потоки, которые
пишут данные, берут эксклюзивную блокировку. Потоки, которые читают данные, берут разделяемую
блокировку. И, по сути, состояние протоколок агерентности, вот эти вот M и S, это вот не что иное,
как, ну, и весь этот протокол к агерентности, это не что иное, как такой вот shared butex,
который реализован вот аппаратно прямо в процессоре. Нет, не так, разумеется. Иначе бы,
ну, все-таки, когда мы читаем в память, читаем из памяти и пишем в память, не то, что мы берем
блокировку, нет. Происходит не так. Мы поддерживаем, скорее про состояние. Что значит состояние? Либо у тебя
много читателей, либо один писатель. Вот о чем я говорю. Не о том, что Mutex берется, разумеется.
Там Mutex ни какого нет. Повтори тогда его.
Да. Ну, и в обратную сторону тоже. И тут возникает вопрос, у кого приоритет, но это out of scope
сегодня. Так вот, как же поддерживаются вот эти варианты протоколок агерентности про S и
M? Они поддерживаются с помощью коммуникации. Вот скажем, у нас ячейка памяти в состоянии E,
и ее хочет прочесть другое ядро. В каком состоянии ячейки должны оказаться, линии в кэшах должны
оказаться после чтения? Ну, видимо, у нас будут две копии одной и той же линии. Значит, они должны
быть в состоянии shared. Ну, а значит, когда мы читаем, нужно поговорить с другими кэшами,
и вот мы отправили сообщение на шину, и это сообщение пришло этому кэшу, и он поменялся
в состоянии с E на S. Раньше он знал, что только он владеет копией, а теперь он знает, что,
возможно, есть другие. Впрочем, смотрите, что могло произойти. Я прочел ячейку A2,
но они отобразились в одну и ту же кэшлинию, но просто в силу ассоциативности кэша, и вот эта
копия ячейки A0 вытеснилась из кэша, но здесь S так и осталось. То есть S не означает, что кэшлини
находятся, по крайней мере, в двух кэшах, она означает, что кэш не уверен, что кэшлини находятся
только у него, но он уверен, что у него актуальное состояние. Так вот, если мы будем читать дальше
ячейку памяти A0, то мы это будем делать без коммуникации, потому что мы знаем, что состояние
shared означает, что ячейка памяти имеет актуальное состояние, что она совпадает с памятью. Вот,
а теперь предположим, что мы хотим записать что-то в ячейку A0, но мы должны перейти после этого в
состояние modified. У нас будет грязная копия, у других не должно быть копий. Опять, это коммуникация.
Мы должны отправить другим кэшам сообщение, чтобы они инвалидировали копию у себя. Тут, правда,
не modified получилось, тут сразу эксклюзив, потому что мы еще в память сбросили. Но,
может быть, если перезаписать, ну вот да, перезаписали, получили modified.
Так, чтение и завершает состоянием shared. Да, вот, смотри, теперь состояние modified,
а что будет, если другое ядро прочтет ячейку 0? Вот, мы должны сказать вот этому кэшу, чтобы он
сбросил свое значение в память, потому что нам нужно два shared, а shared требует, чтобы память и
кэш совпадали по значению. Так что мы отправляем на месте вот этого читателя сообщение про то,
что вот по флаж, пожалуйста, свой кэш сбрось данные в память, перейди в состояние S, а мы
получим копию. Что за вопрос у тебя был, я, наверное, не... Еще раз, любое чтение завершается,
по крайней мере, shared. Но если у других кэшах, когда мы читаем, мы общаемся с другими кэшами,
потому что они должны инверидировать у себя грязные копии или сбросить exclusive. Так что любое
чтение завершается, по крайней мере, в состоянии shared для локального кэша. Но если окажется при
чтении, при коммуникации с другими кэшами, с другими процессорами, что копии ни у кого не было,
то мы попадем в состояние exclusive. Вот я закрываю пальцем у себя этот modified, здесь тоже invalid
написано. Ну, то есть, выгодно оставаться в exclusive, потому что если мы находимся в состоянии
exclusive, ну, давайте мы прочтем какую-нибудь А1, получим exclusive, то чем он ценен? Тем, что в
него можно писать без коммуникации, потому что мы знаем, что копии ни у кого нет. В этом его смысл.
Ну что, мне кажется, я и писал примерно все сценарии, если они понятны, то я бы пошел дальше. Что скажете?
А где ткнуть? Ну, а зачем? Как бы, чего ты ожидаешь от процессора? У него грязная копия, он знает,
поэтому, что он только у него. Но она еще не сбросилась в память, с другой стороны, пока она
никому не нужна, поэтому он может писать в нее дальше. Но если кто-то захочет прочесть, то придется
сначала сбросить ее в память и потом получить два шерода. Как это все примерно реализовано? Ну,
это реализовано с помощью примитива, который называется atomic broadcast, который упорядочивает
все события во всех кышах, для всех кишей, для всех процессоров. Вот сложность в распределенных
системах состоит в том, что такой atomic broadcast сложно сделать. В процессоре гораздо легче,
потому что отдельно ядра не ломаются. Ну и, да, еще одно замечание, что, конечно же, вот протокол
Месси, это не то, чтобы вот прям так процессор работает. Это некоторая модель, которой можно
пользоваться, когда мы рассуждаем о том, как наши ядра, наши потоки на разных ядрах работают с
памятью. В процессоре может быть сколь угодно сложнее. Но нам не нужно понимать точно, как работает.
Нам нужна хорошая модель, в которой можно оптимизировать код. То есть модель будет хороша,
если мы, используя ее, оптимизируем свой код в этой абстрактной машине, а дальше мы его
компилируем, и он эффективно исполняется уже на настоящем процессоре. Вот мы сейчас собираемся
знанием про эти киши воспользоваться для оптимизации каких-то примитивов, которые мы
видели, либо еще увидим. Но смотрите, вот прямо сейчас уже очень важную мораль можно сформулировать
для всего нашего курса. Что вот протокол когерентности и вот эта коммуникация между кышами, которую он
требует в отдельных сценариях, это и есть для вас, как для разработчика, стоимость разделяемой
памяти. Вот это и есть для вас главная статья расходов при синхронизации. Вот это коммуникация.
Чем меньше коммуникации между кышами будет в исполнении вашего кода, тем эффективнее синхронизацию
вы построили. Но есть еще отдельная история с моделем памяти, с упорядочиванием памяти. Это вот
история на следующую неделю. А пока это вот половина правды, половина перформанса тратится
здесь на коммуникацию между ядрами. Да, собственно, и модель памяти тоже самая. Короче, вот эта
коммуникация, вот это движение данных по линии, по интерконекту, по шине между разными кышами и
памятью – это вот главная статья расхода. И вы должны ее экономить. Ну а теперь давайте посмотрим,
как, собственно, можно ее экономить. Мне кажется, что сейчас это может быть забавно. Итак, пример первый.
Пусть вы пишете какую-то большую, сложную, не знаю, распределенную систему, базу данных,
и для такой системы вам полезно за ней уметь наблюдать. Есть такая тема observability. Она включает
себя логирование, распределенный трейсинг, а еще сбор метрик. Ну грубо говоря, вы хотите знать,
сколько запросов пришло на вашу машину в единицу времени? Сколько задач цветпулевы запустили за
последние там 10 секунд? Короче говоря, вы хотите что-то считать. Разумеется, вы хотите считать из
разных потоков. Вам нужен атомарный счетчик. Ну и казалось бы, в чем вопрос вообще? У вас есть
атомик, вот у него есть операция фичет, она атомарная, ну вот атомарный счетчик готов.
Ну это работает. Я начну с положительного. Это работает, ну и вот какой-то тест, где мы запускаем
10 потоков, и дальше они делают по миллиону инкрементов. И мы, видимо, ожидаем после этого
получить значение миллион, если все пойдет хорошо. Тут тест запускается в цикле, ну вот. Вот
пример, насколько стоит миллион инкрементов из 10 потоков. Мы сейчас хотим этот код пооптимизировать,
но пооптимизировать не в том смысле, что мы хотим ускорить инкремент. Нет, не хотим. Мы хотим
ускорить все инкременты. В смысле, мы хотим повысить пропускную способность. Количество
инкрементов – единица времени. Ну и давайте подумаем, что нам мешает прямо сейчас вот с такой
реализацией счетчика. Видимо, счетчик непараллелен. Ну разумно же, да? Каждое ядро для того,
чтобы записать что-то, сделать фичет, это запись с точки зрения кэшей, нужно получить кэшлинию
в монопольное владение, ну, перед записатью. То есть фактически взять на нем юдекс. То есть все
эти инкременты на уровне процесса происходят последовательно. Так можно я немного остановлюсь
и замечание сделаю важное. Операция ComperExchange. Вот с одной стороны, это чтение плюс запись.
Причем чтение и, возможно, запись. Так вот, с точки зрения процессора, это всегда запись.
То есть прежде чем исполнять ComperExchange, процессор должен получить кэшлинию в монопольное
владение. Вот не пытайтесь это оптимизировать. Не думайте, что если у вас ComperFalse, то значит
чтение в записи не произошло. Модель памяти здесь ни при чем абсолютно. Вообще ни при чем.
Руководство Intel является хорошим референсом. Там это написано. Что написано на эти референсы?
Ну, там написано, что у Uniclock нужно делать параметр шаблона при объявлении. И вы все это делаете.
А зачем? Не нужно давно уже. Ладно, хватит нытья. Как нам ускорить atomic counter? Как нам его распараллелить?
Ну вот, thread local сложно обходить. Это хорошая очень идея, но я реализую ее несколько по-тупому.
Я скажу, что я хочу шардировать счетчик. Вот раньше у меня был один счетчик, а теперь у меня массив
счетчиков. Вот этот отдельный счетчик я назову шардом. Почему приходить осенью? Каждый шард это
отдельный счетчик, просто отдельный atomic. У него такие же операции Fichet, Lot и теперь что я делаю,
когда я говорю «инкремент»? Я для потока выбираю некоторые шарды. Каким образом? Я беру
идентификатор потока, хэширую его, хотя это скорее всего и так число, и беру по модулю шардов. Ну,
то есть у меня есть несколько ячеек памяти для счетчиков, и каждый поток попадает в некоторую
ячейку. Но может быть какие-то, конечно, какие-то потоки склеиваются по ячейкам, но вот какие-то
потоки попадают в разные, но, казалось бы, разные ячейки – это повод для параллельности. Ну,
и вот такой вот код. Когда я хочу прочесть значение счетчика, я просто пробегаюсь по
количеству шардов и вот складываю значение. Конструкция понятна? Ну, на самом деле,
конечно, нет, потому что непонятно, почему это будет эквивалентно. Вот. Почему это счетчика
тамарный, будет ли он тамарным – это вообще-то вопрос нетривиальный, гораздо нетривиальный,
чем мы сегодня обсуждаем с вами. Но я предлагаю пока про это не думать, тем более, скажем,
если мы читаем запросы, то нам может быть прямо в точности правильное поведение не нужно. Ну,
это тонкий очень вопрос, но я вас не обманываю. Мне кажется, что это счетчика тамарен, нужно
доказывать строго. Но можно взять этот счетчик, по крайней мере, и вот воспользоваться им. У нас
здесь сколько? Восемь шардов. Ну, давайте запустим. Как-то не стало быстрее, медленнее стало,
интересно. Ну, это несколько неожиданно, конечно. Хотя, наверное, можно понять, стало больше работы,
а параллельность – это больше не стало. Вот мы ожидали здесь, что будет параллельность,
да, наверное. Но мы такого ожидали, если мы просто невнимательно слушали лекцию. Потому что, ну,
на уровне этого кода, разумеется, параллельность есть. Ну, в смысле, вот логическая параллельность
есть у вас в голове. У вас разные потоки работают с разными ячейками памяти в этом массиве. И могли
бы работать параллельно. Но для чего я вам картинки-то показываю? Для того, чтобы мы знали,
что процессор, когда он общается с памятью, для него единица доступа к памяти – это не ячей
к памяти, это целая линия. И что происходит в этом примере? В этом примере происходит явление,
которое называется false sharing. Вот у вас есть две ячейки памяти. Вот есть одно ядро, которое пишет
в X, а другое пишет в Y. Это разные ячейки. Записи в них могли бы идти параллельно. Но не могут,
потому что они находятся в одной кыш-линии. А для того, чтобы сделать запись в кыш-линию,
нужно инвалидировать копию этой линии у другого ядра. В итоге логическая параллельность здесь
есть, а на аппаратном уровне вы берете эксклюзивную блокировку, и все записи происходят
последовательно. Довольно тупая ситуация, правда? Ну, то есть, мы ничего не ускорили,
кажется, стало хуже. Ну, live примеры – это, конечно, сложно. Ну, давайте попробуем ускорить.
Неплохо, да? Волшебство. Нет, пожалуйста, не пишите так о домашних работах. Я честно,
по-человечески прошу, не надо, потому что это вот прям для очень специальных случаев. Вот есть у
нас mutex, и взаимное исключение – нигде такое не нужно. Но вот здесь мы на халяву буквально
ускорили кодки четыре раза. Я сейчас про это расскажу, да. Так вот, это просто для хохмы. На самом
деле, конечно, в C++ нужно делать… Во-первых, что мы сделали? Мы просто взяли и каждую ячейку
поместили в свою отдельную кашлинию. И теперь с ними можно работать параллельно. Ну вот,
в C++ для этого есть более человеческие инструменты, а именно есть Align S. То есть,
вы выравниваете вот это поле по 64 байта. Но это не совсем фундаментальные константы для всего
мира, но это, вероятно, сейчас так. Ну вот, быстро работает. Да, убираем, и становится медленно. Сейчас,
давайте… Компьютер начинает греться, это не к добру. Ну вот, воспроизводится, да. Вот он нагрелся
и стало медленнее. Итак, есть Align S, который задает выравнивание для ваших полей. И более того,
в C++, начиная с 17, есть вот некоторая магическая константа, которая говорит, ну вот, насколько
нужно разнести два объекта в память, чтобы они не попадали в одну кашлиню, чтобы не было
full sharing. Ну то есть, с одной стороны, вы живете счастливы и не знаете про каши ничего, и думаете,
что просто под вами большая память. На самом деле, про каши полезно знать, и вот эта абстракция,
она хоть в определенной степени… То есть, каши прозрачные, о них можно не думать, но если вы о
них думаете, то можно излечь из этого пользу. Давайте поговорим про, может быть, менее тривиальный
пример сейчас. Где еще можно этим воспользоваться, этим full sharing? Ну, значит, с этим мы
разобрались. Посмотрим на такую штуку, как циклический буфер. У нас есть два потока и
циклический буфер. Один поток пишет в него, добавляет элементы в цикле. Другой читает.
Продюсер-консюмер. Методы тут без блокировок. То есть, мы пробуем, если нет, то немного ждем.
Наша задача такой буфер написать. Ну, что мы пишем? Мы пишем массив из значений, и у нас есть две
переменные head и tail. И как мы с ними работаем? Ну, вот мы в продюсе читаем head и tail и смотрим,
если буфер не полон, head указывает на первую занятую ячейку в циклическом буфере, tail – на
первую свободную, с учетом того, что он зацикливается. Вопрос на понимание, как отличить полный буфер от
пустого, если он циклический. Это же и вопрос. Наоборот. Ну, когда буфер пуст, то у нас head
равен tail. Когда буфер полон, когда head почти догнал tail. Что? Тут просто вектор два индекса по
нему бегают. Я не понимаю, слот – это вот один элемент. Мы пишем шаблон. Так вот же.
Мы различаем эти два случая. Так, конечно, не пишите циклический буфер никогда,
потому что тут нужно брать степень двойки и понятно. Ну и вот такой код. Мы читаем tail,
читаем head. Если tail почти догнал head, то мы говорим false, иначе мы в tail записываем
новый элемент и двигаем tail вперед. Для консюмера в методе tryConsume мы читаем head и tail,
если они совпадают, буфер пустой. Иначе мы извлекаем элементы с головы и двигаем голову
вперед. Вот симметрично все. Видите ли вы здесь false sharing? Название false sharing, потому что,
еще раз, у нас логически разделяемых данных нет, а на самом деле фактически на уровне процессора
разделяемые данные есть, это кэшлиния. Вот есть ли здесь что-то, что можно… есть ли здесь лишняя
синхронизация? Ну вот смотрите, у нас есть продюсер и консюмер, и пусть… ну буфер, конечно,
может быть полон, может быть пуст, может быть почти полон, но если вдруг он так вот где-то
посерединке, то вроде бы с head и tail, то есть с хвостом этого буфера и с головой можно было
бы работать параллельно. Но скажем, что здесь мешает? Что мешает работать параллельно? С точки
зрения памяти параллельно, разумеется. Ну не только они рядом лежат. Довольно медленно,
работает вот этот вот с 10 миллионами продюсеров консюма. Ну сейчас давай наивно. Ты говоришь,
что вот для того, чтобы работать с головой очередью, нужно работать с… что предлагается,
разнести вот эти поля по разным кашлениям, да? Ну это же бесполезно. Разве нет? Ну у нас же и
консюм и продюс обращаются к обеим переменам. Ну какая разница? Постоянно кто-то пишет. Один
пишет, другой читает. Все, это уже коммуникация. Так что плохо. Ну по крайней мере, одно место
можно улучшить. Возможно. Хотя не факт. Ну вот если мы работаем с хвостом, если буфер небольшого
размера, там скажем, 8, то продюсер и консюмер пересекаются, по крайней мере, по одной кашлении
в самом буфере. Можно разнести их. Ну вот тут очень тонкий пример, может получиться, а может
не получиться. Давайте посмотрим. Тут у меня еще и компьютер снова. Ну стало быстрее, да? То есть
все-таки это работает. А теперь, смотрите, довольно ловкий трюк. Вот вроде бы продюсер работает с
тейлом очереди, а консюмер работает с хедом очереди. Но при этом и тому, и другому обе ящики нужны.
Смотрите, что я сделаю. Я не буду в продюсере перечитывать чужой хед, который пишет консюмер
каждый раз. Я вместо этого буду хранить последний хед, который я прочитал.
А для консюмера я буду помнить последний tail, который я прочитал. Ну кэша тут смысле не в том
смысле, что он в аппаратном кэше, а вот в моей логике. И что я сделаю? Смотрите, я вот эту точку
закомментирую и раскомментирую эти строчки. Вот если у меня мой прочитанный tail до сих пор
не догнал старый прочитанный хед, то, видимо, я не должен спойлерить модели памяти. Пока. То, видимо,
мне и не нужно читать, точнее, если он догнал, то мне нужно перечесть хед. А если у меня он еще не
догнал, то я могу воспользоваться просто старым хедом и чужую ящику памяти не читать. Понятная
идея. Довольно ловкая оптимизация. Здесь можно сделать точно так же. Я читаю хед, и если он догнал
мой закашированный tail, про который я последний раз помнил, то придется перечесть. Возможно,
tail сдвинулся вперед. А если мы его еще не догнали, то, видимо, не страшно, можно извлекать. И вот теперь,
смотрите, на быстром пути, когда у нас закашированных переменных хватает, продюсер работает только с
одной разделенной ящикой памяти, а консюмер работает только с одной разделенной ящикой памяти. Вот они
уже друг с другом не общаются. И теперь, возможно, если нам повезет, мы посмотрим, что стало. О, стало
еще раз в два раза быстрее. Нет, не стало. В два раза быстрее. Вот такую ивристику используют,
кажется, самая быстрая разумная реализация вот такого буфера. Можете на гитхабе найти. Ну,
про нее автор написал небольшой блокпост, но тут, в общем-то, идею все объяснил, никаких подробностей
новых не нужно. Мы кэшируем другую ящику памяти своего соседа, конкурента, и не пересчитываем ее каждый
раз. Поэтому мы снижаем коммуникацию между кэшами. Вот с точки зрения корректности, с точки зрения
синхронизации, то, что мы делаем, здесь совершенно бесполезно. То есть мы ничего, никакой новой логики
не вносим в синхронизацию. Мы всего лишь заботимся о том, чтобы между кэшлиниями не было фоллс-шеринга.
Вот, кстати, можно его спрятать обратно и посмотреть, замедлится ли программа. Ну вот, наша оптимизация
кату под хвост. Вот, а если мы разделили на две, на два класса ячеек и сделали паттинг между ними, то
работает быстрее. Это еще не все. Можно, ну не знаю, если вы не хотите спойлеров, то закройте глаза сейчас,
а я расставлю memory-order здесь и покажу, что можно еще ускорить. Ну не смотрите, пожалуйста.
Нет, нет, я уверяю тебя, нет. Нет, я могу поспорить ответ. Расставить memory-order правильно
здесь без глубокого понимания невозможно, а у нас его точно нет сейчас. Как-то интуитивно,
по каким-то шаблонам можно, но тут дело не в шаблонах, тут дело в понимании. По шаблонам
memory-order использовать невозможно. Пожалуйста, не думайте, что читаем мы SPP Reference, вы что-то
только плохой может быть. Ну что? Вот видите, вот буквально из ничего, в смысле я не меняю алгоритмы,
никаких вот волшебных там оптимизации, не оптимизирую алгоритмы всякие, я ускорил программу в 6 раз.
Просто оптимизирую синхронизацию. По-моему довольно ловко. Давай проверим.
Как это делается? Вот так, да? Смотрите-ка, я написал датарейс. Это сложно, в отдали памяти
это сложно, и как это все, короче, на семинаре проверим, насколько вы это поймете. Но пока я
просто вам показываю это как некоторое волшебство, как можно, зная про когнитность кэшей, про модели
памяти, ускорить программу синхронизацию в 6 раз. Ну что ж, с этим примером мы закончили,
и давайте посмотрим на еще один пример на, собственно, на синхронизацию, на спинлог. Вот этот
спинлог совершенно наивен, и вот такой код мы написали на первой лекции с вами. Вот его тоже
нужно улучшать, зная про протокол когнитности, правда, немного другими механизмами. Тут дело уже
не фолдшеринги, фолдшеринга тут нет. Тут есть другое явление, которое называется... Нет,
во-первых, давайте запустим этот код. Он снова в десяти потоках делает 100-500 раз lock-on-lock,
и ну вот как-то работает. Первое, что с ним следует сделать, конечно же, написать что-то вот в этот
цикл. Написали, запустили. Стало совокупно быстрее, способность повысилась. Хорошо,
на самом деле, отдельные потоки могут замедлиться, но в единицу времени секция выполняется больше.
А что тут еще есть плохого с точки зрения протоколок когнитности кэшей? Очень неэффективного.
Тут есть явление, которое называется cache-пин-понг. Давайте ему... Нет, здесь есть. Вот здесь
происходит cache-пин-понг. Почему? Но представьте, что у вас было три ядра. Странно. Четыре ядра,
три потока. Вот на первом ядре поток захватил spin-lock. В каком состоянии сейчас кэшлиния с ячейкой
locked у него? У него modified. У других ничего нет. У других invalid. Ну вот давайте... У других invalid.
Потом пришел второй поток, он сделал exchange. Довольно бесполезный, потому что он прочел один,
записал один. Но, тем не менее, он записал. Кэшлиния у него в состоянии modified. Она переехала сюда.
Он пообщался с этим кэшом, инвалидировал у него копию, получил ее себе. Потом пришел третий поток.
Он тоже сделал exchange. Снова прочел единицу, записал единицу. Таким образом, инвалидировал
копию здесь, пообщался с другим кэшом, подождал, пока ему ответят на эту инвалидацию. Получил себе
значение. Ну и опять бесполезно для себя. А что происходит дальше? Ну дальше это ядро, оно же по-прежнему
ждет спинлока. Оно снова делает exchange и получает копию себе. И вот два этих ядра, которые ждут,
пока спинлок освободится, просто двигают между собой эту одну кэшлиню несчастную, без всякой
пользы. Они нагружают общую шину данных, по которой общаются все ядра процессора. Они отвлекают
компьютер от полезной работы, просто передвигая строчку с единицей туда-сюда и без всякой пользы.
Ну просто потому, что состояние modified, запись в ячейку памяти, требует инвалидации кэшлини
и эксклюзивного монопольного владения. Как это оптимизировать? Я ж про это говорил.
Compare exchange это запись всегда, независимо от исхода. Процессор, он не угоден… короче,
запись. Как это ускорить? Это написано, как ускорить. Не нужно постоянно… ну, то есть,
если вы сделали exchange, сделали его неуспешным, то зачем дальше мучить шину памяти? Зачем дальше
мучить протокол гигрианта стекошей? Просто ждите, пока в ячейке не станет ноль. Вот что будет после
такой неудачной попытки на этом ядре? Ну, точнее, не так. Вот у нас три потока сделали exchange,
этот был успешен, этот неуспешен, неуспешен, а потом второй поток на втором ядре после exchange
делает load уже. Ну да, он снова общается с другими кэшами, он получает себе копию,
эта копия сбрасывается в память, и теперь у нас два шерда. А чем хороши шерда? Тем, что из них
можно читать без коммуникации. И дальше два ядра просто дальше крутятся в этих лодах, читают кэшлинию
без коммуникации с другими кэшами, не отвлекая процессор. Ну, то есть, что я хочу сделать? Я хочу
написать здесь следующее. Я забыл, сколько было. Больше двухсот, но сейчас стало в два раза быстрее.
Ну, вот опять, некоторое волшебство, но просто зная про то, за что мы платим при синхронизации,
что синхронизация это на самом деле стоит нам коммуникации между кэшами, мы получаем такую
штуку. А еще, если вы опять закроете глаза, я расставлю здесь memory-ордеры. Код выглядит,
как хороший спинлок, но только такой нужно писать. Вот любые другие спинлоки писать не стоит,
нужно ударять сразу. Синхронизация это не обычный код, это особенный код. Ну что, надеюсь, я был
убедительным. Вот такой вот магии. Вот этим циклом мы уже ускоряем что-то. Тут как бы два
ускорения комбинируются, то есть два типа знаний про память. С одной стороны про memory-ordеры,
там барьеры и модели памяти, и с другой про когенетность кэшей. Но вот, используя ОВА,
используя правильно, аккуратно, можно сильно выиграть. Правда, тут еще есть одна проблема,
она называется Thundering Heart. Идея такая, вот был у вас спинлок, были у вас на него претенденты,
они крутились на своем состоянии S, а потом поток, который спинлоком владел, его отпустил. Что
произошло? Он сделал запись памяти, вот здесь вот. Он у других инвалидировал кэшлинию. Другие
потоки перечитали ее, увидели там ноль, и потом снова каждый из них делает эксченж. Это так
называется, потому что вот напоминает стады бизонов, которые прочли ноль и снова ломанули
сделать эксченжи, и снова нагружают компьютер. Вот у вас сегодня будет маленькая задачка, маленькая,
но странная. Не знаю, компенсирует ли одно другое. Давайте я покажу. Про очень затейливый спинлок,
который используется в ядре линукса, и который как раз оптимизирует и пинг-понг, и вот этот
который оптимизирует промахи по кэшу с помощью очень ловкой интрузивной очереди, что бы это ни значило,
но интрузивность у нас по многим причинам полезна, поэтому вот этот спинлок тоже можно
написать, а потом можно под файберами переписать красиво. Ну короче, попробуйте, разберетесь еще
лучше в том, где вы платите за протокол к гирятости. В этом коде, в этой задачке вас потребует
написать, от вас потребуется написать больше сложного кода, спинлок станет сложнее, там будет
больше действий, больше объектов, односвязанные списки, но при этом он будет лучше, что довольно
неожиданно. Ну это все была в ствол на самом деле, потому что, смотрите, сейчас я скажу, наверное,
самое важное, что можно сказать на этой лекции и вообще может быть в курсе. Ну да, мы можем
оптимизировать спинлок, мы можем в нем оптимизировать синхронизацию, там какие-то разделяемые ячейки
памяти, но если вы используете мьютексы, просто вот вы используете мьютексы в программе, то вам уже
плохо, потому что что такое мьютексы, ну что такое использование мьютекса, у вас есть разные ядра,
на них есть потоки и они берут лог и работают с какими-то данными. Ну да, вы пооптимизировали
что-то в спинлоке, но в конце концов зачем вы брали блокировку, чтобы работать с данными,
и если у вас сначала критическая секция была на одном ядре, а потом она запускается на другом
ядре, то что происходит, вы запустились на одном ядре, записали данные, теперь они у вас в вашем
кэше, вот на этом ядре в состоянии modified, а потом вы взяли блокировку на другом ядре, и вот в этом
кэше сейчас данных нет, просто по инварианту протоколок гириантности, по состоянию modified,
и вам нужно пойти в другой кэш и эти данные к себе привести, и вот если вы используете мьютекс
обычным способом, запуская потоки на разных ядрах, то вы, как бы вы не оптимизировали
саму блокировку, двигаете данные туда-сюда, понимаете проблему, вот мьютекс вас заставляет работать
с процессором так, как он работать не хочет, как ему неэффективно работать, просто мьютекс by
design навязывает процессору очень неэффективный сценарий работы, это очень глубокая мораль на самом
деле, потому что, ну я не знаю, знаете ли вы что-нибудь про продюс? Ну да, знаю, это больше чем ничего,
безусловно, медленно загрузится, ну давайте я пока нагрузится или загрузилась расскажу коротко,
вот у вас есть очень много машин, тысячи, сотни тысяч, может быть даже, десятки, сотни тысяч,
очень много данных на этих машинах, и вы хотите их параллельно обрабатывать, и при этом вы не хотите
думать о том, где данные лежат, что делать с упавшими машинами, как перезапускать фрагменты
вычислений, которые разломались, для этого Google в 2000 каком-то древнем году, ст. 4 года,
придумал такой фреймворк, ну такой вообще парадигму описания вычислений, мы продюс,
вы описываете свое вычисление над большими данными в виде композиции вот таких вот операторов,
а дальше сам фреймворк эффективно исполняет ваши задачи на большом кластере машин
скрывая от вас отказы, так вот в чем идея там, ну представьте у вас есть много данных,
они как-то разложены по машинам, на каких-то они есть, на каких-то нет, и у вас есть вычисления,
где разумно запускать вычисления, ну вот разумно не запускать вычисления где-то, а потом вести
к этому вычислению данные, разумно отправить вычисления вашему программу «Бинарник» туда,
где просто данные лежат. К чему я это говорю? К тому, что распределенная система большая и ваш
процессор мало чем отличаются. У нас есть потоки, которые работают с разрядивыми данными, и у нас
есть эти данные. И вот вместо того, чтобы двигать данные между ядрами, между кышами, гораздо разумнее,
критические секции, которые обрабатывают эти данные, привести на одно ядро и выполнить их там
подряд просто. Тогда не будет промахов по кышу. Это основная статья расхода для вас. Не будет
навалидаций, данные будут горячими локально, и просто между критическими секциями будет
меньше синхронизации. Она не нужна будет. Вот мы через субботу, через выходные поговорим про
фьючи, и там будут экзекьюторы, и там будет асинхронный мьютекс. Так вот, асинхронный мьютекс
звучит довольно странно, но в общем, можно написать так, чтобы он работал эффективнее, чем обычный
синхронный мьютекс. Потому что асинхронному мьютексу не нужно будет возить данные между кышами.
Вот обычный мьютекс для потоков – это сценарий неэффективный. И когда мы будем писать файберы
со своим планировщиком и со своим мьютексом, то можно будет делать так, чтобы критические секции
для файберов кластеризовались на одном ядре, чтобы они запускались вот подряд. Или, скажем, вы
пишете какие-нибудь горутины, и одна горутина шлёт, данные другая их получает. Вот было бы глупо
запускать их на разных ядрах, разумно запустить их одну за другой на том же ядре, потому что данные
уже в кэше. Вот компьютер ваш, STD-mutex и планировщик операционной системы, такую локальность
не учитывает. Но если вы пишете свой планировщик и свои мьютексы там в ГОИ или вот мы пишем в своем
фреймворке, то мы можем эту локальность учесть кластеризовать секции на одном ядре, в одном
потоке, и получить просто большую пропускную способность. Вот это такая очень хитрая мораль,
надеюсь, я смог вам её рассказать. Прокажите на сегодня всё. В следующий раз мы поговорим про
модели памяти, это будет гораздо более сложная и совершенно убийственная история, и я вас немного
обману, потому что я вместо одной лекции прочитаю вам две, потому что за одну лекцию невозможно
уместить, и лучше делать это разом. Вот как именно мы организуемся, мы обсудим в чате, но вот на
следующей неделе мы продолжим разговоры про процессоры и про memory order, и там у нас будет
три часа сложный разговор, где мы даже не узнаем, что такое release-acquire. Вот, так что на сегодня
всё, приходите через неделю, спасибо.

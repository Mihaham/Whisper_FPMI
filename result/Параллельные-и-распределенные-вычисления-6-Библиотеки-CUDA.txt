Всем доброго вечера!
Мы продолжаем с вами изучение окуда, точнее завершаем
изучение окуда, и сегодня мы с вами будем рассматривать
приложение, задачи, которые мы решали в прошлый раз.
Кто помнит, какие мы задачи в прошлый раз научились
решать?
Мы научились с вами решать задачу РЕДЮС, которую считают
сумму чисел массиве, и вторая задача это задача префиксная
сумма.
Сегодня научимся применять эти задачи, поймем как
они решаются, далее мы даже поймем, а куда двигаться
дальше.
Смотрите, задача префиксная сумма, она позволяет делать
такую вот интересную вещь, и вообще куда можно копать
дальше видеокарты.
Смотрите, давайте мы реализуем функцию compaction, или она же
функция filter.
Суть в чём?
Вы когда-нибудь в питоне писали код функции filter?
Допустим, у вас x, 4, 5, 9, 8, 3, 4, 2, и вы пишете следующее.
y равно filter лямбда x.x-5 к нашему массиву x.
И вы получаете список из объектов, равный 4, 3, 4, 2.
Отверждается, что эту задачу можно решать при помощи
видеокарты.
Ну это понятно дело, но утверждение, что и функцию
filter можно распаралелить.
Хочется допустим, быстро считать.
Хорошо, значит функция compaction оставить все элементы
массива меньше преданного значения.
А если мы говорим о продаже сортировки массива, то
это же можно делать на видеокарте.
На самом деле даже больше того скажу для подсчёта
гистограмм.
Как у вас накапливается какая-то огромная массива
значения, вам нужно посчитать гистограмму распределения.
Да, да, да, ну конечно понятно, что это такие вещи, такие
поэфемерные, но всё равно можно делать.
Быструю сортировку можно сделать, а можно делать по
разным сортировкам огромных массивов данных.
Ну да, но по факту мы будем её реализовать.
Смотрите, чем заключается функция compaction.
Так, секунду.
Этот момент надо вырезать или не вырезать.
Visual Studio Code нам мешает.
Значит смотрите, что делает функция compaction.
Compaction, собственно, задача фильтр.
Мы берём наш массив и оставляем только те числа, которые
удовлетворяют условия.
Значит смотрите, мы берём входную массив, предположим,
что у нас есть символы P, которые удовлетворяют условия,
и символы Q, которые не удовлетворяют условия.
Вот этот фильтр можно распараллелить, условия фильтра.
Для каждого массива мы с вами запишем значения,
единички или нолики, которые у нас имеются.
Дальше смотрите, для вот этого массива мы посчитаем
сумму на префиксе.
Да, то есть у нас был элемент 1110011010.
Дальше массив префиксных сумм будет у нас 1233556.
А дальше мы делаем следующую вещь.
Давайте посмотрим те индексы, которые у нас поменялись.
То есть предыдущий элемент от него у нас не такой же, как наш.
Или мы можем взять массив меток для того, чтобы посмотреть,
стоит ли значение метки единички или нет.
И оказывается самое интересное, что вот здесь чиселка 1 указывает,
собственно, на какое место в массиве нам нужно поставить этот элемент.
Чиселка 2 нам показывает, на какое место нам нужно поставить этот элемент.
Этот элемент стоит на третье место, этот элемент стоит на четвертое место,
на пятое место и на шестое место.
То есть самая тяжелая задача это подсчет суммы на префиксе,
а дальше остальные задачи решаются легко.
Вот такое одно из применений.
Смотрите, а если у нас есть решение задачи вот такого,
то мы можем с вами посчитать другой массив.
Можно?
Нагадывайте, что я намекаю.
На квиксорт я намекаю.
Причем, как ни странно, можно две вот этих функции реализовать параллельно.
То есть чтобы они у нас развезли эти две функции.
На самом деле, не сильно это быстрее будет, а по симптотике будет большая.
Но если вам нужно брать какое-то большое количество массивов
и вот их загонять в сортировку,
то это может происходить намного выгоднее.
Вот, на самом деле, если говорить, куда двигаться в дальнейшую сторону,
вообще есть вот такая библиотека.
Я сейчас покажу ее.
Да, называется это.
EnvelopesCube называется.
Библиотека.
В общем, советую ее посмотреть.
В ней есть большое количество реализации разных утилит.
И здесь есть всякие сложные алгоритмы.
Экзамплс.
Давайте откроем экзамплс.
И здесь, видите, есть RadixSort.
Сортировка.
Тут страшная вещь.
Давайте посмотрим.
Установите, тут даже префикс scan написан.
Типа кому будет интересно.
Сейчас я найду примеры.
Секундочку.
Classes.
Сканы есть, Store.
Вот, DeviceRadixSort, то есть сортировки.
Сегменты RadixSort.
Здесь какие-то красивые картинки.
Ну, видите, тут есть свои собственные сортировки, которые можно использовать.
То есть RadixSort тоже можно реализовать таким же способом.
То есть по факту нам нужно что сделать?
Нам нужно в правильном порядке числа расположить в определенном сегменте массива.
То есть все эти условия тоже можно считать префиксным scan'ом.
И таким образом мы куда можем приспособить для scans.
Вот такая интересная задача.
Быстрая сортировка.
Как осуществляется?
Берем опорный элемент.
Делаем два compaction.
Один compaction меньше, чем x.
Другой compaction больше, чем x.
Дальше запускаем рекурсивный обход.
Главное, кстати, чтобы в этом рекурсивном обходе у вас размер массива,
который вы начинаете дальше сортировать, не был очень большим.
Точнее, был очень большим.
Почему, как вы думаете?
Почему вот когда мы бьем в quicksort'е наш массив на две части,
и вот эти две части очень маленькие уже,
почему их не надо запускать заново сортировку на GPU?
Не, ну просто потому что размер массива уже будет меньше, чем размер блока.
Зачем закидывать туда?
Смысла вообще никакого не будет.
Поэтому как бы для больших блогов вы можете запустить по факту
arrangement расстановку этих милетов, а для маленьких там уже как-то самостоятельно это все делать.
Ну как в классической сортировке это делается.
То есть тем самым мы можем с вами запустить рекурсивное учтение элементов.
Так, это вот такое приведение, которое было по сравнению с прошлой задачей,
то есть которая у нас была на скане.
И сейчас нам нужно поговорить про вот такую вещь,
о том, что не надо все писать на видеокарте.
То есть не надо писать самому код на видеокарте,
если уже есть какая-то реализация этого кода на видеокарте.
То есть поговорим немножко про применение.
Библиотека компании NVIDIA представляет огромное количество библиотек, которые можно использовать.
Значит, первая библиотека, с которой, возможно, кто-то из вас познакомится на семинарах, это библиотека Kublas.
Что такое Kublas?
Bicycleaner Algebra Subroutine вспомнил.
Возможно, кто-то слышал среди вас библиотеку Kublas.
Она позволяет всякие массивы перемножать, матрички и так далее.
Какие достаточно простые операции.
И на самом деле, когда вы пользуетесь тем же самым нампаем,
на перемножение матриц,
вы некоторых реализаций, скорее всего,
под капотом вы будете использовать как раз библиотеку Kublas.
Для вот этих всех значений.
Собственно, на ВКУДе тоже реализовали вот эту концепцию.
И сразу скажу, что она может и работает хорошо,
но там нужны некоторые лайфхаки для того, чтобы это работало эффективно.
То есть для массив маленьких оно не эффективно работает,
для массива больших оно работает эффективно.
Всякие разные операции.
В репозиториях с примерами есть пример использования Kublas.
Там создается отдельный контекст, в рамках которого выполняется вычисление.
Там, конечно, весело нужно разбираться тем,
как нумерация элементов массива выполняется.
Там, как говоря, взять одну строку из матрицы,
надо написать функцию из десяти аргументов, что ли.
Ну, это си. Код на сях написан.
Дальше мы переходим к следующей библиотеке.
Если мы хотим рассказать, это QFFT.
Как вы думаете, как расшифровывается FFT?
Pass Fourier Transform.
И здесь библиотека для того, чтобы вычислять быстрое преобразование фурье.
Опять же, тут есть математика всего этого процесса,
который я не собираюсь рассказывать, но скажу вам сразу.
Если вы на каком-нибудь собеседовании скажете себе,
что я начну реализовывать быстрое преобразование фурье на видеокарте,
не смейте этого делать, потому что вас либо обсмеют, либо выкинут.
Собестно. Как вы думаете, почему?
Ну, это раз, да.
Ну вот, scan написали. Reduce написали.
Да. В чем у нас была проблема в операциях scan и reduce?
В каком эффекте?
А, банконфликты у нас были.
Так вот, оказывается, что если вы начнете писать быстрое преобразование фурье на видеокарте,
вы просто с ними столкнетесь и не сможете с вероятностью 95% их решить.
Они очень сложные.
Поэтому тут забавный эффект.
Значит, у вас был курс по теории колец-полей или еще что-нибудь подобное?
Знаете задачу? Классическая задача построить многоугольник правильной циркулем-линейкой.
Классическая задача построить многоугольник правильной циркулем-линейкой.
Вы можете построить треугольник циркулем-линейкой.
Вы можете квадрат построить циркулем-линейкой.
Но какие-то многоугольники вы не можете построить циркулем-линейкой.
Там это связано с решением уравнений, с самыми коэффициентами.
Точнее с, я не помню с чем. В общем, с чем-то подобное.
То есть вы не для всех многоугольников можете построить правильный многоугольник при помощи цирколинейки.
Здесь идея точно такая же.
То есть вы не для всех количества чисел массива,
можете построить быстрое преобразование фурье так, чтобы оно работало эффективно.
И для этого, чтобы мы с вами посмотрели, есть такая библиотека КУФФТ.
Мы с вами идем и читаем документацию.
К ней документация хорошая, и смотрите.
Прочитайте эту строчку.
Алгоритм высоко оптимизировал для матриц входного размера,
который в разложении на простые со множестве имеет двойки, тройки, пятерки, семерки.
Если у вас алгоритм размер массива кратен двойки, зашибись степень двойки.
Если степень двойки и тройки уже не очень.
То есть вы понимаете, насколько оптимизация идет по вот этим элементам?
Ну, то, что алгоритм специально написан таким образом, что в нем есть костыли.
И он заифон сильно.
Ну, собственно, если мы с вами захотим посмотреть, допустим, КУ до КУ-бласс,
мы с вами увидим страшный код.
Тут документация, конечно, очень большая.
Вот, допустим, вот это вот, по факту, код, который устанавливает значения в матрицах.
И как раз здесь используется тот самый Pitching механизм, который вам приходит,
когда вы делаете патриотическое значение в матрицах.
Вот, допустим, вот это вот по факту код, который устанавливает значения в матрицах.
И как раз здесь используется тот самый Pitching механизм,
который вам приходится следовать в домашних заданиях.
Видите, тот алаем он прямо прописан.
В общем, как говорится, пока не посидишь с этим кодом, ты его не разберешь.
Потому что видите, какое огромное количество аргументов.
Но, в принципе, это можно использовать.
Более того, вы можете писать код, вообще не вызывая никакого КУ до ядра.
Дальше. Есть математическая библиотека.
То есть внутри функции куди вы можете вызывать всякие математические операторы.
Мы этого не делали, но, в принципе, можно.
Дальше. После этого есть библиотека Kurant,
которая позволяет генерировать случайные значения на видеокарте.
Не поверите, это может быть даже выгодно вместо того, чтобы генерировать случайные значения на хосте.
Потому что на хосте рандом работает медленно.
Ну, вектор.
На хосте?
А там же всегда плюс-минус всегда случайные генераторы.
То есть давайте вспомним, как генерируются случайные значения.
Вот, вы знаете, как генерируются значения случайной величины?
Не-не-не. Случайные распределения.
То есть вам нужно сгенерить вектор из нормального распределения.
Не.
Да. Смотрите, идея следующая.
Ну, зависит от типа распределения.
У нас общая картина распределения делается следующей.
Смотрите, у вас есть х, минута ступов или тервер.
Х с некоторой случайной величиной.
Ну, какая-нибудь с хорошим распределением, который можно считать.
Вопрос.
Как распределено f от х?
А?
Ну, давайте посчитаем.
Вероятность того, что функция распределения случайной величины не больше, чем c.
Нормально.
Ну, да, можно инвертирует, но на самом деле здесь можно сделать хитрее.
Смотрите, c это параметр от 0 до единицы.
Сейчас, блин, я сам вспомню, как это делается.
Ответ-то я помню.
Ну, да, да, да. Нет, на самом деле, смотрите.
Значит, что такое f от х?
Ну, смотрите, то есть это у нас f от х, это вообще, это получается вероятность того, что х, так давайте маленьким, f от t, это вероятность того, что случайная величина не превосходит t.
Ага.
Она тоже распределена.
Смотрите, у нас случайная величина, функция, измеримая функция случайной величины тоже является случайной величиной.
Да?
То есть смотрите, что у нас получается.
Давайте подставим.
То есть это у нас получается мера по всем t таким, что вероятность того, что х не больше, чем t, не больше, чем c.
Да, может тут неформальное математическое определение, но суть, смотрите, в чем.
Что вот когда мы будем перебирать все параметры t, то в конце концов вы не поверите, но здесь останется ровно c.
То есть если вы рассмотрите все объекты, грубо говоря, мощность всех объектов того, что у вас вероятность распределения случайной величины не больше, чем c, то вы получите вот то же самое значение c.
А это значит на самом деле, что функция распределения случайной величины равномерно на отрезке 0,1.
Это факт, который используется для генерации всех случайных величин.
То есть чтобы вам сгенерировать случайную величину, вам нужно посчитать, сгенерировать случайно, сэмплировать значение, дальше взять обратное значение функции распределения, а ее можно посчитать, потому что она не убывающая практически везде, и вернуть это значение, которое у вас было.
Это работает, потому что у вас функция распределения распредлена равномерно.
Там же есть чистая формула. Для нормальных распределений сложно. Там нужно плюс-минус CPT воспользоваться.
То есть мы знаем, как сэмплировать, берем случайную величину, вычитаем в среде, делим на корень дисперсии.
Ну да, тоже есть. Ну вот, но это просто вот такое. То есть для экспоненциально для вот этого всего работает. То есть вы берете просто, сэмплируете значение, случайные значения на диапазоне 0,1 уже можно сэмплировать.
Вы просто берете там какой-нибудь псевдослучайный генератор и запускаете.
Вот. То есть с этим будет меньше проблем, если мы сделаем это на видеокарте.
Вот. Собственно, поэтому можно курант использовать на видеокарте.
QSolver это решатель линейных уравнений. Что там? Проблем какая-то?
Курт Солвер это решатель линейных уравнений.
А?
Потому что нам есть всякие проблемы.
Хороший вопрос.
Ну тут потому что он работает не только для Dance Matrix, но и еще это.
Умеет работать со Spark Matrix. То есть есть и две отдельных реализации.
QSolver.
Дальше, значит следующая вещь, это QTensor,
которая позволяет работать с тензорными линейными операциями, тоже полезная.
И, значит, редко которая встречается, это AMGX. То есть это GPU Accelerated Solvers для симуляции и всяких не структурированных методов.
Это чисто математическое моделирование. То есть видно, что уже по математическим библиотекам CUDA достаточно активно применяется.
Причем в каких-то реальных случаях.
Что касается MPI, то есть два механизма, к которым можно запустить параллельные CUDA и MPI.
Первый протокол это NVShMEM, NVIDIA Shared Memory.
Но сейчас зачастую используется NCCL. То есть это библиотека для мульти-GPU,
которая позволяет максимизировать пропускную способность.
То есть вы по факту собираете кластер не из вычислительных узлов, а из видеокарт вычислительной.
Вот, честно, я пытался завести это все на нашем кластере.
На вычислительном у меня не получилось.
А если запустить, то будет офигенно. То есть вы по факту можете коллективную операцию в MPI реализовывать параллельно.
Что такое коллективная операция?
Это когда у вас берутся все процессы в коммуникаторе и для них выполняется общая операция.
То есть вам нужно, допустим, посчитать градиентный спуск, вам нужно посчитать среднее значение по всем градиентам внутри бача.
Вы берете, считаете градиент, дальше вы выполняете общую операцию, которая вам распределяет градиент по узлам.
То есть еще раз.
То есть у вас нам DL по DV на первом узле,
DL по DV на втором узле,
DL по DV на третьем узле,
на четвертом узле.
И вы берете и параллельно запускаете одну инструкцию,
которая сама как-то научится коммуницировать между всеми видеокартами,
и на каждом сделает DL по DV среднее, точнее сумму.
То есть запустит reduce не внутри одной видеокарты, а между всеми видеокартами.
Да, да, да.
Можно.
Кстати про это сейчас будет еще занят, этот блок.
Но по факту да, то есть обычно процесс запускается параллельно.
Можно по идее сделать, но обычно работают в пределах одной видеокарты.
Ну да.
Ну да, да.
Самостоятельно умеет перегонять данные.
Так, а отдельный момент, значит, для любителей искусственного интеллекта и же с ними.
Сейчас Nvidia активно в этой сфере развивается.
И из таких популярных библиотек, которые есть, это CUDA DNN.
CUDA DNN. DNN расшифровывается как Deep Neural Networks.
Это библиотека специально, в которой зашита вся реализация известных слоев.
Нейросетивых.
Максимально оптимизированный подкод.
Значит, дальше есть следующая библиотека, Tenzer RT.
Что она делает?
У вас, когда есть какая-то большая нейросеть, вы понимаете, что на самом деле
большое количество весов в этой нейросети вообще не используется.
Вы берете, запускаете оптимизацию видеокарты под нейросетивые модели под видеокарту.
То есть вы загоняете код модели, в которой вам что-то предсказано.
Вы прогоняете через Tenzer RT.
У вас размер этой штуки уменьшается и время инференса.
Дальше тут идут уже всякие дополнительные библиотеки, которые используются
Nvidia для решения задач в режиме реального времени.
Значит, что касается аналогов куды, то здесь есть библиотека OpenCL.
Про которую мы с вами уже говорили.
Это OpenGL для других видеокарт.
Для AMD, допустим.
Либо для Android, кстати.
В Android тоже используется OpenCL.
Если у вас нет графического движка.
Там, точнее смесь, там либо используется OpenGL с шейдерами.
Ну да, да, это общий.
То есть у AMD есть другой язык.
RockM называется.
AMD RockM.
Можно посмотреть какие-нибудь коды.
А, это документация.
Здесь есть код?
А, есть.
Можно посмотреть, но...
Ну да, потому что куда активно для этого времени развилась.
И Nvidia вкладывается в всякие искусственные интеллекта.
Что у нее даже очень хорошо получается.
Главное, чтобы, если не повторить тот же самый эффект, который был...
Который мы говорили в первой лекции.
Что Nvidia такая стала совсем крутой.
Ну все, типа нам никто ни по чем и забить на все.
Ну тогда они проиграют преимущество и вылетят с рынка.
Как-то было с другой компанией.
Чтобы посмотрели.
Есть OpenACC еще, другой фреймворк.
Который называется OpenMP для видеокарт.
Open Accelerate.
Вот.
Ну, собственно, вот такие вот библиотеки.
Их можно использовать, их можно искать.
Там для OpenACC тоже есть код.
Тоже можно посмотреть.
Если вы хотите, допустим, посмотреть куда-нибудь, покопать.
То вы можете зайти...
Light GitHub.
И посмотреть, собственно, TensorFlow.
Light kernels.
Так, это хо-хедеры.
Посмотрим источник кода.
Здесь нету.
Ну, собственно, тут люди...
Видно, что предлагают.
Вот.
Не знаю.
Да, конечно же.
TensorFlow.
Угу.
Это, допустим, вот она на P.
GPU. Нет, это не GPU.
Это не Flex.
Где-то тут код шейдеров был.
Вот это я точно помню.
Кажется, не здесь.
Так.
Чего-чего?
Ну, можно, но это будет сильно сложнее.
Так.
Что-то я не могу найти библиотеку.
Здесь тесты всякие разные.
Ну ладно, если что, я поищу.
Ну, в принципе, тут можно...
А, тут можно сделать вот так.
Успользоваться поиском.
Чего бы интернет ловил?
А, ну понятно почему.
Как вы думаете, интернет поймает?
У меня как-то мы к другому...
Понятно.
Все, смотрите, поиск.
Собственно...
Собственно...
Еще страшный код.
Ну вот он, GPUCL, скорее всего.
А, это тесты.
В общем, где-то здесь надо искать.
А, GAL кстати есть.
А, вот они.
В общем, это код, который вычисляет все вот эти ящики.
А, это ты?
А, это ты?
А, это ты?
А, это ты?
А, это ты?
В общем, это код, который вычисляет все вот эти ядра.
В общем, вот он тут.
А, вот, смотрите.
Вот он вам. Наконец-таки мы нашли код этого.
Вот это код шейдера.
То есть вот он исполняется при помощи OpenGL.
Смотрите, есть разные бэкэнды, которые можно реализовывать.
Ну, не знаю, для того чтобы, видимо, можно было его в рантайме,
типа не подгружать из отдельных файликов.
Тут есть тоже...
Угу.
Да. И вот видите, что вот здесь вот, видите, тоже есть вот такой вот grid.
Здесь есть тоже workgroup-size, собственно, и есть код кернала.
Где у нас кернал тут?
Кернал, кернал, кернал. Вот он.
А где он определяется?
Тут много где есть.
В общем, немножко более страшный код, но здесь...
Сразу скажу, что здесь есть в коде огромное количество именнооберток.
То есть если код без оберток смотреть, то в OpenGL он там приятнее выглядит.
То только то смотрите.
Причем отличие, если мы в коде пишем код прямо в одном файле,
то есть у нас есть файл разрешением .co, то в OpenGL там пишется два файла.
Один код это код, который будет запускаться в наше устройство,
а другой это cpb-шный код, который подключает этот файл и читает код программы.
То есть некоторое различие в вычислениях идет.
Вот, кстати, код CLKернала.
Так, это что касается, собственно, привинений, куда двигаться можно относительно кода.
Да, собственно, что если нужны какие-то свои собственные цели,
то можно их решать при помощи своих задач либо писать обертки под разной библиотеки,
если есть другие механизмы.
Так, это что касается кода, сведения и так далее.
Значит, и сейчас будет немножко продвинутая тема, так сказать, насладкая.
Это то, что в куде есть конкуренция.
Затем, сейчас расскажу.
Значит, суть в чем.
Тема называется кудо-потоки, только теперь не треды, а стримы.
Смотрите, кажется, мы уже с вами научились все делать, но не все так просто.
Какая операция сейчас больше всего времени у нас с вами занимает?
Копирование данных.
А что, если делать копирование ассинхроном?
Можно ли что-то выигрывать?
На самом деле, да.
Смотрите, как обычно выглядит поток комматов на видеокарте.
У нас с течением времени выполняется kudememcpy async,
потом выполняется ядро, потом выполняется kudememcpy async обратную сторону.
Вроде код нормальный.
А теперь смотрите, в чем особенность стоит этого кода.
А можно так сделать.
В общем, знакомьтесь, что в коде, в видеокарте на самом деле есть несколько ассинхронных очередей.
Есть две очереди на копирование и одна очередь, как минимум, на исполнение.
И они работают параллельно между собой.
То есть смотрите, что в этом коде происходит.
В этом коде мы выполняем четыре ядра.
Каждый работает над своим собственным элементом массива.
И как только мы выполнили ядро один, мы сразу скидываем результат в память для этого первого результата.
Ассинхронно в параллельном потоке.
Второй код выполнили.
Во втором потоке скидываем код ассинхронно.
В итоге они попадают у нас в разные очереди,
из-за этого мы не видим, что происходит.
В итоге они попадают у нас в разные очереди, из-за этого ускоряется скорость вычисления.
Понятно, да?
Это можно так.
А можно еще.
А можно и так.
То есть это называется freeway concurrency.
Собственно, что мы с вами делаем?
Мы с вами берем и запускаем кудамин spy host to device тоже в ассинхронных потоках.
Кажется, все замечательно работает.
А более того, не забываем, что у нас есть CPU,
на котором тоже что-то можно считать.
Да, именно так.
А можно еще и 4+, если у вас массивов там больше чем 4, вам нужно разбить это все на задачу.
Знаете, что мне это напоминает?
Задачу про кирпич.
Знаете, что значит про кирпич?
Набор кирпичей.
У вас есть кирпич.
Или учебник.
Или планшет, в зависимости от того.
Дальше вы ставите второй кирпич, максимально отделяя от первого.
Так, чтобы он стоял.
Вы кладете третий кирпич.
Потом четвертый.
Ну и так.
Попрос, какое максимальное количество кирпичей вы можете положить?
Ну да.
Ну точнее, какой максимальный сдвиг будет?
Сколько?
Да, только ну лучше.
Здесь, к сожалению, не сколько угодно.
Здесь у нас всего три возможных способа расширения.
Четыре даже.
И смотрите, что получается.
То есть, если вы на CPU умеете делать 40 ГБ, то вы можете сделать 40 ГБ.
Ну и что получается?
То есть, если вы на CPU умеете делать 40 ГБ, то с одной GPU на уровне одного парализма вы получаете 120 ГБ.
С двумя потоками 180 ГБ.
В три потока, когда у вас все выполняется, вы получаете 260 ГБ.
И комбинация CPU и GPU вам дает следующее.
То есть, вы четыре, четыре конкаренции, которые CPU и GPU получаете 280 ГБ, а в бесконечности вы получаете порядка 330 ГБ.
Не, мы пытаемся бесконечно раздробить нашу задачу.
Эта GPU-шка-то у нас одна.
Это все на одной GPU происходит.
Ну то есть, видите, как это все происходит, интересно.
Вопрос, а как это делать?
Нет, ну вот смотрите, вы вызывали ядро уже много раз, там есть три параметра.
Напоминаю, это количество блоков, количество потоков в блоке и размер разделяемой памяти.
Нет.
Нет.
Смотрите.
Грузись.
Смотрите, параметры CUDA memcpy.
Async принимает четвертый поток, идентификатор потоков.
Смотрите, параметры CUDA memcpy.
Async принимает четвертый поток, идентификатор потоков.
Вот.
И дальше, значит, если вы не против, я сейчас попытаюсь показать какой-нибудь пример кода в Visual Studio Code.
Ай, не надо выходить из терминала.
Ай, не надо выходить из терминала.
Да.
От APGPU кто-нибудь видит?
Ну, допустим, мы откроем какой-нибудь код на видеокарте.
Не знаю, выделит он или нет.
Ну, допустим, мы откроем какой-нибудь код на видеокарте, не знаю, выделит
он или нет, видите, стрим-еррор-нейм,
более того, видите, тут в стриме можно поставить, то есть вы создаете этот стрим, вот и
передаете код в этот стрим, то есть именно в этот поток исполнения. То есть у нас с вами в коде есть
четвертый параметр вызова ядра, это то, в какой стрим вы отправляете вычисления. Соответственно,
если вы несколько кодов выполняете параллельно, отправляете в разные стримы, то, в принципе,
окей, он будет выполняться независимо. Если вы две операции отправляете в один и тот же стрим,
ну, извините, они будут работать последовательно, один друг за другом, то есть это контекст исполнения.
Вот, значит, куда стрим? Заключается следующее, что если задача легкая, то ее можно запустить в
отдельном потоке. Поток по умолчанию ноль, и он является синхроном к хасту и к девайсу. То есть
видели, наверное, что кудамим цпай, изначально он достаточно синхронный. Значит, ассинхронная,
как мы делаем? Значит, запуск ядра, как мы делали, кудамим цпай ассинг. Дальше смотрите,
а, что является ассинхронным? Значит, у нас первый запуск ядра является ассинхронным. Второе,
это кудамим цпай девайс ту девайс. Раз. И если у вас кудамим цпай используется хост ту девайс,
но при этом память такая, что она используется на уровне l1 кэша, то есть она не очень сложная,
арифин, вычислительная, то вы используете кудамим цпай хост ту девайс тоже в ассинхронном режиме.
Ну, редко вы когда увидите на видеокарте, что кудамим цпай хост ту девайс у вас будет
являться ассинхронным. Замечу, что кудамим цпай с хоста на девайс, с девайса на хост
является ассинхронным. Но если вы с девайса на девайс делаете, то эта операция становится ассинхронной.
Вот. Пример синхронного ядра. Собственно, вы берете, вы делаете молок, дальше кудамим цпай,
дальше вы указываете грид блок 0, кстати, да, и грид блок 0. Здесь 0 это размер
shared памяти, если вы укажете тот же самый стрим, то он нулевой, то вы получаете синхронный код.
Значит, ассинхронность без стримов, значит, собственно, как она достигается?
Вы смотрите, у нас есть с вами два кода, и они у нас подсоциально пересекаются. То есть,
вот эти вот kernel2 и сам цеплый метод будут вызываться ассинхронными между собой. То есть,
они могут выполняться параллельно. То есть, пока вы на GPU что-то отправляете, вы на cpu считаете.
Вот. А теперь смотрите. Вот это вот такая вот интересная вещь, что если вы создаете четыре
отдельных стрима, и в каждом из стримов выполняете одну свою собственную операцию, то у вас они все
потенциально могут работать независимо друг от друга. Если видеокарта этого позволяет.
Кстати, важно, что если вы хотите использовать ассинхронное копирование, то, пожалуйста,
привяжите свою память. Механизм PintMemory. То есть, вы алоцируете кудома lockhost, сразу
массив. Именно из него копируете сразу массив. То есть, у вас не может быть память, так называемая,
pageable. То есть, ее не должен уметь читать механизм страниц. Все знакомы с курсом операционных систем,
но что такое страница? То есть, у вас страничность, так сказать, файлов должна быть отключена,
памяти должна быть отключена в этот момент времени. Иначе ничего не заработает. Вот такой код.
Понятно. И вот теперь мы с вами хотим еще один момент посмотреть. Это поговорить про очереди
копирования. Собственно, они устроены таким образом, что у нас есть память, которая копируется с их
Hostano device, и они добавляются в порядке, так сказать, issue. То есть, у нас, смотрите, есть операция с их
Hostano device для массива A1, для массива B1. Дальше у нас выполняется ядро K1, и дальше у нас
идет два копирования. С DH1 и с DH2. И в итоге смотрите, что у нас происходит. У нас DH2 полностью
не зависит от DH1. Но поскольку у нас с вами все элементы очередной копирования встают после
текущего, то этот код выполняться будет по линии времени исполнения вот таким образом. То есть,
у нас device to host 2 будет выполняться после device to host 1. Вопрос, как решить эту проблему?
Чтобы это все выполнилось чутко быстрее. На самом деле, DH2 можно переместить перед DH1.
Перемещаем перед DH1 и получаем результат. Вот, видите? И он проваливается в нашем
Tetris вниз. Вверх по времени исполнения. Значит, дальше делаем следующую вещь. Тоже интересная
очередь выполнения. Допустим, у нас с вами есть над элементом массива, мы с вами выполняем две
операции. То есть, сначала над элементом A, потом над элементом B в первом стриме, а потом над
элементом A и над элементом B во втором стриме. Вот, при этом эти стримы независимы. Тогда смотрите,
что происходит. Поскольку A и B у нас выполняются в первом стриме, то сначала у нас идет A,
потом идет B. Но смотрите, в чем особенность. Поскольку у нас с вами, сейчас скажу, вызов ядра,
который находится с элементом здесь, он не может понять, как эти операции между собой связаны,
то мы делаем следующее. Мы берем и этот элемент массива, то есть, сейчас давайте подумаю.
Всегда на этом моменте я спотыкаюсь. Сначала у нас A1, потом идет B, то есть B1 всегда выполняется
после A1. Все, вспомнил, значит, что происходит. Посмотрите, поскольку задачи в очередь на
исполнение ставятся в последовательном порядке, то если мы какую-то задачу уже поставили в очередь,
в какой-то момент времени T, то следующие задачи будут ставиться в очередь тоже в тот же самый момент
времени T. В итоге смотрите, что у нас происходит. Мы с вами взяли первый поток, выполнили первое
ядро, на первом потоке вставилось оно сюда. После этого мы поставили в очередь на исполнение
вторую задачу B на том же самом стриме. Это означает, что вот этот отчет исполнения вот этой задачи будет
уже после того, как у нас выполнена вот эта задача. А это значит, что все следующие задачи,
которые мы будем запускать здесь, они будут находиться по старту исполнения как раз на этом уровне.
В итоге, видите, у нас возникает блокировка, которая ломает наши все зависимости. То есть,
у нас A2 ждет, пока B1 запустится. То есть, он может стартовать параллельно с B1, но не последовать.
Ну, то есть, они не могут стать параллельными. Но если мы поменяем очередь задач с вами и мы будем
работать вот в таком порядке, то сначала у нас параллельно запустится A1 и A2, а после этого уже
будет параллельно запускаться B1 и B2. Да, потому что мы запускаем задачу во втором стриме в тот
момент времени, пока у нас с вами, как говорится, время не уехало на исполнение операции.
Да, независимые операции, разводить их максимально. По факту, да. Я даже сказал, что это вот конкретные
потоки, которые просто пронумерованы. Ну, как в обычном ЦПУ, то есть, мы можем выделять по факту
отдельный поток, который вырешает отдельную задачу. Да, именно так. То есть, их в нужном порядке закинуть
как раз в очередь для того, чтобы они работали. Вот. Следующий момент такой. Писать так вот не надо.
То есть, смотрите, что у нас идет. У нас идет программа host device 1, host device 3, host device 2, host device 3.
Дальше у нас идет kernel 1, kernel 2, kernel 3, потом device to host 1, device to host 2, device to host 3.
И в итоге получается следующая вещь, что у нас с вами. Что происходит? Значит, host device 1,
host device 2, host device 3 выполняются. Параллельно значит, запускается ядро. Компьют Q называется K1, K2, K3.
Вот. А дальше смотрите, в чем особенность. Особенность в том, что если мы даже с вами сделаем MCPy копия
device to host на sync, то он будет ждать того момента, пока вот эти все три ядра запустятся и отработают.
То есть, мы с вами будем ждать кода выполнения результатов. Поэтому, значит, чтобы избавиться
от синхронизации между разными очередями, между очередями dh1, dh2, между device to host Q и compute Q,
как вы думаете, как можно расположить эти элементы так, чтобы у нас не возникло копирование, блокирование элементов?
Ну да, логично. То есть, видите, мы их группируем по цветам и сразу, как только у нас выполнилось
ядро, мы сразу копируем результат на память. То есть, выгоднее на видеокарте, знаете как, кажется,
ну давайте сгруппируем наш код. То есть, возьмем элемент 1, возьмем элемент 2, возьмем элемент, то есть,
сначала все скопируем, потом все посчитаем, потом все обратно скопируем. Вот на видеокарте так не работает.
То есть, если мы хотим запускать какой-то код эффективно на видеокарте, то нужно делать вот таким образом.
То есть, сделать одновременное копирование. Вот, это такое вот небольшое дополнение про коды стримы,
которое можно разбирать. Собственно, вывод, который есть, вывод следующий, что в коде ядра, собственно,
видеокарты есть еще такая концепция, как очередь вычислений и две очереди на копирование.
То есть, есть отдельная очередь копирования с девайса на хвост и есть отдельная очередь копирования
с эхоста на девайс. После этого мы с вами выполняем гидро внутри. Вот, при этом concurrency есть и в GPU,
по факту его тоже можно дернуть через видеокарту. То есть, если посмотреть на кудо девайс проб,
то это структура, которая позволяет проверять очень большое количество зависимости. То есть,
то есть, видите, что можно знать про видеокарту? То есть, не везде, кстати, есть pageable,
pageable memory access. Видите, не везде поддерживаются sparse, kuda, array. Сейчас.
Вот. И вот, смотрите, вот это важно. Видите, device can possibly execute multiple kernels. То есть,
поддерживается ли запуск параллельных задач? Вот. Он может не поддерживаться. Вот.
И, в общем, вот эти вот все опции, они позволяют нам использовать вот всякие разные хосты. Вот.
И, видите, здесь есть разные движки. То есть, можно посчитать, узнать сколько именно движков
позволяет вам параллелить задачу максимально. Вот. Значит, это что касается вот куды самой.
А, как Arangkernels, да? То есть, а, это у нас это. Это я уже рассказывал. Вот. Значит, это что касается видеокарт.
Вот. Я думаю, что я дал какое-то вот такое введение, связанное именно с вычислением на GPU.
На практике вы тоже посмотрели разные способы alignment данных на видеокарте. Сейчас, значит, куда все идет.
На видеокарте есть такое понятие, как тензорные ядра, которые, не поверите, нужны для того, чтобы тензорные
произведения быстро считать. А где у нас нужны тензорные произведения? Где тензоры? На тензоры надо
быстренько уметь. А? И, и. AI, да, то есть. Ну, много. В общем, потому что сейчас эта тема достаточно широкая,
и сейчас идет новый фазовый переход, как, возможно, знаете, в сфере AI. Все в том, чтобы появились всякие вещи,
чат GPT, диффузионные модели и так далее. И они позволяют нам рисовать мир новыми красками, так сказать.
Вот вы пользуетесь голосовыми помощниками вот этим вот? Ну вот, я тоже пока не использую. Я что-то один раз пообщался
с ЯГПТ, и все. Вторая версия. Ну, в принципе, тоже уже помогает решать какие-то базовые задачи.
Хотя я не верю, что 76% сейчас уже используют, айтишников используют этот механизм. Может быть, мы не попали в
ту выборку. Вот. Поэтому эта сфера развивается. Вот. А если, особенно, вы понимаете внутреннее устройство того,
как это работает, вам это может дать некоторое конкурентное преимущество в том плане, что вы, допустим,
что-то можете реализовать свое собственное и быстро. Причем не обязательно только на видеокарте.
То есть именно вычисления на видеокарте, они позволяют немножко по-другому взглянуть на архитектуру.
То есть оказывается, что есть кэш, и он важен. И то, что мы этим кэшом можем управлять.
То, что оказывается, физические особенности устройства тоже самое, очень важная вещь.
Да, потому что в зависимости от того, как вы правильно расположите элементы в массиве,
оказывается, скорость вычислений сильно зависит. И так далее, и так далее, и так далее.
И более того, наверное, на семинаре вы видели интересный эффект. Я не знаю, видели вы или нет,
но по крайней мере у нас в семинаре был замечательный эффект.
Что вы, грубо говоря, запускаете код на видеокарте, и первый запуск идет медленнее.
То есть вы запускаете ядро, у вас первый запуск медленнее, а остальные быстрее.
Потому что в видеокарте у вас просто переключается в другой режим работы.
Да, да.
Да.
Вот.
Да.
Да.
Угу.
Ну да.
Вот.
Да, кстати, это.
Вот они.
Угу.
Во.
Высик HD Video Playback P8.
А?
Ну, извините, обратная совместимость.
Ну.
Угу.
Ну как, рендер. Рендер.
Вот они, видите, перфоманс.
Обычно по 2 и по 3 и по 8 видно.
Вот видно, что сейчас на одном устройстве у нас с вами что?
P8.
P8? А ту видеокарту, которую якобы сломали сегодня ночью,
да, те, кто смотрят на ютубе, это локальный мем в чате курса,
то видно, что на четвертом видеокарте запущена какая-то нейросеть,
которая ест 9,8 гигабайт, и она уже это.
Да, уже в режиме pad-wap.
Ну.
Угу.
Ну, нормально.
Ну, condo-python, да.
Нет.
Это вообще-то вычислительный кластер.
То есть, смотрите, если вы делаете nvidia-smi,
дальше вы делаете ps-alux.
Ну, пожалуйста.
Ну, в докере запустили.
nvtop?
Есть.
Ну да.
Вот такая вот вещь.
Видно, что кто-то там запускает задачи, да, и все такое.
Да, кстати, да.
Кстати, о мощностях видеокарта, чтобы вы понимали немного.
Я помню, nvtop не стоит.
Не стоит.
Чтобы вы понимали, что 250 ватт, это...
По уровню мощности это слабо.
Вот, видите, вот это другая видеокарта.
Да 3090 огромная.
Это про 3090, потому что 3090 350i слота занимает.
RTX 4090 size, да, хотите сказать?
Ой, мое.
Многовато, да.
Так.
Так, а вот это сколько занимает?
Где тут размера?
Такая же, 3 слота занимает.
Да.
Ну, в общем, она такая огромнющая.
Ну, зато...
А, смотрите, стоп, это я 3090 Ti сравниваю.
Ждите.
Ну, вот это...
Ну, вот это...
Ну, вот это...
Ну, вот это...
Ну, вот это...
Ждите.
Чего-чего?
Ну, это максимальная версия, которая поддерживается.
Версия о подселе, которая поддерживает кудру.
Эту видеокарту.
Ну, вот смотрите, да.
3090, как говорится, бандурины бандуринами.
Но 3090, она имеет тот же самый размер.
Triple float.
То есть, это шутки про 3090 были, а не про 4090.
Смотрите, огромные.
Да.
В общем, давайте немного...
Ну, да.
В общем, давайте просумируем.
Мы с вами познакомились с видеокартами.
Значит, дальше пойдет блок по большим данным.
И вот там уже будет
совсем другая история.
То есть, там уже будет
то, как это все используют
не в научных лабораториях и не для задач ускорения,
а вообще для поддержки
отказа устойчивости
всех данных.
То есть, готовится к тому, что там не просят...
Будут уже просить вас читать быстро,
вас просят организовать так,
чтобы все было максимально надежно.
И для того, чтобы у вас там...
Ну да, то есть мы уходим с вами из парадигма вообще параллельных вычислений.
То есть вот это было параллельное вычисление.
А дальше будет парадигма распределенных вычислений.
Самое главное это отказывая устойчивость.
У вас ничего не упадало.
И я думаю, что на этом мы можем закончить.
Если у вас есть вопросы, то...
Нет, ходу не я буду вести.
Ходов не буду вести Олег Ивченко.
Ну да, там уже будет мини-контрольное.
Тесты и вопросы.
Это обычный великолепный мем, когда возникает кэрпо-ходов.
Тогда очень много людей приходят на лекцию.
Так, давайте вопрос.
Сегодня такая тоже была бы больше общая образовательная лекция.
То есть у вас есть вопросы?
Ну да. То есть смотрите, что такое OpenACC.
Вы берете, при помощи механизма Pragma пытаетесь транслировать код на видеокарте.
То есть то же самое, что делает OpenMP.
OpenMP при помощи механизма Pragma вам пытается реализовать RedPool.
По факту. Ну и многопоточность.
Можно...
Где? Вот тут или дальше?
Да.
Смотрите, почему зависит.
Потому что мы после A1 запустили ядро B1.
Это означает, что вот это ядро ожидает исполнения ядра A1.
Но поскольку следующий стрим, поток мы ставим уже после того задачу,
как мы поставили в очередь задачу B1,
то мы указываем, что именно в этот момент времени параллельно они могут начать запуск.
Ну а не раньше.
Ну да. То есть если вы, по факту, заблокировали очередь,
то дальше вы не можете добавлять задачи раньше этого промежутка времени.
Да, время в очередь не может abuвать.
Да, ну может быть тот же самый момент времени, если вы SPOINOUT passing,
то вже у нас W AGAIN.
То есть dye it a logitech OゆC1,
presentations it a logitech O T1.
Да, ну может быть в тот же самый момент времени t1, либо если не повезло, то еще позже.
Ну для этого уже существуют другие задачи, это слурмом делается.
То есть вы ставите отдельный планировщик задач, который вычисляет вам эти задачи.
Ну мы не сказали, мы сказали, значит вызвали ядро со стримом 1, дальше вызвали вот это ядро со стримом 2.
Поскольку время в очереди не убывает, то они могут стать только так.
Потому что b1 встало здесь, значит b2 встанет параллельно с ним.
А в правом у нас a1 и a2 стартуют параллельно, то есть, смотрите.
Вот смотрите, а в правом у нас а1 и а2 стартуют параллельно.
Вот смотрите, суть в чем. Вот у нас есть а1, мы его поставили в очередь.
Дальше, на следующий момент времени все задачи, если мы поставим a2, то он выполняется в параллельном стриме,
и мы можем поставить его здесь, потому что точка отсчета запуска вот этого ядра это t равная нулю.
Значит в правом случае параллельно в стриме мы запускаем, получаем время запуска a2 равное нулю.
И у нас все запускается параллельно. Если в втором случае мы ставим a1, давайте t со звездой обозначим,
здесь t со звездой равняется нулю.
Потом мы запускаем a2, b1, и здесь t со звездой для этого момента времени становится единичка.
Значит время запуска ядра t старт a2 исторически должен быть больше ли равен, чем t со звездой.
В текущий момент времени.
Момент времени, в который было запустится вот это ядро.
То есть время запуска следующего ядра всегда не меньше, чем время запуска предыдущего ядра. Время старта.
Да.
Еще вопросы?
Окей, тогда спасибо и до встречи.

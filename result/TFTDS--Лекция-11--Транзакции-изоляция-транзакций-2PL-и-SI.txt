Приветствую всех, кто нас слушает сегодня в Zoom. Давайте вспомним, как на второй лекции, в начале семестра,
перед тем, как говорить про задачу репликации кивалиохранилища, мы нарисовали условную архитектуру распределенной базы данных.
Мы сказали, что в самом основании этой архитектуры лежит локальное хранилище.
И эту тему мы уже освоили достаточно подробно. Мы говорили про LSM, ну и сегодня будем говорить про crash consistency,
как на этом уровне переживать отказы диска и какие-то нюансы работы файловых систем.
Поверх локального хранилища мы расположили уровень репликации.
Как, имея, скажем, levelDB или roxDB, в общем LSM, на отдельных машинах реплицировать его так, чтобы пользователи работали с группой машин,
с группы реплик, как с одной отказоустойчивой машиной.
На следующем уровне мы научились справляться с тем, что данные наши в одну машину не помещаются.
Для этого мы их шардировали и разбирались, как именно можно при шардировании избежать узких мест, как их можно преодолеть.
У нас была лекция про exabyte scale файловые системы и key value, и там мы масштабировали преимущественно хранилищами данных.
Вот эти три уровня нами уже освоили, то есть мы можем построить бесконечно большое, горизонтально масштабируемое,
линьоризуемое, отказоустойчивое key value хранилище.
И настал момент подняться на уровень выше, поговорить про транзакции.
Можно подумать, что транзакции это в первую очередь разговоры про базу данных, про таблицы,
про операции, которые трогают много строчек, про гарантии ASET.
Наверное, вы про них слышали.
Наверное, мы интуитивно понимаем, о чем идет речь.
Но сегодня наше понимание транзакций будет гораздо шире.
И для того, чтобы мы это увидели, я хочу, чтобы мы вспомнили несколько примеров,
когда необходимость в транзакциях, в наших задачах уже возникала.
Давайте для начала вспомним систему, которая называлась GFS, Google File System.
В Google File System не было никаких таблиц, никаких строчек, транзакций тоже, разумеется, не было.
Были файлы, и была операция, которая позволяла вам фрагмент файла перезаписать.
Вот у вас есть какой-то большой файл,
и, допустим, вы хотите, давайте это разными цветами нарисуем,
перезаписать какой-то его фрагмент.
Этот фрагмент может быть очень небольшим, не знаю, какие-нибудь мегабайты.
И у вас есть другая запись,
которая пишет в этот же самый диапазон файла, тот же самый мегабайт перезаписывает его на байты B.
Это условно, какие-то байты A, какие-то байты B.
И вот эти две записи конкурируют между собой
и пытаются перезаписать один и тот же диапазон файла.
Давайте вспомним, как GFS обслуживало такие перезаписи.
Перезаписи обслуживались на уровне чанков.
Для каждого чанка GFS среди набора его реплик выбирала реплику primary.
Эта реплика отвечала за то, чтобы принимать конфликтующие записи в чанк и упорядочивать их.
Ну и, собственно, выполнять.
С одной стороны, вроде бы, проблему конкуренции этих двух записей это primary решает.
Но наша беда сегодня в том, что вот эта маленькая запись, этот маленький диапазон файла размером мегабайт,
угодил на границу двух чанков.
И вот этот чанк слева обслуживает...
У нас какие-то проблемы со звуком.
Пожалуйста, выключите микрофон, если он вас включит.
И запись в левой чанк обслуживает какой-то узел primary 1,
а чанк справа обслуживает другой узел primary 2.
Нам сейчас не важно отказа устойчивости.
Я напомню, что в GFS была фундаментальная проблема с тем,
что если primary умирал и назначался новый primary, то согласованности при переходе не было.
То есть в GFS был реализован такой коленочный, не совсем корректный протокол atomic broadcast.
Но нам сейчас это не очень важно, нам важны не отказы, пусть даже никто не отказывает.
Проблема в том, что каждая из записей, и запись A, и запись B, раз они попадают на границу двух чанков,
эти две записи обслуживаются как две.
Каждая из этих двух записей, пусть запись A, красная, обслуживается сразу на двух primary
и упорядочивается независимо, сразу двумя primary.
То же самое касается записи B.
В итоге, каждый primary получает две записи, красную и синюю,
и должен их упорядочить и выполнить в каком-то порядке.
Вот допустим, primary 1 сначала выполнит запись B, потом запись A.
Он имеет на это полное право.
Primary у нас действует независимо друг от друга, просто для того, чтобы снять нагрузку с мастера системы.
И вот это обворачивается тем, что два этих primary для двух чанков упорядочивают две записи разным образом.
В итоге, когда две эти записи завершатся, то в файле вот этот фрагмент размером 1 мегабайт, очень небольшой,
будет выглядеть так. Сначала в нем будут идти символы A, а потом символы B.
Вот никто не умирал, никакие лидеры там primary не перевыбирались, никаких сбоев не было в исполнении.
Но тем не менее, запись даже этого маленького мегабайтного фрагмента файла произошла неатомарно.
Это первый пример. Второй пример связан тоже с файлом.
И здесь нам бы как раз транзакции пригодились. Мы бы хотели атомарно выполнить запись и в первом чанке, в левом чанке и в правом чанке.
Вот, пожалуйста, вполне себе пример транзакции.
Другой пример тоже был связан с файловыми системами, но уже с масштабируемыми и отказоостойчивыми.
Помните, мы сначала заменили GFS мастера на RSM, чтобы перерывать отказы, выделили слой chunk store,
а дальше столкнулись с тем, что мета store, который хранил дерево файловой системы, и iNode не масштабировался.
И для этого мы переложили мета информацию файловой системы в киволюхранилище.
Как мы это сделали? Мы для каждой директории список файлов хранили в виде набора ключей.
У нас была ID-директория и имя файла.
И вот такой служебный ключ указывал на идентификатор файла, по которому можно было найти его список чанков.
А список чанков тоже хранился в виде набора записей в киволюхранилище.
Если бы мы хотели прочесть список чанков файла, то мы бы в киволюхранилище брали snapshot и читали по итератору все ключи,
которые начинаются с данного идентификатора файла.
Чтобы найти идентификатор файла, мы сначала бы выполняли дополнительный лукап, чтобы узнать его ID.
Мы рассматривали такой дизайн на примере файловой системы Facebook, тектоник.
Тектоник свой метод данной раскладывал в киволюхранилище разумным образом так, чтобы все записи,
касающиеся одной директории и все записи, касающиеся одного файла находились в пределах одного таблета,
одного шарда и обслуживались бы от Амарна.
То есть мы можем работать со списком чанков одного файла от Амарна и мы можем работать с одной директорией от Амарна.
Но у нас была проблема. А что если мы хотим выполнить операцию Rename?
Перенести файл из одной директории в другую.
Нам нужно одну запись стереть и другую запись добавить.
Беда в том, что эти записи имеют разные директории ID, а это значит, что могут находиться в разных таблетах, в разных шардах.
И от Амарности между этими таблетами уже нет.
Другая операция, которую мы не можем сделать, это операция конкатинации.
Нам нужно породить новый файл, у которого список чанков будет равен конкатинации списков чанков первого и второго файла.
И снова у нас это не получается сделать, потому что списки чанков для двух исходных файлов лежат потенциально в разных таблетах Key Value.
Нам снова не хватает транзакций, снова не хватает возможности от Амарны с ними работать, с разными таблетами, с разными шардами.
Наконец, третий пример, которого мы еще на самом деле не знаем, но про который я расскажу в параллельном курсе, это очередь сообщений.
Давайте я на примере системы Kafka это расскажу.
Kafka это система, которая позволяет вам надежно хранить потоки данных, хранить потоки в виде очередей,
где сообщения привязаны сквозной нумерацией.
И я в прошлый раз, когда говорил про формальные методы, показывал вам DesignDoc, как в системе Kafka хотели сделать,
ну и сделали, в конце концов, семантику ExactlyOnce.
Что это означает?
Что мы хотим иметь в Kafka ExactlyOnce Processing.
Что это означает?
Пусть у вас есть поток данных, который какой-то клиент вычитывает, каким-то образом обрабатывает,
и после этого записывает измененный результат, какой-то выходной поток данных.
То есть у него есть входная очередь, какой-то обработчик, который запускается на каком-то узле, и выходная очередь,
куда он сохраняет результаты своей обработки.
Беда в том, что у нас две очереди, и чтобы обработка была ExactlyOnce, мы должны атомарно уметь прочесть данные из входной очереди,
передвинуть там курсор на чтение, обработать данные в памяти у себя и записать результат в выходную очередь.
Вот передвинуть курсора во входной очереди и положить новое сообщение, новую запись в выходной очередь.
Это ExactlyOnce Processing, но под капотом, чтобы его реализовать, нам тоже нужны транзакции.
Мы хотим из очереди одно и прочитать, в другую очередь записать.
Тоже пример транзакции, но при этом никаких таблиц у нас в явном виде здесь нет.
Вот три примера, где транзакции естественным образом возникают, и в каждой из этих систем по умолчанию в капке не было транзакций.
В KeyValue, над которым работает файловая система Facebook, нет транзакций.
В GFS в этом месте тоже, конечно, не было транзакций, запись была неатомарная.
Вот все эти примеры демонстрируют вам, что смысл транзакций гораздо шире, чем база данных и гарантия ACID.
Смысл транзакций в следующем. Пусть у вас есть некоторая система, в которой есть какие-то компоненты,
каждый из которых допускает конкурентные операции.
Скажем, мы можем в Чанг писать конкурентно из разных потоков, разные байты, и праймари должны их упорядочивать.
У нас есть KeyValue-хранилище, каждый шарт которого, разумеется, поддерживает конкурентные записи.
У нас есть очереди сообщений в кавке, с которыми мы тоже можем работать конкурентно.
Но при этом под капотом эти компоненты работают последовательно.
То есть праймари упорядочивает внутри себя все записи в один поток.
Каждый таблет в KeyValue-хранилище это отдельный RSM, и если вы пишете RSM в домашней работе,
то вы знаете, что где-то там внутри есть поток, который последовательно вычитывает закоммитченные команды из лога и применяет их к автомату.
В очереди сообщений тоже внутри все упорядочивается, потому что есть сквозная нумерация.
Вот все эти примеры, они про то, что у вас есть объекты с конкурентным доступом, но они устроены последовательно.
Они сами по себе атомарны или линеризуемы.
И мы хотим поверх них уметь выполнять такие операции, которые трогают сразу много объектов,
сразу много чанков, сразу много шардов, сразу много очередей сообщений.
И такие операции, которые мы назовем транзакциями, относительно друга упорядочивались бы как атомарные целые.
Вот транзакции, а не об этом.
Есть ли вопросы?
Наша цель с вами на вот эту тему, на сегодняшнее занятие и занятие через неделю,
разобраться, как устроены транзакции в очень больших распределенных системах,
а именно в Google Bigtable, это кивали ухранилище, который мы обсуждали позапрошлый раз,
в Google Spanner, это вообще в принципе самая большая база данных в мире,
и в ней реализован очень затейливый механизм транзакций.
И мы бы хотели поговорить с вами про Яндекс.ДБ.
Это большая распределенная система в Яндексе, которая служит хранилищем для Яндекс.Облака
и которая реализует довольно радикальный подход к транзакциям.
Очень интересный, мы хотим его тоже обсудить.
Но это наша глобальная цель.
Но вообще-то транзакции не привязаны именно к распределенным системам.
Ну, разумеется, не привязаны к распределенным системам.
И про транзакции можно думать не только на уровне какого-нибудь Spanner или Bigtable,
или на уровне локальной даже базы данных.
Транзакции можно представить себе даже в очень маленьком масштабе.
Вот смотрите, мы говорим, что транзакция это операция, которая атомарно трогает,
атомарно относительно других транзакций трогает сразу много последовательных объектов.
Вот представим себе процессоры память.
В процессоре много ядер.
И они работают с общими ячейками памяти.
И при этом каждая ячейка памяти, она, конечно, позволяет конкурентный доступ.
Вы можете к ней обращаться из разных потоков, при условии, что вы какие-то атомики используете.
Но при этом под капотом в процессоре есть протоколка гириантности,
который говорит, что каждая ячейка памяти, она последовательна.
Все чтения и записи в ней упорядочиваются.
И вот мы бы, возможно, хотели иметь возможность в процессоре
выполнять не просто атомарные операции над отдельными ячейками памяти,
фич-ет, компер-эксченж, подобные операции.
Мы бы хотели иметь атомарные операции, которые трогают сразу много ячеек.
Вот если вы помните конец прошлого семестра, то в самом-самом конце,
в бонусной лекции я вам рассказывал про тему каналы,
про то, как делать примитивы коммуникации, которые позволяют
передавать потоки данных из одного потока в другой или из одного файбера в другой.
И говорил, что вместе с этими каналами хорошо бы иметь селект,
который позволяет из нескольких каналов атомарно дождаться первого значения
и вытащить его.
И когда мы делали селект с гарантией лог-фри, что было особенно сложно,
а так селект реализован в языке Котлин,
то там мы столкнулись с проблемой.
Нужно атомарно потрогать несколько ячеек.
В общем случае нужно потрогать аж три ячеек.
Ну или в простом случае две ячейки.
Нужно передвинуть голову канала, из которой мы достаем значение,
и нужно атомарно взвести какой-то бит в селекторе, что он значение уже забрал.
В общем случае нужно потрогать три ячейки.
И вот тут можно думать про какую-то операцию мультикасс,
можно, имея касс над одним машинным словом, сделать касс,
который работает сразу с несколькими ячейками,
а вместо этого можно на уровне процессора реализовать механизм
железных процессорных транзакций, которые позволят вам
атомарно вот три ячейки памяти потрогать.
Вот такой вот спектр применения транзакций.
Но сегодня мы не будем говорить с вами про распределенные системы даже,
намеренно не будем говорить.
Сегодня наша цель поговорить про базовую теорию транзакций,
про то, как про них вообще можно, про них правильно говорить,
какие слова правильно использовать.
Распределенность, отказоустойчивость, масштабируемость,
все эти вещи нам будут важны, разумеется, но важны будут через неделю.
А пока мы решаем задачу, которая связана именно с concurrent.
Как, имея атомарные объекты и не думая про какие-то отказы,
добиться атомарности на уровне операции сразу над многими объектами.
Ну вот давайте перейдем к постановке задачи.
Примеры мы сотрем.
Хотя ACID я стер напрасно, давайте ACID мы вернем.
Знакомы ли вам аббивиатура ACID, раз уж мы говорим про транзакции?
Вот мы сейчас будем формально ставить задачу и рассуждать про транзакции,
а пока неформальный разговор.
Вот acronym.acid это требование к транзакциям в базах данных.
Как эти буквы расшифровываются? Как это acronym расшифровывается?
ACID это atomicity, C это consistency.
Я сейчас не собираюсь пояснять, что именно каждый слов значит, мы это скоро увидим.
Но сегодня на лекции и на семинаре мы поговорим, кажется, про все эти буквы.
Итак, постановка задачи.
Будем считать, что у нас не база данных, не очереди сообщений,
а у нас абстрактное хранилище,
в котором есть операции,
в котором есть операции записи, значения по ключу и операция чтения по ключу.
Мы считаем, что это хранилище линейализуемое.
То есть оно допускает конкурентный доступ, но при этом любая конкурентная история,
о ней можно думать, как будто бы все операции в ней произошли в некотором порядке
с сохранением порядка операции в конкурентной истории от реального времени.
Под datastore можно понимать самые разные объекты реальности.
Ну, например, можно понимать память в компьютере,
а можно понимать масштабируемое киварю хранилище,
а можно понимать таблицу в базе данных.
Все интерпретации подходят.
Вот мы никакую конкретную не фиксируем сейчас.
Над этим хранилищем мы хотим выполнять транзакции.
Перейдем к ним.
Под транзакциями мы будем понимать такие интерактивные программы,
которые начинаются со служебной операции startTransaction,
за которой следует какое-то количество сетов и гетов.
И в конце концов транзакция завершается двумя способами.
Либо она фиксирует свои результаты,
commitTransaction,
либо она отменяется.
Почему транзакция решила отмениться?
Ну, например, потому что вы в этой транзакции захотели перевести деньги с одного счета на другой,
но запросили для перевода суммы, которая превышает, собственно, ваш баланс.
Видимо, поэтому транзакция не может быть выполнена.
Ну, а если все хорошо, если все варианты сошлись,
то транзакция готова закомититься,
и после комита изменения транзакции должны зафиксироваться надежно в этом хранилище.
Транзакция, вообще говоря, интерактивная.
То есть вы прям пишете программу, где, допустим, вы работаете с определенной системой.
Вы создаете клиента и говорите
client.startTransaction,
client.set,
client.get по какую-то серию
таких операций,
потом в конце говорите client.comitTransaction.
Или, может быть, если вы работаете с традиционной базой данных,
вы пишете какой-то декларативный скорей запрос,
который традиционно что-то делает.
Вот форма представления транзакции нам сейчас не важна.
Мы будем считать, что она вот в общем виде такая программа.
Кто обслуживает
операции в этой программе?
Этим занимается планировщик.
Давайте его перенесем, а то я очень широко пишу.
Обслуживанием транзакции занимается планировщик.
Вот под ним находится хранилище.
А над ним находятся клиенты.
И клиенты в этот планировщик отправляют
свои сеты и геты.
Давайте я переименую сеты и геты.
Я как-то неудачно их выбрал.
Я буду говорить про чтение и записи.
Дальше у нас будет просто чтение и записи.
Например, какой-то клиент отправляет
очередное чтение
в этой транзакции
для какого-то ключа х.
Какой-то клиент отправляет
для своей транзакции
чтение ключа у.
Планировщик принимает
последовательно операции
каждой из транзакций
и должен перенаправлять их в хранилище.
Что?
Да, нотация.
v это запись этой транзакции ключа х.
Значение нам не важно.
Просто запись какого-то значения.
Это чтение житой транзакции по ключу у.
Так вот.
Планировщик получает
для каждой транзакции
последовательно вот такие вот операции,
но при этом для разных транзакций конкурентно, разумеется.
И должен эти
чтение и записи
перенаправлять в хранилище.
Он может делать это прям последовательно.
Может что-то делать параллельно, разумеется.
Наверное, он хочет делать все это параллельно.
И
смотрите,
вот планировщик каким-то образом
планирует транзакции
и на уровне хранилища
что получается? Получается некоторая история.
Запись первой транзакции
по ключу х, запись
второй транзакции по ключу х,
запись третьей транзакции
по ключу у.
Вот планировщик на уровне
хранилища данных
порождает вот такую конкурентную историю
с помощью своего алгоритма планирования.
Мы сказали, что
само хранилище линейризуемое.
А это означает, что
любая конкурентная история
объясняется некоторой последовательной.
То есть
об этом конкурентном исполнении
на уровне хранилища можно думать
как о некотором
последовательном, где, допустим, сначала
выполнилась запись второй транзакции
по ключу х, потом запись первой транзакции
по ключу х, а потом запись
третьей транзакции
по ключу у.
Так вот.
Смотрите.
У нас есть клиенты, которые запускают
транзакции,
отправляют их планировщику.
Планировщик отправляет отдельные
чтения записи на уровне хранилища.
Все операции всех транзакций
выстраиваются в некоторую цепочку
в конце концов.
Вот такую цепочку мы назовем
словом расписание.
Понятно ли, что
планировщик таким образом генерирует
расписание?
Не то чтобы он прям выстраивает
все операции всех транзакций
в некоторую последовательность.
Он
может работать параллельно, то есть он может параллельно
читать и описать в хранилище.
Если, конечно, хранилище, но, разумеется, хранилище
это, наверное, позволяет делать.
Что память, что большое шардирование кивали у хранилища.
Но при этом
можно сказать, что планировщик порождает
именно вот такие,
некоторый класс вот таких вот
последовательных расписаний.
И
наша цель на сегодня
во-первых, подумать
о какие требования
мы предъявляем к планировщику,
то есть какие
расписания ему позволительно
генерируют? Как описать такой класс
хороших расписаний?
Это будет понятие модели изоляции транзакций.
И второй наш
вопрос это то,
как такой планировщик построить, который
бы порождал только расписание
в некотором смысле хорошей из
класс.
Вот давайте подумаем, какие
расписания мы считаем хорошими.
Но есть какие-то очевидные соображения.
Наверное,
расписание, где
все транзакции,
всех транзакций выложены подряд,
в смысле сначала операции там первые транзакции, потом
второй, потом третий,
это расписание очевидно хорошее.
Оно у нас устроило бы.
Вот это то, что называется
расписание.
Это расписание, где
сначала идут
все операции, какой-то транзакции
один, потом все операции, какой-то транзакции
три, потом все операции, транзакции четыре
и так далее.
Определение понятно?
Но, конечно, планировщик
не сможет порождать только такие расписания.
Это было бы очень неразумно,
если бы он так делал,
потому что тогда бы он выполнял по одной транзакции
за раз.
А наше масштабируемое кейвореохранилище
потенциально может обслуживать очень много транзакций
параллельно.
Поэтому наш планировщик будет
действовать сложнее и будет порождать разные
расписания, не только такие.
Вот как
мы могли бы предъявить требования
к такому планировщику?
Чего бы мы могли
потребовать от расписаний,
которые он в конечном итоге
порождает?
Наверное, было бы здорово,
если бы все расписания, которые
планировщик порождал бы,
не были бы отличимы для пользователя
вот от таких серийных расписаний.
Тогда пользователь
мог бы думать о выполнении транзакций
как о транзакциях,
как о атомарных операциях. Сначала
был бы у нас целиком одна, потом целиком другая и так далее.
Для того, чтобы формализовать
эти соображения, введем понятие
view эквивалентности.
Мы скажем, что два расписания
они view эквивалентны,
если все
чтения в этих расписаниях
возвращают одни и те же
значения. И сами
эти расписания со своими записями
переводят хранилища в одно и то же
итоговое состояние.
То есть наблюдатель, который
не видит
то, что делает планировщик,
а наблюдает просто результаты
чтений, которыми планировщик
отвечает,
разницы между двумя расписаниями
не видит. Для наблюдателя
эти два расписания не отличаются.
Хорошо, это определение
эквивалентности, view эквивалентности.
А дальше мы ведем определение
view сериализуемости.
Мы скажем, что расписание
S
view
сериализуемо,
если
S view
эквивалентно
некоторому расписанию
S со звездочкой серийным.
То есть наблюдатель
не может отличить
под конкретное расписание
от некоторого расписания,
в котором все транзакции выполняются просто
подряд.
Видимо, такие расписания
клиента устраивают.
Видимо, такой планировщик клиента
устраивает.
Вот мы определили понятие
view сериализуемости.
Если нам дают
view сериализуемый планировщик,
то пользователь может
не думать о том,
как транзакции под капотом
этого планировщика конкурируют.
Он может считать, что
транзакции как будто бы выполняются
в некотором порядке, как будто бы они
атомарны. Почему это требование
очень удобно? Потому что если вы пишете какую-то
сложную логику, какое-то приложение
поверх базы данных,
и данные ваши вы меняете
с помощью транзакций,
то вам нужно думать лишь о том, чтобы
каждая транзакция сохраняла инварианты
ваших данных
в хранилище. Если каждая отдельная
транзакция переводит базу данных из
корректного состояния в корректное состояние,
то этого
достаточно, чтобы имея
view сериализуемость даже при конкуренции
транзакций всегда
иметь согласованное состояние в базе.
Вот такие простые соображения.
Хорошо.
Маленькое замечание.
View сериализуемость
вообще-то
не требует,
чтобы транзакция,
которая...
View сериализуемость
не требует упорядочивать
транзакции в соответствии
с их порядком в реальном времени.
То есть одна транзакция может завершиться
до старта другой транзакции,
а в порядке сериализации
они выложатся в другом.
Но это и требование
естественно добавить к нашему.
И тогда мы получим то, что называется
strict view serializability.
Вот это то, что называется
моделью изоляции транзакций.
В ACID это
isolation.
Isolation.
Это семантика конкурирующих транзакций.
Strict view serializability
говорит нам, что
если транзакции конкурируют,
то можно думать, что они выполняются
атомарно, при этом сохраняется
порядок предшествования в реальном времени.
И если мы имеем view сериализуемость
для транзакций,
то мы можем обеспечивать
способность наших данных в хранилище.
То есть поддерживать
какие-то наши собственные инварианты.
Вообще говоря, три буквы в этом
акрониме, atomicity, durability
и isolation
это свойства реализации транзакций.
А вот consistency
это буквы, которые добавили только для того,
чтобы получался акроним.
Это не свойства базы данных,
это свойства данных, которые
вы храните в базе,
ваши собственные инварианты,
которые вы можете обеспечить с помощью
остальных трех букв.
Хорошо.
Посмотрим на
экран.
Я вам показывал уже эту картинку,
это диаграмма моделей
согласованности и моделей
изоляции транзакций.
Strip serializer был это корень этой диаграммы
и в левом под дереве
были модели изоляции транзакций.
То есть как мы думаем про конкуренцию транзакций?
В правом под дереве
у нас модели согласованности
для конкурентных объектов.
То есть у нас есть отдельный объект,
например, ящейка памяти
и с ней как-то конкурентно
взаимодействуют.
Вот правое под дерево про
отдельные объекты, левое под дерево
про изоляцию транзакций.
На всякий случай у нас вопросы слышны
потому что обычно вопросы есть,
а тут все молчат.
В этом определении мы не требовали никаких
гарантий относительно расположения
транзакций в реальном времени.
Мы говорили просто, что
планировщик,
который обеспечивает
view serializability, порождает такие
расписания, которые неотличимы
от некоторого серийного расписания.
Но мы не накладывали
никаких ограничений на то, в каком порядке
могут идти вот эти транзакции.
Слово strict означает,
что если у вас была транзакция
T1
и она в реальном времени
предшествовала транзакции
T2,
то вот в этом серийном расписании,
в котором мы готовы объяснить
конкурентное исполнение транзакций,
транзакция T1 тоже должна
предшествовать T2.
Собственно, это то ограничение, которое у нас было в реализуемости.
За это отвечает
слово strict.
Просто серийализуемость никаких гарантий
про конкретный порядок не дает,
но может быть любым.
Вообще в этой диаграмме
слово serializable означает
view serializable, если ты об этом.
Это такой
дефолт. У нас будет сейчас другая
сериализуемость, не view, а мы слово заменим.
Вот оно здесь опущено и имеется в виду view,
то есть неотличимы для наблюдателя.
У меня замечание здесь было такое,
почему вот это
поддерево является поддеревом
сериализуемости?
Потому что мы говорим,
почему линейализуемость здесь выделена
как будто бы частный случай strict
serializability.
Смысл вот в чем.
Если мы возьмем транзакции над базой данных,
вот пусть у нас есть хранилище,
которое умеет операции записи и чтения,
и мы над ним делаем транзакции.
Если мы делали транзакции,
то мы должны объяснить пользователю,
какова симантика конкуренции транзакций.
Для этого мы определяем модель изоляции.
Но в то же время мы
с хранилищем можем работать
как с хранилищем напрямую,
то есть выполнять просто set и get.
И получается, что у нас могут конкурировать транзакции
и точечные операции.
Это довольно неудобно.
Поэтому, если у нас в системе есть
транзакции, то удобнее сказать,
что операции set и get это такие
элементарные транзакции,
состоящие из одной операции.
Так вот, линейализуемость является
частным случаем
strict serializability в том смысле,
что если мы
возьмем все транзакции
и сделаем их вырожденными,
то есть оставим в них только одну
операцию, то гарантия
strict serializability превратится
в линейализуемость.
Вот такие
рассуждения.
Хорошо, возвращаемся на экран.
На доску.
Следующий наш шаг.
Итак, мы хотим,
видимо, построить планировщик,
который обеспечит нам
вот эту вот view-сериализуемость.
Но я бы сказал,
что мы обречены, потому что
view-сериализуемость устроена очень
непонятным образом.
Вот давайте посмотрим
на пример
некоторого расписания
и некоторой работы планировщика.
Пусть у нас было два ключа,
X и Y.
И у нас были три транзакции,
T1, T2, T3.
Вот по вертикали время.
Пусть транзакция T1
записала что-то по ключу
X.
Потом транзакция 2
записала
что-то по ключу
Y.
Потом записала что-то
по ключу X.
А потом первая транзакция записала
что-то по ключу Y.
То есть две транзакции
пишут один и тот же набор ключей,
но в разном порядке.
И мы все эти записи исполнили
вот так вот. Сначала мы исполнили
запись в X первой транзакции,
потом две записи в второй транзакции,
потом запись первой транзакции.
Вот будет ли такое расписание серилизуемым?
Вот расписание серилизуемое...
Оно не будет, потому что у нас два ключа, и вроде бы мы должны...
Если расписание серилизуется, то значит, что выполнилось сначала T1,
сначала T1, потом T2, либо T2, потом T1.
Но в обеих случаях, в обоих случаях мы должны видеть
либо обе записи первой транзакции, либо обе записи второй транзакции.
Вот так что планировщик, который действует таким образом,
он действует немного странно.
Он как будто бы уже серилизуемость нарушил.
Но тут появляется еще одна транзакция,
которая решает записать что-то
по ключу, ну, допустим, X.
А вот такая история будет серилизуемой?
Вот кажется, что вот такое исполнение можно объяснить
таким серийным расписанием,
где сначала выполнилась транзакция T2,
потом выполнилась транзакция T1, а потом выполнилась транзакция T3.
Правда?
Вот это view-серилизуемое расписание.
Но ясно, что никакой планировщик не умеет
предсказывать будущее, поэтому так он работать не может.
Он не может полагаться на то, что появится транзакция T3
и замаскирует вот наш косяк, который мы сделали здесь.
И вообще задача, глядя на серилизуем,
глядя на расписание, то есть на последовательность таких вот операций
чтения и записи из разных транзакций,
определить, серилизуется оно или нет,
вот эта задача NP полная.
То есть класс view-серилизуемых расписаний очень сложно устроен.
Так что мы будем, мы себя упростим задачу.
Мы скажем, что, ну да, вот есть все возможные view-серилизуемые расписания,
но мы не будем пытаться строить планировщика,
который умеет строить произвольные расписания из этого класса.
Мы выделим в этом большом классе расписаний сложно устроенных,
некоторые под класс, расписания, которые устроены просто.
И дальше научим планировщик порождать расписания из этого класса.
Ну а значит они будут по-прежнему хорошими,
потому что они принадлежат классу view-серилизуемых расписаний.
Ну вот давайте научимся это делать.
Давайте подумаем, как можно упростить себе задачу.
Это немного похоже, да.
Сейчас параллелей будет еще больше.
Вот действительно мы какими-то исполнениями готовы пожертвовать,
потому что мы не понимаем, как они устроены.
Вот давайте ведем вспомогательное определение конфликта в расписании.
Вот смотрим на расписание, то есть на цепочку чтений и записей разных транзакций,
как они линеризовались на уровне хранилища.
И скажем, что две операции из двух разных транзакций конфликтуют,
если эти две операции обращаются к одной и той же записи,
к одному и тому же ключу, во-первых.
А во-вторых, по крайней мере, одно из этих обращений – запись.
Ну то есть, например, запись по ключу х из и этой транзакции
конфликтует с чтением житой транзакции по этому же ключу.
Это не конфликтует.
С другой стороны, не конфликтует, например,
две записи из разных транзакций по разным ключам.
Или два чтения одного и того же ключа из разных транзакций.
Вот они тоже не конфликтуют.
Здесь нет, по крайней мере, одной записи, здесь просто ключи разные.
Что нам дает это понятие конфликта?
Вот заметим, что если у нас есть расписание,
и в нем рядом есть две операции,
вот, скажем, давайте вот такая и вот такая,
которые не конфликтуют, то их можно было бы выполнить в другом порядке,
их можно было бы свопнуть,
и при этом для внешнего наблюдателя ничего бы не изменилось.
То есть все чтения вернули бы те же самые результаты,
и конечное состояние базового данного было бы таким же.
Потому что две неконфликтующие операции коммутируют в этом расписании.
С помощью вот такого наблюдения введем понятие конфликтной эквивалентности.
Мы скажем, что расписание S и S' они...
Можете сейчас, пожалуйста, представить конфликт-определение?
Определение конфликта точно такое же, как в моделях памяти в прошлом семестре.
У нас есть два обращения из разных транзакций,
и мы говорим, что они конфликтуют, если эти два обращения обращаются к одному и тому же ключу,
и, по крайней мере, одно из этих обращений – запись.
Смысл конфликтующих операций в том, что неконфликтующие операции можно менять местами.
Соседние неконфликтующие операции, например, два чтения, можно поменять местами.
И при этом ничего не поменяется для пользователя.
Или можно поменять две записи, которые пишут просто по разным ключам.
Ну вот, а теперь можно ввести понятие конфликтной эквивалентности.
Мы скажем, что S и S' эквивалентны, если, ну, очевидно, одно расписание можно получить из другого
серии свопов соседних неконфликтующих операций.
И мы скажем, что расписание S конфликтно-сериализуемо, если S конфликтно-эквивалентно
некоторому серийному расписанию, где все транзакции выполняются подряд.
То есть конфликтная сериализуемость означает, что серий свопов неконфликтующих операций,
стоящих рядом, можно получить вот расписание такого вида.
И вот тут мы получаем под класс расписаний, которые являются конфликтно-сериализуемыми.
Тут довольно много определений возникает по пути, но все они довольно простые.
Еще раз напомню всю конструкцию. У нас есть хранилище данных, над ним есть планировщик.
Планировщик усыпится операцией транзакций, планировщик их направляет в хранилище.
Хранилище линиализуемо, поэтому любая работа планировщика превращается в расписание.
То есть мы все наши конкурирующие деты и сеты каким-то образом линиализуем на уровне хранилища,
и как будто бы планировщик порождает цепочку чтений и записей.
И мы дальше рассуждаем, а какие же вот такие цепочки, какие расписания являются хорошими.
Хорошими являются те, которые неотличимы от таких вот расписаний, где транзакции выполняются подряд.
Но беда в том, что такие хорошие расписания в U-сериализуемой устроены очень сложно,
поэтому мы заменяем их на некоторые под класс, которые также...
Это очевидно под класс, то есть любое конфликтно-сериализуемое расписание является в U-сериализуемым по понятным причинам.
Но при этом я утверждаю, что класс вот таких расписаний устроен очень просто,
что можно легко дать критерий конфликтно-сериализуемости для расписания.
Вот посмотрим на расписание и посмотрим на две его какие-то операции O и O'.
Вот операция O принадлежит транзакции T, операция O' принадлежит транзакции T'.
И вот пусть две эти операции конфликтуют. Что это означает?
Что как бы мы ни свопали не конфликтующие операции, мы не сможем две эти операции O и O' поменять местами.
А это означает, что если мы хотим предъявить сериализацию для этого расписания S,
то в этой потенциальной сериализации T будет обязательно предшествовать T'.
То есть любая пара конфликтующих операций в расписании задает жесткое ограничение
на возможный относительный порядок двух транзакций.
Это вот две конкретные операции конфликтующие.
А если мы возьмем всю совокупность таких пар, то есть всю совокупность таких ограничений,
то мы получим то, что называется граф конфликтов.
Вот мы можем по расписанию построить граф конфликтов,
который устроен следующим образом. У нас кончается место на доске.
Чем бы мы пожертвовали?
А не будем ничем жертвовать. Мы скажем просто, что в этом графе конфликтов множество вершин,
это множество транзакций расписания S,
и между двумя вершинами, то есть двумя транзакциями есть направленная дуга,
когда в расписании есть две операции O' таких, что они конфликтуют,
и O предшествует O'.
То есть мы просто в виде графа конфликтов фиксируем всю систему ограничений
на относительный порядок транзакций, которые в расписании уже заложены.
Определение понятно?
Хорошо, тогда мы теперь с помощью вот этого понятия графа конфликтов
можем легко сформулировать очень простой критерий конфликтной сериализуемости.
Мы скажем, что расписание S конфликтно-сериализуемое тогда и только тогда, когда что?
Но если у нас граф конфликтов задает все возможные ограничения на сериализацию,
когда граф конфликтов для расписания S цикличный,
ну вот давайте это докажем в две стороны.
Сначала в прямую.
То есть пусть у нас S конфликтно-сериализуемое расписание,
то есть серий свопов соседних неконфликтующих операций можно привести S
к расписанию серийному, где все транзакции выложены подряд.
Ну давайте подумаем, как меняется граф при обмене двух соседних неконфликтующих операций?
Что происходит с этим графом?
Ну понятно, что в нем могут появиться и исчезнуть дуги, которые касаются только вот двух транзакций,
которым принадлежат две эти операции.
Но поскольку эти операции не конфликтуют, то значит обмен этих двух соседних неконфликтующих операций
не добавляет некие дуги, не удаляет дуги.
Иначе говоря, граф конфликтов инвариантен относительно свопа этих двух соседних неконфликтующих операций.
Получается, что если мы расписание S можем привести к серийному расписанию S со звездочкой,
то в силу этого свойства графа, графы конфликтов для S и для S со звездочкой одинаковые.
Ну а очевидно, что граф конфликтов для расписания серийного S со звездочкой ацикличный.
Просто вот по его виду, по построению этого графа.
А и графы у этих расписаний одинаковые, ну значит ацикличный и граф для S.
Теперь в обратную сторону.
Пусть у нас граф конфликтов для расписания S ацикличный.
Это означает, что в этом графе есть некоторые вершины,
а вершины некоторые транзакции с нулевой входящей степени.
Вот найдем такую вершину slash транзакцию.
И посмотрим на расписание S. Я пишу под углом почему-то.
Вот у нас где-то есть операция O из этой транзакции T со звездочкой.
Что означает то утверждение, что в T нет входящих дуг.
В T со звездочкой нет входящих дуг.
Любая входящая дуга означает, что перед операцией транзакции есть конфликтующая операция с другой транзакции.
Но вот входящих в T со звездочкой нет.
Это означает, что в расписании S перед любой операции O, с транзакцией T со звездочкой все операции передней не конфликтующие,
не конфликтующие. А это означает, что серии свопов можно передвинуть операцию O
в начало расписания. Таким образом, мы можем построить по расписанию S
расписание S, пусть S0, расписание S1, где в самом начале находится
Т со звездочкой, а дальше что-то. Ну и дальше оторвем эту транзакцию от графа,
то есть займемся стипологической сортировкой. В оставшемся графе снова есть
вершина с нулевой входящей степени, и мы повторим рассуждение для нее.
И вот таким образом мы рано или поздно свопами просто получим расписание S
серийное. Ну вот такой вот простой критерий сериализуемости. Что это нам дает?
Теперь мы можем строить необходимый нам планировщик следующим образом.
Мы должны гарантировать, что если мы гарантируем, что планировщик будет
порождать только такие расписания, что в графах конфликтов для этих
расписаний не будет циклов, то это будет означать, что планировщик
порождает только конфликт на сериализуемые расписания, а значит, только в U
сериализуемые расписания, а значит, пользователь не отличает их от серийных,
то есть пользователь может не думать про конкарнси. Вот наша задача с одной
стороны получить планировщик, который порождает расписания, в которых в этих
графах нет циклов, а с другой стороны этот планировщик по возможности работает
параллельно, то есть он запускает какие-то чтения записи параллельно.
Ну вот давайте рассмотрим дизайн такого планировщика.
Планировщик получает транзакции, он не знает их заранее, то есть планировщик
вот работает и ему прилетает команда start transaction какого-то клиента,
потом прилетает команда первое чтение этого клиента, потом второе чтение,
потом третья запись. Вот он транзакцию все заранее не видит, он работает с ними
как интерактивно. Итак, мы хотим построить планировщик, который порождает только
конфликт на сериализуемые расписания. Такой планировщик будет называться
2PL scheduler, как вам больше нравится. Мы сейчас построим протокол, который
называется двухфазные блокировки. Он очень тупой, очень простой. Итак,
у нас есть хранилище. Давайте свяжем с каждым ключом некоторую блокировку.
Не то, чтобы это мьютекс где-то лежит, но вот такую условную блокировку,
она может быть захвачена, может быть свободна. Когда нам прилетает какая-то
операция транзакции, то чтобы выполнить ее над хранилищем, мы сначала должны
взять блокировку для заданного ключа, к которому мы обращаемся. Давайте я нарисую
жизнь одной транзакции. Вот некоторая транзакция T. Мы получаем некоторую запись
или некоторое чтение, и в первую очередь мы берем лог для ключа. После этого
мы выполняем запись. Лог мы после этого не отпускаем, так и держим. Нам приходит
новое чтение. Мы берем блокировку на ключ, к которому это чтение обращается,
и после этого читаем. Вот у меня здесь горизонтальные стрелки это чтение
записи, вертикальные это блокировки. Вот мы блокировки копим и копим и копим.
В конце концов у нас транзакция завершается, нам прилетает команда commit transaction,
и вот после того как транзакция закоммитчена, то есть мы надежно зафиксировали ее
изменения в какой-то журнал, а про журнал мы сегодня поговорим на второй части занятия,
то мы наконец все наши блокировки отпускаем. Здесь были локи, тут аналоги. Понятна ли идея,
как протокол работает? Да, то есть если мы в транзакции решили что-то прочесть,
то планировщик сначала возьмет блокировку. Пока он ее не получил, он читать не будет.
Ну как, у нас для каждого ключа один лок, если мы им уже владеем, то хорошо,
ничего не делаем, просто сразу читаем или пишем. Но если у нас лока не было еще для данного ключа,
мы сначала должны его взять. Тут единственный аспект это то, что мы локи не отпускаем
до тех пор, пока транзакция не завершится, не закоммитится. После этого мы все локи отпустим.
Так подожди, у нас конфликты конечно могут происходить, то есть разные транзакции могут
читать одни и те же ключи или писать одни и те же ключи. Собственно мы для этого блокировки
и берем, чтобы эти транзакции развести, чтобы одна ждала другую. Про дедлоки поговорим,
но конечно, если ты берешь много локов и не отпускаешь их, то и берешь их в обыкаком порядке,
а ты не контролируешь, в каком порядке ты их берешь, конечно, то дедлок у тебя может возникнуть.
Но давайте начало о хорошем, что если этот алгоритм не зависнет, то он пародит конфликтно-сириализуемое
расписание. То есть он пародит расписание, в графе конфликтов которого не будет циклов.
То есть смотрите, здесь у нас уже параллелизм есть, то есть транзакции, которые читают, которые работают
с разными ключами, у них разные working set, вот они работают параллельно, потому что они берут разные блокировки.
Но если транзакции пересекаются по ключам, то они где-то на какой-то блокировке все-таки затормозятся.
Так что у нас какие-то конкурентные истории, но мы хотим тем не менее доказать, что любая такая история,
если ее линеризовать на уровне хранилища, даст нам конфликтно-сириализуемое расписание.
Для этого мы хотим показать, что в графе конфликтов не будет циклов.
Ну и давайте сделаем этот противного. То есть предположим, что мы запустили такой планировщик,
он пародил какое-то расписание, и в этом расписании в графе конфликтов появился цикл.
Что означает цикл в графе конфликтов? То есть мы породили некоторое расписание,
в графе конфликтов которого есть цикл. Вот давайте посмотрим на каждую дугу в этом цикле.
Посмотрим на первую дугу. Что значит дуга в графе конфликтов?
Что в расписании нашлась операция O1 и операция O2', O11-1 и O2' из T2,
такие что эти операции конфликтуют, и O1 в расписании идет раньше, чем O2'.
Ну почему O1 оказалось в расписании раньше, чем O2'?
Посмотрите, раз они конфликтуют эти две операции, значит они обращаются просто по определению
конфликта к одному и тому же ключу. Чтобы обратиться к одному и тому же ключу,
нужно взять блокировку. Так что и это операция, и это брали одну и ту же блокировку.
И то, что эти две операции упорядочились, означает, что их критические секции упорядочились.
Что в свою очередь означает, что O1, то есть чтение или запись O1, предшествовала
анлоку ключа O1, анлок предшествовал локу ключа, который читал или писал O2.
Ну вот так вот. Такие вот соображения. Они справедливы для каждой дуги.
То есть здесь точно также есть какие-то операции O2 и O3', которые конфликтуют.
И здесь есть операции, ну давайте я напишу их так, O, какое-то KT, и O1',
которые тоже конфликтуют и одно предшествует другому.
И для каждой дуги я могу вот такое соотношение еще извлечь.
Но этого мало, потому что я пока нигде не использовал никакие свойства алгоритма 2PL,
что довольно странно. Вот из первого неравенства, из первой дуги я вывел то,
что анлок для операции O1 предшествовал локу для операции O2'.
А для второй дуги я могу аналогично вывести, что анлок операции O2' предшествует локу
операции O3'. А что еще? Это пока свойство просто цикла.
А где я использовал то, что это расписание построено алгоритмом 2PL?
Алгоритм 2PL есть такое простое свойство, что все анлоки происходят после всех локов.
Поэтому если у нас здесь есть лок в операции O2, у нас есть транзакция O2,
и в одном месте я беру лок для ключа операции O2', а в другом месте делаю анлок для ключа операции O2',
то просто по построению алгоритма 2PL вот этот лок должен предшествовать вот этому анлоку.
Ну а дальше я могу получить такое длинное длинное неравенство, длинную такую цепочку предшествований
и получить, что вот как у меня будет выглядеть последнее соотношение?
Анлок ОК предшествует локу O1'. И вот я начинаю с анлока операции O1,
и получается, что некоторая операция О1 из транзакции T1 предшествует локу
некоторой другой операции из той же транзакции T1. Вот, а это нарушение протокола 2PL.
В нем любой лок предшествует любому анлоку, а я получил наоборот.
Это означает, что для любого расписания, который породил такой вот планировщик,
в графе конфликтов этого расписания не может быть циклов.
А это означает по критерию, что такое расписание будет конфликтно сериализуемым.
Это в свою очередь означает, что такое расписание будет U-сериализуемым.
А это по определению сериализуемости означает, что пользователь не отличает это расписание
от расписания, где все происходило просто подряд.
Таким образом, 2PL гарантирует нам сериализуемость. Вот такой вот тупой протокол.
Правда-беда. Мы в этом протоколе берем много блокировок и не отпускаем их.
И порядок блокировок выбираем не мы, а пользователи, которые выполняют свои операции.
Можно получить это так, что один пользователь возьмет сначала лок на ключ X,
а сначала запишет в X, потом в Y, захочет записать в X, потом в Y.
А другой захочет записать сначала в Y, потом в X.
И вот мы возьмем две блокировки в разном порядке.
Точнее, попытаемся это сделать и получим дедлок.
Нам нужен механизм, который нас от дедлока в этом планировщике защитит.
Предлагается делать двумя способами.
Когда каждая транзакция стартует, давайте она выберет себе временную метку.
Совершенно произвольным образом. Посмотрит на локальные часы.
Нам монотонности никакой не нужны, никакие сильные свойства не нужны.
Просто вот примерное текущее время.
Если у нас есть две транзакции и у одной временной метки меньше, чем у другой,
то, видимо, первая старше, чем вторая. Нам вот таких соображений общих хватит.
А теперь как мы берем локи?
Мы в планировщик получаем операцию какой-то транзакции.
Эта операция хочет взять лог. Мы пытаемся его взять.
Два варианта. Если он свободен, то продолжаем. Захватываем и продолжаем.
Если он оказался занят, то мы сравниваем таймстэмп нашей транзакции,
ее временную метку, с временной меткой транзакции, которая владела локом.
И тут есть две стратегии. Одна называется ваундвейт, другая называется вейтдай.
Они симметричны. Давайте со второй начнем.
Или давайте с этой. Не знаю, какая мне больше нравится. Давайте с этой.
Если мы транзакция, пытаемся взять лог, который владеет другая транзакция,
то мы сравниваем наши таймстэмпы. Если наш таймстэмп меньше, чем у транзакции,
которая владеет локом, то мы считаем, что мы можем ее пооборотить.
И мы эту транзакцию отменяем, мы ее приемтим. А если мы видим, что локом владеет транзакция,
которая старше, чем наша, то мы ждем. Пока этот лог не отпустит.
Если младше, то мы ее отменяем. А если она старше, то есть у нее таймстэмп меньше,
чем у нас, то мы ее ждем. Здесь мы делаем симметрично.
Если у нас таймстэмп меньше, чем у транзакции, которая владеет локом,
то мы ждем. А если у нас таймстэмп больше, то мы сами оборотимся.
Давайте зафиксируем первый способ, чтобы не путаться. И я объясню, почему он гарантирует нам
что-то хорошее. То есть, если мы видим транзакцию с младшим таймстэмпом,
которая уже владеет нужным локом, то мы ее приемтим. А если она старше нас, то мы ее ждем.
Во-первых, почему с такой стратегией не будет циклов, почему не будет дедлоков?
Вот если вы помните прошлый семестр, то мы вроде бы выяснили так с трудом,
что дедлок — это цикл в графе ожидания, где мы рисуем дуги из потока, который ждет,
в поток, которого этот поток ждет через Mutex. Так вот, в такой стратегии мы ждем,
только если у нас таймстэмпы монотонные. То есть у нас более молодая транзакция
ждет более старую транзакцию. Так что цикл в графе ожидания быть не может.
С другой стороны, почему есть прогресс? Потому что рано или поздно,
то есть отдельные транзакции могут приемтить, она будет перезапускаться,
пробовать снова. Но когда транзакция перезапускается, то она свой таймстэмп сохраняет.
И рано или поздно любая транзакция становится самой старой,
потому что все новые транзакции получают в качестве метки текущее время.
И вот самую старую транзакцию уже никто не способен приемтить.
Поэтому рано или поздно она выполнится.
Ну что, идея понятна?
Тут есть еще много нюансов, на самом деле, которые нужно обсудить.
Ну скажем, брать эксклюзивную блокировку и на чтении, и на запись неэффективно.
То ли можно для записи брать эксклюзивную блокировку,
а для чтения брать разделяемую блокировку, чтобы если две транзакции
читают один и тот же ключ, то они могли бы делать это параллельно.
Вот легко понять, что такие изменения в алгоритме не ломают вот эти рассуждения,
потому что блокировки для конфликтующих операций тоже будут конфликтовать,
поэтому все равно секции не будут пересекаться.
Но при этом мы получим чуть больше параллелизма.
Да, вот то, что мы описали, называется на самом деле не просто 2PL, а strict 2PL.
Вот он как раз гарантирует, что расписание будет не просто view-серилизуемым,
оно будет strict-серилизуемым. То есть оно уважает порядок предшествования транзакций в реальном времени.
Вообще теория транзакции, она бесконечно большая, и нам ее конечно не покрыть.
Вот есть отдельная книжка, где тысяча страниц примерно по транзакции,
и серилизуемость это только одна половина, там еще вторая, называется recoverability.
Но нам это сейчас не очень важно. Вообще почему мы изучаем 2PL?
Это очень тупой протокол, но оказывается, что в Google Spanner применяется в том числе он.
Но для того, чтобы разобраться, как именно он применяется, нужно потрудиться.
На самом деле в Spanner не только 2PL, конечно, там есть еще другой протокол,
и я потрачу еще, наверное, минут 15, чтобы о нем коротко рассказать.
Но перед этим небольшое замечание, как вот этот протокол транзакций можно совершенно неожиданным
и очень простым, очень элегантным способом перенести на память в процессоре.
То есть как сделать транзакции на уровне процессора и ядер?
Вот давайте подумаем. Мы процессор, и мы хотим автомарно работать с несколькими ячейками памяти.
Вот этот протокол требует от нас брать блокировки.
Эксклюзивные блокировки на запись ячейки памяти и разделяемые блокировки на чтение,
и блокировки накапливать. Вот как же нам на уровне процессора,
вот прямо внутри процессора такие блокировки реализовать?
Эксклюзивный и разделяемый на ячейки памяти.
Вот оказывается, что в процессоре уже есть, уже реализованы все эти механизмы блокировок на самом деле.
Вот если вы помните, у нас была тема про протокол гениальности кашей.
Давайте мы посмотрим на экран.
Протокол гениальности мы можем прочесть какую-то ячейку памяти,
и у нас эта ячейка оказывается в состоянии, ну ладно, сложно, тут эксклюзив, shared.
Вспомним, у нас был такой протокол, который назывался MSI, протокол к гениальности.
Мы говорили, что у каждого ядра процессора есть свой собственный кэш,
и в этом кэше хранятся блоки памяти, мы называли их кэшлиниями.
Разумеется, разные ядра могут в общей ячейке памяти писать и в общей ячейке читать.
Так вот, чтобы записать что-то в ячейку памяти, вы должны сначала захватить эксклюзивное владение над соответствующей кэшлиней.
Это состояние modified было.
Оно означало, что кэшлиня с этой ячейкой находится только в вашем кэше, а в других кэшах ее инвалидировали, ее сбросили.
Если же вы читаете ячейку памяти, то вы должны снова пройти через кэш и получить эту кэшлинию с этой ячейкой в свой кэш в состоянии shared.
Состояние shared означает, что есть другие shared кэшлини в других кэшах, в других ядер, но при этом нет ни одного modified.
Вот modified и shared это же и есть по сути эксклюзивное владение блоком памяти и разделяемое владение блоком памяти.
А сам протокол когерентности обеспечивает обнаружение конфликтов.
То есть, если вы хотите в ячейку памяти что-то записать, то вы должны сначала инвалидировать все другие modified и shared в других кэшах.
Так вот, как теперь поверх протокола когерентности в кэшах, который уже по сути реализует нам блокировки и обнаружение конфликтов, сделать транзакции?
Когда мы начинаем транзакцию в процессоре, то все наши чтения и записи будут оставаться в нашем кэше.
И мы к этим трем состояниям, к трем альтернативам добавим еще некоторый флажок T – транзакционность.
Когда мы что-то пишем под транзакцией, мы пишем в кэш и получаем кэшлинию в состоянии modified, то есть она у нас только в кэше есть, плюс она транзакционная.
Когда мы что-то читаем в транзакции, то мы получаем кэшлинию в состоянии shared и ставим на ней тоже флажок транзакционная.
И если вдруг какое-то другое ядро решило записать что-то в ту же ячейку, что читали и писали мы, то протокол когерентности должен пойти и, скажем, все modified или все shared для этой ячейки сбросить в других кэшах.
И когда мы получаем от протокола когерентности уведомления, что нашу кэшлинию, например, вот эту какую-то, пишет другое ядро, а у нас это кэшлиния была под транзакцией, то что мы делаем?
Мы просто все кэшлинии, которые у нас были помечены флажком T выбрасываем из своего кэша, не только вот конкретную кэшлинию, с которой у нас случился конфликт, а и все другие.
Вот ощущаете, я не знаю, помните ли вы хорошо протокол когерентности, может быть не помните, но он слишком прост, чтобы его заново в умении придумать.
И вот на основе этого очень простого протокола когерентности и этой очень простой надстройки мы можем внезапно получить транзакции на уровне процессора.
Чтобы закоммитить транзакцию, мы просто стираем флажок T. Вот это операция коммита. Все, после этого протокол когерентности гарантирует, что все наши изменения падут в память.
Это же удивительно простая идея, удивительно изящная идея. Как воспользоваться тем, что уже в процессоре есть, чтобы в него принести транзакции?
И более того, вот такой механизм, он уже в современных x86 реализован, и он позволяет вам делать очень интересные вещи.
Например, этот механизм используется в библиотеке Питредс. То есть, если вы под линуксом пишете и используете 100D Mutex, то вы в качестве реализации Mutex используете Питредный Mutex.
А в этом Питредном Mutex используется, если ваш процессор поддерживает железные транзакции, поддерживает SLOG.REGION, стирание блокировок.
Вот, предположим, вы пишете хэштаблицу, и у вас есть две записи в хэштаблице. И вы говорите, ну я не хочу думать на еженом таблице Mutex, чтобы эти записи могли работать, чтобы параллели на сети никакой не было,
но при этом можно было бы работать с хэштаблицей из разных потоков. Как теперь устроен этот Mutex? Вот два потока приходят к нему. Обычно один из них блокировку захватывает, другой ждет.
Вместо этого две вставки, которые пришли в одну и ту же хэштаблицу, начинают транзакцию. Они говорят, вызывают специальную инструкцию, которая называется XBGIN.
Начинают транзакцию и проверяют, просто читают, что лог в свободном состоянии. Если лог в свободном состоянии, то они добавили его в свой рецепт.
То есть у них в кышах сейчас ячейка с флажком лока находится в виде кыш-линии в состоянии shared с флажком транзакционное. И все, и дальше две критические секции ваших двух вставок выполняются параллельно.
То есть у вас Mutex, в котором критические секции выполняются параллельно. И если две эти критические секции внезапно работают с разными бакетами хэштаблицы и не пересекаются, не ловят конфликты на уровне кыша, то две эти транзакции коммитятся параллельно.
И получается, что у вас есть STD Mutex, но две критические секции в нем выполнены физически параллельно. Но просто потому что два пилета позволяет, и протокол к игретности конфликтов не обнаружил на блокировках, на виртуальных блокировках.
Если же вдруг какие-то операции затрагивали одни и те же бакеты, ну или просто на уровне к игретности вы получили коллизию, то транзакция пооборотится. Но в этом случае вы откатитесь и попробуете уже честно взять Mutex.
С этим механизмом есть проблемы, потому что он не гарантирует прогресса. Т.е. транзакция может переполнить кэш, если он очень больший, и отменить сама себя.
Или просто получить конфликт между своими же записями на уровне кэша, что довольно глупо, и пооборотится. Но тем не менее, хоть железные транзакции не могут гарантировать прогресс, они могут служить полезной оптимизацией.
И по сути эти транзакции реализованы на основе двух наблюдений, что у нас есть два ПЛ с блокировками, и вот эти блокировки по сути и конфликт на этих блокировках уже реализованы в протоколе к игретности процессора.
Мы лишь немного его усложняем. Реальность, конечно, намного сложнее, чем я сейчас рассказываю, но все же вы вполне можете пользоваться вот такими параллельными Mutex, параллельными критическими секциями.
Ладно, это первая часть. Теперь я коротко, насколько у меня силы позволит, расскажу вам про альтернативный механизм реализации планировщика.
Почему мы вообще хотим чего-то альтернативного, когда мы построили уже два ПЛ, которые гарантируют нам сериализуемость?
Дело в том, что планировщик два ПЛ очень пессимистичный. Он для каждого чтения и для каждой записи берет блокировку, чтобы две транзакции друг от друга изолировать.
Можно делать чуть оптимистичнее. Почему это может быть целью? Вот представим себе, что у нас транзакции, что у нас система, в которой выполняются транзакции, это какое-то огромное распределенное хранилище.
И в этом хранилище у нас есть какие-то точечные модификации, а есть какие-то огромные чтения, где мы читаем много-много данных. То есть пусть у нас распределенная база данных, в ней есть гигантские таблицы, и некоторые таблицы настолько большие, что мы читаем их с помощью reproduce операции.
Вот настолько они большие. И при этом мы хотим, чтобы пока мы читаем таблицу, мы получали ее в согласованном состоянии. Так вот, если мы залочим всю таблицу на чтение, пока мы будем ее читать, то никто не сможет ничего делать больше с этой таблицей.
Это неприятно. Мы бы хотели, чтобы в нашей базе, в нашем хранилище читающие транзакции могли бы работать физически параллельно вместе с пишущими, и они бы друг друга не блокировали.
Вот в 2PL у вас для этого есть конфликтующие блокировки на чтение, на запись. Они друг друга исключают. Читающие транзакции исключают пишущие, и наоборот, на одних и тех же ключах. Мы бы хотели, чтобы читающие транзакции могли работать параллельно с пишущими.
Для этого мы воспользуемся альтернативным подходом, который называется изоряция снайпшотов.
Тут нужно вспомнить вашу домашнюю работу, где вы делали реплицированное хранилище отображений с ключей и значений. У вас там была задача.
Каждая реплика хранила простое наблюдение. Как сделать так, чтобы читающие транзакции могли работать одновременно с пишущими?
Если пишущие транзакции меняют состояние хранилища, а читающие должны читать старые, то, видимо, хранилище должно быть мультиверсионное.
То есть нам мало просто операций записи по ключу чтения ключа. Нам нужно, чтобы в нашем хранилище могли лежать разные версии в один и тот же момент времени.
И вот такую задачу вы уже решали в первой домашней работе, как это сделать. Пока я буду просто считать, что у меня есть хранилище, которое поддерживает версии.
Я начинаю с некоторой версии S0, с пустого хранилища. И каждая транзакция, которая в нашем хранилище что-то меняет, будет порождать новую версию.
Вот это просто отдельные версии. И каждая версия будет адресована некоторой временной меткой.
Каждая версия является иммутабельной. То есть если мы коммитим какую-то транзакцию, вот здесь был коммит, то в нашем хранилище порождается новая иммутабельная версия.
Ну разумеется, мы не пытаемся копировать все хранилище. Разумеется, новая версия, объем, который приносит новая версия, пропорциональна количеству ключей, которые мы там перезаписали.
Но тем не менее, конструкция такая. Теперь, как выполняются транзакции? У нас есть мульти, мульти-восточная хранилища.
Как выполняются транзакции? Каждая транзакция, когда она стартует, выбирает себе временную метку для чтения. То есть фиксирует ту версию хранилища, относительно которого будут выполняться все чтения.
И дальше ее читает. Вот реализация этой конструкции должна быть такой, чтобы вот каждая версия уже больше не менялась, чтобы она фиксировалась. Новые транзакции пишущие могут продать новую версию.
Но читающие транзакции будут читать, например, вот эту. Вот мы выбрали в качестве референса вот такую версию S1. Read timestamp указывает на нее.
А дальше, когда мы выполняем все свои записи, то мы не то чтобы в хранилище прям их пишем. Мы их буферизуем, просто запоминаем из себя, что мы сделали запись по ключу.
Какую-то запись В1, запись В2, запись В3. И в какой-то момент мы решаем это вот. Сказал, этот шаг start transaction, раньше у нас его не было, теперь он есть и он выбирает временную метку для чтения.
То есть версию, которую мы будем читать. Потом мы выполняем какие-то записи, накапливаем их и хотим сделать commit.
То есть мы хотим породить новую версию. Мы каким-то образом атомарно помещаем все наши записи в хранилище и порождаем версию 3.
Вот версия 3 это commit timestamp.
Когда мы комитим транзакцию, мы выбираем себе вот такую временную метку и под ней порождаем новую версию хранилища. То есть под ней пишем все свои записи.
Но с нами могла конкурировать другая транзакция. Скажем, вот здесь началась какая-то другая транзакция, которая сделала запись W1, W2, W3. И она тоже хочет закомититься.
Давайте такое пунктирное S4. Так вот, commit может быть успешен, а может быть неуспешен. Мы скажем, что чтобы закомитить все свои записи, чтобы породить новую версию,
мы должны убедиться, что вот начиная с этой версии, с которой мы читали, не было изменений по тем же ключам, что мы пишем.
Если окажется, что, например, запись W3' и W2 конфликтуют, то есть они обращаются к одному и тому же ключу, то это значит, что commit транзакции, это была транзакция, допустим, T1, это транзакция T, это T'.
Вот чтобы штрихованные транзакции не комитятся, то есть ее commit проваливается. Вот это правило называется First Committer Wins.
Идея супер простая. Вот вы ее все хорошо знаете, потому что вам знакома система контроля версий. Вот start транзакции, это вы буквально отщепляете ветку.
Дальше вы в своей ветке, локально, никому не говоря об этом, делаете какие-то изменения. Вы их накопили и потом решили влить их обратно в мастер.
Но вы вливаете в мастер успешно только тогда, когда ваши изменения не конфликтуют с теми изменениями, которые за время жизни вашей ветки накопились уже в мастере.
Вот если конфликтов нет, то изменения вливаются. Если конфликты есть, то изменения откатываются.
Вот такая простая схема. Тут очень много вопросов, как именно это реализовать, и сегодня мы не будем им задаваться.
Ну, например, как именно сделать версионируемое хранилище? Как именно я порождаю новые версии? Как именно они там хранятся?
Как именно я выбираю временные метки? Потому что они должны генерироваться монотонно, это очень важно.
Монотонно в смысле, вот в реальном времени монотонно. То есть если одна транзакция закоммитировалась раньше, чем другая началась,
то временная метка у новой транзакции на чтение должна быть строго больше, чем временная метка на запись у старой транзакции.
В распределенном смысле?
Ну, поначалу в локальном, вообще распределенном, конечно.
Ну и наконец, как мы можем взять и атомарно записать в хранилище сразу много ключей? Это тоже не очень понятно.
Потому что это как бы проект транзакции в каком-то смысле и были.
Ну скажем, в levelDB мы можем атомарно записать много ключей разом, а вот в распределенной кивали у хранилищ в разной шарды мы не можем уже записать.
Но вот эти все вопросы нужно решить, и мы про них поговорим в следующий раз.
А пока вопрос, потеряли ли мы что-нибудь здесь относительно 2PL?
Вот у нас было 2PL, и в нем была параллельность, но на одних и тех же ключах параллельности не было. Там были локи.
Здесь мы от локов ушли, и чтение делаем без блокировок, потому что мы перешли к мультиверсионному хранилищу.
Собственно, в домашке вы так и делали. Вы хотели избавиться от блокировок и перешли к мультиверсионному хранилищу.
Вот здесь та же самая идея. В смысле транзакций, потеряли ли мы что-нибудь?
Вот например, верно ли, что этот мастер, цепочка состояний, каждый из которых соответствует новому комиту транзакций, это есть сериализация?
Верно ли, что порядок комитов здесь, это порядок сериализации транзакций?
Давай нарисуем пример. Наверное, ты про это же говоришь. У примера есть собственное название.
Он называется rights queue. Транзакция T1 и T2. Транзакция T1 делает следующее. Она читает ключ X, и если в ключе X видит единицу, то пишет в Y 0.
Транзакция T2 читает Y, и если в Y видит единицу, то пишет в X 0. Вот две эти транзакции могут конфликтовать только по своим rights set, то есть по множеству ключей, которые они пишут.
Потому что вот только так обнаруживаются конфликты по записям. И смотрите, что может произойти. У нас было некоторое начальное состояние S0, мы от него отщепили T1, T2.
T1 сделал запись Y 0, T2 сделал запись X 0, и после этого эти две транзакции в каком-то порядке закомитились. Конфликтов между ними не было.
Но в итоге у вас... Давайте я пример перерисую, потому что немного тупо. У меня начальное состояние, наверное, все нули. Вот так вот.
И вот я перешел в состояние, где все нули, в состояние, где в обеих транзакциях единица. На состояние, где в обеих ключах единица. Такое состояние не отвечает никакому порядку сериализации.
Если выполнялось бы сначала T1, потом T2, то T2 провалилось. Если бы выполнялось наоборот, то провалилось бы T1. Поэтому одна из транзакций не должна оставить следов, а они обе оставили.
И вот на уровне изоляции никакого конфликта при этом нет. Что довольно неприятно. Вот то, что получилось, называется словом аномалия.
И тут есть достаточно неловкая ситуация. Давайте посмотрим на экран. Вообще базы данных существуют гораздо раньше, чем была придумана изоляция снайпшотов. Вот этот подход.
И так исторически сложилось, что комитет по стандартизации придумал уровни изоляции транзакций. Но вот на этой картинке они тоже есть.
Если у вас был курс по базам данных, возможно, вы что-то такое проходили. И вот гарантии в этих уровнях сформулированы очень странным образом.
Гарантии сформулированы относительно проблем, которых не бывает. И вышло так, что снайпшот изоляция исключает и вот такой вот проблем, и вот такой вот проблем, и такой вот проблем.
То есть по принципу исключения она исключает все аномалии, которые были стандартизированы, поэтому является стерилизуемым.
Стерилизуем по логике. По логике определения. Но при этом нет. Существует сценарий новый, которым этот подход дает нам нестерилизуемое расписание.
Так что, когда вы читаете, скажем, в документации про базу данных слово serializable, то подумайте, что оно значит.
Оно значит это безумное определение из древнего стандарта или оно значит serializability в математическом смысле.
Вот кажется, что любая современная система использует serializable именно в математическом смысле.
Что существуют серийные расписания, такое, что оно view эквивалентно нашему.
Но вот тем не менее для изоляции снайпшотов возможны вот такие странные сценарии. С другой стороны, мы можем читать без блокировок.
Давайте я все-таки кое-что успею сказать напоследок про реализацию. Как выбирать таймстемпы, обсудим в следующий раз.
Как делать коммит атомарно и выполнять такую проверку сложную, обсудим тоже в следующий раз.
А вот по поводу того, как сделать мультиверсионное хранилище.
Вот пусть у нас уже есть хранилище, но которое не мультиверсионное. В нем есть операция put по ключевому значению и операция get.
Ну и может быть еще есть снайпшоты, плюс итераторы.
Вроде бы такое API предполагает, что когда вы мутируете состояние базы данных, то вы его перезаписываете, ставлю ключ на новый.
Но если вы делаете снайпшоты, то вы можете получить снайпшоты на просто текущей версии.
Собственно levelDB вам такое API предоставляет, мы это помним. Давайте на экране покажу еще раз.
Вот API levelDB, вы можете взять снайпшот и по нему итерироваться, и другие потоки могут в это же время базу данных менять, но вы эти изменения не увидите.
Но вы не хотите их видеть, вы хотите иметь фиксированный снайпшот.
Так вот, если ваша база данных, ваше хранилище локальное умеет вот такие операции, то вы поверх него можете сделать мультиверсионное хранилище довольно несложным образом.
Вы можете сказать, что put. Собственно, что значит мультиверсионное хранилище?
Это означает, что у вас есть операция put, ключ k, значение v, временная метка ts.
И у вас есть операция get, прочесть по ключу k максимальную версию не старше, чем временная метка ts bound.
И когда я говорю, что в снайпшот-изоляции мы порождаем новую версию базы, разумеется, мы не то, что мы порождаем новую версию.
Вот у нас было хранилище, где были ключи x и y, и под версией 1 в ключе x хранилось буква a, а в ключе y хранилось под версией 2 буква b.
И мы делаем commit двух ключей x и y под timestamp 5. Пишем сюда какой-нибудь а штрих, b штрих.
Вот мы просто добавляем по ключу k значение с новой временной меткой 5.
А при этом, если другая транзакция читает что-то, то она читает из снайпшота с помощью такого чтения.
Вот мы, например, делаем get по ключу x с ограничением на временную метку ts bound 3.
Вот мы фактически читаем вот такой снайпшот. Мы читаем по ключу x значение a, по ключу y значение b.
Вот имея хранилище с такой семантикой, вы можете сделать операции вот с такой семантикой.
Тут, конечно, нужна аккуратность, как именно вы в это хранилище атомарно записываете сразу много ключей, но это отдельная история.
Но, в принципе, вы в домашней работе уже вот такое преобразование делали.
Вы, конечно, можете сделать его не только в локальном масштабе, а на уровне распределенного киварю хранилища.
Так вот, нужно подвести итог, на чем-то остановиться. Мы, конечно, не разобрали, как устроен снайпшот isolation на уровне реализации.
Но мы, в принципе, понимаем, как схема работает, с какими гарантиями.
Мы понимаем, что гарантии у этого подхода слабее, чем у 2PL. Но у каждого из этих подходов есть свое преимущество.
2PL гарантирует строгую сериализуемость, но это пессимистичный подход.
В нем читающие и пишущие транзакции, обращающиеся к одним и тем же ключам, не могут работать параллельно.
Здесь могут, но ценой потери сериализуемости.
Вот в следующий раз мы посмотрим на три примера. Мы посмотрим на Google Bigtable.
И как Google для своего индексатора интернета, поверх этой системы без транзакций, построил транзакции, реализованные на клиенте вообще снаружи системы,
с помощью вот такого подхода. Им таких гарантий хватило, которые дают изоляция снайпшотов.
А еще мы посмотрим на систему Google Spanner.
И увидим, что Спандеру хочется уметь чтения и снайпшотов.
Но при этом им хочется еще гарантировать и сериализуемость.
Так вот, Spanner умудряется совместить два этих подхода, как мы в следующий раз увидим.
То есть нам полезно знать и про этот подход, который более эффективный, но обладает слабыми гарантиями,
и про этот подход, который более ограничительный для реализации, но при этом дает более сильные гарантии.
И если правильно их скомбинировать, то мы получим что-то такое же эффективное, как Snapshot Isolation,
и в то же время что-то настолько же корректное, как и 2PL.
И вот наконец мы с вами способны будем прочесть статью про Google Spanner.
Собственно, это моя просьба к следующему занятию.
Мы с вами уже знаем про Google TrueTime и умеем им пользоваться для генерации монотонных временных меток.
Это к вопросу о том, как генерировать временные метки для транзакций в этом подходе.
Мы с вами знаем про репликацию через Multipax, если вы сейчас это пишете.
Мы знаем про распределенную файловую систему Google, это Colossus.
Мы знаем про два подхода к изоляции транзакций, 2PL и изоляция Snapshots.
И вот это все преликвидиты, необходимые для того, чтобы статью про Spanner почитать теперь.
Потому что в этой статье, написанной в 2013 году уже достаточно давно,
от читателя понимание всех этих вещей требуется.
И в предположении, что читатель это все понимает по-настоящему,
авторы статьи могут ему рассказать, как именно в их системе реализованы транзакции,
как именно они сочетают два этих подхода.
В следующий раз мы про все это поговорим, а еще про альтернативный подход,
который используется в Яндексе, транзакции типа Kelvin.
И вот тогда уже через неделю мы обсудим все нюансы реализации именно распределенных транзакций.
Ну что ж, а на сегодня тогда все. Спасибо большое, что задержались.

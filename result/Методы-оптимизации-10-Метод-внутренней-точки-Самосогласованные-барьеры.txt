Сегодня продолжаем говорить про задачи с ограничениями, ну и соответственно смотрим
на новый метод, новый подход к этим задачам. Целевая функция минимизируем на всем пространстве,
но теперь у нас есть соответственно ограничение типа неравенства. Ограничение типа неравенства.
Что хочется сделать? Напоминаю, что у нас происходило в прошлый раз. Брали штрафную функцию и к
нашей целевой функции просто добавляли вот такого штрафа, вот такого вида. Ну и соответственно да,
что с ним происходило? Когда у нас функция g была меньше нуля, в некотором смысле он активировался и
вносил какую-то поправку в зависимости от размера параметра ro. Ну и соответственно если когда у
нас функция была больше нуля, штраф не активировался в связи с того, что мы используем вот такую вот
срезку. Один из подходов, мы с вами уже поняли в прошлый раз, что в принципе неплохой подход,
единственное, что нужно увеличивать параметр ro, в связи с этим растет число обусловленности задачи,
растет константа липшица, но при этом какие-то свойства в духе сильной выпуклости не меняются,
поэтому сложность задачи от этого увеличивается. Плюс соответственно проблема здесь в том,
что решение, которое вы получаете, оно может не удовлетворять ограничениям и чаще всего так и есть.
Так, ну и у меня к вам вопрос, а как нам тогда ввести штраф, чтобы обязательно удовлетворять
ограничениям и вот лежать в пределах вот этого множества, которое и этими функциями и вводится.
Как можно ввести так, чтобы вроде как уводить что-то на бесконечность, но в нужный момент оставлять
это в нуле? Какие-нибудь идеи? Как можно это сделать? Ну, топорный вариант сделать вот так. Мы
вводим просто индикаторную функцию, которая и задает наше множество. Смысл этой функции в том,
что она равна нулю на наше множество, которое задано ограничениями, и равна плюс бесконечности
вне этого множества. Получаем что ли вот такую вот стену, которая за пределами множества говорит о том,
что нужно просто возвращаться обратно, потому что значение целевой функции становится равным
плюс бесконечности. Ну а в пределах нашего множества эта функция просто ноль, и она не вносит никакого
вклада именно со саму минимизацию. Ну вот такое вот ограничение. С точки зрения каких-то формальных
вещей, это ограничение на самом деле очень хорошее, потому что задача не меняется, и кажется,
что более чем нормально, но какие вы видите у такого ограничения проблемы? Такого вида штрафа. Да,
правда, он особо не помогает на практике, и в теории вы ничего про него хорошего этой тоже не скажете,
потому что он не дифференцируем на границе, как раз в самой интересной области, у него просто
не существует производный. Ну и как с ним жить, непонятно в теории. Ну в теории с ним можно только
жить, когда у вас множество уже просто, и вы можете делать проекцию на него. Ну с этим мы с вами уже
разбирались. Сейчас у нас хочется окунуться в варианты, когда каких-то проекций или простых
решений, линиейных задач, вы их сделать не можете. Поэтому да, вот такого вида штрафа, ну он конечно
имеет место, но скорее как какая-то теоретическая игрушка и для простых множеств g. Хорошо. Так, ну и
возникает идея, отталкиваясь от этого штрафа, вот такого вида, воспроизвести его, переняв от
него хорошие свойства, но как-то поборясь с теми проблемами, которые мы с вами озвучили. То есть
хочется, чтобы вне множества g штраф уходил на бесконечность, и соответственно в пределах множества
происходило что-то хотя бы около нулевое. Ну и пожалуйста, можно попробовать... так, это что?
Это то же самое. Так, ну и перед тем, как вообще начать что-то описывать, ведем дополнительные
предположения. Пусть у нас внутренняя часть множества, внутренние точки множества g это
некоторые непустые множества. Также предполагаем, что у нас для любой точки x из множества g
существует некоторая последовательность из внутренности, такая, что эта последовательность
сходится к этой точке x. Плюс дополнительно говорим, что у нас множество g ограничено. Ну и просим,
чтобы во внутренности множества g у нас все функции вот эти из ограничений были строго отрицательны.
Ну и также предполагаем, что у нас функция f непрерывно дифференцируемая. Введя вот такие вещи,
давайте посмотрим на вот такую функцию. Функцию f, которая у нас является непрерывно дифференцируемой
на нашей внутренности множества, при этом для любой последовательности точек из внутренности,
которая стремится к границе множества, эта функция просто улетает в бесконечность. Это как раз та самая
конструкция, с которой хочется работать, то есть вести какую-то функцию, которая на внутренности множества
ведет себя хорошо, а приближаясь к границе, начинает в некотором смысле повторять поведение индикатора,
то есть улетать на бесконечность. Окей, ну и вот примеры таких функций можно посмотреть здесь.
Примеры довольно очень простые. 1 делить на x. Обратная функция, все знают, что она просто уходит
на бесконечность, когда соответственно аргумент этой функции стремится к нулю. Ну и пожалуйста,
хотим вести наш нашу функцию, которая повторяет индикатор на бесконечность. Вот используем
пожалуйста вот такую функцию f. Аналогично можно поиграться с логарифмом. Про него мы тоже знаем,
что у него там происходит в нуле. Уходит в бесконечность, но хочется в плюс бесконечность
уходить, поэтому берем со знака минус. Все, очень простые идеи. То есть получается, что вы вместо
индикатора, который по факту ставит вот такую вот стену, за которую вам просто нельзя выходить,
эту стену в некотором смысле опроксимируете, ну или не стену, а в данном случае это все называется
барьером. Еще что в этом случае хорошо, то что действительно из-за того, что вот эти функции
улетают на бесконечность, когда вы приближаетесь к границе, вы интуитивно кажется, что вы за пределы
множества не выйдете. Не выйдете, а значит это что-то хорошее, и тут появляется свойство,
которое у нас не было до этого. То есть множество, что решение, которое мы найдем,
во-первых, мы пока не ввели задачу, но да ладно. Давайте пока скажем, что мы действительно
воспроизвели индикатор. Воспроизвели индикатор, но более гладким и дифференцируем образом.
Вот, единственный вопрос вот к тем функциям, которые у меня определены здесь, у индикатора
значение на самом множестве было ноль. Вот, понятно, что для этих функций это не так. Это не так,
и в зависимости от того, что у вас там происходит в функции g, но они могут принимать явно не нулевые
значения. Как можно с этим побороться, чтобы все еще бесконечность осталась, но вот здесь была ноль на самом множестве.
Ну, минимум, минимум, минимум просто сделает вас этот. Он срезку сделает,
это будет не дифференцируемость. Вот, чем можно сделать? Что можно сделать? Ну, вот предлагается
сделать следующее. А давайте я введу параметры ro. Как и в штрафах, я введу параметры ro, которые
у меня будут выполнять вот такую функцию. Я его добавлю вот так, вот 1 делить на ro к функции f.
Соответственно, если у меня ro фиксировано, функция f все еще ведет себя хорошо в плане ухода на
бесконечность. Ну, понятно, уменьшая ro, я могу контролировать поведение этой функции на необходимом
мне множестве. При достаточно больших ro f, вот это значение функции, вот которая вот здесь,
оно будет близко к нулю. Вот, и таким вот образом я буду контролировать поведение вот этой моей
новой штрафной функции. На самом деле уже и барьерной функции, потому что это уже не штраф, а барьер,
потому что он просто запрещает нам выходить за пределы множества. Свойства. Ну, и соответственно,
рассматриваем вот такую вот задачку. f от ro, f большое от ro, где у нас есть целевая функция, плюс
барьер соответствующим множителям. Один делить на ro. Так, ну давайте быстренько пробежимся по каким-то
простеньким свойствам, что мы теперь знаем про нашу новую целевую функцию f от ro. Она непрерывно
дифференцируема на внутренности множества g. Простой факт, потому что мы предположили, что у нас
функция f непрерывно дифференцируема не просто на внутренности g, а на всем множестве g. Про барьер,
когда мы его водили, барьерную функцию f когда мы водили, мы предположили, что она непрерывно
дифференцируема на внутренности g. Ну, значит, и сумма будет непрерывно дифференцируема на
внутренности. А также можно заметить, то что функция f от ro при стремлении к бесконечности,
при стремлении к границе будут стремиться к бесконечности. Откуда это следует?
исследуют ровно из свойств функции f потому что она стремится к бесконечности
а из непрерывности функции f маленькая на все множестве g мы можем сказать что
она принимает какие-то там конечные значения ограниченные значения
соответственно они не влияют на функцию f' и по факту поведение на
границе определяется вот барьерной функцией вот стремление к бесконечности
бесконечности хорошее свойство так такой у меня к вам вопрос на самом деле
кажется что мы задачу переписали в виде задачи уже безусловной то есть у нас
есть множество x мы по всему rd просто минимизируем нашу функцию f' но формально
это задача все же с ограничениями почему
потому что на ноль делить нельзя как это можно более формально сказать у вас
функция f определена только в пределах внутренности множества g за пределами
этого множества ваш штраф не существует как не существует там условно логарифм и
соответственно по-хорошему вы должны решать в некотором смысле эту задачу на
множестве определения но с этим проблем нету с этим проблем нету потому что на
самом деле если у вас есть некоторая стартовая точка которую вы взяли из
внутренности а дальше вы запускаете какой-то метод спуска который приближается
к решению вот вашей функции к минимуме и к минимуму функции f
рот икс то есть вы можете гарантировать что каждую эту рацию вы становитесь в
некотором смысле только лучше хотя бы не хуже чем стартовая точка с точки
зрения значения по значения функции от текущего текущего икс на текущей
итерации ну и получается так что вы становитесь только ниже с точки зрения
функции но вы знаете что у вас функция f рост стремится бесконечности когда вы
приближаетесь к границе когда вы приближаетесь границе ну и получается что
в какой-то окрестности границы у вас значение функции будет больше чем значение
функции в точке x 0 получаем что тогда у нас экската будет лежать точно не в
этой окрестности границы то есть все еще будет лежать во внутренности множество
вот то есть получается что-то хорошее в духе того что вы действительно за
пределы множества не выходите и это такой приятный факт надавать его можно
более формально доказать более формально доказать следующим образом вот плюс нам
тут понадобится некоторое вспомогательное утверждение пусть у нас да дан некоторые
параметры больше нуля соответственно заданная вот эта функция f с барьером и мы
хотим доказать что эта функция принимает свое минимальное значение на
внутренности множество g вот более того мы хотим доказать что вот множество
такого вида для любого параметра а это туда попадают те в точки из
внутренности где у вас функция с барьером меньше значения а вот является
доказательства довольно простое я не знаю как вам водилось замкнутость множества на
лекциях по мотонализу первом семестре как водилось отлично супер через предельную
точку здесь просто в книге иванова я посмотрел там по-другому ну на самом деле очень
похоже вот рассматривая некоторую последовательность из множества у хочу
доказать что множество у меня является замкнутым рассматриваем некоторую
последовательность сходящиеся в точке x чтобы доказать замкнутость мне нужно
показать что тогда та вот предельная точка x тоже будет лежать в у вот так как
стремиться к teachers для x и при этом иксы то у меня лежат во внутренности множество
g то у меня два варианта есть для точки x либо она лежит на границе либо оно
лежит во внутренности множества жлежища мнеKI что я при этом знаю про
про функцию f от точки x и t. x и t, здесь аргумент у меня лежит в пределах g, я знаю, что он
всегда меньше a, просто по определению множества u. По определению множества u
у меня вот выполнено вот это. Тогда когда я перехожу к пределу под
знаком неравенства, перехожу к пределу под знаком неравенства, что я получаю? То,
что у меня всегда, всегда получается, что вот этот предел, так, сейчас как бы тут лучше
сделать, если мы на границе лежим, непрерывность этого функции f.
Ну тогда да, можно, наверное, сделать что-то в духе того, что... просто у нас f не определено в x,
так как x на границе, там нет, точнее, определено, но нет непрерывности. Нет непрерывности, тогда что
можно сделать? Ну, если у нас x лежит где-то на границе, лежит где-то на границе, то в некоторый
момент у меня x начнут всегда попадать в некоторую эпсилонокрестность этой границы. В эпсилонокрестность,
эпсилонокрестность границы. Ну и, соответственно, в силу того, что я знаю, что на границе функция
ведет себя, функция уходит в плюс бесконечность, я могу подобрать эпсилонокрестность такую,
что у меня значение функции fхt в этой эпсилонокрестности будет больше, чем а. Так? Ну,
я получаю противоречие, потому что у меня, мы точно предположили, что в эту последовательность
хт входят только такие точки, где у меня значение функции меньше а. Вот, получается,
что, тут надо переписать, получается, переход, тогда у меня х принадлежит внутренности, другого
варианта не дано. Ну, а здесь тогда вообще все хорошо, потому что функция f непрерывно у меня на
внутренности множество g. Откуда тогда следует предельный переход? У меня есть f ро х и t,
и когда я делаю, и оно меня меньше, чем а, соответственно, ставлю пределы с обеих сторон,
этот предел у меня существует в силу непрерывности f ро, на внутренности у меня здесь
просто получается f ро в точке х, ну и здесь получается предел от константа. Это просто
константа. Все, конец. Получается, что действительно х у меня лежит во внутренности, и значение в точке
х у меня меньше а, а значит, что у меня х принадлежит множеству у. Так? Получили, что у
меня предельная точка тоже лежит в этом множестве. Отсюда следует то, что множество у у меня
замкнута. Ограниченность множества u следует из того, что мы во множество u берем только точки из
внутренности множества g, а множество g у меня ограничено. Значит его внутренность ограничена,
значит ограниченные множество u. Получается, что у меня это компакт, u это компакт.
Ну и как когда-то в школе зашел в университет, дедушка там так на лекции сказал, как говорил
Веерштраз, если у меня функция непрерывно на компакте, то она достигает своего минимума на
этом компакте. Поэтому я могу сказать, что на множестве u f' достигает минимума на u. Ну и в силу
того, как я определял u, это как получается, туда входят иксы, для которых там значение f' от x меньше
чем некоторое значение. А понятно, что минимум на u есть и будет минимумом этой функции на
все множестве int g. Хорошо, тут подоказывали про уровни, поняли. Теперь похожая теорема на ту,
которую мы с вами рассматривали для барьерных функций. То есть хочется показать, что с увеличением
ρ, а мы действительно хотим увеличивать ρ, чтобы как раз у нас свойство бесконечности оставалось,
но при этом мы прижимали функцию на наше множество к нулю. С увеличением ρ мы будем попадать в
епсилонокрестность, все меньше и меньше епсилонокрестность множество решений x звездой.
Но здесь множество решений x звездой уже определяется как не просто некоторые, не просто x из rd,
а x из g. Я его немного раздуваю на епсилон. Хорошо, что тут дополнительно я предполагаю? Еще то,
что у меня замыкание внутренности моего множества равно самому множеству. В принципе,
нормальное предположение, никаких страшностей нет, просто чтобы там полегче доказывалось.
Хорошо. Пару понятных фактов, которые следуют сразу из утверждения. То, что у меня x звездой
это не пустое множество, то есть множество решений у меня не пустое. Почему? Потому что в силу того,
что я уже предполагал, что у меня g это ограниченное множество, теперь я еще дополнительно предположил,
что оно замкнуто, потому что представляет собой замыкание некоторого открытого множества.
Тогда получается, что мы решаем задачу оптимизации функции f на некотором компакте. Непрерывная
функция на компакте принимает опять же по теореме Бирштрасса свое минимальное значение. Значит,
множество решений в данном случае не пусто. Исходные задачи. Ну а то, что у меня множество
решений для какого-то фиксированного РО моей новой задачи с барьером не пустой, а мы доказывали
в предыдущем свойстве, показывали, что там действительно решение существует, а оно лежит во
внутренности множества g. Хорошо. Ну а теперь поехали доказывать от противного то, что у меня
действительно с увеличением РО я буду попадать в епсилонокрестность все меньше и меньше множества x.
Похожая техника, как мы с вами делали на предыдущем занятии. Говорим, что существует
епсилон больше нуля и соответственно какая-то последовательная стероида, которая стремится к
бесконечности. Такая, что соответственно, что там происходит? У нас существует x и t со звездой из
множества x со звездой и РО и. То есть некоторые x из решения конкретной задачи с фиксированным РО и там,
для которых выполнено, что x и t со звездой лежит вне множества x епсилон x епсилон епсилон
со звездой. То есть не лежит в этой епсилонокрестности. Да, не лежит, не лежит. Вот. Окей. То же самое,
что сделали в прошлый раз. То же самое предположение, которое было. Вот. Оно же здесь записано.
Вот. Ну давайте рассуждать. Давайте рассуждать. В силу того, что у меня теперь мы знаем, что у меня
вот это множество множество решений принадлежит лежит внутри int g, а int g у меня в свою очередь это
ограниченное множество, ограниченное множество. Я опять же говорю, что последовательность x и t со
звездой это у меня ограниченная последовательность. И, ровно как в прошлый раз, теорема, кого? Бальсана
Вирштраса, ограниченная последовательность существует сходящейся под последовательностью. Рассматриваем
под последовательность x и t, которая сходится к точке x с волной, например. x с волной со звездой.
x с волной со звездой. Хорошо. Рассмотрели вот такую точку. Это понятно. То есть существует такая
последовательность, которая сходится со звездой. Так. Написали. Дальше что предполагаем? Почему у меня
точка x со звездой лежит в g? Почему она лежит в g? Кто понимает? Кто понимает, почему она лежит в g?
Да. Да, да, да, да. В силу того, что у меня все x и t лежат в g, то у меня предельная точка x тильдой
g лежит либо во внутренности, либо на границе. Так. Но я знаю, что у меня множество g замкнуто,
поэтому эта вещь, она включает в себя то, что x по факту у меня лежит в g. Так. Ну и как раз
предположение о замкнутости здесь помогает. Вот. Хорошо. Так. Это мы с вами обсудили. Дальше. Что мы
еще говорим? Почему у меня x тильдой не лежит в x со звездой? Не лежит в x со звездой. Почему это так?
Что будет, если оно лежит в x со звездой? Да. То есть если у меня x тильдой со звездой будут
лежать в x со звездой, то с какого-то номера i у меня x и t будут лежать в эпсилонокрестности x со
звездой. Для любого эпсилона это будет справедливо, потому что предел лежит в x со звездой, тогда и точки
начнут попадать в какую-то эпсилонокрестность этого множества. Но мы с вами предположили, что x и t
у нас создаются ровно так, что они не лежат в x со звездой эпсилон. Вот. Ну получается противоречие и в силу
того, что мы предположили, x со звездой не лежит в пределах x. То есть получается x тильдой со звездой
лежит в g, но не лежит в x со звездой. Не лежит в x со звездой. Тогда что я могу сказать о значении в
точке x тильдой со звездой? Так как она, вот эта x тильдой со звездой лежит вне моего множества
решений, вне моего множества решений, то тогда существует некоторое значение дельта, положительное,
положительное значение дельта. Такое, что вот выполнено следующее соотношение. Значение
минимума, вот это минимум на g функции f будет меньше либо равно, чем f тильдой со звездой, причем
еще дельта эту разность контролирует. Ну понятно, в силу того, что это у меня не решение, между ними
есть зазор. Вот, и этот зазор, как раз вот я в этот зазор помещаю дельту. Так, какую-то константу положительную.
Хорошо, хорошо, поместили эту дельту сюда. Бум-бум-бум. Так, здесь это поместили. Дальше,
соответственно, следующее рассуждение. В силу того, что у меня множество, в силу того, что у меня множество g
замкнуто, а функция f непрерывно, то я могу найти некоторую точку x тильдой из внутренности,
которая будет близка в некотором смысле к точке к x звездой. x тильдой близка к x звездой. При этом
вот это из внутренности g. Вот, x ну это просто решение, которое мы с вами уже рассматривали. Вот,
ну и в силу того, что у меня f непрерывно, непрерывно, я эту близость могу контролировать следующим образом.
То есть, я могу подобрать x тильдой настолько близко, что у меня значение в этой точке x тильдой будет
меньше либо равно, чем f от x звездой, плюс вот этот параметр дельта, но теперь уже пополам,
который мы уже с вами определили, как вот то значение. Окей? Вот, хорошо. Вот так вот наопределяли
это всё безобразие. x тильдой f от x звездой плюс дельта. Дальше что хочется сделать? Дальше
хочется попробовать пописать, что у меня будет происходить с функцией row, f row, например,
в точке x тильдой со звездой it, ну и соответствующее row it я здесь беру. Вот. Что это? Будьте здоровы.
Что это такое? Это у меня f от x стильдой со звездой, f от x тильдой со звездой, плюс 1 делить на row.
Моя штрафная функция row it, здесь row it с тильдой, а здесь будет просто f x с тильдой. Так вот. Окей.
Это оптимальное значение для вот этого текущего параметра row. Получается, что это значение будет
меньше либо равно, чем значение функции row it с тильдой в точке x тильдой. Так? Ну, просто потому что это
решение, а это какая-то произвольная точка. Что хорошо, что она из внутренности. Что хорошо,
что она из внутренности, поэтому подставлять ее функцию можно. Подставлять ее функцию можно,
она определена будет. Вот. Так. Ну и тогда здесь это тоже расписываю. Получаю, что у меня здесь f с тильдой,
плюс 1 делить на row it тильда, f x тильда просто, f x тильда. Вот. Хорошо. Хорошо. Давайте я переношу это
аккуратненько в две стороны. Давайте вот так вот. f x и t со звездой меньше либо равно, чем f от x со звездой,
плюс f row и тильда f от x с волной, минус f row и t f от x с волной. Так. Только что с вами доказывали то,
что у меня функция f row x и t в некотором смысле ограничена снизу на int g, на int g, так,
через линии уровня. То есть существует какая-то константа c, которая строго больше,
чем минус бесконечность, которая ограничивает у меня f row. Предыдущее свойство про это было,
про уровни, что у меня вот это множество u, про множество u, где мы это доказывали с вами. Вот.
Аналогично я могу доказать, что у меня и уровни будут ограничены какой-то константой там s тильдой,
которая меньше минус бесконечности, потому что там я что, пользовался только тем, что у меня
есть функция f, но я сейчас уберу просто непрерывные на всем же функциям. Ну, не страшно. Тоже минимально
максимальное значение, соответственно, но она не повлияет на вот это то, что у меня есть там
ограничивающее снизу у барьера функция на множестве int g. Вот. А это хорошо,
то что на int g у меня функция ограничена, потому что точки x тильдой у меня лежат в int g. Вот. Поэтому
вот это выражение у меня тоже в некотором смысле ограничено, потому что ограничено,
если функция ограничена снизу, то минус функция ограничена сверху. И поэтому здесь я могу вот
оценить это вот так. f от x тильдой минус rho и t тильда c. Ну, с тильдой пусть будет. Вот.
Получается что? Получается, что у меня вот это некоторая константа, некоторая константа,
это тоже некоторая константа. Вот. Что будет, когда я перейду к пределу в этом неравенстве? Что будет,
если я возьму здесь предельчики? Справа и слева. Куда будет стремиться правый кусочек в пределе,
если я возьму предел по и? Просто в f от x тильдой. Правильно. Потому что rho у меня будет стремляться
в бесконечность, мы ее так определяем. Вот. Соответственно, все вот эти константы делить
на что-то, которое стремится к бесконечности, это будет просто стремиться к нулю, останется поэтому f от x тильдой.
Вот. А про f от x тильдой мы еще знаем, что оно у меня f от x звездой плюс дельта пополам. Вот. Вот оно. То,
как мы определили f от x тильдой. Так. А правый кусочек куда будет стремиться? Правый кусочек куда
будет стремиться? Кто понимает, куда будет стремиться? Ой, да, левый, левый, левый кусочек. Вот этот левый
кусочек, куда он будет стремиться? f от x тильдой со звездой. f от x тильдой со звездой. Просто потому
вот мы это с вами уже находили. То, что у меня f от x тильдой со звездой лежит в g, а у меня f непрерывно
на g, поэтому предел валиден. Придел валиден. Так. Это все стремится к f от x тильдой со звездой. Но,
с другой стороны, про f от x тильдой со звездой я знаю то, что у меня он меньше, строго меньше,
чем f от x плюс дельта. f от x со звездой плюс дельта. Но теперь смотрим, что получилось. Раз, строго меньше.
Раз. Так. Строго меньше. Получается, что у меня вот здесь вот это выражение меньше, чем вот это
выражение. Ну и по факту оно означает, что положительная дельта меньше, чем дельта пополам.
Противоречие? Противоречие. То есть, изначально предположение о том, что у меня существует такая
последовательность, которая будет выбиваться из того предположения, что у меня прибольший кро будет
все больше-больше вмещаться в этого апсилонокреста, смудует множество, оно было неверным. Согласны? Все,
отлично. Получается, что мы доказали похожий факт, который у нас был на прошлый раз. Здесь
все расписано. Итог по вот этой половине лекции. Поняли, что, опять же, условные задачи превратили
в безусловную. Формально нет, но по факту да. Поняли, что при увеличении row у нас в некотором смысле
апроксимация барьера, который мы строим, становится лучше. Это свойство нуля появляется. Хорошая,
опять же, новость в том, что мы удовлетворяем ограничением. Ну и почему, например, тот сегодняшний
разговор вообще озаглавлен как метод внутренней точки? Потому что, как раз удовлетворяя ограничением,
мы никогда не выходим за пределы нашего множества. Мы доказали, что для каждой функции f-row
минимум его лежит в пределах множества. Inge, мы никогда не выходим за пределы. Барьеры нас
от этого удерживают. Ну и на самом деле, поэтому это все называется внутренняя точка. Ну и в общем-то,
случай и все. Больше ничего сказать тут хорошего нельзя, ровно как было и со штрафами. Как-то можно
попробовать. Что нужно делать? Просто взять какое-то row, попробовать решить задачу безусловной оптимизации,
ну, например, градиентным спуском метод, ньютона, чем угодно. Понравилось решение по качеству,
с точки зрения сходимости, ну тогда можно оставить. Не понравилось, нужно попробовать увеличить row,
тогда у вас в некотором смысле ограничение на себя потянут сильнее, вот, и решение станет
качественнее. Вот, в принципе, на этом все. То есть, что можно в общем случае сказать. Но это далеко
не все, что можно сказать в частных случаях. И вот как раз вторая часть лекции будет про то,
что у нас в некотором смысле патриотичная, потому что как раз в частном случае выпуклой оптимизации,
эту задачу хорошо так исследовали Аркадий Семенович Немировский и Юрий Евгеньевич Нестеров. И поведение
метода внутренней точки для задачи выпуклой оптимизации, это их заслуга. Вот, мы постараемся
просто чуть-чуть прикоснуться к их результатам. Так-то там уже все результаты, которые есть по
моменту внутренней точки, даже классические, занимают книгу тысячи страниц. Понятно,
всю книжку мы с вами прочитать не можем. Но какую-то такую основную теорему на нее посмотрим,
и увидим, что действительно результат классный. И вот то, что мы доказали сейчас, это вот по
сравнению с ним ничего. Мы получим прям хорошие оценки, гарантии сходимости, вплоть до того,
сколько... сколько... как сильно нужно уменьшать РО. Вот, и как быстро мы будем сходиться к решению.
Ну что, продолжаем? Продолжаем разговор. Успокаиваемся, продолжаем разговор. Так,
окей. Чтобы вести на самом деле что-то хорошее про барьерный метод, нужно дополнительное понятие.
В данном случае это самосогласованные функции и самосогласованные барьеры. Определение не
очень приятное, вот, и не очень понятное. Вот. Но на самом деле тут в него вникать особо не
нужно. Потому что по факту пользуются просто хорошими примерами, которые удовлетворяют
этому определению. Вот. А смысл на самом деле у этого определения, ну там даже мне на самом
деле не до конца понятно, откуда оно пришло. Потому что хорошая мотивация у этого всего безобразия,
то что хочется определить какой-то класс функций, который будет неплохо работать в методе Ньютона.
Потому что мы знаем, что у вас метод Ньютона сходится локально. За счет подбора шага, за счет
демфонирования, демфрирования, демфрирования можно добиться того, что будет сходиться и
глобально, но линейно. Никакой супер линейной скорости у вас там не будет. Вот. Поэтому пытаются
определить какие-то более хитрые классы. Какие-то более хитрые классы. Ну, в том числе определяют
вот такой. Потому что в некоторых анализах метода Ньютона, в том числе с демфрированием,
важно, как ведет себя третья производная. Вот. Ну, хорошо. Определяется вот такой класс. Еще раз
я говорю, тут в него вникать не нужно. Опять же, про это написана книга 1000 страниц. Почему вот это
хорошо? В том числе анализ метода Ньютона, я этими фактами просто буду пользоваться. Примерчики.
Примерчики, которые нам нужны. Квадратичная функция, логарифм и комбинация логарифма и
квадратичной функции. Вот. По факту последний пример, это то, что нам надо. То, что у нас
логарифм, это запиханная туда линейная функция квадратичная, будет являться самосогласованной
функцией. Вот. Потому что мы ровно идем к тому, чтобы сделать барьер самосогласованным. То есть,
каким-то хорошим довольно-таки. Почему это хорошо? Ну, вот сейчас будем эти свойства
определяться и с помощью них там красиво все докажется. Также операции, которые сохраняют
самосогласованность, это комбинация сумма двух самосогласованных функций будет самосогласованной,
если соответственно, коэффициенты, с которыми вы суммируете, больше единицы. Вот. Ну, а также,
если вы меняете аргумент самосогласованной функции на какую-то афинную комбинацию, то тогда тоже
новая функция будет самосогласованной. Понятное тоже свойство. Скорее вот они нам и понадобятся.
Вот. Определение, оно у нас страшное. Вот. И да, из него много чего хорошего следует, но это технические
факты, которые, ну, сегодня мы пропустим. Вот. Самосогласованный барьер. Как раз ровно к нему мы идем.
Барьер у нас самосогласованный. В каком случае? Причем вводится параметр ню, параметр
самосогласованности барьера. Что, соответственно, ню всегда у нас больше единицы. Барьер самосогласованный,
когда функция, которая его порождает, самосогласована. Это раз. Плюс выполнено вот такое вот соотношение.
Вот такое вот соотношение. Опять же, какое-то странное соотношение, оно нам понадобится. Вот.
Даже не в таком виде, которым у нас записано у меня сейчас, но вот такое вот определение формальное дано.
Будем работать. В частности, из этого определения сразу же следует, так как у вас вот эта часть
не отрицательная, то у вас получается, что гисиан функции f большое будет положительно полуопределен
по определению. Ровно это написано для любого вектора h. У вас эта квадратичная форма больше
либо равна нулю. На самом деле, если технически покопаться, можно показать, что тут не просто не строгий
знак, тут можно поставить строгий знак. Вот. То есть, гисиан будет просто положительно определен для
самосогласованного барьера. Вот. Пример, опять же, барьера ровно с теми барьерами, с которыми часто и
приходится иметь дело на практике. Логорифм от линейных ограничений. Единственное, что нужно
дополнительно предположить, что эти линейные ограничения удовлетворяют условия слейтера. То есть,
есть такая точка, где знак в ограничениях становится строгим. Помните условия слейтера, соответственно,
что равенства выполняются, а неравенства, соответственно, должны в какой-то точке, в этой же,
где выполняются равенства, принимать значение. Знак должен быть строгим. Понятно, это как раз вам
гарантировать существование сильной двойственности и, соответственно, то, что у вас решение двойственной задачи
там совпадает с решением исходной. Вот. Ну и здесь она тоже требуется, чтобы барьер, соответственно,
был самосогласованным. Вроде бы как естественное свойство, которое, кажется, понятно, откуда вытекает.
Окей. Давайте поехали рассматривать, что там придумали Нестеров с Немеровским. Во-первых,
сразу же оговоримся, что это все работа для выпуклой оптимизации. Только для выпуклой
оптимизации, когда у нас целевая функция f выпукла и все ограничения тоже выпуклы. Ну, в принципе,
в таком сетинге мы с вами обычные работаем. Окей. А теперь я эту задачку чуть-чуть перепишу. Чуть-чуть
перепишу. Вот в таком вот виде. Это так называемый вид эпиграф. Ну, короче, это эпиграф. Вот с этим
множеством вы, скорее всего, знакомы, познакомились на семинарах. Это множество называется эпиграфом,
для которых выполнено вот это ограничение. И что мы знаем про эпиграф, как он связан с выпуклостью
функции? Да, эти вещи эквивалентны. Выпуклость эпиграфа и выпуклость исходной целевой функции,
эти вещи эквивалентны. То есть, функция f выпукла только тогда, когда вот это множество задамаемое
вот этим ограничением тоже будет выпукло. Ну и смотрите, что я делаю. Я выношу как целевую функцию
просто переменную t. Ну а дальше что? Чтобы это ограничение у меня срабатывало, в лучшем случае t
у меня должно быть равно f от x. Ну и понятно, что мне нужно уменьшать f от x. И у меня вместе с этим
будет уменьшаться t. Вот такая вот задачка. Переформулировка исходной задачи выпуклооптимизации
в виде эпиграфа. Чем она хороша? Почему мы говорим только про выпуклооптимизацию? То, что если бы целевая
функция f у нас бы была не выпукла, то понятно, что с эпиграфом так бы не прокатило. И вот это ограничение
у вас бы стало уже не выпуклым. Вот. И новая задача стала бы тоже не выпуклой. А так у вас выпуклая
задача с выпуклыми ограничениями. Причем целевая функция является очень простой. Это просто, ну,
линейная функция. Вы можете сюда добавить там единичку транспонированную. Размер это уже 1 там.
1 умножить на транспонированную. Получается линейная функция. Вот. Поэтому в оставшейся части лекции мы
будем с вами рассматривать задачи линейные только. Вот такого вот вида. Вот. Ну, в этом и соль как раз в том,
что, во-первых, а выпуклые задачи к ним сводятся. Б, ну, сами по себе они тоже более чем популярны.
Более чем популярны. Вот. Опять же на семинаре, когда вы разговаривали про разные классы задач,
они возникали. Ограничения выпуклые. Ну, работаем с такой задачей. В общем виде метод внутренней точки,
про который мы с вами говорили, как решать задачу с барьером. Это вы просто берете некоторые параметры
row, отрешиваете вот эту задачу целевой функции f от row. Ну, и получаете какой-то выход. Дальше вы можете
прекратить попытки по туге делать что дальше, что-то еще. А можете увеличить параметры row. Взять ту старую точку,
которую вы получили до этого в качестве стартовой. Ну, и продолжить процесс. Снова оптимизировать вот эту
функцию f от row. Теперь row уже будет новая. Будет больше. Ну, и соответственно получить новый выход. И уже решить,
останавливается или нет. Это общая схема. Как это обычно работает. Ну, вот если у нас задача произвольная.
Произвольная. Но в чем главная красота? Главная красота так это в том, что в линейном случае,
в линейном случае, когда у нас еще и барьеры, с которыми мы работаем, new,
самосогласованы, будет все довольно значительно приятнее. Метод будет прям точный, будет сказано,
чем отрешивать, как увеличивать row и соответственно будут даже даны гарантии исходимости. Ну, и как мы
видим дальше, чем меньше new параметр самосогласованности барьера, тем лучше, тем сходимость
будет быстрее. Вот. Поэтому, когда определяется барьер, нужно в том числе на это обращать внимание.
Вот. А метод будет довольно хитрый. Метод будет довольно хитрый. Во-первых, что нужно сделать? Нужно
сделать, мы определяем расстояние до решения вот таким вот образом. Через вспомогательную функцию f.
В принципе, вспомогательная функция f, она не особо сложная. Это наша функция f-row умноженная на row.
То есть, просто row из барьера перенесена в линейную часть. А дальше определяется расстояние.
Расстояние до... Как мы можем измерять в некотором смысле расстояние до решения? То есть, в принципе,
то есть, если бы я вот эту убрал матрицу отсюда, что бы это просто было? Чего бы равнилась лямбда?
Норма градиента вот этой функции, которую я по факту переделал чуть-чуть. Вот. Просто норма градиента.
Но с матрицей... А что хорошо в этой матрице, сейчас мы поймем, чему равен гисиан функции f от row?
Кто понимает? Если мы линейную функцию два раза продеференцируем, что-нибудь останется от нее?
Ничего не останется. Получается, что гисиан просто равен гисиану f от x. А мы с вами только что
обсудили, что самосогласованность барьера влечет то, что гисиан положительный. И по факту вот это
порождает в некотором смысле ту норму, которую мы видели. Норма, которая порождается матрицей. То есть,
вместо евклидовой нормы у вас будет норма вот такая вот. Норма, порожденная матрицей. Здесь то же
самое. Как бы сходимость не по норме градиента, а по вот такой вот хитрой норме градиента, которая еще
вот с матрицей. Матрица гисиану. Еще и обратного. Ну хорошо, сейчас увидим зачем это надо. На самом
деле это все следует из сходимости метода Ньютона, демпфированного метода Ньютона для самосогласованной
целевой функции. Вот что делаем в методе. Во-первых, первое, что нужно сделать, это правильно выбрать
стартовую точку. Стартовая точка должна быть довольно близка к решению задачи. Можно реально
заметить, что у нас вот этот критерий равняется нулю, когда соответственно градиент равен нулю,
потому что матрица положительная определенная, а градиент равен нулю в решении. Получается,
что мы должны выбрать точку x0 близко к решению исходной задачи с rho-1. С rho-1 у нас есть какая-то
задача с барьером, ну или вот функция f, которая эту задачу с барьером отображает, где у нас есть
исходное значение rho-1. Я говорю то, что я выбираю x0 так, что у меня решение этой задачи с барьером,
и x0 близки точностью до epsilon, где epsilon меньше единицы. Как такую точку подобрать? Ну,
например, запустить какой-то метод оптимизации для функции либо вот phi, либо функции f, и найти
соответственно довольно хорошее приближение f-rho. Дальше что делаем? Взяли хорошую точку. Каждый раз,
как я описывал, увеличиваем rho. Здесь оно увеличивается линейно, то есть есть единица
плюс какой-то коэффициент e2, который положительный меньше единицы. Ну и вот тут все будет еще завязано
на constant nu самосогласованности барьера. Вот rho увеличили. Получается наша задача f-rho поменялась,
и f-rho поменялась. И оказывается, достаточно сделать всего один шаг демпфированного метода Ньютона,
чтобы мы снова вернулись вот к этому. Только теперь уже для точки, для функции f-rho текущей,
и точки текущие, то есть x1, rho1, ну и так далее. rho k, xk. Понятная идея, да? То есть что мы делаем?
Мы увеличиваем rho, задача меняется. Вроде бы как та предыдущая точка x, которая у меня была,
она, возможно, перестала быть хорошим решением новой задачи. Но один шаг вот такого метода Ньютона,
а по факту это он и есть, вот гисиан, вот ингредиент, а это шаг. Демпфонирование это же есть просто шаг.
Ой, да ладно, короче, вот это слово. Вот, по-английски просто проще там дамп называется,
его проще произносить, а по-русски. Вот шаг гисиан обратный, норма градиента. Делая шаг вот
такого метода Ньютона, одного шага может быть достаточно при правильном подборе e1 и e2,
чтобы вернуться вот к этому условию, что у меня текущая точка, в которой я работаю, будет хорошо,
ну с точки зрения именно близости к решению, к реальному решению задачи с текущим rho. Понятная
идея, да? Сейчас мы это докажем. Вот, что вот этот шаг валиден и что за один шаг метода Ньютона,
вот, мы действительно возвращаемся к правильному, находим правильную точку xk t f плюс 11,
что условие все еще выполняется. Вот, ну это то, что я описал. Берем довольно близко точку x
стартовую и, соответственно, которая близка к x звездой rho, к решению задачи с барьером. Дальше,
соответственно, критерии мы это тоже обсудили, что это близко к норме градиента, бла-бла-бла. Вот,
далее мы увеличиваем rho и за счет шага однимфонированного метода Ньютона мы снова
гарантируем, что новый x будет близок к решению уже новой задачи с барьером. Вот, окей, давайте это
доказывать. Ведем сначала обозначение матрица H. Матрица H, это, соответственно, у нас просто
гессиан нашей функции, вот этой f rho. Но, как мы поняли, это просто гессиан нашего штрафа,
потому что линейная часть на него не влияет. Плюс, соответственно, тут аннотация, которую мы опять же
обсудили, это x транспонированное ax. Ну и здесь тоже тот факт, который я вам сказал, что из
определения самосогласованного барьера можно вытащить то, что он будет, вот, гессиан будет
положительно определён. Положительно определён, поэтому те нормы, которые мы определяем, эти там
обратные матрицы, действия, которые мы будем делать, они будут все валидными. Так, ну что нам
нужно сделать? Нам, по факту, нужно оценить, а вот в новых обозначениях мы ведем, как выглядит
функция λ. Вот, ну просто по тому, как мы ее ввели, это что это? Градиент умножить на, соответственно,
градиент-градиент, а тут гессиан обратный. Вот, ну вот ровно это и есть, соответственно,
норма, а еще там корень был. Вот, ну и вот здесь это и записано. Гессиан, вот он обратный гессиан,
через вот такое вот обозначение. Гессиан встает сюда обратный, здесь встают нормы градиента, вот,
и берем из них корень. Все, просто переобозначили. Дальше я выписываю градиент функции f. Функции f
это ρс плюс градиент моего барьера, градиент моего барьера. Ну что мне нужно сделать? Мне нужно
сделать, во-первых, хочется понять, как меняется... Изначально, когда я захожу в цикл, у меня
гарантировано то, что у меня f ρ к-1 xk меньше, чем ε1. Так я инициализирую, и так я хочу по индукции
показать, что у меня вот это всегда будет выполняться. Предполагаю, что вот это выполнено изначально
в цикле по инициализации, как минимум. А дальше я хочу понять, как увеличение ρ мне все это испортит.
То есть я меняю ρ, а точку не меняю. Вот, вопрос, как вот это я могу оценить? Ну вот сейчас этим мы
будем заниматься. Сейчас вот этим мы будем заниматься. Да, вот оно расстояние, я его выписал.
Дальше что происходит? Дальше в силу того, что это у меня некоторая норма, а для нормы я
могу использовать неравенство треугольника. Поэтому сюда я закидываю умный ноль, вычитаю ρ и
добавляю ρ к-1 на c, на c плюс градиент. Дальше неравенство треугольника, здесь оставляю ρ к-1,
здесь оставляю разницу рошек, здесь оставляю разницу рошек xc. Что мы можем сказать про вот это
выражение, чему оно эквивалентно? С точки зрения обозначений, ну-ка смотрим на обозначение.
Все правильно, лямбда и ρ к-1 xc. Вот про это, вот про это я узнал, что это меньше, чем epsilon 1.
Так мы зашли в цикл, в текущую эту рацию. Но видите, у меня появилась какая-то добавочка,
неприятная добавка, с которой нужно будет поработать. Так, ладно, поработаем здесь. Что я
сделаю? Я сделаю следующее, ρ к-ρ к-1 вынесу за пределы нормы, разделю еще на ρ к. Здесь,
соответственно, ρ к я внесу в норму и на c умножу, h-1 xc. Что я знаю про вот это выражение,
если я подставлю ρ к сюда, что я про него знаю, как это меняется. Там же разница всего лишь
между ними. Эпсилон 2 я вводил в корень из этого. ρ к-1 есть ρ к. Поэтому если я возьму просто
разность, у меня останется что-то в духе epsilon 2 ню, делить на ρ к-1. Ро к-1. Вот получается,
что вот это равно просто вот этому мнению. Согласны? Что-то тут чуть-чуть... Да-да-да-да. Ну давайте
сделаем вот так. Я, может, тогда подпортачил, чтобы... Вот так. Сейчас как бы... Вот так вот.
Сейчас. Рока тогда будет у меня... А, вот тут минус первое. То есть ρ к-1 делить на вот эту скобку.
Так. Это равно рука. Вот. И тогда, когда я вычту, что у меня будет? Тут останется 1 минус
отношение рука минус 1. Делить на ρ? А? Не-не-не. Я чуть-чуть поменял просто. Чуть-чуть поменял,
чтобы у меня красивше вылезло. Почему? Сейчас. Сейчас. Сейчас. Рука. Тогда нет,
рука минус 1, тогда будет больше. Вот, короче, вот такое вот выражение, кажется, нужно взять.
Рока минус 1 делить на... А, нет, делить, тогда она станет... Господи, сейчас.
Вот. Рок... А, с минусом, что ли, тогда нужно взять сейчас. Рока 1 минус эпсилон 2 корень. Вот так.
Вот. Тогда у меня ρ к-1 будет меньше. Меньше, потому что я беру что-то положительное. Рука умножена
что-то меньше единицы, тогда ρ к-1 будет меньше. Ну, это пойдет, это пойдет. То есть там мы увидим,
что главное, что просто ρ увеличивалась линейно. Вот. Сейчас главное подсобрать тут вот это. Так.
Тогда что здесь будет? ρ к-1 делить на ρ к, и это будет 1, 1, минус, это 2, ню, ню, 1,
минус вот так вот. И тогда это будет е2 корень из ню. Вот. В алгоритме этот тогда чуть-чуть поменяем.
Давайте вот вернемся на алгоритм, и я вот здесь поставлю минус. Минус, и здесь минус 1, окей? Разницы
не будут. Сейчас увидите, что там главное, что просто линейно его менять. Вот. Потому что это вещи,
на самом деле, близкие. Вот. В это выражение я ровно и получил. А, ну, здесь, кстати, я
ρ к-1 вытащил. Окей. Ладно, не страшно. Что-то я даже перемудрил. То есть можно было оставить,
как было. Вот. Так. Нужно оценить ρ к-1 умножить на С по матрице H. По матрице H вот этой нормой. Ну,
давайте хорошо оценим. Оно, благо, у меня вот здесь вот вылезает где-то. Оно вылезает обычно в лямде,
вылезает. Давайте глянем, чему равно вот эта лямда ХК. Это как раз ρ к-1 С плюс F от ХК и на
матрицу H-1 ХК. Так. Что я могу про это сказать? Я могу сказать, что на входе алгоритм это у меня
было меньше, чем ε1. Так. А теперь я стираю левую часть. Я про нее нужно записал, что это меньше,
чем ε1. И хочу воспользоваться неравенством треугольника. Опять же, у меня норма. Сумма
норм. Как ее снизу оценить с помощью неравенства треугольника? Что тогда там будет? Модуль разности,
все правильно. Но я даже тут не с модулем возьму, я модуль еще и раскрою. Потому что число меньше,
под модульное выражение просто меньше либо равно, чем модуль. Я вот так вот его раскрою. Так,
здесь у меня будет H-1 ХК. Вот. Здесь, соответственно, минус градиент, градиент H-1. Вот. Окей. Ну и
тогда я получу, что РО К-1 С H-1 меньше либо равно, чем Е1 плюс градиент F от Х H-1. Вот. То есть,
чтобы оценить теперь вот это выражение, вот это у меня уже есть, осталось только оценить норму
градиента по норме H-1. Вот. И для вот этого всего безобразия мне как раз понадобится свойство
барьера, которое я записал. H транспонированная, норма градиента, норма градиента по модуле
меньше либо равно, чем ню, ню корень, H транспонированная, набло F Х H. Вот. Это у меня
выполнено для любого H. В частности, будет выполнено для вектора H, который равен H-1,
градиент F от Х. Так. Раз выполнен для любого H, я могу подставить вот такой вот H,
какой я захотел. Ну и тогда, что у меня будет получаться? H транспонированная. Так.
Когда H транспонированная, подставляю, у меня что будет? Набло F Х транспонированная здесь,
H минус транспонированная, градиент Х по модулю меньше либо равно, чем ню. И когда здесь,
начну сейчас H подставлять, у меня снова вылезет градиент транспонированной. H минус
транспонированная. Одна из H сократится с гессианом, потому что это тоже гессиан.
H-1 сократится с гессианом, у меня останется одна матрица, и здесь будет еще один градиент. Так.
Дальше, что я сделаю? В силу того, что у меня гессиан, это вещь симметричная,
я транспонирование здесь сниму. Окей? Вот. Ну и тогда, что получается? Вот здесь у меня
записано одно выражение, а здесь записан его корень. Согласны? Да? Всё. Тогда у меня,
я делю на этот корень, и получается, что корень вот этот, градиента, H-1 градиент, будет меньше,
чем корень из ню. Вот. А это же и есть вот это выражение. Норма по матрице. Корень, то есть берем
матрицу, ставим, домножаем на векторы, транспонирован, только забыл, и берем из этого корень. Получается,
что вот это у меня будет меньше, чем корень из ню. Окей? Ну вот такое вот странное свойство,
которое мы изначально ввели, оно вот здесь выстреливает. Вот. Чтобы оценить просто вот такую норму
матрицы. Норма градиента по матрице. Вот. Необычное свойство. Понятно, что его нужно отдельно
проверять для каждого из барьеров, но вот оно немного искусственное. Ну и вот оно здесь вот выстреливает.
Выстреливает. Поэтому мы можем оценить рожки, сюда это подставить, и здесь будет что-то в духе E1
плюс E2 делить на ню. Здесь ещё будет E1 плюс корень из ню. Окей? Всё. Оценили вроде как у меня
поменялось расстояние. Вот. Когда я поменял ро. С ро минус первого до ро катого. Вот. Оценил,
оценил. Тан-тан-тан. Тан-тан. Вот. Хорошо. Хорошо. А теперь нужно применить факт,
который мы доказывать не будем. Это просто сходимость демпфонированного метода ньютона для
самосогласованной функции. Оказывается справедливо вот такая вот теорема. Ну вот,
что вот так вот будет сходиться метод ньютона, когда у нас функция f самосогласована. Согласована.
Единственное, что нужно отметить, что у нас действительно функция f является самосогласована,
потому что барьер самосогласованный, так? И у нас ещё к барьеру добавляется линейная функция. А мы
поняли, что там квадратичные, а значит и линейная функция. У нас будут самосогласованными. Ну и сумма
двух самосогласованных функций, это вещь самосогласованная. Вот. Если коэффициенты там больше единицы. Вот.
Ну всё, значит у меня f самосогласована функция, если я ро изначально задал больше единицы. Вот.
Получается, что я могу применять результаты сходимости метода ньютона для этой задачки. Вот. И они
будут вот такими. Вот такими. Хотелось бы, то есть мы же что хотели, после применения метода ньютона,
хотим добиться, чтобы вот это было меньше, чем epsilon1. Один шаг сделали и сразу же вернулись в нужную
окрестность. Только уже решение новой функции. И оказывается, так и будет, если правильно
подобрать e1 и e2. Потому что для альфа, для лямбда мы уже выражение знаем. Понятно, что если взять
e1 и e2 довольно маленькими, у вас этот квадрат будет кушать всё, а здесь 1 минус что-то маленькое,
оно будет там что-то хорошее. Вот. А квадрат там всё съест. В частности, если взять e1 там 0,5,
e2 0,8, получится, что новое расстояние до решения новой функции будет меньше, чем e1. Вот. Это просто
технический подбор вот этого безобразия, учитывая ещё то, что nu больше ещё единицы. Вот. Тут не особо
что-то такое секретное. Вот. Просто нужно аккуратненько это всё подобрать. Понятная идея, почему так
получается. То есть по индукции можно доказать, что изначально мы задали расстояние до решения,
потом поменяли row, поменяли задачу, сделали всего один шаг метода и сразу же получили хорошее
решение для новой задачи за один шаг метода ньютона. Классная идея, красивая идея. Но это
ещё не всё. Это ещё не всё, потому что мы вроде как поняли, что да, у нас row растёт,
row растёт, и это вроде как хорошо. И за одну итерацию метода ньютона мы сразу же получаем решение.
Окей, но всё равно непонятно, а как мы в итоге приближаемся к решению исходной задачи. Сколько
итерации такого алгоритма нужно сделать? То, что итерация рабочая, мы поняли. Вот. А то,
что сколько их нужно сделать, ну давайте это поймём сейчас. Опять же, пак довольно
сложный, поэтому я его покажу, как у Аркадия Семёныча Немировского в его лекциях. Он
показывает только на примерчике алгоритмических барьеров, что это всё работает, а для всех
остальных, ну там смотрите, книжку тысяча страниц. Вот. Упрощаю задачу, говорю, что пусть у меня метод
не просто находит какое-то близкое решение вот этой задачи с барьером. Пусть у меня вот на каждой
итерации мне доступно чёткое решение. Я могу находить каждый раз прям чёткое решение задачи с
барьером. Вот. Ну и, соответственно, барьеры у меня не простые, а алгоритмические. Вот. Теперь,
соответственно, возникает вопрос, как быстро я приближаюсь к решению. Тут всё довольно будет
простенько. Так как у меня x это решение, то что я могу сказать про градиент вот такой вот функции?
Точки x. Почему он равен? Нулю. Просто потому что это же как раз x у меня решение вот этой
задачи. Он равен нулю. Давайте я выпишу этот градиент. Чё там у нас было? Ро, С и, соответственно,
от логарифма. Старый совсем стал. Давайте я представлю просто. Вот. Ро, С плюс вот так вот. Градиент
равен нулю. Вот. Так. Ну вынесли вектор А, а дальше логариф продиференцировали. Так. Получилось что-то
вот такое. Дальше я вот это всё безобразие скалярно домножаю на вектор. Ну это же у меня вектор
получился. Который равен нулевому вектору. Я домножаю на x минус x со звездой. Вот. Когда я
домножу, у меня получится Ро, С транспонированное x минус Ро, С транспонированное x со звездой равно
равно a it транспонированная сумма тут. А здесь x со звездой минус x делить на b it a it транспонированная
x. Вот. Что я могу сказать про a it транспонированная на x со звездой? x со звездой это решение исходной
задачи. Что я могу сказать про a it транспонированная x со звездой? b it. Потому что это же ограничение.
Вот у меня было ограничение b it равно a it x. Так. Вот. Поэтому в решении оно должно быть как бы удовлетворять
ограничением. Вот это будет просто b it. Поэтому вот здесь вот я запишу следующее. b it минус a it
транспонированная x. b it минус a it транспонированная x. Итого это равно просто m. Ну, drop
равна 1. Суммирую от 1 до m, получаю просто m. Вот. Оказывается, для логарифмического барьера вот
такого вида m еще и равна i µ. Вот. Константия вот этой самосогласованности. Вот. Откуда я
получаю, что c и транспонированная x минус c транспонированная x со звездой равно µ делить на
ρ. µ делить на ρ. Вот. А что такое транспонированная x? Это же просто у меня f от x. Так. Потому что у меня
целевая функция линейная. А это f от x со звездой. Вот. И получается, что у меня расстояние между f от
x и f от x со звездой на текущей итерации. Это просто µ делить на ρ. µ делить на ρ. Вот. Но это при условии,
что я нахожу точное решение каждый раз. Что у меня x это точное решение. Опять же обрабатывать
случаи, когда решение не точное, как у нас, что оно там с какой-то точностью близко. Ну, технические
вещи, которые, ну, пропускаем. Просто вы достоверились, что у меня вот так вот меняется. Но здесь что видно?
Я же ρ теперь начинаю уменьшать с геометрической прогрессией. Я начинаю少 reign делать меньше,
меньше, меньше, меньше, меньше. Вот. Оно у меня падает со скор Control... увеличивается со
скоростью геометрической прогрессии, а значит расстояние до решения уменьшается со
скоростью геометрической прогрессии. Так. Всё. Это то, что нужно было. Вот. Получается, что.. это мы выразили..
получается итоговое решение так как у меня коэффициент геометрической прогрессии это что-то
в порядке там 0.8 делить на корень изню вот то мне нужно просто прикинуть за сколько там
итераций вот это у меня там к-1 будет равно епсилон ну и беру логарифмирую и получаю
вот он влезает параметр самосогласованности барьеров вот 0.8 понятно я просто вот ку
засуду ну и логариф мы быть то есть получается линейная сходимость к решению вот
и видно что параметр самосогласованности писался вот параметр самосогласованности
играет ключевую роль чем больше параметр самосогласованности тем ус Canyon метод сходится
медленнее, но скорость при этом все равно линейная. Единственное, конечно, расплата
за это — это использование метода Ньютона на каждой итерации.
Но при этом все равно классная скорость по сравнению с тем, что мы
получили на первой части лекции. Тут прям классные гарантии для любой задачи
выпуклой оптимизации, линейная скорость сходимости, но за счет того, что вы когда-то
считаете десианы. Все, спасибо большое. Я надеюсь, что вот этот классный результат
понравился, потому что, ну, вот это, наверное, такой труд, на котором Аркадий
Семенович и Юрий Евгеньевич сделали себе имя и карьеру, в том числе, потому что, ну,
нестеровский метод, тот же, который про ускорение, ну, он долгое время был непонят
комьюнити, просто потому что, ну, как-то популярностью не пользовался, а потом,
когда это выстрелило в нейронных сетях, сразу стал популярен. А вот это, соответственно,
применяется в куче разных бизнесовых задач от экономики до распределения ресурсов и так далее,
просто потому что задача с ограничениями — это же ровно ресурсы. Ограничение на ресурсы,
какая-то целевая функция, это ваш, это прибыль, вот. И сразу же в селверах этот метод стал суперклассно
работать, просто потому что, чтобы решать эти бизнесовые задачи, как распределить ресурсы,
как распределить деньги, как распределить время, чтобы, соответственно, заработать как можно больше.
Вот. Ну и поэтому вот этот метод, наверное, такая вот прям одна из коронных фишечек Аркадий
Семеновича и Юрия Евгеньевича. Вот. И этот результат прям, наверное, ключевой. Вот. Всё. Спасибо большое.

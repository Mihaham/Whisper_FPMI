Да, давайте тогда с организационного момента и начнем, я хотел в перерыве это сказать,
но раз вопрос возник сейчас, то давайте разберемся. На следующей неделе все семинары будут проходить в
формате онлайн. Это будут семинары по выбору, 7 разных спикеров на разные темы вокруг оптимизации,
в том числе какие-то прикладные вещи, то есть часть семинаров прочтут ваши классические семинаристы,
но по темам, которые выходят за пределы курса, а часть ребят это приглашенные ребята, в том числе,
например, команда Яндекс.Ресеча, которая занимается распределенной оптимизацией,
будут одних семинаров. То есть в течение недели, там вечером в понедельник, вечером во вторник,
среду, четверг, пятницу, иногда по два семинара в день, можно посетить один семинар, который наиболее
интересен. И вот на этом семинаре, в конце либо в начале, будет проведен последний двенадцатый тест в
формате онлайн. Это будет какой-то такой вот мини-контест. Вы как раз сейчас на этой неделе
проходите библиотеку CVXPy, ну и вот, соответственно, по этой библиотеке у вас и будет последняя вот эта
работа на семинаре. И она будет в формате онлайн такого небольшого, совсем быстренького контеста.
Решите одну небольшую задачку. Вот, как-то так, как-то так. Сегодня форму для записи на эти семинары
выкачу. Нужно будет выбрать один семинар как обязательный, ну и если хотите посетить больше,
выбираете еще один тем семинар, может записаться. Тесты пишутся один раз, в итоге считается по
худшему результату. То есть, если хотите написать два, будем считать по худшему. Поэтому пишите один раз,
ну там, на первом семинаре, который посетили или там, если хотите посетить три семинара, хоть на
последнем, пишите. Главное один раз. Задачи будут отличаться от семинара к семинару. Вот, как-то так.
Это, соответственно, по тому, как будут устроены семинары на следующей неделе. А через неделю они
тоже будут в формате онлайн, только уже в качестве небольшой такой конференции от тех, кто разбирает
статьи, от тех, кто разбирает статьи. Что еще? Что еще? Ну, как-то так. А лекции у вас до конца, вот еще у нас
осталось, кроме этой три лекции, в таком формате они продолжаются. Ну и получается, на неделе до зачетной
у вас начинаются уже колоквиумы. Большие группы разбиваются на две части и семинарист, плюс какие-то
приглашенные гости у вас принимают колоквиум. На зачетной неделе и за неделю до этого. Как разбить
группу на две части, ну это вы сами внутри группы решаете. Кто пойдет раньше, кто пойдет позже. Программу
колоквиума я до воскресенья постараюсь выкатить. Окей, это по организационным вопросам. Ну тогда
поехали разбираться с сегодняшней темой. На семинарах вы уже в некотором смысле освоили аппарат,
который нам сегодня будет нужен. Но я вас сразу же спрошу. Является ли функция модуль x выпуклой?
Да, безусловно, она является выпуклой. Но возникает вопрос, является ли она дифференцируемой и гладкой.
Такими функциями мы работали до этого все лекции. Является ли она дифференцируемой и гладкой?
Пусочно. Она является дифференцируемой везде, кроме точки ноль. Там возникают проблемы. Но
выполнена ли гладкость даже вне точки ноль? Давайте, например, возьмем что-то такое. Ну и если у нас
x и y брать из окрестностей 0, 1 справа, 1 слева, то это разница всегда будет равняться двоечке.
Это разница всегда будет равняться двоечке. И тут нету ничего такого замечательного в
духе того, что тут ограничена какая-то константа l умножить на норму. К сожалению,
вот такие проблемы возникают в связи с тем, что даже самые простые выпуклые функции не всегда
являются дифференцируемыми и не всегда при этом являются еще и гладкими. В связи с этим мы будем
рассматривать на этом семинаре вот такое ограничение на функции. То есть выпуклость мы оставляем,
но убираем гладкость, добавляем липшицевость в самой функции, с которой мы в принципе уже сталкивались
на первом семинаре, когда разбирали сложные задачи оптимизации не выпуклой для липшицевых
функций. То есть тут суть такая, что у нас теперь просто значение функции будет вот так вот ограничено,
некоторые константы m, на приращение аргумента. На самом деле вот это все,
это определение, как и в случае гладкости, можно обобщить на ситуацию, когда у вас функция задана
не на всем пространстве rd, когда вы работаете на каком-то там ограниченном множестве x, хотя бы
потому, что если мы говорим вообще про сильно выпуклые задачи, при этом которые целевые функции,
которых являются еще и m липшицевыми, вообще таких функций не существует на rd. Почему? Кто понимает,
почему не существует на rd таких функций? Да, все правильно. То есть липшицевость предполагает,
то что у вас функция сверху ограничена линейной, то есть растет не быстрее, чем линейная на
бесконечности. Если у вас функция сильно выпуклась, вам утверждают о том, что у вас
функция растет не медленнее, чем квадратичная на бесконечности, ну и с этим у вас, понятно,
возникает противоречие. И поэтому, понятно, чаще всего, конечно, такие задачи рассматриваются на
каких-то ограниченных множествах, на каких-то выпуклых компактах, но сегодня, чтобы не
загромождать анализ всеми этими проекциями и так далее, будем рассматривать пока rd. Понятно,
это все более чем легко обобщается тем аппаратом, с которым мы с вами уже знакомы. Так, это мы с вами
обсудили. Ну хорошо, если у нас функция не дифференцируема, градиента нету, что может
существовать место градиента? Субградиент, с которым вы, соответственно, познакомились на
предыдущем семинаре. Вот его определение. Отмечу, напомню скорее о том, что субградиент вообще
определяется только для выпуклых функций и как раз исходя из определения выпуклых функций,
потому что вот здесь вот у нас обычно стоял градиентик, градиентик в точке x, вот, вместо
g. Ну а теперь мы, соответственно, мы ставим туда на вектор, который будет продолжать для
выпуклых функций удовлетворять вот этому определению выпуклости. Теперь, соответственно,
функции не дифференцируемы, градиента нет, мы рассматриваем все возможные такие же. Ну
и все возможные g в этой точке X, которые удовлетворяют этому соотношению выпуклости,
называется субдифференциалом, то есть, множество субградиентов в этой точке называется
с субдифференциалом. С этим вы познакомились, как искать субдифференциалы, их свойства вы уже знаете.
Вы уже знаете. Хорошо, ну давайте тогда обсудим, как конструировать нам методы, но перед тем, как конструировать
методы, давайте разберемся с условиям оптимальности. Оптимальности выпуклых функций, которые, соответственно,
у нас уже являются в общем случае недиференцируемой и негладкой. Условие оптимальности выглядит
следующим образом. х звездой это у нас минимум выпуклой функции f. Здесь, понятно, слово глобально пропущено,
потому что вроде как договорились пропускать это. Тогда и только тогда, когда у нас 0 принадлежит
субдифференциалу точки х звездой функции, субдифференциала функции f точки х звездой.
Окей, давайте посмотрим, что тут можно сделать. Ну давайте в сторону, сначала в ту докажем, вправо,
то есть у меня 0 принадлежит субдифференциалу. Что я тогда могу сделать? Опять же, по определению
выпуклости я пишу следующее. Так g х-х со звездой. Согласны здесь? Это просто из определения
выпуклости g принадлежит субдифференциалу d f х с звездой. Просто из предыдущей вот этой строчечки.
Так? Вот. Но так как это в g у меня лежит, в том числе, в качестве g я могу брать 0,
в качестве g я могу брать 0, поэтому здесь я в качестве g этот 0 и кладу, и у меня здесь получается
просто f от х со звездой. Ну и что получается? Получается ровно то, что нам нужно определение
глобального минимума функции. Глобальную минимум функции для любой точки х, которую бы я не
рассмотрел, у меня значение в точке х будет больше либо равно, чем значение в точке х со звездой.
Это мы вам в ту сторону доказали. Теперь давайте докажем в обратную сторону. Пусть у меня x это
глобальный минимум. X со звездой это глобальный минимум. Когда у меня выполнено вот такое
соотношение для любого х? Для любого х. Хорошо. Хорошо. Тогда как я могу воспользоваться выпуклостью?
Как я могу воспользоваться выпуклостью? Ну давайте посмотрим. Если у меня выполнено это соотношение,
то тогда и выполнено и вот это соотношение. 0 х минус х со звездой. Согласны? Вот. Просто 0 к умной
добавляю. Ну а значит отсюда у меня ровно следует, что у меня 0 лежит в суб дифференциале ровно из
определения суб дифференциала, потому что вот это выполнено для любого х, как мы только что
сказали. Все. Получается, что условие оптимальности очень простое. 0 должен лежать в суб дифференциале
нашей функции. Вот. Хорошо. Хорошо. Здесь это все доказано. Так. Теперь хочется доказать следующее
свойство. То есть липчество. То есть это конечно хорошо. Вроде бы понятны свойства. Не быстрее, чем
линейно растет. Но можно доказать эквивалентную вещь. Оказывается, что у нас функция является
выпуклой тогда и только тогда, когда все суб дифференциалы, все суб градиенты этой функции ограничены.
Ограничены. В принципе, с чем-то похожим мы сталкивались и когда говорили про гладкость, у нас
тогда был как липшицевость градиента и ограничивалась уже вторая производная. То есть
гисян. Здесь получается похожая ситуация в силу того, что липчество сама функция, то ограничен ее
градиент. В данном случае субградиент. Ограничен субградиент. Ну хорошо. Давайте попробуем это доказать.
Доказываем сначала. Получается со стороны у нас есть выпуклость и липчецевость. Хотим тогда показать,
что у нас будет ограничен субградиент. Будет ограничен субградиент. Окей. Давайте попробуем
выписать как-то нашу выпуклость, так как у нас только кроме выпуклости в определении суб дифференциала
ничего и нету. Ничего и нету. g y минус x. Так, ну давайте я чуть по-другому это. Так? Чуть-чуть. Чего-чего?
x минус y. Ну давайте вернемся на всякий случай. Так, как я поставил там в определении? А, f от y?
Да, все правильно, все правильно. Хорошо. Это про это молодцом. Так, вот так вот. Хорошо, хорошо. Ну и что я тогда
знаю про это выражение g? У меня суб дифференциал в какой точке? Субградиент в какой точке? x или y?
Хорошо. Хорошо. Ну тогда это мне должно выполняться для любого x. Согласны? Для любого x, в том числе для
x, который равен x, равен y плюс g. y плюс g. Окей. Смотрим, что получается. Меньше либо равно, чем f от y.
А здесь, когда я подставлю, соответственно, y минус g, y плюс g вместо x, что у меня получится?
Скалярное произведение двух g. А это просто, ой, это просто у меня будет норма g в квадрате.
Евклидова норма g в квадрате. А теперь я могу что сказать? Что у меня g в квадрате меньше либо равен, чем f от x
минус f от y. Пользуюсь чем? Липшицевостью функции и получаю, что у меня m x минус y евклидова норма
евклидова норма. Ну а дальше я подставляю снова x, который мне равен x плюс g. И здесь получается m g
евклидова норма. Ну и в итоге получается, что евклидова норма моего субградиента ограничена
в силу того, что мы брали произвольный субградиент в произвольной точке y, это будет выполняться всегда.
Хорошо, все, тут доказали, в эту сторону понятно. Так, так, так.
Окей, давайте будем действовать в обратную сторону. Снова записываю определение того, что у меня g это
субградиент, что у меня тогда будет g здесь, это из субдифференциала.
Так, и здесь я хочу со знаком минус, то есть вот тут я брал со знаком плюс, тут я брал со знаком плюс, вот здесь вот
это выражение брал со знаком плюс, я теперь хочу его взять со знаком минус, поэтому мне нужно поменять местами y и x.
Минус y, минус x. Так, x минус y, сейчас.
x минус y должно что ли быть? Нет, только что у нас было x. Было вот так, x минус y.
Нет, f-то я оставил здесь такую, y. Я вот здесь только знак поменял. Так, все норм?
А, и да, я поменял еще тут местами, да. Окей, вот так.
Господи, что-то у меня сегодня выпукло, что бы уже проблемы. Так, так норм? Вот, так норм.
Хорошо, так норм. Что у нас известно? У нас известно, что наш субградиент ограничен,
субградиент ограничен, ну тогда что у нас получается? Я потаскаю это теперь
чуть-чуть в разные стороны. Сколько у меня тут y минус x? Переношу вправо, у меня здесь получается f от y
минус f от x. Ну а дальше что? Дальше я делаю Кашибуниковского здесь, у меня получается g
Евклидова норма, y минус x Евклидова норма. Вот, Евклидова норма. Ну и получается, что у меня
f от y минус f от x меньше либо равно, чем g, так, y минус x. Понятно, что я могу поменять x и y
местами и получить, что у меня f от x минус f от y меньше либо равно, чем g, y минус x. Вот.
Тогда я ставлю модуль и получаю то, что от меня и требовалось. Так. Пам-пам-пам. Так. Окей.
Ну теперь давайте поговорим про метод. Поговорим про метод. В принципе, то есть мы рассматриваем какую-то
негладкую задачу. Функция у нас является выпуклой, m липшицовой. Понятно, в общем случае там нет никакой
гладкости, поэтому градиента нету. Ну и довольно простое обобщение, простая модификация, как
выкрутиться из этого случая. Давайте тогда вместо градиента буду брать субградиент. Какой-то
субградиент из моего суб дифференциала в текущей точке x. Как вы знаете, у вас если функция все хорошо,
непрерывно дифференцируемая, там будет просто градиент стоять, он единственный. Более того, не знаю,
обсуждали ли на всех семинарах это или нет, суб дифференциал у вас для выпуклой замкнутой функции
существует всегда на внутренности. На внутренности множество определений этой функции. То есть в принципе,
если могут возникнуть проблемы с существованием суб дифференциала, что он будет пуст, то только на
каких-то границах множества, на котором определена функция. То есть в принципе суб дифференциал
существует, и мы оттуда можем какие-то вытягивать векторы. Ну и соответственно в качестве теперь
градиента мы берем субградиент. Окей? Да, да, да, да. Ну это ровно так же, как и в случае вычисления
градиента. Автодифференцирование, это правда, да. Так, окей, давайте разбираться. Давайте разбираться
здесь, как доказывать. В принципе, я делаю то же самое, что мы делаем с вами обычно. Расписываю
расстояние до решения. Расписываю расстояние до решения, и дальше расписываю то, что у меня вот
здесь вот происходит. Что у меня здесь происходит? Здесь у меня вынес текущее расстояние,
скалярное произведение и норма в квадрате. Дальше я пользуюсь чем? Я знаю то, что у меня
субградиенты ограничены, поэтому я могу ограничить это безобразие, которое у меня здесь написано
гаммой на m в квадрате. Гамма на m в квадрате. Плюс про вот это что-то я знаю тоже. Что я про это знаю?
Про скалярное произведение субградиента на x минус x звездой. Знаю, что у меня вот этот вектор
лежит в суб дифференциале. Точки xk в нашей функции. Что я могу сказать про скалярное произведение?
Как я его могу оценить?
Я могу его оценить через выпуклость. Я могу его оценить через выпуклость f от x звездой
минус f от xk. Просто опять же через определение субградиента функции в точке xk.
Получаем следующую оценку. xk плюс 1 минус x звездой. Это вторая вещь, которую мы оценили
по выпуклости.
xk минус x звездой в квадрате в квадрате плюс 2 гамма f от x звездой
минус f от xk плюс гамма в квадрате m в квадрате.
Теперь делаю небольшую перестановку. Вправо переношу f. Нормы x переношу наоборот влево.
Ровно то же самое, что мы делали в выпуклом случае. Получаю вот такое вот.
Что дальше делать? Просуммировать по всем k и усреднить на всей итерации.
Так, окей, давайте это сделаем.
Просуммировали. Здесь у меня останется нулевое минус последнее.
Что довольно приятно делить на k. Ну а когда я суммирую до t г в квадрате m в квадрате,
потом усредняю, у меня оно же и остается. Просто потому что я k раз просуммировал и потом разделил на k.
Хорошо, здесь что еще можно убрать? Можно убрать вот этот кусочек, потому что я пишу оценку сверху,
а он не положительный, потому что берет со знака минус. Ну и в принципе, а, ну еще гамму то забыл,
гамма в квадрате m в квадрате. Вот, и что у меня в итоге получается? Что у меня в итоге получается?
Так, давайте вот здесь у меня все уже просуммировано. Вот, получилось следующее выражение, я его,
здесь давайте его и оставим. Вот такое вот выражение. Как прийти к какому-то критерию сходимости?
Потому что в левой части у нас стоит что-то странное. Вот, как мы приходили к критерию сходимости в левой части?
Да, ну на самом деле в выпуклом случае, когда мы гладки рассматривали случай, там можно вообще показать то,
что у вас f от xk меньше чем f от xk минус 1. В гладком выпуклом случае справедливо вот такая вот вещь.
Поэтому здесь можно было вообще обойтись без Янсона и сказать то, что у вас значение функции монотонного
бывает, поэтому можно брать сходимость по последней точке. Но это в гладком случае. Здесь такое вы себе
позволить не можете. Хотя бы потому что, ну давайте представим опять же нашу функцию модуль.
Нашу функцию модуль. Ну и в случае, когда у вас там все дифференцируемо бы было, вы бы оказались в нуле,
производная в минимуме равна нулю, и вы из нее не уйдете. Последняя точка, она и остается последней.
Сейчас мы говорим то, что вот оказавшись даже в нуле, мы должны взять что? Какой-то вектор из субдифференциала,
который у вас по факту может оказаться ни разу не нулевым. Вы можете взять какой-то случайный вектор.
Кстати, с какого значения до какого у модуля? От минус одного до одного и соответственно чуть-чуть
выйти за пределу, ну то есть из этого нуля выйти. Поэтому никакой сходимости по последней точке у вас здесь
не будет. Это такая естественная вещь, поэтому ее здесь искать и не нужно. Просто потому что
субдифференциал это такая неприятная вещь, которая у вас даже находясь в оптимальной точке, может из него
выпнуть. Поэтому будет сходимость только по средней точке, и как вы правильно сказали, мы вот это
с вами оцениваем по Янсону, по Янсону, потому что f у вас выпуклая функция, ну и тогда мы это можем
оценить как f по средней точке. Вот так вот. Согласны здесь? Вот, получили вот такую вот сходимость,
еще называется органическая сходимость по средней точке. Вот, дальше возникает вопрос, как подбирать
в правой части шаг, потому что на меня есть и в знаменателе, и в числителе. Возьму очень маленький
шаг, сходимость пропадет, возьму очень большой шаг, она тоже снова пропадет, потому что в одном случае
будет расти вот этот кусочек, левый, когда я буду уменьшать гамма, а когда буду увеличивать гамма,
будет расти правый кусочек. Нужно найти что-то, в некотором смысле, какой-то баланс между вот этими
двумя вещами. Как его найти? Да, здесь соответственно это можно все безобразие оптимизировать.
Как это оптимизировать? Чему будет равен оптимальный шаг? Ну, давайте возьмем эту производную,
ну что здесь будет? x0-x звездой в квадрате в квадрате, минус 2 гамма в квадрате k, плюс m в квадрате 2,
равно нулю. Ну, ищем отсюда гамму, ищем отсюда гамму, гамма получается x0-x звездой делить на m
к корень. Согласно, просто выразил отсюда гамму, просто выразил отсюда гамму, ее можно соответственно
подставить, можно подставить выражение, здесь мы подбираем шаг, ну и подставляем выражение, получаем вот такую вот сходимость.
Понятно, что такой подбор шага не самый практичный, потому что вам нужно знать расстояние до решения,
что не самая очевидная вещь, если вы знаете расстояние до решения, то скорее всего вы знаете,
где оно расположено, знать константу липшица функции, но горизонт итерации это уже не так страшно,
поэтому в реальности, когда вы решаете негладкие задачки, можно применять просто шаг 1 делить на корень из k,
1 делить на корень из k и соответственно получать точно такие же оценки сходимости, точно такие же оценки сходимости.
Корень из количества итераций. Нет, вот здесь, вот здесь можно брать как раз по номеру итерации.
Вот, более практичная вещь, потому что в принципе на практике вы не знаете ни расстояния до решения,
ни m, ну и горизонт итерации вы тоже в некотором смысле не задаете, вы же просто говорите, я запускаю метод,
хочу, чтобы он сошелся до какой-то точности, поэтому просто можно брать уменьшающийся шаг.
В принципе, вы это и пробовали, даже для гладких задач эта вещь работает, ну и для негладких тоже.
Для негладких тоже. Так, вот здесь соответственно дана теорияма сходимости, вот такая вот оценка.
Получается, для выпуклой задачи 1 делить на корень из k, но если в терминах епсилон, то будет епсилон в квадрате.
Понятно, что вот вы кладете равным епсилон и отсюда выражаете k, вылезет епсилон в квадрате.
Кто помнит, какая оценка справедлива для гладкого случая?
Да, для градиентного спуска.
Это сильно выпуклый, да? Сильно выпуклый. Сильно выпуклый, там у нас вообще линейная сходимость.
Линейная сходимость в духе 1, ну давайте так вот опишу, q в степени k.
А если выпуклый случай, какая будет сходимость в гладком?
О, 1 делить на епсилон или 1 делить на k? 1 делить на k, если мы говорим в терминах k.
Тогда что у нас получается? Получается, что оказывается, вот негладкий случай оказался в этом плане сложнее, чем гладкий,
потому что здесь у нас в выпуклом случае сходимость 1 делить на корень из k, что медленнее, чем 1 делить на k.
В выпуклом случае у вас будет оценка для негладкой задачи порядка 1 делить на k, что понятно хуже, чем линейная сходимость в гладком случае.
Получается, что негладкий случай в данном случае просто сложнее, просто сложнее вычислительно.
Ну и давайте быстренько просуммируем итог. Просуммируем итог. У нас получилось обобщение градиентного спуска на негладкие задачи.
С помощью применения субградиента вместо честного градиента.
То, что мы с вами сейчас обсудили, у нас сходимость стала хуже по сравнению с гладким случаем.
Еще давайте такой вопрос. Как думаете, можно ли улучшить результат в этом случае? Будьте здоровы.
Оказывается, нет. То есть, если в случае гладкой задачи у нас можно было ускорить градиентный спуск с помощью моментных членов,
в данном случае для негладких задач нижние оценки говорят о том, что градиентный спуск, субградиентный спуск оказывается оптимальным методом.
И лучше, чем он ничего и не придумать. Ну вот такая вот ситуация, соответственно.
Ну, с ней приходится жить. Получается, что вот здесь оптимальный метод довольно простой.
А в невыпаклом случае, что вы мне скажете? Там-то у нас вообще было возможно было ускорение в гладком случае или нет? Кто помнит?
На самом деле, да, это, кстати, интересное замечание.
Интересное замечание, потому что в реальности, то есть, использование субградиентного метода вам не ускорит в оптимальном случае решение негладкой задачи.
Ну, опять же, если вы учитываете какую-то дополнительную специфику задачи, например, то, что на малой размерности можно решать эту задачу быстрее, быстрее методами нулевого порядка.
Методами нулевого порядка разного рода. То есть некоторые мы посмотрим через лекцию. Не на следующий, а через одну.
А есть еще всякие методы эллипсоидов, методы плоскостей. И они тоже решают маломерные задачи, в том числе негладкие.
И в некотором смысле успешнее чем субградиентный спуск.
Ну, в общем случае, оценка субградиентного спуска не улучшаема.
А в невыпуклом случае, как вы помните, в некотором смысле мы закольцовываем историю. В гладком вы можете гарантировать сходимость какой-то стационарной точки.
А в негладком случае, ну, мы с вами разбирали. На самой первой лекции лучше чем полный перебор, придумать ничего нельзя.
То есть невыпуклый, негладкий случай это довольно такой неприятный случай, неприятный кейс.
Даже когда функция у вас является липшицовой.
Окей, окей.
Так, ну и то, что я говорил в принципе в начале, то, что для выпуклых и сильно выпуклых задач, ну, особо рассматривать негладкие задачи на неограниченных множествах не имеет смысла.
Чаще всего эти задачи возникают в виде какого-то, ну, на каком-то ограниченном множестве, возможно хорошем, с которыми с вами уже взаимодействовали.
Поэтому здесь можно применять и методы проекции, методы проекции, либо вот соответственно зеркальный спуск, где вы вместо евклидового расстояния используете дивергенцию Брегмана.
К сожалению, специфика Франко Вульфа, если мы говорим про его для простых множеств, не позволяет вообще доказать для негладких задач что-то хорошее.
Не позволяет доказать что-то хорошее просто потому, что вот такой вот метод, который у которого доказательство сильно опирается на наличие гладкости, на наличие гладкости.
Окей, так, ну давайте быстренько перейдем к следующей теме. Сколько у нас прошло от лекции? Полчаса.
Давайте перерыв сделаем, а потом уже к следующей теме тогда.
Давайте продолжать. В общем-то мы поняли то, что в принципе гладкие задачи в некотором смысле с точки зрения теории сложнее, ой, негладкие задачи сложнее, чем гладкие.
Оценки исходимости методов хуже, никакого ускорения нету, поэтому вообще взаимодействовать с какими-то негладкими вещами получается хотелось бы избегать.
Вот, и в некотором смысле, ну ту негладкость, которая есть в задаче, можно было бы как-то в некотором смысле спрятать под ковер, спрятать под ковер.
И чтобы сделать это, давайте ведем вот такой вот объект, который называется проксимальным оператором.
Проксимальным оператором пусть у меня есть функция R, которая у меня в общем случае негладкая, тогда проксимальный оператор для нее определяется следующим образом.
Это вот такое вот выражение. Такое вот выражение. Почему оно нам именно в таком виде нужно будет, мы поймем чуть-чуть позже.
Мы поймем чуть-чуть позже, но тут предполагается, что вот такой проксимальный оператор мы можем считать бесплатно.
Бесплатно для функции R. А что будет, если мы его можем в некотором смысле считать только платно?
Ну, обсудим в самом конце.
В общем, пока предполагаем, что вот если существует проксимальный оператор определяется следующим образом, мы его можем считать бесплатно.
Что вы можете сказать про функцию, которого у вас стоит под проксимальным оператором?
Что вы можете сказать про функцию, которая superst feature под проксимальным оператором если R является выпуклый?
тоже выпукла не просто выпукла какая она сильно выпукла да а что мы знаем про
сильно выпуклые задачи кто помнит что у них про про решение сильно выпуклых задач мы знаем
да у них она единственная причем еще и уникальная вот получается что можно доказать что у нас
проксимальный оператор проксимальный оператор это однозначный оператор то есть он не возвращает
многозначные значения единственное что нужно дополнительно предположить что хоть в какой-то
точке функция r не принимает значение плюс бесконечность если она всегда плюс бесконечность
то там смысла вообще в проксимальном операторе нет у нас просто всегда равен там значение под ним
всегда равно плюс бесконечности вот никакого минимума там искать не получится если у вас
какой-то точки есть конечное значение то понятно что тогда уже работают все эти вещи которые
мы с вами обсуждали про сильно выпуклые функции и у нее существует единственное решение поэтому
оператор всегда возвращает одно число проксимальный вот это соответственно здесь у меня и формализована
окей так почему вообще хочется биться за проксимальный оператор ну хотя бы потому что
для каких-то не гладких функций он определяется в замкнутой форме то есть можно просто выписать
в явном виде в явном виде но вот соответственно пример l 1 нормы l 1 нормы по факту это же это
пример просто вашего модуля просто в одном случае многомерный многомерный и там легко удостовериться
что проксимальный оператор будет вычисляться следующим образом причем в некотором смысле так
сепарабельно по каждой из координат он считается отдельно по каждой из координат он считается
отдельно хорошо и на самом деле вот часто вот такой проксимальный оператор ну вот с таким с
такой функцией r которая определена первой нормой первой нормой называют threshold threshold
почему потому что нужно посмотреть как это все безобразие безобразие выглядит
выглядит у вас на графике у вас на графике ну давайте посмотрим посмотрим со срезками
мы с вами работали да со срезками с вами работали ну вот давайте что в нулест происходит с этой
функции со с этим значением prox r x и так что в нуле ну 0 просто сигнум равен нулю тут четко
0 если чуть-чуть это иду от нуля но не за пределы лямбды не за пределы лямбды что тогда будет
происходить с значением функции вот в этой точке да потому что смотрите у меня по модулю x будет
значение будет меньше чем lambda значение будет отрицательным а срезка у меня как раз дело то
что если значение отрицательно она его просто в 0 переводит поэтому я пока не дойду до лямбды
так не дойду до лямбды мне здесь просто 0 у меня здесь просто 0 а потом соответственно функция
начнет подниматься вот так вот линейно и абсолютно симметричная ситуация здесь абсолютно симметричная
до минус лямбды значение отрицательное, а потом начнет это все успешно подниматься наверх,
то есть уже начинает приобретать положительные значения. Хорошее свойство, на самом деле,
мы его обсудим чуть попозже, когда дойдем до метода, который использует проксимальный оператор.
То есть пока запомним то, что проксимальный оператор по такой вот функции помогает отсекать
в некотором смысле какие-то значения координат, которые меньше порогового значения лямбда. Поэтому
и трешхолдом называется пороговое значение лямбда, вы по нему отсекаете координату. Если она,
соответственно, ниже этого порогового значения, вы ее тупо зануляете, тупо зануляете. Это хорошее
свойство, но чуть попозже. Для квадратичной нормы в квадрате, для эвклидовой нормы в квадрате,
проксимальный оператор определяется вот так вот. И оказывается, что для индикаторной функции
множества х, проксимальный оператор тоже можно определить. Как вы думаете, чему он будет равен?
Давайте я выпишу. Прокс, R, X. А? Чему-чему? Правильно, проекции. То есть, давайте я
для тех, кто, для остальных выпишу, у нас все получается вот так вот. Так, вместо R я пишу индикатор,
и здесь будет одна вторая х в квадрате. Понятно, что у меня вот этот арг-минимум определен не на
все множестве R, Rd, потому что просто индикатор у меня улетает в бесконечность за пределами. Поэтому
это же просто эквивалентно арг-минимуму на множестве х. На некотором выпукло множестве х,
а это у нас, ну, так как это арг-минимум, я еще одну вторую тут уберу, одну вторую уберу. Но это же
ровно с евклидовой проекции. Оказывается, проксимальный оператор в некотором смысле это
вообще не евклидовой проекции. Существует довольно много различных результатов для
проксимальных операторов, частных случаев, как они преобразуются, когда вы работаете
суммами функций, когда вы работаете сепарабельными функциями, когда вы делаете какие-то аффинные
преобразования внутри аргумента. Довольно много существует результатов, как считать проксимальные
операторы. Как много считать проксимальные операторы в частных случаях. Но та фишка, которая нам нужна,
это то, что проксимальный оператор, ну, вот, например, здесь в некотором смысле может вам скушать за
бесплатно негладкую функцию. Негладкую функцию, то есть здесь у вас была какая-то вроде бы плохая
функция негладкая, которая там не дифференцируема в нуле, вот, но проксимальный оператор ее умеет
обрабатывать за бесплатно. В явном виде, в замкнутой форме он выдает вам ответ. Где, соответственно,
это можно использовать, ну, поймем чуть попозже. Проекцию мы с вами поняли. Окей, давайте
подоказываем пока некоторые свойства проксимальных операторов, чтобы в дальнейшем уже с ними успешно
взаимодействовать. Ну, первое свойство, следующее, опять же, у нас r выпукла функция, для нее определен
проксимальный оператор. Тогда для любых x и y следующие условия эквивалентны. y у нас равен
проксимальному оператору функции r в точке x. Тогда и только тогда, когда у нас x минус y,
ой, только тут r, r и не f, вот, когда у нас x минус y принадлежит суб дифференциалу точки y,
точки y, вот. Ну как, откуда это взять? Проксимальный оператор, вспоминаем, это опять же у нас задача
некоторой минимизации. Только что мы это выписывали, давайте выпишем еще раз. Некоторая задача
минимизации. Для этой задачи мы с вами что, соответственно, знаем? Что мы про нее знаем?
Прокс, ну давайте тут x tilde уже буду ставить. Что мы для нее можем выписать для этой задачи?
x, r. Условие оптимальности. А как оно будет выглядеть? Как оно будет выглядеть для вот этой задачки,
которая у меня стоит под argmin? Как она будет выглядеть? Условие оптимальности. Ноль лежит
суб дифференциале вот этой функции r от x плюс, тут только с tilde. Вот, давайте я сразу поставлю
оптимальное. r x минус x и здесь x оптимальное. А нет, еще суб дифференциал я не взял. Вот,
одна вторая, x, ну давайте вот так. Вот так запишу. Суб дифференциал того, что у меня здесь записано,
минус x в квадрате. В точке получается x tilde равно x звездой, которое, ну вот решение
проксимального оператора. Решение проксимального оператора, ну или я могу сразу тут y написать,
у меня же y это решение проксимального оператора. Окей, так, чему равен суб дифференциал? Давайте
выпишем dr точки y плюс y минус x. Согласны? Ну и тогда что у меня получается? y минус x,
так, с минусом, принадлежит dr точки y. Согласны? Просто перенес влево. Вот, вот это же вектор
у меня, я его могу вычесть из правой и из левой части. Окей, что получается? Что получается? Ну то,
что нужно получается, то есть вот отсюда сюда действует. Но так как у меня условия оптимальности,
это вообще критерий, то получается то, что у меня x минус y принадлежит суб дифференциалу,
эквивалентно тому, что на самом деле у меня, и вот это будет правда, в силу того, что у меня вот
это критерий, условия оптимальности, оно действует в обе стороны. Поэтому первое и второе вещи
эквивалентные. Осталось показать, ну давайте, эквивалентность второго и третьего. Второго
и третьего здесь мы показали, что первое и второе эквивалентные. Теперь осталось показать
эквивалентность второго и третьего. Что у нас тут для произвольного z? Ну, что тут похоже?
Похоже на что тут похоже, но раз для произвольного задания, видимо, надо писать какую-то выпуклость.
Какую-то выпуклость, она здесь в некотором смысле и записана, вот.
Соответственно, если я знаю, что у меня х-у лежит в субдифференциале R, тогда,
так как х-у это у меня субградиент, для него должно быть выполнено условие выпуклости.
Согласны? Просто из определения субградиента и субдифференциала, субградиент и субдифференциала.
В обратную сторону, что я могу сказать, в обратную сторону у меня для любой точки z выполнено следующее,
х-у, z-у меньше либо равно, чем R от z, минус R от y, для любой точки z.
Получается, что вот тогда у меня х-у просто по определению лежит в субдифференциале точки y.
Согласны? Все. То есть тут довольно просто.
Получается, все эти три условия эквивалентны.
Все эти три условия эквивалентны.
Вот. На самом деле мне скорее будут, сейчас понадобится второе условие,
ой, третье, третье условие. Вот. Мы к нему и шли. Мы к нему и шли.
Окей. Так. Так, это доказательство. Бум-бум-бум. Бум-бум. Так.
Мне вот такое вот понадобится. Я хочу доказать вот еще одно свойство
проксимального оператора. То, что у меня опять же, если проксимальный оператор задан выпуклой функцией,
то тогда вот выполнены такие свойства. Первое называется firmly non-expansive-ness,
firmly, ну, создавайте слабое, не расширяемость. А второе просто называется non-expansive-ness.
То есть не расширяемость. Где мы с ней уже сталкивались?
В проекции. Ну, в принципе, вещь ожидаемая. То есть у нас проекция была не расширяемая,
а, как мы понимаем, уже проксимальный оператор, это такое обобщение проекции.
Поэтому то, что это свойство здесь всплыло, это очень даже хорошо. Это очень даже хорошо.
Вот. Соответственно, мы с ним сможем потом провозаимодействовать. Окей.
Давайте подоказываем. Ну, давайте первое свойство докажем, второе, потому что,
ну, там совсем очевидно будет. Пусть у меня это у. Проксимальный оператор r в точке x,
v это проксимальный оператор r в точке y. В точке y. Что я тогда могу сказать? Что я тогда могу
сказать? Исходя из того свойства, которое у меня было доказано до этого. Давайте,
вот я вот на него отмотаю. Вот это свойство. Ну, давайте его выпишем. Давайте его просто
выпишем в таком виде, в котором оно у меня тут есть. Сейчас. x-y, z-y меньше либо равно,
чем r от z минус r от y. Вот. Хорошо. Потому что x-y у меня вроде как есть. Мне бы нужно
где-то накопать теперь проксимальные операторы. Проксимальные операторы. Вот. Ну, давайте вот напишу
здесь z1 и запишу это условие еще раз. y-x, только теперь поменяем местами. y и x.
Согласны? Согласны? Так.
Так, хорошо. Хорошо. Ну, что тогда теперь сделаем? Давайте попробуем как-то подобрать z1 и z2 так,
чтобы у меня в некотором смысле что-то, сейчас давайте гляну, я сам подзабыл,
что тут надо аккуратненько схлопнуть. А, вот так. Да, ладно, давайте вот так сделаем.
Вот так вот запишем в силу того, что у меня точка u. Для произвольной точки z должно выполняться z1.
Вот. У же у меня должно быть проксимальным оператором. Я не ту точку подставил. Я не ту точку подставил.
Там же было так, что у меня в теореме u это проксимальный под x. Поэтому если у меня здесь u это
прокс x, тогда у меня здесь должно быть u, а здесь v, потому что v прокс y. Вот. Сейчас, сейчас, сейчас все норм.
Вот. Тогда у меня получится вот такое соотношение. z1 и z2 я могу подставлять любыми.
z1 и z2 я могу подставлять любыми. В частности, подставлю их равными z1, подставлю равным v,
а z2 подставлю равным u. Ну, еще посмотрю, что получается. x-u, z1 это v-u, меньше либо равно, чем rv-ru.
А 2-ое это неравенство r-v, z2 это у-v, меньше либо равно, чем ru-rv.
Складываю эти два неравенства и что получаю? Справа у меня будет четко стоять 0, а слева у меня
будет стоять x-u-y плюс v, v-u, меньше либо равно 0, меньше либо равно 0. Ну, тогда что я выделяю?
x-y, v на u, и здесь у меня что получается? Минус v-u в квадрате, меньше либо равно 0, меньше либо равно 0.
Все хорошо, с плюсом вырезала. И тогда у меня что получается? v-u в квадрате,
меньше либо равно, чем x-y, u-v. Подставляю, получается prox x-prox y меньше либо равен,
чем prox x-prox y, x-y. Все, получили вот это свойство ферменон экспансивенни, слабую сжимаемость.
Как из нее тут же доказать, тут только квадрат, как из нее доказать, что prox-prox будет меньше,
чем x-y. Как из нее быстренько доказать это? kbh, то есть здесь мы применяем kbh и получаем,
что у нас здесь prox-prox умножить на x-y больше либо равен, чем вот это выражение сколярного
произведения. Делим на один из проксов, потому что слева у нас два прокса стоит, и получаем ровно
то, что нужно. Вот это то свойство, которое мы с вами уже наблюдали, когда говорили про проекции.
Так, пум-пум-пум. Так, хорошо, свойства доказали. Теперь вообще хочется посмотреть на задачи,
где нам эти проксимальные операторы понадобятся. Вот, рассмотрим вот такую задачу минимизации
функции, где у меня есть, по факту, моя целевая функция состоит из двух частей, из двух компонентов.
Такую задачу называют композитной, потому что два слагаемых. Ну и предположим, что у меня как бы в
данном случае f отвечает за гладкую часть этой функции, а r отвечает за негладкую, но
проксимально дружественную. Это означает то, что у меня, я могу посчитать от r,
проксимальный оператор бесплатно. Ну то есть в явном виде я могу просто записать формулу и
соответственно выписать. Ну вот такая вот постановка довольно общая. То есть раньше мы
рассматривали случаи, когда у нас r равно нулю. Тождественно равно нулю просто гладкая функция была.
До этого, то есть в начале этой лекции рассматривали случаи, когда у нас f равно
тождественно нулю, и у нас просто есть минимизация некоторой функции r, которая является негладкой. Но
про проксимальную дружественность мы не говорили. Проксимальную дружественность мы не говорили.
Вот, но здесь вот такая вот общая постановка. Так, окей. Ну и предлагается следующий метод для
решения этой задачи. То есть в силу того, что у меня f, теперь вещь дифференцируемая,
непрерывно дифференцируемая, гладкая, я от нее беру градиент и делаю следующий шаг. То есть вроде как
делаю градиентный шаг, делаю градиентный шаг, но потом этот шаг оборачиваю в проксимальный оператор.
В принципе, интуиция даже понятна, когда мы смотрим на оператор проекции. То есть в частный
случай оператор проекции. То же самое было шаг градиента, потом накидываем на это все безобразие
проекцию. Супер. Но теперь давайте в общем случае хочется понять, что вообще происходит. То есть когда у
нас, например, r дифференцируемо, вот этот проксимальный оператор, который у нас arg- минимум,
как для него выписываются условия оптимальности? Давайте его еще раз вот. Гамма r, x. Так, плюс одна
вторая. Здесь у нас будет x, k минус гамма. Вот так. Минус x в квадрате. Как запишется
оптимальная условия, если у меня r дифференцируемо? Градиент просто этого выражения равен нулю.
Согласны? Вот. Ну давайте запишу. Запишу, а потом поймем, что мы в итоге выписали. Гамма градиент r от
x плюс x минус x, k плюс гамма f от x, k равно нулю. Ну так как я потом это оптимальное значение
подставляю в x, k плюс 1, поэтому я здесь его и подставлю. Вот. Ну и что здесь получается? Чему
равно x, k плюс 1? Это x, k минус гамма f от x плюс r, x, k плюс 1. Вот. Ну вот такой вот метод в
реальности там записан. Здесь я его записал, так скажем, в неявном виде, потому что в таком
виде, понятно, вы метод записать не можете. Как найти точку x, k плюс 1, когда вам нужно посчитать
градиент точки r в функции r в точке x, k плюс 1. Но вот в реальности реальный градиентный спуск
честный. Если бы мы сказали, что у нас вот есть функция f плюс r, и мы бы не смотрели на ее специфику,
что она состоит из двух частей. Просто сказали, есть дифференцируемая функция, там градиент f,
градиент r, тупо берем градиент p и градиент p, суммируем, получаем общий градиент, делаем шаг
градиентного спуска. Здесь бы стояла точка x, k просто. Ну а в случае вот проксимального,
когда мы можем посчитать от r проксимальный оператор бесплатно, функция будет выглядеть
соответственно следующим образом. Шаг метода будет выглядеть следующим образом. Это неявный
вид. Явный вид вот там. То есть у вас вот этот неявный шаг, он зашит в вычисление проксимального
оператора. В случае проекции это там выбор нужной точки, ну а в случае каких-то простых
регулизаторов, это получается у вас просто вычисление этого прокса. Вот. И здесь, кстати,
как раз хочется поговорить, где в первую очередь применяется проксимальный алгоритм. Это как раз
в случае, когда у вас задача машинного обучения. f это обычно целевая функция. Там, например,
ваша логистическая регрессия. r это регулизатор. Вы с ним, в принципе, взаимодействовали,
добавляли к логистической регрессии регулизатор. Было такое в домашнем задании. l2 регулизатор
совсем простой, тоже гладкий, спокойно дифференцируемый, и вы как раз не использовали
никаких проксов. Но можно добавлять и более хитрые регулизаторы, например, или один регулизатор.
И тут сразу же следует его важное свойство. То, что я помню, показывал вам Threshold, то что он у
вас после шага. То есть что происходит в реальности алгоритма, когда вы, например, используете для
вашей логистической регрессии или один регулизатор. Вы делаете шаг градиента воспуска,
а потом у вас включается проксимальный оператор. И проксимальный оператор что вам сделает? Он вам
отсечет те координаты, которые имеют маленькое значение по модулю. Был такой эффект у вас уже
в домашнем задании. Был. Помните, как раз вы работали с методом Франко Вульфа и решали с
помощью него задачку на l1 шарике. И у вас там как раз был такой классный эффект, что если вы
стартуете из нуля, то количество не нулевых компонент не будет быстро увеличиваться. То есть
в итоге в решении процентов 10, 15 и нулевых компонент. Здесь тот же самый эффект. Похожий,
похоже, потому что природа регулизатора и множество похожие, потому что l1 нормой
порождается. Но здесь просто в l1 норма вам просто запрещало выходить, когда у вас было множество. А
здесь вам разрешается выходить за пределы множества просто потому что регулизатор. Ну и соответственно
природа похожая. То есть здесь трэш холпом отсеивает компоненты, которые близки к нулю. И там
вы отсеивали довольно большое число компонент за счет того, что проецировались на множество. Вот.
Вот такая вот идея, почему это может быть полезно, в том числе в машинном обучении,
причем бесплатно. Сейчас мы это поймем, что это действительно бесплатно. Вот. Окей,
выписал явный вид, явный вот эту схему. Что нам еще нужно? Нужно доказать вот такое вот свойство.
Такое вот свойство, что x звездой у нас является стационарной точкой нашего вот этого
проксимального оператора плюс градиентного шага. Градиентного шага. Ну тут все довольно
стандартно. Опять же, выписываем условия оптимальности. Ну, то есть прокса у нас
в данном случае гамма r от x плюс одна вторая x минус x звездой плюс гамма f от x звездой.
Условия оптимальности выписываю. Тут только тут я уже выписываю через субдифференциал. То есть
0 должен принадлежать субдифференциалу. Вот. Что получается? Что получается? Гамма r от x плюс одна
вторая x. Тоже самое переписывается. Дальше мне просто нужно взять субдифференциал. Что получается?
Гамма субдифференциал r плюс x минус x со звездочкой плюс гамма f от x. 0 принадлежит. Ну и заметим,
что если я поставлю x со звездочкой, то у меня все выполнится. То есть x и вот здесь сократятся. Ну,
а я знаю из условий оптимальности моей исходной задачи то, что у меня субдифференциал r плюс
градиент f от x со звездой. Ну f от x у меня дифференцирован. Получится общий субдифференциал
исходной функции. 0 ему должен принадлежать в оптимальной точке. Вот. В силу того, что опять же
условия оптимальности это у меня критерий. Я это могу провернуть в обе стороны. То есть сначала
подставить сюда x со звездой, вытащить вот так в таком неявном виде. Ну вот так вот просто сказать.
Ну давайте я вот так вот рассмотрю. Дальше, соответственно, в силу того, что это у меня критерий,
поэтому я поднимаюсь наверх. Говорю, что это 0 принадлежит субдифференциалу вот такой вот функции.
Ну а дальше еще выше поднимаюсь. Вот. В силу того, что опять же у меня критерий, вот это выполнено.
Вот это условие выполнено. Окей. Окей. Ну я как раз вот здесь от условий оптимальности и иду. Говорю,
что у меня вот это условие оптимальности. Дальше, умножаю на гамма, добавляю незначащий x.
И здесь у меня что получается? Вот это как раз точка, а это точка, в которой я считаю
проксимальный оператор. Так. И здесь у меня как раз получается х со звездой равен проксимальному оператору.
Ну вот это то условие как раз у меня x было x-y принадлежит dr. На данном случае гамма dr х со звездой.
Вот. Второе условие из теоремы свойств проксимального оператора. Вот. Я им пользуюсь,
она у меня тоже критерий. Она у меня тоже критерий, что прокс равен вот этому значению.
Поэтому в обе стороны эти рассуждения верны. Окей. Здесь разобрались. Здесь разобрались.
Получили вот два таких свойства. Нерасширяемость и то, что у меня стационарная точка вот этого моего шага
метода, это точка х со звездой. Точка х со звездой. Где у нас, кстати, такая же стационарность была?
Кто помнит? Снова в проекции. То есть, в принципе, вот здесь вот это очень похожие свойства на те,
которые мы с вами встречали в проекции. Они прям один в один. Один в один. Поэтому
доказательства будут тоже один в один. х ка плюс один моя точка. Как я ее считаю,
как проксимальный оператор только тут функции r. Только функции r. Проксимальный оператор от шага
градиентного спуска. Окей. Дальше что знаю? Я знаю то, что мне х со звездой. Это прокс гамма r х со
звездой минус гамма f от х со звездой. Вот. Окей. Подставляю сюда. Подставляю сюда. У меня получается
разность двух проксов. Разность двух проксов гамма r х со звездой минус градиент f от х со звездой.
Так-так. Все. То есть, здесь воспользовались свойством, что х со звездой это стационарная
точка и она равна просто проксу от шага градиента. Окей. Дальше нерасширяемость прокса. У меня здесь
останется только х ка минус гамма f от х ка минус х со звездой плюс гамма f от х со звездой. Вот. Что мы
можем сказать про градиентов от х со звездой? Быстренько. Что-то можем сказать или нет?
Можем или нет?
Ну, тут ответ нет тоже подойдет на самом деле. То есть, можем или не получается или нет что-то
сказать. Будто равен нулю или нет. Правильно. Потому что на самом деле у вас задача f от х плюс r от х.
Так. Даже если у вас там дифференцируемая функция r, у вас получается, что вот градиент суммы
должен равен в оптимальной точке быть равен нулю. Так. При этом каждый из компонентов может быть
не равняться нулю. Согласны? Ну, в сумме ноль, но каждый из компонентов, понятно, может быть нулевой.
Вот. Поэтому про градиент с точки х со звездой вы сказать примерно ничего не можете. Вот. Но так же было,
в принципе, когда мы работали с проекциями. Как когда мы работали с проекциями, у нас же тоже там
условия оптимальности было такое, то что, ну если х со звездой, которая минимум на множестве,
не совпадает с глобальным минимумом на все rd, то там градиент может быть и не нулевым. Он вам
просто должен указывать за пределы множества. За пределы множества всегда. Артагонально множество,
это мы тоже с вами выписывали. Вот. А здесь, здесь, соответственно, тоже ничего нельзя сказать,
как и в случае проекции. Потому что, опять же, prox это обобщение проекции. Вот. Выписали такое
безобразие, опять же расписали расстояние. Вот это. Теперь можно раскатать по гладкости. Раскатать
по гладкости. Я, на самом деле, доказательства скопировал просто из проекции, потому что одно и
тоже мы применили свойства prox, прочь на prox заменил просто. Вот. А здесь, кто помнит,
куда девается х со звездой? f от x, градиент f от x со звездой. Куда он там делся?
Он же там не пропадает, но куда-то девается. Куда мы его там запихивали?
Давайте я вам просполирую тогда. Дальше, что тут гладкость расписывается? Ну да, вот здесь
сразу допишется, через дивергенцию Брегмана это мы расписывали. То есть, в случае, когда у вас
градиент f от y, ну в данном случае y равен х со звездой, он просто равен нулю, то у вас
дивергенция Брегмана вот эта выпадала, и у вас получалось что-то в духе xk, то что у нас раньше
получалось х со звездой. Так, в силу того, что теперь у вас градиент не выпадает, потому что не нулевый,
вот, ну тогда дивергенция Брегмана у вас в некотором смысле выглядит более полно. То есть,
скалярное произведение в ней остается, и его за собой нужно тянуть. Но как мы с вами знаем,
дивергенция Брегмана у вас всегда, это вещь положительная, в силу того, что ее порождает
выпуклая функция. Ее порождает выпуклая функция, поэтому это в любом случае некоторый измеритель
расстояния, поэтому это вещь положительная. Ну и когда вы это все распишете, у вас отсюда выпадает
эта дивергенция Брегмана, ну и ровно такой же член, который был. Член, который был, правильно
подбор гамма, его уничтожает, его уничтожает. Вы знаете, что дивергенция не отрицательна, больше
либо равна нуля, гамма соответственно меньше, чем один делить на L, вас спасает. Вас спасает, вы
убиваете второй кусочек и получаете линейную сходимость. Ровно все то же самое, что было в
проекции. Все? Супер. Ну что можно сказать? Что можно сказать, что в некотором смысле
проаксимальный градиентный спуск это обобщение метода с проекцией плюс обобщение градиентного
спуска на случай, когда вы работаете с композитной задачей, где R является
проаксимально дружественной функцией. Причем, что самое классное, то есть в те время сходимости,
которые я сейчас получил, у меня будут фигурировать только mu, константа сильной выпуклости всей функции,
ну или функции f, например, если она сильно выпуклая, и L, константа гладкости функции f. При этом,
то, что у меня функция R не гладкая, вообще никак не влияет, но только влияет в том случае,
когда у меня… не влияет, если у меня R проаксимально дружественная. То есть я вот этим
проаксимальным оператором, который я могу считать в явном виде для некоторых простых функций,
я сжираю эту негладкость и получается правильный метод с точки зрения сходимости. Ну как правильный,
имеется в виду, что если бы я просто посмотрел на мою исходную задачу и сказал, ну так как это
сумма гладкой и негладкой функции, то лучше, чем субградиентный метод, я применить ничего не
могу. Это прямой взгляд, вижу негладкую функцию, применяю субградиентный метод. Тогда всплыли бы
негладкие свойства функции R, и тогда бы сходимость была соответствующая плохая. Но если опять же я
добавляю специфику в задачу, говорю о том, что, например, функция R является проаксимально
дружественной, то я могу скушать вот эти свойства, скушать свойства негладкости и оставить только
хорошие свойства гладкости моей исходной целевой функции F. Все. Ну тут еще написано, что,
что тут еще у меня в выводах написано. Да, R скушается в случае, когда... Во, интересный вопрос.
Кажется, когда, если я у меня F положу равной нулю, я могу вот так вот отрешивать любую негладкую
задачу. Проаксимально дружественную. И это правда будет быстрее. Правда будет быстрее, но опять же
проаксимальная дружественность. То есть по факту я могу находить минимум функций R за бесплатно. Ну,
неинтересно. Но я же могу сказать, что, как мы с вами обсуждали, у нас тоже часто возникали какие-то
вспомогательные arg-минимумы, я вам всегда говорил, что на самом деле их можно отрешивать неточно. То
есть брать численный метод какой-то дополнительный и, пожалуйста, мы его отрешим, появляется небольшая
неточность, но ее можно просто учесть дополнительно потом при анализе. Окей. Здесь, если я, соответственно,
даю вот эту возможность отрешивать задачу неточно. Вот. Даю возможность, я действительно могу теперь
решать любую негладкую задачу. Но на отрешивание arg-минимума мне нужны дополнительные ресурсы.
Будет ли это лучше, чем субградиентный метод?
В общем случае. Ну, ответ вроде очевиден. Давайте, подскажите мне его, может быть,
понимаете уже ответ. Я его уже в некотором смысле давал. Да, с чего его? Потому что нижние оценки
говорят, что субградиентный метод оптимален. Здесь мы тоже просто используем субградиенты и какую-то
комбинацию точек дополнительных. Мы же Prox начинаем вот эту arg-минимум отрешивать с субградиентным
методом в том числе. Вот. Да, это вспомогательная задача. Негладкая вспомогательная задача. Вот.
Которую нужно решить несколько раз. Ну и, соответственно, оценка у вас в итоге будет ровно
такая же, как для субградиентного метода. Никакого там улучшения не будет. Ну да,
с максимальным методом можно отрешивать задачи негладкие. На практике иногда работает даже
лучше. То есть, берете Prox, у вас из-за того, что там есть эта сильная выпуклость, каждая конкретная
задачка у вас решается быстрее. Вот. Но вам ее нужно решить не один раз. Вот. И в итоге там,
в теории, это никакого улучшения не дает. На практике может дать. Окей. Что еще? А все. Все,
у меня презентация закончилась, поэтому ваши вопросы? Так. Все, вопросов нету. Тогда сегодня мы
с вами даже заранее закончили. Супер. Все, спасибо. Увидимся через неделю.

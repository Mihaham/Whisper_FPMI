Добрый день! Сегодня у нас последняя лекция, ну и мы закончим разговор про вероятностные алгоритмы.
Значит, мы начнем более-менее с того же самого места, где мы закончили.
Мы в прошлый раз изучили много разных сложностных классов, связанных с вероятностью.
Мы изучили, как их можно варьировать, как влияет ошибка, которую мы разрешаем.
На основании уменьшения ошибки мы смогли доказать, что все вероятностные алгоритмы можно смоделировать
детерминированными схемами. Вероятностные схемы тоже можно рассматривать, но они ничего не дадут дополнительно.
Схемные классы – это не единственное, что у нас было в первой части. Некоторые связи были с классом PSPACE, с классами NP и CoNP.
Но сейчас я хочу сказать про кое-что другое, а именно про полиномиальную иерархию.
Это называется теорема гачи Сипсера.
Третье заключается в том, что BPP вложено в пересечение сигма два-полиномиальная и пи два-полиномиальная.
Хорошо. Получается, что класс BPP лежит в полиномиальной иерархии и на самом деле очень высоко, на втором уровне.
Во-первых, определение BPP симметрично, поэтому если А лежит в BPP, то и дополнение К лежит в BPP.
Ну или иначе говоря, можно сказать, что BPP равняется CoBPP.
Вот. Соответственно, достаточно что-то одно из этого доказать. В смысле одно из вложений вот сюда вот и вот сюда вот.
Да, поэтому будем доказывать, будем доказывать, что BPP вложено в пи два-полиномиальное.
Сейчас, наоборот, мы докажем, что BPP вложено в сигма два-полиномиальное.
Так, значит, идея. Идея сначала уменьшим ошибку, сделав ее экспоненциально малой.
Насколько конкретно экспоненциально мало этого? Сейчас посмотрим.
Вот. Значит, тогда в пространстве всех R множество тех, которые дают результат один,
это множество либо совсем маленькое, либо совсем маленькое, либо очень большое, занимающее почти все.
Это извините.
Так, хорошо. Может быть, две ситуации, я их так условно нарисую.
Значит, смотрите, вот это прямоугольничек. Прямоугольничек это как бы все пространство.
Значит, пространство этих самых R.
Так, и дальше могут быть разные ситуации. Может быть, тут вот такое небольшое какое-то множество.
Вот, а может быть, какое-то такое вот большое.
То есть, может быть, оно там совсем немножко занимает, а может быть, вот так вот почти все.
Вот, и дальше смотрите. Дальше мы возьмем и так подвигаем немножко.
То есть мы как бы возьмем и вот так вот сдвинем.
Но тогда, если у нас множество было маленькое, то тогда, соответственно, даже если немножко подвинуть,
то оно все равно, даже все эти копии займут немного.
Вот, а вот если взять большое множество и его немножко подвигать.
Вот так вот его продвигать на все стороны.
То вот уже видно, что тут все, значит, все будет покрыто хотя бы один раз.
Значит, я уж стрелка тут не буду рисовать.
Ну, в общем, видно, что вот это исходное, мы его так подвинули, оно все покрыло.
Но, конечно, то, что я рисую, это метафора, потому что наше пространство это никакие не прямоугольники.
Это булливкуб.
Но, тем не менее, на булливом кубе тоже есть слиги.
Значит, слиги по каждой координате по отдельности.
И, соответственно, тоже вот такими слигами можно покрыть все пространство.
Значит, теперь идея.
Значит, общая идея состоит в том, что если множество подходящих R маленькое,
то даже если его сдвинуть на
примерное число разных лекторов, то, соответственно, даже объединение всех копий не покроет все пространство.
Вот, а если множество большое, если же множество большое, то, соответственно, при каком-то подборе слигов
объединение всех множеств покроет все пространство.
И тогда, но это еще надо доказать, значит, тогда идея такая, значит, формула будет такая, что существуют сдвиги,
значит, такие, что любая точка попадает в образ при одном из сдвигов.
Вот это как раз Сигма-2 формула.
Ну, значит, теперь этот план нужно реализовать, то есть нужно доказать, что действительно будет такая же ситуация как на картинках.
Вот, значит, но тут нам, но первая часть более-менее очевидна.
Ну, значит, первая часть очевидна, что если это может экспоненциально маленькое, то полимерное число сдвигов тут не хватит.
Вот, но нужно вторую часть объяснить.
Вот, но, значит, тут будем действовать так, значит, смотрите, пусть у нас rx,
значит, rx это множество таких r, что m от xr равно единице.
То есть здесь длина r равна m, а размер rx у нас будет больше, больше, чем 2 в степени m на 1 минус епсилон.
Вот, ну, а епсилонка эта экспоненциально маленькая, ну, можно считать, что епсилон равняется 1 делить на 2 в степени n, где n длина x, а m, соответственно, больше.
Потому что случайных битов может быть нужно больше, чем длина входа.
Так, значит, вот rx, значит, rx это вот такая штука.
Теперь можно рассмотреть сдвиги.
Сдвиги это r, rx поксоренное каким-то s, то есть это множество таких ксоров по битвах r плюс s, значит, таких, что m от xr равно единице.
Вот, ну, на самом деле это также то же самое, что множество таких r, что m от x и r плюс s равно, ой, r плюс s равно единице.
Значит, потому что, ну, неважно, да, мы тут делаем ксор или тут делаем ксор, поскольку два раза делать ксор то же самое, что ничего не изменить.
Вот, а r все равно все значения предлагает. Ну, в принципе, все значения предлагают.
Вот, хорошо. Значит, теперь мы хотим показать, нужно показать, что для некоторого полиномиального q будет верно, что найдутся s1,
и так далее, sq такие, что объединение всех rx сдвинутых на s и t, значит, равно просто всем словам длины m, то есть всем возможным набору случайных битв.
Так, это реализуется вероятностным методом. Воспользуемся вероятностным методом, но я предполагаю, что у вас уже был вероятностный метод на дискретном анализе.
Соответственно, тут классический метод заключается в том, что мы возьмем эти сдвиги случайно и покажем, что с положительной вероятностью они все подойдут.
Значит, реализуется метод, рассмотрим случайные сдвиги s1, и так далее, sq.
Вот, и посчитаем вероятность того, что какой-то вектор, значит, какая-то строка не попала ни в одно из множеств rx плюс s и t.
Вот, значит, как мы это будем делать?
Да, оно сейчас было случайно, значит, случайное, то есть независимое и равномерно распределенное.
Ну, и посчитаем вероятность того, что какая-то строка не попала ни в одно из множеств rx плюс s и t.
Ну, и посчитаем вероятность того, что какая-то строка не попала ни в одно из множеств rx плюс s и t.
Ну, а значит, что одновременно случились ку событий.
Да, например, случился ку событий.
Так, давайте что-нибудь писать раньше того, что w.
Да, давайте напишем так, что для любого и у нас случилось, что w, значит, точно неверно, что w попало в rx плюс s и t.
Вот, это равно, значит, поскольку тут можно перекидывать вот так вот, s туда и сюда, значит, это равно в точности вероятность того, что для любого и неверно,
что w плюс s и t лежит в rx.
Ну, то есть можно сказать, что для любого и w плюс s и t у нас лежит в rx чертой.
Ну, а смотрите, значит, это еще независимые события, поскольку s и t независимые, и они все сразу произошли, то есть это просто произведение.
Значит, это получается произведение и от единицы до ку вероятности того, что w плюс s и t попало в rx чертой.
Ну, а в этом у нас вероятность епсилон, значит, равно произведению епсилонов.
Так, и это у нас равно епсилон в степени q, то есть это равно 1 делить на 2 в степени nq.
Так, хорошо. Ну, это пока для конкретной строки, значит, для фиксированной строки w.
А теперь посчитаем вероятность того, что найдется такая строка.
Вероятность того, что существует w, значит, эта штука, да, это будет меньше равно, чем сумма по всем w, вероятность вот таких вот.
Так, сейчас только тут где-то, вот тут, наверное, должно быть меньше, тут же больше, значит, тут не равно, а меньше.
Вот, соответственно, это получается меньше, значит, чем сумма по w, соответственно, вот этих вот штук.
И это будет равняться 2 в степени m поделить на 2 в степени nq.
Соответственно, это будет меньше единица при nq больше m, да, ну, то есть q больше, чем m делить на n, что есть полином.
Вот, ну, соответственно, если, значит, если вот эта вот вероятность меньше единицы, то, соответственно, вероятность дополнения больше нуля.
Значит, тут будет наоборот, да, для любого существует, и тут без отрицания это будет больше нуля.
Вот, ну, а это значит, значит, значит, можно зафиксировать и осытая, так чтобы, соответственно, вот это было верно.
Вот, ну, а это значит то, что нам нужно, значит, а это и значит вот это вот.
Ну, вот, значит, соответственно, поскольку с другой стороны, поскольку здесь q полиномиальная, то если наоборот, если это rx совсем маленькое, экспедиционно маленькое, то тогда сдвиги, значит, сдвиги всех этих множеств не позволит покрыть все, да, потому что они экспедиционно маленькие, мы их умножаем на полином, они остаются меньше единиц.
Вот, ну, вот, значит, вот итог получается, да, значит, итог.
Итог получается такой, что у нас x лежит 2 тогда и только тогда, значит, тогда и только тогда, когда верно следующее.
Когда существует s1 и так далее сq, значит, такое, что для любого w длины m, значит, соответственно, верно.
Но вот это вот квантосуществование у нас полиномиальный, поэтому я его так распишу.
Наш w лежит в rx плюс s1 или и так далее, или w лежит в rx плюс sqt.
Вот, ну, вот и есть, это есть сегмент 2 форма, да, что здесь скопка тоже полиномиально проверяется.
Вот, ну, полиномиальный x проверяется за счет того, что это полиномиальная рецензия машина.
Значит, это получается.
Ну, вот, значит, вот так получается.
Ну, есть вопросы.
Ладно.
Следствие такое, что если p равно np, то тогда p равно ppp.
Потому что тогда вся полиномиальная иерархия схлопывается и все, что в иерархии находится, то есть в частности ppp, будет схлопнуто.
Так.
Хорошо.
Дальше.
Вообще-то такой большой вопрос.
Значит, а что вообще известно?
Значит, а что вообще известно про
Проблема равенства p и ppp?
Два нужных знака, да, во-первых, внутри вот здесь вот, да, и во-вторых, как вообще из фразы.
Вот. Ну, значит, на первый взгляд, на первый взгляд, ситуация похожа на вопрос, равный ли p и np.
В каком смысле она похожа?
Ну, в том смысле, что есть задачи, значит, которые решаются за, соответственно, в большем классе,
в большем классе, но не решаются в меньшем.
Значит, есть задачи, которые решаются в большем классе, но не решаются в меньшем.
Да, типа там задача выполнимости, да, и задача проверки равенства многочленов, например.
Вот, значит, однако в np есть полные задачи, причем достаточно много.
В ppp их не известно.
Да, они, конечно, могут быть, особенно если p равно ppp.
Но ни про одну задачу никто точно не знает, не будет ли она ppp полной.
Вот. Но проблема стоит в том, проблема в том, что определение ppp, как говорят, симантическое, а не синтаксическое.
Значит, что это значит? Это означает, что на самом деле далеко не любая, значит, далеко не любая вероятностная машина вообще задает какой-нибудь язык из ppp.
Вот, да, значит, нужно, чтобы вероятности были отделены от нуля.
Значит, вероятности были отделены от нуля. Да, то есть, точнее, от нуля, а вот 1 и 2, значит, вероятность того, что машина примет слово,
знаешь, чтобы она была, была отделена от 1 и 2, да, значит, а именно была бы либо не больше 1 и 3, не больше 1 и 3, либо больше 2 и 3.
Вот. А заранее, значит, заранее по тексту машины невозможно понять, будет ли эта вероятность всегда в таком диапазоне.
Вот. Ну и поэтому, значит, поэтому нет генерического полного языка вида там код машины, там код машины, вход, время работы.
Такой, что машина принимает этот вход, значит, потому что даже если для этого конкретного входа мы можем посчитать вероятность, то, ну, если она будет между 1 и 3, 2 и 3, да, то непонятно, что с этим делать.
Ну вот, поэтому ПП их неизвестно. Это, как бы, такой первый звоночек. Да, значит, и на самом деле, значит, на самом деле большинство исследователей, большинство исследователей верят, что P равно PPP, значит, означает, что любой вероятностный алгоритм
можно дерандомизировать. Вот. И основание для такой веры следующее.
Основание для такой веры. Так, первое. Ну, первое, значит, для схемных моделей, для схемных моделей это верно.
Да, например, BPP поле. BPP поле равно P поле.
Но это даже не очень сильный довод. Ну, значит, более того, вот это уже более сильный довод. BPP AC0 равно AC0.
Ну, тут вот AC0 мы проходили, значит, это означает, что схемы полиминального размера и константной глубины, соответственно, скандируются с дизюнкцией произвольного лентности.
Ну, ABP означает, что невероятностные штаммы, кроме входа, еще какие-то случайные биты. И, соответственно, ошибка не больше 1 и третья.
Так, вот, значит, второе, может быть, даже еще более, еще более веский довод. Hardness versus randomness.
Значит, теорема Нисана Викторсона, значит, если есть достаточно трудные задачи. Ну, что значит достаточно трудные, я сейчас не буду конкретизировать.
Ну, они типа, ну, если NP полные, достаточно сложно решаются. Значит, то тогда можно построить достаточно хорошие генераторы псевдослучейных чисел.
Ну, и тогда, соответственно, P равно BPP.
Ну, и третья обскал, что также хорошие генераторы. То есть идея вообще такая, что мы хотим заменить случайные числа, которые мы используем в алгоритме, на какие-то псевдослучайные числа, которые мы можем сами генерировать или перебирать и как-то это все делать.
Ну, и тогда нам настоящие случайные биты уже не нужны. Мы их заменили на псевдослучайные.
Так же хорошие генераторы можно построить на базе других конструкций. Например, экстракторов. Пока что нужных конструкций неизвестно.
Вот, однако, значит, ну, нет и никаких препятствий для их построения.
Вот.
Ну, то есть тоже, значит, тоже неизвестно.
Ну вот, хорошо. Ну и последняя тема, на которой я немножко поговорю, это что известно про дарандомизацию.
Значит, в некоторых случаях для некоторых задач, для некоторых задач процедуры дарандомизации просто напрямую известны.
Я уж не знаю, сколько я успею. Ну, по крайней мере, одну, один метод успею рассказать.
Значит, рассмотрим.
В некоторых случаях работает так называемый метод условных математических ожиданий.
Так, давайте я расскажу, как он устроен.
Так, посмотрим на примере. Пример. Это задача MaxCut.
Значит, тут нужно разбить вершины графа на две группы, так чтобы число ребер между группами было бы максимальным.
Значит, если граф вообще двудольный, значит, если граф двудольный, тогда это число может быть в принципе максимальным.
Вот, то есть все ребра можно разрезать.
Но если граф не двудольный, то все уже нельзя разрезать, но хочется было как можно больше.
Значит, задача NP полная.
Но можно рассмотреть приближенную задачу.
Мы будем стараться разрезать число ребер, которые больше либо равно, чем РО, умноженное на максимальное количество.
Какое число меньше единицы? Больше максимума в принципе не можем разрезать, но хотим попасть между максимумом и РО на максимуму.
Так, хорошо. Ну и можно считать, тут есть такой вероятностный метод.
Да, выберем просто, значит, разобьем на две группы случайно.
Значит, а именно для каждой вершины равномерно и независимо выберем, в какую часть ее отнести.
Вот, тогда получается утверждение.
Тогда в среднем будет разрезано хотя бы половина ребер.
Действительно, для каждого ребра есть четыре варианта, куда будут отнесены его вершины, но из них в двух вариантах оно будет разрезано.
То есть верности одна-вторая, ну а дальше, конечно, эти факты могут быть зависимы для разных ребер, но по линейности мат ожидания это неважно.
Значит, по линейности мат ожидания среднее число разрезанных ребер равно, соответственно,
Ну, именно равно, размер е пополам.
Ну, значит, существует разрез из больше либо равно, чем е пополам ребер.
Ну и, соответственно, также в нем будет разрезано, точнее не так же, тем более, тем более в нем будет разрезано, тем более в нем будет разрезано, тем более в нем будет разрезано, тем более в нем будет разрезано, тем более в нем будет разрезано.
Ну или в нем будет разрезано больше или меньше, чем одна-вторая умножить на максимум ребер.
Максимум точно не больше, чем все.
Ну а вопрос состоится, как явным образом найти такой разрез.
Ну, оказывается, можно действовать жадным алгоритмом.
Давайте я сначала, наверное, напишу жадный алгоритм, а потом расскажу, как он связан с условными от ожиданиями.
Жадный алгоритм.
Значит, пусть уже, пусть уже часть вершин отнесена, значит, отнесена в одной из частей.
Рассмотрим новую вершину.
Значит, она соединена, значит, она соединена, ну, с какими-то, с какими-то уже отнесенными вершинами.
И мы, соответственно, отправим ее в ту часть, где находится меньшее число, меньшее число, соответственно, вершин, с которыми она уже соединена.
Это разрежет хотя бы половину ребер от данной вершины до уже рассмотренных.
Ну и, соответственно, объединив все оценки, получим, что разрез хотя бы у половины, ну, в разрез вошла хотя бы половина всех ребер.
Вот.
Ну а теперь, причем здесь условные от ожидания?
Значит, можно считать, что у нас есть случайная величина.
Значит, пусть кси-1 и так далее, кси-s.
Это случайная величина, случайная величина, соответственно, не как бы отвечающая за ту часть.
Так, сейчас, почему я? Давайте n, n обычный вершин.
Значит, кси-n, случайная величина, отвечающая за ту часть, которую попали соответствующие вершины.
Тогда, соответственно, пусть кси-s, соответствующие вершины.
Значит, кси-s, случайная величина, отвечающая за ту часть, которую попали соответствующие вершины.
Тогда, соответственно, пусть значение, значит, кси-1 и так далее, ксикаты, уже заданное.
Значит, тогда есть условная вероятность попадания ребра ежиты в разрез.
Значит, она, соответственно, равна нулю или единице.
Значит, если, соответственно, оба конца уже известны, куда попали.
Ну, давайте прям явно напишем.
Ну, если и меньше либо равно к, и меньше либо равно к, и кси-t равно кси-житому.
Соответственно, единица, если также, но кси-t не равно кси-житому.
Ну, если одна вторая, если и больше к или ж больше к.
И в данном случае нет разницы, значит, нет разницы между тем оба этих индекса больше к или только один.
Потому что если оба, то это будут те же самые две четвертых, которые у нас там были.
А если один, то уже действительно одна вторая. Уже остаются два варианта для второго конца, из них один подходит.
Вот, а получается, что если мы, значит, когда мы,
когда мы определяем значение кси-t плюс один, то, соответственно, ну, ожидания, что равны одной-второй,
либо не меняются, ну, не меняются они когда-либо, их было обе больше к.
Значит, если даже кто-то из них к плюс один, то это останется одна вторая.
Либо, соответственно, кто-то больше и к плюс один.
Ну, то есть, можно сказать, что не меняется, если и больше к плюс один или ж больше, чем к плюс один.
Значит, либо сменяются вот с одной-второй, сменяются с одной-второй на ноль или один.
Ну, соответственно, нужно, чтобы изменений на ноль было не больше, чем изменений на один.
Вот, ну, или можно, значит, можно сказать таким образом.
Смотрите, мат ожидания, значит, cat, cat от s, да, это будем считать, что размер разрез, который задан s, то есть число ребр, которые идут от s к дополнению кс.
Значит, при условии, что там какие-то значения кси один и так далее, кси cat-ы фиксированы, это будет вот что-то, это будет одна вторая.
Значит, вот такого же ожидания.
И при условии, что кси к плюс один равно нулю и плюс такая штука, где кси к плюс первый равно единице.
Ну и получается, что одно из, получается, что либо вот это вот больше, либо равно, чем то, что было, да, соответственно, или то же самое для единицы.
Вот, поэтому можно зафиксировать, значит, можно зафиксировать кси к плюс один так, чтобы условная мат ожидания не уменьшилась.
Ну а поскольку оно изначально равно 1 и 2, то после всех этих фиксаций будет больше 1 и 2, поскольку изначально оно равно 1 и 2, то и в конце будет больше равно 1 и 2.
Ну вот такое рассуждение.
Значит, тут это может казаться таким излишним загромождением, вроде же, так ясно, что работает.
Тут мы как-то терроризируем.
На самом деле в некоторых случаях трудно обойтись, в некоторых случаях трудно обойтись без такого явного расписывания.
Значит, поэтому сейчас я расскажу другой пример.
Другой пример, это называется max3sat.
Значит, тут дана формула в формате 3knf,
в которой в каждой скобке ровно три литерала.
Ровно три литерала, ну, значит, вот различных переменных.
Значит, требуется максимизировать число истинных скобок.
Ну, на каком-то наборе.
Вот.
Хорошо, значит, как мы это будем делать?
Как мы это будем максимизировать?
Ну, и, в-первых, рассмотрим случайный набор значений переменных.
Значит, тогда каждая скобка будет выполнена с вероятностью 7 восьмых.
Значит, каждая скобка будет выполнена сразу с 7 восьмых.
Ну, соответственно, среднее число истинных скобок будет равно 7 восьмых m.
Да, m это традиционно считается, что n переменных и m скобок.
Задача выполнимости.
Ну, и среднее число истинных скобок будет 7 восьмых m.
Ну, и, соответственно, значит, для какого-то конкретного набора их будет больше.
Ну, начнем с 7 восьмых m.
Так.
Теперь как его искать?
Как его искать?
Так, сейчас пока не будем пользоваться вот этим медом.
Значит, попробуем просто так найти.
Идея первая.
Идея один.
Рассмотрим.
Значит, рассмотрим все скобки с применной p.
Если больше тех, куда входит сама p, значит, положим p равно 1.
Если больше тех, куда входит отрицание p, значит, положим p равно 0.
Ну, если поровну, то неважно.
Если поровну, то неважно.
Как-нибудь положим.
Ну, и, соответственно, теперь исключим.
Значит, таким образом мы выполнили половину всех скобок с переменной p.
Вот.
Исключим их всех из рассмотрения.
Значит, исключим их из рассмотрения.
Значит, теперь из оставшихся рассмотрим все с переменной q.
Так, ну, тут сейчас надо написать хотя бы половину.
Аналогично выполним хотя бы половину из них.
Ну, и так далее.
Значит, в итоге выполним хотя бы половину всех скобок.
Ну, однако это, конечно, верное рассуждение, но видно, что мы здесь совершенно не оптимально действовали.
А именно, значит, те скобки, которые мы выкидывали, которые были ложны,
может, было не выкидывать, а попробовать их еще раз выполнить, потому что там же еще две переменных остались.
Но зато вот это вот точно работает, даже если у нас не ровно три литералы, а может быть меньше трех литералов.
Потому что в дате 7 восьмых это явно исполнил, что у нас тут три разных.
Значит, три разных переменных.
Так, идея два.
Значит, идея два.
Что, ну, как в предыдущем будем действовать как выше.
Но, соответственно, если литерал в формуле оказался ложным, то не будем выкидывать всю формулу.
Значит, вычеркнем только этот литерал.
И будем смотреть на оставшуюся скобку.
Ну, оставшуюся скобка может быть из двух литералов или просто из одного.
И, соответственно, учитывать при последующих шагах.
Значит, и учитывать ее при последующих шагах.
Так, значит, тут так получится выполнить хотя бы три четверти всех скобок.
Потому что до того, как скобка окончательно станет ложной, нужно вычеркнуть три литерала.
Вот, соответственно, а с каждым из них какая-то другая скобка станет истинной.
То есть на одну ложь у нас как минимум три истины приходится.
Ну, а три четвертых это все еще не семь восьмых.
Ну и тогда, соответственно, идея три будем использовать условным от ожидания.
Значит, идея три будем использовать условным от ожидания.
Значит, и здесь происходит следующее.
Ну, смотрите, пусть у нас вот есть какая-то формула phi.
Значит, phi это наша формула.
Точнее, посмотрим на конкретную скобку.
Пусть C, значит, это некоторая скобка.
Некоторая скобка, да, значит, неопременно.
Так, значит, дальше набор значений P1 и так далее Pk задает вероятность истинности C.
Значит, и эта вероятность будет такая.
Это будет один.
Значит, один, если в C есть истинный литерал.
Дальше ноль, если в C все три литерала ложные.
А если два ложные, то одна вторая.
Если в C уже два литерала ложные.
Если один ложный, то три четверти.
Если в C один ложный литерал.
Ну и, наконец, семь восьмых.
Если два литерала ложные, а третий неопределенный.
То есть тут один ложный, два неопределенных.
И семь восьмых, если все три неопределенных.
Вот, а получается, что определение новой переменной переводит каждую скобку либо в первую категорию,
либо на шаг выше.
То есть из всех этих, кроме нуля, уже никуда нельзя перейти.
Из одной, второй, третьей, четверти, семи восьмых.
Либо литерал истинный, тогда сразу скобка становится истинной.
Либо ложный, тогда, соответственно, на шаг выше.
То есть из семи восьмых можно перейти в один или в три четверти.
То есть из трех четвертей можно перейти в один или в одну вторую.
То есть из одной второй можно перейти в один или в ноль.
И тут получается, что цена вопроса для скобки с одним ложным литералом вдвое выше,
а с двумя ложными литералами в четверо выше, чем для полностью неопределенной скобки.
Вот, ну если случается алгоритм, значит алгоритм такой, аналогично идее 2,
но скобки из двух литералов берем с весом 2, а из четырех литералов,
ой, в смысле из одного литерала, из одного литерала с весом 4.
Получается, что каждый раз мы берем все оставшиеся скобки с данной переменной новой,
вот с такими весами их складываем, смотрим, где больше те, где сама переменная,
соответственно выбираем, чему равна переменная,
и соответственно вычеркиваем все скобки, которые стали истинными,
и вычеркиваем литералы, которые стали ложными, и тогда у нас в скобках уменьшится число литералов неопределенных.
Ну и это на самом деле ровно то, что вот здесь происходит,
то есть вот эти вот 1, 2, 3, 4, 10, 7, 8, это как бы слагаемые в этом ожидаемом размере,
только не разрезывая уже числа выполнения скобок,
ну и соответственно вот эти слагаемые переходят в какие-то другие, мы хотим, чтобы оно не уменьшилось.
Вот, но как раз если мы будем делать так, как здесь написано, то оно не уменьшится.
Вот, значит вот такой вот метод, но соответственно метод условных мат ожиданий,
метод условных мат ожиданий можно применять всегда, когда эти самые ожидания можно вычислить.
Потому что это, ну, соотношение-то верно всегда.
Вот это соотношение верно всегда, всегда у нас решение стоит из битов, всегда мы их можем по очереди как-то определять.
Вот, но если мы их не умеем считать, то мы тогда не поймем, будет вот это вот верно или вот это вот верно.
Вот, значит вот такой вот способ.
Так, но у меня еще остается немножко времени, я вкратце расскажу про метод парнонезависимости.
Другой метод называется метод парнонезависимых случайных величин.
Значит, тут идея такая, я тоже, значит, тоже на примере MaxCut, значит, например задача о максимальном разрезе.
Так, ну тут смотрите какая идея.
Да, значит, можно было, можно выбирать Xi1 и так далее, XiKT.
Значит, достаточно выбирать парнонезависимые, чтобы, соответственно, ожидаемый размер разреза был бы равен числу ребер пополам.
Значит, потому что, ну тут у нас было какое рассуждение, что у нас четыре варианта равновероятны.
Что обе вершины налево, обе вершины направо, первые налево, вторые направо и наоборот.
Но чтобы они были равновероятны, достаточно чтобы эти вершины попали независимо друг от друга налево и направо.
Ну а попарнонезависимые можно генерировать довольно простым образом.
Значит, можно сгенерировать два в степени L-1 попарнонезависимых случайных величин,
имея только L истинно независимых, то есть независимых совокупностей, называется, независимых совокупностях случайных величин.
Значит, именно подойдут X1 и так далее, XL. Да, они все бинарные, да, либо 0, либо единица.
Дальше X1 плюс X2, X1 плюс X3 и так далее, там X1 плюс XL и так далее, там XL минус 1 плюс XL, там X1 плюс X2 плюс X3.
Ну и так далее, да, в общем, это все суммы по всем непустым подможествам по модулю 2.
Вот их раз будет 2 в степени L минус 1, да, значит, кроме нуля.
Ну и тогда, соответственно, утверждается, что если мы две таких суммы рассмотрим, то тогда они будут независимыми.
Но идея в том, что там есть какой-то слагаем, который есть в одной сумме и нет в другой.
Например, вот эти вот X1 плюс X2 и X1 плюс X3. Мы можем, скажем, все кроме X3 зафиксировать.
Да, X1 плюс X2 тоже зафиксируется, да, а вот этот будет еще равномерно 0 и единицей.
Ну то есть развернется, совпадет или не совпадет.
Вот, но это одно из определений по паре независимости, да, что условное распределение точно такое же, как безусловное.
Ну а дальше, соответственно, нужно взять L будет у нас верхней целой частью от логарифма двоичного от N плюс 1.
Да, значит, тогда можно использовать вот эти вот суммы, значит, вышеуказанные суммы вместо X1 и так далее Xn.
И дальше идея следующая. Значит, идея следующая, да, что мат ожидания, мат ожидания по такому распределению, по такому распределению равно е пополам.
Тут уже не будет никаких условов. Мат ожидания самообычное. Безусловно, мат ожидания равно е пополам.
Вот, но дело в том, что тут всего есть полином вариантов. В данном случае ожидание это среднее из полинома вариантов.
Вот, но тогда этот полином, значит, этот полином можно просто перебрать, перебрать и выбрать максимум, который не меньше мат ожидания.
Ну вот, значит, тогда получается полиномиальный алтриц, который выбирает тоже хотя бы половину. Вот, вот такое вот рассуждение.
Значит, ну, на этом лекционная часть курса заканчивается. Да, значит, нам предстоит еще много чего.
Значит, у нас и самостоятельный контроль на и экзамен, и обсуждение проектов.
Вот, но и я надеюсь многих из вас увидеть в следующем семестре на курсе допглав.
Значит, он обязательно для кафедры ДМ, а для всех остальных по желанию в принципе тоже можно изучать.
Вот, там будет много чего интересного. В том числе может быть и про псевдослучайные конструкции поговорим.
Вот.

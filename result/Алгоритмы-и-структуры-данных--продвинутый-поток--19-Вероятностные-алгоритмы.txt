Ну, чем мы сегодня будем заниматься? Сегодня мы будем, наконец, сегодня и, видимо, в следующий раз будем заниматься вероятностными алгоритмами.
Ну, как я уже сказал, по-видимому, мы, видимо, в следующий раз же и закончим, потому что за сегодня всего можем вряд ли.
Вот, ну и в конце концов, надо же изучить такую сладостучирующую парительную тему, как фиши.
А там такие есть чрезвычайки. Вероятности вводить прям хорошо.
Вот. Но давайте начнем все отменить.
Итак, чем у нас вероятностные алгоритмы отличаются от того, по чему мы привыкли?
Что такое вероятностная симпатика?
Вот. Ну, начнем с того, что вероятностный алгоритм – это алгоритм, который иногда подразумевает бросание монетки.
Ну, или кубика. Не обязательно, конечно, мы будем. Чаще всего, конечно, мы будем бросать монетку с вероятностями.
Возможно, вероятностей одна вторая, но теоретически можно ее там бросать и с любой вероятностью.
Это не особая проблема для нас.
Ну, это означает, что у нас будут алгоритмы, в которых мы иногда...
В этом месте мы бросим кубик, посмотрим, что там выпало, и в зависимости от этого будем что-то делать.
Ну, или да, более абстрактная это будет случайность. Ну, она просто вот выражаться будет, скорее всего, именно так.
Вот. Ну, первый, на самом деле, мы уже сталкивались уже, как минимум, с двумя, на самом деле, вероятностными алгоритмами.
Ну, вот. Один из них называется кусок. Второй называется ку-оррерсатистик.
То есть, наконец, сейчас, в общем-то, когда вы уже, наконец, изучили немножко теорет, или еще не изучили,
или у вас была только теория мега, вероятностей вы не знаете.
Она молодца.
Прошла, даже дзшки были. Нет, просто чем это хорошо?
Это хорошо тем, что у нас, что, наконец, кусорты ку-оррерсатистика мы теперь можем исследовать, так сказать, нормально.
То есть, без всяких скидок на то, что мы там что-то там не знаем.
Вот, давайте вспомним, как у нас устроен кусорт классический.
Ну, кусорт устроен так. Как мы сортируем массив? Мы выбираем пайвот.
Значит, там каким-то мистическим образом переупорядочиваем элементы так, что у нас, значит, идут сначала элементы меньше пайвота,
потом, собственно, пайвот, потом больше пайвота, и после этого запускаемся рекурсивно отсюда и отсюда.
Было такое, да?
Ну, и у нас был мистический вопрос, как вычитать пайвот.
Ну, там какие-то детерминированные алгоритмы требовали, что там пайвот будет выбираться как-то алгоритмом поиска медианы за линию.
У нас там был какой-то алгоритм с пятью фамилиями.
Собственно, один из двух.
Но в вероятностном случае, обычно, конечно, используется вероятностная версия, которая говорит,
а давайте пайвот выберем рандомно.
Ну, что значит рандомно?
Ну, просто с вероятностью один делить на n, выберем просто вот один из этих элементов.
Может ли при этом алгоритм работать долларов?
Действительно, вполне.
Ну, а тут, на самом деле, есть два варианта.
На самом деле, как измерять алгоритм.
То есть, в данном случае мы будем говорить, что да, есть вероятность, что алгоритм будет работать плохо,
но давайте посмотрим, а как он будет работать в смысле мат ожидания.
Вот каково математическое ожидание времени работы?
Вот мистическое утверждение.
Мат ожидания, действительно, времени работы равно n лога.
Ну, если мы поверим в это утверждение, ну, сейчас мы его докажем, естественно,
но если мы его поверим, можно вообще в принципе сказать, что вероятность того, что этот алгоритм будет работать за n квадрат, крайне мала.
Да, именно, именно.
То есть, как минимум, говорим, что вот мы знаем, что, допустим, мат ожидания равно, допустим, там n лога, вот ровно n лога, даже не вот n лога на n лога.
Теперь, какова вероятность того, что, вероятно, что действительно это время работы будет, например, больше либо равно, чем n квадрат?
Мы знаем, что у нас есть неравенство Маркова.
Ну, точнее говоря, эта величина не отрицательная, поэтому у нас есть неравенство Маркова,
которое говорит нам о том, что эта вероятность будет не более, чем n лога n поделить на n квадрат.
Ну, в смысле, лога n поделить на n.
Ну, в общем, такая не то чтобы невозможная вероятность, хотя, конечно, если n у вас порядка миллиона, то, в общем-то, значит, там что-то типа получается там 20 миллионов.
То есть, ну, в принципе, если вы будете много запускать этот алгоритм, то, конечно, там раз в миллион операций действительно такая ситуация произойдет.
Ну, на самом деле, если вы знаете точным от ожидания, ну, прямо реально n лога n,
то, на самом деле, после некоторых экспериментов можно использовать чит.
То есть, например, такой красивый чит.
Вы говорите, что, значит, алгоритм работает так.
Вы его запускаете.
Но если вы совершенно случайно выясняете, вот случайно, что у вас t от n в какой-то момент стало больше, либо равно, там, я не знаю, 10 n лога n,
то вы обрубаете алгоритм и запускаете его заново.
Вот можно так же читерить, например.
Да, то есть, как бы, если вы поняли, что слишком долго работает.
Вот. То есть, тогда каково от ожидания времени работы тогда будет, да?
То есть, заметим, что вероятность того, что он будет работать прям 10 n лога n, это у нас получается одна десятая, да?
Тогда получается, значит, мот ожидания, значит, будет получаться какое?
Ну вот. То есть, там, ну вот. То есть, получается, значит, тогда вот, так сказать, t штрих от n, значит, с вероятностью 9 десятых у вас будет мот ожидания не более, чем n лога n, да?
Плюс одна десятая, на, получается, 10 n лога n плюс t штрих от n.
Логично, да?
Нет. Почему t штрих от n?
Ну потому что с вероятностью одна десятая, как бы, во-первых, вы потратили 10 n лога n, а, во-вторых, после этого вы как бы алгоритм запустили заново.
А, ну если он же самый?
Ну, ну было то же самое, что повторим, ту же самую операцию. Будем повторять до тех пор, пока он за 10 n лога n не справится.
Ну вот. Ну вот. То есть, если пораскрывать скобки, то там что получится? 19 десятых n лога n.
Вот.
И плюс одна десятая t штрих от n.
Ну а отсюда автоматически следует, что t штрих от n тогда получается у нас там сколько?
Ну что-то типа 19 девятых n лога n.
Неплохо, в общем-то, правильно?
Вот.
Ну, то есть, конечно, то есть, на самом деле так, вероятность, то есть, она, конечно, не лишает, то есть, в общем-то, не то, чтобы это было...
Ну, то есть, в общем-то, правда, проблему-то, в общем-то, это не решает.
Слушай, как бы теоретически все еще может быть, что, значит, там будет фатально не вести, алгоритм будет работать долго, в общем-то, проблема от этого не решается.
Вот.
Хотя, конечно, можно тогда посчитать, действительно, сколько времени это будет работать.
Вот. Ну, правда, поэтому, видимо, в, например, в СТЛе какой-нибудь проблема решается по-другому.
Там говорят, что если алгоритм работает долго, то есть там глубина рекурсии слишком большая, то на этой глубине рекурсии мы просто используем другой алгоритм сортировки.
Нет, там хипсорт.
Там используется хипсорт, чтобы не использовать лишнюю память.
Ну, хипсорт можно как бы забавахать, не выходя за пределы массива просто и там этим пользоваться.
Вот.
Ну, это мелочи.
Вот.
То есть, мы видим, что в данном случае, этот алгоритм у нас...
То есть, это называется алгоритм типа Лас-Верес.
Ладно.
Значит, алгоритм типа Лас-Верес говорит так, что он всегда работает правильно, просто в среднем работает хорошо, но иногда может работать плохо.
В отличие от алгоритма типа Монте-Карло.
Он говорит так.
Монте-Карло, то есть тут другой чит.
Он говорит там, что этот алгоритм всегда работает быстро, но с некоторой вероятностью он фейлится.
То есть, вообще не сработает.
Ну да.
Ну, там вот у нас будет алгоритм, который будет говорить, что он работает быстро, но он там с вероятностью какой-нибудь, найденным ответ может быть не так.
Ну как бы, если эта вероятность не слишком большая, то, в общем-то, это нас тоже может устроить.
То есть, у нас, конечно, идти-идти алгоритмы, но у нас сортировка всегда работает, просто иногда долгая, Лас-Верес.
Ну а давайте поймем, почему мы от ожидания в таком виде Н-логгер.
Ну, давайте поймем, почему мы от ожидания в таком виде Н-логгер.
Ну, идея очень простая.
Как вы помните, если вы помните алгоритм, то помните, что алгоритм занимается тем, что когда он выбирает пайвет, что он с этим пайветом сравнивает, по сути, все вот эти элементы, правда?
Поэтому мы от ожидания времени работаем, можно с точностью до константа оценить, как количество сравнений.
То есть, вот нам жутко интересно.
То есть, нам вообще, по идее, жутко интересно, сколько сравнений будет выполнено, правда?
То есть, как бы да.
То есть, как бы говорим, что мат ожидания Т от Н равно мат ожидания количество сравнений.
Вот.
А это можно написать неожиданно.
Это можно писать как...
Значит, давайте прибежимся по 1 меньше либо равно i, меньше либо равно j, меньше либо равно m, x и t.
Значит, давайте прибежимся по 1 меньше либо равно i, меньше либо равно j, меньше либо равно m, x и t.
Где x и t ж, это будет такая случайная величина, которая будет говорить, что 1, если i и j были сравнены, и 0 иначе.
Да, на всякий случай я скажу так.
Давайте для простоты анализа считать, что мы с вами сортируем перестановку от 1 до N.
Ну, по большому счету, не сильно принципиально, да?
Вот.
Я бы хотел сказать, что, типа там, а i и t...
Ну, если иначе говорить на булке, а i и t ожиданно сравнены?
Ну да. Ну, правильно надо было писать, было сравнено а в скобочках i и t, потому что вот i и t порядка эстетического так обозначается, да?
Ну, давайте там, не будем сейчас нагаблаживать эти обозначения, скажем, что сортируем числа от 1 до N и не паримся.
Не, а почему нельзя было сказать, что у нас просто массив был от 1 до N? Ну, в плане, это индексы я имею в виду.
Ну, индексы? Нет, потому что они путать. Я имею в виду именно значения, а не...
А, это принципиально.
Да. Это сейчас будет принципиально.
Вот тут x и t и j это значения?
Ну, i и j это значения, да. То есть мы сортируем числа от 1 до N.
И вот i и j, как бы именно сами i и j.
Так. Ну, опять давайте начнем с того, что мы от ожидания линей, да?
Вот. Ну, потому что это там интеграл и бега, там бла-бла-бла.
Так.
По-моему, мы не в теме сейчас.
В смысле?
В смысле.
Да, но это не тот случай.
Нет, ну я просто помню. Что такое от ожидания? Это интеграл и бега такой.
Ну, это что, у вас не было? Вы такими словами не ругаетесь?
Нет, у нас просто еще не было. Ну, понятно, что это интеграл и бега, но мы еще не подходили.
Вели в дискретном случае и довольны.
А, ну ладно. А, ну в общем-то ладно, нам в дискретном случае вот так хватит.
Ну ладно, подождание, короче, линейно.
Более того, у таких индикаторных величин от ожидания это просто вероятность.
Вот. Сравниваются.
Вот один меньше.
И меньше, и меньше, и меньше.
А теперь внимание, вот внимание, первый интересный вопрос на сегодня.
Будут ли сравниваться, с какой вероятностью будут сравнены элементы иежи?
Ну, в идее на мой сей элемент сравниваю с пивотом, да?
Но не только с ним.
Да.
Поэтому единственное, которое я вам предадю, когда либо иди, либо же окажется пивотом.
Ну да.
То есть да, заметил, что действительно нужно, ну не совсем так.
Конечно, для того, чтобы они сравнились, нужно, чтобы и кто-то из них оказался пивотом, это необходимо.
Но с другой стороны, мне очень интересно, по одной простой причине.
Пивотом окажется практически все.
Ну алгоритм у нас устроен так, что у нас почти все оказывается пивотом, но если только какой-то элемент совсем уж отдельно не остался.
Вот.
Дело тут другое.
Если что-то между иежи раньше станет пивотом, то они не будут сравниваться.
Да, но фишка в том, что когда вот если бы они были пивотом, то они не будут сравниваться.
Да.
Да, но фишка в том, что когда вот если бы и станет пивотом, то надо, чтобы в этот момент и жи оказался с ним в одном массиве.
Так.
И тогда действительно, смотрите, как это все происходит.
Вот когда мы выбираем пивот, и иежи, ну некоторые там могут оказаться в одной стороне.
Ну тогда то, что происходит справа, на их сравнивость уже не влияет.
Тут тоже выбирается какой-то пивот, они отправляются в еще.
И так происходит до тех пор, пока не выпадает какой-то пивот.
Пока, опять, впервые в жизни не выпадает какой-то пивот между иежи.
И тогда происходит, и тогда у нас два варианта.
Либо этот пивот совпал с и или жи, тогда элементы сравниваться будут.
Либо выпадет что-то между ними, и тогда, соответственно, они сравниваться уже не будут никогда.
Но возникает вопрос, с какой вероятностью произойдет первое, с какой вероятностью произойдет второе.
То есть с какой вероятностью первый пивот который у нас выпал на отрезке от и до жи,
окажется именно и или жи.
Но очевидно что это равно два поделить на ж и минус от и за все.
Ну потому что, очевидно что каждый раз когда мы кидаем рандом с участником под отрезка иежи,
каждый из этих элементов выпадает с равной вероятностью.
Правда?
Получается, суммируем i и меньше j, меньше либо равно n, и получается 2 поделить на j минус i плюс i.
Ну, дальше, в общем-то, делаем просто немножко алгебры.
То есть, дальше мы перебираем просто отрезки длины, соответственно, там длин от 2 до n, и для каждого отрезка длины l,
значит, у нас будет, вероятно, поделить на l, а умножить на количество. Сколько у нас под отрезком длины l?
Ну, то есть, это равно, смотрите, 2n сумма 1 делить на l, l равно 2n, минус... ну, что-то там.
Ну, это да. Ну, можно сказать, да, минус 2, там 2n минус 1, и плюс еще какая-то гадость.
Вот. Не особо, даже в лом писать, потому что не особо интересно.
Ну, потому что смотрим.
Ну, если быть точнее, мы ждем первого пивода, который лежит на отрезке между i и j.
Этим первым пиводом может быть разно вероятно кто угодно из них.
Но при этом, если это будет i и j, и i и j, то они будут сравниваться. А если кто угодно остальной, то не будет.
Поэтому вероятность того, что элементы i и j сравниваются, это вероятность того, что выпадет именно i и j.
То есть, 2 поделить на... вот сколько тут элементов на отрезке, на отрезке, вот, стоп.
Вот. Так. Ну, к чему нас это приводит?
Ну, на самом деле, очевидно, что эта сумма, ну, заметим, что вот эта вот сумма, да, это тета от логен, как мы знаем.
Так мы уже, по-моему, выясняли. Выясняли? Или не выясняли?
Ну, да. Ну, давайте я напомню, да, что если вы складываете 1, 1, 2, 1, 3, 1, 4, там, 1, 5, 1, 6, 1, 7, там, 1, 8 и так далее, 1, 9.
То, как бы, снизу это можно оценивать как, допустим, 1, 1, 2, там, 1, 4, 1, 4, 1, 8, 1, 8, 1, 8, 1, 8, там, 1, 16, да.
А сверху это можно оценивать как, 1, 2, 1, 2, 1, 4, 1, 4, 1, 4, 1, 8, 1, 8, ну и так далее.
Ну, через интеграл, точнее, цена. Чего? Через интеграл, точнее, цена, как так говорится.
Ну, это да. Тогда еще надо доказывать, как эта сумма связана с интегралом, там, бла-бла-бла.
Ну, просто в этом месте вот так нагляднее.
Сразу достаточно очевидно, что это лог n плюс-минус константа.
Это лог n. Ну, это все по сравнению с n лог n мелочи.
Очень короче, вы пишем, что это тета от n лог n.
Вот. Так что-то. Ну, вот, получилось, вроде, не сильно сложно.
Ну, у нас с вами будет и чуть-чуть повеселее.
Ну, тоже веселее. Чуть веселее, оказывается, если мы пытаемся изучать q-order статистик.
Ну, давайте вспомним. Как мы ищем q-order статистику рандома?
Делаем то же самое, только идем только в одну сторону.
Поэтому здесь оказывается, что... Ну, там тоже, в общем, если...
Ну, я не буду сейчас переписывать эти все заклинания.
Но там тоже оказывается, что тут надо искать вероятность того, что i и g сравниваются.
Но здесь эта вероятность, конечно, хитрее.
Почему?
Потому что заметим, что эта вероятность теперь зависит не только от i и g, но и от собственного числа k.
Который мы ищем.
Вот. То есть и там.
Ну, вот, давайте думать. А каким образом, действительно, она от этого зависит?
Ну, если в какой-то момент пилот так, что с k с одной стороны, а i и g с другой, то там, где i и g, соответственно, не будет.
Ну, да. Поэтому придется рассмотреть, на самом деле, три случая.
То есть случаи, допустим, k меньше i, меньше g, i меньше k, меньше g.
Ну и, соответственно, i меньше g, меньше k.
По-хорошему еще надо рассмотреть, в случае k равно i меньше g, и i меньше g равно k.
Ну, что делать? Это математика.
Математика – наука точная, даже если мы говорим о вероятности асимпотики.
Асимпотические почти, наверное.
Ну, давайте рассмотрим в случае k меньше i, меньше g.
Так, вот у нас был отрезок. Значит, где-то у нас вот этот элемент k, где-то i, где-то g.
И у нас должно произойти, чтобы элементы i и g сравнились.
Ну, во-первых, да, заметим, что пока у нас пивоты падают куда-то слева от k или справа от g, то k и g все в игре, все в порядке.
Вот. Но как только выпадает элемент вот на этом отрезке, нам сразу становится плохо.
Ну, неплохо, а мы сразу понимаем, будут ли сравниваться элементы i и g.
Ну, если выпадает элемент i или элемент g, то сравнение произойдет, правда?
Если выпадет элемент между i и g, то i и g уже не сравнятся, потому что как минимум один из них вылетит.
Но рамец теперь в том, что если выпадет кто-то между k и i, включая k, то сравнения тоже не будет.
Потому что просто элементы i и g вылетят, собственно, до того, как вообще успели пикнуть.
Поэтому получается, что суммируем по всем k, значит, меньше i, меньше g, меньше либо равно n, 2 поделить на g-k.
А тебе, кстати, вычекает вопрос, а не то же самое ли будет, если k у меня будет равно i?
Тоже самое.
Ну, то же самое, да. То есть, если k равно i, вот, допустим, то как бы вот то же самое, то, в общем-то, то же самое.
То есть, как бы хорошими элементами продолжают быть i, t и g.
Вот.
Аналогично, ну, думаю, вы согласитесь со мной, что аналогично можно написать тут k-i+,
для случая, когда i меньше g, меньше либо равно k.
Ну, абсолютно то же самое, то же симметрично, правда?
И вот остался у нас только один случай, такой интересный хотя бы.
Так вот.
Когда k находится строго между i и g.
И с какой какая-то счёт в этом случае i и g будут сравниваться?
Ну, с такой же, в смысле.
Посмотрим на отрезок между i и g.
Если ты вот выкатишься между либо на i, либо на g, то они сравняются иначе нет.
Ну да, то есть опять же, да, то есть.
Ну, короче, да.
Давай делить на i минус g плюс i.
Ладно, наврал, g минус i плюс i.
Так.
Так.
Но остаётся только маленькая задачка.
Это всё теперь свернуть.
Как свернуть?
Каофиксировано?
Каофиксировано, естественно, потому что у нас запрошена катая порядковая статистика.
Может пытаться для каждой дни почитать сколько будет?
Ну да.
Ну давайте так, ладно, не будем прям сразу пытаться.
То есть здесь остаётся только...
То есть заметим, да, что если нас...
Да, то есть если мы перебираем...
То есть можно так, давайте тут перебирать g и при g перебирать подходящий i.
То есть получится два...
Сейчас.
Чего в третьем случае?
В следующем случае k должно быть между i и g.
А, ой, ой, ой, ой, ой, ой.
Да, вот, да.
Забыл, забыл, забыл, забыл.
Вот.
Значит, перебираем.
Значит, случаи, когда k меньше g меньше либо равно n.
И утверждается, что тогда пишем 2 g минус k
поделить на g минус k плюс i.
Ну, по типу, уже неплохо, потому что это не превосходит o от n, правда?
Плюс...
Ну, аналогичным образом здесь можем написать, что здесь будет 2 k минус i
поделить на k минус i плюс i.
Где, наоборот, 1 меньше либо равно i меньше k меньше либо равно n.
Ну, k, правда, фиксировано, да, по этим так?
Не совсем правильно писать.
Да, правильно не вот так писать.
Кстати, заметим, что в сумме вот это вот не превосходит прям не o от n, а просто 2 n.
Видим, да?
Вот.
Ну, а теперь самое интересное.
Ну, здесь, в общем-то, ладно, не очень интересно получается, да.
Ну, теперь надо перебрать...
Так, вот перебрать, соответственно, i и g.
Вот.
Ну, здесь, конечно, хочется...
Здесь мы воспользуемся следующим счетом.
Я снова переберу потенциальные длины отрезков.
Да, оценю сверху.
То есть нам нужно найти, сколько отрезков длины l
может содержать в себе k.
Ну, я утверждаю неожиданный счет.
Количество таких отрезков, содержащих конкретные элементы, длины l, не превосходит l.
Да, вот примерно вот такие вот...
Вот такая вот картинка, в принципе, сразу намекает, почему.
Не намекает?
Ну да, то есть этот элемент может быть либо первым там, в последнем в этом отрезке, либо предпоследним, ну и так далее.
Поэтому получается, сверху это можно оценить как l.
Ну, понятно, что их может быть меньше, если там некоторые отрезки выходят за пределы массива.
Но суть одна.
То есть оценить можно сверху вот так.
Но это, очевидно, меньше либо равно, чем банально 3n.
Все.
То есть оказалось чуть сложнее, тут пришло чуть цитрее подумать, но это все равно у изоров остается маленькой-маленькой техникой.
Ну вот, такие получились два, можно сказать, разминочных для нас алгоритма.
Так, по ним тут какие-то вопросы есть?
Вот, ну да, тут думаю все понятно.
Вот, ну сейчас мы рассмотрим алгоритм, который просто покажет, что вероятности иногда могут дать существенный, прям существенный прогресс, на самом деле.
То есть, ведь мы знаем, что qsort и qord статистики, в принципе, можно реализовать за mlogn и n в честном.
Да, то есть там, там с сильно большими заморочками, но можно.
Вот, то есть пока эта вероятность, это типа не, типа облегчило нам значение.
Сейчас мы просто рассмотрим пример алгоритма, где вероятность дает то, что детерминированность не даст нам никогда.
Давай.
Наверное, по лучшему у нас остался какой-то вопрос.
Что, что давай?
Почему у нас всегда на картинках уже, как бы, ассоциированы ежи?
То есть, мы, конечно, знаем, что имя из ежи, но мы не можем имитирующую позицию.
Ну, потому что, ну, скажем так, на картинке мы это изображаем, потому что, ну, как бы нам, ну, как бы так, тут позициям придираться не надо.
Потому что мы, когда оцениваем вероятность того, что будут они сравнены, мы оцениваем же, как бы, смотрим на пайлаты между ежи.
Отсюда и картинка.
То есть, конечно же, в этот момент же и ежи не обязаны идти в массиве именно в этом порядке.
Вот.
Ну, и, конечно, обратим внимание, что вероятность, заметим, что, заметим, алгоритм устроен так, что вероятность, как бы,
то есть, вероятность, эти вероятности могут зависеть, ну, разве что от числа m, называется, но не от самих данных, да?
То есть, заметим, что, то есть, заметим, что, тут вот, иногда вот, то есть, иногда вот есть ошибочное восприятие, что, как бы, кусок в среднем работает за нлогами.
Это означает, что, если перебрать все возможные данные, то в среднем будет нлогам.
То есть, якобы, есть там какие-то данные, на которых кусок всегда будет n2.
Ну, если, да, можно так написать кусок, но, заметим, это не тот случай.
То есть, мы написали алгоритм так, что, какую бы вам перестановку не дали, да?
То есть, верно так, что, если вы на одной и той же перестановке будете запускать этот кусок, то в среднее время работы будет нлогам.
Хотя, периодически он будет затуплять, но, как бы, на любой перестановке может затупить.
Ну, просто хотелось обратить на это внимание.
То есть, заметим, что, как бы, эта вероятность, конечно, приятна, то есть, заметим, что эта вероятность, ну, то есть, приятна тем, что, как бы, мега-контр-теста против нее не приведешь, если только, конечно, автор тестов случайно не узнал, не узнал, какие у вас будут случайные числа.
Ну, на Кутфорсе было какое-то развлечение, что там, как бы, не, там, детерминированные решения нельзя придумать принципиально, просто его не существует.
А вероятность стоять дальше просто потом, называется, приходят взломеры и, собственно, взламывают вас, там, пытаясь просто угадывать, какой у вас там рандом будет.
По-моему, это очень шедевр.
Ну, не знаю, по-моему, это шедевр, знаете, по-моему, то есть, по-моему, это...
То есть, по-моему, то есть, это называется просто, это был контест МТеха, по-моему, какой-то, и, по-моему, это называется МТех взломал систему, это называется.
Ну, вот, и это классно. Такого бы побольше.
То есть, там, ну, понятно, что, конечно, выкручивалось там все, что, типа, делай рандом так, что тебя не могли завалить, там, в зависимости от времени или что-нибудь еще в этом роде, я помню.
Хотя там авторы говорили, что они там очень много чего умели валить, но, конечно же, в системных тестах таких тестов там такого не было.
Ну, если смотреть, то это хорошая вещь.
Не, ну, там всегда проблемы, да, но, как бы, ну, от взломов это не защищает.
Доказаться, как у тебя появляются числа, послать нужный момент времени.
Да, да, да.
Учесть еще задержку.
Ну, там шедевр был, да.
Да, но это классно было, действительно.
В общем, главное, это называется задача, которая может появиться только на Кутфорсисе, это называется.
Ну, точнее, так, она может появиться в любой Олимпиаде, она будет даже интересной, но...
Ну, как бы, будет не то.
Ну, вот.
Так вот.
Кстати, но вот.
Но сейчас мы рассмотрим, сейчас мы снова окунемся в сладостный терующий упоительный мир теории игр.
Но у нас будет очень тупая игра.
Играет, значит, представьте себе.
Играет два человека.
И они делают там по очереди ходы, каждый делает N ходов.
И каждый ход говорит...
Каждый ход игрок говорит, я иду, условно, там влево или вправо.
И вот.
Ну, получается, вот.
И в конце, значит, они делают так по N раз, и в конце появляется теорующая система, которая выдает, кто победил.
Просто.
Первый и второй.
Можно это вообразить себе, но чтобы все заранее...
Ну, можно так сказать, смотрите.
То есть другими словами.
Есть такое вот дерево.
Высоты, соответственно, там.
Твоя.
Вот такое полное двойечное дерево.
Ну, вот.
И вот.
Ну и так далее.
И в конце у вас тут есть 4 в степени N исхода.
И вам нужно посчитать, кто выиграл.
Ну, кажется, что можно ли это сделать быстрее, чем за 4 в степени N?
Сейчас, а, 2 в степени N.
Сейчас, а, 2 человека играют по игру.
Да, то есть они ходят по очереди только вниз.
Да.
И в каждом листе написано, кто выиграл.
То есть в каких-то листах написано, что выиграет первый игрок, в каких-то листах написано, что выиграет второй.
И мы знаем это дерево?
Вот вопрос, что такое знаем.
В некотором смысле знаем.
Ну, потому что что значит знаем?
Если знаем в плане, что у нас там выписано полное это дерево, то, конечно, быстрее, чем за 4 N, мы не можем это решить в принципе, потому что надо все дерево еще и считать, да?
Вот.
Но чаще бывает другая ситуация.
Бывает, скорее, ситуация, что мы не хотим читать 4 в степени N, прям все.
Но, допустим, мы можем сделать запрос в растеривающую систему про каждый лист, и в этом листе она скажет, кто победил.
То есть в этом случае тогда, казалось бы, нам все листы вроде перебирать необязательно.
Можно ли в этом случае решить задачу быстрее, чем за 4 степени N?
Что теперь в 4 степени N?
Ну, понять, кто выиграет.
Какая же игра.
А.
Какие у нас еще задачи в тюрьме?
Ну, просто у нас нет критерия.
То есть тестирующая система, это челная ящика.
Ну, конечно, и локус.
Ну, как бы да.
Нет, известно только, что для каждого листа ответ детерминирован.
Вот.
Поэтому соответственно.
Ты бы думал, что это тестирующий стиль записано даже в 9?
Да.
Типа, человеки там, например, нажимают кнопки на тестирующий стиль.
Нет, это я все понимаю же.
Ну, да.
У нас вопрос на те...
Я не понимаю.
Может ли первокунить?
И как?
Или просто можно?
Подождите, вопрос как стоит?
Чего?
Ну, можно ли понять, кто выиграет, делая гарантированно меньше чем 4 степени N?
А можно понять гарантированно или как?
Начинаем с гарантии.
Начнем с гарантированного.
Пока вот без вероятности.
Если гарантированно, то 4 степени N.
Почему?
Нет, можно не гарантированно.
Представь, что в левом по дереве, во всей левой половине, побеждает первый.
Тогда первый пойдет на левую половину, в левую по дереву и выиграет.
А это уже ты пользуешься тем крутыми идеями.
Нет, это конец.
Ну, вот тут проблема...
Нет, с одной стороны, да.
Но с другой стороны, правда, для этого нужно как-то еще убедиться, что тут везде единицы, действительно.
Говорят, какие-то боли от всех.
4 степени N?
Опа.
Ну да, действительно.
То есть теоретически можно работать по принципу, что мы запустимся рекурсивно отсюда, поймем, кто выиграет.
Если понимаем, что тут выигрывает первый, то здесь можно вообще ничего не изучать.
Может быть, какой-то такой алгоритм действительно может быть.
Но утверждение такое.
Я утверждаю, что не существует детерминированного алгоритма, который есть в левом по дереву,
не существует детерминированного алгоритма, который гарантированно проверит строго меньше, чем 4 в степени N и 100.
Почему?
Ну, утверждение базируется на следующем.
То есть мы знаем, как будет устроен этот алгоритм.
И...
Нет, вот как будет сформулировать?
Что-то ссылается на определение информации.
Нет, не-не-не.
Ну, на самом деле, идея должна быть простая.
То есть надо гарантировать, что обязательно там найдется, в худшем случае, ситуация, чтобы, допустим, не проверили хотя бы одну штуку,
вот не проверили.
А на самом деле от нее ответ зависит.
То есть вот как-то так.
Да и действительно все равно всегда ли можно действительно такое...
Проверить.
Сейчас.
А мы вообще считаем, что зафиксированные числа записаны в каждом месте, и они могут...
Ну, как бы так.
Они заранее зафиксированы, но естественно у нас дирамический чекер может иметь место.
Можно построить какой-то пример.
Ну да, то есть как бы тут задача, что под каждый детерминированный алгоритм, там надо пытаться строить контрпример.
Который валит...
Ну да, ну вот в качестве примера, допустим, у нас была бы высота дерева вообще один, да?
Утверждение. Не существует алгоритма, который позволит нам проверять только одну монетку.
Доказательства.
Ну, потому что проверим, вот в этой, как бы здесь ходит второй игрок, да?
И мы проверяем какой-то из этих листов и выясняем, что он выигрывает первый.
Вот внимание, вопрос, кто теперь выигрывает при правильной игре?
Для этого мы должны... Понятно, что второй игрок вынужден ходить сюда.
Но если там двойка, значит он выигрывает.
Ну если там не двойка, то он проигрывает. Но это мы должны знать.
Получается, конкретно здесь мы понимаем, что у нас проблемы.
Ого.
Это доказательства или это контрпрек?
Это доказательства, если у нас дерево было высоты 1.
Да, вот давайте посмотрим. Если у нас дерево высоты 2, давайте посмотрим.
Если у нас дерево высоты 2.
Ну вот, то утверждается, что мы...
Переберем алгоритмы.
Чего?
Берем алгоритм, смотрим, какой лист он не посетит и поставим туда.
Нет, ну не совсем, понимаете, там проблема такая, что...
Как бы, скажем так, дело в том, какие листы алгоритм будет проверять, это может зависеть от того, какие результаты алгоритм получил.
То есть он как бы проверил лист, получил лист, а ты в зависимости от этого пошел думать.
Тут еще вот такая проблема может быть.
Можно сказать, пусть он проверит, а, он динамически проверяет.
Вот.
Ну вот, но на самом деле утверждение такое, что...
То есть, ну на самом деле можно сделать так, вот, например, первый игрок думает, куда ему пойти, да?
Но я утверждаю следующее, что пока вот про эти вершины не будет понятно, куда идти,
он никуда сходить не сможет.
То есть мы по нему ничего не скажем.
То есть если мы, то есть если у нас как бы информации недостаточно, чтоб сказать вот про, вот, не про эту вершину, не про эту вершину, кто улыкает, то про эту вершину мы тоже ничего сказать не можем, правда?
Ну просто типа может быть и так, и так.
Если в обеих из них может победить как один, так и другой, в зависимости от того, как мы дополним несуществующие данные, то и здесь также.
Ну вот, ну допустим, там, значит, мы уже знаем, что вот в этом дереве можем подгонять тесты так, что мы пока всем они все не спросим, ничего не узнаем, правда?
И вот наконец, неожиданно выяснилось, что в одном поддереве мы узнали все.
В первом поддереве узнали все.
В чем мы знали, да, что тест можно было подгонять так, что мы прям до последнего не знаем результатов.
Но теперь заметим, что когда нас спрашивают последнюю монетку, наш динамический чекер может подсунуть нам такой тест, на котором это будет, он нам будет сообщать, что здесь выигрывает второй игрок.
Ну тогда получается, чтоб первому понять выигрывает он или нет, придется узнать все здесь.
Ну а здесь мы тоже уже поняли, что тест можно подгонять, правда?
И вот, то есть следовательно, и на дереве высоты 2, соответственно, в худшем случае придется узнавать все.
Понятно, да?
Дальше по индукции.
Ну а дальше, аккуратненько, также по индукции мы доказываем, что любой алгоритм в худшем случае делает четыре степени запроса.
В детерминированных смыслах.
Ну вот, понятно, да?
Все кроме, как мы доказали, что мы всегда можем, наш алгоритм будет спрашивать нас типа, что в этой худшем случае, что в этой, что в этой.
Ну, почему я всегда считаю алгоритм, который сможет подобрать так, чтобы справа была двоечка?
Ну вот, а вот по индукции.
Ну просто давайте, переход, сейчас я покажу, как делается переход индукции.
Ну вот, допустим, давайте скажем, что здесь ходит первый игрок, да?
Допустим, мы по индукции доказали, что на деревьях меньшей высоты, действительно любой алгоритм, значит, подсунет тест, где пока вы не спросите все монетки, вы не будете знать результаты.
Тогда теперь идея такая.
Вот у нас есть два дерева.
И говорим, если, значит, алгоритм устраивает там какие-то запросы.
Какие-то запросы идут в левое под дерево, какие-то в правое, да?
Каждый раз, когда запрос идёт в левое под дерево, мы в нём отвечаем в соответствии с тем вот это придумыванием контратеста для левого под дерево.
А когда у нас приходит запрос в правое под дерево, мы делаем контратест для правого.
Это приводит нас к тому, что пока в каком-то из под деревьев мы не спросили все листы, мы результаты игры на этом под дереве не знаем.
И так в какой-то момент произошло, что мы без ограничения в обществе у левого под дерева спросили листы и, соответственно, теперь знаем результат.
Но заметим, что алгоритм у нас был устроен так, что он может подсунуть такой тест, чтобы этот результат был два.
То есть у нас предположение ещё более сильное.
Не только что мы до последнего не знаем результат, а в том плане, что когда у нас спрашивают последний лист,
мы можем подогнать результат так, чтобы результат был как выигрывает первый, так и выигрывает второй.
Понятно, да?
То есть что значит, что мы до последнего не знаем?
Это означает, что в зависимости от последнего листа результат будет и такой, и такой.
Потому что если мы знаем, что в зависимости от этого победит первый, значит уже можно его не спрашивать.
Но тогда, соответственно, тогда, когда будет спрашивать последний лист, подгоним результат так, чтобы тут победил второй.
Тогда, получается, нам придётся к его бизнесу выяснить, кто победит здесь.
Ну и всё. А здесь мы тоже подгоним. Вот и всё.
То есть, конечно, видите, всё плохо.
То есть, видите, игра такая, что если есть тупое решение за четыре степени, а лучше нельзя принципиально,
потому что интерактор может подогнать контртест.
Ну а что делать, если вам Dobri Code Forces пишет, что интерактор статический, а не динамический?
То есть динамический интерактор, это когда там он как бы строит, то есть как бы на самом деле никакого теста нет,
он как бы подгоняет плохие ответы, но в любой момент времени готов тебе предъявить,
что как бы существует тест, на котором как бы я тебе вытал именно это.
А что делать, если он заранее сгенерировал тест и как бы уже честно выдаёт?
Тогда, нет, тогда оказывается неожиданный чит.
Тогда у вас окажется есть возможность делать вероятностные решения.
То есть заметим, да, что если бы интерактор динамический, то никакая вероятность, то никакая,
каким бы вы монетками решали, куда делать запросы, вам бы это не помогло.
Логично, да?
Так, понятно, понятно, понятно, что я говорю, да?
Вот.
А теперь представим себе, что интерактор статический.
А теперь представим себе, что интерактор статический.
Тогда, значит, утверждается следующее.
Значит, тогда теперь поехали.
Значит, алгоритм теперь такой.
Допустим, значит, мы пытаемся решить дерево высоты h.
Понятно, да?
Ну, то есть пусть у нас дерево высоты h, и мы пытаемся решить.
В этой обществе скажем, что, а давайте вот так, до красоты скажем, что здесь теперь проходит второй игрок,
но если первый, там все то же самое.
Ну так, для разнообразия просто.
Значит, идея такая.
Мы запустимся рекурсивно от одного из ребенков.
От какого ребенка мы запустимся?
От этого.
Совершенно верно.
С вероятностью одна вторая, прям реально монетку кинем.
И чем это нам тогда поможет?
Ну, заметим следующее, что у нас бывает два варианта.
Ну, ладно, по большому счету у нас есть три варианта.
Первый вариант.
Ну, если, то есть заметим, что если хотя бы в одном из, вот, например, если в одном из этих решений выигрывает двойка в другой единица,
то на самом деле с вероятностью одна вторая произойдет следующее.
Мы запустимся от этой вершины.
То есть мы запустимся от этой вершины, поймем, что там выигрывает двойка, и сюда даже не полезем, потому что нам будет по барабану.
Но, правда, если конкретно один-два, то получится, что с вероятностью, что тогда с вероятностью одна вторая, да, мы сюда полезем тем, что выиграет единица, и сюда все-таки полезем.
Но видим, что в среднем у нас уже получится в этом случае не два, а полтора рекурсивных вызовов, правда?
Вот, понятная идея?
Ну, смотрите, идея немножко сокращения себе жизни.
Вот.
Значит, смотрите, то есть мы кидаем монетку в какой лист идти, да, то есть в какой ребенка идти, да, мы не знаем результатов, да, но заметим, что мы пошли, допустим, в правый, в правого ребенка.
Он там запустился-запустился, и мы выяснили, что там, оказывается, результат побеждает второй игрок.
Заметим, что мы тогда в это под дерево не полезем вообще, потому что мы и так знаем, что вот мы сюда пойдем, мы точно выехаем.
Вот.
Но, может, не повезти, но если окажется, что единица, тогда мы второй рекурсивный запуск все-таки сделаем.
Да.
Но к чему это нас приведет?
Тогда нас это приводит к тому, что у нас, как бы, если в одном побеждает первое, в другой второй, то в отожигании рекурсивных запусков полтора.
Ну, потому что мы с приоритетностью одна-вторая сделаем один рекурсивный запуск, а с приоритетностью одна-вторая сделаем два рекурсивных запуска.
Да, при этом, если, да, бывает тест, если в обоих выигрывает второй, то мы, в принципе, сделаем один рекурсивный запуск.
Ну и есть еще плохой случай, если там один-один, то, увы, не судьба.
По-любому придется делать два рекурсивных запуска и грустно заключить, что, то есть грустно заключить, что, собственно, мы по-любому проиграли.
Понимаете, да?
Но оказывается, этот тест не сильно страшен.
Вот, рассмотрим его.
А что, а как мы будем выяснять вот эти идеички?
Вот, допустим, выяснилось, что первый игрок здесь выигрывает оба раза, да?
Вот, допустим, рассмотрим ситуацию, когда вот у нас ходит второй игрок, мы пытаемся понять, выиграет он или нет, и эта вершина, на самом деле, проигрышная, да?
Тогда получается, что мы делаем рекурсивный запуск от двух вершин, в каждой из которых, значит, ходит первая вершина, и она выигрывает, правда?
Ну, что значит, что она выигрывает?
Это означает, что когда мы тут будем запускаться, как минимум в одном и ребенке, то есть, как минимум в одном ребенке, первый игрок реально выиграет, правда?
А то и в другом.
И тогда получается, что на этом уровне, на уровне вот на два нижа, получается мат ожиданий рекурсивного запуска.
Не более чем три.
То есть, как бы мы, ну, потому что, смотрите, здесь мы запустимся и от этого, и от этого, да?
Но у этот запустится, там, в среднем, от полутора из своих детей.
Не более чем, и тут не более чем полтора, в сумме три.
Вот.
А ведь вообще, это было бы очень неплохо, на самом деле, да?
То есть, если вот так аккуратно прописывать...
Потому что у нас как бы там рекуррента получается такая, да?
То есть, ну, там у нас там рекуррента получается что-то типа, значит, мат ожидания t от n.
Ну, так я сейчас в кавычках это все напишу.
Значит, равно, там, типа, от единицы плюс там условно три, там, вот, t от h, да?
t от h, да?
Е, там, мат ожидания t от h минус два.
Ну, так, в кавычках.
Сейчас, это вообще, в любом случае, или...
Ну, пока мы доказали это только в один-один, но если это произойдет всегда, да?
Вот допустим, это произошло всегда, тогда у нас получается вот что-то похожее.
Да?
Ну, правда, по-хорошему, это не совсем так, потому что надо запускать мат ожидания в t от h минус два, да?
Ну, вот.
И еще смотреть, домножать на мат ожидания того, сколько бы там вершин запустил.
Ну, вот.
Но, по-хорошему говоря...
Да, ну, здесь, ну, вот.
Ну, здесь, там, просто, там, более формально надо будет просто запускать там, типа...
Докажем, что мат ожидания, допустим, t от вершины, где, вот, если, там, с высотой h, не превосходит чего-то там.
И это мы пытаемся доказать по индукции.
Вот.
И по индукции мы здесь, очевидно, будем доказывать что.
Что у нас, Даня, это полтора слипнет?
Ну, почему?
Ну, скажем, это 3 в степени h пополам.
Ну, в нашем случае h равно 2n, поэтому, в итоге, получится мат ожидания 3 в степени n.
Неплохо, да?
Детевированный, только 4 в степени n и никуда, а мат ожидания, вон, тебе будет летать за 3 в степени n.
Вот.
Доказательство будет такое, что, допустим, мы находимся на высоте h, да?
Вот вершина находится на высоте h, и здесь ходит какой-то урок, да?
Тогда мы говорим, что вот у нас есть у него дети, а есть и внуки.
А, ну, ладно, база индукции, что для h равно 0 это очевидно, верно?
Понятно почему, да?
Ну да. Ну, типа того, да.
Мы доказываем вот нижнее левое утверждение?
Ну, типа.
Что?
Это, ну, допустим, у нас есть вершина v, и я очень хочу знать ее результат.
То есть это вот ожидание числа действий, чтобы...
Ну да.
Да, то есть, заметим, что я почему так пишу, потому что, по-хорошему, t от v, ну, реальная t от v, даже и вероятности, это все равно очень сильно зависит от того, что это за вершина v и что у нее там за листья, да?
Ну, от конфигурации-то может очень многое зависеть.
Потому что там разные запросы.
Ну, а теперь если мы аккуратно доказываем, что для любой вершины v высоты h в от ожидания не обстоит 3 в степени h по полам, допустим, да?
А, кстати, интересно, вот для высоты 1 это вообще верно?
Ну, хотя да, мы...
А нет, это не верно, кстати.
Да, может так случиться, что придется делать 2, и это неправда.
Да, это неправда.
Ладно, поэтому придется делать аккуратно.
Давайте так, если высота у вершины ровно четная, вот 2h, то тогда я утверждаю, что, действительно, придется делать 10 не более чем 3 в степени h.
А если они нечетные?
А нечетные мы рассматривать не будем.
Хорошо, тогда 1, когда встанет 1, будет не на спине.
Да, то есть теперь тогда проделаем то же самое утверждение, только переходим в этой вершине точно первым человеком.
Значит, поехали.
Это база?
Нет, теперь база 0, а теперь у нас при переходе h заведомо больше либо равно 2.
Ну ладно, не h, а там 2h.
Итак, ну давайте посмотрим.
Теперь думаем, вот у нас вершины там v1, v2, v3, v4, сколько мы будем запускаться.
Так, ну есть там вот вершины o1 и o2.
Ну давайте рассмотрим 3 случая.
Случай номер 1.
Обе вершины эти вернут единицу.
Давайте назовем эти вершины, допустим, o1 и o2.
Тогда, а вот эта вершина v, тогда от ожидания t от v, очевидно, ну даже можно сказать, количество действий.
Ну вот, равно там o от единицы, плюс, значит, там 1,2 на, значит, там, от ожидания t от u1, плюс от ожидания t от u2.
Ну потому что в средстве 1,2 мы запустимся от вершины u1, а в средстве 1,2 запустимся от вершины u2.
И на этом мы закончим, потому что нам каждый из этих вершин скажет, что мы победили, ура.
Поэтому можно вот аккуратно проверить.
Ну, а что у нас там у нас есть?
У нас есть.
А у нас есть?
И мы будем запустить от вершины o1.
победили ура поэтому можно вот аккуратно можно вот так ну как вы не
но здесь это как бы случайная величина но здесь вот т от у1 это случайно ну давайте так
давайте так т от в равно от единицы плюс значит т от у1 если там монетка с вероятностью 1
вторая и т от у2 с вероятностью 1 вторая у нас так алкогольный успеваем да ну то есть да то
есть как мы видим т от в это случайная величина ну каждая т теперь это случайная величина правда
но на самом деле теперь утверждаем то есть это т от у1 тогда мне почему-то почему-то у меня
интуитивно жуткое ощущение что в этом случае мотожидание т от в оно равно от единицы плюс
одна вторая из вот этих вот мотожиданий
я не очень согласен что т от в будет когда у нас случайная величина допустим мы дофиксировали
дерево у нас тогда т от в должно иметь уже комплектное значение
не не это случайная величина потому что напоминаю у нас как бы да дерево не случайно
но запросы которые мы делаем они случайно потому что как решить мы запускаемся от у1 или у2 мы это
решаем монеты и это вот идея понятна
а что мы делаем мы смотрим 4 случая внутри 3 случая 1 2 и 2 1 это одно и то же что мы скажем
ну короче наша цель доказать что так сказать в каком для нас там
может у нас пространство такое что вот это дерево не наши вероятности пространство работает нет
наши вероятности пространство заключается в том что каждый раз когда мы приходим в какую-то
вершину пытаемся выкинуть результат первым делом мы кидаем монетку с вероятностью 1 2
ну и в зависимости от результаты идем вправо или влево
да оно невероятно конечно то есть мы говорим что для корно вот мы считаем
ожидание для любого дерева да но оно невероятно дерево невероятно алгоритм то вероятность ну как
кусок кусоке тоже перестановка который вы сортируете она невероятность надо нас заранее но
на то ждание есть потому что вероятность базируется не на том что сгенерировалось а то какие
рандомы вы выкидываете ну то есть у вас там на каждом шаге запросы там запроса там на какое-то
рандомное число от одного да там сколько-то я согласен с этим если там один один просто я не
понимаю как вы будете вынужден потом 4 случая 3 случая ну просто наша цель же доказать что как
условно т но давайте так ну я бы сказал так ну цель наша такая доказать что пусть у меня
т от аж это равно максимум нет ну хотя нет неважно то есть я хочу доказать что для любой вершины
высоты 2 аж в от ожидания ее не происходит 3 в степени аж сейчас я просто докажу что примерно
что как бы ответ это не более чем 3 умножить нам от ожидания от вершины вершины высоты и вот
то есть я как бы среди нот то есть там вот такая идея так вот так вы согласны с
тем что если случайная личина устроена так том от ожидания устроена так когда подобное
могло бы быть не верно тогда не будет верна вот вот это рекуррента да согласно вот так
скажем да есть люди вот так так должно быть так вот вот теперь просто идея такая начинал как же
теперь рассмотрим е театру 1 е театру 2 значит теперь если тут единички давайте вот рассмотрим
тогда утверждает следующее что у нас ну что у нас какие у нас случаи тут могут быть тут
могут побеждать либо 12 либо 21 либо 11 но не может быть что тут 22 потому что вы знаете что
вот но теперь давайте рассмотрим т от у 1 ну значит если тут у 1 действительно оказалось что там
побеждают один один то тогда т от у 1 равно соответственно что там у от единицы плюс т от
ну а какой-то там вот из этих вершин да но тоже на самом деле просто будет значит
т от в 1 или вот допустим эти вершины называются в этом в 1 и в 2 да но тоже это равно с вероятностью
1 2 и тут с вероятностью 1 2 но тогда мот ожидания т от у 1 равно от единицы плюс 1 2 значит от
этих вот мот ожиданий плюс 2 и это и теперь уже под предположение индукция это не превосходит
там у от единицы плюс плюс соответственно 1 вторая на там 3 в степени 2 аж там получается аж
минус 1 плюс там 3 в степени аж минус 1 то есть это равно то есть это равно от единицы плюс
3 в степени аж минус 1 понятно да да сейчас у нас видимо это у от единицы нам видимо попортит
наверное жизнь ну похоже на правду я боюсь у нас сейчас тут вот просто 3 в степенях сейчас не
сойдет но вот ну в общем ладно основная идея сойдется а довести мы ее доведем значит смотрите
но это простой случай когда 1 1 а если вот 1 2 допустим у нас произошел допустим тут побеждает
первый тут побеждает второй да тогда у нас получается т от у 1 но если время работы равно
о от у 1 у от единицы опять плюс что плюс значит с вероятностью 1 вторая т от в 1
это вот средства до вторая есть вероятность 1 вторая т от в 1 плюс т от в 2 одна вторая
тоже с вероятностью 1 вторая то есть получается мат ожидания в данном случае будет равно от единицы
плюс по-любому т но т от в 1 так или иначе будем вычислять плюс с вероятностью 1 вторая будем
вычислять т от в 2 и это меньше либо равно чем от единицы плюс полтора полтора умножить на 3
степени аж понятно да вот то есть получается из вот этого вот совокупности мы можем теперь
вывести что независимо от того какой тут тест будет т от у 1 да и у 2 тоже не превосходит у от
единицы на там плюс полтора на 3 в степени аж минус один но отсюда следует что вот теперь
вот отсюда и отсюда теперь следует что е т от в там не превосходят от единицы плюс там еще 2 от
единицы плюс 3 умножить на 3 аж минус 1 равно от единицы плюс 3 степени аж
потому что здесь полтора и здесь полтора то есть да здесь не сошлось конечно в том плане что
доказывали что не происходит 3 степени аж надо тут объединить что там нет тут смотрите
а конкретно у нас конкретно сейчас да хорошо ладно здесь все хорошо есть более продвинутые случаи
как всегда у нас же есть продвинутые случаи когда одна единица другая двойка а что будет в
этом случае так давайте давайте я этот случай вот тут рассмотреть один аж вначале что первый
победил да но тогда ситуация не может быть что у единицы оба сына тоже может если первый
побеждаем то он должен быть в вершину такой чтобы не побежать второй здесь мы здесь мы не
работаем в этой лодке здесь первый второй это как бы да это название да мы это да не ну как сказать
я не знаю на всякий конвейс на самом деле там игроки могут называться красный и синий и там
знаете ну знаете это там есть такая вот экзотика к сожалению видимо так и не попадет пока в наш
курс это красный синий хоккей так так и будет мы там изучим вот это то что мы должны изучить мы
изучим видимо за сегодня и в следующий раз и на этом все так что так что так что соответственно
декабре у вас будет будет время вздохнуть ну или там я не знаю ну или там я не знаю вот значит
смотреть теперь предположим что то есть тогда вот допустим у нас в вершине у 1 выиграет первая
иголка в вершине 2 второй тогда ты от нашей вершины вот этой вот флэ равно время время работы
получается от единицы и теперь плюс значит с вероятностью но опять получается ну да либо
т 1 либо т 1 т 2 т 2 вот понятно да вот ну а теперь расписываем что у нас тут будет
но заметил что каждый из этих т то есть т от у 1 и у 2 и у 2 можно пока просто не
заморачиваться и сказать что не превосходит 2 так сказать т от ну можно сказать просто не
превосходит от там просто от единицы да не превосходит о плюс т от в 1 плюс т от в 2 ну типа
в худшем случае мы как бы запустимся от то и от той да и в плане мот ожидания тогда е т от у
1 да и у 2 тоже не будет превосходить от единицы плюс 2 на 3 в степени аж не высадить вот понятно
да тогда получается что е т от в не превосходит от единицы плюс 1 вторая от ну либо мы запустимся
только одно и получим 2 на 3 степень аж минус 1 либо 4 но средняя арифметическая опять
3 равно от единицы плюс 3 степень аж ну небольшая мелочь конечно тут заключается в том что мы
не доказали что 3 т от вы не происходит 3 степени аж потому что у нас у 3 степень аж плюс
какая-то мелочь но тут как бы мы полный но опять в прошлом семействе у нас там были технологии как
тут надо подогнать чтобы было адекватно ну помните там эти домашние задания с документами
а то и никто из вас а нет но на самом деле ладно здесь самое правильное знаете что на самом деле
сказать что т от в это не количество рекурсивных запусков а сколько листов мы узнали если мы будем
вычитать для удобства количество листов которые мы узнали то тогда е т от собственно от нуля
развод тупо один типа если я сприт из новолиста значит тупо надо его узнать и кто выпил тут и
прав и дальше тогда никаких вот этих вот единицы очевидно не будет и тогда получается что ну скажете
гарантирую утверждение количество листов которые мы спросили их 3 степени аж но можно видимо тут
скорее всего аккуратненько подогнать утверждение ну знаете тут как всегда тут написать цена 3 в
степени аж там минус бла бла бла вот это все да там минус какая мелочь и соответственно и это
мелочи вот единицы будет жирать вот то есть в чем собственно но вот это уже технические мелочи нет
смысла мне саморачиваться но просто вот оцените красоту то есть еще раз что у нас в итоге получилось
что если у нас если как бы это детерминировано эту задачу быстрее 4 степени нельзя решить в
принципе в лучшем случае придется проверять все если вы используете вероятности там пользуясь
тем что там никто не подгоняет то неожиданно оказывается что можно значительно ускорить
прям до трех степени аж в среде то есть там то есть это значительный прирост то есть в общем
там могу сразу сказать что то есть например вот обычно в каких-то вот и там теории игр обычно
если там надо перебирать какие-то деревья вариантов то используется альфа бета отсечения да ну там если
вкратце сказать то альфа бета отсечение базируется на том что там что мы там пытаемся оценивать когда
мы приходим в дерши мы уже говорим что как бы нас интересует как бы только в предположении что если
что результат игры он там не обязательно 0 и 1 а вот результат игры но то есть знаете бывают игры когда
вот как бы допустим дерши нас могло быть написано допустим не кто выигрывает а сколько выигрывает
если написано 1 игру выигрывает 5 или 2 игру выигрывает там 2 игру выигрывает 9 и там надо и там
как бы каждый игрок ходит так чтобы мат ожидания его выпишу было как можно больше да ладно не
ну ладно не мат ожидания просто он ходит так чтобы выпишу был как можно больше вот ну тоже можно
эту динамику написать да ну вот ну и альфа бета отсечение нам собственно там говорить что мы запустились
от вершины то есть мы даже общем уже знаем что ответ где-то там вот альфы да бета допустим например и
поэтому если запускаемся от этой вершины ну узнаем там от нее что ответ там допустим заведомо меньше
альфы то можно в общем там ничего не вычислять и вернуть альфа но там вот какие ну там какие-то
даже там если ответ будет это типа запуститься отсюда запуститься отсюда и выбрать максимум
Если мы запустились и получили ответ альфы, если мы от этой вершины уже точно знаем, что там ответ больше альфы не будет, то можно от этой вершины не спускаться.
То есть вот примерно в этом направлении, если рандом кидать, то получаете жесткие отсечения, которые заставляют хорошо работать.
То есть очень красивая на самом деле вещь.
То есть в общем-то на самом деле утверждается, что шахматные движки, в общем-то все умные шахматные движки, в общем-то все на этом и базируются.
Ну в общем так, как работают все шахматы, все эти сфокусшие и так далее. В основном они работают так.
Ну во-первых, да, есть какое-то понятие шахма там был, оценка шахматной позиции.
То есть просто посмотрим на доску, посмотрим на фигуры и посчитаем какой-то функционал.
Вот, ну базовая версия. То есть это была бы базовая версия. Теперь продвинутая версия.
А теперь давайте запустим на глубину 10. Это означает, что мы переберем все варианты там условно ходов, там 10 ходов белых, 10 ходов черных и посмотрим, какие позиции будут получаться там.
И мы можем прикинуть, какой функционал мы можем получить исходя из этого.
Ну вот, и в результате новая версия. Теперь у нас говорит, что хорошие позиции, это типа насколько хороший функционал мы можем получить через 10 ходов.
Ну или через 15, или через 20. То есть дальше в общем-то, то есть в основном движки работают ровно так.
Отличие будет заключаться в том, насколько хорошие там, собственно, это оценки позиции там.
То есть дальше будет основная магия. Ну понятно, да. Есть, конечно, альфа-ко, которая работает не так, естественно.
Но там, как работают нейросетки, это уже отдельная песня.
Ну там это может действительно звучит магически, как это была история.
Мы запустили на 4 минуты нейросетку, которая изначально загнали только правила шахм.
Да, правда эти 4 минуты, видимо, работала просто все мультисти Гугла по всему миру, правда.
Поэтому там условно как-то эксперимент нельзя повторить на своем копе.
Но после этого, на самом деле там самый современный стоприж просто стал, называется, нежно проигрывать.
Человеческий антистримут, она вроде играла сама с собой.
Да, именно так, да. Она как-то там играла сама с собой, записывала варианты, да.
Да, это красиво. Так, крестики нолики. Как играть в крестики нолики?
Так, давайте пойдем вот так. Так, а он пойдет вот так. А я пойду вот так.
Так, а он пойдет вот так. А он пойдет вот так.
Так, нолик теперь думает, так, ладно, значит в этой позиции я вот так ходить не буду.
Так, а как же мне ходить? О, а может я пойду вот так?
Так, нет, я так тоже ходить не буду.
Так, ну и так далее. О, может пойти вот так? Ой, что-то он не выигрывает.
Нет, то есть самое смешное, что да, вот начинаем с этого, а потом, как бы, да,
правильные нейросетки с правильными дифференциалами, в общем, начинают вот работать.
Ну, в общем, ладно, это уже как бы, то есть вам еще предстоит это изучать.
Я думаю, в курсе машинного обучения у вас все это будет.

Не, начнём мы с рандомизированного миноразреза.
Миноразреза, в смысле, в неориентированном графе или с фиксированным стоком?
Не, просто в неориентированном графе.
Да, в неориентированном графе.
В общем, для разминочки мы ещё скажем, что граф у нас невзвешенный.
Я хочу разбить вершины на две не пустые доли, так, чтобы количество ребер пересекающих развес было как можно меньше.
Так, ну я так понимаю, этот маркер мы выкидываем, ребра не ориентированы.
Да, мы решали этот алгоритм Штормвагдера, там Штормвагдер умел это решать либо за куб, либо ММ плюс Н квадрат в ОКН, причём даже когда веса могли быть.
У нас сейчас до 6 веса.
Пока нет, хотя что-то у меня возникло ощущение, что решение, которое у нас сейчас будет, на самом деле от весов сильно ненавидно.
Ну, там ладно, в конспекте указывается ещё какая-то там мистическая симптотика НМ лог Н квадрат поделить на М.
Это лучшая симптотика этой задачи децерминированной.
На момент написания конспекта.
А там мало, что-то могло поменяться.
Скажем, на финале, например, там была лекция современной алгоритмы поиска максимального потока.
Ну, вот и реально там какой-то апгрейд, буквально новая статья 2020 года в архиве лежит, вот там приехала авторизма университета Ватерло.
А сейчас там же лучше вроде НМ?
Ну, децерминированной, да.
Но, правда, там лучше они не придумали в этом плане, но зато вот вероятность, там огромное количество приближений, там чёрная макия какая-то и так далее.
Ну, то есть, я не знаю.
В этом смысле, лекция была полтора часа, но как бы там идеи, которые там были, они были в очень-очень-очень-очень таком зачадочном виде.
Потому что, видимо, чтобы это изучать, это, видимо, нам это в следующем семестре будет отдельный курс.
Как говорится, вероятно, там это называется университет Ватерло, и вероятностно горит на поисках потока.
Просто спецкурс можно обалбасивать.
Важно вопрос, а чем эта симптотика снизу лучше стать, та, которая сверху?
Честно? Я не понял.
Потому что, вот если m это, понятно, z от m то одинаково, если m от z от m квадрата то одинаково, а если между ними, то вроде хуже.
Не вот, в общем, я тоже не понял, поэтому не заморачивайся.
Значит, попробуем решать задачу быстрее.
Значит, идея такая, пусть у нас в графе n вершин, m, допустим, значит, m ребер, а ответ равен, ну, допустим, k.
Ну, мы не знаем ответа, но предположим, что он равен k, да?
Опозначим его.
Хорошо, смотрите.
Значит, алгоритм каркера.
Предлагает неожиданную штуку.
Возьмем рандомное ребро.
И скажем, что, о, я утверждаю, что это ребро в ответе не лежит.
Мы в это свято верим и объявляем эти две вершины одной вершины, так и сжимаем.
Повторяем операцию.
Потом еще повторяем.
Ну и так далее.
Остается в конце две вершины, мы им говорим, что, вот, разгресс между ними, вот, сколько там ребр между ними есть, ну, кратный ребро есть, то это и есть ответ.
Вот такая заява.
Мы просто набираем любое ребро и говорим, что оно у нас не состоит в игре.
Что, короче, сжимаем это ребро в вершину.
Потому что мы заявляем, что это ребро в разгресс не пересекается.
А когда мы стянули и выбираем наоборот, что на ребро, мы выбираем из изначальных или мы кратные ребра в одно?
Нет, из изначальных.
Кратные ребра никогда не превращаются в одно.
Хорошо.
Это для нас важно, потому что...
Да, потому что это влияет на вероятность того, что...
Ну вот, так что вот такая заява.
А у нас где-то есть стопы и стопы?
Нет, это не вариантирован гораздо.
Это просто обычный неориентированный граф.
Мы хотим разбить на две доли.
Единственное ограничение, что они должны быть не пустые.
Если было бы столько и столько, это значило бы наносить на поиску максимального потока.
Ну да.
А тут мы будем искать разрез.
Вот будем искать разрез так.
За какую септутику мы можем реализовать такой алгоритм?
What yet?
Ну, в смысле...
На альфа.
Нам СНМ нужен.
А, СНМ.
Ну, не обязательно.
Ну что-нибудь такое.
Мы берем совершенно ребро и хотим проверить,
что оно уже объединяет две вершинки, которые еще разные.
Ну да.
Но на самом деле я утверждаю, что каждую фазу можно выполнить за ОАТМ.
Ну, просто за ОАТМ можно выполнить.
Каждую фазу, в смысле?
Каждое сжатие и стягивание ребра можно за ОАТМ выполнить.
За ОАТМ слить?
Ну, потому что в тупую.
Ну, во-первых, для каждой вершины у нас есть стипинг.
В соответствии с этим мы сгенерим родномные ребра.
Вот.
И после этого, значит, вот эти две вершины
мы хотим объединить.
Как мы это сделаем?
Ну, вот.
Ну, вот, берем, да, списки смежности объединяем.
Ну, при этом, да, особенно если там
для каждого ребра хранить напарника в другом списке,
то как бы тогда, значит, вы там переписываете
этого напарника.
Можно еще сказать, как мы выбираем случайное ребро в итоге?
Ну, как бы.
У каждой вершины есть, значит, список ребр, да?
И стипинг ребр, да?
То есть, допустим, 5, 7, там, 2, 4, 6, да?
Это степень вершины.
Да, в сумме у тебя там степень получается там 5,5,7.
И мы объединяем случайное число от одного до суммарной
количества ребр.
И дальше просто пробегаемся по списку
и понимаем, где там вот эта
ребро номер вот сгенеренная лежит.
А как же Майка?
Ну, да, объединить список смежности.
Ну, да, объединить список смежности.
А там же будет фейк этой вершины?
Ну, мы обвиняем А и Б.
Ну, значит, потом, значит...
А и Б в этих списках нет?
Ну, а и Б... Ну, то есть, единственное, надо
прибежаться по всем ребрам, у которых
конечная вершина равна Б, и
всех переписать, что там А.
Так это уже не за...
Почему? Ну, вот почему?
Во-первых, если вы для каждого ребра храните напарника,
то это делается.
Или А, нет. Или есть еще...
Ну, там, разве 4 есть?
Можно, например, знаете как, храните список ребр,
у нас полный список ребр, да?
А в списках смежности храните ID-шники этих ребр.
Логично.
Так что... Они нельзя быстрее?
Или неважно?
Ну, может и можно.
Ну, в принципе, нет.
Ну, давайте думать.
В принципе, наверное...
Ну, как вам сказали, вообще нужно следить,
какие ребра вообще у вас вылетели,
а какие нет?
А если мы будем брать случайно,
если он не подходит еще раз брать?
Или это слишком болтно, наверное?
А, ну нормально, в принципе, наверное.
Ну ладно, давайте в тупую скажем,
что это работает за квадрат.
Хотя, в принципе, да.
Можно, в принципе...
Если делать это, просто вершину объединять
с помощью СНМа
и кидать рандом по принципу,
берем рандомное ребро,
если это ребро уже снимется в одну вершину,
то выкидываем,
то там получается что-то типа M на...
Какая-то такая гадость получается.
Чего?
Могут быть кратные ребра?
Конечно.
Это важно.
А вот в конце, когда мы все задали,
нам осталось две компоненты.
Да, и куча ребре между ними.
И ответы в количестве этих ребре.
А почему в ответной СНК нету плюс М?
Чего?
Потому что там, где M квадрат,
нету плюс М.
А, ладно, мы, видимо, почему-то поверили,
что там нет кратных ребр.
Хотя зря мы там поверили,
лучше не верить M квадрата плюс М.
Вопрос. Мы выбираем ребро,
а проверяем на что?
На то, что у нас вершины у ребра разные?
Нет.
Мы выбираем рандомное ребро
и верим на то, что вершины лежат в одной доле.
Ну, в смысле объединяем эти вершины.
И рандомное выбираем
в первый раз.
И в первый раз рандомное, во второй,
и в третий и так далее.
Во второй раз мы выбираем не из нового, а из нового.
Нет, в смысле из нового.
В смысле выбираем совершенно ребро такое,
что две вершины его в разных компонентах.
Можно и так.
Ну вот, новая СНК
работает алгоритм даже неплохо.
Ну, по сравнению с теми кубами,
которые у нас были, это неплохо.
Ну, можно сказать, маленькая проблема.
Почему это работает?
А это не работает.
Алгоритм может работать неправильно.
Мы же постоянно
стягиваем, стягиваем.
Две вершины.
Мы считаем, что это есть разрез.
Да.
Может быть не минимальный,
а может и минимальный.
А теперь давайте посчитаем.
А с какой вероятностью это может быть
ошибкой?
Ну,
можно верить, что
допустим, минимальный разрез
единственный.
Тогда его вероятность, получается,
достаточно маленькая.
Но это так кажется.
А на самом деле, смотрите такая книжка.
На самом деле, если разрез,
ответ маленький с другой стороны,
то вероятность того, что когда вы тыкнете
роддомное ребро, то оно в этот разрез не попадает,
то оно, в принципе, достаточно большая.
Ну, на самом деле, давайте от слов
перейдем к конкретным оценкам.
На самом деле, на очередном шаге,
если у вас вероятность вот эта вот k,
то, то есть если у вас ответ равен k,
то, как бы,
скажем так,
вероятность того, что вы попадете
в разрез,
то есть вероятность, там,
вероятность ошибки,
у нас какая?
k поделить на m, правда?
На первом шаге.
Ну, давайте начнем с первого шага.
Вероятность того, что мы возьмем ребро,
которое должно лежать,
которое на самом деле в минимальном разрезе
не лежит.
Это вероятность ошибки будет вот ровно
k поделить на m.
Сейчас, а у кого m?
Это мое.
Так, ладно, сейчас я уберу дискорд.
А у нас один ответ или нет?
Может, не важно.
Ну, давайте так, зафиксируем какой-то ответ.
Ну, если несколько ответов,
то вероятность ошибки еще меньше.
Ну, то есть, короче, вероятность ошибки
не превосходится на первом шаге,
а что вот на первом шаге мы сразу по face
не превосходит k поделить на m.
Вот.
Заметим маленькую приятную вещь.
Заметим, что
степень каждой вершины,
то есть, для любой вершины v,
верно, что степень каждой вершины
больше либо равна, чем k.
Факт.
Ну, просто потому, что
вершина v и все остальное
это тоже разрез.
Очень минимально.
Внимаю сцена.
Внимаю сцена.
Это в итоговом игре?
Ну, исходно.
Да и в итоговом тоже.
Сейчас вершина хотя бы k.
А, согласна.
Чего?
Мы же сказали, что k предполагаемое.
Нет, k это реальный ответ.
Мы его не знаем, но как бы
обозначим его за k.
Степень решен каждая, значит k.
Отсюда я делаю
интересный выбор.
2m больше либо равно, чем k на m.
Так, сейчас давайте
я могу повисеть еще 30 секунд.
Чтобы понять, откуда я
взял вот это утверждение.
Поняли откуда я взял?
Надеюсь, что да.
Почти.
Почти это как?
Да, понял.
Ну да, давайте.
Два раза посчитали.
Если я просуммирую
степени всех вершин, я получу
ровно 2.
Но сумма степеней всех вершин
как минимум k.
Отсюда
следует, чтобы
вероятность этой ошибки
не превосходим
2 поделить на f.
То есть я на первом шаге
оказывается ошибусь
не с такой большой вероятностью,
не более чем 2 поделить на f.
Ну чем больше вершин, тем эта вероятность меньше.
Неплохо, да?
А вероятность того, что я
не ошибусь на первом шаге
вероятность как минимум
n-2 на n.
Неплохо, да?
А теперь это нам дает очень приятную вещь.
А с какой вероятностью
я теперь не ошибусь вообще?
И чтобы я вообще никогда не ошибся,
то есть я должен не ошибиться
на первом шаге.
Это придет с вероятностью хотя бы
n-2 на n.
На втором шаге
на одну вершину меньше,
поэтому там я не ошибусь
с вероятностью хотя бы
n-3 поделить на n-1.
Потом n-4 на n-2
и так далее.
Ну и тут в конце будет
2,1,4.
Как вы уже догадываетесь, тут будет
куча шлеп-шлеп-шлепов.
И получится, что это равно 2
поделить на n на n-1.
Это может быть даже
больше либо равно, чем 2
поделить на n-4.
То есть получается,
смотрите,
то есть я
если я запущу этот алгоритм,
то я получаю правильный ответ
с вероятностью
не менее чем вот столько.
Ну тогда что делать?
То есть вероятность конечно
не самая приятная.
Но теперь идея такая, а давайте
запустим этот алгоритм
n-2 пополам раз.
Из всех вариантов выберем
наименьший.
А хватит нам ровно n-2 пополам раз?
Что?
Ну давайте прикинем.
Вот смотрите,
мы можем брать n-2 пополам раз
и выбрать из всех
ответов наименьший.
Тогда с какой вероятностью он будет
все еще не правильный?
Если мы ошибались все разы...
То есть вероятность ошибки
у нас...
То есть если мы делаем
алгоритм на c запусков,
то вероятность ошибки
получается не более чем
1-n квадрат пополам
в степени c.
Раз вероятность того, что у нас будет
правильный ответ не менее чем 100,
то это вероятность...
Да, да, наоборот.
Жестко.
Да, уже вероятность.
Вероятность, что неправильный ответ будет.
А это будет вероятность того,
что ответ будет неправильный,
не более чем 100.
Вот.
Теперь если мы делаем c
равно n квадрат пополам,
то запусков получается 1-
вот я так напишу,
1 девять на n квадрат пополам
степени n квадрат пополам.
Это что-то типа,
или даже меньше, либо равно,
1 делить на e.
Ну и приблизительно там столько.
Тут как бы я тебе к слову не могу сказать,
с какой стороны... То есть эта штука,
в принципе, ящик бесконечности,
стремится к 1 делить на e, да?
Это вроде из ботанализа должны знать.
Вот.
Вероятно тем, что оно какая-то константа
между двойкой и тройкой.
То есть, смотрите,
вы получили... То есть мы получили алгоритм,
который работает с вероятностью
ошибки, не превосходящим
какой-то сильно отделенной от единицы
константой. Да, не нравится,
что такое всё выполняется. Отлично.
Хорошо.
Ну раз оно выполняется, ну тогда как сделать
вероятность ошибки поменьше?
Ну запустите... Константу в раз больше.
Да, ну очевидно, если я тут сделаю
10 раз, то у меня вероятность ошибки будет
не более чем 1 делить на e в десятой.
Ну или там, хотите,
20 будет в двадцатой, в общем, какая вам
вероятность нужна, в общем, сколько раз и запускаете.
А симпточка
получается o от вот
n квадрата долгая, то есть получается
итоговая симпточка o от n в четвёртую.
Ну или там o от
сколько там, n квадрата долгая,
вот эта, если хотите.
Пока что всё ещё
устраивается.
Ну кажется
да, может показаться, что многовато,
но зато, возможно, это попроще,
чем что-то в артеры писать.
Ну потому что, видите, алгоритм
очень простой.
Просто нужно запомнить его.
Но это как-то тут
прям совсем тупую.
Она тупо не чекать что-то делая, и мы бабах уже
в четвёртой степени.
Ну ничего.
Как говорится, у нас
есть более крутой алгоритм.
На самом деле.
На самом деле, да,
этот алгоритм можно проапгрейдить.
Потому что, как бы, за счёт чего
тут он может долго работать?
За счёт того, что, как вы получаете,
если мы, то есть мы там делаем
n минус один шаг.
Если где-то на пятом шаге мы пофейлились,
то по идее дальше можно и не делать.
Ну, не угадали с ребром.
Да, мы не узнаем, но тем не менее.
Но просто оптимизация может быть такая.
Вот если мы сделаем там первые 10 шагов,
то может наоборот.
Там с какой-то большой вероятностью на первых 10 шагах
мы не пофейлились.
Ну такая приличная вероятность будет.
И тогда, пока она приличная,
возникает идея такая.
А давайте поверим, что мы там не ошиблись.
Ну или с мелкой вероятностью ошиблись.
Идея такая.
Вторую итерацию мы будем делать, начиная
не с начала, а вот с этого места.
Не запустимся в том месте, где мы...
Ну, типа да.
Развилочка такая будет.
Попробуем заново?
Давай ещё три раза и с начала попробуем?
Нет.
Ну там, конечно, будет немножко по-другому это работать.
Работать будет примерно так.
На самом деле, идея будет такая.
Значит, мы для графа размера n сделаем так.
t присвоим...
n поделить на поле низ двух.
Ладно.
Вот.
Вот.
Вот.
Когнячка вошла в час.
Сейчас чего сейчас?
Это корень из анкаверта пополам.
Всё?
Ну, можно корень из анкаверта пополам.
Можно...
Ну, на самом деле сейчас мы тут немножко по...
Так, поподгоняем.
Ну, пойдёт.
Ладно.
Значит, смотрите.
Что такое?
Что происходит?
Ничего.
Ну, вот. Значит, смотрите.
Просто идея такая.
Делать будем следующее.
Теперь новая версия алгоритма.
Теперь вас приветствует алгоритм Кардера Штайна.
Такая обправреженная версия.
Значит, она нам говорит,
мы делаем
операции
до тех пор,
пока вот у нас количество вершин
не превратилось в t.
Какова вера...
Давайте прикинем.
Какова вера это из того,
что мы ошибёмся?
Ошибёмся когда?
Ну, вот пока будем сжимать
n вершин в t.
То есть, найдётся хотя бы одно вибро
среди
1-2 шагов.
Которые мы неудачно сжали.
С какой вероятностью
мы не ошибёмся?
Сжимая t.
Да.
Какова вероятность получится?
Там можно
чуть-чуть одна-вторая.
У тебя сегодня
день второй.
Сейчас.
Ну, давайте так.
Если пишем
абсолютно то же самое.
Да, на вторая.
Давайте пишем.
Пишем вот это,
вот это,
только заканчиваем мы на этот раз
не на 2 и 1,
а где мы там заканчиваем?
На t и t-1.
Ну, и здесь мы пишем
1-2 и t-1.
Ну, после шлёп-шлёпа
получается, что равно t от m-1
поделить на n на m-1.
Так.
Это вероятность того, что мы не ошиблись.
Чего? Это вероятность того, что мы не ошиблись.
Ну, давайте t, допустим,
у меня будет это вот, вот округлённый
верк, давайте дляточим.
Вот.
Ну, это тогда
больше либо равно, чем n вот там.
Вот как-то так. Получается,
что сейчас получается?
n квадрат пополам
поделить на 2,
поделить на n на
n-1.
Ну, может
мы, конечно, пускать, что
t от Robby, t от t-1, t от t-1, t от t-1.
t от t-1, t от t-1, t от t-1.
t от t-1, t от t-1, t от t-1.
А, сейчас оно не больше.
Нет, нормально.
Я могу вот так выписать.
Вот. То есть, оказывается,
пока мы идём до этого t,
вероятность того,
что мы нигде
не ошибёмся, не менее, чем 1-2.
То есть,
вероятность того, что мы ошибёмся,
не более, чем 1.
Ну, до этого момента.
Понятно, да?
Чего смутило?
Эта вероятность того, что мы не ошибёмся.
Это вероятность того, что мы не ошибёмся.
Да... Следовательно, вероятность того,
что мы ошибёмся не более, чем 1.
Так. Ну что ж, ура!
потому что теперь дальше идея будет такая вот мы генирили вот n из n делали t
а теперь дальше от этого графа t запустим алгоритм рекурсивно два раза
берем вот этот граф и говорим так он же с вероятностью хотя бы там 1-2 хороший
так давайте два раза от него
небольшой ошибку делали
последний переход не верен
n пополам меньше чем от корн из 2
а не больше
да?
не могло быть
ну там не получается
то есть это почти 1-2 но не совсем точно
нет
мы написали что
мы написали что
хотя бы минус n делить пополам
но значит что n пополам хотя бы n делить на корне из 2
это просто неверно
когда мы корень из 2 заменили на 2
мы стали меньше высчитать
это можно оценить как 1-2 и минус
то что вот мы лишние
1-2 и минус что?
ну
1-2 и минус на n
что они так точно оценивают
что?
ну как
у нас разница получается
когда мы заменяем n делить корень из 2
наверное пополам
что мы добавили
мы добавили
n делить
ну n на 1 делить на коре из 2
минус 1-2
числитель
корица
и
короче
1-2 и минус на n
вот что они такое должны таким оценивать
ну не хотел
нам 1-2 принципиально
поэтому видимо придется поподгореть
давайте побольше сделаем
плюс 1 то будет норм
вот теперь хорошо
ты бегаешь точно хорошо
ну хорошо
если вас так смущает
пожалуйста вот так
вот
у меня маленький вопрос
почему если я положу t равно n
то у меня вероятность стоит 1
все ошибки я разнегу
чего какая 1
если ты ничего не сделаешь ты меня ошибешься
так вот
после того как мы сделали граб размеров t
мы запускаемся от этого графа
рекурсивно 2 раза
более того
можно в принципе
сказать что мы
каждый раз запускаемся рекурсивно 2 раза
можно даже наоборот сказать
два раза делаем так
доходим до t запускаемся рекурсивно
вот так
до t
но второй раз запускаемся
от того же графа который в этот момент появился
две копии графа
от одной запускаемся
в данном случае не так
наоборот
чтобы у нас есть граф
мы делаем две копии
и два раза его запускаемся
в каждом запуске делаем доходим до t
и запускаемся рекурсивно
мы там тоже дойдем до нового дж 3
и о них еще раз
получается такой деревце
глубины
лога
ладно 2 лога
ладно чуть больше
ну сколько раз это надо делить на коне
из двух чтобы получилось 2
очевидно это надо делать 2 логами
в мой раз
вот
ну теперь давайте думать
какова вероятность того
что мы ошибемся
где нибудь
или наоборот давайте
какова вероятность того что мы не ошибемся
пусть у меня например есть вот такая величина
которая говорит о том
что у меня будет
в некотором смысле верхней
оценкой на то
что при любом
графе на n вершинах
я никогда не ошибусь
то есть нижней оценкой
наоборот
можно еще раз
ну вот
но я хочу
чтобы
чтобы от 7 точек оценивать
чтобы хорошо формально было правильно
тут к сожалению начинается вещь
что если я могу произвести заклинания которые
похожи на правду
мы в самом начале задачи зафиксировали по условию
что у нас есть граф уже конкретный
на n вершинах все графы были
да но эта вероятность ошибки
на каждом из этих графов стала своя
на каких-то графах больше ошибались
на каких-то меньше
поэтому если мы хотим что-то обобщать
придется вот как-то оценивать
то есть я хочу доказать
что вероятность ошибки
допустим у меня p от n это
вероятность ошибки
на каком-то графе размера n
видимо верхняя оценка какая-то
допустим
вот я хочу принять чтобы доказать
чтобы мне что-то доказать но я знаю что
вероятность того что я ошибусь
на графе размера там
1 или 2 равно очевидно ноль
Ну, более того, там даже работают подхачки, в смысле, что давайте введем какую-нибудь константу С и говорим, что если количество вершин не превосходит С, то там решим задачу любым тупым образом за 1.
Ну да, понятно, если там 6 вершин у вас осталось, то там любым перебором решить задачу и все.
При этом мы обращаем нас теперь уже, может быть, не на mREB, а просто...
Главное, количество вершин.
Ну вот так вот. Давайте посмотрим, какова у нас вероятность ошибки.
Ну, вероятность ошибки у нас не превосходит 1 минус вероятность не ошибки.
С какой вероятностью мы не ошибемся?
Это слово одно, 1 минус вероятность.
Ну, то есть если говорим p от n это, пусть у меня, значит, p от n это вот оценка сверху, то есть мы говорим, что мы ошибемся с вероятностью не более, чем p от n, да?
Значит, вероятность того, что, значит, теперь вычитаем вероятность того, что мы не ошибемся в квадрате.
Аккуратно.
Потому что мы, потому что у нас два запуска, по сути.
Поэтому если мы хотим не ошибиться, мы должны...
То есть так, мы...
Нет, какая нотка?
Так, нет, сейчас все будет.
Ну вот, значит, смотрите, если я хочу...
Значит, если я хочу...
Вероятность ошибки это по-любому, 1 минус вероятность не ошибки, правда?
Вероятность, значит, пишем 1 минус вероятность не ошибки.
Значит, нот.
Вероятность не ошибки.
Вероятность нот.
Если p от n это максимальная вероятность ошибки в любом графе, то она равна 1 минус вероятность не ошибиться в любом графе.
Минимально отмечаем.
Чего-чего-чего?
Нет, чего-чего не так.
Все нормально.
Ну вот.
Чтобы ошибиться,
заметим, что нам надо ошибиться
вот так. И здесь, и здесь. Правда?
Да.
Ну вот.
То есть отсюда
Ну вот.
Ну вот.
Ну получается.
Но какая у нас вероятность ошибки
на каждом из запусков?
То есть надо что-то в квадрате.
В квадрате, заметим,
что у нас два варианта.
То есть на каждом запуске у нас
либо мы с вероятностью не более чем
одна-вторая ошибаемся здесь.
Да?
Либо мы
здесь не ошибаемся.
Ну вот.
Ну вот.
Либо, вот очень хочется написать,
что мы не ошибаемся с вероятностью
п от t.
Похоже, да.
Наградно ошибаемся.
Чего?
Наградно ошибаемся.
Ну это и есть вероятность того, что мы ошибаемся
в качестве размера t.
Правда, единственная проблема,
что вероятность того, что мы тут
не ошибемся, это все-таки
оценивать сверху не одна-вторая.
Потому что вы помните, что у нас нижняя
оценка то, что мы не ошибемся
с вероятностью не меньше, чем одна-вторая.
Да?
Поэтому по-хорошему
здесь, конечно, если тупо сверху
оценивать, то придется вот так писать.
Ну давайте попробуем.
Ну попробуем с этого начать.
Ну вероятность того, что мы
на каждом из этих запусков
на этом ошибемся,
она не просовывает
суммы вероятности
того,iguous мы ошибемся здесь,
и ошибемся здесь.
Вероятность того, что мы здесь ошибемся
ни более чем одна-вторая.
Вероятность того, что мы дальше ошибемся
ни более чем 1.
Отдельных квадратов?
Некоторых, потому что у нас два независимых запуска.
Чтобы ошибиться глобально,
мы должны ошибиться в этом запуске,
должны быть ошибиться в этом запуске.
Вы говорили о том, что он должен спуститься от N до T и оттуда уже сделать назад.
Нет, мы потом сказали, что мы как бы спуск делаем два раза.
То есть два раза делаем, то есть два, то есть делаем два раза следующее.
Берем этот исходный граф размера N, добиваем его, сжимаем его до размера T и спускаемся к нему рекурсивно.
То же самое тут же просто.
Нет, это не то же самое.
Тут подозревало, что от графа N мы спускаемся один раз, и теперь мы сказали, что мы спускаемся два раза.
То есть развилка уже здесь происходит.
А раньше мы говорили, что развилка происходит только здесь.
У нас был граф, мы спустились один раз от N до T.
Ну раньше было так, да, потом два раза спустились рекурсивно.
Теперь наоборот.
Мы два раза спустились независимо.
И для каждого результата спуска запустились рекурсивно.
Чем отличается спуститься и рекурсивно от того, чтобы просто продолжить?
Ну тем, что мы раньше от N спускались один раз, а теперь спускались два.
Нет, нет, нет, я о том, что продолжить спуск уже как новый.
То есть мы спустимся от N до T и идем дальше просто.
Нет, так теперь рекурсивно подразумевает, что мы из графа T будем спускаться два раза.
А, то есть мы теперь два раза и в самом T, и два раза...
Рекурсивно, да.
Рекурсивно это подразумевает.
Если мы при описании функции от N говорим, что N, то мы спускаемся два раза.
То есть когда мы спускаемся рекурсивно от N, мы тоже спускаемся два раза.
Все, хорошо.
То есть дерево по-любому есть.
Так, ну а теперь давайте думать.
Какую оценку мы теперь можем забабахать на P от N?
Вот хотя бы в таком виде.
Ну на самом деле не очень хорошо.
На самом деле заметим, что P от N...
То есть ну как бы из такой рекурренты заметим, что...
Ну заметим следующее, что из такой P от N у нас как бы любая оценка будет там...
Ну как бы больше одной четвертой не получится, правда?
Ну потому что, нет, здесь на самом деле должна стоять вероятность того, что мы не ошибемся до запуска, да?
Но на эту вероятность у нас оценка сверху только единица.
То есть мы типа сложили вероятность того, что...
Сейчас, одна вторая...
Это одна вторая на один плюс один на P от N.
То есть это, наверное, одна вторая на один плюс один на P от N.
Сейчас, одна вторая...
Это одна вторая на один плюс один на P от N.
Поэтому оценим вероятность ошибки максимально.
Ну да, мы посредиставим вероятность ошибки при спуске из N до T и ошибки после.
Ну да, так можно.
Ну вот, ну заметим, что в таком виде как бы заметим, что любая оценка, которая тут есть...
Как бы что-то мы тут по индукции не доказали, уж меньше чем одной четвертой не получится.
Максимум.
Может быть, удастся показать что-нибудь типа, что...
Что вот там...
Может быть, там это будет сходиться к какому-то решению уравнения.
Ну вот.
Нет, то действительно там. Давайте подумаем.
Есть какое-нибудь уравнение...
Ну вот.
Ну давайте так.
Есть какое-нибудь уравнение...
Вот X равно, допустим, одна вторая плюс X в квадрате.
Есть у этого уравнение решение?
Давайте посмотрим.
X в квадрате плюс X плюс одна четвертая равно X.
X в квадрате плюс одна четвертая равно нулю.
Решения нет.
Чек значен.
X равно плюс нулус...
Значит, он будет вообще стремиться уходить на бесконечную притерацию.
Ну что, X...
Ну типа, потому что оно будет все время возрастать.
Причем, если оно будет хотя бы...
Не, ну как сказать, давайте можем просто...
Причем оно будет хотя бы каждый раз, хотя бы у нас какую-то значительную штуку больше, чем в предыдущий раз.
Хотя бы нас учить.
Ну да.
Ну вообще, какая-то ерунда получается.
Не-не.
Не-не.
Ну да, то есть, по ходу, да, по ходу, она...
Да, очень похожа на то, что она...
Ну это возможно...
Не, хотя нет, будет ли она возрастать, это вопрос, конечно.
А, ну видимо, будет.
Потому что если она не будет возрастать, значит, она должна куда-то стремиться, и тогда должна быть неподвижная точка.
Так что все-таки есть подозрение, что как-то...
Это оцентруется.
Значит, надо оценивать как-то поточнее.
А как оценивать поточнее?
Ну, заметить можно было следующее.
Можно еще заметить, что давайте вместо 1 и 2 здесь поставим точную вероятность ошибки.
Точную вероятность того, что мы вот на этом месте ошибемся.
И скажем, что это ошибка, epsilon t.
Тогда можно было написать вот так.
1 минус epsilon t.
Понятно, да?
Ну, заметим теперь следующее.
Что, а так как при epsilon t у меня коэффициент как бы единица, а тут, как мы надеемся, не больше единицы должно быть, да?
Как мы, по крайней мере, надеемся.
То есть подозрение, что если мы будем epsilon увеличивать, значит, если мы epsilon увеличим, а это уменьшим,
то тогда вот эта штука от этого будет только увеличиваться.
Вот, да.
А вот что уменьшим?
Ну, если я увеличу epsilon, то вся вот эта штука уменьшится.
Ну, если я вот прибавлю сюда delta epsilon, тогда тут будет плюс delta epsilon,
а тут будет минус delta epsilon умножить на p, которое как бы и не превосходит единицы.
Значит, все уменьшатся тогда?
Да.
Поэтому это можно оценивать сверху, как если бы мы epsilon t сверху ченнете цилиндров.
Так, если бы мы epsilon t сверху ченнете цилиндров.
Ну, теперь заметьте, что так как epsilon t не превосходит 1 вторая,
то получается, теперь можно написать 1 вторая плюс все-таки 1 вторая p на t квадрате.
Вот такая мелкая аккуратность.
А то, что не больше на вторая, мы уже доказываем?
Нет, доказываем.
Ну, в смысле?
Ну, если раскрыть, там будет 1, ой, p от t плюс epsilon на 1 минус p от t.
Ну, давай так.
Если мы считаем, что p это не больше единицы, то при возрастании epsilon t увеличивается просто.
Ленина штука по epsilon t.
Ну, кстати, может раскрыть и погибнуть.
Ну, там, ну, черт, epsilon при epsilon t 1 минус p.
В p мы считаем не больше единицы, поэтому все.
Ну, сейчас я вот, ну, я распишу, распишу.
Вот.
Ну, то есть, если я добавлю к epsilon вот delta epsilon, то как бы получилось то же, что было раньше, плюс delta epsilon умножить на вот это.
А вот это не меньше нуля.
Да.
Ну, потому что мы верим, что p от t, наверное, не больше единицы.
Значит, при влечении t.
Ну, в этом влечении t.
А вот это не меньше нуля.
Ну, потому что мы верим, что p от t, наверное, не больше единицы.
Значит, при влечении t.
Ну, в давлении epsilon что-то.
Ну, значит, получается, при увеличении epsilon эта штука увеличивается.
И получается, если мы вместо epsilon заберем на более высокий, то как бы тут получится меньше либо равного.
Ну, тогда.
А мы знаем, что epsilon t не превосходит от да вторая.
Мы подгоняли специально.
Поэтому получается так.
Но при таком неравенстве p будет с емецкой веницей.
Чего?
При таком неравенстве p будет с емецкой веницей.
Ну, давайте посмотрим какое.
Да, наверное, посмотрим.
Ну, давайте посмотрим, какому.
Ну, давайте посмотрим.
Ну, вот.
То есть у нас получается.
Ну, да, то есть, конечно.
А, ну да, у этого уравнения теперь коль не один.
Да.
Так.
Итак, p от n.
Ну, с другой стороны, давайте введем действительно вероятность ошибки.
Пусть у меня epsilon not.
То есть, допустим, epsilon n равно 1 минус p от n.
Можем такое сказать?
1 epsilon это вероятность чего?
Ну, то есть, p от n была верхняя оценка на вероятность ошибки.
А epsilon n получается, это нам.
Ну, давайте лучше q от n введем.
Это нижняя оценка на вероятность того, что все в порядке.
Да.
Тогда мы можем написать следующее.
Мы можем написать следующее.
Что 1 минус q от n не превосходит 1 четвертая на.
Сколько там?
На 2 минус q от n.
В квадрате.
Согласны?
Да.
Так, ну или что?
То же самое.
1 минус q от n получается.
Меньше либо равно.
4 минус 4 q от n плюс q в квадрате от n.
Смотри, это не q от n, а q от t.
Ну, хорошо, q от n.
О, q от t, хорошо.
Так.
Можем еще куда-нибудь.
Ну да, то есть это равно у нас 1 минус q от t плюс q в квадрате t поделить на 4.
Но отсюда следует, что q от n, оно на самом деле получается больше либо равно, чем q от t минус q в квадрате t поделить на 4.
Что стремится?
К нулю.
Ну может, ну надо бы даже не стремиться к нулю, а каким образом.
Вот.
Вот действительно нет, просто хочется отсюда выяснить, что q от n, там это значит больше либо равно, чем q от t.
Ну, типа можно ценить как 3 четверти в степени 2 лог 2 н.
Чего 3?
3 четверти в степени 2 лог 2 н.
Ну, у нас это хотя бы, ну это равно q от t на 1 минус q от t 9 на 4.
То есть, q от t хотя бы q от t на 3 четверти.
Но это не очень хорошо.
Так, нет, ну почему? Это больше либо равно 3 четверти q от t, да?
Так хочется написать?
Так можно написать.
А может q от t ни больше ни меньше?
Нет, ну как вам сказать?
Нет, ну что не так?
Что не так?
Но заметим, что, то есть получается, ну из этого можно вывести, что q от n это на самом деле, то есть больше либо равно, чем 3 четверти в степени.
В общем, что-то о от логарифма n короче.
Писать о в степени, конечно, опасно вечно, но в целом, да.
Ну да.
Ну да, тут да, дальше.
Нет, ну почему?
Нет, ну главное тут с ней аккуратно бороться, но да, тут какое-то о.
Откуда мы взяли это?
Ну потому что...
Ну потому что...
А сколько берется?
Ну потому что...
А сколько берется?
Ну потому что...
Ну потому что...
А сколько берется?
t это примерно, это примерно n поделить на корень из двух.
Раз оно, ну вот, то есть получается, как бы, мы будем, получается, рекурсивно сходиться вот к t, n уменьшать, вот, делить на корень из двух, сколько-то раз, пока оно не дойдет до двойки.
И там уже этот q будет равно единице.
И тогда получится, что 3 четверти, ну вот, получается, вот, о, логарифм раз мы это сделали.
И вот...
То есть получается больше, либо 3 четверти, сколько раз мы это делали.
Ну там, сколько раз мы делили на корень из двух, ну получается, что-то типа...
Ну там, видите, то есть проблема в том, что мы тут не просто делили, а еще и там куда-то там округляли.
Поэтому давайте вот, ну там, ладно, верхний оцент, ну, наверное, поверим, что это там какой-нибудь три логарифма двоичных l.
Ну если бы нам не нравится ложка в степени, то как-то 3 двоичных логарифма.
Ну, бляжь, подозрение, что в как-то 3 двоичных логарифмах, наверное, мы как-нибудь уложимся, да?
Ну, пока что оно не очень подойдет.
А, ну, типа...
Такое рекуррентное имя ранее стало?
Ну да, что?
Ну, просто...
Так.
Ну вот, ну, то есть три четверти в степени логарифм, да?
Ну это, в общем, короче говоря, ладно, три, тут не три, или тут даже какая-нибудь константа С уж явно существующая, да?
О, то есть можно тут еще вот эту скобочку ставить, то на самом деле...
То есть на самом деле это равно один делить на N в степени какая-то мистическая там константа.
Ну вот, то есть это...
То есть там, это будет там логарифм двоичный, там четыре трети в степени С.
Вот, понятно, да?
Ну, правда заметим, что да, получается, что по сравнению с предыдущим легче не стало.
Да, потому что да.
То есть вероятность того, что не ошибки, это тоже там, видимо, если не аккуратно делать, то получится что-то типа один делить на N в квадрате.
Да, видимо, пока не хватает.
А теперь давайте подумаем, а что, ну вот, а как бы сделать так, чтобы хватало?
А можно попробовать оценить эту штуку поточнее?
Вот смотрите, вот у вас QOT это какая-то мистическая величина между 0 и единицей, да?
А теперь посмотрим, каков график этой функции от 0 на единицы?
Вот, а, хотя может быть...
Ну как бы, да, я не знаю.
Вот, а, хотя может быть...
Ну как бы, да, это такая парабола, причем вершина которой находится где?
Ну, в какой-то, ну вот, в какой-то точке типа два.
А, ну да, у нас так и получилось, что тогда получается, что...
Мистик?
Ну, вот, вот, вот.
Ну, вот, вот, вот.
Ну да, то есть хотим оценить снизу, значит подставляем.
Ну да.
Так, действительно, да, немножко есть что-то получилось.
Ну да.
Можно как-то геометрическим, геометрическим рядом?
Сейчас.
Думаю.
Ну, после первой тратцы у нас уже QOT будет больше...
Ну, хотим добиться, чтобы тут стояло что-то лучшее, чем один дилет на mst.
Ну, после первой тратцы у нас уже будет не больше чем 3,4, да?
Потому что даже если кой один, то там 3,4 ровно получается.
В следующий раз это должно домножается на число, которое хотя бы 1 минус 3 шестнадцатых.
И вот мы можем пытаться умножить такие слагаемые.
Каждый раз у нас будет 1 минус предыдущее...
1 минус предыдущее предыдущее 3,4, это ужасно.
Ну да, нет, просто не интересен как выглядит.
Слушай, как бы, можно подведеть там, просто искрисковать, да?
Что там ответ скажет? Заметим что.
То есть заметим что, докажем по индукции.
Ну ладно, придется видимо заметить.
Так.
Искок там получается.
Ну, в общем, так.
Ну, кажется, что как-то вот по индукции надо доказать.
Что QAT на самом деле больше либо равно 1 минус 3 шестнадцатых.
Ну, в общем, так.
Ну, в общем, так.
Ну, в общем, так.
Ну, в общем, так.
Ну, в общем, так.
Ну, в общем, так.
1 дилет на c лог n.
Ладно, не ценно.
Для какой-то константа d на самом деле вот так.
Что вы пишем? d на лог 2n?
Да, d на лог 2n.
Интересно.
Ну вот.
Значит, если мы пытаемся доказать по индукции,
а, это по любавщику верно.
И какой-нибудь константе d.
Дальше получается по индукции QAT больше либо равно
QAT минус Q квадрат на 4.
Теперь замечаем следующее, что это парабола.
То есть это у нас парабола.
Векшину у нас где-то больше единицы, ветви направлены вниз.
Значит, парабола на отрезке от 0 до единицы видимо выглядит как-то монотонно.
Ну ладно.
Векшину тут где-то есть.
И тогда получается, что ее можно,
если у нас есть оценка снизу на QAT,
то мы можем ею пользоваться.
Оценка снизу у нас 1 делить на d лог 2.
Ну давайте пока для простоты n поделить на кое-нибудь из двух.
Ну вот, минус 1 делить на 4 d квадрат лог 2.
Н по единисту.
Ну вот, это равно 1 делить на d.
Ну что у нас тут получается?
То есть это получается лог 2 честный n,
минус 1 вторая.
Плюс 4 d лог 2 n.
Так, нет.
Откуда записалось?
Нет, это я слишком, да.
Это я тороплю события.
Ну давайте так, это равно
на d лог 2 честный n корень из двух.
Это мы вынесем просто.
И здесь мы напишем 1 минус 1 делить на 4 d лог 2 честный n делить на корень из двух.
А, лог 2, ну нет.
Ну тут квадрат, значит тут лог 2 тоже надо.
Вот так вот.
Ну так что, это и есть знаменатели.
Ну давайте тут порасписываем.
1 делить на d лог 2 честный n делить на корень из двух умножить на,
смотрите, 4 d умножить на лог 2 честный n минус 1 вторая,
минус 1 делить на, сколько там, 4 d лог 2 n корень из двух.
Вот.
Ну по большому счету, если мы это оцениваем сверху, то заметим, что получится минус 2 d, что там получится, да?
1 делить на d лог 2 честный n корень из двух.
Ну на самом деле тут с формулировкой так же.
При каком-то достаточно адекватном d, да, ну тут как бы минус получится, минус 2 d, минус 1, да?
А тут получается, а тут получается минус d пополам получается, да?
Поэтому там, поэтому есть подозрение, что тут при больше либо равно,
по-моему, кажется, с кем-то из этих шлеп-шлепов можно и по сокращать.
Или нет?
Ну, не потратите.
Ну давайте смотреть.
Итак, туда нам, ну да, хотя нам бы, конечно, в идеале как бы, он называется, d лог n откуда, откуда бы выкавырить.
Сейчас, мне кажется, то, что мы приписали в виде дробис, скобку, это только хуже делал.
Потому что мы хотим уже сказать, что тут 1 делить на d лог 2, n делить на корень из двух, на эту скобку хотя бы 1 делить на d лог 2n, да?
Очень хотим.
Ну если вот перенести в разные стороны, там очень хорошо вынесется, там останется все типа.
Ну давайте.
Так, давайте-ка.
Там вынесется 1 вторая, хотя бы 1 делить на, как раз, 4 d квадратные.
Ну давайте разбираться.
Значит, у нас тут есть 1 делить на лог 2, n делить на корень из двух, минус, сколько там, квадрат поделить до 4, минус 1 делить на 4d, лог квадрат, n делить на корень из двух.
Мы хотим, чтобы это было больше.
Четырнадцать д квадрат, наверное?
Да, квадрат, конечно, да.
И хотим, чтобы тут был лог 2n.
Ну и причем в том, что d-шка убилась, одна, да?
Наверное, как и макияж.
Так.
Ну и получается, что 1 делить на лог 2, n делить на корень из двух, минус 1 делить на лог 2n, должен быть больше либо равен, чем 1 делить на 4d, и это нет.
Вот.
Вот.
Вот.
Макияж.
Так.
Так, а тут у нас что получается?
Так, а это получается равно, если я правильно понимаю, одна вторая поделить на вот эти вот эпические логарифмы, да?
Вот это я правильно понимаю?
А, нет, вы здесь лучше так не делаете.
Ходим на двух, больше либо равно 1 делить на 4d, лог 2, n делить на корень из двух.
Ну тогда видим шлеп, видим шлеп.
А, а еще видим шлеп, и видим тут шлепы, аж и шлепы два.
У нас не нравится было, что код n больше или меньше, а код t, код t, и почему вы там эти подставляете?
Это n на корень из двух.
Да, в тему подставляем.
Да, вот это все.
Ой, опять, то же самое.
И тут получается, да, 2d должно быть больше либо равно, чем лог 2, n делить на лог 2, n поделить на корень из двух.
Ну, что равно лог 2, n поделить на лог 2, n поделить на корень, n на n минус одна вторая.
В общем, что-то мне подсказывает, что при д больше либо равно двух, верно, вообще всегда.
Ну да.
Единственная же проблема, что t это n поделить на корень из двух, а n поделить на корень из двух плюс один.
Поэтому, видимо, замнем.
Да, замнем для ясности, видимо.
Вот.
Ну, это там уже замнем, но я бы уже поверил, что там уже выкручивается.
Вот.
То есть таким образом, да, к сожалению, для меня пока осталась загадка, как можно это увидеть, что тут как бы 1 делить на o от логарифа.
Вот.
Но, в чем это нас приводит?
То есть получается, нет, вообще так как бы привело это нас к тому, что вообще так как бы теперь вероятность того, что мы попадем в правильный ответ, теперь уже не менее чем не 2 поделить на n квадрат, а 1 делить на лог.
Ну ладно, 1 делить на там сколько?
Уж два лога.
Неплохо, правда?
И типа мы повторяем это константу логарифом раз?
Ну получается, да.
То есть получается, если мы повторяем этот алгоритм два лога и 1 раз, то получается, что у нас вероятность того, что мы ошибемся, не происходит опять 1 делить на epsilon.
А мы этот повторение делаем только в самом начале, то есть не в каждом курсе.
Ну да.
Просто казалось бы, можно было бы сделать скрип, который сводит та вершина и на та вершину запускает заново алгоритм, который ходит в минимальный разрыв, видимо, не обязательно.
Ну а мы чем занимаемся?
Мы вызываем алгоритм, который рекурсивно вызывает в себя же два раза, и на все это мы навешиваем уже несколько вызовов.
Нет, ну не совсем. Я бы сказал, что мы берем graph и два раза делаем следующее.
Вжимаем оно на t и запускаем все рекурсивно.
Да, и вот этот алгоритм мы повторяем в какой-то там количестве раз?
Ну по высоте, да. По факту дерева получается, да.
Ну вот теперь мы секретов просто закроем, в какую точку мы умеем это делать.
И потом сказать, сколько работает эта штука.
Ну это рекуррент, так сказать.
Ну давайте, что за... Даже можно написать такая.
То есть время работы алгоритма, но это просто не цифра, если вы видите внимание.
Равно на самом деле 2 умножить на какое время нам потребуется, чтобы сделать это сжатие.
Ну, это подряд по-любому.
Сколько мы еще раз... Как мы еще раз решили делать?
Ну вот у нас алгоритм. Мы двумя способами сводим к этой вершину, вызываем рекурсивно.
Весь алгоритм мы повторяем лог раз. Константа на лог раз.
И вот t от n это как бы асимпточка работы именно одной такой террации.
Потом мы еще должны асимпточку на алгоритм.
Ну вот.
Я короче утверждаю, что t от n на самом деле равно это t от n квадрат лог n.
Ну если вы просто раскроете эту штуку, то получится то, что у вас там дальше будет 2 раза n квадрат пополам.
И когда в следующий раз там будет у вас там 4 раза n квадрат поделить на 4, ну и так далее.
Ну может еще раз. Как мы выпускаемся?
Вот короче вот в адрепрете написано, как мы запускаемся.
Если сам алгоритм так делать?
Короче, постал как мы доказали про квадрат.
Ну алгоритм...
Постал как мы доказали, алгоритм не поменялся очевидно.
На чем доказательство было в этом смысле?
Нашел оценку снизу вероятности, поэтому чтобы у тебя все сошлось, тебе надо пустить обратное число этой вероятности.
Да. Все, что мы теперь делаем, это за сколько этот сам алгоритм один работает.
Один завтра сколько работает.
Мы утверждаем, что это n квадрат лог n.
Потому что вот я и рекуинту написал на это.
Вероятность ошибки стремится к 1 леветной.
Ой, на f все равно на n.
На n, да.
Получается работать это будет...
То есть так как мы будем спускать o от логарифа по раз, то там что-то получается типа...
То есть итог 8. получается o от n квадрат лог квадрат n получается.
И вот это уже неплохо.
Но деморрой ради этого...
Не, ну как сказать, нельзя сказать, что это деморрой был в доказательстве.
Потому что мы запутались.
Ну приязвился, если быть точнее.
На самом деле алгоритм в общем-то...
Не супер-сладно пишется, кажется.
Ну тут тоже.
Ну сложнее, конечно, чем в предыдущем.
Ну правда, если вы там не заморачивайтесь и копируете граф, то будет нормально.
Знаете, иногда это полезно.
Я помню, как-то в какой-то момент мне потребовалось написать Centroid Decomposition.
Но мне было дороже, чтобы все слои Centroid Decomposition написать не надо было.
Поэтому я делал так.
Находил Centroid, а потом честно его удалял.
Графы просто честно копировал, честно перенумеровал.
И просто кидал рекурсивно копии.
Вот.
И в результате решение зашло там, за прям вот тут столичком.
Правда, на поверху оказалось, что авторское решение по Centroid Decomposition,
это более простая динамика с доказательством того, что она там...
Сотокладная сентультика ровняет пояс.
То есть, видимо, знаете, после этого хорошая формулировка от Натальи Бондаренко была в одном из контейнеров.
Такое решение оказалось неожиданностью для юри, поэтому оно не заготовило против него тесты.
Ну, то есть, как бы...
Ну, то философский вопрос, конечно, могло ли так быть, потому что, может,
потому что у меня как бы решение было, вроде, с асимптотикой, там, может быть, там,
вроде как, которая должна заходить, по этому месту ответить.
Ладно.
Вот. Ну.
Так, ну что, тут вроде разобрались.
Или нет?
Вроде, да.
Вроде, плюс, да.
Плюс-минус разобрались какие-то, да.
Самый ритм простой.
Ага.
Ага.
Ну, как бы вот, да.
Ну, да, ладно там, совсем.
Ну, мелочи там, ладно, докручиваются.
Видно, что, может, одну там итерацию как-нибудь без этого сделали,
или там что-нибудь еще.
Ну, вот, но это, в общем-то, и не важно.
Можно тут писать N квадрат плюс один, и там все равно обнагружено,
что на каждом уровне там...
Ну, то есть, понятно, что там лишние единицы, которые тут возникают,
пишем вот сюда в N квадрат и не паримся.
В общем, ладно.
В конце концов, что мы зря,
потому что мы в прошлом сервисе там писали это дезак,
которое там визуально было показать, что эти мелочи не ношу не влияют.
Вот, ладно.
Поэтому давайте-ка перейдем к, возможно, еще более красивому.
Еще более красивой ситуации.
Сейчас мы будем минусов искать.
Жалко.
Так.
Не будет.
Так.
Там есть алгоритм, который...
Ищет его с почерствами.
За два раза.
Ага.
За два раза.
За два раза, если что-то.
А дальше...
Это будет абсолютно все.
Ладно.
Нет, ну там начнем с...
Нет, там, видите, главное еще не путать вероятность алгоритма
приближенным алгоритмом.
Может, вероятность алгоритма дать,
что он там работает с крутой вероятностью,
вот, ожиданием приближенным алгоритмом.
Это означает, что он как бы дает ответ,
который не более чем во сколько-то раз превосходит правильность.
Ну, как теория модулируем к, да?
То есть, сделаем ка и ка раз, соединится,
и после этого гарантируем, что правильный ответ
не больше того, что мы нашли, там,
не более чем в ка плюс один поделить на ка раз.
Не важно.
Так, ладно.
Давайте минусов искать.
Так, к сожалению, мы так и не учились искать минусов
за ено-обратную функцию окермана.
Да.
Хотя, казалось бы, зачем софхип учили?
Да ладно, сейчас можно еще будет надевающие понятия,
то, что им не надо.
Сейчас мы его...
Потому что сейчас мы убедимся, что, оказывается,
можно, на самом деле, минусов искать за оркея.
Это можно.
Правда, в смысле, мы это ожидали?
Вот.
Да.
Так вот.
Поехали.
Значит, как искать минусов?
Ну, для этого, чтобы найти минусов, мы вспомним
такой алгоритм, как алгоритм Борувки.
Ну так, это сложный алгоритм?
Нет, это простой.
Нет, нет, нет.
Как бы есть три базовых алгоритма.
Борувка, прямых раскал.
Все остальное это навороты на них.
Значит, напомню, что Борувка,
он работал следующим образом.
Вот у меня есть град,
которым я ищу именно стол.
Ну и допустим, давайте для простоты,
чтобы не заморачиваться, будем считать,
что все лесоребры попарно различны.
Значит, идея такая.
У каждой вершины находим минимальное ребро,
торчащее из нее.
Вот выделяем все эти минимальные ребра,
получается, какой-то набор вот таких вот цепочек.
Цепочек, цепочек.
Анбатя.
А, Брунт, ладно.
Бывает и деревья, да?
Бывают циклы.
Сейчас, в основном, это ориентированный граф,
который вы хотите мне, как бы, начинать.
Это...
Ну как бы могут быть циклы, но мы специально сказали,
чтобы все было попарно различны.
Ребра были попарно различны для того,
чтобы у нас все упиралось в какое-то ребро,
которое тут с две стороны пошло.
Да, вот.
Ну, получается, деревья, короче, мы их там,
типа, мы там, с помощью там, в ленной разрезе
можем показать, что, действительно,
эта часть миностово сжимаем и радуемся.
Это называется шаг борубки.
Как еще раз смотришь на это?
Ну, для каждой вершины мы добавляем в миностов
минимальное ребро, торчащее из нее.
То есть, можно аккуратно применить там,
лему об безопасном ребре в последовательном,
но можно показать, что этих серебра
можно одновременно добавить в миностов.
Ну, и конечно, сжать компоненты связанности.
Ну, приятность этого шага заключается в том,
что количество вершин В уменьшается хотя бы в два раза.
Помните, да? Было дело?
Ну, видимо, два дечи.
Вот.
Сейчас, Зай, еще раз, как мы сжимаем?
Ну, просто все эти ребра сжимаем.
Для каждой вершины мы в восторг добавляем
минимальное ребро, торчащее из этой вершины.
Если рассмотреть все эти ребра для каждой вершины,
то получится набор компонент связанности.
Ну, реброда добавляем только если оно берет вершину,
из которой еще не было ничего.
Нет, это не важно.
Ну, а если треугольные?
Не может быть.
Если у тебя зациклится,
то ты в первую вершину пришел с меньшим ребром,
чем из нее вышел, это невозможно.
А равных софтных?
А равных нет.
Вот ровно для того, чтобы циклов не было,
я говорю, чтобы равных не было.
Ну да, ты отсюда вышел пять,
сюда вышел три,
сюда пошел два,
а потом сюда пришел один.
А какого хрена ты вышел отсюда пять,
если у тебя тут было один?
Все.
Это называется шаг-борувки.
То есть, по идее, есть алгоритм борувки,
изобретенный в далеком 1926 году,
еще до появления компонентов.
Заключается в том, что давайте сделаем
логарифм В шагов борувки.
Или сколько там надо.
И получится уже, в принципе,
логарифм за елом В.
Вот.
Это один из базовых логов.
То есть, уже в 50-е годы появились алгоритм Прима
и алгоритм Краскала.
Ну и, в общем-то, на самом деле,
все остальное, что мы изучали,
это практически модификация первых троп.
То есть, где-то модификация,
добавление каких-то структур,
которые позволяют то ли в Приме,
то ли в Краскале убирать более адекватные ребра,
и так далее и тому подобное.
Вот.
Значит, что нам предлагается делать теперь?
Значит, здесь нам предлагается начать с того,
предлагается делать следующее.
Значит, мы, да, но я скажу так,
мы будем искать, на самом деле,
даже не минимально остовное дерево,
а минимальный остовный лес.
Ну, потому что будем считать,
что иногда граф у нас бывает несвязный,
и тогда надо искать минимальное,
в каждой компоненте еще минус 100.
Понятно, да?
Да.
Ну вот.
Значит, теперь возникает такая идея.
Значит, следующая идея, которая у нас возникает.
Значит, я утверждаю, что мы будем искать
значит, я утверждаю, сейчас будет мистическое утверждение.
Я, может быть, не умею искать за линию минимальный остов,
но я умею за линию проверять,
является ли остов минимальный,
заданный остов минимальный.
Или переформулирую задачи.
Вот, допустим, у меня есть какой-то граф,
какой-то там веселый граф,
и, допустим, у него есть остов.
Какой?
Какой?
У него, да.
Ну, допустим.
И теперь идея следующая. Я хочу понять, минимальный это остов или нет. Как мне это сделать?
Рассмотрим связанный граф, рассмотрим какой-то остов в нем.
Ну, это типа говорим, что если граф не связанный, будем искать минимальный остовный лес.
А так вот, давайте допустим пока еще есть связанный граф, и в нем есть остов какой-то.
Я хочу проверить, является он минимальным или нет.
Как я это буду проверять?
На уровне идеи возникает следующее.
Помните, у нас была вся рема.
Остов является минимальным тогда и только тогда, когда любой ребро не из остова является максимумом на стягиваемом цикле.
Эта задача была, кто-то ее даже сдавал.
Остов является минимальным тогда и только тогда, когда для любого ребра, необходящего в остов, верно, что это ребро является максимум на стягиваемом им в остове цикле.
Очевидно, что если остов минимальный, то это так, потому что если это ребро меньше кого-то, значит за менее получим меньше.
Но, оказывается, если аккуратно по шамане, то выясняется, что в обратную сторону тоже работает.
И отсюда следовало еще приятная вещь, что если у нас все ребра попали на различие, то минус-стов вообще один.
Красивая лемма, но сейчас это не так принципиально, но вот такой критерий был.
Поэтому теперь нам надо просто научиться проверять этот критерий.
Можем даже попробовать там 5 минут, или просто поверить, что, оказывается, за от В плюс Е это делается.
При помощи DX.
Нет, ну как сказать.
Ну да, то есть фактически задача в том, что дано дерево, но учитесь, пожалуйста, в оффлайне искать пути.
Там не просто пути, а максимум на пути.
То есть длину-то пути найти не сложно, потому что Сашку умеете там золотой единицы искать при подсчете.
Ну и тебе там фарфол бендерами.
А вот как искать максимум на этом пути, вот это, конечно, весело.
Интересно.
Но это на одной трассе? В смысле, за от В плюс Е прям проверить?
Ну, вот, ввижай, что за от В плюс Е для всех этих критерий в оффлайне это можно проверить.
Без предпочтений.
Это каким-то вот мистическим образом утверждается.
Не знаю каким, но вот утверждается.
Кратные ребра у нас есть?
Ну, может быть.
Ну, почему? Может и с кратными.
А чем вам тут кратные ребра пофиг?
Ну ладно, кратные ребра это никогда не проблема.
Потому что вы за В плюс Е можете от кратных ребр всегда избавиться.
Ну, правда.
Так что, если так хочется, можете считать, что их нет.
Отсортируем ребра за Е по вершинам и сожмем кратные.
Там всех кратных оставим и просто минимальным весом.
Да, это за Е делается.
Вспоминайте, что это, например, суффиксный массив.
Ну, там пара чисел от 1 до N отсортировать за от N.
А, да, все, да.
Вот это вот, да.
Ну, или там, я не знаю.
Она родимая, да?
Она родимая.
Ну вот, так что, можно поймать?
Нет, мне интересно 5 минут подумать.
Можем мы сейчас придумать такой алгоритм?
Или просто придется соваться, что какой-то там умный хагеруп это умеет делать?
Хагеруп это фамилия, слушаю.
Ну, то есть проверяем.
Для каждого ребра найти верно лишь, что он там больше, чем все остальное.
Все растягиваем.
Ну, я спросил, потому что у нас в основной вальты играю без мокритерия.
Позже все как раз таки ровно не обижающее имя в нашем острове.
Да.
Хочется как...
Ну, да.
Нет, я просто, почему я задумываюсь, потому что обычно я всегда совался,
но в этом семействе у нас как бы произошла норинка в сравнении с предыдущими поколениями.
Мы научились левелом сестер-клеве делать.
То есть мы умеем за оа-темпель подсчета подниматься в подвешенном дереве на любом высоте.
Чего?
Нет, вот я говорю, вот это мы с вами научились делать.
Поэтому вот интересно, нельзя ли модифицировать эту технологию так, чтобы...
Ну, потому что, смотрите, можно как бы свести задачу, потому что все эти голубые ребра, они обратные.
Ну, то есть, смотрите, допустим, мы подвесим это дерево за какую-нибудь вершину, да?
Любую там первую попавшуюся.
Ну, если очень хочется, можно центроить, но я не думаю, что это будет принципиально.
Теперь заметим следующее.
Что я...
Если я теперь рассматриваю ребро, то заметим, что про...
Ну, там вот это...
Элцао этих двух вершин, я умею там как-нибудь искать золотые единицы после линейного предподсчета.
Это даже не левел-анцестер, это фарреш колтон-бендер.
Поэтому, по большому счету, теперь надо подняться вот на этом пути, найти максимум, и вот на этом пути искать максимум.
Поэтому можно сказать, что теперь, как бы, если у нас задача найти максимум на пути, то как бы задача сводится к обратным ребрам.
Там были какие-то спросы, но...
Да, ну вот возникает вопрос, да.
То есть, интересно, нельзя ли там действительно это поупить?
Но мы что-то же...
То есть, правда, вот с максимум просто у нас всегда проблемы.
Ну, спортси-то да.
Мы хотим искать...
Ну, скажем так.
Если бы у нас были двоичные...
Справляю так.
Если бы у нас были двоичные подъемы в полном составе, да, то на самом деле, да, мы могли бы золотые единицы это найти.
Но проблема в том, что двоичные подъемы...
Ну, потому что там, если расстояние между ними равно L, то находим максимальную степень двойки, берем там вот это и вот это.
Там, где мы находим вот это, ну, там поднимаемся за вот эти единицы.
Но проблема в том, что для этого придется насчитать двоичные подъемы, которые у нас, извиняйте, всегда плохие, да.
То есть, по любасику и анлогу.
Поэтому видим, поиск это максимум, значит, придется, значит, делать что-то более умное.
Потому что, к сожалению, да, когда мы поднимались на Level Ancestor Query, мы что-то по пользованию с тем, что, чтобы подняться на высоту 57, можно опуститься на, там, высоту 122, а потом подняться на высоту 179.
Да, этим мы пользовались.
Да, тут с максимумом, тут, к сожалению, так не прокатит, потому что максимум у нас такой необратимый.
Вот, поэтому...
Поэтому, видимо, ну, вот.
Ну, то есть, видимо, правда, это мы еще пытаемся в онлайне как-то делать.
То есть, видимо, может быть...
По аналогии с алгоритмом Тарьянова.
Да, но алгоритм Тарьяна там...
Ну, скажем так, алгоритм Тарьяна, он как бы...
Как бы еще...
Диви, я вижу, там...
Есть идея.
Да, какая?
У нас с Level Ancestor Query мы решали через, там...
Мне кажется, можно делать ровно так же...
Мне кажется, да, можно делать ровно то же самое, что...
У нас с Level Ancestor Query мы храним просто массивы, и можно...
Для них еще есть у них максимумы.
Так.
Прямо максим на пути от вершины вверх.
Так, а, причем, а, там предподсчет можно сделать за линию на этих путях, и за вот единицу по полмаксиму.
И для билеев тоже, да.
Да.
Для вот этих мелких подграфов, которые там до размера небольшим, там понятно, что можно сочетать вообще целиком.
Так.
Ой, боже, да, как тогда...
А, ну так, нет, там, как с повелом, что еще там были эти мелкие графы, на которых надо все предподсчитывать.
А, кстати, а как с ними работать?
Для них понятно, как искать подъем, потому что там просто мы говорим, что изо... ну, там небольшое количество различных.
Не, ну там актами, да.
Да, да, проблема в том, что если теперь...
У нас деревьев было мало, но у нас теперь не просто деревьев, но у нас не деревьев мало, у нас же теперь на каждом ребре еще и вес какой-то висит.
Нет, у этих лесов, конечно, можно сжать координаты.
Нет, сейчас, а можно же сказать, что, типа, у нас вот этих, вот этих подделеев размера, которые губят, да, их там, типа, не больше, чем ОТН делить на бубен.
Да?
А в каждом мы можем просто все пары вершин перебирать, просто, по сути.
И это будет суммарно за небольший смотан работать.
Нет, погоди, ну, проблема в том, что у нас, смотри, тут проблема такая.
Там фишка была такая, то есть самих деревьев типов, действительно, их очень-очень-очень мало.
Да.
Их даже меньше, чем Н делить на ЛОГН.
Да, ну да, там их мало, но я про то, что именно в любом графе таких подделеев, мелких их, не больше, чем ОТН делить на бубен, да, мы эти доказывали.
Да, да.
А в каждом не больше бубен вершин, ну там два бубен, ну вот такого рода.
Ну вот бубен, да.
В каждом из них мы можем просто все пары вершин перебирать, по сути.
Да, но это будет работать N делить на бубен, умножить на бубен в квадрате.
Ой, не было.
Да, что-то я.
То есть нет, полное ощущение, что придется как бы, там, я не знаю, координаты сжимать, что ли, и это вот как-то страшно.
А как мы делали для маленьких деревьев?
Ну, коренат, да, коренат.
Искали инсайны.
Ну, там имелось предуследование.
Во-первых, самое главное, что мы умели, это по дереву запустить DFS, получить строчку длины 2 бубен, ну из 0 и 1.
И сказать, что дерево такого кода у нас, оказывается, уже там где-то есть.
Да.
У нас там...
Ну, мы в нем не предпочитали доказывать.
Нет, по-моему, сейчас, надо вспомнить.
Мы там, по-моему...
А, ну там...
А, нет.
А знаете, в чем фишка?
Ну, у нас был не логорифм.
Бубен-то у нас был логорифм поделить на 4...
На 8.
Поэтому 2 в степени стока еще в квадрате, это нас устраивало.
А, вот.
И тогда все, и тогда...
Ну, тогда у нас деревьев немного.
То есть, перебрать каждый из этих деревьев, нет.
То есть, можно только все типы перебирать.
И тогда это означает, что типы надо сводить.
Ну, надо щука закодировать, тогда получается бесареба, да.
А это плохо, потому что...
Это добавляет, как, ну, типа количество вер tonих деревьев умножается на будден.
Понятно, на кронок unfortunate.
А это совсем много вообще.
Ну, как...
Да.
Ну будден в степень и будден еще.
Ну, это понятно 2 в степени, будден у plotting.
Да, 2 в степени 누рнважается на будден степени и будден, это становится...
Хотя не нормально, в целом.
Потому что 2 в степени будден это корень в восьмом степени зена...
Еще в степени лог-лог-будден.
«Будден – это корень восьмом степени зена».
Бубен это не колень в восьмой степени, бубен это...
Это два в степени бубен, это колень в восьмой степени изен.
Да.
А еще в этой степени...
Бубен можно так делать.
Ну, короче, нет, это многовато круче.
Ну да.
Не, ну не знаю, надо просто это полчаса...
Полчаса...
Не, ну как сказать...
Как жить без проверки?
Ну не знаю...
Ну хочется же...
Нет, то я не знаю, нет, можно как бы поупить,
потому что на самом деле...
Скажем так, внутри каждого мелкого дерева
можно запустить DFS для каждой вершины,
ну хотя бы там запустить,
найти максимум на пути от нее до корня этого мелкого дерева.
Ну это по-любасику можно, да?
Да.
Вот, все что нам останется,
это значит даны куча мелких деревьев
в каждом мелком дереве научиться искать вот такие запросы.
Ну вот, теперь да.
Не выяснится виносом.
Ну да.
Потом выяснится...
А, ну да, потом начнется чьи-чьи.
Давайте в каждом из этих деревьев мы повторим этот алгоритм.
После этого у нас останутся деревья размера,
даже не лограя, но лограя.
И вот ты, ну вот.
Ну в смысле, ну как бы у нас есть,
у нас проблема в том, что у нас осталась куча деревьев размера лограя.
И внутри каждого из них
надо на эти все вот запросы отвечать.
Ну да, понятно, что на практике
легче уже просто ответить и не париться,
но мы умные,
мы как бы пытаемся
предполагать, что у лограев бывает 100 тысяч.
Вот.
И тогда у нас...
И теперь вот возникает вопрос, да,
можно ли вот с мелкими деревьями
на самом деле как-то расквитаться как-то адекватно?
Учитывая, что суммарные их размеры равен N.
Суммарные их размеры равен N.
От N?
Суммарные от N.
Ну от, ну количество вершин там от N суммарно, да.
Ну как бы у нас надо.
Надо вот научиться тоже подниматься
на какие-то высоты для их.
Желательно.
Желательно тоже.
Не меньше, чем суммарные?
Не N, а будет же.
Ну ладно.
Ну ладно, меньше, потому что те, кто там на эти
итоговые вершины не попал, да, не попадают.
Плоть до N.
А синтетически лучше оценить не получится.
Так, хорошо.
Ну как бы слёту можно сделать,
если мы N превратили в log N,
то внутри каждого из этих деревьев
мы можем свести задачу куча, куча, куча
мелких деревьев, размер которых
будет log log N.
Ну тем же самым методом, да?
Да.
В теории.
Нет, потом будет log log N,
потом log log N, можно даже log log log N,
в общем, логится можно любое константное число раз.
Теперь возникает вопрос.
Насколько должны быть мелкие
деревья?
Да, допустим,
давайте я вот сейчас посмотрю.
Log log log N, да?
Да, но внутри
каждого дерева, значит, сколько у нас
деревьев с точностью до
различных, ну там, в каждом из этих
ну да,
правда, проблема в том, что в каждом из этих деревьев
в поле FN мы поделим на log log log N,
мы сжать координаты не можем.
Ну в том плане, что
как бы
но сфагулирую так,
если бы
ну сфагулирую так, мы бы умели
решать задачу, допустим, да,
если бы у нас, ну допустим так, у нас не было бы
проблем, если бы
все
допустим, если бы у нас все веса,
все ребра были бы от 1 до N, допустим, да?
Можем такое допустить?
Еще раз, что мы умели бы решать, если бы все ребра были
от 1 до N, да?
Ну да.
Ну вот, да.
Ну вот.
Ну то есть, тогда если у нас от 1 до N,
то мы во всех этих деревьев можем
решать координаты так, что в каждом из этих деревьев
веса были только вот от 1 до
до размера.
И тогда это привело бы к тому,
что мы их можем успешно кодировать.
Ну то есть, когда мы проходим по ребру,
мы пишем не только 0 к единичку,
но и это вес этого ребра.
Получается, конечно, деревья, когда получается
2 log log log N,
то есть, ну кодов, по крайней мере, да?
Но в степени...
2 умножено...
А, ну в степени log log log, да.
Так.
Ну вот.
То есть, каждая мы, наверное, ну вот, теперь
вот, каким-то называете,
верите ли вы, что это значительно
меньше, чем N?
Как вы думаете?
Да. Да?
Так чему?
Ну, потому что это...
Ну, давайте так.
Это 2 в степени log log log N,
в общем, это log log log N.
Давайте так, связываемся с N, берем угариф,
log log log N,
log log log N.
Сравниваем с log N.
Так, ну понятно, что это меньше, да?
Можно было даже лишний раз не ловить.
Так.
Да, в чем скорее всего, тут можно там и
на 4, и на 8, и на 16 определить.
Да, мы, кажется, победили.
Почти.
Так, значит, заказировать можно, да?
Ну, положить это на вот это вот в квадрате
можно, чтобы там предпочитать все ответы.
Следовательно...
Кем-то внутри каждого
дерева можно предпочитать все ответы,
сохранить их в таблице, возрадоваться. Ура!
А нота водится.
Мы же сейчас живем
в мире, где у нас
чисто кутнобыля.
Черт. Да.
Так, это мы победили, когда у нас все
весогребы, это того да это, да?
Мы их закодировали, потом, когда
берем под дерево закодированное,
мы берем...
делаем все со втором по счетам и радуем.
Ну да.
Во всех их одновременно делается
какое-то сжатие, ну да-да-да.
Да стоит, стоит.
Только какая-то статья.
Да, но это
работает для вот от одного ДМ.
Да.
В идеале бы нам, конечно,
хотелось, чтобы это работало, когда веса являются
этими мистическими камешками.
Камни и силы.
Ага.
Камни и силы.
Камни и силы,
камни и силы.
Это можно сделать?
Ну, как сказать?
Какой-то алгоритм есть.
Вот есть ссылка на статью какую-то.
Мне сейчас стало интересно, не может ли
вы эту статью изобрести своими силами?
Чего? Ну да.
Часть алгоритма будет, что вот
проверьте за О от Н, что тут, что это
минус 100, да или нет.
Более того,
на самом деле тут даже, на самом деле
происходит даже что-то еще более крутое.
То есть нам надо будет,
то есть там этот алгоритм даже утверждает,
что он будет для каждого ребра хранить, оно адекватное
или неадекватное.
Ну, в роли адекватности у нас теперь
как бы, будем говорить так.
То есть я вот даже определение
напишу и после этого, наверное, нам имеет смысл
на перерыв отправиться.
А то что-то как-то уже бубух, да.
Ну, можно и так сказать, да.
То есть определение.
Значит, будем говорить.
Значит, пусть у нас
G равно VE.
Это какой-то граф.
Действительно, и
допустим, и F
остовный лес в нем,
произвольный остовный лес.
Тогда
значит, ребро
значит, тогда
ребро E, которое
не лежит в этом
лесе, да.
Ну, точнее так, ладно, просто
ребро
будем называть его F
тяжелым.
Если E не лежит
в F
и E стягивает
в F
цикл,
на котором
В, вес этого
ребра максимален.
Вот.
Значит, иначе
ребро E называется, ну, как вы уже догадались,
F
есть.
Просто чем, зачем нам
это нужно? Утверждение.
Смотрите, заметьте следующее.
Значит, мистическое утверждение.
Да, напоминаю, что у нас все ребра попарно
различны, да.
Так вот, мистическое утверждение.
Если
Z
легкое,
то есть, допустим,
то есть, наоборот, если
F тяжелое,
хотя бы
для какого-то F,
то
Z
не лежит в
на столе.
Мы же все ощущаем, что все веса различны.
Да.
Да, напоминаю, что
здесь важно. Иначе
все, да, напоминаем,
все веса попарно различны, ну, иначе это утверждение
будет верно.
Понятно, откуда я это
утверждение сделал?
Ну, идея простая.
F это у вас
F тяжелое
или F тяжелое вроде?
Хотя бы для какого-то
основного веса F.
F тяжелое, там F
везде.
Но ребро тяжелое и легкое тасить
на какого-то конкретного
основа F.
А, E
F тяжелое.
Это значит, что есть цикл с
E, на котором E максимально.
Да, по большому счету это значит, что
ребро E устроено так, что есть E
и есть цикл, на котором это E максимально.
Ну, если E не лежит
в миностове, то это очевидно верно.
Ну, потому что E будет...
Так, там наоборот надо, если E...
А, нам в одну сторону, да?
То есть, подвижение такое.
Если E максимум в каком-то цикле,
то Z точно не лежит в миностове.
Ну, это следует, например, из одного
алгоритма, который
к раскалу.
Ну, почему?
С самой раскалой оно не следует.
Ну, следует, потому что значит, что
в момент, когда мы будем брать E,
весь шуруп будет на компоненте.
А, если предполагать, что у нас единственный граф.
Ну, мы же предполагаем очень.
Ну, да, более надежное
доказательство тут звучит так.
То есть, действительно, рассмотрим
миностов с участием E.
Вот, рассмотрим этот
миностов.
Ну вот, и если
ребро удалить, то дерево распадется
на две компоненты связности.
Теперь заметим, если мы рассмотрим
этот цикл, вот этот максимум,
на котором E максимум, да?
То есть, заметим, что на этом цикле найдется хотя
бы одно ребро, которое соединяет эти две
компоненты. Ну, может и несколько
найдется, я не знаю, хотя бы одно найдется.
Но заметим, что когда мы добавим это ребро,
миностов останется остовом, но он строго уменьшится
противоречия.
Вот.
Поэтому вот, забегая вперед,
то есть, общая идея будет такая, я
буду находить каким-то рандомным образом
более-менее адекватные остовные
леса, а потом
за O от V плюс E буду пробегаться
и выкидывать все ребра, которые
оказались тяжелыми относительно E.
Эн от E это что?
Это 19?
Что? Эн от E.
Где? На котором
E максимально. В.
Да.
Ну да. То есть там вообще
чип будет заключаться в том, что
более того, мы будем говорить так,
выкинем там что-нибудь типа половину ребер,
построим миностов,
типа, ну реально построим миностов, потом
проредим, значит берем старые
ребра, выкинем тяжелые, и потом
в итоговом прореженном графе
найдем уже реальный миностов.
Вот примерно в эту сторону будем думать,
ну вот, собственно,
у этого алгоритма будет преимущество, что он будет работать
средним быстро, но и в лучшем случае он будет
работать за адекватного прибыли в итоге.
Но почему это так, мы узнаем
после перерыва.
Так, я прямо сейчас же так и напишу, и так,
веселый алгоритм
рандомизит
МСФ
передаем на ход
ВВЕ
Чего?
Ну передаем
ну можно написать
Ж равно ВВЕ, да.
Давайте.
Ж равно ВВЕ, пожалуйста.
Так, ну давайте, шаг ноль.
В оригинальном алгоритме его нет,
но на всякий случай его примерим.
Избавимся от кратных,
ну вот, избавимся от кратных скребок.
Так, что там еще надо?
Ну еще там
какой-нибудь ноль, там, ноль штрих
как всегда.
Ну ноль штрих там, если, я не знаю,
модуль ВВЕ
ВВЕ меньше либо равно
чего-нибудь, я не знаю, шесть там
от баллы, я пишу сейчас.
То
то найдем анод,
то найдем ответ
как угодно.
Главное, чтобы завод единился.
Ну и так.
Ну а теперь предположим, что рекурсия нам
реально понадобилась.
И
значит,
теперь
ну ладно, да, можно не один, а просто
один. Теперь ладно, шаг два.
Теперь, чтобы синхронизироваться.
Будем делать так.
Сделаем в Ж
три
почему-то шага борувки.
Вот.
Получим
ну получим граф
Ж звездочка, равен
В звездочка, Е звездочка.
Ну, нет, мы увидим, почему нам три.
Ну можно там четыре, пять, шесть делать, но три нам хватит.
Да, Перича?
Давайте еще
возьмем.
Да.
Значит, далее
построим такой граф
H.
H это такой, значит
граф
В звездочка и Е
какой-нибудь еще.
Ладно, Е звездочка, звездочка.
Е звездочка, звездочка.
Е звездочка, звездочка.
Это подмножество Е звездочка.
Вот. Причем
что важно?
Вот причем
вот каждая ребро
каждое ребро из Е звездочка
добавляется
добавляется
соответственно
вот это вот
Е звездочка, звездочка.
С вероятностью
я не буду там писать это независимо
поэтому и так понятно.
С вероятностью П
равно одна вторая.
Ну можно было видимо
еще сказать, давайте выберем половину ребра
какой-нибудь удобный.
Далее
далее алгоритм говорит следующее
F
значит
короче в этом графе найдем
в этом аше
вызовемся
и найдем
миностов, точнее миностов.
Далее
теперь вернемся к
значит вернемся к
Ж звездочка и скажем так
удалим из
Ж звездочка
все
F тяжелые
ребра.
И вот получим
ну допустим полученное
мы обзовем Ж
Оуф, да?
И вот
и вот
и вот
и вот
и вот
и вот
и вот
и вот
и вот
Оуф, да?
Ну что мы сделаем дальше?
Ну дальше мы скажем
что F штрих равно
рандомайст МСТ
от этого же штриха.
Ну и в конце все что там остается
что там остается
вернуть шаг 1
ладно вру
шаг 2
объединенный шагом 6
ну а я то все имею ввиду, да?
шаг 2
объединенный шаг
объединенный шаг
ну смотрите
ну по сути смотрите
мы сделали три шароборувки
в процессе этого шароборувки мы там нашли какие-то
ребра добавили в подтвер
и там жали грав
ну и теперь соответственно
в этом же звездочка
в итоге
в этом же звездочка в итоге
мы нашли минус 1
Обратите внимание алгоритм железобетонно работает
это братьте внимание. По большому счету, да, заметим, что мы просто сделали три шага
борувки, а потом в полученном графе G звездочка нашли миностов. То есть, просто вот этот шаг,
который мы делали, все что он делал, это выкинул какие-то тяжелые ребра. Сейчас. Почему? Мы выкинули
какие-то ребра из G звездой. Нет, мы не выкинули не просто какие-то ребра, а мы выкинули какие-то
тяжелые ребра. Нет, я про третий шаг. Ну. Нет, ну смотрите. Мы вызвали же randomizeMSF от H. Ага. А, окей. Ну, неважно. То есть,
глобально можно это смотреть так. 3, 4, 5. Что это значит? Мы нашли какой-то лес и выкинули из
графа тяжелое относительно этого леса ребра. Ну да. Там другой вопрос, как мы этот лес нашли,
ну как-то. Неважно. Вот. Выкинули, то есть, их точно во столе нет, и в оставшемся мы нашли
ростов. То есть, как бы чуть более сложный путь, но точно миностов ищем. Можно, в принципе, сразу даже
доказать, что на самом деле этот алгоритм не так плохо работает. Но давайте посмотрим,
за сколько этот алгоритм работает в худшем случае, как вы думаете. Ну, заметим, что мы тут все шаги
делаем за V plus E, правда? Есть такое ощущение? Ну, V plus E и рекурсия, да? Ну, вот, например. Заметим,
что в худшем случае, заметим, что после этих трех шагов рекурсии, вот здесь мы вообще потратили
какое-то время и ни черта не выкинули, да? С одной стороны тут ни черта не выкинули, с другой стороны
тут тоже ни черта не выкинули, потому что нам могло фантастически не повести, да? То есть, получается,
в этом худшем случае получается T от VE, ну, если там, ну ладно, T от NM давайте напишу. Оно равно, значит,
O от N plus M. Ну, там вот все вот эти вот шаги, в том числе вот этот. Вот этот, напомню, у нас есть какая-то
черная магия, которая говорит, что мы это за N plus M как-то делаем. Понятно, да? И дальше мы делаем
O от N plus M шагов плюс 2T от, ну, заведомо N поделить на 8 и, возможно, N. Почему N поделить на 8?
Потому что количество вершин вот в этих двух графах N поделить на 8 не более, чем N поделить на 8.
А что такое T от VE? Что ж там? Что такое T от VE? Ну, то, что мы оцениваем, что это? Ну, то же самое, что T от NM.
Ну, сколько времени работает наш алгоритм, если у нас есть N вершины и M вершины? Ну, в худшем случае, естественно, да.
Вот, получается вот такая вот штука. Ну, возникает такой. Ну, вот, можно сказать, естественно, вопрос,
за какое время, сколько у нас в худшем случае получается?
Ну, точно не больше, чем типа N plus M налог. Сколько? N plus M налог.
Ой. Откуда N налог? N plus M налог, потому что мы спускаемся.
Ну, сколько у нас спусков таких? Это типа, алгоритм. Ага, алгоритм. Ну, разве?
Так, а N? Ой, сейчас N. Там же N на 8 и 2? Ну, тут 2, а тут N длит, потому что, ну, тут-то M.
На первом шаге N plus M, на втором шаге N. Ну, то есть, да. Ну, M это будет, получается, да, 1 плюс 2 плюс 4 плюс и так далее,
плюс 2 в степени, короче, лог по основанию 8M. Ну, это сколько? Ну, да, это, на самом деле,
вот это-вот это кубический код низа, то есть, получается от M на... Да, видимо, так оценивать все-таки нельзя.
У нас проблема, что M не уменьшается вообще. Ну, вот да, но почему-то вот M не уменьшается.
Почти его оцениваешь, кажется, или вот там каждый раз мы половину ребра примерно добавляем?
Ну, мы пока рано. Крисси шаг учитывали? Что-что? Крисси шаг учитывали? Нет, единственное, конечно...
Пока нет, мы говорим, что... Так, ну, давайте, не, ну, третий шаг, не, ну, рандом-то не рандом,
вот там просто, может же, казалось бы, не повезти, но с другой стороны, тут заметим, что этот случай не так уж и везет,
потому что если мы предполагаем, что нам фантастически не везет всегда и, действительно, тут ни одно ребро не удаляется,
то заметим, что когда мы второй раз запускаем, у нас, оказывается, запуск не от M, а, ну, вот, ну, на самом деле не от M,
а от M минус N поделить на 8, то есть, на самом деле, там остается количество ребер, когда мы тяжелые удаляем,
остается просто вот N поделить на 8 минус N, правда?
Ну, потому что, заметим, чтобы у нас остался тот же граф, мы в нем нашли рекурсивно минус 100, но относительно минус 100,
а там все ребра, которые не в минус 100, ведь тяжелые, поэтому все, что там остается, это просто итоговый граф.
Поэтому, на самом деле, вот эта оценка не точна.
Да, то есть, пока, конечно, вот мы уже доказали, что алгоритм работает за стоком, но это не точно.
Это у нас не точно.
Теперь давайте подумаем, как же нам поточнее.
Надо два T, ну, не два у нас на D, кстати, а синкретность.
У нас же хуже случай, теоретически, может быть, что просто H пустой.
Да, возможно.
Что у нас?
Может быть, в смысле, что H пустой, ну, понятно, там, маленький вариант, но, возможно.
Ну тогда, действительно, давайте я так Bagichchi'у,
Т от, ну, я так captured,
значит давайте так,
Т от L звездочка, вот так не пишу.
Ну, понятно, что это такое, да?
Но сколько у нас там вершин будет?
Значит, G4, да?
gave a few thousand.
И TH5,ShU did it?
Then what?
значит g' да и значит сколько у нас будет ребер в соответственно e2'
вот так ну n2' и плюс t какое у нас еще говорит ну количество вершин
где уже то же самое n звездочка
ну вот а вот количество ребер можно здесь оценить как-то
похитрее каким образом можно их оценить я утверждаю что их можно оценить
фишка в том что заметим что там чем больше да особо так чем больше ребер осталось тем
как бы меньше ребер осталось тут потому что в лучшем случае если тут все ребра выжили то тут
все ребра убились кроме самого миностова
ну вот поэтому тут вот поэтому желательно то есть желательно каким-то мистическим
образом сказать что у нас у нас randomized msf всегда возвращает если бы всегда будет
возвращать именно но миностов в смысле дерево миностова нет но нет и граф после
удаления некоторых игр мог бы казаться несвязным поэтому он будет возвращать лес
странно потому что мы же говорили что типа но вот если же который передали связано то
вернется миностов да ну вот теперь вот просто теперь надо посчитать сколько там нашлось теперь
выскакивать вопрос сколько там в этом эфи нашлось компонент связности если вот допустим давайте так
и напишем в этом эфи нашлось какой-то нот пусть в нем будет допустим к ну допустим к там
к звездочка звездочка компонент связности вот ну и теперь утверждается что из этих вот
м звездочка звездочка видимо что-то выкинет это интересно да спорта как нам поможет
звездочка звездочка да вот хочется сказать что если к звездочку звездочка равно один
то осталось только вот заданное число неужели через звездочку звездочка нельзя оценить
в прочем это неважно а смотрите как смотрите м звездочка звездочка те ребра которые передались
уже в них было зашита к звездочку звездочка компонент связанности правда следовательно
я утверждаю что если у вас граф на вот столько количества вершин и вот на столько компонент
связанности то я утверждаю что в нем количество ребер и звездочка минус к звездочка так мы же
удаляем только плохи тяжелые ребра а из ж 3 нет это я сказал что вот из этих ребер выжили выжила
только вот столько есть еще оставшиеся но их я так оставлю их м минус м звездочка
предположим тут уже в лучшем случае предположим что все выжили
да это как которые не вошли в аж но мы предположим что в лучшем случае они они не умерли на пятом шаг
а левая а здесь заметил что вот эти ребра образуют как звездочка звездочка компонент связанности
четвертый шаг в каждой из этих компонент связанности оставил только основ то есть
количество вершин минус одно ребро следует и суммарно там осталось только вот столько
ребер где-то ну вот ну собственно в итоге он же звездочек то что нам вернули нам вернули
вот столько ребер мы к ним добавили вот эти ребра но остальные то можно точно удалить вот так
вот можно вот так сказать и вот теперь теперь нужно быть вот это звездочка
но теперь замечаем ладно что вот я теперь утверждаю что это то есть мистическое утверждение
не превосходит почему я такое утверждаю ну давайте ну давайте переходим по индукции как
всегда а от м да плюс с от м звездочка м звездочка звездочка да ну с точнее с
но логовик лога м звездочка тут один и тот же поэтому получается значит сколько тут м звездочка
звездочка м звездочка к звездочка звездочка плюс м минус м звездочка звездочка умножить
на лог м звездочка доказываем это утверждение так давайте просыпаемся ну вот значит тут шлёп шлёп
а что это мы пишем такой-то штуки меньше
не делал доза в прошлом году ну все тогда тогда тогда это у тебя должно быть уже 5 минут
позавтра автоматически в голове поедет как ты пишешь ты хочешь сказать утверждение
с левой стороны можно списать по форму кивантия рекуррента
ты можешь раскрыть тежки да
ну и вот все
а будет меньше либо равно 3 с очень короче как это ж не та что мы очень
не та цена которую мы пытаемся доказать что это не так у нас
а мы доказали cn плюс м
мы еще не то
наверное надо цена и плюс м логан дека мы немножко слагаем это вам потеряли нет
ваня мы писали одну тежку
давайте хорошо да у нас нет мы сразу мы писали тежку мы вся у них общий логерим мы и
тогда 3 м звездочки ладно тогда здесь мы пишем 3 восьмых а ну нормально
теперь вроде адекватно
ну да это нормально потому что 3 восьмые значит это меньше либо равно обычного м поэтому все
мы же хотели за чем что делать что делать сейчас мы оцениваем
же выfr
в случае это работает а синтетически также как бор同 чтобы не случилось Hawaii
еще больше скажу как лучший случай это работаетester Qué
хуже чем прим. По крайней мере, классический прим за n квадрат. Ну, во-первых, да, заметим, что мы
избавимся от кратных ребер, поэтому там в каждый момент времени у нас n не происходит n квадрат,
да. Поэтому там все шаги делаются за n квадрат, и дальше там у нас есть синтетика, получается, да,
t от n равно o от n квадрат плюс 2t от n делить на 8. Ну, заметим, что, ну, отсюда там по любой
время или по любому развороту рекурсивному, вы видите, что это t от n квадрата. Метод акробазии.
И я по словам вообще не могу, а то у меня акробазия. Ну, там, может, всякие кубрини снимается.
Акробазия, господи. Акробазия, господи. Акробазия, господи. Да, там все что угодно, можно акробазия,
можно там... Я как бы либо аквабати, либо акробатишка. Ну да, акробатишка, это да.
Да. Мачо-малли тут. Конечно, выходит команда финал КВН, да. Выскакивает такой, действительно такое
священник такой в черном там, делает пару круглых вопросов. Да, был не мой вопрос, кто это был, да, это
акробатишка. Не, не то чтобы команда с этого что-то выиграла, но это запомнили. Ладно. Так, короче,
очень видим, что у нас получился крутой алгоритм, который работает не хуже примы, не хуже баруфта
одновременно. Теперь финальное, что нам надо доказать, это то, что это работает, что в смысле
от ожидания это работает круто. Ну, давайте разбираться почему. Сейчас тут будет немножко магии.
Смотрите. Значит, у нас есть теперь, так, вот было у нас вот это вот это в худшем случае, да, а теперь давайте
у нас есть мистическая случайная величина. Ну ладно, давайте всегда обозначим букву изю.
Значит, изю от ве. Это случайная величина типа с временем работы, да. Чему она равна? Она равна
о от в плюс е, да, плюс что? Плюс, ну поехали. Значит, кси от чего-то. Значит, в звездочка и
вот этого два звездочка. Причем обратите внимание, ну в звездочка допустим это детерминировано,
определяется, а вот е2 звездочки это случайный какой-то набор, помните, да? Вот. Плюс, что там еще? Ну плюс
нот. Ну плюс, соответственно, кси от чего-то. Ну, короче, в звездочка и е штрих можно сказать, да.
Согласны? Ну, теперь давайте думать, как связаны между собой е звездочка звездочка и е штрих.
Е штрих это вот. Ну, е штрих это то, что вот в итоге получилось. Вот, значит, будем теперь,
значит, доказывать, ладно, давайте сейчас, да, для простой опустки попытки увидеть и будем
просто доказывать, что, значит, мистическое утверждение. Ясно, по е штриху. Наверное, там е штрих.
Вот, утверждение. Я хочу доказывать по индукции, что е си от v и е не превосходит
с умножить на модуль v плюс модуль е. Вот, что я хочу заявить. Так, ну ладно, я надеюсь, вот
коллизия между вот этим е и вот этим е никого не смущает. Если смущает, эту букву m могу написать.
Ну, понятно, да. Так вот. Значит, доказывать мы это будем, ну, скажем, по индукции по v плюс е.
Логично, да? По размеру. Ну, заметим, что у нас тут как бы размер это строго меньше,
потому что у везет их и размер не более чем v поделить на 8, правда? Так, что получается. Значит,
теперь пишем, что е си от v и е равно, ну, значит, мы по-любому выполняем, значит, не превосходит
а на модуль v плюс модуль е, плюс мат ожидания вот этого вот, правда? Ну, давайте я вот аккуратно
пока напишу. Мат ожидания си, тут как бы подлиннко так покажется, да? Потому что пока слетку как бы то,
что там вот хочется сделать, наверное, нельзя делать или, по крайней мере, я не уверен, что
можно, но давайте пока. То есть, просто проблема в том, что если бы у нас эти графы были фиксированы,
то я мог бы написать там сразу какое-нибудь предположение индукции, да? Ну, вот. Но пока
заметим, что как бы е звездочка звездочка и е штрих это тоже случайные величины, да?
Вот. Но с другой стороны мы заметим, ну, с другой стороны заметим следующее, что мы пытаемся по индукции
доказать линейность, да? То есть, заметим, что вот эта величина, да, значит, понятно, что вот е звездочка
звездочка, вот чему равен модуль е звездочка звездочка, да? Ну, как мы уже говорили, с вероятностью
p1 он там равен, там, допустим, z1, с вероятностью p2 он равен z2 и так далее, с вероятностью pk, там,
pl он равен zl, да? Понимаете, да? Но тогда, смотрите, если он равен z1, то тогда мы здесь должны
там от ожидания написать непревосходящее c на в звездочка, там, модуль в звездочка плюс z1, да?
И это должны нам дожить на p1. Классно? По-любому, да, что если вы вспоминаете, что тут оказалось
размер z1, то тут от ожидания, там, граф мог быть разный, там, но от ожидания по-любому не будет
превосходить вот этого, да? И тот, да, модуль в звездочка плюс z2 на p2, ну и так далее, да? Вот. То есть, отсюда
начнем короче из этого и здесь следует, так, следует, что этот маркер мы тоже выбрасываем. Не, знаете,
полезно записать, просто какие маркеры выбрасывать, что, как бы, слишком маркеры на тому не ходят. И нужно
понимать, если маркеров состава слишком мало, ну значит, значит, пришло время пойти в книжный магазин
и купить новые, да? Или там, я не знаю. Так, а какие у нас маркеры еще вообще есть? Так. А то надо вспомнить,
что это... Так, дальше. Это меньше либо равно. То есть, меньше либо равно получается. То есть, если это
посуммировать, да, то значит, это меньше либо равно a на модуль v плюс e плюс c на, значит, модуль v штрих плюс...
Ой. Такое, да? Ну, может, модуль, ну точно. Вот так нормально. А, так нормально, да? Он там и должен был быть.
Ну, конечно. Плюс дальше c от модуль v звездочка плюс e, значит, модуль e штриха. Ну, начало неплохое.
Потому что, ну-ка, скажите мне, каково мы от ожидания e звездочки и звездочки? Ну, не больше, чем e пополам, как бы.
Так, я могу даже начать оценивать сверху. a на модуль v плюс модуль e плюс c на модуль v поделить на 8 плюс модуль e пополам.
Значит, плюс, плюс c на модуль v звездочка плюс, а вот, ой, потому что я затупил. Так, поэтому, вот, значит, плюс
мат ожидания этого мистического e штриха. Потому что про него мы так пока ничего не узнали. Вот спрашивает, что делать?
Каково мат ожидания e штриха? Ну, хотелось бы получить, чтобы это было, ну, не больше, чем модуль e, на константу меньше 1 и 2.
Ну, что-нибудь по мелочи. Ну, а, значит, а давайте внимательно подумаем. То есть, смотрите, то есть f у нас, то есть, видите, как это устроено так, что, то есть, надо подумать,
с какой вероятностью каждое конкретное ребро выживет после вот всей вот этой вот махинации?
Ну, либо сразу, либо оно не попадет с первого галфа. Ну, скажем так.
Так, если оно в звездочку не выпало, то оно уже все, да, оно уже никуда не попадет.
Мы смотрим ребра из звездочки, которые могут быть чуть-чуть не все, да?
Вот. Так. Ну, как, в каком случае ребро исчезает? Когда вот этот получен в h, оно, сейчас, если оно выпало в h, то оно почти точно исчезнет, если он не является там ребром именно компонента святости, ну, и ребром именно ms.
Совершенно верно. А таких мало, как бы. Ну, таких не мало. Таких у нас не более, таких у нас, то есть, скажем так, ребра бывают двух типов.
Ребра бывают те, которые попали в h и которые туда не попали, правда? А теперь давайте думать, что если ребро попало в h, то, в общем-то, оно либо попало в этот мин-лес от h, либо оно вылетело из ozg.
Логично, да? Да. Вот. Значит, соответственно, таких ребер у нас окажется, ну, там, допустим, можно оценить, как модуль v минус х звездочка, звездочка, допустим, да?
Ну, с другой стороны, да, даже не модуль v, а модуль v поделить на 8 минус. А мы можем просто такой же стукан написать, который мы делали в интерменинном случае?
Ну, давайте так, а давайте попробуем даже. Что-то дотянет меня еще где-то на то, а давайте сделаем не так, как там написано в талмуде, но вот. Ну, если как вы могли заметить, это вообще, собственно, это вообще наш стиль, да.
Зачем делать как в талмуде, давайте придумаем сами. Вот. Значит, давайте скажем, что e' это равно e' называется живые.
Ладно, давайте так.
Игра жизни. Нет. e' h объединенная с e' не h.
Понятно, да?
Ну, давайте, значит, давайте попробуем, да, давайте вот тут переписываем вот эту хрень, да, плюс c, там, модуль v поделить на 8, плюс модуль e пополам, ладно, что-то мне в бом уже, поэтому плюс модуль v поделить на 8, плюс, значит, мат ожидания e' h, плюс мат ожидания e' h.
Значит, e' не h, так. Чем это нам помогает?
Так, вот это вот равно, ну, вот эта величина равна у нас к чему?
Модуль v поделить на 8, минус, минус количество компонентов связанности v, да?
Вот, это вот меньше либо равно вот этого, а это меньше либо равно чего?
Не можно просто посчитать.
Сколько, да, да, сколько, сколько, сколько river из, значит, v не h.
В магазине можно посчитать на подождание, да, сколько river из e' и, ну, e' h, типа оценка, например, сверху, да, да.
Ну, с чего-то непонятно, но вопрос, сколько там компонентов связанности от этого, может зависеть.
Ну, может быть, нам хватит написать, что их не более чем v поделить на 8?
Ну, в e' из множества h, в e' количество river, я вам даже просто скажу, ровно вот столько.
Это даже не средний, это просто вот так.
Потому что все ребра, которые попали в v, они умерли, все, кроме тех, кто там в минус 100 и остался.
Или в минус 100 и остался.
Ну, у нас v попадает каждое второе ребро в среднем из 6 звездой.
Ну, вот.
Получается, что одна вторая, ну, как бы, может быть, одно второе e.
Не очень хорошо.
Или одно второе e со звездой, что-то.
Ну, смотрите, да.
Ну, вот, нет, ну, сейчас, погнали.
А, одно, сейчас.
Ну, нет, в e' очень хочется сказать, что там,
нет, давайте так, вот, e', которые не h.
Там количество river,
ну, вот, то есть, там,
то есть, количество river, которые выше,
там,
то есть, есть river, которые не h.
То есть, сколько легких river там выжило.
Ну, очевидно, выжили, ну, вот.
Есть подозрение, что, как бы,
вы все.
Ну, вот.
Ну, да.
Ну, кроме тех, кто не попал в одну компоненту.
Ну, в худшем случае, допустим, все.
И все это у нас получается сколько?
Модуль e.
Ну, допустим,
модуль e' да?
Или сколько там их было?
Из e' мы оценили как модуль e.
Ну, ладно, да.
Ну, допустим,
здесь можно сказать просто как
модуль e- модуль e' h.
Да, можно это оценивать как вот так.
То есть, только, да, звездочка,
именно они ж три.
То есть, короче,
переводчик говоря,
сколько ребер не попало в h, да?
Ну, мы считаем, что ни одно не вылезало.
Но, заметим, что мата ожидания количества ребер,
которые не попало в h,
оно на сколько? Ровно e' пополам, правда?
То есть, они уже... Ничего-то не сходится.
Почему? Потому что у нас при e будет коэффициент c+.
Ну, да.
Ой.
На.
То есть, совсем e пополам, да.
То есть, совсем так жестко оценивать, видимо, не получится.
Придется, видимо, делать что-то чуть-чуть более умное.
Либо как-то, значит,
все это опять начать сделать так, чтобы
это зависело от кораж. Ну, не получится.
Поэтому придется делать что-то более умное.
Поэтому давайте
себе этот случайный процесс
попадания в g'
мы
проимулируем просто другим способом.
Смотрите. Ну, как мы делаем?
Мы выбираем случайные ребра.
Мы выбираем случайные ребра
на тех, кто выбрался, строим
лес, а потом
возвращаем все ребра и
тяжелые ребра выкидываем, правда?
Тогда идея такая. А давайте сделаем
немножко наоборот. Значит, генерить
этот абсолютно тот же самый процесс
будем следующим образом.
Возьмем все ребра, отсортироваем
их мысленно, конечно,
по весу.
Допустим, у нас все эти ребра
отсортировались по весу.
Я сейчас буду одновременно
кидать монетку и строить
ребра, которые к нам попали,
будем строить на них
соответственно,
миностоп, алгоритм краскала.
Понятно, да?
То есть, смотрите.
Значит,
на каждом шаме будем делать так.
Ну, у нас в каждом момент
времени есть какой-то лес,
уже набранный из выбранных ребр.
Поэтому говорим, если ребро
еще не...
Значит, если ребро тяжелое,
то мы его сразу можем выкидывать.
Логично, да?
Ну, если ребро
уже тяжелое, то нам
на барабану мы либо сейчас его выкинем,
либо потом на какой-то изотерации мы его точно
уже выкинем. Правда?
То есть, можно на него монетку не кидать.
Это мы сейчас сделаем третий шаг?
Это мы делаем третий, четвертый, пятый шаг
все там, все вместе.
То есть, берем все ребра,
вот это вот,
то есть, это вот
е-звездочка. Смотрите, е-звездочка.
Смотрите, это е-звездочка.
Я отсортировал все ребра е-звездочка
по возрастанию весов.
Моя задача
сгенерить вот этот вот
краф-ж-штрих.
Ну, то есть, понятно, как бы мы должны были,
по идее, сгенерить там
взять какие-то половину,
взять, то есть, снять
до вторая ребра, из них сгенерить
мин-лес и убрать все тяжелые ребра
относительно этого леса, да?
Так вот, я буду делать это...
Я буду генерить
ребра и брать лес
следующим читерским образом, смотрите.
Я монетку теперь буду кидать не всегда.
То есть, я буду говорить так.
Я вот кидаю монетку.
Вот если у меня
монетка выпала,
ну вот, ну у меня бывает
монетка выпала, а бывает монетка не выпала.
Но я буду кидать монетку
только в том случае, если это ребро можно
добавить в мино 100.
Ну, то есть, если ребро не попадает еще,
то есть, соединяет
в этом лесе две компоненты
связности.
Мы его добавляем.
Вот, следующий ребро
тоже можно добавить, но я
кидаю монетку.
И я его не добавляю.
Вот.
А следующий, допустим, опять монетка выпала,
я его добавляю.
Еще тут монетка выпала, добавляю.
У следующего ребра монетка не выпала.
А у следующего ребра вообще произошла ситуация,
что оно уже тяжелое,
потому что произошла ситуация, что вот это
ребро на самом деле, то есть уже вот этот вот миностов на самом деле стягивает цикл и так как
ребра отсортированы в порядке возрастания, значит оно уже тяжелое. Это означает, что монетку на него
можно не кидать, оно будет так или иначе в тот или иной момент удалено полюбасу, поэтому монету я
даже не кидаю. Понятно, да? То есть я как бы еще раз кидаю монетку только тогда, когда это ребро может
быть добавлено в минлес. Если уже не может, я просто это ребро выкидываю, а мне оно неинтересно. И по сути,
случайная величина, которой меня интересует, это сколько раз я кину монетку, правда?
То есть типа мы говорим, что вот мы добавляем каждое ребро на вторая и только теперь мы это делаем по
очереди и параллельно находим по сути как раз МСФ в получающемся H. Ну типа да. А нам важно,
ну да, сколько подфига. Мы это явно будем делать? Нет, это мы по-другому определили ту же самую
случайную величину. Ну да, может быть, да, ну не совсем, ну скажем так, мы просто, ну как,
ну скажем так, да, ну можно сказать, что мы определили ту же самую случайную величину? Ну,
скажем так, ну вот, ну потому что мы как бы, ну как бы формально говоря, типа там мы кидали
монетку для вот этого ребра, а тут не кидаем. Ну окей, с тем же, но можно заметить, что как бы,
скажем так, нас интересует такая случайная величина, это количество ребер, которые выжили, да?
Я утверждаю, что количество ребер, которые выжили, это количество раз,
которые мы здесь кинули монетку. Понятно, да?
Не понятно, там же вот красное выпало. Когда оно выпало, мы даже монетку не кидаем,
потому что это по-любому тяжелое ребро относительно найденного нами веса.
Нет, когда мы красную монетку кинули, мы говорили, что мы могли бы добавить это ребро в
минлес, но мы кинули монетку и по результатам кидал, мы решили ее не добавлять. Да, типа ваш не попало.
А, и тогда в итоговом графе все те, что останутся, все, до которых мы кидали монетку.
Ну не более, ладно. На самом деле сформулируем так. На самом деле так,
может быть на пятом шаге это ребро, кстати, удалится? Может и нет, я не знаю. Может удалиться,
но это как бы уже оценка сверху. Разве делаем третий-пятый шаг или третий-шестой? Третий-пятый.
Сейчас, а почему удалится? Мы же таким образом точно получим МСФ в полученном графе.
Пока не проверим, мы шестой шаг пока не трогаем. Мы третий-четвертый-пятый.
Я говорю, что, в смысле, вот те ребра, на которых монетка выпало, они будут обязательно образовывать
МСФ, граф из как бы... Да, после четвертого шага как бы F будет содержать ровно вот эти все
голубые ребра, на которых голубая монетка выпала. Да. Ну вот, на пятом шаге мы удалим, то есть голубые
ребра останутся, мы удалим вот эти все ребра с крестиками, они по-любому будут удалены, а также
будут удалены некоторые ребра из красных монет, с красными монетками. Да. Вот. Поэтому,
значит, давайте сделаем, поэтому, значит, давайте я сделаю вот как так, значит, это я все,
значит, поэтому давайте я скажу, что про E штрих, да? Погодите спать, тут недолго осталось. Так,
значит, С, тут получается модуль, значит, смотрите, я так запишу, E пополам, плюс С на V поделить на 8,
плюс, значит, пока мат ожидания, плюс E от вот этого вот количества монеток, которые выпали. Вот это вот,
я вот так нарисую. То есть возникает вопрос, сколько раз мы кидаем монетку? То есть вот у нас
есть такая красно-синяя монетка. Нужно для каждого ребра почитать, ну, почитать вероятность, как-то оценить.
Не-не-не, все еще круче. Смотрите, у нас устроен процесс так, во-первых, мы просто кидаем монетку
несколько раз, да? Она выпадает либо синяя, либо красная. А теперь интересный факт. Если синяя монетка
выпала там N-1 раз, то больше мы монетку кидать не будем. Ну, просто потому что граф уже там,
все, а стоп уже связал весь граф, все, дальше кидать не будет. Это сколько раз мы кинули эти
монетки, сумасшедшие? То есть обратите внимание, на самом деле это называется, по-моему, отрицательное
бинобиальное распределение. Ну, то есть, представьте себе, ну там, не помню, там вот, по-моему, есть
какое-то, то есть вот утверждать, что есть такое Nb от Np. То есть что это значит? Мы кидаем монетку,
она выпадает с вероятностью p и не выпадает с вероятностью 1 делить на p. Когда она N-ый раз выпадет,
мы останавливаемся. Случайная величина называется, а сколько раз мы монетку кинем, пока это
произойдет. Это обратное бинобиальное распределение. Ну, ладно, ну, я не силен в технологии, да,
наверное отрицательный. Да, может это обратное бинобиальное, может это, я не знаю. Я на экзамене
спутал экспоненциальное распределение с плацлоновским.
Лямда на е в степени минус лямда х непрививная. А у вас его нет? Да, это геометрическое. А это еще и геометрическое? Да, точно. Ну, ладно, не важно.
Так вот, короче говоря, ладно, назовем вот, то есть, короче говоря, это будет меньше либо равно,
чем a на v плюс e. Переписываем. Плюс c, давайте я так напишу, модуль v поделить на 4, плюс модуль
e пополам и плюс c нам от ожидания, допустим, назовем его nb от n минус 1 и 1 вторая. То есть,
тоже не превосходит, да? Потому что мы как бы можем бросать не такое количество раз, а меньше.
В другом месте вообще ничего. Чего-чего? Нб от n минус 1 и 1 вторая.
Ну, как бы да. Вот, n минус 1, 1 вторая. Ну, давайте теперь посчитаем, nb от np. Ну, от ожидания
его я утверждаю очевидно, что оно равно n на значит, значит, от ожидания nb от 1p, да? Да. А это
типа 1. А 1, значит, то есть, ситуация. Кидаем монетку, пока она не выпадет. Она выпадается
на каждый шаг, с вероятностью p. Сколько отскейдем раз мы ее тянем? 1 делит на p кажется. Ну,
давайте это просто посчитаем. То есть, если этот znb, если это равно x, то x равно, соответственно,
ну, с вероятостью p, 1, плюс 1 делить, 1 минус p, получается 1 плюс x. Или что-то
же самое, 1 плюс 1 минус p на x. В общем, короче, x равен 1 делить на p, да. Тоже у вас должно
было быть упражнение на эту тему в домашних заданиях. Ну, вот. Ну, тогда это равно получается.
Тогда, смотрите, это меньше, тогда все это я убираю. Значит, это меньше либо равно, чем, так, тут
а на, так, это тоже погибает смертельно. Так у нас там не n, nb не от n, плюс 1 от 0. Ну, конечно,
модуль v поделить на 8 минус 1, одна вторая, хорошо. Значит, плюс c от модуль v поделить на 4, плюс e пополам,
плюс c на модуль v поделить на 8, умножить на, видимо, 2. Ну, 1 поделить на одну вторую, видимо,
равно 2, да. Так, ну, поздравляю, вы победили. Давайте уж красненьким допишу, что это равно a на
модуль v плюс модуль e, плюс c на, вот, получается, на модуль v пополам, плюс c на модуль e пополам.
Ну, и все это меньше либо равно c на модуль v плюс модуль e, ну, при c, соответственно,
больше либо равно, ну, я не знаю, а, например, все. Так, ну, вроде, репку вытянули, да, или не вытянули.
Ну, равенна. Ну, вы... Минус 1, запятая, одна вторая. Ну, одна вторая, потому что манетки выпадают,
сразу все, одна вторая. Количество вершин там, количество раз, которые синие могут выпустить,
это модуль v на 8 минус 1. Так что, ура! Вот таким не сильно сложным, пока не по коду,
образом алгоритм работает за ОАПХЕ. Так что, будете на лейпяде писать мизостов, пишите его вот так.
Чего? На любой. Ну, как бы, ну, в лучшем случае он будет работать как борувка. А в среднем
он будет работать в задней линии. Ну, это реально? Он быстрее, чем борувка? Не знаю, не пробовал.
Ну, быстрее, чем борувка, в среднем, наверное, точно. Не, ну, там, смотрите, быстрее, чем борувка,
это вопрос. Потому что дело в том, что на рандомном графе борувка будет не деломо работать,
он, скорее, за там более быстрое количество шагов займет. Но, правда, тут тоже вы делаете
три шага борувки, и как бы, как бы, есть вообще шанс, что за три шага борувки, как бы, граф,
критику скопится. Нет, тут проблем написать в том, что мы проверяем. А, да, да, да, пункт 5 есть, да.
Ну, пункт 5 есть. Чё, чё ждёте? Ну, может, его займём, там вот Сашка. Ладно, не пишите,
это толкает. А, жаль. Может, там, хотя да, там писать за линию, это умелость. Смал, там,
С, У, В на 8, Н на 2, да? Чего? С, У, В на 8, Н на 2. Ну, да. Ну, один влит на одну вторую.
Чего? Ну, это алгоритм. Чего алгоритм? Написал ли его кто-нибудь? Ну, кто-нибудь, наверное,
написал. Не, ну, как сказать, там, как бы, есть интрига на то, что, как бы, ссылка на статью,
ссылка на статью, как бы, даёт всего лишь 10 страниц. Лан 12. И там статья называется,
как говорится, даже, там, типа, более простой алгоритм проверки минимальности астола.
Так что здесь статья, то, что там, вот, то, что мы с вами придумали, придумывать не надо.
То есть, там, вот, там, какой-нибудь, там, как бы, адекватные реальности алгоритм такой, может быть,
а не как обычно. Вот это можно сказать. Ну, мы пока не знаем. Как бы, да и будет силы,
может, ещё раз узнаем. А может, нет? А может, нет. Так что вот, так, ну, у нас всё время вышло.

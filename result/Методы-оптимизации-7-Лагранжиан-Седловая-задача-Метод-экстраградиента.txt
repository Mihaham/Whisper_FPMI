давайте потихоньку начинать соответственно продолжаем с вами разговор про задачи
оптимизации теперь уже с ограничениями вот и в некотором смысле на семинарах вы уже начали
осваивать один из аппаратов как сможет с помощью них мы с ним можно взаимодействовать вот это
барат соответственно функция лагранжа ну и соответственно сегодня посмотрим что с этими
функциями лагранжа можно делать на семинарах в принципе что смотрели вы как раз рассматривали
какую-то такую задачку целевая функция здесь я пишу множество что оно не ограничено ну например
сюда запиханы функциональные ограничения вот причем в не единственном числе типа неравенства типа
неравенства вот на самом деле вот то что написано здесь можно было бы обобщить на
случае когда у вас действует какое-то множество и чаще всего это какое-то простое множество вот ну
и когда у вас соответственно носитель функ в том числе их нулевое вот это не все
пространство и рд наш носитель там где у нас функция неравна плюс бесконечности вот в
том числе нам это будет нам это понадобится вот но здесь соответственно предполагаю что у нас
x это рд и носители всех функций тоже рд все пространство чтобы дополнительно не
усложнять хотя это всего лишь небольшая такая на строчке к теории вот напоминаю что вы
соответственно взаимодействовали с функцией лагранжа вот такого вида у вас еще были ограничения
типа они равенств здесь я пока оставляю только неравенство вот вот такого вида у вас лагранжан
был ну и соответственно вот эти альфы они были у вас положить ну и вводилась у вас скорее всего
без положительных но потом уже говорилось когда мы там например считаем двойственную функцию эти
альфы должны быть положить вот что вы соответственно делали на семинаре рассматривали лагранжан рассматривали
соответственно двойственную функцию вот которая соответственно считается как инфимум от лагранжана
инфимум по x от лагранжана и получается что у вас двойственная функция зависит только от параметра лямпы
когда вы возьмете инфимум вот соответственно да вы рассматривали какие-то задачи как двойственную
функцию строить действительно иногда двойственная задача будет легче чем исходная задача минимизации
с ограничениями вот но это как понимаете не по нацель
также вы осознали тоже базовый факт в том числе есть пособие что у вас значение ваша двойственная
функция в любой любого параметра лямпы меньше либо равно чем значение функции f со звездой вот это с
одной стороны хороший факт но и с другой стороны плохой просто хотелось бы здесь получить строго
равенства вот чтобы хотя бы для какого-то лямба хотя бы для какого-то лямба имеет здесь
трогая равенство вот и у меня к вам соответственно возникает вопрос а можем ли мы когда-нибудь
гарантировать здесь строго равен и когда мы можем с вами гарантировать строго равенства когда у нас
здесь стоит двойственная задача двойственная функция и исходная
помню что там должно выполняться кому может это рассказывали может кому-то это не рассказывать
что-то обсудим сейчас было условия слайдера все правильно то есть у вас было должно было быть
было не страшно оно здесь принято приведено условия слайдера оно выглядит следующим образом
будем говорить что для задач с ограничениями выполняется условия слайдера если соответственно
вот это не равенство которое у нас были изначально в нашей системе системе ограничений вот они не
просто выполняются не строго вот они должны для какого-то x выполняться строго ну соответственно
тогда выполним условия слейтера. Ну и факт, который мы не будем доказывать, так много сегодня
нужно будет чего посмотреть, это то, что в случае условия слейтера у нас можно как раз гарантировать,
что supREM по двойственной функции будет давать нам в точности минимум нашей исходной задачки.
И тогда уже действительно имеет смысл рассматривать двойственную задачу,
потому что минимум двойственной совпадает с минимумом исходной задачки на нашем множестве
ограничений. Поэтому, соответственно, решать имеет смысл. Иначе у вас просто появляется
предельный зазор двойственности, который не устраним, и, соответственно, непонятно,
как тогда его вообще находить, и будет ли тогда исходимость. То есть исходимость будет какой-то
констант, если мы будем посмотреть исходимость по этому зазору. Мы обсудим, посмотрим этот зазор еще
в примерах, и я тогда уже уточню, где там будет возникать, когда у нас не выполняется условия слейтера.
Сегодня, соответственно, посмотрим на это все безобразие. На семинарах, скорее всего, кто-то уже
начал, кто-то, соответственно, начнет смотреть на это недели, на задачи Лагранжан через условия
корушек у натакера тоже в фундаментальном теореме оптимизации. В некотором смысле аналог, обобщенный
такой аналог того, что мы с вами записывали для задач без ограничений, задач с ограничениями.
Вот. А мы посмотрим чуть по-другому. Посмотрим чуть по-другому через поиск щедловой точки,
поиск щедловой точки. Вот. Давайте вообще изначально введем определение.
Дополнитель-множитель выскакивает. Так. У нас точка х-звездой, лямбда-с-звездой называют
щедловой для функций Лагранжа. Вот. Если, соответственно, для любых х, здесь просто х без звездочки,
и лямбда выполняется следующим. Ну, то есть, что означает? У нас есть какое-то значение
нашей функции Лагранжа в точке х-звездой, лямбда-с-звездой. Вот. Если мы, соответственно,
пытаемся как-то менять лямбда, то при фиксированном х-звездой, то у нас значение функции только
уменьшается. Вот. А при этом, если мы то же самое меняем х при фиксированном лямбда-с-звездой,
то у нас значение функции увеличивается. Ну, это, соответственно, определение щедловой точки. Вот.
Пока просто определение, как это связано с какими-то задачами, мы сейчас будем уже и понимать. Вот.
Справедливо, следующая теорема. Теорема куна-такера. Это не та теорема куна-такера,
с которой вы будете знакомы на семинаре у вас. На семинаре будут необходимые условия,
необходимые достаточно условия куна-такера. Здесь теорема куна-такера о щедловой точке. Вот.
Пусть, соответственно, мы рассматриваем нашу задачу с ограничениями f0 и, соответственно, f' меньше
0. Меньше либо равно 0. Вот. И, соответственно, что говорим, что эти все функции f0 и f' выпуклые.
И, соответственно, для ограничений выполнено условие слайтера. Вот. Тогда следующие условия,
которые написаны здесь в SQL. Вот. Соответственно, если у функции Lagrange существует седловая точка,
то оказывается, что как раз x, возникающий в этой седловой точке, является и глобальным решением
задачи оптимизации с ограничениями. Задача с оптимизацией с ограничениями. Вот. То есть,
получается, седловая точка Lagrange, это и есть решение нашей исходной задачи, ну и решение двойственной
задачи. Оно как раз в этой седловой точке. И утверждение в обе стороны. То есть, если у вас x
со звездой, это глобальное решение задачи оптимизации с ограничениями. Ну, можно добавить, что λ со звездой
это решение, соответственно, двойственной задачи. То тогда x со звездой, λ со звездой это просто
седловая точка функции Lagrange. Вот. Окей. Давайте это попробуем подоказывать. Так как это у нас
критерии в обе стороны работает, то давайте сначала докажем в одну сторону. Пусть у нас,
соответственно, есть x со звездой и λ со звездой, что вот эта пара x со звездой и лямбда со звездой
является седловой точкой функции Lagrange. Вот. Тогда докажем, что у нас x со звездой это глобальное
решение задачи с ограничениями. Глобальное решение задачи с ограничениями. Во-первых, давайте проверим,
то есть вообще то, что у нас x со звездой это принадлежит какой-то, ну, является частью
седловой точки вместе с каким-то лямбда со звездой вообще нам не говорит о том, что x со звездой это он
вообще удовлетворяет ограничением. То есть первое, что мы будем проверять, это удовлетворяет
x со звездой ограничением, иначе, если он не удовлетворяет, то по факту он не лежит во множестве
ограничений, а значит не является решением на наше множество, которое мы этими ограничениями задали.
Вот. Окей. Давайте подумаем, что будет происходить, если x со звездой, если x со звездой не удовлетворяет,
пусть у нас x со звездой не удовлетворяет какому-то ограничению f и, то есть существует и такое,
что f и x со звездой не удовлетворяет ограничениям. А это что значит?
Больше или в пару? Равно 0? Больше, просто строго больше, потому что ограничения у нас были меньше
либо равно. Поэтому здесь будет просто строго больше. Хорошо, это мы поняли. Соответственно,
значение f от точки x со звездой больше равно 0. Тогда, что мы можем сказать про нашу функцию
Lagrange для такого вот x со звездой и, например, для λ со звездой? Функцию Lagrange, я напоминаю,
вот так будет у нас выглядеть. Лямбда и x со звездой, f и x со звездой. И от 1 до m. Хорошо. Так,
что мы можем сказать про нашу функцию Lagrange? Что мы можем сказать про нашу функцию Lagrange?
Давайте я даже лямбду разнорожу. И у нас лямбды, мы знаем, что лямбда и, они больше либо равны 0.
Что я могу сказать про поведение вот этой функции при меняющемся лямбда? Особое внимание, конечно,
нужно обратить на f и t. Как это ведет в себя функция, если я, например, начну варьировать лямбда и?
Лямбда и и варьируем при вот этом условии. Что будет происходить, например, я начну увеличивать лямбда и?
Lagrange будет расти. Lagrange будет расти, потому что у нас ограничение f и t со звездой больше 0.
Соответственно, когда я домножаю на множитель Lagrange, это тоже больше 0. И, соответственно,
когда я лямбда и начинаю вырастать, вот этот множитель будет увеличиваться. То есть,
зафиксировав все лямбды, кроме лямбда и и, я могу устремить Lagrange к бесконечности. То есть,
я запишу, что с Упрэмом у меня Lagrange по лямбда больше либо равны 0. Это будет плюс бесконечности.
Вот, окей, это хорошо. С одной стороны, мы это поняли, но с другой стороны,
мы знаем, что у нас справедливо вот такое. По определению седловой точки просто.
Для любого лямбда, любого лямбда больше либо равна 0. Ну что, что мы можем тогда сказать?
Тогда это и для Супрэмома выполняется?
Для Супрэмома? Да нет, смотрите, на самом деле мы уже тут уже пришли к противоречию. Потому что,
смотрите, у меня для какого-то лямбда со звездой вот это выражение для любого лямбда меньше.
Вот, ну смотрите, я могу взять лямбда в качестве лямбда, лямбда со звездой, но в нём внутри поменять
лямбда и со звездой, поставить равным 2 лямбда и со звездой. Просто увеличить соответственно
функцию Lagrange за счёт того, что мы вот это свойство с вами поняли. Ну и получается,
что вот такой вот манипуляцией я построил какую-то новую лямбда с тильдой, лямбда с тильдой,
которая на самом деле будет строго больше, чем вот то, что у меня было до этого. Согласны?
Вот. А получается противоречие, потому что мы говорим, что для любого лямбда,
который не равно, либо равной лямбда со звездой, должно быть меньше, либо равно, а тут знак строго
больше. Всё, противоречие. Получается, что у нас х со звездой удовлетворяет всем ограничение.
Вот, взяли это. Так. Всё, с этим закончили. То, что у нас х лежит в ограничении. Дальше, соответственно,
что? Заметим вот такой вот занимательный факт, что в точке х со звездой, если мы возьмём с управом
от нашей функции Lagrange, то это просто значение. Значение. Значение. Если мы возьмём с управом от
Lagrange, допишу это. Lagrange, это просто f0 от х со звездой. Вот. Плюс ограничение лямбда g, f и х со звездой.
Тогда у нас что получается? Что вот чему равен этот Supremo? Почему он равен этот х со звездой?
Почему я здесь это написал? Кто понимает, почему так? Что я могу сказать? Я уже знаю, что х со звездой
меня удовлетворяет ограничение. Так. Тогда вот это получается все меньше, либо равны нуля. Так. Вот.
Ну и лучший вариант, что я могу сделать вот с этим, с помощью лямбды, которые больше либо равны нуля,
это занулить. Согласны? Если мы говорим про Supremo. Вот. Поэтому действительно с
управом это у нас значение f0 в точке х со звездой. f0 в точке х со звездой. Окей. Это мы с вами поняли. Так.
Хорошо. Хорошо. При этом что мы ещё знаем с точки зрения седловой задачи? С точки зрения седловой
задачи мы знаем, что у нас выполнено вот такое. Вот такое у нас выполнено. Вот. Ну так как мы знаем,
что вот это просто Supremo. Вот. По всем лямбда. По всем лямбда. Вот. Ну а значит что этот Supremo
просто достигается на лямбда со звездой. Лямбда со звездой даёт Supremo. Вот. Для какого-то лямбда со звездой
все значения меньше. Значение функции Lagrange при варированном лямбда меньше. Вот. Соответственно да.
И это у меня получается Supremo. Supremo по лямбда больше либо равны нуля. А значит это просто f0 со звездой.
Ну мы это поняли. Вот. Хорошо. Хорошо. Мы поняли, что у нас f0 со звездой от Supremo. Так. Дальше. Дальше мне осталось
воспользоваться первым. Теперь первым кусочком определения функции Lagrange. Вот здесь соответственно
записана функция Lagrange.x лямбда со звездой. Вот. И почему же я здесь соответственно так пишу?
Почему я здесь так пишу? Почему я вот здесь поставил знак больше либо равно? Почему я здесь поставил знак
больше либо равно? Потому что это определение с игловой задачи. Ну что нам оно даст? Что нам оно даст?
Здесь это просто определение с игловой задачи. Вот. Ну что оно нам может дать? Давайте подумаем. То есть нам
же хочется доказать, что у нас f от x со звездой это глобальный минимум задач с ограничением. Вот.
А что это значит? Давайте напомните мне, что такое глобальный минимум. Быстренько. Быстренько.
Точка такая, что для всех остальных точек из нашего множества f от x меньше равно, чем f от этих точек.
Сейчас ответственник отвечу. Да. Это правда. Для любого x из нашего множества x, вот это множество
задается ограничениями. У нас должно быть выполнено условие. Вот. Окей. А сейчас Дмитрию отвечу. Что-то Дмитрию
у меня в телеграме какой-то баг. Я не могу его загрузить. Он не разрешает загрузить файлы. Может его
перезагрузить. Надо обновление накатить. Я позже сделаю. Сейчас не успел, к сожалению. Поэтому,
sorry. Только в таком формате на планшете. А на диск? На диск? Что-то я не подумал. Извиняюсь. Что-то
я аэродропом кинул на планшет. Не получилось. Телеграм что-то у меня залагал. Вот. Ладно. Давайте
продолжим. В принципе, это вроде бы норм. Окей. У нас соответственно множество x в данном случае
обозначают все ограничения по неравенству, которые сюда положены. Вот. И тогда для любого x,
который удовлетворяет неравенством, у нас должно выполняться вот это условие, которое написано
здесь. Вот. Я даже могу пока затереть вот это. Вот. Могу затереть вот это, чтобы не было видно. Вот это
ставить. Вот это затереть. Получается, что у нас вот так. Но x же у нас удовлетворяют ограничениям.
Тогда мы что можем сказать про f от x? Fg от x. Что мы можем про них сказать? То они что? Они меньше 0.
Они меньше либо равны 0. Все правильно. Вот. При этом лямбда у нас больше либо ровные 0. Так.
Получается, вот это все выражение, которое написано в сумме, оно будет что? Меньше либо равно 0.
Положительные вещи не отрицательны. Получается, что это выражение, которое написано здесь,
оно меньше либо равно для любого x, чем f от x. А это и есть определение глобального минимума. Все.
Вот. Доказали. Получается, что действительно из того, что точку у нас и была, следует то,
что у нас это есть глобальный минимум.
Так. Давайте в обратную сторону доказывать. Пусть, соответственно, так. А можете еще раз напомнить,
что такое седловая задача? Седловая задачка, вот. Это у нас определение было. Вот оно.
Мы здесь, соответственно, пользовались и вот этим, и вот этим.
Наша задача найти точку x с звездой ляма с звездой. Пока мы просто говорим, что вот она такая хорошая,
такая хорошая точка, что вот, оказывается, пока мы узнали, что седловая точка, даже без условия
слейтера, кстати, и без выпуклости, мы этим вообще не пользовались пока, оказывается,
является решением функ. Вот. То есть дает минимум за задачу, получается, дает минимум и
наши исходные функции на множестве и дает решение двойство. Вот. Конечно, по факту наша будет цель
найти x с звездой лямда с звездой дальше уже, когда мы будем конструировать метод. Но пока. Пока
просто точка. Просто про нее говорим, что это за точка. Пытаемся понять. Так. Что я здесь, соответственно,
делаю дальше. Пусть у нас существует, соответственно, это старая. Вот. Пусть у нас x с звездой это
глобальное решение задач с ограничениями. Теперь уже я точно добавляю то, что функции выпуклые,
выполнено условие слейтера в предыдущем пункте, это не надо было. На самом деле, это в одну сторону
только нужно. Тогда, соответственно, существует лямда с звездой, больше либо равно нуля. Такое,
что у нас есть точка x с звездой лямда с звездой, которая является глобальной точкой для функции
Lagrange. Вот. Здорово. Хорошо. Все понятно. В обратную сторону. Поехали. Давайте попробуем это все
подоказывать. Ну давайте те, кто проходил условия слейтера, что нам соответственно оно дает. Мы
только что с вами это выписывали. Оно нам дает, что f от x с звездой равно чему.
Условия слейтера.
Максимум функ. Все правильно. Все правильно. Супремум. Так. Супремум g. Да, двойственные функции.
Вот. Соответственно, да. Хорошо. Это дает нам условия слейтера. Вот. Здесь соответственно
то же самое записал у слейтера. Пожалуйста. То есть, когда мы берем максимум и находим решение лямда
с звездой, решение двойственное, нам эквивалент по просто минимуму исходной функции. Вот. Хорошо.
Хорошо. Вот. Ну это как раз на чем настигается супремум. Вот. Мы это осознали. Это осознали.
Соответственно, что делаем дальше? Давайте посмотрим на значение нашей функции Lagrange в точке x
со звездой, лямда со звездой. Видно же, что здесь мы лямда со звездой определили как решение двойственной
задачи. Вот. x со звездой было как решение исходной, лямда со звездой уже определили. Смотрим,
как ведет себя функция Lagrange вот в этой точке. Соответственно, вот функция Lagrange. Что мы можем
опять же сказать про ограничение. Что мы можем сказать про ограничение.
Сила того, что здесь стоит одно и то же.
Это, кстати, это неравенство понятно, откуда взялось? Вот это. То есть у нас f от x со звездой меньше,
чем f. Смотрите, у нас же f от x со звездой. Это infimum. Вот вот это. Потому что это
просто определение функции g в точке лямда со звездой. Мы выберем g в точке лямда со звездой.
Но если infimum равен f от x со звездой, вот. То тогда, если я разморожу x, то в x со звездой у меня
значение просто больше либо равно. Вот будет. Все. Соответственно, здесь выписали дальше эту
функцию Lagrange. Вы выписали функцию Lagrange. Давайте думать теперь. Это можно с двух сторон убрать.
Это можно с двух сторон убрать. Получается, что у меня вот эти ограничения должны быть больше
либо равны нуля. При этом я знаю, что вот x со звездой удовлетворяет функциональным ограничениям.
Вот. Получается, что я могу сказать? Я могу сказать, что на самом деле... Тут можно сразу просто
сказать, что вот то, что написано здесь, оно у нас будет чему? Будет просто равно нулю.
Можно, конечно, распространяться, что если мы вне ограничений, то тогда лямда равна нулю.
То есть если у нас строго меньше нуля, тогда лямда должна быть равно нулю. Когда у нас
достигается ограничения, равенство тогда лямда может быть произвольным. Вот. Ну,
здесь можно сказать, что вот то,
что написано здесь, выражение,
оно у нас будет равно � Number 0.
Почему? Потому что у нас
всё здесь меньше либо равно 0,
лямб decidingphasity у нас больше либо равны 0.
Получается вот то, что написано здесь,
оно будет меньше либо равно 0,
в силу того, что у нас как раз x лежит
во множестве, a лямб GuoL besser либо равны 0.
Но в силу того, что здесь знак больше либо равно, остается только то, что это равно 0.
С одной стороны получается, с этой стороны это больше либо равно 0, с другой стороны меньше либо равно 0.
Получается, что это равно 0. Вот такая идея, соответственно, да, получаются все ограничения у нас равны 0.
Поэтому мы можем сказать, что у нас вот такого вида есть оценка.
Значение Lagrangian в точке х звездой лямбда со звездой это просто f от х со звездой.
Что мы, соответственно, из этого делаем?
Первая часть определения сигловой задачи, то есть ровно то, что мы, опять же, сказали, что у нас f от х со звездой это просто инфимум.
Тогда, опять же, размораживая х в силу того, что это инфимум, мы можем сказать, что вот здесь у нас будет
лямбда х лямбда со звездой равная f х со звездой, которая, соответственно, меньше либо равна, чем lх лямбда со звездой.
Это, соответственно, первый кусочек определения сигловой задачи.
Второй кусочек определения мы вытягиваем откуда из того, что у нас...
Так, здесь равенство должно быть, здесь равенство.
Вот, то, что вот эта функция Lagrangian ровно из того, что мы здесь уже посмотрели.
Вот, что вот этот кусочек на самом деле у нас меньше либо равен 0.
Вот. А, здесь надо лямбду разморозить.
Лямбду разморозить.
Вот так вот.
Так, что мы про этот кусочек можем сказать, что в случае лямбда со звездой он равен 0,
а в случае произвольного лямбда он меньше либо равен 0.
То есть получается, что мы к f х со звездой прибавили что-то неположительное, прибавили что-то неположительное,
и получили функцию Lagrangian в точке х со звездой лямбда.
То есть получается, что она просто будет меньше либо равна чем значение в точке х со звездой,
а значение в точке х со звездой это lх со звездой лямбда со звездой.
Вот.
Хорошо, хорошо.
Более-менее понятно, более-менее понятно, что теперь у нас в две стороны доказано,
что поиск седловой точки эквивалентен по факту решению нашей исходной задачи с ограничениями
и даже эквивалентен решению нашей двойственной задачи.
А на самом деле в жизни седловые задачи это больше, чем просто функции Lagrangian.
Исходно они вообще возникли в теории игры в экономике.
В связи с тем, что, ну давайте примерчик посмотрим, пусть у меня есть некоторая функция,
это возможно просто какая-то функция, не функция Lagrangian, функция двух аргументов.
Хочу следующую физику ей передать.
Пусть у меня есть два игрока.
Первый игрок может выбирать какой-то вектор х, это может быть действие,
может быть распределение ресурсов, все что угодно.
Какая-то его стратегия, описываемая вектором х, из какого-то допустимого множества стратегий.
Лямбда, соответственно, это стратегия второго игрока.
Второй игрок это должен быть, второй игрок это у него лямбда.
Хорошо, что мы говорим, пусть у нас l от x в итоге выдает какое-то значение,
в зависимости от наших x и лямбда, от наших стратегий.
И суть такая, что первый игрок должен заплатить второму игроку сумму, равную вот этому значению функции l,
в этих точках, которые они выбрали.
Соответственно к вам у меня вопрос, чего тогда хочет первый игрок, чего хочет второй игрок?
Минимизации, максимизации.
То есть первый игрок хочет минимизировать свои риски, но не риски, а свои траты,
то есть он платит, поэтому хотелось бы платить меньше.
Второй игрок хочет, наоборот, максимизировать, чтобы ему максимально заплатили.
Соответственно, да.
Ну и в суть этой задачи, на самом деле, найти равновесие, равновесие в этой системе,
потому что какая может быть ситуация?
То есть, например, пусть у нас есть какая-то стратегия для игрока первого x tilde.
Исходя из этой x tilde, я могу подобрать лямбда так, лямбда для второго игрока так,
что выигрыш будет огромным, просто нереальным.
И, соответственно, да.
Тогда возникает мысль, возможно, мне тогда всегда нужно брать вот такое лямбда,
и я буду выигрывать всегда нереально много.
Но игра может быть устроена так, что иногда, выигрывая много,
для этого лямбда существует какая-то контрстратегия x неравная x tilde,
что выигрыш будет нулевой для игрока 2.
Такое может быть или вообще отрицательное.
Понятно, что туда просто платим в обратную сторону.
Получается, что вот такая стратегия, которая просто выбирает что-то наибольшее,
то есть ту лямбда, которая может дать экстремальную победу, экстремальную победу, экстремальную выигрыш,
она может быть не особо хорошей,
потому что игрок x просто может не выбирать x tilde и всегда контрить нашу стратегию.
Но при этом в системе опять же может быть какая-нибудь более-менее хорошая стратегия,
лямбда с tilde, которая вне зависимости от x может давать какой-то постоянный небольшой равномерный выигрыш.
Соответственно, да, хочется найти вот что-то посреднее,
как раз вот что-то такое равномерное выигрыш.
То есть смотрите, да, здесь это можно тоже с точки зрения седловой задачи немного поописывать,
что вот условно у нас есть какая-то средняя, такая вот равномерная стратегия,
и любое отклонение от этой стратегии по лямбда или по x дает ухудшение,
то есть условно по лямбда мы же наоборот хотим максимизировать, мы хотим как можно больше зарабатывать.
Но при вот таком x любое движение в сторону лямбды, изменение лямбды ведет к тому,
что выигрыш не увеличивается, а может даже уменьшаться.
И наоборот, при любом движении по x, при фиксированном лямбда со звездой, наши потери растут, не уменьшаются.
Соответственно, не имеет смысла менять x со звездой, лямбда со звездой это в некотором смысле вот такое вот равновесие.
Такое вот равновесие.
Игра, которая, соответственно, фон неймон этим и занимался только 70 лет назад.
Потом нэш.
И это все еще в некотором смысле ключевая особенность экономики.
Ну не только.
Так, хорошо.
Мне к вам такой вопрос.
Вот сейчас мы какие-то примерчики обсудили.
Вообще есть ли разница между тем, кто будет первым выбирать стратегию?
Первый игрок или второй?
Sorry, я не металлический.
То есть, например, первый сначала игрок выбрал стратегию, потом второй игрок выбрал стратегию, или наоборот?
Если сначала второй игрок выбрал стратегию, потом, исходя из того, чтобы был второй игрок, выбрал первый, будет ли от этого зависеть ответ?
Вот, будет ли вот эта стратегия силовая одинаковой?
Не очень понятно.
Ну, смотрите, пусть у меня есть какая-то стратегия, у игрока x какая-то есть фиксированная стратегия, ну, на данный момент есть фиксированная стратегия.
Игрок лямбда смотрит на эту стратегию и подбирает, исходя из нее, какую-то свою.
Что тогда делать x? Давайте лучше тогда сразу опишем.
Нет, не будет, потому что оптимум...
Но мы рассмотрели то, что мы нашли седло, и если тот, кто выбрал сначала правильно, то он выбрал такую точку, что второму игроку остается выбрать единственную, то есть оптимизировать уже то, что есть.
Ну, давайте чуть-чуть по-другому расскажу. Смотрите, как рассуждает первый игрок, если ему нужно выбирать первым.
Вот, смотрите, то есть он смотрит на ситуацию так.
Например, я выбрал какой-то вектор x, моя стратегия.
Он может быть любым, исходя из того множества, которое у меня есть.
Что будет тогда делать второй игрок?
Он просто будет, исходя из того, максимизировать свою игру.
То есть исходя из моего x, он будет брать supremum по всем возможным своим стратегиям λ и максимизировать.
То есть получается, что мне нужно подобрать x так, чтобы вот этот supremum, который он будет подбирать под каждый из моих x, был наиболее минимальным.
То есть я как бы беру infimum от supremum. Понятно, да, идея?
Да, понятно.
Тогда вот, когда у нас ситуация обратная, что сначала выбирает второй игрок, а потом первый, он рассуждает абсолютно логично.
Первый игрок будет минимизировать свои потери, исходя из того λ, который у меня есть.
Тогда я хочу максимизировать свой выигрыш по λ.
Получаются вот такие вот рассуждения.
На самом деле эта интуиция подсказывает, что действительно эти стратегии, эти игры в общем случае не одинаковые.
К сожалению, эти игры в общем случае не одинаковые.
Ну и в реальности в жизни действительно так и есть.
Но, и нам хочется более попрощенно рассматривать.
В общем случае можно показать это.
Смотрите, если мы, например, говорим, что у нас есть infimum по x, то понятно, что это меньше либо равно, чем значение для любого x из нашего множества.
Вот, тогда я беру от правой, от левой и правой части supremum.
Что у меня тогда получается, что здесь у меня будет supremum и infimum, здесь будет supremum.
Ну и тогда что, тогда я могу еще раз взять от обоих частей infimum.
Понятно, на левую часть уже никак не повлияет, потому что там все оптимизировано.
А направо повлияет, и у меня тогда получится infimum от supremum.
Вот, и получается supremum infimum в общем случае меньше либо равен, чем infimum supremum.
И игра действительно у нас не одинаковая.
В общем случае просто в некотором смысле для каждого из игроков понятно, что один из случаев более выгоден.
Ну вот вообще хочется понять, как вот эти supremum infimum и infimum supremum связаны с седловой задачкой.
Все связаны с седловой задачкой, с поиском седловой точки.
Оказывается, есть теорема о седловой точке, что есть у нас функция l, и у нее есть множество седловых точек.
Тогда и только тогда, когда вот эти supremum infimum и infimum supremum имеют решение, и эти решения совпадают.
Оказывается, получается, что когда мы решаем задачу min-max, если она эквивалентна той же задаче max-min,
эквивалентно и существуют решения, то мы оказывается решаем седловую задачу.
Но в принципе то же самое ведь было и в случае Lagrange, потому что что мы там создавали?
Вы же создаете двойственную функцию как infimum исходно.
Lagrange, Lagrange, вот по иксу.
А дальше что мы говорим? Мы берем дальше supremum по g.
Ну и это что получается? Мы хотим как бы найти supremum от infimum.
Ну и мы уже тоже показали для этого случая, что этот поиск supremum и infimum эквивалентен поиску седловой точки.
Ну в условиях slater, в условиях выпуклости и так далее. Так то только в одну сторону будет работать.
Соответственно вот так. Что еще хочу сказать здесь? Пусть у нас есть соответственно...
Хочется вообще понимать, существуют ли эти седловые точки и какие на это накладываются условия.
Потому что мы уже будем сейчас собираться их искать, какие-то алгоритмы придумывать,
рожать эти алгоритмы. Но вообще возникает вопрос, что искать-то там? Может быть там ничего нет?
В самом деле все там есть при хороших условиях. Вот у нас теорема сиона как покутание.
Есть у нас соответственно множество, на которых решается седловая задача выпукло и компактные.
Функция при этом непрерывна. И выпукло по иксу вогнуто по лямбда.
Что это означает? Это означает, что мы берем нашу функцию и например фиксируем какой-то икс.
Зафиксировали, варьируем только лямбда. Если при фиксированном икс функция по лямбда вогнута,
тогда мы говорим, что она вогнута по лямбда. Ну и аналогично. Фиксируем лямбда, размораживаем икс.
Если для любого лямбда функция l от икс прификсированном лямбда будет выпукла,
тогда соответственно говорим, что наша l выпукла по иксу.
Ну и вот в таком случае, если у нас задача выпукла вогнутая,
тогда у нас l имеют седловые точки на соответственно нашим выпуклым компакте.
В принципе, в некотором смысле ограничительная вещь, потому что у лагранжане, который мы писали,
лямбды не на компакте, иксы могут быть на компакте, могут быть не на компакте. Плохо, плохо.
Но с другой стороны, на самом деле, эти ограничения очень похожи на то, что мы имели и выпуклые оптимизации,
когда не было никаких ограничений, никаких силовых задач, не было никакого лагранжана.
Там же мы тоже говорили, что на функции x на неограниченном множестве r она может уходить в минус бесконечность, решения нет.
И когда мы, например, доказывали сходимость градиатного спуска, там просто в явном виде предполагалось,
что значения f от икса звездой существуют, а расстояние вот это x0 минус f от икса звездой существует,
и там больше минус бесконечность, то есть какое-то конечное значение.
А это значение, соответственно, тоже не находится где-то в минус бесконечности.
Такое предполагалось просто не явно, что решения существуют.
Ну и понятно, что если рассматривать задачу на компакте, то для любой функции выпукла это можно гарантировать.
Ну и здесь по факту то же самое и делается. Говорится, что раз компакт, то для любой функции я могу гарантировать.
Конечно, опять же, когда мы сейчас будем доказывать с вами методы, мы будем предполагать, что решения существуют,
поэтому можно и не заморачиваться с этим вопросом.
Ну понятно, в общем случае компакт наш и все.
На самом деле, чуть-чуть можно порелаксировать теоремы Сиона Кокутани.
На самом деле там не одна у нее релаксация в плане изменений условий, как и ослабление условий,
так и наоборот усиление, чтобы доказать более сильный факт.
Ну вот здесь я привожу одну. Здесь избавляемся от компактности одного из множеств.
То есть можно сказать, что например у нас лямбда может быть неограниченным, или х наоборот неограниченным,
но тогда понятно, нет гарантии существования решения, но хотя бы выполнено вот это, вот это.
И соответственно, если тогда уже предположить, что решение существует, то тогда и существует и седловая точка,
потому что предыдущая теорема нам говорила о том, что если у нас решения существуют,
и инфинум, суправим, и суправим, и инфинум совпадают, тогда и седло есть.
Давайте перед перерывом суммируем то, до чего мы дошли.
Функция Lagrange. Седловая задача на самом деле лежит в ее основе.
Она же задача min-max, то есть поиск, минимизация, максимизация нашей целевой функции L.
Можно менять местами min и max, но в принципе для функции Lagrange это не обязательно,
потому что мы двойственно как раз и вводили, чтобы сначала максимизировать, потом минимизировать,
но другой вопрос, нужно ли вообще считать ее двойственную, или просто смотреть на функцию Lagrange
и решать задачу для нее.
Ну и соответственно то, что как у нас связаны min-max задача и задача поиска седловой точки,
что для хороших функций это все более-менее вещи эквивалентные.
Ну а соответственно после перерыва переходим к тому, как искать уже вот эти min-max и седловые точки.
Окей? Перерыв 5 минут.
Так, ладно. Видимо ничего, тогда мы идем дальше.
Поняли, седловая задача, что есть седловые задачи не только в пределах функции Lagrange, это игры,
в машинном обучении, всякие состязательные обучения, где у вас две сетки обучаются,
либо обучается какой-то дополнительный шум, вот это тоже седловая задача.
Соответственно классы седловых задач довольно большие, то есть возникают они в разных местах,
чуть ли не от экономики и управления, когда функция Lagrange у вас есть,
до динойзинга картинок или сетки, которые генерируют изображение.
Давайте про методы подумаем.
Есть задача минимизации, которую мы умеем решать, есть седловая задача.
Как тогда можно сконструировать метод для седловой задачи?
Давайте самый простой метод для минимизации, градиентный спуск. Поехали.
Что тогда для седловой задачи можно сделать похожего?
Сначала градиент по x берем, потом по λ и прибавляем либо отнимаем.
Хороший вариант, смотрите, самый вообще такой, не особо хитрый, это сказать,
давайте я сначала прификсирую на мексе, максимизирую функцию по λ,
потом наоборот заморожу λ, разморожу x, начну дальше.
Альтернирование в некотором случае такое получается, такие методы есть, мы их разберем чуть попозже,
когда будем про максимальный оператор избирать.
Но там посмотрим, в чем суть у них.
А можно делать аналог градиентного спуска.
В силу того, что вам нужно минимизировать по x, давайте я буду делать градиентный шаг по x.
Но в силу того, что мне нужно максимизировать по λ, я тоже буду делать градиентный шаг.
Градиентный шаг.
Тоже буду делать градиентный шаг, но куда?
Если мне нужно максимизировать.
Какой здесь будет знак?
Какой?
Если мне нужен максимум.
Там был нужен минимум.
Плюс, да, конечно.
Соответственно, спуск, подъем.
То есть по x спуск, по λ подъем.
Хороший вариант в принципе, то есть на практике часто выручает, работает.
Но можно придумать что-то более изощренное, потому что интуиция подсказывает,
что вот тот метод, который мы с вами рассматриваем в качестве спуска-подъема,
вот он здесь и написан, может работать не так хорошо, как хотелось бы.
Лямба.
В чем интуиция?
В чем интуиция?
Давайте рассмотрим вот такую задачку.
x лямбда.
x лямбда на множестве r, на множестве r.
Стартовая точка 1,1.
Где, кстати, у решения будет седловая точка в этой задачке?
Седловая точка будет, кто понимает, в какой точке?
0,0 может?
Конечно, в точке 0,0. Почему так?
Потому что на самом деле тут даже можно условия оптимальности выписать.
Условия оптимальности выписать, потому что вам же что говорят?
В точке оптимума вот у вас что происходит?
Любые движения по лямбде вас выводят из экстремума.
Поэтому градиент в оптимуме по лямбде должен равняться нулю при фиксированном x.
При фиксированном x звездой вас выводите из лямбы, начинаются проблемы.
Аналогично, когда вы выводите себя из x при фиксированном лямбде со звездой,
у вас тоже начинаются проблемы.
Вот. Все.
Обтимальные условия.
Берете градиент по...
Как градиент? Производную.
Производную по игру.
Получаете, что x равен нулю. Получаете, что лямба равна нулю.
Все, хорошо. Это оптимальная точка. 0,0.
Точка 1,1.
Вот. Давайте вообще посмотрим, куда указывать будет то, что мы используем в качестве градиента.
Градиента. Градиента антиградиента.
Куда будет указывать вот этот вектор.
Например, в точке там, в стартованной нашей точке.
Почему он вообще будет равен? Быстренько давайте скажите мне.
Градиент x.
Лямба, а здесь будет минус x, да?
Вот. И тогда это 1,-1.
Вот.
Смотрите, вот этот вектор, который у меня указывает на оптимум.
Этот вектор у меня указывает на оптимум.
Вот.
Этот вектор что? Это вектор просто 1,1.
Так, если 0, это вектор 1,1.
Может быть минус 1,1?
Нет, сейчас я пока просто указываю вектор от текущей точки до оптимума.
А вот вектор, который у меня...
Градиента, он как у меня направлен?
У него направляющий вектор это 1,-1.
Так, 1.
Господи, сейчас правильно бы его нарисовать.
1. Это по x.
Минус 1 по...
Сейчас.
Ладно, короче, он должен быть автогонален.
Как это будет?
Туда?
В ту сторону? По лямбде?
Вправо.
Он идет...
По лямбде он идет вправо, да?
Или вправо должен?
Вправо смотреть вектор должен.
А, я просто неаккуратненько нарисовал вот это.
Давайте я вот это нарисую так, что вот у меня.
1,-1.
Вот.
Чтобы оси координата не были.
Вот, 1,-1.
А это будет куда указывать?
Минус 1, то есть по x.
Вправо, по лямбде вниз.
Вот так.
И смотрите, получается, что они автогональны.
То есть у меня вектор...
На самом деле это вообще будет стандарт для любой ситуации.
Вы еще точку поменяете, будет то же самое.
Вот.
У меня автогональны текущие точки.
У меня автогональны текущие точки.
Это проблема.
Это можно проверить для любой точки,
как будет работать алгоритм.
А смысл же вообще вот этих градиентных методов
был в том, что по-хорошему мне бы хотелось,
чтобы у меня направление, возможно, было не на оптимум,
но хотя бы уголок здесь был бы острый.
И что частично я смещаюсь как-то в оптимум.
Так и было, потому что иначе у вас направление просто не убывает.
Функции не бывают.
И вот здесь, из-за того, что у вас постоянно будет автогональ,
у вас будет такая спираль,
которая будет расходиться, расходиться...
Вот.
Интуиция показывает, что это работает плохо.
Но, еще раз, это интуиция,
потому что на самом деле эта задача,
которую мы рассматриваем,
как вы понимаете, она выпукло-вогнутая,
она не сильно выпукла-вогнутая.
Здесь, как мы уже поняли,
такие задачи можно рассматривать только на компактах.
В общем случае.
Ну и, соответственно, в случае компакта
там на самом деле все будет неплохо,
но с точки зрения интуиции,
с точки зрения интуиции,
там на компакте будет что-то вот такое.
Например, если взять круг какой-то,
то она выйдет на этот круг и будет по нему кружить.
А в центре круга будет 0,0.
Исходимость будет по средней точке.
Исходимость будет по средней точке.
А средняя точка по этому кругу будет 0,0.
Но это так.
Это так просто строгость того,
что примерно на самом деле не до конца корректно.
В некоторых вопросах.
Но вообще суть понятна,
что с силой того, что артагональна траектория,
мы будем уходить постоянно,
даже если мы выйдем на какую-то окружность,
и по факту мы будем на ней,
но решение вообще возвращать не это.
Выглядит немного сомнительно.
И поэтому был придуман чуть-чуть другой метод.
Чуть-чуть другой метод.
Ой, неправильно написал,
здесь не прямо двойственного говорит.
Экстраградиент.
Метод экстраградиента.
Сверху правильно написано.
Метод экстраградиента.
Придумала его жена Борис Теодоровича Полика.
Галина Карпелевич, тоже покойная.
Сколько уже получается?
50 лет назад, 50 лет назад.
Как раз для решения в том числе задач,
которые похожи на задачу Лагранжа.
И суть этого метода в некотором смысле
похожа на Нестеровскую идею,
но не совсем.
Тут есть что-то от Нестера,
хотя метод Нестера был придуман позже.
В чем суть?
Делаем тот же самый спуск-подъем.
По X спустились, по Y поднялись.
Но это у нас только промежуточная точка.
Это промежуточная точка.
А итоговый шаг мы сделаем как раз по градиенту
этой точки, которая как-бы из будущего.
Мы сместились чуть-чуть,
посмотрели в будущее.
Ровно то же самое, как у Нестерова.
Та же самая идея добавить в градиент шаг вперед.
Чуть-чуть пройти вперед.
Но у него там это моментумом делается,
здесь вот таким дополнительным шагом.
Посмотрели чуть-чуть вперед,
смотрели там градиенты в тех точках будущих
и шагнули уже по градиентам в тех точках.
При этом точка сама исходная,
она осталась старая.
X-кат и λ-кат.
Это не два просто шага спуска-подъема.
Это взгляд в будущее и уже шаг отсюда.
Вот такая вот идея.
На самом деле довольно простая.
Как раз основана на этой задачке.
Основана на этой задачке.
Можно в качестве упражнения дома себе посмотреть,
доказать, что вот такого рода метод,
который мы сейчас с вами разбираем,
он дает именно артагональность.
Там уже артагональности не будет.
Там как раз будет уже правильный угол,
он будет острым и будет все хорошо.
Это в качестве упражнения дома просто проделать.
Это несложно.
Если еще раз рассмотреть тот пример
с центром в нуле,
разве мы не будем все равно
оставаться на окружности в лучшем случае?
Давайте посмотрим.
Как раз на него уже захотелось.
Давайте смотреть,
что у нас там будет.
Здесь будет лямбда,
здесь будет минус х.
Посчитали, сделали шаг.
С точки 1.1 у нас будет 1-1.
С шагом это будет это,
минус это.
Тогда у нас точка х 1-2,
лямбда 1-2,
это будет что?
Это будет минус это,
нет, минус это, это.
Теперь мне нужно посчитать уже градиент
вот в этой точке.
Градиент вот в этой точке.
Нет, не так.
Один же еще был.
1.1 стартовал.
1+.
Мне нужно будет посчитать градиент
вот в этой точке.
Давайте посчитаем, что там будет.
Там тоже просто лямбда и минус х,
поэтому градиент будет
градиент 1-2.
Давайте вот так вот.
Градиент 1-2, лямбда так,
минус х вот так.
А у нас же, по идее, там должно быть
1-тау и 1-тау.
Мы же в случае лямбды добавляем.
В случае лямбды...
Сейчас, в случае лямбды прибавляем.
А!
Ага.
Вот так, да?
Ну да.
Все, хорошо.
Так, тогда будет градиент вот такой.
И что тогда будет с итоговым шагом,
когда я по нему шагну,
что у меня будет 1, лямбда 1?
Давайте глянем.
Х, это у меня единичка все еще из единички.
Я вычитаю что-то с шагом.
У меня здесь получается это плюс...
минус это плюс это в квадрате.
А здесь у меня будет...
я буду 1 и буду добавлять...
вот так.
Согласны?
Согласны?
Вроде бы все правильно.
Да, вроде так.
Вот, и смотрите,
если вы берете эту меньше единицы,
вы перемещаетесь к точке, которая ближе.
Ясно, спасибо.
Все, переместились, она стала ближе.
Интуиция здесь такая.
Опять же, это скорее интуиция,
как этот метод создавался.
Мне это рассказал Борис Теодорыч,
поэтому это как-то в некотором смысле из первого хода.
Вот.
Окей.
Давайте его заказывать.
Дальше доказательств я не успел здесь добавить,
поэтому давайте вручную быстренько докажем.
Так.
Видимо, прямо двойственный метод мы уже не успеем.
Он останется просто так.
Хорошо.
Давайте доказывать экстраградиент.
Так.
Давайте я введу следующее обозначение.
Пусть у меня будет вектор Z,
это вектор из Y.
А оператор F от Z,
это у меня как раз будет
градиент по X
и антиградиент по Y.
Ну, чтобы просто короче это все писать.
Вот.
Тогда у меня метод просто экстраградиента
запишется следующим образом.
Согласны?
Ну, вроде да.
Просто чуть покороче сделай обозначение.
Так.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Просто чуть покороче сделай обозначение,
чтобы не таскать
эти за собой две строки
для X и Y.
Ввел просто длинный вектор.
Где как бы
X, Y
выстроены градиенты.
Градиент, антиградиент.
Окей.
Делаем все так же,
как делали раньше.
Только здесь давайте я
пока не буду писать звездочку.
Не пишем ее пока.
Вот. Просто оставлю какую-то точку в запись.
Потом объясним почему.
Вот.
Подставляем итерацию.
Так.
Сквадрать.
Так.
Раскрываю квадратик.
И получаю, соответственно...
Сейчас.
Как я вообще его раскрыть?
Или так?
Сейчас.
Ту-ту-ту-ту.
Так.
Так.
Вот.
Вот.
Вот.
Вот.
Так.
Да.
Норм.
Сквадрать.
Вот.
И вот это я давайте чуть похитрее запишу.
Это же у меня по факту что?
Это у меня то же самое, что
ZKT
плюс 1
квадрат. Согласны?
Я согласен.
Ну и хорошо.
Сейчас.
Тут можно...
А давайте по-другому.
Или так-так-так-так-так.
Давайте чуть по-другому.
Немного по-другому и сделаю.
Хочу чтобы у меня
вылезло скалярное произведение.
Ну или оскалярное произведение
начнем писать.
Давайте пока запишем второе.
Потому что для второго нам тоже это понадобится.
Здесь я поставлю другую точку Q.
ZKT
минус это FZK
минус Q
в квадрате.
Также раскрываю.
ZKT
минус Q в квадрате.
Минус 2 это
FZKT
Здесь ZKT
минус Z.
Так. И здесь соответственно
ровно по той же причине
будет ZKT
плюс 1 вторая
минус ZKT
в квадрате.
Вот. Получилось что-то вот такое.
Получилось что-то
вот такое.
Хорошо. Хорошо.
И теперь я чуть-чуть вот с этим
всем безобразием
его немного подвигаю.
Его немного подвигаю.
Сейчас потом
покажу, что мне хотелось бы
здесь получить.
Здесь я хочу поменять точку
на ZKT
плюс 1.
В самом деле это довольно простые манипуляции,
но мы их сейчас проделаем.
Вот.
Давайте вот так вот.
А здесь соответственно
минус Q.
В каком равенстве
там, где 2AB
вот это слагаемое датамо?
Да. Вот. Хорошо.
Чуть по-другому перепишу.
Вот здесь я вытащу ZKT
плюс 1.
И...
Так.
Соответственно у меня там было ZK.
Я здесь что должен...
Это компенсировать.
Компенсировать.
Компенсируем.
Компенсировал.
Вот.
Но теперь я замечу, что вот то,
что у меня написано вот здесь, вот это выражение,
это же что получается?
Это минус 2.
F.
Это F.
Одна вторая.
Это же что у меня с минусом.
Это что у меня было?
2ZKT плюс 1
минус ZK. Согласны?
Я подставил вот это.
Минус.
Минус ZK. Вот сюда.
Понятно?
Вот оно.
Согласны?
И я тогда запишу.
Так.
Что-то подобное. Это же просто итерация.
Вот. Я подставил. Минус это F.
А здесь вот оно.
Оно мне выражено. Ну, по итерации.
Соответственно, то, что у меня здесь написано будет,
это 2.
Минус 2 ZKT.
Минус ZKT плюс 1
в квадрате.
Согласны?
Вот.
И получается, что тогда,
то, что у меня написано синим,
то, что у меня написано синим,
я могу переписать следующим образом.
ZKT
минус Z
в квадрате.
А это F
ZKT плюс 1 вторая.
ZKT
плюс 1
минус Z.
И вот здесь у меня минус 2,
а здесь был плюс просто.
Поэтому остается минус ZKT
минус ZK
в квадрате.
Согласны?
Вот.
То есть вот это
равняется вот этому.
Аналогичной манипуляции
я могу провернуть и со вторым.
То есть там же суть такая же,
я подставляю скалярное произведение,
вот тут вот добавляю вот эту точку,
одна вторая сюда.
И в связи с этим
у меня дальше вот здесь выскочит
инак мин.
Вот. Как-то так.
Вот. Получили
два равенства.
Два равенства получили.
Теперь нужно как-то с ними будет
провзаимодействовать.
Провзаимодействовать.
Ну давайте сложим. Давайте сложим их.
Давайте их сложим.
Я вот одного уже сложил.
И здесь у меня что будет?
ZKT плюс 1 минус Z
в квадрате
равно
ZKT минус Z
в квадрате.
А ну давайте отсюда просто.
Я скопирую.
Вот. Хорошо.
Второе. Здесь, соответственно, у меня
будет ZKT плюс 1
вторая минус U
в квадрате.
А здесь какая добавочка у меня будет?
Ну, в той страничке и заберем.
Одна добавочка.
Оп. Так.
Это лично скопировалось.
О. Тут не влезло.
Вот так давай.
Вот так.
Потом вот так.
Хорошо.
Хорошо.
Так.
Дальше давайте
немного поработаем
с этими скалярными произведениями.
Вот.
Что там в скалярных произведениях появится?
F
ZKT плюс 1 вторая.
Я вот так вот давайте сделаю.
Минус Z
плюс
F
F
ZKT плюс 1 вторая.
ZKT плюс 1
минус ZKT
плюс 1 вторая.
Минус, соответственно,
F
А, это сейчас минус был, извиняюсь.
А там даже
разве не должно быть
в первом слагаемом K? Или это умный ноль?
Где, где, где?
ZKT в первом слагаемом.
Ну ZKT минус Z, нет?
К плюс 1.
Где, где, где? Вот здесь?
Вот в этом?
Не-не-не, вот то, что вы красным начали переписывать.
Не-не-не, смотрите, да.
Я вот в умный ноль добавляю.
Добавляю и вычитаю ZKT 1 вторую.
Хорошо.
Вот. Здесь, соответственно,
в чем они будут? ZKT
ZKT плюс 1 вторая
минус
F
У. Вот.
И тут как бы ключевая идея.
Взять У
равным ZKT плюс 1.
Ну как ключевая? Ну вот одна есть.
Взять У, взять ZKT плюс 1
и тогда схлопнется.
Давайте вот я здесь
подставлю ТУ.
Вот.
Подставил. Кстати, сразу же
заметить, что вот эти сократятся.
Потому что это одно и то же,
только одно с минусом, другое с плюсом.
А из скалярных произведений
соответственно тогда выскочит.
Минус
вот это вот.
Плюс, соответственно,
разность
операторов.
Разность операторов.
Вот этих F,
которые я записал.
В точке ZKT плюс 1
минус ZKT плюс 1 вторая.
Вот.
Хорошо, хорошо.
Тогда, тогда,
что у меня получится
из всего этого безобразия?
Из всего этого безобразия,
что у меня получится?
Ну давайте выписывать.
Справа у меня,
слева вот так.
Z я пока не подставлял.
Вот.
А справа соответственно у меня будет
ZKT минус Z
в квадрате.
Из того, что пока не убилось
ZKT плюс 1 вторая
ZKT
ZKT в квадрате.
И скалярное произведение.
Одно соответственно вот это минус.
Два это F
ZKT плюс 1
так.
ZKT плюс 1 вторая Z.
Вот.
А второе скалярное произведение
я сразу по каши разложу.
По каши Буниковскому.
Как, соответственно,
это в квадрате
ZKT плюс 1 вторая
ZKT
B
S
плюс 1
и ZKT плюс 1 вторая.
Понятно, да?
Вот это скалярное произведение
просто по KBH разложено.
Там как раз просто еще это болталось
рядом с ним, вот здесь.
Когда мы до того, как мы это раскрывали.
Вот, я его просто в F внес.
Вот, получили следующее.
И тогда здесь что можно сделать еще?
Раз, два.
Вот.
А дальше что предполагается?
Дальше предполагается,
что у нас функции
липшицевые не только выпуклые,
но еще имеют липшицевый градиент.
Градиент по X
L
липшицев
липшицев.
Вот, градиент по Y тоже
L липшицев.
L
липшицев.
Поэтому и оператор, который по факту состоит
из этих двух векторов,
тоже будет L липшицевым.
Согласны?
Почти,
я вас задерживаю уже.
Так.
Попробуем закончить.
Я расписал по липшицевости.
Здесь, соответственно,
что останется?
Так.
Попробуем.
А нет,
зря убрал.
Это нормально.
Так.
Лишнее убрал.
Лишнее убрал.
Тут еще было
минус ZK
плюс одна вторая
минус ZK.
Вот так.
По липшицевой связи
сами только что расписали
как L
ZK плюс одна вторая
минус ZK
в квадрате.
Все.
В принципе, мы уже дошли
почти до ответа,
потому что видно, что если я возьму
это меньше
чем
1 делить на L,
то теперь
я смогу убить
с помощью минуса
вот эти
два кусочка.
Вот эти два кусочка.
И у меня еще остается
скалярное произведение.
Вот такое вот.
На разность.
На самом деле очень похоже на то,
что получается в выпуклом случае
для оптимизации.
Там тоже разность, тоже скалярное произведение,
только там стоял всегда градиент.
Для оптимизации здесь стоял
градиент.
А остальное
один в один.
То есть по факту мы получили что-то очень похоже.
На самом деле,
из этого градиента, который здесь записан,
по выпуклости и вогнатости
функции можно вытащить
вот такое.
Можно вытащить вот такое.
Вот так.
Сейчас точки только расставлю.
Вот она вторая.
Вот. Можно получить что-то вот такое.
Можно получить что-то вот такое.
Дальше вы суммируете.
Дальше вы суммируете.
И получаете
вот среднее.
Среднее. А здесь остается
z0-z
в квадрате делить на 2 это.
Вот.
Окей, окей.
А, на k еще. На k.
Было вот zk
к большой
z в квадрате.
Но это с минусом, поэтому я убил.
Я вот поэтому сразу это убил.
Дальше вот здесь о чем можно заметить,
что по неравенству
Янсона
для выпуклых
и вогнутых функций
для выпуклых и вогнутых функций
для выпуклых и вогнутых функций
неравенство Янсона
можно записать неравенство Янсона соответственно.
И внести вот эту точку внутрь.
У вас точка будет средняя здесь.
x и средняя точка.
Меньше либо равно, чем
z0-z
в квадрате делить на 2.
Все. На k.
Этому уже знаем.
Ее можно поставить как 1 делить на l.
Вот.
Получилась вот такая оценка сходимости.
Очень похоже на то, что получается
для градиентного спуска.
Единственный вопрос, который нужно успеть отразить
это то, что
в градиентном спуске мы сразу же
для задач минимизации
говорили звезда, пожалуйста.
Звезда, пожалуйста.
Вот.
Ну и здесь соответственно тоже
подставляются. Здесь можно также
сказать подставляем звезду.
Ну хорошо, замечательно.
И получили
какой-то результат.
На самом деле, для сильно выпуклых задач это правда,
но там можно получить более тонкий оценок
именно с точки зрения сходимости. Там можно линейную сходимость получить.
Вот.
А для выпуклых задач
тут проблема есть. Проблема есть в том,
что критерий вот такой вот вида
y со звездой
минус f
x со звездой
y
Вот. Он не работает.
Просто потому что
этот критерий
может равняться нулю.
Этот критерий может равняться нулю.
Создавайте примерчик.
x минус 1
y минус 1
Вот.
Ну и смотрите. Решайте вот такую задачу.
Опять же одномерную minmax
на R каждую.
Ну и понимаете,
что у вас там
градиент по
x. Это что просто?
Градиент по x.
Градиент по x это y
минус 1.
Градиент по y
это x минус 1.
Вот.
Решение в данном случае это просто
точка 1 минус 1.
И когда вы... А! Ну, можно даже
градиента не считать.
Градиенты для оптимальности нужны,
но мы уже сами это считали с вами.
Поэтому, когда вы подставляете
просто значение вот эти вот
в эту функцию,
то она вам дает типа 0.
Потому что подставляете
1 сюда
0.
Поэтому критерий такой дает
просто 0. По нему измерять
в обыкновенном случае не всегда
правдоподобно. А так?
Так. Соответственно более чем
обочая схема. Ну и соответственно
скорее всего вам на семинарах
может быть кто-то рассказывал, кто-то не рассказывал.
Ну, в случае как раз нашей функции
Lagrangia. У вас же там как раз...
Ой, здесь уже игрительно что писать. Там были лямбды.
Вот, лямбды.
В случае там функции Lagrangia
у вас что там было соответственно?
f от x
лямбда со звездой.
Это как раз просто...
Что это будет?
Лямбда со звездой
f от x
и
лямбда.
Вот. Там мы как раз измеряли
зазор двойственности.
Зазор двойственности,
вида.
Сейчас вспомню.
Ладно.
Конспект добавлю.
Следующая пара у вас.
Вот. Хорошо.
Поэтому, смотрите, критерии здесь
можно брать чуть-чуть другой.
Критерии тут можно брать чуть-чуть другой.
Можно брать максимум
по y, минимум по
x.
Вот.
Ну, тогда понятно, что
нужна какая-то
компактификация множества,
по которому вы берете
минимум максимум.
Иначе вот это может улететь в бесконечность.
Иначе значения функции могут
улететь в бесконечность.
И это уже проблема.
Поэтому здесь можно
даже если вы решаете безусловно задачу
Седлову,
можно в некотором смысле ввести искусственно
множество c и предположить, что
множество c содержит решение,
но компактно по нему можно
брать максимум, минимум.
Вот такая вот идея.
Задержал чуть-чуть.
Ну ладно, я надеюсь понятно.
Понятно ведь?
Ну да, более-менее.
Хорошо. На самом деле тут в конспекте еще есть
прямодвойственный метод.
Тоже для полезной штучки.
Но понятно его.
Если посмотрите самостоятельно,
будет замечательно.
А те, кто в принципе не ходит,
можно и не говорить, что его не было на лекции.
Мы его успели.
Так что да.
Они все равно ботают по слайдам.
Поэтому на слайдах он будет.
Так, все. Спасибо всем большое.
Извиняюсь, что еще раз задержал.
Все.
Всем хорошего дня.

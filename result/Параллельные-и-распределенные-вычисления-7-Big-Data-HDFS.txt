Итак, всем привет, меня зовут Ивченко Олег, и на этом
курсе я у вас проведу 4 занятия, одно из которых
будет в зуме, и мы с вами обсудим большие данные,
то есть экосистему ходуб.
По плану у нас будет первое занятие сегодняшнее про
HDFS, про хранение больших данных, и уже на этой неделе,
по крайней мере, у некоторых групп начнутся семинары
по HDFS.
Потом еще будет два занятия про MapReduce, где мы будем
писать на ходуб стриминга на питоне, и будет еще последнее
занятие про Apache Hive, это SQL-движок поверх ходуба.
Так что, кто SQL забыл, повторяйте SQL, вам он на этом курсе пригодится.
Ну и так, давайте поймем сначала, что такое большие
данные, потому что четкого критерия, которое отделяет
маленькие данные от больших, его нет, но есть некоторые
свойства.
Например, свойства Volume, если данных так много, что
они не помещаются на один ваш компьютер, на ваш домашний
комп или на один сервер, то значит нужно какую-то
придумать систему, которая будет из нескольких компов
собирать что-то единое, вы будете этим пользоваться
и хранить там эти данные.
Потом Verity, это то, что данные у нас бывают разные, и вот
тут самый яркий пример, это банковские системы, как
они принимают решение, выдавать клиенту кредит
или нет.
Вот, кто может подсказать, какие для этого данные используются,
ну кроме стандартных банковских каких-то историй.
Платёжспособность, ну то есть какой у него доход,
то есть понятно, что мы смотрим историю внутри банка, как
он брал кредиты, не брал, как он их отдавал, есть ли
у него вклады, мы ещё смотрим его доход, иногда это сложно
оценить, потому что он допустим получает зарплату одну в
одном банке, другую в другом, вот, но как можно оценить
ещё, это можно использовать всякие сторонние ресурсы,
например, можно вытащить какие-нибудь данные социальных
сетей, допустим, какие он покупки совершает, в какие
путешествия ездит, какое у него семейное положение,
то есть вот эти вот косвенные данные, они поставляются
из разных мест, в разном формате, с разной периодичностью,
и нужно всё это запихать в какую-то единую воронку,
потом это всё обработается и будет решение true и false,
вот это вот проверяете, ну и про velocity это то, как часто
мы данные обрабатываем, есть несколько вариантов,
ну вот рассмотрим самые два крайних случая, первый
крайний случай это когда вам нужно сформировать
какой-нибудь отчёт раз в месяц или раз в квартал,
когда вы собираете данные в течение трёх месяцев,
запускаете какую-то задачу и она вам этот отчёт делает,
и вторая крайность это real-time обработка, например,
рекламная система, вы, наверное, все сталкивались с тем,
что вот вы ищете какой-то запрос в магазине, хотим
купить там, не знаю, новый powerbank, вы его выбираете
и через какое-то время на всех сайтах, где есть рекламные
баннеры, вам начинают рекомендовать powerbank, это делается быстро,
если мы будем ждать два дня, пока эта джеба отработает,
то это будет никому не нужно, это вот real-time обработка,
ну и для всего этого, для того, чтобы хранить большие
данные, хотя бы просто хранить, нам нужна какая-то
новая система и нам нужно несколько серверов, желательно,
то есть есть два варианта, мы можем купить сервер
побольше, если нам не хватает текущего, но мы рано или
поздно упрёмся в то, что уже некуда будет вставлять
дополнительные плашки, ну или сервер будет просто
тупо дорого стоить, если кто-нибудь видел, как зависит
цена от мощности сервера, то, скажем так, есть стандартные
сервера, линейка, их цена растёт примерно линейно
в зависимости от мощности, в какой-то момент мы переходим
в разряд high-performance computing, HPC, и там цена сразу подскакивает
экспоненциально, ну вот, например, одна из историй
из моего опыта, в одной компании мы как-то искали,
какие сервера можно арендовать для работы нейронок, для
нейронок что нужно, видеокарты, обычно в стандартном серваке
сколько видеокарт, кто знает, 8, обычно 8, бывает 6, но мы
хотели побольше, и вот с трудом получилось найти 12, 16 получилось
найти только в одной какой-то южнокорейской компании,
которая эти сервера производила, и стоил такой сервер не
как два по восемь, а как десять по восемь, потому что
там нужно другое железо, то есть нужна как бы, вся
вот эта обвязка железная должна быть сделана по-другому,
и на это затрачиваются какие-то допресурсы.
Вот, поэтому лучше все-таки перейти к горизонтальному
масштабированию, когда мы возьмем несколько серверов,
объединим их в единую систему, и сейчас мы будем с вами
разбираться, как такие системы работают.
Их систем несколько, я буду рассказывать в основном
про Hadoop, но Hadoop сейчас не единственно далеко, то
есть сейчас есть и всякие облачные хранилища, например,
наш МФТИ-центр обработки данных, его хранилища живут
на системе CF, это тоже такая распределенная файловая
система, в чем-то она похожа на Hadoop, но в чем-то она другая.
С чего началась вообще вот такая эра распределенных
систем, она началась со статьи 2003 года про Google File System,
вот скриншот с этой статьей, но система была разработана
в Гугле, она была закрытая, ей можно было только пользоваться,
и поэтому через какое-то время появилась уже открытая
Система называется Hadoop, потому что это было первое
слово, которое сказал маленький сынишка основного разработчика
этой системы, и, собственно, один из подмодулей этой
системы, это Hadoop Distributed File System, HDFS.
Как он устроен?
Пока такой краткий обзор, дальше мы погрузимся в большее,
на семинарах вы погрузитесь еще побольше, у нас есть
несколько серверов, у серверов есть разные роли, например,
вы здесь видите три основные роли, первая роль, с которой
вы будете больше всего работать, это вот этот маленький
клиент, то, что написано HDFS-клиент, еще называется в литературе
H-Node, вам, скорее всего, на почту еще в начале курса
кому-то только сейчас пришли аккаунты на сервере, который
называется MIP-клиент, было такое?
Ну кому не приходили, значит, еще придут до семинаров,
кому-то уже пришли, вот этот сервер, на который вы будете
заходить, это как раз H-Node, все остальные роли, вы на
них, скорее всего, зайдете на одном семинаре, один
раз, посмотрите, как там все устроено, как все это
хранится, и на этом все закончится, потому что обычно, вы как
разработчики на ходлупе, вам не нужно ходить на вот
эти ноды, вам достаточно клиента, что у нас на этих
нодах есть?
У нас есть так называемые узлы имен и узлы данных,
нейм-нод датонода, узел имен хранит логично имена,
то есть сами данные там не хранятся, там хранится
вот такая деревоподобная структура, в которой хранятся
ссылки на данные, где хранятся какие данные, а сами данные
хранятся на датонодах.
Данные хранятся в виде блоков, ну как в любой файловой
системе есть блоки, кто, кстати, помнит размер блока
в стандартной файловой системе, FAT32 NTFS, 4 килобайта,
кто не помнит, легко проверить, берёте какой-нибудь пустой
носитель под этой файловой системой, неважно там диск,
флешку, записываете минимальный файлик и смотрите в свойствах,
в свойствах будет занято 4 килобайта.
Вот в ходлупе тоже есть блоки, правда они сильно побольше.
Ну и также важно понимать, что информация про эти блоки
хранится на нейм-ноде в оперативке, почему?
Потому что оперативка просто быстрее.
Вот, то есть вот в самом упрощённом виде у нас вот такой вид
имеет наша система HDFS.
Конечно в реальной жизни тут всё посложнее, мы обсудим
некоторые моменты как бы усложнения этой системы,
но вот в самом простом варианте оно так.
Какие есть проблемы?
Ну наверное все увидели, что вот эта красненькая
нода, она одна и если её не будет, то всё, вот поэтому
у нас есть single point of failure и давайте разбираться что
можно с ней сделать.
Как можно решить проблему?
Можно решить проблему тупо, давайте рядом поставим
ещё одну нейм-ноду, они будут между собой синхронизироваться,
одна упадёт, вторая будет жива, всё логично.
Какие проблемы?
Проблема в том, что нейм-нода дорогая и нам во-первых
нужно дорого платить за сеть, потому что нужно гонять
постоянно вот это дерево, оно часто меняется, оно
меняется всё время, потому что хоть какая-то запись,
всё у вас дерево поменялось, надо опять синхронизировать.
Ну и оперативка, оперативки надо много, оперативка
стоит дорого, поэтому это такой самый тупой дорогой
вариант.
Есть вариант не делать бэкапа, но немного помочь
нашей нейм-ноде, вот наверное самый неудачный термин в
литературе про ходу secondary-нейм-нода, потому что она как бы не
secondary, она не вторичная нейм-нода, она просто помощник.
Что этот помощник делает?
Ну вот собственно написано в презе, что он делает, давайте
посмотрим на картинке, то есть ещё раз в нейм-ноде
хранится слепок файловой системы, но для того чтобы
его постоянно не обновлять, не мержить вот это большое
дерево с какими-то изменениями, мы изменения складываем
в какое-то временное хранилище и через какое-то время берём
батч изменений и их отплаиваем сразу.
И вот всё, что делает secondary-нейм-нода, это она делает операцию
берёт слепок файловой системы, берёт эти изменения, мержит
одно с другим и возвращает новый слепок.
Ну мержить каждый раз это будет просто долго, потому
что структура сложная.
Да, вот этот лог хранится, то есть там мы указываем,
к какому файлу эти изменения относятся, но бывает так,
что мы например пишем в один и тот же файл несколько
раз, тогда перед тем как мерзить, мы вот эти 10 записей
в один и тот же файл схлопнем и будет одна запись, но больше.
За это отвечает две ноды, вот name-нода и secondary-нейм-нода.
То есть к этим логам, да.
У name-ноды он есть этот доступ, потому что оно на ней собственно
хранится, а secondary-нейм-нода, как вы видите на схеме, она
делает что?
Так, тут нет никакого указателя, но в общем вы можете прочитать
query for edit logs, то есть с какой-то периодичностью secondary-нейм-нода
запрашивает у name-ноды вот этот файлик с edit-логами,
мержит и отдает назад.
Мержит со слепком файловой системы.
Да.
В name-ноде.
Нет, в name-ноде не про update, в name-ноде есть два файла,
слепок, но старый, и вот эти вот логи, которые надо
апдейтить.
Secondary-нейм-нода собственно их апдейтит, получается
новый слепок, она его возвращает в name-ноду.
Да, и с name-нод сломается все, да, это вот частый вопрос
на собеседованиях, немного вы забежали вперед, что
будет если сломается secondary-нейм-нода и что будет если сломается
name-нода.
Если сломается secondary, то просто вот эту операцию
на себя возьмет name-нода, ничего как бы смертельного
не случится.
Если сломается основная name-нода, то все.
В случае с бэкапом, да, все хорошо, только дорого.
Здесь хуже, но дешевле, потому что secondary-нейм-нода,
она делает одну операцию, ей не надо столько ресурсов.
Но есть еще два варианта, как можно избавиться, хотя
бы помочь избавиться от вот этого SPOV.
Мы хотим еще и разгрузить name-ноду, потому что name-нода,
она у нас контролирует ресурсы HDFS, она хранит слепок файловой
системы, и если она еще и сливает вот эти слепки
с логами, просто получается на нее очень много нагрузки.
Нет, почему, он остается у нее просто.
Откатиться можно, но смотри, а ты имеешь в виду, не можем
ли мы и secondary-нейм-ноды восстановить, если у нас
упала name-нода?
Можем, но не все, и вручную.
Этот слепок устаревший, но что значит устаревший?
Это значит, что часть указателей на блоке уже не работает,
мы их переместили, что-то мы обновили, что-то удалили,
и вот это все мы восстановить не сможем, мы сможем восстановить
какую-то часть.
Ну какую-то часть, да, сможем, вот я знаю, что когда-то
давно, лет, наверное, 7 назад, один мой коллега, работавший
в Яндексе, потратил на это несколько дней полного
рабочего времени, чтобы вот это все восстановить
из secondary-нейм-ноды.
В общем, сделали мы secondary-нейм-ноду, но с single point of failure у нас
никуда не делся.
Какие еще могут быть варианты?
Могут быть варианты HDFS Federation, вот эти, кстати,
вот эти две штуки, они вот прямо в реальной жизни
используются, ну как и secondary-нейм-нода, то есть такого, чтобы не было
ни secondary, ни вот этих двух штук, а была просто одна
нейм-нода, такого нигде, ну, наверное, только в каких-то
очень старых legacy-кластерах можно встретить, сейчас
я такого уже не видел.
Вот, то есть есть HDFS Federation, это когда у нас несколько
нейм-нод и каждый из них берет себе свою какую-то
папку или свой раздел файловой системы.
Есть high availability-нейм-нод, это значит, что у нас несколько
нейм-нод, но в отличие от бэкапа они могут быть
не такие мощные, то есть в основной нейм-ноде нужно
много оперативки, вот этим standby-нейм-нодом им много
оперативки не нужно, они могут все свои данные хранить
на диске, просто если основная нейм-нода отвалилась, то
standby подхватит на какое-то время, будет тормозить,
будет обращаться все время к своему диску, но, по крайней
мере, как-то кластер работать будет.
Да, они периодически синхронизируются с основной нейм-нодой и
забирают слепок.
Чего?
Не может, потому что у нее нет функциональности
отвечать на запросы клиента, то есть, помимо того, чтобы
что-то хранить, должно быть еще API, с помощью которого
будет взаимодействие, там такого API нету.
Потому что они могут отваливаться, то есть, ты можешь одну сделать,
но система как бы реализации HDFS поддерживает несколько,
правда, желательно нечетное число, чтобы был quorum, чтобы
можно было выбрать лидера и одну из standby на время
сделать активной, пока основная лежит.
Тем, что нам здесь не нужна такая мощная оперативка,
мы здесь не стремимся работать быстро, а мы стремимся хоть
как-то работать, то есть, хоть как-то подстраховать
наш кластер, пока нейм-нода лежит.
На счет того, что вот этот слепок, он лежит на диске,
то есть, на все запросы клиента, вот этот standby, он идет
на диск, это сильно дольше, чем идти в оперативку, но
хотя бы какой-то адекватный отклик будет.
Ну да, это как бы медленная замена, медленная, временная
замена.
Вот, теперь поговорим про датаноды.
На датанодах файлы хранятся в виде блоков фиксированного
размера, на самом деле, не всегда он фиксированный,
но об этом позже.
Ну и получается, что так как нод много, машинки будут
падать чаще, и чтобы мы не теряли блоки при каждом
падении, блоки реплицированы, то есть, у каждого блока
есть n-копии.
И эти копии распределяются по машинкам как можно дальше
друг от друга, кстати, почему так, как вы думаете.
Ну есть внутри системы некие данные про топологию
сети, например, что вот у нас такие сервера находятся
в одной стойке, такие в одном датацентре, а такие
где-то георасподелены в разных ЦОДах, и вот система
старается разнести вот эти блоки как можно дальше
друг от друга в разные датацентры, желательно.
А почему он быстрее будет?
Да, и вот так и есть, а насчёт быстрее, ну тут скорее
медленнее, правда есть такой вариант, что, например,
если несколько команд работают с одним и тем же кластером,
то вы просто можете поднять несколько вот этих edge
нод, несколько клиентов, один клиент допустим в
России, другой клиент где-то в США, и вы будете каждый
со своих клиентов ходить на нужные вам ноды, и для
вас это будет быстрее.
Ну да.
На самом деле экстренные ситуации на ходу кластера,
они бывают часто, и цель файловой системы сделать
так, чтобы мы на них не обращали внимания по возможности,
то есть вот если взять вас, у каждого из вас есть
свой нод, свой компьютер, как часто он ломается?
Наверное раз в несколько лет, так?
Вот, чего?
Ну да, сколько-то лет там, 4-5 лет, как вы думаете,
как часто ломаются сервера в больших ЦОДах?
Берите выше.
Нет, пару минут.
Если мы берем большие промышленные ЦОДы, в которых там сотни
тысяч серверов, то вот реально каждые несколько минут приходят
отбивки, там перегрелась карточка, там диск отлетел,
там еще что-то, ну как бы, а понятно, что в таких
больших ЦОДах, когда мы чиним сервер, это обычно
выглядит так, что-то сломалось, мы его выкинули и заменили
на новое, потому что просто нет возможности чинить
все детально, там брать ремапы, диски по целым
суткам, ну такой возможности нет.
Чем это плохо, то, что если у вас один клиент, одна
клиентская машина, а кластер распределен по всей планете,
то доступ до некоторых файлов он будет дольше.
Вот, ну и как раз вот мы на этом слайде видим, что
например блок Ц0 мы разнесли как можно дальше друг от
друга.
Там блок Ц3, вот он в датаноде 2 и больше его нигде нет,
ну где-то он там есть в какой-то датаноде, просто мы его
не видим.
А вот блоки Ц1, они рядышком, блоки Ц5, они тоже рядышком.
Почему?
Потому что в Hadoop работает еще такой процесс, который
называется балансер.
В зависимости от того, как мы его настроим, он периодически
пробегает весь скоп нашей файловой системы и балансирует
ноды, то есть когда он видит, что одна нода перегружена,
на ней очень много блоков, очень много информации,
он переносит эти блоки по другим нодам так, чтобы
все было равномерно.
Поэтому так бывает, что изначально вот этот план
разнести как можно дальше, он немного нарушается.
Да, конечно.
Мы делаем точную копию, вопрос в том, как мы ее
хотим расположить на кластере.
Потому что, смотри, мы это еще обсудим, когда посмотрим
на чтение с файловой системы.
Ну вот, допустим, у тебя есть блок, у него три реплики,
одна в Москве, другая где-нибудь в Европе, а третья где-нибудь
в Бразилии.
Ну вот у тебя та нода, на которой лежит блок в Москве,
она у тебя легла, тебе приходится идти в Бразилию, это долго.
Вот, я сказал, что блоки все одинакового размера,
на самом деле, это не совсем так, и давайте посмотрим,
как это, как бы, посмотрим на деле, как это не так.
Вот, а сейчас мы с вами зайдем на интерфейс Hadoop'a
и посмотрим, что у нас там есть, как все хранится.
Вот, мы с вами зашли на NameNodeUI, и что мы тут видим?
Мы тут видим вот Configured Capacity, то есть сколько вообще
данных мы можем хранить на нашем кластере с учетом
репликаций, бэкапов, всего остального.
DFS used, сколько у нас сейчас хранится, вот половину
кластера у нас занято, non-DFS used, то есть полтерабайта
у нас занято чем-то несвязанным с Hadoop'a, как вы думаете,
что это может быть?
Ну, корзина, если мы имеем дело с файловой системой
Hadoop'a, то корзина, она будет тоже в этой файловой системе,
значит, в Hadoop'a.
А тут получается мы полтерабайта куда-то просто вот, куда-то
использовали.
Куда?
Да, Hadoop'a, он написан на джаве, поэтому тоже часто
на собесах задают вопросы, может ли Hadoop'a жить отдельно
и общат всего?
Ну, конечно же, нет, потому что Hadoop'a на джаве значит
должна стоять джава, значит должна стоять ось, там библиотеки
всякие, логи, и вот на вот это всё суммарно в масштабах
всего кластера тратится полтерабайта.
DFS Remaining, вот мы тут видим тоже, сколько осталось места.
Вот видим, как у нас работают все ноды, сколько места занято
на каждый из них.
Ну, в принципе, тут с точки зрения нод больше ничего
интересного нет, единственное, в каких режимах эти ноды
могут находиться, то есть сейчас они у нас все в режиме
in-service, вот может быть режим такой, что нода упала, может
быть режим in-maintenance, то есть когда мы сами отключаем
ноду для техподдержки какой-нибудь, и статус decommissioned это что
значит?
Это значит, что мы вывели ноду из кластера.
Ну, неважно, списали, не списали, просто она у нас
не является частью кластера, не хранит никакие блоки,
то есть когда мы допускаем процесс decommissioned, это значит,
что те блоки, которые были переместятся на другие
ноды, чтобы сохранился фактор репликации, а саму ноду
мы спокойно выведем, то есть вот это вот разные вещи,
in-maintenance это мы ее просто штатно выключили или штатно
отключили от кластера, но там данные остались, а
вот это полностью мы ее убрали.
Теперь, что касается блока, вот у нас есть возможность
прогуляться по файловой системе, давайте посмотрим,
что у нас тут есть, вот есть различные пользователи,
скоро тут появятся и ваши, они будут начинаться на
PD 2023 и так далее.
Давайте посмотрим на какой-нибудь файл с данными, вот, например,
файл 11,5 Гигов, что мы про него можем увидеть вообще.
Фактор репликации 3, то есть он разбит на блоки, каждый
блок имеет еще две копии.
Блок size 128 Мб, вот тоже можно увидеть.
И дальше вот давайте смотреть, что мы знаем про блок, ну
кроме того, что мы можем его скачать, мы видим ID-шку
блока, видим блокpool, блокpool это некое такое, как бы,
некая такая область, в которой хранятся блоки,
если у нас разветвленная структура, вот действительно
разные дата-центры, разные стойки, тогда эти блокпулы
будут разные.
Но у нас стойка одна, дата-центр один и вообще весь этот
кластер находится у нас там в подвале в КПМ, поэтому
блокпул здесь один.
Это значит, что каждый блок имеет еще две копии
на разных нодах.
Ну, как бы, само понятие блока и копии, у них семантика
одна и та же, то есть если одна из копий недоступна,
ты будешь читать другую копию, они равноценны полностью.
А, здесь написано 128 мегабайт, вообще, в среднем это десятки
или первые сотни мегабайт, то есть примерно вот так
оно и есть, размер блока можно настраивать, то есть
когда вы поднимаете кластер Hadoop, у вас там стандартный
размер блока какой-то будет, а дальше для каждого файла
можно задавать свой.
Дальше, что мы видим еще, Generation Stamp, это некий номер,
по которому Hadoop понимает, какой блок актуальный, какой
нет.
То есть, допустим, у нас есть у блока три реплики,
но в какой-то момент машинка с одной репликой легла,
мы сделали какие-то изменения, а до этой машинки они не
докатились.
Машинка проснулась, говорит, что у меня тоже есть реплика,
но у нее уже другой Generation Stamp, и Hadoop понимает, что
реплика уже устарела, ее надо обновить.
Есть размер блока и написаны ноды, где эти сами реплики
хранятся.
Вот как раз три ноды у нас есть.
Если мы будем переключаться по блокам, то мы увидим,
что у нас меняется.
У нас меняется ID, у нас меняется Generation Stamp, у нас меняются
ноды, но у нас не меняется размер, то есть кажется,
что он постоянный.
Но если мы пойдем в самый последний блок, то, видите,
размер изменился.
Почему?
Потому что, ну, просто размер файла не делится нацело
на размер блока, и последний будет меньше.
Есть еще и случаи, когда последний блок бывает больше.
Когда это бывает?
Это бывает тогда, когда вот этот самый последний
блок, он очень маленький, и Hadoopу нет смысла тратить
ресурсы на его хранение, он просто прикрепляется
к предпоследнему.
То есть если у нас имеются какие-то там первые проценты
от размера блока, если бы тут было не 114, например,
а просто 4, вот такой, если бы был размер, то он бы
скорее всего прилепился к предпоследнему блоку.
Зачем так делать?
Дело в том, что про каждый блок мы храним метаинформацию.
Мы храним ее на одной ноде, на нейм-ноде, в оперативке.
И чем больше у нас блоков, тем хуже нашей оперативки
нейм-ноды, тем быстрее она расходуется, поэтому хранить
маленькие блоки вообще плохо.
И если мы просто положим в Hadoop большой файл, то Hadoop
его сам порежет по блокам, разнесет по нодам, все будет
хорошо.
А вот проблемы начинаются тогда, когда у нас много
маленьких файлов.
Hadoop не умеет слеплевать файлы в один блок, и просто
он делает для каждого файла свой блок.
Вы, например, сделали 100 файликов по 1 мегабайту.
Если бы вы их слепили, у вас бы получился один блок
обычного размера.
Но Hadoop делает вам 100 маленьких блоков по 1 мегабайту.
А вот нейм-нода будет расходоваться так же, как и 100 на 100, как
на 100 блоков по 100 мегабайтам.
Окей, по этой части какие-нибудь вопросы есть?
По блоксайзу, вот тут надо понимать, что такое блоксайз.
То есть, когда вы работаете с обычной файловой системой,
то у вас выделится место на диске определенное.
А здесь у нас как такового диска нет, у нас есть дата-ноды,
есть нейм-ноды.
И вот на нейм-ноде мета-информация, она одинакова действительно
для любого блока, для какого бы размера он не был.
Туда пишутся константы, кто владелец, какая дата-изменение,
какие права доступа, к какому файлу он относится.
То есть, вот такой словарик пишется, и он одинаковый,
там нельзя сделать меньше никак.
А с точки зрения дата-ноды, а какой смысл с точки зрения
дата-ноды резервировать на диске лишнюю память?
Зачем?
Такого не делается.
То есть, физически на дата-ноде будет храниться столько,
сколько ты положишь, а вот нейм-нода будет загружаться
неоптимально.
И на самом деле, за нейм-нодом мы переживаем больше, потому
что это оперативка, и она одна.
В общем, когда мы читаем, мы находимся на клиенте,
и читаем мы в два этапа.
Сначала мы идем в нейм-ноду, спрашиваем, нам нужно прочитать
такой-то файл, где нам взять данные.
Нейм-нода возвращает нам вот эту вот пару, блок ID, блок
location, то есть, она возвращает, какие блоки мы должны прочитать
и где они находятся.
Мы на клиенте выбираем самый ближайший блок по топологии,
идем к нему и читаем.
То есть, вот такие вот два этапа.
Что касается записи.
Запись тоже идет в несколько этапов.
Сначала мы запрашиваем у нейм-ноды, куда нам можно
записать.
Нейм-нода говорит, куда, и мы идем на датоноды.
И датоноды реплицируют данные друг с друга, то есть, мы
не с клиента ходим по каждой ноде, а вот такой каскад
получается, как вы видите на прессе.
Записали на первую датоноду.
Нейм-нода отправила OK в нейм-ноду, что все хорошо,
мы записали, и дальше она уже передает данные на
вторую.
Вторая тоже говорит OK, и она уже передает на третью.
Вот такой вот каскад, но пока этот каскад весь до
конца не пройдет, наша команда записи не заканчивается.
Вот такая репликация называется синхронная.
Есть еще асинхронная, вам про нее расскажет Роман
Липовский, когда будет говорить про кавку.
Там вся соль в том, что репликация по факту не закончилась,
система говорит, что уже все, и мы можем работать дальше.
У HDFS, помимо UI, на которой мы только что смотрели,
есть еще REST API, и сейчас мы как раз через REST API посмотрим,
как работает чтение данных.
Вот такая вот команда.
Что такое REST API, как оно примерно работает, надо пояснять?
Если совсем кратко, то когда мы заходим на какую-то
веб-страницу, мы там видим, что презу видим, какие-то
данные, они как-то там красиво отображаются, но можно сделать
и по-другому, можно эти данные передавать в виде
какой-то служебной информации, в виде джейсонов.
То есть, когда у нас у какого-то сервиса есть набор страничек,
при запросе к которым мы получаем какие-то служебные
данные, плюс еще хорошо, если там есть возможность
передавать параметры, есть возможность авторизации,
то мы говорим, что у системы есть REST API.
Честно, не вспомню, боюсь наврать, но посмотрю.
Вот, ну и здесь мы тоже, собственно, делаем запрос
к вот такой вот веб-страничке, у нее адрес, дальше через
вот эти вот знаки вопроса и структуры типа Keira No Value
мы передаем параметры.
Вообще, есть несколько протоколов, чтобы обращаться
к страничкам по HTTP, наверное, вы знаете, что есть GET-протокол,
вот в данном случае, какой протокол используется.
Да, используется GET, потому что чем характерен GET тем,
что мы передаем плейнтекстом маленькие какие-то объемы
данных и прямо их передаем через адресную строку.
Вот, и так, что мы хотим этой страничкой сделать?
Мы хотим подключиться к NameNode, мы только что на нее
заходили MiptMaster и вот этот порт.
Дальше WebHTFS Version 1, это вот тоже есть такое правило
хорошего тона, что когда мы в системе разрабатываем
Web API в какой-нибудь, мы сразу резервируем версию, вдруг
у нас потом появится версия 2, версия 3, поэтому странички
они содержат еще и версию, но в случае с ходубом версии
2, версии 3 не появилось, поэтому вот версия 1, дальше
путь к данным и дальше параметры, какие.
OP, Operation равно Open и Length равно 10, то есть мы хотим открыть
файл и прочитать 10 символов.
Давайте мы выполним эту команду.
Вот, мы хотели прочитать 10 символов, получилось у
нас что-то не то, все видят, что не то получилось.
Увеличить, наверное, надо.
Я думаю, вам это просто понадобится в будущем.
Есть целая куча кодов ответа разных, у них у каждого
своя семантика, но для обычной жизни, я думаю, достаточно
знать 4 основных класса.
Кто знает, что вот эти классы HTTP ответов говорят?
400, опять 500.
Да, можно такую семантику использовать, чтобы запомнить,
что это значит.
200 проходи, то есть все хорошо, 300 уходи, 400 ты облажался,
500 я облажался, то есть 400 это значит, что проблема
на стороне юзера, как вы сказали, а 500 это вот сервис
500, то есть там что-нибудь на нем не так, обычно пишет
там тайм-аут.
Редирект, ну уходи куда-то, да, вас куда-то послали,
как здесь.
Куда нас здесь послали, нас послали вот сюда, вот
этот адрес, что это за адрес?
Это адрес датоноды, то есть как вот мы только что
увидели на схемке, мы пришли к нейм-ноде, нейм-нода нас
переслала на датоноду, если мы пойдем сюда, то есть
снова qrl-i, теперь мы видим вот это вот слово, это у
нас как раз наши 10 символов.
Ну на самом деле все это можно сделать за одну команду,
надо просто добавить сюда еще минус l большое, тогда
у нас сразу появятся редиректы, мы сразу по нему пройдем.
Вот, как мы вообще можем работать с HDFS?
Давайте я откачусь на вот этот вот слайд, то есть
у HDFS-а есть несколько API, сам HDFS как бы, как часть
Hadoop'а написан на джаве, поэтому у него есть Java API,
есть Web API, мы на него только что смотрели, есть HDFS shell,
то есть всякие команды типа
Вот, то есть HDFS, DFS, минус что-нибудь, вот по ссылке,
если хотите, можете зайти на вики, презы там выложены,
вот пройти по ссылке, можете увидеть описание различных
команд, и вот сейчас увеличу, многие команды, я думаю,
то есть вот cat, chmod, chown, chgrp, cp, du, def, все вот это
вот вы наверное знаете, кто работал с линуксом.
Единственное, что вам не знакомо, это команды типа
copy to local, copy from local, и оно же где-то тут есть get,
и еще где-то тут есть put, то есть это перенести с
обычной файловой системой в Hadoop и обратно, потому
что вот когда мы заходим на клиент, здесь на самом
деле у нас две файловые системы, это легко проверить.
lshom, мы тут видим вот таких вот пользователей,
HDFS, DFS, минус lshom выдаст нам ошибку, то есть такой папки
нет, потому что это совершенно разные файловые системы.
Вы наверное уже заметили, что команды типа HDFS, DFS они
работают дольше, чем команды типа ls, почему?
Понятно, что нам нужно сначала запустить java-процесс,
потом постучаться в нейм-ноду, потом нам вернется ответ,
в общем, это все работает дольше.
Вот, ну, как бы java не все знают, с bash, bash слишком
низкоуровневый тоже работать неудобно, а с REST-опи тем более,
поэтому есть много разных оберток.
Вот, например, несколько библиотек здесь можно увидеть,
вот, например, в этой библиотеке у нас есть
один библиотек, который работает дольше.
И есть еще несколько других библиотек, они посложнее,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
потому что они еще и умеют map-reduce считать,
есть обертки на плюсах, есть обертки на других разных языках,
есть обертки на плюсах, есть обертки на других разных языках,
ну и вот, например, если вы такую команду выполните,
ну и вот, например, если вы такую команду выполните,
вы сможете увидеть, что у вас
команда du, которая в норме выдает одно число,
она выдаст два числа для каждого файла,
она выдаст два числа для каждого файла,
один с учетом реплик, другой без учета реплик.
Вот, теперь уже более такие интересные штуки,
Давайте, мы с вами посмотрим на FSCK.
Давайте, мы с вами посмотрим на FSCK.
Вообще, что делает команда FSCK?
Если не говорить про ходупа,
вообще, что это за команда?
File System Check, да, оно и есть.
Если вы так детально не работали с линуксом,
Если вы так детально не работали с линуксом,
то, возможно, у вас была винда,
и вы на этой винде запускали проверку диска.
Это вот такой синий экран,
который у вас есть в трансмерте,
а тот, который пять шагов с процентами.
Есть такой у вас?
Ну, у кого будет возможность,
дома посмотрите,
там можно запустить проверку диска
любого, который у вас есть в винде,
и по факту будет запускаться тот же File System Check.
и по факту будет запускаться тот же File System Check.
Но если на вашем компьютере
Но если на вашем компьютере
желательно, чтобы у вас был File System Check 100%,
то в ходупе File System Check
далеко не всегда бывает 100%,
это не страшно,
но это позволяет нам поисследовать
File System и понять, что где хранится.
Давайте мы возьмем какую-нибудь папку.
Вот, файл, с которым мы
работали сегодня в начале занятия,
вот он.
То есть, QSCK с параметрами
Minus Files, Minus Blocks, Minus Locations.
Мы узнаем информацию,
где у нас что хранится,
по каким локациям.
Сейчас выдастся большая простыня,
и мы будем в ней разбираться.
Вот она.
Давайте смотреть
с самого начальника,
с самого начальника,
с самого начальника,
с самого начальника,
с самого начальника,
давайте смотреть с самого начала.
Начало, вообще,
когда мы что-нибудь запускаем в ходупе,
вы в этом еще убедитесь,
когда будет MapReduce, Hive.
Начало, оно начинается вот так.
Мы идем в name-ноду.
Дальше, что нам эта команда выдала?
Она выдала, что мы имеем дело с директорией.
В этой директории
у нас есть вот такой файл,
он один, у него такой
размер, столько-то блоков и статус OK.
Дальше.
Вот это вот
0.1.2.3, это информация
про блоки.
Что мы видим про каждый блок?
BlockPool.
BlockID.
Вот это
второе число, кто помнит, что это
с начала занятия.
Версия GenerationStamp,
правильно.
Вот, дальше.
Размер блока.
Количество живых реплик,
три.
И дальше уже идет информация про каждую реплику.
Что за информация?
DataNodeWithStorage.
Мы видим IP-шник датаноды,
на которой все хранится.
ID-шник внутренний,
потому что IP может поменяться.
И нам надо, чтобы
ходуп как-то умел общаться
все равно с этими нодами,
даже если у них поменяется адрес.
Ну и дальше флажок диск,
потому что
на самом деле датаноды можно развернуть
не только на диске,
можно использовать оперативку,
можно развернуть ее
в каком-нибудь облачном хранилище.
Ну вот у нас диск.
Это файлик, который
хранит выборку
со статей Википедии.
Да, он занимает
один этот с половиной гигов
и вот 92 блока.
Но это он совершенно не везде будет
занимать 92 блока.
Вот у меня есть параллельный кластер,
на котором я тестирую разные задачки
для курса.
Там такие же файлы, но вот этот файл
например занимает 367 блока.
Потому что там другой размер блока.
Вот дальше мы тут видим
вот эту всю информацию.
В конце мы видим,
что вот у нас поменялся
размер блока.
Так, дальше статистика.
Статистика по блокам.
Статистика по блокам в каком-нибудь
аномальном статусе, например
underreplicated,
то есть упала нода, реплик меньше,
чем мы хотели бы.
Misreplicated.
Там реплика недоступна
или вообще у нас все реплики потеряны.
А есть еще
overreplicated. Как вы думаете,
в каких случаях может быть overreplicated?
Все так.
Ну вот и в конце
мы видим, что наша файловая система
из healthy.
Можно также
посмотреть инфу для любого блока.
Возьмем блок.
Вот этот допустим.
Ты имеешь в виду вот это то,
что в квадратной скобке здесь указано?
Да, это каждая копия.
Вот у нас, видите,
live-replex 3.
И вот у нас 1, 2, 3.
Ну и обратная ситуация,
когда мы увидели блок,
выполнили для него FSTK
и видим, какому файлу он относится.
Вот на каких нодах хранится
его реплики.
Все это мы тоже видим.
Healthy это значит,
что у нас все в порядке.
То есть статус здоров, нет никаких ошибок.
На самом деле в реальной жизни
спокойно может быть unhealthy.
И это тоже ничего страшного.
Да, у блока да.
То есть вот здесь, вот в этой команде
статусов много. Вот даже мы тут видим
over-replicated, thunder-replicated
и так далее.
9 рабочих нод,
еще плюс мастер и клиент.
Ну вот мы с вами будем такие команды
еще изучать на семинарах.
Вот мы видим,
что у нас доступны вот эти ноды
и кто-то что-то на них даже
запускает.
Пока мы не перешли дальше,
у меня будет к вам небольшой вопрос.
Вы наверное знаете, что есть команды head
и есть команды tail в линуксе.
Head выводит первый n-строг, tail
и последний n-строг. В HDFS
такие команды тоже есть. Давайте проверим.
HDFS, DFS
minus tail
file
У нас вывелась одна строка.
Тут много написано, это просто строка такая
длинная.
HDFS, DFS minus head
не работает.
Вопрос в том,
почему разработчики
HDFS и Hadoop
не сделали head, а сделали tail?
Через cat?
То есть ты мечтал вот так,
что мы через pipe передали и
а что нам мешает
точно так же сделать cat, потом tail
через pipe?
Прочитать все.
А у нас тут может быть 5 терабайт.
И придется 5 терабайт лопатить,
пока мы до конца не найдем.
Да, все именно так.
Давайте тогда
мы с вами решим
вот такую задачку.
Ее часто решают
сис админы, когда нужно
спланировать кластер Hadoop.
То есть к нам
пришел какой-то заказчик
и хочет на нашем кластере
сохранить данные.
Мы им должны сказать,
в каком виде мы должны
эти файлы хранить
с одной стороны.
Вот эти минимальные объемы оперативки,
при каких условиях он будет минимальный?
Как нужно упаковать вот эти вот
данные 2 питабайта, чтобы он был минимальный?
Ну и какой он будет?
А можно еще раз
что мы сначала делаем?
Так.
А где, по твоему, будет храниться
метаинформация
и сам блок?
Так нам именно
на неймноде и надо,
потому что на датаноде
у нас понятно, что объем всех дисков
2 питабайта.
Нет, 2 питабайта
это диск.
А нам нужно посчитать,
сколько мы хотим купить оперативы.
Так.
Как это посчитать?
Еще раз.
А зачем мы прибавляем
размеры метаинформации?
Она где хранится?
Ага.
6,4 на 3 умножить
или поделить?
Так.
Потому что на этих 2 питабайтах
должен помещаться не только сам блок,
а и все его реплики.
Поэтому 6,4 на 3.
И вот 2 питабайта делим на 6,4 на 3,
получаем количество блоков.
У каждого блока метаинформация 600 байт.
Домножаем, вот получаем.
Потому что у каждого блока
метаинформация 600 байт.
Это вот данные про блок.
Вот.
600 байт нам нужно, чтобы хранить
не только все 3 реплики,
чтобы хранить вообще какую-то информацию
про этот блок, чтобы NaimNode понимало,
к чему этот блок относится.
Логически.
Логически.
То есть с учетом всех физических кодей.
Да.
Да, да, да, да. Все так.
У нас осталось еще немного времени.
Я вам пока что покажу,
как работает с HDFS на питоне.
Сейчас я проверю,
установлен ли у нас,
установлен это либо или нет.
Потому что
на дисках, которые 2 питабайта,
должен храниться не только
сам блок, а и его копий.
У каждого блока
есть три копии.
Где их хранить?
У нас кроме вот этого диска,
на каждом блоке
есть 3 копии.
Не всё так же, вот.
Потому что у всех vantageков,
Где их хранить? У нас, кроме вот этого диска 2 питабайта, больше ничего нет.
Значит, они должны там поместиться.
Мы храним информацию о всех копиях, но просто она хранится в одном месте, и вот эти вот 600 байты-то она и есть.
Так, секунду.
Да, где лежит, кто владелец, какие права доступа, все вот это вот.
Ну, скорее не совсем так, просто ссылка на файл.
Давайте тоже посмотрим, пока у нас есть время, как оно вообще хранится, всем кратко.
Для этого нам надо зайти на ноду.
Вот, мы находимся на одной из нод.
Я сразу возьму себе root-up, потому что там часть файлов будет недоступна под обычным пользователем.
Так или иначе, нода это у нас просто сервак под линуксом.
Но в этом серваке под линуксом есть папка dfs.
В папке dfs есть dn, датонода.
Дальше есть current, это вот папка с блоками.
Блок pull, он у нас один.
Снова current.
И там еще должна быть папочка finalized.
Вот, в finalized лежат блоки.
Такой вот странной структуре из списка подпапок.
Можно сделать вот так.
И мы увидим, что у нас бывают подпапки вложенные.
Вложенность в глубины бывает и 2, и 3, и 4.
Бывают пустые.
В общем, вот эти вот сабдиры, на самом деле, только нейм-нода может разобраться,
что соответствует этим сабдирам.
То есть сама датонода, она не знает, чему это соответствует.
Да, в обычной файловой системе.
Но есть вопрос, почему мы не резервируем место на диске, когда у нас блок маленький.
А зачем? Мы просто положили файл на ноду.
Также интересно посмотреть на мастер.
На мастере у нас все хранится в оперативке, но система, по крайней мере,
на этом кластере настроена так, что периодически бэкапится на диск.
У вас будет домашка по HDFS первая.
Там нужно будет немного походить по нодам.
Но в остальном вам больше, кроме первой домашки, это делать не придется.
Но если не будете администратором Big Data инфраструктуры, то, наверное, не придется.
Вот, тоже current.
И у нас тут есть два типа файлов.
Вот мы их видим.
fs-image и edits.
Мы с вами сегодня начали занятия с того, что у нас
name-нода хранит слепок файловых файловых.
И у нас есть два типа файловых файловых файловых файловых файловых файловых файловых.
Мы с вами сегодня начали занятия с того, что у нас
name-нода хранит слепок файловой системы,
edit-логи. Вот у нас edit-in-progress.
Это значит, у нас еще файл с логами не закрыт, не сохранен.
Просто в него кто-то...
Пока мы еще в него пишем.
Если мы сейчас будем как-нибудь изменять файловую систему,
то на логи запишутся сюда.
Что касается Python, для того, чтобы у нас работал HDFS CLI,
нам надо, чтобы
в нашей home-директории был вот такой файлик.
Вам на семинарах тоже покажут.
Вам нужно будет такой файлик себе сделать
в вашей домашней директории.
Он будет у всех одинаковый, будет отличаться только последняя строчка.
Теперь запускаем Python.
Вот.
Что это такое? Мы импортировали config.
Config — это специальный класс, который содержит данные
про файловую систему.
Что-то он берет из самой системы, что-то берет из этого файла.
То есть просто наш Python-процесс понимает, куда ему идти.
Дальше мы создаем директор.
Дальше мы создаем клиент.
Сейчас будет понятно, зачем он нам нужен.
Config get client.
Дальше client, например, list.
Вот. Вот такую команду можно выполнить,
и мы увидим список вложенных файловых папок.
Или вот так, например.
Это статус по папке.
То есть такой fsck в урезанной версии.
Вы могли заметить, что библиотека на Python
почему-то работает сильно быстрее, чем команды
типа HDFS, DFS- что-то.
Как вы думаете, почему?
Ну так это же библиотека более
высокого уровня, по идее.
Оптимизирует.
Понятно, что как-то оптимизирует.
Вопрос как?
Если мы хотим открыть команды HDFS, DFS- что-нибудь,
они запускают Java.
На то, чтобы запустилось Java, надо время.
Потом они эту Java убивают,
потому что мы не храним state.
Если бы мы хранили state между командами,
пришлось бы эти Java процессы держать
открытыми постоянно.
Это накладно, поэтому нам просто приходится
каждый раз писать полные пути.
Мы не можем в HDFS сделать CD.
Мы просто не можем перейти между папками,
потому что вот это состояние, где мы находимся,
надо где-то хранить.
Для этого надо держать Java процесс открытый.
Поэтому мы каждый раз стартуем Java процесс,
каждый раз его стопаем, и на это тратится куча времени.
Здесь мы вот, собственно, вот этот клиент
и этот конфиг мы его создали.
В этот момент у нас стартовал Java процесс.
И дальше мы уже с готовым Java процессом
внутри него играемся.
Вот кто проходил курс по базам данных,
вам, наверное, рассказывали, что есть коннекшены
в базе данных, и можно, когда вы работаете с BDS,
сделать более примитивно,
когда вы на каждое подключение
делаете коннекшен, потом его убиваете.
А можно использовать коннекшен пулы.
То есть набор открытых коннекшенов,
которые сами поддерживаются, сами переоткрываются,
если упали.
Ну, в частности, вот кто ходит на курс по Java,
вам там расскажут про спринг, он использует именно такую модель.
Вот тоже мы ускоряемся,
за счет того, что мы не тратим время на создание коннекшен.
На самом деле,
вот эту вот проблему с тем,
что мы не можем перейти между файлами,
между папками, эту проблему пытались решать,
и ее решили.
Вот у нас на кластере стоит
одна интересная библиотечка,
вам ее покажут на семинаре,
но она очень дырявая.
Тоже семинаристы вам расскажут,
что, пользуясь этой библиотекой,
можно вообще получить рута внутри ходупа.
Не внутри сервера, а внутри файловой системы HDFS
можно получить рута.
Таких дырок в ходупе, к сожалению, много.
Просто когда разрабатывался ходуп,
цель была создать систему, которая будет отказоустойчивой,
надежной в плане работы с данными,
но мы не думали о безопасности,
так детально не думали.
Получилось, что довольно много дырок,
и в реальных бизнес-системах
как минимум кубернетис,
а керберас, прошу прощения,
используется керберас, чтобы сделать хотя бы авторизацию.
В новой версии сделали то,
что более удобная связка с керберасом.
То есть керберас уже прижился
и просто он используется в связке с ходупом.
В принципе, этого достаточно,
керберас для авторизации
и закрыть все лишние порты,
потому что этот порт, который я вам даже сегодня показывал,
этот 50070,
если к нему подключиться, если он будет открыт наружу
то сможет без какой-либо авторизации
выполнить запросы
из разряда того, что мы сегодня делали.
Только, например, не open, а delete.
Можно просто дропнуть все файлы, которые есть в ходу.
Поэтому порты надо закрыть,
керберас в реальной жизни надо поставить.
На этом мы с вами теоретическую часть
про HDFS заканчиваем,
и все, что мне осталось сказать,
это сделать такой обзор литературы,
что можно про HDFS почитать подробнее.
Нет.
На кластере закрыты максимально порты,
поэтому если кто-то что-то сделать нехорошее,
то мы будем знать, что это вы,
и убедительно просим так не делать.
Порт только SSH открыт,
все, больше ничего не открыто.
Это все-таки не очень удобно для бизнеса,
потому что иногда нужно зайти на WebMod,
иногда нужно подключиться к какому-то порту напрямую,
не пробрасывая...
Мы с вами можем сделать SSH туннели,
а иногда SSH туннели делать сложно,
если мы какую-то большую систему запускаем,
которая связывается с Hadoop.
Что касается обзора литературы,
что в действительности делает secondary name-node?
То, что я вам объяснял, только более подробно текстом.
Дальше, статья про архитектуру HDFS.
Когда-то, где-то года два-три назад,
существовала такая компания, как Hortonworks,
она производила систему
для автоматизации работы с Hadoop.
То есть, некий пакетный менеджер
для Hadoop, Aspark, Hyvo
и всего, что в экосистему Hadoop сейчас входит.
Помимо этого, ребята ввели блок,
но в какой-то момент, в 2020 году,
компания Hortonworks была куплена
более крупной компанией Cloudera
и этот блок перестал существовать.
Осталась китайская копия
на этом сайте, поэтому, кому интересно,
посмотрите. Здесь детально описано,
что хранится в datanode, в каком виде,
что хранится в name-node.
Мы с вами видели все эти сабдиры, блоки.
То есть, все детально описано,
что эти папочки хранят.
Дальше, статья Константина Швачко,
она такая чисто научная,
без каких-то деталей про файлы.
Сейчас подъявка догрузится.
Там именно с помощью текста показано,
как работает HDFS, почему именно так.
Статья от Константина Швачко
это один из довольно частых
коммитеров в репозитории Hadoop.
Так, страничка не работает.
Видимо, надо зайти под VPN.
У кого есть VPN, попробуйте зайти.
Тут просто страничка
с очень хорошим конспектом
курса про Hadoop на курсене.
То есть, в 2016-18 годах
команда МФИТА,
которая работает,
делала курс на курсере.
И вот один из слушателей нашего курса
сам по своей инициативе написал
очень подробный конспект занятия про HDFS
и выложил его в GitHub.
Дальше книжка Hadoop
за Definitive Guide.
Глава третья там про HDFS.
На этом на сегодня все.
Есть ли какие-то вопросы?

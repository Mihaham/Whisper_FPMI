Так, давайте потихоньку начнем. Все равно уже по времени надо. В начале все равно
пару слайдов про мотивацию. То, что мы сегодня будем обсуждать, они в некотором смысле не такие
важные, хотя показывают полную картинку того, что сегодня будет происходить. Все помним,
рассматривали много раз этот градиент. Вроде бы с ним никаких проблем не было, но теперь такой
вопрос возникает. Пусть, соответственно, есть у меня мои эти иксы. Я как-то хочу измерять между
ними расстояние. До этого я всегда это делал в евклидовой норме. Здесь ввожу какую-то произвольную
норму. Так, такой вопрос. Банок и пространство у вас уже были на МКН? Да, были. Супер. А сопряженное
пространство? Не было. Ну ладно, я в принципе на справедливости ради рассчитывал, что эту лекцию
должна быть перед предыдущей, но посмотрел, что электро у вас еще не успел рассказать. Думал,
что успеет, но не успел. Последний он вам рассказывал про что. Там не было того,
что мне нужно было. Есть банок и пространство, ничего сверхъестественного нету. Внутри
просто норма, и это норма причем еще и полная. Главное то, что есть норма. Я как-то хочу измерять
расстояние между, соответственно, своими иксами. Исходя из этого, что я могу вообще сказать про мой
градиент f от x-катова? Вроде как градиент это же некоторые операторы, которые действуют на мой
x и как-то его преобразуют. И на самом деле, в общем случае, никто вам не гарантирует то,
что оператор, который вам подействовал на x, отобразил его в то же самое банок и пространство,
в котором лежали x. И получается, что когда мы делаем вот такой вот градиентный спуск,
мы складываем x из одного банка и пространства, градиент с другого банка и пространства,
и получаем вообще не пойми, что, если это вообще можно сложить между собой адекватно. Но на самом
деле ничего сверхъестественного нету. Я думаю, на Функане вам все четко ведут, как выводится,
соответственно, сопряженная норма в сопряженном пространстве, в е со звездочкой. И вы на самом
деле для себя откроете много чего интересного, потому что не зря мы тоже проходили сопряженные
нормы. Потому что для каких-то обычных норм, типа нормы P, где, соответственно, P вас может принимать
от значения от 1 до бесконечности, сопряженная норма будет нормой Q. Причем P и Q связаны с
отношением вот таким вот. То есть, например, для первой нормы будет норма бесконечности сопряженной.
Для второй нормы, и что хорошо, сопряженной нормой будет она же сама. То есть, вы видите из
этого соотношения, что для евклидовой нормы сопряженной будет она же. И в этом плане,
когда то, что мы делали с вами до этого, хорошо, потому что когда мы работали с евклидовой нормой,
мы попадали в то же самое баноково пространство, когда считали градиент. Ну и, соответственно,
все операции были валидны, когда мы складывали и получали X из того же банокового пространства.
Но теперь, соответственно, у нас появилась мотивация. Кто мне сказал, что только с евклидовой
нормой можно жить? Потому что все остальные нормы, они же тоже вводились в некотором смысле
естественным образом. То есть, евклидовая это просто вариант так считать шар, на такой шар
смотреть на другой. Например, если я хочу померить расстояние между двумя вероятностными
распределениями, то, конечно, я могу это сделать евклидовой метрике, но как-то неестественным это
кажется, для распределения так мерить. Есть более натуральные физичные вещи, которые позволяют
это делать. Ну и, соответственно, от евклидовости хочется как-то отходить, как-то, соответственно,
переходить к произвольным нормам, которые могут возникнуть. Хочу измерять расстояние. Ну и,
соответственно, этим мы сегодня и займемся. Я надеюсь, что на фонкане я вам дополню эту картинку и
покажу действительно, что к чему у нас сопряжена норма. Оптимизационный вариант сопряженности
фонкановский у вас схлопнутся, и картинка закроется. Сейчас это нужно просто для мотивации, просто
потому что исторический метод, который мы будем обсуждать, был изобретен именно так. То есть мы
поняли, что если у нас есть пространство, не обязательно, что градиент лежит в нём же. Ну,
соответственно, что? Возникает следующее. Давайте тогда, раз x не лежит в этом Баннхам пространстве,
я веду просто какое-то отображение, которое у меня в это Баннхова пространство эти x и будет
переводить. Тогда теперь у меня x-катый лежит в Баннховом пространстве, e со звёздочкой,
вот градиент тоже там, ну и, соответственно, получается, что всё тервалидно, но теперь я сделал
как бы шаг градиентного спуска в другом Баннхам пространстве, так называемом зеркальном пространстве,
вот поэтому как бы и методы будут называться зеркального спуска. И мне бы нужно вернуться в
исходное. Понятно, что если я веду как-то это отображение довольно хорошо, то я, соответственно,
по обратному отображению просто вернусь в исходный Баннхова пространстве и получу x-катая плюс 1. Вот вся
идея, она такая вот, функа не стоит, довольно общая и выглядит довольно сложно, потому что метод,
ну вот он так как-то записывается, ну вот если еще обратное отображение применю, но выглядит не
user-friendly. Справедливости ради, ну вот результат получается конца 70-х годов, Аркадий Семёнович
Немировский, Дмитрий Юдин, советские математики, вот, и в новых лекциях, которые вот Аркадий
Семёнович уже считает в штатах, он, например, уже не рассказывает, ну я не видел в конспектах,
может, на самих лекциях он это рассказывает, именно зеркальность свою, почему метод,
который они придумали в конце 70-х, называется зеркальным спуском, они просто его вводят и все. А вот
эту физику, которая у них шла на самом деле от Баннхова в пространстве, они, ну не рассказывают
Аркадий Семёновичу, не знаю почему, просто может потому что довольно абстрактно. Ну вся книжка,
классическая по оптимизации Немировского Юдина, она довольно жесткая из-за того,
что получилось сильно абстрактно. Ну ладно, мы от абстракции отойдем и как раз пойдем к тому,
как мы будем измерять расстояние, обоим измерять в следующем образом. Сначала давайте введем
определение сильно выпуклости, и здесь ничего не меняется по сравнению с тем, что мы делали в
Евкалида 2-м случае. Функция называется сильно выпуклой, только единственное, что здесь норма
теперь стала произвольная. Раньше стояла 2, здесь теперь произвольная норма. Определение
сильно выпуклости. Ну я напоминаю, что в курсе мы рассматриваем, что эти определения мы вводим
только для каких-то выпуклых множеств. Хорошо, так, и теперь вот ключевой объект сегодняшней лекции,
с которой мы на самом деле уже знакомились ранее. Здесь он будет определяться чуть-чуть по-другому.
Это дивергенция Брегмана, которую мы с вами вводили для функций. Но теперь мы эту дивергенцию
будем вводить также для функции некоторой D, которую только-только что мы определили,
сильно выпуклой. Но теперь она будет нужна для измерения расстояния. До этого это нужно было
скорее для измерения расстояния как вспомогательный критерий, а здесь это будет уже такой полноценный
измеритель расстояния. Вот смотрите, соответственно, функция D у нас. Один сильно выпуклая.
Дивергенция Брегмана вводится вот следующим образом. Ну вот такое вот расстояние. Пока вещь немного
абстрактная, на примерах посмотрим, что действительно имеет место. Хорошо. На самом деле
определение дивергенции Брегмана можно давать по-разному. Нам нужно будет сегодня именно один
сильно выпуклая для одной сильно выпуклой функции. Как мы помним, раньше мы давали это определение
просто для выпуклой функции. Никто не запрещает. Поэтому так. Давайте посмотрим на примерчиках.
Вот моя дивергенция Брегмана. Давайте я ее попытаюсь породить функцией D, которая равна,
я включаю норме в квадрате пополам. Какую дивергенцию Брегмана при этом мы породим? У кого-то,
может быть, сразу есть ответ. Ну давайте еще по определению сделаем. Косинус. Ну давайте посмотрим.
Не совсем косинус получится. Давайте смотреть. По определению пишу. Просто ничего сложного.
Градиент это будет просто y, а здесь будет x-y. Вот так вот. Давайте одну вторую скобочки вынесу,
еще у меня останется x-квадрат. Вот только y-квадрат. И здесь что у меня? y-x удвоенное,
и здесь у меня плюс удвоенный y-квадрате. Вот что получается. Одна вторая x-квадрате.
Так. Запись остановилась, что ли? Нет, вроде идет. Вот. Ну что это? Что получилось?
Нарбораздности? Конечно.
То есть, по факту, дивергенция Брегмана в евклидовом случае это просто евклидовое расстояние.
Ну с одной и второй дополнительной. Вот. Окей. Да, соответственно, порождает нам евклидовое расстояние в квадрате.
Вот. Более такой, что ли, специфичный пример. В первую очередь, на самом деле его называют таким вот
ключевым примером использования дивергенции Брегмана. Это вот, соответственно, энтропийная
функция, которую мы рассматриваем на вероятностном симплексе. Вот. На вероятностном симплексе.
Вот. Я не знаю, может быть кто-то уже знает. Что может породить энтропийная функция?
С чем она связана обычно? В теории информации возникает распределение. Ну симплекс – это как раз
распределение вероятностного. Вот. Энтропийная функция у нас порождает дивергенцию кульбакалебдера. Вот.
Такую прям классическую вещь, с помощью которой измеряют расстояние между двумя распределениями.
Это ровно о том, что я говорил. О том, что, в принципе, по евклидовой норме измерять расстояние между
распределениями как-то неестественно. Вот KL-дивергенция считается действительно
таким классическим вариантом измерения расстояния между двумя распределениями. Вот. И оказывается,
здесь все действительно корректно, потому что неравенство Пинскера гарантирует, что энтропийная
функция будет 1 сильно выпукло и на симплексе относительно первой нормы. Вот. Поэтому действительно
такой D мы можем порождать дивергенцию Брегмана, которую мы сейчас рассуждаем. Ну неравенство Пинскера
можно найти в интернете. Оно ровно об этом и говорит, что энтропийная функция на симплексе 1 сильно выпукло.
Вот. Хорошо. Здесь еще более специфичные примеры, какие можно порождать дивергенция Брегмана. Ну вот,
например, третий примерчик. Это в некотором смысле обобщение предыдущего. До этого у нас как бы был
вектор распределений. Здесь насматривается распределение. Ну это из-за того, что там клантовые
вещи рассматриваются. Вот. И вот так вот это все безобразие обобщается. Вот. Это мы рассматривать
это особо сильно, конечно, не будем. Вот. Вторую будем хорошо сегодня рассматривать. Да. Два остальных
примерчика. Ну, чтобы просто посмотреть, что действительно есть какие-то красивые вещи, которые
это все зашиты, в том числе вот такие какие-то там сильные модные молодежные, типа клантовые. Вот.
Ну и называется прикольно. Клантовая дивергенция фонеймана, как в мультфильмах. Гипер-дупер,
не знаю, там галактика зеленого, ну что-то такое. Вот. Ладно. Окей. Давайте по свойствам пробежимся,
какие есть вообще у дивергенции Брегмана. Одно сразу же очевидно, это симметричность. Когда мы
смотрим на KL-дивергенцию, тут видно, что x и y между собой явно находятся в неравном положении,
потому что симметричности явно относительно x и y нет. Если поменяем местами, это станет
абсолютно другое расстояние. Вот. Дальше есть, соответственно, сильная выпуклась,
которую мы предположили для D, и она нам дает довольно хорошее свойство. Вот такое вот. То,
что у нас дивергенция Брегмана не просто положительная, не отрицательная. Вот. Она еще и ограничивается
снизу одной-второй x-y в квадрате, причем здесь норма, опять же, не эвклидовая, а произвольная. Но
это следует ровно из определения сильной выпуклости, потому что, если мы глянем на
определение дивергенции Брегмана, вот здесь выражение написано, если мы перелеснем на предыдущие
и перенесем вот эти два члена вправо, то как раз мы получим, что дивергенция Брегмана больше
либо равна, чем, соответственно, mu пополам x-y в квадрате, но mu у нас равно единице. Просто потому,
так как мы взяли один сильно выпуклый фонус. Вот. Тоже важное свойство, которое мы сегодня будем
использовать. Дальше, что не отрицательность, уже обсудили. Вот. И довольно такое неочевидное и,
ну, в некотором смысле неприятное свойство, хотя с которым не особо нам придется взаимодействовать.
Вот. То, что на самом деле дивергенция Брегмана, в общем, в случае не является выпуклой по второму
аргументу. Там для Евклидова норма может быть все и хорошо, но в общем случае ничего хорошего нет.
Вот. То есть, ну вот такое неочевидное. С одной стороны, именно с теоретической точки зрения,
расстояние. Способ измерения расстояния. Вот. Не симметричный, еще и не выпуклый. Вот. Но при
этом довольно физичный. Довольно физичный для некоторых частных случаев. Сегодня, соответственно,
с этим всем безобразием и взаимодействием. А симметричность, это только для Каэль дивергенции или для всех?
Нет. Ну, смотрите, опять же, для Евклидовой ее нет. Ой, она есть. Вот. А для Каэль уже нет. Ну вот,
если мы глянем, например, на те примеры, которые там есть еще. Например, сюда. Вот здесь тоже нету.
В третьем случае тоже нет. Ну, потому что это вообще не Каэль. В четвертом случае тоже нет. То есть,
в общем случае симметричности тут нет. Ну, это, на самом деле, кажется, не особо страшно. Потому
что, да, каких-то хороших свойств нету. Но при этом есть и физичность, которая на самом деле много
будет чего нам вытягивать. Вот. Понадобится нам следующее свойство для дивергенции Брегмана. Это
так называемое равенство параллелограмма или теорем Пифагора тоже в литературе ее называют для
дивергенции Брегмана. Немироски, например, это называют просто magic property. Брегман divergence.
Ну, как угодно это можно называть. На самом деле, это довольно простое свойство. Мы его сейчас с вами
спокойненько и докажем, просто используя определение дивергенции Брегмана. Вот. Я туда же не особо буду
сам что-то писать, хотя ладно, можно как писать. Первая строчка. Просто определение дивергенции
Брегмана. Первый. Вот. Подставляю точки. Просто dz, dx. Ну и, соответственно, разница. dx умножить на
градиенты dx. Вторая строчка. Вторая строчка. Определение v, x, y. Дивергенция Брегмана в точках x,
y. Вот. Тоже все понятно, все четко. Выписано определение. Дальше с ними делать небольшая алгебра.
Что тут, соответственно, что-то можно поуничтожать. Например, увидеть, что x тут одинаковые. Вот. Дальше
что. Дальше я выделю вот здесь вот из y. Добавлю сюда умный 0 минус z плюс z. Вытащу вот этот кусочек.
Вот у меня он здесь вытащился. Вот. Ну и там в силу того, что теперь вот эти вещи схлопнутся,
потому что здесь x минус z, а там z минус x. Вот. Здесь возникнет вот такое вот. Такое вот скалярное
произведение. Ну это ровно то, что нам нужно, потому что вот в этой строчке написано дивергенция
Брегмана z, y. Вот. Минус то скалярное произведение, которое у нас и было в условиях свойств. Вот.
Окей. Такое довольно простое свойство, которое нам нужно потом будет использовать при доказательстве.
Хорошо. Вот. Теперь как раз переходим к методу, которая называется методом зеркального спусков. Вот.
Решаем мы с вами задачу опять же. Безусловная оптимизация. Целева функция выпукла. Множество,
на которое мы оптимизируем, выпукло. Вот. Здесь как бы все ограничения, которые есть, они вносятся во множество x.
Соответственно, на слайде написана итерация метода зеркального спуска. То есть выглядит довольно
страшновато, потому что мы понимаем, что дивергенция вещь не самая простая. Вот. И как вот этот
аргуменимум считать по факту, на что вы нуждают делать. На каждую итерацию говорят, ну давай-ка вот
ты посчитаешь градиент и потом отрешаешь этот аргуменимум. Который пока непонятно, как решать,
на самом деле, потому что это же дополнительная задача оптимизации. Вот. Которую, ну в общем случае,
видимо, придется решать каким-то численным методом. Вот. Который будет вносить дополнительную
неточность. Но это кажется сложным. То есть вроде как выглядит итерация просто, в одну строчку записали,
все и радуются. Вот. Но аргуменимум вещь не самая очевидная. Вот. Мы прекрасно знаем, что часто этот
аргуменимум в каких-то хороших частных случаях может расписаться аналитически. И так, на самом деле,
сейчас и будет, но на первый взгляд довольно сложно. Вот. Давайте посмотрим, что же будет, когда у нас...
Мы породим нашу дивергенцию Брегмана. Евклидова норма в квадрате пополам. Вот. Кто-то может сразу
предугадать, что может получиться в данном случае. Здесь я сразу напишу то, что у нас. Мы знаем,
что у нас дивергенция Брегмана. Это просто Евклидовар в стене в квадрате. Так. Ну что, давайте посмотрим,
что там получается. Тут на самом деле не так все сложно. Посмотрим на скалярное произведение. Так
как у меня аргуменимум. Так как у меня аргуменимум. Вот. Я двоечку вот здесь домножу на двоечку и то и другое.
Домножаю на двоечку и здесь, соответственно, пропадает. Там двоечка появляется. Получается вот как-то вот так.
Так. Х. И здесь у меня х-хк в квадрате. Вот. Чем вообще всегда хороший аргуменимум,
это то, что мы туда можем добавлять много чего, что не зависит от х. Вот. Ну я здесь давайте добавлю
вот еще вот такое вот выражение. Так. Оно от ха не зависит, поэтому на аргуменимум не влияет,
потому что аргуменимум просто возвращает значение оптимальное. Ну х, на котором достигается
оптимальное значение, но не значение функции. Да, на функцию это влияет. Вот. На что похоже? На что
похоже становится? Вот это выражение. Квадрат суммы. Квадрат суммы. Да. То есть чего-то все равно не
хватает чуть-чуть. Вот. И это чуть-чуть я добавлю. Опять же. Потому что оно будет не зависеть от х. Вот это
я добавлю. Вот так вот я это добавлю. Опять же на аргуменимум это все не влияет безобразие. Вот. И тогда
что у меня здесь получится? Кто-то может проверить у меня, что здесь все получается ровно вот так.
Вот. Согласны? Вот. То есть тут вот как раз у меня вылезет х ката минус х. Вот. Норма в
квадрате, норма в квадрате. Все. Это так четко получается. Вот. Поэтому вот этот аргуменим,
эквивалент на аргуменимуму х на множестве х, где мы соответственно делаем вот что-то вот такое.
Ну я пока дописываю какой-то метод быстренько говорим. Так. Какой? Простой вроде вопрос. Обычный
градиентный спуск что ли? Да. Это обычный градиентный спуск только с чем? С проекцией. Так?
С проекцией. То есть видно что происходит. Мы взяли х. Сделали шаг. Так. Ну например посчитали
какую-то точку у. А дальше что? Эту точку у мы спроекцировали. Все. Градиентный спуск
с проекцией. Если бы соответственно у вас бы х равнялся просто всему р. Вот. Тогда у вас было бы
просто вот. Градиентный спуск. То есть на самом деле вот то, что здесь написано, в Евклидовом случае
это просто обычный градиентный спуск с проекцией или нет с Евклидовым. Ну понятно с Евклидовой
проекцией. Вот. Или без нее, когда у вас соответственно решается задача безусловная. Ну в любом случае.
Получается, что ведение диригенция Брегмана просто обобщает нам результаты нашего градиентного
спуска, с которым мы с вами уже неплохо так повзаимодействовали. В том числе когда мы с проекцией
делали. Хорошо. Вот. Но над чем хочется подумать еще? Это конечно здорово, что мы опять пришли
градиентно на спуску, но вроде как мы и не за ним приходили, потому что с ним мы уже как раз более
чем разобрались. Вот. Хочется подумать над тем, что возникает вот в диригенции Брегмана. То есть
что она даст в общем случае. А можете потом тогда скинуть еще презентацию с вот этими вашими записями?
Да, хорошо, хорошо. Спасибо. В принципе, записи-то они во многом дублируются на слайде. Вот. Вот.
Тут вот то, что мы сводили вот этому спуску, оно вроде не дублируется или там дальше будет? Ну хорошо.
Без проблем. Без проблем. Так. Ладно. Давайте в общем случае, это диригенция Брегмана, это же не
просто дефигевое расстояние, а что-то более хитрое. Вот. Ну давайте выписывать. Вот. И вообще с точки
зрения аргуминиума, как раз диригенция расписала, с точки зрения аргуминиума тут можно оставить только
dx-градиент dxk на x. Вот. Ну и давайте я возьму и скажу, что у меня x это просто все пространство
Rd. Вот. Я вот здесь вот его замажу на Rd. Вот. Тогда этот аргуминиум можно найти просто из самого
простейшего условия оптимальности. Градиент функции, у которой мы ищем минимум равен нулю. Условия оптимальности
супер простые. Ну то есть там понятно, мы знаем и для более сложных вариантов мы здесь. Я вот просто
запишу вот это. Условия оптимальности. Так. Градиент d в точке x со звездой, ну x со звездой имеется вот как
раз веду. У нас потом это будет подставлен как xk, а то плюс 1. Вот. Минус градиент dxk. Это все равно нулю. Вот.
Ну вот я здесь это подставлю. Ровно это же выражение получилось. Вот. Ну что? Есть ли какие-то в некотором
смысле флешбеки с первых слайдов? А они на самом деле здесь есть, потому что я вот чуть-чуть
переписал и получилось вот так. То есть тот зеркальный шаг градиентного спуска, который предлагали
Немировский Юдин, где мы просто вводили вот эту функцию phi, которая у нас отображала x в сопряженное
пространство. Вот. И получался зеркальный спуск. Вот она та же самая идея, только здесь вот,
когда мы ввели конкретную дивергенцию Брегмана, у нас соответственно что появилось? У нас четко
появилось что такое phi. Вот. И в данном случае phi это просто вот градиент вот этой порождающей
функции дивергенции Брегмана. Вот. Ну и здесь те же самые идеи, которые мы в принципе уже
обсудили выше, излагаются про то, что у нас соответственно это отображение, что здесь соответственно
там сначала отображаем x в сопряженное, делаем шаг, получаем вектор сопряженного и обратно
возвращаемся с помощью обратного отображения. Вот. На самом деле вот опять же это в некотором смысле что-то
общее. В жизни все будет проще, потому что вот тот аргминиум, который нам нужно отрешивать,
он либо имеет аналитическое решение, сегодня мы только с такими будем взаимодействовать,
ну с одним мы уже взаимодействовали и получили метод проекции градиента. Вот. Либо соответственно
отрешивается это методом оптимизации, ну можно бы с хорошей точностью отрешать просто этот
аргминиум и считать что ошибка там, она не попится. Ну либо учесть в ошибку. Вот. Ну действительно,
если там точность решения хорошая, то ошибка не внесет большого вклада, чтобы аргминиум отрешивать
не точно. Вот. Хорошо. Переходим как раз к доказательству сходимости метода зеркального
спуска в произвольном случае. Вот. Для этого нам понадобится гладкость. Ну гладкость, ну мы от нее
еще ни разу не отказывались. Вот. Но здесь, как вы понимаете, в силу того, что мы теперь измеряем
расстояние не в евклидовой норме, гладкость тоже должна перетерпеть свои изменения. Вот. И в силу того,
что у нас градиент теперь живет в сопряженном пространстве, то он измеряется в сопряженном
пространстве расстояния уже в сопряженной норме. В сопряженной норме, причем как я и говорил,
это действительно те сопряженные нормы, которые вы там рассматривали на семинарах, когда вы говорили
про сопряженные по фентилю функции. Вот. В этом плане, я надеюсь, на функционе эта картинка реально
схлопнется до конца. Вот. Поэтому, смотрите, когда мы рассматриваем, здесь просто вот было до этого 2,2,
теперь стало норма со звездочкой, в ней мы измеряем градиент, расстояние между градиентами. Вот. И,
соответственно, здесь норма P, где мы измеряем расстояние между ексами. Ну вот такое вот общение.
Как вы понимаете, уже обсуждали, что вторая норма сопряженная, она же сама, поэтому в Euclidean случится
здесь тоже все прекрасно работает. Окей. Свойства, которые мы с вами доказывали для гладких функций,
для гладких функций. Ну давайте я его в некотором смысле проскочу, потому что мы его уже делали,
то есть что там делалось. Мне формула Newton-Levnitsa записывалась, потому что сейчас делается ровно
те же самые шаги. Дальше в интеграл заносился, потому что не зависит от tau, и, соответственно,
вычитался вот это скалярное произведение, вот оно здесь появилось и здесь в двух местах с разными
знаками просто лумный ноль добавляется. Вот. Дальше что делалось? Дальше вставился модуль справа и
слева, просто потому что нам нужно вот это выражение рассмотреть по модулю, вставился модуль. Дальше
в силу того, что модуль суммы меньше суммы модулей, модуль заносился под знак интеграла. Вот. А дальше
единственное отличие, которое нам нужно будет сделать по сравнению с тем, что у нас было,
это использовать обобщенный вариант Коши-Буниковского-Шварца. То есть мы с вами работали,
что x, y меньше либо равен x и в клидовую норму y и в клидовую норму. Вот. Можно, соответственно,
обобщить этот вариант на то, что у вас для одного из векторов норма p, для второго нормы,
соответственно, q, которая сопряжена с p. Ну такой вариант, соответственно, вариант Коши-Буниковского-Шварца,
разницу градиентов, поэтому этот модуль, который у меня здесь, я расписываю как разница градиентов в
норме со звездой. Ну и соответственно, разница по аргументам, это просто по этой норме. А дальше,
соответственно, что? Я могу использовать спокойно мое определение теперегладкости, потому что оно
мне как раз дано было, когда разность градиентов учислялась по сопряженной норме. Вот. Я пользуюсь
гладкостью и соответственно, что у меня убилось? Убилось здесь как раз у меня возник y-x в квадрат,
просто умножено tau, как раз в норме p. Она вылезла у меня. Вот. А интеграл потом берется и вылезает
одна вторая. Получается ровно то же самое свойство, которое у нас было в Евклидном случае. То есть
просто обобщение, опять же, того предыдущего свойства, которое мы раньше с вами рассматривали. Вот. А я,
кстати, задам такой вопрос, может быть, кто в курсе, было ли у вас это вообще или нет. Как
вообще между собой соотносится норма? Ну вот, например, есть у меня первая норма, а есть вторая.
Евклидова. Что я могу сказать? Я измеряю x. Какую-то норму x. Какой-то знак я могу здесь поставить.
Вообще могу или нет? Не было, да? Тоже не было. Ну ладно, дойдем. Никакой знак тут не поставить,
потому что если взять параболу и модуль, то в каком-то месте будет одно выше, а в каком-то
другое. Или я фигню говорю. Ну подумайте, подумайте. На самом деле, знак тут четко ставится.
Назвала, что или пространства вложены в друг друга. Вот, это, кстати, хорошо. На самом деле,
в конечном мерном случае, мы, конечно, знаем, что нормы эквивалентны. То есть можно использовать
любую норму, а другая и снизу и сверху. Например, норма какая-то P, она, например, с двух сторон,
просто важна константа. Ограничиваться, например, какой-то нормой. Давайте буковка T, пусть будет T.
Тут важны эти константы C1 и C2. А так, в принципе, в конечном мерном пространстве всегда выполнено
вот такое для любых норм. В принципе, вы можете пользоваться одной, зная, что в другой тоже все
будет хорошо, просто потому что выполнены такие свойства. Но, на самом деле, там все равно есть
соотношение между нормами, в частности. Ну вот такое соотношение, что у вас чем больше, как называется,
степень или порядок нормы. Как это было? Даже не знал, как это называется. Ну, порядок нормы.
Ой, господи, что я написал? Тут неправильно. Вот так. Чем порядок нормы больше, тем, соответственно,
она измеряет что-то более-менше. Ну, тут видно, на самом деле, когда норма бесконечности, это просто
максимум среди X, модулей X. Если у вас вектор состоит из одинаковых, например, элементов,
например, из единичек, то норма бесконечности вам вернет единичку, а норма, соответственно,
вторая, вам вернет корень из D размерность вектора. У вас как раз будет сумма D и вы возьмете корень из них.
Ну, то есть понятно, где корень из D раз больше. На самом деле, вот это все строго можно показать
для произвольных норм, что если у вас там P меньше Q, то тогда у вас норма Pt больше либо равна
нормы Qt. Ну ладно, это не вопрос сегодняшней лекции. Просто премиум это когда-нибудь.
Вот, хорошо. Так, ну возвращаемся к доказательству. Разобрались, как гладкость меняется, какое
свойство из нее вытекает. Нам бы нужно теперь разобраться с самим зеркальным спуском, потому что
если мы, конечно, нам нужно использовать саму итерацию метку, чтобы что-то подоказывать. Давайте
попробуем выписать условия оптимальности для вот этого арга минимума. Мы просто минимизируем
какую-то функцию. Как будут выглядеть условия оптимальности в общем случае, если бы, например,
здесь стояла какая-то функция g от x. Как бы мы выписали условия оптимальности? Здесь x.
Вы производите набло нулевое. Набло нулевое? Серьезно? У нас множество x, которое выпуклое,
и оно не обязательно Rd. Я здесь его положил Rd, просто чтобы была понятна связь с тем,
что было у Немировского с Юдиным. А так это произвольное множество. Simplex, например,
ограниченное множество. Как там будет? Договорились. Пусть будет так. В данном случае
у нас их звездочка, она сразу перетекает в x ката плюс 1, и соответственно это все можно вот так
записать. Здесь у нас соответственно x и с нашего множества x. Для любого x это должно выполняться.
Вот, как-то так. Хорошо, отлично. Это мы разобрались. Условия оптимальности проходили,
когда у нас множество, это произвольное. Я его здесь записал тоже. Вот, пожалуйста.
Условия оптимальности.
Так, и дальше. Здесь я просто подставил функцию g, потому что мы уже с вами поняли,
что там под аргумением у нас стоит какая-то функция f от x ка x g от x равно плюс d от x
минус nabla d x ката x. Ну и соответственно градиент g точки x ка плюс 1 у меня будет просто равен
nabla f x ка плюс d x ка плюс 1 nabla минус nabla d x ка. Вот, он здесь в принципе и реализован.
Вот этот вот. Хорошо, теперь смотрите, над чем поработаем. Так, вот с помощью нашего
магического свойства про дивергенцию Брегмана попробуем разобраться вот с этим безобразием,
которое у нас написано на слайд. То есть nabla d x ката плюс 1 минус nabla d x ка умножить на
x ка плюс 1 минус x. Вот. Отдельно я могу его выписать. К сожалению, на слайде не поставил паузу.
Давайте попробуем вот с этим всем разобраться, чему это будет соответственно равно. Откройте magic
property на слайдах и посмотрите. Продиктуйте, чему это будет мне равно. Разность дивергенции Брегмана.
Ну вот мне интересно каких. Там понятно, будут три дивергенции Брегмана. Вот. Мне интересно,
чего они будут? Что там за аргументы-то будут?
x ка плюс 1, x, x, x ка, x, x ка. Так, так будет, да? Наверное. Ну я не знаю, я вас слушаю. Ну я
говорил x ка плюс 1, x. x ка плюс 1, x, по-моему. Ну есть у меня подозрения, что это не так. Ну давайте
попробуем дальше. Дальше что? Плюс v от x. от x. x ка. x ка. Минус v от x ка плюс 1, x ка. Вот так вот,
значит, получилось. Ну, точно со знаками нигде не напутали. Вот. На самом деле ответ похожий.
Вот здесь он выписан. То есть вот это было правильно. Вот здесь надо было поменять местами. То есть
аккуратненько просто это свойство расписать. Вот. Мы можем это сделать заново, чтобы было четко
понятно, что мы здесь нигде не ошиблись. Вот. Что мне здесь соответственно? В качестве хочется
подставить в качестве x, а x ка плюс 1, x ка плюс 1. В качестве z, x. В качестве y, x ка. Дальше,
соответственно, что я должен выписывать? z, x. x. x ка плюс 1. Вот как раз. z, x. Выписал. Дальше,
соответственно, x, y. x это сколько? x ка плюс 1, y, x ка. Ну и в последний z, y это x, x ка. Все. То есть,
вроде, все четко. То, что мы сделали. Здесь все правильно. Так. Хорошо. Дальше, соответственно,
взяли вот это, взяли гладкость. Взяли вот это, взяли гладкость. С предыдущего слайда гладкость тоже
мы с вами доказали. Я здесь просто свойство гладкости подставил конкретные точки. Конкретные
точки x ка плюс 1 и x ка. Вот. Дальше, соответственно, вот это все безобразие домножается на гамма.
На гамма. Зачем это делается? Чтобы, соответственно, вот они, эти гаммы совпали
здесь. Вот. То есть, все безобразие домножилось на гамма. И, что видно, хорошо будет получаться.
Градиент здесь, видите, одинаковый. Градиент здесь одинаковый. И точечка даже одна одинаковая.
Точечка даже одна одинаковая. Поэтому, соответственно, когда я вот сложу эти два
скалярных произведения между собой, когда я сложу два этих скалярных произведений между собой,
у меня что будет получаться? Гамма градиентов от x ка. Ну и здесь еще у меня, соответственно,
минус. И это с плюсом, и это с минусом. Вот здесь он минус стоит. Вот. Что здесь тогда будет получаться?
Здесь будет получаться. Х ка. Минус х. Вот оно.
Так. Дальше, соответственно, вот это все перенесено просто из гладкости. Ну и дивергенс они тоже болтаются.
Вот. Ну и это опять же тоже перемысл. Ничего тут сверхъестественного не происходит. Ну давайте
тогда дальше листик добавлю и дальше ручками заведу. Хорошо. Здесь мы, соответственно, пришли
вот к чему-то вот такому. Главное, чтобы он вот откопировал. Нет, только линию откопировал.
Так. Ну давайте, ладно, здесь будем смотреть и доводить. Так. Справа, соответственно, у меня что
останется? Справа у меня останется f x ка. x ка минус x гамма. Вот. А слева у меня будет гамма
l пополам x минус x ка плюс один x ка в квадрате. Справа у меня там еще оставалось гамма f x ка плюс
один минус f x ка. Плюс там, соответственно, была дивергенция Брегман. Если вправо перенести,
будет x x ка минус v x x ка плюс один. Так. Окей. Давайте доведем. Что здесь, соответственно, будет? Во-первых,
хочется воспользоваться выпуклостью для вот этой части f x ка x ка минус x минус минус f x ка. Как
я это могу оценить по выпуклости? Как я это могу оценить по выпуклости? Кто-нибудь понимает,
как можно это оценить по выпуклости? У нас как раз же выпуклость. Две точки. Ну, возможно, меньше
либо равно минус f x. Все правильно. Да, выпуклость же всегда как можно представить? Вот у вас,
соответственно, что у вас есть функция. Она подпирается. У вас есть какая-то текущая точка x ка.
Дальше вы смотрите, что там происходит. Градиент в этой точке есть. И вы смотрите на какую-то точку x.
Вот. Ну, здесь, соответственно, что у нас должно получиться? У нас должно
f от x ка плюс x минус x ка. Оно как раз у нас меньше либо равно, чем f от x. Ну,
мы, соответственно, домножаем на минус единицу, потому что здесь обратные вещи. Здесь, соответственно,
остается то, что у нас как раз было наверху, и знак меняется. Вот. Отлично. Соответственно,
да, это у нас в эту сторону. Поэтому вот то, что у нас было до этого, мы просто можем по выпуклости
поменять. Здесь у нас, соответственно, что вместо скалярного произведения и вот этого f ки встает f ка в точке x.
Хорошо. Хорошо. На самом деле уже очень близко, потому что мы когда-то... Что-то забыл. Забыл, забыл, забыл.
Забыл. Я, соответственно... Там же у нас было три норм. Эти... Дивергенция еще была вот такая.
Вот. Я ее подзабыл. Такая вида. Хорошо. Смотрите. А теперь давайте вспомним хорошее свойство
Дивергенции Брегмана. Что мы про нее знаем? Как она подпирается снизу?
Как, соответственно, разность х? Отлично. Тогда... Это же означает, что если знак просто поменяю,
то у меня будут вот так вот. Вот. Я вот эту Дивергенцию Брегмана, соответственно, сейчас таким вот образом и оценю.
Так. Дивергенцию Брегмана мне оценивается вот здесь вот. И можно заметить, что они вот как раз вот с этим будут схлопываться.
Здесь она будет вылезать одна-вторая. Это с плюсом будет. Вот так.
Так. Ну что? Что осталось-то, в принципе? Что осталось, в принципе?
Когда мы доказывали, например, просто в выпуклом случае, что он считается то же самое, что r было.
В дыхе тут на расстоянии k минус расстояние x в квадрате здесь тоже было. xk плюс 1 минус x в квадрате.
Но это явно мешается. Ну, с помощью гаммы это все безобразие и убивается. Вот.
Вот гамма берется меньше, чем один делить на l. Вот. И это все здесь уходит, просто становится меньше нуля.
Вот. И остается что-то очень хорошее.
Так. Ну и все. Дальше делается стандартный трюк. Суммируем, усредняем.
Вот. Здесь, когда мы, соответственно, будем суммировать, все соседние будут уничтожаться между собой.
Вот. И мы будем, и мы получим в итоге 1 делить на k vx x0 минус vx xk.
Договорились? Договорились. Вот. Это у нас вещь не отрицательная, поэтому меньше либо равна нуля, нулему я можно и оценить и убрать.
Хорошо. Чуть-чуть осталось. Для этого функция выпуклая. Применяем неравенство енсу.
Меньше либо равно, а гамма еще забыл, vx x0. Только там под знаком суммы вроде как f не нужно. Ой, да-да-да, конечно-конечно.
Все. То есть средняя точка. Здесь давайте вот так вот опозначу. Средняя точка минус f. Сразу звездочку можно поставить.
Меньше либо равно, чем vx звездочка x0, гамма k. Можно поставить гамму как 1 делить на e или, соответственно, у вас сразу же.
Результат. Все. Супер. Отличается от градиентного спуска или нет в выпуклом случае, кто помнит, кто читал пособия?
Отличается или нет? Что мы в итоге получили? Мы оценили разность в функции v, ее раньше не было. В градиентном спуске вроде бы.
В градиентном спуске здесь стояла просто норма в квадрате. А мы сейчас норму вводим как v или что?
В это у нас суть семинара то, что мы как раз меняем измерение расстояния с нормой Евклидовой на v. Вот. И здесь, соответственно, оно и возникает.
Я не успел. А когда мы сказали, что новая норма, это будет дивергенция Брегмана?
Это не норма. Еще раз, там она не является нормой. Это просто некоторые измерительные расстояния.
Для нее многие свойства норм не выполняются, в общем случае. Но как измерять расстояние, оно более чем.
Это некоторый объект, у которого есть 2x, и мы можем измерять между ними расстояние с помощью дивергенции Брегмана.
А метрика, она это является?
Нет. Вот. Вот так вот.
Соответственно, в некотором смысле проблемы, с другой стороны, как бы, часто здесь проблемы не видно,
потому что физичность, которая есть в частных случаях некоторых, она закрывает все огрехи.
И как раз именно с теоретической точки зрения о физичности результатов, дальше хочется поговорить,
но давайте сделаем перерыв 5 минут. А дальше продолжим.
Давайте продолжать.
Теперь я предлагаю обсудить, зачем мы в это завязывались.
Изначально обсуждали, что у этого всего есть геометрия задачи на другую,
и хочется эту геометрию как-то учесть.
А проявляем это как-то в теории. Давайте смотреть.
Что мы получали, например, для градиентного спуска с евклидовой проекцией?
Евклидовой проекции.
Там мы, соответственно, получали, ну, даже можно отсюда это все выразить,
потому что мы поняли, что это частный случай.
l2 это константа гладкости, когда у нас евклидова норма в определении используется.
Здесь, соответственно, у нас x0, x звездой в квадрате,
евклидова норма делить на k и еще на 2.
Хорошо.
А сейчас, соответственно, вот у нас получилось l,
дивергенция Брегмана, x звездой x0 делить на k.
Я гамму поставил просто равным 1 делить на l.
Вопрос. Лучше-хуже это получилось или нет? Или вообще разницы нет?
Это вроде кашка одна и та же, то есть сублинейная сходимость.
Но числитель разный.
Но какой из них хуже, какой из них лучше? Какие мысли?
Давайте просто посмотрим на вот это определение.
Вот.
И про то, что я уже в некотором смысле чуть-чуть затронул,
то, что у нас норма...
Давайте я вот так напишу.
Это пусть будет qt-норма, это пусть будет pt-норма.
Они связаны вот таким соотношением единицы.
И давайте вообще первой возьмем равные единички,
тогда, соответственно, у равного бесконечности.
И пусть у нас будет 1,
тогда, соответственно, у равного бесконечности.
И пусть у нас вот здесь норма бесконечности,
здесь стоит, соответственно, первая норма.
До этого мы, в принципе, писали что-то очень похожее.
Мы писали вот как-то так.
Вторая норма l.
Ну, тут разность х было тоже по второй норме.
Вот. Как я уже сказал, в принципе,
у нас норма бесконечность,
она меньше второй нормы.
Причем, уже обсудили, в худшем случае,
там превосходить может в корень из n раз.
В корень из d раз или d размер.
То есть, когда норма бесконечности
просто максимум по модулю,
вторая норма это сумма квадратов,
корень из суммы квадратов.
Соответственно, здесь у вас просто возвращают
максимальные элементы,
одинаковые, например, единичка,
а вторая норма вернет корень из d.
Аналогично, на самом деле,
вещь справедлива и для
второй нормы и первой.
Что понятно, вторая норма меньше,
чем первая норма.
Но опять же, смотрите, во второй норме,
если мы возьмем вектор из одинаковых,
из всех единичек, то он нам вернет
корень из d, как раз на то, что мы обсудили.
А первая норма это сумма модулей.
Он нам вернет d.
То есть, здесь и здесь,
в лучшем,
в крайнем случае,
это разница в корне из d.
Понятно, что на каких-то
нулевых векторах это разницы
между ними нет.
То есть, с одной стороны,
они одинаковые,
то есть, коэффициент единичка,
а в худшем случае в d раз
больше правая будет.
Поэтому, когда мы вот здесь
пишем вот такое,
кутэ норма,
на данном случае мы уже рассматриваем
бесконечность,
а здесь стоит вторая норма.
То есть, вот здесь слева стоит что-то больше.
Потому что вот здесь вот
q.
Так?
Хорошо.
А здесь справа,
смотрите, вторая норма,
а здесь норма,
тут один был,
первая норма.
Больше второй.
То есть, что получается?
Вот здесь, когда мы делаем вот это,
мы какой-то,
ну, давайте будем говорить,
вторую норму, оцениваем вторую норму.
А здесь мы что-то,
в лучшем случае, значительно
меньшее, чем вторая норма,
оцениваем
через что-то,
что может быть значительно больше,
чем вторая норма.
Понятная идея, да?
Санта L2
может быть значительно больше,
чем L.
Угу.
Понятно почему?
Я надеюсь, что да.
Хоть какой-то
сигнал,
что двигаться можно дальше.
Вот.
Хорошо.
Смотрите, с другой стороны,
есть дивергенция Брегман.
То есть, мы поняли, что L нам идет в плюс.
Вот L-ки нам идут в плюс.
L у нас будет лучше.
Давайте вот здесь сотру,
здесь L-ка идет в минус, а там нам в плюс.
С другой стороны, дивергенция Брегман.
Мы про дивергенцию Брегмана знаем,
что она,
ну, звездочка,
неважно, там Y.
Одна вторая, X, Y.
Причем не обязательно во второй норме.
То есть, тут произвольна норма.
Если мы, опять же, рассматриваем, например,
П эту норму,
вот, П равную единице,
то вот это будет
больше либо равно, чем
Евклидова норма в квадрате.
Вот. То есть, вот именно вот с этой точки
зрения правой оценки
дивергенция больше,
чем Евклидова норма в квадрате.
То есть, вот здесь вот это идет в минус,
а уже норма будет в плюс.
Понятная идея, да?
Можно еще раз?
Что мы вообще сейчас
пытаемся сделать, чем мы занимаемся?
Мы хотим понять,
какой метод будет сходиться быстрее.
Вот у нас есть оценка сходимости.
Что нам нужно? Столько вот столько итераций.
Вот. Вот она оценка.
Ну, вот для нее я выписал там
эту гамму подставил.
А здесь, соответственно,
нужна для градиентного спуска
с проектов. Какая из них больше
непонятна. Я хочу сравнить.
Вот. Ну и сравниваем.
Что мы именно сравниваем?
Мы для разной гаммы поставляем
и сравниваем?
Нет, смотрите. У нас есть два частных случая.
Ну, например,
градиентный спуск с проекции,
который на самом деле укладывается тоже
в ней
в сетап зеркального спуска.
Но для него мы оценку
и так получали вот.
Вот. А есть произвольный случай,
когда у нас там
обязательно Евклидова норма
порождает дивергенцию Брегмана.
Вот получается такая скорость
сходимости, такая гарантия сходимости.
Я хочу понять, какая из этих оценок
вообще лучше. Вот. Стоит ли
вообще было связываться именно вот
исходя из этих оценок
с зеркальным спуском?
Вот. Если просто право оценка
всегда хуже, то зачем она нужна?
Ну, именно вот с точки зрения теории.
Вот.
Ну и вроде как разобрались,
что константа L в левой оценке
хуже.
Вот.
И что измеряет расстояние,
в левой оценке лучше.
Ну вот и в этом игра.
Ну и сейчас в частном случае
увидим, что
эту игру можно выиграть.
И правая оценка будет лучше вот эта.
То есть мы хотим получить такую же
скорость сходимости либо лучше,
чем у градиентного спуска?
Конечно, конечно. Мы же зачем-то
зачем-то это все это начали.
Вот. Абстрактные слова про то,
что да, мы учили геометрию задачи,
это, конечно, хорошо. Вот. Но хотелось бы
их увидеть в реальности.
Ну точнее как в теории. В теорию, чтобы
они как-то проявились. Вот.
Тогда будет
супер-хорошая мотивация
это исследовать в реальности. Вот.
Рыфро, а можно тогда их распечатать?
Почему у нас справа константа L лучше,
чем слева?
Смотрите. Потому что
вот норма бесконечность, вот здесь
она меньше, чем
вторая норма.
Меньше либо она, чем вторая норма.
То есть вот это что-то
маленькое, а здесь
это что-то большее.
И наоборот справа стоит здесь
что-то большое по сравнению вот с этим.
То есть условно, давайте вот здесь, например,
это
1, это, например, пусть будет
x.
Ну пусть будет d. Давайте вообще
напишу. 1 здесь будет d.
Вот это, например, в d раз больше
корень из d раз больше
и это корень из d. Вот.
Понятно, что вот здесь вот будет
L2 равно единичке,
вот.
А здесь будет L равно 1 делить
на d. Понятно?
Примерная идея.
То есть вот это что-то большое.
То есть мы корень из d применяем норму
и корень из d применяем норму.
А слева мы почему-то сравним
норму единицы и норму d?
Ну, смотрите, давайте
более формально. У нас есть
что-то, давайте вот у нас есть
A.
Это норма бесконечности.
Норма бесконечности.
Здесь, соответственно, она меньше
B, где B у нас, соответственно,
норма
Евклидова. Есть у нас
C.
Это норма 1.
Вот так у нас все
расположено.
Ну и что мы, соответственно, делаем?
Мы вот здесь, вот у разницы градиента,
вычисляем норму бесконечности.
То есть у какого-то...
Давайте я лучше вот так напишу.
Вот так. A-B это для градиентов.
И A-B
здесь B большое.
Здесь у нас, соответственно, вторая
норма, здесь первая.
Вот.
Как-то так.
И что мы здесь, соответственно, оценим? У нас есть
норма бесконечности, вот здесь.
Норма бесконечности, которая A.
Вот. Мы хотим вот так вот
сделать через B.
Так? Вот она, B.
Вот она, A.
Тут разность градиентов просто было стоять.
Здесь, соответственно, разность х.
То есть вот отсюда вытечет
B меньше L2
на A.
Вот.
Ну тогда почему, например, как можно, например,
понять, какой порядок у L?
Ну давайте A делить на B.
Вот это отсюда.
А вот отсюда будет
у L2 порядок
B делить на A.
Вот. Ну и теперь смотрим, как они соотносятся.
A меньше B.
A меньше B.
Вот. A. A большое
меньше, чем B большое.
Вот. Здесь, соответственно,
на A маленькое мы умножаем.
Соответственно, если мы умножаем на меньшее число,
то становится меньше.
Если мы делим на большее число,
то становится тоже меньше.
И получается, что L меньше,
чем L2.
Меньше либо ровно, чем L2.
Понятно?
Ну...
Ну вообще, почему
L2
больше, чем L? Понятно.
А почему вообще рассматривали
такие нормы? То есть норма бесконечность,
L норма 1.
И норма 2, L2 норма 2.
Потому что, например, в случае
симплекса у нас как раз
энтропия выпукла относительно
первой нормы, и там как раз
расстояние измеряется в первой норме.
Но мы используем дивергенцию Брегмана
в нашем зеркальном спуске.
А сопряженная е – это норма бесконечности.
Соответственно, там для градиентов
разность будет измеряться в сопряженной норме.
Это норма бесконечности.
А, соответственно,
разность по х
будет измеряться в первой норме.
Ну а Евкрида в случае-то тот,
который мы рассматривали до этого.
Просто вот такой есть.
Стандартный, базовый.
Сейчас мы как раз просто перейдем
тоже к симплексам.
Я покажу, откуда берется эффект.
Что действительно там
константы лучше.
Это Немировский в свое время тоже показывал.
Так что
скорее оттуда уже все взято.
Понятно, да, в чем идея?
В каких-то частных случаях может быть
что L сильный.
Но по дивергенции при этом можно проиграть.
Будем смотреть через частный случай.
Это мы с вами обсудили.
То, что мы сейчас так.
Я хочу, соответственно,
рассмотреть все же дивергенцию Брегмана,
которая равна
дивергенции Кульба Калейбенера.
Все это мы рассматриваем на симплексе.
То, что все это корректно,
мы с вами поняли.
По пинскеру она действительно
энтропийная функция
11 сильно выпукла.
Хочу найти
решение этого аргуменима
в явном виде. Здесь у меня x теперь
это симплекс.
Ну множество понятно.
x' больше 0, сумма
x' равна 1.
Распределение вероятности.
Все вероятности больше 0
и сумма равна 1.
Записываю в стандартном виде.
Ограничение вида неравенств
плюс ограничение
равенства равно 0.
Плюс моя целевая функция, которую хочу решать.
Здесь, соответственно,
можно уже поставить rd.
Так как я в ограничение,
все это уже внес.
Хорошо.
Задача с ограничениями.
Давайте писать лагранжи.
Пу-пу-пум.
Так.
Гамма,
f,
xk,
x,
v,
x,
xk.
Лагранжи.
У меня минус,
так как здесь минус вытащится.
Лямда и x.
Плюс, соответственно,
сумма хитых минус 1.
Все вроде норм.
Согласны?
Должно быть правильно.
Здесь я сразу еще тоже подставлю,
что это сумма хитых
логарифум
хит
делить на
хитк.
Хк это просто фиксирована
какая-то вещь,
с прошлой итерации x.
Я подставил вместо диургенции брегом.
Вот выражение, я его подставил.
Хорошо.
Более-менее
разобрались.
Тут можно заметить, что функция
будет сепарабельно
по каждой группе перемен.
Потому что скалярное произведение
это просто что
гамма,
f, xк,
it компонента,
в данном случае констант
xу оптимизируем, а xк это фиксировано.
Плюс, соответственно,
диургенция брегмана,
которая тоже является
сепарабельной по каждому из x.
Минус, соответственно,
вот это,
лямбда и иксы.
Плюс, соответственно,
это сумма хитых минус 1.
Согласны?
То есть что?
Каждый слагаем, у меня виды суммы,
для каждого члена
зависит только от x этого.
А явный вид, это мы что хотим получить?
Ну что такое явный вид?
Аргуминиум
решать сложно.
Я хочу тупо формулу, как для градиентного спуска.
Вычти градиент, получи ответ.
Вот.
Ну и xкt плюс 1.
Я хочу то же самое. Выведи мне просто формулу
так, чтобы я просто подставил у него числа
и получил xкt
плюс 1.
Вот такое хочется получить.
Ну а пока записит аргуминиум,
который в общем случае решается численно.
Ну методом оптимизации дополнительно.
Вот, что-то вы писали
в изобразии вот такое.
Поэтому еще раз, по каждому иксы
можно минимизировать отдельно.
Ух, что-то я паузу не наставил.
Ладно.
Так.
Давайте по каждому из иксы
будем минимизировать
отдельно.
Так.
Или я это здесь...
А, ну и ладно.
Давайте вот здесь в принципе.
Я это выражение выписал.
Я это выражение выписал.
Как раз в виде суммы
все это занес под одну сумму.
Теперь у меня вот есть каждое
выражение, которое можно по икситам
найти инфим. Здесь я просто ответ
выписал. Соответственно, давайте
попробуем получить, что этот ответ
верный.
Верный.
Ну, ничего тут сложного нет.
Как найти минимум функций?
Давайте возьмем, опять же, условия
оптимальности. В данном случае-то
мы же на инограничном множестве уже
инфимы берем. Поэтому берем просто
условия оптимальности и записываем его.
Логарифм дифференцируем одномерный.
Что там получится?
1 делить на иксы
x и kt
плюс вот этот...
Что там? Из логарифма еще будут
вылезать.
А нет, там же как?
Логарифм на x.
Тогда у нас что останется?
Почему мы дифференцируем?
По какой переменной?
По x. По иксу.
И тому.
Я хочу, соответственно, инфимам искать.
Поэтому ищу условия оптимальности.
Тогда там будет просто
1 делить на иксы.
Логарифма?
Нет, тут же вот коэффициент есть.
Ну так мы же
разве не должны потом еще продиференцировать
эту штуку как сложную функцию?
Откуда достанется этот коэффициент и он сократится?
Ну вот, я вроде бы все это и сделал.
Вот, да?
Как раз они мне сократились.
И еще вот здесь x болтается.
Вот здесь x болтается.
Я его приписал,
потому что у меня как раз тут произведение
логарифма на x. Я продиференцировал
логарифм, но x у меня остался.
И вот это они заболтались
и в итоге единичка только и осталась.
Вот, дальше что я делаю?
Как раз когда логарифм дифференцировал, а x не дифференцировал.
Теперь обратно.
x дифференцировал, логарифм оставляет.
Так. Ну и дальше там
все то, что осталось болтаться.
Градиент.
Лямбда и клюню.
Вот.
Отсюда...
Тут важно, что эта единичка болтается.
Вот это.
Отсюда надо просто
выразить x и t.
x и t при условии,
что это равно 0, выражается
x и t.
Выражается x и t.
Вот. Это x и t получается...
Ну что?
Вот этот кусочек давайте
с обозначу просто.
А там еще и...
С плюс 1.
С плюс...
Минус 1, минус с переносится вправо.
Берется от этого всего
безобразие экспонента.
Вот. И слева, соответственно,
остается x и t делить на x ка т.
Вот. Откуда получается,
что x и t оптимальное
равно x и t ка т
экспонента 1, минус с.
А можете еще раз
показать, откуда у нас логарифм взялся?
Там вроде с предыдущего слайда.
Логарифм это
с дивергенцией Брегмана взялся.
Вот. Дивергенцией Брегмана.
Вот.
Окей.
Нет, подождите.
А откуда
логарифм там взялся?
Где?
Где? Сейчас.
Как вернуться-то?
Ага. Так.
Бум-бум-бум.
Где? В дивергенции Брегмана,
где он взялся?
Да-да-да.
Давайте мы ее найдем в явном виде.
Просто времени на это все, конечно,
уже тратит. Что там у нас?
dx
минус dy
минус
градиент dy
это выражение для дивергенции
Брегмана. Порождается она
энтропийная функция x
логарифм x, y
о, нет. Господи.
Тогда можно понять, куда это берется.
Порождается она энтропийная функция
и получается такое выражение.
Это пример был выше,
что такая дивергенция порождается
энтропийной функцией. По определению
можно проверить просто, что действительно так.
Появляется такой логарифм.
Здесь
находим этот x,
подставляем, получаем
энфимум.
Сюда подставили
лагранжан, оптимальный x.
Здесь у вас
получилось вот такое выражение.
Это значит у нас
наша двойственная функция.
Наша двойственная функция.
Подставлять уже времени
нет.
Двойственная задача
формулируется,
тут все просто.
Максим нашей двойственной функции
λ, так как они вылезли
из ограничений типа неравенства
не больше 0,
nu
оно у нас
из ограничений типа равенства,
поэтому оно свободное у нас.
Давайте быстренько заметим,
что если я хочу решить эту задачу
максимума, как я должен
обращаться с λ, который у меня
больше нуля,
здесь минус стоит.
Что я должен
сделать с λ?
Занулить?
Да, λ у нас
соответственно увеличение λ
увеличивает нашу экспоненту.
А значит в силу того,
что перед экспонентой стоит знак минус,
общее выражение уменьшается.
Поэтому λ нужно
увеличить, то есть лямбда оптимальная
это ноль.
Лямбда оптимальная
это ноль.
Я это все подставляю.
А, ну и дальше что?
Условия кунатакера.
Лагранжан, наша функция лагранжа
берется у нее просто
градиент по х
и он
приравнивается к нулю. Это одно из условий
кунатакера, это первое условие кунатакера.
Лагранжан равен нулю,
градиент лагранжана по х
равен нулю.
Там можно сразу учесть,
что вот эти лямбды равны нулю.
И когда мы это все безобразие
выпишем,
то вот как раз
это и останется.
Это и останется, что записано
здесь, на
слайде.
Лагранжан
градиент по х лагранжана
равен нулю. Первое условие
кунатакера.
Все, а дальше что?
Это все опять же берется,
аккуратненько
дифференцируется.
Ну давайте что?
Продиференцируем, что здесь
остается.
Здесь НАБЛА,
FXKT,
IT
из дивергенции Брегмана.
Мы ее уже с вами дифференцировали.
Вылезает логарифм
X,
IT как раз.
Вот это выражение вылезает отсюда.
X, KT,
IT.
Это минус 1, когда у нас там
X болтаться будет.
Которое у нас все сократит.
Ровно то же самое, что мы сделали здесь.
Вот они.
Раз, это 1.
Плюс логарифм.
Это дивергенция Брегмана.
Отображается.
Производная по ней.
Это вылезло из скалярного произведения.
Ну и плюс, когда продиференцировали
по X, IT,
еще вылезло NU со звездой.
Опять же, условия кунатакера
получились вот таким.
Отсюда можно легко выразить
X, IT со звездой.
Там получится опять же,
когда мы возьмем экспоненту,
перенесем все вправо,
кроме логарифм,
то получится экспонент.
Давайте так напишу.
Минус гамма-градиент.
А мы же уже это делали.
Зачем еще раз это делаем?
Мы же вот на предыдущем слайде
именно вот дифференцировали.
Не, до этого.
Там у меня была еще лямбда.
Я теперь избавился от лямбды.
Я знаю, что они нули.
И теперь у меня уже все должно быть четко.
Экспонента тут будет стоять
минус ню плюс один.
Вот.
Там будут как раз экспоненты от суммы.
Это просто произведение экспонентов.
Ну и что, смотрите,
по факту выражение это готово.
Вот оно.
Так-то я не знаю только с чего.
Я не знаю ню.
Из каких соображений я его могу найти?
Из каких соображений?
Точнее даже вот этот кусочек
экспонента один минус это.
Икса звездой и всегда удовлетворят
ограничения в данном виде?
Больше нуля?
Там в ККТ какие-то еще условия были?
А вы вспомните исходные ограничения.
Они какие были?
Иксы-то больше нуля.
Но здесь все четко, потому что
исходные эти симплексы, они больше нуля.
Но было еще одно ограничение.
Так как это симплекс,
сумма равна единице.
Но исходя из этого условия,
можно и подобрать ню.
То есть чтобы вот это,
этот коэффициент должен быть
подобран так, что сумма
х итых со звездой равна единице.
Ну и все.
Тогда счет получается.
Сумма х итых катых,
экспонента одна, минус гамма,
а вторую экспоненту можно вынести
вообще за пределы скобки,
потому что она от и не зависит.
Экспонента 1 минус
это, это.
Это должно равняться единице.
Соответственно, экспонента
1 минус ню
равно 1 делить на сумму
х икта, экспонента
минус гамма, пу-пу-пу
градиента.
Это просто нормировочный кусочек будет.
И тогда второе ограничение
выполнится.
Да, мы не все честно сделали по кунатакеру,
но в некотором смысле так поигрались.
Вот.
Это и есть выражение, которое у нас получается
для
зеркального спуска в случае,
когда мы работаем на симплексе.
Вот. Явное выражение.
Знаете градиент,
можете спокойно посчитать
новый хк плюс 1.
Ну, вот так по координату.
Вот. И оказывается,
оказывается, что
последний факт, который нам нужен,
что вот как раз в случае симплекса
можно показать, что зеркальный спуск
работает в D
делить на логарифм D
раз быстрее, чем
градиентный спуск с проекцией.
Вот. Откуда берется D?
Откуда берется D?
Это вот как раз, помните, я показывал, что норма
первая в лучшем случае,
в нашем случае в лучшем.
Корень из D раз больше, чем вторая.
Вторая, в свою очередь,
в D раз больше, чем нормы
бесконечности.
И вот она взялась с D.
А проигрыш в логарифм
раз берется из-за того,
что диаметр симплекса
в евклидовой норме равен
единице.
А
соответственно,
в дивергенции Кульбака-Лебнера
он равен логарифму D.
То есть, в дивергенции Брегмана
вы проиграли именно
в оценке
в лог D раз, но выиграли
по L-ке,
по оценке на L.
И за счет этого у вас действительно
зеркальный спуск может работать
быстрее,
чем метод
евклидовой проекции.
Это мы еще даже не говорим про то, что у нас
есть еще какая-то вложенная физика,
что просто по евклидовому
расстоянию плохо для распределения.
А KL хорошо.
Вот, оно физично.
Все, я надеюсь, что конечно под конец
получилось сумбурно,
но понятно,
я надеюсь, была основная идея того,
что происходило.
Вопросы.
Во-первых,
мы же вот эту штуку
только на вероятностном
симплексе рассматриваем.
Ну, а как же задач
у нас не только на нем?
Ну, смотрите, в этом плане
зеркальный спуск,
как я говорил, основной у него такой,
что ли, краеугольная задача, это симплекс.
Это первый пример, который
приводит, и на самом деле это один из
основных примеров, где его используют.
Есть у него особенные применения
для всяких специальных,
когда дивергенция порождена
довольно специальной функцией.
Ну, такие примеры, они
узкие, один мы из них рассмотрим.
Чуть попозже,
когда будем говорить про распределенную оптимизацию,
но он тоже такой
специфичный.
Но вот,
к сожалению, да, вот в этом у
зеркального спуска есть такой
большой недостаток, он
во многом заточен под
какие-то конкретные задачки.
Поэтому
в некотором смысле я его рассказываю
перед контролем, потому что его на контрольной
будет, а чем-то новым
перед контролем вас грузить не хотелось.
Но все равно это очень классный метод, который
с красивой идеей
и с красивой довольно теорией,
который в том числе используется
для многих прикладных задач, в том числе
потому что, помните,
теоретика и игровые задачи, они решаются на симплекс.
В домашних заданиях это есть.
И, соответственно,
во многом там
зеркальный спуск играет довольно
ключевую роль, а игры, ну вот это
вероятность. Это вероятность выбрать
какое-то действие.
Это игра, не знаю, там
reinforcement learning, обучение с подкреплением,
где вы выбираете действие, получаете выигрыш
или, наоборот, проигрыш.
А у вас все
решение задачи
заточено на то, что вам нужно в некотором
смысле создать вектор вероятности,
с какой вероятностью вы
выбираете каждое действие.
Вам нужно обучить этот вектор.
Но, соответственно, зеркальный спуск
играет более чем хороший вариант
работать на симплексе.
К сожалению, за пределы симплекса там
очень мало примерчиков,
где еще, кроме симплекса,
это прям выстреливает.
Так, ладно, задержал уже
сильно.
Еще вопросик.
А почему у нас в данном случае
дивергенцию Брэдмана порождает
именно функция
вот эта вот, как мы говорили,
энтропийная?
Захотелось.
Да, это тоже захотелось.
Но это же тоже очень сильно сужает класс задач,
или нет?
Ну, смотрите, на самом деле дивергенцию Брэдмана,
как вы понимаете, можно поразить далеко не всеми функциями.
Там, во-первых, нужна дифференцированность,
не все функции дифференцируемы.
Вот.
Ну, да,
это, конечно, сужает класс задач.
Во-первых, градиентный спуск
всегда там, он всегда с нами, он всегда здесь,
потому что Евклидова норма порождается.
Расстояние Евклидова порождается.
А все, что остальное,
что можно породить,
далеко не все.
Хороший частный случай, энтропийная функция
и дивергенция кульбокалиппера.

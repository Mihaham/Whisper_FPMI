Сегодня у нас лекция номер 13. Будем говорить про то, что такое
Прексимальные методы и метод Франко Вульфа. Здесь успеем.
Поставьте плюс, если слышно, видно экран, все в порядке, можно начинать.
Супер. Давайте начнем вот с чего. Напоминаю, что у нас
какое-то время назад был метод градиентного спуска замечательный,
для которого была вот такая вот схема обновления для дифференцируемой функции.
Эту схему обновления можно проинтерпретировать как схему дискретизации для соответствующего
дифференциального уравнения. То есть это как будто бы dx от t по dt равняется минус f' x от t,
где x от нуля равно x0. Понятно ли, что написано в этих трех строчках? Поставьте плюс, если понятно.
Первую производную приближаем конечными разностями классическая схема приближения
и пересчитываем по сетке следующие значения. Такая схема называется прямой схемой Эйдера.
Темнология немножко другая из вычислительной математики. И все свойства по поводу сходимости
градиентного спуска, ограниченность, максимального шага для сходимости и
все прочее это целиком полностью коррелирует с тем, что известно про зависимость сходимости
схемы Эйдера от размера шага. Не абсолютно устойчиво, то есть это условно устойчиво для
соответствующих альф. То есть это все один в один. Теперь чтобы отсюда следует гипотеза о том,
чтобы получить какой-то другой метод, надо сделать следующее. Давайте как-нибудь по-другому
тот же самый дифференциальный уравнение дискретизуем. Вот, например, вот так. Здесь будет все как обычно,
а тут мы возьмем и скажем, что это на самом деле x ка плюс 1. Это называется неявная схема Эйдера
по понятным причинам, я надеюсь. Она неявная, потому что есть вот эта вот штучка, которая
в неизвестной точке участвует в вычислении градиента, который мы пока что не знаем. Так,
неявная схема. И, собственно, она абсолютно устойчива. То есть какой бы вы размер шага сюда
не подставляли, у вас все будет работать. Может он будет работать не очень быстро, может быть будет
работать с некоторыми вычислительными трудностями в силу того, что отдельное уравнение будет решать,
но, тем не менее, все будет не будет какой-либо серьезно расходить. Вот. Значит, пока мы написали
всего лишь, так нажал, всего лишь дискретизацию, давайте дискретизацию перейдем к методу. Вот.
Ну для этого достаточно заметить, что, ну давайте сделаем тривиальное преобразование для начала. Вот.
Равно нулю. И достаточно сейчас внимательно посмотреть на эту штуку и понять, что на самом деле,
когда у нас f выпукло, ну понятно, плюс дифференцируемость, куда же без этого, вот, то вот эта штука,
вот эта звездочка обозначена. То же самое, что и у-хкт, квадрат второй нормы, 1 на 2 альфа кт,
плюс f' от u, все это штрих при u равном xk плюс о первом, равно нулю. То есть то, что стоит по звездочке,
является градиентом вот этой функции. Все ли это видят? 12 лекции. Ставьте плюс, если вы видите,
что это градиент. Так, окей, вижу, есть прогресс. Очень хорошо. Градиент равен нулю, это значит,
что у нас точка xk плюс 1, это не что иное, как аргумен от этой самой. Это называется просто
«проксимальный метод». То есть он вместо того, чтобы, то есть каждый раз на каждой итерации решается
некоторая задача. Понятно, что это имеет смысл использовать только, когда она решается аналитически. Вот.
Почему это еще хорошо? Это еще хорошо, потому что если вдруг по каким-то причинам вам надо
решать численно, то вот эта функция, чем вот f' от u, отличается по своей структуре от вот этой
функции, как вы думаете? По тем критериям, которые мы ранее обозначили для описания характеристик
функции. Если f выпукло, то f' от u конечно сильно выпукло. Ну да, по-другому надо сказать, что если f
просто выпукло, то вся вот эта сумма, она уже будет сильно выпуклая, потому что мы бы, ну,
гисян от этой штуки – это единичная матрица, шкалированная на вот этот вот
преэффициент 1 на 2 αк. Вот отсюда, ну там, 1 на αк будет понятно. Вот отсюда следует просто
факт, что мы как будто бы спектр вот гисян от этой матрицы, если он существует, сдвигаем на чуть-чуть
от нуля. Поэтому характеристики этой функции существенно образом улучшаются. Вот. Значит,
это еще один важный момент, почему вот эта вот эта задача лучше, чем минимизация самой
исходной функции фату. Вот. Так. Значит, это обсудили, это обсудили, про то, что шаг любой тоже
обсудили. Теперь, что из этого можно сделать? Вот. Тут пришло время привести несколько, да-да,
я же не ввел самое главное обозначение. Как бы теперь его правильно ввести-то? Давайте вот так
сделаем. Так. Стука равна вот таким вот обозначениям. Prox от αк f – это не произведение, а это значит,
что αк уехало сюда, вот, а f уехало вот сюда. Это просто подряд доводящий символ, поэтому не думать,
что я там какой-то произведение считаю. Вот точки xk. Такое обозначение. Вот. И теперь вот это все
называется… так, ага, да, правильно, теперь надо вот так сделать. Тиш-тиш-тиш-тиш. Называется
проксимальный метод, то есть просто xk плюс 1 – это проксимальный оператор, результат действия
проксимального оператора на паук xk. Наверное, еще вот так можно сделать. Так. Это уже кревато, конечно,
но здесь понятно. Понятно, что хотел сказать, а? Окей. Люди смотрят на экран, это приятно. Так,
теперь давайте поймем, как, в общем, какой-то новый метод, что-то произошло, какой-то аргумент
появился, пока непонятно, почему все это идет. Вот. Ну, на самом деле, давайте найдем простые
интерпретации этого всего безобразия, чтобы понять, как потом с этим дальше жить. Здесь это пока
некоторый элементарный кирпичик, из которого потом будет много еще следовать. Значит, если мы…
то есть, пункт номер один. Если мы вернемся на уровень, когда мы записывали наше, как бы,
условие оптимальности, то его же можно переписать вот эту звездочку. Следующим образом, что у нас
какой-то единичный оператор плюс альфа-каты на f' действует на xk плюс первое, и это, оказывается,
равно x-катам. На самом деле, у нас, если формально так списывать, абсолютно, что-то с валютаристки
без каких-либо объяснований пока что, то это вот эта штука. То есть, у нас в каком-то смысле идет
речь об обратном операторе к такому вот странному оператору единичный тождественный плюс альфа на
градиент. Ну и если мы вспомним, грубо говоря, как это можно приблизить, наверное, более-менее
справедливо, не только для конечномерного пространства, то это можно приблизить как единицы
минус альфа-кат f' примененное к x-катам. Что в точность, оказывается, равно тут альфа-ката типа к
нулю стремится. Я так напишу, что оно типа мало. Вот. Минус альфа-кат f' от x-кат. И мы гениально
получаем градиентность. Для малого шага можно примерно сказать, что будет все одно и то же.
Теперь, второй пример. Вторая интерпретация. Вспоминаем прошлую лекцию. Не зря же мы там
так долго мучились с выяснением свойств Меда-Ньютона и его способов его получения. Поскольку вот эта
штуковина, она непонятно какая, то давайте мы попробуем в проксимальный оператор засунуть не f,
а f с крышкой, которая будет являться квадратичной оценкой в данной точке. Вот так. Плюс. Вот. И посчитаем
x-к плюс первое как пример на проксимальный оператор от альфа-ф с крышкой от x-к. То это такое.
Готовьте листочек и ручку, сейчас вам надо будет кое-что посчитать. А то предупреждаю. Так. Это
будет f от x-к плюс f' от x-к x минус только не x, а u минус x-к плюс на вторая и так 1 на 2 до альфа-к
норма у минус x-к квадратик. Все, отлично. Смотрите, что получилось. У нас квадратичная функция
под аргмином стоит. То есть квадратичный множитель ввод и квадратичный множитель ввод. Ничего более
высокого порядка или более нелинейного, чем квадратичное здесь нет. Это значит, что этот аргмин
можно найти аналитически из условий первого порядка. Давайте найдем. Пожалуйста, получите градиент
вот этой функции. Ну, от всей вот этой функции, которая стоит под аргмин. Градиент по u. И либо
напишите в чат, либо продиктуете. Думаю, за пару минут усправитесь. Сначала надо написать
first order optimality conditions. Да, давайте. Градиент функции, кажется, под аргмином будет f' от x-к
плюс, у нас же в общем случае гессиан не обязательно симметричный, кажется. Гессиан обязательно симметричный.
Обязательно? Хорошо, тогда плюс f' от x-к на u минус x-к. Дальше? Плюс норма u минус x-к
потереть на альфа-к-т. Уже неправильно норма числа. Градиент должен быть вектором. А просто u минус x-к
без нормы. Что там 1 на альфа-к на u минус x-к? Окей, это нулю должно быть равно. Давайте теперь
свернем все вместе. Что получается? Получается f' от x-к-т плюс 1 на альфа-к-т. Это все умножается на u,
которая нам неизвестна и равняется минус градиенту. Плюс f' на x-к и плюс 1 на альфа-к-к. Окей, правая
часть переписывается более-менее аналогичным образом. Плюс f' x-к плюс 1 на альфа-к единичная
матрица умножается благополучно на x-к. Отлично. Все до чему будет равно u? Равно. Умножаем на
минус 1. Тут регуляризация специальным образом придумана, чтобы это можно было сделать. Вот это
вот гарантирует то, что можно будет посчитать обратную матрицу. Проблем с выраженностью здесь
никаких не ожидается. Что получается? Вот эта штуковина. Так, каким бы надо цветом таким более-менее
светленьким выделить зеленым. Вот эта штуковина благополучно сократится с вот этой штуковиной.
Не останется x-к-т. Дальше будет минус f' x-к-т плюс 1 на альфа-к единичная матрица в минус
1 действует на градиент в точке x-к. Получилась вот такая вот опроксимация. Ну и это как бы x-к плюс
первый наш напоминаю по аналогии тем, какой опроксимальный метод опроксимировать. А что это
похоже? Во-первых, всем ли понятно, что происходило, кто это бодро все это прописал, успели его осознать
произошедшее. Или нужно какой-то из переходов пояснить. Вижу два плюс. Это радует. Как дела
у остальных? Гуд. Здорово. Что происходит дальше? Дальше мы можем посмотреть, что будет для разных
при разных альфах. Первый вариант. Альфа-к-т стремится к плюсу бесконечности. Чему будет равняться x-к
плюс первое? Ну приближенно. То есть если альфа-к-т большой, то это что значит? Кто доминирует?
Какое слагаемое? Гисян. Гисян доминирует, конечно. Знаю, их будет просто x-к-т минус f' x-к-т минус
первое на f' от x-к-тов. Что такое? Просто непонятно, что это значит. За метод. Да-да-да. Да, это метод
Ньютон. Гениально. Ура. Тут что-то помню с прошлого раза. Получили метод Ньютона всего лишь как
опроксимацию, опроксимального градиентного метода при некотором предельном переходе по шагу.
Если альфа-к-т ноль, чему равен x-к плюс первый? Какое слагаемое доминирует? Ну просто какой-то бешеный
градиентный спуск получается. Почему обычный градиентный спуск получается? Потому что получается x-к
плюс один альфа-к-т единицы минус первой на f' от x-к. Получаем тут и x-к минус альфа-к-т на f' от x-к.
Все, прекрасно. Получили градиентный спуск. То есть вот эта штуковина, которая в принципе в случае
некоторые приятные структуры для Гисяна, она позволяет в каком-то смысле регулиризовать метод
Ньютона чтобы, точнее, ну градиентный спуск так чтобы он, грубо говоря, стал методом Ньютона,
при этом Гисян был строго положительно определен. И тем самым мы бы балансировали
между этими методами еще одним способом, помимо того, который был обсужден в прошлый раз в контексте
квазинтунских методов. Вот, то есть видите, можно делать вот так и это связано у этого подхода. Есть
очень красивые интерпретации в контексте задачи, там пример. Если у нас хотим минимизировать одну-вторую,
сумму f и t от x в квадрате. То есть задача нелинейных, наименьших квадратов. И вот такой вот метод,
который вот я тут обозначу плюсиком. Для вот такой вот целевой функции f кодной, которая распадается на
сумму квадратов некоторых функций, то плюсик сводится к методу Левенберга-Маркварта на самом деле,
для этой задачи. Это супер классический метод, который везде можно найти. Давайте даже
можно показать, его можно найти, если есть какое желание. Вот если грубо говоря с IP Optimize залезть. Вот,
сейчас показать screen. Вот, то тут есть видно, да, экран? Алё, видно или экран? Да, видно. Так,
отлично. Вот, тут вот общего вида методы. Вот, однако же, если посмотреть на просто Optimize без
минимайза, то где эта штука? Линалка, бласс, лопатка. А, вот Optimize. Вот, то тут будет, так это понятно,
отдельные методы, границы, BFGS пресловутые, которые в прошлый раз обсуждали. Что-то про методы
безградиентные, которые пытаются найти глобальный минимум в неупаковых задачах. Вот, и значит есть вот
linear least squares, которые вот такое решают. Вот, а есть non-linear least squares, которые решают просто
произвольные наименьшие квадраты. Вот, ровно то, что я написал. То есть, на вторая сумма РО от, то есть,
тут ещё есть параметры, есть функция РО, которая функцию, ну, которая типа ошибку измеряет. Но есть она
типа единичная, которая тут, кажется, есть такой дефолтный случай. Сейчас где-то написано что-ли РО. Не написано, что-ли?
А, вот это написано, наверное, loss. И вот loss, если он, да, короче, если он linear, то это просто
ажестная функция. Это ровно то, что я написал. Standard least squares problem. Ну, короче, вот. Тут ещё разные
другие, вот, про которые можно, если кому интересно, можно потом в чате будет обсудить, вот, что они
делают. Ну, как бы идея не в этом. Идея в том, что есть параметр method, который определяет то, каким
solver это всё решается. Вот, и вот тут есть LM, который, собственно, есть вот тот самый Levenberg-Marquardt,
который я сейчас упомянул. И идея его ровно в том, чтобы регулировать метод Ньютона, вот, дополнительно
сдвинув его на некоторую единичную матрицу, умноженную на соответственную константу. Вот,
ну да, в общем, small constraint problems, это, наверное, самый такой основной метод. Вот, и если хочется
ещё чего-то большего посмотреть про него, то можно посмотреть Levenberg-Marquardt. Да, в каком
огромном количестве библиотек он реализован. Вот это вроде бы страничка раньше была в виде педии,
возможно, её сейчас уже дропнули, да, чуть я уже не вижу. Ну, то есть, да, что-то, конечно, не
получится показать. Да, в общем, в принципе, если вы берёте любой пакет, решение задачи
оптимизации более-менее цельные, то там, скорее всего, эта штука будет реализована. Поэтому лестно
понимать, что для наимнейших квадратов у максимального метода есть такая милая интерпретация. Это
были две интерпретации, которые позволяют как-то склеить то, что мы изучали раньше с тем,
что начали изучать сегодня. Теперь ещё одно важное свойство про максимального оператора заключается
в том, что, внимание, что х со звёздочкой является решением нашей задачи, равносильно тому, что это
неподвижная точка максимального оператора. Вот, то есть, получается, что если, то есть, давайте так,
кто понимает, какая польза от этого результата? Ну, кто понимает, какая польза, напишите, какая
польза в чате. То есть, вопрос очень простой. Зачем это нужно с точки зрения вычислений,
что-то нет идей. А, это правда. Это значит, что можно использовать невязку х со звёздочкой минус,
ой, хк-хк, плюс первое, меньше либо равно эпсилон, как корритерия сходимости. Да, именно так. Это
называлось раньше у нас корритерия сходимости. Потому что, ну, они должны подать. Давайте докажем.
Для простоты сделаем это для дифференцируемых функций. Увидите, в каком месте это будет важно. Так,
ну, в одну сторону. То есть, пусть х решение. Это значит, что f от х больше либо равно f от
х со звёздочкой. Ну, теперь, давайте прибавим к обеим частям неотрицательные функции. Ну,
а это больше либо равно, понятное дело, чем вот х со звёздочкой плюс 1 на 2 альфа х со звёздочкой
минус х со звёздочкой. Следовательно, х со звёздочкой это аргумент для нашей функции f от х плюс 1 на 2
альфа х минус х со звёздочкой 2 на 2 в квадрате. А это есть определение максимального оператора.
Успели? То есть, видите, три строчки в эту сторону доказываются. Довольно прямолинейно.
Пришло определение. Докидываем до нужного вида функции. Оцениваем. Так вижу два плюса.
Дело у Дмитрия видим. Да, спасибо. Теперь, давайте в обратную сторону. Тут похитрее немножко,
и тут как раз-таки будет важно дифференцируем. То есть, если у нас х со звёздочкой это аргумент,
аргумент, понятно отчего, то это значит, что в этой точке выполнено слово первого порядка,
что грагент ноль. То есть, f' от х со звёздочкой на самом деле, то есть, ноль равен f' от х со звёздочкой
плюс 1 на альфа ката х минус х со звёздочкой в точке х равняется х со звёздочкой. А это в точности
f' вот так. И вспомним, что это равно нулю. И, следовательно, х со звёздочкой решение сходной
задачи. Вот. Обобщается это доказательство и на случай, когда f' не дифференцируемо,
вот, но более трудоемко становится. Поэтому сейчас я так. Ну, то есть, вообще не важно.
Вопрос, к примеру, такой. Просто пишут условия оптимальности, и из этого условия
оптимальности выводится условие оптимальности для исходной задачи благодаря тому, как выглядит
градиент для второй нормы, и что он в любом случае будет зануляться в точке х со звёздочкой. Понятно,
что мы доказали, как мы доказали, и что, если в этого следует. Так, вижу плюсы, окей. Спасибо.
Так, теперь немного про то, как это дело вычислять. Вычислять проще, чем могло показаться на первый
взгляд. Для этого, так, мы увеличим масштаб. Посмотрим функцию, у которой перемены разделяются.
Называется сепарабельная функция. Ну, тут у нас сепарабельная функция. Называется такая вот
функция. Сейчас я нарисую. Называется сепарабельной, потому что происходит разделение перемен, а именно f
от x составляется в виде f и t от x и t. Приведите какой-нибудь простой пример такой функции.
Скалярное произведение с константой. Скалярное произведение с константой? Это что значит?
Линейное, что ли? Ну да. Так, хорошо, линейное. Очень поинтереснее. Типа сумма c и t, x и t, да? Вот.
И тогда у нас каждая f и t от x и t это c и t, x и t. Ну да, в общем-то справедливо. Ещё, очень более
реалистичная и насущная. Всё, на линейных функциях Мир и Мир закончился. Больше не существует.
Ну непонятно, что можно просто вместо x и t взять x и t в квадрате или что-нибудь такое, но как раз
реалистично непонятно, что из этого сделать. Да, понятно. Ну то есть стандартный пример – это всякие
нормы. Типа первая норма – сумма модулей. Вот. Вторая норма – сумма квадратов. Тоже прекрасная функция.
Ну только надо же тогда и корень извлечь из этого всего. Не, ничего не надо. Просто берём квадрат и не
страдаем. Так, что ещё полезное бывает? А, ну, например, вот для матрицы, если функция от матрицы, то
тоже типа сумма модулей по компонентных. Вот. Тут вот x из, ну, обычно из SN+. Ну, что-то не так важно. Вот. То есть
очень много функций, которые отвечают за свойства решений дополнительные, типа разреженность,
малоранговость или что-то такое, они как раз-таки зависит напрямую поэлементно от агумента, скажем
так, да. Вот. Поэтому этот час встречается, и если мы поймём, как вычитается максимальный оператор
для этой функции, то много-много задач, и мы сейчас наоборот себе упростим решение. Вот. Ну, в общем,
результат такой, утверждение, наверное, это лучше так написать, что если всепарабельно, то
максимальный оператор от f точки от u этой компоненты – это максимальный оператор от f итого в u итой
точки. То есть, внимание, следите за руками, где находится буква i. Слева она тут, справа она…
слишком толсто. Слева она вот тут, а справа она вот тут и вот тут. То есть, везде, обратите
внимание, скаляры стоят. То есть, лево и справа. Ну, вот тут вопрос. Всем ли это очевидно? Поставьте
плюс, если вам это очевидно, и минус, если это лучше показать более подробно. Очевидно кому-то – это
хорошо. Кому не очевидно. Кому-то не очевидно. Окей. Давайте посмотрим детальнее. Значит,
xk плюс 1. Это у нас был… Ой, ну да, давайте так делать. Аргмин. Тут у нас появляется сумма f ит от x от f у
итого плюс, а дальше я сразу распишу, 2 альфы каты, сумма у иты минус x, ну, типа t в квадрате. Это
было наше определение. Сумма аргмин по u. Теперь, аргмин по u расписывается как сумма, и все вносит
под одну сумму. У иты плюс 1 на 2 альфы каты, у иты минус x каты иты в квадрате. Вот. И мы чудо замечаем,
что на самом деле вот каждый из этих вот и… Каждый из вот этих вот слагаемых, оно зависит только от
своего красного x иты каты. Поэтому для поиска соответствующих компонентов достаточно рассмотреть
x кап плюс 1 иты как аргмин… Ну, снова по u, только теперь вы будете скаляром. f иты от u плюс 1 на 2 альфы каты,
у иты минус x иты каты в квадрате. А это ровно и есть. Проксимальный оператор от альфы f иты в точке… Ну,
тут в точке x иты каты понятно, но, я надеюсь, не менее очевидно, как это переносится на обозначение
u иты, стало понятно. Прекрасно. Вот. То есть достаточно взять вот из этих вот ингредиентов какую-нибудь
одну функцию, вот из этих вот, вот из этих вот… Для нее найти, грубо говоря, решить одномерную задачу
аналитически, потом просто викторизовать результаты и понять, как пересчитывается вся эта история вместе.
Вот. Это полезный хинт о том, как это посчитать эффективно. Так. Ну, теперь там это были все некоторые
истории про то… Так, было занятие прошло. Прекрасно. Про то, что такое проксимальный оператор,
проксимальный метод и какими свойствами он обладает, теперь перейдем непосредственно к
максимальному градиентному методу. Очень классная штука. Если вы будете уметь видеть,
когда им надо пользоваться, многие изучители будут решаться гораздо быстрее. Чем, если вы этого
видеть не будете и будете запускать солверы общего вида для них. Максимальный градиентный метод.
Кроме идеи. Идея немножко похожа на то, что мы видели в стокастике, когда мы в черный ящик заглянули и
обнаружили там, что у нас наша функция представляется в виде суммы большого числа функций.
Помните, что было такое? Девчонки, что помните? Реакцию, конечно, я пока не вижу. Надеюсь, что как
работают стокастические градиентные методы, более-менее большинство разобрали. Вот. Здесь немножко
похожая история, только теперь у нас будет не сумма бесконечного числа большого числа функций,
всего две функции. f от x и g от x. При этом, что мы будем знать? Мы будем знать, что f от x у нас гладкая,
там все выпукло, плюс выпукло. g от x просто выпукло, плюс может принимать бесконечные значения. Вот.
То есть такая типа может быть негладкая и вообще непонятно какой. В общем, все плохо, может быть.
В плане дифференцируемости. То есть суммарно, вот эта функция, давайте обозначим h от x, нам один
раз пригодится. То есть h от x в общем случае не дифференцируемая функция. Градиентный спуск
примять нельзя. Однако, мы знаем, что есть кусок, который дифференцируем. Давайте это использовать.
То есть мы можем сказать, что у нас h от x оценивается сверху как f от x кt плюс f штрих от x кt x
минус x кt плюс l пополам, ой, l пополам, норма, x минус x кt, плюс g от x кt. То есть вот это,
это наша квадратичная оценка сверху, с которой мы относимся уже к эту лекцию. Да, наверное. Так,
поставьте плюс, если помните. Так, помнит, хорошо. Вот. А g от x как был, так и осталось. Давайте искать x
кt плюс 1 как аргумент от вот этой вот шикенции. Вот. Почему? Это хорошо. Это будет хорошо. То есть тут
будет, ой, тут прошу прощения, я немножко напортачил. Тут не x кt, а просто x. Так. Что дальше? Дальше
напрашивается классический подход. Давайте мы полный квадрат выделим. Вот. И полный квадрат
напрашивается выделить ровно вот в этой части. Потому что у нас есть квадрат как будто бы один.
У нас есть произведение, может быть, под квадратом и градиентом. Вот. Поэтому, наверное,
если мы тут аккуратненько сейчас перетинем в правильные стороны наши слагаемые, там
множим поделим, добавим умный ноль, который не зависит от x. Вот. То, в общем-то, все должно
получиться. И посмотрим, к чему это приведет. Аргумент я, в общем, не переписываю. Напишу просто,
что это то же самое, что аргумент от t, g от x остается. От этого никуда не дается. Плюс 2 делить на l.
Вот здесь дальше будьки кропки. Сначала у нас x минус x ка т в квадрате плюс одна вторая. Извините.
Отсюда l пополам выносится сначала. l пополам выносится, поэтому здесь получается 2 на l. 2. Здесь
образуется единица на l f штрих от x ка т, а тут x минус x ка т. То есть в итоге у нас что? Вот квадрат
первого, удвоенное произведение, удвоение отдельно стоит. Первого на второе, то есть получается,
что второе у нас это 1 на l на норму градиента в точке x... 1 на l на градиент в точке x ка от
функции f. Значит, нам надо добавить квадрат 1 на l f штрих от x ка квадрате. Ну и вычислить этот
самый квадрат, честностью. Вот. И плюс f от x ка. Так, и я что-то забыл. Ну да, тут еще,
типа, конечно же, 2 на l f от x ка. Типа так. Вроде бы все правильно. Пожалуйста,
проверьте. Поставьте плюс, если проверили и убедились, что все хорошо. Так вижу,
что вроде нормально. Двух человек подтверждение получилось. А, все, отлично. Спасибо. Вот. То
есть теперь давайте добро свернем. Получается аргумент по x. Тут все еще по x. Ж от x плюс l пополам
на что? На норму x минус x ка плюс единица на l f штрих от x ка. Все остальное мы буквально выкинули,
потому что от x это все остальное не зависит. Теперь, если мы обратно все это внесем. Сейчас,
просто надо ли это все дело вносить обратно. Сейчас, давайте я постепенно сделаю. Аргумент
же от x плюс l пополам норма x минус x ка минус 1 на l f штрих от x ка. Вот, что хотел продемонстрировать.
Меня смущает этот дурацкий l пополам, который сейчас что-то придумать. Я же правильно его вынесу?
Так, ну l пополам вынеслось правильно. Так, тут есть квадраты. Очень хорошо. Так, окей. Теперь,
если мы вспомним, что у нас стандартный альфа ка был равен единиц на l, то тогда вся эта штукенция
станет принимать более привычный вид. Аргумент же от x плюс 1 на 2 альфа ка норма x минус x ка минус
альфа ка f штрих от x ка. Получилось. Смотрите, что происходит. Эту штуку можно записать в
привычном уже иной терминологии. Мы считаем прохимальный оператор от альфа g точки x ка минус альфа
ка f штрих от x ка. Это называется прохимальный градиентный метод. Понятно ли, как он у нас
образовался? Ставьте плюс, если понятно, минус, если нет. Пока лишь только вроде бы один плюс.
Появился второй. Хорошо. Ага, спасибо. Окей. Значит, что про него важно понимать? Важно понимать,
что мы берем как бы лучше всего того, что у нас нам доступно. Изначально у нас функция спалась в сумму
той функции, которая можно, ну не распалась, мы как бы полагаем. Вот. Функции, у которой можно
написать градиент и той функции, у которой гипотетически, мы надеемся, легко посчитать
прохимальный оператор. Поэтому мы берем, делаем как бы все, что можно сделать. x обновляется сначала
как градиентный шаг по f. Потом важно отметить, что с таким же шагом, то есть тут важно их не
начать варьировать, таким же шагом пересчитывается, делается прохимальное отображение по относительной
функции g, для которой градиентный шаг не применим. Теперь, ну что, пример что ли или не пример? Сейчас
секунду, я подумаю, как правильнее это было бы все пояснить. А, да, сначала давайте вот что,
сходимость. Конечно же сейчас я доказывать это все не буду, потому что времени. Вот. Сходимость.
Вот чудо, вот единица на k для гладких выпуклых функций. То есть, смотрите, у нас изначально функция
недефинцируема, градиентный спуск неприменим, но скорость сходимости такого метода совпадает
со скоростью сходимости градиентного спуска. Немножко магии. То есть, негладкая поправочка
аддиктивная нивелируется помощью максимального оператора относительной добавки. Это здорово,
потому что изначально у нас функция всего лишь обычная была, ну, сумма всего лишь обычная,
негладкая, выпуклая. А тут мы взяли и получили скорость сходимости как будто бы все вместе гладко,
что немножко необычно. Ну, небольшой анонс на следующие 15 минут, через 5-10 минут, наверное,
дойдем. Что раз у нас есть скорость 1 на k, то, наверное, мы можем это ускорить по аналогии с тем,
как мы это ускоряли для обычного градиентного спуска и получить 1 на k квадрат. Ну, собственно,
так и происходит. И чуть позже я приведу формулу, ну, там, формула абсолютно аналогичной с точностью
до того, как вот эта штука пересчитывается, а максимальный оператор как был, так и остается.
Теперь частный случай называется метод проекции градиента. В чем идея? Смотрим на вот такую вот задачу
и говорим, что, окей, у нас есть такое ограничение не очень понятное, давайте мы его перепишем вот
в таком видео, где функция и c от x, да, ну тут понятно, c выпукло множество, и выпуклая функция,
дифференцируемая, с ней все в порядке. Вот, а это индикаторная функция множества, которая равна нулю,
если x в множестве лежит, плюс бесконечности, если x в множестве не лежит. По построению решение вот
этой задачи будет совпадать с решением исходной задачи. Это только функция f минимизировалась,
но на множестве. Сейчас она минимизируется не на множестве, но с такой вот аддитивной поправкой.
Понятно ли преобразование? Поставьте плюс, если понятно. Так, окей, еще один плюс нужен, или минус,
не знаю, какая-то реакция точно нужна. Да, здорово. Смотрите, что происходит. Мы получили ровно ту самую
структуру, для которой применялся наш проксимальный градиентный гет. У нас есть одно слагаемое,
которое выпукло и дифференцируемо, и другое слагаемое, которое выпукло, но не дифференцируемо.
Давайте применим наш метод. Получится, получится, что xk, так, сейчас, секундочку.
Получится, что xk плюс 1 это проксимальный оператор от функции альфа индикатор множества c. Давайте,
я не буду аргумент написать. От чего? От x катова минус альфа f штрих от xk. Что такое
проксимальный оператор индикаторной функции? Давайте минуту на размышление. Можно написать в чат
ответ или сказать в микрофон. Пока определение напишу, может быть, оно поможет. Это просто аргумент
второго слагаемого помножить в c. Как это называется? Это правда. Что это такое? Ближайшее к x.
Нет, к u.c. У этого длинного определения есть одно конкретное название, которое, я думаю, все знают.
О, гениально. Это проекция, правильно? Проекция. Поэтому это называется, вот я тут специально причернул,
что он говорит, проекция-градиент. На самом деле, вот эта штука, проекция, но еще иногда обозначается
p, c от u. То есть, если мы теперь запишем наш метод после вот этих вот всех замечательных выкладок,
всего лишь проекция намножится в c точки xk минус альфа f штрих от xk. Видите? То есть, мы делаем
градиентный шаг по нашей функции, по антиградиенту, понятное дело. Потом, куда бы мы ни пришли,
ну типа вот было наше множество, мы где-то живем здесь, xk. Потом прыгнули сюда, каким-то причинам,
не знаю, и спроецировались. Потом мы из этой точки куда-то шагнули и спроецировались. Это xk плюс 1,
это xk плюс 2. Понятен ли метод и понятно ли почему это всего лишь частный случай
проексимального градиентного метода? Ставьте плюс, если понятно, и минус, если не очень. Так,
вроде понятно. Все отлично, good. Значит, это всего лишь частный случай, как я уже сказал. И поэтому
все те технологии, которые были разработаны выше, она, собственно, и ниже будет еще доделана,
она напрямую переносится на метод проекса градиента вот в таком вот виде тоже. Поэтому не обязательно
его рассматривать какой-то отдельно изолированный такой уникальный метод. Настоящим всего лишь
заметить, что это частный случай и проексимального градиентного метода. Теперь ускорение. Ускоренный,
так давайте я то фаст, проексимальный градиентный метод. Идея очень простая. Давайте считать нашу
точку относительно другой какой-то точки, а не предыдущей. Вот. Ну и y ка плюс 1 потом пересчитывается,
как x ка плюс 1 плюс, ну не знаю, какой-то там коэффициентик обычно берут. Простоты ка на
ка плюс 3, другие варианты тоже возможны. В необходимости, я не сомневаюсь, в литературе вы найдете
их достаточное количество. И то. Так, вот тут вот плюсик, а там минус x ка, вот так. То есть берем
линейную комбинацию двух соседних х, в ней считаем y и потом относительно этого y пересчитываем
следующий х. Вот. Абсолютно понятная стратегия, сходимость также 1 на ка квадрат превращается
для гладких выпуклых. Вот. Действительно можно, ну там, пять строчек реализации, можно
пронаблюдать, насколько эта штука быстрее исходит к тому же сационарному, к той же сационарной
точки, да. Так, есть минус 2, прекрасно. А, да, теперь еще вот что надо сказать, что вот сейчас
и случаи про проекцию градиента мы рассмотрели. Вот. Теперь же, если мы в целом посмотрим на вот нашу
структуру, то какие варианты возможны? Возможно, если у нас f-тождественный 0,
мы получаем метод, просто, проексимальный метод. Потому что градиент будет нулем, и наш x ка плюс
1 будет пересчитываться просто как, максимально отображая относительно функции g текущей
точки. В том, если у нас, наоборот, g равно 0, мы получим, какой метод? Чему равно
проексимальное отображение для нуля? Тяжело как-то. X ка т? То есть начальная точка? Почему? Вот наш метод.
Отдается только норма у минуса, так что у нас там в качестве у по этому? Нет, ну, наверное, все-таки вот это
останется. Даже вот такой метод. А, ну да. Что это за метод? Градиентный спуск. Именно так. Получится
градиентный спуск. Ну и третье то, что мы уже обсудили, это то, что если у нас g это индикатор,
получаем проекцию градиента. Видите, как много методов скрыто за одним всего лишь таким, казалось
бы, наивным и простым предположением, что сумма, что функция совпадается на сумму двух функций.
Здорово. Теперь, частный случай, конкретный пример. L1 регулиризация для, работает, для
линейных наименьших квадратов. То есть, если мы такую задачу решаем, нам надо найти
проексимальный оператор от вот этой штуковины. Ну и градиент от вот этой, я верю, все справится.
Вот. Давайте просто на конкретном примере посмотрим, как это дело вычисляется. То есть,
наша функция g от x. Первая норма. X равна сумме модулей. Поэтому нам достаточно для модуля посчитать
проексимальное отображение и потом просто векторизовать. Надо посчитать следующую штуку.
Аргмин. Чего? От модуля x плюс 1 на 2 альфа. Ну типа y минус x в квадрате. x минус y давайте лучше
сделаем. И аргмин по x. Функция негладкая. Давайте из нее сделаем гладкую функцию. Вот. И потом
вернемся к исходной задаче. То есть, будет сейчас гладкая функция с, ну гладкая задача с
ограничениями. Ну понятно какая. Аргмин t плюс 1 на 2 альфы x минус y в квадрате при условии
o меньше либо равно t. То есть, минус t меньше либо равно x меньше уровня t. Вот так. Понятно,
почему это справедливо? Да, нет. Вроде как-то мы это уже изучали, насколько я помню. И нет. Вроде
понятно. То есть, канат графику переходим по статистично в целевой функции и нам это, в общем-то,
хватает. Сейчас, я думаю, в процессе будет понятно, почему. Сейчас, короче, давайте я все-таки напишу,
как а t. Как раз вроде 10 минут должно хватить. Лагранжан. И тут, соответственно, аргмин по x и по t уже
появился. То есть, t плюс 1 на 2 альфы x минус y в квадрате плюс лямбда, что на x минус t и плюс мю
минус t минус x. Это нашел гарнжан. Ну, соответственно, лямбда мю больше либо равно нуля. Лямбда x минус
t равно нулю. Мю минус x минус t равно нулю. Дополняющая не жесткость. Вот. Пам-пам-пам. А, вроде все, да?
А, ну, градианта гарнжана, да. Давайте по x. Это что будет такое? Это будет 1 на альфы x минус y плюс
лямбда минус мю. И по t это будет единица минус. Лямбда минус мю тоже равно нулю. Отставьте плюс,
если вы понимаете, что я написал только что условия какая-то. Кроме там допустимости прямой
задачи, но это мы держим умее еще, пока нам это сейчас не нужно. А, вижу. Отлично. Спасибо. Что из этого
следует? Значит, лямбда плюс мю равно единица. Следовательно, лямбда не равно нулю и мю не равно...
Ну, то есть, короче говоря, они не равны нулю одновременно. Вот. Хорошо. Это уже один случай мы
исключили. Значит, если... Могут ли они быть неравными нулю одновременно? То есть, если лямбда не равно
нулю и мю не равно нулю, из этого следует в силу дополняющей не жесткости, как бы мне тут это обозначить,
не будь... Я вот так же делаю. Так вот, то х равно... Получилось. То х равно t и х равно минус t
одновременно. Что возможно только если х равен нулю и t равно нулю. Но тогда будет ли это
чему-то противоречить? Давайте посмотрим. По секунду. Как будто бы... Ну, хорошо, давайте вот,
типа, следовательно, х равно t равно нулю. Пока какая-то изолированная ситуация, с которой сейчас
будем бороться. Вот. В таком случае, если лямбда не ноль, следовательно, х равно t. Вот. Раз х равно t,
то в каком-то смысле мы раскрыли так модуль. Это один из способов... Ну, один из вариантов мы получили
раскрытие модуля за счет того, что записали один из вариантов дополняющей не жесткости. Вот. Если
лямбда не ноль, а мю равно нулю, следовательно, из градиента Лагрензжана получаем, что лямбда
равно единице. Из-за такого крестика. Отсюда получаем напрямую, что 1 альфа х минус y плюс лямбда
лямбда единица плюс 1 минус мю мю ноль. Вот. Отсюда х выписывается. Очень легко. Х равняется минус альфа
плюс y. Вот. Это раз. И тут сразу же появляется ограничение дополнительное, конечно же. Раз х равно...
У нас х равен t. Не хватает... Так, ну понятно. Не хватает неявного ограничения, что t у нас больше нуля. Вот.
Х равно t, да. Будет больше либо равно нуля в этом случае. Вот. То есть, значит, что это значит? Это
значит, что y у нас больше либо равно альфа. Теперь, наоборот, лямбда у нас равно нулю, мю не равно нулю.
Оттого, что мю не равно нулю, следует, что х равен минус t. Вот. И более того, следует, что мю равно единица.
Теперь мы напишем все то же самое. То есть, 1 альфа х минус y. Только выражение плюс лямбда минус мю
станет плюс ноль минус 1. Равно нулю. Отсюда х будет равен альфа плюс... Альфа плюс y. И эта штука
меньше либо равна нуля, потому что х равен минус t, t положить. То есть, y меньше... Брайан минус альфа.
Вот. То есть, получили вот такие два условия. Теперь осталось понять, что происходит, когда у нас y от
минус альфа до альфы. Вот. Здесь мы можем сделать... Сейчас, секунду. Я пойму одну простую вещь. В какой
случае мы еще не рассмотрели или не до конца рассмотрели? А, наверное, когда y величины не ноль.
Значит, х ноль. И что мы тогда можем сказать относительно того, как y расположен у нас?
То есть... Сейчас. Ну что получается? Минус 1 на альфа y плюс лямбда минус мю равно нулю. Вот. И мы знаем,
что сумма лямбда и мю равна у нас единице. А тут эта штука не сумма. Обидно. Точно все правильно.
Думаю, время это проверить. Да, вроде правильно. Ну да, градиент по t тут исключительно такой,
никуда ему не деться. То есть, смотрите, как что получилось. Получилось, что... Так, здесь 15.
Что если строить зависимость у икса от у, мы только что получили, что у нас, допустим, вот здесь вот у
нас минус альф, тут альфа. И при у меньше, чем минус альфа, то есть вот здесь вот, у нас х равен, а у плюс...
То есть, типа вот так. Если же наоборот, то так. Сейчас вы понимаете, что происходит между ними. Но между
ними хочется, конечно, получить ноль. Вот. И я сейчас пытаюсь сообразить оперативно, каким образом это
можно сделать. Итак, как будто бы нужно, чтобы... Верно, сейчас это легко получится. Сейчас давайте-ка посмотрим.
Так. Это что значит? Что y равен mu минус лямбда делить на альфа. При этом лямбда плюс mu равно
единице. Так, нужно ли нам еще что-то? Так, а тут, кажется, не совсем правильно. Вот так. Так, лямбда,
получается, равняется единице минус mu. И если сюда подставлять, получается, что y равен чему? 1
минус 2 mu делить на альфа. Вот. Если теперь... Что мы можем увидеть? Мы можем сказать, что у нас mu
большое. Каким-то образом хочется получить ограничение на модуль y. Вот. И не хватает. Кто-нибудь,
может быть, видит, чего не хватает? Ой, что-то я что-то неправильно написал. Никто меня не
управляет. Да, прям. Это же все не так. Наоборот. Типа, я переношу, у меня получается, я умножаю на
минус альфа, умножается на mu минус лямбда. Вот так. Поэтому после подстановки... Значит, я уже
перестал спрятать немножко. Так. Лямбда равняется этой штуке и получается это альфа на лямбда минус
mu. То есть это альфа на 1 минус 2 mu. Да, при этом mu может быть не отрицательно. Вот. И, соответственно,
если mu 0, то x равно альфе. А если mu... Сколько там? 1, да? Ну, правильно. Да, все отлично как раз
получается. Тут же вот эта штука означает, что у нас лямбда и mu не ноль одновременно, а значит mu ну
нуля до одного. Вот. Отсюда следует, что y либо принадлежит от минус альфы до альфы. Все,
отлично получилось. То есть при нуле альфа при единиц минус альфы. И в этом случае у нас получается,
что здесь ноль. Отсюда следует, что вот это условие у нас лежит где? Вот здесь. В том,
вот это условие у нас вот и собственно вот. И последнее, какой это еще нужен? Цвет такой,
отличающийся существенно. Красный, розовый. Так, конечно, как палитра так себе. Ну,
давайте так. Синий уже есть. И зеленый. Вот. И зеленый это вот это выражение. Вот оно,
ну вот здесь. Что это нам дает? Это нам дает свойство решения, что если у нас x кт минус альф кт f
штрих от x кт достаточно мало, то есть по норме меньше альфа, то после соответственно x ка плюс
первое с соответствующими компонентами там it-ами будет равно нулю. Потому что оно поет просто-напросто
вот в эту коле. Таким образом, вот это вот оператор называется soft thresholding понятным причинам. То
есть мягкий порог. Вот. И является стандартом для получения разреженного решения, если вы
таковой хотите, вашей задачи. Вот. То есть наличие такой вот регуляризации, где она тут, вот такой вот
регуляризации приводит к тому, что решение для минимизации нашей исходной функции, вот этой
например, будет являться разреженным. Понятно ли это? Поставьте плюс, если понятно, и минус иначе.
Понятно ли каким образом получилось вывести выражение для soft thresholding? Так, вижу плюс,
один, второй. А, окей, вижу, здорово. Вот. Значит, это, да, важно, важно отметить, что метод,
максимальный градиентный метод для такой задачи называется ISTA, может быть где-то встретите,
быстро его ускоряется FISTA. Спольза прямо такими на собственные для примера использования,
собственно, максимального градиентного метода для получения разреженного решения в линейной
модели. Вот. Здесь 23. Значит, мы сегодня, то есть вот такая же абсолютная технология, она
воспроизводится и для случая матриц, когда вам как бы хотите получить разреженные матрицы.
Если вот здесь вот вам, например, в качестве решения подходит матрица, где мало нулей,
мало не нулей, делайте ровно то же самое только с элементами, с элементами матрицы. Если же вам
нужна малоранговость, то тут чуть похитрее, и, видимо, с этого надо будет начать следующее занятие.
Нонс. Как обеспечить малоранговость решения? Учитывая, что функция ранга не выпукла, вот,
там нужно построить некоторые малоранговость решения. Нужно строить какую-то выпуклую
аппроксимацию функции ранга. Вот. Нам потребуется вспомнить, что такое сингулярное разложение,
сингулярные числа, как они связаны между собой, с артегональностью некоторых матриц и всем таким.
Вот. И окажется, что проксимальный оператор от некоторых матричных функций, которые не распадаются
на функции от их элементной матрицы, тем не менее, можно достаточно эффективно вычислить.
Опираюсь исключительно на проксимальное отображение от сингулярных. Такая вот немножко
магия будет. Но, в общем, это будет в следующий раз. Вот. И в следующий раз мы обсудим,
что такое метод из штрафов, что такое метод модифицированный функции лагранжа. Хотя,
по-моему, в плане было немножко не это. Сейчас, может быть, и я вас немножко обманываю.
Где план-то? План, отлично. Да, слушайте, будет не это. Будет про полупределенную оптимизацию,
но, видимо, и про все эти вещи тоже будет немножко сказано. А чтобы уж совсем не ломать,
не ломать линейность, есть ли какие-то вопросы по сегодняшнему материалу? Ну,
поскольку я не вижу в чате никаких вопросов, вроде никто тоже ничего не спрашивает. Вот.
Тогда всем спасибо за внимание, за реакцию и за какие-то уточнения. Вот. Тогда до следующей
недели заметки я сейчас выложу и примерный план на ближайшие несколько недель тоже.
Всем спасибо и до свидания. Спасибо, до свидания.

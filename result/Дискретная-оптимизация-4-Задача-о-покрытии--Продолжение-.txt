Разбираемся с общей задачей покрытия. Давайте мы вспомним про вершинное покрытие быстро.
Мы с вами там что делали? Мы говорили, что у графа должны быть покрыты все ребра за счет того,
что мы берем только некоторые вершины. И мы говорили, что если вершина взята,
то мы соответствующую переменную выставляем в единицу, иначе мы выставляем в ноль. Вот. И
тогда мы хотим, чтобы для каждого ребра x у плюс x в было больше или равно единице. Ну вот у нас
такая с вами была, значит, такая программа, линейная программа, да, потому что мы ставили
это как задачу линейного программирования, и мы минимизировали сумму w витых на x витых. Вот.
Это задача о вершинном покрытии. Вот. Сегодня мы с вами ведем задачу, такую общую задачу о
покрытии, и увидим очень быстро, что задача о вершинном покрытии, это только такой очень
специальный частный случай, и про общую задачу с вами тоже поговорим. Вот. Значит, общая задача
о покрытии. Ну давайте я ее сюда, я думаю, вмещу. Она называется set cover или просто cover.
В этой задачке у нас есть уже теперь не вершины ребра графа, а просто какие-то абстрактные
покрываемые объекты, сущности, да, и покрывающие. Я, по-моему, на первой встрече наши с вами приводил
пример, например, когда мы хотим объявлениями какими-то, да, рекламой покрыть целевую аудиторию,
или покрыть, значит, какими-то сотрудниками множество тех скиллов, которые необходимы для
реализации какого-то проекта. Вот. Или мы хотим покрыть, например, какими-то магазинами, или там
школами, или детскими садами, мы хотим покрыть инфраструктурно какой-то микрорайон, то есть мы
думаем там, где именно размещать там школу, детский сад, и так далее. В общем, задача о покрытии
может быть очень много, они все очень полезны. И мы с вами все такие задачи будем абстрактным
образом математически описывать так. Мы будем говорить, что у нас есть матрица, у которой строчки,
ну давайте строчки, да, значит, у которой строчки это покрывающие объекты, а столбцы покрываемые,
то есть вот я здесь напишу, что столбцы соответствуют покрываемым, покрываемые. А строчки
соответствуют объектам покрывающим. Покрывающие. И на пересечении строки из столбца у нас будет
ставится единица, если соответствующий объект, покрывающий, он действительно охватывает вот
этот вот покрываемый объект. Я думаю, что вы, если графы, в принципе, проходили, то вы проходили
разные способы задавать граф. И в числе таких способов вы упоминали обычно матрицу инциденций,
или матрица инцидентности, или матрица инцидентностей. Матрица смежности, а если ты и та и та матрица,
просто есть на графе два отношения, инцидентности и смежности. Помните, чем они отличаются?
Ну, можно сказать так. Я бы сказал, смежность – это отношение между объектами одного типа,
то есть, например, две вершины смежны, если между ними ребро есть. Два ребра тоже можно назвать
смежными, если у них есть общий конец. А инцидентность – это отношение между объектами
разных типов. Вершина и ребро являются инцидентными, если вершина является концом этого ребра. Вот у
нас есть матрица смежности, которая кодирует граф, есть матрица инцидентности, которая тоже способна
закодировать этот граф. И в матрице инцидентности, ну, ее можно по-разному вращать, но давайте посчитаем,
что в матрице инцидентности строчки соответствуют вершинам, а столбцы соответствуют ребрам. И смотрите,
как получается. Получается, что как раз покрывающие объекты – это у нас вершины, а столбцы – это
покрываемые – это ребра. И вот вершинами покрывать ребра в графе – это то же самое,
что в матрице инциденции этого графа строчками этой матрицы покрывать столбцы. То есть теперь,
когда мы с вами переходим в эту общую постановку, что нам нужно сделать? Нам нужно выбрать такое
наименьшее либо по мощности, либо по весу под множество строчек этой матрицы, чтобы покрыть все
столбцы. А что значит покрыть все столбцы? Это значит, чтобы в каждом столбце была хоть одна
единичка на пересечении с выбранным под множеством строк. Вот такая у нас с вами будет задача –
выбрать множество строк минимального веса, ну или минимальной мощности, покрывающие все столбцы.
Покрывающие все столбцы. Ну вот, ну давайте мы сформулируем эту задачу, как всегда,
как задачу линейного программирования, посмотрим, какие у нее есть вариации и какие у нее отличия
вот этой вот очень частной задачки. Понятно, что в каждой строке матрицы мы способны сопоставить
переменную выбора decision variable. Берем мы эту строчку в покрытие или не берем? Ну давайте считать,
что у нас m-строчек. Вот, и тогда линейно-программистская формулировка для задачи о покрытии целочисленная.
Как она будет выглядеть? Значит, мы минимизируем, ну как всегда, в случае с вершинным покрытием,
абсолютно то же самое функция, сумма w i t x i t по i от единицы до m. Ну да, вроде так. Значит,
сумма весов всех выбранных строчек. Как раз получается такая. Переменные у нас целочисленные,
и от нуля до единицы, конечно, тоже все. И как мы будем с вами условия покрываемости записывать?
Ну, видимо, раньше у нас концы ребра, переменные соответствующие концам ребра в сумме должны
были давать не меньше единицы что-то здесь. Мы смотрим, а где в столбце стоят единички,
и соответствующие переменные суммируем и говорим, что сумма этих переменных должна быть как минимум
единичка. Чтобы хотя бы одна из строк, которая была способна покрыть этот столбец, была взята. Но
если мы будем считать, что в матрице у нас n столбцов, то тогда для каждого n, для каждого g виноват.
Вот единица до n. Мы с вами потребуем, чтобы сумма иксов каких-то, сейчас с вами посмотрим каких,
была больше или равна единице. Вообще, если эту матрицу мы с вами обозначим матрицей A, например.
Да, то могу, конечно. Да то же самое, что сейчас я просто не подписал еще, что у нас снизу суммы,
по какому множеству мы суммируем. Вот смотрите, в вершинном покрытии вот такое было неравенство,
потому что мы хотели, чтобы для каждого ребра хотя бы один из его концов был взят. Вот хотя
бы один из концов ребра был взят. Здесь мы то же самое, что практически хотим, только у нас не такая
задачка ограниченная. Вот в матрице инциденции у нас в каждом столбце было бы ровно по две единички,
и поэтому здесь возникает слева сумма двух иксов. А так вообще, если в столбце три единички,
то здесь будет сумма трех иксов больше или равна единице. Такое неравенство, потому что мы
кодируем как бы, что хотя бы одна из строчек, которая покрывает этот столбец, должна войти в
покрытие. Вот это у нас было, хотя бы одна из вершин, которая способна покрыть это ребро,
должна войти в покрытие. Здесь то же самое. Вот, только надо бы нам как-то компактно это записать,
но если это матрица, вот эту матрицу, которая кодирует нам задачу вообще, обозначить через А,
и ее элементы обозначить А и Т житые, то тогда мы с вами можем записать здесь вот так. Сумма тех
иксов, для которых А и Т житые равняется единице. То есть как раз сумма по тем рочкам, которые
способны покрыть нам житый столбец. Но можем мы вот это как-то записать по-другому? Видимо можем,
поскольку ашки сами это нолики единицы, то вместо того, чтобы писать здесь какую-то сумму по
непонятному множеству ашек, ну то есть оно понятное, но оно какое-то... Где тряпка? Это
действительно непонятно, это гораздо менее тривиально. О, спасибо. Найти тряпку не менее
важно, чем найти решение патрудной задачи иногда. Вот, значит мы можем ту же самую сумму
выписать по-другому. Просто сумма х и на А и Т житые и просуммировать уже по всем и. То есть
сумма А и Т житые помножить на х и. Сумма по и единичке до н. То есть там, где ашка равна нулю,
там слагаемая в сумме автоматически пропадает. Там, где ашка равна единице, мы получаем просто
х и ты участвуют в этой сумме. То есть в принципе то, что было написано вот здесь внизу, это
эквивалентно вот такой вот сумме формально. М на н. Да, спасибо, да, здесь м. Это хорошее
замечание. А то у меня бы сейчас доска вызвала недопустимую операцию и была бы закрыта за
переполнение индексов в массиве. Да нет, теперь это вообще произвольная матрица из 0 единиц,
которая нам кодирует задачу. Да, то есть я просто пытаюсь вам все время привести параллель как бы
с уже известной нам задачей вершинного покрытия, но сейчас это уже произвольная матрица. Значит да,
кстати, давайте заметим, что вот от такой задачки CLP не трудно перейти к задаче CLP для каких-то
задач типа покрытия шаг влево, шаг вправо. Вот, например, не всегда нам нужно покрыть прям все-все-все
столбцы. Давайте предположим, что нам нужно покрыть только 90 процентов, например, столбцов. Вот,
давайте пример такой разберем. Пример, как моделировать, когда нужно покрыть только 90
процентов столбцов. Значит требуется покрыть не менее 90 процентов столбцов, например. Вот,
в CLP постановке это вполне можно обеспечить небольшой модификацией. Давайте мы введем еще
одни переменные. Это уже не будут наши переменные выбора непосредственно, какие строчки мы выбираем,
это будут такие вспомогательные переменные. Давайте для каждого столбца введем переменную
YG, которая будет означать, что этот столбец покрыт. Вот YG по смыслу это единичка, если покрываем
столбец, покрываем житый столбец и ноль иначе. Тогда как можно вот это неравенство преобразовать?
Если нам столбец надо покрыть, то тогда вот такое неравенство должно быть выполнено. Если столбец
не обязательно покрывать, то тогда это неравенство просто как бы нужно исключить эффективно. Как нам
его видоизменить тогда, если есть вот такие переменные YG? А или вот нельзя формально добавлять
задачу CLP? А минимум тоже так просто нельзя добавить. То есть у нас, смотрите, у нас должна быть
задача, система такая большая, никаких совокупностей. Система с фиксированным
числом неравенства с фиксированными коэффициентами, да, с константными. Что же делать?
А если бы мы здесь нолик поставили вот в этом неравенстве, вот такое неравенство всегда
выполняется? Нет, то есть оно вот такое неравенство по факту не является ограничением, да, это такое
чисто эффективное ограничение. А вот это правда ограничение. Не для всех наборов X-ов такое
неравенство выполнится, да, только для некоторых. Так что же нам делать? Как его преобразовать так,
чтобы вот эту переменную сюда как бы вставить? Мы как раз можем поставить вместо единички в правой
части ровно вот эту переменную. То есть написать сумма A и G X и по всем и от единицы до m больше или
равна YG. И это мы сделаем для всех G, да, то есть мы теперь требуем, чтобы для всех G вот единицы до m было
выполнено вот такое неравенство. А это все равно по-прежнему линейное неравенство, не смотрите на
то, что здесь переменная есть в правой части. Вот это как раз никто никто не обещает, что переменные
только в левой части неравенства нам удобно записывать. В конце концов мы его можем преобразовать
как бы в неравенство, где все переменные сгружены влево, а справа константа ноль. Но по смыслу неравенство
получается такое, да, что если эта переменная выставлена в единицу, то мы правда должны покрыть этот
столбец, а если она выставлена в нолик, то это неравенство оно и так всегда выполнено при любых
х. Ура! А как нам тогда вот это вот условие потребовать с помощью неравенства?
Чего у нас? Точно, 0,9 на n. Сумма по всем живут единички. Видите, как просто. То есть на самом деле мы с
помощью вот этого механизма CLP можем задачки превращать из абсолютно строгих, таких часто,
в задачки, где нам нужно вот там на 90 процентов достичь результат или на любой фиксированный
процент, да, то есть можно немножко смягчать вот жесткость задачи, и это очень такой гибкий
механизм CLP. Мы потихоньку с вами вот будем рассматривать примеры, как такие или другие
условия смоделировать на этом языке. Вот как язык программирования прям получается. Ну ладно,
это на самом деле просто пример. Мне хотелось вам показать, а решать-то мы будем все равно вот такую
вот задачку. Давайте представим, что мы ее решили таки, но не CLP, а просто LP, как мы это всегда
делаем. Рассмотрели CLP, потом перешли просто к задаче линейного программирования, потом эту
задачу решили и получили некоторое решение. x1 со звездочкой и так далее, xm со звездочкой,
но решение это уже на отрезке 0,1. Вот, ваши предложения, что нам делать с округлением? Вот
помните, как мы округляли в задаче о вершинном покрытии? Мы там как округляли? Помните, тоже
получали набор. Что мы там делали? Так, точно. Ну, к ближайшему целому, у нас тут все было так
здорово и замечательно, к ближайшему целому. Все округляли, и это интуитивно понятно как-то,
это то, что мы привыкли понимать под округлением, да, но еще и работало у нас, действительно. Вот,
а что у нас вот тут? Можем ли мы всегда гарантировать, что вот эти переменные,
так можно на раз округлить к ближайшему целому, получится корректное решение? Вот,
а в чем может быть проблема? Да, вот тут почему все работало, да, почему мы гарантировали,
что у вас в таком неравенстве хотя бы одна переменная точно уже будет не меньше одной
и второй, потому что их всего две было, да, и когда две переменные, да, наибольшие из них не меньше
одной и второй, а когда у нас матрица произвольная, здесь может быть по три единички в столбце и больше,
то и неравенства возникают с тремя и больше переменными, и тогда может быть что неравенство
выполнено, но там все переменные по одной трети, например. Окей, а мы можем все равно вот какое-то
правило сформулировать, давайте пофантазируем немножко, какое-то правило, чтобы округлять
переменные вот чисто по порогу и говорить, что если х со звездочкой этой больше или равен
какого-то порога, ну тогда переменную округляем к единице, если меньше порога того же самого,
вот, то тогда смело можем в ноль округлить, и вот чтобы это работало, чтобы это приводило
к корректному решению. 1 минус 1 на n, ну да, вот не может быть больше m переменных здесь,
1 минус 1 на n, наверное. Ну и в целом можно сказать, что если у нас есть какое-то ограничение на
максимальное количество единиц в столбце, то тогда можно единицы поделить на вот это количество,
взять в качестве порога, да. А можем мы это записать как-то формально? Единицы поделить на что?
Ничего, ну на m, да, ну что-то может быть более точное, что от матрицы зависит,
давайте попробуем поставить. Но нет, боюсь, что с ранга мне не получится. Давайте, может быть,
я неправильно просто вопрос оформлировал, что от вас хочу. Я хочу, чтобы мы в терминах матрицы
как-то записали это максимальное количество единиц в столбце, давайте я так это и запишу,
максимум по всем столбцам, по всем столбцам, а количество единиц в столбце как вообще?
Да, сумма ежитых, по всем и от единицы до m. Ну, не очень компактная формула, но тем не менее, да.
Конечно, одно число такое для всей матрицы. И тогда, в частном случае, вот, например,
такое округление, в частном случае, когда эта матрица инциденции графа, это дает нам просто в
точности алгоритм для вершинного покрытия, который мы с вами уже рассматривали. Окей, вот такой
выбор иксов, он корректный. А что можно сказать про, поясните здесь еще раз, да, вот, почему мы
гарантируем, что при таком пороге решение точно оказывается корректным? Потому что у нас было вот
такое вот неравенство, так. А сколько переменных иксов входило по факту в левую часть такого
неравенства? Ну, не больше, чем максимальное количество ашек, да, вот этих равных единиц. То есть
икс, каждый раз, когда ашка равна единице, икс у нас появляется, действительно, в этой сумме. Когда
ашка равна нуле, он по факту здесь не появляется, не участвует. Значит, максимальное количество
ненулевых слагаемых вот в этом неравенстве у нас равняется максимальному количеству единиц
среди ашек в житом столбце. Максимум берется по всем ж, чтобы можно было использовать один порог
на все случаи жизни, да, при округлении всех переменных. Значит, максимум берется по всем
столбцам. И вот это, это просто количество слагаемых максимально в этой сумме ненулевых, ну,
зависящих от икса, да. И мы используем то же самое рассуждение, что здесь. Здесь у нас было два икса,
мы говорили, что максимальный из них точно не меньше одной второй. Если у нас три икса, то
максимальный из них точно не меньше одной третьей. Если у нас десять иксов, то максимальный из них
не меньше одной десятой. Вот и все. И тогда мы фактически гарантируем, что после округления,
вот этот самый икс, который точно не меньше одной десятой, он выставится в единичку. И вот здесь
вот у нас, за счёт этой единички, будет по-прежнему что-то больше или равной единице, и неравенство
даже после округления при смене звездочек вот здесь на крышечке, оно останется верным. Нам же
это нужно, по сути, перейти от нецелочисленных к целочисленным переменным, так чтобы все
неравенства у нас сохранились. А теперь давайте тоже вспомним, как мы оценивали значение
целевой функции на округленном решении в задаче о вершинном покрытии, и посмотрим, как нам оценить
значение целевой функции вот в этой задаче. Значит, наша целевая функция, давайте я ее назову,
ладно, давайте я назову obj. Но я здесь неспроста это называю obj, дело в том, что, как правило,
когда программируют, пишут программы в ограничениях, в частности там какие-нибудь
линейные программы, у профессионалов, профессионалы часто называют целевую функцию как раз таким
сокращением obj, точно так же, как мы привыкли называть сокращением opt, оптимальное значение
целевой функции. И вот то значение, которое мы получаем, округлив эти переменные, как его
можно сравнить с оптимальным значением? Ну, по аналогии, если действовать с задачкой о вершинном
покрытии, там помните, какая константа была? 2, да. А здесь какая константа будет? Вот эта, да,
которая здесь стоит в знаменателе. То есть и рассуждение абсолютно такое же, опять-таки,
когда мы округляем от вот такого значения как минимум к единице, то наша переменная возрастает
как максимум вот во столько раз. И раз каждая переменная возрастает как максимум во столько раз,
то и вот такая сумма, которая является нашей целевой функцией, она тоже возрастает как
максимум во столько же раз. Так что здесь мы способны поставить вот эту самую константу,
да, значит, максимум сумма а ежи по и, а максимум по жи. Пока что не делаю так все конспективно,
потому что это просто чистая аналогия с вершинным покрытием, просто один в один,
поэтому не расписываю все. Вот. Давайте посмотрим какие еще, ну понятно, наверное,
что эта штука, когда она большая, то алгоритм получается какой-то ну не очень, да. Мы вынуждены
округлять большинство переменных к единичке, когда порог очень низкий, и тогда показатель
апроксимации у нас тоже страдает. Чем он выше, тем хуже. Вот. Какие мы модификации можем проделать?
Ну, во-первых, мы можем округлять каждую переменную со своим порогом, то есть мы можем смотреть,
вот надо нам округлить переменную х1. Мы же можем вот здесь вот взять максимум только по тем
столцам, которые этой переменной покрываются, да. То есть если переменная участвует не во всех
неравенствах, а только в каких-то, то для нее не надо выставлять супер мега порог, да, какой-то
низкий супер мега, да, очень низкий порог не надо выставлять. То есть здесь максимум можно
брать только по тем столцам, которые покрываются вот конкретной этой строкой. Это раз. Но во-вторых,
то, что может быть более интересно, это можно попытаться выбрать наименьший порог, который общий
для всех переменных так, чтобы переменные образовали покрытие. Адаптивный порог.
Значит, давайте я попытаюсь это как-то изобразить картинкой. Вот у нас x1, x2, x3 и так далее, xm со звездочками.
Это у нас нолик, это единичка. Ну а так вообще x и где-то вот каждый x, он на какой-то отметке
здесь стоит. Какой-то выше, какой-то ниже. Когда мы выбираем некий порог, мы здесь рисуем такую, да,
линию фактически. И все x, которые попали выше этого порога, мы соответствующие строчки матрицы
берем в покрытие. Ну или, что то же самое, округляем соответствующие x единицы. Проводим такую здесь
черту между 0 и единицей. Смотрим, кто выше нее оказался. Воду наливаем в аквариум и смотрим,
какие рыбки оказались в воде. Вот или не рыбки, хомячки, а какие не задохнулись. Вот какие не
задохнулись, мы, значит, берем их в покрытие. Они, значит, сильны духом. Ну вот, и вот здесь вот мы
выставляем порог очень низкий, да, какой-то стелищееся здесь, близко к нулю. Получается,
что много хомячков оказываются в покрытии. Ну и ладно, я оставлю это негуманная аналогия,
извините, с хомячками. Но дальше давайте попробуем посмотреть, а если мы будем двигать вот этот
порог, будем плавненько поднимать, да, вот эту вот линию, или наоборот, скорее плавно опускать.
Нам же нужно, чем выше порог сделать, тем лучше, потому что чем выше порог, тем у нас меньше показатель
аппроксимации. Он с порогом, они связаны как инверстные друг другу числа, да, обратные друг
другу числа. Вот поэтому мы можем начать, просто представьте себе, да, вот с такой прямой, равной единице,
потом потихонечку ее опускать. А что будет с переменами происходить? Мы опускаем-упускаем, ага,
сначала х3 у нас выставится единицей, да, потом снова опускаем-опускаем-опускаем, потом х1 выставится
единицей, потом снова опускаем-опускаем-опускаем, потом хм выставится единицей, потом х2 выставится
единицей, да, вот так потихоньку все больше-больше переменных выставляются единичками. Ну понятно,
что в конце концов мы опустимся до нуля, точно получим покрытие, но где-то посередине существует
какое-то значение порога, для которого мы уже имеем покрытие, правда? Вот, и вот его-то, наверное,
и надо взять. А как вы можете выбирать такое значение порога? Вопрос. Как его быстро выбрать?
Вы же не можете вот так вот плавно менять, ну не знаю, может и можете, можно написать вуличную
программу, где есть такой ползунок, который двигает пользователь и смотрит, как вот эти
вот штуки оказываются над водой, под водой. Вот, но это красиво, но не очень прагматично, да?
Минимум из переменных, но тогда все они в единицу окажутся. Нам нужно выбрать как-то
такой максимальный порог, что если взять только переменные выше этого порога, мы получаем покрытие.
Вот такое максимальное число, при котором это корректное решение. Как вообще такое искать?
Бин поиском можно, да? Вот у нас есть... Что такое? А чего такого в том, что ваше предположение в том,
что оно актуально, почему нет? У нас есть универсальный алгоритм, действительно,
потому что вот проверка того, при фиксированном пороге, проверка того, что он дает корректное
решение, она не такая уж, не такая быстрая, то есть надо как бы не очень много попыток вообще
сделать выбор этого порога, поэтому бин поиск это то, что отлично здесь подходит. За логарифмичное
число действий мы можем подобрать такой порог, чтобы и проверить, что соответствующее покрытие
корректно. А что мы еще можем сделать? Давайте посмотрим альтернативный взгляд на вещи. Значит,
вот я здесь сотру этот пример и напишу как раз первая это ваша идея с бин поиском в выборе
порога. Значит, подход один. Да, берем порог одна-вторая и смотрим. Да, вот образуется некое множество
иксов. Образует оно покрытие или нет? Не образует. Окей, значит, порог нужно взять выше, ниже,
ниже, да, значит, ниже. Берем порог одна четверть тогда. Ого, уже не покрытие. Нет,
еще допустим, уже не покрытие, еще покрытие. Окей, значит, порог выше тогда берем, то есть
одна-вторая, потом одна-четверть, потом что там будет? Да, что-то странно. Одна-вторая,
одна-четверть, а посредине-то что? Три восьмых. Вот, и так дальше, да, но обычный бин поиск. Вот,
подход один, бин поиск. А подход два мы узнаем после перерыва. Все, перерыв. Во-первых,
я открыл форму, теперь если у кого-то она не работала, наверное, если она у всех не работала
до селя, то теперь она работает тоже у всех. Можно, да, да, да, даже нужно. Вот, зачем нужно
стэпик от всех, даже те, кто не собирается додавать задания по курсу, я вот стэпик традиционно
использую стэпик ID как ваш просто ID-шник на курсе, чтобы не по фамилии вас там перечислять,
а по номеру. Так просто веселее по номеру, вот, в табличке перечислять. Но дело в том,
что это какой-то компромисс. На западе вообще в некоторых вузах запрещено публиковать общие
списки оценок, например, студентов, вот, но чтобы как бы не как-то соответствовать,
но не до конца, я решил в таблицах указывать стэпик ID. Вопрос. Да.
Стэпик ID в любом случае потребуется, то есть я его все равно, он мне нужен. Вы финальный тест,
ну, как бы этот тест на получение оценки, помните, про который мы говорили, чтобы ну просто такой тест
по теории, чтобы просто проставить оценку, от него сама оценка не зависит, но его надо закрыть
таки, чтобы эту оценку проставить, чтобы подытожить материал. Вот, он все равно в любом случае,
он точно будет на стэпике, поэтому на стэпике все равно придется регистрироваться. И если вы на
курсере, то тоже мне нужно, потому что я вас буду идентифицировать по стэпик ID, я вас по нему
идентифицирую. Так, ну чего там у нас? Бинарный поиск, это мы с вами выяснили. А как еще можно
посмотреть на вот эту ситуацию с иксами? Вот эти иксы мы сразу можем сказать при понижении порога
сверху вниз, при плавном движении порога сверху вниз, в каком порядке эти иксы будут бамс-бамс-бамс
оказываться над водой последовательно. Мы это понимаем или нет? В порядке убывания, конечно. И это
как раз подход два. Подход два, да, просто отсортировать иксы в порядке убывания. Значит,
пересортировать или просто вот сортировать по убыванию. Отсортировать по убыванию. Вам знакомы
такое обозначение x с индексом p от единички, p от двойки, нет? Да, перестановка, просто берем
такую перестановку, которая сортирует наши иксы. Так это отлично. Смотрите, когда нам что-то
незнакомое, это вообще здорово, потому что мы что-то новое узнали. Икс со звездочкой с индексом p от m.
Вот, то есть p это перестановка, но их часто, самые две буквы частые для обозначения перестановок,
это p и сигма. Значит, а перестановка это взаимно однозначное отображение между множеством,
в данном случае номеров от единички до n, и этим же множеством от единички до n. То есть,
вот эта последовательность p от единицы, p от двойки и так далее, p от m, это просто какая-то
перестановка этих же самых чисел от одного до n. В таком порядке, который является сортировкой
вот этих вот переменных. Вот и все. Вот вся суть этого обозначения. Ну вот, и дальше понимаете,
что мы делаем. Дальше мы вместо того, чтобы вообще рассматривать порог теперь, мы просто
проходимся последовательно по иксам в этом списке и берем очередную строчку матрицы. И
первый же момент, когда взятых строчек достаточно, чтобы у нас получилось покрытие,
мы останавливаемся. Больше того, здесь можно тоже сэкономить. Ведь не всегда,
беря очередную строчку матрицы, мы с вами вообще покрываем новые столбцы. Представьте,
что вы уже достаточно много строк взяли, и у вас там осталось два столбца непокрытым,
например, из всей матрицы. Но велика вероятность, что взяв очередную строчку,
вы никаких новых столбцов не покроете. Тогда его можно просто пропустить. И получается,
что вообще это уже даже будет что-то эффективнее, чем просто пороговое округление. Это какое-то
округление такое с подбором. Мало того, что порога, так еще мы некоторые переменные,
несмотря даже на этот порог, все равно округляем к нулю, если они нам ничего не дают на фоне
остальных переменных. Вот такая забавная получается, такой взгляд на вещи. Это знаете,
на что похоже? Вот такой взгляд со стороны порога и со стороны переменных. Мне очень нравится,
как поступили создатели Пэкмен. Знаете такую игру Пэкмен? Ну, конечно, кто не знает. Такой
колобок, который ходит и всех ест. Нет, он не всех ест, его едят. Привидения за ним гоняются,
а он там съедает шарики. Так вот, как в этой игре в те времена, допотопные, когда не было
никаких мощных компьютеров, игры вообще работали не в компьютерах, а в автоматах. Каким-то монетку
надо опускать, и вот тогда игра запускается на несколько минут, и ты в нее играешь. Значит,
все было ж примитивным, и игра должна была быть очень быстрой. А как в таких играх реализовать
искусственный интеллект, чтобы привидения бегали, ну как-то перемещались более-менее туда,
куда Пэкмен движется. А он же движется, добавок, по лабиринту. Там еще какой-то лабиринт из этих
шариков и стен. И был подход первый. Взять и каждый раз, каждый момент времени решать задачу о
кратчайшем пути. Для каждого привидения запускать, как оно может добраться к самым коротким маршрутам
до Пэкмена. И мало того, что это тормозно, так еще Пэкмен в следующий момент времени уже уйдет
оттуда, окажется где-то в другом месте абсолютно. И гениальный подход, который создатели этой игры
реализовали, они даже статью опубликовали в свое время по этому поводу. Значит, он такой,
просто Пэкмен в лабиринте он оставляет такой запахок. Вот представьте, что Пэкмен не пользовался
дезодорантом, а он бегает шеей, он поэтому так пахнет. Вот, и он по лабиринту движется и оставляет
за собой такой запаховый след. Ну или как муравьи, можно сказать. Вот муравьи, насекомые, они тоже
запахами общаются вообще-то. Когда муравей ползет к чему-то вкусному, когда он ползет просто куда-то
и видит там что-то вкусное, он потом ползет обратно к муравейнику, а по дороге он оставляет такой
след запаха. Есть даже дискретная оптимизация алгоритм муравьиных колоний, который основан вот
ровно на таком подходе. Так вот, в этой игре Пэкмен, значит, в каждой клетке лабиринта просто
хранилась переменная, целочисленная или не целочисленная, уже не помню, в которой хранился запах.
И этот запах, когда там есть Пэкмен, он максимальный, и как только Пэкмен выходит из клетки, запах со
временем начинает притухать, вот так вот, до нуля опускаться, и приведения движутся просто по запаху,
они из текущей клетки движутся туда, где запах выше всего. И вот так вот получается довольно
эффективная модель, которая неплохо соответствует какому-то искусственному интеллекту, ну не очень,
конечно, продвинутому, но забавно. Вот, то есть, это взяли и посмотрели вместо того, чтобы смотреть
с точки зрения агентов, с точки зрения приведений, посмотрели на всю ситуацию с точки зрения игрового
поля. Игровое поле теперь является хранилищем информации само по себе, вот этого запаха
условного. И у нас здесь тоже, мы наоборот перешли от как бы игрового поля к какого-то общего порога
там для переменных, мы взяли и посмотрели аккуратно, как эта вся ситуация вообще выглядит со стороны
переменных, и тогда мы смогли понять в каком порядке эти переменные по сути округлять.
Конечно, и приведения идут в сторону возрастания запаха. Они перестают, не ищут больше, каждый раз
тоже кратчайший путь до точки, где пыкмен опять. Нет, они просто вот в соседние клетки рассматривают
четыре штуки, да, и идут в ту клетку из соседних, где запах больше. Так что так, да. Так, ну вот, я уже
забыл, нет, не забыл, почти забыл, что хотел рассказать дальше, но дальше нам нужно рассматривать
еще какой-то продвинутый алгоритм для этой задачки. Вот давайте это делать. А у вас тервьер пока
вообще еще не так совсем. Давайте тогда мы сегодня начнем жадный алгоритм для этой задачки. А если у
вас все-таки тервьер придет к вам в дом, тервьер, то так он только в следующем не придет он в дом
в этом семестре. Ну, в общем, может, тогда мы и пропустим вероятностное округление. Да, слушайте,
давайте я просто скажу, расскажу вероятностное округление, но мы ничего про него не будем доказывать,
ладно? Какие вы продвинутые. А вы конструировали неизмеримое полибегу множество? Да. О, какие
молодцы. А я уже не знаю, как его конструировать, когда ты вот знал. Третий подход, который подход 3,
который мы с вами без доказательства, я сформулирую просто результат, ну а сам подход мы с вами,
конечно, целиком просто разберем. Он называется вероятностное округление или рандомизированное
округление randomized rounding. И он основан на очень простом соображении. Если у нас переменные все
равно от 0 до 1, то вообще любое число от 0 до 1 можно трактовать как вероятность чего-то. А вероятность
чего? Ну, того, что мы соответствующую строку матрицы возьмем в покрытие. Нет у нас больше
детерминированного какого-то порога. Мы монетку подбрасываем. А вот и вся идея. То есть теперь мы
с вами можем так сформулировать алгоритм. Пробегаемся по И от единички до М. И если,
ну да, значит, и берем эту строчку матрицы в покрытие с вероятностью х и т со звездой. Я здесь
напишу такую штуку интересную. Если random меньше или равен х и т со звездой, то х и т с крышкой
устанавливаем единицу. Знаете такой прием программирования? Если функция random возвращает
случайно равномерно распределенное на отрезке 0 и 1 число, то вот такое вот неравенство как раз
и выполняется с вероятностью в точности х и со звездочкой. То есть фактически здесь можете
написать просто с вероятностью х и со звездочкой, подбросив монетку или там что-то еще, кубик,
да, мы берем строчку в покрытие. То есть округляем х и т к единице. Будет у нас покрытие в результате
вот по истечении этого цикла, когда мы пробежались по каждой строке едино и подбросили
монетку и с вероятностью какой-то отключили строчку покрытия. Непонятно, непонятно,
потому что все случайно, да, то есть может быть мы вообще все строчки возьмем, а может быть мы
ни одной строчки не возьмем, как монетки выпадут, как карта ляжет. И значит это нужно включить в
еще один какой-то цикл, который будет таким стражником все-таки, он будет следить за тем,
чтобы решение у нас получалось корректно. Поэтому давайте мы этот цикл 4 поместим внутри еще одного
цикла while. Но я так напишу, да, пока у нас не покрытие, мы будем делать вот эту. Причем давайте
мы алгоритм оставим вот именно так, как он написан, без реанитализации. То есть можно
считать вот что, что если мы какой-то х итой округлили к единице, то когда мы пойдем на
следующую итерацию вот этого цикла while, мы этот х итой оставим равным единице. Мы не будем
снова для него подбрасывать монетку. Видите, мы здесь нигде не полагаем х итой равными нулю,
здесь только один assignment, одно присваивание. х итой равным нулю можно положить в самом начале,
вот здесь до while. х итой полагается равным нулю для всех и. Это такая инициализация. А потом уже
х итые только в единичку устанавливаются. Понятно, что если этот цикл while проработает
достаточно долго, то у нас все х итые вообще установятся в единицу. Но если бы здесь не было
вот этого условия, если просто цикл while бесконечно долго вращался, то тогда все х и у нас
остановились бы в единицу и, конечно, у нас было бы покрытие. Вот, но естественно, что мы вот
крутимся только до тех пор, пока у нас не покрытие и останавливаемся, когда сможем.
С реинициализацией тоже, в принципе, да, безусловно, да. Вот, но теорема, я почему так написал
все-таки без реинициализации внутри while, потому что теорему я только для такой версии алгоритма
доказал бы вам, если бы у вас был тервер. Смотрите, теоремка такая, что с вероятностью,
ну все числа условные тоже, надо понимать их, все можно немножко двигать. С вероятностью больше
чем, ну скажем, 0,9. С вероятностью больше, чем 0,9, алгоритм остановится за время,
о большое, вот алгоритма n раундов. Что такое раунд? Вот один раунд вероятностного округления – это
один пробег цикла 4. Давайте я это здесь вот напишу. Вот этот цикл 4 – это один раунд. Раунд округления.
Хорошо, что можно писать на двух языках, потому что округление – это и так раундинг, и так
получилось по раундов раундинг. То есть очень получилось непонятно, наверное. А так понятно,
но это тоже, наверное, правда понятно. Значит, алгоритм остановится за логарифмичное число раундов,
ну то есть за полинамиальное и очень вполне какое-то разумное время, и вес покрытия. Так,
вот теперь про вес покрытия, что можно сказать. Поскольку это алгоритм опять-таки с какой-то
случайностью внутри, то для таких алгоритмов мы обычно говорим про ожидаемый вес покрытия,
мат ожидания. Мат ожидания, вы точное определение, даже если кто не знает, можете считать, что это
в среднем. Если много-много раз перезапускать этот алгоритм, то в среднем он будет выдавать
вот ожидаемое какое-то, ожидаемое по весу покрытие. Ожидаемый вес
покрытия будет O от OPT на лоноритм N. Давайте я не буду здесь O писать под O,
просто O от лоноритм N умножить на O. На самом деле можно здесь поставить любую константу,
близкую к единице, настолько, насколько хочется. Просто от того, насколько эта константа близко к
единице, будет зависеть константа вот в этом большом, алгоритм будет чуть тормознее, и
соответственно ожидаемый вес покрытия тоже получится чуть побольше. Смотрите-ка, в отличие
от задачки о вершинном покрытии, мы здесь не имеем константного показателя апроксимации.
Чем матрица больше, чем она шире, чем больше в ней столбцов, тем хуже у нас показатель
апроксимации. Дальше мы с вами рассмотрим жадную иуристику, немножко отдохнем от линейного
программирования, от всяких округлений, и так уже хватило нам и вероятностного округления
появилось, и Бог знает, что еще. Вот мы сейчас рассмотрим вполне детерминированную жадную иуристику
для задачи о покрытии, но увидим, что для нее оценка показателя апроксимации, к сожалению,
такая же. Тоже логариф мотен, не лучше. Ну, давайте начнем это дело. Мы сегодня вряд ли успеем
закончить с доказательством, но начать мы точно сможем. Сейчас я вот это вот постираю, ладно, могу.
Какая схема доказательства? Мы рассматриваем сначала такое событие, вероятность того,
что за один раунд округления какой-то столбец остался непокрытым. Оказывается, что эта вероятность
можно оценить константой единицы на Е. Е — это основа натурального алгорифма. Дальше, исходя из того,
что с хорошей вероятностью мы один столбец покрываем за один раунд, можно оценить,
что за логарифмичное число раундов мы все столбцы покроем, скорее всего. А дальше, учитывая,
что мы логарифмичное число раундов всего работаем с хорошей вероятностью, при этом условии там уже
нетрудно оценить мат ожидания. Дело в том, что, смотрите дальше, как оценивается, вот каково
мат ожидания веса строк, которые будут взяты в покрытие. Вот примерно можно сказать, что раз строка
берется с вероятностью х и т со звездочкой, а вес у нее равен w и t, то мат ожидания как бы вклада
этой строки за один раунд получается вес строки помножить на х и т со звездочкой. А поскольку сумма
весов w и t, не переписывайте это только, я просто напомню, вот наша целевая функция, сумма весов
на х и т со звездочкой — это нижняя оценка на оптимум, то у нас и получается, что мат ожидания
равняется за один раунд веса вот такой штуки, за логарифмическое число раундов оно равняется
вот такой штуке помножить на логарифм n, что не превосходит логарифм n помножить на оптимум. То есть
мы опять там пользуемся, конечно, безусловно, тем, что х со звездочкой — это не а во что, а это
оптимальное решение задачи не целочисленной, задачи лениного программирования, которая связана
с значением оптимального веса покрытия. Ну вот, так что все то же самое, только с тервером. Вот,
но мы опять используем основное свойство ленинной релаксации. А теперь мы не будем
использовать никакие свойства ленинной релаксации, мы просто с вами жадную евристику обсудим для
задачи о покрытии. Жадная евристика. Жадная евристика работает следующим образом.
А вы, наверное, сами сейчас опишите, как она работает. Так, чтобы покрыть как можно больше
столбцов за раз, да? Окей, эти столбцы надо покрыть. Да, то есть понятно, что раз эти столбцы уже
покрыты, нам не надо выбирать строчку, покрывающую наибольшее число среди них. Мы берем наибольшее,
еще не покрытое количество столбцов, строчку, покрывающую их и так дальше. А как сюда добавить
веса? Теперь вот у нас же задача взвешенная, в общем случае. Значит, сначала давайте я сформулирую
вот ваше предложение по жадному алгоритму. Значит, мы берем покрытие, сначала пустое множество,
а потом, пока s это не покрытие, мы берем строчку, знаете, argmax обозначение. Это то,
на чем достигается максимум. Максимум по i от единички до m. Число столбцов,
еще не покрытых множеством s, но покрываемых этой строкой. Число еще не покрытых столбцов,
покрываемых этой рокой. Ух, ну я тут и написал, конечно. Здесь такой номер строки,
которая покрывает максимальное число еще не покрытых столбцов, еще не покрытых множеством
столбцов. Ну, то есть, можете это вообще не писать формально, мне важно, чтобы вы
исключительно понимали содержательно. Ну, жадная евристика, она и в Африке жадная евристика.
Хотя интересно, в какой стране и на каком континенте живут самые жадные евристики,
может не в Африке. Если у нас теперь веса появляются, как же сюда примешать веса? Вот,
допустим, у нас есть строка покрывающая, ну скажем, четыре столбца. Вот, но она веса там
семь, а есть строка покрывающая три столбца, и она веса, ну скажем, пять. Вот какую из этих строк
вы бы взяли? Три пять, почему? Стоимость, да, можно рассмотреть стоимость покрытия одного столбца,
да, у какой строки она выше? Пять третих versus семь четвертых. Семь четвертых выше, чем пять
третих, да, то есть, вот эта строка в расчете на один покрываемый столбец, она чуть подешевле. За
один столбец мы платим меньше. Вот, если все веса были бы одинаковые, то тогда понятно,
что выбирать строку самую дешевую, в смысле, стоимость покрытия одного столбца, это то же
самое, что выбирать строку просто покрывающую, максимальное количество столбцов. Ну, как только
у нас появляется еще вот это вот число стоимости строки, здесь мы можем с вами теперь поставить
другую немножко константу, да, что мы максимизируем. Мы максимизируем, да, можно считать, что мы
максимизируем. Давайте я минимизацию теперь перепишу, минимальную стоимость покрытия одного
столбца. Минимум вес строки поделить на число столбцов. Я уже не буду здесь писать покрываемых
этой строку, еще не покрытых раньше. Вот, мы понимаем с вами, что здесь стоит. И еще раз, да,
если бы все W были одинаковые, равные единичке, например, то тогда минимизировать вот такую
величину, это было бы то же самое, что максимизировать просто тупо число столбцов новых покрываемых.
Теорема такая, что вес жадного покрытия
не превосходит веса оптимального покрытия, помножить на единица плюс лоноритм числа столбцов.
Это получается даже как-то поточнее, чем использовать вероятность на округление. Здесь все-таки O
большое, это еще и константа в O большом, какая будет, да, непонятно. А здесь у нас просто единичка.
Множитель при логарифме, ну еще вот плюс вот эта единица, но это ерунда, конечно.
Видите, как получается, совершенно две разные ивристики, одна на основе линейного
программирования, другая жадная, и вот в обеих в итоге вылезает вот этот множитель,
неконстантный, логариф Маттен. Ну, что же делать? Значит, видимо, это какая-то природозадача.
Может быть, лучше и не получится приближать.
Давайте доказывать начнем, что мы вряд ли успеем сегодня целиком доказать,
но мы хотя бы начнем доказывать, что действительно у нас такая оценка имеется.
И мы с вами введем такие веса на столбцах. Ну, не веса, давайте я их буду называть пометки
на столбцах. Рассмотрим пометки на столбцах. Значит, пусть алгоритм работает на очередном шаге,
у нас выбирается, значит, некая строка, да, и эта строка покрывает несколько новых столбцов,
и вот у нас у этой строки вес Wt. Пусть эти столбцы еще не были покрыты раньше. Давайте
возьмем и над каждым столбцом напишем такую пометку. Wt поделить на три, то есть вот ровно
это число, которое минимизируется при выборе строки, мы напишем на столбце. Wt на три, Wt на три.
Пометка на столбце, пометка на столбце. Давайте мы эти пометки Zg обозначим. Это как раз и есть
Wt поделить на число покрытых столбцов, и эта пометка определяется в момент покрытия столбца,
определяется в момент покрытия столбца жадным алгоритмом, жадным алгоритмом. Вопрос,
к чему будет равна сумма всех этих Z житых? Ну да, понятно, что сумма Z житых,
это сумма позже от 1 до n, Z житых. Это Wt, но сумма вот этих вот конкретно Z житых,
это Wt, то есть вес той строки, которая была выбрана на текущем шаге алгоритма. А если
я теперь просуммирую Zg вообще по всем столбцам матрицы? Да, это и есть вес жадного покрытия.
То есть нам с вами нужно будет просто немножко по-другому поработать с вот этой суммой и
по-другому ее оценить. И мы это сейчас с вами проделаем потихоньку.
Давайте рассмотрим произвольную строчку матрицы.
Осмотрим какую строчку матрицы.
Сумма от 1 до n. Сумма по всем столбцам Z житых. Каждый столбец рано или поздно покроется жадным
алгоритмом. Он же работает до победы. Рассмотрим строчку матрицы и рассмотрим столбцы,
которые покрываются этой строчкой. Пусть s маленькая l и так далее, s маленькая 1,
это столбцы, покрываемые этой строкой. Рассмотрим произвольную строчку матрицы.
Пусть s l и так далее, s 1, это столбцы, которые ей покрываются. Я их довольно странно
занумировал. Зачем-то от конца к началу. Пока так, потом поймем, почему так удобнее нумировать.
Давайте считать, что эти столбцы занумированы в том порядке, в котором они покрываются жадным
алгоритмом. В порядке покрываем жадным алгоритмом. Что это означает? Смотрите,
что я хочу здесь сказать. Вот мы с вами смотрим на строчку, выделяем столбцы,
которые ей покрываются, там где здесь единички. Вот у нас, например, четыре столбца. Жадный
алгоритм, не факт, что он выбирает именно эту строчку. Я вообще сейчас не говорю про строчку
выбираемую непременно жадным алгоритмом. Я просто говорю про произвольную строчку матрицы.
И вот эти столбцы, это не те столбцы, которые там покрыты жадным алгоритмом, когда он выберет
эту строчку. Да нет, это просто все столбцы, на пересечении которых с этой строкой,
стоят единички в матрице. Но все эти столбцы, рано или поздно, покроются же жадным алгоритмом,
правда, потому что он вообще все столбцы матрицы покрывает к окончанию работы. И вот какие-то из
этих столбцов могут покрываться вместе. Например, если бы жадный алгоритм выбрал именно эту строку,
то столбцы все вместе сразу были бы покрыты, все эти за один раз. Но может быть жадный алгоритм
выбирает из своих соображений какие-то другие строки. И, например, сначала покрываются вот
эти вот два столбца. Тогда они у меня окажутся как раз S4 и S3 в любом порядке. А потом, например,
жадный алгоритм выбирает еще какую-то строчку, которая покрывает вот эти два столбца. Ну и вместе
с ними там может еще какие-то. Но тогда у меня дальше будут стоять вот эти вот S1 и S2. То есть
вот это S1 и S2, это S3 и S4. Вот в этой нумерации. Я хочу сказать, что эти столбцы могут покрываться
группами. Не проблема. Жадным алгоритмом они могут покрываться группами. Главное вот в этой
нумерации, что если жадно алгоритм покрыл какой-то столбец позже, строго позже, то он и в этом списке,
я предполагаю, что он стоит строго позже. Ну просто я их так присвоил им номерами. Это же не важно,
в каком порядке я эти числа записал. Вот запишу в таком порядке, который мне удобен. В порядке
покрытие жадным алгоритмом. Давайте посмотрим пристально на шаг жадного алгоритма, на котором он
покрывает ну какой-то кат из столбец. Шаг жадного алгоритма, на котором он покрывает S катом. Вот
кат из столбец этого списка. К началу этого шага этот столбец не покрыт. На вот этом шаге
столбец из ката покрыт. К началу этого шага этот столбец еще не покрыт. Какие столбцы еще
гарантированно не покрыты. Ну с номерами, ну вот в этом списке, да, идущие правее. То есть
S кат, S к-1, S к-2 и так далее, вплоть до S1. К началу этого шага, к началу шага не покрыты,
не покрыты как минимум столбцы S к, S к-1 и так далее S1. О, смотрите, значит к началу этого шага у
нас есть не покрытые как минимум к столбцов в матрице. Если алгоритм выберет вот эту строчку
текущую, давайте мы ее как-нибудь обозначим. R, например, от слова row строка. Если бы алгоритм
выбрал сейчас вот текущую строчку, то он какую стоимость заплатил бы за покрытие вот всех этих
столбцов? Мы просто по определению смотрим сюда. Если бы алгоритм выбрал текущую строчку, если бы
она вдруг минимизировала вот эту вот величину, то алгоритм заплатил бы какую стоимость. Можем
мы как-то оценить? Стоимость строки WR. Количество новых столбцов, которые она бы покрыла, как
минимум к. Значит, стоимость покрытия вот этих вот столбцов у нас была бы WR поделить на к как
максимум. Значит, если бы жадный алгоритм выбрал строку R, да, просто какая строка матрицы,
произвольную. Не обязательно та, которая действительно выбрана жадным алгоритмом,
просто любая из строчек матрицы. Вот, но мы сейчас предполагаем, что давайте еще раз, да,
проговорим этот момент. Рассматриваем произвольную строку матрицы без относительно всякого жадного
алгоритма. А вот столбцы, которые, все столбцы, которые эта строка способна покрыть, мы упорядочим в том
порядке, в котором они покрываются жадным алгоритмом. Поскольку жадный алгоритм все столбцы
матрицы рано или поздно покрывает, то, значит, мы способны упорядочить как-то разумно вот этот
список. Занумируем их просто от L к единице, а не от единицы к L, так нам удобно. Вот, значит,
рассмотрим шаг, на котором покрывается столбец S катой. S катой правее него стоят к столбцов,
включая него самого, да, и все эти столбцы по определению порядка в этом списке они не покрыты
еще к текущему шагу. Но на текущем шаге у них есть шанс быть покрытыми, если жадный алгоритм
выбрал бы вот конкретно эту строку, он тогда бы все оставшиеся столбцы покрыл вот в этом списке.
И стоимость, которую жадный алгоритм бы заплатил, не превосходит WR поделить на K, количество новых
столбцов, покрываемых алгоритмом в знаменателе, а в числителе W этой строки. Да просто так обозначили,
просто какой-то номер K рассмотрели столбца, который покрывается вот на каком-то текущем шаге.
Не факт, что этой строкой, вот в чем дело. Сейчас последний я здесь допишу, вот этот вот кусочек.
Значит, если бы жадный алгоритм выбрал строку R, то заплатил бы не больше WR поделить на K за вот
этот столбец. И за все вот эти столбцы. Но поскольку жадный алгоритм вообще выбирает
минимальную строчку, то мы можем сделать вывод, что вес, который, ну не вес, лейбл,
пометка, которая будет приписана вот этому столбцу SK, она не превосходит точно вот такой вот величины.
Мы не знаем, как бы был ли покрыт столбец SK этой строкой или не этой строкой. Но мы точно знаем,
что если бы он был покрыт этой строкой сейчас, то у него Z была бы вот такой, да, не больше. Но
поскольку жадный алгоритм как можно меньше вообще старается сделать этот Z, то мы получаем такое
неравенство, уж во всяком случае. Есть возможность добиться вот такого результата, но мы ищем еще
более хорошего результата, может, если он есть. Поэтому в итоге Z с индексом SK получается не
больше, чем WR поделить на K. Вот и все на сегодня. И мы с вами уходим на следующий раз в максимальной
непонятности, что произойдет дальше, как мы это вообще используем и как мы сможем оценить все
эти Z-ки и откуда возникает алгоритм. Непонятно. Но мы это узнаем в следующий раз.

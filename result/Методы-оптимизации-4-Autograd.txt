В общем, я только что проговорил, чем символенно отличается от автоматического, что в символенном получается
вот пагеформула для градиента, в автоматическом мы получаем значение в точке.
Это как раз к снижению предыдущих трех минут.
Немножко начали говорить про то, что такое вычислительный граф.
Сейчас, когда я перейду на доску, я пытаюсь его нарисовать и показать, как оно все прокидывается.
Тут гибридический пример, что у нас как бы три функции, и у нас функция квадраты в кидовой норме разности
представляется в виде вот такой вот суперпозиции.
Ну и соответственно, вычисление градиента по х, по а или по b,
оно просто приводится к тому, что надо часто производно прокинуть по правилу сложной функции.
В общем, в силу того, как устроены суперпозиции,
рассматриваются только направленные циклические графы, сокращенные даги.
Поэтому, в общем, если увидите такую аббревиатуру, не пугайтесь, это всего лишь дирекетно циклик граф.
В контексте именно вычислительных графов и автограда это очень часто используемое название.
Ну, собственно, я не помню, по-моему, про это тоже что-то было уже в прошлый раз.
Поэтому, если не было или что-то непонято, вы меня тормозите и ставьте минусы в чат или еще что-то там.
Пишите, я буду восстанавливаться, подробнее объясню.
То есть у нас есть такая вот функция в случае, когда мы смотрим на скалярных суппозициях.
То здесь получается довольно все просто, и размерность градиента должна совпадать с размерностью х, чтобы все заработало.
Ну и по определению это тоже явно следует.
Ну и, соответственно, вот тут dg по du, соответственно, просто будет число, потому что уже скалярная функция.
Если же у нас векторные случаи, то мы тут обязаны посчитать сумму по всем промежуточным аргументам.
И можно заметить, что вот эта вот величина, которая здесь фигурирует, видите, dhj по dxk, это некоторая матрица, матрица якобы, видимо, отображение.
Поэтому это можно делать таким вот образом.
Ну и если вспомнить аккуратно, что если у нас есть df по dxk, то k должен быть внешним индексом относительно тех индексов, которые сворачиваются в знаки суммирования.
Поэтому здесь появляется транспонирование, и мы транспонированные матрицей якобы умножаем...
Ну так якобы написано, в общем, в матрице якобы, понятно. Умножается на градиент dg по dh, поскольку уже по-прежнему скалярная функция.
То есть разница в том, какое отображение одномерное или двумерное стоит внутри.
Так, есть ли вопросы какие-то по этому слайду?
Так, ну вроде никто ничего не пишет. Наверное, это значит, что вопросов нет.
Окей. Ну собственно, дальше это все просто раскручивается по, как сказать, по увеличению сложности, но идеи на все остается тем же самым.
То есть у нас есть теперь, допустим, мы из k, откуда сейчас, из rn отображаемся в rk, а из rk в rm.
Вот тогда у нас эта штука просто произведение соответствующих матриц якобы.
Ну и соответственно, если это все будет суперпозиции из l большой функций, то у нас получается...
Итог грамматается якобы, это просто произведение соответствующих матриц якобы у ингредиентов.
Ну и когда вот эта штука вычисляется, она может быть тает, точно уже был в прошлый раз, вычисляется слева направо, справа налево.
И у каждого из этих способов есть свои достоинства и недостатки.
Наставшиеся, надеюсь, минут пять-десять, мы светим тому, чтобы понять, в чем у него достоинства, в чем у него недостаток или куда же надо использовать.
Ну соответственно, Format Mode тоже начала рассказывать, что мы фиксируем аргумент, индекс элемента аргумента,
и для всех выходных элементов выхода считаем соответствующую производную.
То есть это типа жидкость столбей от смартфона.
Ну и делать это довольно прямолинейно. Выбирается элемент, сдается орд и умножается это все дело рекурсивно на этот орд.
То есть сначала умножается g1, потом g2 и так далее до g3.
Все это, поскольку идет по возрастанию индекса, делается одновременно с вычислением самой функции.
Ну и чтобы это все заработало, надо как бы переопределить все базовые функции, которые мы знаем таким образом, чтобы они поддерживали не только вычисление значений, но и умножение икобиана налево.
То есть тут все довольно сочетарение реализации прямолинейно. В конце будут ссылочки на то, где можно посмотреть, что это на самом деле сделано.
Backward Mode, то есть проход назад или Backpropagation, заключает в том, что мы наоборот сначала фиксируем некоторые индексы в результате.
Ну и потом для всех параметров, для всех элементов параметров, которые у нас были, мы рассчитываем соответственно строк.
Не путайте, сейчас будет немного путанный момент про то, что мы везде в наших теоретических выкладках будем считать, что у нас градиент это вектор-столбец.
Но в сфере нам это нужно будет реализовывать. Нет, вам это не нужно будет реализовывать, вам нужно всего лишь понять, как это и почему это работает.
То есть чуть позже будут ссылки на различные пакеты, где это сделано.
И когда я про один из них буду говорить, я просто открою, может быть, код и прямо покажу, как это переопределение происходит.
Но это просто к тому, что если вдруг вы захотите по каким-то причинам реализовать метод или функцию, которая будет поддерживаться вот таким вот автоматическим дифференцированием,
чтобы вы понимали, что именно вам нужно предоставить помимо вычислений самой функции.
То есть понятно, что довольно широкий класс преобразований может быть подвергнут такой процедуре, но не все они достаточно популярны, чтобы присутствовать в стандартных пакетах.
Вот к чему я это все сейчас рассказываю.
Ответил ли я на ваш вопрос?
Ага, окей, спасибо.
Ну вот, соответственно, когда мы идем назад, то мы умножаем не экипиан на вектор, а транспонированный экипиан на вектор, чтобы эмулировать умножение строки на экипиан слева.
Ну и тут как бы соответственно возникают прокладные расходы некоторые, потому что часто у нас вот это умножение на экипиан связано с тем, чтобы сохранить промежуточный результат.
Поэтому мы два раза будем граф, и на первом проходе что-то может быть сейчас будем сохранять. Это нам требует больше памяти.
Ну в общем, если у нас единичный размер в конце получается скаля, то мы получим по сути дела градиент, который соответствует единственной строке нашей матрицы эка.
То есть тут как бы понятно ли в чем разница подходов. Если вас просят, чем там backward mod отличается от forward mod, подумайте, можете ли вы осознали ли вы, в чем они отличаются, и можете ли вы ответить на такой вопрос, например.
Скорее всего, зачем backward mod, если он кажется просто сложнее вычислительным.
Нет, погодите. Вот насчет того, что он сложнее вычислительный, сейчас мы посмотрим.
А не симпатически, а константно. Что приходится два прохода делать на каждую строку.
Ну хорошо, да.
2n проходов вместо n.
Да, смотрите, давайте рассмотрим случай, как вот этот, например.
Сколько надо сделать проходов forward mod, чтобы посчитать градиент в этом случае, вот в этом?
А n получается?
Именно так.
А тут 1.
Именно так. И мы сейчас будем, то есть, во-первых, я покажу там некоторые теоретические оценки буквально в следующем слайде.
А потом переключимся на ноутбук и я покажу в коде, как реальный замер времени показывает, что насколько сильно будет отличаться время работы этих самых режимов в зависимости от размерности кода и выхода.
Это как бы план на следующие 5 минут, я надеюсь.
Собственно, вычислительная сложность. Это все взято из книжки.
Ссылка была вот здесь. Довольно большая книжка. Если кому интересно, рекомендую ознакомиться.
Там отчасти эволюция прослеживается и более широко это все обсуждается.
На самом деле, очень большая область, которая в 50-е и 60-е годах тоже активно развивалась в связи с символной историей, но потом эволюционировала как будто бы.
Поэтому, в общем, там много всего.
Собственно, сложность. Забавный факт, что сложность вычисления функции и произведения Кабиана на вектор не так уж сильно отличается от сложности вычисления самой функции.
То есть такие вот, на первый раз, кажущиеся странные результаты.
Но это просто к тому, что есть дополнительное дооснащение этой функции умножения Кабиана на вектор.
Это не то, что вы существенно на порядок усложняете в другом числении.
Собственно, память. ForwardMode ничего не требует.
BackwardMode требует промежуточных значений сохранения, чтобы потом в них можно было пересчитать значения промежуточных функций.
Ну, собственно, вывод довольно прямолинейный, что если у вас из малого количества входа получается большая размерность.
То есть, наоборот, если из большой размерности на ходе получается маленькая размерность на выходе, то вы можете использовать BackwardMode, в противном случае использовать ForwardMode.
Тут написано больше либо равно, но мы сейчас увидим, что на самом деле вот это вот больше либо равно, возможно, стоит заменить именно нам сильно больше.
Также вот тут важный комментарий.
Может быть, его как-то в рамочку я выделю, чтобы вы его не пропустили, если будете пересматривать слайды.
То тут важно, что вот эта вся реализация этих вещей непосредственно на входе, она очень может отличаться в разных пакетах.
Потому что, в общем, промежуточные оптимизации здесь можно в разных местах вкручивать и в той или иной степени пытаться сократить какие-то либо учтения, либо память.
В общем, тут довольно большой простор для фантазии и для каких-то тестов, экспериментов и всего такого.
Понятны ли ясные выводы про сложность и память этих двух режимовых вычислений?
Оставьте там плюс или минус, пожалуйста, в чате.
Так, все, спасибо, здорово.
Ну, собственно, где реализованная?
Реализованная в джаксе, все ссылки кликабельные, можно будет посмотреть, торчерия сделана и в автограде.
Вот автоград, наверное, надо будет его сейчас поподробнее показать, потому что он, по сути, взят нам пайс стандартный и немножко доопределен.
То есть тут минимальное количество каких-то наворотов.
Все это на чистом бетоне, джит компиляция поддерживается через обертки.
Но сама по себе конструкция вполне себе несложно осознать, просто глядя на код.
В дальнейшем примеры, которые я покажу через пару минут, будут именно на джаксе,
по причинам которой я объясню, когда буду показывать ноутбук с кодом.
Везде реализованы почти одинаковые функционалы.
Почти одинаковые, где будет отличаться, я скажу, когда будем продолжать.
Потому что, кажется, сейчас здесь реализована максимально широкий функционал с максимально удобным использованием, в отличие от торча, например.
Это пока все по слайдам. Давайте сейчас переключимся на код и на те примеры, которые я хотел показать.
Буквально пару секунд, мне надо все открыть.
Важный небольшой комментарий сейчас будет.
Наверное, это уже можно шатить.
Вот репозиторий Автограда.
Авторы пишут, что они пока все это дело поддерживают, но все полным составом переключились на разработку джакса,
поэтому дальнейшее развитие этой штуки не предусматривается пока что.
Как это работает? Надо импортировать переопределенный нампай и функцию, которую гриден вычисляет.
Дальше вы пишете какую-то функцию, которая использует переопределенный нампай.
Пишете град, и у вас получается функция, которую гриден в точке вычисляет.
Там сведенички, все хорошо.
Понятно, что можно тут для скалярной функции посчитать производные нужного вам порядка без особых проблем.
Тут элемент у Айсград, только видите.
Вот такие графики можно получить.
Еще я хотел показать, что у этой же команды есть дидактика.
Это штука, которая показывает сейчас гиперград или не у них.
Ладно, исхода не находится, тогда давайте просто посмотрим на примеры.
Автоград нампай, значит нам нужно пойти вот сюда и посмотреть, как нампай сделан.
И вот видите тут то, что я в прошлый раз упоминал про VGP и GVP.
То есть сектор JacobinProduct для backward и JacobinVectorProduct для forward.
Тут если смотреть, то огромные файлы, наверное, будут.
Но не очень огромные на самом деле.
И здесь берутся нампайские функции и добавляется метод devVGP, который задает то, как нужно умножать градиент на входной вектор G.
При этом ans – это результат вычисления при проходе вперед.
То есть видите, по сути дела, все методы, которые там есть, экспоненты, логарифмы, все это.
Народ пошел подключаться, интересно.
Прям надо это все определять таким вот образом.
И причем, если у вас функция несколько переменных принимает, по которым вы хотите посчитать градиент, то надо...
Так, пишут, что интернет unstable, поэтому если вся связь плохая, пожалуйста, напишите в чат.
Поэтому, в общем, видите, какая работа, но какую работа надо проделать, чтобы все это в конце концов заработало.
Вот понимаешь, что тут всякие там браткасты делаются, то есть вся механика, которая поддерживается нампаем, она тут реализована с дополнительным оснащением градиента.
И функция умножения, по сути, екабианная, наверное.
Ну давайте какой-нибудь простой вопрос.
Вот что из себя будет представлять...
Например, у вас есть функция cos, которая действует на вектор поэлементно.
И вам нужно определить функцию, которая будет вычислять произведение екабианной функции на вектор.
Какая будет сложность этой функции? Как это все будет работать?
Такой простой вопрос, пока вам тут грузится линалк.
Хочу сказать, что ты линалк гиперкоприализован.
Понятен ли вопрос?
Так, прошу прощения. Проблема со связью, но все в порядке вроде как.
Кто-нибудь придумал ответ на вопрос,
воспользовавшись моим отсутствием?
Можете еще раз вопрос задать?
Да, давайте.
У вас есть функция cos,
которая действует на вектор поэлементно.
Пришел вектор из рандомных чисел,
и функция возвращается в вектор из cos этих чисел.
Как будет выглядеть функция,
которая умножает екабиан этой функции на вектор,
и какая будет у нее сложность?
А данная функция, для которой нужна екабиан?
cos, да.
И вроде мне даже подгрузился нам пай.
В смысле под модуль линалк.
Поэтому сейчас можно будет, когда вы что-нибудь придумаете,
обсудим, что здесь происходит,
и почему это имеет какой-то смысл.
Да.
Так, ну что?
Какие варианты?
Достаточно простой вопрос, кажется.
То есть давайте...
Не, на плане градиа cos это его производное?
Именно так.
Его производное это минус sin?
Ну, вектор одноиметный из его производной.
Ну вот что это за...
То есть смотрите, функция, во-первых,
как я уже сказал, раз она поэлементная,
она улучшает вектор длины размерности n,
вектор размерности n.
А, поэлементный cos, понятно.
Именно так.
К чему будет...
Минус поэлементный sin умножить на вектор.
Хорошо.
Это правда.
А теперь давайте поясним, как это согласуется с тем,
что у нас должна быть матрица якобы какая-то.
Размерности n на r.
Вот какая она будет?
Диагонально, кажется.
Диагонально, гениально.
Да, именно так.
Она будет диагональная.
Вы эту диагональную матрицу умножайте на вектор.
Умножение диагональной матрицы на вектор
это умножение по элементу на диагонали на сам вектор.
Поэтому это будет всего лишь за m,
что достаточно дешево.
Поэтому это касается, в общем-то,
всех поэлементных функций, которые у вас там могут возникнуть.
Вот.
Поэтому как бы попробуй.
Понятно ли почему так?
Почему так?
Я вижу, что есть запрос на расписание.
Давайте потратим немножко времени.
Это не так долго.
Но хочется, чтобы не было каких-то
сомнений о векторе.
Вот смотрите.
Вот у нас есть функция f от x.
Все вроде отображается.
Так только надо делать вертикально.
То есть горизонтально, чтобы оно еще и полный экран занимало.
Вот.
Вот у нас есть функция f от x.
Ну, понятно, cos-x.
Которая делает следующее.
x1, x2 и т.д.
xn
отображает это все
в cos-x1,
cos-xn.
Вот такая функция.
Соответственно, что такое матрица якоби?
Это d, f,
i, g
d, x, g.
И тут мы видим внезапно,
что у нас каждый аргумент,
каждый элемент выхода зависит
только от соответствующего аргумента входа.
То есть нет, условно говоря, зависимости
первого элемента выхода
от последнего элемента входа.
Следовательно, у нас наша матрица будет
диагональная.
И здесь будет стоять, соответственно,
минус sin x1
и т.д.
минус sin xn.
Понятно ли, почему
будет диагональная матрица?
Уставьте плюс, если понятно,
и минус, если нет.
Окей, так вроде
довольно активно
ставите, ставят люди плюс.
Вот, ну и соответственно,
когда дальше нам нужно умножить тебя
на какой-то вектор u,
то это то же самое, что и взять и умножить
минус sin x1 умножить на u1
и т.д. минус sin
xn умножить на un.
Такой вектор получится.
И это всё от m.
И это как бы схема, она работает
для любой поэлементной функции,
чтобы у векторов что-то другое будет преобразовывать.
Просто поэлементные функции
это довольно популярная история
и полезно понимать, как они работают.
Так, есть ли какие-то вопросы
про этот пример?
Всё понятно, это прекрасно.
Я думаю, если вопросы появятся,
то, пожалуйста, пишите в чат, я вернусь к доске.
А пока давайте посмотрим
всё-таки на то, что происходит
в пакете lenal.
Ну, вот
как-то
под модуле
lenal, потому что, возможно, вы здесь
интересно найдёте.
Например, есть функция dead,
которая вычисляет определить.
И есть соответствующий ей градиент,
который
умножает
входную
матрицу
на то,
что
представляет из себя
градиент
у до терминанта.
То есть можно понять, например,
почему уровень градиента терминанта,
глядя на вот эту штуку.
Видите, как тут хитро сделано,
лямбда, то есть лямбда
это точка, это ответ,
g это входной вектор,
и вот они тут вот так вот сделаны.
Есть типа логарь из до терминанта,
с которым мы сегодня ещё встретимся,
s означает sign.
Он проверяет знак,
и только потом вычисляет градиент.
Inf это обратный матриц вычисления.
Что ещё интересного можно показать?
Можно градиент
описан у решения
линейной системы по, я так понимаю,
по
пиксу, видимо.
Sol
или по матрице даже.
Да, значит, ещё раз
давайте вернёмся.
Идея была показать,
что
эти штуки,
которые называются gvp и vgp,
соответственно
означают то,
как
наши матрицы якобы
действуют на соответствующие вектора.
Тут даже, вот видите, есть
функции, которые от
function, вы видите,
eigenvalues, eigenvectors,
на первый взгляд кажется довольно
странной конструкцией,
у вас функция вычисляет собственные векторы,
а вы от неё градиент берёте.
Тут это всё проделано,
и не только здесь,
но и в других пакетах тоже.
Пожалуйста, если вдруг у вас такая
необходимость, используйте максимум весь функционал.
Понятно ли,
как это в принципе устроено?
Я пока не спрашиваю, понятно ли, как это работает
в деталях, но надеюсь, что хотя бы
основная
вы её освоили,
и понятно, как это будет работать.
Можно ли переходить к примерам,
или есть какие-то вопросы?
Спасибо, вижу,
люди тут, это приятно.
Теперь
небольшой
туториал про джакс,
довольно кратко,
по сравнению с тем, сколько там всего.
Основные
особенности
этого пакета,
что это действительно снова обёртым дампаем,
она позволяет автоматически векторизовать
все вычисления, то есть вы там можете циклы
компилировать,
также позволяет
короче говоря,
кто-нибудь знает, что такое
SPMD? Оставьте плюсы,
если знаете, имелось, если нет.
В общем, эта штука
расшифровывается, кто-то знает, классно,
как single-prose
data.
То есть вы можете
параллелить
по процессам достаточно легко,
там называется функция PMAP,
которая условно в современных реалиях
наиболее актуальной историей,
это когда у вас там много GPU,
и вы пишете какой-то код, а потом пишете PMAP,
и он автоматически раскидывается на GPU.
В общем, какая-то такая история.
Понятно, что основная проблема
здесь в наличии железа, чтобы это все
потестить.
В общем, это как бы немного
выходит за рамки
того, о чем мы будем дальше говорить.
Ну, вот, собственно, дифференцирование поддерживается,
вот XLA, это, собственно, бэккант,
на котором Санзерфлоу изначально был написан,
а потом от него отказались,
пересказали на джаз, практически.
Ну, джаз, это инкапиляция,
все как надо.
Питоновский код не страдает
от медленности питона
и запускается со скоростью
пищи штабинарного.
Как уже было показано,
в члене градиентов и все прочее,
это очень простая штука.
Берете, называете, град питонской функции,
получаете то, что у нас.
Важно оборачивать в джид
как саму функцию, так градиент от нее,
и они все как, типа, по цепочке
будут друг другу применяться
и зарабатывать.
Так, проверка, что все работает,
и он сработал, да, ура.
Ну, короче, история та же самая.
import.jax.numpy
Вот. Важный момент,
что по дефолту все делается
в одинарной точности.
То есть у вас там, типа,
5-6 значащих цифр
после запятой.
Вот. Поэтому, чтобы
более точно
и получать подобные
соответственность теории
значения, надо переключиться
в Apollo64. Вот такая
волшебная строчка, видно же,
которая это делает.
И, соответственно,
простая штука, как, простой пример,
как это все дело применяется.
То есть пишете функцию,
делаете операции, возвращаете скаля,
посчитаете градиент от нее,
указываете, по какому из аргументов этой
функции вы хотите посчитать аргумент,
хотите посчитать градиент.
Вот. А дальше указываете,
возвращает ли эта функция
какой-то еще
величины,
объекты, которые
надо сохранить для какого-то
дальнейшего возможности использования.
Поэтому здесь написано false, мы возвращаем
только число.
Понятен ли синтаксис?
Знакомы ли вы
с декораторами?
Или надо что-то еще пояснить?
Вот. Раз, два, три, четыре, пять
строчек. Можно
указать номер строчки и сказать, что
непонятно. Спроси точнее.
Если все понятно, плюсик,
пожалуйста, поставьте, чтобы я понимал, что
можно двигаться дальше. Первая строчка.
А, смотрите, это
называется декоратор.
Вот. И эта штука применяет
функцию к функции, и
что-то с ней делает.
В частности, здесь
это декоратор, который
компилирует эту функцию в
словном бинарне, который потом вызывается.
Вот. То есть ключевое слово
декоратор, и вы найдете
очень много примеров
в условном
туториалов про то, что это такое.
Идея в общем в том,
чтобы преобразовать саму функцию
во что-то.
И потом я снова верну.
Вижу.
Так. Ну окей. Вроде
подавляющее большинство
говорит, что все нормально.
Гуд.
Так. Не знаю. Отдельная история
про то, что случайные числа в джаксе
немножко устроены похитрее, чем
на нампайе,
потому что они порсят
свою воспроизводимость.
Вот. Поэтому там
сейчас вы увидите, как они делаются
в специальном образе, то есть генераторы задания СИДа.
То есть надо вот сначала ключ
задать, потом уже размерности.
В общем, случайные величины зададим.
Икс, соответственно, вектор из N
элементов, A матрица на N
и B вектор из N элемента тоже.
Вот.
Ну и дальше давайте посмотреть,
что происходит.
Ну, собственно, проверка корректности того,
что градиент посчитан верно.
Мы знаем правильный ответ. Вот он.
Вот. И дальше
сравниваем по норме
автоматически посчитанный градиент
в этих же точках по иксу
с градиентом посчитан аналитически.
Вот.
Ну, собственно, 10-11 это, понятно,
почти ноль, хорошая точность.
Вот. Дальше сравнивается скорость выполнения.
Ну вот, аналитически посчитать
стоит 1.76
микро...
Нет, не микро, миллисекунд.
Просто функция, которая участвует в градиент
делает почти одну микро...
Ой, ну, одну миллисекунду, да.
Вот. Если вы ее разоджеттуете,
то она в три раза ускоряется.
Вот. То есть вот такая вот
хитрая штуковина.
Вот. Вот эта штука, блок
until ready, делается для того, чтобы избежать...
Поскольку таймы-то много раз
сочетают функцию, вот, чтобы избежать
проблем с тем, как это может
параллельно исполняться, в общем,
рекомендуется авторами писать вот так,
для более честного сравнения по времени.
Вот. То есть мораль...
Во-первых, мы считали все правильно.
Во-вторых, джит дает нам еще и более
быстрые вычисления.
Ну, собственно, тут пример того, как
можно писать джит без
явного декоратора.
Так. Есть ли какие-то вопросы, если все понятно,
плюс поставьте, пожалуйста.
Good.
Так. Нам что-то немножко ускорится.
Ну, тут про гессиан то же самое.
Я думаю, вы там, если кому интересно, посмотрите
какие здесь методы. То есть гессиан
тоже можно автоматически посчитать,
ничего не страдает.
В общем, сравнение forward-мода и backward-мода
на этой штуке. То есть у нас
одномерное. Мы из n
элемента получаем 1.
Вот. И тут можно, типа, взять
jack forward и jack
rev, типа reverse,
вот. То есть это как бы grad.
Это некоторая тоже обертка
под, по-моему, rev.
Суть я помню. Вот.
Потому что, ну, там понятно, что это будет быстрее.
Вот. Ну, можно как бы
полезть внутрь и вызвать как бы исходную функцию,
чтобы сравнить просто скорость выполнения.
Ну и вот видно, что forward
включается в 13 микросекунд, а backward
типа 300
миллисекунд. Вот.
Я надеюсь, разница
достаточно очевидна. В скорости,
которая, собственно, из теории напрямую следует.
Понятно ли, что
изображено на экране?
Плюс, пожалуйста, поставьте, если понятно.
И напишите минус или вопрос,
если надо выяснить какую-то строчку.
Так. Ну, вроде пока нормально.
Вот. Так. Ну и, собственно,
наоборот.
Взята функция, типа,
что это? softmax, да?
Вот. Которая посчитана
устойчивым образом. Обратите внимание, что это
не просто экспонента
от элемента вектора делить на
сумму экспонента от всех элементов вектора.
Сумма экспонента,
ну, сумма всех экспонентов от всех элементов вектора.
Короче говоря,
подумайте, в общем,
простое упражнение, почему
если считать это в лоб, то...
Ну да, короче, давайте я просто пишу,
что типа эта штука делает вектор
y и y,
это будет проще, действительно,
который равен, типа, е в степени
е в степени x, и делить на сумму по и от е в степени x, и так тут отвратительное.
я знаю, потому что сейчас я отправлю просто в чат сообщений, и все будет просто.
ну в общем, вот такая вот история.
так, сейчас. это нельзя редактировать, их печаль.
а нет, нельзя, да, все равно.
ну короче, я надеюсь, что вы понимаете, что каты index относятся к x,
а не к exponенте. в общем, да.
вот эта функция вычисляется таким вот образом устойчивой.
если вы попробуете посчитать в лоб, то у вас, скорее всего, в случае каких-то больших значений появятся наны.
хорошее упражнение, подумайте, что случилось, почему полезли наны.
ну и тут, соответственно, из-за размерности матрицы можно регулировать то, как будет соотноситься размерность входа и выхода.
и мы сейчас это посмотрим.
ну тут, собственно, используется и кабиан, потому что градиент нам тут уже не поможет.
ну типа одинаковые значения.
если попробовать запустить, собственно, jack, то все хорошо.
если пытаться запустить grad, то он скажет, что просите градиент только от скалерных функций, можно посчитать.
а у вас на выходе тысяча.
то есть вы как бы тут вам не дадут посчитать что-то, что заведомо неправомерно.
ну теперь, собственно, опять же forward и backward вполне себе здесь работают.
между ними равен 10 и 16.
все вроде как в порядке.
и гипотеза такая, что forward и backward должен стать быстрее backward.
потому что здесь размерности совпадают.
посмотрим сейчас, что получится на практике.
ну на практике получилось в общем-то соизмеримо.
то есть получается один раз, ну типа тысяча раз прохода вперед, тысяча раз прохода назад, примерно одинаково.
то есть мы как бы в квадратной матрице собирали либо по строкам, либо по столбцам.
теперь если выкрутить размерности и отображать 10 к тысячу, то здесь уже станет все более наглядно.
опять все хорошо по точности.
ну тут мы как бы сравниваемся с тобой.
на forward-моде получилось 62 микросекунды, на backward-моде 2 миллистекунды.
ну то есть 100 что ли раз.
тут медленнее.
это все эксперимент для того, чтобы просто подтвердить то, что мы уже видели из тюри.
есть ли какие-то вопросы по тому, что мы хотели получить, как мы это сделали и почему все это работает?
вижу 7 плюсов.
при этом 20 человек чуть больше.
все это мне всему этому внимают.
пожалуйста все остальные тоже как-то прореагируйте.
мне хочется, чтобы они отвалились по дороге.
окей, да, спасибо.
в общем, этот туториал лежит в репозитории.
тут еще есть про произведение гисяна на вектор, но я не буду сейчас про это вдаваться в подробности.
мы когда в методах до этого дойдем, то вы поймете, зачем тут этот кусочек нужен.
а пока давайте переключимся к нашей следующей теме про выпуклые функции.
осталось не так много времени, но я постараюсь максимально лаконично и по делу про все это рассказать.
поставьте, пожалуйста, плюс, если у вас уже была эта тема на сембарах.
у всех была?
отлично.
мы тут просто пояснение относительно того, почему.
потому что в доме на сембарах работает, скорее всего.
окей, гуд.
прекрасно.
а теперь надо как-то вытащить чат.
в окно со слайдами.
почти что получилось.
ура.
все, красота.
я надеюсь, поскольку у всех уже практически это было, все помнят, что у нас интересует
какая вот функция, которая наукополношительная, это важно.
полезно понимать, почему это важно.
подумайте на досуге, если не придумаете, напишите мне, я прокомментирую в телеграмме.
соответственно, выполнен такой нераз для любых элементов из области определения.
геометрически нельзя не сказать, что это значит геометрически.
это не полноценно получается.
хотя я думаю, что все это уже, надеюсь, вы лучше или не осознали.
что надо нажать? вот так надо нажать.
победа.
вот у вас есть какая-то функция. вы берете предвольные две точки.
x1, x2. проводите отрезок.
любая точка, которую вы возьмете вот здесь, если вы сравните вот это значение с этим значением,
то у вас получится, что крестик всегда не ниже, чем кружочек.
тут все максимально прямолинейно.
в плане понятности, я надеюсь, они в плане гривизны.
функция может быть достаточно нелинейными.
но при этом и собственно про это был некоторый эпиграф на первой лекции.
про то, что водораздел проходит не между линейностью и нелинейностью, а между выпуклостью и не выпуклостью.
вот так.
это была картинка.
теперь возвращаемся к слайдеру.
вернулись.
соответственно у нас будут вогнутые функции, такие, что минусы выпукла.
определение довольно прямолинейно.
пример выпуклой функции. я надеюсь, что хотя бы частично они были разобраны на семинаре.
пожалуйста, поставьте плюсик, если все они были разобраны на вашем семинаре.
и минус, если хотя бы одна была пропущена.
проверка на кто что помнит.
все разбирали. класс.
возможно это просто люди из одной группы, которая была на семинаре.
которые все это разобрали. интересно.
что-то как-то 1, 2, 3, 4 реакции.
и остальные как-то какой-то ступор.
без лог, да. понятно. хорошо.
это я ожидаемый ответ.
максимум у меня было. окей.
все это не смертельно.
наверное посчитали, что это слишком просто, чтобы разбирать.
окей. понятно.
примерно про все это мы в какой-то момент поговорим.
единственное, что может быть... увидим, как дело пойдет.
важная связь. спасибо.
важная связь про множество.
был максимум и собственную силу. да, прекрасно.
хороший пример про максимум.
у нас есть надграфик.
это то, что будет связывать с предыдущими лекциями про множество.
у нас есть надграфик. это такое множество, которое из xt.
т больше чем f of x.
если возвращаться к предыдущему примеру.
я надеюсь, что он сейчас подгрузится.
или он так не умеет? да, так умеет.
над графиком здесь будет вот эта штука.
все такие t и x.
что t лежит выше, чем x.
уже из этой картинки кажется, что плюс медс очевидно,
что выпуклась надграфик и выпуклась функция. это одно и то же.
действительно это так.
сейчас будет доказательство. я думаю, что сейчас его пропущу.
потому что потом пришлю под эвку.
с выкладками.
буквально три строчки.
ничего такого нет.
важно понимать, что это одно и то же.
это важно, потому что утверждение про выпуклась функции
можно будет переформулировать к выпуклости множеству.
помимо просто выпуклых функций у нас будут
сильные выпуклы.
это страшное определение.
потому что тут еще присутствует некоторый квадратичный член по x.
но к счастью, у этой штуки есть довольно прозрачная
геометрическая интерпретация,
которую мы разберем, когда дадем до коридории выпуклась.
несложно показать, что привели до следующих включений.
выпуклась, строгая выпуклась и сильная выпуклась
относятся таким вот образом.
рекомендую подумать...
привести пример строгой выпуклой,
но не сильно выпуклой.
попытаться понять, где здесь зазоры.
это хорошее упражнение.
это определение на функции нам будет нужно,
когда мы будем говорить про методы
для функций, у которых есть вот это вот свойство,
вот этот констант m больше нуля,
будет немного отличаться приведение и скорости сходимости.
поэтому сейчас это я ввожу, чтобы потом
сослаться на это определение.
погнали теперь к критериям.
можно считать упуклую функцию сильной выпуклость m равной нулю,
поэтому все дальше будет для упуклых функций
и потом просто m попола умножен на кое-что,
будет давать результат уже для сильного выпуклого функции.
утверждение вот такое вот есть.
дифференциальный критерий первого порядка,
который говорит нам,
что дифференцируемо определено на множестве.
сильного выпукла, если выполнено вот это.
давайте поймем геометрически, что это означает.
в случае, когда m равна нулю,
то это значит,
что в любой точке мы можем провести
опорную гиперплоскость
к над-графику,
или если по-другому как-то сформулировать,
то будет касательная,
которая будет дифференцируема,
и это касательно будет глобальной
оценкой снизу на всю функцию.
поставьте, пожалуйста, плюсик,
если нужна картинка.
нужна картинка все-таки.
вот смотрите.
вот у нас есть наша выпуклая функция,
и мы в любой точке, например, вот здесь,
можем провести гиперплоскость касательную.
то есть касательная у нас что такое?
это касательная в точке x, напоминаю, f от x,
плюс колярное произведение градиенс на y-x.
но это, собственно же, от y, наверное.
прямая, которая касается в точке...
при y равна x это просто f от x,
а дальше это гиперплоскость.
и эта штука в любой точке,
глобальная оценка снизу.
какую бы точку вот здесь, вот здесь вы не взяли,
у вас всегда гиперплоскость будет лежать ниже, чем эта функция.
понятна ли картинка?
поставьте плюс, если понятная картинка.
а можно же сдвинуть формулу? я ее не смог приписать.
давайте я просто и тут напишу,
как бы это не проблема, что вот f от y,
ну, если...
то есть для любых двух точек,
x и x от звездочки, это выполнено вот такое вот.
вот эта штука, если m не ноль,
означает, что...
давайте ее надо бы другим цветом нарисовать, конечно.
вот эта штука, она означает,
что... ой, я сейчас не попаду в цвет, боюсь.
сейчас, секунду, я попробую найти вот этот, что ли.
что вот в этой точке у нас
оценка не линейная, а квадратичная.
то есть есть некоторая парабола,
которая глобально снизу подпирает...
подпирает функцию.
это, собственно, и означает сильную выпуклость.
стала ли чуть понятнее геометрия
всех этих определений?
оставьте плюс, если понятный, милость, если надо подточнить.
так, прошу прощения за очередную паузу.
сейчас еще раз покажу картинку,
и вопрос остается в силе.
так, окей.
ну и, соответственно, поскольку есть
квадратичная оценка снизу, то мы можем,
если думать немножко вперед,
можно заменить задачу поиска минимум самой функции
на задачу поиска минимум квадратичной оценки снизу,
которая решается проще и дает некоторое приближение
к тому, что нам надо.
но это пойдет более подробно речь, когда о приметных будем говорить.
так, теперь снова к слайдам.
снова к слайдам.
вроде я нигде не набрал, все хорошо.
так, доказательства.
ну, на самом деле доказать довольно простое.
да, давайте уж я расскажу.
пусть выпукло. это же критерии, поэтому в обе стороны доказывать.
пусть выпукло. у нас есть определение,
которое мы можем переписать вот в таком виде.
что здесь произошло? ну, вынесли альфа за скобки.
и здесь тоже. если мы это на альф поделим,
то получим вот такое вот выражение.
то есть здесь будет разность точки x2,
здесь будет разность x1 и x2.
9 альф. переходим к пределу.
получаем вот такое выражение,
которое прямое равное нулю полностью нас удовлетворяет.
есть ли вопросы к выкладкам?
или здесь все понятно?
плюсик, пожалуйста, по ставке, если все понятно.
так, хорошо, вопрос. почему при переходе к пределу по альфе здесь возникает градиент?
кто может объяснить?
у нас производная по вектору слева получается в пределе,
а произвольная по вектору это скорее произведение градиента на вектор.
да, вы правы.
вы правы немножко утащить формулировку.
при переходе к пределу здесь происходит производная по направлению x1 и x2.
то есть производная это не по вектору.
производная все еще по числу, но просто по направлению фиксированному.
а для производной по направлению
ДС привели вот такую формулу,
которую мы вроде бы на прошлой лекции на слайдах она фигурировала.
так скорее произведение градиента в этой точке на направлении.
еще один способ понять, каким это правда,
если вы вдруг не очень как бы уверенно себя чувствуете,
это сказать, что поскольку альфа постремится к нулю,
вот это некоторая маленькая поправка.
вот это выражение можно по тейлеру разложить в окрестности x2.
и у вас в процессе разложений что будет?
f от x2 плюс колярное произведение градиента на вот этот вектор.
f от x2 сократится с вот этим, альфа поделится, останется ровно вот это.
еще один способ.
выбирайте, какой вам больше нравится.
есть ли еще вопросы по этому объяснению?
и в целом полный код.
вроде тишина, наверное, понятно.
обратно в сторону.
потому что, в общем, сейчас увидите.
пусть у нас есть это самое нереальное.
вот это.
пока см равный нулю, как можно перейти к m больше нуля?
это будет пояснение в конце.
берем произвольную точку.
рассматриваем комбинацию некоторых точек из этих двух.
поскольку условия наши выполнены для любых двух точек,
то сначала берем x1z.
1 в первый раз и x2z в второй раз.
записываем наши нерайны.
сложаем одно на альфа, другое на один.
и складываем.
то есть тут техника.
ну или к счастью.
просто применяете все эти...
ну просто понимаете, как апеллиры с окулярными произведениями.
умножить на альфа умножить на 1 минус альфа,
проводить необходимые выплатки, у вас получается, что f от z,
то есть вот это все дело сокращается в силу того, чему равно z.
f от z равны этой штуке.
меньше либо равно, чем альфа на f от x1 плюс 1 минус альфа на f от x2.
ну все, это очень победа.
получили определение. все хорошо.
теперь, чтобы перейти к сильно выпуклому случаю,
достаточно проделать все, что было проделано до этого,
к функции вот такой.
и это работает, потому что, справедливо,
выпуклась функция равносильна тому,
что вот такая вот функция...
то есть если из выпуклой вычесть квадратичный член
по документу,
и она по-прежнему будет выпукла,
то исходная была сильно выпукла.
это еще небольшая помощь к тому,
чтобы понять, что такое сильная выпуклая функция,
если вы не хотите страдать с определением, которое довольно трудоемко
и, мне кажется, легким для интерпретации.
как это упражнение делать?
ну вот тут надо на самом деле просто взять
и реально подставить в определение,
раскрыть полные квадраты,
и все получится.
вот здесь будет, соответственно,
выпуклая комбинация некоторых двух точек,
она раскрывается, переносится,
и, опять же, некоторые алгебры, которые, я думаю,
всем по силу здесь,
вы получаете результат про сильную выпуклую.
ну и наоборот.
то, что мы проделали, что получили.
оставьте плюс, если понятно.
я вижу плюс от 5 человек.
как сокращается скалярное произведение?
о, прекрасный вопрос. давайте пропишем.
вполне корректно.
мне нужно сейчас нажать вот так
и сделать вот так.
огонь.
так, ну смотрите.
сейчас я быстро перенесу то, что здесь написано.
то есть у нас, смотрите,
ой, странный цвет немножко.
у нас f' от z
умножает она
на z-x1
альфа
плюс
1 минус альфа
на f' от z
z-x2.
вижу новые сообщения в чате.
видеопуток завис.
интересно.
что-нибудь изменилось? о, отвис. прекрасно.
при этом напоминаю, спасибо, что
это делаем.
что z у нас по построению
формировалось как альфа x1
плюс 1 минус альфа x2.
видимо из-за того, что я экран переместил.
теперь все хорошо.
ну, смотрите, что происходит.
у нас z-x1, давайте я кусочком буду расписывать.
это что такое?
плюс 1 минус альфа на x2.
в то же время z-x2
равняется альфа
x1
плюс 1 минус альфа
x2 минус x2.
да?
супер. ну, теперь давайте посмотрим внимательно
на то, что будет, если умножить одно выражение
на альфа,
а другое на 1 минус альфа.
наверное, надо что-то как-то преобразовать, да?
или уже видно, что все сокращается.
сейчас я немножко хочу, наверное, полениться,
но если скажете, что не очевидно, то я пропишу детальнее.
ну, то есть, давайте вот.
видно уже, я надеюсь, что вот эта штука
будет совпадать с вот этой штукой
с точностью ста знака.
ответа нет.
пишешь туда, это видно.
ладно, что-то как-то я, поскольку не вижу большого энтузиазма,
давайте явно пропишу.
здесь плюс, ну и дальше, как бы, следите за руками.
так, ну, вот, смотрите.
во-первых, работает ли видеопоток?
начнем с простого вопроса.
вот сейчас работает и работает, да?
да.
ну, вот.
ну, вот.
ну, вот.
ну, вот.
вот сейчас работает и до этого нет
какое-то время.
поток заработал, это прекрасно.
ну, смотрите, первая строчка здесь,
вторая строчка это вторая строчка, вот этот плюсик
это вот этот плюсик.
ну, и вот сейчас, я надеюсь, видно, что вот
вот эта штука уходит с вот этой штукой.
вот.
вот эта штука уходит с вот этой.
так, Александр, понятно ли, что произошло?
так, ну, прекрасно.
то есть, ну, пополучно, с помощью этих разностей
все удалось починить.
так, ну, окей.
идем дальше.
так, что-то нас встретили, окей.
вот.
вот.
вот.
вот.
вот.
вот.
вот.
вот.
вот.
вот.
вот.
вот.
вот.
вот.
вот.
вот.
пожалуйста скажите понятно ли откуда это неравенство взялось или нужно прописать формулу
так ну судя по тому что нет сильно так вот кому-то уже понятно стало
так может быть кто-нибудь еще
почему это работает это справедливо любой точке x в том числе в x альфа
а распределил x альфа то воспользовавшись определением скалерного произведения
вопроса двух сторон донажаем на y минус x и на x минус x транспонированная и вот здесь у нас
образуются квадраты в кидовой норке это все в общем-то но одна вторая тут одна вторая здесь
так поставьте плюс если нужно написать формулу я пропустил вот андрей вы поставили плюс потому
формулу извините что я переспрашиваю я поставил плюс на предыдущий когда
спрашивали хорошо да надо какой-то это с этим обозначений придумывайте как хитрее ладно так
ну вроде не вижу запроса на то чтобы это детально прописывать окей если появится ну я в любом случае
все эти доказательства постараюсь оформить по дайвке также выставить в чат чтобы можно было
более детально пройтись по ним вот ну собственно да и через этого следует но вот это штука больше
поэтому можем как бы подпереть снизу и получим просто как первого порядка по которому пункса
будет выпукло то есть тут вот вот это квадратично слагаемый заменяется на слагаемый из категории
первого порядка вот теперь обратную сторону пусть есть точка в которой это не выполнено тогда
значит у нас есть направление у нас есть направление по которому выполнено вот это просто из
определения дальше далее тут нам будет важно теперь возьмем и поэтому направление немножко
сдвинемся от икса на маленькое вот и оказывается что поскольку у нас есть непрерывная дифференцируемый
дважды непрерывная дифференцируемость то вот тут кстати пишут что кто-то хочет учиться
извините икс ирик достаточно близки тут наверно z они икс должен быть да прошу прощения я не
поправила печатку так это восьмой слайд икс назад поменять что эта штука работает и для
икс альфа вот то есть тут как бы пользуемся непрерывностью и тем что можно взять достаточно
маленькая епсилон чтобы но то есть чтобы в этой точке нарушалась нарушался этот знак и
как следствие могли не выполнялся бы критерии первого порядка понятно ли вот этот переход и
использование непрерывной дифференцируемости если понятно поставьте пожалуйста плюс то есть
удивительно но для того чтобы доказать работоспособность непосредственно практически
важного критерия мы доказали третий первый порядка потом на него слайд все вот такая вот
ага ходовочку как будто все получилось так ну я вижу что понимание не у всех пока что пришло так
давайте что-нибудь с этим сделаем то есть вот это выражение то есть понятно ли вот это выражение
давайте откуда оно взялось начнем с простого вот то есть мы и наш есть такая точка который
гессиан вы положите не определен не так как требует наш критерий вот поэтому есть такое
направление вот теперь мы берем и язык са переходим в это направление и при достаточно
мы предполагаем что все все это ну мы можем выбрать точки икс и игрек так что они будут
достаточно близки зоя которые выполнена вот это и по непрерывности существует окрестность в которой
знак сохраняется вот наверное правильно так сказать помните это утверждение было вот они
о том что есть функции прерывно то есть окрестность который так сохраняется дам было что-то такое да но
вот мы сейчас этим активно пользуемся еще я еще там про ноль непрерывной пункции вот используется в
доказательствах которые вчера выложил поэтому как бы внезапно какие-то казалось бы не очень
практически важные вещи начинают всплывать как обычно самый подходящий момент вот в общем поэтому
знак сохраняется и для и цальха тоже и поэтому притория первого порядка нарушается вот в этом
давайте тогда наверное сейчас уже 10-20 а у меня пять минут еще даже есть да вот он и я понял что
как-то тяжело это доказательство воспринимается я постараюсь его письменно оформить вот тогда
может быть полегче вот ну в общем тут вот самый важный слайд наверное давайте я его пропущу я
оставлю в следующий раз подробнее по нему скажу вот а пока сформулирую наверное основа факт который
нам будет нужен ну с помощью которого все строится вот факт про то что если у нас есть локальный минивум
функции это локальный минивум является глобально вот то о чем я упоминал несколько лет назад
доказательство противного то есть пусть у нас есть новый глобальный минивум какой-то вот так
слушайте давайте попробую это все записать на доске вот возможно так будет лучше восприниматься
попробуем метампрубаши мы будем искать подходящий формат для всех так вот есть доска уран
присмотрите f нашу функцию у нас есть икса звездочка локальный минивум
пусть существует некоторые игрекса звездочки глобальный минивум давайте там тоже
вот он во-первых глобальный а во-вторых и соответствует значение в нём строгом меньше
чем значение в атыкса звездочек это наше предположение которое сейчас будем искать противоречие
вот что значит локальный минимум это значит что есть окрестность
которые значение f от икса звездочка меньше либо равно f от икс для всех икс
ну таких что икс минус икс со звездочкой меньше там некоторого дельта это поминаю определение
мы его достаточно быстро прошли пролистали когда об этом шла речь я надеюсь что здесь
все понятно насколько я прав то есть пока просто какие-то базовые вещи записываем
можно ли продолжать ставьте плюсики если можно продолжать так раз два три четыре
три человек за то чтобы продолжать а вот тот кто-то еще потягивается прекрасно спасибо
ну то есть дальше смотрите какой тюк берем точку z которая будет записываться как альфа на y со
звездочкой плюс один минус алифа на икс достаточно мало и так чтобы было выполнено что гадаете что
что z минус икс со звездочкой меньше либо равно то есть ну существует же у нас некоторая окрестность
да вот ну и дальше мы берем и подставляем ну точно что это нам дает нам дает что f от икса звездочкой
меньше либо равно f от z потому что z лежит в правильной окрестности а это в свою очередь
поскольку z у нас выпуклая комбинация точек дает нам что это альфа f от икса звездочкой плюс
один минус альфа f от икса но мы знаем что у нас есть вот это предположение которое будет нам
говорить о том что это строго меньше вот важно чем альфа f от икса звездочки плюс один минус альфа
f от икса звездочка что в точности f от икса звездочка и внимание мы получили гениальный противоречи
что вот эта штука меньше чем вот эта штука
все ли увидели это противоречие пожалуйста поставьте плюс если вы его увидели так ok 1 2 3 4 5 6 7 8 так
ну вот 8 человек увидел это половина интересно что происходит со всеми остальными ох ладно ну то есть
получить противоречия из которого следует из которого следует что ну собственно глобальный
вот для выпуклой функции то есть теорема достаточно инструментально в том плане что она
дает рецепт и как бы явно указывать место где нужна выпуклость функции то есть как бы вот
вот у вас есть блестящее следствие которое вам дает как бы локальный эквивалент из
локальности глобальности так ну тогда наверное на сегодня все смотрите что у нас будет в этот
раз сразу я авансирую чтобы вы примерно понимали куда мы движемся вот следующий раз я подробнее
расскажу вот этот слайд про композиции он очень важный в дальнейшем нам будет много раз нужен вот
и расскажу про сложную выпуклую задачу и простую не выпуклую задачу вот очень надо здесь кажется
все а потом будет не россиянцы но это мелочи тут как бы достаточно простой выгодку по инукции вот
и потом мы собственно перейдем по спалкам задачи по оптимизации как и они бывают какие конусы
при этом задействуются и что с этим всем можно делать как преобразовывать так наверное по материалу
на сегодня все если есть какие-то вопросы давайте минутку я готов дождать если они появятся потом
пожалуйста пишите вот я постараюсь на них оперативно ответить вот в общем-то запись я выложу
на google диск скорее всего вот и ссылочка на папку с записями будет также в чате в ближайшее время

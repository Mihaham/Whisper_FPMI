Я еще раз повторю, что мы сейчас поговорим про методы
первого порядка, то есть про методы, которые используют
только информацию о градиенте, вот, и поймем, что будет
происходить, когда этот градиент доступен только
приближенным.
Что значит приближенным?
То есть мы будем посмотреть, что называется, заглянем
в черный ящик, и вот таких вот задач перейдем к задачам,
где у нас целевая функция известной структуры, то
есть х у нас будет лежать по-прежнему в рэн, где n маленькая,
вот, а целевая функция будет представляться в виде
суммы большого числа функций, n большое, вот, и, значит,
из этого, не знаю, вот эту вот структуру замечательную
мы сможем получить методы, которые будут работать
быстрее по времени, хотя с практической оценкой
сходимости будут некоторые соответствующие проблемы,
за более быструю сходимость по времени надо будет что-то
заплатить, вот, значит, вот эта штука, давайте f большой
обозначим что-ли, вот, можно ее понятным образом, я надеюсь,
представить, что это на самом деле мы от ожидания
от некоторые функции там fхxy, xxy, где xxy это там равномерная
юниформ, вот, да, равномерная, ну, в общем, сейчас, мегтры
p и p там, k равно i единиц на n, вот, ну, то есть мы сэмплируем,
равномерное распределение и оцениваем от ожидания как среднее по нашим эффектам.
Вот. Понятно ли переход к отожаданию? Хорошо. Если будет что-то непонятно,
ставьте минус в чате или говорите в микрофон, что что-то непонятно. Вот. Смотрите, значит,
что это нам дает. Это нам дает возможность, собственно, попытаться вместо градиента
точного, который будет равен, ну понятно чему. Вот. Используй градиент неточный. Вот. А именно
градиент, который будет равен равен fх просто какой-то, например, ик от x. То есть ик лежит у
нас в множестве от 1 до n. Вот. И мы можем, например, в качестве оценки на градиент взять просто
какую-то слагаемую. Вот. Ну и важная штуковина и важным свойством этой оценки будет являться
то, что им от ожидания по… Ну давайте я вот так перенапишу сейчас, что все-таки эта штука
немножко случайная, потому что мы ик атэ сэмплируем, ик атэ из… Ну, в общем, из этого
распределения, но вероятность того, что ик атэ равняется и, соответственно, единиц на n. Вот. И мы
случайным образом просэмплировали некоторый индекс, потом взяли нужное нам слагаемое вот отсюда. Вот.
И потом сказали, что окей, теперь это наш градиент. И дальше мы будем его использовать вместо градиента,
который у нас был всегда, и получим так знаменитый… А что у нас за нижний индекс? Ну, кси обозначает то,
что у нас… Пам-пам-пам. Наверное, правильно так записать. Сейчас, как всегда,
какая-то путаница в обозначениях. Ну, это ничего страшного. Вот так. Ну, то есть вот этот вот нижний
индекс, он нужен для того, чтобы обосновать, откуда здесь ик атэ берется. Ик атэ берет сэмплированием
случайной величины кси с равной вероятностями. А лучше ли сейчас? А, окей, гуд. Ну, вот.
Соответственно, мы получаем классический градиентный метод, который обновляет нам
наш иксы по некоторой случайной оценке на градиент. Вот важно, что у нас не смещенные. Вот. К сожалению,
ну, понятно, что это все так-не так. Вот это быстро посчитать. Вот. Ну, то есть мы считаем градиенты
от одной функции вместо того, чтобы считать их от N. Вот. То есть мы в N раз быстрее по просто
количеству операций. И давайте теперь посмотрим на то, каким последствием это приводит. Вот. А именно,
то изменится, как вы думаете, в траектории сходимости по сравнению с тем, что у нас было просто, то есть,
вот давайте сравним с хостичкой основой градиента и точной основой градиента, что изменится,
какие будут наблюдения. Что можно наблюдать и будет ли это хорошо или плохо, и как с этим бороться,
если это будет плохо. Пожалуйста, напишите ваши идеи в чате. И монотонно. Да, это правда. Еще. Вот
эта не монотонность, она с чем связана? Да, вот это на самом деле интересный вопрос, что делаем
больше итерации. Вот. Ну, вот. Он в части связан с тем, что с моим предыдущим вопросом, что вот с чем
связана не монотонность, что типа делаем больше итерации, а что изменится ли то те результаты
сходимости, которые мы до этого получали. Если изменится, то в какую сторону и в чем, собственно,
называется первая причина таких изменений? Ну, вижу, что пока идеи особо нет, но первая причина изменений в том,
что вот здесь появляется дисперсия, которая у нас не была в случае градиентного метода. Вот. И вот
с этой дисперсией она будет нам очень много проблем создавать при честном анализе сходимости. Вот.
То есть, здесь у нас была скорость единицы на к для L-гладкой выпуклой функции. Вот. И это была,
это сходимость, она была, если я правильно помню, типа fkt минус f со звездочкой. Можно вот так писать,
просто писать. Вот. То есть, там был учет меньше, прошлый раз, вроде, доказывали, учет 2L на альфа к,
и тут еще x со звездочкой минус x0. Может быть, помните. Если не помните, посмотрите скрипт прошлой
лекции, там это было более подробно. Теперь важный вопрос. Что будет здесь для этого же класса
функции? Ну, давайте сейчас вот, как вы думаете, что будет? Какая-то интуиция. Проверим вашу интуицию.
Что мы пронаблюдаем? Может быть, сублинейную сходимость? Ну, она и так сублинейная и в градиентном
спуске. А какой порядок-то будет? И будет ли вообще сходимость, что более важно? Ну, не раз
вы так спрашиваете, то, видимо, не будет. Ну, а как бы. Действительно, будут некоторые проблемы.
Сейчас мы, собственно, теоремы, которые я попытаюсь сейчас быстренько, ну, вот, может быть,
относительно быстренько доказать. Вот. Будет нам говорить о том, что, если у нас есть fL-гладкая
выпуклая, то есть, довольно сильные условия, по сравнению с тем, как этот метод применяется более
или менее в реальной жизни. fL-гладкая выпуклая, значит, направление сэмплируется из, ну, понятно,
какого распределения. Вот. И... Чего? И будем обозначать вот то, что у нас сэмплируется за
VIT. Вот. Так. Сейчас. Что-то я всё не метон. А, всегда. Пом-пом-пом-пом. Сейчас. То есть,
обозначим его за VIT. Вот. И мы будем знать, что, то есть, VIT это направление, ценка f' от x-итова.
Вот так правильно сказать. Что ещё известно? Известно, что дисперсия у этого VIT-а,
она ограничена для всех и сигму квадрат. Ну, то есть, понятно, шум конечный. Вот. И также шаг у нас
меньше, либо 1 на L, как это тоже самое нерайс, который был в датаминированном случае. То есть,
пытаемся максимально... Вот я сейчас подчеркну то, что у нас было в датаминированном случае. То есть,
у нас было вот это и у нас было вот это. Всё остальное, это что-то новенькое. Сейчас поймём... Да. И,
собственно, результат-то какой? Результат-то кое-что... Тут сейчас будет хитро. Дани f от x-ка среднее,
минус f от x-а звёздочкой будет меньше либо равен, чем x0 минус x-а звёздочкой. Всё как и было.
Делить на 2 альфа к, вот. И внезапно плюс альфа сигма квадрат пополам. То есть, вот это вот
результат. Ну, xkt с чертой это единица на к сумма x-итых и там от 0 до k-1. Ну, где-то так. Ну, либо
от 1 до k. Понятно, что это усреднение за первой к айтерацией. Сейчас, Ниндия, а можете ещё раз на
v-i-t, что тирает оценка f? Это направление, по которому будем двигаться. То есть, у нас будет выражение
x-i плюс 1 равняется x-i плюс... Там будет плюс или минус. Мы сейчас ещё посмотрим. Там альфа,
i-t, v-i-t. И это как бы v-i-t, это вот та самая оценка градиента, которую мы до этого обозначали вот так.
Видите, много-много символов v-i-t. Побыстрее просто оценивать. Ну, понятно. Вот. И здесь для шума этого,
собственно, для шума этой оценки задна граница вверху. То есть, смотрите, что произошло. У нас
появилось, помимо того, ну, то есть понятно, что с ростом k у нас фразуется что-то, что стремится к
нулю и что-то, что константа и константном альфе. Вот. То есть, если мы возьмём постоянный шаг,
то эта штука перестанет сходиться. Вот. Что есть хорошо. Вот. Поэтому следующим шагом после доказательств
этого факта мы поймём, какой альфа стоит взять. Ну, это более-менее очевидно. Я думаю, давайте.
Какую альфу надо брать, чтобы сходимость появилась? Киньте в чат выражение, пожалуйста.
Чтобы правая часть полюсходилась при к стремящемся бесконечности. 2 в степени минус k. Да, агрессивно.
Ну, тогда будет какая-то грустная сходимость, кажется. Ой-ой. Чё-то вы неправы. Типа, давайте
подставим. Пусть у нас альфа k-ты, да, видимо, равняется 2 в степени минус k, и мы эту штуку
сюда подставляем. И мы внезапно получаем 2 в степени k на x 0 минус x со звёздочкой. Делить
всего лишь на 2k, и эта штука стремится к плюс бесконечности, что вообще-то очевидно. Ну, я думаю,
очевидно. Это не ряд, нет. Это прям вот оценка, как она есть. То есть это не подходит. Да, вот 1 на
корень из k. Из k более правдоподобная гипотеза. Вот. И если, ок, если сейчас это вот сюда подставить,
то мы получим сходимость вида 1 на корень из k. Ну, я думаю, что подставить для вас не составит
большого труда. Вот. Просто вот в эту оценку подставляется альфа равная 1 на корень из k,
и вот в знаменателе здесь получается просто корень из k, а альфа единица на корень из k,
корень из k переходит сюда. Вот. И получаем вот такую оценку, что существенно хуже от 1 на k,
которое было в градиентном спуске. То есть если мы делаем уменьшающиеся последовательность шагов,
ходимость ухудшается в отличие от постоянного шага в градиентном спуске детерминирован. Вот.
Понятно ли разница и то, насколько, ну, то есть степень ухудшения. Ставьте плюс, если понятно,
и минус, если нужно еще подробнее уточнить. Никита, у вас там дела? А, все, вижу, спасибо. Вот. Ну,
давайте докажем, чтобы как-то все более-менее обосновать. Вот. Ну, доказательство начинается
с уже классических шагов, которые были проделаны в прошлый раз. Сначала мы используем квадратичную
оценку сверху в силу L-гладкости. Вот. А вот дальше уже будут некоторые отличия,
которые в себя вберут то, что у нас теряется терминированность в направлении. Вот. То есть
вот тут было вот так, и L-стрик здесь, это точный градиент. Ровно лемма о спуске,
который мы использовали ранее. Вот. Теперь мы замечаем, что у нас, да, здесь все-таки минус,
вот так. Вот. Замечаем, что вот эта штука, это минус альфа и т в и т. Ну, и это понятно то же
самое. Где вы, это некоторая случайная штука, случайный вектор. Теперь, если мы это все подставим,
то получится что? Получится, что х и плюс один меньше либо равно, чем f от х и минус,
ну, минус, да, альфа, f-стрих х и v и и, и плюс l альфа квадрат пополам, норма v и в квадрате.
Вот. Вроде пока что довольно простые преобразования. Вот. Которые превратятся в следующую... Так,
давайте так. Что теперь хочется сделать? Глядя на выражение, которое стоит справа. Ладно,
я думал, что у вас довольно часто такие приемы были в других курсах. Ну, в общем, взять мотождание
хочется, потому что здесь какие-то случайные величины, у нас случайные векторы появились.
Давайте мотождание возьмем. Вот. Ну, если так записать через мотождание по, собственно,
сэмплированному индексу. Вот. У нас образуется тут f от х и т, тут минус альфа. У нас же не смещенный
оценок, поэтому тут появится квадрат. Тут будет l альфа квадрат пополам, а тут будет мотождание,
правильно ли я пишу? Да, мотождание от v и t в квадрате. Так. То теперь мешается. Какая
часть этого выражения, которое стоит справа, кажется, еще не докрученной до конца? Кажется,
последнее с мотожданием вы... Ну, последнее, конечно. Давайте поймем, что это такое. Ну,
вот у нас было прекрасное выражение про дисперсию. Что такое дисперсия от... Так.
Дисперсию от v и t можно, как ее определить, как мотождание нормы квадрата минус квадрат
мотождания. Квадрат, это же квадрат мотождания. Вот так. Ну, собственно, вот эта штука,
которая у нас стоит в третьем слагаемом, раз, два, три, да. Вот. Отсюда как бы напрямую следует,
что это там равно, да. Видимо, можно точную равенство поставить. f от x и t минус альфа норма
градиента. Все прекрасно. Плюс l альфа квадрат пополам. А тут появляется внезапная дисперсия
v и t в плюс... Ну, плюс понятно что, потому что у нас не смещенная оценка. Вот. То же самое выражение,
которое у нас было и предыдущим слагаемом. Вот. Ну и теперь, поскольку мы знаем, что дисперсия
оценивается сверху сигму квадрат, то это все меньше либо равно чем f от x и t. Вот. А дальше,
ну понятно, что вот это группируется с вот этим. Следующее выражение минус альфа. А здесь остается
единица минус l альфа пополам на норму градиента. Вот. И плюс, соответственно, l альфа квадрат
пополам, умножаясь на сигму квадрата. Так. Понятно ли, что произошло? Поставьте плюс,
если понятно и минус, если нужны более детальные пояснения. Так. Здорово. Вижу плюсы. Замечательно.
Вот. Ну а дальше, Иван, пользуемся просто тем же самым фактом, что у нас l альфа меньше либо равно
единицы. Вот. Поэтому это все меньше либо равно чем f от x и минус альфа пополам. Вот. На норму
градиента и плюс, что там получается, альф пополам, да, на сигму квадрат. То есть,
вот это вот все, оно уехало вот сюда и оно же уехало вот сюда. Вот. Получили такую оценку.
Теперь. Что теперь? Теперь мы блестящим образом учитываем наше прекрасное неравенство для f от
x этого. Вот. У нас же теперь и включается. То есть, вот это все, что было до этого, это была исключительная
гладкость. Вот. И мы выпуклость никак не использовали. Ну вот. Теперь мы как бы используем явным образом
выпуклость. Вот. Тем же самым образом, каким мы это использовали в, доказывается, сходимости для
простого градиентного метода. Вот. И там у нас появилось такое неравенство, что f от x
со звездочкой меньше либо равняется, чем f от x it. Ну, собственно, критерии первого порядка выпуклости
задействуем. Плюс f штрих от x i. Колярное произведение x i минус x со звездочкой. Вот. Это,
собственно, выпуклость. Теперь. Ну, собственно, тут частный градиент. Вот. Поэтому мы вот. Ой. Я
немножко тут наврал. Прошу прощения. Тут, на самом деле, наоборот. Вот тут x i, тут x со звездочкой. А,
нет. Наврал только в одном месте. Сейчас. А, нет. Все, да. Да-да-да. Все правильно теперь. И вот эта штука
теперь мы подставляем вот сюда. Получаем еще раз, что в нашем от ожидания f x и плюс один меньше
либо равно. Ну, то есть нам же надо с f от x со звездочкой как-то сравниваться. Вот мы сейчас так и
сравниваемся. f штрих x it x it минус x со звездочкой. А дальше. Ой. Дальше минус альфа пополам f штрих
от x it. Плюс альфа пополам сигн квадрат. Вот. Ну, то есть просто подставили, чтобы у нас получилось. Ой.
Чтобы у нас с одной стороны получилось x и плюс один. А тут появился x со звездочкой. А дальше нам
нужно что-то сделать с вот этой вот непонятной штуковиной. Вот. Сейчас будем думать, что с ней
сделать. Ну и вот этот шрум, он как был, так и у нас останется. Поэтому мы как-то пока его тянем за
собой. Вот. Теперь сделаем следующий, казалось бы, странный трюк. Вот. Суть которого заключается в том,
чтобы внести под мат ожидания справа как можно большее количество выражений, чтобы получить опять
же ту самую телескопическую сумму, которая у нас фигурировала в прошлый раз. Вот. Для этого мы
подставим, как бы странным это не казалось, то, что до этого мы вынесли. А именно у нас f штрих
от x it в этом от ожидания v it. Вот. А норма f от x it в квадрате, это что такое? Ну, то есть это
мат ожидания, понятно. Это мат ожидания от нормы v it в квадрате, в квадрате. Мила дисперсии. Вот. То есть
все немножко наоборот. То есть то, от чего и где берется норма, и это, соответственно, меньше
либо равно, чем, чем что, чем что, чем что, чем норм, мат ожидания опять же. Ну и минус сигма квадрата,
потому что меньше либо равно. Вот. Вот получили такое замечательное неравенство, а поскольку здесь
стоит знак минус, сейчас я уточню, где он стоит. Вот. Знак минус стоит. То раз мы оценили это сверху,
то когда мы будем это вычитать, ну вычитать, то... А я понял вопрос. Да, слушайте, давайте сейчас мы тогда
более более аккуратно это все провернем. Да, тут давайте пока ограничимся вот этим. Сейчас нам
просто плюс же появится. Вот смотрите, вот у нас есть вот это, да. Вот. И мы давайте просто подставим.
То есть получается, что от ожидания слева f от x и плюс один меньше либо равняется f от
x со звездочкой плюс, плюс скалярное произведение от ожидания v и того на x и то минус x со звездочкой,
минус. Это вот важный минус, который нам сейчас все починит. И мы это умножаем на мат ожидания от v
и t в квадрате, минус дисперсию v и то, плюс альфа к полам сигма квадрата. Вот. Здесь просто
определение. Теперь если мы скобки раскроем, f со звездочки я буду так еще списать. Немножко подскорчу
тут скалярное произведение, которое как было так осталось. А дальше остается минус альфа к полам
от ожидания нормы v и t в квадрате и плюс альфа к полам на дисперсию и плюс еще альфа к полам
на сигму квадрата. А это уже меньше либо равно. Вот здесь как бы вот появился плюс, который позволит
нам сейчас сверху оценить. Плюс снова дисперсия, я вот так вот напишу. Вот. Минус снова альфа к полам
от ожидания v и t в квадрате и плюс альфа сигму квадрата. Вот. Понятно, что произошло, да? Ну,
а теперь давайте вот напишем аккуратно равенство. Плюс от ожиданий от чего? От ожидания от, большая
скобка, скалярного произведения v и t на x и t минус x со звездочкой и минус альфа к пополам
норма v и t в квадрате. Плюс альфа сигму квадрата. Вот. То есть у нас какие основные ингредиенты
получившегося равенства? У нас есть вот этот кусочек от нашего от ожидания и вот этот кусочек. Это
как бы то, что уже хорошо. У нас есть довольно приятный кусочек вот этот. Есть не очень понятный,
понятный по содержанию, но хорошо выверенный по структуре кусочек с от ожиданием от некой величины,
которую вы сейчас скажете, на что очень похоже и чего надо с ней теперь сделать. Какие будут
варианты? Чего похоже-то? Третий курс, наверное, должны такое видеть с первого взгляда. Кажется,
не очень. Смотрите, ну, есть v и t и его норма в квадрате. Есть v и t, который на что-то умножается.
Чего дальше надо сделать? В плане, может норму раскрыть в скалярном произведении с самим
собой и свернуть в общее скалярное произведение v и t на… Нет. Мы с этого начинали. Мы с этого
начинали. Нет, нет, нет. Не совсем. Что еще можно сделать? Нужно добавить и вычесть норму вот этой
величины, чтобы полный квадрат выделить. То есть это почти что полный квадрат у нас тут. Отсюда,
собственно, довольно простая гипотеза, что… Ну, в общем, так, давайте это как-то обозначить. Ну,
слушайте, да, у меня уже есть о прекрасном значении. Вот то, что выделено красненьким,
вот оно будет у меня тут равно. Я сейчас вынесу что-нибудь отсюда типа 1 на 2 альфа, как будто бы.
Вот. Дальше тут будет мотождание от вот такой штуки. Будет 2 альфа v и t x и t минус x… Ой,
минус x со звездочкой. Минус… Минус альф… Минус альфа v и t в квадрате. Во, прекрасно. Так,
ну давайте думать. Дальше минус вынести на всякий случай. Здесь останется плюс, здесь будет минус.
Сюда появится плюс… Так, ну давайте, плюс x и t минус x со звездочкой в квадрате и минус x и t
минус x со звездочкой в квадрате. Поставьте плюс, если вы… О, кто-то еще хочет присоединиться,
приятно. Поставьте плюс, если вы увидели, что произошло, и проверьте, что все корректно заодно.
Лидия, приветствую. Мы сейчас занимаемся тем, что доказываем прекрасный результат о том,
что стахастический градиентный спуск с постоянным шагом для сильно… для выпуклой функции или гладкой
не сходится по функции, с вот такой вот оценкой. Вот. Мы почти закончили, на самом деле. Сейчас,
я думаю, ваши коллеги быстро подскажут, проверят и подскажут, что все правильно. Тогда мы пойдем
дальше, здесь будет понятнее. О, я вижу плюс, это прекрасно. Значит, все в порядке. Вот, смотрите,
а дальше что происходит? Дальше мы это записываем как минус 1 на 2 альфа. Мотор ожидания. Дальше
будет некоторый выклад, который я сейчас напишу. В общем, вроде бы получилось что-то как-то сложить
вместе, и минус я, соответственно, внес внутрь. Вот. То есть, смотрите, теперь я вот итоговое
выражение постараюсь записать. То есть, у нас есть мотор ожидания от f, x и плюс 1. Меньше или
бы равняется f со звездочкой. Плюс мотор ожидания от 1 на 2 альфа. Вот такой вот штуки. x и
t минус x со звездочкой. Минус x и плюс 1 минус x со звездочкой. Вот. И плюс альфа сигма квадратов. Вот.
И тут мы снова увидим прекрасное выражение, которое уже было встречено прошлый раз. А именно,
вот такая вот сумма, которая при суммировании благополучно телескопически уничтожится. То есть,
вот это вот сократится с вот этим на двух соседних слагаемых. Поставьте плюс,
если вы это видите и понимаете, о чем я говорю сейчас. А? Вот. Ну и отсюда следует как бы прямое
понятное следствие. То есть, мы просто берем и суммируем теперь вот все оба оба. Суммируем
оба обе части по и от нуля до к минус 1. Смотрим, что получается. Тут мотор ожидания f, x и плюс 1 минус f
со звездочкой. Вот. А здесь остается что? Здесь остается k альфа сигма квадрат. Плюс. Плюс что?
Плюс 1 на 2 альфа x0 минус x со звездочкой в квадрате. И минус. Минус мотор ожидания от нормы xk минус
x со звездочков в квадратике. Вот. Ну это понятно, что раз мы тут из чего-то вычитаем от ожидания
отрицательного числа, то получаем здесь k альфа сигма квадрат плюс 1 на 2 альфа нормы x0 минус x со
звездочкой в квадрате. Смотрите-ка, мы почти что у цели. Вот. Теперь осталось осознать, что делать с
левой частью. То есть, что вот это за сумма от ожидания f от x и т плюс 1? Понятно ли, что осталось с этим
побороться? Или про правую часть тоже что-то хотите спросить? Поставьте плюс, если все понятно, идем
дальше. И минус, если нужно еще про правую часть что-то сказать. Вижу, вижу, вижу, что вроде нормально.
Да, вы правы, но это в общем-то неважно, потому что... Да, тут надо бы дописать, спасибо. Потому что все
равно вычитаем и убираем в оценке. Да, хорошо, для корректности, конечно, нужно добавить множество.
Ну, давайте теперь, собственно, вернемся к тому, с чего начали, а именно к самой формулировке.
В формулировке у нас фигурирует вот такая вот странная величина. Вот. Давайте поймем теперь,
что это вообще такое. Вот. И окажется, что неожиданным образом, вот, что мы вспомним не Райси
Янсона. Надеюсь, вы помните, что это такое. Это вот для выпуклой функции появилась такая вот оценка,
что выпуклая функция от выпуклой комбинации аргументов типа x и t и от 1 до k меньше либо равно,
чем 1 на k сумма f от x и t. Вот. И, значит, что это значит? Вот. Это значит, что если мы теперь
множим обе части на k, то у нас получается k на функцию от x, k, ну, типа средняя, да, и это меньше
либо равно, чем сумма f от x и t. Вот. Далее. Что это как бы... Чем нам это помогает? Это нам помогает тем,
что у нас смотожидание линейно. Потому, когда мы пишем, что у нас тут сумма мотожидания f от x и
плюс один, минус f со звездочкой, да, и от нуля до k минус один, то это равняется мотожидании от
внимания сумма. Ну, а дальше сейчас увидите, что получится. Вот. Получили такое выражение. А вот
это мотожидание снизу подпирается, ну, в общем, понятной какой оценкой, да. То есть вот эта штука
это больше либо равно, чем k на f от x, k среднее. Ну, k вносится, то есть, как бы это правильно
сформулировать? А, да, давайте вот так. k на f от k нам от ожидания f от x, k среднее, вот,
минус f со звездочкой. Вот. Эта штука меньше либо равна получается, чем наша вот эта вот замечательная
оценка. Вот. Ну и значит, мы при делении на k получим, что нашим от ожидания от f от x, k среднее,
минус f со звездочкой будет меньше либо равно, чем вот вся вот эта вот замечательная история. Так,
копировать. Вот. И вот здесь все вставляется. И деленный на k. Это ровно то, что у нас было в правой
части. Видно ли, что это одно и то же? Поставьте плюс, если все понятно и видно, и минус, если
нужно подробнее что-то пояснить. Вижу пока два плюса. Так, Дмитрий. Все ли понятно? Ага, да,
спасибо. Ну все, короче, это мы доказали. Все классно. Так, отлично. Время еще есть. Вот.
Собственно, что... Да, ну и в общем, понятно, что при постановке выражения с 1 на корень
s, k становится понятно, что вот здесь вместо вопросика появляется о от 1 на корень s, k для
обывающего шага, что существенно хуже, чем то, что у нас было для дедаминированного случая как
раз таки еще дисперсии. Вот. Значит, ну понятно, что первое желание, возможно, которое возникает для
того, чтобы пытаться все-таки победить и получить хотя бы такую же скорой исходимости, это сказать,
что ну давайте мы типа ускорим это все дело. У нас же были ускоренные методы. Вот. Давайте ускорим и
будем надеяться на то, что после ускорения мы получим хотя бы сопоставимый с градиентным
методом скорой исходимости. Вот. Оказывается, что ускорение здесь не работает. Вот. То есть
конструкции, которые были в ускоренных методах при постановке неточного градиента, коренные
методы не как бы сформулировать, не приводят к более быстрой асимпатической исходимости при
неточном градиенте. То есть оказывается, что вот те прекрасные последовательности мы строили,
еще там что-то было в прошлый раз. Вот они тут не помогают в точке зрения именно асимпатической
расходимости для лопковых функций. Помогает немножко другая история. Вот. Ну, которая, я думаю,
более-менее очевидна. Вот. Называется она усреднение. Потому что раз у нас большая дисперсия,
ну раз у нас проблемы с дисперсией, то давайте мы ее уменьшим. Вот. Ну, какой самый простой
способ уменьшения дисперсии приходит в голову, вы думаете? Вы уже сказали усреднение, действительно.
Нет, а что это, как это работает? Ну, посчитать несколько независимых одинаково распределенных
оценок. То есть с разными, но... Давайте-давайте, формулируйте, да. Все правильно. Говорите,
формулируйте, это важно. Посчитать f с разными сидами рандома для w? Да, то есть смотрите,
я пытаюсь кратко как-то, кратко повторить то, что вы сформулировали. Самый простой способ
побороться с этой штукой, это сделать следующее, следующее преобразование. Сказать, что, окей, вот мы
берем нашу точку x, которая здесь, она так... Видно лазерную указку или нет? Да, видно. Видно,
прекрасно. Смотрите, взять x текущий и взять несколько слагаемых из этой суммы и для них
посчитать гриденты, просуммировать. Ну, в общем, получите ровно то же самое, что в ней ромка
называется там типа batch, вот, сложно переводимая, к сожалению, на русский слово. Вот. То есть типа
наша оценка f' от x, это типа формально вот что-то такое. И из вот этого множества, вот. И, собственно,
чем можно сказать, ну, довольно логично, это формально можно показать, то чем больше вы берете
эту множество, больше мощность этого множества, тем меньше дисперсия. Вот. Это как бы... Но доказывается,
более высокие скорости сходимости немного для другого способа учета этого самого средней. Вот.
Ну, пока давайте тем способом и общей стратегии закончим. А вот про вот эту схему я хочу
примерно показать. Вот. Чтобы как бы не быть голословным, для более как бы какого-то
теоретически обоснованного, но абсолютно на практике неприменимого способа я приведу,
ну, пока покажу теорию. Вот. А для вот этого наивного и довольно неплохо, ну, относительно неплохо
работающего на практике способа, давайте покажу какую-нибудь картинку, и мы это все дело увидим
просто собственными глазами. Так. Мне нужно для этого прекратить тут демонстрацию. Да, что ж такое,
это все такое ощущение вылезает. Так. И включить вот эти демонстрации. Вот. Для этого возьмем какую-нибудь
простую нейронку. Вот. Так. Runtime. Тут, по-моему, GPU стоит. Да. Вот. Взяли простую нейронку. Неважно.
Просто суперпозиции некоторых функций. Вот. Неважно. Такой там loss. Это все вам расскажут на других
курсах, я думаю. Вот. Ну, и возьмем датасет картинок и зададим, чтобы от часа израниться. Типа вот
абсолютно вырожденный супер, такой называется, крайний случай. Вот. И запустим всю эту балалайку
с ускоренным методом Momentum. Вот. И с методом, у которого там адаптивный шаг подбирается, про это
я тоже чуть попозже скажу. Вот. Наверное, кстати, можно было бы без него обойтись. Ой, что-то как-то
много это... Ну, да. Э-э-э. Надо было, наверное, по-другому запускать. Что-то, что-то не... Большой лог. Да.
Давайте пока тут остановимся. И я, наверное, перепилю сейчас. Ой, кошмар какой-то. Перепилю...
Что перепилю-то? А, там есть параметр, который называется logInterval. Вот. Его надо сделать не
10, типа, например, вот так. Вот. И тут сейчас будет что-то... Ну, да. Тысячи тоже много. А-а-а. Давайте
10 тысяч. Вот. Сейчас будет нормально. Вот здесь видно, как меняется loss. Был 2. Да, нет. Ну да,
заметьте, что, типа, долго, да? Гадаете, почему долго? Хотя батч один. Ну, какие будут идеи?
Может быть, оно работает так, чтобы, ну, эпоха заканчивается, когда у нас loss уменьшился?
Не-не, эпоха заканчивается, когда мы всегда... Ну, одна эпоха, это, типа, мы прошли по всей сумме.
То есть мы, типа, отсмотрели все сэмплы, все слагаемые, но, ну, как бы, для... У соседних батчей у нас
разные параметры. Наверное, это не очень понятно. Понятно. Короче, 43 процента точность. Классифицируем
картинки, если... Короче, классифицируем картинки 10 классов. Вот. Это супер-классика. Вот. Поэтому
я ее сейчас показываю. Вот. Ну, видно, что loss пляшет. То есть, типа, 79, ну, 26, вот сейчас вот. Вот.
А потом я хочу показать картину, как он изменяется на каждой... На каждой этой рации. Там он будет
посчитан только под выборки. Только по батчу, как бы он будет посчитан. Там будет, конечно,
большая осцилляция. Сейчас это будет видно. Так, меня так немножко смущает, что это все длится
over бесконечность, все пять эпох. Ой, печально как-то. Так, короче, вы оценили, насколько это долго в
таком варианте. Да? Ставьте плюс, если вы считаете, что это долго. Оценили. Окей. Ладно, давайте чуть
похитрее сделаем. Как это? Чуть похитрее сделаем. Скажем, что типа, батч 100. Да, все хорошо. Так. И должно
стать немножко побыстрее. Да, это точно на GPU. И я догадываюсь, что это так медленно, потому что
может есть какая-нибудь безумная размерность. Ну, да. 20 каналов, это немало. А тут я еще плохой лог
поставил. То есть, он сейчас будет только в конце каждой эпохи делать принты. Ну, короче, смотрите,
батч size 1 — это грустно, потому что он, эти данные, они типа последовательные. Ну, да. Я думаю, уже видно,
что быстрее немножко. Они обрабатываются типа последовательно. Вот. И при этом, типа, каждый батч
загружается на GPU. И там на этой GPU кучу места оставится свободного, потому что он не может
обработать следующий батч, пока текущий не закончился. Вот. И поэтому получается так, что,
типа, называется utility — производительность, загрузка. Ну, короче, степень того, как активно
используется память GPU для обработки этих данных, она, типа, супер низкая, потому что мы всего лишь
одну картинку загружаем в память каждый момент времени. Вот. Поэтому, типа, один, батч 1 — это,
типа, в таких случаях не очень хорошая стратегия. Вот. Так. Ну, уже почти конец. Я думаю, видно,
что стало, вроде, побыстрее немножко. Или нет? То есть, не особо. Наверное, было лучше бы таймер
поставить. Ну, да. Визуально, наверное, не очень. Это все хорошо. Ну, runtime — вот есть,
типа, GPU стоит. Потому что там какая-нибудь, типа, подобный GPU по дефолту выделивался,
поэтому так медленно. Ну ладно, SGD сейчас докрутится. До 42 он, вроде, долетел. Конечно,
на тесте такой ноль. А, это, типа, сколько обработан. Да, неважно. Да, ну, короче, 46. Вот.
46 доехал. Вот. И сейчас Adam, наверное, тоже за такое же, примерно, время доедет. Ну, делайте в
аштарке, что называется. Будет больше 46 или нет. Начали уже с 35. То есть, сейчас, вот, типа,
на уровне третьей эпохи SGD — одна эпоха, я думаю, типа. Вот. Сейчас я скажу, что за всеми этими словами
скроется по поводу там SGD. Ну, про SGD, вроде, уже сказали. Вот. И там, типа, momentum — это,
вот, собственно, ускоренная техника, которая симпатически лучше не делает, но на практике
все-таки позволяет немного ускориться. Вот. Так. 41%. Видите, немножко замедлилось.
Медлился темп. Кстати, хорошая идея по поводу посчитать, сколько стоит один батч. Сейчас,
наверное, это проделаю. Интересно. Ну, наверное, 45%. С другой стороны, типа, не очень понятно,
зачем это мерить, потому что понятно уже, что в работу несколько картинок будет надо
в время одной эпохи мерить, наверное, тогда. Что лучше сразу обработать с той или десять раз
по одной. Да. Ну, короче, наверное, сейчас я не буду это все делать, только запускать, потому что
не очень очевидно, как именно, что и с чем нужно будет сравнивать. 47%. Смотрите-ка, немножко побольше.
И сейчас еще одна эпоха осталась. Да, 48%. Ну, в общем, немножко он повыше. Смотрим на то. Да. Ну,
короче, вот график ходимости. Вот. Здесь, типа, каждая точка — это loss на соответствующем куске
до куске части слагаемых. Видно, что он, ну, типа, прыгает, ну, где-то 0,2, да, у него,
так, в среднем амплитуде. Понятно, что на картинке понятно, почему 0,2 или не очень. Если не очень,
поставьте минус. Ну, окей, вроде вижу плюсы. Хороший знак. Вот это было 100. Ну, давайте поставим,
типа, 500. И все то же самое. Да, есть еще подозрение, что тут, типа, в тесте он 16,
поэтому пока он пройдет по всем этим 16, все 100 лет может пройти. Тут тоже не дело не особо
преувеличивает скорости. Так, ну, 11%. Ура. Не помню, сколько там было в самом начале. Да,
бать был по меньшим. Сейчас заодно обсудим, если вдруг он сойдет. Ну, там был, типа, 47, да, 48 в
конце. Сейчас сойдется к чему-то, что меньше. Тоже обсудим, почему так произошло. Ну, видно,
что СГД почему-то стухает, да, и никаких там 40% уже, как бы, и нет. Ваши варианты,
почему так произошло, можно писать в чате. Ну, слушайте, ну, детально может буить. Нет,
погодите. Направление не точное. Мы вроде бы увеличили размер бачей, и они стали более точными.
Как будто бы ожидается как раз-таки увидеть что-то противоположенную динамику. Это они были не
точные в самом начале, но в самом начале получилась вышеточность. Немного контраиндуитивно получается.
Ну, видите, тут как бы Adam более устойчив в этой штуке. Сейчас обсудим, что внутри сидит и почему
так-то получается. Немножко осталось подождать. Сейчас мы оцениваем вообще что? Насколько мы близки
к инфинному по вообще возможному множеству функций или насколько мы близки к реальному? Я не понял
вопрос. У нас есть некоторые функции, которые есть параметры. Вот они тут внутри зашиты. Это функция,
ее как бы отображение реализовано. Не важно, что там внутри, там какие-то есть параметры,
которые оптимизируются. Мы хотим, чтобы нам по данному входу были нужны и нам параметры настроились
так, чтобы минимизировать некоторый лос, который адекватно оценивает вот эти проценты. Видно,
что он тут немножко падает. Причем падает сильнее, чем числа, которые были здесь. Вот. Ну да,
возможно даже на тест надо смотреть вот сюда. Тут вот видно, что это 0.40013, 0.009, что гораздо меньше.
Вот. Досчиталось, да? Ну, в общем, до 47 он все-таки долетел. Давайте на график посмотрим. Видно,
что тут не 0.2. Сброс в среднем, то тверждает гипотезу о том, что действительно мы увеличиваем
бачку, уменьшаем дисперсию в агредиенте. Вопрос. Почему эта штука сломалась, вы думаете? Ну,
смотрите, поскольку… Ну да. Не функции агредиентов. Почему нет? Ну, там же было ближе. Когда бач был
меньше, то было точнее. Лос был меньше. Задача не поменялась, поменялась только гиперпараметры.
Смотрите, да. Я надеюсь, запомните картинку. Я постараюсь сейчас что-то изобразить. Заранее,
прошу прощения, если будет кривовато. То есть ключевой момент, про который надо понимать,
про то, что когда мы смотрим на подобного рода функции, во-первых, это все дело не выпукло
катастрофически. Вот. Поэтому у нас три уровня, они какие-то вот такие могут быть. И если мы,
грубо говоря, начинаем идти вот отсюда, куда-нибудь, да, то наличишь, и то мы как бы идем,
идем, идем, куда-нибудь, тык-тык-тык, пришли. Может быть, еще сюда пришли. А тут, оказывается,
слишком такая большая ямка, вот, которая с увеличением точности наших направлений говорит нам,
да мы уже, типа, пришли в победе. Нам и тут хорошо. Не нужно никуда дальше двигаться. Вот. А если шума
больше, то он, типа, смотрит в разные стороны, типа, такой, окей, спрыгну-ка я сюда. Он такой,
о, типа, тут классное значение функции, поменьше и агредиент получше. Я, типа, могу сюда спуститься
запросто. Поэтому шум как бы позволяет вам, ну, так называемый, может быть, в литературе видели,
два свойства методов exploration и exploitation. Вот. Поставьте плюс, если видели эти слова когда-то
раньше в соответствующем контексте. Ну, не видели еще, да? Ну, у тебя впереди еще, значит. Кто-нибудь
видел? Не видели. В общем, exploration это то, насколько вы как бы исследуете ваше допустимое множество,
вот. Exploitation это то, насколько вы в рамках заданного множества хорошо решаете задачу. Вот. В общем,
ну, по-моему, это в reinforcement learning типичная, типичная терминология. Тут примерно похожа
ситуация, когда вы, вам нужно одновременно и получше локальный минимум найти, и в то же время
не застрять в том локальный минимум, который вы только что нашли. Вот. Поэтому получается такая
история. Теперь так, понятно ли обоснование, ну, некоторой интуиции, почему такое может
происходить? То есть вы как бы уменьшаете шум и получается хуже. Окей. Вот. Теперь, собственно,
про адаптивные методы, которых великое множество. Давайте я даже покажу, насколько великое множество,
чтобы вы примерно понимали, что это не фигура речи. Вот. Я, значит, для, если кому будет интересно
что-то с этим поэкспериментировать, я рекомендую библиотеку Optax, вот, которая сделана для джакса,
такого вот довольно гибкого и легковесного инструментария. Вот. Ну и вот common optimizer,
которые, собственно, адаптивные методы, основанные на хаосическом ингридиенте, они вот. То есть,
видите, их тут, типа, ну, много. Мы сегодня кратко, абсолютно, посмотрели на вот это. Сейчас немножко
посмотрим на, блам-блам-блам, на вот это. Вот это. Адаград. И вот это. Ну, типа, немножко устарело,
но кое-где это до сих пор используется. Вот. Все остальное, это некоторые надстройки над этими
базовыми методами, вот, заточенным под решение тех или иных задач из соответствующих предных
областей. Ну, типа, ламп, это для больших языковых моделей делается. Вот. Large batch optimizer. Вот. Ну,
а да, это же смешная статья. Короче, large batch optimizer. То есть, берем большой batch, то, что
мы сейчас пытались сделать, для текстового моделирования трансформеров, там, и всех вот этих
вот штуковин, да, типа, обучаем берц 7,6 минут. Вот. Когда в 7,6 минут получить супер, супер прекрасную
точную языковую модель. Внимание. Это все прекрасно. Бла-бла-бла. Идем в эксперименты. Вот. И
смотрим внимательно на то, что это типа 1024 TPU. То есть, это типа супер-супер топовое железо от
Google, которое, доступ к которому есть только у него. Вот. И вот на таком вот железе они это
параллельно, параллельно минимизировали некоторую функцию на вот таком вот, типа, тысячи, тысячи
размер бача, и получили 7,6 минут. Вот. Ну, то есть, как бы large batch для, скажем так, не бедных, не
бедных собственников топового железа. Вот. Короче, это я к тому, что надо внимательно, ой, надо
внимательно читать, что пишут в соответствующих статьях, какими бы рекламными названиями они
не оперировали. Вот. Короче, да, это вот LAMP, LARS примерно то же самое. Later Inspired. Ну да, в общем,
неважно. Вот. Сейчас я хочу, собственно, про этом поговорить. В основе, собственно, такие вот страшные
формулы. Вот. Идеи которых довольно простая. Смотрите. Значит, что мы знаем? У нас есть оценка на
градиент, и мы знаем, что она шумная. Вот. Первое, что было предложено, давайте мы будем ее сглаживать
экспоненциально. Вот. С помощью вот такой формулы. То есть, уже t это наша оценка. Вот. А nt-1,
ну, nt-1, собственно, вектор, который сохранит сглаженное направление градиентов. Сглаженное
по нескольким итерациям. То есть, мы как бы вносим вклад 1-β1, при этом β1 типа 0,9. То есть,
мы довольно сильно демпфируем новую информацию и предельно актуализируем и используем по максимуму
то, что у нас было на первых итерациях. Вот. То же самое делаем с поэлементными квадратами. То есть,
вот эта вот штука, это типа вектор по элементный квадрат. Вот. Дальше мы это все делаем,
масштабируем, делим на соответствующую историю. И наше направление, собственно,
по которому мы сдвигаемся, это наш вот этот вот learning grade, который шаг, который мы сюда писали,
альфа. Он умножается на вот этот вот вектор, который сглаженный, сглаженный, отшкалированный.
Потом еще поэлементно делится на корень из элементных квадратов. Вот. То есть, вот m нужно
для того, чтобы, собственно, усреднить и сглатить, а v нужно для того, чтобы, ну, вот если у вас,
грубо говоря, есть числа типа 100 и 10 минус второй, вот чтобы вы для каждого числа двигались
в соответствующем масштабе. И, ну, эти числа более-менее стабильны в вашем векторе. То есть,
это некоторое выравнивание траекторий и борьба с плохой обусловленностью локальный. Понятно ли,
из чего сделан метод? Оставьте плюс, если понятно. Окей, вижу. Вот. Ну и смотрите,
значит, наблюдение, которое из этого сразу расследует, это то, что, в общем-то, здесь
довольно много некоторых базовых простых конструкций. То есть, ну, типа, взвесить,
экспоненциально сгладить, кумулировать первые моменты, вторые моменты, там, умножить туда-сюда.
Вот. Что делает Obstax? Почему я, собственно, на его примере все это показываю? Он предлагает вам,
так, давайте я перейду на главную страницу, он предлагает вам некоторый набор базовых
преобразований, которые Obstax Transformations. Вот. И вот, собственно, есть тут тоже просто маленькое
тележко, которое вы можете спокойно применять в любом порядке, к любому оптимайзеру, который,
ну, к любому методу, который вы хотите придумать. Вы можете его строить просто комбинируя вот эти
вот там клипы, сделать Centralize, там что-нибудь, тут еще есть Scale. Вот. Можно сделать шкалирование одним,
шкалирование другим и как-то скомбинировать. То есть, вам выданы некоторые кирпичики, из которых
вы можете свои какие-то методы спокойно комбинировать буквально в несколько строчек. И не надо
заново писать все эти сложные формулы, и переживать о том, что вы запутаетесь, умножите или поделите
не на то. Вот. Здесь уже за вас все написано. Ну, главное как бы разобраться, что это значит,
чтобы как бы грамотно использовать. Вот. А в остальном, как бы, вот все базовые, как бы,
все базовые ингредиенты построения устойчивых, достаточно эффективных методов тут как бы
представлены. То есть, достаточно много работы уже сделано, сделано за вас. Вот. И если вам
нужно будет что-то новое сочинить, новое сочинить, то вы можете взять простые блоки отсюда и получить
соответствующие какие-то новые результаты. Вот. Поэтому это довольно, кажется, полезно и интересно.
Нет ли, о чем я сейчас попытался, какую мысль попытался занести? Вот. Лиги понятно. Как у
остальных дела? Так. Вроде, вроде нормально. Окей. Так. 10.13. Я что-то еще хотел рассказать. А,
у меня же было по плану как дисперсию снижать в общем случае. Да, короче, смотрите. Общий метод,
который как бы приводит к теоретическим некоторым полезным результатам, вот. Отключается в
следующем. Как раз, я думаю, за 7 минут я расскажу. Мне нужно расшарить доску. Секундочку. Немножко я
не успеваю одновременно прекращать трансляцию и начинать трансляцию. Короче, идея очень простая,
как всегда. Если у нас есть некоторые, некоторые х-омега, которые оценивают х. То есть,
мотор ожидания х-омеги равно х. Вот. И мы хотим уменьшить дисперсию этой оценки. Что надо
делать? Общая некоторая методология. Вводим новую величину у, которая равняется х-омега минус у-омега.
То есть, у-омега некоторая новая величина. Для которой справедливо, что ее мотор ожидания примерно,
то есть, мотор ожидания от у-омега примерно х. Ну, то есть, получили ту же самую оценку,
той же самой величины. Но если мы посмотрим на то, из чего сделана дисперсия, то мы увидим,
что это дисперсия от х минус удвоенная кавриация и плюс дисперсия от у. Вот. Теперь, внимание.
Если мы добьемся того, что вот эта штука будет большой, то суммарно эта вся история станет
много меньше, чем дисперсия от ликса. Понятно ли, как это работает? То есть, вы берете некоторую
другую случайную величину с нулевой от ожидания, которая коррелирует с случайной величиной,
которая опроксимирует х. И вычитая ее, получаете новую случайную величину, которая также опроксимирует х,
но имеет меньше и меньше дисперсии. Вот. Ну и, соответственно, метод, который эксплуатирует эту
идею и который сходится также, как градиентный спустодетерминированный случай, называется САК. Это
типа тринадцатый год. И авторы, я, конечно же, не помню. А, помню. Шмидтс и Толк, обычно. Их там
три штуки. Вот. И идея этого метода в следующем. Вы сейчас поразитесь, насколько можно сильно,
что называется, усреднять. Так, это буква Г. Вот. Значит, что происходит? Сначала посчитали первый
шаг. Ну, нулевой шаг. Есть х0. Посчитали g1 равное f1 штриху х0. g2 равняется f2 штриху х0.
И так далее. gn равное fn штриху х0. Далее. На ката-итерации. Ката-итерация. Просэмплировали
замечательный наш индекс. Какой? Новый индекс. Вот. Да, и давайте тут я типа поставлю нолики еще
такие вот. Которые будут обозначать номер итерации. Новый индекс просэмплировали. Посчитали.
Получается gk ik. f штрих ik от xk. Вот. А дальше сделали следующий трюк. Казали, что xk плюс 1 это
xk минус альфа от, внимание, значит, 1 на n g ik на ката-итерации минус 1 на n g ik на k минус первой
итерации и плюс 1 на n сумма g it ката-итерации. Так, ну давайте разбираться в индексах. У нас
изначально был набор векторов, каждый из которых равен соответствующему градиенту по каждому из
слагаемых. На каждой итерации мы вычисляем какой-то один новый градиент в новой точке и используем в
качестве нашего направления вектор, который получен усреднением вот этих вот величин, правленные
на то, что мы посчитали новый градиент. То есть мы вычислили старое значение, прибавили новое.
При этом все остальные значения мы не изменили. А стало ли сейчас понятнее, как устроен метод?
Цель немножко другая. В бейдинге-то мы хотим ансамблировать, и там, по-моему, если я правильно
помню, то там веса как-то подбираются так, чтобы мы правильную функцию заставляли более
правильно разделять под выборки, что ли. Справьте меня, я давно это, может быть, уже плохо помню,
давно не занимался. Здесь как бы задача в том, что мы управляем усредненный градиент в информации
о новой точке, но эта информация передается только через одно слагаемое. А все остальное как бы
оставить как было. И вот он как раз таки сходится, сейчас я запишу формулу, сходится, ой, жуткая
формула на самом деле. Ну, в общем, сходится какого-то единица на к, по тому же самому
функционалу, как мы от ожидания f, x-кат и средний, минус f со звездочкой. Вот, там, конечно, жуткие
всякие предположения на размер шага и прочее, но не важно. В общем, при некоторых предположениях
сходимость такая есть. Вот, теперь если соотносить то, что так, осталось пару минут, как раз я сейчас
наверное получится показать то, что у нас тут есть x, а что у нас тут y. Вот, ну, то есть понятно,
что вектора g, kt и kt, вот это вот, это собственно x. Может, ну, их мы от ожидания по x, который там
типа сэмплируется от вот этой штуки. Вот так, наверное, да. И это будет градиент. Ну, надеюсь,
что плюс-минус понятно. Если не очевидно сходу, то запишите определение того,
что такое мы от ожидания для кси равномерного 1 на n. Ну и, в общем, вы получите ровный градиент.
Так, сумма 1 на n, f и соответствующий f штрих и t. Вот, это x. В качестве y мы берем, получается,
выражение вида g, kt и kt минус вот эта самая сумма. Вот, проблема в том, что мы от ожидания этой
штуки уже не ноль. Вот, вы можете проверить почему. Но, что можно сказать про x и y по норме?
Норме это штука g получается kt и kt минус g минус 1 и k плюс, да, и плюс, ну, там 1 на n ушло.
Это самая сумма. Вот, и тут, значит, мы объединяем вот это в одно и это в одно и говорим, что вот
это стремится к нулю, потому что, ну, у нас градиент стабилизируется более-менее. То есть,
это градиент одной и той же функции, f и t, f и kt, только на разных итерациях. И приказ стремящегося к
бесконечности, мы верим, что эти градиенты начинают, ну, у нас аргумент меняется меньше и меньше,
поэтому у нас градиент начинается быть все ближе и ближе друг к другу. А вот эта штука стремится к нулю,
ну, потому что это просто градиент. Просто f штрих от x. Ну, градиент, в точке минимум, стремится к нулю,
все в порядке. Ну, или, точнее, поэтому это все вместе стремится к нулю, приказ стремится к бесконечности.
Вот. А раз у нас, типа, норма разности стремится к нулю, то и дисперсия тоже будет к нулю стремиться.
Вот. Что, в общем-то, мы и хотели. Понятно ли пояснение про то, как вот эта формула замечательная
соотносится с вот тем, что было вот здесь рассказано? Ставьте, пожалуйста, плюс, если понятно,
и минус, если не очень понятно. Ну, окей, вроде понятно. Это прекрасно. Ну, и, наверное,
наверное, последний, совсем последний кусочек. Вот. То есть там вот у САГа есть еще куча разных,
там, типа, это тринадцатый год. Там вот, хоть до последних, наверное, пары лет, там всякие САГ,
СВРГ, методы МИСО, может такие аббревиатуры быть услышать где-нибудь. Это все вот основано на
этой штуке. Вот. И я надеюсь, сразу отсюда понятно, почему эта штука на практике никогда не работает.
Кто скажет, почему? Ну, почему, например, вот мы там видели кучу адаптивных методов,
которые оперируют с разными шагами и прочим, а вот САГ почему-то там нигде не реализован. В чем его
основной как бы недостаток? Вся вот теория сходится прекрасно. Что-то как-то вариантов особо не видно.
Ну, хорошо, понятно. Смотрите, проблема в чем? Проблема в том, что вот тут вот надо хранить,
сколько n умножить на n векторов всегда. Простите, n векторов размерности n надо хранить. Это очень много.
Никогда никуда не поместится. Потому что вот эту сумму без хранения каждого вектора и вот эту
вот подмену вы никогда не посчитаете. То есть агрегированно хранить нельзя. Иначе непонятно,
что вы читаете. И вы не знаете заранее, какой индекс у вас будет просамплирован. Понятно,
в чем минусы? Вижу два плюса. Интересно. Тот же самый. То есть мы просамплировали индекс и мы из общей
суммы вычитаем предыдущий градиент, который для этого индекса был посчитан, добавляем новый
посчитанный на каты, ну на текущие террации. То есть это типа старый. Тут не k-1 на самом деле,
правильно написать. Типа тот, который был. Вот. А индексы у них, видите, и k одни и те же. То есть тут
как бы правильно позначит. То есть это посчитанный к k-1 террации. То есть как бы то, что там в нашем банке
векторов осталось, вот то мы используем. И как бы обмен происходит. Вот это меняется потом на вот
это. Привычение этой суммы. Вот. Да, хороший вопрос. Спасибо за уточнение. Окей. Так, ладно,
я хотел еще про то, как стахосичный свет оценивать, рассказать. Но, наверное, уже не успеваем. Да,
уже 26 минут. В общем, сегодня у нас была довольно насыщенная программа. Мы поговорили про СГД.
Показали, как он сходится для локального случая. Посмотрели на то, как дисперсия зависит от
размера бача. Посмотрели на адаптивные методы адаптивного поиска шага. И в конце на общую
стратегию мишени дисперсии. Вот. Надеюсь, было интересно и достаточно понятно. Вот. Следующий раз
у нас по плану метод Ньютон и квазинтонские методы. Вот. Более того, я немножко расписал даже
план на следующие занятия. Вот. В репетитории он лежит. Вот. Вкратце осталось не так много. И по
плану. Следующий раз квазинтонские методы. Через раз будут методы проекции градиента и там
максимальные методы. Потом полупределенной оптимизации. В конце закончим занятием про то,
как устроены пакеты решения задачи по оптимизации и что вообще там возможно и как они работают. Вот.
Это, короче говоря, еще где-то четыре занятия. И я думаю, как раз к середине декабря мы
благополучно закончим. У вас там уже сессия начнется. Вот. И я думаю, все успешно все сдадут. Вот такой
план. Вроде еще четыре лекции осталось. Я надеюсь, ничего не помешает их провести в таком же... Надеюсь,
спроса людей будет больше. Может быть. Вот. И будет поинтереснее. Ладно. Всем большой
спасибо за внимание. Если какие-то вопросы появляются, пишите в чат. Я буду на них отвечать.
Вот. Все записи я буду выкладывать, как обычно. Слайды. Я не уверен, что буду выкладывать.
По этой лекции там надо их сильно переделывать. В общем, я буду сейчас смотреть.

Редактор субтитров Е.Воинова Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
Корректор А.Кулакова
единичной длины, а здесь а-b на х, длина лектора а-b умножить на х.
То есть это спектральная норма, это операторная норма, но так уж оказывается, она равна старшему шигляновому числу.
Теперь, вот мы максимизируем по некоторому множеству, если это единичная сфера.
Если мы множество уменьшим, что произойдет с максимумом?
А? Ну увеличится. Правильно.
А уменьшить, я вот что сделаю, сейчас напишу, как я хочу уменьшить.
Только по-такому, только такие х рассматриваю, такие, чтобы b на х ясно.
Ну, значит, максимум уменьшится.
Ну, зачем я это делаю? А чтобы b исчезло?
Я могу произвольно уменьшить множество и максимум уменьшится, правильно?
Я хочу, чтобы b исчезло. Я в точности сказал свою задумку.
Я хочу, чтобы здесь не было... Нет, почему можно? Ещё раз. Можно взять любое меньшее множество.
Да, любое можно взять. Вот я и возьму некоторые поменьше, но с идеей, чтобы b у меня исчезло.
Вот здесь. Нет здесь b. Множество меньше.
А что это за множество такое? Вот сейчас мы тоже немножко вспоминаем.
Вот вы знаете, как это называется. Это называется ядро матрицы B.
Ядро матрицы B. И размерность этого ядра, размерность, то есть ядро матрицы B от лейной подпространства.
Ну, это уж наверняка каждый понимает почему. Подпространство, значит, там можно говорить о размерности.
Размерность ядра, кстати, в теории матрицы называется дефектом матрицы.
Ранг матрицы – это размерность образа этой матрицы. Дефект – это размерность ядра.
Вот, если вы сложите дефекты ранг, ну, почти что один из основных результатов, связанных с системой нелинейной алгебритики.
Что получится? Число столбцов матрицы.
Значит, размерность ядра – это есть n минус ранг.
А ранг у нас меньше или равен k, так? Значит, это больше или равно, чем n минус k. Согласно, да?
N минус k. Снизу оценка на размерность.
Вот, а теперь я хочу взять еще одно подпространство.
Вот с бухты Барахты послепит еще одно подпространство.
А это будет линейная оболочка. Тоже знакомая, да? Линейная оболочка, натянутая в систему векторов. На какую систему векторов?
У нас сингулярные векторы есть. Вот я на них и натяну. Возьму k плюс один сингулярный вектор и натяну на них линейную оболочку.
Это будет подпространство m. Ну, а ядро пусть это будет l. Два подпространства – l и m.
Размерность l не меньше, чем n минус k. Размерность m вы мне сейчас посчитаете, да?
Чему равна размерность m? Молодцы! Правильно?
А теперь еще один факт линейной алгебры. Знаете, не знаете? Вот такая есть арема чудесная.
Размерность суммы двух линейных подпространств. Ну, сумма как устроена? Это всевозможные суммы векторов из l и из m. Это будет тоже линейное подпространство.
И размерность можно вычислять так. Надо вычислить размерность l, прибавить размерность m и вычесть размерность пересечения.
Ну, пересечение, очевидно, будет тоже линейным подпространством. Вот такая формула. У нее даже есть имя. Это формула Грасмана.
Не знаю, встречалось, не встречалось. Я вот когда объясняю эту формулу, я люблю говорить о том, как я бы объяснял эту формулу выпускникам детского сада.
На пальцах совершенно. В том, что я сейчас скажу, фактически доказательство есть. А что понятно выпускникам детского сада?
Вот у вас две кучки яблок. Одна кучка и еще одна. Но они пересекают.
Знаешь, сколько всего яблок? Число яблок в одной кучке, во второй минус число яблок в пересечении.
Но здесь, когда вы будете эту теорему Грасмана доказывать, роль яблока играют векторы базисов. Векторы базисов в l, в пересечение, в l и в m.
То есть так можно собрать их. И вот то, что в пересечение, это будут вот эти общие яблоки.
Ну, примерно так. Хорошо, что знакомы эти ариамы. Вот давайте мы ее применим.
Давайте мы ее применим. Только применим вот как мы. l и m это те самые l и m, которые у нас возникли.
Вот посчитаем размерность пересечения. Значит, это есть размерность l. Размерность l у нас снизу оценена как m-k.
Плюс размерность m, а это есть k плюс 1. Вот минус размерность суммы.
Ну, размерность суммы не больше, чем размерность всего пространства. А это n, да?
Значит, минус. Смело можем вычесть n. И что здесь получилось в результате?
1. А что это значит? А это значит, что в пересечении под пространство m имеется не нулевой вектор.
Это не нулевой подпространство. Вообще есть нулевой вектор z.
Ну, всегда можно считать, что z имеет единичную длину. Такой вектор z.
Вот следствие теоремы Грасмана. И выбора подпространства l и m.
Это линейная оболочка. Ну, можно с pen написать. Ну, по-русски.
Мог бы писать рукописную l, но рукописную l у многих вызывает вопросы, что это такое.
Значит, вот z появился. Есть такой вектор z. А здесь максимум.
А z принадлежит к ядру матрицы b? Принадлежит. Он и l принадлежит, и m? Ну, и l принадлежит.
Значит, я могу написать. Правильно? Максимум же он больше, чем значение на каком-то отдельном векторе.
З имеет единичную длину, аж вот так. Ну, а теперь смотрите ход мыслей, как продолжается.
Ну, не по z, конечно.
Ну, написал очевидную вещь. Правильно? z принадлежит m.
И значит, если мы по большему, по множеству ям будем минимизировать, то получим не больше, чем вот это значение.
Правильно? А вот этот минимум легко вычисляется. Остается вот этот минимум вычислять. Сейчас мы это сделаем.
Ну, что надо сделать? Надо... x принадлежит m. Ну, что это значит? Значит, x раскладывается по каким векторам?
По первым каплю с единичкой векторам u. Ну, с какими-то коэффициентами. Вот, дальше подействовали батрицы a на этот вектор x умножили.
А что будет?
Значит, x а, а здесь а. А что такое а у а?
А это, помните, это sigma alpha умножить на v alpha.
Ну, вектор v alpha образует ортонормированную систему. Значит, длина этого вектора вычисляется как корень квадратный из суммы коэффициентов в квадрате.
Что?
Как фиксировано?
Нет, ну, sigma, если sigma больше, чем r plus 1, то sigma равно 0. Если sigma больше, то r plus 1.
Ну, мы заведомо k выбираем не больше, чем r, правда же? Иначе тривиальная будет задача.
Значит, длину этого вектора легко посчитать. Это из корень квадратный из суммы вот этих вот координат в квадрате.
Правильно?
Ну, что же это за сумма такая?
Здесь можно sigma alpha вытащить, а здесь sigma вообще положительные числа.
А вот это координаты, вообще говоря, комплексные.
Ну, вот так вот. И самая большая sigma здесь какая?
Sigma 1, а самая маленькая?
Значит, совершенно очевидно, что мы можем все эти sigma на sigma k plus 1 заменить и получим меньше.
А вот это чему равно?
Это длина вектора х, а длина вектора х равна 1. То есть это есть sigma k plus 1.
Ну, что произошло?
Ну, что произошло?
Значит, мы доказали, что какую бы матрицу B, какую бы матрицу B ранка, которая не выше ка, мы взяли, мы не взяли,
спектральная норма матрицы A-B больше или равна, чем sigma k plus 1?
Теорема заказана.
Ну, правда, часть теоремы та, которая связана со спектральной нормой.
Ну, что, получили удовольствие?
Мне, конечно, вполне такие элегантные рассуждения.
Ну, есть еще вторая часть.
С нормой фробениуса.
Давай это ее попробуем тоже.
Спасибо.
Да.
Ну, вот эти вот, вот эти можно.
А действительно, жидкость.
Нет, не все. Формула фотоставки, вот эта вот.
Мы сейчас доказательство теоремы пока продолжаем.
А вот это вот, в утверждении теоремы второе равно sigma k plus 1.
Нет, это выше, выше.
Это мы получили за прошлой лекции.
Это вам как упражнение, однако, предлагалось.
Ну, это факт.
Значит, тут, как это получать?
Ну, во-первых, спектральная норма равняется старшему сингулярному числу.
Ну, вот, вам тоже надо было бы упражняться. Почему, да?
Если вы поняли уже, что равно.
Значит, вот есть а-к.
А для а-к сингулярное это разложение легко написать.
Там будут какие сингулярные числа?
Sigma k plus 1 старше, sigma k plus 2, и sigma k plus 1 старше.
Значит, ответ sigma k plus 1.
Вот и все.
Ну, фактически, здесь надо опираться на то, что при умножении на унитарную матрицу
спектральная норма не меняется. Умножать ее слева или справа.
Ну, кстати, норма Фробениуса тоже не меняется.
И это дает возможность вычислить норму Фробениуса для матрица.
Просто надо норму от sigma посчитать.
Норму Фробениусу матрицу.
Спасибо большое.
Итак, вот вторая часть.
Вторая часть.
Ну, здесь мы берем по-прежнему производную матрицу B.
Только же ранг меньше или равен k.
Вот и пишем.
А минус B нас интересует.
Норма Фробениуса.
Помните, что такое норма Фробениуса?
Корень квадратной суммы модулей элементов в квадрате.
И только что мы вспоминали, что если мы умножим матрицу на произвольную унитарную матрицу, то норма Фробениуса не изменится.
Вспоминали?
Давайте возьмем произвольную унитарную матрицу X.
И тогда я пишу вот так вот.
Ну, верно, да?
А теперь выберем X с умом.
С умом.
Выберем унитарную матрицу X с некоторым специальным образом.
Ну, у меня та же навязчивая идея здесь убрать B.
Избавиться от B.
Ну, раньше тоже избавлялись от B.
И сейчас давайте.
Так, ну давайте так вот.
Я выберу.
Вот есть такое ядро матрицы B.
Размерность этого ядра.
Ну, давайте я как-нибудь обозначу размерность этого ядра.
Через буквку какую-нибудь.
Нет, M занято.
Буквок нет никаких.
Ну, что? T.
И T у нас снизу имеет оценку.
Я их не понимаю.
Вот.
Ну, что мы сделаем?
А мы выберем вот в этом пространстве.
Артонормированный базис.
Из векторов X1.
И так далее, Xt.
Значит, это базис.
И достроим этот базис.
До артонормированного базиса.
Всего инмерного пространства.
И получим матрицу X.
Собрав в нее столбцы, которые мы построили.
Вот эти векторы X1, X2 и так далее.
Получится унитарная матрица X.
Вот. Ну, это вот унитарная матрица X.
Она устроена даже так вот.
Здесь матрица, давайте Xt напишем.
Xt содержит только вот t векторов.
От первого до вектора t.
Xt.
Значит, вот это первые столбцы.
И если там...
Давайте здесь сим.
X с домиком это оставшийся вектор.
Понятно. То есть вот два блока.
Здесь t векторов, здесь n минус t векторов остался.
Вот. И, конечно же,
конечно же,
я теперь могу вот так написать.
Ну, посмотрите внимательно.
Если вы мне скажете, о, это очевидно, мы продолжим.
Не очевидно, конечно.
Ну, раз кто-то говорит не очевидно,
придется доказать.
Ну, что такое норма фробениуса?
Взяли длину каждого столбца в квадрате и все сложили.
Вот здесь мы...
Вот t столбцов их сложили.
Это плюс первый, это плюс первый.
Ну, теперь стало очевидно.
А?
Вот.
Ну, объясните.
Он не понимает, что такое норма фробениуса.
Вот у вас столбцы матрицы,
норма фробениуса, это сумма,
квадрат нормы фробениуса матрицы,
это сумма квадратов длины ее столбцов.
Ну, если вы возьмете часть столбцов,
эта сумма уменьшится.
Вот так вот.
Очевидно, что вы там голову оборачиваете.
А b-то теперь исчезло.
Правда? Потому что b на xt это новость.
Мы же взяли вектор x1, xt в ядре матрицы b.
Вот так вот.
Вот так вот.
Столбцы матрицы xt, первые t столбцов,
они в ядре матрицы b.
Мы так выбирали.
Вот. Ну, что это такое написано?
Можно вот так написать.
Это есть след?
Какой матрице?
a xt сопряженная,
а xt,
правильно?
Ну, только корень еще.
Ну, давайте, посмотрите.
Опять кто-то скажет очевидно, кто-то скажет нет.
Ну, да.
Нет, ну, не очевидно.
Ну, вообще, что такое норма Фрабениуса?
То есть, общая формула такая,
квадрат нормы Фрабениуса это след
вот такой матрицы.
Это сумма квадратов элементов матрицы a.
Так это и есть норма Фрабениуса в квадрате.
То есть, вот этот след,
как вы правильно сказали,
это сумма квадратов всех элементов.
А, точно.
Это сумма Фрабениуса по определению.
Так, теперь вот смотрим на этот самый.
Ну, понятное, преобразование очевидное.
Вот такое.
Это очевидно, да?
Скопки просто поставил по-другому.
Вот.
Вот здесь
замечательная матрица возникла.
Замечательная матрица
а со звездой на а.
Что вам об этой матрице
можете хорошего сказать?
Само сопряженная
или, как говорят Эрмитова,
молодцы.
А еще, знаете,
еще не отрицательно определенная.
Помните, что это такое?
То есть, это само сопряженная
или Эрмитовая, и к тому же
еще не отрицательно определенная матрица.
А что вы знаете о собственных значениях
Эрмитовой матрицы?
Нет.
Они вещественные.
А если
Эрмитовая матрица не отрицательно
определена,
как в нашем случае, то они не отрицательны.
Ладно.
Значит, вещественные собственные значения.
Вот.
Ну а теперь
давайте-ка я
вместо вот этой матрицы
рассмотрю вот такую.
Рассмотрю вот такую.
Это, знаете,
буквкой H обозначу.
Эта матрица по-прежнему будет
Эрмитовой, не отрицательно определенной.
Правильно?
Х-то унитарные матрицы.
Значит, вот эта матрица H,
как собственные значения
матрицы H
связаны с собственными значениями
матрицы Эрмитовой?
Газаны с собственными значениями матрицы
со звездой умножена.
Вот он контрольный,
это вопрос.
Правильно. Совпадает. Почему?
Потому что
х со звездой это х-1.
То есть это есть подобные матрицы,
а подобные матрицы имеют одинаковые характеристические значения.
Так?
Совпадает.
Значит, вот H,
собственные значения матрицы H,
это квадраты сингулярных чисел
нашей матрицы А.
Правильно?
Хорошо.
А теперь
ну, ведь
х
теперь давайте вот так вот напишу H.
У подобных матриц совпадает.
То есть
подобные матрицы
имеют одинаковые собственности значения,
а х со звездой это х-1,
поскольку их сумметарная матрица.
То есть это подобие.
А теперь я вот умножаю
вот это х,
я могу х так записать.
Ну а здесь вот так вот.
Правильно?
Значит, естественно
результат тоже
некоторую блочную форму приобретает.
Правильно.
Нет, нет.
Это правильно.
Это вы правильно сказали.
Я не хочу, я хочу со звездой.
Вот, и здесь вот,
и вот матрица, которая здесь
возникает, это в точности вот эта вот матрица.
Правильно?
Ну давайте я ее через Ht обозначу.
Значит, вот эта матрица,
вот эта матрица, это H есть.
А вот этот блок,
как он получается?
хt со звездой
на матрицу А со звездой А на хt.
Ну а здесь какие-то будут блоки,
я их вычислять не буду.
Вот эта матрица,
Эрмитова матрица,
а Ht,
это есть ее
под матрица.
Причем под матрица, расположенная
в левом верхнем углу.
Термин есть такой,
если под матрица, расположенная в левом верхнем углу,
то она называется ведущая.
Давайте договоримся,
мы еще может быть
поиспользуем этот термин.
Ведущая под матрицу.
Значит,
вот здесь, вот эта величина,
что это такое?
Это сумма квадратов
собственных значений
матрицы Ht.
Просто сумма собственных значений
матрицы Ht.
Сумма квадратов сингулярных чисел,
для матрицы А на хt.
А для матрицы Ht
это просто сумма ее собственных значений.
А сумма собственных значений
матрицы Ht, это есть в точности
фробениусовая норма матрицы Ht.
Нет, нет, нет.
Еще раз.
Вот это что такое?
Это норма фробениуса.
Нет, ну это след,
но это в результате вот это...
Вы же знаете, что это норма фробениуса
вот такой матрицы.
А, ну вот оно и написано.
Вот это.
Норма фробениуса.
А норма фробениуса
это корень квадрата
и сумма квадратов сингулярных чисел.
А сингулярные числа
в квадрате
это собственные значения
вот этой матрицы,
которая через Ht обозначена.
Значит, сумма собственных значений
матрицы Ht
это в точности квадрат
вот этой нормы.
Значит, сумма
ну сумма собственных значений
это и след.
То есть след
матрицы Ht, это сумма ее собственных значений.
И это есть вот
квадрат
вот этой самой нормы фробениуса.
Вот, а
след матрицы H
это
квадрат фробениусовой нормы
исходной матрицы A.
Ну и теперь вопрос.
Знаете вы это?
Из вашего курса
или этого не было.
Как связаны
собственные значения
ведущей под матрицы
с собственными значениями
всей матрицы
в случае, когда
вот эта матрица вся является
эрмитовой.
Какая здесь связь?
Значит, эта связь, это одно
из замечательных совершенно
наблюдений, известных в теории
матрицы.
Молчание говорит, что
не знаете.
Было, ну смотрите.
Не-не-не.
Вот сейчас.
Это вот могу
здесь я могу
уже стирать.
Значит, вот мы имеем
возможность
вспомнить
кое-что из
теории матрицы.
Еще.
Вот смотрите.
Вот в нашей теории
два утверждения.
И мы в общем-то разные
инструменты применяем
из теории матрицы.
Значит, есть
замечательный раздел
в матричном анализе
теории эрмитовых матриц.
В этом самом спектральной теории
в общем-то речь о
собственных значениях эрмитовых матриц.
Вот у них столько интересных
свойств.
Масса.
Вот одно из свойств
это связь есть
с эрмитовой квадратичными
формами.
Ну наверняка об этой связи вам
что-то говорили.
И вот есть такое понятие для эрмитовой матрицы
понятие инерции.
Ну эрмитовая матрица, она имеет вещественные собственные значения.
А что такое инерция?
Это такая тройка.
Число положительных собственных значений,
число отрицательных и число нулевых.
И есть
замечательный закон инерции.
Который утверждает, что
инерция сохраняется
при переходе
от одной эрмитовой матрицы
к
эрмитово-конкурентной матрице.
Ну вот такое понятие
конкуренции.
Значит вот конкуренция.
Значит вот есть матрица А,
а вы переходите
к матрице вот такой вот.
И Х здесь просто невыраженная матрица,
не обязательно унитарная.
Если унитарная, то говорят, что
это унитарная конкуренция.
А вот здесь просто конкуренция.
Вот закон инерции говорит,
что конкурентная эрмитовая матрица
имеет одну и ту же
инерцию.
А если инерции эрмитовых матриц
одинаковые, то эти матрицы конкурентны.
Вот такая есть теорема.
Эта теорема на самом деле
равносильна
еще некоторым чудесным утверждением.
Некоторым чудесным утверждением
есть так называемые
формулы
Курнта-Фишера
или теорема Курнта-Фишера
авариационных свойствах
собственных значений.
Ну, видимо, не слышали об этом.
Ну, бог с ним. Ладно, оставим.
Еще одна теорема.
Эта теорема
о так называемых соотношениях разделения.
Вот удивительно важная штука.
Соотношение разделения.
По существу это равносильно
к закону инерции.
То есть из законной инерции можно это вывести.
Вот я что.
Из законной инерции можно вывести
соотношение разделения. Правда, можно
и другими путями прийти к соотношению
разделения.
О чем это?
Вот если у вас есть Эрмитова матрица H, то есть H-Эрмитова, и в ней вырезали ведущую под матрицу порядка n-1,
то давайте я буду писать лямбда и T для матрицы H. Договоримся, что нумеруем лямбда по невозрастанию.
Сначала самое большое, но это счастливые числа, их можно упорядочить. Могут быть отрицательные. Самое большое, потом поменьше, поменьше.
И n-ное самое маленькое. Собственные значения. Значит вот лямбда и T больше или равно, чем лямбда и T для H, для подматрицы.
А вот это собственное значение и T для подматрицы, оно снизу тоже оценивается. И плюс первым собственным значением.
Матрица H. И вот это я со отношениями разбираюсь. Собственные значения. Можно их упорядочить как угодно.
Предполагаем, что собственные значения матрицы H занумерованы вот таким образом.
Ну естественно, надо нумерацию фиксировать. Не для любой нумерации, это правда. Вот такая у нумерации.
И аналогичным образом нумеруем собственные значения, ведущие под матрице, порядок, который на единичку меньше.
Ну понятно, что вы должны вот единицы, да, n без единиц.
Ну, кстати, добавить надо, конечно, и такое, что лямбда M минус первое, для подматрицы снизу оценивается, так и оценивается.
Самым младшим собственным значением большой матрицы.
Вот если вы нарисуете на вещественной оси собственные значения, только нарисовал все неправильно, нумеруя они так.
Вот вопрос, как нумеруем. Здесь лямбда 1, здесь лямбда 2.
То где будут собственные значения ведущие под матрицы между? Оттого и название соотношения и разделения.
То есть собственные значения ведущие под матрицы разделяются собственными значениями большой матрицы.
Но при этом порядок ведущий под матрицы ровно на единичку меньше порядка исходной матрицы.
Это есть один из замечательных фактов теории матриц.
Соотношение и разделение для собственных значений эрбитовой матрицы и ее ведущей под матрицы.
Изучали, не изучали?
Ну, надо этот пробел восполнять. Посмотрите, пожалуйста.
Практически в любом учебнике по теории матрицы это можно, конечно, прочитать.
Ну, вот есть книжка у меня «Маточный анализ и линейный алгебр».
Я, правда, подготовил переработанную версию.
Ну, есть такая. Можно там прочитать, но, в принципе, это знаменитый факт.
Можно взять какую-нибудь книжку по матричному анализу.
Хорнед Джонсон, например, есть такой.
Первый том на русском языке даже есть.
То есть это такая вот фундаментальная вещь.
И полезная, кстати. Вычислить-то науку чрезвычайно полезная.
А вот то, что я вам говорю, я говорю, а я уже сказал это.
А на самом деле, эту теорему можно вывести из закона инерции.
И более того, в закон инерции эта теорема, по существу, это эквивалентное утверждение.
Но если вы с этим разберётесь, вы всё-таки молодцы.
Это один из ключевых утверждений теории Ермитовых матриц.
Это соотношение-разделение.
И вот эти соотношения-разделения мы сейчас используем.
Используем.
Ну, кстати говоря, я могу, как я говорю, некоторые главы переработанной книжки я помещаю в папочку.
Давайте я помещу, вы можете там почитать.
Это будет глава с самым большим номером.
Там сейчас какое-то количество глав есть.
Ну, я думаю, что тут-то вы справитесь, если у вас желание будет найти, там вы уже найдёте.
Нет, подождите, подождите.
Мы должны теорему-то до конца довести.
Замечательная теорема ещё тем, что повод даёт вспомнить классику, причём нужную классику,
которая даже оказалась ей в чём-то незнакомой.
Вот, а у нас здесь ведущий под матрицу, только порядок её не n-1, а t.
Да?
Ну, смотрите как, мы можем же последовательно делать.
Вот эти соотношения и разделения вначале применяем для подматрицы n-1, потом n-2 порядка, потом и тогда t дойдём.
Правильно?
И в результате вот для этой матрицы h-t мы получим вот такие соотношения.
Только здесь будет, с конца надо отсчитывать, но я хочу вот что сказать.
Лямбда t будет больше или равно, чем лямбда n, лямбда t-1 больше, чем лямбда n-1 от h, ну и так далее.
Вот последовательно, применяя соотношения и разделения для ведущих под матрицу порядка трёх на единичку,
мы получим некоторую связь соотношения матрицы h-t и матрицы h.
И вот эту связь я написал, какую получим в результате.
А теперь можно и суммы сравнить.
Да?
Вот отсюда вытекает, отсюда вытекает ровно то, что нам нужно.
Вот сумма вот этих, лямбда t, не лямбда t, сумма всех соотношений,
давайте, ладно, напишу, лямбда 1, h-t плюс и так далее, лямбда t, h-t, вот этот самый след,
больше или равен, ну а здесь будет лямбда n-t плюс 1, наверное, от h, плюс и так далее.
Нет, а почему же?
Нет, что такое ты?
На самом деле...
А, нет, всё правильно.
Здесь всё правильно, что меня смутило, а не должно было смущать.
А вот это сумма.
Ну, смотрим на нашу оценку.
Ну, t у нас больше или равен n-t?
Как?
Что вы хотели спросить?
Нет, то есть вот это вот...
Вот что это за собственные значения?
Вот сколько?
А, здесь мы от сяшка, нет, здесь вы правильно говорите, h.
Вот, и здесь есть в этой сумме не меньше, чем n-k слагаемых.
Не меньше, чем n-k слагаемых.
Вот в чём.
n-k слагаемых, а это значит, какие собственные значения?
От k, плюс первого, до n-ого.
Это будет оценка снизу.
И оценка снизу, она вот такая.
От k, плюс первого, до n-ого.
Что и требовалось доказать.
То есть вот мы снизу получили...
Ну, если бы в квадрате t лямбда как раз есть.
То есть мы как раз и получили сумму...
Сумму квадратов сингулярных чисел от k, плюс первого, до n-ого.
Да.
Вы не доказаете, что у нас, вот, наша норма по Венесу будет больше равна, чем вот это число.
А как доказать, что это минимум, то, что меньше, чем нигде?
Значит, мы с вами доказали, что, какую бы матрицу B не взяли,
ранг, который не больше, чем k.
Вот оценочка такая.
Всё.
А если вы возьмёте матрицу Аккаты, то будет достигаться эта оценка.
Точно что.
Посчитали минимум.
Нет, ты определенно, однозначно.
Это размерность ядра матрицы B.
Чего не всегда верно?
Нет.
Значит, здесь для любой ведущей под матрицу вот эти соотношения справедливы.
Следствие соотношений разделено.
Минимум больше или равен?
Минимум больше или равен, чем сумма.
Правильно, но не минимум.
Мы доказали, что для любой матрицы B конкретной будет больше или равно, чем сумма.
А если мы по B будем минимизировать, значит, минимум тоже будет больше, чем сумма.
Правильно.
Но почему он равен сумме?
Потому что для некоторой матрицы B эта величина достигается.
Если вы возьмёте B равные Аккаты, то вы получите в точности то, что у нас возникло как оценка.
Вот такое вот здесь рассуждение.
Приглашаю вас его обдумать, обдумать.
И к тому же обязательно познакомьтесь по основательной соотношениям разделения для армиттовых матриц.
Это вам такое задание домашнее.
Мы ещё, я думаю, будут моменты, когда соотношения разделения понадобятся нам.
И точно и в других науках, связанных с вычислениями, это такие очень важные.
Может быть, вы слышали об артагональных многочленах что-нибудь?
Есть тоже наука об артагональных многочленах, и они в эти матрицы огромную роль играют.
У артагональных многочленов, скажем, наотрезки корни вещественные.
И для корней имеют место соотношение разделения.
Такие же, как для армиттовых матриц и подматриц.
И более того, эти соотношения получаются как следствие соотношения разделения для армиттовых матриц и подматриц.
Ну, это так вот.
Если когда-нибудь столкнётесь, а кто-нибудь обязательно столкнётся с применением артагональных многочленов,
вот можете вспомнить о соотношениях разделения, которые мы сегодня обсуждаем.
Значит, ну всё, точка теоремы доказана.
Но я бы не сказал, что это уж такая простая теорема.
Вот, но она связана, она так вот базируется на очень примечательных теоремах из матричного анализа.
И объясняет, и объясняет.
Вот, видите, задачу о приближении малого ранга, по крайней мере, при использовании двух вот этих норм чудесных.
Вот здесь сингулярное разложение даёт вам оптимальные приближения.
То есть, если вы умеете вычислять сингулярные разложения, а это можно делать.
Ну, не прямо сейчас, мы поговорим ещё о том, как вычислять сингулярные разложения.
То вы можете решать задачи об оптимальных приближениях матрицы матрицами заданного ранга.
Какое у нас оптимальное?
Оно окатое.
Окатое?
Обрезанное сингулярное разложение.
Как старших членов? Берёте?
И это то же самое в норме спектральной, то же самое в норме фробени.
Обивительно, да?
По грешности?
Нет.
Вы затрагиваете очень интересный и нетривиальный вопрос.
Значит, вопрос, вот какой.
Если мы здесь вот двоечку уберём, то есть, вот для каких норм это останется?
Да?
Для каких норм это останется?
Мы знаем, для спектральной вермы, для фробениуса вермы, а ещё какие-нибудь есть.
Значит, утверждаю, для произвольной нормы это неверно.
Для произвольной нормы здесь теорему написать не получается.
Но есть большой класс норм, для которых теорема обобщается.
И это есть нетривиальный результат.
Причём так существенно более нетривиальный результат, чем то, что мы получили.
Это так называемую унитарно-инвариантная норма.
То есть, если норма унитарно-инвариантная, то есть, если норма матрицы не меняется при умножении справа и слева на любую унитарную матрицу,
такие нормы называются, то справедливая теорема – вот это самое.
Здесь считать как-то норму А-Ак, это будет зависеть от того, какая конкретно унитарно-инвариантная норма.
Но речь о вычислении нормы диагональной матрицы.
Но вот эта теорема имеет место.
Это теорема Мирского.
Связанная, поскольку мы на физ.техе находимся, можно вспомнить, что довольно много лет преподавал Лицкий на физ.техе.
И с одной из теорем Лицкого связана эта теорема Мирского.
Это такой глубокий результат, требующий довольно больших усилий.
В матричном анализе такая изюминка замечательная.
Ну, если говорить о практике, ну, конечно, спектральная норма, норма Фрагениуса – это самая популярная унитарная норма.
Ну, будем двигаться дальше.
Да, будем двигаться дальше.
В каком направлении?
Ну, давайте все-таки начнем разговор о конкретных вычислительных задачах, о классах вычислительных задач,
относящихся к алгебе, прежде всего к линейной алгебре.
Ну, какие задачи? Все-таки некоторую классику изучать тоже нужно нельзя.
Я пытаюсь вам рассказать о всех свежих вещах. Буду пытаться.
Даже относительно недавних. Я обязательно сделаю.
То есть то, что вы можете услышать в этом курсе, здесь будет присутствовать то, что вы нигде не можете услышать.
Нет еще таких курсов.
Но соответствующие вещи уже проникли активно в жизнь.
В научную и даже в инженерную.
И об этом я буду рассказывать.
Но все-таки есть и такие классические вещи.
Вот есть задача, задача важная.
Решать систему линейных алгебрехических уровней.
Вот мы эту задачу с вами обсудим.
Алгоритмы для решения системы линейных алгебрехических уравнений.
И не такая это маленькая область, как может показаться.
Вот знаете, можно подумать, ну что там с системами линейных уравнений?
Школьник может решить, правда?
И это правда.
И это правда.
Но довольно легко возникают ситуации, с которыми школьник может и не справиться.
Допустим, очень большой мат.
Очень большой мат.
Или, так сказать, даже матцы, которые можно получить в принципе.
То есть можно вычислить их элементы, вроде бы есть формула.
Но размер настолько большой, что вычитать нельзя, поскольку негде запомнить.
Нормально.
Ну, кстати, в инженерных задачах, да и в научных очень просто огромные системы возникают.
И то, что может делать школьник, в принципе, можно бы на компьютере записать.
Метод исключения.
Но он требует огромных ресурсов, и задачу можно не решать.
То есть вопросов здесь на самом деле таких совсем не школьных много.
Но многие вещи можно сделать, как бы так, из термяжных соображений.
Поэтому много.
Вот если просто небольшая система, то в инженерных задачах, понятно, исключаешь неизвестный.
И придешь к какому-то алгоритму.
Обычно говорят, что нет гауса, как принято говорить.
Но если вы хотите анализировать этот алгоритм, вот здесь теория мат. уже нужна.
Если вы хотите придумать более эффективный алгоритм, здесь уже трудно обойтись без теории матрицы.
Значит, вот мы будем обсуждать эту задачу, систему линейных алгоритмических параметров.
Это первое.
В том числе и с разными матрицами, которые имеют специальное происхождение.
И какую-то специфику, что что-то о матрицах известно дополнительное.
То есть это не есть просто самые общие матрицы.
А в приложениях, к счастью, обычно возникают матрицы, обладающие какими-то характерными для приложения свойствами.
То есть вы, конечно, можете сказать, вот есть классическая линейная алгебра, есть методы для матриц общего вида, их можно применять.
Но применение общих методов часто оказывается либо невозможным, либо неэффективным.
А значит, увы, приходится, ну, может быть, не увы, приходится все-таки думать и пытаться понять то, чего прямо не сказано.
А чего прямо не сказано? Не сказано в точности, а какие вот особые свойства связаны с этим приложением.
Вот надо эти свойства еще извлекать и думать, а не получится ли эти свойства применить, как-то превратить их в эффективные методы вычисления.
Вот это есть, так сказать, жизнь современной вычислительной линейной алгебры.
Но помимо задачи, наверное, просто с решением системы.
Есть, конечно, задачи, типа, для меньших квадратов, уже такие, как мессонные задачи.
Задачи приближений. Вот мы о сингулярном разложении или разговоре, как об одном инструменте связанном с задачами приближения.
Есть спектральные задачи вычислять спектры.
Те же сингулярные разложения находят собственные значения, собственные векторы.
Вот об этих задачах, конечно, надо будет поговорить.
Вот и некоторые. И вот то, что совершенно новое, что в нашем курсе будет, это теория и алгоритмы совершенно современные,
в которых векторы и матрицы рассматриваются как тензоры.
Это очень популярным стало.
Здесь вот мы в институте вычислительной математики смогли как-то продвинуться в этом направлении.
Вот и придумать удалось некоторые такие новые на тот момент.
Это в 2009 году было такие тензорные разложения.
Как или иначе, я пока о деталях не говорю, мы будем это обсуждать.
Это связано, это форма представления данных.
То есть матрицы, векторов, некие специальные разложения, с помощью которых можно представлять и векторы, и матрицы.
Вот и все это связано с той самой идеей разделения переменных, которую мы обсуждали в своем сингулярном разложении, мы еще будем обсуждать.
Значит, вот примерно такой вот план на будущее.
Сегодня, по-моему, время у нас с вами вышло.

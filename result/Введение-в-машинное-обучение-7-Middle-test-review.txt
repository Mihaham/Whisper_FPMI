Ну что, коллеги, экран видно?
Ну тогда, наверное, поехали, сука, в более удобном положении
работать не умеет, ну тогда я буду стоять где-то вот
здесь.
Итак, предположение ID независимо одинаково распределённое
применяется обычно к наблюдениям, ну да, абсолютно верно,
к признакам.
Так, кто может обосновать, почему к признакам оно
применяется?
Есть такие?
Признакам, очевидно, нет.
Так.
Признаки полцы, разные столбцы, они независимо одинаково
распределённые.
Смотрите, контрпример.
У меня есть один признак, не знаю там, котика или
собачка, второй признак среднее количество калорий,
употребляемых в сутки.
Одно непрерывная величина, другое дискретное, бинарное.
Кажется, они точно не могут быть одинаково распределённые.
Правильно?
Собственно, независимость признаков это то, о чём
мы всегда мечтаем, потому что у нас тогда модель гораздо
лучше работать будет, но при этом мы именно что мечтаем.
А уж одинаковая распределённость это что-то крайне странное,
потому что если у нас все признаки пришли из одного
распределения, то чё-то явно не так идёт, у нас признаки
могут быть вообще разного типа, разные могут быть
дискретные и непрерывные, упорядоченные и упорядоченные
и так далее.
Так что нет, конечно, признаками это не относится.
Тут все согласны, правильно?
Да, они могут быть, но мы как правило этого не требуем
от наших данных абсолютно.
То есть такое может случиться, но в общем случае условно
ID от данных мы хотим примерно всегда, потому что иначе
у нас статистика, она нам не гарантирует, что у нас
что-то там сойдётся.
Если у нас два распределения, одна модель с ними работать
будет далеко не всегда.
А если у нас данные ID, мы спокойны.
Вот, значением целевой переменной, ну тут по минимуму,
что?
Предположение к байсиксам классификатору применяется
лишь частично, мы предполагаем, что признаки независимы,
при условии таргета причём.
А вот то, что они одинаково распределены, нет, мы же
с вами явно параметризовали сами и выбрали, какое распределение
будет.
Причём можем выбрать, что они из разных нормальных
распределений или вообще разные распределения на
них поставить.
Один может быть из поассона, другой биномиальный, третий
нормальный.
Пожалуйста.
И вот ошибки в данных.
Тут ошибка на самом деле в форме, потому что здесь
должны были быть галки вместо единственного варианта
ответа, потому что ошибки-то у нас тоже могут быть случайными
величинами и если они случайные величины, то очень хочется,
чтобы они были независимо одинаково распределённые.
Независимые краски, мы с вами помните теориям Гаусс
Маркова?
Ну там говорили, что у них корреляция должна быть
равна нулю.
На всякий случай.
Нескоррелированность, это более сильные условия
или более слабые условия?
Если корреляция случайных величин ноль, они независимы?
Если они независимы, всё, пожалуйста, вот за этим следите.
Из нулевой корреляции не следует независимость.
Из независимости следует норевая корреляция.
Потому что иначе вас могут на этом поймать.
Я не помню, там есть какая-то очень красивая штука, там
что-то кубическое что ли.
Короче, на Википедии есть пример, ну его можно на самом
деле придумать на доске за две минуты, когда корреляция
у вас будет ноль, но при этом там не то, что независимо,
там вообще как бы одна величина через другую выражается
детерминированно практически.
А?
Ага, ну это далеко не общий случай, правильно?
То есть для некоторых случайных величин, если они не скоррелированы,
значит независимо, если вы знаете что-то об оприорном
распределении, из которого они пришли.
Но в общем, в случае это не работает.
Едем дальше.
Среди признаков, в которых описывается объект, есть
цены в рублях и расстояние в километрах.
Обучили на этом первый раз КНН.
Потом поменяли признаки, точнее поменяли цены в рублях
на цены в евро, табличка у нас стала другая, обучили
второй КНН.
Вопрос предсказания первого и второго КННа одинаковые
или нет?
Ну большинство.
Что?
Что?
Во, замечательный комментарий, но тут про это, собственно,
не слово.
Если данные отнормированы, то абсолютно все равно,
потому что перевод из одной валюты в другую, просто
домножение на какую-то константу.
Все равно потом отнормируемся в 0.1, например.
Если нормировки нет, но именно на этот вопрос и направлен.
Если у вас нет нормировки, шкалы данных играют значение.
Поэтому если вы данные не отнормировали, как большинство
и ответило, КНН будет предсказывать по-другому, почему.
Расстояния поменялись.
Собственно, я вас спрашивал еще в самом начале.
У нас нормировка может быть зашита в КНН, а мы не знаем,
какая именно реальность.
Сейчас, поглядите.
КНН просто считает расстояние.
Окей, хорошо, я могу потом поправить вопрос.
Главное, чтобы еще раз, смотрите.
Тут все вопросы, грубо говоря, главное, чтобы вы поняли,
когда да, когда нет.
В данном случае, если есть нормировка, логично, что
абсолютно неважно.
Если нормировки нет, я надеюсь, для всех логично, что работать
будут по-разному.
Правильно?
Поэтому будьте с этим осторожны.
Да, если у вас есть какие-то вопросы, вы их, пожалуйста,
задавайте.
Какой способ регуляризации в линейной регрессии имеет
тенденцию к отбору признаков?
Большинство L1, 12% L2, 10% оба, 10% не один из очень перечисленных.
Вопрос, что с этими 33% примерно людей есть кто-нибудь, кто
может обосновать, почему.
Что такое отбор признаков?
Вот, что такое отбор признаков?
Если мы вспомним, как у нас с вами ведет себя L1 регуляризация,
мы можем вспомнить, что при использовании L1 регуляризации
можно обнаружить перед некоторыми признаками весовой
коэффициент ровно равный нулю.
То есть не просто какая-то малая величина, а именно
ноль.
На бульдальской на практике нет еще?
Тут вопрос в аудитории.
Стоит еще раз разобрать, почему это происходит или
нет?
Да, хорошо, давайте тогда переведем фокус на доску.
Нам это пригодится.
И карандаш.
Опять же, я приведу обоснование, которое скорее интуитивно
понятно.
Оно не единственное.
Но тем не менее.
Еще раз, смотрите.
Вот у нас с вами есть значение нашей функции эмпирического
риска.
То есть то, как мы штрафуем нашу МАДЫ.
У нас это в общем случае L, это наш ЛОС, плюс R, это
не левый правый как в деревьях, это регуляризатор.
И давайте скажем, что мы с вами градиентные методы
оптимизации сейчас применяем, хорошо?
В таком случае, когда мы с вами считаем dq по dОмега,
это логично, что dl по dОмега плюс dr по dОмега.
Логично, производная сумма, сумма производных.
Эта часть у нас от регуляризации не зависит, верно?
То есть пускай она у нас чему-то там равна.
В какой-то точке у нас производная от функции потерь уже будет
порядка, давайте какой-нибудь другой цвет возьму, тоже
яркий.
Вот этот цвет видно?
Вот.
Эта штука у нас там порядка 10 минус 3, например, предположим.
Давайте посмотрим на производную.
Так, а тут если я пишу видно, вот d норма омеги первая
по, давайте омега итой.
Возьмем какой-то элемент по dОмега итой, чему будет
равен?
Bingo, сигнум омега итой, это всем понятно, правильно?
Потому что первая норма это на сумму модулей просто,
тогда производная итового элемента это просто сигнум,
если больше нуля плюс один, если меньше нуля минус
один, если ноль-ноль.
Верно?
Получается, что у нас вот эта штука всегда порядка
единиц.
Ну, куда у нас попрет по антиградиенту изменения
весов?
Туда, где у нас градиент больше, логично.
Поэтому если у нас вклад вот этой части будет меньше
чем вот этой, выгоднее с точки зрения градиента,
это просто устойчиво максимум минимум будет.
Пойти туда, где у нас будет достигаться ноль, а ноль
в единственном случае, когда вес сам равен нулю.
То есть если признак не сильно влияет на пункт
с ошибки, то есть производная пункт с ошибки по весу этого
признака очень маленькая.
Маленькая, опять же, от чего это зависит?
Ну обычно здесь вот какой-то коэффициент регуляризации
сидит и здесь на самом деле сидит лямбда.
То есть вы сами контролируете, насколько вам важно признаки
учитывать.
Если у вас вот этот вот один на лямбда больше, чем
тот вклад, который делает признак сюда, то он будет
занулён, потому что он менее важен.
Чем вам нужно?
Понятно?
Всем понятно сейчас.
Это вот самое простое за всё время, что я здесь
нахожусь.
Наверное, объяснение, которое приходило в голову.
Тут вроде очевидно, что одно доминирует другое.
Вам выгоднее здесь получить как бы нулевое значение
производной, чем здесь какое-то.
Понятно?
Нет?
Всем понятно.
Вам понятно?
Всё, супер.
Можем двигаться дальше?
Супер.
И на всякий случай для самопроверки.
Почему со второй нормы это не прохватывает?
Бинго, потому что здесь при эпсилон малом значении
омега у вас производной будет 2 омега, поэтому при
уменьшении омеги по весу у вас производной будет
убывать к нулю линейно.
Чем меньше омега, тем меньше производной, в какой-то
момент они друг другу равновесят и нормально.
L1 так не работает, тут либо 1, либо 0.
Либо панель пропал.
Уловили?
Едем дальше.
На какой энтропии?
Каких графичках, где энтропия?
В смысле там?
В смысле здесь?
Погодите, сейчас у нас будут варианты ответа, где у вас
много вариантов, вопросов, где много вариантов ответа,
там у вас будет мультимальное распределение.
И там мы посмотрим, что вы там на выбирали.
Но на самом деле это тоже можете считать, грубо говоря,
вот ваше распределение ответ, перестроите это в
дистаграмму, посчитайте энтропию.
Поняли, нет?
Вам посчитать?
Вам на пальцах или на калькуляторе?
Ну, короче, видно, что здесь, грубо говоря, треть студентов
куда-то пошла не в ту степь.
Надеюсь, сейчас эта треть разобралась, будь то в
аудитории или в онлайне.
Едем дальше.
Аналитическое решение, задача линейной регрессии.
Ну, ошибка МСЕ, это вот то, что я вам рекомендовал
продиференцировать руками в домашке, xtx-1 и xty.
Это полезно уметь выводить, еще более полезно уметь
понимать, что это можно вывести в любой момент дня
и ночи, просто помнить то, что полезно, вас могут где-нибудь
спросить об этом.
xtx, xty, xt… Сейчас, вы имеете в виду в каком-то из следующих
вопросов?
Ну, это нормально.
Ну, ребята, хоро… Спасибо, еще раз, так как этот тест
не оценивается, ничего страшного, здесь мы с вами
видим, что большинство, 77% ответило благо верно.
xtx-1, xty.
Что делать, если вам попалась какая-то подобная формула,
пусть что-то другое, пусть это не про функцию среднеквадратичную,
короче, вам вот кровь из носа надо ответить, не
знаю, какое-то внутреннее тестирование на работе,
какой-нибудь джемат сдаете, короче, что-то, а вы кровь
из носа ничего не помните.
Что делать с такими формулами, как вы думаете?
Бинго смот… Гуглить, нельзя гуглить, если вы там на джемате
сидите.
Сверяйте размерности, тут просто по размерностям
большинство формул не сочетается.
Ну, у вас не работает матрица ленинный оператор, вы не
можете ленинным оператором воздействовать на не пойми
что.
Или так, например, то есть всегда ищите какие-то
крайние условия, если вы не знаете, что делать.
Не думайте, что раз я не знаю формул, значит все плохо,
если это тест, его можно немного обойти.
Тесты поэтому и не очень популярны у нас на экзамене,
у нас именно умстный опрос.
Там как бы можно доп.
вопрос задать и все понятно.
Так, ну тут вроде комментарии излишни, правильно?
В лекции этого водилось, вывести это на бумажке
надо 30 секунд.
Я не знаю, что еще сказать.
Вот вам, пожалуйста, распределение другое.
Домножение всех значений признаков обучающей выборке
на 10 приведет к игнорированию числительной ошибки.
Изменению решения задачи минимизации МСЕ линейной
модели.
Да, нет, вот тут просто ответила 50% ровно 50 на
50.
Кто за то, что да?
Кто за то, что нет?
Кто насчет остальной половины аудитории?
Вы сдержались?
Ну давайте, кто-нибудь проинтерпретируйте, вот, кто за то, что да, почему.
Нет, веса поменяются логично, раз вас на 10 домножили.
Изменится предсказание, давайте так, хорошая поправка,
вы правы, надо будет потом.
То есть мы именно предсказание меняем, потому что, условно,
домножили признаки на 10, логично, что веса должны
на 10 упасть в 10 раз, потому что иначе смысла нет.
Ну что, кто за то, что да?
Все коэффициенты должны упасть 10 раз.
Ну я ровно поэтому, я их спрашиваю, вот вы за то,
что ничего не изменится, правильно?
Кроме этого что-то изменится?
Ну ребят, опять же, ребят, хорошо, вы правы, как бы,
тесты, грубо говоря, они не идеальные, я их составлял,
я был немножко обусловлен на свое состояние сознания,
вот вы говорите, сейчас было не очень понятно, окей,
в будущем поправим, сейчас давайте тогда именно разберемся
концептуальном, а не в том, что вот тест неправильно
составлен.
Нет, таргет вообще не трогаем, все, смотрите, мы взяли
только признаки, поделили в 10 раз, все.
Логично, что у нас оптимальные коэффициенты стали в 10 раз
больше, правильно?
Ну или тут что, умножили на 10 домножение, тогда оптимальные
коэффициенты стали в 10 раз меньше, верно?
Кроме этого что-то поменяется?
Проблем вроде нет, правильно?
Оценка наших предсказаний не поменяется, с этим все
согласны?
Да?
Нет?
Супер, ну по сути так и есть, у нас вроде никаких проблем
с этим быть не должно, тем более мы сказали, что игнорирую
вычислительные ошибки, потому что на самом деле
если вы домножите ваше значение параметров там на 10 восьмой,
то у вас вычислительные ошибки появятся, причем
может появиться достаточно здоровое, но просто потому,
что у вас уже где-то там далеко, ну ладно, 10 восьмой
еще может нормально, 10-25, у вас скорее всего пойдут
в вычислительный характер проблемы.
А если мы мое минимизируем, что-то поменяется?
А почему у нас на 20% меньше ответов «да»?
Ну вот, смотрите, абсолютно индиферентно какая у вас
функция ошибки, правильно?
Если вы решаете задачу, вы домножили, игнорируя
вычислительные проблемы, домножили на 10, поделили
на 10, абсолютно неважно.
Все, хорошо.
Мнение решения задачи минимизации МСЕ плюс Л2-регулизация,
что тут можно сказать?
Так, а тут говорят не поменяется.
Ну-ка.
Я на всякий случай уберу свободный член просто для
удобства.
Хорошо?
Вот мы это умножаем на минимум, правильно?
Все согласны?
Устремляем к минимуму.
Тут все согласны.
Нигде опечатки нет.
Ну на всякий случай это вот условно по какому-то
там множеству иксов, я не знаю, так как это в матричной
форме, так норма этого вектора, все, норма вектора остатка.
Ошибка.
Все понятно.
Хорошо.
Если мы с вами домножили все признаки на 10, у нас
нормы вектора весов, каждый элемент стал в 10 раз меньше,
правильно?
То есть мы по сути можем сказать, что это у нас первый
случай, здесь Омега-1, соответственно, во втором случае у нас будет
х1, х2, Омега-2.
Мы с вами знаем, что Омега-2 это получается 1 десятая,
Омега-1.
Верно?
Что?
Ну раз признаки домножили в 10 раз, значит Омега должна
упасть 10 раз.
Верно?
Вот.
Получается эта штука, минус y, y не менялся, 2 в квадрате,
плюс лямбда, теперь делить на корень с 10, норма опять
же, Омега-1, Омега-1, а, хорошо, да-да, вы правы.
Так, Омега-2, 1 десятая, Омега-1, стоп, чего?
Все правильно, не корень, да, на 100 в смысле поделить
и не умножить.
Все, все, все, я думаю, поделить на 100, да, норма Омега-1,
квадрат.
Ну а тут, соответственно, на самом деле это что?
Это эквивалентно х1, Омега-1, все согласны?
Ну так что, получится то же самое решение?
Убедились?
Если вы вот эту задачу минимизации решаете, у вас получается,
что вот это, вот это, короче, эквивалентны ли эти две
задачи минимизации?
Решение одно и то же будет или нет?
А?
Чего подставить?
Не, смотрите еще раз, вот это все мы минимизируем
по Омеге, я на самом деле это записал, чтобы было
понятно откуда переходы.
Х2, Омега-2 это то же самое, что х1, Омега-1, правильно?
Это наше предсказание.
Х2 это где мы все умножили в 10 раз?
Да, не совсем корректно, но тем не менее у нас
коэффициент регуляризации-то упадет, поэтому решение
получится другим, у нас оптимум съедет.
Уловили?
Оптимальное решение без регуляризации у нас было
вот такое, правильно?
То есть, если у нас х умножили в 10 раз по всем признакам,
то по идее Омега должна просто упасть в 10 раз по
всем признакам и получим ту же самую оценку.
Но при этом у нас коэффициент регуляризации, по сути,
стал 100 раз меньше, потому что все веса упали.
Так что решение на самом деле оптимальное будет другое.
Именно поэтому, помните, мы с вами когда говорили
о первой регуляризации, что будьте очень осторожны
с регуляризацией, потому что вам нормировка данных
вообще играет роль.
Это просто такой демо-случай.
В общем случае, если у вас один признак сильно имеет
больше шкалу, чем другие, у него сильно меньше вес,
у него больше значимость в точке зрения регуляризации,
потому что она его не трогает, а остальные занижает.
Улавливаете?
Так что понятно, что здесь у нас с ответами.
Здесь большинство 68% ответили, что приведет
к изменению решения.
Логично, что случай L2 регуляризации изменится?
Случай L1 что-то поменяется, нет?
Ну коэффициент регуляризации здесь просто будет
на 10, а не на 100, но все равно поменяется, верно?
Все понятно? Разобрались?
Супер.
Чего-чего?
Тести L1...
А, не, это вторая норма, это просто Омега-1.
Я ничего не понял.
А, мое плюс, окей, не, это я просто в голове уже.
Если L1 регуляризации тоже поменяется, правильно?
Я просто подумал, что там написано МЦЕ плюс L1, там
мое плюс L2, суть та же самая.
У вас регуляризатор все равно станет другим.
Все, разобрались.
Супер.
И этим дальше.
После обучения на очень большой выборке линейная
регрессия в режиме inference работает медленнее, чем
кнн, быстрее, чем кнн, работает так же.
Ну, абсолютно большинство говорит, что да, работает
быстрее. Я надеюсь, это всем понятно?
Линейная регрессия на одном объекте работает за
линию, где за линию от количества признаков.
Все, вам надо умножить веса на признаки и сложить.
Кнн работает за квадрат, потому что вам нужно посчитать
расстояние от этой точки до всех.
Соответственно, у вас количество точек на количество
признаков.
Режим inference это режим применения.
Обучение.
Ну, это, скажем так, просто терминолог, то есть режим
обучения. А?
Ну, train inference, да, или train evaluation.
Вот два варианта.
Просто, а?
Вот плохо слово тест, именно inference.
Что такое infra-inference?
Вот вы модель построили, и отдали ее заказчику, она
теперь где-то крутится.
Вот это model inference.
У вас отдельно, на самом деле, кстати, 8 числа, по-моему,
будет открытая лекция по MLEOps о том, как свои модельки
гонять в около продакшн, скажем так, пока на коленке.
Это мы как раз сейчас договорились с специалистом из
Альфа-банк.
Да.
Не, это будет открытая лекция, потому в четверг в 17.05
мы заранее объявление сделаем, просто я сегодня, скажем
так, аудиторию заблоронировал, сразу вам об этом говорю.
То есть у нас есть некоторые открытые лекции, я думаю,
кто-то из вас подписан на журнальный клуб, там иногда
будут какие-то выступления.
На них можно прийти послушать, обычно не онлайн, потому
что все это началось во время ковида.
Но очные выступления мы тоже рады проводить.
Так что вот проведем здесь, он расскажет что-то
любопытное.
Ну ссылочку тогда скину, грубо говоря, туда приходит
как правило или какие-то люди извне, кто хотят просто
рассказать о своих научеств семинар, говоря простым
языком.
То есть туда приходит кто-то рассказывать о чем-то
любопытном, на тему правил машинного обучения или
около.
Например, вот недавно договорился с коллегой, познакомился
с коллегой из Фиана, который занимается квантами
вычислением, у них там квантовый компьютер на
холодных запутанных ионах.
Я очень плохо в этом деле разбираюсь, если что, поэтому
если я где-то там ляпнул что-то не так, спросить
пожалуйста.
Вот, я надеюсь с ним тоже удастся договориться, он
тоже придет и расскажет чем там можно в квантовых
вычислениях позаниматься сейчас в России.
Тема достаточно любопытная.
Вот.
Так, ну тут вроде все понятно.
КНН сильно медленнее, ровно поэтому линейные модели
очень любят на практике.
Они линейные, они простые.
Они работают вот-то вот на вот таких вот часах там
или знаю на телефоне со скоростью в лед.
А всякие сложные нейронки, хотя они сильно проще, чем
КНН тоже на больших данных, работают чуть медленнее.
Поэтому линейные модели себе вряд ли когда-либо
и живут.
Всегда будет нужна скорость.
Так, L1-02 регулизация.
Линейная регрессия работает с логистической регрессией.
Каждый раз одно и то же.
Коллеги, почему на логистическую регрессию на 20 человек
меньше обращать внимание?
В аудитории есть кто-то, кто за то, что линейная регрессия
да, логистическая регрессия нет?
То есть в аудитории все за оба, правильно?
Ну понятно, или не сознаемся.
Все нормально, так как ответа не видно кто.
Я надеюсь, все понимают, что с точки зрения применимости регуляризации
абсолютно индиферентная линейная регрессия, логистическая,
там какой-нибудь SVM.
Если у вас дифференцируемая функция потери, вы к ней
можете любой регуляризатор прилепить и с ней работать.
Применимо.
КНН-ом?
В КНН-е есть параметры?
Это вообще не параметрический метод.
У вас гиперпараметры есть, которые вы сами выбрали, а дальше он работает.
Все.
На Nn-байсе есть параметры?
Не угадали.
На Nn-байсе есть параметры?
Да, например, если вы параметрическое, семейство
распределений выбрали, нормально, у вас есть параметр распределения.
Вы можете его сделать и не параметрическим, ну ваши
место распределения можете это использовать.
Ну какого-то априорного распределения, какого-то
аналитической функции.
Например, да, гистограмма построили, все.
Все. Никаких параметров у вас нет, у вас просто выбручена функция распределения,
тогда мед стал, по сути, никаких параметров он не имеет. Согласились? Поэтому там никой
ль два регулизации, ну и смысла в ней как-то непонятна. А что мы там собрались регулизировать?
То есть фактически он иногда имеет параметры, но что означает ограничение нормы
вектора параметров, особенно с учётом того, что может быть много признаков у всех,
разное распределение всех, разные параметры, чуть значить не понятно. То есть может быть в каком-то
случае это будет полезно, если мы хотим там понить дисперсию, но это какой-то очень-очень
узкий случай, явно здесь не оговорено. Так, логистическая регрессия. Стремится найти
линейную гиперплоскость между двумя классами. Вопрос по русскому языку. Линейную гиперплоскость
это не тавтология? Бывает нелинейная гиперплоскость? Ну да, согласен, но по сути если у нас гиперплоскость,
то она вроде по определению линейная, иначе это уже гиперповерсность какая-то, там уже может быть
что угодно. Ну так, на всякий случай. Линейную, короче, гиперповерсность становится гиперплоскость
между точками. Всё правильно. 55%. Ну можно так сказать, она регрессирует вам вероятности,
но это классификатор. Ну ровно для этого этот вопрос здесь сидит. Логистическая регрессия. Так,
повторить чё такое логистическая регрессия? Кто за? Почему это плоскость? Вот сейчас повторим,
почему это плоскость. Омега икс плюс б задает что? Можно камеру повернуть, пожалуйста? Что
задает омега икс плюс б? Ну в общем случае гиперплоскость, правильно? Вот. Если у нас с вами
есть какая-то гиперплоскость, в данном случае она будет, так как у меня доска двумерная,
это будет вот прямая. Омега где? Что такое омега? Это нормально, это прямой, верно? Вот,
допустим, вот это омега. Давайте опять же я скажу, что b равный нулю, поэтому эта штука проходит
через начало координат, хорошо? Всё, договорились? Двумерный случай. Вот наша омега. Итак,
если у нас классикатор с вами есть, то что мы с вами делаем по сути? Мы говорим, что сверху у нас
точки вот белые, вот они тут сидят, а снизу точки зелёные, вот они вот тут сидят, окей? Вот ваш
классикатор. И теперь мы говорим, хорошо, хочу предсказывать, что такое, где класс, где один,
где другой. Как нам это понять? Ну, давайте тогда посмотрим на что. На сигнум вот этой штуки,
можете тогда по знаку вот этой величины понять, в каком классе находится точка? Как понять? Ну вот,
собственно, если выше, если больше нуля, значит у нас вектор на точку сонаправлен вектором нормальной
плоскости, в скалярном произведении их положительное, значит оно с этой стороны. Если в скалярном
произведении у них отрицательное, значит с другой стороны, верно? Но теперь мы понимаем, что это всё,
конечно, замечательно, только нам как-то хочется не просто метку класса получить, а что-то более
уверенное, потому что нам функция потери, подходящая нужда. Что мы говорим? Окей, давайте попробуем
предсказывать не просто метку класса, а вероятность метки класса. Тогда мы говорим, что в каждой точке
нашего пространства мы париметризуем какую случайную величину. Два исхода. У какой величины два
исхода? Бернулевскую. Все помнят что такое бернулевская случайная величина? Кто не помнит?
Бернулевская случайная величина, подбрасывание монетки, случайная величина с двумя исходами.
Единственный параметр вероятности положительного исхода. Всё. С каждой точки пространства у вас
может быть объект, правильно? Для каждого объекта вы хотите уметь предсказывать вероятность того,
что это объект того или иного класса, правильно? То есть для любой точки мы говорим, хочу вот эту
точку. Он нам говорит, вероятность того, что это зеленый, там 0.27, например. 0.72. Понятно, да? И
остается только вопрос, как нам научиться эту вероятность предсказывать? Ну вот, собственно,
я кидал ссылку на Quora, где вот такой вот матан вывода, почему сегмоида, исходя из экспоненциального
семейства распределения и так далее. Мы говорим, если коротко, что смотрите, сегмоида, она умеет
отображать R в 0.1. Замечательно. Вероятность у нас сидит в 0.1, R. Вот вам, пожалуйста, отображение из
нашего признака пространства в R. Всё. Применили, получили? Поэтому она называется логистическая
регрессия, она нам регрессирует вероятность. Но это метод классификации при этом. На этом часто в
начале краски многие ловятся, ровно потому, что это же регрессия. По сути, регрессия классификации мало
чем отличается. Регрессия, с одной стороны, проще, потому что у вас, как правило, одна переменная,
если это скалярная регрессия. С другой стороны, классификация проще, потому что у вас не
континуальное множество ответов, непрерывное какое-то множество, куда вы отображаетесь, а просто конечное
число класса. Но с другой стороны, мы обычно предсказываем не метку класса, а вероятность метки
класса, поэтому обычно вектор просто предсказываем и всё. Вектор вероятности. Или до Softmax просто
вектор каких-то величин, которые Softmax отобразим вектор вероятности. Что такое Softmax, помните? Или
написать на доске? Смотрите, у меня получилось теперь вот не одна такая плоскость, а три штуки.
Вот первая условно, вот какая-нибудь вторая, вот какая-нибудь третья. И соответственно,
у каждой из них есть какой-то вектор нормалия. Вот, здесь соответственно. Ну, у нас многоклассовая
классификация, тогда у нас, допустим, на три класса пытаемся построить. Для каждого класса,
по сути, строится свой классификатор, который говорит один против всех остальных. Поэтому у нас
теперь для каждого объекта есть, по сути, что такое? Ну, вот это вот, скажем так, расстояние со
знаком до этой гиперплоскости. Вот это скалярное произведение, до вот этой гиперплоскости, до вот
этой и до вот этой. У вас получается какой-то вектор, виды там 4 и 2, 0 и 7 и 8 и 2. Ну, отсюда вроде
с одной стороны понятно, кто ближе всех? Вот этот ближе всех гиперплоскост, правильно? Кто глубже всех
в своем классе? Вот этот. Но как-то с вероятностями не очень. Поэтому мы с вами можем отсюда перейти
к вероятностям. Каким образом? Ну, а? Это расстояние, ну вот, смотрите. Это краски омега 2 на х. Вот она.
Ну, не омега 2, а омега 0. Это омега 1 на х, это омега 2 на х. Понятно? Супер. Мы отсюда можем перейти
к вероятностям. Ну, для этого можно просто применить softmax, softmax функция вида E в степени х,
делить на x и на сумму по g E в степени x g. То есть она к вектору применяется, получается вы все
элементы засовываете в экспоненту и суммируете. Тогда у вас вектор перейдет во что-то вроде 0.2,
0.01 и 0.79. Вот вам ваша вероятность. А? Суммируется. Ну, на формулу посмотрите. У вас в знаменателе
всегда сидит сумма экспонентов по всем элементам вектора, а обчислители элементов вектора.
По определению в единицу перейдет плюс экспонента, она всегда у вас что? Не отрицательная. Верно? Все
работает. Так, разобрались? Попросы, комментарии, предложения? Да. Логистическая регрессия
метапорных векторов отличается оптимизируемой функцией потерь. Ну да, у вас и там и там получается
линейная плоскость, гиперплоскость у вас строится, но метапорных векторов вы минимизируете
максимум из 0 и 1 минус margin плюс вторая норма. Вот это вы на минимум отправляете,
а в логистической регрессии вы pi log pi сумма. В бинарном случае от 1 до 2 плюс соответственно
какой-нибудь регуляризатор, в общем случае не обязательно. Второй отправляете на минимум. Вот,
это просто два разных подхода, они выводятся из разных идей, хотя по факту и то и другое,
линейный классикат просто функции потерь разные. Ну если записать по-другому, это получается
pi log pi. Вы правы, здесь q, конечно. p и p1 log q плюс 1 минус p log 1 минус q. Ну это эквивалент,
надеюсь вы понимаете. У вас два класса, тогда сумма вероятностей. Что такое? 100 процентов,
то есть p1 на log q1 плюс p2 на log q2, то же самое, что 1 лог q1, 1 минус p1 на log 1 минус q1.
Это предсказанная наша вероятность. Вот эта штука, это наша сегмоида от omega x плюс b.
То есть это ваша истинная вероятность, а это предсказанная. Перефразирую, что это такое. Это
мат ожидания логарифма q по вашей выборке. Согласились? Так, давайте по одному и по громче.
Хатин, спасибо. Спасибо. А, pity приходит из нашего, из нашей выборки. Мы же с вами,
это истинные вероятности, откуда они у нас берутся. Мы же мат ожидаем по выборке,
правильно? pity это истинная выборка, это истинные метки класса, то есть если объект класса 1,
вероятность 1 класса 100 процентов, если класс 2, вероятность 2 класса 100 процентов.
Это кроссонтропия. Ну смотрите, нет, смотрите, давайте так. Если убрать минус, то вот эту штуку
надо отправить на максимум, потому что это именно логарифм от ожидания логарифма по
правдоподобию. Так как я написал на минимум, тут нужен минус и тут тоже нужен минус. Но это
именно функция потерь, которую мы минимизируем, потому что мы привыкли обычно задачи минимизации
решать. Вопросы еще? Интропия по определению с минусом. Минус p log p. Ну сумма по всем элементам.
Как себя проверить? Интропия снизу ограничена чем? Нулем. Как бы невыраженный случай,
интропия равна нулю. Если у вас p log p, то p всегда от 0 до 1, и логарифм числа от 0 до 1 какой?
Отрицательный. Значит, нужен минус. Так, еще вопрос есть? По этому вопросу. Так, а, ну мы тут это с вами.
Замереться найти нелинейную границу между двумя классами из-за сигмоидной функции. Но тут
третья ответила да, я надеюсь, что понятно, что нет, правильно? В чем тонкость? Логистическая
регрессия задает нелинейное отображение, потому что мы отображаемся из пространства признаков
во вероятностное пространство. Отображение нелинейное, гиперплоскость линейная. Какой бы это в
талоге не было. Почему я на это обращаю внимание? Мы с вами вот на прошлом занятии говорили, давайте
еще раз. Посмотрим. Говорили про градиентный бустинг. Помните, мы с вами там говорили,
что по факту у нас бустинг это ансамбль вида. Роит на фит от х сумма пои, правильно? Классический
вопрос, я его уже задавал, но тем не менее. Имеет ли смысл в качестве фит использовать
линейные регрессии? Чего? Ну, я сразу просто к этому перешел. Это то же самое, потому что если
фит линейная регрессия, то это линейная комбинация линейных отображений, что есть линейное отображение.
Если фит это логистическая регрессия, что? Ну, видимо-невидимо, а давайте подумаем, почему можно.
Смотрите, если фит, вот линейная регрессия, вы сказали, смысл не имеет. Все, красный крест не
работает. Я иду дальше. Если фит логистическая регрессия, ну, давай тогда перепишем. В каком-то
виде, допустим, это получается сумма пои ρi, с одного там до t, с одного до t-1, ρi на стигмоида от
ωi x плюс bi и плюс, соответственно, то? ρt стигмоида от ωt x bt. Что это, линейная комбинация линейных
отображений? Нет. Так что вы, используя линейную комбинацию нелинейных отображений, можете построить
более сложную решающую поверхность. Простой пример, как бы классический пример. Все, тут зеленая галка.
Работает. Простой пример, вы с помощью логистической регрессии можете решить вот такой случай. Тут у вас
сидят белые точкики, а тут у вас сидят зеленые точкики. Вы можете, на самом деле, построить себе
множество классикаторов, которые как-то вот так вот это начнут делить, и тем не менее у вас получится
отделить одно от другого. Задача линей не разрешимая, бустингом решать можно. Мы с помощью
композиции слабых классикаторов выходим, по сути, за класс линейных оценок. Поняли? Так.
Не обязательно. Нет, у вас нет ограничений. Это скорее, скажем так, евристическая просто
ситуация, что обычно люди используют один тот же класс моделей, который бустит. Потому что,
на самом деле, не суть важна, если вы используете простые деревья или там, ну, логистическая
регрессия достаточно редко бустингу подвергается, но иногда подвергается. Знают два случая, когда она
даже outperform-ила остальных. В смысле, по скорости и другим там параметрам. То есть, суть в чем?
Обычно выбирается какой-то класс моделей, параметрически неважно, деревьев, что-нибудь,
вы можете хоть КНН сюда запихнуть. И потом из них строится ансамбль. Обычно выбирают какие-то
модели, которые достаточно быстро можно строить и инферить одновременно. КНН, например, не будет
сюда ставить. Почему? Дорого. Он и так квадратичный, а тут еще будет N-моделей, где N-глубина ансамбля.
По факту, вы можете на первом шаге построить градиентный бустинг. Первый шаг у вас будет
линейная регрессия, логистическая регрессия, второй какой-нибудь дерево, третий какой-нибудь
КНН, четвертый наивный байс. Вам никто не мешает. Только вопрос, а зачем? Если можно обосновать
зачем, может попробовать. В правиловом смысле нет. Что делает бустинг? Давайте еще раз такой вопрос
вам общий. Бустинг подбирает новые информативные признаки? Кто за то, что да? Кто за то, что нет?
Где остальная часть аудитории?
В каком-то смысле это отличная обработка исходных данных. Что вы имеете?
Почти. Только вы сейчас не про бустинг говорите. У вас на входе во все эти модели что лежит?
У вас исходная X на входе во все модели лежит, правильно? Так что градиентный бустинг у него не
меняется. Признаковое пространство, над которым работает модель. Все модели отображают из X.
То есть для любой И, это отображение из X. Это наш признаковое пространство. Но вот вопрос куда?
Куда отображаем? В 0.1 вероятности нет. У нас целевое пространство меняется каждый раз.
Что делает градиентный бустинг? Мы с вами с помощью модели пытаемся опроксимировать
антиградиент. У нас есть наш DL от ансамбль F от X и Y и по DF от X и. Вот наш антиградиент в данной точке.
Помните? Мы сказали, что вот это DRI. Вот оно. Даже не DRI, просто RIT. И теперь наш с вами FIT от X и Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от
Y и по DF от Y и
по DF от Y и
по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от
Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF
от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
по DF от Y и по DF от Y и по Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF с
от нуля до двух симметричная п лог п симметричная функция
п лог путочнее кажется несимметричная кажется там сидит логаритм
в оооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооо
ооооооооооооооооо-
справа. Логарифма у нас убывает очень быстро, правильно?
Так, а?
Ладно, может проект сомнительный, я перегнул?
Справедливо. Хорошо, ну короче, да, вы правы, спасибо. Короче, вот у вас-то
функция. Вот. Вот у нас логистический лоз, верно? Так, а куда я делаю этот
зеленый? Согласны, не согласны? Да, а сегмой уже там, в сегмой здесь идет маржин, по
сути. Ну, омега икс наш. Ну, давайте банально посмотрим, если у нас объект сидит в
глубине чужого класса, правильно? Мы используем градиентную оптимизацию, считаем антиградиент.
Градиент здесь какой? Минус один. Да, нет. Градиент здесь какой? Наверное, будет больше,
чем минус один, это у нас все-таки тангенс угла наклона, правильно? Производные сегмойды какая
максимум? Минимум. Максимум. Максимум. Максимум. Нет. Ну, можете продиференцировать,
производные сегмойды, на самом деле это сегмойда на один минус сегмойда, максимальное назначение 0,25.
Или меньше? Короче, если эта штука в какой-то момент растет быстрее, чем 4х, то у вас будет
больше влияние этого градиента на ваше предсказание. Окей? Всем окей? Все спокойны.
Может быть. Окей, хорошо, справедливо, да. Хорошо, ладно, да, убедили, надо, видимо, как-то переформулировать
вопрос. То есть логика именно в том, что у вас сам хинжелос на самом деле пострадает от выбросов
больше, потому что он растет просто слева. Ой, господи, логлос слева от нуля растет быстрее. Вот. Я на самом
деле именно на это пытался обратить внимание, вы правы, надо чуть-чуть вопрос переформулировать,
потому что там еще сегмойда вмешивается. Да, я к этому, я именно, вопрос был про то,
какую лоз на самом деле страдает больше, просто вот так я его описал. На самом деле, вы правы,
надо чуть-чуть переформулировать вопрос, иначе можно с этими производными запутаться. Окей?
Вопросы еще здесь есть? Это логлос, да. Иван? Ну простите мне мой подчерк.
Давайте. Кто? А что такое семейство сегмойд?
А, ну чисто теоретически да, но сегмойда конкретно это вот функция ровно 1 на 1 плюс e в степени минус x.
Но вообще, да, вы правы, вы ее можете там сделать более пологой, менее пологой и так далее. Не-не-не.
Смотрите, почему мы этот параметр не берем? Это, кстати, хороший вопрос. Почему смысла растягивать
туда-сюда сегмойда особо нет? Смотрите, сегмойда вот линейно туда-сюда растягивает. Да, но я к чему
говорю, у вас под сегмойдой сидит обычно ωx плюс b, правильно? У вас ровно ω и отвечает за то,
как вы туда-сюда растягиваете все. У вас же аргумент меняется как раз. Так что то, что вы меняете,
то же самое меняется, когда вы параметры меняете. Так, ладно, едем дальше. Выберите
корректную функцию hinge-loss, но только что написали максимум 0,1 margin. А? Архтангенс?
Может геверболический тангенс? Архтангенс я ни разу не встречал. Более того, я не очень помню,
какие у него области значений. Ну да, конечно, это архангенс. Что? Четкую сегмойда? Или задайте
вопрос или погрубче. Функция 1 делить на 1 плюс e в степени минус x. А, вы имеете в виду,
что можно еще какую-то функцию задать такую же? Или что? Которая отображает r в 0,1? Да,
можно кучу функций задать. Почему именно сегмойда? Вот, я говорю, можем попытаться с вами поднять в
опен, открыть вору, прочитать большой талмут об этом. То есть, я, к сожалению, сейчас в ходу даже
не готов на самом деле этот вывод воспроизвести. Скорее, я вот к лектору по статам тярверу отправлю.
Вот, лектор по статам тярверу должен это делать, наверное, среди ночи и проснувшись. Я так делать
не умею, я с этим выводом сам два часа один раз просидел, вывел, но по памяти я вряд ли
его воспроизвезу, скажу честно. Не, опять же, если есть такой запрос, вы об этом скажите, я могу
в этом разобраться, и мы с вами это еще раз проделаем. Но за ненадобностью я за полгода петь забуду.
Так, тут вопрос еще есть? Нет? Нет? Нет? Хорошо. Так, едем дальше. Что общего у Писей и Кэннон?
Обожаю этот вопрос. Оба являются алгоритмами обучения и безучителя. 15 процентов. Успех.
Оба являются особыми формами регуляризации. 2,2 процента. Классно. Хорошо, два мисклика. Они
оба требуют нормировки данных, если вы не знаете, что в конкретном случае это не так. Лоично,
потому что если у вас нет нормировки, в Кэнноне вы считаете квадратичную вторую норму, помолча…
окей, не втопросите. Которую, скорее всего, от шкала зависит. Вот. В Писей вы считаете
норму Фрабениуса, которая очень похожа на Евклидову норму, согласны? По каждому объекту, по отдельности.
Поэтому тоже зависит от шкала, если отнормируете, у вас все станет по-другому. Они оба позволяют
уменьшить размерность набора данных и упростить вычисление. Желтый ответ, логично правильный,
про снижение размерности данных. В принципе, я могу сам себе сделать такую подколку, что с
помощью любой модели, которая решает задачи обучения с учителем, можно стичить размерность.
Например, как? Вы вместо объекта можете выдавать какую-нибудь статистику по его соседям. Ну,
например, распределение вероятности класса по его соседям. Вот вам, пожалуйста, было много признаков,
то есть чисто технически можно. На практике такое я видел не знаю когда, поэтому это скорее такой,
знаете, именно вот подколка, что на самом деле можно, но смысла вроде в этом особо нет. Да? Кого?
А, смотрите, у вас есть какая-то модель, правильно? Она отображает, допустим, объект в ответ, верно?
Ну, например, дерево. Оно вам умеет объект описывать в терминах вероятности меток классов.
Верно? Для каждого объекта, допустим, у вас там было 100-500 признаков, теперь вы для него, знаете,
вектор вероятности на три класса. Но чисто технически, вот вы, теперь умеете тремя чиселками
описывать каждый объект. Чисто формально, вот вам снижение размерности. С КНН-ом то же самое,
но это именно что, знаете, такой надо копаться. Ответ. Первая и вторая главные, обожаю этот ответ,
первая и вторая главные компоненты артагональны. В любом случае 49,5, только если исходные признаки
независимы, 46,2, прям круто. Ну что, признавайтесь, кто за то, что они в любом случае независимы? А?
А, люди не поняли, что это такое? Хорошо. Главные компоненты есть, нет главных компонент. ПЦА это да,
нет главных компонент. Principle component analysis, метод главных компонент по-русски. Первые,
вторые главные компоненты артагональны. Вопрос всегда или нет? Кто за то, что всегда? Нет,
это компоненты. Лекция номер четыре. Так, я понял, хорошо. Справедливо, в одномерном пространстве
вообще одна главная компонента принято. Размерность не меньше двух. Хорошо, коллеги,
давайте так. Кто такое собственные векторы, кто помнит? Что насчет остальных? Хорошо, классно.
Давайте, у нас есть с вами положительно определенная матрица какая-то, квадратная. У нее собственные
векторы какие? Есть, классно. Так, а если она еще и симметричная? Вот чего вы говорите. Собственные
значения какие? Действительные. А собственные векторы? Ребят, если у вас такая матрица,
то вы можете ее перевести в базис из собственных векторов, а она примет какой вид? Если в каком-то
базисе она имеет диагональный вид, что значит? Что все векторы базисные артагональны друг другу.
Согласны? То, что у вас не на диагонали стоит как раз и их скалярное произведение. Если они все равны
нулю, значит все что? Артагональны. Главные компоненты артагональны по определению. Потому
что мы с вами берем коверационную матрицу XTX, которая положительно определенная и симметричная,
и строим ее разложение. Прием. Смена базиса приводит к изменению скалярного произведения.
Переопределению, в смысле функцию другой выбрать. Что такое скалярное произведение?
Скалярное произведение это отображение из декартового произведения наше векторовое
пространство на самого себя в R. Согласны? Классно. Если мы его задали с вами каким-то образом,
например, просто вот дот-продукт классический по элементной перемножении и сумма, вас от смены
базиса чуть меняется, то у вас функция так и задается. Вам не надо чтобы... Любое линейное
отображение это поворот, растяжение, сжатие. У вас свойства артагональны не меняются.
Как-то не были. Были. Мы просто в их базис перешли.
X, XTX. Но чтобы она точно была положительно определенная, квадратно и симметрично.
Да, именно. А так как у нас СВД, метод главных компонентов, он именно что применяется,
по сути мы расложение строим вот этой матрицы XTX. Поэтому главные компоненты у нас всегда там,
во-первых, есть, во-вторых, они артагональны друг к другу. За исключением того случая, когда у нас
на самом деле эффективно размер пространства сильно ниже. Ну словно если вы плоскость запихнете
в пятимерное пространство, там будет две главные компоненты, остальные будут просто нулевые. Их нет.
У них будут нулевые собственные значения. Согласны? Так, давайте так. Сейчас вот задаю
вопрос в аудиторию. Тут что-то непонятно, потому что сейчас я начинаю теряться, что именно непонятно
еще надо разобрать. Смотрите, собственно, векторы, по-моему, вообще всегда артагональны или как бы
их нет? Нет? Хорошо, да, согласен. Согласен. Нет, да, на самом деле это правильное.
Вот, да, но мы всегда можем... Вот, вы правы, это, кстати, тоже хороший вопрос. Я имел в виду невыраженные
случаи, но на всякий случай. В каком случае у нас вот эти главные компоненты или, собственно, векторы
определены неоднозначно? Не с точностью до знака вообще неоднозначно. Вот вам типичный вопрос
собеседования. Ага, да, например. Может пример такой выборке привезти? Ну, я вас понял, но зрительно
проще, я думаю, людям воспринять вот так. Вот, у вас точки ровно на единичной окружности лежат.
Какие у вас здесь главные векторы? Фу, главные компоненты. Любая пара артагональных векторов,
вот вам две компоненты, в которых у вас все раскладывается. Нет, ну, смотрите. Хорошо,
по определению главные компоненты это направление наибольшей дисперсии последовательно,
которые артагональны друг к другу. Так что главные компоненты у вас всегда будут артагональны друг
к другу, если они есть. Почему? Потому что вы, когда проводите первую главную компоненту,
вы на нее проецируете все ваши точки и говорите, вдоль этого направления вся дисперсия объяснена.
Поэтому вторая главная компонента имеет смысл только в артагональном направлении, в других
направлениях вся дисперсия уже известна. То есть, по сути, теперь, когда вы вот на это расстояние
все спроецируете, у вас точки будут только как распределены. У вас будет распределение точек
вдоль вот этого направления. Во всех остальных направлениях дисперсии уже нет. Все, смысл нет.
Там проводить главную компоненту. Мне кажется, вообще потерялись. Особенно вот левая часть аудитории,
там уже тишина, все куда-то в телефон шли. Сложно, непонятно? Сложно вырубай.
Так, короче, что всем рекомендую сделать. Есть книжка, которая одна из рекомендованных.
Называется Deep Learning Book. В ней есть вводная глава, введение она называется. Там нет никакого
диплернига, там введение. Вот там есть кусок Полиналу. Там 25 страниц примерно. Бога ради,
прочитайте. Причем всем абсолютно полезно будет, даже если вы докой себя считаете, в Линале,
в Тиарвере, в Мотанее, в Машинки во всем сразу. Скажем так, я ее читал с удовольствием. Причем
год назад перечитывал. Просто написанный классный пример. И там все будет понятно. Можете читать,
вот сегодня у нас никакого нового материала нет, вот вам по сути домашним. Прочитайте эту главу.
Прям прочитать. Хорошо? Нет, главу Полиналу, конкретное изведение, там страница 25. Я не помню,
примерно сколько, может 40, но короче, за пару часов читается достаточно легко. За часа четыре,
если вы еще руками воспроизводите те выводы, которые там есть, всякие выводы. То есть, кроме как
это проделать или разобраться где-то по учебнику, хоть беклимишеву, хоть кого угодно, у вас другого
варианта нет. То есть, надо разобраться, это один из вариантов как. Он не единственный, но работает. Так,
хорошо. Что у нас там дальше? Логистические регрессии или 1 или 2? Кажется, мы куда-то уехали.
Первая, вторая, главный компонент. Короче, для того, чтобы у вас хотя бы что-то из этого вопроса
стало. Что такое главный компонент? Главный компоненты, вот по определению, это направление
наибольшей дисперсии в ваших данных. Всегда. И вы раскладываетесь по направлению наибольшей дисперсии.
По сути, вы выставляете базу сначала в направлении наибольшей дисперсии раз, вдоль него вы все объяснили.
Логично, что вдоль этого направления у вас дисперсии больше никакой нет, правильно? Логично, что вдоль
любого неортагонального этому вектору направлению у вас компоненты дисперсии, вот кое смысл по крайней
мере, будет нулевой, потому что здесь дисперсии никакой. Поэтому каждое следующее направление наибольшей
дисперсии будет по определению ортагонально всем предыдущим. Потому что иначе оно им не ортагонально и
просто поворотом на какой-то угол вы избавляетесь от вот этой вот космоса на ноль составляющей. Так что
главный компоненты ортагонально по определению. И второй вопрос для самопроверки. Он был на начале
прошлого занятия, там еще так забавно. Лекция обрезана в самом начале, надо будет, наверное, потом
поправить. Там начинается с фразы коллегии, звучит как Олег. Я сначала подвис, какой Олег и почему
лекция начинается со слова Олег. Это были коллеги. Итак, у вас выборка двумерная, например, ну или в
общем случае н-мерная, все отнормированное, все замечательно. Мы с вами взяли и построили
продолжение по главным компонентам и перешли теперь в базис из главных компонентов. Была
двумерная выборка, взяли две главных компоненты и в них описали наши данные. Внимание, вопрос.
У нас матрица X исходная и итоговая одинаковая или нет? То есть, говоря по-другому, у нас есть X
оригинальный, у нас есть PCA. Мы PCA обучили на X оригинальном и применили к X оригинальному,
получили X transformed. Это у нас краски PCA. Прям вот из калерновский вам интерфейс. Fit transform,
господи, как длинно. X ориг. И причем число компонент равно размерности пространства.
Верно ли, что вот это равно друг другу? Кто за то, что да? А в общем случае почему?
Бинго. В общем случае у вас информация не потеряется, а матрица будет другая.
Простей еще пример. Вот вам тот же самый эллипс, вот у вас исходная X1, X2. Где тут главные компоненты
будут? Вот так. В новых компонентах эллипс у вас внезапно ряжет на бок. У вас информация не
потеряется, вы все еще знаете все, что было изначально. Но вы повернули, по сути,
ваше пространство. Так что эти матрицы не совпадают. Да. Но у вас отображение, по сути,
артагональное на матрицу из значений, которые сжатие и растяжение вдоль каждой из главных
компонентов. А потом опять артагонально. Вот. То есть если вы оставили, ну короче,
сигму у вас осталось, то да вы просто повернули как-то. У вас по сути байлист стал теперь другой.
Вот. Так, тут вопросы еще есть? Где не было? Так. Да, сингулярное разложение делаем матрице X,
правильно? А это что такое? Мы можем с вами сделать, мы его делаем с вами через что? По сути,
через eigen decomposition, как это разложение по собственным векторам по-русски. Матрица краски
xtx. Берем и делаем разложение по собственным векторам матрицы xtx. Все. Конец. Потому что мы
хотим разложить матрицу, которая гарантированно имеет собственные вектора, байс собственных
векторов. Симметричную, не отрицательно определенную или положительно определенную. Слушайте,
я уже не понял. Она может быть выраженной, поэтому должна быть не отрицательно определенная,
по идее. Так, ладно, если компонент не более двух, понятно. Решающее дерево задача регрессии.
Предсказывает только константное значение в каждом листе. О, вот это, кстати, дурацкий вопрос в
плане, что, опять же, тут должны были быть все варианты ответа доступны, а не один из всех.
Так, давайте так. У вас силы еще есть, потому что ваши скучающие лица меня начинают... А? Чего?
Сложно? Два часа сложно. Ну, давайте тогда еще десять минут на оставшиеся вопросы и пойдем,
больше вас сегодня ничем грузить не буду. Идет? Давайте. Тут, как раз, интересный вопрос пошли.
Во-первых, ну, дерево предсказывает только константное значение в каждом листе. Согласны?
Согласны. На всякий случай есть куча различных способов придумать более хорошие модели,
и в том числе были статьи на тему того, что давайте в лист дерева запихнем линейную модель,
и там у нас будет не константа, а линейная модель. Такое бывает, я надеюсь, что вы понимаете сейчас,
что вы в листе можете на самом деле любую оценку делать, не обязательно константную. Верно? У вас
в листе просто лежит подвыборка. Какую статистику вы по подвыборке посчитаете, или какую модель вы
натянете на эту подвыборку, от вас зависит. Но так как обычно используют достаточно простые модели
просто в ансамбле, используют обычно просто деревья, где константы лежат. Договорились? Понятно? Супер.
Вот вопросы два и три. Ну ладно, третий, решающий деревень используется для решения задачи регрессии,
тут опять 12 процентов издеваются, видимо. Мисклик. Я надеюсь очень сильно. Или это шутка на тему того,
что не оценивается, значит напишем что угодно. Ну или если нет, то, пожалуйста, разбирайтесь,
что я могу сказать. Давайте с вопросом номер два разберемся, ответом номер два, который красненький,
разберемся и поймем, что имел в виду автор, то есть привет я. Марик, на самом деле вопрос
достаточно важный. Почему? Потому что линейные модели и деревья не вроде как идут в начале, там еще у нас есть
кнн, но с кнн попроще, там просто расстояние считается, там есть наивный байс, с ним тоже попроще, обычно там плотность,
чем дальше от моды мы находимся, чем она меньше. А вот с деревом не очень понятно. Смотрите, что у нас
делается линейная модель? Линейная модель, по сути, у нас выцепляет линейный тренд из наших данных, согласны?
То есть она примерно пытается хотя бы линейную оценку сделать нашей зависимости. Если у нас есть
какие-то квадратичные зависимости или более высокого порядка, мы их никаким образом не уловим. Почему я говорю
про более высокий порядок? Все помнят Ряд Тейлора, правильно? Вот у нас там есть линейный компонент,
квадратичный, третий, четвертый, пятый, так далее. Мы, по сути, с вами первый компонент пытаемся вытащить. Не обязательно, на самом
деле мы с вами можем учитывать и остальные, поэтому она как-то повернется, но тем не менее. Проблем в чем?
Проблема в том, что дерево не этим занимается. И с деревом нужно быть аккуратным в каком смысле.
Представьте себе, что у нас вот точки как-то вот так вот были накиданы, вот они вот здесь, а пространство у
нас все целиком такое. Давайте камеру повернем. Пространство у нас большое, вот оно прям широкое,
и дерево умеет хорошо различать наши точки вот здесь, где-то. Оно все вот эти разделяющие
гиперплоскости понастроит вот тут. Вот выпуклая оболочка наших точек, тех зелененьких. Вот здесь она тоже
будет делать какое-то предсказание, потому что здесь вот эта гиперплоска, например, последняя, вот, вот так,
вот так, типа это все еще к какому-то листу относится. Но при этом здесь дерево на самом деле
предсказывает какой-то бред, потому что оно просто не рассчитано на экстраполяцию за предел вот
этой самой выпуклой оболочки. Оно там никогда ничего не видело. Логично? Линейная модель, она хотя бы
линейно экстраполировать умеет. Вот вам тоже простой пример, вот у вас какая-нибудь штуковина.
Вот. Ну, короче, типа шумные данные. Вот у нас это ось X, это ось Y. Линейная модель у нас каким-то
образом построит что-нибудь вот такое. И даже далеко куда-нибудь вон туда она все равно будет хотя бы
линейный член, то что оно куда-то растет, линейный тренд имеет учитывать. Правильно? Дерево построит
что-нибудь вот такое. У кого ошибка здесь будет больше? Учитывайте, что дерево не умеет в экстраполяцию
за границы вашей имена обучающей выбраки выпуклой оболочки именно с точки зрения вашего
признакового пространства. Можно? Тогда будет. Линейный тренд, да. И смотрите, моя цель ровно в том,
чтобы вы это начали понимать на самом деле. Потому что то, что кто-то когда-то сказал, что дерево
ведет себя так, классно, честь и хвала автору, но при этом дерево это просто отображение. Это просто
способ поделить пространство на куски. Каждому куску соответствует под выбраку, каждый под
выбраке может соответствовать не только константной модели, любая другая. Другое дело, что сложную модель
туда запихивать дорого, вам дорого будет обучать. Но они сложные, может быть и полезны. По крайней
мере, линейные оценки, правда, в листьях пытаются делать. Ну, у вас в каждом листе, по сути так,
какая-то линейная модель задается. Ну, в регрессе, например, да, у вас будет что-нибудь такое,
тут вот так, тут как-нибудь вот так, тут вот так и так далее. Но это в таком случае. Если это
многомерно, это уже сложнее себе представить. По сути, у вас в каждом месте будет что-то, причем на
самом деле не обязательно, вот, обращу внимание, не обязательно, чтобы она у вас была вообще
непрерывная в таком случае. У вас абсолютно спокойно могут быть скачки. Не-не, краски здесь,
просто решающее дерево. Именно поэтому я ответство донсуну, то, что дерево само по себе, вот,
просто классическое, которое мы с вами разбирали, оно в экстраполяцию за пределы этой выборки не
умеет. Просто чтобы это понимали. Это не всегда происходит. И вот опять же вопрос для самопроверки.
Он немножко выходит за границей того, что мы обсуждали, но полезно. У нас временных рядов пока
вообще не было. Вот представьте себе, временной ряд, да, данные зависит от времени, биржа какая-нибудь,
или там не знаю. Количество студентов на лекции, как правило, экспедиционально бывает. Ну,
с кодом семестр, да. Ну как, оно, оно ведет себя как-то вот так. И потом примерно насыщается. Ну да. Ну
вот, оно, грубо говоря, здесь у нас какая-то асинтета есть, которая нормально. Чего? Ну,
это оценка на глаз, но хотите, могу проверить. На самом деле, у меня была такая идея, это, скажем так,
пафосная вещь, но пафосная в плане, абсолютно бесполезная, но пафосная. Сделать как раз какой-то
сайт-проект. У меня валяется пара один, пока живой, Jetson Nano. На него можно прицепить камеру,
ну и сделать штуковину, просто написать, набросать софт, который считает количество людей в аудитории.
Это, я не знаю там, дело полчаса. Вот, сначала следующего семестра поставить в элекционную аудиторию,
заставить его считать любопытно. Я вам больше скажу, к концу следующего
семестра, ну, я думаю, вы и так про это знаете, мы вам расскажем, как вы можете просто надеть
специальную футболку и парсеру станет вообще плохо, он найдет на изображении панду и, я не знаю,
и кого-нибудь еще. Да, и Майкла Джексона. А, вот это нечто, это вы про этот мемасик,
где распознавалка, короче. Была просто фотография, знаете, вот на камерах есть распознавание лиц и
так далее, да. А кто-то написал софт, понятно делал это. Я надеюсь, что это мем, где штука по лицу
примерно оценивает возраст. И там сидит автор довольно такой, его в вебке снимает, пишет там
примерно 27 лет и заднего в темном проеме двери еще какое-то лицо, она находит примерно 300 лет.
Короче, кот смотрит не просто в пустоту, вы поняли. Ладно, короче, с этим вопросом понятно,
почему я говорю про временные ряды. Вы можете подумать, что значит для прогнозирования временных
рядов деревья подходят плохо. Может прийти такая мысль, правильно? Неправильно. Почему? Потому
что зависит от того, в каком признаковом пространстве вы живете. Обычно временные ряды не в признаках оси
времени рассматривают. Там, как правило, переходят каким-нибудь дельтом за предыдущие значения,
например, значение 10 тиков назад, 100 тиков назад и так далее. Поэтому вполне возможно, что ваши все
новые данные находятся внутри выпуклой оболочки все еще. Поэтому не стоит где решающий деревьев
кидывать, зависит от того, каких признаков они живут. Просто вот такие уже тонкие вопросы,
о них сначала не задумываетесь обычно. Ну может вы задумывались, но средним нет. Ладно, мы почти все.
В случае наличия пропусков решающие деревья что? Продолжают работать. Честь им их вала, они просто
идут в левое правое под дерево, потом усредняем с теми весами, которые были на обучении. Все,
тут вроде комментарии излишни. Они не способны автоматически заполнить пропуски, они просто
делают оценку без этого знания. Причем, опять же, упражнение для самопроверки. Кажется,
я обещал вас уже отпустить. Сейчас отпущу. Упражнение для самопроверки. Что делает решающий дерево
на самом деле? Оно оценивает вам, по сути, выбороченное распределение, правильно? При условии
того, что тот или иной признак больше или меньше чему-то, правильно? То есть, по сути, у вас дерево
считает вам какое-то вот такое распределение при условии, допустим, х итой больше 5. Ну,
х итой, давайте верхний индекс, что это именно признак? Верно? По сути, когда вы усредняете левое
и правое под дерево, что у вас происходит? Вы просто-напросто выберете распределение при
условии х итой больше 5, при условии х итой меньше или равен 5 и их усредняете между собой. По сути,
вы от этого условия просто избавляетесь. То есть, вы получаете безусловно на это значение, на
этот признак распределения, вот все. Понятно, непонятно я сейчас сказал? Хорошо. В отличие от
линейных моделей, способность делать предсказания даже при наличии пропусков данных, да, не должны
применяться, должны. Формула энтропии. Ну, формула энтропии мы с вами вроде уже писали. Минус сумма по
И, П лог П. Ну П это и лог П. Нет, ну зеленый как бы зеленый ОК. Да, минус тоже потерял, это уже мой
косяк. Да, синий и красный, но тут как бы я не знаю, что сказать. Я надеюсь, что ошибок дальше не будет.
Так, и вроде почти все. В выборку добавили большое количество признаков, скоррелированных с одним из
уже существующих. Классический вопрос, на экзамене кому-то из восточных падет. Существенно
повлияет на процесс построения ансамбля типа boosting из деревьев решений, никак не повлияет на
процесс построения ансамбля типа boosting из деревьев решений. Но игнорирую вычислительную
сложность большинство за то, что никак не повлияет. Обосновать можете в двух словах?
В boosting есть под выборки? Именно, если у вас просто boosting, то одному дереву по барабану игнорирую
вычислительную сложность. Ансамбль типа boosting. У вас есть bagging и есть boosting,
это две разные вещи. Bagging будет плохо, на самом деле bagging опять же все равно.
Bagging со случайными подпространствами будет плохо, boosting и дифферентно. Одному дереву
и дифферентно, boosting деревьев тоже и дифферентно. Согласны? Одному дереву почему и дифферентно?
Оно выбирает оптимальный признак. Если у вас признак, который был продубилирован оптимальный,
выберет будет выбран любой из них, правильно? Разбиение сохранится на под выборке. Если
добавленный признак не оптимальный, он просто не будет выбран и все. Нет, boosting это не bootstrap,
а не bagging. Boosting это то, что мы с вами на прошлой лекции разбирали. Хорошо? Так, тут понятно,
едем дальше. Дерево работает, дерево каждый признак рассматривает по отдельности и просто
жадным образом выбирает оптимальный. Поэтому если у вас там куча скоррелированных, оно просто
выберет один из оптимальных, если их несколько, и продолжит работать. Ему все равно на их
взаимодействие. Boosting каждое дерево строит по отдельности после других, поэтому это сохраняется.
При построении random forest, там случайное подпространство признаковое, там может быть
проблема, потому что в него может попасть только этот признак, например, тогда дерево только на
этом признаке. Это бесполезное дерево. Boosting над линейными регрессиями обсуждали. Смысл особо
не имеет, похоже на половину пирога какой-то. Позволяет получить нелинейную разделяющую
поверхность над линейными регрессиями. Нет. Позволяет избежать переобучения. Нет. Не имеет смысла. Да.
Градиенты Boosting требуют от функций потерь. Быть дифференцируемой, быть ограниченной, быть
гладкой. Так, ну ограниченная вроде нам ни к чему, более того у нас куча квадратичных функций потерь.
Ограничена? Нет. Уже не работает. Быть гладкой. Вообще по бравон на самом деле нам в каждой точке
нужна производная и всё. Нам больше ничего не надо. Так что дифференцируемая должна быть. Гладкой? Нет.
Ну и всё, вопрос кончились. Я вас поздравляю, вы достаточно уверенно и отлично ответили на
вопрос. Я надеюсь сегодня вам удалось разобраться каким-то нетривиальным моментом. В этом и была
суть данного занятия. Спасибо за внимание. Домашка прочитать голову про линейную алгебру.

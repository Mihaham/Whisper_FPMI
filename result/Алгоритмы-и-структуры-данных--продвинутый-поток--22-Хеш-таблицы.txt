Так, ой, ой, я спорил шпаргалку, да? Ну ладно, кстати, познакомьтесь. Ладно, ну, в общем, не то чтобы я ее собирался скрывать, собственно, это наша литература, да, это вот конспирты Эхилл Кихомира. У меня вот есть таких три, собственно, все три мы с вами уже обсудили.
Ну, пока можно, так что вот туда. Ну, тут действительно, да, на самом деле замечательно написано, на самом деле.
Ставили против ума.
Так, а, и еще, конечно, одна важная у нас будет литература. И, соответственно, вот такая.
Введение. Ну, как вам сказать, введение, смотрите, да. Ну, возможно, кто-то из вас не сталкивался.
Потому что тут, конечно, выбраны такие немножко оригинальные темы, да, то есть, когда вот, там, введение, сортировки.
Вот, то есть, плейдинг. Нет, кстати, вот, ну, в принципе, да, то есть, например, здесь есть доказательства, когда там потенциал вершин, это логарифм округленный вниз.
Вот, тут они, как бы, там писали такое вот доказательство напасть. То есть, да, нам с вами оно не помогло, мы и, там, изучили что-то более трагичное.
Ну, также вот здесь можно было прочитать про лучше всех. Поэтому, говорю, да, точно, да, точно вы не сталкивались с этой книжкой.
Потому что я почти наверно в прошлый месяц его бы уже и планировал.
Вот. Ну, и нас сейчас будет интересовать вот это, конечно. Вот. Вот этот вот красивый азиат.
Так что, вот, Бобенко-Левин, собственно, в ведении Тюрелка ритма по черепу. Ну, да, книжка просто берется и будет интересно, что.
Вот. Так, теперь давайте, а мы с вами откроем презентацию. Так, что это? Что у нас-то? Количная куча.
Бла-бла-бла-бла-бла-бла-бла-бла-бла. Да, обычно, ну, скажем так, долгая презентация долгое время жила на первом курсе.
Я это все обсужал на первом курсе. Значит, как-то там подумал, что лучше, конечно, наверно, США обсуждать, когда вы все-таки знаете теорию.
Вот. И, действительно, сейчас будет все такое, что очень интересно.
Ну, давайте начнем с, конечно же, простого. Вот. Что вообще такое сэштаблица? Да. То есть, ну, в чем смысл США?
То есть, смысл в том, что мы хотим выполнять там операции типа добавление и удаление поиска там читерским методом,
вида, что по каждому значению, по каждому ключу, который мне придет, я вычисляю какой-то там супер хэш.
Ну, вот. И по этому хэшу пытаюсь его найти. То есть, ну, там самая тупая идея.
Какая у нас для строчек возникает, например, могла быть идея, да? Или от строчек или чего-нибудь еще.
То есть, у нас есть какие-нибудь, то есть, такой вот, или даже не для хэшей, а вот самый простой пример.
Допустим, мы хотим реализовать вот сет, вот такой вот урезанный сет с этим вот, на числа до 10 и 18, да?
Вот. Тогда какая возникает идея? Ну, давайте, то есть, такая вот, то есть, такой флаг.
Если мы знаем, что числа накидываются там условно рамтомно, да, то тогда идея могла быть такая.
Давайте введем m равно, ну скажем там, миллион. От балды, да?
И будем, для каждого числа вычислим его остаток отделения на миллион и что-нибудь с этим сделаем.
Вот. Ну, собственно, что, ну вот, ну там, что именно там? Ну, разные варианты могут быть, но там основная идея может быть там,
создадим, например, создадим миллион списков, где в каждом списке будем творить все элементы,
у которых там остаток отделения на m равен, например, там то, что надо, да?
И тогда, если числа гондонная, то, в принципе, тогда поиск это, ну там, получается, во сколько-то в миллион раз сокращает.
То есть, если у вас чисел порядка миллион, то это вообще в среднем получается около 1.
Ну, можно о том и о другом подробнее, да? То есть, точно, потому что тут, на самом деле, можно по-разному разговаривать.
Потому что, да, то есть, можно, конечно, ввести еще такое понятие, как вот коэффициент заполнения, он же вот фактор, да?
То есть, это просто вот, то есть, говорим, сколько мы сравним х-таблицы элементов и сколько, и пусть х-таблица имеет размер l.
Тут вот очень интересно, потому что сейчас у нас вот, вот как вы думаете, альто больше единицы или меньше единицы?
Ну, опять, на самом деле, и так, и так бывает. Все зависит от того, какого вида х-таблицу вы реализуете.
Ну, потому что, да, в идеале, конечно, хотелось бы, конечно, хранить так.
Заводим массив размера m, и в ячейке номер i храним элемент, хэш которого равен i.
Ну, или есть такого элемента нет, то ничего не храним. Все работает, все чимно и хорошо, все работает за от единицы, пока не обнаружится два, случайно не обнаружится два элемента, у которых хэш совпал.
Эта ситуация называется коллизия. Причем действительно замечается, что тут она достаточно быстро растет.
Потому что, ну, вот, возможно, мы уже с вами обсуждали такой парадокс нерождений, то есть, который звучит, такая теорема звучит так, что, предположим, что мы проигнорируем бесслокостные года и рассмотрим там, там, рандомность n людей.
И посмотрим, верно ли, что у них там найдется два человека с одинаковым день рождения.
Ну, заметим, кажется, что мы, по идее, редко встречаем людей, у которых с нами одинаковый день рождения.
Но если брать, ну, если брать конкретно с нами, то, как бы, то есть, да, вероятность того, что я возьму рандомного человека, у него там, совпадет день рождения, равна там один делик на 365.
Но если мы будем рандомных n человек, и будем спрашивать, сколько совпадает, там, найдут ли два человека между ними хотя бы, то, оказывается, эта вероятность растет быстрее.
Скажем, вот, примерно, где-то, вот там, ну, ну, 20 или чуть больше, чем 20, если взять, то эта вероятность уже превысит даже одну-вторую.
Ну, на пальцах можно убедиться так, смотрите, то есть, мы, да, то есть, на пальцах, конечно, нам сейчас очень сложно посчитать вероятость, по крайней мере, в уме, там, как я ж, в питоне это можно сделать сомгновенно, вот.
Ну, а вы можете посчитать мат ожидания. Ну, давайте посмотрим. Вот, если у нас n человек, каково мат ожидания там пар человек, у которого совпали дни рождения?
N, наверное, зелень пополам, умножить на...
Какова вероятность?
На 1,65.
Совершенно верно. Совершенно верно. То есть, получается, да, что если n это где-то порядка корни из этих 3,665, да, то вероятность уже где-то, то там, мат ожидания уже на вторая.
А если там n взять чуть больше, ну, там, я не знаю, сколько там взять, сколько там надо, вот, если не 20, то, например, если взять день, мы там, ну, скажем, ну, от балды так 30, например, взять, то сколько там получится?
900 поделить на это, то есть больше единицы. То есть, вот, то есть, уже в среднем будет больше единицы, значит, скорее всего, там, значит, как минимум, видимо, вероятность того, что коллизии, там, меньше одна вторая, там, по-видимому...
Так, у нас есть, кстати, на эту тему неравенство, Маша, вот, интересно.
Неравенство.
Что?
Неравенство.
Нет, ну да. Нет, ну, а вот давайте вспомним, нам-то по-любому пригодится. Вот давайте с этим немножко поиграемся.
Нет, ну, может быть, просто аналогично доказывается, потому что, смотрите, у нас есть, действительно, такой тупнячок, да, что, действительно, допустим, что у нас есть неотрицательная величина, да?
Тогда мы знаем, что вероятность того, что звездок C больше, чем какая, больше, чем какой-то, там, я не знаю, х, значит, она меньше либо равна, чем это надо ожидание поделить на х.
Это я написал неравенство Маркова, да?
Ну, то есть, понятно, что это эквалентно тому, что х на P от этого х больше х меньше либо равно от ожидания х, а берется оно откуда?
Ну, потому что, практически, действительно, мы берем, то есть, мы берем вот это вот распределение, там, какой-то, этой вот х.
Ну, вот, и здесь, соответственно, все, что больше х оцениваем х.
Ну, вот, то есть, там, допустим, вот это вот все, типа, оцениваем х, то есть, сюда вот прижимаем, ну, вот, а это все прижимаем сюда и получается меньше.
Ну, понятно, что меньше, да?
Вот.
Так, а теперь вот возникает вопрос.
А если я хочу, вот, интересно, не верно ли, что вероятность того, что х меньше, чем игр, меньше либо равно, чем, условно, там, игр умножить на вот это х?
Странно вообще.
Чего?
Агим и немец.
Агим и немец, ну, если в нот.
На отождание, на операцию.
Так.
Чего еще раз?
Нет.
А теперь, берем, как санит события.
Так.
Так.
Понятно.
То есть, хорошо.
Нет, ну, мы что, вероятность оцениваем?
Так.
Хорошо.
А.
Это равно 1 минус п кси больше либо равно игр.
Так.
Но это п теперь надо снизу тогда получать.
Больше равно.
А, ну.
Это же мы хотели.
Нет.
Нет.
Нет, я хотел наоборот.
Предположим, что мы от ожидания 1.
Как бы, можем ли мы сказать что-нибудь типа, что вероятность
того, что случайная личина меньше 1.2, она там, я не знаю,
1.2 или меньше.
Мне вот это интересно.
Ой, тут что-то мне раньше чем-то шоу вспоминать надо что-нибудь.
Там, это там дисперсия там всякая, вот это все.
Ну, ладно.
Так, ну ладно.
Не получилось, не получилось.
Не важно.
Ладно.
Снизу, да.
Сверху можно отсекать, снизу нельзя.
Ну, вот.
Хотя, хотя, да, наверное нельзя, потому что, ну, оценивать
что там, вероятность того, что случайная личина меньше
1.
Типа, обязательно меньше 1.
Нельзя, потому что у вас, как бы, может быть случайной
величины, вероятность 50% 0, вероятность 50% 2.
Так, значит, что можно делать с коллизиями?
Вот.
Ну, на самом деле, если нам очень хочется, действительно,
с этими коллизиями как-то бороться, да, потому что,
ну, то, как бы, какие есть варианты?
Значит, есть два варианта.
Один подходит для, там, load фактора больше 1.
Другой подходит для load фактора меньше 1.
Итак, о том и о другом подробнее.
Так.
Ну, например, метод цепочек.
Ну, это такое, видимо, просто первое, что вам приходит,
приходит в голову, когда говорят хэш-таблицы.
То есть, на каждом потенциальном значении хэша висит односвязанный список.
Односвязанный список всех элементов с таким хэшом.
Вот.
С таким хэшом.
Что это означает?
Ну, вот.
То есть, тогда получается, что...
Ну, то есть, понятно, что если коллизия всего лишь означает,
что у вас будут цепочки больше одного элемента.
Ну, в общем-то, если вы гарантируете, что у вас гарантирует...
Если, там, во все время везет их в цепочки не более чем 5 элементов,
то, в общем-то, вы не спрятаете.
Вот.
Ну, то есть, метод работает, ну, конечно, понятно.
Худше в случае...
Ну, то есть, да. То есть, как добавлять?
Добавляете у нас вообще за вот единицы,
потому что в начало списка можно за вот единицы добавить, да?
А вот если мы ищем какой-нибудь...
Ну, вот.
Ну, вот.
Ну, как бы, единственное, конечно, может быть, еще вопрос,
не требуется ли она наличие дупля, да?
То есть, там...
Потому что, может быть, вы хотите поддерживать сеп,
и вам хочется, прежде чем добавлять, узнать,
а нет ли у вас такого элемента.
Тогда это получается от единицы плюс поиска.
Вот. Ну, удаление, ну, тут, из вариантов.
Просто бежим по соответствующему хрешу, приближаемся по списку
и, там, найденный элемент удаляем.
То есть, совсем, если нам будет фантастически не вести...
Ну, например, у нас там плохая хреш-функция.
Теперь, там, хреш от элемента равен нулю.
Это тоже хреш-функция.
Хреш-функция.
Вот. Но дебильная.
Вот.
Соответственно.
Ну, и поиск, соответственно, тоже.
Ну, тут, в общем-то, думаю, все тривиально, а тут, в общем-то, известно.
Кстати, интересно, вы писали, пытались ли вы писать что-нибудь подобное?
Мы написали, только у нас там был один большой двусоветный список.
И, типа, массив гитараторов на начало каждого батика.
Ну, это детали реализации именно, а не элементы.
Ага, нет.
Ну, я вас помню.
Нет, я даже имел в виду не это, да.
Ну, список вы, конечно, писали.
Да, но вот олимпиаду вам не пригодилось.
Свой писать?
Зачем?
Нет, все.
Олдерец спасал всегда.
Воль меня, да.
Тут отдельно, потому что он вообще не так часто нужен, знаете?
Ну, да.
Точнее, на самом деле, он чаще всего не спасал.
Спасал, обычно, вместо него, или МАП.
Они даже быстрее могут работать.
Ну, там есть всякие ускоренные штуки, там, типа, БТС.
Они быстрее МАПов работают.
Ну, да.
Это все здорово.
Значит, смотрите.
Ну, и теперь, если говорить, на самом деле, о медленной секунде,
то о какой у нас среднее время работы?
Ну, в принципе, можно попытаться сказать, что да, у нас.
Допустим, в некотором смысле, конечно, что мат ожидания времени работы,
это на каждую операцию от 1 плюс альфа.
Почему?
Ну, просто потому что говорим, что...
Ну, правда, теперь давайте внимательно посмотрим,
какое тут собирательское пространство.
Давайте внимательно посмотрим.
То есть, видим, что...
Ну, вот.
Что, как вы видите, тут, на самом деле, подразумевается,
что у нас еще элемент, который нам приходит,
у него называется хэш рандоме.
То есть, во-первых, каждое значение хэша для каждого элемента
может выпасть с равной вероятностью 1 деликатем.
Соответственно, причем более того, у двух разных элементов,
там, собственно, сэши берутся независимо.
Понятно, да?
Ну, по крайней мере, так, по крайней мере, интуитивно хочется сказать.
Да, ключи равновероятные, хэш-кунса равномерна,
чем эти слова сочетания отличаются, непонятно.
И получается вот такое мат ожидания.
Ну, то есть, видим, что для каждого элемента реальное время работы
это 1 плюс там сколько таких элементов в этой цепочке, правда?
Вот.
Ну вот, то есть, этого, ну вот.
То есть, заметим, что тут неважно какие длинные,
главное, что сумма этих длин по любому n,
и в результате получается 1 плюс i.
Кстати, в связи с этим действительно возникает естественный вопрос.
Действительно, какими вероятностями мы пользуемся?
Пользовались ли мы тем, что
каждый хэш,
каждый хэш генерируется с вероятностью 1 деликатем.
Ну да.
То есть, заметим, да, мы тут прям добьемся.
Но, просто, на самом деле, да, интерес к другому.
А пользовались ли мы с вами тем, что хэши генерируются независимо?
А что значит независимо?
Ну, то есть, ну, мы как бы подразумевали.
Ну, как?
То есть, что для каждого элемента,
независит от другого?
Ну да, вот внимание вопроса.
Мы этим где-нибудь, вот тут мы этим воспользовались?
Нас не бы преднамерно дожидали.
Нас столько моего спросили.
Мы говорили, что у нас вероятность для того,
что мы пойдем в каждую, в каждый из m вариантов,
один из m вариантов,
один из m вариантов,
один из m вариантов,
каждый из m вариантов одинаковый.
Для этого не нужна независимость хэши или распределения.
Ну да.
То есть, нет, вот, обойдите внимание, да.
То есть, мы говорим, что вот у нас там,
то есть, мы там сверкаем время поиска обработки ключа.
Вот это вот.
Вот ожидание вот этого очередного ключа.
Почему оно равно?
То есть, мы говорим, что с вероятностью 1 делить на m,
у него значение 0, 1, 2, 3 и так далее.
Если ему равно i, то время работы у него будет 1 плюс ni.
Вот.
И дальше, то есть, мы выполнили просто абсолютно честное,
просто алгебрическое преобразование.
И требующая независимость.
Вот.
То есть, на самом деле, да, неплохая новость.
То есть, на самом деле, получается, нас интересует,
то есть, нам только требуется, чтобы вероятность каждого хэша была...
Ну вот.
То есть, для вот такому утверждения требуется,
что среднее время работы у нас получается...
То есть, требует только того, что каждый хэш генерируется равноверно.
Вот.
Каждый хэш генерируется равноверно.
Вот.
Но, правда, заметим, что может быть...
Вот.
Ну, хотя, да.
То есть, получается, тогда от ожидания получается вот такое.
Если каждый хэш от каждого элемента генерируется равноверно.
Вот.
Ну, в принципе, да.
Выглядит тут вот...
Тут вот некоторая реализация, да.
То есть, вот.
Ну ладно.
Это вам уже бесполезно показывать.
Вы уже круче можете.
Там с интераторами, там все дела.
А теперь пойдем...
Ну вот.
Это была, так сказать, хэш-таблица с закрытой адресацией.
А теперь мы исследуем хэш-таблицу, которая подходит для load-фактора меньше единицы.
То есть, идея в том, что мы храним массив длины m.
И в этом массиве все элементы хотим искать.
Вот такая идея.
То есть, ну, идея...
Ну, самое тупое, что можно было реализовать, это для каждого, значит, элемента в строчке мы вычисляем хэш
и кладем элемент в соответствующий элемент по хэшу.
Спрашивается, что делать...
Значит, там теперь вопрос.
А что делать, если мы...
Вот здесь произошла корридия.
То есть, происходит элемент, и мы его также попытались положить в соответствующий хэш, а он занят.
Все делать.
Ну, один из базовых вариантов предлагает следующее.
Так, смотрим, хотим, класть вот в эту ячейку.
Так, занято, кладем в следующее.
Тоже занято.
Дальше, дальше, дальше, дальше, дальше.
Как только найдем пустую, собственно, вот радует.
Это нас приводит к чему?
Ну, тогда получается...
То есть, тогда...
Ну, это называется, как можно сказать, последовательностью проб.
Вот.
Ну, правда, конкретно у такой штуки тогда будет проблема, что...
То есть, ну, как бы, в логике проблем.
Хорошо, вставлять умеем, но тогда искать тоже придется, когда вычислили хэш, ищите до ближайшего пустого места.
Ну, если там сразу пустое место, то нам повезло, но если не пустое, то как бы идем, идем, идем, проверяем, проверяем, проверяем.
Вот.
И отдельная песня, как удалять.
Как вы думаете, как удалять?
Ну...
Как говорится, это, конечно, тоже.
А если хэштаб лить, то как удалить?
Нужно все, что сдвинулось из-за него, нужно еще найти назад.
То есть, надо для каждого элемента хранить, что там из-за него сдвинулось.
Это да, это странно.
Это больно.
То есть, поэтому приходится там извращаться, типа, самые тупые варианты там.
Просто тупо поменьше пишем, да и идите.
То есть, типа, самого элемента нет, ну, типа, дальше проверяем, потому что могла быть проблема.
А если странно слишком много литов, то перестраиваем, что ли?
Ну, типа, да.
Вот такие вот развлечения начинаются, но...
Да, но тут разные варианты есть, на самом деле.
Потому что, смотрите, дело в том, что я вам сразу скажу, да?
Да, вот про удаление это мы сказали.
Ну, просто вот говорим.
Да, давайте я вам в общем случае скажу, потому что этот метод можно обобщить.
Вот.
То есть, как бы, мы говорим так, что давайте мы ведем не только хэш-функцию,
но на самом деле, то есть, будем считать, что хэш, на самом деле,
генерирует по каждому элементу не один хэш, но перестановку чисел от m.
Перестановка нам будет говорить, в каком порядке, собственно, нам элементы тыкать.
Вот.
Ну, то есть, например, в нашем случае, наверное, оказывалось,
что каждое следующее это как предыдущее плюс один.
Ну, понятно, да? Циклический из них.
Ну, могут быть и другие варианты.
Ну, потому что действительно, наверное, легко представить, что как-то,
если у вас каждое следующее это предыдущее плюс один,
то, конечно, у вас там будет достаточно быстро процветать пластеризация, правда?
Ну, в том плане, что будет большой отрезок,
а чем больше отрезок, тем с большей вероятностью туда что-то будет попадать,
и он будет расширяться, расширяться, расширяться.
Вот.
Ну, и да, тут есть такой недостаток.
Так, ну, это вот все эти, все детально обсудили, да?
Вот.
Но есть...
Вот, да, ну, вот это я вот как сказал, то есть, вот это h и k, это вот,
обычно хэш плюс и, но вот есть пластеризация.
Вот.
Ну, как бы, ну, вот.
Но, на самом деле, бывает, конечно, продвинутая версия, квадратичное предъявление.
Вот.
То есть...
Вот.
То есть, в принципе, к пластеризации такой нет,
но есть так называемая вторичная к пластеризации,
ну, которая заключается в том, что, если у вас есть один ассистент,
то вы по одинаковым сообщениям бегать будете, правда?
Вот.
То есть, соответственно.
То есть, ну, практически это очень похоже на цепочку медленных цепочек,
только там с возможностью пересечения.
Поэтому есть еще веселые варианты.
Не-не-не, нет, это не в смысле там комплексное хэширование.
Нет.
Ну, так просто можно это и так воспринимать.
Но в данном случае нет.
И это все-таки нормальный такой, это индекс от 0 до M-1.
Ну, просто для каждого.
То есть, ну, тут хотя бы приятно, да,
что даже если у хэшей совпал первый хэш,
то тогда, скорее всего, там будет вероятность того, что совпадут прямо оба,
она все-таки уже один деликат на M квадрат.
То есть, вероятность этого крайне мала,
поэтому там они все-таки, там дальше пойдут бегать все-таки по разным местам,
и все-таки есть надежда, что будет адекват.
Вот.
То есть, видим, что, видите, количество последовательств,
что получается от M квадрата, не M, да,
то есть, какие-то наши возможности расширяем.
Ну, отдельная там песня, конечно, за какой интуит,
какой-то вообще будет работать в среднем.
Ну, у нас тут есть теорема без, ну, вот,
ну, вот, отдельно, то есть, без доказательства,
соответственно, есть вот такое интересное впечатление.
Эх.
Осталось только вспомнить.
Это называется прирублено, да.
Прикрая оно, прикрая оно,
при каком нет и где проверке, но, видимо, прирублено.
Получается такая красота, которую вот надо знать.
Ну, красота здесь действительно простая, да.
Такая, что, да, то есть преимущество, конечно, метода,
что нет этих ваших указателей, динамической памяти
Но с другой стороны, оказывается, что hash таблицы иногда бывают заполнены.
Но с другой стороны, ладно, то есть мы уже изучаем адепты вектора и прочих других представителей математического анализа,
поэтому расширение и сужение это для нас не такая точная проблема.
Но, правда, приходится заморачиваться.
Но, с другой стороны, как мы знаем, там unordered mobcast этим всем тоже занимается, если что.
Но, правда, единственная проблема, что и сальса стремится к единице, то время работы стремится тоже к не самой приятной вещи.
Ну, хотя, да, вот это вот, а, ну это вот то, что мы сейчас обсуждаем, да-да-да-да-да, вот это вот все.
Ого. Да, тут вот есть конкретные рекомендации, но думаю, как бы, думаю, общую суть вы понимаете.
Вот. Так. Ну, собственно, так, это я вот действительно в быстром темпе прогнал.
То есть вот просто какие-то вот общие слова, общие знания, которые просто надо знать.
Общие такие, понятно.
Так.
А теперь мы с вами попробуем, значит, теперь ключи к математике и делать что-нибудь реально прикольное.
Если не круто, то прикольно.
Значит, смотри.
Чем мы с вами, значит, какую глобальную задачу мы вообще с вами попробуем решить?
Так. Ну, помимо того, что стереть вот это вот все естественно.
Так. Ну, это простая задача.
Как там называется, как это, да, можно, конечно, там употребить слово глина, но это простая задача.
Так вот.
Значит, какую мы сейчас задачу с вами попробуем неожиданно решить?
Значит, задача будет такая.
Мы хотим создать, так сказать, идеальную статическую хэштаблицу.
Хотя бы статическую.
Работать это будет так.
Давайте представим себе, что у нас есть числа.
X1, X2, X3 и так далее, Xn.
Это целые числа.
Ну, давайте, например, там не происходящее, но я не знаю, 10, 18.
Вот. Ну, допустим.
Вот.
Значит, наша задача создать, значит, да, вот это множество статическое.
То есть оно вот заданное изначально, но не меняет.
Нам очень хочется, желательно его не сортируй я.
Нам нужно создать хэштаблицу.
То есть некую структуру данных, которая в этом множестве умеет делать операцию x.
Ну, то есть существует ли x в этом списке, да или нет?
Причем, внимание, у нас суть быть неожиданная.
Мы очень хотим, чтобы Xist работал за O от единицы.
Причем O от единицы честно.
То есть без всякой амортизации и без всяких вероятностей.
То есть как бы хэши хэшами.
Но нам очень-очень-очень захочется действительно сделать это так, чтобы отвечать на запрос потом с вероятностью 100%.
При этом построение у нас будет работать за O от е.
Внимание в среднем.
Вот.
Понятно?
Да нет, наверное.
Вот.
То есть вот такая вот идея.
То есть насколько это нереально?
Вот.
Ну вот.
Насколько это нереально?
Ну, давайте попробуем покидать какую-нибудь такую, значит, рандомную ситуацию.
То есть давайте вот действительно поанализируем.
То есть так.
На уровне идеи, на самом деле, одна из первых идей будет базироваться на следующем.
Вот давайте попробуем создать.
Вот пока на O от n забьем, но предположим, что n не сильно большое.
То есть числа большие, но n не сильно большое.
Тогда хэштаб лицу можно было бы создать следующим образом.
Давайте создадим мегахэш функцию по модулю n квадрат.
Вот возьмем и создадим.
Предположим, что нам фантастически, что там как-то хэш выбирается действительно для каждого элемента случайная равновероятность.
И еще и так, что потом еще адекватно вычислить можно.
Тогда у нас будет идея такая.
Вот, ну предположим, то есть у нас есть какой-то генератор, который генерирует, который дает нам какую-то хэш функцию так, чтоб там вероятности какие-то были.
А теперь скажите, пожалуйста, каково математическое ожидание коллизий тогда будет?
То есть получается, мат ожидания количества коллизий, оно равно n на n минус 1 пополам на n квадрат, это меньше, чем одна вторя.
То есть хэш это означает?
Это означает, что когда вы генерите хэши, то с вероятностью не менее чем...
Ну тогда вот по неравенству Маркова отсюда следует, что вероятность того, что есть хотя бы одна коллизия, эта вероятность тоже не превосходит одной второй.
Ну просто, я просто по неравенству Маркова, правда?
То есть получается, в принципе, если у нас есть возможность создать хэш таблицу n квадрат размера, то тогда мы просто генерируем хэш функцию и пытаемся по этой хэш функции этот массив ну типа заполнить.
Ну при заполнении как бы понятно, что мы идентифицируем коллизию, если мы попытались положить элемент в какую-то ячейку и опа, в этой ячейке оказывается уже все есть.
Понятно, да?
То есть отсюда получается, ну вот.
И оказывается, что с вероятностью одна вторая никаких проблем нет.
Осталось только взять вопрос, откуда такие красивые хэш функции брать?
Ведь у нас же, у хэш функции как бы фишка не только в том, что она рандомная, но еще и в том, что ее как-то просто вычисляет.
То есть данный элемент, мы ее можем детерминированно вычислить.
То есть конечно пока это работает только в предположении, что у нас есть какой-то мега-ораку, который там внутри себя хранит, значит для каждого элемента как-то умеет в себе хранить какое-то значение и он их как-то независимо генерирует, да?
Вот.
Значит, поэтому, значит, ну придется, значит, например, попроводить чуть более аккуратный анализ, во-вторых, что-то поводить.
Вот. Ну, например.
Да. Вот давайте я, значит, теперь сформулируем то, значит, то, что мы уже говорили.
Значит, вот есть такое понятие, как гипотеза простого равномерного хэширования.
Гипотеза простого равномерного хэширования.
Гипотеза простого равномерного хэширования.
Она будет нам говорить о том, значит, она будет говорить первое, что для любого ключака, вот так,
допустим, х от к будет у нас равномерно распределена.
А что это в смысле? Что значит равномерно распределена?
Это означает, что в некотором смысле х выбирается в некотором смысле случайно.
Ну, то есть, опять же, знаете, можем пока вовлестить себе оракул, да, как он работает отдельной песней,
но предположим, что у нас происходит две вещи.
То есть, во-первых, во-первых, у нас происходит, что, значит, аш от к равномерно распределена.
То есть, как бы, то есть, вероятно, то есть, для любого ключа, верно, что у него там, вероятность того,
что х будет равен 0, 1, чего угодно, дм минус 1, равна 1, 9 на m.
Ну, то есть...
И еще, а еще мы скажем, что для любого к1, значит, к2, уточним, неравных, конечно,
утверждает, что аш от к1 и аш от к2 должны быть независимы.
Обратите внимание, это не такое общее утверждение, как кажется.
Потому что, как вы уже знаете из теормера, то, что каждые две случайно влечены по парной независимой, не значит, что они независимы глобально, правда?
Было такое, да?
Вот.
Вот.
То есть, вот такая вот есть гипотеза.
Ну, вот.
Ну, может, например, такое...
Для разминки можно сформулировать, скажем, утверждение 1.
То есть, условно, смотрите, то есть, можно так сформулировать.
То есть, средняя длина цепочки, там, в гипотезе такого, ну вот, при гипотезе вот так равномерного хэширования, да, ну, имейте в виду в методе цепочек, конечно.
Вот.
Ну, значит, при гипотезе вот этого простого равномерного хэширования, давайте вот так вот.
Уж посмотри, КПРХ.
Да, КПРХ тоже.
Вот.
Ну, допустим, не превосходит 1 плюс, ну, вот это вот N9M.
Вот.
А, вру.
Еще-еще прочее.
Я хочу сказать, что она просто равна N делить на M.
Причем, заметим, средняя здесь не совсем так.
То есть, понятно, что если я возьму все цепочки и возьму среднюю арифметическую длину, то получится, конечно же, N делить на M.
Это неинтересно, да?
Но здесь утверждается немножко круче.
То есть, здесь можно рассмотреть так.
Рассмотрим какой-нибудь элемент K и посмотрим, в какую цепочку он попал.
Так вот, математическое ожидание длины этой цепочки, соответственно, N поделить на M.
Так если у нас элемент попадает случайно в каждую из цепочек, то это и так видно.
Угу.
Ну, в принципе, да.
Ну, хотя, хотя нет, хотя тут надо аккуратно.
То, что я сказал, на самом деле, по-хорошему, заметим, что в какую бы цепочку он не попал, себя-то он в ней точно посчитает,
а каждый из остальных элементов попадет туда с вероятностью N-1 делить на M.
Ну, это в такой формат видно.
Да.
Ну, вот.
То есть, вот так, фиксированного элемента K.
Ну, естественно, при положении, ну да.
Вот так.
Ну, тут разные можно проводить, действительно, такое.
Вот.
Вот.
Ну, и, соответственно, там, бла-бла-бла, бла-бла-бла.
Значит, дальше разные варианты.
Теперь вот возникает вопрос, как же нам, значит, теперь возникает вопрос, как же нам эту крышкунцу генерирует.
Ну, генерируйте ее, оказывается, можно так.
Смотрите.
Значит, то есть, на самом деле, то есть, идея будет в том, что вот как достичь вот такой вот красивой гипотезы.
На самом деле, достичь ее можно, да, если от один из вариантов, если у вас есть семейство хэш-функций, из которых вы рандомно выбираете душу.
Понятно, да?
Ну, вот, например.
То есть, один из вариантов мог бы быть, допустим, таким.
То есть, да, ну вот.
Ну, там разные могут быть варианты.
Но давайте так скажу.
Значит, тут предлагается ввести такое семейство, значит, красивое хэш-функции.
Итак, значит, будем говорить следующее.
Значит, пусть у нас, действительно, аж это семейство хэш-функций,
хэш-функций,
которые отправляют нас из множества ключей наших вот в этот вот сладостный, хирующий и упоительный мир.
0, 1, бла-бла-бла и минус 1.
Понятно, да?
Ла.
Допустим.
Так вот.
Так вот.
Ну, давайте так.
Мистическое определение.
Значит, мы будем называть это семейство универсальным.
Оно будет называться у нас универсальным.
Универсальный.
Там.
Если...
Я так напишу.
Для любых K1, K2, лежащих в ключах и не совпадающих.
Вот.
Верно.
Вот, значит.
Сейчас эту вероятность просто напишу в комбинаторном смысле.
А комбинаторный смысл такой.
То есть количество таких функций h в этом семействе,
что h от K1 случайно совпало с h от K2,
не превосходит, как вы уже догадались, общего количества числа функций на m.
О.
Вот.
Ну вот.
Так что вот такая вот красота у нас получается.
Тогда...
Заметим, что вот что-то очень похожее на гипотезу просторного равноверного хэширования
как-то вот что-то намекает.
В предположении, что хэш-функцию мы будем из этого множества выбирать случайно и равновероятно.
То есть действительно.
Ну то есть у нас, конечно, независимость.
То есть здесь, конечно, говорилось, что h от K1 желательно равноверно распределить,
потому что h от K2 желательно равноверно распределена должна быть.
Но с другой стороны, есть вероятность того, что хэши совпадут.
В общем, по большому счету, давайте проанализируем вот это утверждение.
Средняя длина цепочки какого-то конкретного элемента K, да?
Средняя длина цепочки.
Вот давайте подумаем.
То есть какая она?
Ну, заметьте теперь, что у нас там есть n элементов.
И получается, что каждый элемент будет иметь вероятность того,
что у него хэш совпадет с нашим, на самом деле не превосходит 1 делить на m.
Правда?
То есть поэтому здесь тогда получается, что эта средняя она получается даже меньше,
чем 1 плюс вот это вот.
И плюс, правда?
В чем? Обратите внимание, мы там, ну, или что-то смущает?
Смущает что-то?
Или пока все нормально?
Какое смысл?
Так, ну давайте, давайте.
Давайте.
А мы как тут делаем?
Мы 1 раз уберем хэш функцию и будем пользоваться и все время из семейства?
Да.
Ну, точнее так.
То есть алгоритм будет работать так.
Выбираем случайную хэш функцию из семейства.
И будем вот метр цепочек забабахивать.
И тогда обратите внимание, да, у каждого элемента тогда получается среднее время работы, это что-то типа 1 плюс m делить на n.
То есть у каждого кайфа нот. Причем, заметьте, то есть вероятность генерируется уже нами и нашим семейством.
Обратите внимание, они каким-то там арахулом, да? Ну, точнее, мы изобрели этот арахул.
Точнее, если мы изобретем такое универсальное семейство хэш-функций, конечно же.
Это, видите, это как бы чисто комбинаторная задача.
Вот.
А неравенство можно чувствовать?
Неравенство?
Да.
Ну, смотрите, давайте так.
Ну, вот 1 это, ну, здесь я имел в виду следующее.
Допустим, вот k у нас находит, там, отправляется в какую-то цепочку, да?
Тогда какова его длина этой цепочки?
Пишем, что она 1, это потому что сам элемент там, да?
Плюс сумма по всем, значит, всем добавленным вот этим вот, значит, то есть всем остальным каитам не равно k, да?
Значит, фактически вероятность того, что h от k случайно совпало с h от k.
Логично, да?
Вот.
То есть, ну, замечаем, что, ну, то есть это вот в качестве мат ожидания, да?
Ну, что такое длина цепочки, да?
Это количество элементов, в которых хэш совпал с нашим, правда?
Вот.
По-этому мы это вот, аккуратно суммируем, и эта вероятность равна 1 делить на Н.
То есть, получается, это 1 плюс там всех остальных элементов 1-N, ну и там поделить на Н.
Да, получается, может быть даже где-то равенств?
Вот. То есть, можно было 1 делить, то есть, можно, вот прямо, равенству вот такое написать,
Ну и там более аккуратно можно написать, что если мы делали не сколько элементов предположений, что наш элемент тоже находится,
а просто какой-то рандомный элемент, мы хотим его вставить, мы хотим его проверить или мы его хотим удалить.
То вот аналогичным образом мы показываем, что среднее время работы получается это от 1 плюс длина цепочки, а средняя длина цепочки на m.
Ну а единица просто на то, чтобы вычислить хэш, вообще хоть о чем-то подумать, добавить элемент.
Почему мы не можем вставить меньше кибарону?
Ага.
Ой, а не знаю почему, видимо что-то по-моему пока получается, что просто радость то и все, действительно.
А, ну просто мы в общем вот это вот, то что универсальность мы пока не пользуемся.
Пользуемся.
Ну потому что фактически вот это означает, что для любых двух элементов вероятность того, что у них хэши совпадают, не превосходит 1 делить на m.
То есть мы вот этим начали пользоваться.
То есть как бы да, то есть раньше у нас вообще психологически было, что там просто какие-то числа абсолютно независимо генерятся.
Ну а теперь просто задача хэш-функций сделать так, чтобы они там почти независимо генерировались.
Ну то есть есть у них вероятность того, что они совпадут будет еще меньше, чем 1.
То есть типа мы забиваем на то, что там вот равномерный хэш, и он бы сказал, что вероятность того, что это равно 57, а это 179,
там в принципе равна 1 делить на m квадрат.
Тут мы на это забиваем, нам как бы главное, чтобы они как бы совпадали с вероятностью не более чем 1 делить на m, и остальное нам уже наплевать.
Вот.
Ну а теперь.
Ну вот.
Теперь мы искать вопрос.
Как такое универсальное семейство хэш-функций сгенерировать?
Как это сделать?
Значит, случай easy.
Если m...
Какое дальше слово напишу, как вы будете?
А у нас m это что, это то же самое, что и маленькое?
Да.
Ну простое, наверное?
Да, конечно.
Если m оказалось еще и простое,
то тогда сгенерировать х прощепарим на репы.
Х просто равно...
Значит, смотрите, такое семейство функций, как...
Значит, ax...
Ну вот.
То есть, допустим...
Ну, вот тут так.
Значит, обычно генерировать ax плюс b процент m.
Значит, внимание.
B лежит на отрезке 1, m-1.
И b лежит на отрезке 1.
То есть, от нуля до m-1.
Если бы еще m было достаточно большим, вообще бы мокрее.
Опа.
Так что вот, допустим, вот такое вот...
Да, это сгенерировать будет, действительно, да.
Вы просто говорили, что этот 1, m-1 это время добавления элемента?
Добавление и удаление поиска.
Добавление и удаление поиска.
Типа того.
Ну вот.
Ну, за счет чего тут хочется такое говорить?
То есть, смотрите, давайте предположим, что...
То есть, размер такого h у нас получается какой?
m на m-1.
А теперь давайте думать.
В скольких случаях...
Вот, допустим, нам даны чиселки x1 и x2.
И в скольких случаях у нас окажется, что действительно ax1 плюс b равно ax2 плюс b?
Ну, правда, по модулю m, конечно.
В скольких случаях это окажется?
Это у нас эквалент к тому, что a на x1 минус x2 сравнимо по модулю m с нулю.
Так.
Да, ну и вообще вытекает ощущение, что...
Так, сейчас, пока вот вытекает ощущение, что действительно такого не произойдет никогда.
Если x1 и x2 не сравнимо по модулю m, то это произойдет всегда.
Ну да.
Тогда у нас действительно возникает мелкая проблема, что если они совпадают по модулю m, то...
Значит, хеши заведомо совпадают, причем в таком контексте мы с этим ничего не сделаем.
Но если они не совпадают, то как будто получается, что...
Да, что как будто они просто заведомо никогда совпадать не будут.
Ладно.
Ладно.
Давайте тогда попробуем более сильные вещи.
И смотрите.
То есть у нас был вот этот, конечно, чит.
То есть это бы, конечно, работало, если бы все х были от нуля до m-1.
А мы теперь пойдем по другому.
Давайте скажем, что теперь у нас, значит, продвинутая версия будет такая.
Мы заведем простое число p.
Которое будет больше всех вот этих вот m, x1, x2 и так далее, xn.
Вот прям вот на сток.
Понимаете, да?
И h у нас сейчас будет неожиданно следующая версия.
Значит, ax плюс b процент p и результат уже будет процент m.
a и b, естественно, по модулю p.
Ну, правда, единственное, конечно, у вас будет чит, что вам придется инклюзить этот ваш int 128, конечно.
Ну ладно, вы уже, так, вы умеете писать не только хэш функцию, там это хэштаблицу, но и длинную арифметику.
Или нет.
Умейте писать хэш функцию.
Умейте писать длинную арифметику.
Я этого бы и не сказал.
Си-си-си.
В смысле?
Чтобы писать длинную арифметику, надо исполнить хэш функцию.
В смысле?
Чтобы писать длинную арифметику, надо использовать длинную арифметику.
Нет, в принципе, здесь я использую так.
Ну в смысле?
Ну как бы, если у вас число на 10 и 18, то, ну, смотрите там, допустим, простое p вы как-нибудь найдете.
Ну там понятно, что, ну как минимум, потому что, там, ладно, есть теория, вообще бы, шова на тему того, что между 10 и 18 и двумя на 10 и 18, наверное, есть простое число.
Ну не наверное, а точно.
Там, скорее всего, наверное, там в гугле вам даже еще скажут, какое конкретно.
Их там, скорее всего, больше.
Вот.
Но просто, нет, проблема будет в том, что, когда вы будете выполнять вот это умножение, то у вас будет там просто переполнение лонголга.
Ну, то есть, либо и 128, либо используйте свою длинную арифметику.
Ну или там, я не знаю, притом еще может.
Не, не в тот случай как-то выдумался.
Ну да.
Не, ну мало ли, может вы, может, нет, я ж не знаю, может вы вообще джаваи, я не знаю.
Не, ну мало ли, может вы там какой-то чуть-чуть подавдройте, я не знаю.
Всякое бывает.
Не, кстати, у вас джавы нету?
Нету? А будет?
Нет.
Нет, потому что вы не говорили.
Обязательно точно нет.
А, ну дальше, видимо, зависит от того, вы в математике или кто?
Да я вроде, мне кажется, я ни в каком расписании не видел.
Совсем-совсем я не знаю.
Понятно.
Ну, понятно.
Не важно.
Значит, смотрите, давайте думать.
У меня есть вопрос.
Вот если у нас с этим планетой это неравенство, да?
Что ли?
Боба К1, К2, там не больше, чем айшельт на М.
Где-то в разуме связано, что это должно быть строгое равенство везде?
Ну, потому что у нас просто, понятно, М значений хэш-функций.
И если на каждое такое неравенство, то суммарно должно равенство получаться.
Что-то такое.
Ну, конкретно нет, на самом деле нет.
Потому что, ну, сумма вероятности того, что, нет, в смысле, ну, потому что на самом деле это просто может быть то, что,
как бы, равенство должно было быть, если бы я писал бы тут, что для любых Т1, Т2, верно, что вероятность того, что аш от К1 равно Т1, а аш от К2 равно Т2,
вероятность было бы, не происходило бы там аш поделить на М квадрат, тогда это было бы равенство.
Но здесь мы, как бы, просто не все события ставим.
Вот, ставим.
Хотя, такая равномерность, вообще-то, здесь будет иметь место.
Вот давайте посмотрим, почему вот эта штука все-таки адекватна, да, почему она универсальна.
Она универсальна, вот почему.
Потому что давайте найдем количество таких функций, то есть таких вот АВ, ну, вот адекватных, я не буду сейчас от 1 до P-1, и тут от 0 до P-1,
таких, что АХ1 плюс B процент P равно T1, и, допустим, АХ2 плюс B процент P равно T2.
Вот, сколько таких А и Б вообще можно набрать?
Да, вот эта вот штука, заданное число от 0 до P-1, и вот эта штука от 0 до P-1.
Вот сколько таких пар А и Б я могу подобрать?
АХ1 зафиг фиксированная.
Да, ну, что-нибудь вот из, ну, допустим, АХ1, АХ2, даже вот эти, я тут могу, конечно, АХ2, АХ3 дописать.
Давайте я допишу, чтобы.
АХ1, АХ2 различные.
Ну, что, сколько? Сколько у вас там получилось?
Ну, сейчас, ну, А восстанавливается не более, чем мы назнаем, если Х1 и Х2 различные.
Ну, они различные.
Ну, да. Ну, потому что мы можем, типа, вычесть, получить, что А на И75 равно T2.
Ну, да. То есть, можно просто, если это заметить, что А равно T2-T1 поделить, так сказать, по модулю на Х2-Х1.
Ну, и там вроде так. Вот. Ну, естественно, деление по модулю. Ну, и там В равно, понятно, чему.
Единственное, только надо подлянка, что желательно, чтобы А не оказалось.
Только единственная проблема, что А может оказаться, что если T2 равно T1, то, как мы уже поняли, этого просто не бывает.
Вот. Поэтому это количество, скажем так, меньше либо равно 1. Вот так скажем.
Или оно равно, то есть оно равно, вот так правильно написать, 1, если T1 не равно T2, и 0 иначе.
Неплохо, да?
Угу.
А теперь давайте посмотрим, чтобы у нас, давайте, количество таких h, то есть таких h, что h от X1 равно h от X2.
Да? То есть, это количество чему равно?
Ну, заметим, так как X1 и первое не равно, значит, X и второе, допустим, они не равны, тогда получается, что у нас, значит, T-шки по модулю P могут не совпадать, правда?
То есть, они должны не совпадать по модулю P, но должны совпадать по модулю M, правда?
Вот. Ну, тогда получается, что я должен перебрать потенциально, допустим, все остатки по модулю P.
И для каждого остатка сказать, что если вот, даже не h, вот, допустим, я все вот эти T1 переберу, и теперь я должен подобрать количество соответствующих T2.
То есть, я должен найти количество таких T2 от 0 до P-1, что T1 не равно T2, но T1 сравнимо по модулю M с T2.
Понятно, что я сделал? Так, ну давайте разбираться. Что у нас тогда получается? Что такое T1 и T2? Ну вот.
Ну их строго меньше, чем по 9 на M, для каждого T1.
Ну, давайте так, можно даже в точности посчитать, что для каждого T1 посчитать сколько таких. То есть, ну на самом деле, то есть, вопрос, ну, там, действительно...
Ну, что случится от 0 до P-1, который... Да, имеют, имеют такой остаток. Это да.
Сколько их? Ну их, очевидно, на самом деле, вот так надо написать. То есть, количество таких же остатков, то их, на самом деле, очевидно, P делить на M, то есть, но каждого остатка есть, как минимум, вот столько раз.
Вот. Плюс еще, конечно же, значит, вот эта вот штука. То есть, плюс один еще, если T1%M, значит, строго, допустим, меньше, чем P%M, и еще не забыть, конечно, вот.
Вот. Или вот.
Минус один, потому что... Да, и еще, конечно, не забыть минус один, потому что совсем уж совпадать не должно.
Так. Ну, то есть, видите, это слагаемое где-то больше, чем, там, по 9 на M, там, все, наверное, там, где-то больше, где-то меньше. Ну, давайте аккуратно посчитаем.
Ну, заметим, что вот этого, ну вот, ну, на самом деле, можно уже написать, конечно, что-нибудь типа, там, P на, там, P поделить на M минус один, плюс вот эта вот единица встречается у нас сколько раз?
Эта единица встречается у нас ровно P%M раз. Логично, да?
Вот. А что бы нам хотелось? Нам бы хотелось, чтобы это количество, то есть, вернули, что вот это вот количество не превосходит P на P-1 поделить на M.
А почему?
Сейчас, да, просто. Ну, в общем, что такое, в целой части, поделить на M? Это просто дробь поделить на M минус P...
Да? То есть, действительно, заметим, что вот эта штука, да, если проявить, это равно P на P-M минус P%M, по большому счету, поделить на...
Ну, еще минус один, там, минус.
Еще здесь минус один. Плюс P%M.
Ну, вот, да.
Так, и... Так, вот, смотрите.
Ну, по-моему, достаточно даже значительно получается.
Что-что? Ну, вот.
Ну, давайте так. Получается, пока P на P делить на M минус 1 минус P%M умножить на P делить на M минус 1.
Что? Что больше 0. То есть, можно...
То есть, равно P минус P%M умножить на P делить на M минус 1. Ну, не совсем. Видите, мы с P на P-1 сравниваем P делить на M.
То есть, ну, допустим, вот P-1 делить на M мы берем отсюда. Ну, в принципе, да, можно сказать, что это не превосходит P, это не превосходит P-1 делить на M.
А это правда? Нет.
Что?
Это равно P минус M делить на M.
А, не больше. Ну, да.
Не больше, конечно. Так, ну, что, вроде сошлось, да?
Да.
Вроде не сошлось, правда. Что-то меня спущает, что у нас с таким запасом. Ну, окей.
Ну, мы просто вообще забили на то, что мы считаем не отрицательно ослабляем, и на него просто 0 оценили.
Сейчас мы себя оценили. Я до этого момента вообще предельно чинил.
Ну, по сути, мы сделали что в реальности. Мы сказали, что то, что у нас при P по модели M, оно нулём просто оценивается.
И да, и этого типа нам хватило.
И это нам хватило.
Ну, не совсем. Ну, правда, тут у нас... Нет, ну... Не, ну, не, не, по-моему так...
Слишком легко?
А, ну, хотя да. Нет, ну, да. Нет, ну, я сейчас поглядю, конечно.
Хе-хе-хе-хе-хе. Да, хотя...
Ну, нет.
А, ну, они, правда, видимо, не поскорее, что они тут, видимо, проводили доказательство, что, допустим, зафиксируем одно значение T1.
А, значит, теперь давайте посмотрим там вероятность того, что вторая совпадёт, и окажется, что оно там не более чем то, что нам надо.
Вот. Ну, окей. Хорошо.
Значит, вот такое hash у нас есть.
Мы теперь говорим, что вероятность того, что кто-то там совпадёт, теперь не превосходит...
То есть там, что у разных элементов два hash-асов падут, это у нас вероятность теперь не превосходит, оказывается, один делить на M, обратите внимание, да?
Чем не превосходит, может даже меньше.
Вот.
И теперь заметим, что если бы у нас была возможность создать hash-таблицу размера M2, то мы бы это создали...
Ну, то есть, допустим, вот M2, да?
Нет, M2.
Так.
Пока вот M2.
Ну, то есть, допустим, если бы у нас была возможность создать таблицу размера M2, то есть M было бы там порядка M2, то, получается, мы вот такими hash-функциями могли бы пользоваться.
Потому что, обратите внимание, на M у нас никаких ограничений нет.
Вообще никаких. То есть, теперь говорим, что если M порядка, скажем, M2 какого-нибудь, то есть мы говорим, что M равно M2, и завести вот эту hash-функцию.
То есть, завести hash-функцию, получится, что с вероятностью не менее, чем 1,2, она вообще никаких ограничений нет. То есть, вообще идеальное хреширование.
Вот. Понятная идея?
Вот.
Вообще идеально бы сразу.
Но, конечно же, настолько за M2 мы, конечно, не хотим.
Ну, то есть, мы просто знаем, что в случае чего мы можем ее найти.
Причем, от ожидания построения этой hash-функции было бы там вот это вот M2 плюс потом O от M, согласны?
Вот.
Поэтому смотрите теперь, собственно, какой чит.
Значит, как мы будем строить теперь идеальную hash-таблицу линейного размера?
Да, она еще и по партии будет линейная, как вы уже догадались.
Как же это мы будем делать?
А вот.
Мы сделаем двухуровневую систему.
Ну вот.
Значит, система будет такая.
Значит, уровень 1.
Уровень 1.
Значит, пишем M равно N.
Вот, понятно, да?
Добиваемся того,
что сумма по всем i от 0 до M, до M-1,
длина вот этой вот цепочек, суммы квадратов длины вот этих цепочек,
не превосходит
ну, допустим, O от M в некотором смысле.
То есть, в некотором, ну, у нас там будет что-то типа 4, M6 и M-шотова собираются.
Вот, уровень 2.
Внутри каждой цепочки
длины
длина этой цепочки.
Ну, то есть, смотрите, мы же можем сгенерировать х-функцию
и за O от M просто построить цепочки, просто построить, да?
Это мы делаем тупо за O от M буквально, да?
Мы можем их построить, посчитать их длины,
возвести эти длины в квадрат, просуммировать.
Так вот, мы будем ждать того, что сумма квадратов этих длин не будет превосходить 4N.
Потому что уровень 2 теперь будет говорить нам так, что
внутри каждой цепочки
делаем, объявляем N равно Nит в квадрате.
Вот такая красота.
Ну, заметим, что вот это вот мы делаем в среднем за O от M.
Ну, потому что каждая цепь...
Потому что один раз мы построили...
То есть мы добились...
Один раз мы построили вот эти вот массивы квадратичных размеров
и заполнили их там чем-то там минус единичками суммарно за O от N, да?
А потом внутри каждой цепочки мы делаем там O от единицы
генерации уже внутренней х-функции, да?
И внутренней х-функции проверки нет ли там количества.
Вот, если есть количество, генерируем еще, если нет, то останавливаемся фиксируем.
То есть таким образом мы с вами генерируем N плюс одну х-функцию.
То есть как бы одна глобальная первоуровневая, а на втором уровне мы генерируем N х-функции.
Красиво, правда?
Так, понятно, что происходит?
Или тут уже совсем черная мальчика?
Что сейчас мы внутри каждой цепочки делаем?
Внутри каждой цепочки мы говорим...
Вот у нас есть N и T там элементов, да?
Я генерирую на них вот такую же х-функцию, но N у меня равно теперь уже N и T в квадрате.
Вот так вот сделаем.
Так что, может быть, да.
Ладно, давайте допишем, нам еще придется, наверное, N квадрат еще сюда допустить.
Хотя, может, и не надо.
Потому что если все элементы меньше P, а P будет меньше, чем N квадрат, то как бы любая х-функция будет идеальной.
Так что можно было N квадрат не дописывать.
Но не важно.
Да, давайте выбираем, не дописываем.
Ладно, смотри.
Генерируем.
Теперь алгоритм понятен?
А теперь остается только выяснить его симпатику.
То есть он работает в среднем за O от N плюс O от N умножить на сколько нам потребуется, прежде чем произойдет вот эту.
Понятно, да?
Ну, так, ну, а, ну тут уже поверили, что каждую такую штуку мы будем генерировать от единицы раз в среднем, да?
Нет, не поверили?
Ну, смотрите.
Ну, базируется все на том, что мы знаем, что с вероятностью не менее, чем одна вторая, нам фантастически повезет, правда?
Понимаешь?
Ну да.
И тогда получается мат ожидания количества итерации.
Вот эту вот кси.
Какой у нас?
Оно на самом деле равно, значит вероятность того, что повезет, вероятность повезет на что-то, в том числе, что это фантастически повезет.
Итерацией. Вот это вот кси. Какое у нас? Оно на самом деле равно. Значит, вероятность того,
что повезет. Вот давайте, пусть P это вероятность того, что повезет. Вот так вот, я даже не шукался.
Вот. Тогда это получается, тогда сколько раз мы будем, то есть фактически мы кидаем монетку.
С вероятностью P она выпадет, с вероятностью 1 минус P не выпадет. Спрашивается, сколько средним
мы будем ее кидать, прежде чем она нам выпадет. Тогда мат ожидания работает так, с вероятностью P это
будет 1, а с вероятностью 1 минус P это будет 1 плюс. В общем, то же самое. Это мы расписываем
количество итераций. Вот здесь, да. Вообще мы просто говорим, пока даже в общем случае. То есть мы уже
вычислили с вами, что с помощью простого мат ожидания и неравенства марка, что вероятность того,
что нам повезет не менее чем 1 вторая. То есть когда мы кидаем тут монетку, нам с вероятностью не менее
чем 1 вторая, она выпадет. Повезет это что? Ну, в данном случае это означает, что как бы коллизий не найдется.
И мы так думаем, за от и на это быстро определяем, коллизия есть или нет. Вот. А теперь пока просто давайте
проверим. Пусть у нас есть P, вероятность того, что повезет. Тогда вот нам от ожидания выполнять вот
это уравнение. Ну, это равно 1 плюс 1 минус P на вот эту штуку. В общем, отсюда следует, что х равно банально 1
делить на P. Если P равно 1 вторая или меньше, если P это больше либо равно, чем 1 вторая, то есть при
P больше либо равно, чем 1 вторая, это меньше либо равно. То есть в среднем мы будем делать две операции.
То есть там вероятность, то есть получается вероятность того, что мы там сделаем 10 операций,
это уже не превосходит. Получается там 1 и пятый. И это только так от полтышной оценки. Там чисто
теоретически можно заморочиться, наверное, вычислить дисперсию и нарисовать хуйненький равенство
чебушова. И там может получиться еще круче. Вот. И так. То есть это если так. Ну, в принципе,
вообще заметим, что если P больше либо равно хотя бы 1 делить на хуйню константу K, то тогда мы
делаем O от K и итерации в среднем. И нам остается финал. Ой, умирай. Все, у нас финалочка.
Как бы сейчас это, финального босса тут с этого победим и домой пойдем. Так, кстати, где финальный
босс? Строим совершенно, да, это называется идеальное эфиширование, кстати. Да, это официальное
дросслайне. Вот что мы делаем. То есть мы пытаемся доказать, что этот алгоритм будет работать за O от M.
А для этого нам получается надо найти вот мат ожидания суммы вот этих квадратов. Ну, надо
ее раскрыть, скобки списать, наверное. А как это? Ну, что такое квадраты? Вот давайте так,
пишем. Это 1 плюс 1 плюс загадали в квадрате, да. Ну, вот так, сейчас вот N и T в квадрате. Так, да, что это такое?
Ну, во-первых, там понятно, вылезает просто N, ну типа это сумма 1 квадрата. Да, но это равно
мат ожидания, давайте я пока напишу в сумме. Да, но можно на самом деле написать так, я вот экзотически пишу,
это N плюс 2 в сумме, перебираем по всем ж, да, ж неравным, и конечно. Почему, если мы перебираем по неравным,
тогда 2, наверное, не нужно набрать. Ну, я думаю, что мы переберем пару. Что бы получился квадрат,
мы каждую пару должны два раза вычесть. Ну, просто смотри, вот допустим. А если мы учитываем ее как i и ж, и как ж и это раз,
и мы не читаем уже два раза пару. Да, действительно. Спасибо. И то, что они оказались в одной цепочке.
Да, то есть это называется, то есть как мы говорим, что h от x и равно h от x и. Так, понятно это, откуда мы вообще это заклинание взяли?
Так, ладно, я тут пропустил один шаг. Давайте пропущенный шаг вот такой. Я утверждаю, что вот это вот.
Это сумма по всем i равно от 0 до N минус 1. Сумма по всем ж от 0 до N минус 1. Ну, вот этого,
индикаторные величины равны вернори, что h от x и равно h от x и. Так, вот с этим согласны?
Ну да. Ну, просто смотри, рассмотрим вот эту сумму. А, то есть там пары такие-такие. Да, но заметим, что в этих парах мы учли пары элементов, которые вот сам собой совпадают.
Таких N, поэтому тут выплыло. Вот. Так, теперь. Ну, заметим, что это равно N плюс, ну, мат ожидания этого получается, да, пишем N на N минус 1.
Ладно, меньше либо равно. N на N минус 1. Ну, это, что тут пишем. Ну, вот, что это N на N минус 1 поделить на, в данном случае, N.
Потому что, как бы, мы помним, что оказалось, что спиральность того, что два различных х неожиданно дадут один и тот же хэш, у нас не превосходит 1 делить на N.
Ну, потому что у нас M равно N для 200, правда? Но это равно, ну, то есть шлёп-шлёп, 2N минус 1. То есть тупо меньше, чем 2N.
Ой, поэтому, да, считаю, вот. Чёрт, память сработала. Хотя, сколько лет, честно говоря, сколько лет это не попадало в программу.
Нет, ну, знаете, просто так произошло. Просто, как бы, мне, я помню, как-то из последних раз я просто пытался рассказать это на первом курсе, для этого пришлось вводить, собственно, директорское пространство, мне не понравилось.
Вот, поэтому решил оставить хэш напоследок, но в итоге у нас там несколько лет не хватало времени. Ну, вот у нас с вами время оказалось вагон, поэтому, вот, наконец, мы вспомнили об этой замечательной красоте.
Ну, да, вот про двое я правильно вспомнил. Почему я правильно вспомнил? Потому что теперь мы замечаем, что неравенство Маркова нам сообщает, что вероятность того, что тут будет больше 4N не превосходит 1 и 2.
Ну, и, соответственно, то есть это означает, что это тоже мы будем генерировать от 1 раз.
То есть мы просто будем брать рандомную хэш-функцию и делать вот это, пока не получится так, что...
Да, сумма квадратных цепочек там не происходит 4N.
После этого вы для каждой цепочки генерируете массив на ее длину в квадрате и заполняете его нулями. Это тоже вы теперь делаете за линию.
Ну, и после этого внутри каждой цепочки вы уже генерируете свой локальный идеальный хэш на квадратичном размере.
Ну, там на практике оказывалось, что действительно там что-то фантастически быстро все летает.
Вот. Так что вот. То есть оказывается, да, вот оказывается, да, не такая ушибать я даже оказался.
То есть, на самом деле, в общем, я думаю, там написать код, вот это вот, то есть это вам, конечно, не там...
Ой, не сувечная идея его написать.
Да, сувечная идея тоже не очень сложная.
Ну, да, да, да. А, ну ладно. Ладно, рядом с Атами Кхипом, да.
Или там... Или Соф Кхипом. Но это другой вопрос.
Нет, ну вот это, пожалуйста. То есть, если вам на контесте понадобится хэш-функция, то...
Да, в принципе, это можно заметить, что, да, на самом деле, как бы, да, то есть, если вы хотите хэшировать там строки, я не знаю, да, то, возможно...
Ну, вот. То там, наверное, возможно можно почетерить и брать там какое-нибудь простое число и генерировать там, допустим, полимерные хэши по модулю P, например.
То есть, по модулю P... Ну ладно, вы там скажете, что P, конечно, не будет больше, чем там все эти полинамеральные строки там, что-то.
Но уж пока я нот. Но будем там... Там попробуем, наверное, свято поверить, что там как-нибудь алиниса в платформе.
Хотя на реальном контесте, в общем-то, вам, в общем-то, и логинчик на построение такой хэш-таблицы, в общем, вам не помешает.
Ну что, понятно?
Ну, кстати, заметим, что да, то есть, ну да, осталось только, то есть, честно говоря, к сожалению, у нас, видимо, так и останется за кадром, а как эту штуку динамизировать?
Потому что я могу динамизировать только с доможением за логарифом.
Ну, то есть, там идея такая, у вас есть хранитель логарифом хэш-функции, в каждой хэш-функции N это степень двойки.
Если у нас есть N-лог, то зачем использовать хэш-таблицу, если можно?
Ну вот да, но тут и проблема, да, если у нас есть N-лог, давайте уже честно реализуем наше любимое красночерное дерево.
Или даже вспомним, что мы его уже, что эстрель уже сделал эту зону.
Так что вот, но, тем не менее, вот такая красота есть.
Так, ну что, если тут не так...
Да нет, наверное, нету.
Слушай, ладно, пока пришло время официально объявить, что наш курс закончен.
Этот грустный момент наступил. Не то, чтобы мы с вами не увидимся.
То есть, да, мы с вами увидимся на экзамене.

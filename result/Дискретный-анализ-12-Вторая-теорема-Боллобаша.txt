Так, ну всё, давайте продолжим нашу деятельность. Как у вас дела вообще?
Феместор заканчивается, да? Всё уже. Учиться трудно. Но давайте попробуем. Довольно сложная у
нас сейчас тема. Идёт вот эти хроматические числа и концентрация. Но мне кажется, что в
прошлый раз была очень красивая теорема. Всё-таки прям вообще мега крутая. Сегодня она тоже будет
очень красивая, но она просто сложнее с точки зрения своего, так сказать, технического
доказательства. Значит, она за сегодня, скорее всего, и не докажется. Ну ещё раз я её напомню. Это
ещё одна теорема Балабаши о том, что если вероятность ребра случайного графа равняется одной
второй, мы находимся в таком случае, когда все графы совершенно одинаково возможны, одинаково
вероятны, то существует такая функция phi, она у маленькой от n на логариф m, а симпатически
почти, наверное, модуль хроматического числа случайного графа минус вот это вот не превосходит
phi t. Это утверждение, которое нам нужно в каком-то виде доказать. Ну, там какие-то будут, как всегда,
пропущенные кусочки, наиболее технические, чтобы просто вас не убивать совсем уж. А вот
идейно хотелось бы понять, что делается. Ну, смотрите, я вот в прошлый раз напоминал, что вот эта
пункция, которая стоит в знаменателе, мы про неё знаем, что альфа её не превосходит, а симпатически
почти, наверное, при p равном одной второй. Я напоминал, это надо держать в голове. Ну, я не знаю,
вот здесь вот где-нибудь напишу. Мы знаем, что вот такое вот неравенство выполнено симпатически
почти, наверное. Сейчас мы докажем, что противоположное неравенство не просто выполнено,
но противоположное близкое к вот этому. То есть, справа будет стоять функция, которая очень похожа
на 2 лог 2 ич на n, но всё-таки её поменьше. И вот с этой функцией уже будет выполнено противоположное
неравенство, причем в очень-очень сильной форме. Но для того, чтобы это сформулировать красиво,
аккуратно, так, чтобы это зашло, чтобы вас проканало, нам придется технически чисто,
вот для каких-то целей, которые я проясню позже, аккуратно выбрать некоторые параметры.
Ну, разумеется, как обычно, все эти параметры могут быть представлены на официальной шпаргалке
во время экзамена, поэтому не пугайтесь тому, как это можно запомнить. Например, я введу м очередное,
у нас уже м было с вами. Знаете, когда? Когда я вам доказывал теорему про жадный алгоритм,
там тоже было м. Оно будет похоже. Так, вот такой вот м. У нас n стремится к бесконечности,
м будет стремиться вместе с ним, но при этом, видите, вот эта функция, она уподобна фи. Она
бесконечно мала по сравнению с n поделительно алгоритм. Ну, так вот. Значит, сразу для тех,
кто действительно хочет глубоко понять, чего происходит, не просто формально разобраться,
а глубоко понять. Сразу скажу, что вот важно, что она действительно бесконечно мала по сравнению с
этим, но то, что тут написан именно квадрат, как обычно, это мой произвол, мне так удобно. Ну,
тут целая часть, просто чтобы м было целым числом. А то, что здесь написан квадрат,
это вещь такая, ну, чисто, знаете, для красоты. Могу куб нарисовать, могу пятую степень. Там
это плевать. Взяли какую-то конкретную функцию, которая не слишком мала по сравнению с n поделительно
алгоритм n, но во всяком случае мала. Мала, но не слишком. Слушайте, ну, если я буду очень долго
такие вещи разжевывать, то я никогда ничего не закончу, поэтому давайте я двинусь дальше.
Дальше немножко хуже. Давайте, во-первых, напомню, что у нас есть вот такое обозначение
для числа независимых множеств размера k в графе g. Давайте будет число независимых множеств
на k-вершинах. Нам спускается с неба граф g случайный. Спустился, мы посчитали,
сколько в нем k-вершинных независимых множеств. Это будет значение величины x-каты. Так,
ну, разумеется, мы знаем, что математическое ожидание этого x-катого
это вот такая величина. Так, дорогие друзья, вот это мы знаем точно, ведь в этом месте я никого
не кокну. Линейность мат ожидания. Просто перебираем все возможные множества на k-вершинах
и каждое тестируем на то, что оно независимо. Ну, независимо значит отсутствуют все цископодвы
ребер. Это 2 в степени минус цископодвы. Минус виден? Минус, конечно. Мат ожидания такое. И в
частности, товарищи, вот это вот то, что я здесь напомнил наверху, можете для себя это тоже как-то
пометить. Оно же как получается? Оно получается, внимание, это очень важно, из того, что если вместо
k сюда подставить вот этот удвоенный логарифм, то в пределе получится 0. Вот если сюда вместо k
подставить удвоенный логарифм, то в пределе получится 0. Именно поэтому альфа от g не превосходит
этого логарифма в вероятность стремящейся к единице. Неравенство Маркова просто. Помните,
да, понимаете, чего я говорю? Ну, поймите это для себя, это стандартная вещь, мы это много
раз доказывали. Передоказывать сейчас я не вижу большого смысла. Мне просто важно, чтобы вы
понимали, если вместо пункции k от n вот сюда подставить вот этот 2 лог 2 ич на n, то такое
математическое ожидание будет стремиться к нулю. Именно поэтому вот это неравенство выполнило.
Так, давайте эту функцию еще переобозначим. Вот так вот fk,t от n. Просто по определению положим fk,t
от n равным вот этому мат ожиданию, но в принципе можно было и не обозначать, но я уж так обозначу.
Можно fk,t от n, можно fn,t от k, ну пусть будет fk,t от n. Не помню, как в прошлом году я обозначал,
можно посмотреть, это неважно. Вот, ну так вот обозначу. И смотрите, вот если я вместо k подставляю,
как я уже говорил, 2 лог 2 ич на n, в пределе будет 0. А вот, например, если я напишу что-нибудь типа f3
от n, то это будет c из n по 3 на 2 в минус третьей степени, и это, конечно, стремится к плюсу бесконечности.
Вот я что хочу сказать, что если k маленькое, то рост к бесконечности. Но когда k становится
достаточно большим, а именно, например, вот таким, то тут уже будет стремление к нулю. Вот я утверждаю,
что есть некоторая точка перехода для каждого n, то есть можно рассмотреть такое минимальное k, скажем, 0,
нам дано n, и мы можем по этому n найти самое маленькое k, при котором f0 от n, но, скажем,
все еще меньше единицы. Набравно написать неважно. Вот, найдем такое самое, для данного n найдем
самое маленькое k0, при котором fk0 меньше либо равно единицы. При сильно маленьких будет
очень большим значение, но когда вы начнете их растить, растить-растить, в какой-то момент оно
станет меньше единицы. Ну, может быть, надо считать, что n достаточно велико, чтобы асимптотика
сработала. Здесь вот n можно считать достаточно большим. Вот для каждого достаточно большого n
существует минимальное такое k0, что fk0 от n не превосходит единицы. Вот, определим такое k0.
Значит, смотрите, я утверждаю такое утверждение, чисто аналитическое, которое мне бы не очень
хотелось сейчас подробно объяснять. В прошлом году Тихон Евтеев меня заставил его подробно
объяснить, я потратил минут 20. Если хотите, я, конечно, сделаю это еще раз, а можете посмотреть
запись прошлого года. Да, первый раз, когда оно стало меньше, совершенно верно. Я вот интуицию
осознал, что в какой-то момент должно стать меньше, идет переключение от стремления к нулю к стремлению
к бесконечности. Но вот утверждение состоит в том, что если мы посмотрим асимптотику вот этой
функции k0 от n при n стремящемся к бесконечности, то она будет вот такой.
Не, ну понятно, что k0 от n не превосходит 2 лог 2 ич на n при достаточно больших n.
Понятно, давайте я прямо напишу, это понятно, что k0 от n не превосходит 2 лог 2 ич на n при
каких-то достаточно больших n. Но это просто следует из-за того, что вот я говорил, да,
если вместо k0 от n просто подставить 2 лог 2 ич на n, там будет стремление к нулю,
а минимальная k0, оно только раньше возникнет, вообще говоря. Это понятно. Утверждение состоит
в том, что асимптотически эта оценка не улучшаема. То есть, да, я умудрился это тогда сказать,
я уже не помню. Может быть, а, слушайте, я, наверное, знаете, что сказал, что если взять k0 от n в
каком-то вот таком вот виде 2 лог 2 ич на n минус c лог лог, если вот как-то так взять, ну c
написал там 100, например, 100 точно хватит. Вот если в таком виде k0 от n взять, то уже будет
стремление к бесконечности. Ну можете это проверить. Слушайте, ну очень не хочется это считать,
в чем этого, если Тихон будет смотреть, я напрасно не возвожу. Это Тихон мне просил. Это понятно.
Подставите, увидите, что стремится к бесконечности. Он просил, сейчас скажу, что сейчас, мы до этого еще
не дошли до того, что он просил, мы не дошли. Это как раз все понятно. То, что оно будет таким,
это следует вот отсюда. Просто тупо проверите, как упражнение, что при подстановке такой функции,
вот в это выражение, уже получается стремление к плюс бесконечности. Ну, то есть, она точно не
подходит на кандидатуру минимального k, при котором не больше единицы. Оно стремится к бесконечности.
То k0 от n, которое мы определили, оно при больших n болтается где-то точно в этих пределах. Сейчас
понятно сказал эту часть. Значит, сейчас объясню, что он попросил сказать. Дальше я, смотрите,
я рассмотрю fk. Рассмотрим, давайте я напишу, чтобы было понятно. Я рассмотрю fk от m, вот от этого m.
Скоро поймете зачем, пока я чисто формально ввожу какие-то параметры. Я рассмотрю fk не как
функцию от n, а как функцию от m. Ну, увидите, зачем это нужно. Там будет очень понятно,
чисто вот алгоритмически все будет понятно. Скоро поймете. Ну, чуть-чуть позже. Давайте рассмотрим fk от m.
По нему найдем, вот так я нарисую, чтобы это не путалось со значком стремления. По нему найдем k0 и k0 от m.
Смотрите, k0 и k0 от m асимпатически равно 2 лог 2-ичный m, согласно вот этому утверждению. Но когда вы
берете лог 2-ичный от вот такого m, вы асимпатически получаете исходный 2-ичный логориф. Сейчас понятно,
что логорифом от этой штуки асимпатически равен логориф в учителе. Очевидно абсолютно.
Вот это вот 2 лог 2-ичный. Это просто мне нужно, чтобы запомнить. И вот теперь мы берем зачем-то,
это будет ясно не скоро зачем, но это будет очень круто, это будет катарсис полный. Мы берем k1 от m,
а в n-м к0 от m минус 3. Это уже конечно надо еще косликом подпрыгнуть на одной ножке. Да вот,
понимаете, минус 3. Почему минус 3 я обязательно объясню, то есть я всегда конечно заостряю на
этом внимание. Будет понятно зачем вычитать тройку, но пока не понятно совершенно. Так вот,
смотрите, вот мы вычли эту тройку. Что произошло? Что произошло вот с этой
функцией, с функцией математического ожидания? Она стала куда стремиться к нулю или к бесконечности,
если k0 это была вот такая вот граница. Но может быть константой. Да, да, да, да, да,
совершенно верно. Но во всяком случае мы куда-то поднимаемся по отношению к единице, да ведь?
Потому что это уменьшается, а мы, значит, поднимаемся. Вот я утверждаю еще одно утверждение. Вот это
утверждение Тихон попросил разобрать подробно. Значит, я утверждаю, что это вот fk1 вот так вот,
если мы подставим k1 от m в качестве k и m устремим к бесконечности, ведет себя следующим
образом. Вот так вот. То есть оно становится больше единицы, причем не в константу раз, а в m в
кубе раз. Вот мы три раза сдвигаемся и каждый раз как бы умножаемся на m, ну и с какими-то там
логарифмами, которые в показатель попадают в видеомалово от единицы. Это действительно не очевидно.
Чтобы это стало понятным, надо взять и при соседних значениях k разделить одно на другое,
то есть взять выражение c из m по k на 2 в степени минус c из k по 2 и давайте, знаете как,
по k плюс 1 и разделить на c из m по k на 2 в степени минус c из k по 2. Надо просто найти
отношение двух величин вот этих вот k плюс 1, fk плюс 1 от m и fk и от m. Да.
Да.
Ну, утверждаю, да. Может быть надо вычислить четыре. Сейчас мы посмотрим. Но мне надо,
чтобы m в кубе было. Вот это m в кубе мне очень важно. Не, ну можно вот честно разделить одно на
другое. Давайте, пап. Не, ну давайте, ну если хотите, давайте вот это разделим,
чтобы было понятие и чисто вот аналитическая сторона вопроса. Что будет, если поделить?
Сейчас, наверное лучше только k минус 1, наоборот было вот здесь писать, а здесь k. Ну ладно уж,
как написали, так написали, неважно. Значит, m факториал сокращается, правда же. Так,
тут k плюс 1 факториал, тут k факториал, то есть в знаменателе будет k плюс 1. Так? Так,
а в числителе что будет? m минус k, наверное, да? m минус k будет в числителе. Теперь двойки
в степени, вот это противно. Тут значит будет k на k плюс 1 пополам, минус k на k минус 1 пополам,
только не минус, а плюс. Ой, боже мой. k квадрат пополам сокращается, остается минус k пополам,
и еще раз минус k пополам, то есть минус k. Ничего противного, все легко. m минус k на
k плюс 1 на 2 в степени минус k. Вот такое вот отношение. Теперь смотрите, k у нас,
k у нас, это 2 лог 2-ичный m. Вот если k это 2 лог 2-ичный m, то здесь получается 2 в степени
минус 2 лог 2-ичный m, и это 1 поделить на m в квадрате. k это мизер по сравнению с m,
поэтому тут асимптотика получается вот такая. m поделить на k и умножить на 1 деленное на m в
квадрате. Это будет 1 поделить на km. Ну вот что я и утверждал, то есть при уменьшении на единичку
вот этого k, увеличение происходит вот примерно во столько раз. Ну m умножить на k это m в степени
1 плюс о малое от 1, потому что k это логариф. И вот если мы три раза сдвинемся, то увеличение
произойдет вот так. Ну ладно, повторили тихонно. Сейчас я объяснил, понятно? Каждый раз вырастает
примерно в m раз. Ну все. То есть если мы влево, вправо вернее, прибавим единичку к этому к 0,
это уже будет что-то типа 1 поделить на m, на m в степени 1 плюс о малое от 1. А когда мы влево
сдвигаемся три раза, вычитаем вот эту единичку три раза, мы наоборот вот это примерно 1 умножаем
на m в степени 3 плюс о малое от 1. Вроде все, я объяснил. Ну двигаемся дальше, товарищи,
это все технические вещи. Сейчас будет лемма, формулировка которой сама по себе, если вы ее
осознаете, ну уже катарсисная в каком-то смысле. Вот лему надо применить и потом доказать. Лемма
звучит следующим образом. Вероятность того, ну можно, кстати, асимпатически почти,
наверное. Ладно, напишу вероятность того что. Вероятность того, что для любого s из v. Так,
v, товарищи, это как обычно множество вершин нашего случайного графа, и оно состоит из n чисел.
Обычный граф на n вершинах, вот тот который в теории. Вот для любого s из v. Такого,
что мощность s равняется m. Помните, что такое m? Целая часть от n логарифа в квадрате. Вон там
написано. Число независимости под графа, ограниченного на s, больше либо равняется k1.
К1 от m. Того самого, которое мы нашли. Сейчас долго обсуждать. Оно, естественно, тоже имеет
асимптотику. Два лог двоичные. Так, давайте осознаем в чем действительно катарсисность этого
утверждения. Почему тут грандиозное что-то получилось? А, ну, виноват. Да-да-да-да-да. Ну,
я говорю, можно писать асимптотически почти наверно. Да-да, стремится к единице при n,
стремящимся к бесконечности. Ну или при m, стремящимся, неважно. Ну, при n, конечно,
у нас главный параметр это n. Так, в чем катарсис-то? Смотрите, вот есть множество всех n вершин,
и мы утверждаем, что какое бы подмножество мощности m, какое бы подмножество мощности m в нем
мы не взяли, в этом подмножестве обязательно есть независимый кусочек, кусочек без ревер,
размер которого не меньше, чем k1. То есть, смотрите, с одной стороны, вот я специально
здесь это написал, чтобы вам четче было понятно. С одной стороны, почти наверняка,
число независимости меньше, чем 2 лог 2-ичное, то есть в случайном графе нет вот такого размера
независимых множеств. Но едва вы уменьшаете этот размер до чего-то, практически такого же
в асимптотике, вы берете ту же самую функцию асимптотически, ну, немножко другую там смещаетесь,
может, на повторный логарифм. Понимаете, да, о чём я говорю? Вы читаете из вот этого какой-то
мизер, какой-то может быть повторный логарифм. Вот как только вы это делаете, у вас не просто
возникают независимые множества этого размера, вот этого размера, не просто они возникают,
а они есть в каждом куске размера м, их чертова туча. Понятно говорю, да? Какой рыбок! С одной стороны,
нет независимых множеств размера 2 лог 2-ичное, с другой стороны, независимое множество какого-то
вот такого размера есть почти всюду. Чего бы не взяли размер м, и там найдется такой вот маленький
островок. Вот такой вот. М это n на логарифм в квадрате. Так, друзья, вот эта сложная лемма,
которую мы докажем с помощью концентрации меры, давайте её применим, чтобы завершить доказательство
Балабаша. Теорему Балабаша мы сейчас мгновенно выведем. Так, смотрите, давайте вот это множество,
множество графов на n вершинах. Ну как-нибудь обозначим, как водится а с индексом n. Это множество
графов, события. Давайте предположим, что какой-то граф принадлежит этому множеству, то есть ну вот
выполнено для него это событие. Какой-то граф таков, что вот он обладает свойством, описанным под
знаком вероятности. Смотрите, что мы тогда сделаем. Вот этот граф. Найдем в нем независимый кусочек
размера k1. Он там есть, потому что он там повсюду есть. Правильно? Найдем, покрасим его в первый
цвет, потому что это независимый кусочек. Понимаете? Выкинем это множество и в оставшемся снова
найдем кусочек размера k1 и его покрасим во второй цвет и так далее. Сколько раз мы так можем
сделать, товарищи? Вот давайте осознаем. Исходя из этого свойства, в каждом под множестве мощности
мэй есть кусочек размера k1, который можно красить в очередной цвет. Сколько раз мы так сделаем?
Когда завершится эта процедура в алгоритме? Не, ну давайте напишем явно. Наверное, когда мы пройдем
вот столько шагов просто и все. Если мы сделаем, ну целую часть лучше нарисовать, вот если мы
сделаем столько шагов, то после этого может остаться уже меньше, чем m вершин и это перестанет быть
применительно. Так, друзья, согласны? Вот столько цветов появится. После этого останется m вершин,
каждую из которых в худшем случае мы покрасим в отдельный цвет. После того, как мы сделаем вот
столько шагов, крася вот эти вот кусочки, мы получим в остатке не больше, чем m вершин и в худшем случае
каждую из них покрасим в отдельный цвет. То есть суммарное количество цветов хиадже точно не больше,
чем вот это. Если граф обладает этим свойством, то он вот с помощью такого банального алгоритма
красятся не более, чем в столько цветов. Сейчас вот это понятно объяснил? В смысле, это все что
нам нужно, потому что k1 асимпатически равен 2 лог 2 ищ на n, так? Значит у нас получается,
что вот это выражение асимпатически равно n поделить на 2 лог 2 ищ на n. А m у нас выбрано так,
чтобы быть бесконечно маленьким по сравнению с этим. Ну то маленькое от n на логарифм n. Ну то есть
все вместе вот этого маленького, все вместе тоже асимпатически равно n поделить на 2 лог 2 ищ на n.
И теорем Балабаши доказано, что любой граф из вот этого множества, такие красятся в не более,
чем асимпатически столько цветов. А это множество имеет меру с тремящейся к единице. Значит,
почти каждый граф красятся в вот такое количество цветов. Сейчас, друзья, я что-ли быстро рассказываю?
Нормально? Ну естественно. Вы имеете в виду, что вот тут мы еще вычитаем. Ну m на k1 еще меньше,
чем m. Нет, но мы можем вообще сказать, знаете, что это меньше, чем n поделить на k1 плюс m. n поделить
на k1 асимпатически равно вот этому. m бесконечно мало, значит в сумме получаем ту же асимптотику.
Нет, но тут же вот есть phi от n анонсируемое. Вот это phi от n оно как раз и присутствует вот в этой
асимптотике плюс вот в этой. Вот phi от n получается отсюда. Ну не явно, конечно, но получается же.
Но в частности ясно, что при моем вот этом подходе phi от n точно не меньше,
чем n поделить на логарифм в квадрате. Но еще тут есть какая-то поправка, может быть еще отсюда что-то
вырезать. Но не важно. Важно, что это асимптотически равно, а значит какое-то phi существует. Это просто
равносильные вещи. Все, теорема была ваша, я доказал. Но лемму надо доказать. Понимаете,
вот лемма-то это самое интересное, удивительное результат. Давайте начнем, хорошо? И начнем,
может и закончим, как повезет. Не, ну закончим вряд ли. Так, где бы мне писать? Давайте вот тут пока что
будем писать. Это все, долой.
Так.
Так, друзья, все живы? Готовы воспринимать доказательство леммы? Давайте, давайте. Оно
достаточно трудное, конечно, но я постараюсь рассказывать вам максимально внятно. В том числе
вот как не надо действовать, я тоже расскажу. Но давайте так, давайте для начала, как обычно,
напишем отрицание того события, которое в скобках, чтобы доказывать не стремление к единице,
а к нулю. Оценивать сверху всегда проще. Довероятность того, что существует S из V мощности
M такое, что альфа от G на S больше, нет, меньше, чем k1. Существует множество мощностей M, в котором
нету независимого подмножества мощности k1, альфа меньше. Ну, как это дело оценивать? Стандартно,
тут, в общем, деваться некуда. Значок кванторсуществования это всегда объединение
событий, товарищи. Это мы уже набили оскомину, наверное, да? Постоянно я это повторяю. То есть
здесь получается просто С из N по M, количество способов выбрать множество мощностей M на N
вершинах. Умножить, на вероятность того, я давайте так напишу, что альфа давайте от H, может быть,
поаккуратнее, меньше, чем k1. Имеется в виду, что H это уже граф не на N вершинах, а на M вершинах.
Ну, какая разница, на какое множество S ограничить граф G? Если вы на какое-то M
вершинное множество его ограничиваете, то у вас просто получается случайный граф на M вершинах.
Понимаете, да? Неважно, поэтому никакого суммирования по S не надо писать, надо писать
просто количество этих S и умножить на вероятность того, что на каком-либо из них альфа меньше,
чем k1. Ну, давайте я еще раз подчеркну, вот эта мера на вот это вот мера, она на G от NP, а вот эта
вот мера, она на G от MP. Все. Ну, мера вот этой вот вероятности, она определена здесь на множестве
вот этих случайных графов, а здесь уже на вот таких. Ну, просто чтобы вы для себя как-то
максимально четко пометили. Понятно? Когда мы G ограничиваем на S мощности M, мы получаем
просто случайный граф на M вершинах. Так, ну ладно, это все понятно. Смотрите, друзья, если вы готовы
воспринимать суть, то я потрачу время на то, чтобы уйти в неправильную ветку решения.
Не, но смотрите, я хочу, на самом деле, что я хочу сделать? Я хочу вот это наше неравенство,
помните неравенство, которое похоже на пьяницу? Что если F липшется вот, то F уклоняется от
своего мата ожидания, с вероятностью, которая очень маленькая. Помните такое? Вот я хочу в итоге
применить его для того, чтобы оценить вот эту вероятность. И я это сделаю. Но если я сейчас начну
это делать чисто формально, то у любого нормального человека, который хочет понять вообще чего
происходит, возникнет некоторый как бы когнитивный диссонанс. То есть этот человек подумает,
зачем такой огород городить? Ну мы же умеем оценивать вот такие вероятности через что-то
гораздо более простое. Я сейчас вам награжу огород. Вот смотрите, вы можете даже себе сейчас какое-то
время не записывать, я вам скажу, вот такая врезка будет. Я вам просто поясню, что вот можно, конечно,
пытаться вот эту вероятность оценить как-то по-простому, по-рабочекрестьянски, не используя вот
это вот неравенство с липшицами функции. Ни хрена не получится. Вот я хочу вас в этом убедить, товарищи.
Можно? Но все-таки вот смотрите, вот сейчас, если хотите, можете не записывать, вас это никто не
спросит. Просто, ну не знаю, Аркадий Борисович может что угодно, но он обычно спрашивает формально те
вещи, которые требуются для доказательства. То есть если вы ссылаетесь на что-то и не доказывать,
там формулу Стирлинга, он может потребовать ее доказать. Но уж какие-то врезки про то,
что не работает, он спрашивать не будет. Не-не, но там, где не надо доказывать, я прямо поставил,
что не надо доказывать, так что вас не кокнут. Не переживайте. Вот, ну смотрите, значит, вот такая
небольшая врезка. Вот я хочу оценить эту вероятность. Хочу. Давайте, вероятность того,
что альфа-атаж меньше какого-то k1. Ну, слушайте, это в точности вероятность того, что х с индексом k1,
вот он еще светится тут. Что такое х с индексом k1? Число независимых множеств. Равняется нулю.
Ну, да. Не, она каждый раз вырастает. Я же про это сказал, что если прибавить единицу,
то она наоборот уменьшится в мэра столько. Поэтому меньше, чем 1мт, она тоже не будет.
Но, может быть, надо на 4 сдвинуться, чтобы быть уверенным, но я точно знаю, что на 3
достаточно. Неважно, можно хоть на 10 сдвинуться, от этого в итоге мое доказательство не пострадает.
Если вам спокойнее, сдвиньтесь на 4. Ну, вы вот если вам спокойнее, сдвиньтесь на 4,
этого точно достаточно. Вот на 4 уж точно достаточно. Она в свою очередь за счет того,
что каждый раз идет изменение в м раз, больше ли равна 1 поделить на м в степенью 1 плюс о малой
от единицы. Потому что если 4 раза вы сдвинетесь, там м в кубе будет. Хорошо, да, это нормально.
Так, вернемся сюда. Все понимают, что это одно и то же. х это число независимых множеств на
Поэтому отсутствие независимых множеств можно и так и так обозначить. Вот, ну, стандартный
код. Слушайте, я вообще могу, знаете как, сослаться. У нас же было неравенство. У нас было
неравенство. Смотрите, я его доказывал. Вот такое. Что я буду его переписывать заново? Помните
такую штуку? Я для треугольников ее писал, а потом говорил, ну, смотрите, какая разница. Мне важно
только, что здесь случайная величина, принимающая не отрицательные значения. Не отрицательные целые
значения. Вот если она принимает не отрицательные целые значения, всегда верно такое неравенство.
Я это точно писал. Кто-нибудь хотя бы помнит? Ну, в общем, короче говоря. Не, ну, друзья, ну,
слушайте, ну, что я буду по три раза, что ли, за курс одно и то же доказывать? Я ссылаюсь на то,
что было. Если вы записывали, у вас точно было, вы вспомните. Вот это стандартное простое
неравенство, которое, внимание, товарищи, вытекает из неравенства Чебышова. Это просто следствие
неравенства Чебышова. Ну, неравенство Чебышова это тоже неравенство плотной концентрации и меры,
но если вы помните историю про пьяницу, то как последний гвоздь в крышку гроба его алкоголизма
забивает не Чебышов, а вот то неравенство большого уклонения с экспонентой. Помните,
да? Сейчас давайте я договорю, потом сделаем перерыв. Смотрите, товарищи,
тот стоит в номинации. Вот мы только что с Мишей обсуждали. Ну, примерно М в кубе,
да? В квадрате. М в шестой. Чего? Ой, да. Да, просто вот так, конечно. Да, вот так, извините,
нам же сверху надо оценивать, вы глупость сказали. Короче, вот в нашей ситуации это примерно М в шестой.
Какая там дисперсия, уже наплевать. Почему? Потому что эта дробь, если и стремится к ноль,
ну стремится, то примерно как многочлен. Не важно там десятой степени, двадцатой, шестой,
но это многочлен в знаменатель. А что такое C и Zn по М, на которое это вот стука умножается?
Не, ну дисперсия тоже будет, конечно, полиномом, вспоминайте, как мы с треугольниками считали.
То есть мы потом посчитаем даже эту дисперсию, она будет, конечно, полиновом, то есть здесь вот
будет, ну, отрицательный полином. Может быть, то есть как вот тут М в шестой знаменатель, дисперсия
тоже может быть полиномом в знаменателе. Но даже если вы делите на М в двадцатой степени,
но это значит, что вы делите на n в двадцатой степени, грубо говоря. А вот эта штука
c из n по m, ну слушайте, ну вы на такую ерунду делите, что это уже ну как бы не очень сильно
отличается от c из n по n на 2, которая, как вы понимаете, есть примерно экспонента 2 в степени n.
Убить экспоненту по линомам хрен получится. Понимаете? Да, вот поэтому Кибышов здесь не сработает от слова
совсем. Вот я только это хотел вам пояснить. Почему придется строить Липшицеву функцию и
применять неравенство, которое было с ней. Сейчас перерыв 5 минут. Так, ну все, давайте продолжать.
Ну я написал, собственно, резюме, да, что надо продолжать как-то оценивать. Заодно c из n по m оценил
тупо как 2 в n, но я уже предупреждал, что сильно лучше оценить-то и не получится. Ну получится,
конечно, но эта фигня там какая-то будет знаменателем, она не сработает, я вас уверяю.
Надо придумать какой-то ход, похожий на то, что мы делали с Чебышовым, но как-то вот по-другому
заменить вот это неравенство. Значит, код следующий давайте обозначим через y с индексом k от g.
Ну k это вот, это k. Так, k у нас k1. Ну пусть будет k любое там, в частности k1. Значит, через y с индексом k от g
обозначим случайную величину, которая равна максимуму, среди всех, ну какую букву использовать,
давайте m, а m у нас есть, давайте l. Максимум среди всех чисел l таких, что существует
a1 и так далее, а с индексом l из v под множество. Так, такие, что для любого i мощность аитова
равняется k, для любых и g, вот я сейчас начинаю городить огород, чтобы возникла Липшицева,
сейчас увидите, для любого i мощность равна k, для любых и g мощность аитова пересеченного
сожитом не превосходит единицы. И, наконец, для любого и g, ограниченной на аите, клика,
ой, клика, независимое множество, антиклика, независимое множество. Максимальная l такое,
что, ну это для данного графа, что вот у этого графа есть l под множество мощности k на каждом
из которых клика, и при этом эти клики пересекаются не больше, чем по одной вершине. Каждые две либо
вообще не пересекаются, либо пересекаются ровно по одной вершине. Ну, то есть, картина какая-то,
вот такая опять, есть m вершин, у нас теперь, помните, m вершин, m вершин теперь, мы у графа h
рассматриваем m вершин, не забывайте про это. Ну, в каком-то смысле, ну да-да-да, ну давайте я
напишу h, действительно, чтобы оно, не то чтобы логичнее, но как бы понятнее может быть,
действительно, коррелирует больше запись одна с другой, давайте h напишу. То есть, у нас всего
m вершин, и вот мы можем найти какие-то под множество a1, a2, там a3 и так далее. Вот нам
хочется их найти как можно больше, чтобы каждые два пересекались не более, чем по одной вершине,
не имели общих ребер, если угодно, и при этом все они были бы кликами в этом графе. Значит,
друзья, на самом деле, вы помните, я вам когда-то рассказывал еще в ОКТЧ, про теорию кодирования,
там была история про то, что надо выбрать тройки пьяниц, ну ладно, бог с ними с тройками пьяниц,
может я это не произносил, сейчас вам расскажу. Теория кодирования, она устроена так, надо
выбрать тройки пьяниц, которые бы соображали каждый вечер на троих, сейчас отвечу, и при этом
пересекались бы не больше, чем по одному человеку, потому что иначе они опять морду друг
другу набьют, но они, правда, все равно морду набивают. Так, вопрос. Я сказал клика, потом сказал,
ой, извините, антиклика стерна, писал независимое множество. Нас интересует независимое множество,
я оговорился просто. Наши интересуют альфа, а не омега. Нас интересует независимое множество.
Теперь я возвращаюсь к истории. Вот там ровно про это шла речь, как построить как можно больше
множеств, чтобы они попарно мало пересекались, тогда у нас будет код, который исправляет некоторое
количество ошибок. Ну бог с ними с пьяницами, ну теорию кодирования-то вы помните? Помните, да,
что там кодируют-то не от пьянства, там кодируют именно так, чтобы исправлять ошибки на канале
зашумлённым. И вот если множество соответствует векторам из нулей единиц, то чем меньше они
пересекаются, тем больше ошибок мы можем исправить. Я про это всё рассказывал, там были матрицы Адамара,
границы Плоткина, ну много чего было. Вот на самом деле то, что вот весь определённой yкт атаж,
это в каком-то смысле вот история про кодирование. Ну просто напоминаю, друзья, чтобы у вас в голове
мостик вот этот возник, это не нужно ни для чего, кроме понимания, ну знаете как, всё в мире
взаимосвязанно. Вот в моих курсах на самом деле тоже всё взаимосвязанно. Я себя ощущаю господом бога.
Ну не могу же я вас не развлечь, товарищи. Так, теперь давайте вернёмся к определению. Значит,
смотрите, оно, я надеюсь, что оно перестало быть для вас слишком формальным игромостким после того,
как я напомнил, что оно на самом деле про пьяниц, про вот этих вот, про теорию кодирования. Я только
для этого вспомнил. Теперь, вот согласны ли вы, что я могу написать теперь вот так? Ну какая разница?
У меня просто нет независимых множеств. Или длина самой длинной цепочки из независимых множеств,
которые... а, я все время говорю «клика», я понял, я ещё и тут говорю «клика». Нет, это всё время
независимое множество. Я понял, я всё время оговариваюсь, я теперь понял, почему вы спрашиваете.
Вот это всё независимое множество. Не клики никакие. Это я всё время оговорю.
Значит, друзья, ну вы согласны, да, что если размер вот этот вот L самого большого набора из независимых множеств, которые мало пересекаются между собой, равен нулю, то это в точности означает, что их просто нет.
Ну, то же самое. Как x-катая первая равнялась нулю, так и y-катая первая будет равняться нулю. Согласны, нет?
Но зачем я рассматриваю y-катая первая? Затем, чтобы заиметь липшицевость.
Вот величина x-катая, вот эта, она абсолютно никоим образом не липшится, вы понимаете, да, ну число треугольников там, число независимых множеств на k вершинах.
Вы одно ребро добавили, куча независимых множеств пропала.
Этим общим ребром с этой общей парой вершин. Понимаете, да? То есть вот это ни разу не липшится от слова совсем, как сейчас принято говорить.
Ничего с ней не сделаешь. Пришлось ввести вот такую вот случайную величину y-катая, чтобы она была липшицевой и уже к ней можно было применять что-то. Какая она липшицева? По кому?
По ребрам. По ребрам, потому что мы говорим о том, что тут если и есть что-то общее, то не ребро, а вершина.
То есть если эта величина увеличилась бы на два при добавлении ребра, то это бы означало, что какие-то два независимых множества по этим двум вершинам пересекаются.
Понимаете, да, у нас это запрещено.
y-катая липшицева по ребрам. Это не самая лучшая липшицевость, как вы помните, но этого нам хватит.
В данном случае этого нам хватит. Она липшица по ребрамам.
Ну а тогда, смотрите, я сейчас воспроизведу как раз ту цепочку, которую поленился воспроизводить для Чебышова.
Сейчас вот прям всю эту цепочку смешных перегонок, которые Чебышов нам давал.
Вот тут вот будет так, 2 в степени n, на вероятность того, что y-катая первая не больше, чем ноль, но равняться нулю и быть не больше, чем ноль для величины, которая отрицательных значений не принимает, это одно и то же.
Дальше. Вот у вас должно в голове всплывать, вот мы такое уже делали.
Вот как раз то неравенство с дисперсией квадратом мат ожидания, оно так и получалось.
Здесь будет минус y-катая первая, больше ли б равняется нуля, да множе ли просто на минус 1. Вот было, было прямо вот оно так получалось.
Дальше равно 2 в степени n, на вероятность того, что е-катая первая, минус y-катая первая, больше ли б равняется е-катая первая.
И вот это вот согласно неравенству большого уклонения, ну вот того, который мы не доказывали, но которое похоже на историю с пьяницей.
Значит оно, она это вероятность не больше, чем е маленькое, то есть 2.71 в степени минус, что, почему 2.72, но я понял, да хорошо.
Основание натурального логарифма, так, минус, что там получается, е, y-катая первая в квадрате поделить на 2c из m по 2.
Да, 2 в степени n я потерял, конечно, 2 в степени n. Да, вот эти 2 в степени n я потерял, старательно выписывая результат, который получается из липшицы.
Но тут вот стояла буковка а там, тут было написано е-эф, а тут было еф.
И говорилось, что если еф липшится по ребрам, то тут будет минус а квадрат, но вот наше а, это сейчас мат ожиданий y-катого первого, поделить на 2c из m по 2.
Лори Опер в полном графе на мэй вершине.
Так, друзья, кто-то сравнивает с тем, что я писал? Похоже?
Похоже?
Но вроде люди поддакивают, некоторые, смотри.
Ну, должно быть, я стараюсь.
Так, смотрите, но мы ж не знаем чему, все ужасно.
Одно дело найти мат ожиданий x-катого, вот оно вам все, и дело с концом.
А что такое мат ожидания y-катого?
Жуть же какая-то, как считать среднее значение вот такой вот бешеной теоретикой кодировочной величины.
Значит, мы сейчас сформулируем лему, которую, может быть, даже частично докажем.
Где бы мне ее сформулировать? Давайте вот тут я ее сформулирую, лемма будет такая врезка.
Лемма состоит в том, что мат ожидания y-катого первого больше либо равняется m в квадрате,
поделить на 2k1 в четвертой степени, умножить на 1 плюс о малой от 1.
Значит, сейчас, друзья, я вам кое-что поясню по поводу этой леммы, я ее докажу потом.
Потом я ее докажу.
Но сегодня мы это не закончим, но, в общем, я ее докажу.
Естественно, вопрос в билетах, он будет разбит на много частей.
Понятно, что все это доказывать в одном билете вам не придется.
Да, здесь в знаменателе 2k1 в четвертой.
Я прокомментирую, потом докажу, но давайте с помощью этой леммы, конечно, все получается.
Потому что, смотрите, если эта лемма верна, то у нас получается вот так.
Это равно просто 2 в степени n, на e в какой степени?
Надо вот это возвести в квадрат.
То есть будет m в четвертой, поделить на 4k1 в восьмой.
4k1 в восьмой.
И еще надо поделить вот на это, но это m в квадрат.
Так, друзья, понятно, что дважды c из m под v это m в квадрат.
Оно же в обе стороны верно.
Уклонение в любую сторону, какая разница?
Ну, прочитайте вот это вот-вот так.
Это же то же самое, что ykt1 меньше либо равняется...
Вот сюда перенесли.
А?
Вот так вот.
Сейчас, ну, подождите.
Нет, я неправильно сказал.
Вы говорите f-ef больше либо равняется a.
Но было и вот такое.
f-ef меньше или равняется minus a.
Такое же тоже было.
И вот e и f.
И вот f-ef меньше или равняется minus a.
Такое же тоже было.
Такое же тоже было.
И вот его прочитайте в обратную сторону, вот туда перенесите.
Это будет как раз ef-f больше либо равняется a.
Ну, это сдвиг влево, пьяница ушел влево.
Там все симметрично.
Так, друзья, давайте подставим все-таки.
Вот я вроде уже почти подставил.
Тут надо еще поправить на один плюс о малой от единицы.
То есть оно, во-первых, отсюда вылезает,
а во-вторых, я еще c-ку заменил на m в квадрате пополам.
Но это тоже в симптотике верно.
То есть с точностью до 1 плюс о малой от единицы я прав.
Что все корректно написано.
Написано, все корректно.
Теперь у нас получается, ну, давайте я еще раз перепишу.
2 в степени n на e в степени
minus m в квадрат 1 плюс о малой от единицы
m в квадрат, потому что m в четвертой с m в квадрате сократилось,
подденить на 4k1 в восьмой степени.
Так, друзья, я надеюсь, понятно, что это стремится к нулю.
Или нет?
Ну, смотрите еще раз, m это что такое?
Вот оно светится.
То есть m в квадрате – это n в квадрате.
Ну, поделить на логарифу в четвертой степени.
То есть, если хотите, я уж ладно, перепишу еще раз.
2 в степени n на e в степени
minus n в квадрате на 1 плюс о малой от единицы.
А тут будет...
Слушайте, а k1 это что же логарифом, да?
Ну ладно, ладно.
На 4k1 в восьмой.
И на что там?
На логарифом в четвертой степени, вот такой.
Но это вы меня вынуждаете, потому что никто не подтверждает, что все понятно.
Я пишу очевидности.
Ну, переписал, m в квадрате.
Вот здесь, смотрите, тут просто экспонента,
а тут e, ну отрицательная экспонента,
с n в квадрате, ну там оно делится на какие-то степени логарифмов.
Ладно, тут двенадцатая степень логарифма.
Вот все примерно двойка.
Это она убивает с огромным запасом обычную экспоненту.
Это, конечно, все стремится к нулю со свистом.
Видите, насколько мощнее вот этот гвоздь в гроб пьянства?
Получилось даже сильнее, чем нам нужно, да.
Сильно сильнее, чем нам нужно.
То есть нам действительно наплевать в эту экспоненту,
можно было ее чуть-чуть подчистить, там что-то в знаменатель загнать,
но наплевать не нужно.
Так, ну мне надо доказать теперь эту лему.
Давайте, как я обещал, я сначала ее прокомментирую,
а потом формально буду доказывать.
Там формальное доказательство требует некоторых усилий, понимания.
А смысл очень простой.
Вот я же напомнил здесь, что это определение
фактически строит нам такую вот, такую историю с теорией кодирования.
Напомню.
Ну может вы даже помните, мне казалось я это говорил, но может быть и нет.
Вот смотрите, если мы возьмем yкт не от какого-то графа h,
сейчас вот задумаю, а от графа km с чертой.
Граф km с чертой, товарищи, это пустой граф.
Ну km это полный граф, а с чертой это значит все ребра стерли.
Каких не было провели, но их не было таких, которых не было.
Поэтому это пустой граф.
Вот что такое yкт на пустом графе?
Нет, смотрите, я утверждаю, что это точно не больше, чем c из m Подова.
Вот я хочу, чтобы это осознавали.
Хотя бы те, кто ходят на лекции, чтобы это осознавали.
Ну те, кто послушают тоже.
Значит c из m Подова поделить на t из m Подова.
Я этого не доказывал никогда, я уже не помню.
Это очень простая вещь.
У нас в каждой вот этой маленькой сарделечке отсутствующих ребер пар вершин.
Вот столько, сколько в знаменателе.
А всего пар вершин вот в этом графе km с чертой c из m Подова.
Поскольку общих пар не должно быть, то количество таких сарделечек,
просто по принципу дирихления, больше этой дроби.
Друзья, ну это простое упражнение.
Такой факт из теории Кадир.
Но это примерно, мы к этому еще когда-нибудь вернемся.
m квадрат на k квадрат.
Так вот, я чего утверждаю-то в этой лемме?
Это же надо интуитивно осознать, в этом есть содержательный смысл.
Я утверждаю, что если мы теперь y берем не на пустом графе, а на случайном,
то, конечно, вот эта величина, по идее, должна уменьшиться, если тут не пустой граф, а какой-то.
Согласны?
Вот я, когда беру на случайном графе с p равном 1 и 2, то в среднем она, если уменьшается, то ничтожно.
k1 в квадрате раз.
Ну это же логарифум.
Понимаете, да?
То есть, есть очевидная верхняя оценка, она верна для пустого,
но, значит, она тем более верна для этого мат ожидания.
Я утверждаю, что нижняя оценка почти такая же.
Понятен смысл теперь этой леммы?
Еще раз, у нас всего пар вершин.
Т.е. замкодово вот в этом графе.
В каждой сардельечке, которую мы ищем, независимое множество.
Но нам же надо их построить так, чтобы они не имели общих пар вершин.
Ну, значит, их не больше, чем вот эта дробь.
Получается, что даже не на пустом графе, а на случайном графе в среднем оценка снизу почти такая же,
что и на пустом графе.
Получается, что даже не на пустом графе, а на случайном графе в среднем оценка снизу
почти такая же, как на пустом сверху.
Ну, значит, на любом сверху.
Само yкт первое сверху оценивается как m в квадрате на k первое в квадрате.
А мы утверждаем, что в среднем оно и снизу примерно так же оценивается.
И вот за счет этого получается крутое стремление к нулю.
Сейчас нормально объясню?
Так, ну давайте попробуем начать доказательство этой леммы.
Не хотелось бы все-таки откладывать это на следующий раз.
Что-то я, наверное, успею рассказать.
Ну, друзья, вы меня правильно поймите.
Я, конечно, ни с кого не буду спрашивать вот эти все штучки-дрючки,
которые я пояснениями даю.
Но мне кажется, что если кто-то из вас осознает глубже,
то от этого будет только польза.
Поэтому я стараюсь рассказать максимально глубоко со всеми штучками-дрючками.
Я понятно говорю, да?
Это не мешает ведь вам воспринимать все-таки конву доказательства?
Ну, я стараюсь вроде четко говорить, где штучки-дрючки,
а где прям вот то, что формально нужно.
Так, ну как доказывать лему?
Это, кстати, безумно красиво тоже,
потому что мы сейчас добавим случайности в некотором смысле.
Значит...
Страшно, да? Страшно стало.
Ой, как страшно.
Так, ну смотрите, вот пусть у нас, наоборот,
пока уберем всякую случайность,
пусть есть какой-то граф.
Слушайте, можно я буду писать G? Ну а G, какая разница?
Мне как-то привычнее G писать.
Ну, это то же самое, что H, конечно.
Есть какой-то граф.
Есть какой-то граф.
Вот, давайте возьмем в нем все его независимые множества на K1-вершинах.
Нет, это шмат ожидания, число.
Это просто число, среднее значение.
Тут ничего, никакой вероятности нет.
Так, возвращаемся сюда.
Взяли какой-то совершенно конкретный граф G.
Так, друзья, вы все вот там спеваете еще живы?
Или я беседую только там с частью первого ряда?
Я не кохнул остальных, нормально?
Так, ну смотрите, взяли какой-то конкретный совершенно, пока не случайный, граф G.
И хорошенько им обо что-то стукнули.
Так, значит, и выбрали в нем все независимые множества, какие в нем есть.
Ну, давайте их как-нибудь обозначим, эти независимые множества, как их обозначить.
Ну, пусть будет там какой-нибудь K1.
Ну, а последнее, товарищи, это понятно, что это XKT первое от G.
Ну, сколько всего в графе G независимых множеств.
XKT первое от G независимых множеств на K1-вершинах.
Взяли граф, нашли по нему все эти множества.
Ну, правда же, мы же так и определили, XKT от G это число независимых множеств на K-вершинах.
Ну, XKT первое от G это число независимых множеств на K1-вершинах.
Вот, возьмем просто, их все перечислим.
Возьмем их все перечислим.
Как-нибудь, наверное, знаете, это мы тоже обозначим.
Введем такое K, красивое, каллиграфическое, которое будет представлять собой множество из вот этих товарищей.
Ну, мысло очень простое.
Взяли граф, взяли все его независимые множества с нужным числом вершин.
Перечислили, получилось какое-то множество K большое, красивое, состоящее вот из этих независимых кусочков, дырочек таких в графе.
Так.
Ну, ну, сделали или не сделали, что бы с этого хорошего-то.
Давайте сделаем, как я обещал, дополнительную случайность.
Вот граф пока не случайный, граф просто конкретный.
Ну, а когда он случайный, ну что же, в конечном счете конкретный, просто ему присвоена вероятность какая-то, вы же понимаете.
Случайно, в каком смысле, что у него просто есть некоторая вероятность возникнуть?
Если он возник, то вот эти множества совершенно конкретные.
Если он случайный, ну в каком-то смысле K красивое, это случайная совокупность.
Ну вот.
Значит, давайте возьмем какую-нибудь чиселку, как бы ее обозначить.
Я обычно пишу K со звездочкой, но просто чтобы не путать ее с вероятностью ребра случайного графа,
или вероятностью ее отсутствия.
Не хочу букву P писать, хотя у нас сейчас в нашей задаче P равно 1-2.
Я все равно не хочу писать P, потому что это другая вероятность.
Это какое-то число, пока что просто из отрезка 0,1, которое, товарищи, мы потом чуть-чуть позже подберем оптимально.
Так, чтобы все хорошо получилось.
Вот возьмем число K со звездочкой.
Так.
И проредим множество K.
Проредим, это значит, мы будем бросать монетку с вероятностью успеха K со звездочкой.
И если в очередном бросании успех, то мы сохраняем жизнь вот этому независимому множеству.
Сейчас быстро говорю, да?
Сейчас увидите, ну может не сейчас увидите, но я не хочу бросать, потому что это 15 минут потеря.
Но увидите, вы охренеете, когда поймете, зачем.
Вот человек придумал такое, вы увидите.
Нет, ну слушайте, ну нельзя же ужасаться сразу.
С другой стороны, вот вы лучше понимаете смысл слова катарсис.
Ну что такое катарсис? Это ведь завершение трагедии, когда случается просветление души.
То есть вот вы сейчас смотрите на меня, что за хрень происходит.
И вдруг бабах, и оно все сошлось.
Это и есть катарсис, понимаете?
Сейчас, друзья, я очень быстро говорю, или вы успеваете фиксировать, что происходит?
Я беру и от множества K, ну перехожу, я не знаю, давайте C.
Назовем его как-нибудь.
Как-нибудь назовем, пусть множество C будет.
Так, друзья, друзья, ну пожалуйста, давайте только сосредоточим.
Если мы будем долго дискутировать, мы правда ничего не успеем.
Еще раз, как мы перешли? Мы взяли такую тхему испытаний и вернули.
Вот столько раз бросили монетку с вероятностью успеха Q со звездочкой.
И если в очередном бросании успех, то сохранили жизнь множество отсюда.
Ну то есть, например, с вероятностью Q со звездочкой вот в этой степени мы сохраняем все независимое множество.
Получается такое случайное подмножество этого множества K.
Ну такое случайное подмножество.
Хорошо?
Ну вот зачем-то оно получилось.
Пока поверьте мне, что это поможет.
Почему это поможет? Будет катарсис.
Ну я же для чего стараюсь, чтобы вы поняли какие-то методы.
Не только узнали факты, но мы же с вами не гуманитаристикой занимаемся.
Мы хотим научиться эти факты доказывать.
Может быть вы самостоятельно что-то получите потом.
Вот взяли какое-то такое случайное множество.
Теперь в итоге у нас получаются такие пары G и C от G.
G это уже теперь случайный граф, а C от G это случайное подмножество в множестве независимых множества этого случайного графа.
Такие пары.
Но вероятность пары это просто 1 поделить на 2 в степени C и знам по 2.
Это вероятность графа умножить на вероятность C от этого же.
То есть это просто вот такое немножко усиленное вероятностное пространство.
То есть мы берем не просто случайный граф, а случайный граф и на нем случайное подмножество множества его независимых множеств.
Случайный граф и в его множестве независимых множеств случайное подмножество.
Так, друзья, нить-то не должна потеряться.
Понятно, что произошло.
Для каждого графа взяли случайную такую совокупность.
Теперь, смотрите, давайте, ну, виноват, ведем еще две величины или два множества вернее.
Значит, будет множество.
Я сейчас, вы скоро поймете, нам же надо с этим разобраться.
Вы скоро поймете.
Я бы, конечно, хотел сегодня успеть, поэтому давайте сосредоточено от того, что вы сейчас чего-то не до конца понимаете.
Если мы тут будем дискутировать, мы до катарсиса не дойдем.
Значит, W от G.
Сейчас, товарищи, это множество пар A и B.
Таких, что A и B это подмножество вершин нашего графа.
Множество пар подмножеств.
Таких, что мощность A пересеченного с B не больше единицы.
А, можно, знаете как, я лучше напишу.
Чтобы понять не было, извините, A и B нехорошо.
Давайте я лучше вот так напишу.
Таких, что КАИТОЕ пересеченное с КАЖИТОМ не больше единицы.
Чего писать заново, когда у нас есть обозначение для независимых множеств?
Меня просто интересует количество пар независимых множеств, которые мало пересекаются.
То есть, тех, которые могут присутствовать в определении Y.
Помните, что такое Y?
Наймальное число независимых множеств, каждые два из которых пересекаются вот так.
Помните, да?
Вот мы рассмотрим множество тех пар, которые могут служить кандидатами на попадание вот в эту цепочку.
Поняли определение W?
Нет?
Так, отлично.
И W штрих, оно уже будет зависеть от V, от C и от G, от C.
Они по определению все независимые, понимаете?
Поэтому они соответствуют определению Y.
Там же как раз независимые множества.
Так, теперь смотрите, это соответствует прореженной ситуации.
То есть, пишем то же самое КАИТОЕ КАЖИТОЕ.
Да, смотрите, я их пишу в фигурные скобки, это значит, что они неупорядочены.
Ну то есть, нам плевать в каком порядке.
Так, таких что то же самое, но при этом КАИТОЕ КАЖИТОЕ принадлежат С.
Вот так.
Ну тоже понятно.
Мы зачем-то выбираем кандидатов на попадание в определение Y не из всех независимых множеств, а только из C, которая получена прорешением.
Так.
Так.
Так, друзья, пока не умерли.
Формально, понятно?
Держитесь, осталось не так много времени, но я думаю, что я успеваю, мы можем продолжить.
Значит, смотрите.
Давайте математическое ожидание мощности W.
Мы его не знаем, конечно.
Его нас нет.
Мы его не знаем.
Мы его не знаем.
Мы его не знаем, конечно, его надо считать.
Этим мы займемся в следующий раз.
Но это не очень сложно, я там частично проглочу какие-то сложные выкладки, все будет легко.
Ну давайте, вот мат ожидания пока обозначим просто кандидат пополам.
Пополам?
Ну почему пополам?
Потому что я сказал, они неупорядоченные, а считать будет удобно для упорядоченных.
В этом смысле пополам.
Вот так просто обозначим.
Вот так просто обозначим.
Дельта пополам.
Вот, сейчас нужно что осознать?
Во-первых, а, и вот еще что.
Совсем временно вот это вот мат ожидания, вот это.
Вот это.
Временно обозначим короткобуквой мю, чтобы не таскать все эти громоздкие обозначения.
Просто назовем вот это все мю.
Так, смотрите, мат ожидания, мощность к, ну это естественно мю.
Потому что мощность к, это как раз экскатая первая.
Мат ожидания мощности к, это мат ожидания экскатовой первой, то есть мю в наших нынешних коротких обозначениях.
Так, друзья, понятно?
Так, друзья, понятно?
Так, теперь смотрите, мат ожидания мощности с.
Мат ожидания мощности с, какое?
Мю на ку со звездочкой, конечно, да.
Конечно, ну как в схеме Бернульны, надо умножить на вероятность успеха.
Мю на ку со звездочкой.
Мю на ку со звездочкой.
Теперь смотрите, а мат ожидания мощности w'.
Это хороший вопрос.
Мат ожидания мощности w'.
Кто скажет, чему равно?
Ну, наверное, дельта пополам там будет присутствовать, да?
Потому что мы проредили w.
Абсолютно верно.
Наверное, дельта пополам на ку со звездочкой в квадрате.
Конечно, потому что нам надо, чтобы оба множества попали в с.
Каждый из них, независимо от другого, попадает с вероятностью ку со звездочкой.
Чтобы с вероятностью ку со звездочкой в квадрате.
Все, отлично, смотрите, что мы сейчас сделаем.
Сейчас будет абсолютно гениальный ход.
Так, друзья, помните, у нас когда-то доказывалась великая теория Мердыша про то, что вбивают графы с большим обхватом и большим хроматическим числом?
Там была гениальная идея что-то выкинуть лишнее.
Помните, была такая?
Значит, знаете, что мы сейчас сделаем?
Мы сейчас из с, вот этого, которое у нас получилось, из с, я тебе скажу, из с.
Выкинем по одному представителю, я будет кататься, по одному представителю из каждой пары, принадлежащей к этой паре.
Принадлежащей w'.
В w' находятся пары множества, которые находятся в с.
Ух, как же-то я так.
Конечно, здесь больше либо равняются двойки, наоборот, мне не больше единиц, это плохие пары, виноват.
Портал Катар застанет. Загубил все.
Конечно, больше либо равняется двойки.
Не, ну, друзья, это не очень страшно.
На самом деле ничего страшного не случилось, ну, противоположное неравенство должно быть.
Это не те пары, которые должны попасть в ук, а те, которые не должны туда попасть.
Не, ну, друзья, ну, слушайте, с точки зрения сущности, пока ничего не поменялось.
Ну, как? Ну, хорошо, вот это пары, которых не должно быть.
Что?
Да точно так же. Нет, здесь тоже больше либо равно. Это вот сохранилось, конечно.
Мы берем w с каким-то условием и его прореживаем просто.
Так, чтобы вот мат ожидания мощности w' было мат ожиданием w на q2.
Да что вас так смутило-то сейчас? Ну, но поменял я это неравенство от этого.
Что изменилось? Вы пока все равно не понимали, зачем оно нужно в ту сторону.
Теперь логично выкидывать. Я почему вспомнил?
Ты вот поглядел сюда, поглядел сюда и перестаю понимать, что происходит.
Больше же ничего не произошло, товарищи. Ну, что вас так смутило-то?
Ну, звонок, ну, давайте еще пять минут. Ну, что же я могу сделать?
Ну, я хочу просто, чтобы было понятно. Вот выкинем из c по одному представителю из каждой пары.
У нас останется какое-то множество c со звездочкой.
Друзья, но вы понимаете, что в этом множестве уже нет ни одной пары, которая пересекается вот так.
Вот в этом множестве каждые две k1 элементные штуковины пересекаются так, как нам нужно с точки зрения вот этого определения.
Ну, сейчас будет. Катар, смотрите.
Я, y, kt первое.
Тем самым, больше либо равняется мат ожидания мощности c со звездочкой.
Но больше либо равняется, потому что мы предложили некий рандомизированный алгоритм от искания вот той цепочки, которая заложена сюда.
c со звездочкой это c, из которого выкинули по одному элементу из w штрих.
Но, может быть, из разных мы выкинули одно и то же. Я не знаю, так могло получиться.
Поэтому здесь тоже надо писать не равно, а больше либо равно. Но это хорошо для нас.
Это мат ожидания мощности c минус мат ожидания мощности w штрих.
По линейности просто. Мы из c вычли w штрих, но при этом мы могли что-то вычитать не столько раз, но тогда будет больше либо равно.
Так, переписываем. Мы же знаем, что это такое. Вот мат ожидания мощности c.
Это mu на q со звездочкой, а тут получается дельта пополам на q со звездочкой в квадрате.
Вот катарсис неполный, потому что полный будет в начале следующей лекции, но неполный, смотрите, в чем состоит.
Мы же вольны были с самого начала выбрать q со звездочкой оптимально.
Мы его просто хотим, чтобы он был в 0.1.
За счет того, что здесь получилась квадратичная функция после этой дополнительной рандомизации, выбор q со звездочкой дает оптимизацию этой разности.
Но какое здесь q со звездочкой оптимально? Надо производную приравнять к нулю, там минус b поделить на 2, как в школе учат.
Так, что там получается? Минус mu поделить на минус дельта, то есть mu поделить на дельта.
Друзья, ну давайте подставим mu, деленное на дельта еще сюда, то у нас получится mu квадрат поделить на дельта,
минус дельта пополам, mu квадрат на дельта квадрат, чпок-чпок, mu квадрат на дельта,
минус mu квадрат на 2 дельта, это mu квадрат на 2 дельта.
Сейчас, друзья, понятно?
Ну нам-то надо было вот это получить.
Что отсюда следует?
Ну слушайте, ну я не знаю, я должен, наверное, вас отпустить, у вас следующая пара.
Должен, потому что если мне дать еще 10 нит, я все объясню, но это жду как, ну всю перерыв весь забирать.
Это не мой стиль.
Значит, друзья, катарсис абсолютный будет состоять в том, что, смотрите, для того, чтобы получить вот это,
смотрите, для того, чтобы вот это получить, мы лему чуть-чуть не доказали, для того, чтобы вот это получить отсюда,
нужно, чтобы между mu и дельта было совершенно понятное соотношение.
Ну, вот это mu квадрат на дельта должно вот этому оказаться равным.
Мы увидим, товарищи, что вот это вот q со звездочкой принадлежит 0,1 ровно потому, что k1 это m в кубе плюсу малое от единицы.
Я уж вам раскрою этот катарсис, а то вы забудете к следующему разу.
Вот было совершенно загадочное, мы долго дискутировали, откуда, зачем, вот оно нужно, чтобы вот это попало в 0,1.
Ну, мы это посчитаем аккуратно в начале.
Такая жизнь.

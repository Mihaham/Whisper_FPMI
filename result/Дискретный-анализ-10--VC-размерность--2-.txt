Так, ну, я еще раз формулирую теорему, собственно, Вапника
и Червоненкеса, которую нам предстоит доказать.
У нас есть пространство, размерность которого равняется
Д, у нас есть Эпсилон из 0 и 1.
Утверждается, что тогда для любого С из Х, ой, что
уж такое-то, из Х мощности меньше бесконечности, существует
Эпсилон сеть размера не больше, чем верхняя целая
часть от 8D поделить на Эпсилон, лог двоичный 8D поделить
на Эпсилон.
Вот такое утверждение, правильно, совпадает с тем, что я говорил
в прошлый раз?
Не равна D, а не больше D, но это не важно, конечно,
да, потому что важно только, что она не превосходит
чего-то.
Могу сказать не больше, это не имеет значения.
Так, ну, я не помню, уже обводил я вот это обозначение
или нет, пусть буквой М обозначается эта верхняя
целая часть.
На самом деле, я выкладки подробные с этой целой частью
проводить не буду, там все будет понятно и так, но
неважно, это пусть будет для краткости М.
Доказывать теорему мы будем вероятностным методом.
Так, давайте пусть N, это мощность нашего S, оно конечное,
в нем N элементов.
И давайте будем строить, ну, Эпсилон-сеть, откуда выбирается
из S, правда же, потому что что мы делаем?
Мы пересекаем R маленький с каким-то с этим S, рассматриваем
проекцию R большого на вот это множество S, пересекаем
R маленький из R большого с этим S и пытаемся, рассматривая
вот такие вот, только пытаемся найти для вот этих подмножек
систему общих представителей.
Ну, давайте ее выберем случайным образом.
Как видите, ну, что надо здесь подчеркнуть, когда
мы с вами работали с общим случаем системы представителей,
мы верхние оценки делали жадным алгоритмом, и только
для нижних оценок вероятностный метод работал интересным
образом.
Здесь мы делаем верхнюю оценку размера минимальной
SOP, которую называем Эпсилон-сетью, и для нее применяем вероятностный
метод.
Так, ну, вероятностный метод простейший, вытаскиваем
из множества S, M элементов, ну, то, что называется с
возвращением.
Говорит вам о чем-нибудь такой термин или вам такой
не употребляли?
Ну, то есть, мы строим, как это называется, размещение
с повторением случайное.
Да, я говорил это слово на алкотече, но, может быть,
я не всегда это делаю, да, мы берем, на алкотече точно
было размещение с повторением, но, может быть, я действительно
сказал на алкотече, что в вероятности вам употребляют
термин с возвращением, и вам не употребили, а оно
осталось так от меня, так от информации, ну, может
быть, да.
С возвращением это фактически значит, что мы строим размещение
с повторением, то есть, мы вытаскиваем один элемент
из S случайным образом с вероятностью единицы поделить
на N, каждый конкретный получится, потом, как бы, укладываем
его на место и снова из всего множества выбираем случайный
элемент.
То есть, то, что у нас получится, может получиться мультимножеством,
размещением, да, с повторением.
Ну, ничего страшного, нам же нужно маленькое множество,
если какие-то элементы совпадут, мы их отрождествуем.
Это же не должно кого-то смущать.
В общем, давайте назовем то случайное мультимножество,
которое у нас получится, буквой N.
Если хотите, чтобы было предельно понятно, наверное,
я так всегда и говорю, это вероятность просто равна
1 поделить на N в m-т степени.
Сколько есть всевозможных размещений с повторениями,
ну, такова в минус первой степени будет вероятность,
понятное дело.
Наша цель доказать, что с положительной вероятностью
вот такая вот штуковина является тем, что нам нужно,
то есть, епсалон сетью.
Можно N производить от слова net, ну, сеть, так, чтобы запоминалось
лучше.
Ну, давайте напишем плохое событие, назовем его E1.
В чем состоит плохое событие?
Событие – это множество элементарных исходов,
элементарный исход – это епсалон сеть.
То есть, мы все вот эти m-вытаскивания осуществили,
получилось случайное элементарное событие N, а событие будет
состоять в том, что для случайного множества N существует такое
r маленькое из r большого, что оно как нужно пересекается
с исходным s, то есть не меньше, чем по епсалон n, но в пересечении
с n большим пусто, неприятностей.
Так, согласитесь, что это плохое событие, то есть,
наверное, наша цель – доказать, что его вероятность меньше
единицы.
Если вероятность E1 меньше единицы, то ура, мы победили.
Существует такое n большое, такая епсалон сеть, что
все хорошо.
Так, ну, давайте, как ни странно, ведем вспомогательное
событие.
Это вот здесь наиболее интересный ход, которого нам еще не доводилось
встречать, когда мы с вами изучали какие-то вероятностные
методы, случайные графы, еще что-нибудь.
Интересный ход ввести некую загадочную дополнительную
случайность, но именно она сработает, если вы в целом
охватите доказательства, а я постараюсь, то у вас
будет катарсис.
Не, не, сейчас все будет совершенно не мистично, конечно, просто
я сейчас напишу явно, что такое E2.
Загадочность будет состоять в том, как можно додуматься
до того, чтобы вот ввести такую случайность и потом
на этом сыграть, но я буду стараться по ходу дела это
объяснить.
Вот, короче, мы, смотрите, что делаем, мы берем и еще
одно такое же мультимножество t строим, каким было n большое,
то есть вероятность t, это опять 1 на n в mt, и снова выбираем
случайное размещение с повторением просто с помощью
возвращения элементов, никак не зависящий от того,
как мы до того выбрали n, вот n большое выбрано,
а мы еще берем m испытаний, m раз, вытаскиваем элементы,
берем, получаем t, вот E2 это событие, которое естественно
теперь уже состоит из пар множеств, мультимножеств
n и t, пар мультимножеств n и t, таких, что существует
r, пока все не загадочное, r пересеченное с s больше
либо равно epsilon n, r пересеченное с n пусто, ну то есть ничего
нового, но t же появилось, и вот тут вот, мощность
r пересеченного с t больше либо равняется epsilon m пополам,
вот это вот момент, который сейчас выглядит как загадочный,
когда этого можно было додуматься, там вообще что это, зачем
это, постепенно будем прояснять, то есть теперь в свете того,
что у нас случайными объектами являются пары мультимножеств,
пары размещений, мы конечно и E1 можем проинтерпретировать
таким же Макаром, то есть тоже здесь написать n и t,
но никаких условий на t просто тут не накладывается,
t любое. Согласно, что можно считать, что события E1, E2 живут
на одном и том же вероятностном пространстве, но просто в случае
события E1, E2 не имеет никакого смысла, по тему пробегаем
по всем возможным вариантам, никаких условий на него
не накладывается, а в случае E2 мы налагаем вот это вот
ограничение, которое пока действительно выглядит
как нечто абсолютно загадочное, но скоро прояснится. Нет,
я не стараюсь вам вскружить голову совсем, вы же меня
поймите, но просто если бы я вот так написал и не предпослал
этому вот этого кряка, там еще чего-то, то мне кажется,
что вас действительно бы проканало и вы бы тогда
крякнули, а я заранее крякнул, теперь все понятно. Нет,
всегда что-нибудь новенькое, по-моему, я еще ни разу
про кря не говорил. Так, ладно, вот такие два события,
ну понятно, что из события E2 следует событие E1, оно
просто вложено в него, то есть очевидно, что вероятность
E2 меньше либо равна вероятности E1, но вот удобство состоит
в том, что в обратную сторону оценка почти такая же, то
есть вероятности этих событий близкие, но как мы узнаем
еще позже, оценивать вероятность E2 гораздо проще за счет вот
этого дополнительного условия, как оно сработает,
мы позже узнаем. Идея была у товарищей, которые придумали
такое доказательство, именно в этом, добавить некоторое
дополнительное ограничение, чтобы вот это неравенство
не слишком ухудшилось, но при этом использовать
его потом для оценки вероятности. Так, LEMma1, я утверждаю, что
вероятность E2 не меньше, чем 5 шестых, например,
на вероятность E1. Можно здесь написать 1 вторую, этого
тоже в итоге хватит, но мы докажем с 5 шестых, что
же не написать 5 шестых. Вот это вот ровно то, что
я сказал, с одной стороны очевидно, что E2 меньше,
чем E1, с другой стороны не сильно меньше, и вот это
помогает. Так, ну LEMma1 сейчас докажем. Давайте
напишем E2 при условии E1, условную вероятность напишем.
Ну это понятно, что такое, это вероятность E2 пересеченного
с E1, по определению просто, поделить на вероятность
E1. При этом пересечение E2, E1 это что? E2, правильно,
это вероятность E2 поделить на вероятность E1. Ну, мы
что хотим доказать, что это больше либо равно 5 шестых,
правильно же. То есть цель теперь в рамках LEMma1 доказать,
что вероятность E2 при условии E1 больше либо равна 5 шестых.
Вот, ну на самом деле, если мы знаем, что выполнилось
событие E1, то есть что уже нашелся такой R, который
в пересечении с s не меньше, чем εn, и который с n большое
пуст, то фактически вероятность чего мы считаем? Того, что
R пересеченное с t не меньше, чем, правильно? То есть фактически
интересующая нас условная вероятность, это есть просто
вероятность того, что R пересеченное с t больше либо равняет
цепсалон m по полу. Так, друзья, тут вот очень важный
момент, которого вы не отследили, никто не спросил,
а я не сказал. Поскольку множество t большое равно,
как и множество n большое, мы выбираем с кратностями
возможными, элементы могут повторяться, то мощность
мы считаем ровно в таком же смысле. То есть мы вытягиваем
очередной элемент и записываем единичку, если он содержится
в R и ноль, если не содержится. Поэтому повторяющиеся
элементы мы учитываем столько раз, сколько они туда
попадают. Вот эта мощность, это не мощность множества,
а мощность мультимножества тоже. Это очень важно для
себя пометить, иначе нам будет крайне трудно считать,
но мы воспринимаем это именно так. То есть это пометить
можно было еще вот здесь, но я забыл про это сказать,
а вы не спросили. Сейчас я быстро говорю, что ли, товарищи?
Да-да-да, я думаю, что в этот момент действительно вам
трудно было это отследить. Это надо было сообразить
прям, как это должно было, такая молния. Слушайте, а как
вы понимаете мощность, если множество с кратностями?
Вот так понимаю. Забыл про это сказать, прошу прощения,
но вы себе это обязательно пометите, это очень важно.
Как раз это позволит нам легко считать вот здесь все.
Потому что давайте я картину нарисую, стандартную
сардельку условную, это в данном случае множество
s, который у нас имеет мощность n. Мы пересекаем r с s,
вот это r с r наше, вот есть какое-то пересечение.
Какое это пересечение? Оно мощности не меньше, чем
εn по условию, правильно? Вот здесь вот не меньше,
чем ε умножить на n элементов. То есть когда мы выбираем
очередной элемент при построении множества t большое,
он попадает в эту заштрихованную часть с вероятностью не
меньше, чем что. Я все подсказал, я сказал не меньше, чем что.
Когда мы строим множество t, мы выбираем его из всего s.
Мы каждый очередной его элемент выбираем из множества s
с вероятностью 1 поделить на n. С какой вероятностью
этот элемент оказывается в пересечении r и s? Ну и
ε конечно, тут же εn элементов. Давайте я это словами напишу.
Каждый элемент из t попадает в r пересеченное с s с вероятностью
не меньше, чем ε. Следовательно, вот это пересечение
и его мощность, это биномиальная случайная величина
вида бином, я не знаю, как вам обозначали, я обычно
целиком слово пишу. Вот м, это число испытаний, сколько
раз мы вытаскиваем элементы очередной, столько элементов
в множестве t и не меньше, чем ε. Число испытаний и
вероятность успеха. Успех попали в пересечение r и t,
неудача не попали. Т попало в r пересеченное с с или не
попало в r пересеченное с с? Успех неудача. И мы м раз
бросаем монетку, каждый раз просто проверяем, попало
не попало, попало не попало. Сколько раз попало, такова
и мощность. Я нормально объясняю, да? Вот это биномиальная
случайная величина вот с таким распределением. Ну
значит, вероятность того, что она не меньше чего-то,
не меньше, чем вероятность того, что бином от м и просто
в точности, ε, больше либо равняется того же. Я просто
занизил вероятность успеха, если вероятность успеха
уменьшить, то и вся вероятность только уменьшится. Вероятность,
что успехов будет много, тем меньше, чем меньше
чем меньше вероятность успеха, вот так надо сказать.
Их много нам нужно, но я занизил и получил неравенство в ту сторону, как мне надо,
потому что я 5 шестых доказываю, видите, 5 шестых хочу доказать.
Так, ну, слушайте, бином, атем и апсилон как-то много букв.
Давайте я буквы это обозначу.
Так, к чему равняется математическое ожидание это, товарищи?
Но это, я надеюсь, все понимают.
Ну, ответьте мне, пожалуйста, это имеет биномиальное распределение.
Немо-спытание, апсилон, вероятность успеха.
Какое мат ожидания?
Скажите, пожалуйста.
Забыли, что ли?
А линейность слабо?
Сколько?
Эпсилон М, конечно.
Ну, товарищи, ну что ж такое безобразие?
Ну, М раз бросаем монетку, 0 или единица, неудача или успех.
По линейности, апсилон М, конечно.
Ну, давайте я напишу вот так.
Вероятность того, что это минус мат ожидания это больше либо равняется
Эпсилон М пополам минус мат ожидания это равняется вероятность это
минус мат ожидания это больше либо равняется минус Эпсилон М пополам.
Потому что это это Эпсилон М.
Здесь я сохранил обозначение, ну, конечно, это Эпсилон М.
А здесь вот вычел, честно.
Я хочу к неравенству Чебышова все это дело свести, товарищи.
Это есть единица.
Минус вероятность того, что это минус Е.
Это не превосходит минус Эпсилон М пополам.
И вот к этому я уже могу применить неравенство Чебышова, согласны?
Ну, хорошо, я напишу вот так.
Так, это больше либо равно один минус вероятность того, что модуль
это минус е это на ли это больше либо равняется Эпсилон М пополам.
Но просто вот в этом событии два варианта.
С одной стороны, может быть, вот это разность больше либо равна.
А может быть, вот это же разность меньше либо равна.
Со знаком минус, как тут.
То есть тут есть два варианта, и оба вычитается под знаком вероятности.
А тут только один.
И он вычитается, ну, понятно, что соотношение, как я написал,
соотношения, как я написал. Больше либо равно. Но уж тут-то вы не можете не узнать
неравенство Чебышова. Видите, модуль тут. Получается, больше либо равно 1 минус
дисперсия. Это поделить на квадрат вот этой правой части, я писал, n пополам в
квадрате. Так, товарищи, ну поскольку у вас трудность вызвала подсчет
математического ожидания, то дисперсия, я боюсь, будет еще труднее.
Чему равна дисперсия? А у вас были уже какие-то предельные теоремы? Ну там
муавролапласа, может быть, что-нибудь такое. Медамиальные, это случайная величина у вас
были, правда ж? А предельных теорем про них не было.
Пуассона, муавролапласа, этого вам не рассказывали. Я просто не знаю, в каком
порядке. А муавролапласа еще не было. Ну, тогда вы, наверное, еще не помните, что
бывает такое выражение n, p, q.
Не, ну смотрите, обычно же биномиальное распределение, как пишут, n число
испытаний, p — вероятность успеха, но это общее место. Или у вас как-то по-другому
это делают. n и p, правильно? n, p — это мат ожидания. Вот у нас оно просто получилось
m эпсилон, но это n, p, да? А дисперсия n, p, q. Ну, боже мой, это можно посчитать.
Дисперсия эта равняется дисперсия это 1, плюс и так далее, плюс это m, где это
1 от m — это единичка или нолик, смотря по тому, какой стороной фишка легла, да?
Ну, то есть единица с вероятностью эпсилон, ноль с вероятностью 1 минус эпсилон.
Ну, счастье в том, что они независимы, поэтому дисперсия суммы — это сумма
дисперсий. Так, дорогие товарищи, вы же это знаете, ну что вы мне будете
рассказывать? Середина весеннего семестра, вероятность уже в самом разгаре.
Там уже и мера была совсем... Сессии не было, ну хорошо, да. Я понимаю, что сессии не было,
но это же вы знаете все-таки, что если независимая величина, то дисперсия их
суммы — это сумма дисперсий. Вот. Что такое дисперсия величины? Давайте, я вот здесь
напишу. Это it, 1 с вероятностью эпсилон и ноль с вероятностью 1 минус эпсилон.
Какая дисперсия у такой величины? Ну, надо взять мат ожидания это it в квадрате
и вычесть мат ожидания это it в квадрате. Так проще всего. Квадрат случайной
величины такого же, как она сама, поэтому мат ожидания вот это — это эпсилон.
А тут эпсилон в квадрате получается. Если вынести эпсилон за скобку, то будет, как раз как я
сказал, p на q. Эпсилон на 1 минус эпсилон. А тут еще сложить, поэтому получается m эпсилон
1 минус эпсилон. Ну, обычно пишут npq. Просто у нас такие обозначения для числа испытаний
и вероятности успеха. Так, мы еще заметим, что это, конечно, не больше, чем m эпсилон. Тупо.
Вот. Поэтому все больше либо равно. Давайте я это в скобке возьму как пояснение. Дальше выкладку
продолжу. Больше либо равно единица минус эпсилон m поделить на эпсилон m пополам в квадрате.
Смотрите, как хорошо что-то сокращается. 1 минус 4 поделить на эпсилон m. Так, к m сходить что-ли?
Вон оно. Ну ладно, вы посмотрели и будете. Так, теперь пишем эпсилон m. Больше либо равняется,
там верхняя целая часть, поэтому я могу ее оценить своим аргументом. m умножить,
ой не, m умножить, а эпсилон умножить на 8d на эпсилон лог двоичный 8d на эпсилон. Это я
m просто переписал без верхней целой части, но оцениваю снизу. Так, шлёп-шлёп. Слушайте,
ну d больше либо равно единицы. Мы же не знаем, какая размерность. Ну, а значит, не меньше единицы,
что там. Вот я здесь это не важно, так и напишу. 8d заменил на единицу, здесь эпсилон меньше единицы,
а d больше единицы. Ну, значит, d на эпсилон больше единицы. То есть, еще умножить на логарифом
двоичный 8. Просто d на эпсилон убрал, грубо оценил. Это 3, это 8, того 24. Получаем,
больше либо равно 1 минус 4 на 24. Это 5 шестых. Тяп-ляп. Так, я эпсилон m оценил снизу,
но он в дроби, еще со знаком минус. Поэтому дважды перевернулся, все правильно. Неравенство в ту 100.
Какую нужно. Все, я доказал. Цель реализована. Но это не вся цель. Это пока только наше понимание,
что вероятность E2 на самом деле не сильно меньше вероятности E1. Поэтому, сумеем мы оценить
вероятность E2 как-нибудь достаточно хорошо, значит, вероятность E1 мы тоже почти так же хорошо оценим.
Вы помните, нам нужно доказать, что вероятность E1 меньше единицы. Если мы сейчас докажем,
что вероятность E2 меньше единицы хотя бы в 1,2 раза, то все получится. 6 пятых.
На целые две десятых. Но это вот самое интересное. Там как раз будет катарсис, потому что где тут
размерность ваплика черваненкиса пока совершенно непонятна. Как она сыграет? Какой момент?
Ну все, я пишу лему 2. Это оценка вероятности события E2.
Да, ну и, конечно, я думаю, что вы прекрасно понимаете, что стертая мною только что m,
оно пока не сработало во всей красе, потому что те оценки, какими мы пользовались,
ну они идиотские. Ну что там D больше единицы, E меньше единицы? Неужели бы это так было задумано?
Нет, конечно, сейчас это потребуется в большей красоте, но я не буду саму выкладку проводить,
сейчас вы все увидите. В общем, лему 2, которую мы предельно аккуратно докажем, это утверждение о том,
что вероятность E2 не больше вот такой величины g от 2m запятая D, я сейчас напомню, что это такое,
на 2 в степени минус Эпсилон m пополам, где g от nD это есть сумма по k от 0 до D, c из n по k,
и мы пользовались тем, что это не больше, чем n в степени D. Помните это то, что фигурирует в лемме 1 из прошлой лекции?
В прошлой лекции тоже была лемма 1, она была про то, что если у нас есть пространство данного размера и данной мощности,
то количество областей в нем, мощности r большого, ограничено вот этим g, где n это мощность, а D это размерность.
Ну и мы пользовались в какой-то момент тем, что это не больше, чем n в степени D. Смотрите, давайте я не
доказывать буду эту лему, а сразу поясню, как из этого теорема-то следует. Она следует сразу.
Как из этого следует теорема? Мы знаем, что вероятность E2 больше либо равняется 5 шестых вероятностей E1,
ну значит вероятность E1 не превосходит 1,2 умножить вот на эту штуку.
Дальше мы пишем, это не превосходит 1,2 умножить на n, а на 2m в степени D и умножить на 2 в степени
epsilon m пополам. И еще раз вспоминаем, что m это верхняя целая часть 8d на epsilon, лог двоичный 8d на epsilon.
Я утверждаю, товарищи, это примерно такое же рассуждение, как тоже на прошлой лекции было, только там
была лемма какая-то 2. Я утверждаю, что m подобрано таким образом, чтобы степенная функция 2m в степени D
убивалась отрицательной экспонентой 2 в степени минус epsilon m пополам. Просто вот так подобрано.
Помните, я в прошлый раз такие же мантры произносил? Я утверждаю, что вот если вы подставите вместо m сюда
вот такое выражение, то это будет меньше единицы. Что и требовалось, нам ровно это нужно. Я не буду заставлять вас
проводить эту выкладку на экзамене, и всех попрошу не заставлять, потому что это невеликое умение, черт возьми.
Ну потратите полчаса, восстановите эту выкладку. Я тоже сейчас помучусь, восстановлю.
Но все же понимают, что по параметру m вот эта штука растет как многочлен, а эту убывает как экспонента.
То есть совершенно точно существует такое m, зависище только от D, от epsilon, начиная с которого это все меньше единицы.
Но утверждает, что вот оно такого вида. А пафос ровно в этом мы с этого начинали нашу дискуссию перед лекцией.
Пафос в том, что нет зависимости от мощности s, от n, а есть зависимость только от epsilon и от размерности пространства.
Так что все, я еще раз повторяю, эту выкладку мы проводить не будем, не надо. А вот что надо сделать аккуратно,
это доказатель m2. Ну она не то что тютелька в тютельку, но очень близко.
То есть я думаю, что при каких-то epsilon и D пограничных, это прям в точности.
Но единственное, что, знаете, она в тютельку в тютельку скорее подобрана, если здесь писать не одну целую две десятых, а два.
Так что она не в тютельку в тютельку. Ну просто вот в оригинальной работе, по которой я рассказываю,
там вот это вот было написано не 5 шестых, а одна вторая. И по одной второй уже было подобрано вот такое m.
Но поскольку можно сделать лучше 5 шестых, то, наверное, можно еще лучше сделать m.
Ну я не считал, мне это как-то не очень интересно. Хотите, посчитайте, порадуйтесь.
Ну как бы это правда интересно, ну в принципе. Но мне не особенно, а вам может быть.
Лучше поймете, как жизнь устроена.
Как машинное обучение?
Как машинное обучение, это другой вопрос. Я про статистику, кстати, расскажу потом.
Что?
А, все, я понял, что это не отражание.
Да-да-да, сейчас про другое. Я еще лему-2 не доказал.
Не-не-не, теорема еще не закончилась, ей еще жить и жить и радоваться.
Лему-2 докажу, вот тогда будем считать, что теорема доказана. Хорошо? Вольная идея, да?
Так, ну хорошо, что тут я могу стереть?
Ни в коем случае само событие не надо стирать, а вот это можно вполне себе удалить.
Нет, про машинное обучение, конечно, не расскажу.
Потому что тут много слов нужно. А про статистику расскажу, потому что вы же знаете законы больших чисел в теории вероятности?
Уж это было? Какой-нибудь закончик больших чисел? ЗБЧ.
Ну вот я вам напомню. Я же не буду доказывать ЗБЧ. Вам его докажут в вероятности или уже доказали?
Ладно, давайте действовать. Доказываем лему-2.
Для того, чтобы доказать лему-2, нужно сделать хитрый финт.
Надо переопределить вероятностное пространство, в котором мы живем.
Ну, по-другому просто его задать, так чтобы получилось то же самое пространство.
Как мы его сейчас задаем? Мы последовательно выбираем me элементов и потом еще me элементов, независимо от первых me.
То есть сначала ne, вот это большое, выбираем, потом te большое, независимо друг от друга получается.
И получается 2 me чисел, почему чисел? Элементов, 2 me элементов, которые между собой как угодно могут совпадать.
Вот так устроено наше вероятностное пространство.
Давайте его зададим по-другому. Давайте сперва выберем 2 me элементов множества s.
2 me элементов множества s с возвращением.
Но пока не скажем, что такое n большое и что такое t большое.
А затем, уже выбрав их, назовем это мультимножество или размещение, назовем это мультимножество и размещение буквой u.
Выберем 2 me элементов, вот это мультимножество назовем буквой u.
А затем, разобьем u на две части равных.
Разобьем u на две равные части.
Выбирая половинку разбиения, давайте я все напишу словами.
Не совсем мой сейчас стиль, но неважно.
Половинку, так понять не будет, разбиение n или t, неважно.
С вероятностью 1 поделить на c из 2 mpi, то есть классическим образом.
Согласно классическому определению вероятности, то есть половинку выбираем из множества всех половинок.
Одну из.
Но тут какая опять тонкость, которую надо подчеркнуть.
Представьте себе, что вот это u, например, получилось просто состоящим из 2 me одинаковых элементов.
Может же такое случиться? Вот такое u выбрали.
Все равно каждая половинка выбирается с вероятностью 1 поделить на c из 2 me по me.
Хотя любая из них это просто me, раз повторенное.
Я понятно говорю, да?
То есть у нас могут возникать одинаковые пары n и t.
Ну как, собственно, они и здесь могли возникать.
Понимаете, что мы получили по сути то же самое.
Так сказать, распределение на парах множества.
Здесь мы последовательно их выбирали одно, потом другое.
А здесь мы их все вот это объединение выбираем, называем его u,
и потом разбиваем на две части.
Это то же самое.
Получаем те же самые n и t.
Такой двойной счет, если хотите.
То есть если в невероятностных терминах говорить, то это просто двойной счет.
Мы по-другому считаем, в другом порядке.
Вот так переопределили вероятностное пространство.
И теперь будем действовать.
Так, но эту всю красоту надо стирать.
Так, ну давайте, вот как сделаем.
Давайте заметим, что вероятность событий E2, которая нас интересует,
может быть, конечно, посчитана по формуле полной вероятности.
Следующим образом, можно перебрать все способы выбрать мульти множество u мощности 2m.
И для каждого из этих способов посчитать вероятность события E2,
при условии, что u зафиксировано, и осталось только его разбить на две непересекающиеся части.
Но это формула полной вероятности.
Неправильно написал, надо еще на вероятность u умножить.
Так, надо еще умножить на вероятность u.
Вот это будет формула полной вероятности.
Ну, по u, по всем множествам, мульти множествам u мощности 2m.
У нас двуступенчатый выбор теперь такой.
Мы можем разложить все на способы, как мы выберем u.
Вероятность u, ну, там один поделить на n в степени 2m, что-то такое.
Но поскольку все вероятности u одинаковые, то нам действительно достаточно теперь доказать ту же самую оценку для условной вероятности.
Достаточно доказать, что вероятность E2 при условии u не превосходит g от 2m d на 2 в степени минус epsilon m попало.
Если мы это докажем при каждом конкретном у, то мы докажем и для всего E2 такую же оценку вероятности.
На самом деле даже не важно, что все вероятности u одинаковые.
Если мы условную вероятность вот в этой сумме умеем вот так оценить каждую, то мы вот это все вытаскиваем за знак суммирования,
а сумма по u вероятности u это всегда единица.
Сумма всех вероятностей всегда единица, не важно совпадающие числа, не совпадающие.
В общем, короче, нам достаточно доказать, что для любого u вероятность E2 при условии u вот такая, и больше чем столько.
Так, теперь давайте введем событие E2 запятая r.
E2 у меня, слава богу, не стертая. Сейчас будем сравнивать.
Это множество nt таких, что r пересеченное с s, а, с s не надо, таких, что r пересеченное с c пусто, и r пересеченное с t не меньше, чем epsilon m пополам.
Вот сейчас возникнет как раз понимание, зачем нужно было вот это дополнительное условие.
Оно возникнет ровно когда мы будем оценивать вероятность E2 r.
Но правда не просто его, а при условии u еще.
Так, чем отличается E2 r от E2? Смотрите на E2, смотрите на E2 r.
В E2 существует r, которое вот так пересекается с s, и дальше, собственно, условия события.
Так, друзья, вот очень важный момент. Вы понимаете, что вот это не является условием события?
Потому что s это фиксированное, не случайное множество. Условия события, вот они, их два.
А вот это, это вот сюда относится.
Существует r, которое пересекается с s достаточно сильно, и дальше такое, что наши случайные n и t вот так с ним соотносятся.
Важно понимать. То есть я что хочу сказать?
Я хочу сказать, что E2, это, конечно, объединение E2 r,
по всем r из r большого таким, что мощность r пересеченного с s большим больше либо равняется εn.
Ну, с этим, я надеюсь, вы согласны, потому что вот тут у нас по r квантор существует,
а квантор существования равносилен взятию объединить.
Сейчас будет самый главный, самый тонкий момент, который надо будет осознать.
Ну, тоньше меня.
Ну, не насколько, но не очень. Да, вот уж прямо тонок вы как-то так переживаете.
Ну, я хочу просто вам подсветить ключевые моменты,
поэтому вы когда будете, может, пересматривать лекцию или кто-то будет смотреть, кто сюда не дошел, к сожалению,
тоже услышат, что вот этот вот важный момент.
Понимаете, вот в этом объединении дохренища событий.
Но на самом деле при условии U, если U уже зафиксировано, многие из них совпадают.
Нас-то что интересует? Нас интересует вероятность E2 при условии U.
Вот мы хотим доказать, что выполнено такое неравенство.
Там тупо, можно было бы тупо, вот давайте я напишу тупо, можно было бы написать вот так.
Это не больше, чем сумма по всем r из r таким, что r пересеченное с s не меньше, чем εn, n не меньше, чем εn.
Ну, и здесь написать вероятность E2r при условии U.
Это тупое неравенство, мы просто тупо суммируем по всем вообще r-кам, которые обладают вот этим свойством.
Их много до чертовой бабки, сейчас ничего не получится.
Поэтому я тупое неравенство с вашего позволения сотру.
Я хочу гораздо меньшим объединить, это все оценить, убедившись, что когда U зафиксировано, в этих событиях не так много разных.
Так, куда я взял тряпку? А, у меня целых две, потрясающе.
Тупо не будем оценивать, не хочу тупо, ничего не получится.
Так, смотрите, вот U у нас зафиксировано, давайте вот эта сарделька, это U.
Но только не забывайте, что U все-таки это мультимножество на самом деле, то есть в этой сардельке могут быть совпадающие элементы, но это не так важно.
Не забывайте также, что U состоит из 2m вот этих возможно совпадающих элементов, вот тут написано.
Сейчас я это тоже здесь укажу, тут 2m.
А, кто-то мне звонит, что ли? Я не слышу сейчас, что-то важное. Ой, сейчас, извините, алло.
Да, вообще, ректорат у нас на третьем этаже главного корпуса.
Ну да, в общем, я сейчас лекцию просто закончу читать и приду.
Да, спасибо.
Так, сейчас виноват.
Так, так, так. Вот у нас U зафиксировано.
Сейчас.
Что-то.
Секунду.
Подождите.
Сейчас.
Па-па-па.
Сейчас, что тут? Подождите.
Топлю. Секунду.
U зафиксировано, U мы выбирали из S.
Что такое-то?
Сбился чуть-чуть. Сейчас, извините.
Что такое? Подождите, извините.
Что такое сейчас?
На пустом месте сейчас, на ровном.
Это заскок. Сейчас, секунду.
Что ж такое? Подождите, я не могу привязаться к этому, секунду.
Что-то мы выбирали из S.
R пересеченное.
Что такое?
Подождите.
Сейчас, секунду.
Ух, ух, ух, ух, ух.
И это тоже, конечно.
Конечно, это может быть.
Ух, ух, ух.
Ух, ух, ух, ух.
А, ух, ух, ух, ух.
Ух, ух, ух.
Сейчас, простите, пожалуйста, что-то я сбился в этом месте, не знаю, почему. Вроде совершенно
стандартный момент. Я вот, видимо, действительно очень тонкий. Не, ну, никогда не был тонким,
что-то я не пойму, что меня сейчас смущает, но почему-то вот меня это смущает. 2м элементов
ФС, да, мы с С пересекаемся по Эпсилон Н, Е2Р, а, все, наверное, не, подождите.
Сейчас, вот у нас есть Экелес.
С у нас мощности N, У у нас мощности 2m.
Значит, если R. Что ж такое-то? Что ж я туплю-то? Вот смотрите, пусть у нас R1 какое-то множество,
из С, ой, Господи, из R такое, что R1 пересеченное с С имеет мощность не меньше, чем Эпсилон Н. И R2 такое же,
причем они оба таковы, что если мы R1 пересечем с У и R2 пересечем с У, то получится одно и то,
вот то же. Пусть у нас есть два множества области, которые находятся в этом объединении,
по которым берется объединение, по областям, которые с С пересекаются достаточно жирно.
Вот пусть у нас есть две таких области, которые с С пересекаются достаточно жирно и при этом
их пересечение с У совпадают. Мы же и Суп artists выбираем Н и Т, правильно, мы из У legalized
Мы из У выбираем Н и Т, потом уже У зафиксировано, а на Н и Т мы его потом случайно разбиваем.
Вот если у нас есть два множества R1 и R2, имеющие одинаковые пересечения с множеством У,
я утверждаю, что тогда E2R1, не знаю, что я застрял, все просто,
и E2R2 это одинаковые события, просто одинаковые события.
Потому что в каждом из них говорится о том, как вот это R пересекается с N,
но N само находится внутри У, по определению.
Поэтому важно только, как изначально это R с У пересекалось, чтобы потом смотреть,
с соответствующим N оно пусто пересекается, с соответствующим T оно жирно пересекается,
или что-то из этого не выполнено.
Согласны, да, что если вот эти пересечения завпадают, то и события одинаковые.
Состоят из одних и тех же N и T пар.
Следили за мыслями?
Вот это был тонкий момент, я, честно говоря, не понимаю, почему.
Я, видимо, специально его затончил, чисто подсознательно.
Мне сказали, насколько он тонкий, меняется, вот там перещелкнуло, а вдруг он правда...
Ничего, ничего, это как раз хорошо.
Всегда полезно задуматься, а что же здесь является, так сказать, центральной пружиной.
Ну вот так, конечно, устроено.
То есть получается, что в этом объединении различных событий...
Сколько?
Различных событий вот в этом объединении при условии фиксации U,
если U зафиксировано, то различных событий в этом объединении...
Дайте я словами, пишу прям.
Различных событий в объединении не больше, чем мощность проекции на вот это U нашей системы областей R.
Ну что такое проекция на U системы областей R?
Это как раз множество всех различных пересечений R маленьких из R большого с вот этим U.
Для одинаковых пересечений события одинаковые.
Значит, разных событий столько, сколько разных таких пересечений.
А эти разные пересечения как раз и образуют проекцию, как мы ее определяли в прошлый раз.
Сейчас вроде все аккуратно сказано.
Ну а размер проекции как раз и не превосходит g от 2md.
Это следствие из лемма-1 прошлой лекции.
Потому что у нас размерность пространства d, а мощность U это 2m.
С учетом кратности.
Ну и все. То есть у нас получается, что вероятность...
Ну почти все, еще не все.
Еще откуда-то должна вот эта хрень вылезти.
Это-то еще не все.
Сейчас мы еще хрень откуда-то вытащим.
Так, E2, что мы там оцениваем при условии U, не больше, чем g от 2md
на вероятность E2 запятая R при условии U.
Но это чуть-чуть может быть некорректная запись,
потому что кто сказал, что эти все вероятности одинаковые?
А никто не сказал.
Написать подробнее, наверное, надо.
Давайте я напишу подробнее.
Это вот так.
Сумма по R, вероятность E2RT при условии U,
где количество слагаемых не больше, чем вот такое.
Что в объединении не больше, чем столько разных элементов,
ну значит складывать мы должны не больше, чем столько слагаемых.
Ну короче, я хочу доказать, что для любого R,
это я хочу доказать, что для любого R вероятность E2RU при условии U
не больше, чем 2 в степени −εm½.
Если я докажу, что это верно для любого R,
то то, что я писал и потом стер, вполне корректно.
Действительно, тогда вот по модулю этого, я сейчас докажу,
у нас получается не больше, чем g от 2md на 2 в степени −εm½.
Это есть ровно утверждение L2.
Мы ее доказали.
Но доказали по модулю того, что в скобках.
То есть осталось еще скобочное рассуждение.
Проверьте, утверждение, проверьте.
Так, оно несложное, конечно.
Ну несложное, несложное, все равно думать надо.
Так, самый тонкий момент я стираю с доски.
Но он вроде уже свою роль сыграл.
Так, так, так, как же это доказать?
Ну у нас тут есть картина какая-то.
Вот есть r пересеченная sum.
Давайте буковкой P обозначим его мощность.
Так, P это будет мощность r пересеченного sum.
Мощности все понимаем с учетом кратности, как обычно.
Значит, r пересекаем sum.
Смотрим мощность пересечения.
Называем это буковкой P.
Смотрите, если P меньше, чем epsilon...
Плохо написал, меньше, чем epsilon m пополам,
то интересующая нас вероятность E2r при условии у большого,
чему равна, давайте вы мне это скажете.
Или чего она не превосходит.
Значит, смотрите, вот тут мало элементов.
Тут элементов меньше, чем epsilon m пополам.
А нас интересует вот это событие.
В частности, в этом событии есть условие,
что r пересеченная step больше либо равняется epsilon m пополам.
Но T это живая половинка, так сказать, ату.
Помните, там не важно, что элементы совпадают.
Вот там пример нарисован.
T это мы как-то разрубили пополам.
Но как такое может быть, что r пересеченная sum маленькая,
мы разрубили u пополам, и вот сюда из множества T
попало наоборот больше элементов?
То есть я хочу сказать, что это 0, конечно, вероятность.
Такого быть не может.
Даже с учетом кратности, столько элементов туда попасть не может.
Мы разрубили пополам уже классическим образом,
учитывая кратность заранее.
Но отсюда следует, конечно, что p больше либо равняется epsilon m пополам.
И в этом предположении мы теперь будем оценивать нашу условную вероятность.
Пишем так.
Вероятность E2r при условии u и вот этого, что p не меньше, чем epsilon m пополам.
Ну, просто в этом предположении.
Ну, она в любом случае, конечно, и в этом предположении,
в каком угодно не больше, чем вероятность того,
что r пересеченная sn пусто.
Согласитесь.
Я просто убрал.
Вот в этом месте уже убрал условие, что r пересеченная st большое.
Я им воспользовался для того, чтобы оценить размер вот этой части.
Иначе вероятность совсем ничтожная.
Все, оно сработало.
Оно свою роль отыграло.
Это ружье выстрелило.
Теперь, с какой вероятностью r пересеченная sn пусто?
Но это значит, что половинка n вот как-то так проходит.
Не зацепляет вот это пересечение.
r пересеченная su такое, а r пересеченная sn пусто.
Ну, какая классическая вероятность у такого события,
если n большое состоит из m элементов
и выбирается с вероятностью 1 поделить на c из 2m по m?
Ну, цшки надо какие-то одну на другую разделить.
Какую?
На какую понятно?
На c из 2m по m.
А какую на нее надо разделить цшку?
Ну, 2m минус p.
По m просто, и все.
Товарищи, понятно это?
Ну, нам просто надо выпустить это.
Ну, нам просто надо выбрать те m элементы,
которые с этими p не пересекаются.
И они будут благоприятствовать,
как в теории вероятностей говорят, нашему исходу.
Исход состоит в том, что r пересеченная sn пусто.
Какие n этому благоприятствуют?
Любые, которые с этими p элементами ничего общего не имеют.
Таких c из 2m минус p по m.
Ну, и делим на все c из 2m по m.
Нет?
Сомнения какие-то?
Здесь вроде все совсем просто.
Ну, и давайте через факториал это перепишем.
2m минус p факториал на m факториал на m минус p факториал.
Чуть-чуть места не хватает.
Ну, ничего.
2m факториал.
Тут тоже m факториал.
И еще раз m факториал.
Елки-палки.
Так, ну ладно.
Один вот это вот сократим.
Куда деваться?
А дальше будем сокращать крест-накрест.
Вот 2m минус p факториал и 2m факториал.
А тут m факториал и m минус p факториал.
Ну, придется подняться сюда наверх.
Чуть-чуть не хватило места.
Но сейчас все получится.
Так, значит, это равно.
Господи, два.
Так, так, так, так, так.
Так, так, так.
Так, так, так.
m тут сверху, да?
m, m минус 1, m минус p плюс 1.
Это я сократил.
m факториал, m минус p факториал.
А 2m минус p и 2m факториал в знаменателе дадут.
2m, 2m минус 1, 2m минус p плюс 1.
Ну и все.
Потому что вот эта одна вторая,
а вот эта уже меньше, чем одна вторая.
Потому что 2m минус 1 больше, чем 2m минус 2.
И вот эта меньше, чем одна вторая,
по той же причине с запасом.
Все меньше и меньше.
Видите, да, что они уменьшаются?
Вот, а их п штук.
То есть получается, что это меньше либо равно
2 в степени минус p.
Но p-то у нас больше либо равно, чем epsilon m пополам.
Значит, это меньше либо равно 2 в степени минус epsilon m пополам.
Все доказано.
Ладно.
Давайте, наверное, на этом мы сегодня завершим.

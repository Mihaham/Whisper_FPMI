15.30. Я думаю, можно начинать. Всем доброго дня. Для начала, давайте я напомню, на чем основались
прошлый раз. Мы дали определение дисперсии. Дисперсии случайно увеличены. Мы определили
ее по вот такой формуле, а также доказали формулу для вычленения дисперсии. Кроме того,
я в прошлый раз не сказал про стандартное отклонение. Есть еще такой термин стандартного
отклонения. Обычно обозначается маленькой буквой сигма. Это просто корни дисперсии. Ну,
дело в том, что если у вас кси измеряется в метрах или в рублях или еще в чем-нибудь,
то дисперсия будет уже измеряться в квадратных метрах или в квадратных рублях. Чтобы иметь дело
с правильной размерностью, обычно корни извлекают из дисперсии. Мы также начали обсуждать свойства
дисперсии. Первое свойство было дисперсия положительна, что очевидно, потому что
просто квадрат случайно увеличен и всегда положительный. Следует из первого свойства
математического ожидания. Кроме того, можно даже сказать, когда дисперсия равна нулю, это означает,
что вот такое математическое ожидание равно нулю. А если математическое ожидание квадрата случайно
увеличено равно нулю, мы в прошлый раз обсуждали, что это означает, что случайно лично просто
почти наверно равна нулю. Ну, то или иными словами, кси равна своему вот ожиданию почти наверно.
Ну, то есть такое бывает только когда случайная величина имеет константное распиление. Второе
свойство, которым в прошлый раз было, это как меняется дисперсия при линейном преобразовании.
При сдвиге на константу она не меняется, а при умножении она множественно выносится в квадрате.
Это тоже очень просто. Давайте я сразу сформулирую третье свойство. Третье свойство такое, если
случайные величины независимы, то тогда дисперсия суммы равна сумме дисперсии. Кроме того,
если есть несколько случайных величин, которые попарно независимы, то тогда тоже дисперсия
суммы это сумма дисперсии. Чтобы это доказать, я использую кавариацию, про которую мы тоже
начинали говорить в прошлый раз. Я напомню, что это такое. Кавариация двух случайных величин
определяется по следующей формуле. Для кавариации есть формула, аналогичная формуле, которая есть для
дисперсии. Эту формулу я, кажется, в прошлый раз не доказывал, поэтому докажу ее сейчас.
Наказательство. Пишу определение, раскрываю скобочки. Математическое ожидание – это константа.
Константу можно выносить за знаком от ожидания. Кроме того, от ожидания суммы 4 слагаемых можно
развить в сумму от ожидания полеиндиста. Получается от ожидания предведения. Дальше второе слагаемое,
от ожидания кси, выносится из от ожидания. Получается от ожидания кси, множественное от ожидания
это. Аналогично третье слагаемое. Четвертое слагаемое – это просто константа, от ожидания
константа. Это и есть та же самая константа. Можно сократить подобные слагаемые, и остается как
раз заявленная формула. В прошлый раз я начал обсуждать свойства дисперсии. Одно из свойств
заключается в том, что ковриация кси-кси – это то же самое, что дисперсия кси, что очевидно,
просто посмотрите на определение дисперсии и ковриации. Второе свойство заключается в том,
что если случайные личины независимы, то тогда ковриация равна нулю. Для этого нужно просто
посмотреть на формулу для ковриации, которую мы только что доказали, и вспомнить свойство
от ожидания, которое говорит, что если случайные личины независимы, то от ожидания предведения –
это предведение от ожидания. Поэтому второе свойство тоже уже по сути доказано. Третье
свойство заключается в том, что ковриация билинейна, билинейная симметричная форма. То,
что ковриация симметрична – это очевидно, я все равно это напишу. А еще она билинейна по каждой
кардинации, я напишу по первой кардинации. Билинейность любых констант А и В, можно вот так
по линейности раскрывать. Для приличия можно даже как-нибудь доказать свойства линейности к
ковриации. Например, по определению левая часть – это такое математическое ожидание. Теперь просто
правильно группируем слагаемые, ну и раскрываем по дистрибутивности. А теперь мы уже здесь
использовали линейность мотоожидания, когда вот тут раскрыли мотоожидание суммы. И здесь еще
раз используем линейность мотоожидания. Получаем А на мотоожидание, по сути, на ковриацию. В общем,
то есть правую часть. Билинейность ковриации напрямую следует из линейности математического
ожидания. Окей, теперь можно доказать третье свойство дисперсии, которое я уже сформулировал,
то что для независимых случайных величин дисперсия суммы – это сумма дисперсии. Мы знаем,
что случайные величины независимые, тогда их ковариация равна нулю. Что такое дисперсия суммы?
Дисперсия – это тоже ковариация по первому свойству ковариации. Теперь можно раскрыть по
линейности, по билинейности ковариации. Можно раскрыть, что это ковариация C кси плюс ковариация
и это плюс ковариация это кси плюс ковариация это это. Окей, две из четырех ковариаций равны нулю,
и остается только ковариация кси кси и ковариация это это. То есть дисперсия кси плюс дисперсия для
двух случайных величин доказали. Если их несколько, то, в принципе, все то же самое.
Пишем, что это ковариация суммы запятая сумма и, опять же, раскроем по линейности. Понятно,
что у нас получится N квадрат слагаемых. Там будут все возможные ковариации C и кси ижи по всем
возможным и ижи. Вот, среди них можно выделить тех вариаций, где i равно j, то есть ковариация кси и
кси и и можно посмотреть на оставшиеся. Там, понятно, дело будут просто ковариации. Я еще вот здесь двойку
напишу, а вот эти i меньше, чем j, что было похоже на формулу квадрата суммы. Все возможные удвоенные
ковариации получатся. Ковариация кси и кси это, понятное дело, дисперсия, а оставшиеся ковариации
просто равны нулю, поскольку мы предположили, что случайная величина попарно независима. Я думаю,
что все свойства можно считать доказанными. В качестве примера я посчитаю дисперсию
случайной величины, которая имеет биномиальный распределение. Причем двумя способами. Во-первых,
я первый способ посчитаю просто по формуле. Формула для дисперсии вот такая. Математическое
ожидание мы уже в прошлый раз считали. Математическое ожидание должно быть NP. Осталось
посчитать математическое ожидание квадрата. Для этого есть просто формула. То, что это сумма
k квадрат по всем возможным значениям k. Вероятность, что кси равно k. Распределение
биномиальное. Значения принимаются от нуля до n, и вероятность из значений тоже можно написать.
Далее я использую хитрую формулу, что k на cg по k это n на cg-1 по k-1, которую я еще в
прошлый раз использовал. И с помощью этой формулы тут получится kn вот так. Я еще буду
суммировать от единицы до n, поскольку нулевой слагаемое равно нулю. Понятно, что выносится за
скобочку вот это вот n. И еще p можно вынести за скобочку. Вынесли. Получилось k от единицы до n,
k на c, на p в k-1. А теперь n-k я напишу как n-1-k-1. Здесь хочется сделать замену. k-1 равно m,
чтобы суммирование было от нуля до n-1. Сделаю это прямо. Пользуюсь прекрасным свойством доски,
что можно стирать. m будет минус m. Вот тут m теперь от нуля до n-1, и тут будет m плюс 1.
Окей, теперь разобьем эту сумму на две суммы. В одной будет просто m, а в другой будет вот это
вот плюс единичка. Вот смотрите, вот эта первая сумма, мы ее в прошлый раз считали, она очень
похожа на математическое ожидание биномеральности случайной личины, только уже с параметром n-1p.
Это математическое ожидание у этой случайной личины z, где z имеет распределение биномеральное
с параметром n-1.p. И тут должно получиться n-1 умножить на p, если мне не изменяет
память. Вот, а вторая сумма, ее можно просто по биному ньютона свернуть в единицу и останется
просто n-p. Хорошо, все, осталось просто записать ответ. Дисперсия равна то, что мы вот здесь
насчитали. Я раскрою скобочки n квадрат p квадрат минус n-p квадрат плюс n-p и минус квадратом
от ожидания, то есть минус n-p в квадрате. Что-то сократилось и осталось n умножить на p минус
p квадрат или можно еще написать как n-p на 1 минус p. Окей, можно считать немножко по-другому,
а вспомнить, а именно вспомнить, что такое биномеральное распределение. Биномеральное распределение
это количество успехов в схеме испытаний Bernoulli. То есть xi это сумма следующих индикаторов,
n индикаторов, где событие ai означает, что на иным шаге случился успех. Вот. Утверждение,
которое я не буду доказывать, что эти индикаторы независимы. Это не то чтобы что-то сложное,
возможно вы чем-то таким уже на семинарах занимались. То есть можно показать то, что вот
эти события независимы. Это в принципе логично, потому что у нас же темы испытаний Bernoulli. Мы как
бы проводим независимое n испытаний, а потом из независимости этих событий будет следовать
независимость этих индикаторов. Тут нужно все-таки делать какую-то проверку, но я просто
сформулирую это как утверждение. Я даже не так. Даже напишу, что это упражнение. И если мы знаем,
что они независимы, то тогда дисперсию x можно расписать как сумму дисперсий вот этих индикаторов.
Что такое дисперсия индикатора? Дисперсия индикатора это от ожидания квадрата минус квадрат
от ожидания. Кроме того, индикатор принимает значение только ноль или один, а поэтому он в
квадрате просто равен себе. Поэтому вот эту двоечку можно убрать. Теперь надо сказать,
чему равно математическое ожидание индикатора. Математическое ожидание индикатора события это
его вероятность, а его вероятность это вероятность то, что на этом шаге существует успех, то есть это
просто p. Его подставляем вот сюда, получаем p минус p квадрат. В таких слагаемых n штук,
поэтому дисперсия равна n на p минус p квадрат, что в общем-то сошлось с ответом, полученным первым
способом. И еще одно понятие, которое нужно обсудить, говоря о дисперсии и ковариации,
это понятие корреляции по пленнику. Корреляции случайно включен кси и это, при этом мы сейчас
предполагаем, что они не константные. Корреляции называются по вариации, деленные на стандартное
отклонение. Здесь важно, что они не константные, чтобы дисперсии были не нулевые. Ну и соответственно,
если корреляция равна нулю, то случайные личины называются некоррелированными.
Можно заметить, что вот это пленно очень похоже на формулу для косинуса угла между векторами,
если вы знаете, как искать косинус угла между векторами. Поэтому я формулирую утверждение,
корреляция всегда от минус единицы до единицы. Причем равенство бывает только тогда, когда одна
случайная величина линейно выражается через другую. Это утверждение, благодаря ему,
можно воспомнить корреляцию как такую вот числовую характеристику независимости случайных величин,
потому что если случайные величины независимы, то корреляция равна нулю. Потому что ковариация
равна нулю, значит и корреляция равна нулю. А если они вот совсем зависимы, то есть вот такая вот
линейная зависимость, такая наиболее сильная зависимость, то корреляция настигает свои крайние
значения, то есть плюс-минус единицы. Нужно сказать, что тем не менее понятие некоррелированности
независимости путать не нужно, потому что в одну сторону это верно, если независимость, то корреляция
равна нулю, а в другую сторону это уже неверно. В премьере когда это неверно, я привозил на прошлый
лекции. Поэтому, конечно же, нельзя сказать, что корреляция отражает всю степень зависимости
случайных величин. Независимость это более емкое понятие, нельзя понятие независимости вместить
вот всего лишь в одну числовую характеристику. Нужно доказать утверждение. Нужно доказать, что
модуль кавариации не больше чем произведение дисперсии. Распишем по определению, что такое дисперсия,
что такое кавариация. Слева получается математическое ожидание вот такое, а справа вот что-то такое.
Это верно просто потому, что это частный случай не нравится Кашибуниковскому, которое было доказано
на прошлой лекции. Произведением отождаем квадратов, хотя бы квадратом отождаем
Также я заявлял, что мы можем сказать, когда достигается равенства. Действительно можем, потому что мы уже в прошлый раз выяснили,
когда достигается равенства не равенства Кашибуниковского, а оно, я напомню, достигается, когда
члены личной линии независимы. То есть тут будет равенство. Равенство, если существуют такие
действительные АВ, такие, что А на кси минус мутаж дания кси, плюс В на это минус мутаж дания это, равно нулю почти наверно.
Кроме того, понятно, что А не равно нулю, потому что если А равно нулю, то тогда
случайная личность это почти наверно является константой, а мы сказали, что
корреляцию мы считаем только для неконстантных случайных личин. А раз А не равно нулю, можно написать, что кси
равно минус В на А умножить на это, плюс какая-то константа.
Неправильно разделил на А, надо все делить на А, а не только первое слагаемое.
Вот так получилось. Окей, вот это константа, вот это тоже константа, и мы получили линиенную
зависимость, как и было ранее заявлено. Окей, давайте сделаем 5-минутный перерыв и после этого
продолжим. Продолжаем. Переходим к последнему разделу той части курса, где мы говорим про
вероятность. Это произведет, посвящен на самом деле, будет закону больших чисел. Но для начала
нужно доказать несколько неравенств. Неравенство Маркова и неравенство Чебышова. Неравенство Маркова.
Включаем, начиная кси, должна принимать не отрицательные значения и дана действительно
константа А тоже не отрицательная. Давайте даже скажем положительно, А больше нуля. Неравенство Маркова
утверждает, что вероятность, что кси больше, чем А, не больше, чем вот ожидание кси делить на А.
Для доказательства возьмем математическое ожидание кси и кси представим в виде суммы двух
случайных величин. Сейчас напишу каких. Тут даже можно вот такой знак поставить, чтобы сильнее
получилось неравенство. Кси представляем в виде суммы вот такой случайной величины и вот такой
случайной величины. Понятно дело то, что ровно один этих двух индикаторов равен нулю, а один равен
единице, поэтому то, что вот здесь написано, это просто во всех точках вероятного пространства равно
кси. Пишем по линии математического ожидания. Теперь скажем то, что поскольку кси принимает
только не отрицательные значения, второе слагаемое не отрицательное, кроме того вот такая случайная
величина кси на индикатор больше, чем случайная величина А на индикатор. Ну просто потому что или
обе они равны нулю, или если они не равны нулю, то в левой части написано кси, в правой части написано
А, а еще так они не равны нулю, только если кси больше чем А. Вот поэтому можно написать, что это
больше и равно, чем мотождание А на индикатор того, что кси хотя бы А. Константы вносятся за знак
мотождания, получается мотождание индикатора. Мотождание индикатора это вероятность. Вот осталось
разделить это все на А и доказать с неравенством марки. Второе неравенство, неравенство Чебышова.
Оно утверждает, что для любого Эпсилон больше нуля, вероятность кси отличается от своего
мотождания больше, чем на Эпсилон, не превосходит дисперсии, деленной на Эпсилон квадрат. Это
практически прямое следствие неравенства Маркова, потому что ну надо просто взять случайную величину
вот такую квадрате, а в качестве А взять Эпсилон квадрат. Тогда если мы подставляем
этого неравенства Маркова, мы получаем вероятность, что вот такой квадрат больше, чем Эпсилон квадрат,
не меньше, чем мотождание кси минус мотождание кси квадрате делить на Эпсилон квадрат. Как раз
у числителей появляется дисперсия, а вот это неравенство равносильно вот этому неравенству.
Неравенство Чебышова позволяет с помощью дисперсии оценить разбор значений случайной величины, как-то
понять, насколько мы можем отличаться от своего мотождания. Например, давайте в качестве Эпсилон
возьмем, например, 10 корней из дисперсии. И поставим неравенство Чебышова. Получится,
что вероятность вот такая не больше, чем 0,01. Это значит, что с вероятностью 0,99 случайная
величина принимает значение близки к своему мотомическому ожиданию, близки в том смысле,
что не больше, чем на 10 корней из дисперсии отличается. Вероятностью 0,99 кси это мотождание кси
плюс минус 10 корней из дисперсии. Когда я пишу плюс минус, я имею ввиду, что мы в интервале вот
таком. Поэтому очень круто, когда дисперсия маленькая, когда мы что-то знаем про значение
случайной величины. Такие они. Окей. Все, что не нравится с Чебышовым, мы можем доказать закон
больших чисел. Закон больших чисел в форме Чебышова в следующем семестре у вас будет много
законов больших чисел в каких-то других формировках. В этом семестре только один
будет в форме Чебышова. Как он звучит? Есть кси1, кси2, кси3, последовательность независимых,
одинаково распределенных случайных величин. При этом мы знаем, что существует конечная дисперсия
ну, например, кси1. Я замечу, что, во-первых, если существует дисперсия, то автоматически
должно существовать и математическое ожидание, потому что дисперсия определяется через
математическое ожидание. Во-вторых, формировки теоремы у нас тут случайные величины имеют
одинаковое распределение, а дисперсия это характеристика, которая зависит только от
распределения случайных величин. Таким образом, если у кси1 существует дисперсия, то и у остальных
случайных величин тоже существует дисперсия, и они причем равны. Можете еще раз, что такое NORSP?
Давайте я напишу. N это независимые, OR это одинаково распределенные, то есть имеют одинаковое
распределение, то есть одинаковый набор значений, и эти значения они принимают с одними теми же
вероятностями. SV это случайные величины. Окей, я не закончил формировку. Формировка такая, вероятность,
что средняя отличается от математического ожидания. Давайте я вот так напишу. Вот тут
напишу А. Вот здесь напишу, что А это математическое ожидание. Это вероятно, что среднее значение
ксишек отличается от математического ожидания больше, чем на епсилон. Тремиться к нулю для
любого положительного епсилона при н-сремящемся бесконечности. Вот так. Идейно, что здесь происходит.
Идейно, по сути, закон Больших учитель, это о том, что средняя архитектическая в каком-то смысле
стремится к своему математическому ожиданию. Окей. Доказательства. Я буду применять неравенство
Чебышова. Давайте вспомним, что такое неравенство Чебышова. Посмотрели. В качестве кси в неравенстве
Чебышова я возьму вот это среднее арифметическое. Дальше я утверждаю, что вот это А это еще и
математическое ожидание кси. Потому что, смотрите, математическое ожидание среднего
арифметического 1n можно вынести как константу. Тут получится мат ожидания суммы. Это сумма
математических ожиданий. Получается сумма мат ожиданий с нитой. Но случайные величины имеют одинаковое
распределение, поэтому все математические ожидания равны, и они равны числу А. Слагаем их на штук,
поэтому тут получается А. Окей. То есть, вот эта вероятность, это есть не что иное, как вероятность
модуля кси минус мат ожидания кси. И тут применимо неравенство Чебышова. Это не больше, чем дисперсия
кси делить на эпсилон квадрат. Окей. Дисперсия среднего арифметического зеленого на н делить на
эпсилон квадрат. Константы из дисперсии выносятся в квадрате, в участии н квадрат и эпсилон квадрат
в знаменателе, в числителе остается дисперсия суммы. Вот. Случайные величины по условию независимые, тут
кстати становится понятно, что мы используем на самом деле попарную независимость. Хватает слабой
такой вот независимости, только попарной, чтобы закон больших чисел в форме Чебышова работал.
Получается n на дисперсию одного элемента, n сокращается. Вот. Ну дисперсия константа,
эпсилон константа, n в знаменателе, значит это все стремится к нулю. И закон больших чисел доказан.
Вот. Что тут можно сказать? Я сделаю два замечания. Первое замечание стоит в том, что в формировке
законы больших чисел в форме Чебышова можно даже условия независимости ослабить до условия
некоррелированности. Независимость можно ослабить до некоррелированности случайных
величин. И тоже все будет работать. Ну это связано с тем, что когда мы доказывали вот здесь вот третье
свойство дисперсии, то что сумма дисперсии для независимых случайных величин, мы единственное,
что использовали, это то, что к вариатору она нулю. То есть по сути мы использовали только некоррелированность.
Второе замечание стоит в том, что можно даже ослабить вот это условие одинаковой распределенности,
то есть можно не требовать одинаковой распределенности. И вместо этого просто потребовать,
чтобы сумма дисперсии не слишком сильно росла. На самом деле закон больших чисел это один из самых
важных результатов в теории вероятности наравне с центральной предельной теоремой, которую я только
сформулирую, но доказывать не буду. Вам ее обязательно докажут в следующем семестре,
Максим Евгеньевич, я надеюсь. Центральная предельная теорема без доказательств. Опять есть
последовательность независимых одинаково распределенных случайных величин. Опять же должна
существовать конечная дисперсия, когда любых действительных А и Б, причем не только действительных,
можно в качестве А и Б брать плюс-минус бесконечность. Вероятность вот такая. Я опять
же обозначу математическое ожидание буквой А, а стандартное отклонение буквой сигма,
когда вероятность вот этого события сходится к следующему интегралу. Выглядит, конечно,
довольно ужасно, наверное, по крайней мере, если еще есть первый раз на это смотреть. Какой-то
вопрос? У нас в условии А фигурирует как результат мат ожидания и любое А из Р и плюс-минус бесконечность.
Ой, спасибо за комментарий. Это, конечно, беда. Давайте тогда вот так сделаем. Вместо А и Б
напишем х и у. Теперь все должно нормально быть. Что здесь по сути-то происходит? По сути,
эта теория говорит о том, что мы знаем, что сумма будет примерно nA, потому что средний это
примерно A, значит сумма это примерно nA из закона больших чисел. И вот эта теория говорит о том,
как ведет себя отклонение от вот этого среднего. Оказывается, оно стремится к нормальному закону,
по распределению, к нормальному закону. Что бы это ни значило. Представьте, что вы бросаете кубик и вы
очень хотите выбросить 6. Вот вы бросили его, выпало 1, бросили, выпало 3, бросило снова, снова. Вот
так вот 10 бросков сделали и шестерка не выпала. Вы в этот момент можете начать разочаровываться в
теории вероятности, но на самом деле вам просто не повезло. То есть такое бывает, что вы бросаете
кубик и вот хотите шестерку выкинуть, а она не выпадает. Но вот закон больших чисел говорит о том,
что вот если много раз выкинуть шестерку тысячу раз, то вот она выпадет. Тут уже как бы неважно,
везучие вы, невезучие, она выпадет. Причем выпадет примерно тысяча девять на шесть раз, плюс-минус.
А центральная предельная теория говорит о том, как будет располагаться, как распределено отклонение
от вот этого среднего. То есть если вы, давайте на примере, пусть шестерка выпадает на 1,6 и вы,
допустим, бросаете тысячу раз шестерку, тысячу раз бросаете кубик и тогда количество шестерка будет
иметь биномиальное распределение с параметрами NP, тысяча и одна шестая. Биномиальное распределение
это сумма независимых бернуливских случайных увеличений, поэтому тут применимо закон
больших чисел и центральная предельная теория. Так вот, вот вы провели эксперимент, бросили кубик
тысячу раз и посчитали, сколько шестерок выпало. Центральная предельная теория говорит,
что вот если вы вот такой эксперимент проведете очень-очень много раз, ну, например, тысячу раз,
то есть сделайте миллион бросков, а потом построите гистограмму того, что у вас получилось,
то есть там по оси X откладываете количество шестерок в каждом из экспериментов, а по оси Y то,
как часто такое количество шестерок у вас случалось, то у вас будет что-то такое и оно
будет опроксимироваться нормальным законом. В общем, так себе у меня получается рисовать.
Ну, я думаю, многие из вас такие графики уже когда-то видели и вас это не сильно удивляет.
Ладно, окей, на этом часть курса, которая относится к теории вероятности, подходит к концу. В следующий раз мы уже
начнем заниматься действительным анализом, то есть теория и меры. Пока я скорее просто сформулирую,
зачем нам заниматься действительным анализом. Нам нужно заниматься действительным анализом,
чтобы ответить на некоторые вопросы. Потому что когда мы говорили про вероятности пространства,
тут были какие-то омега, f, p, непонятно, что такое f, что такое p. Когда мы пытаемся вести вероятность,
какую-то классическую модель мы можем построить. Вот даже если уже говорите про геометрическую вероятность,
то мы в геометрической вероятности уже вероятность определяем, как там, условно там,
многомерный объем, делить там на многомерный объем. А объем уже можно считать не у всех множеств.
То есть отсюда следует вывод, что, наверное, вероятность в общем случае можно считать далеко не у всех множеств.
То есть есть какой-то набор множеств, которые называются событиями, и у них вероятность можно считать, а у остальных не можно.
Вот возникает вопрос, что это такое за f, какими свойствами оно должно обладать.
Также возникает вопрос, что такое p, как водить вероятность, и что такое вообще вероятность.
Потом мы начали говорить про случайные личины, и там еще больше вопросов возникло.
Что такое случайная личина в общем случае? Что такое распределение случайной личины в общем случае?
Что такое математическое ожидание случайной величины в общем случае?
Далее, когда мы вот здесь говорили про законы больших чисел и центральную предельную теорему, тут возникали какие-то сходимости.
Но хочется понять, что это за сходимость, и какое вообще смысл в это все складывается.
Что такое сходимость случайных величин?
В общем, вторая часть курса нужна для того, чтобы научиться отвечать на вот эти вопросы.
Если есть какие-то вопросы по сегодняшней лекции или по курсу, то можно спросить.
Если нет, то на этом все. Всем пока. До следующего раза.
До свидания.
До свидания.

Так, сегодня мы говорим про систему, которая называется Cassandra. Cassandra — это новое SQL-хранилище
данных. В нем можно создавать таблицы. В эти таблицы можно вставлять записи. У нас есть строчки,
у нас есть какие-то колонки, у нас есть типы данных. В общем, с виду эта система напоминает базу
данных. Но тем не менее, называется она новое SQL, потому что под капотом это вовсе не система
подобная, скажем, с Spanner, которую мы разбирали на другом курсе. Это система, которая представляет
все еще, разумеется, кивали у хранилища, и поверх этого кивали у хранилища выражена табличная
модель со всеми этими типами, схемами, колонками. Но при этом это хранилище, это кивали у хранилища,
построено по совершенно иным принципам. А именно, это кивали у хранилища позиционирует себя в
терминах капти аремы иначе. Вот не как Spanner, не как HBase. Кстати, у вас был HBase или еще нет?
Алло. Ну, я обычно не хожу, но вроде не было HBase. Понятно, трудно сказать. Ну, в общем,
хорошо было бы вот эту систему Кассандра и кивали у хранилища, которые в этой системе реализованы,
противопоставить другому хранилищу HBase, ну или Spanner, я не знаю. Конечно, противопоставление
довольно неудачное будет, правильно сравнивать с Spanner. Потому что вот Кассандра — это open
source, реализация принципов дизайна, которые были описаны в 2007 году, кажется, в статье,
которую упустил Amazon, назвался Amazon Dynamo. Высоко доступное кивали у хранилища. Но вот его написали
в открытом доступе, коз в открытом доступе лежит, поверх этого кивали у хранилища сделали табличные
модели. Вот получилась Кассандра. Кассандра, конечно, далеко ушла от изначальной задумки. Она
умеет гораздо больше сейчас, но принципы дизайна те же. Так вот, в 2007 году, когда эта статья была
написана, в Google уже примерно в то же время написали систему под названием Bigtable. И опять же,
она может быть знакома вам в вашем курсе по системе, которая называется HBase. Это опять же
open source, реализация конкурента Bigtable. Вот Bigtable и Dynamo — это две системы,
которые были прародителями, соответственно, HBase и Kassandra. И две эти системы, Dynamo и Bigtable,
выбрали разные буквы, как в теореме. То есть, в случае партишна, Dynamo готова оставаться
высокодоступным хранилищем, быть готовым обслужить записи и чтения, а Bigtable выкручен в
сторону согласованности. То есть, он не нарушит линейризуемость в этом случае. Разумеется,
ведутся системы себя по-разному, дизайны у них очень разные. Ну и вот сегодня моя цель рассказать
в первую очередь не как этой системой Kassandra пользоваться, какие там типы и какие таблицы. Это,
в конце концов, можно по документации разобрать довольно легко. А про то, по каким принципам эта
система построена и каких гарантий от них следует ждать, какого поведения, каковы их возможности,
масштабирование, репликация, все вот это. И я буду сегодня рассказывать не про Kassandra,
и не про Dynamo, и про то, и про другое, и периодически обозначать различия. То есть,
что было сделано в Dynamo, что иначе сделали в Kassandra. Но моя цель рассказать про какой-то
общий дизайн, про какие-то общие принципы, которые в основе этого подхода этих двух систем лежат.
Но для того, чтобы объяснить, почему дизайн именно такой, нужно как-то его мотивировать. Так вот,
давай перенесемся в прошлое, в 2000-е годы, в нулевые, и подумаем, что требовалось Amazon
для хранения, для обработки данных. В то время Amazon был еще не облачным провайдером, может быть,
уже и был, но, по крайней мере, все было еще не так, как сейчас. Он был не таким большим. Но он был
большим интернет-магазином. И задачу, которую ему приходилось решать, это хранение данных,
пользователей хранения, в первую очередь, обслуживания покупок в магазине. Если совсем
говорить коротко, то Amazon требовалось сделать такую масштабируемую, доступную, отказоостойчивую
корзину для товаров, куда пользователи могли добавляться, не знаю, там, в свои книжки, одежду,
что-нибудь еще, нажимать на кнопку «купить», и Amazon бы зарабатывал на этом деньги. Вот нужно,
чтобы понять дизайн системы Dynamo, нужно отталкиваться от такой задачи. Какие же
требования были у Amazon в такой системе хранения? Во-первых, эта система должна легко масштабироваться,
потому что нагрузка на магазин, она разная, она разная, не знаю, в течение недели, она разная,
в зависимости от того, есть ли сейчас какой-то праздник или нет. Ну, иногда так происходит,
что, знаю, там, «черная пятница» или «Рождество» и огромный наплыв пользователей, и нужно как-то
увеличить количество машин в системе, чтобы они справлялись с нагрузкой, с рейтом запросов,
с количеством покупок просто. Система должна масштабироваться горизонтально, но, естественно,
у нее не должно быть какой-то точки, какого-то узкого места, которое бы препятствовало этому
масштабированию. Мы хотим, причем не просто масштабироваться, мы хотим масштабироваться очень
гибко, то есть мы хотим наращивать свои емкости и вычислительные ресурсы более-менее произвольно,
то есть мы можем там в два раза увеличиться, а можем увеличиться на 10%, в зависимости от того,
как мы планируем нагрузку на нашу систему. Это первое требование. Система должна быть масштабируемой.
Ну, а если мы что-то знаем про дизайн определенных систем, то мы знаем, что строить масштабируемые
системы довольно сложно. Причиной, чаще всего, узким местом в масштабировании чаще всего служат
некоторые слой координации, некоторые слой, которые отвечают за метаинформацию, за распределение
данных. Ну, вот вы видели в курсе системы Hadoop, и там есть name-node, которое... HDFS, все перепутал. HDFS
тоже, но сначала про HDFS. У HDFS есть name-node, которое отвечает за хранение метаинформации о том,
из каких чанков там файлы состоят, где они лежат. И сама по себе файловая система может масштабироваться
на файлы огромного размера, произвольного размера. Мы просто добавляем машины, и диск растет,
на котором хранятся все эти данные. Но с другой стороны, растет и количество чанков, из которых
состоят файлы, и эта информация может переполнить мастер, name-node. Это некоторое ограничение
масштабируемости. Или, скажем, если мы говорим про Bigtable или про его пансорс-реализацию HBase,
то это киволюхранилище. Уже это ближе к нашей задаче. Это не файловая система,
это киволюхранилище. Но там тоже есть слой координации. Есть некоторый мастер,
который распределяет данные между узлами кластера и который знает, кто что обслуживает, где что лежит.
Если мы говорим про кавку, то опять у нас был координатор, который должен был снова в себя
все вмещать. Он действовал, конечно, аккуратнее. Он хранил данные во внешней системе в звуке
перенадежно, отказоустойчиво и мог отказать и телепортироваться на другую физическую машину.
Но тем не менее, есть такой координирующий узел. Одним из принципов, на которых строилась система
Dynamo, состоял в том, что такого координирующего узла такого центра в системе быть не должно.
Для того, чтобы построить масштабируемую систему, мы будем использовать децентрализованный дизайн.
Не будет мастера, не будет какой-то точке, который управляет всеми остальными. Мы хотим построить
симметричную систему, где все узлы будут одинаковыми и никакой узел не станет узким местом при
масштабировании и никакой узел не станет единой точкой отказа, потому что мы хотим обслуживать
пользователей. И наша задача приоритетная, чтобы, если человек нажимал на кнопку «добавить товар»,
а потом нажимал на кнопку «купить», то его транзакция фиксировалась в хранилище.
Итак, первый принцип – это масштабируемость, второй принцип – децентрализованный дизайн.
Ну и третий принцип, он все это вместе следует. Мы в терминах CAPTIOREM выбираем букву «a»,
то есть мы выбираем доступность. Мы, видимо, не собираемся использовать консенсус, потому что в
консенсусе, в случае partition, в меньшей части partition, система наша блокируется на запись. Мы не
хотим блокироваться на запись. Мы хотим обслуживать пользователей даже из двух половин, из двух осколков
partition. И да, мы понимаем, что если мы отказываемся от консенсуса, то это может привести к некоторой
несогласованности данных, которые мы пишем в системе. Но мы на это готовы пойти, потому что все же
мы Amazon собираемся хранить в нашей системе не какие-то там счета пользователей, не деньги,
а все же их корзину с товарами. Ну и мы готовы себе позволить определенные аномалии. Если мы,
скажем, добавим товар в корзину, а потом он вдруг исчезнет, а потом снова появится. Ну если это
будет случаться нечасто в каких-то вот таких случаях partition, то мы, в общем-то, готовы это
пережить. Главное, чтобы пользователь мог покупать подарки на Рождество. Итак,
вот такие принципы, и мы собираемся свой дизайн заточить вот под них.
HBase и Bigtable соответственно исповедуют совсем другой подход. Там используется консенсус,
там используется некоторый координирующий узел, там есть привычная репликация. Но вот
Cassandra от этого всего отказывается. Ну и, в общем, сейчас я расскажу, как именно она устроена.
Еще некоторое замечание важное, чтобы правильно спозиционировать свое отношение к всему этому.
Cassandra в силу своего дизайна все же заставляет пользователя Cassandra и Dynamo знать про свое
внутреннее устройство. Вот мы сейчас будем говорить про то, как Cassandra и Dynamo устроены,
не только потому, что полезно знать, по каким принципам такие системы вообще конструируются,
а еще и потому, что несмотря на свою табличную модель, несмотря на все эти таблицы, инсерты и
типы данных и схемы, все равно пользователь на самом деле не может не знать про то,
как Cassandra внутри устроена, как она распределяет данные, как она их реплицирует. Все равно,
чтобы понимать, как можно системой корректно пользоваться, какого поведения от нее ожидать,
требуется все же знать про внутренний дизайн. Ну что, давай перейдем на доску.
И будем говорить сначала про дизайн системы Dynamo.
Итак, мы хотим сделать кейвель-украинище с очень скудным, очень скромным API.
Bool Eager. На самом деле, сигнатура чуть сложнее, чуть интереснее, но до этого мы еще дойдем.
Никаких более сложных операций мы пока не хотим. Да и вообще, если говорить про историю этих
систем, то Amazon Dynamo никаких колонок, типов, таблиц и не было. Это было вот именно кейвель-украинище.
Вот для Амазона этого было достаточно. Ну конечно, было бы здорово тогда, вот в начале нулевых,
иметь какую-то распределенную масштабируемую базу данных, где все были бы все привычные
инструменты, таблицы, типы, транзакции, но в 2006 году в начале 2000-х не умели совмещать все вот эти
фичи и требования, которые к системе предъявлялись, а именно горизонтальная масштабируемость,
децентрализованность, высокая доступность. Вот эти цели и вот эти все фичи были не совместимы,
поэтому инженеры Амазона думали, выкинули все, что им не требуется для их задач продуктовой,
и вот остановились на кейвель-украинище. Итак, кейвель-украинища, операции Put и Get. Ну и можно
начать о реализации, говорить в терминах, ну не знаю, мы делаем хэштаблицу распределенную.
Мы можем в ней записать что-то и можем прочитать. Ну и у нас есть набор узлов,
на которых эта хэштаблица будет жить.
Ну и вопрос, как именно распределить между этими узлами ключи, которые мы собираемся хранить?
Вот если думать про кейвель-украинище буквально как про хэштаблицу, то можно применить какой-то
понятный дизайн. Вот у нас каждый узел, это же по сути bucket хэштаблицы, он может хранить
какой-то набор ключей, но не все. Наша задача каким-то образом распределить все ключи между вот этими
bucket. Ну и обычная хэштаблица, самая такая простая, незатейливая, делает это следующим образом.
У нас есть число узлов, но здесь это 9. И мы просто ключ, мы каким-то образом эти узлы
нумеруем и скажем, что за хранение ключа будет отвечать узел вот такой, хэш от ключа по модулю
числа узлов. Можем ли мы так распределить данные между узлами? Хотим ли мы так делать?
Ну можно некоторое узло будет слишком большая нагрузка. Почему? Ты про то, что есть какие-то
горячие ключи, к которым обращается чаще, но это более-менее неизбежно.
Да, давай пока задачу упростим и не будем говорить про отказоустойчивость вообще. Вот наша
задача просто распределить данные, распределить ключи между набором узлов.
А может из некоторых машин? А переконфигурации мы хотим выполнять?
Я же сказал, конечно мы хотим. У нас, не знаю, скоро черная пятница. Число,
если вырастет, мы, конечно, хотим добавить новые машины сюда. Вот, не знаю, плюс еще три вот эти.
Конечно, мы хотим. Причем мы хотим, чтобы это было делать легко. И у нас машин здесь,
может быть, сколько угодно. Там сотни, тысячи. И вот добавляем мы тоже какое-то произвольное
количество. Или можем потом отняти. Но такой способ распределения данных, он, конечно же,
не годится. Потому что он работает на уровне отдельных ключей независимо. И если вдруг мы
добавим, возьмем 9 узлов и добавим 10 узел, то вот это вот отображение ключей в машины,
оно прям все целиком поедет. Потому что у нас изменится. И мы должны будем как-то
перераспределить все данные, подвигать более-менее данные на всем кластере.
Разумеется, если мы добавляем всего лишь одну машину, то мы хотим, чтобы передвигалась какая-то
пропорциональная часть данных. Если у нас вот 9 машин и добавляем 10, то мы бы потрогали одну
девятую всех данных. Но уж точно не все. Так что такой подход, разумеется, не будет работать. Нам
нужно какое-то более сложное, более хитрое отображение. И эта идея называется... Ну как бы
мы делали, если бы у нас был какой-то волшебный узел, который бы координировал остальные?
Этот узел, он бы просто знал про диапазон всех ключей. Он бы нарезал этот диапазон ключей
на какие-то маленькие поддиапазоны, но и каждому узлу поручал свой. Когда у нас появлялся новый
узел, то мы бы что-то делили. Центра координации у нас такого нет. У нас есть отдельные узлы,
и мы должны придумать какую-то максимально простую процедуру распределения ключей между
ними. Эта идея называется consistent hash. Идея из обычных hash таблиц нам не подходит,
потому что при добавлении, при изменении состава пластера нам требуется выдвинуть слишком много
данных. Но мы можем поступать чуть хитрее. Вот у нас есть ключи, есть некоторая хреш-функция.
Вот мы нависуем такое кольцо. Здесь 0, здесь максимальное значение хреш-функции. И скажем,
что каждый узел в нашем кластере
выбирает себе некоторый токен
вот из этого диапазона и приземляется в некоторую точку вот этого кольца.
Но это кольцо, разумеется, виртуальное, оно существует только в нашем сознании.
Но, тем не менее, каждый узел пластера, на котором развернута наша система динамик,
занимает вот иметь такой токен, занимать какую-то позицию. Разумеется, токен нужно хранить надежно,
то есть узел добавляется в систему, он генерирует себе случайный токен и вот занимает какое-то
место. А дальше, когда пользователи приходят с операцией PUT или GET, то ключ этого пользователя
расшируется и опять же попадает в какую-то точку на кольце. И мы скажем, что если идти по кольцу
по часовой стрелке, то первый узел, который мы встречаем на этом пути, он и будет отвечать
за обслуживание нашего PUT или нашего GET. Идея понятна? Понятно. Вот если какой-то узел вдруг
добавляется в наш кластер, вот этот новый узел, то он занимает, на этом кажется, какое-то место
и, скажем, дробит какой-то диапазон. Был раньше такой диапазон, в него угодил новый добавляемый
узел и в итоге этот диапазон подробился на два. Но все остальные остались такими же.
Окей, вопрос. Как схема распределения данных? Чем этот подход не слишком хорош? Какие у него
есть минусы? Ну тогда клиент должен знать эти отображения. Кто за что отвечает?
Какая нота в какой участок кольца? Клиенту, конечно, знать про это не нужно. Нужно знать
самим узла. Что делает клиент? Он приходит с своим PUT или GET на произвольную машину. Эта
произвольная машина, мы ее назовем координатором операции, кэширует ключ, ну а дальше внутри
системы разбирается, что делать дальше. Вот как именно она это понимает, это хороший вопрос,
но в следующей очереди. Сам клиент, конечно, ничего знать не должен, он просто должен знать
про какие-то машины, которые входят в состав кластера. Он ходит, не знаю, через какую-то DNS
запись попадает в какую-то случайную машину и та его дальше обслуживает. Нужно, чтобы узлы самой
системы понимали, что происходит, как устроено это кольцо. Да, от клиентов не требуется. Я говорю
даже не про проблемы какие-то, как это написать пока, а про то, какие есть здесь изъяны в смысле
распределения данных. Вот будет ли это распределение равномерно?
Ну может хэширом кажется, что будет. Ну представь, у нас здесь кайта 64, у нас 2.64 степени различных
значений этой хэш-функции и у нас, не знаю, 100 машин.
Ну прямо скажем, невеликая вероятность, что вот эти 100 точек на этом гигантском кольце хэшей
поделят это кольцо на какие-то с размерной диапазоны. Правда ведь?
Не стоит этого ждать. У нас хэшей слишком много, а точек на этом кольце, ну узлов не так много.
Ну то есть, если у нас там, не знаю, десятки тысяч, это одна история, ну если у нас их просто десятки
или сотни, если мы маленький стартап, то у нас проблемы с распределением ключей.
Вот эта проблема, она на самом деле довольно легко решается, решается понятием виртуального узла.
Ну то есть, у нас физический узел один, но на вот этом кольце хэшей ему могут соответствовать
несколько точек.
Картинка, картинка усложняется, но принцип должен быть понятен.
Вот у нас есть один физический узел и вот у него есть такие виртуальные,
виртуальный экземпляр на этом кольце.
То есть, на одном узле сразу у нас несколько набор токенов и чем больше виртуальных узлов мы
сделаем, тем, ну у нас увеличивается объем метаинформации, но при этом у нас выравнивается
распределение ключей. Вот эти виртуальные узлы, это механизм, который позволяет нам
решить еще одну проблему, а именно неоднородность самих узлов.
Вот может быть какие-то машины просто более емкие, чем другие, там просто стоят более емкие
жесткие диски, дисков больше. Или просто машины, не знаю, там больше, гядер, которые могут
обрабатывать данные. С помощью виртуальных узлов мы можем распределять нагрузку между вот этими
узлами неравномерно. Если у нас есть разные конфигурации, просто железные, то более емки
конфигурация, более емким конфигурациям будет отвечать больше виртуальных узлов на кольце.
С каким образом можно более аккуратно распределять данные. Но у нас все равно есть некоторые
даты, все этот подход, понятно, он рандомизированный, мы токены выбираем случайно,
поэтому какая-то несбалансированность все же возможна. Вот так мы готовы распределять данные.
Но как-то справедливо заметил, не совсем понятно, что дальше делать клиенту, потому что эта картинка,
она, но только у нас в уме существует, а у каждого узла просто есть набор его токен. Вот какой-то
узел получает ут или гет от клиента, и что ему дальше делать? Ему нужно знать вот все эту карту
пластера, ему нужно знать про весь кластер, токены, всех других узлов. При этом состав этого пластера
он еще и меняется к тому же. Как мы будем решить эту проблему? Кажется это несложно,
мы просто будем обмениваться данными со всеми. Что значит обмениваться данными со всеми? Тут
нужно все-таки аккуратнее сказать, как именно мы собираемся делать. Поддерживать отображение,
когда мы себе токен берем, то отправляем всем. У каждого узла на данный момент есть. Как же им
подсинхронизировать всю эту картину мира? Просто мы администратор, мы добавляем в кластер три
новых узла. Вообще говоря, они про всех остальных мало что знают, и остальные пока тоже про них
ничего не знают, и продолжают обслуживать запросы, но вот как обычно. Мы же не можем просто пойти на
месте администратора и поговорить с каждым узлом. Но мы не можем, потому что не все не могут быть
доступны прямо сейчас. Мы должны какой-то более устойчивый протокол придумать. Есть. Гвозди протокола
называется в общем случае. Как можно распространять некоторое знание, ну или не знаю, по аналогии
распространить инфекцию в сети, где ты не знаешь всех участников. Идея такая, что каждый узел
знает про какие-то другие узлы. Картина мира каждого узла может быть неполной. Он может не знать про все
узлы. Ну а скажем, новички, которые добавляются в кластер, они знают только про наборы некоторых,
некоторые выделены под набор узлов, которые называются силами. Про остальных мы ничего не
знаем. И для того, чтобы сойтись к одному и тому же представлению о составе кластера, о этой карте,
в этом кольце, в фоне действует следующая процедура. Мы из всех наших соседей, из всех узлов,
которых мы знаем, выбираем случайный и с ним обмениваем все информации о том, про какие узлы
знаем мы и про то, про какие узлы знает он. Но чуть аккуратнее мы обмениваемся даже не узлами.
Технические трудности. Вот узлы обмениваются с эктрами дельтами о том, про какие версии,
каких узлов они знают. И если вдруг в кластере появляется узел, то он общается с некоторыми
сидноудами. Сидноуды узнают про этот новый узел, а дальше, обмениваясь с другими узлами,
рассказывают постепенно про вот этих новичков. Ну и если все сделать аккуратно, то примерно за
логарифм таких терраций все про всех узнают. То есть у каждого узла будет представление, какие машины
с какими токенами в кластере есть, и получая запрос в кутере к клиенту, любая машина может
перенаправить клиента в нужное место. Это фоновая процедура, она всегда работает на узге,
ну и вот обновляет свое знание о системе. Заметим, что если бы мы строили систему более
централизованную, если бы мы использовали дизайн, скажем, похожий на как, если бы мы
использовали зукипер, то мы бы делали совершенно иначе. Вот зукипер позволял такие проблемы решить
гораздо проще. С укипером можно было бы завести директорию, в которой бы каждый узел системы
создавал бы эфемерный узел, эфемерный z-note, в котором были бы написаны его токены. Вот это
такой централизованный механизм, через который узлы бы узнавали друг от друга. Просто центральная
директория, где отказы устойчивые, туда можно пойти и узнать про скуще состояние кластера. Здесь
система децентрализованная, она не хочет зависеть от доступности зукипера, поэтому вот из-за этого
выбора у нас вместо зукипера здесь появляется госип. И госип в динамо играет двойную функцию. Во-первых,
он обеспечивает обмен информации о составе кластера, о новых узлах и о токенах. А кроме того,
в этот же госип встроен механизм детекции избоев. Но мы сейчас перейдем к репликации, нам нужно
будет все-таки данные хранить в нескольких копиях. Но вот госип он обеспечивает, он реализует еще и
фейнер детектор. Чтобы к нему перейти, нужно поговорить, собственно, про репликации, про то,
как быть с отказавшими узлами. Мы сейчас научились что делать, мы научились распределять данные по
кластеру, и мы научились распространять эти данные, эту информацию, таблицу роутинга между узлами
с помощью госипа. То есть клиент может прийти и узнать про то, кто же ответит ему на запрос. Но вот
пока это конкретный узел, и он может отказать, поэтому, конечно же, мы такой конструкции в
чистом виде пользоваться не можем. Мы должны хранить копии данных на разных углах. Ну вот давай
встроим репликацию в эту схему. Как выбрать набор реплик для ключа? Такие узлы будут являться репликами.
Попользуемся тем, что у нас есть кольцо. Скажем, что вот у нас есть ключ, вот его фреш, он попадает
сюда точку кольца, и мы возьмём, и скажем, что вот эти три узла, следующие три узла по часовой
стрелке будут служить репликами. И, видимо, записать можно аккуратно. Будем считать,
что они все хранят копию этого самого ключа. Опять, нам нужно сделать некоторые поправки на
реальность. Вот прямо так буквально, наивно делать всё же не стоит. Нам нужно какие-то нюансы учесть.
Видно ли, что это за нюансы?
Вот на этой картинке... Может быть, это мог бы быть виртуальная узла одного этого?
На этой картинке нам повезло, и вот эти три виртуальных узла на кольце, это же всё-таки не машины,
но вот этим трём виртуальным узлам соответствуют разные физические узлы, и всё нормально. Но у
нас же в системе есть виртуальные узлы, и поэтому следующие три могут быть одними и теми же машинами,
поэтому у нас коэффициент репликации фактически снизится. Разумеется, мы должны учитывать
виртуальность узлов, и мы должны учитывать всё-таки некоторую топологию нашего кластера,
именно про ковернс. То есть мы, конечно же, не хотим, чтобы три наши реплики для ключа находились,
скажем, в одной стойке. Принципальная схема такая, но нужно её аккуратно потюдить, чтобы мы не
понизили себе случайный коэффициент репликации. Окей, ну вот у нас есть три реплики, и мы, в принципе,
готовы, как обычно будем, как мы раньше поступали в таких случаях. У нас есть вот эти три реплики,
у нас есть операция пользователя, у нас есть какой-то узел, который стал координатором этой
записи или учтения. Ну и как обычно, мы хотим иметь свою доступность, поэтому мы не требуем,
чтобы все реплики были доступны на запись, и не все были доступны на чтение. Мы используем обычно
для таких случаях клоуна. Если мы пишем, то мы пишем реплики. Мы вот знаем, что это за узлы,
мы нашли на нашем кольце решение, и мы, когда пишем, то пишем на какой-то клоун.
Когда мы читаем, мы читаем на какой-то клоун.
Ну и что нас в такой конструкции беспокоит?
Алло, кто-нибудь?
Нас беспокоит здесь, если мы хоть что-то знаем про репликацию, нас беспокоит то,
что эта конструкция, конечно же, не линейризуема. Ну да, она дает какие-то наивные гарантии. Если у
нас запись завершилась до чтения, то, разумеется, на следующее чтение, запись завершилась до начала
чтения, то последующее чтение, конечно же, запись увидит, потому что есть члене форму,
есть одна реплика, которая эту запись знала, которая входила в форму записи. Но если запись и
чтение во времени пересекаются, то, возможно, какие-то странные ситуации, когда мы увидим запись, то не
видим. Вот у нас есть три реплики, и запись успела написать что-то пока на одну, то есть запись
пока уже началась, но еще не завершилась, а потом мы сделали первое чтение вот с такого набора
реплик, второе чтение вот с такого набора. Мы получили такой мигающий ключ, в котором то видно значение, а то не видно.
Но мы же уже говорили, что хотели пожертвовать консистенцией. Это правда, я к тому говорю,
что Кассандра, если ты читаешь документацию по Кассандре, то там написано, что их подход называется
tunable consistency. И tunable тут в том, что ты можешь выбирать значение w и значение e. Ну и пока у тебя
их сумма больше, чем нодь не n, конечно, а repetition factor. Тут якобы согласованности меньше. Но тут
в документации нужно относиться к этому всему скептически. Вот слово consistency, а на самом
деле это не имеет какой-то фиксированной семантики. Это такой зонтик, с которым много конкретных моделей согласованности.
Так вот Кассандра, она здесь никакой разумной модели согласованности нам не обещает ни линеризуемости,
ни sequential consistency. Ничего, просто такой наивный подход с quorum. Если quorum пересеклись, если
операции были уже в порядочном времени, то чтение увидит запись. Если они были конкурентными, то никакой
разумной семантики Кассандра пользователю не дает. Что происходит в случае partition?
Вот допустим у нас partition вырезал какие-то реплики.
Какие-то физические узлы, а вместе с ними какие-то виртуальные, которые входили в наш quorum.
В этом случае Кассандра говорит, что она запишет ваши данные, динамо нужно аккуратнее сказать,
динамо запишет все-таки ваши данные на V узлов, потому что ваши данные должны быть
записаны надежно. Но при этом мы не можем записать их вот на нужный нам quorum,
здесь quorum не соберется из двух узлов. Поэтому динамо действует ну так, оптимистично очень.
Мы координатор операции пишем не только на узлы, которые должны входить в quorum, а еще и на какие-то
другие узлы, если здесь quorum собрать невозможно. Просто какие-то узлы сейчас недоступны по непонятным
причинам. В этом случае мы координатор пишем на какие-то дополнительные узлы вот сюда,
но говорим этим узлам, что запись, которую они получили, нужно eventually дописать вот сюда.
Вот такая незатейливая евристика, она называется hit and off.
То есть у нас quorums есть и мы пишем действительно на V узлов всегда,
но при этом нет гарантии на самом деле, что quorums у нас пересекутся.
Так что мы в одной части partition можно работать с одними узлами, в другой части partition
можно работать с другими узлами. Записи у нас всегда фиксируются в системе,
они фиксируются надежно на как минимум V узлах. Мы сами значение V выбираем и говорим,
что наша запись должна приземлиться на 2 узла в случае того, чтобы отказывать,
но при этом пересечение quorums нас уже не спасет, даже если мы сначала записали и после этого только
начинаем читать. Как вообще понимать, что какие-то узлы недоступны и нужно писать на другие?
Для этого нам нужен детектор сбоев. Детектор сбоев снова использует гостепротокол. Обычно
детекторы сбоев реализуются через hard-бита и все очень удобно, когда у нас есть некоторый
координатор, который эти hard-биты получает. Есть какой-то фиксированный узел, это может быть
узел-координатор, узел name-нода или это может быть зоокипер, которому мы делегировали эту задачу,
но пусть даже зоокипер, раз уж мы про него знаем. Так вот, каждый узел создается пефимерную ноду
в директории и пока у него открыта сессия, пока он жив, в рамках этой сессии клиент-зоокипер
отправляет hard-биты и этот ефемерный узел продолжает жить. Когда клиент умирает, сессия
протухает. Когда узел-система умирает, сессия протухает, в директоре исчезает
ефемерный z-note и другие узлы об этом узнают просто через подписку-зоокипер. Очень удобный
механизм. Вся коммуникация устроена по принципу звездочки, есть один центр и с ним все общаются.
Здесь все узлы должны знать про все, потому что координатором записи может быть пока что кто
угодно, это не совсем точно, потому что в касандре кто угодно может быть координатором операции,
в динаме все-таки нет. Пока можно считать, что кто угодно, поэтому кто угодно должен знать про всех
остальных, какие узлы сейчас живы или нет. Но как это сделать? Мы же не будем всем вслать hard-бит,
потому что всего у нас в сети десятки тысяч узлов. Вот квадратичную коммуникацию мы себе
позволить не можем, поэтому это довольно забавная история. В касандре детектор сбоев встроен в ГОСе
протокол и hard-бит мы можем получить напрямую иногда, а можем получить косвенно, когда нам про
hard-бит расскажет другой узел. Ну и каждый hard-бит за какое-то количество итераций
почти распространяется. Конечно же, по-другому построены тайм-ауты, потому что распределение
hard-битов по времени уже другое на существовании этого ГОСЕПа. Но принцип такой же. У нас
коммуникация не централизованная, коммуникация при этом не квадратичная, коммуникация промежуточная
через ГОСЕП. Ну это такой общий принцип для любой не централизованной системы.
Окей. С чем у нас еще здесь сложности есть? Ну, помимо вот этих кворумов и отсутствия согласованности
разумных. Ну, если мы работаем с кворумами, то есть пишем не на все реплики, то, видимо,
разные узлы могут хранить разные версии значений. И нам нужно каким-то образом эти версии, видимо,
упорядочивать. Узлы должны понимать, какие версии более старые, какие более новые. Вот как мы
собираемся эту задачу решать. Хранить версии неравное значение. Ну да, само-самое. Откуда мы версии возьмем?
Так, нам же клиент присылается в какой-то момент. Ну, клиент, пусть он сам. Ну, клиент же он не один.
Представь, ты делаешь покупки, и у тебя там есть какой-то аккаунт, и ты работаешь с ним, скажем,
с ноутбука, с телефона, и у тебя одна и та же корзина. И корзина одна, а физических клиентов
несколько на разных устройствах. Тут конкарнация, разумеется, может быть. Ну, в конце концов,
мы говорим не только про корзины, мы говорим про, вообще, про киевы и ухранилища сейчас. И, разумеется,
могут быть клиенты, которые конкурентно работают с одними и теми же ключами.
Нет у нас такого клиента, который бы все упорядочивал. У нас разные клиенты могут
порождать разные версии для одного и того же ключа. Ну, в общем, возникают задачи версионирования,
и вот система Cassandra и Dynamo отличаются тем, как они подходят к этому самому версионированию.
Они подходят совершенно по-разному. Cassandra поступает очень просто. Cassandra получает,
вот когда приходит клиент, приходится своя операция PUT. Эта операция PUT приземляется,
то hash занимает какое-то место на кольце, ну а сама операция PUT, она же попадает просто в
какой-то узел, чтобы аккуратно нарисовать, она попадает в какой-то физический узел.
Вот, этот узел является координатором, и просто этот координатор в качестве версии
записи выбирает свое локальное время. Вот, такая стратегия называется last right wins.
Это такой наивный способ, наивный тривиальный способ разрешения конфликтов. Вот, мы клиент отправили
операцию GET, какой-то узел ее получил, у него есть карта кластера, он знает, какие узлы отвечают
за хранение значения для ключа. Мы к ним идем, собираем какой-то квором на чтение, получаем
разные версии и выбираем просто версию, которая по часам старше. Нас не беспокоит то, что эти
версии могли быть назначены разными координаторами, у них могли быть не синхронизированные часы,
и две записи, которые были, скажем, упорядочены во времени и упорядочно причинностью, они получили
противоположные временные метки, и в итоге более ранняя запись перетерла более позднее.
У нас Кассандро это устраивает.
Некоторая техническая заметка, когда координатор чтения читает с кворома, он может видеть разные
версии, и он выбирает старшую версию. Но вот тут можно привести забавную параллель с протоколом
ABD, который у нас был на параллельном курсе. Мы тоже читали с кворома по началу и выбирали
старшую версию, но для того, чтобы гарантировать, что любое чтение, которое начнется после нашего,
гарантированно увидело версию не младше, не старше вернее, мы делали синхронную фазу записи.
Вот чтобы не было такого сценария. Чтение сначала увидело, а запись потом не увидело.
Вот мы использовали такую дополнительную фазу записи, такой хелпинг в терминах лог-фри,
когда мы прочли старшую версию и намазали на квором в случае, если почему-то запись не завершилась.
Кассандро тоже это делает, и это называется read-repair.
Но при этом делает это ассинхронно. То есть она вам возвращает прочитанные
значения со старшим временной метками и в фоне посылает на реплики, которые разошлись,
новое значение, чтобы эти реплики перезаписали свою уже устаревшую версию.
То есть для доступности у нас есть вот этот hint handoff, для конкуренции у нас есть временные метки
и конфликты мы разрешаем с помощью очень простой стратегии lost right weight.
Dynamo делала сложнее. В чем проблема такого подхода? В том, что мы на месте координатора просто
назначаем в качестве временной метки записи локальное время. В том, что координаторы могут
быть разными, часы могут быть не синхронизированы, ну и соответственно причинность, которая была на
уровне клиентов, которая реализовалась за пределами системы, она вот этими узлами
игнорируется, координаторами игнорируется. Было бы здорово все-таки причинность учитывать.
Но а для этого нужно каким-то более сложным образом представлять версии. Вот сейчас у нас версия
это просто физическое время, и это физическое время стирает информацию о причинности. Вместо
физического времени можно использовать логическое время, в котором эту причинность можно закодировать.
А именно мы знаем, что есть понятие happens before, это просто формализация причинности.
И есть способ happens before компактно описать в виде векторных временных мет, в виде векторных часов.
То есть если у нас есть в системе там n узлов, и на каждом происходит и событие, и в чтении записи,
то временная метка события это более-менее вектор, где итая компонента, если у нас есть
итый узел, и он обслуживает какой-то пункт, то итая компонента векторных часов – это
порядковый номер текущей записи на этом узле, а другие компоненты векторных часов хранят
количество событий, количество записей, про количество записей на других узлах,
о которых знает данный узел, выполняющий текущую запись. И мы бы могли хранить вместе с каждым
значением не скалярную временную метку, не физическое время и не какой-то логический
таймстэмп, а векторную временную метку, и тогда бы при чтении, получая с хворума разные версии,
мы могли бы их сравнить. В векторных часах есть такая естественная процедура сравнения,
они захватывают happens before, happens before – частичный порядок, поэтому мы можем сказать, что, скажем,
одно событие предшествовало другому. Если у нас есть один клиент, он сначала записал, получил
подтверждение от системы, потом сказал об этом другому клиенту, другой клиент сделал свою
запись. Но вот эти две записи могли бы быть упорядочены порядочной причиной, и система могла бы
эту причинность захватить, и тогда, в случае сравнения временных меток с разных леплик, могла
бы учесть, что новая версия, она просто наследует старую версию, поэтому можно старую версию
перезаписать. Или наоборот, если мы вдруг видим, что векторные часы конкурируют, то это означает,
что они несравнимы, то есть есть какая-то итая компонента, которая больше в одних часах и житая,
которая меньше, то это означает, что события несравнимы, это означает, что события со стороны
пользователей не были упорядочены, и никакого разумного способа разрешить конфликт здесь нет.
Для того, чтобы эту конструкцию интегрировать в Dynamo, нужна помощь пользователя.
Вот, давай посмотрим на статью, там есть пример.
Вот D1, D2, D3, D4, D5 – это какие-то версии данных, которые хранятся по ключу.
Вот сначала какой-то клиент пишет по ключу, и он порождает первую версию данных,
и векторные часы в прошлом году, в прошлом учебном году, весной, мы говорили про векторные часы в
контексте потоков, но вот здесь у нас в векторных часах компоненты – это некоторые акторы,
которые упорядочивают все свои записи, которые выполняют записи, и вот акторами могут служить
разные узлы. Можно считать, что акторы – это клиенты, а можно считать, что акторы – это узлы,
координаторы внутри системы. Но вот в этой картинке считается, что координатором является узел,
который… узел системы. В смысле, актором, который отвечает за версионирование, за упорядочивание,
является узел системы координатора записи. Вот когда какой-то узел Sx обслуживает запись по ключу,
то он, собственно, записывает ее на реплике и прикрепляет к этим записям временную метку такую
Sx1. То есть, это векторные часы или, как это называется в контексте подобных систем,
вектор версий, в котором есть одна компонента – Sx и версия 1. То есть, через Sx была выполнена
пока одна запись. Потом была выполнена вторая запись, она тоже прошла через Sx и получила
временную метку Sx2. То есть, был узел, который наблюдал порядок двух записей, и вот если вдруг
при чтении с квором мы получим вот такую временную метку и такую временную метку, то мы сможем их
сравнить. Вот это просто обжаривает это. Мы знаем, что вторая запись, она точно логически следует
за первой. Они могут быть логически упорядочно клиентом, могут быть неупорядочно, но на них
порядок зафиксировался. А дальше вдруг возникает partition. И в разных частях partition'a происходят
две конкурирующие записи. В одной части, эту запись обслуживает узел Sy, в другой части,
эту запись обслуживает Sx. И вот у нас появились две новые версии значения, и у них уже две разные,
две несравнимые временные метки, два несравнимых вектора версий. А дальше, скажем, partition лечится,
и какой-то клиент приходит и читает актуальное значение. И с квором он получает версию вот такие
данные, вот с такой версией, и такие данные вот с такой версией. Если бы мы были касандрой,
то мы бы в качестве версии получили две временные метки, просто сравнили бы какая из них старшая,
какая из них более свежая и выбрали соответствующее значение. Здесь мы наблюдаем, что у нас есть два
вектора версий, и они несравнимы между собой. Здесь разные акторы занимались обслуживанием
разных записей, и непонятно, как они могли упрядочить, как они упрядочены между собой две эти записи.
В этом случае Dynama действует следующим образом. Она отдаёт решение этой проблемы пользователю,
диригирует пользователю. То есть сама система говорит, что да, я была доступна, я обслуживала
все записи в обеих частях partition, но у меня получились несогласованные версии, и я не
понимаю, как их упорядочить. Если бы у нас был консенсус, у нас был бы сквозной порядок,
но он требовал доступности большинства. Здесь мы от этого отказались, работаем в меньшей части
partition, но при этом у нас получаются несравнимые версии, расходящиеся ветки истории, и пусть теперь
сам клиент занимается тем, что эти ветки как-то мерзят между собой. Он пишет какое-то произвольное
приложение, и вот пусть на уровне логики своего приложения он понимает как данные,
как согласовать две конфликтующие версии. Вопрос тут, правда, в том ещё есть, но помимо этой
проблемы, что само по себе неприятно, есть ещё такой вопрос, а именно кто именно является вот этим
Sx, Sy, Z? Ну то есть, кто увеличивает соответствующие компоненты? Тут есть два варианта, либо это делает
узел системы, либо это делает сам клиент. Но вот если это делает сам клиент, то, ну понятно, то есть
каждый клиент, мы будем ставить его однопоточным, последовательным, и он назначает монотонные метки.
Но скажем, если вы действуете неаккуратно и запускаете каждую запись, там не знаю,
в новом потоке, и в новом потоке создаете новый клиент системы, и получаете такой новый
идентификатор клиентов, фактически, то у вас эти векторы версии будут раздуваться, они будут
расти. Если же вы перенесете нумерацию версий на узлы системы, то тут можно поступить аккуратнее,
а именно сделать так, чтобы все записи одного ключа по возможности, но не то чтобы это можно
было гарантировать, но по крайней мере можно к этому стремиться, чтобы записи одного ключа
обслуживались одной машиной. Вот если мы посмотрим на доску, то вот есть у нас какой-то узел,
который обслуживает запись, и есть, этот узел знает про три реплики ключа, который клиент хочет
записать. Вот, этот узел перенаправляет запись на первую реплику из вот этой группы реплик,
вот первую в смысле расположения на кольцах решений, так чтобы все записи проходили через
одну реплику, и эта реплика их упорядочила. Если вдруг у нас случился partition, то упорядочить
будут разные реплики независимо, у нас появляются разные компоненты, тогда в этом векторе
версий, ну и тогда ветки расходятся. Идея понятна? Понятно. Тут, правда, есть еще один нюанс,
а именно вот такой дизайн, он влияет на описи системы. Вот раньше мы говорили, что у нас есть
просто операция put и операция get, но на самом деле чуть сложнее. Вот я сейчас найду здесь
пример. Когда мы делаем put и get, мы, когда мы делаем get по ключу, мы получаем не просто значение,
которое мы прочли, а мы получаем пару из значения, которое мы прочли, и контекста. Контекст
какой-то непрозрачный, мы его интерпретировать не умеем. И когда мы пишем, по ключу значения,
то мы в API put тоже добавляем этот контекст. То есть как выглядит наша операция обычно? Мы что-то
читаем, смотрим, апдейтим, пишем новую версию. И для того, чтобы между этими версиями сохранялась
преемственность, на смысл, чтобы система понимала, что мы как бы наследовались от какой-то
предшествующей версии, мы эту версию прокидываем клиенту через самый непрозрачный контекст.
Он клиентом не интерпретируется. Он интерпретируется системой, которая получает запись. Вот,
получив put от клиента с таким контекстом, система понимает, что этот контекст – это
вектор версий, и в нем нужно инкрементировать одну из компонентов. Вот ровно так мы передаем
векторные часы. Ну вот, если вспомнить алгоритм векторных часов, можно смотреть википедию, и там
будет картинка. Когда мы отправляем сообщение другому узлу, когда реализуется некоторая причинность,
вот тогда мы прикладываем сообщение в свои векторные часы, и при получении их мержим. Вот узел
системы, узлы системы и клиенты, они вот взаимодействуют через коммуникацию друг с другом,
и через эту коммуникацию они заодно пробрасывают и вот эти векторы версий. Таким образом,
мы пользователя заставляем вот явным образом аннотировать причинность, которая там за пределами
системы реализуется. Если бы система была линейризуема, если бы она использовала консенсус,
то это было бы не нужно, потому что наш консенсус бы давал линейризацию, то есть бы он учитывал
порядок в реальном времени, значит причинность. Вот Кассандра от этого отказывается, поэтому
заставляет пользователя прокидывать контекст своими собственными руками. Ну и конфликт разрешает,
и разрешает конфликты. То есть, с одной стороны, мы получаем высокую доступность, а с другой стороны,
мы получаем более слабую модель согласованности, но вообще непонятно какое просто пересечение
кворумов. А кроме того, у нас немного меняется API, и мы отребуем от пользователей, чтобы он
причинность явно маркировал. Но, кстати, не всё так плохо, точнее, не то чтобы это было плохо,
это скорее дизайн, это намеренно-сознательный выбор. Но в кавке у вас есть альтернативный способ,
если всё-таки вы хотите какой-то упорядоченности, если вы хотите какой-то согласованности, если вы
хотите понимать всё-таки, что происходит, то у вас есть инструмент, который называется Lightweight
Transactions. В динамо его не было, он появился в Кассандре, появился не так давно, и это такой
аналог операции Compare and Set и Compare and Swap. Вы можете сделать insert в таблицу,
если данного ключа ещё не было. Такой атомарный exchange. Или вы можете обновить по ключу какое-то
поле, если предшествующее значение равно чему-то. Это вот уже буквально касс. И здесь всё-таки вот
в этом месте локально для поддержки. Это называется транзакциями, но, конечно, это не в полном смысле
транзакция. Вот эта транзакция, она имеет такой фиксированный вид, и она умеет работать только
с отдельным ключом. То есть такая точечная операция, это операция CASS. И выполнять кросс-шардовые
транзакции, то есть работать с ключами, которые находятся на разных машинах, Кассандр не умеет.
Но всё-таки в пределах одного ключа мы можем достичь вот такой вот линеризуемости, используя
операцию CASS, которая под капотом реализована через некоторую вариацию протокола Paxos.
Вообще говоря, Paxos — это протокол, который реализует то, что называется Write-Once Register,
то есть ничейку памяти, в которой можно один раз записать надёжно. Ну, то есть выигрывать тот,
кто первым это сделал. Но я как-то вот в параллельном курсе на одном из теменах рассказывал,
что на самом деле, если посмотреть на историю пропозалов в Paxos, если представить их в виде
графа, где пропозал цепляется за другое, если он заадоптил значение после фазы prepare,
то там получается такая забавная конструкция, ориентированная на граф без циклов,
где все принятые пропозалы лежат на одной цепи. Ну, то есть, короче говоря, из single to creep
axis можно сделать некоторую историю апдейтов, то есть перезаписываемый регистр. Правда,
и нам важно, что у нас именно операция CAS, потому что, ну вот, когда мы делаем операцию,
мы по сути, ну, если там некоторые детали отбросить, мы по сути выполняем как бы одну
итерацию Paxos. Она может быть успешной, может быть неуспешной, но она может быть неуспешной,
потому что там, не знаю, какие-то тайм-ауты, а она может быть ещё неуспешной, потому что у нас
конкуренция, и наш пропоз прервали, потому что кто другой сделал prepare. Но такой сценарий сам по
себе не означает для этого протокола, что транзакция была неуспешной. Это лишь означает,
что она не понимает, чем она закончилась, её просто перехватили. Но новая транзакция может как бы
зафиксировать значение нашей. Поэтому вот после такой транзакции, вообще говоря, мы можем не знать,
чем дело закончилось, мы можем получить ошибку в духе, ну вот, непонятно. Но в любом случае это
всё безопасно по retry, потому что, ну вот, если как бы if… Понятно, потому что если наша транзакция
всё-таки закоммитировалась, зафиксировалась, то мы увидим, что… То мы, возможно, увидим эффект этого.
У нас операция провалилась, но другая транзакция, закоммитировавшись, закоммитила ещё и нас вместе
с собой. Поэтому мы можем retry нашу транзакцию и не беспокоиться, что она применяет второй раз,
ну, поскольку у нас операция CAS. Ну, конечно, это не совсем общий случай, но всё же, вот там, скажем,
для таких сценариев, где у нас не бывает откатов в сторону NoteXist, это всё будет работать.
В общем, удивительным образом всё-таки Paxos туда встроили, так встроили сбоку немного,
но он там всё же есть. Так что, если нас не беспокоит согласованность, нас беспокоит доступность,
то нас устраивает, то мы используем базовый протокол репликации с кворумами, с однофазными
записями, с асинхронным read-repair'ом, и не имеем разумной гарантии. Но если нам всё-таки где-то
нужно что-то упорядочить, то мы используем всё-таки такой вот локальный Paxos в виде
Ну и последние какие-то замечания общие.
Cassandra из отдельных узлов, вот как эти отдельные узлы хранят данные сами по себе. Ну, это киволе
хранилища, поэтому Cassandra использует снова LSM, то есть ту технику, которую мы уже много раз
наблюдали. Ну и в Cassandra есть ещё одна забавная техника, которую мы видели в контексте блокчейнов,
но вот Cassandra, эта техника полезна, это деревья меркла. Деревья меркла нужны Cassandra на уровне
хранения, когда узел восстанавливается после сбоев. Вот, допустим, его долго не было, и он должен
понять, какие данные ему нужно забрать в других узлов. Просто реплика была выключена некоторое
время, потом её включили, и она должна нагнать состояние. Вот, чтобы не копировать весь жесткий
диск реплики на реплику после её восстановления, реплика, новая реплика может обменяться с другой,
своим представлением о ключах, которые эта реплика хранит в виде дерева меркла,
точнее, в виде верхушки дерева меркла. Реплика всё-таки отвечает за... реплика отвечает за какой-то
набор ключей, мы делим набор ключей на какие-то поддиапазоны, дотриминированным образом, считаем
хэши, потом считаем, потом комбинируем хэши лево-право под дерево, ну и на другую реплику
отправляем верхушку этого дерева. И реплика, получив эту верхушку, сравнивает эту верхушку
своей верхушкой и может локализовать, в каком именно под дереве, в каком поддиапазоне ключей есть
потенциально див и его переотправить на другую машину. То есть тут в биткоине Merkel 3 использовался
для того, чтобы построить такой категорфический сертификат, здесь это всего лишь способ передать
поменьше информации и локализовать разницу, которая накопилась на узлах за время простой одного из них.
Ну что, примерно такой дизайн Cassandra, он не слишком современен, кажется, люди научились делать
масштабируемые системы, надёжные системы, которые ещё и будут согласованы, но тем не менее вот такой
подход существует и его скорее интереснее противопоставить дизайну Bigtable и HBase, потому
что это буквально решение одной и той же задачи, но которые выбрали очень разные противоположные
пути. В одном случае централизованный дизайн и консенсус, в другом случае децентрализованный
дизайн и вот все эти векторы версий, расходящиеся истории, конфликты, разрешение их на стороне
пользователя. Ну, кстати, конфликт, это само по себе не страшно, потому что вполне себе можно
представить структуру данных, которые могут конфликт разрешать. Если мы строим буквально корзину,
то это множество, ну а скажем, разные вариации множеств можно строить так, чтобы конфликт
разрешались автоматически. Ну скажем, если мы во множество только добавляем элемент, то понятно,
что даже если у нас есть конкурирующие апдейты, конфликтующие апдейты, то, точнее, конфликтующие
апдейты не может быть. У нас есть конкурирующие апдейты, то всегда можно смёрзать общие значения,
общий результат, просто взяв победение двух множеств. Если у нас есть множество,
в которое мы можем добавлять и, скажем, один раз удалять, то у нас тоже есть, ну это называется
CRDT, вот способ построить такое множество и сливать конфликты автоматически.
Если же мы можем и добавлять, и удалять произвольное количество раз, ну короче,
тут есть разные вариации, как это можно делать. Какого-то, понятно, общего, максимально
универсального рецепта не существует, но для некоторых под задач, для каких-то отдельных
вариаций такие решения есть. В конце концов, если эту идею развивать, то можно прийти к какому-то
коллаборативному редактированию текста, Google Doc, это вот как раз воплощение потомной структуры
данных в максимально общем виде, там, где у нас уже не множество, не регистры, а целые деревья или
более сложные структуры. Ну в общем, вот это ограничение, эти проблемы со слиянием версий,
по определённой степени разрешаются просто выбором правильной структуры данных,
которые мы реплицируем. Ну или мы доверяемся просто Кассандре и её наивному подходу
сравнения временных меток, полученных из локальных часов. Ну что, есть какие-то вопросы?
Вот кажется, всё довольно просто, довольно наивно, но это всё-таки было придумано уже сколько,
15 лет назад. Человечество с тех пор ушло далеко вперёд.
Ну ладно, тогда спасибо, что заглянул. Счастливо.

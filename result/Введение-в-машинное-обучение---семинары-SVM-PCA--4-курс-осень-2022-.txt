Итак, внезапно начинает прошлого занятия, мы с вами уже знакомы чуть-чуть совсем капельку с
PyTorch, поэтому на PyTorch мы с вами будем обращать внимание регулярно, и, собственно, ваша третья
домашка, которая вам сегодня будет доступна, будет также зависеть от PyTorch. По факту мы сейчас
начнем делать вашу третью домашку. В каком смысле? Сейчас мы с вами SVM потыкаем палочкой просто в коде
и реализуем его. Он за вас реализован, разберем на PyTorch, а затем дома вам нужно будет
краткий ядро реализовать, чтобы с ним было понятие. И, на всякий случай, я бы еще хотел обратить ваше внимание
какое-то конкретное ядро. По-моему, там вам RBF-ядро радиальных байдсовых функций надо будет
сделать. Что я здесь хотел бы сказать дополнительно. Во время перерыва, как раз-таки, ко мне тут
коллеги ваши подходили с вопросом на всякий случай. То есть, еще раз, собственно, функция называется
ядром, возвращаемся к SVM. Если она представима вот в таком виде, при некотором отображении из
исходного пространства в какое-то целевое гильбертовое пространство, то есть пространство
со скалярным произведением. При этом вот этот самый переход нам зачастую-то не очень важен. Почему?
Потому что нам не нужен сам переход, нам нужно подходящее ядро. И про это еще в 909 году была
доказана теорияма Мерсера, которая, кратцкий говорит, что функция является ядром тогда и только
тогда, когда она симметрична и не отрицательно определенна. То есть любые симметричные не
отрицательно определенные функции, которые могут быть представлены, то есть функция двух векторов,
могут являться ядром. Поэтому нам не обязательно представлять какое именно отображение в целевое
пространство есть, нам это не столь важно. Нам важно, чтоб мы знаем какое ядро. Ядро ввести
проще, чем перейти в кутеновый пространство. Понятно?
Что?
Да, по-моему, все-таки оно должно быть гильбертовым. Давайте, там-там-там.
Слушайте, по-моему, все-таки полнота там тоже должна быть. Я, скажу так, с Вэмом я
пользовался последний раз в году, в 2018, теорему эту читал вот сейчас, так что я лучше это потом
почитаю, видимо, дома, и вам, на всякий случай, потом скажу. Как видите, как бы, я сам не все наизусть
помню, это нормально. Многие вещи, они просто-напросто за неиспользованием начинают чуть-чуть отваливаться.
Но, на всякий случай, вот пример ядер, которые работают. Константа – это ядро. Бесполезное,
но ядро. Произведение любых двух ядер тоже является ядром, потому что это две
неотрицательные функции. Для любой функции, собственно, вот такой, это тоже ядро. Ну и так
далее. Любая линейная комбинация работает. Фурье образ можно также взять. Почему нет преобразования
фурье? Привет! Но по факту, вот этим всем занимаются в крайне редких случаях. На практике,
я лично, вот именно, какую-то подбору ядра через фурье образы за всю жизнь не видел ни разу вообще.
Так что думаю, это не совсем то, что на практике применяется. Ладно, давайте тогда сюда вернемся.
Вам нормально видно, или побольше сделать? Нормально? Хорошо. Ну, давайте запилим маленький
SVM непосредственно на Pytorch. На всякий случай звук, видео, все идет. Правильно, товарищ оператор?
Все, спасибо. Внизу там зеленая штука прыгает, что я говорю. Хорошо. Давайте построим какую-нибудь
выборочку, которая линейно-разделимая. Проиллюстрируем все то, что мы с вами разбирали.
Сейчас немножко проснется. О, выборочка появилась. Мы с вами пользуемся скалерным,
как мы и заявляли, в основном, чтобы картинку рисовать. Вот у нас выборка из двух кучек. Кучка
желтых и кучка... В нынешних реалиях, я предпочту изменить. Симап? Не, слушайте, никаких этих, но...
Образование вне политики, я так скажу. По крайней мере постараемся. Вот. Да и любой, на самом деле,
выберите. Вы можете ввести любой, он у вас выпадет с ошибкой со словами. Вы не правы,
есть вот такие. Очень удобно. О, кстати, есть очень классный этот. Отум, кажется, он называется.
Красный на улице осень.
Вот. Я думаю, сейчас проблем у нас никаких нет, я надеюсь. Короче, вот у нас две кучки. Давайте
реализуем ровно вот эту самую функцию и будем краски ее минимизировать. Хорошо? Ее можно решать
с помощью либо субгренитов методов, либо с помощью решения двойственной задачи оптимизации,
привет, собственно, решение задач с ограничениями. И здесь мы краски вместо подсчета вот этого ядра,
который по умолчанию просто-напросто скалярное произведение, мы с вами можем подобрать любое
другое ядро, которое мы с вами только что обсуждали. Согласны? Хорошо. Как оттуда сюда перейти,
смотрите, на всякий случай, даже с использованием ядра вы можете перейти точно также к двойственной
задаче, просто-напросто в оригинальной форме, и решать ее градиентными методами. Это не совсем
тривиально сделать. В общем случае, об этом целая статья написана, на нее можно сослаться,
ее можно почитать. PageNotFound замечательно. Нельзя почитать, найдем новую ссылку, раньше работала.
Возможно, она не работает с наших айпишников по, опять же, всем понятным причинам. Но ничего.
Давайте реализуем СВМ, но используя СГД, то есть без решения двойственных задач.
Во-первых, тут у нас сидит hingeLoss. Давайте реализуем hingeLoss, нам нужно посчитать hingeLoss.
Как мы можем это сделать? На вход нам пришли лейблы. Лейблы — это истинные значения. И
скоры. Что такое скор? Я думаю, все помнят. Я не помню. А чего?
Простите, говорите, пожалуйста, погромче, вас не слышно.
Все, тишина. Ну ладно.
Пришел у нас батч объектов. Скоро, видимо, это то, что наша моделька предсказала.
Соответственно, нам нужно с вами посчитать, во-первых, кого? Scores на labels и, соответственно,
1 минус вот эта штука. И здесь у нас, я не помню, какого они типа, но пусть будет есть такая функция,
например, torch clamp, кажется, называется. Да, clamp в нужный нам range. Ну вот, давайте проверим,
соответственно, 1, 2, 3, 4, 5. Соответственно, будем его отображать в 2, 3.
Ну хорошо. Вот как работает clamp, то есть он все что нужно нам обрезает. Мы совершенно спокойно
можем сделать так, минус 1, минус 2, 3, 4, 5 и обрезать все то, что у нас краски меньше нуля.
Видите, все отрицательные числа заменены на 0, все положительные остаются. Так что
можем сделать вот так. Torch clamp, 1 минус Scores на labels, обрезаем нулю. Ну как-то так. В принципе, работает.
Вот. Ну и разве что в данном случае, раз у нас кинжалоз, давайте еще это будем тогда усреднять.
Вот пожалуйста, вот наш с вами. Что? А где норма? Так да, смотрите, первая часть это кинжалоз,
вторая это норма. Вот нам пока только кинжалоз нужен. Все, вот мы его по идее посчитали. Ну
собственно, вот у нас внутри сидит ядро. Ядро в простейшем случае линейное. У вас 2 массивчика,
собственно, вам нужно перемножить их друг на друга. Скалярное произведение по парной посчитать.
Хорошо? Как нам это сделать?
Так, и чего? Кого умножаем? Получается Torch matmul. Наверное, x1 на кого? На x2.
Ну у вас же матрица по идее, а? Нет, matmul это как раз таки скалярное произведение. Смотрите,
у вас выборка подна две. В общем случае, вы можете ядро считать не на паре объектов, а сразу на
двух подвыборках. Все по парной посчитать. Вот у вас x1, собственно, это что? Это array,
короче, первая подвыборка, это вторая. По идее, как-то вот так должно работать.
Так, бла-бла-бла, бла-бла-бла.
Редикц корс, короче, один момент.
Я с вашего позволения сейчас не буду изобретать велосипед, потому что сейчас кажется, что я
что-нибудь напутал. Ну тут у нас все абсолютно правильно. x1 на x2t тоже правильно. Дальше я все,
пожалуй, и копирую. От греха подальше. Я просто не тот ноутбук открыл, поэтому в нем пришлось
что-то кудить. Вот смотрите, собственно, вот наше ядро. Пока что просто скалярное. Мы и пишем
скалярное произведение. Один вектор на второй. И поехало теперь тело нашего свм. Во-первых,
в конструкторе нам ничего не надо. У нас есть learning rate для нашего градиентного спуска.
Количество эпох опять же для него. Размер батча, на котором будем это считать. Лямбда. Лямбда
это краска. Та величина, которая у нас отвечает за коэффициент регуляризации. Кто у нас там дальше?
Ядро. Если у нас никакого ядра не задано, то мы используем линейное. Или то ядро,
которое вы сдадите в домашней краске, можете сдать другое ядро. Verbal просто,
чтобы он нам что-то отдавал. И fit от flag, что мы его уже обучали или нет.
И метод fit, соответственно. Вот у нас опять x, y. Привет, sklearn, лайк. У нас с вами есть беты.
Это кто? Это у нас краски виса. И свободный член. Оптимизатор у нас будет классический sgd. И в принципе
здесь обысказать нечего. Что тут хочется сказать? Во-первых, несмотря на то, что свм мы с вами решаем
градиентным образом, мы с вами не будем считать градиент по всей выборке. Это логично. У нас
выборка может быть размером миллион. Мы не хотим по всему миллиону объектов это дело считать.
Поэтому что мы с вами делаем? Мы берем какую-то случайную под выборку. Вот она у нас. И
соответственно теперь мы с вами считаем ядро между ними. И вот у нас краски kbatch на вектор весов,
плюс свободный член. Вот наше с вами линейное отображение. У нас с вами линейный классикатор,
как вы помните. И теперь соответственно наш лос это что? Это наше предсказание. Это наш хинж-лос.
Вот он есть. Плюс как раз таки норма нашего вектора весов.
В одну строчку, чтобы он вам делал случайно под выборку. Да, но обычно это пишут прямо в камере.
Ну вот так. Обычно это если вам это нужно где-то в вашем коде, то вы пишете эту функцию в три
строчки сами и потом ее используете. Баскалерни по-моему ее нет. Если она есть, я ее не пользуюсь,
поэтому я ее не знаю. Ну короче мы с вами сделали ровно то, что у нас здесь написано. Вот мы с
вами по сути переформулировали, точнее воспользовались наработками вот этой статьи. Я найду где новая
ссылка на нее. Вот собственно наш новый f от x, краски beta на kx. Вот мы с вами исключительно
вот это переписали в виде нашего только что кода. Вот наша соответственно хинж-лос, а вот наша
вторая норма вектора весов. Тут только ядро у нас еще появляется, потому что мы в другое пространство
перешли. Ну и соответственно предсказываем мы его абсолютно в лоб. Вот наша по сути линейная
моделька. Если у нас соответственно что-то положительное, то это один класс, что-то
отрицательное, это другой класс. Вот можем его построить. Пожалуйста, лос у нас какой-то есть,
не знаю, что он значит, он с попугаек, но при этом accuracy у нас 100%. Но логично, что у нас все просто,
у нас линейно-разделимые выборки, svm гарантированно должен находить решение для линейно-разделимой
выборки, то все понятно. Но при этом вы совершенно спокойно можете попытаться воспользоваться и
линейно-неразделимой выборкой. Ну, например, давайте здесь. Что сделаем? Число эпох. Смотрите,
что такое эпоха на всякий случай. Когда говорят про обучение медным градиентам спуска наших
моделей, непонятно, сколько раз проходить градиентам. То, что вы можете менять размер бача,
вы можете менять количество шагов, непонятно, что это такое. Велиткое понятие эпоха. Что такое эпоха?
Одна эпоха — это количество парагонов, за которые мы покрываем столько объектов, сколько было во
всей обучающей выборке. Так как у нас бачи, как правило, выделяются случайным образом, вы некоторые
объекты возьмете несколько раз, некоторые вообще не возьмете, но в среднем вы покрыли всю выборку.
То есть, если у вас бач размером 100, а выборка из тысячи элементов, за 10 проходов у вас пройдет
одна эпоха. Понятно? То есть, просто это гораздо проще, чем каждый раз специфицировать и размер
бача и количеству шагов. Так вы говорите, одна эпоха — значит, вы прошлись по условным, сколько объектов у вас было.
Как правило, при обучении сложных моделей число эпоха считается десятками сотен. Это нормально.
Давайте попробуем теперь сделать ситуацию чуть похуже. Сделаем пересекающимся наше множество.
Вот, видите, теперь они у нас накладываются друг на друга. При этом у нас выборка перестала быть
линией наразделимой, но нам абсолютно ничего не мешает точно также обращиться к своему. Как видите,
0.94 он нас показал результат, и мы с ним совершенно спокойно можем с вами смотреть. Более того,
мы с вами можем посмотреть на нашу модулу Betas. Вот те самые веса, которые наша модель выучила.
Ну а теперь, собственно, это было зачем. Вот, можно на самом деле реализовать. Я понимаю, что мы с вами
вроде как тут за 20 минут посмотрели на какой-то огромный кусок кода. Почему я его не разбираю построчно?
Во-первых, разбирать код на семинаре скучно. Во-вторых, вам в любом случае придется разобраться в его
интерфейсах в домашке, потому что вам придется для него ядро написать. Тут, в принципе, все абсолютно
примененно. Мы взяли формулы, которые написаны выше из статьи, которые являются, собственно,
градиентным видом для задачи своема, и их реализовали в коде. Все. Давайте теперь возьмем из
escalern готовый svm и посмотрим, как он себя ведет. Давай. Что значит большая константа? Помните,
у нас в формуле там 1 делить на 2c, правильно? Так что, если константа 1 на 10 десятой, то,
просто-напросто, мы говорим, что 1 делить на c равен нулю. Круг говоря, на регулизацию мы забили.
Ну или у нас ядро зависло, что тоже может быть.
Уважаемый, проснись. Ну, ему плохо.
Эскалерн, эскалерн. Мы же в тебя верили.
У меня она влияет на то, что у меня все зависло.
О, пожалуйста, все, построилось. Смотрите, тут еще у нас удобная функция есть,
которая просто-напросто рисует поверхность, возразляющая для вашего классикатора. Логично,
что она работает только для двумерного случая. Вот, как оно у нас выглядит. Смотрите,
вот у вас константа какая-то одна, вы говорите, она не влияет. Ну, давайте попробуем поменять.
Вот я взял 10 во второй, давайте я возьму c равно просто единиц.
Чет визуально ничего не поменялся, правда. Много опорных векторов.
Ну, давайте поглядим.
Смотрите, вот если у нас будет линейно-разделимая выборка, вот у нас получилось все, что мы хотели
увидеть. Вот наша полоса. Причем, заметьте, сколько у нас будет опорных векторов, мы на них
можем явно посмотреть. Так как у нас выборка линейно-разделимая, у нас всего два барахных
вектора. Крайний объект из одной точки и крайний объект из другой точки и с другой кучки. Согласны?
Если, соответственно, мы с вами выборку сделаем более шумной, они как-то будут пересекаться,
пусть меня там, не знаю, два, то, соответственно, они все еще не пересекаются, все еще должно быть два.
Три опорных вектора, они, видимо, на одинаковом расстоянии оказались. Окей, случилось. Кластер std 4.
Теперь у нас опорных векторов стало больше. Если c сделать,
гораздо меньше.
Вот, смотрите, я константу уменьшил, то есть сила регуляризации стала больше, полоса стала шире.
Так что нет, она влияет. Если вы c просто увеличиваете от ста до миллиона, у вас она в знаменателе,
еще раз напоминаю, если у вас знаменатель был 1 сотый или 1 миллион, это в принципе там уже
умалая относительно функции потерь хинжелоса. Богично, что у вас гиберплоскость особо не меняется.
Чем ближе вы будете c приближать к нулю, тем больше у вас будет сила регуляризации.
Вот, есть, соответственно, опорных векторов, у нас теперь будет сильно больше.
Ну и давайте теперь посмотрим на разные ядра. Вот простейший случай, линия неразделимой выборки.
У вас концентрические окружности. Вы можете как угодно пытаться провести вашу прямую,
ничего у вас не получится, потому что выборка линии неразделимая. Но при этом вот вам какая-нибудь
функция, например, вот у нас ядро радиальное, можем к нему обратиться. Вот что у вас получается в итоге.
Смотрите, радиальные базисные функции, вы применили краски преобразования к вашим данным,
вот по сути ваше ядро, и вы можете увидеть, что теперь у вас все гораздо проще. При этом заметите,
опять же, если вы сделаете константу поменьше, например, там просто один,
ну ладно, это, видимо, надо рядом просто нарисовать. Вот один случай. Вот, у вас видите,
гиперплоскость, у меня она вообще всех в себя включила, вы границы даже не видите на экране. То есть,
у вас одна граница, на самом деле, где-то там в точке, другая по контуру. Или, что то же самое,
вы возьмете константу большую, 1g3, например, и вот для него построите график. Видите,
если соответственно константа большая, то вы видите, что у вас лишь часть объектов попала
в разделяющую полушь, потому что она достаточно узкая. Понятно? Вопросы, пожелания, комментарии
здесь есть? Это радиальные базисные функции, это ядро вот с такого вида.
Но мы с вами только что его в слайдах разглядывали.
Где-то там, sklearn, rbf, rnl.
Нет.
Хоть одну формулу он нам покажет? Нет, не покажет. Ладно, пойдемте вот сюда.
Вот rbf, как выглядит. Вот оно. Так, окей, хорошо.
А, вот здесь вы имеете в виду? Проще всего, наверное, поглядеть. Давайте попробуем. Вот rbf,
вот наша x, y. Она здесь. У нас, наверное, константа какая-то очень маленькая. Пусть
будет константа вообще единицы. Ну вот она вам, по сути, выделила. Вот у вас, опять же,
разделяющая поверхность. Она, на самом деле, я подозреваю, что это замкнутая все равно штука,
она должна быть замкнутая по определению. Но при этом вот у вас краски, множество точек,
которые попали в полосу. Опять же, видите, полоса у вас здесь уже не совсем понятна.
Рbf в среднем лучше, чем… Ну, во-первых, ее считать все-таки дороже, у вас экспоненты
каждый раз считать. Во-вторых, верно ли, что она в среднем лучше? Если у вас выборка
линейно-разделимая, наверное, хуже она не сделает. Но при этом линейно-разделимая выборка
это детский сад. Если выборка какая-нибудь сложная, то она может вам что-нибудь очень странного
сделать. Ну, например, сейчас, давайте проверим. Воскалерни все еще есть? To moons.
Так вот, make moons. Давайте попробуем эти нарисовать две луны. Скажем так. Make moons.
У меня там параметры. Samples, shuffle, noise, random state. Фактор нам нужен.
Вот у нас, например, классический датсет, который показывает, как линейно-разделимый выборок из
которой все модельки страдают. Давайте попробуем запихать сюда тот же самый rbf и посмотреть,
что с ним произойдет. Слушайте, а неплохо. Да, rbf вообще хорошее ядром. Я не смог его сходу сломать.
Ну, слушайте, неспроста, но является, наверное, одним из наиболее таких широко применимых на
практике. У него, собственно, единственная проблема в том, что его правда дорого считать. Если вы
посмотрите на имплементацию, вам придется, по сути, под капотом посчитать по парамескалерной
произведении всех объектов между собой. Ну, точнее, ядро. А если у вас много объектов,
вам еще и экспонента, то этого считать будет дороговато. Ну, вообще, оно меня прям... А?
А, кстати, да. Давайте-ка еще возьмем, собственно, что? Давайте-ка сюда шума добавим,
потому что здесь у нас, по сути, в спрямляющем пространстве вборка линейноразделима. А это,
на самом деле, больно хорошо. Давайте-ка его пробуем. Что-то у нас шума маловато.
Что-то у нас шума многовато.
Я все еще не вижу этих лун вообще. Я пытаюсь подобрать, сколько нам его надо.
Ну вот, что-то у нас там это уже... Давайте, 0,2, 0,25 будет.
Вот какие-то взаимопроникающие кривульны у нас остались. Ну, давайте попробуем опять РБФ
на него натравить. Неплохо. Честно признаюсь, неплохо. А если регуляризацию сделать побольше?
Шикарное ядро. А? Нет, ну тогда у вас просто будет случайная выборка. Ну ладно,
месиво так месиво. Обращайтесь, вот вам месиво. Хотя все еще видно, что справа,
словно больше желтых, а слева больше красных. Не, ну а что? Он пытался.
Ну и, собственно, на самом деле все то, что дальше, мы с вами уже руками чуть-чуть поковыряли.
А именно, вот наша линия разделимая выборка, можно менять блобы. Вот С большой, С маленький.
Соответственно, чем больше С, тем уже полоса. Опять же, это сделано исключительно в качестве
реверанса оригинальной статье, где С стоял из знаменателя. Всем сейчас привычнее, конечно,
что константа регуляризационная линейно влияет на степень регуляризации, а не наоборот, обратным
образом. Поэтому в комментарии, на самом деле, рекомендация будет классическая. Читайте доки,
потому что в доках написано, за что отвечает константа. А иногда бывает, вроде как, очевидно,
что чем больше С, тем больше регуляризация. На деле нет, потому что так исторически сложилось,
например. Будьте осторожны. Вот. Тут еще какие-то вопросы, комментарии, пожелания есть? Нет. Хорошо,
я тогда предлагаю сходить во второй ноутбук. Только я пометую сразу о том, что у нас с вами всего
25 минут. Предлагаю открыть решенную версию, разве что мне придется чуть-чуть поменять пути.
СВД, practice, MLMID становится MLCourse, соответственно, 22FB, а, нет, 21FB.
О, оно даже работает. Смотрите, что я тогда могу с этим сделать. Давайте я его тогда
сейчас ad-hoc добавлю, а потом уже починю нормальной репозиторией.
Так, смотрите, можете тогда сразу зайти сюда, вот у вас появится Solve, в нем можете жанкнуть,
попадете вот сюда. Здесь разве что тоже нам придется ad-hoc поменять MLMID на MLCourse.
А? Почему? Вам не помогло? Может, мне тоже не поможет.
Так, у меня вроде все загрузилось. Смотрите, опять же, придется это дело переименовать.
Так, на всякий случай, у кого не получилось эту штуку загрузить, в смысле, к палике?
Магия. Чего-чего? Не, я в 22-м вроде поменял. Все работает, слава богу.
Короче, смотрите, вот у нас картинка есть. Помним, что картинка это матрица, причем у нас картинка
цветная, поэтому у нас с вами три, на самом деле, канала. У нас три матрицы картинка. У нас канал,
отвечающий за красный цвет, синий цвет и зеленый цвет. РГБ. Мы с вами можем избавиться от среднего,
как вам угодно. Давайте попробуем опять нарисовать.
ПЛТ, Ким Шоу, Пэйс.
Что-то, мне кажется, здесь опечатка.
Что-то, мне кажется, здесь опечатка.
Потому что это ось 2, это кто? Это ось каналов, то есть у нас с вами для каждой матрицы подельность
вычисляется средняя. Что с тобой происходит?
А, что? У вас нормально получается. Вот нормальная картинка.
По идее, сейчас она должна точно так же. А, сейчас, погодите-ка,
минус это поделить на STD. А, понятно, нам надо не на STD меня делить, простите, а на 255.
А, сейчас, 255. Сейчас все скажу. Я сам немножко запутался.
Это нам нафиг не надо.
Ладно, хорошо убедил, я ни фига ничего не слышу.
Согласен.
Я пытался понять, что происходит, я наконец-то понял. Короче, вернусь назад, я зачем-то начал
нормировать изображение, в данном случае нам напрочь не надо. Вот наша с вами изначальная
картинка, вот она. Мы ее разбиваем на три канала с вами. У нас три канала, соответственно,
нулевой, первый, второй, RGB, соответственно. И после этого мы с вами можем посчитать их,
построить в качестве, как сказать, для каждой матрички по отдельности. Куда она все пропала?
Давай. Делали, делали, нарисовали, убрали напрочь. Вот наши с вами, соответственно, три канала RGB.
Можем короткие их попытаться нарисовать теперь, как это у нас выглядит для сингулярных значений,
то есть для каждого канала, каждой матрички абсолютно по отдельности, как оно себя ведет.
Вот, смотрите, у нас сингулярные значения, они отсортированы в порядке убывания, причем здесь
у вас шкала логарифмическая. Как видите, всего у нас значений порядка, сколько там, чуть меньше,
400. На самом деле, откуда взялась размерность 400? Я думаю, примерно понятно откуда. Вот она у нас,
385. Это у нас меньшее размерство, что у нас матрица, ранга 385. Больше, чем ранга 385,
мы с вами по логичным причинам получить не можем, согласны? Троковый и столцовый ранг матрицы
совпадают. Соответственно, мы с вами раскладываем матрицу по K компонентам от 0 до 350. И видим,
что у нас в среднем в районе 350 как раз такие сингулярные значения падают от порядка 1 до
порядка 10-10, то есть там ничего больше важного не остается. Соответственно, по идее мы для того,
чтобы хранить наше изображение о матричке, можем использовать не 385 строк, а 350 и уже почти
ничего не потерять. Или использовать на самом деле еще сильно меньше и терять какую-то часть
информации. Собственно, давайте к это и сделаем. У нас для этого как раз есть функция compress. Что
такое compress? Мы просто-напросто берем и выкидываем часть информации из вот этого нашего латентного
представления. То есть 1 и K элементов мы берем. Как это происходит? Как вы видите, чистой воды вот
здесь мы с вами берем 1 и K элементов, все остальное мы с вами выкидываем. Вот у нас 1 и K векторов для
наших матриц поворота артагональных, 1 и K компонент, 1 и K векторов для артагональной матрицы,
так для всех трех каналов. Ну и давайте посмотрим, что у нас там происходит.
Вот, смотрите, вот 350 компонент, все работает как надо, мы ничего не потеряли. Логично,
мы по графику выше видели, что 350 компонент нам примерно хватает для каждого из цветов.
Вот на 300 компонент начинают появляться какие-то шумы. Вот 250, еще хуже, 200, 150,
150, уже совсем какое-то, 20. Вот у нас одна, одна главный компонент. На всякий случай,
почему у нас с одной главной компонентой все равно какие-то вертикальные горизонтальные линии
появились? Ваши доводы. Чего? Три даже матрицы у нас есть, раз. И два, у нас с вами эта одна
компонента в себя включает все равно линейную комбинацию какой-то всех предыдущих компонентов.
Поэтому, когда мы обратно разворачиваемся, мы для каждой компонент на самом деле что-то находим.
Вот. И заметьте, коллеги, здесь можно обнаружить достаточно любопытную вещь. У нас, в принципе,
по каждому из трех каналов деградация, скажем так, была практически одинаковая. Как вы думаете,
почему? Но у нас же с вами три матрицы вообще независимо полностью разбиваются. Для красного,
зеленого и синего канала. Как вы думаете, почему у нас качество почти везде падает равномерно по
всем трем каналам? Мы везде используем к одинаковым и есть, на самом деле, вторая причина,
которая достаточно простая. У нас картинка-то практически черно-белая. У нас либо все три
компонента одновременно, все три канала одновременно 255 максимальное значение,
либо одновременно 0 черное. У нас почти везде черно и белое. У нас вот не белое,
только кусок галстука и membase.com. Древние времена. Да.
Ну, я так скажу, что сжатием без потери-то назвать можно, только если вы откидываете.
Вам нужно тогда промежуточное представление сохранять. То есть, смотрите, вот у вас при
построении, вот вы построили свд. Видите? И здесь мы с вами полные матрицы. Здесь мы с
вами отбрасываем все до катего минора. По сути, если вот здесь мы с вами сохраним только первые
к-векторов отсюда, первые к-векторов отсюда и первые к-векторов отсюда, вот тогда у вас объем
занимаемый будет меньше. Картинка хранит вам, по сути, маску. Это у вас какое-то будет внутреннее
представление, чтобы получить картинку, вам потом их придется между собой перемножить.
По сути, когда вы открываете картинку на компе, у вас также прогоняется рандомная
программа, которая умеет, например, в кодеке JPEG. Существующие программы, скорее всего, в это не умеют,
потому что так картинки обычно не сжимают, это сильно менее эффективно, чем, опять же, тот же самый JPEG
или lossless JPEG и так далее. То есть это, скажем так, картинки так сжимать плохая идея, на самом деле,
по достаточно простой причине, потому что здесь мы с вами работаем именно что с матрицей. Матрица у нас,
в общем случае, поддается такому разложению. Картинка она, на самом деле, обладает достаточно
большим количеством свойств специфичным именно картинки. Например, у вас есть свойства локальности,
что пиксель зависит от соседних пикселей сильно. В матрице, в общем случае, такого нет, у вас может
что угодно быть. Ну и соответственно, можно тоже самое попытаться сделать с какой-нибудь другой картинкой.
Например, здесь у нас, по-моему, в качестве картинки используется фотография МФТИ. Вот она тут
сейчас будет несколько. Надо было перед тем, как я вам начал отвечать. Ну что-то лень как-то ждать.
Ну хотя почему бы не увеличить? Пусть раньше у нас будет по 10 компонентов добавлять.
Вот вместо двух минут стало 20 секунд, даже меньше.
Странный хвост. А, кажется, я знаю, почему у нас странный хвост.
Сейчас я пытаюсь понять, пофиксил ли я. Дайте мне один момент.
Во, все.
Сейчас.
Норм. Этого минус этого.
Какой-то маркобисие.
Так, слушайте, окей. Я не понимаю. Тут что-то явно с нормировкой не то. Я не понимаю, почему вот этот хвост себя так ведет.
Но, собственно, что можно увидеть? У нас с вами относительно ошибка падает. Ошибка восстановления изображения.
Видите, относительно общего, скажем так. Что происходит? Мы с вами по-пиксельно считаем, насколько
восстановлено изображение отличается от того, что было изначально. И нормируемся просто-напросто,
так как у нас с вами по-пиксельно, кругово говоря, квадратичное отклонение, мы также нормируемся на
изначальное, по сути, стд изначальное изображение по-пиксельно. Получается, что у нас ошибка
примерно на 320 компонентах сваливается примерно в 0, как видите, для изображения. При этом
логично предположить, что при увеличении ранга нашего латентного представления линейно растет
изнимаемое пространство, потому что у нас просто-напросто ранг матрицы растет линейно. Количество
векторов, количество нулевых элементов диагональных растет линейно. По сути, отсюда тоже можно видеть,
что в какой-то момент у нас есть какой-то оптимум поименной эффективности. Другой стороны, не очень
понятно, насколько полезно вообще так делать для сжатия изображений. Опять же, это не алгоритм
сжатия изображений, которые используются во всяких кодеках. Почему мы это здесь добавили? Причина
простая. Показывать это на произвольных матрицах, будь то какие-то датасеты без обучения
соответствующих моделей, странно и долго. Если это показывать просто на произвольных матрицах с
точки зрения, смотрите, мы СВД построили, то вам эта матрица как ничего не значил, так ничего не
значит. Матричка как картинка при восстановлении мы глазами можно посмотреть и понять, что стало хуже,
стало лучше. Понятно? Вот, можно сравнить два изображения и между собой их сравнить,
посмотрев насколько разница восстановления одного и другого, потому что у них априори, скажем так,
структура разная. Здесь много мелких деталей, здесь наоборот больше каких-то однородных цветов и так
далее. Давайте попробуем их, например, сжать на 100 компонент оба. Заметьте, в фистех вроде как
почти не пострадал, вот здесь только сверху слева какие-то артефакты появились, и вот здесь справа
в основном почти полное восстановление, то есть визуально. При этом правая картинка, как вы видите,
гораздо больше пострадало, в ней гораздо больше деталей утеряно. Я не понял. Да, конечно, вот они.
То есть получается, что у нас с вами количество компонент 100, для фистеха плюс минус достаточно,
для вот этой фотографии уже похуже. Или можно попытаться на ту же самую ошибку, их свести,
но не хочется минуту ждать. K-лист. Ладно, начал уже, пускай считает. Пока оно думает,
может у вас какие-то вопросы есть? Наверное, пока оно думает, я вас постараюсь чуть-чуть
упокоить. Как правило, на вот этой лекции происходит отвал внимания у некоторой части потока,
потому что, хоть Матан непонятно пошел с двойственной задачей, вообще ничего непонятно,
электро еще и тоже про это упоминает, зачастую без каких-то деталей. Почему я не зарываюсь,
например, в детали двойственной задачи? Потому что есть 30 процентов аудитории,
которые, наверное, от этого поймут лучше, и 70, которые окончательно отвалятся и ничего не поймут
вообще. Я приношу свои извинения, но про двойственные задачи, правда, надо было тогда на курсе по оптам
нормально разбираться. Что я могу сказать? Те, кто как раз хочет почитать, у нас есть в доп-материалах,
у нас есть ссылочка на доп-материалы в репозитории, надеюсь, вы их уже видели,
если нет, то тут целая пачка всего. Нет, есть ссылки, которые прямо сейчас рекомендованы,
а, на магазин? Нет, ну погодите, там везде практически есть ссылки на открытые материалы,
просто есть книжка на английском языке, от автора выложенной на гитхаб, ее перевели на русский,
и переводчики, они за это хотят денег, а не от издательства, поэтому на русском языке ее в открытом
доступе нет, на английском она есть. Ну вот, смотрите, например, 100page, machine learning book,
ее продают на амазоне, но при этом автор ее положил на гитхаб, потому что, короче, как он сам у себя в
влоге писал, у него была политика read first, buy later, поэтому она была доступна. Видимо,
ссылки регулярно протухают, ну что ж, будем ревьюить, или кидайте нам какие-нибудь эти,
а? Ну да, ну или его вообще забанили по каким-то причинам, я, честно говоря, не знаю. Давайте
проверим вообще, у него репозиторий живой. Что только что произошло?
Я не очень понял, что сейчас произошло, если честно. У меня вылетели все вкладки.
Так, еще раз едем сюда. Вот наши доп-материалы, вот наш гитхаб.
Ладно, предположим. Понять не имею, что стало с репозиторием, раньше там была книжка. Ну,
найдем тогда другую версию. Или, правда, у нее поменялась линцензия, теперь ее нельзя
сначала прочитать, потом купить. Но, тем не менее, смотрите, собственно, есть такой рукописный
учебник, во-первых, от ваших предшественников, во-вторых, есть методичка Воронцова. Воронцов
замечательно краски здесь расписал про метод топорных векторов, поэтому крайне рекомендую всем
тем, кто жаждет двойственных задач и всего прочего, это дело прочитать. Собственно, вот до седы мы с
вами дошли формально, а дальше начинается вот эта красота, которую я вам лишь декларативно указал,
хотя по факту она тоже доказывается. Окей? Вот. В остальном же, смотрите, коллеги, еще раз,
сегодня у вас появится еще аж две домашки, не пугайтесь, потому что одна на автопроверку, вторая
надолго, аж на месяц вперед, это лабо. Но рекомендую не забивать на лабо, она вам пригодится, скажем так,
решенная ближе к концу месяца, потому что потом за один день ее решать не очень удобно. И лабо
это как раз то задание, которое позволяет вам по шагам уже пройти через пайплайн подготовки данных,
проверки разных гипотез, решение задачи и так далее. То есть вот эти маленькие домашки, они про то,
чтобы вам походить и головой подумать, лабо это про то, как решить реальные задачи. Ну а в остальном
дерзайте знать. До новых встреч!

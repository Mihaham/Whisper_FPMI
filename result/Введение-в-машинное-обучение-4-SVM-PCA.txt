Итак, сегодня мы с вами продолжаем беседу про
линейные модели и поговорим про еще две любопытные линейные
модели.
Забавно, но почему-то вот эти два подхода регулярно
люди путают.
Не говорю про всех, но некоторые люди, приходя
на беседу, могут перепут ПСЕ и СВМ, хотя по факту
один мет топорных векторов, второй мет главных компонентов.
Наверное, потому что по-русски звучит похоже, это путают.
К тому сегодня сразу две задачи.
Потому что, во-первых, эти методы во многом связаны
со всеми теми, что мы разбирали с вами ранее.
Во-вторых, на сегодняшнем занятии мы заканчиваем
свое путешествие сквозь чисто линейные модели.
Со следствием мы начнем уже говорить про деревья,
то есть про нелинейное преобразование, про их ансамбли и постепенно
подберемся к современным, относительно современным
сначала, потом современным нейронным сетям.
Опять-таки, сегодня у нас план очень простой.
Во-первых, поговорить про то, что такое мет топорных
векторов, никаких страшных двойственных задач и те
ремы хороших кунтаккеров у нас сегодня не будет,
хотя, по сути, именно оттуда все это дело было выведено
и замечательно выводится.
Я про это скажу лишь отчасти, потому что у кого-то из
вас метоптов вообще еще не было, у кого-то из вас
они были, но ККТ, кто сходу может сформулировать те
ремы хороших кунтаккеров?
Ну вот, стоит революции доказать.
По идее, он на третьем курсе, метопты на фупне, да и на
фифте, по-моему, читаются, раньше, по крайней мере,
так было правильно.
Ну ККТ должен быть в курсе метоптов, тут как минимум
ПМФ четвертого курса, значит вы, по идее, должны знать
что-то такое.
Ну вот, пожалуйста.
Ну, короче, не случилось.
Ладно, иногда, на всякий случай, что такой метод может
или лагранжа, по крайней мере, помните?
Это в Матане там было, решение задач с ограничениями, я
понял.
Ну, я чуть-чуть про это напомню, но глобально мы в это
с вами погружаться сегодня не будем, просто потому
что, ну, это, грубо говоря, стезя других коллег.
Ладно, поговорим с вами про СВМ, краски постараемся
ее вывести исходя из здравого смысла.
Сегодня нам краски понадобится вспомнить, что такое отступ
он же margin.
Все помнят?
Кто не помнит, что такое отступ?
Замечательно.
Огонь.
Я еще раз это проговорю, но практически вся аудитория
не подняла рук.
А потом вспомним с вами, что у нас есть не только
задачи обучения с учителем, есть и без учителя, и поснижаем
размер пространства.
Заодно после сегодняшнего занятия у вас появится
аж две домашки.
Ужас.
Во-первых, у вас появится третья домашка на автопроверку.
Вот тут уже, как я говорил, предложили некоторые коллеги
объюзить проверочную систему.
Ну, в принципе, объюзьте, главное нам потом скажите,
как вы ее поломали.
Это в принципе дело крайне хорошее.
А во-вторых, у вас, собственно, появится третья домашка,
она будет на пару недель.
Да, кстати, на всякий случай, так как многие оказались
крайне нерастыробно, я думаю, все те, кто в аудитории,
все слушатели, кто здесь, заполнили форму своевременно,
но некоторые люди, вот сейчас я буквально перед лекцией
докинул право всем тем, кто заполнил, еще двадцати
человек.
Но, тем не менее, коль уж такое дело, мы до пятого
числа дедлайн продлим, чтобы все могли нормально
первую домашку дослать, потому что, ну, людей жалко,
скажем так, вот маленький реверанс в сторону слушателей.
На всякий случай, это скорее исключение чем правило,
то есть не надо, пожалуйста, каждый раз говорить, давайте
продлим дедлайн, дедлайн известен второй до десятого.
Все дедлайны обычно стоят на понедельник, в общем
говоря, хорошо постоять на воскресенье, но практика
показывает, что на понедельник как-то работает лучше то,
что люди в ночь с воскресенья на понедельник любят что-то
судорожно доделывать.
Нам количество отправок растет очень значительно.
Хорошо, так вот, почему две домашки?
Вторая домашка, это будет, как раз, первая лабораторная
работа, которую вы можете начать уже сейчас.
Помните, кстати, в первой, да и во второй домашке обычной,
у вас были всякие теории вопроса, было непонятно,
куда их девать.
На самом деле, сделано исключительно для вашего удобства, у вас
в лабе будут места, куда вставить ответы на теории
вопросов.
По сути, они там будут те же самые.
Мы по факту просто разнесли некоторые теории вопроса
из лабы, потому что их надо проверять руками, по домашкам,
чтобы вы заранее подумали.
Только и всего.
То есть, вам там придется их вставить, если вы их
не отвечали, придется ответить, если отвечали, Control-C, Control-V,
все замечательно.
Так вот, первая лаба, она у вас будет уже аж на три,
наверное, с лишним недели выдана, она достаточно объемная,
она будет включать в себя кусочек по деревьям, то
есть, по идее, после сегодняшнего занятия вы еще не целиком
можете ее решить, но в ней там, по-моему, аж восемь
шагов, поэтому решать ее можно.
Да.
Ну, это у нас скорее такое следствие.
Она когда-то была на русском, в той переводе на английский
стало гораздо понятнее, и ее так оставили.
А?
В среднем.
Ну, смотрите, у вас иначе получается, воспользуйтесь
методом вот таким-то, обучите модель такую-то, потому
что используйте главные компоненты, но метод называется
PCA, потому что Principle Component Analysis, тут все в тех же терминах
используется.
Вот.
Ну и плюс, опять же, везде ссылки на доки, доки все
на английском, ну, мы в IT-шке сидим, тут вроде как английский
язык доминирует.
Ладно, давайте начинать.
Короче, что происходит?
На всякий случай, вот еще маленькие такие связи
будут с SVD, он же и сингулярное разложение.
Сингулярное разложение кто-нибудь помнит?
Хорошо.
А разложение по собственным векторам?
О, уже лучше, ну классно, тогда с ним краски и свяжемся.
Ладно, давайте посмотрим на задачку линейной классикации,
как она у нас была на прошлой неделе.
И как она была вообще.
У нас есть множество пар XY, X какой-то вектор в каком-то
линейном пространстве.
Опять же, все категориальные признаки в самом простом
случае берем и ванхотом просто кодируем и все.
То есть для каждого категориального признака с P значениями
создаем P-мерный вектор, где на, короче, нужном месте
стоит единичка все остальные нули.
И, соответственно, пусть у нас бинарная классикация,
тогда у нас Y принадлежит плюс-минус один, например.
Опять же, повторюсь, как вы помните, можно абсолютно
его переобозначить красный, зеленый, один-ноль, котики-собачки,
как угодно.
Котики, реальные величины и все.
Мы строим модель линейной классикации, которая выглядит
следующим образом.
Наш классикатор, пусть он будет A от X, плюс у нас
есть параметр омега, веса и омега ноль свободных
членов.
Это, краски, знак вот этой штуки.
На всякий случай можете сразу обратить внимание,
что, погодите, омега X плюс B, а у нас тут минус омега
ноль, то есть минус B.
Внимание, вопрос.
Имеет ли это какую-то разницу?
Никакой разницы нет, потому что омега все равно выучивается.
Если у вас минус омега, значит вы будете просто выучивать
то же самое, но с отрицательным знаком.
Все, ничего не меняется.
И, соответственно, наша функция потерь, именно в
общем случае, в классикации она всегда лишь одна.
Мы просто можем сказать, правильно или неправильно.
Поэтому наш эмпирический риск – это количество ошибок
классикации.
Сколько предсказаний не совпадает с истинной меткой
класса.
Или что то же самое, на скольких объектах у нас отступ отрицательный.
Хорошо?
Все.
Сколько объектов у нас получили отрицательный отступ,
значит не в чужом классе, значит все плохо.
Ну и, соответственно, про отступ мы с вами помним,
что это по сути проекция краски вектора на наш вектор,
который указывает на объект на нормальной плоскости.
Вот здесь краска минус омега 0 позволяет нам перетащить
все это дело в ноль, поэтому все спокойно.
На у, чтобы мы учитывали, по сути, в глубине своего
класса или чужого.
Именно поэтому мы здесь учитываем, грубо говоря,
плюс-минус один, нам так просто удобнее.
Потому что у нас тогда знак, если сонаправленный, будет
положительный, если разнонаправленный, будет отрицательный.
Хорошо, с этим все понятно, правильно?
Класс, не едем дальше.
А теперь давайте подумаем.
Вот у нас с вами задача кружочки против квадратиков
и вот вам три гиперплоскости, к которым можно провести
результат одинаковый.
Ошибок ноль.
Внимание, вопрос.
Какая плоскость лучше?
Вы выбираете вторую.
Вы это интуитивно сделали на основании каких предположений?
Максимальная уверенность во всех, чтобы было симметрично.
Классно.
Еще какие предложения?
Через ноль проходит тоже, на самом деле, классно.
Максимальная удаленность от всех.
Классно.
То есть интуитивно многие из вас понимают, что вторая
как-то получше.
Здесь, на самом деле, можно опять же вернуться к такому
определению.
Оно не совсем формальное, но тем не менее мы его с
вами обсуждали, кажется, две недели назад.
Устойчивость модели.
Помните?
У нас гиперплоскость определяется вектором нормали и свободным
членам.
Мы говорим, что модель является устойчивой, если
малое изменение обучающей выборке не приводит к значительному
изменению параметров.
В данном случае или предсказание модели, что тоже так бывает.
Смотрите, если у нас вот эта модель, вот L1, если
ее чуть-чуть подвигать буквально на, еще не знаю,
мэпсил малую углу, она, грубо говоря, может начать
захватывать кружочек с другой стороны или квадрат
опять же с другой стороны.
Получается, что при небольшом изменении параметров модели
у нас с вами резко меняется предсказание модели на
некоторых объектах.
Получается, у нас некоторые объекты неуверенно классифицируются,
о чем вы как раз тоже говорили.
С L2 такой проблемы нет, с L3 опять такая же проблема
есть.
Но мы это с вами описали вроде как словами.
Как это формализовать?
У вас был вопрос?
Нет?
Хорошо.
Как это формализовать?
Мы можем как раз-таки об этом вспомнить ровно в
терминах отступа, о котором мы с вами говорили.
Давайте выбирать такую гиперплоскость, которая
максимизирует отступ до всех точек.
Более того, опять давайте сходим назад на один шужочек.
Мы вычли свободный член, поэтому наша гиперплоскость
проходит через ноль.
Договорились?
Просто для удобства нам так гораздо проще будет.
Теперь внимание вопрос.
Если у нас с вами гиперплоскость проходит через ноль, скажите,
мы можем с вами утверждать, сделать вот такое утверждение,
если выбракой линии наразделим, выдвинуть такое утверждение.
Что для всех объектов одного класса отступ больше
или равен единице, для всех объектов другого класса
отступ меньше или равен минус единице.
Ой, простите, не отступ, а именно вот это нормирование,
как скажем так, расстояние со знаком, короче, глубина
в классе.
Понятно, почему здесь именно плюс-минус один, а не плюс-минус
ноль-пять.
Ну смотрите, еще раз, омега ноль, во-первых, здесь у
нас омега ноль, и омега обучается, правильно?
По сути, вы, используя свои значения параметров,
можете таким образом перенормировать омега, чтобы всегда выполнялась
вот это неравенство, если у вас выбрака линиейно-разделима.
Что происходит?
Вы, по сути, говорите, вот у меня разделяющая гиперплоскость,
вот это расстояние теперь должно быть равно единице.
Что такое это расстояние?
Это омега и икс.
Все, если вы перенормируете омегу, потому что до икса
вы не можете нормируй все-таки это пространство, не надо
менять просто так значение признаков, в омегу можете
перенормировать так, чтобы неравенство выполнялось.
Согласны?
Теперь.
Опять же, мы это делаем чисто для удобства, потому
что плюс-минус один, ну, с ним понятия.
По факту я здесь могу написать плюс-минус 100, и ничего не
поменяется.
Да?
Сейчас увидите.
Вообще говоря, один, у единицы есть как минимум одно классное
свойство, она ограничивает сверху-снизу всякие наши
периодические функции.
Вот.
Хорошо.
Собственно, мы с вами можем это выписать, правильно?
С этими двумя неравенствами теперь все согласны.
Хорошо.
Ну и едем дальше, соответственно.
Вот опять они у нас переписаны, и мы теперь хотим сделать
что?
Мы с вами говорим, что у нас таким образом все выполняется.
Давайте-ка теперь попробуем из этого поставить задачу
максимизации какую-то, или минимизации, короче,
оптимизационную задачу прямо отсюда вытащить.
Каким образом?
Ну, мы с вами можем посмотреть сюда, домножить второй
неравенствов допустим на минус один, и что у нас тогда
получается?
Омега х минус, минус один на омега х минус минус
омега ноль, больше или равен опять же единице, правильно?
И если вы внимательно посмотрите, если сюда домножить на
минус единицу, то это чистой воды будет отступ для отрицательного
класса.
А это чистая вода-оступ для положительного, потому
что у него плюс один.
Верно?
Ну, если у нас два неравенства типа большего, мы их можем
сложить.
Левая часть, сумма левых частей все равно, большая равна
сумме правых частей.
Вот мы с вами можем сделать это.
Вот мы с вами ее, собственно, и получаем.
Х плюс минус х минус, у нас эта штука симметричная,
все нормально.
Краски, больше ли равен, чем два, здесь норма омеги.
Откуда здесь взяла норму омеги, скажите мне, пожалуйста.
Смотрите, мы с вами только что сказали, что мы можем
с вами перенормировать омегу так, чтобы у нас выполнялась
неравенство, правильно?
Поэтому мы можем сказать, что х плюс минус х минус
умноженное на омега скалярно, минус омега ноль, и опять
ж плюс омега ноль, они поэтому и потерялись, свободные
селены.
Больше ли равно двум, согласны?
Но здесь у нас эта неравенство зависит от обеих.
Давайте опять перенормируем ее теперь обратно в единичный
вектор.
Каким образом?
Поделимся на ее норму.
Все.
Поэтому теперь у нас омега делить на норму омеги, это
единичный вектор, направленный в сторону нормали.
И все в порядке.
И получается, что у нас вот эта штука всегда больше
равна, чем два, делить на норму омеги.
Согласны?
А теперь внимательно посмотрите, что такое х плюс минус
х минус?
У нас один вектор, второй вектор, и, соответственно,
мы вычили их и проецировали на вектор нормали.
Получается, это расстояние по нормали, кратко, между
ближайшим объектом х плюс и х минус.
Верно?
Ну вот оно у нас нарисовано.
Вот у нас два вектора.
Вот х плюс минус х минус, проецированный на направлении
нормали.
Согласны?
Ну получается что?
Мы здесь, по сути, утверждаем, что вот эта штука является
верхней оценкой на двойку всегда, и делим на норму
омеги краски, чтобы это вообще не зависело от омеги
и зависело только от наших точек.
Все, пожалуйста, вы придумали себе новую оптимационную
задачу.
Теперь мы говорим, давайте у нас для всех наших точек
будет выполняться вот эта пара неравенств.
По сути, мы говорим, что отступ всегда больше равен
днице.
Хорошо.
Тогда, максимизируя 2 делить на норму омеги, мы будем
максимизировать зазор между классами.
Потому что зазор — это как раз таки оценка снизу
на расстоянии между ближайшими точками.
Согласны?
Все, мы с вами только что придумали себе новую оптимизационную
задачу.
Да?
Да, омега 0 у нас явно прямо уходит при вычитании.
Смотрите.
Омега х минус омега 0 минус омега х минус минус минус
плюс омега 0.
Все, она ушла вообще.
Согласны?
Все.
И, соответственно, получается, что это у нас оценка сверх.
А теперь получается что?
Если наша выборка линейно-разделима, то среди всех точек, простите,
среди всех гиперплоскостей, которые у нас есть, нас
удовлетворяет та гиперплоскость, которая доставляет максимум
вот этого функционала.
Согласны?
А это вам, кстати, сразу овтоп, но сразу задам вопрос.
Это вам ничего не напоминает?
Норма у нас здесь какая, во-первых?
Если у нас просто обычный скалярный произведение,
какая здесь норма?
Евклидова, правильно?
Вторая норма.
Два делеть на вторую норму нужно максимизировать.
Если мы максимизируем один делеть на вторую норму,
значит мы минимизируем что?
Вторую норму.
Согласны?
Это то же самое ограничение, что у нас было раньше.
Мы с вами только что обнаружили, что л2 регуляризация
распширяет нам разделяющую полосу.
Это так, к слову.
Хорошо.
Вот мы с вами его придумали, и вот, по сути, у нас с вами
теперь решается следующая задача.
По сути, для задачи, где у нас выборки линейно разделимы,
то есть у нас нет никакого шума, мы можем найти какую-то
гиперплоскость, которая подходит, мы решаем с вами
следующую задачу.
Необходимо у нас есть ограничение, чтобы все отступы были
больше или равны единице, и при этом мы должны минимизировать
вторую норму векторовисов.
Ну, тут одна вторая, но, как вы понимаете, константа
никоим образом не влияет.
Все.
Вот раз.
Возникает сразу два вопроса.
Что делать, если у нас с вами задача нелинейно
разделимая, у нас правило шум есть, поэтому мы не
можем прямую провести так, чтобы у нас не было ошибок
или гиперплоскости.
И второе, а что делать с двойственной задачей?
Тут краски можно вспомнить метапты, что мы умеем решать
двойственные задачи и так далее, но что-то ККТ
вызвало некоторые вопросы у аудитории, поэтому будем
решать по старинке методом градиентного спуска.
Так тоже можно.
Хорошо?
Собственно, давайте.
Давайте начнем с первого вопроса, что делать, если
выборка нелинейно разделима.
Может кто-то из вас сразу может предложить, что делать?
Проигнорируем.
Хорошо.
Но как-то штраф вести.
А в какой штраф?
Ну вот, хорошая идея от вашего коллеги как раз.
А давайте за каждую пару, даже не за каждую пару,
за каждый объект, который нарушает вот это ограничение,
будем добавлять какой-то штраф нашей модели.
Классно.
Давайте ему краски введем.
Пусть у нас теперь ограничения отлабляются, и теперь у нас
маржин для каждого объекта, больший равен 1 минус кси
ит.
Кси ита это такая свободная переменная, латентная,
к которой мы просто ввели, она ничему реально не соответствует.
Мы говорим, что для каждого объекта теперь существует
некоторая ксишка.
Ит объект, ит иксишка.
И соответственно, мы говорим, что кси иты всегда положительны,
и теперь мы будем решать соответственно две вещи.
У нас есть первая штуковина, собственно, первое ограничение,
что у нас маржин больше или равен 1 минус кси итой.
Второй, что все кси больше или равны нулю, но потому
что иначе бред какой-то, мы увеличиваем маржин,
зачем-то в ограничении нам это не надо.
И собственно теперь мы добавляем сумму всех этих наших штрафов
в наш оптимизируемый функционал.
Понятно, что произошло?
Логично взять нелинейную, в простейшем случае это
линейная зависимость.
То есть здесь вы на самом деле можете какую-то и другую
ввести, и это приведет вас к другому решению, но
как правило, смотрите, что такое маржин, это насколько
глубоко вы находитесь внутри своего класса.
По сути мы говорим, вот на самом деле мы меньше, чем
на единицах глубине своего класса, например, оно вообще
отрицательное, мы в чужом классе сидим, но расстояние
это у нас линейно суммируется в данном случае, поэтому
логично взять линейную, почему оно по квадрату
должно расти или по любому другому.
Но норма-то у нас краски в квадрате, но у нас это
сумма квадрата под квадратным корнем, степень все равно
первая.
Тут степень тоже первая.
Хорошо?
Окей.
Смотрите.
И получается вот наша теперь с вами оригинальная
задача.
У нас с вами есть, собственно, оригинальная оптимизированная
задача, плюс все наши ограничения, которые породили нам штрафы,
это надо отправить опять джинаминием.
Теперь еще и по всем ксишкам.
Но решать задачу аж двумя ограничениями, по сути у
нас с вами есть краски 1 и 2 ограничения, не жесткие.
Мы можем это все перевести в задачу, скажем так, безусловной
оптимизации.
Краски вспомним, как там двоистные задачи решаются
и так далее.
Смотрите.
Что происходит?
Вот у нас с вами оригинальный оптимизируемый функционал,
правильно?
У нас одна вторая норма омеге в квадрате, с ней мы ничего
сделать не можем, она существует, она отвечает за ширину
полосы.
Все, отложили, она существует.
Дальше.
С на сумму всех ксишек, правильно?
Что такое сумма всех ксишек?
На самом деле, что такое кси?
Ну, кси у нас можно перекинуть сюда, это сюда.
Кси у нас всегда больше или равна, чем один минус
марджин.
Согласны?
И причем мы помним, что кси только больше или равна
нулю.
Если кси меньше нуля, то она нас не интересует.
Все.
У нас не должно быть кси меньше нуля, потому что
иначе это...
Ну короче, если кси меньше нуля, это значит, что у нас
марджин еще более, еще больше, чем единица.
А нам не надо, чтобы он был больше единицы, мы на
это ограничение накладываем.
Ну, получается что?
Получается у нас вот это неравенство, можно тем
образом сказать.
У нас кси всегда больше или равна один минус марджин,
но при этом мы с вами помним, что краски марджин больше
единицы нас вообще не интересует.
Правильно?
Хорошо.
Ну и тогда мы с вами можем переписать в каком виде.
Когда сумма ксишек, она у нас больше или равна чем
что?
Чем сумма один минус марджин?
Правильно?
И эта штука всегда должна быть не отрицательна.
Потому что если она отрицательна, значит один минус марджин
получился каким?
Простите.
Раз, два, три, четыре, пять.
Если она отрицательна, то один минус марджин получился
как раз таки...
Господи.
Отрицательным, значит у нас все отрицательны, чего
она не должна.
Вот.
Короче, мы с вами из двух ограничений, по сути, переформулируем
вот эту часть уже в безусловной задаче оптимизации.
Один минус марджин, по сути, это верхняя нижняя оценка
на кси, куда пропала, и берем только положительную
часть.
То есть если она отрицательна, она равна нулю.
Если положительная, то она равна тому, что есть.
Да?
Зачем нам ограничивать кси с нижним нулем?
Если все отрицательные, то это означает, что у нас
же как-то большая уверенность в запрете?
Да, но нам не нужна большая уверенность, это ограничение.
Смотрите, это не исходная задача, это ограничение
на задачу.
Мы говорим, что наш марджин должен быть не меньше, чем
1 минус кси.
А сверху, если марджин больше единицы, мы точно довольны.
Поэтому нам кси отрицательны, смысла не имеют в том, что
иначе мы получим марджин ограничен снизу еще большим,
чем единицей.
У нас для самых близких объектов марджин должен быть равен
единице в разделимой задаче, а в неразделимой единице
минус кси.
Все.
Уловили?
Нет?
А с – это, как раз таки, некоторая константа, которая
учитывает, насколько у нас ширина полосы по важности
соотносится с нашими нарушенными ограничениями на марджин.
С – это гиперпараметр.
Ну и теперь вы можете, по сути, увидеть итоговую
модель.
У нас с на сумму 1 минус марджин положительный, плюс 1
вторая норма омеги.
Ну, допустим, в квадрате в данном случае, пожалуйста.
Опять же, откуда взялся квадрат, на всякий случай?
Вот тут у нас вроде квадрата не было, потом у нас квадрат
появился.
Квадрат на самом деле появился, исходя из простых соображений.
Во-первых, когда у вас стоит норма вторая, сама по себе
и не в квадрате, ее считать дороже.
Вам нужно сначала посчитать своему квадратов, потом
еще считать квадратный корень.
Так как у нас парабола… парабола, господи… квадратный
корень функции монотонная, правильно?
Соответственно, argmax у нас будет одинаковый.
Поэтому вы завели в квадрат или парабола, опять же, справа
от нуля.
У нас задача не поменяла своего решения, именно точки
аргумента, где достигается максимум, но при этом считать
нам выгиб.
А?
Одна вторая просто остается исходно оттуда, вот она
здесь стояла.
Смотрите, я подеваю, ладно, согласен, должна быть одна
четвертая.
На самом деле, абсолютно, я тогда поправлю, это здесь,
это видимо просто опечатка.
Абсолютно неважно почему, у вас есть константа.
Давайте все просто, короче, вот это все еще домножим
условно на двойку, тогда здесь будет C со звездой
новое.
C со звездой, как раз, отвечает за, по сути, соотношение
регуляризации, не регуляризация, простите, ограничений и
шириной полосы.
Все константы, они, по сути, вырождаются в одну.
Так, ну вот страшные всякие ботан, точнее метапты для
тех, кто знаком с метаптами, я вам их лишний раз показывать
не буду.
Вот.
Я на самом деле хотел бы с вами сейчас внимательно
посмотреть вот на эту формулу, снизу, ККТ показывать не
надо.
Если вы покажете это большой плюс, это плюс балл, молодцы,
но, скажем так, если вас на метаптах этому не научили,
мои полномочия на этом все.
К сожалению.
Я, так скажу, по опыту процентов 80 не может это воспроизвести
на экзамене, так что я к этому отношусь уже по-философски.
Вот.
Ладно, давайте сюда внимательно посмотрим, пожалуйста, посмотрите
внимательно на эту формулу и скажите мне, она вам ничего
не напоминает, классически, если я пока дверь закрою.
Похоже на регуляризацию, но теперь давайте посмотрим
внимательнее.
Это на самом деле я вас туда и подвожу.
У вас есть два члена в вашей функции, скажем так,
эмпирического риска, который вы минимизируете.
Правильно?
Первое.
У вас есть какой-то член, который отвечает за величину
на каждом объекте по отдельности, вот же у вас здесь margin на
каждом объекте стоит, правильно?
1 минус margin положительная часть.
Вторая часть у вас зависит только от вектора весов,
от параметров вашей модели, вообще не зависит ни от
каких объектов по отдельности, только целиком.
Мы с вами раньше видели то же самое.
Вся потеря плюс регуляризатор.
Здесь у нас 1 член, 2 член, давайте я до него сейчас дойду,
по сути вот оно у нас и есть, но почему очень важна
эта часть лекции и почему на этом занятии такое пристальное
внимание уделяется тому, откуда мы это вывели.
Помните нашу изначальную постановку задачи.
Мы сказали, пусть у нас выбор коленина разделимая,
и тогда мы максимизируем ширину полосы, а значит
мы минимизируем вторую норму вектора весов, согласны?
Наша изначальная оптимизационная задача была именно минимизировать
вторую норму вектора весов, плюс у нас были дополнительные
ограничения, что у нас margin везде больше или равен
1.
В классической постановке, если мы сюда посмотрим, у
нас какая-то функция ошибки, плюс регуляризатор в качестве
второй нормы вектора весов, мы это с вами уже раньше
видели, регуляризация потихоньку.
К чему я это веду?
К тому, что зависит от вашей формулировки задачи, что
вы назовете регуляризатором, что вы назовете на самом
деле функцией потерь, можете хоть горшком все назвать,
у вас всегда оптимизационная задача решается и в нее
входят, прям так, у вас либо один функционал оптимизируется,
либо сразу несколько их линейных комбинаций.
Как вы их называете, это ваше дело, это зависит исключительно
от того, какой у вас бэкграунд, какие требования заказчики
и так далее.
Суть в том, что все равно вот эту штуку вам придется
минимизировать.
То, что только что у нас с вами оригинальный функционал
был.
Вот, который мы оптимизируем, а вот на него ограничения.
По факту мы раньше говорили, что у нас оригинальная функция
потерья то, что мы минимизируем, плюс ограничение второй
нормы векторов весов, например.
Что вы назовете ограничением зависит от того, как вы задачу
формулировали.
Здесь у нас получилось по факту наоборот.
Вот наша функция потерь, в кавычке, то, что нам надо
минимизировать.
А вот наши ограничения.
Как видите, я только что по сами руками их поменял
насталь.
Так что, пожалуйста, не привязывайтесь намертвость
тем, что вот функция потерь это функция потерь, регуляризация
ограничения, по факту у вас все эти члены составляют вместе ваш функционал,
который вы оптимизируете. И важно понимать, что откуда пришло, а не что каким
словом назвать. Можешь назвать как угодно просто, не знаю, там, r1, r2. Первый
член, второй член. Все. Хорошо? Тут вопрос есть?
Вот смотрите, регуляризация в классическом смысле, откуда взялось один делить на
2c. Ну, все поделили на c. Это просто исходя из классического, скажем так, вывода,
который только что здесь был, один делить на 2c. Логично, что один делить на 2c
можно переобозначить другой констант и с ней работать. Это просто, скажем так,
реверанс в сторону оригинального представления, которое было представлено
Вапником и Червоненкесом в их работе чуть ли не 53 года. Я могу наврать с датой.
Короче, метапорных витеров это, собственно, наследие советских математиков Вапник
и Червоненкес, которые создали метапорных витеров, потом обобщили его на линии
неразделимую выборку и так далее. Вапник все еще выступает в роли
профессора. Некоторое время назад он был в ныне запрещенной у нас организации
на западе, у них в лабораториях там по искусственному интеллекту.
Червоненкес, к сожалению, по-моему, году в 2014-м отошел в мир иной, по-моему,
гулял в лосе на островском парке и заблудился. Так что, пожалуйста, это. Не теряйте
телефоны. Классный очень мужик. Отвечал, в том числе, помогал запускать шат, который,
я думаю, многие из вас знают, но вот так случилось.
Вот, смотрите, что здесь имеется в виду. Во-первых, исторически для SVM-а
конкретно записывают именно вот эту константу, которая отвечает за баланс
между нормой вектор весов и марджином, именно как 1 делить на 2c. И если у нас
1 делить на 2c, соответственно, если ц большая, значит, нас множит 1 делить на 2c,
эта штука маленькая, и значит, у нас ограничение на ширину полосы тоже небольшое. Мы говорим,
что нам не столь важно, насколько широкая полоса, то есть, по сути, она может больше крутиться,
но при этом нам важно, чтобы у нас ошибка аппроксимации была наименьшая, то есть как
можно меньше объектов должны иметь марджин меньше единиц. По сути, когда мы с вами снижаем ширину
полосы, логично, что внутрь полосы попадает меньше объектов и меньше краткие вот этот член.
Согласны? Вот. Если ц опять же маленькая, то 1 делить на 2c большая величина, значит,
нам нужна широкая полоса, у нее меньше свободы крутиться, но при этом она заметает больше объектов
и этот член становится больше. Все. То есть, просто интерпретация того, что если ц большая, значит,
у нас полоса узкая, объектов в нее попало мало, если ц маленькая, полоса широкая, объектов в нее
попало много. И теперь смотрите, в чем еще магия. Мы же с вами помните, на прошлом занятии выводили
как раз-таки log-loss и обнаружили, что это верхняя оценка на нашу функцию потерь, именно истинную,
которая просто корречит ошибку классификации, ступеньку. Вот она. Вот ваша ступенька марджин
меньше нуля. Что мы здесь видим? 1 минус марджин плюс положительная часть. Вот ваша функция
потерь. Это опять верхняя оценка внезапно, потому что на самом деле она выведена исходя из того,
что мы решаем задачи классификации. И эта функция потерь называется hinge-loss. Не знаю,
функция потери менее hinge-loss. Не имею hinge-loss, нам ее всю жизнь все кличут, как она называется по-русски.
Я даже боюсь вам наврать. Ну вот, собственно, как она у нас выглядит. Хорошо?
Ширина полосы. Еще раз, смотрите. Что означает ширина полосы? Мы говорим, что у нас ширина
полосы — это расстояние между ближайшими к разделяющей гиперплоскости объектами одного
класса и другого класса. И в зависимости от того, как вы проведете гиперплоскость, у вас она может
быть либо больше, либо меньше. Потому что, смотрите, вот для этого, например, видите, у вас раз и два.
По направлению проекции у вас там совсем маленькое расстояние. Для второй прямой у вас расстояние по
направлению проекции сильно больше. Чем больше вот это расстояние, что то же самое, что ширина
полосы, тем более устойчива наша модель с точки зрения того, что небольшое изменение весов не
приведет к тому, что у нас ответы классикации поменяются. Вот. Хорошо. Ну и, собственно, вот это
самое ограничение на наш, где он, вернись, на наш маржин, один минус маржин как раз-таки, это есть
наша часть кинжалоса, причем на нее глядя можно очень просто понять, что на самом деле происходит.
И почему мета называется метапорных векторов? Посмотрите, если мы вот эту штуку будем
градиентным способом оптимизировать, а че, все нормально, дифференцируемо, дифференцируемо,
все дифференцируемо. Если вы вспомните, что кинжалос у нас в точке 1 не дифференцируемый,
ну давайте определим производную в единице нулем, и все. Нам не нужна гладкость, нам достаточно знать
производную в каждой точке. Что здесь происходит? Смотрите, какие объекты являются опорными? Вот
интуитивно, подумайте, глядя на этот функционал. Что такое опорные объекты, как вы думаете?
Классно. Ну сейчас я покажу, откуда это выводится из оригинальной постановки. Смотрите, какие объекты
вообще влияют на решение с точки зрения данного функционала? Если объект находится в глубине своего
класса, он как-то влияет вообще на решение? У него margin больше единицы, 1 минус margin отрицательный,
у нас все отрицательное отбрасывается, настолько положительное, поэтому в этом члене у нас клад
будет 0, на вес он напрямую тоже не влияет 0. Все, объект вообще не влияет на решение. Он
абсолютно не интересен. Именно поэтому это называется опорными хакторов. Опорными являются только те
объекты, у которых вот эта величина меньше или равна меньше единиц. Они опорны потому что они
вообще влияют на решение. Все остальные объекты вообще игнорируются, они могут быть где угодно,
как угодно, нам не важно. Опорны только те объекты, которые оказались внутри полосы или, соответственно,
вообще в чужом классе. Вот они опорные. И опять же, здесь есть, грубо говоря, два типа объектов.
Первое, те, которые сидят внутри полосы, это именно что опорные объекты, на них опирается наша
гиперплоскость. Второе, это шумовые объекты. Мы, к сожалению, ничего с ними сделать не можем,
если объект сидит вообще внутри чужого класса, где-нибудь вот, не знаю, синяя точка будет вот
здесь сидеть, то у него точно также будет 1 минус margin большая величина, и за это мы будем,
грубо говоря, платить. Вот. Понятно? И да, и нет. Смотрите, фильтрация выбросов тема классная,
но у нее есть одна маленькая проблема. Если у вас выборка очень многомерная и у вас там существенный
нелиней на зависимости, то у вас выбросами, грубо говоря, выбросом вы можете обозначить объект,
если у вас либо уже существует модель, и он сильно ей не подчиняется на вопрос ваша модель хорошая
или плохая, или если вы можете это явно увидеть. Например, у вас совершенно спокойно, может быть,
я пока рассказываю, пускай эта штука включится, у вас совершенно спокойно может быть какая-нибудь
разделяющая гиперплоскость, не гиперплоскость, а гиперповерхность, вот такая вот. Синусоида,
представьте себе, и, соответственно, внутри сверху от синусоида один класс, снизу другой. У вас
никакого эвористического правила, который позволит одно от другого отделить, если вы не знаете
истинную зависимость, нет. Поэтому вы эвористически можете обозвать выбросами, например, те точки,
которые сидят на вершинах краски этих вот синусоиды или ксинусоиды. Являются ли они выбросами? Нет,
у вас модель плохая. Так что в общем случае отбора нет, но есть различные способы, один из них,
краски мы разберем с вами на следующем занятии, как фильтровать выброс, который прям явно выброс.
Но в общем случае отбора бошечки, оно на венде, отбор выбросов у нас как такового не эвористического
нет, потому что, чтобы знать, какие точки не подчиняются зависимости, нам надо знать саму
зависимость, а наша задача найти зависимость. Пока мы ее не нашли, для нас точки просто какие-то
странные. Так, ладно, спасибо, можно тебя выключить? Я уже передумал, я уже в воздухе все нарисовал.
Хорошо, но собственно вот, на всякий случай, что здесь написано. Смотрите, вот, ксишка равна нулю,
это те объекты, на которых у нас ограничения не нарушаются, с ним все понятно, они не опорные.
Ксишка равна нулю, соответственно, те объекты, которые находятся на границе, они краски уже на что-то
повлияли, потому что до этого ксишка была не равна нулю. Те объекты, для которых ксишка больше нуля,
значит, не попали внутрь полосы или вообще наружу, это объекты, которые тоже влияют на решение. Вот эта
пара влияет на решение, эти никоим образом не влияют. Опять же, откуда здесь какие-то лямбды,
это собственно из двойственной задачи. Вот, мы можем посмотреть, вот мы вводим одни ограничения
нежесткие, вторые жесткие, типы неравенства и неправенства, переписываем, соответственно,
условия для локального минимума необходимые, решаем, вспоминаем там мед, может ли, ларанжа,
получаем результат. Но это, скажем так, передам Александру Катруце, наверное, он сейчас читает
опты. Знамя, чтобы он вам про это рассказал. Хорошо, ну что, со своим понятно, что происходит? А
теперь, как раз, давайте вспомним, что мы с вами делали на лекции номер два классической нумерации
с начала семестра или на лекции номер ноль, если посмотреть в репозитории, я просто вынес, чтобы
классическую нумерацию иметь. Помните, мы там с вами говорили про скалярное произведение,
правильно? Мы же можем с вами разные скалярные произведения вводить, согласны? И более того,
здесь мы с вами можем видеть краски, здесь это классическое скалярное произведение, когда мы
считаем margin, у нас там ωх плюс b, все понятно. А давайте-ка попробуем выбрать что-то более
подходящее для задачи и ввести саму понятие более подходящее, более формально. Мы, по сути,
с вами, когда считали margin, мы внутри него считали скалярное произведение между вектором весов и
вектором нашего объекта, правильно? А почему бы нам не взять какое-нибудь другое скалярное
произведение, какого-нибудь другого пространства, которое все еще удовлетворяет всем тем же
ограничениям, которые у нас на него есть, чтобы не получить другие виды правильства? Ну, например,
давайте перейдем в какое-нибудь другое гибель этого пространства и введем там какую-нибудь функцию
ядра, по факту переопределим скалярное произведение, которое было изначально, вместо дотпродукта будем
использовать вот такое ядро, которое удовлетворяет следующим ограничениям. Хорошо? Понятно,
что здесь написано, на всякий случай? Прочитайте, пожалуйста, вот здесь. Мне надо, чтобы вы
прочитали эти формулы, правда. Это проще языком математики сказать, чем долго пересказывать
языком обычным, я и так тоже сказал, на самом деле. Смотрите, у вас что написано? Ядро для двух точек,
вот, к от х штрих, это что? Это отображение из декартового произведения пространства х на
пространство х в R. Это просто две точки. К от х задает функционал, который отображает декартовое
произведение n-мерного пространства линейного, в котором мы были, в R. Собственно, х и х штрих это
два вектора. В нашем случае у нас что умножается друг на друга? Вектор, который указывает на точку,
и вектор нормалик нашей гиперплокости. Они же все равно в одном линейном пространстве, правильно?
Раз мы их перемножим, можем скалярно. Вот, а теперь, собственно, что за новое пространство? А мы можем
даже явно его не задавать, мы вместо этого можем сказать, а давайте-ка мы зададим какое-нибудь
ядро ему будет соответствовать какой-то гейлер этого пространства. И более того, за счет того,
что мы поменяли ядро, по сути, мы подмена ядра, поменяли скалярное произведение пространстве. Мы
из скалярном произведении можем индуцировать норму, по сути, мы поменяли наше пространство,
там теперь норма другая стала. Соответственно, расстояния поменялись. А если мы это сделаем,
то мы с вами не явно преобразовали наши признаки пространства, сделав его нелинейным относительно
исходных признаков. Я вам сейчас пример покажу. Вот смотрите, вот наша исходная выборка,
например. Вот у нас с вами линейная гиперплоскость. Вот мы с вами берем и вместо этого вводим
полиномиальное ядро краски степени D. Я не знаю какая там D, я не помню, кажется 2 или 3 степени.
И теперь у нас с вами скалярная произведение считается вот таким образом. D, наверное,
равно 3. Все остальное абсолютно так же. Вы решаете ту же самую оптимизационную задачу.
У вас только скалярное произведение по-другому считается. Итоговая гиперплоскость в исходном
пространстве вот так выглядит. У вас поверхность теперь нелинейно-разделяющая. Да, это ваше ядро,
то есть здесь это именно классическое скалярное произведение, там специально треугольные скобки,
а потом мы это возводим в третьей степени. Вот экспоненциальное ядро, пожалуйста. Взяли,
посчитали норму, возвели в квадрат минус гамма-экспонент. Еще раз, это линейная, по сути,
гиперплоскость, в том новом гильбертом пространстве, куда мы попали. В исходном
пространстве логично, у нас скалярное произведение другое, норма другая, оно выглядит нелинейно.
Плюс в чем? Вы, подобрав правильное ядро, по сути это называется спрямляющее пространство по-другому,
вы спрямляете пространство, в нем ваша выборка линейно-разделима становится. То есть вы,
подобрав правильное ядро, можете решить задачу уже линейным образом. Проблема этого заключается
лишь в одном. Как вы думаете, в чем проблема этого трюка с подменой ядра или кернел-трик, как его
называют? Переобучаемся раз, сложно считать два, еще варианты. Бинго, откуда нам взять ядро? Мы не
знаем, какое ядро подходит. В общем случае, у нас с вами есть вот пачка, грубо говоря, общепринтых
ягер, там штук 20, еще у некоторых из них еще и параметры есть, у полинамиального вот этого
тут D, вы можете варьировать 2, 3, 4, 5, 10. Все. Какое ядро выбрать, это вопрос.
Вообще да, может быть. Я, честно говоря, ваш вопрос не понял, может мне потом в перерыве
тогда сказать конкретно, в чем беда, ладно? Вот этого конкретно? Вот это?
Я ничего не понимаю, какой термин вам? Да. Ну слушайте, самое простое, наверное, взять методичку
Воронцова, там стоит ссылка, скорее всего, на книжку, господи, как она называется, 11 года,
наш Толмут. Все, позор моим сиделом, я забыл, как книжка называется.
Не, погодите, что здесь написано? Здесь мы все лишь говорим, что для нашего ядра должно
выполняться следующее, что существует такое отображение из X в новое пространство H,
что именно про скалярное произведение от образа X и образа X' есть как раз таки результат
применения к исходным их значениям ядра. Все. Мы утверждаем, что существует отображение из
исходного пространства X в целевое спрямляющее пространство H такое, что скалярное произведение
образов есть результат применения ядра к исходным значениям, как они там полностью называются? Есть
образ, а есть что? Прообразов. Можно его найти. Я вам его по памяти не назову, я честно скажу.
Не, как все отексы выглядят, я думаю, надо в каждом коментном случае это вводить. На самом
деле на ядро есть несколько ограничений. Я, кажется, не включил слайд, давайте я потом покажу вам.
Вот. Еще вопрос.
Кажется, все немножко загрустили. По факту, что происходит с ядрами? Во-первых, нужны ли они вам
сейчас вот прям на практике? На практике kernel trick применяют скорее реже, чем чаще. Это происходит
в каких-то задачах, где вам вот SWM очень хорошо нравится, или если вы очень сильно нравится,
или если вы имеете какое-то прям явное предположение, какое пространство спрямляющее вам может
подходить. Ну, например, такое бывает в различных биржевых задачах, там SWM все еще популярен.
По факту, зачем нам это надо? Затем, чтобы первый раз выйти за границы линейных моделей, потому что мы
с вами первый раз говорим, а что сделать, если у нас линейной модели явно не хватает, а что делать мы
не знаем. На самом деле, есть еще шаг номер ноль, который, в принципе, очевиден. Вы можете просто
руками преобразовать ваши признаки, любым образом. Например, добавить там квадратичные признаки,
квадраты всех, попарные произведения, добавить какие-нибудь функции над ними, не знаю, там sinx
сквозь sinx, и так далее. Все это можно сделать для каждого элемента и получить новый признак.
SWM краски был хорош в свое время, потому что тогда выборки были маленькие, задач было не так много.
SWM позволяло найти решение до всей линей разрешимых задач. Сейчас по факту выбирать ядро вручную,
конечно же, боль, страдание и на огромных выборках не работает, потому что вам для этого ядра еще и
придется постоянно его считать под капотом. Скалярный произведение, как вы понимаете, обычное, два вектора
перемножить сильно быстрее работает, чем посчитать там экспоненту от какого-нибудь, не знаю там, нормы
разности. Поэтому здесь мы скорее с вами хотим обратить внимание на то, что мы с вами можем менять
наше признаковое пространство различными способами. Например, используя краски вот это самое новое ядро,
которое позволяет нам не само пространство поменять, мы даже явно не знаем, как оно выглядит, теперь новое
пространство. Мы говорим, что у нас новое ядро, определяя скалярное произведение новое, оно логично
его свойственно должно обладать. И тогда мы, соответственно, с вами получаем новое пространство,
которое как-то там может быть выражено. Мы об этом не задумываемся, мы говорим, вот,
скалярный произведение заменили, все остальное там пусть как-то индуцируется оттуда. Тут вопросы
пожелания есть? Живы, целы или совсем запутались с этими?
В новом пространстве будут.
Так, хорошо, еще вопросы? Как выбрать ядро? По-навучному. Изучаете структуру ваших данных, смотрите на
зависимость, пытаетесь положить их на какие-нибудь гиперповерсии, 1, 2, 3, 4, 5 порядка. На практике,
у вас есть выборка из семи ядер кросс-валидацией. Если у вас датсет размером миллион на миллион,
то берете под выборок штук 10, размером, не знаю, там, один процент, и на них выбираете,
потому что вы на миллион на миллион будете еще все это обучать достаточно долго и дорого.
Ну и на практике, как правило, берут просто-напросто модельку попроще, но при этом способную к
нелинейностям адаптируется самостоятельно, то же самое дерево или их ансамбль, та же самая
сеточка, какая-нибудь трех-четырехслойная, она вам каким-то неявным образом париметризует
гораздо более сложную гиперповерхность, но при этом минус в том, что вы не явно это делаете,
вы не можете сказать, что вот мы выбрали из таких соображений. Вот что там градиентный спуск нашел,
то и нашел. Свобода, но за нее мы расплачиваемся не интерпретируемой ситуацией. Это сейчас основная
проблема диплерринга. Ладно, собственно, это была во многом такая историческая справка, чтобы
показать вам, что разными путями шло развитие методов глубокого обучения и вообще машинного
обучения, и, во-вторых, что, на самом деле, машинное обучение, оно не стоит вот где-то вот особняком.
Мы, на самом деле, раньше показывали прям весь вот этот вывод, краски показывали, все свойства этих
ядер и так далее. Почему оно сейчас достаточно оперативно? То, что запоминало это обычно,
процента 2 понимало процента 1 от слушателей, а необходимость на практике это делать у процента
0, наверное, 0,05 присутствует, потому что СВМ, к сожалению, сейчас это красивая историческая
модель, которая была реально создана, была очень популярна и в том числе показала, что советская
российская школа в области машинного обучения ОГОГО. Долгие годы СВМ на всяких там хог гистограмм
ориентированных градиентов был одним из основных способов работать с изображениями, но времена
меняются. Сейчас СВМ это уже скорее красивый подход, который в некоторых задачах подходит, но
зачастую он бьется другими подходами, которые проще, понятнее и так далее. Но тем же самым
деревьями, собственно, закат СВМ в некотором смысле произошел в 2001 году, я опять же забегаю
немного вперед, когда Фридман представил свою GBM Gradient Boosting Machine, метод градиентного
бустинга. Когда градиентный бустинг появился, он без всякого подбора ядра стал нелинейной
гиперповерности опроксимировать гораздо лучше и без всяких там страданий и приседаний. СВМ сразу
же начал вдавать позиции. Ладно, и второй, собственно, момент, который хочется тоже сегодня разобрать
коротенько, это метод главных компонент. Сейчас у ужаса у нас будет аж вторая, по-моему, теорема,
которая есть в этом курсе. Первая была теория Магаусса Маркова. Кто помнит теория Магаусса Маркова?
А остальные, где были на лекции? Уже забыли? Ладно. Оптимальную средине смещенных оценку дает
вам минимизация МСЕ при условии, что у вас ошибка не смещенная, имеет конечную дисперсию и ошибки
между собой неискоррелированы. Вот. Классно. Ну а теперь давайте чуть-чуть поговорим про задачу снижения
размерности. Эта задача обучения без учителя, но, тем не менее, она очень широко применяется. И почему
PCA здесь вообще стоит, хотя мы вроде с учителем пока работаем, потому что PCA это линейный метод раз
и снижение размерности, в принципе, штука крайне важная и PCA, несмотря на то, что он простой линейный,
все еще один из наиболее широко употребимых методов снижения размерности, который вообще в
мире используется. Есть всякие там отэнкодеры, неявное снижение размерности и так далее. PCA простой,
прямолинейный и работает. За это его все любят. Более того, он как раз и полностью интерпретируем,
понятно, что происходит. Классно. Поехали. Зачастую у нас там десятки, сотни тысяч признаков есть,
нам с ними работать банально неудобно. Во-первых, потому что у нас скорость подсчетов все-таки
линейно к минимуму растет при увеличении размерности пространства. во-вторых,
потому что есть то самое проклятие размерности, где у вас количество равноудаленных точек растет
очень быстро при увеличении размерности пространства. Поэтому многомерных пространств у нас проблемы с
метрическими алгоритмами, например. Плюс у нас с вами и достаточно непонятно, как визуализировать
10-тыщмерное пространство. Вот нам хочется週тами на картинку посмотреть, что с ним делать, неясно. И
некоторые модельки на больших размерностях вообще не работают, а на малых достаточно неплохо debates
Но тот же самый КНН, если у вас размер пространства
миллион, скорее всего не заработает вообще.
На 10-мерном пространстве уже заработает.
Можем попытаться снижать.
Здесь можно вспомнить чуть-чуть, совсем чуть-чуть линал.
Вот помните, были разные матричные разложения.
Там УВ-разложения, разложения Халецкого.
Нет, сингулярное разложение.
Вот сингулярное, хотя бы помните, слава богу.
То есть, в общем случае, мы что хотим?
У нас есть матрица размером L на D.
В общем случае, это любая матрица.
В нашем конкретном случае, как вы понимаете,
это матрица объект-признак, она же матрица плана.
Мы хотим снизить размерность каким образом?
Мы хотим ее перевести к матрице L на K, на K на D.
Смотрите, у вас L на D и K на D.
У вас два члена.
Первая из них, это, собственно, теперь К-мерная матрица,
которая у вас исключительно, точнее, количество столбцов K.
Только K-признаков у вас осталось из D.
А вторая это уже матрица перехода.
Я ее, по сути, может потом выкину.
Потому что у вас матрица V изначально, она тоже скорее...
Короче, это уже транспонированная матрица нарисована.
Да, V это D на K, а вы транспонированный, это K на D.
Да и, в принципе, можете как угодно, главное,
чтобы у вас размерности впадали, у вас L на K,
на K на D должно быть, чтобы L на D получился.
Просто оригинальная запись, опять же, это U на V транспонированный.
Хорошо?
И, соответственно, задача, как правило,
когда мы ищем матричное разложение, матричную декомпозицию,
минимизировать какую-нибудь невязку.
Ну, как правило, это норма Фробениуса.
Что такое норма Фробениуса, все помнят?
Сумма квадратов отклонений по всем элементам.
По сути, мосье, но на матричке.
Вот, все, расписали.
Но, в общем случае, скажем так, разложений бывает много разных.
Сюда краски и разложений Халецкого,
и UV-разложений относятся, и сингулярные разложения,
и там еще десяток, наверное, вы в каких-нибудь учебниках
по линалу и по внезапно финансовому моделированию найдете.
Финансисты любят все эти разложения.
Мы же с вами давайте поговорим про сингулярное разложение.
Что такое сингулярное разложение?
Вот в двух словах.
Все помнят, или никто не помнит, или как?
А кто помнит?
К бою собственных векторов, в какой матрице?
Хорошо.
Какой вот там что-то про сопряженное было?
Какой, какой, самый сопряженный?
О, супер!
Классно.
А зачем нам А на сопряженную отмножать?
Бинго.
Смотрите.
Давайте тогда вспомним для начала.
Первое.
Мы с вами про разложение по собственным векторам.
Можем говорить для любой матрицы,
или у нас все-таки переход к байсу собственных векторов
не для всех матриц доступен?
Давайте так скажем.
У вас в байсе, который состоит из собственных векторов,
матрица какой вид имеет?
Диагональный.
Все ли матрицы можно привести к диагональному виду?
Какие нельзя?
Выраженные раз, а еще-таки?
Какие?
Слушайте, я вас не слышу.
Неквадратные.
Неквадратные, супер.
Если у вас матрица неквадратная,
то вы априори не сможете ее в диагональный вид привести
в то, что вас непонятно, что здесь нет.
На самом деле у вас там есть еще ограничение,
что она должна быть неотрицательной квадратической формой.
Короче, можно умножить матрицу на сопряженную.
Ту же самую матрицу.
Х, ТХ, например.
И краски.
Тогда у вас получится уже матрица,
которая явно диагональная,
соответственно к некоторой квадратической форме,
неотрицательной.
Поэтому ее можно разложить по собственным векторам.
Шаг номер два.
Как вы уже сказали, в байсе из собственных векторов
у нас матрица имеет диагональный вид, правильно?
А теперь тогда давайте вопрос.
Пусть у нас все собственные вектора теперь отнормированы,
мы же их можем отнормировать,
чтобы у них была одничная норма.
Тогда, соответственно, что у нас будет означать
каждый член вот на диагонале
этой самой матрицы
в байсе собственных векторов?
Чего?
Собственные значения. Классно.
Там диагональ ставит собственные значения.
Это вообще понятно? Нет? А то, кажется, у меня тут уже
выразилась какая-то группа из девяти человек,
с которыми я веду беседу. Я не хочу так делать,
я хочу всю аудиторию не терять.
Коллеги, вам там сзади понятно?
Хорошо.
Классно. Собственные значения, они
что-нибудь вам показывают, или это просто какие-то
волшебные числа, которые ни о чем нам больше не говорят?
Вот, еще раз погромче.
О, классно.
Классно.
Супер.
А это что не значит физически?
О, вот, замечательно.
Смотрите, мы с вами
можем вспомнить или разложение по собственным векторам,
или в принципе подумать,
если у нас матрица с вами
в байсе собственных векторов переходит к виду
диагональному, правильно?
А каждый вектор
теперь нормированный,
собственные числа,
которые стоят на диагонали,
должны явно показывать, насколько далеко
вдоль каждого из направлений у нас вытянута
наша матрица. Согласны?
Собственно, это на самом деле для нас очень важно.
Так вот, давайте-ка теперь
возьмем матричку и перейдем
к ее скалярному
разложению, ой, сингулярному,
разложению каким образом? Давайте матрицу
разложим на три вот таких вот
матрицы.
У нас будет У, Сигма и В транспонированы.
Что такое У, Сигма и В?
На самом деле У это будет
матрица артагональная,
Сигма это будет матрица диагональная,
а В опять же будет матрица
артагональная. Во-первых, что такое
артагональная матрица? Все помнят?
Что?
Не, У-то не квадрат.
Почему?
Что с ней не так?
Артагональная матрица не квадратная?
Да.
У вас всего лишь ограничение, что
У на У транспонированная должна
быть
единичной матрицей.
Ну, единичной в смысле
айдент и трансформ на диагонале.
Короче, ладно, кажется,
мы начинаем вас терять.
Давайте разложим матрицу на произведение трех.
Артагональная, диагональная, еще раз артагональная.
Хорошо?
В принципе, можем так сделать.
Тогда давайте сразу зададимся двумя вопросами.
Первое, раз артагональная матрица
артагональная, у нее детерминат какой?
Один. Все это понимают?
Точно?
Окей, согласен.
Если у нас ориентация не имеет,
всегда должен быть один.
Норм детермината точно единица.
А теперь можем вспомнить абсолютно простую
базовую вещь. Если у нас с вами матрица
имеет детерминат
равный единице,
то какое преобразование линейного пространства оно
сдает? Матрица это же при этом линейный оператор,
правильно? Поворот.
Только поворот, правильно?
Все с этим согласны?
Получается у нас с вами
линейное преобразование любое,
это поворот, растяжение, сжатие.
Если матрица артагональная, то она
сдает только поворот.
У нас две артагональных матрицы раз-два,
они сдают повороты.
Сигма у нас будет краской отвечать за растяжение сжатия.
В общем случае. Согласны?
Супер.
Ну и теперь давайте краске
попробуем понять, что происходит.
Что по факту мы пытаемся сделать?
Мы пытаемся найти такое
линейное отображение,
которое позволяет нам выразить
теперь все наши точки
в каком-то новом байсе
и при этом наша исходная матрица
может быть представлена как произведение трех.
Артагональная У, диагональная Сигма
и опять же артагональная В.
Мы можем это сделать
с помощью рингулярного разложения,
и тогда у нас матрицы У и В краски
порождаются откуда. Мы берем матрицу Х,
умножаем на сопряженную к самой себе,
получаем квадратичную форму,
ищем там собственные вектора
и собственные соответствия значения.
И оттуда мы к краске получаем
данное разложение.
В чем плюс на самом деле такого разложения?
Во-первых, есть замечательная
теорема. Сначала я вам скажу теорему,
потому что она красивая,
потом я скажу дровой смык, который под ней лежит.
Есть замечательная теорема Экарта Янга,
которая говорит, что вот такое разложение
краски СВД.
Если вы посмотрите на него внимательно,
вы поймете, что у вас первый к столбцов
из матрицы У, они соответствуют
только первым к элементам из матрицы Сигма
и первым к строкам
из матрицы В. Согласны?
Потому что у нас всегда умножается
строка на столбец.
Если вы хотите снизить размерность
вашего пространства, так как у вас
это исходная матрица, а это
ее представление уже
в коммерном подпространстве,
вы можете взять
миноры ранга К
из этих всех трех матриц, первый к столбцов,
первые к диагональных элементов
и первый к строк, и получить
коммерную аппроксимацию вашей исходной матрицы.
Согласились?
Это называют truncated SVD,
обрезанные SVD, по-русски, видимо.
И выкидываем все, кроме первых к членов.
А вот это я сейчас поправлю.
А теперь давайте вспомним.
Раз матрица артагональная, у нее все
векторы что?
Артагональны друг к другу.
Согласны? Все строки, которые у меня есть.
Или в данном случае все столбцы.
Раз так, значит
диагональная матрица. У нее тоже все
векторы артагональны друг к другу
по определению. Соответственно, третья матрица
опять-таки артагональна, все векторы опять-таки
артагональны друг к другу. Верно ли, что мы с вами
можем перетасовать векторы местами
в любом формате? В любом порядке,
лишь бы это во всех трех матрицах одновременно
происходило. Результат не поменяется,
правильно?
Все согласны?
Если я в матрице У
поменяю первый-второй вектор местами,
в матрице Сигма поменяю
первый-вторую элементу местами,
и здесь тоже поменяю местами, ничего не
поменяется. Верно?
Да, в исходное ничего не поменяется, конечно же,
речами на ней.
Классно.
А теперь давайте
пересортируем все наши
объекты каким образом? Все наши точки.
Так, чтобы у нас на диагонали
матрицы Сигма они строго не
возрастали.
То есть у нас слева сверху самые
большие элементы, справа снизу самые
маленькие. Хорошо?
Тогда мы с вами, соответственно, можем что сделать?
Чего?
По модулю, да. По абсолютному значению.
Вот. И тогда мы, соответственно,
видим, что у нас чем дальше
влез, чем ниже мы сюда переходим,
тем меньше у нас вот это самое
сингулярное значение находится.
Почему нам это важно? Но теперь мы с вами можем
отбросить все кроме первых
к, которые соответствуют максимальным
элементам. Правильно?
Здесь только что тут вспоминали про собственные
векторы, собственные числа, показывают нам, насколько
вдоль соответствующего вектора мы растянуты.
На самом деле, эти направления
не просто так там выбраны, они соответствуют максимальной
дисперсии в исходном презинговом пространстве.
А теперь давайте я вам покажу,
как это все на практике работает.
Ну, в смысле, что это такое в реальности.
Вот вам облакоточка. Логично, что все эти
точки мы можем с вами в матричку уложить.
Надо будет матричка там, не знаю, 100 на 2.
Я не знаю, сколько тут точек.
Мы с вами хотим описать теперь
все точки с помощью одной координаты.
И при этом минимизировать
норму фробениуса невязкие, то есть
сумму квадратов отклонений.
Теперь вы все точки
опроксимируете только одной чиселкой вместо двух.
И пытайтесь каким-то образом...
Опять же, эта одна
чиселка, она не обязана быть в исходных
координатах. Вы можете выбрать любое
направление, на которое вы это спроецируете.
И теперь у вас все точки лежат на этой прямой.
Вы должны это сделать так, чтобы ошибка была у вас
наименьшей. Вот глядя вот сюда,
где у вас будет прямая, на которую
надо все точки спроецировать, чтобы была наименьшая ошибка.
Чёрная дрель на стрелочке.
Большая полуость нашего эллипса, правильно?
Вы это сказали интуитивно.
Почему это так происходит?
Потому что у вас эллипс в эту сторону
растянут сильно, в эту сторону слабо.
Здесь у нас отклонения от
полуоси поменьше, значит должно быть лучше.
Правильно?
А теперь давайте это скажем более ямко.
Что такое норма фробениус вообще?
Это сумма квадратов отклонений, правильно?
Причём квадратов отклонений,
которые у нас есть.
Квадратов отклонений, правильно?
Причём квадратов отклонений от среднего.
А что такое квадрат отклонений
от среднего в общем случае?
На дисперсию очень похоже.
Это и есть дисперсия.
Вы берёте это, минус это
в квадрате. Это минус среднее в квадрате.
По сути у вас
главные компоненты всегда
являются направлением наибольшей
дисперсии в вашем облаке точек.
Это может быть абсолютно сложно, непонятно
облако точек, но во-первых, вы всегда
можете найти, в любом случае.
А во-вторых, вы можете найти вдоль какого направления у вас дисперсия
будет наибольшей.
Всё, раз вы нашли направление наибольшей дисперсии,
это ваш главный компонент.
После чего происходит что?
На самом деле, этому и соответствует
первый вектор вот из этой матрицы.
Он, кстати, отвечает за поворот в этом направлении.
А первая чиселка говорит, насколько большая дисперсия
у вас вдоль этого направления.
Всё.
Теперь вы говорите, что первый член у вас
вашего разложения отвечает
на направление наибольшей дисперсии.
Вы уже его полностью описали,
и теперь у вас остаются только невязки с этим представлением.
Что получается?
У вас остается какое-то ваше представление точек,
вы можете выбрать второе направление
наибольшей дисперсии.
Логично, что он должен быть ордигонально первому.
Опять же, описать все точки в его пространстве
это второй столбец, второе значение.
И так далее.
По сути, мед главный компонент говорит,
первые к главных компонент описывают первые к направлению
наибольшей дисперсии,
поэтому он дает вам наименьшую ошибку
в камерном пространстве
по нормам Фрабидиуса. Логично.
Мы первые к компонент использовали, чтобы
забрать первые к направлению, которые дают
наибольшую ошибку при восстановлении.
Логично, что если мы наибольшую ошибку покрыли,
все остальные представления будут давать ошибку
как минимум...
Скажем так, наибольшую ошибку мы покрыли,
значит, если бы мы кого-то из них не использовали,
ошибка была бы еще больше.
Вот.
Никаких.
Они...
Ну, смотрите, тут возникает два вопроса.
Первый, когда у вас есть направление наибольшей дисперсии
единственное, оно всегда,
если оно единственное, то это есть главный компонент.
Второй, если у вас есть несколько
одинаковых направлений, ну, простейший случай
это центральная симметрия, представь себе
равномерно заполненный точками шар.
У вас любое направление наибольшей дисперсии.
Ну, в таком случае вы можете
выбрать любое из этих направлений,
а все остальные должны быть не мордбинальны.
То есть, у вас нет никакого ограничения,
если у вас несколько направлений
наибольшей дисперсии,
то вас устраивает любое.
Если они все одинаковые дисперсии имеют,
значит, любое не лучше, чем остальные.
Все.
Вот.
Ну и, собственно, эта теорема, на самом деле, называется
теорема Экарта-Янга и говорит на простое.
Если вы хотите построить
аппроксимацию матрицы
другой матрицы, но ранга К,
то оптимальную
аппроксимацию, с точки зрения
наименьшей нормы Фрагениуса,
среди линейной краски дает вам
вот это вот самое PCA,
которое строится через скалярное продолжение.
Все.
Смотрите, у вас есть исходная матрица А,
вот, исходная матрица А.
Да, просто матрица объект Призник.
Вы говорите, я теперь хочу описать
свои данные только камерным
представлением, то есть не L признаков,
а K признаков.
Отверждение, что если вы эту новую матрицу
построите вот таким образом, как
построите U, Сигма,
СВД-разложение оригинальной матрицы,
а потом возьмете только первые K
столбцов из У, первые
коэлементов из Сигмы, короче, обрежете
все кроме матрицы ранга К
и их между собой точно так же перемянуете.
Что такое обрежете? Вы их, на самом деле,
можете оставить, просто-напросто
и все остальное нулями забить, и все.
То же самое получится.
То у вас получится матрица АК,
которая уже будет ранга К, она будет только камерная.
И при этом вы потеряете
наименьшее количество информации относительно
оригинальной вашей матрики.
Вот, смотрите, что такое. АК – это ваша
аппроксимация ранга К через СВД.
Для любой другой
аппроксимации ранга К
верно, что разница
между А и БК всегда
не меньше, чем разница между
А и АК. То есть АК – это
наилучшая аппроксимация с точки зрения нормы фробениз.
Нормы фробениз – сумма квадратов отклонений.
Ну, у вас две матрицы взяли по элементам вычисления.
Теперь у вас каждая есть невязка,
вы завели в квадрат, потому что вас абсолютное значение
волнует, но в квадрате. Вот.
Вот, все.
Ну, смотрите, вы просто можете, если у вас
упорядочно все, то первые K
сингулярно значения им соответствуют
то, что вам надо. Все остальное вы просто заменяете нулями.
Ну, заменяете нулями, потому что иначе
у вас размеры с матриц будут не совпадать, и у вас
начнется проблема. По факту мы понимаем,
что если у нас там стоит 0, все артагонально,
мы можем ничего дальше не считать, дальше
просто будут нулевые столцы. Логично?
Хорошо.
И опять же, это
на практике через год, два, три
многие начинают забывать, как это формально звучит,
неформально. Мед главных компонент
ищет направление наибольшей дисперсии.
Я думаю, это понятно. Вот где у вас больше всего
точки шумят, это то направление,
вдоль которого вам нужно явно больше сил,
чтобы их различать между собой.
Грубо говоря, если вы замените всю координату
на одно значение, вы получите больше штраф
за это. Вот здесь, допустим,
вот оно направление, вдоль него
у нас больше всего шум, это и есть направление
наибольшей дисперсии. И опять же,
рубрика вопроса собеседования, если таких
направлений несколько, это нормально,
так бывает, тогда у вас главные компоненты
неоднозначно определены.
На самом деле, даже здесь они у вас неоднозначно
определены, вы можете совершенно спокойно
все повернуть на 180 градусов,
вот у вас будут две главные компоненты абсолютно
такие же, только направлены в другую сторону.
Ничего не меняйте. Но с точностью до
смены направлений, главные компоненты
определены, если у вас все направления имеют
разную дисперсию. Тогда все окей.
А?
Погодите, у вас матрица
это краски множества
векторов для точек.
Для одной точки оно не имеет
смысла.
Это матрица
сотни на два, потому что у вас две координаты.
То есть у вас была матрица
100 на два, вы говорите
хочу одну координату, тогда у вас
останется матрица 100 на один,
где у вас краски будет известно, что вдоль вот этого
вектора координата нужная вам
лежит, и вы знаете чему оно равно.
На всякий случай еще раз.
И с ней, по сути, говорить вам,
что вы проецируетесь на некоторое
линейное подпространство. У вас было
изначальное пространство размером l, вы выбрали
k признаков, на камерное подпространство
спроецировались.
Причем краски вам и показывают, что
главное число, точнее
главная компонента обладает соответствующим
вектором, который говорит куда, и
числом, который говорит насколько там большая дисперсия.
Так как все матрицы u и v артагональны,
вы их можете совершенно спокойно менять местами,
также меняя местами в том же порядке
векторы в матрице sigma и v.
Потому что если мы упрячим матрицу sigma
по убыванию, по невозрастанию
сингулярных значений,
то первые k значения соответствуют
первым k направлениям с максимальной дисперсией.
Нам просто так удобно.
Смотрите,
сингулярное значение говорит насколько
большая дисперсия.
Чем больше сингулярное значение, тем больше дисперсия.
Мы хотим
в учебнике линалу
за первый курс.
Как это дело выбирать?
На практике вы можете взять
и построить вот такой вот график.
Это либо дисперсия для
каждой из компонент.
Как правило, там есть какой-нибудь редкий переход.
Называют метод складного ножа
иногда почему-то. Вы смотрите,
у вас дисперсия для каждой из компонент,
потом в какой-то момент она остается слишком маленькой.
Допустим, здесь мы видим, что
первые m компонент покрывают
почти всю дисперсию в нашей выборке,
все остальные имеют крайне
малое значение дисперсии
или относительно дисперсии.
Можно всю дисперсию посчитать
и потом в долях от нее считать.
Соответственно, как выбирать
количество компонентов?
Либо оно у вас сверху зафиксировано
кем-то там, либо вы выбираете
то количество компонентов, которое оптимально
для вашей конкретной задачи.
Но опять же стоит помнить, что
ваше количество компонентов,
независимо от того, сколько оно,
вы по-другому.
Несмотря на то, сколько компонентов вы выбираете,
если вы взяли не все компоненты,
значит вы уже взяли какое-то
линейное подпространство, которое все
спроецировали, но при этом вы учили
только линейные, грубо говоря, взаимосвязи
между признаками. Вы часть информации
могли совершенно спокойно потерять.
Вас от этого никто не застрахует.
Если вы теряете, выкидывает часть информации,
вы выкидываете. Но эта штука
снижает пространство размером с
линейным образом. Например, есть
простенький вариант,
где это не будет работать.
Давайте я вам нарисую.
Нет, спасибо.
Условно, у вас данные могут лежать, например,
на какой-нибудь вот такой вот
поверхности.
Но можете себе еще
в трехмерном пространстве.
Согласитесь, что у нас данные лежат
на одной гиперповерхности,
которая просто вот так рулоном закручена.
На одной кривой в данном случае.
И вам одной координаты по этой кривой
уже достаточно было бы, чтобы все точки отделить.
PCA не способен это сделать.
PCA вам найдет что-нибудь типа
вот так вот тогда. Давай всех спроецируем.
И вот куча ошибок у вас будет
вот здесь везде понатыкана.
Согласны?
Помните, что PCA штука линейная.
И теперь самый важный
комментарий. Он там уже проскакивал.
Но как вы думаете, можно просто взять выборку
и ее засунуть в PCA?
Можно попробовать, согласен?
Отнормировать, конечно.
Если у вас данные не отнормированные,
то у вас точно также будут считаться
расстояния в вашем исходном пространстве,
как и в КНН-е том же самом.
Если у вас разные признаки
в разных шкалах,
то вы получите, собственно,
чем больше шкала, тем больше дисперсии
по этому направлению.
Поэтому если вы не отнормируете ваши данные,
вы можете либо это сделать, если вы четко понимаете,
что данные нормировать не надо,
и вы не представляют каким-то смыслом важным,
его надо учесть именно в PCA,
либо вы себе все сломаете.
Я вас неспроста об этом предупреждаю,
я не буду говорить прямо, но, пожалуйста,
нормируйте данные перед PCA, вам скоро это очень сильно понадобится.
Надеюсь, в этом году это услышат больше,
чем обычно людей,
потому что регулярно при
рассмотрении того, что вы там
понасчитали, не обращаясь мне к кому конкретно напрямую,
но регулярно это ошибка номер
один, наверное, который мы наблюдаем
в первой половине курса.
Вот, собственно,
нормируйте данные перед PCA,
пожалуйста. И, соответственно, снижаем
размерность. Сигма K это наше
камерное представление.
Если мы хотим вернуться обратно,
то вот наш будет х чертой,
у на сигма, на v это
матрица обратного перехода.
Ну и вот вам маленький пример,
допустим, библиотека Eigenfaces,
просто различные изображения,
я не знаю, видно ли вам там подписи,
на всякий случай, вы на этих фотографиях
видите хоть кого-то знакомого?
Кого?
О, ну слушайте, классно!
Огонь!
Ну, собственно, да, вон там еще Шварценеггер сидит,
Билл Гейс, по идее,
и так далее.
Логично, что мы
же с вами можем матричку...
Это же матричка, правильно?
Причем черно-белая изображение,
и эта матричка просто-напросто
размером там, не знаю, 64 на 64.
Можем матричку через
SVD тоже самое пытаться разложить?
Можем. Ну вот берем
топ-16 компонент, получаем вот такое.
Страшновато.
Вот берем 50 компонент,
заметьте, уже начинают прорисовываться
различные, скажем так,
черты лица, но они в среднем все равно
ориентированы как-то по центру.
Ой, слушайте, я боюсь наврать,
но раз у нас 250 компонентов,
так что минимум 260 должно быть сверху.
Ну, типа того, да.
На самом деле, в вашем случае
абсолютно неважно, у вас будет
по строкам или полстопцам в данном случае,
потому что матрица, она и есть матрица.
У вас ранг матрицы от транспонирования
не меняется.
Еще раз, у вас есть изначальная матрица,
вы говорите, я хочу
взять эту матрицу, описать ее
в каком-то линейном подпространстве,
допустим, в терминах всего 18-16 компонент,
а потом на основании этих 16 компонентов
вернуться в исходное пространство.
Да, то есть мы аппроксимируем матрицу
в виде 16, у нас была матрица
условно 250 на 250,
мы говорим, на самом деле у меня будет
матрица 250 на 16,
а потом я вернусь обратно 250 на 250.
Нет, да?
Мне кажется,
да, хотя
глядя на вот эту картинку,
у меня ощущение, что их
всех между собой правда перемешали,
тогда у вас была матрица размером
количества картинок на изображение.
Честно, конкретно в данном случае
не знаю, в семинаре мы будем
с одним изображением по отдельности работать.
Вот.
Смотрите, у вас
х изначально это у на сигма
на в транспонированное,
если хотите взять первые всего лишь
к штук, вы берете у к сигма к
в транспонированное к,
то есть первые к элементов из каждой матрицы соответственно.
В данном случае
наверное да.
Я почему говорю наверное, потому что у меня
в голове уже перепутано с этим семинаром,
по отдельности как матрицу рассматривать,
а здесь судя по тому, что у нас
ориентация, видите, у нас все лица почему-то
ориентированные по центру в среднем,
так что видимо их правда вытянули в вектор
и уложили просто в одну матрицу.
Вот.
Хорошо.
Ну что ж.
Ну и в принципе
на этом
лекция у нас закончилась.
Перерыв 15 минут.

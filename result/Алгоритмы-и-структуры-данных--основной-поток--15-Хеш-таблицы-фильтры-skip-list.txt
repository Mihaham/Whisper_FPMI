Вот, про то, насколько эффективно работает. Давайте напишем определение.
Пусть у нас есть опять какое-то семейство хэш-функций, я напоминаю, что в качестве хэш-функций h,
там h1, h2 в случае двойного хэширования, я использую не абы какую случайную функцию,
а из какого-то маленького семейства, который я умею хорошо сэмплировать.
То есть это небольшое семейство функций, которые ведут себя почти так же, как настоящие, случайные,
но которые я умею быстро, эффективно вычислять.
Так вот, пусть у меня есть какое-то семейство хэш-функций, давайте пускать h с индексом s,
это действующие, скажем, из zp в zm, индексированные с-ками.
Такое семейство хэш-функций называется k-независимым,
k-независимое семейство хэш-функций,
если при фиксации любых k элементов из zp, то есть из области определения,
при фиксации любых k их образов, вероятность того, что эти элементы попали ровно в эти образы,
то есть вероятность того, что x1 пришло в y1, x2 в y2 и так далее, xk в yk,
вероятность такая, как если бы эти события были независимы.
Давайте это запишем, для любых попарно различных x1 и так далее, xk, лежащих в zp,
для любых y1 и так далее, yk, лежащих в zm, вероятность по выбору случайного s события,
что хэш-функция в точке x1 равна y1, хэш-функция в точке x2 равна y2 и так далее,
хэш-функция в точке xk равна yk, равна такой величине, как если бы все вот эти вот события проходили независимо.
То есть, грубо говоря, мы а6 от x1 равно y1, это просто один эксперимент.
Мы взяли случайную хэш-функцию, померили, с какой вероятностью в такой-то точке это y1,
потом взяли новую хэш-функцию, померили вероятность, с которой в точке x2 она равна y2 и так далее.
Как если бы все эти события были независимы, какой была бы здесь величина вероятности?
Я верю в вас, вы сможете.
Если бы вот этого не было, если бы у меня была просто вероятность одного вот такого события,
что h в такой-то точке равна такой-то величине, чему должна быть равна эта вероятность,
если у нас все равномерно случайно распределено, если все исходы равновероятны?
Да, 1 делит на m в нашем случае. Если у меня h6 бьет в множество размеров m, было бы 1 mt.
Ну а поскольку теперь у меня k таких событий, я хочу в данных k-точках, чтобы у меня были данные k значений,
если оно ведет себя как независимое, то это просто 1 mt умножается сама на себя k раз, получается 1 на m вкатый.
Это определение. Ещё раз, смысл такой, что тут написано k-независимое, это значит, что значения в k-точках
ведут себя так, как если бы h была настоящая случайно, то есть эти события были бы независимы.
Ну и пример, как можно генерировать что-то, скажем так, похожее на k-независимые семейства hash функций,
ну давайте скажем следующее, что h от x можно брать следующей штучкой.
a0 плюс a1x плюс и так далее. То есть я рассматриваю вот такой вот многочлен от x,
и дальше, как обычно, беру его по модулю p, и потом ещё раз беру по модулю m.
Значит, где h-ки это случайные числа из zp.
То есть в прошлый раз у нас была такая штука, только у нас была линейная вот здесь функция.
Здесь был не многочлен к этой степени, а просто только вот эти два слагаемые a0 и a1x.
Я просто беру и расширяю эту штуку на многочлен k минус первой степени,
соответственно, здесь ровно k неизвестных коэффициентов, ну не неизвестных, а случайных.
И, собственно, можно примерно таким же аргументом показать, что вот это вот событие будет происходить примерно с такой вероятностью.
То есть если раньше у нас было всего лишь две неизвестные h-ки, а 0, a1x мы их складывали,
брали по модулю p, брали по модулю m, теперь мы берём просто k таких вот величин,
ну и можно опять-таки показать, что вероятность будет примерно вот такая.
Ну и здесь используется, например, такой факт, что если у вас есть какой-то многочлен на zp,
и вам известно его значение в таких-то ка-точках, что по сути, что такое hs?
Это многочлен k минус первой степени от x.
Вот этот многочлен, он должен быть в такой точке равен такой-б �о величине,
в такой точке должен быть такой-б этой величине и так далее.
У вас есть k значений.
Ну понятно, что многочлен k минус первой степени однозначно восстанавливается по своим k точкам.
Если нам известны значения mans в ка-точках, то есть нам известны k точек с значениями в этих точках,
跅тн многочленy на значении восстанавливается.
Например, там параболGen1 , нсо Mayo 4 albo..?
Ну и так далее.
многосчетинка минус первой степени по k точку. Это значит, что так же, как и раньше,
если бы мы не брали вот этот процент p, тогда вот эта вещь, грубо говоря, равномерно перемешивает
все zp. То есть вероятность, если бы мы не брали по модулю m, то когда я навешиваю х-функцию на x1,
на x2 и так далее, на xk, то я получаю независимые значения в zp. Потому что любой набор значений
я могу получить с фиксированной вероятностью, потому что многочлен дозначно восстанавливается.
И когда я беру по модулю m, там надо опять что-то то же самое сделать, что если у меня все значения
перемешались случайно, равновероятно, то с какой вероятностью они совпали по модулю m?
Ну вот, с такой-то примером. Ну что, нормально? Хорошо. Ну так вот, теперь если мы знаем,
что такое независимое семейство и знаем, как их строить, то есть теорема, которая говорит,
что при использовании достаточно сильных независимых семейств, вот эта штука работает
за единицу. Значит, во-первых, если h это семейство, так как я это назвал, к-независимо,
ну пусть будет 5 независимых семейств х-функций, то при использовании линейного пробирования
асимптотика ответного запроса будет единичка. При использовании линейного пробирования
ответный запрос, как обычно, амортизированная единичка в среднем. Амортизированная потому,
что иногда вызывается rehash, и у нас мне нужно как бы за линию все старые элементы переложить
в массив вдвое большего размера. Ну в среднем, потому что здесь мотожидание, поскольку все
случайно, я хэш-функцию выбрал, у меня там мотожидание какое-то, ну в общем, все в среднем
работает. Вот, а если использовать двойное хэширование, то достаточно два независимости.
Значит, если h всего лишь два независимая семейства, то при двойном хэшировании тоже
будет единичка. Такая же учетная единичка в среднем. Вот, значит, более того, смотрите,
ну понятно, два независимая – это хорошее, да вот то же, что было в прошлый раз, это скажем
ax плюс b по моделю p по моделю m на вот это вот два независимая. Мы в прошлый раз заказали,
что универсально, но там два независимости тоже на самом деле выполняются. Вот, получается,
если вы используете двойное хэширование, то есть шаг у вас не плюс один, плюс один, плюс один,
а плюс h2 от x, плюс h2 от x и так далее по циклу, тогда вам достаточно использовать ту же самую
хэш-функцию, которую мы рассматривали в прошлый раз. Значит, если мы хотим использовать линейное
пробирование, оно в каком-то смысле может быть чуть лучше, потому что с точки зрения кэша у вас
будет эффективнее, потому что вы двигаетесь только по нескольким подряд идущим ячейкам,
ну и память так работает, что если вы посмотрели вот сюда, то посмотреть сюда проще всего. Как бы
следующую ячейку смотреть очень просто, в отличие от ячейки, которая где-то далеко. Просто обращение
к памяти так устроено, что близлежащие элементы проще всего подгружать. Вот, поэтому с какой
точки зрения это может быть даже эффективнее, но формально, если вы здесь используете что-нибудь
типа там 4 независимое или 2, ну короче менее независимое что-то, тогда как бы, ну на самом деле
там оценка может вырождаться во что-то более худшее, там в логарифм или вообще в корень может
выродиться, но обычно это не происходит. Если вы используете линейное пробирование и пишете
два независимые семейства, скорее всего тоже будет все хорошо. Если еще rehash почаще делать,
то будет вообще замечательно, потому что очень вряд ли кто-то будет строить такой набор тестов,
ну как обычно, чтобы заваливать два независимые семейства, но чтобы 5 независимые проходили.
Это вряд ли, но с точки зрения теории, опять-таки формально лучше всего использовать 5 независимые,
если меньше, то там уже будет формально долго работать, но как получится, короче.
Ну хорошо, значит следующий небольшой сюжет это про экономию памяти. Вот представьте, тут как бы
я всегда, вот везде не явно предполагал, что элементы, которые добавляю в множество, это какие-то
числа, соответственно, которые я могу эффективно хранить. Я говорю, что у меня универсум это zp или
вложено в zp и там число это у меня int или long, long, ну короче что-то маленькое. Теперь представьте
более реалистичную ситуацию, что вам нужно хранить какие-то более сложные объекты, ну там, не знаю,
гиперссылки на сайты или какие-нибудь структуры, на описание какого-нибудь человека, историю его
каких-нибудь заказов в магазине, ну короче какую-то сложную структуру вам надо хранить в таблице.
Тогда вам не хотелось бы, чтобы каждый элемент вашего массива содержал вот такую большую структуру
данных, ну какой-то большой объект, где много информации, которая тяжело класть, тяжело копировать,
тяжело сравнивать друг с другом. Такого бы делать не хотелось. В таком случае, если у вас объекты
тяжелые, можно делать что-то другое. Например, фильтр Блума. Давайте это тоже запишем, что преимущество
здесь такое будет. Экономия памяти. Если объекты из универсума, откуда собственно черпаются все наши
элементы тяжелые. Тут сделаем следующий прикол. Давайте заведем массив булевский,
просто из 0 единиц. Не массив там чисел, а массив булей. Длины m равной n, где n это количество
элементов. Здесь считаем следующее, что у нас заранее известны все элементы, которые кладутся в
структуру. Они сначала положились, а потом поступают только запросы типа find. Нам изначально
известно количество инсертов. У нас будут только запросы типа insert, которых будет не очень много,
их суммарно будет n. И будут запросы типа find. Причем мы позволим себе иногда ошибаться. В целях
экономии памяти мы позволим себе иногда возвращать неправильный ответ. Ну вот как-то так получается,
что если вы хотите сильно сэкономить в одном месте, то, возможно, вам придется чем-то пожертвовать.
Но давайте нам здесь придется пожертвовать корректностью иногда. Вот эта штука find будет
работать следующим образом. Find x всегда говорит либо да, либо нет. Нашла она x в структуре или нет.
Давайте сделаем следующее. Давайте скажем, что если она говорит нет, то это точный правильный ответ.
Это правильный ответ. А если она говорит да, то это скорее всего правильный ответ. А именно это
правильный ответ с вероятностью, ну давайте какого-нибудь там, 99 процентов. С вероятностью примерно 99 процентов.
Ну вроде нормально. В одном проценте случаев мы будем ошибаться. Кажется не очень критично,
но опять-таки с точки зрения какой-то практической реализации иногда ошибаться не страшно, скажем так.
Вот. Ну и на самом деле вот эту константу можно сколь угодно улучшать. Там можно сделать
ее сколь угодно близкой к единице. Это я чуть-чуть позже еще пропишу. То есть получается, что у нас
бывают иногда неправильные ответы да. Это так называемое false positive. То есть нам ложно сказали,
что да. Нам ложно сказали положительный ответ. Ложно положительное срабатывание произошло.
Давайте сразу формализую. Давайте скажу, чтобы правильный ответ у нас был с вероятностью 1
минус Эпсилон. Где Эпсилон, что-то маленькое. Тогда, значит, идея. Смотрите, вот у меня есть большой мой
массив. Булевский массив. Давайте скажем, что в нем изначально все нули. Дальше давайте возьмем
к хэш функций. Ну из какого-нибудь там опять хорошего достаточно семейства. И на запрос insert
x давайте посчитаем от х все вот эти хэш функции. И во всех этих точках поставим единички. Значит,
формально для каждого и от 1 до k. Вот если это массив a был, то давайте я скажу a с индексом h
it от x равно единице. Ну а то есть вот у меня пришел элемент x. Я считаю от него значение всех этих
хэш функций. Все они ведут его в какие-то там конкретные ячейки. Вот я в них ставлю единицы.
Тогда давайте поймем, как работает find. Ну вот если до этого find когда-то пришел insert,
то понятно, что во всех вот этих вот ячейках, ошитая от x, везде стоит единица. Ну и можно
надеяться, что если у меня h достаточно случайно все перемешивает, то можно надеяться, что в другом
случае такого произойти не могло. Ну давайте просто так и сделаем. Давайте пройдемся по всем
этим ячейкам, проверим, что a во всех этих ячейках единица. Если нет, то x точно не было, его раньше
точно не могло добавиться, потому что иначе во всех этих ячейках должна была бы уйти единица. А если
везде единица, то давайте скажем, что ну x скорее всего в таблице есть. Давайте опять посчитаем все эти
хэш функции. Если внезапно a h it от x равно нулю, тогда return false. Ну return no давайте напишу,
что x в структуре точно нет. А иначе, если во всех этих точках была единичка, давайте вернем yes.
Весь фильтр Блума.
Ну а то есть еще раз смотрите, у меня есть, ну там давайте считать, достаточно большой массив вот
здесь вот. Чтобы вставить элемент, я считаю от него в каком-то смысле k случайных каких-то статистик,
k каких-то случайных функций и вставляю единички в те места, от которых я посчитал. Тогда ну как бы
хотелось бы верить, что вот этот набор единичек, он однозначно соответствует x. Ну это там более-менее
правдоподобное предположение. Если мы знаем, что коллизии в хэш функции происходит довольно редко,
а мы как раз выбираем такое место хэш функции, чтобы коллизии там было мало. Ну и тогда как раз вот эти
единички, они скорее всего там более-менее однозначно соответствуют x. Ну или по крайней мере вероятность того,
что есть какой-то другой y с таким же набором хэш функции. Она там маленькая какая-то. Ну и поэтому
давайте просто пройдемся по этим ячейкам, проверим, что здесь везде единицы. Если единицы, то x скорее
всего был, потому что ну а кто еще мог все эти единички туда поставить. Если хотя бы одна из этих
ячеек нулевая, то x точно не было. Если мы вернем no, то это точно верно. Потому что если бы x был,
то он бы во все эти ячейки поставил единицу. Ну как бы хорошо, конечно, но иногда мы можем
ошибаться. Потому что я вот говорю, что скорее всего вот эти вот единички однозначно задают x,
но как бы даже если так, то возможно эти единички, возможно пришли из каких-то разных других там y
e и z. Например могло быть такое, что вот эти вот две ячка поставил какой-то y, а эти две ячка
поставил какой-то z. Вполне могло быть такое, что какие-то две хэш-функции от y это вот эти
две единички какие-то две другие хэш-функции от другого z это вот эти две. То есть мы опять мы
покрыли вот те самые позиции, которые соответствуют как бы  lowering и с помощью других элементов на
потенциально такое бывает. Но мы считаем, что довольно редко. Надеемся на то, что это редко
Вот. Сейчас все пропишем.
Идея понятна, как реализовывать. Тут прям очень мало кода,
буквально на строчке 10. Да, длины такой же, сколько всего будет элементов.
Он всегда размера m, но в этом случае берем m равная n, сколько будет элементов в нём всего.
Вот. Значит утверждается, что оптимальнее всего выбрать следующие параметры. Сейчас,
пардон. Это я, кстати, обманываю, что m должно быть равно n, оно должно быть чуть больше. Виноват.
Да, обманул, извините. Давайте я это сотру. Чуть-чуть побольше надо будет, все-таки виноват.
Значит k, нужно брать оптимальное k, равно, ну давайте напишем двоичный алгорифм,
1 делит на епсилон, m равно, так, n, давайте в натуральных напишу, значит логарифм 1 делит на
епсилон делить на квадрат логарифма 2. Вот так вот. Правильно? Да, вроде правильно. Вот, ну то есть это
можно тоже сгруппировать в логарифм 1 делит на епсилон по основанию 2, но будет ещё 2 в знаменателе,
поэтому оставлю так. Вот, утверждается, если подобрать такие параметры, то будет как раз
из комы заявлена вероятность ошибки примерно 1, точнее примерно епсилон. И, ну и соответственно память
у меня, смотрите, память у меня будет линейная по n умножить на логарифм вероятности ошибки. Вот,
довольно хорошо. Значит, давайте поймём немножко интуитивно, откуда это берётся. Ну, смотрите,
давайте опять погрузимся в идеальный мир, где мы считаем, что у меня h это не х-функция из
какого-то маленького семейства функций, а настоящие случайные функции, которые на каждом
х принимают независимое значение во всём вот этом вот ZM. Значение любого числа от 0 до и минус 1.
Значит, тогда с какой вероятностью я вот здесь могу ошибиться?
Значит, давайте сначала скажем следующее. Давайте представим, что у меня давайте
простое, ну давайте типа доказательства, там на самом деле просто идея, откуда эти
константы, ну там можно вывести или почему они такие, примерно. Значит, понятно, что в среднем у
нас единичек будет ну что-то типа nk. Да, ну то есть, по крайней мере, вот если у меня n элементов
суммарно добавится, то каждый из них какие-то k элементов заединичит, ну и нам хотелось бы,
чтобы, то есть, примерно nk единичек я поставлю. Возможно, какие-то из них склеятся, но там их
будет точно не больше, чем nk. Значит, единиц проставим не больше, чем nk. Вот, ну и мы хотим,
наверное, чтобы опять, чтобы у нас размер х-таблицы был примерно в два раза больше,
чем число элементов, ну потому что нам хочется, чтобы был не очень большой, чтобы память сэкономить,
но при этом не очень маленький, чтобы не было сильно много коллизий. Поэтому нам хотелось бы,
чтобы m было приближительно 2 nk, а в два раза больше, чем единичек. Вот, тогда, если у нас,
смотрите, если у нас каждая вот эта вот штука, это какое-то независимое случайное число от 0
до m-1 и при этом у нас единичек примерно половина, то вероятность того, что данный какой-то y,
сейчас, момент, ну хочется, например, на следующее, что вероятность того, что h1 от x,
давайте так, а h1 от x и так далее, а hkt от x все будут единицами в отсутствие x,
в отсутствие x, это примерно 2 в степени минус k. Ну потому что если у меня nk единиц,
а длина массива 2 nk, то есть у меня примерно nk единиц, примерно nk нулей, значит тогда вероятность,
что в данных конкретных k позициях стоят единицы примерно как раз такая, то есть вот у вас есть
такой большой массив длины 2 nk, у вас здесь примерно половина нулей, половина единицы,
но если вы фиксируете данное множество позиций и спрашиваете, с какой вероятностью здесь и здесь
единицы, ну понятно, если у вас примерно поровнули единиц, то вероятность того, что здесь единица
это 1 вторая, вероятность того, что здесь единица 1 вторая и так далее. Это очень на самом деле не
формально, это очень грубая оценка, потому что конечно эти события зависимые, но вот типа и так
сойдет. Чтобы понять, по крайней мере, откуда это берется, тут такой грубой оценки хватает.
Еще раз, если нулей половина, единиц половина, то вероятность того, что здесь единица, это 1
вторая, потому что случайная позиция, нулей единиц поровну, вероятность того, что здесь единица 1
вторая. Ну и соответственно, если я хочу, чтобы в к позициях были единицы, то вероятность этого 2
в минус катых. Значит вероятность вот этого вот false позитива, вероятность вернуть, вероятность
увидеть здесь везде единицы даже несмотря на то, что фикса не было, это 2 в минус катых. Ну и вот если 2
в минус катых, это как раз одна эпселоновая, точнее просто эпселон, вероятность ошибки была эпселон.
Вот, ну и тогда как раз вот это вот, это вот, это вот. Давайте про логарифмируем, например,
поставим 2. У меня получится минус k равно лог 2 эпселон. Дальше умножаю обе части на минус 1,
получается k равно лог 2 1 директной эпселон. Собственно, откуда это берется примерно.
Вот. Ну и дальше, если m подставить вот такое, то, кажется, ровно это и получится. Ну или там
примерно так. Окей? Вот. Все, то есть, собственно, если это было непонятно, неважно, вот просто есть
какие-то магические каты такие, что если мы хотим ошибаться с вероятностью эпселон, то берем такое
k. Если у нас всего будет n вставок, то берем такое m. Если мы не знаем, какой n, мы можем, как обычно,
делать rehash. Если нам неизвестно n заранее, мы можем сначала выделить какой-то небольшой массив,
туда повставлять, когда элементов стало слишком много, когда единичек стало слишком много,
скажем, больше половины. Тогда мы можем опять m вдвое увеличить и все старые элементы перенести
в новую табличку. Вот. Ну а после этого, если такие константы фиксированы, то вот, буквально,
там 10 строчек и очень часто правильно отвечаем. Разве что с вероятностью эпселон можем ошибиться.
Еще раз? Да, кстати, хороший вопрос, откуда взялся натуральный.
Один тогда. Я могу распределить, что отношение этих двух логарифмов это двоичный логарифм,
но еще остается натуральный логарифм двойки. Да, справедливо. Ну, там будет другая константа.
Да. Окей. Ну, значит, это хорошая оценка только для k. Вот. M получается немножко другим. Сейчас
я подумаю. 2nk. Ну да, потому что это не очень формальный анализ. Справедливое замечание.
Но берется именно такое обычно. Так, еще вопросы, может? Хорошо. Так, значит, зачем это может быть,
например, нужно? Казалось бы, зачем нам ошибаться? Ну, давайте два примера. Значит, во-первых,
это, скажем, база каких-нибудь вредоносных сайтов. Ну, там, не знаю, пусть вы какой-нибудь Google,
вам поступают жалобы от пользователя на какие-то сайты, вы проверяете, да, там реально какой-нибудь,
не знаю, малейшее софтвое раздается, что-нибудь плохое, нехороший сайт. Вот вы хотите их все
добавлять в какую-то структуру, так чтобы, если пользователь пытается зайти на этот сайт или
если он попадается ему в поиске, то вы ему даете предупреждение, там, на этот сайт было много жалоб,
будьте внимательны, не скачивайте отсюда ничего. Вполне себе такая нормальная постановка. Вот.
Значит, давайте заведем две структуры. Сначала давайте заведем одну большую базу данных для
хранения полностью вот этих вот сайтов. Ну, там, можно опять-таки это сделать как хэштаблицу.
Давайте я не буду специфицировать, просто открою, что это база данных. База данных всех плохих сайтов.
Всех плохих сайтов. Прям полностью, да, то есть каждый элемент это одна большая строка там,
такое-то название сайта. Полностью название, да, без сокращений, полностью вся информация в сайте.
Вот. И давайте дальше сделаем небольшую к ней дополнительную структурку, которая будет как
раз являться фильтром Блума. Фильтр Блума. Вот. И давайте все элементы из этой большой базы
добавим в этот маленький фильтр. Это нам очень сильно сэкономит память. То есть у нас есть какая-то
большая громоздкая структура. Ну, мы там где-то ее храним где-нибудь далеко на каком-нибудь удаленном
хранилище. Вот она большая, с ней часто работать не хочется. Давайте мы ее немножечко сожмем,
точнее даже сильно сожмем. Потому что здесь каждый элемент это большая какая-то длинная строка,
она занимает много памяти. Вместо этого мы заведем маленький вот такой массив булевский и все
элементы оттуда сложим в этот маленький массив с помощью фильтра Блума. И тогда дальше, если какой-то
пользователь заходит на какой-то сайт, мы хотим проверить, а лежит ли этот сайт вот в этом
множестве большом. Ну, давайте вместо того, чтобы обращаться сразу непосредственно к этой огромной
базе, в которой все запросы долго работают, грубо говоря, давайте мы сначала посмотрим на фильтр Блума,
лежит ли тот сайт в множестве вредоносных с точки зрения фильтра Блума. Эта штука работает быстро,
а здесь там какой-то небольшой булевский массив, вам надо посчитать всего к хэш функции и посмотреть
на какие-то каечеек этого массива. Это сильно быстрее, чем искать в этой большой базе данную
строчку, проверять на равенстве и так далее. Это сильно эффективнее. Но давайте посмотрим,
есть ли в этом множество строка S. Если фильтр сказал нет, то это сайт хороший, точно причем.
Мы знаем, что все плохие у нас вот здесь, значит и вот здесь. Если фильтр сказал нет, значит сайта
точно нет в множестве плохих. И это отвечает за быстро. Причем это вроде бы и хорошо, потому что у нас
скорее всего таких плохих сайтов мало, по крайней мере пользователь, когда заходит на сайт, они
обычно хорошие, большая их часть. И тогда, соответственно, мы будем часто получать нет, и это будет
работать быстро, то есть мы не будем делать запрос к огромной долгой базе данных. Если нет, то хорошо.
А если да, то это либо реально какой-то плохой сайт, либо это нормальный сайт. Ну и мы, наверное,
не хотим, чтобы пользователю говорили, что хороший сайт плохой. Тогда чтобы уточнить, если
нам сказали да, давайте обратимся сюда и посмотрим, есть ли этот сайт здесь. Поскольку это происходит,
мы считаем, что это происходит довольно редко, плохих сайтов мало, и пользователи на них редко
заходят. Если с вывело да, то давайте еще обратимся сюда и спросим, есть ли с здесь. Это работает долго,
но редко, поэтому мы выиграли. Нам сюда приходится обращаться редко за счет вот этого небольшого
фильтра, который на все отвечает быстро. Ну и второй пример. Кратенько давайте скажу,
например, словарь. Представьте, что вы пишете какие-нибудь тексты и вы хотите подчеркивать
неправильно написанные слова. То есть у вас там есть слова русского языка, например, вы хотите
все эти слова добавить в базу данных, потом, когда пользователь что-то печатает, вы хотите
подчеркивать ему ошибки. Вот, ну опять же, есть у вас большая данных, большая структура, это словарь.
И дальше вы на нем строите небольшой фильтр и потом, когда пользователь что-то вбивает
какое-нибудь слово, вы обращаетесь просто к фильтру, смотрите такое слово есть в словаре, да или нет.
Если нет, значит слово точно плохое, его нет в этом словаре, значит оно точно плохое, пользователь
точно опечатался, давайте подчеркнем ему красненько и скажем, что вот здесь у тебя скорее всего ошибка,
точнее, точно ошибка. Вот, а если фильтр сказал да, ну давайте, можно даже не обращаться к словарю,
можно просто сказать, окей, это хорошее слово, даже если типа мы ошиблись с маленькой вероятностью,
то там, окей, пользователь опечатался, мы не нашли эту опечатку, это происходит редко, не очень страшно,
мало опечаток в тексте не сильно критично. То есть многие опечатки мы увидим, потому что их нет в фильтре,
какие-то не увидим, ну и ладно. Ну вот, как-то так, окей? Хорошо. Так, дальше еще одна модификация с похожей
концепцией того, что мы можем иногда ошибаться, это называется фильтр кукушки. Вот, значит здесь опять
идеология такая же, что мы хотим экономить память, мы не хотим хранить все эти объекты в явном виде,
потому что они могут быть какие-то тяжелые, большие, сложные, мы не хотим их всех хранить. Вместо
булевского массива теперь давайте мы все эти элементы как-нибудь эффективно захэшируем,
то есть каждую там строчку, например, название сайта отобразим в что-нибудь небольшое, ну в
какую-нибудь маленькую структуру, маленькое число. То есть, во-первых, мы введем, давайте скажем,
какую-то функцию, которая все элементы универсума отображает в какую-нибудь маленькую, ну давайте
пусть будет zk. Вот это первая внешняя хэш-функция. И теперь вместо элементов будем хранить значение
хэш-функции от них. Вместо элемента x храним f от x. То есть вместо элемента храним его хэш-значение.
Сам элемент вообще нигде не храним. То есть, если нам пришла команда вот такую длинную строчку
вставить в структуру, я такой, да-да, хорошо, я сначала посчитаю хэш, какое-то маленькое число,
и дальше я в структуру добавлю вот это маленькое число f от x. Какое-то число вот отсюда. То, что
какие-то разные строки могут получить одно и то же хэш-значение, мне не очень страшно. Да, такое
бывает, но редко, вероятность маленькая. Мы можем ошибаться иногда. У меня бывают коллизии, это не страшно.
Вот. Дальше. Соответственно вместо x мы будем хранить всегда его хэш-значение. Дальше давайте
для каждого хэш-значения, для каждого вот этого вот ключа, давайте определим две возможные позиции,
куда его можно было бы поломить в нашем массиве. Давайте я, ну, после будет какой-нибудь y.
Давайте для каждого y определим две возможные позиции массива, где он может лежать. Вот y может
попасть либо сюда, в клетку с номером h, пусть будет 1 от x. Ну ладно, нет, не хочу h1, давайте
просто h от x будет. Вторая клетка, где он может лежать, это будет, нет, нет, нет, нет, тут хитрее, виноват.
У меня же еще преимущество, что когда я кладу x, я знаю, я могу от него убрать хэш-функцию,
то есть, когда мне прошел x, я могу до добавления посчитать в него еще другую хэш-функцию. Так вот,
значит, когда у меня появился x, я могу его положить в клетку с номером h от x, либо в клетку с
номером, давайте вот так вот напишем, h от x xor g от f от x. Вот, где h и g это тоже какие-то хэш-функции.
xor по битовой. Вот, значит, тем самым для каждого конкретного элемента x из универсума, у меня
есть всего две позиции, где он может лежать, либо вот здесь вот в клетке h от x, либо в клетке с
номером h от x, ну короче, какая другая функция. Они там как-то быстро считаются, у меня есть две
конкретные позиции. Тогда, если мне пришел find, то я могу на него легко ответить. Мне говорят find x.
Я смотрю в эти две клетки, смотрю, есть ли там f от x, хотя бы в одну из них. Если есть, то я говорю,
что элемент присутствует, иначе говорю, что не присутствует. То есть, вот у меня есть две
возможные клетки, я кладу в одну из них f от x. Куда хочу, туда и кладу. Давайте запишем, что
элементу x соответствуют клетке h от x и h от x xor g от f от x. В любую из них кладем f от x.
То есть, как обычно, я не хочу хранить самый элемент x, потому что это длинная строка какая-то.
Вместо этого я храню от нее какое-то х значение. То есть, у меня есть две возможные позиции, я
смотрю какая из них пустая и кладу туда f от x. Тогда find работает очень просто. Если мне пришел
find к какой-нибудь y, то я считаю h от y, я считаю h от y xor, ну как обычно, я считаю те две возможные
позиции, где он может лежать и спрашиваю, есть ли хотя бы в одной из этих двух клеток f от y.
А есть ли тут f от y.
Также просто работает erase. Если мне говорят сделать erase y, то у меня есть
всего две возможные клетки, где лежит y. Либо вот здесь, либо вот здесь. Давайте я их обе
посмотрю. Если в одной из них написано f от y, давайте f от y сотру и скажу, что там ничего не лежит.
Опять смотрим на те же две клетки, смотрим на две клетки. Если в одной из них лежит f от y,
то мы его стираем. Мы хотим удалить y, мы вместо y удаляем hash от f от y, потому что сам y я
нигде не храню, я могу только hash от f хранить. Давайте найдем f от y, если где-то лежит, то удалим.
Это было find и erase. Что делать с insert? Insert работает чуть посложнее, но интуитивно
предсказуемо. У меня есть две позиции, куда я хочу положить что-то, и пусть они обе заняты.
Но давайте в одну из них положим, а то, что я вытеснил, переместим в другую возможную для
него. То есть пусть был массив. Я для x посчитал две возможные клетки, куда можно было его положить,
точнее не его, а f от x. Пусть тут занято, пусть тут лежала f от t, пусть тут лежала f от z.
Дальше давайте сделаем следующее. Пусть x вытесняет вот эту штуку, то есть вместо
вот этого я сюда записываю f от x, и я вытеснил сейчас значение f от z. Но мне нужно его тогда
опять куда-то поместить. Раз я его вытеснил, мне нужно обратно его поместить в мою структуру.
Куда его надо поместить? Я утверждаю, что я могу однозначно понять, в какую клетку его нужно
положить. Потому что вот это была какая клетка? Это была либо hash от z, то есть вот эта клетка,
где раньше лежал f от z. Это либо hash от z, либо hash от z xor g от f от z. Ну тогда другая,
давайте я там назову клетку i, тогда другая клетка, где может лежать f от y, это xor g от f от z.
Так вот, мне только его и надо. В этом и прикол. Да, в этом и прикол выбора ровно таких двух хэш
функций, что, зная значение одной, вы однозначно можете понять значение другое, зная только f.
Сам элемент вам знать не надо, вам надо знать только f от него. Ну смотрите, вот если пусть f от z
лежало вот здесь, в позиции hash от z, тогда чтобы получить соседнюю двойственную к нему позицию,
вам нужно это hash от z xor-ить с g от f от z. Ровно это там и написано. И наоборот, если вот эта
позиция, где раньше лежало f от z, то чтобы попасть сюда, вам нужно этот xor аннигилировать,
но xor, если вы два раза с одним и тем же словом проксорите, вы получите то, что было изначально.
Потому что a xor b xor b это a. Два раза xor сам себя убивает. Поэтому, если вы лежали здесь,
то чтобы попасть сюда, в другую позицию, где может лежать z, вам нужно просто вот это отбросить,
это значит xor-ить с g от f от z. Ну все, мы тогда f от z вытеснили, мы знаем тогда другую позицию,
куда он обязан попасть. Но давайте его сюда поместим. Если здесь опять было занято, то я
соответственно сюда f от z поместил и вытеснил отсюда какой-то f от y. Тогда опять я для него знаю
другую позицию, куда он должен был поместиться, давайте его сюда положим. Он возможно что-то еще
вытеснил какой-то там f от u. Тогда мы знаем, куда нужно положить f от u, ну и так далее. К сожалению, да.
Вот, и здесь лучшее решение зацикливаний такое. Давайте, скажем, сделаем достаточно много шагов,
много это 20, например, или там 10. Если мы в течение десяти раз перекладывали, то значит,
наверное, мы зациклились, ну или как бы 10, все равно уже много. Мы хотим, чтобы у нас все работало как
бы за единичку, ну там, амортизировано в среднем и так далее. Но если мы сделали 10 таких перекладываний,
то уже, наверное, что-то плохо. Либо табличка переполненная, либо мы ушли в какой-то цикл,
ну короче, что-то нехорошее. Но у нас есть универсальный рецепт на такие случаи. Давайте
rehash сделаем. Давайте возьмем табличку, если надо, в два раза ее расширим. Возьмем новую hash
функцию. Ну f менять нельзя, потому что у нас лежат вот эти hash значения, ну можно там поменять hg
и все переложить в новую hash табличку в два раза большего размера. То есть если делаем слишком
много перекладываний, то делаем rehash. Делаем слишком много перекладываний. Перекладываний,
ну скажем там 10. А то запускаем rehash. Вот такая идея. Ну как, понятно? Ну круто. Почему это кукушка?
Ну потому что говорят, что кукушки так с яйцами в гнездах поступают, они куда-то прилетают,
кладут в гнездо свои яйца, и другие соответственно вынуждают другую кукушку переместить своих
детенышей куда-то в другое место. Ну это такая легенда говорят. Ну а так чего, мы добились примерно
того же, смотрите. То есть мы сэкономили память, то есть вместо самих строк храним какие-то
hash значения от них, что-то более маленькое. Соответственно у нас памяти сильно меньше, чем если
мы хранили все элементы. При этом, если элемент в структуре есть, то мы его точно найдем. То есть
если у нас х добавлялся, то мы его точно найдем. Мы скажем yes, потому что хотя бы в одной из двух
клеток будет написано f от x. А если элемента нет, то вероятность того, что мы скажем да,
ложно положительное срабатывание, вероятность неправильного ответа да, она маленькая. Ну потому
что это вероятность того, что нашелся другой элемент с тем же hash, который еще и лежит в одной
из вот этих вот двух клеток, это маловероятное событие. То есть вам нужно, чтобы f у них совпали,
чтобы f от x равно было f от y для какого-то другого элемента y. И еще чтобы у них вот эти вот две
ячейки тоже совпали. Одна из них должна совпасть с одной из ячейок для y. Это вероятность дважды
маловероятная, что во-первых у вас f совпало, во-вторых у вас hash совпало, грубо говоря. Ну поэтому
типа работает хорошо. Опять у нас бывают false positives, но с маленькой вероятностью. Здесь каких-то
конкретных оценок я не знаю, но опять же на практике можно поэкспериментировать, да, там какой
размер подобрать. Ну размер надо опять наверное брать порядка в два раза большего числа элементов,
вставляемых в структуру. Ну и там чем, чем меньше вы хотите вероятность ошибки, тем опять же больше
надо расширять ваш массив. Вот такие дела. Так, что по времени? Хорошо. Ну все тогда, значит hash
таблицы мы закончили. Последняя структура у нас это skip list. Это как-то переводится на русский,
типа список с пропусками, но я буду говорить skip list, потому что это быстрее. Значит,
что она умеет? Это на самом деле в каком-то смысле вариация дерева поиска. Давайте так
очень грубо скажу, это вариация дерева поиска. То есть это структура, которая опять-таки позволяет
вам хранить какое-то множество элементов и обрабатывать запросы к нему как обычно. У нас insert,
erase, find. Вот. И еще запросы, которые используют порядок на множестве U. Значит, запросы использующие
порядок. Порядок на U. Чем hash таблицы хуже деревьев поиска? С точки зрения симптотики ничем,
даже сильно лучше, но они очень сильно забивают на порядок элементов, которые были на U. Скажем,
если вы вставили 5 и 10, то они могут находиться в произвольных местах hash таблицы. И, ну скажем,
вот если вас спрашивают, там, следующее за пятеркой число, например, у вас там вставлялись какие-то
числа, были 5 и 10, вам надо узнать какое минимальное число больше чем 5. Вот скажем, таблицы такого
эффективно реализовать нельзя. Вам в худшем случае надо всю таблицу пройти в поисках минимального числа
больше чем 5. Вот. А деревья поиска это умеют эффективно отвечать. То есть все запросы, связанные с
порядком элементов, там, меньше, больше, сколько элементов меньше и так далее, это дерево поиска
легко обрабатывает. Ну, собственно, скиплист тоже одна из вариаций, которая умеет поддерживать какую-то
структуру на элементах, какой-то порядок на элементах. Вот. И здесь порядок, на самом деле,
будет прям явно выписан. Значит, давайте какой-нибудь пример. А у нас будет такая
многоуровневая структура. На нижнем уровне будет просто односвязанный, отсортированный список
всех элементов. Давайте я вот так вот нарисую. У меня будут фиктивные элементы слева и справа,
минус бесконечность плюс бесконечность. Давайте напишем минус бесконечность. Дальше у меня будет
просто односвязанный список, который поддерживает все числа изец в отсортированном порядке. Ну,
давайте какой-нибудь пример. 2, 5, 7, 10, 13, 20. Вот и все заканчивается фиктивным элементом плюс
бесконечность. То есть на нижнем уровне у меня отсортированный список всех элементов S. Вот все
вот это вот, это S, и больше ничего в S нет. То есть я храню по факту просто отсорченную версию S.
Дальше. Это у меня нижний уровень, первый уровень. Дальше, на втором уровне я хочу сделать список
с пропусками. Я хочу построить тот же самый список, но выкинуть из него примерно половину
элементов. Ну, давайте я нарисую красивую картинку. Я выкидываю каждый второй элемент.
Значит у меня минус бесконечность, плюс бесконечность остаются, крайний элемент у меня всегда есть. Вот.
Давайте построим такой же список с выкидыванием каждого второго элемента. Соответственно двойку
я игнорирую, здесь пропускаю, но пишу здесь пятерку. Семерку пропускаю, пишу десятку. Тринадцатку
пропускаю, пишу двадцатку. Ну и получается у меня вот такой вот список. Такой односвязанный список.
Причем я как бы не просто какие-то элементы удаляю, какие-то оставляю. Я еще и как бы запоминаю,
что эта штука ссылается вот сюда. То есть на самом деле у меня для этих элементов будет еще ссылочка
вниз. То есть указать не только вправо на следующий элемент того же списка, но еще и на соответствующий
ему элемент списка на уровень ниже. То есть вот эта пятерка ссылается на эту пятерку, десятка на
десятку, двадцатка на двадцатку. Ну и здесь тоже будут такие ссылочки вниз. Вот дальше еще один
уровень выше. Это тоже будет какой-то подсписок вот этого списка 5, 10, 20, из которого выкинута
примерно половина элементов. Ну давайте я там опять что-нибудь нарисую. Скажем пятерку я оставлю,
десятку удалю и двадцатку тоже оставлю. Вот так вот мне захотелось. Здесь такие ссылки, здесь такие
ссылки. И как обычно не забываем, из каждого элемента верхнего слоя проводим стрелку в элемент
нижнего слоя, который ему соответствует. Ну и давайте последний у меня будет уровень, где хранится
например только двадцать. Это будет аж вот такой вот список, всего лишь с одним содержательным
элементом двадцать. Все, вот этот скип-лист. Еще раз, на нижнем уровне у меня просто отсороченный
список элементов S, а на каждом более высоком, то есть при переходе с ИТ-го уровня на И плюс первой,
у меня остается примерно половина элементов из тех, что были на ИТ-ом уровне. То есть при переходе
вверх я примерно половину отбрасываю и оставляю под множество у тех, которые остались в том же
порядке. То есть я завожу новый список на этом под множестве, связываю их вот в такую цепочку и
еще при этом ссылаешь на то, кто откуда пришел. То есть что эта пятерка соответствовала этой пятерке,
это десять это вот этот элемент, двадцать вот этот элемент, плюс бесконечность это вот этот
элемент. То есть у меня есть ссылки не только вправо, но и вниз. Ну давайте, например, обсудим как
теперь делать find. Давайте попробуем реализовать find какой-нибудь. Что здесь интересно? Ну find
десять давайте попробуем. Find у нас всегда будет начинаться вот из этой вот клетки, самой левой,
самой верхней. Смотрите, skip list нужен для того, чтобы использоваться вот эти пропуски. То есть,
например, смотрите, вот если бы у меня, например, find был бы числа, который скажем 30, то я мог бы
сразу смело отсюда перепрыгнуть вот сюда вот и соответственно все, что между ними лежало,
если бы у меня число было больше чем 20, я мог бы весь этот кусок памяти сразу не просматривать. Все
вот эти элементы я пропустил, я стоял бы сразу здесь. Но поскольку 20 больше чем 10, то я такой
прыжок совершить не могу, значит я как бы в этом списке, ну как бы все исчерпал, я не могу идти
вправо в этом списке, поэтому я иду вниз, спускаюсь сюда. Теперь могу ли я отсюда переместиться сюда?
Могу ли я пропустить вот этот кусок массива? Да, могу, потому что 5 меньше чем 10, и если 10
где-то в массиве есть, то точно правее этой пятерки. Поэтому все вот это я могу скипнуть и перепрыгнуть
отсюда вот сюда. Теперь то же самое, могу ли я продолжиться направо в этом списке? Могу ли я перейти
в 20? Ну не могу, потому что 20 слишком большое, значит здесь я пропускать это не могу, я спускаюсь
просто вниз. Ну и вот уже отсюда я вижу, из пятерки вижу 10, вот она десятка, я ее нашел.
Вот и так собственно будет у меня работать find. Я сначала встаю сюда и пока могу, прыгаю вправо,
то есть пока могу, делаю самые большие возможные вот эти вот скачки, и если я такой скачок сделал,
то я действительно ничего не потерял. Я перехожу в число больше либо равное, чем то, что я ищу,
навсегда прыжок, perdон, в число меньше либо равное, чем то, что я ищу, но если я сделал прыжок,
то мое число, вот это вот которое я ищу, лежит где-то справа, то есть я такой прыжок точно могу
сделать. А если я его не делал, то я иду вниз, то есть я спускаюсь в список чуть-чуть более
подробный, где больше элементов находится, и уже делаю прыжки там. Вот такая идея. Так,
давайте что-нибудь напишу. Какие-нибудь буквы. Так, значит храним несколько связных списков,
несколько отсортированных связных списков. Как работает find? Встаем в левую верхнюю точку,
то есть встаем в начало самого верхнего списка. Дальше, пока можем двигаемся вправо,
если не можем, прыгаем вниз. То есть пока можем, двигаемся вправо, пока можем, то есть пока я могу
сделать такой прыжок без потери х, то есть пока ключ, лежащий в следующей ячейке, меньше либо равен х.
Если не можем, двигаемся вниз. Если не можем, спускаемся вниз. Ну, если нашли х,
останавливаемся, говорим, что х найден. Так, понятно, как find работает? Ну и более-менее тогда
понятно, что это работает типа за что-то похожее на логарифм. Потому что если у меня на каждом
уровне, как бы при подъеме снизу вверх, если у меня на каждом уровне число элементов вдвое
примерно уменьшается, то у меня количество этих скип-листов примерно логарифм. На раз в два раза,
на двойку каждый раз делится количество. Вот. Ну и почему скачков будет логарифм?
Ну понятно, более-менее. Давайте подумаем.
То есть смотрите, в идеальном мире, если бы у меня реально каждый второй элемент был бы на
следующем уровне, то здесь все, здесь каждый чётный, здесь каждый с номером, делящимся на 4,
так далее, то по факту то, что я делаю, – это поиск, это по факту смотри на двоичную запись
моего числа, потому что вот, как бы, ну прыжок, то есть смотрите, прыжок на нижнем уровне – это
плюс один к номеру, плюс один, плюс один, плюс один. Прыжок на этом уровне – это плюс два.他们
на этом уровне, плюс 4, плюс 8 и так далее. Значит соответственно, если я бошел сверху вниз,
то я по факту просто раскладываю моё число в двойечную систему дел Mag Jing и делаю те скачки,
которые нужны. То есть если, скажем, мне нужно, там, не знаю, 18 число, я сначала сделал прыжок
плюс 16, потом спустился вниз, сделал прыжок плюс 2. Соответственно, прыжков будет столько,
какая степень двойки максимально не происходящая n. Понятна идея? Вот, ну все, значит, ответ
примерно за алгоритм. Вот, хорошо. Значит, это с файндом разобрались. Значит, что делать с инсертом?
Давайте с рейзом сначала, с рейзом проще. Давайте сначала запустим файнд, потом, значит, мы найдем
элемент, который надо удалить. Вот, скажем, в этом случае, если бы у меня был rs10, я бы его нашел,
причем я бы не просто его нашел, а нашел бы самое верхнее его вхождение вот этого числа. А то есть,
ну, как раз где эта десятка входит на самом верхнем уровне. Я до сего дошел, мне нужно его удалить.
Давайте его удалим из этого списка, а я умею удалять из списка. Я знаю предыдущий, я знаю следующий,
это стираю и эту стрелку объединяю. Удалять из списка мы умеем. Значит, это удалил, спустился вниз,
это надо удалить. Я знаю предыдущего, я знаю следующего, стер, стрелки перепровел. Поэтому
rs это просто файнд плюс спуск вниз и удаление из списков. Значит, rsx это файнд плюс, значит,
а затем удаление из списков, из списков и спуск вниз, и спуск до самого низа, до самого низа.
Да, видимо, да, видимо скорее да, потому что мне нужно знать. Сейчас, давайте подумаем, надо ли это.
То есть, казалось бы, можно было бы и без этого, наверное. Но зачем?
Загрузили.
Из односвязного? Ну, вот, кажется, не умеем. То есть, если я знаю элемент,
надо знать предыдущий в любом случае. Смотрите, вот, да, вот на самом деле, как бы, вот в этом случае,
смотрите, я когда дошел до этой десятки, я пришел в нее отсюда, поэтому я знаю предыдущий. Поэтому
я могу эту стрелку просто перепривязать. Значит, когда я спустился вниз, на самом деле, видимо,
ну, в нормальный, если бы мы хранили односвязанный список, мне нужно было бы и вот отсюда тоже
спуститься и дойти до этой десятки через эту семерку, соответственно, я бы знал предыдущий.
Тогда мне не нужно было бы хранить указатели назад, и асимплотик бы не испортился. Но давайте,
да, давайте я для удобства скажу, что у меня есть указатели в обе стороны, соответственно, я знаю,
кто у меня был слева, я знаю, кто у меня лежит справа, я могу это сократить. Вот, но можно,
значит, я утверждаю, что можно оптимизировать так, чтобы хранить только именно односвязанный список,
только вправо. Вот, но это тонкость. Давайте для удобства считать, что есть стрелка в обе стороны.
Так, значит, кроме этого и Рейс понятно, как делать? Хорошо, значит, теперь инсерт остался.
Значит, с инсертом идеологически все очень похоже, значит, я встал сюда,
спустился вниз, прыгнул, спустился вниз, прыгнул, прыгнул еще раз, спустился спустился, прыгнул,
спустился и говорю, что вот здесь должен быть x, то есть у меня, скажем, здесь число меньше x,
здесь число больше x, вот, значит, сюда его должен вставить. Вот. Ну, понятно, в этот список я его
как-нибудь могу вставить, это просто. Я знаю предыдущий, я знаю следующий, и сюда его вставляю.
Теперь идея такая. Мне хотелось бы, чтобы у меня на каждом уровне было в 2 меньше, ну как бы,
на следующем уровне был вдвоем меньше элементов, чем на предыдущем. Надо что-то сделать с x.
Давайте сделаем, не думая. Давайте скажем, что мы подбросим монетку, если выпало единичкой,
то я его добавлю вот сюда. Если нолик, то не добавляю, он просто здесь. Если я его сюда добавил,
опять подбрасываю монетку, если выпало единичка, я опять x поднимаю на уровень вверх и добавляю его
вот сюда, в список. Список на один повыше. Опять бросаю монетку, если да, добавил сюда. Дожидаюсь
выпадения нуля. Если здесь выпала монетка нулем, то прекращаю на верхней иду. Все. Ну, смотрите,
мы пока здесь шли, мы можем запомнить, какие последние элементы были на каждом шаге. Вот это,
вот это, вот это, вот это, вот это и вот это. Я здесь добавляю вот такую стрелку в этот x,
дальше откатываюсь назад в рекурсии, помню, что я отсюда, ну как бы здесь был последним,
поэтому здесь добавил x. Здесь отсюда провожу стрелку, здесь отсюда провожу стрелку. Нет
проблем. Ну, соответственно, они также все связаны вот в такой вертикальный список.
Значит, как работает insert? Insert x. Сначала вставляем на нижнем уровне, вставляем на нижнем уровне.
Дальше давайте напишу так, пока монетка выпадает единицей, пока монетка выпадает единицей,
добавляем x в список на уровень повыше. Добавляем x в список на уровень выше.
Вот. Ну и тогда как раз мы добьемся того, что хотели, что у меня на нижнем уровне будет
все элементы храниться, вообще все элементы множества. На втором снизу будет примерно половина,
да, потому что для скольких элементов вот здесь вот монетка выпала единицей, ну для половины
примерно, половину нули, половину единицы. Поэтому на втором уровне примерно n пополам
элементов. Здесь примерно n на 4, потому что чтобы элемент попал сюда, мне нужно, чтобы два раза
выбросшая монетка выпала единицей. Это происходит в разности у 1 четвертая. Поэтому здесь будет
примерно четверть всех элементов. Здесь, соответственно, еще в два раза меньше 1 восьмая, ну и так далее.
То есть теперь у меня уже будет неточная оценка, да, не всегда на этом уровне n делить на 2 в степени
элементов, а примерно, ну в среднем там как раз столько элементов. Там математическое ожидание
что элементов будет такое как надо. Вот, ну и соответственно можно сказать, что при такой реализации
все запросы работают за логарифом в среднем. Отвержение. Все запросы обрабатываются
за логарифом в среднем. Вот. Кажется нет. Вы про то, что не амортизирован или?
У нас, по сути, все оценки не от текущего количества элементов, а от максимального количества элементов,
которое было сделано по памяти. Разве? Нет. У нас выразится все много списков, в которых будет один элемент.
А почему один элемент в них будет? Ну, по удалению все элементы примерно одного, а в какой-то момент их было дофига.
Не, ну так у меня высота каждой башни, для каждого икса, количество списков, в которых она есть, оно не зависит от остальных.
Там то, что мы их добавляли, удаляли, у меня высота не меняется от этого. Каждая конкретная башня, она не зависит от остальных.
Поэтому то, что вы остальные удаляли, оно не влияет на высоту икса. И она такая же, как если бы он был один вообще.
Вы про то, что не будет ли, скажем, вот такого, сейчас, вы про вот такую картинку, когда здесь минус бесконечность, здесь плюс бесконечность?
Да, может быть столбисно так сами.
Еще раз, если у вас x, а, окей. Ну, короче, нет, подождите.
Я понял. Это справедливое замечание. Можно сказать тогда, что если у меня есть верхний, то есть смотрите, если у меня есть пустой список вот такой, который ведет из минус бесконечности в плюс бесконечность,
его можно удалить. Понятно, этот список можно отбросить и сократить всю высоту на один.
Тогда, если у вас один х какой-то был высокий, то он был высокий сам по себе. И он был высокий независимо от того, что здесь были какие-то элементы, здесь были какие-то элементы.
Поэтому от того, что вы их вставили или удалили, у вас мотожидание высоты этой башни будет все равно единица. И, соответственно, и память будет тоже единица в среднем.
Если вы как раз удалите все эти заведомо ненужные штуки крайние, тогда будет без лишней поправки.
Я поверю в мотожидание, потому что в связи с удалинками двадцатки был столбец у четырех элементов.
Да, но еще раз, смотрите, я когда вставлял двадцатку, двадцатка высокая не потому что... А, я понял, я понял, про что вы говорите.
Еще раз? Это сложно проверять, да, это очень сложно проверять.
Это не страшно. Я понял, давайте я так скажу. Я когда вот здесь проговаривал, как я строю скиплист, я говорил, что у меня здесь много n элементов, здесь n пополам, здесь n на четыре.
Я каждый второй добавлял. На самом деле я так не делаю. На самом деле я для каждого элемента подбрасываю монетку и добавляю на уровень выше, если выпало единица.
Вот, поэтому эта двадцатка, она бы была, ну типа, то есть в этом случае она обязательно будет высоты четыре, потому что я каждый второй добавлял.
А в нормальном случае она бы была высоты в среднем один. Вот. Тогда как раз таких бы проблем не было.
Вот. Да.
Еще раз?
Да, да, да, возможно. То есть если х очень высокий, скажем, вот до сюда долез, то нам придется добавить новый список. Вполне возможно.
Еще раз?
Это все можно не думать. Все происходит настолько редко, что это не повлияет ни на что. Вот если вы просто в тупую бросаете монетку, то у вас не будет таких очень высоких башен.
Ну, типа, бывают редкие логарифмические, но больше логарифма не будет.
Так, хорошо. Последнее, что я здесь хочу сказать, это, например, про, собственно, про порядок.
А, нет, сейчас, более важная вещь. Зачем я это вообще рассказал? Зачем нам это, если у нас есть деревья поиска? Пока что никакого выигрыша нет.
Зачем это? Бонус этой штуки в том, давайте замечание, скип-лист хорошо распараллеливается.
Что это значит? Вот представьте, что у вас есть одна большая структура, и у вас есть много процессоров, которые готовы обрабатывать запросы, поступающие к этой структуре.
Не знаю, у вас какая-то большая база данных, и у вас много компьютеров, готовые отвечать на запросы.
Тогда эта штука хорошо параллелится в том смысле, что мы можем отдавать запросы разным процессорам, разным компьютерам,
и они могут почти независимо друг от друга на все отвечать.
Ну, например, вот представьте, что у вас пришел блок из десяти инсертов. Там инсерт х, инсерт у и так далее. Пришло куча инсертов.
Давайте мы все эти инсерты считали, раздали десяти компьютерам, и они параллельно независимо это делают.
Если каждый работает за логарифом, тогда время будет не десять логарифом, а один логарифом.
Если они все параллельно работают, то время не умножается, а распараллеливается в каком-то смысле.
И скип-лист идеально подходит на то, чтобы распараллелиться.
Потому что смотрите, в каждый момент времени, по факту, что мне делает инсерт?
Он как-то там ходит, ходит, ходит. Вот здесь, давайте здесь.
Шли, шли, шли. Потом взяли и сюда добавили х.
Потом поднялись и сюда добавили х и так далее.
Но главное, что в каждый момент времени у вас будет корректный скип-лист.
То есть пока у вас, скажем, один процессор спускается сюда, вставил сюда один х,
другой в каком-то другом месте спустился еще на последний уровень, добавил у.
Потом этот сюда поднялся, добавил сюда х, тот куда-то еще поднялся, тоже добавил у.
Каждая такая процедура, каждая вставка не меняет корректности скип-листа.
То есть, скажем, если бы у меня этого всего не было, это был бы тоже корректный скип-лист.
Потом я поднимаюсь на уровень вверх, этот добавляю, это будет тоже корректный скип-лист и так далее.
Поэтому, если у меня несколько конкурирующих процессоров друг с другом соревнуются и делают разные процедуры, то в любой момент времени все хорошо, у меня корректный скип-лист, поэтому я могу над ним независимо делать вот эти операции.
Вставлять сюда, туда, сюда, увеличивать эту башню и так далее. Я все это распараллеливаю, они друг с другом не конфликтуют.
В отличие, скажем, от дерева поиска какого-нибудь. Потому что если вы в дерево поиска хотите что-то вставить, удалить, вы сначала спускаетесь, потом делаете повороты.
Вы делаете этот поворот, когда вы повернули, у вас, возможно, какой-то элемент изданного по дереву пошел в другое. И, соответственно, тот процессор должен был пойти не туда, а сюда, грубо говоря.
Если нарисовать повороты, то там как раз повороты друг с другом очень сильно конфликтуют. Если вы пошли в одно по дереву, надо в другое, вы что-то повернули, там все пошло по одному месту.
А здесь все хорошо, в любой момент времени у вас корректный скип-лист. Даже когда вы вставляете снизу вверх или когда вы удаляете сверху вниз, то есть когда я рыс делал, мы удаляли сверху вниз, у нас в каждом момент времени корректный скип-лист.
И тогда они друг другу не мешают. Вот такая прелесть.
Тройку, четверку сразу? Ну а что, какая проблема? Дошли до сюда. Один, скажем, поставил сюда 4, другой поставил сюда 3.
Дальше этот поднимается сюда, смотрит на предыдущего, проводит такую ссылку. Этот поднимается сюда и видит уже в этот момент, что он поднялся сюда, и тут оказалось четверка.
Здесь у меня минус, здесь 4, я сюда вставляю тройку и вот так вот стрелочки перепривязываю. Кажется, проблем не будет никаких.
Еще раз, от того, что после каждой конкретной встатки у вас корректный скип-лист, у вас все эти связочки не конфликтуют.
Если вот по этому списку идти или что? Так я это не так делаю. Смотрите, у меня find также работает как insert. Я сначала встаю в левую верхнюю точку, потом иду по верхнему списку пока могу.
Я не по нижнему списку, я, например, по верхнему иду в первую очередь. Тогда будет логарифм как раз.
Ну и, например, почему эта штука поддерживает порядок? Ну понятно, у нас здесь на нижнем уровне все элементы отсорчены.
И, например, если бы у нас был запрос, например, next x, который просил бы меня найти минимальное число, которое больше, чем x.
Ну тогда понятно, давайте найдем x и возьмем следующее от него. За счет того, что у меня на нижнем уровне все отсорчено, и вообще на каждом уровне все отсорчено, то всякие такие запросы, связанные с порядком, меньше, больше и так далее, они переделываются на скип-лист.
Более того, можно, например, хранить длины всех вот этих вот скачков, можно ввести дополнительное поле в каждой ячейке и хранить не просто указатель на следующего чувака, а сколько элементов между ними было и сколько элементов я пропустил.
В этом случае было бы у меня 1, 2, 3, 4, 5 элементов между ними, и здесь был бы параметр 5.
И тогда можно показать, что все эти количества можно поддерживать, при инсертах и рейзах их можно поддерживать, и тогда мы могли бы не просто узнавать следующий, но и по номеру узнавать элемент.
Узнать катую порядковую статистику в нашем множестве. Если нам нужна четвертая, то 5 это слишком много, я спускаюсь сюда. Вот здесь могу сделать прыжок, потому что здесь всего один, это вторая статистика.
Спускаюсь, спускаюсь, ну в общем, вот как раз десятка была бы вот эта, потому что здесь еще было бы плюс один.
Короче, если бы мы хранили длины прыжков, то могли бы, например, еще искать катую порядковую статистику в этом порядке.
Как нам ее поддерживать?
Ну вот можно, можно, я утверждаю, что можно при инсерте и рейзе эти числа хорошо пересчитывать.
Утверждение.
Ну все, спасибо большое за внимание.

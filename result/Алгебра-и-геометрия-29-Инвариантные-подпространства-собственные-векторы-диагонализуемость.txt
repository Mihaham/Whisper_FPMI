Так, добрый день, всем ли меня слышно? На всякий случай спрашиваю. Всем хорошо слышно. Замечательно.
Тогда давайте начинать. Извините за такую задержку. Значит, я на всякий случай напоминаю,
что лекции по материалу этого семестра у нас начались в прошлом семестре. Правильно? Многочлены
мы там уже обсудили. К счастью, они записаны, поэтому можно этот материал освежить. А я сейчас
небольшое напоминание сделаю по поводу того, что в прошлом семестре рассказалось и что нам
сейчас непосредственно будет нужно на всякий случай. И о многочленах, и о чём-то, что в материале
прошлого семестра было. Итак, небольшая напоминалка. Во-первых, многочлены. Вот я не
буду формальных всех определений давать, но я просто напомню, что если у нас F это поле,
то у нас появляется кольцо многочленов. Каждый многочлен единственным образом записывается
вот в таком вот виде, где PIT из F. Единственным образом с точностью до добавления нулевых слагаемых
в начало, естественно. Правильно? То есть каждый многочлен единственным образом раскладывается
в линейную комбинацию степеней вот этого самого загадочного X. Дальше я напоминаю, что у нас,
понятно, раз у нас есть кольцо многочленов, то существует понятие делимости. И в частности
существует понятие наибольшего общего делителя двух многочленов PQ, которое определено с точностью
до умножения на ненулевой скаляр. На скаляр из F со звездой, правильно? И кроме того, этот нот
линейно выражается через многочлены P и Q. P на S плюс Q на T, где S и T тоже многочлены над нашим полем.
Так далее у нас существует основная теорема арифметики для многочленов, то есть каждый многочлен,
давайте я скажу ненулевой, раскладывается на неприводимые сомножители.
На неприводимые сомножители. Так давайте я лучше не P напишу, а скажем A. Гамма на P1 и так далее
на ПКТ, где гамма это ненулевой скаляр, P и T неприводимые. И это разложение единственно с точностью
до, понятно, перестановки сомножителей раз и домножение каких-то сомножителей на константы
ненулевые, так чтобы в целом домножили на единицу. Такое разложение по существу единственное.
Вот так вот давайте я скажу. Детали можно, еще раз говорю, вспомнить из хотя бы даже видео.
Ну и третье, что мы знаем, что у многочленов существуют корни, это означает, что просто P от C равно 0,
и это равносильно тому, что х-с делит наш многочлен. С это корень многочленов, равносильно тому,
что х-с делит наш многочлен. Ну и соответственно, С будет корень кратности, давайте я так скажу,
С корень кратности хотя бы К, если х-с вкатый делит наш многочлен. Ну а соответственно,
кратность этого корня, это наибольшая такая К будет, для которого х-с вкатый делит многочлен.
Ну и отсюда и из теоремы о единственности разложения на неприводимые множители следует,
что количество корней с учетом кратности, то есть по сути дела сумма кратностей всех корней не превосходит
в степени многочленов. Так, это необходимые воспоминания про многочлены. Многие из этих фактов
мы знали и до этого, но возможно не над любым полем. И давайте я еще напомню некоторые
сведения, связанные пролинейные операторы. Давайте я так скажу, пролинейные преобразования,
потому что в этом семестре мы будем заниматься линейными преобразованиями, а не отображениями.
Когда мы будем говорить про линейные операторы, чаще всего мы будем иметь в виду именно линейное
преобразование. Ну во-первых, что такое линейное преобразование? Это отображение из линейного
пространства в себя, которое при этом еще и линейно. Фет у плюс в это фет у плюс фет в,
фет лямбда на у это лямбда на фет у. Если есть базис в, то немедленно появляется у каждого
преобразования у каждого оператора его матрица размера n на n, в столбцах которой, напоминаю,
если нужно, стоят координатные столбцы. Здесь вот у нас в столбцах этой матрицы это координатные
столбцы, фет е1, фет е2 и так далее. Ну и если у нас вектор в имеет координатный столбец х в нашем
базисе, то фет в будет иметь координатный столбец а на х в этом же самом базисе. Базис 1 и тот же,
потому что, напоминаю, мы у преобразования как бы и в этом и в этом пространстве выбираем один
и тот же базис. Мы берем фе-образы базисных векторов и расписываем их по этому же самому
базису в случае преобразования. То есть наше линейное преобразование заключается в координатной
форме, просто в умножении на нашу матрицу А. Если базис у нас будет меняться, если у нас есть
замена базиса, то матрица, естественно, тоже будет меняться. Кто мне напомнит, кстати? Вот я
напоминаю, давайте. С-1 на А на С, правильно? У нас оба базиса в обоих пространствах меняются,
поэтому вот таким вот образом у нас происходит замена матрицы нашего преобразования. И тут,
давайте я сразу сделаю что-нибудь, скажу что-нибудь новенькое, я скажу определение,
что вот такие вот матрицы называются подобными. Матрицы А и А' одного и того же размера n на n
над полем называются подобными. Если существует такая матрица замены базиса, то есть, напоминаю,
невырожденная матрица S. Так, невырожденность, напоминаю, мы записываем вот таким вот образом.
GLn от f это как раз множество, а точнее, группа всех невырожденных, они же обратимые, матриц размера n
на полем S. Итак, если существует матрица S, такая, что А' это С-1 на А на С. В этом случае
матрицы называются подобными, ну и вот из этого вот факта вытекает, что матрицы подобные, это означает,
что это матрицы одного и того же оператора в каких-то двух разных базисах. Если А это матрица
какого-то оператора, то применим замену базиса с матрицы перехода S, получим матрицу А' того же
самого оператора. Правильно? Вот, значит, здесь же я должен сказать, что если мы обозначим через L от V
это множество всех линейных преобразований из V в V, тогда на этом множестве мы можем
определить кучу операций. Операторы, вот эти преобразования нашего пространства V мы можем
складывать. Естественно, фи плюс си от V это фи от V плюс си от V. Мы можем их умножать на константу
лямда фи от V, это просто лямда на фи от V. И таким образом это множество становится линейным
пространством над нашим полем. Также мы имеем право, конечно же, их перемножать. Фи на си от V это
просто композиция этих операторов. Обратите внимание, что это действительно фи от си от V, то есть
к вектору V сначала применяется правый си, потом левый фи. Правильно? Ну и в таком, вот с такими
тремя операциями l от V, таким образом l от V это алгебра над полем f. Алгебра, напоминаю,
это значит, что это кольцо и одновременно с этим линейное пространство. Правильно? Согласованными
операциями. l от V это алгебра над f. Ну а когда мы каждому оператору в каком-то фиксированном
базисе сопоставляем его матрицу, то это изоморфизм между алгеброй операторов l от V и алгеброй
матрицы на n над нашим полем f. То есть это сопоставление, это биекция, и она сохраняет все
операции, о которых мы говорим, в частности, операцию композиции операторов, она переводит в
произведение наших матриц. Вот, это второй факт, который здесь нужно вспомнить. Ну и третий факт,
который связывает все наши напоминания друг с другом, говорит вот о чем. Если у вас есть
многочлен P над полем f, если у вас есть оператор phi на пространстве V над полем f, естественно,
то тогда мы имеем право взять этот многочлен от этого оператора. Если у нас есть многочлен над
полем, мы имеем право брать вот такое вот выражение для любой алгебры над этим же самым полем.
Ну что это такое? Если у нас многочлен имеет вот такой вот ответ, как мы написали, то здесь мы
можем написать просто Pn на phi в n, что такое phi в n, мы знаем, это n-кратная итерация нашего phi,
плюс Pn-1 phi в n-1, плюс и так далее, плюс P0 на единицу, правильно? В любой алгебре,
в которой мы такое вот дело берем, мы берем P0 умножить на единицу, то есть на тождественное
отображение. Вот иногда его обозначают вот таким вот образом. 1 с индексом V это означает
тождественное отображение пространства V в себя. Ну и ровно также, если у нас есть матрицы,
если у нас есть произбольная матрица n на n над полем f, то мы точно также можем взять многочлен
от матрицы, ну и это будет как бы вот операция, которая получается нашим изоморфизмом из
многочлена от оператора. То есть это будет Pn-я в n, плюс и так далее. P0 будет умножаться опять же
таки на единицу в этой алгебре, то есть на единичную матрицу E. Вот, наверное, это все напоминания,
которые хотелось бы сказать. Да, ну и естественно, с этими многочленами от оператора или от матрицы мы
можем работать также, как с обычными многочленами, складывать их, перемножать, получать результаты
применения соответствующих многочленов. Так, вот это вот у нас напоминание. Если вдруг у кого-то
какие-то вопросы, давайте я спрошу, есть ли какие-то вопросы, а то вдруг что-то совсем забылось. Все
понятно, что я сказал, да? Тогда давайте двигаться дальше. Теперь уже будет что-то новое. Итак,
мы начинаем с вами первую большую тему. Ну и как нетрудно понять, вот первая большая
тема у нас будет связана с линейными операторами. Линейные операторы это совсем большая тема,
а тема поменьше называется инвариантное подпространство. Итак, что это такое, даю
определение. Пусть у нас есть, как обычно, линейное пространство над полем f, и есть
линейный оператор на нем, то есть линейное преобразование этого самого пространства. Мы с
вами говорим, что подпространство этого пространства, напоминаем, что вот это вот меньше или равно,
но этот значок обозначает, что мы взяли не просто подможество, а подпространство оператора, пространство
Вы, извините, говорим, что подпространство этого пространства
давайте даже без тире, инвариантно относительно фи,
если, формально я могу написать так, фи от у содержится в у.
То есть, иначе говоря, если любой вектор нашего подпространства
при действии оператором фи переходит тоже в какой-то вектор подпространства у,
то оно обязательно в тот же самый, естественно.
Обратите внимание еще здесь, включение, а не равенство,
то есть, не обязательно в каждый вектор этого подпространства что-то должно переходить,
может быть, что не в каждый.
Оно естественно будет подпространством, потому что фи от любого подпространства это подпространство,
что он в своё время доказывает.
Здесь я могу написать, если хотите, и вот так вот тоже, это тоже будет правдой.
Но это равносильные понятия.
Раз уж включаются, то и подпространство.
Сразу давайте мы приведем какие-нибудь примеры.
Во-первых, если фи – это нулевой оператор,
то любое подпространство инвариантно.
Любое подпространство переходит в ноль, нулевой вектор лежит в любом подпространстве,
поэтому любое подпространство инвариантно.
Второй пример, наверное, более интересный.
Давайте мы, наверное, начнём с геометрического.
Давайте посмотрим на двумерную нашу плоскость.
В – это наша стандартная В2.
Фи от В – это проекция В на какую-то ось.
То есть я зафиксировал А, скажем, вот он направлен для любого вектора В.
Артагональная проекция на эту ось, фи от В.
Тогда утверждаю я, у этого отображения есть вот такие, в частности, инвариантные подпространства.
Подпространство, порождённое А, будет инвариантным,
просто потому что каждый вектор из него переходит в себя, правильно?
И если я возьму вектор В, артагональный А,
то подпространство, порождённое В, также будет инвариантным,
просто потому что каждый вектор этого вот пространства переходит в ноль, правильно?
Ну и, наверное, на всякий случай я скажу более общё.
Если любое наше пространство разложено в прямую сумму двух подпространств,
то, напоминаю, у нас возникает оператор проекции на подпространство У вдоль подпространства В.
Фи – это проекция на У вдоль В.
Ну, то есть давайте я напомню так.
Если вектор В раскладывается по векторам вот этих подпространств как У плюс В,
У лежит в У, В лежит в В, то фи от В – это просто-напросто У.
В этой ситуации У и В, разумеется, оба инвариантны относительно фи.
Ну, много других… Да, давайте всё-таки ещё одно другое от пространства я приведу.
Третье. Если В – это пространство многочленов в степени не выше n, многочлены в степени не выше n.
И я предъявлю вам линейное отображение, заключающееся во взятии производной многочлены.
Напоминаю, что формальная производная у нас определена над любым полем,
поэтому на самом деле над любым полем это можно сделать.
То тогда инвариантными подпространствами будут, давайте я скажу в частности, хотя практически все я перечислю,
любое подпространство многочленов в степени не выше k окажется инвариантным относительно этого оператора.
Многочлены в степени не выше k при дифференцировании переходят в многочлены в степени не выше k-1.
Поэтому любое ПКТ окажется инвариантным.
Ну вот мы с вами уже на этих примерах не трудно почувствовать, что инвариантные подпространства
имеют достаточно большую роль в жизни линейного оператора.
Они много о нем говорят, о каких-то вещах, связанных с геометрией этого оператора.
Ну и мы сейчас еще это увидим с алгебравической точки зрения тоже.
Я надеюсь, этих примеров у нас достаточно. А, да, я забыл привести самый важный пример.
У каждого оператора есть точно два инвариантных подпространства.
Все В и ноль. В и ноль. Инвариантная всегда.
Так, ну давайте теперь переходить к обсуждению свойств наших инвариантных подпространств.
Прежде чем я сформулирую какие-то утверждения, я сделаю еще одно идеологическое замечание.
Если У инвариантное подпространство относительно Ф, что это означает?
Это означает, что Ф переводит любой вектор из У в вектор из У.
Ну и тогда можно посмотреть на то, как действует Фи только на У, и это окажется тоже линейным преобразованием У.
То есть в этом случае можно рассмотреть, вот какой оператор Фи, ограниченный на У,
и он окажется линейным преобразованием этого самого пространства.
Если У будет неинвариантным, такого сделать нельзя, потому что Фи, какие-то векторы из У,
будут переводить вовне У, правильно?
Так что вот это можно сделать ровно в том случае, когда У инвариантно, и это тоже бывает полезным.
Очень часто действия Фи на каких-то инвариантных подпространствах тоже много о самом преобразовании, о самом операторе говорит.
Так, ну поехали изучать свойства этого нового введенного понятия.
Утверждение первое. Давайте мы поймем, как строить новое инвариантное подпространство из старых.
Итак, пусть Фи это линейный оператор на В, а У1 и У2 это инвариантное подпространство относительно Фи.
Тогда их пересечение и их сумма также инвариантны. Естественно, тоже относительно Фи.
Ну это пока что достаточно тривиально, но давайте проговорим на всякий случай.
Что делать с пересечением? Если какой-то вектор лежит в пересечении, то естественно Фи от У будет лежать в У1,
Вектор У лежит в У1, значит и Фи от У лежит в У1. У1 инвариантно, правильно?
Фи от У лежит в У2 по той же самой причине, ну а значит Фи от У лежит в пересечении.
Вот мы и доказали, что пересечение инвариантно. Любой вектор из этого пересечения переходит тоже в вектор из пересечения.
Для суммы не намного сложнее. Если какой-то вектор лежит в сумме двух инвариантных подпространств,
то мы с вами знаем, что он раскладывается в сумму двух векторов из этих подпространств.
Это разложение не обязательно единственное, но нам это не важно.
Ну а тогда Фи от В это будет Фи от У1 плюс Фи от У2. Фи от У1 лежит в У1, поскольку У1 инвариантно.
Фи от У2 аналогичным образом лежит в У2, ну а значит эта сумма лежит в сумме наших У1 и У2.
И таким образом утверждение мы тоже уже доказали.
Так, следующее... О, вот я как раз стираю матрицы наших операторов.
Давайте мы поймем, как инвариантные подпространства влияют на структуру матрицы оператора в соответствующем базе.
Итак, пусть у нас Фи это линейный оператор на В, а Е, состоящий из этих векторов Е1,
и так далее, и ЕН, это базис В.
Тогда давайте мы поймем, что означает, что первые К векторов в этом базисе порождают инвариантное подпространство.
Вот такое вот подпространство с базисом Е1 и так далее ЕКТ окажется инвариантным тогда и только тогда,
когда в нашем базисе Фи имеет вот такую вот блочную матрицу.
Значит, что у нас тут будут за блоки? Вот размеры блоков будет здесь КН-К, здесь тоже КН-К.
Вот здесь будут стоять какие-то три матрицы, а здесь будет стоять ноль.
Доказательства опять же такие очень простые.
Давайте мы выясним, что означает, что это у инвариантно.
Как в терминах базиса сформулировать, что подпространство у вот с таким вот базисом или с такой порождающей системой является инвариантным?
Очень просто. Нам достаточно проверить, что Фи, что образы всех вот этих вот базисных векторов попадают в У, правильно?
Естественно, если У инвариантно, то Фи от яитых будут попадать в У при И от одного до К.
Это вот я объяснил, почему сверху следует нижнее.
Ну и наоборот, естественно, тоже, если образы всех базисных векторов будут лежать в У, то и образы их линейных комбинаций тоже, правильно?
А это все векторы из У.
Так, ну а это что как раз означает?
Фи от яитых лежит в У, это означает, что Фи от яитого это линейная комбинация первых К базисных векторов в нашем базисе.
Так, давайте я не буду мельчить, я перейду на следующую часть, может быть.
Итак, Фи от яитого должно раскладываться только по первым К базисам, по первым К базисным векторам, естественно, извините.
Сумма пожелает одного до К какая-то альфа ежи ежи т.
Ну а эти коэффициенты разложения это как раз элементы нашей матрицы.
В первых К столбцах нашей матрицы должны стоять координатные столбцы ровно вот этих вот товарищей, правильно?
Так здесь у нас И тоже от одного до К.
В первых К столбцах должны стоять координатные столбцы вот этих товарищей.
И вот здесь мы написали, что в этих координатных столбцах только первые К элементов могут отличаться от нуля.
Потому что только по первым К базисным векторам они тут у нас раскладываются.
Это как раз и означает, что Фи в базе Се имеет матрицу, в которой здесь стоят нули.
Ниже вот этой вот планки в К элементов стоят нули.
Ну а здесь стоит все, что угодно.
Вот мы наше утверждение и доказали.
Ну и это, в общем, еще раз показывает, что инвариантные подпространства могут быть полезными.
Поскольку матрица нашего оператора, вот если мы отловили какое-то инвариантное подпространство
и его базис поставили на первые места, то мы уже приходим к хорошей матрице, про которую мы даже что-то знаем.
Напоминаю, что в свое время у нас было утверждение про определитель такой матрицы.
Это как раз матрица с углом нулей.
Здесь уместно сделать замечание.
Естественно, вот если я тут написал матрицу вот в таком виде А, Б, С, то А имеет ясный геометрический смысл.
Что такое А?
Это матрица того самого преобразования подпространства У, правильно?
А это матрица ограничения Фи на У.
Потому что здесь как раз написано, как образы базисных векторов раскладываются по тому же самому базису этого подпространства, правильно?
Матрица этого ограничения в базисе Е1 и так далее Ек.
Тут я сразу хочу сказать, что матрица С тоже может придать геометрический смысл.
Но мы этого сейчас делать не будем, потому что это требует некоторых дополнительных понятий, которые мы пока не ввели.
Но через некоторое время мы об этом тоже попытаемся поговорить.
То есть у матрицы С тоже есть смысл как у матрицы какого-то преобразования, но чуть похитрее определенного.
Так, давайте я лучше перейду на следующую доску.
Ну и следующее утверждение, которым мы часто будем пользоваться.
Еще один важный источник наших инвариантных подпространств.
Вот каков утверждение.
Пусть у нас есть на сей раз два линейных оператора на пространстве В, два линейных преобразования В.
Причем важные условия, они коммутируют.
То есть если их перемножить в любом порядке, получится один и тот же оператор.
Тогда ядро Пси и образ Пси инвариантны относительно Фи.
Прежде чем доказывать, давайте я спрошу, а относительно Пси они будут инвариантными?
Будут ли они инвариантными относительно Пси?
Конечно да, потому что мы можем тоже утверждение применить к Фи равному Пси.
Насколько Пси, они тоже инвариантными будут.
Так, ну раз у нас прозвенел звонок, давайте мы...
Или как вы смотрите, можем мы первый перерыв немножко съесть в счет того, что у нас происходило в начале пары?
Задержка у нас происходила.
Работаем без одного перерыва или не работаем?
Без проблем.
То есть если у кого-то есть проблемы, говорите, сделаем перерыв.
Нет проблем. Хорошо, тогда доказываем.
Опять же таки нужно нам разобраться с двумя случаями.
Давайте докажем, что ядро Пси инвариантно относительно Фи.
Если какой-то вектор лежит в ядре Пси,
ну здесь у нас будет все практически автоматически, главное определение помнить.
Что означает, что какой-то вектор лежит в ядре Пси?
Это означает, что Пси от У это ноль.
А теперь нам хочется проверить, что Фи от У тоже лежит в ядре Пси, правильно?
Что нам для этого нужно выяснить?
Что Пси от Фи от У это ноль, правильно?
Хотелось бы нам понять, что это ноль, но мы-то с вами знаем, что Фи и Пси перестановочны, правильно?
Поэтому здесь мы можем переставить их местами и сказать, что это то же самое, что Фи от Пси от У.
Но Пси от У был нулем, значит и Фи от него это тоже ноль.
Равно Фи от нуля, то есть равно нулю.
Все, мы с вами получили, что Пси от Фи от У это ноль,
значит Фи от У лежит в ядре Пси просто по определению.
Мы проверили, что любой, если какой-то вектор лежит в ядре Пси, то и Фи от него тоже лежит в ядре Пси.
Значит первая часть у нас доказана.
Ну и вторая часть делается аналогичной манипуляцией.
Давайте мы ее, естественно, все равно проговорим.
Значит, если у нас есть ядро Пси, если мы взяли некоторые векторы, извините, образ Пси, конечно,
если мы взяли некоторые векторы из образа Пси, что это означает?
Это означает, что этот вектор есть Пси от кого-то другого, правильно, от какого-то W, где W лежит в В.
Ну а тогда что такое Фи от У?
Раз У это Пси от В, то это Фи от Пси от В.
Ну и опять же таки вспоминаю, что наши операторы перестановочные.
Мы можем сказать, что это Пси от Фи от В.
Да, равенства уже не нужно. Мы уже говорим, это Пси от кого-то, правда?
А значит он лежит в образе Пси.
И таким образом мы опять же таки доказали, что если У лежал в образе, то и Фи от У тоже лежит в образе, что и требовалось доказать.
Так, утверждение интересно само по себе. Чаще всего, я сразу скажу, мы будем использовать его вот в какой форме.
Где взять запас таких Пси, которые коммутируют с данным нам Фи?
Фи, говорят нам, Фи квадрат и так далее.
На самом деле в качестве Пси можно взять, давайте я просто скажу, что можно положить Пси равным любому многочлену от Фи.
Глядите, если я возьму П от Фи умножено Фи, это будет то же самое, что я Фи умножено П от Фи.
Можно это проверить непосредственно, просто скобки раскрыть, правильно?
Можно сказать, что мы работаем с многочленами от Фи, а они ведут себя, вот и алгебравические действия над ними, это то же самое, что алгебравические действия просто над многочленами.
Но в частности они коммутативны.
То есть если я возьму Пси равным П от Фи, то я могу спокойно применять это утверждение.
То есть ядро произвольного многочлена от Фи и образ произвольного многочлена от Фи инвариантная относительно Фи.
Ну и еще, наверное, одно, еще более частное следствие, но тоже из тех, которые нам будут нужны сильнее всего.
Самый простой многочлен, который нам окажется полезным, это многочлен вида Фи минус лямбда, ну то есть Х минус лямбда.
Здесь, конечно же, лямбда это скаляр, это элемент поля.
Ядро Фи минус лямбда и образ Фи минус лямбда инвариантная относительно Фи.
Что такое Фи минус лямбда?
Как я из оператора вычитаю скаляр?
Это Фи минус лямбда на тождественный оператор, естественно, правильно?
Я многочлен Х минус лямбда подставил Фи.
То есть каждый раз, когда мы пишем Фи минус лямбда, мы имеем в виду, естественно, Фи минус лямбда на тождественный оператор на пространстве В.
Так вот, ядро и образ таких операторов всегда инвариантны относительно Фи.
Ну и, наконец, еще одно утверждение, чуть более общее.
Пусть Фи это линейный оператор на В,
тогда любое подпространство в ядре Фи...
Ну хорошо, оставим пока так.
Любое подпространство в ядре Фи и, внимание, любое надпространство образа Фи инвариантны относительно Фи.
Это тоже еще пока что очень просто.
Почему это так? Потому что если У лежит в таком вот У, которое подпространство в ядре Фи, то Фи от У это, конечно же, ноль, и потому лежит в У, правильно?
Первое утверждение я уже доказал. Если В лежит в В большом, то Фи от В, даже если бы он не лежал в В, лежал бы просто в В,
Фи от В все равно лежит в образе Фи, правда?
И реальность верная всегда, ну а образ Фи это подпространство в В, и второе утверждение тоже верное.
Ну и несложное упражнение, которое я предлагаю всем желающим на самостоятельную разработку.
Это утверждение останется верным, если я здесь вместо Фи подставлю Фи минус лямбда.
То есть любое подпространство в ядре Фи минус лямбда и любое надпространство образа Фи минус лямбда также будут инвариантны относительно Фи.
Относительно Фи минус лямбда это они точно инвариантны, правильно? Оказывается и относительно Фи тоже.
Так, ну вот это небольшой блок, касающийся произвольных инвариантных подпространств.
Они нам еще будут встречаться этим понятием, мы естественно будем очень усиленно пользоваться.
Но дальше мы переходим к изучению самой простой ситуации инвариантного подпространства.
Тут у нас появляется новое понятие, которое называется собственные векторы.
Ну давайте прежде чем я дам определение, я спрошу об этом.
Что означает, что подпространство, порожденное одним вектором, инвариантно относительно Фи?
Как по вектору понять, что подпространство, порожденное им одним, ну естественно вектор, давайте мы возьмем не нулевой,
как по вектору понять, что подпространство, порожденное им одним, инвариантно?
Это означает, что когда мы к нему применяем Фи, получается что-то ему пропорциональное, правильно?
Это лямбда на В, где лямбда это какой-то элемент нашего поля.
В должен переходить тоже в эту линейную оболочку, то есть вектор себе пропорциональный.
Ну а больше никаких условий у нас и нет. Если это будет так, то все в порядке.
Вот такой вектор и называется собственным вектором.
Итак, определение, пусть Фи, это линейное преобразование В,
значит, говорим, внимание, что не нулевой вектор В является
собственным вектором оператора Фи.
Внимание, еще одно понятие, мы сразу два вводим.
Собственным значением лямбда, лямбда это как раз скаляр,
если выполнено ровно вот это вот свойство.
Еще раз обращаю ваше внимание, что это определение работает только с не нулевыми векторами.
Если бы мы убрали это условие, то нулевой вектор оказался бы собственным,
с любым собственным значением, правильно?
Этого не хочется, потому что сейчас мы на самом деле увидим,
что вот с таким определением собственных значений у нас будет немного.
То есть весьма избранные лямбды окажутся собственными значениями.
Естественно, давайте я это отдельно сформулирую.
Скаляр лямбда называется собственным значением оператора Фи,
если для него есть собственный вектор.
Если он хотя бы в одной такой ситуации оказывается,
если для него есть собственный вектор с этим самым собственным значением.
Этими понятиями мы сейчас будем пользоваться так часто,
что давайте мы их будем сокращать.
Собственный вектор у нас будет обозначаться СВ,
собственным значением у нас, естественно, будет обозначаться СЗ.
Разумеется.
Давайте сразу примеры.
Если вектор лежит в ядре Фи и при этом не нулевой, правильно?
То он собственный вектор с собственным значением ноль, правильно?
Второй пример, наверное, стоит сказать, что если, например,
Фи – это проекция на У вдоль В, мы с вами уже сказали, по сути дела,
что все векторы В собственные собственным значением ноль, правильно?
Все не нулевые векторы в В – это собственные векторы с собственным значением ноль,
а все векторы в У собственные собственным значением 1, правильно?
Собственные с собственным значением 1, опять же такие, нужно сказать, слова не нулевые.
На самом деле давайте я сразу скажу, что такое будет множество всех векторов,
удовлетворяющих вот тому вот самому свойству, правильно?
Фи от В равно лямда В. Давайте сразу заметим вот что.
Что означает, что Фи от В равно лямда В?
Мы можем перенести правую часть в левую и перегруппировать.
Давайте я даже так сделаю. Сначала Фи от В минус лямда В – это ноль.
А здесь у нас написано, что Фи минус лямда на В, Фи минус лямда, примененное к В – это ноль.
Иначе говоря, В лежит в ядре оператора Фи минус лямда.
Фи минус лямда – это Фи минус лямда на тождественный.
Поэтому стоит сразу сделать вот какое определение.
Итак, если лямда – это собственное значение оператора Фи,
то мы можем ввести вот такое вот подпространство.
Оно у нас будет обозначаться В с нижним индексом лямда.
Это ядро Фи минус лямда.
Оно называется собственным подпространством.
Ну, соответствующее собственному значению лямда.
Что такое собственное подпространство?
Это множество всех собственных векторов, соответствующих лямда.
И еще это добавление нуля, естественно.
Нулевой вектор не собственный, но он здесь тоже будет, чтобы оно оказалось подпространством.
Вот эти вот подпространства будут играть у нас, конечно же, важную роль.
Мы определили важное понятие собственного вектора.
Через некоторое время мы увидим, насколько оно важно, особенно когда этих собственных векторов много.
Но прежде чем это понимать, еще раз, мы вроде как это только что говорили, правильно?
Еще раз, Фи – это оператор, лямда – это скаляр, Фи минус лямда – это Фи минус лямда на тождественный оператор на В.
То есть Фи минус лямда – это Фи минус лямда на тождественный оператор на пространстве В.
Ну, соответственно, давайте я для большей ясности сразу скажу, чему он соответствует.
Если Фи имеет в каком-то базе матрицу А, Фи минус лямда будет иметь матрицу А минус лямда умноженная на Е.
Так, ну и давайте мы сразу уж раз об этом заговорили, то спросим себя, как искать собственные векторы и собственные значения.
А потом увидим, зачем они в первую очередь нам будут нужны.
Собственные векторы и собственные значения искать просто после того, как мы сказали то, что мы уже сказали.
Лямда является собственным значением, когда у нас есть хотя бы один не нулевой собственный вектор, соответствующий этому собственному значению.
То есть, когда ядро Фи минус лямда не нулевое, правильно?
Ядро Фи минус лямда не нулевое, а это означает, что Фи минус лямда – вырожденный оператор.
То есть, если Фи имеет в некотором базе матрицу А, то мы уже сказали, какая матрица у Фи минус лямда, А минус лямда Е – это вырожденная матрица.
А это означает, что определитель А минус лямда Е равен нулю.
Ну и таким образом, если у нас оператор Фи задан в каком-то базисе матрицей А, то как мы можем найти все его собственные значения?
Найти вот этот определитель – это будет что, кстати, такое? Это будет какой-то многочлен от лямды, правильно?
Найти вот этот определитель, как многочлен от лямды, его корни в точности и будут собственными значениями.
Итак, лямда – это собственное значение оператора Фи тогда и только тогда, когда вот этот самый определитель равен нулю, ну и если А – это матрица нашего Фи.
Тут сразу стоит сделать определение, обозначить вот этот определитель каким-то хорошим образом.
Если А – это матрица N на N над полем F, то вот такой вот многочлен, который мы только что написали,
я сразу пишу, как он называет, он обозначается, греческая буква Хи. Давайте я тут Х напишу.
Определитель А минус Х на Е, вот этот многочлен – это характеристический многочлен матрицы А.
Таким образом, лямда – это собственное значение оператора Фи тогда и только тогда, когда лямда – это корень характеристического многочлена матрицы этого оператора.
На самом деле здесь, конечно, хочется сказать, что очень странно у нас получается, у одного и того же оператора в разных базисах будут разные матрицы,
и могут получиться разные характеристические многочлены у этих матриц. На самом деле это не так.
Если Фи имеет матрицу А в каком-то базе СЕ и имеет матрицу А' в каком-то базе СЕ', то характеристические многочлены этих матриц совпадают.
Доказательства. Давайте мы увидим это. Да, давайте я сначала напомню, что означает, что А и А' – это матрицы одного и того же Фи в разных базисах.
Это означает, что они подобны. Мы можем сказать, что А' имеет вид С-1 на АС, для некоторой, естественно, обратимой матрицы С, раз у нас встречается С-1.
Ну и если это так, то мы можем понять, как у нас выглядит характеристический многочлен А'.
Тогда характеристический многочлен А' – это что такое? Я беру определитель А'-хE, то есть я беру определитель вот какой матрицы С-1,
вот какой матрицы С-1, АС, минус Х на Е. Но давайте я сразу это Е тоже немножко распишу.
Минус Х на вот что? С-1 ЕС. С-1 на С это же Е, правильно? Ну а значит у нас здесь написано определитель вот какого произведения.
С-1 я могу вынести слева, а С я могу вынести справа, правильно? И в скобках у меня останется А-хE.
Ну определитель произведения мы с вами знаем, что равен произведению определителей.
Ну и это, естественно, два крайних сомножителя в произведении дают единицы, правильно?
Так же определители взаимно обратных матриц, так же взаимно обратных. Ну и мы получили то, что нам нужно, потому что мы получили характеристический многочлен матрицы А.
Ну и как следствием, коль скоро любая матрица нашего оператора имеет один и тот же характеристический многочлен,
то естественным образом мы можем сказать, что этот многочлен можно назвать характеристическим многочленом этого оператора.
Характеристическим многочленом оператора А-хE теперь мы просто назовем характеристический многочлен любой его матрицы.
Неважно какой, потому что многочлен получится один и тот же.
Так, ну и коль скоро мы с вами заговорили про характеристический многочлен и поняли, что он не зависит, то есть он не меняется при подобии.
Стоит, прежде чем двигаться дальше, немножко посмотреть вблизи на этот самый характеристический многочлен.
Давайте мы сразу это сделаем. Пусть А имеет элементы А и Ж, давайте я даже ее в натуральную величину напишу.
А2-1, А2-2, здесь А1-2, тогда ее характеристический многочлен получается каким образом?
Я должен из А вычесть х на Е, то есть по сути дела вычесть из диагональных элементов по х, правильно?
Здесь у меня будет А1-1-х, А2-2-х, АНН-х, а остальные элементы остаются такими же, как были.
АН1, АН2 и так далее. И вот такой у нас получается многочлен. Давайте мы на него внимательно посмотрим, какая у него будет степень.
Разумеется, Н. У нас всего Н линейных многочленов, которые зависит от х, все остальные константы, правильно?
Больше Н получится в принципе не может, но все эти товарищи у нас конечно перемножатся и дадут нам х в Н, правильно?
Точнее дадут нам х в Н с каким коэффициентом? Минус 1 в Н, потому что перед всеми этими х-ами стоят минусы, правильно?
По сути дела у нас будет минус х в Н. Старший член нашего многочлена это минус 1 в Н на х в Н.
Единственным образом вот таким вот из произведения всех диагональных элементов может получиться х в Н.
Давайте я сразу спрошу, а каким образом может получиться х в Н-1?
Неправда.
Откуда может получиться х в Н-1? Если у нас...
Абсолютно верно. Если мы хотим получить в полном разложении этого определителя, когда мы раскроем все скобки,
моном х в Н-1, он может получиться тоже только из произведения вот этих диагональных элементов,
потому что если у нас Н-1х набрался, то Н-1х был взят из диагональных элементов, правильно?
А тогда и Н-ный элемент тоже будет диагональным, раз уж мы Н-1 элемент в произведении знаем,
и знаем, что они должны быть из разных строк и разных столбцов. Последний тоже должен быть из диагонали.
Х в Н-1 тоже должен взяться из вот этого самого произведения, но только когда мы там скобки раскроем,
у нас же будут члены, в которых Н-1х, а Н-ая константа, правильно?
И, значит, давайте понимать, что у нас там будет. Там будет минус 1 в степени Н-1,
вот эти вот Н-1х возьмутся со знаками минус, правильно?
А дальше будет... а дальше может оказаться любой из вот этих вот диагональных элементов,
то есть здесь будет А-1-1 плюс А-2-2 плюс и так далее, плюс АНН.
Так, дальнейшие члены я вычислять не буду, хотя в принципе некоторые их описания тоже существуют,
но я их оставлю на самостоятельное изучение всем желающим.
Скажу я только, какой будет свободный член. А какой будет свободный член?
Конечно, просто детерминат нашей матрицы, правильно?
Хотя бы потому, что такое свободный член вот этого вот многочлена.
Свободный член любого многочлена, это его значение в нуле.
Если мы сюда подставим ноль, то мы, конечно же, получим просто определитель матрицы А.
Свободный член у нас это определитель А.
Так, ну и настало нам время обозначить вот это вот выражение тоже каким-то образом.
Оно имеет специальное название, определение.
Если А это квадратная матрица с элементами А и житами, то ее след это ровно вот то, что у нас там написано.
Это сумма всех ее диагональных элементов. Не всех, а только диагональных элементов.
Так, ну и следствие из нашего предыдущего утверждения, которое у нас здесь было,
если одно и то же преобразование имеет матрицы А и а' в двух разных базисах,
то след А равен следу а', ну а детерминат А равен детерминату а',
просто потому что это коэффициенты их характеристических многочленов, правильно?
А мы доказали только что, что характеристические многочлены совпадают, значит и соответствующие их коэффициенты тоже совпадают.
Мы знаем, для следа мы знаем, что минус, давайте я для следа напишу, мы знаем, что характеристические многочлены совпадают,
то есть мы знаем в частности, что минус 1 в n-минус 1 на след А равен минус 1 в n-минус 1 на след а',
ну а это и означает, что след А равен следу а'. С детерминатом абсолютно аналогично.
Детерминатом мы могли и раньше сразу доказать, но уж из этого оно следует, правильно?
Другие коэффициенты характеристических многочленов, естественно, с ними тоже можно то же самое сказать,
просто вот эти вот имеют достаточно ясный или известный нам смысл.
Ну и естественно, коль скоро такие понятия, не зависит от того, в каком базе мы берем матрицу нашего оператора,
то мы можем сказать, что эти значения называются следом и детерминатом фи.
То есть мы можем писать понятия след фи и можем писать определение, понятие детерминат фи,
как след любой матрицы этого фи или детерминат любой матрицы этого фи.
Они не зависят от того, в каком базе мы эту матрицу записываем.
Ну что ж, про характеристический многочлен мы с вами поговорили и про то, как собственные значения и собственные векторы искать мы тоже поговорили.
Ну точнее мы сказали, как искать собственные значения, решить вот это вот уравнение, правильно?
Ну а собственные векторы это как после этого искать?
Собственные векторы это поиск ядра известного вам оператора.
Вы нашли конкретную лямду, после этого вам нужно найти ядро фи-лямда, мы это уже знаем как делать, правильно?
Так что собственные значения, собственные векторы мы тоже искать уже должны уметь.
Если хотите, можем устроить перерыв раньше, но я бы хотел доказать еще одно важное подтверждение.
А что, уже устали?
Да, давайте хорошо, раз у нас все равно полторы пары, да?
Давайте мы сейчас сделаем 10 минутный перерыв, а после этого уже двинемся дальше.
Раз уж мы сделали паузу, то давайте прежде чем переходить дальше, я еще пару...
Одно упражнение, одно замечание оставлю.
Так, нехитрое упражнение, которое порой бывает полезным, говорит нам, что...
Раз уж мы ввели новое понятие, то я предлагаю всем желающим доказать вот это вот равенство
для любых матриц, для которых это равенство имеет смысл.
На самом деле, для каких матриц имеет смысл?
Какие матрицы можно перенажать и в этом, и в том порядке?
Не только квадратные.
Вы можете взять матрицу N на K в качестве матрицы A, и тогда матрица B придется вам брать размера K на N, правильно?
А получаются квадратные хоть и разного размера.
Вот даже в такой общности это утверждение все равно верно.
Ну и замечание, которое я, может быть, сказал бы позже, но уж давайте, раз так пришлось, то скажу сейчас.
Естественно, из того, что мы вывели, следует, что у оператора не слишком много собственных значений.
Если phi – это линейное преобразование пространства V, размерность V равна N, то у phi не более N собственных значений есть.
Потому что все собственные значения являются корнями вот этого характеристического многочлена.
Его степень равна N, ну а значит и корней тоже не больше, чем N.
Ну и вот теперь давайте мы пойдем дальше.
Сначала у нас важное, то самое важное утверждение, которым я хотел сказать, уже не связанное с характеристическим многочленом.
Мы выяснили, что собственные векторы и собственные значения мы все-таки уже понимаем более-менее, как искать.
Практически это тоже будет реализовано.
Как они расположены друг относительно друга?
Верен следующий важный факт.
Пусть у нас phi – это линейный оператор на пространстве V, а λ1 и так далее, лямдокаты различны его собственное значение.
Тогда мы для каждого из этих лямд можем взять собственное подпространство.
То есть множество всех собственных факторов, соответствующих этому лямду, пополненное нулем.
И вот утверждение заключается в том, что если мы возьмем все эти собственные подпространства,
то их сумма окажется прямой суммой.
Что это значит, мы обсудим немножко позже.
А пока что давайте мы это докажем.
Как нам это доказать?
Давайте предположим противное.
Я, как обычно, напомню, что у нас происходило.
Сумма называется прямой суммой, если каждый вектор из этой суммы раскладывается по компонентам единственным образом.
И мы знаем критерий, что это происходит тогда и только тогда, когда нулевой вектор раскладывается по этим компонентам не единственным образом.
То есть когда мы предполагаем противное, мы можем считать, что ноль оказался суммой k векторов из этих подпространств.
Это v1 плюс и т.д. плюс vkt, где vi и т.д. это элемент v лямбда и т.д.
Но и при этом не все эти векторы нули.
Единственное разложение нуля было, конечно же, 0 плюс 0 плюс и т.д. плюс 0, в случае когда сумма прямая.
Если сумма не прямая, то есть вот такое нетривиальное разложение.
Не все vt нули.
Давайте мы сразу ограничим себя только на те подпространства, из которых vt не нулевые.
То есть если у нас, скажем, vkt оказалось нулем, то давайте просто забудем про эту лямбду кату и будем смотреть на остальные лямбы.
Мы это сделать спокойно можем. Я же не сказал, что это все собственные значения.
Так что мы можем считать, забывая про лишние индексы, мы считаем, что все vt не нули.
Ну и давайте мы предположим, мы по-прежнему считаем, что лямбд собственных значений у нас k и все vt не нули.
Ну и наконец давайте, пусть k у нас наименьшее число, для которого это возможно.
По сути дела, что у нас возможно? Давайте смотреть.
vt не нулевые, то есть они все являются собственными векторами с собственными значениями лямбд, правильно?
И их сумма равна нулю. То есть мы говорим, что пусть k это наименьшее число.
Такое, что мы можем взять k собственных векторов с различными собственными значениями, у которых сумма нулевая.
Ну и давайте посмотрим, что нам дает вот это вот равенство.
У нас есть равенство 0 равно v1 плюс и так далее, плюс vkt.
Естественно, я должен сказать, что k больше единицы.
Потому что если k равно единице, то у нас тут написано 0 равно v1, а v1 не нулевой.
Этого быть не может, правильно?
И мы можем к обеим частям этого равенства применить наш любимый phi.
Что у нас получится? Слева у нас получится phi от 0, то есть, естественно, 0, правильно?
А справа у нас получится phi от v1, плюс и так далее, плюс phi от vkt.
Но все они собственные с известными нам собственными значениями.
Поэтому на самом деле здесь написано лямбда 1 v1, плюс и так далее, плюс лямбда kt на vkt.
Вот такая вот их линейная комбинация равна 0, и вот такая вот их линейная комбинация равна 0.
Но из этого легко получить противоречие.
Давайте мы отсюда...
Вот у нас есть сумма phi от 1 до k лямбда i t v i t.
Это мы взяли вот эту нулевую сумму.
Давайте мы отсюда вычтем лямбда kt, умноженная на первую сумму.
Сумма phi от 1 до k просто v i t.
Что это у нас такое?
Лямбда kt на vkt у нас сократится, правильно?
Ну а остальные члены как раз не сократятся.
Сумма phi от 1 до k-1, kт и член здесь, и kт и член здесь сократились.
Что у нас здесь будет?
Лямбда i t минус лямбда kt на v i t, правда?
Что у нас получилось?
Каждое такое слагаемое, оно не нулевое,
потому что я взял не нулевой вектор и умножил его на не нулевой скаляр.
Мы считаем все лямбда i t различными, правильно?
Каждый такой вектор лежит, естественно, в v лямбда i t,
и я взял собственный вектор и умножил его на скаляр.
И значит у нас получилась сумма k-1 не нулевого собственного вектора
с различными собственными значениями, которая тоже равна нулю.
Этого быть не может.
Это противоречие с выбором k.
Мы сказали, что только k собственных векторов с различными собственными значениями
могут в сумме давать ноль.
А сейчас мы получили k-1 такой вектор,
и при этом это не ноль векторов, потому что мы сказали, что k больше единицы.
Таким образом, наша теорема доказана.
И что это означает?
Это означает, в частности, важную вещь.
Давайте я скажу сразу про нашу теорему.
Давайте мы вспомним еще один критерий прямой суммы.
Если взять базисы в v лямбда i t,
ну и составить их вместе, взять их инкотинацию,
как мы помним, получится базис вот этой вот суммы пространств,
то есть получится линейно-независимая система.
Я взял базисы всех этих товарищей и составил их вместе,
e1 и так далее, elt.
Первая часть это базис v лямбда 1,
следующая часть базис v лямбда 2 и так далее.
Давайте мы дополним ее до базиса
и спросим себя, любую линейно-независимую систему можно дополнить до базиса,
и спросим себя, какая же будет матрица у нашего φ в этом базисе.
Тогда в этом самом базисе у φ будет вот какая матрица.
Давайте смотреть.
Первые несколько векторов это собственные векторы с собственным значением лямбда 1.
То есть каждый из них переходит в лямбда 1 на него,
скажем там вот φ от e1, это лямбда 1 на e1.
Что это нам говорит про матрицу?
Конечно, мы должны в первый столбец нашей матрицы
поставить φ от e1, расписанный по тому же самому базису e1 и так далее, e1.
Но он расписан очень просто.
Лямбда 1 и куча нулей.
Если вектор с собственным значением лямбда 1 перейдет в лямбда 1 на e2,
то здесь будет еще одно лямбда 1 и так далее.
То есть в этой матрице будет некоторый блок из лямбда 1,
затем некоторый блок из лямбда 2 и так далее.
На диагонали будут стоять наши лямбды,
а во всех остальных местах будут стоять просто-напросто нули.
Ну и так будет продолжаться первая l столбцов, естественно.
Что будет дальше, никому не известно.
Вот такой вид будет у этой матрицы.
Но это все равно очень здорово, потому что начало этой матрицы выглядит совсем хорошо.
Ну и в частности, будет верно вот какое утверждение.
Пусть фи – это линейный оператор на пространстве размерности n,
и пусть у фи есть n различных собственных значений.
Лямбда 1 и так далее, лямбда n.
Тогда существует базис, в котором матрица фи имеет очень простой вид.
По диагонали стоят лямбда 1 и лямбда 2.
В котором матрица фи имеет очень простой вид.
По диагонали стоят лямбда 1, лямбда 2 и так далее, лямбда n, а все остальные нули.
Такая матрица называется диагональной и даже имеет специальное название.
Часто пишут диаг, лямбда 1 и так далее, лямбда n.
Диагональная матрица с вот такими элементами на диагонали.
Из того, что мы сказали, это уже должно быть, наверное, очевидно.
Как взять базис?
Взять для каждого лямбда i по одному собственному вектору, правильно?
Доказательство. Пусть i и t это собственный вектор с собственным значением лямбда i и t.
Для каждого i мы возьмем по одному собственному вектору, больше нам и не надо.
Е и т у нас все не нулевые по определению собственного вектора.
Ну и более того, е1 и так далее, еn окажется линейной независимой системой.
Это непосредственно следует вот из этой же самой теоремы, правильно?
Если мы соберем по одному не нулевому вектору из прямых слагаемых, эта система будет линейно независима.
Следовательно, раз размерность нашего пространства равна n, то это базис.
Ну и наша φ имеет в этом базисе ровно такую матрицу, как нам обещали.
Просто вот в этой вот самой матрице никакой звездочки у нас не будет, правильно?
Тут может возникнуть какое-то сомнение.
А что произойдет, если, скажем, для лямбда первого у вас будет не одномерное, а двумерное собственное пространство?
Такого быть на самом деле не может.
Потому что если в этой ситуации, когда у вас n различных собственных значений, хотя бы одно из собственных пространств окажется больше, чем одномерным,
то вот эта прямая сумма будет иметь размерность больше, чем n, правильно?
Этого быть не может.
Так что, конечно, в этом случае все эти собственные пространства будут одномерными.
И то, что мы берем по одному собственному вектору для каждого из лямбд, это никакое неупущение, больше мы взять и не сможем.
Правильно?
Давайте я тут уже сразу скажу немного более общее определение.
Вы с вами увидели, что вот в этом случае, когда есть n различных собственных значений, у нас существует базис, в котором phi имеет диагональную матрицу.
Это на самом деле очень хорошая ситуация, ее нужно как-то назвать.
Мы говорим, что оператор phi на пространстве V диагонализуем,
если в некотором базисе он имеет диагональную матрицу.
Но в частности, то есть вот что мы сказали, что если у нас есть n различных собственных значений, то такой оператор обязательно диагонализуем.
Иногда это же понятие применяют не к оператору, а к его матрице.
В таком случае, наверное, стоит сказать вот какую вещь.
Говорим, что матрица А диагонализуема, если она подобна диагональной матрице.
Напоминаю, что значит она подобна диагональной матрице.
Это означает, что она и какая-то диагональная матрица, это матрица одного и того же оператора.
Так что по сути дела, матрица диагонализуема, если это матрица диагонализуемого оператора.
Два понятия, одно и то же.
Я сразу хочу заметить, что это понятие диагонализуемости матрицы не оператора, а матрицы.
Диагонализуемость матрицы может зависеть от поля F.
Если у вас есть, например, матрица с рациональными элементами, вы ее можете воспринимать как матрицу над Q,
а можете ее, например, воспринимать как матрицу над R или над C.
Вот может так случиться, что над Q она не диагонализуема, то есть нельзя придумать рациональную обратимую матрицу,
такую, что подобие переводит ее в диагональную, а над R или на C она диагонализуемой оказаться может.
Примеры скоро воспоследуют.
Ну, например, пример я могу дать даже сейчас, конечно же, правильно?
Если у вас у матрицы есть характеристический многошлен, у которого N различных собственных значений,
и они не рациональны, то у нас уже окажется беда, конечно же.
Так, давайте мы сразу немножко поговорим, прежде чем говорить о том, что означает диагонализуемость оператора,
поговорим о том, когда она есть, поговорим о том, что она означает как раз.
Давайте мы сразу скажем, что значит, что оператор Фи в некотором базе Се имеет диагональную матрицу.
Обратите, пожалуйста, внимание, лямды у нас уже не обязательно различны, правильно?
Он диагонализуем, если существует такой базис с не обязательно различными лямдами.
Чем хороша эта ситуация?
Ну, во-первых, такой оператор имеет ясное геометрическое описание.
Что это означает?
Это означает, если мы берем произвольный вектор, берем его координатный столбец в базе Се,
это будет x1 и так далее, xn, тогда Фи от В будет иметь координатный столбец a на x,
вот это вот у нас x, это у нас, конечно, a.
Координатный столбец Фи от В получается умножением на матрицу a,
то есть это будет просто-напросто лямда 1x1, лямда 2x2 и так далее, лямда nxn.
То есть каждая координата спокойно себе умножается на свою соответствующую лямду.
Ну что это означает?
Геометрическая интуиция здесь, конечно, очень простая.
Давайте представим себе, что n равно 3 для совсем наглядности.
У нас есть 3 оси соответствующих вот этим вот 3-м базисным векторам.
И наше преобразование состоит в растяжении в лямда 1 раз вдоль первой оси,
в лямда 2 раз вдоль второй оси, в лямда 3 раз вдоль третьей оси.
То есть геометрический смысл диагонализуемого оператора у нас достаточно простой,
и поэтому имеет смысл понять, когда же оператор диагонализуем.
Ну и, значит, еще, наверное, одно, два замечания, которые хочется сказать
про диагонализуемый оператор, прежде чем мы будем говорить об условиях диагонализуемости.
Вот я там написал лямда 1 и так далее, лямда n.
Далее, в этом случае, в случае, когда оператор диагонализуем,
какой будет характеристический многочлен нашего оператора?
Ну я должен взять определитель A-XE, где A – это вот эта вот самая диагональная матрица,
ну и получится, естественно, вот что такое.
Лямда 1 минус X, лямда 2 минус X и так далее, лямда n-й минус X,
потому что это вот и есть просто диагональные элементы той матрицы,
у которой мы берем определитель, и она сама диагональна.
Напоминаю, что вот это еще вот таким вот образом обозначается.
То есть, естественно, в том случае, когда оператор диагонализуем,
на диагонале у нас будут стоять именно его собственные значения,
причем, обратите внимание, с учетом кратности, правильно?
С учетом кратности их как корней характеристического многочлена.
Ну и наконец, мы с вами можем сказать, что сумма этих лямдаитых равна следу нашего оператора,
ну потому что она равна следу вот этой матрицы, правильно?
А след любой матрицы один и тот же.
А определитель phi будет равен произведению этих самых лямдаитов.
Так, ну что ж, я разрекламировал диагональный вид оператора достаточно сильно вроде как,
много про него хорошего сказал.
Остается вопрос, когда же он существует?
Всегда ли он существует?
Вот как вы думаете, давайте я к залу обращусь.
Любой ли оператор, любое ли линейное преобразование диагонализуемо?
Матрица поворота, наверное, нет, там вообще нет собственных векторов.
Значит, давайте я даже этот вопрос немножко вынесу.
Любой ли оператор диагонализуем?
Ну, кстати, матрица поворота, она не диагонализуема не над любым полем.
Над r она не диагонализуема, а над c она диагонализуема.
На c она как раз будет диагонализуема.
И еще раз, глядите, говорят нам нет, и толстая причина для этого заключается в том,
что, например, у характеристического многочлену может не быть корней.
Если у него корней нет, то нет собственных значений, некого ставить на диагональ, правильно?
Например, характеристический многочлен может не иметь корней.
Более того, мы с вами только что увидели, что если фи диагонализуем,
то не просто корни должны быть у характеристического многочлена,
а он должен раскладываться на n линейных сомножителей, правильно?
Как только у нас есть диагональная матрица, так это вот так вот.
Или не раскладываться на линейные сомножители.
К сожалению или к счастью, мир сложнее порой, чем кажется, и это не единственная причина.
То есть я обращаю внимание, что существуют и другие причины,
по которым оператор может быть недиагонализуем.
То есть даже в том случае, когда характеристический многочлен таким вот образом на ноле раскладывается,
может оказаться, что он недиагонализуем.
Вот сейчас мы как раз начнем говорить о том, почему это может так случиться.
Так, ну давайте, наверное, я сразу дам важное определение, которое нам пригодится в дальнейшем,
а потом мы его немножко поисследуем.
Итак, определение, пусть λ, это собственное значение оператора phi на пространстве V.
То есть давайте я сразу скажу, что это значит.
Это означает попросту, что она корень характеристического многочлена этого оператора, правильно?
Так вот, тогда алгебраической кратностью этого самого лямда собственного значения лямда называется его кратность в алгебраическом смысле.
У нас есть характеристический многочлен, у него есть корень лямда, у этого корня есть некоторая кратность, правильно?
То есть кратность корня лямда в характеристическом многочлене.
Ну, то есть напоминаю, какая степень х-л делит этот самый характеристический многочлен.
А есть еще вторая кратность этого самого собственного значения.
Геометрической кратностью собственного значения лямда называется размерность V лямда.
То есть размерность вот этого самого собственного подпространства, соответствующего нашему лямду.
То есть количество линейно-независимых собственных векторов с этим собственным значением, которое мы можем выбрать максимальное.
Оказывается, что эти кратности связаны между собой. Ну и вот чтобы это доказать, давайте я сначала докажу чуть более общее утверждение.
Значит, пусть phi это линейный оператор, а u это инвариантное подпространство относительно phi.
Тогда, как мы говорили уже, можно взять ограничение phi на u и получить новый линейный оператор уже на подпространстве u.
Так вот, в этом случае характеристический многочлен ограничения делит характеристический многочлен исходного оператора.
На самом деле доказательство очень простое.
Для него достаточно выбрать правильный базис.
Давайте мы выберем базис e1 и так далее, en в v такой, что его начало, его префикс длины, скажем, k, это базис в нашем инвариантном подпространстве.
Но мы с вами знаем, что такой базис всегда выбрать можно. Нам достаточно выбрать базис в u, а после этого дополнить его до базиса v.
Тогда мы с вами прекрасно знаем, мы это уже говорили. Этот базис, как обычно, мы обозначаем через e. В этом базисе phi будет иметь следующую матрицу.
Здесь у нас будут стоять какие-то три матрицы A, B, C, а здесь будет стоять ноль.
А это не что иное, как матрица оператора psi в базисе e1 и так далее. Здесь я должен поставить базис уже подпространство u, вот в этом базисе у нас это и будет.
Это мы все уже с вами говорили. Ну и значит, что такое характеристический многочлен нашего оператора phi?
Это определитель матрицы минус хe. Как мы здесь будем вычитать хe? Напоминаю, у нас здесь вот матрица A имеет размеры k на k, матрица C имеет размеры n-k на n-k.
Поэтому, когда мы здесь будем вычитать хe, мы вычтем матрицу хe размера k на k из A. Вот первая kx мы вычтем из диагонали матрицы A.
Остальные n-kx мы вычтем из диагонали матрицы C. Поэтому здесь будет определитель вот какой блочной матрицы.
Здесь я вычел A минус хe, здесь у меня осталась матрица B нетронутая, х туда не залезли, здесь остался по-прежнему 0, а здесь будет C минус х на e.
Эти х вышлись как раз по диагонали вот этих двух матриц. Ну и следовательно, у нас получился определитель с углом нулей.
А мы знаем, чему он равен. Он равен произведению вот этих вот двух матриц, определителей вот этих вот двух матриц.
Определитель A минус хe на определитель C минус хe. Ну так и замечательно, потому что определитель A минус хe это и есть характеристический многочлен psi.
Ну а определитель C минус хe это еще какой-то многочлен.
Все, мы доказали, мы сказали, что характеристический многочлен phi есть характеристический многочлен psi умножить на еще какой-то многочлен.
Этого нам и было нужно.
Это утверждение, разумеется, полезно само по себе, но сейчас для нас пока что наиболее важным будет вот какое следствие из него.
Пусть λ это собственное значение оператора phi, тогда его алгебравическая кратность не меньше геометрической.
Доказательства очень простые. Теперь уже достаточно понять, какому инвариантному подпространству нам вот это утверждение применить.
Какое у нас появляется инвариантное подпространство, если у нас есть собственное значение лямбда?
В лямбда, конечно же. Давайте возьмем в качестве нашего инвариантного подпространства В лямбда, собственное подпространство собственным значением лямбда.
Оно естественно инвариантно, потому что каждый его вектор переходит в лямбда, умноженный на него.
Разумеется, оно инвариантно.
А кто тогда такое будет ограничение phi на это самое подпространство?
Что делает phi со всеми элементами этого подпространства? Умножает их на лямбду, это подпространство из собственных векторов.
Поэтому наша psi в нашем утверждении это будет просто оператор, который умножает все векторы на лямбду.
Ну и тогда матрица этого самого psi будет просто диагональной с лямбдами на диагонали.
И характеристический многочлен этого самого psi будет равен чему?
Лямдо минус х вентой, правильно. Мы вычитаем х по диагонали и берем определитель этой матрицы.
Конечно, получается лямдо минус х вентой.
Применяем наше утверждение, получаем, что вот этот вот самый характеристический многочлен,
то есть лямдо минус х вентой, не вентой, давайте только. У нас здесь матрица размера k на k,
n мы привыкли обозначать размером всего v, давайте мы скажем, что здесь у нас будет kt.
Итак, этот характеристический многочлен, то есть лямдо минус х в кт, делит характеристический многочлен
всего преобразования всего оператора phi.
Ну а это означает, что кратность корня лямдо вот в этом вот многочлене,
она уже никак не меньше, чем k.
То есть алгебраическая кратность лямдо, то есть кратность его как корня в характеристическом многочлене,
как корня вот этого самого характеристического многочлена, никак не меньше, чем k.
Ну а k это и есть его геометрическая кратность, это размерность v лямдо, то есть его геометрическая кратность.
И наше утверждение уже доказано.
Итак, смотрите, что у нас получилось. Для каждого собственного значения, для каждого корня нашего характеристического многочлена,
у нас алгебраическая кратность его, то есть кратность как корня, никак не меньше, чем геометрическая кратность.
И если она окажется строго больше, то вот тут-то, как мы очень скоро увидим, и наступит проблема.
Ну и давайте я сразу приведу пример того, что стать больше она может.
Давайте мы рассмотрим, забегая вперед, эту матрицу я обозначу через g.
Давайте даже так сделаем. Ну ладно, пусть будет так.
Рассмотрим вот такую вот матрицу, и давайте поймем, что у нее происходит с собственными значениями.
Ее характеристический многочлен это определитель вот такой вот матрицы.
Я учитаю x на е, поэтому по диагонали стоит минус x.
Это x квадрат. И следовательно, какие у нас получаются собственные значения?
У нас собственное значение только 0, правильно? И он алгебраической кратности 2.
λ равно 0. Это собственное значение алгебраической кратности 2.
Какая у него геометрическая кратность? Геометрическая его кратность это просто размерность ядра нашего оператора,
то есть размерность пространства решений системы с такой вот матрицей.
Это размерность пространства решений системы уравнений жина x равно 0.
И она, естественно, равна 1. Следовательно, здесь геометрическая кратность меньше, чем алгебрическая.
Ну и давайте, раз уж мы до этого дошли, я теперь сформулирую теорему,
а доказывать будем уже в следующий раз, потому что не успеем.
С высоты нашего обзора доказательство будет уже на самом деле несложным,
поэтому желающие могут понять самостоятельно, что они все это уже могут соединить.
Но в любом случае в следующий раз мы, конечно, это докажем.
Итак, теорема критерий диагонализуемости, критерии даже, потому что у нас их будет несколько.
Пусть Фи это линейное преобразование пространства В, тогда равносильны следующие утверждения.
Первое утверждение Фи диагонализуемо.
Второе утверждение, практически сразу эквивалентное этому.
В В существует базис, состоящий из собственных векторов, оператора Фи.
Такой базис часто просто называется собственным базисом.
Третье условие, то, что все пространство В раскладывается в прямую сумму собственных подпространств.
Где лямбда 1 и так далее, лямбда kt, это, естественно, собственное значение оператора Фи.
Ну то есть, глядите, что означает третье условие.
Нам уже сказали, что сумма всех этих подпространств прямая, правильно?
А тут нам говорят дополнительно, что она еще и В.
То есть нетривиальная часть в том, что эта прямая сумма равна В.
И четвертое условие, которое у меня сюда не влезет, поэтому я перейду дальше.
Оно немножко более техничное, но зато как бы наиболее непосредственно проверяемое на практике.
Характеристический многочлен нашего Фи раскладывается на линейные сомножители.
И при этом у любого собственного значения, то есть у любого, разумеется, корня нашего характеристического многочлена,
то есть геометрическая кратность равна алгебрической.
Ну то есть тривиальное следствие из вот этого, вот эта матрица диагонализуемой быть не может, правильно?
Потому что здесь есть собственное значение 0, у которого алгебрическая кратность равна 2, а геометрическая равна 1.
В следующий раз мы начнем с доказательства всей этой теоремы. Я ее еще раз хотя бы вкратце выпишу.
А на сегодня все. Вопросы, естественно, приветствуются.

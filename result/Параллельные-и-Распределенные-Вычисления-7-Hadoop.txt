Кто может сказать, какой сейчас самый большой HDD диск? Вот если один диск взять, какой у него
объем будет? Есть уже больше, есть уже 24, может быть и есть 28, но там я уже могу ошибаться. Вот
такие вот объемы. Если этих дисков несколько сделать, из них какой-нибудь raid-massive, у нас
будет под 100 терабайт и это будет один сервер. Точно так же с оперативной памятью. Сейчас есть сервера,
у которых оперативная память 2-5 терабайт, поэтому то, что было большими данными лет 7-8 назад и для чего
нужен был какой-нибудь Hadoop, сейчас для этого Hadoop на самом деле не нужен. Но когда мы выходим даже
за эти рамки, нам все равно нужна какая-то big data. Это что касается volume, что касается остальных
свойств. Следующее свойство variety — это то, что данные приходят из разных источников, с разной
периодичностью, с разного объема. И тут можно взять для примера задачу оценки кредитных рисков. Вот как
вы думаете, какую информацию он должен получить банк, чтобы понять, стоит вам выдавать кредит или
не стоит. Ну задача решается машинным обучением, но к каким источникам подключиться нужно? Там
есть что-то внутри банка, какие были у человека транзакции, какие у него есть вклады, депозиты,
вот это все внутри банка достается. Что еще можно извне банка достать? Какую информацию можно достать?
Делать что? А логи какие, что в них там хранится? А как это связано с тем,
чтобы выдавать кредит или нет? Да, но нам нужно принять какое-то решение. Вот конкретный человек,
что мы про него должны знать, чтобы понять, ему выдавать кредит или он нам его не отдаст и как бы
это будет убыток. Какие были у него до этого кредитные истории, проверка на то, какие там вообще в МПЦ. Какие,
например, данные? Кредитная история, судимость, потом еще выясняются всякие данные, которые есть
в открытых источниках. Например, семейный статус, место работы, как часто он меняет место работы,
куда он едет в отпуск. Это все при желании можно достать и поисследовать. И вот как раз вероятно,
мы видим, что данные приходят с разных каких-то источников, выглядят они по-разному. И третье
свойство velocity это то, как часто мы обрабатываем данные. То есть одно дело, если мы что-то собираем
в течение нескольких месяцев и потом запускаем какой-нибудь код, который нам генерит раз в
квартал отчет. Это как бы одна задача. И вторая задача это какая-нибудь рекламная система. Вы
наверное сталкивались с таким, что вы ищете на сайте каком-нибудь интернет-магазине, например,
я хочу купить ноутбук и через совсем маленькое время вам эти ноутбуки начинают рекомендовать
везде. Лезет реклама на всех сайтах, это происходит максимально быстро. И здесь как раз нужна не просто
обработка больших данных, а нужна real-time обработка. Ну и дальше, что нам нужно вообще для того,
чтобы такую инфраструктуру построить. Вот мы уже поняли, что у нас есть жирный сервер, на нем куча
современных дисков, куча памяти, но все равно не хватает. Что можно сделать еще? Можно масштабироваться
вертикально или горизонтально. Вертикальное масштабирование это когда мы еще больше растим
наш сервер. Если вы посмотрите на ценники, за какую цену можно арендовать сервера разных
мощностей, то увидите, что там в какой-то момент стоимость начинает по экспоненте расти. То есть
есть как бы стандартные линейки серверу, в которых цена по сути линейная, если мы конечно не берем
GPU-шки. И есть в какой-то момент мы переходим в разряд HPC, High Performance Computer, и там цена
просто становится заоблачной. Поэтому вертикальное масштабирование до какого-то момента оно хорошо,
дальше не очень, и еще мы как бы теряем в надежности такой системы, потому что сервер один,
что-то с ним случилось и все. Поэтому второй подход горизонтальное масштабирование. Мы берем
несколько серверов обычных с обычными дисками, с обычной мощностью и создаем из них какую-то
единую систему. И тут главное сделать так, чтобы мы когда работаем с такой системой, чтобы она была
нам так же удобна, как вот то, что вы сейчас делаете за ноутбуком. То есть вы можете там одной кнопкой
взять перетащить куда-то папочку, одной командой что-то скопировать, и нам нужно, чтобы система
работала так же, а у нее под капотом было куча компьютеров между собой связанных. Это кстати
отличие от MPI, с которым мы уже успели поработать. Как вы думаете, какие будут проблемы, если мы
будем заниматься горизонтальным масштабированием? Взаимодействие. Какие там риски могут быть? Да,
балансировка, синхронизация, чтобы у нас все не валилось на один сервер, 10 не простаивал рядом,
еще. Нам нужно распределить роли между машинками правильно, и еще нам нужно думать про сеть,
потому что когда у нас был один компьютер, он мог тупо сломаться, с ним что-то произойдет,
мы будем его чинить. А теперь у нас появляется сеть, и чем больше у нас эта сеть, тем чаще она
ломается. Среди вас, наверное, есть те, кто работает в больших компаниях, типа Яндекс,
МТС, Сбер, еще чего-нибудь. У вас там есть большие дата-центры по несколько, наверное,
сотен тысяч серверов. Как вы думаете, или может как вы знаете, как часто там происходят сбои в одной
отдельно взятой машинке? Если взять какой-нибудь супер ЦОД типа Сбера, как там часто машинка одна
будет ломаться? Какая-нибудь. Еще какие варианты? То есть во всем ЦОДе у тебя один раз сломалась одна
машинка за время твоей работы. А сколько ты времени там работал? На самом деле это, видимо,
говорит о том, что инфраструктура Сбера настолько надежная, что все, что там внутри ломается,
вы просто этого не видите. Ломается оно, очевидно, чаще. На самом деле берите выше несколько минут,
раз в несколько минут. Что-то происходит не так. Вот если вы возьмете любой дата-центр какой-нибудь
и посмотрите на логи того, что происходит, и там постоянно будет перегрев чего-нибудь, битые
сектора, мониторинг сигнализирует о том, что диски отвалились, и это происходит раз в несколько минут,
постоянно сыпется. Да, нам нужны хелс-чеки, и нам нужно сделать что-то такое, чтобы мы как пользователи
не видели и не реагировали постоянно на то, что происходит сбои. Сбои будут происходить всегда.
В общем, нам нужна такая система, которая вот эту всю постоянно ломающуюся штуку от нас скроет,
и мы будем спокойно жить. Такая система одна из нескольких есть. Первая такая система появилась
в 2003 году, называется Google File System. Но она, как и все такие продукты Google, она была закрытая,
можно было просто посмотреть, можно было ее там запустить, попользоваться, но посмотреть код,
посмотреть, как она реализована, было нельзя. И появилась на основе каких-то статей, которые
сотрудники Google написали, появилась система Hadoop, которую мы будем говорить. Кто знает,
почему Hadoop так называется вообще? Это не аббревиатура, это просто название. Почему он
так называется и почему у него символ желтый слоник? Потому что у одного из создателей этой системы
был маленький сынишка, у него была любимая игрушка, желтый слоник. И вот это слово Hadoop,
это первое слово, которое он произнес. Неизвестно, что оно обозначает, но вот это то, что он сказал
впервые в жизни. Ну и собственно HDFS, файловая система Hadoop Distributed File System. Давайте посмотрим,
как она устроена. Вот я уже слышал, сегодня кто-то говорил про мастер-ноду, вот она здесь у нас
наконец-то появилась. Что у нас есть? У нас есть несколько ролей, вы сейчас видите три, на самом
деле их чуть больше, но вот основные три. Первая роль это клиентская машина, вот она там маленькая
изображена слева. Это та машина, на которую вы заходите, и мы с вами будем работать в основном на ней,
то есть все остальное взаимодействие с кластером, с Hadoop будет происходить через нее. Дальше у нас
есть две роли, это узел имен и узел данных, NameNode и Datanode. NameNode это вот этот вот сервер,
который стоит в середине, и он собственно хранит имена, он не хранит данные, он хранит только такие
ссылки на то, где лежат данные, то есть это такая библиотека. На Datanode хранятся данные, но при этом
Datanode не знает о связках, то есть как эти данные между собой связаны. Данные хранятся в виде блоков,
блоки чем-то похожи на блоки в обычной файловой системе. Кстати, кто знает,
какой стандартный размер блока вот в файловых системах типа NTFS, FAT,
GIGABYTE. Бывает и так, на самом деле можно настроить и так, но если взять по дефолту,
кто-нибудь может видел. 4 килобайта, да, легко проверить. Берете какую-нибудь там флешку или диск
обычный, загоняете на нее минимальный файл, например текстурик с одним символом, и видите,
что такой файл реально занимает сколько-то байт, а на диске он будет занимать 4 kb сразу, сразу блок.
В Hadoop'е блоки тоже есть, но они сильно большего размера. Тут написано 64, у нас на кластере,
по-моему, тоже 64, в реальной жизни бывает до 512 где-то так, гигов. То есть вот такая у нас
есть архитектура. Мы с вами заходим на клиентскую машину, на ней будут храниться наши коды,
с нее мы будем запускать коды, которые будут лезть куда-то сюда. И вот на мастере у нас хранится
так называемый слепок файловой системы, то есть мастер знает о том, на каких датанодах какие файлики
лежат. Если посмотреть на эту схему, что вам здесь не нравится, какие проблемы вы здесь видите?
Да, у нас не только все клиенты, у нас и все датаноды через одну машинку работают. То есть это
то, что называется SPOV, Single Point of Failure. И получается, что у нас нейм-нода одна и она самая важная. Если
что-то с ней случится, несмотря на сохранность всех данных, на кластере мы работать не сможем вообще.
Надо что-то с этим сделать. Что можно с этим сделать? Давайте тоже разберем несколько способов. Первый
способ это просто сделать бэкап. У нас есть одна супер дорогая машинка, давайте купим рядом еще
одну такую же супер большую дорогую машинку вместе, их свяжем, поставим. Да, еще сеть нужно тоже
для постоянной синхронизации, тоже какую-нибудь там гигабитную или выше даже. То есть как бы рабочий
вариант, но дорогой. Второй способ это secondary-нейм-нода и здесь я сразу скажу, что это, наверное, самое
неудачное название роли сервера вообще в Hadoop, потому что кажется, что это вторичная нейм-нода,
что это какой-то бэкап, что он может заменить нейм-ноду, на самом деле ничего подобного.
Secondary-нейм-нода это просто машинка, которая делает ровно то, что написано на слайде, то есть она
занимается склеиванием слепка файловой системы с какими-то изменениями. То есть процесс работает так,
у нас есть слепок файловой системы, мы взаимодействуем с Hadoop, что-то туда пишем, что-то туда удаляем и
все это пишется в специальный файл, который называется edit-log. Потом в какой-то момент вот эта
secondary-нейм-нода, она запрашивает этот edit-log, плюс у нее хранится слепок файловой системы и она
делает операцию merge. И потом новый обновленный уже edit-log возвращает на нейм-ноду обратно. На нашем
кластере, на котором мы будем заниматься, это делается раз в час. Во всяких промышленных кластерах
это делается чаще. В общем, если с нейм-нодой что-то случится, то secondary-нейм-нода она заменить
ее не сможет. Все, чем она сможет помочь, это то, что на ней будет лежать вот этот слепок, причем он
будет устаревший на какое-то время, в нашем случае примерно на час, потому что edit-log последние туда
могут не успеть докатиться, нейм-нода упадет, а вот это устаревший слепок он у нас останется.
Вот, собственно, можете увидеть на схемке, как это все происходит. Вот secondary-нейм-нода, вот она
запрашивает edit-log, обновляет, возвращает. Ну и вообще файловая система, вот этот слепок,
он состоит из двух частей, собственно, сам слепок и edit-log. Это может быть физический
сервер, это может быть виртуалка просто обычная, это может быть docker-контейнер, все что угодно.
По сути, вот эти все ноды, они со собой представляют набор всяких там джарников и набор процессов.
Например, у нас на кластере secondary-нейм-нода и обычная нейм-нода находятся на одном и том же сервере.
Не делайте так в реальной жизни, но у нас это так. Вот и вот этот сервер, на котором находится
secondary и обычная нейм-нода. Вместе с датонодами, которых у нас девять штук, кажется, и плюс
клиент, все вот эти штуки, они собой представляют виртуалки, которые хостятся на одном железном
сервере, который стоит в подвале КПМ. Тоже так не делайте, потому что это по сути один железный
сервер. И реально, если что-то произойдет с этим сервером, то у нас грохнется сразу все.
Ну, когда-нибудь мы, наверное, сделаем, когда будет возможность расширить еще больше наш кластер.
Кстати, в какой-то момент у нас действительно будет новый кластер,
он будет сделан лучше, он будет больше. Не знаю, успеем ли мы на вашем потоке,
вот конкретно на ваших параллелках это опробовать. Не факт, что успеем, но к следующему к лету он точно
будет. В общем, да, у нас там будет убунта наконец-то более новая, не четырнадцатая. В общем,
сделали мы secondary name-node, но это разгрузило основную name-node, но ничем особо не помогло,
поэтому думаем, что можно сделать еще. А еще у нас можно сделать вот такие два способа,
которые в реальной жизни делается и то, и то, они одновременно применяются. HDFS Federation,
когда у нас не одна name-node, которая контролирует все, а у нас несколько name-node, каждый из которых
контролирует какой-то кусочек файловой системы, например, там папка users, папка с данными или еще
что-то. И high-availability name-node, это когда у нас вместо одной name-node, у нас еще создается
параллельно несколько штук. Чем это отличается от самого первого способа с backup-ами? Тем,
что эти ноды, они более слабые. И еще я одну важную вещь забыл сказать, что вот этот слепок,
он хранится в оперативной памяти. Для чего? Для того, чтобы мы могли быстрее с ним взаимодействовать.
Так вот, у этих stand-by-node, у них он хранится на диске. То есть, stand-by-node, они более слабые,
чем обычные. Если с обычной что-то случается, то среди stand-by-node происходит выбор лидера,
и среди них временно выбирается основная name-node. Она будет тормозить, мы будем постоянно ходить на
диск, чтобы к этому слепку обратиться, но по крайней мере кластер будет работать.
Ну и теперь, что касается data-node. Файлы на data-node хранятся в виде блоков
фиксированного размера. И для того, чтобы у нас все это надежно работало, есть механизм
репликации. То есть, когда мы пишем что-то в HDFS, мы сразу на лету это все копируем на несколько
серверов. И причем Hadoop устроен так, что эти копии он старается разместить как можно
дальше друг от друга. Вот есть у этого подхода и плюсы, и минусы. Кто может сказать какие?
Да, нужно будет очень часто и очень далеко ходить, поэтому может быть это все происходить довольно медленно.
Вот как вы можете видеть, на этой картинке у нас некоторые блоки размещены далеко друг от друга.
Например, вот C5 здесь и здесь, но C1 рядом. Я только что сказал, что мы их хотим разместить как
можно дальше друг от друга. Почему они оказываются рядом? Потому что параллельно с процессами записи и
репликации у нас еще работает такая штука как балансер. Это такой отдельный специальный процесс,
который следит за тем, чтобы на нотах была примерно одинаковая нагрузка. И если мы видим,
что какая-то нода перегружена, балансер начинает эти реплики переносить с одной ноды на другую. И
там уже мы не следим за тем, чтобы реплики были размещены как можно дальше. То есть может
быть такое, что они потом опять рядом окажутся. Ну и вообще в терминах ходупа, что такое дальше,
что такое рядом? То есть там нигде внутри ходупа не хранится какие-то метрики расстояния между серверами.
А что хранится? Хранится специальный конфиг, в котором мы указываем для каждого сервера стойку и
дата-центр. И у нас есть вместо того, чтобы мерить расстояние, мы меряем уровни локальности данных,
data-locality. Когда-нибудь слышали такое? Уровни data-locality, они бывают несколько уровней. И это не
только касается ходупа, а вообще касается любой распределенной обработки данных. Самый
первый, самый близкий уровень data-locality это процесс local. Вот вы пишете обычную программу на питоне,
у вас там какие-то переменные, все они находятся в одном процессе. И это самый такой базовый уровень
data-locality. А следующий уровень это node-local. То есть процессы уже разные, но сервер один. Дальше
rec.local. Это одна стойка. Data-center.local такого уровня data-locality нет, сразу идет уровень any. То
есть уже неважно нам, где находятся сервера. То есть вот так в совсем упрощенном виде у нас
выглядят ноды. Есть нода, на ней лежат блоки, и у каждого блока есть реплика, которая лежит на
разных нодах. Вот. И я вам только что сказал, что блоки одинакового размера, но на самом деле они
не всегда одинакового размера. И сейчас я зайду на ходуп-кластер, и мы прямо в реальном времени
сможем посмотреть, почему блоки бывают разные и в каком случае они бывают разные.
Нет. Для того, чтобы отключить интернет-долгопрудном, вам нужны root-права на каком-нибудь из серверов.
Ну и Тон, это было давно, и наверное сейчас уже так легко не получится это сделать.
В каком году это было? В каком году это было? Ну, наверное, 16-й год. То есть реально достаточно давно.
Скорее всего, если сейчас, если на уровне нашего центра обработки данных сработает монитор,
что вы что-то большое качаете, ваш аккаунт просто отключит. Кстати, кто сдавал MPI,
вы, наверное, с такой штукой сталкивались. С чем? С тем, что аккаунты на какое-то время банятся.
Вот. Как нам попасть на нейм-ноду? Заходим на кластер, пробрасываем порт 50 070 с мастера,
ну вот вы видите там команду вверху, и после этого заходим в браузер на вот этот порт.
Так лучше? Хорошо. Вот. Что мы тут видим? Давайте делаем такой небольшой обзор вот этого веб-нтерфейса.
Что мы здесь видим? Мы видим, сколько вообще хранить данных мы можем на этом кластере. Вот.
Самая первая строчка. Чуть меньше пяти терабайт, на самом деле, не много. Потому что маленькие диски
на машинках. Ну, на самом деле, для ваших заданий этого просто более чем достаточно, даже если вас
будет не пять человек сейчас, а тысяча. Вот. И дальше мы видим, сколько у нас места занято.
Сколько места занято на нон-дфс нужды, кстати, скажите, а что это вообще такое может быть? То есть
у нас есть кластер, на нем стоит ходуб. Это набор каких-то там джава-программ, жарников и сами
данные, собственно. Куда может уйти вот такое огромное количество ресурсов, если это не связано
с ходубом что-то? Логи? Ну, вот логи, которые весят полтерабайта, это как-то вообще круто для
такого маленького кластера. Да, сама система и причем не на одном сервере. То есть у нас много
нод, на каждом из них стоит операционка, стоят какие-нибудь там сервисы, которые еще нам нужны,
джедека, питон, разные библиотеки питона. Вот у нас будет домашка по спарку, там надо будет
обрабатывать данные на питоне и, соответственно, нужен будет какой-нибудь пандос. И этот пандос должен
стоять на всех узлах кластера. А пандос достаточно тяжелая штука, и там он не один стоит. Поэтому
суммарно получается вот такой большой объем. Что еще тут интересного мы видим? Вот мы видим...
Чего? Еще крупнее. Окей. Ну, больше, наверное, увеличивать смысла нет. Вы просто можете зайти
на тот же интерфейс, что и я. Вот. Здесь мы видим, собственно, наши датаноды и насколько они сейчас
все забиты данными. Видим, что они не сильно поровну, но чего-то критичного нет.
Ну, там много кто сидит. Там сидят дипломники, шестикурсники, перездача прошлых потоков.
Параллельно с вами еще на пятом курсе человек четыреста занимается, поэтому людей много.
Вот. Собственно, про блоки мы будем с вами смотреть вот здесь. Вот наша файловая система.
Находим какой-нибудь большой файл. Вот, например, файл. Ну, для нашего кластера можно сказать,
что он большой. Он почти 12 Гигов. И давайте посмотрим, как он хранится на файловой системе.
Вот у него фактор репликации 3. Размер блока у него 128. И вот мы тут видим свойства по каждому блоку.
Вот блок 0. Что мы видим? Мы видим ID-шник этого блока. Мы видим блок pool ID. В нашем случае,
на нашем кластере блок pool ID это можно сказать константа. Потому что у нас как таковой data locality
нет. Железный сервер у нас один. Ноды все равноценные. Они не помечены никакими лейблами. Поэтому
можно сказать, что у нас вот то, что вы видите блок pool, это константа, которая будет всегда одинаковая.
Дальше generation stamp это, по сути, версия. То есть вот этот процесс репликации, когда идет запись
данных файловую систему, ходу нужно каким-то образом понимать. Если он встретит версию этого файла,
какая из этих версий новее, какая старше. Вот с помощью вот этого generation stamp это и происходит.
Это не совсем прям ID-шник версии, но нечто похожее. Дальше размер и ноды, на которых блок хранится.
И давайте потыкаемся в разные блоки и посмотрим, что меняется. Вот тут поменялся ID-шник,
поменялись ноды. Опять поменялся ID-шник, поменялись ноды. Заметьте, size у нас не меняется.
Теперь давайте идем в самый конец. И здесь у нас поменялся size. Почему? Потому что просто
размер файла такой, что он на размер блока не делится. И еще есть второй случай. Вот если
мы вернемся к презе. Первый случай уже понятно почему, потому что последний блок может быть меньше.
А второй случай, если последний блок у нас совсем маленький, если он не так как сейчас, вот он
занимает где-то процентов 70 от размера блока, а если он занимает процента 2 от размера блока,
тогда мы его, мы не выделяем отдельный блок на него, а мы этот маленький кусочек прикрепляем
к предыдущему блоку. Зачем так делать? Потому что про каждый блок у нас хранится так называемая
метаинформация. Кто создатель, к какому файлу он относится, этот блок, всякие там права доступа,
где он хранится. Вот эта вся метаинформация, она конечно небольшая, она константная для
любого блока, но она хранится на неймноде и она хранится в оперативной памяти. Поэтому если мы
будем создавать много вот этих маленьких блоков, то у нас будет расходоваться оперативка неймноды,
она гораздо дороже стоит, чем диск. Поэтому если у нас последний блок очень маленький,
мы его прикрепляем к предпоследнему, а если у вас просто маленький файл и вы хотите его
сохранить в ходупе, то старайтесь не хранить маленькие файлы в ходупе. Потому что получается,
что вы не расходуете диск, у вас на диске, например, занято вместо 128 мегабайт, занято 1 мегабайт,
и диск простаивает. Но оперативка неймноды затрачивается так же, как если бы вы хранили 100 мегабайт.
Это очень такая распространенная проблема в ходупе. Кому более интересно, загуглите small
file problem in Hadoop, там даже комиксы про это есть. Да, если нужны маленькие данные,
то их можно хранить прямо на клиенте. Или есть еще такая штука, про которую подробнее расскажу
в следующий раз. Это распределенный кэш, distributed кэш. Это как раз такое хранилище,
да, оно в ходупе, но оно не в HDFS. Кому-то еще давно пришли, кому-то сегодня я запускал рассылку,
вам пришли вот такого типа письма. Может быть с другими портами, с другими пользователями,
но вот через такие реквизиты вы будете ходить на кластер.
Вот, мы переходим постепенно к чтению записи, поэтому какие-то вопросы сейчас есть по именно
устройству файловой системы. Дальше у нас будет про чтение записи и еще посмотрим всякие примеры.
Окей, вопросов нет, тогда давайте посмотрим на чтение. Как работает в ходу печатение? Оно работает
в два этапа. То есть когда вам нужно прочитать какой-то файл, где у нас хранятся данные о том,
как найти файл. Они хранятся на нейм-ноде. Поэтому мы с клиента идем сначала на нейм-ноду и нейм-нода
возвращает нам блок location, то есть где находятся блоки, которые нам нужно читать. И следующий
этап, это мы уже идем непосредственно в ту датаноду, куда нас отправили и возвращаемся обратно,
уже минуя нейм-ноду. Что ловим? Нет, блокировки только на этапе записи. Вот с записью тут вообще
все сложнее, потому что действительно есть блокировки и она еще и долго работает, потому что
запись в ходу пей синхронная. Вот посмотрите на эту схему, как происходит запись. Мы тоже сначала
идем в нейм-ноду, говорим, я хочу записать такой-то файл и нейм-нода, оценив размер этого файла,
выдает нам блок range, куда нам писать. И мы начинаем писать вот таким каскадным методом. Сначала на
первую датаноду, потом первая датанода передает данные на вторую, вторая и на третью, и пока у нас
не закончится коэффициент репликации, то есть пока мы не сделаем столько реплик, сколько хотим,
мы запись не закончим. И главное то, что пока мы вот эту длинную, длинный процесс записи не закончим,
мы не сможем работать с этими данными. То есть они у нас будут доступны только тогда, когда полностью
вся вот эта запись пройдет, а тут могут быть еще какие-нибудь падения. То есть каждая нода,
когда мы на нее пишем файл, вот видите стрелочка вверх, в нейм-ноду. То есть датанода каждый раз
отчитывается, что запись успешна. Если она не отчиталась, нейм-нода ждет. Если она не дожидается,
мы начинаем писать на следующую датаноду. То есть это может занять какое-то время. Но вы
собственно сами можете взять, выполнить какую-нибудь команду типа HDFS, dfs-put, положить файл в HDFS и посмотреть,
что даже небольшой файл он будет записываться, ну может быть, секунды три.
Давайте посмотрим, как вообще взаимодействовать с нашей экосистемой Hadoop, с файловой системой.
Подробнее вам расскажут семинаристы, мы просто сделаем такой обзор. Я вам уже показал, что у HDFS
есть WebUI, а помимо WebUI у него еще есть REST API, и мы можем с помощью HTTPS запросов файлы читать,
писать, удалять, в общем, делать с файловой системой все. Поэтому вот этот порт 50070,
он у нас наружу не открыт, потому что иначе можно было без авторизации делать что угодно
в файловой системе. Помимо REST API у нас еще есть просто обычные разные API, например, есть HDFS Shell,
вот тут есть список команд, но если вы посмотрите на этот список, то вы увидите, что большая часть
команды, она похожа на обычные команды в линуксе, там CP, MOV, RAM, CHMOD, CHOWN, все эти команды здесь
есть. Есть API на джава, но, наверное, мы сейчас на него особо останавливаться не будем. Как
показывает практика, не очень многие любят джаву. Согласен, но все равно идем дальше,
и давайте попробуем выполнить команду вот эту на кластере. То есть вот представьте,
что нам нужно прочитать первые 10 символов с какого-то файла, и вот для этого мы курлом
выполняем эту команду. Из чего она состоит? Давайте посмотрим. Мы подключаемся к интерфейсу
на имноды вот по этому порту. Дальше Web HDFS version 1, ну это чисто такая практика, когда в какой-нибудь
системе разрабатывается REST API, то сразу делают закладку под версию, потому что, может быть,
потом будет version 2, version 3, и чтобы была обратная совместимость, чтобы не убивать прошлой версии,
оставляют вот здесь номер версии. В ходу-то никакой version 2 не случилось, работаем под первой
версией. Дальше у нас название файла, и потом идут через параметры get запроса разные аргументы,
что мы с этим файлом делаем. То есть дальше есть операция открытия и чтение 10 знаков. Давайте
проверим, как это работает. Вот, команда сработала, но мы видим не 10 символов, а вместо них мы видим
какие-то логи. Кто может сказать, что тут произошло, если вы посмотрите на экран?
Ну только не данные, а просто нас куда-то перенаправили. Так, а что еще там интересного мы видим?
Да, нас перенаправили на ноду, давайте посмотрим, что выдаст вот эта ссылка, куда нас перенаправили.
То есть что мы видим по ссылке? Нода 4, вот ее интерфейс, путь к данным, операция open,
name-node-rpc-адрес вот этот, то есть мы видим откуда нас перенаправили, и дальше аргументы 10 символов,
offset 0. Вот, а теперь мы уже видим, что ответ у нас 200, и мы видим, что символы мы прочитали.
Да, на самом деле, конечно, вот так вот двухэтапный метод вручную делать не стоит, потому что у
укурла есть еще аргумент минус L. Так, сейчас я его тут добавлю. Вот, минус L большое, авторедиректы, мы тут видим ответ сразу.
Джавапи, давайте я покажу вам, у меня есть код.
Ну вот, в качестве примера код, который читает первые пять символов, да вроде пять, где тут,
вот, топ-5, пять символов с какого-нибудь файла. То есть здесь, конечно, вот этот двухэтапный
способ, он скрыт под капот, мы просто создаем объект файл-систем, подключаемся к файловой
системе и делаем что-то похожее, как мы взаимодействуем с обычной файловой системой. Если
тут есть джависты, наверняка вы работали с файлами в джаве, там все, особенно если брать более старые
версии джавы, то там все достаточно печально выглядит, чтобы подключиться к файлу, нужно
посоздавать объектов, вложенных очень много. Ну а экосистема Hadoop, она сейчас все еще живет
в основном на уровне джавы где-то восьмой-девятой. Есть уже, по-моему, 21 версия, но Hadoop все еще пока
там. 22. Ну они сейчас выходят все-таки не потому, что там какие-то новые фичи появляются, какие-то
глобальные, а по-моему, просто есть периодичность определенная. То есть теперь не нужно дергать
джавак и проходить циклом все файлы. Отлично. Ну, кстати, я знаю очень многих джавистов, которые
говорят, что начиная с версии седьмой, все умерло и работать с таким невозможно, поэтому мнения
бывают разные, но вот даже Hadoop уже довольно давно отошел от седьмой версии. Вот, ну теперь если
посмотреть на то, как выглядит HDFS Shell, то по сути это просто линуксовые команды, которые вы перед ними
пишете префикс и немножко дольше ждете, чем в обычном линуксе. Какие еще есть обертки? Вот есть
обертка на C++. Когда-то HDFS и сам Hadoop был сильно популярен в компании IBM, и там сделали набор
оберток под язык C++. И есть очень много питоновых библиотек. Согласен. Чего? Какой-то вопрос? Нет,
просто непонятно, как можно в такой системе с питоном жить. В такой системе с питоном... В такой
системе зачем питон? Понятно, за тем, чтобы люди, которые просто хотели писать пук-пук три команды...
Питон на самом деле тут везде, потому что вот ты, наверное, хочешь сказать, что Hadoop сложный и на
нем тяжело писать код. Но при этом уже давно есть понимание, что большие данные, где больше всего
применяются большие данные? Это... Чего? Ну и искусственный интеллект, если широко брать. То есть
там и data science, машинка, всякая биоинформатика, направление типа умный дом, рекомендательные
системы, вот это все большие данные. То есть там везде нужен анализ данных. С чем до Hadoop,
с чем работали data scientists, data analytics? Это, по сути, питон. Pandas, Matplotlib, NumPy, вот эти вот основные
библиотеки. И есть понимание, что для того, чтобы вот эта вся сложная система стала популярна в
большом бизнесе, надо, чтобы это все умело работать с питоном. Поэтому Hadoop и Spark, и Hive, все,
что мы будем изучать, оно умеет работать с питоном. И, собственно, здесь есть тоже вот разные
библиотеки. Я привел четыре, хотя их на самом деле намного больше. И вот библиотека HDFS CLI,
она самая простая в плане работы именно с HDFS. То есть если вы ее настроите, на семинарах вам покажут,
как ее настроить. Там определенный конфиг нужно сделать, небольшой. И вы сможете с Hadoop
взаимодействовать так же просто, как просто с обычным файлом. То есть we is open и поехали.
Вот, например, есть EmerJob. Это такая мощная библиотека, которая умеет MapReduce задачи запускать,
умеет с HDFS работать. То есть вам вообще не нужна будет никакая Java, никакой Bash,
если вы будете работать с EmerJob. Но у нее довольно сложные порог хождения. То есть когда я начинал
с ней работать, там довольно тяжело было ее настраивать и вообще понять, почему она
все время падает. Поэтому на нашем курсе EmerJob мы с ней работать не будем, а вам семинаристы покажут
другие инструменты. Вот, ну и если у вас сейчас с собой ноутбуки, вы можете в принципе проделать
сами вот эти команды и убедиться, что мы реально работаем с разными файловыми системами. То есть вы
зашли на сервер, сделали ssh-mipt-клиент, и у вас при этом есть одна файловая система,
которая просто обычный Linux внутри сервера, и есть HDFS, которая совсем другая файловая система и в
которой лежат другие данные. И вот можно даже увидеть, какой коэффициент репликации у каждого
файла, если вы выполните вот этот du-h. Кто знает вообще команду du, что она делает? Она и в обычном Linux есть.
Ну да, если есть su, то больше ничего не надо.
Это ты имеешь в виду завершение циклов, Баша?
Диск usage, du.
Так, может сейчас какие-то вопросы есть, мы как-то довольно быстро идем.
Ну вот давайте тогда немного поисследуем файловую систему с помощью FSTK. Опять же вопрос,
FSTK это вообще что? Кто знает, что это такое? Безотносительно Hadoop.
Да, а ЦК? А у кого-нибудь было такое, что вот на ноутбуке какая-то проблема с диском,
вы запускаете какой-нибудь чекер, чтобы проверить битые сектора или там какие-то ошибки исправить?
На винде есть сервис проверки дисков тоже.
Но она не то что чинит это утилиты, она просто мониторит какие ошибки есть,
то есть собственно File System Check. И в Hadoop тоже есть File System Check. Он не столько для того,
чтобы увидеть какие ошибки, потому что в Hadoop, если мы берем реальный Hadoop систему,
там всегда что-нибудь не так, всегда каких-то не хватает блоков, потому что какой-нибудь сервер
лежит и заменить его еще не успели. Скорее FSTK нужно для того, чтобы поисследовать,
где какие блоки хранятся, как это все вообще сказывается на файловой системе.
Давайте такую команду проверим.
Вот что у нас лежит в этой папке, мы сейчас для нее выполним вот эту вот команду.
Так, вот у нас получилась такая штука, это Summary. Давайте листать наверх,
листаем, листаем, листаем, листаем, долго листаем и давайте посмотрим, что у нас вообще тут есть.
Нет, это у нас ничего не сломалось, это просто файл большой и блоков много.
Ну это просто данность. Мы сейчас будем смотреть, это хорошо или не очень хорошо.
Что мы тут видим? Сначала мы видим, что у нас тут вот такая папка, вот такой файл и мы его
начинаем исследовать. Нет, это просто папка в HDFS, которая вики, в которой лежит кусочек википедии.
Вот такой размер у нас файла, 92 блока и дальше мы начинаем про каждый блок чего-то узнавать.
Что мы тут видим? Блок Pull, как я уже сказал, это константа, потом у нас идет ID-шник блока,
потом идет версия блока, потом идет его размер, количество реплик 3, вот LiveReplex,
DataNodeWithStorage, вот он тоже есть и здесь мы видим про каждую реплику информацию. Что мы видим?
Мы видим IP-шник, это IP-шник датоноды, на которой лежит блок, потом мы видим ID-шник,
это собственно ID-шник тоже датоноды, почему два идентификатора? Потому что IP может меняться,
вообще может меняться все в Hadoop Cluster, может поменяться IP, может поменяться hostname,
а Hadoop Cluster при этом должен работать, поэтому нам нужен ID-шник. Дальше у нас идет, насколько
видишь, вот идентификатор диск, но это собственно обозначает, что данные хранятся на диске,
потому что они могут храниться не только на диске, а еще допустим в докере, еще в каком-нибудь
S3-хранилище, в оперативке, но вот у нас диск и вот такую штуку мы видим по каждому блоку.
Листаем, листаем. Размер у нас все время одинаковый, кроме вот…
Получается вопрос по тому, что мы собрали блок в профессии, но только теперь записано вот таким
обзором. Здесь больше информации. Да, вот видите размер поменялся. Здесь больше информации,
потому что вот мы видим вот эту вот статистику. Over-replicated, under-replicated, то есть если у нас не все
в порядке с репликами. Вот, кстати, кто говорил, что это хорошо, что у нас много выводов, хорошо
это вот здесь, когда тут стоят нули и нет никаких проблемных блоков, потому что вот under-replicated,
ну понятно, упала нода, вместе с ней пропала реплика. А вот это я у вас хочу для начала спросить.
Реплика, можем поменять. Нет, они удаляются. Они сразу удаляются, запускается процесс
удаления. И over-replicated, это не то что у нас как бы больше фактор репликации стал, а это именно
отличие от того, что мы хотим. Вот мы хотим, чтобы было три, а у нас реально хранится четвертая реплика.
Вот, какие варианты? Откуда она может взяться? Гарбич коллектор? Да, только он с данными напрямую
не работает. Он работает с... ГЦ, он удаляет всякие объекты в оперативной памяти, которые
хранятся в java-процессах, а это данные, они лежат на диске. Я ничего-то не видел, чтобы гарбич коллектор
стирал что-то с диска. А не может быть так, что там сборы по сети? Так. И из-за этого получилось так,
что мы дважды создали реплику. То есть мы начали создавать реплику, сеть обрушилась, нам надо
создать новую. Мы пошли на новую, теперь у нас четыре реплика. Именно, да. То есть это может быть сбои сети, это
может быть какие-то временные выпадения машинок. Когда машинка выпала, hard-бит Наимноду переслать не
успела, Наимнода решила, что машинка упала и создала реплику где-то еще. Потом машинка проснулась и говорит,
у меня тоже реплика есть. И получается, у нас лишняя реплика. В таком случае у нас все равно
через какое-то время запускается балансер. И балансер, помимо того, что он двигает реплики между
собой, чтобы сбалансировать, он еще и удаляет лишнее и создает, если что-то не хватает. То есть он
исправляет проблемы, если его правильно настроить, он исправляет проблемы с under-replicated и over-replicated.
Бывает просто такое, что у нас реплика есть, но мы к ней не можем подключиться, допустим.
Ну и можем точно также выполнить команду для какого-нибудь одного блока и посмотреть
информацию про него. Вот это мы здесь видим примерно то, что мы видели в веб-интерфейсе,
то есть где хранится блок, к какому файлу он относится, на каких нодах и какой у него статус.
Теперь давайте мы с вами попробуем решить задачку, которую часто решают ходуб админы. И вообще те,
кто хочет в компании создать ходуб, его допустим еще не было, но стоит задача, чтобы в компании
ходуб появился и понятно, сколько данных мы там хотим хранить. Вот нам нужно спланировать
нейм-ноду. Давайте посмотрим на условия. То есть у нас объем всех дисков кластера вот такой,
размер блока, метаинформацию и фактор репликации мы тоже знаем. И давайте мы оценим минимальный
объем оперативки. Это, кстати, тоже вопрос, почему мы не можем сказать точно, каким он должен быть,
почему мы только оценить можем. Давайте считать, что мы рассчитываем оперативную память только
полезную для хранения вот этого слепка файловой системы. То, что у нас сверху там еще есть операционка,
Java, GC обязательно. Вот это мы как бы сейчас учитывать не будем. Вот именно для хранения слепка.
Как нам это посчитать? Это 2 петабайта, да? Да, 2 петабайта. А также сколько? Это после? Тера, потом пета, да.
Получается на блок у нас 600 байт, оценим 1 килобайт. Это, короче, можно так, погрешность,
можно сказать 64. Минимальный? Ну ладно, 65 мегабайт на 1 блок. У нас на 1 блок делаются 3 реплики,
значит у нас 64 нужно умножить на 4. Почему на 4? Ну типа файл, еще к нему 3 реплики. А, реплики именно
создаются учитывая этот файл, то есть всего 3. Ну оценим как на 4, чтобы умножать на 2. Ну это 2 в 2, это 2 в 6, 2 в 8, 128 мегабайт.
Ты просто расскажи последовательность действий, как ты считаешь, то есть считать в уме петабайта не нужно.
У нас есть блок. Мы считаем, насколько у нас на каждый блок тратится память. Это размер блока плюс метод информации.
На каждый блок у нас реплики, поэтому нам нужно умножить на количество реплик. А вот это все, это что еще раз?
Сейчас я скажу. Нет, мы нашли сначала блок. Вот размер блока. У нас есть размер 7 диск. Нам нужно
посчитать количество таких блоков, которые было найдено. Главное учесть, что на вот этих 2 петабайтах
мы должны хранить не только сам блок, а его реплики тоже. Нам же их больше негде хранить. Так.
Умножить на 3, размер диска разделить на вот это число, так? Да, число блоков получим.
4 килобайта это блок в обычной файловой системе. А у нас метод информация вот написана.
Вот такая у нас получается штука. А теперь скажите, в каком случае будет вот такой вот идеальный
вариант? То есть у нас есть 2 битабайта. Нам принесли кучу данных, разложенных как попало,
как нам их правильно упаковать, чтобы минимизировать затраты на вот нейм-ноду.
А как мы можем неравномерно распределить? Если мы напрямую, вот когда мы что-то пишем в ходу,
мы напрямую не влияем на то, как оно по блокам будет распределяться. Наше дело просто там HDFS,
запулили в ходу. И нам нужно понять, как эти файлы правильно упаковать, какого они должны быть размера.
Каждый файл, получается, нужно делать кратный размер блока, чтобы его можно было нормально,
оптимально положить. Не было так, чтобы один блок заходил там 1 мегабайт. Да, то есть это должно
быть или равен блоку, или кратен размер блока, или такой отдельный вариант, что у нас весь файл
размера 2 питабайта. Если у нас огромный файл какой-то, в ходу сам разберется, как правильно его разложить.
И еще такой вопрос со звездочкой, а можем ли мы сделать так, что память на нейм-ноде будет
еще меньше, чем тут написано. Тоже не надо говорить про джаву, про оптимизацию к ГЦ и вообще
работу с операционкой. Именно с данными мы можем вот как-то их еще лучше упаковать, так чтобы затраты
были меньше. А какая разница, они все равно туда лягут в ту же систему.
И куда мы будем кэш складывать?
Принципе ок, но вот опять же если мы считаем, что данные мы можем только перепаковывать, как-то резать,
то есть сжать мы их и сделать, чтобы глобальная сумма была меньше, мы не можем. А вот я минут 20
назад говорил, что есть случай, когда вот размер блока, а после него идет маленький кусочек блока,
который прикрепляется к предпоследнему. Если помните, такое было, что вот есть файл,
мы его разбили на блоки и последний блок обычно меньше. Если он сильно меньше, то чтобы не расходовать
оперативку на им-ноды, мы вот этот маленький кусочек прикрепляем к предпоследнему блоку и
предпоследний блок становится чуть больше. Так можно здесь на этом сыграть и разбить файлы так,
что у них будет размер не 64 мегабайта, а допустим 64 и 5, ну то есть еще какой-то маленький кусочек.
И тогда ходу эти маленькие кусочки прикрепит к блоку и у нас будет размер блока чуть больше,
будет не 64, а больше, и соответственно оперативки тут будет немного меньше. Это
будет совсем чуть-чуть, скорее всего даже не ощутимо, но вот такой способ есть.
Есть какие-нибудь вопросы сейчас?
Это действительно редко используют, просто желательно, чтобы вы как бы знали,
что такое есть, а по сути на разных ходу кластерах вот этот порог может быть маленький,
плюс его довольно тяжело задавать, он задается не в конфигах настройки, а он задается в самом коде
ходу. Возможно в последних версиях это поправили, надо посмотреть, но вообще если
брать вот ходу версии 2.6, с которой мы сейчас работаем, то там этот порог он внутри кода,
и если мы хотим вот этот порог поменять, нам надо пересобирать все. Где-то это пересобирают,
где-то нет, где-то эту функцию вообще отключили, поэтому это просто вот такая штука,
ее интересно знать, если на собесе спросят, тоже можно ответить, будет круто.
Еще какие-нибудь вопросы, может быть есть?
Окей, тогда на сегодня осталось только два момента. Первое, я вам хочу показать,
как эти файлы хранятся на самой датоноде и на неймноде. Более подробно вам покажут на семинарах,
но я вам сейчас просто структуру этих файлов покажу. Если мы выполняем какие-то команды вот такого
типа, то мы видим обычная файловая система, вот есть файлы, у них есть какие-то метаданные,
где они хранятся. Давайте пойдем для этого на ноду. Вот мы оказались на ноде. Это тоже обычный
сервер под линуксом, неважно, как он устроен технически, это может быть docker-container,
это может быть виртуалка, кстати, у нас это виртуалка, но все равно это линукс. Где тут найти ходуб?
Есть в корне папочка DFS, здесь у нас есть датонода, и меня сюда не пустят, поэтому я получу
рута. Вот датонода, дальше вот наш блок pool, он у нас один, вот наши блоки, и вот тут в папке
finalized можно найти блоки, которые на этой ноде хранятся. Как они хранятся? Давайте посмотрим. Это просто
вот набор папок, сабдир, сабдир, сабдир. Как они мапятся с реальными папками файловой системы,
вот эта датонода об этом ничего не знает. Если мы зайдем в какой-нибудь сабдир,
какой-то у нас сабдир 74, допустим. Что мы тут видим? Мы видим тут опять сабдиры, то есть вот эти
вот сабдиры безымянные друг в друга вложенные, и если мы по этим сабдирам погуляем, внутри будут
храниться сами блоки, но блоки будут собой представлять просто бинарные файлы,
которые как бы название у них будет равно айдишнику. Давайте посмотрим. Скорее всего будет ошибка просто.
Вот сабдир 102, тут что-то есть.
Там, так как это не какой-то исполняемый файл, то понятно, что ничего тут не будет.
Давайте посмотрим.
Тут, смотрите, тут еще какая-то псевдографика, то есть мы тут все даже прочитать не сможем.
Это, кстати, может быть лог, потому что часто сам Hadoop, он свои же логи пишет и сохраняет в HDFS.
Я же не знаю, что это за файл мы открыли, мы просто какой-то рандомный блок открыли.
Да, интересный вопрос, почему так?
Скорее всего, это связано с тем, что мы можем положить в HDFS в том числе исполняемые файлы,
и вот если вы зайдете в HDFS, посмотрите просто разные команды, то вы увидите,
что большинство файлов, у них там права как бы RX сразу. И для папок, и для файлов. Нет различий.
Сам Hadoop его выполнить не сможет через файловую систему, а вы можете зайти на ноду и что-то выполнить.
На самом деле, в Hadoop подобных системах у них с безопасностью очень много всяких
дырок, потому что когда такие системы разрабатывали, думали про отказоустойчивость,
надежность в плане вычислений, но не в плане безопасности. Поэтому в компаниях,
если есть Hadoop, то поверх него накручивают очень много продуктов, связанных с безопасностью,
как минимум. Kerberos всегда есть. Ну и давайте еще посмотрим на нейм-ноду.
Вот смотрите, что мы тут видим. Мы тут видим папку SNN и папку NN, потому что конкретно на этом
кластере, как я уже сказал, у нас и нейм-нода, и секонд и нейм-нода находятся на одной машинке.
Ну соответственно перестанет работать файловая система.
Вот давайте посмотрим, что у нас есть. У нас есть FS Image и есть файлики edit, edit, edit. Вот посмотрите,
каждый час у нас появляются новые эдиты и потом они переходят на секонд и нейм-ноду,
там мержатся и появляется новый FS Image. Вот мы тоже видим эти файлы.
Нет, они тут хранятся. Они удаляются из оперативной памяти, то есть вот эта вся штука,
точнее не вся, а вот FS Image актуальный, он хранится в оперативке, а на диск он скидывается уже в виде
бэкапа. Это где? Ну это просто файл с версией текущего слепка файловой системы. Это в общем
что-то типа того, как вы, возможно, видели какая у вас есть какая-то программа, и там есть файл,
который называется PID, и там записан ID шник текущего процесса, в котором она работает.
Вот здесь записана точно также версия.
Обычно ставят, если у нас какая-то директория, то на нее ставят права RX,
потому что один просто X, ну в реальной жизни смысла нету ставить права на
исполнение, потому что нам надо как минимум прочитать.
На сегодня последний слайд, вот несколько дополнительных статей,
что еще можно прочитать про HDFS. Вот первая статья, что в действительности делает secondary
name-node. Вот картинки, которые я вам про нее показывал, как раз брал отсюда, и тут можно подробнее
прочитать. Потом вот статья про архитектура HDFS, это то, что я показывал вам последние 5 минут,
и то, что на семинарах вам покажут подробнее, только еще в более развернутом виде. Вот такой сайт,
он довольно страшно выглядит, потому что компании Hortonworks вот этой, которая ввела блог, нету. Есть
только вот такая вот китайская копия этого сайта. Сейчас она откроется. Ну очень медленный интернет,
можете отдельно кликнуть по ссылке, посмотреть. В общем, там будет описание вот этой структуры,
которая внутри папок DFS DN, DFS NN, вы там все это сможете увидеть. Дальше еще одна статья уже более
такая научная теоретическая от Константина Швачко, это один из коммитеров сам ходуб, и там просто
описано, почему HDFS работает именно так, с пояснениями, доказательствами, с формулами. И книжка
The Definition Guide, вот глава номер 3 про HDFS. Книжка достаточно старая, ей уже наверное лет 9,
но она интересна тем, что там про каждую из самых популярных ходуб-систем хотя бы по чуть-чуть
написано. То есть HDFS, ходуб, Spark, Hive, HBase, вот эти все системы написано, как они устроены,
для чего нужны, где применяются. Тут такое в качестве обзора можно почитать.
Вот, на этом на сегодня все. Какие-то вопросы есть?

Давайте посмотрим вот сюда. Сначала с рок-кривой разберёмся, с ней на самом деле всё очень просто,
если её ещё раз посмотреть. На всякий случай. Рок-кривая, да? Мы с вами варьируем по рок,
который у нас есть. Вот я для примера пока тут нарисовал маленькую выборочку. Вот у нас там
есть объектов. Сколько? Раз, два, три, четыре, пять, шесть, семь, восемь, девять объектов. Соответственно,
вот вероятности, которые мы ему предсказываем нашим классикатором. Вот это наша P+. Хорошо? А
это соответственно Y, то есть истинная метка класса. 1 либо 0. Согласились? Пока всё понятно. Для
удобства, так как у нас все объекты независимы, можем их отсортировать таким образом, чтобы у
нас убывала вероятность положительная. Окей? То есть как её можно построить на самом деле на
практике? Ну вот, вот так мы построили. И соответственно, на самом деле площадь под кривой нам будет показывать
вероятность, что из случайно заданной пары объектов, вот взяли пару объектов случайно, и они у нас
будут упорядочены по вероятностям классикатора также, как и по своим меткам класса. Вот что он нам
показывает. Что у нас правильно отсортировано будет. Можем вот так вот построить и просто-напросто
двигать порог вверху. То есть сначала у нас порог вообще нулевой, точнее порог где-то единица, туда
ни один объект не попал. True positive rate соответственно у нас равен нулю, правильно? У нас никто не попал
в положительные объекты. False positive rate у нас тоже равен нулю, у нас никто в отрицательные объекты
тоже не попал. Согласны? Вот мы с вами вот здесь сидим. Дальше соответственно мы начинаем порог
понижать. Теперь у нас порог вот здесь проходит. Соответственно теперь у нас один объект попал в
положительный класс. В отрицательный класс пока у нас никто не попал, правильно? Ну значит вот
True positive rate у нас вырос, так на всякий случай тут у нас TPR, тут у нас FPR. Вот он у нас вырос,
мы попали вот сюда. Теперь мы можем сдвинуть на один, но надо сразу сдвигать на два, потому что у нас
два объекта с одинаковой вероятностью. Следующий порог. У нас получается один объект попал в
отрицательный класс, один объект попал в положительный класс. Согласны? Значит True positive rate
подрос на единичку, false positive rate тоже подрос на единичку. Ну соответственно вот мы попали сюда и сюда.
Вот оно у нас. Примерно вот сюда, короче мы попали. Ой, простите, я наврал не туда.
Вот сюда мы с вами попали, вот оно у нас. Едем дальше, теперь опять 0,6 порог идем вверх, 0,5 порог
опять идем вверх, 0,4 порог идем вправо, 0,3 опять вправо, 0,2 вверх, 0,1 вправо. Теперь опять же порог
у нас меньше 0,1, все объекты отнесены уже куда? В положительный класс, правильно? Соответственно True positive rate
у нас равен единице, все объекты попали в положительный класс. False positive rate тоже единица, все
объекты отрицательного класса тоже попали в положительный класс. Вот ваши все точки, вот мы с
вами построили наш урок кривую. И соответственно вот тут задавали вопрос, а что, где гарантия,
что у нас True positive rate растет быстрее, чем False positive rate? Ну на самом деле гарантия лишь в том,
что если наш классикатор лучше, чем случайный, то есть он хотя бы как-то упорядочивает объекты
одного класса относительно другого, то у нас будет кривая выше диагонали. Если он хуже, то понятное
дело ничего хорошего не будет. И собственно почему мне этот график нравится? На самом деле это
иллюстрация, ну авторство ее принадлежит Александру Дяканову, который много где преподавал,
работал в МГУ, много чего рассказывает. У него есть очень классный блок анализ малых данных, я ссылки на
самом деле сегодня после семинара скину. И отсюда, кстати, очень хорошо видно, что такое площадь под
кривой краски, под урок кривой. Вот же мы ее нарисовали, это у нас единичный квадрат, тут у нас была долика
краски положительного класса объектов, тут отрицательного. Заметьте, тут четыре краски,
вот четыре объекта отрицательного класса, тут пять делений, пять объектов. И вот эта вот штуковина под
кривой. Мы с вами краску покрываем какой-то множество квадратиков. Каждый квадратик это что? Это пара,
какой-то объект отрицательного класса, какой-то объект положительного класса, верно? Это и показывает
вам вот сколько у вас пара объектов правильно ориентирована относительно истинных метокласс.
То есть насколько хорошо ваш классикатор умеет ранжировать ваши объекты так, чтобы классикатор,
объект, который положительного класса, имел большую вероятность, чем объект, который отрицательного
класса. Уловили? Да, такое бывает, то есть она может какой-то вот такой вид иметь. Это нормально,
тут нас волнует именно площадь под кривой. Если у вас площадь меньше чем 0,5, значит у вас
вероятность, что объекты ориентированы правильно, меньше чем 0,5. Поменяйте метки классов,
вероятность опять будет больше чем 0,5. Только и всего. Ну значит, что у вас в среднем объекты
случайным образом ориентированы, только и всего. То есть у вас при каких-то порогах, типа вот здесь
она вроде растет лучше, вот здесь наоборот хуже, но в среднем у вас классикатор случайным образом
относит порядок объектов, предсказывает порядок объектов, только и всего. Так, понять не стало срок
кривой, что это такое? А, вот этот наклонный. На самом деле, потому что можно было нарисовать один вверх,
один справа, но непонятно как, потому что у нас два объекта, которых вероятности одинаковые,
а метки классов разные. Соответственно у нас и TPR вырос, и FPR вырос, поэтому мы пошли по
диагонали. Ну опять же, это на самом деле нормально, почему? Предположим, вообще выраженный случай, у вас два
объекта, один положительного класса, другой отрицательного, вероятность для них одинакового.
Тогда у вас должна быть диагональная прямая, потому что у вас вероятность правильно их отранжировать
50 процентов. Размер шажочка, вот это, по сути, да, это труп-пост-фрейд, это один объект относительно
всей выборки положительных объектов. Тут, допустим, их пять, поэтому здесь пять карасушков.
Уловили, нет? Раз, два, три, четыре, пять, шесть, семь, восемь, девять. Пять единиц, четыре нуля.
Да, здесь по ноль двадцать пять, то есть каждый шаг, по сути, это доля выборки. Один объект,
в данном случае, 25 процентов отрицательной выборки и 20 процентов положительной выборки.
Все. Так, ну чего, стало понятней? Да, да, да. Так, смотрите, отранжировал, шел порогом сверху вниз,
соответственно, для каждого порога была точка. Если непонятно, то я вот скину там,
протестовала статья написать. Но, по сути, вот вы идете сверху вниз по отранжированным вещам,
если единичка вверх, если нолик вправо. Но это такой дубовый вариант. Плюс, опять же, у меня сегодня
много фраз вопроса собеседований, это нормально. Про линейную классификацию очень любят гонять на
собеседованиях в условных крупных компаниях, поэтому лучше это, чтобы вас от дубов отскакивало,
если вы пойдете туда собеседоваться. У меня в свое время когда-то завалили вопросом про урок
кривую для многоклассовой классификации. Я вот пытался понять, как ее можно построить. Ответ?
Никак, для One versus Rest можно для всех построить, для каждого класса отдельно. А что-то там
многомерное придумать, пока такого нет. Вот, итак, вопрос по желанию есть?
3, 2, 1. Хорошо. Так, у кого сегодня ноуты есть? Классно. Но я тогда предлагаю немножко покодить,
потому что с ноутбуками на самом деле сегодня будет достаточно любопытно. Мы с вами,
недолго думая, сейчас залезем в самый замечательный PyTorch и все наши линейные классикаторы
напишем уже на нем. Так что открывайте ноуты, я пока закомечу все, что надо.
Вот квадратик – это у вас все возможные пары. Лосик под кривой – это отношение правильно
отсердированных пар ко всем. ТПР – это доля правильных ответов относительно всех
положительных. А ФПР – это доля неправильных ответов про положительные относительно всех
отрицательно. Итак, коллеги, удобно. Но с телефона разве что кодить неудобно. Смотрите,
если с телефона предлагаю, тогда просто на самом деле посмотреть. Уже, я думаю, разницы,
я думаю, не будет. Хорошо, коллеги, в репозитории появился нужный нам, соответственно, пример. Я,
наверное, запущу вообще все локально, потому что локально оно будет работать чуть постабильней.
Смотрите, тут в гет есть, там в мл курсе опечатка, там нижний прочерк вместо дефис. Я глазами
менял ссылку, поэтому ошибся. Короче, смотрите, собственно, о чем на самом деле сегодня у нас
пойдет речь. Речь пойдет в двух, скажем так, шагах про две различные истории. Во-первых,
мы с вами поговорим про PyTorch в целом, потому что это крайне классный фреймворк. Если кто-то из
вас приверженит CTEF, это абсолютно нормально. Они, в принципе, сопоставимы друг с другом. Просто
для учебных целей, по-моему, предпочтительнее PyTorch, он привычнее, он понятнее. Во-вторых,
мы с вами поговорим про линейную классикацию и посмотрим, как можно построить линейный классикатор
на коленке. Более того, по факту мы с вами сегодня уже маленькую игрушечную нейронную сеть построим,
потому что что такое нейронная сеть, как не просто линейная модель на спироидах. Сегодня у нас
спироидов не будет, поэтому будет просто линейное отображение плюс сегмоиды в конце. Но тем не менее,
все остальное работает. Более того, сегодня мы с вами вспомним чуть-чуть о том, какие бывают
проблемы, когда мы работаем с градиентной оптимизацией и почему может быть полезным
нормировать ваши данные. Ну что ж, давайте потихоньку начинать. Все, у кого есть ноутбуки,
рекомендую прям вместе со мной кодить. Там кода понадобится три строчки. Все, у кого ноутбука
нет, внимание на экран. Если что-то не видно, пожалуйста, говорите. Принято? Хорошо. Итак,
ну всякие дифферам бы пои торчу, я с вашего позволения петь сейчас не буду. Если коротко,
то с NumPy вы уже знакомы. По сути, PyTorch это эдакий NumPy на максималках, который умеет
работать со всяким GPU, умеет автоматически читать градиенты, умеет из коробки строить кучу,
ну не строить, а давать вам в качестве строительных блоков кучу различных операций,
которые гораздо удобнее взять оттуда, чем руками реализовывать и так далее. Ну и в принципе,
у него огромная комьюнити. И с недавних пор, кстати, PyTorch теперь находится под крылом не отдельной
крупной корпорации, а под крылом Linux Foundation, по-моему, но точнее, аналогичной ему PyTorch Foundation.
Так что, чистый open-source, наше все. На самом деле, в отделе исследования и образования я считаю,
что это большое добро, когда ни один, грубо говоря, крупный игрок не имеет прямого влияния на тот
или иной инструмент. Конечно, в свое время и PyTorch и TensorFlow сильно поднялись за счет того,
что их гигант IT-индустрии поддерживали. PyTorch, соответственно, запрещенная в России
организация. TensorFlow не запрещенный пока в России Google. Но сейчас, скажем так, они, я так понимаю,
перевели его под крыловое сообщество, что в принципе хорошо. Ну а развиться им это помогло. Итак,
ну что ж, пара, наверное, слов. С NumPyme, я думаю, все работать умеют. Вы можете вывести Shape,
чего-нибудь, вы можете сложить все поэлементно, вы можете там использовать всякие дот-продукты,
операции и так далее. На всякий случай, предупрежу вас, в NumPyme можно дот-продукт даже
с матрицами использовать, несмотря на то, что вроде как с точки зрения теории дот-продукт
определен только для векторов. Как правило, framework может включать себе пачку в библиотеку,
но вообще в данном контексте я бы сказал, что ни в чем. Просто иногда обдывают по-разному.
Просто PyTorch все-таки называют обычный deep learning framework, хотя библиотека для каких-нибудь там
построений граф-учислений в принципе тоже подходит. Но хотя Torch, как правило, включает
себя допустим там, как его называют, короче. Ну с точки зрения структурирования именно кода,
по сути там несколько прям отдельных пакетов, но на самом деле как в NumPyme тоже. Тут надо в
терминологии его право углубить. Короче, я надеюсь NumPyme все пользоваться умеют,
тут комментарии излишни, правильно? Пожелания есть какие-то у вас? Три-два раз? Тишина. Ну ладно.
Плюс PyTorch в том, что вам не придется переучиваться. С PyTorch вы можете делать абсолютно все то же
самое. У вас даже синтаксис меняется практически никак. Вот я почему заговорил про дот-продукт на
самом деле. В PyTorch себе таких вольнский не позволяли, поэтому здесь используется нормальный
матмул, matrix multiplication, который в NumPy есть, он в принципе работает. Тут стоит помнить, что некоторые,
но редкие, но тем не менее некоторые ключевые слова и функции в PyTorch в NumPy работают по-разному,
поэтому, как всегда я говорю, читайте доки. То есть не надо думать, что вы сели за новый фреймворк и
начали писать на нем свободно, хотя с PyTorch почти так и выйдет. Короче, они похожи друг на друга,
исключительно в этом была суть. Но есть небольшие разницы. Например, Bshape переходит во Vue. На самом
деле тут даже стоит сказать, что Vue именно пытается строить Vue. В чем отличие Vue от построения новой
матрицы? Понимаете? Да, то есть на всякий случай, если вдруг у кого там BD не было или чего. Когда
вы строите Vue, у вас данные все еще лежат в памяти тем же образом, вы просто определенным образом их
считываете. У вас нет дюпликации данных. Если вы именно строите новый тензор, новую матрицу с
другой формой, вы именно передаписывать данные. В NumPy? В NumPy, наверное. Просто в PyTorch тоже есть,
и не всегда PyTorch может построить Vue. Он тогда сам об этом говорит с ошибкой и предлагает
именно перестроить ее заново. Я про это предупреждаю. То есть Vue работает в PyTorch не всегда,
иногда у вас размерности может быть какие-то неправильные. Он вам об этом скажет, попросит так
не делать. Во-вторых, самая большая разница и что руки постоянно забывают, постоянно приходится
себя поправлять, это Exus становится Dim Dimension. Ну и типа Danuhup и Torch соответственно тоже свои.
В принципе, почитать Doki ничего страшного. Но давайте для начала что-нибудь тогда красивенькое
находим, просто чтобы было красиво. Тут вы как понимаете, пока ничего не написано. Собственно,
давайте вот эту формулу сверху нарисуем своими руками. Ну я думаю, здесь комментарии излишние.
Просто чтобы начали писать на PyTorch. И коллеги, на всякий случай, я думаю, что у вас уже много было
курсов по программированию и так далее. Но лучше, когда вот у вас такие функции используются,
лучше импортировать их, как бы сказать, не from Torch import cos, а просто импортировать Torch и
дергать Torch cos, чтобы вам явно было видно, а главное не вам, а всем остальным, откуда именно функция
Cosinus используется. Потому что есть Cosinus в математическом пакете, есть NumPy, есть SyPy, есть PyTorch,
есть многое еще. И, вторых, пожалуйста, никогда не используйте импорт звездочка, ладно? Вот за это
прям можно по рукам получить где-нибудь в коллективе. Импорт звездочка не делайте.
Ну скажем так, если вы искренне понимаете, что вы делаете, вы можете делать что угодно,
но в принципе это правило хорошего тона, импорт звездочка не делать. Знаете, дзену... читали дзен
Питона. Многие читали, правильно? Кто не читал Zen of Python? Ну вот вы сейчас в Питоне сидите,
напишите import this. Ну, собственно, явно лучше, чем не явно. Вот это ровно о том, что вы говорите,
как бы в своих файлах по идее можно, но лучше явно что-то импортируйте, потому что импорт звездочка,
вы не знаете, что вы импортировали. Вам туда сосед написал какую-то гадость,
оно вас тоже импортировалось. Не надо так. Вот так. Полтора на синус. Ничего осмысленного в этом нет,
просто возможность нарисовать красивую кривулену, используя уже PyTorch. Все. И так, чтобы вы начали
что-то с вами руками. Но собственно, в чем плюс PyTorch? В принципе, когда мы с вами пишем какую-нибудь
формулу, ну, например, я там не знаю, х квадрат, логарифм, х в третьей степени на логарифм 15х,
не знаю, просто вот так написал. Вот эту штуку продиференцировать все мы с вами можем. Но,
благо, дифференцировать все умеют. Но при этом, скажите, вот вам охота руками это считать,
особенно если у вас там в вашей модели такие функции 15 штук? Обычно нет. Ну, если кому-то хочется,
пожалуйста, я не возражаю. А? Вы намекаете на то, что здесь фон белый или что? Не, не, коллеги,
здесь разве что вас будут ругать, если вы будете осенью подписывать. Это как бы, ну, если это
осмысленные оси, здесь нет никаких осей, это просто две координаты. Но если у вас условно там
зависимость пресижена от реколла, то ось надо подписывать. А так, как бы, ваши графики должны
быть читаемые. Причем, ну, как бы не только вам условно можете проверять на соседей. Если соседу
читаемый, значит все нормально. Вот. Так вот, короче, дифференцировать это руками лень, банально.
Благо, дифференцирование, операция простая, сильно проще интегрирования, поэтому можно научить
вот эту всю технику делать это за нас. Поэтому PyTorch имеет в основатической дифференцировании не
он один, умеет Jax, умеет TensorFlow, умеет Тиана, которая уже давно почила ее в себя, поглотил
TensorFlow по сути, ну и так далее. Поэтому давайте на это посмотрим. На всякий случай я сейчас пару
вещей еще расскажу про течение градиентов, потому что это важно. Смотрите, есть вот этот флажок,
ну не флажок точнее, а атрибут requiresGrad, требует градиентов. Он ровно отвечает за то,
что у тензора есть требование посчитать для него градиенты. На всякий случай в PyTorch и в TensorFlow
на самом деле, вот TensorFlow, все вот эти многомерные матрики обзывают тензорами. Никакая тензорная
алгебра на них в общем случае не задана, это просто многомерные матрики, но их обзывали тензорами.
Хорошо? То есть просто опять же, так же как метрики классификации, к метрикам из линейного алгебра
не имеют никакого отношения, но их так обозвали. Так вот, для каждого тензора мы это можем придумать.
И собственно, что здесь можно сказать? Давайте я сразу тогда чуть-чуть нарисую, что-нибудь красивое.
Вот у вас есть какой-нибудь граф вычислений, что такое граф вычислений? Вы каждую формулу можете
на самом деле представить в виде графа. Ну, например, вот у вас есть x, вот у вас есть y,
вот у вас есть z. У вас соответственно пусть x и y, операция над ними плюс, потом здесь
соответственно операция над ними умножение. Вот здесь у вас соответственно результат. Как
можно это записать в виде формулы? Ну, это у нас получается x плюс y умножить на z. Согласны? Мы
любую формулу можем представить в виде графа вычислений. Собственно, что значит, что кто-то
требует градиента? Значит, что мы попытаемся посчитать для него градиент. Например, вот у нас
с вами есть r, и мы с вами уже откуда-нибудь знаем dr. Точнее, как мы с вами хотим посчитать dr по
dx. Ну, например, вот мы хотим с вами посчитать. Нам для этого что понадобится посчитать? Нам
понадобится вспомнить производную сложные функции и посчитать, собственно, промежуточно. dr по dr
понятно. Потом вот это можно обозначить q, и dr по dx. Это что? Это dr по dq на dq по dx. Правильно?
Собственно, градиенты у нас посчитаются вот здесь, вот здесь, вот здесь и вот здесь. Согласны?
Коллеги, да, это сложение, это умножение. q это просто промежуточное значение, вот здесь я его так
обозвал. q, а, хорошо, да, лишнее, простите, вот это давайте я к вам зову. Вот, да, результат сложения,
спасибо. Вот, собственно, вот мы посчитали. И, по сути, у нас градиенты потекли отсюда сюда, отсюда
сюда, отсюда сюда. Все понятно, правильно? То есть, если мы с вами бы сказали, что y тоже
хочет градиенты, мы бы их и сюда посчитали. По z, например, мы не просили градиенты, если мы здесь
проверим z град, то он нам скажет, что ничего там нет. То есть, каждый раз, когда у вас есть какая-то
формула, у нее есть какие-то вершины этого графа вычислений. По сути, это всегда граф вычислений,
по сути, даже это всегда дерево, потому что мы ждем, что у нас одна будет корневая вершина,
мы за нее можем подвести. У нас результат, как правило, это значение чего? Функции потерь,
мы функции потерь дифференцируем по параметрам, у нас текут градиенты. Вы можете явно указывать,
кому нужны градиенты, кому нет. Вот, например, можем с вами это явно сделать, ну, например,
там a это torch once 5 и он требует градиенты, а b, соответственно, это torch и он не требует
градиенты. По умолчанию никто не требует градиентов. С это a dot b и, соответственно,
смотрите, вот c у нас чему-то там равен. Ну ладно, 0 с плюс 2. Вот он чему-то равен. Мы можем
теперь попросить backward у него. Backward это как раз-таки дергаем рубильник, посчитать производные
для всего, что хочет посчитать для себя градиенты. И после этого, смотрите, вы можете проверить,
что у a градиенты, например, появились, вот, это производная c по a. Логично, что производная
скаляра по вектору будет иметь ту же размерность, что и вектор. Согласны?
Нет, смотрите, проще. У вас есть, как бы, вот у вас есть множество переменных, все они типа
тензор. У каждой переменной есть атрибут, хочет ли она градиенты. Соответственно, либо она их
хочет, потому что вы это явно задали, либо это промежуточная переменная, и так как она является
суммой, короче, эта переменная зависит от той, которая хочет градиенты, она автоматом тоже хочет
градиенты. Все. Потом вы от любой, на самом деле, функции, которая возвращает вам скаляр, можете
запросить backward, то есть продиференцировать ее по всему, что было до нее. Получит градиенты,
и то есть граф будет вот так считаться. А что такое две функции подряд?
Нет, погодите, вы имите, а потом вы a умножить на b, это у вас была функция 1, вы посчитали от нее backward,
потом a умножить на c, это функция 2, вы от нее тоже дернули backward. Не совсем так, градиент тогда от нее
накопится, ну вот давайте сделаем, вот собственно. Да, на самом деле могу объяснить, почему d это a,
например dot b на 2. Вот. Да, это вы тоже сейчас, давайте дойдем, пока вот давайте проследим. Смотрите,
вот у нас две функции, да? Вот, для начала можем проверить, градиента a нет, градиента b нет.
Дернули первую функцию backward, градиенту a появился, градиенту b не появилось. Давайте теперь сделаем d тоже backward.
Градиенту a уже стал 6, это как раз-таки кто? Это градиент от первого и от второго сложен. Зачем
это надо? На самом деле надо это с точки зрения банального удобства при оптимизации. Предположим,
что у нас с вами выборка, там не знаю, из 10 объектов, и мы не можем все 10 объектов одновременно
прогнать через нашу машину и сложить от них градиенты, усреднить. Ну просто банальной памяти,
например, хватает, у тебя не 10, а 10 миллионов. Тогда мы можем сделать что? Мы можем посчитать
градиент от первой пятерки, от второй пятерки, от третьей и так далее. Получается, мы одну и ту
же функцию на разных объектах запускаем. Градиент краски у вас тогда просто-напросто накапливается,
суммируется, чтобы его потом могли при желании усредить. Или, что проще, вот у вас две функции,
например, функция потерь и функция регуляризации. У вас градиенты должны идти от обеих, поэтому они
накапливаются, они не перезаписываются. Уловили? Вот как обнулить? Есть путь человеческий, есть
нечеловеческий. Сейчас покажу. А чего там про Египиан говорили? У кого? Здесь вы, по сути,
тоже можете Египиан и Гессиан посчитать совершенно спокойно. Смотрите, а у вас в данном случае
функция всегда это либо линейное преобразование, либо какие-то нелинейные преобразования. У вас и
параметры и данные, на самом деле, это просто входные данные. Просто параметры у вас требуют
градиенты, а данные не требуют градиентов. Так удобно. Ну и давайте от краски разберемся
на практике. Ух ты! Они в очередной раз что-то там убрали. Я понял. Лу от Бостон скоро вырубится.
Я понял. Хорошо. Ладно, пока работает. Ребят, мы изучаем этические проблемы в датсенсе сегодня.
Короче, давайте построим пока что на пальцах на низком уровне нашу простую модельку,
пока даже линейную регрессию. Собственно, вот наши параметры омега и б. Они требуют градиентов,
это просто скаляры, но тем не менее скаляры это просто одномерный тензор. Нормально. x и y это,
соответственно, наши данные. Видите, это тоже тензоры, абсолютно те же самые, но они градиентов не
требуют. Данные это датасет, цена жилья в Бостоне в зависимости там от кучи каких-то параметров. Мы
взяли просто последний столбец, по-моему это площадь, что ли, или удаленность от центра,
что-то такое. Неважно. Вот у нас специальная картинка нарисована. Вот такую зависимость мы
будем пытаться описать. По оси y, соответственно, целевая переменная, по оси x наш признак. Все,
вот наши данные. Вот наша модель. Да, пока делаем регрессию, пока все просто. Вот наша модель
омега x плюс b. Вот наша функция потерь. Все, видите, явно пишем просто формулу. y пред минус y в
квадрате. И среднее потом считаем, средняя ошибка. Все, посчитали, посчитали градиенты. Вот у нас
градиенты появились. Для омеги и для b. На всякий случай для x мы никакие градиенты с вами не
видим, потому что нет градиентов, он их не просит. Вот. Ну и, соответственно, давайте теперь обучим
нашу модельку, каким образом. Градиентным спуском. У нас каждый раз есть градиент, мы можем обновлять
параметры по антиградиенту, получать результат. Тут весь код абсолютно тот же самый, просто заново
написан. Омег и b реализовали. Нормально. Вот посчитали лоз. Вот наш градиентный шаг, learning
rate 005. Минус равно, то есть вычитаем. И вот как мы это за нуляем. Но, опять же, сейчас мы, по сути,
с вами на низком уровне, и мы залезли немного в потрохар всех тех функций, поэтому мы обращаемся
к методу именно 0 с подчеркиванием нижним, что это на самом деле значит. Все методы с нижним подчеркиванием
по и торче на конце, это inplace метода, то есть мы inplace что-то заменяем. То есть в данном случае
мы говорим, так, градиенты, конкретно их данные, потому что сам град это сложная структура,
за нули. И вот рисуем картинку. Что нечеловеческое? Есть человеческое, мы до него сейчас доберемся.
Да. Ну, в смысле, он не то чтобы нечеловеческий, он, если вам надо конкретно написать какую-то сложную
функцию и не использовать никакой сахара по и торча, никаких удобств. Вот кривуляна у нас постепенно,
видите, подгоняется. Но кривуляна у нас на самом деле пока прямая, потому что мы с вами задли явно
линейную зависимость. Вот она. Так что, собственно, вот вам еще маленькая задачка на 30 секунд. Ну,
поменяйте каким-нибудь образом вот эту зависимость, чтобы она была не линейная от x и перезапустите.
Короче, давайте сделаем модель лучше все-таки, подходящей к данным. Мы явно видим, что зависимость
не линейная. Давайте что-нибудь сделаем. Вы делить на x? Ну, давайте поделим. Ну, вот,
получилось неплохо, например. Ну, c тогда тоже будет параметром, или что? Ну, тогда нам придется,
да, добавить, соответственно, c равняется опять же вот эта штука. Ну, давайте. Я не знаю,
что получится, если что. Минус 0,05 на c град data. Ну, давайте попробуем. Ну, со степенями надо
быть аккуратнее. Нет, он просто улетел куда-то в зону неопределенности. Вот так предлагаете?
Давайте. Вот тоже замечательно подходит. Заметьте, да и неважно, на самом деле, лучше-хуже, да,
логарифм здесь вообще идеально подходит. В чем, на самом деле, прикол? Заметьте, мы ничего в коде
не меняли, мы просто саму формулу нашу поменяли, и все. Весь подсчет лоса, все градиенты обновления,
если у нас новые параметры не появились, все работает так же. У нас просто меняется, по сути,
целевая функция, целевое отображение, и все. И в этом, на самом деле, прелесть вот этого автодифа,
что мы с вами просто задаем нужное отображение, а все эти функции потери, оптимизаторы и так
далее, понятное дело, их уже давно написали, вы можете написать свои, и потом ими из коробки
пользоваться. Вот. Понятно? Тут вопрос-комментарий есть? Хорошо. Чего? Смотрите. Нет, погодите,
если не поняли. Вот ваша формула, вот ваша линейная модель, вот ваша функция потерь.
Все. Вы можете поменять линейную модель на какую-то нелинейную, вам не придется при этом функцию
потерь переписывать какие-то аргументы, менять вообще ничего не придется делать. То, что у вас
функция потерь, она отдельна. Ваша модель, она отдельна. Вы можете ее поменять, граф вычислений
у вас все равно подчиняется всем тем же правилам, работать по той же самой опишке. Вы можете написать
любую подходящую модель. Подходящая в смысле, что у нее на входе та же размер, на выходе та же
размер. Все будет работать. В этом вся прелесть, что вам не нужно об этом думать теперь. Вам не
нужно руками это все писать. Это удобно. Точка дата, потому что когда вы просто говорите... Короче,
возможно сейчас уже даже так будет работать. Давайте проверим. Две звездочки где? Это в степень.
Две звездочки это в степень. Потому что вот эта штука, она не распознается. Короче, дата это
обращение непосредственно к данным, которые в тензоре лежат. Вы с ним можете что-нибудь сделать.
То есть конкретно к области памяти, где они записаны. Вот. Возможно сейчас уже даже без даты
работает. Я честно сказать не знаю. Раньше вроде не работало. Ну, слушайте, я на пейторче вроде 16
года пишу. Так что, если есть какие-то старые старообрядские штуки, прошу прощения, это просто
сила привычки. Вы с торочного грата имеете в виду? Чтобы он в грав-почисления не попал?
А, ну, кстати, да, возможно. Короче, когда вы лезете вот туда под капот, вы уже выходите за
пределы вот этой штуки. Вы просто явно обращаетесь к каким-то данным. Торч про это,
грубо говоря, не знает. Он говорит, я посмотрю в сторонку. Ладно, коллеги, у меня теперь к вам на
самом деле есть маленький вопрос. Тут он уже ниже написан, но тем не менее. Вот тут стоит какое-то
волшебное число, какое-то волшебное число делить на 10. Внимание, вопрос зачем? А почему на 10,
почему не на 20? Бинго, пожалуйста, никогда так не делайте, потому что тут специально даже про это
написано. Данные поделены на 10, какая-то волшебная константа. Что будет, если не делить на 10 весь тот
же самый код, но, соответственно, без деления? Ну, вон она там немного попрыгала, как вы видели,
и теперь у нас lost none. Градиентов нет, ничего нет. Можно, на самом деле, посмотреть на график
градиентов. Ну, собственно, они у нас сейчас построены просто в естественном масштабе. Давайте
посчитаем, NP log делаем. Вот, видите, в логарифмическом масштабе норма градиента растет линейно. То есть
градиент у нас рост экспонентально. Что происходило? У нас с вами для линейной модели,
какая получается производная ω и х? dL под ω, ну, dy под ω будет равна х, правильно? Если у вас х очень
большой, вы получаете большую производную на ω, сильно меняете ω, у вас меняется loss, от этого
вы получаете сильную производную от loss, плюс опять же большая производная будет на ω от х, опять
сильно меняете и так далее. В итоге у вас просто-напросто расходится ваша оптимизация того,
что слишком большая была увеличена градиента. Это лечится либо нормировкой ваших данных,
либо нормировкой градиентного шага, либо что хорошо и тем и другим. Но так как градиентный
шаг пока у нас какая-то константа, это еще один пример, почему полезно нормировать ваши данные.
Не только потому, что это позволяет вам КНН правильно использовать и регуляризация вам в
жизни портит, у вас еще и получается все нормально с градиентной оптимизацией.
Вот, откуда брать нормировку? Давайте отнормируемся, собственно, давайте скажем,
что у нас торч. Фу, х это что? Давайте просто-напросто сделаем классическую стандартизацию. Что это такое будет?
x равно x минус x мин, xs равно 1, xs равно 0, точнее. А, Дим, да, вы правы, мы уже в пайторче.
Делить на xstd, Дим равно 0. Это стандартное отклонение, корень из гисперсии. Вот, пожалуйста,
теперь у вас ничего не упадает, потому что вы теперь все загнали примерно к нулю. Короче,
у вас нулевое среднее стандартное отклонение равно днице гарантирует вам гораздо более
устойчивую работу почти везде. Мы данные отнормировали, но у нас градиенты напрямую приходят из данных,
потому что производная по z будет xмсу. Вот, поэтому частная производная у вас стала меньше по абсолютному
значению, и вы можете красить. Нормировки данных, ну, есть функции для нормировки данных, но вызывать
вам ее все равно придется. Нет, ну, есть условно, есть условно, это там, ну, в скалерне есть, вон,
например, пром скалерн, припроцессинг, импорт, стандарт скейлер. Вот, пожалуйста, можете его
дергать, и, соответственно, у вас будет краски средней дисперсии, на средней дисперсии нормироваться.
Но главное не забывайте, пожалуйста, про нормировку. Очень часто из-за этого возникают проблемы,
которые приходится потом краске, если про это не вспомнить, чинить подгоном learning rate, каким-то
подбором стартовых весов и так далее. Это все боль, данные отнормировали, стало сильно лучше.
Стандартизация – это минус средний поделить на дисперсию, но это стандартный способ нормировки.
Есть еще мин-макс нормировка, например, опять же, вы вычитаете среднее, но делите на разницу между
минимум и максимум. Есть еще много чего. Короче, вот. А давайте подумаем, почему с логарифмом беда?
Не, потому что у вас логарифм от отрицательной величины – это что? У вас логарифм считается
от чего-то с нулевым средним, поэтому как минимум половина х будет отрицательная, поэтому беда.
Ну да, тогда у вас х должен быть строго положительным, чтобы вологрифмировать,
иначе у вас краски выпадет надо. Короче, вот так. Окей, красивые штуки. Короче, красивый кадр,
на самом деле, из фильма, по-моему. Опять забыл. «Семи самураев». Спасибо. Классный фильм, 51-го года.
Рекомендую. Вот. На самом деле, здесь неспроста эта гибка, собственно. Будьте аккуратны со всеми
этими тензорами, потому что даже на простых моделях можно себе замечательно выстрелить в ногу,
например, не отнормировав данные. Тем паче, когда у вас все те же самые, по сути,
преобразования используются с нейронной сетью на 50 слоев вот такой вот, там проблем будет кратно
больше. Поэтому начинаем с простого, идем к сложному. Ну а теперь давайте всем те же самые
позанимаемся, но используя что-то верхнеуровневое. На всякий случай, если у вас нотмнист не
импортируется, значит у вас краски тот код в гетом на коллабе не сработал, потому что там
ссылка неправильная. Хорошо? Да, на всякий случай, коллеги, когда вы видите где-то импорт чего-то,
вы, пожалуйста, проверьте еще, что не должно быть в корне такого файла, потому что первая реакция
у многих людей, когда видит, что импорт не проходит, PIP инсталт что-то там. В данном случае у нас
локальный файл, который мы должны импортировать. Нам не нужен PIP инсталт нотмнист, я не знаю,
что еще там в PIP по индексу нотмнист лежит. Вот. Не, вот смотрите, первая строчка, вот здесь была
опечатка, тут MLCORS было написано через нижнее подчеркивание, а не через DEFICE. Поправьте,
пожалуйста, тогда все и запустите, соответственно, ее раскомментируйте и запустите, тогда заработаю.
Вот. Короче, что такое нотмнист? Есть такой классный датсет MNIST, я не помню,
как это расшифровывается. Короче, это датсет из 9-10 цифр, от 0 до 9 рукописных. Это классический
датсет, собранный, по-моему, Яном Лекуном, но мою хуйну врать. Кто-нибудь помнит? Ладно,
предположим. Предположим, на один из просто основных датсетов, ранее используемых для
классификации, 10 рукописных цифр, там всего 60 тысяч примеров, по-моему, по современным меркам,
это все малюсенькое. Но, тем не менее, это хеллоуолд в мире работы с сетками, обработки
изображений и так далее. Соответственно, нотмнист, это, как можно догадаться, не мнист, но, короче,
это датсет, который содержит в себе уже 10 букв, причем в различных стилях написания,
различных шрифтах на латинице от а до, соответственно, десятой букв алфавита,
я не помню. И наш задачи точно так же их опознавать. Таких датсетов, на самом деле, достаточно много.
Есть fashion minutes, где предметы одежды и пикселизованные, предлагаются там всякие туфли, футболки,
кофты и так далее. Короче, наша задача научиться отличать вот а от б, а в прыжке 10 классов,
все 10 букв друг от друга. Пока что мы с вами не умеем работать с изображениями по-нормальному,
но вспомним первое занятие изображения, то есть матрица черно-белая, может быть,
вытянуто в вектор, согласны? Я согласен. Поэтому пока что мы с вами делаем абсолютно по-дурацки,
берем матрицу, вытягиваем вектор, работаем далее как с вектором. Понятное дело, что ничего хорошего
в этом нет, сдвиг на 1 пиксель уже сильно меняет все, что происходит, но, словно, если мы вот эту
букву а на 1 пиксель вправо подвинем, у нас сильно поменяется векторное представление, при этом
букву а мы все еще будем видеть. Именно поэтому сверочные сети и трансформеры хорошо подходят
к обработке изображений, но об этом будет дальше, пока мы до тут не дошли. Ну а теперь, собственно,
отвечая на вопрос, как делают, скажем так, по привычке. Используют верхние уровни WiPyTorch. Там
есть, во-первых, модуль, который называется NN-модуль, от которого наследуются примерно все
преобразования, которые есть в PyTorch. NN-модуль обладает классными свойствами. Почему? Потому
что NN-модуль написан, у него классный абстрактный класс, но он, по сути, NN-модуль это является
абстрактный класс для всех модулей. На всякий случай. Все помните это такое, да? Да, вот. Что
такое абстрактный класс, понятно? То есть все модули, которые отвечают за любые там линейные
преобразования, свертки и так далее, они наследуются от NN-модуля и, по сути, многие методы уже определены
в NN-модулях. В чем на самом деле плюс? Во-первых, всегда вот, читайте доклад, она даже специально здесь
скрыта. Во-вторых, если вы хотите свой модуль написать, допустим, вот у вас какая-то новая
Uber свертка или какой-нибудь там новый Fast Fourier Transform, неважно, что-нибудь вы придумали,
что вам надо дифференцировать и так далее. Автоматически. И чтобы оно еще в PyTorch
строилось быстро. Вы можете взять, отнаследоваться от NN-модуля и реализовать всего два метода внутри
вашего класса. Первое это конструктор, он должен быть, чтобы вы задали, что там нужно внутри. И
второе это метод forward, то есть ровно как он должен считаться. Прямой проход. Все остальное, если у вас
внутри не используется недеференцируемая операция и не используется что-то не из PyTorch,
например, вы не засудили numpy посреди кода из PyTorch, тогда все остальное PyTorch делает за вас,
и вы точно также сможете вызывать backward от своего модуля и так далее. Вам ничего не
надо писать руками с градиентами. Более того, вы можете его примотать изолентой к коду, который
уже был в библиотеке, и все будет работать. Нет, погодите, модуль это название вот этого абстрактного
класса. То есть любой модуль, смотрите, вот, например, если вы хотите какую-то функцию реализовать,
в смысле, в смысле математическом, не в смысле программирования, то вы можете сделать класс,
который односленного от модуля, который исполняет ровно то, что вам надо, и вы можете только написать
там конструктор, и как она считается на прямом проходе, то есть не градиенты ее считать,
а только как из входа получить ответ. Тогда градиенты выхода по входу она вам сама будет
считать. Это удобно. backward это ровно вот вы от любого скаляра можете дернуть рубильник,
и он тогда пойдет по графу вычислений по всем путям до листьев, которые требуют градиенты.
По умолчанию backward работает только у скалярных, у функций, у которых скалярный output. То есть выход
функции должен являться скаляром, тогда backward работает. Потому что иначе непонятно,
что с производной делать. Делать какой размерности, усреднять или что. То есть вы
можете на самом деле попросить его вежливо, что вам нужно производную вектору по матрице,
чтобы получить трехмерный тензор. Но по умолчанию вам за это по рукам дадут,
и не дадут это сделать. Пайторыч просто вам ошибку выдаст. У вас в код пятимерный, а выход
скаляр. Вот смотрите, вот вам простой пример. У нас все еще сохранены вот наши a тензор,
вот b тензор. Вот давайте a плюс b, c равно a плюс b, c backward. Вот, пожалуйста, он нам говорит,
что можно неявно создавать граф вычислений вот этот вот только для скалярных output. Если вы
хотите для векторного output это делать, вам нужно явно уже задавать, куда какие производные пойдут,
как они будут насыряться и так далее. Ну короче, скажем так, вам это очень долго не понадобится в
тот момент, когда вам все-таки понадобится все производные вектора по вектору в пайторче. Именно
явно я имею в виду. Вы уже будете достаточно опытны и с тем, чтобы это написать без проблем.
Не-не-не, погодите. Вы, смотрите, у вас модуль может совершенно спокойно выдавать
любую величину. Двумерную, трехмерную, неважно. Вопрос лишь в том, что у вас в конечном итоге,
вот, производная идет dr по dx, r скаляр. Вы не можете посчитать отдельно dq по dx,
потому что тогда у вас производная будет краски многомерной для всех величин. Просто у вас
получается всегда при дифференцировании у вас производная, допустим, этого поэтому на это
поэтому, если два оператора в другую руку применить, будет того же размера, что и x. То есть у вас
вызывается backward всегда скаляра. Промежуточная может быть что угодно, вам абсолютно нормально.
Что? Так, ну вот, смотрите, вот у вас, например, вот, кстати, да, я же говорил, любое отображение,
то есть вот целая модель, например, написано, смотрите, там используются свертки, но свертка,
короче, предположим, какое-то линейное преобразование в каком-то смысле, плюс нелинейное. Вот у вас есть
модель, вы сказали, какая она, сказали в каком порядке ее применять, потом она у вас предсказывает
какое-то число или, допустим, метку класса, вероятность положительного класса, и вы потом от нее еще
считаете, допустим, log loss. Потом вы у log loss дергаете рубильник, посчитай backward, он идет сюда и
считает производную для всех вот этих вот матричек за линейное отображение отвечающие. Понятно, нет?
Так, давай свертку забудьте, вообще она будет у нас на десятом занятиях. Она просто там в доках
напитана, поэтому я про нее сказал. Не хватало еще сейчас вас допутать. Так, ладно, я думаю,
пример на практике всегда лучше, мы сейчас это с вами сами напишем, будет понятно. Смотрите,
собственно, вот наша задача построить ту самую логистическую регрессию, которую мы с вами
сегодня разбирали. Вот она, вот наша сегмойда. Собственно, что мы можем сделать? Можем с вами
написать через nn-модуль, можем написать по-простому. Давайте сначала напишем по-простому,
потом через nn-модуль. Собственно, что такое логистическая регрессия? Это линейное отображение
поверх него сегмойда. Согласны? Вот, тогда давайте, во-первых, добавим модуль nn-linear,
вот на всякий случай, можно его посмотреть. Ладно, я забыл, как это делается. А, dir, спасибо.
А, не, это его сигнатура, это все, что у него есть. Вот, все. У него есть метод, вот этот приватный
док. Вот, классно, спасибо. Короче, вот, пожалуйста, есть линейные краски метод nn-linear,
который по факту является чем? Он ровно отвечает за умножение на матричку. То есть,
у вас на входе, допустим, размеры на стен, на выходе к, он внутри явно порождает матричку
m на k и свободный член размера k. Все. Ну, давайте его краской построим. Вот наша
своя модель, она отображает 784. Почему 784? Потому что у нас 784 пикселя, потому что это 28 на 28,
это размерность картинки. Вот. В 1, 1 это что? Это одно число, это будет наша краски вероятность.
Пока что это просто какая-то скалярная штука, ее обычно называют логитом. То есть,
вероятность до того, как она попала под сигмоиду или, в общем случае, под софтмакс,
это в многомерном случае, называют логитом. Хорошо? Логит. Лоджит, логит. Вот. И, соответственно,
в конце мы с вами ставим сигмоиду. Вот. Та же самая сигмоида, она называется NN-сигмоид. Ну,
понятно. Все. Можем посмотреть на параметры нашей модели. Вот в ней матричка есть 11984. Ну,
логично. И один свободный член. Все. Ну, давайте проверим, что наша модель работает. Просто на
практике. Возьмем первые три объекта х. Что? Не-не-не. Погодите, у сигмоида есть разные параметры?
Ну, а линейное отображение как, в общем случае, сдается?
Омега х плюс б. Если я здесь сделаю, например, двойку, то здесь будет два. Видите? Вот. Ну,
соответственно, вот, запускаем нашу модель. Просто на игрушечном кейсе проверить,
что все размены совпали. Запустили. Вот у нас три предсказания. Собственно, вероятность для
первого объекта, второго и третьего. Согласны? Чего? Нет. Погодите, у вас три объекта, для каждого
из них предсказанная вероятность. Они независимые никак. Вот. Ну и, собственно, вот у вас теперь есть
модель. Осталось понять, как нам теперь из предсказаний получить функцию ошибки, потому что мы именно
функцию ошибки будем минимизировать. Ну и скажем так, тут есть два путя. Я вам честно скажу,
я вас честно прошу, вам охота сейчас вот эту функцию потерь расписывать? Пайторче. Мне тоже
неохота, поэтому я предлагаю сделать так. NN binary. Нет. Нет ее там что ли? Вот binary cross entropy.
Поздравляю. Ну вот, да, это binary cross entropy. Я говорю, в пайторче за нас уже вот это все написали.
Ну мне сейчас тоже не очень охота, на самом деле, писать вот эту страшную формулу. Что? Ну да,
можно. Смотрите, окей, можем написать, просто сейчас я поставлю обучаться, могу руками написать,
увидеть, что то же самое. А, что там написано? Торч. Обычно вот здесь это все можно делать.
Дока у пайторча очень приятная, поэтому вот. А, бце лос, она вот называется. Вот она, как
считается? Пожалуйста, вот она же, самая написанная. Вот binary cross entropy. Соответственно,
от кого у нас? Чего? Не, нам здесь просто нужно посчитать. Не, не, лос надо вызвать. Смотрите,
на всякий случай вот с cross entropy очень опасная вещь, которая часто происходит, потому что люди
привыкают работать со средней квадратичной ошибкой с МСЕ. МСЕ, как вы понимаете, симметричных,
потому что это парабола. Вы можете из первого члена второго вычислить, из второго первого,
разницы особо нет. Властно? Иначе не будет одинаковое. С cross entropy такого не работает,
у вас там p log q, поэтому, пожалуйста, читайте доку, что подавать сначала, что подавать потом.
Y predicted и Y. Вот и, по идее, лос у нас должен быть. Вот так cross entropy проверять не будем,
а лос-то можно и попроверять. Вот, даже ассерты.
Ладно. Ассерты были написаны, чтобы руками читать. Вот наш лос и, по сути, теперь вы можете его
точно также дифференцировать и так далее. А теперь, говоря про то, как считают градиенты
по-нормальному, а не руками. Смотрите, в торче есть пакет Optim, который, собственно, отвечает за
оптимизацию. В чем плюс? Вы можете взять оттуда, ну, RMSProp мы с вами пока не знаем, давайте SGD возьмем,
стокастический градиентный спуск. Вы можете совершенно спокойно взять оттуда SGD и прилепить
его к модели, которая у вас есть. То есть вы что говорите явно? Вы говорите, так, уважаемый SGD,
возьми параметры модели, которые получаются за модели вызовом метода parameters, и прицепись ко всем
ним. Теперь ты отвечаешь за обновление всех этих параметров. Все параметры, которые хотят градиенты,
будут им обновляться. Потом вы делаете loss backward, у вас появились все градиенты,
а потом вы делаете optstep. Optstep вам разом обновляет все параметры, которые хотели градиенты,
которые их получили. И после этого вы можете его так же явно попросить занудить градиенты,
чтобы больше он их не использовал. Все, понятно? Так, ну и теперь это все можно написать. Ладно,
я так уж и быть напишу. LR learning rate, но это тот самый гипер параметр. Вот помните,
мы с вами писали, что там ω равно ω old минус 0.05 на dl dω. Вот это как раз learning rate. То есть
это гипер параметр, насколько большой у вас должен быть шаг градиент. Так, давайте я за 30
секунд напишу, чтобы оно у вас работало, и мы на этом закончим. Так, y predicted, это что у нас там?
Model x batch.
Так, проверили. Loss, это соответственно nn functional binary cross entropy, y predicted batch.
На самом деле, вот эта тройка, loss backward, opt step, opt 0 grad, у вас будет где-то уже на подкорке,
я думаю, сидеть, потому что она постоянно везде используется. Вот, ну и все. Пожалуйста,
он пошел обучаться. Вот у нас loss падает, красивых график тут пока не строился. Ну вот,
loss 0.75, 0.40, 0.23. Да, 0.1 соответственно дошло. Ну и можем посмотреть, что там наша модель
на предсказывала. Model y test, 0. Так, у него тип данных что ли тут? Да. Окей, torch
from numpy. Что-то зануда такой. Ой, xtest.
Ну вот, бинарная классификация у нас точностью 97 процентов работает. То есть a от b наша моделька
с вами отличать умеет даже просто на уровне пикселей. В принципе, все вот это то, что мы с
вами написали, можно переписать для работы с многоклассной классификацией буквально тремя
шагами. Я сейчас не буду это запускать, чтобы вас уже отпустить, я вижу, многие уже хотят пойти,
бежать домой. Это нормально. Но просто для примера смотрите, собственно, что нам надо в модели поменять?
Ужас. Во-первых, нам надо, чтобы допустим, это была трехклассовая классификация, у нас должно
соответственно не один логит предсказываться, а три. Отображение в три. Все, модель у нас теперь
такая и соответственно здесь нам надо будет предсказывать, точнее вот этот ассерт убираем.
Соответственно, функция потери у нас теперь будет, где она не бинарная кросснотропия, а просто
кросснотропия многомерная. На самом деле, кажется, все. Ладно, где-то видим. А, зачем вы имейте
в виду там ноль? А, так я скину вам решенную ноутбук. Средством был 2.0, потому что у нас
просто-напросто одна вероятность, мы отбрасывали эффективную размерность. Вот. Так, ну по идее это
должно работать, я не очень понимаю, на что он тут конкретно ругается. Нигде. Игры предикта,
кросснотропия. Target.tensor. Batch. А, погодите, у нас вся, все, я понял. У нас вся выборка пока что
только для бинарной задачи. ABC. Вот. Дрын, дрын, дрын, дрын, дрын, дрын. Вот. Теперь, по идее,
оно должно. Да ты зануда. Ой. А, Y. Batch. Torch. Long. Во, пожалуйста, все. У нас просто выборка была на
два класса, поэтому он ругался, что ничего не работает. Вот, пожалуйста, поменял выборку
теперь на трехклассовую. Мысли изначально. Вот. Поменялось три из турочки кода. Собственно,
вот здесь он хочет индексы теперь классов в виде Torch.Long. Во-вторых, поменялась функция потерь,
потому что для бинарной задачи у нас бинарная кросснотропия, для мультиклассовой мультиклассовая.
И поменялась сама наша модель, теперь она соответственно отображает три класса. Все остальное
сохранилось. Та самая линейная модель, вот которую мы с вами говорили, One versus Rest. Вот у нас,
поэтому есть три выхода на каждый из трехклассов. У нас по одной, по сути, линейной гиперплоскости,
которая строит для одного отделяющую гиперплоскость, для второго и для третьего,
и строит для них ответ. А, ладно, хорошо. Вот еще что надо убрать. Вот. Теперь лучше. Ну,
сегмой, да в бинарном случае применимо, в мультиклассовом неприменимо, это логично. Вот. Ну,
что ж, коллеги, а на этом на самом деле все. Мы с вами будем гораздо глубже погружаться. Добро
пожаловать в мир, на самом деле, глубокого обучения. Это уже непосредственно притеча всех
этих сеточек. Добро пожаловать в линейную классикацию. Ну, и теперь у вас гораздо больше
машинерия. А первую домашку уже можно сдавать, права на вторую тоже сейчас появятся. Все,

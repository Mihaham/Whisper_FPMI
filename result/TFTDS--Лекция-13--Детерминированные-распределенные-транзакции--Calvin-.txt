Как я анонсировал сначала, мы хотим сейчас рассмотреть альтернативный подход к транзакциям,
к определенным транзакциям, который бы позволил вам избежать двухвазного комита.
Давайте подумаем, почему двухвазный комит нас беспокоит. Вот на примере того же самого спаннера
давайте нарисуем картинку, где у нас есть шарды, находящиеся в разных дата-центрах.
Пусть у нас есть первый дата-центр, второй, третий, и в каждом есть по реплике одного из трех шардов.
Вот это какой-то шард, и есть два других еще.
А теперь мы запускаем транзакцию, которая выполняет двухвазный комит над этими шардами.
Какова цена этого двухвазного комита? Смотрите, в нем, я говорил, было три записи.
Сначала запись параллельная в каждый шард на фазе Prepr, где мы надежно сохраняли блокировки,
затем запись решения координатора, и после этого параллельные записи для фиксации изменений в данных.
Секунду, я закрою дверь.
Но каждая запись, это же запись не просто на локальный жесткий диск, это запись прямо в шард системы,
а запись в шард это запись в Paxos, а Paxos у нас между дата-центровый.
И в итоге, когда мы делаем параллельную запись Prepr, мы выполняем коммуникацию через моря, океаны,
потому что эти реплики могут находиться очень далеко друг от друга.
То есть это не просто один раунд трип на комит команды в мультипаксосе оптимизированном,
это то, что называется White Area Network RTT, а он дорогой.
И вот мы делаем три таких раунд трипа. То есть мы сначала пишем в каждый шард,
потом мы пишем в какой-то шард координатор, потом мы пишем снова в каждый шард.
Три записи подряд.
Гораздо приятнее комит был устроен в транзакциях, которые касались только одного шарда,
потому что он был однофазный.
Просто посылали комит, и если этот комит был неуспешен, потому что реплика,
потому что шард забыл про нашу сессию или там истек тайм-аут или какой-то конфликт,
то просто вся транзакция откатывалась, потому что задевал только его.
В случае кроссшардовых транзакций у каждого шарда свое независимое решение,
поэтому нужно было их согласовать, поэтому у нас были двухфазные транзакции.
Так вот, нас сейчас интересует вопрос, насколько фундаментальным является двухфазный комит
для распределенных транзакций? Верно ли, что он необходим?
Верно ли, что его невозможно избежать?
Давайте подумаем, что является причиной двухфазного комита?
Что нас заставляет его выполнять?
Кажется, три причины.
Это тайм-ауты, это смена лидера
и это конфликты блокировок в двухфазных, в 2PL.
Вот по этой причине каждый шард мог отказать в комите транзакций,
поэтому нам нужен был двухфазный протокол, чтобы коллективное решение было общим.
Чтобы решение было общим на всех шардах.
Давайте подумаем, а почему нам нужны были, точнее, можно ли всего этого избежать?
Вот верно ли, что это все прямо необходимо, что мы без этого
распределенной транзакции представить невозможно?
Если мы каким-то образом сможем избавиться от тайм-аутов, которые приводили к аборту транзакций,
если мы сможем избавиться, если мы сможем как-то переживать смену лидеров в шарде,
если мы сможем каким-то образом справляться с конфликтами в 2PL, если, не знаю,
как-то мы сможем вообще избежать, то, может быть, нам не потребуется двухфазный комит,
потому что тогда, если у нас в системе не будет всех этих эффектов недетерминированных,
тогда просто разные шарды не будут расходиться во мнении относительно успеха или отката транзакций.
Отверждается, что проблема в недетерминизме, то есть корень всех бед наших в недетерминизме.
Вот почему возникает тайм-аут? Потому что кто-то где-то залип.
Почему смена лидера приводит к... Ну, смена лидера тоже, в общем,
какой-то внешний эффект протокола транзакций. Конфликты – это тоже следствие недетерминизма.
Идея, про которую мы сейчас поговорим, называется детерминированной транзакцией.
Она о том, что если из систем, которые исполняют эти кросс-шардовые транзакции,
исключить источники недетерминизма, то и сделать так, чтобы транзакция применялась или откатывалась
только потому, что не из-за каких-то внешних эффектов, эффектов среды,
а только потому, что в ней явно выполнился комит и реапорт, то успех транзакции
целиком определяется ее телом и текущим состоянием хранилища.
Вот если мы сможем этого добиться, то тогда двухфазный комит нам просто будет не нужен.
Вот давайте подумаем, как можно все эти источники недетерминированного отката транзакции
на отдельных шардах исключить. Начнем с тайм-аутов.
Почему нам нужен тайм-аут? Потому что у нас, кажется, транзакции интерактивные.
То есть, шарт слушает запись транзакции, чтение транзакции, ожидает комита,
но в какой-то момент это всего может не случиться, потому что просто клиент отказал.
Нужно локи снять и транзакцию откатить.
Но если проблема вне интерактивности, то давайте, чтобы избавиться вот этих тайм-аутов,
просто потребуем, чтобы транзакции перестали быть интерактивными.
Давайте сейчас сузим класс транзакций, который мы готовы исполнять,
до транзакций, которые зафиксированы заранее.
Если клиент работает с системой, то он просто пишет всю транзакцию на некотором языке,
но это может быть какой-то декларативный язык, может быть функциональный язык,
но в общем, какой-то специальный язык для описания транзакций,
и отправляет ее сразу целиком в систему.
То есть, да, мы, конечно, класс транзакций сузили, нас, наверное, это беспокоит,
но вот такой анонс на будущее, мы эту проблему решим.
Но если мы будем отталкиваться от того, что все наши транзакции зафиксированы
с самого начала целиком, в смысле, зафиксированы, плохое слово,
она может нас запутать, если транзакция известна сразу целиком,
если она не интерактивная, то тайм-аутов уже не нужно.
Эту проблему, это источник двухфасного комитта, можно исключить.
Что со сменой ридера? Здесь чуть сложнее, я бы про это разговор отложил,
в смысле, почему, как мы можем его избежать.
Но вот поговорим про конфликты в двухфазных блокировках.
Вот это, наверное, главный источник недетерминизма.
И почему, откуда там недетерминизм берется?
Авторы подхода с детерминированными транзакциями утверждают,
разумная мысль вообще, что в двухфазные блокировки протокол смешивает сразу две задачи.
В двухфазных блокировках транзакции одновременно исполняются и упорядочиваются.
И вот это две разные задачи.
Они как-то запускаются, они управляют свои операции,
какие-то потоки там запускаются, переключаются, все это недетерминировано,
и возникает какой-то порядок за эти блокировок,
и в зависимости от того или иного порядка при исполнении этих транзакций
возникает та или иная сериализация, которая нигде даже явно не представлена.
То есть вот недетерминизм берется здесь,
возникает вот в этом месте,
и он приводит к тому, что у вас возникает какой-то порядок сериализации.
В чем наша большая идея сегодня?
Давайте выберем порядок сериализации заранее.
Просто мы разделим два этих шага.
Когда нам клиент присылает транзакцию, можно уже нарисовать некоторую схему.
У нас будут клиенты.
Они нам посылают свои транзакции целиком.
И мы сразу же их упорядочиваем.
Мы построим отдельный компонент, который называется секленсер.
Его задача – получать транзакции от клиентов и просто выстраивать из них некоторые планы.
Этот секленсер будет фиксировать какой-то определенный порядок,
в котором транзакции нужно исполнить.
Мы не полагаемся на 2P, что в ходе исполнения родится какая-то сериализация.
Мы ее выбираем с самого начала, еще до того, как мы транзакции будем исполнять.
Этот план, конечно, динамически достраивается.
То есть приходит новый клиент с другими транзакциями, этот план продолжается.
Мы хотим зафиксировать его.
А дальше уже мы этот план исполнения транзакций отправим на отдельные шарды.
Data shard 1, data shard 2, data shard 3.
Каждый из них будет получать транзакции целиком и в одном и том же порядке.
И дальше его задача – эти транзакции в этом порядке исполнить.
Например, если у нас транзакция T2 выглядит так.
Я пишу по ключу X и пишу по ключу Y.
А транзакция T3 выглядит так.
Я читаю ключ X.
И если в нем какое-то значение, не будем углубляться, то я пишу ключ Z.
Пусть каждый shard последовательно выполнит вот этот общий план транзакций.
Если транзакции известны заранее, то если они пройдут через секвенсер,
если мы запомнили эти транзакции, то никакие тайм-ауты клиентов нас уже не беспокоит.
И если мы исполняем транзакции в некотором фиксированном заранее выбранном порядке,
пока очень неэффективно последовательно, то у нас не бывает никаких откатов из-за того,
что как-то неудачно взялись локи.
Мы просто пока выполняем транзакции одна за другой.
Мы написали очень неэффективный планировщик на каждом датэшарде.
Ну и беспокоят ли нас перевыборы лидера и рестарты отдельных датэшардов?
Мы считаем, что они отказаустойчивы.
Он может перезагрузиться, но страшно ли это? Нет, не страшно,
потому что в случае, когда перезагружался RSM здесь, он забывал все свои блокировки.
Здесь же план исполнения транзакции останется, его нужно просто снова будет повторить.
Вот кажется, не будет никаких источников, никаких причин транзакцию на одном шарде применять,
а на другом откатывать.
Просто если мы все вот эти источники недотриминизма исключим,
то значит, каждый шард исполнит одну и ту же серию транзакций в одном и том же порядке одинаково.
И все шарды будут согласованы друг с другом, и никаких недотриминированных откатов не будет.
Значит, двухфазный комит не нужен. Понятная идея?
Да, разумеется. Я, собственно, для этого все и рисовал.
Тут нужно некоторое пояснение, как именно эти шарды будут исполнять транзакции.
В первом приближении мы считаем, что каждый шард исполняет весь план транзакций.
Разумеется, у нас шарды возникают по той причине, что они хотят разделить множество строчек, таблиц, множество ключей.
Поэтому пусть шард 1 хранит ключ Y, шард 2 хранит ключ Y, шард 3 хранит ключ Z.
Когда каждый шард получает транзакцию T2 от секундсера, то он просто выполняет свою часть записей.
Этот шард ничего не делает, этот шард пишет в Y и пропускает запись X, а этот шард пишет в X и пропускает запись Y.
Если мы говорим про третью транзакцию, то тут уже сложнее.
Опять, шард 2 может ничего не делать, потому что его транзакция вообще не касается.
Она не касается ключей, которые он хранит.
Шард 3 записи делать не должен никаких, потенциально даже.
Но с другой стороны, шард 1 не сможет выполнить эту транзакцию, пока у него нет результата чтения X с другого шарда.
Но в таких случаях между ними должна быть коммуникация.
Вот мы сюда отправим результат чтения X.
Ой, что-то пошло не так в моей картинке.
Где же стёрка? Я её не вижу.
Стрелка должна быть в обратную сторону.
То есть между шардами есть прямая коммуникация, но каждый шард проходит через одну и ту же серию транзакций,
исполнитых одинаково, и, кажется, никаких ретраев, никаких конфликтов ничего нас не беспокоит.
Вот такая общая схема, которая, кажется, позволяет избежать двухфазного коммита,
только нужно разобраться в деталях, как это всё работает.
Давайте начнём с каждого дата шарда.
Я говорил вам, что мы хотим в качестве примера взять конкретную систему.
Enix Database.
И давайте я немного расскажу, как она устроена.
Точнее, не то чтобы как она устроена, а из каких компонентов она состоит
и по каким принципам эти компоненты написаны.
Enix Database – это геораспределённая база данных,
в которой есть изолированные сервизуемые транзакции, в которой есть, разумеется, отказ за устойчивость.
И из каких компонентов она состоит? Она состоит из таблетов.
Каждый таблет – это некоторый актор, в том смысле, в котором это называется в модели акторов,
то есть это некоторый компонент, который общается с другими отправкой сообщений.
Но этот актор является отказа устойчивым,
потому что он построен поверх слоя хранения, поверх слоя Distributed Storage.
Это называется таблет, это не обязательно кусочек таблицы.
Таблеты бывают служебными, компонент системы самой, то есть ее внутренностей,
и таблет, который отвечает за фрагмент данных пользователей, за фрагмент его таблицы.
Но так или иначе все эти таблеты построены поверх распределённого хранилища,
которое хранит некоторые имутабельные блобики порции данных.
И вот поверх этого хранилища уже строятся отказаустойчивые таблеты, отказаустойчивые акторы.
В этой системе реализован подход, про который я вам уже немного говорил в лекции про масштабируемость.
Когда мы обсуждали Bigtable. Сейчас я найду подходящую картинку.
В Yandex DB слой хранения и слой точек обслуживания разделен.
То есть каждая машина, это с одной стороны некоторые ресурсы для хранения,
а с другой стороны это некоторые процессоры для обслуживания каких-то действий команд пользователей
или каких-то внутренних активностей.
Слой хранения отвечает за имутабельные блобы.
И поверх этого хранилища строится сущность, которая называется таблет, которая по смыслу похожа на RSM,
то есть это некоторый отказаустойчивый узел. Но на самом деле это не то чтобы несколько реплик.
Если у нас хранилища отказаустойчивая, то есть репликация на этом уровне находится,
то у таблета физическое представление может быть одно, это одна единственная машина.
Но если она отказывает, то мы просто перезапускаем таблет на другой физической машине,
и он поднимает свое состояние из слоя distributed storage.
Если вы помните, то таблет и Bigtable строились по тем же принципам.
У нас есть GFS, в котором хранится лог и сестейблы для RSM, из которых состоит каждый кусочек таблицы Bigtable.
И у нас есть просто одна машина, которая обслуживает эти данные,
которая в памяти держит MemTable и пишет записи в Write и HeadLog.
Вот тут смысл такой же. У нас хранение и вычисления разделены,
и хранение выделено в отдельную подсистему. Это не файловая система, все же.
Это более примитивная конструкция. То есть API существенно другое и более простое.
Но смысл декомпозиции такой же. Мы отделяем хранение от точек обслуживания
и используем просто машины как некоторый пул ресурсов у тех и других.
И каждый шард, он является... Шарды бывают служебные.
Есть шарды, которые обслуживают фрагмент таблицы.
Вот есть какие-то другие примеры.
И каждый шард наших таблиц будет таким вот таблицом.
От отказа устойчивого. Если узел, который обслуживает этот шард, откажет,
то он перейдет на другой узел и там запустится поднимет данные из DistributedStorage.
Поэтому, скажем, Restart нас не очень волнует.
В принципе, этого даже достаточно для того, чтобы про транзакции дальше говорить.
Давайте пока вернемся на доску и по необходимости будем к слайдам возвращаться.
Пока вроде нам этого не нужно.
Вот каждый даташард надежен.
Сиквенсер тоже должен быть отказоустойчивым, разумеется.
То есть, когда к нему прилетают транзакции, он должен их надежно запоминать,
ни в коем случае не забывать.
И если все действительно отказоустойчиво,
и все работает вот ровно так, как нарисовано, то очевидно,
все транзакции исполняются одинаково на каждом шарде,
с необходимостью коммуникации, и необходимости в двухфазном комиссии не возникает.
Нужно обсудить какие-то еще детали, а именно про реализацию сиквенсера.
Но как бы вы делали сиквенсер?
К нему прилетают транзакции, и он должен их упорядочивать.
Но можно себе представить какой-то multipax, band奶,
но что-то, что реплицирует какой-то лог.
Реплицирует автомат, но у нас данные,
это просто последовательность транзакций,
поэтому мы реплицируем прямо лог.
Но с другой стороны, сложно представить,
что наurate эффект�ни не разделяет с тиманистами,
gy realmente в harvested input.
Иwaiting ив const,
Но с другой стороны, сложно представить, что даже один RSM сможет пережить все транзакции, которые есть в системе.
Сложно представить, что в него они все поместятся, он будет успевать.
Можно ли как-то шардировать секвенсор?
Заведем несколько экземпляров, и когда будут приходить клиенты с какими-то транзакциями,
то каждый секвенсор, он отказоустойчивый, но это несколько разных секвенсоров, они строят разные порядки,
разные планы для разного набора транзакций.
Каждая транзакция, каждый клиент выбирает себе какого-то секвенсора, но не знаю, похешу от транзакции.
Как устроен каждый секвенсор?
Но он, конечно, не может построить план на бесконечность, он строит его порциями.
Он открывает опять такое пяти миллисекундное окно и копит транзакции, которые в него падают, каким-то образом их упорядочивает,
не важно каким, и получает кусочек общего плана.
Первый кусочек плана секвенсора 1.
Второй кусочек плана секвенсора 1.
Б, потому что, очевидно, батч.
Ну это здорово, но всем шардам нужно знать про общий план исполнения транзакции.
А у них есть много разных планов для разного набора транзакций. Как быть?
Нужно в каком-то порядке все это обойти. Предлагается обойти, скажем, вот так.
То есть можно себе представить, что каждый даташард, каждый такой отказоустойчивый компонент, который отвечает за кусочек пользовательской таблицы,
слушает всех секвенсоров, раз 5 миллисекунд получает от них очередной пачку и выстраивает их в некоторый общий план, после чего применяет.
Набор секвенсоров зафиксирован, так что все даташарды выстраивают один и тот же план.
Будет ли это масштабироваться? Кажется, что нет, потому что даташардов у нас может быть очень много, у нас могут быть их миллионы.
И мы не сможем дать такую нагрузку на секвенсора, чтобы он посылал постоянно на миллион машин вот такие планы.
Нам нужно какое-то промежуточное звено внести в эту конструкцию.
Сделано так. У нас есть набор секвенсоров, есть шарды, их очень много.
И мы между этими секвенсорами и этими шардами ставим набор, они называются медиаторы.
Каждый секвенсор отправляет свою часть плана медиатором, всем медиатором.
Каждый медиатор собирает у себя вот эту последовательность, уже полный план.
И каждый даташард потом хэшот своего кого-то идентификатора, выбирает себе того медиатора, которого он будет слушать.
В итоге число секвенсоров выбирается просто исходя из нагрузки, из количества транзакций в секунду, чтобы они суммарно могли пережить этот поток.
А число медиаторов выбирается исходя из количества даташардов, чтобы мы могли распределять все эти планы между всеми шардами.
Понятно?
Таким образом у нас получается такой лишний хоп, мы не сразу от секвенсора в даташард приходим, а через дополнительную группировку медиаторов.
Но таким образом мы достигаем масштабируемости.
Хорошо. Такой вопрос.
А что такое 1, 2 и 3? Это просто монотонные числа или нет?
Можно ли это сделать просто монотонными числами?
Но в конце концов, каждый секвенсор, если он реплицирован, он может...
Ну, если это RSM в таком простом понимании, хотя это не совсем технически верно в случае Яндекс.ДБ,
может ли он буквально назначать последовательные номера этим пачкам, реплицировать их внутри себя с помощью мультипаксуса и отдавать это все медиаторам?
Оказывается, что не может.
И смотрите почему.
Каждый секвенсор отказоустойчивый, но он может быть некоторое время недоступен, потому что сломалась конкретная машина и нужно перенести работу на другую машину.
Мы ничего не потеряем, но при этом мы некоторое время работать не будем.
И в итоге мы некоторое время не будем генерировать вот эти вот пачки.
А без них нельзя собрать общий план.
Проблема понятна?
И в итоге, если какой-то секвенсор вдруг будет опаздывать сильно, в смысле будет недоступен некоторое время,
то у нас все сильно замедлится и мы будем ждать вот именно его очень долго.
Поэтому Яндекс.ДБ на самом деле используются здесь не такие вот абстрактные индексы, а прямо физическое время.
Если секвенсор перезагружается, переезжает на другую машину, и он понимает, что он не работал там 10 секунд,
то он просто посылает всем медиаторам пустые пачки за вот этот интервал мгновенно.
Он не мог так сделать в случае переезда с такими вот индексами,
потому что он не знает, какой сейчас верхний индекс, то есть сколько он может закрыть себя.
Тут возникает такая задача, можно это делать с помощью фактической синхронизации часов с другими репликами,
но это в этом смысла уже нет, потому что есть NTP и время просто.
Поэтому мы в качестве индекса патчей используем именно физическое время и там каждые 5 миллисекунд открываем новую патчу.
Тут от того, что часы не синхронизированы, не может произойти ничего плохого.
То есть, может быть, какой-то секвенсор отстанет и тогда все будут немного на нем тормозить,
но, скорее всего, NTP тут просто проблемы решит сам по себе.
Никакой монотонности или какого-то вот эпсилона между часами здесь не требуется.
На корректность это не влияет все.
И того, от тайм-аутов мы избавились, потому что клиент сразу отправляет запрос в систему,
и мы его здесь запоминаем, просто реплицируем, и тайм-аут нас здесь не беспокоит.
От конфликтов, откатов на уровне 2PL мы избавляемся, потому что мы исполняем транзакции последовательно
по заранее созданному плану, разделяя вот эти две подзадачи.
И от переезда рестартов отдельных даташардов мы спасаемся тем, что он перезагрузился
и начал с того места, где он остановился.
То есть, все источники имидотерминизма устранены, а значит, устранена причина,
по которой мы делали двухфазный комит.
И в этой конструкции все кажется в этом смысле оптимальней.
Но все же пока не до конца, потому что, с одной стороны, здесь все отказуустойчивое,
и вот масштабируемое мы это обсудили, как сделать.
А с другой стороны, каждый даташард в нашей схеме выполняет транзакции просто одна за другой,
последовательно, очень непараллельно.
Вопрос, можно ли добиться параллельности на этом уровне?
Мы сделаем это с помощью вариации двухфазных блокировок.
Вообще алгоритм двухфазных блокировок предназначен для того, чтобы с помощью локов
построить некоторые, выстроить транзакции в некотором порядке.
Авторы детерминированных транзакций предлагают некоторую вариацию 2PL,
которая тоже берет блокировки, которая тоже позволяет транзакциям исполняться все же параллельно,
но при этом гарантирует, что она порождает не какую-то сериализацию неизвестную,
а вот конкретно вот эту, которая нам спущена сверху.
Для этого используется протокол, который называется детерминированные блокировки.
Давайте я это покажу на экране.
То, что я рассказывал про секвенсер и про даташарды, это некоторые детали устройства Яндекс.ДБ.
А то, про что я говорю сейчас, это задумка, которая была в оригинальной статье,
и оригинальная статья называется Kelvin.
Авторы этой статьи не поленились и написали такую референсную реализацию на C++,
она абсолютно не продакшн качества, но об этом можно судить даже по описанию коммита.
Но это не важно, есть специально обычные люди, которые могут написать промышленную систему по такому скромному описанию.
Но, тем не менее, здесь просто есть реализация вот этого детерминированного менеджера блокировок,
который раньше просто получал отдельные команды, брал блокировку и порождал какое-то расписание сериализуемое.
А в Kelvin этот менеджер порождает конкретное расписание. Каким образом?
Он разделяет внутри 2.pl взятие блокировок и исполнение транзакций.
Взятие блокировок происходит в отдельном потоке, а исполнение происходит уже многопоточно.
Вот прям класс Shadower, который отвечает за исполнение транзакций.
В нем запускается много потоков, которые транзакции готовы исполнять, в смысле делать чтение и записи,
и один поток, который будет работать с блокировками.
Принцип такой.
Если в плане, который нужно исполнить, есть две транзакции, одна из которых предшествует другой, разумеется,
ИТ предшествует житой, и ИТ и житая пересекаются по набору ключей, то есть по набору блокировок, которые они должны взять.
То мы делаем так. Мы требуем, чтобы транзакция, которая идет раньше, получила все свои блокировки до транзакции, которая идет позже.
Делается это так. Мы просто скамливаем лог-менеджеру транзакции по порядку,
и лог-менеджер просто обходит все блокировки для транзакции, которые должны быть для транзакции, и встает на них в очередь.
Может быть, после этого транзакция готова сразу запуститься, потому что в очереди для всех блокировок никого не было.
Тогда она бросается в тредпул, и там исполняется. Если же хотя бы на одной блокировке мы встали в очередь, то мы ждем.
Из этого следует, что если есть житая транзакция, которая следует в плане позже, то она на той блокировке, которую она разделяет с ИТ транзакцией,
обязательно окажется в очереди позже, поэтому блокировки строго позже первой.
Не бывает такого, чтобы мы одну блокировку ИТ транзакции взяли раньше, чем житой, а другую взяли позже, чем житой.
Это понятно? Дедлогов не бывает, не бывают конфликтов, по причине которых нужно транзакции откатывать,
И исполнение в таком планировщике с таким лок-менеджером будет гарантировать, что это по смыслу 2PL, потому что мы берем все локи и только после этого что-то делаем.
В 2PL классическом мы берем локи и сразу пишем и читаем, а здесь мы сначала берем все локи, а потом пишем и читаем.
Но на свойства 2PL это не влияет. Мы получаем стерилизуемое расписание, причем ровно такое, которое мы хотели.
Таким образом мы все-таки можем добиться параллелизма некоторого, но сохранить детерминированный порядок и избежать откатов.
Вот в такой схеме, в таком исполнении внутри самой системы откатов нигде не возникает.
Вот транзакция может откатиться только потому, что в ней явно пользователь написал abort transaction.
Если этого нет, то транзакция успешно применяется везде.
Ну а давайте теперь подумаем, в чем подвох.
Почему так сделать не очень просто?
Ну вот такой lock manager написать не очень просто.
Точнее, написать ты его просто, он был маленький, умещался в 100 строчек.
Почему нам как пользователям это все сильно усложняет жизнь?
Вот мы сейчас используя такой подход к блокировкам, то есть сначала все блокировки взять, а потом транзакцию исполнить.
Смотрите, мы здесь декомпозировали 2PL сначала.
Мы выполнили ordering вообще на уровне выше, в отдельном компоненте.
А потом еще в пределах даташарда мы декомпозировали взятие блокировок и само исполнение.
То есть мы на 3 стадии декомпозировали этот протокол.
Но когда мы так сделали, мы заработали пару ограничений.
Вот первое ограничение возникло здесь, потому что мы должны заранее сконструировать целиком транзакцию, дать ее системе.
А второе ограничение возникло здесь.
Вот планировщик должен сначала взять локи, а потом исполнить транзакцию.
Но давайте пример я нарисую, тогда станет понятно.
Вот мы с этим поборолись, уже можно это забыть.
Пусть наша транзакция выглядит так, чтобы еще удалить.
Какая это транзакция?
Это транзакция, которая работает с вторичным индексом.
Вот у нас есть таблица, мы можем обращаться к ее строкам по первичному ключу.
Но ведь мы не знаем первичного ключа, и мы должны взять какой-то вспомогательный индекс,
по нему прочитать, узнать первичный ключ и по нему записать.
Вот мы не можем для такой транзакции воспользоваться таким протоколом,
потому что в этом протоколе сначала берутся блокировки, а потом транзакция исполняется.
Но пока мы транзакцию не исполнили, мы не знаем, чему равен Y.
И надо как-то с этим быть.
Для этого мы решаем еще одно ограничение.
С одной стороны, не интерактивность, которая появляется здесь,
потому что мы хотим избавиться от сессии тайм-аутов.
Вторая проблема в том, что мы должны знать для транзакции ее read-write-set.
То есть заранее на уровне дата шарда мы должны знать,
какие ключи эта транзакция собирается читать и писать.
Но это полезно еще не только потому, что мы здесь не сможем ничего сделать,
а еще и потому, что мы просто не хотим нагружать шарды,
которые транзакция не нужна, знанием про эту транзакцию.
То есть тоже можно кое-что пооптимизировать.
Я про это чуть позже, наверное, скажу.
Но вот в этом месте нам точно нужно знать X и Y.
Y мы не знаем.
Вот это, к счастью, можно обойти.
Довольно изящно, но смотрите, если вы не догадались еще.
Мы эту транзакцию выполняем в два шага.
На первом шаге мы выполняем оптимистичное чтение.
Просто идем в шарт, который содержит ключ X,
и вот, минуя секвенсер, минуя вот этот заранее план исполнения транзакции,
просто приходим и из шарда читаем по ключу X.
Может быть, это значение тут же устареет, мы не знаем.
А дальше, на втором шаге, мы выпускаем такую служебную транзакцию,
Y', которая строена так.
Мы читаем текущее значение по ключу X.
Если это попадает с ожидаемым значением, то мы пишем по Y'.
И вот X и Y' это то, что известно из тела транзакции,
то, что система может сама узнать и взять соответствующие блокировки.
Но теперь, правда, у нас транзакция может откатиться.
Здесь никакой логики отката не было, здесь она появилась,
но все же этот откат детерминированный.
То есть он произойдет на каждом шарде, который эту транзакцию исполняет.
Этот откат по воле самой транзакции происходит.
Нигде внутри системы никакие детали реализации не приводят к тому,
что транзакция откатывается против воли пользователя.
Но при этом все-таки откаты бывают, рестарты бывают.
Но, опять же, они детерминированные.
Имея такую идею, можно ее обобщить и использовать для того, чтобы побороть неинтерактивность.
Вот наши транзакции неинтерактивны, мы должны их отправить заранее в систему.
Но что если мы хотим с транзакциями работать так?
Прочитали, подумали, записали, подумали, что-то еще сделали.
То есть у нас транзакция состоит из шага start transaction, как обычно,
потом записи, чтение, потом commit и re-abort.
Вот стартуем транзакцию.
Когда мы что-то читаем, мы снова делаем оптимистичное чтение.
Когда мы пишем, мы запоминаем запись.
Когда мы доходим до точки комита, мы выпускаем служебную,
детерминированную транзакцию, которая перечитывает все, что мы читали,
сравнивает значение с тем, что мы прочитали оптимистично,
и если все сошлось, то мы все записываем.
То есть та же самая идея.
По сути мы делаем мульти-касс.
Вот если вы помните, как мы делали, как сделать фич-ет с помощью
операции Comperexchange на томиках.
Прочитать, а потом сделать касс, плюс один, если совпало.
Это же по смыслу то же самое.
И это просто такой мульти-касс, потому что мы трогаем теперь много ключей.
Но если мы что-то ретраем, нужно думать про гарантии прогресса.
В случае двухвазных блокировок, где у нас были ретраи, то мы сохраняли
таймстемп, и все-таки каждая транзакция рано или поздно комитилась.
Вот здесь каждая транзакция может в принципе ретравиться вечно.
Но важно то, что в каждой интерактивной транзакции есть только одна
такая детерминированная транзакция в конце.
И если она проваливается, то это просто означает, что какие-то ключи
изменились, то есть какая-то другая транзакция совершила прогресс.
Поэтому локального прогресса нет, но глобальные все же есть.
Этого достаточно.
Ну вот такая конструкция.
Да.
Смотри, мы в этом протоколе требуем от транзакции...
Какая-то теористика.
Тут шарды, они как-то поделены, таблица поделена на какие-то фрагменты.
И какую транзакцию пользователь напишет, таким шардам она и достанется.
Не то, чтобы мы этим как-то управляем.
Тут скорее речь про то, как именно вот эту коммуникацию выстроить
и как именно доводить транзакции до шардов.
Вот про это я не говорил, а про последнее скажу.
Вот здесь как будто бы клиент посылает транзакцию прямо в Sequencer,
Sequencer ее прогоняет через консенсус, там еще передает его медиаторам,
медиаторам, короче, очень долго.
А транзакция что такое? Это же работа с данными.
И данных может быть много.
Что на самом деле делает Yandex DB?
Клиент пишет свою транзакцию.
Транзакция, чтобы быть обработанной всей этой конструкцией,
должна быть детерминированной, должна быть доступна целиком.
И в ней должны быть написаны, из ее тела можно извлечь все ключи,
которые она пишет и читает.
Поэтому клиент, ну, рякаем про Окси на входе в систему,
берет эту транзакцию, генерирует ей какой-то уникальный идентификатор,
из нее достает все ключи, которым она обращается,
и данные отправляет вот сюда,
на все шарды, которые эта транзакция затрагивает.
Это потенциально много данных.
И каждый шарт должен надежно сохранить свою порцию данных транзакции.
Это очень интересным эффектом приведет сейчас.
После того, как каждый шарт, который в транзакции участвует,
надежно сохранил свою часть транзакции, свои данные,
транзакция, ну, просто идентификатор транзакции фиксируется в секвенсоре,
встраивается в план и план уже рассылается.
То есть мы данные много раз через весь этот конверт не прогоняем.
Но по смыслу мы получаем фактически двухвазный комит здесь.
Потому что раньше у нас была подготовка транзакции,
а теперь мы сохраняем ее данные надежно на каждый шарт,
перед тем, как транзакцию дальше запустить.
Потому что если какой-то шарт ее забудет, а мы уже встроили ее в план,
то понятно, что все развалится.
Так что, как говорят сами разработчики Yandex DB,
у них по эффективности, по количеству раунд-trip,
по лутентности это примерно одно и то же, что и двухвазный комит.
Но зато в их системе нет ретраев.
И это повышает пропускную способность.
То есть просто меньше бесполезных попыток делается, больше всего...
Конечно, это зависит от того, какого типа у нас транзакции.
Если у нас все клиенты интерактивные, то, наверное, в этом пользы не будет никакой.
Потому что мы просто перенесли ретраи.
Трудно сказать, это не одно и то же, конечно,
но ретраи по другой причине происходят.
Но смысл примерно такой же, что мы что-то пробуем делать, не получилось, пробуем заново.
Но не все сценарии применения, баз данных и транзакций,
требуют на самом деле интерактивности.
Я вам про это ничего не рассказывал, потому что все успеть невозможно.
Про еще один протокол, который был реализован в Google еще до Spanner.
Сейчас я его найду.
Это такая каноническая реализация и зарядцы снапшотов поверх Bigtable.
В Bigtable есть транзакции однострочные.
И вот прям реализован двухфазный коммит.
Если вы хотите разобраться, как именно он написан, то он буквально в коде.
Но тут интересно, что транзакции реализованы на стороне клиента.
То есть клиент является координатором.
Сам Bigtable про эти транзакции ничего не знает.
А значит, клиент может упасть и вместе с собой упасть в середине коммита.
И для того, чтобы пережить такого клиента, сама транзакция должна быть lock free.
То есть если она упала по середине, то должна быть другая транзакция,
которая способна допинать то, что она видит, незаконченное.
Я забыл, почему я начал говорить про Bigtable.
Про это, честно говоря, должна быть какая-то причина.
А, про то, что не все сценарии требуют интерактивных транзакций.
Так вот, этот протокол реализован снаружи системы, поэтому он заведомо менее эффективен.
И авторы об этом пишут.
Но они говорят, что у них такая задача, где на самом деле эффективность нас не очень беспокоит.
Google пишет поискового краулера.
Мы обходим интернет.
У нас есть гигантская таблица, где мы храним по рулам документы интернета
и по хэшам от документов храним рулы, чтобы склеивать какие-то дубли.
И для того, чтобы при обходе фиксировать в какой-то большой таблице новый документ,
мы выполняем вот такую транзакцию.
Так вот, здесь, в этой транзакции нет никакой интерактивности.
У нас есть документ, у него есть URL, мы можем посчитать хэш.
А дальше нам нужно по URL что-то записать и по хэшу прочитать, записать.
Вот эта транзакция, она вполне может быть не интерактивной.
Мы можем бросать ее в систему, и система будет обходиться вообще без ретраев.
И пропускная способность в этой системе будет выше.
Она совершает просто больше полезной работы.
Более того, мы даже можем не дожидаться результатов.
Как только транзакция была зафиксированна на каждом шарде и в секвенсере,
то все, мы можем уходить, потому что мы знаем, что ни что не помешает дальше ей исполниться.
От клиента мы уже ничего не ждем, никакого там таймаута, никаких хардбитов.
Каждый шарт может перезагрузиться, но у него остается тот же план запросов,
который он может получить с недиатора и проиграть его заново.
Ну и конфликтов на уровне исполнения транзакции, на уровне планировщика и лог-менеджера тоже нет.
Ну что, вот такая история.
Мне кажется, что у этого подхода есть, в этом подходе есть своя красота,
потому что все-таки какой-нибудь ваундвейт, то есть вот эти откаты транзакций
из-за конфликта блокировок выглядят как некоторые кастели.
Вот мы их изъяли, изъяли весь недотерминизм, добавили два ограничения
на то, чтобы знать Read-Write-Set и на то, чтобы исключить интерактивность.
И с этими ограничениями мы решили задачу гораздо проще
и получили решение, которое на самом деле гораздо более модульное, чем Spanner.
Ну вот в Spanner-е там был TRSM, там лог-менеджеры, вот мы говорили, что там где-то в памяти что-то живет,
ну короче, там много всяких деталей, которые взаимодействуют друг с другом, перемешаны друг с другом.
Вот здесь Sequencer упорядочивает идентификаторы транзакций.
Здесь вот планировщик берет сначала логи, потом исполняет.
Вот все очень изолировано, все очень так слоисто.
И кажется, что для того, чтобы все это рассказать, никаких особенных знаний это и не требуется.
Но кроме того, что Paxos нужно уметь писать как-то.
В YarnXDB еще раз напомню, не совсем такая схема, то есть там не RSM, а автомат поверх реплицированного лога в BlobStorage,
но думать об этом можно примерно одинаково.
И получается все кажется гораздо проще.
Это не значит, что система простая, конечно, система сложная, писали ее там много лет и до сих пор пишут как и что угодно.
Но вот подход к транзакциям тут совершенно иной.
Ну что ж, если у вас вопросы есть, то самое время их задать, потому что я, кажется, исчерпал содержание пока.
Не пока, но сегодня.
Ключевые инсайты. Вот избавиться от недетерминизма, понять, что двухфазный коммит не является необходимым для кросс-шардовых транзакций.
Этот недетерминизм исключить с помощью неинтерактивности и разделения 2PL
на собственно недетерминированные исполнения и детерминированные блокировки.
И таким образом избежать искусственных технических откатов по вине самой системы.
Ну что ж, если вопросов нет, то спасибо. На сегодня все.

Ну, давайте сюда поехали. Да, мы сегодня продолжим.
Ну, как говорится, там многое в геометрии, конечно, заветного на выпущенности, куда подождеваться.
Мы, кстати, определяли строгую выпущенность?
Строгую выпущенность?
Ну, мы понятия выпущенности функции у нас не было.
Скажем так.
Ну, а так шуткость?
А так шуткость строгая выпущенность.
Ну, а так шуткость строгая выпущенность.
Ну, например?
Ну, интуитивно кажется, что, типа, граница не должна собирать какие-то непрерывные отрезки континуальные.
Такого у нас не было и не ценивается особо.
Нет, ну это особо и не с чем.
Да, ну в прошлый раз мы уже искали какие-то выпуклые оболочки.
Ну, что с ними в принципе можно делать?
Что с ними можно делать?
Ну, конечно же, решать.
Ну ладно, в этом порядке, пойдем.
Решать задачи по динамическому программированию, конечно.
Так, ну тут будьте внимательны, да.
Потому что, да, ConvexL3 вещь известная.
Но будьте внимательны, потому что то, что я сейчас расскажу, возможно вызовет у вас некоторый отрыв,
потому что привычный вам ConvexL3 какой-то, якобы не имеет никакого отношения.
Но тем не менее будьте внимательны, потому что есть шанс, что то, что я скажу, иногда пишется сильно проще, чем к чему привыкли вы.
Вот, а иногда нет.
Так что смотрите, какую-то задачу мы будем решать.
Ну, задачи мы даже письмировать особо не будем.
Просто как сейчас на нашем жизни включить, встречаются ситуации, когда задачу можно решить примерно следующим годом.
Каждая задача в доначке.
Ну, вот что-нибудь типа там.
Ну, да.
Вот какая-нибудь динамика подобного рода.
Ну и приблизительно подобного.
На самом деле, конечно, тут не обязательно может быть DP или там CR.
То есть это просто что-то зависит от G и в том числе и от DP от G.
Ну, в том плане, что там какие-то такие вещи, которые мы заранее не узнаем.
То есть может быть DP от G плюс 2, может быть какая-нибудь там DP от G в квадрате или что-нибудь еще в этом.
Вот такая вот задача.
Ну минимум, естественно, по...
Сейчас уточним.
Ну, там какой-то ноль меньше, либо равнуже меньше.
И ну и чему-нибудь там.
DP нулевое, нулевое равно чему-то.
Ну да, часто встречаются у задачи, в которых вот такой код бы зашел.
Только маленькая проблема.
В квадрат все-таки многовато.
Хочется пах быстрее.
Что же делать?
Вот нас выручает код RSTAL 3.
Ну, в каком смысле выручает?
Ну, на самом деле тут нужно быть внимательными,
потому что таким образом у нас выручает есть два принципиально разных смысла.
Впрочем, начинаются они одинаково.
Начнем, конечно, мы с того, что DIT можно вывести.
Это не очень интересно.
А вот DPG2, ну, а вот совсем остальным немножко становится немножко интереснее.
Да, то есть тут важный момент, конечно.
Видите, что AITBIT, то есть они прификсированы и константы, то есть заданы.
А вот DPG2 и CG2 мы перебираем.
Вот как же сделать эту штуку побыстрее?
Ну, надо просто заметить, что то, что находится в скобочках, очень похоже на что?
На какое-нибудь произведение?
Ну да.
В таком виде даже больше на скалярное.
Ну, можно, в принципе, вектора вывести, конечно,
но на самом деле, конечно, больше похоже на скалярное.
То есть это на самом деле скалярное произведение.
То есть давайте так напишем.
Скалярное произведение вектора.
Допустим, AITBIT на соответственно вектор DPG2 и CG2.
Вот так давайте будем.
На соответственно DPG2 и CG2.
Вот такая вот красота.
Спрашивается.
Вот спрашивается.
То есть тогда с точки зрения этой интерпретации нам нужно реализовать следующее.
Нам нужно взять черный ящик.
То есть какой-то красный черный ящик,
красный-красный черный ящик,
в котором хранят земли-то вектора.
Они же точки.
Красно-синее, а во-вторых, соответственно, не то.
И у этого черного ящика должны быть две функции.
Во-первых, add ADEXIG, то есть добавь новый вектор.
Ну, добавь в смысле...
Ну ладно, формально надоится, конечно.
Есть же классический прикол, как кто-то там...
Суп с раками, да?
Как перевести на английский?
Вот кто-то вот на таком автомате привел soup with cancer.
Формально вроде так.
Только маленькие проблемы.
Суп по-английски это мыло, а cancer это рак, который полезен.
Вот.
Поэтому тут вот между ADEXIG там все-таки разница есть.
Ну, в питоне это верно, да.
Ну...
Ну, странно.
Но тем не менее.
Так вот.
Ну ладно.
Впрочем, могу быть неуправно.
А еще insert?
Все-таки insert.
И...
Теперь еще маленькая фишка.
И такую операцию, которую мы назовем find me.
А вот тут уже гораздо интересней.
Давайте я...
Для понятности я тут даже напишу ab.
То есть это вот...
То есть требуется найти...
То есть минимум скалянного произведения ab.
Соответственно, всеми x-ыгроками.
Идеи x-ыгроки лежат в черном ящике.
Вот.
То есть вот такая.
То есть если мы реализуем черный ящик, который умеет это делать в какую-нибудь удобоваемую асинтутику,
то, соответственно, вот и эту динамику мы реализуем быстрее.
Ну, если бы обе эти операции будут работать...
Вот у нас будет мечта сегодня логарифом.
Там за логарифом.
То и, соответственно, вот эту dp-шку мы спокойно насчитаем за n-лог.
Это классно?
Тут видно?
Хорошо.
Вот.
Осталось только придумать, как же этот черный ящик реализоваться.
Как же его реализовать?
Ну, давайте думать.
Итак, давайте себе это вообразить.
Значит, где-то у нас вот начало координат.
Вот это вот 0, 0.
Нет, даже не здесь, а где-нибудь, давайте, здесь оно будет.
И вот тут мы в черном ящике подобавляем эти точки.
Вторая доска будем делать.
Как и бедро.
Что хочется...
Да, ну, точечки можно добавлять.
И теперь прозекает вопрос.
Пришел нам вектор AB.
Ну, допустим, какой-нибудь вот такой.
Хочется найти у него минимальное скалярное произведение с каким-то вот таким вектором.
Теперь давайте подумаем, а что такое скалярное произведение вот этого синего вектора на вот этот красный вектор?
Что это такое?
Проекция умножить...
Да, совершенно верно, совершенно верно.
То есть на самом деле скалярное произведение это длина, собственно, синего вектора AB
на внимание ориентированную, на ориентированную длину красной проекции.
Почему ориентированную?
Ну, потому что, обратите внимание, мы проецируем вот на такую вот прямую.
Мы вот так ориентируем, но в этом случае, конечно, длину проекции нужно будет посчитать со знаком.
Вот, то заметим, что длина AB у нас положительная константа, поэтому получается, что надо найти какую-то точку,
то есть надо найти какую-то точку, у которой проекция вверх и вверх.
Ну, на самом деле подсказку нам дает название этой оптимизации.
Да, как следует из названия, точка находится на выпукловой оболочке.
Казахской, почему?
Ну, в данном случае, конечно, в данном случае как-то видно, но с другой стороны, может быть, это картинка.
Картинка.
Ну, на самом деле доказательство, конечно, очень простое.
Давайте рассмотрим какую-нибудь абсолютно произвольную точку.
Докажем, что существует какую-то точку и рассмотрим ее проекцию на нашу ориентированную прямую AB.
Докажем, что существует точка на выпукловую оболочку, в которой проекция не хуже.
Как это надо говорить?
Давайте от этой произвольной точки мы просто будем идти вдоль ориентированной прямой, ну, по направлению к уменьшению проекции.
Идем, идем, идем, идем.
Упираемся в выпукловую оболочку.
А теперь мы упирались в какое-то ребро.
Теперь давайте идти вдоль этого ребра таким образом, чтобы проекция не уменьшалась.
А ведь в одну из сторон так точно можно.
Ну да.
Идем, идем, идем, доходим до вершины.
Поздравляем.
Мы все время уменьшали проекцию, но не увеличивали, и попали в вершину выпукловой оболочки.
Доказательство ориентированное.
Такое абсолютно стандартное соображение.
Вот.
То есть таким образом нам нужно на самом деле поддерживать, то есть очень хотелось бы поддерживать выпукловую оболочку.
То есть на самом деле все точные, которые уже лежат в кругу выпукловой оболочки, они нас уже не интересуют.
Этим вниманием мы почти не удаляем.
Мы их только добавляем, но ни в коем случае не удаляем.
Вот.
Значит как же поддерживать выпукловую оболочку?
Ну еще когда, видимо нам еще нужно аккуратно рассмотреть крайний случай, когда выпукловая оболочка точкой отрезает.
Ну в плане нужно как минимум учитывать.
Нет, ну возможно придется, но тут надо всегда очень внимательно посмотреть, что происходит.
Вот.
Что здесь происходит?
Потому что на самом деле тут многое может зависеть.
То есть туда нужно всегда смотреть на ограничения и сами эти векторы AB, потому что сами векторы, сами точки.
Потому что в зависимости от того, в каком порядке эти точки добавляются, на самом деле сложность реализации может кратно увеличиваться или наоборот уменьшаться.
Ну, например, давайте добавим какое-нибудь там предположение.
То есть какое-нибудь предположение.
Например, вот у нас будет предположение, что у нас B всегда больше 0.
Ну, на самом деле.
Что это означает?
Ну означает это маленькое упрятывание, смотрите.
Выплавную уголь, значит смотрите, выплавную уголь, вот эту выплавную оболочку, можно разбить на нижнюю выплавную оболочку и верхнюю выплавную оболочку.
Каким образом?
Просто приведем две вертикальные касательные.
Понятно?
Проведем две вертикальные касательные и ванноты, и соответственно то, что снизу это нижняя часть, тут оболочка, то что выше, понимаете, да?
Вот.
Так вот, зависит от того, что если B больше 0, то есть это вот синий вектор смотрит вверх, то легко видеть, что на самом деле, что на самом деле это не так.
Если это вот синий вектор смотрит вверх, то легко видеть, что на самом деле, что на самом деле нас не интересует вся выплавная оболочка, а поддерживать на самом деле нужно только нижнюю выплавную оболочку.
Все разны?
Если B больше 0.
Если B больше 0.
Все равно так смотрит.
Да.
Да, вроде правда.
На самом деле предположение не сильно нарушает общность, потому что обычно вот это предположение верно, но если случайно оказалось, что B бывает рандомным, и вы там X с игроками местами поменять не можете, а вы, кстати, видите, такая формула подразумевает, что вы можете X с игроками местами как-то менять.
То на самом деле вы просто делаете одно и то же, просто храните в нижней выплавной оболочке, в верхней выплавной оболочке храните одно и то же.
Ну там буквально будет один год, только видимо обойдется два черных ящика.
Но это как бы не особо нормально.
Итак, давайте без ограничения общности особого считаем, что нам нужно хранить только нижнюю выплавную оболочку.
Так, ну давайте сначала воврем кое-что в общем.
Так, нет, сначала давайте я эту выплавную оболочку чуть-чуть нарисую поподробнее.
Вот так вот.
Ну допустим так.
И теперь давайте немножко разберемся.
Вот допустим нам дан вот такой ветер.
Какой-нибудь вот такой.
Спрашивается, где же у него тут самая левая относительно него проекция?
Где у него самая левая проекция?
Ну типа вон там внизу.
Ну понятно, что где-то там внизу, осталось только выяснить в виде.
Что искать?
У нас сначала, ну короче, если неформально, то сначала они идут вниз, а потом вверх.
И вот слева-справа.
Ну не совсем.
Ну как бы да, если я тут просто проведу вот этот вот и найду.
Ну слева-справа, короче, сверху или снизу?
Направляющий вектор выплавной оболочки.
Сверху, снизу что?
Короче, знак скалярного произведения.
Скалярная?
Скалярная это такое, Cross или Dot?
А, ну в принципе скалярная, да.
Ну на самом деле я бы сказал так.
Смотрите, повернем этот вектор до 90 градусов и приведем к этой нижней выплавной оболочке вот такую укасательную.
Ну вот.
То есть на самом деле вот к какой точке приведем эту нижнюю укасательную, там очевидно не отвертится.
Логично, да?
То есть получается, что вот можно.
То есть действительно видим, что если рассмотреть вот эти вот вектора.
Так.
И вот.
То есть оказывается действительно, что пока оно, что..
В некоторое время вектор идет в минус по проекции, тут проекция уменьшается, уменьшается, уменьшается, louder..
Но в точке касания происходит периодик, теперь проекция увеличивается, увеличивается, увеличивается.
Вот.
То есть в принципе можно действительно..
если бы вот эта вот клубка-оболочка хранилась в векторе, то мы бы действительно вот эту точку касания,
если это нашли, мы бы просто банально бинарным поискали.
Ну, если бы она в сете хранилась, то тоже.
Ну, в сете аккуратнее надо, потому что в сете это называется...
Ну, если в сете хранить вот эту последовательность векторов, то да.
Ну, надо аккуратно стомпоратором.
Ну, тогда в сете придется хранить вот эти векторы и тогда сравнивать их по алекторному предприятию.
Чтобы они хранились ровно в этом порядке.
Так, понятно, о чем мы сейчас говорим?
Да нет, наверное, все хорошо.
Ну, правда тут...
Ну, тут вообще как-то к векторе сетели что?
Если бы хранили в массиве векторе, то, пожалуйста, бин поиск пишет он хранял.
Но маленькая проблема, у нас точки еще воболочка добавляются.
И вот тут нужно быть внимательнее.
Потому что на самом деле тут как бы есть тоже разные уровни предположения, в разных задачах реализуются.
То есть может быть такое прекрасное предположение.
То есть в инсокте, допустим, х строго возрастает.
То есть каждая следующая точка правее передучит.
Что тогда?
Ну, тогда если у нас там вот воболочка была такая, и мы добавили точку, то, значит, тогда получается надо...
Ну, по сути, что я буду повторять, алгоритм, наверное, да?
То есть мы пытаемся кушбакить точку, но перед этим удаляем все, что нарушает выпуск в нужную нам сторону.
Что это выпускает?
Что у нас в точке добавляются черные ящики, что там в порядке только возрастает.
Ага, то есть в стандартных конвертах.
Да, ну получается, простите, стандартный конверт стал методом Эндрю из любых точек.
То есть тогда это вы делаете в том же векторе, в том же векторе делаете бин поиск.
Бин поиск, это вот идеальный случай, но в 90% случаев он и реализуется.
Вот поэтому, прежде чем...
Будьте внимательны.
Ну, например, конечно, вот с такой интерпретацией будет происходить, что ДП будет вот строго возрастать.
Это, конечно, может быть не так.
Ну, это строго, наверное.
Да, нет, может быть и не так.
Но с другой стороны, там фишка такая, что с другой стороны, вы тут можете координаты поменять местами, потому что, может быть, С возрастает.
То есть там по ситуации.
По многих задачах, на самом деле, этого хватает.
Но вам, конечно, может не повезти.
В чем-то наметим, что это...
Учитывая, что мы помним, как у нас реализован ДП или ДЭК, то у нас...
Ну, или просто...
То есть можно реализовывать, например, ДЭК, и можно заметить, что на самом деле нас бы устраивало, чтобы каждая новая точка, она либо правее всех предыдущих,
либо левее всех предыдущих, потому что мы по две слева делаем то же самое.
Вот.
Ну, а теперь давайте подумаем.
А что делать, если такого нет?
Так, давайте уберу красатель.
Сейчас уберу красатель.
Вот про красатель, наверное, все поняли.
Вот предположим, что этого предположения нет.
То есть точка добавляется абсолютно рандомно.
Тогда, ну, понятно, если она добавилась справа или слева, то делаем буквально то же самое.
Но если она добавилась где-нибудь посередине, то первое, что нужно сделать, это понять, а вообще, имеет ли она отношение к нижней выводовой оболочке.
Как это понять?
Ну, понять, как очень просто.
Надо просто по иксу, например, найти бинпольском, между какими иксами оно попадает, и просто понять, эта точка выше этого отрезка или ниже.
Вот. То есть если она оказалась выше, то добавлять оболочку не надо.
Но если она случайно оказалась ниже, то тогда у нас проблема.
Потому что вам придется эту точку вставить.
Ой.
Еще и поудалять всякие выводы.
Амортизация полностью.
Если бы это была главная проблема.
Нужно обидать с ними.
Вот. То есть придется вот это вот удалять, удалять, удалять, удалять и вот добавлять оболочку.
Ну что, возникает теперь естественный вопрос.
А где же эти точки вообще хранятся?
В списке.
Ну, в такой постановке.
Да. То есть как-то вектор уже так не похоронишь.
В списке тоже.
Не, ну в ДД это точно пойдет, а можно и в сети.
Так, погодите, погодите, погодите.
Потому что в ДД тоже возникнет вопрос.
Вы будете ходить в ДД, видимо, в порядке возрастания иксов.
Ну нет, не иксов.
Сейчас иксов.
Да, видимо иксов.
Ради того, чтобы точку в правильное место вставлять и искать в соседние, вам придется как-то искать там по иксам.
Но теперь возникает естественный вопрос.
А если нижнее огибающее?
Ну, нижнее огибающее.
Ну а в верхней тоже самое, повернуться 80 градусов.
Вот. Но тут суть, наверное, другая.
Хорошо. То есть как же вставку, конечно, делать легко.
До тех пор, пока вас не попросят действительно опять искать эту мистическую касательную.
То есть представьте себе, что вот это нижнее огибающее у вас, как бы, хранятся все эти точки просто как последовательно в дикартовом дереве.
И вам нужно, там, давно направление какое-то, найдите касательную.
Ну еще раз, мы можем сохранять, мы можем хранить не только точку, но еще следующую за ней, поддерживать указатель, чтобы золото дома искать.
Ага.
И делать тот же самый метод поиска.
Ну, и за сколько мы будем работать?
Ну, не бедной, а за спуск, окей.
Вот. Тут ваша аккуратная момента.
Да.
Вот, в нашей дикартешке возникает куча открытий.
Во-первых, для каждого элемента нужно хранить следующий и, видимо, предыдущий.
Вот когда вы там удаляете вот это все.
Вот вы там удаляете то, что, наверное, надо предыдущему искать.
Так что, увы. Это разно.
Ну хорошо.
Ну там, нет, можно искать и за линию, но не за линию, на самом деле, а за лоб.
Нет. Ну, за лишний лоб, это как бы, если вы берете поиск лоб, и в каждой интервью лоб, это лоб квадрат.
Но это не лоб, ну суммарно, это линия вылез.
Ну, короче, если, ну это, короче, обратная дефейс будет, если мы обратно войдем.
Нет, нет, сами по себе удаление, нет, понятно, что сами по себе удаление, конечно, если как-то, вот, дерево суммарно, по-любому, будет за evil grand,
потому что мы добавляем не более чем n точек и удаляем не более чем n точек, понятно.
Но вопрос же не в этом, вопрос же как бинпоиск искать.
А бинпоиск нам только следующий не будет.
Ну хотя хорошо, ладно. А, ну ладно.
Ну вот, тем не менее, ну я для задержицы лучше, конечно, давайте храним следующий, храним предыдущий, будет адекватный по всю эту списку.
Окей, да.
Вот. Но фишка в другом.
То есть фишка в том, что теперь где как-то в дереве нужно найти, то есть как-то бинпоиском эту штуку.
То есть если искать классическим бинпоиском, то есть там, найди, дайди мне кат и вектор, и там, найди мне в ядном виде кат и вектор, и там мы посчитаем, в принципе, вектор от произведения или скалярная, то что получится?
Тогда получится, что как бы придется в дикартовом дереве эту кат и точку найти.
А это делается с куском золотогранички.
Получается бинпоиск делается по квадратам.
Что-то пока не очень хорошо.
Понятно проблема, да?
Но с другой стороны, так же понятно то, что с другой стороны, как бы в дикартовом дереве делать бинпоиск через кат или вектор, это как-то странно.
Потому что дерево поиска создавалось из-за того, что бы не делали этого.
А без этого, просто аккуратненько делали по дереву.
Понимаете, да?
Да. Высота дикартового дерева, конечно, не обязана быть выгорится.
На 4 выгорит.
Да, ну а первых, да, вероятность этого крайне мала, во-вторых, ну не нравится, ну пишите овэйк.
Не ебайтесь.
Да, давайте, конверс, алкликс, овэйк, а что, все.
Пока не идейно.
В реальности, в реальности, конечно, не хочется это писать.
На реальной олимпиаде, ну что поделать.
Так.
Ну а это если писать?
Ну это если писать дикарте.
У кого-то фазирина просто безлимитная.
А вы же, а что, неужели это вот с сетом вообще не пишется?
Да, вообще не пишется.
Да, хочется написать это с сетом, но возникает маленькая проблема.
А какой компаратор вы будете хранить эти точки?
И по какому компаратору вы их там будете хранить?
Потому что вам же с одной стороны нужно искать по иксу, значит получается точка хранить по иксу.
Ну все нормально.
Но потом вам надо искать видимо бинпоиском что-то там вот по скаляному произведению.
Ну, значит компаратор по иксу вас нам уже не устраивает.
Нужно просто передать компаратор свой в бинпоиск.
Ну заметим, что у нас, ну как бы они упорядочены и по иксу, и по скаляному произведению.
Да, по иксу, да.
Поэтому, ну то есть, как бы нам нужно просто в лобербаунд засунуть свой компаратор, чтобы сравнивать не по иксу, а по произведению.
В лобербаунд можно запичивать компаратор.
Тоже не уверен.
Окей, можно не запичивать, но можно сделать свою структуру, у которой я произведен очень.
Какой называется декартовый дерево?
Нет, нет, структура, короче, компаратор.
Написается структура точка и, например, булливый флаг глобальный.
И если он стоит на ноль, то сравнивать по иксу, а если стоит на один, то сравнить по электронному произведению.
Ой-ой-ой-ой.
Вот, и тогда...
В конце правда, для второго компаратора все те надо хранить не точки, а собственные векторы.
Да.
Ну, может и так, но более классический метод просто завести в высота.
В одном храним точки, в другом вектора.
Ну, это без флага, да.
Ну, в суть, в общем, одна.
Ну, то есть, гадость, конечно, в любом случае та еще.
То есть, есть подозрение, что как раз в этих случаях, на самом деле, этот метод действительно годится не очень, не вышеписать старые, используется второй метод.
Вот, впрочем, по ситуации.
По ситуации.
Но тем не менее, в принципе, вот так на самом деле может быть.
Ну, ладно, судя по тому, что не вижу удивления, видимо, этот метод вам все-таки уже знаком.
Ну, то, что в сете можно поддерживать, понятно.
Ну, потому что обычно, как бы, какой-то халтригом подразумевается что-то принципиально другое.
Но обычно они возрастают.
Нет.
Нет, обычно подразумевается другое.
Обычно подразумевается, что в черной ящике прямые.
Ну, это же...
Ну, тут не прямые, а векторы типа...
Тут похоже.
Нет, потому что я многое рассказываю, но вы не просто делаете круглые глаза.
Если ты понимаешь, что прямая и вектор, это по сути одно и то же?
Ну, знаете, прямая и вектор одно и то же.
Два это тоже в какой-то степени восемь и так далее.
Ну, класс.
Стоп, а вот второй сет, как мы там сравним, который не по их самому?
Ну, по векторам произведения.
То есть, вектор меньше второго вектора.
Если вектор произведен первый, то второй больше нуля.
Тут тонкая подлянка, что мы обязаны хранить целый вектор,
потому что следующую точку мы ищем за логарифм в сете.
Либо нам нужно ее как-то поделивать, но...
Вообще логарифм там...
Итератор плюс-плюс, там есть залог.
Не, ну он залог.
А в бинпоиске мы логарифм раз его применяем.
Поэтому, если не хранить следующую точку в явном виде,
то вылезает как раз тот самый лог-код раз.
Стоп, я что-то не очень понял.
Не смотря в каком бинпоиске, погоди.
Откуда он, в каком?
Разве мы не хотели просто один раз от сета вызвать лог?
Хотели, но проверить, как бы...
У тебя проверка, одна бинпоиска, тебе нужно взять точку и следующую точку.
А следующую точку, если ты будешь брать плюс-плюс,
то это навесывает лог сразу.
Миша, какое у тебя решение с сетами?
Что-то я уже, может быть...
Ну, теоретически можно хранить логарифм,
в одном сете,
и фазить компаратор этой точки,
как, типа, один компаратор на добавление,
второй компаратор на поиск.
Ну, просто ты пишешь, был оператор меньше,
и в нем оставляешь, если флаг, то сравниваю по х,
если не флаг, то сравниваю по вот этим вот произведениям.
А потом еще, типа, искать следующую точку, чтобы понять сам вектор.
Но следующая точка, если флаг, то сравниваю по х,
и искать следующую точку, чтобы понять сам вектор.
Но следующая точка будет искаться за логарифм в таком случае.
Нет, подожди.
Вот, когда ты делаешь запрос findbin,
ты делаешь в сете lower bound.
Тебе что-то выдают.
У меня, типа, в lower bound мне выдают точку в сете.
И мне нужно понять, идти как бы меньше или больше.
Но чтобы это понять, нужно понять следующую точку,
потому что нам нужен вектор целый.
А следующая точка возьмется за логарифм,
если мы сделаем...
Нет, нет, в смысле, а где у тебя bin поиск, я не понял.
Нет, bin поиска нет.
Да, а в чем там проблема?
Есть lower bound и поиск следующего элемента.
Да, да.
А поиск следующего элемента каждый раз будет работать за лог.
Ну а каждый раз, так это один раз.
А lower bound лог раз вызывает сравнение.
У меня каждое сравнение нужно найти в следующей точке в сете еще,
чтобы вектор понять какой.
Нет, так реализовать нельзя,
потому что в сет тогда искусство-то не построится.
Ну да, если честно, то что ты говоришь, Мишель?
Да, потому что если по тому же компаратору добавляешь точки в сет,
то как бы он тогда сравнивает в зависимости от следующей точки
от следующей точки в процессе вставки.
Так нет, у меня два компаратора.
У меня оператор меньше.
Если flag, то я сравниваю по x.
А если не flag, то я сравниваю по вот этим вот векторам произведения.
Глобальный flag.
Глобальный flag.
И вот этот flag я выставляю перед добавлением в true,
а перед lower bound в false.
Что-то да, Миша, звучит очень ненадежно.
Ну скажем так, если у вас очень мало памяти.
Ну короче, я согласен, что это не то, что вам хочется писать,
но это можно сделать.
Это еще не факт, что корректно вообще.
Ну и по стандарту.
По стандарту корректно,
потому что они отсортированы одновременно и по иксам,
и по векторам произведения, и поэтому...
Ну да, но просто лучше отдельно хранить точки
и отдельно хранить векторы, чтобы вот так вам делать там.
Ну в общем...
Так, ну, как вы видите, слышите?
Ругунь.
Я правильно понимаю, что это типа личао только?
Это вообще не личао.
Это вообще не личао.
Это прямо совсем-совсем личао.
Потому что личао вызывает, конечно, принципиально другую интерпретацию этой запись.
Итак...
Значит, интерпретация вторая.
Которая мы неожиданно перестаем замечать, что это скалярное произведение.
Вот.
Ну, на самом деле, тут мы, конечно, начинаем неожиданно надеяться,
что на СЖТ можно поделить.
Возможно, даже СЖТ больше нуля.
Но по-хорошему говоря, вторая интерпретация больше даже подходит, знаете, туда.
Она подходит вот туда.
То есть, на самом деле, если бы у нас было что-нибудь типа
ТП ЖД умножить на АИТ, плюс БЖД.
Ну, может быть, плюс СИТ.
Вот больше подходит, на самом деле, для чего-то вот такого.
Ну, в принципе, скорее всего, вот.
То есть, скорее всего, это видео, то есть, сужают.
То есть, не совсем, не такой общий случай, как в прошлый раз был.
Но, например, если в том случае, скажем, С было больше нуля,
то можно было бы на него поделить.
Вот, так что оно не особо принципиально.
Но, значит, что происходит в этой интерпретации?
В этой интерпретации...
То есть, в этой интерпретации, в черном ящике, мы храним не точки.
Не точки, это два разных слова.
Мы храним линейные функции.
Вида F от X равно там KX плюс B.
K2, X плюс B2, ну и так далее.
Храним какие-то прямые.
Храним вот такие линейные функции.
И у нас есть.
И на этот раз у нас тоже есть два запроса.
Первый.
Добавь новую линейную функцию.
K и B.
И find min у нас превращается в один тоже принципиально другой.
Find min от X.
Можно даже сказать, что мы храним find.
Чистил и вот таким такое.
Ну, по сути, мы храним прямые.
Не прямые, а линейные функции.
Вот такая идея.
Ну, по сути, какая это вопрос?
То есть давайте себе воображим.
Если мы додобавляли несколько примеров.
Как выглядит график вот этого минимума от X?
В смысле аргум минимума?
Нет, именно сам график минимума.
Ну, в каждой моей премии минимум это функция от X.
Такая же нижняя лукоположка.
Вот, ну, это не совсем.
Ну, давайте подумаем.
Если у нас в черном ящике одна прямая, то график выглядит вот таким вот.
Если две прямые.
Ну, если у нас в черном ящике одна прямая, то график выглядит вот таким вот.
Если две прямые.
Если две прямые.
Ну, я сейчас не буду рисовать параллельно.
То оказывается такой уголочек.
Если добавить что-то третье.
Ну, может, ничего не поменялось.
Давайте четвертое.
Получается вот так.
Короче говоря, если так идти до конца, то получается примерно следующее.
Получается.
Что у нас получается?
Получается вот.
Получается, на самом деле, что-то типа это обычно называется нижнее одепающее.
Пересечение.
Что пересечение?
Что пересечение?
Почему пересечение?
Нет, пересечение.
Да, пересечение вот тут.
Печально.
Что такое?
Посмотрел, что там ничего не видно.
Вот.
РСФ.
А чего не видно?
Ну, видно.
Да, видно, видно.
Выглядит, правда, черно, но это не проблема.
Нет, это вообще не проблема.
Значит, на самом деле получается так, что функция выглядит как вот такая кусочная,
то есть такая вот кусочная линейная функция, причем функция в объект.
Мы еще пока не ввели определение, но было бы понятно.
Тем более, что мы над анализией какое-то определение уводили.
Можно вопрос?
Давай.
Вот мы хотели поддерживать два сета.
Да.
А как мы будем вставить?
Вот, например, мы, допустим, хотим вставить точку по лицу.
Как мы ее вставим во второй сет?
Ну, скажи так, сначала мы разбираемся с первым сетом.
Да.
И прям честно смотрим, какие точки мы выкинули, а какую ставили.
Аккуратно это записав, мы, собственно, понимаем, какие векторы надо из второго сета уголить
и какие два вставить.
Ага.
Да, лучше это не зависит.
Но нужно это делать неодновременно.
То есть, я помню, мы как-то по приколу пытались написать так, чтобы вся структура,
которая это делает одновременно и...
Ну, скажем так, слова упоролись очень просто.
Вот так вот.
Вот так вот.
Вот так вот.
Вот так вот.
Так вот.
То есть, что нам тогда...
То есть, надо хранить вот такую функцию.
И что в ней уметь делать?
Во-первых, надо уметь добавлять новую прямую.
А во-вторых, искать...
Собственно, искать собственное значение в Q4.
Ну, как же искать?
Здесь, как всегда...
То есть, смотрите, то есть...
В общем-то, которые мы описали раньше, они никуда не делись,
просто они немножко модифицируются.
Потому что, смотрите, какая логика.
Потому что, предположим, что мы эту логу мы храним, честно, в каком-нибудь векторе.
Тогда значение в Q4 мы ищем просто в анальном бюлл-полисе.
Правда?
Теперь, следующее.
Ну, как же нам добавлять прямую?
Ну, здесь опять, значит, оказывается, что...
Если оказывается, что...
То есть, здесь у нас вектор предположения.
То есть, допустим, у нас вектор предположения, что K...
Вот это все будет неожиданно.
Убывает.
Меренно.
Логично.
Тогда это означает, что каждая следующая прямая наклонена куда-то глубже, чем предыдущая.
И тогда каждая новая прямая, это вот что-то такое.
Такое и добавление тогда и происходит просто аналогично аналогично.
Понимаешь, да?
Вот.
И тогда эти точки реально можно хранить вектор и из них заморачивать.
Вот.
Ну, что делать, если у нас прямые все-таки добавляют...
Если этого предположения нет, то прямые добавляют все равно.
То можно в каком случае делать.
Ну вот.
Но оказывается так.
Ну, во-первых, если вам там пришла какая-то новая прямая...
Ну, первое вам нужно сделать.
Эта прямая имеет отношение к минимуму.
Или она оказалась где-то выше.
Как это сделать?
Как так проверить?
Это очень просто.
Надо просто найти нашей функции.
В этом направлении.
Бинбойс по углу отвода.
Да.
А дальше можно заметить, что если эта точка ниже этой прямой, значит тогда и вся эта функция будет ниже этой прямой,
эта прямая на минимум не повлияла снова никак.
Ну, потом от этой точки можно сделать на две стороны.
Да. А так, в чем даже уже не бинбойс, а просто идем в ступу.
Идем по этим точкам и аккуратно удаляем.
Так мы так же делали в прошлый раз.
Ну да, идейно очень похоже.
Ну, в плане в прошлый раз мы тоже просто в ступу ушли и удалили.
Ну, по сути да.
И тогда оказывается диспекер, что опять к нему приходится хранить два сета.
Один сет по иксу.
Другой сет по иксу.
Вот, собственно, у этих векторов по, скажем, векторному прежнему.
Вот.
Вот.
То есть может оказывается суть принципиально не меняется,
но почему-то, скажем, такая-то придастся конкретно, почему-то сильно популярна.
Хотя, в общем-то, по поводу идейно откроется.
Хотя нет, есть еще одна причина.
Потому что на самом деле часто реализуется, даже если прямые добавляются в рандомном порядке,
можно реализоваться неожиданное предположение.
Предположение будет заключаться в том, что все аиты, то есть все иксы заранее известны.
То есть давайте рассмотрим неожиданное предположение.
Все иксы заранее известны.
Ну, в файле.
Ну, вот эта формула, в принципе, намекает, что скорее всего это так.
Потому что видите, как тут аиты, почему-то окажется, что аиты там заданы где-нибудь в одно входе.
Ну, в общем случае, конечно, не так, потому что...
То есть тут, конечно, я написал примерную формулу, но, в принципе, заметим, что конвейсел 3 прекрасно работает,
если тут написано не DP житы, а что-то, зависящее, в том числе, от DP житого.
То есть там DP житы умножить на какой-нибудь вася кубки над житом, я не знаю, или что-нибудь еще в этом роде.
И, на самом деле, ашки и цешки, они тоже, на самом деле, могут вам подаваться в оффлайне.
То есть будут в оффлайне, а могут и в онлайне.
Что, аиты это?
Да, житы умножить на вася кубки над житом, это квадратичная формула, Сань.
Чего квадратичная?
Чего квадратичная?
Ну, в флайне, если мы дожиты еще умножаем на...
Нет, потому что главное, чтобы он просто от жит, то зависело только от жит.
Потому что если у меня тут скобы, потому что у меня тут вот это скобчики, я могу писать все что угодно.
Хоть квадратичная, хоть кубическая.
Главное, чтобы оно от жит зависело.
То есть как бы плечевое, чтобы тут зависит от жит, тут зависит от аты, тут зависит от жит.
Да.
Ну и кое-кто питаться там еще надо.
Так вот, предположим.
Так вот, теперь неожиданная идея.
Пусть там все иши заранее известные, и это уже реализуется в 99-е процессы.
Тогда, действительно, можно, оказывается, этого всего не писать.
Не писать.
Вот.
Нет, в общем, иногда один поиск по иксу не надо писать.
Ладно.
То есть, конечно, отмечу то, что мы сегодня с большинством из вас отмечили на кубке инфа-3, на самом деле.
Что изделали?
Отмечили на кубке инфа-3.
Потому что бывает ситуация, когда в кодексалтрике все работает за вот единицы.
Потому что бывает такая ситуация, когда у вас не только, скажем, прибыли добавляются в порядке правильного угла, да?
Но еще и оказывается, что все иксы отсектированы по возрастанию.
Тогда, оказывается, что никакими венпоисками их искать не надо.
Вместо этого просто храним, где мы нашли предыдущий икс.
И тогда, когда ищем следующий, просто идем от этой точки вправо.
У нас просто противополадная задача.
Прошу прощения.
Вот.
То есть, тогда оказывается, что все амортизировано за единицу работает.
Ну, мы просто вот на кубке инфа-3 столкнулись с такой задачкой.
Собственно, мы очень хорошо ее нашли.
Каждый порешает.
Вот.
Обещанная задача.
Там у вас амортизировка.
Там были инфа-3.
Да, все в одной задаче.
Такой кубок это был.
В этом семестре было.
Ну, в плане какой номер?
Я помню, что была задача F, по-моему.
В каком-то этапе.
В каком-то четвертый-пятый.
Хотя...
Понятно.
Ну, так, и последняя какая-то задача была.
Но помню, в какой-то четвертый-пятый этап мы уже выбирали.
Этот дирюзак и 20 весов?
Там, вроде, просто до того, как вы говорите.
Нет.
Нет, там что-то другое.
Там были...
Мы еще не попали в спутнине.
Нет.
Ну, не важно, ладно.
Нет, там было что-то там.
Были какие-то музыкальные треки.
И там что-то, если поставить в каком-то порядке,
то там было что...
И там что-то было, что надо было ссуммировать.
У них были какие-то F-ки надо было ссуммировать.
Разность соседних в квадрате.
И плюс еще там что-то за вещи от вот этих слешек.
А, то есть, просто разница в квадрате и плюс слешек.
И там просто оказывалось, что...
Ну, там первая идея решения.
Заветим, что надо сначала упорядочить повреждение.
А потом, например, последовательность.
Ну, дальше.
И после этого оказывался какой-то ринзак.
Вот.
И дальше оказывалось, что...
Вот.
Ну, не важно.
Это...
Убираем.
То есть, предположим, что их все тоже даются в рандомном порядке,
но они заранее известны.
Они неожиданно заранее известны.
Тогда оказывается, что вот это вот безобразие,
что в цитат, что в дикарховых деревьях, они не так.
Потому что неожиданно вокруг вступает гениальное изобретение человечества дерево-лечао.
Вот теперь, собственно, пришло время узнать, что это такое.
А зачем нам заранее известны?
Чего?
Зачем нам заранее известны?
Ну, как видим, что высота была логарифом.
Ну, в принципе, да, можно, конечно, пытаться делать не ядрое дерево-лечао,
но алгоритм тогда будет не N, а координат.
Акция, да.
Но не ядрое дерево-лечао.
Ну, давайте, ну, давайте.
Зачем в том, что иксы заранее известны?
Если они заранее известны, я их могу эти иксы просто отсортировать.
У меня будет х1, х2, х3, х4, х5, х6, ну и так далее.
Ну и так далее.
Тогда у меня неожиданная идея.
Я построю что-то типа дерево-адрес.
Вот такой вот.
Ну, это типа дерево-адрес, но правда инвариант тут сформулировать не так тревально.
То есть, на самом деле, инвариант, по сути, в нем будет...
То есть история такая, что в каждой ячейке хонится премьера.
То есть в каждой ячейке хонится премьера, в каждой ячейке хона.
что в каждой ячейке хранится прямая, ну не более чем одна прямая.
Давайте пометим, что у нас хранятся прямые какие-то.
И на самом деле инвариант заключается в том, что все хорошо.
Что такое все хорошо?
Потому что если вы уже делаете инвариант, мы скажем, как мы будем искать минимум точки х.
Очень просто. Мы возьмем эту точку х, прогуляемся от ее соответствующего елиста до корня,
переберем вот эти логарифы в прямых и попробуем подставить этот х в логарифы из этих прямых.
И выберем из них лучше.
Требование инварианта будет заключаться в том, что в каждый момент времени для каждого из этих типсов это правильный ответ.
Понятно, да? То есть мы это сейчас не доказывать будем, мы это поддержим.
Ну, когда у нас одна прямая, это поддерживать легко. Давайте просто добавим прямую во все ячейки или просто поделим более.
Итак, мы теперь предположим, что нам добавилась новая красивая,
исследовать начальство, фиолетовая прямая.
Давайте добавим фиолетовую прямую. Что такое?
Рассмотрим корень.
Ну, во-первых, заметим маленькую, да, рассмотрим в случае, когда фиолетовая прямая оказалась параллельно прямой находящейся в корне.
Тогда смотрите, если эта прямая оказалась параллельно прямой в корне и выше ей,
то в принципе заметим, что эту прямую можно игнорировать, правда?
Да. Приятная идея, да?
Вот. То есть более того, на самом деле даже вверх более общий случай.
Если эта прямая оказалась выше на отрезке x1, xn, то в принципе тоже ее можно игнорировать.
X1, xn?
X1, xn.
Да-да.
То есть они даже не пересекают, то есть они пересекают, но где-то за пределами этого отрезка.
Вот, да. Но с другой стороны, есть еще второй такой случай близне, приятный.
На этом отрезке эта фиолетовая прямая оказалась ниже.
Кто тогда заменил?
Тогда заменяем, что мы можем просто выкинуть вот эту красную корневую прямую,
вместо нее поставить фиолетовую, и будет окей.
Почему? Потому что мы знаем, что ответ на задачу теперь либо то, что было раньше, либо вот эта фиолетовая прямая.
Но вот так как фиолетовая прямая всегда лучше корня, давайте ее подставим и возразимся.
Ну и теперь самый интересный случай.
А что делать, если они пересекаются?
Ну идея такая.
То есть тут вопрос, где они пересекаются?
Где-то в диапазоне левой половины х или в диапазоне правой половины х?
Вот давайте предположим, что правая.
Ну для левых рассуждения будет аналогично.
Вот, вот как-то так.
Тогда идея такая.
Если у нас точка пересечения где-то в диапазоне правых х, где-то вот между ними,
то тогда заметьте, что в левой половине одна из этих прямых доминирует в другую.
В нашем случае сейчас, как я нарисовал, фиолетовая прямая лучше красной.
Тогда если я заменю корня красную на фиолетовую,
то тогда для левой половины х ответ будет уже правильный.
Логично, да?
Но с другой стороны, для правой половины х,
тогда эта бывшая прямая корня вполне может оказаться и правильной.
Что же делать?
А давайте эту прямую добавим рекурсивно в правую половину.
То есть мы теперь пойдем вот этого правого ребенка
и вот эту прямую пересечем уже вот с этой прямой
и будем делать то же самое.
Понятно, да?
Ну еще там мог быть случаи, когда на этой левой прямой
наоборот корневая прямая лучше, чем фиолетовая.
Вот тогда в этом случае рекурсивно будем добавлять все левую прямую,
а корень красную.
То есть вот такое аккуратное решение получается,
которое дает нам логарифм n,
это если дерево не явное.
Вот это если дерево явное, то мы все иксы знаем.
В принципе, да.
То есть ни в какой зичейке прямые лежать не обязаны.
На самом деле...
И в принципе можно хранить не явные дерева.
Личево пишется вот так.
Вот, и тогда...
Тогда тут что получается?
То есть если вы знаете, что все иксы, например, целые
от 1 до 10 в 9, тоже нередко наблюдается,
то вы можете построить неявные дерева,
личево, на 10 в 9 левше.
И тогда у вас логарифм будет не логарифм n,
а логарифм по 5 в 10 в 9.
Ну там еще нужно декорировать.
Если до 10-6, то 10-15 уже.
Но если иксы не целые, может быть,
то до какой еще точности в них?
Ну там уже, да.
Это уже такие детали.
То есть рогатые губопехлоны могут быть.
То есть пропитаны.
Может быть еще там запомнить, чтобы с переспроеки.
Так что вот такая.
Вот про сутан.
Что идет дальше?
Вот такая. Вот про сутан.
Хорошо идет.
Так, это вот что такое у нас
конверс халд-дрифт.
Вот, есть ли тут какие-то вопросы?
Так, хорошо, тут никаких вопросов нет.
Ладно, значит, тогда это все, что я хотел сказать про конверс халд-дрифт.
Так, что же у нас дальше?
Так, дальше у нас такие шуты есть.
Так.
Это плохо, что вы достаёте тетрадь.
Да и почему?
Ага, да, да, да.
Особенно если поверить, что все, что у меня в этой тетради
я должен вам рассказать, то да, это еще больше.
Да.
Ну, во-первых, как бы тут не все имеют отношения к курсу,
а в большую часть из этого я вам уже рассказал.
Потому что там теория игр тут, например,
там дерево гоминаторов и так далее.
То есть и прочая радость, которая у нас уже была.
Так что не волнуйтесь.
Сейчас я ее достаю, чтобы просто посмотреть,
какой список на самом деле не очень сложных задач.
Я сейчас должен с вами рассмотреть.
Вот.
Так.
Я тут должен рассмотреть.
Да?
Да и нет.
Ой, или нет.
Попробую рассмотреть.
Ну, хотя ладно.
Ладно, не буду показывать, где зависит корреспонд.
А лучше просто сохранить себе
немного идей выпусти.
Потому что, говоря о фрешере,
о диаметре нельзя не упомянуть о такой
приятной вещи, как тернарный поиск.
Ну,
ну, и здесь на самом деле тоже.
Вот.
Значит, что же?
Итак, давайте так попробуем.
Вот.
Тернарный поиск.
Так.
Ну, о чем речь?
Вот.
Ну, на самом деле речь о том, что хочется искать минимум
на функция специального вида.
Что это за специальный вид?
Ну, давайте введем так.
То есть определение.
Мы уже почти дошли до генпоиска.
На генпоиск.
Мы близки.
Так вот.
Пусть у нас есть какая-то функция
от A до B.
И будем называть эту функцию унимодальной.
Если
существует такой X0
на отрезке AB,
что F строго убывает
на отрезке AX
и строго возрастает.
Вот.
Я тоже вот это не понял.
Что?
Я тоже не понял.
Еще там две точки.
Ну, я просто привык это отрезка
как подстройки писать.
Вот.
Ну, давайте так.
Так, пожалуй, адекватнее.
Вот.
То есть смотрите.
Чем приятна эта функция?
Функция приятна тем, что вот этот...
Допустим, если нам известно,
что функция унимодальная,
но минимум нам при этом неизвестен.
Тогда оказывается, что этот минимум можно искать
в генпоиске.
Генпоиск?
Ну, вот.
А именно за, допустим...
То есть, ну вот.
А именно...
Ну, точно мы этот X0 не найдем,
но мы найдем маленький отрезочек,
на котором он находится.
Этот отрезок.
Ну вот.
То есть если мы сделаем там K-итерации,
то длина этого отрезочка будет меньше отрезочка AB
где-то в полтора в степени K раз.
Даже не где-то, а просто ровно.
Да, именно полтора.
Почему?
Ну, потому что как у нас работает тогда-то поиск?
То есть функция...
То есть как выглядит функция?
Функция обычно выглядит вот как-то вот так.
Я ее вот так вот нарисую,
чтобы вы не начали думать, что она будет в этом.
Вот такая типичная унимодальная функция.
Как найти минимум?
А вот как.
Значит, здесь у нас допустим отрезок AB.
А, тут отрезок D.
Давайте разделим этот отрезок на три равные части.
Мой такой.
X0, X1.
Или XL, XL.
И давайте вычтем значение функции f в этой точке.
Я выливаю это способом.
Ну, предположим, что мы здесь предполагаем,
что у нас есть черный ящик,
который умеет по заданному X вычислять f от X за одну конец.
Тогда идея такая.
Предположим, что выяснилось, что f от XL больше.
То есть пусть выяснилось, что f от XL больше, чем f от XL.
Тогда выясняет мистическое утверждение.
Мистическое утверждение заключается в том,
что тогда я утверждаю, что слева от XL
минимум находиться заведомо не может.
Почему так?
Ну, понятно, почему.
Да просто потому, что если минимум окажется именно слева,
то у нас, оказывается, мистическая точка такая,
что у нее слева есть что-то меньше и справа что-то меньше.
А у нас такого не бывает.
У нас у любой точки либо слева все больше,
либо справа все больше.
Почему не так?
Ну, строго говоря, конечно, наказывали, но понятно.
Так, хорошо, утверждение.
Для любой точки X века что-либо слева от X значение функции f все больше,
либо справа все значения больше, в общем, строго больше.
Доказательство очевидно.
Вот, поэтому...
Так что тут, как вы видите, что это...
что-то тут не строго.
Ну, вот.
Так вот, тогда получается, что за два вызова функции мы можем сократить
отрезок поиска минимума в полтора раза.
Ну, понятно, что если наказалось наоборот, то мы кидываем правую.
Вот.
Корень из полтора.
Вот, то есть, по сути, в результате асимптотика на самом деле
от логарифма полтора...
Ну, конечно так.
Ага.
В камнячках пишу, потому что понятно, логарифма...
Значит, тут самое интересное, то есть b-a делить на ε.
Где ε, это с какой точностью вы хотите найти минимум?
Ну, еще на 2, да?
Ну, тогда да.
Раз уж вы логарифм полтора не делали.
Ну, тут вещь такая.
Да, тут конечно.
Можно оценивать количество запусков, когда будет...
Да, короче, 2.
Ну, будем это иметь в виду.
Ну, потому что да, отметим, что эту двойку можно немножко оптимизировать.
Да, кстати, в принципе, очень любят, на самом деле, оптимизировать...
То есть, этот тардардный поиск очень любят оптимизировать
на самом деле следующим образом.
То есть, я видел такой красивый...
То есть, когда кто-то пытался пихать тардардный поиск,
я, по-моему, видел, просто едва ли не...
По-моему, едва ли не въел от палок, кстати, в какой-то момент.
Или у кого-то из них.
Видел вообще эпический вариант.
Иксы, агамло...
В общем, что-то такое.
Ну да.
Ну, если вы откроете задачу про вписанную окружность в домашние,
вы там увидите столько оптимизации тернарного поиска в моем решении,
что вам на всю жизнь хватит.
Ну типа дан выпуклый многугольник, найдите вписанную окружность максимального радиуса.
Ну вот, там заходит вложенный тернарник, но нужно пихать с ноги.
Настолько пихать?
Ну типа там ты пишешь уже необычное,
оно работает десять раз дольше, чем тебе нужно.
Да?
Вот, и ты его прям оптимизируешь, оптимизируешь, оптимизируешь, оптимизируешь.
Вот это, видимо, как писать.
Ну, возможно, я не все окимизации тернарные поменял, но...
У меня вон там, типа, один...
По-моему, одиннадцать...
Нет, сейчас.
Двадцать один А плюс двадцать В девять на сорок один.
Ой, ой, ой, ой.
Ну, в принципе, да.
Но, в принципе, будем иметь в виду,
что такие оптимизации дадут.
Такие оптимизации есть, они приятные.
Но тут, конечно, слишком, конечно,
тут семерок разумеет,
он совсем сто один,
как бы, не всегда в полезном смысле,
иногда уже начинает точно сломаться.
Вот.
Значит, у нас двое.
Да, ну и нельзя не отметить,
что, в принципе, если
специально подгонять тут,
на самом деле, XA или XR,
то можно добиться того,
чтобы вот этой двоечки
тут не было.
Если тут у всех есть знаменитые
числа phi.
Ну, просто идея такая.
Идея заключается в том,
что вот у нас есть отрезок АВ.
Давайте подгоним
XA или XR
таким образом, что
когда мы вот перейдем к отрезку, скажем,
XLB, выяснилось, что одна из этих двух точек
это будет в точности XR.
Вот давайте такое подгоним.
Но как же это сделать?
А вот как.
Ну, потому что для того, чтобы это было,
чтобы это было так, заметим, что
должно произойти следующее. Надо, чтобы
XL-A делить на B-A
был просто равен
XR-XA
делить на
XR-A.
Но при этом еще
оказывается верно,
но при этом мы
напоминаем, что еще верно, что
что у нас верно.
Что
B-XR
равно
XL-A.
Ну, типа у нас симметрия такая.
Так, ну вот теперь
выясняет
естественный вопрос
объясните, к чему должно быть
равно XL и к чему
самое главное должна быть равна вот
этого константа.
Ну, давайте.
То есть, допустим, если эту константу
нарисовать она равна к чему-нибудь,
я не знаю, альфа,
ну, тогда какие условия на альфу
нас должны соблюдать?
То есть, давайте тогда
я предположу.
Ну, то есть, тогда
утверждение такое,
что должно быть так, что длина
если вот эта вот константа
это альфа, то тогда получается следующее,
что тогда если принять
отрезка AB за единицу
его длину, то тогда длина вот
этого отрезка равна альфа,
вот этого альфа,
а вот этого она получается
1-α
тоже умножить на альфу.
И тогда отсюда
мы выводим маленькую
приятную вещь.
То есть, альфа плюс
1-α на альфа плюс
альфа должно быть равно 1.
И тогда получается,
что, да, раскрываем
столбочки, что там у нас
получается? Альфа плюс
альфа минус альфа
квадрат плюс альфа
равно 1.
Или что то же самое,
0 равно альфа квадрат
минус 3 альфа плюс
1.
Понятно, да?
Так, ну понятно,
решаем уравнение, дискриминат
равен 5, альфа
равно получается 3,
соответственно, плюс-минус
корень из 5 пополам.
Ну, очевидно, что должно быть равно
минус корень из 5, потому что альфа должна быть
меньше интересной.
Вот такая константа.
Но приятнее, конечно,
посмотреть даже чему равно 1-α.
Потому что 1-α равно
корень из 5 минус 1
пополам, и это то самое
солотое счение.
Ну ладно, не совсем
то самое, конечно, но
ладно, оно равно, знаете, оно равно не 5,
оно равно 1.
Ну, потому что само корень из 5 плюс
идет пополам.
И поэтому
можно делать
итерации, но тогда,
чем приятное солотое счение,
тем, что лишних итераций делать не надо.
На самом деле нужно сказать, что вот тут,
если мы берем очень близкие
А и В, у нас получается
просто бинплост по знаку производной.
Ну, можно и так сказать, да.
А и В?
Ну, если мы берем очень близкие точки,
то получается как будто производная.
Ну, ведь это так.
И типа у нас слева производная меньше,
а справа больше.
Ну, да, правда,
никто не понимал, что производная существует.
Ну, да, но это
приближение.
Теперь вы сказали, что есть такая
воплисть, как торганальная поиска.
Она работает на университете.
Но университет, но в задачах
обычно, чтобы доказать, что торганальная поиска
работает, надо доказать, что функция университета.
Как же доказать, что функция университета?
Поверьте.
Но при этом не зная в точности
ее мегаполисность.
Ну, ладно.
Но самый популярный способ
доказать, что функция
выпуклась.
А что такое выпуклая функция?
Такой, давайте берем.
Для этого давайте уберем переводича у нас.
Оно мешает.
Ну, не то, что помешает, но...
Заговори мне об этом.
Жалко нас.
Ну, хорошая идея.
И ты иди, Игорь Иванович.
Вот. Итак.
Ну, давайте.
Будем делать как, значит, определение.
Значит, функция f
значит a, b,
v, r
выпукла вниз.
Если
ее
над график
на отрезке a, b
является
выпуклым множеством точек.
Вот, по идее, можно
определить функцию.
Не буду писать, но вы будете радоваться, что
аналогичным образом можно
определить понятие выпуклая
функцию с флотом.
Понятно, да?
Вот.
Теперь мы искали этот вопрос.
Что можно искать
выпуклую вниз функцию?
Есть, кстати, больших соблазнов сказать, что
выпуклая вниз на отрезке функции
неприоритно.
Такая полоса.
Ну,
это, конечно,
неправда.
Потому что мы можем
выколоть, мы можем
в самой левой границе
поставить точку разрыва.
Вот.
Но внутренней
точке разрыва, кажется, не должно быть.
Да.
Да,
мы заметим нашу. Пункция, конечно,
не неприоритна. И даже более
этого минимум не достигается.
В принципе, может
минимум быть выколотен.
Но если в концах непрерывно, то, кажется,
на отрезке тоже должно быть.
Да. Но давайте
смотреть. Потому что, очевидно, у нас
в точке разрыва производная
вертикальная, как будто бы.
Нет, ну,
смотрите,
ну, давайте смотреть.
Утверждается, что действительно
внутренних точек разрыва на самом деле
не бывает.
Почему?
А вот почему. Ну, потому что, допустим,
у нас есть какая-то внутренняя точка разрыва.
Так, ну, давайте посмотрим,
какой у этой точки,
то есть, заметим, что
если рассмотреть,
если х стремится,
допустим, вот этой х0 назовем,
х может стремиться справа,
х может стремиться слева. И в каждом из
изестреблений есть какие-то частичные пределы.
Просто?
То есть нам, по сути, надо доказать, что никаких
частичных пределов, кроме вот этого значения
фунта, кроме f от х0 у нас
нет. Правда?
И действительно. Вот давайте себе представим,
что у нас есть частичный предел,
который находится
выше.
Вот, допустим, у нас есть какой-то частичный предел,
который находится выше, чем f от х0.
Тогда смотрите, как это...
Ну, тогда у нас третья противоречия
в пределе f.
Смотрите, как редко.
Вот.
Потому что давайте
предположим, что у нас тут слева есть вот явно
какая-то точка, где-то очень-очень близко
к прямой, да?
Ну, ладно, не совсем так.
Вот, где-то достаточно...
То есть идея такая, что
пусть у нас, допустим, справа есть частичные
пределы.
Да. Давайте самый... Да.
И давайте самый простой случай рассмотрим.
Если у нас этот частичный предел достигается
и справа, и слева,
что это значит?
Тогда это значит, что у этой точки есть просто какая-то
окрестность, в которую тут попадает
там бесконечно много точек
и здесь видно.
Но если тут две точки в такие попадают,
то мы проводим через них
отрезочек,
а, и, собственно, что?
Ну, и, казалось бы, само по себе ничего.
Ну, надо не из них, а точку разрыва провести.
Да. Ну, хорошо.
В внешнем образом.
Какие у нас что-то про внешнюю?
Ну, в плане
и с правой точки провести
лопат-х.
Так, ну, проверили.
Вот, допустим, и с правой точки проверили.
Все эти точки должны лежать внутри
фигуры.
Так, да.
Ну, в принципе, да, этого уже достаточно.
И из этого уже следует.
Да, потому что, в принципе, да, это уже противогит.
Почему противогит?
Потому что наметим следующее, что вот этот
отрезок должен целиком лежать в надгразе.
Значит, получается, все точки по иксам
между ними должны лежать ниже этого отрезка.
Ну, или не выше.
Да.
Поэтому, если у нас тут предел лежит где-то выше,
то у нас проблемы.
Понятно, да?
То есть, в принципе, да. То есть, это означает, что
у нас не может быть предела справа
оказывается выше, чем мы хотели.
А может ли у нас быть...
Может вообще не быть
предела справа.
А может ли у нас... Нет.
Ну, у нас может быть предел ниже, чем это фатекс 0,
и такого противоречия не будет.
Понимаете, да?
Да, действительно. То есть, допустим, у нас нашлась
этот предел, тогда это
противоречие нам не даст. В принципе,
само по себе это действительно не противоречие,
потому что вот бывает вот такой случай.
Ну, смотрите, какая фишка.
Может же быть так?
Ну, смотрите.
Дело в том, что у вас не бывает частичных
пределов выше фатекс 0 ни справа,
ни слева.
Ну, слева тоже не бывает
частичного предела выше, чем
фатекс 0.
В том числе плюс бесконечность.
Но допустим, что у вас слева
есть частичный...
Слева неожиданно образовался
частичный предел тоже ниже, чем
фатекс 0.
А, частичный предел всегда
выше фатекс 0.
Ну, частичный предел, да.
Возможно, минус бесконечность.
Вот.
Но если оба частичных предела ниже,
тогда смотрите, какая фишка.
Берем тут точку, тут точку, соединяем.
И тогда оказывается, что вот эта
фатекс 0 должна быть ниже, чем
вот этот фатекс. А это не так.
Но, кстати, если этот вот частичный предел
был бы фатекс 0, то там думается,
что точку тоже можно было бы дать нафиг.
Понятна логика, да?
То есть получается, что
действительно внутренних точек
разрывов на интервале
оказывается нет.
Если у нас не существует частичного
предела неравного фатекс 0,
то функция не прерывна в данной точке.
Если у нас не существует частичного
предела отличного, то это происходит.
Точничный предел существует всегда так,
что получается дистый предел фатекс 0,
а это есть определение не прерывного.
Так что вот так.
Но на самом деле для нас...
Ну давайте так.
Мы видим, что если бы я определял эту
штуку на интервале, то я бы, конечно,
обнаружил, что на интервале
функция не прерывна, правда?
Я мог бы ее определить на отрезке за интервала,
то есть мы тут доказали утверждение,
что...
То есть выпуклая
там F,
значит, АВ,
она, соответственно,
не прерывна
на, соответственно,
этом полуинтервале.
Хотя у вас в том вот анализе не возникало
налоговых утверждений.
И...
По-моему, в первом смене мы обсуждали
выпуклость.
Нет, вы должны были обсуждать выпуклость,
потому что вы должны были считать вторую
предусмотренную рисовой реалией.
Мы обсуждали выпуклость, вроде бы мы про это говорили,
но сто раз об этом не сказали.
Так.
Ну, хорошо.
Значит, теперь, оказывается, есть еще.
Ну, теперь, значит,
еще такое мистическое утверждение.
Значит, теперь вот есть
мистическое утверждение.
Если у нас на полуинтервале есть
выпуклая F,
выпуклая...
Внимание!
Внимание!
Выпуклая...
Внимание! Не монотонная!
F от A-V.
Ай...
Хочется решение модально.
Ну, давайте спелки.
В смысле выпуклая
всегда вниманная.
Ну, очень хочется так сказать.
Но даже не вылечена тюркрика,
и тут все упало.
Выпуклая, не монотонная,
вот по этой вот причине.
Ну, вот.
Ну, вот.
У нас строго выпуклась.
Ну, надо либо строго выпуклась,
либо для наших целей
на самом деле подойдет
небольшая модификация
определение унимодальности.
Да, я сейчас немножко в наглую
попьем.
когда у вас минимум находится не в какой-то точке, но в
в ней его должно быть строгое убывание, строгое вырастание.
тогда вот это не...
почему, а работает?
выбрали XL, XR, они попали на 1
если они попали на минимум, то уже не важно, что будет
важный момент такой, если они...
ну, допустим, во-первых, XL больше, чем XR, допустим, да?
тогда я все еще утверждаю, но, во-первых, решение такое
XL заведомо не минимум, правда?
сейчас XL...
нет, а что если мы получили F от XL равно F от XR?
тогда я утверждаю, что у вас два варианта
либо XL и XR оба попали в этот минимальный отрезок
либо они попали по разные стороны от этого отрезка
и тебе по парабану, собственно, толком выкидывать
но если на отрезке, то тоже...
нет, сейчас, у нас же теперь может быть такое, что...
вот там отрезок, который горизонтальный
именно снизу у нас утверждение отрезок снизу
да, то есть важное, строгое убывание, строгое возрастание, кстати, осталось
помечу на всех числах
а, все, помню
то есть это важно, потому что если у нас были функции какие-то вот такие
то да, то есть, на самом деле, если у нас попадутся равные значения
то мы действительно не понимаем, что делать
да
вот просто что делать, кстати, интересно, есть у тебя пашный вопрос
ну, вот сейчас перерыв, поэтому буду на перерыв задать вопрос
давайте
сейчас на такие ответы
нет, просто интересно, где у вас стандарты будут
как говорится, в Санкт-Петербурге недавно закрылась перемота одной станции метро
и там по местным телеканалам
и даже там просто вокруг нее появились плакаты
с описанием того, что делать
а как называется эта станция?
я не помню, станция метро в Санкт-Петербурге, это не обязательно
ну а кто автор?
ну, ну что вам говорить, мне на них не скажут
интересно, для этого...
вряд ли для этого вопроса нужно сказать, станция метро в Санкт-Петербурге
ну вот, кто автор к ним?
ну, в смысле, кто виноват и что делать?
ну да
ну беговая, наверное
автор, да, автор, так это фамилия, беговой, да?
ну в плане что делать, бегать, снимать штаны и бегать
нет, конечно
да, так и рассуждено
у ЧГК-шников что делать, как-то фамилия Чернышевский появляется там
но мы ничего не...
но мы, к сожалению, маленькие руки
но я считаю, что беговая одна
нет, у меня нет, у меня был другой, я просто на автоматичку отбегал, телескай
почему?
а, ну потому что это там, потому что это вот эта вот станция, в которой там этот переход, вот этот вот линии переход
вот сегодня там открыли вот это вот
чего, а что делать?
ну что делать?
ну типа что делать, если там этот переход закрыт, а стой, развиви там, вот это все
я там что-то, а они так очень сильно действуются
не, не, я просто не подозревал такого глупого знания Питерского, там Питерского транспорта
ну вот, ну как бы да
там вторая идея, там вторая
ну просто по слову, что делать, у меня просто такая ситуация сегодня
так что
так что, главное, пришла время переговорила
так
ой
вот
итак
так вот, вот утверждается, что если модифицировать
определение униматальной функции, в том числе таким образом с отрезком, то тогда, на самом деле
хочется сказать, что утверждение, ну такое утверждение уже верно
вот
вот, спрашивайте, почему же оно верно?
ну, во-первых, тогда давайте начнем с того, что минимум где-то на этой функции возникает
да, она не монотонная
да, начнем с того, что она не монотонная, потому что была бы она
ну потому что действительно была бы функция, там выпуклая функция монотонная, это могла быть какая-нибудь параллока
с вполне себе частичным пределом плюс бесконечности
вот вполне себе такое могло быть
ну, то есть там отдельно отметим, что не может быть там как-нибудь
то есть видимо, не может быть параметрия, внутри одного отрезка, чтобы частичный предел и плюс бесконечности и минус бесконечности
ba
да, да, да но внутре ограниченной полосы уж точно
вот
Но давайте думаем, что у нас тут.
Вот.
Вот.
Давайте смотреть.
Как доказать, что линия существует так?
Ну, начнем с того, что у нас функция непрерывная.
И сделаем еще вот так.
Так функция не монотонна.
Да?
Ну, когда мы говорим, выпуклое, давайте сейчас мы будем сказать, что оно будет сложнее.
Да?
Во-первых, давайте так.
Что означает, что функция не монотонна?
Ну, есть три точки.
Это означает, что есть три точки, да.
То есть либо вот такого вида, либо вот такого вида.
Но заметим, что у выпуклой вниз функции вот такого вида не бывает.
Не бывает.
Не бывает.
Ну, причина очень проста.
И обнаружим, что вот эта точка отрезка находится под графиком.
Строго.
Да.
Вот.
Поэтому действительно возникает, видимо, какая-то вот такая ситуация.
Возникает какая-то такая ситуация.
Вот.
Чему, соответственно, это нас приводит.
Вот.
Ну, на самом деле...
Ну, можно на самом деле сказать так.
Я упреждаю, что эта функция либо монотонна на этом отрезке, либо монотонна на этом.
Ладно.
Или потому что, в принципе, возможно и то, и то.
Понимаете, да?
Вот.
Ну, впрочем, на самом деле ладно.
Это даже особо неважно.
Не кажется ли вам, что вот, как минимум, вот на этом отрезке h'b' функция f охранечена снизу?
Прямой.
Прямой.
Какой прямой?
Ну, хотя, да, на самом деле...
Ну, хотя на самом деле можно заметить следующее.
Проведем вот такую прямую.
Мистическое утверждение.
Функция левее вот этой точки, назовем ее z'3, она заведомо лежит выше этой привода.
Ну, потому что лежала бы она здесь, был бы вот такой фейер, правда?
Тогда получается, что...
То есть, на самом деле, тут вся функция лежит вот здесь, но и слева тоже.
То есть, значит, это...
То есть, в принципе...
Ну, вот.
То есть, в принципе, это...
Да, может, на самом деле, конечно, очень соблазнительно показаться, что функция,
то есть, минимум, находится четко в этой точке.
Но это, конечно, не так.
Какой-то.
Потому что, на самом деле, вполне себе вероятно, что тут, на самом деле, будет вот...
Вот так.
Вот так.
Вот так.
Ну, как бы, надо просто сделать так, чтобы эта прямая шла вот так.
Понимаете, да?
Вот.
Но факт остается фактом.
Вот.
То есть, факт остается фактом.
То есть, на самом деле, там, можно так сказать,
если мы вот в этой полосе приведем вот такое,
вот такое, вот такое,
то тогда, значит, у нас функция заведомо ограничена снизу, как минимум, в этой точке.
Вообще, тисичный предел немножко бесконечно снимается до чего-то такой вверх.
Или можно, да, ну, просто так доказать проще.
Угу.
Да, я мог бы там начать ругаться вот такими словами,
но как ругаться?
Может даже и хорошие доказательства.
Пока давайте так.
То есть, как минимум, на этом отрезке функция ограничена снизу,
как минимум, на этом отрезке.
Ну, на самом деле, теперь я утверждаю,
что слева от h3 функция обязана быть монотона.
Она может только монотонно убывать.
Ну, для нас, в общем-то, конечно...
Хотя для нас достаточно будет, конечно, только утверждение о том,
что все значения функции слева от h3 будут выше, чем эта точка.
Ну, причина та же.
Вот.
Ну, на самом деле, да, можно...
Ну, на самом деле, теперь еще легко показать,
что там еще и будет монотонность,
потому что если будет не монотонность,
то будет что-то подобного рода,
и опять появился вопрос.
Да?
Понятно, что я говорю?
Вот.
Ну, и справа тоже, кстати,
тут тоже функция будет заветом монотона.
То есть, смотрите, у нас функция,
которая здесь до h3 она убывает,
начинает с h3, возрастает.
И что происходит между h3 и h3?
Наверное, b.
Вот.
Но, смотрите, зачем мы...
Но как вы думаете, зачем мы доказали,
что функция ограничена снизу?
Чтобы sql был не без безгонечности.
Да, именно.
Потому что мы помним, что функция
у непрерывной на отрезке функции
на этом отрезке достигается минимум.
Вот было дело, да?
Так, чего?
Ну, собственно...
Ну, потому что мы доказали,
что функция непрерывна,
а что мы доказали, что она на этом отрезке
непрерывна и ограничена снизу, да?
Значит, минимум...
Ну, нам не надо доказать, что функция
ограничена снизу.
А, ну получается так.
Можно было просто определить в концах
отрезка по непрерывности.
Ага.
А это неважно было.
Не, а что конец отрезка?
А шрифт b шрифт, какие-то функции внутри,
поэтому на этом отрезке она не прерывна.
Я имею в виду, что она непрерывна на отрезке,
значит, достигает инфинума,
причем слижащего в r.
Ну, короче, не безгонечности.
Ну, да.
А шрифт c шрифт b шрифт,
то определим нормально.
Не-не-не.
Это шрифт b шрифт.
Нет, в смысле, вот у нас функция.
Вы сказали, что что-то плохое,
то есть разрыв может быть только в концах
отрезка a или b.
Ага.
Ну, давайте переопределим функцию в концах.
Надо только понять, что частичный предел
не минус бесконечности.
Да, но тогда...
Я сказал, пусть функция не монотонна.
Это вот три точки, благодаря которым
функция не монотонна, да?
Нет, в смысле, ваше доказательство понятно,
говорится, что...
Ну, в смысле, что можно проще доказать.
Ну, понятно, да.
Нет, ну, мое тоже могу сказать так.
А шрифт b шрифт c шрифт.
То есть просто, да, на этом отрезке
а шрифт b шрифт функция непрерывна,
следовательно, минимум не минус бесконечности достигается.
Почему а шрифт b шрифт?
Но а шрифт b шрифт,
это не рандомные точки,
потому что между ними есть точки меньше.
Нет, почему а шрифт b шрифт это не концы?
Это концы отрезка.
Но это не те концы, это не эти концы.
То есть, помните, это три...
а шрифт b шрифт c шрифт – это точки внутри
интервала a, b, которые дают нам
не монотонность функции f.
То есть пусть а шрифт b шрифт c шрифт
такие точки, что
f от c шрифт меньше, чем и f от a шрифт,
а f от a шрифт и f от b шрифт.
То есть зачем нам это надо?
А надо было за тем, чтобы гарантировать,
что минимум на этом отрезке достигается
не в а шрифт или б шрифт,
а достигается где-то между ними.
Итак, ну вот.
Ну и в принципе, да.
То есть тогда получается, что убираем c шрифт,
а вместо этого обнаружим, что где-то
внутри этого отрезка есть прям вот минимум.
Назовем его по традиции x0.
Вот.
Так, ну вот теперь
почему-то жуткое утверждение,
что хочется сказать,
что функция действительно внимательна.
Ну, во-первых, да, заметим, что
этот минимум ристоль может достигаться
не только в точке, а на целом плацдарме.
Но этот плацдарм, очевидно, является отрезком.
У нас функция не пригроза.
Тогда утверждается, что
после этого плацдарма,
то есть слева от этого плацдарма
института функция убывает.
Почему она убывает?
Потому что любые две точки слева
будут как бы больше
по значению, чем этот плацдарм.
И если тут окажется как-то так,
то выпутнустю порядка.
Ну и здесь соответственно тоже.
Вот такие вот утверждения.
Вот такая вот высота у нас получилась.
Сейчас еще раз.
Почему мы не могли сказать?
Просмотрим отрезок h'b'3.
Ну, функция непрерывна
на этом отрезке,
поэтому она достигает минимума максимума.
Так, и дальше.
Только нам здесь очень важно,
что мы здесь воспользовались тем,
кто тут минимум не в h3 и не в h3.
Это, видимо, чтобы сказать, что
слева от h' и справа от b'
не будет точек меньше?
Ну, типа так.
Хотя, конечно, опять видимо подлянка.
Видимо, формальная подлянка
видимо заключается в том, что
чтобы их учитывать
чтобы их учитывать.
Эротические функции никто не мешает быть, например.
Константы
или там быть вот такой глядостью.
И у нас опять проблемы с определением униматальности.
Нормально это.
Я это, конечно, не люблю.
У вас вы же из отрезка взяли x на x?
А, да, да, да, да.
Поэтому нормально.
Ага.
Чтобы у нас тут униматальность имперевали.
Хотя...
Единственное, что, да, в самих точках отрезка
она может быть не непрерывна.
То есть, если она имеет вид
две точки и отрезок
на плане другой...
Я допускаю, что здесь может быть полуинтервал.
В случае полуинтервала здесь тоже отрезка.
Вот так. Сейчас подогнали.
Нет, это еще плохо.
Сейчас.
Ну, короче, она не будет на отрезке x0, x1
константа.
Если мы возьмем функцию,
которая в двух крайних точках
не непрерывна,
а на всем интервале константа.
Уууууууу
Ха-ха-ха
Ууу
Нет, ну еще правильно.
Я хочу, может, в этом случае
на интервале рассмотреть.
А...
Ну, она на всем интервале константа.
Да.
При этом в концах отрезка мы ее
доопределим, чтобы она была не непрерывна
вверху.
Ну, короче,
это, наверное, можно не париться особо.
Нет, ну, идейно так.
Ха-ха-ха
Ну, это, что будет правда на экзамене
Ну, это от вас зависит.
Нет, ну, правильная ответ простой,
как бы, если вы не на экзамене
предемонстрируете, что
скажем так, вы вытепила вся к шагу
то там как бы, тсс, без отрезки не буду.
А если, там, есть обновления,
что вы там будете воспроизведить какой-то там доказательство
что я нашел первую правую расстрелку, тогда будет хорошо.
То есть как всегда.
Ну так всегда и было, в общем-то, на команде.
Вот, ладно.
Дум-дум-дум
Дум-дум-дум
Дум-дум-дум
Да, да, да.
Все, вот с ним.
Кошмар.
Ность.
Оказывается, воскресенье сгорел тот самый отель,
глядя на который было написано песня «Словом зоомор».
Вот, вот я только что просто об этом не знал.
Это вам позвонили?
Владелец отеля?
Не знаю.
Дайте, давайте, давайте.
Каждому человеку может быть дом, который может вам присоединить с космодаром.
У тебя там уже сообщена информация.
Нет, ну мне кажется, что-то в этом есть, короче.
Ну вот.
Пад сложный как бы был.
Так, давай.
А функция не монотонная в каком смысле?
В каком смысле?
Что она не строго, не монотонная?
Ну, может ли такое случиться, что все точки h' b' и вот это c, они типа на одном режиме?
Так.
Так.
Где они могут быть, Влад?
Ну, может ли случиться, что вот f от h'
короче, на отрезке h' b' константа может быть такое?
Так, сейчас.
Сейчас f от h' b'.
Может так?
А, слушайте, мы эту проблему входили.
Все проблемы полно решались здесь тем, что вы в этом утверждении сказали, что функция вот такой не монотонная.
Да.
Да, вот эти вот функции, они все удали, по идее, монотонные, это не строго монотонные.
Ну вот, то есть мы запрещаем даже не строго.
Так что ладно.
Так что на самом деле это не проблема.
Ладно.
Но идея для нас немножко другая.
О, смотри.
Немножко манья.
Немножко манья.
Это да.
Ладно.
Вот, значит, смотрите.
Значит, диспетенция такая.
Ну опять про выпуклый, то есть если функция выпуклая, то она монотонная, минимум в ней, так наверное, поиска пускать можно.
Вот.
Правда теперь стоит, да, теперь что получается, то есть получается следует, чтоб доказать,
что функция не банальная, то нужно доказать, чтоail
Ну нужно что бы, сейчас, мне кажется достаточно, чтобы она сначала возрастала, а потом aprly
Нет, ну это, кстати, это буквы вверх.
Ну, в данном случае, например.
Нет, ну не совсем нет. Почему?
Так-то вот, пожалуйста, вот это сначала убывает, а то вот растает, но выпустостью не пахнет.
Но за того не модально.
Да.
На самом деле, ну, выпуклыми функциями являются, конечно, много приятных базы функций.
Ну, например, функция игрок равно kx плюс b.
Да, вот это понимаешь. Она выпукла, причем как вниз, так и вверх.
Приятно. Единственное в своем роде.
Вот. Ну-ка, отстань ты сюда.
Есть еще, конечно, функция равна какая-то там ax квадрат плюс bx плюс c.
Вот параметр такая.
Эта функция выпукловней.
Ну, там, точнее, если a больше 0, конечно.
Ну, есть, конечно, функция, какая-то, какая-то, которая выпуклывается из x, она выпуклывается вверх.
Есть еще, конечно, всеми любимая функция, модуль x минус a.
Ну, галочка такая, да?
Поняли, да?
Ну, говорите.
Ну, в общем, практически все, да.
На самом деле, для нам сейчас будут интересны два приятных свойства.
Часть синуса.
Да, часть синуса.
Вот.
Вот.
Вот, значит, соответственно.
Потому что на самом деле для нас есть важные приятные свойства.
То есть пусть у нас f и g выпуклые в них.
На, допустим, полуинтервалянды.
Тогда оказывается, верно, два приятных штуки.
Во-первых.
Во-вторых, выпуклым оказывается максимум из этих.
Какая функция f и g?
Ну, это очень просто.
Потому что под график максимума это пересечение под график f и g.
А пересечение выпуклых ложек.
Вот, правда?
Вот, понятно, да?
Да.
Более нетривиально с точки зрения такого определения.
Хотя бы с точки зрения производных было бы совсем халявно заявить,
что f плюс g.
Да, это может показаться, это может быть не так тривиально.
Пока вы не вспомните, что на график этой штуки это что такое.
По сути сумма Минковского над графиком функции f и g,
вы знаете, что это сумма Минковского.
Не очень хорошо.
Это можно смотреть в определении выпуклости,
которая что f там что-то 1, 1 минус t.
Ну, в смысле и тесно.
Давайте введем сумму Минковского,
потому что все равно нам придется ее обсуждать в какой-то момент.
Вот, значит, смотрите.
То есть сумма Минковского двух множеств определяется так.
То есть пусть у меня есть a, b под множество точек на плотности.
Тогда a плюс b это просто множество точек a плюс b таких,
что a лежит в a, b лежит в b.
Но я подозреваю каждую точку как объекта, естественно.
Ладно, на график это, конечно, не сумма Минковского.
На график это, конечно, не сумма Минковского.
Нет, сейчас у нас же есть определение в выпуклости,
что f а плюс b это b там меньше либо равно,
то есть больше либо равно, чем f а плюс a плюс b.
Если а плюс b это равно 1.
Ну не совсем.
После определения в выпуклости.
Ну да, можно на самом деле дать.
То есть f плюс g каждую штуку придется доказывать так,
что действительно рассмотрим какую-нибудь...
То есть пусть у меня есть какие-нибудь точки a, b,
и мы скажем, что есть вот точка...
Ну понятно, есть точка...
Там вот такая a, f от a плюс g от a.
И соответственно b, значит f от b плюс g от b.
Вот, и вот это...
Нам надо доказать, что вот этот вот...
Желательно доказать, что этот отрезок целиком лежит
на графике функции f плюс f плюс g.
Достаточно доказать именно про этот отрезок,
потому что если там брать вместо этих точек
более высокие, тогда отсюда будет следов.
Вот.
Вот, вы заметили на самом деле следующее,
что вот этот отрезок, ну я так скажу,
этот отрезок это часть линейной функции y равно kx плюс b, правда?
Ну какая-то линейная функция, да?
Но это не просто линейная функция,
а на самом деле, смотрите, можно было...
То есть я не буду сейчас полную формулу расписывать,
но суть такая, что...
Например, я мог бы тоже самое расписать для функции f,
там b и f от b, да?
Ой, кx плюс b нехорошо получилось.
Ладно, кx плюс b.
Вот. И тогда вот этот отрезок,
это у нас такой y равно k с индексом f,
значит, x плюс b с индексом f.
То есть вот была такая прямая,
и мы знали, что это прямая...
То есть вот этот отрезок этой прямой целиком лежит
на графике, правильно?
Примаете, да?
Аналогично рисовался отрезок k, g.
То же самое.
b, g от b,
и a, f от...
там a, g от a.
То есть две линейные функции.
Но тогда получается, что вот эта штука
это сумма этих двух линейных функций, правда?
То есть просто вот действительно здесь замечаем,
что k равно просто...
k, f плюс k, g.
И там, соответственно,
там l равно...
но ведь тоже l,
l, f плюс l, g.
Но тогда, если у вас f была,
на этом отметке, больше либо равна этой функции,
а g больше либо равно этой,
то сумма будет больше либо равна сумме.
Здесь мы просто пользуемся тем,
что сумма линейных функций, линейная функция.
Всё.
То есть можно было бы, я мог бы тут
расписать, конечно, точные формулы,
но по такому доказательству
конечно, в этом даже емкость уже нет.
Да, сумма Винковского, конечно, не причём.
Зря мы её помизали.
Нет, ну, то есть, конечно,
чуть позже мы, конечно, её тут ещё
пообсуждаем.
Ну вот. Куда ж? Куда ж делаться?
Я боюсь, что это, возможно,
на ползавета придётся делать.
Впрочем, посмотрим.
Так вот.
Ну, вот так вот.
Вот у нас получается такие два свойства.
Качество управления остаётся доказать,
что какой-нибудь там минимум
из этого же выпукло быть не обязан.
Ну, то есть там какой-нибудь пример имени
из репка, ещё там ножки и шляпки
из репка, я думаю, вы там придумываете
состояние. Зачем? В смысле, просто
две прямые.
Ну или так.
Это уже кому как угодно.
Ну и там вся, конечно,
что разных выпуклёв в функции
тоже ничем быть, естественно, не обязан.
Вот.
Но зато, если
из вот этих свойств часто вот оказывается
достаточно доказать, что
пункт там, допустим, выпуклён.
Ну, потому что, например,
ну, то есть, например,
в трендарном поиске можно искально
решать какую-нибудь весёлую задачу
в духе.
Даны n точек напрямой,
вот, допустим, даны
точек напрямой.
Я хочу найти
какую-нибудь такую точку
точку A,
точку X, что
сумма расстояний до
точек X минимально
воздушна.
Вот, в принципе, уже из вот этих утверждений
можно сказать, что эта задача прекрасно
решается в трендарном поиске.
Понимаете, да?
Вот, понимаете?
Правда, есть оговоры.
Эту задачу решать
в трендарном поиске не надо.
Потому что, на самом деле,
окажется, что
оптимум достигается в точке
X, n пополамка,
как лёдная вверх.
Да.
Вы можете, да,
можете удивиться,
пока не начнёте рисовать
график этой функции.
А как строить график этой функции?
Ну, заметим, что до X, n
у него угол наклона равен
с точностью минус n.
Ну, просто у нас точки делятся на пары
противоположных.
Да, нет.
X1, Xn, X2, Xn, X1.
И если точка внутри отрезка,
то это длина отрезка, что константа.
А если вне отрезка, то это больше, чем длина отрезка.
Ну, можно так сказать.
Минус n плюс 2,
тут минус n плюс 4,
ну и так далее.
То есть, очевидно, что где-то в середине
тут будет переход,
в зависимости от чётности n.
Если n чётно, то там в какой-то момент
угол наклона будет ноль.
Вот.
А если не чётно, то значит,
в середине точки угол сменится
с минус 1 на плюс 1,
и там уже ноль.
Вот.
Понятная идея?
Вот, да, нет, наверное.
Вот.
То есть, можно было решить тернариком, но не надо.
Всё ещё? Зачем ещё нужен тернарик?
Так, ну хорошо.
Можешь решить хорошо.
Давайте ту же самую задачу.
Но,
ну,
но
хорошо.
Бывает, что вот такая.
Так.
Тоже можно решить тернариком.
И тоже не надо.
Почему?
Потому что здесь мы
посмотрим более простым способом,
раскроем скобки.
n х в квадрате
минус 2х сумма эксцитов
плюс
сумма эксцитов в квадрате.
Так.
Чтобы решить эту задачу,
надо просто вспомнить в уроке вашей любимой школе.
Где-нибудь в классе в седьмом.
Или в каком классе?
В третий заклинание.
Кратиком этой партии
является парабола
ветрика только и направо
и вверх.
Было такое заклинание в школе?
Ну да, может и таким идеотическим голосом,
конечно, это говорилось.
Идеоматическим.
Идеоматическим.
Идеоматический голос.
Вот.
То есть кого-то представляет,
это длится включить.
Но главное, что для нас есть,
это то, что у этой параболы мы еще не знаем,
где находится вершина.
То есть х вершины,
ну просто я формулу скажу,
значит минус B делить на 2A.
Поэтому получается просто
сумма эксцитера будет делить на N.
Центр масс.
Да.
Ну сейчас я скажу,
на моей практике оказывается,
если кто-нибудь случайно занимается
липиатры программирования.
Чем?
Слышите когда-нибудь про чемпионаты ICTC?
Нет.
ICTC.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Ну вот.
Это такой случай,
который выделяет
отдельных...
один полезный знак.
В общем, так не работает
и приходится думать.
Нельзя ли запустить торнак.
Ну какие тут классические задачи есть?
Отдать классических задач
естественно это
какая-нибудь задача прогонки.
Например, прогонки на колесницах.
Не колесницах,
есть один из вариантов гонки, задача про биатлон. Едут у нас биатлонисты. Каждый биатлонист в
момент времени 0 находился в точке x0 и едет со скоростью v0, которая почему-то у него не меняется.
А вы корреспондент, вы хотите это сфоткать. Хочется сфоткать трассу так, чтобы были видны
все товарищи. Хочется сделать так, чтобы они получились получше. Хочется сделать так, чтобы
сфоткать их в такой момент, чтобы отрезок, на котором они находятся, был как можно меньше.
Как же его найти? В гонке у нас участие 100 тысяч человек, как всегда. Типичный год для
спортивного программирования. Вот внимание вопрос. Данно 100 тысяч таких партий.
Найти в момент времени, в который они все находятся на отрезке минимальной длины.
Они начинают в разных позициях. Они не знают, когда они начали. Когда корреспондент пришел в момент времени 0, они уже были в разных позициях.
Но при этом их скорость уже устакается. Погодите, он не хочет, чтобы два человека были в одной точке?
Почему? Нет, это его устраивает. Но ему главное, чтобы он хочет сфоткать всех товарищей.
Если они себя окажут в одной точке, его это супер устраивает. Но она может не провести.
Тема, конечно, занятия вымекает. Но нам же интересно, а почему тордарный поиск работает?
Есть, конечно, стандартный механизм имени у прихода. Давайте запустим тордарики и найдем оптимум.
А, получаем 2. Давайте поделим его на 100 отрезков и на каждую мизницу. Тордарик овощ прокачит.
Да, комитивы удачи авторам сделают тесты, которые это валят. Да, местами это бывает недоревиально.
Поэтому иногда, конечно, прежде чем писать тордарный поиск, иногда имеет смысл убеждаться, а может ли, вообще, имеет ли место, вообще какая-то уния, какая-то унимодальция.
Может, заодно и придумаем нормальную решению.
Ну, кстати, да.
Нет, конечно, сталкивались, но наизусть не помним.
А после порадования писали, так нагребы забыли.
Ну, понятно, что задачу помним.
Да.
Ну, скажу с какого возраста. Почему эти функции русские?
Ну, хотя идея простая. Давайте допишем.
Функция длина отрезка t.
Вот давайте так и напишем. Как это написать? Чему равно длина t?
Ну, это максимум.
И что?
Минус.
Минус что?
Ну, минимум.
Независимо, да?
Так, замечательно.
Так, смотрите.
Эта функция выпукла вверх.
В данном случае выпукла вниз.
Ну, точнее, она и то, и то.
Но раз она выпукла вниз, то максимум выпуклых вниз функций, очевидно, выпуклых вниз.
Теперь заметим, что эти функции выпуклые вверх.
Ну, поняли, да?
Можно минимум внести внутрь и минус внести в скобки и минимум заменить на максимум.
Тогда получится то же самое.
Ну, минус заменить на плюс, в скобках нарисовать минус.
Ну, скажем так, все-таки полезно иногда себе воображать все-таки это, что минимум выпуклых вверх функций выпуклых вверх.
Вот полезно это воображать именно в этом месте.
Одно дело, да, доказать-то понятно, можно, как ты сказал.
Но одно дело доказать, а другое дело, как все-таки воображать, чтобы подобные вещи появились.
Так что, да.
Ну, замечательно, что мы вычитаем из выпуклых вниз функции выпуклые вверх.
Но минус выпуклых вверх функции выпуклых вниз.
Поэтому получается сумма выпуклых вниз, сумма выпуклых вниз.
Ну и, собственно, все.
Так что вот так вот, на самом деле, мнонеподобные задачи, на самом деле, могут решаться.
Вот.
Да, кстати, вот это мощно.
Жестко.
Не, ну просто так на это смотрение очевидно.
Ну, то есть, не очевидно, почему это в целом выпукло, а если вот так по кусочкам.
Ну.
Ну, потому что, по-моему, я, наверное, правильно распишу и буду собираться.
Вот.
Значит, что еще?
А еще, ну вот.
Ну, еще можно заметить, что, так, ну, не для задачи.
Ну, на самом деле, можно еще заметить, что выпуклость иногда имеет место не только
на функциях от одной переменной, но и функциях от логик переменной.
Ну, так бывает задача где вот функция от двух переменных.
Ну, определение, в общем-то, никак не поменится.
Думаю, свойство по инфрериуму есть, более-менее, в общем.
Ну, вытекает вопрос.
То есть, можно оно.
Ну, тогда вытекает вопрос.
То есть, не буду проводить уже прям, там, супер, там, там всех супер определений.
Ну, просто скажем, что, в принципе, от функции нескольких переменных тоже подобные
вещи верны.
И часто функция от двух, выпуклая функция от двух переменных,
можно себе даже как-то вообразить вот таким вот красивым образом.
Потому что функция вот как-то выглядит вот примерно так.
Ну, например, функция, допустим, вот f от x равно расстояние от точки, там, f от p,
от точки p до точки a.
То есть, это как выглядит это расстояние?
Ну, это расстояние выглядит как, смотрите, то есть, такая галочка,
которую насадили на шампур и прокрутили.
Конус?
Да.
Это называется, в категории это называется конус.
Нет, это называется галочка, которую посадили.
Ну, я называю это миксер.
У меня как-то миксер просто в голове автоматически возникает.
То есть, при этом галочку насаживаем на миксер, включаем, получаем конус.
Вот.
Или, например, получится вот такая красивая прокрученная параболка,
то есть, вы скажете, что f от p равно расстоянием квадрата.
Ну, это у нас там не было, это орех.
Орех?
Да.
Какой бесконечный вверх орех.
Не, ну если сверху закрыть.
Шляпочкой.
Ну, такая на пальчике еще появляется.
Ладно, много ассоциаций.
Вот.
Еще 25.
Да, много еще будет есть.
Так, теперь есть какой-то вопрос.
Хорошо, если у нас есть функция выполниться от двух перевилок, как это минимум найти?
Ну, оказывается, что работает метод, который называется вложенная тернария.
Ну, во-первых, заметим, что функция от двух переменных рук вниз, она...
Если запиксируем каждый, какой, например, игр,
тогда у нас получится функция, как функция от одной переменной, от x,
и эта функция тоже будет от рук вниз.
То есть, при каждом фиксированном игре можно минимум найти.
Ну, дальше в качестве упражнения стоит доказать, что это минимум,
то есть у нас функция минимум от игрока,
эта функция тоже...
Ну, суть та же.
То есть, если мы найдем две точки каражи, из которых является минимум при своем игре,
то мы их можем соединить отрезочками и обнаружить,
что минимумы должны быть не выше этого отрезочка.
Ну, потому что там есть хотя бы какие-то точки, при каждом игре, не выше этого отрезка.
Значит, минимум будет не выше этого.
Так, я сейчас не слишком высоко говорю?
Нет?
Нет?
Ну, сейчас вроде все понятно, тут, наверное, не к тебе особо говорить.
Вот.
Ну, какие-то задачи действительно могут решаться.
Ну, на самом деле, есть вот одна неожиданная задача,
которую все время дают как упражнение до пересечения полуплоскостей,
хотя решается она вложена в Тернарик.
Задача такая.
Дан выполнен полуплоскостей.
Дан выполнен в угольнях.
Найдите, пожалуйста, оружие с максимального радиуса,
которое можно поместить внутрь этого уголка.
А, сейчас.
Ну да, очевидно.
Ну, потому что теперь будет парад без выпуска и раз.
Вот.
И проверка на высоту.
Да.
Так, ой, кстати, надо бы нам на перерыв еще пересечения полуплоскости.
Да, усудите.
Ничего.
Нормально.
Итак, вот возникает такой вопрос.
Вот как решать такую задачу.
Ну, все, ладно.
Кто-то уже хуже пихал, что-то оно не пихается.
Да, это реально невозможно.
Да, но это...
Я, правда, я Тернарник не делал с золотым сечением,
но я делал с вот этим вот, который от елок палок.
Ну и да.
Короче, очень-очень жаль.
Да, это странно.
Потому что в том решении, который у меня воображается,
то в том, что там константа вообще не угодит.
Нет, ну понятно, что когда я уже...
Когда моя финальная версия, там уже очевидно,
что просто Convex.Half.Prix делаешь и MLGn вырезает.
Convex.Half.Prix.
Всего?
Да.
Видит Convex.Half.Prix.
Ну там, короче...
Ну там просто у меня в коде ищется максимум по прямым в итоге.
Да.
Максимум по прямым в итоге.
Максимум.
В коде ищется...
Максимум.
Так, ну видим, у нас совсем разные решения.
Ну ладно, давайте ваш послушаем.
Значит, давайте послушаем майор.
Значит, идея, на самом деле, такая.
Переформулируем задачу следующим образом.
Что такое найти максимальный радиус?
Надо найти точку, расстояние от которой до границы многоугольника,
как можно больше.
Логично, да?
Ну то есть надо что делать?
А как найти расстояние до границы многоугольника?
Надо просто перебрать все отрезки,
найти расстояние до этих отрезков и выбрать из них минимальное.
Но на самом деле, можно не до отрезков, это привык.
Вот.
Да, но с другой стороны, заметим, что можно с отрезками не заморачиваться,
а можно найти и расстояние до прямых.
Надо только, чтобы она лежала.
И сильно не переживать насчет того, что у некоторых точек окажется это расстояние пойдет куда-то не туда.
Потому что если это расстояние пойдет вне отрезка,
то это означает, что просто до соседей прямой, видимо, расстояние будет широко ближе.
Единственное, чтобы точка была внутри.
Да.
Ну, конечно.
Просто почему нам это выгодно?
Выгодно нам это вот почему.
Нам это выгодно, потому что расстояние до прямой
внутри многоугольника, это функция, выпуклая куда?
Вот.
И туда, и туда.
Как выглядит график функции расстояния от точки до прямой?
На самом деле, это будет такая книжечка.
Вот прямая и график будет вот так.
Логично, да?
Мы рассматриваем, мы живем только в одной полуплоскости, поэтому функция такая.
Выпукло и вниз, и вверх.
Ну, мы знаем, что раз функция выпуклая вверх, то минимум выпуклой функции тоже выпуклый вверх,
поэтому нас не устраивает.
Понятно, да?
Поэтому получается, тронарный поиск возможен, правда, с мелкой оговоркой.
Этот тронарный поиск ищется, мы должны точно не переворять, строго внутреннего угольника.
Потому что сама по себе эта функция, оказывается, не является выпуклой вверх на всей плоскости.
И вот теперь мы искали вопрос.
А как же нам делать тронарный поиск так, чтобы он делался именно внутреннего угольника?
Пересечение прямой.
Ага, да.
Кто-то предлагает в этом месте предельно честно находить это пересечение и искать минимум только на этом отрезке.
Это и быстрее работает.
Ну, не всегда, если нам дадут какое-то очень замороченное вот такое вот,
то настолько же работает, а глобально быстрее.
Я предупреждал.
Так вот.
Но на самом деле есть более читерский метод.
Есть вот, смотрите, внимание.
Читерский метод, как этого избежать?
А давайте все прямые ориентируем
и будем искать ориентированное расстояние до прямой.
Что такое ориентированное?
Это означает, что если мы ближе в левый полуполос, то расстояние будет со знаком плюс,
а справа от него со знаком плюс.
Тогда расстояние до этой прямой будет просто такая вот наклоненная плоскость, правда?
Ну, это чутко.
Это надо запретить такое.
Почему?
Ну, почему долго?
Ну, что делать?
Это олимпионы побеждают толком, кто придумает самое простое решение.
Ну, собственно, я бы и подставлю, чтобы это было решением образка.
Ну, вот.
То есть такая функциональная плоскость уже на всей плоскости,
а если точка лежит вне многоугольника, то минимум расстояния окажется просто отрицательным.
И тогда это выводит в том, что тренированный поиск вычислить уже не внутри многоугольника,
а на всей плоскости, хотя в баунденбоксе.
Это банально дольше работает, потому что у нас границы бенпоиска уже, когда мы внутри многоугольника лечим.
Ну, мне кажется, по константу.
Границы от баунденбокса облечаться могут не сильно,
но компенсируется тем, что у тебя нет этих безобразных бенпоисков с вычислениями, которые...
Зачем бенпоиск?
Ну, или там за линию проходишь и находишь.
Ну, вот тебе и шесткач Константин.
Так нет, у меня это N log N.
У нас же решение за N log квадрат.
Ну, да.
А найти вот эти границы это N log N.
Потому что я это только во внешнем делаю.
И поэтому он вообще не влияет.
Ну, нет.
Ну, нет.
Ну, нет.
Ну, нет.
Ну, не знаю.
Ну, скажем так.
Ладно, формуаль может и не влияет.
Ну, еще раз, мы просто пробегаемся во внешнем тернарнике.
Мы пробегаемся один раз по границе и смотрим просто их координату пересечения.
Вот.
И так делаем N log N раз, а внутренний у нас N log квадрат.
Ну, вот.
Ну, не знаю.
Ну, сформулируем так.
И в моем решении тебе придется исключительно пробежаться по...
Значит, по другому и найти расстояние.
Да.
Но у тебя еще какие-то лишние N действия тратятся на то, чтобы этот отрезок найти.
У меня они не тратятся.
Ну, в плане это не влияет, потому что у вас N log N есть и найти расстояние внутри одного тернарника.
А пробежаться на те границы просто N.
Ну.
Поэтому это какой-то мидер.
Ну, тратится этот мидер, правда, не N, а N log N или N log N?
Нет, суммарно N log, но все решения суммарно N log квадрат.
Ну, да.
Ну, еще раз, это типа, если TL не подогнан так, чтобы заходить в 0,9, то это не влияет.
А если подогнан, то вопрос их задачи.
Ну, возможно.
Правда, у моего как-то преимущество в том, что мы более скучно пишем, то проще.
Ну, давайте дальше двигаться.
Слово.
Без смысла.
А сколько нам летит?
1,45.
1,45.
Так, ну, окей, значит, тогда, так, ну, отлично.
Значит, еще что-то пообсуждать можно.
Так, ну, говоря, конечно, об этой задачи, можно уже, конечно, плавненько перейти и перейти через полуплоскость.
Ну, правда, это такая подрядочная тема.
Обсуждать теоретически, где можно мало что, а кодик почему-то это не разрешается.
У нас есть такая задача?
Нет.
Ну, можете эту задать.
Нет, возможно, вторая задача, окружность такая.
Нет, потому что, ну, да, смотрите, потому что как бы есть, у этой задачи есть модификация.
Пихните, пожалуйста, в многоугольник две окружности одинакового радиуса, которые еще и не пересекаются.
Да, это тоже и домашний.
И вот тут вы уже, я боюсь, без, там, без пересечения плавненько все уже рискуете не обойтись.
Так, ну, вообще, о чем задача вообще пересечения полуплоскости?
Ну, в принципе, как следует из названия.
То есть как бы данные полуплоскости, полуплоскости мы будем создавать ориентированными прямыми.
То есть дана прямая, у нее есть направление, то, что слева от нее, мы будем считать ее полуплоскостью.
Понятно, да?
Значит, к чему это нас приводит?
К чему это нас приводит?
Приводит нас к этому, к чему это нас приводит.
Ну, задача, на самом деле, такая.
В том или ином виде найти пересечения полуплоскости.
Ну, там разные задачи бывают, бывает просто задача.
Данной полуплоскости, скажите, пожалуйста, не пузырит, пересечения не пустят.
Просто да или нет.
Потому что, на самом деле, если у вас есть...
Давайте вот подумать.
Если у вас давно несколько там полуплоскостей каких-то, то теперь давайте думать.
Чем может быть вообще это пересечение?
Обобщенным магниевым.
Да.
Как-нибудь при НОГЛАЭТ перепишут статья, да.
Нет, реально, от реально к чему это может быть?
Ну, может быть, вот так вот.
А может, не выйдет, да?
Так, ну да, все-таки, ладно, начнем с приятного.
Полуплоскость выкупает нож, значит, пересечение полуплоскостей по-любому выкупает нож.
По-любому.
Так, теперь вопрос, как вы...
Ну, и действительно.
Ну, конечно, напрашивается, конечно, выкупил бы уголь, и тем более, что любой выкупил бы уголь,
то это пересечение, вот, понятно, как задано полуплоскостей.
Да.
Так, а что еще может быть?
Ну, пустое множество.
Да, может быть, пустое множество.
Может быть, большое множество.
Большое, это как может быть?
Ну, что угол, конечно.
Ну, может быть, угол или вот, к.
Ну, если еще мегугольники с границы, там бывают точки и отрезки.
Ну, чего?
Точки и отрезки бывают.
Да, бывает отрезок, ну, такие подлые вещи, да.
Такие вот еще будут.
Нет, там будут разные подрядки.
Просто есть еще очень плотный случай.
Какой?
Какой?
Г워 ци dairy cArgyr Portمكن
И пу...
Ладно, может...
Тамël Maysapeetrokoy ab Wonderful
Наверное, мы еще что-то забыли, но в целом вы Speaker.
да нет, уже ничего не забыли, ну даже если забыли, ну на самом деле, чем это у нас, если совместить, да справиться с этим мне помогает съемка, съемка с рекламой, ой, свечерки мороженые, да, да, да, да, да, да, да, да, да, да.
нет, ваше университет.
да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да, да.
для того, чтобы описать, по идее, все случаи, которые есть, нам достаточно гарантировать, что, как бы, смотрите, то есть бывают параллельные прямые, а бывают прямые, у которых не параллельно есть источники пересечения, да?
так вот, давайте предположим, что все точки пересечения в этот ваулинг бокс попадают.
проблема.
что?
проблема.
ну, пока давайте для простоты предположим, да, может быть проблема, что эти точки пересечения уходят, потому что мы помним, потому что, как мы уже вычисляли, конечно, с вами, что, как бы, если у вас там, ограничение на координаты прямых, там, допустим, 1c, то ограничение на точку пересечения может быть порядка c в клубе.
ну, обсуждали, почему это может быть.
ну, вот, ну, по крайней мере, так лучше.
вроде бы, так и не выяснили.
да.
ну, оцепьте лучше, чем c в клубе, у нас не получится.
да.
и что-то, почему у меня есть основания верить, что может и не получиться в природе.
вот.
впрочем, конечно, не бывает что-то там.
вот.
но если нам повезло Bounding Box, как бы, задержит все точки пересечения, то, на самом деле, заметим следующее, что, на самом деле, мы можем сказать, что внутри Bounding Box, то есть, скажем, что Bounding Box, это вот такие четыре дополнительные полуплоски.
и тогда заметим, что любое, что теперь у нас любое пересечение, это какое-то просто выкупло многоугольник, или вот какая-то его вырожденная реинкарнация, типа отрезка или точка.
ну, или плыть, если не пусто так, конечно.
вот.
но приятность, смотрите, какая.
так как у нас, очевидно, там вот в этом пересечении все вот эти вершины, это точки пересечения каких-то прямых, то тогда получается что?
тогда получается, что если это многоугольник был честный, то он и тут останется честным, ничего не поменяется, правда?
а если он был там, выводил куда-то вал, то тогда просто оказывается многоугольник, просто некоторыми сторонами которого является часть Bounding Box, и, в принципе, по этим частям можно ориентироваться, ограниченное у вас реальное пересечение или нет.
то есть это может вам технически помочь.
то есть даже вот эта полоса, в принципе, этим прекрасно кушается.
то есть полоса превратится просто вот в подобного рода полилагран, ну, где там, я не знаю, в четырех.
так что в данном случае, оказывается, метод такого Bounding Box на самом деле очень хорошо работает.
вот, понятно, да?
честно мы знаем.
в чем мы знаем?
ну, что он едет.
нет, слышно, Bounding Box есть всегда.
ну да.
потому что у вас всегда есть ограничение на точке пересечения, если вы знаете, у вас N точек, вы можете понять, найти ограничение на 12 точке пересечения, если они есть.
поэтому Bounding Box какой-то есть, то есть да.
но, что же мы будем делать дальше?
но, значит, найти пересечение.
как же его найти?
ну, тут это тоже один из тех случаев, когда желательно, то есть надо внимательно смотреть, а какие у вас есть возможности, какие ограничения.
в зависимости от этого можно написать сильно разные сложности решений.
ну, как уровень Easy.
полуплоскостей 100.
давайте представим себе, что у вас дано 100 полуплоскостей, и вы хотите найти пересечение.
тогда есть неожиданное решение.
давайте тупо найдем все точки пересечения.
все точки пересечения за квадрат.
и для каждой из них за линию проверим, лежит ли она в пересечении.
как проверить, лежит ли точка пересечения полуплоскостей?
надо просто проверить, вернуть, что она лежит во всех полуплоскостях.
ну и как это?
все?
просто какая?
что?
все или нет?
пока я говорю так.
я просто нахожусь, нашел каждый квадрат пересечения, для каждой из них за линию проверил, кто лежит в пересечении полуплоскости, а кто не лежит.
так вот.
движение такое, что если хотя бы несколько точек найдется, то есть подозрение, что они и будут образовывать полуплоскость.
останется только их отсектировать по полярному углу, относить на одну из них, и в общем вот оно пересечение.
ну не будет точек, который не внутри.
два?
почему внутри?
ну давай, возьмем букву оболочки этих точек, докажем, что внутри ни одной точки нет.
потому что действительно, предположим, что случайно нашла.
ей соответствовала какая-то полуплоскость.
но тогда есть подозрение, что какие-то части этих решений просто не попадет в эту полуплоскость.
и тогда непонятно, и тогда противоречиесь тем, что мы знаем, что эти решения в этой полуплоскости лежать обязаны.
получается за 10 фусков.
ну в общем-то, да.
это видимо знаменитый математик Вира по фамилии Кубецкий нам всегда помогает.
то есть он конечно немного тормоз, но иногда он помогает.
он уж тормозно работает, но быстро отбудется.
так что, знаете, иногда как писали, то есть он...
то есть иногда очень хочется там и писать ПРФ, чтобы найти расстояние от першин до всех остальных,
но на самом деле иногда окончание позволяет просто написать флой и прийти париться.
да, это может быть формально хуже по 8.30, но как-то непринципиально.
ну просто плой быстренько.
еще подсловно стабилизируется.
ну кто-то и прикол, что Флойд пишет ПОРКА, ФОРГИ, ФОРЖИ, РАДОСТЬ.
что, нет?
да.
кто-то и прикол, что был у Флойд это сложный алгоритм, он там каким-то типа дерево доминаторов.
ну конечно, да.
я его в жизни писала как-то два, наверное.
ну да, ну как бы, ну в дубляшной, но все равно это пять строк.
дерево доминатор.
а, ладно, плей, да.
пять дерево доминаторов.
ладно.
давайте, уровень повыше.
как решить задачу за квадрат?
ну в принципе, картинку я уже нарисовал.
мы честно храним пересечения и добавляем прямые пазмы.
вот.
ну честно, и честно вытитываем все вершины, которые туда не попали.
прямо предельно.
если у вас можно писать за квадрат, то это пишет прямо предельно.
честно, можно прям пробежаться по другу угольнику.
если точка лежит в полуплоскости, добавляем.
если не лежит, не добавляем.
если одна лежит, другая нет, то добавляем ее в точку пересечения.
что еще раз мы делаем?
ну, мы интеративно делаем.
добавляем полуплоскости по одной и поддерживаем текущие пересечения полуплоскостей.
оно у нас всегда выплопшенное.
если у нас есть bounding box, то как бы у нас изначально есть такой прямоугольник.
а так на каждом шаге надо найти пересечение полуплоскости и выплопку угольника,
в чем разрешается это делать за линию.
понятно, да?
это мы делаем, внимание, это делается здесь предельно чисто.
мы прям идем по многоугольнику и говорим, что, во-первых,
если вершина лежит в полуплоскости, мы ее добавляем в ответ.
если не лежит, то не добавляем.
причем, если при переходе по нему состояние сменилось,
то мы добавляем точку пересечения прямой и отрезка.
ну, придется там решать еще ситуацию, когда вершина попала на саму эту прямую,
но это уже мелочи.
вот, понятно, да?
так, ну теперь я расскажу этот вопрос.
а если m все-таки в 100 тысяч?
ну, решение на качестве само собой.
делаем то же самое, но в текарном деле.
ну, мы сегодня уже подобные технологии обсуждали.
то есть такое динамическое пересечение полуплоскостей.
то есть, на самом деле, храним в текарном деле.
в текарном деле находим, скажем, касательную параллельную этой.
и, если надо, удаляем эту точку несколько соседних.
и добавляем точки пересечения.
понятная идея, да?
это если...
то есть, в принципе, есть вот такая.
то есть, заметим, что аналогичным образом, на самом деле,
можно поддерживать и что-то типа динамической выпуклой оболочки.
ну, знаете, вот теперь есть какая-нибудь такая классическая задача,
которую вы любите отдавать.
такую, типа, вне УФО.
то есть, задача такая.
дамо множество точек.
и надо выполнять два запроса.
запрос первый.
добавь точку.
и второй.
а найди, пожалуйста, длину пересечения выпуклой оболочки этого множества и прямой.
да.
это же классическая такая задача,
которую вы тоже будете сталкиваться.
нет.
что вы делаете?
ну, не писали, скажем так.
всё?
всё?
всё за линию.
всё за линию?
ну, кто-то ещё, но надо было удалять точку.
а, ну да, поэтому там была пернивка.
ладно, у нас задача полегче.
хорошие соревнования круглые.
ты участвуй.
там сущную дачу вы уже были.
всё?
нативная реклама.
ну да.
ну да, вот так.
вот оказывается, да.
пока за ОАТН.
всё?
за ОАТН.
за ОАТН.
нет, там не за ОАТН.
я там бы сказал, не за ОАТН.
там удалять надо.
не, а вот это.
вот это?
нет, ну тут пока честно за ОАТН.
нет, мы стали обсуждать за НЛОГН.
да.
за ОАТН.
ну за ОАТН.
за ОАТН решение не знаю.
то, что я знаю, потребует отсортировать эти приближения к полярному углу.
а потом уже за ОАТН.
а существует НЛОГН.
там можно ли это отсортировать?
да, боюсь.
никто не увидит.
полевероятно, что нет.
в общем, нет.
в общем, не сортировать.
вот.
значит...
ну вот.
давайте еще раз об этом положено речь.
как у нас писать динамическую выводку оболочек такой?
ну на самом деле, просто да, идея будет та же.
то есть храним...
ну можно так сказать, храним нижнюю руку оболочку
и храним верхнюю руку оболочку.
то есть точки добавляем буквально так же, вот как мы и описывали.
то есть храним в сетах, там добавляем буквально так же.
да.
вот.
но когда приходит, например, там надо прямая,
то значит надо у этой прямой найти пересечение с этой штукой,
то есть у этой штукой это тоже делается каким-то бинпоиском.
правда, на этот раз надо не касательно искать,
а именно бинпоиском искать, то есть по пересечению.
то есть, видимо, самое оптимальное, что можно сделать,
это реально написать дикарту во дерево.
чтобы делать аккуратно.
не хочется.
да.
а тем более, что есть одна маленькая подлянка.
подлянка называется в том, что нижняя рука оболочка
может быть пересечена прямой два раза.
верхняя, при этом, не пересечена вообще, но когда-либо.
ну да.
так что придется решать задачу, видимо, методом,
типа примеди касательно, а потом тут бинпоиском находи.
то есть восприеди катячку там в каждом ритме,
сделай спуск, вот это вся реакция.
но за алгоритм.
да.
как говорится, болезненный, но вот так.
вот.
но это, конечно, жестко.
то есть, конечно, если у вас задача просто дамы
n полуплоскостей, и надо найти их пересечение,
и никто от вас не требует, чтобы это все происходило еще,
что прямые появляются у вас там где-то в жестком онлайне,
вы в это время должны там еще на голове стоять и так далее.
то, конечно же, никаких дикартячек для этого не надо.
потому что основная идея, конечно, заключает в том,
что можно полуплоскости отсортировать по полярному углу,
а потом забаваться в что-то типа греза.
да.
ну, на уровне идеи это понятно.
а вот на уровне реализации что-то становится больновато.
вот.
ну, вот, честно скажу, тут я, конечно, немножко не мастер,
но, по крайней мере, поэтому тут я предлагаю решение, которое,
по крайней мере, железобетонное.
потому что там есть какие-то решения, вида.
давайте пройдемся по полуплоскостям, сделаем как этого,
а потом получится у нас какая-то там гадость, там какая-то вот такая петля
или что-то там еще какая-то гадость получится.
но давайте пройдемся второй раз, и тогда там все зацепится.
вот.
но, к сожалению, для меня это действительно...
то есть для меня в этом месте кто-то просто очень, там,
просто пять минут игрока кричит кукурику.
ну, отлично для меня.
но возможно это кукурику просто, на самом деле,
это для меня кукурику, для умного людей, нормальная математика.
поэтому, лично, я в этом месте...
или как с линкатом.
че?
или как с линкатом.
хорошо, с линкатом.
и все доказательства заняты.
ну да, ну, как вы сами убедились, там все нормально.
оно на самом деле...
то есть нормально существует, поэтому факты верные, но все приводящиеся...
Нормально существует, поэтому факт верный, но все приводящиеся и нудожевые.
Нет, ну просто да, и кто-то совсем глубокий дешевол, да.
Вот, но это...
Значит, смотрите.
Ну, на самом деле, я вам предлагаю так.
То есть для того, чтобы вы избежали какие-то проблемы, у меня идея такая.
Я разделю полуплоскости на те, которые смотрят вверх, и на те, кто смотрит вниз.
Ну, потому что полуплоскости неограниченные сверху, поэтому полуплоскости нечистые.
Ну, и есть, конечно, вот эти болдинбоксовые темы, вот эти лепитекальные.
Ну, давайте так, будем считать, что их нет, потому что они нам просто по парку сбинут болдинбокс.
Итак, так вот, идея такая.
Давайте я найду пересечения вот таких отдельно и найду пересечения вот таких.
Как я найду пересечения смотрящих вверх?
Ну, тут пока все просто.
Я ацертирую их по полярному углу, а дальше буду поддерживать пересечения вот в таком виде
и на каждом шане добавлять новую полуплоскость, просто выкидывая, можно сказать, из стека предыдущие вершины.
Ну, тут явно, видно просто, ничего сложного нет.
То есть все, что вам останется, это только сказать, что вот у вас теперь получилась вот такая чашка и вот такая крышка.
Крышка.
Ну да, кафе-кап.
Ну, кстати, в некоторых науках действительно такие вещи так и не бывают.
Остается только приближаться к проспуску онлайном слева-направо и посмотреть, есть ли пересечения.
Ну, там просто, смотрите как.
До некоторого момента, на самом деле, крышка будет ниже чашки, потом они пересекутся, чашка будет выше.
И наоборот, а потом после этого наоборот.
То есть потом они опять пересекутся и чашка пойдет вниз, а крышка вверх.
Получается залили.
То есть констат, конечно, получается в направлении, оказывается, очень жирноватая, конечно.
Но, по крайней мере, такой метод, хотя бы железопитонная.
Не, а тут что-ли?
Это тут планировка.
Чего?
Ну, в смысле, тут сортировка.
Ну, я не знаю.
Сортировка это вроде.
Ну, честно скажу.
Ну, я не знаю.
Ну, я не знаю.
Ну, я не знаю.
Ну, я не знаю.
Ну, я не знаю.
Ну, я не знаю.
Сортировка это вроде.
Ну, честно скажу.
Я не знаю, кто такой напишет, получит эллиптом, выяснится, что кто-то автор умеет писать вещи и будет все 6 градусов быстрее.
Как это может быть дольше, чем декартачка?
Да, вот мне тоже интересно.
Нет, я не...
Вы уж выгодите, нет, конечно.
Я привел, что, например, а, такая идея есть, б, она помогает решать задачи, когда у вас полуплоскости добавляются в онлайне, а не зарядом.
Тут нам для сортировки надо, чтобы нам их просто всех задали.
Если не задали, то можно упасить писать декартачку.
Зачем?
Сортировка в любом виде в вашем случае.
Вот.
Ну, вот.
Ну, правда, тут, конечно, выручает то, что крайне редко, видимо, дают задачи на причине полуплоскостей, потому что крайне редко, видимо, просто автор задачи захочет это писать, в принципе.
Нет, хотя бы просто нет.
Тут есть, правда, есть другой вопрос.
Потому что, на самом деле, иногда у некоторых авторов контестов возникает вопрос.
Так, ко мне придут очень...
То есть, так, я провожу контест, ко мне приедут очень серьезные люди.
Ну, там, знаете, какая-то там была чемпион и вице-чемпион там какого-нибудь там сильного полуфинала, да?
Где она уловить, да.
Ну, вот.
Нет, то есть, уловить она.
И надо, нет, для этого надо для них сделать какую-нибудь сложную задачу.
Как ее сделать?
На самом деле, один из стандартных способов просто написать какой-нибудь иом.
Потому что иома даже не самая сложная идея, она на самом деле часто оказывается прям волк,
просто потому что иома.
Потому что ее надо писать.
То есть, ее надо писать, ее надо нигде не запутаться,
но и где-то ее надо убедить.
Можно сделать так, то есть это вот...
Можно сделать так.
Нет, можно в этом месте дать задачу, давайте там сортировки на подокресне, конечно.
Но, как показывает практика, называется это там...
Там вице-чемпионов полуфинала это не основано, бывает.
Вот, да и чемпионов тоже.
Но чемпионок, правда, задерживается на час больше, но в итоге все равно столтаном закрывается.
Но это уже другая история. Вот, а так, в принципе, так что вещь такая.
Так что это вот, что я хотел сказать о пересечении плодов.
То есть парадоксальная тема, да, идея на, в общем-то, просто, в общем, кажется, вроде ничего сложного,
но почему-то, как дело доходит до кода, почему-то это будет и будет вообще никто и никогда.
Удивительно. Хотя при этом там другие задачи помогают.
Вот, например, ну, какой-то пример можно посмотреть, какие задачи.
Ну, например, вот нашу задачу с одной окружностью, да, как она на самом деле могла решаться.
Она могла решаться следующим образом. То есть решаем бинпоиском по радиусу.
Вот задача. Можем ли впихнуть в ногоугольник окружность радиуса r?
Идея очень проста. Берем вот эти все полуплоскости, сдвигаем их на r.
Вот так. Получается вот такие полуплоскости.
То есть вот такие, такая, такая, такая.
Впрашивается, пусто ли их пересечение, да или нет.
Ну, вот, все. Вдоль столько найти.
В общем, более того, их полуплоскости даже сортировать не надо,
потому что они уже отсортированы просто вот этим обходом.
Получается решение, конечно, явно более быстрее, чем тернария.
Тернария, конечно, это лог квадрат, а тут получается n лог.
Ну, там такой, что-то подобное. Но логоиф у этого безбрось.
Поэтому такое решение тут, конечно, быстрее, но при мелких ограничениях, конечно, лог квадрат неожиданно можно взять.
Правда, правда, страдать, конечно, приходится.
Как мы пересекаем крышку и крылья?
Конлайн? Прямо сейчас слева направо, прямо с конлайнником идем.
Едва ли мне там текущие, для каждого икса текущие игры поддерживают.
Ну, может, не настолько нагло, но вот так.
Вот. То есть, в принципе, какие еще там задачи есть?
А, ну или там какие?
Да, то есть, есть еще там всякие классические задачи.
А, в духе, когда там выпуклый многоугольник, внутри него лежит точка.
Скажите, пожалуйста, мы хотим убить некоторое количество вершин и заменить многоугольник на выпуклую оболочку оставшейся.
Спрашивается, какое минимальное количество вершин надо убить, чтобы заданная точка оказалась, где выпуклая оболочка?
Вот, не сад, конечно, такой же.
А, ну да, то есть, понятно, то есть, ушло.
Кто-то ВЛЦС, кто человек ВЛЦШ рассказывает, а это нет.
Кто-то ВЛЦС тоже не рассказывает.
Да?
Нет, всем, кого рассказывать, могли не прийти просто.
Может, им это тоже расскажешь.
На первом курсе я рассказывал, кто-то ВЛЦС.
Там обнаружил, что там где-то есть человек, который сам рассказывал.
Ну, пришла-то ВЛЦС.
На первом курсе выборка была в экспоненту разбойчивой.
Чего?
А, ну да.
Непрощенность.
Чего лучше по нему?
Непрощенность.
А, типа в плане, потому что они их еще больше пользуются.
Нет, ну это правда.
Хорошо.
Так.
Но, но, хорошо.
Давайте скажу.
Значит, смотрите, какая экспонентция.
Смотрите такая.
То есть, еще раз.
Желт было глупо.
И в нем лежит, у меня не знаю, король.
Король?
Ну, например.
Вот.
Значит, ну, башни гарантируют защиту.
Но если уничтожить башню, вот там пару башен, да?
То тогда площадь защиты уменьшается до вот такой.
Так вот, внимание, вопрос.
Но нас не интересует площадь.
Нас интересует, какое минимальное количество башен надо убить, чтобы вот эта точка ушла в ау.
Ну, кажется.
А, ну это, это, это.
Это уже гораздо более похоже на правда.
Сейчас, а гарантирую.
Вот.
Гарантируется, что все башни только на границе замка.
Ну да, вершины.
То есть, внутри замка башни нет?
Да, да, да.
А, ну тогда гораздо лучше.
Да.
Правда, такую задачу, конечно, не обязательно решать, конечно, перед счетом углублоскости.
Ну, тогда просто два указателя.
Ну да.
Да.
Два указателя.
А, ну в принципе, да.
Да.
Во-первых, заметим, что нам нужно уничтожить несколько предыдущих квадрат, правда?
Я думаю, у нас башни внутри замка тоже есть.
Нет.
Вот.
И тогда действительно два указателя.
Ну, теперь вот фишка такая.
То есть, теперь у нас, то есть, у каждой точки есть такая функция, да?
Сколько башней надо убить, чтобы эта точка не защитилась.
Да.
Так вот, хочешь?
Теперь, теперь задача.
Посадить короля, чтобы они были защищены.
Да.
Кернард не заработает.
Ну вот, функция не прерывная.
Как-то я не понимаю.
Окей, она имеет значение.
Ну, окей, да.
Она константная, я не знаю.
Ну, на самом деле, идея такая.
То есть, будем участвовать в бинпольском.
Бинпольском.
То есть, допустим, мы хотим убить ка-башень.
Существует ли точка, которая от этого защищена.
Но заменим, что убить ка-башень.
Да, окей.
Ка подряд башень.
Это как-то то же самое, чтобы провести вот эту фото, правда?
Угу.
Ну ладно.
То есть, я не знаю.
Ну ладно.
Ну ладно.
Ну ладно.
Ну ладно.
Ну ладно.
Ну ладно.
Ну ладно.
Ну ладно.
То есть, если убить вот эти ка-башень, получится вот эта полукоскость.
Если вот эта, то вот эта.
Если вот эта, то вот эта.
И так далее.
Ну, в результате опять задача следует, сколько еще полукоскостей.
Угу.
Угу.
О, понятно, да?
Это понятно.
Ой.
Куда?
Кажется, что можно как-нибудь сделать.
То есть, по мне, конечно.
Угу.
Не знаю.
Ладно.
Ну тут разные задачи есть, которые тут помогают.
Ну и конечно так, чтобы перебросить мостик, на самом деле, на видим, уже после перерыва,
можно обсудить, значит, про две окружности.
То есть, смотрите, данного угольника, опять же, то же самое,
но надо впихнуть в него две окружности, одинакового радиуса, чтобы они не пересекались.
Ну, по-любому показаться.
Как это сделать?
Ну, сделать это просто.
Давайте, опять же, бинплоиск сдвигаем наверх.
Находим пересечение полукоскостей.
То есть, делаем как в одной окружности,
попроси ращитного вопроса, проверить, что пересечение пусто.
То теперь надо еще проверить следующее.
Существуют ли в полученном пересечении полукоскостей точки
на расстоянии хотя бы 2, как подруга.
Из диамета.
Да.
Ну, это, получается, к заданию найдите диаметр уголок.
Ну, да, мы пока...
Да, поэтому я и говорю на мостик.
То есть, это будет одна из тех задач, которые мы будем, видимо, обсуждать после перерыва.
Как говорится, да, мы совершенно неожиданно сейчас,
как говорится, может быть, на одном из самых интересных мест, в скобках нет,
остановим геометрию.
Хотя, ну, может быть, если повезет, мы сегодня там в конце еще успеем
обсудить такой метод вращающей калиперов.
Но, честно скажу, возможно, я все-таки попрошу его заботить отдельно.
Ну, там есть пара моментов, которые не очень сложные, на самом деле.
Но, тем не менее.
Ну, нет, ну, как-то...
Нет, ну, знаете, как-то это, чтобы это...
Как-то я вам...
Ну, это вот метод, которым ищутся, скажем, там,
самые удаленные точки выпка многоугольника, например.
Или там, например, покрывающий прямоугольник минимальной площади, например.
То есть это все решается методом, который называется метод вращающихся калиперов,
на самом деле.
Так, прямоугольник минимальной площади не умеет?
Так.
Нет, ну, хорошо, ладно, хорошо.
Ну, хорошо, ладно, диаметр можно найти, конечно, и с умами.
Да, с умами. Да, да, да.
Да, диаметр.
Ну, ладно, хорошо, диаметр уговорили, хоть там, конечно, и да.
А вот с минимальной площадью или минимальным периметром...
То есть там уже, я боюсь, калиперы придется поросить.
Нет, ну, там есть еще пара задач, потому что мы еще, к сожалению,
упустили там какой-нибудь алгоритм препарата, например.
Ну, этот поиск.
Ну, поиск двух ближайших точек.
Меньше сортов.
Это вообще алгоритм препарата называется, да.
Не, если бы, если бы он так решался.
Ну, ладно.
Как говорится, да, может, да, может даже повезет, да.
Ну, понятно, да.
Спроецируем на рандомную прямую, да.
Спроецируем точки на рандомную прямую, рассмотрим все соседи на ней.
Повторим, пока в ТЛ влезает.
Выберем лучше.
Проверяется accept.
Или, видимо, каким-то доказательством того, что на самом деле
там вероятность того, что вам не повезет
и между этими двумя точками влезет.
Кто-то крайне мала.
Нет, как раз вероятность того, что кто-то влезет, кажется,
достаточно быть велика.
Ага, ничего.
А вероятность того, что, как бы,
давайте не все влезут, все нормально.
Ну, можно просто лог проверять.
У вас сортировка ZenLog действует, поэтому алгоритм
соседей проверяет бесплатно.
А, ну, понятно.
Ну, тоже.
Ну, окей.
Ладно.
Но вместо этого мы будем, мы окунемся,
попробуем вкрутиться в сладостный чарующий упоительный мир
вероятностных алгоритмов.
Вот он уже и первый.
Но это да. Осталось про правду идти со слету.
Не очень понятно, как это доказывать.
А мы же, как всегда, в свойственной нам манере
будем стараться все или почти все доказывать.
Ну, типа, мы берем у нас...
Нет, это понятно.
Вертикальная прямая, которая через них проходит,
то они соседние будут.
Да-да-да-да-да.
Ну, вот да.
Самые две ближайшие.
Нет, самые далекие по-другому.
Ну, да.
Ну, я...
Ну, сформулируем так.
На самом деле это валится тестом просто,
когда очень-очень-очень близко
к этой прямой находится очень-очень
много точек.
То есть, на самом деле это...
Но тогда у нас, наоборот, вертикальная прямая
не сработает.
Какая вертикальная?
Там получается, что должно быть
всюду плотное распределение в круге, кажется.
Потому что иначе у нас...
Ну, да. А, ну хорошо, ладно, давайте.
Ладно, давайте для развилочки.
Можно для каждой точки определить,
с какой вероятностью эта точка вообще попадет
в полусу этих отрезков?
Ну, да, с какой вероятностью
это произойдет?
Ну, да.
Чем дальше точка от этих,
от этих тем, наверное, дальше.
Я немножко прикидывал, там, кажется,
мы берем круг с радиусом 1,
кидаем в него равномерно точки и ставим
две ближайшие где-то в центре.
Тогда у нас все время будет довольно много
точек. Ну, я не знаю, мне что-то подозревает,
почему-то начинает быть подозревение, что это пропорционально
и два линии вот этому углу.
Вот.
Ну, потому что что-то типа вот этот угол минус
вот этот угол, вот такое направление
должно быть.
Ну, окей.
Поэтому чем меньше этот угол, тем
конее. Ну, вот. Ну, типа того.
Ладно, давайте просто поговорим не про
геометрию, а в общем контексте.
Потому что зачем нам вероятность
вообще нужна?
Да, ну, потому что, конечно, популярные
направления, в общем-то, едва ли сейчас, вообще,
одно из самых ртуальных.
То есть, действительно, заключается в том, что, да, можно
иногда включать случайность, потому что
оказывается, что какие-то объекты детерминированы
найти очень-очень-очень сложно.
Ну, или не очень, но просто сложно.
Но при этом выясняется, что если их искать
быстро,
если кидать какой-то рандом, то выяснится,
что мы попадем на хороший объект
с большой вероятностью или какой-то, или
просто с хорошей вероятностью за что-то
зацепимся.
Вот. Ну,
конечно, там первая, конечно, вероятность.
То есть, мы на самом деле сейчас вот
там почти сразу
на самом деле наткнемся на пример задачи,
которая там детерминированно решается
будет задолго, а вероятность-то за быстро.
Но на чем мы, конечно, не с этого.
На чем мы, конечно, с
уже известных нам алгоритмов.
То есть, потому что первая вероятность алгоритма, который вы встречаете в жизни,
какой?
Правильно, QSort.
Вот.
QSort.
Вот. Ну, как предлагается делать
QSort вероятноста?
Ну, идея очень простая. То есть, просто
каждый раз у нас задача там дан массив,
надо выбрать на нем
какой-то элемент и за линию
разделить его на те, кто это меньше
x, а сам x и больше x, и запуститься
рекурсивно.
Ну, вот идея. А давайте кидать, выбирать
рандомно. А то мы тут какую-то медиану
выбирали. Значит, предлагается
делать это рандомно.
Вот.
Что можно сказать про этот алгоритм?
С какой вероятностью этот алгоритм работает?
Ну, с вероятностью
1,2
он попадает в отрезок
с 1,4 до 3,4.
Так.
Ну, да.
Знаете, типичный диалог.
Когда...
А как пишут слово великодушно?
V достаточно.
Вот примерно так сейчас
на самом деле могло бы быть.
Потому что я не спрашивал, за какое время это работает.
Я спрашивал, с какой вероятностью этот алгоритм работает.
Чувствуете
разницу, да?
То есть, этот алгоритм работает с вероятностью
1.
Вот. Почему я хочу это отметить?
Потому что сразу давайте скажем.
Потому что на самом деле у нас будут два типа алгоритмов.
Давайте сразу терминологию введем.
Есть терминология, то есть, есть алгоритм
типа Лас-Вегас
и есть типа
Монте-Карло.
Так. Давайте подгляжу,
чтобы не перепутать, кто из них кто.
Ну, вот.
Значит, вот.
Подглядю.
Вот. Значит, смотрите.
Что это, какой смысл?
Значит, смысл такой. То есть, в обоих
используется вероятность.
Ну, значит, ситуация такая.
Значит, Лас-Вегас, алгоритм Лас-Вегас
гарантирует, что найти на ответ будет правильным,
но
вероятность
будет заключаться в том,
что
с некоторой вероятностью
алгоритм будет работать долго.
Ну, то есть, понятно, что с средним алгоритмом
будет работать круто, но с некоторой вероятностью
он зависнет.
То есть, нам может очень не повести.
Это называется алгоритм
типа Лас-Вегас.
А есть, наоборот, алгоритм типа
Монте-Карло. Наоборот, гарантируется,
что он быстро работает.
Но при этом есть вероятность, что найденным
ответ будет неправильным.
Ну, честно говоря, я не знаю.
Честно говоря, скажем так, в данном случае,
как и во многих других, я ссылаюсь на
лекции Миши Тихомирова,
на конспекты, о которых я сейчас опираюсь.
Да, замечательные конспекты.
Вам я их тоже пришлю.
Соответственно.
Ну, в принципе,
про Монте-Карло вы что-то слышали,
потому что есть классический метод
нахождения площади
какой-нибудь Мексской фигуры.
Ну, допустим, вам там нужно...
Ну, представьте, вам там нужно пересечь
многоугольник, эллипс
украденную елку,
и найти площадь пересечения.
Можно там действительно
заморачиваться, что-то там искать,
какую-то там гадость, какие-то там параболы,
гиперболы, там уравнения рисовать.
А можно вместо этого решать метод
на Монте-Карло. Что за метод?
Огораживаем это пересечение баундинг-боксом
и кидаем рандомную точку.
То есть много раз кидаем рандомную точку
для каждой точки и считаем,
с какой вероятностью,
какая доля точек попала
в площадь. И оказывается,
что это может быть неплохой
оценкой на площадь.
Вот тут я, к сожалению, не могу сказать
прям точно,
с какой адекватной точностью
это не готов,
но такие методы применяют.
Скажу в общем.
То есть я сильно
с удивлением узнал, что откуда у меня
взялась задача окружить темногоугольник.
То есть дело в том, что когда-то
когда я был на первом курсе,
еще проводилась такая олимпиада,
как просто олимпиада МФ3.
То есть личная олимпиада МФ3.
Открытая. То есть просто
приходим, пишем ICPC-контест,
что-то кто-то победитель.
Личный кубок.
Нет, кубок это немножко не то.
Кубок это серия и на баянах,
а там это реально олимпиада
с реальными призами. Я ноут выиграл.
Ну ладно, нет, будчик такой.
Называется.
Ну да, осталось только
найти спонсоров.
В принципе, когда-нибудь
можно и попробовать.
Впрочем, это были
стародавние времена, тогда еще Фистер даже
сборы не проводил.
Ну как сейчас в принципе, да.
Только сейчас они просто
видимо по каким-то полувременным причинам
не проводятся. Возможно в будущем
эта традиция вернется.
Но фишка в другом.
Так вот, я как с удивлением узнал, там
была задача просто. Дан круг и треугольник,
найдите площадь пересечения.
Так вот, на разборе рассказали неожиданное.
То есть действительно там предлагалось
просто едва ли не Монтекарло
писать.
Ну в общем,
где-то Монтекарло, а где-то сказать,
что окружность это на самом деле такой
сто тысяч угольник.
Внезапно, да, я так сильно удивился.
Причем более того, там точность
должна была быть абсолютно относительно
10 в минус четвертый.
То есть утверждается, что у авторов там
было математическое доказательство того,
что все евристики работают с адекватной точностью.
То есть в результате я сильно
был очень приятно. А потом
команды физтеха поехали в Петрозаводск.
И в принципе половина этих задач
пошла в контест Петрозаводска.
В том числе и это.
Мне приятно, так что вшипнули,
что моё решение взяли
в качестве авторского.
Потому что оно было точное.
Ну, собственно, было оно
буквально вот как я рассказал.
Соответственно.
То есть так забавно было.
Ну вот, в принципе, да, такой метод
на полном серьезе существует.
То есть, конечно, там точность, может, у него там
бывает, может быть, не ого-го,
то есть там надо ещё анализировать.
То есть там уже, знаете, это может быть даже
не здесь надо рассказывать, там какая точность,
а там где-то на матстатах, на самом деле, это можно поанализировать.
Потому что на матстатах это, я думаю,
одна из базовых задач у вас будет там
оценивать,
ну там как-нибудь оценивать
там какую-нибудь там погрешность,
где вот там попадаете вы куда-нибудь в какую-нибудь там.
То есть я уже даже не помню,
куда вы попадаете.
Не буду вас, это да, так что не буду
вас попадать туда. Вот.
Значит так.
И так, значит, поговорим про QSort.
Но QSort у нас это, конечно, алгоритм типа
Las Vegas.
Здесь интересно, за какое
мотожидание мы можем
работать.
Ну, на самом деле, ну, смотрите.
Ну и да.
Ну, теоретически да.
Но давайте хотя бы с мотожиданием.
Давайте. То есть, честно скажу,
мы, конечно, может быть, с этой точки зрения,
видимо, только чуть-чуть копнём, конечно,
только так. Где-то обзорно, где-то
чуть-чуть просто. Да.
Но там много чего можно, потому что
когда приезжает на финал
какой-нибудь там Ричард Пенко рассказывает про
какой-нибудь там поиск потока за
какую-то экспоненту.
Ну, кстати, какую-то экспоненту это типа
лучший алгоритм, но он вероятностный.
То да, понимаешь, это алгоритмы,
которые придумываются буквально прямо сейчас.
Нет, ну,
нет, мы это искали
за VE. Они там
может и...
А может, наука умеет искать их там
за два в степени log-log-V делить
на log-log-log-V. Там это быстрее,
чем за VE.
Ну, что-то в этом роде там, условно.
Ну, там степень какая-то вот такая
была, честно говоря, я не помню.
Вот. То есть, там отдельная
печь, то есть, какие-то вот,
алгоритмы в эту сторону там думают.
Ну, просто вот было интересно, что это прямо сейчас думают.
То есть, как бы, да.
То есть, где в это время,
где-то в университете Ватерло,
чем может заниматься там...
Ну, как минимум, потому что
этот товарищ был из университета Ватерло в Канаде.
Вот. Кстати,
собственно, туда поехал Ильдар Гайнулин.
Вот это, видимо, один из ответов,
почему он поехал именно туда,
а не куда-нибудь еще.
Нет, ну, нет, редкий случай,
но, с другой стороны, вот,
оправданно чем? Потому что, знаете,
он мне периодически, к сожалению,
приходит в редакцию письма, а занимается ли у нас там...
То есть, кто-то вот ищет научного руководителя,
а занимается ли кто-нибудь вот чем-нибудь алгоритмическим?
Так вот, честно, я никого не знаю.
Вот я как всем отвечаю. Я, к сожалению, никого не знаю.
То есть, я не знаю, можно ли найти научника,
который будет заниматься вот чем-то подобным.
Нет, скажи так. Я теоретически...
Нет, смотри, нет, я как бы на эту тему,
там, ну, в Epsilon Crest,
я знаю, что, видимо, какой-нибудь Максим Бобенко
мог бы заниматься чем-то подобным.
Но он, по-моему, сейчас не имеет отношения к вистеху.
Вот, и что еще?
Ну, вот, и...
Вот Ваня Смирнов, ну, собственно,
там, да, тут надо не перепутать, да.
Просто есть какая-то Ваня Смирнов,
который, возможно, там ваш одногруппник, там, плюс-минус один год.
Ваня Смирнов, которому я сдавал
Матлок.
Да, кстати, вот это...
Так, ну, это похоже.
Так, ну, это...
Он такой темненький, с бородой.
Нет, ну, скажем так.
Ну, хотя, не знаю, я бы сказал, что он длинноволосый,
но может, он уже постричься, я не знаю, я его давно не видел.
Нет, я скажу так, что это в какой-то момент тоже...
Скажем так, в какой-то момент
он тоже был моим сакаманником, на самом деле.
Вот, в свое время.
Вот.
Ну, вот он, ну, он рассказывал там, что
просто в Яндексе, по идее, есть там какая-то научная
группа, в которой там действительно занимаются алгоритмами.
Как это ни странно.
Вот, так вот.
То есть можно, по идее, там переспросить у него.
Это все, что я знаю на эту тему, к сожалению.
То есть, конечно, немножко жалко,
потому что, конечно, да.
То есть, если бы кто-то этим занимался, мне, конечно, было бы это
интересно.
Вот, значит, смотрите.
Ну, вернемся к
кусорту. Вот.
Как здесь оценивать время? Ну, давайте начнем хотя бы
с мотор ожидания.
Ну, как оценить мотор ожидания? Ну, давайте будем оценивать
такую вещь, как количество сравнений
элементов.
Ну, потому что заметим, что
время работает от тета от количества
сравнений всегда, правда?
Ну, думаю, это достаточно очевидно.
Вот.
Так давайте оценивать, сколько у нас сравнений.
Ну, а первое, заметим, что каждый раз, когда у нас на отрезке,
мы выбираем вот этот пай, вот этот
х, то все сравнения, по сути, проходят.
То есть, по сути, идут
с ним.
То есть, можно считать, что мы просто тут все элементы
сравнили именно с ним.
А теперь давайте,
тогда, значит, будем анализировать так.
Чтобы проанализировать мотор ожидания
количества сравнений, давайте просто
надо просто посчитать
для каждой пары элементов, с какой
вероятностью они будут сравнены.
То есть, потому что,
ну, давайте так.
Вот, мотор ожидания,
нас интересует мотор ожидания,
то есть, количество таких пар,
значит,
и ижи,
значит, таких,
что и ижи
были сравнены.
Господи, какое-то, почему-то,
слово какое-то, не, как будто, плохое.
То есть, вроде, по русу,
с точки зрения законов русского языка, вроде
адекватное, но что-то не то, да.
Вот, а,
сравнены тогда.
Сравнивались
в процессе, во.
В процессе кусорта.
Ну, давайте так, да, еще,
конечно, давайте в качестве дополнения
быстренько скажем, что мы сортируем перестоловку
от 1 до n.
Вот, ну, понятно, что с точностью
до понятного.
Ну, как бы, одинаковые элементы,
значит, они жизнь только облегчат.
Вот.
Ну, это если модифицировать кусок.
Ну, да.
Давайте для простоты будем, ну, может,
по индексами считать или
соответственно. Так вот.
На самом деле, перезаморачиваться особо
не будем, то есть, понятно, анализ можно чуть усложнить,
но, как бы, не в этом
сейчас суть. Итак,
но заметим, что такое это мотожидание.
На самом деле,
это равно
мотожидание,
ну, что это такое?
Это мотожидание
суммы
по всем ИЖ
индикаторов
того, что
ИЖ
сравнивались.
Да, что такое индикатор?
Ну, в тяже вере это стандартное слово,
это стандартное слово, которое обозначает такую случайную
величину, которая равна 1,
если событие произошло, и ноль иначе.
Правда?
Так, было такое слово на теории меры?
Не было. Хорошо.
Но тогда у вас
на дискрайне, наверное, было
мистическое свойство.
Заключается в том, что сумма
мотожидания суммы равно
сумме мотожиданий.
Так, это было на дискрайне
или на теории меры?
Нет, на теории меры нифига не было.
Мы доказывали
интеграл.
Потому что мотожидание это такой интеграл
Либерра.
Ну, давайте я вам веду.
Мотожидание
любой случайной величины,
это, собственно, интеграл Либерра, по вероятностной
мере.
Ну, вот все.
Вот и все.
Нет.
Я не буду вас
просить доказывать интеграл Либерра.
Это нот.
Итак,
мотожидание И, И, Ж
и буду повторяться.
Но что такое
мотожидание
индикатора?
Как в науке называется
мотожидание индикатора?
Мотожидание индикатора события А.
Да, правильно.
Называется вероятность события А.
Логично, да?
Вот.
Хотя прям звучит как пафосное
словосочетание.
То есть это
вероятность есть мотожидание
индикатора события.
Вот.
И, И, Ж сравнивались.
Вот.
Так что теперь все, что нам нужно,
это посчитать вероятность того,
что элементы И, И, Ж
в перестановке сравнивались
между собой.
А теперь давайте подумаем.
Вот теперь придется подумать.
То есть это я написал какие-то относительно
общие вещи.
Теперь давайте копнем в алгоритм.
С какой вероятностью они сравнивались?
Вот.
Конкретные элементы И, Ж.
То есть 57 и 179.
С какой вероятностью они
сравнивались?
Ну да.
Но как-то
вот.
Но внимательно можно заметить следующее.
Что некоторое время, когда мы запускали
кусорт, И, Ж
отправлялся в одну и ту же половину
относительно своих пайватов.
А потом в какой-то момент
произошло радикальное событие.
Выпал пайват между И, Ж.
И тогда у нас два варианта.
Либо этот пайват совпался
И и Ж, и тогда их сравнивали,
либо этот пайват оказался
строго между ними,
тогда они навсегда разошлись.
Причем более того,
получается это равно
вероятность того, что первый пайват,
который попал на отрезок от И, Ж
оказался в И и Ж.
Ну и вероятность этого.
Так как вероятность выпадения
каждого из них одинакова,
то получается, что надо по всем...
Теперь я уже в явном виде
напишу.
В такой штуке
получается вероятность равна 2
делить на Ж минус И
плюс 1.
Но остается немножко алгебры.
На отрезке между И и Ж
находится Ж минус И плюс один элемент.
Два из них в качестве...
Чтобы проверить вероятность,
мы ждем первого...
Мы ждем первого пайвата,
который попал между И и Ж.
Если он попал в И и Ж,
нас устраивает, если строго между нет.
Получается, нас устраивает два случая из вот этих.
На самом деле,
это уже суммируется очень легко.
Я буду просто по L
от 2 до N.
Два делить на L
умножить на сколько раз
у меня встречается знаменатель L.
А сколько раз у меня встречается знаменатель L?
Номинатель L.
Ну да,
по сути, сколько подотресков длины L
в массиве длины N?
Их N минус L плюс один.
Чего?
Ну,
потому что у нас и и Ж всегда разные.
Вот. Ишить такое.
Давайте так, я сразу
напишу. Это не превосходит
два N.
Сумма по всем от L до 2 до N
один делить на L.
Что равно O от N
лог N.
Ну, я так просто сверху оценю,
хотя более четкий анализ
вам в явном виде покажет,
что это от ожидания θ от N лог N.
То есть к чему это нас привело?
Это нас привело к тому,
что в среднем
это работает за N лог N.
Вот.
Ну, конечно, это не значит, что оно всегда будет
работать за N лог N.
Потому что в принципе есть вероятность,
что оно будет работать
за 10 N лог N,
за 20 N лог N,
за 100 N лог N.
Или просто
за квадрат.
Вот.
Правда, какие-то оценки можем сделать
и сейчас. Смотрите.
Потому что мы тут сказали,
что у нас тут O от N лог N.
Ну, где-то там вот этому от ожидания
Ну, на самом деле,
можно сказать, что этому от ожидания, допустим,
не превосходит 2 N лог N.
Вот, допустим.
Тогда, смотрите,
на самом деле я могу заявить такое,
что
на самом деле 100 N лог N
будет не так часто.
Потому что я утверждаю, что
100 N лог N будет
с вероятностью
не более чем 1,50.
Может быть даже 1,25.
Почему?
Ну, если тут 2 N лог N,
тут 100.
Да, почему я это
утверждаю?
Да.
А,
ты предлагаешь дисперсию считать?
Нет, это про меру.
Что мера и интеграл.
Так, ну погодите. Ладно, давайте.
Да, давайте начнемте с разминочки.
Потому что у нас есть неравенство Маркова.
Я про то, что
мера точек,
в которой вероятность будет шевелена чем a,
она меньше чем интеграл делить на a.
Сейчас, погодите.
Сейчас я сначала одно напишу,
потом выскажете.
То есть самая простая оценка какая-то
на вероятность того, что будет счет слишком много,
это неравенство Маркова.
То есть если у вас какая-то случайная величина больше
либо равна нуля,
то вероятность того, что xi
больше какого-то епсилона,
она на самом деле не превосходит
ну доказывается в общем-то тривиально.
Потому что
как бы домножаем на епсилон
и оцениваем от ожидания
когда xi
заменяем на
если она больше епсилона
заменяем на епсилон, а если меньше епсилона
заменяем на ноль, получаем буквально
епсилон на вероятность. Ну понятно.
То есть в принципе
уже отсюда следует, что вероятность того,
что у нас тут будет стоен луген, невелика.
Вот.
Поэтому если очень страшно, в принципе там есть
всякие читерские методы в духе, что если вы понимаете,
что оно работает стоен луген,
а давайте
отрубим алгоритм и запустим его
заново.
Это изменит то, что вы
видимо дадите себе
большее гарантии, что у вас никогда не будет
работать за квадрат.
Да.
Нет, стоп, это все равно все еще влияет
нам от ожидания. Нет, ну мы один рандом
заменили на другой. Нет, я бы сказал
мы, нет, мы
не заменили, мы один рандом
еще и страхуем другим.
Ну не страхуем, а страхуем
отсечкой. То есть мы говорим
мы говорим так, стоен луген
нас устраивает условно.
Вообще мы сейчас квадрат заменили на бесконечность, если нам все время
не везет. Ну естественно, да.
Ну в худшем случае да.
Ну с одной стороны
да, давайте тогда в худшем случае
да, но с другой стороны в лучшем все-таки
это можно отсекаться.
В другой стране вероятность того, что он работает за х
столько меньше, чем один делитель
50 в степени х, что-то такое.
Это эксперсия.
Так, ну теперь говорите, что вы говорили
про, собственно, примеры.
Если вы в казино там вам не везет,
вы если встанете
в другой стол, вы идете, ну что изменится?
Ну это может иметь смысл, если
например
если у нас, например, с вероятностью 1-2
мы зацикливаемся, а с вероятностью 1-2
вы даем ответ за ОАТ,
тогда нам умеет смысл там ждать
3n операции, потом перезапускаться.
Ну типа того, да.
Мы не знаем, как наш алгоритм конкретно
устроен по структуре.
Вот. Так, а что
вы говорили про меру делить на А?
Да нет, это прикол,
что
в теории меры неравенства
Маркового называется неравенство Чебушова.
А, ну это странно,
потому что в теории меры
неравенство Чебушова называется немножко
по-другому. То есть там неравенство
Чебушова имеет в виду немножко другое.
Там, по-моему, вероятность того,
что разность
между Кси, между под ожиданием
больше Эпсилона,
она меньше либо равна,
чем, по-моему, дисперсия Кси
делить на Эпсилон в квадрате.
Вообще для меня
вот это называется неравенство Чебушова.
Это не важно, это просто прикол.
А, ну прикол, все хорошо.
Нет, просто такое неравенство есть
и оно как бы применимо.
То есть кто-то его считает это просто...
Кто-то его считает вообще
два линии, одним из нулевых версий закона больших
чисел на самом деле.
Вот.
Значит, соответственно.
Нет, ну там просто это
вкупе с линейностью,
аддитивностью дисперсии для независимых случайных величин
там действительно получается хорошо.
Вот.
Значит, хорошо, да.
Значит, это был у нас
Кусорт. Но у Кусорта есть младший брат.
Называется
найдите... Называется
Q Order Statistics.
Ну это найдите Q Order Statistics.
Вот. В чем тоже очень простой алгоритм.
То есть мы делаем абсолютно
то же самое, только рекуссивно, потом
запускаемся не от двух массивов, а от одного,
который нам надо.
Помните, да?
Вот.
Но давайте посчитаем, а какова у него вот ожидание
времени работы.
Ну, в общем
префикс записи в общем-то не меняется.
То есть на самом деле
изменение начинается вот с этого момента.
Да.
То есть до этого момента все совпадало.
А вот дальше придется думать.
Потому что сравнивать ли и ежи
на самом деле очень сильно зависит от того,
где вот это вот ката-элемент находится,
который мы ищем.
Логично, да?
Да нет, наверное.
Что еще у нас?
Ну вот мы ищем
вот эти вероятности.
Но заметим, что вот эта вот вероятность
для ижи она сильно зависит не только
от ижи, но от того, где относительно
их находится ка.
Где ката-элемент это тот, который мы реально
ищем.
Что мы ищем?
Ката и порядка статистиков?
Да.
Вот.
Ищем ее
вероятностным методом.
Тоже выбираем вероятностный
пайвент, пишем
кто меньше, кто больше, и потом отправляемся
только в один подмассив.
Вот мы пытаемся это оценить.
Теперь какая вероятность того, что
и и жи сравнится?
На самом деле все зависит.
Потому что может так случиться, что
и ты и жи ты элемент,
и ката-элемент, то есть ка на самом деле
находится между ними.
Но тогда заметим, что в этом случае
вероятность остается неизменной.
То есть мы все равно все еще ждем
пайвата, который попадет между и и жи.
Да, с какой вероятностью это произойдет?
Да, но заметим, что
как бы рано или поздно этот пайват
действительно найдется.
Потому что, то смотрите, еще просто
чем этот случай отличается от q-сорта?
Потому что, допустим, у меня k окажется меньше,
чем i.
Тогда помимо ситуации, когда у нас
найдется пайват между и и жи, который
разделит и и жи, может еще найти ситуацию,
когда пайват попадет между k и i,
и и и жи не будут срагиваться, потому что они оба
отправятся в аут.
Ну, во-первых, давайте так.
Но если у нас i меньше k,
допустим, меньше либо
равно k, меньше либо равно жи,
скажем, да,
то тогда здесь вероятность та же,
какая была, 2 поделить тожи
минус i плюс 1.
Понятно, да?
Вот.
Но, с другой стороны,
если у меня k, а теперь давайте
думать, если у меня k оказалось
меньше, чем i,
то с какой вероятностью и и жи будут
сравниваться?
Видимо...
Сейчас.
Это, видимо...
Ну, короче, нам нужно, чтобы
пайват попал, чтобы вот первый пайват
на отрезок k и жи попал
в отрезок и жи.
Нет.
Но это необходимо, но недостаточное.
Потому что, если он попадет
строго между и и жи, то они сравниваться
все еще не будут.
А, никогда?
Нет. Если он попадет между и и жи,
то не будет. Если между k и и не будет,
а вот если он попадет в и, то сравниваться будут.
А в и и с вне условий?
Нет, еще он в жи может попасть.
И тоже они сравниваться будут.
Нужно, чтобы
несколько раз у нас либо было
либо левее k, а тогда все нормально.
Но некоторое время так и будет.
Но просто в какой-то момент
пайват попадет между k и жи
включительно.
И тогда либо он попадет в и или жи
и сравниваться будут, либо куда-то еще
и сравниваться не будут.
Поэтому здесь придется... Поэтому пишем... На этот раз
пишем 2 жи минус k
плюс 1.
Чего?
Почему? А что нет?
В смысле нам еще нужно учесть, что
вот это первое время
связывается с левой и с правой.
Ну, смотрите, некоторое время
просто...
И что? Будут с левой и с правой.
То есть были какие-то пайваты. Пока
пайват не попадет на отрезок от k до жи,
все элементы k и жи будут
попадать в одну и ту же часть относительно
пайвата, и от них будет запускаться рекурсия.
Поэтому на то, будет ли сравниваться и жи,
особо не влияет. А потом в какой-то момент
впервые в жизни, рано или поздно,
будет выпадет какой-то пайват на отрезке
жи. Причем этим первым каждый из этих
элементов может быть очевидно равновероятно.
Но только два из них
дают нам
хорошие случаи.
Поэтому 2 жи минус k плюс 1.
Ну и конечно же
остается только написать симметричное
слагаемое.
Получится вот так. 2 делить на k минус
и плюс 1.
Так, а теперь
смотрите.
Начну, давайте пробовать.
Так, ну а первое давайте так.
Первое слагаемое я тупо перепишу.
k меньше либо равно жи
2 делить на жи
минус и плюс 1.
Да, важный момент, что да и меньше жи
при этом все-таки у нас.
Напоминаю. Ну это конечно мы поняли вот.
Вот.
Да, здесь меньше либо равно. А вот здесь я
начну от, ну вот.
А, ну почему, ладно. Здесь
здесь я могу, просто вот здесь я от и
могу избавиться. То есть я просто здесь
напишу k,
для всех k меньше жи. Я здесь
напишу просто
какую-нибудь вероятность в духе
2 делить на жи минус k плюс 1.
Но ее я умножу на сколько и
между жи и к. А сколько их?
Правильно. Жи минус к
минус 1.
Логично, да?
Ну и плюс
сумма по всем и меньше
чем k. Получается 2
на k минус и плюс 1.
На k минус и минус
1.
Ну, заметим, что это что у нас
такое?
То есть на самом деле я утверждаю, что
каждое такое слагаемое можно оценить как
единицу, правда? Ну ладно, как двойку.
Сверху, правда?
Какое? Вот такое.
Вот.
В смысле?
Это делить на это меньше единицы.
Спасибо.
Так.
Это меньше либо равно.
В сумме 2 делить
на жи минус и плюс 1,
где между
и и жи затесалось k.
И плюс
получается, ну суммарно,
получается 2n на самом деле.
Потому что я тут по всем k пробегаюсь,
по сути.
K я фиксирую и прибираюсь по всем элементам
неравным k, и для каждого элемента
получаю 2. Поэтому сумма получается не больше
чем 2n.
Вот.
Вот.
Но теперь заметим вот эту штуку.
А теперь вот.
Теперь значит я пишу.
То есть на самом деле
это равно 2 делить на l умножить
на количество отрезочков
длины l.
А потому что, смотрите, тут мы
приберем элементы, которые меньше, чем k,
а тут которые больше, чем k.
Поэтому не 4.
А теперь, смотрите, получается,
помимо этих 2n надо просуммировать,
то есть надо взять 2 делить на l
и умножить на количество
отрезков длины l,
которые цепляют элемент k.
Спрашивает, сколько таких отрезков?
Ну, оказывается, их не более, чем l.
Как легко убедиться, потому что этот k-то элемент
может в этом отрезке длины l быть 1, 2, 3 и так далее.
Такое красивое замечание.
Вот.
Ну и, короче говоря, очевидно,
что это равно o от m,
потому что это не превосходит 4n.
Это k-то.
Нет, ну это просто мы от ожидания посчитали, да.
Вот.
Все-таки сумма не пока меньше g.
Пока меньше g.
А по i больше k.
Ну, в смысле, наоборот,
не k меньше g,
и меньше k,
а наоборот k меньше i,
и g меньше k.
Груще, и и g поменялись.
А.
В таком смысле, нет.
Нет, и меньше, смотрите,
и меньше k взялось отсюда,
потому что я убил g.
Поэтому здесь я пишу и меньше g и k,
здесь я,
а здесь я, наоборот, убил i.
Потому что у меня вот эта слагаемая
от i не зависит.
Вот и все.
Так что вот такие, то есть,
как минимум, это еще такие базовые вещи.
Но, правда,
заметим, что в этих алгоритмах вероятности
она еще такая,
ну, идеенный смысл она дает скорее в константе,
потому что чем это алгоритмы приятны,
они приятны тем, что они там, может быть,
проще пишутся и быстрее работают
на практике.
Правда, минус, есть вероятность, что они работают
плохо, поэтому там, на практике, конечно,
там, например, вы стояли, с одной стороны,
ставят кусок, но пишут там оговорку,
видя, что если там глубина пошла
на два алгоритма, то там,
дошедший до туда кусочек,
мы досортировываем там хипом.
То есть это сделано для того,
чтобы гарантировать, что алгоритм ZNLogin работает.
Вот.
Но, как бы, идея,
то есть практически это, конечно, важно,
но теоретически не очень,
потому что там оба эти штуки
мы умеем реализовывать за указанные симптотики
и в честную.
Но тут появляется
мистическая задача,
которую мы вероятно с тобой
решать за время,
в смысле выточнение которой в принципиально
недостижимо в честную.
Что доказуемо?
Не знаю.
Осталось только придумать,
а как вероятностно сортировать?
Есть, конечно, BogoSort,
да, random shuffle,
проверка сортирования,
и все остальное.
И все остальное.
Да, random shuffle, проверка сортировалась ли,
если нет, повторяем.
Ну, если повезет, то да.
Да.
Ну, это если очень.
Random shuffle, ну, элементарно,
я бы его делал так,
кидаю рандомное число от 1 до N,
получаю число и свапаю,
и это элемент с первым.
Или свапаю и это элемент с последним.
Повторяю, операция на префиксе
N-1. Остается только сгенерить рандомное число.
Потому что, ну, как сказать,
ну, потому что как мы герим перестановку
на последнем месте у нас
по такому алгоритму,
каждый элемент попадает равновероятно.
При фиксированном,
попавшем туда элементе, все остальные попадают
на предпоследнее место тоже равновероятно,
ну и так далее. В принципе, получается
это, может, хорошо, что
каждая конкретная перестановка попадет,
станет рандомшавлом с вероятностью
1 делить на N-факториал.
Ну, по сути, да.
Поэтому, да, тут как раз все
несложно.
Ну, по модулю, то есть, конечно, начинается
главный анализ, конечно, там начинается это,
а как, собственно, генерить рандомные числа?
Ой.
Господи.
Мыли.
Нет, ну почему не понятно.
Классический метод, который, кстати,
который реализован, кстати, даже в,
как это называется, который работает
в тесли, в теслиби-полигоне,
вот если вы там откроете рандом,
а там, кстати, рандом предельно честно написан,
рандом не ссылается,
там внутренний генератор,
он говорит так, то есть,
я умею генерировать там, допустим,
64-битное число,
вот это я умею.
А дальше он говорит так,
ну вот, нет,
дальше он говорит так, что если я хочу генерировать число,
допустим, от 0 до N-1,
то я генерирую рандомное число на 64-бита
и беру его по модулю N.
Но при этом говорю, да,
это неравно вероятно, почему?
Четвертый, скорее всего, не делится на N.
Но это называется наша фирма,
это учла. Фирма говорит,
если этот рандом попал вот в этот вот хвостик
в остаток, то мы просто генерируем
еще раз. И еще раз, пока не попадем.
Там буквально это написано.
Ну да. Самый тупой вариант такой.
Более продвинутый вариант, конечно,
давайте лучше кинем монетку
сразу 10 раз,
тогда у нас получится 1024
исхода.
Если там, скажем,
монетка выпала, допустим, у нас там 10
решек, то перебрасываем. Остальные
1023 исхода, собственно,
делим на 3.
Вы уверены, что это более продвинутый
вариант? Это более сложный?
Это под множество моего варианта.
В случае, когда нам действительно выпало 10.
Формально, чтобы на это ответить, надо тогда посчитать
просто о каком от ожидания
количества итераций, которые
мы сделаем.
Мой алгоритм
падает, когда мы хотя бы
один раз сгенерировали...
Если мы сгенерировали ноль, то он завершается.
Ваш алгоритм завершается
не когда мы сгенерировали ноль.
Сгенерировали ноль, и еще там поделали до 10.
Не, парадит, мой алгоритм парадит.
Я его понимаю так. Мы кидаем монетку
два раза. Если там 2 единицы,
то перебрасываем,
в любом ином случае выиграли.
Получается, мы перебрасываем
с вероятностью 1 четверть.
Вот.
Тогда получается...
Я бросаю, но в важном
ожидании количества.
У вас гарантировано 10 бросков
будет всегда. У меня
почти никогда не будет 10.
У тебя...
Давайте так.
Во-первых, давайте
сразу это проделаем,
чтобы в будущем нам это пригодится.
У нас есть классическая
ситуация. У нас есть монетка,
которая выпадает с вероятостью P.
И не выпадает с вероятостью
1 минус P. Такая типичная ситуация.
Так вот.
У нас такая идея. У нас будет такая случайная
величина. Я буду китать монетку, пока она
не выпадет. Внимание,
вопрос. Сколько раз?
И будет случайная величина. Сколько раз мне пришлось
кинуть ее, чтобы это произошло?
Е равняется
П. Вот.
Давайте проделаем это.
Распределение даже как-то
называется. Отрицательное биномиальное
или что-то в этом роде.
То, что Ваня говорит, это очень корректно.
То, что E равняется П
на 1 минус. Давайте так. Смотрите.
Мат ожидания этой величины,
по-моему, это
величины,
кси, назовемую
кси от P.
Оно равно
С вероятностью P, оно будет
случайная величина, будет 1.
С вероятностью
1 минус P умножить на P,
оно равно 2.
С вероятностью 1 минус P в квадрате
на P, она
равна 3.
Так, это понятно, откуда я это беру
вообще?
Я просто предельно
в тупую пишу.
Плюс и так далее.
Через что?
П плюс и так далее.
П минус 1.
Зачем это делать?
П на 1 минус П на
И равно от нуля
до бесконечности.
В плане стандарта, чтобы что?
Значит, 1 минус П в степени И.
Откуда 1 минус П?
Хотя ладно, не надо так делать.
Давайте вот так сделаем.
От И минус П? Да.
Вот. То есть вот такую штуку
мы неожиданно суммируем.
Это можно просто
проинтрируем. Какая-то лажа.
Да.
В степени должно быть
и минус 1, кажется.
Да.
Пожалуйста.
Пожалуйста, хорошо.
И заметим, что это просто
производная
формальная. Так, это да.
Ну да, это да.
Ну да, это да.
Это формула для дискретной величины,
которая принимает не отрицательное значение.
Вероятность того, что больше
броно, чем М, вероятность того,
что больше броно, чем М.
И то он нам нужно просто предъявить этой штуке.
Предъявить нот. Нет, нот уже остался только выяснить,
чему он равен.
Ну, ноль.
Минус. Чёрт, какой ноль.
Нет, нет, нет.
Подождите, тут же можно как бы
два раза суммировать.
Давайте вот.
Смотрите, у нас один бросок будет точно.
Два броска будут с вероятностью
1 минус П.
Три броска будут с вероятностью 1 минус П в квадрате.
Ну хотя бы, да. Да, и что?
Ну вот. Слушайте, а почему
я не могу понять, почему нельзя
написать, что E равняется
P плюс 1 минус П
умножить на E плюс 1?
А почему это правильно?
Потому что мы заметим,
что этот ряд точно
сходится. А если он
сходится, то то, что я написал,
это надежда.
Ну скажем так, видел, потому что
это абсолютно сходящийся ряд, и мы
суммируем по всем, всем, всем, всем возможным
исходам.
Ну конечно, да.
Ну вот, да. То есть на самом деле, да, самое
крутое, что тут можно написать, что это по-любому
равно на
то есть это
равно, то есть равно
П умножить на 1 плюс
1 минус П на, соответственно,
1 плюс E.
Да.
Да, это равно
на самом деле 1
плюс 1 минус
П на
Эпсилон Кси. Ну отсюда следует, на самом деле,
приятная вещь, что Эпсилон Кси
равно 1 делить на П.
Вот.
То есть количество
ритораций, которые мы сделаем,
если собирать с того, что нам будет хорошая
она 1 делить на П. То есть получается,
если ты кидаешь вот эти две монетки,
то П у нас равно 3 четверти.
То есть следует,
в твоем случае тогда получается, что
от ожидания, видимо, общего числа бросков
монеток будет 2 умножить на 4 треть.
Так, что будет у меня?
У меня будет 10
Нет.
Если бы.
Нет.
Еще плюс какой-то.
Нет, не совсем. Вероятность того,
что мне повезет,
она у меня 1024
поделить на 1023.
Вот.
Ну да, 8 третьих меньше,
чем 10 умножить на что-то больше единицы,
это правда.
Но на самом деле надо задуматься.
Нет, даже интересно, а если
3 возьму, что будет?
А, не поможет 8 третьих, да.
Ну ладно.
Нет, ну просто мой алгоритм,
это ваш алгоритм, но
я еще, я обрубаюсь раньше,
то есть вы обрубаетесь,
вот у меня выпал 0,
я сразу обрубаюсь, а вы обрубаетесь
еще спустя какой-то количестве шагов,
чтобы дополнить до десятки.
Возможно, да.
Ну окей, ладно.
Ладно, копать не буду.
Хорошо?
В любом случае, ладно, это запомнили,
это нам пригодится. Еще.
Чтобы потом не пересчитывать.
Значит, давайте.
Но,
правда,
в следующие задачи на самом деле
это нам не понадобится,
но задача будет эпическая.
Задача называется
random3gameeval.
Чего?
Random3gameeval.
Evaluation.
Короче, задача такая.
Потому что у вас есть
интеллектуальная игра.
Езино.
Нет, ну может и корено.
Если бы, нет, все гораздо проще.
Грают два игрока.
Если бы.
Семь канат, да.
Где канат девятка, да-да-да.
Хватит фуксловить ворон, где не пойму,
где взятка, да-да-да, мы помним.
Вот.
Подтяните даму пик, короля направо, да.
Ну, а как, надо же как-то матроса
учить это морскому делу, действительно.
Вот.
Так вот, задача такая.
У вас есть интеллектуальное, у вас есть двоичное дерево
высоты 2n.
Изначально фишка находится в корне,
играют два игрока.
На каждом шаге игрок двигает фишку
в левую или вправую ребенка.
После того, как они сделают pn ходов,
они попадут в какой-то
лист.
И в этом листе будет написано,
кто выиграл.
Первый или второй.
Какой-то альфа-бета отсечение.
Скажем так, это...
Скажем так, это...
Это одна из простых версий альфы-бета отсечения.
И сейчас я на этом простом примере
прошу, почему альфа-бета отсечение
это эффективно.
В самом деле.
В чем фишка?
Идеальное решение детерминированное
подозревает, что давайте возьмем
честную динамику на этом дереве,
запишем, и все.
Это будет работать за от 4 степени.
Казалось бы, быстрее нельзя,
потому что надо просто дерево сочетать.
Но теперь попробуем от этого избавиться.
Для этого скажем, что у нас не дано дерево.
Все дерево большое.
Но у нас есть неожиданное
требование.
У нас есть какой-то
неожиданный черный ящик,
более известный как оракул, который, если мы
тыкнем в лист, он нам скажет, кто в нем выиграет.
Да, типа интерактивки.
И при этом, желательно, да, нам
гарантируется, что тесты там статические.
Говоря на языке полигона.
Не адаптивный интеракт.
Да, именно так.
То есть там гарантируется, что сначала
все сгенерено, и поэтому
интерактор никак не подгоняется.
Вот как-то так.
Вот такая вот красота.
Казалось бы, чем нам
может помочь
рандом?
При таком
интеракторе.
Ну, помочь может нам так.
Смотрите, идея такая.
Как мы должны в вершине
понимать, кто в ней выиграет?
Первый игрок или второй?
Ну, предположим, что ходит в ней
первый игрок, да?
Ну, нам нужно,
мы пойдем влево ребенка выясним,
кто там выигрывает, и пойдем в правого ребенка
и выясним, кто там выиграет, правда?
И потом,
если в обоих выигрывает
противник, значит и тут
выиграет противник, а если
хотя бы в одном из этих
вершин выиграет первый игрок, то значит
у нас и вообще победил первый игрок,
правда?
Вот у меня возникает неожиданная идея.
Теперь заметим следующее.
Если мы пойдем в какого-то ребенка и выясним,
что там выигрывает первый игрок, то во второго
мы в принципе уже идти не обязаны.
Потому что результат мы и так
в принципе знаем.
Поэтому возникает идея.
А давайте идти не просто
там сначала влево, потом вправо.
То есть мы пойдем сначала в одного, потом в другого.
Но
в каком мы будем идти? Мы будем
выбирать броском монетки.
То есть вероятно, что одна-вторая мы пойдем
либо влево, либо вправо.
И теперь нам жутко интересно.
То есть жутко
интересно мат ожидания
количества листов, которые
мы запросим.
Понимаете, да?
Или что
то же самое, в общем-то,
более точно может быть, мат ожидания
количества вершин, относительно которых мы
запустимся рекурсивно.
Это какое-то
жутко интересное.
Почему?
Что такое мат ожидания, мат ожидания?
В смысле у нас при
фиксированных листьях?
Ну да.
Смотрите, да, листья
фиксированы.
То есть у нас
не будет такого, что если
перебрать все варианты
листов и посмотреть в среднем,
то получится хорошо. Нет, такого у нас не будет.
У нас будет
смотрите, то есть у нас листья будут фиксированы.
То есть мат ожидания,
то есть это будет случайная величина, которая будет зависеть
только от того рандома,
то есть рандом ей будет только тот, который мы сгенерим.
Это же может быть
сильно разное при разных листьях.
Да, но
может быть разное, но мы хотим показать,
что при любых листьях
мат ожидания не превосходит
чего-то хорошего.
Но давайте подумаем, чего
оно может не превосходить.
То есть давайте посчитаем. Мат ожидания даже не количество листьев,
а количество в целом вершин, с которыми
мы будем работать.
Вот находимся мы в какой-то вершине
на высоте 2n.
Да?
Тогда смотрите.
Скажем вот что.
Если предположим, что она
выигрышная, да?
Тогда заметим,
что с вероятностью 1,2
я из этой вершины сделаю только один
рекурсивный вызов. Правда?
Понимаете, да?
Если она выигрышная,
то у нее есть проигрышный сын.
Да. Если вероятностью 1,2, мы сразу
в него и попадем.
Да.
Ну, в худшем, да. То есть хотя бы с вероятностью 1,2,
мы просто за один рекурсивный вызов это узнаем.
Правда?
Вот. Но я буду...
Так что...
То есть получается, что
так сказать, среднее время
t, ну я буду подозревать, что это среднее,
от 2n, то есть
если она выигрышная, да?
t там
для выигрышной вершины.
Это...
Вот.
Оно не превосходит к чего?
То есть, что
с вероятностью 1,2, это будет
просто t проигрышная от 2n-1
условно.
Плюс. И с вероятностью
1,2 придется с двумя вершинами
поработать.
Вот.
То есть 1,2 умножить
на 2 максимума
из там...
Не максимума. У нас тогда
гарантированно нижняя будет выигрывающая,
а вторая проигрывающая.
Это просто плюс. Ну хорошо.
1,2 тогда будет так, давайте.
tv от 2n-1.
Одна из них будет точно
проигрышная. 2n-1.
Плюс.
А вторая...
Иначе бы мы
не запустили второй раз.
Да, хорошо. Уговорили, уговорили.
Ну, потому что забираю вперед, я захочу
смотреть не на один уровень вперед, а на два.
Вот.
А теперь смотрите.
А теперь что делать, если
это проигрышное?
Это когда дерево немножко напоминается.
Да?
Ну там было жестче, да, у меня тоже флешбейт.
Ну не знаю, странно, у меня
что-то нет в ситуации. Ну ладно.
Значит, tp от 2n. Это что такое?
Ну это конечно
2tv от 2n-1.
Логично, да?
Ну правда, тут надо
формально нагреть, что
tv и tp это какие-то верхние оценки.
Потому что, как вы правильно
заметили, конкретные значения
tv и tp зависят от конкретной
вершины, от конкретных детей, которые у нее там есть.
Поэтому на самом деле,
формально, конечно, это мы такие...
То есть приблизительно пишем,
чтобы довести до формального доказательства,
придется доказывать, что для любой
вершины, ее мотожидание не превосходит
вот этого вот, и там уже прописывать.
Так что вот.
Идея на это понятна, но
формально поговорить стоит.
Так или не понятно, о чем я?
Вот. Так, Миша, а вот ты живой?
Кто здесь?
Ты на лекции.
Ну вот.
Так вот.
А, я еще забыл.
Еще надо плюс один написать.
А то нули получится.
Ну вот.
Так вот.
Но теперь заметим следующее.
Так, внимание, теперь смотрите.
tv от 2n-1.
Вот. Ну ладно, не превосходит.
Потому что имеем в виду, что это верхние
оценки.
Но tv я тоже могу расписать.
Да, вот это...
Ну у нас...
Да, ну типа
мы там оценивали, мы
вводили несколько разных.
Мы сначала отсекали верхний, потом
нижний, потом квадратик.
А потом мы на квадратик уже писали оценку,
что он хороший.
А то это вообще не сравнилось.
А здесь у нас типа на tv
оценка какая-то хорошая,
на tp не очень хорошая, но мы его
раскрываем в tv.
Ну вот, смотрите. Ну теперь 2n-1
можно расписать вот ровно по этой формуле.
Вот.
То есть по этой формуле она расписывается
так. То есть пишем, что тут
получается.
2tp от
2n-2
плюс tv
от 2n-2
плюс
сколько там получается?
Три.
Ну то есть суть
на самом деле такая.
То есть tp
получается у нас...
Нет, почему? То есть если оценивать
среднее количество запусков, то получилось, что
то получилось следующее,
что tp на уровне
на два уровня ниже сделает в среднем
три рекурсивных запуска.
Ну...
А что не так?
Ну мы tv не знаем, а tp не знаем.
Нет, да, ну просто tp
от 2n свелось к tp от
2n-2 и tv от 2n-2.
Ну мы не можем оценить такой tp.
Нет, ну просто у нас
уже tp
уже хотя бы два.
Так.
Ну и что?
А у нас было четыре.
А ты хотел завалить ее, типа?
Ну да.
Вот.
Нет, ну хорошо.
Нет, ну на самом деле смотрите,
просто останется на самом деле следующее. Вот это вот tp
это что? Давайте вот эти tp и tv
распишем еще, да?
Давайте вот в них пойдем.
Что вот это вот tp
оно сведется к двум запускам.
То есть оно не происходит.
Сразу оно не происходит.
2tp от 2n-2.
Тут без вариантов. Еще плюс
1.
Вот это вот tv от 2n-1, да?
А я так не расписывал на самом деле.
А что не так?
А я просто рекурсивно.
У нас tv это tp плюс 1 вторая tv.
Можно расписать то, что осталось
1 вторая tv снова через tv.
И выразить tv чисто через tv
с умой такой герметрической прогрессии.
Ну,
можно так, но не знаю.
Мне так нравится.
Тут кому как.
Потому что это вот tv теперь расписываем.
Ну, то вот tv на один уровень вниз
мы тоже вот так вот можем расписать, да?
То есть давайте прям вот так вот по чесноку.
Так и пишем.
То есть 1 вторая на скобочках.
tp от 2n-2
плюс 1 вторая
tv от
2n-2 и плюс 1.
Еще тут в конце плюс 1.
Что я забыл?
Где?
Где здесь?
А, tv, да, согласен.
Вот.
То есть это равно.
Так, ну, то есть если тут
раскрыть какие-нибудь скобочки,
то тогда что у нас тут получится?
К вот этому.
Нет,
это я просто вот это продолжил.
Да, да, согласен.
Родомный такой порядок, но
что делать. Так, значит у нас тут
2 tv у нас тут вылезло.
Значит, получается tv
у нас получается что-то типа
9 четвертых tv
от 2n-2
плюс 1 вторая
tv от
2n-2
плюс сколько там у нас получается?
5 вторых.
Вот.
То есть смотрите.
То есть я утверждаю
на самом деле, что
если эти брать верхние оценки,
то...
Вот.
Ну, в принципе, да, то есть заметим
на самом деле следующее, что среднее...
То есть у нас тут получилось такое...
Ну, то есть заметим, что среднее количество
рекурсивных запусков на самом деле не
превосходит... То есть на уровне 2 ниже
не превосходит 3.
То есть в проигрышной вершине у нас эти...
То есть получается мы запустимся, в среднем получается
3 раза.
А в случае проигрышной получается
1 даже не 3, а 1 с их четвертых у нас
вылезло.
То есть видимо можно чуть-чуть
точнее оценить.
Нет, ну давайте посмотрим, что из этого можно
выжать.
Ну просто хочется доказать
утверждение.
Давайте так, утверждение.
ТП
от 2Н и ТВ от
2Н не превосходят
там 3 в степени Н
умножено к какой-нибудь веселую константу.
Например, ну давайте
на С.
Ну, в нашем любимом стиле.
Да, хотя
видимо придется...
Ну, наверное
можно еще больше выжить, потому что
у нас чуть-чуть неравномерно.
Не, ну там видимо придется видимо минус Д подгонять
что-то, насколько я прикидываю.
Ну потому что, смотрите, давайте
делаем индукцию по ТВ.
Ну, точнее, по ТП делаем индукцию.
То есть
по ТП у нас получается 3
в степени Н-1 умножить на 3.
Правда?
То есть получится...
При доказании ТП от 2Н получается
будет не превосходить на
там 2 плюс 1
на С на 3 в степени Н-1.
Поэтому если
и плюс 3 доказать, что это меньше
либо равно, чем С на 3 в степени Н
мы просто так не сможем, потому что это и есть
С на 3 в степени Н.
Поэтому Д видимо придется забабахать.
То есть пишем
минус 3Д плюс 3.
Хочется сказать, что это меньше
либо равно, чем там
Д на 3 в степени Н минус
Д.
Но это верно просто при...
Ну, я не знаю. То есть понятно,
пишем, что Д больше либо равно там, я не знаю,
полутора, по-моему, хватит.
Там что-то полтора типа надо, или две трети.
Нет, наоборот,
три вторых, конечно.
Нет, сколько надо-то?
А, ну да, 2Д, да, понятно,
2Д должно убиться
тройкой, поэтому Д должно быть полторашкой.
Вот.
Причем Д больше либо равно
1.5,
а С должно быть равно...
А С может быть равно чему угодно.
Вот.
Так, ну и еще, на самом деле,
но это было в ТП.
А в ТВ, на самом деле, получается,
еще круче получается,
11 четвертых на
С на 3 в степени Н минус 1
минус 11 четвертых
Д плюс 5
но если Д брать полтора,
то, по-моему, у нас это устраивает.
Ну, потому что
9 четвертых умножить на полтора, это сколько?
По-моему, 54.
9 четвертых умножить на полтора?
А, нет.
А, черт, да.
Да-да-да,
все, да, мои голубые меч...
Ладно, получается, сколько там?
1.35
Нет, что за бред?
А, просто при 13.5
поделить на 4.
Но это больше трех.
Поэтому то, что больше трех,
оно, соответственно,
убивает 5 вторых.
Не, ну я полторашку люблю.
Так, ну полторашка, она меньше,
поэтому она...
Нет, потому что чем больше Д, тем...
Смотрите, откуда я возьму С?
Я С возьму из базы индукции,
когда я равно 0, поэтому чем больше я там возьму Д,
тем больше мне в этой базе индукции придется С
брать.
Поэтому, значит, Д полторашка
пойдет.
То есть меньше либо равно,
чем в данном случае
С на 3 степени. Да, и тут, конечно,
мы, конечно, могли понадеяться, что тут вот эти вот...
То есть у меня тут не хватает четвертинки
С на 3 степени или 0.1, может,
это даже победит как-то, но...
Да, ну в данном случае я сказал, что
у меня тут так вылезло полторашка.
А если вы будете оценивать отдельно ТВ, а отдельно ТВ,
то это наверняка должно быть...
Может быть. Но давайте посмотрим,
какую С мы теперь выжмем.
Так, ну давайте так, какая база индукции?
Да, мы знаем, что
ТВ от 0
равно ТП от 1, равно 1,
тупо, да? Потому что там
мы эту вершину вызвать обязаны будем, правда?
Поэтому теперь надо подогнать...
Ну вот.
Но тогда вызов получается,
что С... В общем, если
D это полторашка, то С придется делать
два с половиной.
Вот.
Ну вот.
Вот.
Так что вызов...
Ну, вывод, на самом деле, очень простой.
А симпточка решения, которая у нас получилась
в отожидании, О от 3
в степени N.
А если бы я еще
оценивал, кстати, не писал бы эту единицу,
потому что оценивал бы в явном виде количество
вызовов интерактора,
то был бы просто 3 в степени N.
Но при этом...
Смотрите, легко заметить следующее.
Если бы требовали детерминированного решения,
и интерактор был бы адаптивный,
то я боюсь, он бы мог нам...
То в худшем случае
без перебора всего
мы бы не обошлись.
А как бы он адаптировался?
А, типа он бы специально подсовывал в худшем случае?
Да.
В чем? Жестко подсовывал.
То есть, смотрите.
Если мы хотим 2N,
в чем он бы подсовывал так?
Допустим, я нахожусь на высоте H, и он нам хочет подсовывать
в случае, когда H выигрывает.
Тогда что он сделает?
Тогда, когда я вызовусь от одного ребенка,
куда-нибудь пойду, он будет мне
подсовывать до последнего так, чтобы...
То есть, он будет мне до последнего мурыжить,
а потом в конце мне выдаст, что я выигрышный.
Чтобы я потом пошел в другого ребенка,
где он меня будет мурыжить до последнего.
И он может мурыжить к проигрышному.
А если я проигрышный,
то он в обоих меня будет мурыжить
до последнего, но в итоге осознается, что там оба выигрышные.
В чем?
В общем, оба раза получается рекурсивно.
То есть это, например, на пальцах не совсем
еще строго доказательства получилось да но но тем не менее так сказать что в худшем случае
придется реально за 4 степень а вероятность в статическом интеракте пожалуйста уже 3 степень
то есть это принципиально то есть это это вам не эти ваши четыре русских с вашими поделил налога
но возрадуйся да то есть это он 4 степень заменили на 3 степень вот то есть в общем-то на самом деле
ровно на этом и основанные там всякие альфа-бета отсечения но давайте вкратце расскажу раз об этом
зашла речь что такое вообще альфа-бета отсечения так я сейчас конечно очень сильно на пальцах
расскажу но тем не менее но суть на самом деле такая потому что часто бывают то есть мы конечно
с вами рассматривали игры в которых просто кто-то выигрывает то есть кто-то выигрывает кто-то
проигрывает и все но бывают на самом деле более продвинутые игры то есть бывает ситуация что
первый игрок ходит второй игрок ходит первый Easy, a не как-то ходит, а потом приходит в какое-то
конечное состояние и там написано число ц, который говорит о том что первый игрок получает от
второго ц. там ц денежек. да причем если будет за может быть как положительным так и отрицательным то
есть это то есть положительно означает что первый игрок выиграл там песка выиграл ц
это означает, что, например, второй игрок выиграл минус
С.
Ну, может быть, ноль, значит, типа ничья.
И тогда, конечно, оба игрока хотят максимизировать
свою прибыль.
Но тогда, если у нас есть такое рекурсивное дерево,
ну, тоже такого же рода дерево, то тогда обычно
появляется такое понятие, как цена игры.
То есть, что значит цена игры?
Это означает, что есть какое-то мистическое число А, такое,
что первый игрок, у первого игрока есть стратегия,
которая гарантирует, что он получит не меньше А.
Но, с другой стороны, как бы ни играл второй.
А у второго, независимо от этого, есть своя стратегия,
которая гарантирует, что как бы первый игрок ни играл,
он больше А не получит.
Вот, понимаете, да?
То есть, если у вас есть какая-то ациклическая игра с конечными
состояниями, то, в принципе, такие штуки очень легко
динамичкой насчитать.
Согласны?
Так, догадываетесь, как делать примерно, да?
Ну, просто для каждой вершины это аккуратненько по индукции
можно честно вывести.
Да, нет, все.
Вот.
Вот.
Но иногда возникает ситуация, конечно, когда это
дерево очень большое, то есть мы там можем в конце
оценить сколько будет.
Но, с другой стороны, откуда берется альфа-бета отсечения?
Потому что, допустим, что у вас есть там в каждой вершине
как и ранее два варианта хода.
Ну, допустим такое, да?
То есть, надо пойти влево или вправо.
Тогда идея такая.
Допустим, мы идем в рандомного ребенка и получили, что
там результат игры С.
Тогда мы должны, конечно, можем ли мы получить лучше?
Ну, конечно, можем.
А можем и не получить.
Но у нас появляется вариант отсечения.
Вариант заключается в следующем.
Он нам скажет, что если вы гарантируете, что у вас,
то есть, если существуют какие-то ивристики, которые
гарантируют, что ответ на задачу заведомо не лежит
в полуинтервале от С до плюс бесконечности, то
есть, он заведомо меньше С, то можно туда не лезть.
Логично, да?
То есть, а можно просто сразу сказать, что ответ С
и сказать, что там все плохо.
То есть, более того, как это дальше будет работать?
То есть, вы вот этой вершине передали, что нас интересует
только ответ от С до плюс бесконечности.
То есть, мы просто, то есть, если мы в какой-то вершине,
если мы уже, допустим, если мы откуда-то точно знаем,
что у нас ответ меньше С, то мы будем просто считать,
что ответ С для нас это одно и то же.
Понятно, да?
А как это будет теперь работать?
Ну, тогда смотрите, какая тут идея.
Допустим, мы пойдем в какого-то ребенка, и выяснится,
что у него ответ D.
Что в нем нашелся ответ D, причем D оказалось больше С.
Тогда, когда мы пойдем во второго ребенка,
получается, нас что интересует?
Да, ну вот.
Да, причем здесь я, когда я под результатом,
я подозреваю результат именно для глобального первого игрока.
Понимаете, да?
Ну, знаете, вот на самом деле в теории игр
как бы есть разница, есть первый и второй игрок,
а есть красный и синий.
Да, это могут быть разные вещи,
потому что, понятно, в разных позициях
как бы кто-то первый, кто-то сейчас ходит,
а кто-то не ходит, да?
Ну, чтобы зафиксировать.
Так вот, то есть мы говорим здесь как бы C и D,
это как будто типа выигрыш там,
то есть это условно там выигрыш первого игрока.
Но теперь смотрите, что нам надо?
Так как в этой вершине мы выбирали максимум,
то в этой вершине мы уже будем выбирать минимум, правда?
Вот, то есть получается,
то есть нас на самом деле получается числа больше D
тоже не интересует,
но и меньше C нас не интересует,
то есть поэтому получается,
что здесь мы запускаемся рекурсивно
и говорим, что нас интересует только числа C и D.
Если мы гарантируем, что у нас
результат этой игры не попадает в этот отрезок,
то мы сюда просто не идем,
потому что нам не интересно,
в чем не интересно даже если там
либо как когда надо супер-меньше,
так как когда супер-больше.
То есть оказывается,
что если вот выбирать рандомные шаги,
то тогда это отсечение
может очень неплохо работать.
Это называется альфа-бета отсечение.
Почему он так называется?
Потому что обычно в этом месте используют буквы альфа и бета,
а не C и D, как это сделал я.
Вот.
И в принципе оказывается,
ну в принципе мы уже даже какие-то
основы посчитали, что действительно
это очень хорошо
в среднем отсекает.
Ну то есть конечно все зависит от ваших
конкретных конечно
эвристик
на самом деле.
Вот.
Но более того в принципе
на этом основаны практически
чуть менее чем все на самом деле
какие-то там
компьютерные эмуляторы каких-нибудь
интеллектуальных игр типа шахмата.
Ну практически как устроен?
Практически любой стоквиш.
То есть устроен он так.
В нем есть
шахматная позиция
и у стоквиша есть какая-то оценка
этой позиции.
А как оценить шахматы?
Ну вот как-то
вот здесь это...
Нет-нет-нет, если бы оценивать так.
Но есть просто оценка позиции.
Например, то есть ну так
есть допустим лишь какая-нибудь пешечка
говорим так.
Это хорошо, это типа плюс один.
Есть какой-нибудь слоник, ну окей, пишем там
три или там два и восемь.
Ну там есть какие-то более точные уже оценки,
то есть там слон это может уже весит не три,
а два и восемь, или там три и два, даже я не помню.
Но это я самое
простое пишу. Или там есть
ферзь, отлично пишем, плюс девять.
Ну
нет, смотрите тут по-разному.
Если брать то, что я читал в детских книжках,
то да, ферзь это десять, лодяя это пять,
кони-слон по три, понятно.
Конь четыре.
Ферзь девять. Вот. То есть более
современно все говорят, что ферзь
все-таки девять. Вот.
Но там дальше начинаются детали.
То есть там начинаются там всякие детали,
там вида, что там скажем...
Проходная пешка. Нет.
Например, да, если там проходная пешка,
которая еще очень далеко прошла, то как бы это уже
это тоже классно.
Вот, это почти ферзь, это раз. Во-вторых,
еще в открытой позиции, допустим, есть
преимущество двух слонов, потому что там слон сильнее
коня, например. А в закрытой,
наоборот, конь сильнее слона. Ну и начинаются
всякие такие детали, позиционные факторы и так
далее. То есть тут уже...
То есть тут...
Чего?
Хорошо, да.
Ну девять, хорошо. Ну девять так девять, хорошо.
Ну,
видимо, потенциально...
Ну, видимо, так на одну пешечку.
Нет, ну там надо
считать, потому что как бы в шахматы
известно такое, что преимущество в одну пешку,
это само по себе не преимущество.
То есть нет гарантии, что этого
хватит для победы.
Ну, то есть дальше там надо
действительно читать всякие таблицы
Налимова и так далее, там смотреть, видимо.
Ну, типа в среднем случае
два ладьи, две ладьи ферсия,
это как бы ничья или чья-то победа все-таки.
Можно не запариваться, просто
пустить 100 рандомных игр
с неумными игроками
и посчитать процент победы.
Не-не-не, ну это же тоже не то,
потому что мы же хотим, чтобы игроки играют оптимально.
А мы говорим, что если у нас позиция
хорошая... Ну, в плане, если у нас
позиция чуть лучше,
то два игрок, которые играют
по спинсу рандомных, у тебя время играть.
Не-не-не, это
тоже и факт,
потому что бывает, на самом деле, позиция хорошая,
но для выигрыша в ней нужно делать точные ходы.
Да, понятно.
И правильный ход сделать выигрыш, а любой другой ход
тебя снесут.
А как вы тогда найдете его с помощью своей оценки?
Считайте так. Ну, во-первых, оценки
делаются с привлечением
профессионалов и может быть
может быть где-то машинного
обучения. Вот это раз.
А во-вторых, точная оценка выводится не так.
Это, я сказал, оценка позиций на глубине ноль.
То есть, реально, стопич работает
так. У него есть такая оценка,
она как-то устроена там, как-то, да?
Ну, у всех движков она как-то по-разному
устроена, просто
чем лучше позиция, тем
лучше движок. А также,
а потом на какую-то глубину h,
заданную там, задаются
пользователям, запускается вот
это вот.
Вот просто вот эти вот стандарты на альфа
и переборы с отсечениями.
Вот. То есть, дальше
все зависит, точность
анализа может зависеть, а, от оценки позиции,
б, от производительности вашего компа.
Ну, понятно, что программа
устроена чуть сложнее, потому что она
не просто лучший ход выдаст, она же вам
обычно несколько лучших ходов выведет
и выведет у каждого его оценки там и
еще посчитает, куда там примерно, как надо ходить.
Вот. Но общая суть
там, соответственно, одна и та же.
Ну, про эти рандомные запуски я не просто
вам сказал. Правда, есть команда,
новая программа катагома. И, короче,
вот, она действительно не очень хорошо считает
форсажи. То есть, выигрыши,
которые вот, прям, вот, расставить.
Но позиционно она просто
сметает под ноль. То есть,
она, как бы, позиционно тебя
уделывает вот так вот.
А потом можно сюда прикрутить просто какого-нибудь
солвера, который сам уже
конкретно считает форсажи.
Вот. И тогда
получается, типа, дикая смесь.
Ну, вот. Ну, там по-разному можно.
Потому что дальше все, конечно, кроется
в оценке, но как ее строить, это вот отдельная
песня. То есть, где-то рандомно, где-то
оно, потому что какой-то диандекс в этом месте бы сказал,
что давайте опросим там квалифицированных
шахматистов. Каждый скажет там, типа,
оценит эту позицию там в пользу белых,
в пользу черных.
Нет, это капча. Оцените
шахматную позицию
на шкале от нода 10.
Ну, вот там. Ну, то есть, там
будет что-то такое. То есть, где-то так. Ну, понятно,
нет. Есть, в конце концов, может быть, нет. Есть, конечно,
где-то к нейросетке машинное обучение.
Потому что альфа-го, конечно, пошла дальше.
То есть, мы там, ну, там машинки,
то есть, ей там объяснили правила
шахмат, называется, и там она
четыре минуты там думала, пытаясь играть
сама собой, анализируя.
Четыре минуты? Четыре минуты.
А как она вообще решение
принимает? Ну, там нет.
Ну, скажем так, об этом вы
поговорите на курсе. Подробно там
в альфа-го вы там поговорите на курсе
машинного обучения, конечно.
Загружает библиотека дебютов?
Нет, нет.
Утверждает, что они даже не загружали.
Ну, нет. То есть, работать она будет совсем в тупую.
Она будет анализировать типа так.
Я сделал вот этот ход, и
оказалось, что я поставил противнику мат. Значит,
это крутой ход. Ура! Значит,
наверное, в этой позиции надо всегда так делать.
Потом следующая мысль. Так, если я пойду
туда, то следующим ходом противник мне поставит
плюс бесконечность. Так значит, так ходить
не надо.
Следующий раз, когда я попаду в эту позицию, так ходить
не буду. Ну и так далее.
Ну да, но дебюты в плане там слишком сложно...
Нет, нет, нет. Утверждает,
что альфа-го не загружала никаких дебютов,
а просто ее обучали
четыре минуты, и после этого она
начала стокреж выносить.
Ну, там не то, чтобы совсем под ноль,
то есть, по-моему, стокреж сколько-то там партии из тысячи
все-таки даже выиграл,
но там их, по-моему, было их
особо много. То есть, там все-таки
альфа-го победила с явным преимуществом.
Но, то есть, маленькая оговорка, что
четыре минуты, но при этом
использовались просто огромные мощности,
то есть, условно говоря, там на вашем лаптопе
вы это не сделаете.
То есть, как бы, да.
Ну,
скорее, да,
на очень большом количестве серверов Гугла.
Там параллельно по всему миру
и так далее.
На своем компе вы это не повторите.
Нет.
Вот.
Нет, ну пожалуйста, нет, пожалуйста,
бери свой комп там, запускай, там может быть
даже на трое суток запустить, а потом сравнись с альфа-го.
Нет, в принципе, это на самом деле
в этом есть какая-то магия действительно,
что шахматистов восхищают, что есть
шахматисты. Да, в том числе внутри
шахматистов есть, там, понятно, есть слабые,
есть сильные, есть сильнее, есть там
grassmeister, есть междуродные grassmeister, есть там
Карлсоны, да,
а есть просто Марлос Карлсон, да.
Ну, который, да, несмотря на потерю чемпионского
титула, все равно сейчас считается тем не менее
лучшим.
А он просто отказался?
Да.
Нет, ну скажешь так, формально он не является чемпионом
мира сейчас.
Почему?
Потому что он отказался защищать титул.
Да, действующим чемпионом мира является китайский
гроссмейстер Дин Лежень.
Ну, это там в чемпионском матче сокрушивший там
гроссмейстера Фиды Яны Непомящего.
Садая.
Не, ну я согласен, да, счет 3-2, конечно, это не тот счет,
чтобы сказать, что прям сокрушил или там 3-2, по-моему.
Не, нет, ну хорошо, победил, ладно.
Не, ну я не знаю, но тут осторожно, я иногда начинаю,
ну ты знаешь, иногда я в этом месте вообще начинаю
использовать слово «грохнул».
Ну вот, но там, я говорю, из этих 14 партий в чемпионском
матче, по-моему, там счет был, по выигранным партиям
был, по-моему, реально 3-2 или даже 4-3.
Переяло.
Ну вот, ладно, более корректно, ладно, обузрал, все, да,
так произошло.
Вот.
И так же соответственно.
Ну, просто магия в том, что есть это, а есть соответственно
стоквиши.
Ну там где-то, начиная с конца 90-х годов, действительно
компьютеры стали играть сильнее человек.
Да, это было не тривиально, потому что в середине 90-х
годов на самом деле были даже просто матчи сильнейших
компьютеров с сильнейшим шахматистом, который был
на тот момент Гарри Каспаров.
Более того, первый матч против Айбебовского компьютера
на самом деле Гарри Каспаров даже выиграл.
Вот.
Но потом во втором матче шансов у него уже не было.
Ну ладно, там 6-4 все-таки было.
Не-не-не, нет, нет, все он как бы все адекватно делал,
все-таки там уже было, первый раз, если компьютер там еще
как-то готовился, то есть его там пугали программу,
то есть программу еще относительно скрывали и так далее.
Но он, правда, тоже к этому матчу готовился, то есть
он там знал, что там понятно, в этот компьютер книгу дебютов
точно загрузят и так далее, поэтому, конечно, у него
была задача там наоборот поставить компьютеру позиции,
с которой он там анализировал с меньшей вероятностью
и все такое.
То есть где-то так запутать.
Вот.
Но потом во втором матче он выиграл, даже несмотря
на то, что ему просто торжественно дали дискетус, или что там
у него было с программой, которую он там мог сидеть
гонять дома, не помогло.
То есть начиная, видимо, примерно с этого момента
компьютеры уже стали заведомо сильнее и как бы превратились
уже в инструмент анализа.
Вот.
Но при этом выясняется, что да, у нас есть стокфиши
разных модификаций, то есть они тоже там растут по
уровню.
Да, то есть они заведомо, типа, вот выше всех мелковых
шахматистов, но при этом есть еще альфа-го, который
еще круче играет.
Который сам мелковых шахматистов.
Нет, то есть это, нет, то есть это просто всех уже
восхищает.
Не, если бы он, наоборот, превосходит мелковых шахматистов
прям на, уже там, в инцельголов.
Нет.
То есть парадокс в том, что при этом как бы шахма
это какая-то простая игра, в которой у игроков есть
непроигрышная стратегия, или у какого-то игроков есть
выигрышная стратегия, я не знаю, как мы с вами это
доказали, собственно, простым образом.
Но, то есть как бы по идее у этой науки чисто теоретически
есть конец.
То есть можно о шахматах все понять, но, как говорится,
до этого еще ступеньки, и ступеньки, и ступеньки.
Вот.
Но, тем не менее, соответственно, но, собственно, вот тут мы
немножко поговорили о том, как они устроены.
Ага, да, да, да, так и рассказать, да, да, да, да, учебник Гарри
Каспаров, «Шахматы как модель жизни», да, да, да, или
любая другая еще книжка Каспарова там, да, ладно.
Так, ну, конечно, в этом месте, конечно, нам пришло время
сделать перерыв, судя по времени.
Во-первых, вам нужно было хотя бы там тупую ботовую
стратегию хотя бы победить, ну, она там почти тупая,
ее там побеждает почти все что угодно, то есть это
квалификация.
Чтобы прошел квалификацию, пошел балл.
Вот.
А потом после этого, значит, среди всех прошедших, ну,
последних версий, забабахивается турнир, и, соответственно,
вы получаете за него от нуля до двух зачетных баллов
в зависимости от доли побед, которые вы одержали.
Вот.
У нас такой турнир веселый был.
Вот.
То есть пока, пока, судя по всему, да, там абсолютным
чемпионом является, называется товарищ Павел Храмович,
если вы такого знаете.
Ну, в общем, да, очень маловероятно, потому что, да, Павел Храмович,
ой, это на год старший Илья Степанова, а не, или, ну,
да, да, да, вот это мое первое поколение студентов Дакар
Гальцев, там, Храмович, там и так далее, вот все эти
люди.
А, и Костя Семенов, конечно.
Вот.
Но, но, но, но этих всех, но этих всех людей прошлого
вы, естественно, уже не знаете, да, то есть потом
уже там появились всякие там, то есть на следующем
поколении уже появились там Женя, там Женя Белых,
там Илья Степанов, там Андрей Сергунин, там и так далее.
Вот.
Ну, да, это все, я говорю, это 14-15 год, это все.
Так.
Ладно.
Идем дальше.
Все?
Все.
А.
Ну, тут.
Ну, с розетками тут, да, напряг.
А, ура.
Ну, хорошо.
Значит, смотрите, что будет дальше.
Так.
Ну, теперь мы будем решать задачу, которую когда-то
уже решали.
Ну, хотя ладно, принципиально других задач у нас особо
и нету.
Вот.
То есть сейчас мы будем искать минимальный разрез
в графе.
Ну, и, и, и, и, и, и, и, и, и, и, и, и, и, и, и, и, и, и,
и, и, и, и, и, и, и, и, и, и, и, и, и, и, и, и, и, и, и, и, и,
Вот.
Да.
Но без вот этого бодслеса.
Потому что мы не будем писать потоки.
Да мы и не написали.
Молодцы.
Ну, что я могу сказать.
Молодцы.
Вот.
Но мы рассмотрим задачу даже в более простенькой
редакции.
Мы будем искать минимальный разрез в невзвешенном
графе.
То есть, короче, дан неориентированный, невзвешенный граф.
То есть там, величиной разреза называется просто
сколько ребер его пересекает прям в тупую.
Задача – найдите минимальный разрез.
То есть разделите вершины на два.
Ну, не то чтобы.
Нет, я бы сказал просто там.
То есть найдите минимальный разрез.
Назовем нож рёбер мостом, если при удалении его теряется
связанность.
Ага, найдите мост.
Хорошо, да.
Ну, можно сказать, да.
Найдите мост минимального размера.
Это капа, капа из дисконата.
Ну, да.
Ну, К-мост, да.
Ну, да, да, да.
Нет, но мы про это всё что-то знаем.
Да, мы знаем, что у нас есть даже компоненты рёберных
связанностей на самом деле.
По-моему, мы с него оценивали Альфа-Ж.
Возможно, да.
Ну, может быть.
Может быть.
Ну, кайфец.
Ну, по-разному, да.
Оно везде.
На кубке МФТ мы выясняли, что у нас есть понятие рёберная
связанность.
Вывод мы этот сделали из структуры дерева Гамариху
внезапно.
Соответственно.
Ну, вот.
Значит, как тут искать?
Ну, тут мы ей не предлагаем этими умными словами бросаться,
а предлагаем вместо этого делать очень простую вещь.
Смотрите.
А давайте возьмём рандомное ребро.
Вот возьмём какое-то первое попавшееся рандомное ребро
и скажем, что оно в разрезе точно не лежит.
Потому что я сказал.
Ну, раз оно точно не лежит, так давайте эти две вершины
сожмём в одну.
Ну, это уже напоминает Гамариху.
Не, ну почему Гамариху?
Там убрали тоже ребро и сжимали вершину.
Нет, там Гамариху не совсем так.
Гамариху говорила, давайте найдём разрез между какими-то
двумя вершинами.
Получился такой большой разрез.
И тогда говорила, что если мы ищем теперь разрез между
двумя вершинами с одной стороны, то другую сторону сжимаем
до вершины.
Здесь Гамариху.
Здесь немножко другое.
Здесь мы взяли рандомное ребро и сказали, так оно
в разрезе не лежит, давайте его сжимаем.
А что делаем дальше?
Так, ну что мы легко сделали?
Мы же рандомное ребро за УАПЕ, наверное, находим
и сжимаем, да?
Е, это как минимум В-1, потому что если граф не связанный,
то соответственно ответ сразу ноль.
А если Е равно В-1, то он ещё дерево, и тогда ответ один.
Либо Е больше, либо равно В, и вместо В плюс Е
всегда пишем Е.
Так вот, за УАПЕ мы находим рандомное ребро и сжимаем
его.
Так, что делать дальше?
А что?
Ну очень легко делается.
Давайте ещё раз это сделаем, вот это сожмём.
И вот это сожмём.
Ну вот это сожмём, ну типа и это сажалось.
А потом вот это сожмём.
Вот.
Ну вот.
Ещё сожмём вот это, и вот, ой у нас две вершины
осталось.
Так, утверждение, этот разрез будем считать минимальным.
Чего?
Вот.
Нет, сколько раз запускаться?
В-2.
В-2 сжатия надо запустить, чтобы получилось две метовершины.
Вот и говорим, этот разрез минимальный.
Нет, в плане, сколько раз вот это...
Так, ну вот давайте теперь анализировать.
В чем прикол?
Да, у нас же вероятность алгоритма, поэтому это означает,
что мы верим, что с хорошей вероятностью нам повезет,
и это будет реально минимальный разрез.
Давайте оценим, с какой.
Как нам это оценить?
Давайте подумаем.
Теперь давайте предположим, что у нас V-вершин, E-рёбер,
и есть ответ ans.
Не надо просто оценить мотожидание числа Греберна,
и ты найдешь один?
Нет, мы не будем оценивать мотожидание.
Это у нас будет алгоритм, на самом деле, из Монте-Карла.
То есть он всегда быстро работает.
Видите, тут мотожидания во времени нет.
Но нам будет интересно, с какой вероятностью он ошибется.
Или, наоборот, сработает.
А теперь давайте подумаем.
С какой вероятностью, например, на самом первом шаге
он все-таки тяпнет ребро из разреза?
Да, ans делить на E.
Я утверждаю, что ans поделить на E можно сверху оценить.
Потому что, чтобы ответ был ans, ребер должно быть достаточно много.
Заметьте маленькое приятное свойство.
Я утверждаю, что степень каждой вершины хотя бы ans.
Потому что есть такие разрезы, одна вершина и все остальное.
Величина этого разреза, соответственно, степень вершины.
Следовательно, степень вершин, как минимум ans.
Вывод очень простой.
Тогда получается, что такое сумма степеней всех вершин?
Вот по всем вершинам V.
С одной стороны, она равна 2E.
Потому что каждый ребро мы учили два раза.
С другой стороны, она больше либо равна, чем ans умножить на V.
Вывод очень простой.
ans делить на E меньше либо равен, чем 2 делить на V.
Внезапно.
Не такая большая вероятность, кстати, да?
Получается, да.
У нас там, допустим, тысячи вершин.
Пожалуйста, это все равно 1,5.
Мы на первом шаге ошибемся.
Но мы кроты дотошные.
Мы начинаем считать, а сколько это зерна в год.
Ну, давайте подумаем, с какой вероятностью мы не ошибемся.
Мы не ошибемся.
Нет, мы не ошибемся на первом шаге с вероятностью 1-2 делить на V.
Но проблема в том, что надо бы еще мало не ошибиться на первом шаге.
Надо не ошибиться на остальных шагах.
Ну, и теперь надо оценить, сколько у нас стянулось ребра.
Нет, даже не надо.
Нет, зачем?
Не надо.
У нас сколько-то ребр стянулось, но оценка-то будет та же.
То есть ответ поделить, насколько ребр там останется после первого шага,
будет не превосходить 2 поделить, насколько там вершин.
То есть уже E-1.
Мы все кратные ребра удаляем?
Нет.
Почему нет?
Они важны.
Потому что каждое кратное ребро дает вклад 1 в разрез.
Поэтому кратные ребра мы не удаляем.
Но правда, если у вас появлялась петля, то да, ее мы, конечно, игнорируем.
Но это неважно.
Эта оценка устроена так, что нам абсолютно неважно,
сжались ребра на предыдущих шагах или нет.
В предположении, что мы не задели ответ, ответ не поменялся.
О, и там, типа, шансы E.
Это запрет.
Нет, погодите.
Нет, нет, нет, нет, нет.
Нет, ни в коем случае.
Потому что, смотрите, вероятность того, что на первом шаге,
вероятность того, что мы не ошиблись, вот такая.
Чтобы нет.
А теперь, смотри.
Дело в том, что чтобы мы не ошиблись на втором шаге,
там должно быть 2 на V-1.
А зачем?
Это будет более точная оценка.
Так хватает чего?
Хватает чего?
Просто если в степени в этом возрасте это уже меньше,
чем 1,9E2.
Это уже мало.
Нет, это не 1,9.
Нет, если это оценить как...
Ну, давай так.
Что считать проще, это еще вопрос.
Ну да.
Значит, смотрите.
Фишка такая.
Нет, с форолером так.
Что за бредятина?
Почему бредятина?
Нет, ну опять-таки можно, конечно, сказать,
вот, видимо, Ваня нам предлагает сказать,
что это больше либо равно, чем 1-2 делить на V в степени V-2, да?
Это же абсолютно неправда.
Очевидно, нас одна треть меньше, чем V-2.
Мы не с той стороны оцениваем.
Да, ну зачем нам эту штуку оценивать сверху?
Мы ее должны снизу оценивать.
Потому что это типа вера...
Мы оцениваем это снизу.
То есть мы пытаемся оценить,
что вероятность того, что мы не ошиблись,
не меньше, чем вот столько.
Вот так-то это один вариант.
Вот.
Нет, эти мотивы, вот эти мотивы, конечно,
у нас будут проявляться, но не сейчас.
Мы ошибемся гарантированно с вероятностью хотя бы 1-1 делить на E2.
Ну, погодите, спокойно.
Это вероятность того, что мы не ошибаемся.
Но это равно, на самом деле, к чему.
Если вот тут единичку занести в числитель,
то получится вот такая интеллектуальная штука.
Нет, не пам-пам-пам, а шлеп-шлеп.
Вот.
Шлеп-шлеп, шлеп-шлеп, шлеп-шлеп, шлеп-шлеп.
2 поделим.
А ты хотел бы большую вероятность таким алгоритмом получать?
Это же лажа какая-то.
Но не самая плохая вероятность.
Запускаем заходы от раз.
Вот.
Ну, давайте так.
Ну, тогда смотрите.
Получается следующее, смотрите.
Если мы запустим этот алгоритм с вероятостью не менее,
чем вот столько, этот алгоритм даст правильный ответ.
Но тогда, так как этот алгоритм, типа, найдете минимальный разрез,
то есть такая опция «запустите этот алгоритм с раз».
И тогда вероятность, и просто из всех ответов,
которые эти с раз будут предложены, выберем минимальный.
Тогда заметим, что вероятность ошибки,
то есть даже что после с запусков будет немного запусков,
она не будет превосходить 1-2 делить на v квадрат в степени c.
Но тут какая идея?
Давайте скажем, что c равно v квадрат пополам.
Тогда по ошибке от v квадрат пополам приблизительно равно 1 делить на e.
Почему у нас вероятность по ошибке?
Ну, предположим.
Это вероятность того, что мы не ошибемся с раз,
если не меньше 100.
Да, вот эта штука приблизительно 1 делить на e,
потому что мы знаем, что 1-1 делить на x в степени x
при x, стремящемся к бесконечности,
стремится к 1 делить на e.
Это мы вот откуда-то из вашего мотоанализа знаем.
Кстати, откуда оно стремится отдельной песне,
потому что оно там сверху стремится.
Нет, оно монотонно.
1 плюс 1 в степени n монотонно стремится к e снизу.
Снизу?
Минус?
Нет, вот я точно знаю.
1 плюс 1 делить на n степени n, да,
она стремится к e, причем снизу.
Это я знаю.
Можно оценить, откуда стремится эта штука.
Тоже монотонно, очевидно, или нет, или нет.
Да, но вопрос откуда?
Давайте так.
Производная у нее какая?
x линум.
Охреново знает, какая у нее производная.
Это там как-то глинисто очень доказывалось
до всяких преисходных, до вообще всего.
Какая разница, откуда стремится?
Нет, ну так, но пошел бы оценивать адекватно.
Потому что хочется оценивать, что эта штука
всегда не превосходит 1 делить на e, поэтому кайф.
На самом деле, если мы взойдем в n плюс 1 степень,
то это будет уже стремиться к e сверху.
Да? Точно?
Да.
Ну хорошо.
Поэтому там ошибка типа в n плюс 1 и n то и раз.
А, ну здесь-то да.
Вот, ну хорошо.
Ну вопрос здесь.
Ну здесь хорошо, приблизительно 1 делить на e.
Поэтому идея такая.
То есть приятно, что 1 делить на e это какая-то константа.
Да, там 1 делить на 2, то есть порядка получится
что-то типа между 0,3 и 0,4, но соответственно.
Ну и тогда возникает идея, что давайте запускать
в квадрат пополам на, там, я не знаю, d раз.
И получать вероятность ошибки не более, чем 1 делить на e
в степени d.
То есть получается, да, за константу, ну то есть
за константу d, ну это нот.
То есть получается, если считать d константой,
то как бы получается, вот асимптотика алгоритма
получается v квадрат умножить на, а за сколько мы
1 запуск вообще делаем?
А за сколько мы, кстати, делаем?
Да.
1 запуск мы делаем за многое.
За e на альфа ве.
Ну, ну это не...
Запуск это все сжатие.
Конечно.
Эх, не получил соединиться.
Ну давайте так.
Нет, обычно это оценивают этот запуск за o от v квадрат.
Ну потому что смотрите, рандомное, но дело в том,
что если я знаю степень каждой вершины, то я могу
в принципе рандомски делиться o от v.
Ну логично, да?
Или не логично?
Какой рандом?
Ну, который генерит нам ребро.
Так, ребят, ну так давайте просто просыпаемся,
это слишком простая идея.
Не понимаю, что вы хотите...
В смысле рандомного?
За вот одного почему нельзя?
Ну, точнее, нам нужно найти рандомное ребро,
его реально найти.
Что значит найти?
Прием любое, но рандомное.
Ну как конкретно?
Но нет, фишка в том, что мы не хотим хранить
прям полный список всех ребер, список всех e ребер.
Вот.
То есть хочется это, ну вот.
То есть на самом деле просто, ну идея будет в том,
что, во-первых, давайте кратные ребра сжимать,
ну в том плане, что хранить, да, хранить кратность.
Ну, можно пошафлить все ребра,
потом поплечить с конца,
и, типа, если оно петля, то, ну,
забьем на него, иначе сожмем.
А...
Ну, можно под...
Ну, кстати, нет, ну как сказать,
тупому алгоритмизовому квадрату предлагается
ДСУ даже не поддерживать.
Хотя, хотя, да, ваша интерпретация,
конечно, даже да.
Ну, в принципе, да, хорошая идея, да, давайте просто
пошафлим ей ребра и будем их просто по очереди
забрали, действительно.
Почему бы нет?
Да.
Да, такое можно.
Вот, да, это нам облечит, и тогда это вообще будет работать
едва ли не за...
Прям Е на альфа.
А, ну да.
Ну вот.
Ну да.
Да, но правда тут проблема, что Е это все равно в квадрат
в этом алгоритме считается почему-то.
Ну да.
Ну вот.
А, ну тогда можно избавиться от Алина.
Да, тогда можно избавиться.
Ну да.
И получится, да.
Но в результате получается, да.
Тогда, да, и в результате получается, что все работает
за О от В в четвертый.
То есть у нас есть алгоритм, который будет за О от В в четвертый,
имеет константную вероятность ошибки.
Ну там, увеличением константы можем сделать эту ошибку
сколь угодно малой.
Вот.
Не нравится.
А, чуть-чуть.
Сейчас.
Я тебе больше скажу.
Шторм вагнер за ВКУ поработал.
Ну да, и предпоток тоже.
Да.
Ну, правда, без одной вершины вагнический проще сидеть.
Ну там так, нет, не совсем.
Просто предпоток, он как бы искал бы разрез
между двумя вершинами фиксированными.
Так что там тоже будет читать.
В таком виде могла бы четвертой степень вылезать.
Если бы там начали там какую-нибудь дерево гомориху
строить и так далее.
Вот.
Да, ну согласен, ВФ четвертая многовато.
Хотя, конечно, уже это достаточно простыми методами
получили ВФ четвертые, то есть мы не изобретали
никаких потоков там этих вот,
что называется, какие-нибудь гоморихов,
там тихих и не очень.
Вот.
Но тут возникает вообще не нот.
Но есть про нот.
Значит, это называется алгоритм кархера.
Это был алгоритм кархера.
Каргер.
Нет, не каркер, а...
Да-да-да.
Нет, это понятно.
В книжке магиста рассеянных наук, да.
Оказывается, там дикари принадлежали
до неизвестного мне племени буль-буль.
И они очень любили математику, особенно алгебру.
Но алгебра у них была какая-то дикая абсолютно,
в общем, буль-булевая алгебра.
Нет, там кошмар.
Ну, представьте, вот логично алгебра,
вот А плюс А сколько будет?
Ну, как бы, вижу, понятно, что 2А.
А у этих буль-бульах А плюс А так и остается А.
Ну, что за бред?
Да, я думал, у них ссоры ноль будет.
Не, у них буля ноль.
Не, у них дезюнкция просто была.
То есть, я им вежливо говорю,
что вы решите против обычной логики.
Они говорят, а нам как ослодикой подсказывает,
что А плюс А равно А.
Так, ну ладно.
С какой-то точки зрения нет.
Да.
Нет, просто она буль-булевая.
Да, буль-бульки страшно на меня обидели.
Ладно, если не читали эту книжку, почитайте.
Получите все еще удовольствие.
Хоть она, конечно, и больше там для школьничков,
типа там пятого класса, на самом деле.
Не, ну сейчас вам.
Ну, я говорю, может это...
Ну, я не знаю.
Я просто где-то во втором-третем классе это читал.
Мне очень нравилось.
Ну да, там...
Нет, это очаровать.
Это очаровать на книжке.
Да, ну вашу...
Ну, а что, картинка называется?
А что вы читали во втором классе, да?
Ладно.
Так, смотрите.
А теперь вас приветствует алгоритм Каргерштайна.
Значит, какие тут еще веселые технологии могут быть
неожиданно применены?
А могут применить неожиданные читерские технологии?
Они говорят...
Вот если я буду прям этот алгоритм,
прям вот это вот сжатие делать сначала до конца,
то у меня получится вот такая вероятность.
Два делительного квадрата, да?
Да.
А теперь мне сказать следующий вопрос.
А если я сделаю чуть меньше шагов,
то вероятность ошибки будет меньше, правда?
Сильно меньше.
Да.
А потом что рвать?
Ну вот.
Нет, нет, нет.
Нет, нет, нет.
Все еще круче.
Смотрите.
У меня идея такая.
Давайте я сделаю так, чтобы у меня Т было приблизительно...
Не буду сейчас там прям идеально подгонять.
Корень...
Сколько там надо-то?
Сейчас.
Сейчас.
Сколько там должно быть?
Давайте попробуем.
Ладно.
Вот давайте я попробую вот как сделать.
Давайте у меня Т будет приблизительно корень из двух.
Корень из двух?
Ладно, корень из два, хорошо.
А, нет, наврал, наврал.
Вот, нет, там был корень из двух, просто я не вспомнил, где...
Извините.
В делить до корень из двух.
Вот, другое дело.
Да, просто радикал просто не там.
Так вот, в чем прикол?
Допустим, смотрите, как я буду теперь работать.
Я хочу сделать так, чтобы у меня вместо В-вершин стало Т-вершин.
Вот я буду делать вот эти сжатия до тех пор, пока у меня не останется Т-вершин.
С какой вероятностью я выживу?
Да?
Сколько останется?
Ну ладно, делить на Е.
С какой вероятностью мы все еще не заденем...
Да, миноразрез.
По какой-то степени.
Нет, нет, нет.
Да, Господи, еще у вас все нет.
Нет, ну давайте так.
Тогда это вероятность того, что мы выживем.
Оно не меньше, чем 1-2 делить на В, 1-2 делить на В-1 и так далее.
И дальше будет 1-2 делить на Т-1.
Это равно тем же методам с шлеп-шлепами.
Оно равно Т на Т-1 делить на В на В-1.
Ну вот, не это.
Это приблизительно 1-2.
Я объясняю, зачем я говорил, что это Е.
Я просто заменяю все слагаемые на 1-2 делить на Т-1.
И возвожу в степень Т-1, получаю 1 Е.
Да, но моя оценка точнее.
Да, у вас получше.
Да, у меня, конечно, получше.
Интересно, я могу оценить, что это больше либо ровно, чем 1-2?
Если подогнать, то это может быть.
Ну я могу подогнать, а может так можно.
Сева.
Или не Сева.
Филипп, да.
Да господи, хотя бы Денис.
Кошмар, господи.
Это отдельная песня, когда представители команды «Ёлки-палки» превратились в привидении.
Но не суть.
Ну да.
Ну мало ли.
Когда-то превратились.
Когда-то они тут были.
Они держались вот, собственно.
Они сдались, собственно, когда они сдались.
Вот ровно когда мы это сделали.
Нет.
Не, когда мы голели Сейферса.
Ну ничего.
Не, ну почему.
Не, ну почему.
Ага.
Или как только им сказали, как начисляются бонусы, они поняли, что им лучить точно
и ничего не надо.
Ну да.
Ну да.
Ну да.
Ну да.
Ну ничего не надо.
Остальное они типа и так знают.
Хотя я не знаю.
Денис Пустафин, между прочим, за прошлый семестр Хор 6 имеет, если что.
Бонусы складываются.
Очень-очень просто он поленился учить на ОТО.
Еще три перездачи бонусом.
Хор 6 и три перездачи бонусом.
У Дениса?
Да.
По плюсам технологиям программирования и питона.
Потому что он поленился учить.
Ну нет, вот это ты зря смеешься на самом деле.
Я не смеюсь, это правда.
Да, но смеешься ты над этим зря.
Нет, я не оспариваю правдивость этого утверждения, но скажем так, юмористичность этой реакции кажется мне сомнительной.
Ну просто как минимум потому, что да.
К сожалению, иногда бывает, что математически ты шаришь, а вот выучить именно программические предметы тебе сложно.
Просто сам такой.
А если вернуться на позитивную тему, то бонусы складываются или вычитаются?
Вычитаются, Оля!
Перемножаются.
Нет, из 1,8 до 4 выбирается максимум.
А может, умножается?
Нет.
Нет, размечтались.
1 на 1.
Ладно.
Так что веримся к еще более позитивной теме.
Значит, t-1, а вы минусы?
R. Рыжая.
R. Выжившая.
Это не рыжая.
Пузик не в той стороне, откуда тут R?
Вот это еще может быть R.
Это не рыжая, а коричневый цвет.
Может быть, конечно, какой-нибудь гриб рыжик может быть такого цвета, конечно.
Давайте отучим туда.
Нет, конец семестра.
Ну что делать?
Вот.
Значит, t-1.
Ну ладно, приблизительно t-2.
В случае чего, можно подогнать, выделить на корень из 2 плюс 1,
округленные в правильную сторону, не будем заморачиваться.
В общем, если хотите, в конспекте написано точно.
Но это сейчас не принципиально.
Так вот, оказывается, уменьшив количество вершин в корень из 2 раз,
оказывается, мы с вероятностью 1 и 2 будем еще жить.
Так вот, а дальше у меня неожиданная идея.
Слушайте, я же с большой вероятностью жив?
Подождите, это не вы живы, а одно ребро из ответа живы?
Нет, не одно ребро, а просто мы ни одно ребро из ответа еще не сжали.
Нет, что такое выжить?
В данном случае я говорю, что мы выжили, если мы делали рандомное сжатие,
и ни одно ребро из ответа не сжалось.
Вот, то есть ответ у текущего графа все еще правильный.
А теперь у меня идея.
Так, если я жив с такой хорошей вероятностью,
так знаете что?
А давайте я поверю, что я жив,
и буду делать несколько запусков именно из этой точки.
Несколько это означает два.
То есть, смотрите, я получил вот это,
я сжал граф до выделить на корень из двух вершин,
и у меня получился какой-то граф.
Так вот, я делаю два рандомных запуска от этого графа.
И выберу из этих ответов, естественно, максимальный.
Понятно, да?
То есть, смотрите, раньше у меня рикурсивные зап unreasonable были устроены так.
Вот я иду по цепочке в, в иду тупо,
я тупой, тупой, тупой, иду один,
потом беру еще один запуск и еще цепочка, цепочка, цепочка, цепочка, цепочка...
и тут цепочек цепочек цепочек тут тут к товарищу каргеру приходит товарищ
штайн и говорит а давай делать их усиленные запуски вот так вот выделить
наклоне из двух а теперь вот расходимся тут в этой точке еще расходимся
че че че че че но да но это не заразветление там просто у нас был
алгоритм поиска миностова с атомик хиппом который выяснял что он там за две за две
вам находит миностов да но это не то потому что алгоритм то есть алгоритм поля то есть
рекурсивный запуск то есть сам алгоритм звучит так ну пока мне один запуск то есть я беру граф
там сжимаю его рандомно до размера выделить наклоне не с двух а дальше от этого графа
запускать рекурсивно два раз то есть создают две копии и откуда и каждую копию пытались
сжать рекурсивно там запускать рекурсивно из результата выжим максимальный вот
а теперь давайте посмотрим смотрите давайте кое-что поймем то есть давайте подумаем с
какой вероятностью из вот этой вот рекурсии мы выковырием правильный ответ
ну давайте так ну давайте так вот давайте я назову так то есть это вот вероятность выжить
на переходе от в до в том до т да а теперь вероятность выжить от в что вот этот алгоритм
от в найдет правильный ответ да он какой он больше либо равен чем что чем одна вторая
потому что сначала одна вторая что мы вообще на первом вот этом запуске не выжили наоборот
выжили и надо это еще умножить на то что на один минус вероятность того что мы там дальше не
пофалились а мы дальше не пофалились с вероятностью что-то типа 1 минус п выжившие
от выделить такой не с двух но в квадрате вот
потому что но потому что скажет так вероятность того что мы там дальше выжили это 1 минус вероятность
того что мы там погибли а мы погибли как бы чтоб погибнуть надо погибнуть и в этом
запуске и в этом запуске поэтому мы берем вероятность того что мы там погибли это
один vere тость того что мы там выжили и возводим ее в квадрат вот или но вот ну или знаете например
Если я скажу, что v равно кое-нибудь из двух в степени n, то тогда
можно написать такое, что p от k больше либо равно 1
вторая на 1 минус 1 минус p от k минус 1 в квадрате.
Теперь вот как вы думаете, как оценить p от k, особенно
с учетом того, что, конечно же, p от 0 равно, естественно,
1.
Но, впрочем, на самом деле вероятность выжить, а, хотя
нет, вероятность выжить.
Теперь внимание, вопрос, как оценить p от k.
Давайте сделаем замену p равно 1 минус q, чтобы с этим
было...
Думаешь...
Так, и че, думаешь, помочь?
Чтобы...
Хорошо.
Ладно, q от k, давай q от k присвоить 1 минус p от k, пожалуйста,
и че.
Поехали.
1 вторая на 1 минус q от k минус 1 в квадрате.
И че, и че, помогает?
Так, хорошо.
Тогда получается, что q от k должно быть...
То есть q от k меньше либо равно чего?
Да, что-то типа 1 вторая плюс 1 вторая q от k минус 1 у квадрате.
Так.
Супер.
Квадратное уравнение.
Теперь можно...
Стоп, это разница квадрата вообще.
Где?
Теперь можно избавиться от коэффициента 1 вторая.
Каким образом?
Оно с квадратом.
Что?
А, что?
Что?
А, предлагать теперь...
А, то есть...
В смысле, можно...
Коэффициент...
А тя-а-а...
Этого я и не знаю.
Сказать, что QKT это RKT на один бодилитный корень из двух в степени K.
Чего?
А, типа дальше пораскрывать что ли?
Ну, давайте напишем, что 2 QKT меньше уровня, чем 1, плюс QKT минус 1 в квадрате.
Нет, а там... сейчас чего еще напишем?
Ну, там на 2 все должно быть.
Да, но тут квадрат.
Потому что так как бы...
Нет, не внутри квадрата, там ножа снаружи.
Я просто дроби не хочу.
Можно дальше раскрывать, что это 1 вторая плюс 1 вторая от QKT 2, потом от QKT минус 3.
Да, но там...
Только так как тут квадрат, там в этих скобках такая гадость начнется.
Что, знаете?
Давайте на 2 все домножим.
Можно QKT?
Это просто будет какая-то...
Давайте домножим на 2 и угадаем функцию.
Можно методом неполной индукции.
Да? Это что значит?
Ну, это в начальных значениях посмотреть, а потом...
Чтобы угадать.
Нет, ладно. Найдите.
Можете написать 2Q от K меньше или равно чем 1 плюс Q от K минус 1 в квадрате?
Чего?
2Q от K минус 1 меньше либо равно Q от K минус 1 в квадрате.
Это я могу написать.
Зачем вам еще одни скобочки?
Не знаю.
На это легче смотреть.
Нет.
Нет, это так не поможет.
Понимаете, потому что P все-таки будет относительно маленькой.
Поэтому тут лучше оценивать через P.
Q это одно и то же.
Да, но дело в том, что P будет порядка достаточно мелкого.
То есть Q придется доказывать, что оно меньше либо равно, чем 1 минус что-то мелкое.
Поэтому просто не особо поможет.
То есть вы же понимаете, что вероятность того, что мы выживем, по-любому меньше 1 и 2.
Просто потому что мы, если я сюда вторая, пофейлимся уже тут.
Поэтому на самом деле лучше смотреть куда-то вот сюда.
Утверждение на самом деле будет примерно такое.
P от K больше либо равно должно быть что-то типа 1 делить на лог K.
Не, не на лог.
1 делить на K.
Ну, приблизительно такой ответ должен быть.
Для сокращения времени давайте не будем пытаться это угадывать, а лучше давайте, как я сразу скажу.
Откуда это, правда?
Ты прямо в уме подставил?
Ну ладно, тогда попробуем сделать то, что...
Ну давайте так.
Ну тут на самом деле получается так.
Если аккуратно раскрыть скобки, то что получится?
Получится на самом деле P от K минус 1, минус...
Ну вот, значит одна вторая P от K минус 1 в квадрате.
Я тупо раскрыл вот эти скобки.
Теперь заметим, что, конечно, если у нас есть оценка по индукции, что это больше P от K минус 1, не очень понятно, как оценивать.
Потому что можно это оценить снизу, а это надо как-то сверху оценивать.
Но не беда, как говорится, будешь можем нарисовать график этой функции.
Даже вообразить себе, что графиком этой функции является параболы ветви, которые направлены куда-то вниз.
И более того, мы еще и можем угадать, где вершина.
Где вершина?
Да, вершина просто в...
Ну-ка давайте.
Смотри, минус 1 делить на 2a.
То есть 1 делить на минус 1, то есть 1.
То есть вывод, вершина тут в единицу.
То есть, на отрезке от 0 до 1 эта функция тупо возрастает.
Поэтому, если мы знаем оценку на P от K минус 1, то на этом отрезке это будет больше либо равно, чем если мы вместо P от K минус 1 подставим нижнюю оценку ныне.
То есть так и пишем.
1 делить на K минус 1.
Минус 1 вторая на 1 делить на K минус 1 в квадрате.
А это равно чему?
Ну давайте тут попробуем это привести.
2K минус 1 в квадрате.
Тут получается...
Ну почему?
Ну какой-то 2K минус 3 получится.
Нет, осталось только сравнить это с 1 делить на K.
Но на самом деле сравнить вот это 1 делить на K, это то же самое, что сравнить 1 делить на K минус 1 с 1 делить на 2 на K минус 1 в квадрате.
Вот.
Ну а к чему я?
Ну вот.
К чему?
Ну скажи так.
Я утверждаю, что это вот больше либо равно 1 делить на K.
Это эквивалентно вот этому.
Да, это перенес, это перенес, привел к общему знаменателю.
Это равно 1 делить на K минус 1, минус 1 делить на K.
В явном виде.
Я не понимаю откуда у вас разница в два раза возникла.
В смысле в два раза?
Ну вон справа у вас в два раза меньше, чем слева.
Чего?
Тут квадрат.
Ну да.
А тут?
Еще.
У вас же совсем рядом было.
Ну я знаю, но одна вторая у меня возникла честно отсюда.
Я не знаю.
Ну ладно.
Ну вот.
Здесь заметим, что тут 2 K минус, если на K минус 1 сократить, то тут останется, тут останется K, тут 2 на K минус 1.
Вот.
Но заметим, что при достаточно больших K вот это заведомо больше.
Что такое достаточно большие?
Ну это конечно когда K равно там хотя бы сколько?
Ну хотя бы два.
Вот.
При хотя бы два это уже работает.
Вот.
Ну раз это больше, значит тогда тут с этой стороны больше либо равно и получается доказано.
Вот.
Вот.
Так что вот такая вот красота получается.
То есть какой из этого вывод?
То есть получается, что вероятность...
То есть получается, что если у нас глубина получилась в итоге K, то вероятность того, что мы выжили хотя бы один делить на K.
То есть это от 1 делить на...
Сейчас мы там получили, что это верно при K больше либо равно 2.
Ну да.
Ну можно сказать, что при K равно 1 это константа, и мы там выживаем с вероятностью 1.
Потому что если у нас количество вершин осталось не более чем константа, мы просто уже любым перебором решаем заразаище за константу.
Вот.
Ну там правда для этого...
То есть если K равно 1, значит вершин...
Корень из двух?
Приблизительно. То есть там одна, две вершины остались.
Так что в общем-то не паримся уже.
Там да.
Вот.
Так что здесь все в порядке.
Вот.
Но тогда получается, смотрите.
То есть видите, что мы научились выживать с вероятностью 1 делить на...
Ну такой.
1 делить на два двоичных логарифма ВЕКТИ.
Это получается.
Почему два?
Потому что у нас тут логарифм по основанию корень из двух, а он в два раза больше.
Вот.
Но тогда получается, что...
Вот.
Но тогда получается, что так как мы тут за В квадрат делаем запуск...
Нет, хотя нет.
А за какую этим точку мы делаем один запуск?
Давайте теперь посчитаем.
Ну за Е так же можно делать.
Ну в принципе да.
За В квадрат?
Да. Можно в принципе это...
Ну вот.
Ну не за Е, а даже за В квадрат.
За Я.
Ну как бы вот это...
Последователь сжати мы умеем делать за В квадрат.
Да?
Сейчас у нас же...
То есть нам же нужно просто посчитать, сколько будет ликшин вот в таком вот дереве.
Сколько будет ликшин.
И умножить это все на В квадрат.
Ну не совсем.
Ну там еще надо...
Ну там не совсем.
Там же надо считать, за сколько мы будем вот эти сжатия делать.
В смысле мы же можем сделать сжатия и потом такое делать не с нуля, а откатиться на этом назад.
И потирать.
Поэтому нам нужно посчитать просто размер этого дерева.
Ну там правда...
Ну желательно правда для рекурсивного запуска еще...
Может быть вот перед этими двумя рекурсивными запусками еще пробежаться по ребраму и выкинуть лишние.
Ну просто тут эти лишние прибегания тогда будут учитываться.
Ну что?
В общем, и ТО это кажется в...
Подъедено по минимум двух.
Нет он...
Ну давайте так.
Ну давайте так аккуратненько.
Ну хотя нет тут...
Нет.
Зачем мы это делаем?
Ну просто скажем так.
Честно скажу.
Вот тут в оригинальном алгоритме вот этих вот рентом шаффлов на ей ребер и с амортизацией этого нет.
Там подразумевается, что как бы вы у каждой вершины есть степень.
Зная эти степени вы можете за О от В загенерить рандомное ребро.
Точнее за О от В сгенерите...
То есть должны сгенерить в какой вершине оно будет и какой порядковый номер оно в нем будет.
То есть можете сначала сгенерить в какой вершине оно будет,
а потом перебрать ребра в вершине и выбрать из них рандомное.
Да, с учетом весов соответственно.
То есть это как бы делать для того, чтобы вот этих вот там амортизированных сжатей у вас не было.
Вот понятно, да?
Кажется, что в В куб получается...
Ну почему?
Ну давай так, смотри.
Одна генерация рандома это О от В.
Одно сжатие...
Ну, кстати, тоже О от В.
Потому что мы прям в честную берем две вершины и там перекопируем ребра.
То есть одно сжатие это О от В.
Тогда получается работает это у нас так.
То есть получается асимпточка.
То есть Т от В равно...
Значит В...
Значит минус В делить на корень из двух.
Значит умножить на В.
Это мы делаем вот эти сжатия.
Плюс два Т от В делить на корень из двух.
Так, ну это что такое?
Ну короче говоря, это О от В квадрат.
Плюс два Т от В делить...
Нет, вообще нет.
Хотя...
Нет, смотри.
Давай раскроем это.
Давай так, получается В квадрат.
Плюс два на...
Плюс два, если В квадрат пополам.
Плюс четыре там...
В квадрат на четыре.
Там на четыре плюс и так далее.
Короче говоря, это В квадрат лог В.
Вообще неплохо.
То есть запуск был В квадрат, стал В квадрат лог В.
Но зато он и дает нам правильный ответ.
С вероятностью один делить на лог В.
То есть получается, чтобы достичь константной ошибки,
получается у нас совсем точка.
О от В квадрат лог В квадрат В.
Что не так?
Ну по-моему...
Шторвагер В.
Ну почему?
Не, ну да, не вылетел.
Там было Z плюс V квадрат.
Нет, извиняюсь.
Стоп, стоп, стоп.
Нет, Шторвагер был там сколько?
А, ну да, там V Z плюс...
Ну да.
Ну вот.
Да.
Правда...
Нет, смотрите.
Нет, там правда есть маленькая подлянка, конечно.
Да, Шторвагер работает как бы с произвольными весами.
А у нас тут как бы они не отрицают.
Правда, с другой стороны, заметим, что...
Правда, теперь заметим маленькую прятную весть.
Да, заметим, что Шторвагер точно работает, если веса целые.
Потому что мы же можем делать буквально...
Мы же мысленно можем считать, что у нас нет у ребра веса миллиард,
мы можем считать, что там есть миллиард кратных ребр, правда?
Ну вот.
Ну, явно в виде моих не хорить, а генерить рандом тот же самый за ОАТВ.
И между прочим, это как раз...
И тогда получается, что, по крайней мере, уже, очевидно, для целых мы, оказывается, умеем.
То есть, оказывается, на самом деле, Каргерштайн прекрасно работает для целых произвольных весов.
Ну и для рациональных.
С другой стороны, да, он так работает и для рациональных.
А не рациональных?
Ну да, предельный переход радость.
Да, поздравляю.
То есть, на самом деле, ограничение на то, что у нас только там какие-то ребра,
на самом деле, не такое уж изначительное.
Изначительное.
А что не так?
Почему мы не изучали рандом в 10 секунд?
Ой.
Ну потому, честно, потому, знаете, вот просто у этого рандома такая маленькая проблема.
Потому что, знаете, я его не так много знаю.
То есть, судя по текущему темпу, вот все, что я на тему знаю, вот, видимо, мы сегодня и обсудим.
Потому что, нет, у меня просто осталось, на самом деле, если я так ничего не пропустил, осталось две большие задачи.
Задачи.
Задачи.
Найти миностов за ОАПЕ и задача с гордым названием свидетели булева перемножения матриц.
Нет, это свидетели.
Так это почти запрещенные на территории России.
Да не, господи.
Признанные экстремистами.
Не-не-не, алгоритмы слова еще никто не признавал.
Экстремистами, да.
Свидеть.
Нет, хотя иногда на прошлом свидетели там отдельный юмор, конечно, потому что там...
Ну, мало нет.
Это наоборот.
Этого у нас везде есть, знаете.
Но там просто иногда экзотично звучит.
То есть, на самом деле, это как бы все интересно, когда это начинают рекламировать и вставлять в телевидение.
Вот видели, когда я еду в автобусе, а там на остановке висит огромная реклама.
Значит, там телеканала с пастом, утренние передачи.
И написано просто гениальное.
Там называется «Господь Бог с нами по будням.
Здесь семи до десяти».
Ну вот, что это такое вообще?
Все на полном серьезе.
На остановке так написано.
Но выяснилось, это у нас еще ничего.
Потому что, когда там случайно в Турции, там тоже американский какой-то канал, видимо, тоже каких-то религиозных деятелей.
И там есть просто передачи, которые на полном серьезе.
Так и называется.
«Иисус Христос приходит нам».
Там называется «Сдори, Тительман».
Сдори, Тительман.
Ну, там девушка какая-то там ведет.
Называется, но там...
Ну, знаете, когда звучит, так звучит, «Господь приходит нам».
Там называется Снэнси Ивановой.
Думаю, а почему он именно с ней приходит?
Почему?
То есть как-то звучит, это очень странно.
Но при этом в студии никакого Иисуса, естественно, нету.
Да, а ведущая есть.
Да, ну, то есть просто экзотично звучит.
То есть я как бы ничего не могу плохого сказать про то, что она там конкретно говорит.
Но с другой стороны, да, конечно, когда все эти вещи начинают пихать в современный маркетинг
или в современной телевидении, звучит экзотично.
Так, ладно.
Ну что, будем делать перерыв?
Ой, да.
Сейчас, короче говоря, вообще утверждается, что кратные ребра либо все одновременно лежат,
либо все одновременно не лежат, так?
Да, и что?
Ну, все еще что-то не так.
Вот.
Сейчас.
Сейчас.
Я не понимаю, почему они нас не путают в кратные ребра?
А почему они нас должны путать?
Просто если какое-то ребро жирно-кратное, значит, оно с большой вероятностью А не входит в разрез,
Б будет выбрано и сжато.
В чем проблема?
Почему не входит в разрез?
Ну, потому что мы же минимальный разрез ищем.
Поэтому очень жирно-кратное ребро, на плюс бесконечность, скорее всего, в этот разрез не входит.
Да, кстати, со временем даже вообще возникает интересный факт.
Минимальный разрез в сжатом графе, он даже может быть больше, чем исходный.
Ну, то есть разрезы не уменьшаются.
Такие разрезы остаются теми же, просто мы некоторые из них просто уже теперь игнорируем в силу сжата хорошим.
Ну, короче, меня немножко напрягает ситуация, а я у нас ровно на одной из ребер Рома Ба навешана куча разных ребр.
И что?
Ну, значит, с большой вероятностью просто мы...
Ну, во-первых, разрез равен 2 и через эти...
И он через... И там несколько таких разрезов, но ни один из них через кратные ребра не проходит.
Я умею 3, ну ладно.
А, ну окей, да.
Вот.
Значит, соответственно, но тогда просто большой вероятностью придет следующее.
В качестве выбранного ребра выберется ребро просто одной из этих кратных, и эти две вершины будут торжественно сжаты, и эти ребра ликвидируются.
Останется треугольник, из которого, собственно, у тебя эти три разреза, кстати, выплывут.
Вот.
Сейчас.
Ладно, я вроде...
Ну, короче, у нас не может быть ситуации, что мы пытаемся сжать не ребро ответа, но вместе с ним случайно сжимаем и ребро ответа.
А, нет, конечно.
Вот. Именно из-за того, что все кратные ребра либо одновременно лежат, либо одновременно не лежат.
Ну, типа.
Ну, то есть, короче, если бы такого не было, то мы бы могли условно...
Ну да, понятно. То есть, мы бы могли пытаться сжать не ребро ответа, но вместе с ним случайно сжать и ребро ответа тоже.
Ну да.
Вот. Но это тоже не получится, и поэтому оценка работает.
Не, ну оценка, не в чем. Оценка, ну оценка вообще, она была просто предельно тупая.
Она вообще ни на что на это не смотрела.
Она просто сказала, мы выбираем...
Потому что мы выбираем ребро, которое сжать, но когда мы его сжимаем, мы вместе с ним сжимаем еще какие-то ребра.
И нам важно, что если мы выбрали ребро, которое не лежит, то он без разреза не сожмется.
А, ну в этом смысле, да, да, да.
Ну да.
Хорошо, да.
Да, ладно.
Идем дальше. Сейчас будет просто одна из, просто два линии, просто жемчужины этой коллекции.
Сейчас мы дадим, да.
Какой там? Миностоп за линию.
Мы же мечтали об этом.
Ну там, детерминированная наука, возможно, об этом даже...
То есть, возможно, об этом уже даже не мечтает, потому что есть, потому что там есть...
Потому что возможно выяснито, что оптимальный алгоритм работает все-таки дольше, чем за O от V.
Ну хотя бы даже от V плюс E, конечно.
В смысле O от E, да.
Но это мы обсуждали, не будем сейчас повторяться.
Но если у нас разрешается мотожидание, то, в общем-то, можно и за E.
Хотя, правда, интересно, что алгоритм, который я скажу, датируется 95-м годам.
Как говорится, авторы Каргер Кляйм Тариян.
Знакомо. Две фамилии вы где-то слышали.
Кляйм?
Ну вот.
А может и клей.
Ну да. Может это даже и он.
А может и нет.
Точно.
Нет. Ну как бы сказать, скажи так, у нас есть метод клей, на который вот это типа Минкоста искал.
Да. Возможно, это скорее всего тот клей.
Это тот клей, который...
Бутылка, то ему есть.
Ну да.
Ну.
Ну клейнов. Ой, ну клейнов много.
Как бы фамилия выглядит так.
Не, бутылка, а да.
Ой.
Я помню даже.
Ладно.
Так вот. Итак, ищем минус 100.
Ну, за счет чего мы его ищем.
Давайте немножко воспомним.
Да, ну давайте так.
Можем для простоты считать, что у нас все ребра попарно различны по весам.
Но помните, для того, чтобы у нас, скажем, вес был один.
Ну или просто, чтобы лишний раз не заморачиваться.
Вот.
Просто смотрите.
Ну вот, то есть на самом деле у нас была такая фишка.
То есть у нас был такой критерий минимальности остова.
Критерий заключался в том, что каждое ребро не из остова должно быть максимумом,
настягиваемым им в цикле. Согласны?
Да.
Причем это условие было очевидно необходимо,
и чуть менее очевидно, достаточно.
Стоп, максимальная остова?
Нет, минимальная остова.
То есть ребро должно быть максимум на стягиваемом цикле, если остов минимальный.
Мы ищем минимальный остов.
Вот.
То есть вот такая у нас красота.
Вот.
То есть это у нас такой критерий.
Но более того, на самом деле нам сейчас поможет вообще очень интересный факт.
На самом деле вообще маленькое такое утверждение.
То есть если существует какой-то лес,
допустим у нас в графе существует в принципе какой-то лес,
какой-нибудь,
относительно которого это ребро тяжелое.
Ну давайте так, давайте введем.
Во-первых, да, есть понятие МСТ, это минимум спаннинг 3.
Но у нас будет иметь место понятие МСФ,
Минимум Спаннинг Форест.
То есть типа если граф не связанный, то мы в каждой компоненте ищем по весовой, это будет МСФ.
И теперь, если у нас есть просто, так сказать, вот этот вот стягивающий лес,
то есть этот вот спаннинг форест,
то тогда оказывается, ну вот,
то у нас оказывается критерий, то этот спаннинг форест он минимальный,
тогда и только тогда, когда любое ребро там самое тяжелое.
Давайте вот введем определение, которое тут помогает.
То есть определение.
Там пусть, допустим, F это какой-то остовный лес,
какой-то не обязательно минимальный.
И у нас есть какое-то просто ребро.
Тогда, значит, внимание, тогда это ребро E,
будем его называть F тяжелым,
если, ну давайте так, E не лежит в F,
и E максимально, нет, на цикле,
на стягиваемом.
Ну вот, ну так напишу.
Иначе, ну так, то есть если вот там ребро либо лежит в F,
либо не максимально,
то есть тогда ребро E будет называться,
ну как несложно догадаться, F легким.
Логично, да?
Вот такая красота.
Маленькое легкое ребро.
Вот, ну опять-таки, ну просто там такая терминология пригодится.
Ну опять-таки, ну, чего?
Я просто так написал, чтобы сказать,
что как бы, чтобы ребро,
что как бы ребра самого остова тоже оказываются там, собственно, легкими.
Так вот, просто в чем фишка?
Максимально на каждом стягиваемом в F цикле?
То есть на всех таких циклах?
В смысле?
В остове, если F заданный остовный лес,
то E стягивает ровно один цикл.
Значит, теперь смотрите.
Мистическое красное утверждение.
Утверждение.
Если E, F легкое относительно...
Каким цветом?
Зачем?
Нет, нет, нет, нет.
Цвет правильный, слово неправильное.
F тяжелое относительно,
относительно,
хотя бы какого-то F,
бы какого-то
остовного леса,
то, что?
Да.
То E не принадлежит
минимальному остову.
Минимальному остовному лесу.
То есть вот жестко.
То есть там не...
То есть просто...
То есть если просто найдется хотя бы какой-нибудь лес,
относительно которого E тяжелое,
то все плохо.
Так, а почему это так-то вообще?
Где заметить?
Ну, в F, конечно, можно.
А глобально?
Так, допустим.
И она не минимальная на своем пути.
Ну, тогда можно добавить вот это вот ребро,
которое меньше, и убрать наше.
Мы все еще получим остовный лес,
и вес у него будет меньше.
Нет, давайте, почему?
Там другой, потому что мы без...
И что? Мы, может, выкинули ребро,
потом добавили ребро, и получилось просто
две компоненты за циклом,
а не остов.
Сейчас, но у нас есть лес,
мы удалили одно ребро
и добавили одно ребро.
При том, что мы потеряли связность.
Ну, а почему мы ее не потеряли?
Что?
Тут утверждение не про...
Сейчас я...
Про все леса.
Если оно E тяжелое, относительно хотя бы какого-то,
то не принадлежит никакому.
Нет, нет, сейчас.
Да, тут поаккуратнее надо, да.
Вот.
Но доказательства, конечно, близко к такому.
То есть доказательства такое.
Пусть у нас действительно E
неожиданно в каком-то остове нарисовалось.
Вот какой-то жил был тут,
остов какой-то...
И вот это E.
Тогда смотрите, какая ситуация.
В этом F у нас E самое тяжелое.
То есть существует какой-то путь
от этой вершины до этой,
без вот этого ребра.
Без вот этого ребра существует путь
по каким-то ребрам,
каждый из которых меньше, чем E.
Ну, теперь заметим следующее.
Если я удалю это ребро из вот этого фиолетового основа,
то дерево распадется на две части.
Теперь давайте пойдем по этим ребрамам.
То есть эти ребра могут быть, кстати, в этом остове
вполне себе быть, могут тут как угодно ходить,
ходить, ходить, ходить, ходить.
Но рано или поздно этот разрез,
путь пересекет, потому что мы должны сюда попасть.
Может быть даже не один раз,
может даже три раза пересекет,
пять раз, семь и так далее.
Но вот найдем вот это вот хоть какой-нибудь ребро,
который эти две компоненты пересекает.
Ну, тогда поздравляю, давайте его добавим в остов
вместо ей, и вот теперь получится меньше.
А все равно можно выкинуть,
потому что мы можем без него обойтись.
Смотрите, давайте так.
Это утверждение в таком виде верно, конечно,
именно в жестком предположении,
что все веса попарно различны.
Идеологический алгоритм будет мыслить так.
Мы тем или иным способом прорядим ребра,
то есть возьмем не все ребра,
а просто какое-то там рандомное подношество.
Из этого рандомного подношества выжмем минус 100,
а потом вернем все старые ребра
и удалим все тяжелые относительно него.
То есть с этой точки зрения мы можем сказать,
что как бы без каждого из таких ребр можно просто обойтись.
То есть оно не нужно.
То есть таким образом мы прорядим ребра,
и на оставшемся графе просто найдем,
то есть еще раз запустимся рекурсивно и найдем минус 100.
То есть получится два рекурсивных запуска.
Чего?
Чего? Не понял.
Ну да.
А мы все равно по одному будем выбирать.
Потому что у нас будет один фиксированный лес,
в котором мы по одному будем убивать ребра.
Так что тут технически никаких проблем нет.
То есть это мы будем делать идеологически.
Да, еще у нас будет черный ящик,
который для нас будет действительно мистически черный ящик.
Да, смотрите, мы не умеем с вами за O от E искать минус 100.
Но мы умеем за O от E его проверять.
Значит, мистическое утверждение.
Существует, ну то есть задача,
дан граф и дан остов в нем.
Минималем ли остов?
Да или нет?
А, вот этому критерию?
Да, вот так вот, но отдельно вопрос как?
Потому что честно говоря, я пытался придумать,
можно ли это сделать за O от E?
Да, можно за O от V.
Можно докопаться, видимо, до Кунти Кирмана.
В конце концов, мы все ил-Сашки умеем за O от единицы искать.
Остается только искать, то есть мы живем в офлайне,
поэтому можно там найти минимум на путях в дереве.
Поэтому там можно как-то ужимать.
К сожалению, я на перемене сидел, пытался придумать,
не упитывается тут какой-нибудь метод 4-х русских,
но мне не удалось.
Утверждается, что есть конкретный алгоритм,
в принципе в конспекте приводится даже точная ссылка.
Что там есть такой товарищ Хагеруп,
собственно у которого там от 2010 года такой алгоритм есть.
2010?
Видимо это какой новый, видимо достаточно простой алгоритм.
Возможно, алгоритм 1995 года предлагал это делать более страшными средствами.
Для Тарьяна это не новость, помните же этот прикол,
что линкат появился за два года до сплей дерева.
Но чтобы упихать лог квадрат за лог, они там сказали,
что давайте просто делать это над нашим супер укуренным деревом,
вот с такими свойствами.
А потом через два года появился сплей дерева,
которая это все схомякала.
Поэтому в стиле Тарьяна сделаем пока так,
а потом может придумываться что-то более красивое.
В принципе логично.
Одно дело как бы победить задачу, а другое дело победить задачу более красиво.
Ну вот.
То есть как бы...
Так вот.
В общем, мы будем считать, у нас есть черный ящик,
который на самом деле...
Более того, этот черный ящик будет за О от В плюс Е,
даже не просто проверять, являются ли остов минимальным да или нет,
а еще и на самом деле просто для каждого ребра проверять,
тяжелое оно или легкое.
Да, это более сильное утверждение.
В общем, короче говоря, если у нас в графе дан какой-нибудь лес,
то мы умеем за О от В плюс Е вытянуть тяжелое относительно его ребра.
Вот, понятно?
Вот.
Да, маленький прикол.
Все в этом мире мелкий прикол.
Вот.
Итак.
Ну и теперь исходя из этого, как мы будем работать.
Работать мы будем следующим образом.
Итак, у нас есть граф.
Как всегда.
Выверт шин, еребер, есть веса.
Ищем минус 100.
Так вот.
Теперь такая весьма неожиданная идея.
То есть алгорнод.
Значит, первое, что мы делаем.
Будем говорить, что у меня есть граф.
Ладно, давайте, Ж штрих.
Это будет равен такой граф на тех же вершинах.
Ребра Е штрих, где Е штрих
это под множество В.
И каждая
там Е
из нот лежит в Е штрих
с вероятностью 1,2.
Вот просто в наглую проредим.
В наглую пьем половину ребра.
Кстати, можно и буквально, в принципе.
Заранее скажу, что есть подозрение, что
скорее всего, если вы выберете
рандомное под множество Е пополам ребер,
тоже будет работать.
Нет.
Нет, есть разница, смотрите.
Здесь как бы в среднем ребер будет Е пополам.
Их мог быть как все, так и ничего.
Вот.
А как бы нота можно попробовать
собственную жестко поставить.
Так вот.
Так вот, идея такая.
Значит, далее построим так.
Значит, F штрих
это будет
там.
Ну, короче, да.
Значит, это будет
найдем, короче, в этой штуке
минус 100.
Вот.
Да.
Видно, да?
Рекурсивно.
Да.
Да, алгоритм будем называть
рандомайст.
Там
MSF от
G.
А это алгоритм Лас-Вегас?
Нет, он Монтакарм, наверное.
Не, Лас-Вегасовский он будет.
Нет, пока он, видите, пока он
Лас-Вегасовский.
Нет, он будет в итоге Лас-Вегасовский.
Да, он с Монтажеданием, да.
Нет, потому что, смотрите, что
вот.
То есть
так вот, значит, просто дальше я сделаю так.
Я возьму просто ребра.
Значит, возьму F штрих, да.
Так вот, теперь я заявлю.
Теперь я скажу, что у меня теперь
ребра E
два штриха.
Нет, эту строчку нужно писать, конечно,
красным цветом.
Значит, E два штриха.
Пусть это будет, ну, за O от В
плюс Е мы найдем
F штрих тяжелые
ребра.
В общем,
не в графе
штрих, а в графе
ну, в графе штрих понятно, там все такие
ребра, очевидно, да.
В G.
Вот, так что, да, третий шаг, давайте скажем
такой.
Ф
тяжелые ребра в G.
То есть
в частоте
часто, то есть
это
ну, то есть туда могут входить
ребра и из E штриха, и из дополнения
E штриха.
Так.
Вот.
Ну, и после этого я скажу, что в некотором
смысле
return
то вы уже догадываетесь, что.
А тут, да, E
без E два штриха.
Ну, да, хотелось бы, да,
хотел бы, чтобы граф как-то сжимался.
А теперь фишка такая.
На самом деле я пропустил
один очень важный шаг.
Да, вот этот шаг номер ноль.
Да.
Чего?
Да, так я не знаю, как ты
понял.
Ну,
ну,
ну,
ну,
как ты понял.
Да.
А действительно, что мы делаем, когда надо
граф чуть-чуть сжать?
Надо просто запустить несколько шагов
в барвке.
Да.
И мы это сделаем.
Выполнить.
Точнее так. Нет, наоборот.
Смотрите. Нет, это в самом начале
надо сделать. Это первый шаг.
А нулевой шаг выполнить
три шага
барвки.
Да, именно три.
Можете четыре, пожалуйста.
Ну, давайте
кашегов.
Вот.
А лучше как равно четырём, чтобы не путаться.
Да, да, да.
Ну, так-то у нас
тот алгоритм с как равно четырём тоже работал.
Нет, там, по-моему,
там, по-моему, ка не меньше четырём было.
Где-то пользовались шагами. Минус два больше либо равно двух.
Ну, было.
Ну, ой, давайте.
Не будем сейчас это вспобедать,
а то это...
Выполнить три шага барвки.
Вот такой алгоритм.
Так, опять очевидно, что этот алгоритм работает
правильно, правда?
Ну, пока это
бесконечная рекурсия,
я не знаю.
Нет, она уже не бесконечная.
Потому что
каждый шаг барвки, зачем он нам нужен?
Он очень приятен тем,
что каждый шаг барвки уменьшает
количество вершин как минимум вдвое.
Я к тому, что тут из функции
не отдавал бы.
Чего?
А, ну, да, да, да.
Надо было сказать, что шаг минус один. Если размер Жени
меньше, чем константа, то давайте
внутри барвки, если
размер Ж равен
трём или меньше,
просто делаем один.
Ну, допустим, да.
Ну, не важно, это понятно, что такие вещи обычно
поюзумеваются, поэтому мы ныне не тратим
места на доске.
Ну, давайте думать.
А вообще, за костьем хоть какой-то работает.
Даже
не в роддомном случае, а хотя
в худшем. Давайте подумаем.
Ну,
да, ну, не совсем. У нас
два рекурсивных запуска.
Правда, есть маленькая оговорка.
В этих рекурсивных запусках,
конечно же, у нас суммарное количество
ребер.
Ну, теоретически
может быть больше.
Ну, насколько больше?
Ну, просто давайте так. Вот мы
в F' ссунули
модуль E'
ребер. Правда?
А теперь заведем, сколько у нас?
А теперь, значит,
что у нас превратилось в вот этом
E'е?
Ну, вот,
то есть мы там нашли из этих E'ов
рандомизированный
лес. То есть в принципе это означает,
что
там действительно,
что на самом деле все
ребра, кроме собственно этого
леса оказались тяжелыми, их надо
выкинуть. Правда?
Вот.
Ну, а также
еще могли быть выкинуты какие-то еще
ребра из тех, которые в роддом не попали.
Правда?
Вот.
Теперь вот возникает
вопрос.
Ну, что же это
за оставшиеся ребра такие?
Вот.
То есть E'
у нас
ну,
ну, то есть сформулируем
так.
То есть я утверждаю,
что на самом деле E'
больше либо равно
на модуль E'.
Я утверждаю, что это больше либо равно
модуль E'
минус
количества компонент
связности
в F'.
В G'. Пардон.
Так.
Так, а понятно ли откуда я взял
вот это утверждение?
Сейчас.
Или не логично.
Ну, это все ребра, которые у нас полетели
в G' 3.
Решение.
Ну, да. Ну, потому что заметим следующее,
что давайте так.
Если у нас была
как бы грав G' оказался связан.
Если грав G' оказался связан,
то тогда я утверждаю, что
то тогда вот из
вот этих вот E'
выжило на самом деле
только V-1
ребр, правда? То есть те, которые
собственно этот минус 100 организовывали, правда?
Но утверждение такое.
Пусть E'
этот нод, пусть вот этот
грав, допустим, G'
оказался связан случайно.
Тогда
я утверждаю, что
тяжелыми, тогда
все ребра E' кроме
этого собственно минус 100
окажутся тяжелыми.
Вот.
То есть получается
на самом деле все ребра
без этих, а я еще кое-что забыл.
Да, плюс
модуль V, конечно. Писать тогда.
То есть модуль V-1.
А нет, плюс модуль V.
Да, если компонент связанности 1,
то надо написать там
минус.
Хорошо.
Ну ладно.
Хорошо, тогда минус тут
и плюс вот так.
Вот теперь вроде хорошо.
Значит, еще раз, откуда я это взял?
Предположим, у меня количество компонентов связанности
в G' одна.
Тогда F' это просто
минус 100 в размере V вершины, V-1
ребро, правда?
И все ребра отсюда,
все ребра E' кроме
этих V-1 ребра, они
заведомо тяжелые
и будут удалены.
Поэтому я и заявляю, что E'
это как минимум вот столько.
Но с другой стороны, я еще могу теперь
торжественно...
А, ну да, про остальные
ребра я ничего сказать не
могу.
То есть вывод на самом деле...
Ну понятно, если компонент связанности больше,
чем одна, то там получится вот примерно так.
Потому что если у вас там L компонент связанности,
то размер минус 100
ровно V-L будет.
И тогда получается, что
E-E', это сколько получается?
То есть модуль E
минус модуль E'
это получается не превосходит
чего?
Модуль E минус модуль E'
плюс модуль V
минус количество компонентов связанности
в G'.
Так, это вот сколько ребер туда запустится.
Вот, понятно, да?
Вот, понятно?
Вот.
Но при этом в эту компоненту у нас тут запустилось
целых E' ребер.
Вот.
Вот.
Ну, пытаемся, да.
Но заметим, что
эта сумма получается просто
сколько ребер было изначально
и плюс еще
вот это вот. То есть вот эти вот
там плюс V минус вот это.
Вот.
Ну, да. То есть как бы
если бы у нас сумма получалась ровно E,
то тогда
отсюда бы следовало, что алгоритм
действительно работает за E' веб, потому что на каждом уровне
суммарно E ребер, правда?
Правда, если бы оно вот.
Но у нас сумма не E, а E
плюс V минус количество там каких-то там еще
левых компонентов связанности.
Понимаете, да?
Но заметим на самом деле следующее.
Заметим, что на самом деле
мы же в самом начале еще делали
вот этот вот шаг борувки.
То есть три шага борувки,
правда?
То есть это на самом деле означает,
что у нас
то есть граф, на самом деле
VE на самом деле в этом месте
превратился в граф
там.
Там, так сказать, VB
В3, E3.
Логично, да?
Согласны?
Ну вот.
Мы заведомо можем теперь заявить,
что так как количество вершин у нас уменьшилось,
то есть на самом деле вот это
E' конечно лежит уже в E3.
И тут на самом деле ссылка на E3.
Вот тут E3, и тут E3.
То теперь стоит только заметить следующее,
что сам по себе модуль E3, я утверждаю,
к чему он равен.
То есть заметим, что если V превратится,
то есть если у нас 179 вершин,
превратилось в 57.
Причем остовным путем.
То есть я утверждаю, что это означает,
что у нас из графа исчезло как минимум
122 ребра.
Потому что каждое ребро,
потому что как у нас уменьшается количество
вершин на одну, мы в остов добавляем новое
ребро, правда?
И больше оно у нас не рассматривается.
И получается, что E3,
оно на самом деле меньше
либо равно, чем
модуль E
минус
модуль V плюс модуль V3.
Понятно, да?
И тогда получается, смотрите,
что суммарное количество ребр,
которое рассмотрели в рекурсивных запусках,
оно получается к чему равно?
Оно получается
не превосходит.
Оно значит, что у нас суммарно E3
плюс модуль E3
минус модуль E2.
Какое? Вот это?
Из борувки.
Ну, смотрите, что делает борувка?
Ну, опуская там технические тонкости,
он находит какие-то ребра,
объявляет, что они точно в миностове
и стягивает, объединяет две вершины,
которые она соединяла, правда?
Тем самым количество вершин
уменьшилось на одно и количество ребер уменьшилось
хотя бы на одно.
Стоп, он разве
за один шаг не стягивает?
У нас не итеративная борувка идет?
Нет, итеративная.
Вопрос просто, что делает шаг борувки?
Он на каждом шаге
берет из каждой вершины
минимальное торчащее ребро
и все эти минимальные ребра типа стягивает.
Да, но я мыслю по-другому.
Я мыслю, что он находит какие-то ребра,
что каждый шаг борувки находит какие-то ребра,
которые надо сжать.
И говорю, что если я сжал
там допустим L-ребер,
то у меня...
То есть так, если у меня
количество вершин уменьшилось на L,
то количество ребер уменьшилось хотя бы на L.
Просто я так мыслю.
Поэтому я говорю, что
E3 у меня не более, чем E-,
вот насколько стало меньше вершин.
Поэтому тут теперь аккуратненько
переписываем.
Значит, E'
плюс модуль E
минус модуль V плюс V3.
Вот.
А, нет, я вот это
переписываю уже, конечно.
Вот.
То есть это подставил E'
и дальше тут надо написать E'
плюс модуль V
минус количество компонентов
бла-бла-бла.
А почему это бла-бла-бла?
Ну, по логичной причине.
У нас бла-бла-бла,
потому что
в логичной причине
у нас бла-бла-бла, потому что
шлеп-шлеп.
И, конечно же...
Подождите.
В четвертом шаге
разве надо подставить
вместо V
V3?
Как он в четвертом шаге?
Четыре точки ретермера.
Ну, давайте подставим.
Так.
Давайте подставим.
Хорошо, хорошо.
Так.
Ну, и чем это нам помогает?
Да.
Что?
Чего?
Чего? Где?
Меньше ребра, потому что помимо ребра, которые мы стягнули для остова,
могли стянуться еще ребра.
Вот.
Вот.
То есть вывод получается вот такой.
Так.
Ну, тогда хорошо. Если я тут написал V'
значит, я тут должен написать где-то V'.
И тогда тут
что-то по-моему... И тогда как будто тут
что-то не сократилось.
Ну, вот. А, потому что...
Ну да, E3, не просто V' V3.
Да.
А, смотрите, да, тут минус V' плюс V3.
Так.
Тут у нас что?
Тут у нас получается E3
минус E2'.
Значит, это меньше либо равно, чем...
Значит, мы тут должны написать, что...
Так.
А, где-то я тут еще E' должен написать.
Так, ладно, давайте еще раз напишем.
Да, просто так, надежно быть.
Значит, что у нас получается?
Это количество ребер, которые у нас тут
в рекурсивных запусках пошло.
Значит, то есть...
Да. То есть мы тут сгенерили
какой-то E', да.
Вот. А потом в конце
сгенерили все ребра без вот этих вот
тяжелых.
Да. Но мы обнаружили,
что тяжелых ребер у нас как минимум.
То есть тяжелых ребер у нас оказалось
просто там весь E'.
То есть тяжелыми оказались
просто все.
Там, кроме вот этих
вот V3 минус вот этих.
То есть тогда получается, что
значит разность мы оцениваем наоборот
сверху. То есть там получается,
когда вычитаем, как
у нас получается,
получается, когда вычитаем,
получается, там вычитаем
E', вычитаем компоненты
и прибавляем V3.
Здесь все правильно.
Вроде как.
Вот.
Да, тут тоже 3, кстати.
Вот. Тогда, когда мы это все суммируем,
получается, модуль E',
плюс, значит, вот эту разность
прям честно переписываем.
E3 минус модуль E',
плюс модуль V3
V3 минус
количество
компонент связности
в G'.
Сейчас.
Ну, давайте так.
Подставляем.
Ну, вот.
Сократилось, сократилось.
Смотрите.
Давайте так. Я сейчас аккуратненько.
Давайте я просто тут.
На самом деле сократилось.
Там. Это модуль E3
плюс модуль V3
минус какое-то там количество.
Ну, нет. Я E3 пока не переписывал.
Вот это вот E3 я еще пока оставил.
То есть пока у меня осталось просто E3 плюс V3
минус что-то там количество.
Какое это получили вообще?
Сказали, что эта разность не превосходит вот этого.
Вот.
Теперь подставляем сюда E3.
Ну, пока у нас эта оценка на суммарный запуск
это не более чем E3
плюс V3 минус вот количество компонент
Ну, по факту да.
Теперь давайте E...
Ну, хотя не знаю.
Я только что еще раз написал.
Получилось пока просто.
Теперь мы еще E3 не подставили.
То есть у нас есть оценка.
Это меньше либо равно модуль E
минус, внимание, модуль V
плюс модуль V3
плюс модуль V3
и минус еще что-то там.
А теперь маленькая фишечка.
А теперь заметим, что это
не превосходит, знаете, чего?
модуль E
минус модуль V...
минус даже там
3 четверти V.
Вот так.
Да.
Напоминаем, что уборовки
есть типическое свойство.
Каждый личный векшин сжимается хотя бы вдвое.
Поэтому вот это вот
это не более чем четверть
от былого величия.
То есть в принципе
отсюда следует,
что мы сделали сжатие
что мы суммарно
действительно все делаем за
то есть получается суммарно запуски
делаются за вполне себе
от E.
А суммируем
E3 каждый раз, да?
Нет, там оговорка, конечно.
Чего? Еще раз.
Что-то не то.
От E равняется
E плюс T
от E минус 3 четверти V.
И минус 3 четверти V.
Это как-то что-то встает.
Ну почему-то у нас просто вот
есть очень удачное прореживание.
Потому что
мы как
вот эти три шара борувки
они нам на самом деле какое-то количество
приличное количество ребер просто убили.
Кстати, это количество ребер
равно вполне себе
между прочим 7 восьмых V.
Где-то.
Где мы вообще пользуемся
рандомностью ребер?
Пока нигде.
А я не понимаю, почему у нас суммарно ребер?
Пока нигде. Пока я
оцениваю даже
насколько это в худшем случае работать будет.
Почему у нас суммарно ребер мало? Я не очень понял.
Потому.
Это же один шаг.
Ну да.
Нет, смотрите. Я просто
доказал, что вот в эти два рекурсивных
запуска у меня вписывается не более чем
суммарно ребер.
Да, но при этом есть старое.
Но скажи так. Если мне
но вот. Ну просто смотрите. У меня есть
граф, в котором есть ребер.
Я запустился два раза рекурсивно
на суммарно не более чем ребер.
Они запустились
еще по два раза рекурсивно, но суммарно
тоже не более чем ребер.
Они уже суммарно
вдвоем. Нет.
Нет. Нет.
Каждый на свою сумму.
Вот.
Ну там ладно. Правда оговорочка, что
граф мог быть несвязаный, поэтому
мы там часто запускаемся не
за Е, а за В плюс Е, правда.
Ну есть такая мелкая оговорка, да?
То есть поэтому тут вот эти
3 четверти В конечно нам могут быть
не очень.
Но с другой стороны это ничего страшного,
потому что учитывая, что мы в каждый граф
передаем, простите, не В вершин, а
сколько?
Да, мы их передаем
повыделить на 8, напоминаю.
Поэтому в принципе если к этому прибавить
два раза повыделить на 8, то в общем-то
криминала не произойдет, правда?
То есть поэтому получается,
что на каждом уровне
значит все
рекурсивные запуски действуют суммарно
на,
то есть передается не более чем
Е ребер.
Ну точнее так, не более чем Е ребер
и суммарно все делается за
получается В плюс Е.
Понятно, да?
То есть я утверждаю, что
смотрите, дело в том, что
помимо рекурсии все работает за В плюс Е,
согласны?
Да, ну самое интересное
конечно вот это прореживание,
но мы верим, что за В плюс
Е оно делается.
Вот мы когда вызвали
рандомайс МСФ,
мы его вызвали
второй раз от Е штрих
и
нет, не от Е штрих,
от
всех ребер, которые были раньше,
без тяжелых ребер, которые мы нашли
на предыдущем шаге.
Да, было
ровно вот
могли,
но при этом суммарно их все равно меньше, чем Е.
Именно, да, они могли
повторяться, но нам важно даже
не это. Смотрите, мы
говорим следующее, что на каждом рекурсивном запуске
мы кроме
рекурсии работаем за В плюс Е.
То есть вот эти шаги боролбки
мы делаем за В плюс Е,
соответствующие
сжатия и так далее, это все делается за В плюс Е.
Получается В плюс Е и рекурсия.
Но отсюда следует такое, что если мы
в рекурсию будем передавать такие графы,
что сумма В плюс Е для них окажется
не более, чем
то есть сумма не более, чем именно
В плюс Е для
исходного графа, то тогда отсюда будет следовать,
что все это работает за В плюс Е на
логарифам уровней рекурсии.
То есть таким образом вывод очень простой.
Из этого уже следует,
что этот алгоритм в худшем случае
работает за Е лог В.
Вот, понимаете, да?
Вот, понимаете?
Еще более простым способом
можно доказать, что этот алгоритм работает
за О от Е плюс В квадрат,
особенно если вы сжимаете кратные ребра.
Ну, фишка там заключается...
Ну, понятно, фишка тут заключается в том,
что вы все шаги сделаете
тут за В квадрат,
а в рекурсию
передаете два раза по графу
на В поделить
на 8 ребер.
Понятно, да?
То есть поэтому получается...
Можно сказать, что алгоритм еще и за В квадрат работает.
Вот.
Ну, во-первых, давай так.
Первое, что мы сделаем неявно,
в том числе и в шагах борувки,
убьем кратные ребра.
Идея такая.
Делаем...
Передаем здесь этот граф.
В нем ребер не более чем
В поделить на 8, все это в квадрате.
То есть не более чем В квадрат на 64.
Логично, да?
То есть передали граф
на вот столько ребер.
В поделить на 8 вершин
и без кратных ребер, допустим, да?
Потом, значит, нам вернулся в ростов,
мы тут что-то за опять В квадрат пошаманили
и передали еще один граф,
видим, и без кратных ребер.
Поэтому там по индукции получается,
что так как В поделить на 8 в квадрате
даже умножить на 2
это заведомо меньше, чем В квадрат.
Но там во второй раз...
Поэтому В квадрат
тоже выклеивается вполне.
Но на самом деле нам пока
3 шага в руки не нужно,
а нужно только 2.
Ну, может, наверное.
Нет, ну да, 3 нам понадобится,
когда мы попытаемся
от ожидания уже считать.
Да, вот теперь давайте вспомним,
пока это просто еще и аргументация,
что алгоритм, в принципе, неплох тем,
что он не хуже Барувки или Прима.
Это уже неплохо.
По модулю вот этого черного ящика
алгоритм, кажется, не самый убойный.
Честно? Не знаю.
В фараде Калдон Бендер
умел LCA искать за единицу
при линейном предподсчете.
Это да.
Конкретно LCA.
Кат и предок тоже.
Вот так маленькая проблема.
Одно дело найти катово предка,
другое дело найти минимум
на пути между ними.
И вот это была проблема.
Мы же можем сжать все ребра.
Мы все писаем.
Сейчас общаем проблемы.
Если мы умеем искать катово предка,
то что-нибудь...
Нет, там фишка такая.
Хорошо.
Там просто проблема.
У нас были две составляющие.
Во-первых, мы насчитывали двоичные подъемы
для...
Там три просто подлые части.
Во-первых, мы для каких-то двоичных подъемов
насчитывали минимумы.
Вот.
Но не для всех.
То есть у нас нет такого,
что если я пойду на предка на 8,
то я, оказывается, двоичный подъем
уже насчитал. Нет, у нас такого нет.
Правда, тут не проблема.
Потому что
другая часть говорила,
что у нас же там был
ледер декомпозицион
суммарного размера N.
И на этом ледер декомпозиционе
минимум мы насчитывать можем
за линию. Тем же фарах Колтон Бендеров.
Тем, что мы
идем в листья, мы уже сразу
ломаем минимум. Нет, не ломаем
на самом деле. Нет, знаешь, почему?
Потому что там идея такая. Дело в том,
что там просто фишка такая,
что как мы насчитывали сами двоичные
подъемы? Мы говорили, что мы поднимаемся
на 8, а потом выясняется, что
подняться еще на 8 из этой вершины можно в
ледер декомпозиционе.
Так давай тогда в ледер декомпозиционе
мы еще и минимум выкопаем, потому что на каждом пути
из ледер декомпозициона можно минимум мы насчитать за линию.
А когда мы из листьев
поднимаемся на 2 степени N?
Вот, а так это из листьев и есть. То есть тут берем минимум,
а тут минимум берется из ледер декомпозицион.
Мы сможем сохранить минимум на пути.
А, а вот это уже да.
А вот это уже проблема. С другой
стороны, правда там,
да, но с другой стороны
с другой стороны оговорка такая.
В этом шаге мы сделаем так. Мы спустимся на
допустим 57 вниз.
Для того, чтобы
двоичный подъем нам
едва ли не тут же
отсюда поднялся.
Просто
тут же наверно
поднялся. Но тогда скорее всего
на этом пути мы просто будем брать минимум не на
всем вот этом суффиксе, а только на этом отрезке.
Что?
Нам нужно на всех отрезках насчитать.
Нет,
хотя
да, проблема, проблема, согласен.
Нет, ну там просто фишка такая,
что двоичный подъем у нас скакнет
на сколько-то, да.
Ну, из 57
он должен скакнуть там, да. Если он скакнет на 64
или больше, там на 128,
то тогда утверждается,
что просто там, скажем так,
в ледер декомпозицион есть
путь, проходящий через эту
вершину, из которого вверх, на вот
это вот расстояние, на эти 7, можно и скакнуть.
Так что это выкапывали,
это выкапали, там больше подлянка в том,
что там самая большая подлянка с этими метавершинами
размеры логарифом,
чтобы для них...
Не, если бы там фишка была в том,
что там же у нас деревьев самих,
типов деревьев мало, их можно там всех предподсчитать.
А там сжимать координаты
придется не только по типам деревьев, но и по тем,
по весам, которые там висят.
Нет, это подходит.
Нет, это мы тут просто начали пытаться гадать,
действительно, что там примерно написано
в этом ухаге группе.
Да, если мы верим в это.
А, вот это?
А, тут?
Ну, в принципе, да.
Но если мы поверим,
что это работает за V плюс E,
то, во-первых, получается E лог В,
а во-вторых, сейчас мы попытаемся доказать,
что мат ожидания времени работает
у этого алгоритма, соответственно,
E.
Будем доказывать это, как всегда, по индукции.
Да, ну ладно, не E,
а, ладно, будем честны, V плюс E.
Давайте честно докажем, что
время работы, то есть мат ожидания,
там T,
то есть, так сказать, T от G
не происходит, С на модуль V
плюс модуль E.
Ну, давайте думать, как же мы это будем делать?
Так, в общем, да, я утверждаю,
что нам даже не потребуется вот это
минус D писать.
Значит, ну, поехали.
Начнем с того, что мы забыли там.
Ну, поехали.
Так, давайте, кажется, пришло время
ликвидировать вот это безобразие.
Ликвидируем это безобразие.
Безобразие-безобразие-безобразие.
Безобразие-безобразие, безобразие, безобразие.
Безобразие.
Безобразие, безобразие, безобразие.
вот
получать следующее значит мат ожидания значит поехали это значит t от g значит оно чему оно
равно ну давайте меньше ли розы на первом шаге ну понятно мы делаем а на в плюс е потому что
барубка понятно да ну давайте ладно я все делаю за а от в плюсе кроме рекурсивных запусков так
что давайте все это пишу за а от в плюсе так теперь значит теперь мы делаем рандомный мсф
рандомный мсф у нас работает зам от ожидания значит получается ц на сколько там загенерилась
да значит модуль f штрих да модуле в штрих пока это но формально надо наверное изе написать
потому что f штрих это случайная величина вот то есть разберешь 3 но но вот а что такое какая коллизия
так ну ладно как обозначает амут ожидания в математике правильно буквой м неправильно вот
нет математик но вот но можно и мне вообще написать шопл а да что получается то есть пишем мат
уже то есть ну да давайте так мат ожидания т от же ночь не превосходит а от в плюсе
ну хорошо хорошо да ну давайте но вроде так мат ожидания т от же штрих но мы что-то то здесь
отказались вот давайте мат ожидания плюс мат ажида плюшо там дальше там мат ожидания от значит
от и от вот этого в 3 я его даже специальным графом не обзывал очень короче так вот
так давайте таким утверждением мы согласны или наше чувство или наше чувство прекрасного
здесь начинает вызывать какие-то сомнения но это другой вопрос нет пока вот идейно
вообще это формула правильная да понятно что в ее потом можно применить индукции
такая вообще какой-то идейно вообще имеем право так писать ну да правда тут видите
но да видимо приходится скажем так приходится иметь в виду правда что мат ожиданий здесь
берется ну когда тут вероятностных распределения не только внутри же штриха но и в том рандоме
который этот же штрих сгенерил да нет ну просто нет ну не совсем нет и ни за что не отвечает
просто у нас есть же штрих ты как бы будет запущен и сделается какое-то время работы
но просто как бы просто имеется в виду что там мат ожиданий здесь имеют в виду что вероятность
генерилась как внутри так и снаружи вот так вот и здесь тоже сам то есть это надо иметь в виду
но с другой стороны заметим следующий теперь
ну можно это да давайте ну вот ну да ну да давайте ну давайте чтобы не путаться давайте так
и напишем вершины в 3 ребра есть три да так теперь мы секрет следующее мы знаем что какой бы
я здесь граф не сгенерил по предположению индукции но там будет меньшее количество вершин и там уж
пока и не большее количество ребер да и то меньше следовательно по предположению индукции могу
предположить что у каждого графа который тут может появиться там мат ожидания время работы
будет линейна правда но давайте давайте я вот давайте я вот распишу для примера так мат аж м т
от v 3 е штрих это равно сумма по всем е штрих которые могут появиться вот так которые могут быть могут
появиться значит вероятность того что е штрих появилась умножить на мат ожидания собственно
этого т от как-то по-другому написать то ну вот ладно на самом деле вот эти мки это две разные
то есть тут мат ожидания берется по вот этому то есть тут мат ожидания вот то есть здесь
подразумевается что е штрих случайно а здесь подразумевается что е штрих фиксировано
это у нас как выясняется то есть смотрите это не превосходит сумме опять п от е штрих появилась
умножить на сколько как выясняется сколько у нас там получается цена модуль v 3 плюс модуль
е штрих логично да сразу вот вот такая вот неожиданная идея так ну что по идее из этого
действительно можно вывести но тогда это на самом деле если я это аккуратно сейчас раскрою
скобки то я могу сказать что это равно равно ц модуль v плюс значит плюс ц на сумму значит
п по всем е штрих п е штрих появилась умножить на модуль е штрих согласны да что да в этой да
но заметим что просто по определению это банально то есть вот это вот это просто ц на мат ожидания
количество то есть мат ожидания размера е штрих потому что это это просто определение правда
вот это вот просто определение понятия мот ожидания нет
или кто тесно сейчас будет это викида а я подсчитал википедию там другое определение там
интеграл и бега написал ну мало ли я просто рассказывает прикол даже что как-то рассказал
что такое корневое дерево это просто обычное дерево у которого объявили корень я посмотрю
определение википедии там вообще другое написано но нет там что-то технически жесткое
описание там понятие детей там родителей и так далее вот так вот а теперь давайте подумаем
а чего нас равно кстати мот ожидания количества и я квадрат какой квадрат
ну да совершенно верно просто равенство да потому что помните мы каждый ребро выкидывали
средства я штрих мы каждый выбрали сразу одна вторая вот да действительно можно было прям
жестко под множество взять кстати чтоб тут была ровно половина и это бы сработала так вот но там
не принципиально да потому что чем больше ребер возьмем там тем как бы меньше ребер там будет
передана в рекурсию так вот значит про я штрих поняли но теперь остается только выяснить а
теперь остается выяснить чему нас равно м т от v 3 и е 3 без е 2 штриха ну пока да
а нет смотрите
да с хорошей вероятностью мы там
да мог бы быть фейл то есть если нам будет фантазия фатально не весь алгоритм будет
работать за 4 степени то есть мы доказали что у нас мот ожидания 3 степени но это не как бы но
как бы есть у нас мот ожидания крутое то есть как бы да что да у вас там да если верить мот
ожидания у вас там где-то в среднем за два раза выпадет орел но это не значит что у вас не
выпадет сто пятьсот реже значит дальше что у нас такое мот ожидания тет в этот три там
е 3 слэш е штрих что это значит ну я вот как бы и вот это вот вещи технические то есть я
честно скажу что это не превосходит цена модуль v 3 получается плюс мот ожидания размера е штрих
без е 2 штриха да
нет рандом то нет рандом то как бы не влиять будет но то есть как минимум видимо дальнейшая
часть видимо покажет что рандом может и повлиять ну скажем так с точки зрения
честности проблем не будет потому что если алгоритм реализовывать честную можно не
рандомизировать можно просто брать первое первое пополам да совершенно да но нам нужны но рандом
нам нужен чтобы среднем было хорошо чтобы выкидывали рандомные ребра они они
в смысле я говорю про то что если тесты не подгоняются не погоди но зорися рандом
но можно не шахтить потому что все равно каждый ребро берешь 1 2 так что разницы никакой то есть
как бы вероятность не должна зависеть от теста если тест фиксирован то как бы то есть рандом
для себя внутри кидаешь то есть не должно быть так что как бы бывают тесты которых мой алгоритм
работает жестко плохо но если я просто буду генерировать тест случайно то этот тест не
выпадет вот такого такого нет то есть вот этих разговоров типа у нас жюри таких тестов не было
нет такого не доехал быть то есть должно гарантировать так что как бы до любого вот что как бы какой бы
бы не был тест в жюри, должно быть так, что если ты много-много
раз на этом тесте твой алгоритм запустил, то в среднем работает
классно.
Что такое вероятность?
Ну да, ну просто есть, наверное, еще и, по-моему, есть еще типа
алгоритмов, где типа на случайных тестах работают
хорошо, но не на случайных плохо.
Не, ну типа даже из случайных тестов задачу дают, например.
Где-то у меня был…
Нет, это, вероятно, такие задачи бывают, но тут тогда
вы учитываете, что вероятность резиста крайне мала, поэтому
можно резист не учитывать.
Да, ну и символы это и пруют, конечно.
Вот, это понятно, да, но это как бы другое.
Так вот.
А теперь давайте думать, вот тут на самом деле может
быть просто ядро этого алгоритма.
А сколько у нас ребер, то есть вот что это за мотожидание?
То есть сколько в среднем ребер здесь выживет?
Так же.
Ну в смысле, вот до этого момента, право, расписали
абсолютно так же.
Я просто вместо из штриха подставил вот это, то есть
вот эти вот рассуждения вообще от этого не зависели.
Так, сейчас мы же в е-штрих появились, ну можно сказать,
что это как минимум.
Нет, е-штрих это типа, скажем так, какое множество ребер
тут появится, это тоже случайная величина.
Оно тоже зависит от е-штриха, только.
Нет, оно не зависит, ну как бы, ну типа.
Да, оно зависит только от е-штриха.
Нет, оно там, ну да, зависит от е-штриха, да, но это вообще
не важно на самом деле, откуда, потому что главное,
потому что я тут бы писал вероятность того, что это
множество как-то появилось, да оно там как-то появилось.
Теперь давайте думать, что это за множество и как
оно вообще появляется.
Но давайте вспоминать, как вообще это ребро появилось.
Это ребро появилось так, то есть мы как бы взяли
рандомные ребра из исходного множества, вот е3, в этом
рандомном множестве, и в этом рандомном множестве
взяли минимальный лес.
Потом вернули все ребра и удалили тяжелое относительно
этого леса.
То есть в переводе говоря, от самого е-штриха мы оставили
только лес, а потом мы вернули те ребра, которые выкинули,
а при этом те ребра, которые тяжелые, убрали обратно.
То есть у нас там по сути останутся только лески
относительно этой штуки ребра, по большому счету
так.
Но с другой стороны, давайте вспомним, а как у нас этот
лес вообще генерился.
Ну да, но с другой стороны он минимальный, поэтому
мы можем себе вообразить, что он генериальный, но
как бы так как ответ возвращаемый заведомо правильный, то
вообразить мы себе можем, что он генерился алгоритмом
Краскала.
Да, вот подключайте воображение на максимум, да, тут логика
будет достаточно нетривиальной, но тем не менее.
Потому что в принципе, да, то есть миностов теоретически
мог быть алгоритмом Краскала, да, а что такое алгоритм
Краскала?
Алгоритм Краскала говорит нам, значит, дайте, то есть
отсортируем ребра в порядке возрастания веса, а потом
каждый ребро смотрим, прибираемся по ребрам, смотрим на очередное
и говорим, что если ребро можно добавить восток без
зацикливания, то добавляем, иначе говорим до свидания.
Логично, да?
А теперь идея такая, а давайте мысленно этот процесс совместим
с выбором рандомных ребр.
То есть смотрите, на самом деле абсолютно так же этот
осток можно генерить следующим образом.
Берем ребра, прям исходные, Е3, сортируем их в порядке
возрастания.
И теперь проходим все по ребрам и говорим, если кидаем
монетку с вероятностью 1-2, если нам не выпало, значит
мы это ребро игнорируем, а если выпало 1-2, а если
выпало, а если ребро выпало, то мы его пытаемся добавить
восток, учитывая выше описанное требование.
Так мы строим вот этот вот f-штрих.
Но это мог получиться не наш f-штрих.
Нет, почему?
Как MSF получится ровно он, вот, да, ровно он.
Но с другой стороны, теперь заметим маленькую приятную
вещь.
Какие из этих ребер могут попасть, какие из этих ребер
попадут вот сюда.
Значит, идея такая, мы идем по этим ребрам Е3 и пытаемся
их добавить.
Такое утверждение такое.
А теперь давайте сделаем тот же алгоритм, но сделаем
одну маленькую модификацию.
Если ребро уже является тяжелым относительно набранного
основа f-штрих, то монетку мы для него даже не кидаем,
потому что даже если оно попадет в Е-штрих, то оно
потом вылетит, как тяжелые ребросы, согласны?
Но тогда получается, для таких ребр мы даже монетку
не кидаем.
Но тогда получается, что количество ребер здесь
не превосходит количество раз, которое мы кинули монетку.
Неожиданно, да?
Ну, вопрос был неожиданно, да?
Ага.
Ну да, короче, а правильный ответ – нет, счет становится
5-4.
Ну, потому что те ребра, которые мы кинули монетку,
они не выпали, но вот они как бы теоретически могли
все равно здесь вот как тяжелые ребра выпасть, хотя
ладно, это неправда, хотя до этого не произойдет.
Ну хорошо, да ладно, уговорили, да, по сути, если ребро сейчас
можно добавить, но вы этого не сделали только из-за
монеток, то как бы в будущем остове оно естественно
там максимум на цикле не будет, это правда, то есть
оно будет либо две компоненты стягивать, либо будет стягивать
цикл, но на этом цикле будет кто-то больше.
Да, хорошо, вот, да, важно, хорошо, ну скажи так, непринципиально
на самом деле уточнение, но хорошо, да, то есть у нас
будет вопрос, то есть вот здесь сюда мы передадим
ребер ровно столько, сколько раз у нас мы кинули монетку.
А теперь вопрос, сколько раз мы кинули монетку?
А теперь неожиданная логика, а сколько раз эта монетка
выпала?
В среднем я попала.
Нууу... не точно.
если мы его в теории не можем добавить, иначе мы кидаем, если получаем, то добавляем.
Ну тогда ребро автоматически получается добавляется в остов, если нет, то значит игнорируем,
но тогда, но второй запуск оно по любому попадет. Нет, но тут фишка такая. Тело в том,
что каждый раз когда ребро, когда монетка выпала по такому раскладу, у вас в остов добавляется
новое ребро. Следовательно монетка выпала не более чем в 3 минус 1 раз. То есть как только
монетка выпала в 3 минус 1 раз, в принципе дальше монетка кидаться не будет точно.
Поэтому получается, что получается, можно сделать даже еще, чтобы вот совсем было понятно,
я скажу так, то есть на самом деле эту монетку можно было кидать следующим образом. То есть надо
было не по ребрам ориентироваться, а заранее сгенерить последовательность, в которой вы кидаете
монетку до тех пор, пока она у вас не выпадет в 3 минус 1 раз. А потом, когда вы генерите по ребрам,
то когда вам требуется рандом, вы уже обращаетесь к очередному члену сгенеренной последовательности
и берете запись из него. Понятно, да? В принципе это одно и то же. Ну или по камере так тоже можно,
просто возможно вам не нужно было добиваться того, чтобы она выпала именно в 3 минус 1 раз,
поэтому вы там какую-то работу лишнюю сделали. Да, как бы мы знаем, что как бы этого нам точно
хватит, потому что как только монетка выпадет в 3 минус 1 раз, мы ее больше кидать не будем.
Тогда получается, что вот это количество ребер мы можем оценить сверху как количество раз,
то есть оцениваем сверху как мотожидание, количество раз, то есть мы кидаем монетку
с вероятностью 1-2 и останавливаемся, когда она сгенерилась в 3 минус 1 раз. Спрашивается,
каково мотожидание? Да, вот это распределение называется отрицательным биномеральным,
если не ошибаюсь. Вот, вот теперь смотрите, но на самом деле и так, но на самом деле заметим,
что так как нам нужно, чтобы n-1 раз монетка выпала, то я утверждаю, что это мотожидание равно n-1 умножить
на сколько раз нужно, чтобы она выпала 1 раз, правда? Ну вот, но мы помним, с какого мотожидания,
то есть если мы просто кидаем монетку с вероятностью 1-2, то есть с какого мотожидания количество
образков до ближайшего выпадения монетки? Да, как мы уже вычисляли в начале занятия 2, но там
был 1 делить на p, но в данном случае 2. Значит отсюда, в общем, из этих всех эпических рассуждений
следует, что вот это мотожидание на самом деле не превосходит просто 2 на ve3.
На там модуль ve3 плюс 2 на модуль ve3. Ну я даже эти там минус что-то там даже писать не буду,
неинтересно. Неплохо тогда. А теперь давайте это схлопнем. То есть раз у нас получается вот так,
то есть получается, значит это у нас не превосходит, как мы уже выяснили, а на ve. На, так, отсюда мы
выжили сколько? Вот столько. С на модуль ve3 плюс модуль ve3 пополам. Плюс что-то там еще. А,
это вообще равно давайте тут 3c модуль ve3 плюс 3c модуль ve3. Неплохо тогда.
Так, ну давайте теперь попробуем, а как бы нам оценить?
А, впрочем, на самом деле нам уже и по барабану. Смотрите, на самом деле это равно a на ve плюс e
плюс 4cv3 плюс c на ve3 пополам. Теперь понятно, что ve3 не превосходит просто e, очевидно, да? Там
можно даже еще повычитать, но в этом даже уже нет необходимости. Почему? Потому что, ну или если
будет сейчас попишем, ну значит a от v плюс e плюс, да, v3 у нас это сколько? Это не более,
чем выделить на 8, правда? v3. То есть тут получается cv пополам плюс, ну а что тут уже ce пополам?
Неплохо, правда? И теперь надо доказать, что это меньше либо равно c на v плюс e. Ну очевидно,
верно при c, просто банально. Каком? Больше или там? Ну при каком c это верно? Больше
либо равным 2a, все. Что такое? Ну этот шаг вас уже не должен удивлять. Не, ну да,
основная магия конечно была здесь, да, то есть это прям вот главная магия. Ну что делать?
Вот, ну это, ну вот такая вот красота.
Вот. Так, ну что, есть ли тут какие-то вопросы?
Подумаю. Не, ну не ниже хора точно. Не отрежу, у вас конспект будет, не волнуйтесь. У этой штуки есть
просто такой очень качественный конспект на русском языке, собственно, от качества математика,
так что не волнуйтесь. На экзамене нет, ну просто так, что там. Нет, ну на экзамене не будет. Ну да,
вот я уже начинаю думать. Просто я не знаю, успели рассказать за полчаса, но давайте попробуем.
Хотя, конечно, по-хорошему это называется донам быщуческой, донам быщуческой 3,
конечно, да, потом. Ну чтобы оставшуюся геому добить. Ну уже ладно, что делать, что делать.
Чего? Что у меня? Ну мне надо, да. Вот. Ну давайте сейчас быстренько. Сейчас будет вообще,
значит, веселая тема. Мы сейчас будем перемножать булевый матриц. Давайте сразу скажу,
то есть даны две булевые матрицы, мы их перемножаем. В каком смысле перемножаем? Ну,
как всегда, перемножение это типа конъюнция, сложение типа дезюнкция. То есть определение,
то есть там по определению, когда вы перемножаете там две матрицы N на N, у вас получается что-то
типа, то есть получается C и TGT равно, значит, дезюнкция по всем K от 1 до N, там A и KT,
B, pardon, KTGT. Вот такую штуку я хочу найти, желать как-то быстро. Но как же мне это сделать? Да,
мне придется ввести еще понятие неожиданное M от N. Это время, за которое вы можете детерминированным
образом перемножить две матрицы. На этот раз обычные числовые. Ну, как мы уже обсуждали когда-то,
то есть на текущий момент M, мы знаем, что M не происходит там что-то типа от N в степени 2.23,
какой-то там. Ну или там какие-то такие цифры я не помню. Суть какая-то такая или что-то в этом
роде, может, я наврал. Ну там в Википедии, в Википедии, наверное, можно посмотреть. А 2,
3, 5, 4 или что-то в этом роде. Вот, ну потому что, ну вот, хочется вот действительно как-то теперь
эти булевые матрицы. Ну вот каким-то мистическим образом перемножать. Так,
ну как минимум наврал, тут не 2, 3, тут по версии конспекта, тут все-таки 3, 7.
Нет, ну да, это потому что я вам говорил фиг год назад.
Нет, ну метод штрафсона это 2.8, если что. Ну вот. Да, вот это называется у нас такие
булевые перемножения матриц, оно нам иногда будет, соответственно, пригождаться. Зачем оно нам
будет пригождаться? Потому что решать это мы будем на самом деле более крутую задачу. Мы будем
решать Флойда. Флойда в невзвешенном графе, в незвешенном, неориентированном графе. Итак,
реальная задача такая. Жила была матрица смежности A, N на N. Надо восстановить матрицу D,
N на E, N, где D и G, T это просто расстояние в этом невзвешенном, напоминаю, графе.
Нет. Ну да, просто матрица смежности из 0 и 1. Вот. Ну возникает вот такой естественный нот. Ну а
первых заметим, конечно, маленькое, значит маленькое приятное свойство. Маленькое приятное
свойство на самом деле будет, ну а первая идея, которая тут возникает, значит, идея тут возникает
такая. Ну а первых заметим, что будет, если я возведу эту матрицу в какую степень. Вот по смыслу.
Ну, а точнее. Да, то есть на самом деле так, количество, то есть A в степени K и T житое,
это количество путей не обязательно простых длины ровно K, соединяющих вершины и ежи.
Нет. Ну, если я A восприниму как числовую матрицу, то нет. Вот, то есть нет, свидетель это вот
булевое перемножение, это нам понадобится, то есть будет такой инструмент. Сейчас не заморачиваемся,
пока все в числах. Так вот, у меня идея такая. Заведу-ка я такую матрицу, как A штрих, и она будет
у меня равна, знаете к чему? Она, значит, будет равна A квадрат или A. Ну, короче говоря,
я хочу в А штрихе, то есть я хочу, чтобы, другими словами, А штрих и ж равно 1, если расстояние от
и ж не превосходит 2 и 0 иначе. То есть я могу получить вот этот вот А штрих, получается,
за время М от Н. Ну, за время перемножения матрицы. И по этой матрице можно, по идее,
получить соответственно матрицу D штрих, то есть расстояние в ней. Понимаете, да? Ну, D штрих
это расстояние, потому что вот это тоже матрица смежности, правда? То есть, по сути, мы в графе
соединяем все вершины, у которых есть общий сосед в исходном графе. И в этом новом графе
тоже есть кое-что. И у меня получилось. Ну вот. Так, теперь давайте пару утверждений правят.
Значит, утверждения. Ну, во-первых, начнем с того, что, конечно, D штрих и T ж это тупо D и T ж пополам
округленное куда? Ну, очевидно, вверх. Господи, это магия уже. Это должно быть просто очевидное утверждение.
Ну, просто в D штрих мы делаем один скачок там, где делали два скачка раньше. Вот.
На самом деле, можно написать более продвинутые утверждения. То есть, на самом деле, D штрих и
T ж. Оно, на самом деле, может быть, равно. Ну, на самом деле, тут можно еще аккуратно
попытаться разобрать случаи на тему того, когда надо округлить вверх, когда надо округлить вниз.
Вот. Так. Сейчас как что-то надо-то аккуратно сказать. Так, ладно,
давайте буду подпозглядывать, так быстрее будет. Так. А, ну давайте так. Значит,
смотрите, утверждение такое. Если D и ж четно, то для любого и и штрих, лежащего, так сказать,
в матрице А. Вот я так напишу. Да. Верно, что верно. Что D штрих от и штрих ж равно, там больше
либо равно, точнее, чем D штрих и ж. Нет, ладно, плохо писать и штрих, наверное, придется писать В.
Ну, тогда, смотрите, если вы перейдете из вершины и на одно ребро, то расстояние изменится на один в
ту или иную сторону. Изменится не более чем на один в ту или иную сторону, правда? Да,
напоминаю, у нас граф неориентированный. Ну, давайте так, простое утверждение. Ну,
простое утверждение такое. D в ж лежит на отрезке D и ж минус 1, D и ж плюс 1. Согласны?
Вот. Ну, отсюда автоматически следует, что если D это четно, то как бы пополам округлить вверх,
это как бы будет не меньше, чем то, что было. То есть не меньше, чем просто D и ж пополам,
поэтому мы так пишем. Вот. Для любого соседа это верно. Понимаете, да? Понимаете? Вот. Ну,
это первое. А теперь второе. Как вы уже догадываетесь, если D и ж нечетно, то тогда
существует такое V, такое, что ИВ лежит в матрице А, такое, что окажется, что D'Vj строго меньше,
чем D'j. Ну, то есть можно, ну, то есть просто берем кратчайший путь, значит, на нем смотрим,
куда идет и куда мы идем из И, вот приходим в вершину В, и оказывается, что там путь оказался,
то есть это округление дало на 1 меньше. То есть вот такое неожиданное замечание.
Ну, утверждается, что, ну да, утверждается так, что здесь у нас есть матрица А и матрица D',
то я утверждаю, что по ним можно однозначно восстановить D. Причем каким образом? Вот,
каким образом надо это восстанавливать? Ну, давайте вот аккуратно, внимательно
посмотрим. То есть идея будет такая, то есть как бы вот, ну, то есть вот давайте для каждой
пары Иj мы попытаемся проверить, то есть перебрать все соседи, всех соседей вершины
И и попытаться понять, там найдется ли действительно, то есть такая, то есть можем ли мы найти вот такую
дэшку? Если можем найти, то там будет нечетно, да, но вот то как бы дэшка получается будет нечетно,
ну, то есть мы видим, да, что D'j, то это по-любому, либо 2D'х, либо 2D'х-1. Тогда,
если эта вешка найдется, там надо писать минус 1, а если не найдется, надо писать без минус 1.
Понятно, да? Вот, теперь остается вопрос, а как бы это сделать не за куб? Потому что для каждого
Иj придется это честно перебирать. Логично, да? А можно ли это как-то свести к передвижению матрицы?
А желательно даже к перемножению, может быть, даже булевых матриц.
Нет, все еще круче. Смотрите, то есть, ну, вот. Но на самом деле можно заметить еще более крутую
вещь. Во-первых, смотрите, тут всегда больше либо равно, а тут найдется меньше. Но также еще верно,
что для любого V окажется, что D'vj меньше, чем D'ij меньше либо равно. Ну, там с округлением,
просто как бы нечетное число обладает свойством, что если ему прибавить один,
то по полам округления вверх ничего не изменится. Просто знаете, что из этого следует? Просто из этого
на самом деле следует супер утверждение. D'ij четно тогда и только тогда, когда сумма по всем И,
там V, лежащим в матрице A, значит D'vj больше либо равна, чем D'ij. Ну да. То есть, по сути,
там будет идея, что если вы перемножаете матрицу A на матрицу D', то есть, можно это записать по
другому. То есть, можно еще написать, что пусть у вас какая-нибудь матрица B, это будет равно тупо
матрица A умножить на матрицу D'. Тогда все, что вам потребуется, это после этого, то есть,
для каждого ij просто посмотреть, верно ли, что B'ij больше либо равно, чем D'ij. То есть,
если да, то D'ij четно, если нет, то нет. Здесь у нас пока сумма, но в принципе,
это означает, это дает там, конечно, такой маленький приятный алгоритм. То есть, алгоритм будет
заключать, как соответственно эту D'й найти? То есть, как вообще эту матрицу D'й теперь насчитывать?
Вот, но идея такая. То есть, давайте выполним за МАТН вот это удвоение, найдем рекурсивно D'.
Потом за одно перемножение матрицы мы, получается, восстановим ответ. То есть,
восстановим по матрице D'. То есть, получается, мы передали рекурсию просто тот же граф,
но с в два раза меньшим диаметром. Да, получается, у нас алгоритм от нахождения самих расстояний
получился от МАТН на логен. Понятно, да? Да, причем тут вероятность? Да, ответ. Ответ. Пока
действительно вообще ни при чем. Да, вот такая вот неожиданная нота. Так, ну давайте,
как вы думаете? Да, но картинка называется, да, ваши ставки. А при чем тут вообще может быть на
самом деле эта вероятность? Причем тут могут быть вот эти вот свидетели булева перемодожения матриц.
Вот эти. Потому что свидетельство будет такое. Найти вот эту матрицу, конечно, булевую, это не
проблема. Вот, кто они такие? А теперь смотрите. Нет. Где-то рядом ходит словосочетание свидетели
булева перемножения матриц. Где-то ходят. Вот надо представить. А ходят они вот где. Задача про
булевые матрицы на самом деле звучит так. Само по себе вот это произведение, да, мы его за ОАТМ находим
без всяких вероятностей. Ну вот что. Ну вот это вот. Да, одно. Это легко. За МАТН эту матрицу ценно
считать булевое произведение легко. Нет, за МАТН. Потому что надо просто честно перемножить матрицы А и
Б как числовые, а потом просто каждое ненулевое число заменить на один. Ну в смысле А и Б булевые
матрицы здесь были. Так что нолики единички. Замечательно. Нет, мы не будем делить, мы будем
просто делать это за степень 2.37. Продвинутый метод комплексмита винограда, который работает за
2.3781. Винограда, да. А остальное? Константа. Ну, да. Ну поехали. Зачем? Но нам интересно
просто произведение. Для каждого произведения C и J ведь существует K какое-то, хотя бы благодаря
которому эта дизюнкция вообще единична, правда? Так вот, это K и называется свидетелем. Свидетелем
булево-перемножение матриц. Поняли, да? То есть на самом деле вот именно здесь и возникает эта
задача. Вот. То есть поэтому так. То есть матрицу мы находим за M от N, а вот за какой-то точку можно
для каждого и J, который от C и J равно 1 найти великого свидетеля, вот этот вопрос. А теперь
вот давайте прежде чем сосредоточиться над этой задачей, давайте-ка мы полезем. Зачем нам
действительно это надо? Ну, идея на самом деле такая. Смотрите. Так, сейчас. Ну-ка давайте
сейчас скажем. Зачем нам этот свидетель, Зачем нам этот свидетель мог бы вообще пригодиться?
А прогодиться он мог бы вот зачем? Потому что заметим маленькую приятную вещь. Заметим,
что нам бы очень неплохо было бы вот в этой задаче не только находить расстояние, но и
восстанавливать ответ, правда? Да, собственно, путь. А что значит восстанавливать путь? Ну,
конечно, если между всеми парами решим в явном виде вектор писать, то это может быть ВКУ просто
на уровне ответа. Но на самом деле восстанавливать не надо. Идея про такая, что для каждой пары и J
неплохо бы просто написать вот эту вершину В, первую вершину на пути, правда? Что? Ну, здесь
мы мыслили в терминах В, да? То есть получается надо найти, то есть для каждого и J надо найти
такого V, то есть такого соседа V, что, то есть и V лежит как бы в ребро, и расстояние от там,
то есть D от VJ равно D и J минус 1, понимаете, да? Да, то есть это мы такое, это мы такое хотим найти,
причем знаем, что для любого такого V, напоминаю верно, что D от VJ, оно лежит на отрезке D и J
минус 1, D и J плюс 1, логично, да? Так вот, смотрите, читерство такое. Так вот, значит, предлагается
это сделать так. Ну почему, если мы сдвинулись на одно ребро, как бы, то можем расстояние как
увеличить на 1, так уменьшить на 1 или оставить таким же, но больше-меньше по неравенству трюгольника
не получится. Так вот, теперь у меня идея такая, сейчас будет бомбочка, я сейчас сгенерирую еще
три матрицы. Я скажу, что M нулевое, да ладно, M нулевое и J, нет, вот M плохо,
ладно, дайте, какую букву мы любим? T. На матрице? M, J, H. Ладно, давайте J. Так вот,
супер-J. Значит, J, матрица J нулевое и TJT, знаете к чему равно? Оно равно 1, если D и TJT делится,
там процент 3 равно 0 и 0 иначе. Аналогичным образом я генерирую J1 и J, и J2 и J. Вот такая
три булевые матрицы. Верно ли, что у меня расстояние, если взять расстояние по модулю 3 равно,
чего мне надо? Заява, да? Ладно, сейчас еще веселее будет. Свидетели, заява, чем мы занимаемся,
да? Ну это еще ничего. Значит, G0, G1, G2. Так вот, идея на самом деле такая. Как найти свидетеля
для IJ? Теперь пусть у нас выяснилось, что неожиданно D и TJT, там, пусть D и TJT процент
3 равно, ну, допустим, двоечки. Ну, допустим, да? Тогда я утверждаю, что V подходит,
если и V лежит в матрице A, и на самом деле DVJ процент 3 равно 1. Вот я утверждаю,
это необходимо и достаточно. Вот так я даже напишу. Так что вот такая матрица появилась.
Вот. То есть тогда просто идея такая. То есть на самом деле давайте, ну, по сути это означает,
что если я перемножу матрицу A на вот эту вот матрицу M1, то где-то в этом месте у меня,
то есть перемножу, то благодаря V у меня появится что-то отличное от нуля. То есть как бы вот эту
информацию, если такое V, я выковарю из информации A умножить на G1. То есть просто если я перемножу
эту матрицу и в клетке IJ у меня окажется что-то отличное от нуля, то тогда я сделаю вывод,
что благодаря какому-то V это произошло. А если я умею, но так как это были в матрице,
то если я умею находить свидетеля, то есть благодаря какому V это произошло,
то собственно это оно и будет. Это называется, зачем нужна задача о свидетелях булева
перемножения матриц. Вот такая красота. Понятно? Так, ну вот. То есть теперь на самом деле про вот
эти вот качайшие пути забываем и теперь решаем задачу о свидетелях. Значит, о свидетелях булева
перемножения матриц. Ну да, то есть мы, да. Ну на самом деле алгоритм будет такой. Мы сделаем какие-то
рандомные ивристики, которые позволят нам восстановить, то есть восстановить свидетелей,
ну кроме может быть не сильно большого количества, там их будет у нас не более чем N в среднем. И тогда
этих остальных мы будем восстанавливать прям предельно. Ну во-первых, что можно сделать? В первую
очередь можно перемножить за, ну во-первых, давайте там можно потратить уже МАТН времени на то,
чтобы матрицы честно перемножить, правда? Да, и выяснить, что для каких пар IJ свидетелей вообще
нужны, а для каких нет. Ну вот, а теперь что же будет дальше? А дальше идея такая. Давайте заведем,
да, вот значит забыли, у нас больше А и Б, это не матрица смешности, напоминаю, да. Так вот,
идея такая, теперь я создам матрицу А, допустим, с тильдой, такую, что матрица с тильдой ик,
знаете к чему равна? Просто обычному А ик, но умножить на к. Да, это уже не будем, но просто в
этом перемножении у нас все прикольнее, потому что если нам фантастически повезло и для пары IJ
свидетель ровно один, то тогда вот в произведении А с тильдой на B в этой клепочке ровно этот
свидетель написан и будет, правда? Логично, да? Вот, значит, но, то есть это есть, то есть можем
так M от N проделать и главное по клеточкам пробежаться и проверить, не появился ли у нас
свидетель, да. Потому что если нам подсунули свидетеля, потенциального свидетеля, то мы можем
быстро за единицу проверить, он реально свидетель или так прикалывается. Вот, но, конечно же,
свидетелей чаще всего бывает больше, чем один, а нам бы хотелось бы сделать так,
чтобы свидетелей почаще было поменьше. Как же это сделать?
Чего рандомные клетки? Ну вот, да, но на самом деле да, за нуляем.
Значит, идея такая, на самом деле мы будем пытаться делать следующий чит. Мы сделаем
А, значит, смотрите, мы будем, значит, мы для того, чтобы у нас свидетелей было поменьше,
мы будем говорить, что нас временно будут устраивать только не просто свидетели,
а свидетели из выбранного нами под множество. Ну, например, так, ребят, давайте, у вас слишком
много, поэтому сегодня мы будем считать только тех свидетелей, которые рыжие. Вот. Ну, рыжие,
конечно, рыжие мы будем генирируем рандомом, конечно. Вот, потом завтра у нас будут рыжими,
будут у нас те, у кого там... Не, ну это плохой рандом какой-то. Ну, как бы он будет очень мало,
мало у кого триботинка. Нет, это, конечно, вариант, да, кто-то, конечно, пел, да, знаете,
хотя, да, это, конечно, опровержение, потому что, знаете, как это, 33 кроссовка, 33 кроссовка,
33 кроссовка в прихожих у меня, 33 кроссовка, напряги мозги, на тусовке кто-то без ноги. Ну вот,
вы, видимо, с этим не согласны. Нет, ну, может, действительно, пришел трехногий марсианин какой-нибудь,
хотя нет, на марсиане люди, это известно, вот на планете фикс, вот там трехногие живут, да.
Да, нет, ну, выдохнуть, да, но, знаете, выдохнуть надо было, потому что, да, значит, идея такая,
по факту мы будем некоторым образом генерить случайное подмножество элементов 1, 2 и так далее n
и заявлять, что у нас будет такая матрица A с индексом R, знаете, шип такое, то есть A, R и K будет равно K умножить на A и K,
как и раньше, но на индикатор того, что K в этом множестве лежит. Понятно, да? То есть у нас будет
ивристика, допустим, я R сгенерил как-то рандомно, я пока не буду говорить как конкретно, да, но не
даже не важно, просто как-то я сгенерил R, тогда у меня за m от n есть такая ивристика, я перемножу
вот это, то есть найду вот это произведение A, R на B, и тогда я утверждаю, что в каждой клетке тоже, то есть
я утверждаю, что если среди множества R у ежи найдется ровно один свидетель, то он будет найден, правда?
Честно, ну, в смысле по числовому.
Да, m от n это значительно лучше, чем Q. Помним, да, m от n у нас пока от m в степени 2.37, да,
3.754, бла-бла-бла. Ну, как говорится, человечество будет благодарно, если их завтра выкатят,
и главное, если потом условно после завтра не обнаружатся, что как бы это фуфло.
А зачем?
А это не важно, потому что можем сделать k плюс n, а можем просто проверить, так, вот у меня есть
3 свидетель, ну, потому что k плюс n, ну, в принципе, можно, да, ладно, хорошая идея, ну, можно так, а можно это, тут неважно.
Ладно, можно и так, и так, но главное, что если у вас ровно один, ну, потому что, знаешь, там еще, если так
сделать, то есть шанс, что у тебя сгенировалось 3 свидетеля, ты их сложил, там, допустим, ну, в этом
множестве получился 3 свидетеля, ты их сложил, получил номер, который оказался номером 4
свидетеля, который сюда не попал, но он случайно тоже свидетель, да, то, что там написано, ты проверишь
завод единицы, свидетель это или нет, поэтому, то есть, это в данном случае, это означает, что, то есть,
неожиданным образом тебе могут выложить, да, мы, конечно, это не будем учитывать в вероятностях, но,
но, как бы, такая приятность может и возникнуть, но я не знаю, там, а симптотику вряд ли докрутим
в этом месте, ну, значит, думаем. Так, ну, теперь возникает вопрос, значит, что же, как же мы это R будем генерить?
Так, ну, давайте подумаем, вот, значит, давайте будем генерить,
значит, ну, давайте, ну, вот, значит, ну, идея такая, давайте, значит, сгенерим рандомное
подмножество, но так, чтобы размер этого подмножества был равен ровно r, ровно r, что не так?
Р маленькое, конечно. Тогда давайте так, пусть у меня для каких-то и g есть ровно w свидетелей,
вот, понимаете, да?
Тогда давайте подумаем, какова вероятность того, что из этих свидетелей в мое множество r попадет ровно 1?
Так, да, это все интересно, только я ничего не понял.
Ну, w, это вероятность, да, так, да, но он должен быть ровно 1, то есть остальные должны не попасть.
Так, нет, у нас размер r, у нас вероятность распределения не то, что каждое мы берем с вероятностью r,
да, то придется сэшки писать, то есть получится что-то типа c из n-1 по r-1, ладно, n-w,
поделить на c из n по r, потому что нам нужно выбрать одного свидетеля и r-1 не свидетелей.
33 кроссовка, 33 кроссовка, 33 кроссовка, да.
Так вот, давайте пока просто попишем, значит, смотрите, значит, это равно, давайте честно вспомним,
ц incense по k равно там, бла, бла, бла, бла, бла, бла, бла, бла, бла, бла, бла бла.
Видно, да, это на самом деле равно чему?
Это равно wr, ну, я тут как бы могу сказать что это шлеп, шлепы р,
да и а здесь получается n-w factorial n-r factorial делить на что тут у нас еще значит n factorial
значит n-w-r плюс 1 factorial удобно так ну на самом деле можно даже пошлеп-шлепать чуть-чуть
может сказать что это равно w-r делить на n значит почему я и минус столько взял для того
чтобы у меня тут в числителе и в знаменателе осталось по w-1 множителю здесь это будет n-1
n-2 и так далее n сколько там да минус w-r плюс 2 потому что я сократил вот это на это
а и потому что я нагнал как всегда так ну действительно так так да нет нет надо просто
нет как-то надо совсем вправо значит ухожу в стенку значит описываю как выхожу оттуда да
вот давайте здесь напишем вода да кстати чудо как это называется то преражу еще какие
новости в мире ну какие коза сети чипс опять выиграл коза сети чипс да ну там да махом с
очередной там 3 тач дауна оформил как всегда ну ну ну пс господи пчелы хорошо морти хорошо
играет в американский футбол ну ну как сказать там по торбаках хорошо защищают там я думаю в
конзессе там найдутся интеллект на эту тему так значит поехали тут у нас n-1 n-2 пум пум пум n-w плюс
значит тут у нас получится n-w n-w плюс 1 n-w минус 1 хорошо хорошо не паникуем что не так
n-в чем минус а чем кого сократил
да да еще все придираетесь ко всяким мелочам
ну да действительно так сколько там минус double и плюс два
английский формулы боже боже боже мой формулы более более более более более формула эхо ведь
по идее ладно давайте уж добьем это безобразие значит смотрите на самом деле да может надо было
по-другому сокращать все-таки но полу но получите примерно следующие это равно 1 минус r-1
делить на n-1 но n-r поделить на n-1 это на самом деле вот эту штуку
пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе пеппе п
ну-ка подглядим а ух ты я все правильно делал надо же так теперь заметим следующее что это вероятность можно теперь
оценивать снизу как если бы у меня тут так вот смотрите если тут все знаменательы заменю на n-w
что получится у меня это увеличится или уменьшится да смотрите давайте так дробь увеличится дробь
увеличится значит 24 месяца все уменьшится отлично оцениваем снизу это больше либо равно чем vr
делить на n смотрите на 1-r-1 делить на n-v в степени какой там степени получается
ну время садим вот ну вот так что вот оно вот да прям степени в нет а вы именно не там вы
вот значит а теперь фишка такая сейчас какая там сейчас а впрочем
значит теперь просто смотрите а теперь смотрите как бы нет хорошо это мы просто пока в общем
а теперь смотрите а теперь давайте предположим мы ж заранее вот этого w не знаем смотрите фишка
в чем w мы заранее не знаем более того для каждого для каждой пары ежи w свое поэтому мы сделаем так
мы перебили вот поэтому предположим что w-r делить на n лежит на отрезке от 1-2 до единиц
да как что нет нет это мы будем брать мы просто будем перебирать r-r равно 1 2 4 8 16 короче все
степени двойки до n конечно понятно да и что получится тогда смотрите что получается значит
в этом предположении это значит больше либо равно то есть vr делить на n значит ну опять это
равно 1-vr-w делить на что там получается там wn-w в степени w-1 зачем я это написал а ведь
зачем-то я это написал да заметим что wr я могу это смотрите это больше либо равно чем wr делить
на n значит 1-n делить на w в степени w-1 но потому что я wr оценил сверху как n вот
да ну теперь получается шлёп шлёп это равно то есть получается это более значит вот это вот
сделали это еще сказали что это больше либо равно одной второй поэтому получилось что это
больше либо равно 1 2 на 1-1 делить на w в степени w-1 а вот здесь видимо заявим что это все-таки
больше либо равно 1 делить на 2 и неплохо то есть получается то есть вывод очень простой если я
пробегусь по всем r то есть по r равно значит r равно 1 2 4 там 8 16 и так далее и для каждой
из них попытаюсь найти свидетеля вот таким вот нехитрым сгенерю случайное множество и
попробую найти свидетеля вот таким способом да то я потрачу на это время какое я потрачу на это
время m на n на логарифом n и при этом но вот и но вот и тогда при этом у каждого и ж получается у
каждого и ж будет как минимум вероятность как минимум 1 делить на 2 и что в результате этой
операции у них свидетель будет найден понимаете да а с какой вероятностью но вот теперь давайте
проверим с какой вероятностью свидетель не будет найден но вот с какой вероятностью у нас
свидетель не будет найден но получается 1 делить там 1 делить на 2 и но потому что там идея будет
такая что мы там проделаем вот это вот допустим сколько-то раз но вот понятно да но вот ну вот
но потому что понятно что да то есть мы но то есть у нас тут то есть как бы ошибка у нас будет
заключаться то есть ошибка у нас предусвероятостью не более чем 1 минус 1 делить на 2 и да
да ну по сути да ну типа того да но теперь смотрим за 3 если мы сделаем но если мы
сделаем это не один раз а скажем но вот а теперь но вот нет мы не бесконечно повторяем мы
повторяем но на самом деле это мы не бесконечно повторяем нас идея такая мы это сделаем значит
мы это сделаем карас где к будет равно логарифом по основанию 1 делить на 2 и 1 делить на
округлённый вверх
что не так вот вот столько раз мы повторим этот алгоритм
в смысле как
мы же знаем мы можем эти матрицы в тупую умножить за ну да да да да сведее даже
свидетель нужен напоминаю нам не просто надо матрицы перемножить нам надо свидетеля найти
можно но предлагает предлагает но сколько мы каждый свидетель будет висеть непонятно каждый
конкретный поэтому есть такой чит нам на самом деле можем заметить так что есть у нас этих
свидетелей осталось там скажем мало например n то в принципе n свидетелей можно уже ручками
найти правда потому что это n квадрат займет вот поэтому идея такая давайте сделаем вот столько
ну даже вот к аш-трих ладно ладно вот ладно вот вот столько давайте сделаем тогда смотрите тогда
у нас вероятность того что какой-то свидетель не будет найден она получается будет не более чем то
есть один делит этом 1 2 е в степени к это не превосходит просто один делить на n но тогда
математическое ожидание количества ненайденных свидетелей не превосходит 1 делить на n на n
квадрат то есть n мы все то есть оказывается то есть идея то что давайте вместо этого просто в
этом месте остановите я оставшихся просто добьем ручками то есть получился алгоритм в итоге
кст но такой лас веговский лас веговсовский алгоритм шана может не повести почему
сейчас почему почему почему не понял не понял не понял почему лог в клубе
погоди где-то видел бинарное возведение матрицы степень нет если вы за все фиг той задачи то как
бы извинять мы сначала нашли 3 матрицы каждую мы нашли за м за м от налога и в 3 раза мы
теперь просто ищем свидетелей перемножения были воперемножение вот мы это делаем
но вот эти расстояния мы еще логарифом раз пересчитываем да но я просто логку бы не
вижу лог квадрат вижу но да отдельно нет там на высоте там не нужно искать это не делать это
логарифом там не нужно на каждой глубине рекурсии этих свидетелей находить вот вообще не нужно не
нужно расстояние матрицы д вот по модула по модула по модулям 3 тройки мы это зачем делали для
того чтобы свести эту задачу к трем задачам ровно трем задачам о поиске свидетелей
и что но мы это делаем абсолютно без всяких вероятностных алгоритмов абсолютно без всяких
свидетелей с помощью там одного перемножения если не буду повторять лекцию но там было так
там по матрице а и по матрице д штрих прекрасно восстанавливалась д с помощью одного перемножения
чтобы восстанавливать сам путь сама матрица д просто скажет что там расстояние между
42 а куда идти непонятно то есть если точка получается да то есть об этом налог квадрат
тот да но да поиск свидетелей да
но на самом деле давайте кратенько на самом деле тут есть такая последняя бонусная главка то есть
главка заключается в том что на самом деле как бы алгоритм можно улучшить там перемножать чуть
поменьше значит фишка такая
значит сейчас
вот но на самом деле просто есть сейчас значит
там просто да была была дополнительная фишка но просто теперь мы просто попытаемся воспользоваться
тем что когда мы если мы тут какие-то строчки занулили то в принципе можно на самом деле там
точка где-то вот здесь занулили то можно попытаться оптимизировать само умножение
матрицы потому что в принципе заметим что потому что если вы занулили какие-то строчки то в принципе
ну хотя ладно мы за правду ладно мы здесь вы здесь правда занулили не строчки мы здесь
занулили столбцы в принципе это в принципе если мы столбцы занулили то можно заметить что там
какие-то строчки из тех из той матрицы то есть из второй матрицы который мы домножаем тоже
можно выкинуть нет ну давайте ты ну где-то в каких-то эрках это может и помочь просто по
факту мы переножаем там что-то типа матрицу то есть мы переножаем матрицу там нет почему мы
мы не занулили мы не мы не занулили всего р столбцов да почему не важно ну то есть мы
переножаем матрицу n на р на матрицу р на р р на н по сути если мы выкинем все лишние столбцы и
строки ну по сути да вот ну да ну вот ну давайте подумаем матрицу n там то есть вот так то есть
n на r и r на n теперь возникает вопрос ну во первых но давайте подумаем да за какой симпатикой можно
было бы вообще такие матрицы переножить ну может быть можно и там их распилить то под матрицы как-то
правильно запустить этот черный ящик а потом сложить ну просто они поставил бы смотреть фишка
такая то есть то есть на самом деле вы переножаете матрицу вот такого вида где вот каждая таблетка
на матрицу вот такого вида нет это работает за r квадрат на m от n делить на
р в шах квадрат раз мы просто переножаем матрицу там р на р значит в чем значит чем нам это помогает
а вот такой вопрос а мы можем наоборот у этих матриц сложить одну большую то есть
сложить вместе n на 1 в смысле до n на 1 может может я просто уже бегу поэтому сейчас не успеваю
значит смотрите значит тут проблема такая смотрите нет то смотрите давайте то есть
мы первое заметим что каждую фазу который мы казали то есть каждую фазу то есть когда когда
мы прибираемся по r вот этим вот да то мы теперь каждую фазу делаем на за что то есть сумму получается
значит р равно то есть это получается равно чему там м от н плюс сколько там получается так то есть
там но плюс два в квадрате на м от н пополам но вот плюс что там дальше плюс два в четвертой на
м от н делить на квадрате но вот плюс там f шестой на м на н делить на f кубе плюс и так далее
ну на что-то квадрат поэтому 2 в 2 в 3 но вот да да да да да да да да да хорошо двойка двойка все
все все нет просто тут фишка такая утверждает следующее да тут два тоже два тоже два хорошо
теперь просто фишка такая что если м это она то есть утверждение такое что если м называется
равно допустим тета от n в степени 2 плюс какой-то все-таки эпсилон то тогда я утверждаю следующее то
тогда просто идея будет в том что у нас тут сумма получается такая то есть получается n в степени 2
плюс эпсилон плюс значит 2 там нет не n степени 2 плюс эпсилон сколько там получается а ну да то
есть n в степени 2 плюс эпсилон плюс 2 в квадрате на значит значит на получается сколько там
получается n степени 2 плюс эпсилон делить на 2 в степени 2 плюс эпсилон да вот плюс 2 в четвертой
на n степени 2 плюс эпсилон делить на там 2 в степени 2 на 2 плюс эпсилон вот ну и так далее
ну короче говоря там то есть вот это будет равно то есть n 2 плюсoks重要 1 plus один
делить на два в степени эпсилон плюс один делить на два в степени 2 эпсилон плюс Ruth
на 2 степени 3 эпсилон плюс и так далее это равно получается это от n в степени 2 плюс
то есть получается что если м окажется м все-таки достаточно большое то есть мы
передражаем матрицы за все-таки достаточно много то на самом деле никаких
лог квадратов нет там вполне себе логариф то есть если мы конечно научимся
преодолеть матрицы быстрее чем завод столько там скажем за н квадрат лог
n там например вот это все да то конечно лог квадрат вылезет но до этого момента
а симпатически все нормально
вот да но мужин да мы уже к сожалению до рамки курса не позволяет вам этому
научиться был бы конечно кайф но вы все так все на этом все я бегу

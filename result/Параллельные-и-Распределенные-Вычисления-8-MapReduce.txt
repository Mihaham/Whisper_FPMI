Хорошо, давайте продолжать тогда. Вспомним особенности HDFS, потому что мы сейчас будем
работать с вычислениями на больших данных, и надо сначала вспомнить, как мы их храним.
То есть HDFS это, по сути, с точки зрения пользователя обычная файловая система,
но у нее такие особенности. Во-первых, она распределенная, во-вторых, из-за того,
что она распределенная, мы устойчивы к отказам, потому что у нас есть копии, есть реплики,
и мы используем подход write once, read many. Вспоминаем почему, потому что когда мы читаем,
мы читаем в два этапа, и это происходит не очень медленно, а когда мы пишем, мы используем синхронную
репликацию, то есть пока все не запишется, пока на все ноды мы все копии не запишем, мы не закончим,
и запись она может работать очень долго, поэтому стараемся лишний раз не писать и больше читать.
Точнее, что касается этой записи, мы стараемся писать редко, но по многу.
Ну и теперь MapReduce. MapReduce такой, как мы его знаем, он появился в 2004 году, вышла статья
от двух сотрудников Google, то есть в этом году MapReduce исполняется 20 лет. По этой статье вы можете
прочитать математическое обоснование, почему он так работает, и он используется не только в больших
данных, а и в питоне. Вот кто знает, есть мап и Reduce раньше был во втором питоне, в третьем питоне его
убрали. Поэтому вот MapReduce используется очень много где, и не только в питоне, в других языках
программирования. Но мы с вами будем говорить про то, как используется MapReduce в Hadoop. А в Hadoop
он используется так, мы работаем с парами типа ключ значения, и эти пары проходят несколько этапов обработки.
Первый этап, можно назвать даже нулевой этап, это чтение, когда мы данные просто считываем,
и формируем из них пары, в любом случае формируем пары, даже если мы считываем какой-нибудь там
какие-нибудь объекты или просто строки, мы все равно делаем пары, это просто значит, что мы поставим
рядом null, и у нас будет два значения. Потом стадия Map, на вход подается одна пара, и на выходе мы
получаем несколько пар, все это зависит от того, как мы напишем код, может быть несколько пар,
может быть одна пара, может быть не одной. Дальше идет стадия Shelf and Sort, когда мы полученные вот
эти пары группируем по ключам, и у нас получается уже не одна пара, получается группа, у которой одинаковый
ключ, один и тот же. Ладно, идем дальше. В общем, после стадии Sort у нас получаются группы, и с этими
группами работает редьюзер. Редьюзер, в отличие от маппера, он на вход получает не одну пару, а целую
группу пар, у которых одинаковый ключ. И на выходе мы получаем еще другие пары, у которых, может быть,
ключ будет совпадать, может быть, он будет каким-то другим, потому что и мап и редьюз это то, что пишут
разработчики сами. И что мы там напишем, как бы заранее система MapReduce не знает.
Чтобы было понятнее, как это все работает, вот все из вас знают питон, я думаю. Скажите,
как отработает первый кусочек кода и второй кусочек кода, что будет в результате?
Не совсем. Ты, видимо, забыл, как range работает с питон, и он начинается с нуля. То есть range 5
это будет 0, 1, 2, 3, 4. Поэтому map будет нам давать не от двойки до пятерки, а от единицы до пятерки,
и редьюз тоже от нуля до четырех это будет 10. 5 не входит. Это парадигма, с помощью которой мы
можем написать алгоритм какой-то, который будет обрабатывать большие данные. То есть mapReduce это
не какой-то алгоритм, который мы вот отдельно изучаем, это не какая-нибудь еще одна сортировка или
BFS. А это целая парадигма, на основе которой разработан фреймворк, и в рамках этого фреймворка мы
будем работать. Что она должна делать? Она должна получать на вход какие-то данные, их как-то
преобразовывать распределенно, и на выход выдавать другие данные. Это может быть все что угодно.
Мы будем работать в основном с текстом, потому что текст можно сразу увидеть, что было, что стало,
но это могут быть вообще любые данные, любые объекты. Да, любое преобразование.
Слева сверху.
Да, мы независимо обрабатываем. Да, это агрегация. То есть мы независимо обрабатываем кусочки,
а в Reduce мы тоже независимо обрабатываем кусочки на самом деле, но они просто больше. Ну и насчет
того, что любое преобразование можно сделать на MapReduce, ну вообще да, любое, но некоторые
преобразования будут делаться очень больно. За счет того, что мы не можем выйти за рамки
MapReduce, то есть вот что бы вы не хотели сделать, вам придется написать MapReduce. И как вы видите,
вот на входе у нас пары, они однородные. Видите, K1, V1. То есть должны быть однородные данные. А теперь
мы смотрим, у нас есть простая операция join, где уже таблички две и данные у них разные. Что делать
вот в такой схеме, если мы хотим реализовать join с разными данными? Я вам это объясню, будет больно.
Ну и давайте теперь посмотрим немного на практике, как это работает на каких-то уже более
смысленных задачах. И как обычно, если кто-то уже изучал Hadoop или MapReduce раньше, то вы, наверное,
знаете, что самый первый пример, с которого начинают изучать MapReduce, где-либо это WordCount. То
есть мы считаем, сколько раз слово встречается в тексте. И вот у нас есть такой текст, всего одна
фраза. Давайте представим, что эту фразу мы взяли и размножили в 100-500 тысяч раз и получилось много
терабайт. И вот будем с помощью MapReduce обрабатывать этот текст, который внезапно стал очень большим.
Вот из того, что я показал на схеме раньше, подумайте, какой может быть мап здесь.
На единицу мапим, да. То есть нам нужно взять, разбить данные по словам. И раз нам нужно понять,
сколько раз каждое слово встречается, то это вот сколько этот подсчет мы будем делать на
редьюсере. И наша задача для редьюсера данные подготовить. То есть на мапере мы их готовим,
это всякий там препроцессинг делаем, всякие фильтры. На редьюсе мы считаем. Поэтому на мапе мы
создадим пары типа слова единичка, потом их постартируем и потом посчитаем. Получится вот примерно
такая схема. Теперь давайте посмотрим, как это делается в коде. Для этого я сейчас возьму пример,
который у нас лежит на кластере. Это кусочек книги Tolkien на Hobbit. И мы попробуем на нем запустить
мапперы и посмотреть, что будет. Сильмориль он тоже будет позже, просто если успеем дойти.
Давайте возьму какой-нибудь материал.
Вот у нас такой маленький кусочек текста. Как вы видите, он уже обработан. То есть тут нет ни знаков
припинания, никаких скобочек, все почищено. И мы вот с помощью мапредьюса попробуем посчитать
вордкаунт по этому тексту. Сначала мы будем делать маппер. Давайте посмотрим на код этого маппера и
посмотрим, что он вообще делает. Мы это потом будем встраивать в Hadoop. Для начала, чтобы вы
поняли примерно, что это такое, Hadoop это большой фреймворк, в котором можно писать на джаве,
прямо используя API этого фреймворка. Но у нас тут, я так понимаю, джавист, вот один джавист,
он точно здесь есть. А кто еще очень любит джаву? Поэтому раз никто, значит мы будем писать на
питоне в основном. Питон в Hadoop работает так. Так как API питона появилось в Hadoop чуть позже,
это по сути даже не API, а некоторый такой сервис, который называется Hadoop Streaming. Это значит,
что нам как пользователям выдается готовая Hadoop программа, и мы в нее встраиваемся вот такими
скриптами. То есть мы берем не свою программу и туда вскапливаем свои мапперы, редьюсеры,
еще всякие другие элементы. И мы будем сразу писать вот этот код, который будет встраиваться в
Hadoop. Как встроить в Hadoop какой-то код? На самом деле очень просто. Все что нужно,
это чтобы этот код умел читать из консоли и писать тоже в консоль. То есть самые простые
программки, которые вы все писали на первом курсе. И вот этот сервис, да сейчас отвечу,
этот сервис Hadoop Streaming, он позволяет писать код на любом языке программирования. То есть теперь
вам, чтобы писать на Hadoop, как вы позже увидите, вам не надо будет знать джаву. Хотя я бы очень
советовал писать на Hadoop и на джаве. Он просто как бы будет понятнее. Но в целом под Hadoop можно
писать на питоне, на перле я видел код, который на плюсах писали под Hadoop. Но единственное,
что файл, который мы подаем в Hadoop, он должен быть исполняемый. Поэтому если это плюсы,
то надо сначала скомпилировать и подать уже бинарник в Hadoop. Какой вопрос?
Да, мы пишем на питоне. Ну, скажем так, Hadoop, он не очень быстро работает,
но он может переваливать большие куски и большие объемы данных. То есть если сравнивать Hadoop с
какими-нибудь другими системами, которые были до Hadoop, или с каким-нибудь там Pandas, с чем Hadoop
часто сравнивают, то Pandas просто ограничен тем, что он не умеет работать на нескольких машинках
одновременно, а Hadoop умеет. Ну, если люди плюсы не знают, что делать. В компании нету людей,
которые знают плюсы. Это правда. Если мы говорим про большие петабайты данных,
и вы постепенно начнете понимать, как работает Hadoop, на самом деле там очень много накладных
расходов на вот это все распределение, балансировку, выделение ресурсов. И то,
что мы напишем вот этот небольшой код на плюсах или на питоне, это не сильно важно.
А в случае Java, как я уже говорил, Hadoop написан на Java, поэтому у него есть
нативный API, и можно сразу писать на Java без использования стриминга. Правда я видел
код, когда в стриминге пишут на Java. Отдельный вопрос, зачем так делать, но это инфраструктурой
продиктовано было, когда поверх Hadoop делалась оболочка, которая работает на стриминге. Туда
приходил человек, который очень любит Java, писал на Java и подсовывал туда Java файлы. Какие файлы
подсовывать в стриминг? Bytecode, file.class. Давайте посмотрим, что наш маппер делает. Он читает
с потока ввода stdin. Дальше мы получаем строчки, сплитим их по словам и подаем пары слов
единичка. Все очень просто. Давайте посмотрим, как это работает. Пока что без Hadoop, а просто как
это работает в консоли. Вот у нас файл. Вот мы запустили маппер. Мы просто передали данные
через pipe и получили слово единичка. Вот этот код, который на прессе. Давайте еще побольше.
А синий это просто путь. Вот у нас что получилось. Теперь давайте посмотрим на редьюсер. Если мы
работаем в Hadoop, то у нас отработает маппер, после этого данные передадутся на стадию сорт,
они там сгруппируются и после этого мы запустим вот этот редьюсер. А теперь вы сразу увидите
минусы написания кода не на джаве. На семинаре вам покажут коды и на джаве, и не на джаве у вас
будет возможность сравнить, но просто вот то, что я рассказывал в начале занятия. Редьюсер работает
с группами. Вот есть группа, запускается редьюсер и ее обрабатывает. Тут мы не видим группы, тут мы
видим, что у нас опять есть файл, который читает из stdin и начинает вот эти вот пары слова единичка
последовательно обрабатывать. То есть нам выдан кусок данных и нам нужно самим отслеживать,
где одна группа заканчивается, где начинается вторая. Кто может показать в коде, в каком месте мы
этим занимаемся. Не, можно на плюсах. Вот между разницей между написанием на питоне и написанием
на плюсах никакой нету. Вы можете домашние коды на семинарах делать на плюсах. Вообще не проблема.
Мы сможем проверить, инфраструктура сможет съесть и джаву, и питон, и плюсы. На джаве оно как-то
понятнее для тех, кто джаву уже знает. Лично мой опыт. А для тех, кто джавой не хочет заниматься,
там уже нет разницы питон, плюсы или что-то еще. Кто может сказать в каком месте мы занимаемся тем,
что отслеживаем изменения группы. Где трай? А где там изменения группы? Там континью. Где трай,
что мы делаем? Мы считаем строчку, проверяем, что ключи значения валидные. Если они у нас не
валидные, значит мы просто пропускаем эту строчку. Какой алгоритм у нас идет? На вход
приходят пары слова единичка, мы их получаем и накапливаем счетчик. Вот то, что там видите,
word sum равно нулю. И мы вот этот word sum накапливаем. Потом в какой-то момент у нас изменяется ключ,
то есть было слово ходуб, ходуб, ходуб, ходуб. Слова ходуб приходили, после этого пришло слово джава.
Это значит, что у нас закончилась группа и мы начали работать с другой группой. Мы не сортировали,
это ходуб отсортировал. Если мы вот эти коды подаем в ходуб, то у нас есть... Да, ходуб сам
сортирует между маппером и редьюсером. Сейчас мы просто не подключили ходуб, мы запустили маппер
без ходуба. То есть мы чуть позже научимся, как это встроить в ходуб. Сейчас мы просто рассматриваем
этот код, запускаем, убеждаемся, что он работает, а следующий шаг мы научимся запускать это дело в ходубе.
Да, мы не подключили ходуб, поэтому мы делаем вот так.
Команда sort? Я вас еще больше обрадую или расстрою, есть команда board-counter,
называется wc. Да, через wc можно было подать, но в какой-то момент у нас бы просто оперативка
кончилась, если бы здесь был не маленький кусочек текста, а вся википедия, которую мы
выгрузили просто и сохранили где-то в hdfs. Слов мало, нам-то надо все это прочитать,
нам надо все это прогнать через маппер и потом через редьюсер. Ну конечно. Да, ну там как бы мы
через через pipe работаем, поэтому вариантов нет. Ну опять же ты можешь написать код, который будет
сохранять это файл периодически, но это уже надо код писать. Зачем его писать, если у нас для этого
есть ходуб. Вот, поэтому вот с таким выводом мы работаем и дальше на этом вот деле запускаем редьюсер.
Ну да, конечно. Ну опять же, как ты напишешь, но факт остается фактом, тебе нужно весь аутпут
где-то сохранить. Нет, тебе все равно нужно будет его хранить в оперативке, это может быть какой-то
буфер, там что-то еще, но на диск у тебя не будет ничего писаться само без твоего участия.
То есть ты хочешь подключиться к этому буферу и как-то его читать по частям. Ну окей, а если у тебя
просто диска не будет хватать, что ты тогда будешь делать? Окей, поэтому давайте посмотрим.
Действительно, вот здесь, если мы посмотрим на вот эти ифы, то в какой-то момент группа у нас
меняется. Что мы делаем, когда изменилась группа? Мы обнуляем счетчик, мы выводим результат,
который у нас был принтом и переходим к следующей группе. И вот так по циклу мы обрабатываем
все данные, пока они не закончатся. Вопрос, а зачем нужно что-то после цикла? Вот видите, цикл 4
закончился, и вот этот вот в конце две строчки, кусочек этот. Последнюю группу так-то выведет,
в плане у вас же триер сейчас на то, чтобы выводить результаты? Изменение, да. А для последней такой? Все
правильно, то есть если мы... ну понятно, что последнюю группу мы не успели отследить, когда она
поменялась, потому что она не поменялась. Она выводилась, и потом данные кончились. Поэтому нам
надо последнюю группу вывести вот этим последним IF. Теперь давайте соберем наш пазл и запустим
MapReduce. Все еще без Hadoop, она уже целиком. Вот, редьюсер добавили. Давайте сделаем head 10 строчек.
Что у нас получилось? Ну пока ничего не понятно. Какие-то слова и какие-то цифры. Давайте отсортируем.
И сортировать мы будем теперь не по первому полю, а по второму, потому что нам нужно... ну если мы
считаем в WorldCount, нам, наверное, интереснее всего понять, какие слова встретились чаще всего. Не
просто вывести какие-то рандомные слова и их каунты. Поэтому сортируем по второму ключу. В обратном
порядке... Чего? Ну можно и так, в принципе. Я просто привык работать с sort с ключиками, потому что в Hadoop
тоже есть sort с ключиками. Там свой есть sort, в котором сделали такие же ключики. Давайте уберу head.
Ожидаемо, что у нас чаще всего встречаются все предлоги, союзы и прочие стоп-слова. Если немножко
увеличить наш head, то мы увидим, что дальше после стоп-слова идут какие-то осмысленные слова,
что там болото вышли, вылезли, что-то такое. В общем, мы видим, что книга, видимо, какая-то про
путешествие. Вот мы даже не читая книгу, определили, о чем она примерно говорит.
Вот такая у нас получилась штука. И кажется, что никакой Hadoop нам не нужен. Мы можем вот такие
pipeline писать, и все будет хорошо. Но у нас в какой-то момент кончится оперативка. Если мы даже
с этим как-то справимся, у нас в какой-то момент кончится диск на машинке. Придется параллелить.
Можно писать на MPI и параллелить руками. Вы уже все почувствовали, что такое MPI,
как можно параллелить процессор руками. И, наверное, не очень хочется это делать.
Почему? Тебе нравится MPI?
Книжка ничего не занимает, а библиотека книжек уже занимает.
Поэтому нам придется все-таки использовать Hadoop и какую-то вот такую схему придумывать.
Давайте посмотрим, как она работает, как устроена обработка данных уже в Hadoop. На вход у нас
подаются теперь данные не с файла, а с блоков. Вот у нас блоки серые с левой стороны. Дальше
запускается маппер, и каждый блок независимо обрабатывается своими мапперами. Причем могут
быть даже несколько мапперов. После этого запускается стадия Shuffle and Sort. Данные передаются,
пока не будем особо разбираться, как они внутри передаются, так чтобы на редюсер они пришли
с одинаковыми ключами, но на редюсер они приходят с одинаковыми ключами. Мы видим K1, K2, K3. Три
блока ключей и два редюсера. Тут важно понимать, что у нас никогда не может быть такого, чтобы один
и тот же ключ попал на редюсер 1 и на редюсер 3, например. Все одинаковые ключи будут на
одном редюсере находиться. Ну и дальше запускается редюсер, и выдается результат уже в HDFS.
Если какого-то ключа очень много, то это все плохо? Все плохо, мы упадем. Если половина ключей, то это один и тот же ключ?
Это называется несбалансированными данными, когда у нас действительно по какому-то ключу перекос.
Допустим, мы можем какую-нибудь предметную область взять, например, анализ зарплаты IT-специалистов
по городам России. Где-то в топе будет Москва, Питер, то есть это большие блоки с большим
количеством данных по ключам. И такие данные называются несбалансированными, когда у нас процентов
70-80 находятся на одном ключе. Тогда это все приходит на один редюсер, не помещается, и мы падаем.
Что в этом случае делать, как вы думаете? Не так. Это от нас не зависит, то есть Hadoop
группирует данные по ключам сам. Мы написали маппер, идеально хорошо оптимизированный редюсер тоже,
но для того, чтобы работал редюсер, надо, чтобы на него подалась группа с одинаковыми ключами.
А как мы это сделаем, если у нас... Что такое в двух местах? Двух местах это значит вообще
на разных серверах где-нибудь. На этих разных серверах запускаются редюсеры.
Да не в этом дело. Дело в том, что у нас редюсер это один шаг. То есть если у нас данные хранятся
на разных серверах с одинаковыми ключами, мы их там поредюсим, а потом нам их надо собрать вместе.
Как мы это сделаем? У нас кончилась стадия редюс. Для того, чтобы что-то еще доделать после
стадии редюс, нам надо запускать новый мапредюс. То есть на самом деле, если мы посмотрим на реальные
всякие программы, которые в компаниях пишут на Hadoop, на Spark, то там будет целая цепочка.
И вот один этот мапредюс, который мы сейчас разбираем, это только вершинка графа. Этот граф
отчисления может быть очень большой на десятки-сотни таких мапредюсов. Но каждый новый мапредюс это
большие затраты по ресурсам. Потому что куда мы пишем данные после того, как редюс закончился? На
диск. Маппер должен с этого диска считать. Потом, когда мы данные сортируем и группируем, это все
опять происходит через диск, как вы увидите. То есть мы постоянно работаем с диском. Плюс нам нужны
постоянно ресурсы на то, чтобы поддерживать джаву и все, что с ней связано, всякие там джава хип и прочий
горбач коллектор. Поэтому лишнюю джобу все-таки делать не хочется, лишний мапредюс. Поэтому делаем
так, чтобы мы могли за один такт средюсить данные с одинаковыми ключами. Итак, если у нас все-таки не
хватает памяти и ресурсов редюсера, чтобы этот ключ съесть, что можно сделать?
То есть как-то его дополнить чем-то. Да, вот это и называется подсаливание, когда мы берем ключ и
добавляем к нему, например, какую-то рандомную соль. У нас будет K1-1, K1-2 и ходу будет думать,
что это разные ключи. Можно сделать это как-то более умным. Если вот я говорил про Москву,
то можно Москву разбить на регионы какие-нибудь и добавить еще это к ключу.
Такую проблему как-будто можно увидеть только если глазами. Запустить, увидеть, что все сломалось,
потому что данные не сбалансированы, понять, что надо что-то там...
Ну, это задача дата-инженера. Просто увидеть это заранее можно всякими иллюристическими методами,
вплоть до того, что ты понимаешь, что у тебя предметная область такая, что, наверное, в основном
там будет Москва присутствовать. У тебя будет 80% данных, ключ равно Москва. Значит, мы эту Москву как-то
дробим еще. Я думаю, что такого фреймворка, который вообще всегда будет работать стабильно,
вы, наверное, даже не найдете. То есть вот если брать так поверхностно самые известные такие
фреймворки, то вот у ходу побывает проблема с ключами и с тем, что он в целом не очень быстро работает.
У Spark'а бывает проблема с тем, что он течет по памяти и не хватает оперативки, начинает падать.
У Hive'а бывает проблема с тем, что он не очень оптимально превращает SQL в MapReduce,
но мы это все тоже будем разбирать. Поэтому у каждого такого фреймворка есть проблемы,
надо просто знать особенности и как-то с ними справляться.
Вот здесь еще справа на слайде написано блок неравного блоку в HDFS. Что это значит?
Вот давайте вспомним, как HDFS разбивает данные по блоку, на что он обращает внимание при разбиении.
Ни на что. Все молчат, потому что ни на что, только на объем. Да, конец оптимизировать,
но тоже потому что там объем, мы не хотим хранить маленький блок. То есть мы смотрим
только на количество байт и больше ни на что. А теперь у нас каждый блок идет на один какой-то
маппер отдельный. Давайте представим, что мы обрабатываем музыку или видео. Все, наверное,
сталкивались с таким, что вы записываете какое-то видео и у вас в конце там запись прерывается,
вы последнюю секунду не успели записать, но из-за этого весь файл, который у вас может быть 2 гига,
видео не читается. Сталкивались с таким? Те, кто снимает и монтирует, я думаю, точно сталкивались.
Поэтому если мы будем класть файлы в HDFS, разбивать их с точностью до объема, то окажется,
что у нас вот это разбиение, оно с концом файла совпадать не будет. И вот мы первый блок считали,
а там ни начала ни конца файла нет. Мы его не можем вообще никак, ну ничего с ним сделать,
с этим куском, который мы прочитали. С другим блоком точно так же. Что можно сделать в такой
ситуации, как вы думаете? То есть когда мы файлы кладли в HDFS и потом читали, просто HDFS,
DFS-GET, скачали файл, было все хорошо, потому что при чтении мы все блоки считали, они собрались
опять в кучку и у нас получился целый файл, как он был. А здесь так не получится, здесь каждый блок
передается на один отдельный маппер, они находятся в разных процессах, на разных машинках,
когда они, ну они уже никогда не встретятся, эти кусочки файлов. Нам придется их обрабатывать
независимо. Что можно в такой ситуации сделать? Мы возьмем файл, добавим чисто какой-то мишанины
в конец, и за счет этого будет чуть больше. А если у нас слишком маленький блок, то есть файл больше,
чем блок, что тогда делать? Давайте очень много данных в конец. Нам не хватает примерно вот этого,
что у нас есть. Вот смотрите, вы сохранили в HDFS наши лекции по курсу. Вот у нас тут первая,
вот у нас тут вторая, вот у нас тут третья, вот вы это сохранили в HDFS. А HDFS у нас бьет данные по
блокам. И здесь он разбил вот так, здесь вот так, здесь вот так, и вот как-то вот это не совпало.
Что делать? Мы понесли вот этот блок на маппер, а считать мы это дело не можем, потому что оно не
закончилось. Здесь вообще все плохо, мы получается вот этот кусочек не можем прочитать, и вот этот
кусочек не можем прочитать. Да, у нас будет много маленьких блоков, потому что вот эта штука,
это будет отдельный блок. Вот это тоже. Это если файлик меньше, чем блок, а если больше?
Мы можем, но это придется делать руками, то есть не хочется руками делать. Вот выше вас сидит человек,
которому не нравится руками делать. Маппер берет на вход какие-то данные, независимо обрабатывает
их и передает дальше. Ну грубо говоря, блоками HDFS, 64-128 мегабайт, не килобайт. В ходу есть такая
штука, как вот в скобочках написано, называется она Split. То есть вот главная мысль то, что разделение
не должно испортить формат, чтобы мы эти файлы вообще могли прочитать как-то. Поэтому когда мы с
помощью маппера читаем данные из HDFS, мы обращаем внимание на формат данных. То есть мы читаем не
блок, а мы читаем Split. И в ходу можно специальным образом с помощью специальных аргументов подать
формат данных, с которым мы работаем. Это может быть видео какое-нибудь, архивы. Если брать некоторые
архивы, то там тоже структура данных блочная. И если мы вот этот блок пополам разрубим, то мы его не
сможем прочитать ни на одном маппере, ни на втором. И нам надо вот эту границу сдвигать. Видите,
такую вот схемку, типа того, что я на доске нарисовал. Внизу у нас блок boundary, то есть то, как HDFS
разбьет данные по блокам. А вверху у нас сплиты.
Смотрите, мы не можем разбивать на блоке как хотим, мы можем задать размер с плита. Размер блока мы не
можем с помощью MapReduce изменить вообще, мы можем задать размер с плита. Что в этом случае будет
делать мап? Размер плита это тот объем данных, с которым будет работать маппер. То есть блок
плюс еще какой-то кусочек или минус какой-то кусочек. И в этом случае можно будет подстроиться под
вот эту вот границу. То есть вот смотрите, у нас какой-то вот архив здесь, TAR BZ2 или что-нибудь еще,
и нам нужно обязательно эти маленькие блоки синие не порезать. И вот сплиты позволяют эти блоки не
порезать. Если будет что-то очень большое, то мы сможем размер сплита задать больше,
если там уже мы упремся в какой-нибудь лимит, тогда придется все-таки руками данные процестить. То
есть придется упаковывать их в пакеты нужного нам размера. В принципе, это умеют архиваторы,
я знаю, что всякие архиваторы еще лет 15 назад имели такую функцию, что можно было большой архив
разбить на какие-то пакеты, которые потом можно было восстановить. Здесь какие-то вопросы есть,
просто дальше уже будем разбирать с вами, как работает сорт между мапом и редьюсом.
Теперь давайте посмотрим, как мап редьюс технически работает более подробно. В конце я вам покажу
схему, если успеем, там она еще более страшная, то есть как сам ходу работает внутри. И вот получается,
что мы считали input-split, это вот тот самый блок с поправкой на формат файла. Мы его считали,
добавили в маппер, маппер его обработал и данные сложил куда? В буфер. Но буфер может переполниться.
Что мы тогда делаем? Тогда мы сбрасываем данные на диск. И вот если вы посмотрите на четвертый вот
этот элемент в схеме, то вы увидите много кусочков данных на диске. То есть маппер работает, пишет
данные в буфер, буфер переполняется, пишется на диск. Снова буфер переполняется, пишется на диск.
И так происходит много раз. Дальше мы вот эти маленькие кусочки слопываем в один и одновременно мы уже
здесь начинаем стадию сорта, то есть мы проводим группировку. Вот до этого момента какие-то вопросы
есть? Или пока ничего не понятно? А вам? А какой вопрос есть?
Ну да, вообще понять зачем и что там происходит.
То есть самое главное то, что так как маппер мы пишем сами, то Hadoop никак предсказать не может,
что у нас из этого маппера в итоге выйдет, какого размера будут данные. То есть на вход
мы подали один сплит, его размер мы знаем и предсказать можем. Что будет на выходе мы не знаем,
поэтому нам надо пойду смотреть механизм, сохранять данные сколько бы их не было. Поэтому вот буфер,
который переполняется и сбрасывается на диск, и в итоге мы получаем вот такой вот файл. Дальше у нас
запускается такая штука как partitioner. Это еще один процесс, который занимается распределением
вот этих вот данных по юсерам. То есть вот вы видите один файл, в котором уже данные сгруппированы
по ключам. Дальше мы запускаем partitioner, который по умолчанию работает вот с такой формулой.
Вот, то есть берется хэш от ключа и делим по модулю на количество редьюсеров. В ответе получаем
число и это число будет индекс редьюсера, то есть индекс той машинки, куда мы данные отправим. Вот
вы видите стрелочки, красные, зеленые. Что это значит? Это значит, что вот этот кусочек пошел на первый
редьюсер, плюс на первый редьюсер пошли данные еще откуда-то извне. И точно так же вот эта первая
машинка отправила данные куда-то еще в другой редьюсер. Это уже следующий этап, потому что вот
мы эти все данные отправили на машинке, и они на машинке хранятся в виде чего? В виде маленьких
файлов. И нам перед тем, как с ними работать дальше, их надо собрать вместе. Поэтому вот merge.
Ну так да, вот то, что слева мы с вами только что проговаривали, что когда у нас буфер переполняется,
мы скидываем данные на диск, и мы это делаем много раз, у нас получаются маленькие файлы.
Потом нам нужно их собрать в один, попутно сгруппировать. После этого мы их разносим по
разным редьюсерам, стрелочками, и у нас ситуация повторяется. Опять маленькие файлы, которые опять
нужно собрать вместе. В виде файлов на диске. В виде просто файлов на диске.
Другой файл. В каком виде, что именно под видом понимается? Формат какой или что?
Что изменилось после мапа? Данные прошли через маппер. Например, мы считали строчки и сделали пары
слова единичка. То есть преобразование какое-то получилось. Данные другие.
То есть у тебя функция изменилась к каждому элементарному объекту, а потом все что дальше мы отскидываем правильно по разному.
Ну условно мы на вход подаем массив какой-то, и считаем в нем максимум минимум, получается пара
максимум минимум. Вот уже другие данные. Все что угодно. Мы маппер и редьюсер пишем сами,
поэтому там может быть все что хочешь. Конечно. Тоже забегая немного вперед, у нас есть распорядитель
ресурсов, который называется Ярн. Он выделяет контейнер. Это определенный набор ядер процессоров.
Ярн контейнер это набор ресурсов, на которых крутится Java. И это не очень
большой набор ресурсов. На любом сервере этих контейнеров может быть несколько десятков.
Очень много. Это сколько, например? Ну вот объем одного ярн контейнера это где-то там 2-3-5 гигов.
То есть вот примерно такой набор. На машинке может быть например 250 гигов оперативки, а в контейнере
может быть 5 гигов оперативки. Да, у нас на кластере это 1-1,5-2 где-то так. В жизни бывает 5-10.
Мы склеиваем в несколько файликов, как ты видишь два файла.
Один ключ. Несколько разных ключей в одном файле. В принципе это может быть, но нам даже не надо
так глубоко залазить, потому что мы работаем с директориями. Если пишем на Java, то да. Если пишем на
Python, то надо будет. Мы будем считать, что нам выдали некую группу данных, и данные эти отсортированы.
То есть ключ приходит к 1, к 1, к 1. Если пришел к 2, то к 1 больше не встретится. Вот такая вот штука.
Почему? Если они отсортированы по хэшу, то хэш для одинаковых ключей выдаст одинаковое значение.
Зачем проверить? Ты к тому, что в ходу может быть плохой хэш? Плохая хэш-функция? У вас какие-то
вопросы? Давайте это после пары все-таки. Вот, можете посмотреть. Вот у нас данные были в буфере,
потом попали сюда, и мы их собрали здесь, в итоговом файле. И вот как работает partitioner.
Тоже. Вот у нас есть несколько кусочков. Мы их проводим через функцию вот эту хэш от K,
и получаем уже сгруппированные данные на reducer. Теперь давайте посмотрим, что делает ходуб,
если что-то падает, и как он вообще с точки зрения отказаустойчивости себя ведет. И тут надо
сначала договориться о терминах. Если мы имеем дело с большой программой, в которой много job,
друг за другом идущих, то это называется application. Собственно, один такт от начала мапа до конца
reducer это называется job. Один синий квадратик, один mapper или один reducer называется task.
Если будете читать русскоязычную литературу, там job и task переводят странно и перемешивают
между собой, поэтому вот еще одна причина, чтобы ходуб читать на английском. И у task может быть
попытка, может быть несколько попыток, несколько отемтов. Давайте посмотрим, что происходит,
если происходят отказы, и вообще какие могут быть в ходубе отказы. Что может произойти такого,
чтобы случился отказ? Ну, name-надо упало, и это все сразу ничего работать не будет. Я имею
в виду отказ на стороне вот этих задачек, что может произойти. Переполнились по памяти,
закончился диск, отвалилась дата нода. Обработчик, ну код некорректный просто. Мы написали плохой код,
но это в итоге тоже какая-то часть нод просто отвалилась. Ну вот что происходит,
если у нас отвалилась какая-то нода одна или какая-то часть нод. Каждая нода, она шлет
хардбиты в name-ноду, и если хардбиты мы не получаем какое-то время, то мы считаем,
что эта нода упала и начинаем пересчитывать на другой. То есть мы посылаем следующую попытку.
Благо у нас есть репликация, то есть мы стараемся запускать мапперы там, где лежат данные. Вот лежат
данные на ноде номер один, мы запускаем там маппер. Если нода номер один упала, то мы запускаем
маппер на ноде номер десять, где лежит реплика этих же данных, которые надо обработать.
Да нет, потому что данные лежат на диске, а мапперы работают в оперативке.
Опять же подходы есть разные. Сейчас есть разные подходы к облачным вычислением,
когда у нас эти сервисы разделяются, и отдельно у нас компьют, отдельно у нас дата, дата ноды.
Вот можно посмотреть там Яндекс.Облако, у них есть такой сервис, который называется Яндекс.Датапрок.
У них отдельно можно сделать дата ноды, отдельно компьют ноды. С одной стороны это хорошо, что никто
ни у кого ничего не отжирает, с другой стороны это сеть. То есть теперь нам нужно данные по сети
передавать. Мы все-таки пишем на диск, это правда, и здесь нужно правильно настроить сам ходу бкластер,
чтобы на дата ноде, если там еще могут быть мапперы, то на дата ноде мы не давали возможность
занять все 100% ресурсов. Обычно вот как у нас настроен кластер, например, если 85% занято,
он начинает уже там пинговать, ципать аллерты, что что-то не так. Поэтому перезапускаются попытки,
и вот вы правильно сказали, что сбоем можно считать и то, что наш код упал. Хадуб, он вообще это не
разделяет. То есть если код, который мы запустили, ответа не выдает, то хадубу все равно,
это нода упала или это код упал, он просто думает, что проблема на его стороне и начинает перезапускать.
Поэтому когда вы будете в домашке писать неправильный код, вы будете наблюдать то,
что его запускает хадуб несколько раз на разных нодах. В случае нашего кластера это четыре.
Четыре раза запустились, получили ошибку, упали.
У нас храбрая программа, она может что угодно делать. В теории она может работать слишком долго.
Да.
Но можно же вести какое-то адекватное ограничение, что на каком-то дреме дамы программа должна работать слишком долго.
Не должна работать слишком долго? Да, такое ограничение.
Есть ли в ходу перепланировки сетев?
Конечно есть. Причем и по времени, и по ресурсам. Когда мы будем ярн разбирать,
вам на семинарах более подробно расскажут, но в целом вот у нас есть ярн-контейнер.
Когда нам не хватает ресурсов, мы идем к главному ресурс-менеджеру и говорим,
мы хотим еще ресурсы. Ресурс-менеджер нам дает еще ресурсы, дает, дает, дает.
Какое-то время контейнер растет, после этого ресурс-менеджер говорит все, и контейнер убивается.
Точно так же, если он у нас работает очень долго, не шлет хорбиты, то вылазит ошибка,
типа weight-output-threads, то есть мы не дождались никакого ответа, и контейнер тоже убивается.
Что значит не шлет хорбиты? В какой момент шлет?
Хорбиты шлются, то есть внутри ходупа есть такой специальный процесс, который считает,
сколько данных мы прочитали, сколько данных мы записали. И если у нас в какое-то время перестает
меняться количество прочитанных и записанных данных, то хорбитов нет,
но и что-то пошло не так, и ходуп понимает, что, наверное, все зависло.
Он начинает перезапускать на других нодах, и на самом деле это помогает иногда. Почему?
Потому что может быть такое, что наш код, например, использует библиотеку Panda с какой-нибудь
специфической версией, а не очень добросовестный админ поставил эту библиотеку только на некоторых
нодах, на остальных обновить не успел, и вот мы перезапустились и где-то рандомно
наскочили на правильную библиотеку и, возможно, отработали.
Он сначала бьет на блоки, потом разделяет по нодам. То есть когда мы перезапускаем код на другой
ноде, мы перезапускаем там, где будет точная копия этого блока. Ну сплиты тоже, соответственно,
точная копия этого сплита. Давайте подведем небольшой итог промежуточный. То есть мы работаем
с парами ключ значения. На мапе мы поэлементно обрабатываем данные, дальше группируем,
сортируем по ключам и на reduce обрабатываем группы. Если говорить, чтобы было понятнее в
терминах SQL, то мап — это какая-нибудь такая функция, для которой не нужны соседние записи в
таблице. То есть вот у нас табличку считаем, считываем, если мы подключаем условия where,
field 1 меньше 5, то никакие другие строчки нас не интересует. Мы просто смотрим,
подошла строчка или не подошла. Точно так же с селектом. Вот это все маппер. Если мы говорим
про джойны, агрегации, всякие оконки, вот это все уже редьюсеры. Ну и теперь с точки зрения того,
где какие процессы работают, мы запускаем программу. Программу мы запускаем на клиентской машине.
Дальше, когда она попадает в ходу, происходит несколько форков и соответственно мапперы и
редьюсеры запускаются на нодах. Нам их надо как-то докатить до нод, поэтому мы эти коды,
если мы пишем на стриминге, то вот эти Python файлики мы должны докатить до нод, а собственно
положить в распределенный кэш. Распределенный кэш это не HDFS, это дополнительная такая папка на
датанодах, где эти файлы хранятся, и мы можем прямо оттуда их запустить. Вот запускаем мапперы,
запускаем редьюсеры и обратите внимание, что промежуточные данные, которые между мапперами
и редьюсерами, они хранятся не в HDFS, а просто на диске. Почему, как вы думаете,
почему не в HDFS? HDFS как бы надежнее. Ну да, мы будем копировать с нодом на ноды,
но почему мы не хотим сохранить эти данные в HDFS для большей надежности? Там есть репликации,
отказа устойчивости. На неймноде считаем результаты?
Ну вообще не будет ли странно, что мы оперируем с данными HDFS и результаты тоже в HDFS?
Ну мы оперируем данными на ноутбуке, на диске и кладем данные на этот же диск,
ничего странного не будет. Мы же покладем другую папку, какая разница? Да, зачем? Потому что
скорость. Вы сказали про временные данные, действительно вы правы, нам эти данные нужны
только до конца работы джобы. Какой смысл нам их сначала класть в HDFS, ждать, пока write once,
read many, пока репликация дойдет, потом опять читать и снова ждать, когда просто можно
с диска на диск перекинуть. Теперь стриминг. Чтобы вам не сильно скучать, мы скоро посмотрим на пример,
больше будет на семинарах, но я вам хотя бы покажу как в принципе это все работает. Пока посмотрим на
стриминг. То есть Hadoop написан на джаве, у него есть хорошая нативная java API, но как я уже сказал,
если мы не хотим писать на джаве никакую, то приходится работать вот с такой вот штукой. То есть у
нас имеется готовая Hadoop программа, куда нам нужно подкинуть вот эти вот мапперы и редьюсеры. С одной
стороны мы получаем полную свободу действий, то есть Hadoop нам выкидывает кусок данных и говорит
обрабатываете вы их чем хотите. Любой исполняемый файл, вы туда можете, любой бинарник, лишь бы
операционка ваша могла его выполнить. Вот это все что надо, но и помимо этого надо, чтобы он умел
писать, читать в консоли. В Hadoop есть специальный аргумент default separator, можно его указать,
это может быть точка запятой там для CSV файлов всяких. По умолчанию это tab, ну и соответственно
можно указать еще и формат хранения файлов и в этом формате этот default separator сохранить.
Да все что угодно может быть, например есть библиотека, которая анализирует звуковые файлы и
можно сделать так, что разделителем будет соответствующий звук какой-нибудь там хлопок,
можно и так сделать. По умолчанию в Hadoop это tab и для нашего курса хватит таба, если вы захотите
там какие-нибудь в своей научке данные попроцессить, у вас скорее всего будет
какие-нибудь CSV, то там вот точка запятой поставите. Что? Ключом что ли? Это разделитель
между ключом и значением, а так ключом может быть что угодно. Все что хочешь, это просто объекты,
джевовые. В итоге вот Hadoop стриминг выглядит он вот так. Сейчас мы это все запускаем,
посмотрим, но вы по крайней мере сможете сказать всем своим коллегам, что Hadoop очень простой,
потому что запускается он с помощью одной команды, вот это вот одна команда. То есть вам чтобы
запустить MapReducer, вам нужно по сути вызвать одну команду, ну и туда поставить mapper,
reducer, все эти настройки, команда все равно одна. Давайте посмотрим, что в этой команде есть.
Для примера запуска давайте разбираться пошагово и заодно будет понятно как и на джаве,
потому что это на самом деле тоже на джаве. Там джар есть, просто он не наш. То есть кто умеет
писать на джаве пишет свои джарники, а кто не умеет писать на джаве, пользуется чужими джарниками
и их настраивает. Если грубо говоря, то вот так все работает. Поэтому вот есть yarn, наш
распределитель ресурсов, команда yarn-jar, туда мы подсовываем джарник, соответственно свой или тот,
который вошел в поставку Hadoop и создаем параметры. Название джабы, количество редюсеров, всякие там
маперы, редюсеры, настройки, дополнительные файлы, которые мы туда прикрепляем, обязательно пути
для входа и для выхода и вот получается вот такая у нас штука, вот такая команда. Сейчас мы попробуем
ее запустить. Вот так она у нас выглядит и сейчас конкретно эту программу будем
запускать. Видите, мы с вами написали маппер, написали редюсер и сейчас мы их просто сюда
поселили и запустим. Давайте посмотрим. Есть, ну и запускаем. Что мы видим?
Вот смотрите, мы видим number of splits равно 2. То есть это значит, что у нас два блока на входе.
У нас сейчас есть другой курс, где очень активно ребята сдают сегодня домашки,
поэтому может быть какое-то время оно будет тупить.
Можно пока воспользоваться вот этим.
Да, тут у нас сейчас 16-16, поэтому прямо сейчас мы, к сожалению, не увидим, как задачка работает.
Какая разница? Это же хадуба, то есть каждая таска хадуба,
для таких случаев, когда у нас кластер перегружен или когда мы просто не хотим ждать,
пока Java все это будет запускаться, есть псевдораспределенный запуск. Вот если вы посмотрите на этот код,
что поменялось по сравнению с тем, что я показывал на слайде? У нас поменялось вот этот конфиг,
если вы там видите, переменная конфиг, которую мы везде подставляли. То есть это специальная
настройка, которая позволяет запустить хадуб в псевдораспределенном режиме. Это значит,
что он как бы распределенный, но все работает на одной ноде, конкретно на клиенте. Там
запускаются и инстанс-неймноты, и инстанс всех этих обработчиков, и все это работает без участия
большого глобального хадуба. Он разворачивает так называемый мини-кластер хадуб, то есть это урезанная
версия. В этом случае, чтобы воспользоваться такой штукой, нам нужно скачать с хадуба
сэмпл данных, поместить его на диск, потому что да, псевдораспределенный режим работает,
в отличие от большого хадуба, он работает не с HDFS, а с диском, просто с диском на этой машинке.
Вот видите, тут написано input in. Что вот это за in? А вот он. Вот у нас in, папка. Давайте посмотрим,
что в этом in есть. Есть, есть кусочек, довольно большой кусочек википедии.
Ну и вот собственно у нас тут все посчиталось, и мы видим, мы тут видим разные счетчики,
вот можно полистать этот лог и посмотреть, что у нас было прочитано, вот столько байт записано,
вот столько, вот read-write. Вот commit-memory, used-memory, то есть вот в этих логах можно подробнее
посмотреть, сколько памяти было выделено, сколько было заиспользовано, сколько было
утилизировано памяти, все это можно здесь увидеть. Есть ли какие-нибудь вопросы? Чего? А,
результат будет в папке out. Нет, почему сломалось? Мы его даже увидели, этот результат, потому что
в команде есть, то есть в нашем файле есть команда HDFS dfs-cat. Вот такой вот результат. Просто слова
у нас не почищенные конкретно вот в этих данных, они не почищены от всяких там точек запятых и так
далее, поэтому вот такой вот мусор, но все равно мы посчитали количество по каждому слову. Тут
какая-то даже не по слову, а по каждому токену. А топ нужно дополнительно отсортировать, топ нужно
дополнительно отсортировать, то есть когда я вам показывал команду, да здесь ее нет, потому что
это еще одна джоба. Вам на семинаре покажут подробнее, вам нужно сделать сорт после редьюса. Для этого
нужно запустить еще одну джобу, которая будет как бы с пустым маппером, с пустым редьюсером,
но там будет сорт, то есть это следующий шаг уже. Да, вот что касается вывода, вот аутдир,
вот здесь она будет сохраняться.
Какие-нибудь вопросы по этой части есть?
Тогда нам осталось совсем чуть-чуть, а именно нам надо понять, для чего ходуб мы не можем
использовать. Во-первых, ходуб у нас постоянно работает с диском, поэтому там, где нужна какая-то
реал-тайм обработка данных, ходуб там не подойдет. Во-вторых, почему он не масштабируемый?
Можем, почему? Можем это сделать? АВС у тебя работает с достаточно однородными нодами,
там должна стоять одинаковая операционка, должны быть плюс-минус одинаковые наборы ядер и параций
всего остального. Вот, например, С3-хранилище так работает, все должно быть однородное. В этом случае
ходуб может съесть любые машинки, например, это могут быть машинки на старых двухядерных процах,
и рядом могут стоять какие-нибудь более современные машинки, и все это может жить в одном ходуб-кластере.
Достаточно часто, если вы работаете в не очень большой компании, где в больших компаниях там,
понятно, все достаточно стандартизовано, все унифицировано, у вас есть огромные ЦОДы,
там сотни тысяч одинаковых машин, если вы работаете в небольшой компании, которая, тем не менее,
занимается большими данными, то вам нужна такая система, которая будет поддерживать любую
инфраструктуру. Если все одинаковое, да, можно воспользоваться ходубом, тоже нормально. Но для
реалтайма ходуб не будет нормально работать, потому что, во-первых, все хранится на диске, во-вторых,
нам нужно каждый раз запускать вот эти вот виртуальные машины Java, потом их останавливать,
тоже на это тратится время. Да, ходуб не предназначен просто в то время, когда ходуб создавался,
запроса на реалтайм-обработку бигдейта вообще не было. То есть тогда достаточно было просто
обрабатывать большие данные, которые другие системы обрабатывать вообще не могли.
Для реалтайма у нас есть всякие аналоги, которые работают в похожей парадигме,
это, например, Spark или Flink для реалтайма, может быть, кто-то слышал.
Ходуб очень востребован как хранилище, то есть вот чистый MapReduce, который запускает
маппер и редьюсеры, у него сохранилась такая некая ниша, как у ассемблера. Есть языки более
высокоуровневые, есть низкоуровневый ассемблер. У ходуба точно так же, есть высокоуровневый,
намного более приятный Spark, Flink, Hive. Ходуб в этом смысле менее приятный, потому что,
чтобы посчитать простейший wordcount, нам нужно написать два файла на Python и вот эту большую
команду на Bash. В Spark в этом случае мы напишем 5 лямбдочек и у нас все будет работать. Поэтому,
как обработка данных, ходуб не так популярен, но с другой стороны, на ходубе можно сделать то,
что нельзя сделать на Spark. Например, тут уже зависит от инфраструктуры. Sparkу нужно много памяти,
он данные обрабатывает в памяти, хранит в памяти. Если у нас памяти мало или если у нас
разнородная инфраструктура, то здесь ходуб будет работать лучше. И еще мы с вами будем разбирать
в следующий раз элементы Shaft and Sort. Есть маппер, есть редьюсер, между ними Shaft and Sort.
Пока это для нас достаточно серый ящик, мы разберем в следующий раз более подробно,
что там внутри находится, и ходуб позволяет лучше, чем Spark, залезть внутрь, прям покопаться,
как сортировка происходит. Если говорить про файловую систему HDFS, она сейчас практически везде есть.
Все, что нам осталось на сегодня, это посмотреть на вот это. Мы этим будем заниматься в следующий раз,
пока просто посмотрите. Это реальная схема того, как работает ходуб. Выглядит она ужасно,
но вот маппер наш вот здесь, редьюсер наш вот здесь. То есть вы просто можете увидеть,
как много всякой работы ходуб за нас делает. В домашней нам надо будет писать только маппер и
редьюсер. Может быть немного подкрутить параметрами сортировку, там уже будете
на семинарах смотреть. Вот этот весь ужас, который посередине написан, вам писать не нужно,
да и вообще те, кто работают с ходубом, они очень редко что-то переписывают из вот этого под себя.
Пишут свои компараторы для того, чтобы можно было задать алгоритм, как мы сравниваем объекты при
сортировке. Пишут свои партишнеры, пишут свои файловые форматы. Ну вот, пожалуй, и все.
Там какие-то вопросы есть про сбер, но в сбере, насколько я знаю, довольно много ходубов разных,
там одновременно разные версии живут и все это очень интересно.
У них есть СДП, достаточно много статей про СДП, почитайте. И даже можете пойти на
ходуб админ с метап, тоже погуглите, что это такое, там достаточно много докладов про СДП, сбер датаплатформ.
На последнем слайде такая инструкция. Она старая, ей почти 10 лет, но здесь подробно
описано с пояснениями, как писать на ходуб стриминги свои коды и что куда ставить и почему.
У меня на этом все, есть ли какие-то вопросы? Тогда всем спасибо и до следующего раза.

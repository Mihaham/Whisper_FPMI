Давайте начнем наш доп-семинар, значит, что мы сегодня сделаем? Мы сегодня решим какой-нибудь
вариантик, в нем будет 6 задач, вот, и, наверное, какую-то вспомогательную теорию, которая нам
будет нужна, повторим. Первый вопрос, который хотел бы вам задать, насколько хорошо вы помните
сходимости из прошлого семестра? Потому что у нас сходимости перекочевали в Тиар-Вер, и поэтому мы
уже очень многое там рассмотрели. Чуть-чуть повторить надо, наверное. Чисто определение и
задачку сразу решим. Отлично, хорошо. А, еще я должен сказать, что вот сейчас общего варианта
какого-то нет, поэтому нет понимания, как будет выглядеть контрольная. Сколько у вас будет задач,
сколько у вас чего будет, поэтому просто какой-то вариант контрольный решим. У всех групп разный,
да. Ну, давайте начнем тогда. Первая тема у нас это сходимости. Так, что мы хотим научиться делать
в данной задаче? В данной задаче мы хотим научиться считать какие-нибудь сходимости каких-нибудь
страшных случайных величин. Что мы должны знать? У нас сходимости бывают разные. Вот, мы с вами
сначала определяли сходимости для случайных величин именно. У нас была сходимость為 что,
наверное, была сходимость по вероятности, была сходимость по распределению, именно случайных
величин. Потом мы с вами рассматривали случайные виктора. И одна из ключевых теорем говорит о том,
что вот, сходимость по компонентное случайных величин эквивалентна сходимости викторов. Я это
это обозначено следующим образом. Вот. Вот. Вот. Вот этим утверждением, например, можно
пользоваться. Для чего? Мы можем с вами взять сходимость там, почти наверное, или
по вероятности случайных величин, составить из них вектор и что-то с ними потом сделать.
Что мы сейчас разберемся? Вот. Надо помнить, что для сходимости по распределению это неверно. То
есть из сходимости компонент не следует сходимость векторов, составленные из этих компонентов. Вот.
Итак, мы уже, значит, имеем с вами сходимость, сходимость, сходимость, сходимость, сходимость,
почти наверное, по вероятности по распределению сходимость не имеем векторов. В обратную сторону
эта стрелочка выполняется, действительно. Но вот есть случай, в котором стрелка вправо работает.
Стрелка вправо работает в следующем случае. Пусть у вас одна случайная величина, одна
последовательность случайных величин сходится по распределению к случайной величине. А вторая
последовательность случайных величин, ттм назовем, сходится по распределению константе. Вот. На самом
деле вот из этого будет следовать, что вектор, составленный из этих компонентов, будет сходиться
вот так вот. Хорошо, больше не буду. Отлично. Мы уже каким-то образом научились получать сходимости
векторов. Что мы с ними можем делать? Дальше на самом деле мы можем просто применять к ним
непрерывные функции. Вот. Это называется теорема непрерывности. Если опустить какие-то не особо
значимые детали, то если у вас аж это непрерывная функция, где ям эта размерность вот наших векторов,
то на самом деле если у вас была какая-то сходимость векторов, вот в каком угодном смысле
почти наверно, по вероятности, по распределению, то на самом деле у вас будет и сходимость
случайных личин, которые получаются применением какой-то непрерывной функции. Вот. Это теорема
наследования сходимости или теорема непрерывности. Вот. Если у вас есть какая-то непрерывная функция,
то применив ее к вектору, сходящемуся в каком-то из этих трех смыслов, вы получите новую
последовательность векторов, которая сходится вот в новом пространстве в том же смысле. То есть
если было почти наверно, то почти наверно, было по вероятности, по вероятности получили,
было по распределению, получили по распределению. Вот. Ну и в принципе это уже достаточно много нам
дает. Почему? Потому что вот представьте, что у вас есть две случайные величины, две последовательности
случайных личин, которые сходятся к чему-то. Это вот пример небольшой. То вы можете составить
из них вектор следующим шагом. Он будет сходиться. И можете применить какую-нибудь непрерывную
функцию. Ну, например, сложение. Вот. И из этого можно получить, что у вас, допустим, сумма,
последовательность сумм сходится в сумме пределов. Понятно, да? То есть в принципе какую-то страшную
сходимость можно будет получить из покомпонентных сходимостей, составить из них векторочек и к
векторочек применить какую-нибудь непрерывную функцию. Вот. Но опять-таки тоже заглоска в том,
что у нас по распределению как бы вектор так просто составить нельзя. Нужно чтобы была
сходимость константия. Вот. И, соответственно, вот такой вот отдельный случай называется
Лемма-Судского. Что если у вас кси-н сходится по распределению кси-тт-н сходится по распределению
константия, то тогда у вас последовательность сумм сходится к сумме пределов. Вот так. Ну и то же
самое для произведения, верно? Пока все понятно, наверное, да? Отлично. Вот. Что еще мы, наверное,
с вами не разобрали? Это Дельта-метод. Надо его немножко повторить, потому что обычно на него
первая задача как раз-таки есть. Доказательства мы разбирали с вами на прошлом ДОП-семинаре в
прошлом году, на втором. Вот. Поэтому сейчас давай только формулировкой ограничимся.
Значит, что нам дано? Нам дано, что у нас есть последовательность
случайных векторов, которая сходится по распределению. Нам дана какая-то числовая
последовательность, которая сходится к нулю. Вот. А еще нам дана какая-то функция, дифференцируемая
в точке А. Вот. Тогда на самом деле верна следующая импликация. Что из этого следует?
Сходится так. Ну, в случае векторов, к скалярному произведению градиента а аж в точке А максим.
Вот. В принципе, по теории, наверное, это все, что нам нужно, чтобы решить задачу. Ну,
еще, наверное, какие-то базовые теоремы нам пригодятся. Там УЗБЧ, что у вас? Если существует
математическое ожидание, то выполнена вот такая сходимость. Это вот УЗБЧ. Ну, УЗБЧ тоже выполнена,
то есть тут у вас будет сходимость уже по вероятности. Там чуть меньше условий мы накладываем на случайные
величины. Вот. И еще есть CPT, что если у вас существует математическое ожидание дисперсия, то
верна вот такая сходимость по распределению. Да, думаю, это все помнит более-менее.
Итак. Какой план решения задач в данном разделе? Вы берете изначально какую-то сходимость из
классических теорем, из сходимости почти наверное. Ну, сходимость почти наверное можно получить из
УЗБЧ, сходимость по вероятности можно получить из УЗБЧ, сходимость по распределению можно
получить из CPT. И затем, применяя теорему непрерывности, лему Слуцкого или Дельта-метод,
получаете сходимость той страшной функции, той страшной последовательности случайных величин,
которых вам требуется найти. План понятен, да? Ну, давайте задачу решим. Мы ее до конца доводить не
будем, потому что она достаточно тяжелая, в том смысле, что там вычислять достаточно много.
Мы с вами очертим план, доведем до какого-то этапа и бросим ее. Итак, задача первая из нашего
сегодняшнего варианта. Итак, пусть у нас есть выборка из гамма распределения с параметрами
от это. Вот, рассматриваем вот такие выражения, z это у нас будет средняя квадратов. Вот, и нас
спрашивают. Еще мы рассматриваем вот такое выражение. Вот, и, собственно, вопрос, к чему у нас
сходится. Так, здесь просили не писать. Вопрос, к чему сходится вот такая последовательность
случайных величин. Так, давайте, чтобы какой-то контакт с вами поддерживать, как будем решать? Да,
t это z минус y квадрат поделить на y. Вот y у нас определен, вот у нас определен z. x1 xn это просто
случайный величин, имеющий вот такое распределение. Вот, и спрашивают, есть у нас вот такая
последовательность случайных величин, к чему она будет сходиться. Хорошо, значит, мы будем
использовать CPT и дельта метод. Действительно, встает первый вопрос, какой CPT мы будем использовать,
для случайных величин или для векторов? Действительно, мы будем использовать его для векторов,
потому что CPT для векторов всегда верен, а если вы запишете CPT сначала для вот этой
случайной величины, а потом для этой, вы получите две сходимости по распределению, так ведь? Но из
них, если вы составите вектор, у вас сходимости не будет вектора. Вот, поэтому, действительно,
первый шаг, это необходимо применить CPT для случайных векторов. Вот, ну давайте запишем просто это
как-нибудь. Там у нас сначала идет среднее, ну вот у нас по сути записано YZ, такой вот векторочек,
минус их математические ожидания. Вот, и это все сходится под CPT к нормальному гауссовскому
вектору с нулевым вектором средних и с матрицей к вариации. Так, этот шаг всем понятен. Просто
записали с вами CPT для векторов. Отлично. Давайте поймем, как у нас выглядят элементы вот этой
матрицы и вот этим от ожидания. Как можно посчитать математическое ожидание? Да.
А если вам не разрешат? У нас просто очень хороший семинарист. Наверное, разрешит. Спросим у
него потом. Так, ладно. Значит, если что, можно это по определению посчитать. Ну да, это на самом
деле несложно получается. Просто давайте кому-то просто могут не дать справочную информацию,
поэтому один интеграл посчитаем, все остальные будут читаться аналогично.
Вот. Как такой интегральчик посчитать? Вот это, если что, я записал плотность гамма распределения.
У гамма распределений несколько параметризаций, у них плотности немножко могут видом отличаться,
но в целом они все выглядят примерно вот таким образом. Вот я записал
определение математического ожидания в абсолютно непрерывном случае. Как такой интеграл взять?
Ну да, это правда. Нужно сделать одну замену переменных. Наверное, вот такую. И мы тогда
с вами получим какой интеграл? Здесь смотрите у вас, по сути, х в степени k на θ в степени k,
то здесь будет t в степени k, e в степени минус t, гамма k. Вот. И еще из-под дифференциала у нас
вылезет с вами, да? dx это у нас dt на θ. Здесь у нас тепь вылезает. Чему равен интеграл 600?
Вот смотрите, вот это это просто определение гамма функции, если вы знаменатель тоже можете вынести,
это же по сути константа. А это у вас определение просто интеграл от гамма функции. Ну это вот
просто определение гамма. И чему оно равно? Ну в определении там k минус 1, поэтому на самом деле
вот эта функция это будет гамма от k плюс 1, поэтому у вас получится от этого. Вот. Ну и все, все
аналогично. У вас вот тут, допустим, нужно будет в отождание квадрата посчитать. Все такие интегралы
считаются одним и тем же способом, вы просто приводите гамма функции к определению гамма
функции. Да. И все далее понятно. То есть вы можете найти теперь в отождании x1, в отождании x1 в квадрате,
давайте сейчас обсудим, как найти элементы каваритационной матрицы.
Так, что является элементами каваритационной матрицы?
Ну да, на самом деле здесь у вас матрица 2 в нашем случае будет. Здесь будет дисперсия просто x1,
здесь будет дисперсия x1 в квадрате, а здесь будет кавариация x1 x1 в квадрате. Ну здесь по сути
тоже самое. В силу симметричности каваритации. Вот это? По-моему вот k на k плюс 1 в квадрате. Вот. Вот
это у нас k на 5 получилось. Вот. Каваритационную матрицу мы с вами досчитывать не будем, но просто
поймем с вами. Дисперсия это в принципе тоже у вас раскладывается на моменты, кавариация у
вас раскладывается на моменты. То есть вот кавариация по определению. Да. Как считать вот
мотождания, которые входят в кавариацию? Мы только что с вами обсудили, просто приводим там как-то в
интеграле гамма-функции. Все элементы этой матрики они вычислимы. Все окей, да? То есть мы с вами
записали по сути цпт и посчитали просто какие элементы стоят вот ну чему равному от ожидания и
чему равна матрицей кавариации. Отлично. Следующий шаг какой? Действительно. Теперь мы хотим
воспользоваться дельта методом. Смотрите, чтобы дельта метод получился, у нас должна быть сходимость
по распределению. Должна быть какая-то числовая последовательность, которая сходится к нулю и
должна быть какая-то дифференцируемая в точке а функция. Ну и точку а надо выбрать. Давайте,
что мы делаем с этим? Во-первых, последовательность сходящейся по распределению у нас уже есть. Вот,
мы ее из цпт только что с вами получили. Это первый шаг. Второй вопрос. Какую последовательность bn возьмем?
Во всех таких задачах берется один на корень из n, чтобы у вас вот с этим корень из n убилась.
Отлично. Давайте ее как-нибудь обозначим. Пусть она будет от двух переменных ab. То есть она у вас
будет выглядеть. Если первый у нас y, первый это a, b-a в квадрате на a. С этим согласны, да? Отлично.
Вот и точка a. Осталось еще выбрать точку a. Какую точку возьмем?
Да, точка a. Так, у нас колидия сейчас произошла. Давайте здесь функция будет от b, от cb.
Аккуратней будет. Потому что точка a у нас как бы в определении теоремы есть. Вот, и мы в качестве этой
точки рассматриваем как раз таки вот. Вектор математических нужданий. Вот. Ну и все. На самом
деле все, что осталось, это принять как раз таки дельта метод. То есть записать вот эту сходимость.
Мы получим на самом деле с вами ответ. Давайте аккуратно запишем. Так, условия дельты метода нам
уже не нужны. Вот. То есть применяем нашу функцию. Так, давайте по порядку. Значит,
давайте с того, что внутри начнем. Ксиенно bn у нас это что такое? Это по сути вот бескорное вот
это выражение. Мы к нему добавляем a, у нас мотожи убиваются. И просто h применяем по сути вот к
этому вектору. Ну то есть получаем с вами t. Это вот мы первое слагаемое в числителе только что
посчитали. h в точке a чему равно? Да, подставляем вот сюда нашим от ожидания, который мы посчитали.
Но на самом деле там действительно получится тета. Аккуратно если проделаете, там получится вот
тот тета, который у нас вот в условии. Затем мы это все делим на bn, ну то есть делим на 1 на корень
bn, то есть умножаем на корень bn. Вот. И все это по распределению сходится вот к нашему нормальному
распределению. Да. И градиент нужно не забыть. В точке a. То есть как у нас будет выглядеть градиент.
D по dc. Так давайте посчитаем. 1 на c, то есть будет минус b на c квадрате. А там минус c получается.
Минус 1. Не обманул вроде, да? Это первая частная производная. Вторая частная производная,
которая нам понадобится по b. Тут одно слагаемое, тут очень просто. Просто будет 1c. Да. Подставь.
Если ты тут посчитаешь математическое ожидание, у тебя вот здесь будет k тета. Вот здесь у тебя
получится k на k плюс 1 тета в квадрате. Вот. И давай просто поставим вот сюда. Что у тебя получится b?
b это у тебя по сути вот это минус вот это в квадрате у тебя получится просто k тета в квадрате. И
поделить на k тета у тебя получится тета. Ну аккуратно проделай, просто поставь вместо
c и b значение нашего вектора. И у тебя действительно получится тет. То есть в этой задаче в некотором
смысле пронзгон произошел. Ну это в условии. Ну обычно делают так специально. Вот. И так градиент
мы с вами получили. И получается вот предельный закон распределения у нас будет скалярное
произведение нормального гауссовского вектора и нашего градиента в точке математического ожидания.
Вот. Ну то есть на самом деле здесь у вас поменяется только матрица кавариаций. Итоговая матрица
у вас будет выглядеть следующим образом. Это будет градиент вашей функции. Вот
который мы с вами посчитали. В точке a транспонированный. Так. Вопросы?
Ну смотрите. У нас есть. Мы можем умножать на константу нормальной случайной величины. Помните?
У вас грубо говоря на квадрат увеличивается дисперсия и умножается там средняя. С гауссовскими
векторами в многомерном случае происходит то же самое. У вас получается умножается вектор средних
на вот этот вектор, который вы подставляете. А матрица кавариаций не на квадрат умножается
вот таким вот образом. Понятно, да? Ну да. То есть это многомерный аналог тех свойств,
которые у нас были для обычного нормального распорядления. Здесь вот градиент. Вопрос
еще раз. Вот мы его записали вот здесь. Мы посмотрели вот на это. Поняли,
что вот то, что внутри находится. Да, вот это это h от y z. А h мы положили вот таким вот образом.
Получается мы ровно получили t по определению. Справедливо? Вот. Потом мы посмотрели,
что такое h в точке a. Мы поняли, что это t. Намножили на b. Ну поделили на bn. Ну то есть
умножили на корень z. Вот смотри, у нас проще найти вот такую исходимость. Так ведь,
где вот у тебя тета должно здесь получиться. Ты из твоей статистики вычитаешь тета. Вот у нас
тета вот здесь появляется. Смотри, мы посчитали с тобой математическое ожидание вот этой штуки.
Математическое ожидание ну просто x1. Получили вот. А теперь давай просто вот к такому вектору из
вот таких двух чисел составленных применим нашу функцию. То есть вот h в точке a. h у тебя определяется
вот таким образом. Вместо c подставь k тета. Вместо b подставь k на k плюс 1 тета квадрата. У тебя
получается вот честно тета. Справедливо? Вот. Все это сходится к скалярному произведению
градиента функции в точке a на вот гауссовский вектор, который у нас был из цепоты. Да,
умножение вектора на гауссовскую случайную величину дает все еще гауссовскую случайную
величину, но с другой матрицы кавариации. Новая матрица кавариации выглядит вот таким образом.
Вот смотри, вот эти значения мы знаем, мы их можем посчитать. Да, а вот это просто градиент в
точке a вашей. И все. Градиент вот мы посчитали. Вместо b и c опять подставляйте те значения,
которые у нас в точке a. И вот можно в принципе матрицу кавариации досчитать до конца.
Ну и все, ответ. Вот такая последовательность случайных величин сходится к гауссовскому
вектору вот с такой матрицей кавариации. Итак, идем дальше. Первую задачу мы с вами разобрали.
Мораль первого раздела. У вас есть сходимость из ЗБЧ-ЦПТ или УЗБЧ. Вы применяете какие-то
теоремы, которые у нас были. Получаете сходимость того, что от вас требуют. Там
Лемма Слуцкого может быть еще понадобится. Теорема непрерывности и все такое. Это первая задача.
Идем дальше.
Отлично. Следующий раздел у нас это статистики и оценки. Сейчас посмотрю на задачу,
которая у нас есть. Да, нужно какие-то, наверное, определения сначала дать. Вот у нас есть некоторое
выборочное пространство. Что такое выборочное пространство? Ну, по сути, это вот ваше наблюдение.
Оно может быть конечным, бесконечным. То есть, это ваша выборка. Давайте считать,
что это примерно одно и то же. Сейчас. ХН. Н, возможно, стремится к бесконечности,
а, возможно, у вас конечная выборка. Вот. И на самом деле статистика — это любая измеримая функция
из X, R. Вот. А оценка — это любая измеримая функция из X, множество тета. Сейчас мы
это множество с вами поймем, откуда берется. Значит, статистика — это просто вот какая-то
функция от ваших данных, которые вам дали на вход. Среднее, максимальное значение,
минимальное значение. Может быть, сумма кубов. Неважно. Это все статистики какие-то. Медиана.
Вот. Дальше у вас появляется понятие параметрического семейства. Мы дальше
с вами занимаемся параметрической статистикой. То есть, у вас есть какое-то семейство распределений,
как красивое. Вот. Все они, грубо говоря, имеют одинаковую плотность или одинаковую
функцию распределения, а просто с разными параметрами. Вот. Ну и пример этой штуки — ну,
допустим, семейство всех нормальных распределений. Семейство всех нормальных распределений
параметризуется параметрами a и sigma2. Вот. Какие ограничения на a накладываются?
Нет. А у нас любые как раз таки. А вот дисперсия должна быть строго положительной. Вот. Вот,
допустим, параметрическое семейство нормальных распределений. Вот. Теперь давайте поймем
качественное отличие между статистикой и оценкой. Вот, смотрите, вы, допустим, хотите в такой модели
понять, какой параметр лучше всего подходит вам на должность sigma2. То есть, как бы вы не знаете,
из какого распределения именно пришли ваши данные, но вы знаете, что оно нормально с какими-то
параметрами. И вот задача параметрической статистики вообще — это подобрать такие самые лучшие в
некотором смысле параметры. Вот. Оценка — это что такое? Оценка — это вот какая-то измеримая
функция от ваших данных, которая возвращает вам валидное значение. Допустим, если вы захотите в
нормальной модели оценить дисперсию чем-нибудь, что может быть отрицательным, это не дисперсия,
потому что у вас дисперсия не может быть отрицательная. Так ведь? То есть, чем оценка
отличается от статистики, оценка принимает только те значения, которые вот можно взять,
подставить вместо параметров, и у вас получится нормальное распределение. Ну,
хорошее распределение. Существующее. Допустим, если вы скажете, что дисперсия — это минус модуль
x1, вот это — это измеримая функция от выборки, так ведь? То кажется, что это какая-то лажа,
потому что у вас дисперсия отрицательной быть не может. Это какая-то статистика, но не оценка.
Так, с этим разобрались. Дальше, когда мы ввели с вами понятие оценок, у нас появляются какие-то
свойства, у этих оценочек. Вот, свойства основных четыре. Давайте их в виде схемки нарисуем,
наверное, чтобы понять не было. Значит, смотрите, первое свойство — это сильная состоятельность.
Сильная состоятельность говорит о том, что вот если вы вашу выборку будете увеличивать,
значение, которое дает ваша оценка, будет стремиться к истинному значению параметра θ. То есть
какое бы вы истинное значение параметра θ не зафиксировали, ну, вот вы не знаете, какое оно
истинное, но оно вот какое-то зафиксированное, θ 0. То вот последовательность ваших оценок должна
почти наверно стремиться вот к этому истинному значению. То есть, если вы выборку сделаете
бесконечной, вы почти наверно получите то значение, которое настоящее. Вот. Дальше есть
просто состоятельность. Просто состоятельность — это вот чуть более слабое условие,
у вас сходимость не почти наверная оценок, а по вероякостью. Ну и кажется,
понятно, как эти два сло freshmen с собой связаны. По-моему, очевидно, да, почему? Просто из-за того,
что сходимость почти наверной сильнее, чем сходимость по вероякостям. И сходимость почти
наверное, следует сходимость по вероятности. Вот. Дальше есть свойство ассинтетической нормальности.
Записывается оно следующим образом.
То есть, грубо говоря, последовательность ваших оценок при бесконечно растущем размере выборки
находится вот в районе истинного значения параметра и имеет какое-то нормальное распределение.
Вот. Есть утверждение о том, что из-за синтетической нормальности оценки следует состоятельность.
Как это доказывать, кто не узнает? Не стесняйтесь, ребята. Так, ну ладно. Ну,
давайте просто лему Слуцкого с вами применим вот здесь. Рассмотрим просто один на корень
из n. Это сходится к нулю, так ведь? Ну, это как числовая последовательность сходится. Ну,
на самом деле и по распределению тоже сходится. И рассмотрим вот нашу вот эту последовательность.
По лему Слуцкому мы можем взять умножить левую часть вот на левую часть здесь, правую часть на
правую здесь, потому что это сходится константе. Справа у вас получится нолик, а слева у вас
получится вот. Дальше мы вспоминаем с вами замечательный факт, что сходимость
константе по распределению и по вероятности это одно и то же. Так ведь? Вот. Ну, по сути,
мы с вами получили то, что там написано. Окей? Вот. То есть из-за синтетической нормальности следует
состоятельность отца. Вот. И последнее условие, которое никак, последнее свойство вернее,
которое никак с другими свойствами не связано, это несмещенность. Одну секунду, я запишу определение
и потом можно будет. Да. Да. То есть, грубо говоря, вот, к любому тетнулевому истинному значению. То есть,
у тебя у неизвестной выборки, если она из нормального распределения, есть какие-то истинные
значения параметров. Ну вот, а 0 и sigma квадрат 0, который настоящий, но ты их не знаешь. И вот все
вот эти свойства, они о чем говорят, что вот какое бы истинное значение не было, последовательность
твоих оценок вот здесь сходится к истинному значению. Вот. Здесь то же самое, просто чуть
послабже сходимость. Здесь говорит, что у вас в среднем значение вашей оценки дает истинное
значение. Вот. Ну а это асимпатическая нормальность. Вот здесь вот? Да, да, да.
Нет, как раз-таки ты сначала фиксируешь, что это 0. Не совсем правда. То есть, для любого
тета 0 у тебя должна вот такая последовательность сходиться. То есть, ты сначала фиксируешь оценку и
потом говоришь, что для любого тета 0 у тебя вот такая сходимость выполняется. Ну иначе смысл не
имеет. Смотри, если ты сначала выбираешь тета 0, а потом последовательность оценок, грубо говоря,
чтобы выбрать хорошую оценку, ты сначала должен узнать ее настоящее значение. Это глупо немного.
У тебя ты придумал какую-то оценку и она для любого тета 0 должна работать. Одна и та же. Вот.
По Лемме Слуцкого домножим левую часть на 1 на корень Z. Она по распределению сходит к 0. Справа
получается 0 при изведении? Да. Ну и все, по Лемме Слуцкого это выполнено. Вот. Дальше давайте
посмотрим, что с этими свойствами происходит, если мы к ним применяем функции. Давайте применим
какую-нибудь непрерывную функцию. Что вы можете сказать про вот такую исходимость?
Будет ли вот такая исходимость верна? Почему? По теореме непрерывности. То есть мы их на самом
деле не зря проходили. То есть если вы возьмете какую-нибудь непрерывную функцию, примените ее
к состоятельной последовательности оценок, то новая последовательность оценок будет также
состоятельна для новой оценки. То есть мы теперь оцениваем не сам θ0, а функцию от θ0.
Пример может быть разберем, если успеем. Вот. Здесь в принципе тоже самое из той же теоремы
о непрерывности. Так, это просто по вероятности сходится. Так, что вы можете вот здесь сказать?
Будет ли какое-нибудь наследование исходимости, если мы какую-нибудь непрерывную функцию захотим
применить? Будет ли аж, вот если мы применим какую-нибудь непрерывную функцию, возможно,
дополнительными условиями? Будет ли она асимпатически нормальная? Будет ли вот это
к чему-то сходиться по распределению? На самом деле, да. Если аж это непрерывная деференцируемая
функция точки, то мы можем с вами применить что? Дельта метод. Если мы применим с вами к этому
дельта метод, то у нас как раз таки получится, что вот здесь у вас будет аж ттн со звездочкой,
здесь минус аж от θ0. Вот. Осталось только понять, как у вас асимпатическая дисперсия
вот здесь изменится. Это будет сходиться к нормальной случайной величине с параметром 0,
сигму ттн. Ну и мы там домножали на градиент. Давайте в одномерном случае рассмотрим. Аж
ттн. Согласны? То есть мы просто с вами применили дельта метод для функции аж. И вот из этого мы на
самом деле получим на 8 асимпатически нормальную оценку. Для уже других параметров. Ну и давайте
какими примеры приведем. Как я уже говорил, в подобных задачах у вас есть всегда источник
сходимости. Источник сходимости почти наверное это, например, УЗБЧ. Вот. Ну допустим мы хотим с
вами оценить параметр среднего в нормальном распределении. Что мы можем с вами сказать? Ну
мы знаем с вами, что последовательность вот такая средних. У вас сходится к математическому
ожиданию ха, а это а. То есть на самом деле мы можем сказать, что х средняя это сильно состоятельная
оценка параметра а. Если вы захотите оценить, допустим, а квадрат, то есть вас не интересует какое
значение среднего у вашего распределения, какое значение принимает а в квадрате, то вы можете
применить непрерывную функцию и по теореме непрерывности вы получите, что х средняя в квадрате
почти наверное сходится к в квадрате. Это просто примеры, которые иллюстрируют, что есть оценки,
которые обладают данными свойствами и что при применении непрерывной функции вот это будет
выполняться. Вот. А задачу на вот это мы сейчас с вами решим одну. Когда мы с вами еще что-нибудь
скажем про оценки статистики, что мы еще хотим сказать с вами?
Какие еще статистики вы знаете, кроме вот, допустим, среднего и чего-то такого еще,
что-нибудь, какой-нибудь другой класс статистики вы знаете? Порядковые статистики есть, правда?
Что-то такое, думаю, все знаете, да? Это просто какой элемент у вас стоит на
катом месте в ассортированном ряду. Порядковые статистики есть, вот. Ну, есть еще выборочные
статистики, это просто какие-то вот функции от ваших элементов усредняются по выборке.
Тоже такой достаточно широкий класс статистики, оценок. А моменты сюда относятся? Моменты
тоже сюда относятся, да. То есть это по сути просто сумма X-катых будет, наверное. Это выборочные
моменты. Ну вот, ну там понятно, что это будет сходиться к математическому ожиданию X степени K.
Вот. В целом, думаю, ничего сложного быть не должно. В этом разделе какие задачки могут быть? Вам
могут дать какое-нибудь семейство распределений, допустим, там равномерное на отрезке 0 тета,
и дадут какую-нибудь оценку. Такие задачи были в домашней, их было очень много.
Какого плана вот тут могут быть задачки? Вообще идейно. Вам дают какие-то оценки,
вы их не сами придумываете, вам их просто дают. Вот, допустим, Xn, там X средняя, n плюс 1
на минимум, ну на первую порядковую статистику в модели у 0 тета. И вас могут попросить вот в этой
задачи понять, какие свойства есть вот у этих оценок. Ну и дальше вам просто нужно по
определению проверить те или иные свойства. То есть посчитать вот такое математическое
ожидание, чтобы понять, допустим, что вот эта оценка смещенная, вот эта не смещенная,
вот эта тоже не смещенная, по-моему, да, не смещенная. Да, потом применить определение,
тут применить УЗБЧа, понять, что вот эта оценка сильно состоятельной будет оценкой,
вот только там по-моему 2x, вот так должно быть. 2x средних. Поймете, что она сильно
состоятельная, допустим. Ну поэтому просто состоятельная. Примените ЦПТ, поймете,
что эта оценка еще асимпатически нормальная. Вот. Тут вот будут проблемы с состоятельностью.
Дома такая задача была, но при этом оценка не смещенная, но не асимпатически нормальная. Вот.
Эта оценка простосостоятельна, даже сильносостоятельна, но не смещенная.
Такие задачки должны были быть дома, и задачка на контрольную нормальная.
Вам дают какую-то оценку, какую-то модель, и просят сказать, какие у этой оценки есть свойства.
Конкретно какую задачу мы в данном разделе рассмотрим?
Это задачка на симпатическую нормальность.
Я специально не буду стирать то, что нам пригодится сейчас.
Она на самом деле очень простая задача.
Что вас в ней просят показать?
Вас в ней просят показать, что пусть у нас есть выборка x1,xn из равномерного распределения на отрезке θ пополам θ.
И просят показать, что статистика является симпатически нормальной оценкой для функций от параметра логарифма Пета.
То есть, грубо говоря, вам дали какую-то выборку, вам дали что-то, что нужно оценить, и нужно проверить, что вот объект, который вам дали, обладает некоторым свойством как оценка.
Как будем решать?
На θ есть ограничение, что θ больше 0.
Как будем решать, товарищи?
А потом?
А потом какую-нибудь неперерывную функцию применим. Ну, дифференцируемую, самое главное.
Ну правда, что у нас по CPT получится? У нас по CPT получается, что корень из n и средний минус. Математическое ожидание здесь какое у нас будет?
Ну, разный отрезок пополам.
3 четверти θ. Сходится к нормальному распределению с параметрами 0. Дисперсия у такого распределения какая?
На 12. В квадрате 12.
Ну, то есть, θ на 48.
θ в квадрате на 48.
Так, почему мы такое получили? От ожидания нормального равномерного распределения это просто полусумма концов?
Дисперсия это b-a в квадрате поделить на 4. На 12.
Это дисперсия.
Какая-то вот справочная информация. Это можно либо там через интегральтик посчитать, либо там справочники где-нибудь найти.
Отлично. Ну, смотрите, все складывается хорошо.
Давайте просто вот воспользуемся теоремой онаследования симпатической нормальности для функции h равной 4 третьих.
Логарифм 4 третьих.
Сейчас, а почему у нас там 3 четвертых? У нас же θ минус так пополам. Пополам или еще раз? Почему 3 четвертых?
Там же θ минус, θ пополам, пополам.
Нет, там плюс. А, там плюс.
Там плюс.
Здесь минуса нет. Это, возможно, я не стер. Извините.
Ну, применяем такую функцию. Пользуемся с вами теоремой онаследования симпатической нормальности.
Получаем с вами что корень из n.
Тут уже можно как бы не париться насчет дельта метода. Можно просто вот воспользоваться этой теоремкой.
Логарифм 4 третьих х средняя.
Минус логарифм θ. Сходится к нормальному распределению с параметрами 0.
θ квадратно 48.
Так, производная нашей функции какая?
Производная нашей функции это 1 поделить на 4 третьих.
И умножаем еще на 4 третьих.
Поэтому просто 1 на t.
Вот. И умножаем получается на производную в квадрате в точке θ.
Производная в квадрате от 1 на t квадрат. То есть будет 1 на t квадрате.
Ну и все. Итоговая дисперсия 1,48.
Ну это нам просто зло. А не можно доказать, что наоборот оценка не является симпатически нормальной?
Такие задачи очень редки на самом деле. И показывать, что нет сходимости гораздо тяжелее, чем то, что она есть.
Ну можно привести пример. Вот у нас был такой хороший пример. Я не думаю, что вам что-то сложное дадут. Я таких задач по крайней мере не видел.
Ну вот как показать, что такая оценка не является симпатически нормальной? В модели у 0 θ.
Ну что вы можете сказать про разность θ минус?
При деле 0. А по знаку?
Вот эта стучка всегда больше 0.
А теперь посмотрите вот сюда. Чтобы у вас была асинтетическая нормальность, у вас как бы, видите, отличие должно быть на знакопеременной. Плюсик, минусик должен быть.
Ну типа вот эта штучка должна сходиться к нормальному распределению. А так у вас по сути значения будут только с одной стороны.
Вот так будут они у вас располагаться. То есть а плотность у вас, если бы вы сходились к нормальному распределению, у вас вот такой холмин должен быть.
А, как еще такие задачки можно решать? Можно просто найти предельный их закон. Вот пользуются функции распределений. То есть можете расписать функцию распределения этой штуки. Это сделать достаточно просто.
Вот. И показать, что предельная какая-нибудь такая штука будет сходиться к чему-то не похожему, ну ни на что. Ну то есть чему-то другому. То есть там может быть, задачка тоже дома такая была.
А если, допустим, мы сразу не поняли, что она не будет абсолютно нормальной иначе решать этот вопрос? И в каком моменте вообще может быть что-то так? Ну надо уже понять, что оценка не абсолютно нормальная.
В какой момент остановиться? Я не знаю, надо задачу смотреть. Я так сходу не могу сказать. Но обычно не дают чего-то такого сложного прям.
И просто посмотреть, с чем она будет сходиться. Ну вот, допустим, дома у нас был пример. По-моему, вот эта задача, если не помню.
Там просто, если аккуратно пределы расписать, у вас появляется здесь число E. И там получается, что итоговая функция распределения будет E в степени минус C. То есть вы получаете в пределе экспоненциальное распределение.
Там какая-то константа вылазит, да.
Что мы можем исследовать? Мы можем исследовать, чему сходится функция распределения. Сходимость по распределению – это сходимость функции распределения в каждой точке.
То есть мы можем на самом деле исследовать с вами объект. Это функция распределения по распределению.
И можно смотреть, к чему это сходится в пределе.
Да. Ну то есть если там что-то вычитать, добавлять, можно смотреть, что здесь будет с добавочками, и смотреть, к чему это сходится.
Там может быть какая-то нетрибиальная сходимость. Такая дома задача должна была быть. Не должно быть чего-то такого на контрольной. Если попадется, ну F.
И в чем мы приходим к экспонению? Почему это сходится в пункте распределения? А почему она должна сходиться, чтобы мы поняли, что отзакон есть?
Какой-то в одной из задачах там получилась, что вот такая последовательность или похожая сходилась. Просто там получалось число E в степени C, какой-то константой.
Ну и вот это все сходится к E в степени C. Ну и там еще X, по-моему, тоже. То есть E в степени C-X. В общем, это сходилось к экспоненциальному распределению.
Да. Ну проблема в том, что у тебя в пределе получалось не нормальное распределение, а экспоненциальное.
Я задачку скину потом, хорошо? Давай сейчас не будем на ней зацикливаться.
Так, по данному разделу есть вопросы? Давайте резюме небольшое.
В данном разделе вам скорее всего дадут какую-нибудь выборку из какого-нибудь параметрического семейства и дадут какую-нибудь оценку.
Ее придумают уже за вас и попросят просто проверить ее на свойства. Вот. Ну все, с вторым разделом разобрались. Идем дальше.
В чем основная проблема сейчас у нас?
Да, дают мы их сами не придумываем, а еще мы должны у них свойства проверять.
Было бы очень круто, если бы у нас были какие-то универсальные методы, которые бы работали всегда вне зависимости от того, какая у нас модель.
Ну более-менее всегда там при выполнении каких-то условий регулярности.
Вот. И чтобы они сразу обладали какими-то хорошими свойствами.
Вот. И дальше мы с вами рассмотрим три таких метода построения оценок, и задачка будет там ими всеми тремя построить оценки.
Итак, методы построения оценок.
То есть нам нужна какая-то машинерия, которая нам будет сама давать вот эту измеримую функцию от выборки, которая сразу будет обладать какими-то хорошими свойствами.
Вот. Первый метод – это метод моментов.
Это частный случай метода подстановки. Что такое метод подстановки мы сейчас обсуждать не будем?
А в самом таком классическом случае вы решаете вот такую систему уравнений.
И так далее.
И так далее.
Вы составляете систему уравнений, в которых приравниваете теоретические моменты и эмпирические моменты.
Вот. Ну вот это понятно выражается каким-то образом через параметры, так ведь?
А правая часть – это вот какие-то у вас функции.
И на самом деле таким образом вы можете получить оценку.
Давайте сразу вот пример.
Самый простенький, самый банальный.
Пусть у вас N A sigma квадрат.
И вы хотите по методу моментов оценить параметры.
Что в методе моментов нужно сделать?
Ну, нужно составить систему уравнений.
Теоретические моменты приравнять к эмпирическим.
Теоретический момент первый – это просто A средняя приравниваемая к среднему.
Вот.
Второй момент чему равен?
Так, ну давайте с вами вспомним.
Дисперсия – это sigma в квадрате.
dx равняется ex в квадрате.
Минус ex в квадрате.
То есть если мы это сюда перенесем?
Сумма.
Sigma квадрат плюс A квадрат.
Это выборочный второй момент.
Понятно, да?
Количество уравнений у вас будет совпадать с количеством параметров.
Ну, чтобы все хорошо было.
Вот.
Решаете такую систему.
Если у нее решение единственное, и там все хорошо,
то вы получили оценку по методу моментов.
Отсюда в частности следует, чтобы вы получили оценку по методу моментов.
Отсюда в частности следует, что A это у нас эксредняя,
а sigma в квадрате мы оцениваем как
минус x в квадрате.
Понятно, да?
Составляем стенку и решаем.
То есть вот здесь мы просто вместо A квадрат
подставили оценку A
и получили вот то, что нужно было нам.
Есть распределение циклов.
Есть, конечно.
Можно сколько угодно параметров поделить.
Может быть еще раз мы получили sigma в квадрате с квадратом.
Откуда мы не оцениваем?
Вот это откуда мы знаем?
Еще раз.
Вот это теоретический момент.
От ожидания.
Ты знаешь, чему равно вот такое распределение?
Ну, A.
А.
А чему второй момент равен?
Ну, хорошо.
Второй момент может быть равен.
Ну, хорошо.
Второй момент можно посчитать явно.
То есть можно посчитать интеграл от x в квадрате
на плотность x dx.
Такую штуку.
Можно ее найти.
А можно вспомнить, что дисперсия на самом деле
это у нас мотождание в квадрате минус
мотождание квадратов
минус квадрат мотождания.
Вот.
И мы знаем, что дисперсия вот в такой модели
это sigma в квадрат.
И отсюда мы просто получили чему равняется.
Второй момент.
Это такой классический трюк, типа
что вы берете и выражаете через дисперсию,
потому что дисперсия это табличные данные.
Там на Википедии английская она есть, англоязычный.
А мотождание тоже есть.
И в принципе можно легко получить оценку методом моментов.
Всегда ли существует оценка методом моментов?
Ну, у зрения каши у них будет оценка методом моментов.
Какой?
Мотождание.
Да, вот давайте такой пример рассмотрим.
Утверждается, что для распределения каши
вот такой классический метод моментов
не сработает.
Ну да, не сработает.
Если у вас не определены мотождания,
то вы, грубо говоря, такое уравнение составить не можете.
Ну вот, слева нечего поставить.
Но есть обобщенный метод моментов.
Это когда вы составляете уравнения
от каких-то функций.
Так, тут у нас будет просто нет.
Вот.
То есть, вот этот момент момента,
в котором...
Это частный случай,
когда у вас g1
это просто х,
g2
это х в квадрате
и так далее.
Это называется стандартные пробные функции.
На самом деле, сюда можно любые там
баррельские функции подставлять
и вот что-то подобное решать.
То есть, если у вас есть
что-то подобное подставлять
и вот что-то подобное решать.
Так вот, утверждается, что вот таким обобщенным методом моментов
можно найти оценку для распределения каши,
если в качестве
жера смотреть вот такую функцию.
На семинаре такая задачка должна была быть.
То есть, если чуть-чуть подправить,
сделать так, чтобы ваш интеграл сошелся,
то такую систему можно составить и решить.
Ну, как бы это
вообще так никто не делает, на самом деле.
Это как бы максимум такая задачка может попасться,
что во время распределения каши у вас попросят
найти методом моментов оценку.
Это маловероятно, скорее всего, там удобнее будет чем-нибудь другим действовать.
Вот, но такое тоже есть.
А какими свойствами обладает оценка по методу момента?
Давайте по порядку.
Будет ли она сильно состоятельной?
Видите оценка по методу моментов сильно состоятельной?
Почему?
Она точно будет не смещенной.
Нет, она будет смещенной.
Вот, смотри, вот здесь у нас какая оценка на дисперсе получилась?
Вот эта оценка смещенная.
Ну, это вы знаете, да, наверное?
Это точно должно было быть на семинаре.
Но эта штука смещенная.
И вот вам пример, что метод моментов может
вернуть смещенную оценку.
То есть, еще раз, ни один метод
не смещенность не гарантирует.
Но зато вот сильная состоятельность, допустим, гарантируется.
Но почему?
Потому что, смотрите, у вас же по сути вот эта функция от параметра, да, какая-то?
Ну, как вектор функции
приравниваете m от t
к мат ожиданиям.
Вы знаете, что вот у вас есть
сходимость вот такая, по
тогда, на самом деле, если у вас вот эта вот функция m
она непрерывная,
короче, если она обратимая, то есть там монотонно-непрерывная,
то у вас
по наследованию сходимости
ну, мат ожидания, ну
мне сейчас я аккуратно
напишу.
Это у нас какой-то выборочный момент.
А, сейчас.
Ну давайте вот в общем виде
запишем как что-нибудь среднее.
Так.
Ну, вот, вот, вот.
То есть у вас
сильная состоятельность, она сохраняется.
То есть у вас вот
это сходится почти, наверное, по УЗБЧ.
Если вы обратную функцию накинете
и она там непрерывно дифференцируемая, то у вас
и оценка будет сходиться к этому.
А это у вас на самом деле истинное значение параметра.
Так, давай еще раз.
Давай посмотрим
на те уравнения, которые мы записали.
Ну, можно их записать как некоторые
векторные уравнения, а так ведь?
Сейчас я посмотрю,
как это можно было бы аккуратно сделать.
Мы по сути приравниваем сами
каким-то выборочным функциям.
С этим согласен?
Ну по сути я вот переписал просто вот это
в более общем виде.
Ну, вот это у нас какое-то значение,
ну какая-то функция от параметров,
мы ее обозначим за m1 от theta.
Ну то есть в случае нормальной модели
у нас вот m1 от theta от параметров
мы его обозначим за m1 от theta.
Ну, то есть в случае нормальной модели
у нас вот m1 от theta от параметров
это просто a было.
Ну да, по сути здесь у тебя как бы фигурируют
теоретические значения параметров, какие-то функции от них.
А справа какие-то выборочные
характеристики, которые их приближают.
Вот. У тебя есть сходимость
на самом деле, ладно, вот такая.
То есть вот так у тебя есть сходимость по УЗБЧ.
Согласен?
Ну, на самом деле для
выборочных функций более-менее для всех.
Но если от ожидания у них существует,
то это будет выполнено.
Вот. И дальше в чем идея?
Мы можем, если все хорошо и у нас вот эта
функция обратима, и она непрерывна,
мы можем, грубо говоря, в нашем неравенстве
навесить m в минус первой,
воспользоваться теоремой наследования сходимости.
У тебя получится, что к истинному значению
у тебя сходится m в минус первой.
Тут аккуратно в векторном виде переписать.
Так, тут m1, тут mn.
Согласен?
Ну, давай считать, что да.
На что?
Ну, на m какой-то.
На самом деле это любой функционал нам подходит.
Это частный случай метода подстановки.
Что такое метод подстановки?
У вас есть какой-то функционал?
Вот ожидание – это частный случай.
Вы там считаете вот такую штуку, например, да?
На самом деле можно считать там от чего угодно
такой интеграл. Он может там
существовать или не существовать.
Про что говорит метод подстановки?
Давайте сюда вместо истинного значения
функцию распределения поставим эмпирическую
функцию распределения.
Вот эти штуки будут сходиться,
потому что у вас функция распределения
эмпирически сходится к истинным мусорным распределениям.
Это оценка по методу
подстановки. А это как бы частный случай.
Вот здесь функционал может быть... Ну, грубо говоря,
давайте считать, что это вот взятие
от ожидания, от какой-то функции, от случайной вечны.
И вопрос, а почему мы можем агрантуру кэрсить?
Мы предполагаем,
что такая существует. То есть если она
существует, то все хорошо.
Теорема. Если существует...
В общем, если m – это объекция, она у вас
однозначная функция, то тогда мы можем
налезть обратную и получим,
что у вас была сходимость почти наверная.
И оценки по этим методам
тоже получатся.
Сильно светает.
Прылило?
Какими еще свойствами обладает оценка
методом моментов?
Смотрите, мы доказали,
что... Ну, показали аккуратно,
но не очень аккуратно,
что у нас есть сильная
состоятельность оценки по методу моментов.
Она может быть смещенной.
Вот пример смещенной оценки.
Вот. Но есть еще симпатическая нормальность,
и симпатическая нормальность на самом деле
будет из CPT проследовать.
Смотрите, предположим,
что у вас существует отжидание
awn,
то есть,
или, видимо,
после этого
она였NAWork
и уenda
полuccvable type
или
ולт报у
предположим, что у вас существует от ожидания и дисперсии в этой стуке, тогда у вас есть корень из-за н,
а в качестве среднего вот вы как раз таки рассматриваете средние ваших функций.
ну вот выборочную характеристику такого вида, вы знаете их истинным от ожидания,
ну вот мы полагаем, что это мхт, и оно в силу цпт сходится к нормальному распределению с
какими-то парами примерно, окей? и все дальше применяем просто теория
о наследовании сходимости, теория о наследовании симпатической нормальности для функции м-1 и мы как
раз с вами получим вот, ну нужно потребовать, чтобы обратная функция была дифференцируема,
объекции, чтобы все хорошо было, но вот таким образом можно показать, что оценка по методу
моментов будет также симпатически нормальной, цпт применяем, просто полагаем, что у наших
функций, которые мы рассматриваем, у них существует математическое ожидание и дисперсия,
окей? вот, то есть оценка по методу моментов, она обладает там вот почти всеми качествами,
которые мы рассмотрели, кроме, кроме несмещенности. Ну в смысле, ровно так же как теория о наследовании
симпатической нормальности у нас была. Если вот эта функция дифференцируема непрерывно,
то по сути по дельта методу мы можем сюда применить м-1. Вот тут у вас как-то дисперсия поменяется,
но сходимость такая останется. Так, ладно, наверное.
Давайте еще оценку по...
оценка методом максимального правдоподобия. Давайте это обсудим еще быстро и потом задачку
решим и сделаем перерыв небольшой. Значит, смотрите, вводится функция правдоподобия.
И полагаем оценка по методу максимального правдоподобия.
Да, мы-то это ищем. Ну это просто обозначение, я мог это вот сюда записать, ладно. Вот. И соответственно,
оценка по методу максимального правдоподобия, это такая оценка, на которой эта штука достигает
своего максимума. Вот. Тут есть классический такой достаточно, не знаю, пример. Вот давайте
рассмотрим все нормальные распределения с параметром, у которых средний неизвестно,
а дисперсия фиксирована. Ну и давайте просто это будет пять. Вот. И, предположим, вам дали
какие-нибудь данные, которые вокруг вот этой точки сгруппированы. Давайте рассмотрим вот какую-нибудь
такую произвольную плотность. Понятно, что вот это значение функции правдоподобия для вот такой
плотности будет маленькое, потому что вот вы берете произведение плотностей вот в этих точках и у вас все
не очень маленькие. Понятно, что в некотором смысле лучше подходит плотность, у которой горбик вот
вокруг этих значений, потому что у вас тогда произведение плотностей будет больше. Ну вот,
смотри. Пусть у тебя все данные лежат вот здесь, все твои наблюдения лежат вот тут. Вот,
вот точки я нарисовал. X1 и так далее, Xn. Пусть они даже упорядочены. Вот. Согласись,
что такая плотность будет иметь достаточно маленькое значение. Вернее, такая функция правдоподобия
будет иметь маленькое значение. Почему? Потому что у тебя, грубо говоря, каждый множитель будет
очень маленьким. Вот. А если мы рассмотрим какую-нибудь вот такую плотность, то ее уже произведение будет
побольше. Да, это мы предположили. То есть вот это не будет оценкой максимального правдоподобия,
а вот это уже больше похоже, потому что вот эти значения, как бы каждый множитель будет больше.
Вот. И вот на таком примере достаточно легко понять, что происходит. У вас есть какие-то
параметры, и вы пытаетесь подобрать такие параметры, чтобы вот они были, чтобы вот это
распределение было максимально похоже на те данные, которые вам дали. Идея понятна, да?
Вот. Встает вопрос, как искать такую оценку? Ну да, первое, что заметим с вами,
у вас функция правдоподобия в принципе не отрицательная, ну на самом деле даже положительная.
Обычно там плотности рассматривают, которые не за нуляются, либо там с индикаторами аккуратно
играются. И логарифм непрерывная функция, поэтому если мы ее применим, с максимумом ничего не
произойдет. Поэтому можно рассмотреть логарифмическую функцию правдоподобия. Вот. Это по сути будет просто
сумма логарифм плотностей. Вот. И уже ее максимизировать. Хорошо, как мы будем искать максимум этой функции?
Вот то, что на облупку это не гарантируется, но обычно в задачах действительно достаточно проверить
только необходимые условия. То есть что производная, под это будет нулевая. Или если многомерный параметр,
то у вас градиент, то есть частные производные, все за нуляются. Чаще всего такая точка и подходит.
Понятно, да? Вот. То есть следующий шаг, если у вас одномерный случай, вы просто дифференцируете по параметру.
В точке экстремума у вас должен быть ноль. Вот. И уже решая вот это уравнение правдоподобия,
находите такую оценку, которая, ну, кандидатно хорошую, и чаще всего она и есть хорошая. Она и есть
оценка максимального правдоподобия. Вот. Тут уже теорема гораздо сложнее, то есть тут так просто
на пальцах не показать, но оценка максимального правдоподобия, она на самом деле состоятельна,
а симпатически нормально. Вот. Ну, еще и эффективно. Но это уже не показать как бы за три минуты. Там
достаточно сложная теорема. Задача. Нет. Хорошо, не всегда. В общем, случай не является.
Это вы из доказательства поймете. Там в доказательстве у вас только с вероятностью
единицы будет сходимость. Вот. Почти наверно и сходимости там не будет. Итак, задача. Уже третья.
Итак, пусть x1, xn выборкой из распределения рилея. Давайте обозначим его как-нибудь вот так,
с параметром sigma2. Плотность я сейчас напишу. Проще найти оценку вот этого параметра двумя
методами. Методом максимального правдоподобия и методом моментов. Вот. Плотность. Какая плотность?
Вот так. То есть распределение с такой плотностью называется распределением рилея. И что нас
просят? Нас просят по выборке размера n найти оценку методом моментов и методом максимального
правдоподобия. Сейчас я проверю, что я правильно плотность написал. Иначе мы задачу с вами можем
не решить. А это плохо, да ведь? Так. Нет, вроде все правильно. Давайте по методу моментов.
Как будем решать? Ну да, нам на самом деле, смотрите, параметр один, поэтому нам достаточно только
первый момент найти. Мы хотим составить с вами уравнение такого вида. Чтобы такое уравнение
составить, нам нужно найти математическое ожидание. Давайте найдем математическое ожидание.
Ой, уже индикатор лишний, потому что я уже написал в ограничениях интеграла.
Как будем считать? По-моему, тяжело будет. Ну да, на самом деле тут нужно, вот такой интеграл
появляется, когда вы считаете дисперсию нормального распределения, например. И вот в нашем
случае тоже появился. Что в таком случае делать? Ну на самом деле давайте разобьем просто на
x и x поделить на sigma квадрат. Есть степень минус x в квадрате. Ну и по частям это будем брать.
Так, что у нас получится? Интеграл от этой штуки чему равен?
Вот это все занесется по дифференциалу, и на самом деле будет просто e в степени минус 100.
Да, то есть здесь будет действительно e на 2 sigma квадрат. Подстановки от 0 до бесконечности.
Ну на бесконечности зановляется, 0 тоже зановляется, поэтому вот это просто 0.
Так, дальше формулю интегрируем не по частям. У нас остается так,
берем производную от x это единичку, поэтому она никак не влияет. Ну и по сути нам осталось
посчитать вот такую штуку. Так, вроде не ошибся. Нет, не ошибся. Как это будем считать?
Заметно. Заметно, чтобы множитель вынести, и потом кой не спит на какой-то пицце.
Действительно, давайте просто докидаем туда каких-нибудь множителей,
каких-нибудь констант, чтобы какой-нибудь хороший интеграл получился. Ну а хороший
интеграл это у нас, например, интеграл плотности нормального распределения. А это
почти похоже, кстати, на интеграл плотности нормального распределения. В силу симметрии,
наверное, можно сказать, что это что-то вот такое. Почему?
Да, ты прав. То есть тут ты говоришь минус одна вторая, да? Нет, мы когда бы под дифференциал
пополам. Ну в смысле интеграл от х. Окей? Так, ну смотрите, вот в силу симметрии, наверное,
можно положить вот таким образом, да? То есть продлили просто интеграл чётным образом от
минус бесконечности до плюс бесконечности. Вот, ну и давайте докидаем чего-нибудь, чтобы это было
похоже на плотность нормального распределения. Плотность нормального распределения, что нужно
сделать? Корень из 2АП сигма квадрат. Ну соответственно, все, что мы, на все,
что поделили, давайте намножим. Вот это это единичка. Итоговый ответ. Получается корень
и пополам сигма квадрат. Так, под интегралом минус. Вот этот? Так, я утверждаю, что он должен
был где-то уйти. Давайте посмотрим, где. Вот здесь, когда мы брали интеграл е в степени минус t,
там минус должен был вылезти, поэтому здесь, на самом деле, плюсик должен был быть, да,
по маркам. Ну вот, это уже правильное значение от ожидания. Ну и все, соответственно, оценка
по методу моментов. Это просто мы с вами приравниваем теоретическое значение.
А тут у нас что будет? Сигма квадрат приравниваем к иперическому значению. Ну и отсюда оценка сигма
квадрата, это просто 3 в квадрате на 2П. Согласны? Да, вроде меня так получилось, когда я считал.
Что именно? Мы посчитали математическое ожидание. Оценка по методу моментов, вот она. То есть,
это система уравнений. В нашем случае, так как параметр 1, у нас всего одно уравнение.
Приравниваем математическое ожидание к иперическому математическому ожиданию,
то есть, к среднему. Посчитали мотош, он вот такой. Приравняли, получили чему ровно сигму квадрат.
Окей? Да, одномерная задача. Ну, параметр уравнений в методе моментов столько,
сколько у вас параметры. Так, давайте посчитаем оценку методом максимального продуподобия.
Так, вот это оценка по методу моментов. Давайте ее стирать не будем.
Оценка методом моментов. Так, а теперь давайте оценку методом максимального продуподобия найдем.
Так, значит, нам нужно что первым делом? Составить функцию продуподобия.
Она будет выглядеть как произведение плотностей. Плотности у нас это что такое? Это x на сигма
квадрат. E в степени минус x квадрат на 2 сигма квадрат. Вот, ну, на индикаторы еще.
Так, это просто определение функции продуподобия, произведение плотностей в точках, которые нас интересуют.
Дальше, трюк с логарифмированием, потому что гораздо проще будет нам действовать,
если мы прологарифмируем. Логарифм функции продуподобия, это будет у нас по сути
сумма логаритмов x и так. Минус что? Почему 2n? А почему 2n?
Пока через сумму, да. Просто чтобы давайте лишних шагов пока не
попускать. Согласен? Согласен. Сейчас решаем и перерыв строим. Да, все, теперь с этим согласен.
И получается плюс еще, можно минус, сумма p от 1 до n, x в квадрате на 2 сигма в квадрате.
Вроде больше не денег косячил, так, Вить? Супер. Дальше следующим шагом, что мы должны сделать?
Да, давайте найдем максимум по параметрам, то есть найдем такой параметр, который лучше всего
подходит. Вот, чтобы найти максимум нам нужно найти точку подозреваемую на экстремум. Давайте
продиференцируем, только сразу будем по сигма квадрата дифференцировать. Зачем по сигма? Мы
для сигма квадрата. Ну да, да, да, мы просто делаем сейчас небольшую замену. Пусть это сигма
квадрат и давайте просто дифференцировать по t. По сути, да, мы это и сделаем. Так, по d сигма
квадрат. Ну по сути, просто считайте, что я sigma квадрат за t обозвал, и сейчас будет 0. Так,
производной вот этой штуки будет 0. Производной вот этой штуки это будет? Нет, это будет 1 на
сигма квадрат. А производной а? Ну потому что мы, тут по сути логарифм t, производная просто 1 на t.
Так, с минусом вопрос остался, да? Минус остается. Так, что здесь у нас будет?
Так, а здесь будет минус сумма в квадрате. Да, это у нас по сути t в минус 1, поэтому
плюс вылезет. Делим на 2 сигма в четвертый. Ну теперь похоже на правду, да?
Дальше. Необходимые условия экстремума. Вот в этой точке производной должна быть 0. Вот,
следовательно, у нас появляется уравнение правдоподобия. Ну вот тут n на сигма квадрат.
Это у нас сумма в квадрате, поделить на 2 сигмы в четвертый. Вот, ну положим,
что сигма у нас не 0, там на самом деле по условию сигма больше 0.
Можем сократить на сигма в квадрате. Вот, и отсюда получить уже оценку.
Так, на 2. Вот. Ну сейчас, по сути, мы нашли с вами оценку максимального правдоподобия.
Ну это не совсем понятно, что это такое. На самом деле вот это вот похоже на второй момент.
А вот откуда здесь двоечка появилась уже не совсем очевидно. Ну на самом деле вот
эта оценка будет не смещенной, по-моему. Если вы ее, честно, посчитаете,
математическое ожидание от нее-то. Да, такое бывает. Бывает, что они совпадают. Вот,
допустим, в нормальном оценке параметра A при известном сигме квадрат, то у вас получится
и там, и там среднее. А иногда бывает, что методы дают разные оценки. Иногда совпадают,
иногда отличаются. Понятно все пока? Вот. То есть задачи этого раздела вам дадут какое-нибудь
семейство и скажут, а найдите нам асимпатическую оценку, асимпатически нормальную. Или найдите нам
состоятельную оценку. Могут типа сказать, найдите оценку с такими-то свойствами. Ну тогда вы можете
применить метод какой-нибудь и найти оценку, которую просят. Могут просто сказать, типа
найдите таким-то методом оценку. Там методом максимального продуподобия или методом
математического. Вот. Единственное, что мы еще с вами не обсудили, это метод выборочной квантили.
Буквально пару слов про него скажем и потом точно пойдем на фирер. Да. Ну про эффективность там
аккуратно нужно говорить, там асимпатическая эффективность, но вообще да. А чего? Нет,
все. Пока что хватит. И по задачи они обычно идут по разделам, то есть здесь мы эффективность,
допустим, ее исследовать не хотим и не будем. Так, давайте еще про метод выборочной квантили поговорим.
Вот смотрите, как мы с вами строили сходимость по распределению. Мы брали с вами CPT, а потом
применяли к нему дельта метод и получали какую-то сходимость, так ведь? То есть если нас просят
показать или построить оценку асимпатически нормальную, мы с вами записываем CPT.
Вот так вот. А потом применяем к ней дельта метод. Вот какая сходимость,
которой у нас есть. Вот, но понятно же, что CPT, например, не будет работать, если у вас нет
от ожидания. То есть вам уже нет куда взять вот такую сходимость, чтобы дальше какие-то
применять к ней теоремы. А вас могут попросить построить асимпатически нормальную оценку
для двига распределения каши. Вот, что будем делать? Во-первых, давайте с вами вспомним,
как у нас плотность записывается для каши в таком случае. Вот. Ну просто положим, что параметр
масштаба единичка фиксированный, а вот у нас параметризован только с двига распределения. То
есть плотность у вас выглядит примерно так же, как и в случае нормального распределения. Примерно
какой-то колокол, но у него просто более толстые хвости, поэтому эта штучка не интегрируема. То
есть там от ожидания не будет. Вот. И вас проще построить асимпатически нормальную оценку для
параметра с двига. Что в таком случае сделать? Можно. А что бы вы делали? ОМП сложно строить для
каши. Мы сделали это на семинаре. Там уже для двух значений у нас там появлялся квадратичный,
по-моему, трех или нет. Многочлен третьей степени в знаменателе. Мы что-то его исследовали потом.
Да. Вот тут как раз-таки нам приходит на выручку метод выборочной квантили. То есть нам нужен еще
какой-то источник вот такой сходимости. Если CPT не работает, то есть аналог. Метод называется
метод выборочной квантили. Вот. Что такое квантиль? Квантиль есть как бы теоретическая и квантиль есть
практическая. Теоретическая квантиль — это такое значение XA, что вероятность того,
что ваша случайная величина будет меньше этого значения, ну, равна P. Просто ZP. Да, обозначается
ZPET-квантиль. Это как бы такое значение, что вот ваша случайная величина, такое значение XA,
что ваша случайная величина с такой вероятностью будет меньше этого значения. То есть, грубо говоря,
вот здесь значение функции распределения будет P. Если эта штучка не прерывит. Вот. То есть,
рассматривая Z1-2-ую квантиль, это медианно-теоретическая. То есть, грубо говоря, так же точка XA,
что ваша случайная величина попадет левее этой точки, то есть меньше будет XA с вероятностью 1-2.
Если у вас распределение симметрично, то понятно, что теоретическая квантиль будет находиться как бы
вот в этом горбике. То есть, унимодальная случайная величина с одним горбиком. И это вот как раз наш
луч из каши. Медианно – это и есть параметр theta. Вот. И тогда есть на самом деле теорема
выборочной квантили, которая утверждает, что вот выборочный NP-квантиль стоит циклическому квантилю
в таком смысле.
Вот.
Так, давайте тогда аккуратно. Вот, смотрите, мы выяснили, что есть теоретические квантили, да?
А есть как бы практические квантили. Вот, смотрите, у выборки есть медианно, вы знаете, да?
В случае, когда это нечетная выборка, то это просто средний элемент. Когда выборка четного размера,
то это полусумма средних двух элементов. Вот. И смотрите, ZNP определяется как NP-порядковая статистика.
Вы сортируете возрастание и смотрите, какой элемент у вас стоит на NP этом месте.
Ну, по сути, вот если вы рассмотрите выборочную медианну и теоретическую медианну, то есть P равняется
здесь у вас получится ровно вот этот элемент с точностью до там полусуммы, ну, в общем, пока на это забейте,
который стоит ровно посередине выборки, так ведь? Вот. И есть у вас теорическое значение этой квантили,
то есть вот оно, например. И вот теорема выборочной квантили утверждает, что вот эта вот практическая
квантиль, которая по выборке строится, ну, допустим, практической медианной, она сходится к выборочной
медиане. То же самое там и с 0,75 квантили, то есть вот вы возьмете точку такую, что вероятность того,
что ваша случайная величина будет меньше этой точки 0,75, вот, то есть здесь вот 0,75, то у вас
0,75 квантиль, это называется третий квартиль, то есть третий квартиль, то есть если вы возьмете
вашу выборку, отсортируете, и у вас в ней, допустим, 100 элементов, то это будет элемент,
который стоит на 0,75 месте, так ведь? 0,75 квантиль, и вот эта вот квантиль при растущей выборке,
он тоже будет как бы, он будет у вас сходиться вот к этой точке, к теоретической. Окей? Вот. А теперь
встает вопрос. Это плотность в квадрате в точке zp. Да, плотность в квадрате. Давайте я аккуратно
напишу, извините. Давайте сразу... Вот здесь вот? Скорее всего np плюс один, да. Там аккуратно
нужно посмотреть. Что не закрыто? Спасибо. Не, это справедливо, справедливо, спасибо. А что с минусом?
На самом деле, это та же самая параметризация, просто у вас будет... Они будут симметричны относительно...
Ну короче, это не столь критично, ну хорошо, давайте x- это сделаем. Ну ладно, классическое
определение такое, действительно. Так, давайте пример быстро посмотрим с вами. Вот давайте
построим с вами асинтетически нормальную оценку двумя способами для нормального распределения
с параметрами a sigma квадрат. Да, сейчас я посмотрю, вроде я не ошибся. Я просто примерно
доказательство помню, и там вроде-то именно такая штука вылезает. Да, вроде это правда. Так,
давайте построим двумя способами асинтетически нормальную оценку для параметра a. Первый
способ какой? CPT. Да, построить асинтетически нормальную оценку двумя способами. Вот. А второй
способ какой еще можно? Метод квантилий. Ну понятно, что а у вас будет медианой теоретической,
потому что у вас симметричное распределение относительно точки a, поэтому вероятность того,
что вы попадете левее точки a, у вас будет одна вторая. Это можно аккуратно через интеграл
показать, но мы давайте скажем, что мы в это верим. Поэтому выборочно медиана, она обозначается вот так.
Тоже будет асинтетически нормальной оценкой, но у нее будет другая асинтетическая дисперсия.
Какая асинтетическая дисперсия будет? Вопрос, чему, какое значение принимает функция плотности
в точке a? Сигма квадрат, есть это пи минус х в квадрате, х минус а в квадрате на 2,
сигма в квадрате. Ну вот это все будет единичка, потому что у вас х равен а. И вот это в квадрате,
это получается будет 2 пи сигма квадрат. Но это переходит вверх, поэтому это будет пи сигма
квадрат. Грубо говоря, вы можете оценивать параметр a в нормальной модели как средним,
так и медианой. То есть у вас наверное в каких-нибудь статистических задачках вас уже просили оценить
через средний и через медиану. То есть это две асинтетические нормальные оценки, одна правда
из них хуже. Сейчас поймем почему. Скоро. Ну вот хуже вот эта, потому что у нее дисперсия больше.
Ну потому что у тебя вот это получилось, ты на это делишь, получается перекидываешь
Ну п на 1у сп, одна вторая на одну вторую, одна четверть была. Медиану это одна вторая квантиль.
Так товарищи, не расслабляемся. Нам за час нужно еще сделать много. С этим понятно? Отлично.
Сейчас, подождите одну секунду. Мы еще должны с вами про каши поговорить. Вот смотрите,
все вас просят построить асинтетически нормальную оценку для сдвига каши. Воспользоваться цпт вы
не можете, потому что у вас нет математического ожидания дисперсии и подавна. Вот, поэтому давайте
построим через квантиль, через выборочную квантиль. Понятно ли, что опять-таки медианой здесь будет
это? Да, это правда? Хорошо. На самом деле, смотрите, как определяется квантиль по-честному.
Вот, но в случае непрерывной функции вы можете сказать, что у вас есть квантильная функция,
у вас просто квантильная функция будет обратной от квантили.
Но что не определено? По всем эксамус прямой. То есть, грубо говоря, вы ищете в вашем распределении
такую точку, чтобы, вот, самую маленькую, чтобы вероятность попасть больше, чтобы у вас
функция распределения вот здесь была почти в точности равна тому, чего мы хотите. Если функция
непрерывная, то вот эти точки как бы взаимно однозначно связаны. Инфинум там нужен для
непрерывного случая, когда у вас есть какой-нибудь разрыв. Функция распределения вот такой быть может,
например. Вот, и поэтому там есть определение через инфинум, чтобы, если вы захотели найти
какую-нибудь вот квантиль, которая попадает в разрыв, вы просто взяли вот эту вот точечку.
Понятно, да? Если функция будет непрерывная, то на самом деле у вас есть биекция между ними,
монотонно непрерывная, все хорошо. Есть вот такая биекция. Вот, то есть на самом деле квантили
очень легко находить. Если у вас функция распределения непрерывная, это просто вот обратная функция к распределению.
Понятно, да? Вы фиксируете точку P и находите такую точку вот такая, что вот левее него
вероятность случайной величины будет P. Вот. И, соответственно, на самом деле можно квантильную
функцию в общем случае получить. Ну как в общем случае получить квантильную функцию для распределения
каши? Максим, помоги. Максим. А? Максим. А? Квантильную функцию для распределения каши.
Ну да, на самом деле, чтобы найти квантильную функцию, чтобы по-честному показать, что вот это
у вас будет медиана, вам нужно найти функцию распределения сначала. Функцию распределения
это просто интеграл от этой штуки будет. Да, это Аркангенс. Максим, подскажи, какой там будет Аркангенс?
По-моему, х-минус это. Х-минус это, да. Ну и там еще на самом деле плюс одна вторая. Вот. А отсюда на
самом деле вы уже легко квантильную функцию найдете. Почему? Потому что квантильная это обратная?
Наверное. То есть вот вы получили функцию распределения и вам функцию распределения могут дать там в условии,
допустим. Как найти квантильную функцию? Ну давайте просто обратную функцию найдем.
Один напиток появляется? Вот. Давайте найдем обратную функцию. Обратная функция это у нас будет х-минус
одна вторая. Ко всему этому применяем тангенс и добавляем это. И на пи, да? Внутри тангенса на пи,
вот. Ну и по сути вот у вас квантильная функция обычно вот так обозначается. З от П. То есть вы
фиксируете какой-то уровень П. Если мы аккуратно это перепишем. Теперь это будет выглядеть вот таким
образом. И теперь мы допустим легко показать, что вот это это будет как раз таки медианой нашего
распределения. Ну почему? Давайте найдем такую точку П. Давайте просто подставим. П равный одной
второй. Тангенс нуля это ноль. То есть у вас получится З от П в плотности тета. То есть мы
только что с вами доказали, что медиана это тета. Супер! Если медиана это тета, то теперь мы
можем воспользоваться теоремой выборочной квантили для медианы. То есть для П равна одна вторая.
Мы получаем с вами, что корень З опять выборочно медиана минус это. Сходится к нормальному
распределению с параметрами ноль. Одна четвертая. И на плотность в точке тета. Максим. Максим,
какая плотность точки тета будет? Один на П, да? Ну то есть П в квадрате на 4. Ну вот,
если у вас 1 на 5 есть, то П в квадрате будет 1 на П квадрат. Переносим П в квадрате на 4.
Получается. Одна четвертая у нас появилась как П на 1 минус П, при П равна одна вторая.
Вот. То есть смотрите, вы можете построить на самом деле симпатически нормальную оценку,
даже если у вас нет математического ожидания. Для этого можно воспользоваться выборочной квантилью.
А выборочная квантиль на самом деле, она более-менее всегда существует, потому что чтобы
существовала просто квантиль, вам нужна функция распределения, а функция распределения у вас более-менее
существует всегда, потому что мера однозначно задается в функции распределения. Вот. И нужно,
чтобы просто в окрестности этой точки у вас существовала плотность, чтобы функция распределения
была дифференцируема на самом деле в окрестности ZP. То есть грубо говоря, метод выборочной квантили
можно чуть чаще использовать, чем CPT. Вот. Ну все, давайте режиме, небольшой ее отдых. Значит,
режиме какое? В данном разделе вам могут дать задачу. Построите хорошую оценку, которая обладает
какими-то свойствами для вот такого семейства распределений. Вы просто применяете методы,
а эти методы обладают свойствами. Это там надо написать, например. Вот. Или вас могут просто
напрямую попросить, найдите вот такую-то оценку, допустим, симпатически нормальную, для такого-то
распределения. Или найдите оценку методом максимального продуподобия для такого-то
семейства. Вот. И такие задачи в этом разделе могут быть. Да не, ну не, не могут. Вряд ли. То есть,
на самом деле, несмещенные далеко не всегда существуют, допустим. То есть, для некоторых
функций у вас вообще не существует несмещенных оценок. И это как бы не страшно, с этим можно жить.
Все тогда перерыв. В общем, мы отдохнули полтора часа. Можем продолжать.
Так, друзья, все. Времени больше нет, надо заканчивать. Садыка, ты куда?
Еще 30 секунд осталось. А, наверное.
Все, поехали. У нас очень мало времени.
Нормально. Все хорошо будет, друзья. Так, давайте буквально пару слов скажем про сравнение оценок.
Хорошо? Вот. Значит, как можно сравнивать оценки? Первый подход. А симпатический?
Так. Ну, подход для сравнения симпатически нормальных оценок. Вот. Или асимпатически,
как он еще называется. То есть, вот у вас есть две оценки. Одного и того же параметра там,
или одной и той же функции от параметра. Вот. Ну и получается, логично будет сравнить просто их
дисперсии. Потому что все остальное как бы в записи одинаково, кроме того, какую оценку вы
здесь используете. Но у них разные дисперсии асимпатические. Вот. И давайте просто как бы
графики построим этих асимпатических дисперсий. Соответственно, вот пусть у вас какая-нибудь вот
такая будет первая асимпатическая дисперсия. А для второй оценки у вас будет какая-нибудь вот
такая синтетическая дисперсия вторая. Какая оценка из этих двух лучше? Ну, понятно, что первая,
потому что она как бы, видите, везде получше, в каждой точке. Вот. На самом деле у вас задан частичный
порядок, потому что вот если у вас будет две вот таких оценки, да, которые в какой-то точке одна
лучше, чем другая, в других точках хуже, то как бы они уже в равномерном подходе несравнимы. То
есть вы не можете сказать, что одна оценка в каждой точке лучше, чем другая. Давай вернем старую. Это
не метод максимального продоподобия. Метод максимального продоподобия, да, самую лучшую из
них. То есть если есть какая-то последовательность оценок, которая сходится к какой-то вот предельной,
то вот у метода максимального продоподобия будет самая лучшая синтетическая дисперсия. Ну, в
некотором смысле. Там есть очень много звездочек, которые мы опускаем. Хорошо. Вот. И соответственно,
вы просто сравниваете синтетические дисперсии в каждой точке. Если синтетическая дисперсия в
каждой точке для одной функции, ну, как бы меньше, то понятно, что вот эта функция как бы предпочтительнее.
Такой синтетической дисперсии такая оценка лучше. Вот. Но есть в подходе оценки, вот, допустим,
у вас одна синтетическая дисперсия лучше вот на этом отрезке, да, а другая лучше вот на этом
отрезке. Тогда как бы они в равномерном подходе несравнимы. Тут выбрать лучше уже не получится.
Это называется асимпатический подход. Вы, грубо говоря, в каждой точке сравниваете
асимпатические дисперсии для двух разных оценок. И вот у нас пример был. Мы получили с вами две
оценки. И средняя, и медиановыборщная. Для параметра A в модели E на сигма квадрат. Вот. И у этой
асимпатической дисперсии у нас получилась, вроде бы, P. Как у нас асимпатическая дисперсия
получилась? Кто-нибудь записал? Нормальная для параметра A в нормальном распределении.
Да, вот здесь просто сигма квадрат, потому что это СПТ. А вот здесь у вас будет P на 2, да, на сигма квадрата.
Вот. И если вы сравните, то у вас на самом деле вот эта функция, она как бы меньше, чем вот это в
каждой точечке. Для любого сигма квадрат. Ну и для любого A тоже. Вот. Ну для любых параметров вообще у
вас вот эта функция ведет себя, она хуже, она большее значение принимает, чем вот это. Поэтому в равномерном
подходе, в асимпатическом подходе у вас х среднее лучше чем выборочно медиана
для параметра I в нормальной модели. Вот. tired в асимпатическом подходе
мы сравниваем оценки. Первое в каком подходе в асимпатическом? Что значит, что мы сравниваем
в асимпатическом подходе? Значит мы сравниваем их асимпатически дисперсии? Мы получили с вами две
оценки. Х средняя и выборочная медиана. Вот это по ЦПТ, вот это через выборочную
квантиль. У этого дисперсия вот такая. У первой оценки такая дисперсия, у
второй вот такая оценка. Вот такая дисперсия, у второй оценки, прошу прощения.
Соответственно, в таком подходе х средняя лучше, чем выборочная медиана, потому
что у нее асимпатическая дисперсия меньше, чем у выборочной медианы.
Окей?
Вопрос, какое значение в каждой точке.
То есть, если в каждой точке одна функция лучше, то есть меньше, чем другая, то понятно, что вот оценка,
у которой вот такая асимпатическая дисперсия, она лучше. Если они в каком-то, до какого-то момента
одна лучше, потом на какой-то области другая лучше, то они несравнимы. То есть у вас как бы есть
частичный порядок. Понятно, да? У вас есть несравнимые, есть сравнимые между собой.
Вот. И у вас есть как бы на множествах всех оценок какой-то частичный порядок в таком вот подходе сравнения.
Это понятно, да?
Ну вот в таком подходе они несравнимы, но есть другие подходы для сравнения.
Ну вот по симпатическому подходу именно дисперсии сравниваются.
Одну секунду. Мы дойдем до этого.
Вот. Затем вводится понятие функции, функции потерь.
Это вот некоторая функция, которая... Что про нее можно сказать?
Это функция такая, что она не отрицательная, и g от x и y равно 0 тогда и только тогда, когда g...
В общем, когда у вас там два аргумента. То есть если у вас значения не совпали, то это 0,
а если не совпали, то больше 0. То есть это в некотором смысле какая-то метрика.
Поэтому как бы и рассматривают функции потерь вот такого вида. На функционе у вас уже должна была быть.
Ну и вот такого вида функции потерь у вас тоже рассматривались.
Вот. Вот такие функции ввели. Ну давайте с вами вот на что посмотрим.
Давайте мы с вами посмотрим, насколько у вас отличается от истинного значения.
Ну только понятно, что здесь у вас случайная выборка, а вот это какое-то фиксированное число.
Вам необходимо усреднить. Поэтому на самом деле у вас появляется вот такая новая функция g' от t,
которая просто есть в отожимании функ
Понятно, что происходит? Давайте какой-нибудь конкретный пример разберем.
Давайте разберем квадратичную функцию потерь. Вот эта функция потерь называется квадратичная.
Давайте для нее запишем функцию риска.
Тут просто будет математическое ожидание в арте оценки с истинное значение оценки в квадрате.
Понятно ли, что здесь записано?
То есть смотрите, вот это истинное значение параметра, вот это то значение, которое ваша оценка вернула.
И вы смотрите, насколько оно отличается в среднем. Понятно?
Вот. То есть на самом деле там в машинном обучении такие же метрики используют MSE типа mean square error, чем такого.
Ну и соответственно мы дальше с чем-то таким будем работать.
Ну да, это же штрих от одного параметра.
То есть грубо говоря, вы фиксируете θ0 какое-то, да, истинное значение.
И смотрите, насколько в среднем ваша оценка отличается от настоящего значения.
То есть вот здесь у вас на самом деле функция от выборки какая-то идет, и вы как бы по любой выборке как бы усредняете.
Это понятно?
Супер.
Соответственно дальше мы можем рассуждать вот ровно так же, как и в асимпатическом подходе.
А как у нас было в асимпатическом подходе?
Смотрите, у нас здесь опять появилась функция от одного параметра, от истинного значения параметра θ.
Вы опять можете грубо говоря нарисовать такие графики.
То есть вот эта функция риска для одной оценки, а вот эта будет функция риска там для другой оценки.
Вот, ну и опять можно выбрать оценку, которая лучше вот в таком подходе.
Понятно каким образом?
Так же, которая меньше, но у вас так же они могут быть несравнимы.
Например вот так.
Ну какая-нибудь функция риска, которая выглядит таким вот образом.
Да.
Функция риска или функция потери?
Функция риска.
Функция потери это то, что у вас под мотожаданием стоит, а функция риска это вот уже мотожадание этой штуки.
То есть какой риск у нас при фиксированном θ 0, какой риск мы получаем?
То есть насколько мы рискуем?
Насколько мы отклоняемся от истинного значения?
Ну, я у меня один реальный вопрос.
А в чем как бы, если мы придумали этот метод, то значит где-то асимпатический метод был не особо применен,
потому что они в принципе примеры.
Вот асимпатический метод работает только для асимпатически нормальных оценок.
Оценки асимпатически нормальные далеко не всегда.
Вот этот метод более общий.
Вот.
Ну и соответственно давайте сейчас зафиксируем с вами одну единственную функцию потерь квадратическую, ну квадратичную,
и будем вот сравнивать оценки вот в таком смысле.
Давайте сделаем какое-нибудь с вами замечание.
Существует ли наилучшая оценка, такая оценка, которая лучше любой другой оценки?
Наверное не всегда.
Да, потому что давайте мы с вами оценку зафиксируем просто равной какой-нибудь константе.
Мы же можем с вами просто говорить, что независимость от того, какое у вас распределение, а просто параметр один.
Он никак с данными не связан.
Тогда давайте посмотрим, как у вас будет выглядеть функция риска.
Вот здесь ваше значение тета.
Вот здесь у вас где-то тета 1, и функция риска вот здесь, в этой точке, чему будет равна?
Нулю.
Нулю.
Во всех остальных точках она может как угодно себя вести некрасиво,
но вот у вас есть какая-то адекватная оценка, которая почти всегда дает что-то хорошее,
но вот в этой точке вот эта тривиальная глупая оценка лучше.
Поэтому в равномерном подходе у вас нет во всем классе оценок как бы наилучшей.
Не существуют оценки, потому что вы берете вот такую глупую оценку, и она лучше чем в этой точке, чем любая другая.
То есть несравнимая не на самом деле.
Это понятно, да?
Это понятно, да?
Может больше чем в одной, может где-то...
Нет.
Смотри, ты здесь берешь мотожидание.
Если ты возьмешь константную оценку,
то только в одной точке у тебя здесь будет ноль под мотожиданием.
Во всех остальных точках у тебя квадрат даст какое-то положительное число.
Если просто мотожидание константной считаешь, то у тебя дальше функция риска будет...
Как-то, ну короче, она скорее всего будет расти получается не линейно, а квадратично.
Вот так она будет себя вести.
Вот конкретно для этой штуки.
Потому что у тебя тета ноль начинает отставать от вот этой вот константы фиксированной,
возводишь квадрат, а это мотожидание константы, просто константа, и она вот так квадратично будет расти.
А есть какая-то хорошая оценка, которая в целом всегда хорошо себя ведет,
но в этой точке она, конечно, не такая идеальная оценка, но она как бы не сравнима с ней.
Вот.
Поэтому наилучшие оценки в классе всех оценок в таком подходе не существует.
Поэтому рассматривают более узкий класс, класс несмещенных оценок.
Вот там уже имеет смысл искать наилучшие оценки.
Давайте зафиксируем здесь и далее с вами несмещенные оценки.
Как класс несмещенных оценок и будем искать только среди них наилучшую.
Вот.
По определению, несмещенность оценки говорит о том, что
ее мотожидание это просто тета, для любого тета.
Так ведь?
Что в таком случае можно сказать про функцию риска?
Совпадает с дисперсией.
Ну да.
На самом деле у вас тогда функция риска, это будет просто дисперсия.
Потому что, смотрите, у вас тета 0, это математическое ожидание
тета со звездочкой по вот этому условию, которое мы с вами наложили.
Соответственно, здесь у вас просто дисперсия этой оценки.
Понятно?
Вот.
И дальше, получается, мы выбираем ту оценку, у которой дисперсия будет наименьшая.
Понятно, да?
То есть дальше у нас уже вот эта функция риска на параметры зависит не будет,
потому что у вас всегда здесь математическое ожидание.
У вас в таком случае просто значение функции риска – это вот дисперсия.
Дисперсия данной оценки.
Вот.
И теперь встает вопрос.
Мы хотим выбрать такую оценку, у которой самая маленькая дисперсия.
Дисперсия снизу подперта числом 0.
То есть дисперсия – это не отрицательное число,
поэтому вот кажется, что мы можем бесконечно улучшать оценку,
пока типа дисперсию не сделаем нулевую.
На самом деле, это неправда.
Почему это неправда?
Потому что есть теорема…
Теорема не равен 100 раукрамеру.
О чем говорит неравенность раукраммера?
Что на самом деле…
Вот рассмотрим класс тех несмещенных оценок.
Ну и пусть у нее конечный второй момент у этой оценки.
Вот.
Тогда на самом деле дисперсия этой оценки – это не отрицательное число.
Это не отрицательное число.
Это не отрицательное число.
дисперсия этой оценки больше либо равна чем единицка на какую-то вот непонятную
на какую-то константу но это если ты функцию оцениваешь если это звездочка
несмещенная оценка для тета тогда вот так для функции если ты оцениваешь
функцию там да действительно того что будет еще должно быть условия регулярности
но мы сейчас задачи хотим порешать поэтому мы это проскипаем это еще не всегда выполняться
мотош квадрата ну чтобы дисперсия была у тебя должен быть второй момент
это информация фишер сейчас мы про нее поговорим немножко вот то есть n это n на и маленькая от
это это информация фишера выборки сейчас мы про нее чуть-чуть поговорим вот то есть на самом
деле неравенство у кроммера говорит что вот среди всех несмещенных оценок
средне квадратичным подход выбираем мы считаем лучшую ту у которой меньше дисперсия
бесконечно уменьшать вы не можете потому что она снизу подпёрта каким-то не отрицательным
числом понятно да вот и если в этом неравенстве достигает такая оценка называется эффективной
супер теперь дальше сейчас будет две теоремы
но перед теоремами мы парочку определений ведем давайте сначала разберемся что такое
информация фишера вот какими свойствами она обладает вот как она определяется и что
она примерно обозначает вот значит сначала вводится функция вклада вклады элементов вот и
функция вклада элементов определяется следующим образом сейчас я возьму свой конспект
вот давайте подумаем почему оно выглядит так вот буквально
посмотрим на то что у нас здесь записано здесь у нас по сути записано производная патета
на патета от x вот разные разные наблюдения обладают разной информацией
почему это так потому что смотрите вот допустим вы уже считаете что у вас распределение какое-то
вот такое и если вам начинает приходить наблюдение из области где очень маленькая плотность то они
несут очень много информации они говорят что скорее всего вот то что вы оценили это не совсем
правда вот а если же приходит что-то чтобы очень сильно ожидаете что-то обычное то оно как бы меньше
информации несет а как там теории информации там теорий кодирование типа что-то редкое больше
информации несет чем что-то очень часто примерно такая примерно такая идея за этим стоит то есть
Действительно, вклад выборки у каждого элемента – он
разный.
Вот.
И через вот такую функцию определяется на самом деле
информация фишера, про которую мы вот там говорили.
Это просто усреднение вклада элемента в квадрат.
Ну, это по сути, сколько в среднем один одно наблюдение
несет информации о параметре.
Покажите.
То есть, мы ввели какой-то объект, который называется
вклад выборки.
Он действительно как-то связан с тем, насколько это
ожидаемый элемент пришел относительно того, что вы
уже оценили, или просто вообще, насколько он как
бы много информации в себе несет.
Вот.
Информация одного элемента – это просто усреднение
этого вклада по всем х.
То есть, сколько в среднем один элемент несет в себе
информации.
Это есть информация по фишеру.
Вот.
Первое замечание важное – информация фишера – она
линейна.
То есть, если вы возьмете выборку из независимых
элементов, то информация фишера по вот этим элементам
– это все равно, что сумма информации фишера по отдельным
кусочкам выборки.
Что большая?
Это по выборке уже называется.
Ну, типа, смотрите, вы же понимаете, что вот тут мы
определили с вами для одного элемента, аналогично можно
определить для всей выборки.
Ну, вы вместо плотности берете функцию правдоподобия.
Ну, это как бы небольшой очевидный шаг для меня был.
Ну ладно.
Мы можем определить вклад не одного наблюдения, а
вклад всей выборки.
Сколько информации принесла вся выборка.
А вся выборка принесла информации, вот здесь все
то же самое, а здесь вместо плотности, как бы аналог
плотности для выборки, это функция правдоподобия.
Вот сюда можно записать функцию правдоподобия,
и это будет вклад выборки.
Соответственно, определяется информация фишера для всей
выборки.
Это просто математическое ожидание также вклада
всей выборки в квадрате.
Аналог плотности для выборки это функция правдоподобия.
То есть, просто произведение плотностей во всех точках,
которые тебе даны.
Вот.
Ну, дальше утверждение, что вот если у вас есть две
выборки x и y, и элементы независимые, то на самом
деле у вас информация фишеры линейна.
То есть, на самом деле, почему в неравенстве Рау Крамера
в знаменателе стоит вот такая штука, которую мы
написали?
На самом деле, потому что информация фишеры линейна.
Можно было написать единичку поделительную информацию
фишера всей выборки, но так как информация фишеры
линейна, можно посчитать информацию фишеры одного
элемента и умножить на это.
Вот эквивалентность этих записей следует из линейности
информации фишеры.
Так, что мы дальше хотим понять.
А, про информацию фишеры мы с вами сказали, и теперь
погнали в пару теорем, которые нам нужны будут сейчас для
решения следующей задачи.
Теорема один.
Так, сейчас я её найду.
Эффективная оценка существует не всегда.
Ещё раз, эффективная оценка — это такая оценка, что
в неравенстве Рау Крамера достигается равенство.
Эффективная оценка существует тогда и только тогда, когда
семейство распределений, которое вы оцениваете, экспоненциально.
Вот.
Что такое экспоненциальное семейство распределений?
Это распределения, у которых хотя бы одна плотность имеет
вот такой вид.
Ну, у вас же плотностей много.
Они определены с точностью до почти наверно.
Если хотя бы один вид плотности имеет вот такой
вид, то всё семейство мы называем экспоненциальным.
Ну, ты можешь на множестве меры ноль плотность поменять,
и у тебя интеграл ведь никак не поменяется.
Ты можешь какие-нибудь точки повыкалывать, и у тебя
всё равно плотность будет та же самая, по сути.
Ну, а если одна плотность экспоненциальна, то и другие
плотности?
Ну, просто если хотя бы какая-нибудь плотность вот
в таком виде представима, то всё, это экспоненциальное
семейство.
Не семейство экспоненциальное, а остальные плотности.
Это параметр.
Сейчас проговорим с вами.
Вот.
Давайте с вами поймём, что такое тета, и какие вообще
распределения можно назвать, относящимися к экспоненциальному
распределению.
Ну, я не знаю.
Я не знаю.
Я не знаю.
Я не знаю.
Я не знаю.
Я не знаю.
Я не знаю.
И какие вообще распределения можно назвать, относящимися
к экспоненциальному распределению?
Так.
Значит, вот в этом определении нужно ещё немножко доделать
его.
Продолжим.
То есть, смотрите.
Эффективная оценка существует.
Тогда и только тогда вы оцениваете что-то в экспоненциальном
семействе распределений.
семейство распределений, это такое семейство, для которых плотность имеет вот такой вид,
где а0, θ и так далее, аk, θ — это какие-то линейные зависимые функции.
Вот, аk — это просто количество параметров в модели.
Это просто какая-то статистика. Давайте просто пример сейчас приведем и поймем сами.
Давайте рассмотрим самое какое-то банальное семейство,
относящееся к экспоненциальному семейству, экспоненциальные распределения.
Плотность у них имеет вид. Вообще, λe в степени минус λх на индикатор х больше либо равна нулю,
но вообще она представима вот в таком виде.
Вот это вот, это ваш h от x, функция, которая от параметра никак не зависит,
мы ее как бы как множитель вынесли. Вот, экспонента, а дальше мы просто с вами логарифмируем по сути
то, что здесь написано, то есть это будет логарифм λ, плюс, вернее, минус, λх.
Поняли? Вот, мы получили ровно в таком виде. То есть смотрите, плотность ваша распадает на что-то,
что не зависит от параметра. В нашем случае вот, этот индикатор, он никак не зависит от параметра.
И на экспоненту в степени вот такая сумма. А 0 это вот какая-то функция от параметров,
которая не зависит никак от элемента выборки. Вот она, это логарифм λ. И дальше идет к слагаемым,
где к – это количество параметров. В нашем случае параметры только один, поэтому у нас одно слагаемое,
которое от х будет зависеть. Это вот лямда х. Понятно. Ну и очевидно, что логарифм лямда и лямда – это
линейная независимая функция. Вот. Следовательно, мы получили, что вот экспоненциальное семейство,
семейство экспоненциальных распределений, действительно относится к экспоненциальному
семейству. Вот, упражнения. В этом блоке вас могут попросить показать, что там нормальное распределение,
бета-распределение, гамма-распределение и всякие остальные распределения относятся к этому
семейству. Просто плотность раскладываете? Да, просто плотность раскладываете, показываете,
что оно действительно представимо вот в таком виде, где вот эти функции линейной независимой ровно
к, ну и т.е. к плюс 1 на самом деле. Вот. Если это выполнено, то на самом деле ваше семейство,
которое вы рассматриваете, экспоненциально. И вот теорема утверждает, что эффективная
оценка существует тогда и только тогда, когда вы работаете с экспоненциальным семейством распределений.
Т.е. пример неэкспоненциального семейства распределений, например, это равномерное
распределение на отрезке 0,1. Неэкспоненциальное. Неэкспоненциальное. Ну, у вас там просто константа,
по сути, которую как экспоненту никак не расписать. Ну, у вас там индикатор,
поделись на константу. Она как экспоненту не расписывается. Вот. Это первая теорема. Соответственно,
если вам дадут в данном блоке какую-нибудь задачу с неэкспоненциальным семейством,
ну непонятно, что делать, что от вас хотят. Наверное, просто бы как-то их посравнивать
в средне квадратичном подходе, посчитать к ним дисперсии, чем-нибудь еще. Но вот теоремы
Рао Краймера и все дальше уже можно, в принципе, не применять. Хотя, на самом деле, применять можно,
но как бы эффективную оценку вы никак не построите не для экспоненциального семейства. Вот. Сейчас
могут спросить, существует ли для данного распределения какая-нибудь эффективная оценка.
Вы скажете, нет, не существует, потому что оно неэкспоненциальное, допустим. Да. Тут как бы критерий.
Это первая теорема. Вот. На самом деле, с экспоненциальными семействами там очень много
содержательных задач, и чтобы их решать, нам нужны еще две теоремы.
Про информацию Fisher мы пока с вами немножко забудем. Еще парочку утверждений.
Не представимо в таком виде. Ну, это, кстати, интересный вопрос.
Вот определение экспоненциального типа, что представимо в таком виде, а как показать, что
непредставимо. Так, есть, наверное, какие-то функции, которые в виде экспонента непредставимы.
Хороший вопрос, я подумаю. Давайте пока дальше. Вопрос хороший, я постараюсь на него ответить.
Так. Теорема два, которая нам понадобится.
Критерии эффективности.
Вот. Получается, теорема номер один говорит о том, что у вас эффективная оценка существует
тогда и только тогда, когда вы работаете с экспоненциальным семейством распределений.
Теорема номер два говорит, что если у вас есть какая-то эффективная оценка, вот эта эффективная
оценка, то вот это, то есть вы можете сам параметр оценивать, можете функцию какую-то параметр
оценивать. Вот. То она на самом деле линейно зависит от клаба выборки. Вот. Вот это мы с вами уже
поняли, что это такое, да? Это там типа производная, ну, природная логарифма, а там функции
правдоподобия для всей выборки. Вот. И более того, что можно сказать, что вот это равенство
выполнено тогда и только тогда, когда вот C от T представимо вот в таком виде. То есть как
бы в некотором смысле все эффективные оценки для модели, они линейно между собой зависимы,
потому что они у вас линейно зависимы складом выборки, соответственно, они между собой будут
тоже все линейно зависимы. То есть если вы найдете хотя бы одну эффективную оценку, вы найдете их все,
потому что все остальные будут просто линейно от нее зависеть. Это мы сейчас воспользуемся этим фактом.
Вот. Информация фишера от выборки, да. Так, и наверное нам уже нужна еще одна теорема какая-нибудь.
Или достаточно пока что? Слушайте, а пока что достаточно. Мы с вами только давайте еще
вот одно утверждение докажем вспомогательное, которое очень помогает при решении задач.
Давайте мы с вами поймем, что найти эффективную оценку для экспоненциальной модели,
когда у вас параметры один, очень просто. Так, утверждение, которое нам поможет при решении задач.
Пусть у вас семейство экспоненциально, и пусть размерность параметра это единичка.
В таком случае на самом деле очень легко сразу предъявить для какой функции от параметра
существует эффективная оценка. В таком случае вот через вот такую запись. В таком случае
вот такая статистика, то есть усреднение t от x будет эффективной оценкой.
Чтобы это утверждение доказать, нужно немножко понимать, как доказывается вот это утверждение,
что эффективная оценка существует тогда и только тогда, когда экспоненциальное семейству существует.
Давайте мы сейчас это аккуратно с вами докажем. Данного? Смотрите, мы сказали,
что эффективная оценка существует только если вы что-то оцениваете в экспоненциальном семействе.
Возможно у вас не существуют несмещенные оценки для самого параметра θ. Возможно у вас
существует несмещенная оценка только для какой-то функции от θ. Мы знаем, что все такие эффективные
оценки между собой связаны линейно. То есть на самом деле, если у вас существует несмещенная
оценка для tau от θ, то у вас будут проблемы для поиска несмещенной оценки вот для такой
функции tau квадрат θ, потому что они не линейны между собой. То есть на самом деле,
чтобы найти какую-нибудь эффективную оценку, вам достаточно предъявить хотя бы одну,
а все остальные от нее будут зависеть линейно. Вот, и в одномерном случае найти хотя бы одну
очень просто. Утверждение вот на доске, что чтобы найти эффективную оценку, можно рассмотреть вот
такую оценочку. t от y берется вот из-за определения экспоненциального семейства. Давайте это с вами
докажем. Это как раз таки вот функция с параметром. Доказательство. Оно очень простое.
Вот, докажем утверждение с вами сейчас. Давайте рассмотрим, как у нас выглядит.
Функция вклада от выборки. По определению. Что это у нас такое? Нет, это информация фишер.
А вклад выборки? Мы считаем, что параметры у нас один. Логарифм. Ну и давайте сразу для
выборки, поэтому у нас есть функция правдоподобия. Но мы с вами знаем, что так как семейство
экспоненциальное, то у него плотность представима вот в таком виде. Справедливо? Да, давайте мы с
вами вот просто воспользуемся вот этим определением и поставим его вот сюда. Что мы с вами получим?
Мы с вами получим, что вот это от x. Так, что мы хотим сказать про это? Смотрите, логарифмы h от x при
логарифмировании вылезут в отдельные слагаемые, поэтому когда мы будем брать от них производные,
они все зановятся. Останется на самом деле только вот то, что вот здесь в экспоненте. Так, по порядку.
Смотрите, мы хотим понять, как у нас выглядит вклад наблюдения.
Вот это функция ваша.
А 0 и 1 вот у вас из определения экспоненциального семейства идет. Еще раз у вас параметр одномерный,
у вас только 0 и 1 существует. Еще раз, давайте зарекапим. Теорема 1.
Эффективная оценка существует тогда и только тогда, когда рассматривая семейство распределения
экспоненциально. Экспоненциально значит ее функция плотности представима вот в таком виде.
Это первое утверждение. Второе утверждение. Давайте тогда к утверждению перейдем сразу.
Смотрите, если у нас одномерный случай, то вот такая оценочка из вот этого определения t от x
средняя будет эффективной оценкой для вот такой функции от параметра.
Что значит средняя? У нас же там таблик, это просто какая-то функция от x.
Да, но это смотри, для одного наблюдения у тебя же здесь их будет целая выборка.
Сейчас докажем. Из доказательства будет понятно. Договорились?
Окей, что мы хотим сделать с вами? Мы хотим понять, как у нас выглядит вот эта штука.
Как выглядит вклад наблюдения? По определению, это просто производная патета, логарифма функции
правдоподобия, ну для выборки. Для одного элемента это просто плотность, для выборки это функция
правдоподобия. Давайте ее... Функция правдоподобия у нас по условию, так как семейство экспоненциально выглядит
таким образом. Давайте логарифмировать. У вас вылезут слагаемые с h от x, производные патеты,
они все за нулятых, потому что вот эти штуки от это не зависят. У вас по сути останутся только слагаемые
производные вот таких штучек. То есть на самом деле здесь у вас получится просто n а 0 штрих
плюс сумма t и x на а1. Согласны с этим?
Экспонента у нас пропала при логарифмировании. При логарифмировании у нас вылезли логарифмы вот этих
вот слагаемых, которые от это не зависят, они у нас занулились, потому что мы берем производную. И остались у нас
еще... Мы взяли еще логарифм экспоненты. По сути у нас мы складываем вот это для n элементов выборки,
ну потому что у нас здесь функция правдоподобия.
А что там написано плюс?
Перепишу.
Плюс а1 штрих в точке θ, сумма t и x их по и от единички до n.
Так, куда n?
n, потому что у вас здесь для выборки. Выборка размером n.
Там произведение плотностей записано? Ты их логарифмируешь?
Становится сумма их.
Ну возьмем логарифм от вот этого вот. От произведения вот таких штук.
А это p и x, вот эта вот штукка, которая у нас записана, записана где x это одна звучанная или что?
Вот снизу, тут оттуда сейчас покажешь.
Да, это p от x, это плотность.
Это размерность параметра.
Что?
Это размерность параметра.
Ну да, то есть у тебя может быть распределение, которое двух параметров, а трех зависит.
Мы сейчас рассматриваем частный случай, когда у тебя параметр одномерный, то есть один.
И в таком случае у тебя просто а1 от тета. То есть из всех этих слагаемых у тебя здесь только одно слагаемое, а один от тета.
А откуда вот эти n слагаемых появляются? Ну потому что это функция правдоподобия.
Это просто произведение n плотностей, мы их логарифмируем и берем производную.
Ну в общем, если посчитаете, у вас получится ровно вот такая штука.
Берем?
Что еще намножили?
Ну вот тут наверное, ну второе слагаемое.
Вот это?
Найдомножили, а второе слагаемое?
Да потому что здесь у тебя n слагаемых.
Они разные, их нельзя найдомножать.
Так, ладно, давайте аккуратно доделаем.
Хорошо, а как у нас будет выглядеть функция правдоподобия в данном случае?
Это будет просто произведение h от x экспонента a0 плюс a1 tx.
Tx и t. Согласен? Егор?
Произведение?
Да, функция правдоподобия, произведение от единички dn плотностей в каждой точке.
Я уже понял, почему я что-то спрашивал.
Почему вот это верно, да?
Все, берем логарифм от этого, у нас выживают только вот эти слагаемые.
И у нас их n штук.
N на ноль и плюс tx и t на a1 штрих.
Штрихи появляются производные, потому что мы берем производные.
До этого шага всем понятно? Садык?
Ну, до этого...
Все, отлично. Мы в одном шаге с вами.
На самом деле, товарищи, мы в одном шаге до победы.
Почему? Потому что, смотрите, у нас есть вот эта теорема, критерии эффективности.
Критерии эффективности что утверждает?
Что если у вас есть эффективная оценка, то она линейно зависит от вклада в выборке.
У нас почти есть линейная зависимость от вклада в выборке.
Понимаете?
Почему?
Почему вы не понимаете?
Понятно, кто от чего зависит, кто от чего должен голосовать.
Ну, то есть, я не вижу аналога, я не утвержден.
Хорошо, мы сейчас аккуратно это выпишем.
А мы доказываем теорию.
Нет, мы доказываем утверждение.
Мы хотим воспользоваться теоремой.
Мы хотим сейчас воспользоваться теоремой. Это критерии эффективности.
Если мы показываем, что наша оценка представимого в таком виде, то все будет хорошо.
Давайте поделим на n и поделим на a1, наверное.
Ну да.
Что у нас получится?
У нас получится aθ от х поделить на n, а1 от это.
Ну, полагаем, что это все там в ноль не обращается, и с этим все хорошо будет.
Это будет a0.
А1т от это.
Плюс?
Т средний.
Т средний, да.
Ну, как раз таки, т средний.
Как раз таки, т средний.
Что мы с вами получили?
Мы получили, что вот, это наша оценка, ну или статистика.
Вот, это она.
Минус функция, которую мы оцениваем.
Функция, которую мы оцениваем, вот она.
Линейно зависит, то есть вот это какой-то коэффициент, который зависит от это.
От вклада в выборке.
Вот у вас вклад в выборке, а вот у вас коэффициент, который зависит от этого.
Понятно?
Все, мы получили с вами в одномерном случае способ, как построить эффективную оценку.
Понятненько?
Вот, то есть мы просто с вами, в доказательстве этой теоремы мы чем с вами воспользовались?
Мы просто каким видом у нас выглядит плотность экспоненциального распределения,
записали просто аккуратно, как у нас выглядит вклад в выборке,
и воспользовались теоремой о критерии эффективности,
о том, что у нас оценка эффективна тогда и только тогда, когда она линейно зависит от вклада в выборке.
Отлично, все, вот с этим уже можно решать практически любые задачи, которые у вас могут быть.
Какие задачи у вас могут быть?
Первое, ну, наверное, нужно будет понять, ого, нужно будет понять, является ли семейство экспоненциальным.
Это вот лучше выписать, потому что так аккуратнее и, возможно, вам это поможет в решении.
Мы сейчас с вами сразу задачку решим.
Ну, пока не придут, значит.
Так, задача.
Так, задачка номер четыре.
Пусть у нас есть выборка, x1 и так далее, xn, из геометричества с параметром f.
Встает вопрос.
Первый, найдите какую-нибудь эффективную оценку, любая, для какой-нибудь функции от параметра.
Для какой-нибудь функции от параметра.
То есть, найти какую-нибудь функцию от параметра, для которой существует эффективная оценка.
Это первый вопрос.
А второй вопрос, найти информацию фишер одного наблюдения.
Мы сейчас эту задачу просто двумя способами решим, и все будет хорошо.
Первая.
Так, ну, про дискретную плотность вам, наверное, уже говорили, да?
Что функция вероятности, вот, которую у нас, помните, была в дискретном случае.
То есть, вы каждой точке сопоставляете вероятность.
На самом деле, тоже является плотность в посчитающей мере.
Отлично.
Соответственно, давайте запишем с вами просто плотность геометрического распределения.
Как задается?
Ну, задается как 1 минус по степени x на p.
Геометрическое распределение говорит о том, сколько неудач должно произойти до первой удачи.
То есть, вот, 1 на минус p – это вероятность неудачи.
В степени x – это сколько у нас таких неудач произошло, и потом должна произойти удача.
И х у нас целое?
Да.
Ну, считающая мера, мы считаем, что х у нас, типа, целое.
В данном случае х принадлежит к концу, то есть, натуральным числам.
Вот.
И является ли вот это экспоненциальным распределением?
Да.
Да.
Ну, ответ правильный.
Почему?
Потому что плотность представима вот в таком виде, в каком нам надо.
А в таком виде это в каком?
Берем экспонент, или как?
Берем п, экспонента, 4.
Нет, p, видите, мы все в экспоненту должны, что от параметра зависит, должны закинуть в экспоненту.
У вас получится экспонента, логарифм.
Логарифм от этого это что будет?
Логарифм p, наверное, плюс х, логарифм 1 минус p.
Вот в таком виде у вас представима функция плотности.
Высадим согласны?
Отлично.
Значит, это экспоненциальные распределения.
То есть, на самом деле и биномиальные, и вернули, и геометрическое, вот такие дискреты распределения тоже входят в класс экспоненциальных распределений.
Значит, для них есть смысл искать эффективную оценку.
То есть, условия задачи нормальные.
Теперь давайте воспользуемся утверждением, которое мы только что доказали.
Мы только что с вами доказали, что существует эффективная оценка для вот a0 штрих тета поделить на a1 штрих тета.
В нашем конкретном случае a0 это логарифм p, а 1 это логарифм 1 минус p.
Таким образом, мы с вами получаем что?
Так, t от x у нас это просто x, то есть, x средняя это эффективная оценка для производного логарифма это 1 на p, здесь еще минусик будет, 1 на 1 минус p.
Да, должен вылезти минус вот здесь, когда будете производным брать.
Вот, ну и получается это эффективная оценка для 1 минус p поделить на p. Справедливо?
Все, мы нашли с вами функцию от p, для которой существует эффективная оценка.
Утверждение.
t от x является эффективной оценкой вот такой штуки, когда параметр одномерный.
Параметр у нас одномерный, вот это t от x это просто t от x по сути в твоей плотности.
Логарифм 1 минус p это a1 от teta, ну a1 от p, а логарифм p это a0 от p.
Ну и соответственно пользуемся утверждениями, которые мы только что с вами показали.
Таким образом, мы нашли с вами функцию от p, для которой существует эффективная оценка.
Супер, что мы дальше хотим сказать?
Как будем искать информацию фишера?
Информацию фишера можно найти двумя способами.
По определению, то есть мы сначала считаем, как выглядит вклад наблюдения, а потом считаем информацию фишера через интеграл квадрата.
И второй вариант, воспользоваться критериям эффективности.
Давайте двумя способами.
А там мы и в форму, что информация фишер, это минус на к заданию второй производины.
Логарифм и фишер.
Ну да, так тоже можно, они эквивалентны, там это не сложно показать.
Но это все еще через определение, как бы считать.
Есть какие-то теоремы, которые помогут нам это решить проще.
Вот.
Соответственно в нашем случае теорема, это опять-таки критерии эффективности.
Который был записан на этой доске до того, как я его стер.
Давайте еще раз запишу.
Что у нас там должно быть?
У нас наша оценка, минус функция, которую мы оцениваем, должна линейно зависеть от клада выборки.
Но там еще было одно важное условие, что вот эта константа,
она имеет какой-то конкретный вид.
И какой-то конкретный вид, это на самом деле производная теория на информацию фишер от всей выборки.
Супер, да?
Наверное, появилось понимание теперь, как это дорешивать.
Как сам делить?
Ну, смотрите.
Давайте мы с вами поймем что-нибудь.
Справа, это значит, это н на и умалое.
Да, как и в любой другой задаче, потому что информация фишер линейна.
То есть, смотрите, давайте заметим с вами.
Вот эта кон visas.
И вот эта кон vis.
И вот эта кон vis.
И вот эта кон vis.
И вот эта кон vis.
И вот эта кон vis.
И вот эта кон vis.
Давайте заметим с вами. Вот это у нас известно? Какая оценка нам подходит?
Известно, вот она. Какую функцию мы оцениваем известно? Вот она. Производную
эту функцию, наверное, посчитаем. А по сути нам нужно найти вклад выборки, и тогда
мы найдем информацию фишера. То есть отсюда из критерии эффективности на самом
деле легко найти информацию фишера, то есть информация фишера, она линейна,
поэтому это N на информацию фишер одного элемента. Было что-то такое? Да не, нормально,
уже 11 часов просто, это нормально. Так, ну и что, как у нас будет выражаться? Здесь
будет вклад от выборки. Здесь у нас будет производная нашей функции. Я теперь
просто тета поменяю на п, чтобы вот чуть понятнее было. Так, а в номинателе у нас
будет с вами оценка. Минус tau от тета. Ещё N. И N, да, ты прав, абсолютно. Только хотел написать вот так.
Супер. То есть чем это проще, чем искать по определению, вам не нужно от ожидания квадрата
считать. То есть это чуть-чуть проще будет, но немного. Так, что у нас информации фишера получается?
А производная чему равна? Минус один на п квадрат. Тут минус единичка, так что все. Так, дальше сейчас
вклад выборки посчитаем. Что у нас в номинателе записано? X средняя. А, на самом деле подождите,
мы можем с вами заметить один факт. Ведь вот это равенство верно для любого N, то есть для любого
размера выборки. На самом деле мы можем при N равным единичку рассмотреть данное равенство.
Поэтому вот на самом деле давайте чуть-чуть упростим себе задачу и сотрём здесь N.
Ну потому что критерия эффективности выполнена для любого N.
Для любой выборки, для выборки любого размера это выполнено. Давай рассмотрим его для выборки
размера 1. Вот критерия эффективности. Критерия эффективности утверждает, что оценка является
эффективной, если она линейно представима через вклад наблюдения, ну вклад выборки.
Поделить на вот, ну умноженную вот такую константу. Да, то есть это на самом деле выполнено для любой
выборки, для выборки любого размера. Если выполнено для выборки любого размера, давайте рассмотрим для
выборки размера 1. Что мы с вами получаем?
Просто X.
Так, одну секундочку. Тау, ой, прощепочини, у меня тут немножко поплыли. Тут везде П должна быть.
П, П, П, П, П. Здесь тоже должно быть П.
Потому что тут все было в терминах тета, а мы теперь применяем в наши задачи, а у нас параметр П.
Поэтому мы его назвали П. Так, производная. Вот, Х минус функция, которую мы оцениваем, это единичка
минус П на П. Вот, и вопрос только, чему равен вклад одного наблюдения. Давайте запишем его сразу в
развернутой форме. Это будет производная по П в логарифме плотности. Так, плотность у нас как выглядит?
Мы с вами записывали ее, вот она. 1 минус П от Х на П. Давайте вот это выражение где-нибудь
отдельно почитаем. Вот здесь? Ты прав, кстати, абсолютно. Берем логарифм, получается у нас
экспонента просто умирает. Берем от нее производную, это 1 на П будет, да? Давайте дочитаем. Так, если мы
логарифмируем, у нас экспонента уходит. Логарифм П производный, это что у нас? 1 на П. Так, плюс,
а может и минус. Действительно, минус. Х у нас остается. Производная логарифма.
И делим это все на, так, тут немножко смешалось, прошу прощения. Х минус 1 минус П на П. В принципе,
почти все получилось, осталось чуть-чуть подсокращать там и... Ну да, это почти ответ.
Тогда у тебя здесь вылезло и к средне, и было просто неприятно. А может мы разучили
дифференцировать? Что, мы как-то не умеем дифференцировать, да, уже? Ну, в истях уже не тот.
Смотри, плотность вот у тебя экспоненциальная, логарифм просто закрывает экспоненту, берем
производную от того, что здесь. Производная вот эта 1 на П, плюс производная вот этой штуки. Х умножить
на производный логарифм 1 минус П. Минус 1 поделить на 1 минус П. От одного наблюдения?
Информация одного нет. Информация у тебя же линейная. Они независимы между собой. Как бы
абстрактно, если понимать, то у тебя независимые случайные величины не дают как бы информации друг
другу. Нет. Информация фишер для одного? Было в знаменателе N. Но у нас здесь, видишь,
оценка тогда не X будет, а X средняя. А в X среднем у тебя N зашит в знаменателе. Они сократятся,
там останется сумма. И потом у тебя еще вклад выборки будет считаться уже не как логарифм плотности,
а как логарифм функций правдоподобия. Так, давайте досчитаем. Чуть осталось уже. Смотрите,
приведем просто к общему знаменателю. Что здесь получится? Это нужно на П, наверное,
домножить. А вот это нужно на 1 минус П домножить. В знаменателе что у нас получится? XP делим
на П. Так, давайте досчитаем аккуратно. Потому что плохую задачу бросать, когда она дорешенна
почти. И не сделать в выводах... Ты не поверишь, это правда. Так, П квадрат точно выживает. Тут у
нас что будет? 1 минус П минус XP. А здесь 1 минус П минус XP. Ну по сути сокращается,
только еще минусик вылазит, да? В знаменателе у нас что идет? П на 1 минус П? А в числителе
у нас идет П. Ну еще знак минус плюс. Итого мы получаем, вот эти П сократились. 1 на П в квадрате
на 1 минус П. Это информация Фишера для данного наблюдения. Нет, нет, нет, нет, она не должна
зависеть. Да, да, да, да. То есть, если бы вы взяли выборку большего размера, у вас бы X тоже
сократился, у вас вот здесь было бы X среднее, а здесь у вас было бы, ну, понимаете, да? Функция
правдоподобия между плотностями. И все было бы хорошо. То есть, смотрите, это просто как бы какая-то
чиселка информации Фишера. Сколько информация себе несет одно наблюдение, неважно какое,
это усредненная величина. Вклад наблюдения зависит от X, а как бы информация Фишера,
она уже усреднена по всем X. Давайте какой-нибудь вывод постараемся сделать вот из этого. Мы можем
сделать какой-нибудь вывод из того, что информация Фишера выглядит таким образом.
Не, давайте какие-нибудь крайние случаи просматриваем. Ну, как бы там вот бесконечности
нужно, наверное, поаккуратнее быть.
Да, то есть при фиксированных каких-то это будет, типа мы все знаем сразу, это правда.
Так, давайте вот в общем виде Раокраймера запишем. У вас на самом деле, если вы оцениваете
функцию от это, то у вас в числителе стоит уже не единичка, а производная. Вот, и делите вы на
N информации Фишера от этого по выбору. Такая штучка у нас получается. Это вот не равен второго
Краймера. Чем меньше информации, тем больше ваша дисперсия. Чем больше информации,
которую вы несете, тем меньше ваша дисперсия может быть. Чем меньше информации одного наблюдения,
тем больше дисперсия. То есть тебе каждое наблюдение в среднем мало информации дает,
соответственно дисперсия твоей оценки будет очень большая. Если информация Фишера одного
наблюдения очень большая, то есть тебе много информации приходит от каждого наблюдения,
тогда наоборот дисперсия может быть очень маленькой. Понятно, да? В этом разделе,
в этом разделе, значит вас могут попросить либо сравнить оценки в каких-нибудь подходах,
повторяйте, что еще можно сделать. Либо, что скорее всего вам дадут какое-то экспоненциальное
семейство и попросят предъявить эффективную оценку. В таком случае вот воспользуйтесь просто
утверждением, которое мы доказали. В одномерном случае оно очень хорошо работает. Скорее всего
лучше показать будет. Но оно показывается очень просто. Теоремами можно пользоваться,
утверждением не очень хорошо пользоваться, потому что оно использует как бы элементы
доказательства. Само доказательство этого факта как бы оно появляется при доказательстве того,
что у нас, если семейство экспоненциальное, существует эффективная оценка. Вот это утверждение,
это кусочек доказательства, вот эти аремы об том, что эффективная оценка существует тогда и только
тогда, когда семейство экспоненциально. Не будем вторым способом искать, я передумал. Вторым
способом информацию фишера можно было посчитать по определению. То есть вы бы посчитали вклад,
вклад мы с вами вот вместе посчитали уже здесь. И дальше осталось на самом деле вот это просто
проинтегрировать с плотностью в квадрате. Ну типа там там от ожидания вклада в квадрат идет.
Если бы вы посчитали это честно, то же самое бы получилось на самом деле. И еще вот в этих
задачах как можно сработать. Вот типа смотрите, вас просят предъявить какую-нибудь эффективную
оценку. Вы можете посчитать информацию фишер сначала по определению, а потом сказать,
что я знаю оценку, для которой выполняется равенство в неравенстве раокрамера. И все. Ну типа вы
можете как бы подобрать, просто подобрать. Типа ну видите х средняя, это какая-то очень очевидная,
да это была оценка? На самом деле я вам больше скажу, вот это это среднее значение, ну типа
этом от ожидания в геометрическом распределении. То есть на самом деле мы бы то же самое получили,
если бы мы с вами просто бы решали методом моментов. Метод моментов вам дал бы то же самое. 1
минус p на p равняется х среднему. Потому что вот это там от ожидания х. То есть здесь можно было
и угадать. Неравенство раокрамера? Просто считаешь дисперсию данной оценки? Считаешь
информацию отдельно? По определению считаешь информацию фишера, ну и просто считаешь дисперсию,
как мы обычно считали дисперсию оценок. И показываешь, что у тебя выполнено равенство в
неравенство раокрамера. То есть вот тебе нужно посчитать по сути вот это и вот это по отдельности.
Интеграл, ну от ожидания от вклада элементов в квадрате. В смысле? Для данной задачи. Для
данной задачи информация фишера одного наблюдения, как и информация фишера для любой другой задачи.
Это просто математическое ожидание вклада в квадрате.
Соответственно это у нас что такое? Интеграл. Вклад мы с вами вот тут посчитали.
А тут будет 1 минус П? Минус ХП. Вот. Ну еще это нужно на плотность домножить. Потому что это же
мат ожидания. То есть вот функция ваша. Еще нужно на плотность домножить. А плотность у нас какая была?
1 минус П в степени Х, да. На П. Тут даже что-то посокращается. Здесь будет 1 минус П в степени Х минус 1.
Да, это правда. Вот это в квадрате. ДХ. Нет, ты усредняешь по Х. Да.
Вот этот интеграл превращается в сумму. Ну типа в общем случае интеграл, но интеграл по считающей мере
действительно это сумма. То есть на самом деле здесь будет даже не интеграл. Абсолютно спасибо за
замечание. То есть у вас здесь будет сумма по Х принадлежащим натуральным числам. Вот такого ряда.
Вот. А суммируете вы получается квадрат вот этой штуки. То есть квадрат вклада наблюдения на плотность.
Плотность у вас выглядит вот таким образом. Ну и кажется этот ряд можно посчитать, потому что это
просто какой-нибудь геометрический ряд будет. Ну сумма геометрической прогрессии. Или не совсем.
Наверное не совсем. Ну вот у вас Х это значит будет геометрическая прогрессия, но у вас тут
еще будет Х-овый коэффициент. Ну короче через ряды Тейлора можно посчитать. То есть вы берете
ряд Тейлора, его дифференцируете в области исходимости и у вас там получается. Такую сумму можно
посчитать, но это гораздо сложнее может быть. Чем? Да правда можно. Да.
Ну у тебя если бы вот отсюда вылезло. У тебя же здесь не вклад наблюдения, а был бы вклад выборки.
То есть у тебя еще отсюда бы вылезло много слагаемых. У тебя в любом случае бы здесь
числитель и знаменатель с Х и с Н посокращался бы. Можешь проделать просто. Вот. Еще говорят,
что информацию Фишера можно по-другому считать, да? Что там на Википедии пишут?
Так, да. Наверное это не совсем важно. Давайте пойдем дальше. В общем понятно, да? Что вас в данном
разделе могут попросить посчитать? Информацию Фишера? Не важно для какого распределения.
Попросить какого-нибудь там через раукраймера что-нибудь поценить. Сказать существует ли
эффективная оценка, но если экспоненциально существует, если не экспоненциально и не
существует. Вот. А дальше вас могут попросить привести эффективную оценку. Тогда вы пользуетесь
либо утверждением, либо критерием, либо чем-нибудь вот из того, что мы обсудили. Либо можете угадать.
Понятно? Отлично. Задача номер пять. На самом деле две. Не, они посложнее.
Новая тема. Так, ну давайте сначала доверить на интервалы обсудим, а потом к полным статистикам
перейдем. Достаточно. К оптимальным оценкам и ко всему вот этому.
У вас пробайс уже был? Рано? Это просто один из последних семинаров по идее.
Так, ну ладно, у кого будут бояться, сочувствую. Наверное. Так, задача номер пять. Тема доверить на
интервалы. Вот. Мы сейчас с вами быстренько пару способов разберем основных классических и все
поймем. Доверить на интервалы. Значит так, давайте кто-нибудь расскажите, почему есть мотивация
строить доверительные интервалы. Ну это правда. Давайте быстренько с вами какую-нибудь
игрушечную задачу разберем. Вот вас просят оценить. Вас просят оценить, а какая там величина там? Сейчас.
Вот допустим вы действительно строите плотину и у вас есть случайная величина, которая обозначает
уровень воды в реке. Вот. И вы наверное хотите что-то пооценивать, что-то с этим сделать. Вы
можете взять среднее, но наверное среднее вас не очень интересует. Потому что вас же гораздо
больше будет интересовать там какой максимальный уровень, например, да? Потому что плотину нет
смысла строить для среднего уровня, есть смысл строить для максимального уровня. Вот. Но на
самом деле, типа плотину высотой 10 метров вы скорее всего не построите, вы хотите построить какую-то
плотину, чтобы она там в 99% случаев вас спасла. Ну потому что на 10 метровую плотину у вас линей нет.
Таким образом, ну и вот допустим у вас уровень воды в реке подчиняется какому-нибудь вот такому закону.
Там хотя бы один метр, ну и до тета. На самом деле нам достаточно построить доверичный интервал.
Что такое доверичный интервал для параметра? Это такая пара статистик.
Что ваше значение с большой вероятностью лежит в этом интервале? Больше либо равно. Больше либо равно
уровня доверия. То есть грубо говоря вы говорите, что с вероятностью 99% уровень воды в реке будет
там от одного метра до трех метров. Ну понятно, что может быть сильнее там скакнуть, но вероятность
этого очень мала. Вы строите как бы такой интервальчик, в котором с большой вероятностью
истинное значение параметра будет лежать. Тогда вы можете как бы ориентироваться на только вот
эти значения, из большой вероятности именно они и будут выполняться. Что такое доверительный
интервал? Смотрите, у вас есть вот ваша выборка и вы как бы хотите какую-то оценку для нижней
границы построить, какую-то статистику, которая снизу подобрет и какую-то статистику, которая
сверху подобрет ваш параметр. Так чтобы вероятность того, что ваше истинное значение параметра
попадет между этими двумя статистиками, было больше либо равно гамма. Вот эта гамма чаще всего
полагаю там 0.99. Там 0.95 можно, да. 0.9995 или там всякие рассматривают. Нет, статистики. А,
хотя оценки, да, да, ты прав. Вообще в общем случае это пара статистик, потому что ты можешь
какой-нибудь глупый интервал придумать. Там от минус бесконечности до плюс бесконечности,
когда у тебя параметры только положительные. Ну типа это тоже доверительный интервал будет,
нормально. Ну поэтому это говорят что статистики. Вот, что еще интересного хочется сказать. Да,
наверное, ничего. Наверное, можно решать. Какие у нас есть методы построения доверительных
интервалов? Через центральную статистику это основной? Самый скучный метод это через Чебышова,
да. Мы его даже рассматривать не будем, потому что он плохой. Мы сразу будем через центральную
статистику действовать. Итак, метод центральной статистики. Если что в методичке Родионова-Шабанова,
там разобран пример с неравенством Чебышова. Вот. Кстати, хороший вопрос. Вот смотрите,
вы построили два доверительных интервала для параметра. Одного и того же уровня доверия.
Один оказался вот такой маленький, а второй вот такой большой. Какой из них двух вы предпочтете?
Ну, который поменьше, потому что вы хотите более точно как бы с той же вероятностью в более
маленький отрезочек загнать. Понятно что? Они могут содержаться, могут не содержаться.
Просто тебе, просто Чебышев чем плох? Тем, что у вас там оценка сверху идет через дисперсию
на епсилон в квадрате. На самом деле, если вы епсилон возьмете очень маленький, то у вас
здесь оценка будет для вероятности меньше там десяти. Ну, то есть вот это число может быть
больше единицы. Тогда это какая-то глупая оценка на вероятность получится. Поэтому,
на самом деле, Чебышев вам построит какой-то очень большой интервал. Вот. А центральная
статистика вам построит хороший интервал. Он может быть не самый лучший, но хороший.
Который вам подойдет для решения задач точно. Итак, что же такое центральная статистика?
Центральная статистика это такая функция g, вернее это статистика g от х тета. То есть
она принимает во внимание данные, которые у вас есть и принимает во внимание параметр неизвестный.
Вот. И сейчас будет немножко так крышесносная штука. Первая. Мы с вами говорим, что вот эта
функция, если мы рассмотрим ее как функцию по параметру тета. Давайте даже, чтобы понять,
что это параметру тета, мы с вами ее вот так обозначим. То есть выборка фиксированная,
а функция по тета. Она должна быть перерывная и монотонная. А второе. Сама функция, ну сама
статистика, как видите, от тета зависит, но ее распределение не зависит от тета. Понятненько?
Да, то есть ваша функция на тета как бы учитывает, а распределения нет. То есть в некотором смысле вы
решаете уравнение здесь с неизвестной тета. Сейчас поймем. Задача. В смысле, это вот,
еще раз, вот это это статистика, это какая-то измеримая функция от выборки. Смотрите,
х вы зафиксируете какую-то выборку. Теперь же вы можете смотреть на это как на функцию от тета. То
есть вот условно давайте рассмотрим а на х средняя. Ну типа нормальная модель. Вот какая-то непонятная
статистика, она не будет центральной, я в этом уверен, но давайте просто посмотрим. Видите,
она как бы зависит от параметра неизвестного и от данных. Если мы данные зафиксируем,
то это просто функция по параметру. И вот в этом смысле, что если у вас фиксирована выборка,
то под тетой у вас должна быть непрерывность и монотонность. Сейчас поймем для чего. Вот,
это первое. А второе, ее распределение не должно зависеть от это. Вот, ну давайте сразу задачку
просто решать. Задача очень интересная и очень необычная. Я бы даже сказал нестандартная.
И нетривиальная, кстати. И достаточно сложная. Так, смотрите. Давайте мы с вами сложную задачу
разберем, чтобы вы простую не смогли решить. Ну это как всегда, сложные задачи разбирают,
а потом простенькие не решают. Но мы рассмотрим хорошую задачу. Смотрите,
x1, xn имеет экспоненциальное распределение с параметром тета1. y1, ym имеет экспоненциальное
распределение с параметром тета2. Вас просят построить доверительный интервал для отношения
параметра. Давайте секунду подумаем. Нет, это очень хорошая задача. Какого-то уровня доверия.
Ну уровень доверия произвольный, давайте гамма просто какой-нибудь затоссируем. Ну на самом деле у
нас будет просто функция этого и в зависимости от того, какой гамма тут подставишь, у тебя разные
доверительные интервалы будут. Да. Наверное, это сложный пример. Не, он вообще очень интересный на
самом деле и совсем непонятный, да? Ну смотрите, вот эти все независимо одинаково распределенные,
вот эти независимо одинаково распределенные. Можно было бы, конечно, рассмотреть вот экспоненциальную
модель только, но это было бы слишком просто. Дойти доверительный интервал для тета1 это очень
просто. Мы потом поймем, как в общем случае это делать. Иногда вам могут дать вот какую-нибудь такую
штуку оценить. Хорошо, методом центральной статистики. Нам нужно придумать какую-то
статистику, которая будет зависеть от тета1, тета2, но распределение которой не будет зависеть от
параметра. Идеи? Это плохо, это не работает. Почему это не работает? Потому что смотри,
ты берешь два доверительных интервала и вероятность того, что ты в один из них не попадешь,
типа это не произведение. Ну короче, там сложно будет с вероятностью, там не совсем так очевидно
получится, как ты хочешь. Средние не нужны. Я утверждаю вот что. Давайте рассмотрим статистику тета2.
Не, это просто сложный пример, а простой пример вы потом сами решите. Суть этой задачи в том,
чтобы придумать мотивацию, почему нужно общий случай для какой-то статистики центральной найти,
почему лучше пользоваться каким-то общим методом. Вот есть задачи, в которых общий метод не работает,
но чаще всего общий метод работает. И вот мы к общему методу придем сейчас с вами, решив очень
плохой частный случай. Нет, они все между собой независимы. Иксы не зависят с другими иксами,
иксы не зависят с другими игреками. В общем, на чем идея данной задачи строится? На том,
что, смотрите, если вы... Прошу прощения. На чем решение задачи строится? На том,
что параметр тета это на самом деле параметр масштаба. Если вы домножите экспоненциальное
распределение на константу лямбда, то у нее параметр просто поделится на лямбда. Это вы знаете?
Ну вот такое было. Это называется параметром масштаба, что если вы домножаете на константу,
у вас тоже типа либо умножается на константу, либо делится на константу. Ну давайте это с вами
аккуратно покажем. Вот смотрите, плотность phi от кси, где кси имеет просто экспоненциальное
распределение с параметром 1 пусть будет. phi от кси это просто лямбда кси. Ну просто
на какую-то константу намножаем. Давайте посмотрим, что произойдет с плотностью.
Давайте даже через функцию распределения это сделаем. Функция распределения phi от кси
в точке t, это вероятность того, что phi от кси меньше ли бы равняется t. phi непрерывно
монотонные, ну phi у нас это просто домножение на константу, то есть phi это на самом деле
просто лямбда x. Это на самом деле вероятность того, что кси меньше ли бы равняется, чем phi в
минус 1 от t. И это просто функция распределения кси в точке phi в минус 1 от t. С этим согласны?
Это просто простой трюк, мы его тысячу раз делали на тервере. Просто хотим найти распределение
экспоненциальной случайной величины, умноженной на константу. Так, ну смотрите, обратная функция
это что у нас? 1 на лямбда просто по сути, так ведь будет? Ну и все, вы просто в определение функции
распределения единицы минус экспонента было единица, да, 1 на лямбда t, вот на индикатор того,
что t больше либо на нуля. Все. То есть мы с вами что заметили? Что если мы домножаем с вами на
константу экспоненциальное распределение, то на самом деле это тоже экспоненциальное распределение,
но с параметром лямбда будет. Ну то есть в более общем виде, если кси имеет экспоненциальное
распределение с параметром m каким-нибудь, то лямбда кси будет иметь распределение экспоненциальное
m на лямбда. Понятно? Вот этим фактом мы хотим воспользоваться. Почему мы хотим этим фактом
воспользоваться? Потому что смотрите, если мы домножим... Да, вот здесь получим экспоненциальное
распределение с параметром 1 и здесь получим экспоненциальное распределение с параметром 1.
И получим какую-то по сути отношение экспоненциальных величин с параметром 1, скорее всего ее распределение
будет зависеть от параметра z. Давайте покажем это. То есть тут скорее само решение будет очень
простое, но вот придумать и доказать, что это подходит, сложнее гораздо. Так, а где мне писать?
Да, нет. Мы сейчас общий способ тоже разберем. Поехали. Нам нужно показать, что распределение вот
этой штуки не зависит от параметра. То есть нам можно найти плотность, например, у этой штуки,
так ведь? Давайте просто искать плотность. Так, наверное центральная статистика пострадает
сейчас. Я ее сейчас сотру. Давайте, ребята, крепитесь. Немного осталось. Как показать,
что распределение вот этой штуки будет не зависеть от это? Найти плотность, например. Как
найти плотность отношения двух случайных величин? По теореме Лебега о замене переменной.
Первый шаг. Смотрите, совместная плотность x и y это какая? В силу того, что они независимы,
это просто произведение плотностей. Так, мы тета1 обозначали. То есть это будет тета1
e в степени минус тета1 x на индикатор x больше либо равен нуля. И умножить на тета2 e в степени
минус тета2 на x индикатор того, что... Ой, y, простите. y больше либо равен нуля. С этим согласны?
Две случайные величины независимы, поэтому плотность их совместна. Это просто произведение
плотностей. Дальше делаем замену. Уровняется вот ровно то, что нам нужно. Так, y на x. А v,
ну потому что там дефиаморфизм должен быть, полагаем просто равным y. Да? Было что-то такое у
нас когда-то. Делаем обратную замену. y равняется v, x равняется v... сейчас.
x. Согласен, тета2 на тета1, а v поделить на u. Так, и кабиан.
Так, здесь по u это у нас будет тета2, тета1 на v в квадрате, так ведь?
По v. По v не важно, что здесь будет, потому что у нас по u здесь 0, а здесь будет единичка.
Нет, единичка. Так, согласны с этим? Так, ну и соответственно модуль и кабианы это у нас
тета2 на тета1, v на u в квадрат. Так, а у меня так получилось?
Так, и друзья. Не, все нормально. Дорешаем, я думаю. Так, чуть по-другому просто. Просто все по-другому
пошло, но все хорошо будет.
Себар, а 1,1 и кабианы почему там есть? Тут ничего нету, тут три точки.
Все, давайте считать новую плотность. p2v чему у нас равна? Нет, ну если мы будем брать интеграл,
нам нужно сразу по одной переменной усреднять. Давайте сначала совместную плотность найдем.
Тета1. Дальше. Да, е в степени минус тета1, вместо х подставляем. Тета1 сократится,
тета2 v на u. На индикаторе х больше, либо на 0 мы его можем забить, потому что там все будет
хорошо. Тета2. Так, вместо у мы подставляем с вами v, минус тета v. Тета2, да. Вот, на этот индикатор
забиваем и домножаем на модули кабиана. Модуль кабиана у нас как раз уже вот написан. Сейчас,
почему индикатор не записали? Ну потому что, смотри, вот это понятно, что это больше, либо равна 0?
Ну, вы и у тебя как определены? Это просто положительная случайная величина, которая больше,
а вот эти отношения положительных случайных величин. Хорошо, давайте домножим на индикатор того,
что u больше, либо равна 0, и v больше, либо равна 0. Хорошо, согласен. Так, и тогда давайте найдем плотность u.
Это просто усреднение по координате, которая нам не нужна. То есть это будет интеграл от 0 до
бесконечности. Что у нас, кстати, с тетами произойдет? Вот эта тета сократится вот с этой тетой?
А тета2 будет в квадрате, да. То есть у нас будет тета2 в квадрате, экспонента, минус тета2 v выносится,
1 на u плюс 1. Какая-то такая штука, да, получается? Еще на b на u квадрат, здорово. Этого нам не хватало.
Супер. Не, вообще все замечательно, на самом деле. Почему все замечательно? Нет, приведем это просто
к плотности экспоненциального распределения. Каким образом? Вот смотрите, вот это все у вас под
экспоненты, как бы. Смотрите, по v мы интегрируем, поэтому вот это у вас параметр λ, по сути. Давайте
просто параметр λ здесь тоже соберем, и это у вас будет плотность распределения, так ведь?
Что у нас, получается, должно быть? Тета2 на вот эту константу u плюс 1 на u. Тета2 у нас выносится,
одна вторая так, v остается внутри. Вот, и все это экспонента в степени, потому что у тебя здесь
она в квадрате, одну тета2 вынесем, а одну останем, чтобы у нас плотность осталась. Вот плотность для
экспоненциального распределения, она вот так выглядит. Мы сейчас в таком виде ровно ее и собираем.
Вот у тебя, по сути, переменная, по которой ты дифференцируешь, вот это минус лямда новая.
Соответственно, минус лямда нам нужно собрать вот здесь. Мы собрали вот как раз тета2, ставили.
Чтобы вот этот интеграл досчитать. Можно по частям посчитать, а можно просто выделить плотность и
досчитать вот так. Так, мне нужно попить. Сейчас, ой. Да, мы усоединяем по ненужной нам координате.
Сейчас досчитаем, друзья, секундочку. Не беспокойтесь.
Не, ну, просто дело не в том. Дело в том, что материала становится настолько много,
что его уже невозможно за один доп семинар отказать.
Ну, я не знаю. Так, давайте дособираем плотность. Друзья,
все получится. Давайте, главное верить. Так, еще экспоненты у нас. Минус тета2 у плюс 1 на u.
Такая штучка, так ведь у нас? На v. v это то, почему мы ее дифференцируем. dv здесь будет.
Ой, интегрируем, прошу прощения. dv. Что у нас здесь остается? У нас остается v поделить на u в квадрате.
Так ведь? Вот оно. И тета2 в квадрате. Вот, я более аккуратно просто это переписал. Что мы с этим
можем сделать? Кажется, что, смотрите, v на вот это, это мотождание даже будет. Там даже лучше будет.
Мы сейчас мотождание соберем. Одну тета2 мы вынесем с вами за скобочку. Ну, за интеграл.
У нас остается только тета2. Вот, и чтобы вот это стало плотностью,
нам нужно домножить вот на вот это. Понимаете, да?
Действительно.
Так, продолжаем наше правильство. Вы только следите за мной, я уже могу ошибаться начать.
Так, на что нам нужно домножить, чтобы все получилось? У плюс один. Так, у нас выносится
тета2 в квадрате, так ведь? Ой, это просто тета2. Одну ушку мы сами забираем. И у плюс один еще добавляем.
Потому что нам там домножить нужно, согласен. Интеграл. Тета2. Сейчас проверим, что все хорошо.
v это у нас x будет, а здесь у нас будет тета2 на что? На plus 1 на u и умножить на экспоненту от
минус того же самого v. Вот. Ну, заметим, что вот это на самом деле это у вас просто математическое
ожидание экспоненциального распределения с параметром... Какой параметр? Вот это. У плюс 1 на u
поделить на? И на тета2. Математическое ожидание такой штуки это что? Это 1 поделить на этот коэффициент.
Отлично. Мы с вами получили практически уже. То есть это у нас будет тета2, u, u плюс 1 и делим вот на это.
Делим на u плюс 1, умножаем на u, тета2 сокращается, ушки сокращаются. Ура! У нас
получилось 1 на u плюс 1 в квадрате. Что мы с вами показали таким образом? Мы показали,
что вот это отношение, которое мы с вами рассматривали, вот это вот, его распределение
не зависит от параметра. Посмотрите, их плотность фиксирована. В плотности никак параметр не
фигурирует. Это центральная статистика. Нет, нет, это просто плотность в точке u, вот такая.
А у у это у нас просто по сути, ну это плотность от x, просто переименуй в x и все, это плотность
у тебя просто. Уже плотность от x никак не зависит. Вот, и что мы имеем? Давайте,
я уберу вот эту всю замену переменных, потому что это более-менее очевидно. И давайте подведем
итог того, что мы имеем. Почти. Друзья, давайте не теряйте бдительность. Почти закончили. Так,
мы получили с вами, что вот θ2уθ1х имеет распределение, у которого плотность в точке t
1 поделить на 1 по квадрате. Здорово? Все рады? То есть смотрите, мы получили с вами статистику,
вот это вот g от x, ну в нашем случае у вас данные не просто x, а x и y. g от наших данных и от
параметров, ну в нашем случае от параметра θ1 и θ2. Смотрите, вот эта статистика, она зависит от
наших параметров, она зависит от наших данных, но распределение этой статистики не зависит от
наших исходных данных. Супер, значит мы готовы с вами применить метод центральной статистики,
а метод центральной статистики заключается в том, что вот смотрите, у вас есть центральная
статистика и вы знаете, что оно имеет какое-то распределение. Тогда вы можете взять у него две
квантили, вот возьмем p1 и p2, это две квантили, давайте так, zp1 и zp2. Это две такие квантили,
что p2-p1 это уровень нашего доверия. То есть мы теперь, грубо говоря, вкладываем наши статистики,
для которой мы знаем, как выглядит распределение, не зависящее от параметра, выбираем интервал,
вот для этой статистики, выбираем интервал, в котором оно будет заключено. Давайте конкретный
пример рассмотрим. Сначала найдем функцию, так, ты наверное зря написал, найдем функцию распределения
сначала, чтобы найти квантильную функцию. Функция распределения, это просто интегралов плотности,
ну там от минус бесконечности до x нашей плотности, dt. Вот, а этот интегралчик к чему равен?
Ну это просто полином. Ну это просто полином, да, мы на самом деле можем 1 и т сюда записать. Это
что у нас получится? 1 поделить на 1 плюс t, с минусом, по-моему, только, да? И постановка
от минус бесконечности до x. Только там не минус бесконечность будет, а на самом деле нижняя
граница, там будет единичка. Нолик, да, вы правы, нолик. Прошу прощения. У нас там все индикаторы больше
нуля, да, все справедливо. Вот, ну и того ваша функция распределения имеет вид. В нолике это
будет единичка, вот, минус единичка поделить на 1 плюс x. Справедливо? Это функция распределения,
но почему? Сейчас мы еще запишем с вами индикатор. Ну потому что она выглядит на самом деле вот таким
образом. Х больше либо равно ноль. Ну этот индикатор у нас просто там бы повылазил везде,
если мы аккуратно писали. Вот, то есть на самом деле здесь у нас получится в нуле
ноль. Вот, и потом она стремится к единичке, ну то есть все аксиомы функции распределения
выполнены этой функцией распределения. У этой штуки есть квантильная функция. Как искать
квантильную функцию мы с вами уже обсуждали, так ведь? Нужно найти просто обратную функцию к этой
функции. Это мы легко сделаем. Тут как раз такой еще вид красивый у этой функции. То есть y равняется
единичка минус 1 на 1 плюс x. Тогда обратная это y минус 1. 1 поделить на y минус 1. Это 1 плюс
x мы сюда перенесли, минус 1. Я не ошибся? 1 минус y, согласен. Теперь точно правильно?
Так, y минус 1, меняем знак.
А в ладжу у нас никакой не получается.
А квантильная функция 1 минус 1 на y минус 1. Ну да, все супер.
Да, я согласен. Хорошо, давайте теперь тогда вот какой уровень доверия вы хотите найти. Давайте
просто фиксируем с вами какой-нибудь уровень доверия. 0,95. 0,9. Просто проще будет. Так, какая
точка будет соответствовать? Давайте от нуля до 0,9 квантили возьмем. Давайте нулевой квантили это
будет просто 0, так ведь? z 0. Это просто 0, а z 0,9 это что у нас будет? 1 на 0,1 минус 1,9.
9. Красота. Что из этого следует? Из этого следует, что вот смотрите, вероятность того, что theta2
поделить на theta1, y на x, вот это наша центральная статистика, зажата от нуля до 9, в точности равняется 0,9.
Так ведь? А теперь смотрите, в центральной статистике мы требовали, чтобы по theta1 и theta2 эта штука
была непрерывная и монотонная. Навешиваем обратное преобразование, что получаем?
В силу того, что она монотонно-непрерывная, мы можем навесить обратное и у вас будет то же
самое. То есть у вас будет 0 меньше ли равняется theta2 на theta1, меньше ли
равняется, чем 9ху. И вероятность у вас сохранится. Понятно?
Нас же просили найти доверительный интервал для этой штуки. Вот две статистики, которые
зависят от наших данных. Ну ладно, первая не зависит, а вторая вот зависит вот таким образом.
Мы построили доверительный интервал для статистики, ну вот для вот этих двух, для отношения цены.
Супер, это была сложная задача. Теперь давайте в общем случае как это делать.
Ну мы с вами рассматривали изначально статистику, просто theta2y, 1, ну вот одно любое наблюдение.
На theta1, x1, любое наблюдение. Ребят, давайте теперь в общий случай рассмотрим, что делать в общем случае.
Эта задачка красивая была? Мне кажется очень-очень замечательная.
Так в смысле? Ну вероятность на линейках она так и работает, что чем дальше, тем больше… ладно.
Вот, и как мы сейчас с вами общий метод центральной статистики проговорим, и как вот эти
центральные статистики строить. Сейчас мы с вами центральную статистику по сути просто придумали.
Так ведь? Это сложно и неудобно. Я утверждаю, что есть общая центральная статистика,
которая подходит для всех непрерывных распределений. Теорема.
Вот, пусть у вас, в вашем параметрическом семействе все распределения имеют непрерывную
функцию распределения. Знаете, какую статистику тогда можно рассмотреть? Вот, утверждение.
Имеет гамма распределения с параметрами N1. Понятно, да? Быстенько докажем этот факт.
То есть, смотрите, в таком случае вот это и будет центральной статистикой. Потому что,
смотрите, оно зависит от ваших наблюдений, зависит и от ваших параметров, потому что вы
применяете как бы неизвестную функцию распределения. Но распределение ее не зависит от параметров.
Справедливо? Окей, давайте докажем, что это действительно так. Первый шаг. Понимаете ли вы,
что fθ от x1 имеет равномерное распределение на отрезке 0,1? Покажем быстренько. Давайте,
как мы это покажем? Функция распределения. Да, это просто вероятность по определению
функции распределения θ от x меньше либо равняется t. Так как мы положили, что у нас все монотонные,
то мы можем взять обратную. То это x меньше либо равняется чем f-1 в точке t. Вот, а это на самом
деле опять просто функция распределения x в точке f-1 от m. Ну и все, вы применяете функцию к обратной,
но вы получаете просто t. То есть первое утверждение, если вы просто к случайной величине,
случайную величину вашу засунете в функцию распределения, ее же, вы получите новую
случайную величину, которая распределена у 0,1. На этом, кстати, основан метод обратного
преобразования. Если вы умеете генерировать вот такие штуки, то если вы на них будете вешать
обратные функции распределения, вы будете получать новые случайные величины с той
функцией распределения, с которой вам надо. Показывается точно так же, только для f-1.
Ну да, то есть на самом деле, если вы умеете генерировать 0,1, а компьютер только это и умеет
делать. Он умеет 0,1 делать. Из 0,1 можно составить любое число на отрезке от 0 до 1, применить метод
обратного преобразования и получить. Да, ну там могут быть проблемы, если у вас функция распределения
существует, или если она какая-нибудь очень редко растущая, там другие методы тогда используются.
Но вот есть вот такой метод обратного преобразования. Итак, первый факт, который мы с вами установили,
что вот это, это у 0,1. Если мы на нашу вот, смотри, вот эта штучка имеет функцию распределения f
от t. Если мы ее же засунем в функцию распределения, то есть мы возьмем распределение от любой
функции распределения, мы получим распределение равномерное от распределения. Да. Как бы в чем суть, у вас
какая бы ни была функция распределения, вот такая, да, вы как бы вот, ваша чиселка сгенерированная,
вы ее подставляете в функцию распределения, и она у вас отображается вот сюда. Понятно? То есть смотрите,
вот ваша функция распределения истинная, вы приняли вот это значение, загнали вот сюда,
и оно вот отображается в 0,1, и утверждается, что равномерно. Почему равномерно? Показали только
что. Все, супер. Итак, смотрите, мы уже на самом деле практически получили с вами центральную
статистику, потому что уже f тета от x1 уже не зависит как бы от параметра, так ведь? Но мы
хотим как бы учесть все наблюдения, которые у нас есть, поэтому нам нужно как-то вот эти равномерные
случайные величины усреднить, ну как-то их сложить или что-то еще сделать. Складывать просто равномерные
случайные величины, них хорошо, они плохо складываются. Зато если мы сделаем из них
экспоненциальные распределения, сумма экспоненциальных распределений это гамма распределения вот с такими
параметрами. Под задачу номер два. Покажем, что у логориджа есть вот такой штуки,
это экспоненциальное распределение с параметром один. Доказательства. Итак, пусть к себе имеет
распределение u01, подействуем на него функцией минус логорифм t. Покажем, какую функцию распределения
имеет данная функция распределения. Итак, функция распределения, так это f at,
функция распределения f at в точке x. Это что такое? По определению, это вероятность того, что f at меньше
либо равна x. Так ведь? Монотонное все нормально, обратное, накидываем обратное преобразование,
t меньше либо равняется чем phi в минус 1 от x. Теперь заметим, что это по сути функция распределения
для равномерного распределения вот в этой точке, ну а там в этой точке она равняется просто phi в
минус 1 от x. Потому что у вас функция распределения для равномерной случайной величины, это просто x.
Супер, а обратное преобразование, ну обратная функция к вот этой, это что? Да, это экспонента
в степени минус t. Ну есть в степени минус t. То есть по сути мы с вами получили, что вот это у нас
имеет экспоненциальное распределение. Ну по сути мы применили обратное преобразование q01,
как я уже говорил. Если ваш компьютер умеет генерировать чиселку от 0 до единицы, применяя
обратную функцию, мы получим функцию с тем распределением, которое нам нужно. Вот,
таким образом мы получили с вами, да, мы показали, что функция распределения phi от t имеет просто
функцию распределения, равную функцию распределения экспоненциального распределения. Таким образом,
мы показали, что phi от t имеет экспоненциальное распределение. Супер? Всё, смотрите, дальше здесь
уже свойство экспоненциального распределения. Если мы n слагаемых экспоненциальных сложим,
то у нас с одинаковым параметром, то у нас как раз таки получится гамма распределения с параметром
n1. Всё, мы построили с вами центральную статистику в общем виде. Если у вас функция распределения
непрерывна, то центральную статистику можно построить вот так. Понятно, почему это центральная
статистика? Она зависит от параметра. Тогда вот такую штуку можно воспользоваться, да. Ну,
её нужно как бы записать в явном виде, то есть записать вот суммирование логарифма вот этих
штук. Всё. И давайте вот ещё раз подрекапим, как у нас выглядит метод решения таких задач на
доверительные интервалы. Все задачи на доверительные интервалы вот через центральные статистики
решаются следующим образом. Вы либо придумываете, если какой-то нестандартный случай, центральную
статистику g от х тета. Придумывайте, если что-то нестандартное. Либо пользуйтесь вот такой
стандартной штукой. Показывайте, что это центральная статистика, то есть то, что она непрерывна и
монотонна по тета и её распределение не зависит от параметра. В таком случае, смотрите, у вас g от
х тета с известным распределением зажимается между какими-то квантилиями, так ведь?
Квантили вот у этого распределения. То есть вот вы фиксируете какую-то вероятность. Больше и гамма.
Ну и всё, отлично. Да, гамма обычная просто параметра. Вот и всё. А так как вы потребовали,
что функция непрерывна и монотонна, вы можете взять в некотором смысле обратную и получить,
что у вас тета меньше либо равен, чем g в минус 1, от z по 2 меньше либо равняется,
чем g в минус 1 от z по 1. Ну и давайте скажем, что вот это верно, если функция у вас возрастает.
Потому что если у вас g будет убывать, у вас квантилими сами меняются. Так, вот это верно,
если g х тета возрастающее вот это. Всё, в принципе, мы научились с вами строить
доверительные интервалы методом центральной статистики. Какое?
Почему она покрывает непрерывность и монотонность? Смотрите, у вас f тета,
мы положили, что она непрерывная. То, что она монотонная, мы тоже знаем. Сумма монотонных,
ну и логарифм монотонный от монотонный тоже монотонный. То есть вы получили,
что вот эта штука будет монотонная и непрерывная, как композиция непрерывных.
супер, значит, это центральная статистика, потому что она, видите, ее распределение
не зависит от параметра. Ну и она не прерывная и монотонная, соответственно,
это центральная статистика. Вы берете вместо zP1 и zP2 квантилия вот этого распределения,
потому что у нас центральная статистика имеет именно вот такое распределение.
вот и потом в силу того, что она не прерывная и монотонная,
накидываете обратное и получаете доверительный интервал для параметра тета.
Еще одна задача?
Последняя
Ну, в плане, промолв сучиться, так что, окей, шаг, шаг, шаг
И в какой-то момент, я не получаю то, что мою сумму логипмов умеет в домораспределении
Это невозможно
Мы только что доказали, что так всегда, если у тебя Ft, это непрерывно
Сейчас, ну, просто задача какая-то
Ну, может, я что-то не понял, но она, получается, какая-то очень очевидная, очень простая
Мы говорим в нашем определении непрерывное
Да
А если бы они непрерывное дадут, там уже придется что-то другое придумывать
А, то есть можно проверить на непрерывность?
Ну да
О, определение непрерывное
То есть, еще раз, у вас просят построить доверительный интервал
Первый метод построения, это через Чебышева и через всякие остальные неравенства
Но это грубые доверительные интервалы, мы их не используем
Скорее всего, будет метод центральной статистики
Вот, метод центральной статистики мы с вами обсудили
Либо вам нужно какую-то придумать самим статистику, которая будет центральной
Либо воспользоваться общим случаем, если у вас Ft непрерывно
То есть мы придумываем, если у нас нейния непрерывная
Да
Супер?
Супер
А, кстати, еще по доверительным интервалам нужно одну вещь добавить, друзья
Ну задачка еще одна и все
Давайте
Вот за 47 минут мы точно успеем
Обещаю
Чего?
Ну у меня время очень медленно идет
Последняя задача на полной статистике
Оптимальные оценки
Мы просто с вами там алгоритм обсудим без жесткой теории
Просто как-то задачу решать
И решим задачку
Так, все собрались, приготовились
Еще про доверительные интервалы одну штуку надо сказать
Как строить асинтетически доверительные интервалы?
На самом деле доверительные интервалы можно строить не всегда
Иногда гораздо проще построить асинтетически доверительный интервал
Давайте быстро поем
Это очень просто
Вот пусть у вас есть асимпатический нормальная оценка
Чтобы построить доверительный интервал
Нужны асимпатически нормальные оценки
То, что асимпатически нормальная
Следует, что она представима вот так
справедливо вот то есть возможно там что-то с распределением получилось не
очень удачно но зато у вас получилось симпатически нормальная оценка знаете
что из этого следует из этого следует что вот мы как и в центральной
статистике хотим получить предельное распределение которое будет не зависеть
от параметра нормально но смотрите сейчас она зависит от параметра видите как
и в центральной статистике мы хотим получить чтобы вот это вот распределение
которое справа стоит не зависит от параметра что мы для этого хотели бы
сделать наверное как-то отнормировать чтобы здесь стало n 0 1
так ведь вот если мы отнормируем получим такую штуку то все дальше
рассуждение абсолютно как в центральной статистике да как мы это
сделаем предлагается поделить на это 0 на сигма тета но это не совсем правда
сейчас не будем даваться подробности почему это не совсем правда и тут сейчас
уже становится сложнее гораздо выразить параметр тета ну тут это 0 это 0 это 0
это 0 но на самом деле по лиме сутского мы можем это домножить на вот такую
штуку то есть вместо вот истинного значения который мы не знаем подставить
нашу оценку которую мы знаем смотрите по лиме сутского вот эта штука сходится
к единице почему потому что у вас оценка вот это вот асимпатически нормально для
вот этого параметра мы навешиваем на нее непрерывную функцию так это все еще
сходится соответственно отношение будет сходиться к единичке все и по
лиме сутского вот это сходится константе ну справа типа нам нужно на
единицу все и получим вот вот это у вас сократится и из этого мы получим с вами
что есть вот такая сходимость
сходится к нормальному распределению с параметрами 0 1 справедливо отлично все мы
решили мы построили асимпатически доверительный интервал в чем его
отличие от точного точный он сразу действует а вот тут как бы у вас вот это
распределение только в пределе достигается то есть вам нужно много
наблюдений чтобы построить доверительный интервал а все очень просто просто ты
считаешь что с какого-то момента у тебя это уже почти нормальное распределение
ты берешь квантилию вот этого распределения и строишь также как и с
центральной статистикой то есть как будет выглядеть доверительный интервал
легко понять вы берете квантилию нормального распределения
они через У обычно обозначаются. Обычно берутся центральные, вот такие.
Это в пределе выполняется, так ведь? И все, и смотрите, вы просто домножаете на
вот эту дисперсию, по сути, делите на корень из n, в общем, оставляйте здесь только тета 0.
Просто домножайте на обратные функции с двух сторон неравенства, и все, и у вас тета 0 здесь
остается, а вот здесь остаются хватчики. Ну то есть, вот это переписывается, давайте аккуратно
делаем. Думаешь? Ну тут будет тета так, плюс поделить на корень из n, сигма так, на квантиль.
1 минус гамма пополам. Меньше либо равняется, чем тета 0, и меньше либо равняется, чем то же
самое, только с другой квантилью. Мы построили асимпатический доверительный интервал. Ход
рассуждений примерно такой же, как в центральной статистике, вы получаете что-то, чье распределение
не зависит от параметра. Так ведь? Не зависит от параметра. Здесь вот мы воспользовались таким
трюком и получили вот такую исходимость какому-то распределению без параметра. Но один минус это
асимпатический доверительный интервал, то есть вам нужно достаточно много наблюдений, чтобы начал
работать. Ну типа да. Ну вот допустим обратную функцию для функции распределения нормального
вы не найдете, потому что у вас нет просто функции распределения нормальной, поэтому все квантили для
нормального распределения стандартного это вообще типа табличные данные. Они вычислены там на
компьютере уже давно и типа при решении их можно использовать какие-то константные. Ну у вас точно
вот в праке по моцетатам что-то такое будет? Это прям сто процентов. Все,
продоверительный интервал закончили? Еще одна задачка. Не, ну пока все понятно более-менее.
Отлично, если все понятно, тогда мы сейчас быстренько последнюю задачку разберем и разойдемся. Я буду
скутять по вам, друзья. У нас среду семинар, да? В среду. А потом воскресенье. Так, вот тут у нас
сразу будет несколько определений. На самом деле вот последняя тема, она самая глубокая с точки
зрения теории, а с точки зрения практики нет. Поэтому мы сейчас просто несколько определений с вами
обозначим и покажем как решать задачи, которые скорее всего будут на достаточной оценке. Так,
у нас уже недоверительные интервалы. Недоверительные интервалы. Уровень недоверия.
Так, товарищи, достаточные оценки. Что нам нужно понимать? Первое, есть понятие
о достаточной статистике. Что такое достаточная статистика? Это такая статистика,
что при S3 не зависит от этой. Так, давайте как на это посмотрим с вами. У нас здесь какое-то
условное распределение появляется. Это условное распределение должно зависеть от
тета. Пример. Допустим, вы играете с монеткой. Нужно ли вам хранить все нолики и единички в
каком месте они выпали, чтобы параметр тета оценить, чтобы все сказать про параметр тета.
На самом деле достаточно только среднее знать. Если вы будете знать только среднее, то вы уже
понимаете какой параметр вы его оценили, грубо говоря, во всех возможных хороших смыслах. И
как бы знание, на каких местах у вас выпали единички, на каких нольках, вам ничего не дает. И вот как
раз-таки достаточная статистика S от X, это вот такая статистика, что зная ее, зная, что сумма
вернулистских случайных величин у вас там допустим, вернее средняя, там 0.3, вы уже понимаете,
что ваше распределение не будет зависеть от тета. Вы уже оценили параметры. Да-да-да-да, это как
раз-таки очень сильно связано с умом. Мы на семинаре более внимательно на это посмотрим. Сейчас нам
нужно только задачки научиться решать. Вот, это первое определение. Сразу теорема. Критерий
факторизации. Вот такую в некотором смысле достаточную статистику сложно проверять по
определению. То есть вам нужно найти, чтобы вот условное распределение не зависело и достаточно
сложно проверять. Есть простой на самом деле критерий проверки того, что статистика является
достаточной. Таким критериями является критерий факторизации. Он утверждает, что статистика S от X
достаточно тогда и только тогда, когда ваша плотность представима в виде вот такого
произведения. Давайте сейчас немножко на него посмотрим, помедитируем. То есть как бы смотрите,
понятно, что просто функция какая-то. То есть смотрите, вы можете какую-то часть плотности
как бы откинуть в сторону, да? Она как бы уже от параметра, видите, не зависит. Как бы какая идея.
Зная только, чему S от X равно, вы понимаете, чему у вас равен theta как бы в некотором смысле.
Ну типа смотри, плотность у тебя, давай просто плотность рассмотрим как сумму каких-нибудь параметров,
да? Ты знаешь S от X? Это какие-нибудь коэффициенты перед ними? Там K1, K2, K3. А что-то еще от выборки?
Что-то лишнее? Какие-нибудь еще знания про выборку? Они как бы отдельно, так сейчас, как бы отдельно,
они как бы отдельно рассматриваются. Тут какие-то H от X, а вот тут какие-то статистики. Это 1 от X,
здесь какая-то статистика S2 от X, здесь вот статистика S3 от X будет. Как бы вот эти статистики
влияют на параметры, да? В некотором смысле, они определяют как выглядит функция. А вот эти
статистики? Нет, вот то есть 2-ой множитель как бы не влияет на то, как у тебя параметры будут
видеть твою плотность. Вот. Сейчас сразу за то пример сможем привести. То есть там на самом деле
можно это в лоб проверять, выполняется критерий факторизации или нет. Но на самом деле у нас есть
целое семейство хороших распределений, для которых это сразу выполнено. Я думаю вы сможете сказать,
какое? Экспоненциальное. Но мы как раз его ровно в таком виде примерно и записывали. Мы записывали,
что у нас есть какая-то функция от X независящая, о, вернее от это независящая, на экспоненту от
параметров и каких-то других функций. Вот. На самом деле для экспоненциальных распределений существуют
хорошие достаточные статистики. Понятно, что в качестве от X можно положить равным единичку,
и тогда, грубо говоря, всю плотность учитывается как достаточную статистику. То есть как бы вся
плотность у вас все говорит о распределении, а вы хотите меньше информации взять, но получить
столько же, сколько и от всей большой. Ключевая идея. Вот. И теперь давайте какой-нибудь пример с
вами рассмотрим. Ну вот. Примерно понятно, почему для экспоненциальной модели у вас выполнен критерий
факторизации. Давайте запишем его. То есть у вас в некотором смысле здесь вот единичка,
да, входит? Ну это как бы единичка. А дальше у вас вот какие-то вот статистики вот эти идут. И на самом
деле вот эти статистики будут достаточными. Потому что вот по критерию факторизации, вот у вас
h от X, который от параметра никак не зависит. Мы как бы его игнорируем. И у вас вот параметр
входит через вот эти статистики. То есть как бы от них именно зависит. От вот этих статистик не
зависит. Вот. И утверждается тогда, что вот эти t и t, они как раз таки будут достаточными статистиками.
Вот в частности из этого будет следовать, что для Bernoulli у вас средняя, это будет
достаточная статистика, как мы с вами и проходили. Ну как мы с вами это только что обсуждали. Что
типа не надо знать на каких местах у вас 0 и 1. Достаточно знать, что средняя элементическая равно
чему-то там. Ну как раз таки, как у вас для Bernoulli будет выглядеть? Как плотность будет выглядеть
Bernoulli? Bernoulli не биномиальная. В степени 1-х. Давайте к экспоненциальной форме приведем.
Что у нас в экспоненциальной форме получится? Икс алгорифм п. Плюс 1-х на алгорифм. Вот. Теперь
на самом деле все, что с иксом можно собрать в одном месте. Вот. И на самом деле смотрите,
что здесь у вас в итоге получится? У вас здесь в итоге получится экспоненциальное распределение.
Икс. Так, давайте сначала то, что без икса. Алгорифм 1-п. Прошу прощения, я что-то вертикально пишу.
Так, вот здесь вот значок равенства пишу, обожу его в кружочек, продолжаю равенство вот здесь.
Это получается имеет вид экспоненциальная экспонента. То, что без икса, сначала соберем алгорифм 1-п.
Плюс икс. То, что с иксом соберем, там будет алгорифм п. Вот он у нас. Минус алгорифм 1-п. Вот. Ну и как
утверждалось, t от икс у вас будет являться достаточной статистикой. То есть для одного
наблюдения это икс. Ну, понятно, для многих наблюдений это просто икс средний. Ну, то есть
t штрих от икс будет у вас достаточной статистикой. То есть как более глубоко на семинаре познакомитесь,
но вот сейчас у нас будет несколько цепочек определений друг за другом. Первое определение
достаточность. Если вам дают экспоненциальное семейство, сразу понятно, какие статистики будут
достаточными. Ну, вы просто записываете в таком виде, применяете критерии факторизации и сразу
увидите, какие у вас статистики достаточно. Справедливо? Справедливо. А если не экспоненциальное
семейство, то это нужно уже расписывать плотность и пытаться получить ее вот в таком виде. Для
экспоненциального сразу выполнено, для других нужно посидеть, помучиться. Есть пример, когда для
не экспоненциального распределения это вот работает. Не для экспоненциального семейства.
Есть хороший, понятный, простой критерий вот этой штуки. Супер? Мы близки к победе.
Следующее определение. Нам всего нужно, друзья, три определения сделать и мы решаем задачу за три
минуты. Ну, может быть, за пять. Я буду, я ускорюсь. К часу закончим. К часу обещаю.
Следующее понятие, полные статистики. Так, смотрите, тут такое немножко контроиттуитивное
определение. Если вы его сейчас не поймете, ничего страшного. А знаете, на семинаре главное
понять, как задачи решать, правильно? Давайте просто дадим определение этой штуке. Смотрите,
что если у вас статистика S от X называется полной, если для любой функции Борелевской из того,
что от ожидания равно нулю, следует, что для любого тета F от S от X равняется нулю почти
и всюду. Ну, почти, наверное. Контроиттуитивное определение пока что не совсем понятное,
но в некотором смысле, смотрите, достаточная статистика это значит такая статистика,
которая в себе очень много информации несет. А полная статистика, наоборот, несет в себе
не всю информацию. Если у вас, грубо говоря, ноль оценивается только единственным образом.
Только единственным образом. Ну, смотрите, у вас ноль, вот ожидание F от S от X равно нулю для
любой F Борелевской. Из этого должно следовать, что у вас сама функция везде ноль. То есть,
вы как бы не можете ноль оценить двумя разными способами. У вас какая-то статистика такая,
то есть там с квадратом могут быть проблемы, когда у вас знаки, типа, чередуются, и вы
можете бы читать и прибавлять. Давайте не будем сильно зацикливаться на этом, просто вот определение.
Проверять можно по определению, то есть, смотрите, если вас просят проверить, является ли статистика
полной, можно воспользоваться критерием, который мы сейчас приведем, признаком, вернее. А можно
просто привести функцию или доказать, что для любой F вот это будет полной статистикой. На семинарах,
если что, вы можете обсудить более подробно. Сейчас, к сожалению, времени у нас нет,
если мы хотим задачку решить. Обсудим позже. Хорошо? Значит, вот есть полные статистики. Хороший,
понятный признак. Признак полноты. Он тоже только в экспоненциальном семействе выполняется.
Вот помните, у нас в определении экспоненциального семейства появлялись функции а0, а1 от тета и
так далее. А0 от тета, а1 от тета и так далее. Так вот, если вот это множество содержит хотя бы
одну внутреннюю точку, то тогда вот эти статистики, которые у нас стоят х стояли,
они будут полными. Но у вас вот это множество телесно. То есть, у вас они все линейно независимы
в окрестности какой-нибудь, да? И они содержат одну маленькую точку внутри. Для какого-то тета. То
есть, у вас, грубо говоря, здесь есть параллелограмм какой-то. Да, это инмерное пространство,
и там это используется при доказательстве того, что вот такая статистика будет полной. Да,
вы натягиваете линейную оболочку и показываете, что она вот в этом... Сколько у вас тут функций? Ка
плюс одна получается? Что она не вырождена, то есть, что у нее, грубо говоря, есть объем. Если есть хотя
бы одна точка, ну это для всех наших классических, красивых распределений выполняется экспоненциальных.
То есть, на самом деле, если вот это выполнено, тогда вот эти вот ииты, которые, помните,
у нас с вами шли с коэффициентами аитах, будут полной статистикой. Что левое? А, ну вы просто
смотрите на то, какие у вас функции, если они линейно независимы. Ну смотрите, чтобы у вас отсутствовал
объем, у вас должна быть линейная зависимость, так ведь? Показывайте, что функции линейной независимой
в окрестностях какой-нибудь точки у вас есть объем. Да, есть хотя бы одна точка внутренняя,
все, значит, у вас уже не нулевой объем. Соответственно, вот этот набор статистик ТХ будет как раз
таки тот, который нам нужен. Отлично, это полные статистики. То есть, достаточные — это те,
которые хранят достаточные информации. Полные — это те, которые, грубо говоря, одним образом
представляют ноль через мотож. И теперь мы с вами готовы. Мы с вами готовы к основной теореме
сегодняшнего вечера. Ну да. Так, а где мой листочек? Вот, теорема. Называется теорема
«Лемо на шафе». Это одна из самых таких интересных и нетривиальных теорем-курсов.
Теорема «Лемо на…» Надо на лекции ходить. Лекторов. Так, ладно. Это шутка плохая.
Что? Да можно не вырезать ничего. Я шучу же. Интересное определение.
Непонятно ничего, да? Так, друзья, давайте мы вот что с вами заметим. Вот смотрите, пусть у нас
S от X. Вот мы с вами два определения ввели — полнота и достаточность. Так вот, пусть S от X будет
сразу полный и достаточный. Одновременно оба условия выполняются. Так, только тут какое-то
не очень красивое определение. Одну секундочку. Я сейчас возьму хорошую формулировку этой теоремы,
потому что она очень важная. Так, теорема «Лемо на шафе». Вот. Смотрите, потом у вас есть несмещённая
оценка. Несмещённая оценка. То есть, математическое ожидание вот этой оценки — это вот. Вот из этих
вот условий следует вот такая импликация. Новая оценка, которая получена как условный
математическое ожидание, будет оптимальной. Оптимальная — это наилучшая как раз таки в
средне квадратичном подходе. То есть, в чём суть? У вас есть какая-то полная достаточная статистика,
и есть какая-то оценка несмещённая. Тогда вот эту несмещённую оценку можно улучшить полной
достаточной статистики до самой наилучшей в классе оценки. Там более-менее она всегда существует.
В широком классе случаев существует? Да. Тут как раз таки вот нужно очень много всего потребовать.
Мы сейчас с вами задачку решим и пойдём. То есть, смотрите ещё раз, SLX и полная, и достаточная
одновременно. У вас есть несмещённая оценка какого-то функции от параметров. Вот это, по сути,
одно и то же. В таком случае вы можете улучшить вашу несмещённую оценку, полной достаточной,
и получить оптимальную оценку, то есть самую лучшую в этом классе, для вот той функции,
для которой у вас была и оценка несмещённая. Примерно так, но давайте мы с вами теперь подумаем,
как решать задачу. Теперь, смотрите, вас просят найти. Всё, вот в таких задачах вас сразу просят
найти. Найдите оптимальную оценку для какого-то параметра. Давайте я запишу условия. Последние
задачи. Десять минут, друзья. Смотрите, условия задачи. К сожалению, у нас просто времени не
хватает нормальную систему обсудить. Там, на самом деле, очень много можно всего сказать,
но время ограничено. У нас 11-я аудитория. Пусть вот это будет экспоненциально с параметром
тета. Не, ну не хочется задерживаться просто. Вот, и вас просят найти оптимальную оценку
параметра тету в квадрате. Давайте посмотрим на теорему Лемо на шафе. Она, по сути,
ключевая в данном разделе, и мы будем пользоваться для решения данной задачи.
Что нам нужно сделать? Нам нужно найти полную достаточную статистику. А что дальше нам нужно
сделать? Вот нашли мы с вами полную достаточную статистику. Действительно, нам нужно найти
несмещенную оценку. Но более того, мы с вами кое-что знаем. Мы знаем, что вот это это условное
математическое ожидание. Условное математическое ожидание, помните, это же какая-то бареллевская
функция от условия. Так ведь? Было такое. То есть, на самом деле, нам нужно просто решить уравнение
несмещенности. Всё, тогда будет выполнена теорема Лемо на шафе. Мы находим какую-то полную достаточную
статистику S от X. Решаем уравнение несмещенности. И что мы с вами получаем? Мы как раз попадаем в
условия теоремы Лемо на шафе. Вот полная достаточная, вот у нас несмещенная. Тогда вот то,
что мы получим, будет хорошей оценочкой. Тогда вот эта функция phi от S от X и будет оптимальной
оценкой. Окей? Это уравнение нефункциональное? Да. Ещё что? Уравнение нефункциональное? Ну да,
то есть, найти такую функцию, это сложно иногда бывает. Вот сейчас мы рассмотрим не тривиальный
случай. Вот как раз таки вот это условное отожидание, это вот наша новая оценка phi от S от X. Но вот
эта оценка, она должна быть несмещенной, а S от X должна быть полной достаточной. Ребят,
давайте по формулировке и плану решения. Есть вопросы? Это сейчас самое важное.
Мы находим полную достаточную статистику, решаем уравнение несмещенности, находим такую
функцию, чтобы вот эта штучка была несмещенная. И всё, и мы по сути попадаем с вами в условия
теоремы Лемо на шафе, получаем оптимальную оценку. Получаем, что phi от S от X, оптимальная оценка.
Давайте с вами обсудим план решения. Вот смотрите, если мы работаем с неэкспоненциальным семейством,
тогда вам нужно полноту отдельно проверять по определению. Ну вот здесь у нас будет, да,
действительно, мы оцениваем theta квадрат, должно быть вот так. Наша конкретная задача, вот мы для
theta квадрата ищем, поэтому ищем несмещенную оценку вот такую. Итак, давайте обсудим с вами
план решения. Это в принципе все оптимальные оценки ищутся примерно таким образом. Сначала
вам нужно найти достаточную статистику. Достаточная статистика находится вVIS на
А если вы в экспедиционном смесь, простой признак был, что если у вас там телесин в париллограмм, то есть если у вас есть хотя бы одна внутренняя точка, то статистика будет еще и полной.
Вот эти вот t и x, которые у нас были в определении. Сейчас мы делаем это с вами.
А потом просто нужно найти такую функцию, чтобы вот в отожидании от нее было это в квадрате.
Да, мы работаем с экспедиционным смесь, поэтому по сути, ну вот просто с вами разочек покажем аккуратно, что у нас почти сразу есть полная статистика и достаточно s от x.
Так, давайте решать данную задачу.
То есть как бы там очень много краевых случаев, которые мы, наверное, еще там часа два бы рассматривали, если бы рассматривали.
Вот, но в случае экспедиционного смейста все да более просто.
Шаг первый. Выпишем плотность экспедиционного распределения в каноническом виде.
Что это у нас такое?
Экспонента.
Благорифм λ.
Я уже какие-то странные скобочки пишу.
Так, у нас там что было? Минус λх.
Минус λх.
Согласны?
Да.
Супер. У нас выполнен критерий факторизации.
Вот у нас h от x это единичка.
А вот это?
Это g от s от x и от параметра.
Ну да, это какая-то экспонента, в нее входит наша полная статистика и параметры.
Ну смотрите, полная статистика, вот она у нас.
Ну видно, да, что она входит.
Вот есть функция g, это экспонента.
Вот наша полная статистика.
Ну просто статистика какая-то, которая будет достаточной, прошу прощения.
Критерий факторизации проверяет достаточность.
И вот есть параметр λ, который входит.
Вот, мы только что с вами показали, что если мы просуммируем, то у нас x средняя будет полной статистикой.
Прошу прощения, достаточно, я уже заговариваюсь.
Она будет достаточной статистикой.
Справедливо?
Нет, ладно, справедливо.
Тут на самом деле сумма скорее будет.
Давайте просто аккуратно запишем v от x.
Давайте запишем функцию правдоподобия.
Экспонента.
Так.
Мы получается с...
Уже туплю.
N логарифм...
N логарифм...
N логарифм лямда...
Минус лямда на сумму x.
Да, вы абсолютно правы, здесь вот статистика наша будет достаточной.
Достаточно.
Ну да, почему вот в данном случае средняя будет плохим?
Средняя она будет плохим.
На самом деле, наверное, можно показать, что они будут даже эквивалентны.
Давайте мы покажем с вами, что вот сумма от x.
Потому что в конечном итоге она у нас через функцию войдет, там все будет хорошо.
На самом деле и сумма от x.
Будет являться достаточной статистикой.
Давайте зафиксируем сумму от x.
Вот.
Ну смотрите, a0 на a1 от лямда это какое-то телесное множество, ну потому что они линейные независимо.
У них есть точка, соответственно, вот это еще и будет полной статистикой.
Какую?
Полнота следует из того, что у нас признак полноты, что вот если вот эти коэффициенты, которые от параметров зависят,
образуют, ну короче, линейные независимые в окрестности какой-то точки, то у вас все хорошо будет.
Они линейные независимые.
Так, полное достаточное.
Ну все, друзья.
Мы нашли вот, на первый пункт мы с вами ответили, просто из свойств экспоненциального семейства.
Теперь нам нужно ответить на второй пункт.
Найти какую-то функцию, чтобы вот в ожидании от этой функции было вот такое.
Тета в квадрате.
Что вы можете сказать про математическое ожидание экспоненциального распределения с параметром лямда?
Один на лямдо.
Действительно, то есть смотрите, если мы будем какие-то обычные функции применять, у нас там будет что-то один на лямдо в какой-то степени.
Ну то есть там если мы возьмем второй момент, третий момент и так далее, то ничего хорошего у нас не получится.
Скорее всего нам нужно будет смотреть какие-нибудь статистики вида...
Один поделить на что-нибудь такое, да?
На сумму...
На сумму...
Иксов, да, это уже что-то будет похожее там, ну если мы поделим на среднее, да, например...
То вот это было один на лямдо, теперь если мы это перевернем, то это уже будет лямдо, да, и что-то вот такое в квадрате предлагается посмотреть.
Справедливо?
Ну как бы это просто пока гипотезы выдвигаем.
Какого-то общего метода решения вот таких уравнений, его как бы не существует.
Ну у меня как бы это...
Так, смотрю не ошибаюсь я.
Все правильно?
Все правильно.
Так, и мы хотим что-то вот такое посчитать.
Может быть даже просто как сумму иксов, может быть не как среднее, но что-то вот такое, да, наверное.
Какие есть предложения, как будем действовать?
Нам нужно по сути с вами посчитать математическое ожидание, один поделить на сумму иксов.
Как вот это сделать?
Где иксы-то у вас определены экспоненциально?
Сумма экспоненциальна иксов экспоненциальная.
Сумма экспоненциальна иксов не экспоненциальная, а гамма распределения.
А вот это уже можно использовать.
Смотрите, у вас же здесь и от 1 до n просто складываются экспоненциальные случайные величины.
То есть вот это на самом деле имеет точно такое же распределение, как и 1 на гамма с параметрами...
Не n1, а nθ.
Вот.
Нет, все еще этим пользоваться нельзя.
У тебя все еще математическое ожидание функция случайной величины.
Это не f от математического ожидания x.
Все еще вот так писать нельзя.
Это верно только если f линейно.
В общем случае это неверно.
Только для линейных это верно.
Как вот это посчитать?
По пределению.
1 на x, вот такой интеграл будем считать с вами, да?
Да.
Вот, на плотность гамма распределения.
Плотность гамма распределения мы с вами сегодня уже записывали.
У нас типа x в степени k минус 1.
В степени k минус 1?
В степени k.
В степени k?
В степени k.
В степени k?
В степени k?
G от k.
Вот.
На E в степени.
Минус.
X на t.
Да.
И вот так это можно посчитать.
Понятно.
Ну давайте посмотрим, что у нас получится.
Здесь у вас получится
x в степени k минус 2, а так вот в числителе.
В числителе.
В числителе.
В числителе.
В числителе.
В числителе.
В числителе.
X в степени k минус 2, а так вот в числителе.
На самом деле на t в степени
k минус 2.
T t на в квадрате мы вынесем за интеграл.
E в степени минус x на t.
Гамма функции от k.
Все верно.
И это от 0 до бесконечности.
Чему этот интегралчик равен?
Тут опять сделаем замену.
У нас t.
Минус x на t.
Значит умножим на t.
То есть здесь будет 1 на t.
Тето.
Тето.
Тето.
Тето.
Тето.
Тето.
Тето.
Вот.
А интеграл от вот этой штуки это будет просто
гамма функция.
К.
К минус 1.
К минус 1.
К минус 1.
Да, от k минус 1.
Вы правы.
Вот.
Только здесь должно быть k минус 1 на самом деле.
Здесь степень больше.
То есть отсюда тет у нас тоже уходит.
Сейчас я посмотрю на определение.
Прошу прощения.
Так.
Е в степени х минус тета.
К минус 1.
Тета в степени к.
1 на х.
Гамма к.
Е в степени минус х на тета.
Так.
Сейчас все правильно.
Х.
К минус 2.
Отсюда.
Нет, пока рано.
Пока рано.
Вот здесь у нас пока что собралась гамма функция
от k минус 1.
Нет такого ощущения.
Короче, досчитаем.
У меня получил.
Да, аккуратно досчитаем сейчас.
В смысле неважно.
А потом семинарист у вас не разрешает
пользоваться вольфраммой.
Так, ну мы с вами почти посчитали.
Смотрите.
Получилось.
Гамма от k минус 1, да?
А, ну по сути получилось 1 на k тета.
Все.
Справедливо.
То есть смотрите, вот это у вас гамма функция
от k минус 1.
А наверху k минус 2 факториал.
Наоборот, k минус 1 факториал внизу у вас.
А наверху k минус 2 факториал.
То есть у вас осталось.
Доброе утро.
Действительно, k минус 1.
Согласен.
Вот так.
Ну на самом деле это очень похоже на
у нас в простом от ожидания
гамма наследление ткт
это мы с вами в начале считали.
В начале семинара.
То есть почти похоже.
На какую-то константу он отличается,
но мы потом функцию сможем такую подбить,
чтобы у нас вот эта штука получилась
это в квадрате ровно.
Так ведь?
Вот.
Почти все готово.
Есть одно небольшое замечание.
Так это мы посчитали мотош 1.22.
Да.
Верно.
Я тебе скажу почему.
Потому что эта параметризация
немножко другая.
Нам нужно последнюю строку стереть
и переписать ее заново.
В смысле не надо считать.
Мы только вот до этого момента
дойдем и все.
Я прошу прощения, что ввел вас
заблуждение.
Вот смотрите, в такой параметризации,
я просто уже немножко сам затупил.
Сумма экспоненциальных медичин
с параметром θ
это гамма распределения
n 1 на θ.
Вот в такой параметризации такое.
Я в самом начале семинара говорил,
что для гаммы распределения
существует несколько параметризаций.
Они эквивалентны.
Вы можете как бы параметр масштаба
делать 1 на θ.
То есть можете его менять
на противоположный.
И у вас все получится.
Вот.
На самом деле в нашем случае вот.
Да, да.
Я тут немножко сейчас.
Сейчас я просто перепутал параметризацию.
То есть сейчас на самом деле нужно
сделать все то же самое.
Даже параметризация останется
та же самая.
Просто у нас здесь соотношение будет немножко другое.
Там для разных параметризаций
просто чуть-чуть разные формулки вылазят.
Вот.
В этой же параметризации просто у нас
вот такая штучка будет.
Да, да, да.
Вот.
И что у нас получится?
1 на х.
Плотность какая у нас получится?
Х в степени k-1.
Гамма от k все еще у нас остается внизу.
θ у нас.
θ в степени k вот такое.
И e в степени
минус
θ dx.
То есть видите, это эквивалентные как бы
ну одно и то же.
Можно изначально полагать гамма распределение вот таким,
а можно как бы на 1 на θ менять.
Это одно и то же.
И вот в такой параметризации у вас сумма экспоненциальных
имеет вот такое распределение?
Вот.
А в другой параметризации немножко
там вот 1 на θ меняется просто на θ.
Понятно, да?
Извините, что вел у вас заблуждение.
Что мы теперь имеем с вами?
Да, теперь на самом деле все хорошо.
Выносим θ квадрат.
Выносим θ квадрат.
Да, все правильно.
И делаем замену xt на
на что?
На θ.
Просто у нас там когда дает xt.
А, нет.
Давайте досчитаем
и все, и пойдем по домам.
Давайте, друзья, напрягитесь.
Давайте, друзья, напрягитесь.
Давайте досчитаем.
Значит у нас остается и в степени k-2.
xt на θ.
Здесь у нас что будет?
θ в степени k-2 тоже оставим?
k-2 тоже оставим?
А e в степени −xθ
на γ функцию от k.
И вынеси с вами θ в квадрате.
И теперь делаем замену
t равняется чему?
xt.
Так ведь?
Вот, в таком случае dx это у нас что такое?
Это на самом деле dt
на θ.
Одна θ у нас как раз таки сократится.
Все правильно получается.
То есть это будет θ
от 0 до бесконечности.
Вот, ну и тут будет по сути
γ функция сейчас у нас соберется
t в степени k-2
e в степени −t
делим на γ функцию
точки k
dt.
И все, и вот это у нас получилось.
θ
здесь вот, это γ функция
в точке k-1.
Вот, здесь k.
Что у нас получилось?
Делим на k-1.
На k-1?
Да.
Ну потому что γ функция в точке k
это k-1 факториал.
Вот здесь у вас получается
γ функция от k-1.
Соответственно здесь будет k-1 факториал.
Вот, отлично, получили с вами почти.
Но нам нужно для θ квадрат.
То есть на самом деле давайте проделаем
теперь то же самое,
только для 1 поделить на x в квадрате.
Все, теперь я утверждаю, что мы будем
рассматривать оценку вот такого вида.
Вот это в квадрате будет?
То есть это у нас будет
γ распределение в квадрате?
Здесь будет 1 на x в квадрате?
Нет, нет, нет.
Гамма т 1.1 в параметре.
Да, здесь будет параметр 1.1 на θ.
Спасибо большое.
Вот это очень важное замечание.
Вот, потому что там от параметризации
много зависит.
И там как бы параметризация есть советская, есть
есть зарубежная.
И на самом деле, я не знаю, я помню даже
задачки давал, типа там нужно было
что-то доказать, а оно не доказывалось,
потому что у гамма распределения
допустим есть адидитивность по одному
то есть в большинстве от параметризации
это могли в каком-то случае не доказать.
Это?
Допсемная.
Так.
Здесь что у нас будет?
k-3, так ведь?
В степени k-3 тоже оставим.
Соответственно у нас θ в кубе вылезет,
так ведь?
Вот.
Отсюда опять от замены 1 θ вылезает.
То есть у нас θ в квадрате?
Гамма от k-2.
Да.
То есть на самом деле мы здесь получим с вами
θ в квадрат.
θ в квадрат.
k-1 на k-2.
Ура, друзья!
Мы почти с вами решили задачу.
Давайте подытожим.
Почти все.
Вы же помните, какой у нас был план решения?
Мы нашли с вами полную достаточную статистику,
а затем должны были решить уравнение
несмещенности?
Мы почти его решили.
У нас еще константа мешается.
Потому что пока что математическое ожидание
этой штуки, нет это в квадрате.
А какое же оно?
Да.
То есть давайте резюмировать.
Давайте резюмировать.
Потому что мы потратили достаточно много времени
на гамма-функцию.
Я просто забыл про то, что там параметризация
другая.
Если вы даже на Википедию зайдете,
там короче две параметризации есть.
А мой товарищ третью придумал.
Все, давайте подытожим.
Прошу прощения за заминку.
Ну вот, до часа и закончили как раз.
Первое.
Мы нашли с вами полную и достаточную
статистику по критерию факторизации
и по телесности множества.
По телесности множества.
И эта статистика у нас оказалась.
Сумма иксов.
Мы нашли полную и достаточную.
А вторым шагом мы должны были решить уравнение
несмещенности.
Найти такую функцию phi.
И мы нашли полную и достаточную.
Мы должны были решить уравнение
несмещенности.
Найти такую функцию phi,
что phi от суммы иксов
будет равняться это в квадрате.
Ну от ожидания от phi от суммы иксов.
Мы почти решили.
Потому что если мы возьмем phi
1 поделить на вот это в квадрате,
то мы получим как раз таки.
Давайте просто домножим.
Откуда у нас k1 и k2 обрались?
Количество наблюдений.
Количество чего?
Ну количество наблюдений у нас.
Ну да.
Ну да.
Ну да.
То есть на самом деле у вас, смотрите.
Тут вот это k1 и k2.
Это на самом деле k.
Прошу прощения.
Все опять параметризация упёрлась.
В общем вот то, что мы там k подразумевали,
это размер выборки.
То есть я там просто в какой-то момент
сделал скачок.
У нас была гамма n1 на θ,
а потом там откуда-то появились k.
Вот эти k это n если что.
Прошу прощения.
Вот тут вот этот товарищ мой придумал параметризацию.
Прошу прощения.
Вот ожидание получилось n-1
на n-2.
Вот.
Понятно.
По линейности математического ожидания
какую фи мы должны взять, чтобы
на это ожидание получилось вот такое?
n-1
на n-2
1 поделить на
сумму хоп
в квадрате.
То есть на самом деле эта задача,
она не очень сложная.
В чем ее сложность заключалась?
Понять какую параметризацию нужно взять
и не продолбаться в этом.
То есть найти полную достаточную статистику,
решить уровни несмещенности.
Самая большая проблема здесь была с
уровнением несмещенности, потому что вам нужно было
вот такой вот крокодил оценить.
То есть там
линейностью не сделаешь, как бы никак не сделаешь,
нужно как-то вот по определениям считать.
Потому что иногда вот уровни несмещенности
здесь более простые попадаются.
Если бы нас попросили
с уровнением несмещенности
найти вот такой параметр,
но это было бы очень просто.
Это на дисперсию похоже у экспоненциального распределения.
Ну просто там какой-нибудь второй момент
выборочный бы подошел, нормированный,
но вот второй момент бы какой-нибудь подошел.
Понятно, да, в чем проблема?
То есть это мы гораздо бы проще оценили.
Нам дали просто здесь не очень хорошую функцию,
для которой нужно построить оптимальную оценку.
Поэтому нам пришлось здесь немножко ползиться,
когда мы решали уравнение несмещенности
через функции.
Когда мы искали такую функцию, чтобы вот это было выполнено.
Вот.
Что еще?
Вот получается
в этих задачах, когда вас могут попросить,
что вас могут попросить вообще сделать?
Проверить достаточность,
проверяется по критерию факторизации.
По определению
По определению, если
просто для экспоненциальных пользуемся свойством.
Хорошо.
Вот.
Найти оптимальную оценку.
В таком случае ищете сначала достаточную оценку,
потом проверяете, что она полная,
либо сразу полную достаточную, если это экспоненциальное подсмещение,
и решаете уравнение несмещенности.
Чтобы в отождании фи от этой штуки было вот таким.
Потеряем релемма на шафе,
вот эта оценка у нас получилась оптимальной.
Если есть вопросы, задавайте, пока.
Это буквально все статьистика.
Ещё одна задача.
Да, кстати, есть у меня пару секретных.
5 секретных.
4 секретных.
4 секретных.
3 секретных.
4 секретных.
3 секретных.
2 секретных.
3 секретных.
Поехали!

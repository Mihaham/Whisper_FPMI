Ну давайте начнем. Я Кате в общем сбросил задачки, которые мы хотим сегодня разобрать,
ну и как обычно, наверное, разберем какую-нибудь теорию, чтобы задачке было проще решать и чтобы
понималось, что происходит вообще в целом. Вот. Сегодня у нас в меню характеристические функции,
сходимости и предельные теоремы, гауссовские векторы и условные математические ожидания. Так,
еще скажу, что вот тот вариант, который я скинул Кате, это пробный вариант прошлого года. Вот.
Вряд ли будет что-то сильно от меня отличаться в этом году. Вот. Но пробного варианта этого года
пока что еще нет. Вот. Поэтому ориентируемся на пробный вариант прошлого года. Собственно,
ну давайте начинать тогда. Начнем с вами с характеристической функции.
Собственно, начнем с определения, наверное. Итак, пусть у нас есть какая-нибудь случайная
величина, тогда ее характеристическая функция — это следующий объект.
Математическое ожидание E в степени и ксито. Вот. Возможно, не совсем понятно, как интегрировать
комплексные функции. Ну вот, здесь можно воспользоваться формулой Эйлера и свести это
уже к обычным вещественным интегралам. Вот. И тут уже никакой магии нет. Обычные матожи,
которые мы учились считать в первой половине семиастра. Отлично. С определением разобрались.
Утверждение — для любой случайной величины существует характеристическая функция. Ну,
кажется, что это правда. Потому что, заметим, следующая. Модуль характеристической функции
меньше либо равен единичке, ну так как это комплексные экспоненты. Здравствуйте. Вот. Так
что эта функция точно интегрируема и никаких проблем вот здесь вот у вас не будет. Так что
характеристические функции существуют всегда. Отлично. Ну и давайте что-нибудь тогда посчитаем.
Посчитаем пару примеров дискретных и, наверное, пару примеров непрерывных. Начнем с дискретных
примеров. Первый пример. Си равняется константе С. Вот. Как выглядит характеристическая функция
такой случайной величины? Ладно. Наверное, вопрос сложный. Ответ — Е в степени И, Ц, П. Ну да. Это
у нас выстрелит чуть позже. Просто давайте вот какие-то базовые вещи тоже повторим. Пусть у нас
С имеет Бернулевское распределение с параметром П. Тогда характеристическая функция такой случайной
величины будет Е в степени И, так Т на П плюс 1 минус П. Вот. Ну действительно у нас случайная
величина принимает два значения 1 и 0. Здесь мы подставляем вместо С единичку вероятность единички
это П. Потом здесь у нас такая же экспонента получается, но мы в нее нолик подставляем,
оно в единичку превращается и умножаем на вероятность нолика 1 минус П. Вот. Что-то
понятное. Ну давайте теперь что-нибудь посложнее. Пусть у нас С имеет нормальное распределение с
параметром 0.1. То бишь стандартное нормальное распределение. Давайте посчитаем ее характеристическую
функцию. Вот. Ну здесь воспользуемся с вами формулой Эйлера сразу.
Получается что-то вот такое. Заметим, что плотность у нас симметрична относительно 0,
поэтому четная. Поэтому вот эта функция вот здесь под интегралом будет нечетная. Соответственно
интеграл это нечетная функция. Ну и в общем вот это слагаемое зануляется. Так что у нас остается
только один интегральчик. Вот этот вот. Перепишем его как-нибудь в интегральной форме. Домножим на
плотность по определению математического ожидания в абсолютно непрерывном случае.
Вот. Ну и дальше утверждается, что такие интегралы вы умеете считать. Там можно либо по частям
посчитать, либо брать производную того, что под интегралом. В общем утверждение фиксия Т это
Е в степени минус Т в кладате пополам. Вот. Какие-то выкладки отпустим. Отлично. Ну смотрите,
в общем случае у нас характеристическая функция, она как бы комплекснозначная. Ну потому что вот у
вас есть действительная часть и вот есть мнимая часть. А вот здесь у нас получилась характеристическая
функция, которая принимает только действительные значения. Вот. А существует ли какой-то вот как бы
признак того, что у вас характеристическая функция будет действительно значной? Существует. Это
вот свойство. Так. Здесь неудобно будет. Ну ладно. Свойство. Давайте его назовем два. Это было,
если что, свойство один. Вот. Свойство два о чем гласит. Что если у вас распределение симметрично,
симметрично распределена случайно величина, то характеристическая функция ее
бесчастно значна, обратно верно так же. Вот. Ну доказывается это очень просто на самом деле. В
первом случае, то есть давайте стрелочку сначала одну какую-нибудь докажем. Пусть у вас распределение
симметрично, то есть стрелочка вправо. Просуждения абсолютно такие же, как и с нормальным распределением
с параметрами 0,1. У вас просто мнимая часть зумляется, потому что у вас там будет интегральчик от
нечетной функции. То есть в эту сторону давайте считать, что доказали. Вот. Давайте покажем теперь
это в обратную сторону. Соответственно, что мы хотим с вами заметить? Первое, давайте мы с вами заметим
вот такое равенство. Это верно для всех характеристических функций и более-менее очевидно,
ну потому что вы вот здесь вот если минус допишите, вы можете его либо к аргументу добавить,
либо вот как бы приклеить к случайной величине. Поэтому вот это более-менее понятно, ну и
линейное преобразование нас чуть позже также появится. Вот. Дальше мы с вами воспользуемся
вот таким вот свойством. Это на самом деле свойство 3. Что у нас в общем о связи характеристической
функции с сопряженным значением. Ну более-менее понятно, что если вы возьмете сопряжение
характеристической функции, то у вас по сути поменяется знак вот при имнимой части. Вот.
А соответственно, почему вот это равенство верно? Потому что вот этот минус, который у вас здесь
появится, можно занести как бы под синус. Синус нечетная функция, поэтому минус у вас
сюда внутрь залазит. А косинус четная, в него как бы можно минус тоже запихнуть и ничего не
изменится. Соответственно, вот это равенство верно тоже для всех характеристических функций. Это
свойство 3 вот проверяется просто по определению. Вот. Ну и последнее, что мы хотим заметить, так как у
нас по условию характеристическая функция вещественна, сопряжение ее никак не меняет. То есть это
фикси от m. Итого мы получили, что характеристическая функция случайной величины совпадает с
характеристической функцией минус случайная величина. Вот. Соответственно, характеристические
функции, их совпадают, значит распределение симметрично. Давайте так аккуратно скажем.
Вот. Супер. Ну и соответственно, вот хороший вот пример, что если у вас стандартная нормальная
случайная величина с параметром 0.1, она симметрична относительно нуля, и у нее действительно значенная
характеристическая функция. Так, идем дальше. Дальше давайте еще посчитаем характеристическую
функцию экспоненциального распределения, экспоненциальной случайной величины. Она нам сегодня
тоже в задачке пригодится. Потом мы просто сошлемся на вот тот результат, который сейчас получим.
Так. Куда мне ногу положил? Ну ладно. Итак, пусть кси у нас имеет экспоненциальное распределение
с параметром лямбда. Хотим посчитать характеристическую функцию. Вот. Ну, здесь можно опять-таки
формула эйлера воспользоваться. Можно немножко схитрить и сказать, что и это константы, и просто
экспоненты поинтегрировать. Ну давайте в этот раз допустим так сделаем. У нас получается интегральчик
е в степени xt на плотность экспоненциальной случайной величины. Плотность экспоненциальной случайной
величины с параметром лямбда это лямбда е в степени минус лямбда х dx. Ну и там индикатор еще,
поэтому от 0 до бесконечности. В плотности экспоненциального распределения зашит индикатор
х больше либо равен нуля, поэтому мы его сразу вот в пределы интегрирования вписали. Вот. Ну что
хотим заметить? Константы у нас выносятся из-под интеграла, и внутри у нас можно собрать экспоненты.
Значит у нас х выносится, здесь будет ит минус лямбда dx. Вот. Ну и вот здесь давайте мы комплексный
анализ конечно не знаем, но скажем, что и это такая же константа, как все остальные, с которыми мы
здесь работаем, кроме ха, поэтому мы можем с вами вот на это все домножить х и соответственно поделить тоже.
Ит минус лямбда, прошу прощения. Вот. И у нас получается, нужно посмотреть еще вот,
чему у нас будет равен вот этот интегральчик. Ну интегрируем экспоненты у нас по идее получается
то же самое в постановке от нуля до бесконечности. Вот. Ну в бесконечности,
я здесь минус потерял или не потерял, минус лямбда х. Да, у нас х вот к минусу лямбда относится. В общем,
бесконечности у вас в эту штучку зановится, потому что у вас вещественная часть к нулю стремится. Вот.
А в нуле это будет единичка. Поэтому ответ, сверить, что я не ошибся. Вот здесь еще из формулы
Ньютона Лебницы минус вылезет, поэтому минус мы можем к нам сюда закинуть и ответ вот такой. Вот.
Этот результат нам пригодится сегодня еще. Ну мы только начали. Еще не вечер. Я думаю,
скоро мы дойдем до какого-нибудь подобия катарсиса. Минут через 10 там будет что-то интересное. Вот. Но
пока что все, что мы с вами научились сделать, это просто считать какие-то характеристические
функции стандартных распределений, которые у нас до этого встречались. Давайте какие-нибудь
дальше свойства исследуем. Так, это я стирать пока не буду. Давайте посмотрим на характеристическую
функцию линейного преобразования. Вот пусть у вас есть случайная величина кси, и вы знаете,
что ей соответствует характеристическая функция фи кси от н. Вопрос. А как будет
выглядеть характеристическая функция линейного преобразования? Ну то есть домножили на константу
и прибавили какую-то константу. Утверждение. Ее можно выразить вот в терминах характеристической
функции исходной случайной величины. Доказательства. Ну давайте просто по определению попробуем
найти характеристическую функцию вот этого чуда. По определению альфа кси плюс бета. И т еще. Вот.
Дальше здесь можем в принципе просто раскрыть скобочки. Заметим, что вот одно слагаемое никак
от кси не зависит, поэтому мы его можем за от ожидания вынести. Будет и бета. А здесь останется
математика. Математическое ожидание. Е в степени и альфа кси. Вот. Ну и вот это альфа мы можем
прилепить к т к нашему параметру. И тогда мы с вами получим, что на самом деле это и бета
на характеристическую функцию кси в точке альфа. Вот. Замечательный результат. Прекрасная компания.
Вот. А для чего мы это сделали с вами? Ну давайте теперь найдем характеристическую функцию произвольного
нормального распределения. Произвольное нормальное распределение. То есть у него есть какие-то параметры
а и сигма квадрат. Мы с вами знаем, что такая вот случайная величина может быть получена из
стандартной нормальной случайной величины путем домножения на константу и добавления константы.
Домножить нужно на сигма, а прибавить нужно а. Вот. Ну и соответственно, пользуясь вот результатом,
который мы вот здесь получили для линейного преобразования случайной величины, мы найдем
сами характеристическую функцию произвольной нормальной случайной величины. Фиксиат. А будет
просто е в степени иат минус так сигма квадрат, т квадрат пополам. Вот. Этот результат нам тоже
еще поможет и пригодится. Можете его как-нибудь в рамочку ввести. Отлично. Почитали с вами
характеристическую функцию нормальной случайной величины с произвольными параметрами. Дальше.
Что еще позволяет удобно делать характеристические функции? Ну, сворачивать распределение. То есть
искать распределение суммы двух случайных величин независимых. То есть раньше у нас,
чтобы найти распределение суммы, мы с вами сворачивали плотности, и там достаточно
неприятные интегралы были. Если вы пробовали доказать, что сумма двух нормальных случайных
величин независимых это нормальная случайная величина, то вы, наверное, знаете, что там
неприятная интеграла вылазит. Вот. А если нет, то мы сейчас с вами аккуратно это через
характеристические функции докажем. Итак, утверждение. Пусть у нас есть две независимых
случайных величины к сиэтам, а тогда характеристическая функция суммы это просто произведение
характеристических функций. Вот. Этот факт, в принципе, тоже несложно доказать. По определению,
характеристическая функция суммы это вот просто математическое ожидание вот такого вот объекта,
вот. Дальше у нас это распадается на два множителя. Всего того, что к сиэту у вас независимы,
мы можем разбить на произведение двух математических ожиданий. Вот. Такое свойство
сумма тоже у нас было. И это Т. Вот. Ну и, соответственно, а это по определению уже две
характеристические функции исходных случайных величин. Замечательно. Ну и давайте с вами вот
такой полезный факт докажем. Достаточно, на самом деле, практический. Что вот если у вас есть к си,
нормальная случайная величина с параметрами a1 в σ1 в квадрате, и есть случайная величина это,
которая также нормально с параметрами a2 σ2 в квадрате, и они независимы, то их сумма
тоже нормальное распределение, имеет тоже нормальное распределение с параметрами a1
плюс a2 σ1 в квадрате плюс σ2 в квадрате. Так, σ1 в квадрате плюс σ2 в квадрате. Вот. Ну,
доказательства просто полностью строятся на предыдущем факте, который мы с вами только что
доказали. Запишем с вами характеристические функции. Просто найдем с вами характеристическую
функцию к си плюс это по вот тому утверждению выше. Это просто произведение соответствующей
характеристической функции. Ну и заметим, что там можно то, что у нас находится в показателе
экспонента сгруппировать правильно, и у нас получится a1 плюс a2 t, а минус получается
σ1 в квадрате плюс σ2 в квадрате, t в квадрате по полам. Вот. Просто перемножив две вот такие
функции соответствующими параметрами, вы получите вот ровно вот такое выражение. Супер. На самом деле
этот факт действительно очень полезен, потому что на практике, если у вас там курьеры примерно
нормальное распределение времени доставки имеют, время готовки тоже нормальное, то вот суммарно
время от поступления заказа до доставки его клиенту тоже имеет нормальное распределение. Этим
можно как-то пользоваться на практике, если вы какие-нибудь там лабы делаете. Где-то это может
пригодиться, в общем. Супер. Но кто нам сказал, что вот этот объект, который у нас получился, это все
еще характеристическая функция какой-то случайной величины, и почему именно нормальные случайные
величины? Вот. Ответ на этот вопрос дает теорема о единственности. В общем, утверждение такое,
что если у вас две случайные величины совпадают по распределению, то у вас по точечно равны их
характеристические функции. Вот. И теперь уже в принципе можно начать отвечать на вопрос,
почему хар функция это классно, и это очень удобный инструмент для доказательства чего-то.
Как бы сейчас немножко вот такой гуманитарный, гуманитарная минутка. То есть, смотрите, у нас
были с вами просто какие-то меры вероятностные на прямой, и там в силу теоремы короттадора они
взаимооднозначно соответствуют функциям распределения на прямой. А теперь у нас появился новый объект,
характеристические функции. И вот в силу теоремы о единственности у вас как бы вот здесь везде
однозначные соотношения между ними стоят. И теперь, если вы хотите доказать, что сумма двух,
допустим, нормальных случайных величин, это тоже нормальная случайная величина, вы можете
сделать, по сути, обратное преобразование фурье, посчитав характеристическую функцию, перейти вот в
это пространство. Здесь воспользоваться теоремой о свертке, которая очень просто выглядит. Вот.
Здесь вы доказываете этот факт, а затем в силу теоремы о единственности вы можете обратно
вернуться как бы к вашим случайным величинам от характеристических функций, и, по сути, вы
для ваших случайных величин доказали какое-то утверждение более простым образом. То есть и
в принципе все доказательства, которые строятся на характеристических функциях, строятся на том,
что вы вот как бы ищите у случайных величин характеристические функции, для них что-то
доказываете какой-то результат, более просто. И возвращаетесь обратно в исходное пространство и
и говорите, что вот, в CPT у вас получается предельная
случайная величина, а это m0,1, например.
Окей?
Более-менее, думаю, понятно.
Так, давайте дальше.
Соответственно, какие еще удобные штуки нам позволяют
делать характеристические функции?
Ну, на самом деле, при помощи характеристических функций
можно вычислять моменты очень легко.
Момент случайных величин и давайте дифференцирование.
Ну ладно, это писать не будем, просто моменты случайных
величин.
Значит, утверждение.
Пусть у вас существует абсолютный n-тый момент у случайной
штуки.
То есть, просто вот таком от ожидания определено.
Ну и пусть будет меньше бесконечности, например.
Тогда можно сказать следующее.
Можно зачем-то, ну сейчас поймем зачем, рассматривать
производные характеристической функции, которые на самом
деле будут равны следующему.
И более того, можно удобно искать моменты, то есть
от ожидания случайной величины в какой-то степени.
Вот это утверждение без доказательства.
Мы сейчас просто обсудим неправильную идею доказательства.
Правильная у вас там на лекциях будет.
В общем, идейно можно смотреть на это так.
Вот это же у вас по сути интеграл с параметром.
И вот здесь мы просто с вами по сути по параметру
дифференцируем.
Если вы продифференцируете вот это выражение по параметру,
то у вас как раз таки вот x из-под экспонента будет
вылазить s раз, и поэтому вот это равенство более-менее
кажется верным.
Вот.
А дальше, если мы просто поставим вот в это выражение
нолик, то у вас вот эта экспонента станет единичкой.
У вас останется математическое ожидание x в степени s.
Вот.
Мы видим на y в степени s, мы выразим с вами математическое
ожидание x в этой степени.
Вот.
Ну и можно какой-нибудь пример рассмотреть.
То есть теперь вместо того, чтобы искать математическое
ожидание нормальной случайной величины или дисперсию,
например, интегрируя там что-то неприятное, можно
просто дифференцировать характеристическую функцию.
Вот.
Давайте попробуем найти математическое ожидание,
пусть x у нас имеет нормальное распределение с параметрами
a sigma квадрат, давайте найдем с вами математическое
ожидание x.
Ну вот, в силу сказанного выше, это получается будет
первая производная в нуле поделить на i, а соответственно
характеристическую функцию я стер.
Ну вот, это почти она, давайте я ее чуть-чуть подправлю.
Уберу просто второй параметр, потому что сейчас у нас
вот такая у нас характеристическая функция, вот у этой случайной
величины.
Соответственно, давайте ее продиференцируем разок
и поставим в точке ноль.
Вот.
Ну если будем дифференцировать, так iat, что у нас будет?
Значит, производное того, что там это у нас ia, минус
производная вот этой штучки по t, получается у нас двоечка
спускается, два сокращаем, ну и sigma квадрат дам.
Ну и на экспоненту, вот давайте я здесь многоточие
поставлю, я по сути переписал все то, что в показателе
было.
Теперь мы это подставляем в точке ноль, все что у вас
было в экспоненте в показателе у вас превращается в нолик,
потому что у вас вот эта слагаемая зануляется, эта
слагаемая зануляется, поэтому вы e возбудите в степень
ноль, это будет единичка.
Здесь я тачку потерял.
Зануляется вот эта слагаемая, и у вас остается только
ia.
Соответственно, по вот этому утверждению нужно еще
на i поделить.
Ну и вот если вы на i поделите, у вас как раз-таки получится
тот факт, который мы уже с вами и так знаем, в отождании
икси это вот просто первый параметр нормальной случайной
величины.
Соответственно, так может быть удобнее искать, чем
интегрировать что-то, ну всегда там брать производную
проще, чем интегрировать, поэтому так вот можно все
моменты искать случайной величины, если вам в какой-то
задаче это может понадобиться.
Да, потому что мы подставляем с вами, вот мы сначала взяли
производную, а потом ее в точке ноль, и поделили
на i в соответствующей степени.
Вот эта слагаемая занулилась, потому что у нас тут т-шечка
одна осталась, вот все, что здесь в показателе тоже
занулилась, потому что там т-шечки были.
Ну и соответственно здесь будет а.
Ну и если хотите, можете там тоже самое для дисперсии
посчитать, то есть найти второй момент, а потом просто
вот из него вычесть икси в квадрате, и у вас тоже получится
сигма квадрат.
Вот, супер, тоже удобно на самом деле, и полезно.
Так, что дальше у нас, теорему единственности мы с вами
обсудили, вот, давайте еще немножко вот вернемся
к теореме единственности.
Вот если вам дали плотность, более-менее понятно, как
считать характеристическую функцию, но просто по определению.
А вот если вам дали характеристическую функцию, как найти плотность?
Ответ на этот вопрос, грубо говоря мы знаем, как с Вами
от распределения переходить к характеристическим функциям,
мы просто считаем с вами по определению обратное
преобразование фурье, а как вернуться из характеристических
функций в пространство распределений или плотностей,
если у Вас плотность существует.
Ответ на этот вопрос дает теорема об обращение.
Так, там, в общем, у нее два случая есть, то есть у вас
же не всегда плотность существует, и вот в случае, когда у вас
плотность не существует, мы сейчас рассматривать
не будем, там нужно функции распределения выражать.
То есть мы вот этот пункт с вами не рассматриваем
сейчас.
Мы рассмотрим с вами второй пункт, то есть пусть у вас
модуль характеристической функции интегрируем, то
есть меньше бесконечности, тогда плотность можно найти,
по сути, как прямое преобразование фурье от характеристической
функции.
Вот, ну здесь ДТ, короче, то есть вы там в мотоне, наверное,
рассматривали тоже преобразование фурье, и вот у вас там каждый
раз был коэффициент один поделить на корень из
двух пи, вот, а здесь мы просто с вами, когда считаем преобразование
из плотностей в характеристические функции, мы ни на какой умножитель
не домножаем, вот, ну соответственно, чтобы мы вернулись к той
же плотности, видимо, значит, нужно будет домножить вот
один на два пи, вот, ну, это просто как константа расставить.
Это, значит, теорема об обращении.
Ну и давайте небольшую задачку с вами решим.
Она почти, наверное, была у вас в семинарских листочках,
но она заслуживает внимания, и сделаем какие-то выводы
из нее.
Так, так, так, так, так, так.
Задачка, собственно, была такая, она о нескольких пунктах.
Мы что-то с вами пропустим, потому что будем считать,
что это было на семинарах, а что-то вот детально обсудим.
Итак, первое, первый пункт, если вы знаете, что плотность
вашей случайной величины имеет следующий вид,
ну, то есть распределение лапласа, посчитайте ее
характеристическую функцию, вот, но мы сейчас с вами
считать не будем, будем считать, что вы это сделали
на семинарах, поэтому просто сразу напишем, что ее характеристическая
функция имеет вот такой вид.
Тут формулейлеры можно воспользоваться, а потом
там два раза по частям интегральчик взять, косинус
на вот какое-то такое выражение, ну и там все берется, сейчас
не хочется на это время тратить, вот, то есть мы
имеем плотность и посчитали характеристическую функцию,
вот, это мы пропустили с вами, но ничего страшного,
поверим.
Второе, рассмотрим случайную величину кси, так, давайте
теперь это здесь будет, имеющая распределение
коши с параметрами 0, ну тоже давайте, альфа, соответственно
плотность у нее, у такой случайной величины, это
альфа поделить на пи альфа в квадрате плюс х в квадрате.
А, собственно, в чем проблема? Вообще, исходная задача
состоит в следующем, найдите как бы характеристическую
функцию для распределения коши, вот, ну такой вот интеграл
считать, если вы это домножите на ИТЕ в степени ИТХ, в общем
очень сложно, если бы ТФКП прошли курс, то вы бы такой
интегральщик очень легко взяли, но если бы вот ТФКП
еще не проходили, то такой интеграл очень сложно
берется, но здесь есть один лайфхак, этот лайфхак
заключается в том, чтобы воспользоваться формулой
утеряемой об обращении, ну и собственно давайте
это аккуратно сделаем.
Да, да, да, идея в том, чтобы свести к первому, то есть
решение этой задачи очень простое, первое, для распределения
лапласа выполнено теорема об обращении, ну просто
потому что вот этот модуль этой функции интегрируем,
там все хорошо с этим, поэтому мы попадаем с вами во второй
пункт теоремы об обращении, соответственно мы можем
восстановить плотность, мы знаем с вами и плотность,
и характеристическую функцию, ну вот давайте вот это вот
равенство из теорем об обращении запишем, если мы
его запишем, то мы получим следующее, альфа пополам,
е в степени минус альфа моли х, это плотность, это
интеграл по r, так, е в степени, так, тут еще константа,
1 на 2 π, е в степени минус и ТХ на характеристическую
функцию, характеристическая функция у нас вот такая,
вот, это формула обращения для распределения лапласа,
отлично, значит вот у нас формула обращения для
распределения лапласа, а что мы с вами хотим заметить,
по определению, чтобы посчитать характеристическую функцию
каши, мы должны с вами какой интегральчик взять, ну
мы должны с вами взять интегральчик, е в степени
и ТХ на плотность, плотность распределения каши, вот
такая, вот, ну и давайте заметим, смотрите, вот это равенство
у нас выполнено в силу теоремы обращения, а вот это вот
уж больно напоминает вот это выражение, ну давайте
мы с вами какими-то манипуляциями, там, домножением на какие-то
константы приведем вот это выражение к выражению
вот такого вида, а что для этого нужно сделать, ну
вот смотрите, π мы можем вот сюда внутрь занести,
на двоечку, видимо, мы можем домножить обе части, у вас
вот эта двоечка пропадет, а, видимо, мы можем еще поделить
обе части на алифа, потому что, ну, параметры больше
нуля, ну супер, и у нас теперь в правой части
получился вот тот интеграл, который нам нужно было посчитать
изначально, ну, за одним исключением вот здесь минус
есть, но этот минус не так страшен, потому что мы
можем сделать с вами замену переменных, Т поменять на
минус Т, вот, здесь тогда у вас минус уйдет, в силу
симметрии ничего не сломается, все, задача решена, то есть
ответ, характеристическая функция, случайные величины,
имеющие распределение каши, имеет вот такой вид, а, может
вопрос повторить?
Ну вот почему у нас там есть, да, смотри, вот это, это
по определению просто характеристическая функция, в точке Т, это вот просто
определение мы использовались, окей, все, супер, посчитали
с вами характеристическую функцию, ну и, собственно,
мы считали это не просто так, это контрпример о том,
что свертка необратима, утверждение следующее,
утверждение следующее, значит, давайте вспомним с вами
свертку, если у вас случайные величины независимы, тогда
у вас характеристическая функция суммы, это произведение
характеристических функций, что-то такое было, вот, ну
вот, тут стрелочка вправо была, а вот сейчас мы с вами
по сути приведем контрпример, что стрелочки влево нет,
в обратную сторону неверно, то есть если характеристическая
функция суммы двух случайных величин распалась в произведении
характеристической функции, из этого не следует, что
у вас случайная величина независима, ну вот, доказательства,
контрпримеры.
Рассмотрим с вами кси, а случайную величину имеющую
распределение коши с параметром 0.1, тогда ее характеристическая
функция, ну вот, мы с вами ее посчитали уже, то есть
это е в степени минус модульта.
И давайте с вами посмотрим на вот такое равенство.
Ну, заметим, что оно верно, вот, ну, почему, потому что
здесь у вас по сути будут вот два таких выражения,
они у вас при перемножении здесь будут е в степени
минус два модульт.
А слева у вас два кси, то есть у вас двоечка здесь
под модулем будет, двоечка вылезет из-под модулей,
и по сути у вас вот это равенство верное сейчас.
Но понятно, что кси зависит с кси, то есть здесь вот
независимости такой нет.
Ну и, собственно, мы с вами привели контрпример к
тому, что свертка в обратную сторону не работает.
Ну это как бы тоже такой интересный факт, который
на экзамен могут спросить.
Супер.
А тут еще кое-что видно на самом деле, что сумма
двух случайных величин, имеющих распределение
каши, ну, давайте допустим с параметром 0,1, даже если
они независимы, имеют распределение каши с параметрами 0,2.
Ну и вот тоже хороший контрпример, по сути у вас
ЗБЧ не выполняется.
Ну и УЗБЧ тоже получается.
Смотрите, у распределения каши нет математического
ожидания.
Это более-менее известный факт, думаю, вы все это
знаете.
Вот, а как у нас выглядит формулировка УЗБЧ?
Это вот немножко сбежим вперед, но это контрпример
мы сейчас с вами посмотрим, то есть у нас есть формулировка
УЗБЧ такая, что если у вас у последовательности
независимых случайных величин, независимых одинаково распределенных
случайных величин, а существует математическое ожидание,
тогда у вас выполнена вот такая сходимость.
Sn на n сходится к математическому ожиданию кси.
Ну и тут даже почти наверно можно выписать, потому что
это усиленный закон больших чисел.
Вот, и соответственно, в случае распределения каши
это не работает.
То есть смотрите, сумма двух случайных величин, имеющих
распределение каши, имеет тоже распределение каши,
но вот в нашем случае с параметром 0.2, если мы с вами
отнормируем вот на эту n-ку, то у вас здесь будет все
время распределение каши с параметром 0.1.
Ну и понятно, что это сходится к распределению каши с
параметром 0.1.
Вот, таким образом, как видите, условия на существование
математического ожидания в УЗБЧ, ну и в УЗБЧ тоже существенны.
Если мы откажемся от вот этого условия, то сходимости
у вас уже не будет, Константин.
Вот, супер, эту задачку мы с вами решили, мы почти
все по теории, это мы обсудили.
Это мы обсудили и это мы обсудили.
Все, давайте тогда задачи решать.
Соответственно, что могут дать на характеристические
функции в контрольной работе?
Сейчас сначала с вами обсудим, что могут дать, какие задачи,
какого вида, затем какую-то задачу разберем из прошлогоднего
пробника.
Так, какие задачи?
Все мы, к сожалению, обсудить не успеем, поэтому я в порядке
вероятности появления того или иного типа их вот
сейчас отсортирую, самые вероятные будут сверху, маловероятные
снизу.
Значит, ну вот, задачка как у нас, то есть посчитать
характеристическую функцию чего-то, ну там какой-нибудь
суммы произведения, ну вот, в нашем случае, ксен
минус кси, в общем, функции от нескольких случайных
величин.
А что еще могут дать?
На самом деле, вот смотрите, вы же знаете, что понять
является функцией плотности или не является очень просто.
Там достаточно условий, это просто чтобы функция
была не отрицательная, и второе условие, чтобы она
интегральщика от нее была равна единичке.
А вот проверить, является ли функция характеристическая
на самом деле вообще не тривиальна.
Вот, и задача второго вида, это вот проверить, является
ли данная функция, которую вам дадут, характеристическая.
Вот, такие задачки очень волоснов любят давать.
А может там где-нибудь на экзамене дать тоже.
Вот, ну что делать, в этот момент молиться, а потом
вспоминать теорему Бохна-Рахинчина критерий.
Вот, можно попробовать проверить функцию на не отрицательную
определенность.
Вот, и можно к ним там вспомогать на утверждение еще использовать.
Я потом к ним ссылки наверное оставлю.
Ну либо если вы хотите сказать, что функция не является
характеристической, достаточно показать, что какое-то из
свойств, которые мы сегодня с вами обсудили, не выполняется.
Вот, ну есть там еще на самом деле несколько теорем,
я не знаю какие у вас, может быть вам семинарист какие-нибудь
давал.
Там на самом деле очень много признаков, критерий
один очень сложный, но есть очень много признаков.
Вот, и может быть там каким-то признаком можно будет воспользоваться.
Вот, такие задачи мы сейчас разбирать не будем.
Вот, и третий вид задач, это на теорему непрерывности.
Мы вот такие задачи обсудим чуть позже.
По сути о чем это?
Если у вас есть сходимость по распределению, то это
практически, давайте я как-нибудь вот так нарисую,
это практически сходимость характеристических функций
в каждой точке.
Ну там, за небольшой оговоркой.
Вот, такие задачи мы обсудим чуть позже, когда сходимости
обсудим во втором разделе.
Вот, и давайте сейчас тогда решим задачу один, которая
вот в предложенном варианте имеется.
Так, если что, у меня все решения есть затеханные,
но мы сейчас с вами перерешаем задачи, если найдем ошибки,
то я поправлю и потом пришлю решение задач куда-нибудь.
Вот, у всех условия есть или нужно записать его?
Ну, давайте, ладно, кратненько напишем.
Значит, у нас есть две независимые случайно величины.
Кси имеет экспоненциальное распределение с параметром
2, это имеет равномерное распределение на отрезке
минус 1,1.
И, собственно, вопрос данной задачи, найдите характеристическую
функцию, то есть вводится новая случайная величина,
кси это минус кси, и вопрос данной задачи, найти характеристическую
функцию.
То есть первый тип задачи, это вот как от какого-то
преобразования случайных величин найти характеристическую
функцию.
Ну, давайте решать.
А первый шаг в этой задаче, это, наверное, заметить,
что вот считать характеристическую функцию сначала вот этого,
а потом как-то пытаться вот с этим объединять неприятно.
Почему?
Потому что у вас вот эти две случайные величины
уже будут зависимы, а независимые сворачивать мы не умеем.
То есть, наверное, первый шаг, это справедливо заметить,
что, короче, на множители разложить вот это выражение.
Ну и понятно, что вот эта случайная величина будет
уже от вот этой независима.
Ну и давайте какое-нибудь новое обозначение для
нее придумаем.
Это штрих, например.
Супер.
Но мы с вами не обсуждали, как искать характеристическую
функцию произведения двух независимых случайных
величин.
Ну вот самое время.
Давайте сначала запишем по определению, что от нас
хотят.
По определению характеристической функции, от нас очень хотят,
чтобы мы посчитали математическое ожидание E в степени i,
xi, e, a, t.
Дальше мы можем с вами воспользоваться...
Это штрих, так?
Да.
Спасибо большое.
Это штрих.
Вот такую характеристическую функцию от нас, вот такой
интегральщик от нас хотят посчитать.
Давайте воспользуемся определением математического ожидания
просто здесь.
Это у нас по сути интегральщик, и xi это штрих t, умножить
на совместную плотность, xi это штрих.
Так, сейчас, x, y, давайте введем тогда в точку x, y.
Вот так.
Это просто определение математического ожидания.
Я думаю, вы помните, что математическое ожидание
функции от вектора случайного, это просто вот такой вот
интегральщик, функции на совместную плотность.
Вот.
Ну что мы из этого знаем?
Знаем ли мы с вами совместную плотность?
Да, потому что у нас xi это штрих независимая, поэтому
в силу независимости у вас совместная плотность
двух случайных лечений, это просто произведение
плотностей.
Вот.
Ну и дальше в силу теоремы Фубини просто переходим
к повторному интегралу.
То есть здесь сразу мы два действия делаем.
Первое переходим к повторному интегралу.
Вот.
А второе, у нас вот эта плотность распадается на произведение
плотностей.
Вот.
Давайте одну плотность вот внутрь засунем.
Давайте допустим плотность xi dx.
Вот это получается первый интегральщик.
И здесь у нас еще будет плотность штрих у, второй интегральщик.
Ну просто теоремы Фубини.
Вот.
К повторному интегралу перешли.
Так.
Вот это что такое?
Ну вот, на что-то похоже.
Да, это характеристическая функция.
Вопрос чего?
Случайная величина xi видимо, раз у нас плотность xi.
И вопрос в какой точке?
В точке y.
Вот.
То есть теперь мы это можем сами на самом деле упростить
и записать это следующим образом.
Это у нас получается математическое ожидание.
И фикси в точке
ty.
Вот.
Вроде никакой магии не произошло, все более-менее понятно.
То есть просто здесь же у вас y по сути фиксированный,
то есть y это константа.
Вот это это характеристическая функция xi в точке yt.
Собственно, вот она.
Ну и по сути мы вот свернули вот к такому виду.
А это штрих куда пропало?
Ну вот это штрих у нас вот здесь спрятано в плотности.
Вот в этом математическом ожидании у вас как раз таки плотность это штриха скрыта.
Давайте вот здесь напишем математическое ожидание по вот это штрих,
чтобы ничего не терялось.
Ну и собственно самая достаточная вот эта вот формула.
Вот это.
То есть самое главное равенство здесь между вот этим утверждением,
вот этой записью и вот этой записью.
Потому что сейчас мы ее будем пользоваться, чтобы досчитать.
Все.
Так.
Я же говорил, что нам пригодится характеристическая функция экспоненциальной случайной величины.
И вот она нам сейчас пригодится.
А у xi у нас как раз таки экспоненциальное распределение с параметром 2.
Следовательно, характеристическая функция случайной величины xi это лямда на лямда минус и т.
Вроде бы так.
А, ну прошу прощения, лямда у нас два, поэтому можно двоечку подставить.
Вот.
Ну и все.
Что от нас требуется?
По сути требуется.
Посчитать.
Вот.
Теперь возвращаемся вот к той записи в правой части доски.
От нас требуется посчитать.
Математическое ожидание.
Ладно, здесь можно.
В общем, здесь лучше тогда.
Давайте мы так сделаем.
У же это у нас по сути это и есть.
Мы поэтому здесь перепишем с вами вот так.
И вот тогда.
Все.
Вот.
Ну, а это уже считается вроде как не очень сложно.
А просто по определению математического ожидания это будет интегральчик.
По R вот этой функции.
Вот здесь уже переходим к переменным.
И плотность.
Штрих с точки Y.
Для Y.
Вот.
А плотность, это штрих.
Чему у нас равна?
Видимо, это одна вторая штриха.
Видимо, это одна вторая.
На индикатор.
А Y принадлежит от минус двух до двух.
От минус двух до нуля.
Потому что у нас изначально была случайная величина от минус единички до единички.
Здесь мы вычли из ее единичку.
Она осталась также равномерно распредлена.
Только теперь на новом отрезке минус два ноль.
Вот.
И, соответственно, вот эта плотность у нас вот так представлена.
Ну, и то есть.
Нам нужно посчитать интегральщик от минус двух до нуля.
Вот.
Одна вторая двоечка сократится.
Получается, будет единичка на два.
Минус лямбда T.
Лямбда и T.
Все.
Тишечка не теряется.
Так.
Сейчас, одну секунду.
Так.
Два.
Два минус и T.
Два.
Два минус.
Так.
Два минус и T у нас, по идее, было.
Мы подставляем вместо T.
TY.
TY.
Ну, ТТТ штрих, по сути.
Вот.
А теперь переходим к переменам.
И у нас здесь остается.
И.
TY.
Вот.
И.
TY.
Вот так вот, вот такой интегральщик.
TY.
Да.
Спасибо большое.
Немножко потерялся в обозначениях.
Вот.
Ну, как такой интеграл дочитывать, более-менее понятно.
Домножаем просто на сопряженный знаменатель. У вас будет просто сумма двух интегралов.
Один будет вещественный, из второго ишечка у вас вылазит, и второй интегральчик тоже посчитать надо.
Можем еще один шаг сделать.
Так, если мы сделаем еще один шаг, то по сути давайте домножим на сопряженный знаменатель.
2 плюс и ty. У нас по сути будет интегральчик от минус 2 до нуля,
плюс и ty, а в знаменателе у нас будет, видимо, 4 плюс ty в квадрате.
Ну это разбивается на два интеграла по линейности просто.
Вот это что-то с арктангенсом будет, вот, и второй интегральчик будет вот такой.
Ty на 4 плюс ty в квадрате dy. Вот, ну а здесь просто вот это выражение можно под диффенциал засунуть,
ну это будет логарифм какой-то. Ну и там в ответе будет как раз таки арктангенс чего-то там,
плюс логарифм, ну плюс и на логарифм. Вот, и это итоговое. Давайте я выпишу просто ответ тогда,
наверное. Ответ у меня получился вот такой. Арктангенс. Так, да.
Так, давайте вот так. И это вот сюда. Арктанг с t на t, минус t пополам,
на логарифм t в квадрате плюс 1 поделить на t. Все. Есть вопросы по этой задачке? Вроде вот, в общем,
на самом деле, вот если самое главное, что в этой задаче есть, это вот переход вот отсюда вот к такому
выражению. Получается от непонятной как считающейся хор функции, а к мотожиданию хор функции от
случайной величины. Вот. И в принципе, этим можно пользоваться фактом на контрольной, если у вас там
будет хор функция произведения двух независимых случайных величин. Так. Окей вопросы? Ну тогда
пойдемте дальше. Соответственно, с характеристическими функциями мы, наверное, закончили. Да. Переходим к следующему
разделу. Следующий раздел у нас это сходимость и предельная теория. Собственно, я вот тут где-то
мог там с константами по ошибаться. Я потом, в общем, пришли в решение затеханное. Там я вроде не
должен был ошибиться, хотя тоже мог. Так, так, так, так. Сходимости и предельная теория.
Значит, начнем мы с вами с сходимости скалярных, то есть в одномерном случае. Потом посмотрим на
сходимости векторов, а потом уже какие-нибудь преобразования векторов будем рассматривать и
сходимости их. Так, собственно. Собственно, вот. Вообще мы рассматривали в курсе четыре вида
сходимости, но сейчас мы с вами обсудим только три. То есть эта сходимость, вот, наверное,
по определению. Это получается же вероятность тех аминь, где у вас нет сходимости, равно нулю.
Так, сходимость по мере. По определению, это вот такое выражение.
И сходимость по распределению. Для любой f непрерывно-ограничной
сходятся математические ожидания.
Вот. Наверное, определение долго обсуждать не будем. Перейдем к чему-нибудь более интересному.
Давайте сначала поймем с вами, как эти сходимости между собой связаны.
Значит так, утверждение. Связь сходимости. Ну, видов сходимости. Ну, более-менее. Думаю,
что вы все знаете, что сходимости почти наверное следует сходимость по вероятности, а сходимости
по вероятности следует сходимость по распределению. Вот. Стрелочки этим доказывать с вами не будем.
Они у вас будут доказываться, ну, или уже доказаны были на лекциях. А, соответственно,
дальше хочется, наверное, показать, что вот обратные стрелочки не верны в общем случае,
то есть попроводить контрпримеры. Два контрпримера, соответственно, нам нужно,
когда есть сходимость по вероятности и нет сходимости почти наверное. Это первый контрпример.
Ну, здесь базовый пример риса. Думаю, все помните. Там, в общем, рассматривается вероятностное
пространство просто отрезочек 0,1. В качестве сигма алгебры рассматривается баррельская сигма
алгебра под множество отрезка 0,1. А в качестве меры рассматривается вот стандартная мера либега.
Вот. И дальше строится последовательность множеств. Такого вида.
И дальше строится последовательность как раз таки случайных величин,
которые просто являются индикаторами этих множеств. И уже из них строится итоговая
последовательность. И так далее. Ну, если графически вот это все изображает, что я нарисовал,
то, в общем, там первая случайная величина. Это, по сути, просто константа 1 на всем отрезке 0,1.
Вторая случайная величина. Это просто получается константа на первой половинке отрезка. То есть
от 0 до индикатора, в общем, отрезка от 0 до 1,2. То есть функция вот так выглядит. А третья, так,
это первая, это вторая. Третья выглядит следующим образом. Это уже будет индикатор отрезка от 1,2 до 1,
и так далее. То есть в чем суть? Вы покрыли весь отрезок, потом вы покрыли его сначала одну
половинку, потом вторую половинку. Потом разбиваете на три части их. То есть дальше берете следующую
функцию. Четвертая. Это будет индикатор первой, третьей отрезка. Пятая функция индикатор второй,
третьей отрезка. Шестая функция индикатор третьей, третьей отрезка. Дальше делим на четыре части.
Можно четверть, не особо важно. В чем суть? В том, что если вы зафиксируете какую-то омега,
то вот смотрите, у вас эта омега, во-первых, при фиксированном омега у вас последовательность
превращается в обычную числовую последовательность. То есть если омега зафиксирована, то у вас будет
кси1 от омега, там кси1-1, кси2-1 от омега, кси2-2 от омега. Ну вот такая последовательность,
просто числовая. И соответственно, если вы зафиксируете какую-то омега, то вот у вас здесь
будет единичка. Ну потом единичка будет одна из вот этих двух. Один из этих двух элементов будет
единичкой. То есть если вы выбрали омега, которая от 0 до 1 и второй, то у вас получается вторая
функция, его накроют. Если в правом половинке, то третья. Вот. И в чем суть? Каждая омега,
у вас накрывается первая функция, накрывается 2 или 3, накрываются 3, 4, получается 4, 5 или 6,
накрывается 7, 8, 9 или 10. И так далее. То есть грубо говоря, у вас в этой последовательности
бесконечно встречаются единички. Они встречаются все реже, реже и реже, но они встречаются
бесконечно. Ну и получается у вас нет сходимости вот этой последовательности к 0. Собственно
сходимость почти наверно мы не имеем. Ну на самом деле вообще просто ни в одной
точке у нас сходимости нет. То есть при любом фиксированном омега вот эти
рассуждения верны. Почему есть сходимость по мере? Ну потому что меры тех точек,
где вы отличаетесь больше чем на Эпсилон, это по сути длина вот этого отрезка. И как
видите, она уменьшается. И соответственно вот это вот определение выполнено. А вот
это нет. Ну и все. То есть мы имеем с вами сходимость по вероятности и не имеем
сходимости почти наверно. Соответственно первый контрпример мы с
вами построили. Вот. Если чуть более детально нужно, я могу что-то еще сказать,
но вроде как более-менее все понятно. Это там на витами было где-то еще. Больше
наверно не имеет смысла повторять. Так, далее. Второй контрпример. Значит ксен у
нас сходится по распределению, но у нас нет сходимости по вероятности. Вот. Здесь
контрпример такой. Обычно очень искусственно строится. То есть рассматривается какое-нибудь
пространство из двух элементарных сходов. Вот. И соответственно все ксенты
полагаются так. Так. Давайте аккуратно зададим. Значит это пространство элементарных
сходов в качестве сигма алгебры, так как дискретное пространство. Возьмем просто множество
всех подножеств. В качестве меры, ну возьмем равномерно распределенную меру на этих
точках. То есть мера такая, что вероятность омега 1 равняется вероятности омега 2 и равняется
1 и 2. Что имеем? Соответственно, определим с вами ксен следующим образом. Так,
давай так. Ксен от омега равняется единичка, если омега равняется омега 1, и 0, если омега
равняется омега 2. А предельную случайную величину определим симметрично наоборот.
Таким вот образом мы определили. Вот. Ну и что мы по сути с вами имеем? В пределе у вас,
ну по сути, обе случайные величины имеют Бернулевское распределение. Они немножко по
разному устроены, но у обоих этих случайных величин, у этой последовательности и у предельных
случайной величины одно и то же распределение Бернулевское с параметром 1 и 2. Вот. Соответственно,
сходимость по распределению будет. У вас просто совпадают распределения, вследовательно у вас
будут совпадать математические ожидания. Вот в этом определении у вас каждый элемент вот в этой
последовательности математических ожиданий будет совпадать с предельным,
поэтому вот эта сходимость у восточной есть, а почему не будет сходимости по
вероятности, ну потому что давайте вот зафиксируем любой епсилон, давайте
зафиксируем епсилон равный 1 и 2, и рассмотрим с вами вероятность того, что
ксин минус кси больше епсилона, вот, ну при любом омега у вас вот здесь значение
противоположное, поэтому вот это всегда единичка, соответственно для любого омега
у вас вот эта разность больше, чем епсилон, равная 1 и 2, ну и соответственно вот это
просто равно 1, и не сходится к нулю. Такой вот искусственный пример, но он, собственно,
показывает, что сходимость по распределению не следует сходимости по вероятности. Так,
по этому конфпремеру вопрос есть? Ну я думаю, что все понятно. Так, идем дальше. Дальше.
Ну задачек на них скорее всего не будет. Ну как бы лп-сходимость, наверное, у вас там
только во время определения дневника встречалась, а дальше она достаточно редко встречается в
приложениях, по крайней мере в тех, которые мы обсуждаем. Так, идем дальше. Но все-таки вот
эти стрелочки иногда можно оборачивать. То есть в общем случае исходимости по распределению не
следует сходимости по вероятности, исходимости по вероятности не следует сходимости почти
наверно. Но есть как бы такие частные случаи, когда у вас как бы эти стрелочки верны. Собственно,
давайте это обозначим так. Когда можно стрелочки оборачивать? Я так и напишу.
Ну вот первую стрелочку можно обернуть в случае дискретного вероятностного пространства.
Ну ладно, давайте если у вас дискретно вероятностное пространство, то исходимости по мере, то есть по
вероятности, у вас будет следовать сходимость почти наверно. Но это действительно так. Давайте
просто с вами запишем определение сходимости по вероятности. Зафиксируем какой-нибудь
элементарный исход из тех, которых у нас есть. У нас элементарных исходов всего счетное число,
поэтому можем его зафиксировать. И у нас из этого определения выполнена следующая сходимость.
Так? Вроде так. А это просто определение сходимости по вероятности. Соответственно,
если вот это сходится к нулю, значит с какого-то момента это будет меньше. Ну существует номер,
начиная с которого вероятность вот этого множества будет меньше, чем вероятность вот этого омега,
который мы с вами зафиксировали. Соответственно, вот это омега здесь уже лежать не может. И
аналогичное рассуждение можно для всех омега провернуть. То есть на самом деле у вас вот в этом
множестве ни одно омега не предлежит. Соответственно, у вас все омеги попадают в дополнение к этому
событию, а значит у вас есть сходимость почти наверно. Есть вопросы? Вот. Ну думаю, тоже не
очень сложно. Вот. И более интересный случай это, конечно, вот когда мы можем оборачивать вот эту
стрелочку. Для этого нужно сейчас теориям Александрова сказать. Да, ладно, сейчас.
Собственно, если идти в каком-то хронологическом порядке, то мы можем с вами сначала рассмотреть
какое-то дискретное вероятностное пространство и изучить в нем сходимость по распределению немножко
подробнее. Давайте зафиксируем с вами дискретное вероятностное пространство. Например, связанное
со случайной величиной кси, которая принимает, допустим, пусть только натуральные значения. Такая
задачка, по-моему, у вас висточки должны были быть. То есть давайте так запишем это. Сумма вероятностей
того, что кси равняется n, где n принадлежит натуральным числам, равна 1. То есть кси принимает
только натуральные значения. Тогда ну и кси и кси n. То есть рассматриваем последовательность
случайных величин и предельную случайную величину какую-то, которую принимают только натуральные
значения. Так вот, тогда сходимость по распределению в дискретном случае равносильна тому, что для любого n.
Так, только сейчас будет коллапс обозначений. Давайте мы здесь лучше вот этот наказчик поменяем,
чтобы с n не конфликтовало.
То есть в дискретном случае сходимость по распределению эквивалентна тому, что вы в каждой
точке, то есть вы принимаете только натуральные значения и значит сходимость по распределению
эквивалентна тому, что у вас есть сходимость в каждой вот натуральной точке таких вот вероятностей.
Вот, но это доказывается очень просто на самом деле. Давайте в одну сторону это покажем. Давайте
покажем это вправо допустим. В левой чуть сложнее, но мы покажем вправо. То есть из
определения сходимости по распределению у нас верно, что для любой f непрерывно ограниченный
математическое ожидание f кси от m сходится к математическому ожиданию f от кси. Давайте
мы с вами рассмотрим функцию f, f кату. Ну вот, я ее сейчас графически нарисую. Она в точке k
будет принимать значение 1, а затем как-то вот так вот по непрерывности продолжена вот в общем
вот такая вот горочка. В точке k минус 1 она будет 0, в точке k плюс 1 она будет 0, в точке k она
будет 1. А дальше как-нибудь по непрерывности просто это продолжить. Вот, соответственно такую
функцию, такая функция попадает под определение вот непрерывно ограниченной функции сходимости по
распределению. Значит, ну ее можем применить. Супер. Ну и если вы просто запишете определение вот
этого математического ожидания, то у вас по сути здесь и получится, что вероятность того,
что кси равняется k, потому что во всех остальных точках у вас эта функция зановляется, соответственно
у вас остается только одно слагаемое. Значение единичка умножается на вероятность этого значения.
Получается только вот эта вероятность, которую мы ищем. Кси равняется k. То есть это выполнено.
В обратную сторону чуть сложнее и сейчас это обсуждать не будем. То есть как бы если вот
смотреть на дискретный случай такой достаточно простой, то как бы сходимость по распределению
эквивалентна тому, что вы в каждой точке сходитесь, вот ваша вероятность в каждой
точке сходится. Далее была попытка обобщить вот этот результат на непрерывный случай. Ну
вот в непрерывном случае у нас как бы вероятность каждой точки равна нулю,
поэтому что-то такое писать не совсем логично. Вот. Но зато можно сформулировать теорему
Александрова, которая по сути дает какой-то похожий результат в непрерывном случае.
Теорема Александрова. Ну это или следствие, там в общем в зависимости от того, как читает лекции,
это может быть следствием, а может быть и самой теорема Александрова.
Сходимость по распределению по сути эквивалентна тому, что для любой точки,
сейчас я поясню эту запись, у вас есть сходимость функции распределения.
Вот. Вот такая запись обозначает множество точек непрерывности предельной функции распределения.
Ну то есть по сути мы с вами здесь имеем сходимость вот таких вот вероятностей.
Какое-то вот обобщение вот того результата в дискретном случае,
которое просто доказывается на неперерывный случай.
А что здесь хочется сказать? Вот это существенно. Давайте рассмотрим пример.
Пример такой. Пусть у нас есть последовательность случайных величин со следующими функциями
распределения. Вот у первой случайной величины функция распределения будет вот такая.
Ну то есть вот здесь эта единичка. И вот. В общем она вот так выглядит.
А у второй случайной величины будет просто уголок острее.
То есть будет какая-то случайная величина вот такая вот с такой функцией распределения.
А третья функция распределения будет в еще более острый уголок вот таким.
И соответственно более-менее понятно, что предельная функция распределения
Это просто функция распределения константы, то есть вот такая штука.
Но вот для каждой функции вот этой у вас в 0 значение 0.
А что у предельной? По непрерывности справа здесь по идее должен быть 1.
На самом деле не совсем так, но утверждать, что есть сходимость вот в этой точке мы не можем.
То есть для всех точек непрерывности, где вот ваша итоговая функция распределения непрерывна,
у вас сходимость есть честная, а вот в точках разрыва итоговая функция распределения
вы про сходимость ничего говорить не можете.
Ну и собственно вот про это теорема Александрова и говорит.
То есть как бы сходимость по распределению как-то в терминах функции распределения выражается.
И это мы сейчас будем использовать.
Напоминаю, что мы хотели с вами обратить стрелочку.
То есть мы хотели привести какой-то частный случай, когда сходимость по распределению следует сходимость по вероятности.
Так вот, утверждение. Оно нам сегодня еще не раз пригодится.
А утверждение следующее, что если у нас последовательность случайных величин
сходится по распределению Константия, тогда она сходится и по вероятности.
Ну, стрелочка справа налево более-менее очевидна, потому что изходимости по вероятности всегда следует изходимость по распределению.
Гораздо интереснее стрелочка вправо.
Ну и здесь по сути нужно просто два раза применить теорему Александрова, который мы только что выписали.
Давайте запишем определение сходимости по вероятности.
То есть для любого Эпсилона вероятность того, что к 7 минус Константа С больше Эпсилон должно стремиться, по идее, к нулю.
Ну давайте мы это сейчас как-нибудь распишем.
По сути просто модуль раскрыли.
Вот что-то такое получается.
И что здесь хочется заметить?
Ну смотрите, вот это же по сути практически функция единичка минус функция распределения в точке С плюс Эпсилон.
А вот это практически функция распределения в точке С минус Эпсилон.
Придельная функция распределения у нас выглядит вот так.
То есть у нас предельная случайная величина это Константа, вот здесь по утверждению.
Соответственно предельная функция распределения имеет вот такой вид.
То есть про сходимость функции распределения в точке С мы ничего говорить не можем.
Но вот для всех остальных точек мы уже про сходимость что-то знаем.
Мы знаем, что какую бы точку в праве Ц вы не закрепили, она будет стремиться,
последовательность функции распределения в этой точке к С будет стремиться к единичке.
соответственно, вот это будет стремиться к нулю. А если вы влево отступите, то
предельная функция распределения равна нулю, значит и все f-ксиенты в этой
точке будут стремиться к нулю. Соответственно, вся вот эта сумма
стремится к нулю, ну и мы получили с вами, что вот это выражение сходит к нулю.
То есть мы с вами получили, что исходимость по распределению в
константе, следует исходимость по вероятности в константе. Вот. Есть вопросы?
Жаль. Так, идем дальше. Стрелочки мы с вами пообращали, теперь можно переходить к
векторной исходимости.
Так, векторная исходимость.
Ну вот, на самом деле гораздо интереснее рассматривать не исходимость случайных
величин, а исходимость случайных векторов. И вот там на самом деле такие же
виды исходимости, как и для случайных величин. Вот. Обозначение ровно такие же.
Вот. Определения немножко меняются. Ну, как меняются
определения? Здесь не так.
Вот здесь просто модуль меняется на норму, потому что мы теперь с векторами
работаем.
Так, ну и здесь у нас теперь мы к векторам применяем функции, поэтому у нас теперь
для любой f ограниченный непрерывный, f действует у нас из r в, ну давайте мы с
вами скажем, что мы живем в ммерном пространстве, потому что n это у нас
индекс у последовательности, живем в ммерном пространстве. Вот. Есть такая же
исходимость моментов.
Супер. Что с этим можно делать? Ну, пока ничего. Пока можно понять, как связана
исходимость векторов и по компонентной исходимость. То есть правда ли, что если
у вас сходятся компоненты в том или ином смысле, тогда у вас сходятся и вектора в
том же смысле. Ну и вот ответ на этот вопрос положителен для сходимости по
почти наверное и по вероятности. А для сходимости по распределению есть небольшая
проблема, из-за которой собственно у вас появилась вторая задача в контрольной
работе. Если бы этой проблемы не было, эти задачи бы не решались. Ну в смысле, они
были бы неинтересны. Итак, следующий подпараграф связь по компонентной и
векторной исходимости.
Значит, утверждение такое. Вот если у вас вектора сходится почти наверное, это
эквивалентно тому, что и каждые компоненты будут сходиться почти наверное.
Если у вас вектора сходится по вероятности, это эквивалентно также тому, что и у вас
компоненты сходятся по вероятности. Если у вас вектора сходится по распределению,
то из этого следует, что у вас компоненты сходятся по распределению. В обратную
сторону стрелочка не верна. Вот. Ну и давайте как-то вот чисто идейно обсудим
доказательства этого утверждения.
Оно не очень сложное.
Значит, для сходимости почти наверное, но это мы докажем аккуратно. То есть нам
такие омеги подходят, где сходятся
наши вектора и где сходятся наши компоненты.
Ну вот, с курсом от анализа мы знаем, что сходимость вектора эквивалентна тому,
что сходится каждого компонента, ну, в обычном смысле. А теперь, если у вас вектор
сходится почти наверное, значит вероятность вот этого множества равна единичке.
Соответственно, вот это множество вложено в каждый из них. Значит, у каждого вот
этого множества из пересечения вероятность хотя бы единичка. Ну, значит
единичка. В обратную сторону предположим, что у вас все компоненты сходятся и у
каждого вот такого множества вероятность единичка. Пересечение множества единичной
конечного числа тоже множество единичные меры. Соответственно, ну тоже доказали.
Первый пункт доказан. Пункт номер два. Так, здесь немножко поинтереснее. Здесь нужно следующее,
заметите. Значит, первое, что есть вот такая вложенность. Так, ну почему? Если у вас хотя бы одна
компонента отличается больше чем на эпсилон, то понятно, что и норма вашего вектора отличается
больше чем на эпсилон. Ну вот, от предельного. Это более-менее понятно, если вы просто вот
эту норму запишете. Это обычная евклидовая норма. То есть мы имеем сами здесь сумму кснk
минус кск в квадрате. Пока от единички даем. Соответственно, если у вас одно слагаемое хотя бы
эпсилон, то вот эта штучка тоже хотя бы эпсилон будет. Супер. То есть вот это событие вложено вот в
это. То есть если у вас вот это теперь значит, если у вас вероятность вот этих вот событий
сходится к нулю. Сейчас, одну секунду. Тут же вложение в другую сторону нужно было.
Так, друзья, подсказывайте. Или я буду думать сейчас.
А нет, ну все верно. Теперь, да... чуть затупил. Соответственно вероятность, если вот у вас
вероятность вот этих множеств по определению векторной сходимости стремится к нулю, то получается
для каждой компоненты у вас вероятность вот этих множеств тоже будетobookт средса к нулю.
Потому что у вас здесь есть вот такая монотонность. Соответственно, если у вас есть
векторная сходимость по вероятности, то у вас есть и покомпонентная сходимость по вероятности.
Потому что вот по определению векторной сходимости по вероятности вот они стремятся к нулю.
в силу вложенности, вот эта вероятность меньше, чем вероятность вот этого множества,
значит она тоже стремится к нулю. Я просто стрелочку перепутал, да.
Так, и второе вложение, которое стоит заметить, это вот такое вложение.
Вот. А почему это вложение верно? Ну, смотрите, предположим, что оно неверно, то есть у нас
модуль вектора отличается больше, чем на Эпсилон, а модуль каждой координаты, мы в объединение не
попали, значит модуль каждой координаты отличается меньше, чем на Эпсилон поделить на корень из М.
Ну, тогда если вы вот эту оценку подставите в определение нормы, то у вас здесь как раз-таки
корень из М в квадрате даст М, М слагаемых, М очко уйдет, здесь будет Эпсилон в квадрате,
просто Эпсилон. То есть мы изначально предположили, что у нас норма вектора больше, чем Эпсилон,
и мы сюда не попадаем, но при этом мы с вами получаем, что норма вектора меньше либо равна,
чем Эпсилон. Противоречие. Соответственно, у нас есть вот такое вложение. Поэтому,
если здесь у нас конечное число последовательных событий стремится к нулю, то их объединение
тоже будет стремиться к нулю и, соответственно, вот это будет стремиться к нулю. Тоже опять-таки
по монотонности, потому что здесь вот такая влажность есть. Все. Так, есть вопросы по
вот этому доказательству? Так, давай тогда первая пункт, где сходимость почти наверная. Смотри,
предположим, у нас есть сходимость вектора. Покажем, что есть сходимость компонента. Если
есть сходимость вектора, значит вероятность вот этого множества единичка. Если множество
принадлежит пересечению каких-то множеств, значит оно лежит в каждом из них. Значит,
вот это множество, ОМЕГ, принадлежит каждому вот такому событию. Нет, здесь, что каты компонента
сходятся. Окей? Вот, то есть смотри, вот это множество у тебя вложено в каждое множество из
вот этого пересечения, значит вероятность каждого вот такого множества больше либо равна чем единичка,
ну получается единичка. Окей? Так, вот здесь я перепутал просто стороны, в которые я доказывал,
но сейчас вроде все нормально. То есть вот это вложение понятно всем, да? И вот это вложение
вроде тоже более-менее понятно. Окей? Ну, могу еще раз повторить, если надо. Ну, думаю... ну все,
ладно, да. Идем дальше тогда. А, что дальше хотелось рассказать? Так, сейчас. Это мы поговорили
про теория моих следований сходимости. Это мы поговорили о связи по компонентной и векторной
сходимости. Еще мы не обсудили сходимость, связь сходимости вот в случае сходимости по распределению.
Почему стрелочка вправо верна? Ну вот, давайте это докажем. Ну, потому что мы можем с вами
рассмотреть вот такие выражения. Сейчас я запишу, потом объясню. Вот. Смотрите,
p-катая это проектор, ну получается, просто который от вектора оставляет только одну координату
вот. Вот эта функция все еще непрерывно и ограничена. Соответственно, вот эта сходимость
выполнена просто по определению сходимости по распределению для векторов. Справедливо? Ну ладно,
думаю, что справедливо. Вот. Ну и, собственно, вот если мы возьмем в качестве f непрерывно
ограниченную функцию, то композиции непрерывно ограничены непрерывно и будут непрерывно ограничены.
Соответственно, мы с вами тем самым показали, что каждая компонента сходится по распределению.
Так, это понятно? Окей. Теперь давайте приведем контрпример, почему вот здесь вот, вот здесь везде
были эквивалентности, а вот здесь вот стрелочки в обратную сторону нет. Контрпример здесь банальный.
Значит, можно рассмотреть. Соответственно, мы сейчас доказываем, что нет стрелочки вправо.
Можно рассмотреть последовательность xn, ttn. xn будет равно x, ttn будет равно theta. То есть,
грубо говоря, это последовательность, в которой на каждой позиции стоит одна и
та же случайная величина. Ну причем вот эти случайные величины x это независимые и имеют
одинаковое распределение. Вот. То есть более-менее очевидно, что xn сходится по распределению x,
просто потому что у вас каждая случайная величина в этой последовательности имеет распределение x.
Но на самом деле очевидно и вот такое вот утверждение, что ttn тоже сходится по распределению
x. Ну потому что у вас распределение x и это совпадают. Вот. Следовательно, вот предположим,
что мы составили вектор из двух таких последовательных случайных величин,
тогда по идее этот вектор сходится x, x. Но это неправда, потому что мы положили с вами независимые
случайные величины, значит вот этот случайный вектор мог как-то вот рандомно на всю плоскость
попадать. То есть куда угодно. А вот этот вектор у вас распределен на прямой y равно x. Ну и понятно,
что вот эта сходимость уже неверна. Так. Это понятно? Да, мне кажется, я просто леница начал.
Да, сейчас. Так, на секундочку. Так, какой вопрос? Что-то противоположное?
Ну да, так как это непрерывная ограниченная функция. А почему она ограниченная?
Ну вот не совсем очевидно. Ладно. В общем, давай тогда еще раз я вот пример повторю,
потому что кто-то попросил повторить. Значит, смотрите. Мы зафиксировали с вами две случайные
величины с одинаковым распределением, и они независимы изначально были. Вот просто рассмотрим
последовательность, в которую вот все компоненты это просто одна и та же случайная величина x. Вторая
последовательность, такая последовательность будет, у которой каждый компонент это. Вот. Ну вот эта
сходимость более-менее очевидна, но на самом деле вот такая сходимость тоже будет верна, просто
потому что у вас кси и это по распределению совпадают, тогда если бы мы составили из них вектор, у нас по идее должна быть
выполнена вот такая сходимость, но понятно, что такой сходимости у нас нет, потому что вот эти случайные величины как бы независимы,
ну и а вот эти уже зависимы, то есть вот эти две
распределения вот этого вектора предельного
находятся напрямую у равно х, а распределение каждой компоненты
оно могло быть где угодно на плоскости.
Вот, ладно, надеюсь, что понятно. Собственно, это мы с вами обсудили,
это мы обсудили, то есть из сходимости компонент по распределению не следует сходимость векторов по распределению,
но как всегда есть какое-то исключение и вот это исключение это Лемма Слуцкого.
Давайте обсудим с вами Лему Слуцкого.
На самом деле вот эту стрелочку можно в частном случае обернуть.
В каком? Ну, если у вас одна последовательность сходится по распределению
х, а вторая последовательность сходится по распределению константия,
тогда у вас сходимость векторов составленный из этих компонент будет.
То есть в общем случае это неверно, но если одна из компонент сходится константе, тогда вот такая сходимость векторная будет верна.
Окей.
Это замечательно.
Этим мы будем пользоваться чуть позже. Так, Лему Слуцкого я сказал.
Дальше, для векторов на самом деле
взаимосвязь сходимости такая же, как и для случайных величин, то есть все та же диаграмма коверна и сходимости почти наверно векторов следует
сходимость по вероятности векторов и следует сходимость по распределению векторов. Ну и там когда можно обращать и
контрпримеры на самом деле примерно такие же.
Вот.
Можно еще, например, интересный интересное утверждение быстро доказать. Вот такое.
Так, давайте напишу его вот на новой доске. Утверждение следующее.
Помните, что у нас в одномерном случае сходимость константе по распределению была эквивалентна сходимости константе по вероятности.
Ну вот в многомерном случае это тоже верно.
Утверждение.
Пусть у вас
ну вот вектор ксен сходится к вектору константа
по распределению, тогда у вас будет на самом деле верна и сходимость по вероятности.
Как вот это доказывать? Есть идеи?
Так.
Ну вот у вас сходимость векторов. И сходимости векторов следует сходимость компонента. То есть мы знаем, что по компонентной сходимость будет.
По распределению.
Так, только здесь опять коллизия обозначений. Давайте мы к. Вот так назовем.
Вот. Здесь у нас уже одномерные сходимости по распределению. Одномерные сходимости по распределению можно превратить в одномерные сходимости по вероятности.
Вот. Здесь у нас уже одномерные сходимости по распределению. Одномерные сходимости по распределению можно превратить в одномерные сходимости по вероятности.
Вот. А для сходимости по вероятности уже верно, что из сходимости компонент следует сходимость векторов.
То есть из вот этих вот компонент можно собрать вектор. Вот такой.
Ну в общем это будет по сути опять наш вектор.
Ксен, который будет сходится к C1 ЦК по вероятности. Вот.
То есть в одномерном случае мы это через лему Александрова доказывали. Теориям Александрова.
А сейчас мы будем использовать Л.C1 ЦК.
Вот.
То есть в одномерном случае мы это через лему Александрова доказывали, теорему Александрова.
А сейчас мы вот просто таким вот хитрым способом перешли к одномерному случаю,
а потом обратно из компонент собрали вектор и доказали это утверждение.
Супер. Это тоже может где-то пригодиться.
Так, и теперь уже почти переходим к задаче.
Обсудим еще очень важную теорему, которая называется теорема наследования исходимости.
Ну, исходимости по вероятности всегда следуют исходимость по распределению.
То есть я до этого сказал, что для векторов соотношение между исходимостьми точно такое же, как и для случайных величин.
То есть исходимости по вероятности следует исходимость по распределению.
Ну и как бы только содержаченную часть этого утверждения записал.
Конечно, можно здесь в обе стороны стрелочки нарисовать.
Так, супер. Идем дальше.
Теорема наследования исходимости.
Почти катарсис уже готов.
Так, в чем ее суть?
Вот, жили вы в одном пространстве, и была у вас исходимость в каком-то смысле.
По распределению почти, наверное, по вероятности.
И захотелось вам применить какую-нибудь непрерывную функцию.
Ну вот, мы сейчас будем говорить, что вот она просто непрерывная, либо локально непрерывная.
Что бы это ни значило.
Что бы это ни значило.
Так вот, теорема наследования исходимости утверждает, что вот в новом пространстве,
после того, как вы подействуете на вектор или случайные величины,
на последовательность случайных величин, у вас исходимость сохранится в том же смысле.
Вот. То есть у вас вектора сходились, вы применили какую-то непрерывную функцию
к тому, что слева стоит, к тому, что справа, и исходимость у вас сохранилась в том же смысле,
но уже в новом пространстве, грубо говоря.
Ну, там есть более тонкое условие, там связано с мерой единичка, короче,
но мы сейчас это опустим, у нас будут непрерывные функции.
Давайте для решения задач считать, что функция либо непрерывная, либо локально непрерывная.
То есть в окрестности какой-то точки, которая нас интересует.
Так, ну это более-менее понятно.
И теперь давайте поймем с вами, почему чаще всего в задаче номер два на контрольной работе
будет сходимость по распределению.
Ну, потому что для сходимости более сильных, то есть для сходимости по вероятности
или сходимости почти наверное, похожие задачки были бы очень простые.
Ну, то есть, предположим, у вас задача.
Давайте такая вот гипотетическая задача на контрольной.
Вот вам дали последовательность Xn, которая сходится по вероятности Qi,
вам дали последовательность Ytn, которая сходится по вероятности к Theta,
и вам дали последовательность, ну, какой-нибудь счёт Nz square,
которая сходится по вероятности X Ц.
И вас спросят, а к чему сходится X q?
вероятности z. И вас спросят, а к чему сходится ксен, там
допустим плюс ттн, поделить на ztn. Да, здесь тн должен
быть. И вот вас могут спросить, к чему вот это сходится
по вероятности. Ну, задачи ни о чем. Как такие задачи
решать? Шаг первый. Вы составляете вектор из компонент, потому
что из компонентной сходимости следует векторная сходимость.
В силу вот этого утверждения, которое на доске еще осталось,
у вас есть сходимость векторов. Ну, замечательно. А дальше
пользуемся теоремой наследования сходимости. Для функции, ну,
от трех аргументов получается x плюс y поделить на z. И все,
и получаем, что вот это все сходится кси плюс это поделить
на z. Ну, задачи ни о чем. Вот. Плюс напишу теорема
наследования сходимости. И, собственно, почему на
контрольной появляются задачи на сходимость по распределению?
Потому что для них вот первый шаг проблематичен. То есть
нельзя просто взять и составить вектор, чтобы он сходился
по распределению и потом применить к нему теорему
наследования сходимости. Поэтому там нужно что-то изобретать.
И это что-то, это дельта-метод. Ну вот, в частности, так вот,
небольшое лирическое отступление. Если мы знаем с вами теорему
о связи по компонентной сходимости и векторной сходимости, и
знаем теорему наследования сходимости, мы знаем с
вами, что, допустим, предел суммы — это сумма пределов.
Ну и для произведений то же самое. То есть, в принципе,
все какие-то базовые свойства очень легко уводятся с учетом
вот этих двух фактов. Для всех сходимости, кроме сходимости
по распределению. Ключевая проблема там в том, что мы
не можем составить вектор из компонент, чтобы он сходился.
Вот. Давайте я хорошо доску помою, а то что-то грязно
осталось. Как у вас настроение? А чего так грустно? Ладно,
некоторые вопросы должны остаться без ответа. Пока.
Так, идем дальше. Дельте метод. Собственно, чтобы решать
задачки, которые, скорее всего, у вас будут в листочке,
нужно уметь пользоваться дельте методом. Давайте мы
сформулируем и аккуратно докажем. Пусть у вас есть сходимость
по распределению откуда-то и пусть у вас есть числовая
последовательность, которая сходится к нулю. Пусть у
вас также есть какая-то точка А. И пусть у вас есть
H-непрерывная функция. Даже так, она будет дифференцируемая
в точке А. Тогда верно следующее. Ну вот, такая сходимость
верна. Давайте я ее аккуратно на одну доску постараюсь
уместить. Кси на производную точку А. Какая дельта? А,
ну дельта метода, потому что маленькие превращения.
Потому что, по сути, смотрите, здесь же у вас практически
производная записана. Как бы это, грубо говоря, вероятностная
производная какая-то. Вот, поэтому это дельта метод.
Дельта, потому что маленькие превращения. Да, сходимость
по распределению будет. Вот как раз-таки это инструмент,
при помощи которого мы будем решать задачи. Вот, обсудим
как решать дальше. Сначала давайте докажем и поймем,
что вот эта штука действительно работает, а потом попробуем
применить ее в задачке. Вот, ну доказательство очень
простое. Первое, мы можем функцию, в силу того, что
она дифференцируема в какой-то окрестности, представить
в следующем виде по формуле Тейлора. Вот в таком виде,
например. Ну там, равенство писать не совсем корректно,
но какой-то эквивалентный писать. Вот. Ну и дальше
можем вести вспомогательную функцию. Давайте это я так
как-нибудь на пальцах быстро объясню. Значит, если х...
Если х не равен нулю, то вот такое выражение, а если
х равен нулю, то вот, определим по непрерывности производной.
Ну вот, функция будет локально непрерывна в окрестности
точки А. Соответственно, чтобы получить вот это вот
утверждение, что нужно заметить? Ну первое. Так, это у нас
наверное был первый шаг. Вот это второй шаг. Давайте
третий шаг. Ксиенбен сходится к нулю. Почему? По распределению.
Почему она сходится к нулю? Ну потому что у нас мы
пользуемся по сути леммой Судского. У вас bn сходится
к константе, ксин сходится кси по распределению. Что-то
одно сходится к константе, а второе сходится по распределению.
Значит, мы можем перемножать, допустим. И все аккуратно
получается. А дальше мы просто применяем непрерывную
функцию, то есть пользуемся теоремой наследования
сходимости. И у нас получается что? Ну, пользуемся определением
h от x, то есть у нас h от ксиенбен сходится к h от нуля. Вот. А что, например, из этого
следует? Если мы распишем вот это вот определение,
которое у нас есть, плюс ксиенбен, плюс h от a поделить на ксиенбен,
сходится к h от нуля, h от нуля у нас определена как h3t. Окей? Ну, практически то, что нам
нужно, с учетом того, что у нас только правда ксиен
здесь знаменателя, но смотрите, здесь у нас есть сходимость
константе по распределению, значит, мы можем домножить
на ксиен, сходящийся кси. Соответственно, кси у вас
здесь уйдет, а здесь у вас появится кси в пределе.
То есть у вас пользовались дважды леммой Слуцкого и
те реммой о наследовании сходимости. Окей? Справедливо.
Доказали. Еще вот такое небольшое замечание,
смотрите, иногда в задачах встречается дельта метод
второго порядка, то есть что делать, если у вас вдруг
у функций, которые вы собираетесь применять, вот эта производная
нулевая? Ну, на самом деле, нужно просто ряд Тейлора
разложить до второго порядка, вот это слагаемого за нулица
и, по сути, записать дельта метод для второго порядка.
Окей? Ну, то есть смотрите, если вот функция, которую
вы применяете имеет первую производную нулевую в точке,
такое возможно, то у вас будет сходимость к нулю,
а это может быть неправда, потому что вы, может быть,
не достаточно порядок малости рассмотрели, может быть,
нужно было до второй производной раскладывать формул Тейлора.
Тогда вы просто раскладываете до второй производной,
вот, там у вас получается будет плюс х квадрат пополам
аш-два штрихата, вот. Ну и тогда у вас, по сути, вот
это если за нулица, вот это можно перенести влево,
поделить на х квадрати, и, в принципе, у вас получится
что-то похожее. Давайте я запишу тогда аккуратно,
потому что это может вам пригодиться. В общем, если
мы проделаем все абсолютно так же, то мы получим с
вами дельт метод второго порядка. Давайте тогда мы
сейчас это доказательство переделаем теперь для второго
порядка. То есть, предположим, что у нас первый производный
за нулилась, но нам не повезло, и функция дважды дифференцируема.
Тогда у нас, по сути, теперь вот эта слагаемая здесь
доминирующая, мы аш от х теперь определяем похожим
образом, только делим на х квадрате, а здесь определяем
как аш-два штриха пополам. Да? Так же опять пользуемся
леммой Слуцкого, применяем вот к этой функции, получаем
вот такую сходимость, и в итоге записываем определение,
здесь у нас будет кси-н в квадрате, б-н в квадрате.
Окей. И все это сходится теперь ко второй производной
пополам. Вот. Ну и, по сути, осталось только
домножить на кси-н в квадрате, которая сходится кси в квадрате.
Вот эта сходимость верна, потому что у нас кси-н сходится
кси по распределению, применяем функцию квадрата, вот такая
сходимость будет выполнена, домножим обе части, ну левую
часть на кси-н в квадрате, правую часть на кси в квадрате,
и у вас получится вот такое утверждение. То есть давайте
я выпишу его. То есть вот это дельта-метод, а дельта-метод
второго порядка.
Дельта-метод второго порядка записывается следующим
образом. Значит, пусть у нас есть сходимость по распределению,
пусть у нас есть последовательность b-n, которая сходится к нулю,
пусть у нас есть точка a, и h дважды дифференцируемо
в окрестности точка a. Ну, точка. И что мы еще потребуем?
А, ну мы потребуем, чтобы первая производная была равна
нулю в точке a. Вот. Соответственно тогда, тогда верна сходится
на следующую сходимость. h от a плюс кси-н b-н, минус h от
a, поделить на b-n в квадрате, сходится кси в квадрате
на h дважды триха в точке a пополам. Да, ну вот ровно то же самое,
что и было раньше. Вот. Это дельта-метод второго порядка,
иногда может встретиться. Сейчас поймем, как определить,
что скорее всего дельта-метод второго порядка. Вот. Но обычно
дельта-метод первого порядка вполне достаточно для решения
задач. Вот. Ну и собственно мы готовы решить задачу.
Ну, да. Надо только понять, какую функцию применять.
Сейчас поймем. Ну, нужно просто понять, какая функция,
к чему применяем, и поймем, что если в этой точке ноль,
тогда да. Тогда действительно дельта-метод второго порядка.
Итак, это задача номер два. Задача номер два. Смотрите.
Перед задачей два хочется сказать, вот, а откуда вообще
брать вот эти сходимости, которые нам нужны для решения задач?
Ну вот, основными источниками сходимости у нас на самом деле
является ЗБЧ. Ну, ЗБЧ давайте так. СН на Н сходится к
мотожиданию КСИ. Вот. Почти, наверное. Если мотожидание
КСИ конечное. И второй источник сходимости у нас, вот,
сходимости по распределению, это вообще на самом деле
единственный источник, это ЦПТ. Ну, можем записать
это в следующей форме. Вот. Ну, давайте так, для краткости.
СН это сумма ксиитых, по и от 1 до n. А х средняя это СН
поделить на n. Ну, то есть какие-то базовые предельные
теоремы, которые у нас в курсе есть и которые по сути являются
источниками сходимости каких-то. Тогда на самом деле все задачи
на слабую сходимость, это вот просто взять из источника
слабой сходимости, то есть из ЦПТ, сходимость и применить
к ней дельта метод. Вот. Ну, возможно, если будет что-то
сложнее, то можно там Лему Слуцкого применить, что-то
еще там, короче, как-нибудь это повертеть. Но в общем
случае у вас есть ЦПТ, который дает вам сходимость
по распределению. И если у вас есть сходимость по распределению,
дальше вы пользуетесь как раз таки уже дельтам методам,
которые мы с вами обсудили. Вот. Ну и, соответственно,
задача два. Я уже написал. Так, условия второй задачи.
Пусть у нас случайная величина ХН. Так, независимые, одинаково
распределенные, имеют экспоненциальное распределение
с параметром тета. Вот, соответственно, X среднее
это SN на N. Необходимо найти. К чему сходится вот такая
штука. Собственно, в этом заключается задача. Вот.
Ну, скорее всего, короче, похожие задачи будут задачи
на дельтаметод. И, в принципе, они все решаются очень
однотипно. Вы полагаете, вот, чтобы воспользоваться
дельтаметодом первого порядка, здесь будет дельтамет
первого порядка, нам нужно определить вот все, что в
условии есть. То есть какую-то последовательность, чаще
всего последовательность, это 1 на корень из N, сходящуюся
к нулю. Определить точку. Ну, вот эта точка, это, сейчас
поймем какая, это 1 на тета. Немножко в другом порядке
определяю, сейчас будет понятно, почему это 1 на тета.
Нам нужна сходимость по распределению какая-то. Ну,
сходимость по распределению мы с вами умеем брать только
из CPT, по сути. Из CPT у нас следует, что корень из N,
х средняя, минус математическое ожидание экспоненциальной
случайной величины, это 1 на тета, сходится к нормальному
закону с параметрами 0, 1 на тетов квадрате.
Сейчас, сейчас скажу. Так, это вот так, так. Значит,
смотрите, мы определили последовательность, в таких задачах
часть всего 1 на корень из N. Определили, положили
какую-то сходимость по распределению. Вот мы ее
взяли из CPT для тех независимо, одинаково распределенных
случайных величин, которые нам даны в условиях. Вот
она. А как подобрать точку? Ну, точка, вот это обычно,
то, что в CPT справа. Ну, мы сейчас поймем, почему. То
есть, у этого есть, конечно, глубинный смысл, но вот давайте
пока считать, что это вот то, что вот тут справа в CPT
написано. И нам чего-то еще не хватает, нам не хватает
функции. Ну, вот функция обычно это то вот, то преобразование,
которое вот то, что с левой или с правой переводит в то,
что с левой или с правой здесь, соответственно. То
есть, функция в нашем случае, это 1 поделить, давайте
какую-нибудь букву новую придумаем, M, например. 1
поделить на M в квадрате. Вот, то есть, сейчас, возможно,
это как магия выглядит, что я все подобрал типа на
угад, но на самом деле это более-менее понятный извод
того, что от нас требуется. Ну, и давайте просто применим
дельта-метод. Мы с вами дельта-метод уже доказали.
Так, давайте еще раз выпишу дельта-метод.
Вот, без условий.
Вот. Ну, и давайте просто применять те бенты, ашки
и последовательности, которые мы с вами определили.
Ну, что мы с вами имеем? Во-первых, давайте вот начнем
с самой глубокой уровней вложенности. Ксиен умножим
на bn. То есть, ксиен это у нас вот эта сходимость
по распределению. Если мы ее умножим на bn, то у нас
вот этот корень из n сократится. Дальше. Мы добавляем точку
a. Ну, собственно, поэтому мы в качестве точки a положили
вот это, потому что когда мы домножим на bn, у нас корень
из n ушел, остается и к средней минус 1 на θ, и как раз мы
добавляем 1 на θ вот здесь, и у вас корень из n ушел,
корень из n ушел за счет bn, вот это ушло за счет того,
что мы точку a добавили с вами. Осталось только х средняя.
И к х среднему, вот здесь, мы применяем функцию 1 поделить
на m в квадрате. То есть, по сути, переворачиваем
эту штуку. И вот эта слагаемая все превращается в 1 поделить
на х средняя в квадрате. Затем. Минус h от a. h в точке a это
просто получается у нас будет 5 в квадрате. Затем мы
это делим на bn. Последность bn у нас 1 на корень из n, ну
значит соответственно домножаем на корень из n. Ура! В левой
части мы получили ровно то, что от нас требовалось
по условию задачи. Вот. Соответственно, то, что будет
справа, нам дает дельтаметод. Из дельтаметода следует,
что справа у нас здесь будет исходная предельная
случайная величина. Исходная предельная случайная
величина, это у нас, вот это нормальная случайная
величина. Домноженная на производную в точке a.
Производная 1 на m в квадрате это у нас минус 2 на m в
кубе, видимо. Так, минус 2. Ну да. Вот такое. И
подставляем вместо m 1 на тета. Соответственно, здесь
у нас будет тета в третьей в числителе. То есть вот
это все мы домножаем на 2 тета в третьей. Вот. Ну и
хочется понять, какой предел будет. Мы можем вот эту
константу занести, ну, домножить вот эту нормальную
случайную величину на константу. И тогда мы с вами
получим нормальное распределение с параметрами 0, а
4 тета в четвертый. Справедливо? Вот. Сейчас. Только у меня,
по-моему, другой коэффициент получился. Сейчас я быстро
посмотрю свое решение. Минус никак не влияет, потому
что у вас вот нормальная случайная величина, она
симметрична относительно нуля. Поэтому домножай ее
на минус, не домножай, она не изменится.
Еще раз. Смотрите, если у вас кси имеет нормальное
распределение с параметрами a sigma квадрат, то вот альфа
кси плюс бета у вас имеет нормальное распределение
с параметрами альфа, а плюс бета, а дисперсия просто
умножается на альфа в квадрате. Ну, то есть, если ты
сдвигаешь случайную величину, у нее среднее меняется.
Если ты ее домножаешь, то у нее домножается среднее,
а дисперсия на квадрат. Вот. Это можно из характеристических
функций, например, показать.
Да. Ну, то есть, если у вас среднее находилось
в точке 5, если вы домножите такую случайную величину
на 10, у вас теперь среднее будет в точке 50 находиться.
Ну, более-менее это логично. А, так. Смотрите, не совсем
вот это не очевидно, да? Можно просто вспомнить линейное
преобразование характеристической функции. Ну, то есть, характеристическая
функция линейного преобразования. Так, почему у нас ответ
такой получился? Да вопрос? Или мы на него ответили?
Да, сейчас я посмотрю. А, нет, 4 тета в четвертую
у меня тоже получилось. Я, правда, это в 6 утра решал.
Вроде тоже не ошибся. Да. Вот это как получилось?
Да. Да, да, все верно. Давайте аккуратно еще раз.
Смотрим. Ну, вот, во-первых, ксен сходится к си. Вот у нас
сходимость из CPT. Вот эта сходимость. Далее. Мы домножаем
ее на bn. По сути, у нас теперь корень из n отсюда уходит.
Ну, ксен bn это просто будет вот это без корня из n.
Будет вот эта штучка. Мы добавили a, точку a мы положили
1 на тета. Соответственно, вот это убралось. Осталось
только x среднее. И теперь мы к x среднему вот здесь
внутри применили функцию h. Функцию h у нас это 1 на
m в квадрате. Справедливо? Да. Ну, смотри, вот это,
это ксен, которая сходится по распределению кси. То
есть вот это это кси, вот это это ксен. Да. Что мы делаем
с ксен? Мы домножаем на bn. Bn мы как раз положили 1
на корень из n. Корень из n сократилась. Добавили
константу, вот это ушло. Применили h, получилось. Все,
задачку мы решили. Если бы у нас был не корень из
n от m, то мы тогда брали bn как 1 на n. Мы брали bn как
1 на корень из n, потому что у нас множество другого
корня из n. Да. В общем, вот здесь, сейчас давайте
тогда небольшое. Так, есть ли вопросы по решению вот
этой задачи? То есть она вот правда в две строчки
решилась. Мы просто определили с вами нужные все компоненты
для того, чтобы применить дельта метод, и применили
дельта метод. Больше мы ничего с веркости основания
сделали. Нет. Ответ нет. Нужно ли брать в качестве
bn что-то другое? Чаще всего в таких задачах всегда берется
1 на корень из n. Вот у нас в домашке была задача, сейчас
покажу какая. У нас в домашке была задача, ну там в листочке.
В листочке у нас была задачка такого вида. n косинус там
от чего-то там, минус единица, сходится по распределению
к чему-то там. А ну вот понять к чему. Вот такая задачка
у нас была дома. В чем здесь фишка? Здесь фишка в том,
что это просто дельта метод второго порядка. Почему?
Ну потому что на самом деле косинус у вас там точка
ноль рассматривается, и у вас первая производная
косинуса в нуле будет нолик. Ну то есть это будет sin x,
в точке ноль это будет ноль. Производная косинус
минус sin в нуле ноль. Вот. И вы помните вот как раз дельта
метод второго порядка, у нас вот здесь в знаменателе
bn в квадрате выскакивает. И соответственно вот этот
множитель n просто появился от того, что мы в качестве
bn все также взяли 1 поделить на корень из n, но мы просто
видите вот здесь вот делим теперь на bn в квадрате.
Поэтому у вас вот здесь вот n в квадрате появляется,
ну просто n появляется. То есть в принципе еще раз,
план решения таких задач всегда такой. bn всегда 1
поделить на корень из n. В точка a берется просто вот
правая часть из CPT. Ну по сути точка a это математическое
задание вот тех случайных личин, для которых CPT для
которых вы рассматриваете. А xn сходится по распределению
x. Вот это просто берется из CPT непосредственно. Вот.
А в качестве функции берется то, что ну вот, вот здесь
бы cos мы взяли. Ну вот. Это очевидно из того, что у вас
написано в левой части. Вот. Ну и дальше вы просто
смотрите, какая функция скорее всего нужно применить.
Если у этой функции вот в этой точке вдруг окажется
первая производная нулевая, то вы применяете дельтамет
с второго порядка. А если нет, то вам первого будет
достаточно. И все. Вот эта задача просто была на дельтамет
с второго порядка. Так. Остались ли вопросы какие-то
по сходимостим? То есть еще раз, основная идея в том,
что у нас есть сходимость компонентная, из CPT или
из ABG, вот откуда-то мы можем их брать. Затем, если мы
работаем со сходимостьми сильными, например, по вероятности
или почти наверное, то мы можем стакать из таких
по компонентной сходимости сходимости векторов, а потом
к ним применять непрерывные функции и получать нужные
нам сходимости. С сходимостью по распределению такое
не работает. С сходимостью по распределению нужно
хитрить, и для этого вот есть дельтаметод. А как
пользоваться дельтаметодом? Ну вот. Нужно подобрать
правильно bn, точку a, xn и h. И все правильно в правильной
последовательности применить. И по идее тогда такие задачки
решаются. Возможно, потом это нужно еще подтюнить,
воспользовавшись еще раз леммой Слуцкого, ну например,
на что-нибудь домножить. То есть сейчас у вас будет
сходиться в случайной величине, можно это домножить
на какую-нибудь константу или на какую-нибудь случайную
величину, которая сходится к константе. Ну, на последовательные
случайные величины, которые сходятся
к констanте. То есть возможно еще после
применения дельтаметодом нужно будет это чуть
доделать при помощи ну вот, леммы Слуцкого, например.
Но в целом, идея решения такая вот. Берем дельтаметод,
Берем дальнейший метод, применяем и готово.
База.
Так, ну ладно, идем дальше.
А сколько времени?
Не-не-не.
Без перерывов сегодня.
Ну, сейчас будет интересная тема, по встрече пойдет.
А потом будет очень интересная тема.
А?
А потом, я надеюсь, мы уже спать ляжем.
Условные математические ожидания.
Доживем.
У меня пока что рекорд был пятичасовой дополнительный семинар.
Если вы хотите, вы готовы убить рекорды.
Да, не, ну...
Сколько времени осталось?
Ну, за это как?
Ещё хватит девять минут, полчаса.
Ну, дальше будем записывать, значит, стенографией займемся.
Можно зарисовывать будет доску.
Так, ну ладно, вопрос такой.
По векторам более-менее понятно, что происходит?
Ну, то есть, у вас есть одномерные исходимости какие-то, они как-то между собой связаны.
Есть какие-то всякие обходные пути, когда там у вас и слабые исходимости следуют сильные.
Это всё можно как-то применять, там теория моноследования исходимости пользоваться.
И вот квинтэссенция – это дельта-метод.
Универсальная штука, которая позволяет решать задачу.
Так, ну давайте, тема 3 – гауссовские векторы.
Смотрите, гауссовские...
Ну вот, в одномерном случае нормальные случайные величины играют ключевую роль.
Хотя...
Перед гауссовскими векторами нужно ещё кое-что сказать.
Извините, что я вас... перебил.
Я сейчас вспомнил, что я очень интересную вещь ещё хотел рассказать.
Значит, смотрите, у нас была центральная предельная теорема.
Ещё раз обсудим.
То есть, чтобы CPT выполнялось, вам нужно, чтобы у вас была последовательность
независимо от одинаково распределённых случайных величин с конечной дисперсией.
Тогда у вас выполнена CPT.
Вот, мы в такой форме её запишем.
Вот.
Ещё из таких заинтересных предельных теорем у нас есть УЗБЧ, который мы уже тоже обсуждали.
То есть, если математическое ожидание кси Катова меньше бесконечности,
и кси Н это у нас независимо от одинаково распределённых случайных величин,
тогда СН на Н у нас сходится к математическому ожиданию кси.
Первого, например.
И УЗБЧ у нас ещё был, это вот слабый закон больших чисел.
Ну, мы знаем, например, с вами в форме Чубышова,
что если у вас математическое ожидание конечно...
Ну, давайте тоже дисперсия будет конечна.
Тогда вот выполнена слабая УЗБЧ в форме Чубышова.
Но по сути, результат ровно такой же, только сходимость не почти наверная, как в УЗБЧ.
А по мере.
Вот.
И просто хочется ещё пару каких-то замечаний сказать о том, как вот эти предельные теоремы между собой связаны.
Вот что хочется сказать.
Первое.
Я это говорил на своём семинаре.
Из СПТ очевидно, что следует УЗБЧ.
Ну, более-менее понятно.
Чтобы было выполнено СПТ, вам нужна конечная дисперсия,
ну и тут у вас тоже слабый УЗБЧ, у вас тоже конечная дисперсия.
Это можно показать непосредственно.
Ну, каким образом?
То есть вот у вас...
Давайте запишем СПТ.
И применим лему Слуцкого.
Домножим на 1, поделить на корень и зе на обе части.
Если мы это сделаем, то у нас справа будет нолик,
слева у нас будет и к среднему минусу от ожидания ОКСИ стремиться к нулю по распределению.
Но сходимость константия по распределению и по вероятности эквивалентна.
Справедливо?
Всё.
То есть мы с вами только что показали, что СПТ следует УЗБЧ.
На самом деле, вот здесь хочется ещё немножко к характеристическим функциям вернуться.
А характеристические функции – это очень мощный инструмент.
И вот кто-то спрашивал, зачем они нужны.
В основном характеристические функции нужны для того, чтобы всякие предельные законы доказывать.
Давайте...
Вот мы знаем с вами доказательства слабого закона больших чисел через неравенство ЧБШО,
мы это все делали.
Но на самом деле ЗБЧ можно доказать и через характеристические функции.
Нет, нет.
Кто-то спрашивал больше.
То есть какой-то теор...
Ну, не даже не теор, а просто...
Это просто удобный инструмент.
Да.
Помогает доказывать сходимости, помогает какие-то штуки проще доказывать.
То есть по сути, ещё раз, идея такая, чтобы в исходном пространстве делать преобразование фурье,
там что-то доказываете, в силу теоремы единственности возвращаетесь обратно.
Соответственно, вы доказали для исходных случайных лечений.
Отлично.
Так, ну...
Давайте ЗБЧ аккуратно докажем через Хар функции.
Смотрите.
Как мы уже с вами обсуждали, у характеристических функций обычно рассматриваются производные.
Ну и на самом деле характеристическую функцию можно просто разложить в ряд Тейлора,
так как это экспоненты какая-то обычная.
И если у вас есть математическое ожидание дисперсия,
то на самом деле ваша характеристическая функция представима в таком виде.
Вот.
Ну это по сути просто разложение экспонента в ряд Тейлора.
Вот производные, а вот там типа точка Т, Т квадрата и так далее.
Вот.
Соответственно, давайте попробуем с вами доказать ЗБЧ через характеристические функции.
Так, значит, давайте сначала сформулируем теоремы непрерывности.
Теорема о непрерывности связывает сходимость по распределению и сходимость характеристических функций.
Значит, у теоремы о непрерывности, по сути, два пункта.
Первый пункт, что если у вас последовательность случайных величин сходится по распределению,
тогда у вас и характеристические функции соответствующие сходятся в каждой точке.
Это, думаю, более-менее понятно.
Вот.
В обратную сторону на самом деле тоже верно.
То есть, если у вас есть последовательность каких-то функций характеристических,
и они сходятся к какой-то функции фиат Т,
если фиат Т непрерывно в нуле,
то фиат Т – это характеристическая функция предельной случайной величины вот этих ксенок.
То есть то ксен сходится ксим по распределению.
В некотором смысле сходимость по распределению задается сходимостью характеристических функций.
Практически в обе стороны это верно,
здесь вот с небольшой оговорочкой, что предельная функция должна быть непрерывна в нуле.
Если это так, то этоothо эквивалентное, как бы, два эквивалента условия.
То есть сходимость по распределению можно сдавать как сходимость в силу теоремы Александровой
сходимость функций распределения в каждой точке,
можно задавать через характеристические функции, сходимости характеристических
функций в каждой точке.
Вот.
Ну и собственно давайте с учетом этого докажем
все-таки уже за быча.
Значит, что у нас за быча есть?
Ну пусть у нас есть какие-нибудь случайные величины, которые
просто...
Ну пусть будут независимые и имеют конечное математическое
ожидание и дисперсия.
Тогда у ксен есть характеристическая функция.
У каждого слагайма, у каждой ксенки они одинаково распределены.
Вот.
И у них есть какая-то характеристическая функция, которую вот в силу такого
разложения можно разложить вот до первого порядка.
Что-нибудь вот такое, да?
Дальше.
Давайте попробуем с вами посчитать характеристическую
функцию sn на n.
Характеристическая функция линейного преобразования
это просто характеристическая функция sn в точке t поделить
на n.
Вот.
А характеристическая функция sn, так как у нас эти случайные
величины считаются что независимы, то это просто
произведение тех характеристических функций, которые у нас выше
записаны.
То есть это будет единичка плюс иt математическое ожидание
кси в степени n, ну вот так, в общем.
Ну а это замечательный предел.
Это просто е в степени иt математического ожидания
кси.
А это характеристическая функция константы.
Мы это уже с вами сегодня показывали.
Если у вас характеристическая функция итt, то это характеристическая
функция константы.
Таким образом, sn на n в силу теоремы непрерывности
у вас сходится по распределению константии математической
ожидания кси.
Ну а сходимость по распределению константии эквивалентна
сходимости к константе по вероятности.
Это мы с вами тоже сегодня доказывали.
Ну вот всё.
Таким образом мы с вами доказали ЗБЧ через характеристические
функции.
Вот.
Ну и на самом деле идея доказательства ЦПТ точно
такая же.
Вы раскладываете вашу характеристическую функцию до второго порядка.
Дальше.
Обратите внимание, что мы там нормируем случайные
величины.
Мы из СН вычитаем математические ожидания.
По сути мы вот это зануляем.
У нас остаются только вот эти слагаемые.
И опять-таки пользуемся замечательным пределом.
У нас всё вот это, чудо, вот похожее чудо, будет
сходиться к Е в степени минус Т в квадрате пополам.
Ну а это характеристическая функция стандарта нормальной
случайной величины.
Ну и всё, собственно.
Всё доказательство ЦПТ в том, чтобы отнормировать
случайные величины, разложить их до второго порядка, применить
второй замечательный предел и получить, что предельная
функция у вас вот такая.
А это у вас характеристическая функция для нормальной
случайной величины.
ЦПТ доказано.
ЗБЧ вот тоже через Х функцию доказали.
Ну вот у ЗБЧ только доказать через Х функции нельзя,
потому что там почти наверная сходимость, а она уже никак
не связана с сходимостью по распределению.
Вот.
Так, теперь про предельные теоремы вроде как всё.
Теперь действительно можно переходить к гауссовским
векторам.
Давайте, может, правда, отдохнём две минуты.
Надо?
Ну или здесь станет меньше.
Я не боюсь.
Однажды на мой концерт придёт один человек.
Не, это песня просто есть такая.
Я взгляну ему в глаза.
А дальше я забыл слова.

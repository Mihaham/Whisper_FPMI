Итак, всем привет.
Моя тема называется синтез музыкальных фраз и мелодий
генеративными алгоритмами.
Вообще область создания генеративной музыки достаточно
молодая, и в ней нет каких-то серьезных солидных достижений,
которые можно было бы пустить в массы.
Пока все, что создается, звучит не очень вкусно,
и пока что вот так вот.
Но недавние достижения в области генерации картинок,
лиц способствуют, расширяют возможность генерации
музыки, но создание настраиваемого, полного, красивого мелодии
является еще достаточно тяжелой задачей.
Есть несколько основных методов в области генерации
музыки, и один из них является использование генеративно-соседательных
сетей.
Поэтому целью моей работы является попробовать применить
эти ганы в области создания музыки и получить какую-то
приятную мелодию.
Наверное, вы все знаете, как работают эти генеративно-соседательные
сети.
Она состоит из двух нейронных сетей.
Одна из них генерирует стэмпл, а вторая пытается
предугадать, является он реальным или генерированным.
И так они друг друга тренируют.
Если понаблюдать, как музыканты создают песни, они чаще
всего пользуются программами работы с midi-файлами.
Это всегда небольшое число хорошо сочетаемых звуков,
немного баса.
На бас я пока не зарекаюсь.
Поэтому мое предложение подать на вход генератору
заранее выбранный, возможно, достаточно большой набор
тех звуков, которые станут базовыми в генерируемых
мелодиях.
И на выходе я должен получить хорошо звучащий мелодию.
Вот такие мои ожидания.
Наверное, это все.
Я примерно представляю, как ганн генерируется лица,
например.
Но у меня сейчас такая мысль, что лица будут более
однородны, что ли.
Массив данных гораздо, чем сэмпл.
Как вообще будет бороться с этим?
Откуда взять хорошую базу данных сэмплов, чтобы они
были условно как-то примерно?
Вообще существуют базы данных midi-файлов.
Само себе midi-файл, там может быть что угодно.
Совершенно неприятные.
Существуют базы хороших уже готовых мелодий.
И если пробовать применить.
То, что я видел, эти примеры обычно связаны с какими-то
классическими инструментами.
Я, наверное, хотел бы, во-первых, попробовать сделать
мелодии в другом стиле, не использовать классические
инструменты, а использовать какие-то синтетические
звуки, которые популярны в каких-то поп, хайпер поп
треках.
Наверное, так.
Но ты же не можешь просто там послушать и сказать,
о, у меня какие-то слухи с кем-то остальные.
Какой-то кейсик почитать.
То, что я читал, честно говоря, предлагается все-таки
оценивание человеком.
Метрикой хорошего стрека будет то, что моя сеть уловит
то, что надо повторять какие-то части, то есть
будут какие-то повторяющиеся куски.
Ну, будет мелодия с какой-то, ну, какой-то явной структурой.
Ну, как-то так.
Всем привет, я Алиан Алиев и тема моей работы
«Афинность взаимодействия протеинов».
Вот.
В целом мы изучаем протеиновые комплексы и их взаимодействия,
где мерой силы взаимодействия мы будем рассматривать
константу диссоциации.
Наша цель – это вывести модель для предсказания
этой самой константа.
Чтобы чуть подробнее разобраться, как это все выглядит,
можно обратиться к картинке.
Это обычный пример протеинного комплекса.
И мы считаем, что он состоит из двух протеинов,
которые, собственно, взаимодействуют друг с другом,
где каждый протеин – это набор каких-то цепочек.
Цепочка состоит, в свою очередь, из атомов,
и эти атомы образуют некоторые аминокислоты,
их там примерно 20 штук, вообще известные.
Истресноатом – это просто точка в трехмерном пространстве.
То есть, если подытожить, вся наша фигура – это просто
какой-то набор точек в 3D.
Здесь вы можете видеть список литературы,
которым я вдохновлялся в процессе.
Еще немного поговорим про наш датасет.
Основная проблема с данными в том,
что не до конца понятно, какие именно атомы
или даже цепочки взаимодействуют друг с другом.
И это тоже одна из проблем,
которую нам предстоит решить.
В частности, самый интуитивно-логичный подход,
который мы применяем в данный момент –
это сказать, что атомы взаимодействующие,
если они расположены близко друг к другу,
а, соответственно, цепочки назовем взаимодействующими
и будем рассматривать дальше их,
если у них как можно больше взаимодействующих пар атомов.
Продолжим отсюда.
Мы будем предсказывать таргет константа диссоциации,
логарифм константа диссоциации,
потому что она визуально похожа на нормальную,
поэтому очень приятная величина для предсказания.
И будем в частности использовать метрику МСЕ
и batch accuracy.
То есть мы разобьём вот этот вот логарифм
на бачи примерно равномерные
и будем считать точность уже на вот этих вот категориальных таргетах.
Перед нами теперь встает проблема,
а как же вот эту всю большую фигуру,
которая просто набор точек в трёхмерии,
представить в каком-то адекватном виде.
И мы предлагаем два пути.
Первый из них – это сказать,
что вот мы выбрали две цепочки взаимодействующих.
Взять центр масс всех взаимодействующих пар атомов,
сказать, что это центр взаимодействия.
Дальше отложить в каждую сторону небольшое расстояние.
Мы рассматриваем примерно 10-20 angstroms.
Angstrom – это 10-10 метра.
И сказать, что теперь у нас есть кубик 20 на 20 на 20.
Все атомы, которые уже попали, мы учтём,
а остальные нас не так уж и интересуют.
Таким образом мы получим вот этот кубик,
и его уже можно скормить любой КНН-модели.
Второй подход, крайне различающийся, –
это скормить что-нибудь гафом, нейронком.
В частности, у нас есть такой вариант
сказать, что набор вершин – это все атомы,
и мы проводим ориентированное ребро из итового атома в житый,
если житый попал в ка ближайших соседей к итому.
Тут какие-то графики обучения наших моделей,
которые в целом пока сделаны на искусственных данных,
более-менее, поэтому не очень нас интересует.
Тут тоже какие-то метрики.
И вывод, что наше достижение в том,
что мы представили два новых взгляда на проблему
и провели какой-то эксперимент, все посчитали, поведи.
Ну вот, к Майе планирую это все доделать на всех данных, как минимум,
и планирую сделать какой-то более подробный вывод.
Они не совсем искусственные, я бы сказал, что они неполные.
Ну и в частности у нас есть еще несколько идей.
Например, можно подробнее осмотреть графовый подход,
можно, например, рассматривать как вершины не только атомы,
но и цепочки, брать их в центр масс,
можно ребра проводить не только как КНН,
а, например, ставить какой-то trash-hold один на все,
можно также брать диаграмму Вороновой
и проводить ребра руководствуясь ей.
В общем, простор для воображения есть.
Минимизируем функцию риска, представляемого в виде суммы,
соответственно, f, i, t, h, n, l, d, n,
где n мы берем достаточно большим.
В данном случае наша работа исследуется для функции риска конкретного вида,
вы можете видеть на слайде.
Естественно, существуют различные алгоритмы поиска минимум этих функций,
алгоритмы первого порядка имеют особенность в том,
что они зависят от...
Так, да.
Я понял.
Что они были выкопаны,
что они были выкопаны,
что они были выкопаны,
что они были выкопаны,
я понял.
Что они в определенных сетках
имеют плохую скорость сходимости,
то есть, например, при большом отношении
между собственным значением и матрицей
а транспонированная а
с алгоритмом могут сходиться очень плохо.
Поэтому используются методы Ньютона второго порядка,
соответственно, зависят от матрицы Гесиана.
Тем не менее, в случае большого Н,
если использовать обычный нестахотический алгоритм,
на каждой тарасе нам придется вычислять Н Гесианов,
что достаточно долго и сложно.
Поэтому хотелось бы использовать стахотический алгоритм,
в отличие от методов первого порядка,
он не будет сходиться.
То есть, за счет того, что метода второго порядка
сходится очень быстро к функции,
в случае использования бачей,
то есть, выбора какого-то подможества функции F
и сходения к максимуму конкретному подможеству,
мы будем сходиться к...
очень быстро сходиться к оптимуму конкретно этих функций,
из-за чего оптимум глобальной суммы будет находиться очень плохо.
Поэтому используется мотивикация алгоритма,
использующая память.
То есть, мы не...
Каждый шаг стахотического алгоритма
мы не выбираем какое-то подможество
и переходим к...
и сдвигаемся в сторону оптимума,
а сдвигаемся лишь с каким-то весом
в сторону оптимума.
Не в сторону оптимума,
мы лишь изменяем...
Давайте открою.
Мы лишь изменяем
какую-то долю значений из нашей суммы.
В случае обычного стахотического алгоритма
функции выбираются равновероятно
соответственно из множества от 1 до n.
Мы же решили попробовать
другие стратеги выбора
батча.
В данном случае мы берем
константу липшица для каждой функции афыта
и выбираем функцию афыта с вероятностью пропорционального этой константе.
За основу были взяты алгоритмы из статей,
которые вы можете видеть на слайдинг,
и стратеги сэмплирования из 2 ст.
Мы рассматриваем следующую характеристику
нашей сходимости.
Поскольку алгоритм работает
только в окрестности максимума,
мы рассматриваем отношение
до оптимального значения
к расстоянию из точки,
в которой мы стартовали.
Мы сначала запускаем обычный
гридиентный спуск
с той итерацией,
и дальше смотрим,
насколько наш алгоритм
запускаем наш алгоритм из данной точки.
Сравнивается он с алгоритмом
запущенного на 10 000 полуметрацей.
На данный момент мы провели
пробный эксперимент для небольшого количества лямб и бачей.
Лямб – это соответственно
параметр регулизации
нашей функции фриты.
Пока что ускорение получилось
в 2.2 раза.
Мы в перспективе проведем
в более адекватный вид
в виде какой-то гипотезы
и длительного интервала.
Это план на ближайший месяц.
Спасибо.
По сравнению с классическим алгоритмом
с весом взятым одной N.
Из-за задачи используется именно этот алгоритм.
Задача довольно узкая в том смысле,
что в нашей задачи мы осмотрим очень большое N
и при этом хотим использовать стахистические алгоритмы.
Это классический алгоритм
стахистического вида.
В нём используется обычный выбор весов.
Мы немножко модифицируем
и рассмотрим разные стратегии сэмплинга.
Два раза больше?
Два раза больше.
Два раза больше.
Два раза, да.
Но это на бачах небольшого размера.
Обычно используются бачи большого размера.
Нам конкретно в какой-то проблеме
научного руководителя нужны именно маленькие бачи.
Поэтому на маленьких бачах
классический алгоритм работает не очень хорошо.
Мы его оптимизируем под конкретные бачи.
Просто нужна была задача в узком виде.
Но в сайте действительно уже есть
некоторое теоретическое обоснование,
пока не до конца.
В общем, доказана сходимость
этого алгоритма с взвешенными вероятностями
в случае, если они отклоняются
от того, что мы делали.
В общем, доказана сходимость
этого алгоритма с взвешенными вероятностями
если они отклоняются от 1n
не более чем в 4 раза.
То есть, если
каждый вес хотя бы 1 на 4n
вниз не больше чем в 4 раза.
Так же будем стараться
улучшить эту техническую оценку
до произвольных весов.
Вот так было
построение модели,
которая могла бы находить
формату различных слов в тексте
независимо от его языковой принадлежности.
Также были
установлены некоторые
жесткие ограничения на размерственной модели
для ее практического использования,
на 100 гигабайт объем,
а также время обучения должно
происходить 24 часа.
Именно самая задача текстового поиска
в стандартный момент решается как получение фил
либо эмбридинга текстового документа,
а далее уже в данном пространстве векторном
мы кандидатами на плагиат называем те документы,
которые имеют довольно близкое расстояние.
И в нашей работе
предполагается использовать в качестве модели,
которая получает текстовый эмбридинг
к тематическому эмбридованию.
Сама тематическая модель по коллекции документов
строит вариаторное распределение
слов в тексте, а также
принадлежность к тексту в тексте.
А также можно использовать модальность.
И мы можем понимать различные методы информации
по статье, например, автора статьи, различные
изображения на этой статье, те, кто не был
ссылал, или кто был ссылал статья. И вся эта информация
также позволяет нам больше получить информацию
о самом документе.
Также сам подвод.
И мы планируем использовать 100 языков в качестве
кроме модальности. Основной литературой,
которую я прислал в данной статье, является методичка
Испаранцова, которая описана тематическое моделирование,
а также обычный способ регуляризации для того, чтобы
его использовать.
Теперь я хочу описать самые данные о изучении модели.
Это были статьи с двух сайтов. Один в Икипетии,
а другой в Элайвере.
Все статьи были взяты примерно на 100 языках.
И для большинства этих статьей уже были заранее известны
УДК, это Универсальный Личный Классификатор.
Это 10 основных тематик довольно обширных.
И Гарантии. Гарантии это тоже рубрики,
при этом немного большие, около 70. И они более точно
описывают тему документов.
Также был произведен машинный перевод по точной статье
на 42 языка.
Далее я хочу описать оценки качества.
Для каждого документа я просто сказал, что заранее был 10-й перевод.
Я хотел бы просто понять, как точная модель
составляет перевод и оригинал в объекторном пространстве.
Насколько были взяты.
Поэтому были предложены две значения.
Первая – это метрика качества личного поиска.
То есть та доля случаев, в которых документы перевода
попадают в 10% в ближайших документах оригинала.
То есть фиксируется оригинал, для него уже сортируются
все кандидаты на публикаторе.
А также средний процент документов с той же самой рубрикой.
То есть также мы знаем рубрику документа оригинала.
И мы смотрим, какая средняя доля документов
имеет ту же самую рубрику в 10%
ближайших 10% документов.
Также стоит отметить, что для каждого документа
мы проводим поиск не по всей рубрике документов,
лишь по ее подубрике.
Мы устроили таким образом, что 10% документов имеет тот же самый код,
к примеру, ДК. А 90% документов
имеют отличный код.
На самой модели, качестве базовой модели,
была взята темдическая модель, как я уже сказал.
И для нее был предложен ряд эвристий.
Во-первых, были выделены 125 отдельных тем.
Так как в темдической модели можно оригиналировать
технические темы, и вот было предложено остановиться
на 25. Также были все физиков, которые
включались в различные столбства.
И сам алгоритм
обучения темдической модели итерационно.
Соответственно, на каждой итерации мы делали
подвыборку тех документов более равномерно
для обучения того, чтобы модель
включалась на всех сразу по рубрикам.
Потому что, если я вот туда-то немного ориентируюсь,
мы видим, сами рубрики вообще не сбалансировали.
Вот. Сами первая рубрика про УДК
существует, бля, шестая рубрика, в которой очень много документов,
как откажется, тысяча и седьмая рубрика, в которой почти ничего нет.
Вот. Ну и, соответственно, то же самое с ГРНТИ.
Также стоит отметить, что в рубрике ГРНТИ существует рубрика «Нет»,
в которую попали все документы, для которой не было
не было понятного такой рубрики в отношении.
Это, кажется, четвертая рубрика или пятая.
То есть эта рубрика довольно большая,
модель сложена на таких документах,
в которых не понятно, какая у них тема,
ну, определить ее тема. Вот.
Также в процессе предобработки текстов используется
такая организация, это разбиение слова
на группы, ну, группы губ,
например, три или четыре, вот.
Ими используют для каждого языка 11 тысяч токенов.
Для того, чтобы, во-первых, ограничить предмет модели,
потому что существует именно количество токенов
и влияет на общие нуждевые модели.
А также для того, чтобы сохранить родительность
и способность модели именно, как обрабатывать текст.
Вот. Ну, и было использовано временное решение
отгадаться в самую среднюю рубрику «Нет» для того, чтобы
ну, не загружать модель, чтобы она не пыталась построить свою цитату.
Вот. И в итоге модель
занимает 96 гигабайт,
с теми ограничениями, которые были установлены.
И обучение занимает примерно 27 часов. Вот.
Здесь показано, как меняется одна из метрик,
например, средняя частота ГРМТИ. Вот, мы видим, что
на первых, там, десяти итерациях определяем
алгоритм при пехотичке составляется. Вот.
Далее ставим сравнение базовой модели
тематической, которая без пехотички будет быть, и итоговой.
Как мы видим, качество довольно серьезно повышается.
Вот. А также в будущем планируется
использовать также еще
брейтлайн-модель Exelembramberta. Это нерестивая модель,
которая, ну, которая тоже строит
рубрик и текст. Вот.
Ну, и в заключение мы сказали, что мы предложили метод,
который получает реакторное
представление текста, а также была построена модель для поиска
дублика, и ограничения, которые были
установлены, были достигнуты. Вот.
За ближайший месяц планируется провести также эвакуационные эксперименты,
которые покажут, насколько серьезными были те
или иные выклады каких-то отдельных предок пиши.
У меня все. Спасибо.
Ограничение о каком железе?
Просто говорите про время.
Все-таки зависит от того, где вы учитесь.
Да, там, в более строгих ограничениях, там указано
даже как это железо, которым можно было проследить ограничение.
А я вот здесь его не указываю.
Но оно вот так действует.
Дальше вопрос про машины перевода.
Сделано для того, чтобы отыскивать плагиаты, которые
должны в машинном переводе, или как истинный
перевод текста? Ну, он сделан для того, чтобы
и поискать реальную машину в переводе, но ей не стоящих
текста. Вот. Ну, значит, кажется мне, что
именно эти документы нужно было переводить для того,
чтобы перевести на какие-то не очень распространенные языки.
Там было указано про 15 языков. Это основные
французский, английский, например, немецкий. Вот. А есть
какие-то языки, например, что я даже не помню.
Ну, не очень распространенные, на которые просто перевода
документов в частке нет. Поэтому нужно было как-то такие данные
тоже получать. Поэтому было предложено
для обучения себя в модели и использовать на машину
перевода.
Как вы считаете, Реги не помыли, может быть, в каждом
моделе?
А, Гриндель, это у вас число?
Да, это просто первая категория.
А вы видите, что это
первая категория, а вторая категория.
Мы перебираем декабры
по всему документу. И у этого документа может быть
только первая.
И это же усоединяется по всем документам. Мы зафиксировали
регионал. У кого регионал есть какой-то под, вот уже
на стене этого пода на складке.
Что еще раз?
Ну, типа того.
Угу.
Такие названия предположены.
Да.
Ну, у вас, как раз, тоже, Реги,
переобучалась под это. Мы использовали именно
генерацию больше, то есть мелких публичных, чтобы как раз
контенсировать в некотором роде.
Была такая герма, и в ее решении мы использовали
именно генерацию больше, например, тех, которые мало.
И наоборот, меньше, чем больше, чтобы не было просто
полного переобучения в сторону больших.
Так, давайте начнем.
Так, меня зовут Лидер Кача и я. Я сегодня хочу рассказать про свой проект.
Это верификация Cypher на Coq. Значит, давайте сначала обсудим,
что такое Cypher, что такое Coq. Значит, Cypher
это, значит, есть SQL.
Это довольно привычный, значит, язык запросов
к революционной базе данных.
Но люди в какой-то момент подумались о том,
что есть, ну, некоторые данные,
но удобнее представлять веграфов.
И, соответственно, появился графовый база данных.
И Cypher – это язык запросов.
Значит, так как SQL появился достаточно давно,
то его достаточно давно и верифицировали.
По-моему, они полностью верифицированы.
Большая часть SQL – она верифицирована.
То есть фактически известно, что то, что делают,
если вы пишете запрос, то под капотом
делается именно то, что вы хотите от запроса.
То есть есть некоторые соответствия,
семантики языка и кода языка.
И возник вопрос в том, что хорошо бы
для SQL это тоже сделать.
Значит, давайте перед тем,
как я расскажу про цели,
я вообще в целом расскажу, как это все устроено,
потому что цели завязаны на то, как это устроено.
Значит, нужно сначала понять,
какая у нас семантика языка.
Значит, и у нас вот эта семантика
нашей базы данных представляется в виде
graph-своист или property-graph.
Тут что-то достаточно простое в том плане,
что это ребра, вершины и метки.
Метки – это, соответственно,
это, во-первых, те данные, которые хранятся.
То есть там в конце фея обозначены.
Фея – пара ключ-значений.
То есть у меня может данные храниться как на ребре,
так и на вершине.
А еще есть метки, которые позволяют нам
какие-то ограничения.
То есть это неляется непосредственно данными,
но при этом они будут давать нам ограничения
на поиск каких-то данных внутри этой базы данных.
Вот, значит, тут пример такой графический.
Наши графы базы данных,
то есть у нас есть вершины, ребра между ними,
и, соответственно, мы можем находить
вершины, значит, какие-то люди,
ребра – это какие-то связи между ними,
например, коллеги, или один выучит другого.
Естественно, я могу сделать запрос такого вида,
условно выведение, пожалуйста, человека,
у которого учитель-профессор.
Ну и, собственно, такие запросы в графы базы данных.
Значит, как вы видите,
как я показал, что любой запрос в графы базы данных –
это какой-то поиск пути в графы базы данных.
И последующая вывод информации.
И нам нужно для формализации вообще понять,
как вы видите паттерны,
то есть паттерны, которые еще внутри графа.
И для этого нужно понять, что такой вообще
паттерн вершины,
нот-паттерн – это, фактически,
некоторые ограничения, которые мы не сделаем для вершины.
То есть мы даем ограничения на лобла,
то есть на предыдущей картинке,
например, то, что вершина должен быть учителем.
И также мы можем дать какие-то ограничения на пары ключ значения.
Гразно более интересный является
паттерн для…
Тут описка, значит, здесь не нот-паттерн,
это h-паттерн. Он достаточно похож,
но нот-паттерн… Отличия только
к красным цветам здесь выделены.
Выделены красным цветом.
Первое, мы можем давать ограничения на ребра,
это первое. А второе,
мы можем давать ограничения на количество
ребра, которые мы можем пройти в паттерне.
То есть, если нот-паттерн для одной вершины,
то, значит,
h-паттерн он для
последовательной…
Некоторых предыдущих ребр. То есть, вы можете мне сказать,
что я хочу найти вершину,
которая будет связана с данной вершиной через два ребра.
И эти ребра должны быть приведены к каким-то свойствам.
В общем, значит…
В целом, это все приведет нас к тому, что у нас есть
Reddit-паттерн. Это фактически просто
чередование этих, значит,
нот-паттернов и h-паттернов.
То есть, вы делаете ограничение на ноту,
потом ограничение на путь из этой ноты куда-то,
на следующую ноту и так далее. То есть, таким образом, ваш запрос
непосредственный,
он перформируется в этот паттерн,
с которым он будет работать.
Теперь наша задача — сопоставить этот паттерн в некоторый путь и граф.
То есть, паттерн в некоторую структуру, которую мы ищем,
а путь — это то, что практически будет нашим ответом.
Для этого у нас есть гомоморфизм.
Это фактически поиск этого пути.
Почему он волк? Потому что там есть еще
другие гомоморфизмы.
Гомоморфизм является самым общим, потому что он разрешает
всевозможные пути. А и гомоморфизмы, которые запрещают,
чтобы были какие-то циклы, чтобы были посещены
вершины ребра, которые мы уже посещали и так далее.
Собственно, здесь снова наш
пример.
Здесь я написал
как это выглядит.
Первая строчка — как выглядит запрос
на джакуэля.
То есть, я хочу найти вершину,
у которой есть label-person.
Ребро
должно удовлетворять такому свойству,
что у него есть метка DAS.
Ребро должно быть одно, а то, куда я приду, мне все равно.
И после этого я показал,
как это переформулируется в терминах
риги-паттерна.
Хорошо, это вообще в целом про семантику.
Давайте поговорим, причем тут кок.
Что хочется сделать? Хочется понять, что
вот у меня есть запрос, и он удовлетворяет кому-то свойство запрос.
Он должен вывести то, что
я от него хочу, семантически.
Это можно понять.
Для этого используется следующая модель.
То есть, я строю модель на коке,
и для этой модели буду что-то доказывать.
Соответственно, если я буду доказывать корректность модели на коке,
если я считаю, что я модель построил правильно,
то это происходит в GQL, в Syfy.
Значит, как мы строим модель?
Модель строится следующим образом.
Вот я показывал, что у нас есть лейбл
и у нас есть пары ключ-значений.
Для каждой пары ключ-значений
я привезу матрицу смежности.
То есть, там будет полик идиучки,
которая будет соответствовать, что на этом ребре есть такой-то лейбл
и пара ключ-значений.
То есть, у меня будет множество таких матриц.
И смотрите, что дальше происходит.
Дальше любой вот тот паттерн, который я должен искать,
я могу его переформулировать с точки зрения
регулярного выражения.
Потому что, условно, если я хочу найти путь
из одной вершины в другую,
длину условно 2,
то что нужно сделать?
Мне нужно просто взять матрицу смежности,
взять другую матрицу смежности и просто их перемножить.
Дальше просто посмотреть на нужную чейку.
Есть там нолик и есть не там единичка.
Соответственно, что мы делаем?
Мы строим матрицу регулярных выражений.
То есть, мы по паттерну строим регулярное выражение из матриц.
И потом подставляем туда матрицу смежности нашего графа.
И дальше мы уже с получившейся матрицой работаем.
Значит, в чем тут проблема?
Потому что казалось бы достаточно просто
какие-то там выражения с матрицами делать
и смотреть результат.
На самом деле, сегодня так тривиально. Почему?
Потому что, давайте назад откатимся.
Тут я пишу, и это пара натуральных чисел,
которые означают минимальное, максимальное количество ребра в обходе.
В Cypher есть такие запросы такого вида,
что ты можешь идти,
условно, давайте запрос сформулирован,
что я хочу дойти из вершины А
до какой-то вершины Б
по ребрам какого-то типа,
и это ребро какого-то типа я могу брать сколько угодно раз.
То есть, фактически правая граница, второе натуральное число
является бесконечностью.
Если работать, ну, просто пишете регулярное выражение,
то это будет просто звезда к линии.
Вот.
Ну, а как подставлять,
ну, то есть, если у вас есть регулярное выражение, которое нужно оценить
вот матрицами, как туда подставлять
матрицу смежности, считать не очень понятно.
То есть, из-за вот этого бесконечности.
Вот в частности, из-за этого, значит,
нужно пользоваться инструментами релиционной алгебы,
которыми я и пользуюсь.
И, собственно, есть такая прекрасная библиотека.
Вот, но она достаточно абстрактная, наверное, потому что
как бы вы, ну, там
из-за того, что у вас выражение,
оно должно быть не вычислимым, а перечислимым,
вот, то там, как бы, есть некоторые
достаточно
большой уровень абстракции. Условно,
например, как бы
матрица там представляется, как просто
некоторое, типа,
множество морфизмов над манойдом.
Вот, значит,
да.
В частности, еще также, смотрите, почему
библиотека достаточно абстрактная, потому что
мне хочется работать как с матрицами,
значит, над обычными буллским значениями,
так и над матрицами с регулярными выражениями
внутри и снаружи. Вот, поэтому
это требует достаточно
большой абстракции.
Вот, значит,
какие основные цели? Значит, первое, самое важное,
это построение базовой модели графа и паттерна,
под базовостью, а именно то, что она поддерживает
какие-то очень простые запросы. Условно,
у вас нет никакой свободы на то,
сколько у вас ребер может идти, у вас может быть идти только одно ребро,
у вас не может идти несколько ребер. Кроме того,
значит,
ну, такое, ну, основное ограничение.
Значит, второе, то есть вы
после того, как я, значит,
вы построили все эти паттерны и модель графа
в непосредственной оплоке, нужно использовать эту губитепу
для того, чтобы можно было как-то валидировать
запросы непосредственно.
Собственно, первые два пункта мы уже сделали,
и в оставшийся месяц я собираюсь
сделать оставшиеся три. Значит,
это будет очень здорово, если у меня удастся это сделать. Значит,
какие это три? Первое, это улучшение модели паттернов,
я делаю их более приближенным к тому,
что находятся непосредственно в Cypher, за счет
того, что я попытаюсь снять это ограничение
на пути. Кроме этого,
в Cypher есть различные вариации модель запросов,
то есть, например, до этого есть,
если моя модель до этого только обрабатывает, что давайте мы найдем путь
и выведем последнюю лобу, что в ней там хранится,
то в Cypher можно, например, искать путь и
выводить то, что находится посередине.
То есть, это более затруднительно,
потому что вам нужно
просто строить регулярное выражение,
считаете, что-то находите, а тут нужно
построить частично два регулярных выражения,
типа до и после, посмотреть, что происходит в середине.
Соответственно, и последнее,
сам важное, это формализация доказательства
корректности поиска пути. То есть, есть
некоторые утверждения о том, что значит, что
Cypher работает корректно,
и нужно непосредственно доказать, что
для этой операции
выполняется какое-то свойство.
Вот, собственно, для чего это все и делается. То есть, это неинтересно
строить модель. Это уже все.
Да. То есть, не очень интересно строить
модель и определить на какие-то конкретных
случаев, а хочется доказать общее утверждение
для всех возможных графов и запросов.
Ну, это у меня все. Так, спасибо.
Если есть какие-то вопросы.
Да, да.
Ну, то есть, смотрите, что происходит.
Вот у меня есть некоторая операция,
условно запрос в графовом языке,
и я описываю семантику, как это работает
под капотом.
Семантика языка – это не специальная
семантика.
Семантика языка – это не специальная
семантика.
Семантика – она есть имбиризация. Да.
Да.
Я это понял.
А...
Ну, если кто-то спросит о том, чтобы
на экране наоборот обвенчали две медицины,
то что бы он сказал?
Ну...
Просто у вас фактически не будет
бесконечного ответа, почему? Потому что у вас база данных
конечная. Вот.
Вы не ограничиваете путь. То есть, вы говорите,
что путь может быть сколько на большом. То есть, вы говорите, что путь 10.
Можете там 100 взять, путь 150.
Вот. Но фактически это будет конечный ответ, потому что у вас база данных
конечная.
Сейчас. Нет. Значит, что
вы говорите?
Ну...
Ну...
Ну...
Ну...
Ну...
Ну...
Ну...
Ну...
Ну...
Ну.
Ну...
Ну...
так то есть не знаю я что на данный момент
на данный момент у меня построена база модель графа что имеет то есть есть граф и
есть я поддерживаю паттерны типа ну то есть ригит паттерна в которых их паттерны они
нет фиксировано типа ограничение 1 1 то есть я могу только по одному ребру походить между
ребрами то есть я обязательно должен между ними тыкать ограничение по нормам и для значит для
такой модели собственно я умею строить по паттерну непосредственно регулярное выражение и поэтому
регулярным выражением я могу оценивать матрицу непосредственно какого-то конкретного графа и
вводить результат то есть практически пятый пункт то есть пока не действительно ничего общего
про эту модель не доказано но показано что это модель рабочая условно если поставить
конкретное значение но сделать то что нужно перед тем моего доклада стояние триктория движения
руки по низу основной целью исследования заключается обобщить метод канонического
с помощью менты судех�с для решений задачи по решению задачи высекает две подзадачи это
получение трикторного пространство по временному ряду и выбор метрики для медитации и предлагая
предлагается решение предлагаемо решение заключается в построении матрицы двигов по временному
ряду и обучение представления фазового пространства вообще глобально решаем датчик
что прогнозирование многомерного временного ряда на основе
другого и одной из трудностей, которая возникает в решении
данной задачи, заключается в том, что размер сисотных
временных рядов она миляка, и это приводит к неустойчивости
паралитической модели.
И одним из возможных решений данной сдачи заключается
в применении методов конечного коллекционного анализа
или методов, методчатечных на меньше квадратов, сокращённо
они называются CCA и PLS.
В чём где заключаются этих методов?
У нас есть и сотные, и целевая нанимерные ряды, x, y, и мы
проецируем эти ряды в некое латетное пространство
меньшей размерности, причём полученные проекции должны
быть согласованы.
Под согласованностью подразумевается в случае PLS максимизация
к вариации соответствующих столбцов матрицы ТИУ, а
в случае CCA максимум корреляции.
Другим методом, другим способом решения данной проблемы
является метод сугехары, в чём основная идея его
заключается в том, что мы для каждой компоненты
целевого временного ряда y, zhi, t хотим выбрать некое
фиксированное количество компонентов x, y, t, которые
наиболее связаны с этим рядом.
Тезисно в чём заключается данный метод, мы проецируем,
мы составляем по одномерным временам x, y, zhi, t трикторные
матрицы и полученные столбцы в этих трикторных матрицах
образуют трикторное пространство hx, hx соответственно.
И далее мы задаём между этими пространствами некое
отображение phi и утверждается, что эти ряды связаны друг
с другом, если phi от y0 с крышкой, оно близко в некотором
смысле с y0, и в теоретической части будет показано, что
метод сугехары является, что методы PLS ECC являются
в частном случае методом сугехары, вот здесь приведён
список литературы и далее более подробно про метод
сугехары, трикторная матрица для одномерного временного
ряда выглядит следующим образом, мы берём окно размера
n и это у нас получается будет первый столбец нашей
матрицы, потом сдвигаем окно на один шаг вправо
или там на какой-то шаг tau, получаем второй столбец
нашей матрицы и так далее.
Теперь вот отображение phi, его определение приведено
на слайде, только стоит сказать, что индексы t1 и так
далее tk, это индексы ближайших соседей элемента x0 в пространстве
hx, вот, и мы утверждаем, что временные ряды связаны
если, то есть один из способов заключается в том, что отображение
phi является Липшицевым, другим способом, который
проведён был в оригинальной статье заключается в том,
то что коэффициент корреляции, коэффициент корреляции
пирсона между z0 с крышкой z0, он растёт с увеличением
обучающей выборки и довольно высок, вот, и поэтому метрика
связанности временных рядов, здесь рассматривается
разность метода сугехара, применённого для всего
ряда и для некоторой его части. Пару слов про представительную
модель, которая применяется, это просто авторегрессионная
модель порядка 0-1, вот, при этом вот эти вот элементы
x с индексом t0 являются полимерным произведением
элементов x и t, x и t мало и вес w и t, который пропорционален
расстоянию от некой точки x до точки x с индексом t0.
Далее был проведён вычлительный эксперимент, основной целью
которого заключается сравнение различной стратегии снижения
размерности целевого пространства. Более подробно про данные,
которые использовались. Слева на слайде вы видите
данные акселерометрии гироскопа, а справа данные
видео кейпоинтов, значит, так они были получены.
По видеодвижению руки человека с помощью фреймворка
AlphaPose было получено скелетное пристреление человека,
и на этом графике показано как изменение конкретного
кейпоинта за течение времени. Получены результаты.
В первой таблице вы видите строение ошибок, видно ошибка
и её дисперсия паралитической модели, применённой в траекторном
пространстве и в его подпространстве. Вот по вертикали,
stop to space, subspace соответствует траекторную пространацию
и подпространство, а по горизонтали это наши целевые
переменные, это показания акселерометра по осям
и показания гироскопа. И видно, что применение предсказательной
модели в подпространстве увеличивает её точность.
На другой таблице вы видите результаты применения
предсказательной модели использованных различных
методов снижения размерности, по горизонтали это метод
гехар, плс, се, я говорил ранее, а наив это то, что
мы получается не делаем никакой отбор признаков.
Это то, что мы не применяем дополнительных данных,
то есть мы, то есть во всех предыдущих методах мы использовали
дополнительные данные, которые представляют
по себе наборки поинтов, полученных по видео, а
соответственно, стопцы наив, мы не использовали
данные из видеоряда. Вот здесь видно, что использование
данных из видеоряда, оно повышает качество предсказания.
И что сделано на текущий момент, был проведен вычислительный
эксперимент на данных устройств и видеоряда, и были подтверждены
гипотезы о том, что использовать данных из видео повышает
качество прогнозирования и что прогнозическая модель
менее устойчива в случае, когда та применяется в
На будущее планируется доделать теорическую часть и явным
образом показать, как из метод SugeHarp вывести методы
PLS и CCM. Спасибо, на этом все.
Нет, мы предсказываем сейчас. Нет, у нас вот по руке, то есть
есть набор никаких keypoint, то есть вот с помощью фреймворка
мы получаем вот такой многомерный временной ряд, то есть у нас
есть изменение конкретного keypoint с течением времени,
но их там довольно много, то есть 150 где-то получается,
то есть размерность вот то есть 150 keypoint это довольно много и
то есть они получены по всему телу и понятное дело то, что
ну то есть да, то есть данных keypoints понятно, что они все
keypoints влияют на показания акселером три гироскопа,
поэтому хочется снизить размерность данных полученных
по видео ряду, и мы применяем эти данные в припрогнозировании
показаний акселером три гироскопа. Они на руке закреплены,
да. Нет, мы хотим вот именно показаний
цельевых. Да, я думаю, я готов начинать. Приветствую всех, меня зовут Шишацкий
Михаил и моя научная работа посвящена построению развертывания
кластеров Kubernetes с применением стандарта азистоско. Сначала я расскажу, что такое
кубернетис, причем здесь азистоско, затем подведу к постановке моей задачи,
опишу решение, которое я хочу получить к танцу этого семейства.
Итак, кубернетис. Кубернетис – это популярная распределенная платформа для
кистрации контейнеров, которая является чуть ли не стандартом de facto и используется
повсеместно, согласно последним вопросам. Таким образом, актуальность
развертывания и поддержки таких кластеров возрастает. Кубернетис
чрезвычайно гибок и может быть развернут во множестве различных окружений и
различных конфигураций. В часть конфигурации вы можете видеть на слайде.
Однако, существующие инструменты для предоставляющего возможность развертывания
кубернетис либо предоставляют ограниченный набор конфигураций,
давая готовое решение, либо слишком низкоуровневые и не позволяют сделать
все автоматически и требуют какое-то количество специалистов, разбирающихся в
области для настройки и поддержки таких кластеров. Одним из возможных
решений уменьшения сложности может быть добавление еще одного слоя абстракции,
которым может стать модель Азистоско. Что такое Азистоско? Это язык для
моделирования облачных приложений и задача управления, связанных с этими
приложениями. Как вы можете видеть из слайда, этот язык графовый и атомарные
элементы топологии представляются в нем в виде вершин графа, которые имеют свои
способности и требования. Ноды между собой связываются с помощью отношений.
Язык тоско является наиболее популярным в последнее время языком для описания
такого рода облачных топологий и почему нам выгодно его применять. Можете,
пожалуйста, пролистать слайд на следующий, еще один. Там разные слайды, просто зуммей
в моей презентации. В общем, основные плюсы использования Азистоско это один
язык для описания как инфраструктуру, так и сервисов, которые работают в этой
инфраструктуре. Это дает нам переносимости в встраиваемость, это
позволяет не терять контроль над всеми параметрами конфигурации, составляющих
Kubernetes и упрощает навигацию в таком большом количестве параметров с помощью
графового представления. Также это позволяет верифицировать конфигурацию
перед развертыванием, так как все параметры отображены, собственно, в графе и
стандарт Азистоско позволяет накладывать ограничения на параметры. Это же
позволяет нам обновлять отдельные Kubernetes, отдельные компоненты, инфраструктуры
и автоматически масштабировать их. Также это не привязывает нас к
конкретному инструменту, потому что Азистоско это только язык для описания,
который может быть поддержан большим количеством оркестраторов. Таким образом
это, можете вернуть на предыдущий слайд, это приводит меня к постановке задачи.
Моей задачей является построение системы типов и топологии сервисов на
языке Тоска для описания инфраструктуры Kubernetes и дальнейшей ее поддержки
конфигурации. Новизна работы заключается в том, что существующая работа в данной
области исследовали топологии сервисов, запускаемых в Kubernetes, но не уделяли
внимания описанию самой инфраструктуры Kubernetes. В моем
предположении такое описание в комбинации с существующими оркестраторами
создаст расширяемый, ширококонфигурируемый инструмент для управления
инфраструктуры Kubernetes.
Деталями, теперь перейдем к деталям решения. Сам язык Тоска использует язык
разметки Ямл. Все типы и топологии описываются на языке Ямл. Для дальнейшего
развертывания этой топологии используются скрипты, написанные на
ансибл. В своей работе я использую скрипты из инструмента
CubeSpray, адаптируя их под модель Тоска, потому что в инструменте CubeSpray нет
возможности атомарного развертывания отдельных компонентов
топологии. В качестве оркестратора я использую Xopera, это легковесный
Тоска-оркестратор, который написан на Python. В качестве результатов ожидается
получение полной модели инфраструктуры Kubernetes, то есть нужно выделить
отдельные типы и отношения между этими типами. Разработать ансибл роли для
деплоя всей инфраструктуры. И к майской конференции я планирую получить тестовый
стенд, который позволит продемонстрировать возможность деплоя Kubernetes и обновления
отдельных его компонентов. На этом у меня все, здесь представлены источники,
которыми я пользуюсь. Спасибо за внимание, готов идти на ваши вопросы.
Я могу предоставить какой-то демонстрационный стенд, также планируется измерить время
деплоя, когда у меня появится что-то, какой-то прототип, я измерю время деплоя, время
обновления, какие-то другие операционные метрики, которые позволят оценить качество
концепции, которые я предлагаю. В связи с программой, просто я на ноутбуке могу запустить что-то.
Меня зовут Ковалев Константин, у меня тема работы, то есть аргументов теории EOPEN и отношения
между ними. Это тема по математической логике. Будем работать в такой сигнатуре,
у нас есть символ S для последователя, плюс умножить ноль и отношение неравенства.
У нас будет такая базовая теория, все теории, которые мы будем рассматривать,
будет ее расширять. Она называется такая теория Айхмедика Робинсона. Вот она состоит из следующих
аксиомов. Тех, наверное, перечитывать не буду, все. Вот они здесь так на слайде есть.
Вот такая наша основная теория. Это как раз будет это EOPEN. Это теория, состоящая из этих аксиомов Q,
которые были на предыдущем слайде, и схема аксиома индукции для бискваторной формулы.
То есть такая формула, что если у нас phi от нуля и для любого х из phi от х и до phi от следующего
элемента, то тогда для любого х phi от х и с каким-то еще параметром. Вот чем хороша эта теория,
тем что у нее есть рекуссивные нестандартные модели. В отличие от каких-то более сильных
теорий, например, индукции по ограниченным формулам, неформально это означает, что мы можем
следовать эту теорию и ее какие-то по теории теоретико-модельными методами. То есть как-то
предъявляя какие-то контрмодели для того, чтобы окраирна доказуемость каких-то утверждений.
Ну как, например, это сделал Шеферсен в своей статье и доказал, что нельзя в этой теории доказать
рациональность корни из двух, ну и теремофирма, например. Ну вот известной логик Фарри Фридман
заметил, что большая часть основных свойств, которые может доказать про модели этой теории,
их можно доказать используя индукцию не по всем бисклаторным формулам, а только по атомарным
и их отрицаниям. Ну это вот мотивирует нас на такое-то определение. Вот и от ELITE, ну ELITE это типа
литералы. Это теория, состоящая из аксемку и индукции по атомарным формулам и их отрицаниям.
Ну логично можно определить вот такие еще более слабые теории. Индукция по равенству,
отрицание у равенства, порядку и отрицание порядка. Ну и вот у нас появились такие вопросы. Правда ли,
что EOPEN сильнее, чем вот эта индукция по литералам? Если да, то какие можно предъявить доказания
в EOPEN, которые нельзя доказать в ELITE? Можно ли ELITE как-то оконечно аксемитизировать на тпу?
И какие у нас вообще есть отношения между вот этими слабыми теориями? Вот этими четырьмя в конце.
Так, ну вот промежутки индукции у меня практически такие, что вот мы, у нас первый вопрос был,
эквелинт на ли, да, мы получили, что у нас EOPEN эквелинт на ELITE. Вот схема доказательств
примерно следующее. Ну, нам сначала мы хотим сформулировать анатирему, вот такую анатирему
про EOPEN. Для этого нам вот еще понадобится пара определений. Вот, мы скажем, что у нас, если у нас
есть какие-то два порядоченных кольца, m и r, которые оно уложено в другое, и причем m действительно
упорядочено, ну то есть у нас енис это наименьший элемент в m, то тогда m называется целой частью r,
если вот для любого элемента r, из нашего тогда большего кольца r, такой элемент m, вот только
здесь печатка, здесь получается m меньше ебровной чем r, меньше чем m плюс 1. То есть получается для
каждого элемента r существует такая типа целая часть для m. Ну можно заметить, что так как у нас
кольцо дискретно упорядочено m, то такое m будет единственным. Еще одно определение, вещественное
замкнутое поле. Вот если у нас есть какое-то упорядоченное поле, то мы будем называть его
вещественным замкнутым, если в нем выполняется и ремо промежуточного значения для всех многочленов.
Ну то есть если у нас у нас на щелен f и f от a больше 0, f от b меньше 0, то у нас существует корень между a и b.
И если у нас есть какое-то упорядоченное поле, то можно доказать, что у него существует какое-то
наименьшее его вещественное замкнутое расширение. И такое расширение мы будем называть вещественным
замыканием. Ну и еще там можно доказать, что оно будет гибридическое. Так, вот и ремо,
который доказал тоже ферцон. Вот заключается в следующем, что вот если у нас есть какое-то
дискретно упорядоченное кольцо, мы обозначим через m плюс полукольцо его неотрасательных элементов,
то тогда это m плюс будет являться моделью и open, тогда только тогда, когда у нас m является
целой частью вот этого вещественного замыкания более частных этого кольца. И вот с помощью как
раз-таки ремы мы сможем доказать этот эквивалент. Вот схема примерно такая. Сначала мы можем
вот так вот руками доказать, что у нас выводятся все аксиомы дискретно упорядоченных полуколец
нашей этой более слабой теории. Ну потом мы фиксируем какую-то модель нашей теории и
вот эта лемма нам позволяет построить такое кольцо m, которое вкладывается, вот это кольцо наше
полукольцо m0, так что у нас m0 равно полукольцу неотрасательных элементов m. Мы еще обозначим
через r вещественного замыкания более частных m и тогда можно доказать такую лему, что у нас
если есть какой-то мночлен f с коэффициентом из m, такой, что на каком-то n делит на q0 на
каком-то m на q больше 0, где m и q из m0, то тогда у нас существует какой-то корень его вещественного
замыкания такой, что у него есть целая часть в m. Он не обязательно будет лежать на вот этом интервале
от n на q до m на q, но из этого можно будет выяснить, что именно все элементы r будут иметь
целую часть в m. Ну отсюда, пользуясь теориям и шеферационами, можно доказать, что m0 будет моделью eopen.
На второй вопрос ответ тоже отрицательный, то есть у нас elite не является конечной аксиматизированной,
но идея доказательства закая, что вот мы можем посмотреть на ту модель, которую придумал шеферацион,
для доказательства той теории, мы в самом начале, которая рекурсивная и не стандартная. И вот мы можем
как-то вот аккуратно выкинуть из нее некоторые элементы так, чтобы она была моделью вот этой
конечной аксиматизации, которую мы якобы предполагаем, но там существовали элементы, целые части не для всех
элементов естественного замыкания, а только для элементов, которых небольшая степень над этим кольцом.
Но на этот вопрос пока еще до конца не удалось ответить. Пока я доказал, что у нас из трех теорий
не выводятся остальные, ну у меня предположение такое, что они все из друг друга не выводятся,
то есть они все по парню как-то не вкладываются друг в друга. Дальнейшие планы какие у нас есть?
Вот можем мы рассмотреть вот такой аналог вот этого первого опроса только уже для теорий, в которых у нас
не зависит он порядок. То есть получается вот это и от равенства и отрицания равенства, это
ну такой аналог ELITE без знака неравенства, и его панат равенства это индукция по всем
дисконторным формулам, в которых все атомарные формулы имеют mit s равно t. Можно еще исследовать
как-то более подробно вот эту теорию и от равенства, потому что она как-то выглядит более-менее естественно
в отличие от остальных. Возможно у нее есть какая-то хорошая аксиматизация, возможно даже конечная,
вот с этим я пока не развалился до конца. Ну еще интересный вопрос, который можно позадавать
насчет этих теорий. Вот мы можем определить такое множество разрешимых диафантовых уравнений в моделях
нашей теории какой-то t. Вот это получается формально такое множество, что это пара термов s и t, такие,
что у нас существует модель нашей теории и такой формул, что существует такое решение уравнение
s равно t. Ну вот есть такие частичные результаты, что если мы возьмем ту самую такую базовую
нашу теорию, рифмитику Робинсона, то d от q будет разрешима. Это не данный результат. Еще можно заметить,
что d от вот этой теории и от равенства тоже разрешима. Ну там, в принципе, все не сложно доказываться,
там на самом деле, но хватает всего лишь одной простой модели, которая у нас содержат все решения,
если не есть вообще. Но разрешимость d от eopen является до сих пор открытой проблемой. Там есть
частичные результаты. Вот Билки доказал, что есть такой критерий, который формулирует,
формулируется терминных идеалов, что вот если у нас есть какой-то хороший идеал,
у нас такое уравнение разрешимо, когда у нас какой-то хороший идеал в кольце z от x,
в котором лежит наше такое многочлено. Вандедопиз доказал, что если мы ограничимся не всеми термами,
а термами только с не более чем двумя переменными, то это все-таки будет разрешимо. И ADER доказала,
что у нас разрешимо такое множество, что если мы будем рассматривать не все такие термы,
а только термы определенного вида. Меня зовут Клупа Дмитрий. Я в РАБКах инновационно-працикума
занимался задачами экстремальной комбинаторики. Конкретно задача, которая решала это была,
ну в частном случае, гипотеза Эрди-Шапова сочетания. Давайте посмотрим, что у нас есть. Пусть у нас есть
натуральные числа mk, тогда можно рассмотреть какое-то семейство коэлементных подмножеств,
н-элементного множества, ну скажем чисел от 1 до n. И давайте через ню от f обозначим максимальное
количество попарно непересекающихся элементов этого семейства. Ну тогда же натуральных чисел
sk и n больше, чем с плюс 1 на k, можно рассмотреть вот такие два семейства. Одно это все коэлементные
подмножества, множество чисел от 1 до s плюс 1 на k минус 1. И другое это все множества коэлементные,
которые пересекаются подмножество с множеством чисел от 1 до s. Ну для этих семейств не трудно
понять, что у одного и другого максимальное количество попарно непересекающихся будет s.
В чем состоит гипотеза Эрди-Шапова сочетания? Она состоит в том, что вот эти два семейства I-B,
их мощность это максимально то, что может быть. То есть если у нас семействе f максимальное
количество попарно непересекающихся не больше s, то тогда и мощность не больше, чем максимум из
мощности a и мощности b. Вот, ну для s равно 1 на самом деле это превращается в известную теорему Эрди-Ши
Курада. Ну если рассматривать при n больше, чем больше или равно 2 умножить на k, тогда там,
как известно, ответ это число сочетания из n минус 1 по k минус 1. Ну и там на самом деле, например,
там реализуется и обе конструкции, когда a максимальный, когда b. Ну когда b максимальный,
это там получается пример, который называется звезда, это когда все множества содержат один
общий элемент. А конструкция, когда максимальная мощность равна мощности a, она только при n равно
2 умножить на k, это надо взять все коэлементные множества 2k минус 1 элементное множество. Дальше,
при достаточно больших n, на самом деле при не очень больших n, уже выполнено то, что мощность
a меньше, чем мощность b. И вот работа, ну случай, когда максимальная конструкция является, вот как
раз множество b, они много где разбирались, вот в частности сам Эрди-Ши он доказал, что как раз
реализуется максимальная оценка это мощность b при всех достаточно больших n, ну если зафиксировать
k и s. Вот потом Франкл доказал, что это верно при n больше, ну что-то типа 2 умножить на k и s,
кажется 2 на s плюс 1 на k. Вот потом Франкл и Купаски доказали, что там при всех достаточно
больших s и при n больше, чем 5 с третьих s и k, минус что-то еще маленькое. Вот, ну я занимался,
ну работал в случае, когда максимальным, ну когда правильной верхней оценкой является мощность a. Вот.
В 2017 году Франкл доказал вот такую теорему, то что при n больше, главное с плюс 1 на k и при этом
меньше, чем с плюс 1 умножить на k плюс что-то очень маленькое, как раз максимальным, максимальной
мощностью является мощность a. Вот, там на самом деле может показать, что, ну там цель работы была в общем
доказать, что какое-то вот число существует, то есть оно тут очень маленькое, получилось k в степени
минус 2 k минус 1, но там просто задача была показать, что какое-то существует. На самом деле можно заметить,
что при не очень больших s, ну при s меньше, чем k в степени, ну вот как раз то, что там написано,
там у нас вот это неравенство, оно превращается в равенство n равно s плюс 1 на k, и для него все
так уже давно было все доказано, это доказано Клейтманом, там можно это доказать, используя
вероятностный метод, похожий на то, что в теории мерде Шикарады было. Вот. Я в рамках математического
практику на втором курсе, который был, доказал вот такой результат, то есть что можно заменить вот
это страшное число k в степени минус 2 k минус 1 на что-то побольше, там k в степени минус 2,
но при этом там требовалось достаточно не очень хорошее ограничение на s, там s больше, чем k в
степени 4 k. На самом деле от него было достаточно легко избавиться, там, ну это я в общем сделал,
чтобы было проще доказывать. Вот уже в рамках математического практику, у меня получилось
доказать вот это на данный момент, видимо больше ничего не получится доказать, потому что там
уже сильно сложнее, то что при s тут уже достаточно разумное ограничение 100k в кубе, то есть это
естественно намного лучше, чем k в степени k и что-то такое ужасное, и при n меньше чем s
плюс 1 на k плюс 1 делить на 100k, вот выполнено то же самое. Вообще хочется глобально доказать,
что 1 делить, ну вот это 1 делить на k можно заменить на константу, но вот это на самом деле
кажется достаточно сложным, потому что уже видно, что при константе 1 вторая там мощность будет
больше, чем мощность а, и там не работают те методы, которыми я доказывал вот это, я на самом деле
доказывал это используя ту же технику, что была в статье Франкла, там достаточно хитрый двойной
счет и плюс то, что можем считать, что f, ну так называемое сдвинутое множество, то есть
негоязычный термин shifted там водится на элементы, ну на множество, на коэлементных множествах специальный
shifted порядок, и мы считаем, ну семейство сдвинутое, это когда для любого множества все, которые меньше
него в этом специальном порядке, они тоже будут лежать в семействе, вот и там используя ту же технику
получить, доказать вот это, ну в общем, видимо, больше ничего не смогу доказать, так что на это все, наверное
как минимум записать вот это, потому что пока она не записана, ну да, вот это я не так давно доказал, неделю
две назад, то есть до этого была оценка вида, где 1 делить на k, на какую-то константу, она уже была достаточно
давно у меня, но при этом там по-прежнему на s осталось вот это плохое ограничение, то что там, ну
какая-то такая в степени k, и вот я занимался, ну и вот нормальным ограничением пытался сделать на s,
чтобы оно было хотя бы полиномиальным, вот на данный момент такое есть, да,
а
сейчас нет, нет, звезда не про это говорил, я звезда говорил про, там звезда это я, когда упоминал теорию
я уже говорил пример звезда, потому что там у нас s равно 1, то есть это означает, что у нас вообще нет по парам не пересекающихся
элементов, то есть у нас любые 2 будут пересекаться, ну звезда она как раз вот в общем случае соответствует
вот конструкции, которая семейство b.
Добрый день, я занимался пересечением дисков в скоросочетаниях максимальной суммой, значит вначале я
опишу формировку задачи, над которой я работал, про контекст, задача в котором она возникла,
дальше опишу какие результаты удалось получить, и над какими дальнейшими задачами, близкими можно
думать, значит формировка изначальной задачи, вот пусть у нас есть на множестве четное число точек,
некоторые из которых возможно совпадают, и для этого паросочетания мы рассматриваем полное
паросочетание, которое максимизирует сумму отрезков между точками паросочетания, и хочется
доказать лично на плоскости точки о, такое что вот выполнено это равенство, другими словами,
что она суммарно от двух концов любого отрезка паросочетания отстоит не больше, чем соответственно
вот какая-то константа больше единицы на отрезок паросочетания, вот, ну кинем у нас первое
наблюдение, вот это условие, оно у нас задает, вот если мы в нем поставим равенство, оно у нас
будет задавать эллипс, соответственно если мы не стал границы ставим, то это получается внутренность
эллипса замкнутая, и задача можно понимать так, что у нас есть четный число точек на плоскости,
мы взяли максимальное паросочетание, нужно доказать, что все эллипсы вот с такой константой у нас имеют
общую точку, собственно, да, и вот эта константа 2,9 на корень из трех, она берется у нас не случайно,
это эллипс, в общем, если мы посмотрим равномерно этот угольник с углом 120 и с основанием отрезок
паросочетания, то его вершина будет на нашем эллипсе, то есть вот у нас констант 2,9 на корень из трех,
ее нельзя улучшить, потому что если мы, например, посмотрим на равномерный треугольник, то у него
каждую вершину взяли дважды, то ясно, что максимальное паросочетание, это будет просто
все стороны этого треугольника, они у нас будут идти через точку центр этого треугольника, да, и вот основное
наблюдение, которое можно сделать, это то, что эллипсы у нас это выпуклые, ну, в смысле эллипс вместе с
внутренностью, это выпуклое множество, поэтому нам нужно доказать, что n выпуклых множеств пересекаются для
максимального паросочетания, вот, можно доказывать любые три из них, что они пересекаются, и поэтому можно
эквалентную формировку задачи рассмотреть, что вот пусть у нас есть шесть точек, рассматриваемые шести точек
максимального паросочетания, и тогда у нас эллипс будет иметь общую точку, вот, значит, ну, эта задача
про эллипса, она возникла в контексте довольно шокового класса, задача про диски, вот, значит, пусть у нас есть
две точки на плоскости, и через этот эксцепик мы обозначим круг, который построен на отрезке эксцепика, как на
диаметре, вот, тогда граф Тверберга, назовем граф, так что если мы на каждом либре этого графа построили диск, как на
диаметре, все эти диски будут иметь общую точку, то есть, ну, вот, про такие графы достаточно много изучено, есть
множество результатов про там разные циклы, которые являются графами Тверберга, и вот, в частности, нас
интересует, вот, в мои задачи нас интересует паросочетание, и вот, верен факт, что для любой конфигурации точек в РД есть
паросочетание, которое является графом Тверберга, вот, интересно понять, является ли одним из таких паросочетаний, для
которых пересекаются все круги, является ли одним из таких паросочетаний и паросочетаний, которые максимизируют сумму, то есть,
та же самая фактически задача, что у меня, только мы на отрезках паросочетания строим не элипсы, а круги, вот, значит,
какие результаты удалось получить. Задачу вот этого самого, которое про элипсы, удалось доказать, и вот, как бы, верно, что, ну, в общем, верен факт, что максимальное
паросочетание, действительно, все элипсы имеют общую точку. Также, вот, статья в 2019 году был опубликован результат, что для
максимального паросочетания круги пересекаются, и он был переборный, то есть, там, просто, как бы, перечисляли все разные конфигурации точек в апостоле,
какие могут быть отрезки, все они рассматривались, и доказывалось, что вот, всегда у нас круги пересекаются, ну, вот, и удалось получить, доказать
и доказывалось, что у нас есть устранение для кругов, которое значительно менее объемное, чем статья Берега. Ну, Берега, это автор для следующего
года. И вот, следующие задачи, над которыми можно думать, это аналогичное утверждение в R3, то есть, тоже рассматривать такие паросочетания, и рассматривать элипсы,
там уже другая константа будет, которая будет соответствовать правильному тетраидову, так, чтобы элипс, построенный на его стороне, проходил через центр правильного
автотрайдера. Да, вот, и интересно еще подумать, что будет, если элипсы заменить на какие-то другие выпуклые центрально-осимметричные тела, и смотреть, что на отрезках максимально, что вот, какие
свойства они должны обладать, чтобы на отрезках максимального паросочетания удалось, чтобы они тоже все имели общую точку. Вот, ну, спасибо за внимание, и вот, как бы, результаты, которые удалось получить, я ставлю в мае более конкретно.
Ну, вот, сейчас, как раз, я думаю над утверждениями в R3, может быть, там удастся что-то получить, но там сильно сложнее, что-то.
Вот так.
Да.
Почтение вида, что когда у нас осмотрится точка, то мы собираемся проделать это на, допустим, 9 нока, мы рассматриваем рупление на, не на пару, там, на камеры, пожалуйста.
Ну, нет, не рассматривались. То есть, с парами тут понятно, какие-то вот, можно взять какой-то функционал сказать, давайте, вот, в основном, просвещение, какое-то экстремальное окружение, что-то такое.
Ну, нет, не рассматривались. То есть, с парами тут понятно, какие-то вот, можно взять какой-то функционал сказать, давайте, вот, в основном, просвещение, какое-то экстремальное, которое там, в котором функционал достигает минимума или максимума, а для K, по множеству точек, там сложно что-то, как бы, подобное придумать.
Смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набьем, смарт набь
Ну, то есть, задача, она не меняет вид, кроме того, что плоскость заменяется просто на rd, и мы просматриваем множество точек в rd и какие-то просочетания между ними.
Ну и меняется констант эллипс.
Ну, как раз вот для сфер это тоже интересный результат, как я говорил, что для любой конфигурации точек есть пара сочетания, так что сферы все будут иметь общую точку, но известно, является ли максимальное просочение одним из них.
То есть, для плоскости это уже понятно, это удалось получить, а для сферы, для r3 непонятно пока что.
Добрый день, моя работа посвящена задачам геометрии чисел и их связи с системой общих представителей.
Вот, соответственно, основным объектом, с которым мы будем работать, будет решетка. То есть, решеткой мы называем, следующий объект мы берем некоторое количество линейных независимых векторов и рассматриваем их линейные комбинации с целыми коэффициентами.
Ну, и конкретно те решетки нас будут интересовать, которые содержат в качестве множества всех точек пространства с целыми координатами.
И основная характеристика решетки, которая нас будет интересовать, это ее дефект. То есть, мы говорим, что у нас в любом базисе есть решетки, которых много может быть, есть как единичные векторы, то есть те, у которых одна из координат один, остальные нули, так и все остальные.
Вот, и для каждой решетки мы смотрим минимальное возможное количество не единичных векторов в базисе. И, соответственно, это минимальное количество для решетки, это дефект.
И наша задача стоит в том, чтобы оценивать для разных классов решеток, оценивать максимально возможный дефект.
Для просто решеток эта задача не очень интересная, и мы будем рассматривать решетки, припустимые относительно некоторых тел.
Мы будем говорить, что решетка допустима относительно тела, если тело из всех точек решетки попадает только ноль.
И, в частности, удалось получить некоторую оценку для тел, которые обозначены S от PRN, то есть это сфера с центром в нуле, в n-мерном пространстве, в p-normе.
Вот, и мы хотим посчитать максимальный дефект, n-мерные решетки, допустимые относительно этого.
И удалось оценить некоторым образом верхнюю, получить верхнюю оценку, но уже был известен заранее аналогичный результат для Гиперкуба.
А Гиперкуб – это шар в существенно по равной бесконечности.
Вот, а удалось получить аналогии с известным уже рассуждением, удалось получить верхнюю оценку для максимального дефекта решетки, допустимая относительно шара в любой такой метрике.
Но для Гиперкуба еще известна нижняя оценка, то есть построена конструкция с достаточно большим дефектом, для более общего случая не удалось получить нижние оценки.
И поэтому было решено, сейчас мы занимаемся частным случаем, а именно мы рассматриваем единичный октайдер размерности n, он уже довольно во многом хорошо изучен.
И в частности известна асимптическая точная оценка для некоторого класса решеток, а именно следующих.
Если мы возьмем линейную оболочку всех единичных векторов и какого-то еще добавленного к ним вектора с рациональными координатами А, то тогда известна асимптически точная оценка на максимальный дефект решетки.
Дефект решетки как раз и показывает связь со системами общих представителей, то есть оно основано на том, что чтобы посчитать дефект решетки можно посчитать размер минимальной системы общих представителей некоторых множеств.
Каждая из множеств это множество некоторых координат вектора А, то есть мест таких, что знаменательные.
У нас стоят какие-то дроби, точнее координат вектора А, и вот если знаменатели этих дробей не делятся на какое-то простое число, на которое делится хотя бы одна из всех знаменателей, то мы сводим их всех в множество, берем все такие множества,
выбираем их минимальной системы общих представителей, и было известно, что она та и разная дефект решетки.
И эта оценка она асимптически точная, потому что есть асимптически точная оценка на размер минимальной системы общих представителей семейства множеств с задними параметрами.
И в обратную сторону тоже было известно рассуждение, которое строит систему с достаточно большим дефектом из известного примера системы с достаточно большим размером систем общих представителей.
И вот верхняя оценка тоже уже известно, что ее можно обобщить.
И, соответственно, факты тоже используют теорию систем общих представителей.
Но хотелось бы... Нет, ну это же у нас зависит по сути от самой решетки.
Да, вот. Ну, скажем так, Octoider выбран потому, что там есть... Ну, там есть решетки, там есть, собственно, у нас не только бактерии, но и в других местах.
В других местах также нам хорошо помогает Лева Минковского, который позволяет, соответственно, связать объем тела и пределитель решетки.
Вот. И, ну, Octoider хорошо считается.
Так. Всем добрый день. Я занимаюсь Андреем Борисовичем Купавским одной задачей экстремальной комбинаторики, которая называется гипотез захвата.
Ну, в общем-то, это главная тема моего исследования. Она выглядит, в общем случае, достаточно просто.
То есть, вот, что я рассматриваю? Я рассматриваю, конечно, множество от 1 до N, и мы хотим что-то узнать о системах его подможеств.
Мы называем семейство подможеств замкнутым вниз, если для любого множества этой системы все его поднося также лежат в этой системе.
Ну, и формула Егоровка гипотеза состоит в том, что для его замкнутого низ такого семейства, наибольший пересекающийся подсемейство,
Ну, пересекающийся подсемейство мы называем тема, где каждые два множества этого семейства имеют непустой пересечения.
И утверждается, что наибольший пересекающийся подсемейство, оно имеет вид все множества данного семейства, содержащие какой-либо заданный элемент.
Так, ну, в общем, как с этим работать? Что вообще известно про эту проблему?
Тут очень помогает анализ булевых функций.
Скажем так, что тут можно вести равномерную меру на булевом кубе.
Ну, очевидно, что булев куб можно представить, можно отрешить с подношествами количества множества.
Если мы введем там равномерную меру, то можно вести так называемую корреляцию семейств.
Это просто мера пересечения этих семестов минус произведение мер.
Мера самих семестов. Думаю, понятно, как не выводится.
Так, ну, и что у нас еще тут есть? Мы можем вести так называемый инфлюенс катова элемента.
Наверное, я зря это не прописал тут, но ладно, скажу на словах.
Что инфлюенс катового элемента в одного семеста множества.
Это удвоенная мера множества множества этого семеста таких, что если поменять в нем принадлежность или не принадлежность этого катового элемента, то оно уже лежать в семесте не будет.
Он обозначается и КТ от того, как мы обозначаем связь, в данном случае обозначен АТА.
И минимальный АТА это минимальный инфлюенс по всем К.
Ну и оказывается, что гипотеза Кватова просто эквивалентна тому, что сейчас вот написано на слайде.
То есть можно также заметить, что мы теперь замыкаем множество вверх, то есть с каждым множеством теперь содержится любое его надо множество в семесте.
То есть если А и Б станут по версиям под множество от 1 до n, а Б еще и антиподальная система, то есть такая система под множество, что для любого под множество 2 в степени n в квадратных скобках, оно содержит либо это семейство, либо его дополнение, то оказывается вот таких вот А и Б, оно больше или равно 1 четверти минимального инфлюенса А.
Это эквивалентная переформулировка гипотеза Кватова, там это, вообще говоря, достаточно сложно доказывается.
Но это гипотеза, но существует теорема, что если мы возьмем какой-нибудь, если мы ведем, скажем так, случай...
Вот у нас есть совокупность всех таких замкнутых версий систем под множество, и если мы будем брать случайные из них, то мотожидание вот этих вот выражений, которые написаны здесь, правда, тут, насколько я помню, еще нужно меру А сделать равной 1 и 2, то если мы возьмем мотожидание, то они будут относиться ровно так, как тут написано, то есть мотожидание корриации А, Б больше или равно 1 четверти мотожидание минимального инфлюенса А.
Это уже доказанная теорема.
Ну и, собственно говоря, еще немножко про то, как можно подступить к этой гипотезе, тут можно также рассмотреть анализ Афурия.
Афурия. Оказывается, что пространство из функций из булева Куба в действительные числа, оно и в КЛИ2, там можно вести ассоциированный базис, и если мы будем раскладывать функции по этим коэффициентам Фурии, то там все внезапно окажется очень хорошо.
То есть, например, если мы будем выражать инфлюенс ИТ-переменной этого элемента, то окажется, что он просто равен 4 суммам квадратам коэффициентов Фурии характеристической функции заданного семейства.
Но не всего, а множество этого семейства таких, что в них лежит ИТ-элемент.
Сложно выразить, тут, наверное, лучше проще написать.
Ладно, если будут вопросы, то я в конце у них отвечу.
А если у нас еще так окажется, что система множества, она еще и замкнута куда-либо, то есть вверх или вниз, то там вообще все прекрасно с выражением через коэффициент Фурии.
Там никаких сумм квадратов нет, а там просто удвоенный коэффициент Фурии множества стоящего из одного элемента И.
То есть, тут, по общей анализе Фурии, тут очень много теорем доказано, которые в будущем также могут помочь, но я их пущу пока.
Ну, я пока расскажу свои планы на то, на будущее, которое я понял осуществить в этой задаче.
Так что-то, учитывая мой метод дельта-систем, это уже отличный от анальзабуливых функций метод.
Он немножко другой, и конкретно, как в дипутсе, я еще не рассматривал его применение, но он больше такой чисто комбинаторный и аналитический.
Как в одном из прошлых глухов было сказано, это уже намного более приблизно к одной из предыдущих докладов.
И также есть так называемый метод Р распространяющихся мер.
Это тоже, про этот метод я пока что знаю совсем мало, но то, что я знаю, оно, опять же, отбыскивается к более комбинаторным рассуждениям.
И сегодня я о том, что еще могу дать коэффициент Фурье. Вот как я уже до этого сказал, что коэффициент Фурье этих множеств, которые содержат какой-либо элемент, они дают просто инфлюенс этого элемента, а то еще и лучше.
И также я еще забыл сказать, что коэффициент Фурье от пустого множества, он будет равен просто мере само этого множества. То есть, как закономерность в этом прослеживается.
Если мы какие-нибудь более хорошие, ну или не очень хорошие множества рассмотрим, то там еще что-нибудь интересное, наверное, для нашей проблемы.
Ну, на этом у меня все.
Хорошо. Коротко к Максиму. Я буду, я занимаюсь защищением комагоровской сложности с ограничениями на ресурсы.
Комагоровская сложность это объект, который довольно широко изучается. Идейный это длина наименьшей программы, которая печатает, ну комагоровская сложность данного слова, длина наименьшей программы, которая печатает это слово.
Понятно, встает вопрос, какой язык программирования, но я говорю идейно. Комагоровская сложность не является вычислимой функцией, как известно.
И это можно решить, накладывая какие-то ограничения на процессы вычисления. Можно наложить ограничения на память, можно наложить ограничения на время.
Много также исследований, где накладывается ограничение на время и машина, на которой мы работаем, является детерминированной или не детерминированной.
Я собираюсь исследовать случаи, когда машина альтернирующая, у нас есть ограничение на количество смен состояний.
У нас есть некоторые универсальные альтернирующие машины тюнинга с доступом к аракулам. Мы сразу будем рассматривать проблемы с аракулами, потому что многие утверждения связаны с комагоровской сложностью без аракула.
Для них можно показать эквалентность проблем вида P равно NP, которые решать сложно, поэтому мы сразу будем работать с добавлением аракула.
Мы можем определить сложность вычисления, сложность, которую я обозначил как C sigma N D для машины U со временем T и аракулом A.
Это длина наименьшей такой программы, что U с аракулом A на программе P на воде Y дает единицу, когда X равно Y, то есть наша программа распознает слово X.
Есть ограничение T на время работы программы. Главное ограничение, которое я добавляю в сравнение со состоянием результатов, это я ограничиваю число переходов в состояние существует для любого.
Первое определение должно происходить в существует состоянии. В аналогичном мы можем определить сложность TPN, где первое определение будет происходить не в существует состоянии, а в состоянии для любого.
Для сложностей в работе на не детерминированных и детерминированных машинах уже есть какие-то результаты, я сейчас их расскажу и расскажу как планирую улучшить.
Если у нас машины детерминированные, то сложность можно ограничить следующим образом.
Для всех слов X длины N из языка A можно ограничить через удвоенный логарифм часа элементов длины N в множестве A плюс от логарифма.
Это делается при помощи хэширования слов из A и никак нельзя улучшить этот метод для перехода к детерминированным машинам, чтобы понизить константу.
При этом если добавить время, то можно получить, что на втором уровне для A2 и P2 константу можно улучшить и оценивать сверх логарифм.
Почему такая идея возникает? Потому что удвоенный логарифм дает оценку 2N при языке A большой плотности.
А есть оценки величиной N, поэтому есть идея, что эту двойку можно убрать в некоторых случаях.
Эта идея оказывается неверна для работы с детерминированными машинами. Есть аремы, которые утверждают, что для некоторого примера есть похожая оценка снизу через удвоенный логарифм.
Вот как я сказал, если пленом заменить на что-то большее, а именно на экспонент, то уже на втором уровне эту двойку можно убрать.
По-видимому для детерминированного случая это не получается, но это тоже нужно проверить.
Есть оценки снизу. Хочется оценивать сложность при доступе к ракулу A для всех слов из этого языка.
Например, почему именно так? Это дает некоторые оценки на плотность, некоторые соотношения между классами P и NP, например, которые заключаются в существовании языков какой-то плотности,
как под языка из одного, который не являющимся языком из другого.
В общем, это обобщается на какие-то другие задачи, поэтому это интересно.
Есть следующие результаты. Теорема для детерминированного случая и ее улучшенная и распространенная версия для случая sigma1.
Хочется как-то расширить это для произвольного, но оказывается, что это невозможно, что уже для P1 есть верхняя оценка в виде логарифма для некоторой последовательности слов из языка.
Напомню, здесь было линейное, это с точностью до константа, близко к максимуму, а тут мы получаем логарифмический для некоторого под языка.
Ну и как следствие, можно получить, что то же самое верно и для всех Pn, для n больше равно 1, и для всех sigma n, для n больше равно 2.
То есть гипотеза, что подобные линейные оценки можно получить и для других альтернирующих машин, оказывается неверной.
В связи с этим ломается и другая гипотеза, которая изначально ставилась.
Хочется как-то отличить сложности при заменах альтернирующих машин. Изначально выдвигается гипотеза, что при увеличении числа допустимых переходов в состоянии мы можем как-то улучшить сложность.
Конкретно это сформулировано в виде гипотезы, которые я записал.
Хотелось бы утверждать, что для всех n и любого polynomial P существует язык A, такой же для всех слов из него.
При вычислениях с ним как с A, сложность с увеличенным количеством шагов есть у маленького, сложность с прошлого числа шагов переходов в состоянии.
Но ввиду прошлого утверждения она уже неверна для n больше единички.
Но планируется ее скорректировать и проверить при корректировке одним из следующих двух способов.
Либо можно в условии звездочка требовать только для хотя бы какой-то доли слов из языка A, либо же можно требовать его на некотором языке B, который может не зависеть.
Возможно второе более естественно, но кажется, что оно приводит к из него уволиться меньше внешних результатов.
Оно слабее и выглядит подобным, поэтому возможно, что там можно получить результат.
Из других результатов есть некоторые минорные утверждения про всякие технические переносы, утверждения, которые мы знаем для полиномиальной иерархии.
Все такие утверждения, например, об эквалентности определений можно переформулировать и перенести на сложности, которые я сформулировал.
Возможно эти технические результаты помогут в дальнейшем, но я не стал их выписывать, потому что они довольно минорные.
Дальнейшие планы описаны здесь, и в оценках сверху планируется как-то понизить константу или показать, что сделать этого невозможно.
На этом все.
Дальше все время будет логарифмическое.
То есть это в первый раз падает, а потом будет логарифмическое.
Да, это верхнее.
Там можно показать, что это утверждение ломается для эмблочений.
Там несложно получается, но логарифм не обязательно точный, но маленького уже не достичь.
Еще такой удар.
Ну, в общем, ассоциатива речи такая, по-настоящему, раз мы рассматриваем поведомство.
Я объяснял, что это приводит также к некоторым внешним следствиям.
Например, есть теорема, что существует язык ИСП, у которого нет под языков какой-то небольшой плотности.
То есть мы можем задать такое ограничение на плотность, выбрать язык ИСП, некоторые ораков.
Из двух авторов этой теоремы, например, она следует из второй теоремы на этом слайде.
В этих теоремах интересно изучать какое-то подмножество А.
В этих теоремах А потому что получается не только для подмножества, а для всего А.
Почему интересно изучать подмножество? Потому что это дает результаты по типу того, который я говорил.
Можно разделять классы языков таким способом, что не разделять, а исследовать взаимоотношения.
Мы можем как следствие из второй теоремы показать, что существует при вычислениях с некоторым оракулом язык ИСП такой, что у него уже нет никакого подмножества,
которое является языком ИСП и при этом не очень большой плотности, то есть не непримерно разреженный, но там и лучшую оценку можно дать.
Да, все здравствуйте, меня зовут Бучаев Абдукадыр. Моя тема это агентивная комбинаторика, а именно я буду изучать некоторые характеристики конечных множеств, нормы.
Давайте начнем с видения. В общем, агентивная комбинаторика у нас абелевая группа какая-то есть объемлющая.
Классическая комбинаторика изучает некоторые вопросы про мощности множества, про какие-то может экстремальные.
Агентивная комбинаторика изучает вопросы такого же рода, но в формулировке должна присутствовать групповая операция плюс.
Основные понятия это сумма и разность множеств, то есть это еще называется сумма Минковского, как все знают, но мы будем говорить про сумму и разность.
И вот изучается страшная разность, можно, то есть вот, допустим, два минус два взять.
Да, ну вот, то есть да, и вот мы сразу понимаем, что у нас появляются сразу в зависимости от того, как устроена наша групповая операция, разные множества.
То есть вот, например, возьмем альфантическую прогрессию, то есть позже мы живем в ЗТН и возьмем, допустим, там числа от одного до П, вычеты.
Вот, ну мы сразу понимаем, что между ними много всяких соотношений, но при этом если мы возьмем, не знаю, какую-нибудь геометрическую прогрессию, например, там в степени десятки в целых числах, мы понимаем, что между ними таких вот аддитивных соотношений вообще почти нет.
Вот. Далее. Мы хотим как-то характеризовать, вот, что означает, что множество аддитивно-богато, то оно не богато, это уводим вот определение энергии.
Вот аддитивная энергия наводится для двух множеств и определяется просто как количество вот решений такого-то равнения, что a1 плюс b1 равно a2 плюс b2.
Вот. Ну и в частности изучаются, когда b равно a, пишут просто и это, изучается вот, значит, такая постановка, то есть сколько вот внутри a есть отношений таких аддитивных.
Вот. Ну, аддитивная энергия она довольно хорошо изучена, но при этом есть еще старшая энергия, когда мы, значит, к тому же самому дописываем еще несколько равнений.
Да, старшие энергии, они уже определяются для одного a, ну потому что для b, в общем, там есть некоторые сложности в определении.
Вот. И на самом деле можно понять, что, ну это естественно обобщение, потому что вот вообще как считать аддитивную энергию?
Вот для множества, то есть у нас там a1 плюс a2 равно a3 плюс a4, это то же самое, что если мы перекинем a3 влево и a2 вправо, a1 минус a3 равно a4 минус a2.
Это все равно какого-то s. То есть количество решений уравнения, это что такое?
Это мы берем, смотрим, сколько, сколькими способами число s можно представить в виде разности элементов из a и возводим это в квадрат.
Когда мы смотрим на e-каты, мы понимаем, что это то же самое, только мы возводим эту увеличенную не в квадрат, а в катую степени.
То есть таким образом это в некотором смысле более старший момент, то есть вот мы же знаем, да вот, из теории вероятности, например, что старший момент звучать полезно, да, то есть вот, что не только там от ожидания квадрата, но еще и кубы, и табяли.
Вот так вот. Теперь дальше, вот это одна из характеристик, энергия, точнее класса характеристик.
Следующая это норма Гаурса, катаформула, конечно, да, изначально вообще непонятно, что означает, вот, но давайте вот посмотрим, что, что это такое для, значит, ну для k равно там 2 или 3.
Пускай, допустим, k равно 2, да, то есть вот у нас, вот смотри, у нас есть произведение по epsilon из 0.1 в квадрате.
Да, еще оператор комплекса на сопряжение, но вот внизу написано, что мы нормой Гаурса, не функция, а конечного множества, называем нормой Гаурса у характеристической функции.
Характеристическая функция она принимает значение только 0 и 1, вещественное число, поэтому сопряжение можно про него забыть в нашем случае.
Вот, то есть там написано, что a от x, a от x плюс s1, a от x плюс s2 и a от x плюс s1 плюс s2, то есть мы, по сути, берем такой вот аддитивный квадрат, x, x плюс s1, x плюс s2, x плюс s1 плюс s2, вот.
Соответственно, да, и вот суммируем по всем x-алам, по всем s, то есть по сути, вот эта норма Гаурса, она показывает, сколько у нас множества таких квадратов.
Когда мы берем k равно 3, то вместо квадратов у нас кубы, ну и так далее, в общем, да.
Что интересного можно сказать про эту величину, на самом деле, когда k равно 2, то у нас совпадает с аддитивной энергией,
а когда k равно 1, то она совпадает с квадратом мощностью множества.
Вот, при этом, значит, энергии, то есть e1 это тоже считается, что первая энергия это просто квадрат множества, а вторая энергия это аддитивная энергия, то есть они вот на первых двух совпадают.
Вот, теперь в чем же, значит, была задача?
Мы изучаем множество а-а. Почему его логично изучать?
Потому что если а само по себе был аддитивно-богатым, то есть внутри него было много соотношений, там, например, а был прогрессией,
то а-а будет тоже аддитивно-богатым, потому что если из прогрессии вычесть само себя, то получится тоже вот прогрессия.
При этом если а не аддитивно-богатая, то когда мы рассматриваем а-а, то мы как бы насильно добавляем много соотношений.
Это означает, что множество а-а, по идее, должно быть в любом случае аддитивно-богатым в некоторой степени.
Ну и, значит, в одной статье Шкредов мой научный руководитель доказал, что третья энергия d в некоторых приположениях, то что аддитивная энергия а меньше, чем а в кубе на к,
с некоторой консантой, то что тогда третья энергия множества d будет, снизу у него оценка, это к в степени 7 четвертых на а в четвертый.
Вот. И, значит, моя задача была основной доказать что-нибудь подобное не для третьей энергии d, а для ката энергии d.
Вот. Да, кстати, немножко про энергии, давайте я вернусь назад.
Вот. Из определения ката энергии видно, что на нее оценка сверху, тривиальная, это а в к плюс первой степени, потому что в этом а1, а2 и ак, вот все эти,
зафиксируем какими угодно, тем самым уже а в ката вариантов получили.
Далее а1 штрих положим еще чему-нибудь, и тогда у нас все остальные а2 штрих, и так далее, ак и штрих тоже зафиксируются.
Вот. То есть оценка сверху это а в к плюс первой, а оценка снизу это просто а в катой, потому что можно а1 положить равным а1 штрих, 0 получится.
И так здесь во всех уравнениях. То есть у нас я ката, они живут от а в катой до а в ка плюс первой.
Вот. Да, и соответственно получается, что третья энергия от d, у нее оценка сверху, это d в четвертый, и в аддитивно-богатом случае это то же самое, что а в четвертый, то есть то, что это константа.
А мы, получается, доказали, что, точнее, было доказано, что это оценка снизу тоже, то есть а в четвертую какую-то константу.
Вот. Это была первая цель. Да, вот. Да, после этого, что еще нужно сделать? То есть оценка есть, нужно проверить, насколько она точна.
После этого можно, да, после этого нужно рассмотреть, естественно, обобщение энергии, это норма Галлса третья, и доказать тоже что-нибудь похожее.
А дальше проверить точность на разных классах множеств. Да, вот класс у нас в аддитивной календарике их несколько.
Основные это прогрессии, как аддитивно-богатые, потом аддитивно-небогатые, это случайное множество.
Между ними бывает малость на отношении. И наконец, ну еще есть такой промежуточный вариант, это подпространство плюс дезинсертивное множество,
но можно понимать его просто как случайно множество. То есть пространство какое-то, подпространство, плюс случайный шум.
Вот. Да. В общем, какие у меня результаты? Оценка есть на карту энергии от D. Она вот такая чуть страшной получилась.
Вот. Да. А малая это мощность, а просто. Так же в ней возникнула бит энергии, которую вот я обозначу как E к и к-1.
На самом деле можно еще там сверху оценить и получить оценку только на энергию от D. Вот.
Но я пока не стал этого делать, потому что то есть это новый вид энергии.
Вот тоже можно интересно изучить без того, чтобы терять точность. Возможно лучше пока оставить как есть и потом посмотреть.
То есть вот. Да. Дальше что еще по продвижениям? У меня уже есть оценка для третьей нормы Гаурса.
Но пока я не стал ее писать, потому что я не уверен, что она закончена.
В каком плане? То есть я еще планирую проверить точностью. Вот. Если точность совпадет, то оставим как есть.
Если нет, то придется немножко доработать. То есть она скорее всего в адестивно-богатом случае точно совпадает.
То есть, ну, точна. Вот. Но в случайном случае, когда у нас случайно множество, я пока там не уверен.
Вот. Даут. Дальнейшие цели. То есть доработать эту оценку, проверив ее точность и подкорректировав, если нужно.
Даут. Кстати, если точка сходится, то задача может стать решенной. То есть мы доказали точную оценку.
Да. Ну и на самом деле еще, в общем, когда закончится норма Гаурса, я планирую вернуться к оценке энергии,
потому что мне кажется, что я там очень много где грубо и нереально использовал.
Вот. И где-то можно оценить потоньше и лучше результат получить.
Ну, такие цели. То есть в целом у меня уже нереально готовы все. Осталось только там проверить, насколько они хороши.
И если это возможно, как-то из инструментов выжать максимум и доработать их. Уточнить.
Все, спасибо за внимание. Если есть вопросы, кто-то есть.
Да.
Да.
Не, не, не. Ну, кстати, да, я забыл сказать, что E это, просто E это вторая энергия, не первая.
Вот. Ну, это такое ограничение. Не очень. Ну да, по сути. То есть у нас на самом деле есть оценка,
то, что E это больше оборудование овку поделить на K. Вот. А тут меньше-менше. То есть это символ винограда.
То есть существует некоторая констанция.
Ну, да, но это энергия не от множества, а от разного множества. То есть D, а-а.
Ну, в целом, да. Ну, то есть там не на самом деле со случайным множеством все не так понятно,
потому что да, там есть небольшие сложности с подсчетом. Если а случайно, то а-а чуть тяжело считать.
Вот. И соответственно, это константа двоения тоже там, она в общем для случайного множества не такая тревожная.
Ну, в целом, да. То есть там такая, что, в общем, так называется это неравенство, там отдельно доказано одно неравенство
для отдельно богатого случая, отдельное неравенство для небогатого случая, дальше не скомбинируемое, и получается такое вообще.
Вот. Ну, в целом, да. Ну, да, в общем, философия в том, что для D должна получиться большая энергия,
для D мы его насильно делаем интимным богатым. Вот.
Да.
Ну. Ну, да, да. В смысле, разговор, потому что результат ужасен. Ну, да, есть такое. Не, на самом деле.
Заново? Ну, такое и хотели. Ну, то есть. Нет, просто вопрос в том, что.
Ну, проблема в том, что там есть вот этот новый вид энергии. E к, E к-1. То есть можно еще чуть-чуть оценить через E к,
и потерять в точности. Вот. Потому что там важна не только симптатика А. То есть А в степени, там, К квадрат, еще что-то там.
Но у нас еще там стоит К большое, констант удвоения. И мы также немножко за ней уборемся, то есть какой там показатель в степени и так далее.
Вот. Потому что от него много зависит. Потому что, допустим, в небогатом случае, допустим, возьмем геометрическую прогрессию, да.
У нас А это у него, допустим, мощность стен. Тогда у А-а будет мощность примерно N в квадрате. Правильно? То есть этот К большое, оно может быть по порядку где-то А.
То есть за него тоже надо бороться. И да.

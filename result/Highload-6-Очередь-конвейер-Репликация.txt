Так, архитектурный паттерд очереди. Значит, общий смысл заключается в том, что у вас есть
хранилище, которое может быть специализированное, может быть самое-самое обычное, самое простое. Любое
хранилище, в которое вы пишете данные и извлекаете их в том же самом порядке, в котором записали. Эти
данные могут быть задачки, сигналы, еще что-то, обычно какие-то задачки, какие-то таски. То есть
есть очередь, в нее кто-то пишет, из нее кто-то читает, и эти данные обрабатывают. Читают их,
так называемые воркеры. Бывают очереди различных приоритетизаций, еще что-то параметрически и
так далее, так далее, так далее. Нам по большому счету не важно. Для чего нужен паттерд? Все очень
просто. У вас гарантированное количество воркеров, например, 1, 2, 3, 4, 5. Вы точно знаете,
сколько им нужны ресурсов, точно знаете, как они работают. А что происходит здесь, неизвестно.
Да, то есть здесь может быть какая-нибудь там пиковая нагрузка, здесь все ровно. Вот это вы
можете делать тогда, когда вам удобно. Вот это вы делаете тогда, когда удобно пользователю. Это
первый механизм использования, второй механизм использования. То, что выполняемая работа может
быть долгой. Перекодировать видео. Вы не кладете сюда видео, вы кладете сюда задачку. Перекодируй,
сходи, пожалуйста, вот сюда, вот это хранилище. Возьми вот это видео, перекодируй его, положи его вот
туда. Вот это так звучит задачка. Воркеры ее выполняют. Очень часто используется это исключение. Это первая
вводная задача, которую мы сейчас с вами будем обсуждать. Вторая вводная задача, которую мы
сами будем обсуждать. Сервисно-ориентированная модель. Сервисно-ориентированная модель построена по
принципу, потому что у вас есть всю вашу систему, вы делите на несколько сервисов.
и каждый из этих сервисов обладает какой-то своей внутренней логикой, своей внутренней
архитектурой, своими базами данных и так далее. Например, сервис фотографии построен совсем по
другому, нежели сервис постов. Сервис фотографии — это большие диски, много места для того,
чтобы хранить фотографии. Сервис постов — это SQL-база данных. В фотографиях база данных используется
для хранения фотографии баз данных с пользователями. И, соответственно, у вас отдельный сервис
фото, отдельный сервис паспорта. Это прям отдельный сервис, это чуть ли не отдельный сайт,
то есть они как бы выглядят в полностью отдельной сущности. Принципиальным является сделать так,
что вот такие вот связи запрещены. Нельзя из программы входить в базу данных другого сервиса.
Почему? Потому что не вы его разрабатываете, его могут разрабатывать другие ребята, они не обязаны
вам ничего говорить и так далее. То есть это как раз делается, вот это правило вводится для
облегчения совместной работы. Если за каждую команду отвечает за свой пустот, то все нормально.
Они как бы договорились о том, как они друг с другом общаются и внутренним друг другу не
поняли. Если сервис постов решает поменять систему хранения там, не знаю, с базы данных на файлы,
им никому об этом не надо говорить. Как они общаются? Они общаются вот так. Через некий
маппи интерфейс. То есть каждый из них говорит, ребят, ко мне можно приходить и вызывать у меня вот
такие-то, такие-то, такие-то, такие-то методы. Каким способом приходить? Как угодно, например.
Самый такой простой способ. Дергать по HTTP, выполнять дикт-запрос. Вот точно так же,
когда вы набираете вконтакте, вы выполняете дикт-запрос с сайта www.vk.com. Точно такой же дикт-запрос
выполняется каком-нибудь методом. Просто мы договорились, что, например, я не знаю,
фото.facebook.com.slash.upload.photo это некий метод, куда нужно передать фотографию и она будет
загружена в хранилище. Все. И все остальные сервисы знают, что если нужно загрузить что-то в
хранилище, они вызывают фото.facebook.com.upload.photo. В принципе, понятно? А теперь давайте сделаем
следующее. Давайте придумаем, построим систему общения. Понятное дело, что это как бы внутреннее,
внутреннее для надействия. Понимаете, да? То есть это не идет через глобальный интернет. Это какие-то
внутренние, внутренние сети, еще что-нибудь, еще что-нибудь. Может быть, общение по РС. Нам не
принципиально. Такого нам не принципиально. Как сделать так? Вот у нас есть сервис постов и сервис
рассылок. Пришел новый пост. Нам нужно опубликовать, разослать тем друзьям этого пользователя
информацию о том, что этот пользователь сделал новый пост. Сервис постов, сервис рассылок. Давайте. Как это сделать?
Понятно, почему я выделил в отдельную историю. Потому что сервис рассылок может отправить, ну то
есть, например, этот пост может опубликовать Zuckerberg. Тогда нам нужно сделать 100, отправить 100 миллионов
писем. Это может быть долго. Zuckerberg не дождется. Даже по блату не дождется ответа. Нужно что-то делать.
Давай. Как? У каждого из этих сервисов есть некий акт?
Кто конкретно, кому дает ответ? Давай. Вот так. Пришел новый пост. Итак, вот этот чувачок, сервис
постов вызывает сервис рассылок. Да? Как? Значит, сервис постов опубликовал себя, сохранил у себя пост
где-то там у себя внутри. И вызывает метод сервиса рассылок. Какой? Расслать подписчикам такого-то
пользователя такой-то пост. Окей. Расслать. Первый. Почему эта штука? Что внутри сервиса рассылок
происходит в таком случае? Он сразу начинает рассылать. Итак, у него есть внутренняя очередь.
То есть у нас с вами есть какой-то внешний аппен и есть внутренняя очередь. Есть какой-то
оппен. И так вот, соответственно, вот эта штука идет сюда. Это идет сюда. Это идет сюда. Так?
Вот этим, да? Отправляем сразу, чтобы принята задача. Верно? Окей. Какие минусы?
Чем можно сломаться? Не, ну а как она зависит? У нас как раз вся эпишка, что ничего не зависит. Это один
интерес, это другой. Ну придет. Фейсбук делает очень тупо. Он вышли там через, на следующий день получат ответы.
Ответ. Ну дайте там где-нибудь откроем. Мы же сдохли. Слушай, они же открывали. А, ты специально
закрыл для того, чтобы он не сломался. Так, в чем проблема в этой схеме, ребята? Не все продумано. Она может
сломаться. Давай считать, что есть. Будем считать, что у нас какая-нибудь персидентная со сбрасыванием
на диск. Когда-нибудь выполнится? Много задач. Мы сами что будем делать? Мы сами мониторим длину этой очереди и следить за тем,
чтобы она не была не слишком длинной. Как в Москве Ирия Ивановна. Куда едут и едут, и все нормально.
Запускаем, никакой блокируемой. Запускаем второй в горку и начинаем разбирать в два раза быстрее.
Понятия не имею. Сколько угодно можно сделать. Смотри, обычно это делается так. Вот ты запускаешь эту историю, а очередь для
того и нужна, чтобы вот это вот сглаживать, вот эти вот пики. Вот ты мониторишь, у тебя средние показания за сутки,
например, растет и за месяц вырос, что-то, не знаю, с пяти сообщений до 15. И вот тогда ты принимаешь решение,
что запускаешь решение в горку. Понятно, что это будет скакать. 0, 0, 10, 10, 15, 20 и так далее. Ты ориентируешься
на среднее значение и на максимальное. Максимальное, чтобы не выходило вообще за грамотеразумного, чтобы
не было каких-нибудь там, не знаю, многодневных ожиданий. Ну и на среднее. И запускаешь новые ворки.
То есть, они не так, что прям всего сейчас запустит, через час вырубит. Да сколько хочешь запустить,
не важно это вообще. Два, три, пять. Обычно по одному и не запускают. Так сразу запускай несколько штучек.
Хорошо, запускай. Я не против, запускай. Тебе выделили под эту штуку сервак, запускай на нем столько
воркеров, сколько сможешь? 5, 10. Не суть важна. Почему? Запускай одного воркера, он будет ходить с другой машиной,
выдели очередь на одну машину, воркеры на другую машину. Нет, не страшно. Такая же какая-то задача
удалить рассылку из очереди, если она еще не прошла. Ребята, эта схема не заработает. Почему?
В этой схеме есть проблемы. Какие? Вот это вот Кринь принимает у нас внешний интерфейс нашего
вот этого сервиса рассылок. Нет, этот наш воркер берет задачу разослать пользователям,
подписчикам, пользователя, такого-то информации о таком-то посте и начинает ее выполнять,
потихонечку ее рассылать. Мы сейчас внутрь того, как он это делает, не пойдем. Не пойдем. Нам нужно
научиться коммуницировать между вот этим сервисом и вот этим. Хорошо. Спрашиваем,
если этот сервис сломался, сломался, не отвечает. Он не отвечает. Вот этот афки интерфейс не отвечает.
Вот этот сервис постов к нему стучится, а тот молчит. Мы не знаем почему. Связь пропала.
Леха, в программистской ворочке что-то выпустили неправильно. Он не работает.
Как ты перед постем? Постя 10 минут ждет? Постя у тебя 3 минуты.
У тебя 3 секунды. Можно? Как ты предлагаешь, можно. А, ПКМАУТ не сработало, то что?
Но только без условий. Сразу положим локальную очередь. Пойдем в локальную очередь.
Нет, нет, нет. Орбита у тебя вот здесь. 2 сервисы. У тебя может быть сеть пропасть.
И как бы обхорбиться не будет работать все. Здесь и входящий, не только исходящий сервис,
сервер запросов. Тоже по большому счету вортир. Это вортир. Хорошо. Мы кладем пост. Мы не
отправляем его сюда. Сразу в внешнюю. Кладем его в нашу локальную исходящую очередь.
Не обязательно локальная машина. В нашем сервисе. Она расположена в нашем сервисе. Мы ее
контролируем. Мы знаем, что с ним происходит. Мы понятия не имеем, что происходит во внешнем
сервисе. Мы понятия имеем, что происходит там. Но мы знаем все, что происходит здесь.
Мы это контролируем. Это наше. Поэтому мы кладем в локальную очередь. И отсюда вортиры,
которые отвечают за то, чтобы отправить информацию куда-то еще. Отвечают за то,
чтобы отправить информацию во внешние сервисы. Берут задачки и отправляют. Итог после того,
как отсюда пришло принято, удаляют ее из очереди. А если не получилось,
оставляют ее в очереди и возвращаются к этому через какое-то время.
Можно. Это усложнить тебе несколько логику, но можно.
Hardbit это конкретное название технологии, которое позволяет объединить два сервиса,
два сервера под одним IP адресом. И работает во внешний мир один, а второй тупо стоит,
работает, но не отличает. Во внешний мир он не смотрит. Он только мониторит первый. И если
первый сломался, то второй говорит, а теперь я. И поток в тот же самый переключается на другой
IP адрес. Вот это просто общая схема. Так, казалось бы, все нормально? Добавили сходящую очередь, да?
Еще раз? Вот здесь. Как только мы в локальную базу данных, мы записываем пост в локальную базу данных,
мы записываем задачу в исходящую очередь. Сразу точно, что все хорошо. Нет. Первый путь
черевах тем, что мы ломаемся, если внешний сервис нам не отвечает. Мы не можем так работать.
Работает у нас вот эта очередь. Наша информация не теряется. Мы отвечаем за себя. У нас ничего не
теряется, у нас ничего не пропадает. Когда заработает, тогда мы отошлем. Репост публикуется нами, он не
расцелается. Зачем она так говорит? Ну он же умер, хорошо, он умер, но это внештатная ситуация. Мы можем
рассчитывать на то, что он в принципе поднимется через какое-то время и письмо уйдет. Мы ему об этом не
говорим. Если ты посмотришь Facebook, об этом не говори. Просто через какое-то время присылать. Без записи, так скажем.
Точно так же, ребят, у вас может быть, у вас может быть что угодно. У вас может быть вместо рассылки у
вас может быть какая-то статистика, какая-то индексация, обработка видосиков, если в постели видосики
есть и так далее. То есть вообще здесь может быть, например, какая-то параметрическая очередь с
разными типами задач и каждый исходящий сервис отправляет в разные сервисы. Каждый исходящий
отправляет свой тип задач, обсужит свой тип задач, отправляет их в разные сервисы. Вот примерно так на самом
деле. Вот это, это некая классика. Так раньше делали. Это неубиваемая конструкция. Если этот сдохнет,
как бы никого это не коснется. Если этот сдохнет, никого это не коснется. Данные не теряются. Общий
принцип простой. Ты удаляешь данные от тебя, удаляешь задачи от себя, только после того, как тебе
внешняя сторона прогарантировала, что она задачу приняла. Все. Сейчас есть такая история, которая
называется Worker-сообщение. Вот это все? Вот это все? С исходящей очереди. Потому что здесь у нас
Worker-ы, которые ну как бы... Здесь нам тоже нужно учить, потому что задачи могут выполняться до...
Заменяется на Broker-сообщение.
Профтинка, которая делает примерно то же самое. Некая коммуникационная среда между различными
сервисами, которая гарантирует, что информация будет передаваться из одного места в другое.
То есть либо вот так, либо через Broker. Так, а теперь смотрите. Вопрос. Внимание, вопрос.
Хорошо. Внутрь сервиса рассылки пойдем. Вот у нас есть сервис рассылки. И вот мы получили задачу.
Есть очередь. И в эту очередь попала задача разошли вот этот пост по всем друзьям вот этого
пользователя. Пост пользователя. И вот здесь у нас Worker. В чем опасность этой схемы? Нет. Нет. Нет. Нет.
Подавляться с постами у нас здесь. Это рассылщик. Он берет каждого друга пользователя,
Цукерберга, например, и формирует письмо и отправляет каждому другу. Посты сохраняются вот здесь.
Вот у нас сервис постов. Это сервис рассылок. У них разные вещи. Мы просто хотим, чтобы про каждый пост
узнали друзья.
Клиент там где-то здесь. Да, где-то в сервисе постов. Вот сервис постов хранит подпы, хранит ленты и все прочее.
Рассылщик не хранит эти посты, они нахрен не нужны, он разослал, забыл. Итак, внимание, вопрос.
Вот эта схема, чем плоха? Какую здесь видите опасность?
Первая проблема.
Еще.
У Цукерберга, напоминаю, 100 миллионов друзей.
Сколько времени будет рассылаться 100 миллионов писем?
Я вам отвечу.
Сутки. Несколько суток.
То есть проблема у нас в том, что если мы делаем вот так очередь с задачами рассылщик, то у нас задачи могут быть разные.
Одна выполняется за минуту, другая за сутки. Это может быть неприемлемо.
А может быть и больше.
Так, давай подожди. Кто, где, чего разбивает?
Давай алгоритм конкретный.
Ну ладно.
Так.
Так.
Они не будут в порядке.
Ну ладно.
У нас проблема, ждут-то ладно.
Мы для этого очередь и вводим.
У нас проблема в том, что задача,
она будет выполняться очень долго.
Друзья Цукерберга будут самыми притесняемыми пользователями сети.
Ну по бачам все правильно.
Короче, приходит задача.
Вот здесь она звучит.
Вот этот вот пост, вот этот вот сервис наш, сервис постов.
Он понятия не имеет, что там внутри у сервиса рассылок.
Он ставит задачу.
Вот пост, вот пользователь.
Рассылай его друзьям.
Все.
Сколько этих друзей?
Он не знает ничего.
Он не знает, что длинная процедура или не длинная.
Это его не касается.
Он поставил задачу.
Она прилетела в самую подходящую очередь.
А потом начинает все обрабатываться.
Первый воркер.
Берет пост, берет пользователь.
Дальше, ну что он ждет?
Выяснить сколько у пользователей друзей.
Где он идет?
Он идет к сервису друзей.
Он идет к сервису друзей
и точно так же спрашивает.
Сервис дружбы.
Вот мне подсунули вот этого пользователя.
Отдай мне всех его друзей.
И тот ему говорит.
Отлично, держи.
Вот тебе миллиончик.
Что делает этот ворк?
Он ставит следующую, блин, задачу.
Следующую очередь.
Которой уже немножко по-другому.
То есть, если здесь у нас задача звучала.
Разошли вот этот пост
друзьям вот этого пользователя.
То есть, здесь задача уже может звучать.
Разошли вот этот пост
вот этому человеку.
Но этому человеку это правильно?
Пачка, конечно, пачка.
Потому что если этому человеку,
то мы просто другую проблему себе создали.
Мы же сделали миллион задач.
Например, пачка людей.
По 100, по 1000.
И здесь уже другой воркер.
И здесь их может быть уже больше.
Эта очередь у нас входящая
с одним типом задач.
Это очередь там, не знаю, внутренняя
с другим типом задач.
Например, так.
И вот этот уже выполняет задачки,
которые не кажутся максимально,
ну, страшными.
Ну, по 1000 штук, ну окей,
потратим минуту, потратим, так не знаю,
две минуты, три минуты.
Это нормально.
И тогда, когда придет Сукерберг,
то у нас будет некоторую проблему,
нагрузку, у нас появится здесь сразу много задачек.
Одна задачка отсюда будет
сконвертирована там, не знаю, в тысячу задачек сюда.
Но это все равно довольно быстро переварится.
Но это переваривается,
в всяком случае, примерно равномерно.
То есть это не будет...
А до этого у нас была последовательная
рассылка всему этому миллиону.
А сейчас мы это делаем
в то количество потоков.
Сколько у нас здесь воркеров?
Ну, ты можешь воркеров поставить 100.
Потому что у тебя задача одна.
Понимаешь?
Она просто огромная.
Тебе нужно засплитить.
На кусочки, ты ее сплитишь на кусочки
и рассылаешь каждый кусочек отдельно.
Ты сделаешь из него маленькие
ратируемые задачи.
А вот этот паттерн как называется?
Мы его проходили.
Мы его проходили.
Конвейер, господи.
Обычный, самый обычный конвейер.
Когда вы берете задачи, например,
или какие-то данные,
и начинаете их выполнять,
обрабатывать несколько этапов,
на каждом этапе обрабатывается своя обработка.
Давай.
Вот так.
Можно попробовать смешивать
данные с задачей,
чтобы на каждом этапе
обрабатывать одну паттерн,
чтобы он не сидел и отдал
сумки какой-то.
Можно.
Как решить эту задачу?
Во-первых, нужно прийти
к менеджеру и сказать,
нужна ли какая-нибудь гарантия
по отправке этого письма?
У тебя могут быть разные ответы.
У тебя может быть ответ, например,
если банк-клиент,
какой-нибудь банк
высылает тебе письмо
о том, что твои деньги списались,
ты должен сделать это прямо сейчас.
Это нельзя ждать.
А я не знаю, пока
что-то пройдет.
И очевидно, что это разные будут сервисы рассылок
для банковской истории
и для вот этой.
Или разные очереди.
А может тебе менеджер сказать,
да пофиг, через сутки,
через три часа или через час,
не важно.
Менеджеры пейзбука, насколько я понимаю,
сказали так, что же нет никакой гарантии,
когда это письмо придет о том,
что кто-то тебе что-то написал или прокомментировал.
Может прийти сразу через час,
через два, через несколько дней.
Но если
скажут,
в течение часа.
Что у нас меняется?
Менеджер сказал, что письмо
о новом сообщении, о новом посте
гарантированно должно прийти в течение
часа.
А менеджер приходит и говорит,
это полная хер.
Давай переделывать
в течение часа.
Как вообще это сделать?
Нет.
Нет.
Я не справился,
поэтому выполнять уже задача
не получилось.
Не получилось.
Давайте так,
смотрите, вот у нас есть два варианта
этой подзадачи.
Первый вариант подзадачи,
те сообщения должны быть
отправлены в течение часа
даже для Цукерберга.
Не помогут тебе приоритеты,
потому что для Цукерберга тоже.
Если менеджер говорит,
что любое сообщение
в течение часа должно быть отправлено
о любом посте, хоть там
100 Цукербергов,
то вы занимаетесь математикой.
Вы вычисляете производительность
каждого вортера, производительность своей
сети и так далее.
И вычисляете, что может произойти.
Сколько Цукербергов одновременно могут
записать посты.
Смотрите статистику, сколько это было раньше,
берете какой-то максимум,
увеличиваете его в 5 раз,
делите на производительность одного вортера
и получаете нужное количество вортеров.
То есть, если менеджер
идиот, то вы решаете задачу
в лоб.
Вы приходите к нему и говорите,
без проблем, говно вопросно, пожалуйста,
мне нужно 100 серверов
под вортеры.
Когда вы так скажете, менеджер говорит,
ну слушай, ладно,
давай так, давай всем
гарантированно, Цукербергов можно
помурыжить.
Да. Как будете делать?
А как ты это сделаешь?
А? Мы вот здесь,
вот на вот этом этапе мы сняем здесь
бакс, херак и миллион друзей пришел.
Да.
Например, да.
Да. Либо как-то
играться с очереди приоритетов,
либо отдельная очередь для Цукерберга.
Нет.
Не надо, я кажется, Цукерберга.
Вот просто очередь для больших ребят.
Очередь для нормальных.
Для этих.
Ну, для популярных
и для неинтересных
скучных подписчиков.
А?
Две очереди
и у тебя вот на эту работает, например.
Я не знаю. 70%.
А на эту
30.
Не, вот эти 70
работают только на эту.
И берут задачи только отсюда.
А вот эти 30 работают только с вот этой.
30% воркеров, да.
Не, не, не. Не со всеми.
Ты правда.
Вот так.
Может.
Может. Давайте сделаем
их поумнее.
Что если вдруг его очередь
не уста, то помочь соседу.
Цукербергу.
Или наоборот.
Если Цукерберг улетел
на Марс и не постит ничего,
связь нет. Берем, работаем с обычным.
Простая задача, да, письма
разослать.
Но, в принципе, если вы
логику поймете, оно вот
все вот из таких вот, есть хайлот,
он из таких кусочек создает.
Очередь, воркер, опишечка,
очередь, воркер, опишечка.
Много воркеров, много
серверов под API и так далее.
Что вы будете делать, если, например,
запросов к сервису рассылщиков
очень много. И вот этот вот
веб-сервер, который
у нас задачки принимает, который API
во внешний мир выставляет, он не справляется.
Давайте сюда еще.
Масштабируйте.
То есть, как бы, принцип один и тот же.
Масштаби...
Газотонтальное масштабирование.
Так.
Едем дальше.
Давайте зажимаем.
Давайте зайдем
в область баз данных, немножечко
в ней...
помученец.
Итак, базы данных.
Все, что вот мы сейчас
с вами проходили, это было...
Как сказать?
Это было масштабирование
бэкэн.
Масштабирование
бэкэнда. Да, еще что-то.
У нас есть с вами некая
общая для всех часть,
которая называется
хранилища данных.
И вот с ним мы сейчас с вами будем
участвовать. Обычно в роли хранилища данных
выступает какая-нибудь база.
Не знаю, MSQL, PostgreSQL,
Oracle, что-нибудь еще и так далее.
Понятно, MSQL
проект, не суть важна.
И те бэкэнды ходят
к нему.
Нам нужно с вами
понять...
Все бэкэнды ходят к нему.
И если бэкэнды мы с вами разобрались, как масштабировать,
нужно просто стремиться к тому, чтобы
было как можно меньше
общих частей.
Нужно стремиться к тому, чтобы они не хранили
состояние, они не менялись.
Чтобы запрос-то было не важно, на какой из бэкэндов
прийти. Не было никакой там, не знаю,
внутренней истории.
Как быть с базами данных?
Они будут писать в одну базу данных.
И она в конце концов сдохнет.
Она не перестанет справляться.
Нам ее тоже нужно масштабировать.
То есть тоже
научиться базу данных
держать на нескольких серверах.
Распределенная база данных.
Как они устроены?
Понеслась, сейчас будем
все это изучать.
Сначала место слов.
Ключевые.
И
сейчас скажу
еще одно слово.
Еще одно слово,
которое называется
А что вы имеете в виду под бэсь?
Да. Нет.
Нет.
Итак.
Что это?
Про транзакционность.
Да, все верно.
Ашечка, про что?
Атомарность.
Что это значит?
Трантакция полностью
либо выполнена, либо не выполнена.
Трантакция состоит из нескольких кусочков.
Из нескольких запросов.
Она либо полностью выполняется, либо полностью не выполняется.
И
Консистенция. Что значит?
Не противоречивое состояние.
Да. Если транзакция выполнена,
она выполнена окончательно.
И так далее.
То есть база данных
согласована как до, так и после транзакции.
Ай.
Трантакция выполняется
может выполняться параллельно
в своем кусочке.
Как реализуется
Ай.
Она как раз реализуется с помощью вот этой вот фигни.
Эта штука
расшифровывается как
Multiversion Concurrency Control.
И обеспечивает
вам то, что у вас каждая
транзакция
выполняется в своей копии
базы данных.
Реально это конечно же не копия.
Но там сложнее конечно история,
не просто снапшоты.
А-а-а-а-а, вот
отсюда следует одна из проблем.
Если вы вовремя
транзакцию не закрыли,
не закрываете,
то через какое-то время у вас все будет очень грустно.
Самая ошибка
новичка.
Если ты не закрыл вовремя транзакцию,
соответственно вот база данных,
которая поддерживает транзакционность,
продолжает следить за тем,
была своя изолированная копия. Другие нормальные ребята, которые умеют программировать, транзакцию
закрывают, база данных изменяется. Твоя версия конкретная, не закрытая, транзакция постоянно пухнет,
пухнет, пухнет, пухнет, пухнет. База данных тратит все больше, больше, больше ресурсов.
Не факт, может быть это там постоянно ты на горшечке написал такой весь модный и транзакцию не закрывает.
Бесит вечно. Тайм-аут по транзакции, может быть, да, хорошо, если есть таких гавриков с неверной
работой транзакций много, то все вообще грустно. Вообще транзакции довольно стрёмные. Первое,
что смотрят ребята, которые по базам данных работают, консультируют, это транзакции,
нет ли открытой транзакции, не висит ли не закрытой транзакции, все ли там нормально,
это самое ключевое. Так, Лады, Д, что означает? Да, если мы ответили, слово пацана, если ответили,
что записали, значит записали. Так, капте аремы. Я понимаю, что выберете два из трёх. Рашутовый.
Консистентность, наступность. Разберёмся, что означает, давайте разберёмся с вот
этого волшебной буквой QP. Partition Tolerance, то есть, грубо говоря, если вот эта распределённая
система хранения состоит из нескольких узлов, то она устойчива к тому, что какая-то из вот этих
связей пропадёт. Или какой-то пакет не дойдёт. Теперь внимание, вопрос. Вот это, в принципе, реализуемо.
Не, у тебя вот здесь два компьютера объединённых сетью. Unix, компьютеры, надёжная сеть. У тебя есть
гарантия, что пакет будет пелен. Нет. Короче, выдержать, то есть, как бы не выдерживать пешечку, то есть,
как бы выбросить её из разговора. Вы всегда должны о ней помнить. Любая система, которая будет
разрабатываться, и буквка P будет по умолчанию. И выбор у вас, на самом деле, либо вот этот, либо вот этот.
P будет, потому что вы в интернете. Если вы хотите убрать эту P, то у вас что? Вот эта штука вырождается
в один сервис. Вся распределённость пропадает. То есть, у вас либо, ну, потому что если у тебя
частей нет, то и, собственно говоря, выдерживать и думать над тем, чтобы у тебя эта штука работала.
Но это у тебя вся база отвалилась. То есть, у тебя нету коммуникации внутри твоей системы.
Приведите пример с ТА. Ну, консистентность и датук.
Подгрес на одной машине. Ну, на одной машине, да. Нет, на все три. Все три ты не выполнишь. Если ты на одной машине,
то ты автобантом не выполняешь. У нас получается что? А, да. Если у тебя кладётся машина, то всё.
Ты не можешь гарантировать ашечку. Нет, если кладётся машина, и у тебя сервис не отвечает во мне.
P — это внутренняя история. То есть, как бы, система продолжает работать, чтобы у вас внутри не происходило.
Если у вас один блок вылетел, там, не знаю, канал вылетел между ними, связь разрубилась и так далее.
Чисто внешне это всё равно выглядит надёжно. Вот это, собственно говоря, за это отвечает ашечка.
Если ты на одной машине, то ты ашечку не выдержишь. Короче, это, на самом деле, ключевая история картеарима,
потому что она нас будет ограничивать. Ну, например, мы пишем, вот, допустим, у нас есть один сервер,
мы на него больше не справляемся. Мы ставим второй. Как работает картеарима?
Газ данных — первая, газ данных — вторая. Теперь, если я пишу, вот, раньше у меня было всё вот так,
а теперь мне нужно записать два места. Вариант, как конкретно нас ограничивает картеарима, вот, как она конкретно просто тупо работает.
Смотрите, если я беру, вот, мне пришёл сюда запрос, я после этого пишу, я сразу отвечаю пользователю, окей,
вот это первый шаг, вот это второй шаг, потом я иду сюда, третий шаг и до записываю данные.
Если я так сделаю, что у меня нарушено? Консистентность. То есть, вот в этот момент, в момент два с половиной,
информация в этой базе данных и в этой базе данных разная. Консистентность не выполняется. Окей, тогда я сделаю не так.
Я сделаю сначала вот так, а потом отвечу пользователю о том, что я всё записал. А в этом случае что у меня не работает?
Атомарность. То есть пользователь ждёт, пока я обработаю и разложу информацию, а если там их несколько,
и т.д. Вот, собственно говоря, вот она, вот картеарима очень тупая и простая, вот она примерно так работает.
Вы всегда будете выбирать, либо вы заставляете пользователя ждать, но тогда всё гарантировано,
либо вы придумываете какие-то ощущения, пользователь не ждёт, но тогда и информация не консистентна.
То есть некоторое время база данных в несогласованном состоянии. Где-то одна информация, где-то другая.
И отсюда пошли все вот эти вот механизмы, как они называются у нас, да, поезд консенсусов и т.д.
Что происходит, когда вот здесь одна информация, здесь другая, связь разорвалась, потом восстановилась,
здесь один слепок баз данных, здесь другой, как найти истину и т.д.
Ну а если здесь не два, а 2 и 1, 2 и 2, 2 и 3, доступность означает, что ты получаешь ответ в течение код определённого времени.
Ну да, то есть это конечное время, это не бесконечное время.
В этом как раз вся фишка и в этом как раз критика кафти аремы, потому что она на самом деле немножко так умалчивает эту историю.
На самом деле для нас доступность имеет смысл в жестко ограниченном временном промежутке.
Не знаю, 5 миллисекунд, нет ответов, ну всё.
Итак, соответственно понеслась.
Теперь способы масштабирования баз данных.
Самый простой вытекает из вот этой вот схемки и из нашей кафти аремы.
Называется репликация.
Его суть, у нас есть некая мастер-база данных, есть плейбы.
Да, это уже не политкорректно, но мы сами в России, поэтому будем так говорить.
Была буча, я не знаю, чем она закончилась кстати.
Переминовали, не переминовали, что словами.
А?
Во что?
уже не политкорректно, но мы с вами в России, поэтому будем так говорить. Была
буча, я не знаю, чем она закончилась стать, переименовали, не переименовали, что
словами. Мы запись ведет на русском языке, никто из нас не послушает. Общая логика
заключается в том, что мы пишем сюда, отчитаем. Что это нам дают? И где это работает? Это работает
в проекте, где чтений из базы данных гораздо больше, чем записей, то бишь практически во всех
интернет-проекте. В любом интернет-проекте записей меньше, чем чтений, причем иногда в
разы. Поэтому просто-напросто построив вот такую структуру и сделав там, не знаю, несколько слоев,
вы себе в несколько раз увеличиваете производительность системы. Вот здесь есть,
здесь уже варианты зависит от того, как ты настроишь эту репликацию. Есть такая настройка
ждать, пока не будет получен ответ с каждого из слоев, но обычно делают не так. Обычно мирится
с тем, что некоторое время эта информация будет до слоев докатываться. Есть специальный механизм
у всех баз данных, он называется врайт и хед лог, через который эта репликация происходит. Не суть.
Что нам делать? Вот мы настроили эту репликацию. Пользователь написал пост, делает refresh,
а поста нет. Не докатилась информация, он читает отсюда, опишет сюда. Что делать?
Какие варианты предложить?
Нет. Это из прошлого века. Сейчас так нельзя.
Не можем мы ждать ответа, все-таки слишком долго. Мы должны пользователю, первое правило,
мы пользователи должны сразу дать ответ. Ждать не можно.
Первое. Толстый клиент. Решить проблему с задержки толстым клиентом. То есть у вас,
опять же, если мы возьмем Facebook, он так делает. Попробуйте отправить сообщение. Ну, во всяком
случае, у меня такого было не раз. Отправляешь сообщение человеку, оно пишет, что вроде как
ушло. И только через некоторое время Facebook через минуту пишет, что-то не получилось,
красненький подсвечит. Телеграмм так же делает. Выкачивать по умнее. Эти ребятки делают, во всяком
случае раньше делали это, я сейчас не знаю. Но короче, суть заключается в следующем. Что он на
самом деле делает вид, что он отправил и пытается отправлять. Ждет какого-то там ответа, еще что-нибудь.
То есть, короче, толстый клиент может просто после этого сразу показать. Это первый вариант,
второй вариант. Что еще можно сделать? Часто делает. Ну, это толстый клиент, да. Еще варианты?
Ну, давай-давай-давай. Ну, и выбирай, да. То есть, в одном-одна информация, в другом-другая.
Ну, не-не-не, это еще хуже. Мы на клиент перекладываем задачу как-то разобраться в том бардаке,
который мы натворили на сервере. Почти. Часто делается такая история, что если мы изменили данные,
мы читаем их не со слейва, а с мастер. Это редко происходит. Мастер выдержит. Если, ну, например,
один к десяти у нас по отношению записей к чтению. Ну, будет раз в десять раз читаться с мастером.
Короче, читаем с мастером. Нет, мастер-то обновил данные сразу в себя. Не, погоди, мы сейчас, это мы
боремся сейчас с нагрузкой. Тем, когда это перестанет влезать на одну машинку, мы чуть попозже поговорим.
Да? Ну, смотри, это на самом деле даже делается, знаешь, где это делается? Это делается, вот у тебя,
вот мы сейчас с вами архитектуру рассматриваем на уровне серверов и так далее. А у тебя есть точно
такая же архитектура на уровне приложений. У тебя там точно также несколько слоев. Одна функция вызывает
какой-то набор других функций и так далее. Внизу у тебя лежит общение с базой данных. Иногда используют
ОРМ-ки так называемые. ОРМ это зло. Почему ОРМ зло? Это, во-первых, ОРМ-ка это слой, который отвечает за работу с базой данных.
Ты вызываешь функцию, ты не пишешь запрос, ты вызываешь функцию. Ну, например, отбери из такой-то таблицы по таким-то полям.
И ОРМ-ка сама конструирует нужный запрос. То есть она от тебя скрывает всю базу данных. Ты не знаешь,
какая она у тебя есть. То есть некий слой сработает с базой данных. Это очень удобно для тупых простых
запросов. Подними объект, ну, например, вместо того, чтобы сделать SQL запрос в базе данных про пользователя,
потом получить все эти данные, расшифровать каждый из полей, создать объект и все это заполнить данными из SQL-запроса.
Ты вызываешь просто-напросто метод ОРМ-ки. Верни мне пользователя по такому-то методу. И она все делает для тебя.
Это хорошо. Это удобно. Но минус в том, что ты не контролируешь ничего. То есть если какой-то сложный запрос,
ОРМ его сконтролирует не в факт, что в оптимальный способ. Я к чему говорил? А, я говорил, что вот эта вот проверка
происходит на уровне ОРМ. То есть вот идет выполнение запросов. И мы знаем, что сейчас мы пишем в базу данных.
ОРМ-ка знает, что мы писали в базу данных. Это означает, что и чтение, если в рамках выполнения вот этой же программы
будут еще и чтение из базы данных, то она будет читать из мастера. Вот и все. То есть приложение вообще может не знать о том,
что мы вот этот способ используем.
Так. Это у нас... Здесь бывают еще другие способы репликации. Бывает мастер-мастер репликации, еще что-то.
То есть какие-то там сложные. Для нас не суть важна. Общий принцип заключается в том, что пишем в одно место, читаем из нее.
Это касается не только баз данных, это касается любых хранищ. Видео. Точно так же. Кладем на один сервак.
Потом раскладываем еще на десять. И поэтому какое-нибудь популярное видео будет сразу доступно с нескольких серверов.
Для чего это нам нужно? Для того, чтобы повысить пропускную способность. Что мы будем делать?
Она внутри баз данных? Нет. УРМ это еще твоя сейчас. Это твое приложение. УРМ формулирует SQL-запросы или любые другие запросы,
которые идут в базу данных. И репликация, она уже вот здесь.
Ну, значит, репликация вот этих. Вот эти процессы. Вот эти. Они вот здесь.
А вот этот процесс, откуда читать? Отсюда или отсюда? Он может контролироваться вот этим.
Так. Хорошо. Нагрузка разобрались. Что мы с вами будем делать, если мы не влезаем на одну машину?
Мы построили такую красивую штуку. У нас один мастер, четыре слоева. Слоева на разных машинах.
А данных стало слишком много.
Давай, что такое шардирование? Второй ключевой механизм. Первая репликация, второе шардирование. Что это за хрень?
Какая проблема?
Окей. Согласен. Сейчас обсудим эту проблему.
Итак, общая история. У нас есть кусок, который слишком большой и на одну тачку не влезает.
Мы его берем и по какому-то принципу делим. На разные тачки.
Какие это могут быть принципы?
Пользователи. У нас пользователи. По какому принципу будете делить пользователей?
Ну, подожди. Вот у тебя база данных пользователей.
Анкета. Ты делаешь сайт-бодушечка.
По географии. Первый вариант. Мальчики и девочки. Второй вариант.
А не знаю. География.
Дальше.
Хорошо, я чего?
Ну, что вы могли бы с этим делением?
Хорошо. А теперь предложение.
Согласен.
Ключ какой-то.
Айди пользователь.
Рафиши. Ребят, рафиш от анкеты. Это извращение немножко.
Как вы вычисляете это? А если вам анкета неизвестна, вам ее поднять нужно.
Так. Лады. Гео. В чем плюс-минус?
Да, окей. Можно разбросать по разным регионам. Полегче будет.
В чем колоциальный черный минус, который перечеркивает все плюсы?
У нас в Москве они резиновые.
У вас будет неравномерное распределение.
То есть вы замучаетесь равномерно раскидывать это все по регионам.
Да, возможно. А вот тебе пришел пользователь.
Как ты определишь его географическую принадлежность?
Ну, здравия.
Ненадежность.
А?
Ну...
Короче, вы не подняв анкету, вы по гео не разделились.
Поэтому не прокатывай.
Но все равно сначала анкета, сначала какой-то айдишник нужен.
Дальше. Ну, гендер совсем не прокатил.
У нас их всего два.
В прогрессивном западе их больше, но все равно они неравномерны.
Так. Ключ, айдишечка. Ну, на самом деле почти одно и то же.
Айдишка.
Два айдишки.
Выбираете айдишку и по какому-то принципу ее делите.
По какому?
А после у нас айдишка пользователя Login.
Согласен.
Согласен.
Можно как хэши, можно просто сравнить с ручкой.
Точно так же можно сравнить с ручкой.
Не важно. В любом случае у вас есть некая функция.
Которая ключевая, да.
То есть некая функция кэширования.
И вот здесь у вас этот идентификатор.
А функция возвращает вам номер сервака, грубо говоря.
Или IP-адрес сервака. Не важно.
Вот это вот ключевая история.
Вот она как-то должна быть написана.
Вот эта штука.
И здесь есть две вещи.
Как она может быть написана?
Это может быть функция от хэша.
А как еще можно?
А можно просто тупо таблицы.
Огромная таблица.
Соответствие ID пользователя.
Какой-то огромный KBI.
Называется центральный диспетчер.
То есть так у нас какая-то функция.
А так центральный диспетчер.
В чем плюс, в чем минус?
Так. К следующему разу две задачи.
Первая.
Вот те, кто здесь сидят.
И вы вдвоем.
Вы доказываете, что центральный диспетчер это хорошо.
А все остальные доказывают, что вот такая вот функция это хорошо.
А центральный диспетчер как-то.
Правильно? Ответа нет.
Это первая история.
А вторая история. Хорошо.
Мы поделили.
Первые 5 букв на один сервак.
Вторые 5 букв на другой сервак.
Третий 5 букв на третий сервак.
А потом первые 5 букв перестали влезать на первый сервак.
И что сделали?
У нас функция ломает.
Она уже не такая красивая.
То есть там уже получились исключения.
Если бы мы брали кэш, то было бы еще хуже.
Потому что при добавлении еще одного сервака,
у тебя на него часть перельется сюда,
часть перельется сюда, часть сюда, часть сюда, часть сюда,
и все сервера будут переколбошены.
Как вы сейчас этой базы берете?
Ты с ума сошел?
Ты отсюда сюда переложил, отсюда сюда переложил.
У тебя пока все кем-то в граналии себе придет.
А что делать пользователям?
Они анкеты смотрят.
Они даже, извините, у нас сервак выкупили.
Мы перекладываем, понимаете, перед нашими.
Короче, как вообще, в принципе,
с этой больдой вторая задача для всех
подумать, как вот этим
мерить, что делать,
когда у нас новый сервер добавляется
в нашу
шардируемую идеальную
историю.
Можно сказать, что
некоторые люди, которые сидят в арте,
будут относительно интересны к другому серверу,
когда это не будут делать тогда.
Это, видимо, в разном смысле,
когда данные смотрят.
Нет.
То есть, вы замечаете, что это надо
перенести, и он вытягивает.
Хорошо.
А по запросу,
то есть, у тебя, тебе нужно
перенести блок данных.
Пользователь приходит
из этого диапазона.
Как он узнает, идти на новое место или на старое?
Ну, ребят.
У вас есть упрощенная схема?
Предлагаю.
Если
у тебя
задача,
есть вот задача
Это у тебя будет
работать вот здесь.
Это
центральный диспетчер.
А вот функции, как это будет работать?
У тебя функция, она не хранит вот
это соответственно. Я просто пользователь
Выдаешь сервис?
Остаток отделения на 10.
А, ну да.
А она стокнет.
Вот помирает у тебя эта машинка, которая с этой штукой работает.
А.
А.
Ну так, тебе идея.
Ну, подожди.
Ну, подожди.
На тебя вернет два ответа, что ли,
для какого-то пользования?
Мы еще не переложили?
Ну, погоди. У нас там терабайки.
Нам перекладывать сутки.
Разбежал такой, молодец.
Два на три поменял, все заработал. Нифига.
Они перекладывают сутки,
а пользователи, скотины,
в этот момент что-то пишут.
И ты не можешь
им сказать, ребята, извините,
новый сервис.
А вот те ребята, которые
в течение этих суток
писали,
они пишут что туда и туда, да?
А мы потом в старом перезапишем?
Короче,
давайте как-то по страннее,
ребята, слишком много вопросов
возникает в вашем одежде.
Мы пишем и туда и туда, да?
Или пишем только на новый?
Хорошо,
эти данные уже перенеслись.
Но без вот этой таблицы
ваша схема не работает.
Либо вот такая таблица,
либо ходить в два раза при миграции.
В два места, так?
Я не хочу ходить в два места
на каждый запрос.
Все, давайте.
До следующего раза.
У нас еще майские не начнутся в следующий раз?
Да.

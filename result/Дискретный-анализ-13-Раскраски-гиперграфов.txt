Давайте я, наверное, еще одну оценку для МОТН напишу. Докажу. Все-таки это для полноты картины
будет тоже хорошо. На прошлый раз мы остановились на следующем. Давайте я напомню, что МОТН,
ну МОТН это минимальное число ребер в эноднородном гиперграфе, который нельзя покрасить в два цвета.
Так, чтобы вы вспомнили. Минимальное число ребер в контрпримере в таком гиперграфе,
который нельзя в два цвета покрасить, чтобы каждое ребро было не одноцветно. У нас была вот такая оценка.
Мы ее доказали и я сказал, что это самое лучшее, что вообще известно человечеству к настоящему
времени. Никто пока не умеет эту оценку дальше улучшать. Вот и у нас была вот такая оценка снизу.
Ну то есть по сути зазор между n квадрат и корень четвертой степени из n сейчас у нас,
вот текущий рекорд такой. Нижняя оценка не является восьминтотикой рекордной,
вот я сейчас докажу еще лучшую. Хотя вот для случая МОТ5, если вы помните, подход,
который давала вот эта оценка, работал лучше, чем тривиальный, конечно, там получалось 26 вместо
17 или 16. То есть довольно существенный скачок получался. Та оценка, которую мы сейчас получим,
она восьминтотике работает, для МОТ5 лучшего не даст, но восьминтотике будет лучше. Значит,
теорема, которую я предполагаю доказать, состоит в том, что МОТn больше либо равняется вот такой
значены. Так вот, только, наверное, тут надо вот так написать. Ну это смешно нелепо, но пусть
будет 1 четверть от 2 в степени n, конечно. Наверное, вот так. Ну 2 в n минус 1, конечно,
можно писать как 2 в n, и тогда будет 1 четверть. Я так пишу, потому что я так буду доказывать.
Нет, ну вот здесь стоит корень кубический из n на логариф men. Конечно, это лучше, чем корень
четвертой степени из n, но вот это хуже, чем он же, но просто при маленьких n. Константы
разнятся, и за этот при маленьких n вот эта оценка работает хуже. Но восьминтотике выигрыш очевиден.
Давайте я сразу скажу, пока не забыл, что текущий рекорд еще больше. Текущий рекорд вот такой,
корень просто из n на логариф men. Это вот текущий рекорд в оценке снизу. Зазор между
корнем из n на лог n и n квадрат. Пока это такая картина здесь. Но это я не буду доказывать,
а вот это докажу. Это доказывается очень красиво, и метод, который там использован, полезный,
потому что это то, что называется рандомизированный алгоритм. Один из вариантов построения такого
рандомизированного алгоритма в данном случае покраски. То есть, сама техника, которая здесь
срабатывает, она полезная и интересная, и тоже, в общем, ее имеет смысл осознать. Так, ну давайте я
вот так вот сделаю. Все, пошло доказательство. Фиксируем n однородный гиперграф,
у которого
m ребер, где давайте пока вот так напишем, m это x на 2 в степени n-1. Пусть будет просто
x какой-то. Конечно, вы мне скажете, x это, наверное, вот эта вот величина, эта вот x. Но правильно,
вы скажете, в итоге мы получим такой x. Но давайте рассуждать так, как будто его даже не было,
то есть, вот просто написано здесь x, и мы хотим подобрать оптимальность, хотим сделать его как
можно большим в зависимости от n. Понимаете, да? То есть, вот пока это какой-то параметр x,
конечно, в итоге мы подставим ровно то, что написано в теории. Но, как обычно, будет компьютерный
вариант утверждения, когда x можно будет оптимизировать при каждом конкретном n, ну и будет
следствие, которое мы, собственно, доказываем. В частности, в качестве такого x можно взять
половинку от корня убийства. Я понятно рассуждаю? Ну вот. Так, зафиксировали. Нам нужно доказать,
что этот гиперграф красятся в два цвета. Так, вы понимаете, что нам нужно это доказать? Что такое
m от n больше либо равняется чего-то. Это значит, если вы возьмете любой гиперграф с таким количеством
рёдер, то он красятся в два цвета. Так что каждое ребро не одноцветно. Это важно понимать сходу,
иначе дальше ничего не будет понятно. Вот давайте организуем такой процесс случайной раскраски.
В нём будет два этапа. Первый этап. Красим вершины стандартным образом, с вероятностью одна-вторая в
красный и синий цвета независимо друг от друга. Просто делаем вот то же самое, что делали,
когда доказывали оценку просто 2 в n-минус первой степени. Ну конечно, если остановиться на этом
этапе, то получится x равно 1. Это мы с вами уже проходили. Ничего хорошего. А нам хочется x побольше.
Давайте поймём, из-за чего могли быть проблемы. Вот мы покрасили случайно. Что могло случиться? Ну
какие-то множества, какие-то ребра могли оказаться одноцветны. Могли, случайно. Давайте
буквой D обозначим множество дурных вершин. D, если хотите, от слова дурные верши. Значит,
это множество вершин, принадлежащих одноцветным ребрам.
То есть это случайное множество, оно получается в результате раскраски на этапе 1. Не, ну если нам
повезло и D пустое, конечно, всё очень здорово, но с какой-то довольно высокой вероятностью,
к сожалению, оно будет не пустым. И вот нам надо что-то с этим не пустым множеством сделать.
Понятно, как оно устроено? Получились какие-то одноцветные ребра. Вот все их кучу свалили,
объединение этих ребр. Это и есть D. Второй этап. Мы берём новую монету. Мы тоже бросали монету,
фактически, причем с таким несмещённым центром тяжества. Мы берём новую монету,
коррекционную, корректирующую. Берём монету, у которой какой-то P из 0,1. Это, ну скажем,
вероятность трешки, как я обычно интерпретирую. Ну пусть P, вероятность трешки, не важно. В общем,
давайте так. Мы двигаемся вдоль вот этого множества D, рассматриваем вершину за вершиной,
бросаем эту монету. Если решка, то меняем цвет вершины на противоположной, а иначе просто
ничего не делаем. Ну, то есть, P – это вероятность мены цвета у вершины из множества дурных вершин.
Пытаемся перекрашивать дурные вершины, причем перекрашиваем каждую независимо от
остальных с вероятностью P, и не перекрашиваем с вероятностью 1-P. Вот такой вот рандомизированный
алгоритм. Всего два этапа, то есть больше никаких прогонок. Один раз пробегаем все вершины
несмещённым центром тяжести, появляется какое-то множество D, затем пробегаем вершины из этого
множества D с новой монеткой, и либо меняем цвет на противоположной, либо сохраняем.
Понятен процесс? Всё, после того, как прошли этап номер два, всё,
что завершилось? Получилась какая-то случайная раскраска. Двухэтапная, но в итоге случайная
раскраска. Понятно, что теперь уже раскраски неравно вероятны, там у них какие-то сложные
вероятности, но мы должны будем вникать в то, как устроено пространство, а будем оценивать
вероятности плохих событий. Вот давайте на выходе F это событие, то есть множество раскрасок,
которое состоит в том, что существует одноцветное ребро. Мы старались, что-то там поправляли,
но всё равно нашлось одноцветное ребро. Наша цель получить неравенство P от F меньше единиц,
но это под вопросом пока, но вот это наша цель. Друзья, я кое-что забыл ещё подчеркнуть,
но конечно, вот это P это параметр, который мы тоже подберём позже. То есть это точно не одна,
вторая, это вы увидите. А что это? Ну вот X я сказал, я просто сформулировал теорему,
X я сказал, что такое. А что такое P? Я умолчу до последнего, это вообще катарсис. Глядеть на
это P, это умереть просто. Мы вам шпаргалку его включим, конечно, но тем не менее. Ну хорошо,
давайте попробуем как-нибудь оценить вот эту вероятность как функцию от X и от P маленького,
после чего мы сможем подбирать X и P из условия, что вот эта функция оценки меньше единиц. Идея
понятна? Так, ну хорошо, вот пусть у нас есть какое-то ребро, е маленькое из е большого,
не путайся с числом е. Нас на протяжении этого доказательства ещё будет число е. Я всё равно
упрямо назову ребро буквой е, потому что когда число е появится, ребра уже не будет. Ну про ребра
забудь, это не число, это ребро. А? Ну да, будем ребра сокращать. Так, ну хорошо, какие могут быть
неприятности с ним? Вот давайте я а с индексом я обозначу следующую неприятность е, ну скажем,
красная на этапе 1 и синяя, нет, наоборот, наверное, мне лучше, я привык, наверное,
так сейчас я соображу как лучше, синяя или красная стаканка, не важно. Ладно, красная на
этапе 1 и красная на этапе 2. Ну то есть мы случайно красили с несмещённым центром тяжести,
оно получилось целиком красным, это плохо. Потом мы меняли, пытались менять цвет вершин,
а оно всё равно целиком красное. Плохо же? Так, ну давайте я напишу а е с волной, е синяя на
этапе 1 и синяя на этапе 2. Понимаете, что вероятность этих событий одинаковая? Потому
что п это вероятность смены цвета, не важно, красного на синей, синего на красной. Я дальше,
в дальнейшем буду писать только одно из двух, чтобы не рисовать слишком много, там у каждого
события будет вот такое симметричное ему. Значит, ещё будет событие давайте а е в стрих, это е красное
на этапе 1 и синяя на этапе 2. То есть наоборот полный перелёт мы стали исправлять ошибки такой
вот случайной совсем раскраски и те красные превратили во все синие. Неприятность. Вот так
вот перелёт, вот здесь недолёт, слишком недолёт, здесь перелёт. Ну симметричное синее превратилось
в красное, я уже не пишу. Ну и самое интересное, этот т с индексом е, значит у нас е красное на
этапе 1, ой, е хорошее, ну то есть не одноцветное, на этапе 1 и скажем красное на этапе 2. То есть
всё было хорошо, е было одноцветным, но оно испортилось, стало целиком красным, ну или целиком
синим, это симметрично. Ну то есть короче говоря, f это объединение по всем е с е, а е с волной и так
далее, с е с волной. Вот так вот можно написать. Понятно? И других вариантов нету. Там их шесть штук,
да. Ну естественно, вероятность f не больше чем 2 на сумму по всем е с е, и здесь будет вероятность
а е, плюс вероятность а е штрих, плюс вероятность с е. Вот тут их уже три штуки, но я двойку вынес за
скоб. Согласны? Ну отлично, вероятность а е очевидно считается, чему она равна
красное на этапе 1, и потом мы независимо перекрашивали, и стало целиком красное снова.
Так, в каждом ребре n вершина, это вы помните, да? У нас n однородный гиперграф. Но вот 2 в степени
минус n, это то, что оно красное на этапе 1. И умножить на что там, на 1 минус f в n, это что не одна
его вершина не перекрасилась. Все его вершины по определению попали в d, каждую из них вы пытались
перекрасить, но нам ничего не удалось. Значит каждый раз монетка ложилась орлом кверху, а это
происходит с вероятностью 1 минус f. Так, вероятность а е штрих абсолютно так же, это 2 в степени минус
n на f в n. Каждый раз цвет менялся на противоположном. А вот ce это вопрос. Давайте поймем, вот если у
нас случилось ce, вообще что это значит? Как такое могло случиться, что ребро, которое изначально
на этапе 1 было хорошим, то есть не одноцветным, вдруг взяло и стало красным? У нас было какое-то
ребро g, состоящий из n вершин. Как могло такое случиться, то в нем были и красные, и синие вершины,
но на этапе 2 какие-то синие вершины перекрасились, превратились в красные, какие-то красные просто
сохранили свой цвет. Почему так? Но синие точно должны были перекраситься, правда? В нем были
синие вершины, что ж было хорошо, а потому что было видимо еще какое-то ребро, которое в
первый раз краски было целиком синим, поэтому вершины этого ребра, в том числе и вершина
ребра ели несколько его вершин, попали в множество d, попробовали перекраситься, перекрасили и
случилось несчастье. Понятно? Говорю нет. Ну то есть я вот так напишу, следовательно существует f из e такое,
что e пересеченная с f не пусто, не и, а вот так, f синие на этапе 1. Что еще мы знаем?
Ну давайте опять напишем e хорошее на этапе 1 и e красное на этапе 2. Ну то есть я повторю те
условия, которые есть с e, что e хорошее на этапе 1, e красное на этапе 2, но я добавляю,
что есть некое синие на этапе 1 ребро, которое не пусто пересекается с e, ну и вот в частности
отсюда следует, что все вершины из пересечения e и f перекрасить. Хорошо? Знаете, я вот это вот
обозначу b с индексом e запятая f, ну событие такое множество раскрасок, для которых вот это
конкретное ребро f пересекающееся с e не пустым образом синие на этапе 1, а e было хорошим,
ну и вот стало за счет пересечения с f плохим красным. Это событие b и f. Ну то есть я хочу
сказать, что вероятность с e она точно не больше, чем сумма по всем f из e таким, что e пересеченная
с f не пустая, вероятность и вот этих b с индексами e f. Ну я думаю это все понимают, если из события
следует событие, то вероятность не больше. Ну а поскольку здесь еще квантор существования,
то как тогда мы делаем сумми. Так, что же я могу стереть-то, а? Я хочу b и f отценивать.
Так, плохо дело. Ну ладно, вот тут-то точно могу стереть?
Ну попробуем, я думаю здесь места хватит. Вероятность b, e, f. Давайте еще вот чего сделаем,
обозначим, вот тут я напишу буквой h, мощность пересечения e и f. h конечно больше либо равно
единице, уже они не пусто пересекаются. Ну так просто, идем такое обозначение. h это уже не
ребро, h это мощность их пересечения. Давайте подумаем, как оценить вероятность b и f. Надо
смотреть вот так, я сейчас картину снова нарисую. Вот e, вот f, есть общая часть, есть вот эти части,
которые e минус f, а есть еще вот эти части, которые f минус e. Надо все посмотреть.
Ну что мы знаем, f синяя в изначальном первом этапе, а? Ну это 2 в степени минус m.
Это даже было бы равно, не меньше либо равно, а равно, но вы потом поймете,
откуда меньше либо равно берется, пока это даже равно. Так, дальше общая часть перекрасилась,
то есть тут f в степени h. h общих вершин мы точно знаем, поменяли свой цвет на крас. f в
степени h. Ну и осталось посмотреть вот на эти волнистые вершины, которые из e минус f, из e
сет минус f. Вот это самое интересное. Вот-вот-вот, да, там все хитро. Именно отсюда получается меньше
либо равно, а не равно, потому что мы не знаем, что с ними происходит. Но мы точно знаем,
что yet стало красным, да? Давайте возьмем какую-нибудь вершину отсюда. Одну пока.
Для нее есть два варианта. Первый вариант, она была синей, стала красной. Второй вариант,
она была красной и осталась красной. Друзья, это понятно? То есть пишем такую формулу полной
вероятности из двух случаев. Либо была синей, стала красной, либо была красной, осталась красной.
Пишу в скобках, потому что два случая, будет два слагаемых. Была синей, это одна, вторая. Стала
красной, это p. Никаких вариантов, по-другому она красной не становится. Значит, она пыталась
перекраситься и ей это удалось. Либо она была красной и осталась красной. Вот тут, извините,
совершенно ничего не понятно. Но почему она осталась красной? Может, она пыталась перекрашиваться,
ей это не удалось. А может, она вообще не попала в D. Тогда она даже не пыталась перекраситься.
Дайте я умножу на 1, это точно правильно. Я написал формулу полной вероятности. Понимаете, да?
Ну вот. Но здесь я огрубил. Я не знаю вероятность того, что она осталась красной. Как это посчитать,
я без понятия. Но это вероятность, как любая, не больше, чем единица. Ну, поэтому не больше. И
это все надо возвести в N-H в ту степень, потому что это происходит с каждой из этих N-H.
Все, я оценил вероятность BNF.
Не-не-не, мы же знаем, что е целиком красная, а эти вершины все были синие. Значит, они все
перекрасили. Это важно, да. Вопрос важный. Но это именно так. Хорошо. Так, ну, давайте я
немножко упрощу еще. Ну, приведу к общему знаменателю вот тут. Я напишу вот так. P в степени H.
1 плюс P в степени N-H. А двойку, в какой степени у нас получится двойка H-N-N, то есть H-2. Вот так.
H-2. Следите, ну вроде правильно. Значит, смотрите, легко видеть. Я не буду даже, наверное, проверять.
Совершенно понятно, что вот эта величина хуже всего устроена, то есть максимально. Если H равно
единице. Ну, просто вот эта функция по H, она там какая, не убывающая. Не убывающая, не возрастающая,
не возрастающая. При H равном 1, она принимает максимальное значение. Мы не знаем, чему равно
H, понимаете. Мы знаем, что они пересекаются, но мы не знаем, чему оно равно. Мы только знаем,
что H больше либо равно единице. Ну, я хочу сказать, что вот это вот точно не больше,
чем P на 1 плюс P в N-1 на 2 в степени 1-2. Ну, это, в общем, разделите просто H плюс первое на H,
и проверите. Ну, это очевидно. Понятно сказать? Ну вот. Значит, смотрим сюда. Смотрите, вот эту
вероятность мы знаем, эту знаем, и эту теперь тоже оценили. Если продолжить вот это неравенство,
вы знаете, как можно сказать, но это точно не больше, чем мощность E умножить на ту оценку,
которую у нас получилось на P на 1 плюс P в N-1 на 2 в степени 1-2. Ну, это грубая,
вообще говоря, оценка, но мы же не знаем, сколько ребер пересекаются с E, может, все вообще.
Поэтому приходится писать так. Ну, кстати, для мощности E у нас есть обозначение, видите,
буковка M, которая еще равна х на 2 в N-1. Я думаю, что я могу сразу вот так подставить. Сейчас вы
увидите, почему я выбирал именно в этом виде, почему х на 2 в N-1. Мне нужно, чтобы все сократилось
очень красиво, оно и сократится. Так, ну давайте я где-нибудь уже вот тут это напишу. Так. У нас
получается, что вероятность F не превосходит 2 на мощность E, ну или давайте я сразу буду
писать на х на 2 в N-1. Это откуда я взял? Смотрите, я каждое слагаемое, вот в этой сумме трех слагаемых,
оценил величиной, которая от E маленького никак не зависит. Смотрите, сюда, сюда, сюда, от E маленького
эти величины не зависят, правильно? Поэтому я всю вот эту двойную сумму, в свою очередь, оцениваю,
как 2 на мощность E, помножить на сумму трех оценок. Вот я пишу 2 на мощность E и дальше пишу сумму
трех оценок. А в минус N-й на 1 минус P в N-й, 2 в минус N-й на P в N-й и тут вот еще х на 2 в N-1.
Так, на P, на 1 плюс P в N-1 и на 2 в степени 1 минус 2.
Ну вот это тоже подставил. У меня так получилось. Ну смотрите, какая красота.
Чпок, чпок, чпок, чпок и чпок. Не, не понятно. Сейчас, что-то я наврал, да? Не, не, я что-то
забыл. Сейчас, подожди, что же я не дописал? А, сейчас, что такое? А, не, не, не, не, все правильно,
все правильно, все правильно, все правильно, да. Конечно, N-1, единички сокращаются, минус 2N плюс N
это минус N, вот этим 2 в степени N оно и сократилось. Нет, все правильно. Чпок, чпок. Видите, да? То есть все
степени двоих сократились именно за счет того, что я вот так красиво выбрал, чтобы оно сократилось.
Короче, у меня получилось вот так, х на 1 минус P в N-й плюс х на P в N-й и плюс х квадрат, вот тут х и
тут х, на P на 1 плюс P в N-1. Вот такая вот штука получилась. Полца от х и от P, как я и обещал. То есть
компьютерная теорема теперь получается такая. Компьютерная теорема, там я теоремы штрих, по-моему,
обозначал. Теорема штрих получается такая. Пусть для данного N числа х и P таковы,
то P из 0,1, потому что это вероятность, и ну как-то вот так вот я сейчас обведу,
чтобы не переписывать. Вот это выражение меньше единицы. Тогда M от N больше либо
равняется х на 2 в N-1. Вот такая теория. Ну то есть, как обычно, бежим в двойном цикле,
пытаемся максимизировать х для данного N, при котором во внутреннем цикле удается подобрать
чиселку из 0,1, ну там с каким-то шашком бежите, да, при которой выполняется вот это нерадость.
Я утверждаю, что можно взять в качестве х действительно вот эту функцию от N,
но надо еще некоторую функцию P подобрать, тоже от N, чтобы они вместе удовлетворяли вот этому
нерадость. Но это тоже некий аналитический катарсис. Сейчас я напишу функцию P, как обещал,
это круто, вот я ее тут напишу. P это будет вот такая штука, 1 треть, логарифм от N делить на
логарифм N, делить на N. Ну это особенно круто, это что надо под логарифмом написать N деленное
на логарифм N. Ну казалось бы, господи ты боже мой, ну когда ты берешь логарифм от этой дроби,
ты получаешь разницу между логарифмом и повторным логарифмом, то есть ну а симпатически ничто,
но логарифм будет. А вот нужно, сейчас увидите, сейчас будем проверять неравенство, что это меньше
единицы и все сработает. Но сейчас будем проверять, пишем 1 вторая, N на логарифм N в степени 1 треть,
это я написал X, X это 1 вторая, N на логарифм N в степени 1 треть, так удобно сейчас писать в
корень кубический. Так, знаете, я вот так нарисую стрелочку, опять же вот отсюда, я хочу доказать,
что это меньше единицы, меньше либо равно, и вот тут вот напишу E в степени минус P, умноженное
на N. Так, все помнят, что 1 минус P в N и не больше чем E в степени минус P N, миллион раз проходил,
но что такое E в степени минус P N? Это E в степени минус 1 треть от логарифм N делить на логарифм N.
N-ка в знаменателе, которая есть у P, она убилась до вножения на N. Так, дальше пишем X, но 1 вторая,
N на логарифм N в степени 1 треть, на P в N. Ну, P в N я оценю как-нибудь тяп-ляп, ну не знаю,
ну пусть будет ладно, пусть не тяп-ляп, пусть будет вот так. И хочу вот здесь рисовать эту дропе,
она работает только в показателе экспоненты, а когда я внизу пишу P, то эта дропа, конечно,
никакой роли не играет. Только при экспоненцировании такое важно. Понимаете, да, что я хочу сказать?
А здесь я грубо оценю, да, я уже досчитаю, потом сделаем перерыв. Ну, катарсис не хочется прерывать,
красиво же. Так, ну и последнее слагаемое, это X квадрат, ну то есть 1 четверть N на логарифм N в
степени 2 третьих, это X квадрат. Дальше умножаем на P, снова грубо, забивая на логарифм N, вот в этом
знаменателе, вот здесь забиваем. Так, это у нас P, а 1 плюс P в N минус 1, опять же оцениваем как E в
степени P умножить на N. Ну N минус 1 меньше, чем N. 1 плюс P в N минус 1 меньше, чем E в степени P умножить
на N. То есть тут будет E в степени, так, не минус, а просто, да, 1 треть логарифм от N на логарифм N.
Но вот это сейчас производит впечатление, как красиво все сокращается, смотрите. Значит, тут равно 1
вторая N на логарифм N в степени 1 треть, а что такое E в степени логарифм? Это его аргумент,
правильно? E в степени логарифм, это то, что стоит под знаком логарифм. И потом оно еще возводится в
степень минус 1 треть. Мы пишем N на логарифм N в степени минус 1 треть. Прямо в одну вторую
шлепнулась. Так, плюс, ну тут вот эта фигня, я ее просто вот так нарисую, это повтор вот
это, тут все то же самое. А здесь пишу 1 двенадцатая, ну 1 двенадцатая, N на логарифм N в степени 2 третьих,
дальше идет логарифм N на N, вот этот, и дальше идет еще N на логарифм N в степени 1 треть. Ну,
видите, тоже все сокращается. Вот так прям чпок, чпок, чпок. То есть получилось 1 вторая,
плюс 1 двенадцатая. Ну, смотрите сюда. Ну, боже ты мой. Даже если у вас N маленькая там,
какой-нибудь, я не знаю, ну 3, например. Вот что такое логарифм 3? Ну, это примерно 1. А вы здесь
получаете уже 1 девятую, да еще в кубе. То есть, ну, что такое 1 девятая в кубе? Это 1 семьсот двадцать
девятую. Понимаете? То есть вы одну семьсот двадцать девятую, а тут, ну, какой-то корень кубический
из трех пополам. Ну, конечно, это ничтожное число. И дальше, чем больше N, тем оно ничтожнее.
Ну, я не знаю, что там будет. Ну, одна сотая какая-нибудь. Тут меньше либо равно. Ну,
неважно. Даже одна сотая нам не нужна. Тут хватит гора. Тут запас колоссальный. Конечно,
это меньше. Ну, давайте утверждать, что это верно для всех N, начиная с трех, например. Потому
что для N равного двойки. Ну, слушайте, чему равно N от двух, товарищи? Вот мы с вами тут обсуждаем
какие-то высокие материи. Чему равно M от двух? Кто-нибудь сейчас понимает? М от двух. Что это такое?
Это минимальное число ребер в графе, который нельзя покрасить в два цвета. В графе, в обычном.
Для N равно двум это граф. Три, конечно. M от двух равно трём. Поэтому для N равно двойки можете
не смотреть. Ну, вот читайте, что N больше либо равно тройки. Тут, конечно, всё пройдёт.
Так, ну, в рамках этой темы осталось ещё кое-что. То есть больше оценки я никакие тут делать не
буду. Я сказал, что можно улучшить, но этого я делать прямо сейчас не стану. Зато у нас была в
рамках этой деятельности ещё одна задачка, которую мы в своё время решили частично с помощью матрица
Адамара. Там речь шла о том, чтобы не просто покрасить в два цвета так, чтобы каждое ребро
было не одноцветно, а так покрасить, чтобы каждое ребро примерно пополам делилось на красные и
синие вершины. Не помните такую постановку? Я сейчас напомню формулировку того результата,
которую мы сделали с помощью матрица Адамара. Сейчас напомню. Это, наверное, его КТЧ было,
да? Ну, сейчас напомню. Теорема, которую мы когда-то давно очень доказали, звучала,
наверное, следующим образом. Конечно, если бы вы могли её посмотреть прямо в той формулировке.
Ну, хорошо. Но мне помнится так. Значит, звучала она по-видимому так. Пусть
для данного N
существует в матрице Адамара
порядка N.
Ну, как вы помните, это почти любое такое. Ну, они часто встречаются в натуральном ряде. Я
сейчас напомню следствие. Пусть для данного N существует матрица Адамара порядка N. Тогда
можно
так выбрать под множество, ну или ребра, под множество М1
МН множество числа от единицы до N. Чтобы при любой раскраске Rn красные и синие цвета,
при любой раскраске вот этого Rn красные и синие цвета, нашлось МИТ такое, что в нём
разность писел красных и синих вершин
не меньше, чем корень N пополам. Невозможно слишком равномерно покрасить. То есть
как ни старайся, как ни крути, как ни красить в два цвета, какое-то ребро не пополам покрасится,
а с некоторым дискрепансом, как говорят, с некоторым уклонением величины корень N пополам.
Ну, то есть в идеале как бы хотелось, чтобы каждый ребро пополам покрасилось красные и синие
цвета. Но вот, оказывается, бывают такие гиперграфы. Заметьте, у них столько же ребер,
сколько и вершин. Это вот за счёт матрицы Адамара они строились. Я могу чуть-чуть напомнить,
там строчки матрицы Адамара соответствовали вот этим множествам. То есть М1 это было всё Rn,
а мощность каждого из оставшихся МИТ-х равнялась N пополам.
То есть МИТ это было множество тех позиций в строчке, на которых стояли единички.
Сейчас понятно, говорили не очень. Матрица Адамара, она из плюс-минус единиц и в нормальной
форме у неё верхняя строчка из сплошных единиц, а остальные строчки содержат половинку единиц,
половинку минус единиц. И вот мы гиперграф строили так, мы брали каждую строчку матрицы и
смотрели на позициях, с какими номерами стоят единички. Вот номера этих позиций
образовывали ребра гиперграфа. И мы доказывали, что поскольку это матрица Адамара, то при любой
раскраске вершин вот этого гиперграфа, то есть чисел от единицы до N, хоть одно из этих множества
обязательно сильно выбивается. В том смысле, что разность между красными и синими вершинами
по модулю не меньше, чем корень Низанфопала. Чуть-чуть вспомнилось, нет? Совсем забыли, но это
ОКТЧ, это больше года назад. Но настолько вот эта тема проходит красной нитью через разные наши
подходы, через разные приложения, которые я рассказываю, что да, вот приходится вспоминать
то, что было больше года назад. Но я ж не прошу сейчас помнить доказательства этой теории,
формулировка понятна или нет? Ну это точно было, но главное, понятно ли сейчас формулировка,
если вдруг забылось, что это было. Такая вот вещь. Но в обратную сторону тоже интересно, но хорошо,
да, бывает гиперграф, который трудно слишком так вот пополам покрасить, пополамную раскраску такую
сделать, чтобы каждое ребро пополам красилось красные и синие цвета. Но наоборот-то, может быть,
любой гиперграф все-таки относительно хорошо красятся. Насколько маленьким можно сделать это
уклонение на каждом ребре для произвольного гиперграфа? Вот теорема 1 на сегодня, похоже,
что ей все, наверное, и ограничится. Но посмотрим, там есть очень интересный подход к теореме 2,
я про нее, конечно, хочу говорить, но теорема 1 звучит так. Пусть, ну, n теперь любое вообще... А,
я забыл, слушай, я же еще следствие отсюда обещал, извините. Значит, следствие говорит,
что нам плевать, какое n вообще, он может быть любым совершенно. Это я говорил точно,
Бог, это честное следствие, состоит в том, что для любого вообще n можно вот эту оценку
заменить на корень из n пополам на 1 минус какой-то малое от единицы, плюс о малое от единицы,
но оно отрицательное. Потому что, да, вот эти вот простые числа плотно встречаются в натуральном
ряде, то есть порядки матрицы Адамара, они встречаются в натуральном ряде плотно. Между n и n плюс
о малое от n всегда есть там какой-то порядок матрицы Адамара, поэтому можно чуть-чуть подправить,
вычтать что-то, стремяющееся к нулю, получить утверждение для всех n. Это я тоже тогда говорил.
Вот теперь смотрите, пусть n любое, как и в этом следствии, так. Тут h имеет опять
вершины, это вот эти числа от единицы dn, какие-то ребра, и мощность E равняется m,
и не обязательно равняется n, вообще любое. M любое, никак не связано. Даже так напишу,
n, m любые. Пусть n и m любые, гиперграф имеет n вершины, м рет. Любые. Тогда существует
раскраска. Такая, что для любого м и, разность писел красных и синих вершин
не больше, чем, если не путаю вот так, но сейчас докажем и поймем, по-моему, вот так.
Корень из 2n, но логарифт натуральный 2n. Да-да-да, вот m это количество ребер,
а n это количество вершин. То есть, если ребра обозначать по-прежнему, то вот здесь будет индекс
m. Ну, то есть, в частности, если мы находимся в условиях той теоремы, которая была в аукатече,
то здесь будет n. И, конечно, вот эта верхняя оценка, корень из логарифма раз по порядку,
отличается от нижней оценки через матрицу Адамара. Видите, да? Чуть-чуть хуже. Матрица Адамара
немножко как будто бы не дотягивает. Вот теорема 2, которую мы докажем, ну, почти докажем, там,
идейно докажем, все, что нужно, по сути, мы расскажем. Теорема 2 говорит следующее, что,
если такие m равняется n, то вот это можно заменить на, ну, константу тут можно писать
примерно какую угодно, например, на 6 корней из n. Но я, наверное, докажу с 11 ее. Ну, 6 тоже правда.
Но это я уже в следующий раз буду доказывать. Там энтропия будет использоваться. Очень крутая
теорема, очень красивая, очень нетривиальная, но поймем, все поймем, как следует крутость. Вот,
то есть, на самом деле, оценка, которая дает матрица Адамара с точностью до константа,
все-таки не улучшаем. Приме равном n действительно имеет место точности этой оценки с точностью до
константа постоянного сомножителя. Но в общем случае, такого улучшения нет, и все-таки логариф
некий набегает. Ну, и вот, а можно немножко улучшить дальше, но не сильно. Так, ну, давайте я вот
эту теорему докажу. Я ее точно не доказывал. Никто не помнит. Ну, наверное, забыли, в любом случае,
если доказывал, я сейчас докажу снова. Думаю, что нет, потому что когда бы я мог ее доказывать?
В ОКТЧ этих методов еще не было. Но доказывается вероятностно. Поэтому я думаю, что скорее не доказывать.
Но там черт его знает, если повторюсь, значит повторюсь. Никто не помнит, я сам не помню, значит,
лучше повторюсь. Так, доказываю эту теорему с корнем из логарифма, а на это уже пойдет следующая
лекция, видимо. Значит, смотри. А, да, еще важное замечание. Видите, я не утверждаю, что здесь
гиперграф однородный. То есть, если в задаче про m от n, n это было число вершин в каждом ребре,
то здесь никакая однородность не предполагается. То есть, вот здесь они все одинаковые, а это
мощность n. И тут в этих теоремах ничего не говорится о мощностях множеств. Ребра могут быть
какими уголами. Хоть одинаковыми, хоть разными по длине. Важное замечание, которое стоит как-то для
себя отразить, что мы долгое время работали с однородными гиперграфами. В этих теоремах они
не обязаны быть однородными. Так, доказательства теоремы 1. Рассмотрим случайную раскраску,
банальную и случайную раскраску. Ну, то есть, с вероятностью одна-вторая,
независимо друг от друга, красим каждую вершину в красный или синий цвет.
У нас n вершин в этом РН. Каждую, независимо от остальных, красим в красный или синий цвет,
с вероятностью одна-вторая. Так, что, товарищи, такое разность между числом красных и числом
синих вершин? Как ее удобно записать? Вот удобно записывать так. Давайте считать,
что хи от ик это есть плюс единица с вероятностью одна-вторая и минус единица с вероятностью одна-вторая.
Это похоже на случайное блуждание. Помните, как пьяник ходит по прямой? Либо добавляет единицу к
своей позиции, смещается направо, либо вычитает, смещается налево. Но здесь мы это интерпретируем как
красный цвет и синий цвет. Просто красный цвет это плюс единица, синий цвет это минус единица.
Тогда, если мы обозначим немножко противоречиво хи от митого сумму по всем жи, принадлежащем
митому величин хи от жи, то это и будет разность между количеством красных и синих вершин.
Сложим единицы, вычтем, прибавим еще. Сложим столько, сколько красных, вычтем столько,
сколько синих. Это будет хи от митого. Это как раз вот величина уклонения, насколько митая
равномерна пополамно или не пополамно покрашена. Но давайте посчитаем вероятность того, кто
существует. И такое, что модуль хи от митого больше не равняется, ну или там больше строго,
чем корень из 2n, благориф не двое. Но я хочу доказать, что это вероятность меньше единицы,
конечно же. Так, друзья, понимаете, что если мы докажем, что это вероятность меньше единицы,
то мы и получим то, что утверждает теория М1. Тогда с положительной вероятностью, наоборот,
будет для любого и модуль хи от митого не превосходит, а модуль хи от митого — это и есть
разных чисел красных и синих вершин. Ну, по модулю взятую. Ну и там тоже имел с виду понятное дело,
по модулю просто красный и синий — это симметрично. Ну вот, так, ну давайте так сначала. Вероятность
того, что модуль просто хи от какого-то митого больше, чем какое-то а. Вот это, товарищи,
ну точно случайное блуждание. Мы берем вот этого пьяницу и складываем в него позиции плюс-минус
один. У нас для этого было неравенство. Все, вспомнили что-то? Да, минуса квадрат поделить на
удвоенную мощность вот этого множества. Значит, оценка была такая. Два? Ну два, потому что по модулю.
Помните, да, я смотрел, когда он вправо уходит от кабака, ну влево все симметрично. Это вот мы
в прошлом году, в прошлом семестр подробно обсуждали. Там еще было неравенство зумы,
потом для хроматических числов мы применяли. Ну вот не превосходит два на е в степени минус
а квадрат, вот это а квадрат, и поделить на удвоенную мощность митого. Ну сколько раз мы
бросали монетку, сколько раз пьяница смещался, он же смещается столько раз, сколько мы жишек вот
этих суммируем. А суммируем мы их модуля митого, мощность митого. Так, ну я уже сказал, я не знаю
чему равняется мощность митого, она точно не больше чем n. Не больше, не меньше, не больше. Да, в
дроби не меньше, а потом с минусом не больше. То есть это будет не больше чем два в степени, два на
е в степени минус. Ну давайте а возведем в квадрат, вот наше а, возведем его в квадрат, будет 2n логарифм
2m, и делить надо на 2n, потому что я сказал мощность митого не больше чем n. Фок-чфок, как всегда.
Так, и что у меня получилось, у меня получилось 1mt, е в степени минус логарифм от 2m, это 1 поделить
на 2m, ну еще на двойку умножить получается 1mt. Но у наших гиперграф без кратных ребер, все остальные
его ребра дают вот здесь вот значок строго меньше, потому что они имеют уже мощность строго
меньшую чем n. Поэтому вот эта вероятность, но она оценивается суммой таких, да, но она все-таки
меньше, чем m, умножить на 1m, то ребер всего m штук, и вот мы каждый раз складываем величины, только одна из
которых может равняться 1mt, а все остальные точно меньше. Все, мы получили то, что нужно. Вот, я что-то еще успею.
Так, вот это теория. Вот она простая, а вот эта теория довольно сложная, я все равно не начну
ее сегодня доказывать, но я хоть расскажу основной инструмент, который в ней используется. Это само
по себе интересно, потому что метод новый тоже вероятностный, но основанный на инструменте,
который называется энтропия. Плово энтропия вы могли слышать в контексте дискретного анализа в
самом начале, когда я оценивал c и z по а умножить на n. Но сейчас я более общо скажу, вот представим себе,
что у нас есть какая-то случайная величина, ну пусть принимающая дискретное множество значений.
Сейчас напишу.
Так, есть какая-то случайная величина x, там принимающая какое-то множество значений s,
ну s скажем так, не более чем счетное.
Я не буду определять энтропию в недискретном случае, пусть не более чем счетное. Может быть
конечное, может быть счетное множество значений у x. Энтропия x обозначается буквой h от x. Энтропия
случайной величины x, это есть минус сумма по s маленькая из s большого, вероятность того,
что x равняется s маленькой на лог двоичной той же самой вероятности.
Ну вот такой вот объект в теории вероятности, я долго не хочу комментировать, но называется
эта штука энтропия. Ну да, можно смотреть просто, что будет если распределение равномерное,
что если не равномерно, вот там энтропия будет меняться важным образом, но нам это будет вот как
некий комбинаторный инструмент. Так, ну вот, это энтропия такая. Ну можно в принципе определить
энтропию для нескольких случайных величин, например, что такое h от x и y, но мы просто
воспринимаем вот это вот как случайный вектор. У него тоже есть некоторое множество значений,
и мы вот так вот подставляем сюда, но просто s тоже будет вектором. Понятно, что такая энтропия
нескольких случайных величин. Там местная такая энтропия, это мы просто воспринимаем, это как
случайный вектор, у него есть какие-то векторные значения. Можно 2 написать, можно там 100 написать,
сколько угол, и тогда рисуем. Теорема, которая нам поможет в наших изысканиях и которую я надеюсь
сейчас успеть доказать, она не очень сложная, но очень нам полезная, она говорит, что h от x и y
точно не больше, чем h от x плюс h от y. Вот его я сейчас формально докажу. Это нам пригодится
при доказательстве теоремы 2, которая пропала, потому что я ее стек. Так, смотрите, доказательства
я вот так вот напишу. Так, давайте так, пусть у принимает на омега соответственно значение из
множества t большое, ну а x из множества s большое попришь. Что у нас получается? Давайте я вот так
напишу h от x плюс h от y минус h от x и y, перенесу вправо и буду доказывать, что это не отрицатель,
буду доказывать, что вот это выражение не отрицательное. Это равно, давайте я сначала напишу
вот это минус h от x и y, ну чтобы оно с плюсом было, тропия она сама по определению с минусом,
а тут минус, минус на минус даст плюс, вот я его сначала напишу. Я напишу сумма по s из s,
сумма по t из t, вероятность того, что x равняется s, y равняется t, ну вот вектор принимает значение
st на лог двоичной той же самой вероятности, x равняется s, y равняется t. Так, ну а тут будет со
знаком минус каждый из антропии, потому что она просто по определению со знаком минус,
с из s, вероятность того, что x равняется s, на лог двоичной той же самой вероятности и минус
сумма по t из t, тут y равняется t, лог двоичной той же самой вероятности, y равняется t. Ну все занудно,
понятно, просто по определению переписал. Теперь смотрите, если я вот здесь вот, например,
добавлю еще одно суммирование по t маленькому, а вот сюда через запятую запишу y равно t маленькому,
то это же будет правдой по формуле полной вероятности. Но давайте так и сделаем. Сейчас я попробую
сразу написать, значит, сумма по s из s большого, сумма по t из t большого. Видите, я вот эту штуку
сейчас хочу написать и тут, и на самом деле тут тоже. Но тут добавлю соответственно суммирование
по s маленькому и дорисую вперед x равняется s маленькой. Понятно, да, как я делаю? По формуле
полной вероятности. То есть я выношу за скобку вероятность того, что x равняется s маленькой,
y равняется t маленькой. Оно теперь вместе тут, и тут, и тут. Постал, как я добавил суммирование.
А в скобках останется лог двоичной. В кислителе будет вот эта вот вероятность.
x равняется s, y равняется t. А в знаменателе будет вероятность того, что x равняется s,
умножить на вероятность того, что y равняется t. Ну потому что они со знаком минус под логарифом.
Ну как, сложили просто логарифы, вычислили. Получилась такая дрот.
Давайте. Ну это, конечно, не ноль, вообще-то говоря, да. Какое было бы счастье, если бы
случайно величины x и y были независимы, то вот эта дрот равнялась бы единице и мы бы
получили вожделенный ноль. Но правда, это было бы точным равенством, а мы все-таки утверждаем,
что это больше либо равно. У нас ничего не говорится про то, зависимы эти величины или нет.
Ну ладно, это простой трюк.
Особенно набивать ему цену, наверное, и не приходится. Так, смотрите, мы вот как сделали.
Это равно пока что сумма по s из s большого по t из t большого. Так, я напишу вот как. Вероятность
того, что x равняется s, на вероятность того, что y равняется t. Так, это я их как бы перемножу.
Но, естественно, я на это должен и поделить теперь, потому что этого не было. Я пишу совместную
вероятность x равно s, y равно t. Делю на штуку, которую я здесь вынес. x равно s, y равно t. И
пишу лог двоичный. А здесь то же самое. Вероятность того, что x равно s, y равно t,
делить на произведение вероятности. x равно s, y равно t. Согласны? Но давайте я вот это вот
все обозначу z с индексом st. Но тут, естественно, тоже написано z с индексом st. Теперь я введу вот
такую функцию f от z равняется z на лог двоичный z. Но она вот здесь написана как раз для zst.
Понимаете, что это функция выпуклая?
И она суммируется с коэффициентами, сумма которых самих этих коэффициентов по всем
s и t маленькая равна одному. Пользуемся неравенством выпуклости. То есть мы говорим,
что вся вот эта сумма с этими коэффициентами zst на лог двоичный zst, она больше либо равна
за счет выпуклости? 5 это сумма, 5 это сумма больше либо равна, чем f от суммы по s из s,
сумма по t из t, f от zst. Вот так вот. Сейчас, наверное, еще коэффициенты надо написать,
да? Коэффициенты забыл. Коэффициенты, вот эти вероятности того, что нет, сейчас,
сейчас еще сейчас. А, какой, что ж я пишу-то? А, все понял, вот вот так, извините, наговорил не
того, поторопился вот так умножить на zst. Вот так, о господи, да, вот это вот большая, вот она,
все, все, запутался. Сейчас распутаемся, сейчас распутаемся, извините. Смотрите,
мы суммируем по s маленькому и t маленькому, коэффициентами, сумма которых равна единице,
что? Сейчас не то написал. А, ой-ой-ой-ой-ой, вот тут тоже запутался, елки-палки. Ну,
конечно, коэффициенты-то вот такие, ой, извините. Еще раз, коэффициенты-то произведение passion.
Сумма с этими коэффициентами f от zst не меньше, чем f от суммы с этими коэффициентами просто zst.
Это не равенствует вкусу.
Идет суммирование с какими-то коэффициентами с весами, выражение f от z с какими-то индексами.
f выпуклая функция. Поэтому эта сумма с весами не меньше, чем f от такой же суммы. Такими же
весами, но уже взяты в точке просто zst. zst это вот такая вот дробь, но она же умножается как раз
на свой знаменатель. Коэффициент это знаменатель этой дроби. То есть вот это все, что здесь написано,
вот это все, это есть просто вероятность того, что x равняется s, y равняется t. Почему я запутался в
итоге получается-то вот так. Видите, да? Сократилось. И мы снова суммируем по всем
s, по всем t вот такие вероятности. Ну единицу же мы получаем. То есть мы получаем f от единицы,
а это есть один налог двоичный один. И это таки ноль. Все. Ну простите, простая вещь, я просто
немножко неаккуратно рассказал, виноват. Попутался в очевидных местах, не то написал. Но сейчас все
правильно. Все, я доказал неравенство. Вот в следующий раз мы этим как-то хитро воспользуем.

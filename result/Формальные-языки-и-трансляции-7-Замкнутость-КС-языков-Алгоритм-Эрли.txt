Всем доброго дня! Мы с вами продолжаем изучать темы, которые связаны с грамматиками.
Сегодня у нас вторая лекция. Какая вторая? Третья лекция по грамматикам.
И сегодня мы с вами закончим ту тему, которую разбирали в прошлый раз.
Во-вторых, мы с вами разберем сегодня новый алгоритм.
Начнем его разбирать. Не уверен, разберем ли сегодня его или нет,
потому что мне сказали, что по нему нужно пройтись достаточно конкретно.
Потому что с нормальной симпточкой мало кто пишет.
Как ни странно. В общем, алгоритм называется early.
Он назван в честь автора этого алгоритма.
И мы с вами должны закрыть еще гештальт по поводу темы замкнутость КС языков.
Но давайте прежде, чем мы с вами как обычно начнем, я спрошу, что было в прошлой серии.
Да, мы с вами привели грамматику к нормальной форме холмского раз.
Нет, мы доказали замкнутость КС языков только относительно объединения, конкатинации и итерации.
А про другие операции мы пока что еще ничего с вами не говорили.
Пересечения, дополнения и так далее.
Так, что еще мы с вами успели разобрать вкратце?
Да, принадлежит к автоматному языку.
Мы с вами сформулировали, точнее рассказали идею доказательства лемма о разрастании.
Сегодня мы его доведем.
Мы с вами привели грамматику к нормальной форме холмского раз.
Во-вторых, рассмотрели идею доказательства лемма о разрастании.
И в-третьих, мы с вами рассмотрели первый парсер, который был динамическим программированием.
За кубическую степень.
Видимо, я снова начну светиться отчасти, потому что...
Все, хорошо.
Сегодняшний план это разобрать лемма о разрастании детальным образом.
И, во-вторых, разобрать алгоритм Эрли.
По крайней мере, начать его смотреть.
Давайте сформулируем лему, которую мы с вами пытались доказать в прошлый раз.
Лему стоит в следующем.
Оно называется его разрастание для кс-языков.
Пусть у нас l это кс-язык.
Тогда существует такое p, что для любого слова w, опять же, длина которого хотя бы p,
существует разбиение в виде пяти слов.
Ой, Господи, теперь главное правильно перечислить.
Как там? x, y, u, v, z.
u, y, v, z.
Да, значит такие, что у нас длина слова u, y, v не больше, чем p, длина слова u, v больше 0.
Такая, что для любого k, тут я почекну, больше равного 0,
будет выполнено следующего свойства.
x, y, v, z будут лежать в том же самом языке.
Так, я забыл, что для любого слова из языка это верно.
Вот такая вот интересная и смешная лемма.
Как ее доказывать? Давайте вспомним идею.
Смотрите, давайте рассмотрим J.F. и N.F. Хомского.
Напоминая, правила имеют вид такие a в bc,
где bc не стартовой силы, либо s в epsilon, либо a в a.
И рассмотрим, положим p равное 2 в степени количества нитерминалов.
Тогда давайте оценим высоту дерева вывода.
Относительно длины слова.
Ой, извините.
Смотрите, идея какая.
Значит, если у нас слово однобуквенное,
тогда высота нашего дерева будет равняться чему?
А?
Да, 2 в степени количества нитерминалов в нашей грамматике в нормальной форме Хомского.
Смотрите, если у нас длина слова один, то высота дерева вывода будет равняться один.
То есть у нас скорее всего будет правило, s выводит a.
Если у нас длина слова два, то, скорее всего, высота дерева вывода будет хотя бы два.
Почему? Потому что у нас будет s какой-нибудь, наверное, a-b.
Здесь раскроется a, здесь раскроется b.
И тогда мы можем сказать с вами следующее, что высота дерева будет больше или равна чему?
Ну, по крайней мере, вот такая оценка будет верна точно.
Даже плюс один.
Да.
То есть, смотрите, каждый раз у нас слово разрастается как минимум в два раза.
Точнее, каждый у нас нитерминал порождает не более чем два нитерминала.
В итоге у нас по факту высота дерева ограничивает сверху длину слова.
Вот приблизительно вот в таком масштабе.
То есть, опять же, скажу, что нам не важно точно...
То есть высота ограничивает сверху длину слова, значит высота дерева будет больше равна, чем длина слова.
Для конкретного слова.
Ну а что, давайте тогда подставим.
Если мы рассмотрим слово w, длина которого больше равна чем два в степени n,
тогда высота дерева нашего будет по нитерминалам, я прошу обратить на это внимание,
будет больше равна длина n плюс один.
А это что значит?
Что у нас высота дерева больше равна, чем количество нитерминалов плюс один.
У нас найдется вот такой нитерминал, который повторяется два раза.
Рассмотрим самый глубокий нитерминал.
Почему мы с вами рассматриваем самый глубокий нитерминал с точки зрения второго вхождения?
Потому что у нас получается вот такая вот интересная история.
У нас из s будет вот такой нитерминал, который повторяется дважды.
И вот в этом нитерминале мы рассматриваем самый глубокий нитерминал.
Потому что получается вот такая вот интересная история.
У нас из s будет выводиться некоторое xaz.
Дальше, поскольку у нас в этой цепи находится вот такое вывод, мы получаем вот такое дерево.
Возьмем вот этот вот кусок и посмотрим на левую и правую часть относительно него.
Эта часть у нас будет пара слов uv.
Мы рассмотрим, значит, вот это вот нитерминал.
Рассмотрим цепочку вывода нашего слова и рассмотрим такое дерево, у которых вот эта вот высота будет минимальна.
Рассмотрим такое нитерминал, который повторяется два раза, у которого длина от, собственно, вот этого слова вывода y до второго вхождения будет минимальна.
Зачем мы так делаем?
Да, смотрите. Зачем мы взяли вот такую хитрую вещь? Высоту минимальна.
Потому что предположим, что у нас с вами длина у yv будет больше, чем p.
А тогда что это означает?
Это означает, что вот эта штука будет равняться 2 в степени n.
Но тогда у нас высота дерева вот этого будет больше, чем количество нитерминалов плюс 1.
И значит, здесь есть два одинаковых нитерминала.
Тут, наверное, даже больше можно поставить.
И в итоге, смотрите, что у нас происходит. У нас уже другой найдется нитерминал, где-то здесь b и где-то здесь b, у которого глубина еще ниже вот этого второго вхождения.
Ага, понятно вот эта идея? Как мы ограничиваем сверху длину нашего слова?
Если мы взяли этаж минимально, оно больше, чем n, то у нас, по нашему зрению, найдется внутри нитерминал.
Ну да, именно так и говорится. Просто, возможно, я это другими словами сказал.
Ну да. То есть у нас найдется второе вхождение нитерминала. Поэтому это нас приводит в противоречию.
А вы, наверное, до этого не знали, что нитерминал есть?
Чего?
Нет, не обязательно. То есть они вот…
Ну да, то есть смотрите, если мы рассмотрим вывод из a в уйз, то здесь две b будут в одной и той же цепи.
Вот так будет, да. И у них высота будет минимальна.
А осталось понять, почему длина uv больше нуля?
Почему длина слова uv будет больше нуля?
Ну у нас а, а, а и еще что-то.
Ну смотрите, да. Почему это так? У нас грамматика в нормальной форме хомского. Напомню.
Значит, в нашем a за один шаг грамматики мы вывели какое-то bc, образно говоря.
А дальше мы говорим следующее, смотрите.
Дальше у нас либо из b, либо из c выводится а за какое-то количество шагов.
То есть мы можем сказать, допустим, без ограничений общественности у нас из b выводится следующее а.
Тогда у нас получается альфа а, здесь у нас бета c.
Нам не важно в каком порядке раскрывать правила грамматики.
Но тогда мы с вами понимаем, что либо из альфы у нас будет выводиться какой-нибудь х-а, у-с.
Вот. И поскольку у нас с вами здесь получается вот такая вещь, на самом деле не важно.
А c у нас не эпсимум порождающий.
Помните, мы в прошлый раз говорили про символы, которые у нас есть.
То есть у нас все не терминалы, которые появляются в правых частях правил в грамматике нормальной формы хомского, они не являются эпсимум порождающими.
Было такое.
Вот. А это значит, что если у нас из c выводится какое-нибудь слово t, то длина t больше, чем ноль.
Собственно, и как раз получается, что у нас из этого будет длина слова b больше, чем ноль.
То есть рассматриваем, тот не терминал, который у нас не превращается в цикл, не заводит наш цикл, он не эпсимум порождающий, значит он выводит какое-то не пустое слово.
Да.
Мы выяснили, что у нас высота треугольничка не больше, чем n.
Высота вот этой вот, всей Бадурины не больше, чем n.
То есть, короче, вы знаете, что это не только высота треугольничка, но и вообще вплоть до низа.
Да-да-да, вплоть до низа.
Понятно, почему катинация слов u, e, v не пустая?
Да.
Ну а как получить теперь слово x у вкатай, y в вкатай, z?
Да, из a выводить уав, выводить уав, уав, то есть подцеплять это дерево несколько раз.
Понятная идея?
Все.
На этом мы с вами заканчиваем доказательства этой леммы.
И переходим к следствиям из этой леммы.
Первое следствие, которое мы можем с вами сказать, это рассмотреть вот такое вот слово.
Оно не является контекстом свободным.
Почему это такое?
Давайте подумаем.
Опять же, нам нужно сформулировать отрицание леммы о разрастании.
Стираем график с денежными потоками.
И пишем следующее.
Смотрите, нам нужно доказать, что для любого P...
Я пишу отрицание.
Существует слово w такое, что длина его хотя бы P.
Давайте рассмотрим слово a в п, b в п, c в п.
Тогда у нас для любого п, для любого в п, c в п, c в п, c в п, z в п.
тогда у нас для любого для любого разбиения у, у, в, з. Вот у нас такое
разбиение такое, что длина слова у, у, в не больше, чем п. Вот. Что означает, что длина
этого слова не больше, чем п. Могут ли в нём встретиться три буквы одинаково
одновременно? Могут ли в слове у, у, в быть три разных букв? В плане А, Б, С.
Ну нет, не может быть. Смотрите, потому что последняя А здесь находится, первая С
находится здесь, а тут Б, Б букв Б. А? Да, для фиксированного слова нам нужно
понять, что для него ломается разрастание. Вы видите, то есть у нас получается, чтобы
было и буквы А, и буквы Б, и буквы С, у нас должно быть три этих, П плюс два, П плюс два даже.
Да, у нас длина не больше, чем П. Мы можем сказать, что длина, не умоляя общности,
можем сказать, БО, без ограничения общности, можно сказать, что количество буква в этом
слове равняется нулю. Да, но при этом, да, при этом, поскольку у нас длина слова УВ больше,
чем ноль, следовательно, там существует какая-то буква, опять же, без ограничения
общности можно считать, что количество букв Б больше нуля. Опять же, тут неважно на
вариации. Ну и тогда давайте посчитаем. У в квадрате У, В в квадрате З количество букв Б.
Это количество слов, букв В в слове В, плюс количество букв В в слове ОВ. Но это будет больше,
чем количество букв в этом же слове, букв С, ой, букв А. Потому что эта штука равнается нулю.
То есть у нас количество букв В в нашем слове будет нарушено. А это значит, что наш язык
не автоматный, ой, не КС, потому что у нас нарушено разрастание, условие разрастания ЛЕМа.
То есть небольшие манипуляции с количеством букв. Ну если у нас в УВ количество букв А ноль,
тогда логично, что и в УВ количество букв А равняется нулю. Бывает, не чувствую. Так,
понятная идея. Из этого можно сделать некоторый другой вывод. У нас появился не автоматный язык.
Ой, извините, не КС язык. Да, не знаю. Но с учетом того, что у нас машины тюринга будут
эквалютны вообще любому распознаванию слова, да, то можно в машине тюринга запрограммировать цикл фор,
который проверит это все дело. Ну, который проверит, что это слово принадлежит языку или нет. В нем нужно
в количестве памяти посчитать количество букв А, количество букв В, количество букв С. Ну да,
по-моему, контекстно зависимыми грамматиками это тоже можно распознать. Там вообще любую комбинацию
А, В, В, В, С, В, Д, В можно задать. Нет, нет. У нас память лимитирована немножко. Ну да, тогда машины
тюринга уже. Да, значит, смотрите, некоторые выводы из этого состоят. Следствие. А КС языки не замкнуты относительно
пересечения. Как вы думаете, как это доказывается? Что контекст свободной языки не замкнут
пересечения. Ну, давайте рассмотрим такой пример. АВН и БВМ в цевкатой пересечу с авкатой БВМ
ВЦВМ. Ну, то есть, смотрите, здесь мы грамматику легко можем задать. Нам достаточно создать АВН и БВМ
и перевести его цевкатой. То есть, это можно сделать вот так вот. АТБ или Эпсилон, АИЗУ выводить получается С, ЦУ или Эпсилон.
Вот, то есть, вот это грамматика для этого языка. А в пересечении они дают язык АВН, ТБВМ и ЦВМ. То есть, они не замкнуты относительно пересечения.
Ну, я смотрю, там про лошади что-то было весело сказано. Ну, я что-то тоже не увидел.
Ну, да. Да, непонятно, при чем лошади здесь.
Ну, конечно, конечно. То есть, наш цель был получить именно этот язык. Следствие второе, которое возникает, это почему вот эта штука не замкнута относительно дополнения?
Эпсилон. Ну, затем, что мы разделили нашу грамматику на две части. Одна будет заниматься выводом буквы А и Б, а вторая будет заниматься выводом буквы С.
Ну, слева из нуля буквы А и Б и С тоже подходят.
Второе следствие, которое здесь есть, это КС языки не замкнуты относительно дополнения. Давайте это обсудим.
Нет. Нет, относительно объединения не замкнуты.
Правда, вы получаете, скорее всего, неоднозначную грамматику после этого.
КС языки не замкнуты. Давайте я буду не капсом писать.
Ну, можно. Ну, а потом нам выслушивать, а почему это правда, когда вы приведите конкретный пример.
Возникает проблема. Итак, смотрите, пример. Сигма со звездой без АВ, СВ, БВ.
Давайте поймем, как задать все слова, которые не имеют вид АВ, БВ и СВ.
Первая категория Б идет перед А.
Ну, это легко, кажется. Это даже регуляркой какой-нибудь можно задать.
Ура, задали регуляркой. Ну и понятно, все такие условия можно задать регулярным выражением.
Остаются что? Остаются слова вида АВ, БВ, СВ, КАТЫ, где у нас N не равно M, или M не равно K, или N не равно K.
Но каждое из этих условий можно легко запрограммировать контекстно-свободной грамматикой.
А объединение контекстно-свободных языков является контекстно свободным.
Объединение вот этих всех условий, которые будут, они как раз дают дополнение языка АВ, БВ, СВ, а дополнение этого языка
ожк является овента б венты цodka
который не является контекстом
свободной
поэтому этот язык не является
контекстом свободным
так сказать, поэтому·
контекст свободного языка
замкнуюто есть еще одно условие
которое мы сейчас не будем проверять
для этого нам нужно будет
вернуться к этой теме еще
еще через лекцию, и мы докажем, что контексты свободной языки замкнуты относительно пересечения с регулярными языками.
То есть, если вы контекстно-свободный язык пересечаете с каким-то регулярным языком, то вы получите контексты свободный язык.
Но для этого нам нужно будет перейти к автоматам, потому что доказательство через нормальную форму хомского, оно неприятное.
Ну да, а проблема в том, что регулярные языки это под множество кс-язаков, это не все кс-язаки.
Да, ну ладно, давайте придем к грамматике, и сегодня мы с вами детальнее разберем деревья вывода.
Потому что в чем у нас был алгоритм...
Давайте я пока спрошу, понятно ли вот то, что было в первой части нашего балета.
Хорошо, переходим к второй части нашего балета, и сегодня мы с вами поговорим про то, как выводить слова и выведем отдельный алгоритм парсинга.
Давайте мы с вами поймем, что нам необходимо для того, чтобы выводить слово в грамматике.
Нам нужно построить дерево вывода. Причем построить дерево вывода не хотя бы одно.
Какое-то дерево вывода, которое распознает слово целиком.
И смотрите, в чем еще заключается мотивация.
Вы когда-нибудь играли в конструктор LEGO или подобные?
В чем суть конструкторов LEGO состоит?
А каким образом строить?
Не с кирпичиком, а при этом как у вас один кирпичик прикрепляется к другому?
Там есть баги.
Суть в том, что тут есть блоки и кирпичи, которые можно присоединить друг к другу.
То есть вы берете один кирпич, присоединяете к другому кирпичу.
А теперь мы будем делать то же самое.
Вот у нас с вами по факту есть правила вывода в грамматике.
То есть у нас есть правила вывода в грамматике, у нас есть не терминал, он раскрывается в посадке с не терминалом.
Так вот представьте, что у вас не терминал, который есть.
Это вот эти присоски, при помощи которых вы прикрепляете другое правило.
То есть у вас вот такой схлопывающийся механизм возникает.
То есть у вас есть не терминал, вы можете его присобачить к другому месту в выводе грамматики.
Я просто сейчас объясняю мотивацию алгоритма.
Главное теперь, чтобы еще одна мотивация, пазла собирали.
Что в пазле обычно происходит?
Там есть пупырочки.
Да, в пупырочки вы в принципе присоединяете пупырочки.
Но может оказаться так, что вы какую-то пупырку присоединили, но при этом после этого пазл не собирается.
Ну да, то есть вам нужно проверять еще какой-то контекст, относительно которого вы ходите.
Вот, и здесь контекстом будет определенное слово, которое мы разбираем.
То есть мы будем присоединять с вами наши не терминалы таким образом в деле вывода, чтобы мы получали с вами вывод.
Ну хорошо.
Не знаю.
Ну вот нету контексту этого пазла.
Вот, да.
Да, контекстозависимые пазлы.
Хороший термин, мне нравится.
Ну то есть понятно, в чем заключается мотивация.
Вот.
Да, развеселились.
Вот смотрите, вот у нас есть грамматика.
Только суть в том, что считать, что у нас здесь пазл не один, а количество блоков каждого типа у нас бесконечно много.
Блок каждого типа это правило вывода в грамматике.
Ну то есть у нас есть, грубо говоря, два блока.
Первый блок это из S в SBS, а другой блок это из S в Epsilon.
Да, живем на заводе Lego, правильно.
У нас есть абстрактная фабрика по производству конструкторов Lego.
Нам надо собрать наше слово произвольное.
Вот, и смотрите в чем суть.
Давайте просмотрим конкретное слово и попробуем построить дерево разбора.
Это PSP, да, это правильная скоробочная последовательность.
Ну, один из примеров вывода разбора состоит следующим образом.
Что у нас есть из S, мы выводим SBS, а SBS Epsilon Epsilon.
И вот давайте мы с вами проэмулируем этот эффект построения этого дерева.
Именно конкретно этого дерева.
Деревь может быть больше, чем одно, сразу подчеркну.
Ну вот, что у нас с вами происходит.
Вот давайте посмотрим.
Значит, мы находимся здесь, здесь точка неспроста.
То есть у нас есть блок S, к которому нам нужно присоединить деталь.
Значит, мы понимаем, скорее всего, что следующее слово SBS, это нам знание дает как некий оракул,
которого мы пока не знаем.
Вот, а дальше мы берем и прихлобучиваем деталь SBS.
Смотрите, при этом, чтобы состыковать эту деталь,
мы, помимо того, что будем хранить текущая позиция в дереве,
мы будем хранить еще и позицию, откуда мы пришли.
Для того, чтобы, если у нас все дерево, все правила закроются, мы знали, куда возвращаться.
Ну да, то есть у нас будет ссылка на себя и ссылка на родителя, где мы сейчас находимся.
Причем, смотрите, дальше мы прочитываем букву, потом мы спускаемся вниз.
Получаем ссылку на родителя.
Давайте как раз сейчас я нарисую вот эту часть дерева, чтобы мы с вами кое-что поняли.
Давайте я нарисую даже это дерево.
Вы пропустили самую веселую часть лекции.
Вот, смотрите, то есть мы с вами сейчас находимся вот здесь, а позиция родителя вот здесь.
Дальше что мы делаем? Мы дальше спускаемся вниз, для того, чтобы раскрыть часть дерева.
Мы могли бы пройти дальше, при желании.
Дальше читаем следующую букву.
Потом что происходит? Смотрите внимательно. Мы спускаемся в дерево вывода.
И находимся здесь.
А вот эта проблема в том, что вам придется так или иначе перебирать все возможные варианты.
То есть для всех возможных деревьев вывода.
Вот, мы находимся в этой позиции. Что нам нужно сделать?
Мы с вами закрыли правило грамматики, которое у нас было.
То есть у нас получается, смотрите, позиция такая, что мы находимся здесь и точка у нас находится здесь.
Давайте вспоминать, что было у родителя.
А у родителя была здесь, смотрите, какая ситуация.
У родителя была ситуация такая, что мы находились здесь и точка находилась здесь.
Да, только мы сейчас будем делать умные…
Да, смотрите. Вот сейчас произойдет схлопывание нашего конструктора.
Вот у нас есть позиция здесь, позиция в родителе.
Здесь у нас есть позиция здесь, позиция в родителе.
И вот эти вот две части у нас, вот эта вот точка у родителя и вот эта наша позиция родителя, они схлапываются, и в итоге мы можем передвинуть вот эту точку вот сюда.
То есть тем самым завершить дерево под разбор вот этого правила. То есть получается, мы находились в текущей позиции, разобрали все под дерево, которое было здесь, возвращаемся наверх.
Все еще не очень понятно, как мы это выбрали. То есть ВСВС еще понятны, какое логическое может быть? В чем-то более сложном, кажется, могут быть проблемы.
Ну да, в более сложном могут быть проблемы, но здесь правила из СВ эпсилон. То есть правила из СВ эпсилон разбираются за ноль символов.
А, ну это хороший вопрос. Для этого нам нужно...
Да, поэтому рекурсивный разбор здесь не подходит, и надо придумать что-то другое.
Сейчас поймем, как отсекаться. Как раз для этого нам нужен этот алгоритм.
Давайте заметим, каким образом можно попробовать закодировать точки здесь. Это важный тезис.
Точки, да. Точки можно закодировать, как под слова, верно. Можно сказать, что вот эту точку я буду кодировать символом 1.
Почему я эту точку буду кодировать символом 1? Потому что вот эта позиция, когда мы находимся в этой позиции, мы уже разобрали один символ нашего слова.
Вот эта точка будет кодироваться символом 2. Потому что мы разобрали А и А. Мы уже встретили два символа в нашем слове.
Приблизительно. Не, оно будет тут более умным.
Оно будет включать в себя еще правила грамматики, в которых мы сейчас находимся, в дереву.
Сейчас я покажу вам интересную вещь. И дальше идет интересная инфографика, как это обходится дерево.
Вот, и смотрите, что мы заметили. Мы дерево обходим в глубину. Ну как бы сюрприз-сюрприз. Мы спускали с дерева...
При этом, смотрите, если нам дать позицию кодирования точек разбора, то есть позицию точек в слове,
и правила, в которых мы находимся, мы на самом деле с вами можем однозначно восстановить дерево разбора.
Это тоже вроде не заставляет сложности. И точки кодируют правила вывода, которые у нас сейчас будут.
Поэтому давайте мы попробуем зашифровать вот эти вот блоки нашего конструктора через позиции точек и правила вывода.
Ну, смотрите, давайте я тогда сразу скажу, что мы с вами будем говорить следующее.
То вот, допустим, мы находимся в вот этой позиции. Зум картинки у нас. S, A, точка S, B, S.
Вот позиция вот этой точки кодируется единичкой. А потому что мы прочитали один символ.
Позиция этой точки кодируется двойкой, потому что здесь уже второй символ был прочитан.
Не, смотрите, мы привязываемся к конкретному слову. То есть у нас внизу написано слово A, A, B, B.
То есть когда мы находимся вот этой точке, мы разобрали вот эту часть слова.
Когда мы находимся вот этой точке, мы разобрали вот до сюда.
Вот, давайте тогда вот кодировать вот эту возможную ситуацию, в которой мы возникли следующим образом.
Что мы находимся в S? Значит, A, точка S, B, S.
Значением 1, вот позиции вот этой верхней точки.
И дальше будем хранить вот эти вот позиции, все вот такие ситуации возможных правил в определенном множестве.
Это множество будет называться DIT.
Что вот эта вот математическая абстракция означает?
Это означает, что мы сейчас с вами находимся в правиле вывода S, стрелочка, S, B, S. При этом находимся вот в этом конкретном месте.
Более того, если мы поднимемся в родительском деле вывода, мы с вами разобрали уже одно, один символ из нашего алфавита, из нашего слова.
А вот эта позиция точки кодируется вот этим вот символом и говорит, что мы разобрали два символа из нашего слова.
Чего?
Первый символ A, вот этот, второй символ A, вот этот вот.
Тут мы будем хранить две точки, одна реальная внутри текущего правила, а вторая, эфемерная, это точка родительского узла, то есть где у нас находится.
Значит, вот эта позиция точки шифрует то, где мы сейчас с вами находимся.
При этом, чтобы учитывать общий контекст слова, мы указываем, сколько символов слова мы разобрали, когда дошли до этого момента.
То есть здесь говорится, что мы разобрали два символа из нашего слова, это A.
Ну а чтобы вернуться, чтобы состаковать наш конструктор, нам нужно указать позицию родительской точки, из которой мы прыгнули.
И здесь, в этом моменте, мы с вами разобрали одно слово, один символ, поэтому вот эта вот позиция говорит следующее, что мы спустились в наши правила тогда, когда мы разобрали уже один символ нашего слова.
Вот. Да, понимаю, такая абстракция. А теперь фишка в чем?
Фишка в том, что мы сейчас по факту будем забивать на дерево вывода.
То есть мы сотрем все наше дерево вывода и утверждение, что только при помощи вот этих вот вещей можно управлять выводом.
При помощи вот только вот этих правил. Точнее, при помощи вот этих ситуаций, которые мы с вами задали.
Чего? Ну это по факту, какие переходы? А, типа правила переходов.
Да, да.
Да, да, нам нужно теперь научиться между ними ходить. И здесь нам нужно будет рассмотреть три варианта.
Давайте подумаем, каких три варианта нам надо?
Да.
Ситуация только это называется.
Во-первых переходом сплошных позиций точки в этом переходе С unnecessaryütях количества символов, которые мы до этой точки разобрали.
Почти. Значит, правила, количество позиций, которые мы разобрали до этой точки. Это двойка.
Это не позиции самой точки.
Нет, это не позиции этой точки. Вот это число означает сколько мы символов разобрали до тех пор, пока мы спустились в это правило.
Ну да. Ну да, получается. Значит, еще раз. У нас есть правило. У нас есть позиция точки в самом правиле. Дальше, количество символов, которые мы разобрали к этому моменту, и количество символов, которые мы разобрали до того момента, пока мы не спустились в это правило.
Но на самом деле мы понимаем, что мы можем хранить историю того, как это правило появилось. Давайте я сейчас как раз начну разговаривать про правила перехода, и дальше мы поймем с вами, что происходит.
Итак, давайте для каждого правила определим вот такое понятие. У нас ситуация. Это вот такое вот свойство. Это у нас не терминал и последовательность терминалов не терминалов.
Вот. Где И у нас может быть от нуля до длины слова, и G у нас тоже может быть от нуля до длины слова. Позиция родительской точки это И, позиция нашей точки это точки G.
Пример. Давайте попробуем сформулировать. Вот мы находимся здесь, и попрактикуемся с тем, а какая ситуация задается вот этим вот правилом.
D это мы будем специально хранить это все дело в сете, в котором будут объединяться все позиции символов на текущем слове.
То есть смотрите, D2 это означает, что это все возможные ситуации, в которые мы можем с вами попасть, разобрав два символа нашего слова, прочитав два символа нашего слова и стандартного потока.
Вот, поэтому это D2, это множество. Так, ну давайте скажи. Да, давайте. Давайте я его перепишу даже.
Это вот такой вот объект. А, стрелочка альфа бета принадлежит правилам нашей грамматики.
Смотрите, нет, здесь нужно скобочки правильно поставить. Вот так вот. То есть это вот такой вот карта. Да, слово изначально зафиксировано.
Что это за объект, да? Ну это по факту кодирование того места, где мы сейчас оказались. Ну, тут ситуация как раз хорошо описывает этот термин.
Да, это множество ситуаций, в которых мы оказались, когда мы разобрали, когда мы прочитали изводы g символов нашего слова.
Нет, и это позиция. Смотрите, еще раз. Давайте нарисую вот так. Значит, у нас вот здесь уже разобрано g символов нашего алфавита.
А вот если посмотреть, где мы здесь находимся, здесь мы разобрали и символов нашего слова уже. До того момента, как мы спустились в наше правило.
Ну j это количество символов в текущем слове, которые мы разобрали до текущего момента. Да, ну по факту можно сказать, что еще и g, просто мы их объединили по множеству.
Ну да, тройка. Да, до того, пока мы, ровно в тот момент, пока мы не спустились в вот наше правило грамматики.
Ну да, позицию родили, чтобы мы могли состыковаться. И это, кстати, нам позволит избавиться от бесконечного цикла. Потому что мы с вами уже привязываемся к конкретному блоку и можем отрекать эту позицию.
Для какого уровня? Не, не, не, мы отвязываемся от уровней, то есть это глобальное уже. То есть по факту нам нужно знать, сколько символов мы прочитали, сколько символов мы прочитали до этого и где мы сейчас находимся в нашем правиле. Все.
Ситуация, давайте. Формально, если говорить ситуацию, это просто тройка. Вот. Для конкретного слова и для конкретной грамматики. Ладно, давайте определим формально.
Для фиксированного слова w и сигма со звездой определяем и правило нашей грамматики.
Альфа-бета принадлежит ситуации. Это тройка.
Значит, а альфа точка бета и ж, где и от нуля до длины слова, от нуля до длины слова.
Вот. Давайте, раз такой вопрос возник, я сразу напишу формальный смысл вот этого всего.
Ну, по факту, да.
Вот это формальное определение. Теперь, если мы говорим про строгий математический смысл, давайте я сейчас тогда его напишу, раз оно возникло.
Вот это смысл. Математический строгий смысл вот этого факта.
Давайте я так напишу.
Тогда и только тогда, когда, смотрите, у нас есть из штрих вывод слова от нуля до итого, а, какой-то там получается, psi.
Дальше, за один вывод этого слова мы а раскрываем вы последовательность альфа-бета, psi, и дальше за какое-то количество шагов мы это раскрываем.
То есть, смотрите, мы будем говорить следующее, что у нас вот эта вот ситуация у нас конкретно определена в том и только в том случае, когда мы до слова a разобрали и символов нашей грамматики,
дальше мы за один шаг раскрыли правило, вот как раз которое у нас имеется на текущий момент, и дальше вот эта альфа раскодируется в символ грамматики, символ нашего слова от итого до житого.
Да, и жит, конечно, больше равно, чем и.
Ноль символов.
Ноль.
Ну если и равно жит, то значит альфа раскодировала ноль.
Вы знаете, это питоновская конструкция.
Так, я хотел спросить, вот теперь, когда я дал формальное вот это определение, формальный смысл того, что здесь указано, стало понятнее?
Да, хорошо, да, добавлю.
Что здесь указано? Стало понятнее?
Да, хорошо, да, добавлю.
Так, но кажется, что мы, если мы с вами определили теперь вот эфемерный смысл, который мы с вами можем достигнуть,
вот этим всем, да?
Давайте попробуем определить, каким образом мы с вами будем переходить в этой грамматике.
Как мы будем от одной ситуации переходить к другой?
Какие три варианта у нас существуют? Вот мы находимся в текущей позиции дерева вывода.
Да?
Ну просто именно альфа будет раскрывать символ от этого дожитого?
Нет, альфа как раз раскрывается символ от этого дожитого.
Оно жи, это то, что будет раскрыто с дожитого.
И это то, что было раскрыто до входа вообще в одну квест-операцию.
Да.
И это начало этого слова, ажи с дожитого.
Вот так.
Да.
А что такое экстрик? Почему экстрик?
А потому что, хороший вопрос, потому что здесь еще есть тонкий момент, мы добавляем новый стартованный терминал.
Когда?
Ну мы перемотались вот сразу.
Давайте мы сразу скажем, что мы будем рассматривать пополненную грамматику и добавим правила из H3 в OTS.
Я просто обычно рассказываю про мотивацию, а дальше уже даю формализм.
Ну раз мы пошли немножко по другому пути, давайте сначала немножко формализма, а потом мотивацию.
Вот это понятно, да?
Все, просто начало.
Типа вот это формально.
Теперь давайте попробуем определить правила перехода, чтобы у нас все было согласовано.
Первые правила перехода, которые у нас есть, это правило scan.
Смотрите.
Вот давайте посмотрим на вот это вот любопытное дерево.
Вот мы находимся вот в таком месте.
И у нас джитый символ алфавита, это А.
А что мы можем с вами сделать?
Операция, если что, называется scan.
И у нас получается следующее, что у нас А.
Джита, и оказывается внезапно, что джитый символ нашего алфавита это буква А.
Это множество, в которых мы будем хранить ситуацию.
Давайте, ладно, давайте будем писать вот так.
Да, согласен, надо уже про это, я тут уже буду просто алгоритм дать, вот так будем говорить.
Да, вот смотрите, у нас следующий символ это буква.
Что мы можем сделать в нашем слове?
Прочитать этот символ.
Ну да, конечно же.
У нас получается так.
И что у нас получается?
Назовите новую ситуацию, в которой мы с вами окажемся.
Вот, вот эта операция называется операция scan.
Могично, почему она так называется?
Потому что мы берем и сканируем наш символ отзвода.
Ну да, мы выкидываем эту ситуацию из рассмотрения.
Следующая картинка, которая у нас могла случиться.
Операция будет называться предикт.
Вот, мы находимся с вами А, альфа б бета и вводим гамму.
Мы находимся здесь.
То есть у нас была ситуация, альфа стрелочка, а стрелочка альфа б бета и g.
И при этом у нас есть правила грамматики, b вводит гамму.
Что мы с вами можем сделать в данном случае?
Пуститься в дерево.
Получаем.
Точка здесь, точка здесь.
Скажите, в какой ситуации мы с вами теперь окажемся?
Ага, позиции какие будут?
То есть у нас появляется новая ситуация вот такого типа.
То есть мы спускаемся вниз по дереву ввода.
Остался еще один вариант.
Да, ну просто while пока ситуация не обновляется.
Сейчас мы заметим один эксперимент.
Просто дублируем ее.
Мы просто говорим, что смысл еще раз опускаться.
Чего?
Ну позицию в дереве вывода.
Ой, позицию в нашем правиле, сколько символов мы прочитали, сколько символов мы прочитали до того, как спуститься.
Да, да, да.
Вот с таким состоянием.
Да.
Да, цель двинуть точку за b.
Вот.
Давайте еще одну картинку разберем.
Вот смотрите, мы находимся вот в такой ситуации.
Значит.
Так, что там у нас?
Альфа точка b.
И к.
И b.
Стрелочка гамма точка.
То есть мы закончили разбирать какое-то правило грамматики.
Вот таким образом.
То есть у нас получается у родительского дерева позиция i.
Здесь у нас позиция k, здесь у нас позиция g.
Как вы думаете, как подняться в это все дело?
Смотрите, мы находимся в конце дерева вывода, а это значит, что мы можем сделать следующую вещь.
Поставить точку сюда и забыть про дерево вывода, которое мы разобрали.
То есть у нас вот эта точка уезжает наверх.
А какая ситуация возникает в данный момент?
Понятно.
Индексы надо обновить.
Какой?
А сколько мы символов разобрали с того момента, как мы поднялись?
Вот он.
Вот этот индекс непонятно, да, какой будет?
Нет, ну стоп, все понятно.
Я утверждаю, что здесь i будет стоять, здесь будет стоять g.
Ну смотрите, у нас вот для вот этого куска, в котором мы с вами находимся, известно i и k.
То есть для вот этого правила, для вот этой ситуации мы знаем, что вот здесь мы разобрали k символов,
а до этого момента, до тех пор, как мы дожили, у нас здесь i символов есть.
Может переходим из нашего перегравка?
Для этого нам нужно пару, нам нужно пару найти, состыковать их.
И стыкуются они как раз, во-первых, по не терминалу, первая стыковка,
а, во-вторых, они стыкуются по вот этому символу k.
То есть вот получается, позиция родителя должна совпадать с текущей позицией правил, в котором мы спустились.
То есть вот это правило получилось.
Да, да, здесь как раз нужна пара ситуаций для того, чтобы вывести новую ситуацию.
Впускаемся вниз, раскрываем правила вывода.
Да, да, закрываем правила.
Такие вот штуки, мы их спеиваем.
Да, да, да. Ну не стэки, на самом деле, в множестве их надо хранить.
То есть нам нужно будет эффективно доставать вот это.
То есть смотрите, мы рассматриваем произвольную ситуацию gt, мы вытаскиваем позицию k,
нам нужно быстро и эффективно искать все позиции, все ситуации, в которых есть k.
А вот после того, как мы это сделали, у нас получается, что мы обработали k плюс g символы сразу, нет?
Нет, почему?
Нет, ну смотрите, вот здесь у нас g-тая позиция.
Вот здесь указано, что мы разобрали g символ наш слой, поднимаемся вверх, у нас, конечно, символ не меняется от этого.
Вот, а теперь смотрите, фишка в чем.
Вот давайте посмотрим на все это дело и поймем, как это эффективно хранить.
То есть у нас, смотрите, здесь g-та, здесь для того, чтобы нам пройти, нам нужно поставить k ту ситуацию,
то есть найти k, и здесь нам нужно выяснить позицию k символа.
То есть найти все ситуации, в которых разобрана k символов.
Внимание, вопрос. Как быстро находить?
Кажется, что для этого нам нужно все ситуации, у которых есть разобрана g символов, нам нужно обменить в какое-то множество.
Да, вот это множество, вот эти будут храниться в джитом.
Какое?
Да, да. То есть если мы говорим про эффективное хранение, все ситуации должны храниться следующим образом,
что у нас будет g-тый символ здесь, джитая первая параметризация, а вторая параметризация это не терминал.
По которому у нас хранится левая часть правил. То есть смотрите, образно говоря, так, давайте, вот это.
Вот эта вот ситуация в качестве примера будет храниться в правиле множества джитая от а.
То есть указывается, во-первых, позиция, вот эта позиция, а во-вторых указывается следующий символ, который идет за точкой.
Это эффективный способ хранения этих ситуаций.
Надо мне это в слайды вставить, чтобы типа было понятно.
Да.
Да.
Да, смотрите, вот как раз давайте обсудим. Добавим правило из h-трих выводит s.
Отдельно.
И проверим следующий старт. Давайте подумаем, если у нас есть правило вывода из h-трих выводит s.
То какой старт у нас будет?
Да.
Ноль-ноль.
Конец.
Да, ноль длина слова.
Вот. Если оказалось, что мы внезапно в разборе появилась вот эта вот ситуация, мы говорим, что слово принадлежит нашему языку.
Что это означает? Это означает, что мы разобрали наше все слово и у нас из вот этого терминала s выводится под слово от нуля до длины нашего слова.
Да.
Значит, мотивация. Почему мы это рассказываем?
Хоть алгоритм асимптотически будет работать за куб.
Тот же самый, что алгоритм Кока-Янгера-Касами.
Но для однозначной грамматики он будет работать за квадрат от длины слова.
Вот. И в среднем у него асимптотика будет ниже.
У этого?
У этого.
На практике он будет работать быстрее.
Да, вот это самая тяжелая вещь.
А состояние кодируется как минимум длиной слова?
Да.
Ну, если неправильно написать, то будет четвертая степень.
Окей.
Там амортизационная сложность возникать будет.
И алгоритм действительно будет за куб работать.
Вот, кстати, про алгоритм.
А там еще вроде и суммарный длиной переходов не нравится?
Да, да, да.
Да, на суммарное количество символов в каждой части.
Итак, смотрите, вот так вот это правило выглядит вот так.
Мы можем заметить, смотрите.
Да, да, но может, но типа если запустить, то будет работать быстрее.
И по памяти будет тоже эффективнее храниться.
Значит, смотрите, давайте заметим, что позиция вот этого символа,
она не меняется для каждого из выводов грамматики.
То есть здесь у нас из G to the G to the получается.
Здесь тоже мы не выдвигаемся.
То есть здесь есть G to the, G to the позиция.
Мы снова получаем G to the позицию.
У нас идет сдвиг символов только в том случае, когда мы делаем операцию scan.
Мы из G to the символа перемещаемся в G плюс первый.
Поэтому мы можем оперировать в терминах множество DGT.
И пишем следующее.
Мы добавляем множество ситуаций D0.
Ситуацию s-sh.s.00.
И дальше делаем следующее.
Пока у нас D0 меняется, мы выполняем операции complete и predict.
Вот это операция complete, вот это predict.
Можешь делать predict-complete, неважно.
Все равно множество пока не меняется.
Мы пытаемся это сделать.
То есть пытаемся спуститься вниз, закрыть правила.
Спуститься вниз, закрыть правила.
Зачем нужна именно комбинация complete-predict?
Почему нельзя обойтись только одной из них?
Как раз для того, чтобы у нас накопились правила вида s-стрелочка.
Смотрите.
Допустим, у нас вот такая вот...
Я не знаю.
Вот такие правила.
Вот. И утверждение, что здесь нам нужно будет выполнить predict несколько раз.
То есть после того, как мы все это выполним,
у нас множество D0 будет состоять из следующего.
s-sh.s.
Дальше s-sh.s.00.
s-sh.s.a.s.b.00.
t-sh.0.s-sh.t.a.0.
Но смотрите, какие здесь еще веселые ребята возникают.
Здесь еще возникают ребята следующие.
Здесь же появятся правила из s,
появятся правила из s, выводят s.a.s.b.
Или нет, такого не у нас...
Нет, такого у нас не возник, нет?
Но у нас точно появится правило s-стрелочка t.a.0.
То есть мы дополнительно здесь обработаем все эпсилон порождающие символы.
Как вот это правило появилось?
Вот это правило появилось как комплит вот этого с этим.
То есть мы закрываем вот это правило грамматики t.a.0. и t.a.0.
И двигаем точку над не терминалом, перед не терминалом t.a.0.
Вот.
Вот такая история у нас с вами возникает.
Да, вот все ситуации, в которые мы с вами можем попасть, разобрав ноль символов нашего слова.
То есть мы циклический повторяем эту вещь.
Дальше что у нас происходит?
Мы берем, сканируем следующий символ и начинаем опять тот же самый операцию.
Пытаемся спуститься, подняться, спуститься, подняться, спуститься, подняться.
И повторяем это до тех пор, пока у нас множество ситуаций не стабилизировалось.
И повторяем это до конца слова.
Собственно самое сложное здесь доказать, почему это все дело работает.
Индукция здесь будет.
Значит, смотрите, здесь есть тонкий подвох, который заключается в том, что такое changes.
Вот если changes неверно реализовать, то у вас симптотика будет четвертой степень.
Вот длинные слова.
Вот тут есть некоторая нотация о том, почему changes работает.
Тут нужно аккуратненько отслеживать, каким образом у нас возникают операции complete и predict.
То есть нужно отслеживать, какие множества у нас, какие ситуации у нас появляются.
И правильно их трекать.
То есть смотрите, вот в операции complete, когда у нас могут появиться новые множества,
это значит либо у нас вот это множество затриггировало новое.
И тогда нам нужно рассматривать вот эту множество ситуаций.
Либо другой момент, который мог возникнуть в этом правиле, это мы берем вот эту ситуацию.
И здесь, смотрите, у нас возникают пары множеств.
То есть у нас либо пара множеств, либо пара множеств.
То есть у нас либо мы относительно нового вот этого будем выполнять complete,
либо относительно нового вот этого будем выполнять complete.
Вот это новым тоже может быть, если k равняется j.
И нужно как раз аккуратно отслеживать пары возможных новых комбинаций,
и тогда эта точка будет эффективной.
Вот, это такой тонкий момент.
То есть changes, тут надо воспринимать именно в этом смысле этого слова.
Давайте еще раз, чтобы было понятно.
То есть храним обновленные пары ситуаций.
И значит мы говорим, что либо у нас DGT обновилось,
и тогда мы смотрим здесь множество DGT и Dcate.
Либо мы говорим следующее, что у нас с вами вот здесь вот множество DGT,
точнее вот здесь множество DGT, а здесь Ddcate.
То есть посмотрим по новым множествам, которые у нас здесь нашлись.
И тогда алгоритм будет эффективный.
Так, значит что мы будем с вами наверное делать уже в следующий раз,
потому что это будет сложно.
Значит мы будем доказывать вот эту лему,
что в процессе нашего алгоритма появляется вот эта ситуация,
тогда и только тогда, когда вот существует реально по факту такой вывод.
Где?
Мы в одну сторону по факту ее доказали сейчас.
То есть мы доказали ее слева направо.
Да, поэтому мы доказали слева направо.
Но в плане, у нас любой вывод кассы работает.
У нас если кто вывел, надо вывести на кассу, когда мы его нашли.
А если мы с кассой его вывели, то мы его и пошли.
Нет, вот это пока тонкий момент, что если мы наше слово, так сказать,
довели до какой-то вот такой ситуации, то оно выводится.
А почему? Потому что вот каждые правила, они написаны ровно вот таким образом,
как они требуются.
А другое значит, что если у нас выявилось какое-то слово по нашему алгоритму,
то у нас есть какая-то вот ситуация.
Но то...
Ну да, и нужно аккуратненько рассмотреть, как это слово было выведено,
и провернуть это индукцией.
Потому что без индукции тут, к сожалению, никак.
То есть смотрите...
Доказательство выводимости.
То есть смотрите, что первое делается.
Я сейчас вкратце идею расскажу.
В общем, индукция делается по количеству эффективных шагов в алгоритме.
То есть получается, у нас будет два перехода.
Первый, это индукция по длине алгоритма,
по количеству шагов в алгоритме, которое мы делаем.
А второе, это индукция по выводам правила грамматики.
Что если у нас оказался какой-то вывод,
и что у нас вот по факту до позиции точки мы разобрали и символов,
и до вот этой позиции мы разобрали g символ,
то вот такая ситуация появится.
Рано или поздно.
То есть поэтому доказательство выводимости парсеров очень сложное.
Потому что нужно трекать выводы в обе стороны.
Давайте посмотрим.
Собственно, первая вещь,
которая у нас с вами есть,
это посмотреть, когда у нас появилась ситуация из-за штриха вывода точка s.
0 принадлежит до 0.
Да, вот у нас появилась ситуация,
и тогда смотрите, что у нас происходит.
Вот кто может видеть?
Нет.
Нет.
Нет, мы же смотрим это.
Нет, у нас мало количества правил в правой части.
Или вы про количество ситуации говорите?
Ну, у нас там вышла ситуация,
но я имею в виду вот эти две количества прав,
по которым мы будем вот в последней части оборить мотофиз.
Ну, оно квадратичное.
Вот.
Оно будет...
Оно кубическое по индексам, очевидно.
Ну да.
Но оно будет квадратичным.
Квадратичным по левой части.
Ну да, в итоге там получится симпточка на один шаг,
и сумма на квадрата длины слова.
То есть он получается, как бы в случае с куби,
на суммарную длину перехода в квадрат.
Да, да.
Но это все равно работает быстрее, чем с куби, а отлично.
Да, да.
Средним.
Значит, смотрите, давайте первую идею расскажу,
а дальше, собственно, уже будем смотреть.
Значит, смотрите, у нас появляется вот такое правило,
стрелочка штрих, s, 0, 0.
Давайте попробуем его замачить на вот эту вот вещь.
Ну, смотрите, s штрих, тогда у нас получается следующее,
что мы выводим здесь слово s эпсилон.
То есть phi в данном случае это эпсилон.
Вот, а дальше мы за один...
А, нет, смотрите, вот так делаем.
Мы выводим с вами s штрих.
Значит, здесь у нас получается phi, равное эпсилон.
И дальше за один шаг мы раскрываем s.
Так, альфа, бета у нас...
Вот здесь альфа в нашем случае эпсилон, а бета это s.
Вот.
Получаем вывод, что вот такой вывод совпадает при, значит,
i равном 0, альфа равном эпсилон,
b равняется s, phi равняется эпсилон.
То есть это старт алгоритма.
Ну и дальше нам нужно будет рассматривать,
что у нас происходит при операции scan, predict и complete.
То есть рассмотреть, какое последнее правило грамматики у нас появилось.
То есть получается у нас с вами следующее,
что если какое-то правило у нас появляется,
то появляется вывод в грамматике.
Но при этом, смотрите, в обратную сторону это может быть неверно,
может быть мы какие-то правила опустили.
Пропустили, и в итоге у нас вывод не является корректным.
В общем, давайте я буду доказывать это уже чуть-чуть в следующий раз.
Вот эту вот лему.
Я надеюсь, что мы не выдохнем.
Поэтому я попрошу, пожалуйста, хоть чуть-чуть проглядеть содержимое этой лекции
перед тем, как приходить на следующее.
Чтобы вы хотя бы были в сути того, что мы будем делать.
Ну вообще всем тем, кто собирается прийти на следующую лекцию.
Нет, нет, там дальше доказательства просто будут.
Да, да, да.
Не, мы просто будем проходить по нему, и там какие-то моменты возникают, если это.
Да, я понимаю, начались сложные лекции.
Но я надеюсь, что мы с ним в итоге разберемся.

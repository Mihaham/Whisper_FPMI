Сегодня, соответственно, продолжаем решать задачу безусловной оптимизации в последний раз.
И разбираемся с такими методами, как метод Ньютона, квазиньютоновские методы и все,
что с ними связано. Так, хорошо, хорошо. И начинаем мы чуть-чуть необычно. И рассматриваем следующую
задачу. Задача у нас вот такая. То есть мы ищем не какой-то минимум функции и так далее, мы ищем точку,
в которой значение, причем скалярной функции, равняется нулю. Равняется нулю. Вот такая вот задачка.
Связана это с тем, что именно такую задачу в свое время рассматривал Исаак Ньютон, когда придумывал
свой метод Ньютона. Соответственно, с нее мы и стартуем. Окей, смотрите в чем идея. Давайте
посмотрим на какую-то точку t0. Ну, мы взяли какую-то точку t0. И как-то двигаясь итеративно от этой
точки t0, мы хотим прийти к точке t со звездой. В самом лучшем случае, вообще говоря, мы хотим прийти
за одну итерацию. Давайте попробуем найти такое дельта t, чтобы у нас вот t0 плюс дельта t в итоге
равнялась t со звездой. В худшем случае примерно, а в лучшем случае прям точно. Вот, окей. Давайте
тогда разложим нашу функцию в точке t0 плюс дельта t по тейлору. Что у нас тогда получается?
Fiat t0 плюс производная точка t0 умножить на дельта t плюс умалая от дельта t. Ну, а дальше что скажем?
Скажем следующее. Пусть это будет примерно равно дельта t0 плюс производная t0 на дельта t. Выкинем
просто умало. В то же время мы предположили, что вот это у нас примерно t со звездой значение
под функцией. А здесь соответственно у нас что будет? Тогда если это значение в точке t со звездой,
то здесь у нас можно сказать, что это значение будет равняться примерно нулю. Тогда получается,
что у нас, если мы соединим эти два выражения, вот это и вот это, получится, что fiat t0 плюс
производная t0 дельта t равняется нулю. Откуда можно выразить значение для дельта t? И оно
соответственно будет равно минус значению функции в точке t0 делить на производную точке t0. Вот,
и таким вот образом можно получить итеративную схему метода Ньютона, именно в классическом
варианте tkt, это tkt, плюс дельта t, а дельта t соответственно вычисляется так, как мы
только что обсудили. И от tktova делить на phi штрих от tktova. Вот такой вот метод придуман,
соответственно, был в середине 17 века Исаком Ньютоном для решения вот такой вот задачки,
или поиска корня функ. Хорошо, на слайдах тоже самое у вас, дельта t и вот метод.
Вот, давайте я вас спрошу, я рассказал какую-то интуицию, как до этого метода можно дойти. А насколько
интуиция вообще рабочая? Вот, которую я изложил. Что там, какие были в некотором смысле переходы,
которые могли быть некорректны? Раз. Два. Ну то есть вообще существование производной,
это вещь не очевидна. Ну если производная существует, пусть будет производная существует.
Вообще да, ключевой вопрос то, что я очень успешно так сказал, что у мало дельта t это что-то
очень маленькое, поэтому там что-то примерно равно и поэтому все хорошо. На самом деле у метода Ньютона
в связи с этим есть одна довольно большая проблема. Вот, то что по факту нужно рассматривать точки,
которые, ну даже стартовую точку, довольно хорошей окрестности, точки теста звездой,
чтобы действительно там умало было реально умалым. Вот, давайте посмотрим на такую функцию. Вот,
на такую функцию. Одномерная функция. Какое у нее решение? Быстренько. Ноль. Единственное решение
phi от нуля равно нулю только в точке ноль. Понятно, знаменатель всегда положительный,
а числитель только ноль в одной точке. Хорошо. Давайте, я уж не буду вас мучать подсчетом
производной. Я ее за вас посчитал. Давайте запишем итерацию метода Ньютона для данной задачки.
tk t plus 1 равно tk t минус значение функции в точке tk делить на производную функцию в точке tk.
Так, tk. Дальше что у нас значение функции в точке tk? Это tk делить на корень из 1 плюс tk в квадрате.
Вот. И делим это еще на 1 плюс, ну, 1 делить на 1 плюс t в квадрате, tk t в квадрате в степени
3 вторых. Вот. Тут сокращается 1 корешок, как вы видите, да? Вот. И остается соответственно,
что tk t минус tk t. Я звук не замутил. Слышу себя. Вот. tk t, а здесь соответственно получается,
что tk t 1 плюс tk t в квадрате. Так? Ну и все это раскрыв скобочки, получаем, что у нас
на каждой итерации метода Ньютона мы будем приходить вот в точку tk t плюс 1, вот такого вида.
Окей. К вам теперь вопрос. Насколько это вообще точка адекватная?
То есть, да, смотрите, если у нас все, если tk t нулевое у нас, например, меньше единицы по модулю,
так тогда мы соответственно tk t плюс 1 будет тоже по модулю меньше единицы, причем значительно
меньше, потому что мы в куб возводим, да, знак поменяется, но вот такой атеративной схемы,
мы все ближе и ближе будем подходить к решению. Да? Если, соответственно, у нас t0 по модулю
равно единицы, мы просто будем астеллировать плюс-минус единицы. Ну и, соответственно,
если t0 у нас больше по модулю единицы, понятно, все это будет успешно, далеко и расходится. Вот.
Это вот тот пример того ключевого особенности метода Ньютона, даже того классического,
который придумал в свое время Исаак Ньютон, то, что этот метод является локальным. Локальным. Что
это значит? Вот до этого мы с вами рассматривали, например, градиентный спуск, который находил нам
решение. Да, там, другое задача не такой, там, не ноль мы искали функций, а минимум. Вот. Да,
он, мы из какой бы с точки ни стартовали, мы все равно гарантированно доползем до решения. Вот. Здесь
сходимость локальна, потому что нужно стартовать из нужной окрестности решение. Стартуете далеко
от решения, никуда не сойдетесь. Проблема в некотором смысле да. В некотором смысле да, ну и,
соответственно, да, здесь мы отмечаем то, что у нас метод является локальным. Вот. Вот. Окей,
теперь давайте подберемся от метода Ньютона, вообще, от классического того, что придумал Исаак Ньютон,
к нашей задаче-то минимизации. Мы минимизируем нашу функцию, безусловную, выпуклую, дважды
непрерывно дифференцируемую. Вот. На РД, на всем пространстве, нет ограничений никаких. Что,
какой ноль мы там у нее искали в свое время? А? Минимум. Мы у нее искали минимум, а ноль чего
получается? Ноль градиента, да, то есть мы знаем, что для выпуклых задач необходимым и достаточным
условиям минимума является ноль градиента. Вот. И, соответственно, по факту, что мы можем сделать?
От задачи, которую рассматривал Ньютон, когда он искал ноль функций, мы можем перейти к задаче уже
нашей, когда мы искали, ищем ноль градиента. Ноль градиента. Вот. И тогда метод Ньютона переписывается
следующим образом. Ну, в силу того, что у нас теперь уже этот градиент, то надо брать еще
производную, это уже гессиан. Вот. Ну и понятно, что там операция 1 делить на производную уже не
особо валидна. Нам нужно в некотором смысле эту операцию поменять. Ну вот здесь, соответственно,
она меняется на обратную матрицу гессиана. Вот. Вот так вот этот метод выглядит. Сейчас более строго
поймем, откуда это тоже берется, но пока это в некотором смысле просто исторический референс,
как мы пришли от классического метода Ньютона, который был придуман в 17 веке, к тому методу Ньютона,
который нужен нам для задач безусловной оптимизации. Вот. Хорошо. Здесь мы, соответственно, его пишем.
Ну вот здесь прямо листинг алгоритма. Метода Ньютона. Вот. Хорошо. Как мы к нему можно еще прийти?
Можно вспомнить, что у нас есть опроксимации по Тейлору. Вот. Опять же все можно разложить в ряд.
Тут мало как раз не написано, потому что знак уже стоит приближенный. Вот. И в случае градиентного
спуска мы с вами работали с опроксимацией первого порядка. Здесь, соответственно, давайте рассмотрим
опроксимацию чуть более высокого порядка второго. Вот. И скажем, давайте-ка я минимизирую эту опроксимацию
второго порядка. Минимизирую по x. Минимум по x. Минимум по x. Ну, соответственно, что? Когда я беру минимум по
x от этой опроксимации, что у меня будет получаться? Я, опять же, воспользуюсь необходимым условием и просто
возьму градиент по x. От этого выражения будет получаться градиент от x-катова. Это вылезает
из скалярного произведения. Плюс, соответственно, из квадратичного кусочка у меня вылезет f от x-катова
гисян на x на минус x-каты. Вот. Здесь я сразу воспользовался тем, что у меня гисян симметричен.
Вот. Поэтому там по факту должно было вылезти a транспонированная. Ну, там сумма a и a транспонирована
в матрице. На одну вторую на гисян симметричен, поэтому все. Сразу написал так. Соответственно,
я ищу 0 и x звездой. Ну и, соответственно, как? В градиентном спуске мы проставляли просто другую
точку, а здесь будет x звездой. Это как раз следующая точка. Вот. Ну и отсюда можно выразить как раз
x-каты плюс 1, и получится ровно то, что мы хотели, метод Ньютона. Вот. Это более, так скажем,
понятное объяснение, откуда берется он. Просто там непонятно, откуда взялся гисян,
почему мы его обратили на течение. Понятно, почему именно вот он так возник. Ну вот здесь
это становится более понятно. Вот. Получили метод Ньютона. К сожалению, он от своего, так скажем,
классического предшественника берет ту же самую проблему локальной сходимости, плюс вбирает
себя дополнительно в некотором смысле проблемы, с которыми мы с вами не сталкивались в градиентном
спуске. Например, подсчет гисяна. Операция довольно дорогая. Вот. Ну, то есть, иногда и градиент взять
сложно, а тут гисян еще нужно считать. Плюс, соответственно, именно с точки зрения арифметических
операций вы его посчитали, вы еще нужно его обратить. Вот. Обращение матрицы – вещь довольно дорогая,
как мы знаем. То есть, мы до этого как бы избегали этого всего. Вот. Как раз, вот, условно, в методе,
когда мы говорили про сопряженный градиент и решали систему, мы как раз пытались отойти от того,
чтобы решать эту систему в явном виде, то есть, обращать матрицу. Хорошо. Хорошо. Здесь все
записано. То есть, стоимость итерации значительно возрастает. Вот. Обращение гисяна. Такой вопрос. А
за сколько итераций, вообще, метод Ньютона сойдется для квадратичной задачи с положительно
определенной матрицей? Квадратной положительно определенной матрицы. Вот у нас есть задачка,
например, 1 вторая, x транспонированная, ax. Она у нас симметричная, положительно определенная. Вот.
Минус bx. Вот. За сколько итераций мы найдем методом Ньютона оптимум вот этой задачки?
Во. За одну. Хорошая ответ. Давайте просто посмотрим, что получится. Запишем итерацию. Так.
А у нас здесь получится гисян. А что у нас обратный гисян для квадратичной задачи? Симметричной
матрицы. Ростоматрица А это гисян. Соответственно, обратно гисян это минус. А в минус 1. Вот. Окей.
А градиент? Градиент сколько будет здесь? ax-b. Все правильно. Опять же, тут пользуемся сразу же
симметричностью. Ну и тогда чему равно xкат? Быстренько это заметит. А-1, минус 1, b. Просто решение
системы. То есть, видите, квадратичная задача начинает сразу подсказывать, где мы вообще
находимся. То есть, кажется, что действительно итерация подорожала. Посчитали гисян. Вот. Ну,
может быть, это даже, кстати, в случае квадратичной задачи и недорого, потому что матрицу-то мы знаем.
Вот. Вот обращение гисяна в данном случае ключевая операция. Но метод при этом сошелся за одну
итерацию. Вот. И это подталкивает нас к мысли, что метод Ньютона, который по факту выбирает
себе довольно много недостатков, как вычислительных, так и, например, относящихся к сходимости. Вот. У него
есть одно замечательное свойство, что он очень быстрый. Очень быстрый при своей стоимости итерации.
Вот. Ну, давайте тогда, что? Попробуем показать, насколько он быстрый. Вот. Для этого вводится
следующее предположение. Пусть у нас, опять же, задача безусловной оптимизации. Стелевая функция у
нас мюсильно выпуклая. Вот. Здесь я, соответственно, записываю критерии второго порядка, с которым вы уже
взаимодействовали. Вот. В том числе в домашнем задании. И, соответственно, также я предполагаю,
что у нас гисиан является липшицевым. Липшицевым. Вот. Здесь сразу нужно отметить, что в силу того,
что теперь мы записываем эту разность, как у нас была до этого градиентов. Была также функция,
когда мы говорили про липшицы, то есть функция, там был просто модуль. Вот. А здесь у нас получается
уже разность матриц. Вот. И здесь, соответственно, вот эта вторая норма, это, как вы думаете, какая норма?
Там не так много вариантов для матриц норм, которые обозначаются второй. Неправильная,
нефробениуса. Вот. Евклидова норма. Какую норму индуцируют для матрицы? Какую норму? Какую норму индуцируют?
Кто помнит? Нефробениусов, там ты делаешь что? Как бы каль... А! Всё, отлично. Вот. Просто мне
интересно было, у вас вообще есть курсы вот такие, которые про нормы матрицы, или только это
в функоне у вас есть? Нету, да? Ну вот на функоне только нормы оператора. Да, про матрицы вы не говорите,
как сопрягаются. Вот. Ну смотрите, как обычно определяется норма матрицы. То есть вы берёте
какую-нибудь векторную норму, берёте и говорите, что она у вас там на шарике. Вот. И берёте с
упремом вот такое выражение. На функоне, да? На мотоне. Вот. На мотоне? Нафига вам на мотоне
была нужна эта? Ну ладно. Вот. Так. Окей. Соответственно так вот определяется норма матрицы. И в случае,
когда вы сюда вбахиваете евклидово-векторную норму, вот, у вас она индуцирует спектральную норму. То
есть в принципе вы со спектральной нормой тоже в некотором смысле должны быть знакомы, когда вы
с Константу Липши Саэль оценивали в домашнем задании. Ну кто познакомился, кто не познакомился.
Но спектральная норма, она на то и спектральная. Вот. Кто знает, что там лежит в спектральной норме,
на что она опирается? Может быть, кто знает? На сингулярные числа. Она опирается на сингулярные
числа, на собственные значения. Соответственно, если вы берёте спектральную нормницу матрицы А,
то она опирается на собственные значения матрицы А транспонированных корешок. Это сингулярные
числа для вещественной матрицы. Вот. Ну и в нашем случае, когда у нас, например, гисианы транспонируемые,
то есть симметричные, то есть сингулярные значения будут просто вылезать в собственные значения.
Вот. Получается, что так. Ну давайте тогда попробуем. Давайте я сразу же спрошу вопрос. Если у меня
гисиан ограничен снизу, то есть является положительно определённым, то есть ещё просто ограничен
с константа mu на i. Вот. Что можно сказать про его спектральную норму? Ну смотрите,
спектральная норма, она опирается на собственные значения. Ну вот, то есть максимум здесь просто
спектральной нормы, здесь будет достигаться, будет на максимальном собственном значении? Ага. То есть
максимум будет хотя бы mu. Но с другой стороны, он и снизу будет подпираться mu, так? Потому что
минимальное собственное значение тоже mu. Поэтому вот здесь вот вы можете сказать, что у вас гисиан,
спектральная его норма, она подпирается, ой, просто-просто больше либо равна, чем mu. Так, я понял,
то есть это, может быть, по особе нужно будет добавить про нормы матриц, хотя там вроде есть
про нормы матриц. Вот. То есть, чтобы это было понятно. Вот. Ну то есть, главное, что тут понять суть,
то есть спектральная норма, которая вот здесь вот индуцируется, ифклидовая норма у вас, она
зависит от собственных значений, от сингулярных чисел в данном случае, просто собственных значений,
поэтому вот у вас спектральная норма гисианы снизу подпирается mu, потому что он положительно
полуопределён, причём так, что ещё и с константа mu подпирается. Вот. Окей. Давайте пойдём по
сходимости. Всё тут довольно в некотором смысле интуитивно. Как мы с вами доказывали
градиентный спуск, просто смотрели, как меняется расстояние до решения в зависимости от номера
итерации, да. Поэтому берём cat k плюс первую итерацию. Вот. И подставляем то, что мы как раз в методе
Ньютона используем. Вот. А дальше, соответственно, будет не совсем, что ли, как привычно, но мы в
некотором смысле такие трюки с вами уже проделывали. Проделывали. Скажите мне, а что вы можете
сказать, например, про вот такое вот выражение, если мы вспомним формулу Ньютона-Лебница. Формулу
Ньютона-Лебница. Помните, для функций мы такое же проделывали, когда доказывали липчество. Что мы
можем сказать? Что это интеграл от нуля до одного, здесь стоит гессиан, tau, x ката минус x
со звездой. Вот. Здесь ещё x ката минус x со звездой до tau. Помните точно такой же трюк мы проделывали
с значениями функции и градиентами, да? Просто та же формула Ньютона-Лебница, когда вы интегрируете
вдоль, когда у вас кривая интегрирование, она запираметризована каким-то. Мы с вами тогда
брали отрезочек, здесь тоже мы берем отрезочек от x со звездой до xk. Ровно то же самое, тут ничего
такого сверхъестественного нету. Вот. Только единственное, что теперь мы это записали для векторов.
Для векторов и, соответственно, для градиентов, и там возник гессиан. Вот. Что мы знаем про x со звездой,
про градиент в точке x со звездой. Он равен нулю, так? Тогда вот то, что записано вот здесь вот,
это просто выражение для градиента, да? Ну, я, соответственно, вот это подставлю вместо градиента.
Давайте я вот так сделаю. Здесь будет, соответственно, минус гессиан в точке xk минус первое, а здесь будет,
соответственно, интегральщик наш.
Вот. Хорошо. Хорошо. Смотрите, что я еще замечу. Я замечу, что у меня подинтегральное выражение
вообще не зависит от x икса звездой. Вот от этого кусочка. От вот этого кусочка. Вот.
Поэтому я его могу вынести за пределы интеграла. Так. Сделаю вот так. Скобочка, скобочка, xk минус x
со звездой. Вот. Смотрите теперь, что замечу. Замечу, что у меня вот здесь вот болтается как бы одинаковый
вектор. Одинаковый вектор. Что, соответственно, можно сделать? Вот. Что можно, соответственно,
сделать? Давайте воспользуемся умной единичкой вот тут вот. Умную единичку добавлю. Вот. Чтобы
воспроизвести здесь еще как бы минус гисян. Гисян минус первый. Вот. Ну умножить на гисян.
Вот. Я тут перед вектором xкт воспроизвожу вот это. Гисян минус первый и гисян в первый. Просто умная
единичка. Вот. А дальше у меня все остается. А теперь что я могу сделать, значит? Теперь я
могу получается в некотором смысле вот, вот это и вот это вынести за скобки. То есть слева и
справа. Вот. И получается, что у меня в скобках будет разница между гисяном в точке xкт минус вот
этот интегральчик. Давайте это сделаем. Так. Так. Так. Здесь это сделано. Здесь уже домножено.
Сейчас. Где тут у меня? Так. Так. Во. Здесь это сделано. Вот уже здесь вот на этом моменте сделано.
Просто вот обозначено за xкт нашу разность гисяна в точке xкт и интеграла. Хорошо. Теперь давайте
оценивать разность в евклидовой норме. В евклидовой норме как у нас меняется расстояние. Так. Соответственно,
что здесь получается? Будет f xkt минус первая норма gkt. Так. Теперь смотрите. У меня евклидовая
норма. Опять же, свойств получается вы не знаете, что можно сделать. Ну, смотрите. Давайте покажем
просто довольно такой простой факт. У меня есть матрица что-то типа вот такой евклидовой нормы.
Понятно ли то, что это будет вот так вот? Где, соответственно, вот это уже спектральная норма. Вот.
Индуцированная норма и, соответственно, евклидовая. То есть спектральная норма, она индуцируется
евклидовой нормой. Ну, это понятно из чего? Из определения. Потому что вот это по факту,
это же supremum по всем x, которые равны единичке. Давайте я вот здесь x0 поставлю. Вот. А x. Вот. А вот
то, что я вынес, это же по факту то, что это значит. Я просто мог бы как вот это записать. Если бы
я хотел, например, записать норму по-другому же, определение нормы матрицы, можно вот так вот
записать. Можно брать supremum по x, которые по длине, например, как x0. Как норма x0 в квадрате. Вот.
Ну и, соответственно, здесь у меня будет ax. Вот. Ну тогда еще нужно будет отнормировать на x0. Вот.
Ну и по факту здесь я вот его просто вынес. Вот. Спектральную норму. Сказал, что у меня есть единичка.
Отнормировал как бы вектор x. Вектор x его норму вынес. Я оставил под нормой вот здесь
единичный вектор. Ну а там по определению спектральной нормы просто получил норму a.
Здесь понятно? Я надеюсь, что да. Вот. Ну если еще раз, можете быстренько глянуть потом по особи,
там про нормы матриц есть, в том числе их свойства. Вот. Ну вот здесь я еще раз повторю, довольно
просто. Здесь что я сделал? Я взял вектор x, разделил его там условно на норму x0 вторую. Вот.
Вынес эту норму x0 искусственно. Вот. И здесь у меня получился вектор длины 1 в евклидовой норме. Да.
И поэтому здесь я могу просто сказать, что если я возьму suprem по всем этим x, то у меня будет
просто норма матрица спектральная по определению. Вот. Ну это хорошее свойство, оно это работает
только с евклидовыми нормами. Это работает со всеми нормами, соответственно, когда у вас норма
матрицы индуцируется или как вот создается, задается норма вектора. Вот. Соответственно,
здесь я делаю то же самое и выношу вот так вот. Вот. Дальше можно с матрицами в данном случае
тоже как с числами, когда у нас там произведение двух матриц по норме, это просто меньше либо
равно, чем норма произведения. Вот. Смотрите. Теперь что у меня осталось? По факту вот здесь уже
все хорошо. Расстояние до решения, расстояние до решения. Мне нужно оценить вот эти два кусочка.
Вот. Ну мы что-то уже знаем про матрицу Гесиана. Мы же знаем, что Гесиан у нас как бы положительно
определен, более того, ограничен еще снизу константа μ. А что мы тогда можем сказать про
обратную матрицу? Чем она ограничена? И как? Один μ сверху, снизу. Ну вот тут она снизу как бы
ограничена. А тут сверху будет все правильно. Ну смотрите, вы обращаете, то есть условно.
Смотрите, на примере диагональной матрицы. Вот. Или здесь, о, здесь даже видно, вы домножите
сейчас на матрицу Гесиана. Нет, смотрите, вот здесь просто домножите это на матрицу Гесиана.
Вот. Правую левую часть вот этого выражения. Правую левую часть этого выражения домножите на
матрицу 10. А что получится тогда? У вас здесь получится единичная, меньше
либо равна, чем 1 на µ, 10. µ домножаете, нам домножаете на µ, получаете то, что было
до этого. Согласны?
Вот, тогда смотрите, в силу того, что у вас вот это нам уже не нужно, нам главное,
что мы знаем вот это. Вот, тогда смотрите, вы знаете, что у вас матрица сверху
ограничена, собственные значения у нее ограничены, 1 делить на µ, поэтому и
спектральная норма тоже ограничена, 1 делить на µ.
Вот, ну и все, тогда получается вот следующая оценка. Осталось только по факту
здесь разобраться со спектральной нормой вот этого безобразия.
Жкт, то есть то, что я ввел, это как разность вот этого десианов точки к и интеграла.
Вот, ну давайте сейчас с ними будем разбираться, здесь я в принципе просто с µ поигрался,
то же самое сделал. Бум-бум-бум-бум. Все, теперь с жк разбираемся. Там странички нету,
свободные давайте я создам ее, чтобы расписать это. Так, добавлю страничку. Так,
жкт, норма спектральная, это что у нас есть? По факту гисиан в xk там, просто
потому как я определил. Вот, на гисиан в точке x звездой плюс tau, xk минус x звездой и здесь
d tau. Вот, дальше смотрите, я внесу вот этот гисиан под интеграл, потому что гисиан
вообще не зависит от tau. Вот, и проинтегрировав от 0 до 1, я просто получу его же.
Так, хорошо. Дальше опять же стандартный трюк, который мы уже с вами применяли, когда опять же
говорили про липшицевость градиентов. Вот, норма интеграла, то есть, ну, модуль интеграла меньше
чем, получается, модуль от суммы, вот, меньше, чем сумма модулей. Вот, поэтому я норму просто
занесу под интеграл и оценю сверху. Ну, а теперь что? Что можно применить?
Нет, зачем треугольник? Липшицевость гисиана. Липшицевость гисиана. Здесь вылезет что? m к
вылезет и вылезут соответственно, сейчас скажу, что там вылезет. 1 минус tau, да, 1 минус tau вроде вылезет.
x kt по норме минус x звездой по норме d tau. Вроде так. Ну, вроде да. Вот, так, тогда можно за пределы интеграла
вынести m xk минус x звездой в квадрате. Интеграл останется от нуля до единицы, тот, который, в принципе,
уже берущийся. Так, ну что, сколько там получается у этого интеграла? Одна, вторая, все правильно.
Все, оценили. jkt оценено. Вот, хорошо. Закроем. И переходим на следующую страничку. Здесь, соответственно,
у нас как раз оценил jkt. Вот оно. Ну и подставил. Получилось вот такое вот выражение. Вот. И, на самом
деле, это все. Это уже теорема сходимости метода Ньютона за одну итерацию. Вот. Особенности. Смотрите.
Степень разная. Степень разная. Вот. Ключево. Плюс коэффициент еще здесь. Давайте рассуждать,
что мы в итоге получили. Вот. Сходится, вот судя по этой оценке, сходится ли этот метод всегда?
Если за одну итерацию, мы можем гарантировать вот такое.
Хотим, чтобы, например, было что-то вот такое, да? Вот. Тогда мы что можем гарантировать? Тогда
мы вот в этой оценке можем затереть вот это, затереть одну из степеней и здесь затереть вот
нестрогий знак, да? Сделать его, ой, нестрогий знак, делать его строгим. Вот. Все. Соответственно,
вот так вот. Когда мы вот попали в такую окрестность, тогда мы действительно можем
гарантировать. Вот. И каждую итерацию мы будем приближаться, приближаться, приближаться. Иначе,
понятно, никаких гарантий нет. Это вот ровно то, что, в принципе, есть и в классическом методе Ньютона,
там, который сам Ньютон придумал. Локальная сходимость. Вот. Но при этом, что можно сказать про именно
скорость сходимости, она прям бешена. Если мы уже залетели в решение, то там уже все хорошо. Ну,
давайте подставим, например, условно m равно 2, mu равно единички. Ну и вот стартовая точка пусть у меня
на расстоянии 1 и 2. Тогда что я могу гарантировать? Что следующая точка у меня на расстоянии 1 и 4.
А следующая, лавинообразная, уже на расстоянии 1 и 4 в квадрате, да? И так далее, и так далее,
и так далее. Это квадратичная скорость сходимости. Вот эквивалент ее то есть там рассматривали
с вами другое определение. Но вот это очень быстро, это квадратичная скорость сходимости
по факту у нас курсен методов быстрее не будет. Это квадратичная скорость сходимости. То есть,
когда вы попали в решение, когда вы попали в окрестность решения. Сваливаетесь, вы в него
начинаете очень-очень быстро. Там, за несколько итераций, вы там прям уже найдете точность там,
чуть ли не на 10, минус 10 и так далее. Вот. Это быстрый метод, но опять же проблема
есть в локальной сходимости. Ну и в дороге возне итерации.
Окей, давайте быстренько порассуждаем, как можно порешать вообще проблему с
локальной сходимостью. Вот. Я на самом деле немного удивлен, что когда я вообще
расписывал интуицию метода Ньютона, когда я вот оставлял это квадратичную
аппроксимацию, это же не совсем то, что мы всегда рассматривали в градиентном
спуске. Когда мы смотрим на градиентный спуск, что вы там помните, когда мы
рассматривали его итерацию, например, когда мы рассматривали, как он доказывается,
какие там были такие, что ли, ключевые идеи.
Что есть, например, в градиентном спуске, чего нет в методе Ньютона, который мы
рассмотрели? Ну или гладкость тут можно добавить, а скорее вот, ну вот на сам
метод посмотрите, что есть тут, чего нет тут и есть там. Шаг, конечно.
Потому что мы же как раз, когда говорили про линейную аппроксимацию, когда мы
говорили про градиентный спуск, мы говорим, линейная аппроксимация, выходить далеко
нельзя, улетим. А здесь, соответственно, да, что-то мы про шаг забыли, то есть,
ну вот есть идея, что просто можно добавлять шаг. На самом деле, это не самая крутая
идея, вот. Сильно она не фиксит метод Ньютона, но может в некотором смысле
точно увеличить радиус сходимости, это раз. Плюс, если вы будете очень умно
выбирать шаг, вот, действительно, как-то можно метод сделать более рабастым, с точки
зрениями, на окрестности сходимости. В прошлый раз, вы помните, какую-то идею
обсуждали выборы шага? Не просто какую-то там константину, как можно было подбирать,
что у нас было в методе сопряженных градиентов. Вот что-то вот такое, помните?
Гамма-к, давайте тут альфа обозначу. Нет, давайте гамма-к пусть все равно останется в предыдущем
складе гамма. Пкта, да, то есть у нас было направление, ну так вот оно направление-то
с минусом. Пкта, вот, направление, которое нам дает метод Ньютона, подбирайте гамма,
ну и попытайтесь, соответственно, что-то найти. Вопрос, найдете ли вы это гамма, конечно, вот,
для некоторых задач. То есть, как бы там, конечно, полностью от локальной сходимости
таким способом не избавиться. Вот, тут не всегда эта гамма-к найдется, там нужно, или если линейным
поиском, непонятно, какой отрезок брать. Можно добавлять до каких-то условий, которые вы будете
разбирать на семинаре, но в общем все равно тут есть как бы в некотором смысле игра. Вот, еще вспоминайте,
что было, когда мы с вами доказывали сходимость градиентного спуска. Кому-то даже на семинаре
такую задачу давали про то, как подбирать шаг в градиентном спуске. Если помните, из каких
вообще соображений подбирается вот оптимальный шаг, например, в градиентном спуске?
Ну, помните, как выглядела картинка для линейной апроксимации? И вот,
типа того, да, у нас, помните, было как? У нас была как бы, вот сама функция как-то выглядит,
и была у нее верхняя апроксимация через l, да, вот, и была нижняя апроксимация через mu, ну, или там не было
ее вообще, была там линейная просто апроксимация, если mu нету. Вот, и шаг-то там подбирался как? Чтобы
минимизировать что? Я вам, помню, рассказывал, забыли, наверное, вот, минимизировать вот эту
параболу, потому что она как бы говорит, насколько функция вообще быстро может меняться. Вот это
константа l, видно, что она растет быстрее, чем сама функция, вот, и поэтому, как бы исходя из этой
параболы, вы можете делать какие-то выводы, насколько большим шаг вы можете брать, то есть в худшем случае
у вас функция реально может только сильно скакать, вот, сильно расти, вот, ну и, соответственно, да,
мы хотим, чтобы мы как бы в некотором смысле не перескакивали через ветви параболы, а наоборот,
как-то хорошо сходились. Ну и самый такой хороший вариант, это просто взять и упасть в минимум этой
параболы, да? Да, он не отражает до конца свойства функции, минимум может быть реально ниже, вот, но
делая так вот итеративно, мы приближаемся, мы приближаемся, то есть каждый раз восстанавливаю эту
параболу, вот, и поэтому шаг градиентного спуска, там вот, градиентный спуск с шагом 1 делить на l, это
более чем хороший выбор. Есть выбор и получше, вот, как показывает теория, но вот шаг 1 делить на l,
в выпуклом случае он как бы вообще не улучшаем, вот, соответственно, да, тут направление, соответственно,
мы с вами поговорили, вот, как раз минимизировали вот эту параболу по иксу, вот, и это было эквивалент
тому, что мы просто делаем шаг градиентного спуска с 1 делить на l, вот, а здесь, вы помните,
когда я же, когда вам метод Newton это показал, я сказал, что вот вам, как бы, место линейной
аппроксимации квадратичной, вот, только я все малое, а оно может быть и нифига не умалое,
забыл, вот, ну и получил какой-то метод, но и здесь такая же мысль, если вот это гарантированно работает,
если мы как бы ограничили функцию сверху, знаем, что она растет не быстрее, чем вот это, вот, и тогда
вот, подбирая такой шаг, будет все сходиться, вот, то здесь аналогичная идея, вот, причем вот, как вы
помните, вот, то, что записано под argmin в первом случае, оно ведь реально мажорирует функцию,
когда у вас l гладкий градиент, вот, здесь же это как, это по факту argmin по иксу, это же тоже
самое, что minimum как бы по иксу, то есть, вот так вот, вы можете записать вот такое свойство, это вот как
раз просто гладкость, липшица из градиента, вот, и оказывается, это тоже справедливо, вторая строчка
тоже справедлива, когда у вас липшица в гессиан, вот, и вот идея как раз в том, чтобы минимизировать,
делать шаг ньютона, но вот добавляя вот эту вот дополнительную, дополнительный член, который как
раз вылезает из-за того, что вы говорите, что у вас липшица в гессиан, поэтому сверху вы можете это
ограничить, вот, такой метод называется кубический метод ньютона за авторством Полика и Нестерова,
15 лет назад, вот, соответственно, в домашнем задании, в бонусной, ну, там, частью с треугольничком,
у вас он есть, в части с треугольничком у вас он есть, можно с ним поиграться, вот, как он работает,
соответственно, вот, вот такие способы, как можно фиксить локальную сходимость метода ньютона, но,
смотрите, вообще глобальная, какая ключевая проблема метода ньютона при этом остается, это подсчет
гессиана, ну и его обращение, возникает мысль, давайте-ка сделаем перерыв и потом перейдем к следующей,
как раз к теме, как мы будем избегать подсчет гессианов, все, давайте перерыв 5 минут, так, ну,
ладно, давайте продолжать, продолжать, вот, про что теперь пойдет речь, опять же, поняли, что вроде
как локальность, ну, она, конечно, остается, но частично решаема, вот, но подсчет гессиана и его
обращение, это не решаемая проблема для метода ньютона, в некотором смысле, вот, поэтому давайте
запишем, опять же, итерацию, вот, похожую на метод ньютона, где вот матрица есть, а некоторые
матрица ашкаты, то есть в случае метода ньютона здесь встает обратный гессиан, ну, а мы зададимся
целью, давайте-ка этот обратный гессиан, ну, как-то дешево, что ли, считать, вот, или как-то дешево
опроксимировать, вот, смотрите, в чем идея будет, идея будет следующим, вот, смотрите, давайте опять же
запишем нашу формулу Тейлора, которую мы уже с вами несколько раз сегодня писали, например, в таком
виде, для градиента, для градиента, вот так вот, 1, xk-xk+, 1, плюс-малая от xk-xk+, 1, вот, ну, и как уже привычно
нам делать, чтобы получить какую-то интуицию, давайте, опять же, я уберу малое, вот, и скажу, что это
примерно равно, вот, тогда, тогда, смотрите, какое свойство, в некотором смысле, гессиана, мы из этого
можем вытащить, f гессиан в точке k+, 1, в минус первой, это получается, что градиент f от xk+, 1,
минус градиент f от xk, равно xk+, 1, минус xk, вот, просто из строчки выше выразил, ну, то есть,
сделал так, чтобы у меня появилась гессиан в минус первой, вот, это в некотором смысле то
свойство гессиана, которым он обладает, если мы пишем какую-то связь этого гессиана и, например,
градиентов, вот, ну, и смотрите, соответственно, из этого уравнения я могу что сказать, ну, давайте,
пусть у нас будет, что вот эта матрица hк, которая у нас как бы будет заменять гессиан, и мы от нее
потребуем, чтобы было выполнено вот это свойство, потребуем это свойство, почему нет, потребовали,
вот, а какие еще свойства гессианы есть довольно простенькие, например, симметричность, тоже можно
потребовать симметричность, вот, ну, окей, какие-то есть два уравнения, соответственно, я их тут еще и
выписываю, сейчас выпишу их, вот, вот, оно, здесь вводится нотация sк, это разница х, вот, ук, это
разница градиентов, ну, это классическая нотация для этого класса методов, который мы сейчас будем
рассматривать, поэтому, ну, она для удобства рассматривается просто потому, что сейчас выражения будут
довольно длинные и громоздкие, поэтому, чуть-чуть их уменьшаем вот таким образом, за счет sк и yк,
у это разница градиентов, с разница х, записали уравнение, которое, как бы, вот, хотим, чтобы оно
для нашего hкт, которое мы будем считать, должно выполняться, вот, хорошо, ну, вот это вот это
уравнение называется квази-нютоновским уравнением, вот, плюс, соответственно, еще добавляем
симметричность, окей, все хорошо, вроде как, скт можем вычислить, yкт можем вычислить,
плюс у нас есть ограничение, что матрица симметрична, получилось вот такое вот уравнение
на матрицу hкт, плюс один, решается, нет, и сколько решений вообще имеет,
чтобы прям симметрично, можно даже бессимметрично пока сказать, про это забудем, сказать,
ну, что решается, вот это, или нет, ну, что там нажимаем, на yкт транспонирован,
наделим на что, на норму, что получится, что получится,
а что мне оттуда как найти, это будет h, ну, хорошо, вариант, а вообще, в принципе, это, я думаю,
рабочий вариант, так, смотрите, сколько уравнений в этой системе, ну, не k, размеры задачи сколько,
den, ну, пусть размеры задачи d, поэтому у нас в этой системе d уравнений, а неизвестных сколько,
ну, если там транспонированность не добавлять, то d в квадрате, вот, если транспонированность,
там нужно главную диагональ и одну оставить, верхнюю, например, часть матрицы, а вторая,
там она как бы достраивается, но в любом случае, у вас тут о, d в квадрате неизвестных, да,
получается, что это уравнение, в принципе, имеет бесконечное число решений, вот, и как, соответственно,
подбирать теперь что-то, непонятно, нужно что-то требовать еще, нужно что-то требовать еще,
кроме квазинютовского уравнения и симметричности, вот, давайте первая идея, потребуем, ну, вот,
как в прошлый раз, довольно дешевый апдейт, то есть, пусть у нас hкаты обновляется
атеративно, причем я прошу, чтобы она обновлялась довольно дешево, то есть, смотрите, вот,
чтобы посчитать вот это, где пока mu — это какой-то скаляр, q — это какой-то вектор,
пока неизвестный нам, вот, чтобы обновить h, нам сколько нужно, например, на рифматических
операций, чтобы два вектора перемножить, но внешним образом, то есть, получится матрица,
матрица получится, d квадрат, d квадрат, то есть, здесь получится o от d в квадрате, то есть,
каждое обновление матрицы h будет занимать o от d в квадрате, вот, теперь наша задача понять,
какими мы вообще можем взять вот эти значения qкаты и muкаты, ну, для этого, в этом нам поможет,
как ни странно, квазинюттонское уравнение, давайте домножим на y, вот, из квазинюттонского
уравнения мы знаем, что вот это, соответственно, у нас skt, вот, с другой стороны, мы можем подставить
выражение для hкт, hкт плюс 1 это hkt ykt плюс muqk qk транспонированное yk, вот, а заметим,
что вот это что, что с размерностью у этого безобразия, это скаляр, это просто число,
поэтому, смотрите, что сделаю, я вот запишу вот так вот, muк, этот скаляр сюда впишу, транспонировано
yк, так, и здесь останется qк, то есть, вот, вот это у меня скаляр, скаляр, и остался вектор, вот,
этот вектор в свою очередь равен skt минус hкт yкт, вот, вектор, вектор, и слева вектор, и справа вектор,
что можем сказать про них, если они отличаются по факту только на множители, они коллиниарны,
получается вектор q и вектор sk минус hk yk, они коллиниарны, вот, поэтому, в силу того,
что всё равно с помощью mu и всё равно дополнительной q, я могу в некотором смысле нормировать мой вектор q,
как бы тут в знаменателе просто вектора, в числителе он как бы вот у меня возникает,
поэтому давайте я просто положу qкт равна skt минус hкт yкт, вот, то есть, получается,
что вот этот коэффициент, который здесь, он равен 1, вот, отсюда сразу же выражается mu,
это qкт и транспонированная yкт, всё, мы нашли q, как обновлять q и, соответственно,
как считать mu, вот, получается метод, вот, квази-нютоновский метод, первый квази-нютоновский
метод, так называемый одноранговый, одноранговый update, просто потому, что вот мы добавляем кусочек,
у этой матрицы будет rank 1, потому что это просто комбинация из одного и того же вектора,
там на свои же коэффициенты, вот, так называемый одноранговый update, квази-нютоновский метод,
он же метод Бройдена, метод Бройдена, всё, здесь он записан, вот, всё записано,
меня уже в явном виде, как матрица h обновляется, вот, мы поняли, что довольно дешево это получается
такой одноранговый update стоимостью от d в квадрате, вот, но, опять же, как вы понимаете, в силу того,
что система, которая из квази-нютоновских уравнений, которые мы задали, даже с симметричной матрицей,
имеет множество решений, вот, и это в некотором смысле простор для творчества, в том числе для
исследователей сейчас, вот, но в некотором смысле есть такой прям железобетонный квази-нютоновский
метод, который пока работает лучше всего, вот, и не придумали, как его можно улучшить, в общем,
в случае в некотором смысле, прям хорош на практике, вот, и в теории тоже, вот, смотрите, та идея,
которая почему-то изложена в статье про него и в книгах, когда про него рассказывают, излагают вот
эту идею, что давайте-ка мы будем смотреть на задачу следующим образом, вот, поиска матрицы
h-катая плюс 1 в следующем вопрос, пусть это будет в некотором смысле задача минимизации с
ограничениями, ограничение это квази-нютоновского уравнения и симметричность, плюс я потребую,
чтобы у меня матрица h, которую я ищу, была близка к моей матрице h-катой в некоторой норме, причем
норма здесь может быть любая, то есть, как бы в зависимости того, какую вы норму поставите, у вас
будет новый квази-нютоновский метод и, соответственно, решение вот это r-g минимум будет меняться. Для каких-то,
понятно, норм решения будет в явном виде, для каких-то будет не в явном виде, ну, вот, оказывается,
что если рассмотреть норму, вот, вот здесь норма, это норма пробеньюся, причем необычная, а взвешенная,
где веса должны удовлетворять вот такому вот уравнению, то вот можно получить, что решение вот
этой задачи минимизации, это вот это. Я, если честно, вот, когда на это смотрю, ну, я не понимаю,
как это берется, откуда это берется, вот. Сейчас посмотрим, вот, не знаю, вот, в книжках рассказывают,
что это вот так, вот такая интуиция, если люди реально дошли, то вот четыре автора как раз BFGS,
это четыре мужика, четыре фамилии, по фамилии метод называется BFGS, вот, как, если они вот реально
вот оттуда дошли вот до сюда реально через матрицу взвешенную, ну, респект, вот, потому что мне не
интуитивно, как вот это прямо придумать еще, чтобы взять матрицу такую хитрую, видимо, они искали
в явном виде решение, чтобы выписывалось, вот, и я оказался такой матрицей, но вообще, вообще,
я думаю, что можно объяснить и вот так, вот. Смотрите, на самом деле, квази-ньютоновское уравнение
можно записать не только для матрицы H, а для некоторого смысла для ее обратной, ну, это как
для гессиана и обратного гессиана, то есть, H это у нас как раз уже обратный гессиан, оно от матрицы B,
пусть будет что-то типа нашей аппроксимации через прямой гессиан, ну, честный гессиан без обратности,
ну, то есть, там нужно просто H у нас стояло здесь, поэтому, если B, то надо ее перенести как раз к S-комп,
ну, то есть, просто обратить, вот, как бы гессиан. Получается вот такое вот уравнение, и для него,
например, тоже можно записать одноранговый апдейт, ровно также искать в виде miukate, kukate,
на kukate транспонированные, вот, и получится вот такое вот уравнение. Выражение это одноранговый
апдейт для матрицы, ну, вот, для, соответственно, уже матрицы B. Вот, матрица B сама по себе не особо нам
интересна, потому что ее нужно обращать, вот. А другой вопрос, если вы это можете сделать в явном
виде, а не обращать численно, то это уже другое, более интересная тема. Вот, смотрите, что я вот хочу
про эту матрицу сказать. Видно, что в одноранговом апдейте, смотрите, какие комбинации встречаются,
ykate, ykate на ykate транспонированы, и bkate-skate на bkate-skate транспонированы. Ну, именно,
с точки зрения матрицы на векторном вычислении, то, что стоит в знаменателе, это просто число,
вот. А сверху вот такое вот получается. Я смотрю, что здесь вот возникает, перемножаю вот эти,
перемножаю вот эти, возникает вот такое. Там еще будут вот эти, как бы, скалярные произведения,
ну, удвоенные, вот. Но, в принципе, это, ну, как бы, кросс-произведения y на bkate-skate. Но, в принципе,
вот, как бы, кусочки-то вот такие. Ну, и возникает мысль, давайте я тогда буду в таком виде искать
update. Как бы, не одноранговый, а двухранговый. Раз вектор, два вектор. Прогласны? Вот. Ну, почему
нет? Давайте попробуем. Как бы, это же в некотором смысле творческий процесс. Вот. ykate-1 и ykate-2
подбираются. Опять же, вы просто пользуетесь квази-нютонским уравнением и домножаете на
ykate. Вот. Подставляете, там получается вот здесь вот у вас будет коэффициент один, здесь будет
другой, его можно спокойно найти. Опять же, ровно так же, как мы делали в sr1. И получится вот такой вот
update. Ну, как-то выглядит, опять же, я говорю, bkate, оно не особо нужно, если вы не умеете его обращать.
Но, оказывается, вот, bkate в таком виде можно задешево обратить. В явном виде это можно обратить.
И вот, вот такое bkate в обратном виде, ну, если его обратить, в явном виде, там есть специальная формула,
формула, формула Шермана Мориса Вудбери, вот, для обращения матрицы. Оно дает то, что как раз мы
выписывали для bfgs. Вот. Я не знаю, вот мне кажется, вот так bfgs получить легче. То есть, написать для
bkato, а потом, ну, вот, формула, конечно, не самая тривиальная. Вот. У меня есть доказательства, как это
все переписывается, bkato через hkate, hkate через bkate, вот, как это обращается, вот это выражение
получается нужное. Вот. Но мне кажется, вот так интуитивнее. Интуитивнее понять, почему bfgs
получается вот таким, чем рассматривать взвешенную матрицу. Возможно, через взвешенную матрицу что-то
похоже получается. Там нужно просто правильно порассуждать. Вот. Для меня вот это просто проще.
Хорошо. Смотрите, а такой вообще вопрос. Я вот выписал апдейд для bfgs здесь еще раз,
а он вообще дешевый или нет? Просто для sr1 мы сказали, что он стоит oad в квадрате. А здесь,
как я вижу, матрица, матрица, матрица. Матрица на матрицу на матрицу дорого. Вот. Это же
перемножение матрицы, это как раз что? oad в кубе. Что в итоге? Дорого это или дешево? Вот это ладно,
это понятно. Это как раз вектор на вектор oad в квадрате. А здесь что? Вот. Когда мы матрицы
перемножаем, там дорого или нет? Да в кубе уже плохо, потому что это обращение гисиана. Это
хочется избежать, потому что мы как раз шли за тем, чтобы в том числе решать проблемы обращения
гисиана. Смотрите на явный вид. Давайте посмотрим, если мы раскроем. Там вылезет ашката сначала и
вылезет также вот такие произведения. Например, вот такое. Ну так на него же можно чуть по-другому
посмотреть. Так. Вектор на матрицу. oad в квадрате. Дальше снова что получилось? Вектор на вектор.
Ну получается oad в квадрате. То есть, если правильно перемножать, не сначала создавать матрицу из вот
этого. И здесь получается у вас матрица d в квадрате на d в квадрате. Ее придется перемножать с матрицей
d в квадрате на d в квадрате. А сначала вы берете вектор на матрицу, делаете это z o d в квадрате,
получаете новый вектор. И два вектора нужно перемножить внешним образом, получаете o d в квадрате
снова. Все. То есть, вся идея тут такая. Тут потому что возникают ровно такие же произведения матрицы
на вот эту матрицу. Ну и возникает еще вот это. Ну вот это вообще одноранговый, это тоже o d в квадрате.
Вот. Вся суть. То есть, главное вот эту формулу чуть немного для себя переписать и сделать ее более
выгодной с точки зрения вычислений. Вот. Тогда окажется, что тут действительно o d в квадрате операции.
Вот. Когда мы вообще говорим про инициализацию матрицы h, то оказывается можно брать ее просто
единичной. Вот. Можно брать ее просто единичной. Работает и так. Вот. И работает хорошо. Есть более
какие-то евристики. Брать ее похитрее. Вот. Но это как бы в некотором смысле не дает особо
каких-то сильных улучшений. На начальных витерациях чуть лучше, но потом все более-менее выравнивается.
Вот. То есть, единичная матрица и так хороший вариант. Просто берете h0 единичной. Дальше уже исходя
из того, что получается, там какие градиенты, какие точки, считаете уже новые апдейты. Вот. Вот такая
соответственная идея квазий ньютоновских методов. Что мы соответственно про них можем сказать в целом?
Не считаем гисиан и не делаем обращение гисиана. То есть, сложности с подсчетом гисиана устранена,
и устранена проблема с тем, что есть арифметическая операция, которая требует от d в кубе вычислений. Вот.
Стало значительно дешевле. Вот. Более того, причем это вот финальный результат вот в этой области,
финальный результат в этой области, это результат последних пяти лет. Антон Родоманов, Юрий Нестеров,
Юрий Евгениевич Нестеров, про сходимость квазий ньютоновских методов. Вот. В оригинальной статье тоже
была сходимость Power of G jest депутат. Что есть сверх Ethereum hoch rankine. Вот. Ну, вот финальный такой и прям
финальная точка в этом вопросе, вот скорее... скорее всего это финальная точка. Redamans
Нестеров глобально – сверх Ethereum hoch rank. То есть в отличие от методов Ньют Bentans, квазий
ньютоновские методы сходятся глобально независимо от точки старта. Вот. Но, сверх
сверхлинейно. Сверхлинейно – это чуть медленнее, чем, как вы помните из оценок,
это медленнее, чем квадратично. Но на практике, на выпуклых, на выпуклых
задачках, все говорим про выпуклые. На самом деле, если мы говорим про
не выпуклые задачи и квазий-нютонские методы, и вообще метод Ньютона, запускать
можно на не выпуклых задачах, в том числе для нейронных сетей. Я иногда вижу такие
статьи, квазий-нютонский метод для нейронных сетей. Есть своя специфика, не для
всего работают, потому что вообще вот эти методы очень хорошо находят точку
минимум. Как вы понимаете, ньютонский метод быстро сваливается в минимум,
причем точно. Квазий-нютонский на самом деле тоже. Чуть медленнее, но на
практике очень близко по скорости к ньютону сваливаются к минимуму за
несколько десятков итераций. И в случае каких-нибудь нейронных сетей,
где у вас этих локальных минимумов много, свалиться в него и потом не вылезти из
него, довольно плохая перспектива. Поэтому вот часто квазий-нютонские методы
тупо застревают в каком-то локальном минимуме для нейронной сети. А нам же
нужен какой-то хороший глобальный минимум, который, да, он где-то там чуть ниже,
эта чашка расположена, но вот он просто для него не доходит до того, что сваливается.
Там нужно как-то дополнительно это исправлять, делать какие-то процедуры, там
выталкивать эту точку, либо брать какую-то стахастику, либо делать старт из
разных точек. В общем, дополнительно геморрой, для выпуклых задач классно все
работа, для не выпуклых там уже сложнее, вот именно для нейросетей вот прям неприятно.
Для некоторых не выпуклых тоже классно. Вот. Смотрите, квазий-нютоновские методы
по факту считают только градиенты, да, и для них доказана сверхлинейная
сходимость, а для метода Нестерова она только линейная. Мы с вами что-то говорили,
что метод Нестера вроде как оптимален среди методов, которые считают градиент.
В чем я вас обманул? Там или сейчас? Да, смотрите, здесь, когда мы вычисляли нижние оценки для
класса методов, которых Нестеров оптимален, это соответственно вы можете брать только линейную
комбинацию источек и градиентов. Здесь же, когда вы работаете с квазий-нютоновским методом,
вы берете произведения векторов, в том числе как скалярные произведения, так и внешние
произведения, вот, то есть из матрицы в них превращаете. Такие операции были запрещены в том
классе задач, в которых Нестеров оптимален, и поэтому, соответственно, квазий-нютоновские методы,
они его могут обогнать. Могут обогнать, потому что класс методов, в которых, соответственно,
Нестеров оптимален, квазий-нютоновские методы просто туда не входят. Квазий-нютоновские методы
просто туда не входят. Так, неразрешенные векторные произведения, соответственно, метод Ньютона очень
хорош, как метод именно дорешивателя, то есть доползли до окрестности решения, потом взяли пару
итераций Ньютона, запустили и упали там очень хорошо глубоко. Квазий-нютоновские методы тоже
хороши, как дорешиватели, потому что все равно стоимость итераций отличается от метода градиентного
спуска просто хотя бы из-за арифметических операций. Но при этом его можно использовать как бы
просто для старта, сразу как стартовый метод, потому что есть глобальная сходимость. А с какими-то
дополнительными техниками квазий-нютоновских методов, например, как ограниченная память,
потому что вот, например, в тех итерациях, которых я писал, у вас h, она обновлялась
постоянно итеративно, используя предыдущую h. Ну h себе копит в некотором смысле старые какие-то
разность градиентов, старую разность точек, а это старые свойства функций. Вы могли уйти
далеко от этих точек, поэтому тянуть эти свойства функции не имеет смысла. Не имеет смысла
просто потому что это старые какие-то свойства и, возможно, текущие локальные свойства градиента,
в том числе и выпуклости какой-то, квадратичности, они отличаются. Поэтому стоит опираться только на
что-то близкое. Поэтому есть методы квазий-нютоновские, которые так называемые с ограниченной памятью,
с ограниченной памятью. И они в некотором смысле уничтожают все, что было до этого. Плюс этих методов
составляют в том, что они еще и не требуют хранения матрицы. Потому что вот здесь вот до этого
всего безобразия нам нужно было хранить матрицу h и каждый раз все пересчитывали. Вот, когда вы в
некотором смысле храните какой-то набор векторов, из которых матрицу h можно восстанавливать,
это может быть дешевле. Это может быть дешевле. Ну и вот, соответственно, в связи с этим появилась,
так скажем, l limited версия BFGS метода, с которой вам и нужно разобраться в домашнем задании.
Хорошо. Смотрите, теперь обсудим такой вопрос. Я решил это тоже рассказать здесь. Возможно,
исторически это возникло чуть-чуть из других идей, но с этим это тоже ложится. На самом деле,
не только квазий-нютоновский вариант подходит на тот случай, когда мы хотим использовать какую-то
матрицу A вместо матрицы гесса в методе ньютона. Можно, например, там брать какую-то константную
матрицу. Это называется preconditioning, часто используется. В чем там может помочь,
но каких-то глобальных улучшений, таких именно теоретических не дает. На практике может чуть-чуть
помочь. Но есть более интересные варианты, чем можно заменить гессианы. Можно заменить,
например, вот такого-то проксимации. В чем ее суть? Смотрите, мы берем градиент. У нас
как бы вот тут вот вылезает какое-то произведение. Градиент на вектор, а потом мы покомпонентно
домножаем его на этот же вектор. А скажите мне, насколько дорого вообще будет посчитать, а тут
гессиан, я извиняюсь, тут ошибка, тут опечатка. Насколько дорого, как вы думаете, будет посчитать
вот этот гессиан на вектор? Кажется, что это просто подсчет гессиана, умножишь на вектор, но это
дорого значит. Если гессиан дорогой, для нейронных сетей невозможно посчитать дешево. Но можно что
сделать? Какие есть идеи? Нет. Гессиан на вектор это же что-то хорошее, смотрите. Вот смотрите,
у меня есть градиент. Я посчитал просто градиент. А если я градиент умножу на вектор скалярно,
это что будет? Число. Если снова посчитать теперь градиент от вот этого числа, это же тоже
эквивалентно просто взять градиент от скалярной функции. Это уже не гессиан. То есть вот посчитать
вот это на самом деле это два раза взять градиент. Просто второй градиент он хитрый. Ну и за это,
конечно, вы платите тем, что это на самом деле не гессиан, а гессиан умножить на какой-то
вектор, на который вы домножили градиент в свое время. Кстати, вот такая функциональ реализован
в петерче. То есть вы можете реально вот это посчитать почти за бесплатно. То есть граф этих
вычислений сохраняет, вы можете домножить на вектор и прогнать еще раз. А зачем домножение на
УК здесь? Ну давайте разберемся. То есть я тут в принципе рассказываю, что можно сделать. На УК тут
по компонентно, потому что вот это вектор, это тоже вектор. Вы получаете вот здесь вот вектор и вы
его выстраиваете в диагональ матрицы. То есть у вас получается диагональная матрица, где нули. А по
центру будут какие-то числа. Вопрос. Что вы можете сказать, например, про мотожидание вот этой
диагональной матрицы? Вот. Если у нас компоненты вектора УК генерируются как минус один и один,
с вероятностью одна вторая. Независимо. Что там будет? Вот вы гессиан домножаете на вектор, у вас
получается первая, там диагональная компонента первая умножается на У1, дальше вторая компонент
на У2, дальше У3 и так далее. Когда вы домножите на УК еще раз по компонентно, что у вас будет стоять
там в векторе итогового? У вас будет стоять компонента гессиана первая на У1 в квадрате,
дальше компоненты гессиана в первой строке второго столбца на У1, У2 и так далее. Согласны? Вот. Что
вы можете сказать про мотожидание таких вещей? Вот. Почему ноль? У1 в квадрате это сколько? Это
всегда один, вот это всегда один, а вот это по мотожиданию сколько? Вот это уже ноль. Получается,
что по мотожиданию в декатам будут всегда стоять диагональные элементы гессиан. Ну вот это лучше,
чем можно добиться, просто получить диагональные элементы гессиана. Вот из такого вот. И это прямо
по мотожиданию. При этом вы не знаете, что происходит в реальности, потому что это в среднем хорошо все,
а так у вас происходят разбросы. Поэтому когда работают вот с этим, тут я еще модуль забыл,
модуль, вот, модуль, то вот предлагается вот такая вот схема, вот такая вот схема, в некотором смысле
итеративно обновлять быкаты тоже, вот, и добавлять это в декаты. Вот. Это помогает чем? Это помогает
бороться со стахастикой, потому что когда вы предполагаете, что, например, ну пусть у меня гессиан
меняется не сильно, поэтому когда я вот так вот суммирую декаты и в этом в быкатах их
аккумулирую, то, соответственно, кажется, что вот это сильные разбросы из-за того, что у меня вот
этот укат как-то генерировался случайно, они в некотором смысле утихают, утихают и становится
легче. Но есть и обратный эффект, который тоже полезен, вот, в силу того, что опять же у меня
локальные свойства гессиана меняются, меняются, вот, я не хочу особо помнить, что у меня происходило
там миллионы терраций назад, сто тысяч терраций назад, вот, и в связи с тем, что вы здесь домножаете
на какой-то коэффициент, который меньше единицы, у вас старое все схлопывается, старое схлопывается,
вот. Получается такой хороший эффект. С одной стороны, боремся с астахастикой и как бы не в
тупую используем просто диагональ гессиана, вот, с другой стороны, как бы и предысторию вовремя
успеваем вычищать, вот. Окей, есть еще идея, соответственно, так называемые суперпопулярные
методы RMSProp и метод Adam, вот. Там тоже используется как раз матрица диагональная, которая обращается
вот. Вот, соответственно, только я тут чуть-чуть накосиполил, вот так вот надо, квадраты, вот.
Квадраты тут нужны. Смотрите, RMSProp был придуман, тут важно, что квадраты, потому что у вас тут
градиенты в квадрате, вот. То есть, тут берется просто вектор градиентов, вектор градиентов, вот,
и, опять же, по компоненту они умножаются друг на друга, вот, и выставляются на диагональ, вот,
на диагональ. Вопрос, а что вообще могут сказать хорошего градиента? Что вообще могут сказать хорошего
градиента? Ну, в случае, например, когда у нас квадратичная задача диагональная, у нас там,
например, что-то в духе x1 в квадрате плюс 100x2 в квадрате. Стартуем, например, с точки 1,1, да,
тогда, в принципе, градиенты нам действительно много скажут о том, как выглядит, как бы,
гессиан задач. То есть, тут будет, что получается, 2 и 200, да, в градиентах. Вы когда их там просуммируете,
ой, вы там перемножите, у вас получится как раз, условно, что-то хорошее. Вот, возьмете корень,
у вас это реально будет равно гессиану, вот, поэтому это может работать, вот. Почему это работает в
реальности, мало, ну, никто не знает. То есть, доказательства сходимости RMS-пропы и Адама,
чтобы они побили, например, сходимость градиентного спуска, такого нет. Вот, такого нет. То есть,
почему вот это все работает, до конца непонятно. С квазий Ньютона все понятно, там доказано это все.
Здесь ничего не доказано, просто какие-то еврестики. Эти подходы, они проще, чем квазий Ньютоновский,
видно же. Просто нужно там градиенты перемножить по компонентно, вот, и как-то сложить с предыдущей
матрицей. Хорошо, не проблема, вот. Но при этом это хорошо работает для тех же неровных сетей, вот,
и там нет такой проблемы, что вы застреваете в локальных минимумах, вот. Ну, просто потому,
что метод стал проще, он не так быстро сходится, вот, и получается. Но при этом вот именно вот эта
структура, что вы домножаете на эту матрицу вот здесь, вот, она помогает, она помогает, метод
сходится лучше. До конца никто не понимает, почему. Вот, какие-то идеи в духе того, что давайте
рассмотрим квадротичную задачу, посмотрим на ее градиент. В градиенте будут 200, 2 200. Теперь
давайте выстроим это в диагональную матрицу, тут получится 2 200 тоже, матрицу B. Вот, но это
пропорционально там условно гессиану, это реально гессиан, вот. Ну и вот, поэтому хорошо может быть.
А почему это в общем случае хорошо, непонятно. Отличие Адама от RMSProp вообще небольшое, то есть
у RMSProp фиксирован коэффициент, с которым вы как бы эти матрицы складываете. У Адама он меняется
в зависимости от номера террации, стремится он тоже к бета какому-то, вот. На начальных террациях
он ведет себя чуть по-другому, вот, и вы в некотором смысле меньше доверяете вот этим начальным
аппроксимациям гессиану. То, что вы используете в качестве вот этой матрицы предобрабочика,
или вот то, что методы Ньюта вместо гессиана. Вот. Вот такая вот идея. Возможно, то есть изначально
авторы, наверное, вот Адама и RMSProp они мотивировались чуть другим вопросом, ну то есть чуть другой
техникой. Но мы когда будем просто хастику разговаривать, тоже я и с той стороны к этому всему
вернусь, потому что методы суперпопулярны, там статья Адама это уже десятки тысяч цитирований,
хотя в статье содержится теоретическая ошибка, сходимость там неверная, вот. И на самом деле шаг
от Адама, от RMSProp до Адама он совсем небольшой, поменять только коэффициент БКТ. Но статья
суперпопулярна и метод является одним из ключевых методов решения обучения нейронных сетей. Вот.
Соответственно, как-то так, вот. Опять же про исчерпывающую теорию Адама никто ничего не
знает. Вот. Какие-то предположения есть, но они успешно через несколько лет опровергаются
оптимизационным комьюнити уже другими ребятами. Всё. Всё, спасибо. Спасибо, на сегодня всё.

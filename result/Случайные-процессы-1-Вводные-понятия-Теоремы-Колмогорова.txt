Так, ну что, ребята, я вас всех приветствую. Меня зовут Шаробоков Максим Геннадьевич. Я буду
вести у вас лекции по случайным процессам. Ну, это предмет второй в вероятностном цикле вашем
с кафедромоу. Первое это теория вероятностей, второе это случайные процессы, и в следующем
семестре у вас будет мат-статистика. Что вас ждет, в принципе? Ну, сначала про организационные моменты
поговорим. Вы должны будете сдать два задания. Возможно, в середине семестра будет контрольная
работа. Ну, если у нас будут лекции, семинары там мощными оставаться, то, скорее всего, будет
потоковая большая контрольная работа. Ну, а если там как-то мы перейдем на дистанционку,
ну, там тогда на месте разберемся. И экзамен в конце. Вот. Так что два задания, контрольный экзамен.
Так, что еще сказать? Ну, есть сайт, на котором лежат все мои материалы. Это они находятся на
Notion. Ну, там ссылка сложная, но если вы загуглите просто Notion, там Шарабоков, случайные процессы,
вы найдете. И там, вот на странице, посвященном случайным процессам, вы дадете очень много всего.
Там лекции, PDF-видеозаписи, семинары, PDF-видеозаписи, программа курса, список литературы, список вопросов
там на понимание, которые я часто задаю там на экзаменах, где-то на сдачах и так далее. Там же
будет появляться список задач для сдачи заданий. И, в общем, очень полезная вещь. Так что вы,
как можно раньше, с этим ознакомьтесь, туда зайдите, посмотрите, что там за материалы, и это должно
стать вашим рабочим ресурсом. Вот эта вещь, потому что там все уже лежит. Ну, там уже есть какие-то
лекции, я первый раз их прочитал в прошлом году, и я не думаю, что будут какие-то очень существенные
изменения в этом году. Ну, может быть, где-то там файн-тюнинг какой-то проведу в отдельных лекциях,
в отдельные моменты. Кто-то пытается сюда на улицу выйти. Ну так вот, хорошо, это что касается
организации. Значит, куда писать вопросы? Вы можете мне писать вопросы по предмету. Значит,
на почту. Вот, можете писать мне в ВК. Кому как удобно. Ну, я в ВК сейчас стало что-то редко
появляться, и уведомления у меня нет, во всяком случае. А сюда можете спокойно мне писать. Вот,
если возникают какие-то вопросы по организации, по предмету, по науке, по всему. Вот, и я всегда
отвечу, помогу. Так, ну, наверное, по организации все основное, что я хотел сказать. Если у вас
вопросы какие-то. Так более-менее понятно, что происходит. Ну, хорошо. Значит, еще раз обращаясь
к списку литературы. Мой курс предполагает, что вам ничего не надо, кроме моих лекций. То есть,
лекции абсолютно самодостаточны. Литература, по случайным процессам, ее очень много. Просто
невероятное количество. Но предмет сам по себе нетривиальный, сложный. И все эти книжки,
они написаны по-разному, разным языком. И вряд ли вы найдете книжку, которая очень близка к нашему
курсу. Но единственное, что может быть, книжку, которую мы написали в соавторстве, у нас там коллектив был,
Гасников, я и там еще несколько наших товарищей. Вот. Наверное, эта книжка близка. Ну, наверное,
самая близкая, на сто процентов близкая, это то, что мои лекции ПДФ, которые я выкладываю. Вот. А так,
если вы туда зайдете, вы увидите, что я там даже загруппировал некоторые литературу, там литературу,
в которой много примеров, с которой надо начинать читать литературу, которые фундаментальные,
классические книжки. Вот. Специализированная литература. В общем, там вы все найдете. А я,
наверное, перейду уже к самому предмету, если вопросов по организации нет. Значит, смотрите,
случайные процессы. Про что это вообще? Есть такие явления, которые развиваются во времени,
предсказать поведение которых мы не можем. Почему мы не можем предсказать? По разным причинам. Мы
можем перезапускать один и тот же процесс, если у нас есть такая возможность, и наблюдать разные
его реализации. Сейчас он пошел так, потом он пошел всяк. Например, вы взяли монетку, подкидываете ее,
да, и следите за результатами. Орел, решка, там, решка, решка, орел, решка. И если вы заново
перезапустите этот процесс, то вы увидите другую последовательность. Вот. И вы не можете
предсказать заранее, что выпадет потом. На самом деле, процесс может быть детерминированный. То есть,
если напрячься и его описать, то, может быть, и можно понять, как он будет дальше развиваться. Но
есть такие детерминированные процессы, которые очень трудно описать. И проще к ним относиться,
как к случайным процессам. Вот. Неформально говоря, вот такие развивающиеся во времени случайные
явления, когда есть какая-то элемент неопределенности будущего поведения, вот такие явления мы будем
называть случайными процессами. И встречаются они просто вообще повсюду. Вот я просто из разных
областей науки возьму примеры. Вот, как я говорил, там, допустим, подбрасывание монетки. Если вы
приходите на остановку, ждете автобус, и вы не знаете, когда он придет, если вы рассмотрите там
функцию, значит, индикатор наличия автобуса на остановке, то какое-то случайное время его нет,
потом какое-то случайное время он стоит, потом его снова какого-то случайное время нет. Такая
случайная функция получается. В науке, например, в теории сигналов радиосигнал приходит вам с
какими-то помехами. Эти помехи делают его случайным. В машинном обучении, когда вы обучаете какую-то
модель и следите за качеством модели как функции времени. Вот такая спадающая величина. Если вы
процесс перезапустите, у вас оптимизатор стахастический, то он пойдет по другому пути,
тоже случайная кривая. Какие еще можно привести там примеры? Из физики, например, если вы возьмете
какую-то жидкость, возьмете очень маленький подкрашенный шарик, поместите его в эту жидкость,
будьте за ним под микроскопом следить, то вы увидите, как он трясется за счет того,
что молекулы ударяют по этому шарику. То есть координат этого шарика тоже можно описывать как
какие-то случайные функции. В общем, повсюду там курс валюты, курс акций, вот эти все кривые,
так как мы не знаем будущего, как они себя поведут, это к ним тоже можно относиться как к случайным
каким-то функциям. В общем, повсюду вот эта случайность, которая развивается во времени,
она присутствует. И что мы должны сразу уяснить по этому поводу? И самое первое, что вы должны
запомнить, то что мы не всегда можем предсказать, как будет дальше развиваться процесс. На самом деле
почти всегда мы этого не можем. То есть мы не можем однозначно сказать, посмотрев на предысторию
случайного процесса, как он будет развиваться в дальнейшем. Все, эту мысль вы можете сразу себе
запомнить. Если вы не имеете возможности максимально точно, детерминированными моделями,
описать поведение процесса, все, вы его не предскажете, даже смотря на его историю,
что будет дальше. Ну что мы можем делать? Не все так грустно, хотя мы не можем предсказать,
что будет дальше, как будет развиваться процесс. Мы можем попробовать оценить,
вычислить вероятность того, что процесс поведет себя тем или иным образом, вероятность того,
что процесс будет возрастать в каком-то участке времени, или убывать, или сначала убывать,
потом возрастать, или посчитать какие-то численные характеристики, связанные процессом,
там средние процессы, как ведет себя процесс в среднем, каков его размах. Но для того,
чтобы довести до числа, чтобы понять, что будет происходить с процессом, хотя бы в этих терминах,
вероятностных терминах, чтобы довести до числа, чтобы вычислить эти характеристики, нам нужна модель,
подходящая модель описания случайного явления, случайного процесса. Вот, и вот тут начинается
математика. Как мы будем это моделировать? Мы построим модели случайных явлений, случайных
процессов на основе теории вероятности. У вас была теория вероятности, много чего там было,
вам сказали, что такое случайные величины, функция распределения, плотность, мат ожидания,
дисперсия, формулы все эти, хар-функция и так далее, всякие корреляции, независимость и так далее.
Вот это у вас все было, и мы попробуем применить весь вот этот аппарат к изучению уже случайных
функций, не случайных величин. Когда ты касаешься к величине, она тебе выдает какое-то значение
мгновенно, снова коснулся до нее, она тебе снова какое-то значение выдала, а теперь все это
разворачивается во времени. Но как перейти от случайных величин, стационардных, независящих от
времени, вещей, сущностей, к изависищим от времени, давайте мы добавим просто некий свободный
детерминированный параметр t, и что мы получим? Вот давайте смотреть. Сегодня на этой лекции мы
попробуем с вами вот этот фундамент случайных процессов построить и посмотрим, какие из него
будут следствия. Первое определение, которое мы дадим, это определение случайного процесса, ведь про это
же наш курс. Случайным процессом, я буду сокращать, случайным процессом обозначается он вот так вот,
я все поясню сейчас t, значит с какого-то множества t большое, как под множество r. Случайным
процессом называется параметрическое семейство случайных величин, определенных на одном
вероятностном пространстве. Давайте мы его сразу обозначим, Омега ФП. Смотрите, тут сложные слова,
да, я понимаю, но смотрите, в чем суть. Случайная величина это функция исхода, это кси от Омега,
вы это должны прекрасно знать. Здесь что? Мы добавляем еще одну вещь, которую называем временем,
хотя физически это может не обязательно быть время, но его люди называют временем,
это t. Оно может быть дискретным, может быть непрерывным, неважно. И у нас появляются две
переменные. Если мы зафиксируем t, то по определению мы получим случайную величину.
Вот, t значит время, давайте так, Омега исход. Значит, если мы фиксируем время, t0, допустим,
обозначим, то мы получим случайную величину, она называется сечением процесса. Это случайная
величина. Наоборот, если мы зафиксируем исход, мы получаем обычную детерминированную функцию,
обычную функцию экспонента, синус, косинус, всякие линейные функции, всякое может здесь быть.
Эта функция называется реализацией процесса. Еще и называют траекторией. Траектория процесса.
Вот, зафиксировали время, получили сечение, случайную величину. Она принимает различные
свои значения в зависимости от того, какой исход будет. Если мы зафиксируем исход, у нас
фиксируется вся эта кривая, она определена, это детерминированная кривая. Ну вот, сечение и
реализация. И теперь смотрите, хорошо, мы ввели такую модель, теперь вернемся к нашей проблеме,
прогнозы, предсказания поведения процессов. Вот рассмотрим какой-нибудь процесс, например,
там курс акций, он как-то развивается, и пусть вот этот момент времени Т настоящее. И есть где-то
Т-будущее. Все, что нам дано, это прошлое и настоящее. В будущий момент времени мы не знаем,
что будет. Мы не знаем, что будет. Процесс может принять вот такое значение. Такое, такое, такое.
Видите, может разные значения принимать. Где-то точки могут быть плотнее расположены, то есть вероятность
попасть туда больше. Вот это отвечает распределение соответствующей случайной величины. Вот эта
случайная величина, которая принимает вот эти значения в этот момент времени, это есть сечение в
этот момент времени. Вот это вот Си от Омега Т-будущее. Т-будущее мы зафиксировали, видите,
расцекли сечение. А вот эти различные реализации отвечают просто различным исходам. И проблема
здесь состоит вся в том, что в общем случае различным исходам может соответствовать одна и та же
предыстория. То есть если у вас вот это вот Омега ноль, и для реализации Омега ноль процесс развивается
вот так, то для какой-нибудь другой Омеги один, прошлое и настоящее может быть одинаковым, а будущее
разным. И вы не знаете, какая Омега на самом деле реализовалась. То есть когда, смотрите, как вот на
это можно посмотреть. Вот вы смотрите на курс акции, вот растет вот эта кривая. Как это можно
интерпретировать? Что мы реально наблюдаем? Мы реально наблюдаем некоторую реализацию. Реально
мы наблюдаем некую реализацию. Мы видим ее историю, прошлое и настоящее. Мы не знаем ее
будущее. Почему? Где-то как будто где-то во Вселенной реализовался этот исход для некоторой функции
кси Омега Т, и вот мы наблюдаем эту конкретную реализацию. Но мы не знаем эту функцию, как функцию
двух переменных, и мы не знаем эту реализацию, и поэтому мы не знаем будущее. И бывает так, что у нас
может быть одно и то же прошлое и настоящее, но разное будущее для разных исходов. Мы не знаем,
какой исход. Мы не можем по предыстории понять, что будет дальше, потому что предыстория одинакова
для разных исходов. Видите, в чем проблема? Вот такая проблема. Но что мы можем делать? Мы
можем попробовать вычислять вероятности какие-нибудь. Например, вероятность того,
что процесс попадет в какой-нибудь интервал. Мы здесь опираемся на случайные величины,
здесь мы рассматриваем вот такую случайную величину. Это случайная величина, у нее есть какая-то
функция распределения. А если есть функция распределения, значит, потенциально теоретически
мы можем вычислить вероятность попадания в какой-нибудь интервал. Можем посчитать среднее
математическое ожидание и так далее. Но для того, чтобы это делать, нам нужно обобщить понятие
функции распределения на процессы. Что такое функция распределения для случайной величины,
мы знаем. А что такое функция распределения у случайного процесса? Это самая главная
характеристика случайного процесса. Давайте мы это посмотрим, как это можно определить. Смотрите,
что мы сделаем, чтобы это определить. Мы рассмотрим с вами несколько сечений процесса, то есть
случайные величины в разные моменты времени. Например, кси от омега t1, кси от омега t2 и так далее.
Конечное число их рассмотрим. Омега tn. t у них могут, в принципе, у кого-то даже совпадать, но давайте
для простоты считать, что они все разные. Омеги везде одинаковые. Вот это все сечения, взятые в
разный момент времени. Это все случайные величины. И так как по определению они заданы на одном
вероятностном пространстве, это значит, что они образуют случайный вектор. Вектор случайных
величин, заданных на одном вероятностном пространстве, это случайный вектор. А у случайного вектора есть
функция распределения. Мы ее знаем. Я ее буду обозначать вот так. f кси x1 и так далее xn, точка
запятая t1 и так далее tn, чтобы почернуть, в какие моменты были взяты эти сечения. Это есть
вероятность на множестве исходов таких, что кси от омега t1 меньше x1 и так далее кси от омега tn
меньше xn. Вот такую вещь рассмотрим. Они определены все на одном вероятностном пространстве. Каждый из
них является случайной величиной. Множество омег, которые удовлетворяют этому условию, оно измеримо,
потому что это случайная величина, измеримая функция исхода, и каждая из них это функция измеримая.
Здесь стоит конечное пересечение множества сигма алгебра, поэтому их пересечение тоже является
функцией сигма алгебра, поэтому эта вероятность определена. Ну хорошо, такую вещь мы вести можем.
Вот она называется н-мерной функцией ей распределения случайного процесса.
Н-мерная функция распределения случайного процесса кси. Здесь n больше либо равно 1, t1
и тн произвольные из вот этого t большого. Вот множество таких вот функций f кси от x1 и т.д., xn, t1 и т.д., tn при n
больше либо равных единице и t1, tn из t, если мы проварьируем, по n и по t. Вот множество
вот этих вот функций называется семейством конечномерных распределений,
семейством конечномерных распределений случайного процесса. Вот это будет нашей
основной величиной, которая будет, с помощью которой можно вычислять вероятности различных событий.
Ну, например, просто пример, переду, как мы можем этим пользоваться. Допустим, нам нужно в какой-то
будущий момент времени посчитать вероятность попадания в некоторый интервал. Тогда для этого нам
нужна одномерная функция распределения f кси. Допустим, мы ее знаем, f xt, которая равна вероятности
по всем исходам таким, что и кси от ωt меньше xа. Тогда как нам у момента t будущее, допустим,
посчитать вероятность того, что мы попадаем в некоторый интервал, скажем, ab интервал. Вероятность
того, что кси от ωt, будущее, попадает в какой-то интервал. Вот это есть просто разница. Функция
распределения кси в точке b в момент t, будущее, минус функция распределения кси а в момент t,
будущее. Ведь если мы функцию f xt, как функцию двух переменных, знаем, то просто подставляем туда
наши b, a и t, будущее, вычисляем, и то число, которое мы получим, это есть вероятность того,
что в этот момент мы попадем в этот интервал. Но можно, конечно, и какие-то более сложные
вероятности считать со использованием f. Это я просто привел пример. Теперь движемся дальше,
немножко более глубоко посмотрим, какими же свойствами обладает эта f и действительно ли
ее достаточно для описания вероятностных свойств процесса. Потому что, видите, я тут ввел
почему-то конечное число t, почему конечное, почему он конечный. Оно сколько угодно большое может
быть, но оно конечное. Почему я не бесконечное взял? Давайте рассмотрим поглубже свойства вот
этой функции f семейства, конечно, мерных распределений. Какими свойствами обладает,
ну, значит, свойство мерных функций распределения случайных процессов. Первое свойство очевидное,
так как f, что бы это ни было, f это вероятность. Вероятность у нас лежит в интервале от нуля до
единицы. Значит, никуда не деться, эта функция тоже обязана лежать в интервале от нуля до единицы.
Для любых n, x и t. Второе, значит, эти функции, я не буду это переписывать, вот это все,
эти функции непрерывны слева по каждой переменной x. То есть, если мы зафиксируем все x кроме одного и
посмотрим на эту функцию как функции этого x, то эта функция будет непрерывна слева. Ну, вы можете
какую-нибудь другую книжку открыть и увидеть, что там написано непрерывно справа, а я говорю,
непрерывна слева. Почему? А потому что вот почему. У нас в курсах во всех вероятность и функция
распределения, вернее, она определяется через строгое неравенство. Видите, меньше. Если здесь
меньше функция распределения, то тогда будет непрерывность слева. Если здесь меньше либо равно,
то тогда будет функция непрерывная справа. Ну, такая тонкость есть. Ну, не то чтобы это было
где-то принципиально важным, но как бы конкретно здесь вы можете не понять, почему где-то написано
справа, хотя у меня было слева. Вот все дело в том, как определить функцию распределения,
строгий там знак поставить или нет. Если строгий, то слева. Так, ну, это следствие из теории вероятности,
я не буду на этом останавливаться. Вот, следует из свойств непрерывности вероятности. Так,
еще одно свойство. Если хотя бы одна временная эксцита стремится к минус бесконечности, а остальные
постоянно, ну или никуда не стремятся, а остальные постоянно, то тогда что получается? Если эксцита
стремится к минус бесконечности, например, x1, тогда вот это событие становится все более невозможным,
оно пересекается с другими, неважно с какими, туда эта вероятность будет стремиться просто к нулю,
по свойству непрерывности вероятности. Это значит, что f к c, x1 и так далее, xn, t1, tn стремится к нулю. Вот,
при эксцитам стремящимся к минус бесконечности. Вот, а если все, здесь все, а здесь хотя бы одна
переменная, если все эксциты стремятся к плюс бесконечности, тогда, если все эксы стремятся к плюс
бесконечности, все эти события стремятся быть более достоверными. Вот, так что эта вероятность будет
стремиться к единице. Вот, то тогда, тут, наверное, внизу не видно, я вот тут продолжил, f к c от x1, ну и
так далее. Все это стремится к единице, когда все они стремятся к плюс бесконечности, все эксы
стремятся к плюс бесконечности. А здесь, когда один стремится к минус бесконечности, тогда это стремится
к нулю. Вот. Тут такие свойства, которые лежат на поверхности. Еще одно свойство, такое как бы не
не очень тривиальная, не очень очевидная. То, что эта функция, она монотонна в некотором смысле.
В следующем смысле. Смотрите, ну давайте я пока так напишу и поясню. Значит, дельта 1 это
оператор троеточие дельта n, n операторов, которые действуют на f к си, оператор конечной разности,
x1 и так далее, xn, t1 и так далее, tn. Вот эта вещь больше либо равна нуля. Где дельта и это
линейный оператор, который, ну да, звучит очень заумно на самом деле, но за этим тривианщина просто
скрывается. Смотрите, линейный оператор, который действует так. Если дельта ита действует на f к си,
я не буду здесь все переписывать, то это получается функция, которая вычислена до х и минус 1, и к х и
прибавляется h иты больше либо равен ноль. Дальше идет и плюс один и так далее до конца. То есть мы
иты x повысили на h итую и вычли, как если бы ничего не делали. Вот. Просто подействовали на итую
компоненту, повысили ее на h итую. Вот. И то, что здесь стоит, это означает, что мы подействовали на x
с n таким образом. Получили разность. Потом дельта n-1 подействовала на эту разность. То есть получилось,
что в этой разности мы должны уже x n-1 увеличить и отнять, когда он не увеличен. И мы получим уже
четыре слагаемых. А потом дельта n-2 подействует на эти четыре слагаемых. Мы должны снова вот такую
разность посчитать. Понятно, что это такое? Вот. Сложная такая вещь. Так вот, это всегда больше либо
равно нуля. Чуть позже я поясню геометрический смысл этой вещи, а пока просто зафиксировали
для себя, что есть такое свойство монотонности. Вот. Так. Ну, еще пара тривиальных свойств.
Еще пара свойств. Там, по-моему, про перестановку. Да. Пятое. Смотрите, какое еще свойство есть.
Здесь стоят пересечения событий. Но если мы их переставим местами, ничего не изменится. Если в
пересечении переставлять множество местами, пересечение не меняется. Так что, как бы ты не
переставлял, функция от этого не поменяется. Поэтому для любой перестановки элементов множества 1n,
для любой перестановки, как это обозначается, k1, kn этого множества, мы получаем, что fкс,
xk1 и так далее, xkn, когда мы x переставим местами и, соответственно, мы переставим t местами точно
таким же образом, то тогда это будет то же самое, как до перестановки. Вот. x и t мы не меняем. Мы
просто их переставляем местами. Как бы вы их не переставляли, мы получим одну и ту же функцию.
Вот такое свойство, тоже кривиальное. И еще одно свойство, если мы возьмем вот эту функцию и,
начиная с некоторого, скажем, k плюс 1 до конца, все x возьмем равными плюс бесконечности, то тогда то,
что мы получим, будет являться функцией распределения, но уже k-мерной. Вот как. То есть fкс,
x1 и так далее, xk плюс бесконечность и так далее, плюс бесконечность, t1 и так далее, tn. Здесь
их n было. Но вот это, это k плюс 1 и так далее до n. Здесь мы заменили на плюс бесконечность. То это
есть fкс от x1 и так далее, xk, t1 и так далее, tk. Вот. Вот такие шесть свойств. Это свойство
согласования. То есть вот эти функции, смысл в чем? Вот эти функции, n-мерные функции распределения,
они не могут быть всякими. Они обязаны удовлетворять вот этим свойствам. От нуля до единицы,
непрерывность слева, стремление к нулю к единице, когда x и там, соответственно, куда-то стремятся,
монотонность, перестановки и вот такая согласованность между этими f. Вот это свойство
семейства конечномерных распределений. Мы с вами уже получали что-то подобное в теории вероятности,
когда изучали свойства функции распределения. Помните, она тоже была сейчас от нуля до единицы,
тоже была непрерывна слева, была монотонна. Так. Тоже там стремилась к нулю при х, стремящемся к
минус бесконечности и к единице при х, стремящемся к плюс бесконечности. И так получалось, что вот
функция распределения обладает этими свойствами. Если вы возьмете просто какую-то функцию,
которая обладает этими свойствами, то тогда можно доказать, что она отвечает некоторой случайной
величине, что она на самом деле является функцией распределения некоторой случайной величины.
И вот теперь смотрите, здесь то же самое. Семейство конечномерных распределений обладает всеми этими
свойствами. Теперь наоборот, если вы возьмете семейство каких-то функций, каких-то, которые
обладают всеми этими свойствами, то на самом деле это семейство функций является семейством
конечномерных распределений для некоторого процесса. Вот как. И то, что я назвал, это теорема
Колмогорова. Сейчас я ее запишу, у вас вопрос был. Да, да, именно ТК. Ну потому что плюс бесконечность,
меньше Х, там неважно, там пропадает уже это Т. Так что все. Вот то, что я сейчас сказал, это есть
теорема Колмогорова. Это первая наша с вами теорема в этом курсе. Она пойдет без доказательства.
Теорема Колмогорова. Звездочка я обозначу, потому что без доказательства. Сейчас я ее точно
сформулирую. Значит, пусть задано семейство функций, семейство функции Ф. Я к себе не пишу, потому что это
просто какое-то семейство функций вот стольких переменных при n больше либо равных единице t и tn,
удовлетворяющие условиям 1.6. Тогда существует вероятностное пространство Омега ФП и случайный
процесс, и случайный процесс, вот. Определенный на этом пространстве.
Ну, семейство конечномерных распределений,
которого совпадает с F. А, подождите, тут у меня сигма-алгебра F, и там F не очень это удачно. Давайте мы
сигма-алгебру будем такой функции каллиграфической F писать, а здесь я лучше как-нибудь попрямее,
попрямее сделаю. Это семейство функций, а там сигма-алгебра. Вот так. Еще у меня t большой где-то отсутствует,
вот здесь вот. Вот так. Пусть задано семейство функций со свойствами 1.6. Тогда найдется вероятностное
пространство и процесс на нем, у которого семейство конечномерных распределений совпадает с F. Важное
здесь дополнение, такое пространство и процесс могут быть не единственные. Вот. Не обязательно одно
найдется такое, но обязательно найдется. Вот так вот. Обязательно найдется. И теперь смотрите,
зачем вообще эта теория нужна. Это очень важно. Я ввел определение случайного процесса как функции
исхода и времени двух независимых переменных. Исхода и времени. Это один из способов задания процессов.
Точно так же как задание функции исхода, это один из способов задания случайной величины. Но гораздо
удобнее случайные величины задавать не какими-то функциями исходов, а распределениями. Мы говорим там,
дали нам случайную величину с нормальным распределением. Ну какая разница какая там,
кси, атомеха? Она существует. Какая-то она существует. Какую-то можно подобрать. Ну какая разница?
Тебе нужны от случайной величины вероятностные численные характеристики. Тебе функция распределения
важна. Вот так и здесь. Мы не будем задавать случайные процессы как функции исхода и времени, хотя это
их определение. На практике это не нужно. На практике гораздо важнее вероятностные свойства. И вот эта
теорема говорит о том, что процесс можно задать через задание семейства конечномерных распределений.
Для случайной величины мы можем задать функцию распределения. И все, мы работаем с ней как случайной
величиной. А здесь одной функции не обойтись, но вот семейству нужны функции. Семейство конечномерных
распределений. И это будет наш как бы основной подход. Мы процессы будем определять по семействам
конечномерных распределений. Потому что согласно этой теореме нам главное, чтобы 6 свойств выполняли,
чтобы было согласовано. Семейство было согласованным. Если это так, то вот это абстрактное пространство,
вот эта функция, они найдутся. Все. Так что вот в чем польза вот этой теоремы. Мы можем задавать
процессы, задавая их функцией распределения н-мерные, а не функцию времени и исходов, что неудобно.
Еще одна польза этой теоремы, которую мы правда сейчас не прочувствовали, потому что ее не доказывал,
то, что на самом деле доказательство этой теоремы конструктивно. То есть реально не просто
знать, что теорема существования существует, а пример ты привести не можешь. Нет. Если посмотреть
доказательства, то можно увидеть, как именно строится это пространство и как строится процесс как
функция исхода и времени. Так что вот польза этой теоремы, она еще и такая. Если тебе все-таки
хочется построить этот процесс, открываешь доказательства этой теоремы и смотришь, как
он устроится в самом общем случае. Там прямо алгоритм есть. Так что очень полезная теорема,
но она очень большая и очень долго тяжело доказывать, поэтому я это делать не буду,
потому что наш курс, он не про это. Так, хорошо, об этом я сказал. Следующая вещь,
которую здесь надо пояснить. Здесь написано, что семейству распределений соответствует обязательно
какой-то процесс. По этому семейству можно построить процесс. Возможно не единственный.
Вот за счет того, что у вас могут быть две случайные величины с одним и тем же распределением,
у вас могут быть точно также и два случайных процесса с одним и тем же распределением,
с одним и тем же семейством конечномерных распределений. То есть как функции исхода и
времени, они разные. Функции амигиты, они разные. Вот распределение у них одинаковое.
И вот для этого, сейчас я определение веду и на перерыв идем. Для этого вводят такое вот
полезное определение. Оно нам пригодится потом на протяжении почти всего курса.
Понятие об эквивалентных случайных процессах. Еще одно определение ведем.
То есть случайные процессы кси-т и это, естественно, определенные на одном вероятностном пространстве.
Определенные на одном вероятностном пространстве и удовлетворяющие свойства вот такому,
что для любых ТСТ вероятность исходов, на которых кси от омега Т равняется это от омега Т равно единице,
называются стахастически эквивалентными или просто эквивалентными. И в этом случае,
если для кси это вот это выполнено, то кси называется модификацией еты, а это называется
модификацией кси. Вот такая терминология. Модификация еты. А это модификация кси.
Обратите внимание, что квантор для любого стоит вне вероятности, не внутри. Если мы это поместим
внутрь, это будет другое. Такое тоже есть определение, тоже в некотором смысле эквивалентность,
но она нам не пригодится. Для любого ТСТ стоит снаружи. Вот это важный момент. Обратите на это
внимание, не внутри. Вот такое определение запомним, оно нам еще пригодится. Пойдем на перерыв.
Ладно, ребята, давайте мы продолжим. Здесь вот логическая линия пока завершилась, которая вышла
из этого семейства теоремы Калмогорова. Давайте я вернусь теперь вот к этой вещи. Что это такое
значит геометрически. Вот если n равняется единице, я показал, что это такое. Тут эта вот разность.
И если мы рассмотрим, например, дельта 1 f х1 t1, это будет f х1 плюс h1 t1 минус f х1 t1. А разность
функции распределений, это вероятность того, что хи, давайте я омеги опущу, как мы для случайных
величин опускаем, так и я и здесь опущу. Значит в t1 принадлежит, значит вот так по-моему, х1 х1
плюс h1. Вот вероятность. Она больше либо равна нуля. Видите, то есть 1 дельта больше либо равна нуля.
Теперь давайте рассмотрим двумерный случай дельта 1, дельта 2. Дельта это вероятность попадания
процесса в интервал от х1 до х1 плюс h1. Теперь что такое дельта 1, дельта 2 f от х1 х2 t1 t2.
Ну сначала подействуем дельтой 2. Дельта 1, подействуя, что получим f, значит х1 х2 плюс h2 t1 t2
минус f х1 х2 t1 t2. Вот, и скобочку еще закрываем. Вот я подействовал дельтой 2. Видите, х1 на месте
находится. Дельта 2 действует только на второй аргумент, только на х2. Вот, а теперь дельта 1
действует по определению от линейного оператора. То есть дельта разности равняется разности дельт.
Вот, можно так на это посмотреть. Ну либо можно целиком ко всей этой штуке, всю эту штуку
увеличивать на х1. Давайте так, наверное, поступим. Сначала я увеличу х1, а потом вычту то, что у нас было.
х1 плюс h1, х2 плюс h2, t1 t2, минус f х1 плюс h1, х2 t1 t2. И минус то, что было до этого. То есть когда
мы х1 не трогаем. То есть надо просто вычесть вот эту скобку, которая идет выше. Минус f х1 х2 плюс h2 t1 t2. Ну и минус на минус даст плюс здесь.
Видите, f х1 х2 t1 t2. Вот какая вещь. Ну и что это такое? Так, где тряпка-то?
Давайте мы это обозначим. Вот у нас есть плоскость х1 х2. Вот этих двух переменных. И у нас здесь есть
где-то точка х1 плюс h1, х1, ну давайте так, есть точка х2 и х2 плюс h2. Вот у нас четыре точки на плоскости.
Что такое первое f? От х1 плюс h1 х2 плюс h2. Это вероятность того, что хи t1 меньше вот этого,
а хи t2 меньше вот этого. Меньше этого, меньше этого. Вот на плоскости это есть вероятность попадания вот в этот угол.
Вот в этот все, вот в этот угол. Вероятность попадания вот сюда. Так? Вот эта вещь.
Вероятность попадания в этот угол. Здесь стоит минус. Вот такая вероятность. А это что такое?
Значит у нас здесь х2 и х1 плюс h1. Минус вероятность попадания вот в этот угол.
Видите? То есть та вероятность. Минус эта вероятность. Идем ниже. Х1, х2 плюс h2.
И это уже вероятность попадания вот в этот угол. Она тоже вычитается.
А потом идет плюс вероятность вот этого угла х1 х2. Потому что, видите, мы его два раза отняли.
Мы его отняли, когда вы читали этот угол и отняли, когда вы читали этот угол. Так что вот этот
меньший угол мы отняли два раза. А здесь мы его прибавили один раз. Так что в сумме получается так,
что мы получаем вероятность попадания в этот прямоугольник. Вот. Вот каков геометрический
смысл вот этой конструкции. Это есть вероятность попадания в тот прямоугольник. Вероятность того,
что кси t1 принадлежит х1 х1 плюс h1, запятая, кси2 принадлежит х2 х2 плюс h2. Вот. А раз это вероятность,
она не отрицательна. Поэтому и дельта 1 дельта 2 тоже не отрицательна. Вот. И в общем случае можно
показать, что вот эта серия дельт, когда n произвольна, это есть вероятность попадания в какой-то
там параллелепипед в инмерном пространстве. А раз это вероятность, то она не отрицательна. Вот
смысл геометрический вот этой вот конструкции. Хотя, да, выглядит она сложно. Вот видите, даже для n
равно 2 тут уже пришлось постараться, чтобы ее выписывать. А для n равно 3 там будет еще больше слагаемых.
Вот. Понятно это? Ну, вроде так. А? А как это вероятность попадания отрицательна? Это как?
Ну, что-то мне не очень понятно, что вы имеете в виду. То, что здесь какие-то скачки возможны,
ну и пусть они не ухудшат. Если прямоугольник станет больше, вероятность не уменьшится. Как она
может уменьшиться? Потому что вероятность попадания в маленький прямоугольник здесь,
она тоже положительна. Если вы прямоугольник расширите, то вы не уменьшите вероятность.
Потому что какая разница? Что-то мне не очень понятно. К минус бесконечности?
Ну, получим угол большой.
Ну, не может быть вероятность отрицательна.
Ну, это невозможно. Ну, как вероятность отрицательна может быть даже на каких-то прямых?
Это непонятно, что вы имеете в виду. Что значит на прямой вероятность отрицательна?
Какого события и вероятность надо посмотреть? Ну, давайте мы потом с вами поговорим после этого.
Я это все закончил, потом разберемся. Это я прокомментировал этот четвертый пункт. Дальше едем.
Дальше едем. Что еще можно сказать? По поводу астахастических эквивалентностей. А, нет, это мы
закрыли. Теперь давайте поговорим о том, каким может быть Т, каким может быть время и о проблемах,
которые там возникают. Что мы сейчас с вами поняли? Итак, мы предложили определение случайного
процесса. Окей. Поняли, что слишком сильно оно нам не помогает с точки зрения предсказания, но мы
можем попробовать вычислять вероятности. Для этого мы ввели понятие семейства конечномерных
распределений, как и функция распределения при случайной величины. Семейство конечномерных
распределений — это такая важная характеристика, которая все говорит о процессе. Мы посмотрели
свойства этого семейства распределений и установили, что эти свойства необходимы и достаточно для того,
чтобы быть семейством конечномерных распределений какого-то случайного процесса. Это теорема
Колмогорова. Выяснили, что в принципе могут быть два разных процесса, как разные функции — омеге и Т,
но у которых семейство конечномерных распределений совпадает. Я тут единственный только забыл
сказать, что если два процесса стахастически эквалентны в этом смысле, то тогда у них семейство
конечномерных распределений совпадает. Я не буду это доказывать, но это очень легко доказать,
как бы я это вам оставляю в качестве упражнения. Но вы, если пока не знаете, как это делается,
ничего страшного, просто пока запомните, что если они стахастически эквалентны, значит у них
совпадают распределения, семейство конечномерных распределений. Теперь, когда мы ввели вот эту Т в новое
время, то мы на самом деле получили не только от этого пользу, но и некоторые проблемы. Какие могут
быть Т? Вот Т-большое я впереди вводил. Т-маленькое принадлежит Т-большому. А что это за множество Т такое?
Каким оно может быть? Если Т одноэлементно, то есть в момент времени всего один, то по сути никакого
семейства нет. В твоем семействе только одна случайная величина. Значит, твой процесс – это
случайная величина. То есть если один элемент, следовательно, случайный процесс – это случайная
величина, по сути. Если там N элементов, конечно их число, то по сути ты имеешь дело со случайным вектором.
Тебя несколько случайных величин, заданных на одной вероятности пространства, не образуют вектор.
Поэтому твой процесс, по сути, это вектор. Но вот эти две ситуации – это то, что вы изучали в теории
вероятностей. А нас интересует то, что идет дальше. Если там, если T равняется N множеству натуральных
чисел или множеству целых чисел, то тогда это уже бесконечная множество случайных величин. Такие
случайные процессы называются случайными последовательностями. В общем-то, вам эта вещь
тоже известна. Вы изучали по теории вероятностей там сходимость в разных смыслах, случайных
последовательностей. Так что вот на самом деле вы уже работали со случайными процессами. И еще один
случай, который уже новый для вас. T может быть равно R, может быть R+, я имею в виду от нуля до плюс
бесконечности, включая ноль. Такое полезное обозначение у нас будет встречаться. Это может быть какой-то
интервал от A до B. Такие случайные процессы называются случайными функциями.
Потому что время здесь непрерывно. Вот здесь время дискретно. Во всех первых трех случаях время
дискретно. Здесь так оно вообще конечно. Здесь время непрерывно. И вот в этом случае возникают
определенного рода проблемы. Сейчас я вам скажу, какие. Смотрите, мы в дальнейшем захотим вычислять
вероятности различных событий, которые связаны с процессом. Одно дело, вопрос типа посчитать
вероятность того, что процесс в какой-то момент времени принадлежит отрезку A-B. Это одно. А вот
как насчет таких событий? Типа найти вероятность того, что процесс возрастает на некотором интервале.
Или даже еще проще. Вероятность того, что на всем интервале процесс строго больше нуля.
То есть это событие, оно касается этого процесса во всех точках для любых T. Если у тебя T непрерывно,
то получается, что это событие, которое связано с процессом, внесет на множество точек.
Вот определены ли такие вероятности? Вот давайте мы об этом поговорим.
Рассмотрим такую проблему. Пусть дан некий процесс, и нам нужно найти вероятность вот такую.
Пусть им Supremum по T, принадлежащим от нуля до единицы, некоторого процесса x и от T, и мы
учитываем вот такое событие. Меньше x. Ну как бы вероятность того, что Supremum процесса на отрезке
не превышает какое-то значение x. То есть процесс на отрезке не превзошел x на этом интервале.
Нормальное такое событие, правильно? Может такое быть? Может такое быть? Почему бы и нет?
Вопрос, определена ли это вероятность? Во-первых, надо понять, а в чем здесь, собственно, проблема.
Смотрите, для каждого фиксированного T, x и от T это случайная величина. Вот такое событие,
оно обладает вероятностью, оно измеримо. Множество амих таких, что вот это, оно измеримо,
над ним определенная вероятность. Это элемент sigma-алгебра f, на котором определены случайные
величины x. Правильно? Просто по определению случайных величин. Вот такое событие, скажем,
Assassins' не можно здесь, вот по каким-то причинам, пересечь sting historicum,
тау и тау, возьму там, SAM. versioning.com, вот это тоже элемент sigma-алгебры. Тоже множество таких f
подходят, потому что это элемент sigma-алгебры, это элемент sigma-алгебры, ну, и конечное пересечение
Вероятность всякого события, которое состоит из конечных пересечений вот таких штук.
А теперь посмотрим сюда, что вот это такое.
Ведь для того, чтобы супремум был меньше х,
необходимо и достаточно, чтобы в каждой точке t он был меньше х.
То есть пересечение для всех t от нуля до единицы множество кси от t меньше х.
Но здесь не счетное число пересечений.
А если мы не счетное число раз пересечем элементы сигма-алгебры,
мы не обязательно получим элемент сигма-алгебры.
Поэтому ответ такой.
В общем случае, когда мы ничего не знаем априори про процесс,
про его реализации траектории, мы не можем сказать, определена эта вероятность или нет.
Она может быть определена.
Вдруг так совпало для этого процесса всё хитро,
что на самом деле вероятность определена на такой штуке.
Она может быть, в принципе, определена.
Мы не можем поставить следствие, что она определена,
потому что эти определены.
Но в принципе, может оказаться, что это событие измеримое,
вероятность о нём определена.
А в каких-то случаях может так не быть.
В каких-то случаях нет.
на самом деле, потому что ты там такой весь из себя, значит, строишь свои модели, потом там
пытаешься вычитать такие вероятности, а потом такой раз, и ты задумываешься, а что ты вообще
считаешь? То, что ты считаешь, оно вообще хотя бы существует, потому что так-то вообще нехилая
задачка, вот такие вот вероятности считать, и может быть, ты тратишь много времени, а на самом
деле эта вероятность просто не существует, у тебя просто ее в принципе не получится вычислить.
Когда мы имели дело с случайными величинами, векторами, последовательностями, там не было
таких проблем, там все счетное, не более чем счетное, и как ни пересекаю, у тебя все существует и прекрасно,
все эти вероятности, какие себе можно помыслить для практики, они все существуют, а вот здесь вот
такая вот тонкость есть, что вот несчетное пересечение, поэтому вероятность может не существовать.
Ну а смотрите, в каких случаях мы можем точно сказать, что эта вещь будет существовать, ну хотя
бы вот это вот, этот supremum. Например, если мы уверены, что мы имеем дело с процессом, у которого
все траектории непрерывны, если все траектории процесса непрерывны, то вот это событие равносильно,
ну во-первых, вот этому, а во-вторых, пересечению по всем рациональным точкам отрезка 0,1, а их уже счетно,
то есть равно для случая, когда все траектории непрерывны, пересечению уже Т принадлежит из 0,1
пересечь с Q, а это счетное множество, значит оно измеримое, значит вероятность определена,
но другой вопрос, как посчитать, пока его не берем, пока только принципиальные вещи рассматриваем,
потому что если мы уверены, что все траектории непрерывны, тогда да, тогда абсолютно точно говорим,
все можно считать без проблем, по крайней мере вот такую вероятность, а если мы этого не знаем,
ну тогда хрен его знает, надо-то осмотреть, более глубоко изучать процесс, с чем мы имеем дело,
и это уже зависит от задачи и того, с чем мы имеем дело. Вот, теперь, одна из удивительных вещей
случайных процессов стоит в том, что один и тот же процесс можно определить так, что у него множество
траекторий произвольно и множество траекторий непрерывно. Представляете, процесс вы определили
одним образом, так что множество его траекторий всякое, но может так оказаться, что почти все его
траектории непрерывны, или что-то можно сделать, какое-то другое вероятностное пространство задать
так, чтобы на новом вероятностном пространстве этот процесс имел непрерывные траектории, а семейство
конечномерных распределений было такоже, как у исходного. То есть, понимаете, вот на практике нас,
опять же повторяю, не волнует, на каких абстрактных пространствах вероятностных задан случайный
процесс Омега-ФП. Нас интересует только семейство конечномерных распределений, и мы вольны делать
процессом всё, что угодно, лишь бы его семейство конечномерных распределений оставалось одним
и тем же, потому что именно его мы используем для вычисления каких-то характеристик. Так вот, может
оказаться так, что для вашего процесса существует непрерывная модификация. Модификация в том смысле,
в котором я написал. Вот, может существовать непрерывная модификация, то есть на том же
самом вероятностном пространстве задать другой процесс, у которого будут другие, у которого будут
те же самые характери... свойства вероятностные. И есть теорема, тоже Колмогорова, о том, как выяснить,
какие процессы, у каких процессов существует непрерывная модификация. Значит, смотрите,
ещё одна теорема на сегодня, последняя теорема, тоже Колмогорова, Колмогорова. Значит,
пусть кси от Т из Т большого, это случайный процесс, и пусть Т большое, это есть отрезок
от А до Б, конечный. Вот, конечный отрезок. Тогда, если существуют числа А больше нуля,
Б больше нуля, С, ну С меньше бесконечности, ну число и С меньше бесконечности, неважно,
такие, что для любых Т и Т плюс H, H больше либо равно нуля, вероятность... Сейчас, дайте,
я напишу, а потом вы напишите. Я сейчас не сразу вспомню. По-моему, по-моему, я не наврал.
А, подождите, А, Б плохо, у меня А и Б вот здесь. Да, видите, давайте я вот тут А и обозначу
Бетой. А, значит, там пусть будет С, здесь А, здесь Б. По-моему, по-моему, так. Да. А, ну, кстати говоря, тут
даже может быть отрицать, раз модули стоят. Это тоже давайте сотрем, неважно. Вот, то кси от Т имеет
непрерывную модификацию, то есть модификацию с почти всеми непрерывными траекториями. Вот. И уже как
следствие этой теоремы, можно создать другое вероятностное пространство и определить этот процесс,
переопределить процесс кси на нем так, чтобы у него уже все траектории были непрерывны. И тогда на практике
вы работаете уже, по сути, не с исходного пространства на плохом, на котором эта вероятность
может быть не определена, а на другом хорошем, на котором все траектории уже непрерывны. И там уже
все хорошо, и вы уже можете вычислять такие вероятности. Да. Не-не-не, смотрите, вот процессы надо
отличать от траекторий. У нас здесь почти всеми непрерывными траекториями, а не процесс. Траектория
это реализация процесса. Да, это конкретная кривая. Вот. Сам процесс это как бы совокупность всех
реализаций. Процесс совокупность реализаций. А реализация это какая-то одна кривая среди всего
этого множества. Почти всеми это означает, что почти все по исходам. Вот, если вы рассмотрите кси от
омега т, то что такое почти все непрерывные? Это значит, что есть некое множество омег,
меры единица. Вот. Что все омеги оттуда, для всех омег оттуда траектория непрерывна. Всюду по т.
Почти всюду по исходам, всюду по т. Вот. Знаете, вот когда случайные процессы, тут надо думать о двух
сразу вещах. Вот. Процесс это функция двух вещей. Исхода и времени. И поэтому вот все время приходится
оперировать вот этими двумя вещами. Когда у нас будут там понятия предела, производной, то мы
будем ко времени относиться как к параметру. Знаете, такие в школе были задачи с параметрами.
Когда у тебя что-то там при определенных значениях параметра. Вот здесь у нас как бы теория вероятности,
это как бы случайные процессы, это теория вероятности с параметром. Вот. В зависимости от параметра
что-то может быть. Поэтому-то надо быть аккуратным, что для одних значений параметра одно, для других
другое. Вот я так, к слову. Вообще вот такая теорема. Вот. А вообще такая теорема. Это полезная теорема,
мы будем ей потом пользоваться. Я ее тоже привожу без доказательства, потому что у нас будут такие процессы,
для которых мы захотим считать что-то вот такое, но про которое мы сходу не скажем, как бы существуют
эти вероятности или нет. Но мы сможем для них установить, что у них существует непрерывная
модификация. И тогда отсюда следует, что рассматривая непрерывную модификацию процесса,
мы можем вычислять такие вещи. Вероятно, существует, что все места конечномерных распределений
совпадают. А раз совпадает, значит вероятность это существует. Все, профит. Таким образом мы
избегаем вот этой вот сложной проблемы, связанной с несчетностью пересечения. Вот. Так. Ну что еще?
Давайте еще раз оглянемся назад. Что мы сегодня уже сделали? Мы сегодня подвели большой фундамент
подо всем. Первое, ввели определение случайного процесса сечения траектории. Второе, ввели
важнейшую характеристику семейства конечномерных распределений. Посмотрели свойства этого
семейства и выяснили, что их необходимы достаточно для того, чтобы они задавали какой-то
процесс. Так. Процессов, отвечающих одному и тому же семейству распределений, может быть много.
Поэтому нам понадобится понятие о модификации или о стахастической эквивалентности процессов.
Так. Это мы рассмотрели. И мы с вами выяснили, что вот в этих вот случаях, когда мы имеем дело
с случайными функциями непрерывным множеством времени, оказывается, что мы не всегда можем
вычислить какую-нибудь вероятность, потому что то, что стоит под вероятностью, не всегда события в
формальном смысле этого слова. Вот. С этим мы ничего в общем случае не можем поделать. Могут быть
ситуации, когда мы не знаем, реально не знаем, существует вероятность или нет, и мы не можем это
никак выяснить. Вот такое может быть. Но в некоторых случаях мы можем посчитать. Например, когда все
траектории непрерывны, и мы сводим утверждение о несчетном числе точек к утверждению насчетном,
всюдоплотном множестве точек. Для этого нам помогает вот эта теорема о существовании непрерывной
модификации. Если это выполнено, значит у процесса существуют непрерывные модификации, значит
вероятность от такого рода уже посчитать можно. Вот. И мы будем потом этим пользоваться. Вот. Я
понимаю, да, сегодняшняя лекция, она получилась очень абстрактная такая, да. Мы вспомнили множество
всяких, ну нам вернее потребовалось вспомнить множество фактов из теории вероятности. Видите,
тут какой язык, сигма-алгебра, вот эти все абстрактные. Но я хочу сказать, что дальше ничего этого не будет,
по большому счету. То есть мы не будем работать там сигма-алгебрами, заморачиваться ими. Мы будем
решать такие более насущные проблемы. Вот. А то, что я здесь рассказал, это как бы вот введение во всю эту
проблематику, что тут тоже не все так просто. Нельзя, знаете, вот просто так взять, ввести параметр t,
и у тебя, значит, все прекрасно, все считается, все вероятности, никаких проблем ни в чем ты не
испытываешь. Нет. Вот есть определенные проблемы. Это только верхушка асберга,
которая я показал здесь. Вот. Но все, что здесь есть, нам потом приходится решать конкретные задачи и
в дальнейшем выстраивать теорию. Вот. Как я уже сказал, мы не будем определять процессы как функции
исхода и времени. Это неудобно. Так же, как и неудобно, определять случайные величины как
функции исходов. Мы будем задавать процессы через семейство распределений. Вот. Через распределение.
В дальнейшем мы рассмотрим с вами, ну что мы будем дальше делать, пусть только мы фундамент навели.
Мы посмотрим, что такое математическое ожидание, дисперсия процесса. Новые для вас понятия
возникнет корреляционная функция, кавариационная функция процесса. Мы рассмотрим некоторые просто
сверх важные, важнейшие виды случайных процессов, которые проникают во все области деятельности. Это
пуассоновский процесс и виннеровский процесс. Гауссовские процессы. Потом у нас пойдут так
называемые стационарные процессы, оргодические процессы. Это все некие классы процессов с хорошими
свойствами, которые хорошо глубоко изучены, которые встречаются там в науке и технике повсюду и про
которые надо все хорошо знать, потому что они помогают понимать, что происходит и решать задачи.
Мы не будем с вами иметь дела никогда с какими-то совсем произвольными процессами. Мы будем о них
что-то подразумевать и получать из этих свойств, изначальных аксиоматических про эти процессы,
какие-то следствия далеко идущие. Вот такие модели, которые основаны на теории вероимости,
они позволяют эти далеко идущие следствия получать. Так что как бы это абстрактно здесь не выглядело,
это только на начальном этапе. А дальше будем уже заниматься более конкретными вещами.
Наверное, это все, что я хотел сегодня рассказать. Смотрите там мои ПДФки. В моих ПДФках иногда
может быть что-то немножко больше, чем я рассказываю. Так что имейте это в виду.
Я сегодня, может быть, только не коснулся вторичного вероятностного пространства.
Прочитайте в ПДФке, а пока просто на словах расскажу. Смотрите, случайные величины, случайный
процесс, мы их определяем на вероятностных пространствах. Вероятностное пространство это
тройка, ωfp, пространство исходов, сигма-алгебра, некая структура над множеством исходов и
вероятностная мера, которая определена на элементах сигма-алгебра. Это пространство может быть
абстрактным, ω. Это мог быть не только числа, это мог быть строки, слова, кошки, собаки, не знаю,
все что угодно. Омегой может быть. Но на практике используют не это абстрактное вероятностное
пространство, на котором определено, а другое, то, что называется вторичное вероятностное
пространство или выборочное вероятностное пространство. Вот есть у нас исходная ωfp и есть
вторичная множество значений случайной величины, например, R. Все R. Потом сигма-алгебра над R, это,
как правило, баррельская сигма-алгебра. И вероятностная мера на баррельской сигма-алгебре такая,
чтобы она соответствовала P из исходного вероятностного пространства. Вот. Для случайных
процессов тоже можно определить вторичное вероятностное пространство. В этом случае пространством
исходов будет являться множество функций. Для случайной величины это множество ее значений. А
что такое значение процесса? Это его реализация. Значит, вторичным вероятностным пространством для
случайного процесса служит тройка. Первая ω, уже другая ω, это множество функций. Вот это
наше выборочное пространство. Второе сигма-алгебра, уже построенная на множестве функций определенным
образом, как почитаете. Вот. И вероятностная мера, которая соответствует исходной вероятностной мере.
Вот. Тогда, смотрите, тут два подхода возникают к взгляду на случайные процессы. Если вы работаете
в терминах исходного пространства, вы зафиксировали какую-то ω, вы получили конкретную реализацию
функцию. Как си, ω от t. Омега ноль, запятая t. Кривая вам дана. В терминах вторичного пространства,
когда вы фиксируете точку в пространстве функций, вы по сути фиксируете функцию. Вот это и есть
реализация процесса. Так что, что там вы зафиксировали омегу ноль и получили кси от омега ноль t как
реализацию. А здесь вы ткнули сразу в пространство функций. И вот эта функция, в которую вы ткнули,
это и есть реализация процесса. Очень удобно. Вот, например, теорема Калмагорова, первая,
вот она доказывается как раз так. Там рассматривается вторичное вероятностное пространство множества
функций. Сигма алгебры на этом множестве функций. Вероятностная мера, все это определяется.
Ну, это вот просто к слову, что обобщения, они есть полезные. И еще у меня чуть-чуть
время остается. Случайные процессы, это не единственный способ обобщить случайные величины.
Ведь мы добавили один параметр, но никто не запрещает добавлять два параметра, три параметра,
много параметров, бесконечно множество параметров, счетное и несчетное множество параметров.
Понимаете? Если добавляется, допустим, несколько параметров не случайных, то это уже называется
случайными полями. У нас есть случайный процесс, то, что мы T интерпретируем как время, хотя оно вовсе не
обязано быть временем, в таком привычном понимании слова, это не обязано быть физическое время. Это
просто мы назвали T временем, а так мало ли что это может быть. А когда у вас много параметров,
ну какое же это время, если оно двумерное? Это поле, то есть на плоскости у вас загораются какие-то точки
случайно, в случайных местах. Это вот называется случайными полями. Мы в нашем курсе не будем
работать со случайными полями, но я просто говорю, что об общении, они тут самые разные есть и тоже
очень интересные. И про вот эти случайные поля чуть-чуть вы можете почитать вот этой книжке,
которую мы писали в соавторстве с Гастеньковым Александром. Вообще, эту книжку я вам советую
обязательно посмотреть. Сейчас, кстати, выходит переиздание ее в ОРСС, по-моему, называется,
издательство. Там мы много опечаток исправили, но вам, наверное, электронная версия уже без
запечаток должна быть доступна. И там есть теория. Моя теория, наверное, ближе к моему курсу,
чем та, которая там описана, хотя корреляция сильная. Но чем эта книжка особенно цена,
с моей точки зрения, то, что там описано множество приложений случайных процессов в реальности.
Например, там описана задача пейдж ранг гугловская, как гугл ранжирует страницы. Это мы будем проходить
во второй части нашего курса, когда будут марковские цепи пойдут. Там всякие процессы марковские
с непрерывным временем. В общем, несколько приложений там описано, и будет здорово,
если вы туда посмотрите. Они написаны на таком, где-то на научном языке, где-то на
научном популярном языке. На соавторов было много, у всех язык разный. Обязательно полистайте,
почитайте, так вы увидите, где вот это все реально прямо сейчас живет. И как все это применяется,
потому что тогда вы поймете, зачем все вот это вообще надо. Не просто там вы... Ведь кому нужны
просто определения, какие-то теоремы, ведь нужно же это все применять. И вот в этой книжке вы
найдете все эти применения во всей своей красе. Так что читайте, смотрите, интересуйтесь. У меня
все на сегодня.

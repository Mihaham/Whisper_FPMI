Итак, поехали.
В прошлый раз мы с вами говорили про multi-type access.
Мы взяли алгоритм консенсуса, который помогает Nuslam выбрать общее значение,
и с помощью него строили RSM.
Ну, точнее, мы реплицировали логи, которые помогали,
эти реплицированные логи помогали дальше уже репликам реплицировать состояние автомата.
Они читали эти логи и применяли все закомично зафиксированные надежные команды
к своему состоянию локальному, и отвечали клиенту, когда это случается.
Как наш алгоритм в прошлый раз был устроен?
Ну, вот мы взяли single decreed access, там были две фазы, prepare, accept.
С помощью фазы prepare каждый пропозер сначала захватывал внимание accept,
а с помощью фазы accept он дальше предлагал им какое-то значение.
Ну, месяц им был вот номер.
А дальше мы хотели все это уложить в слоты лога.
Ну, начали мы с такого наивного алгоритма, где каждая реплика, получая очередную команду,
выбирает первую свободную позицию в логе и становится пропозером для данного слота,
то есть в этом слоте запускает свой собственный экземпляр Paxos,
становится там пропозером.
Вот, так мы получили какую-то наивную реализацию реплицированного лога,
наивную реализацию RSM, а дальше мы сделали две важные оптимизации,
которые, собственно, и сделали то, что мы получили multipaxos.
Во-первых, мы заметили, что зачем репликам конкурировать в одних и тех же слотах.
Каждый из них является пропозером, но, а может быть, стоит выделить среди них какую-то одну,
которая будет являться выделенным пропозером или лидером,
и вот только эта реплика будет предлагать команды.
А дальше мы заметили следующее, что фаза prepare алгоритм Paxos кажется нужна на случай конкуренции,
когда пропозеры конкурируют друг с другом, а мы конкуренцию убрали.
Поэтому очень странно, что мы на быстром пути, точнее лидер на быстром пути,
постоянно проходит через первую фазу.
Он вроде бы один, он ни с кем не конкурирует, но чаще всего ни с кем не конкурирует,
если сеть работает стабильно, и при этом он все равно выполняет фазу prepare.
Ну и мы избавились от этой фазы, но не навсегда, конечно, она необходима алгоритму,
мы избавились от нее в сценарии, когда сеть работает стабильно, когда лидер у нас стабильный, не меняется.
Мы заметили, что в фазе prepare не участвует команда,
и мы выбрали для... мы виртуально проходили через фазу prepare
сразу для большого количества слотов, для всего суффикса.
Мы использовали там один и тот же n и говорили prepare, а acceptor смотрел на свой лог,
другие реплики смотрели на свои логи, и если они видели, что вот начиная с некоторого слота,
никаких больше команд в этих логах нет, никакие другие пропозеры не предлагают какие-то свои команды,
то acceptor давал обещание, отвечал промессом не на один слот, а сразу на весь суффикс.
И после этого выделенный этот пропозер, лидер, получая команды от клиентов,
сразу проходил через вторую фазу паксиса, через репликацию, минуя первую фазу.
Ну, конечно, этот конвейер мог сбиться, если вдруг появится другой лидер, такое могло бы быть.
Но в этом случае конвейер сбрасывается, видимо, лидер опять откатывается на первую фазу.
Но на быстром пути у нас только одна фаза остается, один раунд три.
Так мы получили довольно эффективный алгоритм.
Он гораздо эффективнее, конечно же, относительно нашей наивной реализации,
где был просто синглут крипакса в каждом слоте.
Но все же это еще не вся история, потому что вроде бы мы все понимаем,
и вы даже задавали вопросы после лекции, что в алгоритме осталось много белых пятен.
Вроде бы лидер должен предлагать, захватывать суффикс лога с помощью некоторого n.
А как выбрать это n, мы не говорили.
Понятно, что, видимо, у нового лидера должен быть какое-то больше n.
В результате должны быть связаны выборы лидера и выборы нового n, переход в новую эпоху.
Но мы не сказали, как именно.
Мы не сказали, как именно реплики понимают, что команды надежно зафиксированы в логе,
что они закомничаны.
Разумеется, это лидер понимает, если он получил команду,
прошел сразу через вторую фазу, ему ответили подтверждением,
хвором собрался.
И лидер, наверное, мог бы об этом говорить другим репликам.
Но с другой стороны, лидер может отказать, и реплики об этом не узнают.
То есть нужен какой-то протокол, который поможет им все-таки узнать,
что команды закомничаны, их можно применять к своему локальному состоянию.
Какие-то другие нюансы, у нас не было хорошего протокола выбора лидера.
Выбирали реплику с максимальным идентификатором,
может быть, у нее пустая логика, это не всегда будет эффективно.
В общем, очень многие вещи можно было бы потюнить,
или можно было бы дорассказать.
Интуитивно понятно, что, наверное, базовый алгоритм у нас есть,
корректность его доказана.
Так что нам ничего в принципе не угрожает, можно как-нибудь докрутить.
Но можно и ошибиться.
Ну и просто такая дополнительная когнитивная нагрузка на нас,
и на любого разработчика, который хочет этот multipax использовать.
Поэтому сегодня мы поговорим про альтернативный алгоритм,
который называется RAFT.
Ну и давайте перейдем на проектор.
Итак, статья про RAFT, она называется не RAFT,
она называется в поисках понятного алгоритма консенсуса,
и, видимо, она своим названием в чем-то упрекает алгоритм баксов.
Начинается все с такого заявления, что вот авторы алгоритма RAFT
придумали алгоритм, который позволяет реплицировать лог
и их алгоритм эквивалентен multipax в том смысле,
что он также реплицирует лог в асинхронной системе,
также сохраняет доступность при отказе менее чем...
сохраняет доступность, если жив кворум, большинство узлов,
также не теряет свою согласованность с сейфти свойства в асинхронной модели,
даже если количество отказов произвольное.
В общем, решает ту же самую задачу с теми же самыми свойствами.
Но в чем же достоинство этого алгоритма?
Вот авторы утверждают, что он лучше паксуса, если коротко совсем.
Ну и в чем-то они правы, потому что если посмотреть, что случилось после этой статьи,
эта статья была написана в 2013 году, вот после этой статьи более-менее каждая
Open Source база данных использует в качестве алгоритма консенсуса именно паксус.
Именно, простите, RAFT.
Так что им удалось добиться того, чего они хотели.
Они, видимо, убедили людей, что их алгоритм проще или лучше подходит для того,
чтобы реализовать RSM.
Ну а чтобы понять, почему так получилось, нужно, собственно, объяснить их аргументацию.
Они, конечно, говорят, что алгоритм паксус...
Они пишут, что это буквально синоним консенсуса был долгое время.
Действительно, но они все-таки утверждают, что алгоритм паксуса что-то не так.
И вот они про это целый параграф в статье пишут.
Что они говорят?
Во-первых, они говорят, что в алгоритме паксус просто выбрана неудачная декомпозиция.
В чем здесь смысл?
У вас есть задача репликации лога.
Вот есть этот самый лог, последовательность слотов.
Что делает Lamport?
Lamport этот самый лог берет и делит на слоты.
И внутри каждого слота решает независимую задачу консенсуса.
Вот разные слоты друг с другом никак не связаны.
Можно думать о них в изоляции.
Мы в прошлый раз так и начали строить multipaxis.
С одной стороны, вот так задача очень локализуется,
и сводится к задаче консенсуса.
Мы не думаем про всяких лидеров, про какие-то эпохи.
Мы решаем одну такую изолированную простую задачу.
С другой стороны, у Lamport получается решение single-decree-paxis,
которое хоть и короткое, то есть описывается в 20 строчек псевдокода.
Но с другой стороны, никакого самостоятельного смысла у этих фаз и у переменных в этом алгоритме нет.
В прошлый раз мы увидели, что N...
В прошлый раз, когда мы перешли от single-decree-paxis к multipaxis,
от отдельной задачи консенсуса к логам и репликам,
только тогда мы поняли, что N — это эпоха,
что сравнение NP и N — это блокировка старых лидеров или переход в новую эпоху за новым лидером,
что фаза prepare — это голосование за лидера, а accept — это репликация.
Но вот такого смысла внутри single-decree-paxis нет.
И ровно поэтому его трудно понять.
Он неинтуитивен просто-напросто.
Там есть две фазы, они крепко зацеплены друг за друга,
и у них нет понятного собственного смысла в контексте такой маленькой задачи.
RAFT делает по-другому.
RAFT делает, можно сказать, что разумнее.
Он декомпозирует задачу не горизонтально по слотам, а вертикально по фазам.
RAFT говорит, что в жизни вашего RSM есть периоды.
Вот есть период, когда система стабильна, и если один лидер, он реплицирует команду.
А потом лидер вдруг умирает, ну или, не знаю, какая-то турбулентная сети начинается,
и нужно выбирать нового лидера.
И вот у вас фазы со стабильным лидером и репликацией сменяются фазами,
где у вас выбирается новый лидер.
Происходит смена эпохи.
Потом снова новый лидер долго работает, потом он снова, не знаю, умирает или перезагружается,
или просто начинается асинхронность репартишн, убирается новый лидер и так далее.
И эти вот такие стадии, фазы, они рассматриваются поначалу в изоляции.
То есть авторы говорят, что их можно рассматривать отдельно.
Это не совсем правда, мы увидим сегодня.
Но все же, вот такой взгляд на задачу, он просто больше соответствует происходящему в реальности.
Это про декомпозицию.
То есть в принципе структура алгоритма будет такой же.
Ну и вообще у меня сегодня есть план убедить вас, что несмотря на заявления авторов,
что алгоритм проще, это тот же самый алгоритм.
Но тем не менее, вот тот же самый алгоритм структурирован иначе.
А второй аргумент против Паксуса и Zaraft у авторов состоит в том, что Паксус, по их мнению,
сейчас найду это место, что Паксус, по их мнению,
плохо подходит в качестве фундамента для построения распределенных систем.
Это довольно странное утверждение, потому что я показывал ему статью про Spanner,
ну и кажется, писал недавно в чат, кидал ссылку на Twitter,
где говорили, что сколько запросов в секунду Spanner обрабатывает.
Это самая большая база данных на свете, больше не бывает ни у кого нигде.
И этот Spanner использует Paxus для репликации.
Поэтому кажется, что подходит он все-таки для построения промышленных систем,
слегка нагруженных.
Но речь здесь не о том, что с помощью Paxus нельзя построить.
С помощью Paxus можно построить, замечательно можно построить, но неудобно.
Во-первых, у алгоритма Paxus нет какого-то канонического описания,
где были бы четко расписаны, какие там сообщения,
как обрабатывать эти сообщения, как на них реагировать,
в какое состояние реплик.
То есть есть статья Lampard про парламент, но она все-таки про греков,
про парламент, про дощечки, про песочные часы, про овец.
Все это неудобно.
И начал эту лекцию из того, что просто в алгоритме,
даже если мы придумываем все без греков,
все равно остаются какие-то свободные места
и нужно чем-то заполнять.
Это требует усилий разработчика,
это требует некоторой квалификации разработчика,
в смысле он должен понимать хорошо, что происходит,
что он может сделать, а что ему нельзя делать,
потому что он поломает сейфти, скажем.
В RAFT все не так.
В RAFT в статье есть просто страница одна,
на которую авторы уместили весь протокол.
То есть тут написано состояние каждой реплики,
причем даже разделено, что мы храним на диске,
что мы храним в памяти, что можно хранить в памяти.
Тут описаны роли возможные, кандидаты, фолловеры, лидеры,
но про это чуть позже.
Тут описаны два типа сообщений, которые отправляются,
и вот все поля в этих сообщениях.
В принципе думать не нужно,
то есть вам не нужно понимать, почему алгоритм корректен,
точнее как, это всегда полезно.
Но в то же время, если вы напишете аккуратно то,
что вот выписана эта страница,
то вы получите видимо реализацию,
то вы получите реплицированный лог.
Вам не нужно будет добавлять к нему какие-то,
заполнять какие-то темные места, потому что просто их нет.
Все описано.
Каждая реакция в каждом состоянии на каждое сообщение.
Сообщения описаны.
Вам не нужно придумывать их самостоятельно.
Это с одной стороны здорово, то есть это проще, понятно.
Это больше отвечает у как бы задачи,
которые инженером стоит, который пишет определенную систему.
Но с другой стороны, я бы сказал, что в алгоритме RAFT
все очень жестко подогнуло друг к другу,
и у вас очень мало свободы.
А в алгоритме Multipax с одной стороны очень много белых пятен,
а с другой стороны вы можете, ну какие-то вот эти,
у вас большое количество степеней свободы есть,
вы можете иметь реализацию выбора лидера,
вы можете там что-то еще настраивать,
и в итоге вы можете комбинировать разные идеи,
довольно свободно.
И если вы умеете доказать, почему это корректно,
то вы можете получить алгоритм, который будет эффективнее, чем RAFT.
Так что тут с одной стороны минус,
но с другой стороны плюс есть у Multipax, разумеется.
Но тем не менее, вот авторы говорят, что канонического описания нет,
а вот в RAFT есть эта страница.
Но мало того, что страница есть.
Они говорят, что у Multipax еще нет хороших опенсорс реализаций,
что, наверное, важно.
Потому что даже если вы пишете свою базу данных,
то вы не пишете свое собственное локальное хранилище,
вы берете LevelDB или RocksDB.
Но точно так же вы можете не брать консенсус,
в смысле не писать консенсус сами,
вы можете взять готовый консенсус,
готовую библиотеку, которая реализует репликацию лога.
И с Multipax самых хороших нет,
просто потому что Google свой код не выкладывал в опенсорс.
А авторы RAFT, они сразу вместе с статьей и вот вместе с этой страницей,
они написали, во-первых, PhD,
который все детали реализации промышленной системы обсудили
на основе Paxos и RSM.
То есть как делать exec nuance,
как устраивать переконфигурацию.
Такой целый учебник на 260 страниц.
Это тема второго нашего занятия сегодня.
И плюс к этому автор написал просто референсную реализацию
алгоритма RAFT.
Реализация даже не RAFT, а RSM поверх RAFT.
То есть если вдруг вы читаете статью на английском,
и вы чего-то не понимаете в ней, какая-то бессмысленность остается,
если вам чего-то не хватает вот на этой страничке,
то вы просто идете открываете код на C++, кстати.
Ну и его там читаете.
И у вас уже не остается никаких сомнений о том, что происходит.
Вот, пожалуйста, у вас есть два типа сообщений в RAFT.
Append Entries для репликации и RequestWall для выбора лидера.
И вот открываете репозиторий LookAbin
и смотрите обработчик Append Entries.
Вот как реплика реагирует на это сообщение.
Пожалуйста, читайте код,
и никаких сомнений у вас уже не остается в том, что происходит.
Довольно мелко.
Можно разглядеть что-то.
Вот у вас есть request, вот у вас есть response.
Читаете request, заполните response.
Ну и да, тут можно посмотреть на протокол.
Вот протоописание всех сообщений опять.
Можно посмотреть, что именно RAFT с каждым сообщением отправляет.
Вот все поля, которые есть в описании статьи, они тут будут.
Ну, может быть, будет еще что-то, если какую-то юристику можно дополнительно придумать.
Замечательно. Есть референсная реализация.
Кроме того, у RAFT есть собственный сайт, где есть список промышленных реализаций RAFT.
Ну, точнее, список всех реализаций RAFT open-source.
Но вот наверху, в топе, где больше всего звездочек,
это реализации, которые используются в промышленных системах.
TCD – это Key Value хранилища сервисной переконфигурации.
TKW – это Key Value хранилища.
Ну, в общем, разные промышленные системы используют для себя RAFT.
Написано на разных языках.
И вот, пожалуйста, есть ссылки на репозитории.
И есть описание, какой функциональности они обладают.
Ну, видимо, популярные умеют все, что угодно.
Вот, опять же, если вы пишете, не знаю, на ГО, приходите и используете вот эту реализацию.
Рокс DB – это локальное хранилище.
Ему не нужен консенсус.
Ну, то есть это более низкий уровень архитектуры,
который отвечает за хранение данных на одной машине.
А дальше, если ты хочешь, чтобы эти данные хранились не только на одной машине,
на трех машинах, то ты должен поверхне сделать консенсус.
И вот уже консенсус делаешь с помощью RAFT.
Ну, или с помощью мультипаксиса.
То есть это такие дополняющие друг друга части, отдельные слои архитектуры.
Вот, ну, да, я говорю о том, что и то, и другое можно не писать своими руками.
Каждый раз можно взять реализацию Open Source RAFT, реализацию Open Source,
реализацию библиотека у Рокс DB и вот скомбинировать из них свое решение.
Ну и вообще, помимо того, что я сказал, есть полное описание в статье,
есть референсная реализация, есть Open Source реализация.
Еще одна сильная сторона RAFT в том, что авторы постарались сделать его понятным.
Ну, то есть это было их дополнительное усилие,
помимо того, что они придумали более разумный алгоритм консенсуса,
они более подходящие для промышленной реализации, они постарались еще доступно донести его.
Вот они сделали слайды, которые мы как раз будем смотреть сейчас.
Они прочитали про него лекцию, они сделали слайд с анимацией.
Ну, короче, они сделали все, чтобы вы полюбили RAFT.
Ну, а Лэмпорт написал про Грегов смешную статью.
Я не знаю, что вам ближе, но если перед вами задача через 3 месяца запуститься в продакшн или через год,
то, наверное, понятно, что вы выберете.
Ну что, вот так авторы аргументируют, что RAFT лучше, чем Paxos.
RAFT больше подходит вам как инженеру, чем Paxos.
Ну и, наверное, можно послушать теперь про сам алгоритм, как он устроен.
Для этого мы будем смотреть слайды AfterFRAFT.
Ну что, поехали. RAFT решает задачу репликации лога.
Как обычно, у нас есть клиенты, у нас есть, допустим, 3 реплики.
Каждый из них хранит копию лога и хранит свою копию автомата.
Когда клиент посылает команду какой-то реплики, она видимо лидеру об этом чуть позже,
он помещает ее в какой-то слот и реплицирует с помощью некоторого модуля консенсуса,
это такая очень условная картинка, больше про RSM, чем про RAFT, в логе других реплик.
Ну и после того, как реплика зафиксировала команду в своем логе и зафиксировала все команды в логе перед ней,
она может применить ее к своей копии автомата.
Вот здесь все, в общем, как обычно.
Да, мы работаем в модели с отказами или стартами узлов, сообщения в сети могут задерживаться или теряться,
но мы не допускаем византийских отказов, то есть мы не ожидаем, что реплика может потерять свой диск
и мы не ожидаем, что реплика может нарушать протокол, разумеется.
И мы должны обслуживать клиентов, когда доступно большинство узлов в кластере.
RAFT использует подход с выбором лидера.
Вот в прошлый раз мы отталкивались от мультипакс наивного, где у нас лидера не было, где все реплики конкурировали.
В RAFT лидер выбирается, и более того, RAFT без лидера не работает.
Для того, чтобы реплицировать команду, для того, чтобы обслуживать клиентов, RAFT-у необходим лидер.
Ну и я уже сказал, что RAFT декомпозирует стадии, где лидер выбирается, потому что предыдущий умер,
и стадию, где лидер стабильный, он просто реплицирует команды.
Вот мы рассматриваем эти случаи в изоляции.
По крайней мере, начнем.
Итак, наш план. Сначала мы обсудим, как выбирать лидера, потом мы обсудим, как работает репликация,
потом мы докажем, что все корректно.
Во-первых, в RAFT, как и в алгоритме мультипакс, как и во всех наших алгоритмах, есть роли.
Есть роль лидера, который реплицирует, который получает запросы клиентов,
выкладывает их в лог и реплицирует их.
Есть роль фолловера, который слушает лидера, принимает от него команды и подтверждает,
что он их надежно сохранил у себя.
И есть кандидат. Это такое промежуточное состояние.
При переходе от, когда фолловер понимает, что старый лидер умер,
он становится кандидатом и, возможно, избирается новым лидером после этого.
Ну и вот тут есть автомат переходов между этими ролями. Его мы разберем чуть позже.
Сейчас важно отметить, что эти роли, в отличие от ролей алгоритмы мультипакса,
совзаимо исключающие. То есть каждая реплика или лидер, или фолловер, или кандидат.
Можно сказать, что это еще одно преимущество RAF, потому что об этом думать проще.
В мультипаксасе у нас реплика и acceptor, и proposer совмещает в себе разные поведения,
а вот в RAF-те поведение одно, и мы фиксируем его в переменной состоянии реплики.
Каждая реплика проходит, время для нее делится на термы,
пронумерованные подряд натуральными числами.
Каждый терм состоит из двух фаз. В каждом терме сначала выбирается лидер,
а потом этот лидер реплицирует команды. Если лидер умирает, то, видимо,
начинается новый терм, в нем выбирается новый лидер,
и дальше он реплицирует команду, пока с ним что-то не случится.
RAF гарантирует, что в каждом терме может быть выбран не более одного лидера,
но бывают термы, в которых лидер вообще не выбран.
Здесь третий терм, в нем лидера вообще не будет.
Эта картинка у каждой реплики своя.
То есть, скажем, реплика могла надолго заснуть и какие-то термы вообще пропустить,
ничего о них не знать. Или она просто лежала выключенная, потом поднялась и узнала,
что сейчас уже какой-то сотый терм идет. Это нормально.
Ну и границы термов могут немного совпадать, потому что у каждой реплики
свое локальное знание. В начале каждого терма выбирается лидер,
потом он реплицирует команды. Во всем протоколе используется всего два сообщения.
На фазе выбора лидера используется сообщение, оно называется request-vote.
На фазе репликации используется сообщение append-endress.
Всего лишь два сообщения. Request-vote отправляет кандидат, когда он хочет выбраться лидером.
Append-endress отправляет лидер-фолловером, когда он реплицирует команды.
И как обычно, нужно каким-то образом понимать, что есть лидеры из старых эпох и из новых эпох.
Точно так же, как в мультипаксисе. В мультипаксисе у нас реплика, получая команду от лидера,
от другой реплики смотрела на n этой команды и сравнивала с своим n.
В RAFT происходит та же самая история. Каждая реплика помнит, в каком терме она находится.
И каждый лидер, когда он реплицирует команды с помощью сообщения append-endress,
перекладывает к этому сообщению свой терм.
Если оказывается, что терм лидера меньше, чем терм узла, который получает от него сообщение,
то этот узел считает, что лидер уже устарел, услышать его не надо. Он из прошлого терма, из прошлой эпохи.
Терм здесь это эквивалентная эпоха в мультипаксисе.
Этот терм, конечно же, хранится надежно на узле. То есть узел запоминает его на жесткий диск,
если он перезагрузится, то должен его вспомнить.
Ну точно так же, как у нас accept-multipaxis надежно запоминает n и отданный голос.
Ну по крайней мере, наш терм будет не ниже, чем коронтерм, который мы помнили.
Тут речь не о том, что мы не знаем. Мы, конечно, можем пропустить какие-то термы.
Но мы не окажемся в терме из прошлого, который мы уже точно прошли.
Что? Пока это не протокол, пока это просто идея.
Роли мы обсудили, то, что время делится на термы, на эпохи мы обсудили.
И теперь давайте поговорим в сторону выбора лидера.
Лидер выбирается в начале терма и реплицирует команды.
И реплики должны его слушать и как-то понимать, что он все еще жив.
Для этого каждый лидер посылает фоллверам, репликам, которые его слушают, специальные сообщения, хардбиты.
На самом деле, здесь нет выделенного сообщения хардбит.
В RAFT все так потюнино, что сообщений минимум, типа сообщений минимум.
Для хардбитов используется append-entries.
Если у лидера есть команды, то реплика получает append-entries и понимает, что лидер жив.
Если у лидера нет команд, то он посылает просто пустой append-entries, говорит, что я просто жив, но команды у меня пока нет.
Каждая реплика, каждый фоллвер заводит себе election-timeout.
Заводит таймер величиной election-timeout.
И если за вот этот timeout от лидера не приходит ни одного хардбита, ни одного append-entries, пустого лидера с командами,
то реплика фоллвер справедливо считает, что, видимо, лидер умер и нужно выбрать нового.
И вот мы переходим к процедуре выбора лидера.
Для начала реплика, у которой истек этот самый таймер, увеличивает себе терм и становится кандидатом.
Ее план – стать лидером.
Для этого она посылает сообщение request-vote всем остальным репликам.
Говорит, проголосуйте за меня.
Ее цель – собрать большинство голосов, собрать quorum.
Вот она посылает на все другие реплики сообщение request-vote и ждет.
Что случится раньше?
Может быть, большинство реплик проголосуют за нее.
По какому принципу они будут голосовать?
Они будут голосовать пока что за первого кандидата.
Вот если вы фоллвер в терме 3, и вам приходит сообщение от кандидата в терме 4,
потому что он уже перешел в новый терм, потому что он считает, что лидер старый умер,
то вы смотрите, голосовали ли вы за кого-то.
Видимо, у вас еще третий терм, у вас уже просто в четвертом проголосовать,
вы еще ни за кого не голосовали, вы отдаете голос за этого кандидата и запоминаете его.
И больше в терме 4 не голосуете.
Так что если вы кандидат, вы либо получите большинство quorum,
вернее, таких вот подтверждений на свой request-vote,
либо у вас исечет таймер, потому что вы, когда стали кандидатом,
снова завели этот таймер на election-timeout,
либо выберут вообще не вас.
Как вы узнаете, что выбрали не вас?
Ну, можно отказы считать, а можно смотреть на сообщения, которые вам приходят.
Если вы перешли в четвертый терм и стали там кандидатом,
отправили request-vote, а потом получили от кого-то append-entries в терме 4,
то это означает, что лидера выбрали, но это не вы, к сожалению.
Вы просто знаете, что только лидер может отправлять append-entries,
поэтому если вы его получили в терме 4, то значит, что лидер выбран просто это не вы.
Так что вы либо получаете большинство подтверждений,
либо вы получаете append-entries от нового лидера,
либо у вас истекает таймер election-timeout,
и вы снова инкриментируете счетчик терма,
переходите в новый терм и снова становитесь кандидатом.
Понятная идея.
Вроде бы очень тривиально все.
И вот из такого протокола следует очевидно, что в каждом терме
не может быть более одного лидера.
Просто потому, что в каждом терме каждая реплика голосует не более одного раза,
так что невозможно собрать два разных quorum'а за разных кандидатов.
В пересечении будет реплика, которая обязана проголосовать дважды,
и это запрещается делать.
Но внимание, это не означает, что у вас не может быть двух лидеров в кластере.
У вас не может быть двух лидеров в одном терме, а в разных могут.
Ну и вот такой процедурой есть недостатки.
Вот один из них большой и маленький, простой и сложный.
Вот простой недостаток в том, что откуда возьмется LiveNas в таком решении.
Представьте, что у вас есть реплики, ну и вообще в RAFT'е кластер начинает жить в состоянии follower.
То есть каждая реплика является follower.
Они все заводят себе тайм-аут, заводят один и тот же таймер,
он в одно и то же время истекает на всех репликах.
Они все становятся кандидатами и пытаются голосовать.
И в итоге, я не знаю, каждый голосует за себя.
Или у вас 5 реплик, и первые две проголосовали за одного кандидата,
третий проголосовал за второго, четвертый и пятый проголосовал за третьего.
Никто большинства не набрал.
В итоге у всех истек таймер election-тайм-аута, они все перешли в новую эпоху
и пытаются друг за друга поголосовать, ну за себя в первую очередь.
Никакого прогресса не будет, какой-то lifelock получается бесполезный.
Поэтому, чтобы как-то, чтобы добиться прогресса, мы хотим рандомизировать тайм-ауты,
которые заводит реплика на случай, когда хорбиты заканчиваются.
Рандомизация помогает по очень простой причине.
Какая-то реплика, на какой-то реплике тайм-аут протухнет раньше, чем на других,
и она первой станет кандидатом, и в одиночестве запросит голоса у других.
И, видимо, выиграет. Понятна идея?
Голосы дают реплики.
Смотри.
Во-первых, если ты, любая реплика, и ты получаешь сообщение, любое,
ты сравниваешь терм свой локальный и терм сообщения.
Если он меньше, ты его просто игнорируешь. Он не актуален для тебя.
А если он больше, то ты голосуешь или нет.
Потому что тебя переводит в будущую эпоху, про которую ты раньше не знал.
Про это все на стадии RAF есть анимация. Можно посмотреть на анимацию.
Смотрите. Что здесь происходит? Здесь есть лидер, S1,
и он периодически рассылает append-endress пустые, то есть хорбиты,
напоминая репликам, что он все еще жив, что он все еще лидер.
Давайте замедлим время.
Видите, у каждой реплики такой бар ползет вниз.
Это reaction-timeout. Он истекает, истекает, истекает.
Если он истечет, а реплика не получит хорбита, то она решит, что лидер умер.
И когда реплика получает хорбит, ну вот ждем, смотрим на это,
вот хорбит летит, бабах, и таймер сбросился.
Ну вот полный круг, это 2t, 2 reaction-timeout.
Но когда мы получаем хорбит, мы сбрасываем на какое-то,
на случайное значение в диапазоне от t до 2t.
Ну а теперь, видимо, остановим лидера.
И ускорим время. Кто должен выиграть?
S3, видимо, должен выиграть.
За счет рандомизации тайм-аутов этот S3 станет первым кандидатом.
И вот он отправляет request-vote.
Он перешел в терм 3, отправил request-vote, и сейчас собирается получить кворум подтверждения, видимо.
Да, ну потому что такова процедура. Вот мы говорили об этом сейчас.
Вот, если истекает reaction-timeout, если мы не получаем долгое время хорбитов,
то мы увеличиваем терм и становимся кандидатом в новом терме.
Ну и вот мы получили большинство подтверждений, и теперь мы новый лидер в новом терме.
Они перешли в новый терм, потому что они получили сообщение с термом 3.
Ну и они стали фолловером, потому что теперь у нас есть новый лидер.
Ну вот такой простой протокол выбора лидера.
Про safety все очевидно, про liveness тоже понятно, что мы рандомизируем timeout,
что это помогает нам избежать конкуренции среди кандидат.
Ну а дальше лидер, если он выбран, начинает реплицировать лог.
Что такое лог? Ну опять, это последовательство слотов.
В каждом слоте лежит команда и терм, в котором ее туда положили.
Судя по этой картинке, у нас есть 3 терма, лидер прямо сейчас в терме 3 находится,
а команда red попала в лог в терме 1. Ее туда положил лидер из терма 1.
Ну и как обычно, у нас логи есть, но не факт, что все команды этого лога можно применить,
потому что не все из них надежно зафиксированы.
Так что у каждого лога есть некоторый языкомичный префикс,
у каждой реплики языкомичный префикс, про который реплика знает,
что его можно безопасно применять.
RAFT пытается и здесь быть проще мультипаксиса.
Ну вот он был проще, потому что, во-первых, мы в процедуре выбора лидера
связали n и лидеров. Теперь лидеры и эпохи связаны друг с другом
вот этими, через переменную карантеру. Мы упростили протокол репликации за счет того,
что теперь реплика бывает только в одном состоянии, она играет ровно одну роль всегда.
И дальше RAFT упрощает задачу репликации лога еще и тем, что состояние лога меньше,
чем в мультипаксисе. В мультипаксисе мы видели, что в каждом слоте написано
предложение аксептора изрированного инстанца паксиса. Там написано nA, va, nP.
И вот это va может меняться, потому что аксептор может голосовать сначала за одно,
предложение потом за другое, и команда может меняться в слоте.
Так что логи, вообще говоря, могут быть совершенно разные, и в них еще и дырки могут быть.
В RAFT все проще, потому что, во-первых, дырок не бывает.
Все операции с логом, они меняют его суффикс, либо добавляют в конец лога что-то,
либо стирают из конца лога что-то. Делать что-то с середины лога нельзя.
RAFT так не делает. И есть очень простое свойство, согласно которому лидер
никогда из своего лога ничего не стирает. Он только в него добавляет.
То есть вы стали лидером, вы получаете команды, вы добавляете их в лог,
и вы их реплицируете. Что значит реплицировать? Смотрите по-другому.
Логи на разных репликах, конечно, могут расходиться, потому что лидеры меняются,
они реплицируют какие-то команды в разном порядке, но RAFT поддерживает такой простой вариант,
простой и очень мощный. Возьмем две произвольные реплики, возьмем их логи,
посмотрим в некоторые слоты на этих двух логах. Например, слот 4.
Если в этом слоте лежит запись с одним и тем же термом, например, 2,
то это означает, что во-первых, и команда в этом слоте совпадает на двух репликах,
а во-вторых, что совпадают и префикс этих логов целиком до данного слота включительного.
Понятно? Вот ровно поэтому дальше в слайдах про RAFT нигде не пишут команды.
Если у вас есть третий слот в двух логах, и там написана одна и та же цифра, одно и то же число,
один и тот же терм, то команды совпадают в этих слотах и совпадают оба префикса.
Как мы гарантируем такое свойство? Индуктивной проверкой простой.
Вы лидер, вы получили от клиента команду jump и положили ее в пятый слот.
И хотите реплицировать ее на другой лог, на другую реплику.
Вы посылаете сообщение append-entries для этого.
Прикладываете к нему свой терм, чтобы реплика могла понять, нужно вас слушать или нет.
Но вместе с этим вы еще хотите убедиться, что префиксы ваших логов совпадают.
Предполагаем по индукции, что это свойство уже имеет место, и нам нужно просто его продлить дальше.
А вместе с append-entries и с командой запись, которую мы положили в float5, нужно отправить еще позицию,
от которой мы добавляем эту команду, и номер терма в этой позиции в логе лидера, в нашем логе.
То есть мы говорим реплике фоллуверу, что мы хотим записать команду jump после индекса 4,
и у нас в логе в индексе 4 находится команда с термом 2.
Что делает фоллувер, который получает такой append-entries?
Он берет свой четвертый слот и сравнивает содержимое терма в своей записи.
Если он видит, что терма совпадает, то он команду jump себе добавляет.
Если логи расходятся в предыдущей позиции, то, видимо, команду добавлять нельзя, потому что мы нарушим это свойство.
Что делать?
Ну, видимо, нужно чужие логи чинить.
Вот мы лидер в терме 7, мы хотим положить команду в слот 11.
Мы отправляем другим фоллуверам вместе с этой командой информацию, что у нас последний слот это 10,
и в нем лежит команда из терма 6.
Ну, если у фоллувера совпадает все, то он просто добавляет новую запись.
А если не совпадает, то по двум причинам не совпадает.
Либо этот слот у фоллувера вообще пустой, либо он отличается.
Ну, если пустой, то нужно просто откатиться на какую-то позицию раньше.
Видимо, реплика отстает, и на нее нужно дореплицировать наш лог.
Если же содержимое расходится, то есть здесь лежит запись с другим термом,
то мы должны лог у этой реплики стереть.
Вот стереть до тех пор, пока префикс не начнет совпадать.
Видимо, до позиции 3.
Ну, и тут просится какие-то простые оптимизации.
Скажем, если вы отправили append-entries на реплику с позиции 10,
а на реплике лог короткий, то что нам должно ответить?
Во-первых, что не получилось, а во-вторых, что у меня лог сейчас длины 4,
чтобы лидер мог откатиться сразу на эту позицию.
А что делать в реплике, у которой лог разошелся с лидером?
А что он отвечает?
Ну, она говорит, разошлось.
Что вы делаете на месте лидера?
Ну, сначала откатываетесь на одну позицию назад.
Пробуйте еще раз. Не сошлось снова.
Откатываетесь на две позиции назад. На 4, на 8 и так далее.
Ну, в общем, тут нужно как-то чуть быстрее откатываться, чем по одной позиции.
По одной будет не эффективно, конечно.
Стираем логи. Ну, в смысле, мы говорим, мы лидер,
мы должны реплицировать свой лог на каждого фоллувера.
Вот ты для каждого фоллувера берешь и пытаешься в него писать свой лог.
А он тебе говорит, не сходится.
Ну, значит, ты пробуешь писать в него свой лог с позиции раньше.
Вот. Когда я говорю, что лидер стирает лог на фоллувере, я говорю,
что вот если здесь не сошлось, например, в восьмой позиции,
то на фоллувере дропается весь суффикс этого лога с восьмой позиции.
В смысле, он посылает с каждым сообщением свой терм.
Если фоллувер этот терм устраивает, значит, все нормально.
Да вообще может по-разному случаться.
Вот тут изображена какая-то картинка, как образовались какие-то логи.
Какое-то странное исполнение, в нем логи получились такими.
Я сейчас к ней перейду, но мне важно еще что сказать.
Смотрите, лидер всегда верит в свой лог, никогда в нем не сомневается
и стирает логи других. Поэтому лидера нужно выбирать аккуратно.
В мультипаксисе можно было не аккуратно выбирать лидера,
потому что, помните, мы выбирали лидера с пустым логом,
и тем не менее он через первую фазу Prepr через Promises узнавал про закоммичные команды.
Здесь лидера так не делает. Он получает лог и приводит все остальные логи к своему виду.
Поэтому если мы выберем лидера с пустым логом, например,
то он просто сотрет все, что было записано на репликах,
и кажется наш RSM пострадает от этого.
Поэтому процедура голосования, которая описана в начале слайдов, неправильная, конечно.
По крайней мере, я перестаю авторам Рафта верить, что у них все просто и декомпозированно.
Вот они говорят, у них есть фаза выбора лидера и фаза репликации,
но при этом фаза выбора лидера сначала неправильно описывается, а потом мы ее чиним,
потому что в таком виде она не годится.
Она не годилась, потому что фолловеры голосовали за первого кандидата, которого они видели.
Так нельзя, конечно. Они должны голосовать тогда, когда у нового кандидата какой-то разумный лог.
А что значит разумный? Давайте подумаем.
Вот за какого кандидата вы стали бы голосовать? На что бы вы смотрели?
Ну, что значит на лог? На что именно в логе ты бы смотрел?
На размер лога смотреть не стоит, потому что у вас может быть лидер в маленьком терме,
на которого пришло 150 команд, положил их к себе в лог, а потом с ним что-то случилось,
эпоха поменялась, что там произошло полезно, а потом вернулись вы с этим длинным логом,
и в итоге почему-то у вас все должны верить. Нет, нужно выбирать лидера, который много повидал,
у которого последняя запись в логе с большим термом, который прошел через много эпох.
Так что правила голосования, сейчас мы их найдем, выглядят следующим образом.
Мы голосуем за кандидата, если, во-первых, в текущем терме мы еще не голосовали,
а во-вторых, если у него последний терм в логе больше нашего, или он равен, но лог длиннее.
Вот так мы тюнем правила голосования.
Автор RAF так говорит, что алгоритм Paxos не очевиден, мне кажется, что алгоритм RAF тоже не очевиден,
потому что, ну я не знаю, почему бы он был очевиден, почему должно быть очевидно,
что мы выбирая лидера таким образом стирать чужие логи, никогда ничего не поломаем.
Но к этому мы сейчас вернемся еще, а пока посмотрим на картинку, вот какая-то картинка,
давайте подумаем, как такое могло получиться вообще.
Сначала лидером в первом терме был кто-то, не важно кто, ну, допустим, S1.
Во втором терме лидером стал кто?
S5, он положил в себе в лог две команды, одну из них успел реплицировать на S4,
а потом что-то случилось.
Начался третий терм и лидером стал 100 у S5, он положил в себе еще три команды в лог,
не успел ничего сделать, перезагрузился и лидером стал S4.
Почему? Ну, вообще-то у него терм последний меньше, чем у S5,
но с другой стороны, он мог собрать хвором из первых трех, первых четырех реплик.
То есть, пятый он проигрывает в этом правиле голосования,
но с другой стороны, хвором у него собирается.
Поэтому S4 становится лидером в четвертом терме, кладет в свой лог одну команду от клиента,
после этого снова перезагружается, наступает пятый терм, в нем лидером выбирается, видимо, S3.
S3 кладет две команды себе в лог, одну из них пишет на первые три реплики.
Да, вот почему выбрали лидером S3?
Хотя у него лог заканчивается на единицу, а здесь S4, а здесь С3.
Ну опять, потому что собирается вот такой вот хвором из первых трех.
Мы кладем сюда три команды эти, перезагружаемся, лидером выбирается S1,
потому что она набирает хвором, она не может собрать в хвороме S3, например,
потому что S3 лог длиннее. Последний терм такой же, а лог длиннее.
Но она может собрать хвором из S1, S2 и, скажем, S4 и S5 тоже туда могут войти.
Пишем сюда шестерки, опять что-то случается, ну и потом лидером в седьмом терме становится S2.
В общем, понятно, что логи как-то странно расходятся.
Ну и что? А что остается? Ну расходятся и расходятся.
При этом, вроде бы, неприятно, конечно, но вроде бы они не в паксис, они всегда сходились.
Что нам остается сделать? Вот мы вроде бы проговорили, как работает процедура выбора лидера.
Мы заводим election timer, ждем heartbeats, append entries. Если мы их не получаем и таймер протухает,
то мы переходим в новую эпоху, становимся кандидатом, отправляемся им в request-votes,
в которых указываем свой терм, свою последнюю запись в логе, терм последней записи в логе и длину лога.
И если мы собираем большинство голосов, то мы становимся лидером.
А дальше мы введем свой лог, реплицируем его и сводим логи всех фолверов к своему
с помощью такой индуктивной проверки, сравнивая последнюю предшествующую запись.
В принципе, это весь RAFT.
Но что остается-то, чтобы он стал нам полезен? То есть правила репликации я рассказал.
Вот они.
Вот так мы логи реплицируем, стираем и добавляем в них, и вот так мы выбираем голосуем за лидера.
Это есть протокол репликации. Что нам не хватает?
Что?
Ну да, мы не поговорили о том, когда команду можно применять из лога.
Когда мы считаем, что она надежно зафиксирована.
Вот давайте я у вас спрошу, глядя на эту картинку. Можно ли считать, что команда 5 надежно зафиксирована или нет?
Ну или давайте просто придумаем какое-то определение, какая запись является закоммиченной.
Сейчас, делать даже не в том, как все узнают о том, что она закоммично зафиксирована, а про то, какую запись мы считаем зафиксированной.
Ну то есть, если у нас в одном и том же слоте на Quorum лежит одна и та же команда, то она является надежно зафиксированной и не сотрется уже. Мы в это верим.
Вот опыт Paxos вас ничему не научил. Там я тоже спрашивал, верно ли, что если у нас значение лежит на Quorum acceptors, то оно будет выбрано.
Вот в Paxos это было не так. Я как-то утверждал, что RAF то тот же самый multipaxos, хоть и называется иначе.
Ну, я потому так и говорю, потому что там сценарий такой же есть проблемный. Смотрите.
Смотрим в статью. Это не статья. Спасибо.
Итак, у нас есть 5 реплик. Сначала лидером выбирается, ну не знаю, кто-то в первом терме, неважно.
Потом во втором терме лидером выбирается S1, пишет команду 2 на себя и на S2.
Потом что-то случается и лидером в третьем терме выбирается S5, потому что собирает вот такой Quorum.
S1 и S2 не проголосуют за S5, а вот эти три проголосуют. S5 стала лидером, получила команду от клиента,
положила ее в свой лог в терме 3 и перезагрузилась. Наступил четвертый терм, в нем лидером был выбран S1,
потому что он собрал вот такой вот Quorum, допустим. И он получил команду от клиента,
какую-то розовую, положил ее в свой лог и хочет ее реплицировать. Он пытается реплицировать ее на реплику S3,
и у него не получается, потому что он посылает ей информацию, что у него предшествующий слот был равен 2,
и в нем была команда из терма 2, а у реплики S3 там пусто, поэтому она откатывается и дореплицирует туда 2.
И вот желтая команда лежит на Quorum, на большинстве реплик. Ну а дальше эпоха снова сменяется,
почему-то наступают новые выборы в новом терме, и лидером в терме 5 уже становится реплика S5.
Почему бы и нет? За нее могли проголосовать S4, ну сама S5, разумеется, S4, ну и S3 тоже мог проголосовать,
потому что у реплики старший терм 3, а здесь старший терм 2. S5 становится лидером в терме 5,
ну и благополучно намазывает свой лог на остальных, стирая все, что там было написано.
Ну а могло иначе случиться. Могло бы случиться так, что в пятом терме вместо того, чтобы лидером было выбрано S5,
лидером мог бы стать снова S1, и тогда он бы дореплицировал там свою команду сверху еще.
Вру, конечно, альтернативная картинка, она не про S5, она про S4, ой, про терм 4.
Итак, у нас был терм 4, реплика S1, лидер, намазала на реплику S3 запись желтую из слота 2,
а дальше либо терм сменился и реплика стала S5, а она стерла эту запись,
либо S1 продолжила в четвертом терме и дореплицировала вот эту запись.
Вот две такие альтернативные реальности, альтернативные ветки истории.
И вот заметим, что здесь S5 могла стать лидером, а здесь уже не могла.
Чем-то эти две картинки друг от друга сильно отличаются.
Ну, картинки отличаются, разумеется, еще бы, они разные.
Речь о том, что, видимо, в одном случае, здесь мы точно не можем считать,
что желтая команда закомично зафиксирована, она сотрется.
А вот здесь почему-то желтую команду уже довольно сложно стереть,
потому что лидером кто здесь может стать?
Либо S1, либо S2, либо S3.
А по свойству RAF-та лидер никогда не стирает свой лог.
Ну что, если у вас какая-то интуиция, когда же команда все-таки зафиксирована в логе надежно?
Когда мы можем так сказать?
Это максимальный.
Дважды стал лидером.
Но я боюсь, что я для такого могу продеть новую картинку, где просто больше шагов будет.
Вот смотрите, здесь четверка придавила команду 2, и вот все, ее теперь уже не скинуть с Quorum.
В RAF-те говорится следующее.
Давайте найдем слайд.
Мы скажем, что если лидер в термика получил от клиента команду и положил ее на Quorum,
то она закоммичена.
А поскольку, если команда закоммичена, если лидер в термика положил ее на Quorum,
получил ее от клиента и положил на Quorum,
то в этой картинке определение коммита не работает, потому что здесь лидер в терме 4
положил команду из терма 2 на Quorum, дописал ее на Quorum.
А я говорю, что лидер должен получить команду в терме 4 и положить ее в терме 4.
В RAF-те эта команда является закоммиченной.
А по свойству согласованности логов, то есть она закоммичена, в смысле ее, видимо, нельзя стереть оттуда уже.
А по свойству согласованности логов RAF-та, если у нас на Quorum совпадает команда в одном слоте,
то совпадают и префиксы. Так что и весь префикс можно считать закоммиченным.
Так что отдельная команда закоммичена, если она написана на Quorum лидером текущего терма,
если она добавлена в текущем терме в лог и записана на Quorum текущем терме,
либо ее сверху придавливает такая команда, то есть либо команда 4 закоммичена,
команда розовая закоммичена, потому что она была написана на Quorum и получен в терме 4,
а команда 2 закоммичена, потому что она придавливает сверху такой командой.
Ну а теперь что остается? Теперь остается доказать. В смысле доказать, что действительно такое определение безопасно использовать в протоколе репликации.
То есть команды, зафиксированные по такому определению, действительно из логов не стираются.
Вообще у RAF-то все эти утверждения как-то называются. Ну скажем, там есть свойство election safety.
Но оно было тривиально, мы про него уже поговорили, что в каждом терме может быть не более одного лидера.
Просто потому что пересечение кворумов и каждый реплик голосуют только один раз в терме.
Лидер append only означает, что лидер только добавляет в свой лог, никогда из него ничего не стирает.
Свойство log matching означает, что если в двух логах в каком-то слоте лежит запись с одинаковым термом, то значит, совпадают и префиксы.
Ну вот это свойство, это следствие процедуры выбора лидера. Это свойство, это просто по построению алгоритма оно у нас есть.
Это свойство, это простая индукция, которая следует из правила append entries, что мы отвергаем его, если у нас предыдущая команда не матчится в логе.
И из всего этого мы хотим вывести свойство, которое называется leader completeness.
Звучит так, если запись в логе закоммичена, согласно нашему определению, что она была получена и записана на большинство узлов лидером текущего терма,
то эта запись сохранится в логах всех будущих лидеров, то есть в логах лидеров всех больших термов.
Ну вот давайте этой докажем.
Итак, у нас есть лидер и в его логе, лидер в терме N, он пишет в лог какую-то команду C и реплицирует ее на хвору.
Вот эта команда по нашему определению является зафиксированной в логе.
Это просто определение, оно никого ни к чему не обязывает.
Мы хотим показать, что для любого N' не меньше N, в логе лидера терма N', эта команда по-прежнему должна находиться.
Команда является закоммиченной либо когда ее в текущем терме пишут на большинство, либо когда ее придавливают сверху такой командой.
Но вот второй случай нам не интересен, потому что если придавила, то давайте доказывать теорему про будущую команду, которая придавила.
Так что мы говорим сейчас про команду C, которая была получена в терме K и записана на хвором в терме K.
Мы хотим показать, что она переживет смену лидеров, смену эпох.
Ну а по свойству logmatching и все предшествующие ей команды тоже в логах лидеров новых останутся.
Итак, команда была закоммичена. Что это означает?
Что лидер терма N записал ее на некоторый хвором.
Что означает, что в терме N' был выбран новый лидер?
Ну сейчас я не сказал. Так же, как в паксосе.
Но вообще-то формулировка примерно такая же, как в паксосе, что должна уже наладить на какие-то мысли.
И доказываем мы точно так же по индукции.
Вот база вроде бы очевидна. Мы сейчас смотрим на какое-то N' большее, чем N.
И считаем, что утверждение верно для всех промежуточных N.
То есть для всех N' меньше, чем N' и больше либо равных, чем N в логах лидеров этих термов закоммиченная команда все еще оставалась.
И вот мы смотрим на какой-то новый терм N'.
Команда была закоммичена в терме N, то есть реплицирована на большинство узлов.
А в терме N' был выбран новый лидер какой-то.
Это означает, что за него проголосовал Quorum.
Ну и, как обычно, в пересечении у нас есть какая-то реплика.
Смотрим на нее.
Как-то я размахнулся очень, чем дальше.
Скажи, где мне лучше писать, справа или слева?
Хорошо.
Посмотрим на реплику R.
Вот она жила, жила, жила.
Она получила с одной стороны AppendEntry, с другой стороны RequestVote.
В каком порядке она их получила?
Она получила RequestVote N'.
И он ей понравился.
Но еще жена AppendEntry получала.
Могла ли она получить AppendEntry после RequestVote термы N'?
Не могла, потому что кажется, что тут после этого сообщения
она обязана находиться в терме не меньше, чем N'.
Ну и так же по тем же соображениям, что и в терме ProPaxos.
Мы говорим, что это случилось раньше.
Хорошо.
Как мы дальше рассуждаем?
Посмотрим на лог реплики R в момент получения RequestVote.
Что мы можем сказать про команду, закоммиченную в терме N' в этом логе?
Она жила до этого времени или нет?
Вот смотрите, что мы хотим показать, что когда лидер,
если он собрал RequestVote в терме N',
ну вот новый лидер зеленый,
если он собрал RequestVote, то в его логе обязана быть команда C.
Но вот сейчас я говорю про лог не вот этого зеленого кандидата пока еще,
а говорю про лог реплики R.
Вот здесь в нем оказалась команда C.
Она была закоммичена, записана на целый quorum.
Вот доживет ли в этом логе, в логе этой реплики эта команда до этого момента или нет?
По предположению индукции в лидерах всех промежуточных термов эта команда все еще оставалась.
По свойству лидера Pent Only все промежуточные лидеры никогда не стирали логи, которые у них были.
Конечно же промежуточные лидеры могли стирать лог реплики R, ну какой-то хвостик его.
Но по предположению индукции у них у всех есть в своем логе эта же команда,
а это означает, что стереть они ее не могли ни у кого.
Просто по правилу поведения лидера.
Поэтому в логе реплики R к этому моменту команда, закоммиченная в терме N, обязана все еще оставаться.
Ну а теперь посмотрим на зеленого кандидата.
Вот реплика R за него проголосовала.
Почему она за него проголосовала? Возможно и два варианта.
Может быть, последний терм в логе реплики R был равен последнему терму в логе зеленого кандидата.
И вместе с этим, видимо, длина лога реплики R был не длиннее, чем лог зеленого кандидата.
И второй случай, может быть, реплика проголосовала за кандидата зеленого,
потому что у него last терм был просто больше.
Ну вот давайте про два этих случая подумаем.
Первый случай, последние термы в логах равны.
О чем это говорит? Ну вот у зеленого кандидата и у реплики R к этому моменту.
Ну смотрите, у нас было свойство log-matching.
Что она означала? Что если в одном и том же слоте, ну точнее не так,
про один и тот же слот нельзя говорить.
Тут нужно сказать, что в этом случае просто две реплики R и этот самый зеленый кандидат,
у них логи являются префиксами друг друга.
Один из них является префиксом другого.
И очевидно, что судя по этому неравенству, лог R является префиксом зеленого кандидата.
И при этом в логе R, как мы уже выяснили, есть все еще наша закомиченная команда.
Они являются префиксами друг друга, потому что у них последний общий терм,
видимо, в них писал один и тот же лидер в последнюю очередь.
Значит, у них логи совпадают с какого-то момента.
Ну до какого-то момента, до какой-то записи этого терма.
Лог зеленого кандидата больше, чем лог этой реплики.
Поэтому у зеленого кандидата есть продолжение этого лога,
а в этом логе у реплики R есть закомиченная команда.
Поэтому мы ее сохранили, перенесли в наш терм n-штрих. Все хорошо.
Ну а в этом случае что мы можем сказать?
Тут уже про префиксы сказать нельзя, что один является префиксом другого.
Термы разные, непонятно, что это означает.
Ну вот посмотрим на лидера, который записал в лог зеленого кандидата
последнюю запись.
Вот этот last term c, он же меньше, чем n-штрих, да?
Вот, а эта штука last term r, он же не меньше, чем n.
Ну потому что у реплики R все еще есть закомиченная команда.
Ну это означает, что лидер, который записал в зеленого кандидата
последнюю запись, находился вот в этом диапазоне,
значит, у него действует предположение индукции,
значит, у него запись из терма n, закомиченная наша, была.
И он еще что-то сверху записал туда.
Ну значит, она все еще есть из зеленого кандидата.
Ну вот и все, случаи закончились.
Предположение индукции, самоутверждение предположения индукции,
оно говорит о том, что у промежуточных лидеров,
утверждение говорит о том, что у лидера в терме n-штрих,
в логе обязательно будет закомиченная команда.
Но поскольку лидеры не стирают никогда свои логи,
они замечают, что эта команда будет переживать сменную эпоху.
Все, получилось.
Ну вообще утверждение, оно доказывается ровно так же, как в мульте,
как в сингл декрип аксессии.
Я бы сказал, что это то же самое утверждение.
Ну то есть там мы точно так же говорили, что если у нас значение выбранное,
то оно прошло через вторую фазу.
Если значение кто-то предлагает с новым n-штрих,
то он прошел через первую фазу по аксессу.
Они пересекаются, в пересечении есть acceptor.
Этот acceptor получил сначала сообщение с более ранней мэн,
потом с более поздней мэн,
потому что иначе был отверг первое сообщение.
Ну и дальше там точно такие же суждения про этот отрезок
и про то, что здесь мы получим...
Ну в случае аксесса мы через prepare получим нужное значение с большим n-а,
а в случае RAF-та мы это все переносим на лог и говорим,
что в логе этого кандидата будет нужная команда.
Но смысл-то одинаковый.
Это одна и та же теория, потому что...
Поэтому когда вам авторы говорят, что RAF проще паксуса,
ну я бы сомневался, что это так,
потому что мне кажется, что судя по теориям, это одно и то же абсолютно.
Разве что немного другие обозначения,
но просто потому что мы говорим теперь не про отдельный слот,
а про целый лог, и вот он чуть сложнее устроен.
Ну так или иначе, мы получаем то, что называется leader completeness,
а это дает нам то, что называется street machine-safe.
То есть если каждая реплика применяет только за камечные команды...
Что?
Да, спасибо.
И нужно, наверное, вернуть...
Ну нет, плохо видно.
Ну вот, мы доказали leader completeness,
а с него следует последнее свойство,
про то, что мы коллектно реплицируем наш автомат.
Небольшой вопрос на понимание,
а где рассуждения теоремы ломаются вот в этом примере?
Ну просто вроде бы мы доказали, что если команда написана...
Короче, как этот пример не сходится с теоремой?
Всегда же полезно это понимать, да?
Это тут не очень очевидно, но смотрите, у нас теорема говорит,
что если команда закоммичена в терме N,
то она будет в логах всех последующих лидеров.
А здесь у нас, смотрите, у нас команда из термы 2 закоммичена в терме 4.
В итоге у нас эта N раздваивается на 2N.
У нас есть N равное 2, N равное 4.
И вот непонятно, про что теорема говорит.
Вот теорема говорит про терм номер 4.
Вот если здесь команда получена в терме 4 и записана на большинство,
то она здесь надежно закоммичена.
А что происходит на этой картинке?
Смотрите, у нас как будто бы команда из термы 2 закоммичена,
как будто бы N равно 2.
А вот для N' равного 3 почему-то у лидера нет этой команды.
В чем же обман?
А в том, что, смотрите, мы говорим, пересечение кворумов.
Вот действительно, кворум, на котором была реплицирована команда 2,
пересекается с кворумом, который выбрал реплику 3.
Но что не так?
А не так то, что у нас больше не работает рассуждение о том,
что append-entries был получен до request-vote.
Вот здесь append-entries был получен после request-vote.
Потому что append-entries терм 4, а у request-vote терм 3.
Потому что здесь N не 4, а 2.
Ну, короче, если вы следите, то понимаете, что произошло.
Тут N разошлось.
В этом примере команда вроде бы из термы 2 закомичена,
но закомичена в терме 4.
Вот чтобы такого не было, терема говорит про один и тот же терм,
и тогда уже там все четко получается.
То есть вот здесь вот команда из термы 4 закомичена в терме 4,
и все хорошо с ней.
Ну что?
Мы убедились, что RAFT обеспечивает safety.
Да?
Обеспечивает ли RAFT LiveNas?
Разумно. Мы говорили, кажется, про алгоритм консенсуса,
доказывали какие-то теоремы,
и выяснили, что safety мы должны обеспечивать всегда,
даже в синхронной модели, даже с произвольным числом отказов.
И RAFT с этим справляется.
А LiveNas мы гарантировать не можем, потому что есть FTP-теорема.
И мы должны что-то делать, чтобы все-таки прогресс был.
Ну что мы делаем?
В RAFT для LiveNas мы это делали в самом начале нашего рассказа.
Мы рандомизируем тайм-ауты.
Достаточно ли этого для LiveNas?
Это не очень страшно, потому что если у реплики,
у которой короткий лог будет самый маленький тайм-аут,
ну просто за нее не проголосуют.
То есть если вдруг ты реплика, у которой нулевой лог,
и ты стал первым кандидатом почему-то,
то слушать тебя просто никто не станет.
И другие реплики не израсходуют свой голос в терме.
Поэтому это само по себе не страшно.
Но у RAFT есть одна неприятная особенность.
В нем выбор лидера, они дизраптив, они ломают терм.
Вот представьте, что у вас есть, ну не знаю, 5 реплик,
и возникает partition.
Вот в большей части partition где, допустим, осталось 3 реплики,
ну есть хвором, его можно собрать,
значит эти 3 реплики выбирают лидера и живут дальше.
Там, не знаю, наступает у них там пятый терм,
в пятом терме выбирается лидер,
и он дальше спокойно реплицирует команду, обслуживает клиент.
А что происходит в меньшей части partition?
Ну это не правда, что ничего не происходит,
что происходит все же.
Там каждая реплика заводит себе таймер, reaction timer,
и ждет хардбитов от лидера.
Не дождется, потому что лидер в другой половине выбран partition,
а в этой половине лидер выбран не может, потому что их слишком мало.
В итоге рано или поздно таймер протухает,
и эта реплика уничивает себе терм.
Заводит таймер заново, посылает request vote,
и снова ждет, и снова ничего не происходит.
Таймер снова истекает,
мы снова увеличиваем терм,
и снова заводим таймер, посылаем request vote,
и снова заводим таймер.
У него есть reaction timeout, еще раз,
ты когда становишься кандидатом, смотри вот здесь,
ты посылаешь свой request vote и ждешь,
что либо у тебя соберется quorum,
либо ты получишь append entry с нового лидера,
потому что ты проиграл,
либо таймер протухнет, потому что эпоха провалена,
терм провален, split vote получился.
И здесь всегда будет таймер,
потому что ты и quorum собрать не можешь,
и никто другой не может собрать quorum.
Ну и к чему это приведет, когда partition вылечится?
Смотрите, в большей части partition все стабильно,
там эпоха, там терм 5,
там лидер живет спокойно себе,
а в другой половине реплики себе растят этот quorum-терм,
partition исчезает,
и наконец их request vote и приходит.
И вот там терм 5, а там терм 100-500 уже,
потому что их долго не было.
И вроде бы ничего полезного не происходит,
в смысле все хорошо и так,
но они приходят со своим большим термом,
и те реплики, которые жили себя в большей части partition,
переходят за ними в этот терм,
и происходит процедура выбора лидера.
То есть терм сбивается,
проходит фаза выбора лидера,
и в конце концов все сходится,
но тем не менее алгоритм немножко прервался,
прервал ход своей работы нормальный.
Но это само по себе не страшно,
это просто неприятно.
Страшно становится тогда,
когда у вас возникают сбои в сети.
Смотрите, есть такая компания Cloudflare.
Слышали про нее?
Вот.
Вот у них, кажется, был шестичасовой downtime
из-за рафта.
И он был, смотрите, по каким причинам.
Он был потому, что они называются византийским сбоем,
это не византийский сбой.
Им в твиттере это доступно написали много раз,
когда они опубликовали эту статью,
не разбирайтесь.
Они сделали об этом занятку вместо того,
чтобы статью исправить.
Смотрите, что может произойти.
Вот представьте,
что у вас пять узлов,
и по задумке алгоритмы консенсуса
эти пять узлов должны уметь друг с другом взаимодействовать.
Но так получилось,
что у вас поломалась какая-то сетевая коробка,
и теперь не все пары узлов
могут напрямую друг с другом общаться.
Вот скажем, второй с пятым не может общаться.
Как это влияет на рафт?
На лайвность рафта?
Ну вот, с одной стороны,
вот у вас здесь есть три реплики,
и они бы могли
сформировать кворум, выбрать лидера и дальше работать.
Ну вот, давайте выберем соединительных лидеров.
Кто это будет? Один, два или три?
Один.
Выбрал самый неинтересный вариант.
Ну хорошо, пусть будет один.
Вот он может общаться только с
два и три, но они собирают вместе кворум.
Он становится лидером.
Он начинает сломать им хардбиты.
При этом ни четыре, ни пять их не получают,
потому что прямой провод сломался.
То есть маршруты, пакеты
не достигают
ни четыре, ни пять
отправленные соединительные.
Что происходит с четыре и пять тогда?
На них протухает эрекшн тайм аут.
Они становятся кандидатами,
посылают свой реквест в отэ,
и вот здесь
кажется стабильность нарушается.
И начинаются новые выборы.
Кто выбирается дальше?
Два выбирается.
Он становится лидером,
но тут у него получше дела.
То есть он может посылать хардбиты и один, и три, и четыре.
Но пять все равно не может.
И в итоге теперь пять будет недоволен.
То есть кто бы не был выбран,
один, два или три будут недовольны
либо оба, либо
четыре, либо пять.
Но кто-то из них обязательно будет.
И вот он будет у себя растить терм
и сбивать всех остальных.
И лидер будет постоянно сбиваться.
И рафт не сможет перейти в стабильное состояние.
Нужно ему для работы.
Неприятная ситуация, но вот так можно продакшн положить.
Поэтому одного лишь
протокола, описанного
в статье, оказывается недостаточно.
Вот этого мало.
То есть этого достаточно, если у вас
не бывают таких сбоев в сети.
Как вы это гарантируете?
Поэтому
нужно чинить.
Предлагается добавить рафт...
В чем беда?
В том, что
реплика четыре или реплика пять
становятся кандидатами и сбивают
нормальный кворум.
Но с другой стороны,
на этой картинке же ни у четыре, ни у пять
нет шансов никаких.
Потому что они не могут кворум собрать, в принципе.
Так что давайте,
перед тем, как они будут становиться кандидатами
и собирать много кворума, и посылать request-vote,
избивать остальных, они попробуют узнать,
а вообще способны ли они стать лидерами.
То есть они
запустят фазу pre-vote,
в которой...
Это такая фаза request-vote, просто виртуальная.
Они пробуют,
говорят, а что, если я отправлю тебе
request-vote? Как ты мне ответишь?
Вот. И если...
Что делает реплика два?
Она отвечает на этот pre-vote
положительно,
если лог реплики четыре
ее устраивает,
и если она сама
не получает хардбиты от лидера.
А если она получает хардбиты от лидера,
то, видимо, она всем довольна, и ну зачем
реагировать на эти request-vote?
Ну вот такой
изящный костыль.
Ну, как у любого костыля,
есть здесь нюансы некоторые.
Оказывается, что
это чинит лайв на таком сценарии,
но ломает в таком сценарии.
Вот мы оторвали еще один провод.
Лидером у нас
была реплика четыре,
когда провода порвались.
Она шлют хардбиты два.
Вот этих двух все устраивает.
И если она не получает хардбиты,
вот этих двух все устраивает.
Вот, 1 на 3 не получает хардбиты.
Ну, 5 там не важно, она никому не навредит уже.
1 на 3 недовольны,
они хардбитов не получают.
Они становятся кандидатами.
Они могли бы собрать
хорум.
Но как бы связи достаточно.
Но при этом
2-то все устраивает,
потому что от лидера она получает голоса.
В итоге 4 лидер,
он реплицирует свой голос на 2,
хорум не собирает при этом.
То есть команда не коммитит.
А 1 на 3 не могут пройти через преволут,
потому что 2 все устраивает.
Поэтому
нужен второй костырь, а именно
проверка чек хорум,
которая на лидеры проверяет,
что вообще-то он сам собирает хорум.
То есть он не бесполезен сейчас.
Что?
Вот из костырей, да.
Хорошее замечание.
Ну вот, это две эвристики, которые нужно в продакшене использовать.
Нет, а если
всех соединить в цепочку, то ничего хорошего случиться
не может.
В этот случай он довольно странный.
Вероятность его невелика.
Но ты видишь, что люди 6 часов не работали.
С одной стороны, да,
а с другой стороны люди пострадали.
Поэтому ты можешь в это не верить,
а можешь сделать
две эти эвристики у себя.
И, конечно, все хорошие реализации рафта
эти эвристики используют, иначе...
Что?
Можно придумывать
много разных эвристик,
а можно остановиться.
Я, кстати, не видел строгих
рассуждений, что этого прямо достаточно,
и что там ничего другого случиться не может.
Но кажется, что сейчас
промышленная реализация
работает вот так.
Но без привода, конечно, никто не работает,
потому что у рафта есть такой дурацкий сценарий,
что реплика,
которая долго где-то
была в маленькой части партишена,
она прерывает
нормальный ход рафта.
Итак, значит,
сейфти у нас есть,
лайвнес у нас, кажется, есть
с этими всеми приключениями.
Последнее, что я скажу про рафт.
Про то, что алгоритм проще.
Автор говорит, что он проще и понятнее.
Что вы думаете по этому поводу?
Какая-то красивая картина.
Сейчас я...
Сейчас я найду что-то забавное.
Покажу вам.
Ну, во-первых,
авторы рафта,
они разумные вещи говорят.
Действительно, у них протокол описан,
есть референсная реализация,
есть какие-то промышленные реализации.
Промышленные реализации появились
после, потому что была референсная реализация,
потому что была статья,
то есть всех все устраивает, это удобно.
В этом смысле, конечно,
рафт удобнее писать.
Рафт проще написать, но не потому,
что алгоритм проще, а потому, что
больше деталей описано.
Ну, авторы рафта, они не поленились,
они даже...
Они сделали две лекции про мультипаксис
и про рафт,
заставляли их студентов проходить в произвольном порядке,
чтобы...
Ну, короче, у них есть прямо в PhD
глава,
который называется
Рафт Езерстадия,
где они прямо на студентах испытывают,
что рафт на самом деле проще.
Ну, и получают, конечно, правильные результаты,
что рафт проще.
Мое мнение, что это одно и то же просто,
ну, то есть теорема о
рафта, те же самые частные случаи,
такое же утверждение про корректность,
ну, в самом деле, смотрите, утверждение про корректность
рафта, а теперь вспомните, что было
в паксосе. Значит, если
пара N, V выбрана chosen
в смысле консенсуса, то для любого
bellot number N3 не меньше,
чем N, ничего другого
может быть, не может. Вот
переложим это утверждение на
multipaxis. Там у нас была N это эпоха,
а аксепт
делал только лидер.
И вот утверждение звучит так, если
лидер
с эпохой N
зафиксировал команду в логе,
то все аксепты
лидеров из больших эпох
не могут
эту команду стереть.
Вот то же самое утверждение,
что оказывается точно так же.
Ну, как бы, те же самые N
с тем же самым смыслом, чтобы ограничивать
старых лидеров и переходить в новую эпоху.
Этот N также хранится
на диске. Ну, короче, тут
буквально все то же самое,
разве что все подогдано и все понятно,
все детали описаны.
А алгоритм, мне кажется, точно такой же,
в смысле, raft это, кажется, вариация multipaxis,
потому что multipaxis это вообще не алгоритм, а
work. Вот в нем можно построить raft,
подогнав аккуратно все детали.
Ну, как бы, есть
независимые исследования. Ну, как бы, не то, что
исследования. Мы с вами независимые исследования проведем,
потому что у нас будет multipaxis и raft.
Можно будет написать одно, потом другое, а потом подумать, что же вам
оказалось проще.
Есть еще опыт MIT.
Вот они до 2013 года рассказывали
paxis, а потом стали рассказывать raft.
Вот. И есть
тут, как бы, такая длинная рефлексия
по поводу того, что с этим,
что случилось в итоге.
И вот они здесь пишут, что
raft проще
написать, ну, то есть
трансформировать его в какой-то рабочий код.
Но абсолютно
непроще понять, почему же он работает.
Вот даже сами авторы
raft в своем слайде вас обманывают.
Они говорят, вот мы декомпозируем
задачу на фазу выбора лидера
и фазу репликации. И сначала рассказывают
фазу выбора лидера, а потом фазу репликации.
А потом оказывается, что в фазе выбора лидера
нужно подправить что-то. Потому что без репликации,
но без учета репликации это не работает.
Вот. Ну, как бы, это не похоже
на артагональность. Вот в multipaxis
артагонально. Как бы вы лидеры не выбирали,
ничего плохого не произойдет.
А тут нужно быть аккуратны.
Короче, мне кажется, что нужно
трезво оценивать
все происходящее.
И, конечно же, если вы пишете новую продакшн
систему, то вам нужно брать, видимо, raft.
Если вы не суперспециалист, который знает
все про эти консенсы,
то вы берете open source реализацию raft точно так же,
как вы берете open source реализацию хранилища
локального, типа RocksDB.
Но при этом
под капотом у вас все равно
те же самые две фазы,
те же самые n, те же самые правила,
те же самые теоремы.
Так что в каком-то смысле это одно и то же.
Вот. А дальше вы решаете,
потратили ли мы время сегодня
или узнали что-то новое.
Ну вот, про raft
наверное сейчас
все.
Если есть вопросы, то
спрашивайте.
Ну, смотрите, raft мы успели за одну пару всего.
Что?
Нет, так и планировалось.
Нет, потому что это
еще один аргумент в копилку raft.
В пользу raft.
Ну, и смотри, есть
зукипер, который
использует собственный протокол.
И про зукипер мы сейчас поговорим,
но не только про зукипер,
но и про него тоже поговорим сейчас
на второй половине.
Вот.
А все остальное это либо Paxos, либо raft.
Ну вот, например, AVS,
облака Амазона, они используют
Paxos у себя.
Google использует Paxos.
Ну, Google использует Paxos, понятно, по каким причинам.
Это не потому что сознательные выборы,
потому что они давно уже Paxos написали,
и как-то глупо все стирать без raft теперь,
с теми же самыми свойствами.
Если вы не Google,
практически никто не Google,
никто не Google кроме Google,
то вы берете raft.
Ну, пишете свой raft
или берете опенсорсный raft?
А, в Яндексе,
у IT там писали
консенсус примерно тогда,
когда raft публиковал статью,
то есть там в 2013-2012 году,
так что там...
Ну, он...
Это не raft,
это какой-то протокол консенсуса,
который, ну,
в каком-то смысле все одно и то же,
поэтому...
Ну, у него есть внутреннее название,
но это все равно
некоторая вариация на тему
в конце концов.
То есть там тоже должны быть
вот эти эпохи, эти правила про то,
чтобы заблокировать или перейти в эпоху,
ну и так далее.
Ну, кажется,
что и вот невозможно
ничего другого придумать,
и что лэмпард, он вот захватил самую суть.
Поэтому
лэмпард так и хорош.
Греки уже все придумали.
Греки, да, вот...
Греки действительно все придумали для нас.
Ладно, давайте перерыв 10 минут,
а потом...
Потом продолжим.

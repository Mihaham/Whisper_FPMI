Что мы с вами не успели доказать в прошлый раз, неравенство
к ошибке Николаевскому для математического ожидания,
оно было в таком виде, что математическое ожидание
модуля произведения случайных причин в квадрате будет
меньше либо равно, чем произведение мат ожиданий
квадратов этих случайных величин.
В соответствии на первый случай мы с вами аккуратно
разобрали, когда ни один из этих квадратов не равен
нулю.
Мы там рассматривали специальные случайные величины, которые
получались из ксиэты делением на корень вот из этих величин.
Мы с вами недоразобрали случай, когда кто-то равен
нулю, но без ограничения общности можно считать то,
что пускай от ожидания квадрата кси у нас будет равно нулю.
Верно следующее замечание.
Оно почти очевидное, но давайте это приговорим.
Что если математическое ожидание случайной величины
z равно нулю, и при этом случайная величина z у вас больше
либо равна нуля, то что можно утверждать про случайную
величину z?
Ну, почти.
Причем это почти имеет в соответствии почти наверное.
То есть вы должны осознать сейчас, что почти наверное
это научный термин.
Обычно народ так радуется, когда впервые слышат это
понятие, что оно означает.
То есть вероятность того, что случайная величина
z равна нулю, равна единице.
И в будущем у нас всегда будет, то есть что-то выплывано
почти наверное, это значит, что вероятность того, что
выплывано то, что утверждается, это один.
Особенно заключается в том, что никакими средствами
теории вероятности вы не сможете почувствовать
вещь, которая изменена почти наверное.
Изменена почти наверное как-то безграмотно звучит.
Наверное, это по сути.
Ну, понятно, да?
То есть, предположим, у вас есть две случайные величины,
которые одинаковые почти наверное.
То есть множество омек, на которых они отличаются,
оно равно нулю.
Так вот, никакими средствами теории вероятности вы не
сможете это почувствовать.
Идея ясна?
Поэтому в будущем такие случайные величины вы будете
называть эквивалентными, в теории вам все это будет
рассказываться.
Встрекайте к этим понятиям, кивайте.
Хорошо?
Замечательно.
Так, доказательства.
Вот эта история, она простейшая, поскольку, я напомню, мы
умеем с вами строить сейчас только дискретные модели.
Ну, другие не научились, мы выяснили то, что математического
аппарата нам для этого недостаточно.
Поэтому для нас с вами математическое ожидание случайных Z это
бесконечный ряд, который состоит из слагаемых,
каждая из которых имеет вид такого произведения.
Ну, то есть значение случайных значений в данной точке
омега на вероятность данной точки омега.
Ага.
Теперь, что мы видим, поскольку у нас случайно, господи,
извините меня за это, но поскольку у меня случайная
значения не отрицательная, вот здесь вот у меня тоже
больше либо равно нуля, ну вот, то что мы получаем?
То есть у нас сумма не отрицательных слагаемых, которая оказалась
равна нулю.
Как такое возможно?
Все слагаемые ноль.
Все.
Поэтому у нас два варианта.
Ну вот, либо у вас Z равно нулю, либо у вас вероятность
равна нулю.
Можно я не буду вот отсюда получать вот это?
Ну, то есть как бы мы собираем вместе все омега маленькие
такие, что Z у вас больше нуля, вероятность каждого
такого омега маленького ноль, их счетное число,
ну значит и сумма их тоже будет ноль.
То есть вероятность, когда Z строго больше нуля, она
ноль.
Но это значит, что вероятность, когда Z равна нулю, она единичка,
потому что всегда выполнил вот это.
Все понял, что я сказал?
Покевайте как-то.
Ну, просто.
Окей.
Соответственно, что мы можем сказать, то что если в силу
этого замечания, если математическое ожидание квадрат случайной
значения равна нулю, значит Z квадрат может, извините,
Прокси квадрат.
Мы что можем сказать?
Она почти, наверное, ноль.
Ну а теперь давайте посмотрим на вот это вот математическое
ожидание.
Если случайно значена Кси, почти, наверное, ноль, то
что можно сказать про это случайно значение?
Она тоже почти, наверное, ноль.
Это ясно?
Потому что всегда, когда Кси 0, это произведение тоже
ноль, то есть тут может только увеличится количество
ситуации, когда это произведение равно нулю.
А если у вас случайно начинает почти наверно ноль, то ее математическое ожидание, очевидно, равно нолю.
Четыре д. Тут ноль, тут ноль. Ура! Все доказали.
В тервере будет отдельное обсуждение вот этих вот вещей почти наверно.
Во-первых, вы должны привыкнуть к терму, почти наверно.
Не в тервере, а в теории меры это будет почти всюду. То есть свойство, которое выполнено.
Везде.
Свойство, которое выполнено почти всюду, это будет свойство, что там, где оно не выполнено, равно нулю.
Потому что у нас в терверах хорошо, у нас единичка, а
меры же, она не обязательно единичка будет. Сейчас понимаете, что я говорю, да?
У вас такие недоуменные лица. Слушайте, у меня огромная просьба. Смотрите, вот я чего-то говорю, говорю, говорю, говорю, потом все, да, закончили, ко мне подходит толпа людей.
Слушайте, мы вот там не поняли.
Понимаете, есть как бы
базовые вещи. Если вы чего-то не поняли, есть точно еще один человек, который точно не понял, он вам будет благодарен, если вы зададите вопрос по поводу этого непонятного. Это ясно?
Поэтому, пожалуйста, задавайте вопросы по ходу. Договорились с Киваем. Отлично.
Напросился, да.
Вы говорите почти нереально. Почему вы не впрасно говорили, что это равно 0?
Ну, потому что это неправда. То есть у тебя может получиться история, то, что есть какая-то одна точечка омега, то есть вот у тебя есть омега большое, у тебя есть какая-то одна точка.
Вот вероятность этой точечки равна нулю.
В дискретных случаях у нас такое не бывало, но у нас это бывало в ситуации, когда мы в качестве омега рассматриваем отрезок от 0 до 1.
То есть ты можешь рассматривать, мы не допостроили здесь математическую модель, мы не очень понимаем, что такое множество событий, но мы же это сделаем рано или поздно.
Вы мне сейчас поверите, что если я рассматриваю две случайные величины, кси тождественной равны нулю на отрезке, и это, которая есть в функции дирекля.
Что у нас там было? Единичка в рациональных или иррациональных?
Один в рациональных.
Это функция дирекля.
То есть она принимает значение 0 в иррациональных точках.
Единичка в рациональных точках.
Естественно пересеченные с 0 и 1.
С точки зрения меры Лебега, мера рациональных точек у нас равна нулю.
То есть получается, что вот эта кси равна эти почти наверно.
И мысль в том, что никакими средствами теории вероятности, тем аппаратам, которые будут построены, вы не сможете почувствовать их отличия друг от друга.
Вероятности там все будут одинаковые.
Отсюда все характеристики, которые завязаны на вероятностях на распределениях, они будут одинаковые.
Никак не почувствуете их друг от друга.
При этом это сильно разные штуки.
Идея ясна?
Поэтому просто написать равна нулю мы не можем, потому что это неправда.
Сейчас нормально, да? Хорошо.
Но еще раз, центральная мысль заключается в чем?
Нас именно во всей этой истории теория вероятности кси как функции от омега маленького не интересует.
То есть мы с ней работаем как функции от омега маленького, но просто она так устроена.
Но она нас не интересует, нас всегда интересует вероятность.
Смысл заключается в том, что если у вас омега маленькая известна, то есть эксперимент уже имел место быть,
то телевизора тут не нужна, уже все, вы в ДТП или вы в чем-то плохом, или наоборот в чем-то хорошем, оно уже есть.
Поэтому нас всегда интересует вероятность.
Так, спасибо за вопрос больше.
Вот я уверен, что были люди, которые тоже хотели это услышать.
Соответственно, что дальше? Следующий объект, который возникает,
ну понятие, это катый момент, случайная величина.
Потому что ненавиственно у Каши Банюковского мы с вами закрывали вопрос о свойствах математического ожидания от случайной величины.
Соответственно, что касается катого момента.
Так, первая вещь на сегодня.
Так, первая вещь на сегодня.
Катый момент.
То есть на определение.
Катый момент, извините,
случайной величиной кси
называется математическое ожидание кси в степени К.
Ну как так, не очень интеллектуально.
То есть обычный мат ожидания – это первый момент нашей случайной величины.
Какие замечания?
Замечания первые.
То, что мы с вами об этом говорили, я еще раз обращу на это внимание.
То, что катый момент у вас существует тогда и только тогда, когда у вас существует мат ожидания кси в степени К под модулем.
Мы это с вами обсуждали, причем это верно будет не только в случае, когда мы с вами разговариваем,
мы это с вами обсуждали, причем это верно будет не только в случае, когда мы с вами работаем с дискретными вероятностными моделями.
Это будет верно всегда.
Если вы расписываете математическое ожидание, у вас получается ряд, который не индексирован.
Сейчас подкивайте, чтобы понятно.
А раз он не индексирован, то вопрос об условной исходимости ряда вообще не стоит, поэтому вот так.
Это первая мысль, которая есть, и вторая мысль, которая есть.
Все ли понимают, что у нас мат ожидания может не существовать?
Ну, если у нас бесконечное число значений случайной величины, этот ряд может, ну, во-первых, он может зайти из бесконечности,
во-вторых, он может как-то условно сходиться, а значит, он нам не подходит.
Это всем понятно, да?
Возникает вопрос о том, о том, что как связано вообще существование катых моментов между собой.
Тут приходит на помощь неравенство промо от ожидания.
У нас их будет много.
То, которое я сейчас формулирую, доказывать не буду, вы его докажете на тервере.
Но вот, я вас формулирую для того, чтобы закрыть вопрос о том, как связаны катые моменты между собой.
Называется оно неравенство Лепунова.
Лепунов – это наш терверщик советский, у него там блестящие результаты, там было все здорово.
Соответственно, выглядит так, что если у вас чиселки s и t связаны с таким соотношением,
то мот ожидания будут связаны с следующим соотношением.
Мот ожидания кси в степени s в степени 1s t будет меньше либо равно
мот ожидания модуль кси в степени t в степени 1t t.
Ну, понятно то, что если кто-то из них в какой-то момент не существует,
у вас просто соответствующее значение уйдет в бесконечность.
Какой отсюда вывод мы можем сделать?
Какой вывод мы отсюда можем сделать?
Если существует каждая катая момент, то существуют все моменты порядками ниже.
Это понятно?
То есть, если существует катый момент, следовательно, существует k минус этой моменты.
Покрупнее я буду стараться.
Ну, смотри, предположим, у тебя существует пятый момент.
Ну, то есть, ряд сошел в свой раз.
Значит, четвертый, третий, второй и первый тоже существуют.
Почему по неравенству?
То есть, смотри, ты получаешь, берешь вот здесь s и t.
То есть, почему у тебя может не получиться какой-то момент?
У тебя расходится соответствующий ряд.
Ну, то есть, расходится, уходит в бесконечность.
У нас тут модули.
О чем говорит вот это неравенство?
Если у тебя при t равном 5 вот этот ряд сошелся, значит, при s равном 4, 3, 2 и так далее это тоже сойдется,
потому что вера на это неравенство.
Она не может себе позволить уйти в бесконечность.
Хорошо?
K минус i.
Я могу даже написать i от единички до k минус 1.
Так? Хорошо.
Ну, догадывайте.
Ну вот.
s, 1s, t, t, 1t, t.
Это какие-то чиселки. s и t это какие-то фиксированные чиселки.
Будет выполнено такое неравенство.
Я тут...
Я же в школе работаю.
У меня сейчас не с ними тема.
Мы проходим векторы меткоординат в 9 классе во второй школе.
Я как-то пытаюсь какую-то наукообразность всю эту школьную историю навести,
чтобы хоть как-то оправдать, что я там вузовский преподаватель, работаю в школе.
Я зашел с чего?
Я им дал общее определение линейного пространства.
Ну там...
Сейчас я что-то не понял, к чему это.
Ну ладно, опустим этот смех.
Опустим этот смех.
Ну вот.
Я говорю, чуваки, ну вот у вас будет линал, вы будете заниматься линейным пространством.
Линейное пространство, что там?
Абстрактное множество, две операции, которые утворяют вот этим свойством, аксиомом.
Мы с вами разберем там, ну вот, конкретный пример линейного пространства, все будем проверять.
Проходит месяц.
Ну я же понимаю, что они ни фига не поняли.
Ну вот.
Я это еще раз проговариваю.
Они говорят, Иван Георгиевич, вы вообще что-то нам...
Все, вы доказываете аксиомы?
Ну то есть они не поняли, почему мы те свойства, которые зашиты в определении линейного пространства,
почему мы для множества направленных отрезков на плоскости почему-то начали доказывать.
Это же аксиом.
Ну вот.
К чему я это?
Все присутствующие понимают то, что когда мы с вами аккуратно построим весь необходимый математический аппарат,
измеряемых пространств и измеряемых функций, на нем интегра, олибега,
и когда вы придете на теорию вероятностей, с чего начнется тервер.
Он скажет, пусть у нас есть вероятностное пространство,
Омега ФП, произвольное.
И погнали формулировать все результаты теории вероятностей на произвольном линейном пространстве.
Все, что делаем сейчас мы, мы делаем в частном случае на дискретном вероятностном пространстве.
Это частный случай.
Но результаты, естественно, все будут переноситься туда.
Это же вы понимаете, да?
В прошлый раз просто кто-то задал вопрос о том, что, а вот то, что вы формулируете, там будет работать?
Ну да, конечно.
Хорошо?
Ладно.
Так, с катом моментом все.
Следующий объект, который нам нужен, это дисперсия случайной величины.
Дисперсия случайной величины.
В чем целеполагание введения такого объекта?
Можно рассмотреть две случайные величины с такими распределениями.
Я напомню то, что математическое ожидание это характеристика распределения случайной величины,
а не случайной величины как функции от Омега.
Надеюсь, все поняли, что я имел в виду.
Поэтому, когда мы говорим о моментах, нас интересует всегда распределение.
И поэтому вы же уже решали, скорее всего, задачки на случайную величину.
Потому что всегда в условиях сказано, дана такая случайная величина с таким-то распределением.
И редко вам задает случайную величину как функцию от Омега маленького.
Но такое будет для упражнений, чтобы вы поработали.
И в середине курса нам нужно, чтобы мы сигма-алгебр построили порожденные измеримыми функциями.
А так всегда задают только распределение.
Давайте рассмотрим две.
То есть у вас случайная величина 1 принимает значение 1-1 с вероятностями 1-2.
И случайная величина эта принимает значение 100 и минус 100 с вероятностями тоже 1-2.
Понятно, что мат ожидания этих случайных величин одинаковый.
Это очевидно.
При этом видно, что разброс данных случайных величин огромный.
И в этот момент кажется, что мат ожидания так себе характеристика.
То есть у вас две сильно разные случайные начины, которые принимают разные значения, сильно отличающие друг друга.
При этом эта характеристика у них одинаковая.
И в этот момент логично возникает мысль о том, что хочется заиметь характеристику,
которая будет показывать степень разброса случайной величины от своего среднего.
Понятная история.
Потому что если эта характеристика мала, это значит, что случайная величина от этой чиселки,
которая есть мат ожидания, отстоит не сильно.
Если она большая, значит она отстоит сильно.
Уже какие-то выводы о случайной величине можно делать.
Соответственно, мы вводим такую характеристику, называется она дисперсию случайной кси.
И определяться она будет как?
То есть вы берете свою случайную величину.
Вам же нужен как?
Степень разброса от среднего.
Поэтому вы из нее вычитаете среднее.
Дальше разброс.
То есть вас интересует абсолютное значение.
Поэтому я здесь возведу в квадрат, чтобы решить этот вопрос.
Разброс влево, разброс вправо.
Разброс вправо.
И возьму среднее.
То есть возьму от ожидания.
Одна дисперсия случайной величины определяется так.
Какие эмоции в этот момент должны возникнуть у слушателей?
Ну это первое.
Да.
Ну муторно.
Ну слушай, ну понимаешь, что-то на физтехе учиться не аясь, ездить далеко муторно.
А что делать?
Вопрос к Сталину.
Вопрос к Сталину.
Ну вот.
Это одна история.
А вторая история то, что это правильно.
Почему квадрат?
Логичнее всего модуль.
Причем вслед за дисперсией.
Водится понятие средне квадратического отклонения.
Средне квадратическое отклонение.
Которое определяется как корень дисперсии с такой аргументацией.
То, что видно, что размерность не сходится.
Ну так.
То есть у вас и у ксиблок есть какая-то размерность,
то в квадрате у вас выбираете среднее, то размерность не та.
И надо извлечь квадратный корень.
Ну и кажется историей то, что ну как-то действительно было логичнее навесить модуль.
Ну потому что модуль это и в той же ситуации.
Действительно было логичнее навесить модуль.
Ну потому что модуль это и в точности есть.
Расстояние кси от этой точки.
И вы берете среднее из этих вот расстояний.
Ответ на этот вопрос.
Типа так математику устроено.
Этот объект оказывается самым удобным.
Сейчас, когда мы сформулируем его свойства,
вы увидите прелести всего этого объекта,
которых вообще нету у объекта, когда вы считаете математическое ожидание модуля.
Интересно вслед за дисперсией еще вводится понятие
аналогичных к этому моменту.
Это центрированный катый момент.
Центрированный катый момент.
Определяется он...
Так обозначается он...
О господи!
Я не помню.
Ну, определяется он, соответственно, как мат ожидания кси
минус мат ожидания кси в степени к.
Ну, соответственно.
Нет, там какое-то было обозначение.
Там что-то типа того, что мат ожидания кси в степени к с черточкой.
Слушайте, я боюсь наводять.
Но с черточкой это выборочно.
Без обозначений, в общем.
Хорошо?
Ну, погнали.
Теперь свойства дисперсии.
Свойства этого объекта, который есть.
Да.
Давайте последовательно.
Вопрос.
Да.
Да.
Давайте последовательно.
Вопрос.
В степень возводится функционально.
Возводится функционально.
У вас же изначально, случайно, величина, это что?
Это функция.
Из-за мега вл, правильно?
То есть, когда вы определяете кси в кубе, вы что делаете?
Вы сначала делаете вот так вот, да?
А потом эту чиселку возводите в куб.
Ну, у тебя может получиться так то, что там будут накладывать.
Может быть, вот, например, я возьму кси в квадрате, да?
У тебя могут накладываться.
Ну, предположим, кси принимала значение плюс-минус единичка с вероятностями 1 четверть.
Она будет принимать значение 1 с вероятностями 1 вторая.
Но, идейно, как бы, это изначально функция.
И когда вы возводите в степень, это функционально возводится.
Ну, то есть, для того, чтобы посчитать значение кси в кубе,
вам сначала надо посчитать значение кси, а потом позвести в куб.
От конкретной точки омега.
А как-то другой вопрос какой был?
Мюката.
Мюката, да, точно.
Что-то такое было.
Мюката.
В общем, я не могу сказать, что это нужная вещь, если честно.
Смотрите, теперь зачем вот эта история нужна?
Потому что, если мат ожидания и дисперсия это вещи хорошие и понятные,
но вот их физический смысл, то моменты, нафиг эта фигня нужна.
Ну, то есть, первая реакция такая.
Ну вот, технически.
То есть, они, то есть, там вылезают вводе того,
что вы иногда, когда будете раскладывать там в ряд специальные функции,
у вас там коэффициенты будут, вот эти каты и моменты.
Но это и на тервере вы будете происходить.
То есть, смотрите.
Сейчас я текст придумываю, как тут аккуратно сказать.
Нет примеров просто в той математике, которую вот вы делаете.
Я когда, то есть, у меня самая такая вещь, когда я больше всего прокачался в математике,
я к аспирантуре готовил госэкзамен себе с дачи.
Потому что нам там нужно было вот весь тервер,
который рассказывали в университете и потом в аспирантуре,
вот его весь выучить.
А вы же, как люди, которые проходят через сессию,
вы понимаете, что когда вы садитесь, готовитесь к экзамену,
у вас все вот эти вот вещи, которые вы тут поняли, там поняли,
они складываются в единую картину дисциплины.
Это, собственно, моя главная претензия к Выше школы экономики заключается в том,
что они немножечко вот убили сессию.
Именно как вот момент, когда вы садитесь,
и 4 дня занимаетесь одной дисциплиной, и у вас она складывается в единое целое.
И в тот момент, извините, я просто издалека,
я не знаю, как по-другому объяснить.
Там вот был один раздел, я прям видел, как развивалась,
поскольку тот тервер, который учился, он в рамках XX века шел,
я видел, как люди приходили к той мысли, вот постепенно,
к тем результатам, которые они сделали,
и там было все понятно, как они пришли к этой мысли.
Так, кивайте. Хорошо? Окей.
Ну вот, потом я открываю, есть такая книжка Булинского-Шираева,
«Случайные процессы», скорее всего, она будет вам рекомендована
в списке литературы.
Шираев – это тот, кто ученик Кумогорову, про которую я уже говорил,
Булинский, это тоже у нас профессор тервера.
И там как, там идет теорема, и там доказательство теоремы,
оно такое техничное и очень быстрое, но совершенно непонятно,
как люди придумали, то есть это доказательство совершенно
не раскрывает содержание тех понятий, которые там заложены.
Ну то есть, как вот математика развивалась, а потом говорят,
чуваки, слушайте, я придумал классное доказательство этого же факта,
оно быстрее, лучше, за счет того, что оно быстрее и лучше,
оно уходит в массы, и студент начинает учить,
и студентов начинают через это учить, но при этом теряется
понимание, как люди пришли к этим результатам.
Сейчас понятно немножечко стало?
Вот теперь смотрите, в теории вероятности есть отдельные
математические приемы, с помощью которых эти результаты доказываются.
Например, метод характеристических функций, с помощью которых
очень легко и быстро доказывается центральная предельная теорема.
О ней, я надеюсь, сегодня не успею, в конце буду говорить.
Мы сегодня должны с тервером, по идее, закончить.
Ну вот, изначально там центральная предельная теорема,
она доказывалась там, сложно, тяжело, вот для медумиальной схемы,
ну вот, и прямо вот ты, так прочувствовав это доказательство,
понимаешь этот результат, а через метод характеристических функций
ты получаешь все очень быстро, легко, при этом мало понимаешь
содержание результата. Объяснил?
То есть мысль в чем? Тервер очень техничная наука,
поэтому, и вот эти, в частности, карты и моменты, они нужны для этой техники.
Не очень понятно, какой кроется, может быть, за ними смысл
и почему доказательства идут именно так, как они идут,
но я объяснил, почему так будет происходить в тервере. Хорошо?
Ладно. Это просто технический аппарат, который нам нужен.
Результаты, результаты, а дальше, понимаешь, мы же математики,
нам эти результаты надо доказать, причем так, чтобы все было аккуратно.
Ну вот, а дальше как? То есть есть тяжелые доказательства,
но которые, оно выставлено поколениями, и понятно, почему оно такое.
А потом кто-то придумал, слушайте, а зачем мы так мучаемся,
можно вот так вот зайти. То есть математики сейчас вообще адовая история,
тот вот сейчас Александр Скрипченко, она стала деканом матфака вышки,
может быть, кто-то слышал. Я ее даже учил какой-то момент
во второй школе. Ну вот, она занимается теорией узлов.
Оказывается, что эта теория узлов, она связана со всеми разделами математики,
которая вообще никакого, казалось бы, теория узлов,
на мой взгляд, это вообще фигня какая-то. Вот эти узлы, можно так, а можно так.
То есть детям рассказывать, она очень популярная, она детям рассказывать,
шикарно рассказывать, все здорово. Там какая-то высокая математика,
которая потом увязана в различные разделы математики.
Я не понимаю, как это происходит. Ну как бы я наукой давно не занимаюсь,
и мне сложно представить, как они умудряются связывать различные разделы математики,
которые, казалось бы, совсем друг с другом не связаны.
С помощью узлов, да, понятно. Ну вот. То есть это, на самом деле,
я говорил в самом начале об этом, о том, что математика,
если философски опускаться в эту историю, это очень сложно.
Я ответил на вопросы, хорошо? Ладно, все, я сейчас уйду далеко,
была боль в стороне. Давайте, свойство дисперсии.
Свойство дисперсии. Так, соответственно, первое.
Понятно, что считать дисперсию по определению сложно, просто потому что,
ведь для того, чтобы посчитать математическое ожидание,
мы с вами говорили, нам нужно знать распределение случайной величины.
У вас как получилась та случайная величина, которая стоит под скобочкой?
Вызвали свою случайную величину, вычили чиселку и возвели в квадрат.
Ну, как это происходит? Ну, как это происходит?
Ну, просто чтобы найти распределение, вот это измененное распределение
новой случайной величины, это будет тяжко.
Никто так, естественно, не делает.
И есть достаточно простая форма, мы ее сейчас выведем, и это будет результат.
То есть дисперсия определяется вот так.
Соответственно, здесь у вас под мат ожидания находятся квадраты разности,
начиная с седьмого, наверное, класса.
Ну, кто когда, вы знаете.
Формула для квадраты разности.
Соответственно, здесь мы можем, смотрим.
Поскольку математическое ожидание, это у нас есть линейный оператор
на множестве случайных величин,
то мат ожидания от суммы, это сумма мат ожиданий.
При этом мы видим, что вот эта случайная величина,
вот эта случайная величина, это чиселка и это чиселка.
Мы видим, что вот эта случайная величина, вот эта случайная величина,
это чиселка и это чиселка.
Давно я был в седьмом классе.
И это тоже чиселка.
Поэтому у нас здесь получается мат ожидания кси в квадрате,
минус двойку вынесли, мат ожидания кси вынесли,
а вот мат ожидания кси осталось.
Дальше здесь у нас получается мат ожидания от мат ожидания кси в квадрате.
Кого с кем?
Кого с кем?
Независимость, это там должна быть пара случайных величин.
Тут нет, она одна.
Поэтому нет независимости, не задумывайте.
Я напомню то, что я пользуюсь линейностью математического ожидания.
То есть когда мы считаем мат ожидания вот такой вот,
получается а, мат ожидания кси плюс b.
Ну, смотри, мы берем мат ожидания вот от этого произведения.
Что у нас здесь?
Двойка чиселка, мат ожидания кси чиселка.
Вот они вынеслись, две штуки.
А вот это вот кси получается мы берем от ожидания от одного вот этого множителя.
Вот у нас.
Третьим слагаемым давайте посмотрим.
То есть мы берем от ожидания от чего?
От чиселки.
Мат ожидания и чиселки к чему равно?
Самой этой чиселки.
В результате мы видим то, что у нас получается мат ожидания кси в квадрате,
в квадрате плюс мотождание икси в квадрате.
Ну, дальше мы проводим подобные слагаемые, это, наверное, класс
шестой, пятый.
И получаем вот такую форму.
То есть, для того, чтобы посчитать дисперсию случайной величины икси,
нам нужно посчитать мотождание его возрасте в квадрат и посчитать мотождание
от квадрата случайной величины.
А эта история нормальная, потому что вот
ну, мотождание мы умеем считать, а насчет мотождания квадрата мы с вами
помним, что если мы считаем математическое ожидание функции,
какой-то функции от случайной величины, при этом мы знаем распределение
случайной величины икси, то есть, у нее распределение принимает значение
хкат и с вероятностью мипкат, что посчитать вот такое мотождание
достаточно посчитать вот такой вот ряд.
Поэтому считать дисперсию мы всегда будем вот по этой формуле.
Я напоминаю то, что определение, можно было в принципе так и
определять, но я напоминаю, что определение должно скрывать суть объекта,
а вот отсюда вообще ничего не понятно. Хотя считать удобнее, конечно.
Что для того, чтобы посчитать мотождание откси в квадрате,
можно воспользоваться вот таким результатом, который мы...
А, там тоже был комментарий? Да, этого нет, извините.
Это продолжение доказательств.
А так сейчас кто-то пишет?
Нет, по-моему, ставят просто свыше и в строку пишут, нет?
Так, погнали дальше.
Второе.
Дисперсия случайночной кси строго больше нуля,
но это очевидно, просто все его определения,
поскольку это мотождание от неотрицательной случайночной.
И что важно, что если дисперсия случайночной оказалась равна нулю,
то это значит, что у нас есть суть,
а если случайночная оказалась равна нулю,
то что в этом случае можно сказать о случайночной?
Констант «почти наверное».
Потому что это термин действительно в анализе.
А у нас теория вероятности. Там «почти наверное».
Одно и частный случай другого.
То есть «почти наверное» – это частный случай почти всюду.
То есть мы с вами будем, когда в следующем разделе работать с мерами,
меры у нас могут принимать, во-первых, конечные меры,
то есть они могут… мера всего может быть любым числом, положительным, конечно.
Плюс у нас будут с вами сигмы – конечные меры,
когда у нас наше все пространство может иметь бесконечную меру,
но в чем сигма заключается?
Вы можете нарезать на счетное число непересекающихся кусочков,
и мера каждого кусочка конечна.
Ну, как прямая. Прямую можно нарезать на счетное число кусков,
и мера каждого конечна.
Нет, дисперсия – чиселка. Дисперсия – это…
Еще раз, смотрите, это характеристики, это числа.
То есть одна из проблем со случайными величинами,
которая заключается, непонятно, как с ней работать,
потому что у нее аргумент непонятный,
а мы хотим какие-то понятные вещи, которые в Excel-ку загнать можно.
Характеристики по этому – это все числовые истории.
Главная характеристика распределения,
которая у вас будет – это функция распределения,
просто в рамках нашей истории дискретной.
Функция распределения – интересная история,
там всегда будет кусочек линейной истории ступенчатая.
В серверу вы подробно с ними поработаете.
Потому что «почти наверное» – это значит,
что там, где она не константа, вероятность ноль.
То есть вопрос заключается, как доказать отсюда и сюда.
Давайте докажем. Я не планировал, но давайте докажем.
Пускай у вас кси – констант «почти наверное».
Давайте попробуем сначала подумать про математическое ожидание кси.
То есть у вас кси равно с почти наверное.
Что мы тогда можем сказать про математическое ожидание кси?
Ведь это что? Сумма по Омего, П от Омего на кси от Омего.
Но что значит, что кси равно с почти наверное?
Это там, где она не с, вероятности ноль.
Но это значит обнуляться эти слагаемые.
Поэтому я могу суммировать только по таким Омего,
что кси от Омего равно с.
С на П от Омего.
С вынесется за скобочку. У меня получится сумма по всем Омего таким,
что кси от Омего равно с.
Но в силу этого замечания сумма вероятности таких Омего маленьких – один.
То есть я получил С.
Дальше что у меня получается?
У меня получается, что я считаю математическое ожидание кси
минус мат ожидания кси в квадрате.
При этом что я могу сказать об этой разности?
Она равна нулю почти наверное.
То есть если я сейчас аккуратно выпишу ряд
по всем Омего, П от Омего на кси от Омего минус с в квадрате,
то получается у меня либо этот множитель ноль, либо этот множитель ноль.
То есть кси минус мат ожидания кси, который есть с.
Оно ноль почти наверное.
Но это что значит?
Что там, где не ноль, вероятность Омега ноль.
Получается либо этот множитель ноль, либо этот множитель ноль.
По-моему я чрезмерно подробно рассказываю.
Ну и все, получился нолик.
Ну в обратную сторону так же.
Так что с дисперсией тут надо как-то чтобы это в кровь вошло.
Потому что если дисперсия нолик,
это значит, что случайная величина у вас констант.
Только не говорите пожалуйста, что на ноль.
Третье это, что происходит с дисперсией,
когда у нас есть линейные преобразования случайной величины.
Какой ответ?
Ну тут сообразить несложно.
Еще?
А в квадрате?
Б уходит.
Ну давайте идейно сначала.
То есть что произошло?
То есть параметр масштаба.
То есть если мы кси увеличили в несколько раз,
во сколько же раз у вас увеличится им от ожидания кси,
ну это понятно.
А значит вот этот квадрат увеличится в квадрат раз.
Ну и собственно дисперсия увеличится в квадрат.
Теперь параметр сдвига.
То есть что мы сделали?
Мы сдвинули кси на б.
Всю кси на б.
Понятно, что им от ожидания так же съедет на б.
Поэтому их разность не изменится.
И поэтому соответственно средняя тоже не изменится.
Можно доказательство сами.
Ладно?
Надо просто преобразовать вот эту вот историю.
Убедиться, что b здесь сокращается.
А сначала выносится вот здесь за скобочку.
Потом в квадрате и вылезает под знаком от ожидания.
Хорошо?
Я не буду доказывать.
Пока все.
Дисперсии пока все.
Переходим к следующему объекту.
Это ковариация двух случайных величин.
То есть пока у нас с вами была характеристика.
Мы говорили о характеристиках одной случайной величины.
Теперь у нас с вами есть пара случайных величин кси и b.
Так, первые, вторые, третьи свойства дисперсии.
Дисперсия была вторым, значит сейчас третья.
Ковариация случайных величин.
Ковариация.
Случайных величин.
То есть отличие в том, что у вас их сейчас есть две кси и b.
Соответственно, определение.
Ковариация пары случайных величин
будет называться такой объект.
Вы считаете мат ожидания
от произведения центрированной кси и центрированной b.
Что такое операция центрирования?
Вы из своей случайной величины
путем вычитания чиселки делаете
те случайные величины мат ожидания, которые ноль.
То есть понятно, что если возьмете мат ожидания
вот этой дырани, это будет ноль.
Это понятно?
Ну и отдельно этого тоже ноль.
Дальше вы их умножаете
и берете средние.
Вот это вот называется ковариация двух случайных величин.
Соответственно, какие свойства?
Свойства первые.
Опять же то, что считать вот эту вещь,
даже когда вам дано совместное распределение
двух случайных величин, это сдохнуть.
Поэтому есть простая форму.
Для того, чтобы посчитать ковариацию,
вам нужно посчитать математическое ожидание
произведения двух случайных величин
и вычесть из них произведение математических ожиданий.
Доказательства в точности такой же
можно и я не буду это делать.
Отличие только в том, что мы здесь раскрывали квадрат,
а там нужно просто раскрыть скобки.
А так все тоже самое будет.
Окей?
Второе.
Так, после формулы у нас
еще что было вот здесь вот.
Что происходит с линейными прообразованиями?
Я не буду расписывать,
а с вами скажу, что ковариация
есть белинейная форма.
Есть белинейная форма
на множестве случайных величин.
На множестве случайных величин.
Ковариация есть билинейная форма.
Это было же, да?
Это чего? Аугиброгеометрия, да?
А кой второй семестр? Первый?
Это второй семестр.
Ну, то есть это значит то, что
если я вот здесь вот
поставлю какую-то линейную комбинацию,
давайте одну вещь распишу,
ну, то есть если я здесь делаю
альфа-1, кси-1 плюс альфа-2,
то есть если я здесь делаю альфа-1,
кси-1 плюс альфа-2, кси-2,
это,
то это будет что такое?
альфа-1 на ковариацию,
кси-1 это,
плюс альфа-2 на ковариацию,
кси-2 это.
Ну, это как-то пример.
Что такое?
Будет линейность по первому адгументу,
ну, по второму тоже.
Ну, и какое тут замечание, кажется, логично
в этот момент?
Дисперсия оказывается
квадратичной формой,
порожденной этой билинейной формы.
У нас ведь всегда, когда есть
билинейная форма,
она автоматически порождает квадратичную форму,
когда вы берете билинейную
вектор от самого себя.
У нас ведь квадратичное
на линейное пространство определялось, да?
Ну, да.
Поэтому у вас получается
третье замечание, что
у вас дисперсия,
это есть ковариация,
дисперсия это
билинейная форма, которая,
дисперсия это квадратичная форма,
которая соответствует этой билинейной форме.
Типа, ну и что такого, да?
Тут это подводочка,
вы не думайте, что это просто так.
Смотрите.
Четвертое.
Когда я
хочу посчитать дисперсию
суммы двух случайных величин?
Пока я ничего там не знаю
о их независимости.
Что утверждается?
Что для того, чтобы посчитать дисперсию суммы двух,
нужно посчитать дисперсию первой плюс дисперсию
второго плюс удвоенную
ковариацию первого на второе.
Доказательство этой истории, оно такое же
техническое, давайте его проведем.
То есть, что такое дисперсия
кси плюс это?
По определению. Это математическое
ожидание квадрата
разности. Чего?
Вашей случайной величины и ее математического
ожидания.
Ну, соответственно, тут можно перегруппировать
слагаемые под знаком квадрата.
У нас получится мат ожидания
кси минус мат ожидания
кси плюс это
минус мат ожидания эт.
Дальше опять седьмой кваз,
в котором, как мы выяснили, я не
силен.
То есть это
квадрат первого слагаемого,
но если мы возьмем от него мат ожидания, мы
получим дисперсию. Дальше квадрат второго
слагаемого, если мы возьмем от него
мат ожидания, мы получим дисперсию эт.
И дальше будет удвоенное
произведение первого на второе. Если мы возьмем
от него мат ожидания, там двойка вынесется
и мы получим две ковырятся.
История ясна?
Хорошо.
Ну, легко понять, что когда вы будете считать дисперсию
суммы случайных величин, то вам нужно будет просуммировать
дисперсии этих случайных величин и еще прибавить
две суммы всевозможной кавариации, ксиито и ксижито.
Ну, понятно, что доказываться будет точно так же, точно
так же вы будете раскрывать здесь квадрат, у вас будет
сколько-то слагаемых.
Вот так вот.
Прям интерриг, да?
Ну, то есть он мне даже помогает, он нагнетает, да?
К чему же все это идет?
Пункт пятый.
Давайте посмотрим на эту кавариацию.
Давайте вот, смотрите, вы как люди, которые хорошо
знают прошлую лекцию, что можете сказать вот о таком
выражении?
Ну, какой-то ассоциативный ряд у вас возникает?
Мотожнание произведений, произведение математических
ожиданий.
Ну, что-то там было про независимость, да?
То есть мы говорили, что когда две случайные величины
независимы, то произведение, то мат ожидания произведения
случайных величин равно произведению мат ожиданий.
Было такое свойство.
Отсюда вывод, что если у вас случайная величина
независима, отсюда следует то, что кавариация двух
случайных величин равна нулю.
В ясности, я говорю, она к деталям все сильнее.
То есть если случайная величина независима, то кавариация
равна нулю.
Как вы думаете, что будет в обратную сторону?
То есть если кавариация равна нулю, то...
Ни фига подобного.
Во-первых, почти, наверное, независима – это неправильное
вещество.
Такого не бывает.
Независимость – это отношение, там только да или нет.
В обратную сторону это неверно.
Ну потому что что значит, что кавариация у вас равна
нулю?
Это какое-то там математическое ожидание ноль.
На самом деле понятно, что можно подкрутить значение
случайной величины и добиться того, чтобы математическое
ожидание у вас стало равно нулю.
Что-то вы меня так смотрите, как будто не верите.
А можно пример какой-нибудь?
Можно я его сделаю на окружности.
Вы меня простите, не дискретный пример.
Хорошо?
То есть вы в процессе изучения тервера будете сталкиваться
с этими вот постоянно.
Я его не помню, я сейчас буду подгонять, так что я заранее
прошу прощения.
Так, ну давайте, пускай Омега у нас будет от 0 до 2 пипа.
Вам придется поверить мне, что те как бы… А, че, у нас
тогда мат ожидания – это интегралы.
Это тяжело, нет?
Ну что, нам нужно, чтобы… То есть че я сейчас буду добиваться?
Я буду добиваться, чтобы все эти три хрени были нулями.
Ну просто, мне же нужно в итоге 0, чтобы получить 0,
чтобы просто все три должны быть нулями.
Хорошо?
Ну давайте.
Ну пускай у меня Си это будет косинус от Омега, а это
это будет синус от Омега.
Ну интуитивно понятно, что среднее должно быть 0.
Ну и у косинуса, и у синуса, потому что у вас косинус
симметрично относительно горизонтали, то есть у него
как бы одинаковые значения все время, поэтому средним
образом, если мы будем считать по всей окружности,
у нас получится 0.
Да?
То есть мат ожидания Кси – это у нас 0, мат ожидания
Эта, ну по тем же соображениям тоже будет 0.
Ну вот.
А теперь давайте подумаем, что такое произведение
двух случайных величин.
Косинус от Омега умножить на синус от Омега.
Это че такое?
Синус 2 Омега делись пополам.
Правильно?
А теперь посмотрим, то есть когда Омега у вас пробегает
от 0 до 2 пи, 2 Омега у вас че пробегает?
Ну вот, поэтому мат ожидания Кси на это – это тоже будет
0.
Ну то есть ковидриация их оказала все равно 0, потому
что просто там все 0 по формуле.
Вот это вот первый пункт, что в дисперсии, что в ковидриации
– это формула вычисления.
Теперь давайте подумаем, зависимы вот эти две хрени
или независимы.
Ведь тут проблема в том, что это непрерывные, да,
я так нехорошо делаю.
Ну, Косинус Омега и Синус Омега зависимы или нет,
как вы думаете?
Не, ну точку брать бесполезно, потому что вероятность
будет равна нулю, нам нужны множества.
Кто предложит множество?
Опять хорошо с нулями играться, да, ведь, ну то есть мысль
в чем?
Предполагается, подобрать два таких множества, извините,
множества, что не выполнено вот это равенство.
Для этого что надо сделать?
Я предлагаю проще с нулями, проще всего играться.
Давайте сделаем так, чтобы вот эти два были не ноль,
а вот это было ноль.
Хорошо?
А чтобы вот это было ноль, надо чтобы вот это событие
было каким?
Проще всего пустым.
Правильно.
Кого взять в качестве А и Б?
То есть нужно так, чтобы, ну, чтобы вероятности были
положительными, да, но при этом чтобы одномоментно
они не могли быть.
Понятно, что имеется в виду, да?
Ну то есть мы как бы говорим то, что вот у нас наша пина
четыре, но нам нужно говорить, что косинус попадает вот
сюда, а синус попадает вот сюда.
Пересечение этих двух даже можно взять больше или
бы равно, но будет по точке.
Но вероятность точки у нас ноль, то есть А это у
нас единица на корень из двух до единички.
Б – это единица на корень из двух до единички.
Тогда вот это у вас там будет...
Че, пИнадово, поучается.
Здесь пИнадово, здесь вот это будет пИнадово..
Че хрен не с Heh, пИнадово делить на 2 пИ.
ПИнадово делить на 2 пИ, пИнадово делить на 2 П.
То есть по четверти, да?
Да.
А это ноль.
потому что одномоментно эти события невозможны.
Вот, пожалуйста, вес случайной величины, который очевидно
зависимый и тихо нам подобрал такие А и Б, что вероятность
пересечения не равна произведению вероятностей.
При этом коваряция получилась нулем.
Просто мы там все нули получились.
Это же мот ожидания.
Сделать так, чтобы оно было нулем не так сложно.
В дискретном случае это тоже можно сделать, но
последние два года я говорил, типа, поверьте, все говорили,
ладно.
И как-то выходило.
Вы удовлетворены примером?
Ну, плюс-минус.
Хорошо.
В дискретном тоже можно подобрать, это несложно.
Да, Оля.
Мы же обычно в независимых случайных причинах делали
для одного исхода, для значения одного.
Ну, потому что это непрерывный случай, и у тебя, если ты
берешь равно конкретные чиселки, у тебя всегда
будут нули, и это не годится.
Ну, то есть, я не люблю в любых случаях поверить,
что они здесь, но это другое?
Да, конечно.
Еще раз.
Все, что мы делали, мы делали в дискретном.
Слушайте, в дискретном тоже подбирается.
Просто я так экспромтом не уверен, что я...
Мысль заключается в чем?
Надо подобрать.
Да, числа подобрать.
Просто здесь я понял, что сразу эти можно брать,
и все хорошо получится.
Вы понимаете, что на самом деле можно взять вероятность
на пространство с четырьмя точками и там так подобрать
значение вероятности, то что в результате все будет
хорошо.
Ну да, да.
Хорошо.
Все.
Так, кто понял, что сказали, ребята, и что это доводимо
до результата?
Никто не понял.
Ну и ладно.
Ладно.
В общем, мысль заключается в том, что если у вас случайно
величины независимы, только вариация у них равна нулю,
то в обратную сторону неверно.
В связи с этим...
Но, теперь смотрите, интрига, мы подходим к развязке.
Что мы с вами получаем?
Шестое.
Что если квадрация двух случайных величин равна нулю,
что выполняется, например, когда случайная величина
независима, то дисперсия суммы случайных величин
равна сумме дисперсии.
И вот в этот момент должно стать...
Вот, смотрите, вы же как смотрите, вот у вас математика
развивается, и, в принципе, все там логично происходит,
более или менее.
Вот в этот момент должен произойти ступор.
Почему?
Вот в этот момент.
Погромче.
По формулке светит просто.
Это да, это хорошо, но идейно.
Вы точно сейчас форму накируете.
Конечно.
Ведь у нас что получилось?
Все было здорово.
Что такое квадрация?
На самом деле, почему мы ввели квадрацию?
Потому что это та белинейная форма, которую у вас продуцирует
квадратичную форму, коей является дисперсия.
И все понятно.
И дисперсия себя ведет как квадратичную форму
хорошо, квадрация как белинейная, все здорово.
А потому что получается, что если мы накладываем
на слагаемые какое-то странное требование, то, что квадрация
ноль, оно вообще непонятно, но давайте возьмем независимость.
Если слагаемые независимы, то почему-то дисперсия
начинает вести себя линейно, хотя изначально это квадратичная
форма.
Что за фигня?
Ну, объективно.
И тут включается вот именно то, что я вам говорил.
То есть, Богачев, который сейчас ведет на продвинутом
потоке, он же мой семер по действительному анализу,
он сотрудник арте, теории, функции и функционального
анализа.
Он совершенно искренне считает, что самая главная наука
в математике это функциональный анализ, а все остальное
братье меньше, включая тервер.
И он нам это на полном серьезе, нам в группе тервера этого
объявлял на семинарах.
Потом он делал такую паузу и говорил, ну да, вот в тервере
есть одно, вот это понятие независимости, и оно делает
тервер как бы отдельной дисциплины, в нем суть.
И вот здесь это явно выражается.
У вас будут постоянные отсылки тервера к функциональному
анализу.
Понятно, что там действительно анализ, это такой подкласс
функционального анализа, но все время будут отсылки.
Тоже самое не анализ Вакаши Буниковского, это же по сути
то же самое.
Но когда влезает независимость, а если вы будете заниматься
тервером, там разные виды зависимости, там, слабая,
сильная зависимость между случайными величинами,
там как бы вся вот эта математика, которая была до этого, она
летит, летит в плане логичного представления о структуре.
Вы осознали?
Окей.
Теперь смотрите, в силу пятого у нас что получается?
У нас получается то, что квариация является понятием
более слабым, чем независимость, но при этом его достаточно
для того, чтобы дисперсия вот так вот вела себя прикольно.
Хорошо?
Поэтому вот в этот момент водится отдельное понятие
некоррелированных случайных величин.
Так, определение.
То есть случайные величины ксиэт, случайные е, величины
ксиэт называются, называются некоррелированными, некоррелированными,
если их квариация равна нулю.
Понятие корреляции у нас сейчас тоже будет, не пугайтесь.
В конце я сегодня постараюсь успеть объяснить, зачем
вот такое отдельное понятие некоррели...
Сейчас конспект, момент.
Знаете, там во время всяких публичных выступлений
надо всегда ссылаться на классику.
Но это удобно, потому что с классиками невозможно
пользоваться, поэтому я сейчас сошлюсь на величайшую
художественную произведение человечества, это серия
его папины дочки.
Там в какой-то момент, Галина Сергеевна, вы наверно в
ваше поколение вообще не знаете, что это такое, да?
Ну это...
Да?
Ну вот, в какой-то момент Галина Сергеевна, когда
Маша поступила в Баунку, она там говорит, что Маша
это девушка абсолютно некоррелированная из физики.
Понятно, что сценаристы использовали это для того,
чтобы как-то придать наукообразности речи Галины
Сергеевны.
Ну вот.
Но вот эта вот история, она очень болезненная для
прикладников.
В чем заключается история?
То понятие независимых случайных величин именно
с точки зрения математики этой истории очень тяжелая.
Вот мы сейчас с Олей дискутируем, потому что я, правда, тамстер,
то что если случайные величины у вас, они будут в общем случае
определены, то как будет определяться независимость?
Для любых баррельских множеств А и Б, этих множеств вот
до черта.
Для любых этих множеств вероятность того, что кси
принадлежит А, это принадлежит Б, должно распадаться в
произведение вероятностей.
Ну то есть проверить вот эту историю, это ну просто
сдохнуть, потому что А и Б, таких А и Б у вас очень
много.
Вопрос некролированности случайной величины кажется
очень простым.
Вопрос-то надо посчитать какое-то там математическое
ожидание.
Вы будете этим много заниматься на семинарах, и это не будет
представлять никакой проблемы.
Надо просто знать совместное распределение этих двух
случайных величин.
Вы посчитаете это матожедание, посчитаете это, посчитаете
это, чиселку получится.
Это первая привлекательность к вариации.
Вторая привлекательность к вариации заключается
в том, что статистически эта вещь оцениваема, то
есть потом, когда после тервера у вас будет математическая
статистика, там будет прям целый раздел, как вот эти
вот характеристики по наблюдениям примерно прикинуть.
И будет всякая теория, то есть насколько точно вы
своими вот этими оценками их приближаете.
Будет такое понятие выборочное среднее, то есть у вас вот
это вот математическое ожидание, а можно посчитать
выборочное математическое ожидание по тем наблюдениям,
которые вы получили.
Что происходит дальше?
Сидят эти несчастные прикладники, химики, биологи всякие,
для которых вся эта математика нафиг не нужна, всё это грустно
и скучно.
И как проверять независимость вообще непонятно, то есть
вот у них есть какие-то показатели, которые они
снимают.
И вот им надо понять, вот эти показатели друг от
друга зависят или они друг от друга не зависят.
Очень поверхностная задачка же, вот как и как это делать.
И они считают так называемый коэффициент корреляции.
Выборочный, конечно, не наш теоретический, который
мы считаем.
Через год вам объяснят, что такое выборочный, поэтому
сейчас давайте его введём.
Они их по наблюдениям считают.
То есть, предположим, вот у них есть, смотрите, история
какая.
В XL, везде есть вот эти вот коэффициенты, вот если
вы XL откроете, господи, вы же второй курс, вы слишком
мелкий.
Когда был, в какой-то момент, Яндекс сделал свою олимпиаду
на ФКН.
Для нас это был удар под дых, понятно, пишу, в общем,
ФПМИ это было тогда очень больно.
Мы сказали, слушайте, давайте на ФПМИ, Яндекс тоже сделает
свою стипендию.
Они говорят, окей, давайте сделаем, но придумайте нам
какую-нибудь вот такую интересную стипендию, мы не хотим просто
давать там завсе раз, сколько можно.
Всех этих стипендий завсе раз, если сложить там столько
солей получается в месяц.
Но придумайте что-нибудь интересное.
Они сказали придумывать мне, я придумал там какую-то
комплексную историю, то, что на входе за олимпиады
за школьные достижения, а потом там типа вот ребята
учатся, и там нарастающая идет оценка, то есть прибавляются
ваши активности в учебе, в преподавании, в публикациях
и во всяких олимпиадах типа ACPC, и будет вот эта вот накопительная
история.
И моя цель заключалась в чем?
Мне нужно было показать, почему надо учитывать олимпиады
при дальнейшей оценке.
Я говорил, что если человек крутой олимпиадник, он потом
будет хорошо учиться.
И я что сделал?
Я просто посчитал коэффициент корреляции между баллами.
Мы на приемке, у нас такой есть ваш рейтинг, вы его
наверное не видели, он такой тайный, ну то есть там за
каждую олимпиадку назначаются баллы и суммируются.
И у каждого абитуриента есть балл, ну то есть числовая
история хорошая.
И потом у нас мы выкачали, как вы учитесь, ну не вы,
а как бы те ребята, потому что олимпиад уже там сколько,
три года, степень уже три года существует.
И дальше выборка, а выборка огромная, это ФПМИ, господи,
но тех, кто поступает по конейми по бюджету, там
300 человек, репрезентативность хорошая.
И я считал коэффициенты корреляции, я показывал,
говорю, смотрите, как хорошо зависят успехи ребят потом
во время учебы от их олимпиадной активности, а от проектной
зависит вообще от отрицательной корреляции была, да, типа
те, кто пошли в бизнес, пускай там и остаются, нефиг им учиться
и в разработку.
То есть это история, которая считается по результатам,
то есть было 300 наблюдений и две чиселки, понятно,
да, рейтинг и потом дальнейший, нет, там было даже, там как-то
я смотрел, как учатся потом в течение времени, там было
несколько семестров.
Так, корреляция двух случайных величин.
Так, определяется это как, то есть корреляция случайных
величин кси, это определяется, как частные корреляции
их делить на корень из дисперсии кси.
То есть понятно, что эта вещь определяется, когда
дисперсии не равны нулю, ни та, ни другая.
Ну, давайте...
А какое обозначение корреляции?
Корреляция с двумя R, я прошу прощения, дикция.
Соответственно, основная мысль, которая есть в этой
истории, это то, что теорема, это что корреляция двух
случайных величин по модулю меньше или равна 1.
При этом И выполнило следующее.
Корреляция будет равна 1 тогда, когда существуют
чиселки А и В, причем А больше нуля, что кси равняется
А это плюс В, и корреляция равна минус 1 тогда и только
тогда, когда существуют чиселки А В, где А меньше
нуля, что кси равняется А это плюс В.
То есть если корреляция принимает свои крайние
значения, они просто линейно зависимы, одна выражается
через другую, причем неважно кто.
Какие-то есть, это сдвиг, он ни на что не влияет.
Вот в этот момент складывается очень симпатичная картинка.
Смотрите, если случайная величина независимая, то
корреляция 0, значит корреляция тоже 0, то есть мы посерединке.
Если у вас лучшая степень зависимости, линейная зависимость,
то корреляция принимает свои крайние значения.
И складываются вообще очень приятные истории.
То есть, господи, мы получили числовую характеристику
зависимости, степень независимости двух случайных величин.
Если мы посерединке, они независимы, вообще кайф.
А если мы по краям, то зависимость такая, самая контовая, но
то есть линейная.
Кайф?
Но это не так.
Только что мы с вами просто сходу, причем таких примеров
будет очень много, показали, что если корреляция равна
нулю, это не значит, что случайные величины независимы.
То есть они даже зависимы, понятно, вот косинус и синус,
понятно, что это зависимые характеристики две.
Но беда заключается в чем?
То, что вот эта вещь считается и оценивается, и в статистике
вы будете заниматься оценками.
И есть эти несчастные прикладники, которые очень хотят найти
вот эту вот степень зависимости.
Будете, когда заниматься этим своим машинным обучением,
ну вот, и вы будете там в частности проходить историю
с регрессией.
Ведь что такое регрессия?
Это то, что у вас линейная зависимость между признаками.
Но беда состоит в том, что это неверно.
Есть следующий корреляцион, то есть что неверно?
Неверно говорить о том, что если у вас корреляция
близка к нулю, это значит то, что у вас эти ваши характеристики
независимы между собой.
Это неправильно.
Они поэтому и придумали отдельный термин «не коррелированный».
Что в этом случае, то есть, я же разговаривал всегда
у меня родители доктора химических наук, я у них
спрашивал, я говорю, вы эти штуки читаете?
Он говорит, ну да, как же, мы строим зависимости,
там все хорошо.
Я говорю, а как вы это объясняете?
Ну вот они, то, что они там зависимы слабо или сильно
зависимы в зависимости от того, какой вот коэффициент
корреляции, близкий к нулю или близкий к единиц.
Я же тоже, когда, ну я не Бунин и, конечно, писал в тот
момент Евгением, Женем, в общем, которые всеми образовательными
проектами Яндекса, я и посчитал их коэффициентом корреляции,
говорю, вот, смотрите, больше половинки, значит хорошо.
Ну то есть такая зависимость хорошая.
Ну вот, и вот тут, на самом деле, это очень иронично,
то есть ты открываешь даже википедию, я сегодня, когда
готовился, я открыл википедию, открываете статью корреляции
и читаете, и там написано, то есть если корреляция
ноль, это не значит то, что они независимы.
Вообще нет.
Ну вот, и если корреляция близка к единице, это не
значит то, что один зависим на другом, там может быть
очень сложная история.
Но как они это объясняют, и как я это понимаю, то,
что прикладники, я же как бы не прикладник, они
говорят, что если корреляции такие, то это повод для
нас задуматься, что зависимость должна быть такой.
Идея ясна?
Я это к чему?
К тому, что будьте, пожалуйста, аккуратны, то есть всякий
раз, когда вы будете сталкиваться с вот этими двумя понятиями
независимости и некоррелированности, во-первых, четко их отличайте,
понимаете, почему все носятся с этой некоррелированностью,
потому что с ней работать легко.
Ее и посчитать можно в теоретическом случае очень
просто, а в случае прикладном, там вот эти выборочные
моменты можно посчитать, ее оценить можно тоже достаточно
легко.
Эксельку открываете, там есть этот выборочный коэффициент
корреляции, но это не независимость.
Так, теперь нам вот этот вот результат надо доказать,
что это да, это все правда.
Я сегодня опять не успею.
Так, погнали, доказательства.
Водим две случайно вечные.
Кси-штрих, который есть, что мы делаем, мы кси сначала
отцентрируем, а потом нормируем.
И это штрих тоже самое, мы ее сначала центрируем,
а потом нормируем.
Поднимите руку, кто четко понимает, что кроется за
свалами, центрируем и нормируем, и что я действительно это
сделал, что я отцентрирую, отнормировал.
Что-то не все поднимаю.
Ну давайте посмотрим, чему равно от ожидания кси-штрих.
Эта чиселка просто выносится за знак математического
ожидания.
У нас получается вот так.
Ну это ноль.
Да?
Окей.
Теперь, когда мы считаем дисперсию кси-штрих, соответственно,
для того, чтобы посчитать дисперсию, нам что нужно?
Нам нужно посчитать математическое ожидание квадрата случайно
ввечной минус ее от ожидания, но от ожидания у нас ноль,
поэтому можно сразу писать, переписывать просто случайно
Да?
Теперь смотрим.
Это же чиселка, да, а мы с вами знаем то, что чиселка
сначала выносится из-под квадрата, уходит к коренью, а потом
она выносится из-под мат ожидания, будет просто единицей
дисперсию кси.
Так, у нас получается просто мат ожидания кси-мат ожидания
кси в квадрате.
Так это и есть дисперсия, она сократилась, получилась
1.
То есть мы добились того, что вот этой процедурой
центризования мы сделали так, чтобы мат ожидания
было ноль, а нормирование, чтобы дисперсия была 1.
Хорошо.
Дальше давайте посчитаем дисперсию кси штрих плюс
минус это штрих.
Как только бы недавно мы с вами выяснили то, что
дисперсия суммы в общем случае тут же про независимость
ничего не сказано, но мы можем расписать по той формуле,
что было.
То есть это дисперсия кси штрих плюс дисперсия
это штрих, плюс минус две кавариации, кси штрих это
штрих.
Ну дисперсия эта и эта это двойки, два я вынесу за
скобку, у меня получается единица, плюс минус.
Теперь давайте смотреть вот на эту кавариацию.
Ну вот, соответственно, чтобы посчитать кавариацию
двух случайных величин, мне что нужно?
Мне нужно мат ожидания произведения, из него вычесть
произведение мат ожидания, но мат ожидания каждой штуки
ноль.
Поэтому вот этой штуки не будет, просто мат ожидания
произведения.
А кавариация и белинейная форма они вынесятся из-под
кавариации.
И останется мат ожидания произведения вот этих штук,
а это что?
Это кавариация.
То есть у нас получилась здесь ковариация кси и эт,
делить на корень с дисперсией кси, корень с дисперсией эт.
Да?
Ой, это ж она!
То есть у нас получилось 2 единицы плюс-минус корреляция
кси и эт.
Понятно, что первое утверждение теоремы я доказал.
Поднимите руки, кто видит, что первое утверждение теоремы я доказал.
Ну, у нас же дисперсия, дисперсия штука не отрицательная.
Всегда.
Правильно?
А здесь у нас что получается?
Единицы плюс-минус и плюс-минус, как бы это я обоих вариантов расписывал.
Но понятно, что корреляция не может вылезти за пределы отрезка,
потому что если она больше единицы,
единицы минус корреляция становится отрицательным,
а если она меньше минус единицы, единицы плюс корреляция будет отрицательной.
Всё.
Поэтому первый результат, что корреляция по модулю меньше равна 1, мы показали.
Но куда интересен вот этот вот второй результат?
Потому что чем он интересен?
Именно с тем, что это единственная ситуация, когда мы хоть что-то можем утверждать наверняка.
То есть если у вас корреляция оказалась равна 1, значит они зависимы, даже мы знаем как, линейно.
То есть это единственная история, когда у нас что-то мы можем утверждать.
Так, погнали.
Ну, смотрим.
Я, с вашего позволения, разберу один вариант с единичкой давайте.
Или минус единичкой?
Ну, не важно, на самом деле.
Пускай корреляция равна 1.
Это что значит?
Это значит, что единицы минус корреляция 0.
Правильно?
Это значит, что дисперсия разности,
она равна 0.
Согласны?
А мы с вами недавно выяснили, что дисперсия у нас 0, тогда и только тогда, когда что.
Ну, это не правда, С.
Констант.
Почти, наверное, правильно?
Ну, кстати, я его там наврал, вот здесь надо было естественно записать, почти наверное.
На самом деле всегда в теории вероятности, когда вы будете утверждать равенство или не равенство,
вот что-то по случайной величины,
там всегда будет почти наверное,
потому что вы измените случайную величину в каком-то количестве точек мира,
которых 0, и вы никак это не сможете почувствовать.
У вас все такие значения есть, и вы не сможете их почувствовать.
точек меры, которых ноль, и вы никак это не сможете почувствовать.
У вас все характеристики, которые вы пишете, останутся теми же.
Давайте смотреть, что такое кси с чертой. Это есть кси, минус мотоожидание кси,
делительная коренность дисперсии кси, это у нас это, минус мотоожидание это,
делительная коренность дисперсии это и это с. Можно я не буду выписывать вот те а и b.
Что такое будет а? Это будет с, домноженный на дисперсию кси, плюс, понятно? Видно,
что кси и это выражается, потому что случайная величина здесь только вот это и вот это.
Все остальное это чиселки. То же самое будет, если у вас будет минус единица,
у вас здесь будет плюс, здесь будет плюс, и все. Что ты дашь? Вот здесь вот? Потому что,
коверяция расписываешь, аккуратно. Что такое коверяция? Это мотоожидание произведения
минус произведение мотоожидания, но
Вот мы его здесь посчитали.
Поэтому это есть просто мотождание, кси минус мотождание
кси, делить на конец дисперсии кси, это минус мотождание
это, делить на конец дисперсии это.
Всё.
А вот эта вот штука, извините, вот тут умножить, умножить.
Это же чиселки, множители, они выносятся из-под знака
мотождания.
Ну и всё, у тебя остается просто мотождание, произведение
вот этих двух разностей.
Это просто определение коваряции.
Вот.
А эти штуки остались в знаменащинах и неожиданно
получили корреляцию.
Ну то есть, ну подогнали.
Вот если честно, я когда, ну сколько-то лет назад
я готовился, ну то есть как бы вот всё, что я вам говорю,
я просто вот как с куста гладлю, да, а я в какой-то
момент готовился к лекции и я думал, чёрт, а как это
доказывать?
Ну вот именно линейность, да, вот эту вот.
И я так думал, а как?
Вот именно то, что равно почти, наверное, блин.
И меня пришлось в Шираево, в Шираево лезть.
Ну вот, это как бы с доказательствами Шираева, оно же такое непрозрачное
абсолютно же.
Ну то есть реально, что мы сделали, да, мы выписали,
у нас тут типа получилось два плюс два, плюс-минус
два, вынесли за скобку, всё хорошо.
Ну догадаться до такого решения, ну как его придумать,
я не знаю.
А как объяснить по поводу мотождания ксиш-3, почему
ноль получается?
Ну потому что у тебя вот эта скобочка, мотождание
этой скобочки ноль, у тебя же мотождание линейное,
да, ты берёшь мотождание это, минус мотождание это,
но это число, поэтому мотождание, оно само, у тебя получается
екси минус екси, это ноль, вот, теперь зачем всё это
нужно?
Так, я надеюсь, про некоррелированность вы осознали.
Я тут тоже смотрю, каждый повышает настроение как
можно, я начал смотреть, пересматривать детство
Шелдона, там Шелдон доказывал этому пастеру о том, что
он говорит.
Вероятно из того, что Бог есть, 50 процентов, Шелдон
говорит ха-ха.
Вот, теперь зачем всё это надо, это всё, да, у нас
остались законы больших чисел и центральная преддельная
теорема, я прошу прощения, я что-то опять балаболил,
надо было терверы закончить, но мы его закончим, у нас
остался закон больших чисел и центральная преддельная
теорема.
То есть, говорят, то, ради чего весь тервер и существует,
который обосновывает применение в жизни.
Спасибо огромное, что пришли.

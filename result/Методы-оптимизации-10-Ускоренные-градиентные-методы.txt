Смотрите, сегодня мы говорим про ускоренные методы.
Для того, чтобы понять, что это за ускоренные методы,
нас сейчас понять, относительно чего мы их ускоряем.
И для этого я все-таки хочу привести аккуратное доказательство
необходимости гарнитного метода для просто выпуклой задачи,
чтобы вы примерно понимали, почему в случае, когда мы выпуклые с ней,
используем у нас была просто сходимость на стороны точки.
А когда мы выпуклые сначала использовали,
то у нас внезапно появилась сходимость к решению с определенной скоростью.
Я вижу три включенных микрофона.
Вы хотите что-то спросить?
Или так случайно получилось?
Случайно.
Окей.
Ну давайте еще раз второе утверждение.
Утверждение такое, что если у нас есть выпуклая или гладкая функция,
и размер шага, который мы используем меньше единиц на L,
тогда будет вот такая скорая сходимость.
То есть это все у большой единиц на K.
Почему здесь возникла K и из чего это будет следовать,
сейчас пытаемся разобраться.
Для этого сначала вспомним, что у нас было прекрасное неравенство,
следующее только из-за L-гладкости.
Липшица власти градиента.
Мы его в прошлый раз достаточно быстро и не очень сложно получить.
Теперь давайте внимательно посмотрим на вот это
и будем его оценивать немножко по-другому в сравнении с тем,
как мы это делали в прошлый раз.
Оцениваем только компоненту, который с вынесенной альфой.
Эта штука меньше либо равна,
ну вообще понятно, что она просто равняется L альфа пола минус 1.
Ну а раз у нас альфа ограничена сверху величиной 1 с на L,
то эта штука меньше либо равна, чем на вторая минус 1,
то есть минус одна вторая.
Дальше мы это все подставляем в оценку.
Получаем, что у нас и плюс один меньше либо равно, чем f от x и
минус альфа пополам норму градиента.
Это пока первое важное неравенство.
Дальше вспоминаем, что у нас есть выпуклость,
которая есть в критерии первого порядка.
Запишем его для точек х со звездочкой и x и.
Вот, то есть вот так.
Дальше идет градиент и х со звездочкой минус x и.
До этого момента все понятно?
Или какие-то вопросы есть?
Ставьте плюс, есть все понятно.
Окей, вроде вопросов пока нет.
Значит, описали критерии первого порядка для выпуклой функции,
а дальше подставляем его, да, ну в общем, можно переписать,
перефразируем, что называется,
что f от x и-то меньше либо равно, чем f от x со звездочкой
плюс скалярное произведение x и-то минус x со звездочкой.
То есть перенесли знак, поменяли в скалярном произведении,
вроде ничего не сломалось.
Потом подставляем вот это вот сюда,
и получаем прекрасное выражение о том,
что у нас оказывается x от x и плюс первое меньше либо равно
чем f от x со звездочкой
плюс вот это вот прекрасное скалярное произведение
и плюс, точнее, минус полам нормы нашего градиента.
Вот, то есть подставили, все хорошо.
Теперь, собственно, введем разность наших функций в одну сторону,
все остальное в другую сторону,
и если вы внимательно посмотрите,
то здесь можно заметить, что вот эта штука очень сильно
напоминает неполный квадрат.
Сейчас будем к нему стремиться.
Что у нас есть?
У нас есть f от x и плюс один минус f от x со звездочкой,
а тут меньше либо равно мы еще вот так сделаем.
Поделим на, ну то есть вынесем единицы делить на два алифа,
чтобы у нас тут заодно правильные коэффициенты начали появляться.
То есть идея очень простая – получить полный квадрат.
Сейчас мы его и получим.
Вот так.
То это такое.
То есть если у нас тут альфа на…
А что?
Альфа на градиент – это первый множитель,
то есть что получается?
Что мы…
Ну, наверное, тут будет минус стоять, раз тут с минусом, да?
Альфа на градиент – это первый множитель,
а тут дальше противоположный знак,
поэтому будет идти минус.
Минус что?
Минус x со звездочкой,
а минус x со звездочкой,
плюс x и т.
Понятно ли, откуда взялось это выражение?
Или нужно подробнее расписать, что куда мы прибавили?
Если выражение понятно откуда взялось,
поставьте плюс, пожалуйста,
чтобы я мог синтироваться,
насколько подробнее нужно переписывать.
М?
Окей, спасибо.
Ну и соответственно что?
Раз мы там добавили квадрат нормы разности,
то его надо будет чистить.
Вот.
Поэтому…
Наоборот, там, когда выносился минус,
давайте я его вынесу,
появляется плюс и появляется минус.
Вот.
Соответственно…
Да, мы прибавляем и вычитаем.
Вот.
Но поскольку мы учитываем вот это вот
1 на 2 альфа здесь уже с плюсом,
тут появится минус.
То есть плюс.
Квадрат нормы x и t – минус x со звездочкой.
Вот.
Смотрим теперь сюда и понимаем, что…
Что мы понимаем?
Ой, я минус не там поставил, извините.
Так и вот так.
Хорошо.
Понимаем, что вот это и вот это вместе
образует не что иное, как x и плюс первое.
Есть на 2 альфа.
Здесь будет норма x и минус x со звездочкой.
Минус норма x и плюс первое,
минус x со звездочкой.
Вот.
Получаем вот такое выражение.
То есть слева у нас стоит разность функций,
справа у нас стоит разность норм
по отношению к x со звездочкой.
Теперь.
То, что понимает про такие неравенства
и что с ними можно сделать.
Это телекопическое неравенство.
Именно так.
Именно так, да.
Надо все дело просуммировать.
Вот.
Ну там по i от нуля до, например, там k-1,
чтобы было ровно k.
Вот.
Просуммируем вот это.
Какое еще не телескопическое неравенство.
Вот.
А вот при суммировании того, что стоит справа,
останется только…
Что останется только?
x0 минус x со звездочкой
минус xk минус x со звездочкой.
Все остальное благополучно взаимно уничтожится.
Вот.
Ну и раз тут мы вычитаем из них
чего-то положительно,
вот это меньше либо равно,
чем 1 на 2 альфа
норма x0 минус x со звездочкой.
Теперь осталось с едой частью разобраться,
чтобы получить то, что нам нужно.
Ну, для этого мы воспользуемся тем,
что мы знаем,
знаем, что мы каждый раз хотим убивать,
что на каждой итерации у нас значение функций меньше.
Поэтому распишем f от x
k-t минус f от x со звездочкой
следующим образом.
Это то же самое, что 1 на k
сумма по i от 0 до k-1
этого же самого выражения.
Ну, то есть умножили и поделили на k,
просто потом занеся выражение с функциями,
которые от i никак не зависят.
А дальше мы узнаем,
что f от x k-t у нас всегда меньше либо равно,
чем f от x it для всех i
от 0 до k-1 получается.
Ну, и до k в том числе.
Вот.
Поэтому эта штука меньше либо равна
f от x i плюс 1
минус f от x со звездочкой
и от 0 до k-1.
Вот.
И это, в общем, вот это выражение,
которое мы тут получили,
в точности совпадает с вот этим выражением.
То есть мы теперь знаем,
что на самом деле это выражение
снизу можно дополнительно оценить,
как a умножить на вот эту разность.
Вот, а значит, сама
вот эту разность.
Значит, сама эта разность
f от x k-t минус f от x со звездочкой
будет меньше либо равна,
чем то, что мы здесь получили,
делить на 2 альфа умножить на k.
Вот, получили такое самое.
Понятно ли процедура,
которую мы исследовали
и финальный результат?
Поставьте плюс, если все понятно,
и минус, если есть какие-то вопросы.
Пока вижу два плюса,
дела у остальных три,
видимо, не плюс.
Окей.
Так, здорово.
Тогда давайте так, с этим разобрались.
Значит, про градиентный метод все доказано,
все замечательно.
Теперь давайте обсудим,
что происходит,
не что происходит,
как, в принципе, можно
пытаться сравнить методы
независимо от
конкретной функции,
для которых они применяются.
Для этого следует такой
немного, может быть,
странной конструкции.
Сейчас я попытаюсь ее
понятнее объяснить.
Вот, это всегда некоторые вызов
объяснить эту конструкцию,
но мы сейчас разберемся.
Значит, что говорят?
Говорят, что мы, поскольку,
да, нижние оценки
сейчас можно сказать,
поскольку мы хотим понять,
как у нас все работает
в концепте черного ящика,
то есть мы не знаем,
какая у нас функция конкретная.
Мы знаем такое свойство,
то есть f там l гладкой, например,
или там f l гладко
плюс mu сильно выпала.
И хочется понять в лучшем случае
метод из определенного класса.
То есть что такое класс метода?
То есть мы будем рассматривать методы,
которые имеют вот такой вот вид сейчас.
То есть у нас xk плюс 1 – это x0
плюс линейная комбинация градиентов
во всех предыдущих точках.
Это класс методов.
Любой метод,
какой бы мы из этого класса не взяли,
он как-то в каком-то интервале
будет лежать в его сходимости.
И концепция нижних оценок
говорит нам о том,
что если мы представим
такую функцию, какую-то f крышкой,
какую, что для f с крышкой
будет выполнено такое вот ограничение,
тут o от чего-то.
Видите, тут вот важно
подчеркиваю, что тут оценка
в другую сторону.
Она говорит не о том,
что мы будем как минимум
такими же быстрыми, как что-то.
Вот было вот здесь.
Она говорит наоборот,
что лучше чем что-то мы не будем.
Достаточно предъявить одну такую функцию,
чтобы можно было заключить,
что если мы возьмем,
что для этого класса функций
и этого класса методов справедливо,
что если мы ничего про функцию не знаем,
то в худшем случае
можем рассчитывать только
на вот ту скорость сходимости,
которая вот здесь.
То есть грубо говоря,
смотрите, что если там
рисовать картинку,
вот у нас есть число итерации,
вот у нас есть там гэп какой-то.
Вот у нас была сходимость
гридиентного спуска вот такая.
Это был гридиентный спуск.
А вот оценка скорости сходимости
на эти глотки функции,
она вот так идет.
То есть это вот lower bound.
Соответственно, если мы видим,
что между оценкой сверху нашего метода
и оценкой снизу
для всего класса методов
и класса функций есть зазор,
это значит, что мы можем
что-то улучшить.
Если мы видим, что зазора нет
и они совпадают,
это значит, что мы нашли
в смысле нижних оценок
оптимальный метод.
Понятна ли логика построения?
Ставьте плюс, если понятно,
минус, если непонятно.
Саможнежер оранжевая,
это какая теоретическая оценка?
Которую я пока еще не написал.
Просто некоторая оценка снизу,
которая вот здесь вот сидит.
Вроде пишут, что более-менее понятно.
То есть на самом деле,
если уже переходить к конкретике,
по оценке для l-гладких функций,
выпуклых функций,
такие, что f каплюс первое
минус f со звездочкой
больше либо равно,
чем там 3l на норму
x0 минус x со звездочкой
делить на k плюс 1 в квадрате.
То есть это на самом деле
от 1 на k квадрат.
А для л-гладких,
имиусильно выпуклых,
там немножко похитрее,
там примерно то же самое,
что у нас было
в оценках на вариантный спуск,
только тут корень
и часа обусловленности 2, как у нас.
И x0 минус x со звездочки тоже в квадрате.
То есть эти нижние оценки,
я сейчас, наверное,
не буду детально их выводить,
потому что это типа еще
плюс-минус 40, наверное, час.
В слайдах, в принципе,
будет приведен детальный вывод
для l-гладких функций.
Можете увидеть, какая там плохая функция,
как она строится.
Если будут какие-то вопросы,
мы в следующий раз можем обсудить.
Там действительно все не очень просто
и не очень быстро делать.
И, соответственно, что получается?
Получается, что для градиентного спуска
у нас сходимость только 1 на k,
а оценка снизу говорит нам,
что можно, в общем-то,
и ускориться до 1 на k квадрат.
Ну и для сильного выпукового случая
та же самая история,
что мы сначала говорим про
сходимость вида по минус 1
делить на k в плюс 1.
Здесь мы видим, что начинает
фигурировать уже корень,
что, оказывается,
действительно существенно быстрее.
Наверное, надо показать,
почему это будет существенно быстрее.
То есть, смотрите,
если у нас кап для простоты 10 четвертый,
то коэффициент q,
который будет в градиентном спуске,
это 10 четвертый минус 1
на 10 четвертый плюс 1.
Но можете представить,
насколько это близко к единице
градиентной спуске.
А q со звездочкой
это всего лишь 0,99.
И для того, чтобы получить
нужную точность ε,
вот такой вот,
таким вот нужно, по-моему,
в разы меньше итерации,
чем вот такой.
Понятно ли, почему корень
так сильно ускоряет,
или нужно подробнее это расписать
и показать реально,
на реальных числах,
как это будет выглядеть?
Или понятно, что вот это гораздо
в разы лучше, чем вот это?
Вроде понятно.
Окей.
Вот вижу, что Марка и Андрею понятно.
Как дела у них?
Что-то тишина.
Наверное, давайте я все-таки
пропишу подробнее.
Переживаю, что, может быть,
не очень понятно,
что это я прописал.
Ну, смотрите,
что происходит на самом деле.
На самом деле нам нужно сравнить
две оценки сверху.
Одна оценка сверху вида,
вот так, в степени 2k,
на норму.
Другая у нас была,
типа вот такая, по-моему,
в степени 2k,
на ту же самую норму.
И мы хотим, чтобы
и там, и там был меньше ε.
Для одного и того же ε.
Ну, например, ε мы хотим,
чтобы был 10-5.
Чтобы мы были близки
с оптимальным значением функции
на уровне 10-5.
Тогда что надо сделать?
Надо взять, выразить,
ну, то есть это будет q в степени 2k,
потом просто разные q подставим,
получим разные числа.
Меньше либо равно
ε делить на вот эту норму.
Вот, про логарифмировав.
Стоп, неправильно.
Вот, ну и, соответственно,
если k выражать, то что будет?
k меньше либо равно,
чем ε делить на 2,
на две вот этих нормы.
И тут еще логарифм q.
То есть, соответственно,
то, что здесь,
а, и больше либо равно, наверное.
Потому что логарифм q
у нас отрицательный,
мы на него делим,
получаем, и знак меняется.
Ну, и это он
с логарифм q дает нам...
Все неправильно, простите.
Сейчас.
Что-то я немножко напортачил.
Да, но вот логарифм вот здесь стоит
и будет идти уже не так все легко
и непринужденно.
Будет отношение логарифм.
Отлично.
Ну и, соответственно, что?
k больше либо равно,
чем такая величина?
В первом случае,
когда у нас был градиентный спуск,
у нас q равняется, короче,
просто отношение этих штук.
Посчитайте все.
Да.
То есть, давайте посчитаем
отношение минимального чела и тараса
для градиентного спуска
и для оптимального метода.
Два логарифма q со звездочкой,
ну и тут будет то же самое.
В общем, это все сократится,
останется просто отношение логарифмов.
А это, понятно будет,
что такое, можно даже посчитать.
То есть, в одном случае будет
логарифм 0,99 сверху будет.
А в другом будет логарифм
от 10 в четвертый,
минус один,
10 в четвертый плюс один.
У кого по другой калькуляторе,
у кого по другой калькуляторе,
давайте попробуем это посчитать
и скажите, какие значения
у вас получат.
Примерно 50.
Ну вот, да, примерно 50.
То есть, в 50 раз
тараси будет меньше.
Чувствуете разницу?
То есть, условно,
градиентному спуску нужно
порядка тысячи,
нужно будет порядка 100.
Это действительно существенное
получается ускорение.
Гуд.
Есть ли какие-то вопросы
по этой теме про нижние отцам?
Сейчас к мне уже перейдем.
Ну, вопросов вроде бы нет.
Тогда давайте сейчас
немножко я поправлю
последовательность.
Начнем с общих методов.
Если останется время,
рассмотрим один частный,
но очень важный.
Итак, первый называется метод
тяжелого шарика,
который, наверное, самый простой
для понимания
и в то же время
достаточно эффективный.
Идея была очень простая.
Это Борис Садорович Поляк,
1964 год.
Соответственно, там,
если найти оригинальную,
есть открытым доске.
Вот так.
Да, идея была очень простая.
Давайте мы перепишем
наш замечательный градиент
из пуск, добавив в него
некоторые слагаемые
дополнительные, которые нам
еще трение будет учитывать.
Если думать об этом,
как о моделировании
динамической системы трения.
Поэтому тяжелый шарик.
То есть, есть некоторая
индационность, которая
позволяет сглаживать
плохо обусловленные
линии уровня.
То есть у нас был как?
Если было вот так,
то градиент из пуск
у нас прыгал вот так,
тихонько оближаясь.
То вот эта штуковина
из-за вот этого коэффициента.
Сделай следующее.
Она прыгнет вот так.
Потом ей захочется прыгнуть.
Направление одно будет
сюда вести.
Но мы будем помнить,
что до этого мы шли сюда.
И с коэффициентами альфа и бета
у нас итоговое направление
будет каким-то вот таким.
То же самое будет происходить
и из этой точки.
То есть видите,
тут как бы для картины,
конечно, не так,
чтобы супер-хорошая.
Но здесь понятно, что мы
за счет вот этого учета
предыдущего направления
вместо двух итераций
как бы сделали одну.
Ну если сравнить количество палочек,
которые получилось нарисовать.
Ну и из этой точки аналогично.
Мы хотели прыгнуть вот сюда.
У нас предыдущее направление,
оно нас двигает вот сюда.
И поэтому суммарно
мы прыгнем куда-нибудь сюда.
И таким образом
мы взгладим нашу траекторию
и сойдемся к точке минимум
гораздо быстрее.
Понятно ли, почему это работает?
И почему, по идее,
должно работать быстрее.
Ставьте плюс, если понятно.
Вроде нормально.
Ну, собственно, теперь надо понять,
как эта штука будет сходиться.
А ну важно сказать, что
да, самое главное
это, наверное, не сказал.
Первое, свойство метода,
что он двухшаговый,
в отличие от одношагового
гранитного спуска.
То есть тут мы дополнительно
еще вынуждены хранить
предыдущую точку,
xk-1,
для того, чтобы посчитать
предыдущую, ну,
принять в внимание
предыдущие направления.
И второе, он не монотонный.
То есть мы отказываемся
от требования того,
чтобы на каждой итерация
без этого бывали.
Потому что здесь может
что-нибудь произойти по дороге
так, что мы будем
как-то стилировать немножко.
Ничего страшного.
Значит, как это теперь доказывается?
Ну, вот я сейчас
драфт некоторой
оказательства сформулирую.
А дальше, в общем,
есть некоторые нюансы в том,
как именно это доводится до конца.
И формулы, которые
получаются на выходе,
они получаются довольно
не очень приятные.
В общем, сходимость доказана
только для сильного
упакового случая.
Вот так, сходимость.
Так, сходимость
для сильного упакового функции.
Вот.
И для того, чтобы
получить все результаты,
надо сделать следующее.
Переписать этот метод
в блочном виде.
Странно это мне казалось
на первый взгляд.
То есть, а именно,
блочный вектор у нас
будет xk плюс 1 и xk.
Вот.
И мы попытаемся сейчас
вытащить все линейное
и отправить все нелинейное
в буквально слагаемое.
То есть тут у нас будет
x, соответственно, k
и xk минус 1.
Так.
Ну и там, например,
геечная матрица 0,
минус альфа к на градиент.
Ну и здесь будет просто 0.
То есть, просто записали
вот первая строчка.
Это наше исходное равенство.
Вот.
А вторая строчка
просто тождественная равенство.
Не сложно перемножить
и убедиться в этом.
Значит, это первый шаг.
Далее мы сделаем
следующий небольшой трюк.
А именно, возьмем
и припишем эту штуку.
То есть, вычтем
из обеих частей
и к созвездочке.
Чтобы получить, собственно,
наши обычные искомые разности
между текущим положением
и x-озвездочкой.
Но, помимо этого,
помимо этого,
мы еще и учтем,
что в x-озвездочке
у нас градиент 0.
Поэтому вот здесь появится
дополнительно разность
с градиентом в точке 0.
Которую мы, ну, то есть,
грубо говоря, запишем,
что f' от xk
минус f' от x-озвездочкой
у нас будет там
равен интеграл от 0 до 1
f' х от tau
ну, то есть,
перепишем в виде интеграла.
Вот.
И если это все вместе объединить,
то получим, что
остается единица
плюс бета катой
на единичную матрицу.
Вот.
Только здесь теперь остается
минус альфа катой
на вот этот вот интеграл
при ряда 1
f'
потому что он будет
тоже умножаться на
x катой минус x-озвездочкой.
Да.
Все-все-все.
Правильно?
А, нет, неправильно.
А, нет, правильно.
Сорри.
Здесь xxx от tau.
Соответственно, путь
x катого
плюс tau
x-озвездочкой
минус x катой.
Правда?
Нет.
Стойте.
Что-то не то.
Тут вот еще надо домножить
на x
со звездочкой
минус xk.
И как раз тогда
все будет правомерно.
От x от tau до tau.
Вот.
Коэффициент
минус бета катой
единичная матрица
останется.
Тоже все будет хорошо,
но получится,
что здесь у нас
x k
минус x-озвездочкой.
А тут
x k
минус 1
минус x-озвездочкой.
По историю,
наш интеракционный
процесс
виду y kt
равняется некой
матрице
на y k
минус 1.
Это,
собственно,
была цель.
Дальше,
дальше мы знаем,
что такой
интеракционный
процесс сходит
с линейной скорости
сходимости
пропорциональной
спектральному радиусу
матрицы и
трассы.
Все знают,
что такое спектральный радиус.
Ставьте плюс,
если вы знаете,
что такое спектральный
радиус.
Ага.
То есть,
в этой матрице
есть определенная
структура.
То есть,
она какого
вида,
на самом деле?
Вот тут
диагональ,
тут
диагональ,
а вот здесь
стоит что-то
типа
диагональ
плюс
что-то плотное.
Ну,
это что-то плотное.
Мы можем,
на самом деле,
воспользовавшись
теоремой в среднем,
можем вот эту
штуку заменить
на десян в какой-то
промежуточной
точке u.
И,
соответственно,
если мы ее
ну,
там,
семитечно,
все с ней понятно,
все хорошо,
если мы ее
диагонализуем,
вот,
то окажется,
что
в базе
там u,
если вот так вот
сделать,
тут останется
диагональка,
там,
единица плюс
диагональка,
тоже диагональ,
тут ноль,
а тут будет
утранспонировано,
утранспонировано.
Что-то такое.
Тут,
ну,
все хорошо,
окей,
вроде понятно,
это хорошая новость.
Вот,
а дальше
надо,
по сути дела,
значить силой
в том,
что надо оценить
спектры,
вот такой вот матрицы.
Вот,
и вот у то
чудо,
на самом деле,
что
с помощью
некоторых перестановок
это можно привести
к
матрице
блочной
диагональной.
И тут матрица
будет 2 на 2.
Ну,
получается, что спектр этой матрицы сводится к анализу
спектра вот этих вот блоков, из которых там что-то сидит.
И взяв максимальное собственное значение из этих штук и
променивизировав его по альфа и бета, мы получим
довольно жуткие значения, которые записываются так,
что если у нас альфа-кате — это 4 на корень из L плюс
корень из μ в квадрате, бета-кате — это максимум
из 1 минус корень альфа-кате L и 1 минус корень альфа-кате
μ.
Ну, это, собственно, мотивирована тема, что там надо отрицательные
и положительные все учесть.
Это все в квадрате.
Если коэффициент выбран так, то есть альфа-кате — это
у нас мершага, у нас классический, бета-кате — это коэффициент
перед...
Коэффициент перед дополнительным слагаемом, то метод будет
сходить сверху...
Будет сходиться линейно с правильным коэффициентом.
Хк плюс 1 минус х со звездочкой по норме, хк минус х со звездочкой.
Будет меньше либрага, чем корень из ка-п минус 1 на
корень из ка-п плюс 1 в степени k, ну, на норму того же вектора
только...
Так, надо написать.
Х1 минус х со звездочкой, и тут будет х0 минус х со звездочкой.
То есть желаемого коэффициента нам удалось добиться.
Понятны ли понятия на результаты?
Поставьте плюс или результат понятия на минус, если не
понятен.
Так, ну, каких-то вопросов вроде пока нет явных.
Так, надо показать картинку.
Это все добро сходится.
Сейчас секундочку я все подготовлю.
Тут стоп шер.
Так вот.
Ну, короче говоря, вот взяли случайную квадратичную
задачу.
Вот.
И поскольку для квадратичной задачи у нас l и μ соответствуют
максимальному и минимальному значению матрицы, то мы
можем явно посчитать оптимальные параметры.
Вот.
И оказывается, что вот так будет сходиться градиентный
спуск.
Пока на зеленую не смотрите.
Вот так будет сходиться градиентный спуск почти
что сублинеида, потому что, в общем, число бусловности
достаточно большое.
И за 5000 терраций всего лишь 10-1 сойдется.
В то же время тяжелый шарик сошелся в 10-5 за почти 4000
терраций.
Вот.
То есть оцените, насколько быстрее стало все работать
только из-за того, что мы добавили дополнительное
наслагаемое.
Вот.
При этом вроде кажется, что монотонность есть.
Норма градиента монотонного бывает.
Однако же, если внимательно присмотреться, то вот здесь
в самом начале у него есть небольшой интервал, когда
его там монотонно осцилирует, и уже такой строгой монотонности
как градиентного спуска нет.
Вот.
Поэтому, в общем, если параметры не удаются найти оптимальные,
это довольно часто случается, то обычно бету выбирают
близко к единице, типа 0,8-0,9.
Вот.
А альфу выбирают исходя из масштаба задачи и того.
В общем, в какой там масштаб ничего в голову не приходит.
То есть там 10-3, 10-2 типичные значения, для которых более
или менее все работает.
А можно вопрос?
А нельзя?
Просто, ну, потому что получается градиентный спуск, ну,
в начале эффективнее, пока мы видим.
Нельзя сначала делать градиентный спуск без вот этого
дополнительного наслагаемого с весом, ну, с…
А непонятно, когда переключаться.
Когда у нас, ну, уменьшается приращение, ну, ниже какого-то
рога.
А у нас шаг дает эффективность, ну, уменьшение нормы…
уменьшение нормы между последовательными вот
иксами меньше, чем во сколько-то раз.
Или меньше, чем на сколько.
Да, ну, просто во сколько это раз – это непонятно.
Вы это сложно точно оценить, потому что у вас всех
этих констант нет, которые участвуют в оценках в этом
проблемах.
То есть вы можете там грубо говоря поставить какую-то
рандомную чиселку и надеяться, что когда-нибудь она будет
выполняться.
Но гарантий вам никто не даст.
Ну, в общем случае, для произвольной задачи.
Поэтому, да, конечно, есть свои проблемы.
Это была картинка для тяжелого шарика.
Теперь едем дальше.
И посмотрим, посмотрим на, собственно, быстрый грядентный
метод, который, который достигает оптимальных оценок
как для сильного пуклого случая, который здесь уже
был упомянут.
И для просто эль гладкого выпуклого.
Так, быстрый метод.
Это Юрий Евгеньевич Нестеров, 983.
Статья доступна.
Статья доступна тоже в открытом доступе, вы можете
ознакомиться.
Один из вариантов его формулировки.
Не самый, может быть, точный, но как минимум самый
понятный для того, чтобы, наверное, интуицию какую-то
развить.
Такой.
Вводим дополнительную последовательность.
Помимо х ноль, у нас будет еще у ноль.
Значит, х ка плюс первый у нас теперь пересчитывается
вот таким вот образом.
У пересчитывает на основе почитанных х.
То есть сейчас надо внимательно посмотреть на эти выражения
и сказать, чем они принципиально отличаются от того, что
было в тяжелом шарике на предыдущей странице.
Если вы помните формулу, там она в деле очевидна.
Что существенно образом изменилось в сравнении
с тяжелым шариком?
Какие будут версии?
Мне кажется, теперь даже трех-шаговая.
Ну, если считать, что у нас шаговая.
Почему?
Если у и х объединяются в одну последовательность,
то нам нужно знать и предыдущий.
Нам для подсчета у ка плюс первого нужно знать и
х ка плюс первый и х ка.
А значит, это уже три предыдущих членов,
потому что между ними еще и у ка.
Не, ну то, что надо сохранять три вектора, это правда.
Но в истории мы идем по-прежнему только на один шаг назад.
То есть шаги, сколько в истории,
как далеко в истории мы идем,
а не то, сколько вторых мы храним.
У нас история состоит из чередующихся у и х.
И нам нужно три последних элемента истории,
чтобы насчитать следующий.
Нет, еще раз.
История это история индексировать с номером интераций.
А не тем, сколько элементов вы храните.
А, то есть считается, что мы на одной итерации насчитываем и х и у.
Ну, написано же.
Ну, в плане, просто у нас два пересчета,
логично их склеить, ну, последовательно,
что на чётных итерациях пересчитываем х и на ничётных у.
Погодите, вы видите, что тут х ка плюс один написано?
Там и там.
Да, да, да, да.
Ну, индексы, да, но просто у нас два индекса появилось,
две последовательности.
А мы раньше работали с одной, логично склеить.
Ну, и они склеиваются в одну итерацию, наверное.
Так, в чём разница-то с тяжёлым шариком?
Ну, да, появилась последовательность, окей.
Ну, в тяжёлом шарике мы тоже могли в общем-то вести эту последовательность,
просто сказав, что вот это у нас типа игрит.
А вот ещё одно последовательство.
Кто внимательно смотрит на формулы?
Ну, кажется, что особо никто не смотрит на формулы внимательно.
Ладно, смотрите, в чём дело.
В тяжёлом шарике главное, конечно же, происходит вот в этой строчке,
потому что здесь получается так, что мы вычисляем градиент
точки отличной от той точки у нас… точки той последовательности,
которую мы получаем.
То есть здесь, хоть у нас и была какая-то последовательность y, k, t,
мы всё равно считали градиент в текущей точке –
того, чтобы получить следующую.
Здесь мы как будто бы эти две последовательности,
они друг с другом как-бы перемinnно пересчитываются.
И получается, что для того, чтобы посчитать следующий x,
нужно посчитать градиент в другой совсем точке,
точке другой последовательности.
То есть, еслиoa рисовать картинку, то она будет выглядеть вот так.
сейчас я хочу оставить формулы, типа пусть это был x0 равный y0, мы пересчитали вот сюда,
получили x1, вот дальше мы берем и из x1 идем немножко вперед, немножко вот сюда, это будет наш,
давайте я другим цветом сделаю, это будет наш y1, относительно y1 мы делаем градиентный шаг,
получаем здесь x2, затем на вот этой вот линии мы получаем y2, и дальше из y2 мы куда-то там идем,
чтобы потом получить x3, x3 и тут получить y3, понятно ли как теперь выглядит траектория,
ставьте плюс если понятно, вижу пока два плюса, неплохо, вроде вопросов пока таких нет по поводу
того что происходит, вот и значит оказывается, что вот такая стратегия, она более общая,
скажем так, то есть она работает хорошо для не только сильно выпуклых функций, но и для просто
l гладкий, при этом почему вот здесь вот тройка, вот давайте я просто сейчас статью пришлю в чат,
чтобы если будет интересно почитать, там короче целый раздел про магическую константу тройка,
и почему туда можно поставить что-то больше тройки, но нельзя поставить ничего меньше тройки, вот,
с это все основывается на устойчивости некоторых соответствующих дифференциальных уравнений,
вот и так это уже на счет ремонт, ну да сход не нашлось, ладно, я после лекции тогда пришлю
в стулчику, вот, и заодно дальше с мешотом покажу как это устроено, в общем, почему здесь тройка,
отдельная история, но тройка работает достаточно неплохо, вот, и то что вот эта вот последовательность
она стремится к единичке, вот, тоже как бы достаточно разумно оказывается, так, значит этот метод
по-прежнему, так как мы назвали двухшаговый, да, но он стремится к вектору, вот, плюс он также не
монотонный, мы сейчас увидим насколько он не монотонный, вот, и ему также соответствуют некоторые
специальные уравнения второго порядка, вот, изучение которого можно понять, что происходит
с самим методом, вот, так, картинка видимости, обязательно надо показать, да, вот как это выглядит,
на той же самой задаче. Тут, короче, нас интересует сейчас красная линия, видно, что вот у нее такие вот
называются... Зигзаг... Зигзагообразные, забыл, секунду. В общем, типа похоже на зыбь,
вот, скучкообразные траектории, вот, это, значит, если вы такое наблюдаете, значит, у вас, скорее
всего, все правильно реализовали и все работает, вот, не надо пугаться, что он не монотонный, это уже
как-то встроено в структуру самого метода, и это нормально, вот. Интересно, что вот здесь работает быстрее,
чем все, что мы до этого обсуждали, доходит где-то до 10-3, там, 10-4, начинают все эти ассоциации,
и на них его догоняет тяжелый шаг, вот. Понятно, что вот в этой точке хочется сделать какую-то
штуку, так чтобы он шел не вот сюда, а шел еще дальше вниз. Как вы думаете, каким образом это можно
проделать? Ну, то есть, грубо говоря, понять, что в этой точке мы начали возрастать, можно.
Просто значение функции посчитали, или там норма градиента, увидели, что, а, тут мы типа,
у нас увеличивается. Давайте мы что-нибудь сделаем так, чтобы не ходить в эту точку, а ходить в другую
точку. Вопрос, как вы думаете, каким образом это можно сделать? Ну, будем идти по антиградиенту,
или там, возьмем Xкат, и с минусом, ну, короче, добавим минус вот где-то в пересчет. Да, ну,
то есть, наверное, я, кстати, не уверен, что взять просто противоположное направление, оно, так, ну,
для функции-то оно может сработать, но для градиента не уверен. Неважно пока что. Важно другое. Важно,
что можно взять просто антиградиент, вы правы. Вот. И это в каком-то смысле будет означать то,
что мы взяли и забыли всю вашу историю, сделали просто restart. Вот из этой точки запустили наш метод,
как будто у нас не было предыдущей истории, никакой никаких Y мы не копили. И из этой точки
запустить restart, и он пойдет благополучно ниже. И сойдется, ну, за какое-то разумное время,
ну, то есть, вот этого уже не будет. Будет, ну, то есть, он сделает шаг, потом, возможно,
снова, значит, попытается начать вот эти вот волнообразные какие-то движения. Вот. И они
также могут гаситься тем же самым техникой restart. Вот. То есть, это довольно продуктивная история,
которая, в общем, 5-6 лет назад была показана, и там, как бы, все довольно неплохо работает.
Поэтому, в общем, этого пугаться не надо. Это все, все с этим, это была картинка. Теперь,
собственно, сходимость надо сформулировать. Здесь 0,7. Очень хорошо. Вот. Ну, и результаты
сходимости тоже достаточно, достаточно понятные, то есть, для сходимости. Для L-гладких выпуклых
будет единица на, как, квадрат, собственно. Для мио-сильно выпуклых, там, L-гладких тоже.
Будет Q линейная сходимость с нужным коэффициентом. Вот. То есть, метод на самом деле является оптимальным
в контексте использования нижних оценок, вот, о котором мы до этого обсуждали. Вот. Значит,
вот эти два метода, они очень здорово укладываются во всю эту довольно красивую теорию. Вот. И,
значит, довольно, довольно активно используется на практике. Теперь еще есть один метод,
называется метод сопряженных градиентов, за который мне нужно вам рассказать за, типа,
10 минут. Ну ладно, я пытаюсь. Вот. Значит, изначально он был разработан для решения задачи вот таких,
которые сводятся к решению системы линейных уравнений. Вот. Про него там много теорий придумали,
там, результаты сходимости прекрасные. Вот. Но интересно друг, интересно то, как он обобщается
на нелинейные случаи. Вот. На нелинейные случаи он обобщается очень похожим образом на те методы,
которые мы только что обсудили. Вот. Единственное, что для него нет таких хороших оценок, потому что,
в общем, почему сейчас я пытаюсь объяснить. То есть, как он работает? Он берет и говорит,
что мы возьмем и будем строить хк плюс первый, как х0 плюс хкт плюс альфкт и пкт. Вот. А пкт у
нас будет, ну там понятно, по нолю равен минус градиент. Вот. А пкт будет равно минус, ну там,
пкт плюс первый. Минус пкт плюс, так, неправильно. Минус градиент в новые точки, вот так правильно.
Все, я мельчу, это плохо, ничего не видно. Так, минус. Берем то же самое направление антиградиента,
которое было в грядном спуске, плюс бета катое на пкт. Отправляем его в предыдущем направлении. Вот.
То есть, концепция очень похожа на тяжелую шарику, в самом деле. Только здесь альф и бета немножко
другую роль выполняют. То есть, если в тяжелом шарике у нас бета подправляла итоговое направление,
то здесь у нас итоговое направление генерируется через бету, а шаг альф выделяется в отдельную,
как бы, историю. Значит, что делать, когда, то есть, для вот этой задачи там есть отдельное выражение
для альфа и бета замкнув виде, но нас интересует, понятное дело, не такие задачи, а задачи более
общего вида, пока что, вот такие. Вот. И что делать с альфы и бетой? Ну, вот альфа катое делают
просто адаптивный поиск. Правила армии, там, вот эта вся история. Я надеюсь, что на семинарах у вас
в том или ином виде это уже было. Поставьте пилотики, если у вас уже все это было на семинарах.
Интересно, кстати, вы все знаете. Так, вот было все, да. И минус, если не было. И минус более
интересный. Наличие. Ага. Что не было? Чего не было? Адаптивного поиска или... Ничего. Ладно,
я тогда сейчас ничего расскажу про адаптивный поиск, как раз в своем есть. Значит... Да, адаптивного
поиска не было. Хорошо. Значит, альфа катое делается адаптивным поиском каким-то. То есть,
подстраивается под текущие направления, точку и градиент. Вот бета катое, оно пересчитывается,
может пересчитываться разными способами. Вот. Например, можно брать для каждого способа, типа,
свой именной метод. Ну, что-то типа такого. Называется метод Fletcher-Reeves. Fletcher-Reeves.
Если бета катое вот такое... Сейчас, секунду. Всегда я плохо помню фото. F'x-k-1. А здесь, типа,
разность. Деленно. Опять же, на норму. Вот. Это метод полукарибьера. Два разных человека. Вот.
Там еще есть хестенс-фестиваля. Ну, короче, там зоопарк этих всех способов выбрать бетла. Вот.
Почему это интересно? Потому что все это вы можете обнаружить в тайфай, который метод спряженных
градиентов как раз-таки версии полукарибьера используют. Я постараюсь показать. Пам-пам-пам.
Зону потерял. Ну вот. Так, то есть, вот если вы посмотрите в документации, то тут будет понятно
метод спряженных градиентов conjugate gradient CG. Вот. И... А, это я сюда зря перешел. Вот. И если
почитать детали про то, что там реализовано внутри, вот, то он тут вам напишет, что вот, типа,
полукарибьер. Вот. В общем, метод Fletcher-Reeves можно в литературе посмотреть. Наверное,
это на Сидаль Райт. Вот. Мы обсудим в дальнейшем, что такое FGS, Newton CG и LBFGS без вот этого
вот дополнения. Вот, что оно значит, я тоже расскажу. Вот. И в целом кратко, может быть,
перечислим то, почему это что за trust. Так, что это за приписка, зачем она нужна. Вот. Все остальное
останется вне курса, потому что, ну, что это называется, не так, чтобы хорошо работает,
в каком-то смысле. Вот. Ну, то есть, вот эта штука, это, типа, эти безградентные методы. Мы про них
немножко поговорим, но это супер какая-то базовая история, которая сейчас где-то может применять,
но не так, чтобы супер активна. Вот. Все остальное, оно решает невыпуклые задачи. Вот. Информация
об ограничении. Ну, в общем, для этого, скорее всего, существуют более эффективные методы. То есть,
тут, как бы, некоторые baseline есть. Вы можете им при необходимости воспользоваться, но детально
погружаться в то, почему это работает, наверное, сейчас. Вот. Единственное, что вот эта вот штука,
довольно неплохая концепция про, сейчас я себя правильно помню, self-sequential squares. Да. Вот.
Может быть, про это немножко еще поговорим. Ладно. Короче, CG, классная штука. Хорошо работает,
вот. И я что-то еще хотел показать. А, я хотел бы рассказать про адаптивный поиск. Вот. Ну,
адаптивный поиск или еще так называемый backtracking основан на следующей простой идее. Вот. Мы хотим в
данном случае, чтобы наше значение функции в новой точке все-таки было меньше. Вот. В этих
ускоренных методах мы его, как бы, как бы, получно забыли про него. Здесь все-таки хочется как-то,
ну, чтобы было вот так. Ну, наивный самый. Нулевая попытка. Это что такое? Ну, мы берем и говорим,
что слева у нас стоит f от xk плюс альфа кт пкт. Пкт же, да? Пкт. Вот. И мы хотим,
чтобы эта штука была меньше tмf от xkt. Вот. Ну, то есть как? Мы берем, запускаем цикл, и если вот это
нарушается, то альфа делим пополам, например. Или там умножаем на 0.3, на 0.1. До тех пор, пока мы не
достаточно уменьшим здесь шаг, чтобы это направление, которое там, оказывается, что локальному
все-таки будет, чтобы оно уменьшило значение функций. Это первое такое, нулевое приближение. Вот.
Первое приближение, которое, в общем-то, в большинстве случаев используется, что они в большинстве,
но хороший такой solid baseline называется. Это искать такую альфа, что будет не только меньше, чем f от
xkt, но меньше, чем f от xkt, а плюс бета кт на скалярное произведение градиента с кт. И тут еще умножить
на альфа кт. Эта штука меньше нуля, поэтому у нас слева знака не равен сосредоточительной функции,
довольно странная. Справа у нас стоит линейная функция по альфе. Поэтому если строить картинку,
тут будет f от альфы, который вот здесь вот. Вот здесь будет 0, это наша f от xkt. То есть,
если мы нарисуем картинку выражения, которая находится вот здесь, то это, типа, какая-то
прямая с отрицательным коэффициентом наклона. Вот. А то, что стоит слева, это что-то вот, типа,
локально, которое бывает, что оно может быть локально вот так. Тут вот у него что-то с ним
происходит. И вот нам нужно выбрать альфы, которые лежат, соответственно, вот тут, тут, тут. То есть,
нужно попасть в какой-то, знаете, интервал. Ой, что-то я... Что-то такое. Вот. При этом понятно,
что если мы с бетой промахнемся, может получиться ровно вот такая вот история, когда мы поуменьшились
вот досюда и все равно оказались выше. А может, промахнулись беты. Вот. Поэтому обычно беты берут
достаточно маленькой, типа, около 0.1. Вот. Ну и так атеративно пересчитывают альфы,
пока не будет выполнено этого условия. Так. Понятно ли, что это за метод? Ставьте плюс,
если понятно, и минус, если непонятно. Но при этом понятно, что если у вас там функция фигурирует
какой-нибудь логариф, например, у вас f от x, типа, сумма, помню, там, логариф, мот, v и t минус,
a транспонированная, и t x. Надо учитывать, чтобы у вас шаг был достаточно маленьким,
чтобы вы за области определения не вышли. Тут, как бы, в процессе конкретизации для
каждой конкретной задачи нарешивают еще это плотное условие, чтобы вы оставались в области
определения. Пока только один плюс. Дмитрий, а вам непонятно, или как, или понятно, пока еще
не поставили плюс, или есть какие-то вопросы? Все понятно. Окей. Просов нет. Ага. Хорошо. Спасибо.
Ну, в общем, да, 10 и 20. Давайте на сегодня тогда закончим. В следующий раз мы посмотрим
настокофические методы и поймем, что происходит со всеми этими прекрасными результатами про
ускорение, когда у вас градиент не точен. Анонс. Что происходит при неточном градиенте. Вот. Надеюсь,
как это удалось, настоятельно образом заинтриговать, чтобы вы в следующий раз тоже
подключились, и мы бы с вами рассмотрели такие посмотки знаешь, когда градиент точно
посчитать не получает. Ладно, всем большое спасибо за внимание. Извините, а может, пожалуйста,
пока мы не закончили, показать еще раз постановку метода соображенных градиентов? Ну вот.
То есть я говорю, тяжелый шарик, только коэффицианты другие. Ну, другой ролик играет. Да, спасибо. Вот так.
Ну, я запись-то выложу все равно сейчас в чат, в общем, если что-то забыли, не успели переписать,
ничего страшного.

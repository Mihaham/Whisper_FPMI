Хорошо, сегодня будет вторая лекция курса математика больших данных и у меня было несколько вариантов,
вокруг чего эту лекцию сделать, но я в итоге решил вокруг Монте-Карло, то есть мы начали говорить про подход идеи метода Монте-Карло
и в общем-то начали не самого простого примера, а какой самый простой пример, в принципе мы сегодня будем про это говорить,
про Монте-Карло и окрестности, но вообще какой пример демонстрации того, что такой метод Монте-Карло вы знаете,
ну вот на вскидку кто-нибудь может сказать, взять интеграл, замечательный пример, еще какие примеры есть,
ну я имел ввиду совсем самый простой, чтобы сразу почувствовать идею метода, ну хорошо, давайте так, взять интегралов, да, это прям хороший пример,
давайте так, я назову лекцию, элементы, элементы, плохо видно, да, или нормально, элементы теории случайных процессов,
так что те кто хорошо знает этот курс, немножко будет сегодня скучно, но даже если вы прям идеально знаете этот курс,
если брать студенческую классическую программу физтеха, вы все равно узнаете довольно много новых вещей,
но если вы ничего не знаете, то лекция будет максимально полезна, потому что я буду повторять какие-то важные базовые вещи из теории случайных процессов,
но Монте-Карло это тоже вот эти вещи связанные с элементами теории случайных процессов, там мы будем сегодня приложения финансовой математики рассматривать,
мульти-левел Монте-Карло, симуляции Таниллинг, это вот все то, что сегодня планируется обсуждать, но давайте начнем просто с напоминания Монте-Карло,
это о чем вообще, ну это вообще говоря вот о чем, вам надо найти площадь множества, у вас есть возможность точки бросать случайно равновероятно в этот квадрат,
как бы вы действовали, чтобы найти площадь этого множества, ну правильно, кидать точки, так, хорошо, что дальше, так, ну правильно, то есть если квадрат имеет единичную площадь,
то число попаданий точек, давайте нюм число попаданий, число попаданий, попаданий, ну и дальше, что на n, на число бросаний, общее число точек,
число точек, да, ну и соответственно, вот отношение этих двух чисел дает приблизительно площадь, то есть s, s приблизительно равно ню на n, так ведь, хорошо,
это метод Монте-Карло, да, ну правильно, это метод Монте-Карло, окей, а если вы хотите число pi найти, допустим, ну это так, немножко хулиганю,
просто проверяю как бы, что, что мне с какой детализацией рассказывать на лекции, если вы хотите число pi найти, возьмите, значит, линий тетрадь в линейку,
ну тоже тут единичка, расстояние сделайте, вот единичка, ну и возьмите какую-нибудь, ну для простоты возьмите два, иглу размера 2, ну понятно, ну или там чуть меньше 2, ну 2, 2,
и кидайте случайно на тетрадь в линейку, упражнение, я не буду его доказывать, что с помощью такой же оценки, оценки числа пересечения, ну у вас игла может упасть как?
Она может пересечь либо одну линейку, либо не одной, правильно?
Третьего не дано, ну в смысле, вы, конечно, можете нарисовать вот эту ситуацию, но она и мера ноль, то есть, что иголка в случайном положении будет занимать такое положение,
ну, так сказать, при случайном исходе будет занимать именно такое положение, это мера ноль на фоне всех вот сетов и входов, которые могут быть, поэтому вы можете не учитывать, что, что могут быть пересечений больше, чем одно, одно или ноль,
ну вот если есть пересечение, то успех, общее число бросаний, понятно, что здесь, здесь понятно, что нискомая, так сказать, вероятность попадания равняется площади,
а тут задача геометрической теории вероятности связать, связать вероятность пересечения с числом π, но если вы знаете число π, то вы знаете, соответственно, вы можете его оценить,
то есть, как с помощью условно бросания иголки на тетрадьку линейку можно оценить неизвестное число π, удивительно, правда, но этим еще Бифон увлекался много веков назад,
я это говорю не потому, что это big data, а потому, что в принципе, чтобы почувствовать идею, вот частично этого достаточно, потому что мы как бы не знаем что-то, но мы придумываем схемы эксперимента,
которые можно реально провести и соответственно из нее выжать нужный нам результат, это какие-то простые вещи, но я обязан это проговорить, потому что если нет понимания вот этого, то дальше идти никакого смысла нет,
то есть это просто должно быть идеально понятно, что еще раз, есть что-то, что мы хотим оценить, вот роль вот в этой общей задаче про дешифрование письма, вот там было завалировано максимум некой функции распределений,
мы в итоге шли на эту функцию, шли на ее максимум, а здесь это одно число, то есть мы придумали схему эксперимента даже без всяких марковских процессов, которое дает нам по сути нужное число просто частотным образом,
я сейчас вернусь к этому эксперименту, пока я просто еще упомяну одну задачу, которую я почти уверен точно все знают, которая сводится ровно к этому же, а представьте себе, что в каком-то смысле вот это множество, площадь которого я ищу,
это на самом деле множество тех, оно непонятно как, оно может быть не выпуклым, не связанным, вот оно как-то так вот расположено, это множество тех людей, районов, где голосуют за кандидата А,
а соответственно другое множество, которое не заштриховано, это множество людей, которых голосуют за кандидата В, и вот Exit Pulse проводится, и надо определить, с какой процент наберет кандидат А,
по результатам второго тура выборов. Понятно, что это как бы о том же, то есть неизвестный вам параметр, в данном случае доля людей, площадь, какую они занимают, где они живут, вы можете как бы заменить площадь на просто реальную долю людей,
то есть вам не надо огромное количество людей, вы просто оцениваете, ну тут как бы вот этой площади неизвестно, а тут как бы люди, вот их очень много, они все это засеяли, вот и они разных цветов, и вам надо случайно вытаскивать чека, просто по ним определить, вот сколько будет в итоге, но не всех, всех это миллион жителей города, а вам достаточно там тысячу, ну не тысячу, три тысячи, человека просить, и уже, так сказать, очень высокая вероятность определить победитель
Монте-Карло, в каком смысле, но это статистика, все это рядом, то есть вот то, что я сейчас сказал, это статистика, а вот это Монте-Карло, а давайте сделаем из этого марка в чейн Монте-Карло, чтобы совсем уж как-то, значит это было близко к тому, что я рассказывал на прошлой лекции, чтобы понять в чем проблема, когда вот я рассказал схему метода Монте-Карло, давайте сделаем так, давайте попробуем найти площадь, в данном случае площадь Шара, ну это будет круг, в случае
f2n равняется 2 размерность, dd размерность 2, и найти площадь круга, вписанного в квадрат, не составляет проблемы, то есть мы действительно можем кидать точки, все хорошо, а что будет, если d стремится к бесконечности, это имеет смысл схема, которую я сейчас рассказал, ну вот я хочу найти условно площадь Шара, ну не площадь, простите, объем, объем Шара таким способом имеет смысл искать демерного Шара,
ну она будет очень сильно стремиться к нулю, ну то есть в чем проблема, можете объяснить, почему плохая идея искать объем Шара, вот как я здесь написал, бросая точки в куб, многомерный куб, ну куб какой объем имеет, один, правильно, все зафиксировали, какой объем Шар имеет,
ну Шар просто имеет действительно объем, как сказать, ну меньше сильно, чем объем, не знаю, я не хочу сейчас забегать вперед, мы про это будем говорить просто в будущем, но мысль такая, что не всегда, то есть да, вот если мы рассматриваем какие-то многомерные объекты,
то вот эта вот интуиция двухмерная, она, ну вообще говоря, не очень хорошо работает, потому что чем больше размерность пространства, тем вам сложнее правильно вписать, тем больше пустот остается, это в двухмерном пространстве кажется, что пустот немного, а в многомерном пространстве они как бы занимают больше места, и это я к чему, к тому, что вообще говоря, сказать, что мы генерируем точки случайно, ну в равномерном шаре-то это нормально, в равномерном кубе это нормально,
но вообще говоря, бывает удобнее, чтобы точнее что-то искать, генерировать точки в каком-то множестве, то есть искать, значит, есть какое-то множество, и хочется в этом множестве генерировать точки, так будет точнее, а не обязательно в кубе, в кубе-то понятно, что генерировать несложно, это просто n, ну d, d равномерных случайных величин, а вот если это какое-то множество, как можно генерировать точки?
Вот как бы вот та процедура, которую я писал про, значит, соответственно, ну как бы накидывание случайных точек, она много где используется, например, есть целый класс методов оптимизации, называются методы отсечений, где логика такая, ищется центр тяжести множества, потом проводится гиперплоскость через этого центра тяжести, отвечающая градиенту, дальше используется теорема Гринбама Хаммера, которая говорит, что
как бы мы эту гиперплоскость не провели, мы всегда можем быть уверены, что объем 1 на e мы точно отсечем, то есть уж либо это, либо это, ну, в смысле, минимально из этих половинок имеет объем, значит, 1 на e, значит, и соответственно мы можем каждый раз там, как минимум, вот столько отсекать, ну, то есть вот с таким фактором, и таким образом локализовать решение, локализовать решение, но проблема-то в этом подходе в том, что
нам каждый раз отсекая, нужно искать Центр тяжести множества и, вообще говоря, это множество, мне очень понятно какое,
то есть каждый раз от там множества чьё-то отсекается, всё сложнее и сложнее становится, вопрос, как искать Центр тяжести множества, ну, вот оказывается,
что это немного трудная задача, то есть это вообще плохая задача, с точки зрения обычной, plaintinome,
как сказать, обычной теории сложности. Но если мы допускаем классы рандомизированных
алгоритмов, задача решается за полином AD, ну естественно там от всяких желаемых
точностей, желаемых доверительных уровней, но только в классе рандомизированных методов.
За этот алгоритм, который по-моему в конце 80-х что-ли был предложен, это надо, было в общем
много всяких премий, потом это целая индустрия стала, одна из фамилий Канан, кто-то еще
очень канан такая фамилия, значит идея очень простая, ну потом это вот эти Ласло Лоос
развивал, ну и вот я сейчас расскажу Hit'n'Run подход, который заключается в том, что вы просто
придумаете марковскую цепь, которая, ну естественно она не дает первоздано случайные точки, но она как-то
более-менее в асимптотике, ну не более-менее в точности, в асимптотике выходит на то, что точка
после некоторого числа итерации этого подхода, будет случайно выбрана, значит подход заключается в том
что выбирается просто какая-то точка, через эту точку проводится случайное направление равновероятно,
на этом направлении выбирается, но это отрезок у нас есть граничный оракул, то есть задав
точку и направление, мы можем однозначно определить концы этого отрезка, вот такое
граничный оракул, дальше мы случайно выбираем на этом отрезке новую точку, новую точку и проводим
через нее случайно новую прямую через эту новую прямую мы тоже берем гранич на раку отсекаем
значит концы но то есть получается отрезок и выбираем случайно новую точку ну и соответственно
мы из этой точки перешли в эту точку потом еще в какую-то точку понятно что новая точка связана с
предыдущей понятно что это марковская цепь почему такой марковская цепь будущее вот если
у вас есть траектория процесса то будущее процесс и вероятностное описание будущего процесса не
зависит от того как вы пришли как вы дошли до такой жизни как вы здесь оказались уж если вы здесь
оказались то все как вы здесь оказались никого не интересует и ваше будущее определяется ну
вероятностное будущее определяется чисто этим положением но нет тем как вы в нем оказались ну и
понятно здесь что неважно как вы пришли в эту точку просто по самой процедуре построения это
марковская динамика а что мы знаем про марковскую динамику что при весьма общих условиях она
выходит на стационарное распределение то есть такое распределение которое инвариантно от
этого в результате эволюции и вот несложно показать ну даже интуиция должна здесь как бы это
подсказывать я не буду это делать это все-таки занимать некое время это не big data поэтому это
ну вы должны это почувствовать вот что что-то интересно рядом то вот таким образом я
стационарная мера будет равномерная и получается что я не могу сгенерировать допустим точки
случайно равновероятно но я могу придумать марковский процесс который генерирует эти точки ну как бы
не случайно равновероятно но так что если я беру первую точку а вторую через много итераций то
они будут практически независимые и они будут равномерно более-менее распределены если речь
идет о старших итерациях то есть даже на одной траектории марковского процесса я просто могу
брать разные точки просто надо через какие-то промежутки брать ну и замечать на то что спектр
лгб такой марковской цепи точнее это не цепь это марковский процесс в дискретном времени нет это
цепь но не счетом числом состоянии вот сейчас подождите это по моему сейчас терминологи вспомнить
это по моему время дискретно нет нет это нет это не цепь это не счетное число состоянии да значит
что здесь интересного но интересно то что действительно вот как бы берет классический
берем совсем простую постановку найти площадь фигуры мы в общем-то пришли уже опять к тому что
ну вот возникает необходимость использования метода монтекарло и возвращаясь теперь к вопросу
который не вопроса комментарию который вот мне в начале лекции сказали комментарий был в том
что вообще монтекарло это том чтобы считать интегралы вот да монтекарло это в том числе
о том чтобы считать интегралы давайте этот аспект осветим и вот сейчас уже частично мы
немножко туда пойдем в сторону биг даты вот и рассмотрим мы такую простую задачу значит
значит определение определение сначала дадим определение потом значит я расскажу как это
пригодится значит пускай у нас есть значит некое пространство на нем sigma алгебра ну пространство
исходов sigma алгебра мера на этом пространстве и оператор перехода который отображает омега в
мега вот не прошу прощения значит икс выкс но измеримый естественно оператор вот то есть
значит то есть мы преобразование это называется не оператора преобразование значит дальше мы
значит говорим что этот оператор это мера она инвариантно относительно действия
т что это означает для любого а из сигма алгебры значит верно что мю вот эта мера от пе минус 1
а ну то есть прообраз а равняется мере а вот такое определение то есть просто задана так сказать
стационарное распределение которое значит но оставляет эту динамику не подвинутка сказать
динамика такая что у нас эта мера если задать она как бы этой динамика и переходит сама в себя
так грубо говорит ну давайте примеры кинь рассмотрим вот предположим что икс примеры ну
зачем это нужно сейчас попробуем понять вообще и как это с манте карло связано пример первый
давайте рассмотрим в качестве икс как это называется отрезок 0 1 в котором точка 1
отоврестляется с точкой 0 то есть это окружность окружность единичная единичная окружность
это динамическая система да извините я забыл что это динамическая система просто дина
нет это прообраз это прообраз множество это не то что это же не так объяснить вот у вас есть
множество некоторое а и т-1 это все то что приходит в а вот что что как бы это сказать в результате
преобразование т приходит в а вот просто такое определение т-1 это же множество то есть это
прообраз множество вот и все мы считаем что да это корректно определенный оператор вообще все
что мы как бы пишем предполагаем что ну так сказать обладает такими свойствами что это
существует корректно определено вот значит при первый пример что это окружность единичного
периметра то есть мы отождествили точку 0 и 1 0 и 1 это как бы одна и та же точка в качестве от
омеги я не буду писать это баррелевская сигма алгебра то есть просто все измеримые множество
в качестве mu dx мы возьмем просто dx ну равномерную меру на единичной окружности а в качестве t мы
возьмем поворот окружности на соответственно иррациональный какой-то угол давайте я напишу это так
значит омега это элемент икса то есть омега принадлежит x переходит в омега ну дробная
часть в омега плюс альфа альфа это какое-то число иррационально иррационально иррационально
вот такая динамическая система ну просто пример динамической системы ну несложно проверить что
значит она действительно будет динамической системой то есть если вы зададите равномерную
меру вот ну на этой окружности да то есть просто горный рельеф вы нарисовали гору постоянной
высоты и преобразование которое происходит поворачивает все на угол альфа ну кстати то
что иррациональный вот конкретно сейчас это не критично я могу взять другой угол это тоже будет
мера инвариантная то есть одинаковая высота вот этого горного рельефа меры плотности одинаково
theta плотность меры я поворачив ча она переходит сама себя то есть все понятно менее тривиальный
пример называется это пример б��게 но сдвиг окружности значит поворот окружности поворота
кругом сейчас будет понятно к чему это все поворот окружности окружности второй пример значит да
это вот динамическая система. Не слышу вопрос. Какое умножение? Так, ну это же
рациональный угол, это просто поворотный рациональный угол и точка, если она
проходит через единичку, она просто от нее берется дробная часть. Еще раз в чем вопрос,
я пока не понял. Нет, мы еще раз. Т это просто некое преобразование элементов,
ну как сказать, отрезка 0.1 в себя, то есть считайте, что у вас 0.1, это просто есть множество
х, а Т это отображение, преобразование множества х себя. Вот, мы каким-то образом одной точке
к множеству х ставим другую точку множества х. Ну как мы это делаем? Вот по правилу, где это
дробная часть. Вот, ну давайте, может, второй пример рассмотрим. Пример два. Пример два. Это сдвиг
бернули, сдвиг бернули, бернули. Значит, пример заключается в том, что все то же самое, все то же
самое. Вот, но разница в том, что Т теперь будет, Т теперь будет даваться по другой формуле. Омега
переходит в дробную часть 2 Омега. Вот, то есть, чтобы теперь понять, что это все, значит,
действительно отображает правильно, значит, нам надо нарисовать эту функцию. Т это Омега. Одна,
вторая. Ну и так вот. Вот, и если мы возьмем какое-то множество, ну какой-то, так сказать, площади дельта,
то, значит, вот здесь будет дельта пополам. Прообраз дельта пополам. Ну и вот здесь будет
прообраз дельта пополам. Вот, то есть, выполняются условия. Да, ну двойку я не случайно взял. Вот,
ну и тоже я имею в виду, что тоже здесь х это 0,1. Ну и мера это значит, мю равняется, мю dx
равняется dx. Теперь вопрос. Вот все это как бы к чему. К чему? Вот то, что я сейчас рассказываю,
случайные процессы. Вот к чему. Эргодическая теорема. Я сейчас формулирую очень важный результат.
Мы уже в каком-то виде этой теоремы пользовались для марковских процессов случайных. Вот это не
случайный процесс, это динамическая система. Но вы сейчас поймете, что с точки зрения как бы того,
как мы пользуемся вот этими результатами, то нам не так уж и важно, что это случайный процесс. Это
может быть динамическая система, но порождать те же самые результаты. И это можно использовать
методок Монте-Карло. И так, эргодическая теорема. Эргодическая теорема Бергофа-Кинчена. Эргодическая
теорема. Ну Ифан Нейман. Эргодическая теорема. Да, вот мне задали вопрос, совершенно правильный,
что действительно, я как бы сейчас дал определение. Динамическая система оргодична. Существует
единственная вариантная мера. Давайте действительно посмотрим, что могут быть проблемы. Вот я задал
меру на окружность, дельта функция здесь, дельта функция здесь. Это мера? Да, это мера. Единственная
будет, так сказать, она будет вариантной относительно поворотов. Нет, подождите,
она будет им варианта относительно чего она будет один вариант на поворота на какой угол
то есть на п, она инвариантно хорошо это единственная мера, которая инварианта относительно поворота на п
нет потому что я могу здесь нарисовать вот ну 1 четвертая 1 четвертая 1 четвертая и вот
пожалуйста еще одна мера инвариантная относительно понятно да это важно потому что если вспомните
как-вот, случаи марковских процессов. Если, например, у вас есть вот такой марковский
процесс, вот такой марковский процесс какой-нибудь вот-да, вот он блуждает.
И вы стартуете вот отсюда, то вообще говоря у вас есть предел, у вас есть
предельное состояние, но это предельное состояние может быть вот здесь
вы окажетесь, а может быть вы здесь окажетесь и выйдете на стационах. От
здесь вы окажетесь и просто, так сказать, всегда всю жизнь будете здесь проводить, а если здесь вы окажетесь, то выйдете на какой-то стационар.
И в зависимости от того, куда вы свалитесь, от этого будет зависеть предельное распределение.
Более того, если вы изначально стартуете вот здесь, то вы в принципе не можете оказаться вот здесь.
Это означает, что как бы эргадичность – это единственность.
То есть не будет вот такой ситуации, что вот предел может быть разный.
Вообще говоря, предел может зависеть от точки старта.
Вот мы, я сейчас формулирую результат, и мы не хотим этого.
То есть мы не хотим, чтобы у нас от точки старта что-то зависело.
Это нам не нужно. И вот нам и нужна единственность.
Поэтому, когда мы говорим о том, что инвариантная мера единственная, это важная оговорка.
Вот формулировка теоремы. Пускай есть функция.
Для простоты будем считать, что она принадлежит классу L2.
L2 функции на вот этом пространстве x.
Ну там можно писать еще вот x. Правильно так.
То есть у нее есть, по сути, мы можем считать интеграл от f в квадрате.
Например, вот что это означает.
Тогда вот такой вот предел, единица на t, сумма f от t к от x.
Входится, и дальше тут вот уже варианты L2, почти, наверное.
И зависит от того, или даже просто для всех x.
Это вот уже зависит от того, в каком варианте вы эту теорему формулируете.
Если L2, то это фон Нейман. Если почти, наверное, то Бергов Хинчен.
Входится к мат ожиданию f от x по мере, которая инвариантная.
По мере, которая инвариантная. f от x dμx.
Вот сейчас мы с вами ощутим, значит, полезность этой теоремы.
То есть еще раз. Вот есть какая-то динамическая система.
И, соответственно, для нее можно тогда утверждать, что вот такая последовательность
Да, да, да, конечно.
Я понял. Да, с фантазией у меня, конечно, проблемы.
Так, ну какую букву возьмем? N. Вот. N большое.
Вот так вот.
Тогда вот такой вот предел при N большой равняется...
То есть среднее временное равняется среднему пространственному.
Ну, как бы это верно, если динамическая система оргаатична, то есть если существует единственная вариантная мера.
И тогда тут нет вариантов, чего ставить. То есть у меня здесь она единственная.
Если мера не единственная, тогда вот ответ будет зависеть от того...
То есть предел тоже может быть, но он будет зависеть от того, откуда я стартую.
Вот здесь, неважно откуда я стартую, можно говорить для любого x от из x.
Ну или для почти всех. Почти, наверное, для почти всех, как PV.
Для почти всех x из x вот это выполняется, если речь идет о сходимости почти, наверное.
Но если речь идет о сходимости в L2, то x просто тут играет роль, как бы сказать, как объяснить.
То есть по нему происходит интегрирование.
То есть это число, это число, это число, а это функция от x.
Это функция от x.
Но эта функция от x, если предел берется, она вырождается в константу.
Но дальше вопрос, в каком смысле этот предел берется.
Поскольку речь идет о функциональной последовательности, зависящей от x,
то мы, конечно, можем говорить о пределе почти, наверное, то есть для почти всех x это верно.
А можно говорить про L2, L2 имеется в виду для интегрирования по вот этому x.
Ну а здесь x другой. Давайте x с волной напишу, чтобы было понятно.
Это переменная интегрирования, а здесь x это вот вход.
Вот сейчас, наверное, стало понятнее.
То есть здесь это просто какое-то число переменной интегрирования, а здесь это параметр.
И что? В чем прок от этой теоремы?
На самом деле это одна из, может быть, не знаю, пяти наиболее важных теорем математики, на мой взгляд.
Если еще раз, что тождественная?
Ну если это тождественная ноль, то просто у вас точно будет не единственная.
Ну тогда это будет какое-то тривиальное утверждение.
То есть оно неинтересное будет.
Ну как бы просто давайте подумаем.
Если tt это единичный оператор, то там у вас непонятно, что будет инвариантной мерой.
То есть здесь как бы нужны какие-то содержательные примеры, чтобы эта мера стала единственной.
Чтобы она выделяла, как бы, что-то выделяла.
Ну давайте на этом примере мы сейчас разберем.
Я вернусь к дегубернуле попозже, а теперь разберем пример 1, вот точки зрения этой теоремы.
Ну что вы можете сказать в связи с теоремой оргодической и примером 1?
Что можно утверждать? Можно утверждать следующее, что если у нас есть последовательность, вообще говоря, не случайная.
А нет, tkt это просто, как бы сказать, t от x это же отображение.
Ну так давайте это просто степень отображения.
Ну tkt от x это есть t от t от t.
K раз так я должен сделать от t от x.
И вот это все k раз.
K раз.
Вот это есть tk от x.
Обычно скобки не пишут, поэтому это не я придумал.
Это как бы такие принятые обозначения в оргодической теории.
Ну давайте одну книжку укажу. Я г синай.
Оргодическая теория.
Синай. Оргодическая теория.
Оргодическая теория, как так называется.
Ну то есть, в принципе, так принято писать.
И t это тоже не я придумал.
Это, в общем, насколько я понимаю, это идет от Колмогорова, по-моему.
Чуть ли не от Колмогорова.
Но он этим занимался, потом его ученик Синай.
Кстати, Синай более-менее заработал в этом направлении.
Несколько лет назад получил премию Абеля.
Это одна из самых престижных наград в математике.
Ладно, ну как бы это давно было известно.
Бергов-Хинчин, я помню, там, конечно, более продвинутые результаты.
Ну что мы в примере можем сказать?
Что если взять последовательность xk,
которая есть, собственно, порождена tk
от произвольно выбранной начальной точки,
ну или точнее, для почти всех начальных точек.
На самом деле, можно доказать, что для, просто для всех.
Ну вот мы взяли какую-то начальную точку Омега.
Ну и порождаем последовательность,
хорошо, просто x мы тут обозначали.
x, да, значит просто x.
Ну и вот поворачиваем окружность k раз на угол альфа.
То есть это что такое есть?
Это есть, значит, x плюс альфа, плюс альфа, плюс и так далее.
Плюс альфа, кнопка закрывается и здесь.
Вот, ну ясно, что это преобразование вы легко можете осуществить тут k раз.
k раз.
Вот, вы это преобразование можете легко осуществить и тогда,
тогда что верно?
Что 1 на n сумма f от xk,
сумма f от xk к чему сходится?
Так, к интегралу от f от x dx по отрезку 0,1.
Ну, правильно ведь?
Ну, а теперь возвращаюсь к вашему вопросу.
Я хочу научиться считать интеграл на отрезке 0,1.
Что я могу делать?
Я могу генерировать случайный величин из равномерного распределения на отрезке 0,1.
И тогда, по методу Монте-Карло, у меня просто по закону больших чисел,
если xk, случайная величина, которая равномерно распределена на отрезке 0,1,
так, то f от x, f от x, ой, зря стираю.
Значит, то мат ожидания вот такой случайной величины.
Чему равняется f от x?
Ну, просто давайте x напишу.
f от x, мат ожидания, чему равняется?
Можете сказать?
Ну, чему?
Интеграл от 0 до единицы дальше.
Правильно, f от x правильно.
Теперь, если я напишу такую формулу.
1 на n, 1 на n, сумма f от xk, f от xk, где k iid.
iid означает independent, identical, distributed, независимо одинаково распределенный.
k от 1 до n.
То, что вы можете сказать, по закону больших чисел я должен написать.
Почти, наверное, к чему сходится.
При n, стремящемся к бесконечности.
Это же независимые случайные величины.
Ну, если xk независимые, то f от xk тоже независимые.
А если xk одинаково распределены, то f от xk тоже одинаково распределены.
Значит, они сходятся к мат ожиданию f от x.
Правильно?
Но мат ожидания f от x, чему равняется?
То есть, если бы я имел первозданно случайную последовательность, я бы имел такой же результат.
Да, только тут в предел.
То есть, я имел бы такой же результат.
Правильно?
Но я-то только что это получил в варианте совершенно другом.
Абсолютно не связанным со случайностью.
Ну, как бы кажется, не связанным со случайностью.
Так ведь?
То есть, получается, что на самом деле для части результатов каких-то вот нам нужно, например, использовать метод Монте-Карло.
Но у нас нет случайной последовательности.
Но нам она, возможно, и не нужна, потому что вот такой результат имеет место.
Более того, я мог бы и CPT на самом деле доказать.
То есть, там дальше уже зависит от того, в каком смысле мы понимаем случайность.
Но можно доказывать всякие более тонкие результаты, в том числе законы всяких повторных логарифмов.
Это уже совсем тонкие результаты теории вероятности.
И вот для таких динамических системы такое тоже доказывают.
И даже для более хитрых определений того, чем порождена последовательность.
Вот это место, оно чувствуется.
То есть, есть какое-то ощущение того, что Монте-Карло это не только о том,
что, в общем, у нас должно быть ай-айдили, случайность.
А это просто о том, что если у нас есть последовательность с какими-то свойствами,
но торгодическая последовательность, то она обладает, возможно, похожими нужными нам свойствами.
И можно этим пользоваться.
То есть, просто считать, оценивать интегралы.
Более того, что еще удивительнее, на самом деле оказывается,
что если специально выбирать эти последовательности точек не случайным образом,
а вот согласно алгоритму Холтона Соболя, то качество оценки получится лучше,
сильно даже лучше, чем если бы независимо и случайно.
То есть, существуют специальные способы генерации вот этих точек,
конструктивно-алгоритмически такие реализуемые, их можно использовать.
Если будет интерес, вы мне напишите.
Я это просто либо ссылку пришлю, либо отдельно расскажу.
То есть, за счет чего здесь можно как бы добиваться большой выгоды.
Ну а теперь, собственно, почему, собственно, Монте-Карло используется при подсчете интегралов?
Чтобы вот это посчитать, не нужно использовать Монте-Карло.
Надо использовать сумму дарбу или там либега суммы.
Это понятно, потому что, в общем, генерировать n точек,
это ничуть не проще, чем взять, разбить на отрезок.
И, соответственно, интегральную сумму посчитать.
А вот если теперь интеграл многомерный, если интеграл многомерный, то что тогда?
А вот тогда уже возникает совершенно другая история.
Потому что посчитать вот такой интеграл разбиением,
сколько будет маленьких кубиков в большом кубе?
Ну, если вы сторону делите на, условно, 10 частей, это мало,
а размерность у вас пространства 100, то у вас будет 10 в сотой степени кубиков.
И это при том, что вы мелкости разбиения взяли 10 на стороне.
А 10 в сотой это уже невыполнимо.
Интегралы, где как бы стомерные какие-то, вполне можно себе представить,
это вот как раз в анализе данных очень даже такое возникает.
А Монте-Карло ему, в принципе, все равно, по большому счету,
при условии, что он умеет генерировать хоть каким-то образом вот эту вот самую последовательность,
равномерно распределенную в нужной области.
И это как бы и есть такой трюк Монте-Карло.
Во всяком случае, в классическом варианте.
Он заключается не в том, что мы вот умеем одномерный интеграл считать.
Это демонстрация.
А трюк в том, что как раз в многомерии это становится очень даже актуальным.
Теперь вот этот пример.
Давайте подумаем, что здесь.
Кто-нибудь, вообще-то сказать, чувствует, о чем этот пример или не совсем.
Ну сейчас, я давайте сформулирую.
Ну то есть здесь тоже можно показать, что система эргодическая.
Вот эта система эргодическая была за счет того, что угол иррационален.
И мы не можем сделать какую-то такую периодическую, сингулярную меру.
Можно показать, что эта система эргодическая.
Не то, чтобы очень просто делается.
Но в книжке сена это показано.
Кстати, ТФКП там используется.
Что касается этой динамической системы.
Это тоже динамическая система эргодическая.
И у нее есть в каком-то смысле геометрия.
Давайте немножко по философствам.
Это правда важно.
Как можно генерировать по-настоящему случайную последовательность.
Вот эта последовательность обладает реально очень хорошими случайными свойствами.
Тут есть перемешивание.
Для предыдущей системы перемешивания нет.
То есть близкие точки остаются близкими.
И если вы берете две соседние точки, то уже нельзя оценивать f от xk и xk плюс 1.
То есть если вы будете брать агрегат на двух соседних точках.
То есть вот в этой теореме вы будете брать tk от x, tk плюс 1 от x.
То уже здесь нельзя будет написать функцию двух аргументов f от x, y и по двойной мере.
Вот это уже будет неверно.
А вот для этой можно такие обобщения делать.
Это система с перемешиванием.
Она хорошая.
Но возникает вопрос, который на самом деле очень важный такой философский вопрос.
Как тривиальная по сути система динамическая генерирует случайность?
Откуда она ее берет?
Откуда?
Это же противоречит интуиции, которая была у Калмогорова.
Ну во всяком случае я неправильно говорю.
У Калмогорова правильная была интуиция.
Она противоречит некой концепции, которую Калмогоров в свое время предложил.
Это концепция сложности.
То есть случайность это сложность описания.
Вот последовательность 1111 она не сложная.
Я могу сказать, компактно ее описать.
Это 1, 2, 3, 4, 5, 6, 7 единичек.
Я сказал 7 единичек, мог нарисовать миллион единичек и тоже сказать миллион единичек.
А попробуйте компактно описать как бы закономерный способ порождения
какой-то случайной последовательности.
И соответственно алгоритмически это невозможно сделать.
То есть соответственно, длина описания будет соответствовать самой последовательности.
Если она не случайная, есть закономерность, то я могу это кратко сделать.
А соответственно, если это случайно, то я не могу сделать.
Но и кажется, что в каком-то смысле, чтобы породить случайность, которую можно там интегралы считать,
мне нужно соответственно что-то сложное сделать, которое за счет сложности порождается случайность,
сложности писания алгоритма. Сложность этого алгоритма должна порождать достаточно сложные последовательности.
У меня простой алгоритм. Откуда здесь случайность в последовательности точек, которые генерируются?
Здесь же тоже инвариантная мера равномерная. Ну и то же самое, это точки случайные. То есть то, что будет получаться при такой динамике,
последовательность точек, это будет в каком смысле просто последовательность случайных точек.
Подказка. Почему все-таки это генерирует случайность? Вот почему.
Если мы рассмотрим битовое представление числа Омега, битовое представление, то что делает 2 Омега?
2 Омега сдвигает, поэтому называется сдвиг Бернуля. Она берет вот эту последовательность, представление Омега в виде числа 0, Омега 1, Омега 2 и так далее,
и просто берет и переносит, зачеркивает вот это, и теперь окошко вот здесь. То есть как бы, ну как называется, ну вот да, то есть просто сдвигает,
и 0, Омега 2, Омега 3 и так далее. Ну и следующий 0, Омега 4, Омега 5, и вот она последовательность считывает вот эти числа.
Ну понятно, что оно выдает все число, но мы-то знаем, что либо сюда, либо сюда попало, то есть либо попало в первую половинку отрезка, либо во вторую.
Так вот, куда попала новая последовательность, сюда или сюда, это реально более-менее первозданная случайная последовательность для почти всех Омега, точек старта.
Почему? А очень просто, просто потому, что почти все точки отрезка 0,1, это точки в битовом представлении, случайные, то есть динамическая система ничего хитрого особо не делает,
она вычленяет случайность из начальных данных, то есть если вы даете на вход ей кушать точку, и эта точка просто случайно взята из отрезка 0,1,
то случайность, которую порождает динамическая система, воспринята из начальных условий.
Это важно понимать, когда мы пытаемся объяснить, за счет чего это все происходит.
Ну тут происходит считывание просто начальных условий.
Ну и как бы иногда это объясняет тем, что экспоненциальное разбегание траектории, то есть такая неустойчивость порождает какое-то такое вот хорошее свойство, что все перемешивается.
Но это довольно красивая штука, и обратите, кстати, внимание, что в определении, которое я давал, оно уже, по-моему, стерлось, нет, оно не стерлось.
Очень важно t-1 от а, а не если бы я написал а, t от а, то это было бы неверно просто для этой системы.
То есть эта система наоборот, она в два раза увеличивает, а в обратную сторону то, что надо.
Вот резюмировать это все, мне хотелось бы примером, который совершенно неожиданный, мне кажется, на первый взгляд, называется Арнольд.
В твое время любил Владимир Игоревич Арнольд его рассказывать на разных мероприятиях для школьников.
Ну, на мой взгляд, для школьников, конечно, все-таки тяжеловато, а вот здесь прокатит.
И я бы хотел резюмировать вот это все очень красивым примером о цепных дробях.
Вот это вот эргодические всякие вопросы, а именно, ну, я только понимаю, что в целом более-менее понятно, о чем эргодическая теория,
что это в каком-то смысле обобщение законов больших чисел на последовательности, которые не то что не случайны, не то что не как бы марковские, они вообще могут быть не случайные.
То есть это в каком-то смысле обобщение и понимание того, что когда мы берем модель явления, нам вообще не принципиально, что это первозданная случайность.
Вот это важно понять, потому что вы когда описываете тот или иной процесс и вы говорите, ну, вот предположим, что шум винаровский, предположим, что гауссовский там шум, еще что-то,
да он может быть динамической системой порожден, но эта динамическая система достаточно долго пожила, или там много всяких факторов,
и по факту вы с хорошим качеством можете пользоваться вот этими результатами, но вам не обязательно иметь это первозданно, случайно, и вас никто не обманывает, когда говорит, что вот входящий поток заявок плацоновский, он реально плацоновский, но он не случайный, но он плацоновский,
он как бы потому, что если система большая, то вот эти закономерности все ярче проявляются, и вот в пределе то, что это не случайная последовательность, в этом смысле просто это исчезает,
вы можете пользоваться ей как случайной, ну, с точки зрения оценки интеграла, и вот уже такой не совсем big data пример, но с точки зрения математики то, что мне кажется нужно, это пример цепные дроби, цепные дроби, вот он есть в книжке Ширяева, например, по теории вероятности, цепные дроби, там тоже есть органическая теорема, еще есть хорошая книжка Каралова-Синайя, там тоже органическая теория рассказывается, я не помню, по-моему, там тоже есть, но не факт.
Значит, давайте рассмотрим опять полюбившееся нам пространство Х, отрезок 0,1, рассмотрим опять сигма-алгебру Борелевскую, и рассмотрим меру Гауса, которая равняется 1 на логарифм 2, а 1 на 1 плюс Х, ну, естественно, надо вести динамику, динамика описывается таким образом,
ты от Омега, значит, от Омега переходит в 1 на Омега дробная часть, но надо понять, что эта динамика вообще к чему она нам нужна, динамика это связана вот с чем, что, значит, предположим, что у нас есть какое-то число, 1 на корень из 5 на 2, золотое сечение, вот любое число можно раскладывать, так называемую цепную дробь, ну, в данном случае эта цепная дробь получается очень простой, бесконечной,
для квадратичных рациональностей эти дроби получаются периодически, для конечных, для обычных дробей можно строить тоже такое разложение, и эти дроби получаются конечные цепные дроби,
ну, а для каких-нибудь трансцендентных чисел типа П, эти дроби тоже можно строить, но они уже не удовлетворяют каким-то таким законам, о которых я сейчас говорил,
и возникает естественный вопрос, насколько, например, часто встречается семерка среди, вот это называется основание цепной дроби, то есть вот эта единичка зафиксирована, в общем случае цепной дробью называется вот такое разложение числа,
ну, мы считаем число из отрезка 0,1, поэтому сразу пишу 1 на А1 от Омега плюс единица на А2 от Омега и так далее, плюс единица на, вот пошло, вот так, да, и вот это называется основание цепной дроби,
значит, несложно понять, что преобразование, которое я сейчас сделал, это преобразование, которое что делает, оно берет и, соответственно, сначала вот это делает, да, то есть вот у нас получается вот такая вот штука, то есть основание мы смотрим,
а потом зачеркивает вот эту штуку, то есть фактически как бы вот это преобразование, вообще говоря, преобразование отрезка 0,1 в себя, оно, по сути, зачеркивает, оно как бы вот так делает, то есть оно вот оставляет вот эту цепную дробь,
и дальше возникает вопрос, ну, как, что означает событие, что А2 от Омега равняется m, вот нам интересно, как часто число m появляется в, соответственно, ну, среди оснований цепной дроби, но для этого нужно, чтобы что произошло, для этого нужно, чтобы соответствующая, ну, или не А2, а А1, давайте, А1 от Омега равняется m, для этого нужно, чтобы Омега принадлежала какому промежутку?
Какому промежутку Омега должно принадлежать?
Ну, наверное, наоборот, 1 на m, ну, тут, наверное, вот так, да, как-то так.
Хорошо, а теперь, собственно, наблюдение, что относительно вот этой динамики, относительно вот этой динамики, вот эта мера инвариантна, ну, как ее подобрали, это уже другой вопрос, но вот угадали, мера Гауса, значит, ну, на самом деле там Гаусс, Гильден, Вимана, Кузьмина, это вот в конце, ну, в второй половине XIX века, в начале этого века, есть замечательная книжка Хинчин, а я Хинчин цепные дроби,
а я это не целый, вот, значит,
ну, да, то есть, значит, дальше берем индикаторную функцию, то есть в качестве функции f от х берем функцию, индикаторную функцию, отрезка 1 на m плюс 1, ну, промежутка, давайте там уж, неважно, 1 на m от х, то есть эта функция равняется, значит,
значит единицы если x принадлежит вот искомому промежутку 1 на m плюс 1 1 на m ну и соответственно
оно иначе иначе вот ну и берем маргадическую теорему с этой функции то есть пишем что значит
единица на n тумма а к измите и от значит вот это вот индикаторная функция м плюс 1 на 1 на
м от соответственно точки которая получается тк от какой-то начальной точки x значит этот
предел при ну так вот я напишу как одного до n предел предел вот этого дела почти наверное
для всех x почти наверное по x при н стремящимся к бесконечности равен интегралу равен интегралу
который вот по этой мере значит один на логарифм 2 интеграл от нуля до единицы значит вот от этой
индикаторной функции индикаторная функция 1 на m плюс 1 1 на m от x да вот это функция ткс на что
там у нас 1 плюс x на dx на 1 плюс x вот и это можно явно посчитать значит у меня это записано сейчас
значит это получается логарифм по основанию так ждите до логарифм по основанию 2 вот так вот от
такой штуки 1 плюс 1 на м на м плюс 2 на м плюс 2 вот не очень много времени потратил на эту всю
все это дело но главное вот пояснил то есть это дробь такая 1 а тут дробь 1 на м плюс 2 так вот так вот
просто как бы давайте остановимся подумать что случилось по сути только что ну какими-то
оговорками я пояснил почему ну во-первых как бы два факта на самом деле первый факт что этот
предел есть то есть вообще говоря частота встречаемости числа м как основание цепной
дроби но вот в этой последовательности она существует теперь вопрос у кого она существует
у каких чисел ответ да у почти всех чисел она существует по терминбергов и хинчин у почти
всех чисел эта частота есть более того что самое интересное они просто есть она одинаково то есть
почти все числа из отрезка 0 1 они статистически они одинаковые то есть число частота встречаемости
того или иного числа м произвольного она у них одинаково вот это вообще конечно очень интересно
ну и замечу что если м большое то возникает степенной закон то есть что тоже как бы довольно
характерно то есть частота встречаемости от того или иного числа пропорционально 1 на
м в квадрате будет вот и чем меньшим тем постепеном и закон это частота меньше но ваша логарифма
один плюс что-то это приблизительно вот если это что-то маленькое вот это что-то и еще раз вот
смотрите я как бы рассказываю какие-то конструкции и рассказываю более-менее конструкции как бы
одинаковые но но но разные что у них общего давайте я проверю как вы это сказать вот вот понимаете
что общего в том что до сего момента рассказывал попробуем экспериментальным путем определить
вот насколько есть понимание происходящего кстати в зуме если кто-то хочет комментировать
задавать вопросы пожалуйста тоже это делайте в том числе если я сейчас вот какой-то вопрос
можно к аудитории можно отвечать зуме да нет подождите это я для примера взял а 1 м а когда
вы сделаете преобразование вы получите то же самое а 2 и так далее то есть это просто вопрос
о том вот при каком условии при каком условии вот в данном случае для этого числа а 1 от омега
равняется м когда а 1 от омега равняется мне неудобно формулировать это и в терминах а 1 от
омега мне удобно формулировать это в терминах омега это условия при котором омега принадлежит
вот этому и так я могу делать сделав преобразование я могу теперь про 2 то же самое сказать то есть
за счет того что здесь стоит тк я как бы последовательно применяю тоже самый трюк к 2 к 3 и
так далее вот за счет того что здесь тыка и еще раз все почти все числа из отрезка 0 1 обладают
одним и тем же частотным вот этими свойствами и так что общего то общего всем что я рассказывал
ну давайте попробуем это да не у всех а у почти всех да нет нет нет подождите не на месте окатова
что написано написан предел кат 1 до n 1 на n так это что значит это значит что мы первое слагаемое
первое слагаемое это вот это второе слагаемое это вот этому отвечает ну какому-то свойству этого
2 третья вот и так далее и поэтому когда мы считаем такой предел мы не говорим что мы то
сказать где-то в конкретном месте чего-то то сказать имеем даже как с человечками блуждающими по
графу мы не можем сказать кто где будет но мы можем посчитать сколько человечков там сколько там
вы какие-то и сделать это или частотным образом как часто человек в данном случае частотным образом как часто блуждающий
человечек был в первой вершине во второй в третьей тут аналогично как часто вам встречается число m
как часто не я не отвечаю на вопрос встречается оно здесь или не встречается я отвечаю как часто
среди этих чисел встречается число им ну для написанного числа но ни разу не встречается и вы можете сказать что это какой-то
результат обман это не обман множество рациональных множество
и рациональных вот этих квадратичных и рациональности она просто имеет меру ноль
ли бега меру ноль поэтому это не не есть как бы проблема ну да вот для них это неверно но о них мало этих чисел мало
не зависит почти почти для всех чисел она не зависит от того что это за число неожиданно
я понимаю я я не знаю до сих пор не могу как-то с этим
выкатываться хотя еще когда в общем ну в общем давным-давно больше 20 лет назад я все это
знал и как-то 20 лет прошло до сих пор меня это удивляет и и
как бы вдохновляет в том смысле что ну вообще это реально мощно и не только здесь проявляется мощь органической теории
мы не можем сюда отрицательные числа подставлять мы живем на отрезке 0 1 по правилам и
по правилам мы преобразуем число не отрицательное тоже в число то есть тут нет обмана то есть все честно
мы предположили что число из этого отрезка
преобразование сохраняет это свойство почему потому что дробная часть и вот и дробная часть нас спасает во всех этих
ну это не единственный пример и там их очень много но просто и так давайте все-таки
перейдем но это правда важно что общего-то во всем что я рассказывал
ну какие-то можно найти общие элементы к чему я все это вот я назвал это элементы теории случайных процессов но пока наверное не совсем понятно
почему теория случайных процессов
...
Ну хорошо давайте я понял я значит
смотрите мы вот на данный момент мы изучали всякие предельные
законы предельные теоремы и было важно понять что
В этих теоремах была некая общая логика, что нам надо что-то посчитать, что-то оценить, и мы придумывали некоторую динамику,
чтобы оценить то, что нам нужно, просто эта динамика имела вот это интересующее нас число, функцию, имела пределам.
Переходя к пределам, мы получаем то, что нам интересно.
И дальше, собственно, результат о том, что это предел, можно использовать как численную процедуру.
То есть мы знаем, что с отрезком 0.1, что точки не случайным иррациональным поворотом окружности образуют оргодическую последовательность.
Значит, интеграл от какой-то функции по равномерной мере, просто по отрезку 0.1, можно заменить вот такой суммой.
Почему? Потому что мы придумали такую динамику, которая, ну вот она, значит, оргодическая и имеет инвариантный вот такую меру.
На самом деле, Монте-Карло, это ровно об этом, вот самой общности, что то, что вам надо оценить, вы придумываете некую марковскую динамику,
или вот не марковскую, а просто стационарный случайный процесс, и, значит, как бы добиваетесь нужного вам результата.
Но, значит, мне 18-20 надо делать паузу. Но вот обратите внимание на такую вещь, что сейчас вот я вам рассказывал это все сквозь призму теории динамических систем.
Но если бы я с самого начала сказал, что у меня х, это вероятностное пространство, а не пространство с мерой, и просто точка старта выбирается случайно согласно этой мере,
тогда это что было бы за динамика? Еще раз, я описал некоторую такую процедуру, которая более-менее просто соответствует реальной динамической системе,
то есть какому-то детерминированному процессу. Но я мог поступить по-другому, я мог сказать, что точка старта выбирается случайно согласно этой мере, и понеслось.
Ну и дальше там преобразуется, преобразуется, преобразуется. Вот это уже был бы случайный процесс, ведь так? Правильно.
И тогда все, что я сейчас рассказывал, было бы уже, ну как бы тоже верно, но это уже было бы в терминах теории случайных процессов. Так ведь? Ага.
А теперь тогда такой возникает вопрос. А случайный процесс, который мы рассматривали раньше, просто набор независимых случайных величин, это же тоже случайный процесс?
Тоже, да. Марковский процесс, это случайный процесс? Случайный процесс, ну то есть, ну это как бы не некоторая такая случайная траектория.
Так, интересно, что дальше? То есть вообще сколько этих процессов есть? Какие процессы удовлетворяют вот этим свойствам, что среднее по траектории там, ну что-то там оценивает?
Вот этим вопросами можно задаться. И часть из них, вообще говоря, очень важно на них получать ответы, в том числе в финансовой там математике.
И вот, собственно, дальше то, что мы будем делать, я все-таки действительно начну рассказывать вам элементы теории случайных процессов.
Но вот сквозь такую призму, что, ну вот сейчас мы фактически посмотрели на так называемые, значит, стационарные процессы.
То есть вот если μ, реальная мера вероятностная, и я задаю точку старта случайно, то тот процесс случайный, который получается, он называется стационарный.
И в общем случае, если у меня просто есть какой-то стационарный случайный процесс, то вот эта теорема, это есть теорема, которая формулируется для стационарных случайных процессов.
То есть берется, значит, просто какой-то процесс, он называется x от t, ну обычно так.
Ну вот по нашей терминологии нам удобнее будет его называть, как бы, ну я не знаю, то есть у нас эта запись, ее в теории случайных процессов пишут вот так, x от k.
Вот просто случайный процесс принято обозначать буквкой x, вот так.
И теорему, которую я сформулировал в теории случайных процессов, она формулируется вот таким образом k от 1 до n, значит, стремится к мат ожиданию x от k.
Вот если, ну там неважно, какой x от k вы возьмете, потому что процесс стационарный.
То есть вы можете взять x от 1, допустим, неважно, что сюда подставить, неважно, что сюда подставить, потому что процесс стационарный.
Ну и стремление это, например, в L2, то есть как бы в пространстве, ну по сути это означает, что мат ожидания вот этой конструкции, вот этой конструкции, минус число, ну это уже число x от 1.
Вот это все в квадрате, стремится к 0, вот что эта запись означает в L2, стремится к 0.
Ну естественно, как вот, так сказать, мат ожидания берется по той всей случайности, что это вот результат в теории случайных процессов.
И мне было важно, чтобы на самом деле вы как бы почувствовали, что все вот это, что я рассказываю, это как бы грани, нечто такого, нечто более общего,
то есть связано с какой-то последовательностью, можно понимать ее случайной, неслучайной, но это как бы проявление одного и того же.
Вот чтобы закрепить вот этот материал именно с точки зрения случайных процессов, я расскажу просто совершенно реальный трюк,
который используется на бирже, на рынке фондовом, и это связано с big data, и это связано с оргодичностью, а именно,
предположим, что у нас есть некий процесс XAT, вот это тоже про оргодичность, он как бы закрепит, который имеет вид экспонента,
ну и здесь вот стоит AAT плюс некий sigma WAT, это винаровский процесс.
Я буду в будущем, может быть даже вот на второй лекции рассказывать, что такое винаровский процесс, это просто по сути предел случайных блужданий,
то есть вот процесс, который мы выведем, это все, то есть я имею в виду, что такое WAT, это процесс, который есть вот случайные блуждания,
только в пределе, шаги маленькие, то есть, а именно, вот это процесс, получающийся таким образом, что шаг делается в масштабе
дельта, сейчас, значит, 1 на n, ну как бы решетка, и здесь вот соответственно, скачок масштаба 1 на корень, вот, то есть,
и вот значительно, с вероятностью одна вторая наверх идет, с вероятностью одна вторая вниз, размер шага уменьшается,
при измельчении, ну и размер скачка, должным образом тоже эшкалируется, подробнее мы...
Это все будет, я выведу модель Бошелье Самуэйсона, это будет просто, просто, просто сейчас мне хочется, чтобы вы мне поверили на слово, относительно того,
что вот этим вот, часто описывают цены акции, цена акции ведет себя как такой процесс, называется геометрическое бронзское движение,
и дальше возникает вопрос, ну и что, вот я знаю, что это допустим так, то есть я в это верю, вот то, что я вам рассказывал,
что оно как-то жить помогает, что оно нам дает, вот, вот это вот, что оно нам дает, давайте, раз у меня время непрерывное,
я приведу аналог этой теоремы, ну в непрерывном времени, теперь я уж точно могу Т-большое писать,
потому что у меня, значит, ну давайте для простоты, делаю, мне так просто сейчас будет удобнее,
а единица на Т плюс один, вот, на Т плюс один, сейчас будет понятно, почему так, и вот я беру некоторую функцию от этого процесса,
Ф от ХТ, и хочу интеграл по ДТ, чтобы в Л2 это сходилось к А, я не знаю, чему равняется А, я хочу определить, вот моя задача,
я реально, когда, ну вот, сколько лет, 15 назад работал, ну как сказать, консультировал трейдеров, ну надо было просто вот,
заниматься некой математикой, связанной с работой на бирже, там на арбитражах сидели, я тоже про это немножко расскажу,
как это связано с такой современной математикой, так вот, ну это все в будущем, так вот, и реально вот возникает задача,
просто по траектории истории цены акции, оценить вот этот параметр, но как это сделать, ну, допустим, есть какая-то органическая теорема,
ну то есть, я могу взять траекторию, взять какую-то функцию от этого ХТ, и подобрать эту функцию таким образом,
чтобы вот это сходилось к А, теперь вопрос к вам, вот что я должен сделать с функцией ХФ, чтобы действительно надеяться на то,
что А там получится, вы можете не знать никакую математику, я имею ввиду, не знать, как доказывать что-то,
мне сейчас важно не то, чтобы вы умели доказывать это, это факт из курса случайных процессов,
там это, ну, просто строго обосновывается, почему такой процесс органичный, который я сейчас напишу,
мне просто хочется, чтобы вы почувствовали, как, значит, как придумываются вот эти вот оценки,
как я имею ввиду, придумываются результаты, вот как оценивать неизвестный параметр,
вот они придумываются таким образом, значит, что здесь должно стоять в качестве А,
в качестве А должно стоять мат ожидания некого, соответственно, процесса F от XT,
если это процесс стационарен, то у него мат ожидания должно быть постоянным,
но мат ожидания этой штуки, она вообще никак не постоянно, давайте поколдуем, давайте возьмем логарифм
от XT, чему это равняется? Логарифм от XT равняется AT плюс sigma WT,
ну, это уже хорошо, мат ожидания XT подходит, мат ожидания XT равняется,
ну, как бы если на T разделить, если на T разделить XT на T, то получается,
что это равняется A на sigma WT на T, ну, это уже неплохо,
потому что Винеровский процесс, он же постоянный, он как бы симметричный,
случайное блуждание, значит, у него T на ноль,
сейчас, сейчас еще раз, что надо, а, ой, да, да, да, да, да, да,
значит, мы ищем F как функцию от X, ну, хорошо, ну, ладно, пожалуйста,
проблема решена, но это правда не проблема, ну, то есть, сейчас я ищу F как функцию от T,
мне же это не так важно, то есть, мне важно здесь конструировать агрегат,
конструировать агрегат, мат ожидания которого равняется,
ну, то есть, просто какой-то свой случайный процесс, который давайте назовем YT,
то есть, YT, который был бы стационарный, вот я говорю, что YT порожденный XT,
это вот такой процесс, вот уже этот процесс YT, который задан на отрезке 1T плюс 1,
вот этот процесс, он будет, он не будет стационарным,
но он будет иметь мат ожидания нулевое, а на самом деле,
для вот этих арготических результатов, уже для случайных процессов,
оказывается, что и этого достаточно, то есть, если мат ожидания,
вот, то, что как бы единственный из tint вариантной меры, вот там, вот так,
если оно константо, то уже можно говорить об арготичности,
уже можно говорить об арготическом, то есть, стационарность является достаточным условием,
не необходимым, необходимым условием является вот это,
вот это условие у нас выполняется, и стало быть, мы можем, в принципе
принципе, брать вот такую штуку 1 от t плюс 1 на 1 на t от y от t dt. И вот это реально можно
проинтегрировать. И можно надеяться, что это будет приближено а. Почему? Потому что мат ожидания
этой штуки равняется а. И вот это действительно так. То есть можно это доказывать. Это называется
оргодическая теорема. Ну, к следствию, оргодическая теорема для случайных процессов. И это есть
в курсе случайных процессов идет как простое упражнение, которое на тройку. Вот чтобы получить
тройку на экзамене у студентов ФУПМА, ну как школа ПМИ и ФУПМ направления, то им задают
вопрос. Исследуйте Винеровский процесс на эргадичность по мат ожиданию. И исследуйте Винеровский
процесс поделить на t на эргадичность по мат ожидания, если процесс живет от t больше 1. То есть
то, что я рассказываю, это же как бы многократно проговорено в курсах. Поэтому я тут как бы не
говорю что-то новое, но контекст, в котором я все это рассказываю сейчас уже, он другой. Он не
такой как в случайных процессах. Сейчас будет пауза. Мы этот контекст как бы поясняем таким
образом, что если есть какой-то процесс случайный, это может быть набор независимых случайных
величин. Марковский процесс, я не знаю, Винеровский какой-то процесс, динамическая система, то
дальше вот собственно резюме. Пытайтесь найти инвариантную меру или пытайтесь найти мат ожидания.
То есть либо это типа закона больших чисел, значит, около мат ожидания концентрируется,
либо вот инвариантная мера это как-то описывает, но по инвариантной мере вы тоже считаете мат
ожидания. То есть инвариантная мера это как бы просто она вскрывает случайную природу. То есть
инвариантная мера это там, где ну вот тут ее нет вероятностной каких-то вещей и они появляются,
когда я ввожу инвариантную меру. И тогда появляется вот эта вот как бы связь со случайными
процессами. Здесь мне не надо вводить инвариантную меру. У меня уже случайность сама сидит и мне
просто надо пользоваться неким обобщением закона больших чисел. Ведь если бы xk были независимые,
то это бы сказали, что это тривиальный результат. Это просто результат, называется закон больших
чисел. Независимые случайные величины при усреднении сходятся к мат ожиданию. Все,
что я сейчас сказал, что на самом деле независимость не нужна. Можно ее ослабить до просто вот неких
условий, которые называются оргодичность. И эта оргодичность, например, обеспечивается,
если корреляционная функция вот этого процесса должным образом себя ведет. Это легко проверять.
То есть я не буду сейчас эти упражнения делать. Это просто стандартное упражнение в
курсе случайных процессов. И просто изучив корреляционную функцию Винеровского процесса,
то есть тот факт, что мин t1, t2, вот эта вот функция, это корреляционная функция вот этого процесса,
она будет t1, t2 умножить. То, что вот эта функция 1 на t в квадрате от 1 до t плюс 1,
1 до t плюс 1, значит стремится к нулю этот интеграл. Вот это условие дает dt1, dt2. Это условие дает
собственно нужное нам свойство, что есть оргодичность. Но это конструктивно проверяется.
То есть для случайных процессов это достаточно все просто делается. И резюме такое, что вы реально
можете оценивать неизвестный параметр уже теперь не с кем испытания Бернули, не какие-то
простые параметры, а вот в каком-то сложном процессе за счет наблюдения за траекторией.
И роль объема выборки здесь играет отрезок траектории. То есть в случайных процессах
роль объема выборки это длина отрезка траектории. И то же самое в Марковских процессах. Только в
Марковских процессах мы дополнительно еще можем оценивать скорость сходимости, зная spectral gap.
Здесь с этим посложнее. Здесь результаты я формулирую как бы не говоря о том,
как скорость сходимости какая. Я не знаю, мне кажется опять очень быстро рассказываю и
сложно это воспринимается. Давайте все-таки попробуем по обратной связи понять,
что дальше делать. В зуме есть вопросы? Нет. А какие-то комментарии? Все сложно. Хорошо,
в аудитории вот я так понимаю, что вы совсем загрустили, потому что слишком много математики
какой-то и она такая как бы без деталей. И вам становится сложно уследить. Да, но вот смотрите,
насчет деталей. Вот это мне и хотелось с вами обсудить. Понимаете, если я сейчас буду это
рассказывать с деталями, я просто прочитаю вам те же лекции, которые я в свое время читал,
когда я откул случайных процессов. Это занимает 2-3 лекции. Жалко. Да, то есть я думаю, что один из
вариантов я просто вам указываю, где про это можно подробнее прочитать. И вы просто вот то,
что я сейчас рассказываю. Ну как бы сказать, так вот концептуально без деталей вы можете там
посмотреть. Но вообще это и местами не очень просто. Например, вот конкретно мера гауса, ее мало
где можно найти, что это именно... Ну кроме того, чтобы проверить, что она инвариантна, понять,
как ее до нее догадались, ну я так не могу как-то быстро объяснить. То есть есть какие-то вещи,
которые просто действительно не такие просты. Но какие-то вещи просты, изящные, их даже стоит
посмотреть. И вот на следующей лекции мы как раз займемся тем, что начнем вот это понимать,
откуда это все пришло. Но, не знаю, то есть мне все-таки кажется, что я вас как-то так это...
Вот в этом и есть опасность, что не надо выбрать золотую середину. Потому что если я совсем
голопом по Европам буду, так сказать, то это будет вообще ни о чем. То есть просто какая-то такая...
С другой стороны, погружаться я точно никуда не хочу, потому что цель курса на вертолете пролететь
над этим всем. А чтобы успеть до конца долететь, надо все-таки достаточно высоко подняться. Ну как-то
вот... Ну то есть чтобы вот это все обозреть и как бы быстро лететь достаточно. Мне очень важно,
чтобы это было полезно и полезно не только присутствующим в аудитории, но и в зуме. Поэтому
чем больше обратной связи, желательно конструктивной, тем, конечно, оно полезнее.
Хорошо кто-нибудь резюме может сделать, ну просто сказать, как понял то, что было до сего момента
рассказано. Последний пример, он сложен тем, что мы еще просто не очень много говорили вообще
про случайные процессы. И как бы оно и понятно, что это не должно восприниматься. Это я вам еще
раз выведу просто. Вот вы узнаете, откуда это следует. Модель Башюлье с Эмуэльсеном мы просто
выведем и вообще про случайные процессы мы сейчас поговорим. Ну смотрите, ведь моя цель
здесь была просто показать вам ход мысли. Вот еще раз смотрите, у вас есть какой-то процесс. Вы
знаете, что принцип есть такой, что если предел есть, то его посчитать часто проще, чем доказать,
что он есть. Ну помните самый первый пример, который рассказывал. Поэтому давайте выдумаем,
выдумаем что-то, что имеет пределом, нужное нам неизвестное число А. Мы не знаем дрифт,
параметр сдвига или как там, ну да вот. И хотим его определить. Надо значит по этому
процессу. Ну наша цель-то чего сделать? А, найти, оценить. Нам же неизвестно А и Сигма. Ну Сигма
будем считать известна. Нам надо оценить А. Что нам дано? Дано ХАТ. То есть по траектории
случайного процесса ХАТ, ну вот реально задача из области финансовой математики. Оценить
неизвестный параметр А. Ну помните я говорил, что мы кидаем монетку и схемы испытаний вернули
вот это. Вероятность успеха. И мы пытались оценить схему вот эту вероятность успеха по результатам
бросаний. Вот ХАТ, траектория ХАТ на отрезке от 1 до t плюс 1, это и есть вот та самая траектория.
Вот траектория. Та самая выборка. Ну и дальше по этой выборке мы оцениваем А. Вот и все.
Как мы используем тот или иной результат? Вот в теории вероятности мы получаем так называем
определенные теоремы, законы больших чисел и пользуемся прямым образом. Нам даны последовательность
случайных чисел и мы по закону распределения, который известен этих случайных величин, мы
находим закон распределения суммы случайных величин или там выбора как бы среднего арифметического
или там каких-нибудь нормированных сумм. Это называется вот просто предельные теоремы. Но
в математической статистике мы делаем приблизительно то же самое, но в обратную сторону. То есть у нас
есть как бы сказать результаты измерений. Есть по-прежнему эта теорема предельная там
центрально-предельная теорема или клините результаты концентрации, но неизвестен закон
распределения вероятностей. Он может быть неизвестен с точностью до параметра, может быть
вообще неизвестен. И анализ данных и математическая статистика это о том, что как бы обратные
задачи надо решать, какие законы распределения породили эти данные. Из данных надо вычислить
эту информацию. И мы как бы, то есть понятно, что умея строить результаты в одну сторону,
это наверное как-то должно помогать строить результаты в обратную сторону. То есть имеется
в виду, что зная как распределены случайные величины, умение находить соответственно закон
распределения каких-то статистик, сумм, это полезно. Ну и понятно, что когда мы будем решать
задачу статистики, мы как бы будем тоже пользоваться этими же сами теоремами. И в этом
смысле вопрос, который был задан, он приблизительно о том же, что мы можем использовать эргодическую
теорему в варианте вот в таком. То есть, ну как бы, как объяснить, ну то есть это не совсем полная
аналогия, но то есть как бы у нас есть у нас есть мера, которая задана. Ну то есть вот то, что я
говорил в теории вероятности, задан закон распределения. Вот здесь мы просто знаем,
что мы хотим посчитать интеграл от резки от нуля до единицы по равномерной мере. То есть,
мы просто хотим посчитать интеграл от функции 0, 1, f от x. Все, нам задачу поставили. Тут нет
пространства для маневра. Я написал интеграл по d от x. Все, мера задана. У меня как бы в этом
смысле никаких других вариантов нет. Меру я задал. Стало быть, моя задача сейчас, а подобрать t,
то есть конкретно в примере поворота окружности, ну во всяком случае как я на этот пример смотрел,
у меня задача была подобрать такую динамику, которая бы оставляла вариантный вот эту равномерную
меру. Ну понятно, что здесь не надо быть особо прозорливым, чтобы догадаться, что если мера,
что такое мера, это просто горный рельеф. Вот это как бы плотность меры. Ну и если мы живем на
шаре, на земле, ну в данном случае на окружности, то какой должен быть горный рельеф, чтобы по сути
поворот, то есть уровень одинаков, то какие преобразования сохраняют вот этот уровень,
одинаков. Ну ясно, что повороты вот этой сферы, они, ну как бы вот этот рельеф, он не меняется.
То есть если у вас одинаковый радиус от того, что повернете эта окружность, она, ну как бы не
изменится ее радиус, это инвариантное преобразование окружности, группа поворотов на любой угол. И вот
эту динамику я здесь записал. Единственное нетривиальное место, действительно нетривиальное,
которое здесь было, это догадаться, что нельзя брать рациональные числа. Если я буду брать
рациональные числа, неважно какие, то будут возникать, ну они будут иметь конечное основание,
то я могу так разместить с вот этой вот частотой q с промежутками 1 на q, q это p на q дробь,
так разместить вот эти вот меры сингулярные, что повороты начнут их переводить в себя,
но поворотов может быть много. Вот пример один я приводил. То есть здесь задача была именно в том,
чтобы подобрать такую динамику, для которой эргадичность есть, не просто инвариантность,
а эргадичность. В этом была задача. Ну и отвечая на этот вопрос, я говорю, что этот пример
соответствует как бы теории вероятности подходу. То есть нам не надо ничего оценивать, у нас есть
закон распределения, мы просто пользуемся эргадической теоремой, то есть подбираем, собственно,
t как бы вот как-то объект изучения, как объект изучения, чтобы ну просто закон распределения здесь
задан. Вот здесь в обратную сторону. Я говорю, аналогия неполная, то есть я может быть сейчас даже
кого-то спутаю, но ну просто это разные как раз ситуации, что здесь я подбираю t, имея закон
распределения меру, то есть в каком-то смысле задав распределение. Вот здесь, наоборот, мне
интересна физика процесса. Я вообще как бы не очень понимаю, что здесь может быть в качестве
инвариантной меры. Это не очень тривиально, но интересна вот эта сама динамика, во всяком
случае в том контексте, в котором я это рассказывал. Эта динамика интересна, значит уже сейчас появилась
задача обратная. Подобрать такую меру, при которой эта динамика оставляет инвариантный,
и это очень важная задача, потому что эта тогда мера будет, во всяком случае, кандидатом на
симптотику, что по ней надо будет считать всякие, ну предельные интересные нам свойства вот этой
траектории. В частности, вот такую частоту встречаемости. Это не единственный функционал,
который можно посчитать. И здесь обратная история, то есть я задавал t, а интересно мне как бы было
подобрать mu. То есть t не мог варьировать, это было в условии задачи, t была задана по условию,
потому что мне был именно этот пример интересен. Вот здесь я обращу внимание,
что динамика могла быть другой. Помните, я писал сдвиг Бернули? Это другая динамика,
и я ту же цель выполняю, то есть я подбираю динамику по то, чтобы инвариантная мера была
равномерной. Это можно сделать, естественно, не единственным образом. Просто вопрос,
как вычислительно эффективнее это сделать. Хорошо, сейчас мы будем говорить о случайных
процессах, как предельных переходах от каких-то простых процессов, и придем вот к тому закону.
Я очень надеюсь, что все-таки успею в конце хотя бы семилетить анилинг вам рассказать. Очень
надеюсь, потому что это действительно такая важная с точки зрения и Монте-Карло, и всего
конструкция. Но если не успею, в следующий раз. Тем более, что мульти-левл Монте-Карло я, похоже,
сегодня не успеваю рассказать. Начнем мы с простого примера. Мартингалы, финансовая математика,
мартингалы, финансовая математика. Ну, пускай у нас есть рынок, на котором есть акция S,
которая может в цене вырасти фактор U больше единицы, ну, либо упасть за один промежуток
времени. Так, вот, за такт времени, значит, она может увеличиться, либо упасть. Ну, U и D известны.
Вот, пускай у нас есть так называемый опцион, то есть просто ценная бумага, которая вот в том же
промежутке живет вместе с этой акцией, вот, которая известен опцион, ну, как бы тем, что у него
опцион отличается от акции тем, что жестко задается цена исполнения. То есть, вот, будем считать,
что однопериодный опцион, и вот здесь C, U, C, D заданные числа известны, то есть просто
опционы, и вот это известно. В чем задача? Задача заключается в том, чтобы определить
справедливую цену опциона C, вот, то есть, соответственно, вот это какой-то исход, то есть,
природа что-то, так сказать, делает, и вот это соответствует, то есть, это одновременно происходит,
то есть, еще раз, вот это очень важно понять. Тогда, когда акция поднимается в цене, ровно тогда и
опцион поднимается в цене, то есть, он не поднимается, он будет C, U, а когда, значит, акция становится D,
S, то и опцион становится D, S, ну, условно можно считать, что цена акции единицы, это вообще неважно,
вот, ну и тогда, значит, цена опциона тоже, как бы, ну, в смысле, тут, ну, давайте S,
ну, не важно, просто можно считать, что C, U это какая-то функция от S момент времени 1, вот это,
давайте так, S1 момент времени 1, а это S2, вот, S2, это что такое, это с какой, ну, как, это либо U от S,
либо D от S, где S, это вот S1, вот, S это S1, а T от U, это будет некая просто функция от S2, вот,
это вот C от U и D от U, это C от D, это некая функция от S2, просто вот в этот момент времени, поэтому,
да, зависит от того, как скакнул этот опцион, так вот будет, как скакнула акция, так тебя будет
вести опцион, а значит, что задано, задано U и D, U и D известны, D меньше единицы, и они оба известны,
известны, известны, так, а что неизвестно, они, вот это тоже известно, а неизвестно,
C это неизвестно, ну, вот такая задача. Теперь, как бы, основной вопрос, вот, если такие правила
игры максимально простые, но все-таки, все-таки как-то связаны с реальностью, то в каком случае,
вот это все будет долго жить и счастливо, а не, так сказать, сразу по каким-то причинам исчезнет,
давайте подумаем. Да, ну и давайте еще я введу процент R, это банка, ну, ставка,
ставка банков, значит, этот процент можно класть в банк, банковский процент, процент вклада в банки,
вклад в банки, в банк, ну, то есть вы можете положить на месяц деньги в банк и не рисковать
на всяких этих опционах и акциях, так оно надежнее, но, естественно, R больше единицы,
но, правильно сказать точнее, то D меньше единицы меньше R, а меньше U, вот так вот, правильно сказать,
вот, когда, когда вообще все это имеет смысл и это будет жить, ну, давайте напишем условия на то,
что это действительно содержательно, что может, что может помешать тому,
чтобы это нормально существовало, помешать может то, что цена на опцион, вот, изначально,
будет несправедливой и это приведет к тому, что возникнет арбитраж, то есть человек,
который купит в нужных пропорциях акции и опциона, независимо от исхода, выиграет,
то есть он не проиграет, у него станет больше денег, чем было в начальный момент, вот, если это при
любом исходе возможность играть аккуратнее, то он не проиграет ни при каком исходе, а хотя бы при
одном выиграет, если такая ситуация возможна, то это называется арбитраж, то есть ничем не рискуя,
ни нулевой вероятностью, с ни нулевым каким-то исходом мы можем что-то выиграть, вот если арбитраж
есть, то по принципу рациональности рынка, он должен быть сразу исчерпан, то есть сразу найдется
арбитражер, который сообразит и исчерпает его, то есть не оптимальность, он выкупит и все это
закроется, и вот, значит, надо как бы описывать рынок таким образом, что этого быть не может,
а что это за условия, давайте посмотрим, значит, если у нас есть, значит, цена акции, ну, есть
какой-то набор, значит, допустим, вот тут x1, x2, это xs, давайте xc, xc, это вот пропорции, в которых
мы купили акции и опционов, вот у нас есть начальный момент, вот столько акций и опционов, значит,
цена акции s умножить на xs, цена опциона c умножить на xc, соответственно, в момент времени два,
что произойдет, произойдет то, что в одном исходе здесь будет что стоять, соответственно,
u s на xs, значит, так, здесь будет u c, будет t на xs, на xc, вот, а если второй исход произойдет,
то будет, соответственно, ds на x, на x, соответственно, s плюс c, cd на x, вот так,
но это все, не, подождите, c, u, cd, это индекс, это индексы, индексы, индексы, это не умножение,
это индексы, индекс, индекс, вот, это индекс, да, это важно, это индексы, индекс, так, хорошо,
но у нас есть процент, у нас есть процент, значит, таким образом у нас, значит, получается,
что это все на самом деле в момент времени, момент времени от два будет r, то есть вот так вот,
ну и что мы должны сказать, в каком случае рынок будет безорбитражный, то есть в каком случае у нас,
значит, будет выполняться, что, значит, одновременно два неравенства будут иметь
места, какие неравенства, ну, соответственно, что вот, вот эта штука больше вот этой и вот эта
штука больше вот этой, ну, понятно, что cd должно быть тогда, как бы подбирается опцион, чтобы он
страховал нас от, значит, такого падения, наоборот, почему наоборот, потому что это же как бы выгодно,
банк выгодно не класть, выгодно сыграть на этом, а дальше можно просто на этом бесконечно сыграть,
то есть орбитраж мы исчерпываем так, что если где-то хоть есть какая-то выгода, то есть это
неравенство строгое, то за счет увеличения вот этих пропорций мы можем, это же как бы, если в альф раз
увеличу все, то это, ну, инвариантно, то есть я могу в альф раз увеличить и, соответственно, прокрутить
на этом орбитраже весь тот объем неоптимальности, что есть на рынке, исчерпать этот орбитраж,
то есть я могу увеличивать все в альф раз, это условие того, что орбитраж есть, а мне нужно,
чтобы этого не было, то есть чтобы это была система не совместно, мне нужно, чтобы эта
система была не совместна, вот такая как бы история, но давайте напишем, значит, более
общую систему, пускай матрица R состоит из элементов R и gt, и это номер акции, а нет, это номер
исхода, номер исхода, ну, в нашем случае их два, номер исхода природы, исхода природы, ну, то есть,
грубо говоря, как в случае природы, как в случае разыгрался, а g это акция, номер акции, номер ценной
бумаги, потому что опцион это не акция, номер ценной бумаги, ценные бумаги, бумаги, ну, и вот,
соответственно, у нас есть портфель x из этих ценных бумаг, и вот мы, значит, R от x это,
собственно, вектор, который получается при разных исходах, в нашем случае это вот, вот такой вот
вектор будет R от x, и условие, которое нам интересно, в каком случае R от x получается совместно,
а нам нужно, чтобы оно было не совместно, потому что, если есть орбитраж, я могу в альфа раз все
увеличить, и понеслось, и я, как бы, буду бесконечно зарабатывать, ну, ответ, что существует вектор,
то есть, альтернатива, значит, либо вот это, либо вот это, либо вот это, что-то одно выполняется,
либо существует, то есть, это для всех, значит, либо существует такой вектор P, что, соответственно,
значит, P транспонированное на R равняется нулю, вот, то есть, мне нужно, чтобы существовал такой
вектор, значит, не отрицательный, ну и, как бы, неравный нулю, вот, естественно, неравный нулю,
для которого вот это было бы верно. Почему это так? Ну, в таком случае, значит, то есть, здесь вопрос
идет о том, что я беру столбцы вот этой матрицы, точнее, не столбцы, а строки, строки вот этой матрицы,
и задаюсь вопросом, когда существует возможность их, вот, значит, так сказать,
описать, поместить в полупространство, поместить в полупространство, когда это возможность есть.
Вообще говоря, это эти вектора задают какой-то такой типа Tetrider, ну, так сказать, в большем числом
грани, но вот такого типа множества, и если, значит, получается, что все, так сказать, векторы внутри
такого множества лежат, то замечательно, но если существует вектор, который, давайте, симметрично
вот это множество отражу, то есть, я просто могу нарисовать, ну, как бы, зеркально, так сказать,
симметрично вот эти прямые продолжить. Вот если у меня есть какой-то вектор, вот в этом множестве,
векторы все, в одном множестве, один в противоположном направлении, то такого быть не может, вот если есть
какой это еще вектор здесь, это непроблема, я все равно вот так гиперплоскость проведу.
То есть, мне, чтобы испортить ситуацию, мне нужно собрать конструкцию такого вида, что все векторы
лежат в каком-то конусе, палидральном конусе, а, соответственно, один вектор лежит в противоположности
этого конуса это единственный вариант единственный вариант когда это будет не так а
иначе это всегда будет так то есть чтобы этого не было мне нужно чтобы картинка была такая все это
лежит по лидеральном конусе один или хотя бы один вектор лежит вот здесь но вот это и есть условия
что ну то есть потому что иначе я могу гипер плоскость проводить по вот этим граням этого конуса и
они будут полупространстве соответственно но вот не сложно заметить что это условие и есть
поскольку этот вектор лежит выпуклой комбинации вот этих но с противоположным знаком то это и есть
условия как бы вырожденности то есть это и есть условия того что я написал не очень так сказать
аккуратно это сейчас об основы но как бы геометрия она такая что одна из альтернатив имеет место
либо все лежит в одной полуплоскости какой-то который можно найти либо соответственно есть вот
такая вот развилка и мы можем это использовать вот здесь и мы можем это использовать вот здесь
каким образом а просто написав условия п транспонировано нр равняется ну и из-за
этого условия мы сразу выводим вы можете это проверить но вообще-то в классических книжках
по поводу то сказать финансовой математики приводится что ц необходимо как следствие будет
равняться то есть в нашем случае это просто сводится к тому что матрица будет вырожденная
матрица при xs xc должна быть вырожденная иначе это условие не выполнится из условия вырожденности
матрицы мы получаем что ц равняется некая п на соответственно ц у и 1 минус п минус 1
минус п на цд никаких п в исходной постановке задачи вообще не было но появилась п которая есть
терминусу на у минус д которая возникла в этой задачи удивительным образом да то есть какая-то
вероятность появилась она называется маркингальная а которая при вот как бы если если ну что ли
человек какой-то оценивает исход не согласно п то есть если как бы люди вот не так делают то
орбитраж тогда возникает то есть если если цена то есть если как бы рынок если рынок исход не
исходит из того что это п это 1 минус п то возникает возможность орбитража и вот вот тут надо на
самом деле немножко подумать что как бы объяснить что вообще говоря в задаче нет случайности ну то
есть вы не знаете какой это сказать ну что произойдет поднимется акция в цене или не
поднимется вы также не знаете ну соответственно вот как бы это вспомогательная вещь просто на
одном рынке существуют разные ценные бумаги но если как бы вам интересно развивать некую
математику связанную с тем как оценивать всякие деривативы всякие производные ценные бумаги
зависящие от вот этих вот основных то фактически с точки зрения описания вот этой ценной бумаги
цена описывается так как будто бы это есть мат ожидания ну то есть есть мат ожидания вот
вот ну двух исходов что справедливая цена вот этого опциона это есть математическое ожидание
что с вероятностью п реализуются цу вероятность 1 минус по реализуется цд понятно что в жизни это
как бы но это искусственная конструкция ее ее как бы нет то есть есть просто так сказать поведение
людей но это поведение людей приводит к тому что цена опциона будет именно такая и это
можно использовать моделирование этой ценной бумаге то есть как бы вероятностей нет но они
порождены а дальше возникает ну это как бы вот собственно маркингальность она означает ровно то
что у нас как бы я сейчас напишу то есть если есть вот какой-то такой процесс длинный но то
есть можно продолжать вот это все то собственно вот такого типа формула ну неважно какой
дериватив я возьму цк плюс 1 мат ожидания от и тут берется условия цк тк там минус 1 и так далее
то есть мы фиксируем историю и берем условная мат ожидания вот если это равняется это же
случайно величина будет если это равняется цк то это называется маркингалом вот это называется
маркингалом и вот мы как бы по сути находим пейз условия что поведение вот этих финансовых
инструментов должны быть как бы маркингал себя вести вот возникает вот такие маркингальные такой
закон дальше собственно возникает вопрос вот это что такое вот это откуда и вообще как это
можно завязать на то что я сейчас рассказывал да да да да да да да именно из этого то есть
это просто упражнение надо но я не хочу на арифметике становится это это уже как бы значит ну просто там
начало чинче не успею сейчас значит то есть это просто следствие того что у нас вот получается
такая значит ну как бы вот необходимо вот условия что должна быть не совместность не должно
быть орбитража вот теперь давайте рассмотрим для начала совсем простую то сказать ситуацию что
у нас есть движение которое вот таким образом выглядит вот значит но давайте даже для
симметричности вот так сделаем чтобы совсем все было просто вот немножко эту картинку рисовал
но сейчас можно ее a немножко другом контексте привезти вот такой симметричная не обязательно
симметрично значит случайное блуждание вот случайно б internacional давайте для Multi schön
симметрично и сделаем значит такой скейлинг такой скейлинг значит пускай шаг будет какой то
да значит так так да шат будет дельта t ну или не знаю 1 на n 1 на n давайте 1 на n
а n это будет вот сколько здесь значит ну нам интересно допустим t шагов ну вот t на n
вот мы сделали t на n шагов t это просто момент времени пускай каждый шажок каждый шажок вверх
вот это же движение по этой оси он занимает дельта x ну то есть вот это дельта t дельта t да
а вот это дельта x и с вероятностью 1 2 происходит с вероятностью 1 2 все у нас все 1 2 и каждый
скачок занимает дельта x естественно дельта x как-то завязать на значит скачок скачок дельта x это
тоже дельта x вот дельта x ну вот пускай этот скачок как-то завязан на n сейчас мы поймем как
но прежде чем понять как давайте поймем что получается в асимптотике то есть если у меня
шагов довольно много н большое то значит чему равняется вероятность того что мой процесс
скакнет к раз то есть что значит будет давайте это назовем x от соответственно ну вот этот
аргумент мы назовем ка да точнее ка это будет как это называется 1 на n да начнут будет соответственно
ка от одного до n какова вероятность какой закон распределения вероятности будет что
столько то или там ну какой-то конкретный число будет скачков вот то есть значит как бы это
написать ну давайте ведем этот процесс с от ка я садка вот и нас интересует поведение
с отн вот и с отн так и ка у нас будет сейчас давайте все-таки вот так сделаем это значит
и большое будет момент времени а это индекс я ты маленькой сделай мне так удобнее потому что
запутаюсь дельта т будет тогда 1 н вот сейчас лучше вот с отн это цена акции все хорошо только
вот здесь я с отн да момент времени с отн а здесь просто с вот так все какова вероятность того что
и с отн и с отн равняется равняется ка на дельта x вот ка на дельта x но это вероятность того что
было просто какое-то число от соответственно от нуля до до до n получается ну а на самом деле от
то есть если уж так скачки у меня минус 1 1 минус и да и что это от нуля это не но
ну хорошо давайте с от нуля это 0 ка будет с от нуля значит с от нуля 0 давайте да
эст от нуля с ка будет но это на самом деле не очень важно но с от нуля 0 да да давайте чтобы
не вычитать тут ничего ему эта вероятность равна
ну все и все чего там из n пока на что дальше ну одна вторая тепень и чего да вот ну хорошо
это как бы с точки зрения значит таких вот как бы явного закона распределения вероятности ну а
можем мы как-то по-другому охарактеризовать я с отн например tpt что говорит что я с отн
поделенная на корень из n значит давайте поймем какая будет как что это что это за случайная
величина значит s от n это есть сумма стума до каких-то скачков и иксит их иксит их и от одного
до n как иксит это что за случайная величина какие значения принимает она принимает значение
дельты x с вероятностью 1 вторая и минус дельты x вероятностью 1 вторая так ведь и вот сумма
независимых таких случайных величин это есть с отн значит дальше я могу написать центральную
предельную теорему для с отн значит с отн поделить на корень из чего из дисперсии да из
дисперсии вот этой суммы чему равняется дисперсии этой суммы ну как бы если у меня есть случайная
величина которая принимает давайте просто в лоб сделаем значит квадрат этой случайной величины
я от иксит я от иксит и в квадрате чему равняется равняется дельты икс дельты икс и дельты икс просто
в квадрате это запитание индекс дельты икс в квадрате 1 вторая плюс дельты икс в квадрате 1
вторая так потому что просто по определению у нас два исхода вот этот и вот этот но я конечно
могу здесь по-умному написать минус дельты икс но в квадрате это все равно будет одно и то же ну и
получается что это просто дельты икс в квадрать вот значит то есть у меня получается мат ожидания
ко второму момента ноρο момент до дельты это квадрате мат ожидания равняется нулю мат ожидания
равняется ну you стало быть дисперсия просто совпадает вот с этой величиной я здесь пишу
дельты икс в квадрате но значит могу вынести дельты икс вот сюда дельта икс на корень из
так хорошо с этим разобрались а что здесь мы здесь тоже 0 поэтому я могу сказать что вот эта величина
должна иметь нормальное распределение новой симпатике
стандартное нормальное распределение вот что я получил но это что означает это означает что
в общем-то просто s от n s от n
значит имеет такой масштаб дельта x дельта x на корень из n на стандартная нормальная случайная
величина какая-то 0 1 да но это это извините это я все делал в предположении что у меня число вот
этих промежутков давайте это единичка пускай будет единичка что у меня число этих промежутков 1
на n ну так чтобы вот просто было каждый отрезок 1 на n ну я сейчас это максимально упростил чтобы
мне еще сейчас не возиться со связью с т вот то есть вот у меня каждый промежуток 1 на n 1 на n 1
на n если у меня дельта t 1 на n дельта t 1 на n то какой разумно выбрать дельта x что вы можете
сказать относительно того чтобы дельта x выбрать чтобы вы посоветовали мне выбрать в качестве
дельта x еще раз 1 на корень из n вы хотели сказать 1 на корень из n так что тогда будет тогда s от
n будет приближенно иметь закон распределения n 0 1 правильно 0 1 ok а если я возьму соответственно
вот течение не в момент времени 1 а в момент времени 2 то есть я просто сейчас вам рассказал
про сечение вот это я назвал s от n а мог бы это назвать значением некоторого прошкалированного
процесса в момент времени 1 то есть это давайте назовем как бы s от n я теперь вожу процесс
w от t который определяется таким образом он определяется как s от nt ну вот вот вот то есть
это тот самый процесс который я только что описывал только здесь еще вот как бы шкалируется на
t вот такой процесс но тогда тогда распределение этого процесса будет выглядеть вот так n 0 t 0 t
то есть у него сечение будет соответственно иметь нормальное распределение но с параметром t что
еще вы можете сказать про такой процесс w от то есть вот что мы вот сейчас что происходит вообще
что я делаю я сначала сказал что вообще говоря при оценке цены опциона имеет смысл поспользоваться
соображением арбитража и ввести некоторую вероятность мартингального вот при которой вот
можно считать справедливую цену опциона если цена опциона будет рассчитываться не из этой формулы
то возможен арбитраж и такой рынок вряд ли будет реально как бы существовать если будут
существовать то как-то это недолго такое будет наблюдаться ну и стало быть возникло понятие
некой вероятности ну здесь я упростила 1 2 1 2 более того здесь у меня процент простой то есть
аддитивно все прибавляется когда будет геометрическая прогрессия то будет хитрее но мы сейчас увидим но у
нас нам пока надо разобраться простой ситуации мы получаем некий случайный процесс пределе
случайных блужданий который обладает двумя свойствами первое свойство называется независимость
сечений могу я утверждать что если я возьму два промежутка времени то w от там я не знаю
tk-w от тк-1 запитает w от тк-1 минус w от тк-2 ну и так далее то есть просто возьму какой-то
промежуток t1 t2 t3 там и так далее что то что произойдет процессом на этих промежутках
независимые случайные величины это верно независимые случайные величины это верно
ну верно просто по построению я монетку кидаю независимо я монетку кидаю независимо это
называется независимость приращений независимость приращений приращений второе свойство
параметром 0 и мне неважно какие два
сечения я возьму то есть какое ты возьму
это просто схема такая она однородная и
можно здесь ну как бы однородность
понимается что зависит только от с я
могу здесь другие законы распределения
писать значит понятно да то есть что
произошло я сказал что по аккуратному я
должен был говорить что с помощью
принципа донскера про хорова мы делаем
предельный переход из случайного
julian получаем слабую сходимый случайного
процесса verses вhaps кому процессу но это
все оговорки которые по факту вот вот
вот это вот мы поняли а вот этот
ревяльные какие то свойство самой
схемы вот то есть ещё раз cz Roger
процесс он родился родился просто
случайно бруждаем и получился в those
что процесс теперь возникает вопрос а
это вообще как бы какой-то процесс
часто возникает что из жизни да ровно
потому что нормальное распределение так
часто возникает в жизни и винерский процесс часто возникает в жизни единственное что конечно тут
бывают всякие дисперсии другие дрифт какой-то то есть не обязательно 1 вторая 1 вторая но это в
общем есть что-то что притягивает в каком-то смысле сумму разных мелких случайных процессов более того
собственно оказывается что число процессов которые удовлетворяют вот этим двум свойствам первое
свойства независимость превращений и второе свойство без уточнения что за закон распределения
давайте я это так напишу зависит распределение зависит только от с не распределение не зависит
от п распределение не зависит от п не зависит от вот эти два свойства они достаточно точно
характеризуют класс процессов это называется процесс или и и вот почему это как бы какой-то
очень такой привилегированный класс сейчас станет понятно вот в чем дело оказывается что если у вас
есть вот такие случайные величины которые обладают свойством значит значит что сейчас
как бы сформулировать что то есть x случайная величина x имеет такой закон распределения что
для любого n существует а я иди и x кн вообще говоря зависящие от n вот что верно представление то
есть вы для любого n можете подобрать можете подобрать такие случайные величины что это
верно ну например для нормального распределения это верно 0 1 равняется стумма случайных величин
которые есть 0 1 на n вот значит но если для любого n можно подобрать такие независимые одинаково
распределенные случайные величины что это верно то значит хорошо значит вот у вас это как бы
называется безграничная делимая случайная величина то есть их безграничная делимая безграничная
безграничная делимая по определению делимая случайная величина это некоторая привилегированная
такая случайная величина дождись по ассон от лямбды делить на n вот как одного да и это
просто одинаковые случайные величины естественно по распределению и они должны быть а иди то есть
понятно что нормальное распределение просто на какие то особые распределение которые обладают
свойством самоваспроизводимости есть например возьмете равномерную случайную величину она не
обладает свойством нам безграничной делимости и соответственно возникает вопрос чем от из случайной
величины примечательны эти случайной величины безграничной делимой примечательны тем что они
и только они могут возникать у таких вот сумм, если у вас есть сумма xk от 1 до n, вот и вас интересует,
к чему это может сходиться в пределе, вот когда эти одинаково распределены, это id, то здесь может
возникать только такая величина, то есть с одной стороны это определение, что должно быть точно
равенство, с другой стороны вы можете брать какие угодно здесь случайные величины, если предел есть,
то он неизбежно будет вот из этой серии, то самое замечательное это то, что на самом деле вот
класс величин, которые вот такие, это класс величин, либо нормальная, либо так называемая
Пуассоновская, сложная Пуассоновская случайная величина, которая определяется вот таким вот
образом, это индекс суммирования, а это Пуассоновская случайная величина, то есть k большое это индекс,
давайте и назову, а то будет путаница, то k большое это Пуассоновская случайная величина,
а это соответственно id, независимые какие-то одинаково распределенные случайные величины,
неважно как, то есть у меня получается, что вот этот вот x, вот этот вот x это либо нормальная
случайная величина, либо ну частный случай, когда w тождественная единичка, это будет Пуассон в чистом
виде, но можно обобщить Пуассон и сказать, что это сложный Пуассоновский процесс, он получается так,
что w и t, каждая имеет распределение какое-то, может даже несобственное, то есть например равномерное
на каком-то неограниченном множестве, и вот соответственно k это Пуассоновская случайная величина,
все других быть не может, то есть в каком-то смысле базис это вот эти два, а дальше это как бы так
называемая сложная Пуассоновская случайная величина, это вот эта, она порождена Пуассоном, и вот теперь
оказывается, что если мы рассматриваем нам интересные процессы, которые подобно только что
описанному обладают свойством независимости приращений и однородности, то это неизбежно процессы,
которые порождены безгранично делимыми случайными величинами, то есть ничего кроме безгранично
делимого закона здесь быть не может, вот и все, то есть тут обязан быть безгранично делимый закон
распределения, иначе это несовместно, как это получить? Очень просто, возьмите соответственно x1,
вычтите x0, напишите, что это равно для любого n, для любого n это есть xk n
плюс 1 минус xk n, вот, значит и такую сумму возьмите от k, от 0 до n минус 1, для любого n это верно,
значит вот эти случайные величины они по определению iid, они по определению iid это верно
для любого n, все, все доказательства, то есть у меня варианта нет, кроме как просто ну я тут
могу взять любой отрезок, мне не важно какой отрезок взять и провести ту же самую конструкцию,
то есть получается, что если мне интересны процессы с независимыми приращениями и однородные,
то неизбежно это процессы, которые порождены безгранично делимыми законами, то есть получается,
что фактически, фактически, это два процесса, ну то есть как бы в основу лежат, это винаровский
процесс, который я только что описал, потому что у него нормальная величина, а второй процесс
это пласоновский или сложный пласоновский, то есть сложный пласоновский процесс, это вот что такое,
такое kt это пулассоновский процесс пулассоновский процесс значит это просто вот вот что такое
значит случайная величина которая имеет экспоненциальное распределение с параметром
лямбда в момент когда она реализуется происходит скачок то есть процесс от нуля качит идет
единичка момент когда реализуется вторая независимая случайная величина т2 происходит еще
один скачок она также распределена вот он значит процесс уже 2 значит принимает значение 2 ну и так
далее то есть здесь используется тот факт что показательная случайная величина давайте ее все
таки кси напишем потому что сейчас я буду ста возиться вот она характеризуется тем что кси при
вот вот такая вероятность при условии что кси больше чем т это вероятность равняется тому что
кси больше чем тал то есть отсутствие последействия вот и собственно отсюда неизбежно следует что
это есть с точностью параметра вот такой закон то есть вероятность того что кси больше чем тау
это есть показательная закон это есть собственно July минус лямбда тал и не сложно показать что
собственно вероятность того что кси реализуется в промежутке t на t плюс дельта t вот что кси при
условии что кси больше чем t мы уже дожили до t то есть кси как бы уже до t мы дожили что это
вероятность просто равняется лямбда дельта t то есть при условии что мы дожили до дельта t
ну извините до t мы дожили до момента времени t какова вероятность того что случайная величина
именно вот в промежуток t t плюс дельта t скакнет лямбда дельта t то есть неважно сколько мы уже
прожили вероятность того что скачок произойдет сейчас а это одна и та же вероятность ну в этом
промежутке как это получить ну очень просто надо воспользоваться тем что значит вероятность того
что кси принадлежит t плюс дельта t ну вот этому промежутку t плюс дельта t значит и одновременно
что кси больше чем t вот вот это произведение событий оно как бы тавтологично потому что вот
это уже как бы ну так сказать конкретнее говорит чем вот это ну и разделить это надо на вероятность
того что кси просто больше чем t то есть у нас получается здесь надо посчитать производную
вот этой плотности получается λ предсказам да и в степени минус лямбда ты на дельта т но об
знаменателе получается ей минус лямба t ц leftover ну и вот отсюда получается я надо дельта t вот
это яançо дельта te это лямба дельта т а означает отсутствие послед действие то есть процесс как
Ну неважно, сколько вы уже жили, скачок произойдет с равной интенсивностью в любой момент, вот такой
Пуассоновский процесс, а сложный Пуассоновский процесс, это процесс, который определяется вот
таким образом. Сумма Витых тех же случайных величин, что и раньше произвольных, а здесь стоит Пуассоновский
процесс. И это можно понимать, как сделки, вы продаете что-то и сделка случается вот с постоянной
интенсивностью поток клиентов, сделка случается, вот размер сделки, случайная величина, вы живете,
продолжаете жить, снова сделка, случайный момент, вот это количество сделок, а это вот каждый раз
какую прибыль вы имеете, ну это вот прибыль ваша за день. И можно поисследовать, как себя вот эти
величины ведут, в общем это есть такая современная теория случайных процессов, нам сейчас важно
другое, нам сейчас важно, что вот теории случайных процессов, даже как в Теории вероятности, есть
привилегированные законы. Это Винеровский bias, это такой случайный процесс, который
привилегирован и пуассоновский. Вот они выполняют ту же самую функцию, что и
соответственно в Теории вероятности выполняют нормальное распределение и распределение
плосона. Кстати сказать, давайте сейчас паузу легкую такую, я вас проверю, я этим
резюмирую сегодняшнюю лекцию. Это все, что я, пожалуй, спиру. Нет, x появится тогда,
когда я здесь буду говорить, что мы будем, соответственно, цена акции в u раз увеличивается,
в d распадает. Ну то есть нам надо, чтобы получить экспоненциальный закон, то есть чтобы вот так вот
написать, что s от, ну то есть вот все это было в показатель экспонента. Мне надо, чтобы это
суммирование было в показатель экспонента. Для этого надо, чтобы с вероятностью 1 вторая не
добавлялась дельта x, а с вероятностью 1 вторая умножался на, ну что-то умножалось на множитель
1 вторая, вот тот же самый, единица на корень из n, вот что-то такое, тут какой-нибудь u, вот так вот,
вот что происходило. А соответственно, это с вероятностью 1 вторая, вероятностью 1 вторая,
это процент, это, ну как сказать, умножается. И с вероятностью e, соответственно, минус d на
корень из n, нет, там как бы d, там сейчас единица, это надо все-таки писать вот так,
минус d на корень из n, вот, с вероятностью 1 вторая. Ну давайте я до этого дойду, просто, и это
будет немножко другое, то есть вы не аддитивно добавляете, а как бы мультипликативно факторы
добавляете, то есть то, насколько вы смещаетесь зависит от того, где вы находитесь, вы находитесь
здесь и просто умножаете на вот этот фактор, ну и так далее. И эта предельность возникнет
в экспоненте, то есть произведение расписывается как сумма, если вы будете смотреть в экспоненту,
и поэтому здесь появится x от sn, то есть вот, ну не будем сейчас торопиться. Значит, я хотел,
значит, подождите, а на чем я остановился? Я чего-то хотел... Да, хорошо, вопрос такой. Ну,
допустим, вы играете в казино, это очень плохо, но допустим, и мы сейчас вас проверим, как вы
вы оценивали, вот решали задачу. Предположим, что вы ходите в казино, ну давайте, тысяча раз,
тысяча раз вы сходите в казино, вероятность выиграть в рулетку 1.36, 1.36, как бы вы оценили
вероятность того, что вы выиграете, ну скажем, не знаю, от там, сейчас, так, сколько там будет,
ну, допустим, от 20 до 40 рубля, значит, сейчас скажу, значит, каждый выигрыш в рулетку дает вам
условно доллар, но вот какова вероятность того, что вы выиграете вот что-то, сумму вашего выигрыша
будет в промежутке от 20 до 40, если один раз выигрыш, это один доллар, а раз выигрыши тысяча,
как бы вы эту задачу решали, просто играем в рулетку, в рулетку, тысяча розыгрыши,
розыгрыши, значит, вероятность выигрыша, это вероятность выигрыша, вероятность выигрыша,
выигрыша одного доллара, и какова вероятность того, что этого мы выиграем вот столько,
ну, наверное, надо как-то оценить, так сказать, что надо сделать, надо применить центральную
предельную теорему, и центральная предельная теорема говорит, что число выигрыш, выигрыш это что
будет, мат ожидания, то есть это нормальная случайная величина, мат ожиданием равным 1000 на 36,
так ведь, да, дисперсии какой, ну, 1,36, соответственно, 1-1,36, вот так вот, что-то такое,
да, да, ну, еще надо умножить это все на 1000, это что такое будет, это распределение, распределение
вот этой вероятности, стало быть, вероятность того, что мы выиграем вот от 20 до 40, надо просто
посчитать вероятность того, что это случайная величина нормальная принадлежит, соответственно,
вот этому промежутку это можно сделать, а теперь такой вопрос, а если все то же самое, что я сейчас
сказал, но играете вы в казино тысяча раз, а вероятность выигрыша у вас 1,500 и 1,300, давайте,
что тогда, как бы вы решали эту задачу, вот, вот, вот, то есть здесь бы я задал вопрос, какова вероятность
того, что вы, например, выиграете три раза, какова вероятность того, что вы выиграете там три доллара,
если здесь такая вероятность очень маленькая, конкретная вероятность, то здесь вполне можно
задавать вопрос, какова вероятность того, что вы выиграете три доллара, вот, обратите внимание,
что здесь вероятность в каком-то смысле можно считать обратно пропорционально числу исходов,
то есть если p равняется лямбда на n, где это n, да, ну, где лямбда у нас видимо 1,3, 1,3,
то это будет скорее закон пуласона с параметром лямбда, вот, закон пуласона с параметром лямбда,
ну, и соответственно вы можете эту вероятность оценить, считая, что ваш выигрыш, это есть
пуласоновская случайная величина, почему я это, более-менее одна и та же задача, если немножко
поменять схему, она становится подходящей, ну, либо к пуласону, либо к нормальному, и этот пример,
он просто показывает, что отчасти вопрос шкалировки, что у вас возникнет Винеровский
процесс, там, пуласоновский, что именно, вот, как бы, здесь же тоже, смотрите, пуласоновский процесс,
ну, пуласон возникает там, где до этого нормальность была, та же самая схема,
да можем, а зачем, но это же, как бы, ну, еще раз, смотрите, мы абсолютно честно будем делать то,
что вообще у вас не было в курсах, то, что у вас есть хорошо в курсах проработано, ну, зачем мне
рассказывать, значит, критерии органичности, если в курсе случайных процессов, это одна из
главных теорем, зачем мне дублировать, это когда это уже есть курс, где это есть, ну, и подобные
соображения, я, в общем, буду стараться то, что доказывать, в основном, все-таки это какие-то
вещи, которые, ну, действительно сложно где-то еще найти, ну, блин, мне надо в 1930 заканчивать,
сейчас в 1933, ну, давайте тогда корраговоркой просто, ну, я не знаю, то есть, если вот в этой
схеме, которую я сейчас рассказывал, сделать вероятности переходов, я просто напишу результат,
а то это будет очень долго, и все, и мы на этом закончим, это займет две минуты, значит, если в
схеме, которая только что рассказывал, сделать, значит, вот так, вероятность сложна, ну, давайте
введем процент, пускай он будет, значит, пускай он будет µt, не, ну ладно, я сейчас долго буду писать,
это будет не очень понятно, давайте лучше в следующий раз, это долго, я в следующий раз вам сразу
начну с геометрического Броновского движения, то есть, по сути, е в степени Виннеровский
процесс, вот откуда это появляется, и вот мы, то есть, вернемся к тому примеру, это первое,
второе, что мы сделаем, это посчитаем дифференциал, и поймем, что такое уравнение и то, и уравнение
Калмогорова-Фокера-Планка, уравнение Калмогорова-Фокера-Планка позволит нам объяснить,
что такое семилетний тоннелинг, то есть, мы просто обоснуем то, что повсеместно используется для
решения задач глобальной оптимизации в big data, в том числе, ну, вот когда это еще называют имитацией
отжига, а потом мы расскажем multilevel Monte Carlo, что такое метод multilevel Monte Carlo, ну и, в общем,
потом мы начнем переходить к другим случайным процессам, которые есть мартингалы разности,
мартингалы в стахастической оптимизации, и попробуем понять, что, на самом деле, кроме
этих классических неравенств Азума Хёвнинга, там есть неравенство Бернштейна, и это уже очень
современно Бернштейна Фридмана, и вот с помощью него мы клипинг, попробуем понять, что такое, это
очень такая современная процедура обрезания хвостов 100 градиентов, она изложена, например,
в книжке Goodfellow Benjo-Courville, но вот, чтобы понять математику за ней стоящей, это вот потребуются
такие тонкие неравенства концентрации, как Азума Хевдинг, точнее, Бернштейн Фридман. Да,
я что-то не успеваю рассказывать все, что хочу, это проблема, но я все, что сегодня рассказал,
вам ссылки пришлем, естественно, в телеграмме, и, ну, видео, которое будет выложено, там,
естественно, к подписям, подпись к видео будет.

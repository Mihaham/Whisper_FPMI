Помните, когда-то перед лекцией про масштабируемость, на которой мы занимались поиском узких мест в распределенных файловых системах и киваревых хранилищах,
я говорил, что эта лекция, во-первых, новая была, во-вторых, очень важная, потому что все, что мы делали до этой лекции,
было нужно для того, чтобы в какой-то момент взять и аккумулировать их в некоторые дизайны, настоящие промышленные системы, увидеть, как это все работает вместе.
Вот сегодня скажу то же самое, что сегодняшняя лекция про то, чтобы собрать вместе все, что мы изучали три месяца.
Сегодня мы воспользуемся нашими знаниями про multiprocess, про версионируемое хранилище,
про Colossus, про TrueTime, про протоколы транзакций и соберем это все в одну большую систему, которая будет называться Google Spanner.
Это будет наша основная иллюстрация, ну, по крайней мере, на первую половину занятия. Я надеюсь, что это все окажется нам полезным.
Поводом поговорить про Google Spanner является наша сегодняшняя тема, это распределенные транзакции.
Помните, я многократно уже рисовал такую слоеную архитектуру, где у нас в основании был слой хранения,
потом был слой репликации, потом был слой шардирования, ну и вот наконец транзакции.
Сегодня мы до этого слоя доберемся и соберем всю эту конструкцию вместе.
Но чтобы перейти к распределенным транзакциям, нужно вспомнить про транзакции нераспределенные,
потому что это было достаточно давно уже. Итак, я напомню, у нас есть такая задача.
Есть хранилище, которое пусть умеет выполнять простые операции.
Мы можем в него что-то записать и можем из этого хранилища потом прочитать то, что мы записали.
Читаем и пишем мы по ключам. Мы сейчас ничего не говорим про то, распределенное это хранилище,
или это локальное хранилище, или это может быть даже просто память в компьютере.
Нам это не важно. Но нам важно, что это хранилище является атомарным, в смысле линейризуемым.
С ним можно работать конкурентно, и конкурентно исполнено этим хранилищем.
О нем можно думать как у последовательного, который при этом еще уважает порядок операции в реальном времени.
И мы над таким хранилищем хотим выполнять транзакции.
Интуитивно транзакции – это операция, которая трогает атомарно сразу несколько ключей.
В нашем случае транзакции – это интерактивные программы, которые начинаются со служебной операции startTransaction,
за которой следует серия записей и чтений, которые организованы какой-то логикой.
Мы читаем один ключ, смотрим на результат, и на основе того, что мы прочитали, делаем какую-то запись.
И завершается эта интерактивная программа, эта транзакция, либо служебной операции commitTransaction,
которая означает, что пользователь, клиент нашей системы хочет зафиксировать те изменения, которые он сделал в хранилище,
или же abortTransaction.
Пользователь проверил какие-то инварианты в своей транзакции, они не сошлись, поэтому транзакция откатывается.
И пользователь адресовал все эти операции, все эти шаги транзакции компоненту, который назывался планировщик.
Планировщик находился между клиентами, которые выполняют транзакции, и хранилищем.
Он получал от каждого клиента, пусть последовательно, все его операции, и его задача была перенаправлять их в уровень хранилища таким образом,
чтобы с одной стороны транзакции исполнялись параллельно, а с другой стороны пользователь получал какую-то понятную модель исполнения этих транзакций.
Можно представить себе очень простую модель, где все планировщики выполняют транзакции просто последовательно.
У него есть внутри один глобальный Mutex, и он упорядочивает все транзакции.
Это было бы делать очень неэффективно, зато мы бы знали, что планировщик порождает, чтобы транзакции исполняются незатейливо,
и не нужно думать про то, как они конкурируют. Так делать, конечно, не нужно, но нужно исполнить транзакции параллельно.
Но вместе с этим нужно предоставить пользователю какую-то понятную модель изоляции.
Конечно же планировщик хочет запускать параллельно чтение и записи в хранилище.
Между планировщиком и хранилищем рождаются конкурентные истории, состоящие из чтений и записи отдельных транзакций.
А поскольку само хранилище является линейризуемым, то об исполнении планировщиком всех записей и чтений
можно думать как о некоторой последовательной истории из отдельных записей и отдельных чтений.
То есть любая конкурентная история, о ней можно думать как о последовательной.
И можно сказать, что планировщик порождает вот такие последовательные истории, или как они называются в этом контексте расписания.
И модель согласованности говорит пользователю о том, какие расписания может порождать планировщик.
Вот простой планировщик, который устроен внутри так, что он берет глобальный мьютекс, он порождает только серийные расписания.
То есть расписания, где все операции каждой транзакции просто идут группой.
Но пользователю это не нужно, потому что это такая избыточная гарантия, потому что пользователь не наблюдает вот этих самых расписаний.
Он работает только с планировщиком и смотрит и видит только те результаты, которые он получает из чтений, из планировщика.
Он не наблюдает самих расписаний, которые рождаются между планировщиком и хранилищем.
Поэтому мы сказали, что нам достаточно, нам не нужно строить в планировщике только серийные расписания.
Нам достаточно строить расписания, которые неотличимы от серийных.
И такие расписания мы назвали бью-сериализуемыми.
А моделизация мы назвали сериализуемость.
Ну или даже строгая сериализуемость, если мы дополнительно требуем, чтобы планировщик порождал,
во-первых, такие расписания, которые неотличимы от серийных, от последовательных,
и к тому же он бы уважал предшествование транзакций в реальном времени.
Если одна транзакция зафиксировалась до старта другой, то вот в этой сериализации транзакции шли бы в том же самом порядке.
Но мы на лекции про транзакции выяснили, что вот такие вот расписания, которые являются сериализуемыми,
которые неотличимы для наблюдателя от серийных, довольно сложно устроены,
что задача тестирования таких расписаний на сериализуемость, она, наверное, полная.
Поэтому мы не смогли бы построить такой планировщик, который порождает все такие расписания.
Поэтому мы ограничились некоторым под классом.
Мы сказали, что хорошо, пусть планировщик не может порождать все подобные расписания, не слишком сложно устроенные,
пусть он порождает только расписания, которые неотличимы от серийных в том смысле,
что можно получить из нашего расписания серийные путем свопов соседних неконфликтующих операций.
Скажем, двух чтений одного и того же ключа или двух записей произвольных ключей.
Но, скажем, переставлять местами запись и чтение одного и того же ключа в разных транзакциях нельзя,
потому что, очевидно, это повлияет на результат этих самых чтений.
А если мы переставляем две записи по одному и тому же ключу, то это повлияет на конечное состояние базы данных.
Для такого определения сериализуемости мы нашли критерий.
Мы сказали, что можно построить по расписанию граф конфликтов, в котором вершины транзакций в расписании
а дуги соединяют две транзакции, если в расписании есть две операции, одна из которых предшествует другое,
они конфликтуют. Из этих двух транзакций, которые конфликтуют, одна предшествует другой.
То есть граф конфликтов у нас задавал систему ограничений на потенциальную сериализацию в расписании.
И, пользуясь этим критерием, мы построили протокол, построили планировщик, который гарантированно порождает
конфликтно-сериализуемое расписание, то есть расписание не отличимое для пользователя от серийных,
ну и пользователей этого будет достаточно. Этот протокол назывался двухфазной блокировки.
И, коротко, 2PL. Как он был устроен? Мы связывали с каждым ключом в нашем хранилище блокировку.
Когда транзакция приходила в планировщик с первой своей операцией, то планировщик,
перед тем как обслужить запись либо чтение, сначала брал блокировку на соответствующий ключ.
Это мы рисовали вот такой вертикальной стрелочкой. И после этого уже, скажем, выполнял запись.
После этого пользователь, например, начинал чтение, присылал нам соответствующий ключ с операцией,
мы для него брали блокировку и выполняли чтение. Важно, чтобы блокировки после чтения и записи
мы не отпускали, а накапливали в транзакции и делали так до тех пор, пока транзакция не решала закоммититься.
После этого, в момент коммита транзакции, планировщик должен был где-то надежно зафиксировать,
что транзакция завершилась, что она хочет сохранить свои изменения.
Если мы говорим про базу данных локальной, то можно представить себе журнал на жестком диске,
где мы с каждой записью логируем либо откат транзакции, либо новый ключ, который мы хотим записать.
И в момент коммита мы пишем служебную запись в этот журнал, что все, транзакция надежно сохранена.
И даже после рестарта системы мы сможем этот журнал проиграть, повторить и восстановить состояние.
Но после того, как мы запись в журнал сделали, после того, как мы зафиксировали надежно, то логи можно отпускать.
То есть это протокол двухфазных блокировок. И вот почему он так назывался?
Эта фаза называлась фаза роста, а эта фаза сжатия.
Вот это все происходило в разрезе одной конкретной транзакции.
И планировщик так поступил с каждой отдельной транзакцией.
И если транзакции не пересекались, скажем, по множеству ключей, которым они обращаются,
то планировщик мог исполнять их параллельно.
Если транзакции пересекались по ключам, то одна транзакция, видимо, должна ждать другую.
Мы показали, что такой планировщик порождает только серилизуемые расписания,
показав, что в графе конфликтов для любого расписания, который порождает такой планировщик, не может быть циклов.
То есть мы воспользовались тем критерием, которое мы построили для вот такого определения серилизуемости.
А дальше мы сделали пару наблюдений.
Ну, во-первых, если мы берем блокировку на чтение, то мы можем брать блокировку разделяемую.
Но для записей нам нужно брать эксклюзивную.
То есть если две транзакции читают разные ключи, то они могут читать параллельно.
Но если одна транзакция читает ключи другая и пишет, то они исключают друг друга,
исключают параллельное исполнение друг друга, потому что одна из блокировок будет эксклюзивной.
Мы столкнулись с проблемой двухфазной блокировки, которая была связана с тем, что транзакции интерактивные.
Планировщик заранее не знает, каким ключам будет обращаться пользователь своей транзакции,
поэтому он не может гарантировать, что локи будут браться монотонно.
Это значит, что возникают дедлоки.
Поэтому нужен механизм, который позволяет из этих дедлоков как-то выбираться.
И мы рассмотрели для этого стратегию, которая называлась Soundweight.
Мы говорили, пусть каждая транзакция на старте выбирает себе временную метку.
Они не обязательно должны быть строгими монотонно.
Они должны всего лишь коррелировать с возрастом транзакции.
Чем раньше транзакция родилась, тем меньше у нее временная метка.
И если какая-то транзакция текущая хочет взять лок, а этим локом уже владеет другая транзакция,
то мы сравним их в временные метки.
И если оказывается, что наша транзакция старше, чем транзакция, которая владеет блокировкой,
то мы транзакцию, которая владеет блокировкой, кикаем.
Она откатывается, и мы блокировку подбираем.
Если же оказывается, что мы младшая транзакция, то есть наш timestamp больше,
то мы дожидаемся, пока транзакция владеет блокировкой и ее не отпустит.
Такой подход гарантирует, что дедлоков не будет, потому что дедлок, по определению,
этот цикл в графе конфликтов, в графе, где дугами соединены транзакции,
если они друг друга ожидают, если одна ожидает другой.
И в этой схеме видно, что если дуга есть, то это дуга монотонная.
То есть мы ждем, только если мы младшая транзакция.
Таким образом, циклов быть не может. Значит, не может быть дедлоков.
Но с другой стороны, ровно из-за этой стратегии, из-за этой стратегии,
которая помогает нам сбегать дедлоков, у нас появляются откаты транзакций,
которые происходят не по воле самой транзакции, потому что в ней не выполнились какие-то варианты,
а просто потому, что наш планировщик иначе зависает.
Может показаться, что это незначительный момент, но вот сегодня он будет для нас принципиально важен.
Это первый протокол, который мы рассмотрели.
Но у этого протокола был не то что быизъян, некоторое несовершенство.
Вот предположим, что среди наших транзакций есть какие-то небольшие транзакции,
которые берут локи, что-то делают и уходят.
А есть транзакции, которые длятся долго, и они долго что-то читают.
Вот можно себе представить разные примеры.
Можно представить себе, что вы хотите сделать бэкап вашей базы данных.
Она там внутри может быть даже отказоустойчивая, реплицированная,
а может быть это локальная база данных, и вы просто хотите зафиксировать текущее состояние,
записать куда-то на диск и безопасно сохранить где-то в холодном хранилище.
Для этого вы хотите прочитать всю базу данных, какое-то ее согласованное состояние,
моментальное состояние. При этом вы не хотите на это время брать блокировки на чтение,
потому что вы заблокируете все апдейты, и вам нужно работать.
Ну или может быть, если мы говорим про распределенные системы,
может быть у вас данных так много, и вы хотите их прочитать, ну скажем, в целые мы продюс-операции.
То есть вы хотите даже чтение распараллелись, потому что оно супер долгое и супер большое.
Вот в этом случае такой подход неприменим, он слишком пессимистичен.
И альтернативный вариант называется изоряция Snapshot.
Если мы хотим одновременно и читать, и записывать новые данные в системе в нашем хранилище,
то очевидно мы должны поддерживать просто разные версии данных.
То есть если в двухфазных блокировках, когда мы пишем что-то транзакцию,
то мы перезаписываем ключи, то сейчас мы хотим, чтобы каждая транзакция,
когда она что-то пишет, когда она комитится, порождала бы новую версию хранилища.
И вот эти версии, они были бы имутабельными, и каждая версия адресовалась бы некоторым числом.
Когда транзакция стартовала бы, она получала бы на старте временной метку,
которая бы адресовала версию хранилища, относительно которой транзакция будет выполнять все свои чтения.
Давайте я нарисую транзакцию.
Вот транзакция стартовала, и в момент старта, на шаге Start Transaction, получила себе read timestamp.
Этот read timestamp подрисует, ну давайте пронумеруем это, нулевая версия, это первая версия.
Не то чтобы мы требуем, чтобы версии нумировались подрятыдущими натуральными числами,
этого как раз мы не требуем, но для картинки подойдет.
Мы получили read timestamp, допустим один, и вот относительно этой версии мы собираемся выполнять все свои чтения.
Когда же мы делаем запись в транзакции, то мы эту запись просто буферизируем на клиенте.
То есть мы пока не сообщаем об этой записи в хранилище.
И когда мы решаем транзакцию в хранилище зафиксировать, когда мы хотим сделать commit timestamp,
то мы просто вливаем автомарно каким-то магическим образом все наши записи в хранилище, порождая в нем новую версию.
Вот это все напоминает система контроля версий.
Здесь мы очипили ветку, здесь мы сделали какие-то локальные комиты, пока не говоря об этом мастеру.
И вот здесь мы собираемся влить изменения в мастер, породить там новое звено.
Но, как вы понимаете, в системе контроля версии ничего даже не так гладко бывает.
Когда вы хотите сделать мерч, то он не всегда происходит, потому что случаются конфликты с другими разработчиками,
потому что есть конкуренция.
Но вот здесь у нас в транзакциях, конечно же, тоже есть конкуренция.
Представим, что у нас была другая транзакция.
Я не сказал, что когда мы комитим транзакцию, мы выбираем себе новую версию, она называется commit timestamp.
И вот под этой версией мы порождаем новое состояние хранилищ.
Как происходит конкуренция транзакции?
Вот, допустим, у нас была другая синяя транзакция.
Она на старте получила временную метку, скажем, 2.
Потом сделала какие-то свои записи.
И тоже решила влить их в мастер.
Но так получилось, что какие-то записи, например, в 2 и в 2 штрих, обращались, перезаписывали один и тот же ключ.
Тогда получается, что с момента старта этой транзакции, с момента отщепления ветки,
в мастере ключ, который эта транзакция перезаписала, тоже был перезаписан.
И когда мы пытаемся закомитить транзакцию, то мы этот конфликт должны обнаружить и транзакцию откатить.
Если какие-то две операции конфликтуют, две операции записи, то комит этой транзакции будет неуспешен.
Это мы называли правилом first-committer wins.
У нас здесь конфликтует только записи, поэтому это такой частный случай определения конфликта,
когда две записи обращаются к одному и тому же ключу просто-напросто.
Если между стартом и попыткой комит транзакции была другая транзакция,
которая успела закомитить свои изменения, то наша транзакция закомититься не может, должна откатываться.
И мы должны ее рестартовать.
Я забыл сказать, что если мы сохраняем timestamp при рестарте, то транзакциям гарантируется прогресс.
Рано или поздно транзакция станет самой старшей, ее уже некому будет вытеснить, и она завершится.
То есть мы просто ретравимся.
Такой подход не требует от чтения никаких блокировок.
Этот подход позволяет нам гораздо больше параллелизма иметь.
Но с другой стороны, у него есть своя цена.
А именно, мы лишаемся нашей гарантии, сериализуемости.
Почему?
Может показаться, что история изменений хранилища, история вот этих версий, это и есть сериализация.
Каждая версия это комит некоторые транзакции, они в каком-то порядке выстроены.
Но я напомню, что этот протокол позволял себе генерировать такие исполнения и такие истории,
которые не объяснялись никаким последовательным применением транзакций.
Мы рассматривали сценарий, который называется writes queue.
На языке транзакций это называлось аномалией.
Когда у нас были две транзакции, и одна из них, скажем, читала ключ Х.
Короче, напишу.
Другая симметрично читала ключ Y, и если видела в нем ноль, то писала в X.
И если мы применяли две такие транзакции, хранилища, где изначально X и Y были равны нулю,
то если вот эти две транзакции отщепились от начальной версии, то они обе прочитали эти два ключа,
увидели нули, и обе комитятся.
Комитятся в каком-то порядке, выбирая себе какие-то commit timestamp.
Они должны быть разные, это уже деталь реализации.
Важно, что на стадии комита вот это правило никаких конфликтов не обнаружит.
Просто потому что конфликтов на writesets нет.
Транзакции пишут разные ключи.
В итоге после комита двух этих транзакций в хранилище у нас остаются значения для X единицы, для Y единицы.
И понятно, что такой результат не соответствует никакому порядку сериализации, ни одному, ни другому.
Неприятно, но зато параллельно.
Вот два таких подхода.
Да, еще маленькое замечание.
Двухфазные блокировки давали нам, конечно, строгую сериализуемость.
Словом можно добавить сюда.
Ну вот, два подхода к изоляции транзакций.
Это я коротко повторил предшествующие занятия, предшествующие логические занятия.
Вот здесь нет ни слова про распределенность, про темы нашего курса.
Мы здесь не говорим ничего про отказы, про шардирование, про какие-то рестарты.
Нас здесь все это не волнует, потому что все это посвящено модели изоляции.
Или иначе говоря, конкуренции транзакций.
Мы совсем не беспокоились о том, как устроено хранилище.
Что там могут быть какие-то шарды, что у нас узлы могут перезагружаться,
перезагружаться могут узлы системы, перезагружаться могут сами клиенты или вообще отказывать.
Во всем этом мы не заботились, потому что мы говорили, что в конце концов оба этих протокола можно переносить,
можно использовать как локально на одной машине, можно даже использовать внутри процессора.
Я, кажется, рассказывал про то, как с помощью такого протокола можно с помощью такого протокола на уровне,
реализовав блокировки и конфликты на уровне протокола к гирянности кашей,
добиться транзакционности вращениях ячейком памяти.
Так вот, все это работало в самом разном масштабе.
Но вот сегодня мы хотим поговорить именно про распределенность,
про то, как эти протоколы можно реализовать в распределенной системе.
И про то, что вот на этом уровне уже становится важно, что наше хранилище не монолитное,
что в нем есть какие-то невидимые границы шардов, что клиенты могут отказывать, что узлы могут пересгружаться.
И вот все это мы рассмотрим на двух больших иллюстрациях.
Мы сегодня поговорим про две системы, про Google Spanner, но это самая большая база в данных мире,
вполне уместно про нее поговорить, и это Яндекс.дб.
Почему две системы? Почему именно такие, почему две?
Потому что две эти системы используют очень разный подход к реализации распределенных транзакций.
Google Spanner использует с одной стороны классический подход, двухфазный коммит, про который еще пойдет чуть позже,
но в то же время добавляет туда нечто особенное, а именно TrueTime.
А Яндекс.дб, который... Google Spanner статья была опубликована в 2012 или 2013 году,
статья, по которой начали писать Яндекс.дб была опубликована в 2013 году,
так вот эта система использует совершенно другой подход к транзакциям и пытается исправить
некоторую фундаментальную проблему, которая заложена в двухфазном коммите и в дизайне спаннера.
И почему... Этим, конечно, все не исчерпывается, двухфазным коммитом и детерминированными транзакциями,
но все же я бы сказал, что в промышленных системах в реальном мире это два главных подхода.
Ну и я обращаю внимание, что наша цель сегодня все же не про конкретные системы говорить, это всего лишь иллюстрации,
а увидеть, как на примере этих систем, как в дизайне этих систем возникают те или иные задачи,
и как они там решаются, с помощью каких алгоритмов. Ну а побочный эффект, обсуждая дизайн спаннера,
мы, конечно же, соберем вместе все, что мы к этому моменту накопили с вами в курсе.
Ну что, давайте поговорим про спаннер. И давайте для этого телепортируемся в проектор.
Итак, по списку авторов видно, что система Google Spanner довольно сложная,
поэтому нам потребуется много времени.
Итак, что из себя представляет спаннер? Спаннер представляет из себя базу данных,
то есть он предоставляет пользователю табличную модель, там есть строчки, колонки, ячейки, типы, схемы,
распределенные SQL запросы, то есть все, что вы ожидаете от базы данных.
Но для наших целей можно думать о спаннере как о распределенном кивелю хранилище.
И вот давайте посмотрим, как организованы те уровни архитектуры, про которые мы уже много раз говорили,
шардирование, репликация и хранение.
Вот на этой картинке изображен отдельный шарт кивелю хранилища, над которым строятся таблицы.
Каждый шарт образован разными, несколькими репликами, тремя репликами здесь.
Каждая реплика для повышения доступности находится в отдельном датацентре.
Реплики между собой реплицируют данные с помощью протокола multipax.
Состояние каждой реплики это данные таблицы, то есть некоторого фрагмента какой-то большой таблицы.
И что примечательно, что каждая реплика в этом multipax хранит данные не на локальном каком-то жестком диске,
который может отказать, и тогда реплика, тогда мы потеряем целиком реплику.
Нет, каждая реплика для хранения данных, своей копии данных,
использует распределенную файловую систему, колосус, который находится, напомню, в каждом датацентре,
которая своя в каждом датацентре. У нас есть 3DC, в каждом свой колосус, и реплика хранит данные в этом колосусе.
Если вдруг какая-то реплика, какой-то узел, который реализуется в logarитме multipax вдруг откажет,
то это не значит, что система потеряет реплику навсегда, и что нужно будет это сложным образом чинить все это.
Мы можем перенести реплику на другую машину, и при этом она сохранит свое персистентное состояние,
потому что оно было в колосусе, оно было там реплицировано, и там могут использоваться как-то не тройная репликация,
а может там используются erasure-коды. Помните, я про это как-то рассказывал.
И что нам сейчас особенно важно, в этом multipax есть стабильный reader.
Мы его каким-то образом выбираем, и он живет, мы рассчитываем, что долго.
Нам сейчас не нужно сильно погружаться в устройство колосуса, в устройство multipax,
окажется, мы по отдельности все это обсудили, и сейчас мы можем пользоваться этим как просто отдельными кубиками.
Ну вот, так организован отдельный шард. И в нашей системе много таких шардов,
и транзакции потенциально задевают тоже много таких шардов, ну, какое-то подможество из всех шардов,
которые обслуживают данную таблицу.
Если слой хранения репликации шардирования понятен, то можно приступить к разговору о транзакциях.
Почему мы вообще должны обо всем этом думать, когда мы говорим про реализацию транзакций с панели?
Итак, Spanner в первом приближении реализует протокол транзакции двухфазной блокировки.
Вот видите, тут подсвечено.
И для того, чтобы избегать дедлоков, Spanner использует стратегию ваунвейт.
То есть пока ничего нового. Пока мы все это знаем, и непонятно, зачем мы продолжаем эту лекцию, хотя вроде по частям все известно.
Давайте объясню общую трудность.
Ну, потому что есть отказы. Потому что клиент может отказывать, и потому что могут перезагружаться реплики какого-то шарда.
Ну вот мы эти проблемы сначала изучим в контексте транзакций, которые касаются только одного шарда.
Вот пусть наша транзакция работает с несколькими ключами, но все эти ключи находятся в пределах одного таблета и обслуживаются одним набором реплик.
Вот я утверждаю, что это сильно упрощает задачу.
Вот ограничимся такой картинкой.
Как же вот поверх вот этой всей конструкции реализуется протокол двухфазных блокировок?
Ну, я уже говорил, что в двухфазных блокировках нужно связывать с каждым ключом в хранилище блокировку, собственно.
Не то, что в этом UTX, это просто некоторая служебная запись, что вот для ключа вся та блокировка.
И все эти записи находятся в структуре под названием Logtable.
И этот Logtable обслуживается компонентом, который называется Transaction Manager.
Вот Transaction Manager это примерно планировщик, который был изображен вот на доске, где-то здесь.
Вот этот Transaction Manager получает записи, чтения от клиентов, берет блокировки, записывает их в эту таблицу и дальше работает с данными таблета.
Что любопытно, что Transaction Manager живет на лидере и блокировки хранит тоже только на нем.
Вот таблица с блокировками – это не персистентное состояние.
Если RSM, то есть вот этот набор реплик, этот мультипаксус перезагрузится в смысле, там, не знаю, перевыберется лидер,
потому что этот лидер отказал и пришлось выбрать нового.
Вообще говоря, отказ узла, но я буду говорить, что это перезагрузка RSM, потому что RSM не отказывает совсем, скорее всего.
Но он может передвигать роль лидера, и при перевыборе лидера сгорает вот это состояние.
Так вот, этот RSM может перезагрузиться, и тогда таблица блокировок будет потеряна.
А еще может клиент отказать. И то, и другое является проблемой.
Давайте подумаем, как мы с этим живем.
Во-первых, что делать, если отказал клиент? Он же, в конце концов, блокировки берет.
И, как мы уже знаем, если клиент умирает и блокировка остается навсегда за ним, то кажется, что система остается недоступной больше.
Как мы решаем эту проблему? Мы это обсуждали уже?
Мы вводим понятие «сессии» для клиента.
Клиент, когда он начинает свою транзакцию в операции StartTransaction, он приходит в TransactionManager и говорит «вот я стартую транзакцию».
И TransactionManager заводит для него некоторую запись, что вот есть клиент с транзакцией с ID таким-то.
И с этим клиентом ассоциируется некоторый таймер.
И клиент, видимо, должен посылать какие-то хардбиты, чтобы эту сессию поддерживать в живом состоянии.
Если клиент набирает блокировки, не успевает сделать коммит и отказывает, что вполне возможно, потому что от клиента мы отказаустойчивости не ожидаем,
то на TransactionManager в какой-то момент протухает этот таймер, и он просто отзывает все блокировки и откатывает транзакцию.
Маленькая деталь. Когда клиент что-то читает, он блокировку берет сразу.
Когда клиент что-то пишет, он буферизирует запись у себя.
И когда он говорит коммит, то отправляет все записи TransactionManager. TransactionManager забирает оставшиеся блокировки на запись.
Ну и если никаких конфликтов не возникло, если никто не отменился, то фиксирует запись, то есть коммит транзакции через multipax надежно.
Сам shard, блокировки снимает, и вот транзакция завершается.
Сейчас, подожди, мы здесь реализуем 2PL.
Аномалия RightSqueue, которую я писал, это аномалия, которая возникает при конкуренции транзакций.
В 2PL их не возникает. Мы это доказали, что все расписания будут неотличимы от реализуемых для пользователя.
И сейчас мы говорим просто про реализацию 2PL. Мы говорим, что возьмем блокировку. Я сейчас объясняю, что такое взять блокировку.
Мы переходим к TransactionManager, и TransactionManager делает запись в Logtable.
Ну, либо он видит, что... Ну ладно, пусть пока так. Твой вопрос теперь.
Вы сейчас сказали, что клиент сначала буферизирует все записи, а потом отслабит их с коммитом, правильно?
Да, но это же не важно. То есть мы просто откладываем запись на последнюю часть транзакции.
Протокол-то от этого не меняется.
Да блокировки можно тоже позже брать.
Но мы просто в транзакции переупорядочиваем часть операции. Это же не влияет на происходящее никак.
Нет, непонятно.
Вот мы, если x равен 0, то y записать y.
Ну вот тут происходит сначала чтение, потом запись. А может быть мы прочли, потом записали.
Потом прочли другой ключ, потом записали третий ключ.
Ну вот мы же можем запись в конец транзакции просто отложить. То есть просто перегруппировать нашу программу.
От этого ее семантика не поменяется.
То есть мы запись откладываем, но здесь есть некоторые технические нюансы.
Наверное, он сейчас нам не очень важен. Может быть, я только тебе запутаю этим.
Но по смыслу это все равно, это тот же самый 2PL.
Ладно, можно же об этом забыть и считать, что вот буквально 2PL.
Просто пока транзакция не закомитилась, транзакшн-менеджер нигде не фиксирует персистентно ее записи.
То есть мы через мультипаксы, через аппликацию ничего пока не пишем.
Мы просто запоминаем в лидере, что вот мы взяли какие-то блокировки.
Вот давайте теперь подумаем, что происходит, когда клиент умирает.
Сессия его протухает, транзакшн-менеджер отзывает блокировки, и другие транзакции могут продолжить работу.
Никаких следов транзакция пока не оставила.
Но с другой стороны, когда у нас есть один узел клиент, другой узел лидер, и между ними есть какие-то хардбиты,
мы всегда можем ожидать, что кто-то зависнет, кто-то будет работать медленно, какие-то хардбиты вовремя не долетят.
И сессия на транзакшн-менеджере протухнет, даже если клиент все еще жив.
Но это неприятно, но это не страшно. То есть клиент интерактивный, он выполняет чтение, выполняет чтение, выполняет чтение,
и вдруг на третьем чтении транзакшн-менеджер может говорить, что твоя сессия вообще-то протухла,
поэтому мы считаем тебя мертвым, и все твои локи отозваны.
Поэтому клиент должен перезапустить и попробовать снова.
Вот в случае, когда мы говорим про распределенность, у нас возникает еще одна причина,
по которой транзакция может откатиться и притравиться.
Это тайм-аут сессии. Ну потому что иначе с блокировками работать нельзя, их нужно ограничивать во времени.
А что делать, если перезагрузился РСМ, то есть если был перевыбран лидер?
Ну опять не страшно, все эти записи сгорят, сгорят все сессии,
поэтому просто отменятся все транзакции, которые еще не успели сгромиться и которые работали с этим шардом.
Что скажете?
Ну как именно организован этот слой тут?
Даже не очень важно, нет, не обязательно большой объем данных,
но представь, как выглядит райт-охэт лог в базе,
ну даже в локальной базе данных.
Вот когда ты в двухфазном блокировках делаешь какую-то запись,
ты пишешь в райт-охэт лог запись, что вот раньше по ключу х сохранилось значение 7
и перезаписываешь значение 8, и вот так они копятся, копятся, копятся,
а потом ты в момент комитта транзакции добавляешь служебную запись,
что вот транзакция с таким ID закомичена,
и когда ты вдруг перезагружаешься, должен накатить этот лог,
и если, ну это не то чтобы одна запись про комит транзакции,
это много маленьких и сообщения комит.
Ну честно говоря, я точно не знаю, как сделаны здесь,
но это можно по-разному себе представить, но вполне можно представить каким-то таким образом.
Вот, кажется, что мы сделали два PL поверх одного шарда,
и мы переживаем как перезагрузку РСМ, перевыбор лидера, так и отказ клиента.
Да?
Тогда вот кусочек транзакции готов.
А теперь посмотрим на случай, когда транзакция трогает ключи,
принадлежащие обслуживаемые разными шардами.
Вот утвердается, что сложность распределенных транзакций как раз в таком сценарии,
который называется кроссшардовой транзакции.
Вот здесь действительно просто два PL плюс сессии, плюс сгорание всех этих сессий при рестарте РСМ.
Вот в кроссшардовых транзакциях появляется новая проблема,
которая и составляет всю сложность распределенных транзакций.
Вот ровно ради этого мы сегодня и собрались.
Давайте подумаем, в чем разница.
Давайте еще раз повторю, что происходит.
Мы, клиент, когда мы стартуем транзакцию, мы приходим в Transaction Manager,
регистрируемся в нем со своим идентификатором, идентификатором своей транзакции.
Открывается сессия, и мы в рамках этой сессии читаем, пишем, берем блокировки здесь,
и в момент комитта транзакции мы все свои изменения сбрасываем в РСМ, в таблет, надежно их фиксируем.
Что может пойти не так?
Может протухнуть сессия у клиента по непонятным причинам.
Просто у клиента началась сборка мусора, потому что он на джаве написан.
Потому что перевыбран лидер, или просто на уровне 2PL произошел конфликт.
Мы сказали, что для того, чтобы 2PL блокировок дедлоков избегал,
нам нужно периодически откатывать транзакции даже против их воли.
Ну вот три причины, которые могут привести к откату транзакций,
но вроде бы ни одна из этих причин не смертельна.
А теперь мы работаем не с одним шардом, а с несколькими шардами.
Да, вот беда именно на стадии комита.
Мы что-то читали, читали, читали, на разных шардах брали блокировки на чтение,
копили записи, мы клиенты, и решаем транзакцию закомитить.
Вот теперь в комите транзакции участвуют разные шарты,
и мы должны каждому отправить сообщение комита своими записями.
И каждый шард, независимо от других, получается, принимает решение,
его этот комит устраивает или нет.
Может быть, ему прилетели записи, может быть, шарду прилетели записи,
которые по блокировкам конфликтуют с другими.
Может быть, сессия на этом шарде протухла.
Может быть, шард за это время перезагрузился.
И вот все это могло происходить независимо на разных шардах.
В итоге у нас могут быть, у нас могут расходиться мнения относительно того,
нужно транзакцию комитить или не нужно.
Один шард голосует за то, что он готов, точнее, один шард ее комитит успешно
и фиксирует изменения вот здесь, на уровне RSM,
а другой шард отказывается, потому что протух тайм-аут.
И в итоге мы теряем атомарность транзакций.
Она применилась частично.
Вот эта задача, которая здесь естественным образом возникла
в случае кроссшардовых транзакций, называется задачей атомарного комита.
И формулируется она в общем случае следующим образом.
Получилось.
Пусть у нас есть какие-то ресурс-менеджеры, чтобы это не значило.
И есть некоторые координаторы транзакции.
Вот координатор транзакции хочет, чтобы его изменение,
его транзакция применилась на всех ресурс-менеджерах.
И каждый ресурс-менеджер независимо от других принимает локальное решение
о том, устраивает его транзакция или нет.
Готов он ее за комиссию или нет.
Задача стоит в том, чтобы либо все ресурс-менеджеры применили транзакцию,
если все не согласны, либо, если хотя бы один не согласен,
чтобы транзакция откатилась на всех ресурс-менеджерах.
Это просто такая каноническая постановка задач еще из прошлого тысячеретия.
В нашем случае ресурс-менеджеры это отдельные шарды спаннера.
Как же добиться атомарного коммита?
В случае одношардовых транзакций у нас был коммит то, что называется однофазный.
Вы просто отправили коммит, и он был либо успешен,
либо шард нам отвечал, что он не успешен,
тогда транзакция откатывалась целиком.
Этого было достаточно для атомарности.
Сейчас мы так делать не можем в одну фазу,
просто отправив команду коммит.
Что же нужно сделать?
Видимо, нужно сделать две фазы.
Мы говорим сейчас про двухфазный коммит.
Это такой канонический способ сделать распределенные транзакции.
У нас есть узел-координатор,
и есть ресурс-менеджеры.
Каждый из них принимает независимое решение о коммите или откате транзакции.
Мы не знаем, что он выберет,
поэтому мы сначала отправляем его в коммит.
Это первая фаза.
Мы просим ресурс-менеджера подготовить транзакцию к коммиту.
То есть либо откати ее,
либо пообещай нам, что ты ее гарантированно применишь.
И мы можем сделать это.
И мы можем сделать это.
И мы можем сделать это.
И мы можем сделать это.
И либо пообещай нам, что ты ее гарантированно применишь.
Вот если нам ресурс-менеджер пообещал,
что он транзакцию применит,
он больше не вправе от этого решения отказаться.
То есть если мы говорим про шарт-спандере,
мы отправляем ему препэр и просим.
Шарт, если тебе транзакция устраивает,
то ты ни в коем случае не должен про нее больше забыть.
И вот мы собираем ответы от всех ресурс-менеджеров,
которые затрагивают транзакции, от всех шардов.
И если каждый шарт согласен за комитет транзакции,
он ее подготовил,
то мы принимаем решение комитета транзакции
и посылаем его всем ресурс-менеджерам.
Если хотя бы один шарт отказал,
то мы посылаем команду Abort.
Вот такая общая схема.
И обычно начинают рассуждать про отказоустойчивость.
У нас ресурс-менеджеры это шарды спандера.
Это отказоустойчивые сущности.
Они могут перезагружаться логически,
перевыбирая мастера.
Но если они пообещали клиенту,
что они транзакцию зафиксируют,
то они этого уже забыть не могут.
Поэтому когда я говорю препэр шарду спандера,
то давайте нарисуем уже
двухфазный комит в спандере.
В двухфазном комите координатором является
клиент.
Есть data-shart1, data-shart2.
Маловато. Давайте больше.
Мы отправляем препэр.
Каждый шарт получает препэр для транзакции.
Если сессия не протухла,
если конфликтов с локами никаких нет,
если лидер все еще тот же и у него есть
собственно запись о сессии,
то шарт, получая препэр, надежно сохраняет
все взятые блокировки
в персидентное хранилище.
То есть блокировки для транзакций,
которые подготовлены шардом, находятся уже
не только в памяти лидера,
в оперативной памяти.
Они еще сброшены в пакса,
где они уже ногами находятся.
Мы получаем ответы от каждого шарда
и посылаем комит.
Если у нас все устраивает.
Если хотя бы один шарт отказался,
потому что там протухла сессия,
то нужно уведомить остальные шарды,
что транзакция за комитство
нужно откатить, то есть отпустить
все взятые надежный персидент на блокировке.
Почему это не работает?
Каждый шарт,
все нужно говорить про то,
что случается при отказах и здесь, и здесь.
Если шарт еще не успел получать препэр
и перезагрузился или тайм-аут истек,
то ничего страшного.
Он ответит на препэр отказом
и клиент отменит транзакцию
на других шардах.
Если шарт прошел через препэр,
то перезагрузка уже не имеет значения,
потому что все данные надежно
сохранены в мультипаксос
и при смене ридера они восстановятся.
Блокировки мы не забудем уже.
Но вот беда, если клиент пропадет.
Потому что если какой-то шарт
пообещал клиенту,
что он подготовил транзакцию,
что он надежно сохранил все локи этого клиента
и не потеряет их уже и не забудет
никакому тайм-ауту.
А клиент после этого отказал
и не отправил шарду сообщения
ни коммит, ни аборт,
то локи остались висеть навечно.
И тут уже никаким тайм-аутом
проблема не решается,
потому что вот до того, как ответить на препэр,
шарт имел право принять локальные решения
о коммите транзакции.
Он мог ее отменить один.
После препэра он отменить ее не может,
потому что не понимает, на что решились другие шарды.
Как же это починить?
Вот мы хотим избавиться в этой схеме
от, это вообще фундаментальная проблема
двухвазного коммита,
а именно блокировка при отказе координатора.
У лэмпорта есть целая статья
про то, как сделать это все,
как это все скреить с паксосом.
Мы про это не будем говорить,
но мы можем сделать координатор
отказоустойчивым очень легко,
просто перенеся его внутри системы,
потому что внутри системы у нас уже есть отказоустойчивые существа,
это сами шарды.
Поэтому что мы сделаем?
Клиент, когда он
переходит к фазе коммита
своей кросс-шардовой транзакции,
выбирает среди
шардов, которые
он трогает
главный координатор.
И посылает
каждому шарду
по-прежнему команду Prepr.
Но плюс к этому сообщает, что
координатором транзакции
является, скажем, S1.
И каждый шард,
подготовив транзакцию,
отвечает о том,
что он подготовил ее
не к клиенту уже,
а вот этому координатору.
Вот здесь вот происходят
записи
в мультипаксос.
В мультипаксос.
Это позже нам понадобится.
Координатор,
шард номер один,
собрав все Prepr
успешные, ну либо получив
хотя бы один неуспешный, принимает решение
о комите и работе
об откате транзакции.
Но важно, у нас же шард очень
отказоустойчивый, но он умеет перезагружаться.
И нужно, чтобы он после
перезагрузки ни в коем случае не забыл принятое решение.
Поэтому он сначала принимает решение,
пишет его
еще раз в свой мультипаксос
и уже после этого
отправляет
другим шардам, которые участвовали в транзакции
результат,
финальную команду
commit или abort.
Это уже
почти окончательный вариант.
Нужно лишь подумать
о кое-каких деталях.
Вот скажем,
кто здесь у нас может отказывать?
Целиком отказывать, разве что клиент может.
И вот будет ли страшно,
если клиент успел
отправить Prepr
скажем, на шард
на какие-то шарды,
а на какие-то не успел?
Где все это
восстановится?
Кто отменит транзакцию?
Если хотя бы
ни один шард не получил
Prepr, значит транзакция
не закомитилась вообще, все протухнет
и отменится самой собой.
Если хотя бы
один шард получил Prepr
и принял решение,
что транзакция должна быть
на какие-то шарды,
а на какие-то не успел,
и принял решение, что транзакция
должна быть подготовлена и зафиксировала
ее надежно в своем мультипаксисе,
то он
об этом напишет S1
координатору,
координатор это запомнит,
и он будет знать,
что в транзакции участвует
еще, скажем, два шарда.
И если он от них не дождется решения
за некоторое время,
то он в праве уже принять сам
локально любое решение.
Он может выбрать комит, только если он получил
все положительные ответы,
а если он получил хотя бы один
отрицательный ответ или просто таймаут,
то он может консервативно транзакцию все отменить.
Это всегда безопасно.
Так что
смерть клиента
вот где-то на этом участке
она ни на что не влияет,
и против рестартов
РСМ мы боремся тем, что мы пишем
надежно в хранилище,
пишем мультипаксис.
Вот тут происходит
в комите транзакции
три последовательные записи.
Сначала мы надежно сохраняем блокировки,
потом мы надежно параллельно
на разных шардах, потом мы на координаторе
надежно сохраняем решение комит транзакции
и потом мы параллельно
пишем данные
в само хранилище, которое мы реплицируем.
Ну конечно,
если координатор,
там можно ответить, всегда клиент.
Ну если клиент
прям отказал
и если его
так настолько, ему настолько
важен результат транзакции,
наверное это его собственная забота.
То есть можно
если он не готов
восстановиться от, то есть ему дают
гарантию, что транзакция либо
целиком накатывается, либо целиком
откатывается, и что она
серилизуется относительно других транзакций,
то это то, что может сделать система.
А если клиенту важно понимать,
все-таки успел он сделать что-то или нет,
когда он умер, то это его забота.
Мне кажется, он на уровне транзакции может это сделать.
Я не знаю, скажем,
это не то чтобы про транзакции,
речь про базы данных,
но вот в ZooKeeper у тебя есть
апдейт, где ты фактически
выполняешь операцию CAS. Если у тебя
текущая версия какого-то узла
равна чему-то, то я перезаписываю ее на
другую версию.
Так каждый
препер,
нет, конечно, может, но просто
если shard2 получил препер,
он же получил еще и указание,
кто является координатором, поэтому он
сообщит координатору, поэтому координатор
о транзакции узнает.
Но если он сам не получил сообщение
от клиента, он не знает свою порцию
транзакции, то есть свои записи,
то просто
транзакцию вправе отклинить, потому что
он голосует против.
Голосовать против всегда
легально до тех пор, пока ты
получил препер. Вот как только ты его получил
и согласился, все, ты
закомитился, что транзакция,
что ты транзакцию по своей воле
уже не отменишь. Если ты не получил препер,
значит ты можешь говорить что угодно,
ну, говорить нет.
Ну что, двухфазный комит,
разобрались? Вот, пожалуйста,
я понимаю, что здесь
есть очень похожие
названия,
двухфазный комит, двухфазные блокировки,
но это же совершенно
проразное.
Вот двухфазные блокировки,
вот Spanner, он использует
двухфазные блокировки
для изоляции транзакций,
то есть мы здесь говорим про конкуренцию,
а двухфазный комит,
то есть это про много транзакций,
которые одновременно в системе работают,
а двухфазный комит,
он про отказоустойчивость и он
про одну отдельную транзакцию.
То есть это вот разные
проблемы,
тоже две фазы, у нас везде две фазы просто в курсе,
и это некоторая фундаментальная
особенность всего происходящего, но
вот здесь задачи разные, хоть
решения называются похожим образом,
задачи разные,
одно про отказоустойчивость, другое про
конкуренцию, про изоляцию.
Вот, пожалуйста,
не путайтесь.
Специфика
двухфазного комита в Spanner
пока довольно несущественная,
мы перенесли картинатор внутри системы,
сказали, что один из
шардов системы является,
в Spanner такой шард-участник
называется Participant.
Где бы это написать здесь?
А шард-координатор
называется Participant-Leader.
Вот если получится
быстро вернуться на
статью, на экран,
получится ведь?
Давайте я так покажу, потому что лень.
Participant-Leader это как раз
роль, что этот шард является координатором
транзакции, он отвечает за
глобальный, за атомарный комит.
Вот есть и другие, вот с ними
мы общаемся.
Итак, значит,
эта конструкция вроде бы
полностью разобрана,
и мы готовы перейти к
следующему шагу реализации
транзакции в Spanner,
а именно мы хотим оптимизировать
чтение. Мы хотим оптимизировать
транзакции, которые только читают.
Да, Spanner использует двухфазные
блокировки для того, чтобы
реализовать транзакции, которые
и читают, и пишут.
Но вот если транзакция только читает,
то Spanner хочет действовать эффективнее.
Мы видели, что для
редон или транзакций
блокировки в изоляции
снапшотов вообще не требуется.
Вот забудем пока про двухфазные блокировки,
вспомним про изоляцию снапшотов.
Здесь для транзакций, которые
читают, никакие блокировки,
никакие конфликты, все это не требовалось.
Они просто запускаются, получают себе временную метку
и читают свою версию
хранилища мультиверсионного.
С одной стороны, им хорошо,
а с другой стороны они
не читают, они читают что-то сомнительное,
потому что вот вся эта
история,
все эти версии хранилища,
это не сериализация транзакций.
То есть мы можем породить версии,
которые не объясняются
никакой сериализацией.
То есть, чтение без блокировок
из мультиверсионного хранилища,
это хорошо, а вот
First Committer Wins и
поиск конфликтов только
про айцетом, это путь к
аномалии.
Так вот, что решает Spanner?
Что можно совместить два подхода.
Можно взять двухфазные блокировки
для транзакций, которые
только читают,
которые и читают,
и пишут. Но если транзакция
только читает,
давайте я что-то уничтожу.
Для транзакций,
которые и читают, и пишут,
мы будем использовать 2PL.
А вот для транзакций,
которые только читают,
мы будем использовать изоляцию
снапшотов.
Что я имею под этим в виду?
Потому что пока непонятно. То есть нельзя просто взять
два подхода и сказать, что я хочу получить
сильные стороны каждого
и игнорировать слабости.
То есть мы можем
изоляцию снапшотов каждого
и игнорировать слабости.
Когда мы
серилизуем порядок
изоляции снапшотов, хорошо то, что
чтение происходит без блокировок относительно стабильной версии
некоторой, но плохо то, что
история изменений
не объясняется никакой
серилизацией.
С другой стороны, если мы используем 2PL,
то внутри 2PL серилизация
не объясняется.
Другое дело, что она неявная.
То есть когда мы запускаем
этот протокол, берем там какие-то блокировки,
то мы знаем, что для
исполнения в 2PL
существует некоторая
серилизация,
но какая именно, мы не говорим. То есть мы нигде явно
ее не строим. Мы просто говорим, что в графе
конфликтов для расписания, который порождается
в 2PL, не будет циклов, поэтому
по критерию в расписании
расписание будет конфликтно-серилизуемо, значит
отключимо для пользователя от серийного.
Но конкретно расписание
мы не знаем.
Так вот,
давайте мы научимся
материализовывать вот эту серилизацию,
которая возникает в 2PL.
Когда мы коммитим транзакцию,
давайте не перезаписывать ключи
в хранилище, а
давайте порождать новую версию
хранилища.
Вот мы хотим сделать хранилище
версионным.
Потом покажу это в статье Spanner.
В Spanner хранилище, которое реализуется
через шардирование, через
multipax, через колоссусы, это на самом деле
версионируемое хранилище. То есть мы по паре
ключ-таймстэмп
храним значение.
Ровно потому, что в коммите транзакции,
когда мы пишем данные
в хранилище, мы хотим не
перезаписывать, а генерировать временную
метку и записывать данные под ней.
Вот мы из 2PL возьмем сериализацию
и материализуем
ее в виде истории изменений
хранилища.
И таким образом,
когда транзакция будет только читать,
если транзакция собирается только читать,
то она будет просто читать с хранилища
напрямую.
А если транзакция будет и читать, и писать,
то пусть она использует протокол 2PL
и сериализуется относительно других
читающих, пишущих транзакций.
Идея такова.
Но
если бы так все было просто, все бы так
давно и сделали.
Давайте я объясню,
в чем проблема.
В том, что
для транзакций, которые собираются
только читать,
порядок
записей — это порядок
на временных метках.
Транзакции, которые
используют 2PL,
упорядочиваются с помощью блокировок.
Вот для разных транзакций
упорядочивание выглядит по-разному.
И, разумеется, оно должно быть согласовано.
Вот пусть у нас есть две транзакции,
которые конкурируют.
И пусть они работают
с пересекающимся набором ключей.
Они читают и пишут,
пересекаются по набору ключей,
поэтому
используют протокол 2PL.
И это означает, что они набирают локи.
И вот давайте я сейчас обозначу
красным отрезке, где
собраны все локи.
Вот здесь
взят последний лок,
и здесь
локи начинают
отпускаться.
Здесь взят последний лок,
и локи начали отпускаться.
Если транзакции конфликтуют
по ключам,
то они берут какие-то общие локи,
и вот этот отрезок
между
взятием
всех локов
и отпусканием всех локов,
вот эти отрезки
для двух этих транзакций не пересекаются.
Потому что есть
какой-то общий лок, который они берут.
То есть, если
две транзакции используют 2PL,
то вот они как-то упорядочились.
Вот где-то здесь происходит
коммит.
Где-то здесь мы надежно
фиксируем на диске,
в RSA, о том, что транзакция закоммичена.
В 2PL
в 2PL
в 2PL вот так вот рождается
стерилизация.
Мы просто нигде ее физически
не представляем никак.
Но теперь
коммит – это запись
версионируемое хранилище
по некоторой временной меткой.
Так вот, чего мы хотим?
Мы хотим, чтобы если вот этот
интервал в транзакции T1
предшествовал этому интервалу
в транзакции T2,
то из этого бы следовало,
что транзакция T1
запишет
свои записи
в хранилище под временной меткой,
которая будет обязательно меньше,
чем временная метка, которая выберет
для коммита транзакции T2.
Потому что читающие транзакции
хотят смотреть на временные метки
и не знают ничего про блокировки,
а 2PL работает с блокировками,
упорядочивается через них.
Вот такая вот задача – согласовать
два этих порядка.
Кажется, что мы это умеем делать, правда?
Ровно
ради этого момента
мы этому и учились когда-то.
И снова появляется экран.
Итак, это слайды с
самой первой презентации Spanner в 2012 году.
Как статья, эта презентация
целиком посвящена транзакциям.
И давайте обсудим, как решать такую задачу.
Кто-то поторопился, ну ладно.
Откатывает назад лениво.
Итак, задача – выбирать
монотонные временные метки.
У нас есть два интервала в двух транзакциях
между взятием всех логов
и отпусканием всех логов.
И если два интервала не пересекаются во времени,
если один предшествует другому,
то в первом интервале в момент коммита
должна быть выбрана временная метка
строго меньше, чем во втором интервале.
Как такую задачу можно решать?
Эту задачу решают обычно.
Обычно строят вспомогательный сервис,
который называется Timestamp Oracle.
Оракул временных меток.
Оракул времени.
Чем он занимается? К нему можно прийти
с запроса, и он скажет, вот тебе временная метка.
И он дает их монотонно.
Вопрос.
Как такого оракула построить?
Действительно, можно взять просто RSM.
Мы берем RSM,
который реплицирует просто атомик
с операцией FetchEd.
Мы приходим к нему, говорим FetchEd,
он увеличивает на единиц, возвращает нам значение.
Все монотонно, отказы устойчиво.
Ну, конечно же, если мы собираемся
выполнять, там не знаю,
десятки, сотен тысяч транзакций в секунду,
то мы будем использовать
фичет.
Ну, конечно же, если мы собираемся
выполнять, там не знаю,
десятки, сотен тысяч транзакций в секунду,
то мы не сможем прийти
сто тысяч раз или миллион раз
в этот мультипак со следа рафта сказать FetchEd.
Он лопнет от количества команд.
Поэтому
как мы поступим на самом деле?
Мы на месте этого timestamp
оракула будем открывать
каждые пять миллисекунд окно.
И копить все запросы, которые
к нам приходят.
Вот ждем пять миллисекунд,
набираем, скажем, десять тысяч
инкрементов.
И отправляем в мультипак
со СВСМ команду
FetchEd десять тысяч.
Получаем старое значение.
И раздаем
десять тысяч разных ответов
вот в интервале от полученного значения
до полученного значения плюс число десять тысяч
разным клиентам.
Понятная идея?
То есть
количество запросов
в этот timestamp oracle
в принципе
может пережить любое количество
инкрементов.
Просто потому что чем больше инкрементов,
количество операций над RSM,
количество команд в RSM не увеличивается.
Просто растет
значение FetchEd.
Хорошо.
То есть мы можем построить
себе такой компонент.
Но Google почему-то делает не так.
Но многие системы делают так.
Скажем, если вы пойдете в Яндекс работать,
там система в IT, там есть транзакции, динамические таблицы,
и у них есть компонент stamp oracle,
который генерирует временные метки для транзакций.
Вот Spanner так не делает.
Ясно ли почему?
Мы смотрели на картинку
и там были реплики,
которые находились в разных датацентрах.
И датацентров Google довольно много,
и они находятся вообще на разных континентах,
но очень далеко друг от друга.
Беда здесь в том, что
если мы построим один компонент
timestamp oracle,
это точка централизации.
Все транзакции должны посещать его.
А если мы говорим про multipax,
то timestamp oracle,
там несколько реплик,
они будут устойчивы сам по себе.
И реплики могут находиться далеко друг от друга
для большей доступности.
Но в конце концов,
среди них есть лидер,
и всех пойдет в него.
И даже если он переживает любой рейд запросов,
то все равно со всего глобуса
вы стекаетесь к какой-то конкретной машине.
То есть кому-то повезет, а для кого-то она будет далеко.
Поэтому, что делает Google?
Они говорят, что поскольку у нас система
геораспределенная, она масштабируется на весь глобус,
мы хотим
выбирать глобально
монотонные временные метки
по возможности, избегая координации,
чтобы транзакции где-нибудь
в Антарктиде и на Северном полюсе, они
друг с другом не коммуницировали
с общими узлами.
Для этого у Google
есть компания, которая называется TrueTime.
В чем
идея? Выбрать
временную метку для ком... Простите
этот пример,
почему не стоит делать темные слайды
и там оранжевые надписи.
Свет выключить, хорошая идея.
Но стало лучше
все же.
Задача сделать так, чтобы
если два отрезка не пересекаются
в двух транзакциях, то временные метки были бы монотонными.
Кажется, про это даже есть
отдельный слайд, можно его и
показать.
Ну ладно.
Идея в том,
что можно не думать про
монотонность,
можно не думать
про координацию и про две транзакции,
можно думать только про одну транзакцию,
если в качестве временных меток
вы используете не
какие-то монотонные числа,
а прямо физическое время.
То есть понятно, что если вы
выберете в качестве временной метки
для записи
now между точкой, где вы
взяли последний лок, где вы первый отпустили,
то монотонность будет соблюдена.
Но поскольку now
вы не можете использовать, потому что
часы могут быть рассинхронизированы,
вы используете что-то сложнее,
вы используете true time, который
возвращает вам вместо now интервал
вот earliest to latest
и говорит, что время, что точка
вопроса находится в этом интервале,
или интервал вопроса пересекается с интервалом этой
операции. И что
приятно, что вот этот сервис
true time now в этом
вызове никакой коммуникации с другими узлами
не выполняет.
Коммуникация там, конечно, есть, он раз в 30 секунд
общается, но про это тоже написано
в слайдах спаннера.
Это система, которая развернута внутри
датацентра. В каждом
датацентре есть GPS
антенны, есть узлы
таймастера с атомарными часами.
И раз в 30 секунд
каждый узел
в кластере общается с разными
таймастерами, получает от них очень
точное время,
а потом 30 секунд с ним
живет.
То есть он знает текущее время
с очень маленькой, с очень высокой точностью,
а потом 30 секунд живет
по своим собственным
часам, просто закладывая в них большой
дрейф.
Когда у него спрашивают нау, он смотрит,
насколько давно
он синхронизировался с таймастерами
и сдвигает интервал вправо.
Но поскольку он не знает,
сколько времени в точности прошло между текущим
запросом клиента, точкой синхронизации,
он закладывает в эту величину,
он закладывает еще дрейф
возможный, поэтому расширяет интервал.
Но даже в такой схеме,
если мы закладываем
дрейф в 200 микро секунд в секунду,
это в 10 раз хуже,
чем очень плохие кварцевые часы,
то даже в такой схеме
этого достаточно, чтобы получить
ширину окна примерно
6 миллисекунд. То есть этот сервис довольно
точный, но у них еще есть здесь
графики,
что вот, не знаю, 90
процентов
времени True Time
оценивает
вот этот Эпсел,
ширину интервала примерно в ноль вообще.
Но если мы говорим про 99
процентов времени, то на вот
10 миллисекунд.
Но это все равно, даже
если мы
берем вот
такой процент, то все равно
это намного лучше,
чем коммуникация
через очень большие расстояния.
То есть мы ее делаем
все равно, это коммуникацию, но мы делаем это в фоне
через GPS,
либо общаясь
только с локальными атомными часами
в своем ДЦ, либо общаясь со спутниками
через радиосигнал.
Мы с спутниками не общаемся, это они с нами
общаются.
И имея такой сервис,
который, вообще говоря, не очень-то просто
повторить, кажется, что никто
и не пытался больше. Мы можем
легко построить процедуру,
которая генерирует вот эти самые
временные метки.
Мы берем
true time now, правую границу,
потому что мы знаем, что она точно правее,
чем точка, где мы взяли локи.
А потом дожидаемся,
пока S окажется в прошлом,
чтобы попасть в интервал.
Правую границу мы управляем, точка, когда мы
отпускаем локи. Так вот, локи мы отпускаем
только тогда, когда мы от true time now
получили интервал, левая граница, которого
оказалось не меньше, чем
вот эта выбранная S.
Но это вы, кажется, знаете, если вы делали
первую домашнюю работу и не скипали
часть про true time.
Таким образом, когда транзакция
коммитится, она
выбирает себе timestamp, она с помощью
вот этой процедуры генерирует
на тонную временную метку, заменяя коммуникацию
на ожидания, и после этого
делает запись. Таким образом, мы
генерируем
в 2P или версии хранилища
явно,
и читающая транзакция теперь
может читать вообще без блокировок
и читать именно
из истории сервизованных транзакций.
Правда, тут есть некоторые технические нюансы,
совсем маленький,
иностранный клиент. Давайте я
его покажу.
Время поговорить
про
тонячий клиент для спандера
почему-то. Смотрите,
у нас есть два механизма исполнения
транзакции. Если транзакция
RedOnly, то она должна идти по одному
пути, то есть получать read timestamp и читать
без блоков из хранилища напрямую.
Если у нас транзакция
пишет,
то она должна использовать
двухфазные блокировки и вот
выполнить протокол, который описан был до этого.
Но транзакция у нас интерактивная, поэтому
у нее на лбу заранее не написано, что она собирается
делать, будет ли она писать или нет.
Так вот, на уровне клиентской
библиотеки транзакция фактически анонсируется
с теми, как она собирается работать.
Просто для одного и другого
типа транзакции есть отдельные API.
Если мы говорим про транзакции,
которые только читают, то
в клиентской библиотеке они выглядят так.
Вот здесь context manager,
мы берем снапшот хранилища,
то есть фактически
выбираем некоторую временную метку для чтения
и потом через этот снапшот читаем.
Вот мы здесь дальше не можем
ничего писать, у нас просто нет вызовов таких
в объекте снапшот.
Если же мы
собираемся писать транзакции,
то мы должны воспользоваться
другими API,
написать тело транзакции
и передать его функцию
runTransaction.
Мы передаем свое тело
транзакции, и нам этот runTransaction
вызывает
этот UnitOfWork,
передавая ему объект транзакции.
И дальше мы через него
делаем какие-то апдейты, удаления и так далее.
Почему API такое?
Потому что
эта транзакция рестартовать не может.
Просто нет повода
транзакцию откатывать.
Она просто читает из некоторого
мгновенного состояния.
Вот эта транзакция откатываться может,
и ретраи они завернуты
вот в эту вспомогательную функцию.
То есть это тело может ретраиться,
но и поэтому важно, чтобы здесь не было каких-то сайд-эффектов еще.
Так вот, используя либо одно, либо другое
API, вы сразу системе объявляете,
как будет работать ваша транзакция,
и она исполняется либо одним, либо другим
способом.
Окей,
почти готово.
Мы почти
добрались до конца.
Но кое-что еще осталось.
Давайте подумаем вот над чем.
А как выполнять
чтение вашей транзакции?
Как выполнять чтение в вашей системе?
Вот вы клиент,
вы хотите выполнить транзакцию,
которая будет только читать.
У вас для этого есть
вот
вот
потерялся вот такой API.
Здесь вы сгенировали некоторую
временную метку.
В чем ваш замысел?
Ваша временная метка, конечно же, должна быть
не меньше, чем временная метка
каждой закомиченной к этому времени
транзакции.
Как выбрать такую временную метку?
Ну точно так же можно выбрать.
То есть снова можно воспользоваться
TrueTime на машине спаннера.
И он возьмет
TrueTime Latest.
И вы будете по этой временной метке читать.
Она будет не меньше, чем любая транзакция,
которая уже закомитилась.
Но смотрите, в чем сложность.
Вы приходите с этой временной меткой
на какой-то шарт.
Да, я, кстати, не показал вам,
когда говорил про мультиверсионное
хранилище,
что вот спаннер действительно
его реализует.
То есть он хранит отображение
из ключей и таймстэмпов
в какие-то строчки.
Так вот, вы приходите на шарт, у вас временная метка,
и вы говорите, хочу читать из снэпшота.
Ну снэпшот — это же то,
что мы на самом деле в уме
придумали себе.
Никаких снэпшотов в системе нет.
В системе просто есть
мультиверсионное хранилище, и туда можно
написать пары ключ-таймстэмп значения.
Так вот, когда вы приходите в шарт
с временной меткой, шарт же не знает,
есть ли в нем все
записи с этим...
Верно ли, что он в себе уже
содержит все записи с таймстэмпом
клиента и со всеми
меньшими таймстэмпами?
Или туда еще могут прилететь какие-то другие транзакции,
которые еще просто не успели
доставить в него свои записи,
какие-то незакомичные транзакции?
Понятна проблема?
То есть пока снэпшот — это только то,
что мы на доске нарисовали, а физически
они как-то не представлены.
Так вот, если вы клиент и вы приходите в шарт,
то вы должны каким-то образом дождаться,
дождаться того момента,
когда вы будете уверены, что в шарде
не появятся новых записей с таймстэмпом
меньшим или равным вашему.
И для этого
в спаннере
снова есть некоторые ухищрения.
А именно каждый шарт,
но вот об этом написано в статье,
и добрая часть статьи этому посвящена.
Каждый шарт поддерживает
вот такую величину,
он называется save time.
Это оценка
снизу на возможные
таймстэмпы транзакций,
которые могут этому шарду
еще прийти.
Если вы приходите с таймстэмпом
больше, чем save time шарда,
то вы должны ждать,
пока этот save time не дорастет
до значения большего,
чем вашего.
И каким образом оно поддерживается?
Давайте я покажу картинку,
я в детали углубляться не буду,
потому что можно запутаться.
Но все это встраивается
в двухфазный комит.
Каждый шарт поддерживает свою
нижнюю оценку,
и всем своим записям
он их монотонно генерирует,
потому что каждый шарт
он в целом атомарен,
он может генировать монотонные временные метки,
даже с учетом рестартов и смены лидера.
И когда шарду приходит
команда Prepr от координатора,
он пишет ее, он сохраняет ее надежно,
сохраняет локи и генерирует
временную метку и отправляет ее координатору.
А координатор
в итоге выбирает
временную метку комита
как максимум из true time latest,
true time now latest
и нижние границы,
которые прислали шарды.
Это такой механизм
ставить границу снизу
на возможные записи в данный шарт,
и таким образом
мы можем
сейфтайм на шарде поддерживать
и помогать клиенту дожидаться,
и помогать клиенту
читать только действительно снимки состояния.
Если он читает, то он уверен, что
никаких записей с меньшим таймстемпом в этот шарт уже не придет.
И снова никакой коммуникации не нужно,
мы просто ждем.
Итак, мне кажется, что
я помодлю некоторых
оптимизаций, которые я
наверное уже не в силах
рассказать.
Спаннер разобрал.
Итак, напомню всю конструкцию.
Спаннер это
шардированное мультиверсионное кивалио-хранилище.
Каждый шарт представляет собой
набор реплик мультипаксиса,
которые хранят свои данные
в колоссе и на жестком диске.
И над
этими шардами исполняются
распределенные транзакции.
В первом приближении транзакции используют двухфазные блокировки.
И если
транзакция касается только одного шарда,
то каждый шарт хранит
эти блокировки в
оперативной памяти лидера.
И если
транзакция касается только одного шарда,
то в момент коммита
клиент отправляет этому шарду
все свои записи, которые он накопил.
Шарт проверяет.
Трансакшн менеджер на лидере шарда проверяет,
что сессия не стекла,
что локтейбл
не конфликтует,
что сессия
вообще ему
известно,
что локтейбл не конфликтует, в смысле ваундвейта.
И тогда транзакцию фиксирует.
Если же мы говорим про транзакции,
которые касаются нескольких шардов,
то для того, чтобы автоматно
на всех транзакцию либо за коммит, либо откатить,
мы используем протокол двухфазного коммита,
но чтобы побороться с отказами клиента,
мы переносим координацию
транзакции на
узел, точнее на компонент системы
отказа и устойчивость на один из шардов.
А дальше мы говорим, что здорово
было бы редон для транзакций пооптимизировать,
и поэтому, когда мы пишем
в 2PL
записи в хранилище,
то мы порождаем, мы не перезаписываем значение
в хранилище, мы порождаем новую версию.
Но для того, чтобы
это делать согласовано, чтобы
порядок на версиях был согласован с порядком
в 2PL, мы используем TrueTime
для распределенной генерации
временных меток.
Ну и вот в итоге
получается
вот такая общая схема.
Итого, кажется,
мы использовали здесь мультиверсионность, мы использовали
Multipax, мы использовали
Colossus, мы использовали транзакции,
мы использовали TrueTime.
Вот все вместе это собирается.
Что здесь принципиально нового?
На самом деле не так уж много.
Вот Spanner — это пример системы,
это пример хорошего дизайна,
в смысле
взять много всего
известного и собрать во что-то большое
и работающее.
Вот из таких вот инноваций прямо в Spanner
это именно TrueTime, то есть
способ генерации распределенных временных меток.
2PL — это что-то
из далекого-далекого прошлого,
когда компьютеров еще не было.
Двухфазный Комит тоже — это какие-то
70-80-е годы, наверное.
Изоляция снайпшотов — это чуть попозже придумано,
но все равно.
То есть мы просто собрали
очень много идей и построили из них
большую-большую систему. Ну и вот это
повторить довольно сложно.
Все-таки есть тут специфика самого Google,
потому что не каждому,
не каждой системе, не каждой компании
просто нужно решать вот такие
задачи, как глобальные,
глобальные монотонные временные метки.
Если вы живете в одном дата-центре, то, может быть, вам
это все не нужно, потому что вам достаточно
вот этого самого Timestep Oracle,
который использует RAFT.
Вот Google недостаточно.
Хорошо, тогда
со Spanner
и что главное мы должны вынести,
наверное, из этого, что сложность,
которая у нас возникла здесь по пути,
сложность в распределенных транзакциях,
она возникает именно из-за распределенности
и из-за многошардовости. И вот тут
рождается двухфазный коммит. Такая абсолютно
фундаментальная идея, которая много где
используется, но и вообще
не знаю,
можно было бы даже сказать, что
сам алгоритм ПАКСС похож
на некоторый отказоустойчивый вариант
двухфазного коммита. Короче,
многие можно параллели проводить.
Так вот, что мы сделаем дальше?
После перерыва мы поговорим
про другую систему,
про другой подход к транзакциям,
совершенно другой, и про
систему Яндекс.ДБ, которая его использует.
И в этом подходе
мы все еще хотим
делать распределенные транзакции, наша цель,
собственно, но мы хотим
побороться с двухфазным коммитом.
Нам нравится однофазный коммит,
который был в одношардовых транзакциях,
когда мы просто говорим «коммит».
Здесь появляется лишняя фаза, и мы разберемся,
каким причинам эта фаза появляется,
насколько они фундаментальные,
и как можно, если мы
исправим эти причины, что-то пооптимизировать.
Это совершенно альтернативный
дизайн, который был придуман, ну, примерно
в то же время,
и на свете пока очень мало мест,
где его смогли применить. Вот Яндекс —
это как раз то место, где такой дизайн
написали.
Ну и, что забавно,
Яндекс.ДБ и Google Spanner — они
как открытые системы,
недоступен исходный код,
но при этом и ту, и другую
систему можно использовать в облаках.
Яндекс.ДБ — это система хранения для Яндекс.Облака,
Google Spanner — это система хранения
для Google Cloud.
Так что тут много параллелей между ними.
Ну, давайте сделаем прерыв, после этого
мы продолжим.

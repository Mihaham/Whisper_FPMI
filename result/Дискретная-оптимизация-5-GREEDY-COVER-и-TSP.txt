так народ значит сначала я отвечу все запись есть да отвечу на два вопроса
первый вопрос какой дедлайн у нас дедлайн по всем заданиям но ориентировочно за неделю
до зачетной сессии чтобы мы зачетной сессии уже понимали у кого что отдельных дедлайнов
по заданиям нету да то есть вы в принципе можете оставить все на последнюю неделю но это не
рекомендуется делать как всегда вот что касается использования солвера на мой взгляд на стэпике
самый ну как бы самый простой вариант сейчас это использовать сайпай линпрок оптимайз вот есть
такая библиотека сайпай там есть модуль такой в этой библиотеке который называется оптимайз и
в этом модуле есть такая функция которая называется линпрок она на вход принимает
но я не буду сейчас пересказывать документации этой библиотеке но в общем она на вход принимает
матрицы которые описывают задачи линейного программирования вот задача линейного
программирования у нас как выглядит да у нас есть неравенство зависящий от х и у этих х
коэффициенты можно обозвать там а 1 1 и так далее а 1 n значит там меньше или равно чем b 1 и так
далее у вас получается что-то типа системы ну линейных уравнений только не уравнение
а неравенство dir chest мы системы линейных уравнений можем записать как произведение
матрицы на вектор равняется другому вектору до т mix lovelyould something iranice
принципе можем записывается а некоторые матрице помноженные на вектор их транспониров disposition
вектор столбец по компонентным не превосходит какого-то вектор какой-то вектор столбец
вот так мы записываем неравенство но вот в этой функции линпрок там можно Separately задать
неравенство и отдельно задать равенство. Вот там как раз есть соответственно
upper bound и upper bound. И есть такая уравненческая часть этой системы.
a eq на x равняется b eq, тоже вектор столбит. И вот это такой общий вид
задачи. На самом деле, в линейном программировании не обязательно иметь
отдельно равенства, отдельно неравенства. Потому что вы всегда можете
промоделировать равенство двумя неравенствами в противоположную
сторону, правда. А кроме того, вы можете промоделировать неравенства равенством.
Ну, не совсем. Промоделировать неравенство равенством, плюс ограничения
на знак переменной. Вот я напоминаю, что неравенство вот такое.
Можно промоделировать следующим образом. Вы вводите переменную недостатка такого,
то есть насколько левая часть действительно меньше, чем правая, раз она не превосходит правой, вот какая разность этих двух частей, и тогда вы обозначаете вот этот вот недостаток
переменной y, ну y1, скажем, да, и говорите, что вот если бы у нас еще плюс вот y1 было, тогда бы было точное равенство, равно b1, и
y1 по определению
неотрицательная величина, то есть
задачу линейного программирования с неравенствами можно свести к задаче, где только равенство, а из неравенств только вот такие вот штуки, только ограничения на знаке переменных
иногда это бывает удобно, но в вашем случае
по-моему и так нормально
вот там у нас задачах в основном как раз неравенства сплошные
вот это по поводу
линпрога
но на степике библиотеки чуть-чуть устаревшие, так на годик, на два
ну и соображение совместимости там обратно, чтобы ничего не рушилось, но
последние фичи вот библиотеки линпрог, я не уверен, что они там есть
типа
использование солверов совсем
мощных
значит high GHS такой есть, вот там этого пока нет, но зато там базовая функциональность вся присутствует, используйте это
так, чего еще хотел сказать, ну да, дедлайны
как делать линейное программирование
вроде сказал, если вы пишете на плюсах, то вы можете заморочиться и реализовывать симплекс метод, и он должен работать нормально
вот либо вы можете, ну, посмотреть псевдокод симплекс метода, его закодить, то есть за
там какой-то
код
использованный именно в этой части, в части симплекс метода
плющить никого никто не будет
вообще пишите все самостоятельно
у меня есть скрипт, который со степика все выгружает и
проверяет это все по метрике
НСД, знаете, что такое НСД метрика, я расскажу просто, потому что это интересная идея
вот как сравнивать, вообще как сравнивать тексты на совпадение, да, вот как вы представляете там
сравнивать на плагиат, да, два текста, вы как бы сравнивали?
вы бы не сравнивали? от греха подальше, да, не будем сравнивать, закроем глаза на это
а если бы надо было сравнивать, то что бы вы делали?
ну, а я имею ввиду алгоритмически, вот есть два текста, там две строки какие-то большие, что бы с ними делали?
ну хорошо, расстояние левенштейна, например, вы знаете такую штуку?
вот редакторское расстояние, да, сколько символов нужно вынуть из одной строки и сколько в нее вставить, чтобы из первой строки
получилась вторая
редакторское расстояние, вы даже скорее всего
динамическое программирование какое-нибудь прогали, вот в отношении редакторского расстояния, но есть гораздо более интересная и прикольная штука, которая называется ncd
это не относится к нашему курсу вообще, мне просто хотелось с вами поделиться
идеей, не надо это записывать там и так далее, да, не будет на зачете
вот ncd это расшифровывается как normalized compression distance, то есть нормализованное
компрессионное расстояние, да, ну compression сжатие
а идея очень простая, вот если два текста совсем совпадали бы, то, наверное, если бы вы их
сжимали каким-нибудь там zip или rare или 7 zip, то эти два текста они
сжатыми были бы по размеру не больше чем один текст, да, если его сжать
ну то есть как бы если второй текст не несет никакой новой информации по сравнению по отношению к первому, то он почти
забесплатно прибавляется к первому, да, при сжатии, но мы ожидаем, что архиваторы они как-то так должны работать, да, то есть они берут только какую-то вот
уникальную часть информации, а повторения все они стараются максимально как-то
значит максимально эффективно их закодировать и
вот как раз ncd метрика она работает ровно таким образом, мы сжимаем
сжимаем текст, ну, допустим, у нас тексты хранятся в
значит переменных x и y, значит мы берем просто конкатинацию этих штук, ну, можем как-то это дальше вычесть
вычесть, например, сжатый текст x и
поделить тоже на
comp x, вот получается такая штуковина
если, например, x это какой-то большой текст, из которого взят просто кусочек, такой y не очень большой, ну, какой-то
абзац, да, то вот эта штука будет практически нулевой, ну, очень маленькой, во всяком случае уж по отношению к
объему текста x, она-то будет совсем маленькой
вот, ну, вот, пожалуйста, идея такая, как вы можете сжимать, в смысле, мерить расстояние, не особо заморачиваясь вообще про природу данных
и эта штука, оказывается, отлично работает на
на всяких текстах решений в лотехе, вот, на втором курсе, а во второй курсе
ну, вот, вот, на втором курсе фифта
значит, у нас там
студенты в техе, в лотехе оформляют решение математических задач и мы там на плагиат
той же самой метрикой проверяем, очень хорошо работает, ну, то есть, очень грустно работает потом, когда кого-то уловишь на плагиате, пишешь там письмо
что, ну, как же так, вы списали, вот, смотрите, вот, и человек что-нибудь пытается объяснить, что, ну, типа, там, на самом деле, там, не знаю
я очень торопился, или что-нибудь еще, ну, в общем, короче, очень стыдная ситуация какая-то получается
вот, но, как бы, хорошо работает
и программный код тоже отлично, на самом деле, этой штукой
отрабатывается, ну, наверное, можно что-то эффективнее предложить, конечно, то есть, парстить в абстрактные синтоксические деревья этот код и сравнивать уже
абстрактные синтоксические деревья, но до этого я пока не доказал, что это может быть
вот, это надо потом еще вложить, годик труда, наверное, чтобы этим заняться
ну, ладно, это было лирическое отступление
вот такое запугивание, не списывайте, а то иначе вы пойдете под инсидиметрику
вот, но, ладно, кто-нибудь кто-то, кто так категорически не боится таких вещей, то-то и так не станет бояться, я думаю
а кто хочет писать самостоятельно, и так будет писать самостоятельно, как показывает оба
так, теперь вернемся к нашему, к нашей теме, наконец, занятия
у нас, мы с вами остановились, по-моему, на том, что сформулировали жадный алгоритм для покрытия матрицы
и этот жадный алгоритм, он у нас как действовал
он на каждом шаге брал, значит, строчку, которая покрывает
у этой строки вес мы обозначали W
так, окей, значит, мы брали строчку, какую-то R, у которой максимальное отношение
для чего, наверное, минимальное, мне так удобно будет записать, минимальное отношение веса строки
к количеству столбцов, которые покрываются этой строкой
число покрытых столбцов
при этом, естественно, имеется в виду число столбцов, которые не были раньше покрыты, но покрываются на этом шаге конкретно
то есть это такая стоимость покрытия одного столбца у нас получается
и мы с вами договорились, что на каждом столбце матрицы в момент, когда он покрывается жадным алгоритмом
мы будем писать такой Z на этом столбце, то есть вот если, например, строка покрывает нам три столбца
то это значит, что мы на этих столбцах
я уже не знаю, хочу использовать букву Z, честно говоря, или нет
назовем ее буквой Q
для этих столбцах будет вес WR натрия, WR натрия, WR натрия, у всех одинаковый
который равен как раз вот этой вот величине
благодаря тому, что мы это делаем, получается, что в итоге сумма вот этих всех фуитах
по всем столбцам и вот единицы до n будет равна как раз весу всех строк, которые были включены жадным алгоритмом
на протяжении работы, на протяжении его работы
вот как раз вот ровно потому, что это все суммируется к WR
и поскольку каждый столбец обязательно на протяжении алгоритма ровно один раз покроется
как вот вновь покрытый столбец, то как раз у каждого столбца эта штука не нулевая
а просуммировав все мы получим сумму весов всех выбранных строчек
Q это то, что раньше я обозначал Z, но мне не хочется писать букву Z, я, пожалуй, напишу Q
по определению, по определению это было количество, это был вес строки, которая выбрана
поделить на количество столбцов, которые мы покрыли этой строкой
то есть если мы, например, выбираем строку, она там покрывает 10 столбцов
но вот 7 столбцов уже раньше были покрыты, мы на них уже написали свои кушки
а 3 столбца у нас новых, которые мы покрываем
вот тогда мы на каждом из этих трех столбцов напишем WR поделить на 3
и мы с вами взяли вот эту сумму и сказали, что с одной стороны, вот эта сумма всех кушек
по всем столбцам равняется весу жадного решения, просто по определению
а с другой стороны, мы эту сумму как-то по-другому хотим оценить
как-то по-другому хотим оценить, чтобы связать это все с оптимальным покрытием
и, значит, чего мы с вами проделали, я пока что напоминаю в известном смысле
мы дальше сказали, что пусть RT строка произвольная
может быть она не была выбрана жадным алгоритмом, может быть была
вот посмотрим произвольную строчку матрицы
и рассмотрим столбцы, которые этой строчкой покрываются
я не помню, по-моему, S их обозначали, типа SL и так далее, S1
и мы сказали, давайте их занумируем
это пока краткое содержание предыдущей серии, как в Санте-Барбаре
то есть там нового ничего пока для вас, скорее всего, нету
так вот, мы сказали, давайте мы занумируем все столбцы, которые покрываются этой строчкой
все столбцы, в которых в этой строке стоят единички в матрице
и занумируем их в том порядке, в котором они покрывались жадным алгоритмом
жадно алгоритм, в итоге все столбцы покрывают
значит, он покрывает и эти столбцы в том числе
и естественно, что жадно алгоритм, их покрывает, не обязательно, по одному
то есть, он, в какой-то момент, выбирает строчку, которая, может, там
первые четыре столбца, в этом списке покрыла
потом, какую-нибудь строчку, которая следующий столбец, покрыла
потом строчку, которую все остальные столбцы покрыли
то есть они, может быть, покрываются какими-то группами
группами как они занумерованы там внутри каждой одной группы это не важно главное чтобы вот в
целом они шли да по порядку вот так вот покрытие значит что если столбец идет в этом списке правее
то это значит что он покрыт не раньше чем тот столбец который идет в нем левее теперь мы с
вами дальше сказали вот что рассмотрим момент который покрывается столбец эскатой и к этому
моменту у нас не покрытыми являются как минимум сколько столбцов как минимум南 столбцов правда
эскатый я с камени Wolf первые но все кто в этом списке идут провели не покрытыми являются как
минимум ка столбцов и эти ка столбцов да у нас есть шанс вот в этот момент как когда у нас этот
Jesse не покрытый со столбиц все эти столбцы тоже еще не покрыты в этот момент у нас есть
шанс покрытия к все за счет того, что мы возьмем как раз таки r-ую строку матрицы.
И если мы возьмем сейчас r-ую строку матрицы, то стоимость покрытия вот этих вот столбцов,
каждого из них, будет какая? Ну не больше, чем вес строки. Пусть вот эти столбцы покрываются
строкой r. Вот если мы сейчас прям возьмем именно строку r, то мы покроем все эти столбцы
гарантированно за один раз. А вес у этой строки, ну wr, и стало быть, взяв r-ую строку,
мы в расчете на один столбец будем иметь вот такую стоимость покрытия этого столбца.
Но жадно алгоритм он может взять любую строку, у которой наименьшее вот это вот число. И стало
быть, если мы рассматриваем конкретно тот шаг, на котором гарантированно покрывается вот именно
этот столбец, то на этом столбце пушка, которая написана кукатая, будет не больше, чем wr делить
на k. То есть жадно алгоритм сейчас покроет sk. Может быть вместе с этими столбцами, взяв строку r,
может быть вместе с какими-то другими столбцами, но во всяком случае у жадно алгоритма есть
возможность истратить на этот столбец вот такую вот стоимость. Жадно алгоритм старается
минимизировать стоимость, следовательно он выберет что-то еще может быть более оптимальное,
но уж как максимум вот такое, потому что он здесь минимизирует. Вот, собственно поэтому мы и
выписываем такое нерайстое в эту сторону. Ну как вам сказать, да, да, да, то есть кукатая это вообще
кушки, это такие константы, которые присваиваются на разных шагах алгоритма столбцам, и мы знаем,
что у каждого столбца есть единственный шаг алгоритма, на котором этот столбец получит вот
такую пометку, и больше эта пометка меняться уже никогда не будет, потому что столбец только один
раз меняет свой статус как бы с непокрытого на покрытый, да, и вот ровно на этом шаге,
на котором столбец изменяет свой статус, мы и приписываем этому столбцу свою кушку. И вот как
раз мы рассматриваем тот шаг, на котором покрывается именно вот этот вот столбец эскатый,
и говорим, что кушка, которая может быть ему приписана, уж заведомо не больше, чем вот такая
штука, потому что вот конкретная строка, да, вот есть, если мы ее взяли, то приписали бы ровно
такую кушку, но может быть даже возьмем что-то еще лучшее и припишем еще лучшую кушку, ну,
если меньше, да, потратим чуть поменьше денег на покрытие этого столбца. Вот, и вот у нас
получается вот такая вот, такая штука, у нас получается такая оценка. Ну, исходя из этой оценки,
мы с вами сможем оценить вот эту вот сумму. Давайте это проделаем. Давайте рассмотрим
оптимальное покрытие матрицы, оптимальное покрытие, с которым нам надо сравниться. Вес
этого оптимального покрытия, да, вес опт. Вес опт. Как связаны K и L? Никак.
S или, так далее, S1. Покрываются, вот здесь, имеется в виду, просто это те столбцы,
на пересечении которых со строкой, стоят единички, то есть столбцы, могущие быть покрыты
этой строкой. Да, я просто слово «покрывать», «покрывается», да, я использую в двух, может быть,
разных качествах, хотя не часто это дело. Первое качество — это что столбец, в принципе,
может быть покрыт, если мы возьмем эту строку. Это просто значит, что на их пересечении стоит
единичка в матрице. А второе качество — это что вот этот столбец конкретно перешел из статуса
«непокрытого» в статус «покрытый» по ходу жадного алгоритма, вот ровно в тот момент,
когда мы взяли какую-то строчку. Вот это я выписал список столбцов, которые просто могут быть покрыты
этой строкой, да, в принципе, все столбцы покрывабельны этой строкой. Вот, дальше мы
рассматриваем произвольный столбец из этого списка и немножко про него рассуждаем. Это может
быть в том числе с l-ты или в том числе с первой, да, вот просто смотрим на произвольное любое k
от единички до l и про него рассуждаем. Вот, вот что мы сейчас делаем, да, можно написать здесь,
рассмотрим произвольное k от 1 до l. Значит, и говорим, что в момент, когда покрывается столбец
s-катой, а он когда-то же покрывается жадным алгоритмом, у нас не покрыты вот как минимум
эти столбцы. Если мы выберем эту строчку, то мы точно их все покроем прям с ходу и
так далее, так далее. Вот оттуда мы выписываем вот это неравенство. Значит, дальше мы с вами,
нам нужно как-то вот сравнить, сравнить с вот этим вот оптимальным покрытием, сравнить с
оптимальным покрытием. Давайте с ним сравнимся, давайте запишем, что это такое. Это сумма wr по
всем строчкам, принадлежащим оптимальному покрытию, ну просто по определению. Это сумма wr по всем
строчкам, принадлежащим оптимальному покрытию. Так, чудесно. Так, давайте мы теперь, давайте мы
теперь еще запишем здесь кое-что. Мне сейчас к алгоритму тут хочется перейти. Парам-парам.
Запишем вот такую вот штуку. Сумма wr по всем i, вот единица dn. Мы знаем, что это вес жадного
покрытия. Она не превосходит вот такой вот штуки. Сумма по всем строчкам из оптимального покрытия
здесь я напишу сумму по всем столбцам, покрываемым этой строкой. Вот, но я это напишу вот таким вот
образом. Сумма по g от lr до единички. Но для каждой строки r столбцы, которые ей покрываются,
они какие-то свои. Вот это l, оно естественно для каждого r свое. И вот здесь я хочу написать вот
что. Просуммировать по всем строчкам из оптимального покрытия, а здесь под этой суммой
просуммировать по всем столбцам, которые покрываются, покрывабельны этой строкой. Просуммировать
соответствующую кушку с индексом s ж. Сейчас давайте разберемся, что здесь происходит. Каждый
столбец, он обязательно покрыт оптимальным покрытием, иначе оно не было бы покрытием всей
матрицы. То есть для каждого столбца найдется какая-то строка из оптимального покрытия,
которой он покрывается. И вот мы теперь говорим, вот здесь вот в этой сумме, эта сумма по всем
вообще столбцам матрицы, сумма кушек. А здесь у нас тоже сумма кушек, но такая двойная сумма кушек,
в которой мы перебираем все строчки из оптимального покрытия и для каждой строки
рассматриваем все столбцы, покрываемые этой строкой. Можем мы между двумя такими суммами
поставить знак меньше или равно? Ну в принципе да, просто в этой сумме все кушки учитываются ровно
по одному разу, каждый столбец дает вклад ровно в одну такую кушку, в соответствующую сумму. А
здесь мы с вами что сделали? Здесь мы сказали, рассмотрим все все строчки, которые у нас в матрице,
которые принадлежат оптимальному покрытию. Значит оптимальное покрытие. И для каждой такой
строчки R возьмем все столбцы, которые покрываются этой строкой, на пересечении которых с этой
строкой стоят единички. И просуммируем вот эту кушку, эту, эту, эту, эту. Потом еще допустим вот
строка покрывает вот этот столбец, вот этот столбец, этот столбец, этот, этот. Просуммируем
теперь для этой строки еще вот эту кушку, эту, эту, эту, эту. При этом некоторые кушки будут у нас
вот в такой двойной сумме повторяться по многу раз. Например вот этот столбец, его кушка она в
в эту сумму войдет и для этой строки, и для этой строки
с повторением.
Ну не беда, ничего страшного.
Ну и так дальше.
Мы главное знаем, что каждый столбец какой-то строкой
из оптимального покрытия точно покрывается, и значит
его кушка, кушка этого столбца, она окажется обязательно
вот в этой вот двойной сумме.
Хотя бы разочек, может быть пару раз, может быть
10 раз, да, ну вот хотя бы по разу.
Если хотите, я здесь, значит, словами просто могу переписать
сумма по всем столбцам, покрываемым Эртой строкой.
Здесь написать Qs, да, значит, по столбцам, по столбцам
Qs покрываемым строкой R.
Но я надеюсь, что на картинке, на такой, это как-то более
прозрачно.
Перебираем строчки оптимального покрытия, для каждой строчки
суммируем кушки всех столбцов, покрываемых вот этой вот
строкой.
Используем просто определение, что покрытие, это такое множество
строчек, что у каждого столбца матрицы без отключения
найдется единичка хотя бы с одной строкой, значит,
вот этого вот множества, да, а это значит, что каждый
столбец обязательно хотя бы разочек, но вот в такую
двойную сумму войдет.
Найдется такая строка, которая покрывает этот столбец,
то есть найдется такая строка, что вот этот столбец,
его кушка войдет вот в эту сумму для этой строки.
Ну вот, я напишу так, что в принципе вот это вот
неравенство вытекает из определения покрытия, то
есть вот это неравенство, оно справедливое не только
для оптимального покрытия, а вообще для любого абсолютно
покрытия матрицы, это было бы справедливо, вытекает
и следует из определения, из определения покрытия.
Ну вот, еще раз повторю, что это не вытекает не из
оптимальности, это ничего общего с оптимальностью
не имеет, это просто вытекает из того, что вот это множество
опт является покрытием матрицы и больше ни с чего.
Ну и из того, что кушки не отрицательные числа.
И вот теперь мы можем продолжить вот эту вот штуку, теперь
мы можем продолжить вот эту вот цепочку, меньше
или равно.
Народ, если чего хочется спросить, уточнить, то вы
пожалуйста не стесняйтесь, мне не страшно там проговорить
еще пару раз другими словами, или еще какую-нибудь картинку
нарисовать.
Вот, если надумаете, обращайтесь, да, ага, хуито, ну это просто
какое-то число, написанное на столбце, ну да, да, это
вес строчки жадного, выбранный жадным алгоритмом, который
я покрыл этот столбец, поделенный на количество
столбцов, которые одномоментно были покрыты этой строкой.
Да.
Но вот в этом неравенстве, народ, в этом неравенстве
это неважно, неважно, как определены куиты на самом
деле.
В этом неравенстве куиты могут быть произвольными
не отрицательными числами.
Как бы ни были определены не отрицательные числа
на столбцах матрицы, вот эти вот куиты, если опт это
множество строчек матрицы, образующих ее покрытие,
необязательно оптимальное, вот это неравенство все
равно будет выполнено, оно все равно будет выполнено.
Значит, это ничего, как бы ничего здесь не следует,
никакого rocket science здесь нет, ничего не следует ни
из жадности, ни из оптимальности, это очень простое неравенство,
вытекающее из определения, что у каждого столбца найдется
такая строка, которая его покроет, то есть для каждого
номера от единички до n найдется такое r, что соответствующая
кушка войдет вот в такую сумму, для r строки, вот
и все, что здесь написано.
Не, мы естественно используем это неравенство с конкретно
определенными кушками, верно оно для любых не отрицательных
чисел там куиты, но мы его используем вот с этими.
Тут я слышу, что вы поразбирались основательно с тем, что
происходит, и это хорошо, значит, ну вот теперь мы
с вами видим, что жадное покрытие, вес жадного
покрытия это сумма всех кушек вот этих, дальше мы
написали вот такое неравенство, а дальше нам нужно оценить
его, и мы с вами сейчас оценим вот эту внутреннюю сумму
просто для каждого r, как мы это сделаем, о, смотрите,
а мы же здесь не зря занимались вот этой вот деятельностью,
для произвольной строки матрицы, для произвольной
строки матрицы мы понимаем, что кушка катово столбца
в этом списке, она не превосходит вот такой величины вес этой
строки поделенной на k, спрашивается тогда, а как
можно оценить сумму qs, давайте qs, qsk получается, надо мне
было здесь описать, qs и так по и от единички до l, вот
если просуммировать по всем этим столбцам их ушки,
то как мы можем оценить такую штуку, qslt не превосходит
wr делить на l, здесь, когда slt покрывается, то вместе
с ним не покрыто еще как минимум l столбцов, значит,
это не превосходит wr делить на l, для s l-1 его кушка будет
оценена wrt делить на l-1, l-1, плюс и так далее, плюс wrt
на 1, это когда мы будем оценивать qs-ms1, то есть мы
здесь для каждого столбца записали такую оценку, осталось
просто просуммировать эти оценки по всем k от единички
до l, что мы и делаем, но к счастью получается сумма
дробей, у которых одинаковые числители, естественно
что мы этот числитель можем вынести за знак суммы,
давайте мы это сразу сделаем, а под суммой остается очень
чего-то хорошее и нам знакомое, что остается, сумма единицы
на k под единицы до l, а мы знаем, как оцениваются
такие штуки, а как оцениваются такие суммы, почему это
логарифм? Разложение в ряд Тейлора, да, совершенно
верно, а если не знать про разложение логарифма
в ряд Тейлора, то что мы с вами можем еще сделать?
Интегральный метод, например, применить в оценке, знаете
интегральный метод для оценивания рядов? Нет? А вы кто-то знает?
А? А, вы знаете интегральный признак сходимости, но я
здесь, да, база, это полезно, значит, вот нам нужно оценить
вот такую сумму, что там с ней происходит, мы для
этого представляем ее слагаемое как такие столбики, значит,
столбик высотой 1, вот такой вот столбик, дальше столбик
высотой 1,2, дальше столбик высотой 1,3, дальше столбик
высотой 1,4, ну и так далее, да, и вот у нас с вами появляется
вот такая последовательность столбиков, площадь каждого
такого столбика это произведение ширины, которая у всех
столбиков единичная, на высоту, да, то есть вот, значит,
если мы с вами дойдем до к здесь, виноват, до l, да,
верхний предел суммы, то вот как раз высота последнего
столбика будет единичка на l, и площадь суммарная
вот этой вот фигуры, составленной из столбиков, это и будет
точное значение вот этой вот суммы, ну а теперь нам
остается просто обзавестись функцией единица делить
на х, которая проходит через правые части этих столбиков,
да, вот эта вот функция единичка делить на х, она проходит
через правые границы этих столбиков, 1,2, 1,3, 1,4 и так
далее, 1, l, и нам теперь что можно сделать, вот
первый столбик мы запишем, как он есть, его площадь,
его площадь равна единичке, а для всех остальных столбиков
суммарная площадь их не превосходит площади под
графиком вот этой функции от единицы до l, то есть интеграла
от единицы до l, функция единица на х, вот, ну и поскольку
интеграл от единицы на х это логарифм, вот единичке
до l, вот, то мы как раз получаем вот эту вот интегральную,
вернее, получаем вот эту вот оценку с помощью логарифма,
логарифм l, да, ну плюс единичка, единичка никуда не денется.
Вот, так что теперь мы можем с вами записать вот здесь
вот меньше ли равно, и это уже окончательное такое
меньше ли равно, которое нам годиться единица плюс
логарифм l, ну что такое l, l это максимальное количество
единичек, как универсально оценить это l, да, это максимальное
количество единичек столбце, вот виноват, в столбце или
в строке, в строке, потому что l это максимальное число
покрываемых строкой значит это максимальное количество единичек в строке матрицы да можно здесь написать так и написать
максимальное число столбцов покрываемых одной строкой ну я здесь давайте напишу просто логарифм n
ладно потому что общее это число столбцов матрицы это n
так что ну универсальная оценка у нас получается такая единичка плюс лонарифм n вот и
и
что мы теперь сделаем вот с этой вот оценкой да мы ее просто подставим сюда для того чтобы оценить вот такую вот сумму
вот это вот сумма
вот эта сумма мне превосходит вес этой строки помножить на какую-то
вещь чину которая не зависит от строк вообще да вот это абсолютно какая-то
неизменная вещь которая
кульматый куль скорой матрица зафиксировано она уже не зависит от никакого конкретного покрытия да значит
мы можем вынести вот это вот единица плюс логарифм
давайте здесь звездочку поставлю звездочка
вынести единица плюс логарифм
вот а тут у нас будет сумма по всем строчкам матрицы
входящим в оптимальное покрытие
wr
да я так вот здесь стоял как раз вот этот множитель я его так раз
вынес
но и теперь только мы вспоминаем что это оптимальное покрытие у нас это опт
все то есть мы теперь знаем что это не что иное как вес оптимального покрытия
от чего мы отталкивались мы отталкивались от веса
жадного покрытия от суммы всех кушек давайте я напишу напомню что это на самом деле дв гриве и
закончили мы с вами вот такой вот штуковины и то есть мы с вами дали некую оценку на показатель
ассимации нашего алгоритма во сколько раз жадное покрытие по весу больше оптимально
это не
константный показатель
аппроксимации да то есть это не какая-то там двоечка вот а чем матрица больше тем к сожалению будет аппроксимация хуже
но во всяком случае
во всяком случае в теории да
вот теории может быть все не очень хорошо
на практике жадно алгоритм работает очень неплохо
но сейчас мы тем не менее предъявим пример
который показывает что
действительно может здесь вот возникать логарифмичный по размеру матрице
саммножитель в худшем случае а мы с вами приводили пример такой матрицы нет не помните
не было да вот как раз мы сейчас этим займемся но перед тем как мы этим займемся
не знаю если чего-то вот еще могу прокомментировать то пока я не стер вот
сейчас могу потом уже сотру будет сложнее
интегральный метод стираю
приятно стирать какие-то вещи типа интегрального метода да которые не имеют отношения в дискретной оптимизации
непрерывчина уйди
так
ладно это мы тоже сотрем под шумок
это тоже сотрем
ну а дальше уже неважно раз уж столько всего потерли то и это не жалко
все стереть
давайте пример приводить пример
пример матрицы
на который жадно алгоритм
ну можете написать лажает но я не буду писать лажает мне неудобно да на который жадно алгоритм работает не оптимально
на который жадно алгоритм не оптимален
но не оптимален это мягко сказано да не оптимален вот именно в этом смысле что возникает такой по порядку
сомножитель
давайте рассмотрим
такую
невысокую матрицу но очень широкую
сделаем мы ее организуем мы ее вот так вот
возьмем единичку первой строке выписываем первую строку матрицы
возьмем единичку и
куча нулей
сейчас мы с вами поймем сколько нулей а потом еще раз давайте возьмем повторим вот это вот все единичка и
куча куча нулей
дальше вторую строку выпишем возьмем
подставим под единичкой 0 tuning потом напишем две единички и кучу нулей и потом снова две единички и куча нулей до конца
потом а угадайте что будет потом попытка не пытка
три единички да действительно
возьмем три единички
чтобы не было одиноко добавим к ним четвертую возьмем четыре единички и куча нулей
но 4 единички включают же 3 единички да и то же самое потом возьмем значит 4
единички и куча нулей и так дальше а теперь а дальше можете сказать что будет
дальше уже понятно что будет не 5 единичек 8 16 и так далее вот у нас
получается что строчки они являются такими повторениями до одного и того же
вот ну и в конце концов естественно мы берем там какую-то строку с 2 в
тепени а единичками значит которые вот здесь вот при как это сказать как это
сказать по-русски пэддит по-английски это будет пэддит то есть отступ да вот
отступ выполнен нулями в общем короче единички передвы зироус
которых два в степени а штук но это еще не вся матрица последние две строки будут
такие 0 не давайте так 1 и так далее 1 а здесь нули и наоборот куча нулей это
вот единственные две строки которые не по предыдущему правилу делаются да а в
этих строках как раз отличается левая и правая половинка всего у нас получается
в матрице сколько строчек значит это сколько строчек получается значит здесь
у нас 2 стих 2 в первый единичек 2 в квадрате единичек 2 в кубе 2 в
степени а плюс один единичек да то есть есть вот уже а плюс один строчек ну еще две строки
а плюс три строки такая матрица получается размера а плюс три на уох а сколько же здесь
столбцов но количество столбцов определяется просто тем что нам нужно вот эти вот единички
друг с другом не перекрываясь поставить да вот так вот по диагонали как бы и количество
единичек здесь 2 плюс 2 в квадрате плюс и так далее плюс 2 в степени а а чему равна
такая сумма 2 в степени а плюс 1 да получается не два степеня плюс 2 наверное 2 в степени
а плюс 2 и минус 2 минус 2 да слушайте похоже вот но геометрическая прогрессия 2 1 плюс 2
в квадрате плюс и так далее плюс 2 в степени а плюс 1 вот эта штука равняется 2 в степени
а плюс 2 минус 2 вот это и будет стоять тут а потому что здесь у нас два степеня единичек но
у нас же две половинки здесь столько и здесь только да вот но вы помните формулы для суммы
геометрической прогрессии хорош не да я сам все время забываю поэтому я очень люблю
ее заново выводить здесь я не буду конечно ее выводить заново вот просто напомню да что
здесь можно добавить вот так вот а вы знаете что такое телескопирование нет телескопирование
это сворачивание математического выражения ну когда она допускает за счет такого такой
группировки по парной при которой все схлопывается слопывается слопывается друг за другом это как когда
вы телескоп складываете звенья телескопа как бы одно на другое тогда налазит ну и телескопическая
антенна все то же самое да и в итоге весь телескоп такой длинный он такой раз и в такую штучку
сокращается вот здесь вот телескопирование такого выражения как можно устроить добавить
сюда 2 в первый и тогда 2 1 плюс 2 1 это будет 2 по второй да 2 плюс 2 4 а 4 плюс 4 это уже 8
а 8 плюс 8 это 16 и так далее и вот это вот вот эта спичка она весь этот бигфордов шнур заставляет
прогореть и в итоге он бабах и превращается в два степеня плюс два просто ну значит вот
исходно этот бигфордов шнур имел длину два степеня плюс два минус два не считая спички
которую мы добавили вот так ладненько это можно стереть вот такая у нас получается матрица
смотрите какая она не сбалансированная да у нее очень маленькая высота а по ширине
она экспоненциально относительно параметра давайте посмотрим как на этой матрице выглядит
оптимальное покрытие чему равен опт веса всех строчек считаем единичными то есть вес покрытия
это просто мощность покрытия как множество строчек какое оптимальное покрытие этой матрицы
предлагаете единичный да да две последние ну как оптимальные вроде одной строкой матрицу не
покроешь а двумя последними до первой половины столбцов этой строкой покрывается вторая половина
столбцов этой строкой значит у нас получается два и давайте посмотрим как жадно алгоритм будет
на ней работать давайте мы посмотрим сколько тут последние две строки сколько у них единичек
да вот здесь вот две единички 4 единички 8 единичек бла бла бла 2 в степени а плюс одна единичка а
вот здесь у нас сколько единичек 1 плюс 2 плюс 4 и так далее то есть плюс два степень а и того
сколько да а плюс один да здесь последняя степень это а значит сумма вплоть до 2 степеня она
будет два степеня плюс один потому что в этой строчке до у нас единички стоят только вот в
этой половинке вот и ну и в этой строчке все тоже самое два степеня плюс один минус один кто
побеждает побеждает с небольшим отрывом побеждает вот этот вот спортсмен да и мы этого
вычеркиваем он победила мы его вычеркиваем да ну в общем мы вычеркиваем эту строчку из
матрицы как бы мысленно вычеркиваем все столбцы которые ей покрылись что в матрице остается
остается та же самая картина но как бы на единичку меньше до параметр все строчки вплоть
до той которая содержит 2 степени а единичек а вот в этих строчках в этих строчках остается
поскольку единиц, когда мы повычерпим все стропцы, которые покрыты здесь, два
степеня минус один. То есть опять-таки каждый из этих строк проиграет вот этой
вот строке и так далее, и так далее, и так далее. То есть мы вынуждены будем
поперебирать друг за другом все строчки вплоть до самой первой. Мы, конечно, матрицу
покроем, но возьмем для этого все строки, кроме последних двух. То есть
жадное покрытие имеет размер а плюс один. Вот, но это печальный факт. Ну как
печальный? Ну не такой печальный, потому что это вполне соответствует нашей
оценке худшего случая о работе жадного алгоритма, правда?
Значит, мы понимаем, что если вот это вот n, число столбцов матрицы, то вот это
вот величина а плюс один, она имеет порядок. Давайте я напишу тета. Помните
обозначение тета? Вот это как бы, о большое, это оценка только сверху, но здесь
достаточно бессмысленно писать, что это оценка там сверху. Нам нужно, это же
пример, показывающий оценку снизу, что жадный алгоритм, мы можем его заставить
работать неоптимально, подав ему неудобную матрицу. Поэтому здесь тета и чего?
Логарифм от n, ведь h это логарифмичная величина по отношению к вот такой вот
экспоненте. Порядка логарифма n, действительно. Видно, что здесь есть, ну
некоторый разрыв в смысле константы, ну в смысле основания логарифма, да, то есть
здесь у нас логарифм двоичный на самом деле стоит, если быть точнее, да, и в
этой константе чего-то, значит, заглядывать, вот константа в этом тете, а тут у нас
логарифм натуральный по основанию 2.7, но что делает? Такой разрыв есть.
Принципиально, если p не равно np, организовать решение задачи о покрытии
полинамиальным алгоритмом, имеющим константные показатели аппроксимации, мы
бы с вами не смогли, то есть это такой известный факт. Замечание. Замечание, если
p не равняется np, то для задачи о покрытии,
нету алгоритма полинамиального алгоритма
с константным показателем аппроксимации.
Константным показателем аппроксимации. Так, про это мы с вами сказали, ну и на этом
мы заканчиваем работать с задачей о покрытии. Вот, хватит, хватит. И дальше мы с вами
переместимся к задаче камевые жора, потом будем рассматривать всякие, значит,
схемы приближения полинамиальные. Вот, перед тем, как я все опять не потру и не
перейду к задаче камевые жора, вам про нее напоминать и вводить, вот, вы можете
мне что-нибудь еще задать, какой-нибудь вопрос.
Имеется в виду, вот смотрите, вот когда мы задачу о рюкзаке, например, решали, мы
сказали, что можно взять и построить рюкзак, вес которого, стоимость которого не
меньше, чем половинка стоимости оптимального рюкзака, да, то есть там у нас тоже мы
сравнивали нашу ивристическую стоимость и оптимальную, но между ними
разброс был вот этот константный, да, в одну-вторую, а здесь у нас логарифм N это
величина, которая может быть сколь угодно большой. Ну, для этого, естественно,
матрицу гигантскую придется выдумывать, но тем не менее, вот, все-таки есть
качественное различие вот этих двух задач, например, о рюкзаке и о покрытии.
Задача о покрытии хуже решается, вот в этом смысле, в смысле приближенного
решения, возможности ее приближенной решить.
Вообще, чего мы с вами, давайте вы выберите направление, о котором мы дальше пойдем, мы
можем дальше пойти рассматривать схемы приближения, то есть такие классные
алгоритмы, которые могут с любым, сколько угодно близким к единице, показателям
проксимации решить задачу, а можем пойти рассматривать задача Камево-Ижора,
такую геометрическую, как точки обойти, значит, по циклу наиболее удобным путем.
Вот, вы, Камево-Ижора, ну это наиболее такая громогласная часть населения
сказала. Вот, давайте проголосуем, кто за Камево-Ижора? Ага, а кто за схемы приближения?
Ага, хорошо, Камево-Ижор, так Камево-Ижор. Значит, задача Камево-Ижора. Я напоминаю, что задача Камево-Ижора
или traveling salesperson problem, это задача в выборе гамильтонового цикла в графе наименьшего
веса. Помните, что такое гамильтоновый цикл? Это задача в выборе гамильтонового цикла в графе наименьшего
веса. Помните, что такое гамильтоновый цикл? Цикл, проходящий через каждую вершину, ровно по одному разу.
Выбор кратчайшего гамильтонового цикла.
Ну, например, данные точки, и нам требуется их обойти по циклу так, чтобы
минимизировать суммарную длину. Ну, например, вот такой маршрут можно предложить. Он, скорее всего,
будет действительно оптимальным. Ну, так, смотришь на картинку, кажется, что лучше сложно себе
представить. Для любого графа, для произвольного графа гамильтонов цикл, это вот что-то такое же.
Это цикл, проходящий по каждой вершине графа без повторений. Все равно, на какой вершине начинать его,
построить его очень трудно. Задача, даже определение для заданного графа, есть ли в нем
гамильтонов цикл, это импотрудная задача. Задача выбора оптимального гамильтонового цикла в данном
графе тоже импотрудная задача. То есть, мы, как всегда, с вами начинаем с того, что это задача
трудно решать, а значит обосновываем, что мы в праве решать приближенно. И у задачи ТСП,
задача Ками-Воежора, есть три разных варианта, самые общие из которых мы вообще не будем им
заниматься, потому что для него сложно предложить сколь угодно реалистичные какие-то схемы
приближения. Мы будем с вами рассматривать два вида задач. Это метрическая задача Ками-Воежора.
На самом деле, мы на ней как раз сосредоточимся. Это задача на метрическом графе. Задача ТСП
на графе с метрической функцией весов. Что это такое? Что такое метрическая функция весов?
Знаете, что такое метрика? Да, отлично. А что такое метрика? Какими свойствами она должна
обладать, чтобы быть метрикой? Большая нуля, не нравится треугольника, но не обязательно.
Симметричная неотрицательная функция, но еще она должна быть равна нулю тогда и только тогда,
когда два объекта совпадают. Но самое главное для нас это неравенство треугольника вот здесь.
Фактически на все остальное можно забить. Самое нетривиальное в метрике, самое интересное ее
свойство, это именно неравенство треугольника. И я его напомню, что когда мы говорим с вами про
графы с неравенством треугольника, то это означает, что для любых трех вершин графа A, B, C. Помните,
что множество вершин заданного графа G обозначается через V, A, G. Поэтому это я записал.
Для любых трех вершин A, B, C графа G мы хотим, чтобы вес ребра A, T не превосходил суммы весов
ребер AB и BC. В частности, это означает, что такое ребро должно существовать. То есть,
если существуют ребра AB и BC, то ребро AC тоже существует, и вес его не превосходит вот этого
сила. Из этого следует вообще говоря, что наш граф, если он связанный, то он должен быть полный.
То есть в этом графе мы не задумываемся уже над вопросом, а можно ли пройти напрямик между двумя
вершинами. Всегда можно. То есть считаем граф полным. Полный граф – это такой граф, у которого между
каждой парой вершин есть ребро. Поэтому вопрос о существовании гамильтонового цикла здесь
полностью снимается. Гамильтонов цикл существует всегда, причем их куча. А сколько примерно,
по порядку гамильтоновых циклов у нас в полном графе? Да, все перестановки. Их на самом деле
чуток меньше, как вы понимаете, чем перестановок, потому что одному и тому же циклу, как картинки,
соответствует много перестановок, в зависимости от того, с какой вершины вы начнете его считывать.
И в зависимости от того, в какую сторону вы пойдете считывать его или записывать,
наоборот, начиная с какой-то вершины. То есть точное количество гамильтоновых циклов – это
n факториал делить на 2n. n-1 факториал пополам, но все равно очень много. Факториал – это круче,
чем экспоненты. Это суперэкспоненциальная функция, сверхэкспоненциальная функция.
Ну так и есть. Официальные названия – суперэкспоненциальная функция или
сверхэкспоненциальная функция – это функция, растущая быстрее любой экспоненты с константным
основанием. Субэкспоненциальная функция – функция, растущая медленнее любой экспоненты,
тоже с константным основанием больше единицы. Так, к чему я это все говорил? Забыл. Просто к тому,
что перебором решать эту задачу, наверное, трудно. Вот что я хотел сказать. Ну так вот. Значит,
задача-то существование гамильдонного цикла не стоит больше, но все равно перебором ее не
решишь. Не перебираешь все перестановки, скорее всего, так просто. Но есть еще задача более
специфическая, а именно задача Евклидова за датчиками выезжора. В Евклидовой за датчиками
выезжора вершины графа это прям точки. То есть это вот то, как правило, то, как мы понимаем вот
такие примеры. Когда я на доске рисую какие-то примеры применения алгоритма какого-нибудь для
решения задачек на выезжора, то я же просто точки рисую, чтобы не задумываться, как там веса,
ребер определяются. Просто мы предполагаем, что и раз две точки нарисованы, то между ними всегда
можно провести ребро потенциально, и его вес – это просто длина соответствующего отрезка. То есть мы
буквально ищем коротчайший геометрический маршрут между точками на плоскости. Вот это Евклидова
за датчиками выезжора. Ну а формально можно ее так определить, что вершины графа – это просто
точки под множество Евклидового пространства РД, ну и вес ребра – это Евклидовое расстояние. Вес
ребра АВ – это Евклидовое расстояние между точками. Длина отрезка АВ. По понятным причинам
это очень удобно делать до смотрения примеров, когда не надо думать, как определять веса там,
и так далее. Рисуешь картинку, и все сразу понятно, вроде интуитивно. Мы с вами рассмотрим
несколько алгоритмов для решения за датчиками выезжора, и начнем мы вот с какого. Я его сейчас
просто опишу. Вообще опишу два алгоритма сразу на затравку, чтобы в следующий раз было с чего
начинать. Значит, они очень близкие по духу. Один называется ближайший сосед,
другой называется кратчайшие вставки. Я не буду записывать их словами, ладно,
я просто порисую немножко. Нарисую какой-то пример, и мы с вами поймем, как на этом примере
работал бы алгоритм. Ближайший сосед начинает из произвольной вершины графа. Идет, знаете,
куда? Не догадаетесь. Ближайшего соседа. Идет в ближайшую соседнюю вершину, которая еще не
была посещена, естественно, потому что повторяться мы не имеем права. Отсюда он пойдет сюда,
отсюда он пойдет, ну, скорее всего, сюда, отсюда он пойдет сюда, сюда и так далее.
Давайте я, ну ладно, нет, не буду ничего пытаться переделать. Получается неплохо,
не так уж худо получается. Кратчайшие вставки. Что делают кратчайшие вставки? Алгоритм кратчайших
вставок начинает с того, что выбирает самую близкую пару вершин друг к другу, то есть самое
короткое ребро в графе. Ну, давайте я вот эти вот две вершины нарисую чуть подальше друг от друга,
чтобы у меня самое короткое ребро было вот именно это. Сначала выбираем самое короткое ребро,
потом выбираем вершину, которая ближайшая к этому ребру. Ближайшая к этому ребру это значит
ближайшая к самому близкому концу этого ребра, какая-то вершина. Ну, вот это вот. Да, вот к ней
ближайший конец ребра, он находится на таком расстоянии, и, видимо, это расстояние лучше,
чем у любой другой вершины графа. Вот. Эта вершина добавляется третьей. Третьим будешь,
спрашивают двое ее. Буду, говорит. Ну, я примеры демонстрирую для Евклидова случая, но на самом
деле это для произвольного метрического графа вполне работает. И дальше мы что делаем? Мы находим
ближайшую вершину к вот этому циклу. Какую какая-то вершина? Наверное, вот это. И после того,
как мы зафиксировали эту вершину, мы ее на цикл добавляем. А как мы ее добавляем? Как добавить
вершину на цикл? Раззамкнуть, да. Раззамкнуть-самкнуть. Раз и два, да. Вот. То есть мы удаляем из цикла два
рябра. Одно рябро добавляем два рябра. Дальше ближайшую вершину находим к этому циклу. Какая это
будет вершина? Вот. А самое лучшее. То есть мы... То есть мы... Не, но серьезно. Серьезно. То есть мы
добавляем рябро, которое минимизирует добавленную стоимость. Вот. По всем возможным ребрам, которые из
цикла можно было удалить, вот, минимизируем. Но уже после того, как мы зафиксировали вершину... То
есть важно, что сначала фиксируется, какую вершину мы добавляем, и потом мы уже выбираем наилучший,
наиболее оптимальный способ добавить эту вершину на цикл за счет удаления одного рябра и вставки этой
вершины вместо этого рябра. Вот. Следующая вершина, какую мы вставим? Наверное, вот эту, да. Ну,
наверное, скорее всего, вот так. Следующая вершина, скорее всего, вот эта будет.
Плохое? Что значит плохое? Но рябро удаляется. Наилучшее, какое вы можете удалить? Ну,
может быть... Не, ну, может, какое-то другое будет. Не, ну, я мог бы написать плохую картинку. В любом
случае, этот алгоритм не работает оптимально. То есть у нас задача камевые жоры на метрических
графах и эвклидовая задача камевые жоры, она тоже импотрудная. Вот. То есть здесь все равно все
плохо, не переживайте. Вот. Дальше, ну, вот эту вершину добавляем. Ну, непонятно. Вот так вот или
вот так вот. Ну, давайте представим, что вот так, да. И в конце добавляем вот эту вот вершину. Ну,
допустим, вот так. Опс. Получается вот такой цикл тоже, да? И вот мы с вами в следующий раз
посмотрим на показатели аппроксимации вот этих вот двух алгоритмов. Оба жадные такие по духу,
да? Каждый раз чего-то делаем локально оптимально. Либо в ближайшего соседа идем,
либо в ближайшую вершину к текущему циклу берем и ее добавляем, да? Но показатели аппроксимации
оказываются очень разными в худшем случае у этих двух алгоритмов. У одного константный,
двойка, а у другого логарифмичный по размеру графа. Угадайся, у какого какой. Но я вам не буду
говорить час-ответ для следующего раза. Вот до следующего раза подумайте, ну не подумайте,
просто погадайте, какой из алгоритмов лучше. Спасибо.

Мы разобрались с тем, что такое энтропия, и доказали, что энтропия от нескольких случайных величин
не больше, чем сумма отдельных энтропий. С помощью неравенства выпуклости мы в конце это доказали.
Я, конечно, могу написать просто само утверждение. Такая теорема была, что энтропия, скажем так, совместного
распределения не больше, чем сумма отдельных энтропий. И вот мы хотим как-то с помощью этого
замечательного факта умудриться доказать теорему о раскраске. Теорема раскраски, наверное,
звучала следующим образом. Пусть h. Совершенно не помню, как я обозначал множество вершин.
Может, оно было r с индексом n, но давайте я так и напишу. h это r с индексом n, а тут, наверное,
тогда m каллиграфически. E было. Оно всё взаимно заменило, понятное дело. Множество вершин
это множество из n элементов. Про D мы знаем, что оно тоже состоит из n элементов, то есть ребер
столько же, сколько вершин. Но при этом гиперграф какой угодно, не обязательно однородный. Я еще
там рассуждал про матрицу Адамару, у которой гиперграф почти однородный, но всё-таки одно
ребро выбивает. Вот есть какой-то произвольный такой гиперграф, тогда существует раскраска rn
в красные и синие цвета. Такая, что для любого ребра из E разница между
числом его красных и синих вершин
не больше, чем что-то типа 11 карнизов.
Ну да, известно, что 5 карнизов. Вы увидите, я расскажу основную идею, но я выкладки последние
делать не стану, потому что они скучные, суть ясна и запоминать их не имеет никакого смысла.
Поэтому вот в эту константу 11 особенно не впериваетесь, так сказать. Ну 11, ну 20,
ну 100, неважно. Важно, что корень из n без корни из логарифма, который вытекал из простой оценки,
доказанной на прошлой лекции. Что-нибудь помните? Я на прошлой лекции доказал, что если ребер просто
какое-то количество m штук, то здесь вот к этому корню из n добавляется еще корень из логарифма
m. А тут вот оказывается, что если m равно n, то этот корень из логарифма можно просто убить
каким-то образом. И вот убивать мы его будем с помощью в том числе вот этого неравенства для
энтропии. Какая-то энтропия будет. Но идея следующая. Давайте докажем вот такую теорему,
пускай штрих, как обычно, я их не нумирую. Докажем не совсем пока то, что хотелось бы доказать,
а докажем, что вот чего выполнено. Ну, давайте вот так. Все то же самое. Пусть h
равняется n, тогда существует неполная раскраска. Сейчас как бы это лучше сказать, чтобы не ковнуть
сразу. Да нет, все просто. Существует раскраска, в которой почти все покрашено, но кроме какой-то
очень небольшой части. Существует раскраска
rn,
которой не менее 1, минус 10 в минус 9 степени n вершин
покрашены красные и синие цвета. Частичная раскраска, как говорят, красные и синие цвета.
Ну а остальные вершины, вообще говоря, условно просто не покрашены. Остальные вершины пока не
докрашены. Ни менее, чем столько вершин покрашены в красный, в синий цвета, остальные вершины
покрашены нейтрально, не покрашены. Остальные вершины пока не покрашены.
Вот, причем снова в каждом ребре для любого омытого S-E разница между числом красных и
синих вершин такая, как нужно. Разница между числом красных и синих вершин, ну скажем,
не больше чем 10 корней Z. То есть разница хорошая, но некоторые вершины не покрашены,
она даже лучше, чем то, что мы хотим в итоге получить. Она оценивается как 10 корней Z. Но какие-то
вершины могут быть недокрашены. Понимаете идею? Вот, допустим, мы это доказали. Почти все покрашено,
ну почти все не в асимптатическом смысле этого слова, а вот ровно в том, как здесь написано,
одна миллиардная доля не покрашена, но зато все остальное покрашено. Вот все остальное покрашено
очень хорошо. Как мы будем действовать дальше? Мы возьмем вот эту одну миллиардную от общего
числа вершин и попробуем доказать на этой одной миллиардной аналогичный результат. Ну, то есть
вот на этой одной миллиардной от общего числа вершин будут какие-то хвостики, недокрашенные
хвостики каждого ребра, и мы снова попробуем красить красные и синие цвета, так чтобы различие
между количеством красных и синих вершин на каждом из хвостиков, но было уже не 10 корней
из N, а там какая-нибудь 1 сотая корня из N. И так будем действовать итеративно, после чего в сумме
накопится как раз 11. Ну, разности мы всегда смотрим по модулю, как-то в прошлый раз вы не
спросили, но в общем, конечно, это подразумевалось. Но вы можете, да-да-да, понимаете, в одном ребре
красных больше, чем синих, в другом синих больше, чем красных, это нормально. Конечно,
по модулю, да, то есть вот эта разница, имеется в виду модуль разницы, то всегда так подразумевалось.
Сейчас понятно, или нет? Значит, смотрите, я ее целиком, еще раз повторю, я ее целиком не буду
реализовывать, и на экзамене, естественно, с вас никто не спросит, вы должны просто понимать,
как дело развивается дальше. То есть я не собираюсь доказывать теорему два штриха с оценкой того,
насколько разброс велик или мал на вот этой одной миллиардной доле. Абсолютно аналогичное
рассуждение там будет работать, но мучить людей на экзамен двумя одинаковыми рассуждениями,
чтобы теорема сошлась прямо вот к этим 11, это было бы жестоко. Мне кажется, это не нужно.
Главное, чтобы вы понимали, почти все покрасили, ну уже хорошо, даже вот уклонение не слишком
большое. Ну, маленький кусочек точно так же покрасим, там уклонение будет сильно меньше,
и сумма этих уклонений, она как раз должна докопиться во что-то такое. То есть, короче,
еще раз, я вот сейчас эту теорему докажу, а больше ничего доказывать не буду, но смысл,
я надеюсь, вы поняли. Или плохо? Поняли, да? Вот, но она уже достаточно катарсисогенная,
красивая, замечательная, так что сейчас мы ее и произведем на свет доказательства. Так,
ну, во-первых, давайте вспомним, что красный и синий цвета, когда мы работаем вот с этой
задачей, мы интерпретировали как плюс и минус единицу просто. То есть, каждой вершине из
множества rn, каждому числу от единицы до n, мы присваиваем плюс или минус единичку. Помните,
да? Ну, давайте вот, если вершина окажется в итоге не покрашенной, будем считать, что ей присвоен
ноль. Плюс единица – это условно красный цвет, минус единица – синий, и ноль – это никак. Просто на
будущее, чтобы было понятно. Но пока давайте действовать прямо как в доказательстве теоремы
с логарифом. То есть, случайно, в обычном смысле этого слова, покрасим rn. Каждому числу с
вероятностью одна вторая присвоим плюс единицу красный цвет или минус единица синий цвет. Никаких
нейтральных вершин пока нет. Просто берем обычную случайную раскраску. Четко уясните для себя,
что обычная случайная раскраска – это вектор из плюс-минус единицы длины. Вектор из плюс-минус
единиц длины. Пока нейтральных вершин нет. Когда-то они в конце появятся. Но вот пока есть
случайная раскраска. Целиком все множество покрасили. Все элементы в красные и синие цвета,
причем банальные. С вероятностью одна вторая – плюс один, с вероятностью одна вторая – минус один.
Так, давайте введем вспомогательные случайные величины. Введем не случайные величины,
обозначим их BIT – случайные величины. И здесь от единицы до N. И это номер ребра. BIT
отвечает очередному ребру из множества ребр нашего гипрограмма. У нас их N штук – ребр. Поэтому
соответствующих величин, которые я сейчас определю, их будет как раз N штук. Так,
зависеть они будут, естественно, от раскраски, как случайные величины, на множестве элементарных
событий. Раскраска – это элементарное событие. Так, давайте считать, что BIT равняется нулю,
если… Сейчас чихну. Уже чихну, сейчас еще чихну. Возможно, не один раз. Так, BIT равняется нулю,
если выполнена вот такая вот цепочка неравенств. Ну, давайте где-нибудь сделаем строгий значок,
где-то не строгий, чтобы все было однозначно. Вот так вот, если хи от Эмитова лежит вот в таких
пределах. Так, что такое хи от Эмитова? Это вот то самое уклонение, которое нас интересует. Это
было, конечно, в прошлый раз, но я повторю. Итак, хи от жи – это плюс или минус единица,
с вероятностью одна-вторая, и сумма хи от жи по жи из Эмитова – это как раз разница между
количеством красных и синих вершин вот в этом конкретном ребре Эмитова. Но в данном случае
именно даже не по модулю, а просто разница между красными и синими. Складываем столько плюс единиц,
сколько есть красных вершин, и потом вычитаем все сини. Ну вот, если эта разница лежит вот в
таких пределах полустрогих, то боитое считаем равной нулю. Ну и дальше двигаемся по таким вот
отрезкам длины 20 корней и зен. Пишем 10 корней и зен меньше строку, чем хи от Эмитова меньше
либо равно 30 корней и зен. Здесь будет боитое равно единице и так далее, причем в обе стороны.
Ну понятно, что хи от Эмитова не может принимать бесконечно много значений. То есть когда-то дело
оборвется, но неважно. Так понятно, да, как определено боитое? Очень просто.
Так, ну давайте рассмотрим зачем-то. Пока совершенно непонятно зачем. Ну друзья,
в конечном счете все будет понятно, но случайные величины понятны как определенно. Давайте
рассмотрим их на тропе. Х от боитое. Рассмотрим и оценим. Так, что такое х от боитое? Это минус,
минус сумма. Какая у нас будет сумма боитое? Какие значения принимает 0, 1, минус 1? Все целые.
Сейчас какой-то вопрос. Тоже самое, да, я вот сюда нарисовал многоточие. Я сказал вот многоточие в
обе стороны. То есть если от минус 30 до минус 10, то боитое это минус 1 и так далее. Да-да-да-да. То есть
сумма будет формальна от минус бесконечности до бесконечности, но как я сказал, конечно для
конкретного n она принимает лишь конечное много значение. Не ограниченное с ростом n, но конечное
при каждом конкретном значении n. Как бы тут написать-то по k может быть, да, формально от
минус бесконечности до бесконечности будет вероятность того, что боитое равняется k на
дворичный логарифом этой же самой вероятности. Ну опять же понятно на самом деле, что можно было
суммировать от нуля до бесконечности, вернее даже не так, выделить отдельно слагаемое с нулем,
а остальные они одинаковые. Какая разница, уклониться вправо или уклониться влево,
как у пьяницы. Все же симметрично, блуждание симметрично. Здесь фактически же блуждание,
мы это и в прошлый раз обсуждали, складываются плюс и минус единички. Ну давайте посмотрим.
Минус вероятность того, что боитое равно нулю на лог дворичной вероятности того, что боитое
равно нулю. Что значит, что боитое равно нулю? Это значит, что модуль хиатемитова не превосходит
10 карниза. Давайте я отдельно напишу. Вероятность того, что боитое равно нулю, это вероятность того,
что модуль хиатемитова не превосходит 10 карниза. Ну вы можете придраться, наверное, к тому,
что здесь значок строго больше, но, но в общем, конечно, это ничего больше либо ровно что-ли?
Извините, что-то я не то чихаю, то каждая непростуженно, совершенно что-то попало.
Как это называется? Поняли, что я написал? Что здесь не совсем равенство, потому что здесь вот я
старался, вроде как, разделал не крепекающиеся отрядки. В какую сторону неравенство-то получается?
Если здесь я написал меньше либо равно, я могу здесь меньше написать, тогда будет в одну сторону.
Могу написать меньше либо равно, тогда в другую. Так, в какую сторону сейчас неравенство? Я хочу,
чтобы вы участвовали в процессе. Как? Меньше либо равно? Меньше либо равно. А мне что-то кажется,
что мне лучше будет, чтобы было больше либо равно. Давайте я вот так вот. Ну, теперь это есть
единица минус вероятность того, что модуль h от m и t больше либо равен 10 корней z. Это просто
отрицание написал. Ну, а это снова неравенство большого уклонения, которое с e в степени. Мы в
прошлый раз вспоминали как раз про пьяница, которая далеко уходит от кабака с ничтожно маленькой
вероятностью. Это будет больше либо равно, чем единица минус 2e в степени минус 100n поделить на что?
На 2n, да? Ну ладно, нет, я нормально пишу. На удвоенную мощность m. На удвоенную мощность m.
Сейчас правильно? По-моему, правильно. Что, забыли уже за неделю неравенство большого уклонения?
Ну, там e в степени минус a квадрат поделить на 2n, где n это количество слагаемых. Ну,
сейчас у нас слагаемых столько, сколько элементов в множестве мытое. Вот столько будет слагаемых.
Вот. Больше либо равно. Ну, понятно, потому что вероятность не больше, чем вот это удвоенное,
потому что тут модуль. Без модуля была бы просто экспонента, а двойка за счет того,
что модуль сразу. Так, мощность мытого точно не превосходит n, не превосходит, значит переворачиваем
больше либо равно, с минусом меньше либо равно, с этим минусом все правильно, больше либо равно.
Снак неравенства в нужную сторону, поэтому я могу написать, что это больше либо равно 1-2e в степени
просто минус 50. Заменяем на n, n сокращается. Ну, очень близкая клиниция, вероятно, что?
10-9 Поршу будет совершенно неожиданным образом, вы пока не догадаетесь. Это не 10-9, нет. Вы
пока даже не догадываетесь, как я собираюсь применить. Это там катарсис будет, если вы все
поймете. Это совершенно неожиданно. Сейчас пока, я уверен, вы не понимаете. Это не 10-9,
это конечно гораздо меньше. Но это меньше, а само-то оно близко к единице. Смотрите,
как устроена функция энтропии. Вот на отрезке от нуля до единицы, на котором находится значение
вероятности. Так, друзья, функция энтропии в смысле как функция от вероятности. Она устроена
вот так. Ее максимум в 1 и 2, и она в нуле ноль, и в единице ноль. Ну, я надеюсь,
это можно сообразить без подробных пояснений. То есть, если мы знаем, что вероятность больше
либо равна чего-то очень близкого к единице, то это значит, что сама энтропия не превосходит
своего значения в точке равной вот это. Вот это. Вот не больше, чем там минус. Вот, минус 1 на 2e в
минус 50, на лог двоичный вот 1 минус 2e в минус 50. Она в этом месте уже убывает. Поэтому, если мы
написали неравенство в эту сторону, то сама энтропия в эту оценивается. Ну, при этом вы понимаете,
конечно, что этот логарифм отрицательный. Соответственно, с минусом там получается
положительное число. Но это положительное число ничтожно маленькое. Ну, я с вашего позволения не
буду вспоминать, чему оно равно. Я и не помню. Ну, если вы хотите, вы можете посчитать на
калькуляторе. Оно очень-очень маленькое. Хорошо. Мы разобрались с центральным слагаемым в этой
бесконечной в обе стороны сумме. Теперь посмотрим на слагаемые с положительными k. Все остальные
слагаемые им симметрично, то есть дают там какой-то удвоение. Ну, вот берем, например, слагаемое с k
равном единице. У нас получается минус вероятность того, что bit равно единице,
на лог-двоичной вероятность того что bit равно единице. Что такое вероятность того что bit
равно единице. Вот, давайте я тут führt, с того что bit равно единице это вероятность
того, что хи от эму те лежит вот в этом полуинтервале.
Ну, это уж точно не больше, чем вероятность того, что
хи от эму те больше, чем 10 корней z.
Я опускаю вот это вот ограничение, оставляя только нижнюю
границу.
Снова пользуюсь неравенством большого уклонения с
пьяницей и получаю, что это не превосходит не в степени
минус 50.
Прям вот пьяница в чистом виде.
Ну точно так же оцениваю как здесь, но только уже
теперь сверху, а не снизу.
Но опять, пользуясь вот этой картинкой, раз у меня
вероятность очень маленькая, значит энтропия тоже очень
маленькая.
Она оценивается путем подстановки этой величины
Вот в эту, вот какую там формулу где она?
Вот в эту формулу, да?
А, вот в эту все, нашелась.
Вот, она не превосходит минус, e в минус 50.
Заметьте, двойки тут нет, потому что модуля нет.
Ну, не важно, e в минус 50, налог двоичный, e в минус
50.
Это снова ничтожно, маленькая, конечно, положительная
величина.
А дальше они убывают в какой-то геометрической прогрессии,
если не хлеще того.
То есть, когда вы на следующий шаг переходите, что там
получается?
E в какой степени?
Можете быстро сообразить в уме?
Минус 450, абсолютно верно, но понимаете.
То есть, вот здесь e в минус 50 на что-то, а следующее
слагаемое это e в минус 450, ну там на что-то может чуть
побольше, но все равно.
Бешенно сходящийся ряд.
Вот этот ряд, бешенно сходящийся, состоит из очень маленьких
величин.
То есть, короче, у нас получается, что это меньше либо равно
некоторого epsilon, которое можно вычислить, ну или оценить
там на компьютере руками, как угодно, я помню, что
это 3 на 10 минус 20.
Ну, вы можете в это просто поверить, естественно,
ни один человек на экзамене не потребуется вас доказывать,
что именно 10 минус 20, да на 3.
Если вы очень герливы, вы можете попробовать запустить
компьютер и проверить, а то вдруг я ошибся.
Меня еще пока ни разу не поправлял никто, но это
совершенно не значит, что я ни разу не ошибался.
Вполне возможно, я каждый год рассказываю 3 на 10 минус
20 и каждый год лажаю.
Но смысл от этого не меняется, пусть не 3 на 10 минус 20,
пусть 2 на 10 минус 19, может быть там в итоге тогда получится
не 10 минус 9, а что-то другое, но все равно все в итоге
сойдется просто не к 11, а к 20 скажешь.
Главное, чтобы вы суть поняли, доказательство, понимаете,
выкладки, они никому особо не интересны, но я для определенности
просто пишу и вроде бы это правильное, вроде бы это
правильное.
Ну, товарищи, воспользуемся вот этой теоремой, из нее
следует, что энтропия от вектора с координатами
b1 и так далее bn не превосходит эпсалону множительной.
Эпсалон это вот то сам, 3 на 10 минус 20, пользуемся вот
этой субоддитивности.
Так, ну, что бы из этого следовало?
Я утверждаю, что из этого вот чего следует.
Следует, что существует набор конкретных значений
s1, s2 для вот этих случайных величин такой, что вероятность,
при которой случайный вектор b1, bn принимает именно этот
векторный набор значений, больше либо равна 2 в скепени
1 минус эпсалон умножительная.
Ну, это я сейчас подробно поясню, это почти очевидно,
на самом деле, просто из определения энтропии, но
на всякий случай я аккуратно поясню, предположим, противно.
Предположим, что все такие вероятности b1, bn равняется
чему-то меньше, чем 2 в скепени 1 минус эпсалон.
Так, нет, наверное, не один минус эпсалон.
Вероятность, я не прав, вероятность 2 в скепени минус эпсалон
просто 2 в скепени, ой, черт, вот так вот, конечно, поторопился,
вот так, один минус появится чуть позже, он появится,
но в другом контексте, сейчас сразу после этого
появится, предположим, что все вероятности совсем
меньше, чем 2 в скепени минус эпсалон, но тогда энтропия
этого вектора b1, bn, ну, она, естественно, равна минус
сумма вот этих самых вероятностей, b1, bn, равно там чему-то, умноженных
на логарифум двоичный тех же самых вероятностей,
b1, bn равно чему-то.
Так, и мы предположили, что вероятность меньше, чем 2 в
степени минус эпсалон n, но давайте вот сюда эту оценку
подставим, под логарифум, только под логарифум, мы
же предположили, что все вероятности меньше, чем 2
в минус эпсалон n, значит, под логарифум стоит величина
меньше, чем 2 в скепени минус эпсалон n, то есть логарифум
меньше, чем минус эпсалон n, успеваете, да, но он со
знаком минус, поэтому все получается строго больше,
чем эпсалон n на сумму вероятностей, ну, сумма вероятностей
равна единице.
Вот это вот, это просто один противоречие с тем, что
мы здесь вот имеем.
То есть это очевидное неравенство вот это вот, но я подробно
про это поясню.
Итак, существует конкретный набор значений, которые
случайный вектор принимает с сравнительно большой
вероятностью, больше либо равны 2 в степени минус
эпсалон n.
Вот сейчас будет 1 минус эпсалон, смотрите, тут написать
нет, тут написать нет, тут, ну, знаете, вот тут нормально.
Определение пока стирать не буду, оно может пригодиться
попозже, а вот это можно, можно стереть, сейчас отчертим
верхнюю часть доски используем, что из этого следует.
Смотрите, ну, что такое вероятность в нашем-то случае.
У нас же раскраска-то случайная, в смысле, что каждый цвет
присваивается независимо от другого, с вероятностью
одна-вторая, но если это же вероятность множества
каких-то раскрасок, правда, ну, то есть в знаменателе
там стоит 2 в степени n в определении этой вероятности.
Давайте перейдем обратно к количествам, от вероятностей
к количествам.
Если вероятность равномерная, это классическая вероятность
чего-то больше либо равна, значит тех объектов, которые
считает эта вероятность, не меньше, чем вот это число
умножить на 2 в степени, то есть получается, что существует
не менее 2 в степени, вот, наконец, 1 минус епсилон
на n раскрасок, для которых вектор b1 и так далее bn принимает
одно и то же значение, вот какой вывод.
А раскраски, напоминаю, это векторы из плюс-минус
единиц размерности n, раскраска это вектор с плюс-минус единиц
размерности n, и вот у нас есть огромное на самом деле
множество раскрасок в каком-то смысле, ну, как бы почти
все не совсем корректно так говорить, но и 3 на 10
минус 20 всего раскрасок 2 в n степени, а тут их 2 в
степени 1 минус вот эта фигня умножить на n, то есть очень-очень
жирное множество векторов из плюс-минус единиц, на
которых при этом все вот эти значения, я их специально
не стирал, одинаковые, на каждом из которых все эти
значения одинаковые, понятно я говорю?
Может быть вот теперь уже, может быть кто-то понимает
к чему дело идет, не знаю, я неоднократно в рамках
этого двухгодичного курса рассказывал про теорию
кодирования, помните, нет, на множестве из векторов,
которые состоят из двух координат, но обычно это
были нолики и единички, но можно и плюс-минус один,
можно ввести то, что называется хеммингового расстояния,
по идее вы должны помнить, что это такое количество
несовпадающих координат, вот давайте его введем, я
сейчас вот здесь сотру, это уже ничего не нужно, давайте
снабдим 2 в степени n последовательностей расстоянием хемминга,
но это все поняли, это я писать не буду, теперь смотрите,
вот допустим, сейчас будет некий факт без доказательства,
но интуитивно понятный, я его оставлю просто без
доказательства, много сил не тратит ваше, допустим,
есть какое-то множество, зовем его а, раскрасок, ну
или что то же самое векторов из плюс-минус единиц, векторов
из плюс-минус единиц, есть какое-то множество, в котором
любые две точки отстоят друг от друга, отстоят друг
от друга на расстояние хемминга, который мы обсуждали,
на расстояние хемминга, не превосходящее ну скажем
какого-нибудь d, ну вот мы знаем про некоторое множество
раскрасок, что каждые две раскраски в нем, как вектор
из плюс-минус единиц, находятся друг от друга на хемминговом
расстоянии не больше чем d, как вы думаете, как можно
тогда мощность а оценить, чем можно оценить мощность
а, то есть по-другому говоря, если дан диаметр множества,
ну вот эта вот граница, это граница на диаметр множества,
максимальное расстояние между точками в нем, то
чего объем этого множества, ну объем в комбинаторном
смысле, мощности, не превосходит заведомо, какое множество
данного диаметра самое жирное, самое объемное, а, нет,
ну два в степени это много, какое множество самое объемное
все-таки еще раз, вот в обычном пространстве, как вы думаете,
какое множество самое объемное, если данного диаметра, ну
шар наверное, интуитивно это должно быть понятно,
давайте не будем с вас требовать доказательства этого, в комбинаторном
случае это не очень сложно, это можно сделать, но я не хочу
тратить ваше время и свое тоже, давайте поверим в интуитивно
понятную вещь, что если есть какое-то множество с диаметром
не большим, чем d, то мощность этого множества не больше,
чем мощность шара с диаметром d, я утверждаю, что тогда
мощность не больше, чем мощность объема, это не важно,
но комбинаторная это мощность, мощность шара, ну давайте я скажу
радиус d попала, радиус d попала, так, дорогие товарищи, вы понимаете,
что такое мощность шара в комбинаторном смысле, для плюс-минус единичных
векторов радиуса d попала, что такое шар, это множество
точек, которые от данной отстоят на не более, чем такое расстояние,
ну какая разница от какой данной, считайте, например, что она из
одних нульней или там из одних плюс единиц, сколько точек
отстоят от нее на расстояние ноль, одна, сколько точек отстоит
от нее на расстояние один, c и z по одному, надо одну координату поменять,
сумма c-шек до вот этой в той сумме, да, то есть это равно просто сумма по
какому-нибудь там k от нуля до d пополам, ну или до целой части d пополам,
если вдруг не целое число, но понятно, до целой части d пополам, c и z по k,
сейчас я достаточно понятен, я не спешу, все успевают,
тут я проглотил доказательства на идее ясна, а эту, ну я надеюсь, понятно,
сколько векторов отличается в нуле координата, в одной, в д пополам координата, такая вот сумма,
так, смотрите, что отсюда следует, у нас есть сейчас множество, которое состоит из больше или
не меньше, чем два в степени 1 минус эпсалон на n раскрасок векторов,
так, что стало быть можно сказать про его диаметр, исходя вот из этого рассуждения,
ну давайте обозначим это множество тоже как-нибудь буквой а, например, а это вот множество
состоящее из не менее чем два в степени 1 минус эпсалон на n раскрасок, есть такое множество,
как можно оценить его диаметр, наверное, чем-то достаточно большим можно оценить снизу,
вот давайте подумаем, как его можно оценить снизу, это, конечно, вот как бы некая проверка на то,
насколько вы все-таки осознали науку про асимптотики цешек, с этого начинался первый
семестр дискретного анализа, ну понятно, что если вот эта величина d пополам больше,
чем n пополам, то все плохо, потому что эта сумма зачерпывает k равный n пополам в том числе,
это очень плохо, а вот если d пополам лежит строго ниже серединки суммирования,
ну ниже чем n пополам, тогда вот эта сумма просто возрастающих величины,
с которых последняя самая большая, так, ну сейчас я попробую как-то по-понятней объяснить,
так, как бы это по-понятней объяснить, как бы это по-понятней объяснить, может быть,
прямо сразу на вас все вот этот катарсис-то выплеснуть, да нет, ну это катарсис, понимаете,
где ж 10-9, где 10-9, откуда она полезет, 10-9, вот она сейчас полезет, в оценке идеальна,
ну кто мне там еще бы сгумал звонить, не знаю, так, ну что, большой мощный,
но смотрите, вот идея-то какая, я просто хочу, чтобы вы идею послушали, а потом уже я на вас
выплеснул 10-9, идея-то какая, нам нужно подобрать вот это d пополам в виде 1 вторая, 1 минус там
какой-нибудь дельта, но вот это и будет 10-9, я все равно раскрываю карты уже, ну тут деваться некуда,
на n, вот если мы вот эту верхнюю границу выберем так, чтобы асимптотика логарифма вот этой сумм
совпадала с асимптотикой логарифмы или была там больше, ни как меньше, чем асимптотика вот
этого логарифма, то мы придем к оценке идеали, ну ладно, давайте, вот пусть d пополам имеет
какой-то вот такой вид, пока что дельта это не 10 минут 9, а что хотите, вот имеет какой-то такой
дельта, это какое-то фиксированное число, давайте подумаем вместе, как устроена асимптотика логарифма
вот этой суммы, но мы с вами умеем считать c и z по альфы n, помните, что мы умеем такое
дело считать, самое начало первого семестра второго курса, между прочим, там тоже слово
энтропия появлялось в другом контексте, но просто сама оценка выглядит так же, как энтропия,
там получается вот так, один поделить на альфа в степени альфа, один минус альфа в степени 1
минус альфа, плюс там какое-то маленькое от единицы и все это в n степени, но то есть по
другому, это можно переписать вот так, это 2 в степени минус альфа лог 2 ичный альфа,
минус 1 минус альфа лог 2 ичный 1 минус альфа, но это прямо h от альфа, ну так, если что, я говорю,
что та энтропия, которую мы оценивали, нам уже не нужна, но здесь чудесным образом другая энтропия
появилась, фиг бы с ней, но просто вот такое вот выражение, и дальше вот это все, ну, наверное,
надо умножить на 1 плюс о малое от единицы и еще на n, вот так, то же самое абсолютно, написать
выражение вот в таком виде или сэкспоненцировать его, прологарифмировав то, что стоит под знаком
с этим экспонентом, так, у кого-то вот это вызывает сложности, все уже понимают, да,
так вызывает, то есть показатель вот этой экспонента асимпатически равен тому, что тут
напитка n и тут у нас тоже n, но тут 1 минус эпсилон, а тут вот эта вот энтропия от альфы,
ну, от распределения, которое принимает два значения, альфа и минуса, неважно, с вероятностями,
альфа и минуса, так, никого не кохнуло, понятно, что происходит, теперь смотрите, ну, все предыдущие
слагания имеют только меньшую асимптотику, чем вот это, поэтому, когда мы суммируем с точностью до
выбора вот этой функции, которая стремится к нулю, получится все равно вот это выражение,
если дельта меньше строго, вернее, больше строго нуля, если вот это строго меньше 1 минус дельта,
то асимптотика лагеризма сосредоточена в максимальном слагаемом, ну, почему,
давайте я по-другому скажу, есть максимальная слагаемая, а всего слагаемых меньше, чем n,
ну, значит, вся сумма меньше, чем n умножить на максимальная слагаемая, и больше в то же
время, чем максимальная слагаемая, но если максимальная слагаемая, по сути, экспоненциальная,
то и вся сумма будет, по сути, экспоненциальная, потому что она сверху оценивается вот этим,
умноженным на n, а снизу просто вот этим, нормально объясню? Точно? Я понял.
Асимптотика лагеризма не меняется. Вся, конечно, меняется, на нас интересует только 1 плюс 1 от 1
в показателе. Вся меняется, конечно, на n все-таки домножение может случиться, но домножение на n
не меняет асимптотику лагеризма, поэтому вот это равенство справедливо не только для последнего
слагаемого, но для всей суммы тоже, в смысле выбора вот этого маленького от 1, это в точности утверждение
о том, что асимптотика лагеризма этой суммы это вот выражение, стоящее сейчас в показателе двойки
в степи. Все, вроде объяснил, да? Теперь, смотрите, что получается? Получается, если вот это вот выражение,
ну альфа это у нас вот это вот альфа, это вот альфа, альфа это на самом деле что-то
зависящее от дельта. Если окажется, что это выражение меньше, чем 1 минус эпсилон, строго меньше,
чем 1 минус эпсилон, то что у нас получится? У нас получится как бы противоречие, да?
У нас получится противоречие. С одной стороны, мы знаем, что мощность а большая больше либо равна вот
это, а с другой стороны, предполагая, что диаметр не превосходит вот такой вот удвоенной величины,
мы получаем противоречащее неравенство при больших n. Нормально, четко объясняю. То есть,
получается, что если мы выбираем максимальная дельта, вернее, минимальная дельта, минимальная
дельта, при котором вот это выражение меньше, чем 1 минус эпсилон, то мы получаем, что диаметр
множества а не превосходит вот этой величины. Больше либо равен вот этой величине, извините.
Знаете, еще раз, если у множества диаметр не больше, чем d, то его мощность не больше,
чем вот столько, тогда она будет меньше, чем столько, сколько у нас есть на самом деле. И это
будет противоречием, означающим, что на самом деле диаметр множества больше, чем вот эта величина.
Вот утверждается, что в качестве такого дельта и надо взять пресловутые 10 минус 9. Если вы возьмете
в качестве дельты 10 минус 9, то вот это выражение, показатели двойки, будет строго меньше, чем 1 минус
3 на 10 минус 20. Но опять, можете на калькуляторе или на компьютере подставить и проверить. Просто
тупо подставляете 10 минус 9 вот сюда, получаете альфа, считаете вот эту штуку для такого альфа и
убеждаетесь, что она меньше, чем 1 минус 3 на 10 минус 20. Откуда следует, что диаметр
множества больше либо равен, или даже строго больше, вот этой вот величины. Ибо,
будь он меньше, было бы меньше векторов, чем у нас есть на самом деле. Сумел объяснить?
Это место как-то трудно, просто по-моему, знаю воспринимается, поэтому я стараюсь его подробно.
Ну, нам не важно, что такое один плюс во маленькое. Если константа строго меньше, чем 1 минус
Эпсуум, то рано или поздно оценки вступят в противоречие друг с дружкой. А нас интересует
только Н, стремящийся к бесконечному. Для больших Н, конечно, да. Ну да. Наверное,
это означает, что мне по-хорошему надо переформулировать, что существует такое нулевое,
что начинает с этого н-нулевого. Разница будет не больше, чем один дискорнейзен. Слушайте,
вот это хорошее замечание, да. Поскольку здесь присутствует асимптотика, мы вынуждены говорить
о том, что N достаточно велико. Поправьте это в формулировке, извините, да, это я не подумал.
Действительно, в этом месте возникает какое-то н-нулевое, начиная с которого это точно верно.
Какое, там, посчитаешь. Так, все, нормально. Вот. Ну, что из этого следует-то? Из этого следует,
раз диаметр большой, что существуют две раскраски хи1, хи2, принадлежащие множеству А, такие,
что значение вот этого вот вектора b1b на обеих одно и то же. Ну, я могу вот так, знаете,
b со стрелочкой от хи1 равно b со стрелочкой от хи2. Подставляю вот сюда хи1, подставляю
сюда хи2, мы получаем один и тот же набор значений, которые я старательно не стираю,
как определяются. Здесь написано, как они определяются. Это значение. Причем,
расстояние хэминга, как бы его назвать, ну хэм, например, между хи1 и хи2 больше
либо равняется вот этой вот величины. 1-10-9. Ну, то есть, они почти ни в одной координате не
совпадают. Совпадают на не более чем одной миллиардной доле от координат, от числа
координат. Вот эти две раскраски почти не совпадают. Тем не менее, у них одинаковые
значения вот этих векторов. Ну, что надо теперь сделать, чтобы получить обещанную частичную
раскраску из формулировки теоремы штрих? Кто соображает сходу? Как построить частичную
раскраску, в которой будут нолики, и этих ноликов будет мало? Очень легко сообразить.
Есть два вектора из плюс-минус единиц, две раскраски, которые очень мало где различаются.
Очень много где различаются, очень мало где совпадают.
Ну, а формулу не хотите предложить явную, без словок, ссор?
Ну, нет, это я понимаю. Нет, нет, это вот не совсем то. Формула другая. Надо взять полуразность.
Надо взять полуразности. Координаты, на которых они совпадают, занулятся. Но этих координат не
больше, чем одна миллиардная от общего числа координат. Ну, давайте я напишу,
раз вызвала все эти трудности. Сейчас вот тут напишу. Я думал, кто-то сразу скажет.
Вот такую взять штуку. Хи1 минус хи2 попала. Это будет не раскраска, но частичная раскраска.
Почти все, за исключением не более, чем 10 в минус 9 умножить на n координат у нее плюс или
минус единица, потому что столько было разнящихся координат у хи1 хи2. Но на не более, чем одной
миллиардной доли от общего числа координат они совпадают. Значит, при вычитании получается ноль,
после деления пополам тоже получается ноль. А там, где они различаются, ну да, вычитаем,
получаем либо минус единица, либо минус двойку, потом делим пополам, либо плюс двойку, потом делим
пополам. Так, теперь вопрос. Почему уклонение такое, как мы хотели? Помните, я утверждал,
что на каждом множестве разница, ну модуль разности между красными и синими вершинами,
не превосходит 10 корней из n. Это вот ровно из этого построения. Потому что у них одинаковые
значения на все хомытые. Вот у этих хи1 и хи2. Ну, допустим, скажем, значение лежит вот на этом
отрезке, на этом полуинтервале. У каждой из них лежит на этом полуинтервале. Значит,
чего модуль полуразности не превосходит? Ну как раз 10 корней из n, потому что модуль разности не
превосходит 20 корней из n, а после деления на 2 – 10 корней из n. То же самое здесь. Опять,
они отличаются друг от друга реально. Не больше, чем на 20 корней из n, после деления на 2 – не
больше, чем на 10 корней из n. Все. На любом интервале. Так, длины 20 корней из n.
Понятно, да? Ну, по-моему, это такое очень продвинутое, красивое решение. Идейно,
не тривиально, идейно. Мне казалось, что вот очень полезно. Все-таки ты осознаешь,
антропия так работает в комбинаторике. Оказывается, что она нужна не только для физики или для каких-то
вероятностных нужд, но в комбинаторике она тоже является удобным инструментом для получения таких
вот, в итоге, мощностных оценок. Например, мощность какого-то множества большая. Дальше уже вступает в
силу такая вот комбинаторика теории кодировки. Если мощность большая, то диаметр, например,
большой. А раз диаметр большой, значит, есть два объекта, которые сильно разнятся. Возьмем
полуразность, получим объект, который почти без нули. И при этом, вот видите, значения совпадают.
Ну, то есть, и 1 от m, и и 2 от m отстанет друг от друга не больше, чем на 20 корней из n. Значит,
здесь вот уклонение быть не больше 10 корней из n на каждом.

так соответственно дам сегодня новая лекция сегодня мы наконец-то переходим от оптимизации на
безусловно задачи оптимизации когда мы минимизировали нашу функцию на всем
пространстве rd теперь мы соответственно переходим в случае когда у нас наше множество
на котором мы можем оптимизировать переменные оно ограничено соответственно да вот та же
самая целевая функция которая нам нужно было минимизировать ну и соответственно теперь
появляется какое-то множество x на котором соответственно мы должны эту функцию минимизировать
раньше у нас было такое что у нас вот это множество x просто равнялась rd это была задача
безусловной оптимизации теперь мы соответственно рассматриваем что в данном случае это множество x
некоторое выпуклое множество вот и как вы понимаете в связи с тем что у нас теперь задача условная то
есть теперь минимум наш глобальный минимум на всем пространстве rd он может не совпадать с тем
минимумом который содержится на множестве x соответственно здесь изображена чашка на
картинке вот этот центр чашки может быть за пределами вот этого тут многоугольника на котором
мы и оптимизируем нашу функцию вот поэтому да с этим придется иметь дело вот мы соответственно
будем смотреть как начнем смотреть сегодня как можно уже вот с этими множествами x взаимодействовать
вот вот такой вопрос зачем вообще и появляются те ограничения и какая у них часто бывает суть
может они вообще и не нужны мизировали бы и минимизировали на обычном множестве
правильно ну то есть действительно задачи оптимизации во многом это какие-то физические
задачи одно из применений это физические задачи экономические задачи соответственно у вас есть
какие-то ограничения которые просто возникают в реальной жизни то есть ограничений какие-то на
скорость ограничения какие-то на ресурсы просто у вас нету бесконечного числа денег хотя хотелось
бы наверное вот нет у вас какого-то бесконечного числа древесины ну вот соответственно вы это
все ограничивайте появляются какие-то ограничения как-то учитываете ограничение хотите там
максимизировать свою прибыль минимизировать свои издержки и так далее вот более того часто
Часто, например, ограничения возникают не совсем естественным путем, их можно вести дополнительно,
просто чтобы в некотором смысле улучшить задачу или улучшить в некотором, опять же, смысле решение.
Если сегодня успеем, я покажу примерчик, как, например, в машинном обучении можно использовать дополнительные хитрые ограничения,
чтобы качество решения было в некотором смысле лучше.
Ну, опять же, посмотрим, расскажу, почему оно лучше и почему, соответственно, нужны были эти ограничения.
Ну, пока, соответственно, разбираемся с общим случаем. Целевая функция множество, выпукло множество, выпукла целевая функция.
Поехали. Соответственно, да, это мы с вами в некотором смысле уже обсуждали.
Условие оптимальности как раз нашей задачи, но оно уже не в безусловном случае.
В безусловном случае я напоминаю, что у нас просто градиент равнялся нулю.
Вот. Сила того, что теперь у нас просто глобальный минимум может не лежать во множестве х,
ну, который глобальный минимум на rd, вот.
Градиент, поэтому он в оптимуме, именно на х, он может быть и не давать ноль-ноль градиента, вот.
Соответственно, справедливо вот такое вот условие. На лекции второй я давал его без доказательства, просто его показал.
Вот. Сейчас мы его, соответственно, докажем, потому что оно нам будет нужно.
Нужно. Вот. Ну, вот такое вот условие.
Давайте доказывать. Давайте доказывать.
Ну, давайте в одну сторону. Достаточное условие. Что это значит? Достаточность условия.
Да, да. То есть в достаточном условии, что если мы предположим, что у нас вот выполнено вот это вот, соответственно, условие,
записываем его, там, больше либо равно нуля, то из этого должно следовать, что у нас х звездой глобальный минимум на х,
ну, понятно, функции f. Вот. Окей. Давайте сделаем все равно так же, как мы делали с вами в случае безусловной задачи,
ну, достаточное условие. Что мы там записали? Мы там записали просто выпуклость.
Можно, например, оценить значение функции f.x через выпуклость.
Через выпуклость. Через определение выпуклости. Что там будет?
Ну, вот что-то вот такое.
Окей. Ну, это просто определение выпуклости. Вот. Как раз через, когда у нас функция непрерывно дифференцируема.
Про это мы только что знаем, что в силу того, что мы доказываем достаточные условия, мы предположили, что это больше либо равно нуля,
поэтому второе слагаемое здесь больше либо равно нуля, и, соответственно, что мы можем сказать, что вот то, что написано здесь,
оно больше либо равно очень просто значения функции в точке х звездой, но это значит, что как раз по определению у нас х звездой глобальный минимум функции f на х.
Все. Ну, то есть тут все просто. Ничего такого сверхъестественного мы тут и говорим.
Дальше, соответственно, что? Хочется доказать необходимые условия, то есть в обратную сторону мы предполагаем, что у нас х является глобальным минимумом функции f на множестве х.
Вот. И хотим, соответственно, доказать, что будет выполнено вот это условие, которое написано выше. В том числе вот такое условие.
Пойдем от противного. Вот противного. Вот. И рассмотрим, соответственно, скажем, что у нас существует, существует некоторая точка, ну пусть будет х с тильдой, вот, из множества х нашего.
Такая, что вот у нас вот это выражение, вот это скалярное произведение х со звездой х тильда х минус х со звездой меньше нуля строго, меньше нуля строго, вот, для некоторой точки х с тильдой.
Вот. Рассмотрим семейство точек, как мы в принципе с вами уже и делали, которые параметризуются параметром лямбда, вот, в следующем виде, в следующем виде, лямбда х со звездой плюс 1 минус лямбда х с тильдой.
Что мы можем сказать про х лямбда для любого лямбда от нуля до одного? Да, она лежит в х просто потому, что у нас что? Х выпукло множество, а просто по определению выпуклого множества любая линейная комбинация его точек, вот, она лежит в этом множестве.
Соответственно, мы можем сказать, что х лямбда принадлежит множеству х. Окей, давайте посмотрим, как соответственно ведет себя, вот, соответственно, посмотрим, как ведет себя функция, которая зависит от вот этого х лямбда.
Вот, тут х тильдой, только я уже делал с тильдой. Вот, как ведет себя функция, вот такая вот, в которой мы подставили значение нашей точки. Вот просто вот эту точку, семейство точек в зависимости от лямбда, они все принадлежат множеству х.
Вот. Заметим, что производная вот этой функции по лямбда равна следующему выражению. Я надеюсь, вы понимаете, откуда это берется. Вы, скорее всего, в первом семинаре, когда вы дифференцировали по матрице, там в том числе было упражнение дифференцирование по сколяру.
Вот. И, соответственно, вылезает вот такое вот выражение. Вот такое вот выражение. Производная нашей функции phi от лямбда по лямбда. По лямбда получается вот такое вот сколярное произведение.
Что мы о нем можем сказать, например, при лямбда равном нулю? При лямбда равном нулю, что мы можем сказать об этом сколярном произведении?
Оно меньше нуля, согласны? Просто потому что мы предположили, что существует точка х с тильдой, что будет выполнено, что вот это сколярное произведение меньше нуля.
Вот. А значит, что в окрестности нуля именно функция phi от лямбда, она что в окрестности нуля? Ну, если производная равна меньше нуля, то что в окрестности нуля с ней происходит?
Оно убывает. Убывает в некоторой окрестности нуля. Вот. Ну, значит, если оно убывает в некоторой окрестности нуля, то и функция f, которая вот зависит от х лямбда, убывает в некоторой окрестности лямбда.
Вот. А значит, что у нас получается f от х со звездой будет больше, больше, чем некоторый f от х лямбда для некоторого малого лямбда?
Так? Согласны? Функция убывает вдоль этого направления некоторой окрестности нуля. Ну, направление, соответственно, задается вектором х тильдой минус х со звездой.
Вот. Ну и, соответственно, получается, что и значение функции в некоторые точки, а мы знаем, что вот эта точка, она у нас лежит во множестве х, просто потому что мы ее так определили.
Ну, а значит, пришли к противоречию, противоречию, противоречию того, что у нас х со звездой – это глобальный минимум функции f на х. Окей?
Все. Значит, условия доказаны в обе стороны. И необходимо, и достаточно всего доказано. Вот. Хорошо. Здесь это все та-та-та.
Хорошо. Смотрите, в чем вопрос вообще. Появилось множество х. Соответственно, нам нужно рискать решение на нем. Запустим, например, обычный градиентный спуск.
Вот. Он может уйти куда угодно. Просто глобальное решение в другом месте находится абсолютно далеко от множества х, которое мы рассматриваем. Вот. И, соответственно, мы туда и упремся.
Как дешево, в некотором смысле, адаптировать, например, тот же градиентный спуск под уже условную задачу оптимизации?
Штраф. Кстати, это классная идея. Мы это будем обсуждать через четыре лекции. Метод штрафов – так называемый тоже идея Нестеров-Немировский.
Супер классный метод. Справедливости ради, вот как раз, в некотором смысле, Нестеров впервые стал популярен из-за метода штрафа. Ну, барьеров. Там есть и барьеры, и штрафы. Разные вещи.
Ну, суть похожая. Вот. Штрафовать, когда вы приближаетесь к границе. Вот. А как раз Нестеров стал популярен не с ускоренным методом, потому что пока не находилось для него применения.
А вот метод барьеров и штрафов. Вот. Это как раз то, на чем он в некотором смысле стал известен во всем мире.
Ладно. Окей. Штрафы. Поняли. Просто добавляем штраф. Говорим, что если мы приближаемся к границе, добавляем, например, какую-нибудь там функцию, которая на бесконечности, просто за пределами ноль, что она бесконечности, она не дифференцируем, она не очень хорошая.
Но можно как-то эту бесконечность в некотором смысле апроксимировать. Ну, вот как-то вот в духе того, что ставить логарифмические барьеры. Ну, это мы все обсудим. Вот. Чтобы функция просто рядом с границей множества уходила в бесконечность.
Окей. Что еще можно делать? Что еще можно делать? Сегодня просто не про это разговор. Вот. А какие еще есть идеи? Ну, вот. Сделали мы шаг градиентного спуска. Вышли за пределы множества. Что можно сделать?
Уменьшить шаг – вариант. Ну, а если мы, соответственно, уже где-то очень близко к границе. Вот. И уменьшение шага, оно, особенно в экспоненциальное число рас, оно, видите, сходимость сильно уменьшит.
Потому что вы знаете, что там 1 делить на k будет сильно делать из линии на сходимость и сублинии. Вот. А экспоненциальное уменьшение шага, оно, конечно, так. Будет еще жестче. Вот.
Ну, ладно. Смотрите. Просполирую идею. Давайте просто возвращать точку. Вышли градиентным спуском за пределы множества. Давайте просто вернем нас в это множество. Вот.
Для этого используется оператор проекции. Для этого используется оператор проекции. Записывается он следующим образом. Соответственно, это просто некоторый аргуминимум.
И мы ищем ближайшую точку. То есть, хотим испроектировать, например, точку y на наше множество x. И мы ищем ближайшую точку нашего множества x к этой точке y.
Ну, опять же, исходя из Евклидова расстояния. Вот. Евклидова проекция выглядит следующим образом. Понятно, что можно проецироваться не только так. Вот.
Но вот, соответственно, сегодня рассмотрим Евклидову проекцию. Ближайшая точка с точки зрения Евклидового расстояния к нашему множеству.
Идея очень простая. Раз выходим за множество, давайте как только вышли сразу же у нас возвращать в него. Вот. Из-за его предела не выходить.
Ну и в связи с этим градиентный спуск модифицируется довольно просто. К шагу просто добавляется этот оператор проекции. Вот. Вышли, зашли обратно.
То есть, всегда x каты у нас будет в пределах множества. Хорошо. Давайте подоказываем какие-то свойства для этого оператора проекции.
Просто потому что нам нужно понять вообще, будет это к чему-то сходиться или нет. Вот. Может быть он нам даст какие-то плохие артефакты из-за того, что мы проецируемся.
Ну и ни к чему мы в итоге не сойдемся. Вот. Во-первых, важное свойство, то что оператор проекции у нас существует.
И при этом еще и единственен. При этом он еще и единственен. Вот. Довольно простое свойство.
А следует оно из того, что у нас задача проекции, она какая? Какая целевая функция там по свойствам?
Что вы про нее можете сказать? Она выпукла вообще или нет? Вот эта целевая функция, когда мы проецируемся. Вот эта.
Выпукла. Более того, она какая еще? Она сильно выпукла, да? А про сильно выпуклые задачи оптимизации мы знаем, что у них решение существует единственно.
Вот. Уникальное, единственное решение. Поэтому, соответственно, оператор проекции всегда существует и всегда единственен.
Хорошее свойство. Следующее свойство выглядит немного необычно, но оно в некотором смысле вспомогательное.
Вспомогательное свойство, которое нам понадобится, чтобы доказать следующее свойство.
Ну давайте вот это сначала докажем. Здесь, что у нас множество х выпуклое.
Точка х берется из множества х обязательно, а вот точка у из любого множества.
Вот. Ну просто из Rd. Соответственно, справедливо вот такое вот выражение.
Хорошо, давайте попытаемся доказать. А доказывать мы будем с помощью вот этого условия, которое мы с вами только что доказали до этого.
Условия оптимальности. Условия оптимальности для выпуклых функций на выпуклом множестве.
Так, оператор проекции ведь тоже представляет собой решение задачи минимизации.
Довольно простой, на самом деле, задачи минимизации на выпуклом множестве х.
Вот. И, соответственно, для него тоже будет справедливо вот это свойство. Согласны?
Вот. У нас какая задача? Мы минимизируем нашу целевую функцию d.
Ну, например, от y, а где d представляет собой y-x. Ну, где x у нас точка.
Или лучше наоборот. Давайте наоборот.
Y у нас оно за пределами как раз.
Так. y-x в квадрате.
Вот такую задачу мы минимизировали на нашем множестве х.
Соответственно, будет справедливо абсолютно это же условие оптимальности.
Так.
Здесь давайте я поставлю.
Давайте вот так вот сделаем. Здесь z пусть будет у меня.
Чтобы переменные неодинаковые были.
Вот. Соответственно, да, здесь у меня этот x произвольный из x.
Чему равен градиент? Градиент d. Чему он равен?
Чему равен градиент d?
Квадратичная задача. Чему равен градиент?
2y-z. Кто согласен, кто не согласен?
Минус, конечно, нужно добавить.
Потому что вы когда же вы дифференцируете квадрат сначала, потом внутреннюю функцию еще.
Из внутренней функции по z же вы дифференцируете. Там еще минус вылезет.
Конечно. Получится вот так. z-y.
Ну, это на самом деле эквивалент к тому, что вы просто вот так задачку перепишете.
Квадратную развернуть же можно. Выражение под квадратом.
2y-z-y.
Вот. Ну и соответственно, что записываем?
Записываем, что градиент 2y. Давайте 2y за сразу скалярное произведение.
Здесь что будет? x-звездой.
Минус y.
Здесь будет x.
Минус x-звездой.
Так?
Вот.
А давайте вспомним, что же такое x-звездой для нашей задачи?
x-звездой. Это чему равен для нашей задачи?
Проекция точки y.
Проекция точки y. Мы же ее искали. Это решение.
Вот. Ну и все. Подставляем сюда.
Я двоечку сразу сокращу, потому что она там...
Знак больше либо равно 0. На число разделю.
Что у нас получается? Проекция точки y.
Минус y.
x, который из множества x.
Вот. И проекция точки y.
Больше либо равно 0. Но это же ровно то, что нам нужно и было доказать.
Согласны?
Вот.
Оно и написано сверху. Просто там поменено местами.
В первом члене поменено местами y.
Минус проекции y поменено местами.
Соответственно, вот так. Одно и то же.
Вот. Все. Доказательство закончено.
Просто через условия оптимальности мы к этому пришли.
Окей. Окей.
Хорошо. И вот с помощью этого условия оптимальности,
вот этого вспомогательного утверждения,
я хочу доказать вот такое вот свойство.
Оно называется по-английски non-expansiveness.
По-русски, наверное, это как-то переводится как нерасширяемость.
Вот. Оператора.
Оператор проекции у вас не расширяет разность между x.
То есть были у вас две точки какие-то x1 и x2.
Если вы их спроекцируете на множество, то расстояние либо не изменится,
либо еще уменьшится между ними. Вот.
Ну, еще как это можно сказать? Ну, липшицевость.
Один липшицевость оператора.
Ну, хорошее свойство.
Давайте его как раз и докажем с помощью вот этого вспомогательной вещи,
которая у нас тут и была.
Давайте я перепишу ее сюда.
Ой, да-да-да. Здесь произвольные.
Здесь произвольные x. Спасибо.
Здесь, конечно, для произвольных действий.
Там для x понятно, что там просто это эквивалентные вещи.
Да, спасибо. Спасибо.
Здесь, соответственно, произвольные x.
Вот.
Перепишу свойство со слайда или...
Ладно, может быть его...
Вот.
А в таком виде я тогда его оставлю здесь сразу.
Пусть оно вот будет.
Это просто свойство с предыдущего слайда.
Здесь как раз x берется.
x берется из x, вот этот.
А x1 и x2 как раз они произволен.
Здесь пока только x1 есть.
Вот. Это просто свойство с предыдущего слайда.
Для любого x и x1 вот оно будет справедливо.
И x соответственно у нас принадлежит множеству.
Вот. В качестве x давайте я возьму проекцию точки x2.
Точки x2.
И получу x1-проекция x1.
Проекция x2-проекция x1.
Это меньше либо равно 0.
Вот. Аналогичным образом, просто поменяя переменные местами,
я могу получить, что x2-проекция x2.
Проекция x1-проекция x2.
Меньше либо равна 0.
Вот. Меньше либо равна 0.
Теперь я вот эти два кусочка складываю.
Складываю.
Ну давайте что-нибудь получим из этого.
Сложим и получим.
Вот. Что тут получается?
x1-проекция x1.
Дальше, соответственно, минус x2 плюс проекция x2.
И здесь общая будет проекция x2-проекция x1.
Меньше либо равно 0.
Согласны?
Сложили. Сложили. Вроде все хорошо.
Вот. Дальше я чуть-чуть сгруппирую это все безобразие
следующим образом, следующим образом.
Справа оставлю, то есть вот только вот эти члены
с проекцией, вот.
А все остальное перенесу влево.
Это справа, ой, слева наоборот осталось, слева осталось.
А вправо у меня ушло, соответственно, что там будет?
x2-x1-проекция x2-проекция x1.
Ну, что теперь у меня слева стоит?
Что у меня слева стоит?
Квадрат просто, да, стоит слева, слева стоит просто квадрат.
Так.
Как раз.
Во, докрутил до этого.
Слева у меня стоит квадрат.
Ну, а справа вот это вот скалярное произведение.
Вот.
Что я могу применить к сексуальности?
Вот.
Что я могу применить к сексуально-скалярному произведению,
чтобы его как-то оценить через нормы?
КБШ.
Соответственно, применяю КБШ.
Вот.
И что у меня здесь получается?
Разность проекций.
И разность x.
Ну и все, готово.
Видно, что один у меня квадрат тут сокращается,
и это уходит, и ровно то, что нам нужно было доказать.
Вот.
Получается, что действительно оператор проекции
только может уменьшать расстояние,
ну, не увеличивать расстояние между точками.
Между точками.
Вот.
Хорошее свойство.
Последнее свойство, которое нам понадобится.
Вот такое вот.
Оказывается, что вот оператор проекции,
у оператора проекции, ну, вот в некотором смысле есть такая,
что ли, у градиентного спуска с оператором проекции
есть стационарная точка.
Угу.
Ну, такое вот свойство.
Ну, вот пусть если у нас там градиентный спуск
это оператор, с проекцией это оператор t,
ну, у него засыпихивана точка x звездой,
ну, оказывается, она отображается в точку x звездой.
Вот.
Ну, грубо говоря, то, что если вы нашли уже оптимальную точку
на вашем множестве x, вот,
и пытаетесь сделать градиентный шаг,
вы уже никуда, понятно, не уйдете.
Вот.
Вы просто пытаетесь выйти за пределы множества
и вас возвращают обратно.
Что логично.
Что просто минимума больше нигде нет.
Вам градиент показывает на направлении убывания,
вы по нему пытаетесь идти,
но это вас сразу же уводит за пределы множества.
В пределах множества ничего такого хорошего и нету.
Ну, давайте это формально докажем.
Формально докажем.
Следующим образом.
Так как у нас оператор проекции,
вот этого x звездой
минус гамма f от x звездой,
это же просто аргуминимум,
поэтому давайте, ну, так и запишем.
Аргуминимум, вот, x
из некоторого множества x,
x минус x звездой
минус гамма
f от x звездой
в квадрате.
Окей.
Давайте чуть-чуть поиграемся
с выражением под аргуминимум,
я его просто пораскрываю.
Этот квадрат, соответственно, у меня там были с квадрат,
я его раскрою.
В квадрате.
Минус...
Что там будет сейчас получаться?
Минус...
А, тут плюс.
Минус был, состав плюс.
Вот.
Плюс 2 гамма f от x звездой.
x минус x звездой.
Скалярно.
Плюс гамма в квадрате
f от x звездой.
В квадрате.
Окей.
С точки зрения
вообще аргминимума,
кто понимает, мне вообще важно
вот этот последний член
с нормой градиента.
Именно аргминимума.
Он от x не зависит,
поэтому, ну,
именно значение,
когда я спрашиваю аргминимум,
мне же нужно просто вернуть значение x.
Вот. А когда я уж, конечно,
спрашиваю значение функции,
нужно знать, что вот какие-то дополнительные
константы, которые могут ее уменьшать или
увеличивать. Вот. Но именно с точки зрения
x, вот это нам вообще не важно.
То есть, он не влияет на значение
x. Вот. Поэтому
из аргминимума можно последний кусочек
выкинуть. Последний кусочек
выкинуть. А теперь скажите мне,
что вы можете сказать про
первый кусок и второй кусок?
Ну, про первый кусок вы можете сказать, что он
больше либо равен нуля точно, да?
Вот. А второй?
Второй.
Если гамма положительная.
Зачем уже оценивать? Если это же
ровно то, что у нас было в условии
оптимальности. В условии
оптимальности доказали, что вот это
оно у нас всегда какое?
Ну, это всегда какое?
Вот.
А оно всегда какое?
Больше либо равно нуля.
Получается, что вот то выражение,
которое у нас написано здесь, это сумма выражений,
которые больше либо равны нуля.
Так? То есть, ну, в лучшем
случае они равны нулю. Да?
И ноль, как вы понимаете, он
достижим. В какой точке?
Иксозвездой. Всё.
Конец.
Вот так.
Соответственно, получается, что действительно
у нас вот проекция подействует
на градиентный спуск с точки
иксозвездой. Она просто свернет точку иксозвездой.
Вот так вот.
Всё. С этим свойствами
покончили. Они вроде бы более-менее физичны.
То есть, особенно два последних. Два последних
нам в принципе и понадобится.
Для доказательства сходимости
градиентного спуска.
Для доказательства сходимости градиентного спуска.
Пум-пум-пум.
Так.
Окей. Поехали
подобраться с доказательством градиентного спуска.
Здесь я выписал просто
ровно то, как мы с вами начинали
доказательство обычного градиентного спуска
без проекции.
Смотрим расстояние
между икска.
Только тут вверх надо было.
Съехало. Ладно.
Икска. Смотрим расстояние между икска и
иксозвездой. Подставляем
икскатой.
Выражение для икска плюс один
в нашу расстояние
до решения.
И, соответственно, давайте что-нибудь с ним делать.
Давайте сначала
воспользуемся нашим
последним свойством.
Потому что мы знаем, что у нас
иксозвездой на самом деле
равен просто проекции.
Иксозвездой минус градиент.
Поэтому здесь я в некотором смысле
его заменю.
γк
fxk
проекция
минус проекция иксозвездой
минус γк
f иксозвездой
в квадрате.
Вот. А теперь
чем воспользуемся?
Так как у нас тут разность двух проекций.
Что можно сделать?
Пожалуйста.
Липшицевая, но экспансивность
этого оператора. То есть то, что он
не
увеличивает расстояние между точками,
которые стоят под проекциями.
Ну, я это здесь и выпишу.
Соответственно.
Ну, смотрите. Уже очень хорошее
выражение получается.
x как x минус x звездой.
Вот. Давайте расписывать
как раньше. Как раз расстояние
до решения в текущий момент времени.
Вот.
И плюс
что там будет выскакивать дополнительно?
xk в квадрате.
Тут соответственно выскочит как раз
разность градиентов.
И это, кстати, даже хорошо. Почему?
Потому что, помните, градиент на вспуске
у нас просто выскакивал градиент, да?
А нам нужно было воспользоваться
липшицевостью. Нам как раз нужно было
вот такое что-то, да?
Ну, мы просто искусственно
добавляли этот нулевой градиент
просто потому что у нас задача была
безусловная. В точке x звездой он был
нулевой, поэтому здесь добавили. Здесь мы его в некотором
смысле добавили.
Он уже не нулевой, но мы его добавили с помощью
вот этого свойства проекции.
Свойства проекции, что в точке
x звездой там все хорошо.
Окей. И плюс еще удвоенное скалярное
произведение. Там должно
остаться, ну, тут только со знака минус.
Здесь там еще f от
xкт минус
x звездой
умножить на xкт
минус x звездой.
Вот.
В принципе, уже очень близко
к тому, что мы имели в градиентном спуске.
Соответственно, вот это
мы уже знаем, что надо... Первый кусочек
мы знаем, что по липшицевости надо.
Вот этот кусочек по липшицевости.
Вот что-то вот такое
по, соответственно,
здесь, соответственно, чего?
l гладкость,
вот для этого
безобразие, мю
сильная выпуклость.
Единственное, что вот осталось вот это
скалярное произведение, неприятное, которое
завязано на f от x звездой.
Причем оно, видите, оно положительное.
Потому что...
А?
Где-где-где? Потому что я просто раскрыл квадрат.
Вот.
Сильная выпуклость, вот к этому применению,
то, что зелененьким выделил, сильная выпуклость.
И это гладкость, через гладкость расписывается.
Насталось неприятное скалярное произведение.
При этом оно положительное, потому что,
видите, минус на минус даст вам плюс.
Ну и, соответственно,
будет у вас градиент f от x звездой
на x ката минус x звездой.
А мы знаем, что из условия оптимальности
этот член, он положительный.
Вот. И нужно бы что-нибудь
с ним поделать.
На самом деле ничего страшного он вам не даст.
Если вы помните, что мы с вами, как мы с вами сейчас...
Так.
Вот.
Дошли до этого. Давайте
вы поможете расписать гладкость.
Вот здесь вот гладкость, что мы там с вами, как расписывали?
Как это можно оценить через гладкость?
Что там вылезало?
Кто помнит?
Что там вылезало из выражения
для гладкости?
Разность функций и как раз
это солярное произведение.
Что-то вот такое.
x ката минус f от x звездой
плюс, соответственно,
f от x звездой
минус, да, минус x
минус x звездой. Вот так.
Вот такое вот выражение
вылезало там.
Но в безусловном случае, в чем мы говорили?
Вот этот равен нулю в силу того,
что градиент равен нулю.
Мы так говорили, просто его уничтожали
в безусловном случае.
Здесь, соответственно, мы так сделать не можем.
Но, как вы понимаете,
тут в этом плане
все будет довольно хорошо,
потому что мы знаем, что вот этот кусочек
он у нас
положительный.
Давайте четко это распишу,
как это все можно аккуратно
уничтожить. Вот только это не надо стирать.
Теперь мы не можем уничтожить как раньше,
но давайте это оставим.
Во-первых, я знаю, что вот это выражение
оно у меня положительное
в силу того, что оно как раз оценивает
что-то положительное сверху.
На самом деле
для этого выражения даже есть специальное
обозначение, его называют
дивергенцией Брегмана в функции
порожденной функции f.
Это ровно его определение.
То есть наберутся две точки
x и y, и, соответственно,
если записать такое выражение,
получается дивергенция Брегмана.
Она положительная, ее можно использовать
в качестве расстояния. Здесь нам, в принципе,
достаточно того, что эта вещь
положительная.
Как ее использовать в качестве расстояния мы с вами будем
рассуждать через две лекции.
Пока вот так.
Просто у нас есть какое-то выражение, которое
положительное, не отрицательное.
Дивергенция Брегмана.
Смотрите,
а теперь давайте вот здесь,
для вот этих кусочков,
воспользуемся сильной выпуклостью.
Сильной выпуклостью.
Здесь у меня что-то тогда выскочит.
Два гаммака,
разной функции,
плюс, соответственно,
как раз сильная выпуклость.
И, соответственно, там еще останется
кусок,
который зависит
от скалярного произведения
в x звездой.
xk
минус x звездой.
Вот.
И вот здесь вот видно, что я вот это
могу переписать в каком виде.
Когда я сгруппирую вот это
и вот это.
Видно, что здесь вылезает тоже
дивергенция Брегмана.
Видно?
Как раз вот тут будет как бы
минус, здесь плюс, разные знаки,
разность функций.
И получается разность функций и минус скалярное
произведение. Поэтому здесь просто
вылезет дивергенция Брегмана.
Дивергенция Брегмана
xk
x звездой.
Ну, и здесь будет
все еще здесь.
Согласны?
Окей.
Окей.
Вот. Выскочилась дивергенция Брегмана.
Выскочила мюшка.
Ну, давайте теперь это все аккуратненько
схлопнем.
Так.
Ввели дивергенцию Брегмана.
Все, я вот здесь тоже самое сделал,
расписал это все аккуратно.
И здесь у меня вот вылезло такое выражение.
Вот. Ну и что мы там дальше
с вами делали, кто помнит?
Кто там нужно было сделать, чтобы
все хорошо получить?
Вот.
Да, подбирали γк.
Вот мы смотрели вот на это выражение,
потому что оно в принципе нам мешает.
Вот здесь вот как бы есть расстояние до решения.
Здесь есть как бы расстояние до решения на предыдущей
итерации. И видно, что они вроде как уменьшаются.
Причем там геометрической скоростью.
Вот. Нужно подобрать
γк, чтобы вот это просто ушло.
Вот. Но в силу того, что мы знаем, что
дивергенция Брегмана положительная,
проблем нет. Мы подбираем γк,
ровно так же, как раньше. Берем
γк меньше, чем 1 делить на L.
Вот. И вот это выражение
у нас сразу же уходит.
Так. Все.
Доказательство остается прежним.
Вот. Дальше уже можно
шаги ровно те же повторять, какие мы делали
градиент на спуске, запускали рекурсы,
получали оценку сходимости.
Так вот здесь вот нужно было чуть-чуть
поиграться в дивергенцию Брегмана.
На самом деле, в безусловном случае,
мы тоже как бы ее и рассматривали.
Просто там градиент равнялся нулю,
и дивергенция была в некотором смысле
усеченная такая, более приятная,
потому что там стояла разность функций.
Хорошо. Хорошо.
Вот. Соответственно, да,
теория именно сходимости для градиентного
спуска с проекции и без проекции, они
оценки абсолютно одинаковые.
То есть сублинейная сходимость
в выпуклом случае, гладком,
гладком выпуклом случае.
А линейная сходимость
в
сильно выпуклом, гладком случае.
Вот. И получается, что на самом деле
итерационные и аракульные
сложности для
градиентного спуска,
что с проекцией, что без проекции совпадают.
Единственная проблема,
что
градиентный спуск с проекцией
имеет более высокую
аналитическую сложность в связи
с чем? В связи с тем, что у него есть оператор
и он предлагает дополнительные вычисления.
Потому что на самом деле
решение задачи
вот этого arg-minimum проекции
это же дополнительная оптимизационная задача.
И вам никто не гарантирует, что в общем
случае у него есть решение
в явном виде.
В явном виде нужно, соответственно,
это отрешивать дополнительно.
Понятно, что для каких-то хороших множеств
эти решения есть.
Эти решения есть. Ну, например,
шарик L2 легко спроецироваться
и въевклидывать они друг другу
в похожем расстоянии. Это одно и то же
расстояние, что в этом шаре используется,
что в проекции вы просто в некотором смысле
вот находитесь вокруг этого шарика,
здесь берете центр, ну и вот сюда приходите.
Проекция.
Если, соответственно, у вас просто какие-то ограничения
типа кубика с двух сторон XI
ограничены, ну вы просто
если, соответственно, вышли за пределы кубика,
ну вот, соответственно, туда вас и вернули.
По грани. Ну, для линейных
даже ограничений, когда у вас
ограничения заданы какой-то системой
линейных уравнений, тоже можно на самом деле
спроецироваться, но видно, что уже не так
дешево. Один раз нужно посчитать обратную
матрицу, может, она отраспонирована,
но каждую эту рацию нужно будет
считать матрично-векторные вычисления.
Что тоже
ни разу не дешево.
Поэтому вообще
оператор проекции считается
довольно дорогим, то есть
для мало чего он существует,
именно в явном виде,
для некоторых хороших множеств
существуют эффективные алгоритмы
проекции, которые тоже
на самом деле пришлось довольно долго
разрабатывать комьюнити,
но для многих множество,
даже когда есть аналитическое решение
и эффективный алгоритм, это дорого,
проецироваться дорого.
И как уйти, соответственно, от проекции
и прийти к чему-то более дешевому,
к какому-то альтернативному варианту,
на второй половине мы с вами и поговорим.
Перерыв. Так, ну что?
Давайте продолжать?
Давайте продолжать.
Так, давайте это.
Важное объявление,
я еще в телеграмм это тоже напишу.
Я уезжаю в командировку,
соответственно, следующие две лекции
будут в режиме онлайн.
Потом у вас контрольная,
контрольная,
она тоже будет на лекции в лекционное время.
Ну, соответственно, ее проведут семинаристы.
И вот уже там получается
через три недели,
то есть три лекционных времени,
уже на четвертую неделю я уже буду очна.
Следующие две лекции у нас онлайн,
потом контрольная на лекции,
потом уже возвращаемся в очный режим.
Не-не, на лекции и в лекционное время.
Контрольная в лекционное время.
Ну и, соответственно, это формат контрольная,
где-то на 2 балла
из трех,
где-то суммарно контрольная
3,5 балла, на 2 балла из трех
это задача уровня 10 минуток.
Вот, 10 минуток, 8-10
задач уровня 10 минуток.
Ну, соответственно, одну 10 минутку вы там решаете,
сколько? Две задачи.
Ну и, соответственно, если будут 10 задач,
то у вас где-то 50 минут, чтобы решить
вот эту часть, набрать два балла,
плюс еще полтора балла,
это задача посложнее.
Ну, чтобы, соответственно, у вас там еще минут 40,
чтобы решать вот эти задачи
и там что-то добрать.
Вот, примерно такой формат.
Вот, контрольный.
Я надеюсь, ну, мне кажется,
что это более-менее адекватно,
то есть, как раз, если вы умеете решать 10 минут полностью,
ну, довольно быстро так,
за минуту, там, за час,
если 10 минутку умеете решать, то оценку хорошо,
а вам за предмет точно можно поставить.
Ну, соответственно, 2 балла, поэтому есть.
Если хотите что-то решить,
ну, показывать, что знаете на отлично,
то это сложнее.
Поэтому есть время, чтобы и на это
порешать.
Да, я говорю, как 10 минут, как то,
что вы получаете в начале,
вот на 2 балла, это ровно вот
такого уровня задачи.
Кому-то давали в том числе задачи
по сходимости методов,
ну, не просто там написать
сходимость там,
итерацию и так далее, или теорему,
ну, и немного подумать, там подобрать,
если мы подберем шаг вот таким вот образом,
какой получится метод.
Кому-то писали градиентный спуск,
ну, и вы там писали тяжелый шарик, на самом деле,
тот же градиентный спуск, просто шаг такой вот.
Ну, вот такого рода задачки небольшие.
Подоказывать что-то
про методы. То, что мы, например,
на лекциях не доказывали,
но факты такие маленькие, которые
в строчку в две делаются.
Такое тоже может встретиться.
И как в первой части,
так и во второй.
Я надеюсь, что мы сделаем
не два варианта контроля, а три,
чтобы у вас было в некотором смысле пример,
то есть один мы как пример просто разместим,
чтобы вы посмотрели, как это выглядит.
Окей, ладно, это по объявлениям,
главное тут объявление, это то, что
две следующие лекции в режиме онлайн.
Так, хорошо, разговаривали
про проекции, поняли, что это может быть
дорого, вот.
По факту проекция ведь это очень простая задача,
с одной стороны, это квадратичная задача.
Кажется, что
легче особо не придумаешь,
но что тогда может быть легче, чем квадратичная задача?
Линейно, соответственно, да.
Если проблема
уже есть квадратичная, то давайте перейдем
к линейной задаче.
Понятно, что мы знаем, что
на неограниченном множестве
линейной задачи может не иметь решения,
просто потому что вы в минус бесконечность свалитесь
и все. Вот.
И поэтому, конечно, вот такие
линейные минимизации рассматривают
и на эксе, который ограничен.
Ограничен.
Вот, чтобы эта линейная минимизация
имела хотя бы какое-то конечное
решение. Вот.
Ну и на основе этой
линейной минимизации
возникает вопрос. А вообще
она легко делается или нет?
Легко делается или нет?
Для некоторых множеств действительно да, легко.
Например, тут приведены L1-шары,
вероятность на симплекс, для которых
проекцию делать
нетривиально. То есть там
под этим лежит такая хорошая теория
и, соответственно, эффективные
алгоритмы для этого всего
разрабатывались не так
быстро. Соответственно, да.
В случае линейной задачи тут
довольно просто.
Довольно просто. Ну вот
давайте на примере какого-нибудь, например,
L1-шара
и посмотрим, почему
решение линейной задачи будет таким,
будет таким.
Вот. Почему, думайте,
давайте порассуждаем, почему
решение линейной минимизации
для шарика
будет вот вот таким
или один шар.
Ну смотрите, что мы тут делаем.
Мы знаем, что у нас компоненты
этого шарика в сумме дают
меньше единички, да?
Вот. А у нас есть вектор,
вектор, который, ну вот, наш вектор
g, который мы минимизируем,
который мы рассматриваем в эту комбинацию,
g1 и так далее,
gn, ну и мы его скалярно
как-то перемножаем с этим
вектором s, s1 и так далее,
sd.
Вот. Хорошо. И мы
хотим сделать эту линейную комбинацию
как можно меньше, как можно меньше, при этом
в некотором смысле можно говорить, что у нас
ресурс, который у нас есть в x-ах,
он ограничен. Мы ограничены единичкой.
Вот. И кажется естественным
в некотором смысле вот
эту всю единичку пустить
самую наименьшую координату по модулю.
Да, наибольшую по модулю координату.
Мы пускаем ее, например,
находим эту наибольшую координату.
Вот. Пускаем туда весь
наш ресурс s, ну то есть
берем как раз s параллельно
этому направлению.
Вот. Единственное, что нам понятно,
нужно взять отрицательный
знак g, чтобы у нас как раз был минимум.
Потому что если мы просто возьмем это
направление и возьмем знак g,
то у нас будет как раз максимум.
Мы будем параллельны прямо
этой координате.
Вот. Возьмем с минимумом, будет минус.
Вот. Соответственно очень простое решение
для l1 шара. Аналогичное рассуждение
можно сделать, например, для симплекса.
Ну и вот для l бесконечности.
l бесконечного шара.
Вот. Вот такая вот соответственная
идея. То есть вот иногда
вот эта линейная минимизация действительно
значительно дешевле,
чем проекция. Вот. И значительно
легче в получении именно результата.
Теоретического, аналитического
решения. Вот. Но опять же
это не панацея. Это не панацея
когда-то проекция может быть лучше,
когда-то может быть линейная
минимизация лучше.
Вот.
Поэтому это в некотором смысле альтернатива.
Но никак не замена методы
проекции градиента. Ну и другим
методом, который мы будем рассматривать дальше.
Вот.
Окей.
На основе вот этого всего безобразия
линейной оптимизации
был сделан
вот такой вот алгоритм. Алгоритм
Франк Вульфа.
Алгоритм Франк Вульфа
70 лет назад.
Вот выглядит следующим образом.
Решаем линейную минимизацию,
но мы ее решаем для
градиента. То есть видите, мы смотрим
на градиент.
И тут можно на самом деле
чуть-чуть, я вот мне это не добавил,
но давайте это мы быстренько здесь сразу же
проделаем, чтобы понять
физику вот этого
всего безобразия.
Вот что нам вообще даст? Вот это
линейная минимизация
градиента.
Линейная минимизация градиента. Вот.
Опять же, в силу того, что аргуминимум
это вещь у нас,
которая может вбирать
у тебя то, что не зависит от s
за бесплатно.
За бесплатно.
Вот. Поэтому, поэтому
что я хочу?
Что я хочу? Я хочу вот в этот
аргуминимум добавить
x катой. Вот следующим
образом.
Понятно, что значение
аргуминимума от этого не поменяется.
Да?
Просто потому что, ну, это скалярное произведение,
которое я добавил, оно, ну, никак
не зависит от s, поэтому
на аргуминимум оно не влияет.
Вот. Хорошо.
Тогда мы, соответственно, когда
решим вот это выражение,
когда мы решим вот эту
линейную минимизацию,
мы получим некоторый
вектор s со звездой.
И из этого вектора
s со звездой мы можем вычесть
x каты. Вот.
Получается, что вот это,
вот это в некотором
смысле какое-то направление,
которое нам указывается. Ну, давайте вот
здесь. Вот у нас есть точка x.
Нам, соответственно, говорят,
вычислили как-то вот эту
линейную минимизацию, получилась
получилась
какая-то точка. Причем, например,
для каких-то, например, симплекса
L1 шара мы знаем, что эти
точки расположены на границе, причем
еще и на углах этих множеств. Там берется
просто один из базистных векторов.
Вот. И по факту нам говорят,
что если мы сделаем шаг по вот этому
направлению, которое мы здесь вычислили,
например, единичный шаг какой-то,
то мы просто перейдем вот этот угол.
Просто перейдем вот в этот угол.
Вот здесь хорошая картинка как раз
говорит о том, что мы что делаем. Мы
говорим, что пусть наша задача...
Вот. А давайте я еще
добавлю дополнительно. Помните, когда мы
рассматривали градиентный спуск, мы же
рассматривали линейную аппроксимацию,
вот такого вида.
Вот. И говорили, что давайте мы
предположим, что наша функция линейна.
Вот. И будем, ну, локально
линейна. И соответственно, будем
небольшими шажками сдвигаться по градиенту,
предполагая, что функция линейна.
Франк Вульф в некотором смысле
это альтернатива этому
всему. Вы как бы вот
решая вот эту задачу оптимизации
линейную, решаете вот эту
же задачу, которая как бы возникает в линейной
аппроксимации. Да?
Только вы решаете ее агрессивно.
Вы говорите, я найду
вот такое s на моем ограниченном
множестве, предполагая, что
моя функция линейна.
Ну вот, исходя из того, что вам сказал
градиент, вы говорите, что вот тогда
моя функция линейна. Вот.
И вы минимизируете
вашу линейную функцию
на множестве
ну, вашей оптимизации x.
Понятная идея, да? То есть
в градиентном спуске маленькие шажки,
маленькие шажки
по направлению.
Здесь, когда вы
ищете направление s, вы тоже, как градиент
на спуске, смотрите на линейную аппроксимацию.
И просто у этой линейной
аппроксимации функции
ищете минимум
исходя из того,
ну, исходя из вашего множества x.
В силу того, что множество ограничено, минимум, понятно, существует.
Вот.
Окей? Идея понятна?
Вот. Это, соответственно,
идея первая. То есть, что вообще
означает этот аргуменим? Вы просто
минимизировали линейную аппроксимацию.
Вот. Но, франк-мульф, понятно,
что делает шаги тоже не равные
единице, потому что, если вы просто
будете делать шаги равные единице, как мы поняли,
вы просто будете перемещаться между
вот этими точками, которые вы
при линейной минимизации
находили. Ну, это будут какие-то
точки на границе, какие-то уголки вашего
множества. Они вам ничего особо не скажут.
Вот. Поэтому, смотрите,
вот в x
запихивается
вот этот s, то есть, уголок,
который вы нашли, но
с уменьшающимся весом.
С уменьшающимся весом.
И тогда у вас как бы получается что-то вот такое
вот гладенькое. Ну, вот сейчас мы это как раз
и поймем, что у нас получится. Видите,
как раз у нас есть x0.
Мы смотрим в сторону
уголка. Вот этого как раз у нас
вычислился s.
Дальше мы делаем небольшое
смещение. То есть,
в самом жестком случае с шагом 1
мы бы сместились тупо в s.
Вот. Дальше, из новой точки
мы снова делаем линейную минимизацию.
Находим новый уголок,
который как бы минимизирует
нашу функцию, вот уже новую
линейную опроксимацию. Вот.
И, соответственно, идем
в ту сторону. Но, опять же, шаг уменьшается.
Поэтому у нас получается какая-то
комбинация меньше.
Ну и дальше, соответственно, в силу того,
что у нас шаг становится меньше, меньше,
меньше, меньше, меньше, вот.
Точки, вы смотрите все
новые и новые уголки, вы получаете
некоторую комбинацию вот этих уголков.
Вот. А если вы еще глянете
на то, как выбирается шаг,
что вы можете сказать вообще?
На что это похоже?
Вот если мы вот так вот на это смотрим выражение.
На что это похоже?
Выпукла комбинация, но она
же где-то еще у вас, скорее всего, встречалась.
Как, например, онлайн
не считать среднее число?
Если я знаю текущее среднее,
и мне добавили новое число.
Да, если я знаю, что
среднее, например, XCAD
посчитано по качествам,
как и мне прислали новое XCAD
плюс один, например.
Типа того. То есть, смотрите,
у вас есть XCAD, это в некотором смысле
среднее, так сказать, среднее
тех уголочков, которые вы имели
до этого. Каждый раз вам
как Вульф, вот эта линейная минимизация,
показывает какой-то из уголочков. Говорит, что вот сейчас
вот это минимум моей линейной
апроксимации. Вы говорите, окей,
хорошо, я тебе поверю, давайте я возьму этот уголочек,
ну и буду брать средние этих уголочков.
То есть, вам, условно, приходит
новый уголок, и вы что
запихиваете? Если вы
прям делали честно среднее, вы должны что там
умножить? На k разделить
на k плюс один и, соответственно,
добавить вот этот новый уголочек
с весом k
1 делить на k плюс 1.
Ну вот здесь же что-то очень похоже и записано,
просто чуть-чуть поменен этот
коэффициент внизу.
То есть, по факту, метод
Франко-Вульфа – это агрессивная линейная
минимизация, когда каждый раз вы
минимизируете вашу локальную функцию. Она вам
говорит, что минимум будет достигаться вот на границе
вот в этой точке.
Вы говорите, окей, да, я приму это к сведению,
но буду как бы усреднять
те показания, которые
у меня получались. И понятно,
что если минимум будет где-то близко
какому-то, например, одному из
уголков, вы на этот уголок будете просто
смотреть чаще
в линейной минимизации.
И соответственно, когда вы это все будете усреднять,
усреднять, усреднять, усреднять,
этот уголок будет у вас выскакивать чаще
и, соответственно, комбинация
усредненной, он будет у вас как раз
давать включевой вклад, и вы будете потихонечку
сходиться к минимуму.
Вот эта идея метода Франко-Вульфа.
Понятно?
Я надеюсь, что да. То есть, вот такой агрессивный взгляд, но все равно такое мягкое усреднение.
Окей. Так, давайте тогда доказывать его сходимость. Что-то там я паузу не поставил, поэтому давайте листик создам.
Так, листик создам.
Докажем давайте сходимость в методе Франко Вульфа для выпуклых задач. Я скажу, почему не для сильных выпуклых концептов.
До этого, вроде, всегда сильно выпуклыми задачами работали. Тут будет выпуклое. Давайте гладкость. Гладкость будем писать. Я запишу гладкость.
В том виде тоже довольно стандартное определение, которое мы с вами встречали. Это мы его с вами до более того доказывали.
Вот. Пишу гладкость.
Гладкость.
Согласны? Вот.
Теперь давайте подставлять итерацию метода Франко Вульфа.
Давайте на всякий случай глянем эту итерацию. Что там у нас с вами было?
Уголочки, уголочки. Вот. X-каты это комбинация вот такая вот. Комбинация вот такая вот.
Вот.
Поэтому вот я ее в таком виде перепишу. Это x-k
минус гамма-k
x-k
минус x-k. Пойдет?
Чтобы у меня как раз была разность x-k плюс 1 и x-k, она более в более явном виде. Окей?
Вот.
Поэтому здесь я вот буду записывать вот так.
Вот.
x-k это минус гамма-k
x-k минус x-k. Окей? Вот.
Подставляю предыдущее выражение.
Что соответственно получаю? Получаю следующее.
Следующее.
Минус гамма-k
f
x-k. Здесь соответственно что у меня
будет x-k минус x-k.
Плюс
l
гамма-k в квадрате пополам. И здесь будет x-k
минус x-k в квадрате в квадрате.
Вот.
Вот с этим все довольно просто. В силу того, что мы работаем на ограниченном множестве, я веду диаметр этого множества.
Диаметр этого множества нашего. Как просто
максимум по всем точкам из этого множества, ну расстояние Евклидова.
Диаметр в квадрате пусть будет вот так.
Соответственно вот это выражение я могу оценить через диаметр. Это выражение могу оценить через диаметр.
Будет что-то вот такое.
Давайте диаметр пусть будет D у меня сразу. D в квадрате.
Вот.
Чтобы длина не писать.
Интересно посмотреть на скалярное произведение. Интересно посмотреть на скалярное произведение, потому что у меня тут возникает что-то в духе минус f
xk
xk минус xk.
Вот. Я чуть-чуть опять же выделю сейчас вот этот кусочек, который у меня завязан на xk.
Вот так вот. И оставлю вот этот кусок.
А вот про этот кусочек мы же что-то можем сказать? Что мы про него можем сказать?
Ск же как-то получался?
То, что слева написано. Но я по-другому чуть-чуть хочу. То есть понятно, что ск это
минимум линейной минимизации, да?
То есть для любого вектора какого-то x вот это меньше либо равно, чем вот
вот что-то вот такое.
x
где x соответственно должен быть из множества x. Просто потому что ск это он искался как минимум этой задачи.
Но я сюда что поставлю? Я сюда поставлю оптимум.
Просто оптимум. Ну все. Давайте посмотрим теперь, что будет получаться.
Вот. Получится соответственно у меня здесь больше либо равно, чем минус
f xk.
xk минус x звездой. Вот. Как я это могу оценить? Скалярное произведение?
Ну нет, я хочу похитрее. Чего-то помощнее. Что еще есть? Что еще есть скалярное произведение, где возникало?
Гладкость там была. Ну здесь соответственно что можно еще? Выпуклась, да? Выпуклась. Поэтому я по выпуклости его оцениваю вот так.
Здесь просто выпуклась применяется.
Окей. Оценил скалярное произведение. А сейчас увидим, почему мне нужна была эта выпуклость.
Вот.
Вот.
Здесь просто выпуклась применяется.
Окей. Оценил скалярное произведение. А сейчас мы увидим, почему мне нужна была эта выпуклость. Здесь когда я все это подставлю аккуратно наверх?
Вот. У меня вот здесь это выражение болтается.
Что такое?
Ладно, по ходу придется переписывать.
Ладно, я перепишу.
Вот, у меня...
Что такое-то?
Не сел?
Вот не сел.
Ладно.
Так, давайте на слайдах тогда посмотрим.
Вот.
Смотрите.
А что я сделал?
Переписал ровно то же самое, что и я.
Диаметр написал, добавил, дальше по выпуклости это
расписал.
Вот.
То, что мы как раз х со звездой добавили, расписал
по выпуклости, и у меня получилась вот такая вот
разность.
Вот.
Здесь я еще с права и слева вычел по минус х со звездой.
Ну, валидная операция, могу и с права и слева вычесть
просто по часу.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Валидная операция, могу и с права и слева вычесть
просто по числу какому-то.
Хорошо.
Ц заменил.
Смотрите, что у меня получилось.
У меня получилось то, что мы, в принципе, получали
учет для градиентного спуска.
Вот.
Но чуть похуже.
Чуть похуже, потому что, видите, у меня как бы есть
разность функций в ка плюс первой точке, разность
функций в ка этой точке, но умноженный коэффициент,
как раз коэффициент вроде как хороший, будет линейная
сходимость.
Но вот то, что болтается дальше, оно не компенсируется.
Потому что мы это просто убивали с помощью подбора
шага.
Оно прямо уходило в ноль оно у нас и становилось
даже отрицательным.
Вот.
Здесь, ну, просто болтается.
Вот.
Поэтому, соответственно, и шаг будет, будет подбираться
как-то хитро.
Но вот главное, что мы получили вот такую рекурренту, что
у нас ка плюс первая зависит от катова линейно, вот так
вот, ну, в некотором смысле линейно, плюс вот этот кусочек,
который у нас болтается, который пропорционален
гамма ка в квадрате.
Каверондаш не одумался, не одумался.
Ладно.
Хорошо.
Смотрите.
Все очень просто дальше будет.
Дальше будет по индукции.
Мы будем доказывать, что справедливо вот такая вот
оценка.
Вот.
На нашу, для нашего метода, на кат и итерации мы можем
гарантировать, что разность по функции будет вот такая
вот.
Вот, вот такая вот.
Ладно.
Так.
Что делаем?
База индукции, понятно, следует автоматически просто
потому, что у нас f от х0 меньше, чем f от х0.
Вот.
Окей.
Дальше, соответственно, что?
Дальше, соответственно, что?
Только четверку вроде надо было вынести за пределы.
Ладно, это не страшно.
Дальше делаем шаг.
Посмотрим, насколько поменяется от х к плюс первого до х катого.
Вот.
Подставляю реально мой шаг в ранг вульфа.
Сюда и сюда.
Соответственно, гамма просто меняю на этот шаг.
Подставляю предположение индукции.
Предположение индукции вместо f от х ката минус f от х
со звездой.
И вот.
Вот.
Вот.
Вот.
Вот.
А дальше сейчас будет просто в некотором смысле манипуляции.
Хочу оценить правую часть.
Показать, что для нее действительно будет выполнено выражение.
Вот.
Это списано с предыдущего слайда.
Только вот, соответственно, вместо c справа подставлено
выражение, которое максимум от 4c f от х0, а минус f от х
со звездой.
Понятно, что c, ну там 4c, потому что здесь двоечка,
она в квадраты возведется, будет все хорошо.
Будет 4c.
Понятно, что 4c меньше либо равно, чем максимум из
4c и разность функций.
Окей.
Соответственно, это здесь и подставлено.
Получается вот такое вот.
Дальше что?
Играю со скобочкой и дохожу до ровно того, что в принципе
и нужно.
Ровно того, что в принципе и нужно, потому что как раз
в знаменателе у меня выскочило k плюс 1 плюс 2.
Вот.
То есть на k плюс 1 итерация это будет справедливо.
Окей?
Вот.
Очень простое доказательство.
Ну, для метафранка Вульфа.
Блин.
Что с ним случилось-то?
Ладно.
Вот.
Теоремка, соответственно, сформулирована.
Получается какая?
Собленейная сходимость 1 делить на k для выпуклой
гладкой задачи.
Вот.
Результат ровно такой же, как для градиентного
спуска.
Да?
То точно такие же результаты для градиентного спуска
у нас были.
Вот.
Единственная проблема в том, что в субленейном случае
в гладком выпуклом случае все окей, действительно
совпадает с градиентным спуском.
Проблема в том, что в сильно выпуклом случае то же самое
получается.
То есть в общем случае, когда у вас выпукло множество
сильно выпуклая функция, вы не получите линейную
скорость сходимости для метода Франко Вульфа.
Ну вот такая специфика.
Причем эта специфика связана именно с тем, что вы используете
линейную минимизацию.
Она тормозит метод.
Она тормозит метод именно с точки зрения оракульных
вызовов.
Вот.
То есть можно решить эту проблему, например, делая
на один вызов градиента несколько оракульных вызовов.
Несколько оракульных вызовов.
Ой, ой, Господи.
На один вызов градиента делать несколько вычислений
линейной минимизации.
И тогда действительно у вас получится, что количество
вызовов градиента будет как у градиентного спуска,
например.
А количество линейных минимизаций будет 1 делить на k.
Потому что именно линейные минимизации тормозят метод
Франко Вульфа.
И именно для них нижняя оценка, ну показывается, что для
метода Франко Вульфа 1 делить на k, это лучше, что можно
достичь именно из-за того, что нужно сделать 1 делить
на k.
Ну, то есть, точнее как, k пропорционально 1 делить
на epsilon линейных минимизаций.
То есть важная деталь, что именно во Франке Вульфе
линейная минимизация делает его медленным.
Ну, в некотором смысле именно с точки зрения оракульных
сложностей.
С арифметической точки зрения это может быть действительно
когда-то лучше, потому что линейная минимизация
довольно дешево стоит.
Но если там линейная минимизация, и при этом приходится часто
считать градиент, хотя можно было считать реже,
это может быть послужить в некотором смысле отторжением,
почему метод Франко Вульфа можно для некоторых задач
не использовать.
Либо пытаться модифицировать, ну и такие модификации
на самом деле в литературе есть, где на 1 вызов градиента
используется подсчет линейной минимизации несколько
раз.
Несколько раз, и там получаются уже правильные оценки.
Вот.
Хорошо.
Все.
На сегодня это все.
Вот.
Вопросы?
Сейчас.
Куда-то, да?
Или предыдущие?
А, С?
Пожалуйста.
Где оно у нас было?
А где я его определил?
Сейчас.
Секундочку.
Во.
Во-во.
А, там еще L.
Там еще L запихано.
Ну вот, короче, вот это выражение, чтобы там просто
С осталось.
Так.
Ну если вопросов нет, тогда всем спасибо.
В следующей неделе еще раз онлайн.

Сегодня мы поговорим про систему, которая, в отличие от прошлой лекции, помогает работать с данными.
Помните, в прошлый раз мы говорили про ZooKeeper, и эта система позволяла строить, мы говорили,
зачем она нужна, чтобы строить другие распределённые системы. Вот опачкавка,
сегодняшняя наша система как раз использует ZooKeeper, мы сегодня свяжем прошлую лекцию с
прошлой лекции с сегодняшней. В общем, следите за этим, там будет много забавных вещей.
Итак, представим себе задачу. Вот мы большая компания, у нас есть сервис такси, у нас есть
сервис доставки еды, у нас есть поиск, у нас есть... ну что у нас ещё есть? У нас есть рекламная сеть.
В общем, у нас очень много разных сервисов, и, разумеется, это разные системы, они обслуживаются,
разрабатываются разными подразделениями, деплоатируются на разных компьютерах. В общем,
это такой огромный феодальный мир, где каждый усочек живёт более-менее, может быть, обособлено.
И представим себе такую задачу. Вот вы пишете поиск, и для того, чтобы этот поиск был
персенализирован, вам нужно агрегировать очень много разных источников данных. Вам полезно знать
про пользователей, что он заказал себе, не знаю, суши или что-то другое заказал, или гамбургер.
Вам может быть интересно, куда он поехал, вам важно, что он искал в поиске совершенно недавно,
вам важно, на какие рекламные объявления он кликнул. Вам это всё нужно агрегировать в
реальном времени для того, чтобы по вот этим данным рассчитывать какие-то профили пользователей,
ну и персенализировать выдачу для него в поиске. И представим себе, что мы занимаемся машинным
обучением и готовим поисковую формулу, которая будет ранжировать выдачу. Для этого нам нужны,
в принципе, те же самые данные, нам нужны действия всех пользователей, все их клики в поисковой
сессии, но, конечно же, они нужны не в реальном времени уже, а в системе, которая позволяет
выполнять батч-убработку этих данных. Грубо говоря, нам нужно положить те же самые данные
куда-то в reproduce для того, чтобы раз в неделю или раз в день или ещё чаще эти данные обрабатывать
с помощью конвейера и высчитывать какие-то коэффициенты. Ну или можно придумать себе много разных
таких приложений, в конце концов, не знаю, хочется для аудито просто иметь логи, что делали
пользователи ваших сервисов. То есть у вас есть очень-очень много источников данных, данных
разной природы, разной структуры, которые пишут с разными, которые генируются с разными сервисами,
за которые отвечают совершенно разные люди. И есть довольно много потребителей этих данных,
которые тоже очень по-разному устроены. Где-то real-time обработка, где-то batch обработка,
где-то просто хранение. Ну а теперь представим, что вот этих всех производителей данных и этих
потребителей данных нужно связать друг с другу. Это сделать, ну на прямую это сделать, конечно,
не получится. Потому что каждый продюсер данных должен знать про каждого потребителя этих данных,
они должны договориться о протоколе, они должны договориться о транспорте, они должны договориться
там о сроках хранения данных. Они должны подобрать какие-то коэффициенты,大哥, не коэффициенты,
должны настроить скорость отправки данных, какие-то лимиты. В общем, огромное количество
технической работы, которыми ни продюсеры, ни консюмеры заниматься вообще-то не хотят,
потому что зачем продюсеру поиску думать про там какого-то, какую-то подсистему,
которая только начинает обрабатывать эти данные. И с другой стороны, если вы занимаетесь
персонализацией, то вам будет очень сложно ходить и договариваться с каждым источником данных,
делать это напрямую. Вот задача понятна, надеюсь, ну и она должна быть довольно понятна и за
пределами какой-нибудь большой компании, и за пределами IT, потому что, ну в самом деле,
представим себе такую задачу. У вас есть много компаний, которые просто торгуют своими акциями,
у вас есть много покупателей потенциальных этих акций. Но покупатели, они очень маленькие,
отдельные люди. Компания очень большие, и конечно же, Microsoft не зачем продавать акции каждому
отдельному человеку. Для этого продавцов и покупателей, компаний и акционеры, они связаны
с помощью брокера, который вот может дегрегировать покупки и покупать сразу много акций, ну в общем-то
уменьшать накладные расходы на все эти операции. В нашей задаче, в нашем мире, где есть очень много
потребителей данных и производителей, возникает та же самая задача, и можно решать это точно таким же
образом. Можно производителей и потребителей, продюсеров и консюмеров, как мы будем называть
сегодня, объединить с помощью брокера. Вот продюсеры и консюмеры друг по другу напрямую не знают,
но, разумеется, они знают, что там, не знаю, персонализация знает про то, что там пользователь
ездит на такси и хочет получить эти данные. Но напрямую сервисы, вот эти вот напрямую
персонализация и такси друг с другом не договариваются. Они общаются друг с другом через
некоторую шину сообщений, через некоторого брокера, который позволяет доставлять данные от
продюсеров консюмеров. И этот брокер называется в нашем случае Apache Kafka. Kafka предоставляет вам
довольно простую модель данных, а именно понятие данных, которое называется LOG. LOG — это Append-Only
структура данных, в нее можно добавлять сообщения, ну и, в общем-то, все, что с ней можно делать, в смысле,
из мутации. Нельзя исправлять сообщения, нельзя удалять сообщения, в середине, по крайней мере,
нельзя их менять. Мы просто добавляем сообщения в LOG, и ему присваивается новый порядковый номер. А дальше
эти сообщения могут читать клиенты. Это LOG — это не очередь, в том смысле, что клиенты, консюмеры данных,
не достают эти самые сообщения, которые отправляет продюсер. Они их просто читают. Вот если мы говорим,
про сессии пользователей или про клики на выдаче поисковой или на рекламу, или про заказы в сервисе
доставки, то для каждого продюсера данных разумно завести свой собственный LOG, так чтобы в пределах
этого LOGа хранились какие-то гомогенные сообщения. То есть, вот пользователь кликнул по такому
урлу в выдаче. И вот можно себе представить, что огромный поисковый кластер, где многие тысяч,
десятки тысяч машин обрабатывают клики и запросы пользователей, вот эти машины пишут свои логи и
отправляют их в эту самую кавку, этот самый брокер. А кавка, в свою очередь, всем этим событиям
назначает порядковые номера и укладывает вот один такой общий, видимо, отказоустойчивый
распределенно хранящийся LOG. И дальше сервис, на котором работают поисковые демоны, он забывает
про все, он же не думает про отдельных консюмеров, потому что теперь задача кавки, чтобы консюмеры
могли эти данные вычитывать. Персонализация или какие-нибудь процессы, которые загружают данные
в таблице, в produce, это все будет независимо уже от продюсера происходить. Консюмеров у одних
этих данных, разумеется, может быть, много. Система batch обработки, система real-time обработки,
с точки зрения кавки, с точки зрения лога, это отдельный консюмер, отдельный клиент. И у каждого
клиента есть своя собственная позиция в логе, до которой он в данный момент дочитал. Разумеется,
консюмеры могут читать в разном темпе, и никто из них данные не достает, они просто их учитывают,
продвигают свой курсор дальше. В кавке лог, ну лог это такая абстрактная структура данных,
в кавке этот лог называется топиком. И разумно, если мы говорим про хранение разных данных,
для разного источника данных, для разных продюсеров заводить разные топики, чтобы в пределах
топика данные были какими-то однородными. Клики, или поиски, или заказы, или что-то подобное.
Давайте посмотрим на то, как с этими топиками дальше можно работать. Вот в кавке все клиенты,
все пользователи выступают в двух ролях. Либо продюсеры, либо консюмеры. Продюсеры
загружают данные в топики, консюмеры читают данные из топиков. Вот, пожалуйста,
пример консюма, пример продюсера. Вот мы создаем клиента и в цикле отправляем в какой-то топик,
который называется mytopic. Давайте это схлопнем. Отправляем 100 записей. Записи – это пара ключа
значения, как здесь видно. Вот ключ, вот значение. Но это всего лишь такая фиксированная структура
записи. Семантик этих ключей и значений в кавке, ну в данном по крайней мере случае, в общем случае,
не фиксировано. Это просто некоторые блогики, и кавка их внутри себя никак не интерпретирует.
Пусть продюсер и консюмер за пределами системы договорятся о том, каково же содержимое этих
записей, в каком они формате. Там может быть JSON, там может быть протобув, там могут быть просто
какие-то строчки, могут быть бинарные данные, что угодно. Про конкретный формат, про протокол
договариваются снаружи системы, а кавка выступает здесь просто посредником.
Вот мы видим, что есть интересного, что мы в клиенте указываем точку входа, какую-то машину,
через которую мы узнаем дальше, где же кавка будет данные этого топика хранить. Вот мы здесь
указываем число ретроев. Что будет, если мы ретрои повысим, но в случае, если вдруг что-то где-то
залипнет, мы можем сделать ретрои одного и того же сэнда, и в результате мы можем получить
потенциально в топике дубли этих же данных. В случае ретрои 0 мы говорим, что мы ретрои делать не хотим.
Но мы можем, конечно, бачить отправку, то есть мы можем отправлять не по одной записи в кавку,
это было бы абсолютным безумием в высоконагруженной системе. Мы хотим группировать, накапливать пачки
сообщений и отправлять их уже вот такими большими блоками. Ну, serializer, я уже сказал,
что кавка не интерпретирует содержимое этих данных. Про протокол договариваются вот здесь,
в клиенте продюсер и в клиенте консюмер. Вот продюсер занимается тем, что он отправляет
данные в кавку. Консюмер занимается тем, что он читает данные из кавки и из топика. Вот мы
давайте пример попроще найдем. Вот мы здесь создаем консюмера. Мы подписываемся на топике и дальше в цикле,
ну потому что сам топик это бесконечный лог, он где-то начинается, когда-то дальше растет
непрерывно. Мы в этом бесконечном цикле обрабатываем данные из топика, не знаю, читаем какие-нибудь
запросы пользователей, как-то их анализируем. Для этого у нас есть вызов пол. Консюмер использует
пол, то есть мы опрашиваем кавку, когда у нее появляются данные. Пол означает, что мы готовы
ждать 100 миллисекунд, пока кавка не наберет либо достаточно много сообщений, мы ждем пока,
либо сама кавка не наберет достаточно много сообщений для нас, либо просто истечет тайм-аут,
и мы получим очередную пачку записи. Ну а дальше мы их как-то обработаем, в данном случае просто
напечатаем на экран. У консюмера, разумеется, есть свое собственное состояние, потому что что
делать, если консюмер вдруг перезагрузится? С какого места он начнет читать? Это, собственно,
стейт консюмера, его состояние, и этот стейт называется оффсетами, то есть для топика мы знаем,
до какого индекса мы уже дочитали, с какого индекса нам нужно начать в следующий раз. И в данном
примере эти оффсеты фиксируют, ну кто отвечает за хранение этих самых оффсетов? Это достаточно
тонкий момент, важный момент, за хранение оффсетов отвечает сама система. Клиент не должен помнить,
на каких позициях он остановился, просто потому что клиент, ну понятно, мы говорим про определенную
систему, клиент может отказывать, клиент может переехать на другую машину, и пусть его оффсеты,
его состояние хранится централизовано в кавке. И вот эта настройка отвечает за то, что сам клиент
кавки, то есть сам консюмер, будет периодически в кавку фиксировать текущий прогресс клиента. Ну
какой позиции он начитал. Мы доверяем здесь самому клиенту и не забоимся вообще об этом. Ну вот такая
общая модель, есть продюсеры, есть консюмеры, продюсеры добавляют в лог, в топик, консюмеры из
этого лога топика читают записи, ну и периодически передвигают свой курсор персистентный вперед.
А как связаны консюмеры и клиента, не очень понятно. Клиент кавки, он может быть в двух ролях,
либо продюсер, либо генерирует данные, пишет их в кавку, либо он консюмер, то есть он читает из
кавки. Был еще какой-то вопрос с эхом. Это он и был. Ну вот такая модель данных, но на самом деле
модель данных чуть сложнее, потому что кавка, конечно, не смогла бы для каждого логического
потока данных, логического потока записи заводить просто вот отдельный такой лог-топик.
Ну представьте себе, вот вы пишете поисковые системы, сколько там поисков секунды происходит,
и сложно представить, что кавка сможет все эти там запросы пользователей упорядочивать,
каждому из них присваивать такой вот порядковый номер, который еще непрерывно растут. Понятно,
что чтобы присваивать записям, сообщениям продюсеров порядковые номера, нужно вот где-то
завести машину, которая через себя будет все это пропускать и упорядочивать. Вот разумеется,
одна машина с этим не справится. Кроме того, с другой стороны, у нас есть консюмеры, и они тоже
читают из топика, и разумеется, они хотят читать параллельно. Ну то есть у вас опять много данных,
и вы готовы учитывать и обрабатывать их на многих машинах. Ну скажем, вы загружаете данные в
mupreduce, почему бы вам не писать в mupreduce параллельно? Сам mupreduce это прекрасно позволяет.
Так что в модели данных кавки топик, в свою очередь, делится на партиции. Вот топик это,
логически, это такие однородные данные, но физически эти данные, эти сообщения одного
топика, представлены в виде набора партиций. И каждая партиция уже является логом. Вот внутри
каждой партиции номерация сообщений, такая сквозная, непрерывная, но между партициями никакой связи
нет. Партиции более-менее независимы. То есть мы ожидаем, что в пределах одного топика однородные
данные и на уровне, и внутри кавки, эти данные хранятся в виде набора партиции, в виде набора логов.
Когда мы пишем в топик, вот, например, в этом примере, то мы отправляем очередное сообщение
в какую-то партицию. В самом простом случае это происходит просто по-хорошему от ваших данных.
Ну и таким образом вы можете параллелить работу продюсеров легко. У вас много продюсеров, там, не знаю,
тысячи машин, они пишут в кавку, они пишут в разные партиции, ну и вы выбираете число партий,
вы их конфигурируете исходя из ожидаемой нагрузки на запись и на чтение. Если у вас данных мало,
ваш топик может быть одной партицией. Если у вас данных очень-очень много, то вы этот топик
дробите нам много партиций. Нужна ли вам сквозная нумерация? Ну, вопрос. Как правило, вы можете без
нее все же обойтись. Вы можете, там, не знаю, вам в пределах пользователя может быть нужна одна
сквозная нумерация. Ну, тогда вы сделаете так, чтобы записи одного пользователя были в одной
партиции. Но сами партиции могут быть друг от друга, сами партиции друг от друга независимы.
Это такой способ промасштабироваться в пределах одного топика. Ну, и если у вас много партиций,
то, разумеется, писать в эти партиции параллельно очень легко. А вот читать чуть сложнее,
потому что, ну представьте, у каждого клиента есть прогресс, у каждого клиента есть позиция в
этом логе, на который он остановился. И сложно представить себе, как могли бы несколько независимых
клиентов параллельно читать одну партицию. Ну, то есть, им же нужно друг с другом данные,
им нужно поделить между собой данные, которые они вычитывают. Вот кавка позволяет вам из топика
читать параллельно, просто распределяя партиции топика между консюмерами. Один консюмер может
читать несколько партиций, но одну партицию читает только один консюмер. Ну, разумеется,
если у вас консюмеры просто логически разные, там реал-тайм какой-нибудь, процессинг и бач
процессинг, то это вот разные группы консюмеров. И, конечно же, они могут читать одни те же партиции.
Но если у вас консюмер логически один, просто он представлен в виде набора машин, то в этом
случае кавка сама умеет распределить партиции топика между вот конкретными узлами клиентами
и консюмерами. Вы можете не заботиться о том, как это распределение сделать, потому что,
понятно, у вас отдельные узлы, на которые запущены консюмеры, могут отказывать, могут
перезагружаться, консюмеры могут пропадать, могут добавляться новые. Так вот, в кавке для этого
устроен довольно нетривиальный протокол, который обеспечивает балансировку партий
между консюмерами. Когда вы подключаетесь к кавке, вы задаете группа ID. И если вы параллельно
с разных машин подключаетесь к кавке к одному и тому же топику, ну, с одним и тем же группой,
к одному и тому же топику, с одним и тем же группой ID, то кавка считает, что это логически один клиент,
и между всеми вот конкретными инстанциями этого клиента нужно партиции распределить. Для этого
в кавке выбирается, среди узлов кавки, про которые мы пока не говорили, выбирается узел,
который играет роль координатора, координатора группы, и дальше координатор группы вот в одиночку
распределяет партиции между конкретными узлами-консюмерами. Если вдруг появился новый консюмер,
или если вдруг отказал один из существующих консюмеров, ну, просто потому что машина
взорвалась его, то в первом случае координатор получает от клиента сообщение, что вот я новый
консюмер, я готов присоединиться к группе, или в случае, когда клиент-консюмер умирает,
брокер-координатор, на брокере-координаторе истекает тайм-аут, ну, в общем, по одному из этих событий
координатор понимает, что нужно пересмотреть распределение партиции между консюмерами одной
группы и проводит их там, ну, можно сказать, что через барьер. Вот, если вы помните, мы писали
барьер на кондварах, но вот тут такой же барьер нужно, чтобы все консюмеры перестали читать,
и мы с ними передоговоримся о составе, о наборе живых консюмеров и о распределении партий.
Вот просто так добавить нового консюмера, конечно же, нельзя, потому что он начнет читать
какой-то топик, а в то же время какой-то другой консюмер его уже раньше читал,
и он начнет обновлять офсеты для данной партиции, ну, и в общем, тут случится какая-то
согласованность, непонятно, кто именно читает партицию, кто именно двигает для нее курсор.
Вот курсор при чтении из топика, он опять же связан с этим самым группой ID,
то есть с логическим консюмером, и он свой для каждой партиции. Вот, то есть машины – это
отдельные машины, отдельные узлы, которые запускают на себе клиентов, это уже следующий
уровень, и офсеты хранятся не для машин, разумеется, а для таких вот логических клиентов. Поэтому
координатор нужен здесь за тем, чтобы аккуратно перераспределять партиции и не допускать вот
таких вот гонок, когда один консюмер, казалось бы, умер, у него протухла сессия, а потом он появился
и закомитил старый офсет. Проблема понятна, идея понятна? Что будет, если координатор умрет?
Если координатор умрет, то это не страшно, потому что выберется другой. Это узел системы,
и эта роль, конечно, не закреплена за каким-то конкретным узлом, потому что клиентов более-менее
произвольное число. То есть они опять сами между собой общаются, посылают храбиты и выбирают лидеров?
Ну, это сделано не так, и давай мы, наверное, если про модель данных понятно, что вот есть
топики, в которые должны лежать однородные сообщения, эти топики делятся на партиции
для параллелизма, для масштабирования, есть продюсеры-консюмеры, продюсеры пишутся,
топики добавляют в конец, консюмеры читают из топика и фиксируют свой прогресс, фиксируют
позицию курсора в каждом топике, в каждой партиции, и есть балансировка партиции между экземплярами,
физическими экземплярами одного логического клиента. Вот если с моделью данных понятно,
то можно перейти к тому, как это все физически выпущено самой системе, потому что вопрос уже
про реализацию. Но реализовано это более-менее стандартным образом.
И давайте все же про модель данных еще одну вещь поговорим, прежде чем говорить,
как это все хранится и как там кто выбирается. Поговорим про консюмера. Вот здесь мы сказали,
что консюмер просто автоматически фиксирует свой прогресс. Сам кавка-консюмер где-то в фоне,
где-то в фоновом потоке периодически уведомляет систему о том, что вот клиент дочитал до какой-то
позиции и можно передвинуть его offset вперед. Но может быть мы хотим действовать аккуратнее,
мы хотим вручную управлять фиксацией текущих offset. Представим себе вот такой код. Консюмер
здесь устроен чуть сложнее, он опять в бесконечном цикле читает данные, он из них накапливает какие-то
пачки и если он накопил пачку какого-то достаточного размера для каких-то своих целей, то он эти данные
перекладывает в базу. Ну то есть вот этот код, этот клиент это просто он использует кавку как такой
персистентный буфер для данных, для того чтобы продюсер туда писали данные, а вот этот клиент
будет их перекладывать в базу для быстрой аналитики. Ну представим себе какой-нибудь
криг хаус, который не тормозит как известно. Так вот представим, что мы здесь пишем самый
условный криг хаус в базу данных, а потом мы фиксируем offset. Вот здесь мы делаем это вручную.
Подумаем, что происходит с этим кодом в случае отказа, в случае рестарта клиента, рестарта машины,
на которой запущен клиент или в случае, когда вот машина целиком умирает и этот клиент для вот
тех же топиков, тех же партий, переезжает на другую машину. Ну вот представим себе,
что машина перезагрузилась после этой строчки и до этой строчки. В этом случае после перезапуска
клиент, до запуска клиента, не знаю, прочел 100500 записей из партий из топика и записал их в базу,
но не успел сделать последний commit sync и не зафиксировал, что последние 100 записей действительно
были им прочитаны и обработаны. Он перезагрузился и его offset равен сейчас 100400, но при этом вот в
этой базе лежит уже 100500 записей из топика. Он перезапустится и вот последние 100 записей в базе
получается продублируют. Так мы получаем семантику at least once. Мы гарантируем, что каждое сообщение из
топика, по крайней мере один раз, окажется в этой внешней базе данных. Если нас это не устраивает,
то мы можем использовать то, что мы можем сделать. Мы можем переставить эти строчки местами. Мы сначала
зафиксируем offset, до которых мы дочитали, а потом положим данные в базу. В этом случае,
если мы перезагрузимся после commit sync, но до insert ntdb, то мы получим семантику at most once,
то есть каждая запись из топика будет обработана, попадет в базу не более одного раза.
Ну вот тут мы в таком API, то есть при синхронном комите offset, мы хотя бы управляем этим. В случае,
когда мы ставим галку autocommit, то commit offset происходит вообще фоново и без понятных для нас гарантий.
Ну вот, конечно же, нам хотелось бы иметь семантику, как мы понимаем, exactly once, когда каждая
запись из топика будет обработана ровно один раз. Ну вот как этого достичь, мы поговорим чуть позже.
А пока, как все эти топики, партиции хранятся внутри кавки? Ну как обычно, кавка — это система,
которая состоит из некоторого набора узлов. Эти узлы находятся в каких-то стойках, и каждый
узел называется брокера. Наши топики поделены на партиции, и каждый брокер отвечает за хранение
какого-то набора партиции. Партиции мы считаем небольшими, то есть каждая партиция должна
умещаться в одну машину, в одного брокера, в один диск. Ну и разумеется, реплики партиции разумно
раскладывать по разным стойкам, чтобы учитывать домины отказов, использовать, реализовывать
для того, чтобы повысить доступность наших данных. На что это похоже? Ну это такой более-менее
стандартный дизайн. Вот представьте себе HDFS. Вот там же что-то похожее. У вас есть много датанод,
они хранят кусочки файлов, в чанке файлов. Ну и как и в HDFS, для такой раскладки нужен какой-то
узел, который будет координировать хранение этих данных, вообще понимать, где что лежит,
на каких машинах брокерах хранятся какие партиции, каких топиков. Если мы вспомним HDFS, то вот за это
отвечает у нас кто? Узел NameNode. Такой вот мастер, который хранит метаданные. Карту кластера. В кавке
тоже есть подобная централизация. Тоже есть узел, который называется координатором, который как
раз и следит за всеми партициями, за всеми брокерами и знает про расположение, про отображение партиции
топиков в физические машины. Но, разумеется, кавка должна быть готова к тому, что вот такой узел
координатора откажет. Поэтому этот узел нефиксированный. Поэтому координатор – это не конкретный узел,
это роль. И в случае отказа координатора любой другой узел системы может на себя взять его
функции. Каким образом выбирается координатор? Для этого кавке нужен зукипер. Но не только для этого,
но для разных вещей. Вот кавке нужен зукипер и один из поводов использовать зукипер – это выбор
лидера, то есть выбор координатора. Координатор просто берет зукипер и блокировку. То есть у него узел,
который хочет стать координатором, берет зукипер и блокировку и становится координатором.
Если возвращаться к вопросу про координатора группы клиентов, то опять такая же переходящая роль.
Она не прибита жесткоконкретному узлу, это всего лишь роль отдельного брокера. Каждый брокер может
стать координатором для группы клиентов. Идея понятна?
Понятно. Тогда следующий вопрос, который нас волнует. Что же делать? Отлично, любой узел может
стать координатором, но координатор уже нужно знать, что в системе происходит. Ему нужно знать про
все брокеры, про все партиции, где что лежит. Так вот, координатор это все на своем жестком диске,
конечно, не хранит, потому что координатор может умереть. Мы используем довольно стандартную схему,
мы отделяем точку обслуживания от самих данных. Вот метаданные про кавку, не все метаданные, но часть.
Кавка хранит в зукипере. И когда узел становится координатором, он просто из зукипера учитывает
нужные ему для работы данные. Если координатор меняет распределение данных на кластере, то он
записывает это в зукипер. Ну а зукипер здесь уже является такой отказоустойчивой памятью,
отказоустойчивым диском. Он не ломается. Ну и разумеется, зукипер помогает нам отслеживать,
то есть делать фенсинг, про который мы в прошлый раз говорили. Что делать, когда у нас старый
координатор на самом деле не умер за рип, и мы уже выбрали нового. Ну зукипер с разрывами сессии,
вот опять с этим всем помогаем. Как устроена отдельная партиция, как устроена отдельная
партиция топика? Ну вот есть брокеры, у них есть реплики партиции. Вот у нас красная партиция,
хранится на трех узлах. И эти партиции нужно друг с другом как-то синхронизировать. Вот давайте
посмотрим, как происходит общение с клиентом. Когда продюсер отправляет в партицию сообщения,
ну он отправляет в топик сообщения, но скажу аккуратно, продюсер может отправлять данные просто
в топик, и в этом случае сообщение отправится просто в одну из партиций по хэшу. Либо же продюсер
может делать аккуратнее и направлять данные в конкретную партицию топика, ну потому что он
хочет, чтобы, например, действия одного клиента были строго упорядочены. Вот одна партиция,
это вот логически одно целое, но физически это реплики. И только одна из этих реплик обрабатывает
запрос клиента. Эта реплика называется лидером. Вот на этой картинке реплика один лидер. Она
получает сообщение от клиентов, она присваивает им порядковые номера, записывает их в свою реплику
партиции. А дальше другие реплики. Вот это по смыслу похоже чем-то на RAFT, но организованно немного не
так. В RAFT лидер получает команду от клиента, кладет в свой лог и раздает всем. Здесь же просто
каждая реплика партиции, независимо от других реплик, в своем темпе пулит данные из лидера. И
когда она запрашивает в него, скажем, offset с третьей позиции, хотя не похоже, что третья позиция под
линей лога, то лидер понимает, что эта реплика дошла уже до такого префикса, ну и фиксирует ее
прогресс и понимает, что сообщение лежит на большинстве. Ну аккуратно, в случае кавки мы говорим не
про большинство, мы говорим про некоторые quorum синхронных реплик. Вот с каждым набором реплик,
с каждой партицией связано такое состояние, называется quorum state кавки, и она образована
тройкой. Кто сейчас лидер? В какой эпохе это лидер? Ну потому что опять, как и в рафте,
как и в мультипакс, во всех протоколах консенсуса нужно блокировать старых лидеров, и мы их
блокируем просто по порядковому номеру эпохи. Каждый лидер, он лидер в своей эпохе. И мы считаем,
что запись надежно зафиксирована в партиции топика, когда лидер отреплицировал ее на конкретный
quorum, он называется ISR. Вот этот ISR, это вот в данном случае первые две реплики, это не просто там
любое большинство, а вот конкретное большинство, ну точнее даже не обязательно большинство,
просто конкретный набор реплик. И если лидер умирает, если он отказывает, то перевыбирается новый
лидер вот из этого quorum, потому что в этом quorum-е гарантированно есть все зафиксированные
записи, все записи, которые подтверждены клиентам. Идея понятна?
Окей, тогда как именно выбираются лидеры, как это все работает, где хранится этот quorum
стоит, вот вся эта информация хранится в зоокипере. И лидеры выбираются, точнее не так, нужно выбирать,
нужно обнаруживать, что в системе отказал лидер какой-то партийцы, ну то есть допустим отказывает
машина брокер, она являлась лидером для какого-то количества партийций, вот нужно об этом узнать
всем партийцам, которые были задействованы, и перевыбрать нового лидера. Так вот, за это все
отвечает координатор, и он это все делает с помощью зоокипера. Вот как в прошлый раз рассказывал,
зоокипер может применяться для задачи обнаружения сбоев, там есть эфемерные узлы, которые живут до
тех пор, пока жива сессия клиента, который эти эфемерные узлы создал. Так вот, каждый брокер
создает в зоокипере в директории с кавкой эфемерный узел, который говорит, что я брокер жив. Сам
координатор с помощью зоокипера опять же хранит информацию о том, какие партийцы, каких топиков
лежат на каких брокерах, и если вдруг координатор, подписавшись на директорию с эфемерными узлами
брокеров, узнает о том, что какой-то брокер отказал, то он понимает, для каких партийцей этот
брокер был лидером, читает их аэсары из зоокипера, и для каждой пострадавшей партийцы просто бампает
эпоху и перевыбирает нового лидера. То есть детектор отказов и перевыбор лидера он централизован
в кавке для разных партийцей, и всем этим занимается координатор с помощью зоокипера.
Вот такая конструкция. То есть это по смыслу похоже на мультипаксис или RAF, но техники те же,
просто декомпозировано все немного по-другому. То есть у нас централизованный тектор сбоев,
у нас централизован перевыбор лидера для разных партийцей, и состояние каждой
партийцы хранится надежно еще в зоокипере. Метод данной каждой партийцы QuorumState хранится
централизованно надежно в зоокипере. Любопытное замечание про нынешнее и будущее состояние
капки. Вот как я сказал, координатор играет очень важную роль. Он отслеживает отказ узлов
лидеров партийцей, переназначает новых лидеров, распределяет партийцы между брокерами,
и все свое состояние для отказа устойчивости координатор хранит в зоокипере. Если узел,
который играет роль координатора, отказывает, то новый узел подхватывает освободившуюся блокировку
координатора в зоокипере и загружает в себя все это состояние. Так вот, есть идея, сейчас мы найдем.
Кавка хочет в конце концов отказаться от зоокипера. С одной стороны, вот начинать строить сложную
систему, когда зоокипер есть, гораздо проще. Мы ему делегируем задачу консенсусом. Мы там
поддерживаем монотонную историю QuorumState для реплик, мы там выбираем лидера, обнаруживаем отказы.
В общем, с помощью этого одного зоокипера мы реализуем много-много разных согласованно реплицированных
партиций. Но в то же время у этого дизайна есть свои недостатки. Во-первых, такая банальная проблема,
что зоокипер это просто довольно большая зависимость кавкина, то есть вы должны поддерживать не одну
систему кавку, а две системы кавка плюс зоокипер. А во-вторых, вот такой вот дизайн, он вам увеличивает
время восстановления после сбоев. Если вдруг координатор отказал, то выбрать нового, допустим,
не так уж и сложно, но новый координатор должен загрузить довольно большое состояние себе в свою
машину зоокипера. Это не то чтобы очень страшно, это не влияет на корректность. Мы
никакие данные не потеряем, потому что зоокиперы они хранятся надежно, ну и система доступна до тех
пор, пока доступно большинство узлов зоокипера. Но все же восстановление после сбоев замедляется.
Так вот, идея в том, чтобы просто вместо зоокипера использовать рафт, чтобы координатор был
реплицирован с помощью рафта, и тогда, если один узел в рафте отказывает, то другой узел он уже
в себе содержит копию всех данных, ну просто потому что так рафт устроен, как вы помните, и время
восстановления после сбоя центрального узла, оно уменьшится. Но что любопытно, вообще говоря,
отказ координатора, он не является, координатор здесь не является прям вот точкой отказа всей
системы, потому что если координатор умер, в принципе, это не мешает реплике обслуживать записи,
не мешает зоокиперу обслуживать записи в отдельные партиции. Вот на быстром пути,
на пути записи данных зоокипер не участвует. Зоокипер участвует тогда, когда отказывает лидер,
или когда отказывает какая-нибудь из реплик ASR. Вот если отказов нет, просто отказал координатор,
то система может продолжать работу и обслуживать записи чтения клиентов.
Теперь по поводу состояния клиентов. Ну вот что хранится в зоокипере? Карта кластера,
распределение партийцы по брокерам, хранится метаинформация для каждой партийцы о статусе
репликации. Кстати, пока мы о репликации не ушли, если кому интересно, кто ходит в субботу меня
слушать, то вот для кавки разработчики написали спецификацию на тело и плюс, как именно устроена
репликация. Там можно посмотреть на все события, которые могут происходить, и как именно кавка их
вырабатывает. Вот в зоокипере хранится карта распределения данных, хранится блокировка
координатора, хранится состояние репликации для каждой партиции. Вот то, что было нарисовано
вот этот лидер эпоха и ISR. Не расшифровал, кажется, ISR, это инсинг-реплики, то есть реплики,
которые синхронизированы с лидером, которые содержат все эти же данные. И на старте, но вот
давным-давно кавка, помимо вот этих данных, хранила в зоокипере еще и оффсеты клиентов.
Но мы договорились, что мы делегируем хранение стейта каждого клиента самой кавки, но вот кавка
тоже должна это хранить отказаустойчиво. Почему бы ей не хранить это в зоокипере? Но клиентов
становилось много, партийцы становилось много, оффсеты обновляются часто. Вот вот это состояние
обновляется, тогда как кто-то отказывает. Довольно редко может быть. А клиенты продвигают вперед
оффсеты часто. И в какой-то момент система просто перестала справляться с нагрузкой. Ее нужно
было масштабировать в этом месте, в месте хранения партийцы. Так вот, кавка идет по интересному
пути и по важному для нас, потому что это нам позволит сделать в конечном итоге семантику
и экзорпеванс. Кавка говорит, давайте мы будем хранить данные, оффсеты для клиента для партийцы
в виде топика. То есть у нас будет специальный служебный топик, в котором данными будут не вот
какие-то записи пользователя, а наши служебные данные мы будем для пользователя, запятая партийцы,
хранить там оффсеты. То есть когда пользователь, когда клиент говорит commit sync, вот здесь вот,
то кавка в служебный топик записывает текущий оффсет для данного клиента, для партийц,
которыми он работает. Но поскольку топики это сущность, которая масштабируется внутри кавки,
то значит хранение оффсетов не будет больше узким местом. Правда есть один нюанс, если мы просто
записываем апдейты в такой монотонно растущий топик, то чтобы прочесть текущие оффсеты клиента,
нужно будет этот топик прочесть с самого начала, что конечно не эффективно. Поэтому в кавке есть
служебные топики, какой-то служебный механизм компактификации. И вот это одна из причин,
по которой в модели данных кавки лог это не просто массив из сообщений, это массив из записей,
ключ значения. Вот в случае компактификации внутри топика работает фоновая процедура,
внутри партиции топика работает фоновая процедура, которая в этой партиции забывает старые значения
по одному и тому же ключу. Вот мы можем забыть вот такую запись, потому что у нас есть K1V2,
и K1V2 есть. Эту запись из топика можно ударить. Таким образом, чтобы прочесть оффсеты клиента,
нам не нужно читать гигантский топик, нам достаточно читать топик, который будет уже гораздо меньше,
потому что оффсеты там случаются. Комит оффсетов может происходить каждые несколько секунд,
но для одного клиента, для одной партиции у нас будет одна запись в этом топике.
Почему это важно? Потому что, во-первых, этот механизм позволяет кавке масштабироваться,
а во-вторых, он нам поможет достичь семантики exactly once, которой нам не хватало вот здесь.
Вот у нас есть две строчки, мы пишем что-то в базу данных и мы комитим оффсета, то есть пишем что-то
в топик кавки. Мы можем упорядочивать две эти строчки двумя способами получить семантику,
либо at most once, либо at least once. В идеальном мире мы бы хотели сделать и то, и другое атомарно,
мы бы хотели записать в базу, плюс закомитить оффсеты, то есть сделать запись в служебный
топик кавки. Но эта задача безнадежная, то есть система, в которой мы пишем данные про кавку,
ничего не знает, а мы хотим какую-то распределенную транзакцию, эта система транзакций может не уметь.
Но все же мы можем достичь exactly once в некотором ограниченном наборе случаев, когда мы читаем из
топика, обрабатываем данные и их пишем не в какую-то внешнюю систему, а в другой топик в самой кавке.
Вот в этом случае мы уже можем достичь семантики exactly once, причем смотрите каким способом. Вот если мы
читаем данные из топика и пишем с другой, читаем данные из топика и комитимом оффсета, сейчас
аккуратно, читаем данные из топика, процессируем их, пишем выходной топик и фиксируем прогресс,
комитим оффсеты, то что это на уровне кавки, внутри кавки происходит? Когда мы пишем данные выходной
топика, мы пишем в какой-то там какие-то партиции того самого топика, когда мы комитим оффсеты для
входного топика, то мы снова пишем какие-то записи уже в служебные партиции, в партиции служебного
топика. То есть комит оффсетов и запись данных выходной топик – это на уровне кавки просто
записи в два разных топика. Так что если вдруг мы на уровне кавки научимся делать транзакции,
то есть мы научимся атомарно писать в несколько топиков, несколько партиций, то тем самым мы
сможем достичь семантика exactly once при процессинге из кавки в кавку. Понятно?
Да. Отлично, спасибо за обратную связь. Тогда следующий шаг – мы хотим сделать транзакции в
кавке. Ну, давайте сначала делать их как-то наивно. Вот можно про правую часть акценки не думать
пока. Вот мы продюсер. Ну, во-первых, как вообще выглядит работа с транзакцией в кавке? Вот можно
посмотреть еще раз на продюсера, на транзакционный пример. Вот в случае, когда мы писали просто данные
в кавку, то вот мы в цикле делали send. Мы можем делать аккуратнее. Ну, во-первых, можно завести в
кавке так называемого idempotentного продюсера, который даже в случае ретраев не будет дублировать
данные. Но это похоже все на протокол TCP, когда у нас есть sequential number, когда у нас есть
идентификатор клиента, и партия запоминает для каждого продюсера до какого своего логического
индекса он записал. Но про это мы не успеем сейчас. Давайте лучше про транзакции. Вот в случае,
вот самый общий API, этот транзакционный API, мы на месте продюсера начинаем транзакцию. И мы в
пределах этой транзакции пишем какие-то топики. Сейчас надо найти какой-то неинтересный пример,
потому что здесь нет аффсетов. Давайте найдем пример поинтереснее. Да, вот здесь уже поинтереснее.
Вот смотрите, у нас есть клиент, который является одновременно и консюмером, и продюсером. То есть
он читает данные из какого-то топика, а потом он начинает транзакцию. Он данные, которые он
прочел, пишет выходной топик, и он в рамках своей транзакции фиксирует аффсеты для себя в роли
консюмера и говорит commit transaction. Тут разные API, send и send-offset, но под капотом и то и другое,
это просто записи в топике. Это в топик с данными пользователей, а это в служебный топик с аффсетами
и компактификацией. Так вот, как же сделана запись от амарной в несколько топиков?
Протокол простой. Когда мы под транзакцией делаем сенды, то мы эти сенды маркируем с помощью
флажка транзакционности. То есть мы просто добавляем в партизу выходного топика очередное
сообщение. Просто ставим на нем зарубку такую, что это запись под транзакцией. И когда консюмер
будет этот топик читать, он не имеет права вот такую транзакционную запись сразу выдать клиенту.
Потому что непонятно вообще, закомитится ли эта запись или нет. Что если продюсер просто откажет
до выполнения строчки 20. Поэтому когда консюмер пишет данные под транзакцией, он просто помечает
эти записи как транзакционные. Когда консюмер их читает, он их не выбрасывает, разумеется,
но он их накапливает у себя в памяти. Как же происходит фиксация транзакции? Продюсер должен
просто в каждую транзакцию поместить специальный маркер. Маркер комита. Для каждой транзакции
должен быть выбран уникальный идентификатор, который отличает транзакцию других. И продюсер,
когда он делает commit transaction, в каждую партицию каждого топика, который он писал,
должен положить маркер комита, который говорит, что теперь транзакционные записи, которые шли
раньше, которые были помечены ID транзакции, теперь они закомечены. И логически все эти записи
попадают разом вот сюда. Идея понятна? Ну да. Тогда нужно подумать, что в этом подходе не работает.
Ну, прям так делать нельзя, конечно. Ну, клиент может commit не записать, а перезагрузиться. Ну,
во-первых, клиент, во-первых, продюсер может не завершить транзакцию и отказать. В этом случае
в топике останутся транзакционные записи, которые в памяти накапливают клиент,
но и у клиента растет расход памяти. Это неудобно. Память у него может просто переполниться. То есть
для каждой транзакции нужно вообще принять решение. Она все-таки в конце концов докатится,
или все-таки ее нужно откатить и эти записи выкинуть. Другая проблема. Ну вот, продюсер,
который делает commit транзакции, то есть если продюсер не сделал, вообще не дошел до этой строчки,
то транзакцию нужно откатить. А вот если продюсер дошел до этой строчки и, скажем, в две партиции
успел записать маркеры commit, а в третью не успел, то получилась довольно странная ситуация,
что какие-то констюмеры транзакции уже видят, а какие-то еще нет. Ну и в третьих, то есть это
проблема с отказоустойчивостью, и в третьих есть проблема с атомарностью. Вот представим,
что у нас есть два продюсера, которые выполняют две транзакции, и вот они пишут в одни и те же
партиции. Но так получилось, что первый продюсер свой комит, свой маркер комита написал раньше,
чем второй продюсер, в эту партицию, а в эту партицию сначала написал маркер комита второй
продюсер, а потом первый. В итоге у нас как будто бы в две партиции комиты случились в разном порядке.
Понятно проблема? Просто потому, что каждая партиция, это вот такой отдельный, отдельная
группа реплик, и в ней лидеры по-разному упорядочивают эти самые комиты. Вот все эти
проблемы решаются тем, что мы переносим роль координатора транзакции, то есть роль координатора,
тот кто управляет комитом. Вот мы переносим роль координатора транзакции с продюсера,
который неотказа устойчивый, на узел брокер. Мы просто говорим, что один из брокеров является
координатором транзакции. И если мы пишем в одни и те же топики, то это будет один и тот же
аккуратнее скажу. У нас есть партиции, в которые мы пишем, и оффсеты этих партий хранятся в служебных
партициях. У нас есть для транзакции набор партий, в которые мы пишем вот эти маркеры,
данные и маркеры. И для одних и тех же партий координатором будет выбран один и тот же узел,
один и тот же брокер кластера кавки. И что делает теперь продюсер, когда он говорит комит? Он говорит
этому брокеру координатору транзакции, что я готов закомитить свою транзакцию. И теперь,
ну во-первых, когда продюсер начинает транзакцию, он с самого начала говорит об этом координатору.
Зачем? Затем, что если вдруг продюсер напишет данные в пределах транзакции, а потом откажет,
то транзакцию рано или поздно нужно откатить. Нужно все-таки в топик написать, что транзакция
с таким ID просто отменилась, и, пожалуйста, консюмеры, забудьте про те записи, которые вы прочитали
и бафилизировали у себя. Вот, поэтому координатор, вообще говоря, должен запомнить, что транзакция
началась. Потому что сам координатор – это же тоже не конкретный узел. Брокер, который является
координатором, может это показать, и координатор переедет в другое место кластера, на другого
брокера. Так вот, новый брокер должен вспомнить, что была транзакция, и что продюсер рано или поздно
должен ее закомитить. А если он этого не сделает, то вот я, новый координатор, новый брокер, должен
эту транзакцию отменить. Поэтому для транзакции снова нужно иметь некоторое надежное состояние,
нужно поддерживать ее статус. И, опять же, можно было бы это сделать из-у киперя, но это было бы
снова не масштабируемо, поэтому координатору транзакции нужен собственный лог, отказу устойчивый,
в котором он будет писать события, что вот транзакция началась, или вот транзакция готова
закомититься, или вот транзакция закомитилась, потому что мы записали все маркеры. И вот этот лог событий
для транзакции снова нужно хранить надежным, масштабируемо, отказу устойчиво, ну а для всего
этого мы снова можем использовать топик кавки служебной. Ну то есть кавка, она здесь поддерживает
сама себя. Вот ей для функционирования нужно хранить аффсеты, нужно хранить лог транзакции,
ну и вот все это в свою очередь тоже топики кавки. Окей, теперь продюсер на старте транзакции говорит
координатору, пожалуйста, зафиксируй мою транзакцию в логе. Когда продюсер говорит commit, он говорит
координатору, что вот я готов закомититься, и координатор пишет сообщение, пишет, что транзакция
перешла в статус prepare. После этого координатор пишет флажки commit в каждую выходную партизу.
Если вдруг координатор отказал, написал два флажка и отказал, не написал в третий, то выберется
новый координатор, он из этого лога прочтет, что транзакция была на фазе prepare уже, прошла
через нее, и ее нужно докатить, и запишет флажок в третью партизу. Ну и поскольку координатор будет
один для разных транзакций, то этот координатор гарантирует, что флажки разных транзакций будут
записаны в эти партиции в одном и том же порядке. Ну что, мы получили таким образом отказоустойчивые
распределенные транзакции на несколькими топиками, и по модулю того, что мы хранили
оффсеты для клиента в отдельном топике, мы получили и семантику exactly once. То есть мы
здесь гарантируем, что для каждой записи входного топика мы гарантированно пройдем
только одну запись выходного топика. Ну как, понятна идея? Понятно. Вот, но у нас остается все еще
ограничение, что мы не можем работать так с внешним состоянием, но... где это было? Вот здесь. Но
кавка и здесь нам кое-что предлагает на самом деле. Вот пусть мы хотим делать процессинг, вот что мы
умеем сейчас с помощью кавки делать? Мы можем читать из топика, обрабатывать в памяти и класть в
топик выходной, и иметь семантику exactly once. А что если наш процессинг сложнее? Что если мы
хотим читать из топика, накапливать какое-то сложное состояние, именно накапливать его? Ну,
то есть, не знаю, высчитывать какие-то средние по потоку, а потом с учетом этого писать данный выходной
топик. То есть у нас узел, который находится между входным и выходным топиком, он имеет свое
состояние, он не stateless. Вот кавка позволяет организовать и такие вычисления. То есть,
где каждый узел, каждый узел процессинга, он обладает собственным персистентным статусом.
Каким образом это делается? Ну, это делается с помощью отдельного framework, который называется
Kafka Streams. Вот, и в этом framework вы можете писать, ну, вот такие вот программы. То есть,
у вас есть некоторый топик с, не знаю, с документами, и вы начинаете его процессить,
разбивать их на слова и делать count by key. Ну, то есть, вы делаете такой word count,
который вы, наверное, делали уже в GoProduce, но только в кавке без бачинга. То есть вы читаете
топики, процессите их, сплитите сразу, и все эти записи текут дальше. Ну, вот для того, чтобы,
то есть, для вот такой программы мы на уровне Kafka Streams встроим такой граф вычислений,
и в этом графе вычислений есть узлы, которые просто читают входной топик и сплитят его. Ну,
какую-то картинку сейчас можно показать. А есть какие-то узлы, которые считают в каком-то
скользящем окне частоту слов. И вот для случая, ну, и в модели Kafka Streams есть два понятия stream
и table. Вот stream, вот вы можете создать stream, который получается обработкой данных какого-то
другого стрима. Ну, есть один топик на выходе физически, есть другой топик на выходе. И где-то
на узле вашего кластера запускается такая stateless задачка, которая читает один топик,
процессит его, поднимает в uppercase какие-то поля и записей. Ну, потому что теперь уже топики будут
в Kafka Streams схематизированы, то есть у каждого топика, у каждого сообщения будет какая-то
фиксированная схема. Вот. И пишет на выход и делает это с помощью транзакций, то есть семантических
execuants. Мы можем легко такое вычисление запустить на кластере. И Kafka Streams нам дает декларативный
язык для того, чтобы удобно описывать эти вычисления. Чтобы вы не код на джаве писали, а вот такое
декларативное описание. Но что если мы хотим между двумя топиками поместить узел, который будет
уже вычислять что-то с учетом, если здесь есть пример, с учетом средних. Вот ему нужно более
сложное состояние поддерживать. Как сделать это? Понятно ли вам? Возможно, свои промежуточные
расчеты писать в отдельный топик? Вот смотрите, как организован в общем случае такой, если мы
говорим про произвольный процессинг, то у нас есть граф, где истоки это какие-то топики,
промежуточные узлы это вычисления, которые читают из входного топика, пишут что-то в выходной топик,
и в общем случае накапливают какое-то состояние у себя. Но вот если состояния бы не было, то мы
просто использовали бы транзакции, которые мы только что обсудили, и получили бы execuants
процессинг. Такой граф, в котором каждая запись исходная проходит через граф ровно один раз.
Но если у нас есть стейт, то при рестарте, казалось бы, мы не можем просто перезапустить
этот процессинг на другой машине, потому что там стейт потерялся. Поэтому стейт тоже должен быть
отказоустойчивым. Ну а этот стейт, он вообще говоря произвольно устроен, таблица со средними
значениями, как здесь. Но опять же, даже произвольное состояние можно представить в виде топика.
Вот мы offset представили в виде топика, мы состояние transaction manager представили в виде топика,
мы теперь и произвольную таблицу со средними значениями тоже представили в виде топика. Каким
образом? Ну очень просто, у нас любое состояние, любое состояние конкретное, там таблица может
быть представлено просто в виде changelog. Вот скажем, у нас есть состояние таблицы счетчика,
сколько встречается каждое слово. Вот с одной стороны, это вот такая вот таблица, с другой
стороны, это вот такой changelog. И если мы теперь представим вот здесь изменение этой таблицы,
этого состояния в виде changelog, где мы добавляем очередное значение к сумме и очередное значение
к знаменателю, то мы получим состояние этой таблицы со средними значениями сенсоров в виде просто
служебного топика. Этот топик, опять же, пользователю не виден, пользователь просто создает таблицу,
а под капотом вот этот create table разворачивается в создание служебного топика. И когда
когда мы теперь хотим пропустить очередную запись через узел обработки, то мы с одной стороны должны
записать данный выходной топик, обновить состояние узла и зафиксировать offset на входном топике. И вот
три этих действия, это на самом деле всего лишь три записи в три разных топика, в changelog состояние,
в топик offset исходного входных данных и в топик с данными для выходных данных, в топик выходных
данных. И мы должны сделать все это автоматно. Но для этого у нас уже есть транзакции, мы их
поддержали. Так что мы теперь используем транзакционные IP, мы используем служебные
топики для транзакций, для offset транзакций и стейта, и таким образом мы можем делать отказоустойчивую
stateful вычисления. То есть мы можем, и Kafka это делает в виде framework Kafka Streams,
который позволяет вам более-менее декларативно описывать граф вычислений, а дальше это декларативное
описание развернется в конкретный граф с процессингом на отдельных узлах кластера, и отказ любого из
этих узлов Kafka переживет. Просто вычисление перейдет на другой узел и продолжит ровно с того
места, где оно остановилось. Восстановит свой стейт, там, не знаю, таблицу со средними значениями и побежит
дальше. Вот, ну и все потому, что мы научились делать транзакции, и мы все свои структуры данных
представили в виде топиков. Вот одно плюс второе дает нам в общем случае stateful exactly once вычисления.
Что скажете?
Я бы сказал, что на сегодня это все. Я, в смысле, по плану рассказал все, что хотел и так израсходовал
времени больше, чем у меня было. Если у вас есть вопросы, то самое время их задать.
Я хотел точнить, для чего ключи нужны, когда мы пишем именно как какой-то пользовательский код.
Записи с одним ключом удаляются старые или нет?
Ну ключи интерпретируются, вот смотри, есть механизм компактификации, когда вот записи,
когда новые записи по тому же ключу стирают старые записи. Ну не то чтобы прямо стирают,
они добавляются, а потом фоновый процесс старые записи выкидывает, сборку мусора делает.
Если же мы говорим про просто произвольный топик с данными, то никакой семантики у ключа значения нет.
Это просто такая фиксированная структура из двух полей.
Ну можно себе представить, что ты, например, если мы собираем логи, скажем, в кавке,
вот логи всех там действий пользователя, которые пишутся на сотнях, на тысячах,
то можно писать в value значение записи, саму записи с лога, а в ключ, там, не знаю,
метаинформацию, с какой машины она доехала. Или если у тебя, скажем, в топике все-таки
разнородные данные хранятся, в смысле схема данных разная, то есть ты не знаешь заранее,
как десеревизовать эти данные, то есть там разные данные могут быть, то ты можешь
положить в ключ, там, схему этих данных. Ну короче говоря, как ты именно используешь
вот этот ключ значения, это уже твоя воля. Да, а можно сделать так, чтобы пользовательский
топик обрабатывался тоже с семантикой уникальных ключей? По-моему, да, можно, но вроде нет никаких
причин, почему вы это будете сделать. Ну, например, складывать вместо ключа ID-шники,
таким образом добиваться уникальности данных. Ну, смотри, кавка, я не помню, она гарантирует ли
тебе, что она прям, вот, при чтении ты не увидишь дубль, и кажется, что такой гарантии у тебя все же
не будет. Ну, потому что ты начал читать, прочел старую версию, потом положили новую, ты прочел
еще раз более новую версию. Ну, то есть это такая оптимизация, она просто позволяет тебе схлопывать
лишние данные. Ну, это все-таки не стоит думать в этом смысле про кавку, про киварию хранилища,
это не киварию хранилища совсем, а на другой сценарии, когда ты очень-очень много пишешь подряд,
поэтому это ивристика для того, чтобы компактить данные, которые вот так хорошо компактятся. Если же
мы говорим про произвольные данные, то, разумеется, они тоже расти бесконечно не могут, и ты должен
настроить, но вот как именно ты готов про старые записи забывать. Ну, скажем, ты готов хранить
данные в топике, данные в партиции до тех пор, пока размеры партийцы не превысит, там не знаю,
100 гигабайт или три дня. Вот что наступит раньше. А можно настроить, чтобы до чтения, например,
они хранятся? Честно говоря, я в смысле по аффсетам клиента, честно говоря, я не помню,
но честно говоря, по своему опыту я не видел, чтобы кто-то как-то так делал. Просто настраивают,
ну не знаю, если у тебя данных относительно немного, ты хранишь там три дня эти данные, и если там ты
за три дня не способен их прочесть, то значит у тебя что-то с процессом сильно не так, ну то есть
у тебя просто есть запас по времени, и если ты прямо его израсходовал, то значит, что у тебя твой
сервис несколько дней не работает, это довольно печально. Я правильно понимаю, что Kafka в основном
используется для записи каких-нибудь real-time данных, каталоги, какие-то статистики, которые
просто нужны там для аналитики в первую очередь. Используется для разных вещей, для разных, не
только для, ну да, если у тебя вот прям, не знаю, какие-то данные, какие-то транзакции пользователя,
ну транзакции пользователя ты, наверное, вырабатываешь как-то иначе. Да, это в первую очередь
какие-то статистики, аналитические данные, какие-то сенсоры, ну вот что-то подобное.
Если у тебя задача более, ну это такая надежная труба, по которой ты данные переправляешь. Если у
тебя какая-то специфичная работа с этими данными, то ты просто, ну не знаю, работаешь с какой-то
системой напрямую, с базой данных напрямую, транзакции выполняешь. Kafka тебе здесь не нужно.
Да, спасибо.

ну да значит наверное мне надо будет сегодня немножко как сказать помедленнее
идти потому что кажется что тот скорость то скорость тот темп который у нас имеет
место он все-таки быстровато так вот ощущение меня создается поэтому наверное я даже вот
позволю себе какие-то совсем простые вещи рассказать которые не хотела рассказывать но
уж извините как-то все-таки надо какие-то вещи вот прям закрепить что вы их точно знаете и
потом мы их значит будем значит неоднократно использовать хотя это классические вещи в кустах
теории вероятности они тоже есть вот ну что делать мне потребуется они многократно поэтому
получу значит центрально-предельные теорема вот такая тема но это как сказать надеюсь быстро мы
ее пройдем центральная предельная теорема центральная предельная теорема предельная
теорема ну такая жемчужина теория вероятности ну как она формулируется ну тут как есть совокупность
независимых одинаково распределенных случайных величин xk у этих случайных величин пускай есть
ну в общем они достаточно хорошие что у них есть характеристическая функция
которая представляется в виде единица плюс я напомню что это такое это мат ожидания где в
степени и tx то есть их случайно величина ну xк ты хорошо вот и значит вот у них соответственно
есть характеристическая функция которая представляется в виде значит ну для простаты
будем считать что они имеют нулевой мат ожидания поэтому я не буду здесь значит в них мат ожидания
0 мат ожидания 0 ну и ладно пускай будет м да мат ожидания м значит тут будет сейчас и tm соответственно
плюс значит ну в общем я раскладываю в ряд и мне нужно чтобы у них было разложение в ряда второго
порядка включить то и вот что я делаю я просто пишу что это есть мат ожидания просто вы понимали
логику того что я делаю значит в ряд первый слагаемый и tx к плюс и ты xк в квадрате на два
факториал мы плюс у маленькая t в квадрате вот такая вот формула дата тут квадратная
скобка мы то есть я считаю что это имеет место и потом применяю применяем от ожидания к слагаемым
мы получаем что значит и будет минус t в квадрате на два то есть тут минус t в квадрате на два
значит кто там у нас получается ну вот тут будет значит маст второй момент мат ожидания xк ну я не
знаю как его обозначить поэтому вот я так просто напишу вот он конечен то есть мы считаем что это
число конечно иначе будет проблема плюс у маленькая t в квадрате вот мы считаем что характеристическая
функция каждого слагаемого удовлетворяет вот такому значит соотношению ну и что мы можем
написать можем написать что суммой катах это предположение минус m на n ну и обычно мы пишем
большое как одного до n поделить на значит ну давайте ведем дисперсию сигма в квадрате равняется
я от x к в квадрате вот это значит ровней ну по сути это есть вот что это такое да ну то есть
если мы считаем что я тк в квадрате конечно значит дисперсии есть ну и вот мы здесь пишем значит
n сигма корень sigma sigma в квадрате n корень вот и вот это сходится по распределению по
распределению при n стремящимся к бесконечности к 0 1 вот вот это называется центральной предельной
теоремы есть всякие варианты как ее догазом доказывать с помощью метода липунова метода
сумм линдеберга а с помощью аппарата характеристических функций с помощью метода
ренорм группы например книжки кораллово синая в общем тьма способов как его какая геометрия есть
геометрический взгляд на все эти вопросы которым мы вероятно будем говорить восходящий вот к
по анкаре наверное левим но сейчас нам достаточно удобно будет воспользоваться аппаратом характеристических
функций основная сказать ишка вот этого аппарата она заключается в том что phi от x плюс y от t если
соответственно x и y независимые случайные величины это есть мат ожидания и от и x плюс y но поскольку
это независимые случайные величины то тогда мы можем написать следующее что раз случайные
величины независимые то и любые функции от них независимы и значит мат ожидания независимых
выражения равняется произведению мат ожиданий ну там даже более слабо достаточно некоррелированность
то есть мы получаем что имеет место вот такое соотношение вот ну и как следствие мы можем
написать что все это равно все это равно вот просто phi x на t на phi y на t то есть характеристическая
функция сумма случайных величин равняется произведению характеристические функции но этот
факт вам должен быть известен искусством от анализа когда мы говорим о преосвоисток преобразования
фурье собственно вот это свойство вы знаете ну дальше собственно давайте возьмем случайную
величину y которая есть соответственно x к минус m поделить на sigma что про эту случайную
величину можно сказать можно сказать что phi от y от t равняется ну характеристическая функция
единица плюс у нее мат ожидания будет 0 вот значит тут будет минус t в квадрате t в квадрате пополам
и плюсу маленько t в квадрате вот вот так вот да будет замечательно это просто характеристическая
функция отдельного слагаемого замечу что это и совсем не означает что эта функция характеристическая
что она скажем с той в точности соответствует нормальной случайной величине чем примечательно
нормальная случайная величина вот как бы то есть вообще говоря какая будет функция у нормальной
случайной величины значит если случайная величина кси принадлежит нормальному закону
n01 то характеристическая функция кси от равняется значит е в степени минус t в квадрате пополам
но если стандартная нормальная закон то есть и если мы разложим эту функцию в ряд если мы разложим
эту функцию в ряд то как раз и получится что это есть единицы минус t в квадрате пополам и так
далее но как бы совпадение ну не мне но как бы ну да но это еще ничего не означает то есть просто
что вы почувствовали к чему мы стремимся мы стремимся показать что вот этот вот предел с
точки зрения характеристических функций мы сейчас пытаемся доказать этот результат просто вот так
что фи от вот этого агрегата от и сходится при n стремящимся к бесконечности но тут фи n да
будет при стремящихся бесконечности к фи нормальные случайные величины вот где это есть n01 вот что мы
хотим показать и если эта сходимость уже сходимость не случайная будет скажем равномерная в окрестности
точки 0 то этого будет достаточно или скажем так это функция непрерывно просто в окрестности точки
0 есть поточная сходимость эта функция непрерывно она непрерывно в нуле ну вот у нас будет результат что
отсюда следует слабая сходимость вот вот это будет иметь место то есть нам надо вот исследовать
вопрос сходимости в пространстве вот полученную с помощью преобразования фурье то есть доказать
сходимость характеристических функций но смотрите этот трюк он реально полезный то есть я его
рассказываю не потому что я думаю что вы не знаете что такое центральная предельная терема я хочу
показать что наверное самое главное утверждение курса теории вероятности и вообще за то что как
бы является жемчужины современной математики оно получается за там не знаю 5-7 минут если вы
более-менее в целом знаете неплохо математику вам не обязательно это помнить вот я и не помню
центральный прием вы это видите на я так сказать я как бы знаю что я должен получить и так сказать
по ходу дела вы сейчас будем вместе со мной это не получать вот это действительно делается то есть
надо Night Kit общие принципы они кто как что детали доказательства тысяч мы сейчас просто
получим доказательства понимаю что значит сумма случайных величин а именно к этому мы свели вот
эту штуку то есть мы свели изучения вопроса к изучению суммы случайных величин ну значит
эта сумма ее характеристическая функция этой суммы будет произведение характеристические
функции слагаемых вот значит я перехожу сюда то есть мне надо изучать характеристическую
функцию суммы и суммы игрекатых кат одного до n от т но я как бы забыл про корень из
n поэтому мне надо написать здесь корень из n а вот это уже немножко другая
история то есть мне надо на самом деле сделать следующего меня есть некоторая
величина y вот которая один на корень из n от t а я хочу изучать случайную
величину просто y что мне для этого надо сделать мне для этого надо еще
раз написать определение и от и а тут y на корень из n и обратить внимание на
то что это тоже самое что мат ожидания я и ты на корень из n y вот а это уже есть
просто-напросто значит и от y на t на корень из n таким образом моя задача
заключается в том чтобы изучить поведение фи от сумма игрекатых реально
уже вот этой вот суммы ну давайте здесь напишу фи от сумма игрекатых от t на
корень из n от t на корень из n как одного до n но как я уже говорил вот
там такой какой-то стиль кого-то мне напоминает но неважно вот там я говорил
значит давайте это как-то звездочки обозначим вот и звездочки и звездочки я
могу заключить что это равняется произведению произведению фи игрекатых
а и на корень из n соответственно к от одного до n но это же все одинаковые
функции они у нас выписаны вот здесь значит и получается что это есть единица
минус t в квадрате пополам только теперь t в квадрате норме вот-вот
надо писать t в квадрате на n на 2n получается да и все это в степени n ну я
кое-что забыл плюс и маленько от t в квадрате плюс и маленько от t в
квадрате ну тут конечно t на давайте так t в квадрате на n на n вот так вот так и
теперь все это в степени n если он стремится к бесконечности только
то что вы можете сказать про эту штуку
экспонента показателем с показателем минус и квадрат пополам все правильно
и квадрат пополам все я все что надо доказал еще раз вот много мы потратили на
это времени да не очень много а что мы получили на выходе а мы получили на
самом деле что чтобы ну как бы доказывать какие-то результаты нужно
вообще говоря иногда просто на задачу правильного
ракурса смотреть вот в пространстве случайных величин сумма случайных
величин ну если плотностью описывать например то плотность распределения
суммы случайных величин это свертка это такое довольно сложное
преобразование но оно превращается в произведение когда мы переходим в
пространство фурье поэтому понятно что удобнее работать смотреть на это все с
точки зрения ну как бы преобразование фурье ну вот получили чем привилегировано
чем привилегировано нормальное распределение да приблизительно тем чем
привилегировано экспонента и чем привилегировано ну вот как бы вот такой
закон чем он привилегирован а тем что он возникает естественным образом то есть
просто привилегировано не нормальное распределение вот это а вот это
привилегировано потому что оно получается из первого замечательного предела просто мы как бы
сказать мы пришли к преобразованию фурье вот в этом пространстве получен
с помощью вот такого перехода и уже действительно получили что восстанавливая
происходные законы распределения это будет нормальный но неважно какой какие
были игрекаты то есть все эти детали они вот здесь ограбляются это подобно
тому как как вот вы решаете задачу допустим оптимизации но локально ваша
задача любая оптимизация ну выпукло допустим это какая-то квадратичная
выпукло манука симметричной матрицы задается не отрицательно определенные вот
и фактически локально вы имеете класс задач квадратичной оптимизации это вот
самый так сказать характерная такая задача оптимизация здесь тоже самое у
нас нормальность как бы случайно величины она вот вот как бы вот об
этом то есть что она наиболее то сказать характерно для то есть она
описывает асимптотику как бы этот результат усреднения но вот то что я
сейчас буду рассказывать это она но далеко не все знают что будет если все
таки ну мы хотим что-то более интересное чем да да пожалуйста не совсем
заметьте как я хитро сделал нечестно я сказал вот есть такие условия линдеберга
там еще что-то значит на самом деле есть даже некоторые теоремы для
закона студента для распределения студента что даже второй момент иногда не
нужен и тоже можно типа цпт получать я сейчас не буду в эти тонкости давать
есть замечательные книжки и сикеева стоянова по всяким контрпремерам
теории вероятности вот там много всяких вопросов относительно того насколько
чего допускает релаксацию вот место которое не очень как бы я явно
проговаривал но оно на самом деле важно что не просто второй момент есть а еще
важно что все что дальше у маленькой от f квадрате то есть по факту это так
если вы например говорить что есть третий момент это не совсем как бы ну
можно ослабить но все-таки я существенно использовал что хвостик
маленький и как-то это надо говаривать то есть если вы просто скажете второй
момент этого будет недостаточно есть значит да теперь мне бы хотелось ну мы
вернемся к тому а что было на прошлой лекции будет построено геометрическое
броноско движение и все это будет но как бы вот ключевой вывод из блин только
что стертой центральной предельной теоремы прошу прощения не подумал да он
заключается в том что если есть сумма случайных величин сумма x катых да вот
1 на n сумма x катых вот так грубо говоря да вот вот какими-то естественными
характеристиками то это сумма себя ведет приблизительно вот так это есть
мат ожидания плюс плюс стигма корень из n на нормальной случайной величина это
вот n 0 1 и мы будем этим фактом пользоваться то есть если мы например
хотим понять качество оценки вот такой вот и м то нам надо вот ну качество это
оценки будет 1 на корень из-за мы многократно будем в рассуждениях
использовать вот эту как бы оценку то есть поправку то есть что точность такого
типа оценок она 1 на корень из n но проблема здесь в том что я пишу приближенно
равно и никак не оцениваю вообще говоря качество качество вот такого типа
результатов вот давайте я сейчас приведу два неравенства я потом и одно
из них докажу это неравенство азума хевдинга и бернштейна и вот эти
неравенства они просто уточняют центральную предельную теорему для
нескольких режимов которые довольно типичны во многих приложениях значит
пускай у нас есть значит неравенство концентрации
тарю что мы докажем значит одно из них неравенство концентрация он
центрация трассы первое неравенство будет неравенство хевдинга значит у нас
постановка одинаково для для обоих для обе обоих неравенств значит что есть
экскаты совокупность айди случайных величин то есть это айдин более того мы
знаем что эти случайные величины значит ну на самом деле даже неважно что они
айди это я уж для простоты они могут иметь разные мат ожидания это сейчас
неважно значит пускай у них вот мат ожидания м все хотя я повторю что это
вообще неважно вот значит давайте считать что дисперсия каждый из этих
случайных величин знаю дисперсия д пишете ли вар как лучше д д иксы ты
равняется мат ожидания иксы ты центрированная в квадрате равняется
сигма в квадрате тоже одинаковая это все для простоты делается то есть повторю
что это сейчас не принципиально ну и сделаем упрощающее предложение
предположение то все иксы ты ну стало быть это просто для любой любой от
экскаты да можно сказать что он ну раз они одинаково распределены то мне не
важно какой я скажу меньше либо равно ц то есть все они по модуле меньше ц
почти наверно почти наверно это в том смысле что так научно говорят это не то
что я не уверен это с точностью до меры но вероятность событий мерной и вот
в этом случае имеет место два неравенства 1 неравенство называется
неравенство хевдинга неравенство хевдинга по моему по-русски он пишется
с одной буквы f по-английски с двумя я не знаю почему так вот это как бы я не
смог найти объяснение но вроде это уже общепринято очень неравенство выглядит
таким образом мы сейчас его запишем ну как бы в стандартном виде в котором
оно легко гуглиться вот но есть разные формы записи в общем это сейчас не очень
как бы принципиально значит это больше чем ты не важно я его помню с
точностью до числовых константов это у меня всегда проблема ну вот я сейчас
их подсмотрю прошу прощения за это это надеюсь не очень страшно да давайте
здесь индексы будем суммировать ка от одного тоже до n большого ну и здесь
соответственно тогда будет 2n большое соответственно t в квадрате ну да пускай
будет t в квадрате на что там у нас просто да тут два тут два больше индексов
больше чисел нет b минус а в квадрате все а не b минус а ц ц квадрате так давайте
ц в квадрате ц в квадрате ну по-хорошему я давайте могу поумничать и могу здесь
отрезок написать ладно не будем умничать по нам умничать это просто ц
в квадрате оно так только тут будет 20 в квадрате стало бы здесь будет тогда
да вот я все-таки накосячил 20 вот так 20 в квадрате вот не зря посмотрел теперь
неравенство а значит и никакое просто любой это больше нуля ну пошли нуля но
не интересно вот теперь неравенство бенштейна неравенство бенштейна
бенштейна неравенство бенштейна выглядит под немножко посложнее но тоже в общем-то
ну по сути также то есть правая часть выглядит левая часть выглядит одинаково 1 на n сумма
x катых меньше чем м кат одного так тут фактор n он должен совпадать жалко что меня никто не
поправил как от одного до n но оно поточнее оценивает соответственно вот правую часть меньше
либо равняется значит тут будет экспонента на как бы это мне сейчас экспонента надеюсь уместить
нет не умещу так ну давайте внизу напишу вот это кажется выход это же видно будет
снизу значит это будет экспонента минус n на т в квадрате сейчас будет
похожее выражение на то что было значит два фигма в квадрате два сигма в квадрате
плюс два значит т на три вот значит я буду это все пояснять пока вот просто мы записали два
неравенства и вообще как бы давайте попробуем понять чего эти неравенства нам говорят уже в них
вот если я вот этот результат попробую как-то вытащить в них говорится более точно то есть не
просто что вы симптотики если она очень большое то имеет место близость по распределению здесь
очень много каких-то моментов ну непонятных а насколько n должно быть большое вообще насколько
насколько пора распределить равенство по распределению это вот правильный способ
описания вот это все-таки не совсем привычный нам такой вот для использования способ записи вот
давайте пока бернштейн и не трогать мы чуть-чуть позже его рассмотреть пока сконцентрируемся на
просто азуме хевдинге замечу что просто хевдинге сигма в квадрате здесь не используется но это
слишком грубое такое неравенство но тем не менее в общем мы считаем что сигма и цетом одного порядка
если xk ограничено цету ясно что и сигма в квадрате запасом будет ограничена там ц так что в этом
смысле в общем ничего тут такого необычного нет то есть все ограничено какими-то числовыми
константами ну и какой масштаб будет иметь сумма вот эта самая сумма которая написана в
центральной предельной теореме какой масштаб это будет иметь ну давайте выпишем также вот что
1 на n сумма xк так это я уже пытаюсь развернуть вот этот результат кат одного до n минус ну при
порядка там м но только теперь порядка мы уже можем говорить точнее это равно м что дальше равно
м дальше что там и вот надо понять что здесь да ты такое да то есть вот это ты надо добавить то
есть мы можем это написать так что это равняется м можно так написать что это меньше либо равняется
некое t с какой-то большой вероятностью вероятностью с вероятностью больше бы равняется 1 минус ну скажем
сигма ой сигма занята дельта ну давайте зададим это дельта дельта ну чего у нас тогда получается
логарифмируем смотрим что получается значит nt в квадрате на 20 в квадрате равняется логарифм
1 на дельта так ведь так ну значит получается что ты отсюда следует что ты равняется 2 ц в
квадрате поделить на n на логарифм 1 только т в квадрате логарифм 1 на дельта вот ну отсюда
следует что ты равняется чему ц но два корень из двух на ц на ц на корень из n на корень из
логарифма 1 на дельта ну давайте просто здесь запишем что значит ну удобнее это будет записать так
ц на и начинок корень из 2 логарифм 1 на дельта значит соответственно на на что на n правильно вот
видите отличия вот да вот этим у меня проблема да это хорошо что вы заметили я иногда теряю такие
моменты из виду спасибо это правда это правда очень хорошо да вы следите за этим всем да это
правильно давайте попробуем поймем в чем разница ну понятно что из бернштейна я на самом деле могу
вытащить как бы ну как минимум такой же результат что-то похожее только с чем замены ц на сигма то
есть если бы этого не было то все тоже самое было бы но там бы стояла сигма а если все-таки это
учитывать то здесь появится слагаемая которая будет содержать просто порядок n тут какие-то
уже что-то то сказать другое то есть будет вот что-то такое логарифм 1 на дельта вот вот так
будет то я не буду сейчас писать что я тот уже как бы из берс voices получается то есть из бέρстена
здесь будет стоять то тоже значит как раз будет сигма а для этого будет sh и повторю это это для
бернштейна balance reign он что улучшает он улучшает с потому что ц и это константа которая ограничивает
все экскаты она может быть большая представьте себе что вас распределение
сосредоточено вот как бы в основном здесь и дисперсия небольшая отце
большое то есть фенитный носитель и это плохо то есть это как бы на самом деле
грубо ну вот значит бернштейн позволяет
точнее оценивать то есть он все уводит в порядок следующий вот сюда
ну хорошо это я уж как бы это сказать бонусом а сейчас вот нам надо разобраться
вообще с основным этим неравенством но мы видим тот же самый закон то есть
тут закон в чем не точность этого закона в том что вообще говоря это
приближенно и не очень понятно как это с асимптотикой
насколько точно это с 8 тотики мы про это сейчас скажем ответ на этот вопрос
есть а вот здесь все точно но правда мы за это заплатили тем что сделали
дополнительные предположения которые вообще говоря обременительные ну вообще
например сами нормальные случайные величины которые при суммировании дают
нормальную они сюда не подходят поэтому ценность такого результата может
показаться маленькой но вообще говоря когда вы рассматриваете задачи
приходящие там из комбинаторики с каких-то ну таких приложений таких скажем
компьютер саенс компьютер саенс и оптимизация машин леонинг то это очень
часто так то есть вот эти два неравенства как ни странно хотя по
вот чисто то сказать таким фактором что ну как финитный носитель это плохо
но вот машинное обучение это это не страшно для машинного обучения и мы про
это будем говорить ну я формально смотрите я что сделал я сказал что
вероятность того что я что хочу я хочу чтобы это верно было с вероятностью
больше чем один на дельта значит я написал такое неравенство потом я
тогда сказал ага а вот так чтобы вот так было да мне что надо
не тогда вот такое неравенство будет верно 1 минус вот это так ведь дальше я
сказал чтобы это было верно с вероятностью 1 минус дельта мне надо
чтобы эта штука была меньше чем дельта я приравнял вот это дельта ну то есть я
неправильно написал дельта надо ну как бы я правильно написал просто речь о
том что надо было вот этот шаг объяснить что это 1 минус тут меньше либо
равняется потом я приравниваю вот это дельте вот эту часть и тогда у меня
аналогия здесь один минус дельта и здесь один минус дельта но я должен был
развернуть неравенство чтобы это получить я развернул получил потом
вытащил т вот это ты у меня ты которая там обозначена и я получил тот же самый
закон то есть у меня как бы x к равняется им плюс минус то есть вот это
но же знак не определен и знак не определен и в каком-то смысле можно про
cousin сказать вот это самое то есть вот это определяет кси вероятность того
что кси куда-то уйдет то есть я могу прок на вот вот грубо говоря из этого
представления при замене sigma наце следует тоже
самое в точности с точностью до числовой константы то есть как бы сказать если я
понимаю это как равенство ну вот как вот то есть вот просто пишу минус вот вот
скажем вот вот что я могу получить вот из этого написать минус m вот так вот
меньше либо равняется меньше либо равняется и вот здесь вот пользуясь тем
что это нормальная случайная величина так и написать что это sigma корень из n
и здесь будет корень из логарифма там один нос на дельта вот это будет верно
только это будет верно непонятной асимптотики то есть непонятно насколько
большой n надо брать чтобы это стало верным вот и в общем и кроме того здесь
как бы тоже не совсем понятно что ну хорошо в общем вот вот этот момент он
такой неприятный здесь все понятно здесь для любого n это верно расплата в том
что мы здесь ставим c вместо дисперсии ну либо мы ставим sigma но тогда
получаем следующую поправку возникает резонный вопрос вообще зачем все эти
поправки вообще что будет если если ну если у нас распределение скажем с
тяжелыми хвостами вообще насколько точная центрально предельная теорема и
чему надо отдавать предпочтение когда мы значит ну пытаемся какие-то
закономерности там улавливать вот оказывается что ну к сожалению если у
вас нет каких-то таких предположений в которых работает неравенство
концентрации то центрально предельная теорема не очень точная так скажем я
сейчас это продемонстрирую вот но вот эти неравенство концентрации они
замечательные они замечательны потому что в них все абсолютно строго не
асимптотично этим можно пользоваться и тот же та же самая закономерность имеет
место вопрос просто в деталях какие вы тут
константы пишете ну и в зависимости от неравенства вы можете их там улучшать
повторю что неравенство хевдинга мы докажем а неравенство бернштейна мы
будем пользоваться вот ну то есть это все как бы не не просто так я говорю и мы
будем с ними сталкиваться практически регулярно но сейчас я хочу немножко про
цпт еще сказать про так сказать тяжелые хвосты потому что на самом деле вот это
вся так сказать наука связанная центрально предельная теорема она имеет
довольно интересный такой аспект который я почти уверен вы с ним не
сталкивались но он существует и мне очень хочется вам его
продемонстрировать а именно я сейчас расскажу про ну как бы цпт с тяжелыми
хвостами цпт тяжелыми хвостами тяжелыми хвостами о чем идет речь мы
сейчас посмотрели на как бы один закон распределения данных мысля нормальный
но ты у нас сейчас мы тоже хотим нормальный закон получать но для того
чтобы получать нам нужны какие-то то сказать хорошие свойства например
конечный второй момент либо вообще финитный носитель вот если финитный
носитель у нас вот это есть это классно давайте пока еще сразу не начали про
тяжелые хвосты я напишу еще один такой некоторое такое
свойство давайте предположим что икс кате это есть кси кате в квадрате где
кси кате это ну стандартная нормальная случайная величина будем так считать
это 0 1 то в этом случае просто такое небольшое разминочное упражнение но
оно не очень тривиально что в этом случае можно сказать вот про соответственно
такую сумму что про такую сумму можно сказать вот оказывается что если не
финитный носитель но вы знаете что нормальное распределение имеет
соответственно хвосты субгаусовские когда вы введете в квадрат то е минус
x в квадрате закон распределения то перейдет в е в степени минус y потому что
ну вы случайную величину как бы берете квадратичные у него будут
экспоненциальные хвосты то есть вопрос такой если писать результат типа
центральной предельной теоремы для ну по сути вектора нормального случайного
вектора вот вот то есть мы берем нормальный случайный вектор это единичная
корреляционная матрица это мат ожидания нулево и задаемся вопросом какая у него
норма его норма будет как раз равняться а значит кси и 1 в квадрате плюс и так
далее плюс кси и кси сколько там компонент ну ксен большой мы пишем ксен
большой в квадрате вот эта задача часто возникает в разных приложениях это
закон распределения называется хи квадрат в статистике он постоянно
встречается и понятно что это не совсем тот режим которым с которым мы привыкли
работать то есть это не ну не привыкли а скажем так встретились это нефинитный
носитель и экспоненциальные хвосты это даже круче чем субгаусовские какой
результат в этом случае имеет место нарушается нефинитный носитель означает
что мы не можем xk почти наверно ограничить какой-то константы вот если
у нас кси нормальная случайная величина то ее квадрат ну заведомо не имеет
финитного носителя но вот тем не менее тем не менее значит с точностью до
числовых констант мы имеем вот такой вот результат значит ну то есть я просто
переписываю с правильными константами вот этот вот результат здесь будет 6 6
значит вот этот вот значит логарифм 1 на дельта ну на корень из-за это вот
первая слагаемая константа я могу опять напутать ну то есть там может быть
чуть-чуть больше там чуть-чуть меньше это сейчас вообще не принципиально ну
вот здесь константа тоже 6 и 6 но вот у меня так записано это вот я из статьи
спокойного взял такой результат значит логарифм 1 на дельта логарифм 1 на дельта
а здесь стоит просто n то есть смотрите что получается вот эта самая штука
которую нам надо ограничивать ну здесь мат ожидания равняется единице то есть
мат ожидания ксикатому то есть мат ожидания ксикатова в квадрате равняется
единица вот поэтому м 1 и это дает на самом деле уже как бы некую надежду что
вообще говоря не обязательно финитный носитель нужен чтобы иметь какую-то
такую вот концентрацию вот удивительным образом вот я сейчас к чему вообще все
это говорю удивительным образом на красивая наука более менее красивая
наука вот здесь заканчивается ну скажем так вот есть класс результатов вот типа
центральный предельных теорем их там много это богатый класс но это все
результаты из серии если и оно очень большое то тогда и вот мы сейчас эти
результаты напишем то есть у вас будет картина в голове но эта картина не дает
вам как бы сказать возможности аккуратно чего-то такое сильное делать с этим
потому что все эти результаты асимпатические и вы не контролируете
качество этих результатов кто знает в какой асимпатике это будет верно а вот
здесь нет никакой асимпатики это просто неравенство она верно для любого n в том
числе маленького и это конечно круто но проблема в том что это все можно сделать
для случайных величин ну максимум с экспоненциальными хвостами но во всяком
случае так вот что было просто и красиво еще раз вот если у вас субгаусовские
хвосты и овдинговская концентрация с фенитный носитель ну или не сильнее чем
экспоненциально то есть если еще медленнее бывает чем экспоненциально то
это уже плохо вот надо чтобы хотя бы как экспоненциально вот если хотя бы как
экспоненциально то у вас в принципе вот этот закономерности она есть то есть
главное слагаемая это вот один на куре низен и здесь доверить на вот этот
уровень входит под корнем из логарифма это означает субгаусовская концентрация
а вот тут логарифм один на дельт это означает экспоненциальная концентрация то
есть это отвечает тому что ну если очень большие выбросы то они моделируются никак
то есть этот закон он его же в цпт нет цпт нет вот этой закономерности от дельта
его ну как бы мы не видим а оно есть потому что экспоненциальные хвосты но
оно есть и в бердштейне потому что мы здесь очень точно оценили здесь заплатили
то есть даже если у нас вот хвосты финитные то есть их нет все равно там
так такого типа штука возникает и это полезно понимать потому что дальше
когда вы будете сталкиваться с суммами независимых случайных величин вы реально
будете ну как бы нам нужно будет использовать что-то такое но это давайте
закрепим это таким упражнением представьте себе что прошел второй тур
выборов два кандидата ну и соответственно надо определить
вероятность успеха мы уже эту задачу решали ну в какой-то степени да там
помните на прошлой лекции в самом начале мы говорили что если есть там
неизвестная площадь мы случайно кидаем точки площадь это просто доля населения
которые вот и хотим оценить вот эту площадь то если мы случайно равновероятно
кидаем точки в эту область то доля точек попавших в эту часть она есть
просто площадь ну в нашем случае это как бы площадь понимается как доля
людей то есть вот мы попадаем в тех людей расположенных вот и как бы ну
неважно как они сплошь что они там заголосуют за а тут забыть вот ну и
соответственно чтобы определить неизвестную вероятность мы вводим
случайную величину x к которая единица если за а и 0 если забыли ну просто
результат опроса эти текста независимо одинаково распределенные значит имеют
закон распределения бернули с параметром п то есть это означает что
случайная величина x к ну вот она вот это принимает с вероятностью п а это с
вероятностью 1 минус п значит к чему это подходит вот сюда это подходит к
ернингу подходит подходит ну а чего нет то есть более того чему ц равняется ну
единицы да то есть вы можете написать здесь что просто единица ну это можно не
писать ну то есть у нас получается такой результат сразу сходу что вот это x к
минус m меньше либо равняется что там у нас корень из 2 логарифма 2 на дельта
на корень из n вот так вот получается так значит хорошо коллеги вот это уже
вполне конкретный результат который позволяет понять почему когда я
формулирую результат мне нужны две категории первая категория с какой
вероятностью я хочу чтобы мой прогноз был достоверным то есть я хочу сказать
что за кандидата а проголосовала столько-то процентов с вероятностью не
меньше чем 0 95 и как я это сделаю я просто оцениваю это число то есть я не
знаю м это п мат ожидания это п в данном случае и вот я оцениваю значит ну это
грубо на самом деле здесь можно точнее писать и потому что ну все-таки их
обденк это как бы слишком то сказать ну этого достаточно на самом деле то есть
это грубо но не очень человек константа просто немножко не такая но вот смотрите
то есть я могу просто приравнять это желаемой точности то есть насколько я
хочу чтобы отличалась п от оценки ну допустим 0 0 1 там или 0 0 1 да вот на там
один процент ну и сказать с какой доверительной вероятностью но вот я
это называю эпсилон эпсилон это называют дельты вот могу определить как
зависит от и Ков lahlo ну вот так она зависит от ив�ито как логари ф
корень извлаго рифма один на дельта ну поделить на и п сон квадрат или не
просто логари в просто логари то есть логари ф мо логари ф мо 1 на дельта на
и взял в квадрате тоесть я хочу высокую точность прогноза
вот тут высокую точность мне конечно надо сильно за это платить а если я
хочу высокий доверительный уровень то мне за это не сильно надо платить ну и
понятно что вот я беру 0 1 да здесь получается здесь получается более-менее
10 чего там 10 4 но 10 тысяч человек ну это вообще не очень много ну если делать
еще точнее со всякие эти константы бороться тут реально будет еще какая-то
константа понижающий меньше единицы по моему вот то есть получается что
откуда эти 10 тысяч можно получить но вот вот из таких рассуждений и это и есть
как бы ну ну как бы современная наука то есть современная наука не о том что
надо вот вот вот как бы цпт писать а надо вот такого типа неравенства писать
с двумя степенями свободы то есть как бы зависимость от н и зависимость от
дельта это точнее это не степень свободы а параметры вот нужно уметь
отслеживать какая зависимость от этих двух параметров но изменить это было
отвлечение теперь я возвращаюсь к цпт с тяжелыми хвостами я просто вот я
сейчас иду скороговоркой и почти ничего не доказываю но от хивинга я обещал вам
доказать и моего докажем но почему это не хочу делать прямо сейчас потому что
мне сначала хочется что у вас была картина некая целостная а потом я
докажу хивинга но в более общем контексте то есть как бы не для
случайных независимых одинаково распределенных величин а для мартингала
разности что нам очень сильно поможет в современный сток оптимизации
разобраться потому что оптимизация если с точки зрения брать не раз
концентрация это все-таки изучение последовательности так называемых
мартингала разности и поэтому нам не просто будет нужен хивинг а зума хивинг
все то же самое только вот и кста будет уже чуть-чуть более сложной
последовательностью нет хвосты это прям точная формула то есть вот то что я
написал это вот был вот вот этот вариант это хивинг это хивинг если я
здесь зачеркнул с поставлю сигма а сигма меньше чем ц и то мне надо будет
воспользоваться тогда не раньше берштейна потому что я не просто так
сигма не могу поставить вот это уже в чистом виде соответствует цпт то есть
вот это цпт но цпт оно как бы асимпатический результат что будет
атосимптотики я пишу добавляю вот это слагаемая это расплата за уход
атосимптотики но это расплата за уход атосимптотики только в случае когда
есть фенитный носитель то есть это просто поправочная слагаемая поправочная
слагаемый тут сядь кайти словы константа которую я опустил вот
когда сейчас когда у нас нет фенитного носителя то давайте я просто пытался
одну идею донести когда у нас нет фенитного носителя но есть конкретный
пример характерный пример например сумма квадратов гауссерских величин
нормальный случайный вектор кси он имеет нормальное распределение с матрицы
единичной корреляционной размера и значит это вот такой вектор его квадрат
это сумма квадратов компонент каждый компонент и независимая гауссовская
случайная величина стандартная гауссовская значит и у ем от ожидания 1 и мы
для нее можем оказывается написать такое же неравенство но с конкретными
числами тут нет абстрактного сигма потому что мат ожидания 1 дисперсия там
тоже что-то такой типа 1 вот мы можем конкретные числа написать и я замечу
что это будет такой же тип и неравенство но это неравенство уже
теперь получено для случайных величин не с финитным носителем не с финитным но
все-таки предполагается экспоненциальность хвостов потому что
гауссовская случайная величина имеет вот такие хвосты плотность распределения
имеет такие хвосты когда мы переходим к случайной величине который есть
квадрат гауссовской и обозначаем ее игрек обозначим ее игрек тогда вот это
игрек будет иметь экспоненциальные хвосты и экспоненциальные хвосты медленнее
убывают чем гауссовские но даже для такой случайной величины у которой хвосты
убывают хвосты плотности распределения убывают экспоненциально хуже чем
гауссовской можно написать закон аналогичный неравенство берштейна но я
это не сформулировал как какая-то супер теорема для всех всех всех задач
это был просто конкретный пример он был сделан для того чтобы было
ощущение что на самом деле это не просто так это вот все о том что есть
класс задач в которых xk либо финитные либо быстро убывают понятия быстро
убывает по сути мы сферии к тому что экспоненциальные хвосты или быстрее вот
если хвосты экспоненциальные или быстрее тогда вот так если соответственно
медленнее то то возникает уже как бы сильно слаболее сложная теория и вот
непонятно чем вот это дело заменять ну в общем случае и вот мы сейчас как раз
переходим к тому а что например если хвосты степенные это вот как бы
экспоненциально что если степенные мы сейчас увидим что все намного печальнее
что вообще говоря в таком случае но мы это увидим в примере как бы все-таки
уже не таких результатов красивых то есть не в симпатическом варианте мы
увидим что будет если хвосты степенные вот мы сейчас переходим к цпт с
тяжелыми хвостами а можно уточнить ответила на вопрос видимо в зуме или
в ютюбе а это ваш классно да пожалуйста
да его можно это правда его можно строго сформулируйте доказать для
экспоненциальных понятно то я все что и всю специфику которую я использовал это
что ну этих величин определенной мат ожиданий дисперсия вы естественно можете
переписать это ну в терминах общих как бы мат ожиданий дисперсии я немножко
отвлекся так вот важно что вот эти неравенство концентрации это действительно
такой современный язык который позволяет решать огромное количество
задач ну например и просто в комбинаторики казалось бы да причем
здесь вот это или я не знаю вот в оптимизации это просто основной
инструментарий вот я вам покажу как пернштейн совсем недавно помог сделать
ну небольшой такой прорыв понимание оценок липпинга это восходит к работе
Немировского, Наззина, Цыбакова, Юдицкого. Три года назад была статья вот чисто за
счет того что неравенство Хьевнига поменяли на Бернштейна получили
обоснование клиппинга я вам прием расскажу это совсем современный
результат а клиппинг это то о чем пишут в книжках типа good fellow, benjo, curvil
как способ борьбы там с тяжелыми хвостами градиентов при обучении
нейронных сетей то есть все это довольно сильно связано в общем с
каким-то вещь то есть это не просто так здесь появляется и вот именно эти
сюжеты которые связаны с концентрацией меры это очень важные сюжеты просто
это вот прям реально одно из самых может быть важных частей курса поэтому
конечно мы докажем конечно мы докажем ну как минимум одно из этих неравенств
и может не одно но пока пока у нас нет цели это делать потому что в простейшем
случае мне не интересно доказывать это ну вы можете это легко посмотреть это в
общем мне это нужно в контексте сделать как правильном в котором это
реально используется чаще всего вот а теперь я приведу результаты с тяжелыми
хвостами то есть пускай все то же самое то есть есть последовательность xk это
последовательность iid то есть независимых одинаково распределенных
случайных величин значит дисперсии всех этих случайных величин ну мы от
ожидания будем считать что нулевые для простоты ну и ладно не будем считать то
есть м значит а дисперсии будут как и раньше xk в квадрате равняется sigma в
квадрате но они одинаковые и давайте ведем вот эту функцию которую ну
стандартной гауссовская такая функция которая связана с плотностью гауссовской
единица на корень из 2 п соответственно интеграл от нуля до x и в степени 1
минус 1 вторая y в квадрате dy но это ясно что это по сути вероятность того что
случайная величина имеющая закон распределения гауса ну по по другому
говоря нормальный закон распределения вероятность того что она меньше чем кси
вот чему это вероятность равна просто по определению ну при условии что кси
имеет закон n 0 1 n 0 1 вот это вот это вот так сказать что мы имеем это функция
введена и теперь значит предположим сделаем такое предположение что
вероятность того токс x случайно величина xк ты ну можно любую взять
больше чем x значит это вероятность пропорционально ну числу x степени
минус альфа вот ну и мы считаем что альфа ну некоторое число такое что
больше чем два иначе иначе там не будет дисперс существовать вот еще раз
спасибо да конечно от минус бесконечности дает вот так вот да спасибо да да да
спасибо это правильно теперь ну попрошу прощения бернштейна я все-таки сотру
нас он всплывет не здесь но всплывет точно не раньше бернштейн у нас точно
будет использоваться вот но не сейчас я видимо не успеваю хотя не знаю посмотрю
как пойдет у меня принципе сегодня было цель рассказать там про мултил его
монтекарло и там про монтекарло добить сюжеты вот давайте значит сейчас
напишем что будет для вот этой суммы иметь место то есть результат для вот
этой вот суммы ну я позволю себе немножко значит переписать ну просто
форма записи будет немножко другой ну или не знаю ну да чтобы чтобы просто
просто в таких категориях сформулировать сложно вот значит давайте
ведем сумма икс-катых равняется эсэн эсэну просто для компактности икс-катых
эсэн такое кат 1 до n ну и вот оказывается что имеет место такое
неравенство вероятность того что эсэн больше либо равняется икса приближенно
равна ну в том же смысле что и центрально-предельная теорема то есть
имеется в виду в пределе при н перемещемся к бесконечности то есть это
результат аля центрально-предельная теорема в режиме если n больше чем корень
из n сигма в квадрате вот если x больше чем n на сигма в квадрате то это есть
единица минус f соответственно x на n корень из n сигма в квадрате n сигма в
квадрате но вот это не все здесь возникает слагаемая которая есть n на
v от x что v от x это вот эта функция которая которую я обозначу v от x v
от x вот это вот v от x ну v от x естественно да но а вот это уже
утверждение вот это как бы становится примерно то есть это вы симптотики
просто верно да только тут n большое то есть это это как бы равенство но только
каждого симптотики а если это при приближенной равенстве давайте
напишем так n больше единицы это при приближенной равенства ровно в таком
же смысле я могу понимать центральную предельную теорему то если если как бы
я говорю просто от центральной предельной теореме то это тоже верно но
но проблема в том с центральной предельной теоремой что да да да
Это просто параметр, который определяет тяжесть хвоста.
В каком-то смысле можно даже понимать, что это просто
обычное равенство, где я опускаю некоторые небольшие
поправочки.
То есть можно считать, что это лучше, чем CPT, потому
что CPT это реально асимптотика.
А здесь все-таки это не асимптотика, это результат, который пытается
исправить CPT.
Еще раз, отличие от CPT в том, что в CPT это реально
просто предел нужно делать, а здесь это все-таки не
предел, это попытка исправить CPT тем, чтобы скорректировать
как-то.
То есть что было бы, если бы я написал CPT, то есть центрально-предельная
теорема, как бы она формулировалась.
Я прошу прощения, что так записал необычно, но я думаю,
вы понимаете, почему я это сделал.
Я это сделал для того, чтобы последняя слагаемая в такой
форме записи выглядела максимально просто.
Я мог бы сделать и наоборот, я мог бы сделать вот здесь
стандартную конструкцию, но здесь было бы тоже стандартно,
просто было бы там f, phi, но тогда здесь было бы хуже,
поэтому мне удобнее записывать, вот как бы если я это не
пишу, то это CPT, это понятно или нет, что это CPT, потому
что 1 минус эта функция, это есть интеграл как раз,
что вероятность, что кси будет больше, чем х, и при
должной нормировке с учетом, ну это написано, правда,
прошу прощения, для вот этого случая, то есть да,
то есть я тут как бы рассматриваю случай, ну m0, если это не
ноль, то мне это просто надо корректировать здесь,
я для простоты буду считать, что у меня случайные величины
центрированы, так вот, вот это вот неравенство, оно
например есть в книжке Боровкова, такой известный специалист
по теории вероятности статистики, случайным процессом, вот
такое неравенство, а значит обратите внимание, что если
х большое, да, тут во-первых делается оговорка, х тут
не может быть очень маленьким, то есть в свою очередь х
должен быть такой, что это число больше единицы, то
есть мы именно смотрим на хвосты справа достаточно
тяжело, то есть чтобы вот у вас как бы есть центральная,
нормальная случайная величина, нормально распределенная,
у нее дисперсия единица, ну вот вам интересно именно
диапазон, вот такой вот х больше единицы, то есть
сюда что-то содержательно попадает, но в общем вот
попадает, но уже так сказать такой хвост, то есть как бы
я бы не сказал, что информация о всем распределении, это
информация именно о хвосте распределения, достаточно
большом, вот когда х очень большой, вот это дает описание
хвоста, и это есть альтернатива, ну не альтернатива, а как
бы другое описание того, что будет вот в этих режимах,
это описание мы видим оно сильно хуже, то есть если
посмотреть, присмотреться, то это сильно хуже описание,
чем на самом деле вон там, но это вот как бы такая
расплата, теперь возникает вопрос, эта формула она
точная, может она в свою очередь как бы чего-то
не учитывает, и вот сейчас мы с вами убедимся, я не
знаю где эту выкладку посмотреть, то есть просто это наблюдение
очень его несложно сделать, сопоставим с неравенством
Берие-Сена, то есть у cpt, в центральной предельной
теоремы есть на самом деле вариант не асимпатический,
точнее я сейчас его напишу, это называется неравенство
Берие-Сена, это неравенство характеризует точность
центральной предельной теоремы, и вот мы сейчас
попробуем сопоставить вот этот результат и вот это
вот неравенство Берие-Сена, и естественно вот можно
ну уж если есть вариант cpt который есть оценка скорости сходимости этот спасает положение но
вот нет потому что это оценка она хоть и не улучшаемая но она довольно грубая ну грубая с точки
зрения общего неравенства верно всегда но оно для то сказать вот например хёвдинговской
концентрации будет излишним итак неравенство бери эсэна неравенство бери эсэна это про cpt бери
эсэна вокруг cpt значит вот такой вот результат supremum по x по всем множеству действительно чисел
от такого разности вероятность того что ну поскольку меня мот ожидания 0 то я и буду
продолжать писать это в парадигме мот ожидания 0 и xk 0 и xk 0 просто чтобы было одинаково вот
с этой сюжетной линии значит здесь sigma на корень из n а от одного до n это вот более
менее та самая сумма которая стоит в центральной предельной теореме ну и здесь конечно должно
быть минус и от x то самое что стоит в ну вот вот вот здесь то есть это как бы cpt если бы я
сказал что это стремится к нулю это был бы классический вариант cpt вот классический вариант
cpt что при n большой стремящимся к нулю этот предел этого supremum равен нулю но я говорю
точнее я говорю что существует некая константа существует некая константа c волной чтобы не
путать с предыдущими константами такая что вот эта штука значит xk в кубе вот она значит
оценивается вот таким образом sigma в кубе на корень из n вот третьем то есть получается что
получается что у нас вообще говоря есть оценка скорости сходимости ну и это константа c с волной
она лежит диапазоне 0.4 0.71 0.4 0.71 и это не равенство не улучшаемая то есть в общем
случае доказано вы не можете получить что-то лучшее при предположении что существует
третий момент то есть естественно это не раньше работает когда есть третий момент и вот в общем
случае вы не можете получить что-то лучше вот довольно такой не неожиданный результат вот что
это не плюс это так тут доска артефакт доски это минус не знаю понятно или нет там так получается
как плюс но это минус это называется неравенство бире сена так вот давайте
сопоставим неравенство бире сена и соответственно вот это неравенство точнее это равенство в
специальном режиме в режиме когда точка x равняется некому xn которая подбирается вот таким образом
альфа минус 2 альфа всегда больше двух иначе не существует дисперсия то есть вот отсюда
следует что альфа больше 2 вот иначе дисперсии не существует ну и соответственно значит вот
здесь будет n да извините большое значит n на логарифм n да ну сигма в квадрате я конечно
опустил надо его написать сигма в квадрате n на логарифм n то есть мы возьмем специальный
сказать вот такой вот режим откуда эта точка возникла то есть почему я именно такую точку
взял дело в том что если вы подставите эту точку вот сюда и вот сюда вы получите что эти
слагаемые сравняются то есть вклад вот этого дела и вклад вот этого дела будет одинаков то есть
одинаковый вклад давайте как-то это так выведем одинаковый вклад одинаковый вклад то есть это
точка когда когда цпт то есть то что связано цпт переключается то есть режим центральный
предельный теорема переключается на режим тяжелых хвостов но если еще раз посмотреть на график
на график вот этого закона значит центральный предельный теорема то фактически фактически
получается что у нас как объяснить диапазон в котором работает цпт он очень узкий то есть
цпт работает в диапазоне когда вот это вот правая часть когда правая часть имеет масштаб ну точнее
вот вы смотрите на на сумму 1 на n сумма x катых вот как одного до n и вы говорите что это
есть м плюс ну некоторое такое sigma кси на корень из n вот что говорит нормальная центральная
центрально предельная теорема что по распределению вот что-то такое имеет место дальше возникает
вопрос как бы если все-таки не брать цпт а брать вот жизнь реальность то насколько насколько вот
асимптодика при каких хвостах она справедливо так вот эта симптотика справедливо если x уже
теперь не считать m01 а просто считать какой-то величиной которая случайно но не взять нормально
то вот это справедливо если x ставится в диапазоне до логарифма n то есть вот
до логарифма n forsk satellites это еще куда как бы это еще куда не шла то есть если x и имеет
нормальное распределение оно реализуется так что оно меньше чем логариф мэн это еще как бы
адекватно отражает поведение этого этой суммы но вот если а это может быть кси реализуется так что
оно выходит за рамки вот этого диапазона то извините это уже как бы поведение то есть если кси
логариф мэн масштаб то это уже не точная формула вот то есть не не большие хвосты они так сказать
описываются по-другому они начинают начинает по-другому другой режим быть и это вот здесь видно
тоже то есть если у вас как бы вы хотите с очень большим доверительным уровнем чего-то добиться то
первый как бы вот это корень из логарифм один носи на дельта и это соответствует супгаусовской
концентрации ну то есть нормальный но когда дельта очень очень маленький а н зафиксировано то вот
это слагаемое начнет перебивать вот это слагаемое это интересно потому что у вас как бы вообще
говоря естественно что тут корень из н а тут n но тут корень из логарифм один на дельта а тут
логарифм один на дельта и вот как бы аналогом является то что вот это слагаемое оно момент
когда вы берете достаточно большие отклонения оно там-то это случается довольно как бы не скоро
там диапазон то есть здесь у вас запас корень из н то есть чтобы это случилось надо чтобы вот
этот логарифм отличался от этого в корень из н раз это вообще говоря много то есть этого дождаться
практически нереально то есть надо чтобы масштаб кси стал корень из н не логарифм и на корень из н
но это как-то не не тот сказать слишком долго ждать а вот тут коридор получается очень небольшой
здесь мы уже сразу как бы начинаем наблюдать эффект тяжелых хвостов но к чему я все это клоню
я это клоню к тому что давайте возьмем точку переключения и посмотрим что будет если вот
альфа если альфа такое что есть третий момент вот простое упражнение покажите что если есть
третий момент то есть если мат ожидания x к квадрате ну можно не писать но нолик потому что тут
в кубе если glove good меньше бесконечности меньше бесконечности то в этом случае больше трех
связаны это с тем что чтобы существовал мат ожидания случайной величины какой у
него должен должен быть плотность распределения чтобы был аматор ждание случайной величины у
него плотность распределений должна быть ну то есть вот очень простая логика если у вас есть
случайно величина x то что такое мат ожидания x мат ожидания x это есть
интеграл от x и x dx и x dx интеграла там от нуля до бесконечности да чтобы эта
штука сходилась надо чтобы она убывала как минимум как один на x
в тепени вся эта штука вместе один плюс чего-то маленькая бета бета маленькая
совсем но это больше нуля да но если грубо говорить это означает что x на p
от x должно убывать как хотя бы x на 1 плюс бета это значит что p от x должно
убывать как 1 на x 2 плюс бета бета сколько угодно маленькая но это значит
что если вы хотите третий момент чтобы был третий момент чтобы был вам надо
здесь поставить третью степень вот и вы тогда здесь ставите значит п от x
должно убывать как 4 как 4 плюс бета но теперь что такое вы от x вы от x вы от
x это вероятность вы от x это вероятность того что случайно величина x больше чем
x это есть в свою очередь интеграл это есть интеграл от p от x от p от давайте
y dy от x до бесконечности то есть это в свою очередь есть как бы сказать ну то
есть если это ведет себя как 1 на x 4 плюс б ну просто в четвертые то это есть
1 на x кубе правильно то есть вы от x как раз то то альфа которая определяется это
альфа альфа то есть альфа должно быть больше трем потому что вот это должно
быть больше четырех я объяснил не очень быстро но это и арифметика она такая не
очень интересная смотрите то есть мы получаем что альфа должно быть больше
трех но если альфа больше трех тогда ну просто вот я здесь напишу альфа больше трех это это
следует и существование третьего момента и там тоже третий момент так давайте сюда подставим
вот сюда подставим соответственно как себя ведет это слагаемая в этом режиме ну просто
конкретно то есть насколько поправка к цпт большая какая поправка будет к цпт вот вы ровно в
этой точке когда у нас происходит переключение когда фактор цпт ну сравнивается с фактором
больших хвостов это как раз будет нам что-то что позволяется поставиться вот с этой оценкой ну
и давайте посмотрим если мы подставляем закон значит вот сюда я уже это вынужден стирать если
мы подставляем в этот закон соответственно что нам надо поставить точку x равняется я уж не буду
вот эти все факторы писать пропорционален корень из n я ушла гори в тоже не буду писать корень
из n то у нас получается что в от x вот этой точке xn это xn да он будет пропорционален чему n в
степени минус 3 вторых правильно потому что это минус третья степень n в степени минус 3 вторых
вот так вот а теперь я умножаю это на n то есть мне надо на самом деле вот такую штуку изучить
n на v от xn и это будет себя вести как n в степени минус 1 вторая смотрите что мы получаем мы получаем
в точности то что здесь написано то есть схлопывается схлопывается ситуация то есть мы
действительно как бы целостным образом как-то постарались охарактеризовать современное
состояние дел вот в этих вот законах то сказать типу закону больших чисел предельных точив
типа предельных теорем то есть мы говорили о том что предельный теорем это все-таки не совсем то
что надо и вот это это как бы поправка это уже хорошо это поправка работает только в экспоненциальном
режиме а в режиме когда у нас соответственно нет гарантии что хвосты экспоненциальные обратите
внимание как все резко хуже портится то есть когда этого нет мы переходим в другую лигу то есть
если здесь у нас вот еще раз я повторю у нас здесь как бы это поправка типично маленькая по
сравнению с главным членом типично маленькая и чтобы она стала значимая нам надо ну за счет
чего она станет значимой если это одно и то же единственный фактор за счет чего она может стать
значимой что доверительный уровень вырастет сильно вот он станет настолько маленьким что это
слагаемое станет сопоставимо с этим что для этого надо сделать для этого надо чтобы корень из
логарифма 1 на дельта побил корень из n но это это я не знаю это как бы не адекватно ну в типичных
ситуациях а вот здесь это совсем не так здесь и много не надо чтобы побить чтобы выйти из
режима цпт то есть когда хвосты перестают быть экспоненциальные и это поскольку это результат не
улучшаем то это как бы не то что артефакт наших не знания просто жизнь становится как бы более
тяжелый в смысле тяжести хвостов и с этим надо бороться и вот как раз вот это вот хеовдинг и так
далее это и есть на самом деле подсказка как бороться а зачем нам эти тяжелые хвосты давайте
обрезать и часто это работает то есть вот этот клиппинг обрезание и работаем уже с величинами
которые имеют как бы ну просто надо правильно размер клипа выбирать и это трюки вот вот эта
наука которая с вами рассказывала она позволяет немножко как бы более целостно что ли обозреть
вообще все все вот это ну смотрите у меня сейчас надо паузу делать мне такой вопрос к вам как
лучше сделать оставшийся час значит я могу рассказать за оставшийся час вывод геометрического
броуновского движения это значит бушелье с муэльсен вывести ну это будет дальше дефузионный
процесс это определён потом мы напишем уравнение колмогорова фокера планка и
симуляции танилинг это вот то чем мы тогда закончим имитация отжига будет обоснование ну
а в следующий раз тогда я вам расскажу мулте лавел монтекарло то что не успел сегодня это
сделать второй вариант что я докажу неравенство азума хеовдинга но введу понятия мартинга
разности и покажу почему это важно на разобрав пример сходимости метода стахастического
градиентного спуска ну точнее начну это разбирать и покажу где это встречается а если не успею
продолжу в следующий раз то есть насколько стоит дожать эту линию продемонстрировал там вывод
допустим неравенство азума хеовдинга или вот можно переключиться оказом и хеовдингу вернемся
когда будет сказать подобающего мы настроение но это точно будет переключиться да то есть
немножко хочется развеяться и на большинстве само и цена да это веселее будет хорошо тогда
давайте паузу значит по пожеланиям присутствующих на лекции ну вот возникла траектория сегодняшней
лекции такая что мы на этом останавливаемся ну во всяком случае по части по части предельных
теорем и неравенство концентрации и доказательства доказательства неравенства хеовдинга мы
перенесем на ну видимо одну из следующих лекции но ближайших точно ближайших то есть мы не
будем откладывать это в долгий ящик а сейчас мы переключимся на вывод геометрического броновского
движения процесса башире самуэльсона вот и соответственно кстати сказать по-моему башире
был учеником по он корей конечно могу ошибаться но что-то у меня такое ощущение что это откуда-то
оттуда вот так вот мы сейчас попробуем получить геометрическое броновское движение как процесс
башире самуэльсона потом попробуем посмотреть на это все как на некий стахастический процесс
который называется диффузионный процесс и то напишем для него так называемое стахастическое
дифференциальное уравнение вот потом значит попробуем в целом разобраться какие с каким
ну сопоставить стахастическим дифференциальным уравнением уравнение калмогороба фокерапланка
и вот последнее позволит нам сделать такую важную вещь обосновать процедуру simulated
они линк имитацию отжига или там схема оклаждения вот по-разному все называется вот
наоборот затвердевание там я разные встречал как бы сказать сказки про то как это все можно
интерпретировать но я не знаю в какой степени мы до этого дойдем то есть я не имею желание как-то
сильно всех задерживать вот что у нас довольно много времени впереди и все мы это успеем и цель
именно вот разобраться сегодня вообще какая-то коллекция достаточно простая вот но вот она не
будет сильно сложнее во второй части итак значит вторая тема называется геометрическое
бровновское движение геометрическое бровновское движение ну бруновская я не знаю правильно я
пишу или нет потому что вообще по-английски браун ну коричневый не бронже вот ну брауновское
движение так труденем нас не принято говорить бровновское движение движение который кстати
скотинштейн занимался значит процесс бушелье самуэйсона процесс бушель я ба илья самуэйсона
самуэйсона ну и диффузионный скейлинг самуэйсона смысл нобелевский в ряд я не знаю жив он сейчас или
нет но относительно недавно он приезжал в москву вот и в общем очень такой известный матэкономист
значит диффузионный скейлинг диффузионный скейлинг скейлинг но книжка есть хорошую по экономике так
диффузионный скейлинг ну в принципе не знаю в общем то что я сейчас буду рассказывать отчасти
повторяет то что было у нас значит я попытаюсь понять насколько это да это довольно близко к
тому что было на прошлой лекции но есть небольшие отличия так процесс башелье самуэйсона не очень
хорошо у меня ша буква ше уехала башель я это самуэйсон самуэйсон значит вообще говоря этого
финансовой математики очень важные схемы она отличается от той схемы которую я рассказывал
на прошлой лекции только тем что сейчас цена акции может с ней происходить следующее она может
увеличиться значит экспонента sigma на корень из n давайте n возьмем вот корень из n а может
уменьшиться экспонента на цене то есть это фактор мы его называли у и д то есть д но когда
мы делали объясняли процесс винара мы делали скачки аддитивные то есть цена акции росла
на какую-то величину независящую от текущей цены акции то есть вот как бы просто либо прибавляется
либо отнимается но это же не не совсем так в жизни то есть у нас сложный процент процент берется
если его как бы и но налоги как мы платим мы имеем какой-нибудь бюджет там зарплату там 100 тысяч
допустим дают с нее платим налог 13 процентов вот но 13 процентов берется от 100 тысяч на зарплата
была ну 10 тысяч мы платили 13 процентов от 10 тысяч вот так значит ну соответственно процесс
продолжается и мы для простоты считаем что ну видите как подобраны уиде что они взаимосокращаются
но это важно чтобы у нас действительно было то сказать попроще хотя ну как на самом деле это
не очень важно нет стоп это важно это вот это конкретно важно да то есть чтобы они были иначе не
получится диффузионный скин да это не да правильно значение скал это важно вот и значит дальше у нас
вопрос такой то есть надо это все как-то значит делать во времени во времени и пускай у нас вот
время которое мы живем оно измеряется вот так это будет т большое которое есть и маленькая умножить
на n и это как бы соответствует реальному времени масштаба ты маленькая то есть у нас есть единица
вот она разделена на и на трезвочках ну блин плохо нарисовал значит вот это вот вот это вот
отрезочки вот и это вот условно единицы это n отрезков n отрезков n отрезков n отрезков мы
размещаем на единицы на на на промежутке времени 1 то есть tt которая здесь масштаб времени это
вот как раз один здесь и на отрезке от нуля до единицы времени мы разместили n отрезочков то
есть нас по и получается как бы маленькие скачки каждый отрезочек каждый отрезочек имеет длину
1 на n вот каждый отрезочек имеет длину 1 на n вот ну а масштаб скачка на отрезочке вот я сказал
какой скачу масштаб скачка на отрезочке то есть мы либо акцина в акции повышается либо
понижается ну и каждый тик это называется в финансовой математике вот каждый тик мы что-то
может произойти либо на поднимется в цене либо упадет ну и вот так вот по тика мы идем ты это
реальное время ну а как бы ты в секундах а это сколько за секунду может происходить каких-то вот
изменения условно секунды это конечно условно это не обязательно реально секунда ну дальше у нас
возникает в общем понятие цены акции в момент времени t с от чему она равна цена акции момент
времени t ну давайте считать что у нас есть случайная величина xk которая равняется соответственно
единицы значит единицы единицы извините если было движение вверх если двигались вверх двигались
вверх на катом шаге на катом шаге катом шаге ну и соответственно 0 иначе 0 иначе 0 иначе да а
ясно что эти x каты по определению независимые давайте зададим вероятности просто по определению
будем считать что эти вероятности одна вторая с вероятностью вероятностью с вероятностью то
есть скачки равновероятной 1 2 а 1 2 вероятность того что скакнем верх вскакнем вниз это вот
равновероятно ну в общем это довольно просто то есть мы просто случайно блуждаем по прежнему
но только теперь случайное блуждание осуществляется в масштабе что следующий
мой скачок он как бы зависит от того насколько далеко я ушел от начала ну не
от начала от этого масштаба s если я имею бы высокую цену акции я уже скачу как
бы масштабно если цена акции небольшая то ну то есть это пропорционально
осуществляется поэтому цена мультипликативно меняется то есть
надо записать ст как случайно величину определяющейся суммы экскатах но это
будет с в начальный момент времени умножить на экспонента вот опять
неудачно получается мне там а сейчас опять будет это мешать перегородка между
досками значит надо вот мне где-то написать вот здесь я наверное напишу
это мне ты на ты маленькая на н значит вот здесь я напишу что с от равняется с
умножить на что на экспонента экспонента ну и теперь надо аккуратно
уже значит писать что здесь получается значит давайте обозначим сумму ню и это
есть сумма экскат их ка от одного до ты ну напомню что ты равняется ты маленько
умножить на n вот такая вот история вот точка запятая ну соответственно здесь
стоять не что будет стоять ню те sigma на корень из n это сколько
раз мы двигались чего вверх ну на что еще на соответственно если technological
это сколько раз мы двиг Toris вверх то сколько раз мы двигались вниз еще рас
да ты совершенно верно здесь надо умножить на то насколько сколько раз мы
двигались вниз это будет t минус ню ты а ну и это будет соответственно тоже на
сигма масштаб у нас скачка на корень из-за да на корень из-за я так
подобрал что можем минус 1 сделать а я хотел чтобы вы как бы ну как сказать
это банально вот это как бы творческий сейчас подхожу мы сейчас поймем в чем
творчество заключается оно маленькое но все-таки элемент творчества здесь есть
чего да и нужен минус к экспоненте дада просто это как вы будете ну сейчас
поймем о чем я я просто хочу собрать здесь как бы не совсем стандартных цпт
чтобы вас возникли какие-то ну не знаю чуть-чуть другие формулы потому что уж
уж как бы приелись эти стандарты что у нас здесь получится у нас здесь
здесь получится такое выражение, что будет 2νt-t, давайте даже
сделаем так, мы вынесем соответственно 2σνt-t пополам, поделить на корень из n,
теперь что вы можете сказать про вот это безобразие, которое стоит в экспоненте?
ну я согласен, что я мог вести x-k-1, но мне просто вот захотелось собрать
стандартную схему Бернули, потому что с ней мы уже работали многократно,
νt чему равняется? то есть вообще что это такое? это есть с точностью до вот этого
фактора, то есть смотрите, значит 2σ это просто как бы сбоку бантик, как говорят,
давайте рассмотрим суть, а суть вот здесь, сумма x-катых ка Бернуливская случайная величина с
параметром 1-2, то есть это вот значит t-большое, t-большое минус, что здесь стоит?
мат ожидания этой штуки чему равняется? t пополам, так, на что надо поделить?
корень из чего? откуда 1-4? 1-2 и виниться минус 1-2, потому что дисперсия Бернуливской
случайной величины равняется p1-p равняется 1-2, 1-1-2, а мат ожидания
Бернуливской случайной величины равняется p, ну вот так, а p у нас равняется 1-2, вот, ну и
стало быть здесь они действительно могут сразу писать корень из 1-4, а что еще должен написать?
правильно, вот так, да, вот так, так, хорошо, и чему это равно? ну приближенно, если t-большое,
и которое есть n-0-1, так, так, так, хорошо, ну давайте теперь это перепишем, как бы,
ну сопоставим, сопоставим это вот с тем, что здесь написано, это же все-таки не совсем одно и то же,
ну первое, что происходит, значит, тут стоит корень из n, тут стоит корень из t, ну давайте
вспомним, что t равняется t на n, так, да, то есть надо, значит, что сделать? надо вот здесь написать
t на n, t на n, так, хорошо, что еще надо сделать? ну, наверное, надо, как бы, вот эту четверку,
она прям напрашивается, вот сюда ее перенести, стереть, и здесь написать двойка, так, хорошо,
чего еще надо сделать? ну, наверное, надо тогда корень из t, раз оно здесь его нет, значит,
надо его куда-то записать, да, то есть надо тогда его, значит, вот отсюда стереть, написать n,
а вот здесь его написать, корень из t, оно присутствует, но еще надо сделать следующее,
надо здесь сделать два сигма, два сигма, но тогда здесь надо сигма написать, сигма, вот так, правильно?
т.е. ты пополам, в чем вопрос? так, я пока что сделал, я просто собрал из того, что есть результаты,
вот такой результат, у меня сейчас вот так верно, то есть у меня есть, что вот эта конструкция,
которая здесь, которая здесь стоит, вот ровно эта конструкция, она есть вот это, ну, это понятно,
но, чтобы получить нормальную стандартную величину, мне надо ее отнормировать на 1 на сигма,
на корень из t, и тогда я получаю следующее, что, стало быть, вот эта конструкция, это есть
сигма на корень из t, на стандартную нормальную случайную величину, согласны? теперь вопрос к
если я умножу стандартную нормальную случайную величину на число, как она поменяется? это стандартно,
так, значит, вот, если у меня есть, значит, ну, как бы кси, то альфа кси имеет какое мата ожидания?
ну альфа на мата ожидания кси, т.е. мата ожидания просто на альфа умножится, ну альфа это вот это вот
А если у меня есть дисперсия? А дисперсия это что такое? Это мат ожидания центрального момента второго.
Я беру альфа, тут альфа стоит, когда это альфа с квадратом, так ведь?
Так, ну значит получается, что если я хочу как бы расписать, чему будет равняться закон распределения вот этой штуки,
я должен просто перенести вот это с квадратом вот сюда. Ну то есть написать, что вот эта штука
имеет нормальное распределение с параметром 0 сигма в квадрате на t. Я ответил на ваш вопрос.
Не ожидали до такого поворота. Ну вот, ну у вас был такой вопрос на самом деле.
Ну я не знаю, там все тоже самое на самом деле происходило. В общем, это все,
все то, что там происходило с обычной диффузией происходит здесь, просто здесь это под экспоненты
происходит. Ну еще раз, все, что я сделал, чтобы получить геометрическое броновское движение,
ну отличного от того, чтобы получить брон, просто броновское движение, я как бы перешел к факторам
вот мультипликативным, но сделал это хитро, сделал это с экспонентами. И это позволило мне,
то есть на самом деле я мог сделать любые другие факторы. И тогда мне надо было раскладывать в ряд,
и там писать как бы, ну не точно, и там тогда это было в первом приближении, но в 8 точки это было бы
верно. В этом пределе премьер стремляющейся бесконечности я решил, вот в это не морочиться,
и сразу сделать в нужном мне виде фактор. Зачем? Чтобы сразу ответить на, ну по аналогии с обычной,
так сказать, с обычным диффузионным скейлингом, вот это все называется диффузионный скейлинг,
чтобы у нас была аналогия просто с выводом Виннерского закона. То есть вообще говоря,
что мы сейчас получили? Мы получили, что SAT, SAT, если переходить уже в шкалированный вариант,
то есть писать здесь аргумент t маленькая, чтобы в реальном времени, то есть в пределе
премьер стремляющимся к бесконечности SAT равняется s, начальный момент s0, на экспонента,
что тут стоит? Ну стоит стандартный Виннеровский процесс w at, но это как бы не стандартный,
стандартный это с дисперсией 1. Сигма на w at, вот что здесь стоит, вот что такое получили мы сейчас,
то есть мы получили закон, который, ну собственно Виннеровский, то есть вот это вот Виннеровский
закон. Почему? Потому что когда я перехожу к логарифмам, это и есть то самое обычное случайное
блуждание вот в этом пространстве, то есть в пространстве, которое сидит вот в экспоненте,
я получаю тот самый закон с те самые независимые приращения, потому что вот мы сюда пришли и дальше
мы пройдем будем двигаться, они независимые, то приращение, которое я получил на отрезке на этом и
на не пересекающемся отрезке, это независимые случайные величины, то есть процесс с независимыми
приращениями, какие бы два сечения я не взял, просто за счет измельчения можно всегда так вот
измельчить вот эту динамику предельным переходом, что мы получим как раз независимость приращения,
ну а то, что сечение имеет такой закон, однозначно определяет сам закон, сам процесс случайный.
Здесь еще есть очень важное место, которое меня почему-то никто не спросил, оно действительно
важное. Вот я два раза сделал один и тот же трюк и все как-то вот приняли это как должное,
ну вот так вот надо выбрать и все и я как бы так сказать рассказал и все хорошо все-таки
ну замечательно схлопнулось, а почему я так выбирал вот эти величины, никто не задумывался
над этим вопросом, что было бы, если бы я выбрал по-другому. Вот действительно, если дело в том,
что это вообще не просто так, это называется диффузионный скейлинг, это приблизительно из той
же серии, а почему в центральной предельной теореме шкалируем на корень и зен, так если мы
будем шкалировать не на корень и зен в центральной предельной теореме, то у нас получится,
но и на другой степени, то у вас будет либо 0, либо плюс бесконечность, ну в смысле что-либо
соответственно, ну то есть как объяснить, ну вообще-то какая-то философия, а почему в закон
всемирного тяготения там двойка стоит, так если была бы не двойка, не было бы вот этих вот всяких
красивых вещей там эллиптических траекторий, много чего, то есть интересная как бы жизнь
появляется только при правильном скейлинге, в других вариантах все тривиально, все тривиально,
все вырождается, и вот именно при одном значении n что-то интересное получается, совпадение нет,
это не совпадение, потому что ну просто как бы повезло, да нет, это мы шли к этому, то есть мы как
бы заранее понимали, что единственный масштаб вот как такой движухе, который приводит к чему-то
осмысленному, он только такой, ну понятно, что здесь факторы могут быть, но если вы по-другому
будете шкалировать, то вы либо взорветесь, ну в смысле траектория взорвется вот это вот, уйдет на
бесконечность за там конечное время или почти сразу, либо ну около нуля болтаться будем, и не ощутим,
что вообще что-то происходит, то есть в масштабе цены акции на масштабе реального времени мы не
будем различать наш процесс от нуля, то есть он будет что-то происходить, но это будет какие-то там,
я не знаю, то есть это будет как бы ну если мы скелинг выберем неправильно, а вот это правильный
скелинг, он обращая внимание такой, что время идет как 1 на n, то есть масштаб шага 1 на n, а если как
бы посмотреть на экспонента, то это есть 1 плюс соответственно стигма на коре низен, из точки
зрения вот как бы поправочного фактора, то у вас поправочный фактор получается как бы больше,
то есть шаг больше, чем шаг по координате, чем шаг по времени, и это важно, но то же самое было при
обосновании Винеровского процесса, почему важно, потому что если вы возьмете шаг скажем тоже 1 на n,
то за счет того, что есть вот это вот разброс, вы просто не заметите, тоже еще CPT надо применять,
то есть здесь важно, чтобы как бы вы то, что вы идете в одну сторону, вы идете и обратную сторону,
и поэтому масштаб вот этих блужданий, он должен быть сильно больше, чем вот это, то есть вот это
колебания, мы должны как бы перейти на масштаб CPT и различать вот это все, то есть другой скелинг
не диффузионный, здесь ничего не даст, и поэтому только в таком режиме наблюдается что-то интересное,
значит понятно, что ну в реальной жизни наверное это какая-то модель искусственная слишком,
но как это часто бывает, в общем если брать какую-то идею, то можно на самом деле накручивать на это
уже более содержательные какие-то постановки и в принципе приходить к тому же, то есть просто
обобщать, также как CPT, вот я рассказал центральную предельную теорию, ну я например не упоминал,
что она может быть верна для например слабозависимых случайных величин, ну что же она верна для
неодинаково распределенных, это вам в курсе теории вероятности рассказывают, она верна для там грубо
говоря для мартингалов, там мартингала разности, ну такого типа, для марковских цепей она верна,
то есть XK это будет последовательность случайных величин, которые цепь Маркова, в эргодических
теоремах, то есть когда динамическая система, там тоже есть теоремы типа центральной предельной,
это все очень интересно и в этом смысле как бы ясно, что это идея, то есть это как бы заготовка,
на базе которой можно уже что-то более такое жизненное, и вот здесь просто важно понять вот эту
идею, понять что есть вот такой диффузионный процесс, который называется вот так, процесс Бышельев-Самуэйсона
геометрическое броновское движение, теперь это был какой-то вопрос, могли, мы вообще все можем,
ну и хорошо выбрали бы вы два сигма и получили бы здесь Винеровский процесс какой-то константой,
и это было бы, ну это дело вкуса, это дело вкуса того, что называть сигма, и загнать это можно было бы
в это сигма, вот я подобрал так сигма, чтобы эта сигма в точности соответствовала, ну в неком смысле
дисперсии, да, то есть чтобы это было компактнее, но подберете вы тут два сигма, ну вылезет у вас тут двойка, ну
если мы шагаем вперед меньше, чем назад, у вас не получится вот этого места, и там все в разнос пойдет,
то есть вас как бы вынесет в сторону, то есть вы как это, то есть так вы пьяница, честный пьяница,
а так вы не пьяница, а уже как бы вполне, так сказать, имеете направление движения, это не совсем
случайное блуждание, вот, то есть вам надо быть прям вот реально как бы, как это вот в этих
задачках, какова вероятность того, что там, ну, собственно, это вот и серия, что, я не знаю, что, ну,
когда вот эти задачки исследовали, то было обнаружено, что, например, по парку гуляет,
люди часто встречаются друг с другом, а гуляют они случайно, а парк имеет Манхэттенскую топологию,
почему это так? Ну, грубо говоря, перейдя в систему, координат связан с одной парой, которая гуляет,
другая пара просто независимо случайно блуждает по вот этому парку, и случайное блуждание
возвратно, то есть она возвращается, вот, грубо говоря, в изначальную точку пересечения,
но бесконечное число раз там, в общем, за, на бесконечном времени, то есть через конечные
промежутки будет возвращение. В пространстве это не так, вот, и это я к чему говорю, что, вообще говоря,
вот все это очень, как бы так сказать, вот эти вот вопросы, связанные с вот этими постановками,
они довольно сильно будораживали людей, они просто вот реально, многие классики, типа Поя, там вот,
ну, Коднот, они просто в жизни все это пытались как-то сказать, такие законы, которые связаны
с этими эффектами, наблюдать, и потом появлялись очень такие яркие задачки, вот у меня нет
возможности сейчас про все это рассказывать, там действительно очень много сюжетов, моя мысль была
только в том, что вообще все вопросы, которые конкретно по случайным блужданиям, вот, можно
задать, ну, вот, есть замечательная книжка Фюллера, там очень много написано, ну, есть специализированные
учебники, и вот, например, популярные книжки, где очень много ярких сюжетов на эту тему разобраны,
то есть я думаю, что просто, наверное, действительно, вот, конкретная тематика, вот это, она очень хорошо
изучена, и мне не хочется на ней застряться, потому что она в чем-то карикатурная, то есть это реально
такая, как бы, ну, слишком упрощенная модель реальности, то есть мне хочется уже перейти немножко к более
взрослым вещам, которые изучают в курсе стахастических дифференциальных уравнений, то есть сейчас я
прям расскажу вам элементы 100 диффуров, диффуров, стахастических дифференциальных уравнений, уравнений,
дайте, чтобы это было по-человечески, уравнений, диффуров. Так, ну, хорошо, пускай у нас есть процесс
САТ, геометрическое броносское движение, ну, честное геометрическое броносское движение, которое
записывается вот так, значит, АТ плюс Сигма ВВАТ, это Винерский процесс. Что можно сказать про дельта
САТ? Что такое дельта САТ? Дельта САТ, ну, как бы, наверное, надо просто продеференцировать, да, вот это
дело про ВВАТ и написать, что это есть там С0, ну, просто САТ, да, на, соответственно, А на дельта Т
плюс, соответственно, САТ, ну, это можно вынести за скоб в таком случае, значит, что там будет, ну, Сигма на дельта
Дельта В ВВАТ, и вот так ведь будет, да, но, как объяснить, на самом деле, на самом деле, здесь есть какое
неприятное место, что Винеровский процесс, Винеровский процесс, который вот вы видите, он обладает, ну, не совсем, что ли, как
объяснить, классическими такими свойствами, потому что, ну, что-то я как-то написал так, как будто две независимые
переменные, так ведь? То есть, я написал формулу, как будто у меня, ну, отдельно Дельта ВВ, как-то меняется. На
самом деле, здесь есть эффект так называемого второго дифференциала, что Дельта ВВАТ в квадрате, какой масштаб он
имеет, вот, если все-таки Дельта Т в квадрате, но это неправильно, потому что, ну, что мы понимаем по Дельта Т,
Ф от Т плюс Дельта Т минус ВВАТ, вот, что мы понимаем. И, как бы, вот эта форму, она перестает уже работать, потому что у нас сейчас, как бы,
это Дельта надо искать, ну, тоньше, то есть, это же Дельта, это не Д, а Дельта, то есть, я же не пишу Д, пишу Дельта, и проблема в том,
что, вот, если, как бы, посмотреть на закон распределения вот этой случайной величины, он какой будет? Он будет нормальный
закон с параметром 0 Дельта Т, так ведь? А если я возьму от ожидания от этой штуки, чему оно равно? Нулю. А если я возьму от ожидания от квадрата этой штуки,
чему оно будет равно? Дельта Т. Вот, вот это принципиальное место, которое отличает Винеровский процесс от, как бы, просто, от какой-то траектории функции, то есть,
если вы возьмете любую функцию и возьмете эту функцию, возьмете Дельта В в квадрате, и вот, как бы, будете смотреть, что как Дельта В зависит от Дельта Т, то для любой нормальной функции
масштаб Дельта В должен равняться Дельта Т, а для Винеровского процесса Дельта В в квадрате равняется Дельта Т, это значит, что процесс очень такой, постоянно скачущий,
и это значит, что типичная траектория Винеровского процесса нигде не дифференцируемая функция, то есть, у нее нигде не ограничена константа липшица, но все время так вот, но непрерывная, но непрерывная траектория.
То есть, если вас попросят сходу назвать пример функций, которые нигде не дифференцируемы, но везде непрерывно, возьмите любую траекторию Винеровского процесса, то есть, типичная траектория Винеровского процесса, она такая.
Но я к чему это сказал? Я это сказал к тому, что на самом деле, вот если аккуратно писать приращение вот этого процесса как бы по Дельте Т, то есть, надо просто разложить в ряд по Дельте Т, то есть, вот здесь мы просто смотрим, что будет, если разложить в ряд Дельте Т, но проблема-то в том, что вот эта ВВАТ, оно не классическим образом себя ведет, и поэтому здесь надо учитывать,
вот если по-хорошему писать, какие-то надо писать слагаемые Дельта Т, Дельта там, ну какие-то факторы, да, фактор Дельта ВВ от Т на Дельта Т, и вот будет фактор Дельта ВВ в квадрате, Дельта ВВ в квадрате, там от Т, и будут еще всякие повышенные степени, так вот, казалось бы, если бы мы работали в обычном анализе, то вот это все надо зачеркнуть, то есть в Дельте это неинтересно, потому что мы стремим Дельта к нулю, ну как бы дифференциал функции,
как он по сути производный определяется, да, то есть Дельта определяется производной вот этой штуки умножить на Дельта Т, но сейчас это не так, потому что с этой функцией связана проблема, она связана с тем, что ее второй дифференциал, он в среднем себя ведет как бы как Дельта Т, и вот оказывается, что действительно, вот эта вот штука, то есть, которая здесь значит стоит, она собственно получается разложением
в ряд, то есть нам надо будет здесь получается Сигма в квадрате на два фактор сделать, потому что это разложение в ряд, вот отвечающий Т Лора, отвечающий этому слагаем, то есть просто разложите в ряд вот эту функцию, в ряд по Т, считая, что ВВ тоже как бы соответствующая, так сказать, функции, вот вы получите такое слагаемое, но проблема в том, что оно теперь не сокращается, то есть оно теперь дает вклад, и вы получите, что Дельта САТ, ну я, естественно, очень грубо это все рассказываю, но тем не менее,
физика будет такая, плюс Сигма в квадрате, значит, что там у нас пополам, вот, значит, плюс, сейчас, минутку, плюс, так, у нас там САТ, да, плюс,
плюс получится вот это вот, Дельта Т здесь будет, Дельта Т, ну а здесь будет Сигма, значит, плюс САТ на Дельта В, вот, ну, естественно, тоже можно писать АТ, вот, то есть, производное превращение вот этого САТ, оно определяется хитрее, то есть, нам надо не забывать учитывать вот это слагаемое, но это как бы не самое сложное,
что сейчас может быть важное в этом во всем, а самое важное начинает происходить, когда мы начинаем, вот если мы это закрепим, то есть вы, если что-то не поняли, это не страшно, потому что мы сейчас попробуем сделать нечто более такое общее, то есть мы попробуем это дело закрепить, каким образом, мы попробуем расписать, что будет, если мы возьмем функцию от какого-нибудь вот такого процесса С, Ф от С, и попробуем взять дифференциал вот от этой функции,
ну, не дифференциал, но Дельта, превращение, вот давайте закрепим это такое типичное управление, можно для общности написать, что это есть процесс ХАТ, где ХАТ это вот такой процесс, Дельта, ну давайте, формально он так записан, Дельта ХАТ равняется А от Х на Дельта Т плюс Сигма от Х, ну Сигма от Х на Дельта В от Т, и для, значит, общности мы даже
будем считать, что Х это, Х это вектор, вот, А это вектор, Сигма это, соответственно, вектор, и умножение понимается по-компонентно, а это траектория Винеровского процесса, то есть В это, значит, Д, ну, как бы, это В1 от Т, и так далее, ВН от Т, где-то независимые траектории разных Винеровских процессов, которые называются, ну, вот такой многомерный Винеровский процесс.
А Х это неизвестный, ну, в смысле, Х это то, что, как бы, мы хотим определить, то есть Х это вот то самое, что нам интересно, то есть мы, это, это называется сток дифференциального уравнения на, на, как бы, вот, неизвестный параметр, ну, например, вот если у нас есть классическая система каши, dx, pdt, равняется минус градиент f от х, что это за такая система дифференциальных уравнений, ну, это система дифференциального уравнения,
дифференциальных уравнений, которые позволяет по траекториям, если непрерывно их отслеживать, находить минимум функций, ну, или локальный минимум, то есть это система, которая скатывается по антиградиентной динамике, но есть проблема с этой системой, если система в локальный минимум упадет, она там застрянет, и на практике, на практике делают как, берут вот эту систему dx, pdt, минус градиент f от х, и добавляют шум, который позволяет
искакивать иногда, вот, из этих вот, я это рассматриваю, такую систему, но шум надо как-то описать как процесс к сиатте, вот, собственно,
оказалось, что удачная форма записи, сток дифферов, где это, как бы, производная шума винеровского,
неудобно так записывать, потому что шум нигде не дифференцируем, и он винеровский, ровно потому, что нормальное распределение
аккумулируют независимые шумы, а винеровский процесс аккумулирует, соответственно, всякие маленькие факторы, маленькие процессы,
независимые, вот он аккумулирует, получается, винеровский процесс, но записывает это вот так, dx равняется минус градиент f от х,
соответственно, по dt, по дельта t, ну dt, давайте, я сушу, начал писать, вот, но и здесь добавляется, соответственно, плюс,
обычно пишут фактор температуры, корень из 2t, это то, что сигма будет число, это уже теперь не функция, ну, для данной модели,
на винеровский шум, но винеровский шум той же размерности, что и вектор x, то есть этот процесс векторный, и вот оказывается, что вот такие вот,
вот такая вот штука, вот такая вот штука, собственно, к чему мы сейчас идем,
то вот такое дифференциальное уравнение стахастическое,
это же x, случайная величина, и вот оказывается, что в осимпотике при t, стремящемся к бесконечности,
при t, стремящемся к бесконечности, плотность распределения случайного вектора x, это даже не случайная величина, а случайный вектор, значит, эта плотность,
она пропорциональна, то есть с точностью донормировочного фактора экспоненте от минус f от х,
минус f от х, делить на t,
то есть получается, что если вы замените
настоящую систему каши,
зашумленной системой каши с шумом корень из 2t, это винеровский процесс, то вы не сойдетесь, конечно, к минимуму, оно и понятно, потому что у вас вообще
нет понятия сходимости как таковой в обычном понимании, потому что x это случайная траектория на бесконечном времени,
ну раз это процесс марковский, естественно там,
ну в общем, короче говоря, на него можно написать вот эти стандартные условия, что будет в осимпотике, это условие пишется так,
что dx под dt перестает меняться, значит, возникает какое-то такое уравнение, его надо как-то разрешать, вот как это делать, мы сейчас этим займемся,
но то, что получается, мы это проверим, это будет такая функция и
собственно, идея использовать шум вот в таком методе,
ну или в его дискретном варианте, эта идея заключается в том, что мы хотим найти минимум функции,
если мы сделаем t маленьким, маленький шум,
то тогда это распределение будет концентрироваться
около того самого минимума, который нужен, что замечательно, это означает, что если t маленькая,
то как бы предел вот этой динамики, он будет давать глобальный минимум решения некой задачи,
и это уже решает проблему застревания в локальных минимумах,
и как бы это круто, это вот вроде как вообще все, мы умеем решать задачи глобальной оптимизации, но дело в деталях,
чем t маленький, чем t большое меньше,
тем время выхода больше,
тем время выхода больше, и с точки зрения практики это становится, в общем, той же самой проблемой, что можно найти минимум функции
перебором, можно, в принципе, можно, но как, ну с нужной точностью, но это экспоненциальный перебор,
здесь та же проблема, то есть это не решает, естественно, никаких проблем, но есть такая идея, она неплохо работает,
идея заключается в том, что вот если мы рассматриваем такое стахастическое дифференциальное уравнение и делаем t большое,
зависящим от t маленького, и делаем эту зависимость,
значит сейчас подсмотрю, по-моему, вот такой, и t большое это вот то, что стоит здесь,
это температура,
температура, в данном случае это температура,
вот, это называется схема охлаждения, сейчас я подсмотрю какая там температура у нас,
а температуру у нас, ну я, конечно, не точно написал, как всегда, но можно подправить,
поправимо не точно, c на 2 плюс t, вот так, вот это называется схема охлаждения,
схема охлаждения, короче, одна из самых популярных схем охлаждения, вот, и это уже на практике часто очень неплохо работает
такого типа результатов, и наша задача сейчас,
насколько это возможно за оставшееся время, постараться понять вообще,
как сделан ключевой переход, потому что в принципе понятно, что я сейчас рассказал, ну мне кажется, это рассказал схему имитации отжига,
simulated annealing,
simulated, simulated annealing,
ну, в общем, annealing,
и вот эта схема базируется на одном простом факте, ну относительно простом,
если взять систему стахастических дифференциальных уравнений, полученные за шумлением
искусственным, его оно как бы не было изначально, чтобы выскакивать из этих минимумов, то вот получается такая плотность, и дальше игра на том, что мы уже любим делать
концентрации меры вокруг
минимума, потому что вот это распределение, оно будет
сосредоточено около минимума и значит просто случайно,
ну как бы типичная траектория, она будет не просто сходиться на эту меру, она еще с большой вероятностью
будет находиться там где минимум этой функции, поэтому просто живя на какой-то случайной траектории, на большом
времени мы не просто выйдем на это распределение.
Мы еще и будем жить около минимума этого распределения, то, что нам нужно, то, что мы уже встречали, но остается вопрос, как вся эта математика
тематика связано вот с вот как это получить как получить переход отсюда вот сюда по
мото интересно и хотелось бы это сделать не слушая кустов где фуров потому что это
семистровый куст а как-то вот самое главное из него взять ну самое главное для наших целей да
смотрите это формально формально так определяется вот что то есть я понимаю то есть по-хорошему мне
надо значит строить здесь некую науку наука будет заключаться того вот в чем что здесь
определяется x от и плюс дельта t значит минус x от и дельта t это т т плюс ну как бы дельта
t есть дельта t а дв от это есть дв от значит плюс один плюс дельта t минус дв от возникает
следующий вопрос хорошо а тогда что такое вот это а вот это именно в точке x от то есть здесь
важно чтобы не брать какую-нибудь промежуточную точку вот это принципиально важно потому что
если я буду понимать под вот этой схемы ну когда я написал где вот в таком виде то дальше вы
можете просто сказать что соответственно x от и конкретная штука просто есть вот предел в
каком-то смысле вот при измельчении полученный от конечно разностной такой схемы то есть вы
просто заменяете что вы что такое есть вот эта величина это просто есть гауссовская случайная
величина с параметрами 0 дельта t ну вот и все то есть вы можете как бы симулировать вот эту
по динамику чисто пределам дискретных ну как бы процессов то есть ну вы же можете генерирует
нормальную случайно величины можете сделать вот это вот разбиение отрезка от нуля до t на
маленькие отрезочки и рекуррентно решать вот это дальше доказывается что то есть формально
запись вот это означает что как бы мы это измельчение не делали этот предел существует не зависит вот
это все такое но это я не знаю это как бы немножко уводит нас в сторону то есть от того то есть это
вопрос определение его корректности это корректность требует чтобы эти функции были липшицы вы ну если
интересно можно об этом поговорить но это еще раз это уводит нас от главного то есть нам вот как
бы липшица в это функции не очень это сейчас не самое важное потому что любые разумные функции
которые мы работаем они липшица вы на той области где вот мы с ними работаем это правда виновски это
проблема да это проблема ровно по этой причине мы рассматриваем такую форму записи я не за не
пишу ее как дифур вы видите что это не дифур нет подождите никто зачем нам делить на дельта t нам
не надо делить на дельта t потому что если делить на дельта t будет проблема но смотрите мы можем
проинтегрировать можем получим икса t икса t равняется интеграл а икса t то есть вот вот что
нам надо нам надо понимать дифуру вот в таком виде если уж хотите давайте тогда вот я просто
формально определю где интегралы понимаются в смысле среднего квадратического ну сходимость
я не знаю насколько это сейчас стоит сюда погружаться то есть здесь надо определять что такое вот этот
интеграл что такой вот этот интеграл это не очень интересно надо пределить что такой вот этот
интеграл но вот этот интеграл он определяется таким образом ну тут формально значит он
определяется как предел среднем квадратичном то есть при вот таких вот сумм sigma x от t и t на
соответственно дельта w t и t плюс один минус w t и t вот соответственно значит мы берем отрезок
от нуля до t берем вот эти t и t и t и максимум максимум дельта t и това при по и стремится
к нулю при соответственно при ну тут берется предел и от одного до n при n стремящимся к
бесконечности вот и предел понимается смысле среднего квадратичного то есть вот эта
последовательность случайных величин сходится в л2 а в пространстве гильбертовом пространстве
скалярных гибридом пространства случайных величин обладающих вторым моментом сходится к
чему-то вот то что к чему это сходится называется таким интегралом все катарсис вот ну все теперь
уже с чистой совестью можно идти дальше хорошо идем дальше ну значит что ну мне известны дата
ну в принципе в книжке книжки аксиндалия это все есть есть хорошая книжка аксиндалия стокастические
дифференциальные уравнения там все это можно найти много книжек есть но вот аксиндалия переведен
на русский язык довольно хорошо там все написано коллеги значит нам надо добить это дело иначе как-то
вот прям обидно будет все что-то вокруг да около ходим я остановился на том что нам надо найти
дифференциал вот этой вот функции дельта f от x при условии что дельта x равняется а от x на дельта
t плюс соответственно стигма от x на дельта w вот как найти дифференциал f от x ну давайте
повторим вот эту всю конструкцию которую я сейчас делал ну то есть нам надо просто написать
ну это и кстати удовлетворяет вот этому уравнению нам надо просто написать что у нас как бы
происходит с точки зрения как сказать изменений с точки зрения изменений вот этой вот функции то
есть у нас во первых будет из-за того что дельта t меняется мы как производную сложные функции
будем считать у нас будет вот следующее то есть мы берем значит соответственно градиент f от x
умножаем его скалярно на a от x вот ну можно это по написать как сумму давайте изнаю то есть
чтобы уж как бы совсем было понятно d f под иксы т ну умножить на а от x и т ну на дельта t вот с
этим я думаю все понятно откуда это слагаемо появилось потому что дельта x сам имеет такое
превращение там от одного да и на число компонент то есть dim x равняется размерность
x равняется n так это все нет конечно это не все потом естественно будет слагаемая которая
определяется значит вот этим вот делом да ну и давайте для сказать пущей пущего интереса нам
будет интересно все это брать от ожидания просто от этого брать вот но мот ожидание условное то
есть момент времени t то есть это x до момент времени t момент времени t и при замороженном x ну
то есть как объяснить я немножко будто на это объясняю значит смотрите я просто хочу потом
взять мот ожидания чтобы не возиться с этими слагаемыми которые там имеют нулевой мот ожидания
поэтому не надо брать мот ожидания но с другой стороны я не хочу брать мот ожидания по вот этим
вот x и там то есть я не хочу сюда навешивать мат ожидания что мне для
этого надо сделать мне для этого надо сделать следующее мне надо определить
значит момент времени t и сказать что x от заморожен x от ну известен и просто
это вектор x вот это известно и поэтому как бы я здесь напишу вот такой верхний
индекс x это означает вот я поясню что здесь мат ожидания от чего-то при
замороженном x от и в момент времени x момент времени t момент времени t от
чего я беру мот ожидания а мот ожидания беру от дельта f дельта f от x
дельта f от xt но вот эта штука за счет того что здесь дельта стоит это есть что
такое это есть f от xt плюс дельта t вот что здесь стоит минус f от xt то есть
замороженным тут будет только вот это слагаемая а вот это не заморожено то
то есть вот это случайная и это важно понимать то есть все таки мат ожидания
будет почему брать вот этой по определению есть мат ожидания от xt
ну естественно моментứng просто без уже не будем писать условности вот я
продолжаю писать то есть мне здесь сейчас мот ожидание брать не надо
потому что xt замороженный в момент времени t берется то есть это именно тот
икс который в момент времени t вот ну вот этот вот дальше я пишу соответственно
при вот этой вариации то есть то что получается при этой вариации то есть у меня получается значит
тогда так сумма df но опять-таки по dx это на что на сигма и ты от x на дельта w и ты ну я от одного
до n вот но когда я возьму от ожидания от этой штуки по x вот тут уже как раз все обнулиться вот
вот это все обнулиться и нам от ожидания потому что мы от ожидания дубы до дельта
дубль вы это равняется нулю но это не все здесь будут слагаемые соответственно которые отвечают
ну следующему разложению в ряд ты лора то есть здесь будет значит соответственно сумма df по
ну в общем по разные смешанные производные и здесь будет стоять дельта ты аи ты аи ты
сигма жито и здесь будет значит дельта дельта ты но не дельта дд нет дельта дельта ты на дельта
w дельта ты и ты над дельта w жито но это тоже 0 потому что дельта w жито тоже равняется нулю да тут
одна вторая конечно она у меня остается еще вот такой слагаемая она слагаемая по дельта т в
квадрате мне не интересно оно как бы малая а вот интересно слагаемая вот такое по x 1
вторая сумма d 2 f по dx и т в квадрате dx и т в квадрате на дельта w и т ну еще есть смешанные
слагаемые дельта w блин ну я не знаю это все писать но они тоже обнуляются потому что они
независимы то есть тут будет либо так либо либо это один вариант а второй вариант давайте
напишем сигма и ты сигма жито но тут дельта в дубль выитая дельта дубль выжита и вот эти дельта
дубль выитая дубль выжита когда и не равняется же они независимы по определению и тоже 0 то есть
это я как бы это убиваю а вот здесь остается вот квадрат вот вот такая штука остается 1 на n и
вот от этой штуки уже мот ожидания давайте вот сюда перенесемся не равняется чему-то маленькому
оно будет сопоставимо таким образом мы еще раз проделываем этот трюк то есть берем от ожидания
от икса от дельта f от икс вот икс да дельта f от икса он равняется чему оно равняется соответственно
сумма df по dx и t на а и т от x на дельта t и от одного до n плюс плюс значит ну вот то что
там получилось 1 вторая чуть-чуть место сэкономим 1 вторая сумма df d2 и по dx и т в квадрате
соответственно на что там у нас стоит сигма а подождите никто меня не поправил а тут же
сигма должна быть в квадрате вот да тут должна быть сигма в квадрате она же отсюда приходит и
куда она денется никуда фигма и т в квадрате дельта w и т вот но сигма зависит от икса который
заморожен поэтому оно у нас как бы а это уже в будущее идет ну можно делать матрицы но это
будет более сложная тогда здесь будет недиагональная форма я решил упростить жизнь было благо пример
который нам нужен он как бы диагональная да поэтому я не хочу общую ситуацию рассматривать это
не сложно это действительно можно сделать сигма и т в квадрате на дельта на дельта т но я считаю что
все дельта т и ты одинаковые то есть мне тут ты дельта т можно и ты не писать вот смотрите что
получается то есть получается что если так формально написать что что что мы имеем мы имеем что f от x от
t плюс дельта t минус f от x t поделить на дельта t на мат ожидания этого всего от x ну в момент
времени t вот то есть это мат ожидания от x напомню оно просто фиксирует в момент времени t чему
равняется x от t так вот эта штука равняется оператор а на функцию f этот оператор называется
ифизимальным оператором вот это вот марковская марковского процесса или то сказать оператор
задающий полугруппу ну группу полугруппу точнее вот значит и у нас получается что мы как бы можем
достаточно изящно вот я посмотрю сколько у меня времени остается хорошо но наверное мы тогда на
этом остановимся значит просто намечу план что будет следующий раз значит мы сейчас сделали очень
важную вещь мы ввели вообще говоря некоторые условия на то как преобразуется вот это вот
функция от случайного процесса и дальше на самом деле с помощью вот этого ну как бы
свойства мы можем написать достаточно красивую формулу которая называется формула по моему дынки
на это называется да

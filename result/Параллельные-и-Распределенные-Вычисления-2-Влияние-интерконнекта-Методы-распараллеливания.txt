Сегодня, наверное, будет немножко поскучнее, чем было
в прошлый раз, потому что будут там определённые,
с вашей стороны должны быть рассуждения, напряг.
Ну посмотрим.
Итак, мы в прошлый раз остановились на законе Амдала, который
говорит о том, что если у нас не вся программа, не
весь алгоритм может быть распараллелен, то существуют
определённые не то что трудности, но отклонения,
скажем так, нашего графика ускорения возможного от
идеального случая.
Мы здесь рассматривали о том, что если альфа – это
часть последовательного алгоритма, то к чему стремится
ускорение в зависимости от степени параллелизации
нашего алгоритма.
Если часть, которая не может быть распараллелена,
составляет, например, 10%, то есть альфа от 0,1, то у нас
ускорение, сколько бы мы много ни брали вычислительных
ядер, то есть решателей, не может быть больше, чем
десятка.
То есть это накладывает определённые ограничения
на возможное ускорение.
Но на само ускорение влияет не только та часть алгоритма,
которая не может быть распараллелена, но ещё определённые другие
факторы.
Об этих факторах мы сейчас и поговорим.
Второй причиной, которая влияет на ускорение, является
в общем-то говоря та самая сеть, в которой соединены
узлы в кластере.
Поскольку вы будете большей частью проходить MPI, то, ещё
раз повторяю, MPI в переводе или в расшифровке означает
Message Person Interface, то есть это интерфейс передачи сообщений.
Сообщения передаются между процессами, сами процессы
работают на вычислительных ядрах, представим в голове,
что наш кластер состоит из узлов, каждое узло это
одно вычислительное ядро, представим себе именно
так.
И вот на таком ядре вычислительным работает процесс, поскольку
все процессы они выполняют или решают какую-то одну
большую задачу, то рано или поздно они должны обмениваться
между собой информацией.
И вот эта информация в виде переменных, в виде частей
массивов, она передаётся между узлами, то есть между
вычислительными ядрами, то есть между процессами
и она передаётся по какой-то сети.
Эта сеть называется интерконектом и она, конечно же, будет влиять
на вообще время работы программы, потому что она требует
время для передачи сообщений.
Вот рассмотрим это влияние.
Здесь сейчас мы на этом занятии будем рассматривать
времена затрачиваемой программой в теоретическом плане, то
есть мы будем представлять о том, сколько примерно
тратится времени на вычисление какого-то числа, то есть
мы будем говорить о времени выполнения какой-то арифметической
операции.
Конечно, мы не будем учитывать на самом деле время, затрачиваем
на организацию цикла либо на времена считывания
переменных из оперативной памяти, потому что в целом
это не как рассмотрение без этого, рассмотрение
просто какой-то арифметической операции в целом не накладывает
каких-либо ограничений и вообще позволит нам определить
характеристики алгоритма, то есть это не помешает нам
это сделать.
Поэтому мы там и архитектурные особенности какие-то не
будем рассматривать.
И вот если говорить о временах расчета, то мы просто будем
например перемножать количество арифметических операций
на время одной операции.
Вот tau c это время одной операции, c это calculation, то
есть tau c это время выполнения чего-то.
Не будем тоже вдаваться в подробности, что там умножение
это больше по времени чем сложение, деление больше
чем умножение и так далее, просто мы делаем какие-то
определенные оценки и в целом говорим о том, как
алгоритм работает.
А вот когда вы будете на семинаре, там вы можете
уже измерять времена, реальное время работы программы.
Примерно будет то же самое, конечно что-то будет отличаться,
но в целом поведение алгоритма или вашей программы должно
соответствовать теоретическому рассмотрению.
В этом весь есть смысл.
Итак, вы будете измерять как раз ускорение алгоритма
и вообще говоря в первой задаче у вас алгоритм будет
практически полностью распараллелен, то есть там
альфа будет не очень большой, понятно, там будут какие-то
определенные действия, направленные на выделение
памяти, организацию тех же циклов, это нельзя распараллелить,
но большая часть нагрузки вполне подвержена распараллеливанию,
поэтому ожидается, что альфа будет маленький, не
очень большой, но так или иначе у вас при различных
запусках могут быть различия в ускорении, и вот на это
влияет как раз сеть, интерконнект.
Да.
А в каких единицах ускорение принтое измеряет скорость
программы?
Здесь скорость программы, сейчас мы к этому как раз
об этом поговорим, естественно не в километрах в час, а
в чем-то другом, что значит скорость программы.
Итак, если вы вспомните определение, ускорение,
то оно было вот на прошлом занятии, на прошлой неделе.
Ускорение напоминает отношение времени работы программы
на одном вычислительном ядре ко времени работы программы
на нескольких вычислительных ядрах, это основная характеристика
вашей программы, параллельной программы, и вот это наверное
связано с вашим вопросом, что такое скорость работы
программы.
Здесь не сама по себе скорость или на самом деле время работы
программы интересно, а интересное ускорение, которое
достигается при работе вашей параллельной программы
на какой-то многопроцессорной машине.
Нужно просто понять, с чем оно сравнивается.
Оно в этом время меньше по сравнению с чем?
По сравнению с временем работы на одном ядре, то
есть со временем работы последовательного алгоритма.
Мы сейчас тоже оставляем за скобками, лучший ли это
алгоритм или неху, или не лучший.
Вообще можно сравнивать с лучшим последовательным
вашу параллельную программу, потому что не самый лучший
последовательный хорошо параллелится, я об этом
тоже упоминал.
На самом деле ваша параллельная программа, которая может
достигать хорошего ускорения, она может быть основана
на каком-то другом последовательном алгоритме, не самом лучшем,
но при этом показывает хорошие характеристики по ускорению.
В данном случае у вас будет одна и та же программа,
если говорить о практических занятиях, поэтому вы будете
сравнивать именно саму программу с собой, просто
запущенная на разных вычислительных ядах, на разном количестве.
На этом давайте остановимся пока что.
Допустим, требуется посчитать сумму массива, что у вас
будет.
У вас массив из n элементов, если n достаточно большое,
то время работы на одном ядре можно оценить примерно
как перемножение количества элементов на количество
выполняемых арибметических операций.
Здесь мы, конечно, поточнее не можем все учесть, и здесь
n-1 вообще теряется на фоне возможных других накладных
расходов, в том числе и на фоне количества.
А у нас эти все операции независимые, поэтому мы
можем, вообще говоря, посчитать отдельные частичные
суммы на отдельных решателей.
Вот таких решателей, то есть вычислительных ядер,
может быть P, processes, поэтому в идеальном случае у нас
время просто делится на P, так, и получается, что у
нас n tau c поделить на P, это в идеальном случае.
Нам требуется рассмотреть еще влияние интерконнекта,
то есть как сеть влияет на наше время, на наше ускорение.
К этому времени расчета, то, что нам посчиталось,
нужно прибавить время передачи сообщений.
Зачем это нужно?
Потому что частичные результаты, они будут находиться в
памяти каждого из процессов, а нужно бы получить кому-то
одному.
Ну представим еще и сейчас, что эти процессы одновременно
ему передают, идеальный случай, они одновременно
передают и множество посылок, их будет тоже P-1, но пусть
будет их P, неважно, они одновременно происходят
и поэтому время, затрачиваемое на передачу будет tau s, s это
sending.
Да.
Немного конфликта в значении голове, tau c это время выполнения
одной операции сложения, а tau s это время передачи
информации.
Нет, просто одной передачи.
А почему у тебя такая формула?
Я только что об этом сказал, вы когда тянули руку, вы
наверно немножко отвлеклись.
Почему она одна, а не может объединиться?
Потому что я говорю, что они одновременно это делают.
Представим, что они это делают одновременно.
Если они это делают одновременно, значит это затрачивает.
То есть, их последовательность, это чрезвычайно.
Просто представим, что сейчас они происходят передачи
одновременно.
Не последовательно они передают, но если передача
происходит одновременно, значит это затрачивается
время одно.
Ну, смотрите, вот, например, хорошо, просто вы стреляете
из лука.
Вот если один раз стрельнули, то полет стрелы будет tau.
Если десять лучников стоит и стреляют на одно и то
же расстояние, сколько будет время полета всех
стрел, если они летят одновременно?
Сколько?
tau.
Примерно то же самое.
О том, что если это будет неодновременно, мы дальше
пойдем.
Сегодня это рассмотрим.
Как организовывать пересылки.
К этому вернемся.
Но сейчас, пока для оценки, пока tau s.
Хорошо?
Отлично.
Итак, у нас добавляется какая-то величина.
Тогда у нас ускорение, t1 поделить на вот это tp, получим
вот следующую формулу, делим просто на tau, на n tau c, тут
единичка, тут единичка, и вот здесь у нас получается
вот такая дробь в знаменателе.
Вот, смотрите, теперь если tau s равно нулю, у нас нет
интерконнекта, то наше ускорение равняется p, превращается
в идеальный случай.
Но у нас tau s не равняется нулю.
Возникает вот такая дробь.
n это наша задача, то есть сложность нашей задачи,
а вот tau s на tau c это как раз характеристика вычислительной
машины.
tau s это характеристика временная интерконнекта,
а tau c это характеристика временная вычислительного
ядра.
Насколько быстро оно считает.
Ну, вот давайте оценим влияние этого интерконнекта.
Значит, нам нужно оценить вот эту дробь, но это делается
не очень сложно.
Как можно оценить?
Вот, смотрите, это характеристики интерконнекта, ну, не современного
несколько лет назад, но примерно не сильно они стали
быстрее.
Значит, в основном существует таких две основные сети,
которая называется Ethernet и InfiniBand.
Сейчас используется InfiniBand, она побыстрее, но это тоже
для оценки никак не испортит картину.
Значит, что здесь, какие у нее характеристики основные?
Первая это характеристика – это латентность.
Здесь она есть, как пример для Ethernet это 50 микросекунд
примерно.
А для InfiniBand – 3-5 микросекунды.
Латентность – это характеристика сети, о чем она говорит?
Это время задержки между инициализацией пересылки
и фактической пересылкой, началом фактической пересылки.
То есть, мы говорим, нам нужно переслать что-то.
Начинает идти время, пересылка не идет, время тратится,
а потом спустя какое-то время латентности у нас начинается
физическая пересылка.
Как видно здесь, потом у нас еще пропускная способность.
То есть, сколько за единицу времени мы можем передать
мегабайт или килобайт.
Здесь видно, что с увеличением размера данных для Ethernet
время растет, а для InfiniBand это время растет, но не очень
сильно.
Поэтому передача нескольких килобайт на самом деле
сильно на времени играет, поэтому мы можем сделать
в качестве оценки латентность.
Нам это будет достаточно.
Еще раз формула.
И вот смотрите, если у нас вот этот параметр дроби,
если порядка единицы, то у нас очевидно ускорение
будет меньше единицы.
Мы его на самом деле особо не рассматриваем, потому
что здесь бессмысленно использовать эту машину, она слишком
медленная для нашей задачи, потому что TOS очень большой
получается.
Нам интересен случай, когда у нас все-таки немножко
сеть побыстрее.
Итак, про InfiniBand мы договорились, что будем брать латентность,
и латентность у нас 5 микросекунд, значит, вот такая оценка
для TOS получается, оценка для время передачи.
Это вполне нормально, потому что килобайт это тысяча
байт.
Нужно посчитать, сколько можно передать, какой размер
массива, за какое время.
Если мы передаем одну, две, десять переменных, даже
целого типа, или даже дабловского типа, то у нас вполне это
все укладывается в оценку.
Теперь нам нужно оценить TAU-C, это тоже несложно сделать.
Правильно?
Как можно сделать эту оценку?
У нас есть числота игра.
Да, тактовая числота процессора у нас есть, но этого тоже
вполне достаточно, можно ее принять как основу.
Считаем, что за один такт у нас произошла какая-то
одна арифметическая операция.
Какие частоты можно использовать?
Один гигагерц.
Ну, например, один, но поскольку у нас здесь пять, для удобства
просто возьмем два гигагерца.
Это не важно, это тоже особо.
Если мы возьмем два гигагерца, то время вычисления одной
арифметической операции будет обратной величины
от нашей частоты, 5 на 10-то секунды.
Ну, это очень маленькая величина, но теперь давайте
подставим все эти значения вот в эту дробь и получим,
что у нас, если одно на другое поделить, то это отношение
10 тысяч получается.
То есть у нас сеть медленнее, чем сам процессор 10 тысяч
раз.
Наших оценок.
Сейчас сети побыстрее, может быть, тысяча будет,
но так или иначе это составляет несколько порядков.
А теперь смотрите, если у нас вот эта дробь хотя
бы порядка единички, то оказывается, чтобы такое
было при таком соотношении времен, нам нужно, чтобы
СН было тоже примерно такого же порядка.
То есть СН будет порядка 10 тысяч.
То есть для того, чтобы у нас получить ускорение
хотя бы около единички, величина нашего массива
должна быть хотя бы тоже 10 тысяч элементов.
И мы не рассматриваем там никакие другие накладные
расходы, только сложение и вот пересылка и то, и пересылка
одновременная.
Значит, о чем это говорит?
Какие отсюда выводы можно сделать?
Первый очевидный вывод – это то, что наша сеть все-таки
влияет.
Потому что, если бы не было влияния, у нас не было
бы вообще никаких ограничений на сложность нашей задачи.
Это первое.
Второе, если у нас есть какая-то машина, то на ней нужно
решать, вообще-то говоря, довольно-таки сложные задачи.
Вот представим, что здесь будет N, здесь в шестой,
сюда подставляем, здесь вот эта величина будет
10 минус второй, но она хотелось бы, чтобы была как можно
меньше, поменьше, чем вот эта 1 на P.
Если P там 10, ну вроде нормально.
Если P 100, уже не очень хорошо.
То есть, когда у нас P 100, это будет соизмеримые величины.
Поэтому можно еще сделать меньше.
Как меньше сделать вот эту дробь, увеличить N.
То есть, все больше и больше сложной задачи.
Чем задача сложнее, тем лучше будут параметры ускорения.
Вот, по идее, вот это и в качестве лабораторной
работы вам нужно будет провести на семинарах.
У вас одна и та же будет программа, и нужно будет
просто менять входящие параметры вашей задачи,
менять ее сложность и смотреть, как будет меняться графику
ускорения.
Если же мы хотим хотя бы, ну хотя бы, а если равно
P пополам, то что должно?
Ну просто подставляем эту величину в нашу формулу.
Тогда оказывается, что вот эта дробь у нас будет порядка
1 на P.
Что оказывается?
Что N будет P умножить на 10 тысяч.
Как я уже сказал, если десятка P равно десятке, то значит
вот это 100 тысяч, а если P равно 100, значит N должно
быть равно миллиону, хотя бы, хотя бы миллиону.
Ну соответственно, чем больше ускорения, тем нужно
побольше взять N.
Какие здесь вопросы есть?
Если нет вопросов, тогда следующий.
Давайте.
Довольно часто, если я правильно понимаю, когда речь идет
о проникновениях, обычно, условно, компьютеры, которые,
или процессоры, которые используются для них, слабее, чем те,
которые используются для, если нужно что-то посчитать
на нотэшену.
На каком?
На последовательном.
На последовательном?
На последовательном.
На последовательном.
Ну, во-первых, смотрите, я думаю, что это не совсем
так бывает, потому что бывает...
Короче, смысл такой, когда вы приходите на какую-то
удаленную машину, скорее всего, она сделана не вчера,
а несколько лет назад, скорее всего, потому что как только
вложили какие-то средства, то вообще хотелось бы, чтобы
они работали как можно дольше и дольше и дольше,
и получать от них какую-то отдачу.
Соответственно, это машине уже несколько лет, скорее
всего.
Скорее всего, на тот момент, когда она была создана,
эти процессоры были нормальны.
Они были на современном уровне, они были современными.
Но когда прошло уже даже два, три, четыре года, и вы
купили, например, свой ноутбук вчера, то, конечно же,
современный ноутбук даже, он будет, процессор может
быть мощнее, чем то, что было пять лет назад.
Это вполне очевидно.
Второй момент, что при создании такой машины все опять ограничивается
какими-то финансами.
Например, тот делал машину, у него есть какой-то бюджет,
и он ориентируется на него.
Сколько он может купить процессоров, ну и всю остальную
обвязку, инфраструктуру на этот бюджет, и начинать
размышлять.
Купить мощнее процессоров, их должно быть поменьше,
либо взять побольше по количеству, но они будут не
такими мощными.
И вполне возможно, что он изберет именно второй вариант.
Взять побольше, но они могут быть не самыми мощными
на тот момент.
Но опять же, даже если он будет делать сегодня, то
вполне возможно, что ваш ноутбук будет один на одном.
Один процессор будет мощнее, чем процессор на той машине,
которая строится.
В целом, смотрите, конкретные числа, да, но в целом такое
наблюдается.
Это неважно, просто это будут другие цифры, другие
показатели.
Естественно, идет прогресс, и может быть через 5-10 лет,
условно говоря, эта дробь будет не 10, а еще 100.
Но так или иначе, эта сеть будет все равно медленнее,
чем сам процессор.
Просто цифры другие.
Да, это будет лучше, конечно, к этому все стремятся,
чтобы сеть делать побыстрее.
Но даже, скажем так, шина, которая считывает данные
с оперативной памяти, она все равно будет медленной
по сравнению с самим процессором.
Поэтому здесь паритета не будет.
Что будет лучше?
Несколько процессоров послабее или поменьше процессоров
и посильнее?
Зависит от задачи.
Здесь так просто, очевидно, не ответишь.
Ну, к примеру, просто пример такой.
Если предполагать, что будет много народу работать,
то хотелось бы, чтобы, скажем, 10 процессоров или 100,
и 10 человек.
Ну, наверное, 10 процессоров, но я не знаю.
Наверное, чтобы, если много народу,
то лучше бы побольше процессов,
чтобы каждый человек сидел и что-то делал.
Пусть это будет дольше, его задача решается,
но зато все они.
А если у вас один человек, ну, или там два человека,
ну, наверное, они могут как-то договориться,
использовать меньшее количество.
Здесь просто так не скажешь.
Еще какие задачи решаются?
Здесь какого-то алгоритма или рецепта на все случаи жизни,
наверное, не существует.
Это по обстоятельствам.
Ну, и вообще, от человека зависит, что ему больше нравится.
В целом, конечно же, там за одни те же деньги,
наверное, можно купить машину и в том, и в другом случае
одинаковой производительности.
Ну, наверное, или примерно одинаковой производительности.
Здесь, наверное, разницы особо не будет.
Ну, хотя где-то может комплектующими.
И можно пожертвовать какими-то.
В целом, наверное, разницы не будут.
Ну, вот такие рассуждения.
Ну, плюс, наверное, если это какое-то обвязка,
может быть, чем дешевле, тем может быть меньше надежность.
Здесь еще и надежность на это смотреть.
Потому что машина будет на работу круглосуточно
и месяцами, годами должна, по идее.
Поэтому здесь еще нужно...
Ну, много факторов, в общем.
Трудно, сразу не скажешь.
Еще есть вопросы?
Так, давайте тогда перейдем вот к третьей причине,
которая может влиять на ускорение.
Здесь немножко будет формул и некая физическая задача,
но на саму физику можно не смотреть,
можно смотреть просто на уравнения.
Этого будет достаточно.
То есть смотрите просто на формулу и все.
Но задача...
Почему?
Потому что здесь вот что важно.
Ну, давайте я расскажу, а потом на этом заострю внимание.
Значит, задача просто это нагретый стержень,
представим себе горячий,
и к двум концам одномерно задача, самая простая.
И с левой справа к нему подсоединяются холодные резервуары,
имеющие другую температуру, скажем так, ноль,
а стержень нагрет до температуры 1.
Ну, все мы знаем.
И вот, например, у нас есть такой стержень,
скажем так, ноль, а стержень нагрет до температуры 1.
Ну, все мы знаем из нашего жизненного опыта.
Что будет происходить со стержнем?
Охлаждаться.
Как будет охлаждаться?
Постепенно.
Постепенно, с двух сторон.
С левой справа температура здесь будет,
и здесь будет быстрее падать,
а вот падение температуры к центру,
оно произойдет несколько позднее.
Какую?
Ой, эта температура.
Почему она 1?
Ну, модальная задача.
Почему?
А какое другое?
Любое другое значение – это неважно.
Может...
Смотрите.
Синяя – это температура.
Ноль резервуар.
Один.
Ноль.
Синяя – это температура.
Обознатил ее буквой У.
Я так захотел, потому что Т у нас за время отвечает.
Трудно, чтобы не спутать.
L – это единичка, от нуля до единицы.
А вот температура.
Синяя – это график температуры.
Все.
И потом он будет как-то таким колокообразным.
Вот такая задача.
Каким уравнением описывается изменение температуры
со временем?
Вот таким уравнением, дифференциальным уравнением.
Я не знаю...
Нормально это или...
Нормально.
Хорошо.
Теперь, для того, чтобы ее решить на машине,
его нужно представить в виде...
То есть каждую прозводную нужно представить в виде
конечной разности.
Это у вас, не знаю, было или не было,
но вот эта прозводная представляется в виде
такой конечной разности.
Прозводной второго порядка представляется вот такой.
Ну, откуда вот это берется?
Что такое прозводная?
Прозводная – это предел вот такого отношения,
когда Тау стремится к нулю.
Так?
Мы убираем...
То есть это будет тожественно равно.
Мы теперь убираем этот лимит, то есть предел,
и получается примерно.
Наша прозводная равна вот такой разности.
Это называется конечная разность.
Почему?
Потому что она не бесконечно малая.
Она имеет какую-то конечную величину.
Ну, а вторая прозводная – это разница двух вот таких
прозводных, если сюда поставить там и плюс один...
А, ну да, верхний индекс отвечает за время,
если вы не в курсе.
Нижний индекс отвечает за дискретизацию по пространству.
То есть n – это текущая температура в настоящем,
n плюс один – это то, что нужно найти в будущем.
Если мы рассматриваем какую-то точку i,
какую-то одну, то i плюс один – это точка правее,
если у нас ось направлена вправо,
i минус один – левее.
И вот здесь h – это размер шага по пространству.
Tau – размер шага по времени.
Ну, это дельта t часто обозначается,
а это дельта х.
Это разнозначное.
Ну, вторая производная получается как?
Вот эти две производные,
то есть здесь будет i плюс один минус i,
а вторая производная i минус i минус один.
Ну, и получается вот такая формула.
Разница двух производных деленная на h тоже.
И получается, что здесь h в квадрате,
а здесь разница производных.
Ну, в общем, подставьте, получите.
Без предела получается примерное равенство.
Теперь мы эти конечные разности подставляем сюда,
потому что именно конечные разности
используются машиной.
Получаем вот такое уравнение.
И теперь это самое простое уравнение.
Это называется явная схема,
когда слева отравна у нас одна величина,
которую нужно найти,
а справа отравна у нас получаются те величины,
которые известны в данный момент времени.
Вот нам теперь нужно выразить вот это неизвестное.
Вот так мы выражаем,
формула для вычисления температуры в будущем,
если у нас известна температура в настоящем,
в разных точках.
А у нас известны температуры в самом начале,
они все единички,
поэтому в любой момент времени,
то есть последовательно мы можем решать эту задачу
и каждый раз на новом шаге по времени находить
все распределение температур.
И вот, значит, здесь, правда, есть определенные там условия,
если будете там решать.
Вот на этот множитель,
условия куранта Фридрикса Леви,
или коротко условия куранта,
если у нас вот этот тао,
или вот этот множитель будет меньше половинки,
0,5,
то у нас схема будет устойчивая.
Если больше, то она будет неустойчива,
но это пока особо вас не должно касаться.
Ну и здесь коэффициенты.
Как они называются?
Это коэффициент температуры проводности,
шаг по времени, шаг по пространству.
И шаблон разноцветной схемы выглядит вот так.
Это нам нужно знать,
нужно представлять в голове,
интересно было бы это представлять в голове.
Почему?
Потому что есть одна тонкость.
Почему рассматривается именно такая задача?
Так вот, еще раз.
Чтобы найти температуру вот в этой точке i,
нам нужно знать температуру в этой же точке n.
В настоящем.
И в соседних точках.
И вот здесь оказывается
как раз та самая тонкость.
Почему рассматривается,
почему вообще рассматриваем. Видите, что у нас шаблон именно вот такой, поэтому у нас есть
определенные зависимости по данным. У нас, если мы теперь собираемся решать эту задачу на множестве
вычислительных ядер, то у нас нельзя просто взять и поделить нашу область рассмотрения на части,
как это было, например, в сумме массива. Почему? Потому что у нас вот есть эти зависимости.
Вот допустим, есть у нас вот этот стержень и, допустим, есть один из точек, в которых мы
смотрим температуру, но мы смотрим на концах маленьких отрезков, поделили, дискретизировали
нашу область расчета, она простиралась от нуля до единицы. Если мы на 10 отрезочков поделим,
у нас будет 10 отрезков и h будет равно 0,1. Потом просто договариваемся или там условно,
где смотрится температура в центрах этих отрезков или на краях. Это не важно, на самом деле,
это просто вопрос договоренности. Но здесь я захотел, например, взять с краев. Получается у
нас 11 точек, 9 внутренних и 2 граничные. На граничных точках 0 будет, ну а в центре там как-то будет
температура распределена. И что дальше? У нас получится 11 температур. Это тоже хорошо,
это нечётное количество. Почему? Потому что нужно сейчас будет подумать. Если мы собираемся
решать на двух ядрах, то как нужно поделить вот эти точки по процессам, чтобы они примерно,
ну нагрузка у них была примерно одинаковая. Мы рассматриваем сейчас статическую балансировку
нагрузки, так называется. Как нужно поделить 11 на 2? Ну сколько будет температура к одному относится
процесс, а сколько к другому? По серединке это слишком обычно. Это будет 12. Тогда будет 9.
Но как-то это нужно решать задачу? Только здесь 11, 8-8 это 16.
Я, может быть, и догадываюсь, но все-таки без пересечений, хорошо. Специально для вас. Как
поделить пополам, но без пересечений, потому что пересечения здесь они будут бессмысленные.
Ну конечно. Такое в принципе возможно, но это будет в дальнейшем вы поймете, что это будет не
совсем оптимально. Я об этом еще не сказал, но сразу скажу, что это будет неоптимально. Так можно
было сделать для расчета, когда мы сумму массива рассматривали, типа расчесочкой такой. В принципе
можно. Надеюсь, это не очень хорошо будет. Это не дурацкий, это самый лучший ответ. Я задаю простые
вопросы. Вот мы сейчас к этому вернемся. Это не обязательно знать соседние. Сейчас мы поймете,
почему. 6,5 это правильный ответ, но он правильный в одном случае. Он правильный в одном случае,
если у нас эти вычислительные ядры примерно одинаковой мощности. Если же они у нас различны,
то вполне возможно, что можно сделать 8,3, поделить на 8,3. 8 это если быстрее процессор работает,
ну просто гипотетическая ситуация, а 3, который помедленнее. И вот можно уже заранее распределить.
Ну скорее всего это просто гипотетическая, просто это маленькая такая ремарка о том,
что мы делим примерно одинаково. Мы не можем 5,5 отдать одному 5,5 другому. Это целые числа,
поэтому чтобы разница была плюс-минус один. Это нормально. Да, вопрос. Ну если они равные,
то значит примерно и на равной части нужно делить. Но еще раз, чисто на равной мы поделить не можем
целые числа, поэтому как можно ближе к равности нужно бы поделить. 6,5 это будет хороший ответ.
Получаем, что 6 температура у нас относится к процессу номер 0 и 5 температур к процессу номер
1. Номера процессов, они номируются в MPI начиная с нуля. Если у нас два процесса, значит их номера
будут 0 и 1. Это называется ID процессов, но это грубо говоря имена, чтобы можно было случайно
обращаться к процессам. 0 и 1. Если их 7, значит от нуля до 6. Максимальное число будет, это размер,
обычно оно обозначается как size, size минус 1. Если у нас два, size равно 2, значит у нас номера
процессов будет 0 и 1. Все, поделили, договорились. Теперь процесс номер 0 вычисляет температуры
в 6 точках по той формуле, которая была на предыдущем слайде. Просто берет и вычисляет. А процесс номер
1 вычисляет температуру в 5 точках. Все бы хорошо, но как говорится но. Что но? Почему но?
Да, абсолютно верно. Вот смотрите, у нас есть это воображаемая граница. Это к одному процессу
относится, это к другому. Теперь накладываем сюда шаблон и видим, что для того, чтобы посчитать
температуру вот в этой граничной точке, нам нужно бы знать температуру вот в этой точке. Но процесс
0, об этом не зная, у него нет доступа к оперативной памяти процесс номер 1. Поэтому здесь нужно сделать
пересылку от первому процессу к нулевому. И когда нулевой получит эту температуру,
у него будет вся информация на текущий момент и он сможет посчитать в будущем. Ну и точно также
относится к процессу номер 1, вот здесь. Ему неизвестна температура в точке 6, ему нужно ее
получить и это можно сделать с помощью отправки сообщения. Сообщение это отправка данных. В данном
случае отправка правильной температуры. Так? Все. Но дело в том, что почему этот еще раз пример
рассматривается, потому что здесь есть производная, а это значит, что у нас есть некие зависимости по
данным. То есть, минус 1, плюс 1, имеется в виду i минус 1, i плюс 1. И когда у нас есть производные,
мы их приближаем разностями, то это будет наблюдаться. То есть, у нас не просто данная
независимая внутри, все нормально, а вот на границах есть такой нюанс. Так, перерыв,
да? Давайте 5 минут. Давайте продолжим. Коротко скажу о том, как это обычно делается. Во-первых,
когда у нас рассматривается какие-то граничные условия, то делаются фиктивные чайки дополнительные,
в которых не идет расчет. Здесь они показаны таким же штрихпунктиром. Ну и на воображаемой границе
между процессами тоже делаются такие фиктивные чайки. И вот в эти фиктивные чайки, они являются
частью массива, но расчетом не идет, пересылается значимая, то есть настоящая температура из других
процессов. Вот так. Такой перекрестная пересылка. Ну и тогда вот в этом квадратике, который с
сплошными линиями, можно посчитать температуру, потому что после пересылки здесь будет правильная
температура. И тогда единообразно для всех процессов, у каждого процесса нужно представлять,
что будет название массива одинаковое. В самом лучшем случае, как это может быть реализовано.
Массивы начинаются от нуля до количества элементов. Это количество элементов сидит в какой-то
переменной. Ну и в общем просто идет один блок программы, который относится ко всем процессам,
тут просто разные там разные данные лежат. Ну ладно, это детали. Главное, что нам нужно сейчас
рассмотреть. Вот это более частно, как влияет, помимо закона Омдала, помимо интерконнекта, как еще
влияет способ налаживания этих пересылок на ускорение. Это тоже важно. И рассмотрим один из методов
параллелизма, параллельных вычислений. Это называется геометрический параллелизм. Ну он то
же самое, он был применен и для расчета суммы массива. В чем его смысл? В том, что тоже очень
все просто. Наш стержень, в котором хранятся температуры, ну в общем стержень, мы проецируем
этот наш стержень в программу в качестве массива, в котором хранятся температуры. И эта массив делится
на части. Можно представить, что и наш стержень в геометрическом смысле, когда он находится в
реальности, пилится на кусочки. Каждый кусочек рассматривается каждым из процессов. Вот геометрический
параллелизм. Больше ничего сложного нет. Ну получается, что, например, если у нас есть массив из N точек,
N точек. Таоце это, грубо говоря, вычислить одну температуру. Уже не одна арифметическая операция,
их несколько, но это время, затрачиваемое на вычисление одной температуры. Мы делаем оценку,
как, что на что влияет. Вот и K. Это количество шагов по времени, которое нужно пройти для того,
чтобы понять распределение температуры на какой-то момент времени. То есть N это точек, то есть
количество шагов, грубо говоря, по пространству. K это количество временных шагов. Таким образом,
тао1, то есть время, затрачиваемое одним ядром, на полный расчет. K N таоце по времени, N по
пространству, таоце время вычисления одной температуры. Так? Теперь, когда мы делим на P,
в данном случае здесь 7, просто держим на голове 7, на P процессов, то у нас получается следующее.
Ну, это схема просто тех же самых пересылок. И вроде как мы... Каждая такая стрелочка переслать
данные, на самом деле, в терминах MPI состоит из двух функций. Такая стрелочка — это MPI send,
называется. Отправить сообщение. А вот такая галочка — это MPI receive. Принять. Ну вот,
смотрите, получается, что вот на таких границах между процессами у нас получается одна пересылка,
это значит две функции. Но в итоге такая одна пересылка, она занимает время taos. Для того,
чтобы правильно описать, вернее правильно посчитать все распределение температур,
каждый из процессов должен в первую очередь что? Отослать на левой границе свое значение и
принять слева. И на правой границе отправить, здесь отправить и принять. То есть два своих
граничных значения отправить влево-вправо. Ну, влево-вправо — это тоже таки жаргон в некотором
смысле. Когда мы рассматриваем вот так схему пересылок, то обычно справа у нас оказываются
процессы с номером. Этот номер еще называется рангом. С рангом на единицу больше, а слева у нас
оказывается с рангом на единицу меньше. Мы если так и договоримся, то оказывается,
что вправо это значит нужно переслать процессу с рангом на единицу больше, а влево это с рангом
на единицу меньше. Потому что на картинке это видно — лево-право. И принять опять же слева-справо
для того, чтобы нам сделать расчет в этих граничных точках. Но вроде как получается,
что у нас четыре пересылки и вроде как они должны быть как раз друг за другом. Вот здесь процесс,
почему он друг за другом? Потому что мы рассматриваем здесь пересылка с блокировкой,
блокирующей пересылка, например. Это говорит, что пока происходит передача данных, у нас этот
процесс и тот процесс, который принимает данные, они заблокированы на приеме или пересылке и ничего
другого не делают. Поэтому они могут только переслать один раз, после этого этот процесс может перейти
к приему или к передаче следующего сообщения и так далее. Здесь у нас идут как раз пересылки
последовательно. И время какое получится? Вот это время, деленное на П, плюс четыре пересылки — раз,
два, три, четыре. Их какое будет количество? Тауэ с время одной пересылки на одном временном шаге — это
будет четыре тауэс, а всего временных шагов — к. И получается четыре ка тауэс. Вроде как из схемы того,
что мы видим, это вроде должно так быть. Но как это будет в реальности на самом деле, когда вы будете
это реализовывать? Давайте посмотрим. Здесь пока понятно. Замечательно. Идем дальше. Значит,
можно ли такое получить в реальности-то или нет? Давайте посмотрим первый вариант. У нас есть
семь процессов. Номера у них или ранги от нуля до шести. Шесть — это максимальная схема пересылок.
И как можно это сделать схему? Давайте так. Первый вариант. Каждый из процессов делает следующее.
Сначала пересылает влево. Кроме нулевого, потому что у нулевого нет левого соседа. Нет ранга меньше
нуля. Все они пересылают влево. Потом все пересылают вправо. Нолевой пересылает вправо. Здесь
вот номера процессов, а здесь итерации, то есть шаги. Поскольку это пересылки, то это еще и шаги
по времени. Сколько итераций временных для налаживания пересылок будет наблюдаться. Все
вправо, кроме последнего. Последнего нет соседа справа, потому что у него номер максимальный.
Потом это идет обмен. В данном случае у этого на левой границе и вот здесь вот для каждого
счета на левой границе. Потом все они принимают справа, а потом все принимают слева. Получается
вот такая схема пересылок. Просто мы берем все, отправили, а потом все получили. Вообще говоря,
на каждой границе должно быть по две стрелочки и по две галочки. Вроде все это наблюдается.
Две стрелочки, две галочки, две стрелочки, две галочки. Это значит, что мы направили, переслали
влево-вправо и получили отлево-право на каждой границе. Теперь вопрос. Сработает ли эта схема?
Что вы говорили про процесс, когда что-то начинает делать, он блокируется.
Да, мы рассматриваем здесь... Просто есть не блокирующая операция, но это всегда можно сделать
не блокирующие операции. Но по сути, все можно сделать, все от операций, что можно сделать не
блокирующими, можно делать и блокирующими операциями. Но когда мы рассматриваем блокирующие операции,
мы можем почувствовать разницу при применении того или иного алгоритма. Когда не блокирующие
операции они нивелируются я сейчас об этом не буду говорить да поэтому мы
рассматриваем блокирующие операции как самый простой самый ну первый способ
который вообще рассматривается самой первой функции и на них можно почувствовать
разницу реализации алгоритма пересылок и так да и так операции у нас блокирующие
то есть при приеме или при передаче процесс блокируется на этом приеме или
передачи и другие никакие операции не выполняет
подробнее про эту модель, в которой мы работаем, что если два соседних блоков хотят, например, обмудременно, то будут что-то отправить
давайте просуждаем
моментально мы умрем, или вот они как-то договорятся и сделают...
так, хорошо
когда отправляют, ожидает приеме то, что будет принято
так, хорошо
ну второй раз мы сможем отправить, потому что первое правило, он будет ждать пока тот приемик
тот это какой?
ну сталон на...
давайте по номеркам
вы правильно рассуждаете
例えば первый процесс
вот первые ноль вой страны
ну, lamps
друг к другу
но первые ждают, что примет перв swapped
ну, любой ждает, что примет первый
второй ждет, что примет другой
и получается они доверятся блокированию
да, это верно
это получается взаимная блокировка
называется deadlock, или классический тупик, когда они собираются что-то отправить друг другу, но
никогда эти пересылки не закончатся, потому что каждый из них не может принять, они заплакированы
оба на пересылке, принять не могут, просто программа ваша зависнет, оба эти процесса будут ждать друг
другу, но остальные тоже все подзависнут, потому что никакой из этих процессов не сможет дождаться
окончания пересылки, потому что этого не произойдет никогда, это правильно, это тупик,
да, и время у нас будет бесконечным, время наших пересылок и время собственно работ нашей
программы, какое будет ускорение? Нулевое. Вариант номер один написано, наверное,
будет еще какие-то вариантики. Сейчас мы дождем. У нас есть функции MPI, мы ими пользуемся, есть
блокирующие, в принципе это пройдет, если вы воспользуетеся не блокирующими функциями,
это пройдет, но, опять же, разницу мы не почувствуем, мы не почувствуем разницу в
реализации. Давайте это все-таки попробуем сюда прийти, почувствовать, а потом уже будем там
будете работать, когда надо, с чем надо. С этим тоже можно все делать. Теперь, как избежать этот тупик?
Нужно просто поменять местами прием и пересылка, например, в нулевом. Если мы поменяем местами,
то нулевой примет, ну вообще везде, нулевой примет, значит эта пересылка закончится, и вот если
мы везде вот эти стрелочки поменяем местами, то после переправки влево первая начнет не отправка
вправо, а прием, ну какой-нибудь вот эти вот прием, справа, ну и все у нас разрулится, как говорится. Мы
избавимся от этой проблемы. Просто тоже приемов может быть разные, например, использования не
блокирующих пересылок, либо использования там совместной функции приема-передачи такие тоже
есть, но один из самых простых способов это перестать местами center-receive соответствующие.
Ну давайте это сделаем. Это будет вариант два. Все отправляют влево, как у нас в первом случае,
а потом нужно сделать прием для каждого из процессов. Видите, у нас получаются пары приема
и отправки. И вот всегда нужно, опять же, мыслить именно в этой парадигме, скажем. Если где-то
есть отправка, значит обязательно где-то нужно сделать прием. Если какой-то у вас процесс принимает,
то значит где-то должен второй процесс, с каким он коммуницирует, отправить. В общем, нужно всегда
мыслить парами. Не просто все отправляют, а нужно искать пары, нужно делать пары. Итак, мы сделали
пары приема и отправки. Прием у каждого из них и отправка у всех остальных. Чтобы у нас не было
тупика. Пока мы так, а я хочу так. Кто мне запрещает? А потому что у нас блокирующая операция. И если так,
то процесс номер один может принять процесс номер два только после того, как он сделал вот эту
пересылку. А это у нас, еще раз напоминаю, итерации во времени. Это значит, что у нас после первой
итерации вот этой идет следующая итерация вот эта. Но у нас это будет TAO-S. Время равняется.
Время равняется. Каждую итерацию равно TAO-S. После первой идет вторая пересылка, вот на этой
границе. Только после нее, возможно, вот эта и так далее. И получится, что у нас идет передача
сообщений последовательно, друг за дружкой, и у нас растет время пересылок. Вот в таком виде.
Но это, смотрите, произошел обмен на каждой границе, но только в одну сторону. Влево. Теперь нам
нужно все это отправить вправо и получить слева. То есть нам нужно еще то же самое сделать,
но в другую сторону. Видите? Отправить вправо и получить слева. И опять на каждой границе у нас
две стрелочки в одну сторону. То есть две стрелочки и две галочки. В одну сторону отправка и в другую
сторону с приемом. Получается вот такая структура отправки сообщения. Все сообщения отправляются
последовательно, потому что мы используем отправка и прием с бакировкой. Не могут они
произойдет одновременно. И получается, какое у нас время работы алгоритма будет? Здесь у нас
тупиков нет, видно? Еще раз погромче. 12. А что такое 12? Это я знаю. А вы почему здесь 12 получается?
Откуда она? Но не минус 1. Минус 2. Почему? Смотрите, у нас границ. Если у нас 7 процессов,
границ между ними на сколько будет? 6. 6 это n-1. Но нам нужно отправить в одну сторону, это будет
один раз n-1 и в другую сторону. Это еще раз n-1. Поэтому будет 2 умножить на p-1. p это количество
процессов. В одну и в другую сторону. А все остальное тоже самое. Получается, что у нас будет
2 умножить на 6. Вот откуда 12. Так, это лучше, чем было раньше? Чем бесконечность? Ну, вроде лучше,
да? Мы по крайней мере уже видим, что у нас время, ну, наверное, меньше, чем tau-1. Вот здесь мы tau-1,
вот здесь оно делим на p, это точно меньше. tau-s большое. Ну, в общем, кажется, что меньше. Но
плохо, что у нас это время пересылок зависит от p. Мы хотим tp увеличить, тогда у нас вот эта
дропа уменьшится, у нас будет хорошее должно быть ускорение. Но потому что оно вот здесь линейно
входит во время пересылок, это нам все портит. Да, ускорение будет, но оно такое будет, корявенькое.
И, помимо этого, у нас не получится четверка. Помните, тогда, когда мы теоретически рассматривали,
у нас вот здесь вот четверку хотелось бы видеть. А четверка, это что значит? Что у нас время пересылок
не будет зависеть от количества процессов, это будет константа. Вот такое поведение, это называется,
что у нас время пересылок линейно зависит от p, это значит O от p. О большое, помните, да? То есть
пропорциональность. А когда четверка, это O от единички, это константа какая-то. Можно ли
сделать вот эту четверку-то? Здесь у нас лучше, чем бесконечность, ускорение будет, но вот эта вот
зависимость линейная, она портит всю картину. Как сделать четверку? И откуда берется четверка?
Это не я говорил. Ну вообще, да. Так, и что это получается? Что-то нечетное, то что? Это вам уже на
семинаре говорили? Здесь видно, хорошо. Да, смотрите, вот когда заняты пересылкой 0 и единичка,
вот ничто не мешает на самом деле делать пересылку между двойкой и тройкой. Заблокированы только 0 и
1, а все остальные, в общем-то, не связаны с этой пересылкой, и блокировка именно с этой пересылкой
им не обязательно. То есть, если рассмотреть, сейчас-то 2 и 3 тоже заблокированы, потому что
2 связан с 1, здесь он хочет передать 1 что-то, а 1 не может это принять. Но если чуть-чуть поменять
местами вот эти отправку и прием, то у нас ничто не мешает, когда пересылает 0 и 1 сделать вот этот
обмен. Или вот этот, правильно? Они ведь могут быть не заблокированными. Если мы это сделаем, то
оказывается, действительно. А это что значит? Что нулевой принимает, второй тоже должен принять,
четвертый тоже должен принять и так далее. То есть, каждый четный будет принимать, а каждый нечетный
отправлять. Ну давайте так и сделаем. Третий вариант. На каждом временном шаге нечетные отправляют
влево. В этот же момент четные должны принять. Принимаем. Принимаем справа. Произошел обмен,
одновременный обмен. На одной границе, то есть не на всех границах, а на каждой, ну не знаю,
как это, четной, нечетной, не важно. На каждой границе эти границы два раза меньше. Потом на этой
же границе мы делаем обмен в другую сторону. Нечетные должны принимать, а четные должны отправлять.
Таким образом, мы за две временные итерации, временные в плане пересылки по сети, мы сделаем
обмен на одной границе. Осталась вторая граница, вот эта. Сделаем обмен здесь, то же самое. Все,
все обменяли. Как у нас получилось? Четыре пересылки. За счет чего? За счет того,
что у нас обмен происходит одновременно. То есть здесь у нас распараллелия не только при расчете,
но и при пересылках. Видите, тоже можно, я просто не знаю, как зовут человек, который смотрит в
телефон. Можно ведь сделать одновременную пересылку? В принципе, не полностью, но в принципе можно.
Да, замечательно. Только я вас не просил разрешения, я спрашивал. И что тогда получается,
теперь если по временам посмотреть? Четыре. И вот теперь, если мы перейдем к ускорению,
то вот можно получить вот такую замечательную штучку. А что в этой замечательной штучке,
в знаменателе? Вот такая штучка, которую мы уже рассматривали. Что здесь? Уже нам известны
отношения tau s к tau c, tau s к tau c, p, n. Только еще добавилось p. Но оно тоже было на самом деле,
когда у нас там s. Если s сказать, что это p пополам, то у нас там все равно это появляется. Короче,
я кчмаю к тому, что у нас все равно это отношение p к n, tau s к tau c очень часто будет появляться.
И в данном случае оно тоже существует. И вот здесь сложность задач. Чем сложнее задача, тем у нас
ускорение лучше. Чем сеть медленнее, тем оно хуже. Чем больше p, то что тоже хорошо, тем лучше. Ну вот,
в общем, можно анализировать. Вот эта штучка у нас появляется. Так, давайте вопросов у нас сколько?
Еще 20 минут. Еще один метод рассмотрим. Пока. Так, к чему это? Это вот к чему.
Главным параметром из главнейших параметров является, я просто напоминаю, график ускорения.
График ускорения, чтобы понимать, как ваша программа работает на параллельной машине. Почему?
Потому что, еще раз, она должна решать две задачи. Первая, получить правильный ответ. Второй,
получить хорошее ускорение. Чтобы понять, что ускорение хорошее, нужно программу
протестировать. Или, если у вас нет возможности протестировать, можно оценить. Это раз. Второе.
Что влияет на график нашего ускорения? Ну, понимать, во-первых, докуда он масштабируется наш алгоритм.
Сколько можно вычислительных ядер брать, чтобы получить то или иное ускорение. Чтобы потом у вас
была наглядна табличка по доступности, например, ядер. Сколько можно использовать и какое вы
ускорение получите. На что влияет ускорение? Влияет на ускорение. Закон Амдала. Раз. Второе. Интерконнект.
Третье. То, как вы реализуете пересылку сообщений. Мы сейчас поговорили о законе Амдала. Мы
говорили в прошлый раз. Сейчас мы о втором способе. То есть, о второй причине. Интерконнект. Третья
причина тоже. От ваших рук зависит. От бесконечности время пересылок может быть. То есть, ускорение
от нуля. Потом оно может плавно загибаться за счет того, что у вас время пересылок зависит
от количества процессов. И еще более лучший график ускорения, когда у вас время пересылок не зависит
от количества процессов. А это что говорит о том, как вы это сделаете. То есть, лично от вас. Мы
вы можете повлиять на графику ускорения. Поскольку вы хорошо или сделать не очень хорошо пересылки.
Это тоже будет влияние. Вот мы это рассмотрели. Как это влияет. Вплоть до того, что у вас программа
просто зависит. Ну это понятно, неправильно написана программа. Но вариант номер два и
вариант номер три, они вполне жизнеспособны, но до определенной степени. То есть, вы можете и так,
и так. И разница уже будет. И мы говорили, и вот в этой задаче уравнение теплопроводности как раз
можно почувствовать эту разницу. Это первое. А второе, это был один из методов параллельных вычислений.
Метод геометрического параллелизма. Когда мы просто ваш стержень делим на кусочки, разрезаем. И
каждый кусочек отдельно считается каждым из процессов. Еще один метод. У нас как раз есть время
для того, чтобы посмотреть. Это метод сдваивания. Вот давайте опять же рассмотрим массив, который
состоит из восьми элементов. И опять же сделаем сумму элементов. Как теперь можно сделать? Допустим,
у нас есть время. Здесь восемь элементов, поэтому здесь N-1 вполне нормально. Есть у нас в два раза
меньше процессов. Их четыре штуки. Каждый из процессов может взять себе пару и вычислить
сумму этой пары. Получит четыре ответа. На следующем шаге работают два процесса и получают суммы одной
половины массива и суммы второй половины массива. И на последнем шаге работает один процесс, который
делает конечную сумму. Если у нас N является какой-то степенью двойки, то алгоритм сдваивания
выполнится вот за такое количество шагов. Мы берем алгоритм по основанию два от N. То есть
если у нас было восемь, то выполнится за раз, два, три шага. Потому что три – это алгоритм восьми по
основанию два. Правильно? Два раза меньше сначала. Четыре. Где? Я думаю, что восемь там должно быть,
по крайней мере. Так, десять восьмой. Вы про что сейчас говорите? Почему десять восьмой? Два в
третьей в данном случае. Это элементов массива. И это вообще даже не процессов, не количество
процессов. Десять восьмой может быть у вас в задаче. Можно представить себе. Сейчас спокойно,
мы рассмотрим. Давайте все. Забудьте, что было раньше. У нас новый метод. Метод сдваивания. Там
восемь элементов и четыре процесса. Давайте на этом остановимся, а потом что-то будем размышлять.
Хорошо. Тогда идем дальше. Вот такое кью получается. Это у нас количество шагов для того,
чтобы у нас метод сдваивания сработал. Степень парализма – это количество независимо выполняемых
операций на какой-то стадии. Ну, боксинг. Ладно, идем дальше. Время работы, если рассмотрим
последовательно алгоритм, это множество tau c умножить на n-1. Ну, если n большое,
то можно примерно n. Так в голове держим. Если у нас p – это, как я уже сказал, n поделить пополам,
то время работы будет tau c сумма умножить на количество таких шагов. Мы здесь сейчас не
рассматриваем пересылки. Просто мы говорим о времени. Просто понять, как он работает без пересылок.
Просто у нас процесс каким-то образом получает результат с предыдущей стадии и просто складывает.
Без пересылок. Пересылки мы чуть позже. Я думаю, что успеем рассмотреть. Вот такое у нас получается
время работы параллельного алгоритма. Если одно на другое поделить, вот такая величина получается.
Эффективность. Это s, напоминаю, поделить на p. Если у нас pn пополам, вот тут вырисовывается 2 на n,
здесь примерно n-1 равно n, сокращается, получаем вот такую величину. Нормально это или нет?
Более-менее. Если у нас n примерно 1000, ну степень двойки 1024, то у нас s получится 102,
а эффективность примерно 20%. Но если s еще ладно, у нас тоже так себе. p у нас 500, 500 процессов,
ускорение 100, ну пусть будет, но эффективность совсем не очень хорошая, маленькая. А все почему?
Эффективность – это ускорение, поделенное на количество процессов. Показывает, насколько загружены
процессы в целом. Видно, что 20%. То есть мы берем 500, а по факту, после того, как мы все посчитали,
оказывается загруженными полностью 100. Но это, в общем-то, очевидно. Потому что на каждой стадии
количество работающих процессов уменьшается. Сначала они все работают, все 500, потом их работает
250, потом их 125 и так далее и так далее. На каждой стадии их работают два раза меньше. Ну и они
просто простаивают. Можно как-то переиспользоваться? Можно как. А это скорее вопрос «как». Ну сейчас посмотрим. То есть как-то
нужно действительно получше-то их использовать, правильно? Потому что, ну так-сяк. Ну давайте
смотреть. Если у нас n большое, то что можно сделать? Применить уже сюда чуть-чуть метод геометрического
параллелизма. Немножечко. Вот если n большое, то вычисление последовательной суммы будет n на tau c.
Потом мы этот массив делим сначала на части. Количество частей равно количеству процессов.
Сначала вычисляем в каждой части свою сумму. Это будет, значит, вот n tau c поделить на p. Это
время вычисления суммы в этих частях. А потом воспользуемся методом сдваивания. Метод сдвания
сколько будет? tau c, пока без пересылок. Логарифм по основанию 2 от p. Почему от p?
Количество изначально было n. Там как-то дерево издваивание, у нас листья в нём было. Поэтому
когда п процессов посчитали частичные суммы в своих частях, то у нас получилось p ответов.
А потом мы знаем, что количество следующих шагов в следующей стадии метода сдвания,
логарифм по основанию 2, от p, от изначального количества значений или элементов. И теперь
можем посчитать ускорение. Вот все поделили. Но когда они посчитали свои вот здесь результаты,
то они получили p результатов. И получилось p чисел. И теперь эти p чисел должны по методу
сдвавания попарно складываться. Понятно? Хорошо. Теперь мы просто поставляем одно в другое.
Получаем s равно вот этому числу. Эффективность равно вот этому числу. Теперь представим,
что p меньше, чем n. Ну и желательно как можно меньше. n большое в общем. p у нас сколько есть,
а вот n большое. Сложная задача. И пусть p будет вот таким числом. n поделить логарифм по основанию
2, от n. И подставим вот сюда. Тогда у нас s получится примерно p пополам. Эффективность 50%.
То есть вместо p вот в знаменатель подставляем вот это число. Понятно почему это получается или
нужно поподробнее здесь? Было недавно. То есть тогда слишком много на самом деле было. Ну почему?
Можно n большое взять. Ну по примеру, если n равно 1024, то логарифм это сколько? Десятки будет?
Вот. Десятка. Здесь будет p равно уже 100. Ускорение будет 50 при 100 процессах. А тогда у нас было
ускорение 100 при 500 процессах. Ну в общем, лучше показатели. Ну пожалуйста, если у вас есть 500,
ну посчитайте сколько там n должно быть. Все наоборот, сделайте. То есть когда мы применяем теперь
совместно два метода, то это вообще говоря очень сильно нам помогает и улучшает результаты как
ускорению, так и по эффективности. И это хорошо. Дальше. Теперь мы добавляем еще. Что можно добавить?
Можем добавить еще пересылку. Время затрачивано пересылкой. Теперь тогда у нас вот здесь вот,
где у нас количество шагов в методе сдваивания. Вот сюда еще добавится помимо сложения еще как
раз и пересылки. Опять добавляем. В знаменатель видим, что все эти пересылки опять у нас все
портит на самом деле. Вот. И это очевидно в общем-то говоря, что когда мы добавляем сеть,
это у нас все ухудшает, потому что добавляется знаменатель. Так, теперь, где этот метод сдваивания
можно использовать? Ну во-первых, просто при обыкновенном сложении, потом при всяком агрегировании,
умножение, сложение, поиск минимум, максимум. Все это можно. Еще это можно использовать при рассылке,
если один, то есть сделать его обратно, пустить в обратном направлении. Можно сбор данных в принципе
сделать. Есть специальные операции в MPI, которые делают сбор данных. Это gather и они собирают не
по такому же принципу, а просто один все собирает от всех. Есть еще broad cost, то есть такая рассылка,
broad это широковещательная типа. Один процесс отправляем в одну и ту же операцию всем остальным. То
есть есть сбор, есть рассылка. Вот можно это сделать, улучшить эти алгоритмы, встроенные в MPI с помощью
метода сдваивания. В одном случае у вас, как я показал, сверху вниз идет и в итоге собирается
вся информация в одном процессе. А если один процесс хочет одну и ту же информацию отставать
всем остальным, то можно сделать в обратном направлении. Можно разные способы, не реализации,
а применение этого метода сдваивания придумать и иметь в голове. Все у нас осталось где-то немного,
минуты четыре, поэтому давайте на вопросы, если они есть, я отвечу. Там еще есть в материалах,
посмотрите, топология соединения процессов в процессоров в машине. Для общего развития я
на это время не стал тратить. Просто по-разному можно процессоры соединять в параллельной машине.
То есть в историческом порядке примерно так все и шло, развитие усложнялось,
и там есть интересные моменты. Потом сами почитайте. Давайте вопросы, если есть, задавайте.
Что непонятного или что нужно прояснить?
Ну ладно, хорошо тогда, если все понятно. Конечно, за два занятия трудно что-то уместить более-менее
полезно, но хотя бы то, что есть. Хорошо, тогда я сделаю какую-то квинтэссенцию того,
что я рассказал. Что было? Мы посмотрели о том, почему параллельное программирование нужно изучать
в том или ином виде. Потом рассмотрели примерную классификацию параллельных машин и как это
связано с этой классификацией библиотеки параллельного программирования. У нас там
были машины с массовым параллелизмом, которые делились еще на два больших подкласса. Это машины
с распределенной памятью и машины с общей памятью. И вот для машин с распределенной памятью
предназначен MPA, если говорить более-менее сухо. Для машин с общей памятью предназначена библиотека
OpenMP. Это самая популярная на данный момент библиотеки, и вообще говоря, знать их было бы неплохо.
Ну и теперь, если говорить об MPA, в принципе все то же самое относится и к OpenMP, но там вместо
времени на пересылку по Interconnect, там уже будет время связано с синхронизацией потоков. То есть
там тоже будут накладные расходы, они будут немножко другие, поменьше, но они все равно
присутствуют. То есть в принципе здесь, если есть ТОС на время пересылок, то это просто накладные
расходы, которые не связаны чисто с вычислениями, но они всегда будут где-то присутствовать,
так или иначе. И мы рассмотрели по крайней мере три причины снижения ускорения. Первая это просто
связано с тем, что не всякий алгоритм может быть распараллелен, и если он не может быть
распараллелен, то параллелить его не надо. Мы увидели, что даже небольшая часть, которая
нельзя распараллелить, очень сильно влияет на ускорение. Вторая причина связана с самой машиной,
не с алгоритмом уже, а с машиной связана, с ее техническим несовершенством, и это тоже влияет
на ускорение. И третья причина, это связано уже, возможно, уже с несовершенством человека, как он
подойдет к вопросу реализации пересылок. Ну, я так утрирую, конечно. То есть зависит уже другая
причина, связанная с самим человеком. Как он это реализует, как он реализует алгоритм передачи
сообщений, к примеру. Есть алгоритмическая, машинная и персональная ответственность за то,
что ускорение может быть нехорошим. Все. Ну и немножко поговорили о возможных методах параллельных
вычислений. Как можно ускорить тот или иной алгоритм, по каким схемам. Ну все. Примерно вот это и было.

Смотрите, мы, к сожалению, опять не успели в прошлый раз завершить некое доказательство,
но в данном случае, мне кажется, нам не стоит начинать его повторять сначала,
как это было в начале прошлой лекции. Оно длинное было, и мы свели всю историю к выкладке.
То есть нам надо оценить некую ужасную сумму. Вы это помните, нет?
То есть вы готовы к тому, что у нас будет чистый анализ сейчас,
а уже комбинаторика вся была в прошлый раз в этом месте.
Так, но мне надо, наверное, напомнить, или, может быть, я сам вспомню.
Давайте, значит, во-первых, я помню, что у нас был вот такой параметр.
Целая часть от n поделить на 1 минус епсилон лог 2-ичный n.
Дальше у нас были чиселки a1 и так далее, a с индексом m, каждая из которых была не больше, чем...
А, да, да, да, да, конечно, здесь двойка, спасибо, да.
Да, каждая из которых не больше, чем вот эта как раз величина, да.
А тут без двойки, да, сейчас, а почему тут без a?
Да, да, да, конечно, тут без двойки, да. Да, да, да, конечно, тут двойка не нужна, правильно.
Меньше просто, чем 1 минус епсилон лог 2-ичный n.
И дальше у нас были множество c1 и так далее, cm такие, что мощность цитова равняется аитому.
И была жутковатая сумма, в которой внешнее m-кратное суммирование вот по всем таким чиселкам в этих пределах,
внутреннее m-кратное суммирование по всем таким множествам мощности m, мощности аитое,
но так что они еще попарно не пересекаются.
Было такое, да? Важно, что суммировалось.
Значит, суммировалось, наверное, вот такая бяка, произведение по i от единицы до m,
1 минус 1 на 2 в степени аитое, все это, вот это все в степени он пополам, да?
Вот так.
Да ладно, на самом деле все просто.
Ну, во-первых, аитое, каждое, не превосходит вот этой величины, да?
Значит, 2 в степени аитое, стоящее в знаменателе, не превосходит n в степени 1 минус эпсилон.
Соответственно, если вы переворачиваете, то есть берете единицу, поделите на 2 в степени аитое,
то будет больше либо равно соответствующей дроби, а еще раз со знаком минус снова будет меньше либо равно.
Но наша цель доказать, что вся сумма стремится к нулю.
Значит, у нас получается, что это меньше либо равно, опять произведение по i от единицы до m,
1 минус 1 поделенное на n в степени 1 минус эпсилон, и все это в степени n попало.
Так? Вроде так.
Эти произведения суммировались по всем a1am и c1cm, да. Я же это проговорил. Нет разве?
Мы как бы зафиксировали во внешнем суммировании вот эти числа, во внутреннем вот эти множество.
Как бы мы их не зафиксировали, то произведение, которое суммируется, оценивается так, как я написал.
Ну потому что каждая ита не больше этой величины, а дальше сначала стало больше либо равно, потом снова меньше либо равно.
То есть знак неравенства в нужную сторону.
Дальше пользуемся стандартным соображением, что если вы возьмете какой-нибудь 1 минус п,
то это е в степени логарифм от 1 минус п, и соответственно это не больше, чем е в степени минус п.
Ну такое стандартное неравенство, которое мы уже не раз использовали.
Было такое ведь, да?
Ну и здесь то же самое.
П у нас в данном случае это вот эта величина, 1 поделить на n в степени 1 минус эпсилон.
У нас получается меньше либо равно.
Значит, тут получается е в степени минус 1 поделенное на n в степени 1 минус эпсилон,
умноженное на m и возведенное в степени n пополам.
Но можно было сразу умножить на n пополам.
Так, я понятно объяснил, откуда это взялось.
Мы каждую скобку оценили как е в степени 1 поделенное на n в степени 1 минус эпсилон,
скобок m штук, и еще это все в степени n пополам.
То есть вот так получается е в степени mn пополам 1 поделить на n в степени 1 минус эпсилон,
или можно вот так написать е в степени mn в степени эпсилон пополам.
А, минус потерялся, да, спасибо.
Здесь минус, здесь тоже минус, конечно, это важно.
Нам надо стремление к нулю получить, а не к бесконечности.
Это понятно, да.
Так, ну вон видим, какой у нас там m.
Ну чего не превосходит?
Я не знаю, ну давайте я скажу, что оно не превосходит.
n поделить на лог двоичный n.
Коль скоро n больше либо равняется какого-то, n первого.
Что? Еще раз?
А, больше либо равно, да, нам же нужно больше либо равно.
Ну хорошо, меньше либо равно может тоже пригодиться, не знаю, а может и нет.
Хорошо, давайте, да, больше либо равно, это проще.
Оно, очевидно, больше либо равно n поделить на 2 лог двоичный n.
Ну просто потому что 1 минус эпсилон меньше единицы.
Ну тоже может быть, начиная с какого-то очень небольшого n,
потому что я целую часть снял, но я думаю, что это уже слишком такое ковыряние в очевидных вещах.
Так, меньше либо равно е в степени минус n стало.
n стало снова в степени 1 плюс эпсилон, но надо поделить на 2 лог двоичный n.
Ну, если хотите, я еще вот так напишу.
Ясно, что поскольку эпсилон мы зафиксировали заранее,
то n в степени эпсилон с какого-то момента, конечно, превосходит логарифом.
И превосходит существенно.
Ну, то есть, вот, начиная с какого-нибудь там очередного n1 или n2, не важно,
мы можем написать е в степени минус n в степени 1 плюс, скажем, эпсилон попало.
Убрать этот 2-логариф на двоичный n паразитически он не нужен.
Что?
Четыре потерял, да?
А почему?
А, я вот эту двойку потерял.
Чего ж я так сегодня, а?
Ну, строго говоря, четыре.
Давайте все-таки писать, как правильно.
Понятное дело, что я и четверкой здесь пренебрегу с большим удовольствием.
Вот.
Но, тем не менее.
Конечно, так правильнее.
Вот.
Ну, а теперь вспоминаем, что у нас есть внутреннее суммирование вот это по c1 и так далее cm.
И суммируются теперь у нас величины е в степени минус n в степени 1 плюс эпсилон пополам.
Ну, видно, что эти величины не зависят от c1, cm вообще никак.
Правда же?
Ну, то есть, вся эта сумма, она равна количеству способов зафиксировать вот эти множества c1, cm.
И это количество умножается просто на такую вот константу.
Ну, в кавычках, конечно, константу по c1, cm.
Вот.
Ну, формально тут получается c из n по a1, c из n минус a1 по a2, c из n минус a1, минус и так далее, минус am, ой, am минус 1, по am.
Не путать с полинамиальным коэффициентом, потому что сумма вот здесь вот все-таки не равна нулю.
Ну, потому что 1 ам, они там выбираются до серединки примерно от n, до половинки от n.
Вот.
Но мы-то по-дурацки это считаем на е в степени минус n в степени 1 плюс эпсилон пополам.
Мы по-дурацки сейчас эти цешки оценим с огромным запасом, но, к сожалению, эпсилон это не поможет убрать.
Ну, то есть, вот смотрите, в нашей формулировке эпсилон больше нуля это важно.
Заменить эпсилон на ноль не получится, сейчас вы увидите.
Даже если здесь аккуратнее оценить, не получится ничего.
Но мы оценим идиотским образом.
Мы это оценим вот так.
n в степени a1 плюс и так далее, плюс am, на е все в той же степени 1 плюс эпсилон попало.
n в степени 1 плюс эпсилон попало.
Поняли откуда, да?
Ну, просто c из чего-то почему-то уж точно меньше, чем это в степени это.
А n-а1 меньше, чем n, n-это меньше, чем n.
Вот я так тупо оценил.
Но реально это ни на что не влияет.
Понимаете, потому что в знаменателе написать вот эти факториалы,
так это факториалы чисел, которые порядка лог 2хn, от этого ничего не изменится.
Но это так, чтобы вы понимали.
Формально я могу про это не говорить, но оценил, победители сейчас судить не будут.
Победители не судят, а я сейчас победю.
Так, а1 плюс и так далее плюс am мы знаем, это меньше либо равно n пополам.
То есть все это меньше либо равно n в степени n пополам,
на е в степени минус n в степени 1 плюс эпсилон пополам.
Это равняется е в степени n пополам логарифм n
минус n в степени 1 плюс эпсилон пополам.
Но это еще не все, потому что мы пока не разобрались с внешним суммированием по а1 на m.
Мы зафиксировали а1 на m и разобрались с внутренним суммированием.
Вот оно так оценивается.
Так, друзья, это понятно или нет?
Непонятна оценка? Все понятно?
Нормально, да?
Ну смотрите просто, если что-то непонятно, лучше спросить.
А во внешнем суммировании сколько слагаемых?
Ну уж точно меньше.
То есть я вот так напишу сумма по а1 и так далее.
Ам теперь получается вот этих выражений, которые опять от а1 ам никак не зависят.
Н пополам логарифм n минус n в степени 1 прибавить эпсилон пополам.
Это меньше тупо, чем лог двоичный n в степени m умножить вот на эту штуку.
n пополам минус n в степени 1 плюс эпсилон пополам.
Так, откуда я взял это сомножитель, понятно?
У нас в этом суммировании m в независимых слагаемых,
каждый из которых точно меньше, чем лог двоичный n.
Ну то есть выбрать первое слагаемое можно вот таким количеством способов.
Второе таким же.
Ну у нас получается вот столько способов зафиксировать числа а1 и так далее ам.
Ну...
Что?
Какие логарифмы?
А вот здесь потерял. Да что ж такое-то сегодня?
Опечатка на опечатке, простите, пожалуйста.
Конечно, здесь логарифм.
Но от него тоже, понятно, ни тепло, ни холодно.
Тут катерсис-то в чем аналитический?
В том, что эпсилон все-таки фиксированная константа,
и тут к единице какое-то число прибавляется.
n в степени какое-то положительное число.
А здесь n в первой степени умножается на логарифм.
И когда из такой функции вычитается такая, то это со свистом идет в ноль.
А вот если эпсилон заменить на ноль, то ничего не получится.
Логарифм важен.
Понятно говорю, да?
Вот. Но это вообще фигня.
Все-таки не зря я m оценил сверху тоже.
Пригодилось.
Это меньше, чем e в степени n делить налог двоичный n.
Это оценка n.
Это оценка для m.
И умножить на повторный...
Ой, какой повторный логарифм.
Логарифм натуральный внешний, а внутри логарифм двоичный.
Гениально.
Но это все равно повторный логарифм по порядку.
Даже асимпатически это неважно.
Ой.
Ну и вот плюс вот это n пополам на логарифм n,
минус n в степени 1, плюс эпсилон попал в показатель экспонента.
Но это слагаемое еще меньше, чем вот это.
Оно даже меньше, чем n, потому что делится налог двоичный,
а умножается налог повторный.
Вот это меньше, чем n.
Это больше, чем n, но в логарифм раз.
А вычитаемое больше, чем n, в n в какой-то положительной степени раз.
Поэтому все вот это вычитаемое, вся вот эта разность, сумма разности,
она стремится к минус бесконечности, а экспонент от нее стремится к нулю.
Теорема доказана наконец.
Вы поймите, тут ничего сложного нет, очень грубые оценки.
Главное придумать вот эту вот идею, что жадный алгоритм наловил много маленьких множеств,
потому что ничего к ним добавить дальше не сумел.
И вот за счет того, что он много чего не сумел добавить, получается вот такое вот понижение.
m умножить на n гораздо больше, чем n в степени 1, минус эпсилон.
Вот в этом смысл.
А дальше это просто такая тупая техника стандартная, тут ничего особо умного нет.
Вот.
Ну, что я вам хочу еще про жадный алгоритм сказать, давайте я все-таки прокомментирую.
На самом деле, это прямо проблема открытая.
Существуют ли хоть какие-нибудь полиномиальные алгоритмы,
которые
опроксимировали
и,
ну,
и
и
и
и
опроксимировали
бы
ну, скажем, альфа от g,
или, что то же самое, хи от g.
Химы не обсуждали, там технически чуть посложнее, ну, пусть будет альфа, так понятнее.
Значит, существуют ли полиномиальные алгоритмы, которые опроксимировали бы вот эти величины
с еще большей точностью, в следующем смысле,
с точностью
до саммножителя
строго меньшего двойки?
Вот это прямо проблема открытая.
Ну, то есть, что мы с вами сейчас доказали?
Мы доказали, что если мы возьмем жадный алгоритм,
то реальное число независимости
по отношению,
да, ну, конечно, с точностью до саммножителя меньшего двойки
и вероятностью стремящейся к единице на случайном графе,
и вероятностью стремящейся к единице.
То есть, мы по-прежнему говорим про случайные графы,
стремящиеся к единице.
Ну, то есть, чтобы они работали не на всех графах,
а на почти всех графах, вот такие существуют или нет,
мы сейчас доказали, что вот такое отношение меньше
либо равняется 2 плюс Эпсилон,
ну, просто 2 плюс Эпсилон,
а симпатически почти наверно
на случайном графе g от n одна вторая,
то есть, когда все графы равновозможны, равновероятны.
Вот мы это доказали, правильно?
Люди умеют убрать вот это Эпсилон,
но не для жадного алгоритма,
а для некоторого более сложного алгоритма,
который тоже работает за полином от n.
Работает за полином от n на всех графах,
и а симпатически почти наверно дает вот здесь неравенство без Эпсилон.
Это люди умеют делать.
А вот сделать здесь 2 минус Эпсилон
с каким-нибудь строго положительным Эпсилон
фиксированным, константный, скажем, 1 и 999.
Вот это проблема.
Проблема, существует ли такой алгоритм,
что α g поделить на α a g
меньше либо равняется, скажем, 1, 9, 9, 9,
а симпатически почти наверно.
Вот в этом проблема, существует ли такой полиномиальный
на всех графах алгоритм,
который бы давал с вероятностью стремящихся к единице
вот такую вот оценку.
Похоже, что его не существует.
То есть это еще один довод в пользу, как ни странно, жадности.
Получается, что жадный алгоритм при всей своей простоте
фактически это лучшее, что на случайном графе можно реализовать.
Ну да, там Эпсилон можно убрать, конечно,
это чуть лучше, но это не бог весть что.
Убрать вот эту двойку, ее, по-видимому, в принципе не получится.
Настолько сложная задача,
что даже на почти всех графах
нету, по-видимому, такого алгоритма,
который работал бы за полином
и при этом выдавал бы результат,
отличающийся от правильного,
не больше, чем вот в такую константу раз,
или там 10 девяток, напишите.
Все равно, по-видимому, такого нет.
Я понятно объяснил?
Это довольно любопытная вещь, конечно.
С другой стороны, конечно,
ну все вы интуитивно понимаете, что где-то вас обманывают.
Ну потому что есть же, наверное, графы,
на которых это все не работает.
Подумаешь, на почти всех работает.
Что значит на почти всех?
Всего графов сколько?
Два в степени c и n по 2, правильно,
вот столько всего графов.
А что значит на почти всех?
Это значит, вычтем отсюда какое-то количество,
которое бесконечно мало,
по сравнению с этой величиной.
Ну, например, два в степени n лог н.
Это тоже величина, которая бесконечномала,
по сравнению с два в степени н квадрат.
на самом деле может быть не обслужено нашим алгоритмом, тем не менее у нас получится почти все.
То есть некоторая обман, конечно, в этом присутствует.
И, к сожалению, вот на этом остаточном множестве, да, оно бесконечно мало по сравнению с главным слагаемым,
но вот на этом остаточном множестве есть совершенно убийственные примеры ситуации.
Сейчас я сформулирую теорему, которую не буду доказывать, но которую считаю важным всегда говорить,
потому что она свидетельствует уже против жадного алгоритма.
То есть тут надо смотреть, что выбирать в каждой конкретной ситуации.
Все-таки бывают такие графы, на которых все вообще ужасно.
Сейчас я объясню, что именно.
Давайте прежде всего заметим, что во всем вот этом вот рассуждении, во всей этой истории, которую я сейчас рассказал,
номерация вершины изначально была фиксирована.
Мы даже никак не пользовались тем, что мы имеем право выбирать номерацию.
Понятно, да? Ну и вроде все уже хорошо.
Вот сейчас я про эти номерации немножко скажу. Страшная вещь.
Так. Как бы это сказать?
Доказал я Кучера.
Сейчас попробую сказать.
Существует... Нет, не существует.
Для любого эпсилон и для любого дельта большего нуля
существует последовательность графов
g с индексом n таких, что мощность v от g с индексом n равняется n.
Последность графов на n вершинах, проще говоря.
Существует такая последовательность графов на n вершинах,
что...
Давайте сигмой обозначим номерацию этих вершин.
Ну как обычно, перестановку.
Сигма маленькая, да?
Ну сейчас оно появится на доске, просто я заранее предупреждаю, что сигма будет означать перестановку.
Так, как бы мне это лучше написать? Вот здесь напишу.
Так, альфа от gn поделить на альфа жадная с индексом сигма.
Сейчас я все поясню от gn.
И, вероятно, здесь по сигму берется.
Больше либо равняется n в степени 1 минус эпсилон,
больше либо равняется 1 минус дельта.
Во!
Так, ничего не понятно, сейчас я все буду пояснять.
Ну я старался пояснить, значит, что имеется в виду?
Мы здесь не граф берем случайный, а перестановку множества его вершин берем случайный.
Ну то есть вероятность каждой перестановки это просто единица поделить на n факториал.
Это понятно, да?
Граф фиксированный, существует такая последовательность графов,
что вот если мы посчитаем реальное число независимости у этого графа
и разделим его на то число независимости,
которое выдает жадный алгоритм на вот этой вот перестановке сигма,
и получится почти n.
Вот вероятность того, что на случайной перестановке получится такая ужасная хрень,
но согласите, ужасная хрень, реальное число больше,
чем то, которое отыщет нам наш замечательный жадный алгоритм
почти в n раз, где n количество вершин.
Так вот вероятность этого сама тоже почти единица,
что мы вольны зафиксировать сколь угодно маленькое ε, сколь угодно маленькое дельта.
Ну может быть проще говорить не в терминах вероятности, чтоб было понятнее,
просто вам придется перебрать вот столько нумераций,
ну что понятное дело невозможно сделать,
перебрать вот такое количество нумераций 1 минус дельта на n в степени,
и тем не менее вот на этих конкретных графах отношение альфа джен
к результату применения жадного алгоритма на вот этих перебранных нумерациях
будет настолько ужасным, что дальше ехать некуда.
Почти n равняться.
Реальное от найденного.
Я понятно объяснил?
То есть вот в этом вычитаемом маленьком множестве,
маленьком по сравнению с количеством всех графов,
которая гарантирует доказанная нами теорема,
вот в нем есть такие вычурные бяки.
То есть на вас может случайно свалиться граф,
который имеет асимпатическую вероятность, стремящуюся к нулю,
но граф вот из этой последовательности.
И тогда случится полное несчастье, потому что вы не в два раза приблизите,
как нам гарантирует вот эта теорема,
а хуже, чем в n в степени 1-эпсилон раз.
С ошибетесь, почти в n раз.
Я не знаю, понятен пафос? Нет? Антипафос такой.
Ну по сигму это отношение числа, вот я же написал,
это отношение числа всех нумераций, в которых будет получаться такой результат,
к n факториал, к числу всех нумераций.
Ну то есть мы сигму берем с вероятностью 1 поделить на n факториал.
У нас случайно не граф, у нас случайно перестановка.
P сигма означает просто, что мы вероятно считаем на пространстве,
в котором случайно перестановки на множестве вершин вот этого графа.
Ух ты господи!
Ой, ну я же это написал.
Ну давайте я так скажу, мощность множества тех сигма,
для которых α от gn поделить на α g сигма от gn,
больше либо равняется n в степени 1 минус эпсилон.
Вот мощность множества таких нумераций,
на которых случилась хрень при применении жадного алгоритма,
она больше либо равна, чем вот эта величина,
чем 1 минус дельта на n факториал.
Вот это в точности то, что здесь написано.
Классическое определение вероятности.
Это просто отношение числа тех элементарных исходов,
которые благоприятствуют тому, что написано в скобках.
А элементарные исходы – это сигмы.
К числу всех элементарных исходов, к n факториал.
Ну вот я этот n факториал, который в знаменателе здесь стоит,
перенес вот сюда направо.
Сейчас понятно?
То есть вам скорее всего придется жуткое количество нумераций перебрать,
чтобы уйти вот от этого проклятия.
Ну а вы столько перебрать не сможете.
Жадный-то алгоритм линейный по числу ребер,
а нумерация – n факториал.
И все, кронтец.
Таких графов мало.
И это мы доказали.
Они сидят вот в этом каком-то там вычитаемом.
Но они есть.
И если на вас случайно такой граф капнет,
если вы этого хоть немножко боитесь,
то вам, наверное, жадный алгоритм применять не стоит.
То есть, с одной стороны, на почти всех графах жадный алгоритм
дает аппроксимацию вдвое,
а с другой стороны, вот на тех, почти никаких,
которые остаются,
есть такие ужасные ситуации,
что он ошибается не вдвое, а в n в степени 1 минус и апсалон раз.
И больше того, делает это не на какой-то фиксированной нумерации,
как в доказательстве вот этой теоремы,
а на почти всех нумерациях.
Ну, наверное, сейчас объясню.
Вот.
Ну, давайте еще немножко позанимаемся хроматическим числом.
Я могу, конечно, разбавить эту историю с хроматическим числом.
Может быть, даже это стоит сделать, чтобы вам не так скучно было.
Есть еще одна очень важная тема,
которая касается случайных графов
и которую обязательно нужно знать.
Я что-то подумал, давайте сейчас пока ее обсудим,
а то вы подумаете, что хроматическое число,
это типа на нем мир клином сошел.
Мы потом к нему вернемся.
Оно действительно важное.
На нем во многом мир клином сошелся.
Но давайте рассмотрим какую-нибудь такую немножко другую тему,
чтобы было вкрапление содержательное тоже.
Я подумал, что так будет лучше.
Я подумал, что так будет лучше.
Так, связность случайного графа.
Но это такая самая прикладная, наверное, тема.
Очевидно, прикладная,
потому что связность – это вопрос о том,
сохраните ли вы какую-нибудь там конкретную инфраструктуру.
То есть представьте себе, что у вас вершина графа –
это какие-то компьютеры, которые изначально соединены линиями связи.
Потом линии связи могут разрываться,
и может граф перестать быть связным.
Тогда вы не сможете с какого-то одного компьютера
на какой-то другой передавать информацию
не только напрямую, но даже по цепочке.
Так, друзья, я быстро сказал, или понятно,
почему важна связность?
Ну елки-палки, случайного графа.
Вот мотивацию я очень быстро объяснил,
и все-таки можно было понять.
То есть представьте себе еще раз,
вот у вас такие вершины какие-то,
они изначально попарно соединены ребрами.
Это могут быть, например, линии связи
между какими-то компьютерами.
И вот они есть между любыми двумя компьютерами изначально.
А потом вы с какой-то вероятностью P сохраняете жизнь каждому ребру,
а с какой-то, соответственно, вероятностью Q равной 1-P
это ребро удаляете.
То есть возникают помехи, например,
или может быть как-то прямо кто-то взял
и целенаправленно вам кокнул это ребро.
Спрашивается, насколько большой или маленькой
должна быть вероятность сохранения жизни ребру,
чтобы хотя бы по какой-то цепочке можно было
передать информацию с одного компьютера на другой.
Это и есть вопрос о связанности случайного графа.
Ну и вот первая основополагающая теорема,
которая, собственно, была доказана здесь
Эрда Шамеренни в свое время.
Давайте я напишу.
Так, она звучит следующим образом.
Пусть P это не константа, а функция от N,
которая ведет себя вот так.
Это C логариф натуральный N поделить на N,
где C, естественно, больше нуля,
просто какая-то константа.
То есть чем больше вершин, тем меньше вероятность того,
что мы сохраняем жизнь ребру.
В любом случае, если у нас есть вероятность,
тем меньше вероятность того, что мы сохраняем жизнь ребру.
В любом случае, какой бы ни была эта константа,
вероятность сохранения жизни ребра, она стремится к нулю.
Тем не менее утверждается, что даже в этих рамках
есть две прямо противоположные ситуации.
Значит, первая ситуация состоит в том,
что если C больше единицы,
то асимптатически почти, наверное, случайный граф
G от NP связан.
И вторая принципиально противоположная ситуация,
если C строго меньше единицы,
то асимптатически почти, наверное,
наоборот случайный граф разваливается.
Получайный граф не является связанным.
Не связан.
Ну, вы, конечно, спросите, что будет, если C равно единицы.
Это я вам отвечу и даже более подробно потом, сейчас чуть позже.
Вот такая штука в физике называется фазовым переходом.
Ну и в математике это слово тоже переняли.
То есть резкий скачок асимптатически почти, наверное, одному свойству
асимптатически почти, наверное, его противоположности.
Видите, да, с практической точки зрения, что это означает?
Вот если вернуться к той истории с уничтожающимися ребрами,
связями между компами там или между чем-то еще,
означает очень простую вещь.
То есть у нас очень много изначально вот этих компьютеров.
Ну скажем N, я всегда говорю, пусть равняется 2000.
Я не знаю, много очень компьютеров.
И вы возьмете какой-нибудь C, ну скажем, равное тройке.
Оно больше единицы.
Можно доказать, что в этом случае не просто вероятность связности стремится к единице,
а можно оценить, с какой скоростью она стремится к единице
и доказать, что это стремление вот такое, 1 минус 1n.
Это оценка снизу вероятности того, что связность сохранится в случайном графе.
Вот при таком C равном тройке.
Я понятно говорю, да?
Тогда что получается?
У вас получается, что вероятность сохранения ребра каждого отдельно взятого вот такая.
Но это, если вы посчитаете, это примерно две сотых.
Две сотых примерно, это вероятность сохранения жизни ребра.
То есть 0,98 это вероятность того, что ребро пропадет.
Тем не менее, с вероятностью 0,9995, 1 минус 1 2000 граф сохранит связность.
Каждое ребро уничтожается с вероятностью 98%.
А вот с такой вот вероятностью, без пятидесяти тысячных, тем не менее,
по какой-то цепочке вы сможете передать информацию.
То есть очень низкая вероятность того, что вы потеряете инфраструктуру,
так сказать, возможность передачи информации.
А, про C равная единица, да.
Про C равной единицы сейчас скажу.
Слушайте, ну, наверное, сейчас перерыв будет, нет?
Ну, я не знаю, давайте сделаем, наверное.
Сейчас как раз 40 минут прошло от начала пары.
Конечно, прерывался там на какой-то разговор, но неважно.
Давайте сейчас уместно сделать перерыв.
Потом скажу, что будет при C равном единице.
И начну доказывать эту теорию.
Так, про C равной единице.
Ну, если просто C равной единице, тогда можно доказать, но я не буду этого делать,
что вероятность, с которой g от np связан, стремится к1 поделить на e.
Но можно еще точнее сказать, это я тоже не буду доказывать.
Докажу только, собственно, теорему.
Значит, можно сказать еще точнее.
Если p от n имеет вот такой вот вид,
logarithm n плюс гамма поделить на n,
это ситуация, которая, в принципе, не укладывается в теорему,
потому что здесь C константа.
А тут получается асимптотика такая, как если бы C равнялась единице.
Но все-таки тут еще есть добавочная вот эта слагаемая гамма поделить на n.
Да, гамма теперь тоже произвольная константа, причем может быть отрицательная.
Вот утверждается, что тогда вот эта вероятность того, что g от np связан,
стремится к e в степени минус e в степени минус гамма.
Вот так.
Ну, понятно, что если γ очень большое положительное, 100 миллионов, например,
то e в степени минус гамма это почти 0,
ну то есть вот это e в степени, вот это, это почти единица.
А если гамма очень большая, но отрицательная, большая по модулю, но отрицательная,
минус 100 миллионов, тогда здесь будет e в какой-то очень большой положительной степени,
То есть со знаком минус будет очень-очень большое число, а все вместе будет почти ноль.
То есть на самом деле фазовый переход, который я докажу, он даже еще более такой быстрый, что ли.
Не нужно переключаться с c больше единицы на c меньше единицы.
Достаточно прибавить что-то очень большое, чтобы получилось почти что единица,
или вычесть что-то очень большое, чтобы была почти ноль вероятности того, что сохранится связность.
Сейчас я понятно это объяснил?
Чисто просто прокомментировал утверждение, доказывать я это не буду.
А теорему, ну, давайте с какого пункта начнем?
Неравенство Чебышова я вроде вам рассказал, так что я могу пользоваться немножко попроще второй пункт.
Вот как вы думаете, за счет чего скорее всего граф потеряет связность?
Что проще всего откалывать от графа?
Какие должны появиться компоненты связности в первую очередь?
Изолированные вершины, конечно, да.
Разорвать граф пополам на двудольные, разорвать просто пополам.
Здесь n пополам в вершин, здесь n пополам в вершин.
Это значит удалить n в квадрате поделить на четыре ребер.
А отцепить одну вершину, это значит удалить n минус одно ребро.
Гораздо меньше.
То есть интуитивно понятно, что скорее всего должны появляться изолированные вершины.
Вот ровно это и произойдет.
Итак, вот если мы доказываем пункт два, то давайте мы введем случайную величину х,
которая на графе равняется количеству изолированных вершин.
То есть вершин, которые имеют степень ноль.
Такие компоненты связности из одной вершины.
Так, ну, хорошо количество изолированных вершин.
Ну давайте посчитаем математическое ожидание для начала.
Наверное.
А, нет, давайте не так.
Потом посчитаем математическое ожидание, дисперсию, все посчитаем.
Я объясню, зачем их считать.
Главное, зачем их считать.
То есть надо применить неравенство Чебышова.
Мы хотим доказать, что вот такая вот вероятность стремится к единице.
Если мы это докажем, то пункт два будет доказан, правда?
Что есть хотя бы одна изолированная вершина.
Вот мы хотим доказать, что эта вероятность стремится к единице.
Ну давайте это перепишем вот так.
Это один минус вероятность того, что х не больше нуля.
Просто отрицание события.
Оно вот такое.
И вы мне скажете, х не может быть отрицательным.
А я все-таки неравенство сохраню.
Какая разница? Это же правда.
Я могу сказать, равно нулю в точности.
Но я хочу сохранить неравенство.
Все равно это верно.
А дальше я сделаю два смешных преобразования таких.
Я, во-первых, вот так напишу.
Опять переверну знаки,
и минус единицу левую и правую часть неравенства под знаком вероятности.
Вот.
А затем добавлю слева и справа некоторую константу.
А именно математическое ожидание х.
Это константа.
Я напишу единица.
Минус вероятность того, что ех минус х больше либо равняется ех.
А вот это уже очень похоже на неравенство Чебышова.
Как выглядит неравенство Чебышова?
Вероятность того, что модуль х минус ех больше либо равняется а,
не превосходит дисперсии х, поделенной на а в квадрате.
Это вот неравенство Чебышова, правильно?
Я описал в прошлый раз, я его доказал фактически в нашем случае.
Может быть, это уже были на основном курсе, но неважно.
Вот оно такое.
А здесь нет модуля, да?
Но в остальном все очень похоже.
Но согласитесь, что вероятность, с которой выполнено такое неравенство,
она не превосходит вероятности, с которой выполнено неравенство с навешанным слева модулем.
Потому что модуль, чаще говоря, больше либо равен чего-то, чем сама случайная величина.
Модульши это удаляются отрицательные случаи.
Больше либо равно, но со знаком минус...
Ой, меньше либо равно, но со знаком минус больше либо равно.
То есть получается вот так.
Больше либо равно один минус вероятность того, что модуль х-ех больше либо равняется ех.
Согласны, да?
Вот, все, сюда применяем неравенство Чебышова.
Эта вероятность не больше, но со знаком минус опять знак неравенства в нужную нам сторону.
Больше либо равно один минус дисперсии х поделить на квадрат математического ожидания.
Ну и вообще я замечу, что это неравенство ведь никак не использовало специфику этой случайной величины.
Мы не использовали никак то, что мы считаем именно количество изолированных вершин.
Мы использовали только то, что эта величина принимает неотрицательные целые значения, правильно?
Я хочу, чтобы вы для себя просто усвоили, что в принципе такое неравенство можно использовать,
коль скорая х это любая совершенно неотрицательно значенная целочисленная случайная величина.
Вероятность того, что она отлична от нуля, больше либо равна единице, всегда не меньше, чем один минус вот такая дробь.
То есть это некое общее неравенство, очень полезно.
То есть теперь нам осталось доказать, что для вот этой конкретной величины, вот в этих конкретных условиях c меньше единицы,
вот эта разность стремится к одному, то есть вот эта дробь стремится к нулю.
Осталось это доказать, правильно?
Сейчас я по-моему кокнул.
Ну ладно, тогда давайте считать мат ожиданий дисперсии и убеждаться в том, что дробь в наших текущих условиях стремится к нулю.
Так, ну математическое ожидание это очень просто.
Это линейность, конечно.
Давайте просто сложим индикаторные случайные величины x1 и так далее xn,
где x и t это единица, если вершина с номером i изолирована и 0 иначе.
То есть мы каждую вершину просто тестируем, на то является она изолированной в графе или нет.
Нам на вход поступает граф, мы смотрим, вершина с номером i изолированна или нет.
Если изолированная, добавляем в счетчик еще одну единичку.
Пользуемся линейностью.
Величины, конечно, зависимые, но линейность верна всегда, поэтому можем написать вот так.
Ну а чему равно математическое ожидание x этого?
1-p, наверное, в степени n-1.
1-p в степени n-1, то есть вот так получается.
Ну что, конкретная вершина не отправляет ни одного ребра в n-1 оставшуюся.
Это 1-p в n-1, а n за счет того, что у нас сумма n одинаковых слагаемых.
Повторяю, величины зависимые, поэтому дисперсию так легко не посчитать.
Вы знаете, наверное, да, что дисперсия суммы независимых величин от суммы дисперсий.
Но в данном случае так не получится, потому что это зависимые величины, но не сильно зависимые, но все-таки зависимые.
Поэтому мы честно напишем, что это мат ожидания x в квадрате минус квадрат мат ожидания, одна из формул для дисперсии.
И посчитаем, вычитаем, а мы знаем, посчитаем вот это вот уменьшаемое.
Второй момент. Посчитаем e x в квадрат.
Значит, e x в квадрат, это мат ожидания квадрата вот этой суммы, вот этой суммы, с теми же самыми индикаторами.
Честно возводим в квадрат, получаем x1 в квадрате, так далее xn в квадрате, плюс сумма по i неравном g,
имеется в виду по упорядоченным парам, поэтому я не пишу двойку перед суммой в качестве сомножителя, а дальше x и t, x и t.
Ну то есть просто слагаемых здесь n умножить на n минус 1. Сразу.
Затем замечаю, что вот эти квадраты можно вычеркнуть, потому что в квадрат мы возводим величину, которая принимает значение 1 и 0, а значит, я в квадрат возводить не надо.
Снова пользуемся линейностью, которая верна всегда.
Вот в этой части получаем e x, а в этой части получаем сумму по i неравном g в мат ожиданий произведения x и t на x и t.
Что такое произведение x и t на x и t? Это что же индикатор, правда?
x и t умножить на x и t. Индикатор чего? Что обе вершины изолированы. Да, вот картина такая.
И g, тут еще сарделька из n минус 2 вершин.
И вот надо посчитать фактически вероятность того, что и отсюда нет ни одного ребра, и отсюда нет ни одного ребра, и вот этого ребра нет.
Правильно? А сколько всего здесь нарисован ребер, которых не должно быть?
Нарисовано дважды n минус 2 и плюс 1. 2n минус 3.
То есть у нас получается e x, которое мы знаем, плюс n на n минус 1 и 1 минус p в степени 2n минус 3.
Что? n минус 2 плюс n минус 2 плюс 1.
n минус 2 плюс n минус 2 плюс 1. Это 2n минус 3.
Ну, это мало влияет на итоговый результат на самом деле, но, конечно, все-таки хочется написать правильно.
В какую сторону корреляция есть между этими величинами?
Так, ну все, нам осталось посчитать dx поделить на квадрат математического ожидания.
Так, это e x плюс n на n минус 1 на 1 минус p в 2n минус 3 минус e x в квадрате поделить на e x в квадрате.
Вот так. Верно?
Ключевой момент на самом деле, откуда вылезает вот эта функция и зачем нужно, чтобы c было меньше единицы, он состоит в том, что вот эта дробь стремится к нулю.
Сейчас я объясню. Она стремится к нулю, потому что мы сейчас убедимся в том, что мат ожидания наша стремится к бесконечности.
Ну, если мат ожидания стремится к бесконечности, тогда вот эта дробь будет стремиться к нулю.
Но вот это надо доказать. Смотрите, мат ожидания это n на 1 минус p в n минус 1, я просто переписал формулу.
Это n на e в степени n минус 1 логарифм от 1 минус p, стандартная замена.
Так, это равняется n на e в степени 1 плюс о малое от единицы на n на p со знаком минус.
Ну, то есть логарифм от 1 минус p, поскольку p у нас стремится к нулю, я могу асимпатически заменить на минус p.
Логарифм от 1 минус p асимпатически равен минус p, потому что p стремится к нулю.
Ну, n минус 1 асимпатически равняется n, конечно. А симптотику я написал вот в таком виде.
Вот, теперь смотрите, какой тетрис. E в степени минус 1 плюс о малое от единицы.
np это c логарифм n. Получаем n умножить на 1, поделенное на n в степени c, помножить на 1 плюс о малое от единицы.
Я надеюсь, вы это видите, потому что e в степени логарифм n это просто n.
И вот это просто n, оно прыгает в знаменатель за счет минуса, и в числе в этом показателе остается c умножить на 1 плюс о малое от единицы.
Нормально? Успеваете? Ну и все.
С у нас больше, меньше одного, правильно? С меньше одного. Мы во втором случае находимся.
Поэтому, начиная с некоторого момента, вот это произведение тоже меньше одного, строго.
А значит, n поделить на это больше нуля. Ну, в смысле, единица, которая стоит здесь в показателе числа n, если вы из нее вычитаете вот это произведение,
то это все равно положительное число. n возводит в какую-то степень, которая положительна.
И это действительно стремится к плюс бесконечности, но прямо на грани.
Сейчас, друзья, я понятно объяснил или нет? Вроде очень просто.
Это стремится к плюс бесконечности, значит, действительно вот эта дробь стремится к нулю.
Так, ну здесь у нас что происходит? Ну здесь-то все просто.
Смотрите, n на n минус 1 на 1 минус p в степени 2n минус 3.
Так, а квадрат математического ожидания это n квадрат 1 минус p в степени 2n минус 2.
Так, товарищи, вы услышиваете происходящее? Ну анализ такой идет несложный.
Так, куда эта штука стремится? К нулю. А паук единица.
Но смотрите, n на n минус 1, это же асимптотический n квадрат.
1 минус p в 2n минус 3 на 1 минус p в 2n минус 2, ну давайте я напишу.
Вот эта n квадрат сократилась, превратилась в асимптотику, а здесь осталось 1 поделить на 1 минус p.
Но p-то стремится к нулю, то есть это асимптотическая единица.
p стремится к нулю в любом случае, поэтому это асимптотическая единица.
А ех квадрат поделить на ех квадрат это просто единица, правильно?
То есть мы берем асимптотический ноль, прибавляем асимптотическую единицу и вычитаем просто единицу.
Что мы получаем в пределе? Ноль, конечно, все. Я доказал.
Пункт два я доказал. В пределе получаем ноль.
Вот это? Я складываю члены.
Вот один член очевидно стремится к нулю, потому что мы доказали, что есть стремление к бесконечности ума от ожидания.
Потом я смотрю среднее слагаемое, вот оно здесь. Оно асимптотически равно единице.
И есть еще одно вычитаемое, которое просто равно единице.
Вот вы из асимптотической единицы вычитаете единицу, вы получаете в пределе ноль.
И тут в пределе ноль. Сумма двух нулей ноль. Все.
Смотрите, что здесь важно. Математическое ожидание, вот вдумайтесь, числа изолированных вершин, оно стремится к бесконечности.
Это совсем просто, но этого не хватает для доказательств.
Нам еще нужно доказать, что случайная величина достаточно тесно сконцентрирована, как говорят, то есть прижата к своему математическому ожиданию.
Представьте себе, что вот есть какое-то среднее, да, оно стремится к бесконечности, но случайная величина ужасно болтается, все больше и больше.
Тогда нельзя будет утверждать, что с вероятностью стремящейся к единице она больше либо равна одного, несмотря на то, что среднее растет.
Нам нужно было дополнительно доказать, что она около этого растущего среднего плотненько сконцентрирована, и для этого мы использовали неравенство Чебышова.
Вот вы вникнете в эту идею, потому что у нас подобные идеи еще будут на курсе возникать.
Про еще более плотную концентрацию меры, это очень важно. Неравенство Чебышова это такое неравенство концентрации меры около среднего значения.
Без него не получилось бы доказать этот пункт. Хотя то, что мат ожидания стремится к бесконечности, очевидно практически.
И вы понимаете, что первый пункт формально отличается тем, что в нем мат ожидания стремится к нулю.
Понимаете, да? В чем отличие? То есть фазовый переход ровно на том основан, что в одном случае мат ожидания растет неограниченно, а в другом стремится к нулю.
Но опять же, из того, что мат ожидания числа изолированных вершин стремится к нулю, еще не следует, что в графе не появится какая-нибудь компонента связности.
Но изолированных вершин нет. Может какие-то другие есть? И это усложняет доказательство пункта 1.
Но поскольку время у нас еще какое-то есть, наверное надо начать.
Так, ну, пункт 1.
Ну, давайте просто посмотрим на вероятность того, что случайный граф не связан.
Конечно, это можно в терминах неравенства Маркова записать. Может быть, образцово показательно так и надо было бы сделать, но в данном случае все можно вообще без каких-либо неравенств делать.
Мы хотим доказать, что вот эта вероятность в нашем пункте стремится к нулю. Вы согласны, да?
Ну а что значит граф не связан? Это значит, что он принадлежит объединению множеств каких-то там несвязанных графов.
Ну давайте я так напишу. Это вероятность того, что существует k от единицы до n-1 и существует w из w. w это множество вершин нашего графа.
Ну можно так вот написать w из 1 и так далее n. У нас множество вершин. Это числа от единицы до n. Так мы определяли случайный граф.
Вот существует w такое, что мощность w равняется k и g, ограниченные на этом множество, это связанная компонента.
Вот так. Ну я просто формально переписал то же самое. Что значит граф не связан? Это значит, что в нем есть компоненты связанности размера до n-1.
Иначе это бессмыслится полное. Конечно до n-1. Граф не связан, если в нем есть нетривиальная связанная компонента.
Вот все, что я написал. Тривиальность некую. Ну существует множество его вершин, которые имеют мощность строго меньшую, чем мощность всего множества вершин.
И на котором граф обработан. Кусок графа является связанным и никуда дальше ребра не идут.
А я же вроде определял уже в рамках этого курса такую запись. Было, было это g, ограниченные на этом множество просто.
То есть мы берем наш граф, рассматриваем в нем кусок и смотрим индуцированный под граф. Я даже определение давал. Индуцированный или порожденный под граф.
Нет, я сказал связанная компонента. То есть оно максимальное по включению, да.
Ну я могу еще подробнее сказать, что g на w связан, давайте это нам пригодится, g на w связан и для любой вершины не принадлежащей w x нет ребер, нет ребер из x в w.
Ну давайте я так напишу, да. То есть что вот этот кусок граф, ограниченный на множество w, он связан, если вы возьмете любую вершину, которая не входит в w, то таких ребер нет.
Это и есть связанная компонента, да, совершенно верно. Я фактически это и подразумевал, но так наверное понятнее.
Тем более, что нам это пригодится. Ну сейчас понятно все, да, что я написал.
Существует нетривиальная связанная компонента, размер которой меньше, чем размер всего графа.
Сколько-то вершин она имеет, есть множество вот этой мощности, на котором кусок графа связанный и наружу никакие ребра не идут.
Соответственно все кванторы существования означают объединение множеств графов.
Мы как всегда, как и в предыдущей теореме, помните, с жадным алгоритмом все кванторы существования оцениваем суммами.
Это понятно? Я сейчас напишу, что это меньше либо равно.
Суммы пока от единицы до n-1, суммы по всем w из 1 и так далее n мощности k.
Ну вот этой вероятности, что g на w связан и для любого x не из w, нет ребер из x в w.
Вот так. Я совершенно не понимаю, как оценивать вероятность того, что граф связан, но поскольку это пересечение двух условий,
то я тупо сейчас вот это забью и у меня получится опять верхняя оценка.
Ну то есть я напишу верхнюю оценку вот так.
Так, сумма пока от единицы до n-1, сумма по w мощности k, понятно каким.
Вероятность того, что, ну давайте я так напишу, для любого x не из w и для любого y из w пара x и y не является ребром.
Ну это то же самое, что я писал словами, что нет ребер.
Берем любую вершину снаружи, любую внутри, и они ребром не соединены.
То есть я тупо просто пересечение двух событий оценил одним.
Это понятно. Чему равна вот эта вероятность? Это совсем легко.
Ну на картину смотрите, вот таких ребер нет. Сколько всего таких ребер?
k умножить на n-k, товарищи.
Ну k способов выбрать y из w и n-k способов выбрать x не из w.
То есть вот эта вся вероятность, это просто 1-p в степени k на n-k.
Соответственно от w эта вероятность не зависит, поэтому получаем вот такую запись.
Сумма по k от единицы до n-1, c из n по k на 1-p в степени k на n-k.
c из n по k это просто количество слагаемых. Сколько есть множеств мощности k? Это понятно.
Смотрите, если k равно единице, то c из n по k на 1-p в степени k на n-k это в точности n на 1-p в n-1.
Я надеюсь вы это узнаете. Это мата ожидания числа изолированных вершин.
Про которые мы знаем, что в нашей текущей ситуации оно стремится к нулю.
Потому что c теперь больше единицы, оно стремится к нулю.
Но это стремление к нулю только самого первого слагаемого.
О слагаемых замесище от n-количества.
Даже если каждое последующее меньше предыдущего, из этого еще не следует, что вся сумма стремится к нулю.
Потому что стремление к нулю этой штуки уже на грани.
Помните, что если c очень близко к единице, то стремление к нулю прямо по кромочке проходит.
Поэтому так вот за здорово живешь на n, например, умножить не получится.
Ну какая может быть идея, что с этим делать дальше?
Но надо не просто каждое из них оценить сверху,
а надо доказать, что каждое последующее не просто меньше предыдущего, а в растущее число раз меньше.
Давайте я вот эти штуки обозначу ак, т, ат, н.
Объясню общую идею, а выкладку опять оставлю на следующий раз.
У нас как-то так в этом семестре повелось, что вся идейная часть на одной лекции, а выкладка на другой.
Но ничего страшного.
Вот давайте слагаемый обозначим ак, т, ат, н.
Мы знаем, что а1, ат, н стремится к нулю.
Это вот как раз вот эта величина.
Это мы знаем.
Идея такая. Мы хотим посмотреть на отношение ак плюс первого ат, н к ак, т, ат, н.
И попробовать доказать, что оно оценивается сверху каким-то q, ат, н независящим от k.
И при этом стремящимся к нулю.
Прен стремящимся к бесконечности.
Вот если это получится, если это получится, тогда итоговая оценка всей суммы выглядела бы так.
Мы вынесли бы за скобки а1, ат, н, которая стремится к нулю.
В скобках была единица b от а1, ат, оставшаяся.
Потом а2, ат, н поделить на а1, ат, н.
Потом а3, ат, н поделить на а2, ат, н.
На а2, ат, н поделить на а1, ат, н.
Но мы уже пользовались такой идеей.
И так далее.
Чего?
Не понял. В чем вопрос?
Если бы оказалось, что вот это вот так, как я написал,
то я бы воспользовался вот такой вот простой идеей.
У нас уже один раз такая идея была.
В чем проблема?
А, ни в чем, да?
Всем понятно, что там вот так получается.
Ну так вот это у нас было бы меньше либо равно q от n,
и это было бы меньше либо равно q квадрата от n.
Там дальше q в кубе, q в четвертой и так далее.
То есть это сумма геометрической прогрессии, хоть бесконечной.
То есть это меньше, чем а1 от n умножить на 1, поделить на 1, минус q от n.
А q от n стремилось бы к нулю.
И все.
Эта штука ерундовская, она стремится к единице, а а1 от n стремится к нулю.
Идея такая.
Есть еще чуть-чуть времени.
Это очень хорошо.
Потому что техническая реализация, к сожалению, чуть сложнее.
Нечто в духе асимптотики для унициклических графов, если помните.
Там надо сумму разбить на две части.
Ну, во-первых, надо заметить, что фактически нас интересует вот такая сумма.
Ну, потому что верхняя часть, вот эта, она симметричная.
Ну, потому что верхняя часть, вот эта, она симметричная.
c и z пока симметрично относительно серединки.
И q умножить на n минус q тоже симметрично относительно серединки.
Поэтому достаточно доказать, что вот такая сумма стремится к нулю, правда ж?
Да.
К сожалению, вот это вот все верно только в случае, если мы суммируем до какого-нибудь верхнего предела,
который бесконечно мал по сравнению с n.
Но это вы увидите потом.
Это уже на следующей лекции будет.
К сожалению, прямо вот до n пополам эта идея не реализуется.
Начиная с какого-то момента отношение перестает быть ограниченным функцией, стремящееся к нулю.
Но к этому времени само окатое становится настолько маленьким,
что можно отцепить этот оставшийся хвостик и его с запасом оценить.
Короче, техническая реализация такая.
Пишем отдельно сумма пока от единицы до n поделить на корень из логарифма n.
Почему именно на корень из логарифма, этого узнаете в следующий раз.
И отдельно пишем сумму пока вот n на корень из логарифма n плюс 1 до n пополам того же самого.
Вот этот хвостик будет оценен просто за счет того, что здесь уже окаты совсем ничтожно маленькие,
а эта часть основная, она будет оценена за счет реализации этой идеи.
Ещё раз, не надо суммировать до n минус 1, потому что функция, которую мы суммируем, симметрична относительно n пополам.
Но фактически эта сумма, это просто удвоенная вот эта сумма, примерно удвоенная.
Мы хотим доказать, что она стремится к нулю, но не надо удваивать. Зачем?
Ну, просто понятно, что сначала эти штуки убывают, убывают, вот как я написал, а дальше они симметрично начинают обратно возрастать.
Но зачем нам это возрастать, ловить?
Оно же удваивает только исходную сумму.
Поэтому мы смотрим только левую половинку, на которой действительно удастся реализовать вот эту идею, по крайней мере, вот досюда.
Ну или даже до чего-то другого, просто вот конкретно в этой части приятнее считать, когда в знаменателе корень из логарифма.
Это вы увидите. Но это большого значения не имеет.
Все на сегодня.

Ну нехорошо, а что я могу сделать? Я что, не лечусь что-то? Народными, ненародными способами? Не лечу?
К сожалению, не лечится
Добрый день
Так, товарищи, ну чего? С голосом-то у меня, конечно, все так же
Беда, вот просто беда, понимаете, вот не знаю, что с этим делать, читаю лекции, он не восстанавливается
Ну а как-то бросить их читать, мне жалко
Ну как вот я их брошу на неделю? Кто ж вам прочитает про матрицу Адамара, как не я?
Помните, чем мы закончили прошлую лекцию? А, вы не думали ни над чем?
Да, понятно, а какая очевидная?
Ну, это кому как очевидно. Вот интересно, поднимите, пожалуйста, руки те, кто вообще думал над вопросом про матрицу Адамара
Ну не густо, не густо, дальше можно не спрашивать
Так, ну давайте я все-таки поясню, потому что мне кажется, что это важно пояснить
Это важно просто с той точки зрения, что вы дополнительно замотивируетесь заниматься такими вещами, это тоже очень полезно
Так, значит
Предлагалось рассматривать вот такой вот дистанционный граф, такой граф Джонсона
Я напоминаю, что это граф Джонсона
Называется граф Джонсона
Так, слушайте, а кто-нибудь помнит вообще, как он определялся?
Нет, товарищи, если вы даже этого не помните, то это не очень хорошо, потому что я стараюсь читать же не на предмет того, чтобы вы записали, мы это как раз в прошлый раз обсуждали
А на предмет того, чтобы вы еще и поняли, чего происходит, ну и могли это еще успевали записать
Поэтому как-то хочется, чтобы вы старались следить за происходящим на протяжении всего семестра
Так, сейчас я разговорюсь как-то, я уже это чувствую
Значит, смотрите, я напоминаю, что конкретно в этом случае вершинами графа служат все возможные под множество множества, которые я обозначил там R с индексом N
Это было множество просто состоящее из каких-то N элементов
Ну, R с индексом N это 1, 2 и так далее N
И мощность каждого множества равняется N пополам
Вот такими являются вершины
Это все возможные N пополам элементные под множество некоторого N элементного множества
Так, а ребрами мы соединяем две вершины, два под множество, если мощность их пересечения в данном конкретном случае равняется величине N поделить на 4
Ну, сейчас должны вспомнить
Так, причем же здесь матрица Адамара?
Матрица Адамара
Смотрите
Матрица Адамара, как я напоминал в прошлый раз, это матрица, которая целиком и полностью состоит из плюс и минус единиц
Причем, каждые две ее строчки перпендикулярны друг другу, то есть их скалярное произведение равняется нулю
Так, товарищи, тоже помним? Тоже помним, молодцы
Так
Ну, смотрите, я напомню, что это в частности означает, что матрицу можно привести к каноническому виду
А именно, можно считать, что матрица Адамара выглядит следующим образом
У нее верхняя строчка состоит из сплошных единиц, а дальше каждая последующая строчка содержит половину единиц и половину минус единиц
Так, друзья, вот это помните, нет?
Я с большой надеждой на вас гляжу
Шикарно
Так, ну, можно, например, типично вот так вот нарисовать половину единиц, половину минус единиц, вот они накладываются, получается ноль
Скалярное произведение получается равным нулю
Ну, дальше там тоже как-то происходит
Тут минус один, например, тут один, тут минус один, ну и так далее
Это матрица размера N умножить на N
Ну и когда мы обсуждали матрицу Адамара в прошлом году, мы в частности поняли, что вот из всего этого знаете, чего следует?
Что N делится на 4
Было, конечно, такое, N обязательно делится на 4, это простое упражнение для тех, кто этого не знает
Ну, можете послушать соответствующую лекцию в записи, это было тоже сделано
В общем, N, давайте так напишу, делится на 4, как следствие из определения
Ну и великая гипотеза состояла в том, что если четверка является делителем числа N, то такая матрица всегда существует
Причем гипотеза не доказана, а доказана это только в частных случаях, и я приводил там какие-то примеры конструкции, когда это действительно получается
Вот, ну хорошо, причем же здесь вот этот граф?
Смотрите, давайте я от этой матрицы перейду к матрице, которая по сути ей равносильна
Мы даже когда строили примеры таких матриц или доказывали какие-то оценки про них, что-то подобное делали
Ну, может быть еще проделаем чуть позже
Значит, мы в этой матрице единицы сохраним, а минус единицы заменим нулями
Так, очень просто, возьмем просто единицы сохраним, а минус единицы заменим нулями
Половина единиц теперь будет половина нулей
Тут вот четверть единиц, четверть нулей, четверть единиц, четверть нулей, ну и так далее
Так, все пока понятно
Так, ну посмотрите сюда, вот на эту строчку
В ней половина, то есть n пополам единиц и n пополам нулей
Ну, да-да-да, я говорю, посмотрите вот на эту строчку, я ее показал, да
Первую строчку забываем, вот так отделяем и смотрим все, что ниже первой строчки
Так, друзья, успеваете все фиксировать?
Значит, вот все, что ниже первой строчки, там каждый раз половина, то есть n пополам единиц и n пополам нулей
Да?
Ну, слушайте, как этому теперь поставить в соответствие вершины графа Джонсона?
Ну, давайте скажем, что вот здесь, например, вершина графа Джонсона, которая отвечает строчке, это просто множество, состоящее из первых n пополам чисел вот отсюда
1, 2 и так далее n пополам
Вот для этой строчки, давайте еще вот так напишем, 1, 2 и так далее n пополам
Вот для этой строчки соответственно у нас получится 1, 2 и так далее n поделить на 4, потом будет n пополам плюс 1 и так далее 3n на 4
Ну, я настолько подробно рассказываю, чтобы у вас точно было понимание, чего происходит
То есть, я просто сопоставляю строчки из нулей единиц номера позиции, на которых стоят единицы
Образуя тем самым вершины этого графа
Ну и последний штрих
Смотрите, изначально вот эти векторы были артагональны
Значит, как пересекались множество их единиц между собой?
Poin на 4
Друзья, вот это понятно?
Множество единиц одного вектора и множество единиц любого другого пересекаются по общим n на 4 элементам
То есть, что образуют вот эти вершины в графе Джонсона?
О, многие знают даже слово клика
Ну, я бы сказал, полный подграф
Полный подграф, он не будет в графе Джонсона
О, многие знают даже слово клика
Ну, я бы сказал, полный подграф
Полный подграф, он по-другому еще называется словом клика
Поднимите, пожалуйста, руки те, кто знает слово клика
А, ну все знают, отлично
Значит, понимаете, что в терминах вот этого графа, который имеет прямое отношение к теории кодирования
Максимальная клика в этом графе
Максимальная клика, то есть максимальный полный подграф
Это в точности вопрос о существовании матрицы Адамара
Вопрос о том, чтобы найти максимальную клику в этом графе, равносилен тому, чтобы построить матрицу Адамара
Нет, она не может быть больше, чем на n вершинах
Он именно равносилен
То есть, если у вас есть клика, вы же можете обратно по ней построить вот такую матрицу
Тут все взаимно заменимо
И тогда у вас получится что-то большее, чем матрица Адамара, но такого не бывает
Да, да, да, но матрица Адамара это максимум, что можно построить, конечно
И она-то неизвестно существует ли
Но если она существует, то несложно из линейно-алгебраических соображений понять, что к ней уже ничего ниже не добавишь
Просто, ну как, если они попарно-ортагональны в пространстве, то их не больше, чем размерность
А при этом они лежат еще в гиперплоскости, которая имеет размерность n-1, потому что сумма координат равняется нулю
Поэтому их точно не больше, чем n-1
Я не знаю, друзья, вот эта последняя дискуссия была понятна?
Это важный момент, что если матрица Адамара существует, это уже круто, но добавить к ней какой-то вектор ниже точно не получится
Так, нормально все?
Ну кроме того, что я поскрипываю голосом в остальном, все нормально?
Это хорошо
Так, на этот вопрос ответили, теперь смотрите
Второй вопрос
Какой у нас был второй вопрос?
Число треугольников
Вот сложная задача
Кстати, да, число треугольников – это число способов составить три строчки в матрице Адамара, правильно?
Число треугольников – это и есть три строчки в матрице Адамара, вот сколькими способами их можно построить
Так, ну, смотрите
Я написал количество вершин
Согласны?
Давайте я напишу это число вершин
Давайте теперь посчитаем количество ребер
Вот мы зафиксировали какую-то вершину, да?
Правильно!
Да, давайте зафиксируем одну вершину
Ну, например, один, два, и так далее, n пополам
И посчитаем, какова ее степень
То есть, сколько ребер из нее выходит
Как это посчитать?
Надо отсюда выбрать n поделить на 4 элемента, правильно?
И из оставшихся тоже выбрать n поделить на 4
Так, друзья, никого не кокает такая простая комбинаторика
Все успевают понять, что происходит
Ну, нам нужна вот эта вершина А
Есть вершина B, которая вместе с А образует ребро
Ну, значит, она должна пересекаться с этим множеством по n поделить на 4 элемента
Мы можем t из n на 2 по n поделить на 4 способами выбрать вот эти общие n поделить на 4 элементов
Правильно!
И это надо возвести в квадрат
И из оставшихся n пополам элементов нам тоже надо выбрать n поделить на 4
Так, это степень каждой вершины
Степень каждой вершины
Но это называется граф регулярно
Ну, мы знаем, что сумма степеней всех вершин – это удвоенное количество ребер
Поэтому число ребер вот в таком графе Джонсона, который мы сейчас рассматриваем, это 1 вторая
c из n по n на 2
на c из n на 2
по n на 4 в квадрате
Друзья, все не зря вы увидите, сейчас будет некий микрокатарсис, когда мы посчитаем
Так, но нас-то интересует число треугольников, правильно?
Вот мы посчитали, сколько есть ребер
Как теперь посчитать, сколько есть треугольников?
Надо найти число способов построить вершину, которая и вот с этой, и вот с этой образует ребро
Смотрите, давайте я картинку такую нарисую
Вот это a, в ней n пополам элементов
Дальше есть b, в ней тоже n пополам элементов, а вот эта общая часть, она размера n поделить на 4
Потому что они образуют ребро
А, вот это b, и картинка в терминах множества, она получается вот такая
Нормально?
Так, теперь нас интересует c, такая, что она образует ребро из a из b
То есть, смотрите, она должна вот эту a пересекать по n на 4 элементов
И b тоже должна пересекать по n поделить на 4
Все успевают?
Ну, давайте буквой, сейчас нарисую красиво
Знаете, по мне проснулся художественный талант
Я всегда считал, что я очень плохо рисую, а смотрите, как красиво
Чертежник прям
Так, вот здесь давайте будет, так, здесь чертежник
Так, здесь у нас n на 4
А давайте b
Сейчас я как-то так вот нарисую, о, не, плохо, плохой рисунок
О, вот так, а?
Докторская что, диссертация?
А, колбаса, сарделька
Ну, докторская колбаса, да, видите, вот, у кого какая ассоциация, да?
Вроде бы я главный специалист по сарделькам, но мне говорят, докторская, говорю, диссертация
О чем мне докторская диссертация? У меня уже 20 лет как докторская диссертация
Так
Ну, похоже, я себя перехвалил
Давайте буковка k, это количество элементов той части множества c, которая попадает вот сюда
Просто так обозначим
То есть, ну, если хотите, k это что такое? Это a-b пересеченное с c, просто по мощности
Просто по определению k это мощность вот этого пересечения
Так, сколько стало быть в этой докторской вот здесь элементов?
А?
n на 4 минус k
Смотрите, c должно пересекаться с a, чтобы образовать ребро по n поделительно 4 элементам
Должно?
Так, кто-то мне решил позвонить, секунду
А я не знаю кто это
Интересно, кстати
Так, ну, значит, вот здесь у нас n поделительно 4 минус k
Видно что-нибудь?
Вот эта вот маленькая сардельечка, она размера n поделительно 4 минус k
Так, дальше есть кусочек множества c, который находится вот в этой части, каков его размер?
k правильно, потому что тумба вот этих двух чисел должна быть n поделительно 4
Чтобы образовалось ребро уже с b
Успеваете?
Так, и у нас еще вот тут вот сколько элементов?
Еще n на 4, правильно
Потому что тут n на 4, тут n на 4, тут n на 4
А всего же n значит есть еще свободное n поделительно 4 элементов
И здесь есть кусочек нашей сардельки
Ну, ясно, что его размер это n поделительно 4 минус k
То есть, смотрите, вершину c можно выбрать
t из n на 4 по k умножить на c из n на 4 по n на 4 минус k
умножить на c из n на 4 по k
и умножить на c из n на 4 по n на 4 минус k способами, если k зафиксировано
Друзья, мне кажется, я предельно понятен, но может быть я...
Что?
Но вот мы их зафиксировали одним из вот стольких способов
Да?
И при каждой фиксации a и b вот в таком количестве
У нас столько способов выбрать c при данном k
Ну, то есть, дальше надо просуммировать по k
от 0 до n на 4, правильно?
Надо просуммировать по k от 0 до n на 4
И умножить вот на это
Ай, виноват, и еще поделить на 3
Так, все понимают, почему поделить на 3?
Ну, конечно!
Что?
Тут уже пополам поделили, поэтому когда третью добавляем только на 3
То есть, ответ про число треугольников, который мы делаем
Ответ про число треугольников, куда бы его написать?
Это 1 шестая
c из n по n на 2
c из n на 2
по n на 4 в квадрате
Умножить на сумму
по k от 0
до n поделить на 4
c из n на 4 по k
В четвертой степени
Поняли, да, почему я эту задачу поставил?
Помните про четвертую степень?
Говорили
Сумма четвертых степеней, ее асимптотика
Отвечает за асимптотику числа треугольников в дистанционном графе Джонсона
Ну, по модулю вот этой фигни, которую мы в прошлый раз вообще сосчитали с помощью Стирлинга
Это совсем легко
Вот так вот
По-моему это очень иллюстративно, все-таки надо было это проговорить
Так
Есть какие-то вопросы?
Комментарии?
Впечатления?
Вообще голос, конечно, скрипучий, но чуть-чуть как будто бы позвончий
Так, в целом скрипучий, но вроде бы что-то такое
Говорят, вот есть народные средства, интересно, конечно
Надо че-то делать
Вот так вот
Вот так вот
Вот так вот
Вот так вот
Вот так вот
Вот так вот
Че-то такое шарахнуть
Чтобы уже никогда не заболевал
Че такого, да?
Я вот сам думаю, чего
Так, друзья, давайте двигаться дальше
На самом деле, конечно, все вот эти асимптотические истории я как мог замотивировал
Но они будут вылезать у нас постоянно в разных задачах
И вот сейчас я хочу перейти к очень важному разделу
Который касается того, что называется случайные графы
Но смысл этого раздела, он очень естественный
Он очень простой
Вот что можно сказать про типичный граф?
Можно найти какие-то типичные его свойства
Может быть, отладить какие-то алгоритмы можно не на всех графах, но только на типичных
Вот что значит типичный граф?
Ну, это, конечно, верно, особенно если просто проведено либо нет
Но это правильно, да
То есть, на самом деле, я хотел сказать философски более широко
Типичный граф, это зависит от ситуации
Действительно, если мы изучаем какие-нибудь процессы, связанные с формированием сложных сетей типа интернета
То кажется, что типичный граф устроен не совсем так, как Миша предлагает
Потому что типичный граф, который интерпретируется как вершины сайта, а ребра это гиперссылки между ними
Он совсем не такой случайный, что в нем либо проведено ребро, либо нет
На самом деле, у него есть какие-то свойства, которые делают его сильно отличающимся
Поэтому в зависимости от того, какую жизненную ситуацию вы изучаете
Друзья, вы это для себя как-то вот отразите
В зависимости от того, с какой именно конкретной физической ситуацией или там
Ситуацией в теории алгоритмов вы встречаетесь
Случайность нужно понимать по-разному, типичность нужно понимать по-разному
Но вот мы начнем с очень важной модели
Я ее тоже сейчас замотивирую
Которая была предложена давно, давайте так скажем
Но всерьез начала с математической точки зрения исследоваться на рубеже 50-х и 60-х годов 20-го века
Давайте я что-нибудь вот так сделаю
Это годы, когда появились самые такие вот, как бы это сказать, пионерские работы на эту тему
В которых действительно доказывались очень существенные свойства относительно этой модели
Принято ее приписывать в честь этого двум товарищам
Одного и второго, а другого и третьего
Принято ее приписывать в честь этого двум товарищам
Одного из которых звали Эрдеш, другого Ренни
Ну я просто ударение поставил, чтобы было понятно
Вот
Но физики, вы тут у нас не физики, но физики обижаются
Говорят, да мы знали эту модель задолго, да, Эрдеш и Ренни
Нахрена вы этих математиков вообще сюда приплели?
Ну, приплели, потому что, повторяю, они стали математически, систематически исследовать эту модель, о которой я сейчас скажу
Но, чтобы не обижать физиков, говорят, классическая модель
А еще, совсем правильно, отражая суть модели, называют ее биномиальной
Она же биномиальная
Классическая, она же биномиальная
Ну а смысл ровно такой, как было сказано
У нас есть, давайте, просто числа от единицы до n
И мы их интерпретируем как вершины нашего графа
Ну, то есть, есть n объектов, неважно какой природе, можно считать, что это просто первые n натуральных чисел
Вот они образуют множество вершин графа
Это множество никоим образом не случайно
Дальше мы берем ребра полного графа на этом множестве вершин
Графы считаем пока что обыкновенными, то есть, без петель, без кратных ребер, без ориентации
Вот в полном графе c и z подва ребер
Дальше берем некоторое число p из отрезка 0,1
И интерпретируем его как вероятность возникновения каждого из этих ребер
То есть, e и t проводим с вероятностью p
И не проводим с вероятностью...
Какой?
1 минус p, да, но я думал вы скажете q
У вас в курсе теории вероятностей было что-то про такую схему? Или не успели еще?
Ну, хорошо, да
Ничего страшного, я все напомню, все докажу независимо от теории вероятностей
Это будет только на пользу
Да, я продублирую какой-то материал, но я его со своей точки зрения проинтерпретирую
И мне кажется, так будет только лучше
Называется это схема испытаний Бернули
Если вы такого не слышали в курсе теории вероятностей, не переживайте, скоро услышите
А поднимите руки, кто слышал?
А, ну вроде практически все
Схема испытаний Бернули, то есть я не сказал важную вещь
Все вот эти вот операции проведения и непроведения, они, конечно, взаимно независимы
Но потому что вообще это важно сказать
Вдруг есть какие-то зависимости?
Но мы предполагаем, что никаких зависимости нет
Так, дурацкий вопрос
Вот представим себе, что n равняется 4
И вот нам дан вот такой конкретный граф
1, 2, 3, 4 на наших четырех вершинах
Какова вероятность его возникновения, если схема описана вот так?
Правильно совершенно, да, p в четвертый умножить на q в квадрате
Мой богатейший опыт подсказывает, что не все это сходу понимают
То есть вот чему ж вас точно, я думаю, учили в курсе теории вероятностей
Подтвердите или опровергните, пожалуйста
Это что все зиждется на элементарных исходах
Они же элементарные события
Было такое?
Вот здесь кто является элементарным событием?
Нет
Граф
Значит, если давать формальное теоретико-вероятностное определение, то есть множество элементарных событий
И это множество состоит из всевозможных графов на n вершинах
То есть мощность множества элементарных событий это 2 в степени c из n под 2
Значит, мы не знаем, с какой вероятностью граф связан?
Так
Вот такое вот обозначение было
Или f писали?
А, Борелевская, черт
А вот так писали?
Ну не попал сходу, да
У вас было слово страшное сигма-алгебра
А нам не надо
Ну у нас все конечное, товарищи
Ну пусть большое, конечно
Но конечное же, понимаете?
То есть что такое fn?
Это просто 2 в степени ωn
Ну множество всех под множество в множестве всех графов
Сейчас, друзья, понятно?
То есть событие, событие
Это просто, как я уже говорил
Множество графов
По-другому, это свойство графа
Вот мы говорим граф связан
Это значит, что он принадлежит множеству, состоящему из всех связанных графов
Друзья, я понятно говорю?
Вероятность того, что граф связан, это просто сумма вероятностей связанных графов
Но вы на меня смотрите грустными, наверное, глазами, потому что непонятно, как такую сумму считать
Почему я, типа, клоню
А вот я клоню к тому, что методы теории вероятностей в сочетании с методами дискретной математики
И в сочетании со всякими вот этими асимптатическими ужасами
Они позволяют это решать
Так, друзья, вот это все
Так, друзья, почему я вообще на связанность так напираю?
Потому что для свойства связанности вот эта модель
Наиболее естественна с точки зрения именно практической интерпретации
Ну, потому что, как я обычно говорю людям
Сейчас я сотру и скажу
Как я обычно говорю людям?
Я им говорю так
Вот послушайте
Есть компьютеры
Видите какие компьютеры?
Я сегодня в ударе
А? Какое художество
Я в ударе
Я в ударе
Я в ударе
Я в ударе
Я в ударе
Какое художество
Один компьютер стоит в Москве, другой в Санкт-Петербурге
Третий там в Мурманске
Твертый на Байкале
Пятый где-нибудь в Сочи
Видите, как я знаю географию
Но важно другое
Важно, что мы давайте предположим
Изначально
Каждые два компьютера на этой картинке
Но я умру это все рисовать
Попарно соединены какими-то линиями связи
Ну, то есть, есть два телефона, там два модема
Можно между ними передавать какую-то информацию туда-обратно
И тут тоже есть два телефона
Пожалуйста, можно между ними передавать информацию
Все-все-все
Я не все нарисовал
Но и это есть
И вот такое есть
Я погибну все это рисовать
Друзья, понятно, да?
Раз, два, три, четыре, пять, шесть
То есть, с из шести по два это двадцать, что ли? Пятнадцать
Пятнадцать
Ну, тут не пятнадцать получилось
Так, а дальше происходит очень простая вещь
Естественная
Возникают помехи
Ну, помехи могут возникать как?
Они могут просто случайно возникать
Просто из-за того, что канал связи зашумлен
Могут целенаправленно возникать
Есть какой-то противник, который пытается нам повредить
Который не позволяет, значит, передавать информацию
По каждому отдельно взятому кану
Вот, пусть, например, буква П
Пусть, например, буква П
Это вероятность, с которой каждая отдельная связь сохраняется
Вы понимаете, что такое вопрос о связанности случайного графа?
Это в точности вопрос о том, что несмотря на помехи
На злонамеренные действия противника
У нас сохраняется возможность передачи информации
Между любыми двумя компами
Согласны?
Ну, не напрямую, там, по какой-то цепочке
Ну, свет идет со скоростью света, это ж хорошо
У нас что-то сегодня сплошная физика, прям
Вот, так что...
Такой вот очень естественный вопрос
При этом, смотрите, вот здесь же непонятно, что такое типичный граф
То есть, мы же не знаем, какое П
Может быть, например, знаете, какой вопрос поставлен?
Насколько маленьким можно выбрать вот это П?
Насколько маленьким можно выбрать вот это П?
Если мы хотим, чтобы связанность сохранилась с вероятностью, например, больше, чем одна вторая
Так, друзья, понимаете, чем меньше вероятность, тем
меньше вероятность сохранить ребро
То есть, тем больше вероятность его потерять
Вот я хочу найти, например,
самое маленькое значение П
При котором, вероятность сохранения связанности
все-таки больше, чем одна вторая
То есть, чаще связанность сохраняется, нежели не сохраняется
notified, Save
нежели не сохраняются. Понимаете, да? Для чего раз на ИП рассматривать? А то вы мне можете
сказать, а зачем мне тут П? Я могу одну-вторую написать. Тогда все графы равны вероятности.
Заметьте, давайте я вот здесь напишу вероятность конкретного графа по аналогии вот с этой ситуации,
которую мы как пример рассмотрели. Это P в степени мощность E на Q в степени C из N по 2 минус
мощность E. Это понятно. И вероятность любого события, как я уже говорил, это просто сумма
вероятностей конкретных графов, которые этому событию благоприятствуют. А вы мне скажете,
P равно Q равно 1 и 2, тогда это просто 1 поделить на 2 в степени C из N по 2. Классическое такое
вероятностное пространство. Но нет, потому что, видите, вот в этой задаче не обязательно P равно
1 и 2. Нам, может быть, хочется действительно выбрать очень маленьким это P, то есть разрешить
гаду противнику уничтожать ребра с вероятностью близко к единице, и вдруг, тем не менее, получится,
что граф останется связанным с вероятностью больше, чем 1 и 2. Понимаете, да? Но я вам точную
формулировку скоро дам. Перерыв 5 минут. Давайте рассаживайтесь, пожалуйста, и затихайте по
возможности. Я, конечно, голос, как вы видите, не особо берегу, потому что мне жалко как-то,
но я ж люблю читать эмоционально, но как вот я буду шептать, что ли? Можно завести микрофон и
шептать в микрофон. Вы смеетесь, а я однажды это делал. Ладно, я вам расскажу этот исторический
анекдот. У меня на самом деле вот эта проблема с голосом, она вечная, она уже много лет существует,
я регулярно теряю его и теряю вот так где-то на месяц, на два, никак не могу восстановить,
потому что читаю очень много лекций. В 2011 году, как некоторые, наверное, из вас знают,
мне случилось получить премию президента. Ну, такая большая премия, меня в Кремль там возили,
награждали и так далее, но я не знаю, вы в курсе, не в курсе, это было в феврале 2012 года. За 2011 год
была премия. В общем, тогда Медведев был президентом, в общем, с Медведевым я еще
повстречался, там я еще был в голосе, на следующий день я потерял голос. Хрен его знает почему,
может как простуда была, неважно, на следующий день я его потерял, в чем потерял вот не как сейчас,
а я просто открываю щуку рот, не слышно что пойдет, вообще ничего не слышно. А на фистехе, конечно,
очень обрадовались, что я получил эту премию и в субботу, а это был голос, я потерял в четверг,
в субботу было общее собрание профессорско преподавательского состава, на котором я
должен был выступить с докладом о том, за что я получил эту премию, ну чтобы как-то так вот
попиарить меня, как человек, который вот на фистехе получил такую премию. Но все, голоса нет,
большой концертный зал, знаете наш концертный зал, но он чуть-чуть другой был, ну неважно,
то же самое помещение, это его отремонтировали сейчас, но размер примерно такой же. Все,
я мог только шептать, мне дали микрофон, я вот так его прям поднес совсем к губам,
ну и там что-то спасибо и так далее. Бывает, так что это не впервой, вы не думаете. Так,
давайте чтобы вам как-то было, ну поинтереснее, я сейчас сформулирую теорему,
которую частично докажу в конечном счете, частично не докажу, вот, но, наверное,
начну доказывать не сегодня. Сейчас вы поймете почему. Собственно, вот это вот один из основополагающих
результатов, который был получен в упомянутые годы, 59-й, 61-й, это один из многих важных результатов,
которые они получили. Значит, пусть вероятность ребра случайного графа, кстати знаете,
что я забыл, я забыл написать, что эта модель еще вот так вот обозначается. Это модель классическая,
она же биномиальная. Так, ну все поняли, почему она биномиальная, да, схема Бернули, это бином
Ньютона, конечно. Вот, значит, она еще обозначается GATNP. Значит, пусть P сейчас чуть-чуть кокнет,
и я буду стараться это потом исправить, то есть пояснять на примерах, почему так важно, нужно и
так далее. А потом докажу, и вы поймете, откуда это берется. Вот я подчеркну, что это не константа,
а это пункция, которая изменяется с изменением числа вершин. А именно так изменяется. Вот кокнет,
когда я напишу, как она изменяется. C умножить на логариф M и поделить на M. Ну, может быть,
кто-то когда-то слышал, я это и школьникам люблю рассказывать, но не знаю, слышал кто-то из
присутствующих от меня это раньше или нет, неважно, вот пусть P ведет себя таким загадочным образом,
почему-то. Да, точку я зря поставил, но давайте я подчеркну, что C это просто константа,
которая больше нуля. Вот какая-то такая странная функция, но единственное, что про нее понятно,
это что она при любом C стремится к нулю. Ну, это очевидно. Тогда, сейчас будет опять физика,
шучу. Если C больше единицы, то вероятность, с которой случайный граф, вот я так напишу,
воспользуюсь обозначением, вероятность, с которой случайный граф связан, стремится к единице при
N, стремящемся к бесконечности. Второе, если T меньше единицы, то та же самая вероятность
стремится к нулю при N, стремящемся к бесконечности. Что? А, если равна, это я позже скажу. Вот эту теорему
мы целиком и полностью докажем в конечном счете. Если равна, это правильный вопрос. Да,
что если C равно единице? Давайте я чуть-чуть отложу ответ на этот вопрос, хорошо? Но я на него
отвечу сегодня. На этот вопрос я сегодня отвечу. Что будет, если C равно единице? Давайте сначала
поймем пафос этого результата. Ну, во-первых, имеет место такой качественный скачок. Смотрите,
здесь вероятность стремится к единице, а здесь она уже стремится к нулю. Но согласитесь, что если
говорить на банальном таком общечеловеческом языке, то в каком-то смысле тут, в этой ситуации,
почти любой граф связан, а тут почти любой не связан. Но, то есть, вот эта ситуация, которая даже
лучше, чем то, о чем я говорил здесь, помните? С вероятностью больше, чем одна вторая сохраняется
связанность. А здесь она не просто больше, она стремится к единице. Ну, правда, надо еще понимать,
с какой скорость. Это отдельный как бы практический вопрос. Про него я тоже скажу. Но чисто качественно,
понятно? Почему я сказал, что это физика? Потому что это в каком-то смысле похоже на переход,
ну, например, от жидкого состояния к какому-нибудь там твердому, да? Вот ноль градусов, чуть-чуть влево
свалились, в минус, и получился лед. Чуть-чуть право свалились, и получилась вода, да? Вот здесь
такая же ситуация. Смотрите, вы чуть-чуть вправо свалились от единицы, чуть-чуть умножили,
там на одну целую, я не знаю, одну квадриллиону, и уже почти все графы связаны. Чуть-чуть свалились
влево, умножили на ноль, там 6 девяток после запятой, и все, почти все графы не связаны. В физике это
называется фазовый переход. Вот в этой теории случайных графов, часть физиков, которые, как
известно, на самом-то деле ее и придумали, такие ситуации тоже называются фазовыми переходами.
Так, друзья, ну это не то, чтобы там обязательно записывать, но вы успеваете при желании, да? Я не
очень быстро говорю. Так, ну чувствуете, что вообще это удивительно, да? Подождите. Сейчас я вам
совсем практический смысл дам. Вот такой пример, один со звездочкой, не то что он сложный, а просто,
ну я не буду это доказывать от какие-то скучные расчеты, это просто тебе запишите, это можно
посчитать из доказательства исходя. Значит, если n больше 100, а c равняется 3, то мы находимся,
конечно, вот в этом режиме, но больше того, вероятность того, что g от np связан,
больше либо равна 1-1n. То есть мы можем контролировать скорость, с которой вот эта
вероятность стремится к единице. Вот, друзья, чтобы просто вас не кокало, чтобы вы понимали,
как устроена вот эта вот граница, вот этот момент фазового перехода, вот здесь написано
сейчас 3 умножить на логарифм поделить на n, да? Вот представьте себе, что n это 2000, ну в России
2000 городов, например, в каждом компьютере. Один. Так, подставляем 3 логарифм 2000 поделить на 2000.
Ну, я вроде помню, это примерно 0011. Можете на калькуляторе проверить, это в общем невелика
задача, вдруг ошибся. Мне почему-то кажется, что так. Это p, товарищи, это вероятность,
с которой сохраняется отдельное ребро. А вероятность, с которой сохраняется общая связность,
какая? 0, 9, 9, 9, 5. Ну, 1 минус 1, 2000. Уж это я могу посчитать. Друзья, понимаете, вот пафосы
именно в этом, поэтому важно, чтобы p была функцией от n в том числе. Чем больше изначально этих
компьютеров, тем меньшую можно взять вероятность сохранения жизни каждой отдельной связи с тем,
чтобы вероятность того, что общая связность сохранится, была вот настолько близко к единице,
или еще ближе. Понимаете, что это впечатляющий результат? Убедил? Так, ну теперь я отвечу на ваш
вопрос, наконец. Отвечу я издевательски. Да ладно, ладно. Не издевательски, а с перехлёстом.
Значит, пункт 3 с двумя звездочками в смысле, что он сложнее, я его доказывать точно не
собираюсь, а это просто скучная выкладка, поэтому я этого тоже делать не буду, и с вас не потребую.
Значит, 3 с двумя звездочками утверждение такое, пусть p от n равняется логарифму n плюс гамма
поделить на n, где гамма это константа. Ну, если гамма равняется нулю, то это вот ровно то,
что вы меня спросили. Это c равно единице вот здесь, правильно? Если гамма равняется нулю. А если
гамма не равняется нулю, друзья аналитики, вы же понимаете, что эта ситуация не укладывается
вообще в эту теорию? Ну, здесь c это константа, а не функция от n. То есть, асимптатически
равно логарифм n поделить на n, но не равно в точности, если гамма не равно нулю. То есть,
это как бы еще один случай, который в целом не укладывается сюда. Нет, но гамма может быть и
больше и меньше нуля. Совершенно верно. Вот если гамма будет стремиться к плюс бесконечности,
то уже будет стремление к единице, а гамма стремится к минус бесконечности, будет стремление к нулю.
Значит, смотрите, в этом случае вероятность того, что g от np связан,
но это просто красиво. Стремится к е в степени минус е в степени минус гамма.
Я поэтому так и приберегал, но не сразу же вам такое показать. Нет, ну вы может быть угадали,
в случае гамма равно нулю, там 1 поделить на е ответ. Если гамма равняется нулю,
тут будет единица, е в минус первое, это 1 поделить на е. Ну и правильно, если гамма стремится к
плюс бесконечности, то е в степени минус гамма это ноль в пределе, е в минус нуля будет единица.
Если гамма стремится к минус бесконечности, тогда тут получается положительная бесконечность,
е в степени минус положительная бесконечность это ноль. То есть фазовый переход, он даже не от
с больше единицы к с меньше единицы, а от вот этого гамма очень большого, гамма очень маленькая.
Настолько, знаете, говорят, очень узкое окно фазового перехода. Вот эти ноль градусов, когда замерзает,
а если гамма равно нулю, это е в минус первое, 1 поделить на е. Ну да, в каком-то смысле. Похоже
на вероятность беспорядка. Считали вероятность беспорядка, да, ведь? А, мы со мной считали,
ну отлично, да. Похоже, да. Да, и последнее, что надо сказать здесь, это что когда вероятность
какой-то последовательности событий стремится к единице, говорят, что эта последовательность
событий выполнена, внимание, асимпатически почти, наверное. Пишут вот так.
Асимпатически почти, наверное. По-английски тоже вот так пишут. Асимптотик Леолмас Чурли.
Ну иногда еще пишут по-английски вот так. With high probability.
VHP, да, with high probability. Я же вроде расшифровал. Асимптотик Леолмас Чурли.
Так, для того, чтобы доказать теорему, а сделаем мы это уже в следующий раз,
докажем теорему. Нам понадобятся такие инструменты, как неравенство Маркова и Чебышова. Знаете ли вы их? Вряд ли.
Наверно, потому что у нас какое сегодня число? 21, да. Просто не могли успеть наши вероятности
это рассказать. Хорошо. А знаете ли вы, что такое мат ожидания случайной величины? Напомнить, да?
Ну давайте так. Вам же дают вероятность в относительно общем ключе, но дискретном пока что,
но тем не менее там бывают, наверное, и бесконечные пространства, правда же? Бывают, да? А у меня их
не бывает. Значит, друзья, как я уже говорил, я хочу немножко задублировать материал курса теории
вероятностей. Это делаю сознательно. То есть они вам дадут очень мощную системную подготовку по этому
вопросу, а я вам дам некоторую интуицию, которая и там будет полезна и будет совершенно строгой в
случае конечных вероятностных пространств. У нас же все конечное. Помните, да? Графов же конечное число.
Так, хорошо. Ну давайте я еще лучше спрошу, а что такое случайная величина-то вы знаете? Или тоже не
проходили еще? Понятно. Но смотрите, вот есть какое-то пространство. Вот это вы проходили. Друзья,
все проходили? Знаете, что есть элементарные исходы, есть события и есть вероятность. Ну,
друзья, но воспринимайте, что вот это множество графов, например, ну или какое это конечное множество,
f для нас всегда это просто 2 в степени омега. То есть нас никакие сигма-алгебры не беспокоят.
f это просто множество всех под множество. Его можно даже не писать в том контексте, в котором
мы с вами будем работать. Ну а вероятность, это просто писалки отдельные. То есть вероятность
устроена как? У нас омега состоит из каких-то элементарных исходов и вероятность просто вот
так определяется. p от омегаитова равняется какому-то питому и, естественно, сумма этих питов равна
единице. Все, вот вся вероятность. Так, друзья, я не очень быстро говорю, понятно? Точно? Вероятность
задана просто значениями вероятности на элементарных исходах. Так что если у нас есть какое-то
событие, то вероятность этого события, это сумма по всем омега, которые ему благоприятствуют,
вероятности вот этих омег. Все. Понятно? Что такое случайная величина? Тяжело? Дала.
Место кси напишу х. А я вот не очень, кси красиво, да я не очень. Ну кси вот какая-то, кси. А как интересно.
Все, товарищи, вот это вот случайная величина. Любая функция. Знаете, когда я школьникам объясняю,
но вы почти школьники еще, да, я обычно говорю, представьте себе, что вот это множество, это вы
сидящие в аудитории, а я кого-то одного вызываю к доске случайным образом. Вот такой вот вероятности.
У меня есть любимчики, есть те, кого я не очень хочу вызывать, поэтому у каждого вероятность своя.
А дальше вызвал к доске и измеряю вес. У меня весы тут стоят. Ну согласитесь, что до того,
как я человека вызвал к доске, это случайная величина, потому что человек вызывается
случайным образом. Но когда он уже сюда вышел, все, ничего случайного тут не будет,
он встанет на весы и у нас получится конкретное число. Вот случайная величина, это такой термин,
это любая функция на множестве элементарных случайных событий. В том числе константа,
конечно, да, вес, рост, количество треугольников в графе, чего хотите. Кого? Количество волос,
да, прекрасно. Так, ну, друзья, я думаю, что понятно. У случайных величин есть такие
характеристики, называются мат ожидания. Математическое ожидание. Математическое
ожидание. Значит, у математического ожидания есть два обозначения традиционных. Одно из них
вот здесь, и я буду использовать его, потому что наши лекторы используют его. Но есть альтернатива,
можно писать вот так. Про это у меня тоже есть анекдот жизненный. Е, конечно, происходит от
слова expectation, ожидание. Я, когда начинал преподавать вероятность, я думал, что М от слова мат ожидания.
И хотя слово математика не русское, но мне казалось, что мат ожидания более, как сказать,
патриотично, что ли, нежели expectation. Я очень привык вот это писать и долгие годы так преподавал.
Потом мне объяснили, что вот это м, оно не от мат ожидания, а оно от, ну, вообще даже не от
английского, а тогда еще от немецкого mittelwerte, но по-английски это mean value, среднее значение.
Но как называется, шах и мат, да? Вот, я продолжал использовать это, потому что привык, а потом
стал использовать это, потому что почему-то на фистехе все это используют. Ну и что же я буду вас
переучивать, что ли? Я сам переучился. Так, определение. Базовое определение совсем простое,
это сумма по всем омега х от омега умножить на п от омега. Ну, то есть, мы вызываем людей к доске,
взвешиваем, а потом складываем полученные величины, виноват тоже с весами, с весами,
которые равны вероятностям быть вызванными к доске. Так, друзья, понятно, такое взвешенное среднее.
Ну, как бы, сколько мы ожидаем получить веса в среднем? Наверное, если у нас в любимчиках
ходят толстяки, то в среднем мы получим больше, чем просто среднеаррифметическое, правда? Хотя,
если вдруг вес Константин по аудитории, то что не делай, а все время получится среднеаррифметическое.
Но так не бывает. Я думаю, что все мы весим немножко по-разному. Даже если выражать в килограммах,
ну а уж если до граммов доходить, то, конечно. Так, друзья, ну смысл этого определения, по-моему,
предельно ясен. Нет вопросов? Можно переписать по-другому. Смотрите, чего произошло вот тут на
верхней строчке. Вместо всей прямой я написал просто перечень разных значений, которые принимает
случайная величина. Их k, при том, что здесь n. Ну, k может равняться единице, правильно? Может быть
константа. k может равняться n, если все значения разные. Ну и любому другому числу в пределах от
единицы даем. Я надеюсь, это тоже понятно в таком темпе, да? Ну, давайте сгруппируем просто
слагаемые по величинам вот этих икс от омега. Может быть y1, может быть y2, там и так далее.
Чего у нас получится? У нас получится y1 умножить на сумму по всем омега, на которых х принимает
значение y1, p от омега, плюс и так далее, плюс ykt на сумму по всем омега, на которых x от омега
принимает значение ykt, p от омега. Понятно, да, чего я написал? Я сгруппировал, вынес за скобку.
Начало вынес за скобку y1, но оно умножается на сумму всех вероятностей, которые соответствуют
элементарным исходам, дающим y1. Ну и так далее. Так, друзья, я не очень спешу. Спеваем? Нет, mx это
все. Это просто альтернативное обозначение, которое я похерил, потому что все используют сейчас вот это.
Про mx не надо ничего. Равенство пошло вот сюда. Это вот это равенство. Я просто в этой сумме
сгруппировал слагаемые по величине х от омега. Может быть она равна y1. Ну сколько раз она равна
y1. Столько раз, сколько есть х от омега равных y1. Вот я и написал это. Просто складываю вероятности
тех омег, на которых х принимает значение y1. А за скобками стоит как раз y1. Ну и так далее.
Ну а это по-другому можно написать вот так. Это сумма по i от 1 до k yt на вероятность того,
что x равняется yt. Так, друзья, опять темп нормальный. Понятно, что такое вероятность того,
что x равняется yt. Это вот ровно вот эта сумма. Это просто мера множества тех омег, на которых х от
омега равняется yt. Ну не принято писать вот эти фигурные скобки омега, двоеточия, пишут просто
вот так, подразумевая это. Понятно? Но тоже очень естественное определение. Можно вот так взвесить
значения, а можно перечислить разные значения и каждые умножить на свой тоже вес. Это называется
мат ожидания случайной величины. Есть роскошное свойство математического ожидания, которое
мгновенно следует из определения. Причем вот это. Свойство такое, оно называется линейность.
Мат ожидания любой линейной комбинации двух случайных величин, это линейная комбинация мат
ожидания. Так, доказательство оно мгновенно следует из определения. Можно я его писать не буду?
Или писать? Писать. Кокнул что ли? Друзья, ну не должен был, ну не может быть, чтобы я в таком
темпе был невоспринимаем. Понятно, что происходит, нет? Можно я не буду раскрывать скобки тут?
Это просто тривиальное следствие того определения, прям мгновенно. Почему это мега свойство? Вот смотрите,
какое замечательное наблюдение. Пусть х это число треугольников. Помните, мы сегодня считали
треугольник. Х это число треугольников, но в графе g. То есть мы находимся в пространстве g от NP,
мы работаем со случайными графами, и нас интересует количество треугольников. Так, дорогие друзья,
если граф на n вершинах, сколько может быть в графе треугольников? Может быть 0, да? Самый
максимум это c из NP. Все понимают, да? То есть вот это x от g, оно принимает вот такие значения 0,
1, 2, c из NP. Это вот y1 и так далее и yk. Для такой случайной величины k равняется c из NP
плюс a1. Такой вот простой факт. Вопрос, как посчитать математическое ожидание такой случайной величины x?
Не, ну можно, конечно, взять вот это определение, да? Сейчас я его возьму. Я уж поиздеваюсь. Нет,
не хотите. А я хочу. Это полезно. Сейчас. Во-первых, понимаете, да, что именно так получится? Вот здесь
написано какие-то y и t, но у меня y и t это просто y или k. y и t это просто последовательные натуральные
числа. Поэтому я их могу обозначить k. И тут будет тоже k. Все успевают? Почему это полезно? Друзья,
попробуйте дома. Настоятельно рекомендую, прям вот очень рекомендую посчитать вероятность того,
что x равняется нулю. Ну или там вы мне скажете, ладно, k равно нулю, такой вероятности тут нет.
Ну посчитайте вероятность того, что x равно и t. Как? Не, это не унициклический граф. Это граф без
треугольников. Вот это граф без треугольников. Это с ровно одним треугольником. Ну ровно один
треугольник, это во-первых, он не обязан быть связан, он унициклический связан. А главное,
что треугольник же, а не непроизвольный цикл, да? В общем, это другое. И, ладно бы, количество,
если вы, а, ну вы не понимаете, я же, конечно, беру вот такое пространство и просто количество.
У каждого графа с одним треугольником вообще-то своя вероятность в зависимости от того,
сколько в нем рёдер. Друзья, я понятно говорю, нет? Может быть, граф, у которого просто один
треугольник, а дальше изолированные вершины. А может быть, граф, у которого один треугольник,
из него растут тэпи какие-то. Тэпи растут из треугольника. Три штуки. В общем, очень прошу,
попробуйте это посчитать. Офигеете? Ну, серьёзно, попробуйте, хотя бы там для n равного пяти или шести.
Прогать придётся, но вы попробуйте. Так, всё, то есть этот путь покнул нас. Ну, потому что мы
даже отдельные вероятности не можем посчитать, как ещё их складывать потом. Давайте линейность применим.
Да, и ещё вот глядя сюда, вы только поймите, что здесь стоит на самом деле формула включения
исключений где-то в глубине. Поэтому считать реально тяжело. Ну, попробуйте. Так, значит,
чего надо сделать? Надо наступить на горло своей алгоритмической песни. Вот вы здесь люди,
которые любят программировать. Ну, большинство из вас, да? Не, ну, по крайней мере вы плохо,
наверное, отнесётесь к человеку, который вам предложит считать треугольники в графе следующим
образом. Вот вам поступает на вход граф, наверное, на вершинах, а человек вам предлагает делать
следующее. Запускаем тройной цикл такой по и по ж и по к. И для каждой троечки и ж к смотрим,
вот в поступившем графе, это троечка вершин, образует треугольник или нет. Согласитесь, да,
классный алгоритм. Ну, я об этом, да, то есть я же сказал, надо наступить на горло своей
алгоритмической песни. То есть люди, которые хотят оптимизировать время, от такого должны
охренеть. Ну, вот посортировать мы не будем. Да, да, да, да, все тройки ребер, нет, я все тройки
вершин. Короче, друзья, я же не просто так издеваюсь, чтобы вы там сейчас попридумывали
какие-то более быстрые алгоритмы. Конечно, с алгоритмической точки зрения задача подсчета
числа треугольников это интересная задача. Она до конца не решена, там есть масса вопросов. Но
я все-таки подойду к задаче именно так. Значит, что такое х и т? х и т от g это единица. Если
и т тройка вершин так образует треугольник в g и 0 иначе. Так, друзья, понимаете, что х и
т это тоже случайная величина. Но она смешная, конечно, она принимает всего два значения, 1 и 0,
но это случайная величина. На каждом графе она равна либо единице, либо нулю. Мы как-то
нумируем все тройки вершин от единицы до c и z, как угодно. И дальше каждую очередную тройку,
как я обещал в цикле в тройном, тестируем на то, что она образовала треугольник, поступившим
на вход графе. Так, друзья, это ровно тот самый тупой алгоритм. Я понятно выразился? Теперь
смотрите. Мат ожиданий х всегда равно сумме мат ожиданий. Вы мне скажете, а эти величины зависимы?
А вы мне скажете, это знаете, что такое зависимо-независимо? Ну, понимаете, если вот такой треугольник
есть в графе, то, наверное, вот такой треугольник есть уже с большей вероятностью, правда? То есть
вот эти величины, вот эти индикаторы, их называют индикаторами, они зависимы. Но линейность верна всегда,
мы тут ничего не говорили про независимость. Всегда верна. Плевать, что она независима. Да,
независимая, но плевать. Это равно ех1, плюс ех3. Следили? Осталось чуть, сейчас будет звонок.
Ему равняется мат ожиданий индикатора. Вот тут полезен второй вариант определения. Мат ожидания.
У индикатора всего два значения. 1 и 0. То есть вот это ех1. Успеваете? А вот это ех2. То есть мат
ожиданий ех2 это единица умножить на вероятность того, что ех2 равно единице. Но с какой вероятностью
ех2 равно единице? С какой вероятностью конкретная тройка точек образует треугольник в случайном
графе? П в кубе, правильно. То есть мы получаем просто C из N по 3 на P в кубе и вся недолга. Вот,
посчитайте дома вот эту вероятность. Поймите, что это катастрофа, а тут мы в одну строчку
и нашли мат ожидания. Вот потренируйтесь, следующий раз продолжим. Все.

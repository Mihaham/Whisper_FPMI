Я его даже не знаю, хотелось бы сказать, что это ободряюще,
представляя сегодняшние политические события.
Не могу сказать много, надеюсь, что все из вас доучатся,
по крайней мере, буквовряд, никого по крайней мере вашу
личную жизнь это не затронет.
Очень сильно это, надеюсь.
Вот, а пока давайте делать вид, что все нормально.
Итак, насколько я знаю, вам на плюсах рассказывали про он-ордер рецепт,
и нам надо понять, что лежит там внутри, поэтому давайте вот сегодняшний лекции
так будет не очень вписываться в общую канву, но сегодня про хэштаблицу поговорим.
Итак, представьте, ну давайте вспомним нашу задачу, когда мы про дерево поиска
как разговаривали, есть у нас какие-то ключи, и мы хотим уметь делать три самые
главные операции.
Давайте напомним, что это были деревья поиска у нас.
Там был insert, erase и find.
Ну find, то есть проверить, лежит ли элемент в множестве в текущем.
Считаем, что дубликатов нет, то есть если какой-то элемент
вставляется два раза, то он, на самом деле, множество
только один раз, то есть не мульти множество, а обычное
множество.
Чтобы дубликатов не было.
Мы решали это всякими там декартовыми деревьями, сплей деревьями
и так далее.
У нас там получалось, что все запросы отвечаются, обрабатываются
за алгорифм.
Там либо амортизированный, либо вероятностный, либо
чистый, как в красно-черном AVL.
А сегодня мы будем решать ту же самую задачу с помощью
хэштаблиц с целью улучшить еще время работ, время работ
у нас будет от единицы.
Время обработки всех этих запросов будет от единицы
в хэштаблице.
Но это от единицы будет, во-первых, вероятностная,
во-вторых, амортизированная.
То есть, во-первых, здесь будет звездочка в том же понимании,
в котором была в прошлом семестре, амортизированный
анализ.
То есть каждая конкретная операция может работать
долго, но в среднем они работают суммарно за линейное время.
Если каждый за единицу, то суммарно все за линейное
время.
А вероятность в том смысле, что только мотождание
времени работы будет таким, то есть мотождание времени
работы с учетом делений на все количество операций
будет единицами на операцию.
Здесь совмещение и амортизационного анализа из сплей дерева
и вероятностный подход из декартового дерева, то
есть только мотождание времени работы будет таким.
Но бывают случаи, когда какие-то операции вываливаются
из этих границ, из золотой единицы, работают подольше.
Но тем не менее.
Тут есть два основных подхода.
Это хэширование таблицы цепочками и с открытой адресацией.
Мы начнем с цепочек.
Идея такая.
Вот смотрите.
Пусть все наши ключи – это элементы какого-то универсума
U.
Все ключи принадлежат U.
То есть те элементы, которые вы вставляете и удаляете
в ваше множество – это обязательно элементы U.
Тогда давайте попытаемся как-нибудь закодировать
все элементы U числами от 0 до M-1.
Давайте я прям множество напишу.
0, 1 и так далее.
M-1.
Мы придумаем какую-нибудь функцию.
Назовем ее H от слова хэш-функция.
А на каждому ключу из U ставится соответствие какое-то
маленькое натуральное число от 0 до M-1.
Ну M небольшое, грубо говоря, 10 в пятую, давайте я напишу
там 10 в шестой примерно, а U может быть большим.
Например, U – это все, что помещается в тип int, до 2
в 32 примерно.
То есть мы всем ключам, всем вот этим товарищам
назначаем какое-нибудь маленькое число.
Понятно, что тогда будет коллизия, в том смысле, что
разным ключам может соответствовать одно и то же хэш-значение.
H от X может быть равно H от Y, понятно дело, это не может
быть инъекцией, поскольку размеры этого множества
сильно меньше размера вот этого множества.
Значит, будет много таких коллизий, когда две стрелочки
ведут в один и тот же элемент.
Но тем не менее, мы вот так будем считать, что чтобы
хранить какое-то множество U, чтобы хранить какое-то
множество ключей в нашей структуре, мы давайте
вместо них будем хранить хэши просто.
Просто вместо элементов U мы будем хранить их хэш-значение.
И тогда, поскольку у меня здесь элементов мало, мы
можем просто завести массив длины 10 в шестой, вот этой
вот M, такой большой массивчик длины M.
И здесь для каждого хэша будем хранить, есть ключ
с таким хэшом или нет, есть ли ключ с таким хэш-значением.
Более формально, у нас будет массив длины M, это и будет
наша хэш-таблица.
И эта ячейка этого массива будет представлять собой
односвязанный список тех элементов, в которых хэш
равен И.
И это элемент массива, односвязанный список ключей с хэшем равным
И.
То есть если нас тогда просят какой-то элемент вставить,
то мы сначала от него считаем хэш, h от x, значит мы понимаем
в какую ячейку за этих вот M этот ключ принадлежит,
в какую ячейку он принадлежит.
Дальше смотрим, то есть мы посчитали h от x, здесь какой-то
односвязанный список.
Ну, односвязанный список, я такого не говорил раньше,
но это вот то, как мы реализовывали стэк.
У меня есть какая-то структурка, у которой есть хранящееся
значение и ссылка на следующее значение.
То есть вот есть там какая-то голова нашего списка и указательно
на следующую вершинку.
Здесь тоже эта вершина, у нее есть значение и указательно
на следующую.
Значение и указательно на следующую.
И у последней вершины указателей будет там в науптр, ну куда-то
в пустоту, означает, что это последний элемент нашего
списка.
В общем, так как мы реализовывали стэк, мы, собственно, можно
даже говорить, что мы тут просто стэк храним, а не односвязанный
список, но только с доступом там вот.
Стэк, у которого есть еще возможность пройти все
элементы.
Ну вот, и тогда, чтобы вставить элемент, мне нужно куда-нибудь
в этот список добавить новый х.
Здесь вообще лежат все ключи с хэшом равным h от х, и
чтобы добавить сюда х, мне нужно куда-то сюда в этот
список этот х новый вставить.
Чтобы там, скажем, удалить ключ, мне нужно пройтись по
вот этой цепочке, найти, где в этой цепочке лежит тот
самый ключ, который надо удалить, и сделать удаление
элемента из списка.
Например, если надо удалить вот этот, вот этот какой-нибудь
y надо удалить.
То есть если h от х равно h от y, они лежат в одном вот этом
вот списке, мне нужно удалить y.
Тогда я перехожусь по этому списку, нахожу y, удаляю
его, ну и, соответственно, перенаправляю стрелки.
Очень легко вот в этом списке удалить произволенный
элемент, надо просто перенаправить там какую-то стрелочку.
Вот это удаление.
Ну и поиск, соответственно, нужно просто всю эту цепочку пройти.
И, значит, наш дальнейший анализ, ну такой, мы не
будем ничего доказывать, скорее всего, он покажет нам,
что вот эти вот длинные цепочки, длины всех цепочек
при должном выборе m и при должном выборе h, все длинные
цепочек будут не очень большими.
Вот.
И это значит, что время работы, время ответа на все запросы
будет маленьким.
Значит, давайте теперь все это конкретизируем.
Как работает у нас find?
Find x, то есть проверить, есть ли элемент x в нашей х-таблице.
Находим, давайте я напишу i равно h от x, проходим по
итому списку, проходим по итому списку в поисках
x.
Ну потому что если x где-то и лежит в нашей х-таблице,
то он обязательно будет в этом итом, итом списке.
Если мы весь список прошли, его там не оказалось, то
его вообще нету в нашей х-таблице.
Иначе вот мы его нашли и локализовали в х-таблице.
Окей?
Insert x, insert x.
Находим опять таких х-ж значения, h от x.
Вот.
И самое простое, это надо будет просто добавить x в
начало списка, в начало вот этого вот списка.
Ну при условии, что его там нет.
То есть давайте проверим сначала.
Проверим, что x нет в итом списке.
Нет в итом списке.
Потому что если он есть, то ничего вставлять не нужно.
Вот.
Иначе вставим его в начало нашего списка.
Иначе вставим в начало.
Ну здесь тоже очень просто реализуется.
Есть у вас такой списочек, представьте, что здесь
x нету.
Там какой-нибудь z, a и b.
А вам нужно новый элемент x сюда добавить.
Тогда вы заводите новую ноду, ну как бы новую структурку,
новый указатель.
Пишете в нем значение x.
И начинаете ссылаться вот этой штукой сюда.
И при этом еще говорите, что теперь вот эта ячейка
начинается не с этого элемента, а вот с этого.
Ну то есть список ваш сдвигается на один элемент вверх,
по сути.
Так.
Вот.
Ну и race тоже просто здесь реализуется.
Нужно просто посчитать х значение, пройтись по
списку соответствующему и вытащить оттуда ненужную
вершинку.
Пройтись по этому списку в поисках x.
Удалить его.
Ну в том смысле, в котором я рисовал, что если вы
нашли какой-то x, он на кого-то ссылается, и кто-то
ссылается на него, тогда нужно x вырезать из картинки
и перенаправить вот этот указатель сюда.
Вот.
Вот такая несложная реализация.
Нам нужно просто M односвязанных списков поддерживать
и все.
Хорошо.
Значит, что я хочу сказать?
Что я хочу сказать?
Что все сейчас у нас есть.
Что все сейчас у нас есть.
Что все сейчас у нас есть.
Что все сейчас у нас есть.
Что все сейчас у нас очень сильно зависит от этих
двух параметров, от h и от m.
Понятно, что функция h, в ней обязательно будут какие-то
коллизии, как я сказал, потому что это множество
сильно больше, чем то, поэтому многие элементы будут
склеиваться.
И понятно, нам хотелось бы, чтобы разным элементам
соответствовали разные х-значения, потому что в
идеале, если у всех ключей отсюда, или по крайней мере
те, которые поступают к нам в запросы, разные х-значения,
то у нас все списки имеют длину максимума единицы.
И тогда вообще все работает за чистую единицу.
Но, конечно, бывают коллизии.
Какие-то цепочки могут быть достаточно длинными.
Это как раз те ключи, у которых хэши совпали.
Потому что каждая цепочка, это множество, по сути,
множество ключей с одинаковым х-значением.
Мы хотим, чтобы этого было не очень много.
Ну и чего?
Тогда надо h как-то поаккуратнее работать.
Нужно говорить, какая у нас функция h.
И понятно, что если h какая-то детерминированная,
вот, не знаю, я там напишу, функция h такая функция.
Не знаю, какая-нибудь функция кермана или еще чего-нибудь.
Что-нибудь я тут написал.
Тогда, если злоумышленник знает, как у вас устроена h,
он может вам посылать такие запросы, что все вот эти товарищи
выстраиваются в одну большую цепочку.
Потому что у них у всех хэш-значения одинаковые.
И тогда вы на все запросы отвечаете за линейное время.
В общем, никакого выигрыша от хэш-таблицы у вас нет.
Поэтому, чтобы такого не было, чтобы наш злоумышленник
не смог нам ничего противопоставить,
мы будем h убирать случайно.
Общая идея.
Функция h случайна.
Ну, поскольку террорвера еще вроде как у нас не было,
давайте я немножечко поясню, что имеется в виду.
Имеется в виду следующее.
Мы рассматриваем какое-то семейство функций h.
Просто какое-то семейство функций.
Семейство функций того же типа.
Из u в маленькие натуральные числа.
Вот есть у нас какое-то семейство функций.
h1, h2 и так далее.
И когда я говорю, что h случайная,
я имею в виду следующее.
Когда наш алгоритм начинает работу,
когда мы делаем его какой-то запуск,
он выбирает случайно и равновероятно
одну из функций вот этого множества.
То есть там вот есть, не знаю, 10 функций.
Тогда он каждую из них возьмет с одной и той же вероятностью одна десятая.
То есть вот бросает, не знаю, там многогранный кубик,
у которого грани столько же, сколько вот этот h.
Соответственно, все грани выпадают равновероятно.
Соответственно, каждая функция выбирает с одной и той же вероятностью.
И пишем, что алгоритм
выбирает
h
случайно
равновероятно
из h. Из h красиво.
И тогда, поскольку наш контрагент не знает,
какую мы h выбираем,
потому что мы ее выбрали и оппоненту не сообщаем,
можно так сказать,
тогда он не сможет никакую последовательность,
то есть он не может предоставить гарантированную какую-то последовательность запросов,
которая нас выродит,
которая сделает очень длинную цепочку,
потому что не знают, какая h, мы ее выбрали случайно, все хорошо.
Ну и, значит, здесь
есть какие-то результаты.
Давайте я их все-таки сформулирую,
чтобы понимать вообще, насколько важно,
насколько важно, какой у нас h.
Значит, определение.
Семейство хэш-функций
h
называется универсальным,
если
для любых различных элементов
для любых различных элементов
нашего универсума
вероятность того, что h от х
равно h от у
не больше одной m-той,
не больше одной m-той.
Значит, мы
вольны фиксировать любые два различных ключа из у
из универсума,
и мы хотим, чтобы вероятность равенства
вот этих вот хэш-взначений
была не очень большим, не большим одной m-той.
И здесь вероятность берется
как раз таки по h.
Давайте подпишем, что вероятность
по случайной h,
потому что все остальное у нас здесь диссерминировано.
х и у мы фиксировали в самом начале,
и случайность здесь вся исключительно по h.
Вот такая странная
конструкция, что
у нас случайная функция,
это не элементы как бы случайные,
а вот сама функция,
которая действует на эти элементы, случайная.
Значит, более
неформально, да, а что это значит
с точки зрения нашей простой вероятности,
вот этой равномерной?
Это значит, что если мы посчитаем,
для скольких функций из h красивого
выполняется равенство,
то их доля во всем h красивом
не больше, чем вот такое.
Давайте напишу, что значит эта штука,
если вас смущает значок вероятности.
Значит, значиство,
ну, мощность множества тех h,
что h от x равно h от y,
не превосходит
m этой доли
всего множество h большое.
Ну, это как раз то самое и значит,
что если вы берете случайную функцию отсюда,
то вот это событие происходит с маленькой вероятностью.
То есть есть все функции h красивые,
а есть плохие функции,
на которых h от x равно h от y.
И вот эта вот облачка
занимает маленький объем по сравнению со всем
пространственным функцией.
Не больше, чем 1m
от всего объема функций.
Вот это универсальное семейство.
Ну и тогда можно сравнивать
пути арему.
Если h
это универсальное семейство
хэш функций,
так, мне не лень писать
семейство хэш функций, вот так напишу,
то
математическое ожидание времени обработки
всех запросов, инсерта, эрейза и файнда,
математическое ожидание
времени
обработки
всех запросов,
есть
O от 1
плюс альфа,
где альфа
это m делить
на мощность универсума,
еще называется load factor,
степень загруженности нашей
таблицы,
наш хэш таблица.
То есть вот, да, мы знаем, что у нас всего,
пардон, извините, не нау,
виноват, глупо сказал, на n,
где n это количество вставленных ключей.
n
количество
вставленных
ключей.
Вот, то есть, смотрите, если у меня
поступило n запросов инсерт,
то понятно, что у меня суммарная длина
вот этих моих цепочек, она n,
да, потому что каждый ключ, ну, то есть,
если не было дубликатов, то у меня n
вершинок суммарно
во всех односвязанных списках.
Вот, если мы поделим
m на n, то это как бы средняя загруженность
одной ячейки.
Ну, точнее, видимо, будет n на m, ну, вот, неважно,
давайте такое обратно, обратно напишу.
Вот.
Понятно, что если, если вот эта штука
слишком большая,
так, сейчас, момент, m на n, да?
Вот, вот, вот.
Вот, да, сейчас
как-то странно, да, что нам хорошо, когда
альфа маленькая. Нет, ну, да, да, да.
То есть, нам, нам хорошо, когда
кто-то меня обманывает, мне кажется.
Извините, я сегодня
волнуюсь, n на m, конечно,
n на m, да. Наоборот,
число вставленных ключей делить на
количество элементов. Так как, как раз, вот это
средняя длина всех цепочек, да, потому что если
суммарная длина n, всего m цепочек,
в смысле m списков, тогда лот фактора
вот такой, средняя длина каждой цепочки
n делить на m.
Ну, вот. И нам как раз
хорошо, да, что если m растет,
то эта штука убывает, время работы как бы
будет меньше. Вот такой результат.
Значит,
что обычно, что обычно делают? Во-первых,
фиксирует альфа
непревосходящим какой-нибудь константы, обычно
меньше единицы. Вот.
Ну, зависит от того, что вы хотите
в разных, в разных реализациях по-разному.
Ну, давайте там считать, что альфа всегда
не больше, чем там 0,95, например.
То есть, в нашей х-таблице
поддерживается вот такое вот соотношение, что
число вставленных ключей,
деленное на общий размер таблицы,
не больше 0,95.
И тогда время работы, ну, от ожидания,
по крайней мере, время работы будет всегда
от единицы.
Но проблема возникает в том,
что нам нужно будет еще вот это вот как-то
научиться поддерживать, потому что если мы
m фиксировали где-то в начале и сказали,
что вот у нас х-таблица такого размера,
то после многих инсертов у меня нарушится,
что лод-фактор не превышает какой-то констант.
Значит, нам нужно будет, на самом деле, m
увеличивать. Ну, и здесь будет очень простая
идея, если у меня была какая-то х-таблица размера m,
и она перегрузилась,
то есть альфа превысила какой-то допустимый
порог, тогда мы просто создаем
таблицу два раза большего размера,
2m,
и делаем, ну, процедуру
rehash.
То есть мы просто выбираем новую хэш-функцию,
если раньше у меня хэш-функция била в
множество 0 и минус 1, то теперь она
будет бить в множество от 0 до 2 и минус 1.
Взяли новую хэш-функцию, и
все элементы вот этой вот старой хэш-таблицы
перекинули в новую, а про старую
забыли.
Вот это очень похоже на то, что мы делали в векторе,
у нас что-то там переполняется,
превышает какой-то порог, тогда давайте
мы просто выделим в себя два раза больше памяти,
все старые элементы перекинем в новую таблицу.
Ровно то же самое происходит здесь, только мы еще
обновляем нашу хэш-функцию, потому что раньше у меня
хэш-функция принимала m значений,
теперь мы хотим, чтобы хэш-функция принимала 2m значений.
Ну и, соответственно,
алгоритм будет такой. Мы поддерживаем
текущее отношение n делить на m.
Как теперь обрабатывать запрос?
Обрабатывать запрос точно так же, как было на этой доске написано,
insert, find и erase.
Но если вдруг альфа, loadFactor
превысила вот эту константу, то мы
временно останавливаемся, перестанем
вычислять на запрос и делаем rehash.
Расширяем в два раза нашу хэш-таблицу,
перекидываем, то есть у нас, в частности,
вот эти все цепочки перегруппируются,
потому что у меня теперь поменялась хэш-функция,
и, скорее всего, те
ключи у которых была раньше одинаковая хэш-функция
старая, они, скорее всего, теперь будут лежать
в разных корзинках, в разных списках
в новой хэш-таблице.
Делаем rehash, все переложили, понятно, что это работает
за n, ну, за n плюс m, на самом деле.
За n плюс m.
Потому что нам нужно выделить память,
переложить n элементов туда, и еще вот эту память
освободить в идеале.
Вот.
Отсюда возникает амортизация.
Отсюда возникает амортизация, потому что
да, средняя, обычная, простая операция,
в антаждании времени работы это вот константа.
Если мы альфа ограничим
каким-то порогом, то время работы будет константа.
Вот.
Но там
в среднем, там сколько получается?
Каждую, первую, вторую, четвертую,
восьмую и так далее, ну, то есть по степеням двойки
операции мы посмотрим,
нам нужно будет делать rehash.
Поэтому возникает амортизация, и опять вот мы можем
так же, как в векторе, размазать вот это вот
время по
более простым операциям, где не нужен rehash.
Значит, давайте
все это напишем.
Выполняем
операции
как обычно.
Но если
альфа превышает порог,
ну, тут какая-то константа, она,
еще раз повторю, не очень существенная,
можно там 0,5 писать, я привык 0,5
писать.
Ну, если мы
на практике что-то более близко к единице,
там 0,75 можно здесь написать,
короче, что-то такое,
то делаем
rehash,
расширяем
таблицу в два раза,
в два раза,
генерируем новую hash функцию,
генерируем новую hash функцию,
новую hash функцию,
перекладываем элементы.
Вот, и здесь тогда как раз получается
та самая симпотика, про которую я говорил
в самом начале, что, во-первых, она
вероятностная, что мы от ожидания константа,
но, с другой стороны, она еще и на каких-то
конкретных запросах может
работать долго,
поэтому нам нужна амортизация.
В итоге амортизированная
от ожидания
есть, ну, я напишу
от единицы в том смысле,
что то альфа, которая там написана,
оно не ограничено таким порогом,
поэтому от ожидания будет тоже ограничено
от единицы. Понятно?
Вот, ну да, это, конечно, без доказательства,
потому что тервера
тервер мы пока трогать не хотим.
Так, ну и все, то есть мы поняли, что
нам достаточно выполнение
какого-то требования на семейство
hash функций,
и давайте
напишем, что нам, например, подойдет.
Пример универсального семейства.
Здесь давайте считать, что
у это zp,
ну или, по крайней мере, вложено в zp,
то есть это
числа от 0 до
поминус 1, где p простое.
Тогда
семейство будет таким у нас.
Это все функции вида
ax плюс b
по модулю m,
где a не нулевое
число из zp,
а b произвольное число из zp.
Вот это будет универсальный
семейство hash функций.
То есть функция очень простая, да,
вот есть у меня, то есть я считаю, что все ключи
у меня, это числа
натуральные,
и у меня есть
числа натуральные,
и что там, p это
какой-то простой, который больше всех
элементов из нашего универсума.
Тогда просто рассмотрим такую линию функцию,
вот это вот имеется в виду,
ну опять-таки это вычисление
происходит в zp, то есть, по сути, я
сначала умножаю два числа, потом прибавляю
b, потом беру остаток, по-моему, для p.
Вот эти вот все вычисления производятся в zp.
В zp более формально я мог бы написать так,
ax плюс b
в процент p, потом еще
в процент m.
Вот сначала вот это вот вычисление
производится по моделю p,
и потом еще по моделю m берется.
По моделю m берется.
Вот.
То есть, по сути, просто линейные функции,
ну афинные функции, ax плюс b,
где старший коэффициент не нулевой.
Оказывается, что это универсальное семейство,
и значит, в общем-то, уже нам
очень простую реализацию там hash таблицы
написать. Мы сначала
ну там определяем p
достаточно большое, то есть, если нам, скажем, гарантируем,
что ключи это числа от 0 до 29,
мы можем сказать, что p это, скажем, 10 и 9
плюс 7 или что-нибудь такое, ну какое-нибудь простое,
побольше чем универсум.
Генирируем случайные a и b
вот с такими вот условиями.
Получаем тем самым случайную функцию из a
красиво, ну и потом работаем вот в соответствии
с тем, что я рассказывал.
Да.
Ну единственное условие там
вот такое на самом деле возникает.
Но
оно не существенное.
То есть, смотрите еще раз, да, что мы делаем.
Что имеется в виду?
Имеется в виду, что ключи, которые мы хотим хранить в hash таблице,
это, скажем, числа от 0 до 10 и 9.
А m
это размер hash таблицы.
Любой, каким мы его хотим, мы его таким и выбираем.
10 и 5, например.
Чтобы большое множество поместилось в каком-то маленьком.
Пусть m это 10 и 5.
Тогда чтобы посчитать hash значение
какого-то конкретного числа,
вы заранее
генерируете вот эти a и b,
неприсходящие p,
то есть, неприсходящие 10 в 9 плюс 6, грубо говоря.
Затем вы для данного
x считаете вот это значение по моделю p
и потом берете его еще раз по моделю m.
И это и будет
та ячейка hash таблицы, в которой
надо этот x положить.
По сути, это независимые сущности.
m это всегда размер hash таблицы,
а p это то, по чему мы делаем вычисления.
Да?
А если у нас x будут одинаковые,
то у нас получают основные члены?
x и x одинаковые или их hash значение одинаковое?
Но если x и x одинаковые,
то мы говорим, что у нас без дубликатов все.
То есть мы храним множество,
а не мультим множество.
Если бы мы хотели хранить мультим множество,
то в пару с каждым x можно было бы хранить слой его копий.
Вот.
Ну вот не будет.
Мы дожидаем, что эта штука универсальная,
а значит, по теореме,
мы дожидаем время работы вот такое.
Это, грубо говоря,
средняя длина всех цепочек,
значит, и коллизий там будет немного.
Вообще говоря, конечно,
у этой штуки,
я могу сказать следующее,
что если вы фиксировали a и b,
ну так, неформально,
это не очень верно,
но вот эта вот штука,
префиксированных a и b,
заметает примерно равномерно
все вот это вот m.
То есть, грубо говоря,
если вы варьируете x от 0 до p-1,
считаете вот это значение по моделю m,
то у вас примерно равномерно все ячейки заполнятся,
и в каждой ячейке будет
что-то типа p делить на m ключей.
Из-за этой равномерности
у нас, как раз, если возьмете
наизвольные n ключи, которые вы вставляете,
у вас степень загруженности
будет вот такая. Это как раз альфа.
Поэтому, да, конечно, коллизии есть,
но из-за универсальности, которую мы не доказываем,
то есть нам важнее разобраться с тем,
как это работает,
из-за универсальности будет такое,
что в среднем коллизий будет немного.
Вот такие дела.
Это хэширование с цепочками.
Ок, теперь давайте
поговорим про совершенное хэширование.
Немножко другая задача,
но тоже с цепочками будет у нас.
Совершенное хэширование.
Тут постановка немножко
не такая, как
была раньше,
а именно здесь нам известно заранее,
с какими ключами нам придется работать.
Постановка такая, значит, нам даны
изначально какие-то ключи,
давайте я их назову A1 и так далее, An,
а потом поступают запросы
и гарантируется, что все запросы работают
только с этими ключами.
Далее
запросы
запросы
запросы
только с этими ключами.
То есть все вот эти вот иксы, которые
insert, erase и refind, они все обязательно
лежат в этом вот маленьком множестве.
То есть вам изначально сообщили,
с чем надо уметь работать,
с какими ключами, то есть не со всем универсумом U,
а с этими. И потом запросы поступают,
где параметры X только в этом множестве всегда.
То есть это в каком-то смысле
похоже на работу оффлайн,
в том плане, что вам как бы
известны все запросы заранее,
или там не все запросы, а по крайней мере все ключи,
с которыми придется работать. Вот вам они откуда-то
известны, и дальше нужно
что-то с ними сделать, как-то
сделать какую-то предобработку, и потом
как можно быстрее отвечать на все запросы.
То есть это как бы оффлайн, что нам известны все вот эти ключи.
На самом деле это вполне реалистичная
установка, потому что, например,
вы организуете какое-то
соревнование, у вас люди
до какого-то момента регистрируются,
после какого-то момента все, регистрация закончена.
Соответственно, вам известны имена всех участников,
и дальше это множество никак меняться не будет.
А дальше во время соревнования происходят какие-то события.
Один человек кого-то обогнал,
сдал задачку, что-то такое.
Соответственно, множество ключей у вас всегда фиксировано,
а события выступают только с ними,
и это в принципе что-то реалистичное.
Вот, и здесь
давайте я
пропишу еще раз,
какие у нас есть запросы.
x это всегда аи для
кода i,
для кода i.
Ну и тут понятно, как можно было бы решать
за алгорифом. Можно было бы
посредствовать этот массив,
построить дальше там массив
0 единиц вот этого размера,
и x искать бы им поиском в этом массиве,
соответственно, понимать, какой у него номер
в ацертуральном порядке.
Ну, я не буду это записывать, потому что все равно
бесполезно. Но, короче, за алгорифом можно
легко делать. Сортировка,
потом bin поиском определяем номер в этом ацертурном порядке,
а значит знаем, как бы, ну то есть
для каждого числа храним там 0 и ледничку,
есть он в массиве или нет, есть он множество или нет.
И дальше по bin поискам понимаем, какой тут
хранится бит.
Вот, но это все равно алгорифом, да, и особо выигрыша
по сравнению с деревом поиска даже нет.
А мы сделаем следующее,
мы за линейный
предпочет,
за o от n,
построим такую структуру, которая потом на все запросы
отвечает за единицу, прям за чистую единицу,
без вероятностей
и без
амортизации.
Построим
за o от n
в среднем
такую структуру,
которая будет отвечать на все запросы за чистую единицу.
Чистое значит невероятностное,
не амортизированное.
Идея следующая, мы проведем
двухуровневое хэширование.
Во-первых, мы заведем
хэштаблицу,
у которой m равно n.
В ней будет ячеек столько же, сколько ключей
вообще в нашем универсуме.
Ну вот, то самое a1, так далее, an.
И пусть здесь какая-то, здесь будет h,
не знаю, hout,
внешняя хэш-функция.
Сначала все наши ключи
распихаются как-то по этой хэштаблице
в какие-то цепочки.
То есть давайте сделаем вид, что вот эти h
изначально добавляются в эту хэштаблицу.
Тогда понятно, что
время работы у нас будет тем
лучше, чем короче самые длинные
из этих цепочек. Если нам вдруг
удастся так подгадать вот этот hout,
что вот эти все
цепочки будут короткие,
то это будет очень хорошо.
Тогда потом мы на все запросы
будем отвечать за длину
цепочки.
Вот, ну хорошо.
Сейчас продолжаем.
Пусть
hout
фиксирована,
а, скажем, cit
это количество
ключей, которые попали в эту
ячейку.
Более формально это мощность множества тех g,
что hout
от ajt
равно i.
Количество тех ключей из исходного
множества, у которых h значение
по действиям этой функции в точности i,
которые попали в эту цепочку.
Тогда первый шаг будет такой.
hout
нужно выбрать так,
нужно выбрать так,
чтобы
сумма квадратов
длины всех цепочек не превосходила
4n.
Такая пока странная магическая формула,
давайте пока не будем задумываться, что это значит.
Значит, сумма квадратов
всех цепочек не превосходит
4n.
И это мы будем делать следующим тупым
образом. Повторюсь, у меня
фиксированы какие-то n ключи.
Я хочу найти какую-то
hout, какую-то первую хэш-функцию,
что после ее применения
мои ключи разбиваются вот в эти вот ячейки,
в эти односвязанные списки,
я хочу, чтобы сумма квадратов размеров
всех списков была не больше чем 4n.
А алгоритм поиска такой hout
будет очень простой. Давайте случайно
hout генерить до тех пор, пока
не будет выполняться это неравенство.
Ну, в общем, просто генерируем
hout
снова и снова,
пока неравенство не выполнится.
Пока неравенство
не выполнится.
Вот. И оказывается, что
вот это вот событие
происходит с вероятностью хотя бы
одна вторая. Если наше семейство
функций h универсально,
если наше семейство функций
h универсально, то есть вот те самые
наши ax плюс b по модулю m подойдет,
тогда вот это событие происходит
с вероятностью, по крайней мере, одна вторая.
Если вы берете случайную функцию hout,
то в половине случаев,
хотя бы в половине случаев, это выполнится.
Значит, количество раз, которые вам придется
перегенерировать эту самую h,
оно маленькое. Ну, то есть, представьте,
вы подбрасываете монетку и дожидаетесь
первого выпадения орла. Понятно, что
там решка, ну, там несколько
первых раз выпадет, но потом
довольно скоро выпадет орел.
Более того, математическое ожидание
числа, в общем, этих перебрасываний
будет ровно два.
Это несложно установить. Да, то есть, еще раз, у вас есть
монетка, у вас есть хорошие события,
которые происходят с вероятностью хотя бы одна вторая,
и есть плохое. И вы ее много раз
подбрасываете, дожидаясь
первого хорошего события. То есть,
это то же самое, что бросать о монетку, дожидаетесь
первого орла. Ну, понятно, что там
первый орел очень-очень,
короче, рано выпадет.
Его сильно долго ждать
не придется. Вот.
Давайте я напишу, что это происходит
происходит с вероятностью
с вероятностью хотя бы
одна вторая.
Вот. Окей.
Получается, первый шаг сделан.
Теперь у меня сумма квадратов всех длин,
всех цепочек, не больше чем 4n.
А дальше делаем следующее.
В каждом, в каждой ячейке,
в каждой ячейке
нашего вот этого внешнего массива
после разбивания по hout,
мы построим еще одну хэш-функцию.
Для каждого i
построим
свою хэш-таблицу
размера как раз
такие c в квадрате.
Размера c и t в квадрате.
Ну и поскольку мы гарантировали,
что сумма квадратов размеров не больше,
чем линия от n,
то и сумма длин вот этих хэш-таблиц
будет тоже не очень большая.
Значит, смотрите, давайте я, чтобы было понятнее,
какой-нибудь пример, наверное, нарисую.
Пусть у меня n равно
Так, сейчас. Что будет содержать?
Ну давайте скажем
6.
m равно тоже получается 6.
Соответственно, у меня есть 6 ключей.
Раз, два, три, четыре, пять,
шесть.
Есть 6 ключей,
6 ячеек
во внешней хэш-таблице,
и мы подбираем функцию hout так,
чтобы сумма квадратов длин всех ячеек
была не больше, чем 24.
Например, может быть что-нибудь такое.
Вот здесь 4 элемента, а здесь 2.
Такой нам подойдет.
То есть, если наша hout
на каких-то
четырех элементах возвращает вот это значение,
а на каких-то двух еще вот это,
то такая hout нам подойдет.
Например, вот эта вот hout сгодится.
Такая hout
сгодится.
Ну потому что здесь квадрат этого размера
это 16, здесь квадрат этого размера 4,
сумма 20 не превосходит 4n.
Это окей.
Ну смысл как раз таки такой, что
длинных ячеек здесь не будет.
Раз сумма квадратов у меня ограничена 4n,
то в частности каждый квадрат ограничен корнем,
в частности все вот эти штуки не больше, чем корень.
Это уже в принципе не очень плохо.
Дальше я делаю следующее. Смотрите.
Что это значит? Что значит hout?
Что для каких-то четырех ключей
у меня одна и та же hout.
То есть, они по действиям первой функции
попали бы в одну ячейку.
Тогда давайте сделаем следующее.
Вот здесь заведем хэштаблицу на 16 элементов,
размера 16.
И подберем еще внутри такую внутреннюю функцию h,
которая бы эти четыре элемента
раскидывала по этой хэштаблице
внутренней, без коллизий.
Я завожу как раз для каждого i,
строю свою хэштаблицу размера квадратичного,
то есть здесь будет таблица размера 16,
и подбираю в ней внутреннюю
хэштаблицу h, которая бы их раскидывала
без коллизий.
Далее
подбираю
hi
так,
чтобы
она
не давала коллизий.
То есть, по сути, у меня будет
здесь содержать будет две хэш-функции,
то есть здесь я определяю h0, не важно какой h1,
вот у меня важна будет h2 и h4.
Мне нужна такая функция h2,
которая действует
в 16-элементное множество
с одной стороны, с другой стороны
вот эти четыре элемента
отображают без коллизий.
Отображают без коллизий.
Ну и здесь то же самое, эта штука должна действовать
в множестве размера 4 и так, чтобы вот эти
два элемента отобразились в разные множества.
И опять утверждается, что
за счет вот этого квадратичного
запаса, то есть, по сути, что мне нужно?
Мне нужно k элементов,
вот скажем, вот эти k элементов
разместить в хэштаб лист у размера k квадрат.
Ну и тогда как бы интуитивно понятно,
что случайная хэш-функция подойдет.
То есть, если у вас есть 4 элемента,
а у вас есть 16 корзинок,
то понятно, что случайное отображение
их раскидает без коллизий.
Ну, неформально, но тем не менее.
Теорема опять-таки без заказательства.
Если
х1 и так далее,
х1 и так далее, хк
различные ключи,
m равно k квадрат,
а h это универсальное семейство
хэш-функций,
то опять вероятность того, что
у вас не произошло коллизий,
то есть, все h от x различны.
Давайте от xj напишу, различны.
Хотя бы одна вторая.
Вероятность того, что
после выбора случайной хэш-функции,
то здесь опять вероятность идет по выбору h,
по выбору случайной h,
вероятность того,
что у вас не произойдет коллизий,
то есть, хэш значения для всех х будут различны,
она хотя бы одна вторая.
Поэтому здесь работает тот же самый трюк.
То есть, это второй шаг для каждого,
и мы много раз пытаемся
подобрать свою функцию h так,
чтобы она раскидала все ключи,
попавшие в эту ячейку без коллизий.
Второй шаг.
Для каждого i
от 0 до n-1
подбираем
h и t
так, чтобы не было коллизий.
Ну и за счет того,
что случайная функция подходит вероятностью
одна вторая, опять тот же самый аргумент
с монетками,
хотя бы половина хэш-функций хорошая,
значит, если вы просто будете брать случайную
и перегенерировать, если она плохая,
то математическое ожидание числа
попыток будет константным.
То есть, вы взяли какой-то h,
попытались его примерить на роль h и t,
не подошло, пытайтесь новое,
не подошло, пытайтесь новое, не подошло, пытайтесь новое и так далее.
Очень скоро подойдет.
Поскольку каждое событие
происходит с вероятностью одна вторая,
вы хотите дождаться его выпадения,
то очень скоро оно произойдет.
То же самое дождание, когда орел выпадет.
Ну и все.
Тогда алгоритм суммарно будет такой.
Мы сначала строим такую структуру,
что мы сначала находим hout,
потом здесь в каждой штуке строим квадратичного размера хэштаблицу,
то есть тут не 4 будет аж 16,
а здесь будет 4.
Здесь 4, тут 16.
Вот есть такая
доумерная хэштаблица.
Значит сначала, если у меня есть какой-то x,
я сначала считаю его hout,
понимаю, в какой
внутренней хэштаблице он лежит,
скажем, в какой-то там it.
И затем, если я понимаю, что он лежит в этой хэштаблице,
то я понимаю, что нужно на него навесить еще hita,
чтобы понять, где он лежит в этой хэштаблице.
Такое двухступенчатое хэширование.
Сначала понимаем,
какая хэш функция нам нужна,
из вот этих вот n, то есть какой маленькой хэштаблице он лежит,
а потом навесим ту хэш функцию,
которая ему соответствует.
И как раз таки получается,
что время работного запроса будет...
Да, давайте.
Нет, смотрите еще раз, здесь множество ключей фиксировано.
У меня всегда n ключей,
поэтому никакого rehash не происходит.
Не понял.
А тут некуда улучшать,
у нас время работы единицы на запрос.
Лучше некуда.
Ну можно, но не понятно зачем.
Да, значит, еще раз.
Как тогда...
За сколько мы отвечаем на каждый запрос?
Скажем, insert x.
Сначала мы считаем hashout от x,
скажем, он попадает куда-нибудь сюда.
Тогда мы понимаем, что чтобы понять его,
где он лежит в этой хэштаблице,
нужно взять от него h4 от x,
где h4 у нас где-то сохранена,
просто ее применяем и понимаем,
что он лежит в этой хэштаблице.
Все. И поскольку нет коллизий,
тут можно считать, что это опять хэштаблица с цепочками,
но длина каждой цепочки максимум единица,
поскольку нет коллизий.
Никакая цепочка не будет иметь длину больше одного.
Значит, мы однозначно понимаем,
где он лежит,
и все, мы получаем ответственный запрос за единицу.
То есть либо говорим, что этот элемент добавился,
либо удалился,
либо надо проверить, что он там есть или нет.
Время работы ответа,
точнее время обработки каждого запроса,
чисто единица, а предпочет у меня
линейный, ну и по времени и по памяти.
Потому что мы поняли, что
количество перегенерации будет константа,
поэтому суммарно первый и второй шаг работает
за линию в среднем, за отn.
Ну и память тоже, понятное дело, линейна,
потому что мы вот эту вот штуку поддерживаем.
Сумма квадратов размеров,
это как раз таки вся память нужная
на нашей х-таблице,
она линейна. Значит, время и память линейны,
все хорошо, и ответный запрос за единицу.
Понятно?
Окей.
Давайте еще один пример приведу, зачем это может быть нужно.
Например, у нас есть
граф какой-то,
то есть какие-то вершины и ребра между ними,
и нам граф заранее известен.
И нам поступают запросы,
вида, проверить, есть ли какой-то
элемент, точнее, есть ли ребро между двумя вершинами.
То есть вот есть граф фиксированный,
в нем ребра не добавляются, не удаляются,
нам известен граф,
и поступают запросы, проверить наличие
кого-то ребра, есть ли ребро в графе или нет.
Тогда вот как раз таки вот эта
конструкция идеально ложится,
что у нас есть фиксированное множество ключей,
мы сначала можем построить
должного вида х-таблицу,
вот такую вложенную по всем ребрам,
вставить все элементы в эту х-таблицу,
все будет линейное пока что.
А дальше, чтобы проверить наличие ребра,
мне нужно два раза прочитать х-функцию
и понять, есть этот элемент в х-таблице или нет.
Понятно?
Так, хорошо.
Хорошо, хорошо, хорошо.
Что дальше?
Да, дальше про открытую адресацию будем.
Так.
Да, значит, следующий сюжет
это х-таблица с открытой адресацией.
Вот, значит, здесь мы отказываемся
от цепочек, мы отказываемся
от адрес разных списков,
и наша х-таблица будет
просто представлять собой массив длины m
без всяких там внутренних структур,
просто массив чисел.
Массив чисел.
Тогда смотрите,
как работают всякие инсерты.
Например, чтобы вставить какой-то х,
ну, я как всегда генерирую какую-то случайную
функцию h, это у нас будет всегда одинаково,
генерируем какой-то h случайно-равноверательно
и сфиксированно множество функций.
Чтобы вставить х, я считаю h от x,
то есть мы получаем какую-то ячейку
в нашем массиве.
Если она пустая, если тут никого нет,
то я туда могу смело положить х.
Если, скажем,
мне потом поступает запрос insert y,
и так оказалось, что h от x равно h от y,
то мне, ну, как бы вот, как раз
произошла коллизия,
мне, по сути, нужно вставить y сюда же,
но вместо того, чтобы создавать здесь цепочку
вот такую вот вниз, не x,
к нему подвешено y и так далее,
а я лучше просто y возьму и положу в соседнюю клетку.
То есть вот сюда.
Не важно, что здесь, на самом деле,
по сути, другие...
что здесь другое hash значение,
просто вот я его положу в соседнюю.
Ну, если там было свободно, конечно.
Если там приходит опять z, у которого
hash функция опять такая же,
то я вновь встаю сюда, понимаю, что здесь
занято, иду направо, понимаю, что здесь занято, иду направо
и так далее. Короче, я прохожу до тех пор,
пока не увижу первую свободную клетку, и туда ее вставляю.
Вот такое очень тупое хеширование.
Вместо того, чтобы создавать какую-то нормальную структуру
внутри каждой ячейки,
я просто буду идти вправо и класть в первую свободную.
Вот такой будет инсерт.
Соответственно, find работает
очень просто.
Мы считаем hash,
встаем сюда, ну, понимаем,
что здесь может лежать не x, а просто
тот, у кого hash значение такое же,
поэтому просто пойдем направо
до первой свободной клетки и проверим,
что никто из них не x, ну, или, соответственно, есть ли там x.
То есть теперь,
поскольку у меня
здесь не обязательно
лежит x, то мне придется идти вправо до тех пор,
пока я не найду первую свободную клетку,
где никого не лежит, и мне нужно проверить,
есть там x или нет.
И наконец, erase тут самое тонкое,
что если мне нужно сделать erase y вот этого вот,
то я опять считаю от него hash,
иду направо, нахожу, где он лежит в таблице,
и я не могу просто эту ячейку
освободить, потому что если я просто скажу,
что здесь никого нет, то я потом z не найду,
потому что чтобы найти z, мне нужно от x пройти
до z, и чтобы все эти клетки были заняты.
То есть я вот так сделать не смогу.
Поэтому я просто скажу, что этот элемент
мертвый, то есть я положу на него
отгробный камень, что называется tombstone,
скажу, что он мертвый, его в нашей
hash таблице нет, то есть число там лежит,
но оно нужно только для того, чтобы
вот этот вот там проход в инсерте
или в файнде понимал, что нужно идти дальше,
что это не свободная клетка, нужно идти дальше.
Вот и все.
Давайте это тоже как-нибудь напишем.
Ну и надо не забывать, что нужно делать иногда
rehash'ы, если load factor
превысил какую-то константу,
то понятное дело,
что нужно m увеличивать и
перекладывать все элементы.
Итак,
давайте insert
insert x.
Считаем
так, я вот не хочу здесь
и писать, давайте напишу
j
равно h от x.
Встаем в j-ту ячейку,
идем направо в поисках первой свободной.
Значит, идем
направо,
то есть по сути мы перебираем клетки
j, j plus 1, j plus 2
и так далее.
В поисках первой свободной
клетки.
То есть там, где никакой элемент не лежит.
Ну свободной или той,
которая помечена tombstone.
Или клетки
с tombstone.
То есть tombstone
это тот элемент, который был удален,
значит, на место него можно положить новый,
соответственно, tombstone надо снять.
То есть ячейку
ячейку ячейку ячейку
ячейку ячейку
ячейку ячейку
ячейку ячейку
соответственно, tombstone надо снять.
Тогда
его надо снять.
Понятно?
Ну, с find
все аналогично.
Считаем hash функцию.
И просто идем
слева направо, опять-таки, по вот этим
вот ячейкам в поисках
первой свободной клетки.
Идем
по g, g плюс 1,
и так далее,
в поисках x
или первой свободной.
Значит, тогда понятно, что мы обязательно,
если x был в таблице, мы его найдем,
потому что мы начинаем вот с этой клетки,
и если x когда-то добавлялся,
то он обязательно лежит вот в одной из этих клеток
до первой свободной, потому что
первая свободная
могла быть только с самого начала,
то есть мы за нее раньше не проходили,
значит, теперь нет смысла идти дальше
первой свободной.
Так, ну и raise,
я уже говорил, что просто находим и кладем tombstone.
Не удаляем явным образом из таблицы,
а просто помечаем, что этот элемент
сейчас неживой,
и его не надо учитывать
в ответе
на запрос find.
Значит, то же самое
только кладем tombstone.
Вот такая штука.
Ну и да, не забываем про
проли хэш.
То есть если у нас
load factor слишком большой,
но здесь load factor, кажется, близко к единице
брать даже плохо, потому что если он близко к единице,
тогда у нас довольно большие
все вот эти вот сплошные блоки будут,
но здесь как раз таки альфа выгодно брать
что-то в районе 1 и 2.
Если альфа превышает 1 и 2,
то нужно создать новую хэш таблицу,
новую хэш функцию и переложить все старые ключи
в новую хэш таблицу.
Так, окей.
Это вот открытая адресация.
Тут тоже есть теоремы,
говорящие о том, насколько
это все эффективно, насколько это быстро.
Но для этого
нужно еще вести одно определение.
Определение семейства
хэш функций,
аж красивая,
называется k независимым,
если
для любых k различных
х,
для любых k различных ключей,
различные ключи
случайной величины
h от x1,
h от x2,
и так далее,
h от xk
независимы в совокупности.
Вот это формальное
определение из Тервера.
На пальцах это означает следующее,
что даже если у вас фиксированы значения
всех предыдущих хэш функций,
то есть h от x1,
и так далее, h от xk-1,
тогда про вот этого значения
вы ничего сказать не можете.
То есть информация
о значениях h
в каких-то других функциях
или в других функциях
или в других функциях,
о значениях h в каких-то других точках
в какой-то k-1 точке,
не дает никакой информации
о том, чему равно значение h в точке xk.
Если, например,
мы знаем, что h от xk
равномерно распределена в множестве 0,
и так далее, минус 1,
то есть, по сути, вот это значение
равновероятно принимает все возможные ячейки
из хэштаблицы,
то, по сути, это означает, что
вне зависимости от того,
что вы знаете, куда попали,
то про этот вы ничего предсказать не можете.
То есть, не зная h, вы не можете предсказать,
чему равно значение вот этого штуки.
Значит, такая независимость.
Так, и вот тут есть результаты
про то, что у нас
было про открытую адресацию,
про линейное пробирование.
То есть, теоремы для
линейного
пробирования.
Это вот то самое, когда мы встали
в точку g, то есть, посчитали х-функцию,
встали в точку g, потом пытаемся положить
в g плюс 1, g плюс 2, и так далее, и так далее.
То есть, просто идем слева направо.
Давайте напишу так, что
run it
равно h от x
плюс i процент m.
То есть,
у меня есть какое-то х значение
х, и чтобы попытаться
его положить в х таблицу, или чтобы найти, где он там
лежит, я сначала смотрю в клетку
h от x, потом в h тк плюс 1, h тк плюс 2,
и так далее, и так далее. По модуле
значит, что если я дошел до конца, то я начинаю сначала.
Мне же нужно его куда-то положить. То есть, если дошел
до конца, то я начинаю опять с нулевых элементов.
Ну а run это значит,
что я бегу слева направо в поисках вот как раз первой
свободной клетки.
Итак, если
семейство
h
является 5 независимым,
то
амортизированное
математическое ожидание
времени обработки каждого запроса
есть от 1.
Звездочку
тоже дорисую, чтобы
подчеркнуть, что амортизированное.
Вот, и здесь
уже нам нужно более сильные условия.
То есть, в цепочках нам было достаточно
универсальности, а здесь нужна 5 независимость.
То есть, скорее всего,
на практике вам достаточно
и универсальности, чтобы у вас
было достаточно быстро.
Но теоретически умеют доказывать только,
что если оно 5 независимое, то получается
единица на запрос.
То есть, скорее всего, на практике вам
подойдет почти любое семейство,
ну, такое адекватное, как вот было
с a и x плюс b.
Но, еще раз, с точки зрения теории,
вот здесь умеют доказывать только, что
если оно 5 независимое.
Так, что-то еще
хотел сказать.
5 независимое.
А, да, как
такие семейства можно, например, строить?
Ну, смотрите, вот это вот то, что у нас было
a и x плюс b, оно на самом деле 2 независимое.
А x плюс b по модулю m,
оно на самом деле было
2 независимое.
А если мы возьмем вот здесь вот не ленинный многочлен,
а многочлен на четвертой степени от x,
то у нас получится уже 5 независимое
семейство. То есть, если мы будем
считать не ленинную функцию,
а функцию четвертой степени,
вот такую вот,
так, c d e
по модулю m,
то это уже будет 5 независимое
семейство.
Ну и интуитивно это означает следующее,
что, если вы знаете значение
многочлена в четвертой степени
в четырех точках,
то про значения его в пятой точке
вы ничего сказать не можете, потому что
четвертая степень определяется только
по 5 точкам.
Зная значение этого многочлена в четырех точках
про значение в пятой точке вы не можете сказать
ровным счетом ничего, потому что
многочлен однозначно определяется
только по 5 точкам.
Нагочлен в четвертой степени
однозначно определяется 5 точками.
Поэтому, в принципе, это не бог весть какое ограничение, можно, в принципе, если вы хотите быть уверенными, что у вас все работает за единицу, то можно вместо такой хеш-функции использовать такую.
Да, тут чуть дольше считать, но не сильно больше, не сильно это страшнее.
Вот это пролинейное пробирование, когда мы просто идем слева-направо с шагом 1.
Есть еще другие пробирования, значит, есть квадратичные пробирования.
Ну, еще раз, это качественное объяснение, понятно, что там нужно повозиться.
Да, то есть формально нам надо было бы не брать по моделю M, тогда бы все однозначно задавалось.
Но еще раз, это такое объяснение на пальцах, почему это семейство 5 независимое.
Не претендую на то, что это полная доказательств, но вот идея такая, квадратичная пробирование.
Ну, здесь формула для рана такая, h от x плюс i квадрат по моделю M.
То есть если раньше мы шли точка, следующая, после следующей и так далее, просто слева-направо с шагом 1, то здесь мы будем делать так.
Сначала мы пытаемся положить x в точку h от x, потом h от x плюс 1, потом h от x плюс 4, потом плюс 9, ну и так далее.
Почему это, не знаю, хоть сколько это осмысленно, зачем так делать?
Ну, например, потому что если у вас есть какие-то два ключа с похожими хэж значениями, например, у вас есть ключ, у которого хэж вот такой, есть ключ, у которого хэж вот такой,
то мы, наверное, хотим, чтобы их вот эти раны были не совпадающими.
Потому что раньше, когда у меня было линейное пробирование, и было два элемента с соседними хэшами, h от x и h от x плюс 1, то у них раны одинаковые.
Мы встаем и просто идем слева-направо, пока не встретим первую свободную клетку.
И в смысле цепочек, это как бы означает, что у меня две цепочки склеились в одну, x и x плюс 1, по сути они склеились в один длинный ряд до первой свободной клетки.
А здесь у меня как бы вот эта вот цепочка x и вот эта цепочка, они на самом деле разделяют только один общий элемент,
потому что здесь прыжки будут h от x плюс 2, h от x плюс 5, то есть вот эти вот, получается, ну и так далее.
То есть они пересекаться не будут.
И есть еще более хорошее, есть двойное хэширование.
Это когда у вас есть две хэш-функции, h1 и h2.
И вы говорите, что run it, это h1 от x плюс i на h2 от x.
Соответственно, прыжки у вас все одинаковые длины, но длина этого прыжка зависит от значения хэш-функций в x, точнее равна просто h2 от x.
Просто равна h2 от x.
Вот, но это опять то же самое, что если у вас два товарища попали в соседние ячейки, то есть у них хэш значения по первой функции отличается на единицу,
то скорее всего у них h2 разные, ну там сильно разные, тоже неформально, но грубо говоря, если у них близкие h1, то у них скорее всего различные h2.
И поэтому у них вот эти скачки будут не пересекаться.
И для вот этого двойного хэширования на самом деле достаточно два независимости, тогда результат будет точно такой же.
Теорема опять без доказательства, что для двойного хэширования достаточно два независимости.
Тогда будет опять-таки выполняться, что амортизированным от ожиданий это единица.
Для квадратичного я не знаю результатов.
Вот, то есть лучше всего, то есть тут понятно нужно какое-то более сильное условие, а здесь подойдет наше на самом деле просто ax плюс b.
Поэтому лучше всего, если вы хотите написать открытую адресацию, делать следующее, завести семейство хэш-функций, скажем вот такое, вот такое нам подойдет,
и делать двойное хэширование, то есть для того, чтобы положить x в нашу таблицу, мы сначала считаем h1 от x, потом прыгаем с шагом длины h2 от x в поисках первой свободной клетки и туда его вставляем.
Вопросы?
Хорошо, ну и тогда последний обзорный кусок.
Последний обзорный кусок это фильтр Блума.
Фильтр Блума.
Тут опять все то же самое, что и в обычных хэш-таблице нам нужно вставлять. Нет, сейчас, момент. Да, тут не будет удаления, тут будет только вставка и проверка наличия, insert и find.
Вот, но мы хотим работать супер быстро, но при этом возможно иногда ошибаться. Позволим себе неправильно отвечать на запросы типа find.
Find x.
Вот это в принципе не очень страшно.
По следующей причине. Что мы им конкретно разрешим себе? Мы разрешим себе так называемое false positive.
То есть когда мы ложно возвращаем, что элемент есть в таблице, его на самом деле нет, а мы ложно сообщаем, что он есть.
False, то есть говорим ложь, при этом говорим, что ответ да, что x есть в таблице, false positive.
Вот, и разрешаем мы это себе с некоторой небольшой вероятностью epsilon.
Формально мы хотим от структуры следующее, чтобы мы умели туда вставлять и проверять наличие со следующими условиями.
Что если x в таблице, то мы обязательно говорим да, мы для x-ов, лежащих в таблице, всегда говорим правильный ответ, а для x-ов не в таблице, не лежащих в множестве, мы можем ошибиться, то есть выдать неправильный ответ, что он там есть, хотя на самом деле его нет.
То есть мы можем выдать false positive с некоторой маленькой вероятностью epsilon, ну там, один процент или какие-то десятые доли процента, например.
Тогда работает следующая интересная структура.
Работает такая структура.
Значит, давайте мы выберем k hash функций и заведем один массив длины m из нулей единиц.
Массив bit.
Массив bit.
Тогда чтобы вставить какой-то элемент, insert x, чтобы сделать, давайте мы посчитаем его hash значения от всех k функций и поставим единицы во все вот эти вот коклетки.
То есть вот я взял x, посчитал от него h1 от x, поставил сюда единицу, изначально считаю, что все нули.
Дальше посчитал h2 от x, поставил сюда единицу, посчитал h3 от x, поставил сюда единицу и так далее.
То есть для того, чтобы вставить x, я прохожусь по всем моим k функциям и ставлю единички в теме 100, которые они посчитали.
Ставлю единички в теме 100, ну в общем, в значениях hash функций.
Вот тогда это работает, ну, понятное дело, за от k, за количество хеш функций.
Дальше, чтобы проверить наличие x, давайте я просто опять посчитаю от него все моих к hash функций и проверю, что здесь везде стоит единица.
Проверю, что здесь везде стоит единица.
за от k, за количество хэш-функций. Дальше, чтобы проверить наличие х, давайте я просто
опять посчитаю от него все мои хэш-функции и проверю, что здесь везде стоит единица.
Понятно, что если х лежит в таблице, то обязательно на этих местах единица, а если он не
лежит в таблице, то, возможно, мы себя обманули. То есть вполне может быть такое, что там какие-то
единицы пришли от y, какая-то единица пришла от z, и тем не менее вот эти все три единицы
выставлены, и мы как бы считаем, что х тоже есть. Это, понятное дело, ошибка, но такое будет
происходить редко. Если наши хэш-функции случайны, m достаточно велико по сравнению с числом ключей,
то такое будет происходить редко. Идея такая, что давайте я пропишу, как мы делаем все операции,
insert x. По всем i от одного дока мы выполняем просто, давайте a от h i t от x равно единице.
Мы выставляем bit с номером h i t от x единицей.
Find работает так. Мы проходим опять-таки по всем этим ячейкам и проверяем, что они все единицы.
Давайте я напишу вот так ans равно true for i от одного до k ans ant равно a h i t от x return ans.
То есть я пробегаюсь по всем k ячейкам по значениям х от x, проверяю, что они все единицы,
соответственно, если они все единицы, то вернется единица, если хоть кто-то ноль, то вернется ноль.
Вот, и действительно получается, что если х лежит в таблице, лежит в фильтре, давайте напишу,
если х лежит в фильтре, то find точно вернет true, то find точно вернет true, иначе с неплохой вероятностью вернет false.
Иначе, давайте напишу наоборот, ошибется с маленькой вероятностью.
Я не лезу в детали того, почему она маленькая, почему она равна, но на пальцах вот действительно,
что если у нас много, ну там не много, а скорее m достаточно велико, мы случайные точки разбрасываем эти единицы,
то вряд ли уж для кого-то х, которого мы не добавляли, все вот эти вот k значения будут истинными.
Это очень неформально, но как бы качественно, качественно вот так.
Так, хорошо, ну и вот давайте, да, еще раз, ну вот так себе он поддерживается, да, я поэтому его и не дал,
потому что, не, ну вообще пересечения, конечно, бывают, то есть скажем, значения х функции от х,
а значения х функции от у могут вполне себе пересекаться, если мы вот это затрем, то мы в частности удалим у,
а это, ну в том смысле, что по крайней мере у нас будет тогда и false негатив, что возможно иногда мы,
если мы сотрем вот эти единицы, то мы для у тоже ошибемся.
Ну вот, вот в этой структуре вот именно такая постановка, мы ничего не удаляем, мы только, иногда можем выдавать false позитив, все.
Сейчас я приведу примеры, зачем это может быть нужно.
Итак, что я хотел сказать, а, я хотел написать формулы, какие оптимально здесь выбирать.
Так, тут я спешу, это тоже заучивать не надо, просто чтобы понимать порядок вот этих величин.
Если вставляется н ключи, то оптимально, а, и мы хотим, мы еще фиксируем false позитив rate,
а ошибаться можно с вероятностью небольшим эпсилон, вот так.
То есть мы фиксировали число ключей и фиксировали вот этот процент погрешности, с какой вероятностью можем ошибаться.
То, что оптимально выбрать.
Вот так.
То есть M пропорционально числу ставленных ключей, но еще растет по мере того, как убывает эпсилон.
Понятно, что чем меньше эпсилон, тем больше нужна нам точность, соответственно, тем возрастает 1 делит на эпсилон.
Поэтому растет логарифм.
Чем меньше эпсилон, тем больше вот этот логарифм 1 делит на эпсилон.
Ну и с K то же самое, что чем больше нужна точность, тем больше нам нужно выбрать х-функции.
То есть логарифм 2 здесь, здесь логарифм 2 в квадрате.
Тогда получается, что время ответа на все запросы это от K.
Время ответа на запрос это всегда есть от K, потому что, ну просто смотрим на вот эти два кусочка кода,
как х-функции считаем, тут выставляем единицы, там берем and к значениям.
Это от K, но с точки зрения эпсилона это логарифм 1 делит на эпсилон.
Вот.
Такая вот структура. Зачем она может быть нужна?
Зачем она может быть нужна?
Ну, например, чтобы создавать там какие-нибудь базы, спам почт или, наоборот, вайт-листы почт.
То есть вы сидите там на какой-нибудь почте, который скоро заблокирует,
и хотите фильтровать сообщения со спам-ящиков.
Тогда, смотрите, вы хотите, чтобы, например...
Ну да, например, вы хотите создать фильтр спам-ящиков, спам-адресов,
и тогда уметь туда вставлять и запрашивать find.
То есть вам какое-то пришло письмо, вы хотите понять, оно спам или нет.
Тогда вы берете адрес этого письма отправителя, задаете запрос к фильтру.
Если он говорит да, то скорее всего это спам, если нет, то, скорее всего, нормальное письмо.
То есть если нет, то, точнее, точно нормальное письмо.
И если у вас эпсилон достаточно маленькая, там десятая доля процента,
то вы почти все, точнее, вы все спам-письма точно удалите
и удалите небольшую долю нормальных писем от нормальных людей.
Ну, тогда лучше наоборот, конечно. Можно наоборот создать, получается, вайт-лист ящиков.
Для нормальных ящиков вы точно получите от них сообщения
и еще небольшую долю от спам-адресов.
Ну вот в этом и проблема, что скорее вы можете легко добавить спам в какой-то фильтр,
в спам-фильтр, чем нормальный ящик добавить, потому что вы не сможете от него получить.
Ну вот, тут, короче, надо выбирать, что либо вы какие-то нормальные письма ударяете,
либо вам нужно еще заморачиваться с ставкой новых адресов, которых никогда не было.
Ну вот, тогда, наверное, на этом давайте закончим. Всем успехов!

Вроде со связью все пока что нормально, будем надеяться, так и продолжится.
Сегодня мы продолжаем говорить про виды задач в буквооптимизации.
Начнем с классического линейного программирования.
Сегодня я стараюсь писать на доске, если потребуется я переключусь на слайды.
Надеюсь, что получится обойтись доской сегодня.
Важно, что целевая функция линейная.
В наших задачах всегда ограничение равенства только линейные,
но вместе с тем ограничение неравенства тоже линейные.
Важно, что это стандартный вид задачи.
Очень много различных преобразований для того, чтобы какие-то другие виды задач.
Можно сказать, что...
Вот рассмотреть вот такую вот задачу.
AX равно B, CX меньше либо равно D.
Такая задача может быть приведена к такой задаче.
Давайте посмотрим, как это делается.
Смотрите, что надо сделать.
Целевая функция, понятно, не изменится.
Целевая функция, понятно, не изменится.
Вся магия произойдет с функциями, которые ограничение задают.
То есть надо CX плюс некоторая переменная U равна D.
При этом U станет больше либо равно нуля.
То есть мы, по сути дела, избавились от ограничений неравенств,
в которых конфигурировала некоторая матрица.
Понятен этот переход.
У нас что-то меньше другого.
Мы добавляем некоторый вектор положительный по компонентам.
По компонентам, поэтому становится равенство.
Поставьте минус, если непонятно.
Ну окей, вроде минусов пока таких нет.
Быстрых.
Ну а дальше что?
Вот это вот две штуки, это по сути означает, что у нас вот такая вот система.
Из блочных матриц, которые умножаются на такой вот новый вектор.
И тут B и D, соответственно.
Вроде я ничего не попутал.
Переменные у нас U больше либо равно нуля.
А тут у нас...
То есть вот тут есть некоторое разночтение.
И на x у нас никаких ограничений нет.
В этом случае надо сказать следующее, что x можно...
То есть если у нас нет ограничений на знак,
то мы можем представить соответствующую переменную как разность,
типа v-w,
где v больше либо равно нуля и w больше либо равно нуля.
Соответственно, подставляя вот это выражение вместо x,
мы получаем итоговую задачу.
Минимум c, видимо...
Ну да, в общем, c транспонированное v-w.
То есть что у нас чего?
v-w-u больше либо равно нуля.
И матрица преобразуется в a0,
что там, c,
единичная.
Вот здесь образуется v-w,
здесь образуется u.
Ну и понятно, что раз у нас
образовалось три переменных,
то матрица будет 3 на 3.
Это равняется BD.
Ну то есть что надо сделать?
Это...
Как правильно делать?
Ну типа a-a, что ли, да?
0, а тут будет c-с,
а вектор будет v-w.
Ну вроде так, да?
Вроде ничему не противоречит.
Равняется BD.
Ну в общем, все как мы любим,
в том плане, что число строк mn и m меньше n,
поэтому там подпространство какое-то образуется.
Ну и видно, что...
Видимо, просто нет.
Так, понятно ли преобразование,
которое мы проделали,
и тот результат, который получился?
Поставьте плюс, если понятно, и минус, если нет.
Ну окей, вроде всем все понятно, это прекрасно.
Смотрите, что важно.
Важно понимать, что, в принципе,
все эти преобразования,
они меняют структуру,
но не приводят к тому,
что у вас растет количество переменных,
количество данных, которые надо хранить.
То есть вам достаточно отследить,
для нее нет ограничений никакого,
то это значит, что в матрице,
в структуре матрицы появятся
не такие вот соответствующие блоки,
при этом физически вы, понятное дело, их не дублируете.
Физически у вас как было хранение матрицы,
а так оно и осталось.
Как у вас была матрица C,
так она эта матрица C у вас здесь и осталась.
То есть важно, чтобы
если вдруг вы когда-нибудь
надо будет что-то подобное реализовывать,
что не надо ничего дублировать,
надо просто грамотно прокидывать указатели
то, как эти указатели взаимодействуют
для соответствующих переменных.
Ну, в общем, понятно, что размерность
была вот такая,
типа x стало x у w,
разрослось все.
Ну и вам, понятное дело, надо будет в конце,
когда вы все это решите, вытащить
v-w, это будет ваше решение.
Ну и по построению оно будет не отрицательным.
Вот.
А, да, точнее неправильно я говорю.
Смотрите. А, нет, все правильно я говорю.
То есть оно не будет не отрицательным,
но
этого ограничения у нас в нашей
задачи исходной и не было.
То есть тут не было требований отрицательности на x.
Вот. В итоге, что здесь надо подсветить еще раз,
что у нас основные приемы
перевода различного знача классическим,
классическому виду это вот добавление
чего-то положительного, умножение
на минус единичку, разумеется, если знак не тот.
Вот. Ну и вот разбиение
на разбиение переменной,
для которой нет ограничений, на разность
двух переменных, каждую из которых
есть ограничение отрицательности.
Вот. И это ключевые
моменты про линейное
программирование, про то, как его
переводить и
переводить к стандартному виду и как этот
стандартный вид, собственно, используется.
Именно он используется в пакетах,
которые это все делают. Вот. То есть эта задача,
она довольно важная на практике.
Именно там всякие задачи в расписаниях
получаются, решаются.
Ой, прошу прощения. Вот.
И в целом
в индустрии очень много всего
делается именно линейное программирование.
Понятно, что от него не очень далеко
до... То есть это был
LP. Давайте я так буду аббревиатуру
тоже сразу писать. Вот. Но от него
не очень далеко к задачам типа
MILP. Вот. Тоже такая важная аббревиатура.
Может быть, вы ее будете встречать в дальнейшем.
Которая, по сути, все то же самое.
Вот. Только тут есть типа
некоторые вот так, а некоторые
другие индексы, типа...
В общем, для некоторых индексов,
короче, есть дополнительные ограничения столчисленности.
Давайте даже не 01 я сделаю.
Просто типа Z. Вот.
Там Z+, да, получается. У нас
целых чисел не отрицательных. Вот.
Ну и, соответственно,
понятно же, что
вот это гораздо сложнее, чем LP.
Вот. И геометрия
у этих задач она тоже отличается.
То есть если в LP мы имели бы
какой-то там многоугольничек, вот.
Ну, например, вот здесь вот, да.
Вот. И решение у нас было бы в какой-нибудь вершине
их со звездочкой, и нам не так важно
где этот вершин находится.
Вот это LP-случай. Вот. Если же мы про Милб
говорим, то нам важно, чтобы те координаты,
которые вот здесь вот отмечены,
они попадали идеально в некоторые
столчисленные точки. Вот.
Поэтому так уже не получится просто
решить. Но вот эту штуку
с ней борется тем, что решает
последовательность некоторую...
последовательность LP задачи. Вот.
Ну и идея у методов,
которые в основном такие задачи решают,
может быть они не очень... может быть не самые
лучшие, но на основе этих методов
как бы все остальные методы строятся
и более такие, наверное, продвинутые.
Вот. Заключается след в том, что мы вот
берем как бы вот... сейчас.
Че, надо вырезать. Берем вот эту вот картиночку,
да. Сбирать. Вот.
Берем вот эту картиночку, и вот у нас
получился X со звездочковой-то кривой.
Вот. При этом мы знаем, что
нам, типа, это не подходит.
И мы берем, и на основе некоторой там дополнительной
информации проводим дополнительные... добавляем
некоторые ограничения к этой задачи.
Ну, не знаю, типа, говорим, что
теперь наше множество, которое мы хотим...
Ну, в общем, этот X нам не подходит.
Давайте мы возьмем какой-нибудь другой.
То есть, не правильный, говорю.
Этот X не подходит, поэтому давайте мы проведем
какой-нибудь дополненный разрез. Так, надо другой цвет выбрать.
Ну, типа, вот давайте такой.
Вот. Какое-нибудь вот такое вот
дополнительное ограничение, так что
теперь мы будем решать нашу новую задачу
уже... уже на вот... вот... вот здесь.
То есть, вот тот X, который...
со звездочкой нам не... не... не... не повезло
с ним, вот, то
мы его, как бы, отсекаем.
Вот. И решаем уже здесь.
Снова решаем, получаем снова какой-то X.
Тут давайте X1.
Какой-нибудь вот здесь, например, X2 со звездочкой.
Смотрим, подходит он под наше
целочное ограничение или нет. Если не подходит, то снова встроим
некоторый разрез, который
дополнительно... дополнительно будет нам
отсекать неподходящую точку.
И решаем. И таким образом мы, как бы, локализуем решение
по счёт множества набора вот этих вот отрезов
из нашего допустимого множества.
Понятно ли в целом, как это работает?
Чем мил поотличается толпе?
И, в принципе,
почему это тяжелее? Тут, наверное, почему это
тяжелее, вам должен быть понятный
экскурс по сложности вычислений. Ну, дискретная оптимизация,
она, типа, переборная в основном вся.
Так. Если никогда
не наскривёмся, значит нет решения.
Такое множество такое бывает. Но там, типа, есть
проблема в том, что...
Ну, не проблема, скорее, а про там
отдельно проверяется разрешимость. То есть там
можно проверить разрешимость.
И понятно, что это я, типа, нарисовал
в 2D, но когда у вас, типа, там, миллионные
пространства, там, типа, скорее всего
вы на что-то наткнётесь.
То есть вы, скорее всего, у вас получится ситуация,
когда ваша вот эта вот допустимая
множество одержит, например, одну точку
с целыми координатами. Эта точка, понятно, будет
то, что вам надо, потому что она единственная допустимая с целыми
координатами. Вот, наверное, правильно так сказать.
Я надеюсь, я как-то пояснил, может быть, не достаточно
подробно, но идею постарался донести.
Так, есть ли какие-то вопросы?
Слушайте, просто
линейное программирование, там в
ограничении cx плюс уровну,
c – это не c транспонированная,
это одна и та же линейная.
Сейчас, смотрите, c транспонированная
здесь, это маленькая буква c,
это типа вектор, а тут c большой – это матрица,
если я правильно понимаю вопрос.
Ага, всё, спасибо.
Отлично. Да, может быть, я тут немножко масштабы
габариты снёс. Ну, сейчас я поправлю.
Ну, как-то так.
Так, ещё комментарии, вопросы.
Вот если останется время, про вот это всё
будем на следующей лекции, когда про методы будем
говорить, может быть, я немножко подробнее покажу
и картинки какие-нибудь реализации.
Это, в принципе, несложно делается.
Так, 30 секунд на вопросы,
минусы, плюсы, любой фидбек
годится. Так, сейчас пока 4
человек как-то откликнулись.
Тут у нас остальными
12, видимо,
как вы там поживаете.
Так, окей, вижу, что вроде
понимание почему-то приходит, это прекрасно.
Ладно, это был небольшой экскурс ленина
программирования, мы как-бы периодически не можем
возвращаться. Следующий пункт
нашей программы, это понятное дело, квадратичное
программирование, тут как бы неудивительно,
неудивительное усложнение логики.
Причём будем
ключевой как бы аббревиатуры, quadratic
programming, quadratic constraint.
Сейчас я правильно сформулирую,
наоборот, квадратик или
constraint, quadratic programming. То есть вот такие
вот буквы, это типа,
не знаю, писать или расшифровку.
Сейчас напишу.
Собственно, вот почему, так называется.
Ну и задача тоже довольно понятная.
Хотим минимизировать некоторую квадратичную
форму. Сейчас я обозначение сверю.
Понятно.
Минимизировать квадратичную форму при
квадратичных условиях. Понятно, что
у нас вся эта история будет
выпухла только когда все функции
поиты будут выпухл... все матрицы
поиты будут симметрично положительно определены.
Вот так
поита х
плюс там, поита
транспонентная х плюс.
Вот. Выпукло
при поитых
принадлежащих сн плюс.
Вот. Ну то есть
основка такая. Вот.
Все это матрицы, векторы.
Понятно, где векторы,
где числа, где матрицы, я надеюсь,
из размерностей. Вот. И
понятно, наверное, что раз мы вот
здесь вот жили на конусе, получается
каком... Ну, все это так или иначе сводилось
к
рн плюс, вот. То вот это все
будет сводиться, понятно, наверное, что
к конусу к2, то есть конус, который порожден
второй нормой. Вот. Я надеюсь, рн плюс
не надо напоминать, а вот про что такое
к2 я напомню все-таки.
рн r плюс
вторая норма меньше либо равно t. Вот такая
штука. Вот. Ну и будет
отделаться с помощью
преобразований
каких. Так. Ну что надо сделать? Первое
будет преобразовать вот эту штуку, ну
самое главное, да, к виду, что
некоторая, некоторая линейная
функция... Так, ладно, сейчас давайте
не буду, не буду
перебегать
стороны в сторону, давайте последовательно идем.
Ну, что еще ясно? Ясно, что если у вас
p и t
равны нулю, следовательно,
получаем lp. Это всем видно,
что если у нас матрицы за нуляются,
то мы получаем линию программирования.
Поставьте плюс, если вам это видно.
Вот. Ну, то есть, соответственно, можно сказать,
что вот это вот, вот эти или
constrain, что lp
под множество этой штуки.
В общем, будем сейчас иерархию устроить до
максимально широкого класса
задач, вида задач, которая
будет включать себя все выше перечисления. Поэтому
идем по степени, по направлению
усложнения постановок. Вот.
Значит, что еще здесь важно?
Ну, примеры. Почему это вообще
куда такое берется? Вот. Ну,
то, что мы уже несколько раз, я кажется, упоминал
или не упоминал. Ну, если не упоминал, то еще
буду упоминать. Вот. Примеры.
Вот. Ну, все, что касается про минимизацию
там каких-то норм, вторых,
например, там наименьшие квадраты,
стандартная задача. А это было, наверное,
на первой лекции я упоминал, да?
Вот. Это все квадратичное.
Вот. Сюда можно добавлять понятные ограничения,
где лежит у нас
x it. Вот. Это все подходит
под, под
тот тип, который был ранее записан.
Вот. Можно еще
искать решение минимальной нормы,
то есть, если у вас задача
переопределенная, ну, типа у вас
ax равно b, и вы знаете,
что у вас x-ов очень много. Ну, это наша стандартная
ситуация, когда матрица A
имеет вот такой вот вид. Это вот m,
это n, и m сильно меньше
n. Вот.
Так. Здесь видно, да? Вот такой
вот вид. Х – это целое подпространство.
И чтобы как-то конкретизировать,
какое решение нам подходит, мы говорим,
что, окей, нам нужно решение минимальной нормы. Вот.
Решение тоже, соответственно, квадратичная функция.
Вот. И тут как бы
ключевое место – это, конечно же, выбор
того, какая здесь норма. То есть, из-за того, что есть вторая норма,
у нас получается именно квадратичное
программирование. Вот. И поэтому
естественным образом задаться вопросом, а что будет,
если, например, захотеть искать
минимальную норму, типа бесконечной, или первую?
Вы думаете, какой задачи такой,
такая обстановка сведется? Вот это
что такое будет? Пишите варианты в комментариях.
Ну, линейное будет. Ну, линейное будет, конечно.
Давайте приведем это к линейному.
Кто понимает, как это делать?
Давайте. Третьих слов на то, чтобы подумать
и потом обсудим. Так, ну что?
Давайте тогда обсудим,
что делать надо. Но сначала, понятное дело,
надо написать определение. Если это
мин, максимум нормы х этого,
да, по И, минимум по х.
Представим, что х равно b. Вот.
Ну, то есть, раз тут стоит какой-то максимум,
то, если у нас максимум
чему-то равен, то это значит, что
мы, по сути, хотим минимизировать
некоторое t, кое-что.
Ну, понятно, х равно b.
И х и t меньше либо равно t.
Вот. Ну, а дальше,
я думаю, понятно более-менее, что
раз это минимум t, здесь
ах равно b. И если модуль
меньше либо равен числа, то это значит, что
само число от минус t до t. Вот.
Ну, и соответственно, вы тут, что, х и t.
Ну, вот. В общем, получили не на программирование.
Понятно, что привести это к стандартной
форме дополнительно некоторая работа.
Вот. Рекомендую, кстати, потренироваться
и проделать. Это, может быть, достаточно полезно,
как-то руку набить. Вот, может быть.
Ну, тренироваться с пользой стандартных приемов.
Понятно, что, скорее всего, размерность как-то сильно
возрастет в результате.
Но, как бы, не так важно сейчас это.
Так. Понятно ли, какие преобразования были
проделаны и почему они привели к успеху?
Поставьте плюс или минус.
Так, окей. Вроде все в порядке.
Ну, соответственно,
следующий пример квадратичных задач
это всякие задачи. Ой.
Задачи про финансы
и составление оптимального портфеля. Вот.
У нас есть там, как обычно было...
Ой, третий пример, да?
Есть инактивов. Вот.
И у нас есть там
информация о том, что изменение
цены это по с чертой.
Вот.
Это типа средняя цена.
Треднее изменение цены.
Вот. И сигма – это соответственно
кавируционный матриц этого случайного вектора.
Вот. То есть раз это кавируционный матрица,
то она там СН++.
Вот. И чтобы...
В общем, значит, наш минимизирует риск
под риском... Риск.
Риск – это вот такая штука.
Вот. Где наш ИКС –
это некоторое распределение
средств по активам.
Вот. То есть мы смотрим на то,
как бы нам разложить
наши средства по
активам так, чтобы суммарный
риск был минимален.
Мы хотим минимизировать эту штуку
при условии, что
мы хотим получить некоторый гарантированный доход.
Вот. А гарантированный доход – это наш
ПС чертой ИКС больше любви
чем некоторый РС чертой. Минимальный доход.
Вот. Ну ИКС, соответственно, распределение,
поэтому для него выполнены
соответствующие условия нормировки. Вот. Но видите, тоже
как бы квадратичная задача на самом деле,
потому что целевая функция квадратична,
все остальное – линия. Вот. Поэтому здесь тоже
это все... То есть это типа
задачи тоже подпадает под наше определение
квадратичной оптимизации
с квадратичными ограничениями.
Вот. Значит, вот такой
еще пример. Да.
Еще один пример. Давайте про классификацию
немножко поговорим. Вот. Что у нас есть
стандартный там
SVM, классический SVM-классификатор.
То задачи его обучения
также сводятся к
квадратичной оптимизации с
нескольких сторон. Сейчас посмотрим
с одной. Вот. Возможно, у кого-то на
семинарах в домашнем здании там где-то будут разобраны
почему там с другой стороны
это тоже квадратичная задача. Вот.
Но еще ничего у нас есть. Так. Тут, возможно, кстати,
стоит уже сайты показать, потому что тут
важна картинка. То есть картинок, наверное,
это может быть тяжело воспринимать.
Так. Стоп.
Так. Сейчас я тут все выпилю.
Share screen.
Так. Где эта лекция? Это вот.
Вот. Ну, короче, у нас есть выборка, то есть набор
из векторов и чиселок.
Чиселки плюс-минус единицы.
И мы хотим гиперплоскость построить, чтобы
для одних элементов у нас был больше 0,
а значение для других был меньше 0.
То есть стандартное определение
разделяющей гиперплоскости вот только
не на множествах, а на
просто наборе точек.
Ну, соответственно, вот они как-то могут быть
нарисованы. Ну и понятно, что если такая
идеальная конструкция, то
мы можем гиперплоскость очень по-разному проводить.
Можно вот так проводить, можно вот так,
можно вот даже вот так.
И, собственно,
закономерный вопрос, как эту
разделяющую гиперплоскость однозначно образом задать,
чтобы у нас как бы не было произвола.
Пожалуйста, напишите, какие варианты
и задания, как однозначно задать
гиперплоскость вы можете предложить
в чате, пожалуйста.
30 секунд-минута на то, чтобы
какой-то подумать над этим.
То есть вопрос, что будет являться критерием
единственности и оптимальности.
Вот почему, может быть, как понять,
вот эта вот желтая гиперплоскость,
зеленая гиперплоскость, она лучше
или хуже красная или нет? То есть как
вот оценить качество гиперплоскости
надо понять, и отсюда получится критерия
единственности, критерии, которые надо
оптимизировать, чтобы получить
единственного кандидата.
Да, осталось только
маржины, и как это связано
с той картинкой, которая сейчас изображена.
Продемонстрируйте свой уровень понимания того,
что у вас было на вашем обучении,
и вы найдете правильный ответ.
Гадите, от всех до всех y итых,
а что что-то с...
сейчас, а что за...
от прямой до всех...
сейчас не очень понятно.
Там не y итые, а x итые, я так понимаю.
Да,
минимизирует расстояние,
знаковое расстояние до разделяющей
гиперплоскости, положительное для
правильного. А что такое?
Расстояние до...
по модулю проекцию вектор на нормаль
к прямой для каждого класса.
Очень интересно. По модулю
проекцию векторов точек на
нормаль. Да я вижу.
Так, ладно, пишите, пишите.
Да, интересно почитать.
Просто марж
расстояние до разделяющей. А от чего
расстояние? Непонятно от чего.
По модулю от точки.
А какой? Ее точку еще-то найти какую-то?
Каждой точке свой марж.
Ну хорошо, а максимум только почему
берется? Точки выборки.
Нет, да, хорошо. А как это гиперплоскость
относится? То есть как максимум
по выборке брать?
Можно как-то... Во-во-во!
Вот тут про ширину полосы что-то начинает
писать. Это вот... Это понятнее,
как, наверное, представить, да?
Все точки в эту полосу оценивались.
То есть оценивались.
Как оценивались? Ну хорошо, давайте
так, простые вопросы. Как параметризуется
гиперплоскость? Что такое модуль?
Короче говоря, вопрос простой. Какие у нас
переменные, которые мы хотим определить?
Так, прошу прощения. Вроде...
Алло, алло. Тест, тест.
А сейчас нормально? Ой-ой, печаль.
Так, ладно. Прямо сейчас? Да.
Ладно, будем наде... Вроде получше стало.
Окей, спасибо.
Так, ну хорошо. Смотрите, более-менее я
понял, что вы хотели написать выше.
Вот. Если сейчас будут какие-то проблемы,
пока я буду говорить, вы тоже скажите,
чтобы как бы заодно...
Заодно можно было бы проконтролировать,
что связь устаканивалась более-менее
стабильная.
В общем, идея очень простая.
На самом деле, вот из всех этих
гиперплоскостей мы выбираем, типа,
тут ближайшие друг другу точки
из двух множеств.
Что-то никто по мне не сказал.
Вот. И пытаемся найти
такую гиперплоскость.
Вот.
Так, чтобы параллельно,
если мы параллельно ей проведем
гиперплоскости по этим точкам,
то расстояние между ними будет максимально.
То есть зазор, тот, который там
пытались сформулировать, что это такое,
прознаковое расстояние
и все прочее.
Это просто-напросто та самая ширина
полосы, которая тоже упоминалась.
Вот формализация этой ширины полосы
это, собственно, расстояние между
большими точками, это расстояние
между гиперплоскостями,
проведенными через ближайшие точки
параллельно разделяющими.
То есть как это формализовать?
Формализуется очень просто. Вводится в понятие
оборных объектов. Это, собственно, те самые
объекты, на которых будет выполнено
наше требование. То есть так можно сделать,
потому что мы всегда можем отнормировать A и B,
так чтобы тут были справа единички.
Ну и дальше, поскольку они параллельны,
то мы знаем формулу из аналитической геометрии
про то, как считается расстояние между гиперплоскостями.
Считается, как норма
свободных членов здесь.
Делить на модуль разности
между свободными членами
в правой части.
Делить на норму нормальной к гиперплоскости.
Поскольку тут можно,
так нам повезло, что это 1-1,
то эта штука просто двоечка равна.
Нам надо, по сути, максимизировать
это замечательное расстояние.
А теперь давайте поймем, почему
максимизация вот этого
расстояния, довольно естественная
репция, приводит нас к такой вот задаче.
Понятно, что максимизация вот этой штуки
то же самое, что минимизация
обратной величины.
А дальше нам надо потребовать, чтобы
для всех элементов,
для которых мы это все записывали,
было выполнено правильное неравенство,
поскольку это опорные объекты,
и в них выполнено
строгое равенство, а во всех остальных местах
должно быть выполнение неравенства.
Ну и, соответственно, поскольку у нас Y знаковый,
то вот это произведение, которое мы здесь
написали, должно быть больше, чем на единицах.
Вот это, видимо, этап, когда вы писали
про максимизацию,
ну, когда вы писали про
то, что максимум по выборке считается,
ну вот он как бы не совсем максимум по выборке,
он вот скорее в качестве ограничений сюда заходит.
Ну и получается, что мы, опять же,
минимизируем некоторую квадратичную функцию
при линейных ограничениях.
То есть это, опять же, получается
квадратичная задача оптимизации.
Вот если ее решать, то получается вот такая вот картинка.
То вот у вас по опорным объектам проходят
параллельные гиберплоскости.
Ну и та самая разделяющая,
ну если их несколько, то любой подойдет.
Если они выстроились,
если они выстроились как бы в такие вот прямые,
ну не страшно, любой подойдет.
Да, и, соответственно, серединка между ними
как раз таки будет считаться
как раз таки и оптимальной
разделяющей гиберплоскостью.
В общем, простой пример.
Понятно, что на вашем обучении вам рассказывали,
что делать, если выборки нелинейно-разделимы.
Ну в общем,
рекомендую еще раз это вспомнить
и увидеть, что в принципе
на класс итоговой задачи оптимизации
это никак не влияет. То есть как у вас была квадратичная задача,
так она и останется с некоторыми поправками.
Вот, так, есть ли вопросы?
Так, дальше да. Дальше как раз таки более
нагруженный слайд.
Поэтому хочется все вопросы по
примеру с картинками
решить сейчас.
Так, ну, вопросов каких-то,
комментариев я пока не вижу.
Наверное, надеюсь, что понятно.
Значит, смотрите, следующий пример
это, собственно, сразу
как-то сразу, не сразу,
явным образом записанная задача
оптимизации именно на конусе второго порядка.
Так как order-con-programming, так называемый.
Давайте-ка я все это попытаюсь сейчас записать
явно в доске,
потому что тут надо, возможно, что-то будет пояснять.
Так, давайте-ка я сейчас
как-нибудь это все снова
расшарю правильным образом.
Так, сейчас сначала вот так,
что елочка гори, есть уран.
Так, пишу слайды.
Вот.
Так, да, следующий пункт
это
second-order
cone.
Так, cone
programming
second-order
cone-programming.
Ну, как вы, наверное, уже догадались,
тут все то же самое, что у нас было до этого
в линейном программировании.
То есть тут ax равно b.
Только теперь ограничения
неравенства у нас вот такие.
c i t транспонированный
x там плюс d i t, например.
Вот так.
То есть все, что у нас тут есть,
так и тут двоечек стоит.
Означает напрямую, что
вот эта штука, конкретно вот это
ограничение, это что означает?
Это означает, что вектор вида
ax b плюс b i t
запитает c i t транспонированный
c i t больше либо равно
нуля, в смысле k 2.
Вот. То есть вот это вот k 2, оно вот как раз
таки по определению, то есть мы
как я уже вот тут упоминал,
где у нас тут был конус, то я записывал.
Вот. Вот-вот-вот. Напоминаю.
Вот у нас был конус k 2, и мы просто взяли и
афином образом преобразовали аргументы.
Вот афинное преобразование ничего не меняет,
как мы уже выяснили ранее.
Поэтому можно записать в максимально общем виде
именно таким образом. Понятно ли,
так, я надеюсь, сейчас все видно,
понятно ли постановка задачи
и то, почему она выглядит именно так,
как связана с конусом? Так, ну поставьте
плюсы, если понятно. Алло-алло, а сейчас?
Ага, так, а в каком
в каком месте...
Короче, не важно, какой интернет, главное,
чтобы связь останавливалась так или иначе.
Так, а на каком месте я пропал? В общем,
еще раз. Задача оптимизации на конусе
второго порядка заключается в следующем,
что мы по-прежнему пытаемся работать
линейными целевыми функциями,
у нас по-прежнему линейные ограничения
типа равенства, а не равенства, которые
записаны в виде вот такой вот
связанной со второй
нормы здесь.
Они переписываются просто как вот, что вот такой
вот вектор больше либо равен нуля
в смысле конуса K2. Вот. Конус K2
собственно, я уже показывал, покажу еще раз,
чего страшного, это вот такая штука.
Вот. И проделаны, по сути дела,
просто афинные преобразования
аргументов. Они проделаны
и получилась вот такая штука. Это
опечатка. Теперь там K2 должно
быть на самом деле. Почему больше либо
равно? А, ну потому что у нас было,
когда мы про конусы говорили, вот,
сейчас, может быть, там будет меньше либо равно,
но, по-моему, должно все-таки больше.
Идея была в том, что если,
то есть, еще раз, вот эта штука
в смысле нуля
это равносильно тому, что X лежит
в конусе. Вот. Кажется,
теперь все должно быть понятно. Или еще
нет? Ну, то есть, для того, чтобы как
наше классическое больше либо равно нуля
стало бы эквивалентом, что X лежит в Rn+.
Да, сейчас секундочку про применение.
Пару слов скажу тоже. То есть, вот
чтобы вот эта вот эквивалентность, она как бы
была перенесена по аналогии, то
поэтому, типа, здесь меньше либо равно,
а тут больше либо равно в смысле конуса. Так,
Никита, стало ли понятнее? Плюс-минус.
Так, интересно. А что не очень понятно?
То есть, давайте тогда еще раз я продублирую определение
нашего конуса.
Ну вот. То есть,
здесь в конусе
стоит знак меньше
либо равно. Но если у нас
A+.
А, да. Вроде дошло. Окей.
Это прекрасно.
Так. А как мы сравним
результат скалярного произведения?
Скаляр по конусу.
Результат скалярного произведения?
А где у нас есть скалярное произведение по конусу?
Я не очень понял.
Я как раз правильно написал A+.
Вот дальше.
Давай еще раз показали определение.
Все-все-все.
Да-да. Прекрасно. Я теперь...
Спасибо, что прокомментировали.
Так. Вроде бы остался вопрос только про применение.
Ну, смотрите. Во-первых,
поскольку эта штука...
Вот эта штука, она в каком смысле стандартна,
потому что формулируется через конус.
К ней сводится то, что мы до этого
обсуждали про квадратические задачи.
Сейчас я попытаюсь
как раз продемонстрировать,
каким образом мы можем привести
вот это в
вот этот вид и воспользоваться
солверами, которые, собственно, сделаны
специально образом для вот этого.
Ну, здесь всего плюс-минус...
Ну, проблема понятна.
То есть надо преобразовать вот эту штуку,
которая у нас была,
в то, что какой-то вектор
будет лежать в K2.
И тут нам сейчас честно образом поможет то,
что если у нас есть матрица
строго положительно определенная,
то у нее есть там разложение Халецкого
верхнюю и нижнюю треугольную матрицу.
То есть это нижнюю треугольную, это, соответственно,
верхнюю треугольную. Это некоторый факт, который
приведет. Вот. Тогда мы можем, понятное дело,
записать вот так. Ой,
ничего не видно.
Плюс, чтобы это Х, плюс
Р, да? Вот. Ну, а дальше хочется, как будто
полный квадрат выделить. Вот.
И давайте сейчас попытаемся это проделать
и поймем, почему это все дело приведет.
Что еще у нас тут есть? У нас тут есть
LTX транспонированный на
LTX, да?
Потом идет плюс удвоенное произведение
LTX на какой-то,
видимо, Q.
Типа транспонированное, да?
На Q и плюс
Q транспонированное Q.
Ну, давайте сейчас просто внимательно
посмотрим и
поймем, что
все это как бы будет означать.
Да, я не пропал. Я умышленно
замьючил микрофон, чтобы
не шуметь, пока вы, возможно, пытаетесь
вывести полный квадрат самостоятельно. Так.
Короче говоря, я немножко переписал
то, что было до этого написано.
Смотрите, вот у нас, типа, первый умножитель,
вот он здесь сидит. Вот. Вот у нас, типа,
второй умножитель. Ой. Вот он, второй
умножитель, который пошел сюда.
И, учитывая то, что у нас там было неравенство
менее шлюра нуля, он пошел вот сюда.
Ну и минус R просто перенеслось. Вот.
Отсюда что следует? Отсюда следует, что у нас
как бы, если мы теперь попробуем
это все дело свести, у нас получится, что у нас
там LTX,
вроде. Алло, алло. Так.
А сейчас, а сейчас слышно?
Тест-тест. Алло. Есть ли контакт
какой-то? Тест-тест.
Сейчас. Что-нибудь слышно? Алло.
Дайте знак, какой-нибудь, пожалуйста.
Так. Ну вот сейчас, вроде, все должно было вернуться.
Пожалуйста, проверьте и откликнитесь.
Что меня там слышно, видно?
Ну, сейчас видно. Слышно, видно.
Отлично. Все. Слышу. Здорово.
Так. Сейчас я перевернусь. Ой.
Перекручу. Да. Вот сейчас точно пропали.
Все. На этом мы закончили. Так.
Ну, короче, я вроде получилась
выделить полный квадрат.
Вот я тут попытался что-то
подсветить какие-то одинаковые
компоненты. То есть первое слагаем,
второе слагаемое.
Квадрат второго. Поскольку тут меньше либо равно,
то...
Так, а давайте я лучше вот так здесь напишу.
Меньше либо равно нуля, да.
Потом мы как бы прибавляем к обеим частям одно и то же.
В частности, вот это. Вот. И R переносится вот сюда.
Вот. Поэтому тут в итоге
образуется выражение вида
LTX
плюс L
минус T
с волной, да, квадрат
и меньше либо равно, чем
ну, собственно, вот это вот L-1
с волной,
минус R. Вот.
В общем, в итоге
это все линейное преобразование
вроде бы свели
к тому самому конусу,
который нам был нужен. Вот.
То есть если у вас есть вот такая вот задача,
то вам надо там проделать
разложение Халецкого для каждой матрицы,
получить с волной,
который связан с B, понятно,
я надеюсь, как там, типа, надбойку надо, конечно, поделить.
Вот. И, возможно, там
с треугольной линией с тем решить.
И отсюда получатся
ингредиенты, которые уже будут участвовать
в построении концентрального порядка.
То есть отсюда следует,
то у нас было вот такое.
Вот. Но сейчас будет еще вот такое.
Вот. Да. На самом деле
давайте вставим это в качестве упражнений.
Если вы не придумаете, в следующий раз я, может быть,
что-то про это расскажу. Вот здесь видите
вот этот вот значечек. В общем,
упражнение, привести пример
или объяснить почему. Почему.
То есть, типа, задача, давайте я обозначу
какой-нибудь фигурной буквой P. P
принадлежит вот такому семейству,
но, но P
не может быть представима как вот так.
То есть, хочется понять, где зазор. Вот.
Подумайте, это, наверное, будет полезно.
Так. Все ли понятно по поводу
сведения и того, как они
с друг с другом связаны с задачей?
Алё, меня слышно сейчас? Вижу плюсики, отлично.
Гуд, спасибо. Да.
Давайте
вопросы. Так. Ну, окей. Давайте
пойдём дальше. Наверное, геометрические
задачи я пока пропущу.
Вот. Там потому, что надо много всего
вводить. И
в общем, они, я бы не
сказал, что очень часто, наверное,
встречаются. Вот. Ну, в общем, в слайдах
это есть. Поэтому, если кому интересно, может
в слайде в описании там всё довольно подробно написано
со ссылочками, со всеми делами. То есть, в общем,
разобраться можно. Вот. Если будут какие-то вопросы,
просто в чат напишите. Я там прокомментирую
что-то. Вот. А сейчас давайте перейдём
как раз временно это осталось.
К следующему конусу, который у нас был,
это задача с DP. Вот.
Semi-definite. Semi-definite
programming. Вот. Ну, и тут как бы
два варианта есть, как обычно. Ну,
не как обычно, конечно, но по-прежнему
ищем X. Вот. Только теперь...
Извините, а можете на секунду поднять на
страницу? Да могу, конечно.
В чём проблема? Вообще легко. Презентации лежат
на GitHub, ёлки-палки. Ссылки вроде бы
в прикреплённом сообщении
в чате. Ну и временный вопрос
почти к концу первой
половины, где лежат презентации.
Сейчас я проверю.
Ну, спасибо. Слушайте, ну да, вот в
прикреплённом сообщении в первом, там куча
ссылок. И вот первая ссылка у нас
на репетиторию с слайдами лекций.
Всё закреплённо. Ну, по крайней мере, у меня
получилось. Вряд ли у меня какие-то...
Там надо просто несколько раз нажать. Так, всё.
Вроде все всё спасали, что хотели.
И, соответственно, хитрость в том,
что ограничение, которое мы сейчас здесь
встретим, оно имеет довольно жуткий вид
на первый взгляд. А именно вот такой
там хит, давайте
на фит. Больше ли бы равно
нуля, где ж
и фит все симметрично.
Вот. То есть смотрите, что произошло.
У нас внезапно наши
иксы некоторым образом взвешивают
симметричные матрицы,
так чтобы в результате получилась положительно
полуопределённая матрица.
Чувствуете, насколько пошло усложнение?
То есть теперь у нас тут, типа, неопределённость
надо проверять. Ну, другая форма записи,
смысл которой мы поймём
через, наверное, две недели, я надеюсь,
может быть, а может уже и на следующий поймём.
Вот. Она вот так вот выглядит.
То есть мы минимизируем
некоторый след. То есть тут уже
перемена эта матрица. Вот.
При условии, что сама матрица положительно
полуопределена, а след
вот такой штуки равен там
условно быитому. Вот. То есть
так. Всё. Окей.
Значит, ещё раз. У нас такая задача,
где целевая функция всё ещё
линейна. А в другой постановке
та же самая задача. Через некоторое время
поймом почему это та же самая задача. Там тоже
целевая функция линейна, только она линейна по натрице.
То есть вот эта штука, это ведь, на самом деле,
есть не что иное как скалярное произведение
между матрицами. И вот это тоже это скалярное
произведение между матрицами. То есть в принципе
можно углядеть, что вот то, что
мы здесь видим справа, есть
не что иное, как об highlight линейного программирования
на случай переменной,
которая является матрицей. Тем или видно и понятно,
аналогия. Оставьте, пожалуйста, плюсики, если понятно. Вижу четыре плюсика. Интересно, это все кто
остался в живых. Так, вижу пятый, но не уловил идею. В втором случае x это матрица, это правда,
да, x из Rn, тут вот надо написать x из Rn на n. Да, но идея в том, что вот у нас было линейное
программирование, вот видите, что мы тут в самом начале делали. У нас было скалярное произведение
между векторами, линейное ограничение, которое тоже как бы набор скалярных произведений между
строками матрица и столбцом x и то, что x больше либо ровно нуля. А теперь, если мы хотим это
обобщить на уровне матрицы, то мы что делаем? Мы говорим, что окей, у нас теперь про скалярное
произведение между матрицами, скалярное произведение между, вместо строк матрицы у нас
типа сами матрицы появились, аиты, и вместо того, чтобы x у нас был больше либо равно нуля,
мы потребуем положительную полуопределенность. То есть как бы пытаемся обобщить то, что мы
хорошо знаем? Стало ли понятной идеи? Процесс симметричный? Да, да, да, все симметрично. Ага, вижу, окей. Так, ну
окей, давайте дальше пройдем. Вот, значит, что нам это все даст? Это нам даст, во-первых, ну мы понятно,
что если у нас есть такая задача, то мы можем с легкостью получить lp. Каким образом получить lp?
Если, что надо сказать, ну уже 0. Ой, вот сейчас будем про левую форму говорить. Вот, ну не плюс,
конечно. И if it, ну выдавайте, что должно произойти с if it, и какой вид они должны
иметь, чтобы в итоге мы получили линейное программирование. То есть, смотрите, нам нужно
сделать так, чтобы не отрицательно приносить некоторые матрицы, выродилось в требовании,
чтобы все x и больше либо равно нуля. Что это за матрица такая? Диагональная. Совершенно верно,
диагональная. Что на диагонале должно стоять? Положительные числа. Не совсем. Как с x это связано?
Как диагональ связана с x? Да, именно так. То есть, да, на i-той позиции будет 1, то есть,
это на самом деле диагональная матрица, только у них везде 0, 1 только на i-той позиции. Вот если
мы зададим таким образом, то мы сведем нашу задачу к линейному программированию. Поэтому lp,
понятно, тоже от множества sdp. Вот, теперь более хитрая штука про то, как свести sdp к конусу второго
порядка. Точнее, ну да, как свести sdp к конусу второго порядка. Тут надо знать некоторые специальные
факты, названием, что такое дополнение по шуру. Шур комплимент еще так называемый. Давайте я напишу
сразу правильное название. Шур комплимент. Вот, это значит такая история. Если у вас есть блочная
матрица симметричная, то, значит, если c положительно определена, то вот положительная
определенность вот тогда, прошу прощения, что это какие-то проблемы со связью. Так, в общем,
я остановился на том, что, сейчас я покажу картинку, что у нас будет сейчас заполнение по шуру,
в котором мы рассмотрим блочную матрицу, и, рассмотрев правильную блочную матрицу,
мы сведем полупределенную задачу, полупределенную оптимизацию к конусу второго порядка.
Можете еще раз? Можно еще дописать предыдущую? Да, конечно, вот, пожалуйста. Видно, да, все?
Понятно было про fit и почему единичная на нужном месте должна стоять. Так, можно продолжить? Да.
Отлично, спасибо. Вот, смотрите, это нам поможет в следующем моменте, что мы представим, что наше
вот ограничение основное, которое было, вот меньше либо равно t, это на самом деле будет нам
равносильно тому, что вот такая матрица t, x, x-транспонированная t на единичную матрицу,
будет положить на полупределенную. Вот, ну, то есть, смотрите, что произошло. Вот здесь роль, ну,
типа, понятно, что результат про c таким же образом преобразуется в результат про a,
только надо комплимент считать относительно a, а не c. Вот, и поэтому вот эта штука, как вот это
связано с вот этим. Давайте рассмотрим, сейчас давайте поймем, что надо рассмотреть. Собственно,
что у нас здесь положить на определенную? Ну, вот это положение поделено, да? Вот, только, да, только
не это надо будет сейчас рассматривать. То есть у нас положить на определенный блок строго вот
вот это t, а t больше нуля у нас. Вот, давайте посмотрим, что это значит. Что мы должны взять,
а, все было правильно, сейчас. Вот, давайте сравним вот эти две матрицы. Вот эту матрицу и вот эту
матрицу. Вот, и запишем вот это выражение. Не тот стиль. Вот, то есть, что у нас тут такое? А это t
минус b, b это xt, умножается на c в минус первый. c в минус первый это единица на t на единичную
матрицу, на bt, bt это x. Эта штука, это же число, я надеюсь, это понятно, больше либо равно нуля. Ну, и это
напрямую, напрямую означает, что t квадрат больше либо равно квадрат рефлидовой нормы x. Понятно ли,
какие преобразования были сделаны? Вроде бы понятно. Вот, ну, соответственно, если мы проделаем
линейные, ну, линейные преобразования аргументов, то матрица, понятно, как изменится. То здесь просто
появится. А почему, если матрица a, b, b транспонирована, c положительно определенная, то верное неравенство? Ну, то есть, не отрицательное. Ну, то есть вопрос, почему это верно, да? Да. Ну,
давайте сейчас попытаемся доказать своевременный вопрос. Вот. Ну, значит, это все связано с так называемым
площным исключением. Вот. И, в общем, сейчас давайте я подумаю, насколько тут обе стороны действительно
важно. Сейчас. Бам-бам-бам-бам-бам. Ну, наверное, давайте начнем тогда с... Сейчас, секунду. Рассмотрим вот эту
вот матрицу сейчас. Попробуем доказательство какое-то провести. Может быть, не до конца, но вот. И пусть вот это
выполнено. Значит, тогда что мы знаем? Мы знаем, что раз квадратичная форма положить на полуопределенное, то у нас
есть что для любого вектора x... Рассмотрим блочный вектор. Теперь x, y. И мы имеем, что выполнено вот это.
Там, по-моему, двойка должна появиться. ytbx плюс ytcy, да? Это будет больше либо равна нуля. Теперь, если мы
рассмотрим функцию... Да, собственно, возьмем, проминимизируем вот эту штуку по y, то есть это будет там
g от x и y. Вот раз эта штука больше либо равна нуля, то эта штука тоже будет больше либо равна нуля. Это
будет некоторая функция f от x. Надеюсь, пока что все было понятно. Ну, то есть у нас функция от блочного
вектора. Мы взяли квадратичную форму. Поскольку она больше либо равна нуля всегда для любого, то мы взяв
минимум по одному блоку, мы положительность не сломаем и получим новую некоторую функцию от оставшегося
блока. Вот все, что произошло. Да, понятно. Окей. Значит, смотрите, теперь важный может быть момент,
что при каждом фиксированном x наш функция g, она будет выпукла. Поэтому мы можем явным
образом решить эту задачу при каждом фиксированном x. Ну, ответ... Ну, то есть, понятно, взять... А, мы пока не
можем решить эту задачу, конечно. В общем, я надеюсь, что вспомнив какие-то азы матанализа, более-менее
понятно, что раз эта будет функция выпукла, то нам достаточно найти локальный минимум, а локальный минимум
мы ищем тем, что у нас градиент блочки должен быть ноль. Я как раз хотел успеть, возможно даже успею это
показать более аккуратно. В общем, нам достаточно посчитать градиент этой штуки по y, приравнить его к нулю
и получить тот результат, что y со звездочкой у нас будет равен, подгляжу, понятное дело, своей
записи, минус c в минус первый bx. Вот. Вот этому будет равен y со звездочкой. Соответственно, если мы
теперь подставим этот y со звездочкой в g, то получим, что наш f от x будет равно следующей
штуке. Вы не поверите, но это будет ровно то, что нам надо продемонстрировать. x транспонировано a
минус bc минус первый bt, если я правильно проставил транспонирование. Нет, конечно, неправильно. Вот, на x.
И эта штука всегда больше 0. А значит, эта штука положить на полуопределена по определению положить
на полуопределенную квадратичную форму. Понятна ли логика? Если нужно провести какие-то детальные
выплотки, вы, пожалуйста, скажите. То есть, тут два места, где их надо может быть провести, вот здесь. Давайте
я поставлю галочку. Вот. И вот здесь. Давайте я поставлю две галочки. Понятно ли, что надо делать
с каждым из этих этапов? Вот. Или не очень понятно? Пожалуйста, поясните. Наверное, не очень понятно. Не
очень понятно. Да. Ну, смотрите тогда, давайте. Значит, вот как мы получили y со звездочкой? Получаем
y со звездочкой. Мы берем g штрих по y от x, y. Вот. Ну, градиент. Понятно, что все, что зависит от x,
у нас улетает благополучно. От квадратичной формы у нас остается 20y. Вот. От линейной функции у нас
остается плюс два, что тут еще остается-то? x транспонирован bx, я понимаю, да? Вот. Равно нулю. Вот.
Ну, отсюда y со звездочкой, вот он равен блестящим образом минус c в минус первый bx. Вот. Понятно ли,
что здесь произошло? Да. Очень хорошо. Вот. Ну, теперь давайте это все добро подставлять. Так. Тут, видимо,
мне нужно немножко сжать, чтобы все было видно. Ну, смотрите, x транспонирован аx плюс два, y
транспонирован. Ну, в первую очередь, минус появляется. Вот. Минус сейчас нам поможет. Что тут?
x транспонирован, b транспонирован, c минус транспонирован на bx и плюс. Ну, тут плюс останется,
потому что 2a минусы сократятся. x транспонирован, что там, b транспонирован, c минус транспонирован
на c на c минус первый bx. Вот. Такое выражение. Я надеюсь, я нигде не напортачил. Вот. Ну, сейчас
все должно сократиться моментально. Сейчас мы это поймем. Вот. Ну, что тут происходит? Ну, во-первых,
вот это в единичку уходит очевидным образом. Остается x транспонирован на ax минус 2 x
транспонирован на btc-tbx плюс. Что осталось? x транспонирован на btc-tbx. Вот. Чудо. Вот это
совпадает с вот этим. Поэтому это значит, что это все дело равно x транспонирован на a минус btc-t.
Ну, минус t уходит в минус единицу, потому что там симметричность есть. Насколько я понимаю,
да, симметричность там точно есть. Вот. Поэтому тут я потом давайте это допишу. b и x. Вот это уходит
в c минус первую, так как это симметричная. Вот. Вот получилось такое выражение, которое я вроде
где-то вы же там и записал. Стало ли понятнее? Да, спасибо. Окей, супер. Так, good. Это было, значит,
доказательство прошел комплимент. Вот. Да, это было только в одну сторону. Что делать в другую
сторону? В другую сторону там, по-моему, делается что-то от противного. Давайте тогда в другую,
в обратную сторону я в следующий раз с этого начну. Вот. А сейчас так, я тут хотел еще рассказать про
много чего. Да, давайте расскажу еще про всякие забавные вещи. Да, и тут, наверное, уже писать
ничего не надо будет. Топ шер и... А можете показать еще раз? Экран чуть-чуть пониже. Слушайте,
а давайте я выложу просто запись и все. Давайте, было бы вообще шикарно. Прекрасно. Все остальные,
кстати, тоже надо будет выкладывать. Что-то я это забыл об этом. Так, лекция. Да, вот, лекция. Вот.
Короче, задач MaxCut. Пояс максимального разреза граф. Я надеюсь, все в курсе, что такой граф и что
такое его разрез. Вот. И, собственно, надо найти, по сути, такой набор ребер, так чтобы сумма весов
была максимальной, и они разделяли бы вершины на два не пересекающихся множества. Вот. Собственно,
MaxCut формулируется вот таким вот образом. Видите, тут все дискретное. Вот. Мы с этой задачей еще поговорим,
встретимся ближе к концу курса, когда будем про выпукла и релаксации обсуждать, и сдача из
DP туда же пойдет. Вот. Идея в том, что мы суммируем. Собственно, у нас X это наши вершинки. Вот. И они либо
в одном ношусь, либо в другом. И нам нужно максимизировать сумму весов тех ребер, которые
их соединяют. Вот. Соответственно, если X одного знака, то эта штука зануляется, и мы вес не учитываем.
Если они разного знака, то эта штука отрицательна. Вот. Поэтому тут одна вторая стоит. Вот. На самом деле,
тут, возможно, надо еще... А, ну да, тут одна вторая стоит, и мы суммируем только, ну, понятное,
повторяющиеся пары мы не учитываем. Поэтому одна вторая достаточно. Понятная ли пословка задачи?
Вы слишком быстро. Почему выбираете одна вторая? Потому что, если X разного знака, то это все в сумме
дают двоечку. И нам нужно, ну, чтобы сумма весов была честная, нам нужно двоечку поделить. Ага. Так.
Еще к вопросу. Так. Ну, вопросов вроде нет. Окей. Вот это все можно переписать в матричной векторном
виде. Вот эту целевую функцию. Явным образом. Вот таким вот. Вот таким вот. Потому что, ну,
понятно, что max, тут стоит минус, поэтому мы переписываем в мин. А x, i, t, x, z, z на w и z,
это просто кадротичная форма. Вот. Понятно, что, если мы уберем здесь i и z, то тут появится одна
четвертая. Вот. Ну, в принципе, наверное, стоит вообще убрать эту константу и уже не мучиться с ней.
Вот. Соответственно, какой будет эквивалентный вид у этого выражения? Ну, мы можем воспользоваться
свойством следа и перенести x сюда. И записать, что это след от x, x-транспонированной w. Вот. Обозначив,
а, то есть, наоборот, w на x, x-транспонированной. Так будет лучше. Вот. Обозначим x, x-транспонированной
за большую матрицу, x большой. И скажем, что она у нас положительно полуопределена. Ее диагона
равна единице, потому что все x либо плюс, либо минус один. Ее ранг равен единице. Понятно ли,
почему этого достаточно? Алло, меня вообще слышно? Да, слышно. Так, это хорошо. А есть ли
понимание, почему такое-такая перезапись, она правомерна? Вот люди, почему-то начинают
писать, что вроде понятно. То есть, мы, по сути, ввели новое обозначение для матрицы ранга 1 и
навесили все необходимые ограничения на эту матрицу. Вот. А дальше домашнее упражнение — понять,
что множество матриц фиксированного ранга образует невыпуклое множество. Вот. И чтобы,
как бы, нам все-таки можно было как-то решить задачу методами оптимизации, мы просто берем и
говорим, окей, давайте мы откажемся от этого ограничения на ранг. Вот. Но получим какое-то
решение, а дальше будем смотреть, каким образом можно восстановить исходный вектор. Про то,
как он восстанавливается, мы посмотрим на одной из последних лекций. Там довольно
знаменитый алгоритм его восстановления существует, вероятно, знаете. Вот. Это про
MaxCAD и то, почему, как связана задача с ДП, с этой задачей. Вот. Так. Вопросы какие-нибудь есть
по тому, что мы только что проделали? Так. Вопросов вроде бы нет. Ну ладно. Так. Ну и последнее,
что хочется показать, красивую картину, конечно же. Вот. Смотрите. Такая задачка, короче, есть набор
точек. Вот. И хочется построить такой эллипсоид, чтобы его площадь была минимальна, но он покрывал бы
все эти точки. То есть, короче, хочется получить вот это. Понятно ли задача на неформальном уровне?
То есть, как из вот такого набора точек получить вот такую картинку? Все. Замечательно. Отлично. Я
понял. Замечательно картинках объяснять. Кажется, это гораздо быстрее приходит понимание. Ну вот.
Теперь как это, собственно, происходит формально? Формально надо немножко поднапрячься. Поскольку
у нас, то есть, мы хотим найти эллипс. Эллипс – это трансформированный круг. Площадь эллипса,
она параметризуется детерминатом матрицы преобразования обратной, которой был преобразован
шарик. Помните такое в матанализе было, да, что там типа мы замены переменных делали в интеграле,
и надо было на детерминат матрицы якобы умножить. Было такое. Ну вот. Есть та же самая история.
Поскольку мы хотим сделать минимальную площадь, то нам надо минимизировать детерминат обратной
матрицы. Вот. Детерминат является выпуклой, не вогнутой функцией. Тоже домашнее упражнение,
проверьте, что это так. Вот. Однако его логорифм, его логорифм – это монотонная функция, поэтому
точку минимума это не изменит. Вот. Но минус логорифм детермината – уже выпуклая функция. Мы это вроде
бы даже показывали на одной из прошлых лекций. Поэтому вместо… Поэтому, по сути, в чем задача
заключается? Нам нужно найти такую матрицу и такой вектор B так, чтобы логоринат детермината,
который как бы прокси-функция для площади, был поменьше. При этом матрица была положить на
определенной. А все точки, которые лежат в эллипсе, то есть для которых вот это вот… Ну то есть то,
что они лежат в эллипсе, означает, что, грубо говоря, вот эта штука меньше единицы. Вот. Единица
здесь выбрана чисто условно, потому что мы всегда можем отшкалировать, поделив на какую-нибудь
константу, для того, чтобы, ну как бы, было это нераянство выполнено. Вот и все. То есть мы,
по сути, минимизируем логорину детермината. То есть минус… Ну, логорина детермината в минус
первой – это минус логорины детермината A. Понятно. Я надеюсь. Понятно же, да? Так. Детермината в
минус первой – это там единичка на детерминат, поэтому логорифм… Да, все. Минус… Минус вроде
понятно. Вот. Поэтому мы это минимизируем. И вот получаем выпуклую… Задачу выпуклой оптимизации,
которая сходу не очень понятна, к чему относится. Ну, судя по тому, что есть такое ограничение,
это будет СДП, очевидно. Но там будет также конус со второго порядка. То есть это такой будет
микс конусов, который будут каким-то образом взаимодействовать, и таким образом финальный
конус, который будет получаться, будет позволять так изоще решать. Вот. Поэтому, в общем, получается
вот такое решение, довольно красивое. Вот. Применение у этой достаточно геометрической
постановки следующее. Ну, обычное. Какое приводит? Наверняка есть много разных других. Ну, первое,
там это вся в какой-нибудь статистике. Там надо что-то оценить разбросы, где кто находится. Вот.
Из какой-то более-менее инженерии это типа у вас есть набор этих потребителей. Ну, типа мобильной
связи, например. Вот. И вам нужно понять, куда поставить там, условно, эту вышку. Вот. Так,
чтобы минимизировать потребление энергии. Но так, чтобы все потребители были в охвате этой
самой вышки. Вот. Ну, вот как бы понятно, что построив такой эллипс, вы можете понять, где у
него какие-то фокусы, центры. Вот. И таким образом оценить, сколько, какая мощная сигнала вам нужна
для того, чтобы покрыть всех потребителей. Вот. Ну, я надеюсь, понятно ли пример на приложение? Ну да,
вот кому-то понятно. Это прекрасно. Наверное, все остальные уже убежали на следующую пару. Хотя,
да, сейчас уже будем заканчивать. Так. Два человека что-то осознали. Все понятно. Это прекрасно. Вот.
Так. Это я уже сегодня сказал. Да. Ура. Мы вроде добили эту лекцию. Тогда, значит, следующая лекция
будет самой такой, наверное, тяжелой и теоретически нагруженной. Вот. Плюс меня в обратную сторону этот
факт. Вом от противного как-то все делается. Сейчас сходу не хочу отображать. Вот. Лучше в следующий
раз аккуратно все покажу. Вот. В следующий раз на чем-то слое оптимальностей. Вот. Я надеюсь, как-нибудь
и двойцами скоснемся. Вот. Поэтому пока что мы более-менее, я надеюсь, освоились тем, что
так выпуклые задачи. Вот. Как они могут выглядеть. Какие конусы с ними связаны. Как они друг к другу
переходят. Вот. То есть, в следующий раз смотрим, как их решать. Через раз там двойственность. Потом
солверы. И потом начнем уже методы. То есть, да, плюс неделя, немножко надержки. Но я надеюсь,
что на условиях оптимальности. Я самое сложное доказательство выложил в PDF. Вот. Поэтому там
мы и двойственность немножко захватим в более как бы активном образе. Все. Всем спасибо, что
подключились. Я постараюсь в следующий раз как-то купировать все эти разрывы,
которые довольно раздражают сильно. Вот. И постараюсь, чтобы была более гладкая связь.

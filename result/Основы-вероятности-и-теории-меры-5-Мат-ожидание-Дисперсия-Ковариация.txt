В общем, да, можно начать.
Вначале я всё равно напомню, что было прямо от ожидания
уже в прошлый раз.
Мы предполагаем, что вероятность на пространстве или конечная,
или в крайнем случае не более чем счётная, и тогда
математические ожидания случайно увеличены.
Букавка E обозначается, называется такая сумма
по всем исходам.
Я, напомню, случайно её лечена, это функция из вероятностного
пространства в длительные числа.
Я в прошлый раз не сказал, но вот если элементарных
исходов счётное число, и тут написан ряд, надо
ещё требовать, чтобы он абсолютно сходился, потому
что мы, наверное, хотим, чтобы математическое
ожидание не зависело от порядка нумерации элементарных
исходов, поэтому, если он только условно сходится,
то мы говорим, что математическое ожидание не существует.
Всё, в прошлый раз я показал формулу
о том, что математическое ожидание можно посчитать
как сумму значений случайно увеличены на вероятность
этого значения по всем возможным значениям.
Я через кси от омега обозначаю множество значений случайно
увеличены.
Когда вы у меня спросили, почему обреление именно
такое, почему среднее арифметическое, а не геометрическое или
там среднее квадратическое или не медиано, я сказал,
что оно такое, потому что у математического ожидания
есть много всяких классных свойств.
Вот, хорошо бы сейчас их сформулировать и доказать.
Этим я и собираюсь заняться.
Нумеруем.
Если случайно увеличина не отрицательная, то и математическое
ожидание не отрицательное.
Я иногда буду доказывать, иногда буду просто как-то
пояснять.
Тут, я думаю, можно просто посмотреть сюда и сказать,
что если слагаемые положительные, то, наверное, и сумма положительная.
Второе свойство, самое главное свойство – линейность.
Если мы умножаем на константу случайно увеличенную, то
и математическое ожидание умножается на константу.
И математическое ожидание суммы – это сумма математических
ожиданий.
Так, на всякий случай, вот эта буквка называется
КСИ, вот эта буквка называется ЭТО, и это буквы греческого
алфавита.
Почему-то на экзамене все вот эту букву называют
буквой ню, я не знаю, почему.
Это свойство можно, наверное, даже доказать.
Ну, с константой совсем все просто, да, можно просто
из суммы вынести константу наружу.
А вот линейность я распишу.
Это по определению левая часть.
Вот, ну тут нам надо, наверное, еще прокомментировать,
что такое сумма случайных величин.
Во-первых, если случайные величины действуют из
разных вероятностных пространств, первая из Омега-1, а вторая
из Омега-2 в Р, то их просто складывать нельзя, да?
Когда я пишу сумму, я подразумеваю то, что случайные величины
действуют из одного и того же вероятностного пространства,
и тогда я их могу складывать просто поточечной, как
функции.
Вот.
Тогда по определению суммы случайных величин тут
написано что-то такое.
Ну а теперь я могу перегруппировать солгаемое и разбить в две
суммы.
Первая сумма от ожидания КСИ, вторая сумма является
от ожидания ЭТО.
И вторая сумма от ожидания КСИ, вторая сумма от ожидания
что если вот это существует и это существует, то тогда
вот это существует.
Ну, еще раз, вы можете считать, что сейчас вероятностное
пространство конечное, и тогда никаких вопросов
о сходимости вообще не возникает.
Все эти свойства, монотонность, если одна случайная величина
больше другой, то и от ожидания тоже больше.
Это комбинация первого и второго свойств, потому
что из того, что КСИ больше, чем это, следует, что разность
больше нуля.
Из того, что разность больше нуля, следует, что мотождание
разности больше нуля.
Мотождание разности, это разность мотожданий как
следствие линейности.
Больше нуля, это мотождание разности.
Если разность мотожданий больше нуля, то тогда первое
больше второго.
Тут ничего интересного.
Четвертое свойство будет про связь модуля мотожданий
и мотождания модуля, или по-другому это еще называется
неразница треугольника.
Доказательство довольно простое, модуль суммы не
больше, чем сумма модулей.
Мы существенно используем, что математическое ожидание
элементарного исхода положительно, значит его
модуль, это просто есть вот эта вероятность, то
есть модуль можно вот так перекинуть.
И получается мотождание модуля кси.
Пятое свойство обычно называют нераненством к ошибке Буниковского.
Квадрат мотождания произведения не больше, чем произведение
мотождания квадратов.
Причем можно даже сказать, когда выполняется равенство.
Равенство выполняется, когда случайные причины
линейно-зависимы.
Причем равенство, если, а тогда и только тогда, когда
есть линейно-зависимость.
Ну что то же самое, что или одна из них прожестная
или кси можно выразить через это умножением на
константу.
Доказательства.
Посмотрим произвольное t действительное.
Не сложно понять, что квадрат случайной причины t кси
минус это не отрицательный для любого t.
Теперь я использую свойства математического ожидания,
что такое мотождание вот этого квадрата.
t у меня константа, t квадрат константа, тут сумма случайных
причин.
Могу написать, что это равно.
Я еще тут немножко странно написал, что вот например
t квадрат вынес константу не сюда, а вот сюда.
Я это сделал для того, чтобы было очевидно, что тут написан
квадратный трехчлен от переменной t, ну или возможно
выраженный квадратный трехчлен.
Я про него знаю, что он всегда больше лидерально
нуля.
Это как учат детей в восьмом классе означает, что дискриминат
неположительный.
Давайте я это напишу.
Вот это дискриминант, и он меньше и равен нуля.
Если перенести вот эту штуку в правую часть, получится
неравенство к ошибнику.
Теперь нужно разобраться с тем, когда достигается
равенства.
Для этого надо вспомнить, когда бывает так, что дискриминант
равен нулю.
Ну во-первых, такое может быть, если у нас квадратный
трехчлен просто вырожденный.
То есть вот эта штука равна нулю, вот эта равна нулю.
У нас просто тут написано константы больше нуля.
В этом случае, первый случай, вырожденный.
Ну от ожидания x квадрат тогда тоже равно нулю.
Осталось понять, что такое, когда такое вообще бывает,
что от ожидания равно нулю.
Ну от ожидания это сумма x омега квадрат вероятность
омега, и это почему-то равно нулю.
Тут все слагаемые не отрицательны, значит все они должны быть
просто равны нулю.
Каждым слагаемым или вот этот множество равен нулю,
или вот этот множество равен нулю.
Это значит то, что везде, где случайная величина
не равна нулю, вероятность таких исходов равна нулю.
По-другому, это можно переписать как вероятность, что кси
равна нулю, равна единице.
Кси равна нулю с вероятностью 1.
Или еще в теории вероятности говорят, что тогда кси равна
нулю почти наверно.
В теории вероятности почти наверно это термин, и он
означает то, что написано на доске.
И тут мы возвращаемся, что я не совсем точно сформулировал
пятые свойства, потому что вот тут тоже надо написать
почти наверно.
То есть случайные величины, они почти наверно линиенозависимы.
Ну это альфа-бета, почему?
Надо еще написать, что альфа-бета не равно нулю.
В первом случае мы выяснили, что случайная величина
кси почти наверно равна нулю, и тогда она понятна
линиенозависима с это.
Кси, например, кси плюс нулю умножить на это равно нулю
почти наверно.
Бета равно нулю, альфа равно единиц.
Итак, выраженные случаи разобрали, осталось разобраться
с невыраженным случаем, то есть когда там действительно
квадратный трехчелем.
Тогда тот факт, что дискриминант равен нулю, означает, что
у него есть корень.
То есть существует такое t, что математическое ожидание
t кси минус это в квадрате равно нулю.
Так же, как и в предыдущем случае, это означает, что
вот эта случайная величина, она равна нулю почти наверно.
Ну вот мы видим, что ты кси, и это линиенозависима.
Пятое свойство доказали.
Шестое свойство.
О том, чему равна, равно математическое ожидание
индикатора.
Я сейчас напишу, а потом объясню, что здесь написано.
Если для любого события из f, для любого события можно
определить понятие индикатора.
Этого события, индикатор, это будет случайно величина
определенной следующим образом.
Она равна единице, если линитарный исход попадает
в событие, и нулю в противном случае.
Кроме того, если случайная величина принимает значение
только 0 или 1, то можно то множество значений, где
то множество исходов, на которых случайная величина
принимает значение 1, обозвать множеством a и получить,
что случайная величина являетсяaron.
то множество значений, где
то множество из кодов, на которых случайная величина принимает значение 1
обозвать множеством a и получи, что
случайная величина является индикатором какого-то события
То есть индикаторы в Iowa oriented, это просто
случайная величина, которая принимает два значение 0 и 1
свойство 6 이� chaque даёт то, attempt
математически ожидания индикатора
равно его вероятности. Вероятности события a
Доказательства. Тут я воспользуюсь уже не определением, а вот этой формулой.
Индикатор принимает два значения, 0 и 1, поэтому надо написать, что индикатор
математически ожидания индикатора это 1 на вероятность, что индикатор
равен 1 плюс 0 на вероятность, что индикатор равен 0. Второе слагаемое
очевидно равно 0. Первое слагаемое равно 1 на вероятность, что индикатор
равен 1. Но вот тут сюда подходят ровно те исходы, которые лежат в событии А.
Здесь и написано просто вероятность события А. Шестое свойство доказано. Я
предлагаю не делать перерыв, потому что у нас недавно лекция началась, кажется.
Я думаю, кто-нибудь против? Никто не против. Отлично. Следующее свойство касается
независимости. Пока у нас была только независимость событий. Вот. Независимость
событий можно это понятие перенести на независимость случайных величин.
Случайные величины называются независимыми, если для любых действительных x и y события вот
такие. x равно x, это равно y независимо. Можно на самом деле брать только x и y из
множества значений случайных величин. То есть x из x от омега и y из это от омега.
Поскольку, если событие имеет вероятность 0 или 1, то оно независимо с чем угодно. Поэтому
можно проверять только вот такие x и y, которые в принципе принимаются. Также надо сказать,
что это опять же в общем случае будет неправильное определение, как это обычно бывает. Оно работает
только для дискретного случая. Вот. И еще когда мы говорили про независимость событий, когда
событий несколько, мы говорили про попарную и в совокупности. Тут тоже можно говорить про
попарную независимость случайных величин и про независимость совокупности случайных величин.
Ну, попарная независимость случайных величин, я просто скажу, что случайные
величины попарно независимы, если любая пара независима. Я не буду этого писать, это и так
понятно. Я напишу, что такое независимость совокупности. x1 и так далее, xn независима
совокупности. Даже не обязательно. Пусть будет конечное число. Нет, я все же напишу, что их
может быть бесконечно много, на всякий случай. Если любого x1, для любого x2, для любого x3 и так
далее. Событие x1 равно x1, x2 равно x2, x3 равно x3, и так далее. Независимость совокупности.
Мне кажется, очень логичное определение. Опять же, можно брать только те x,
которые принадлежат множеству значений ксиитах. Я не помню, давал ли я определение независимости
совокупности бесконечного числа событий? Если событий бесконечно много, то они называются
независимыми совокупности, если любой конечный поднабор независима совокупности. Седьмое
свойство. Если случайные величины независимы, то математическое ожидание произведения равно
произведению математических ожиданий. Догадательства. Для догадательств я буду пользоваться не
определением математического ожидания, а формулой. Обратное неверно. Да, я приведу к вам
пример. Но сначала докажу. По определению, нет, не по определению, а по формуле математического
ожидания произведения это такая сумма. Я качестве переменной выберу не x, а z для удобства.
z из множества значений.
А теперь надо подумать, что значит, что
произведение случайных величин равно коммунточислу z. Это означает, что x равно коммунточислу x,
это равно коммунточислу y, и произведение x, y равно z. Причем, понятно дело, эти события не
пересекаются, поэтому я могу расписать, что вот эта вероятность равна сумме следующих
вероятностей. Вероятность, что xi равно x, это равно y по всем x, y таким, что x, y равно z. Что я
делаю дальше? Во-первых, если я здесь суммирую только по таким x, y, то я вот этот z могу заменить
на x, y, а во-вторых, вот эту вероятность я могу заменить на произведение вероятностей,
используя определение независимости. Нехорошо, да-да, то есть надо сначала z засунуть вот сюда,
а потом заменить на x, y. Правильное замечание. Дальше я замечу, что если я перебираю все возможные
значения x, y, то я получаю все возможные значения z. То есть, если мне не важно порядок в программах,
я могу написать, что это просто сумма по всем x, сумма по всем y. Вот такой штуки. И осталось
просто разложить на множители. У меня теперь переменные x и y независимые,
значит это просто произведение вот такой суммы на вот такую сумму.
Первая сумма равна математическому ожиданию x, вторая математическому ожиданию это. Теперь
я приведу пример, когда в обратную сторону не работает. В общем, случайно я начну x,
я возьму так то, что с вероятностью 1,4 она будет принимать значение плюс-минус 1,
а с вероятностью 1,2 она будет принимать значение 0. А случайную личину это я определю как квадрат
случайной личины x. Кто понимает, чему равно мотождание x? Нулё. Ну это очевидно,
потому что просто распление симметричное. Так, а что нам надо проверить-то? Нам надо проверить,
что мотождание x это равно произведению мотожданий. Правая часть уже равна нулю,
потому что здесь один из множеств равен нулю, а левая часть это мотождание x в кубе. Ну,
оно тоже равно нулю. Например, можно сказать, что просто x в кубе и x это то же самое.
Одна и та же случайная величина. Ну или можно опять же сказать, что x в кубе имеет симметричное
распление. В общем, вот это условие выполняется. Теперь надо догадать, что они зависимы. В принципе,
интуитивно это вроде как очевидно, да, то есть это зависит от x вот так. Но если формально,
то это надо доказывать. Чтобы это доказать, надо привести такие значения x и y, потому что вот
эта вероятность, я сразу буду писать кс квадрат, не равнялась вероятности
произведению вероятностей. Ну, например, можно взять x равно нулю, y равно нулю. Тогда слева написано
одна вторая, потому что это происходит только когда x принимает значение ноль, это происходит
вероятностей одна вторая. А справа тоже одна вторая умножена на вторая. Неравно. И последнее
свойство, которое я сформирую, еще одна формула, которая обобщает вот эту формулу.
Здесь phi это произвольная функция из r в r. Догадательство. Догадательство будет
почти такое же, как догадательство вот этой формулы. Это я написал определение. А теперь
я просто правильным образом группирую слагаемые. Для каждого значения x, величины,
величины кс, я сгруппирую слагаемые так, чтобы сгруппировать все слагаемые, где сиатомика равна x.
Опять нехорошо получилось. У нас омега здесь получается. Я прошу прощения. Надо, конечно же,
сначала писать вот так. Сумму по всем.
Теперь мы говорим, что раз сиатомика равно x, я могу вот здесь вот сиатомика заменить на x. И теперь я
могу phi от x вынести за скобку. И получается сумма phi от x на сумма phi от омега тут по всем
возможным x, а тут по тем исходам, где x от омега равна x. Но если мы суммируем вероятность всех
исходов, где x от омега равна x, то мы получаем вероятность того, что x равна x. Ура, формула
доказана. Теперь хотелось бы привести какой-то пример, как считать математическое ожидание.
Я для примера возьму биномиальную случайную величину. Для подсчета использую вот ту формулу с первой
доски. Я должен просуммировать по всем значениям. Биномиальная случайная величина принимает
значение от 0 до n, поэтому я ставлю k от 0 до n. Беру вот это значение и умножаю на вероятность
этого значения. Теперь нужно подставить. Вероятность значения вот такая. Теперь
надо как-то такую сумму посчитать. Что я сделал? Я во-первых скажу, что пусть сумма будет от единицы,
потому что если k на нулю, то тут с нулью слагаемая. А во-вторых, использую хитрую формулу k на cзн по k,
это n на cзн минус 1 по k минус 1. Если кто-то не знает, я думаю, можно просто расписать через
факториалы и сойдется. И по этой формуле получается n на вот эту c и на p вкатый 1
n благополучно выносится за скобки, остается сумма. Давайте я еще за скобки вынесу p,
чтобы здесь стояло k минус 1. c из n минус 1 по k минус 1 пока от единицы до n. Тут будет p в k минус 1,
а тут будет 1 минус p. Я 1 минус p напишу как n минус 1 минус k минус 1. И видно,
что вот здесь образовался бином ньютона. Если k минус 1 обозвать теперь за новую переменную,
то эта перемена будет как раз от нуля до n минус 1, и по биному ньютону здесь все
соберется в p плюс 1 минус p в n, то есть в единицу. Итого ответ получился np. В каком-то смысле
можно было и проще это сделать, потому что ответ-то простой. Наверное есть какое-то
простое объяснение, почему просто n умножить на p. Другой взгляд на вещи. Мы понимаем, что биномнальное
распление берется из схемы с витанием b0. То есть у нас есть какое-то вероятностное пространство,
где 2 вены исходов. Исходы это последовательность из нулей единиц. Вероятность одного исхода это
p в степени количества единиц на 1 минус p в степени количества нулей. И кси это количество единиц.
Давайте рассмотрим, для любого k, рассмотрим событие окаты, которое заключается в том, что
на окатом месте стоит единица. Тогда я утверждаю, что кси будет равна сумме индикаторов событий окаты.
Ну действительно, количество единиц, это как бы надо сложить несколько единиц, где каждый единица
будет соответствовать единице вот здесь. Я надеюсь, что я сказал что-то понятное сейчас. В общем,
это надо просто осознать. И тогда мы можем почитать математическое ожидание кси используя свойства
линейности. Можно сказать, что мотождание кси это мотождание суммы индикаторов. А мотождание суммы
это сумма мотожданий. А мотождание индикаторов по шестому свойству это вероятность события. То есть тут написано сумма
вероятности событий окаты. И осталось просто сказать, что вероятность окаты равна p. Ну на интуитивном
уровне это очевидно, потому что события окаты это события, что на катом броске случился успех. У нас
вероятность успеха равна p, поэтому должно быть, наверное, вот так. Но если совсем строго, то мы же
вероятностное пространство определили просто как множество исходов и набор их вероятностей. Поэтому
формально вот это равенство надо проверить. То есть надо написать то, что это сумма вероятностей всех
элементарных исходов, для которых xkt равно 1, а дальше вот эту сумму посчитать по binom-utom и получится p.
Я думаю, стоит подробнее написать то, что эта сумма xkt равно 1 можно суммировать по всем остальным x.
Суммирование по всем x кроме xkt и тут будет p в степени их количества на 1 минус p в степени их
количества n минус их количества. Вот, только вот здесь надо еще прибавить единичку, потому что мы не
учитываем теперь xkt. p выносится за знак суммы и тут по binom-utom получается единица.
Окей, то есть вот здесь каждым слагаемым написано p, n раз складываем p и получаем np.
Дисперсия, можно начать про дисперсию. Математическое ожидание это конечно очень хорошо, но вот бывает так,
что случайная личина константная, просто всегда рано нулю, а бывает так, что она с вероятностью 1 вторая
принимает миллион, а с вероятностью 1 вторая принимает значение минус миллион.
Математическое ожидание здесь и здесь будет равно нулю, но вообще тут как бы большая разница, то есть если у нас вот
такой вот доход и если у нас вот такой доход, это наверно совершенно разные вещи. Поэтому хочется
иметь какой-нибудь еще параметр, чтобы измерять отклонение случайной личины от среднего значения.
В качестве такого параметра можно рассматривать дисперсию, которая определяется как математическое ожидание
квадрата от отклонения. Вот тут сразу можно вывести формулу,
раскрываем квадрат.
Раскрываем пленейности.
Двойку забыл, точно. Спасибо.
Еще есть ошибки? Это был тест на внимательность.
Вот, осталось понять, что с этим делать. Ну, вот ожидание квадрата кси, так оно и останется.
Дальше вот здесь двойка и мотождание. И то и то это просто константы. Константы можно выносить
по линейности. То есть получается минус два мотождания кси умножить на мотождание кси.
Или что то же самое, что то же самое удвоенный квадрат мотождания.
Еще вот здесь вот мотождание, квадрат и мотождание. Но мотождание это константа,
квадрат тоже константа, значит тут просто мотождание константы. А мотождание константы
это просто сама эта константа. Получилось вот так. Можно что-то сократить, наверное.
И получилось какая-то формула. Обычно при подсчете удобнее ее использовать, чем вот это.
Какие у нас есть свойства? Первое свойство это не отрицательность, что очевидно, потому что это
мотождание чего-то положительного по определению. Второе свойство такое. Во-первых, дисперсия не
меняется, если сдвинуть на константу. Но это связано с тем, что если прибавить к
случайной веществе константу, то как бы тут она прибавится, а тут она вычтется из мотождания.
Точнее к мотожданию тоже прибавится. Короче, вот эта разность не изменится. Значит действительно
дисперсия не поменяется. И еще можно выносить константу, только константа будет выноситься со
знаком квадрата. Ну нужно вот здесь квадрат, значит там тоже должен стоять квадрат. Для того чтобы
сформулировать третье свойство, мне понадобится объяснить, почему дисперсия определяется именно так.
То есть если мы говорим про отклонение среднего, то мы могли бы, например, сказать, что дисперсия
это мотождание модуля разности. Почему это? Почему не так? Это же логично было бы. Вот. Правильный ответ
такой. Дисперсия определяется так, потому что так она обладает хорошими свойствами и с ней удобно работать.
Определение к вариации случайно личной кси и это называется
вот такое мотождание.
Вот. Какие свойства есть у к вариации? Во-первых, если мы поставим кси равно
это мы получим дисперсию. Ковариация кси-кси это дисперсия. Во-вторых, ковариация будет
являться симметричной линейной формой, чтобы это не значило. Я думаю, это сочетание уже на втором курсе можно
понимать после курса линейной алгебры. Может и не. Я напишу, что это значит. Значит, что, во-первых,
ковариация кси-это это то же самое, что ковариация это-кси симметричность. А во-вторых, она билинейна,
то есть линейна по каждой координате. То есть ковариация акси-это равна а на ковариацию кси-это,
то есть можно константу выносить. А во-вторых, линейность.
То есть свойства очень похожи на свойства скалярного произведения.
То есть можно воспринимать ковариацию как скалярное произведение случайных величин. Особенно если посмотреть еще на первое свойство и
соединить с первым свойством дисперсии, получится, что ковариация кси-кси не отрицательная, симметричная и билинейная.
Вот. А дисперсии это ковариация кси-кси. В линейной алгебре это называется квадратичная форма со всеми вытекающими последствиями.
А какие эти вытекающие последствия мы узнаем на следующей лекции. А сейчас закончилось.

Так, у нас остался небольшой должок с вами с прошлой
лекции.
Сейчас я тут подсотру немного.
Ну ладно, немного подстёр.
Должок-то какой, напомните, или мне самому вспоминать.
Ну что-то я помню, там была вот такая сумма по t от
2 до k-1, которая вот такая, c из n по k на c из k по t на
c из n минус k по k минус t, на одну вторую степень 2
т, минус c из t по 2.
Как вам такое?
Хотите так на экзамене помнить?
Нет, ну там просто действительно всё легко, мы там считали,
нас было множество из n вершин, мы фиксировали какие-то
Потом из них выбирали t штук общих и потом добавляли
ещё одну подсордельку размера k.
Нам нужны были пары множеств мощности k, каждая из которых
независима в случайном графе, поэтому мы сначала
из n выбираем k, потом из k выбираем t общих, из n
минус k выбираем k минус t оставшихся, а это вероятность
того, что оба множества пересекающихся по t-вершинам
являются независимыми, это можно в уме посчитать,
поэтому я на самом деле не помню, а воспроизвожу.
Друзья, это вам лайфхак такой на экзамен.
Вот если поймёте как следует, что делается, то всё хорошо.
Что я обещал доказать?
Я обещал доказать, что если в качестве t взять двойку,
взять двойку, то получится как раз та симптотика, которая
анонсирована, то есть c из n по k на c из k по 2.
На c из n минус k по k минус 2, на одну вторую в степени
2 c из k по 2 минус 1.
Чему а симптотически должно равняться?
Вот это я сейчас не помню, но там через мю выражение
какое-то, помните, ну подскажите мне, там через мю должно
быть выражение.
Как k в четвёртой мю в квадрате делить на, ну там м, наверное,
было, да, я его перевозначил через n, да?
Ну давайте я уж, чтобы вас не путать, здесь напишу
тогда м, здесь тоже м, это я подзабыл.
Ну k1, конечно, да, ну ладно, пусть будет k1, мне ужасно
противно его таскать, но было бы k, это k1.
Можем переобозначить k1, здесь м, здесь k1, здесь k1,
а здесь м в четвёртой, да, вот так, в квадрате, вот
так.
Вот нам нужно это просто проверить, но это совсем
легко.
Давайте вспомним, что мю это c из m по k1 на 1 вторая
c из k1 по 2, это правильно, да, такое же у нас мю, но
мю это просто мат ожидания числа независимых множеств
на k1 вершина, поэтому оно, конечно, такое, то есть фактически
это k1 в четвёртой на m в квадрате на c из m по k1 в квадрате
на 1 вторая 2 c из k1 по 2, я в квадрат возвёл, так только
поделить что ли, а нет, это мю в квадрат, всё правильно,
мю в квадратном числителе, да, поэтому делить не надо,
надо умножать.
Всё, ну надо просто разделить левую часть на правую и
убедиться в том, что в пределе будет 1, ну сейчас разделим,
у нас получается, так тут м тоже, у нас получается,
что надо разделить, ох, как же это лучше написать,
о, что тут очевидно, c из m по k1 в квадрате, если в
знаменатель пошёл, давайте напишем вот так, c из m по
k1, это я разделил на квадрат, вот эту первую степень,
вот так получилось первая степень в знаменателе,
дальше у меня будет c из k1 по 2, одна вторая, да, точно,
ну давайте сейчас посчитаем, вот это нужно, там правильно
написано, что одна вторая, потому что там было дельта
пополам, дельта вычисляла количество упорядоченых,
вот оно здесь такое пополам, вроде как действительно
одна вторая, ну давайте пока посчитаем без одной второй,
и я поправлю, если это потребуется, наверное поправлю, действительно
вы правы, ну если одна вторая, ну давайте нарисую одну
вторую, ладно, видимо вы правы!
Ну тогда если одна вторая, тут надо что сделать, убрать
что-ли эту минус единицу?
Ну, вроде как на одну вторую в первую я умножаю.
Что-то меня немножко это смущает.
Ну, если убрать, тогда вообще вот шлёп-шлёп.
Никакой одной второй вроде не остаётся.
Интересное кино.
Ну, сейчас посмотрим.
Сейчас будем разбираться.
C из K1 по 2 дальше.
Здесь C из M минус K1 по K1 минус 2.
И надо ещё разделить на K1 в четвёртый и умножить
M в квадрате.
Так, ну, сейчас я сократил правильно, да?
То есть, если считать, что здесь стоит одна вторая,
то одна вторая умножается вот на эту минус единичку
на одну вторую в минус первой, и остаётся просто одна
вторая в степени 2C из K1 по 2.
Ну, делим.
Вроде получается.
Так, что же у нас происходит?
Ну, понятно, что K1 стремится к бесконечности, потому
что это удвоенный двоичный логарифм, он куда-то стремится.
Поэтому C из K1 по 2 асимптатически это K1 в квадрате пополам.
Так, дальше будет M в квадрате, то, что очевидно, потом я
оставлю пока что C из M минус K1 по K1 минус 2.
Так, а здесь C из M по K1.
Это… и K1… а, K1 в четвёртый я ещё не нарисовал, что ли.
K1 в четвёртый.
Так, хлоп-хлёп.
Это всё неплохо.
Давайте вот эту дробь посчитаем отдельно.
Хотелось бы, чтобы она получилась такой, как вы видите.
То есть, K1 в квадрате, наверное, умножить на 2 и поделить
на M в квадрате.
Так, ну давайте.
Ничего тут сложного нет, но всё-таки надо написать.
C из M минус K1 по K1 минус 2 поделить на C из M по K1.
Это M минус K1 M минус K1 минус K1 плюс 2 плюс 1.
Бяк это как… поделить на K1 минус 2 факториал.
А здесь M, M минус 1, M минус K1 плюс 1 и умножить на K1 факториал.
Так, дорогие друзья, вы поняли, почему я закончил вверху в числителе
именно таким саммножителем, каким закончил?
И вообще прав ли я?
Может, я ошибся.
Ну как устроено произведение?
Начинается оно с M минус K1, а заканчивается разностью
вот этой величины и вот этой, но увеличенной на единичку.
Но вот это написана разность, и она ещё увеличена на единичку.
На самом деле, дорогие друзья, поднимите руки.
Кто понимает, что вот этот числитель асимпатически равен
просто M в степени K1 минус 2?
Вот есть кто-нибудь, кому сходу очевидно, что числитель
это просто M в степени количества скобок?
Да, проблема в том, что K это в общем не константа.
То есть если бы K была константой, все бы мгновенно подняли,
я надеюсь, руки.
Если вы перемножаете фиксированное количество саммножителей,
из которых вычитаются какие-то константы,
ну ясно, что асимпатически это будет просто M в степени
количества саммножителей.
Но здесь K1, это важное замечание, да,
это асимпатически 2 лог 2-ичный M.
То есть это не константа, это тоже функция от M.
Тем не менее, я утверждаю, что эта функция настолько
медленно растущая, что это произведение все равно
можно заменить в асимптотике M-кой,
возведенной в степень количества саммножителей.
Но я так понимаю, что это как-то не очевидно, да?
Ну, можем сразу написать.
Я боюсь, что людей смущает не наличие вот этого индекса.
Тут мы можем, мы можем воспользоваться
просто результатом первой лекции,
в которой говорилось, что если вот этот верхний индекс C,
будучи возведенным в квадрат,
бесконечно мало по сравнению с нижним индексом,
ну давайте я действительно так напишу.
Смотрите, мы же знаем с вами, что C из N пока ведет тебя
в асимптотике, как N вкатый,
поделить на K факториал,
если K квадрат, это маленькая от N.
Вот знаем мы такой факт?
Друзья, это мы знаем, это самая первая лекция этого семестра.
Это очень простой, ну может вторая, не помню, но не важно.
Отсюда, конечно, следует, что C из M минус K1 по K1 минус 2
с хорошим запасом, это M минус K1 в степени K1 минус 2
поделить на K1 минус 2 факториал.
Действительно, можно так сказать.
И, соответственно, C из M по K1 асимптотически
это M в степени K1 поделить на K1 факториал.
Почему? Потому что K это логарифум,
и в квадрате он умал по сравнению с нижним индексом.
Это-то понятно, ведь, да?
То есть, конечно, такая симптотика есть.
Но тут мы вычитаем K1, это можно в свою очередь как переписать,
как, что надо сделать, надо M вынести за скобку.
То есть будет M в степени K1 минус 2,
а тут будет 1 минус K1 поделенное на M в степени K1 минус 2
поделить на K1 минус 2 факториал.
Ну и это равняется, давайте я уж честно напишу,
M в степени K1 минус 2 на K1 минус 2 факториал,
а тут будет E в степени минус K1 на M
K1 минус 2 на 1 плюсом малое от единицы.
Но обычная вещь я просто взял и представил основание,
как E в степени логарифум от него,
а логарифум раскрыл по первому члену ряда Тейлора.
Ряда Тейлора.
Слушайте, товарищи, привыкли к такой скорости? Нет еще?
Привыкли? Сливаете?
Ну и снова мы пользуемся тем, что K1 в квадрате сильно мал
по сравнению с M, это на самом деле повтор того же рассуждения,
которое давало вот этот результат.
K1 сильно мал по сравнению с M настолько мал,
что конечно вот это все асимпатически равно единице.
Тут стоит K1 в квадрате примерно, ну так это фигня.
Которая делится на M, все стремится к нулю,
а экспонента, стало быть, уходит в единицу.
То есть все это M в степени K1 минус 2
поделить на K1 минус 2 факториал.
Итого, возвращаясь сюда, можем вот так написать.
M в степени K1 минус 2 поделить на K1 минус 2 факториал,
умножить на K1 факториал и поделить на M в степени K1.
И у нас остается K1, K1 без единицы,
поделить на M в квадрате.
И это то же самое, что K1 в квадрате на M в квадрате.
Восемь точек, конечно.
K1 в квадрате на M в квадрате.
Ну и что у нас получилось?
Вот мне не нравится, что это одна-вторая все-таки вылезла,
которую мне напомнили.
Но вопрос в том, уверены ли вы, что здесь этой двойки нет?
Не, ну как так?
Что, вспоминать, как это шло в прошлый раз?
Рассуждение. Что?
Да вот мне тоже кажется, что мы просто зря эту одну-вторую добавили.
Сейчас, конечно, хорошо бы вспомнить концовку прошлой лекции,
потому что у нас все получилось с точностью до зачем-то добавленной вот этой одной-второй.
Знаете, какое дело?
Я вот боюсь, что проблема в том, что мы ее зря просто добавили.
Друзья, будем вспоминать, как заканчивалась прошлая лекция?
Или все понимают, что мы зря ее здесь добавили, и все хорошо?
Ну, это тоже верно. Конечно, вы сейчас не будете во всех деталях разбираться.
Ну, поскольку там будет какая-то консультация, если что, мы, конечно, можем к этому вернуть.
Но да, вот как-то вспоминать еще про прошлую лекцию в подробностях.
Конечно, зря мы ее тут добавили.
Я просто немножко вас запутал в тот момент, когда рассказывал сначала через пары в фигурных скобках,
потом через пары в круглых скобках, и вот эта одна-вторая, она паразитом таким появилась.
Я утверждаю, что она здесь не нужна. Я просто помню, что она здесь не нужна.
То есть тут должна появиться минус единичка.
Вот эта двойка пропадет, и действительно все сойдется к одному.
Главное понимать вот эту асимпатическую хрень.
А все остальное, как я сказал, это ненужное.
И все остальное, как я сказал, это ненужное.
А все остальное, как я сказал, это ненужное.
А все остальное, как я сказал, это ненужное.
Не хочу вас кокоть, мучать, считать там пяти равном тройке, четверки.
Хорошо?
Ну то есть у нас практически полностью доказан очень мощный результат,
но, конечно, остались какие-то аналитические фиговины, которые вы понимаете, для чего нужны асимптотики.
Вот люди учатся ими жонглировать, в том числе для того, чтобы не только примерно так проверить, что все получилось,
но и дальше убедиться в справедливости строго этого утверждения.
Так, я продолжаю с вами сейчас изучать случайные графы, но заканчиваю, наконец, с раскрасками.
Друзья, немножко резюмировать вам, что мы успели сделать, чтобы вы как-то общую картинку видели.
Мы много там разные техники изучили, по ходу дела были нетривиальные вещи, какие-то неравенства, зумы там, что-то такое.
А что на самом деле-то происходило?
Вот на самом деле, что происходило?
В течение значимой части семестра.
Ну что происходило? Мы сначала убедились в том, что у хроматического числа графа,
которая в общем играет очень большую роль, в том числе в приложениях, есть два типа оценок.
Одна через мощность В поделить на альфа, другая через амегу, это нижние оценки.
И быстренько поняли, что оценка через мощность В поделить на альфа гораздо лучше, если граф случайный.
Вы понимаете, с практической точки зрения граф не всегда случайный, конечно.
То есть если вам на вход на каком-то производстве или в каких-то задачах будут графы, то вам надо очень аккуратно посмотреть,
а вообще вы можете их считать случайными или нет.
Но я об этом говорил.
Если все-таки вы можете их считать абсолютно случайными, тогда одна оценка сильно лучше другой.
Дальше была иллюстрация совершенно офигенного следствия из того, что одна оценка лучше другой,
а именно была теорема о том, что бывают графы на свете, у которых сколь угодно большой обхват
и при этом сколь угодно большое хроматическое число.
Это в каком-то смысле иллюстрация на то, что мощность В поделить на альфа гораздо больше, чем амега.
Вот это то, что тогда произошло.
Тогда я сказал, ну слушайте, это здорово, конечно, а вообще мы можем как-нибудь вычислить хроматическое число?
Ну вот вычислили с помощью жадного алгоритма.
На примере числа независимости, которая как раз играет роль наилучшей оценки,
мы выяснили, что жадный алгоритм почти всегда работает не хуже, чем в два раза.
Ну правда, потом возникли примеры последовательности, которые совсем ужасные, но это уже как бы за скобками.
Тогда я сказал, слушайте, товарищи, давайте вообще разберемся, как могут себя вести хроматические числа случайных графов.
И мы довольно подробно в этом разобрались.
Поскольку это длилось несколько лекций, то вот я посчитал нужным сейчас произнести просто это резюме.
Мы, конечно, разобрались не во всех существующих ситуациях.
У нас были совсем маленькие p, потом p побольше, потом еще побольше, а потом вдруг, бац, мы перескочили на одну вторую.
Ну то есть там было вот так. Там было сначала совсем смешно, мало от 1 на n в квадрате, ребер нет, хроматическое число 1.
Потом было мало от 1n, но вот хроматическое число уже 2.
Потом было c поделить на n, вот с ним я не доказывал, но сказал, что хроматическое число 3.
Потом было n в степени минус альфа, альфа больше чем 5 шестых.
Мы доказали, что оно уже, ну мы знаем, что оно растет, и вы можете это проверить.
Но вот та растущая функция, которая отвечает за хроматическое число, вокруг нее оно очень хорошо концентрируется.
То есть принимает лишь, ну мы доказали, что четыре возможных значения.
А потом мы перескочили на одну вторую.
Ну друзья, ну мне кажется, что для курса дискретного анализа более глубоко в это закапываться было бы чересчур.
Вот поэтому я говорю все. Хроматическими числами мы картинку заканчиваем.
Хватит мне вас ими мучать. Больше у случайных графов мы их изучать не будем.
А теперь мы займемся другим очень важным свойством случайного графа, который называется связность.
И это с практической точки зрения, конечно, никак не меньше по значимости свойства.
Ну потому что связность случайного графа это вопрос о том, сохраняется инфраструктура где-то или не сохраняется.
Вы же помните, как я объяснял, что такое вообще случайный граф?
Как он, какой смысл он имеет? Вершины это какие-то сервера. Говорил я про это, да?
Между ними связи, они разрываются независимо друг от друга.
Знаете, вот они разрываются и вдруг граф развалился. Так это значит, что мы с одного сервера на какой-то другой вообще не сможем передать информацию, если граф перестал быть связанным.
Я понятно ведь выражаюсь, да? Поэтому вопрос о том, при каких условиях граф связан, при каких нет, какие в нем компоненты, как они устроены, это вопрос практически очень значимый.
Ну и теоретически, конечно, тоже очень красивый. Вот об этом мы сегодня и в следующий раз поговорим.
Отмечу, что если вы будете смотреть записи прежних лет, то я читал по-другому. Я сначала рассказывал про связанность, а потом уже переходил к хроматическим числам.
Сейчас я посчитал нужным более последовательно изложить всю историю с хроматикой и уже потом рассказывать про связанность.
Ну кроме всего прочего, это еще связано с тем, что я ведь прочитал часть графов в ОКТЧ. Этого раньше тоже не было.
Поэтому суммарно в течение года я успею больше, чем я успевал ранее.
Так, ну хорошо, значит, связанность случайного графа.
Вот сегодня все-таки будем от противительной суммы считать аналитические, но к ним придем содержательным методом, который, конечно, я постараюсь максимально аккуратно подробно донести.
Первая теорема, которую мы докажем обязательно, звучит следующим образом. Пусть вероятность ребра случайного графа это функция от числа ему вершин.
Ну уж к этому мы привыкли, она может быть функцией, а не просто константой.
Функция вот такого вот вида, целогариф натуральный n поделить на n, где c это константа, строго большая нуля.
Пусть вероятность ребра задается в таком, ну все равно как-то кажется, наверное, очень специфическом виде.
Но я сейчас все поясню.
Тогда имеют место два, по сути, противоположных утверждений.
Если, так, да, вот так давайте, если c больше единицы, да, если c строго больше единицы, то асимпатически,
почти, наверное, случайный граф связан.
А если c строго меньше единицы, то асимпатически, почти, наверное, случайный граф не является связан.
Не связан.
Ну это, конечно, не совсем прикладное утверждение, когда оно сформулировано в таком виде.
Потому что асимпатически, почти, наверное, это еще не значит, что при маленьких n вероятность большая.
Поэтому если мы хотим применять это на практике, то нужны какие-то более эффективные оценки скорости сходимости, вероятности к единице.
Об этом я сейчас скажу.
Но качественное утверждение красивое, потому что оказывается, что наличие или отсутствие связанности, то есть сохранение или утрата инфраструктуры в сети, обладает фазовым переходом, как говорят физики.
Кто-нибудь из присутствующих слышал выражение фазовых переходов?
Ну то есть вы физику-то не изучаете, но все-таки выражение стандартное можно даже в школе когда-нибудь произносить, я не знаю.
Ну понятно, лед и вода превращаются друг друга на нуле градусов.
Вот здесь роль нуля градусов играет функция логариф Мен поделительная.
Выше нуля, вот в этом смысле.
Умножаем на что-то хоть немножко больше единицы.
И граф почти, наверное, связан.
А ниже нуля, то есть С меньше единицы, мы на этот С умножаем, вероятность стремится к нулю.
Граф почти, наверное, не связан.
Такой резкий скачок на грани, на пороге, как говорят вот этой функции логариф Мен поделительная.
Вот почти, наверное, связанности к почти, наверное, не связанности.
Качественный смысл утверждения понятен.
Вообще вот это вот явление фазового перехода в проявлении каких-то свойств случайного графа, это очень характерная вещь.
Поэтому зачастую вот эти сложные сети, случайные графы, изучаются физиками в своих терминах.
И там получаются очень интересные тоже результаты и наблюдения.
Так, ну давайте я, прежде чем буду формально долго вот это доказывать, я еще кое-что скажу.
Во-первых, про практическую сторону применения.
Ну давайте я напишу теорема штрих.
Теорема штрих, я ее не буду доказывать, она легко получается из тех рассуждений, которые я проведу, но явные оценки это скучно.
Теорема штрих утверждает, что, например, если c больше либо равняется 3, то вероятность того, что g от np связан, случайный граф связан,
больше либо равна 1 минус 1n, забыл сказать, коль скоро n больше либо равняется 100.
Ну там 100 или 90.
Ну то есть при достаточно больших n мы можем стать явную оценку.
Ну еще c все-таки не совсем близко к единице, а, например, больше либо равно тройке.
Но я всегда абсолютно уже на протяжении многих лет привожу стандартный пример, вот просто я его помню наизусть, поэтому чего бы мне его не повторить.
Вот есть две тысячи тех самых серверов, раскиданных по всей стране, по всему миру.
Есть две тысячи серверов, ну две тысячи куда как больше, чем 100, да?
Вот смотрите, возьмем c равное тройке, тогда p у нас будет равно 3 логариф натуральный 2000 поделить на 2000, правильно?
Ну это можно посчитать, я думаю все присутствующие догадываются, что это точно не большая величина, что логарифум все-таки гораздо меньше, чем 2000, но я утверждаю, что это примерно одна сотая.
Там 0011 что ли?
Примерно одна сотая.
Смотрите, что такое p?
Это вероятность присутствия ребра в случайном графе, правильно?
То есть это вероятность сохранения связи, если идут атаки.
Вероятность уничтожения отдельно взятой связи, ну это 0,99 что-нибудь или 0,98 что-то такое.
Офигенно казалось бы высокой, то есть каждую отдельную линию мы теряем с вероятностью очень близко к единице, конкретно вот 0,98 или 0,988 что-то такое.
Понимаете, да?
Тем не менее, вероятность сохранения всей инфраструктуры, то есть возможности передачи информации любого сервера на любой другой.
Смотрите, как же?
1-1, 2000 это 0,9995.
Вы согласны, что это очень круто?
0,98 вероятность потерять отдельно взятую связь и 0,99 и так далее вероятность сохранить возможность передачи информации между любыми двумя серверами.
То есть с практической точки зрения это очень классный результат.
Ну а я докажу качественную версию, про которую все равно мне кажется, что вопрос-то остался.
Неужели никто его не задаст? Всегда задают.
Да, что будет при ц равном единице?
На этот вопрос я абсолютно всегда отвечаю теоремой два штриха, которым вы доказывать не будете.
Ну и его часто задают, но она без доказательства.
Она пойдет без доказательства в курсе.
Вот эта конкретная теорема, конечно она тоже без доказательства, но это простое упражнение вывести ее как следствие просто из тех выкладок, которые я сделаю в рамках пункта 1.
Она скучная, не нужная, но простая.
А теорема два штриха это довольно сложное утверждение.
Мы скажем вот так. Пусть п от n имеет вот какой вид.
Логариф mn плюс гамма поделить на n.
То есть я вам отвечу на вопрос, который я из вас вытащил клещами.
А отвечу я более сложно.
То есть вы-то меня спросили, что будет вот так.
Правильно?
С равно единице это вот так.
А здесь я еще константочку добавил.
Гамма может быть хоть положительной, хоть отрицательной.
Вычил может быть что-то, может прибавил.
Согласитесь, если гамма не равна нулю, это не то же самое, что вот здесь С равно единице.
Поэтому это как бы отдельная теорема.
Потому что не вся жизнь исчерпывается тем, что С равно единице, больше единицы или меньше единицы.
Можно сюда еще прибавлять или отсюда вычитать какие-то функции, которые бесконечно малы по сравнению с вот этой дробью, с этим порохом.
Ну вот я здесь, видите, прибавляю гамма поделить на n.
Она бесконечно мала по сравнению с логарифом m поделить на n.
Я надеюсь, что студентам уже понятно, в чем разница.
Так вот, тогда тоже можно ответить, как устроена симпатичная вероятность связности.
Вероятность того, что g от np связан.
Ну, я очень люблю этот момент, он такой зажигательный.
Стремится к е в степени минус, е в степени минус гамма.
Сейчас не зажгло, да?
Мне кажется, очень круто.
Двойная экспонента, да?
Двойная отрицательная экспонента, так скажем.
Ну, конечно, отсюда сразу следует ответ на тот вопрос, который я из вас вытащил.
То есть, если мы подставим гамма равная нулю, мы получим случай c равный единице.
Гамма равная нулю дает просто е в минус первой.
То есть, вероятность стремится не к нулю, не к единице, не к нулю, не к единице, а к е в минус первой степени.
Ну, а тут видно, что порог еще тоньше.
То есть, если в этой формулировке порогом была просто функция алгориф man потереть на n,
в смысле умножим на c больше одного, получится одно, умножим на c меньше одного, получится другое.
А здесь сам этот переход еще конкретизирован.
То есть, смотрите, если добавочка вот этой гаммы очень большое положительное число,
то е в минус гаммы это что? Почти ноль, правда?
Вот этот почти ноль со знаком минусов показателей экспонента это что?
Почти единица, правильно?
Ну, вот это как раз почти, наверное, связано.
То есть, когда гамма просто стремится к плюсу бесконечности,
мы все время будем получать не функцию, которая умножается на что-то строго больше единицы,
а функцию, которая симпатически равнала алгорифму n, не умножается ни на что строго больше единицы.
Тем не менее, мы видим, что когда гамма стремится к плюсу бесконечности,
мы получаем практически уже апн-связность.
А когда гамма стремится к минусу бесконечности,
то е в степени минус гаммы это что-то офигенно большое положительное,
и оно со знаком минус показателей экспонента дает практически ноль.
Вот такой вот очень тонкий переход внутри порога.
Почти, наверное, связанности к почти, наверное, отсутствию единицы.
Так.
Сейчас буду доказывать теорию.
Смотрите.
А, слушайте, не буду, да?
Нет, но еще 4 минуты есть или 3 до перерыва.
Давайте до перерыва просто обсудим.
Вот если мы, например, хотим развалить граф на компоненты,
то как это легче сделать, с вероятностной точки зрения?
Что легче пытаться отколоть от n-вершинного графа, чтобы
он развалился на компоненты?
Одну вершину, согласны, да?
Одну вершину это надо разорвать только n-1 связь.
А если вы хотите пополам его разорвать, сколько
связей надо разорвать, понимаете?
n-2 на 4.
Если у него n-вершин, и вы его хотите разорвать пополам,
то там будет n-2 на 4, как в 2-дольном графе в полном связи.
То есть, конечно, очень естественно пытаться отколоть одну
вершину.
И вот этот второй пункт, он в этом смысле доказывается
даже проще.
Ну, давайте просто да начну.
Что там?
x, а g – это число изолированных вершин в случайном графе.
Изолированная вершина – это вершина, образующая компоненту
такую одновершинную.
Из нее никаких ребер не идет, вот это называется
изолированная вершина.
Чему равняется математическое ожидание x?
Линейность в чистом виде, хотелось бы, наверное, услышать
от аудитории.
Правильно, да, n, 1-1, но только в n-1.
Может, так и было сказано, я просто не услышал.
Ну и хорошо.
Ну, у меня послышалось в степени n, но может в степени
n-1 или в n-1.
Понятно всем, почему так.
Фиксированная вершина не имеет ни одного ребра
в оставшейся, вот с такой вероятностью.
Так же складываем индикаторы, получаем.
Вот.
Ну, на самом деле, из этого сразу видно, откуда взялась
функция логариф мэн поделить на n, потому что 100 раз уже
проходили.
Это вот так выглядит.
Буквально сегодня такую штуку писали, 1-p, это e в степени
логариф мата 1-p, логариф мата 1-p – это –p, если p стремится
к нулю.
Ну и что надо подставить?
Вот надо подставить логариф мэн поделить на n, то есть
если мы вот сюда подставляем, c логариф мэн поделить на
n, это прямо угадывается, в уме можно сказать, то это
получается cn множить на n, видите, тут кок-кок, а e в степени
логариф мэн – это n, ну еще в минус с этой степени – это
n в минус с этой, не совсем в минус с этой, еще надо
на поправочку вот эту асимптатически равную единице
домножить, так ведь, да?
Но все равно ясно, что это стремится к бесконечности,
если c строго больше одного и к нулю, если c строго меньше
одного, или наоборот, наоборот, к бесконечности если c
строго меньше одного и к нулю, если c строго больше
одного.
Друзья, сейчас перерыв, ну это-то ясно, и это как-бы
создает интуицию, откуда взялась пороговая функция,
но формально все придется доказывать через пять минут
и это займет время.
Так, значит, дорогие друзья, ну действительно, из этого
формально ничего не следует, из-за того, что мат ожидания
стремится к нулю, следует, что асимптатически почти
наверное изолированных вершин нет, ну так мало
лишь, что их нет, а может есть какие-то другие компоненты,
это у нас только интуиция была, что проще всего отколоть
изолированную вершину, правда же?
А из того, что мат ожидания стремится к плюс бесконечности,
это мы обсуждали, когда треугольники считали в случайном
графе, тоже еще не следует, что сами эти изолированные
вершины появятся с высокой вероятностью, что надо считать
дисперсию.
Помните?
Помните?
Ну я сейчас напомню.
Проще доказывать пункт два, вот давайте пункт два,
в нем надо будет посчитать дисперсию, но реально итог
получится быстрее.
То есть в пункте два рассуждаем как с треугольниками вероятность
того, что х больше либо равно единице, давайте повторение
мат учения, я не буду ссылаться на ту лекцию, а еще раз произведу
эту смешную выкладку, она выглядит вот так, но вы
заодно поймете, что это действительно общий такой факт, который
можно использовать всегда, когда у вас случайная величина
это 0, 1, 2, 3, то есть она принимает неотрицательные целые значения.
Это единица минус вероятность того, что х не больше нуля,
ну не больше нуля равно нулю, это одно и то же, но мне
удобнее писать не больше, чтобы подогнать просто к
виду неравенства Чебышова, что это единица минус вероятность
того, что минус х больше либо равен нуля, это единица
минус вероятность того, что мат ожидания х минус х больше
либо равняется мат ожидания х, может быть у вас всплывает
в памяти такая технология, ну такая совершенно древиальная
вещь, конечно, меняем знаки, добавляем мат ожидания
и пользуемся неравенством Чебышова, это больше либо
равно единица минус дисперсия х, поделенная на квадрат
математического ожидания, то есть если мы докажем,
что вот эта дробь стремится к нулю, вот тогда мы докажем
пункт два, понятно ведь, да, почти наверное есть изолированные
вершины, значит почти наверное, как следствие, граф не связан,
просто потому что есть изолированные вершины, пока мы знаем только,
что мат ожидания стремится к плюсу бесконечности,
это важно, но надо посчитать дисперсию, дисперсия х,
как обычно, это второй момент, минус квадрат, мат ожидания,
дорогие товарищи математики, как у вас на лекциях сейчас
тела обстоят, что вы там проходите, Гёльдерша, это
хорошо, значит случайная величина у вас уже есть, нет,
другого Гёльдера проходить, а что такое Гёльдер, какое именно,
но интеграл, да, но интеграл, то есть мат ожидания, ладно, понятно,
да-да-да, то есть я понял, ну хорошо, сейчас посчитаем все,
это-то мы знаем, вот оно, его можно не считать, надо посчитать второй момент,
ну давайте посчитаем второй момент, это просто, как мы считали мат ожидания,
мы складывали n индикаторов, которые указывали на изолированность отдельно
взятой вершины, правильно, так понятно, что такое x и t, это те самые индикаторы,
которые сложились в n умножить на 1 минус p в n минус 1, x и t это индикатор того, что и
изолированная вершина, тесно возводим в квадрат, эти индикаторы скоррелированы,
поэтому надо тесно возводить в квадрат, будет x1 в квадрате, далее плюс xn в квадрате, плюс сумма
по i неравным g, x и t на x и t, ну как обычно я не рисую двойку перед суммой, потому что я считаю,
что пары индексов вот эти упорядочены, так хлоп-хлоп, потому что индикатор в квадрате,
то он же сам, 1 в квадрате и 0 в квадрате, это опять то, что мы знаем, то есть математическое
ожидание x, ну а дальше по линейности сумма по вот этим кортежам и g с не совпадающими индексами,
мат ожидания x и t на x и t, ну еще раз, они скоррелированы, поэтому я ни в коем случае не утверждаю,
что это произведение мат ожиданий, это не произведение мат ожиданий, не вздумайте так считать,
но что такое мат ожидания произведения индикаторов, это вот у нас есть вершина с номером i, есть вершина
с номером g и произведение вот этих индикаторов равно единице тогда и только тогда, когда обе вот
эти вершины изолированы, то есть у нас есть еще сарделька из n-2 оставшихся вершин и надо чтобы
тут не было ребра, тут не было ребер и тут не было ребер, сколько всего таких ребер, которые должны
отсутствовать, вот здесь одно, здесь n-2 и здесь n-2, то есть 2n-3, правильно, 2n-3, получаем ех,
n на n-1 на 1-p вероятность отсутствия ребра в степени 2n-3, n умножить на n-1 это как раз количество
пар несовпадающих индексов, если они упорядочены, а из n-2, а из n-2, n на n-1, так, но осталось разделить,
придется где-то отдельно здесь делить, нам нужно разделить дисперсию х на квадрат математического
ожидания, значит это будет ну ех квадрата мы вычислили, давайте напишем это будет ех плюс
n, n-1, 1-p в 2n-3 поделить на ех в квадрате и вычесть надо просто единицу, так единицу вычитаем,
потому что ех в квадрате делится на ех в квадрате, ех деленное на ех в квадрате стремится к нулю,
потому что мат ожидания стремится к плюс бесконечности, видите это все-таки очень важно,
мат ожидания обязана стремиться к плюс бесконечности иначе не получится, вот эта штука стремится к нулю,
в нашем режиме c меньше 1 это обеспечивает благополучно, а дальше у нас есть n умножить
на n-1 на 1-p в 2n-3 и мы это делим снова на квадрат мат ожидания, который я сейчас напишу явно,
то есть это n в квадрате на 1-p в 2n-2, это просто квадрат мат ожидания написанный явно,
вот эта часть асимптатически равна единице, а что происходит с этой частью, что вообще,
ну это 1 поделить на 1-p, правильно, то что осталось это вот, это вот, это 1 поделить на 1-p,
но p у нас стремится к нулю, смотрите на p это целый, Гаррифмен поделить на n-c константа,
p стремится к нулю, поэтому это тоже асимпатическая единица,
и того асимпатически 0 плюс асимпатическая единица и минус просто единица, куда это все,
0 плюс почти единица, минус ровно единица, 0, то есть это действительно все вместе в пределе
дает 0, а тут стало быть в пределе получается единица, что и требовалось в пункте 2 доказать,
ну отлично, у меня осталось как раз полчаса на то, чтобы доказать пункт 1, ну и может быть сформулировать
теорему следующего раза, если успеем, будет важное нововведение, я частично ее докажу,
такого раньше не было, так, ну давайте так, мы помним, что мат ожидания стремится к нулю в
этом случае, само мат ожидания мы тоже помним, давайте хотя бы вот это все сотру, но из того,
что оно стремится к нулю, ничего не следует, потому что могут все-таки теоретически быть
другие компоненты, да их труднее отщепить, это понятно, но поскольку само количество возможностей,
возможных размеров этих компонент растет с ростом числа вершин, то вдруг в сумме там накопится и непонятно.
Вот здесь я сейчас честно просто посчитаю оценку для суммы,
что бы я не делал в начале лекции, сейчас я сделаю, ну здесь попроще просто и так мучить,
как там я не буду, а здесь немножко помучаю, давайте, пункт один, пусть х на графе, это теперь не число
изолированных вершин, а просто число нетривиальных компонентов связанности, так, что значит тривиальная
компонента, но тривиальная компонента это весь граф, если он связан, то есть это число компонент
связанности, ну хорошо, давайте так, это число компонент связанности, в каждой из которых не больше,
чем n-1 вершина, сейчас понятно, конечно, да, не только полного связанного, любого связанного
графа x будет равняться нулю, любого связанного графа x равняется нулю, ну то есть можно сказать так,
x это конечно сумма x1 плюс и так далее, плюс xn минус 1, где xt от g это число компонент на i вершинах,
компонент на i вершинах, ну то есть в частности x1 это как раз число изолированных вершин,
в котором мы работали, соответственно, математическое ожидание x это сумма по i от 1 до n-1,
мат ожидания xt, мы даже знаем, что первая слагаемая в этой сумме стремится к нулю, вот если бы оказалось,
что вся эта сумма стремится к нулю, тогда мы бы по неравенству Маркова получили, что компонент
связанных нетривиальных нет и стал быть граф связанных, я не очень быстро говорю, а нормально точно так,
но давайте посчитаем или оценим ex и t в общем случае, как это посчитать, но опять можно какую-то
линейность использовать, то есть надо просуммировать по кому, мы берем n вершин, пресловутая сарделька,
и мы выбираем произвольные и вершин, которые хотим протестировать на то, что вот именно это
подсарделька образует связанную компоненту, то есть надо просуммировать по всем i из v мощности,
и маленькая вероятность того, что i является компонентой в случайном графе g, g от np,
это линейность тоже, мы перебираем все возможные потенциальные множество вершин вот этих компонент
связанности, перебираем все под множество вершин, имеющие мощности маленькая, и каждое
тестируем на то, что оно таки образует компоненту, мат ожидания индикатора это вероятность того
условия, которое задает на нем единицу, на который этот индикатор указывает, слушайте, но есть
проблема, непонятно как посчитать вероятность того, что данное конкретное множество вершин является
компонентой, но давайте так, а что значит, что данное конкретное множество является компонентой,
это значит на самом деле пересечение двух условий, пересечение в смысле одновременное
выполнение двух условий, во-первых, индуцированное это множество под граф должен быть связанным,
согласны, а во-вторых, это же компонента, вот таких ребер быть не может, никаких,
ребер у которых одна вершина внутри конкретного и большого, а другая вершина вне его, потому что
это компоненты, это максимальный остров связности, вот давайте тупо это оценим сверху суммой по тем
же самым и большой мощности и маленькой вероятности последнего условия, про связанность не очень
удобно рассуждать, непонятно с какой вероятностью индуцированный граф связан, а вот вероятность того,
что из И в В-И нет ребер, вот это легко посчитать, и это, конечно, ну может быть грубая верхняя оценка
для вероятности, которая нас интересует, вероятность пересечения двух условий мы просто тупо оцениваем
вероятностью одного из них, понятно, тем не менее даже этого для нашего относительно грубого
утверждения хватит, вот если бы мы захотели вот это доказывать, так такой бы номер не прошел,
а для нашего утверждения, когда С там строго чего-то меньше, больше этого хватает, ну-ка скажите
мне, что получается, есть прям совершенно явный ответ, для всей суммы сразу, очевидно, что все
слагаемые одинаковые, только чему они равны, очевидно, потому что какая разница, какое И большой
размер и маленькое зафиксировать, но зафиксировали какое-то, какова вероятность, что ни одно ребро
из него наружу не идет, нет мостика между ним и остальными вершинами, n-ы умножить на И, вот ответ,
c из n по И, 1-p в тепени И на n-ы, вот так, и умножить на n-ы это количество потенциальных мостиков,
каждая вершина из И большого может быть соединена с любой вершины из вне его, из В-ы большое,
таких пар вершин, таких потенциальных мостиков и умножить на n-ы, они все должны отсутствовать,
ну а самих вот этих множец слагаемых, их c из n по И, поэтому сумма получается такой,
как я написал, соответственно, интересующая нас мат ожидания получается равным, в свою очередь,
сумма по И от 1 до n-1, c из n по И на 1-p в тепени И на n-ы, но только неравном, конечно, а меньше,
либо равно, получилось, да, еще раз, вы согласны, что если вся эта сумма стремится к нулю, то мы в
героях мы доказали пункт 1, вот это точно понятно, это просто неравенство Маркова, вероятность того,
что х больше, либо равно единице не больше, чем мат ожидания х, если мы знаем, что оно стремится
к нулю, ну все, значит, вероятность со стремящейся к нулю есть хоть одна связанная компонента,
отличная от всего графа, но, стало быть, вероятность со стремящейся к единице,
таких компонентов нет, и граф связан, но вроде разъяснил совсем подробно, может быть, для тех,
кого здесь нет, так, может быть, действительно, здесь все понимали, но кто будет слушать записи,
вот, ну хорошо, то есть надо как-то оценить теперь вот эту биоку, видите, мы знаем, что первая слагаемая
в ней и равно единице, это как раз n умножить на 1-p в n-1, смотрите, и равно единице, это n на 1-p в n-1,
среднее число изолированных вершин, с которым мы работаем, мы знаем, что оно стремится к нулю,
но из этого не следует, что сумма растущего числа слагаемых стремится к нулю, да, наверное,
каждая последующая меньше предыдущего, но и что, ну, из интуитивных соображений отколоть одну вершину
проще, чем отколоть две, а это в свою очередь проще, чем три и так далее, но вдруг накопится много
слагаемых стремящихся к нулю, растущее их число может перестать стремиться к нулю, вот сейчас мы
с этим справимся,
мы с этим справимся как-нибудь, а, ну, во-первых, давайте сразу заметим, товарищи, что суммировать
достаточно до серединки, потому что все, что идет после серединки, абсолютно симметрично
к началу, и c симметрично относительно n пополам, и вот эта штука тоже симметрично относительно
n пополам, достаточно суммировать до n пополам, это важно, ну, важно в каком-то смысле, так удобнее
считать, нас реально интересует сумма pi от единицы до n пополам, c из n pi 1-p в степени
и на n минусы, мы хотим доказать, что она стремится к нулю, давайте я для начала объясню идею,
обозначим каждая слагаемая, ну, скажем a i t от n, я вроде так всегда делаю, это удобно, так у меня совсем
из всяких кусочек мелко, сейчас я возьму другой, есть очень хорошие и большие, так, вот давайте так,
первая идея, вынесем-ка мы за скобку a 1 от n, мы знаем, что оно стремится к нулю, потому что это в очередной
раз повторяю, как раз мат ожидания числа изолированных вершин, вынесем за скобку,
ну, первая слагаемая будет 1, вторая слагаемая будет a 2 от n поделить на a 1 от n, третья слагаемая
будет a 3 от n поделить на 2 от n, умножить на 2 от n, поделить на a 1 от n, ну и так далее, понятная идея,
идея состоит в том, чтобы заменить это все геометрической прогрессией, ну и чем-то на нее
похожем, если бы оказалось, что каждая вот такая дробь ограничена сверху чем-то стремящимся к нулю,
то сумма вся сходилась бы к единице и будучи умноженной на что-то, опять же, стремящимся к нулю,
давала бы ноль в пределе, знаете, как круто было бы, но к сожалению, прям так у нас не получится,
вот совсем так не получится, поэтому я сделаю следующий финт, я разобью вот эту сумму на две
части, у нас такие случаи уже бывали в нашей практике, когда по-разному приходилось оценивать
на разных участках, ну хорошо, хоть не на бесконечное число, на две только, значит,
в первой части у нас будет сумма по и от единицы до, как у меня обычно бывает,
ну давайте до n на корень из логарифма n. Не, ну я очень аккуратно объясню обязательно,
почему на корень из логарифма, там можно не на корень из логарифма, а на повторный логарифм,
там это я вам объясню, как я именно на корень взял, все будет понятно, можно не на корень. Так,
здесь t из n по i, а можно было а и от n написать, ну ладно, 1 минус p в степени i на n минусы,
ну а дальше остаток сумма по i. Ох, давайте будем издеваться друг над другом от этой
целой части плюс один до n пополам того же самого. Нормально так многоточиями? Ну понятно,
то же самое суммируется, и нижний предел такой же, как верхний в первой сумме, тут понятно,
ну плюс один еще. Смотрите, в чем смысл этого подразделения я сразу скажу. Вот на этой части,
за счет того, что i все-таки относительно маленькая, не дотягивается прямо до n пополам,
а дотягивается только вот до такой функции, мы таки сумеем воспользоваться идеей с
геометрической прогрессией, у нас все получится. А на этой части все станет уже, вот все, в смысле,
каждое слагаемое станет настолько маленьким, что мы ее оценим как хвостик, который с посвистом
стремится к нудам. Вот замысел такой. Друзья, я примерно понял, кто сказал замысел. Сейчас я его
реализую, у вас все формально будет прописано, конечно. Давайте действительно посчитаем вот это
отношение, то есть аи плюс первая от n к аитому от n. Посчитаем, как относятся. Хочется успеть
за сегодня это сделать. Значит аи плюс первая от n на аит от n. Я утверждаю, что если i ограничена
сверху какой-то функции, которая бесконечно мала по сравнению с n, ну вот такой, например,
как мы взяли, то все хорошо. Давайте честно напишем это c из n по i плюс 1, 1 минус p в степени
i плюс 1 на n минус i минус 1 поделить на c из n по i 1 минус p в степени i на n минус i. Ну просто
честно написал. Так, ну я не знаю, насколько вы в уме прямо готовы делить ц, но мы сейчас это
сделаем. Тут n факториал, тут n факториал, они сокращаются, очевидно. Теперь вот у числителя
у числителя в знаменателе стоит i плюс 1 факториал, а у знаменателя, в знаменателе же стоит i факториал.
Но понимаете, что в итоге в знаменателе остается i плюс 1. Вот так и пишем. В знаменателе остался
i плюс 1 и точно также в числителе выживает n минус i. Ну по совершенно такой же причине,
можете это проверить. Я ж вроде с этого начала, это компоненты размера от 1 до n минус 1. Любая
компонента отличная от всего графа. Почему? Вопрос-то. Мы вернулись к началу или мы разбираемся
с тем, что я сейчас делаю? Вопрос-то о чем вообще? Что? Ну мы с самого начала это уточняли. Меня вот
спрашивают, что значит нетривиальная? Я сказал, что если граф связан, то нетривиальных компонент ноль. Именно
поэтому мы хотим доказать, что вероятность вот этого события стремится к нулю. И сейчас оцениваем
мат ожидания чем-то стремящимся к нулю. Все, мы свели задачу к чисто аналитической. Сейчас я занимаюсь
мат-анализом, позволяющим убедиться в том, что вся эта бяка стремится к нулю. И все это будет давать
доказательство теоремы. Так, ну давайте вот здесь сокращение тоже проведем. Смотрите, вот тут тоже
есть и, тут тоже есть n минус и, то есть внизу все сокращается. А вверху остается, ну оно условно
вверху, оно может и вниз пойдет в каком-то смысле. Вверху остается минус и, 1 минус п в степени минус и,
вот так, и умножить на минус 1. И прибавить надо еще вот эту вот единичку на n минус и минус 1,
то есть правильно, да? Плюс n минус и минус 1, вот так. Единичку надо умножить на всю скобку,
а ишку только на минус единицу. Ой, ну слушайте, и у нас вообще какое угодно вот в этих пределах,
правда же? В общем, по-моему, ничего сильно лучше с точностью до константы, чем оценить это вот так,
я бы сделать не смог. Тут будет n минус 2 и минус 1. Я хочу сказать, что вот эта дробь,
вот эта дробь, по сути, ну меньше n, это очевидно совершенно, а лучше и не скажешь. Потому что
когда и, например, равняется единице, но это n минус 1 пополам и есть. Ну, в общем, я и оцениваю,
это не окупляет ничего. Ну, оценил, как ты оценил, победитель не судя, сейчас я победю через некоторое
время. Так, теперь смотрите, вот сейчас важный момент. Поскольку и у нас находится в пределах
до функции, которая бесконечно мала по сравнению с n, то, ну ладно, давайте я честно напишу, и у нас
меньше, чем n поделить на корень из логарифма. Меньше? Меньше. Минусом больше, но тут чиселка
меньше единицы опять меньше. Я могу вот так написать n на 1 минус p в степени n минус 2n на корень из
логарифма n минус 1. Но согласитесь, что в показателе экспонента написано выражение, которое от и уже
никак не зависит, но в любом случае асимпатически равно n. Вот сразу пометьте себе, в этом смысле,
в этом месте не важно, что написать, n на корень из логарифма или n просто на логарифм или n на
повторный логарифм. Любая вот эта верхняя граница, которая бесконечно мала по сравнению с n, тут
сработает. Но нам еще с хвостом потом предстоит разбираться, и вот там прояснится, почему я
нарисовал именно корень из логарифма. Это будет довольно весело. Понятно сказал? Так, ну здесь-то
что получилось? Ну, смотрите, что получилось. n на 1 минус p в степени n на 1 плюс о малой от единицы.
Вы понимаете, что это стремится к нулю? Ну, мы с вами знаем, что n на 1 минус p в n минус 1
стремится к нулю, потому что мы находимся в нынешнем режиме пункте 1. Ну, а что будет,
кто изменится, если снова на 1 плюс о малой от единицы, тут до множества? Все равно стремится к нулю.
Так, мы выкладку делали. Там вот е в степени минус p, там тоже самое. Поэтому давайте это обозначим
буквой q от n, вот это, и она стремится к нулю. Она стремится к нулю, потому что c больше единицы.
Ну и у нас получается, что вот эта вся сумма, это есть a1 от n, умножить на 1 не равняется
меньше или равняется. 1 плюс q от n плюс q квадрат от n, и давайте ее просто тупо оценим бесконечной суммой.
Так, друзья, поняли, откуда геометрическое прогрессия все сейчас? Вдруг тот раньше не
понимал. Ну а это равняется, конечно, вот так стираем, это равняется a1 от n на 1 поделить на 1
минус q от n. a1 от n мы знаем стремится к нулю, q от n мы знаем стремится к нулю, значит все стремится к нулю.
С первой суммой разобрались. Но видите, мы не имеем права в этой первой сумме брать какую-то
функцию порядка n. n пополам бы не сработало, потому что вот здесь вычилось бы n пополам и вышла бы
неприятность, стремления к нему уже бы не было. Это важно. Но хвост-то все равно сейчас будет маленький.
Толстый хвост.
Остался хвост.
Так, хвост. Это сумма по и вот целой части n на корень из логариф men плюс 1 до n пополам,
c из n по i 1 минус p в степени i на n минус i. Это товарищи меньше, чем n умножить на 2 в степени n
умножить на 1 минус p в степени n пополам, я сейчас напишу потом поясню, в степени n пополам умножить
на n деленное на корень из логариф men. Это издевательская на самом деле оценка. Я просто
c тупо оценил как 2 в степени n, но любая c меньше, чем 2 в степени n. Количество слагаемых тупо оценил
числом n, а вот эту вот величину оценил следующим образом. Вот здесь со знаком минус я воспользовался
тем, что i меньше, чем n пополам не больше, чем n пополам. Минус, но еще в показателе числа
меньшего единицы, поэтому можно подставить n пополам, получается n пополам. А вместо i вот
подставил этот n логариф men, которого она больше. Получил вот такую оценку. Все, тут грубее некуда.
Теперь смотрите, это как обычно равняется на 2 в n на е в степени логариф men от 1 минус p,
но снова на n пополам на n на корень из логариф men. Это меньше либо равно n на 2 в степени n на
е в степени минус p, n пополам, n на корень из логариф men. Это логариф от 1 минус p не больше,
чем минус p. Стандартное неравенство. Мы им тоже когда-то пользовались. Теперь подставляем честно
p, n, 2 в n, а p это у нас c, логариф men поделить на n. Дальше идет n пополам, идет n поделить на
корень из логарифа n. И вот тут-то и проясняется, почему я выбрал именно корень из логарифа n.
Потому что я сейчас сделаю вот так шлёп. Только ради этого, товарищи. Но, конечно, я должен был
выбрать тут такую функцию, чтобы она не полностью убила вот этот логариф. Вот это важно, потому что
я же еще вот такой шлёп-шлёп сделаю. И у меня останется вот так. Я все под одну экспоненту
загоню. E в степени логариф натуральный n, это n, плюс n логариф натуральной двойки, это 2 в n. И тут
остается минус c пополам, умножить на n корней из логарифа n, и все. Да, все, больше ничего.
Делить на единицу это не нужно. Так, друзья, видите, что так осталось? Мне было важно выбрать такую
функцию, чтобы она с одной стороны удобно красиво на доске сокращалась вот этим логарифом,
а с другой стороны не сокращала его полностью, потому что мы здесь n умножаем на растущую
функцию, а вычитаем это дело из n умноженной на константу. Но уж логарифом это вообще фигня. То есть
когда мы берем вот эту сумму и вычитаем из нее такую штуку, мы получаем нечто стремящееся к
минус бесконечности, но это показатели экспоненты, значит, то радость все стремится к нулю. То есть
вот в качестве разделителя между двумя суммами можно было брать любую функцию. Можно взять вот
такую, как я взял, можно взять какой-нибудь n на ln. Она должна быть маленькая от n с одной стороны,
но она должна быть больше-больше, чем n поделить на логариф mn. Если я возьму просто n поделить
на логариф mn, я кокну вот этот логарифом, и вот это преимущество потеряется. А так, в принципе,
вот раздел мог быть каким угодно. Понятно поясню? Нет, ну чисто технически-то всем понятно,
сократили и все получилось. Все, я доказал теорему. Но знаете, поскольку у меня остается меньше
пяти минут, я даже не буду формулировать теорему следующего раза, я просто ее в следующий раз
начну. А на сегодня тогда все.

было винарский процесс, ну я не буду перечислять его свойства на доске, но напомню, что это
независимые приращения и приращение гауссовские, одно приращение гауссовское с дисперсией вот
у такого приращения дисперсия t-s, если t больше s, значит, почему непрерывные траектории?
ну я напомню, значит, это по теореме Колмогорова, там такая была теорема, что если вот такая вот
оценка есть, то непрерывные траектории есть, так у нас с чем это нужно применять, значит,
смотрите, а если так совсем банально сделать, ну с двойкой, тут самое милое дело с двойкой,
уже считать ничего не надо, но с двойкой не дотягивает, видите, эпсилон нет, с двойкой
получится не эпсилон, значит, бета вовсе не двойка, а бету нужно взять четверку, так вот,
если сосчитать четвертый момент, так, а четвертый момент у гауссовской случайной величины заданной
дисперсии, ну давайте это не будем считать, будем считать, что это относится там к какому-нибудь
прошлому семестру, то тогда получится, что единица плюс эпсилон можно взять равное 3,
но нам даже не важно какое, давайте я не буду даже писать, тогда оно точно будет единица,
ну вот сосчитайте, какой будет ответ, ну годится, ответ будет 4 на t-s в кубе, но это даже не важно,
что именно в кубе, важно, что стало больше единицы, вот, а теперь обсуждается, вот это у нас,
так сказать, было, обсуждается альтернативный способ, вот в виде функционального ряда,
ен базис вл2, ну скажем, давайте для определенности на 0.2p, мы там в прошлый раз на 0.2p обсуждали,
значит базис какой угодно годится, артнормированный, и как я в прошлый раз говорил,
значит есть теорема, о которой полезно знать, но доказывать мы ее не будем, и самой даже,
так сказать, у нас самого обсуждения этого нет, почему так будет, вот, ну как это полезно,
как факт просто знать, значит, но если ен это синусы нт, косинусы нт, ну там с множителями,
так, то получается, что когда вы проинтегрируете, так, значит, получаются ряды,
получаются ряды вот такого вот вида, вот мы на этом закончились, давайте я, значит,
выпишу, какого вида ряды получаются, получаются вот такого вот, ну там с коэффициентами, конечно,
значит, по синусам и по косинусам, так, аналогичные ряды, так, значит, спрашивается,
почему хотя бы в L2 это даст что-то, при таком способе действия трудность никуда не уйдет,
потому что тут же цель, чтобы была равномерная сходимость, а тут не станет от этого легче
доказывать равномерную сходимость, потому что, видите, если бы тут стояли квадраты, то все было
бы чудненько, ряд бы равномерно сходился, но увы, стоят те, кто не сходятся, но хотя бы в L2,
хотя бы в L2 сходимость есть, значит, почему есть хотя бы в L2 сходимость, значит, вот такой вот ряд
сходится почти наверно, потому что, потому что ряд из интегралов,
который как раз уже хороший ряд, значит, он сходится, значит, раз ряд из интегралов сходится,
то сходится, значит, как известно, почти в чуду сам функциональный ряд, поэтому для тех омега,
для тех омега, для которых вот та сумма конечна, ну, можно считать, что это для всех,
выбросив множество меры нуль, можно считать, что это для всех омега, так, значит, получается,
что ряды сходятся в L2, так, ну, на самом деле, на самом деле сходимость, как видно из этой оценки,
сходимость в L2 на, по двум переменам, значит, по мере, вот меру либега умножить на p, даже в таком будет,
значит, эти вот из-за этой оценки следует, что этот ряд как двух функций, двух переменных сходится,
так, поэтому можно положить, можно взять то, что мы хотим, функции двух переменных, как сумму ряда,
значит, как сумму ряда, где он сходится, значит, он тогда сходится почти всю, ну,
для при почти всех, значит, то есть при почти всех парах t омега, но вы скажете, по нашей доктрине,
процесс, это функция двух переменных, но она все-таки при всех t должна быть определена, так, а не при
почти всех, как быть, а здесь, значит, получилось при почти всех, ну, тут можно подправить, можно
подправить то, что получилось, а пока что у нас получилось, значит, вот так, но давайте подправим,
значит, по Фубини, так, поминаем итальянскую мафию, значит, по Фубини, при почти всех,
значит, при почти всех фиксированных t есть сходимость ряда при почти всех омега, так, ну, множество
таких t пусть будет, ну, какое-то там t0, так, вот, значит, при каждом t0, при каждом t0, при
почти всех, значит, омега будет сходимость, так, а теперь, значит, мы сделаем следующее, давайте
сосчитаем, ну, там, где у нас все определено, давайте сосчитаем мат ожидания и дисперсию,
ну, раз ряд сходится в l2, то смотрите, что получается, значит, получается, что мат ожидания вот этого, так,
это будет 0, ну, при почти всех t, почему? Ну, потому что мат ожидания вот этих всех нулей, так,
а теперь давайте посмотрим, теперь давайте посмотрим,
чему вот это равно мат ожидания квадрата, так, значит, смотрите, что получается, получается,
что это мат ожидания, вот от чего, значит, на первый взгляд, значит, ну, что-то ужасное предстоит делать,
ряд возводить в квадрат, так, но давайте посмотрим, что это за ряд, значит, это вот какой ряд,
давайте, давайте, вот я напишу вот так, да нет, давайте сразу интеграл напишу,
значит, смотрите, это ряд, вот какой, стоят стандартные гауссовские случайные величины,
так, и при них что-то, так, значит, и при них что-то, но тогда они независимые, так,
и тогда получается вот какой ряд, будет просто ряд из квадратов вот этих вот,
потому что вот эти независимые стандартные гауссовские, а тут же T и S фиксировано,
так что они пока, ну, какие-то числа стоят, при этих гауссах стоят какие-то числа,
значит, дисперсия суммы будет просто сумма дисперсий, так, а что вот это, что такое,
смотрим внимательно вот на это, значит, стоит, ну, вроде какие-то ужасные интегралы какие-то стоят,
так, значит, если вот пуститься во все тяжкие, начать сюда подставлять синусы и косусы,
ну, какой-то тихий ужас получится, так, но на самом деле не надо на это смотреть,
как на интегралы, а на это надо посмотреть вот как на что, это стоят индикаторы отрезка скалярно,
скалярно с Dn в L2, то есть, смотрите, тут стоит сумма квадратов коэффициентов фурье, так,
и поэтому по парсивалю, значит, по парсивалю, так, это будет просто квадрат нормы индикатора в L2,
ну, а это будет t-s, потому что вот считать нормы в L2 индикаторов одно удовольствие,
это просто длинные квадраты норм, это просто длинных отрезков, значит, смотрите,
получили заказанную величину, так сказать, на самом деле можно считать, что пока t и s,
вот из этого t0, где есть сходимость, потому что мы что-то исполняли при фиксированных t и s,
ну и поэтому нужно, чтобы при них была сходимость, но на самом деле из этого вычисления видно,
из этого вычисления видно, что это верно при всех t и s, когда мы проделали вычисление,
то видно, что тут не важно сходился ряд, сходился ряд, так сказать, или нет, а вот этот, вот этот ряд
будет сходиться в L2, так, значит, это ответ, это оправдывает, значит, смотрите, что у нас получилось,
значит, у нас получилось, ну, то, что мы заказывали, что действительно у приращения средняя ноль,
дисперсия какая надо, так, а теперь при t не из t0, мы теперь переопределяем вот это,
как предел в L2, w, s, так, ну, по каким s, ну, s стремится к t и s из t0, ну, значит,
вы берем последовательность, у нас множество полной меры было, значит, это множество t0
всюду плотно, значит, t0 всюду плотно, раз у него дополнение имело меру ноль, поэтому мы, чтобы у нас
вот эта функция двух переменных была не при почти всех t задана, а, честно, как полагается у процесса,
при всех t, мы вот еще такой последний финт делаем, переопределяем, так, значит, в итоге у нас
получилось вот это при всех, значит, у нас при каждом t появилась случайная величина, так, значит,
ну, при таком способе остаются вот эти свойства, значит, остаются вот эти вот,
конечно, остаются уже при всех t, кроме того, понятно, что при таком способе задания wt
гауссовский процесс, процесс гауссовский получился, а теперь, значит, нам нужно еще что,
чтобы приращения были независимы, значит, еще мы хотели, значит, уже почти все, так сказать,
сделано, так, но, значит, нужны приращения, значит, приращения независимы, а из-за гауссовости,
из-за гауссовости процесса, проверяем, проверяем некоррелированность приращений,
значит, вам нужно взять вот такие вот, так, и, ну, скажем, вот так, значит, сейчас давайте, так,
4, 3, так, значит, где t1 меньше t2 меньше либо равно t3 меньше t4, значит, вот у этих нужно проверить
некоррелированность, так, значит, уже процесс гауссовский, значит, для независимости достаточно
некоррелированность, значит, средняя ноль, поэтому никаких там мат ожиданий нет и получается,
что нам нужно сосчитать, значит, смотрите, что нам нужно сосчитать, нам нужно сосчитать вот такую вот вещь,
ну, значит, считаем, значит, смотрите, что получается, тут получается вот такая вещь,
сумма ксиенных на интеграл от t1 до t2, ен, значит, ds, так, а тут, значит, ксиенный на интеграл вот такой,
но опять стоят, значит, два ряда перемножаются с независимыми гауссовскими и с какими-то
числовыми коэффициентами, так, значит, получается, что эта сумма будет просто произведение вот этих,
все попарно уйдут, когда мы честно будем перемножать ряды, то тут будут всякие кси, ен, на ксяка, но они все уйдут,
когда k не равно n, потому что они попарно-ортагональны, значит, останутся только одноименные и при
одноименных будет вот такая вот вещь стоять, но в этом опять узнаем, в этом опять узнаем скалярные
произведения вл2, значит, индикаторов с базисными, опять тут стоит произведение скалярных произведений,
ну и, значит, опять парсиваль, значит, дает, что это будет скалярное произведение вл2 индикаторов,
но отрезки у нас не накладываются, поэтому это будет ноль, то есть, видите, значит, вот тут все вычисления при таком подходе
основаны на равенстве парсивали, ну, я полагаю, что у вас в функциональном анализе оно уже когда-то там давно уже было,
это равенство парсивали, так что, видите, штука полезная, вот мужик сто лет назад придумал, ну там сто с лишним лет назад,
и, видите, до сих пор не устарела, значит, тут все очень мило, получился нужный процесс, но у него какой, так сказать, дефект,
ну вот мы при таком подходе, мы не знаем, что у него непрывные траектории, чтобы узнать, что непрывные траектории,
нужно либо к теориями Колмогорова обращаться, ну, она здесь опять применима, потому что мы же как только приращение гауссовской
из известной дисперсии, все, четвертый момент читаем, точно так же теориями Колмогорова применима, ну, я считаю, что это самый эффективный
и дешевый способ тут отделаться, но, возможно, более сильные средства из теории функций можно, так сказать, засучив рукава,
браться за сходимость этого ряда, ну, что само по себе довольно вещь любопытная, но довольно коровопролитные подробности,
поэтому мы их опускаем. Теперь спрашивается, а как было у Винера, который вроде бы это придумал, до Колмогорова?
Значит, Винер это в двадцатых годах придумал, а Колмогоров это, так сказать, преобразовал, ну, там можно сказать, в конце двадцатых, в начале тридцатых.
Ну, вот у самого Винера изначально, честно говоря, было не очень внятно, а у него на самом деле, так сказать, по нынешним меркам как бы не было какого-то аккуратного доказательств.
То, что у него написано, ну, я когда-то пробовал читать, это довольно тяжело читать, и, в общем, не так просто довести до ума, но потом у Винера появился талантливый ученик Пелли,
который потом, к сожалению, погиб в горах, был адвинистом. Вот он успел внести важный вклад в это дело, и вот с его помощью уже, так сказать, позже, уже в тридцатых годах, уже после Колмогорова,
они все-таки довели вот этот, так сказать, первоначальный способ Винера, ну, вот изменили его, и довели до некоторого строгого рассуждения,
но вот у них вот как раз был такой вот анализ вот этих вот тригонометрических рядов со случайными коэффициентами, вот у них такой был способ.
Он технически более длинный, но, ну, имеет самостоятельную ценность, если не считать, что основная цель как-то получить процесс, то вот их способ через разложение функциональной ряды тоже представляет интерес, тоже был красивый результат,
но, несомненно, самый такой ясный способ построить аккуратно, мне кажется, самый ясный и короткий способ, это Колмогоровский.
Ну, у Колмогорова всегда так, вот когда он делал какие-то задачи, у него они всегда как-то получались ясные и короткие.
Он даже вообще считал, что не может быть длинных математических доказательств, потому что он говорил, что в человеческом мозгу не может поместиться больше, чем на пять страниц чего-то,
поэтому он как-то с сомнением относился к работам, у которых доказательства не помещались на пять страниц.
Получается так, что у нас Винеровский процесс появился как комбинация двух теорем Колмогорова, и вторая нужна для того, чтобы траектории стали непрывными, вот у нас какой итог.
Так, теперь, но это я рассказываю на самом деле некоторые дополнительные вещи, которые в принципе полезно представлять, а собственно, как всегда говорили Колмогоров,
что студентам не важно, что вы рассказываете, им важно как полагается на билеты отвечать, значит, так вот, на билет тут полагается отвечать так,
что должно быть, так сказать, определение Винеровского процесса, ну и объяснение, как он получается из двух теорем Колмогорова, которые, обратите внимание, у нас без доказательств,
у нас обе эти теорем Колмогорова, они без доказательств. Ну, почему они у нас без доказательств? Ну, потому что, если еще их доказывать, то это будет какая-то сплошная теория меры,
случайные процессы, поэтому, в общем, где-то нужно остановиться в теории меры, чтобы, ну, хоть какое-то место, так сказать, каким-то, так сказать, статистическим понятиям дать.
Так, теперь, значит, вот некоторые свойства, некоторые свойства, это у нас все без доказательства, значит, Винеровского процесса,
значит, ну, свойства, значит, первое свойство траектории, на самом деле, Гельдеровы, значит, почти всякая, почти всякая траектория,
значит, почти всякая траектория Гельдерова, порядка альфа больше одной второй, значит, это вот, что значит, это значит, что w от t плюс h от омега,
минус w от t оценивается, ну, с некоторой константой, зависящей от омега, конечно, и от альфа, на, значит, a модуль h в степени альфа.
Что?
Можно, чтобы множество этих омега было фиксировано и обслуживало все альфы, ну, которые меньше одной второй.
Ну, вы спросите, почему, почему такая странная избирательность, а почему исключать, ну, значит, во-первых, это видно, что это лучше, чем, видно, что это лучше, чем просто непрывность, правда,
то есть, это такая вот, такая еще количественная непрывность, так, ну, а, кстати, откуда это берется?
Ну, это можно усмотреть, на самом деле, из доказательства теория Макалмогорова, которого у вас, у нас не было,
потому что в действительности его доказательство непрывности дает вовсе не непррывность, а дает Гельдеровасть, вот, собственно, откуда это берется.
Теперь спрашивается, почему, почему такое странное, ну, ладно бы еще, ладно бы еще альфа не было бы единицей, ну, это мы сейчас увидим, почему альфа точно не единицы, так,
ну, почему, можно было бы думать, ну, годятся все альфы меньше единиц, так, вот, почему, значит, вот давайте я сразу напишу свойства два,
а почти, наверное, траектории, траектории не имеют точек дифференцируемости,
так, ну, из этого видно, что не годится альфа равно единицы, потому что если бы годилась альфа равно единицы, то траектории были бы Липшицевы,
а у Липшицевой функции есть обязательно точки дифференцируемости, их много, а тут ни одной нет точки дифференцируемости,
смотрите, это второе утверждение сильнее, чем то, что в заданной точке нет дифференцируемости, вот то, что в заданной точке нет дифференцируемости,
это понятно, потому что когда вы точку фиксируете, так, когда вы точку фиксируете и делите на корень, когда вы делите на корень из аж,
то вот это имеет нормальное распределение, так, с дисперсией 1, так, поэтому если вы еще на один корень поделите, ну, как полагается для разностного отношения,
то это пойдет в бесконечность, поэтому то, что в отдельной точке Т0 нет дифференцируемости, это очевидное из, вот, просто определение,
но тут ведь не говорится, что в заданной точке почти, наверное, нет дифференцируемости, тут гораздо сильнее, что вы берете траекторию,
а у нее нет точек, вообще ни одной, нет точек дифференцируемости, это довольно нетривиальная теорема, так, значит, но она объясняет,
она объясняет, почему не годится тут α равно 1, но она не объясняет, почему α, скажем, две трети не годится, так, значит, ну, на самом деле,
это следует, ну, вот, примерно из этого соотношения, но давайте это оставим в качестве задачи, значит, альфа больше одной второй не годится,
так, значит, так что ответ точный, альфа, простите, альфа больше либо равна одной второй, ну, на самом деле, чтобы увидеть,
что альфа больше либо равна одной второй не годится, достаточно, конечно, увидеть, что не годится альфа в точности равной одной второй, так,
но вот почему альфа не годится в точности равной одной второй, это как раз предмет задачи, и это надо как-то вывести из того,
что как раз тогда такое отношение получается, так, и оно 0,1, значит, гауссовское 0,1, так, ну, в общем, нужно понять,
как это помогает, но на самом деле, на самом деле есть еще два последних факта, которые я приведу без доказательства тоже,
как все в этом разделе, потому что все эти факты, они, ну, в принципе, обозримые у них доказательства, но это тогда
какой-то такой удвоенный был бы курс, значит, если все это с доказательствами, а кроме того еще,
что плохо, доказательства довольно еще и тяжелые, поэтому, вот еще и поэтому имеет смысл иногда их не приводить,
значит, теорем, значит, вот что говорит, значит, с вероятностью, с вероятностью 1, вот такие вот
предельные соотношения, техний предел при Т, стремящемся к нулю, wT от омега делить на корень из 2T,
значит, логарифм, модуль логарифм T, так, равен единице, так, и аналогичная вещь, значит, верхний предел,
когда T идет в бесконечность, но это когда мы уже не на отрезке процесс строим, а на всей, значит, полупрямой,
ну, примерно то же самое на бесконечности, 2T на логарифм, логарифма T тоже равен единице,
значит, смотрите, значит, смотрите, что получается из вот этого первого, из этого первого получается,
из этого первого получается, что при фиксированном, при фиксированном T0 вот будет, значит,
значит, ну и тоже h стремится к нулю, будет вот такая вещь,
значит, 2h логарифм модуля логарифм h, значит, смотрите, видите, получается, что верхний предел,
а, значит, приращение деленного на что-то, то есть получается, что модуль непрерывности,
видите, вот такой, так, а это значит, ну это немного хуже, чем Гольдер, одна вторая, так,
вот если бы не было вот этого логарифма, значит, вот это все называется закон повторного логарифма,
значит, если бы не было этого повторного логарифма, то как раз бы получалась Гольдеровость одна вторая,
но тут, видите, этот ухудшающий множитель, так, вот он немножко, значит, он, так сказать,
немножко мешает вот одной второй, но задача, в этой задаче предполагается, конечно,
доказать, что альфа одна вторая не годится, не пользуясь, конечно, этой теоремой,
потому что из этой теоремы это очевидно, доказывать нечего, а в задаче предлагается
голыми руками сделать, значит, так, значит, это все про Винеровский процесс, сейчас я что-то еще
хотел про них сказать, а вот еще, значит, вот еще полезный процесс, значит, вот замечание,
значит, похожий процесс, значит, В от Т от Омега, это WT от Омега минус Т на W1 от Омега,
так, значит, эта штука называется Броуновский мост, ну вот в качестве упражнения найти его
к вариацию надо, значит, чем интересен этот Броуновский мост у Винеровского процесса,
в нуле фиксированное значение, так, ноль, а у Броуновского процесса, а еще и вот у этого
Броуновского моста, а еще и в единицы, ну, разумеется, этот мост можно устраивать между нулем и какой
угодно другой точкой, не обязательно единицы, это я для упрощения обозначения пишу единицы,
тоже довольно интересный процесс, ну, который там тоже во всяких там физических и прикладных
задачах возникает, так, теперь, ну, там в конспекте у меня еще какие-то есть примеры, но сейчас на них
уже нет времени останавливаться, потому что надо энергично обсудить условные математические ожидания,
так, значит, вот еще некая порция теории и меры, значит, зачем нужны эти условные математические
ожидания, ну, потому что это необходимый бэкграунд для последнего куска курса, в котором речь идет
про мартингалы и марковские процессы, значит, вот я уже перечислял небольшой список, так сказать,
основных классов процессов, которые там встречаются у нас и в приложениях, он, конечно, не исчерпывает
всех полезных процессов, но вот для такого короткого курса, ну, представляется разумным небольшой
список иметь, но вот для того, чтобы вот эти два еще не обсуждавшихся класса, мартингалы и марковские
обсудить, там нужно понятие условного математического ожидания, значит, ну, можно, конечно, объявить,
что это якобы было в теории вероятностей, но это, я знаю, это лекторы так любят делать, говорить,
что якобы в другом курсе что-то было, а зачастую в этих других курсах, наоборот, бывает так,
что вот студент говорит, а вот у вас будет такой-то курс, и вот там про это расскажут, поэтому давайте,
значит, кратко обсудим, значит, это понятие, значит, вот, значит, дано, значит, дано вероятностное,
значит, основное вероятностное пространство, ну, как водится, сигма алгебра на нем видена, так,
и, значит, вот появилась под сигма алгебра, значит, под сигма алгебра в b, так, значит,
значит, определение, значит, определение, значит, пусть, пусть, значит, кси интегрируемая случайная величина, так,
значит, условное, условное, значит, смотрите, вот это условное математическое ожидание, немножко длинный термин,
поэтому чтобы на доске было короче написано, ну и драгоценного мела, чтобы меньше тратилось, будем это называть условное среднее, так,
в английской литературе для математического ожидания используются слова expectation, ну и тогда здесь, значит, появляется conditional expectation,
там нет этой добавки mathematical expectation, просто expectation, так, ну expectation это в английском разные, имеют значение, значит, вот ожидания, надежды,
значит, вот, например, там роман классика, значит, большие надежды, он great expectations, но там вовсе не о математических ожиданиях идет речь, так,
значит, условное среднее, значит, есть, ну или, значит, еще другое обозначение, вот такое,
ну оно вот будет видно, что иногда вот это удобное, значит, есть, что-что, кси интегрируемая случайная величина, ну вот можно написать так,
кси лежит в L1P, вот так, значит, смотрите, значит, кто ее, значит, какие у нее great expectations, значит, у нее, вот, что это такое,
это A измеримое, A измеримое интегрируемое случайная величина, для которой,
интеграл, вот это кси dP равен, значит, интегралу это, ну вот с этой вот случайной величиной,
для всех ограниченных A измеримых, A измеримых это, ну понятно, что это равносильно, вот чему, что интеграл по каждому множеству из этой сигма-алгебры
равен интегралу вот от этой случайной величины, значит, для всех A из вот этой под сигма-алгебры, так,
значит, а почему такая есть, значит, можно двумя способами это объяснить, а, значит, если, если кси лежит в L2, то,
вот эта штука, это ортогональная проекция, проекция кси на подпространство, на подпространство кси,
вот эта штука, это ортогональная проекция, проекция кси на подпространство L2A, порожденное A измеримыми.
А, значит, ну, кстати, я сказал, что они равносильны, а почему они равносильны, ну, потому что, ну понятно, что это частный случай этого, когда к индикатору применяется, так,
но если вот это верно для всех, если это верно для всех A, то значит, что это верно для всех индикаторов, так, ну, значит, это верно для линейных комбинаций индикаторов, так,
а всякая ограниченная A измеримая функция равномерно приближается конечными линейными комбинациями индикаторов, поэтому получаем, что верные для всех ограниченных.
А почему тут ограничение на ограниченные? Ну, потому что это всего-навсего интегрируемо, поэтому ничего больше мы пока не можем подставлять,
но вот если бы она была из L2, ну, что не обязательно, конечно, то сюда можно было бы в качестве это подставлять и другие из L2, так, это получилось бы скалярное произведение в L2,
это выглядит как скалярное произведение, но оно не является скалярным произведением, потому что скалярное произведение, это, ну, что это такое, это обычно никто не помнит,
но по крайней мере нужно помнить, что эта штука равноправная по отношению к двум, а тут получается, так сказать, неравноправная, если она не из L2, так, вот,
но если она из L2, то можно взять замкнутое линейное подпространство во всем L2, вот, порожденное A измеримыми функциями, так, и на него спроектировать, это будет то, что надо.
А в общем случае, значит, это можно, значит, общий случай, общий случай, значит, откуда берется, значит, вывод, вывод из случая, значит, ограниченного кси,
как вывести, ну, для ограниченного делаем проекции в L2, так, а как дальше, когда кси не ограничено, ну, разбиваем, как водится, общую интегрируемую на положительно-отрицательной части и возимся с каждой в отдельности,
значит, сводим к случаю, когда это неотрицательное, так, значит, рассматриваем, когда это неотрицательное, берем, берем, обрезаем по уровню n, так, значит, когда обрезаем, эти стримятся,
эти ограниченные стремятся к этому, ну, и устремляем n в бесконечности, ну, и оказывается, что у них есть предел, почему у них есть предел, по теореме, по теореме там, фату или беппелеви, потому что оказывается, что они возрастают,
а интегралы у них ограничены, ну, из-за чего у них ограничены интегралы, это из-за того, что если сюда подставить единицу, то будет, ну, тут единица будет, интегралы у них единицы будут, ну, если у этого кси был интеграл единицы,
но тут неважно, какой, я даже зря сказал единица, ни при чем тут, если подставить эту единицу, будет просто интеграл от кси, значит, интегралы от этих ограничены интегралом от кси, значит, вот, получится, что у этих, так, значит, смотрите, что получится при таком подходе.
При таком подходе получится, что вот эти вот, ну, к какой-то штуке возрастают, вот к чему они возрастают, это мы и берем в качестве кси.
Ну, а самый общий случай разложение на две компоненты, значит, другой вариант, если кто, у кого еще не выветрился прошлый семестр, значит, то другой вариант такой, значит, другой вариант, теорема, теорема Радона-Никодима.
Значит, мера, мера Q на A задана такой формулой, значит, Q от A есть интеграл по A кси dp.
Ну, это если, ну, опять, если кси не отрицательно, так, значит, получается, что есть такая мера, значит, эта мера, эта мера абсолютно непрерывна относительно P на A.
Ну, из этого следует, из этого следует, что Q есть ρ умножить на P на A, так, где ρ, ρ, A измеримая функция, значит, это общая теорема Радона-Никодима,
что если есть две меры на сигма-алгебре, и одна зануляется на нулях другой, но это и значит, что абсолютно непрерывна, значит, зануляется на ее нулях, то та, которая зануляется на нулях, есть основная умножить на некую плотность Радона-Никодима.
Ну, вот это очевидно и есть, тогда очевидно, что вот это ρ, оно и есть нужная плотность.
Но эта теорема не очень элементарная, обычно лекторы, даже которые включают ее в курс, предпочитают не ввязываться в доказательства.
Значит, если ее аккуратно доказывать, то, ну, лекцию ухлопаешь на это дело, а когда лекции там всего десяток с небольшим, то бывает жалко на единичную теорему ухлопать целую лекцию.
Но теорема, конечно, принадлежит именитым мужам, и выдающаяся, конечно, теорема.
Значит, Радон – это австрийский математик, ну, выдающийся, так сказать, не только по этой теореме, реально выдающийся математик, выдающийся аналитик.
Ну, вот можно сказать, что это, так сказать, отец-основатель темографии.
Вот за, так сказать, технологическое воплощение его математических идей в начале 20 века, в середине 20 века, вот люди получили Нобелевскую премию за томограф.
Ну и другими он достижениями знаменит. Ну, вот эта теорема тоже, так сказать, ну, вот тут не поймешь, что важнее, томография или эта теорема, ну, кто его знает.
Значит, Никодим – это был такой чешско-польский математик, ну, он вроде как формально польский, но, кажется, ему это очень не нравилось, что он польский, и он себя как чешским выставлял.
Ну, естественно, как это часто бывает с теми, кто очень заботится, так сказать, о таких аспектах, он, конечно, все это исполнял не в Польше, не в Чехословакии, а в США, конечно.
Но этот вопрос его даже и там занимал, настолько занимал, что он даже в конце жизни ввел в написание своей фамилии диакритический знак,
которого не было в его вот этих знаменитых статьях тридцатых годов, в частности, где теорема Родон Никодима появилась.
Значит, он вот в это имя Никодим, он ввел, значит, диакритический знак, по-видимому с целью показать, что он не поляк, потому что у поляков таких диакритических знаков нет.
И даже вот на его памятники на кладбище США даже вот фамилия выведена с этим диакритическим знаком.
Ну, я точно не знаю, когда он это сделал, ну, очень похоже, что когда эмигрировал в США.
Но эта теорема появилась еще до того, значит, она появилась вот абстрактная теорема.
Значит, Родон это придумал для обычной меры Либега, ну, или там для иных мер на РН, а у Никодима в абстрактной ситуации.
В тридцатом году он это придумал. Но прославил его, конечно же, Колмогоров.
Значит, Колмогоров его прославил так.
Колмогоров в это время, когда вышел журнал со статьей Никодима, Колмогоров находился в Гетингене.
Ну, тогда еще можно было выезжать за границу.
Ну, у нас какие-то такие странные периоды бывают.
Можно, потом нельзя, потом опять можно.
Ну, как-то и не очень понятно, когда что будет.
Ну, вот Колмогоров попал как раз в такой небольшой кусочек, когда можно было.
И находился в Гетингене.
И, значит, в библиотеке там лежали на полках, ну, там у них раньше такой обычай был,
только что вышедшие журналы на полках выставлять.
Потом этот обычай долго продолжался, ну, больше ста лет.
Но примерно лет десять назад он почти исчез.
Вот я долгие годы в Германии наблюдал этот обычай в библиотеках,
а потом, когда все перешло, так сказать, в онлайн, он, значит, этот обычай исчез.
И там, так сказать, полки уже стояли пустыми.
Но во времена Колмогорова на них вот стояли эти вышедшие журналы.
Ну, и Колмогоров в этой библиотеке полистал этот журнал и наткнулся на теорему Никодима.
Она его заинтересовала.
А сам он в этот момент мыслями был со своей выдающейся статьей
об основаниях теории вероятности, которая его, так сказать, прославила
и действительно сделала отцом-основателем современной теории вероятности.
И по этой теории вероятности он подрядился писать монографию в издательстве Шпрингер.
Ну, зачем ему нужно было писать монографию в издательстве Шпрингер?
Дело в том, что они с Александровым на двоих незадолго до того приобрели в Подмосковье,
в поселке Комаровка, приобрели дачный дом.
И у него прохудилась крыша, и надо было эту крышу чинить.
Ну, вот сейчас я иногда бываю в этом мемориальном доме.
Сейчас там нынешний владелец Ширяев его, так сказать, в идеальном порядке держит.
Но тогда он был такой изрядной развалюхой, зверявой крышей.
Ну и вот надо было, так сказать, как-то снискать средства, чтобы починить.
Ну и Колмогорову кто-то присоветовал, вот, напиши монографию в издательстве Шпрингер,
прилично заплатит, и крышу можно починить.
Ну, Колмогоров за эту идею хватился.
Но каково ему было, когда он рекомендовал по пять страниц статьи писать, а тут монография.
И вот он написал страниц 70, а по договору там, кажется, больше надо было.
Я уж точно деталей не помню, но не хватало ему.
В общем, не пять страниц ему надо было написать для этой монографии, какой-то там объем.
И вот он уже все написал, выдающаяся работа, которая и сейчас считается одним из фундаментальных застяжений века.
А ему страниц не хватает.
И вот он увидел эту статью Никодима и его осенило.
Вот что у него сейчас будет. Условное математическое ожидание.
Это то, что мы сейчас с вами обсуждаем.
И вот это Колмогоров, как раз вот этот второй способ с помощью теоремы Никодима, он туда сразу и ухнул в свою книгу.
Значит, книга вышла, крыша была починена, и потом еще полвека они с Александровым на этой даче жили,
на лыжах катались, ученики на лыжах катались.
Так что теорема оказалась очень-таки прикладная.
Это Родон Никодима во всех отношениях.
Вот у нас появилось это условное математическое ожидание.
Что про него можно хорошего еще сказать, кроме того, что оно есть?
Во-первых, понятно, что если кси сама аизмерима, то ничего не нужно, то условное мат ожидания будет сама кси.
Дальше, значит, если взять это равно единице, то получаем, что мат ожидания от условного мат ожидания совпадает просто с мат ожидания.
Дальше, естественно, из этого первого получается, что от константа, конечно, константа будет.
Что еще можно сказать хорошего?
Еще можно сказать следующее, что если кси1 больше либо равно кси2, то условное мат ожидания почти всюду,
если вот эти вот почти всюду, то и эти будут почти всюду.
Но это очевидно из построения, потому что мы так строили, что у неотрицательных неотрицательные условные мат ожидания.
Ну и у нас линейная эта процедура, поэтому это свойство очевидно.
В частности, можно было даже так сказать, что если кси неотрицательно почти всюду, то условное мат ожидания тоже почти всюду неотрицательно.
Значит, это все, так сказать, такие довольно банальные вещи.
Теперь вот полезное предложение, давайте его даже отдельное оформим, это нам понадобится.
Все эти банальные наблюдения, конечно, тоже можно было бы назвать каким-нибудь там леммами, но вот реально полезная вещь, вот какая.
Если это ограниченное, а измеримое, то условное мат ожидания,
то ее можно выносить из подусловного мат ожидания.
Ну как водится, все такие равенства почти всюду.
Ну почему почти всюду?
Ну потому что случайная величина, она у нас определяется своим действием через интегралы.
Поэтому если ее заменить на множество меры 0, то интеграл этих манипуляций не заметен.
Но обратите внимание, заменять на множество меры 0 тут нельзя бы как, потому что условное мат ожидания должно быть а измеримым.
Поэтому если мы где-то его подменяем на множество меры 0, то само это множество должно быть из а, а не а бы какое меры 0.
Ну давайте это докажем, доказательства.
Это предложение, доказательства.
Ну смотрите, что нужно доказывать.
Нужно сравнивать интегралы по множеству из а.
Спрашивается, верно ли, что интеграл вот от этого равен интегралу вот от этого.
Вот вопрос еще такой, будет ли такое равенство.
Ну давайте посмотрим, что это за равенство.
Смотрите, что тут слева стоит.
Слева стоит интеграл по омега от индикатора а умножить на условное мат ожидание от это кси dp.
Вот кто такой слева стоит.
Ну и по определению, это есть интеграл по омега от индикатора а умножить на вот на это дело.
Значит здесь я dp забыл.
Значит это тот, кто стоял слева.
А справа, а справа, кто стоит.
А справа стоит интеграл по омега от единицы на а умножить на это и умножить на вот на это дело.
Вот кто стоит.
Но вот эта штука, вот эта штука, она аизмерима.
Значит вот это произведение, оно аизмеримо.
А поэтому, поэтому это это же будет интеграл.
То есть то же самое.
То же самое получается.
Так что видите, вот доказали путем сравнения интегралов по множеством из а.
Теперь давайте какой-нибудь пример рассмотрим простой.
Давайте рассмотрим простой пример вычисления.
Вообще это страшно важная вещь.
Вот эти условные мат ожидания в теории вероятности, в теории случайных процессов, в статистике и в теоретической прикладной.
Это все страшно важная вещь.
Так что Калмогорову воистине повезло, что он нуждался в ремонте дачи и наткнулся на эту статью Никодима в библиотеке немецкого университета.
Почти никогда эти условные маты... вот это такая странная вещь.
Почти никогда нельзя вычислить.
Вот такой вроде важный объект.
Но это не тот неуловимый Джо, которого никто не может поймать.
Это реальная вещь, которая нужна и применяется.
Давайте посмотрим.
Пусть Омега разбита на...
Вот давайте так.
А1...
Аn дизюнктные.
Дизюнктные множества.
Вот это сигма алгебра.
Разбита на конечное число кусков.
Аa это порожденное вот этими множествами.
А сигма алгебра...
Но она очень просто устроена.
Когда у вас пространство разбито на несколько кусков дизюнктных,
вот такая вот картинка.
То как устроены множества из сигма алгебры?
Это пустое множество.
И какие-то конечные поднаборы вот этих.
Можно брать их по отдельности парами, тройками.
В общем, всякие такие наборы, ну и плюс еще пустое, будут образовывать сигма алгебру.
Ну как очевидно, это сигма алгебра на конечный просто будет.
И как устроено...
Давайте посмотрим, как устроено условное мат ожидания интегрируемой функции
вот для такой очень простенькой сигма алгебры.
Значит, оно устроено так.
Значит, эта сумма будет вот чего.
Нужно взять интегралы по этим множествам.
А давайте я еще только забыл сказать, пусть эти множества еще положительные меры.
Вот это я еще забыл сказать.
Значит, я про это вспомнил, когда стало нужно делить на них.
Вот, значит, вот какой ответ.
Смотрите, это будет такая ступенчатая функция, которая постоянно на каждом вот этом кусочке.
И чему оно равно, какое значение этой функции на кусочке.
Это среднее значение ее по этому кусочку.
Среднее значение это интеграл делить на меру кусочка.
Значит, как устроены функции измеримые относительно такой очень простенькой сигма алгебры.
Это и есть ступенчатые функции.
Измеримые функции это те, которые на этих кусочках постоянные.
То есть получается, что конечномерное пространство.
Это конечномерное пространство таких вот ступенек на этих множествах.
И то, что здесь написано, это выглядит как артагональная проекция на конечномерное подпространство.
Если бы кси была из s2, то это и было бы артагональной проекции на это конечномерное подпространство ступенек.
Но в общем случае это не скалярное произведение интеграл, но формула в общем случае такая же.
Ну давайте проверим, почему это верно.
Давайте проверим, почему это верно.
Ну достаточно, поскольку у нас линейно, поскольку у нас условная мат ожидания линейна,
то достаточно рассмотреть вместо кси умножить на индикатор.
Ну скажем первого множества.
Ну для остальных аналогично.
Значит, эта функция слагается из суммы.
Она умножить на индикаторы вот этих.
Потому что сумма этих индикаторов единица.
Значит, если мы для каждой такой проверим, то ответ будет перенесенный на общий случай.
Значит, смотрите, в этом случае получается следующее.
А условная, ну вот это что нам эта формула дает?
Значит, эта формула нам говорит, ну это еще подлежит проверке.
Но если это так, то смотрите, в этой формуле исчезли все интегралы, кроме одного.
Потому что новая функция, она вне всех ноль, значит все интегралы исчезают кроме первого.
Поэтому получается, что вот такой должен быть ответ.
Интеграл по a1 делить на меру a1 и умножить на индикатор a1.
Вот какой должен быть ответ.
Значит, если эта формула правильная.
Ну и наоборот, если ответ такой, то значит формула правильная.
Значит, ну давайте проверять.
То есть видите, это я еще пока, так сказать, со знаком вопрос поставил, верно ли.
Значит, правая часть определена, а левая живет в какой-то своей внутренней жизни.
И вот почему она совпадает с правой.
Значит, ну давайте проверять.
Значит, смотрим, что тут полагается смотреть.
Значит, смотрим на интегралы обеих частей по множествам a из сигма алгебры.
Но эти множества, это конечные объединения ожитых.
Поэтому достаточно смотреть на интегралы по ожитым, ну по отдельным вот этим ожитым.
Значит, у левой части, интеграл левой части.
Значит, интеграл левой части, что это такое?
Это просто интеграл по ожитому, ну вот этого вот.
Xi умножить на индикатор a1.
Значит, правой части.
Значит, правая часть.
Значит, смотрите, что дает правая часть.
Ну это вообще число, оно выносится при интегрировании.
А это интеграл, ну по ожитому, вот индикатора a1.
Значит, чему это равно?
Ожитое это какое-то из этих множеств.
A1 это первое фиксированное.
Значит, если ожитое, ну если Xi равно 1, то это просто будет,
из всего этого останется интеграл вот этого.
Потому что здесь будет просто мера ожитого,
то есть мера ожитого равна a1 и сократится с этим.
Это если Xi равно 1.
А 0 будет если Xi не равно 1.
Вот такой ответ получился.
Значит, у правой части будет либо 0, либо этот интеграл.
А у левой части, что будет?
Ну опять, ну это будет как и левая часть.
Потому что здесь то же самое.
Если же не 1, то это 0.
А если же 1, то это просто интеграл от Xi по a1.
Так что вот прямая проверка показывает, что все так.
Ну еще, если же пользоваться вот этой апелляцией к L2,
то тут ничего и обосновывать не надо.
Потому что смотрите, какой наглядный смысл этой формулы.
Это как раз ортогональная проекция на конечномерное подпространство.
Как устраивается ортогональная проекция на конечномерное подпространство?
В нем берется базис ортонормированный.
И как будет выглядеть ортогональная проекция?
Нужно брать скалярные произведения с элементами этого базиса, ну и умножать на них и суммировать.
Вот как будто бы мы просто по базису суммируем,
но только суммируется не по базису всего пространства, а по базису этого подпространства.
Но кто здесь в этом подпространстве, кто здесь базис?
А вот эти индикаторы этих множеств ожитые, они попарно ортогональные.
Чтобы они стали базисом, нужно еще на константы их умножить, пронормировать, тогда это будет базис.
И тогда если пользоваться этой формулой ортогонального проектора, то в точности то, что у нас написано, и получится.
Поэтому, в общем, ответ неудивительный.
Ну, кстати, вот таким способом можно было, без вот этой проверки, таким способом можно было бы это обосновать.
Так, теперь, сейчас, ну еще есть несколько минут, сейчас я гляну, что-то я еще собирался.
Так, что-то я еще собирался.
Значит, что-то я еще собирался сказать.
А, вот, значит, полезное, сейчас у нас будут некие связанные с этими делами объекты, но это уже в следующий раз.
Значит, мартингалы, ну и уже такая вот более, так сказать, вероятностная пойдет дискуссия.
Но сейчас еще одно техническое неравенство.
Значит, неравенство Янсона.
Неравенство Янсона.
Значит, давайте я, чтобы обозначения были как в конспекте.
Значит, пусть В – выпуклая функция, значит, выпуклая функция на R.
Кси – интегрируемая случайная величина.
И еще дополнительно предполагается, что В от кси тоже интегрируемо.
Тоже интегрируемо.
Тогда верно вот такое вот неравенство.
А В от, давайте так напишем, В от условного мат ожидания относительно А меньше либо равно, чем условное мат ожидания В от кси.
Вот так.
Значит, это почти всюду.
Значит, это почти всюду.
Значит, смотрите, обычная, обычный Янсон.
Значит, обычный Янсон.
Это условное мат ожидание, наоборот, обычное мат ожидание, сейчас, В от обычного мат ожидания меньше либо равно, чем мат ожидания от композиции.
Это обычный Янсон.
Видите, это неравенство для интегралов.
Можно еще так, естественно, написать, что В от интеграла от кси оценивается через В от кси проинтегрированное.
Это для чисел неравенства, для чисел и для интегралов.
А тут не совсем так.
Потому что, смотрите, сюда-то вы подставляете числа, ну, там, завище от Омега, вот эта функция, вы ее в В подставили.
Но справа, справа, у вас уже получилось, ну, у вас в чистом виде функция получилась.
Это, так сказать, не число завище.
Ну, конечно, функция тоже есть число, завище от Омега, но это не так, что, так сказать, это число как бы другого сорта, чем в левой части.
Поэтому так не очень банально, не очень банальным и очевидным кажется, как вот это связано с тем.
Понятно, что вроде как вещи общие, но и там Янсон, и там Янсон, там ожидание, и здесь ожидание, и там выпукло, и здесь выпукло.
Вроде ключевые слова те же, а как бы это доказать?
Ну вот сейчас нам уже надо сделать перерыв, но, значит, способы доказательства такие, способы доказательства такие.
Один способ, значит, посмотреть, вспомнить, как доказывается неравенство Янсона вот в этом виде, ну и, так сказать, как-то модифицировать рассуждение в этом случае.
Это один способ. Другой способ.
Вспомнить, что когда что-то доказывают для там какие-то неравенства общего вида для функций, то обычная технология такая,
доказывают для простых функций с конечным числом значений, но потом там предельным переходом устраивают в общем случае.
Это другой способ, но, значит, каждый раз мы будем утыкаться, что вот при чем тут почти всюду, там никаких почти всюду не было, а тут почти всюду.
Это каждый раз будет, значит, некий предмет раздумий.
Ну и, наконец, третий способ. Вот это я сейчас так на обум скажу и сделаю перерыв, чтобы не надо было его обосновывать, потому что я никогда таким способом не делал.
Третий способ такой. Ну для того, чтобы доказать, что это верно, ну вообще-то вроде бы как надо что сделать?
Просто-напросто проинтегрировать обе части по множеству из А.
И если окажется, что после интегрирования обеих частей каждому множеству из А будет неравенство, ну значит все окей и есть.
Но когда вы проинтегрируете, когда вы проинтегрируете по множеству, значит, А обе части, то здесь, ну здесь понятно, что получится.
Вот эта часть, это будет просто интеграл, вот такой будет интеграл, если вы проинтегрировали.
Вот будет вот такой интеграл. А здесь что получится? А здесь получится вот что.
Интеграл, ну вот от этого дела получится интеграл. Вот смотрите какая вещь.
Ну и вот вопрос, что с этим можно поделать? Там уже никаких условных мат ожиданий нет, ну и там некое число стоит.
А здесь стоит вот такой интеграл. Но вот все упование, все упование, вот на это неравенство.
Вот надо подумать, нельзя ли как-то грамотно, нельзя ли как-то грамотно вот то неравенство применить.
Вот это надо подумать, можно ли его как-то применить.
Но это такой способ сомнительный, поэтому я в этот момент лучше всего и останавливаюсь.
Ну а в качестве задачи, ну вот как придумайте, как доказать этого Янсона.
Так, все, значит тогда на этом пока закончим.

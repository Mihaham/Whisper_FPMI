Уважаемые коллеги, у нас сегодня будет заключительная лекция в рамках курса математика больших данных.
Я попросил Алексея Наумова эту лекцию провести. Алексей оказался у него в конце года, достаточно там сейчас много дел.
И доклад будет делан Дениил Тяпкин. Дениил Тяпкин очень перспективный, очень активный молодой ученый.
У нас уже несколько статей на топовой конференции. Вообще у нас он уже выступал и очень здорово.
Мне кажется, что это хорошо, что у нас сейчас будет такая возможность Дениила послушать.
И рассказ будет про математику обучения с подкреплением. Это будет заключительная тема.
И на этом основной цикл заканчивается. Дальше уже будет контрольная и сдача курса, сдача проектов и так далее.
Но замечу, что элементы reinforcement learning могут войти в контрольную, которая будет в следующем среду.
Вот, собственно, мы начинаем. Формат стандартный. Сначала полуторачасовая лекция, потом часовая.
Через небольшой перерыв. Дениил, пожалуйста, начинайте.
Дениил, меня слышно было?
Да, вас было слышно.
Так, я не верю, что Дениила нас слышит. Дениил, вы нас слышите? Меня слышно?
Можно начинать, говорю.
Не коллеги, не палатки.
А, вот сейчас слышно. Сейчас слышно.
Но презентацию зато не видно.
Коллеги, подскажите, видно ли презентацию?
Да, да, сейчас видно.
Все отлично. Да, к сожалению, я из-за неполадок пропустил представление от Александра Роднировича.
Поэтому я сразу перейду к своему докладу.
Алексей попросил меня прочитать эту лекцию по математике машинного обучения.
Поэтому как бы по названию курса мы будем в первую очередь ориентироваться на некоторую математику.
По-другому эту лекцию можно назвать приложение концентрации меры в обучении с подкреплением.
И в первую очередь речь будет как раз об этом.
То есть немножко напомню, что такое обучение с подкреплением.
В данном случае мы хотим обучить некоторого агента тайно замениться средой.
Агент находится в некотором состоянии СТ.
Он делает некое действие АТ.
Действие это допустим пойти влево, пойти вправо.
Что-то такого духа.
После чего окружение, нечто называемое окружение, реагирует на это действие.
Это дает следующее состояние, в котором будет находиться агент и некоторую награду, которую он получает.
И так агент пытается максимизировать сумму наград, которую он получает.
Концепт довольно простой, но понятно, что здесь очень много подводных камней.
В первую очередь хочется оппозиции такого окружения.
То есть для этого мы расскажем некоторые базовые вещи про марксиспроцепция решений,
как называемые МДП.
И докажем оценки на регресс для некоторого магического алгоритма обучения с подкреплением.
Начну с таких базовых определений.
Марксиспроцепция решений или MarkovetitionProtos или МДП.
Это некий кортеж из, в этом случае, 5 элементов.
С – это процесс состояния, то есть всего может состояние, где может находиться агент.
А – это действие, которое может совершать агент.
То есть пойти влево, пойти вправо, и все в таком духе.
П – это переходное ядро.
То есть если агент находится в состоянии С и делает действие А, то в какое состояние после этого попадет?
Вот это определяется, основываясь на текущем состоянии и выбранном действии.
И это ядро – это в некомерном состоянии, которое ассоциируется с следующим состоянием, которое получается.
Также есть еще награда, которая зависит только от текущего состояния действия.
Будем для просты считать, что она детерминирована.
На самом деле это, конечно, не так.
То есть награда обычно случайна, но для просты картинки будем считать, что она детерминирована.
И мы будем рассматривать в случае конечного горизонта планирования.
То есть после вот этого аж большого шагов игра начинается заново.
То есть допустим, игра шагов.
С – это количество возможностей в состоянии доски.
А – это возможное действие, куда поставить, допустим, какую-нибудь коня, все вот эти возможные установки.
А П в этом случае будет некая детерминированная вещь.
Он будет не случайным.
Она будет просто стоять там единицей в неком состоянии, о котором мы прийдем в следующее.
А функция наград уже будет как-то правильно дизайниться.
Есть вообще большая наука про дизайн наград, как правильно это делать.
И горизонт – это максимальная длина игры в жахматы.
Там, не знаю, словно 100 шагов, после 100 шагов игра заканчивается, объявив ничья, допустим.
Соответственно говоря, есть простые среды.
Допустим, вот такой вот мини-грид среда, которая забрана слева.
Цель агента в этом случае треугольничка – это найти зеленую награду, зеленую клетку.
У него награда для всех состояний действий нулевая, кроме случая, когда он достигает зеленой клетки.
Туда у него награда ничная.
Есть более сложные примеры, когда процесс состояния очень большое.
Это, допустим, игры в Atari.
То есть здесь процесс состояния – это просто все возможные картинки, все возможные состояния доски.
По сути говоря, не доски, а экрана самой игровой приставки.
И действие – это подвинуть влево, подвинуть вправо нашу платформу.
И награда, соответственно говоря, это в точности количества заработанных очков для выполнения действия.
Сам процесс обучения в MDP устроен так, что у нас есть, допустим, табулярное MDP, то есть слишком большое.
Для большого количества состояний нужна всякая высокая наука про опроксимацию и все прочее.
Ну, для нас это довольно простой случай.
Есть некоторый горизонт планирования, как я уже говорил ранее.
С – это число состояний, А – число действий.
Все конечное для простоты.
А сам процесс обучения, как я до этого описывал, у нас есть какое-то количество эпизодов.
Это количество партий в шахматах, скажем.
Внутри одного эпизода у нас есть, допустим, десятый шаг нашей партии.
STH – это состояние доски в игре T в момент времени H.
Дальше выбирается некоторое действие – ATH.
И, собственно говоря, выбирается…
Следующее состояние – оно поделяется средой.
То есть поделяется неким вот этим ядром P, который мы не знаем.
Но если бы мы его знали, все было бы просто.
В нашем случае все не так просто.
И мы получаем после этого некоторую награду вместе с состоянием,
которую для простыбы считать известной.
Опять же, есть вариации на тему, когда она случайная, еще в таком духе.
Как выбирается действие?
Здесь выбирается согласно некоторой политике P.
То есть к некоторому правилу как выбирать действие.
Высокая наука предписывает, что можно…
Высокая наука предписывает, что можно это вот P выбирать не от каких-то распределений на действия, а от терминированных.
То есть некий результат, который можно доказать.
Мы его примем бездокачественно, доказываться несложно.
И цель агента – это найти такую политику P,
которая максимизирует value-функцию.
На самом деле, это небольшой обман, что политика P зависит от шага H.
То есть, условно, в конце игры она выгоднает некие жадные шаги,
которые дают какую-то максимальную награду прямо сейчас.
А в начале мы хотим действовать более стратегически.
То есть при маленьких H наши шаги должны как-то вести в зону с большой наградой.
То есть, допустим, вот в этом примере, если у нас была бы еще какая-нибудь добытная награда маленькая,
то под конец игры нам выгодна просто пойти в эту маленькую награду.
Если же у нас начало игры, нам выгодно развернуться и пойти в сторону большой.
Вопрос такой, а вот у нас как бы меняется состояние S с шагом?
Да, у нас каждый шаг выбирает новое состояние, и мы дальше уже от него шагаем.
То есть у нас статируется новое состояние ST H+,
и в следующий раз у нас будет политика P H+,
которая по этому состоянию определяет следующее действие, которое мы хотим выбрать.
Да, я как раз хотел спросить.
То есть у нас получается вот S тоже зависит от сделанных нами шагов,
и мы как бы все в совокупности учитываем при политике следующей.
Да, да. Все так.
То есть это такой, скажем так, это сеттинг теорический,
потому что на практике, можно назвать дисконтированный сеттинг,
когда у нас игра считается бесконечно долгой,
но у нас следующее действие имеет меньшую стоимость.
Все, что будет в будущем.
Там профессионально, некоторым дисконтирующим фактором.
Для теории гораздо удобнее сеттинг кобизонический,
там, где у нас для каждого H, ну по сути можно считать, что меняется состояние.
То есть все возможное там партии на первом шаге, все возможно партии на втором шаге.
Шаг можно, допустим.
То есть вот мы играем вот такую игру,
скажем так, что у нас меняется состояние,
она может на самом деле стать тем плюсом,
но политика у нас изменится.
У нас политика, это не она функция из состояния в действии,
а набор из H функции на самом деле.
Ну да, просто мне, просто я как-то хотел
следить за тем, что у нас возможно,
чтобы мы в модели не учитывали
будущее знание в прошлом,
чтобы мы не моделировали так
нашу систему,
но, наверное, так все и делается.
Да, то есть эта политика, она не знает
будущее состояние, они простомплируют в такой момент выбора в действии,
поэтому все получается довольно некой в мысли справедливо.
Соответственно говоря, дальше у нас есть эта value функция,
то есть это сумма наград начинает с шага H.
То есть мы хотим максимизировать наш сумму наград на первом шаге в идеале,
но политика у нас каждый раз меняется,
поэтому мы хотим на самом деле найти такую последность действий,
чтобы начало состояния S
максимизировать сумму наград.
Ровно то, что я описывал на вот этой красивой схеме,
которая бывает в каждом общем докладе по обучению с нащуплением.
То есть мы находим состояние S на шаге
на момент времени H внутри одного эпизода.
То есть если H маленький равно 10,
то после 10 шагов шагмата мы говорим, что пусть доска такая,
то на какую награду мы можем получить,
если будем поддерживать стратегию P, начиная с этого момента времени.
Вот эта политика P у нас разная здесь,
поскольку значит, что каждое следующее состояние,
каждое следующее действие выбирается согласно политике P
от предыдущего действия, а следующее действие
S H плюс один, допустим, оно будет согласно
этому ядру
для предыдущего состояния S H
и действия H, которое выбирается из политики.
Вот концепт какой-то такой.
То есть мы ходим, мы играем, мы выбираем шаги,
и мы хотим правильное правило выбора шагов
для каждого возможного состояния, для каждого шага внутренней партии.
Вместе с value функцией,
функцией полезности, можно найти action value функцию,
то есть Q функцию.
Q функция – это объект, который закрепляет
на шаге H не только состояние, но еще и действие.
То есть мы считаем, что получилось бы дальше,
если бы мы играли согласно политике P,
и какую сумму награду мы получили в этом случае.
Опять же, задача найти политику,
чтобы максимизировать все, что угодно.
Value функцию и Q функцию можно вместе
клеить в неком смысле помощи так называемых уровней Белмана.
То есть Q функция – это награда на текущем шаге,
но оно и понятно, почему оно должно быть наградой на текущем шаге,
потому что у нас здесь первая слагаемая в неком смысле закреплена.
То есть reward на F при H3 равном H маленькому
он у нас уже известен,
потому что мы знаем текущий состояние, текущие действия.
А все, что дальше, на самом деле,
определяется под этим от ожидания.
То есть вот это вот ПВ,
по-хорошему В должны быть скобки вокруг вот этого ПВ.
Может это действие, это марковская ядра на функцию.
В таком от ожидания, это Value функция в следующем состоянии,
которые мы уже знаем.
Я не очень понимаю,
то ли у меня проблемы с интернетом,
то ли Даниил подвис, коллегия,
если вы меня слышите, можете сказать.
У вас так же ощущения?
Я слышу вас, я не слышу.
Да, Даниил,
у вас какие-то проблемы со связи?
какие-то проблемы со связи? мы вас сейчас вы отрубились в видео. вы что-то притормозил.
можете повторить и снова расшарить презентацию? да конечно. вот Bellman equation,
где вы начали объяснять марковский оператор. вот тут было уже все, не слышно. да конечно.
то есть вот этот оператор это так называемые действия ядра на функцию. то есть по сути
говоря это вот. можно просто смотреть на эту формулу. то есть мы сэмплируем действие s'
нашего ядра и усредняем все возможные состояния функции. то есть вот эта штука это от ожидания по
следующему состоянию при условии, что мы сейчас сидим в состоянии s и делаем действие a.
такое усреднение по будущему состоянию. то есть если бы у нас не было бы действий,
то это было бы некое стандартное действие теории марковских цепей. то есть от ожидания по
следующему состоянию. так как у нас в следующем состоянии зависит не только от состояния
начать действия, то получается, немножко не стандартная нотация, что у нас здесь два
аргумента есть. то есть мы по сути некому смыслу их умножаем. это ядро p на f и смотрим точки s'
примерно выглядит так. то есть на самом деле эта штука это в точности некоторое следствие
свойства условного манемического ожидания. это можно очень трудно доказать. то есть будем
доказывать по индукции. наша биозидическая структура позволяет нам легко делать индукции.
то есть для h плюс один мы здесь все определяем равно нулю. и в функцию и ко функцию. просто по
определению. потому что если у нас h маленькое равно h плюс один, у нас сумма по нулю слагаемых,
она будет конечно равно нулю. поэтому для h плюс один все верно. теперь мы записываем
определение ко функции. а просто в таком от ожидания. затем по линейному от ожидания выносим
первое слагаемое. у нас остается вот такая сумма. а затем мы можем сказать, что у нас просто
мот ожидания, а давайте еще внутрь запишем условно мот ожидания. это ничего не изменит, потому что мы
будем от ожидать по этому h плюс один. но мы можем заметить, что вот эта штука это в точности
валют функции. по определению. поэтому можно записать это как валют функцию и получить, что ко функции это
reward плюс мот ожидания валют функции в следующем шаге. а действие ядра мы приняли ровно так. то есть
можно на самом деле об этом думать именно вот так, что вот это действие pvp с точностью вот
такая запись на предпоследней строчке. мот ожидания vp h плюс один при условии, что предыдущее
состояние это sh и действие это a h. то есть на втором уровне Боумана, который определяет vp через q,
он тоже пишет довольно просто. мы просто используем power property, то есть
кископическое свойство условно мот ожидания. просто здесь добавляем на h, потому что почему бы и нет,
мы все равно по нему промот ожидаем и замечаем, что внутри мы получаем в точности q функцию. после этого
мы видим, что наша q функция при условии закрепленного состояния, следующее действие
определяется однозначно из политики p. мы получаем, что v функции вращается через q функцию. то есть вот эта
запись, это в точности некая fancy запись условных мот ожиданий, чтобы v функции вращались через q функции
наоборот. это мега удобно. это мега удобно еще по причине того, что можно сделать некое расширение
этой штуки для оптимальной политики. то есть оптимальная политика, она максимизирует value функцию
для всех возможных состояний, для всех возможных шагов h. для конечного размера, конечного множества
состояний и построить довольно не трудно, нужно просто решить вот эти оптимальные уровни bellman.
то есть здесь опять же наша политика, это будет то действие, на котором этот максимум достигает.
то есть что такое оптимальный уровень bellman? у нас q функция пришла через оптимальную v функцию
в следующем шаге точно так же, как и в обычном уровне bellman. но v функция считает через q функцию как максимум.
то есть мы выбираем наиболее жадное действие, взаимодействие на наши оценки на q функцию,
которая приносится на максимальную награду. и утверждается, что эти уравнения ровно определяют
оптимальную v функцию. если не должно быть этого индекса h, это некая опечатка. но идея такая, что
просто если мы возьмем максимум вместо взятия в политике, то мы по всей этой конструкции можем
построить и оптимальную политику, и у нас есть оптимальная q и v функция, что замечательно. это
доказывается используя обычный уровень bellman довольно несложно. то есть первое, что мы можем заметить,
что наша q функция это вот supremo по всем политикам. наша q функция для p. а для q p можно
поставить оптимальную уровню bellman. тогда все вынесется сюда направо. после чего мы можем
воспользоваться, допустим, тиремой Bepa-levia, потому что все у нас здесь ограничено, все у нас
замечательно. у нас v функция ограничена, поэтому supremo можно передащить само дожидание.
с действием midrp. а это supremo это в точности p в s звездой. далее, чтобы получить v функцию как
максимум функций, просто уходим с тем, что если мы максимизируем политику, то это то же самое,
что for bellman это просто qp h от s и некоторое действие в основном на политике. давайте по
нему промаксимизируем, по действию, которое у нас здесь есть. после чего у нас получится два
максимума, можем их переставить и получить точность оптимальной уровни bellman. то есть,
казалось бы, все хорошо, просто решаем уравнение, начиная с конца, потому что v с звездой h плюс 1
большого, оно известно, все замечательно. но проблема в том, что мы не знаем ядро. и как бы все
интересные моменты обучения сцепления начинаются как раз с момента, что дети мы не знаем ядро и мы не
умеем из него допущенно сэмплировать. то есть мы не умеем сэмплировать для произвольного
nsa. ну давайте посэмплируем, получим хорошую оценку и решим уровень bellman. замечательный подход,
получается в правильном порядке. но в нашем случае мы будем рассматривать так называемое
анонимное обучение сцепления, когда можно выходить только траекториями. у нас есть некая политика
api, мы можем по ней пройтись траектории, получить какие-то данные, в основе этих данных улучшить
политику. мы не можем допустим обученно улучшить качество ядра для какого-нибудь
отдаленного действия, в которое мы редко были. нужно как-то придумать сначала политику, которая его
посетит, а только потом его как-то использовать. то есть вот в этом framework у нас есть некая
политика api. то есть api 1 это некая изначальная политика. допустим, он случайно действует,
потому что мы ничего не знаем о среде. на втором шаге мы уже какие-то знания получили о среде,
мы можем ее немножко улучшить и так далее. а в чем главный пафос всей этой обосновки в том,
что мы должны, эта политика api не просто делать максимально жадные действия относительно текущих
представлений о среде, но еще как-то правильно исследовать нашу среду. то есть у нас получается
некий trade-off между тем, что эта политика api, с другой стороны, должна быть довольно хорошая,
потому что у нас в нашей мере качества, в нашем rig-rate, она есть прямо вот невязка
между оптимальной v-функцией и v-pt. но и при этом мы должны исследовать, чтобы следующие политики
были еще лучше. прежде чем как-то погрузиться в эту тему, как раз таки Стофим говорит в начале,
что доклад было правильно назвать применение констрации меры обучения с подкреплением. я напомню
некоторые классические результаты констрации меры. первый классический результат, который нам
понадобится, это неравенство зума кердинга. то есть пусть у нас есть некая последовательность моркови
разности. то есть нечто с конечным от ожидания, что измеримость некой фильтрации заранее закрепленной,
и мот ожидания этого уйн при условии предыдущей фильтрации равно 0. то есть, скажем, если уйн просто
аид исключенной величины строевого мот ожидания, то это отлично подходит. и для таких мобилоразностей
можно написать неравенство зума кердинга. то есть если все уйн ограничены, то на самом деле они
довольно хорошо констрируются от ожидания, которого было 0. то есть в точности это здесь
подтверждает, что вероятность того, что сумма ук будет больше 1 на t, она шкалируется как квадрат,
то есть некая опечатка. то есть она шкалируется как квадраты этих штук сумма квадратов,
если у нас было бы среднее, и еще мы поделим все на n, и допустим у граничной единицей изначальная,
то есть пусть yk это 1 делить на n некой другие, xk скажем, тогда эти ck квадрата это будет 1 делить на n
квадрат и сумма n штук. у нас получается специальный констракт. к сожалению, я пропустил квадрат,
довольно важен, но в следующем неразистое оно есть, что при помощи неразистое суммы кердинга
можно доказать некий более могучий факт, что если у нас есть некая функция, которая уничтожает
некоторому свойство граничной разности bound difference property, то говорит то, что функция не
сильно чувствительна в каждой отдельной координате. то есть меняя одну координату, мы можем изменить
значение функции не больше, чем на некое cit, которое имеет точно такую же роль, как и здесь, что у
нашей yk не больше, чем ck, то есть есть примерно такая же штука. тогда эта функция f отлично консервируется
в своем отожидании, тоже экспоненциально, если мы все у средней провели. если наши cit
какие-то маленькие, допустим 1 на n, то у нас получается правильная констракция.
вероятно говоря, используя этот замечательный факт, неразистое могдярмидо, мы можем получить
вот такой результат на констракцию для 1 норм. давайте к нему вернемся чуть позже, чтобы было
понятно, зачем он нужен. то есть давайте вернемся, собственно, к количеству крепления.
перед нами стоит задача исследования, как я говорил ранее, что у нас в нашем пределении regret
у нас сумма ошибок нашей политики для каждой из этих политик pt. в чем проблема в том, что если
мы будем шагать pt слишком жадно, то следующие политики будут плохие, и эта сумма regret будет
большой. сумма этих невязок. если мы будем исследовать в начале много, чтобы потом была хорошая
политика, то в начале у нас будет большие эти невязки. нужно как-то это правильно балансировать.
есть вариант, как балансирование происходит неправильно, допустим, есть action grid исследования.
то есть action жадно, у нас есть некая оценка на q-функция, допустим, получена уровень bellman,
если мы вместо изра поставим его at. какая-нибудь оценка на q-функцию правильная, которая по моему
ожиданию как хорошую q-функцию. и мы сравниваем, что epsilon действует случайно, чтобы исследовать
среду. 1-epsilon действует жадно, согласно текущим председаниям от 3d. тогда если даже эти epsilon
неким правильным образом уменьшать в зависимости от t, то regret получается сублинейный, что уже неплохо,
но не самый лучший. и более того, здесь на самом деле спрятана вот под этим большим экспоненциальной
зависимость от изменения состояния. если мы возьмем такую среду, то есть среда так называемая, и много
названий этой среды, это можно назвать chain, может быть reverse swim. но в чем идея? мы находим состояние с 1. у нас
есть два действия. у нас есть действие пойти налево, что у нас всегда удается, либо действие пойти
направо. пойти направо у нас получается с трудностью 1.9 на 1.7. то есть мы можем проиграть, тогда мы
снова вернемся налево. правильная стратегия это всегда идти направо. мы идем-идем-идем-идем-идем направо,
пока не назовем это наградой 1. если он отдаст его назад, неважно, мы еще раз делаем. еще проблема
потому что на сцене с2 он будет пробовать пойти налево. все-таки с какой-то вероятностью. он будет
пробовать пойти налево и тем самым сдаться свой прогресс по достижению этой награды. даже если он хоть
один раз дойдет до правой награды и узнает, что там есть хорошая награда и больше идти некуда,
начнут всегда идти туда, то F2D будет нас сбивать с пути, будем очень часто идти назад. и таким
образом у нас получается очень плохое количество состояний. даже для F равно 100 это уже что-то не
очень приемлемо. и вот такое вот балансировать исследование и использование данных получается
довольно плохим. как делать правильно? на самом деле правильно неким образом использовать
так называемые оптимистические оценки ко функции. то есть использовать принципы оптимизма
предсовывая неопиневеленность. то есть пусть наши средние оценки наших ко функций обозначены синей
чертой. то есть действовать жадно действует согласно максимальной этой ко функции. если мы будем действовать
жадно, надо шагать по этой синей черте. мы не постепенно сходимся к кусо-звездой каждым новым шагом,
но мы на самом деле никогда не будем после этого дергать за левую ручку. у нас есть 3 действия. для
каждой из них есть некая uppercut and bounce. то есть некая верхняя квантиль. то есть мы с большой вероятностью
нашу реальную кусо-звездой находим где-то ниже этой uppercut and bounce. и ее другой важный свой
то, что она постепенно концентрируется вокруг кусо-звездой. то есть с каждым дополнительным
использованием этого действия наши границы, с левой справа, начинают схлопываться вокруг
кусо-звездой. но в текущий момент у нас есть такие вот такие оценки. данных мало, имею что имеем.
если мы будем действовать жадно согласно среднему, то мы никогда не будем использовать первое действие.
просто потому что текущая оценка среднего она точно меньше чем оптимальная кусо-звездой,
к которой мы будем сходить. здесь даже вот здесь немножко не поместилась счет надпись,
но здесь нижняя оценка на вот это второе действие, она лежит выше этого среднего. то есть если мы
будем действовать согласно среднему, то мы наверняка будем пытаться делать только второе действие,
тем самым у нас будут плохие оценки. то есть у нас наше UCB будет постепенно уменьшаться кусо-звездой.
если мы будем действовать согласно среднему, то у нас сейчас схлопливается во втором действии,
мы будем только лишь его дергать. возможно треть. но третья тоже у него средняя, тоже ниже кусо-звездой
для второго. поэтому средние будут постепенно опускаться для второго действия. и мы никогда не
будем дергать ни первого ни третья, хотя там реально кусо-звездой она больше. то есть нам следовало
бы или пробовало первое и третье действия. если мы действуем согласно UCB, то мы на самом деле
некогда не можем пролететь мимо оптимального действия. то есть они будут постепенно сконцентрироваться
в группу кусо-звездой, но UCB никогда не может стать меньше, чем кусо-звездой просто по
строению. поэтому мы не можем его пролететь. некого смысла. оно будет постепенно сходиться в кусо-звездой,
но при этом, если вот для второго действия у нас какой-то момент, наша UCB SA2, она станет ниже,
чем кусо-звездой для первого. просто потому что оно должно сходиться. тогда мы будем дергать за первое действие.
вот. то есть концепт немножко запутанный, а в предыдущем примере это выглядит примерно так.
то есть давайте сначала определим некую в функцию для некого ядра p-штрих. то есть это ядро
p-штрих может отличаться от того, с чем мы реально играем, но для некоторой модели p-штрих мы делим
функцию вот так. тогда мы хотим действовать так, а поделим для моделей некое дополнительное множество.
такое, что там содержится реальная p. это близко к этой идее, что мы хотим, чтобы наша кусо-звездой всегда
была ниже этого UCB, что у нас все возможные функции, которые мы имеем в виду, лежат здесь. где-то вот здесь.
тогда что мы можем сделать? тогда мы можем играть в политику следующим образом. мы находим некое
самооптимистичное ядро p-штрих из множества, для которого вот в функции для текущей политики максимально.
и затем играем вот эту оптимальную политику для самого оптимистичного ядра. то есть из-за того,
что в этом множестве живет просто p, то в функция в новой модели она будет всегда больше, чем оптимальная
в функции. оптимальная в функции для модели просто p или реальной модели. вот. то есть концепция
какой-то такой, что у нас есть некое множество моделей, в котором точно содержится p. и мы
пытаемся играть вот этой верхней квантили всегда. мы пытаемся выбирать действия, которые оптимистичны,
которые жадно согласны верхней квантили. на этой картинке это будет соответствовать тому, что сначала
мы будем дергать, допустим, за третий ручек. вот эта вот верхняя граница будет постепенно сужаться,
она будет сужаться, сужаться, сужаться. потом она перескочит, станет ниже, чем вторая граница.
будем будем использовать действия А2. будут уменьшаться, уменьшаться, уменьшаться. стоит меньше, чем третья.
они друг друга будут постепенно выбирать только эти два действия. а затем они станут меньше, чем вот это
UCB. чем UCB SA1. а после этого мы будем использовать это действие. оно тоже будет как-то уменьшаться. но в
конечном итоге, после того, как мы много надергаем за А2 и А3, использовать эти действия, то их UCB станут
меньше, чем UCB SA1. и мы будем точно все знать, что оптимальное действие было первым.
мы пытаемся так сходиться сверху, чтобы совершенно не пропустить правильное действие. в этом случае
мы стараемся не пропустить правильную политику. есть ли вопросы в текущем моменте?
я понимаю, вопросов нет. тогда давайте разберемся, как строить этот CDELTA. звучит все как-то
немножко странно. есть какие-то додавительные множества, как его строить. но в самом деле его можно
построить следующим образом. давайте рассмотрим некая плесклышка. это самая разумная оценка модели
по текущим данным. то есть у нас как-то наш агент ходил по нашему MDP. он в действии SA переходил
в S3 какое-то количество раз. это НТ S3 при условии SA. давайте просто, чтобы посчитать это под
крышкой, просто поделим часов переходов из SA в S3 на часов всех посещений SA. а если мы ничего не
знаем про эту текшен пару SA, то просто будем ходить случайно. просто наша под крышкой будет как-то
случайно нас разграстывать. на самом деле можно еще также показать, что вот эта оценка это будет
оценка максимального продоподобия при известных данных о переходах. при известных вот этих вот
SHT'. то есть мы проходили какой-то T игр и после этого мы можем написать такую оценку
максимального продоподобия на P с крышкой. если мы будем сходить согласно допустим оптимального уровня
BOMA на согласно P с крышкой, потому что когда сильно его, нам могут допустим не повести
в эту ситуацию. то есть у нас P с крышкой для какой-то политики 2 ВП для этой модели будет большим,
кем-то поврется, потому что ему повезло так. но вообще говоря, так не бывает. то есть если мы
будем шагать больше, то подсветов это среднее станет меньше. но мы никак не можем влиять условно
повезло-неповезло с темпами из модели. они уже есть. по всей данной мы больше ничего не можем сделать.
поэтому можно верить на самом деле модель так, что у нас эта P с крышкой это некая самая правильная
вещь, самая правильная модель. мы ничего больше не можем сказать. но давайте скажем, что реально
модель с крышкой недалеко находится. как бы P с крышкой должно сходиться к P, если мы стоплируем из SA,
если мы находимся в состоянии SA, находимся в состоянии S, дергаем ручку A, выбираем действие A,
то мы получим в состоянии S штрих. и вот эта вещь при бесконечном частотопочине должна сходить
к правильной модели. значит скорее всего, наше правильное дурительное множество должно нагреть
где-то вокруг нашего P с крышкой. и это по факту формализуется вот с помощью вот этой леммы.
то есть наше множество моделей, это как раз-таки множество таких переходных ядер P штрих, для всех
состояний P-P с крышкой не больше чем вот эта B дельта, где beta delta оно скалируется как 1
на корень Z. то есть постепенно это множество будет сужаться. и при этом мы точно знаем,
что внутри этого множества живет настоящая модель. и ровно для этого нам нужна концентрация
для 1 норма. мы хотим показать, что реальная модель, мотоожидание D P с крышкой будет удовлетворять
вот такому нерайд. в этом моменте я хочу вернуться к пропущенному моменту, как писать концентрацию
на эту послышку, почему вот эта B дельта не гнустит правильно. если к этому моменту какие-то вопросы.
окей, я понимаю, вопросов нет скорее всего. либо все понятно, либо ничего не понятно.
в обоих случаях я возможно помочь ничем не могу прямо сейчас.
давайте я еще раз расскажу этот принцип, как работает агритмический рельт.
мы на самом деле можем сказать так, что мы по этому множеству C дельта T, это не это выпукло множества
в нашем среднем случае. поэтому вот эту максимизацию проводить легко. ну как легко?
не фундаментально сложно. в предвестной модели можно считать B P и C дельта B. поэтому вот эту
максимизацию можно как-то делать. как ее делать? вопрос отдельный. этот алгоритм, он из 60-го года,
уже вышло очень много чего гораздо лучше, но он довольно простой для качества, поэтому я
выбрал его. то есть выполнять его на практике это, конечно, абсолютно бессмысленно-беспощадно занятие.
но мы можем посчитать вот вот некого максимум взять по этому множеству и затем на тему
оптимальную политику. можно наоборот. можно максимум переставить. то есть мы для панели P4 находим
оптимальную политику на уровне Балман. то есть мы находим ВС звездой H от действия S
при условии P штрих. при этой модели известной. а затем мы находим такую модель P штрих,
которая максимизирует ВС звездой. наверное даже более правильно думать об этом так,
что представить эти два максимума местами, а потом взять оптимальную политику для вот
этой самой оптимистичной модели, для которой максимизируется ВС звездой. это считается
примерно вот так политика. зачем нам нужен оптимизм? так называемый. это будет видно из
качества частично. по факту это принцип того, что мы не хотим пройти мимо ВС звездой при уменьшении
давительных интервалов на кустах звездой. то есть принцип говорит какой-то такой. и качество
множества цдельта мы выбираем вот такой или один шар вокруг оценки максимально правдоподобия.
и вот результат, что оно концентрируется скоростью 1 дизель на корень СН. там еще есть S плюс
алгоритм. вот это S плюс алгоритм, оно как раз таки получается из не раз это концентрация для 1 нормы,
которая я вот пропустил вот здесь. вот оно. если его формулировать, формально оно выглядит так.
пусть у нас есть n векторов, сенсированных, независимых, ограниченных в 1 норме. почему бы и нет?
тогда для каждого дельта, хотя бы 1 минус дельта, у нас верного такого не нравится.
раз на самом деле у нас разделена ошибка такая фуктуация, и это по сути говоря будет оценком от
ожидания. не будет ли тут под алгоритм еще фактора, зависящего от n? то есть там алгоритм n на дельта,
вот именно такое неравенство имеет место или все-таки под алгоритмом n должно быть? вот у меня
какое-то ощущение из того, что и то, что я видел, мне попадалось. там все-таки под алгоритм еще n
должно быть. может такое быть? конкретно в этом образцом неравенства кажется нет. давайте проверим
сейчас, но мне казалось, что здесь его нет, просто потому что алгоритм 1 на дельта это просто
макдярмид. в макдярмиде никакого бытия на n под дельта не резет, но конкретно в применении его для
алгоритма crl действительно там будут какие-нибудь n под этим алгоритмом, потому что мы будем делать
мучение union bound. в этом будет чуть позже. да-да-да, это я понимаю, я просто к тому, что вообще говоря,
иногда такие неравенства называют неравенство хейовдинга в гильбертовом пространстве или не в
гильбертовом банаховом пространстве, и ими занимались, например, немировские юди, у них есть такого
типа неравенства, и вот у них, если я правильно помню, за счет того, что это все-таки 1 норма,
и это как бы не гильбертово пространство, там появлялись какие-то такие логарифмы от размерности,
но это я сейчас, так сказать, 100% говорить не буду, но если вы знаете, как это доказать с помощью
макдярмида, то здорово. может быть, действительно и нет, но так с входа не очевидно. с входа не очевидно,
давайте посмотрим, давайте сейчас проведем немножко погальшества, время чуть-чуть есть,
что мы хотим проверить, что на самом деле наша 1 норма удовлетворяет этому свойству 1, этому
определению, в самом деле, на скопорте, отчасти на векторах х. если мы это проверим, то у нас
автомический очконтраст вокруг среднего. после этого остается отдельно значка, как оценим среднее,
но именно от дельты ничего больше зависеть не будет. возможно, я здесь мог упустить что-нибудь в этой
экспоненте, но давайте проверим. давайте рассмотрим вот эту f, как я говорил, как просто эту 1 норму.
надо сходить по не разу треугольника любого и другого х, то есть если мы поставим эти x не случайными,
у этой функции f, после того, что вот это вот f от x1 и далее xn, это вот такая сумма. что здесь?
можно здесь прибавить и отнять под нормой x'k. почему бы и нет? в таком случае эта сумма x1 xn,
то есть мы прибавили вот такую разность xk'-xk под нашей нормой.
если мы используем не разу треугольника, вытащили отдельно слагаемую разность. если мы вычтем xk,
то в точности получится сумма всех, кроме xk, еще прибавим xk'. то есть точка ниже. вот эту штуку
мы можем еще раз использовать не разу треугольника. используется свойством того, что все наши xk-ты
не больше чем b по 1 норме. и собственно говоря получается вот так. если у нас есть оценка не с 1
нормы, а с 2 нормы, то чтобы перевести это в неравенство по 1 норме, нужно запустить размерность. можно где-то так.
и после чего мы можем просто применить макдярмиду к этому всему. то есть у нас есть вполне дифференс попортия.
когда у нас надо 2b для всех x, тогда мы можем перевести эту экспоненту, то есть что это экспонент равно
дельте и в таком случае получить 2t квадрат делить на 2bn равно дельте. ну и казалось бы тут
никаких проблем нет. да согласен. и получается здесь такой фуктационный член. еще нужно
мотождание оценить. а мотождание можно оценить заменив 1 норму на 2 норму. заменив неравенство
эквивалентности норм. то есть как известно в конечном мерном пространстве все нормы эквивалентны и более того
можно выписать константы явно при известной размерности. на данном случае мы знаем, что 1 норма не больше чем
корень из d размерности на 2 норма. поэтому сначала подменяем 1 норму на 2 норму и
еще применяем ентенна сразу же. то есть мы загоняем все это под корень. но
теперь квадрат 2 нормы это штука довольно понятная, а просто сумма квадрата. у нас здесь получается
сумма квадрата этих координат нашего вот этого xk. все это в квадрате просто потому что они квадрат
суммирования. но у нас все xk они независимы. поэтому на самом деле этот квадрат можно внести как бы
под сумму. они все независимы, значит не коррелированы. значит просто вот это вот мотождание суммы ратов по
вот этим k это то же самое сумма дисперсии. или же сумма вторых моментов, потому что мотождание равно 0.
поэтому получаем что мотождание 1 нормы не больше чем скорень из дизмерности умножить на такую сумму
мотождания квадратов координат, которую можно свернуть обратно вот сюда. а дальше услуживается тем, что 2 норма не больше
чем 1 норма. то есть у нас есть оценка на 1 норму для x, но у нас есть оценка на 2 нормы для x. поэтому вот эта штука
она не больше чем b квадрат. b квадрат под корнем просто b, поэтому корень из dn. вот и все. то есть
доказательство вот такого простого неравенства из двух частей. сначала мы избавляемся от флуктуации,
заменяем вот эту статистическую штуку на мотождание. на тему оценки мотождания выложите кулинастный норм
и с некими свойствами дисперсии. в чем главный пафос? как я уже говорил, что здесь зависимость от размерности
она здесь есть под корнем. это плохо, но хорошо то, что она разделена от этой флуктуации, от алгоритма 1 дельта.
то есть у нас нет, допустим, произведения размерности на алгоритм 1 дельта. в некоторых случаях довольно важно,
если мы хотим работать с маленькой дельтой в большой размерности. на самом деле мы можем в независимости с ними работать.
пока у нас это алгоритма доминирует, мы можем уменьшать дельты сколько угодно много, без существенной потери в опроксимации.
теперь возвращаясь к алгоритму srl, мы хотим показать, что наше p, то есть мотождание это p с крышкой,
будет находиться вот в таком увеличительном множестве. в чем тут главная проблема?
проблема тут в том, что вот эта nt относительно не что-то случайное. то есть у нас какие-то случайные вещи,
верхняя оценка говорит случайная, а эта штука тоже случайная. в чем довольно не тривиально случайно,
потому что у вас проход по траекториям довольно не тривиальный. и казалось бы здесь как-то сложно
еще выйдет, но тут уже помогает union bound. давайте сначала рассмотрим множество srl.
здесь у нас говорится, что для всяких srl у вас верного такого не нравится.
давайте разделим эти srl по одному. если мы получим вот такую штуку с равностью 1-10,
то здесь даже это t не особо нужно именно здесь, но это неважно. у нас получится верность таких событий хотя бы такой,
то при помощи union bound мы можем в ничке ездить для всех srl. если у нас это событие
у нас с очень большой вероятностью, то мы можем заплатить это srl под алгоритмом по сути и получить
одновременно для всех seduction parts. теперь вопрос в том, как доказать такую штуку. тут уже хотя бы у нас нет
разного состояния, уже неплохо, но все равно есть неприятная nt. а для этого мы на самом деле
делаем некий трюк, который в литературе на группе 1 называется reward tape. в этом случае мы скажем так,
что у нас вот эта nt это некое случайное число. мы хотим для всех t по этим случайным числом,
а если мы докажем просто для всех возможных n, которые могут здесь получаться в этой правой части,
и соответственно они будут здесь. вместо этого t, которое на кладочное количество
сэмплов используется для оценки под вспышкой, а давайте заменим это на что-то более конкретное.
просто заведем такую оценку ровно по n sample, где n это просто фиксирует на констанции.
посмотрим вот такое событие. для всех n, где n это возможно количество сэмплов, их может быть
сильно больше, чем nt на последний момент времени. вот эта n маленькая, которая допустим максимально
ровно t аж, она может сильно превышать вот это вот n в верхнем ныксом t. но это на самом деле не проблема.
просто потому что мы не сильно много за это платим. все мы поделим вот такие модели, построенные по n
sample. то есть у нас есть для каждой selection пары, пусть у нас есть некий набор состояния x1,
x2 и т.д. xk, который мы получили, сапплировав вот этого p. тогда во время самой игры мы на самом деле
можем не сапплировать каждый раз заново. просто у нас есть вот эта вот лента со следующими
состояниями. мы просто берем следующий элемент с этой лентой. эти две модели абсолютно кевалетные,
потому что мы не смотрим на эту ленту выбора решения. на эту ленту можно смотреть только среда.
а я не тратил сагнировать совсем новый xk и взять следующий с этой лентой. поэтому у нас есть
некая лента с нашими состояниями xk, с 1 и т.д. xth. может быть довольно много потенциально. и мы по
вот этим первым xм строим оценку под скрышкой от n скобочек. это будет в точности такая же
правильная оценка. если nм равно nт, то эти две оценки точно совпадают. то есть под скрышкой t
и под скрышкой от nt это одно и то же. тогда мы докажем некое большее событие, что для всякого n
у нас есть концентрация. для всякого вот этого n. то есть для этого мы требуем, чтобы у нас
была концентрация неких случайных индексов. это вот это событие внутреннее. а если мы требуем,
чтобы для всех возможно. ну конечно вот эта e с крышкой будет больше чем e. поэтому мы получаем
все что надо. а для этой штуки мы уже можем сказать, что окей, а давайте снова сделаем
union bound, снова разобьем все для каждого прессированного n. и мы хотим сделать вот такую штуку. но на самом
деле это штука о точности концентрация индицентрируемых векторов в 1 норме. то есть
на отождание вот этого слогамма под 1 нормой, на диратус этих моделей равно 0. если мы рассмотрим
вот такие y, то есть это 1 на n на индикатор минус вот этот p h3. на отождание этой штуки равно 0
просто по построению как мы эти xk строили. тогда мы можем просто применить нашу концентрацию
для 1 нормы. и получается что надо. то есть мы получаем, что для каждого прессированного n вот это 1 норма
не больше чем b дельта с вероятностью хотя бы 1 делит на дельта s a t h. делаем union bound по n
по всем возможным от 1 до t h. получаем событие e с крышкой. дальше наше e оно вложено в e с крышкой.
поэтому мы имеем вот это вот e s a. значит делаем union bound по 7 s a и получаем то что надо. получаем ровно вот это событие.
вот этот трюк он как бы довольно важен и в бандитах и в обученном креплении. именно сведение в некоем
смысле к id случая. тогда id случая даже как-то и не пахнет. как-то все выбирается случайным образом,
но нам это не важно. если мы просто рассмотрим что-то больше, больше состояний, больше что-нибудь
такого духа, то никаких проблем не будет. просто потому что мы будем делать union bound по каждому из них.
есть ли вопрос к этому моменту?
вроде нет.
окей. тогда давайте уже сам результат объявим. то есть это выкручивающий рель, который еще раз напомнишь что он делает.
мы для каждой модели п штрих умеем считать высот звездой. мы будем считать максимум по моделям,
максимум по политикам vp. максимум по политику это оптимальная v функция, которая считается в
уровне Беомена, который напомню выглядит вот так. вот эти уравнения очень легко считать если мы
знаем p, просто считаем с конца. сначала считаем для h большого, потом у нас есть v функция следующего шага,
но просто считаем действие ядра. просто умножение матриц делается замечательно быстро,
возможно с другими операциями. дальше мы считаем высот звездой и снова шагаем дальше, уменьшаем h.
мы посчитали оптимальную v функцию, оптимальную политику для этой модели.
мы посчитали высот звездой, а затем мы находим такую политику p, такую модель p с крышкой из этого
вдавительного множества, которая максимирует эти оптимальные v функции. это делается возможно
тривиально, но хорошо что тут у нас все выпокоено, поэтому это делаться в принципе может, но довольно
сложно, но можно. а затем мы берем оптимальную политику для этого оптимического шага. затем играем один
эпизод этой политикой, у нас пересчитываем нашу множество власти дельта и повторяем все действия.
это очень время затратно, но мы можем доказать для этого алгоритма такого сенсона регрета,
уже с неким правильным шкалированием по количеству эпизодов. то есть регрета от алгоритма это то,
что до алгоритма корень из h в кубе, x² at. скажу, что x² тут не оптимален, можно улучшать, и это уже
давным-давно сделали, но для этого алгоритма можно построить такую ассоциацию. еще есть слагаемо второго
порядка, это тоже не оптимально, можно делать значимость по s линейной, но это на самом деле особо не важно.
то есть важно первое слагаемое, что для регрета корня ст. в сравнении с х2н гриде довольно заметное
улучшение с 2х3 до корня ст. вот здесь мы тоже будем пользоваться некой концентрацией, но на самом
деле самое главное мы уже доказали. самое главное это вот эта лемма. она обеспечивает довольно большую
часть успеха, это доказательство. то есть оптимизм очень важен для доказательства.
да, то есть мы сначала будем скажем, что пусть у нас событие из этой леммы выполнено,
что у нас все дополнительные множества корректны, в смысле что реальная модель живет в дополнительном множестве
для всех t. затем мы можем, вот то что я говорю о конструкции, построить v с чертой th, это будет
максимум по штриху, максимум по политикам vp vph. то есть это точности то, почему мы здесь берем
арк максимум. то есть если убрать здесь арк, то это будет точности v с чертой. вот если приставить
2 максимума местами, вообще ровно такую штуку. максимум по политикам vp vph, а это максимум по моделям.
но эту штуку можно как-то посчитать. манификция таблик точно существует, потому что c компакт,
эта штука сочная линия на самом деле. вообще все прекрасно. и пусть с чертой аналогично считаем
просто вот такую максимум паку функции. и также насчет того, что вот это может создать так компакт,
можно найти модель, на котором она достигается, назовем ее v с чертой. это нам понадобится. то есть v с чертой
на самом деле считают через уровень Белмана для этой модели v с чертой, через оптимальный уровень Белмана.
и наша политика, это будет жадная политика на сильных хуй с чертой, потому что следует за уровнем Белмана.
можно брать такую политику, можно брать другую. но зачем? когда мы можем брать жадно. то есть здесь вот я об этом говорю,
что предположено, что вот наша pt, она жадная в значении ко функции. это будет действительно то, что нам нужно,
это будет одной из возможностей выбора оптимальной политики для компакта звездой, нашей p с чертой.
возьмем вот такие дельты. дельта у нас по сути разница между ошибкой и опроксимацией. то есть мы хотим,
чтобы наша верхняя оценка опроксимировала vp для нашего этого шага. насколько они далеки друг от друга,
это точность сущности дельта th, именно на шаге, на котором мы играем. затем, на самом деле нам
интересно для h равно 1, но для этого количества хорошо считать для всех маленьких h. разница,
которую определили регрета, потому что регрета это сумма каких-то невязок между высот звездой и vpt.
мы можем просто прибавить и отнять v с чертой. мы по общению первым оставляем и вторым оставляем.
но просто по построению этого v с чертой, здесь что мы говорим? мы говорим, что v с чертой это
максимум. мы знаем, что у нас есть p, наша реальная модель. поэтому максимум всегда больше,
чем высот звездой для реальной модели. в нашем случае мы просто обращаем его в высот звездой. без
выбора модели, если модели нет, это считается в реальной модели, к которой мы играем. поэтому
это сагамма не больше нуля. у нас есть только estimation error, к которому мы начали этим дельтам.
главный пафос всего происходящего в том, что если мы выберем такие уровни bellman, то действия,
которые выбираются там, они не согласовываются с этой политикой. то есть они как-то идут в разнобой,
потому что политика не знает ничего о высот звездой во время вычинения, только начинает узнавать. а v с чертой и v
pt, они в некотором смысле эволюционируются согласно негде же правилам. то есть pt это жадность
сильно q с чертой. ну и даже можно считать, что pt это жадность сильно просто q pt. вернее не жадная,
просто наша v pt это q функция, посчитанная в этом состоянии выбрана дальше. надеюсь, я этим и
пользуюсь, что v с чертой это q с чертой для этого действия выбранного. потому что эта политика
жадная, та политика выбиралась жадной по q с чертой. ну и при этом просто по уровням bellman для
нашей политики p, наша v pt это q pt для этих двух действий, которые мы выбираем. когда мы пишем
уровень bellman, награды сокращаются. дело несколько прибавить и отнять. один из главных трюков во всей
математике может поделить прибавить и отнять формула теора неравенствовательно. все что нужно для
того, чтобы делать математику. вот в этом случае можно прибавить и отнять из них несколько солгаемых,
и вводит такое разложение. первое солгаемое это ошибка модели. насколько наша p с чертой
предсказывает реальные p. второе солгаемое, небольшой спойлер, это будет некая маркетиговоразность,
потому что st h++1 оно простемплировано из модели с выбранными такими параметрами. это p это
мат ожидания. это буквально мат ожидания первого солгаемого с х чертой по всем возможным st h++1.
второе солгаемое имеет такую же структуру. еще у нас есть сама дельта. мы дельту убросили через дельту только
с большим шагом h. мы таким образом немножко перевели ошибку дальше. следующий h, а для h большого у нас
все ноль. поэтому все прекрасно. вот первое, что мы можем сделать, это посчитать ошибку модели.
ошибку модели можно посчитать просто по нерасту гельдера. просто мы разлагаем ошибку на это
между v с чертой и моделью state action пары. значит наша 10 ядра не больше
произведения. не больше чем произведения норм. на случай 1 норма и бесконечная норма.
а наша v с чертой не больше чем h, потому что у нас награда не больше единиц. дальше здесь
мы тоже можем приводить ст h не раз в треугольник. у нас получается p с чертой минус p с крышкой и
под крышкой минус p. но ровно так по дизайну нашего длительного множества эти оба
оставляем не больше чем beta delta. мы получили удвоенную ошибку, удвоенный радиус нашего
длительного множества. мы получаем, что нашу дельту мы разлагаем вот таким образом. у нас
есть аппроксимация модели, ошибка. у нас есть два марксингальна салагаемого и следующая дельта.
если мы свишем весь эгрет, это сумма v с чертой и v пт. это небольшая сумма дельты. дальше мы
работаем только с дельтами и начинаем их раскручивать по шагам h. если мы продолжим
раскручивать сумму этих дельт, которые есть последняя слагаемая на нижней строчке дальше,
то мы получим ровно такую структуру. наш эгрет это не больше чем сумма ошибок аппроксимации
модели во все моменты времени t и на всех шагах h для вот этого счетчика nt. и сумма этих марксигал разности.
и в принципе количество активистических алгоритмов, то есть недоятельная эссерия,
устроена примерно так, что у нас в конечном итоге получается некая ошибка аппроксимации
модели или сумма бонусов, что будет ведущим слагаемым. и какие-то марксигалы, которые совсем
мелочь, которые как-то суммируются, грубо азума хюдинга или бенштейна. но в принципе
все эти два слагаемых маленьких, главное слагаемая первая. мы можем оценить,
расспомнив что наша бодельта, я напомню, отсюда это 1 делить на корнез n не чисто, то есть как корнез s
плюс алгоритм. корнез s плюс алгоритм делить на число посещений, делить на n.
у нас получается вот то, что ровно есть сверху. если мы просто расспишем определение бодельта,
но еще здесь немножко забило на том, что здесь была сумма. в этом случае сумма не больше
чем произведение, потому что все довольно маленькое. тельта довольно маленькая, поэтому алгоритм большой,
больше единица. это грубо, но для наших целей сойдет. и остается вот такая вот сумма,
они делят на корень nt. на самом деле мы можем перегруппировать правильно.
если мы выделим одну straight action пару, и будем считать сколько раз мы вылетят в эту сумму,
на самом деле это число будет вот так постепенно расти. при первом посещении будет просто единство,
при втором посещении это будет 1 делить на корнез одного и так далее.
у нас будет nt, а в итоге все равно 3.
И всего количество раз слова присутствует, а на этой текшн паре в сумме это n для t большого от n.
Затем мы можем сделать самый простой трюк, заменить сумму на интеграл, оценивать это сверху.
Дальше, прооптимизировав по nt, мы можем получить, что эта сумма 1 на корень из этих n.
Эта сумма анализируется как корень из nt, nt большого.
Эта сумма во второй формуле.
Это будет 1 плюс корень из nt большого.
Но вот эта сумма по всем текшн парам, 1 плюс вот эта штука,
1 просто выносится как SA, а второе на самом деле максимизируется, когда все nt большого равны между собой.
Получается ровно вот эта вот 2 умножена корень из SATH.
Затем, если мы еще вспомним, что если мы замутили на S и H,
у нас получится точности вот эта вот ведущая слагаемая из регрета.
А оставшая слагаемая это какая-то мелочёпка, просто нужно определить правильную фильтрацию, чтобы сказать, что это маркетовая разница.
В этом случае правильная фильтрация, да будет по сути фильтрация по времени.
То есть у нас есть какие-то эпизоды t, значит со нашей фильтрацией мы будем обуславливаться
на все наши посещения, для всех эпизодов более ранних, для всех шагов, которые у нас были.
И для последнего эпизода, для всех этих h, для всех шагов внутреннего эпизода, до того, на котором мы сейчас находимся.
То есть по сути говоря, у нас есть как-то время, мы обуславливаемся на всё, что было до этого момента.
Если мы так сделаем, мы получаем маркетовую разницу.
Которая по нерадостному зуму хердинга.
Получается какой-то такой вид.
Я не стал выбирать точные константы, потому что они нам не интересны.
Но идейная слагама будет как-то так.
Важно здесь корень из t не везутся до размерности, то бишь f.
И вот эта штука с разностью 1-2 дельта зрищется вот так.
После чего мы можем всё просуммировать и получить нужный отец.
Давайте я еще раз по этому всему пройдусь.
По структуре доказательства.
И структуре алгоритма.
То есть как выглядит наш алгоритм?
На самом деле понятнее кажется вот здесь.
Получается, что мы определяем некие такие b чертой, q чертой, как вот такие максимумы.
То есть у нас некий кусок звезды для любописного модели считается несложно.
Мы промаксимизируем внутри наших длительных множеств.
А затем играем шаду с сильной атакой чертой.
Вот наша политика.
Как является c дельта?
c дельта это некий шар в один норме, можно это назвать так.
Вокруг оценки максимально правдоподобие.
То есть самая разумная оценка, которую можно сделать.
Вот где-то рядом с ней должна находиться реальная модель.
И при помощи концентрации меры мы можем выписать точно это b дельта.
Получаем на самом деле точную скорость сходимости, а как оно сходит.
Как этот длительный провал сужается со временем.
Но вот там всегда будет c и p.
Поэтому вот эта v чертой, которая у нас здесь определяется,
когда мы строили политику, она будет всегда больше, чем w и c звездой для реальной модели.
Это важно, это позволяет нам в некотором смысле закаплить то есть оценку на регресс.
То есть делать так, чтобы мы могли распручивать наш регресс по уровню Белмана.
Без этого доказывается сложно.
Очень сложно, но в принципе можно.
Есть работа над чему, как делать это без оптимизма на качестве на регресс.
То есть оптимизм это то, что q с чертой больше, чем q с звездой.
И v с чертой больше, чем v с звездой.
Тогда как нам доказать центральный регресс на этой штуке.
Сначала пользуемся оптимизмом.
Меняем v с чертой на v с звездой.
И v с чертой и v с звездой эволюционируют по одному принту.
Выбираются одно и то же действия, такие же есть q функции.
И все в прочем дует.
Значит можно стать уровнем Белмана.
Потому что v с чертой это v функция для некоторой модели, для некоторой политики.
Точно ровно политика PT там используется.
То есть v с чертой это v функция для политики PT, но в другой модели.
Дальше мы пишем уровень Белмана, прибавить и отнять.
Разлагаем на 4 слагаемых.
Первая слагаемая это ошибка модели.
То есть насколько далеко p с чертой от p.
Вторая слагаемая.
Это нечто маркетингальное, потому что мы хотим дальше подключить дельты.
Не от ожидания неких дельт, а сами дельты.
Поэтому мы как-то подальше подключим дальше, прибавить и отнять.
Это v с чертой в следующем состоянии.
И v пт в следующем состоянии.
Дальше эти два слагаемых, они кажут довольно мелочью, просто потому что у нас будет сумма их.
Кажут, что гаммаодельность может быть большим.
Допустим, какое-то не типичное состояние выпало, и все, и все плохо.
Но если мы просуммируем, а в игрете мы ровно получаем сумму,
то эта сумма будет влиять мало на итоговую оценку.
Логичность просто с кси.
И дельта просто начинаем отключивать дальше по тому же самому принципу.
Ошибка модели считается просто 11 Гельдера,
потому что мы знаем, как далеко p с чертой от p живет в 1 норме.
Они максимум как бы...
У нас это по сути некий шарик,
есть вокруг p с крышкой.
p и p с чертой могут лежать на разных катах этого шарика,
поэтому расстояние между ними не больше, чем 2 на радиошарик, то есть диаметр.
Еще h это...
Сколько большая может быть эта v функция с чертой?
Для нашего ошибка дельта t,
ошибка оценки нашей с чертой реальной модели,
она не больше, чем эта ошибка аппроксимации модели
плюс 2 маркенгальных члена и плюс следующая дельта.
Дальше можно продолжать писать неравенство для всех h
и получить, что сейчас вот такая сумма.
Первая сумма – это сумма ошибок в модели.
Вторая сумма – это что-то маленькое,
потому что это не что с нулевым от ожидания, что мы просуммировали,
но маленькое, если мы поделим это, допустим, на t.
Уж на t h, но h у нас по порядку меньше, чем t, как правило.
Эта штука будет маленьким, то есть последние 2 совгамы будут довольно маленькими
в сумме во второй строчке.
И главная совгама – это сумма ошибок в модели.
Ну, если правильно перегруппировать совгамы,
то становится понятно, как эта штука себя ведет.
Если просто следить за отдельной стрит-экшен парой,
мы про нее будем думать про 1 s a,
сколько раз она здесь возникает и с какими весами,
то мы получим ровно вот этот англо справа.
Здесь, на самом деле, скорее равенство, а не неравенство.
Но неравенство точно верно.
Вот.
Может быть, эта еничка может...
Скажем так, если мы не поделим стрит-экшен пару,
у нас не будет здесь енички вообще.
Но будем считать вам еничку,
потому что в лучшем случае мы посадим все стрит-экшен пары.
Все s a.
Тогда заменяем...
суммы как просто корм из NT.
Затем сумма всем стрит-экшен парам
максимизируется, когда все NT равны между собой.
То есть при условии того, что
сумма всех NT равно TH,
то эта сумма корней максимизируется,
когда они все равны между собой.
Потом, в общем, оценку вот такую s a TH.
Еще двойка, потому что
интеграль не нравится неточная.
Вот здесь тоже что-то может вылезти.
Чтобы вылезти в такую интеграль.
То есть у нас получается ведущая слагаемая в регрете,
если мы учтем эти a TH,
по которой у нас много материалов.
Вторая слагаемая это какая-то мелочевка
некая сумма морского разности,
которая лично консенсируется около 0 согласно зуме Хердин.
И таким образом мы получаем вот эта первая слагаемая.
Это будет как раз таки слагаемая
вот эта ведущая,
которая внизу справа.
Это вот как ведет в себя эта сумма
один делит на счетчике в общем случае.
Вторая слагаемая это просто
вот эта вот s a,
которая здесь тоже снизу берется.
И третья слагаемая,
это маркегальная слагаемая.
У нас первая слагаемая доминирует третья,
поэтому у нас получается такая сумма.
Алгоритмы сейчас спрятаны под O с tilde,
поэтому тут все спрятано под O с tilde,
но степень алгоритмов,
если здесь можно отследить,
это корень алгоритма максимум.
Поэтому это не особо проблема.
И мы получаем такую основу на рекрете.
Есть ли какие-нибудь вопросы?
Коллеги, пожалуйста, есть ли вопросы?
Видимо нет.
Но у нас сейчас как раз по времени
должна быть пауза где-то,
ну может еще пять минут.
И потом еще часовой доклад.
У вас как сейчас идет это?
Где-то точку паузу можно поставить?
Или пока еще рано?
Я думаю, что в течение минут 50 можно закончить.
Да-да, отлично.
Давайте так и сделаем.
Давайте немножко просуммируем,
что здесь было,
что в принципе мы получили по итогу.
Мы получили, что при помощи концентрации меры
можно получать оценки на регрет
для алгоритмов обучения сопротивления.
То есть в принципе все,
что из математики мы использовали,
это качество, это прибавительность,
можно поделить и не раз концентрация.
То есть ничего здесь больше в принципе не нужно.
Это как бы главная математическая идея,
которая здесь используется
для деконстрации меры.
И она здесь используется просто повсеместно.
Все время вылезают какие-нибудь маркетингауы,
которые нужно оценивать.
Все время вылезает что-то такого духа,
может вылезать что-то более сложное,
но довольно редко.
И в принципе на самом деле возникает вопрос,
а оптимальна ли оценка,
которую мы получили на регрет?
На самом деле нет, на самом деле можно сделать лучше.
А лучше делается даже более вычисленно
дружелюбным способом.
Давайте у нас есть ровняний Бэмман,
оптимальный, слева.
Мы у них не знаем модель.
И единственное, что мы можем сделать,
это, называемый статистический плагин estimate,
мы можем поставить под крышкой.
Но при этом мы знаем,
что под крышкой не работает.
То есть под крышкой оно будет сходить куда-то не туда,
просто потому что тп в генерации данных
зависит от самой этой оценки.
Если оценка плохая, данных делаются плохие,
а так остается плохой.
Горбачин, горбачаут.
Во всей красе.
Но вместо этого мы можем поощрять
агента исследовать.
Дополнительным образом.
То есть можно делать эксенгриде,
а эксенгриде плохо. Давайте по-другому.
Будем добавлять некий exploration bonus,
нашу оценку на ку-функцию.
То есть если мы посетили
as a selection bar мало раз,
то это exploration bonus большой.
То есть мы будем в любом случае
выбирать это действие,
даже если на него оценка довольно плохая
по первым думам с алгамма,
то есть по ревордам плюс
с крышкой на В чертой.
Если по этим штукам оценка у нас плохая,
но exploration bonus хороший,
то сразу туда пойдем.
Все равно выберем это действие
в этом состоянии.
Наша политика это ARC максимум
по Q чертой.
Если же у нас
бонус маленький,
но большая вот эта алгамма,
которая идет из уровня Баумана,
то тоже хорошо.
Тут гарантированно хорошие награды есть.
И что из этого получается,
как выбираются эти бонусы,
чтобы у нас был этот принцип оптимизма,
чтобы куча всего было больше
чем куча звездой,
чтобы у нас сработал этот первый шаг
до качества,
самый первый,
когда мы выкинули вот эту ошибку
после звездой минус Q чертой.
И мы получили разность
двух value функций,
которые некому другому соответствуют.
Которые расключиваются
одинаковым образом.
И алгоритм получше называется
UCBVI,
для upper content bounce reiteration.
В 2017 году он получает
вот такую оптимальную оценку на регресс.
Здесь еще в таком
более общем стетинге,
зависит еще от шага h,
но это абсолютно не важно.
То есть эти бонусы,
как они выбираются,
либо как h делить на корень
числа посещений,
либо как использовать
оценку на дисперсию.
То есть в этом случае мы
получим качество использования разницы хевдинга,
чтобы получить оптимизм.
В этом случае мы используем разницу берштейна.
Но потом еще есть большая говно
больше с этим дисперсиями,
потому что это не просто дисперсия
с этой звездой в реальной модели,
а у вас и модель неправильная, и оценка
неправильная, и вообще все неправильное, оно пипит.
Поэтому это все можно скорректировать.
Он делится на число посещений,
и можно получить, что этот алгоритм
оптимальен
на число посещений или грэпов.
Точно алгоритмах
в этой науке никто
не заботится,
потому что объекты гораздо сложнее,
чем в условных бандитах.
То есть эти велифункции не раскручиваются,
и все это делать довольно сложно.
Как безумим баунда здесь обходится,
как в бандитах не особо понятно.
И что еще может быть дальше
с этим сделано?
То есть можно использовать на самом деле
алгоритм, который не связан с оптимизмом.
Когда мы вместо бонуса
добавляем шум в гаусс,
мы получаем
зашумленную оттенку
уровня Белмана,
и магическим образом это работает.
То есть можно делать
пейст рель,
posterior sampling for revolving.
Делаем вместо пейст-крышкой
модель,
просто берем некую базовскую модель
на все наши модели,
возможно, и сэмплируем
как-то из апостелюрного
распределения
и решаем такие уровни Белмана.
Этот алгоритм
отлично работает
на практике,
но, к сожалению, для него пока неизвестно
никаких технических результатов.
Конкретно я и Алексей
занимались таким
сеттингом совмещения
техник оптимизма
и эфирандомизации
по очереди два алгоритма.
То есть один алгоритм – это bias UCBVI.
Он тоже следствует
такой идеи,
но у него нет бонусов.
Вместо бонусов мы тоже пользуемся базовской моделью
и берем верхнюю клонтиль
по возможным моделям.
Есть также оптимистический posterior sampling.
В этом случае мы вместо клонтиль
берем максимум
по просмотрованных моделей
из апостелюрного распределения.
Мимо прочего можно
развивать эту
оптимистическую идею дальше
и говорить, что в любом случае это слишком плохо,
потому что мы не покрываем
каких-нибудь атарий, ничего разумного мы не покрываем.
Поэтому
хочется сказать, что
допустим у нас кофункция
скажем так, у нас
как-то апрактируется какой-то
нейросеть или там
какие-нибудь веревые методами
оптимальная кофункция.
Что можно туда про это говорить и сказать?
Здесь тоже есть довольно развитая история,
где помимо кастрации меры
еще возникает
теория проксимации.
И помимо прочего, можно пытаться
придумывать, как историческое соображение
общается на реальные алгоритмы,
на реальные игры в атаре
и все прочее.
Это уже более инженерная история,
но тем не менее, я считаю, что довольно
интересно и довольно
поучительное правило.
На этом
у меня все.
Спасибо.
Да, пожалуйста, вопросы.
Спасибо за доклад, Данил.
Есть ли вопросы, коллеги?
Ну вот, хорошо мы познакомились
как бы в библиотеке
с нерастными концентрациями,
как это все работает.
Будем в курсе.
Но каких-то большого количества
примеров мы не разбирали.
Нераста Макдермита
вообще очень полезная машина
в обучении и здорово, что оно
всплыло в конце
курса.
Ребят, есть ли какие-то вопросы
Данилу?
Так, ну
не знаю, видимо
вопросов нет.
Вот.
Нет вопросов. Давайте благодарим Данила
за сделанный доклад,
лекцию прочитанную. Он был
интересный. Как раз правильные
детали были проговорены.
Ну и тогда, наверное,
все. На этом курс заканчивается.
Ну и дальше следите
за новостями. Данил, большое
спасибо. Спасибо, что откликнулись.
Спасибо.
Да, хорошо.
Вроде тогда все. Да, коллеги,
спасибо. До свидания.
До свидания.

Окей, ребят, ну что, давайте потихоньку начинать.
Так, окей, ребят, пока мы не перешли, собственно, к семинару, у меня есть еще
парочка очень важных, на самом деле, замечаний, на которые тоже крайне необходимо обратить
внимание. Смотрите, я на самом деле про них хотел сказать, но, к сожалению, во время лекции они у
меня чуть-чуть сначала вылетели из головы. Ссылочка на практику уже в чатике, но я вам сейчас скажу
другую вещь. Давайте вот на эту доску камеру передвинем, я сейчас хочу порисовать. Можно мне
порисовать. Классно. Смотрите, в чём очень важно не ошибиться, когда мы с вами говорим про
регуляризацию. Первое, помните, что мы с вами изначально сказали, что мы добавляем в нашу матрицу,
собственно, вот этот самый фиктивный столбец из единиц, матрица-объект-признак. Так вот,
если вы так вдруг на практике сделаете, ни в коем случае не учитывайте ω0 при подсчёте
коэффициент регуляризации. Понятно? То есть у вас, когда мы говорим, что мы хотим норму ω, причем
неважно какую, отправить на минимум у нас b свободный член, он не принадлежит ω. Понятно,
почему, да? Потому что непонятно, потому что иначе вы требуете, чтобы у вас свободный член был
как можно ближе к нулю. Что с ней такое, я не могу понять. Эй, работай. Она в режиме, я устала,
я понимаю. Слушайте, фиг с ним, мы больше разбираться будем, потом разберём. Смотрите,
ещё раз, если вы пытаетесь свободный член загнать к нулю, вот смотрите, вот у вас, допустим, будет
вот такой набор точек. Какую прямую надо через него провести? Логично, что вот такую. Правильно? Вот
у вас вот такой набор точек. Вот такую прямую надо провести. А если у вас регуляризация учитывает
свободный член, то вот здесь у вас клад регуляризации будет сильно больше, чем вот здесь. Это как вообще,
почему? Почему вы требуете, чтобы ваша линейная модель обязательно через ноль проходила? Это из
каких предположений идёт? Не из каких. Если в это следует из каких-то предположений, что ваша
гиперплоскость должна через ноль проходить, тогда замечательно. Если же нет, но вы забыли,
что у вас там просто в матрицу объект-признак добавили инфективный столбец, вы просто сделали
ровно то самое неправильное предположение, которое приведёт к неправильному решению. Половили?
Так мы всего лишь говорим, что у нас, собственно, норма вектора весов, где у нас B в него не входит,
стремится к минимуму, что краски приводит к тому, что у него либо, пускай вырубается,
либо, грубо говоря, норма минимальная, значит, у нас там решение становится не, решение становится
единственным, либо у нас прореживается значение этих самых весов, некоторые признаки выкидываются,
всё то, что мы с вами обсуждали. Про свободный член мы ничего с вами не говорили.
Кх плюс B, это если у вас в одномерном случае, я просто не умею многомерные картинки рисовать,
в общем случае у вас ωх, где ω это уже вектор. А зачем?
Ну мы предполагаем, что норма вектора весов как можно меньше, всё. Что такое наклон в многомерном
случае? У нас каждый отдельный член должен быть, можно быть, ближе к нулю, если он не влияет на
значение ошибки. Вот. Я, честно говоря, вопрос не понимаю. Просто что такое регуляризация, в общем
случае мы с вами вроде уже разбирались только что. Потому что наклон у вас зависит, грубо говоря,
коэффициент перед каждым признаком зависит от того, какой вклад этот признак делает в решение.
Перед свободным членом зависит только от того, насколько у вас ваша целевая переменная сдвинута
относительно нуля. Там признаки никак ни на что не влияют. Так, коллеги, это у всех вызывает вопрос,
или только здесь. Просто если только здесь, я потом это в конце семинара обсужу, чтобы всех не
затягивать. Так, кому непонятно, почему не нужно ограничивать свободный член? Почему не нужно
ограничивать свободный член? Кому непонятно? Ну как бы в общем случае, смотрите, у вас задача регрессии,
грубо говоря. Вы ищете оптимальную гиперплоску с сетей линейной регрессии. У вас решение,
как правило, от того, что вы сдвинете ваши все точки куда угодно, то есть добавите любую константу
к ним, у вас суть решения не меняется. Свободный член ровно за эту константу и отвечает. Если вы
вдруг его ограничиваете, то вы почему-то говорите, что вы на самом деле не можете никуда сдвинуть,
и у вас от сдвига меняется решение. Это что-то странное. Да, именно, это для вектора именно весов
без свободного члена. То есть это именно для случая ωх плюс b. Если мы с вами для удобства запихнули
в матрицу объект-признак свободный столбец, столбец из единиц, то вы здесь тогда должны
норму с отбрасыванием первого члена считать. Да, он вообще, его вообще там нет, это по сути мы
для удобства записи только написали и все, больше ни для чего. Все, хорошо? Классно, супер. И второй
момент, я думаю, вы на самом деле на это дело уже можете обратить внимание. Коллеги, смотрите,
если у нас с вами норма вектора весов вторая используется, и у нас, допустим, вектор весов это
1, 1, 1, 10, то из-за какого элемента вектора весов у нас большой коэффициент, большое значение
регуляризатора будет? Из-за последнего, правильно? А теперь внимание, вопрос, а если у нас шкалы
признаков просто-напросто разные? Первый признак условно порядка единицы, второй порядка единицы,
третий порядка единицы, четвертый тоже, а пятый порядка 10 минус второй, а у тоже порядка единицы,
и все они одинаково значимы, то какие коэффициенты будут? Все будут порядка одного, кроме того,
у кого шкала большая, тогда признак будет маленький, шкала маленькая, признак будет
иметь большой вес, согласны? Поэтому регуляризацию вы можете вот такую применять только в условии
нормированных данных, потому что иначе вы просто-напросто будете штрафовать признаки в малых шкалах,
вы будете пытаться от них избавиться, потому что чем меньше шкала, но больше значимость признака,
тем больше у него будет значений коэффициента. Х штрих это х минус мю от х делить на СТД от х,
вот вам стандартизация. Минус среднее поделить на дисперсию, для каждого признака по отдельности,
ну или мин-макс скейлинг тоже можно, но как правило именно стандартизацию делают 9 из 10 случаев,
то есть это именно я вас предостерегаю, потому что, во-первых, на это сейчас не обращать внимания,
во-вторых, если вы это сделаете, это именно то, о чем я говорил, это неправильное предположение,
которое ломает вам задачу. Если вы не отформировали данные и наложили регуляризацию, вы по сути всем
признакам, у которых шкала маленькая, сказали, ты сейчас вообще отсюда улетишь, на тебя неважно.
То, что признак маленький, вес тоже маленький, вклад маленький, мы его по сути выкинули. Признак
неважный, но в большой шкале, вес маленький, он сильно влияет. Получается фигня. Стандартного
отклонения, standard deviation, корень из дисперсии от x. Так это же есть нормировка. Вот, x со штрихом
это отнормированный x, x минус среднее делить на дисперсию. Это и есть стандартизация, то есть
перед регуляризацией вам надо вот это сделать. Если вы это не сделаете, вы получите какую-то
фигню. По причине, обычной выше. Это всем понятно? Хорошо. И возвращаясь назад, это очень похоже на то,
что у нас происходило, когда мы говорили про КНН на прошлом занятии. Помните? Если вы признаки
не отнормируете, то получается у вас по одной оси, грубо говоря, расстояние в километрах,
а по другой сумма в копейках. Получается, различие в одну копейку у вас эквивалентно расстоянию в
один километр. Если, грубо говоря, это не соответствует вашим априорным предположениям,
то фигня какая-то получается. Поэтому, если у вас нет каких-то предположений о ваших данных,
которые вам позволяют не нормировать данные, нормируйте данные. Знаете, вот правило,
если вы не понимаете, почему не надо это делать, вы должны это делать. Всё, он нам не нужен.
Нет. Ещё раз, смотрите, у вас нормировка, она меняет шкалу признаков. Б отвечает за сдвиг
относительно нуля. Грубо говоря, у вас линейное преобразование, в общем случае, это поворот,
растяжение, сжатие. Вы за сдвиг, за который бы отвечаете, никак не можете это сделать.
И что? А у вас при этом через ноль проходит? По-моему, в общем случае это нельзя гарантировать.
Ну, потому что, по идее, да. Для нулевого х тогда у должен быть равен нулю. Разве это верно?
По идее, вы тогда ещё у должны отформировать, а вот у лучше вообще не трогать. Целевой
переменную лучше не трогать, если вы опять же, если вы не понимаете, зачем вам это надо, вам это не
надо. На самом деле, это вообще очень широкое правило, которое работает очень много где. То,
что здесь очень много вещей, которые я вам рассказываю сейчас и буду рассказывать дальше,
и на самом деле не только в машинном обучении, там на самом деле можно сказать, вот это работа так,
так, так-то, но это также на самом деле было на физике. Вот вы механику изучали, а потом,
но кто-то не изучал, но тем не менее. Те, кто изучали механику, вы всё равно в школе изучали
физику. Даже в школе не было физики. Ну, вы в школе изучали физику, но на уроках было приятно
поспать. Тоже вариант. Ну и к чему? Есть, собственно, классическая механика, а есть теория относительности,
или там, не знаю, квантовая механика. Это не отменяет просто классической физикой, классической
механики, например, термеха. Просто это говорит, что граница применимости есть. В школе-то вам
про это не говорили. Тут, на самом деле, то же самое. Тут некоторые вещи, они работают в каких-то
предположениях, а когда мы выходим за эти предположения, гораздо более сложные вещи приходится делать.
Мы сейчас пока с вами сидим там, где достаточно просто, просто того, что мы только начали. То есть,
грубо говоря, простое правило. Если вы понимаете, что правило, которое я вам озвучил, больше не
работает, и можете это обосновать, значит, оно больше не работает. Я вот говорю про все эти там вещи.
Например, про нормировку. Я могу вам привести пример, когда надо регулировать сегодня свободный
член, накладывать и так далее. Только это тогда надо вообще залезть в методы байс, методы оптимизации,
в боесовщину, и там повариться где-то еще, не знаю, занятия 5-10. И вот тогда мы найдем частный
случай, когда нам необходимо на него регулироваться и накладывать. Сейчас там не надо этого. Хорошо? То
есть, просто то, что я говорю, всегда используйте нормировку, если не знаете, что делать, или если
вы не видите никаких противоречий. Если видите, это нормально, вы развиваетесь, вы изучаете новые
вещи. То есть, это не истинно в последней инстанции. Мы не можем с вами все машинное обучение за там
две лекции или даже за 10-20 покрыть. Эту область в ней можно там десятилетиями развиваться. Тут все
уловили? Вопросов нет? Тогда давайте на практику посмотрим. Итак, ссылочку в чат я скинул. А что,
оно все вырубилось? Там, соответственно, вы можете найти ноутбук. Я, собственно, скинул. Я
предлагаю открыть именно решенную версию ноутбука, чтобы самим сейчас не кодить.
Ну, слушайте, отсюда всем видно, нет? Просто здесь, кажется, даже гораздо лучше видно.
Не, я пока не открыл ничего. Сейчас я QuickTime включу. QuickTime. Ну, слушайте,
но доска классная. Она, по ходу, в 4К что ли работает. Я просто на экране практически ничего не вижу.
Здесь у меня тут размер шрифтов невыносимый. Интернет, приди, порядок наведи.
Сейчас.
О, успех.
Что, что-нибудь видно или маловато? А он почему-то даже больше не увеличивает. Вот зараза. Ладно,
давайте вот так. Так нормально? Окей, на самом деле вы можете просто по ноутбуку дальше идти,
там примерно понятно о чем речь. Так что рекомендую. Ой-ой-ой, так только смотрите это. Так как у нас
репетиторий по некоторым причинам из MLMivт переехал в MLCourse, я думаю, вы понимаете,
по каким причинам. Я думаю, вы слышали о том, сколько организаций на GitHub уже забанили. Вот здесь
в ссылочке надо поменять MLMivт на MLCourse. Иначе оно просто не скачается. Вот. Хорошо. Вот. Ну и, собственно,
смотрите, сейчас мы с вами... План семинара какой? Мы с вами сначала посмотрим на то,
что мы с вами пока декларативно видели на лекции, то есть на ту самую неустойчивость, на то,
как у нас работает при использовании регуляризации аналитическое решение, а потом поговорим,
почему аналитическое решение, конечно, классно, но при этом оно не используется. Да. 45? Слушайте,
понять не имею, если честно. Я люблю random seed 42, потому что 42. Но никакого подаёного смысла нет.
Просто зафиксировать random seed, на самом деле, чтобы ноутбук на одном том железе был
воспроизводим. Всё. Почему там два раза, видимо, это просто опечатка. Вот. Итак, смотрите. Ну что
ж, давайте тогда начнём с простой ситуации. Собственно, у нас есть наша матричка X. Мы её
сгенерировали сами. Каким образом? Мы просто взяли получайную матричку из равномерного распределения,
и, соответственно, у нас есть omega true, которую мы точно так же придумали. Всё. Тогда мы с
вами берём, перенормируем нашу величину, нашу матричку так, чтобы признаков были разные,
эти нормы, ненормы, шкалы, и так далее. Ну и получаем наши Y. Вот всё. Можем посмотреть,
на самом деле, на наши X и на наши Y. Давайте его даже нарисуем. Я-то вам сейчас просто нарисую PLT,
ColorMesh, X. Вот, собственно, у нас матричка из двух признаков. Вы можете увидеть,
что первый признак гораздо меньше шкале, чем второй. Согласны? Вот он у нас нарисовался,
этот у нас порядка десяток, этот порядка единиц. Вот наша линейная модель, вот наш функционал
империатричек-кваритиска, функция империатричек-кваритиска среднего-патриотичная ошибка,
сумма квадратов отклонений. Аналитическое решение выглядит так, как мы с вами сказали. Ну давайте
вот тогда посчитаем, то есть xtx-1, xtx, xty, точнее. Вот наш omega star, мы его получили. Да,
на всякий случай это формально вроде w, но по привычке параметры машин обучения называют omega,
а так как на клавиатуре нет omega, обычно omega и w, короче, постоянно друг друга сменяются в
точке верии того, что пишем w, а говорим omega. Это, я надеюсь, никого не смущает, окей? Вот наши
решения, которые мы получили, 0.47-0.14, вот наш omega true, 0.49-0.13. Ну, мы достаточно близки,
с учетом того, что у нас на самом деле был, собственно, нормальный шум добавлен в наш у. Это
абсолютно нормально, потому что мы точное решение никак не получим, у нас данные зашумленные. Мы
назад уже никак не вернемся. Да, у нас здесь, в данном случае, шум, они абсолютно независимы,
они сгенерированы из одного распределения, у них нулевое среднее, единичная дисперсия,
и кавриатс, конечно же, равно 0, они независимы вообще. Вот, все, классно. Ну и, соответственно,
вот мы его посчитали и так далее. А теперь давайте сделаем все то же самое, но теперь у нас,
соответственно, будет что? У нас появляется еще и третий столбец, видите? Минус первый и минус
второй равны, на всякий случай, минус первый это последний, минус второй это второй с конца,
кстати, в данном случае есть минус третий, а он же нулевой, то есть всего 3 столбца. И они равны с
точностью до маленького шума, ε, потому что если они будут полностью равны, у нас просто ничего не
обратится, поэтому они просто будут близки. И опять же, мы пытаемся найти наше решение. Вот,
заметьте, наше решение истинное, которое у нас было изначально. Что? Простите,
сейчас. Смотрите, мы омега тру, мы здесь просто модельную задачу ставим, то есть мы говорим,
у нас есть выборка, у нас есть истинная зависимость, вот мы ее только что придумали,
а теперь мы на основании истинной зависимости генерируем себе целевые переменные у значения
таргета, но зашумляем его, а теперь пытаемся только по х и у восстановить нашу вектор весов. То есть
здесь мы просто себе, грубо говоря, придумали нашу зависимость, а теперь мы пытаемся ее восстановить.
И мы видим, что у нас здесь веса 0,18 минус 0,85 истинные были, а получил минус 0,69 и 0,68. Видите,
опять и не в сумме дают одно и то же, если мы на это посмотрим, но при этом ничего хорошего у нас
не получается, в том плане, что у нас решение краски неустойчиво. И заметьте, в чем суть? Вот краски,
помните, я вам говорил про неустойчивость, да? У нас уже х зафиксированы, давайте я просто
еще раз перестрою y, то есть х я трогать не буду. Смотрите, вот сейчас у нас другое случайное,
по идее, случайный шум добавился, можем опять смотреть на нашу омега стар. Да?
Мы просто, смотрите, у нас есть какая-то зависимость просто вот с такими-то весами,
веса взяли с потолка, я вам могу их сам руками набрать, я буду генератором случайных щелков.
Это просто, не, смотрите, мы просто сделали себе обучающую выборку синтетическую, все. Вот истинная
омега тру, вот просто она откуда-то пришла, например, из генератора случайных щелков, нам по барабану,
она откуда-то пришла, все. Мы пытаемся по x и y восстановить, что там было, откуда мы его взяли,
не знаю, вы могли данные людей пронаблюдать, просто здесь на синтетических данных все. И так,
заметьте, вот я могу это, грубо говоря, повторить 10 раз, собственно, уро и даже не я этот.
Вот, вы можете замечательно увидеть, как это будет меняться постоянно.
И смотрите, у нас с вами просто чуть-чуть зашумляется наш y, можно даже поменять дисперсию шума,
сделать его еще меньше. И заметьте, у нас с вами выборка обучающих, считайте, не меняется,
просто чуть-чуть шум разный, а решение у нас постоянно очень сильно скачан, 0-1, 4-5, 0-0, 1-0 и
так далее, 10. Видите, у нас в зависимости от шума, просто случайного, очень сильно меняется выборка,
точнее, очень сильно меняется решение. Вот что такое неустойчивость, о которой я вам говорил. То есть,
по сути, если мы с вами пронаблюдаем еще, допустим, 5 человек на реальных данных, у нас все равно с
какими-то, допустим, шумами, не знаю, там пульс будет считываться, эти шумы приведут к другому решению.
Это плохо. А истинное значение у нас вот такое, на самом деле, которое мы бы с вами хотели. Видите,
это проблема. Но, на самом деле, сумма-то у нас всегда практически равна. 0,66, 0,67, ну, типа,
очень близко друг к другу. Там разница в третьем знаке после запятой. Вот. Чтобы это починить,
давайте-ка возьмем скраски за регуляризацию. Что мы тогда можем сделать? Во-первых, обращаю
ваше внимание, в данном случае у нас иксы все в одной и той же шкале, никакой перенормировки не
происходило. То есть, нормировать их не надо. Окей? Вот. Ну и тогда наше аналитическое решение,
то есть, решение вот для вот этой вот штуковины. Вот оно. Получается, NP или налог тра-та-та. Наша
лямбда в данном случае равна 0,05. А? Ну, это гиперпараметр. Вот, я выбрал 0,05. Сейчас
можем посмотреть, что будет... Как выбирать гиперпараметры глобально? Или что? Чем больше лямбда,
тем больше вы обращаете внимание на регуляционный член относительно функций потерь. То есть,
грубо говоря, тем больше вы требуете, чтобы у вас было решение с малой нормой, и тем больше вам все
равно на то, какая у вас ошибка. Только и всего. Грубо говоря, у вас такая линейная шкала. То есть,
чем дальше туда, тем больше вы отдаете предпочтение. Вы на самом деле это можете переписать в каком виде.
Вы можете здесь написать, грубо говоря, альфа плюс бета, так чтобы они в сумме давали единицу. Но
бета к альфа относилась как к 0,05 к единице. Тогда у вас просто получается линейная выпуклая комбинация
двух функций потерь. У вас для функций потерь абсолютно неважно, если вы ее домножите на любую константу.
Правильно? Ну вот, можете домножить так, чтобы они в сумме давали единицу. Тогда у вас просто будет
линейная их выпуклая комбинация двух функций потерь. Заметьте, вот мы добавили сюда регуляризацию. И
давайте теперь сделаем все то же самое. То есть, посмотрим, а что, если у нас с вами меняется каждый
раз, собственно, все, что мы хотели с вами сделать. То есть, вот я беру этот код, только теперь
омега стар рег будем писать вместо того, что я до этого писал. Вот опять 10 раз. Посмотрите на решение.
Видите, у нас данные шумят, а решение получается одно и то же. Вот это, что называется устойчивое
решение. Мы можем чуть-чуть поменять данные, ничего не поменяется. Но, заметьте, конечно, если у
нас данные будут шуметь сильно, вам, конечно же, не получится гарантировать решение устойчивое,
потому что у нас теперь данные очень сильно меняются, мы каждый раз получаем разные решения. А тут
дело не в этом. У нас просто каждый раз Y по сути уже разные, потому что у нас сигнал уже меньше шума на
самом деле. 100 раз даже стал больше. Он был 0.1, а теперь он 10 по дисперсии. Да, именно. То есть,
видите, когда у нас шум, грубо говоря, является именно шумовым, то есть он малый, у нас решение
является устойчивым. Оно всегда устойчиво, просто у нас данные уже плохие, это мы никакой регуляризации
не починим. Это опять же пример, вот что я вам говорил, garbage in, garbage out. Если данный отстой,
хоть 10 раз регулирую, у нас все равно получится какая-то фигня. Но опять же, а вот пример того,
что происходит, когда у нас, допустим, большой шум, но давайте у нас будет коэффициент регуляризации,
например, 50. Ой, опечатка. Вот, упал. Ну, заметьте, они все равно разнятся, но уже сильно меньше,
можно сделать его еще больше. Но почему? Он везде, там в районе уже 0.3-0.4, можно сделать еще больше.
Видите, они все приближаются примерно к одной и той же калепсии. Оно выравнивается. Почему? Да
потому что у нас регуляризация, просто член второй регуляционный перевешивает на себя. Мы начинаем
все больше и больше забивать на нашу ошибку, мы просто смотрим на то, что норма вектора весов
была минимальна. Видите, что они почти все одинаковые становятся. Согласны? То есть чем больше
у нас члены регуляционного, не надо так делать на практике, это просто вам демонстрация. Чем
ближе он туда, тем, соответственно, более однородно у нас получается вектор. Согласны? Вот. Хорошо.
Ну что ж, все вроде здесь, разобрались. Что такое истойчивое решение, стало понятней? Замечательно.
Ну и собственно, мы с вами можем видеть, что у нас сумма-то на самом деле сохранилась, но мы из всех
возможных вот этих самых пар выбрали то, которое доставляет нам наименьшую сумму квадрат.
Сейчас до этого дойдем. Вот мы говорим про градиентный спуск. И собственно, почему не
используется аналитическое решение? То, что в нем вам надо обращать матрицу xtx или xtx
плюс лямд и. А обращать матрицу, сложность будет p куб плюс p квадрат n. Если у вас в матрице,
не знаю там, 10 тысяч признаков, то п в кубе, короче, кубическая сложность, вообще говоря,
не очень приятная симптомика. Поэтому если у вас много данных, то это дорого. А мы с вами можем
просто перейти к градиентному спуску, использовать градиентную оптимизацию. Если у нас функция
выпуклая, то мы знаем, что градиентный спуск находит оптимум гарантированно. Правильно? Все это
помнят. Ну хорошо, но тем не менее. Градиентный спуск, то есть вы идете всегда по антиградиенту,
если у вас функционал, который оптимизируется, является выпуклым. Что такое выпуклый функционал,
по крайней мере, все помнят? Это на первой лекции, как правило. Но на всякий случай. Что такое выпуклое
множество? Кто не помнит? Выпуклое множество, если вы берете любые две точки в этом множестве и
напрямую между этими точками все точки тоже лежат в этом множестве. Окей? Супер. Так вот,
если соответственно у нас функция выпуклая, функционал, то градиентный спуск нас приведет
в оптимум гарантированно. Если он не выпуклый, то, как понятное дело, гарантии нет. Если у вас там
есть локальный минимум, вы можете в него попасть и там сидеть. Никаких проблем. Как бы у нас
оптимизация градиентная не работает, не гарантирует нам оптимальное решение, если мы с вами не работаем
с выпуклой задачей. Но так как у нас нет ничего лучше, чем градиентная оптимизация практически,
мы ее используем всегда. И пытаемся сделать задачу более выпуклой различными способами. Ну,
окей, почти всегда, ладно. Иногда мы ее не используем. Все-таки, чтобы быть правдивым.
Собственно, вот это называется learning rate или величиной градиентного шага. Так часто делают.
Ну, в смысле, ее часто выбирают в зависимости того, как быстро мы хотим сходить. И, собственно,
мы с вами точно так же можем градиент явно аналитически выразить. Вот он у нас,
2х транспонированное на х омега минус н. Опять же, вывести это, я надеюсь, не у кого проблем нет.
Продиференцировать функцию потерь по омеге. Один раз. Хорошо? Не поравнивать, не олел, просто
продиференцировать. И, собственно, заметьте, посчитать градиент у нас какой будет? П на н. Вот,
П на н. Вместо от П куб или П квадрат на н. Ну, как бы, сильно дешевле. Нам это придется
несколько раз повторить, но нам это даже не придется повторять там n раз. То есть, это не
ПН квадрат, а ПН, допустим, на 100. Нам 100 шагов, как правило, более чем достаточно, если мы по полной
выбраке считаем градиент. А? Н это размер выбраки. А часто используется, собственно, еще градиентный
спуск не по всей выбраке, скажем так, не полный градиентный спуск, а градиентный спуск, я не знаю,
по бачам или по подвыбракам, говоря по-русски. Но подвыбрака — слово длинное, бач — слово короткое,
поэтому в сообществе, как правило, говорят по бачам. Бач — это подвыбрака. Вот мы его будем
называть бачом, хорошо? Просто я буду это говорить, меня все равно будет проскакивать, чтобы вас
сразу не напрягало. Собственно, как правило, используют градиент даже не по всей выбраке,
а по подвыбраке, поэтому даже сложность каждого шага — это П на к, где К — размер подвыбраки.
Почему? Сейчас. Потому что, как бы, у нас закон больших чисел, грубо говоря, работает, и если мы с вами
градиенту средняем по тысяче элементов или по миллиону, то у нас, в принципе, оценки уже
похожи друг на друга будут. Там по десяти элементам, да, он может шуметь. Поэтому если размер бача
достаточно большой, то у нас оценка градиента даже на подвыбраке случайной все равно будет
достаточно качественной. И все. У вас вопрос был? А это я вам сейчас покажу. Это еще один гиперпараметр,
поэтому его тоже надо выбирать. С гиперпараметрами, к сожалению, ситуация такая. Вам надо подобрать
хороший гиперпараметр для вашей задачи. Вам их никто не подскажет, у вас нет пути их
автоматически выбрать, кроме как какую-то евристику написать, чтобы их автоматом подбирать. Это на то
и гиперпараметры. Их надо выбирать эксперту. Вот. Ну и давайте посмотрим как выглядит градиент
в спуск. На всякий случай, вот тут построим красивую картинку. Тут просто код, который нам
все это рисует. На всякий случай здесь у нас уже, как это называется, пространство, грубо говоря,
вытянутое. У нас один признак больше, чем другой в шкалах признаков. Видите, вот у краски. Там один
на один умножается, другой на два, третий на три и так далее. Ну и давайте посчитаем краски,
как у нас, посмотрим точнее, как себя ведет градиентный спуск. Вот у нас вытянутая какая-то
штуковина. Логично, что у нас функция потерь это парабола, в двумерном уже случае. Параболоид
получается. Вытянутый параболоид вращение. По-моему, это так называется, правильно? Кто помнит? По-моему,
да. Вот мы откуда начали, вот мы куда пошли. Обратите внимание, что у нас на самом деле спуск
достаточно эффективный. Мы сначала скачем вниз, потом вверх. Короче, у нас с кимзигдагами ходит.
Собственно, внимание, вопрос, почему? Шаг большой, да, а еще почему? Не, оно дошло до оптимума,
и шаг у нас, у нас это константное, у нас значение градиента это краски что? Два омега всегда. Ой,
простите, это для регуляризации. У нас значение градиента это, собственно, 2х на отклонение. Чем
меньше отклонения, тем меньше градиента. Вот. Поэтому, да, он уменьшается. Ну, собственно,
почему? А? Не-не-не, тут пока только вторая норма, тут ничего не меняется. Почему оно ползет
именно сначала вниз, потом вверх? А почему? А чего я показываю? Я ничего не понял. Не, да,
все, каждая точка каждый раз уменьшается. Но почему бы нам, смотрите, оптимальней же было
сразу, например, вот сюда шагать, не вот сюда, а на такой же шаг вот сюда. Согласитесь, тогда
функция ошибки была бы меньше. Согласны? Почему мы идем сюда, а не сюда? Бинго. Во-первых,
мы локально считаем вот здесь. А во-вторых, у нас градиент всегда направлен перпендикулярной
линием уровня. Это доказывает, как правило, на первом курсе. У нас градиент всегда перпендикулярной
линии уровня нашей функции. Потому что это направление наискорейшего возрастания,
если с минусом убывания. Если мы на любой другой угол повернемся, то у нас будет только на синус
или на костюм, соответственно, сдвиг, он становится меньше, и так далее. Поэтому мы идем
ортогонально вот этой, по сути, прямой, которого здесь. А так как у нас не центрально-симметричный
случай, то есть паравал наш вытянут в каком-то направлении, поэтому нам в этом направлении на
самом деле идти менее выгодно, чем в этом. Здесь мы как бы быстрее будем идти вниз, поэтому тут
градиент больше. И это еще одна причина, почему нормировать данные, вообще говоря, хорошая идея.
Потому что если мы сейчас с вами данные отнормируем, давайте я ту же самую картинку построю, но уже с
нормировкой. Вот, собственно, смотрите, что без нормировки? Все очень логично, оно идет ровно в
центр как и надо, без всяких колебаний. Согласны? Ну и на всякий случай, опять же, что будет,
если здесь, наоборот, как бы еще больше его раскинуть. Ладно, оно не смогло от такой большой,
как бы, короче, плотлив сломался, это нормально, так бывает. Ну ладно, ему что-то плохо становится,
извините, пожалуйста, я верну туда двойку. Короче, идею вы поняли. Вот сейчас, вот это без нормировки.
На самом деле мы-то идем правильно, мы всегда идем ортогонально линией уровня. Просто у нас,
как бы, в одном направлении вот в этом градиент меньше, потому что она вытянута более, у нас
более пологий склон, а в этом направлении у нас склон очень крутой, там градиент большой. Все.
Ладно, это, собственно, первая ситуация. Но, во-первых, еще раз, давайте я вам покажу просто красивую
картинку. Кстати, этот ноутбук частично базируется на наработках Евгения Соколова из вышки. Замечательный
курс, тоже у них есть у них есть репетитории, тут есть ссылка, так что тоже рекомендую посмотреть
в качестве доп-материала, классно заходит. Вот, соответственно, вот как направлены антиградиенты
везде, так на всякий случай, чтобы вы визуально видели, что они всегда перпендикулярованы линиям
уровня. Но, опять же, это, вроде, совсем базовые вещи. А теперь давайте возьмем, собственно,
по подвыборкам градиентный спуск или стахастический градиентный спуск с размером выборки под
выборки бача равным 10. Раньше мы каждый раз считали градиент по всей выборке целиком. Там размер
выборки. Там было 300 объектов, мы каждый раз на 300 объектов считали. Теперь будем на 10. Давайте
посчитаем. Мы каждый раз выбираем с вами случайные 10 объектов, для них все считаем. Вот наше
аналитическое значение градиента. Ну, вроде, тут копировать соб нечего. Заметьте, оно вроде
сходится, но заметьте, что мы гораздо более шумно стали болтаться вокруг центра. Почему так
произошло? Ваши ставки. Да. Смотрите, почему это происходит. Да, потому что, во-первых,
мы с вами берем только маленькую подвыборку, но на самом деле это не единственная причина. Вторая
причина заключается, собственно, в том, что у нас в данных есть шум. Если мы с вами уберем этот шум,
то у нас никаких проблем не будет. Смотрите, он по подвыборкам идет, но тем не менее, видите,
оно сошлось в центр и замечательно там стало. У нас стахастический градиентный спуск делает,
что он каждый раз 10 или размер выборки объектов выбирает, по ним считает ошибку, по ним считает
градиент, усредняет этот градиент по 10 объектам, идет туда, куда показал градиент, ну антиградиент.
Поэтому если у вас попали объекты, у которых, грубо говоря, все показания завышены, он будет
пытаться, наоборот, предсказывать большую величину. Если все показания занижены, меньше и так далее. То
есть у нас шум влияет на наши шаги обновления. И, собственно, именно поэтому стараются выбирать
достаточный размер подвыборки, потому что чем больше размер подвыборки, тем больше мы объектов
усредняем, соответственно, тем меньше у нас шум, потому что мы предполагаем, что шум у нас случайный и
центрированный. То есть мы ничего не завышаем, как бы у нас нет систематической ошибки, у нас случайная
ошибка. Окей, тут понятно? Ну, грубо говоря, у нас размер бача не позволяет компенсировать тот шум
достаточно сильно, поэтому мы болтаемся. То есть когда мы болтаемся вокруг центра, это означает,
что у нас, грубо говоря, характерно размер шума сопоставим с тем, что мы получаем в конце, как бы
отклонением от оптимума. Это нормально. То есть мы все равно бы увидели, что у нас функция потерь
вышла на какой-то плато. Другое дело, что, возможно, мы могли дойти куда-то до более хорошей точки,
но мы на тут не смогли добраться. Если уменьшить гридентный шаг, вы тоже правы, у нас будет более
устойчивая сходимость. И, собственно, сейчас до этого и дойдем. Не, на самом деле, класс, задавайте,
пожалуйста, вопрос. То, что я говорю сейчас, до этого дойдем. На самом деле, вследствие просто того,
что, как вы понимаете, курс все-таки не первый год читается, и ноутбуки и материалы дорабатываются,
в том числе по вашим вопросам. Так что задавайте вопрос. Ага. Да, ну, собственно, если у вас несколько
минимумов, да, если у вас несколько минимумов, то вы попадете в любой локальный минимум и никаких
гарантий, что вы пойдете в главный оптимум, у вас нет. Ну, это, скажем так, это работает скорее в том
смысле, что вы из глобального минимума с меньшей вероятностью выпадете в локальный, потому что
локальный выше глобального. Вот все. А за счет стахастического гридентного спуска, за счет стахастики,
у вас более шумные грубые, у вас шаги более случайные, вы можете из него выпрыгнуть. Но вообще говоря,
в общем случае и стахастический, и полновыборочный гридентный спуск, если у вас задача не выпуклая,
то никаких гарантий на то, что вы найдете глобальный минимум, у вас нет вообще. Все, конец. Здесь как
бы полномочия гридентной оптимизации, все. Она не работает, она вообще не дает никаких гарантий.
Она работает в тех предположениях, которые есть. Собственно, а про размер шага абсолютно верно
заметили. Мы видим, что он болтается где-то вокруг оптима, и на самом деле здесь есть пара вещей. Во-первых,
есть красивые условия Робинса Монро, которые обычно вот на этом занятии мы просто говорим, про них
все слушают, где-то они остаются на задворках памяти, но на практике их, как правило, не используют. Так
же давайте я вам сначала скажу про них, а потом скажу, как делать на практике. Собственно, давайте
вспомним чуть-чуть матанта, второй курс, ряды и так далее. Вам ничего не напоминает вот эта штука.
У нас вот этот ряд расходится, а ряд уже квадратов сходится. Это еще раз шаг на катом шаге. Размер
learning rate на катом шаге. Гармонический ряд. А, ну 1 на n, например, да, 1 на n будет работать.
Собственно, если есть условия Робинса Монро, там есть работа классная, ее даже можно почитать. Сейчас
мы ее читать, конечно же, не будем, тем более она вон скачивается. Он доказал, что если learning
rate, грубо говоря, убывает достаточно медленно, но при этом квадрат всех этих элементов это сходящийся
ряд, то вы можете, в принципе, дойти с любой задней точностью до любого оптима. Собственно,
почему это происходит? Ну вот здесь есть простое объяснение, что, во-первых, то, что у вас ряд этот
расходится говорит о том, что вы можете до любой точки дойти, откуда бы вы ни стартовали, вы всегда
можете добраться куда угодно. Он не слишком быстро сходится. А с другой стороны, вы можете, он
достаточно быстро сходится, чтобы вы, в конце концов, с любой заданной точностью дошли до оптима.
Не, погоди, если у вас стахистический гряднецный спуск, нет, не сойдетесь. Ну и, собственно,
давайте посмотрим на пример краски гармонических ряд. Там 1 делить на n, например, будет работать. Ну
или, на самом деле, можно любую другую взять, например, 1 делить на n в степени 0,5, 0, 0, 5. Ну,
короче, любая штука, которая в квадрате, будет давать степень больше единицы. Вот. Ну вот,
пожалуйста, наш стахистический гряднец спуск тогда. Тебе что, плохо что ли? Ладно.
Не, мы попали в центр, просто что-то, собственно, стахастика. Давайте я просто шум поменяю,
чтобы оно рисовалось лучше. Так как бы, как вы понимаете, оно ничем особо не влияет,
мы все равно сошлись, просто видно плохо. Ну и просто на экране достаточно плохо видно. Давай рисуйся.
Да, давайте я просто learning rate в начальной сделаю. А? Во, пожалуйста. Ну, короче, там просто
начальный learning rate был очень большой, поэтому он очень далеко скакал. А? Это, это, вот это вот
самое. Это на градиент. Это тот, а? Не-не-не, в данном случае, а, размер шага. Простите, я думал,
размер выборки нет. Да, это именно размер шага. Тот коэффициент, грубо говоря, на который мы
его домножаем. Вот, пожалуйста, если он поменьше, то мы и сошлись. Тут у нас уже градиент маленький.
Тут мы просто выпрыгивали слишком далеко. Там у нас опять был большой градиент, он несколько
раз болтался. Ну, это нормально, на самом деле. Собственно, но так делают обычно, скажем так,
на практике редко. Типа, в какую-то ивристику, как он должен сходиться. Плюс, для невыпуклых
задач это работает плохо. Поэтому что делают, как правило, на практике? Во-первых, делают так
называемый learning rate, scheduler, scheduler. Короче, штуку, которая по какому-то расписанию вам
понижает learning rate. Причем это может происходить даже с какими-то умными, скажем так, стратегиями.
Ну, например, что делать, если у вас слишком большой learning rate, вы болтаетесь вокруг оптимума? Его,
по идее, надо понизить, правильно? Чтобы не болтаться. Но тогда вопрос, а во сколько раз его понижать?
Непонятно. Собственно, и когда это делать? Понятное дело, руками это все делать лень,
поэтому люди давно уже придумали, как это делать автоматом. Ну, например, берут и делают вот такую
зависимость. У вас при выходе на плато, например, это в том же самом пайторче, так и называется,
reduce LR on plateau, понизь learning rate на плато, там правило какое? Вот у вас график, грубо говоря,
строится от шага. Если видит модель, что learning rate, грубо говоря, точнее loss примерно, верни мне
мой график, loss примерно не меняется, то есть по факту он у вас на самом деле будет такой зашумленный,
типа вот такой, но видите, что он болтается где-то там около одного и того же уровня. Тогда срабатывает,
грубо говоря, правило, что за последние 10 шагов среднее значение ошибки не упало, и тогда у
срона у нас LR, например, там домножается на 0.1, ну или на 0.5. Короче, на какое-то число меньше
единицы. После этого learning rate падает, как правило, после этого ошибка опять начинает чуть-чуть падать
вниз, выходит на новое плато. Так можно сделать несколько раз, но каждый раз у вас будет все меньше,
меньше, меньше вот этот вот прирост, потому что это обычно происходит почему? У вас есть какой-то
оптимум, который вот условно здесь вот еще вот вниз, здесь еще вниз, и вот здесь вот он у вас. Тут
вы могли болтаться по этим краям, потом вы дошли куда-нибудь вот сюда, потом вы дошли уже сюда в
самый оптимум. То есть каждый раз вы будете, как правило, меньше, меньше прирост получать, но
понижение learning rate это штука классная. Не, погодите, стало плохо, в смысле это хуже,
чем было точно не будет? Да, так это конечно может. Бинго, это самый простой случай, он работает
только если вы находитесь около, вообще говоря, выпуклой задачи работаете. Собственно, что делают,
когда задача не выпуклая, то есть в большинстве случаев. Как правило, на него еще наворачивают
что-нибудь типа любой периодической функции, то есть он на самом деле то падает, то потом растет.
Зачем это нужно? Чтобы если мы вдруг попали в какой-то локальный минимум, мы попытались из него
выпрыгнуть. То есть мы опять его повышаем, наша модель куда-то ускакивает, и потом если мы опять
туда же сойдемся, хорошо, если мы выпадем в другой минимум, ну замечательно. Вот все. Ну и
собственно вторая вещь запоминается для... Ладно, он мне больше не нужен. Мы из него выпасть не можем,
ну мы можем из него выпасть локальный, и поэтому, собственно, я не договорил. Запоминается, собственно,
лучшие на текущий момент значения модели, например, по качеству на отложенной выборке,
и соответствующие параметры. То есть мы до него добрались, это мы уже помним. Пытаемся дойти куда-то
еще. То есть хуже мы точно не сделаем, мы помним все свои на лучшие результаты. Вот. Но опять же,
если задача не выпуклая, никаких гарантий, что вы получите глобальный оптимум, вас вообще нет.
По объективным причинам. Окей. Ну а теперь давайте посмотрим на последнюю вещь. Смотрите, тут
собственно в семинаре две части есть. Первая часть, она обязательная, и мы ее как раз сейчас закончим до
восьми. Вторая часть опциональная, поэтому можем ее тоже с вами пройти. Я, когда закончу обязательную,
вам про нее скажу. На нее идет еще, наверное, минут десять, то она любопытная, но вообще,
говоря опционально, именно опционально. Просто прикольно. Давайте просто сравним время, с
которой сходится скорость сходимости СГД и просто СГД. Ну СГД соответственно по бачам, ГД по объектам.
Размер. Что-то здесь. Сейчас, момент. Я уже помню эту ошибку.
Ну зачем оно так? P, range, len. Вот так. Короче, я тут что-то перемудрился во время с кодом.
Так сильно проще. Вот. И что? Где картинка? А, я понял. А вот XSlim, это собственно len residuals.
Вот. Собственно, вот наш СГД, как сходится. А где же наш градиентный спуск, простите меня.
А, все, спасибо. Вы абсолютно... А, я понял. Это все, я понял. Это ноутбук, который на коленке ковыряли.
Спасибо. Вот, собственно, смотрите. Вот у нас градиентный спуск, как сходится.
Наметьте, он там за 100 шагов уже где-то около нуля с ошибки находится. В принципе, мы сошлись.
Дальше он почти не меняется. Вот стахастический градиентный спуск вроде как идет медленнее,
да? Но как бы с другой стороны, можем даже перестроить, если хотите, этот график. Если посмотреть
на то, чтобы здесь кстати... А, здесь как раз раньше код был, собственно, для числа объектов. Я понял.
Вот. На nobjects, а здесь на batch size. Вот, собственно. А теперь посмотрим на количество объектов,
которые понадобилось стахастическому градиентному спуску, чтобы сходить и полному. Наметьте,
то есть мы потратили там, я не знаю, сколько это, 10 в пятый, то есть где-то 2 на 10 четвертых объектов
для стахастического градиентного спуска, и все, мы уже сидим где-то около оптимума. По всему бачу
нам пришлось делать шагов гораздо больше. Видите? То есть вроде как мы шагов-то сделали градиентным
спуском меньше, чем стахастическим градиентным спуском, но зато на каждом мы использовали 300
объектов, а не 10. От размера выборки. Да, соответственно, нам чем больше объектов в выборке,
тем сложнее. На самом деле это не совсем так. Почему? Потому что у нас, грубо говоря, есть векторные
инструкции в процессорах, они у нас до какой-то степени могут, грубо говоря, за константное
время обрабатывать там и 4 объекта, и 8, и 16, и 32, и так далее. Потому что они одновременно на
одном такте будут обрабатываться. Потому что вектор инструкции есть. Но, тем не менее, на больших,
грубо говоря, объемах все равно. Чем больше размер выборки, тем дольше по ней считать,
тем более устойчивый градиент, но тем дороже это нам будет. То есть все равно стахастический
градиентный спуск имеет смысл. Ну вот, на самом деле. Ну и последнее, что хочется сказать этого
красивого. Смотрите, две вещи. Первое, помните, я там говорил, есть всякие мопе, смопе и так далее.
Короче, что это такое, почитать можно всегда допам, это не особо критично, но от коэффициент
детерминации хочется, чтобы вы все знали, помнили и главное понимали. Потому что он очень любим в
экономике, эконометрике, в банках и вообще в регрессии. Им очень часто пользуется. Собственно,
вот коэффициент детерминации. Давайте не унимательно посмотрим, что это такое. Во-первых,
он очень похож на среднюю квадратичную ошибку. Согласны? Вот она у нас тут сидит внутри, по сути.
Но при этом он на самом деле что нам показывает. Мы берем среднюю квадратичную ошибку, а просто
квадратичную ошибку без усреднения и сравниваем, насколько наша модель, по сути, лучше или хуже,
чем просто средняя. Вот все. То есть мы говорим, у нас есть среднее значение нашей целевой
переменной, просто предсказанной целевой переменной константой, вообще без всяких параметров. Если наша
модель лучше, то, соответственно, числитель будет меньше, чем знаменатель и, соответственно, вот эта
штука будет ближе к единице, правильно? Если наша модель каким-то образом хуже, чем средняя, то
числитель больше, чем знаменатель, и тогда у нас r квадрат вообще отрицательный. Именно что это r2 на самом
деле, а не r квадрат, поэтому не надо думать, что он не может быть отрицательным. Его просто так называют.
Эффициент детерминации. Хорошо? И как бы дурацкий вопрос с собеседованием, с экзаменом и так далее. Он
сверху ограничен, а снизу? Вот именно. Очень часто, пустя какое-то время, у людей почему-то в голове
возникает ощущение, что он и снизу ограничен. Ни разу он снизу не ограничен. Как бы худший случай,
когда у нас модель вообще ужасная, любой. Как бы худший осмысленный случай, если наша модель хуже,
чем средняя, то он равен нулю. То есть, грубо говоря, если он меньше нуля, то значит наша модель,
заменяем на среднюю, уже радуемся, все. Вот. То есть, грубо говоря, осмысленное значение у него от нуля
до единицы. Все, что меньше нуля, это бред какой-то. Нашу модель надо просто выкидывать, заново делать.
Проверка, в смысле, про вопрос? Если модель меньше нуля, то да.
А, ну короче. Да. Ну, ее очень часто используют, да. То есть, по сути, во-первых, она чем хороша? Она у вас
не зависит от шкалы вашей целевой переменной, видите? То есть, у вас там могут быть хоть миллионы,
хоть нули, 0,001, там, 10-5. У вас все равно, чем ближе к единице, тем лучше. Она нормируется, собственно,
в этом ее плюс. Ну плюс, ее экономисты любят там за какие-то свойства. Короче, в банках она вообще
сплошь рядом везде в регрессии. Поэтому знать про нее лучше надо, а? Коэффициент детерминации или R2
score или R2? Ну, на английском R2 score или коэффициент детерминации, determination coefficient, как он называется?
Я не понял вопрос. Давайте сейчас тогда вам тут отвечу, а то уже я не слышу вас просто. Ну ладно,
ребята, и последний, собственно, момент. Смотрите, все как бы обязательно на этом заканчивается. В
скалерне, понятное дело, все это написано. Вы можете этим замечательно пользоваться. Но простой
пример, собственно. Вот почему... А, кстати, очень важные... У меня очень много очень важных
замечаний, я понимаю. Но еще одно очень важное замечание. Читайте доки, пожалуйста. Вот все,
что вы используете, где угодно, на самом деле, в любом случае, когда прогрете, читайте доки. Но
конкретно в машинном обучении, если вы берете какую-то либу, из нее что-то импортируете, читайте,
пожалуйста, доки. Что она на самом деле делает? Например, если вы почитаете под капотом,
линейная регрессия из-за скалерна, именно вот просто линия regression, под капотом использует
аналитическое решение. А, например, вот где он там называется? Какой-нибудь Lassa Ridge, это красая
ледяная ледовая регуляризация, или SGD Regressor использует сахастическое решение. Я к чему говорю?
Давайте просто посмотрим. Вот у нас решение, грубо говоря, задачки регрессии уже на больших
скавычках данных, на большой выборке. 700 признаков. Сколько у нас тут? 100 тысяч объектов. Собственно,
вот наша линейная регрессия. Она считается, считается, считается, считается. Ну, скорость
досчитается. Вот, она 8 секунд, почти 9 считалось. Вот у нас Ridge, который с альфой 0, короче,
это линейная регрессия, но градиентная. Альфа 0, то есть коэффициент регуляризации 0, но он
использует уже градиентное решение под капотом. Ну, как бы, одна секунда. Как бы, R2 0,99, 0,99.
Разница почти в 10 раз по скорости. Поэтому, пожалуйста, читайте, что модели, которые вы
используете, используют под капотом. То, что можно просто взять, типа, его туда, например, дёрнуть,
регуляризация вам почему-то не нужна. Работает всё 10 раз дольше, просто пружь не та модель. Вот,
как-то так. Ну, что ж, на этом вся обязательная часть занятия закончена. Тут есть анализ
нестабильности. Если хотите, можем сейчас посмотреть. Я бы, на самом деле, его куда-нибудь
на следующих занятиях вытащил, потому что он нетривиальный, и, мне кажется, все устали. И
вторую домашку по линейной регрессии я вам тоже прямо сейчас, на самом деле, выгружу. Во-первых,
потому что первая, ну, она очень простая, там, на 20 минут работы. У неё, соответственно, дедлайн
будет три недели, чтобы вы могли совершенно спокойно её писать. Ну, всё.

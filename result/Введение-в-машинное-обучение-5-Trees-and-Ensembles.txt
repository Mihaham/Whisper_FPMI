Мы начинаем. Итак, ребята, сегодня мы с вами продолжаем беседу про различные методы машинного
обучения. Сегодня нас ждет ужас, страшное и великое дерево. Вот кто-то из вас, скорее всего,
на алгоритмах все эти деревья уже строил, там можно было строить сбалансированные деревья,
вообще структура данных дерева много где используется. В машинке бинарное дерево тоже всеми
любимо. И это один из немногих алгоритмов, которые можно использовать, при этом не имея никакого
градиентного метода оптимизации под капотом, скажем так. По сути, у нас здесь все будет нажат на
оптимизации сидеть и, тем не менее, это работает. За это стоит на них обратить внимание. Итак,
сегодня в меню. Ты чего не работаешь? Сегодня в меню. Во-первых, мы поговорим про решающие деревья
концептуально. Что это такое? С чем их едят? Почему в век нейронок они все еще актуальны? Во-вторых,
мы с вами разберемся непосредственно, как строится дерево и, более того, построим с вами его прям по
шагам. Раз, два, три, четыре, пять. Более того, у вас будет опциональная домашка. Опциональная в том
плане, что чтобы получить отличный по курсе, не обязательно делать, но она вызывает часто страдания
у тех, кто не подготовлен писать много кода на питоне. Поэтому, если вы не решаете, мы вас за это
не журием, скажем так. У вас будет краска четвертая, опциональная домашка по реализации дерева на
питончики. Крайне рекомендую ее решить, но скажем так, если совсем не смогли, ну не смогли.
Она дает баллы, то есть, грубо говоря, она есть в числителе, но нет знаменателей, ваша оценка.
Грубо говоря, вы можете, решив ее, получить там 105 процентов баллов за курс, ну типа то, что
национально. То есть, если вы ее не сделаете, то не столь страшно. Крайне рекомендую сделать,
но опять же up to you. Но, тем не менее, она является хорошим таким, не пререквизитом, а подготовкой к
пятой домашке, которая обязательная. Там вам надо будет написать дерево с нуля, но вам, типа, дадут
игнатуру класса какую-то минимальную, типа у него есть какие-то методы, типа feed, тратата. Вам надо
все написать. Короче, как кнн, только у вас простор больше, а злобные тесты на время более злобные.
Ребят, вы на фистехе первый год учитесь? Прикольно, а кто на фистехе первый год? Ну вот, не так много.
Ребят, ну тогда привыкайте, это фистеха. Здесь основная суть, чтобы вы знали, а не чтобы вы сдали.
Работает в обе стороны, можно узнать и не сдать. Такое тоже бывает. Я один раз пришел на экзамен по...
Что это было? По статфизу, короче, кажется, там была прям боль. Нас на статфизе гоняли в хвосты,
в гриву. Это был последний семестр бакловриата. Мы статфизчев, на-говори, работали, но не очень
хорошо. Мы диплом писали. Я с Фуфма. Ладно, давайте перейдем к машинам обучения, а то у нас так сегодня
семинар сгорит. Короче, что такое дерево, как работает, как построить. Как построить, я вас прошу
сегодня очень внимательно отнесись к этому и понять. Ну а потом поговорим про то, почему деревья
хороши, как их можно объединить между собой, чтобы порешать их проблемы. И внезапно посмотрим на
деревья сквозь призму линейных моделей. Потому что на самом деле линейные модели и деревья это
суть одно и то же в разных, очень разных признаковых пространствах. Именно поэтому я не раз говорил,
что линейные модели это вообще, наверное, корень почти всего. Итак, поехали. Так, номер раз. Датсет
вам уже знаком. Это те же самые Ирис и Фишер. Мы их с вами видели на первом занятии, возможно,
на втором и так далее. Три класса, четыре признака. Ну тут мы с вами только два нарисовали. И,
соответственно, необходимость сепульки и петульки используя отделить три класса друг от друга.
То можем сказать про то, что мы видим на экране. Смотрите, давайте каждый раз, вот просто будем
действовать по шагам. Будем брать какой-то признак. В данном случае вот два признака взяли, не помню
какие. Это PL и PV. Petal Length, Petal Width. И по ним проводить просто какой-то порог. То есть по одному
признаку. Условно признак там больше или равен 2.5, иди направо, иначе иди налево. То есть каждый раз
мы будем, по сути, от нашего какого-то пространства делить его пополам. Ну или уже от нашего
полупространства делить его опять же пополам. Окей? Ладно, подпространством это некорректно.
Называть это неподпространство это, не знаю, полупространство. Короче, одна часть пространства и
другая. Все. Вот отделили все красное допустим снизу, сверху у нас осталось. Второй шаг. Говорим,
все что справа от вот этой прямой зеленое, слева непонятно. Теперь делим опять. Все что ниже этой
прямой желтое, но тут есть второе условие, которое говорит, что все что ниже этой красное, оно
доминирует то, что оно было раньше, ближе к корню дерева. Все поделили. Получается, что мы с одной
стороны построили какое-то абсолютно деревянное решение, причем реально деревянное дерево,
которое у нас просто умеет тыфами делить какое-то наше пространство на подобности, а с другой стороны
посмотрите на разделяющую поверхность. Она нелинейная. Мы никаких с вами ядер не придумали,
ничего мы с вами делали, но получили нелинейную разделяющую поверхность. Возникает, собственно,
вопрос. А. Как это дело строить? И Б. А насколько это хорошо? Пл меньше 2,5 и Пл.
Пл больше 2,5 и меньше или равен 5? От 2,5 до 5. Вот у вас закрытый открытый. Пл больше 5 направо,
Пл налево полз, направо тру. Пл меньше 5, вот. Это желтые, да, все правильно. Короче, вопрос,
как это дело строить? То, что выглядит неплохо. Согласны? Ну вот сейчас мы как раз это разберем. И
собственно, в чем второй плюс деревья? Первое. Давайте сразу оговоримся о некоторые вводные.
Во-первых, в каждом листе дерева мы предсказываем с вами здесь какую-то метку класса, да, ну или мы
там можем предсказывать число абсолютно так же. Вот вам задача регрессии. Посмотрите, это дерево
точно так же. У нас в зависимости от аргумента меняется предсказание. Просто если такой-то такая
константа, если больше такая константа и так далее. То есть, по сути, это кусочно-постоянная
функция. Согласны? Что можно сразу сказать про дерево? В классических деревьях, опять говорю
классические, потому что есть неклассические деревья или там неоклассические как угодно
назовите, в классических деревьях в листе сидит константа. То есть, каждый лист на ответственно
какой-то константе. Будь то метка класса, число и так далее. Есть всякие там более, скажем так,
новомодные деревья, которые в каких-то статях встречаются, но на практике редко используются,
там в лист запихивают прям линейную модель и так далее, но это все не очень удобно. Хорошо? Вот я
сразу хочу с вами оговориться об одной вещи, чтобы у вас оно все равно в голове откладывал.
Давайте договоримся, что вот отныне и до упора, я это уже на третьем занятии говорил, у нас в листик
дерева и вообще в задачи пластикации мы не метку класса предсказываем, мы предсказываем
вероятность на распределение над метками классов. Хорошо? То, что так будет гораздо проще и думать,
мыслите и вообще работать со всеми моделями. То, что как средние две метки, непонятно,
у вас два распределения, как средние два вектора, ну положили, поделили пополам. Все понятно. Так что
здесь мы везде будем предсказывать вероятность класса. Как нам ее взять, тоже с вами разберемся.
Ну и, собственно, вопрос, как вот эту красоту построить? Тут я сразу хотел бы еще один вопрос
задать вам, вот особенно на этой картинке понятно, как вы думаете, способны ли деревья
работать вне выпуклой оболочки, скажем так, точек из обучающей выборки?
Да, все во все стороны мы с вами покрываем, но вот в регрессии чуть лучше видно. Мы можем
предсказывать, смотрите, здесь у нас будет правило условно х меньше, чем там 0,1, тогда предсказывай
константу. Но о чем стоит помнить? У вас дерево в каждой конкретной точке, ну условно на каждой
подобности, предсказывает константу, в том числе вот от крайней точки и до упора влево, от крайней
точки до упора вправо, если это одномерный случай, многомерно может сами обобщить. Поэтому к
экстраполяции дерево не очень предвосположено, оно просто предсказывает константу вне вот этой
самой выпуклой оболочки, обучающей выборке. Почему об этом важно помнить, это от ваших признаков
зависит, но иногда у вас там может быть, что вы условно находитесь где-то на границе, и за границей
вас всегда будет константа предсказывать. Условно линейная модель может хотя бы линейный тренд вытащить
куда-то наружу, за область определения. Может он не факт, что будет правильно, но тем не менее. Дерево
неспособно, оно вот вне области определения, оно ничего не знает. Оно просто берет свое крайнее
значение из области определения и просто его до бесконечности распространяет. Это такая маленькая
вещь, о ней потом почему-то любят люди забывать. Это понятно? Хорошо. Кто такое выпуклая оболочка?
Ай-ай-ай, на контрольный вас об этом спросит. Простите, сейчас я отвечу. Слушаю вас. Так,
давайте очень коротко, я на лекции. Бу-бу-бу-бу-бу, спасибо. Смотрите, у вас есть их телеграммы?
Можете мне в телегу написать, я вам все пришлю. Я вам скину все ссылки, они уже подключатся.
Хорошо, я вам где-то 18.30 все пришлю, потому что сейчас я на лекции. Договорились, спасибо.
Простите, орг-моменты. Поехали. О, все, люди убежали. Деревья страшные. Короче, как дел строить?
Смотрите, строится дерево максимально прямолинейно и тут можно вспомнить алгоритмы.
Короче, кто помнит что такое жадный алгоритм? Ну, замечательно. Вот если вы понимаете,
как работает жадный алгоритм, то вы понимаете, как строится дерево. Смотрите, первое. Давайте
предположим, что у нас с вами есть один признак, нам по нему вот надо поделиться. У нас один признак,
нам надо выбрать наилучшую, скажем так, границу, по которой разделить наши данные. Предсказываю их
константами. Чем можем сделать? Ну, мы можем какую-нибудь середину выбрать или мы можем,
на самом деле, перебрать возможные границы, ну, медиану или выбрать какую-то границу, на самом
деле, и сказать, вот здесь у нас будет разделение. Тогда для левой и правой части мы предсказываем
свои константы, правильно? Ну да, ну, собственно, у нас дерево всегда предсказывает константу в
каждом листе. Пока у нас дерево с вами, вот изначально это просто будет одно дерево из одной
вершины. Значит, допустим, предсказываем среднее. Почему среднее оптимальное,
тоже потом скажу. Предсказываем среднее с вами. Хорошо, теперь нам нужно где-то поделить это на
два листа, то есть две константы начать предсказывать, так, чтобы у нас дерево стало лучше. Тут, собственно,
возникает вопрос, что такое лучше? Пока мы это не облекли в какие-то формальные критерии, типа,
средней кватратической ошибки, например, или средней абсолютной меньше. Что такое лучше, непонятно.
Окей? Мы должны найти точку, в которой мы проведем какие-то константы, выберем новые константы,
и станет лучше. В этом дерево и заключается. Я возвращаюсь обратно на этот слайд. Смотрите,
собственно, что мы делаем. Мы находимся, допустим, на всю выборку смотрим. Я сразу,
на всякий случай. Так, что такое мат-индукция, все помнят? Что происходит с физтехом? База
индукции, шаг индукции. Если выражение верно на и там шаге, тогда вот вам переход на и плюс
первый. Если вы ледницу, например, докажете, значит вы доказали для любого и. Ура! Ну,
потому что по умолчанию строят бинарное дерево, потому что у вас у операции сравнение больше или
меньше два выхода, да или нет. Вы можете, на самом деле, деревья с множеством выходов строить,
и такое даже умеет, например, катбуст. Но вообще говоря, можно с помощью той же самой бинаризации
привезти к бинарному дереву все равно. Категориальный признак, например, разбить на кучу бинарных
признаков, и дерево тогда тоже построится. Просто раньше было гораздо удобнее строить бинарное
дерево чисто из вычислительных соображений, его удобнее хранить и так далее. Так вот, давайте
скажем так. Пока что мы с вами выбрали какой-то признак, и по нему пробегаемся по всем возможным
трешхолдам, вот этим вот разделением, точкам, где мы разделяем. Причем для каждого трешхолда мы
собственно бьем нашу выборку на левую подвыборку и правую подвыборку. Они у нас не пересекаются,
это логично? Логично. После чего мы с вами повторяем операцию опять, опять, опять. Короче,
рекурсия. Вот. Возникает, собственно, вопрос. Да, кто не увидел морфюса, вот вам кусочек морфюса
с рекурсией. Возникает маленький вопрос, собственно. А почему, точнее, а по какому критерию нам разделять?
Вот давайте введем некоторый функционал просто, который будет измерять, насколько подвыборка
стала упорядочней, лучше. Потому что наша конечная цель какая? Мы же в каждом листе константа
предсказываем, правильно? То есть эта константа должна быть максимально похожа на все объекты,
обучающие выборке, которые туда попали. Ну, собственно, вот эта самоднародность мы с вами и будем
пытаться посчитать. До свидания. До свидания. Восстание машины не начало. Итак, вот давайте введем
некоторый функционал. Пока что не будем говорить, что это. Просто нам нужен какой-то функционал,
который умеет оценивать упорядоченность левой подвыборки и правой подвыборки,
правильно? Исходно. Короче, для любой выборки из элементов он умеет оценивать, грубо говоря,
степень хорошести. И, соответственно, ну ладно, здесь будет степень нехорошести раз на минимум,
степень нехорошести. Поставим минус. И, соответственно, при этом нам нужно учесть,
насколько много объектов попало влево, насколько много объектов попало вправо. Ну почему? Простой
пример. Вот у нас есть с вами выборка, мы с вами отделили один объект, знаете, вот такой очень
тоненький кусочек колбасы. Вот один объект из класса зеленые. Все, теперь у нас есть сравнение,
крайний объект взяли и говорим, вот этот зеленый, про все остальные, ну не знаю рандомно. С одной
стороны, зеленый мы классифицируем теперь 100% точно. С другой стороны, на фоне 10 миллионов
объектов это не то, чтобы полезно вообще. Ну типа нас осталось 999 тысяч, 900, бла-бла-бла,
объектов, которые и плохо классифицируются. Да? Зачем мы на Q делим? На Q мы делим, ну на самом
деле да, вы правы, на самом деле что такое Q? Вы правы, на Q можно особо не делить. Зачем мы делим
на Q? Q это мощность исходного множества, чтобы у нас всегда просто была доля, чтобы было просто
понятно. Иначе у вас начнет постепенно условно, когда у вас, если на Q не делить, а у вас выборка
там размером, я не знаю, миллиард, то у вас будет большое число. Вот тогда у вас получается
мощность здесь миллиард, здесь два, типа они слишком в разных порядках. Так вы избавляетесь и у вас
всегда число от 0 до 1. То есть просто с точки зрения нормировки. Пока понятно, что происходит? Да,
что такое? Аж мы сейчас с вами про это целые 10 минут будем говорить. Ну смотрите, раз на минимум,
то есть смотрите, мы хотим сделать так, чтобы вот эта сумма стала меньше. То есть тогда это
мера неупорядоченности. Я, простите, оговорился. Сейчас я дверь закрою.
Погромче повелось, говорите. Сейчас тоже скажу. Не переживайте, сейчас разберем. Вот если один
признак у нас есть, мы пробегаемся по всем возможным разбеяниям. То есть, допустим, длина стебля
2,5, длина стебля 5, длина стебля 7,5 и так далее. Что значит по всем возможным? Смотрите, по идее,
у нас с вами признак числовой непрерывный. Вот, он же у нас континуальный, непрерывный. Мы можем
любое множество значений выбрать, континуум целый. Но у нас выборка-то обучающая конечная,
правильно? Поэтому нам не имеет смысла выбирать все возможные значения. Нам максимум имеет
смысла выбирать значения, которые лежат между двумя объектами. Потому что при сдвиге вот в рамках
вот нас а, один объект, второй объект, если мы между ними подвигаем, у нас наш критерий
информативности не поменяет то, что он для подвыборки работает. У нас подвыборки такие же останутся.
Поэтому по всем вот возможным, в худшем случае, мы прибираемся. То есть, сколько у вас? 10 объектов,
соответственно, между ними у нас сколько? 9 возможных трешфлодов. На самом деле 11,
2 крайних тоже работает. Хотя нет, зачем 2 крайних нет? 9. В лучшем случае у вас это n элементов в
минус 1. На самом деле работают по подвыборкам, потому что на миллиардах объектов смысла не имеет.
Все, пока понятно? Еще вопрос есть, кроме того, что такое аш, про него будем говорить. Аша это
какая-то мера неупорядоченности. Хорошо? Классно. Ну тогда давайте как раз-таки говорим. Ашка вообще
называют критерием информативности. Information criteria, критерии информативности. И на них
зиждется вообще построение деревьев. Например, вот у нас есть какое-то простое задание по
классицировать шарики. У нас одномерная с вами выборка, то есть у нас одно число признак и два
класса. Нам нужно как-то поделить выборку так, чтобы она была наиболее похожа на упорядоченную,
на однородную в каждом из листьев. Вот здесь хорошо видно, что происходит. Мы с вами можем
пробегать, собственно, поставили трешфл здесь. И что, допустим, будем предсказывать? Давайте
предсказывать на 1, 2, 3. Наиболее часто встречающийся класс, например, чтобы минимизировать
количество ошибок. Простейший случай вот деревянный. Вообще можно предсказывать вектор
вероятности как раз к сосредним, но давайте совсем деревянное дерево возьмем. Собственно,
отделили оранжевый. Оранжевый налево, тогда там у нас 100-процентные попадания. Все остальные
направо, тут примерно 50 на 50. Ну допустим, желтый доминирует, все равно 50 процентов ошибок.
Соответственно, потом попробовали сдвинуться куда? Вот сюда. Теперь отодвигаем сюда, тогда синий
налево. Там условно 5 шестых, что это синий. Маленькая ошибка. Справа у нас желтый доминирует уже
где-то семь десятых желтых. Нормально. Можем сдвинуться сюда. Теперь у нас справа 50 на 50,
слева 50 на 50. Не очень хорошо. Можем сдвинуться сюда и так далее. Короче, по всем трешхолдам
прошлись. Выбрали оптимальное разбиение вот здесь. Желтые направо, эти налево. Здесь, соответственно,
предсказываем желтые. Здесь предсказываем, ну, наверное, синий не доминирует. Теперь опять же
по каждой подвыбраке повторяем ту же самую операцию. Пробегаемся по всем здесь, пробегаемся по всем
здесь, ищем две подвыбраки и, собственно, смотрим, насколько стало лучше. Окей? Ну и, соответственно,
вот мы с вами пока нарисовал, потом будем дальше. Вопрос, как измерять вот эту самую упорядоченность.
Способ первый деревянный. Давайте посчитаем. Вот, деревянный способ. Просто напросто количество
ошибок классификации. Ну, по сути, вы всегда предсказываете доминирующий класс, тогда вы смотрите,
сколько объектов вы классифицировали неправильно. Способ, на самом деле, плохой по множеству причин,
особенно в многоклассовой классификации. То, что может быть дерево, может быть, хорошо отделяет
условно второй-третий класс от всех остальных, и оно полезно, но при этом оно все равно ошибается
много. Но это способ, чисто чтобы вот так на словах рассказать, по факту используется вот эта парочка.
Во-первых, энтропия. Все помнят чутку энтропии. Сейчас, да, это по сути до доли этого класса,
то есть нормированная частота, если мы говорим про выборку. Вот у вас попали объекты в выборку,
под выборку. У вас есть объекты одного класса второго. Пит, соответственно, частота этого класса,
и так далее. Нормированы на общий объем. Ну вот, и соответственно, можем посчитать либо энтропию
для каждой под выборки, либо, соответственно, посчитать критерии джинни или неоднородность джинни,
и так далее. Не путать, а? Да, не-не, это неправильных, а смотрите, у вас два класса, например, то есть у вас
P0 это доля класса 0 в под выборке. P1 это доля класса 1 в под выборке. P1 плюс P2 будет давать единицу,
если у вас всего два класса. Бинарная классикация. P0 нулевой класс, P1 первый класс. Помните,
мы же с вами в каждом этом листе предсказываем константу, правильно? Мы будем предсказывать
с вами вектор вероятностей. Краски, если у нас два класса, это P0, P1. Если десять классов,
то P0, P1, P2, P3, и так далее, да по десять. Все, пока понятно? Да?
Аж от левой, аж от правой, где вы аж от всей выборки считаете?
Конечно, у вас разные, все, вы поделили пространство на две части, перерос в левый,
в правый, все же вообще независимо. Пожалуйста, еще вопрос есть?
Не-не-не, размер дерева мы пока вообще никак не трогаем от гиперпараметра. Мы хотим сделать
наиболее однородные объекты в наших листьях, однородные с точки зрения нашей цельевой
переменной. Пластикации, например, мы можем с вами считать энтропию. Так, на всякий случай,
краткий ликбез. Энтропия, скажем так, неформальное определение, мера хаоса. Чем выше энтропия,
тем больше неоднородность. Теория информации повсеместно, в принципе, энтропия Шенона или
Шенона, кто как переводит. Шенона обычно? Шенона, спасибо. Энтропия Шенона, собственно,
повсеместно используется, ввел в 1951 году, по-моему, могу ошибаться, короче, где-то вот в районе
50-х годов. И давайте тогда принесу пару вещей, скажу так, мне здесь будет видно, где у нас
заметки, вот у нас заметки. Смотрите, приведу простой пример, так как у нас пока распределение
все дискретные, у нас все очень просто. Интересно, можно это как-нибудь сделать на черном фоне?
Никто не умеет? Ладно, пускай будет на белом. Чего? Долго. Смотрите, коллеги, давайте нарисуем с
вами вот такую штуку. Вот у нас с вами есть распределение, у которого есть четыре исхода,
0, 1, 2 и 3. Тут у нас будет вероятность 100%, тут 0%, ну и тут, соответственно, тоже 0, 0. Хорошо?
Согласны, что у нас может быть случайная величина с таким распределением дискретное? Хорошо,
вот вам второе распределение, словно 50%, вот он тут у нас будет 25%, и тут будет 25%, а тут нет.
Ну, иногда бывают такие новости, знаете, тут, соответственно, 0%, опять 0, 1, 2, 3. Хорошо,
ну и вот третье распределение, соответственно, там будет, я думаю, вы уже догадались, что там
будет, там, соответственно, у нас 1, 2, 3, 4, 0, 1, 2, 3, по 25% везде. Смотрите, мы с вами вытаскиваем,
собственно, вот у нас три распределения, допустим, у вас чан с шариками. Распределение шариков,
частота шариков разных цветов в одном чане, все красные, во втором есть красные, синие, зеленые,
нет черных, в третьем все поровну. Логично? Вот, собственно, вопрос, насколько хаотичен будет
наш выбор шарика из чана? Если у нас распределение вот 100% шариков красные, у нас все шарики,
которые мы вынем будут какие? Одинаковые. У нас мера хаоса, у нас нет никакого хаоса,
мы детерминированно получаем шарик красного цвета. Все, энтропия, на самом деле, здесь будет
наименьшее, его, кстати, можете проверить, собственно, минус P, лог P, на самом деле можете
посчитать, где здесь минимум достигается для любого распределения и где максимум достигается. А, что?
Ну, окей, давайте здесь, скажем так, будет плюс эпсилон, бесконечно мало какое-то число, чтобы у
вас логарифм не сломался, логарифм от нуля немножко не работает. Ну, если у вас логарифм P, где P0,
берите тогда предел какой-нибудь, не знаю, или смотрите что-нибудь, короче, у вас эта штука
улетает тогда куда? Вот, можем просто определить логарифм нуля нулем, потому что у вас получается,
что вот эта штука, какое число, это 0, 0 на какое-то приближение, все равно дает 0. Просто иначе
он уйдет у вас в минус бесконечности и будет нам не счастлив. Ну вот, предел все равно нулевой,
все правильно. Ну, просто народ к пятому курсу не любит считать предел, особенно по лопиталю,
а любит как бы, ну вот взяли приблизительно каким-то P плюс эпсилон, это констант,
и констант на 0, 0 всего, молодцы. Хорошо, короче, понятно, здесь у нас соответственно шанс уже
вытащить объекты другого класса выше, но при этом, допустим, черных все равно нет, правильно? Здесь
у нас все максимально рандомно, мы вытаскиваем какой-то шарик, мы ничего не можем сказать про
наш исход, правильно? Грубо говоря, вот вам энтропия на пальцах. Чем выше у вас энтропия,
тем больше хаос у вас будет в исходных экспериментах. По факту, энтропия тоже, ну,
она сильно связана с тем, сколько понадобится, можем двигать камеру,
сколько понадобится Bit, ш receive, чтобы закодировать ваш сигнал. Чем выше у сигнала
энтропия, тем больше вам надо информ Presidential In position, чтобы закодировать. Чем более он
однородный, тем меньше вам надо информации, чтобы его кодировать. Логично? Понятно, нет? Я так
понимаю, придется вам отдельно рассказать про cross-entropy, потому что по сути Log loss это
Cross-entropy, но судя по вопросам про энтропию это вызывает некоторые вопросы. Ничего, починим.
Хорошо, короче, энтропия, мера неупорядоченности. И давайте с вами на неё краски внимательно
посмотрим, краски на примере этого. Слушайте, коллеги, можно я тогда на будущие занятия,
наверное, попрошу это, вот у нас там стоит вот этот столб с этим прожектором, я, наверное, попрошу
потом от него это, чуть подальше, наверное, сидеть вот на этом ряду, потому что этот шуток постоянно
болтается, мне кажется, у вас в голове уже скоро всё закурится, нет? У вас нет такого? Просто я вижу,
что он оболтается, у меня ощущение, вот как я в машине с ноутбуком еду, у меня немножко в глазах
хребит. Ладно, если вам удобно, то up to you. Хорошо, короче, вот давайте энтропии разберёмся,
собственно, считаю, делим, на самом деле это не просто так разделение, это разделение оптимальное
энтропии, почему, смотрите, правая подвыборка, почти вся оранжевая, получается, энтропия у неё
очень сильно просела в этот момент. Левая подвыборка, всё равно синие доминирует оранжевое,
опять энтропия стала меньше, она более упорядочная. Далее, нам выгодно разбить на самом деле левую вот
так, получается, опять все оранжевые, здесь синий, жёлтый, дальше выгодно вот этот разбить,
ну потому что здесь оранжевых больше, мера просто-напросто больше, потом это опять разбиваем пополам,
всё, мы все дерево побили, теперь у нас с каждым листей энтропия какая? Чему равна? Нулю, всё
правильно. Ну потому что всё, энтропия ноль, у нас нет никакого хаоса, у нас всё однородно. И,
соответственно, вот мы с вами это посчитали. В бинарном случае энтропию, точнее, в многоклассном
случае, в принципе, энтропия это вот ПК, лог ПК, на всякий случай тут могут сразу задать вопрос,
часто задают, а по какому основанию? Ну, ответ, собственно, в принципе, все логарифмы похожи друг
на друга с точностью до константа, ровно поэтому тут торчит константа. В принципе, обычно считают
по основанию два, иногда по натуральному логарифму, в целом, если вы не занимаетесь обработкой
сигналов цифровой, то по барабану, потому что энтропию вы будете сравнивать между собой для
разных классов, для разных, точнее, простите, не классов, а выборок. Вот что важно. А по какому
основанию? Он с точностью до константа и вообще неважно. Обычно берут те, которые реализованы
хорошо в компе, ну, например, там либо двойку, либо ешку, ту, что удобнее, десятку, по-моему,
реже берут, но я могу ошибаться, я от этого всё-таки несколько подальше. Ну и на всякий случай,
я надеюсь, понятно, что для бинарного случая можно переписать вот в таком виде. Тобственно,
ничего вам не напоминает вот эта запись. Лог-лоз, там все дела. Помните, у нас там было
P лог-q, минус 1 минус P лог-q, лог-1 минус q. Поэтому она и называется кросс натуропия. Тут у нас одно
распределение, и мы смотрим, насколько правдоподобен логариф, вероятно, данного распределения
по нему самому. В кросс натуропии мы смотрим на правдоподобие предсказанного распределения по
истному распределению, только и всего. А это предусловие «да», это бинарный случай, в общем
случае вот у вас такая сумма. Хорошо. Вероятность положительного класса. Но если у вас дерево,
то соответственно вероятность каждого класса будет соответствовать чему. Доли данного класса в
том листе, куда он попал. Например, у нас в листе 6 к 1, тогда вероятность синих будет 6 седьмых,
вероятность желтых будет одна седьмая. По всем листям считаем, да. Но это итоговый. А при
построении, собственно, каждый лист мы бьем пополам, до тех пор, пока не выполнено будет одно из
условий. Потому что у нас, когда дерево строится, вы видите, у нас дерево может строиться либо до
упора, когда смысла больше нет, но представьте себе, что у вас достаточно неоднородные данные,
и у вас там миллиарды объектов. Бить дерево до упора, может быть, там, не знаю, у них, конечно,
глубина лагериста будет от числа этих объектов, но все равно он может быть очень глубоким. Как
правило, ставят максимальную глубину дерева, не более 50 и все. 50 глубже не пойдем. Или другое
дело, говорят, минимальный прирост информативности. То есть, например, насколько минимум должна упасть
на тропии при разбиении, чтобы мы приняли это разбиение. Ну, словно, скорее всего, вот такие
случаи просто-напросто не рассматриваются в реальных деревьях, потому что там прирост на тропии
мизерный относительно общей выборки, а надо еще одну ноду ставить, еще одно биение ставить.
Это просто-напросто не выгодит. Вот. Ну, по сути, у нас такой trade-off. Мы за точность покупаем скорость.
Не, а между нулем и двоечкой, собственно, у нас с вами жадная оптимизация, мы говорим,
наибольшей прирост информативности должен быть, то есть наибольшее падение тропии. Да, если бы мы
шли до максимальной глубины, мы могли бы по-разному на самом деле делить, но в реальной жизни мы не
можем себе позволить строить деревья до упора, и вторая проблема на самом деле есть. Чем глубже у
вас дерево, бинга, тем больше у вас дерево переобучается, причем дерево это делает вообще в лоб. Дерево,
по сути, просто делит вам ваше пространство признаковое на какие-то кусочки, в котором говорит,
здесь то, здесь это. В худшем случае дерево тупо запомнит для каждого объекта его кусочек пространства,
скажут, там его метка класса или его целевая переменная, и все, у вас ошибка дерева ноль. Кроме
случая, когда у вас у выборки есть эти, как их называют, объекты разных классов с абсолютно
одинаковыми этими координатами. Но если у вас объекты разные, одинаковые, а метка класса разная,
тогда дерево не может предсказать константу, которая удовлетворяет обоих. Хорошо, и давайте теперь
посмотрим на, собственно, неоднородность джинни. Пожалуйста, не путайте из экономики с критерием джинни,
кажется, я уже даже один раз и говорил критерии джинни, короче, джинни, все. Это не то, что вот в
экономике считают там индекс джинни, есть там индекс джинни, это не про нас. Я на всякий случай,
опять же, рекомендую, есть статья от Яконова в его блоге «Анализ малых данных» про критерии джинни и
что не надо упудать с индексом джинни. Смотрите, на всякий случай тут проверим ваше интуитивное
понимание теории вероятности. Хотя, говорил мне мой преподаватель, что интуитивного понимания
краски у человека нет. Теория вера, поэтому его сложно учить. Я думаю, кто-то понял, кто был
мой преподаватель. Клятчий вопрос. Итак, смотрите, посмотрите внимательно, пожалуйста, сюда,
что такое неоднородность джинни? Вот посмотрите внимательно, подумайте, что на самом деле она нам
показывает? Дисперсию. Ну, смотрите, вот P плюс — это вероятность того, что у вас объект класса плюс,
правильно? Бинго. Если мы берем два объекта из нашей подвыборки, какова вероятность того,
что они окажутся что? Одного класса или разных классов? Разных классов. Абсолютно верно. Смотрите,
мы смотрим, вот это вероятность того, что два объекта одного класса просумированы по всем классам.
Соответственно, если мы хотим получить два объекта разных классов, один минус вероятности того,
что они одного класса. Получается, чем более у нас однородная выборка, тем больше вероятности того,
что мы пару объектов возьмем одного класса. Логично. На самом деле на практике индекс джинни и индекс,
вот я опять оговариваюсь, критерий джинни, простите, вырезать, вряд ли вырезать будем,
но короче, это оговорка. Критерий джинни — это вот. На самом деле, если нарисовать их для бинарного
класса, для бинарных классов, смотрите, у нас по оси XP плюс, а тут соответственно наша самая
энтропия или джинни. Можем видеть, что в принципе джинни, вот он синенький, энтропия, вот она
фиолетовенькая. Если джинни умножить на два, то энтропия с джинни вообще говоря ведутся очень
похоже. То есть, грубо говоря, чем более у нас неоднородная выборка, тем больше они штрафуют.
Максимум, логично, все достигают в 0,5 для бинарной классикации, но потому что максимальная энтропия
для бинарного случая будет в краске 0,5-0,5. И в принципе разницы между ними на практике особо нет,
в последнее время вроде как больше использовали джинни, мне лично больше импонирует энтропия,
просто то, что энтропия у нас почти везде сидит, плюс она смотрит на распределение целиком,
а джинни говорит только, почему про выбор двух объектов одного класса, а почему не три объекта,
почему не 10, по-хорошему там взять 2, 3, 4, 5, 10 и вот это все просуммировать, но тогда ряд долго
считать и не очень удобно. Какую идею? Смотрите, энтропия смотрит на неоднородное распределение
в целом, то есть это, грубо говоря, сколько информации надо, чтобы декодировать данные за это
распределение. Джинни это вероятность взять пару объектов одного класса, точнее, наоборот,
вероятность, что взяв пару объектов, они будут разного класса. А почему мы не берем еще тройку
объектов или четверку или пятерку? А вот это любопытно.
Да, 1 минус норм это вектор, ну, кстати, да, но, правда, без корня. Любопытно слушать, не думал об этом.
Да, да, да, согласен.
Какой график? А, вот это, вот это. А missed class, это, собственно, первый самый критерий,
который мы с вами обсуждали, просто ошибка. То есть, смотрите, она линейно
растет, собственно, пока у нас один класс доминирует, но у него вероятности меньше и меньше,
она просто линейно растет. Потом опять линейно падает, потому что другой класс стал максимум.
Собственно, глядя вот сюда, можно видеть, что у нас мисс классификация штрафует слишком мало за
малые ошибки. Видите, он очень медленно растет относительно и джинни энтропии. И джинни энтропии
видят, что у нас вероятность сразу стала, скажем так, не нулевой для второго класса, а надо,
давай за это бить указкой, потому что не надо так делать. Мисс классификация такой, ну ладно, что-то там.
Вот. Так, тут еще вопрос есть?
Ну, в принципе-то вы можете это перенормировать в любое, но как бы, да?
Ну, кстати, возможно, поэтому его в последних деревьях используют. Вот, смотрите, есть различные
реализации, тут где-то даже, наверное, будет. Бла-бла-бла, бла-бла-бла. Нет, похоже, не будет. Возможно,
я вытяну этот слайд. Короче, есть различные алгоритмы построения деревьев. ID3, это кто-то там
4-5, карт, вот сам последний classification and regression trees, который в эскалерне сидит, они все были с разными,
условно, способами построения, но по факту сейчас, ну вот дерево вы напишете в домашке, если захотите,
но по факту, конечно, выгоднее использовать как и библиотеки, которые вам векторное умножение
под капотом используют, так и деревья строят, потому что они с кучей всяких доработок,
имплементации и так далее работают быстро. Причем, желательно, не эскалерна, а какой-нибудь
cutboost, xgboost или lightgbm, они под капотом используют краски, умеют строить деревья. По факту, там гораздо лучше, да.
Потому что, смотрите, у вас необходимость построить хороший классикатор, то есть чтобы он максимально,
как сказать, максимально точно предсказывал вам метки классов. Грубо говоря, у нас классикатор
тем лучше, чем меньше вероятность совершать ошибку. То есть получается, что у нас джинни и энтропия
штрафуют слабо в районе там 0,6 или 0,5, это вероятность правильного класса, то что и так и так у вас
какой-то рандом, но штраф тем сильнее, чем ближе вы к нулю, то есть либо у вас вообще однородная
выборка, либо она неоднородная, и сразу за это получается штраф модели. А мисс классикиш, ну вообще
все равно, высовка вероятность низкая, то есть он никак не разделяет точность, что она почти
стопроцентная, что она почти 0,5. Да, и чего? У вас не может быть, если у вас единицы минус
п квадрат, еще раз, у вас сумма по обоим классам, тогда у вас будет 1 минус 0 и 1 минус 1, получается 0.
Так, хорошо, коллеги, ну и смотрите, в чем еще на самом деле плюс вот этого всего дела? В том, что в
отличие от всех пока что предыдущих алгоритмов, кроме, наверное, КННа, решающие деревья вообще в лед
и в лоб обобщаются на регрессию. Нам единственное, что надо сделать, это другой критерий информативности,
ввести и вместо энтропии считать, например, что? Вот вы для выборки предсказываете константу,
что можно считать? Ну МСЕ, а что по факту будет МСЕ? Точнее, какая оценка оптимальна с точки
зрения МСЕ? У вас есть выборка, вы хотите предсказать значение для нее константой, минимизируя МСЕ?
Средняя. Ну по сути, тогда вы будете просто, например, дисперсию считать относительно среднего,
а если вы МАЕ оптимизируете, уже вопрос со звездочкой, медиану, бинго. Ну те, у кого тяровер был,
видимо, это уже понимают. То есть если мы с вами минимизируем, например, вот эту величину,
отклонение от некоторой константы, то как раз таки оптимальное с точки зрения, это по сути МЛЕ,
максимум лайк и худ скимаш, оценка максимального правдоподобия на вот эту цешку, это средняя. Если
вы посчитаете точно также оценку максимального правдоподобия на МЛЕ, то у вас получится медиана
и так далее. Если хотите, кстати, можете ко мне потом пристать, можем с вами это попробовать даже
вывести где-нибудь прямо после-после. Ну или можете сами повыводить, я вам могу просто,
у меня даже на планшете это все написано, я могу просто сейчас скинуть потом, если хотите. Я там
тоже писал на комнате семинаров. Окей, короче, плюс дерево в том, что вы его один раз написали,
вы туда вставили критерии для регрессии, работает регрессия, критерии для классификации,
работает классификация. И плюс в чем? Он вам строит нелинейную разделяющую поверхность или
нелинейную аппроксимацию, при этом, по сути, дешево. Ну как дешево? У вас с одной стороны вроде
как дерево строить долго, но с другой стороны, во-первых, жадные алгоритмы работают неплохо,
во-вторых, деревья не такие глубокие, там глубина обычно, ну словно до 50 до 70, это не так сложно.
И дерево замечательно в двух апостасях. Во-первых, его абсолютно замечательно применять,
вам, по сути, на применение просто кучу ифов надо провести и все. Это работает быстро. Для каждого
объекта вы просто, по сути, кидаете объект и у нас куда-то падает. И второе, дерево абсолютно
феноменально объяснять, как работает. Вы по дереву можете четко сказать, куда, кто попал и почему.
У вас все вот эти вот разделения, они вам всем известны. Они очень хорошо интерпретируются,
именно поэтому их любят во всяких банках, там страховых и так далее. Но у дерева есть минус,
который мы с вами уже вроде как озвучили. Дерево переобучается просто как нехорошее дерево,
как буратино, я не знаю, которое много врет. Как мы работаем на практике, давайте еще раз. Вот у
нас есть с вами выборка, одномерная или многомерная, не важно. Мы просто методично,
занудно берем каждый признак, для него выбираем все возможные трешхолды или между какими-то
подгруппами по этому признаку, если там объекты однородные, грубо говоря, или рандомно их
накидываем, потому что если у вас там условно выборка в 10 миллионов объектов дороговато,
как ты их накидываете. По всем трешхолдам считайте разбийния направо-налево, считайте критерии
информативности. Для всех трешхолдов, да, просто перебираем в жадную. Почему на самом деле недолго,
потому что вы, во-первых, можете предыдущий статистик предыдущего шага на самом деле
переиспользовать во многом, и так далее. Я и говорю, дерево строится жадно, почему?
Потому что даже если вы на него посмотрите, это кусочная постоянная функция. О каких гридентах
может идти речь? У вас там нет гридентов, у нас гридент всегда 0 либо неопределен в точке разрыва, все.
Да, можно, собственно, я говорю, на больших выборках обычно или упорядчивают по подвыборкам,
какой-то предпочет делают, или просто рандомно накидывают, да и все, потому что вам надо как-то
получить эти самые трешхолды. Вы каждый по отдельности перебираете. Смотрите, в этом еще одна фишка
дерева. Все признаки разморуются отдельно, вообще независимо друг от друга. Вы взяли один признак,
по нему у вас объект упорядочно. Потом вы взяли второй признак, а потом вы выбираете тот признак и
то разбиение, которое оптимально с точки зрения вот этого самого критерия, который вы выбрали. То есть
дерево действует жадно. Вот у вас разбиение, вот у вас была подвыборка, вы перебрали все возможные
признаки и все трешхолды для каждого из признаков и выбрали ту пару трешхолд-признак, которые
оптимальны с точки зрения того, что у вас прирост информативности максимальный или меньше всего
стала энтропия после этого. Теперь вы попали в две новые подвыборки, полностью делаете то же самое.
Все, рекурсии, пока не уйдете до конца. Вот, с регрессией абсолютно все то же самое. Ну и смотрите,
собственно, основная беда деревьев, как я уже сказал, к чем? Переобучаются очень сильно, но благо,
так пруйник давайте потом тогда скажу, но благо есть у деревьев, помимо того, что они слабые,
у них есть еще и сила. Их слабость, можно обратить краски в силу. И тут опять вопрос,
кто знаком с процедурой бутстрапа? Ну вот сейчас мы к нему подойдем. Понял, никто не знаком. Давайте
тогда опять же на пальцах вам объясню. Но для начала придется вам показать простенькую вещь.
Смотрите, давайте введем понятие бутстрапированные выбраки. Что такое бутстрапированные выбрака? Вот у
нас есть выбрака из к-элементов. Давайте на доску чуть-чуть перейдем. Вот у нас есть в
выбраке это у нас х. Пусть у х размер n. Внимание, вопрос, как мы с вами можем сгенерировать из х
новую выбраку размера n так, чтобы она полностью не дублировала х, но сохраняла плюс-минус
зависимости из х? Звучит как бред, правда? Бинго, давайте построим новые выбраки, собственно,
тоже размером n, хоть и не поменьше x1, x2, x3 и так далее, xk. Вот они у нас, они тоже все размера n,
но при этом мы их будем строить как? Мы будем выбирать объекты с возвратом или выброс
повторений. То есть грубо говоря, мы каждый раз, вот n раз, выбираем из х случайный элемент и все,
и кладем его обратно. Тогда у нас получится, если мы такое четыре раза приведем, у нас получатся
четыре выбраки, которые с одной стороны очень похожи на оригинальный х, то что все объекты в
них содержатся и в оригинальном х, правильно? С другой стороны, они с ним полностью не совпадают,
потому что какие-то объекты случайно попали туда n раз, несколько раз точнее, n у нас уже
зарезервировано, какие-то не попали вовсе. Да, все эти выбраки,
вот четыре строем независимо, то есть что мы делаем? Мы берем новую выбраку и n раз просто, по сути,
можете себе представить каким образом. У вас объекты лежат где-то, вы в выбраку себе просто их
индекса накидываете, то есть в этой выбраке у меня будут объекты 1, 1, 1, 7, 4, 4, 4, 28, 25, 32 и так далее.
То есть, соответственно, у вас объекты могут быть одного класса, могут быть разных классов,
один и тот же объект может вообще не попасть и так далее. Нет, почему? Каждая выбрака размером n,
точно так же, чтобы у вас было n различных выборок, но при этом такого же объема. Зачем? Смотрите,
во-первых, общие слова. Бусттрап это очень сильный статический метод, чтобы получать какие-то
оценки. Например, вы можете посчитать какую-нибудь оценку, я не знаю, на ваш параметр по каждой из
бусттрапированных выборок и получить целое распределение на оценку зависимости от того,
что у вас туда попало. То, что вы за счет случайности краски выбора объекта, грубо говоря, вы можете
какой-то выбор случайно не взять или что-то еще. Потому что если мы предполагаем, что у нас
оригинальные данные пришли из какого-то распределения, то бустстрапированная выборка
по-хорошему тоже должна оттуда же идти, все объекты оттуда же. Вот. Еще раз, все, давайте я нарисую тогда
1. Вот у меня есть выборка х. Я выбираю вот сюда, просто random choice и получаю выборку х с крышкой.
То есть я n раз выбираю случайно объект из х. Да, практически. То есть я по сути случайно,
размер х с крышкой n. Вы каждый раз выбираете случайно. Да, выброс повторения. Вы по сути объект
взяли и сказали так, красный будет здесь, но вы его по сути обратно вернули. Вы можете каждый раз
случайно, с вероятностью условно, получается, если выборка размера n, то 1 делить на n в степени n,
это вот вероятность вообще, что у вас выборка будет одного объекта. Ну типа она конечно маленькая,
но и так далее. Ладно, это на самом деле. Да, по умолчанию у вас размер будет строфирован в
выборке такой же, как и оригинальный. По факту это такой же гиперпараметр, вы можете его сделать
меньше. Не-не-не, количество может быть любым, то есть это вы выбираете для ПК штук. Собственно,
зачем нам это надо? Смотрите, теперь внимание на флайды. Давайте я сюда вот отойду, чтобы у меня
было хотя бы как-то на записи видно. Собственно, пусть у нас есть выборка краски из-за объектов.
Собственно, каждый раз выбираем объект с возвращением из X и получаем n различных дат сетов. Вот,
ну тут разве что нотация другая, тут размер выборки был n, тут наоборот m. Короче, не запутаться,
пожалуйста. Соответственно, в чем прикол? Давайте сюда посмотрим. Что это у нас такое? Мат ожидания
B житой от X минус Y от X. B житой это как раз-таки то. Это житая модель, обученная на житой
бутстрапированной выборке. Согласны? Соответственно, минус Y от X вот такая штука. Окей? Тогда у нас
средняя ошибка всех n моделей. Это что такое? Все ошибки просуббировали и поделили на n. Согласны?
Ну, вроде пока все логично, никакой магии, скажем так, не произошло. Верно? Тут вопросов нет?
Y от X это истинное значение таргета. Нет, это сигнал, то есть это наша выборка, по сути. Для каждого
X мы знаем ответ. B житой это модель обученная на житой бутстрапированной выборке, которая
что-то там предсказала. Все. А теперь начинается магия. Теперь давайте сделаем два, собственно,
предположения. Во-первых, что у нас все ошибки не смещенные, то есть ошибка на каждой из под
выборок не смещенная, то есть ее от ожидания равно нулю. Хорошо? Нормальное предположение,
потому что если у нас ошибка смещенная, что-то у нас не то с моделью. Второе, что ошибки независимы
друг от друга. Точнее, что у них корреляция нулевая. Окей? Договорились? А теперь следить за магией.
Ну, давайте тогда посчитаем, собственно, вот у нас будет модель. А от X это что? Это средняя из
N моделей. То есть у нас была N модель из бутстрапированных выборок. Давайте тогда посчитаем. Тогда у нас
ошибка для среднего. Это мат ожидания вот этой штуковины. Согласны? Тут как бы по математике давайте
пройдемся. Мат ожидания или что-то то же самое. 1 делить на N е житой от X, потому что это ошибка
каждой из моделей. Это все у нас в квадрате. Или мы можем N из квадрата вытащить. Получается 1 делить
на N квадрат вот от этой штуки. Согласны? Умеем как бы квадрат-суммы расписывать вот таким
образом. Бином-ньютон, там все дела. Хорошо? И это мы с вами только что сказали, что оно равно 0,
потому что у нас корреляция, ошибка нулевая. Остается вот эта часть. И получается это 1 делить
на N квадрат. А это что такое? Это ошибка N моделей. Ну и все. Получается 1 делить на N, ошибка одной
модели. Масс ошибка с вами упала в N раз только что. Вот ошибка была для всех этих моделей средняя.
1 делить на N, вот эта штука. А здесь 1 делить на N квадрат. Заметили? Чего о любой подвыборке?
E1 это смотрите нет, у вас 10 подвыборок условно. Вы обучили 10 моделей. E1 это ошибка на каких-то
там данных. Модель обычно на 1 подвыборке. Обучены на 1, а тестируем вообще на отложенных данных.
E2 обучены на 2, тестируем на отложенных. Короче, внимание, вопрос, где здесь обман? Потому что у
нас ошибка только что в N раз упала. Зачем нам вообще все остальные методы машинного обучения?
Делаем кучу. Предположение правильное. На самом деле вот эта вот штука, что у нас ошибки не смещенные,
бог с ними еще не скоррелированы, это наши мечты по факту такого не будет никогда. По факту, что мы
с вами сказали, давайте у нас будет множество выборок, которые вообще независимы друг от друга,
из одного этого распределения. Тогда мы можем с вами с минимальной ошибкой все это описать.
Логично у нас чем больше данных, тем меньше у нас будет ошибка. По сути мы замощаем с вами
все пространство какими-то объектами. Можем себе позволить. Но тем не менее это нас привело к тому,
что был изобретен метод собственно беггинга. Беггинг это что? Это краски bootstrap aggregating,
так и расшифровывается. И он направлен на что? На то, чтобы полечить основную беду наших
деревьев. Потому что смотрите, у нас вот эта ошибка состоит из двух частей. У нас вот эта
часть это ошибка каждого алгоритма, правильно? А это то, как они похожи друг на друга. Ошибку каждого
алгоритма мы вряд ли как-то сможем исправить. Его построили и все, он готов. А вот сделать
алгоритмом максимально непохожими друг на друга позволит нам второй члену меньше. И
пускай он будет, скажем так, не нулевой, он все равно будет существенно меньше, чем если бы
деревья были одинаковые, например. Если б кориация между ними была сто процентов. Короче, какое
предложение? Давайте я вам сначала попытаюсь это на пальцах тогда объяснить, чтобы вы немножко
оживились от мотанка, это страшно. А потом соответственно повторю. Смотрите, в чем суть? У вас
каждое дерево может быть подстроено, правильно? Вот вашу под выборку. Теперь наша цель сделать
модель, которая, собственно, модели так, чтобы у них ошибки были максимально непохожи друг на
друга, правильно? Грубо говоря, модели должны быть непохожи друг на друга, значит у них
предсказания разные, значит у них ошибки разные, верно? Тогда у нас вот этот результат будет плюс-минус
достигаться. Ну так давайте тогда возьмем и обернем слабость деревьев, а именно их способность
переобучаться нам во благо. Давайте возьмем нашу обучающую выборку, набудстрапируем из нее несколько,
и на каждый из будстрапированных выборок переобучим наше дерево. Что тогда получается? У нас N деревьев,
каждый переобучается под что-то свое, а потом мы их усредняем. Теперь, собственно, внимание, что
происходит? У нас с вами краткие. Происходит ровно вот эта самая история, только здесь у нас вот этот
член не в ноль обращается, а он просто меньше, чем он был бы, потому что у вас теперь ошибки между
собой не полностью скоррелированы, а значит у вас все равно ошибка упадет не в N раз, но хотя бы в
два-три раза совершенно спокойно может упасть. Если говорить на пальцах, что происходит? Здесь заходит
классная вот эта история, всегда я, наверное, рассказываю, называется мудрой столпы. Представь себе
повторальная картинка, вот средневековая, да, он самый, средневековая деревушка, ну вот как в фильмах,
знаете, там все красиво, птички бают, какой-то тамада выходит на сцену, говорит, уважаемые
крестьяне, да, не эй-холопы, что пришли, уважаемые крестьяне, вот бык, кто из вас угадает его вес,
точность туда фунта, понятное дело, где-то там, в Британии условно работает, тот получит этого быка,
ну а бык это же вообще подъем хозяйства, все круто, а соответственно ходят ассистенты и записывают,
что там человек сказал. Ну в итоге записали, посмотрели и средний, усреднив все предсказания,
получилось, что там условно вес быка 1148 фунтов, из балды говорю, не знаю сколько бык висит,
а предсказанная средняя 1147, короче, почти с точки до фунта угадали, хотя каждый оценивал на глаз,
ну понятное дело, быка никому не дали, потому что типа никто не угадал, средниковая история,
все дела, быка вообще жалко, математика, конечно, штука хорошая, ладно, это все байки. Собственно,
история в чем? Смотрите, у вас получается куча различных оценок от каждого человека по
отдельности была записана. Каждый человек на глаз умеет каким-то образом оценивать вес,
ну вы можете на глаз прикинуть, там сколько человек рядом висит, по этому все равно будет
плюс-минус 10 процентов, скорее всего, но у нас глаз привык, и условно, я понимаю, что здесь
примерно 250 грамм, а здесь, например, где-то 180, но это я на ощупь не на глаз, но визуально они
тоже, один меньше другого. Поэтому что мы с вами делаем? У нас каждый, грубо говоря, человек
предсказывает какую-то оценку, правильно? Но они все обусловлены на одного и того же быка,
и мы можем вам сказать что что, что у человека есть какая-то случайная ошибка, он либо завышает
оценку либо занижает, окей? Получается, когда мы с вами много предсказаний независимых, что самое
важное, они между собой не совещались, они просто посмотрели на бока и их записали, они даже ничего
голоса не говорили, много не зависимых предсказаний записали, у среднили, у нас с вами люди-то, в
среднем, скорее всего случайно, завышают или занижают одинаково часто, нет систематической
ошибки. А потому в среднем у нас ошибка расслоптана, ноль. Потому что у нас предсказание независимое
друг от друга, это очень важно. Они друг на другу не влияют, они завышают и
занижают одинаково. Вероятно, в итоге мы получаем более точную оценку. Согласны?
Вот вам мудрость толпы. А теперь, собственно, вопрос для самопроверки
второй. Называется нос императора. Древняя какая-то восточная страна, и император
очень сильно комплексовал, что у него длинный нос. Его, соответственно, советник
решил помочь и отправил гонцов во все края империи, чтобы те опросили граждан
и спросили у них, какой длины у императора нос. Вот гонцы все вернулись, в итоге
усреднили, что у них получилось, и получилась абсолютно среднестатистическая
длина носа в этой самой империи. Император обрадовался и больше не
комплексовал ходить на люди. А нос у него, правда, был длинный. Внимание, вопрос,
почему не работает? Что случилось? Откуда оно взялось? Вариант да. Бинго. Это какие-то
там древние века, там императора как бы видели, разве что члены гвардии и кто-нибудь
там приближенный к императорской семье. У них каждое предсказание не было
основано на X. Они ничего не знали про X. Они просто прикидывали, какая длина нос
у императора. Они по сути знали примерно длину носа у своего окружения и говорили,
сколько в среднем. Вот вам средний получил, длина в среднем. Поэтому вся эта история работает,
когда если у вас оценки вашего алгоритма все еще информативны, то есть если у вас есть оценки,
которые предсказывают что-то корректно, но с большой дисперсией, вы можете их усреднить и
получить хороший результат. Если же у вас алгоритм просто какой-то рандом полный предсказывает,
то есть его даже в среднем оценка плохая, то ничего не поможет. Мы на самом деле на следующих
занятиях с вами поговорим про смещение и дисперсию, точнее про bias и variance, если говорить таким
более корректным языком, смещение и разброс. И краски у деревьев, в чем суть? У деревьев смещение
очень маленькое. То есть у них bias, но составляющая составная часть ошибки, она говорит, что на каждой
подвыборке дерево очень хорошо опсилимирует свою подвыборку. Но при этом для разных подвыборок
предсказания могут сильно разниться. То, что оно переобучено. Усреднение позволяет вам краски от
дисперсии, variance, разброса избавиться частично. На самом деле тут даже видно почему, потому что
у вас разброс дисперсии, дисперсия у вас будет в квадрате. При усреднении у вас n в квадрате,
соответственно, n в квадрате вытаскивается, одна n-ка остается. Это лучше. Но это мы с вами более
глубоко разберем. Ну смотрите, здесь я, к сожалению, могу отослать к литературе по бутстрапу. Напишите
в чате, я вам прям скину, что почитать, почему бутстрап выгоднее, чем просто разбиение. Потому что
по сути вы себе генерируете новые подвыборки. Они все равно статистически обладают почти теми же
самыми свойствами. Если вы побьете на маленькие подвыборки, то с одной стороны вы правы, с другой
стороны у вас просто выборка будет меньше, больше переобучений и так далее. Хотя по факту, конечно,
у вас больше информации не становится. Да не то чтобы. Но смотрите, дерево по сравнению с каким-нибудь
ядром для свм-а считать гораздо дешевле, поэтому 10 раз вы посчитаете на 10 процессоров. Возьмите и
посчитайте. Ну вот это, кстати, мы с вами можем посмотреть, в том числе в домашке.
Не, смотрите, обычно деревья строят там десятки, сотни, режут тысячи, больше не строят обычно.
Да, рандом. А, не, погодите, это бутстрап. Во, все. Не, все. Лес так и строится. Смотрите,
я ж совсем забыл. Просто на бутстрапированных выборках-то у нас деревья все равно будут
сильно похожи с какого-то момента, потому что у нас все равно в среднем распределение сохраняется.
Я ж не договорил. Собственно, вопрос, как нам сделать бутстрап, точнее беггинг над деревьями,
более эффективным, а нам надо сделать деревья более непохожими? Давайте, мы каждый раз, по сути,
выбираем случайное множество объектов. Ну вот, бутстрапируем выборку, на нем учим. Что мы еще
можем сделать случайно? Давайте. Смотрите, что произошло. Это средняя ошибка для десяти
моделей различных. Здесь мы взяли модель, которая усредняет предыдущие десять предсказания,
то есть усредняем, поэтому оно на стуре и сидит. Ага. Ну так чего, ребят, что можно еще случайно
выбирать? Гиперпараметры сложно. Что еще? Границу можно разделять, да, а можно еще признаки выбирать.
Короче, люди сидели-сидели и подумали, а давайте-ка введем метод RSM, random subspace method,
метод случайных подпространств. Давайте мы каждый раз будем случайным образом выбирать
под множество признаков, именно, что случайно у вас склонны 50 признаков, из них 25 случайных
выбрали, и только над ними строите дерево. Тут два варианта. Либо вы выбираете в самом начале для
всего дерева целиком, либо вы его выбираете в каждой вершине вообще по отдельности. То есть
каждой вершине вы говорите, выбери оптимальный признак из пятого, седьмого, десятого и двадцать
пятого, и все. Тем самым деревья у вас становится, с одной стороны, вроде как слабее, но с другой
стороны, мы их глубину не ограничиваем, поэтому они все равно хорошо подстроятся. А во-вторых,
у нас с вами деревья гораздо менее похожи друг на друга, потому что они теперь на разных признаках
вообще построены. И, собственно, когда люди взяли и объединили между собой random subspace method,
метод случайных подпространств и bagging, родился random porost. По праву, один из наиболее, наверное,
универсальных и удобных алгоритмов в машинном обучении, в частности для табличных данных.
Bagging – это bootstrap aggregating, то есть вы построили n bootstrap выборок,
обучили на них модели, усреднили предсказания. Нет, bootstrap – это метод генерации выборки,
а bagging – это, по сути, механизм машинного обучения, как из этих самых выборок получить модели.
Просто в статах вы можете по bootstrap выборкам всякие статистики считать и так далее.
У вас каждая модель предсказывает вектор вероятности? Именно поэтому я говорил вначале,
всегда, пожалуйста, начинайте думать векторами вероятности. Одни метки классов, а не вас. Всё,
это как бы детский сад. Вектор вероятности можно совершенно спокойно средний. Ну и, соответственно,
получается, что random forest работает достаточно хорошо, он переобучается, но очень паршиво
переобучается на фоне остальных моделей. Ну, собственно, random forest – это что? Мы берем и,
скажем так, я скажу, наверное, каноничную версию, но мое мнение может отличаться от других. То,
что я сказал, два варианта. Либо на каждое дерево отдельная подножица признаков,
либо в каждой вершине вообще. Он в каждой вершине выбирает случайное подножицу признаков,
и вот на bootstrap выбирает оптимальный threshold. То есть у вас деревья теперь случайные и по
подвыборкам и по признакам. За счет этого они меньше похожи друг на друга, за счет этого у вас
достигается большее падение вот здесь. И плюс здесь мы перестаем бояться переобучения, в каком
смысле, то, что да, пожалуйста, пускай переобучается, мы потом деревью усредним,
переобучение исчезнет. Ну, точнее, его эффект снижется в n раз, где n число деревьев. Есть,
конечно, но если вы начнете слишком много деревьев строить, например, у вас там выборка размером
тысячи и вы 500 деревьев построили, то вы заметите, что у вас куча деревьев всё равно похожа друг на
другу. Потому что у вас случайно постоянно выбираются похожие траектории и так далее.
Поэтому random forest переобучить можно, надо сильно постараться. Это не панацея, но именно поэтому
он является классным деполтным алгоритмом. Он агрегирует на собой деревья, а поэтому он что? Он
строит нелинейную разделяющую поверхность или нелинейную зависимость. Во-вторых, он достаточно
устойчив к переобучению. И третье, я думаю, кто-то из вас уже заметил, давайте я на это дело на
всякий случай обращу внимание. Тут пока всякие классные штуки. А как вы думаете, как деревья
работают со скоррелированными признаками? Если у нас сильная корреляция между признаками
деревьев хуже или лучше? Ну, лучше вряд ли, а если сильная корреляция? Не хуже. Бинго! У нас же
дерево при построении смотрит на каждый признак по отдельности, правильно? Поэтому дереву абсолютно
всё равно на то, что они там скоррелированы. Максимум, что можно сделать, если вы просто вот в худший
случай, у вас два признака, а потом вы берёте вторую и сто раз его дублируете. Дерево просто
будет медленнее считаться, потому что ему просто будет дольше это всё считать. Всё. В остальном ему по
барабану, оно не сломается. Никаких огромных эффициентов не появится и так далее. В смысле порядок.
Вообще неважно. Если у вас у Н признаков падение одинаковое, ну по-хорошему выбирайте случайный,
вот и всё. Не гарантируем. В среднем просто. Мы условно в среднем выбираем какое-то подможество
признаков. Как правило, если условно в каждой вершине мы выбираем условно одну-вторую признаков,
то скорее всего мы должны это сделать там N раз, чтобы покрыть все. Да?
Не-не-не-не-не. Погодите. Мы ещё раз. Дерево строится как? Множество признаков, перебираются все
возможные признаки, для каждого признака все возможные трешхолды. Мы всегда для одного признака
в ноде что-то выбираем. А какой признак, какой трешхолд мы выбираем уже одном образом. Теперь мы
добавили две случайности. Первая, у нас выборка буддстрапированная, то есть не исходная выборка,
а та, которую мы получили методом буддстрапа. Вторая, у вас случайное подможество признаков,
а не все, которые были в каждой ноде. Вы в каждой ноде ставите трешхолд на один признак и на N?
На один признак, но выбираете вы оптимальные не из всего множества, а из случайного подможества.
Это гиперпараметр. То есть количество, размер подможества вы сами выбираете условно. В каждой
ноде выбирай там пять признаков, из них выбирай оптимальный. Это гиперпараметр ваш.
Бинго, не случайно выбираю признак, а случайно выбираю подможество, из которого будет выбрано
оптимальный. В пределе вы можете выбирать один случайный признак, по нему тогда просто трешхолд
На самом деле такое тоже есть. Называется extremely randomized trees.
Короче, ребят, смотрите, сейчас у нас еще минут десять лекции будет. Пожалуйста, найдите себе
силы. Я понимаю, всем очень хочется написать контрольную и пойти домой. Дайте, пожалуйста,
мы закончим. Не смотрите на то, что у нас звонок звенит. Еще у нас немножко время есть. Итак,
смотрите. Во-первых, плюсов деревьев на самом деле много. То есть, а они у нас работают достаточно
хорошо на нелинейных зависимости. Они работают, когда у нас есть коррелированные признаки. Если у
нас признаки в разных шкалах, что будет с деревом? Дерево вообще по барабану, дерево их все
раздельно рассматривать по отдельности. Все наши проблемы, которые были раньше, пока что решаются.
Четвертое. Если у нас пропуски в данных, что делать с деревом? Ну, в смысле, у вас бывает данные,
вот допустим, я не знаю, у вас датчик вышел из строя в какой-то момент, молния рядом ударила,
и там нету данных на этот момент. Ничего и не известно. Линейная модель сломается, потому что
линейная модель должна вместо нан умножить на что-то. Если умножить нан на вес, вы получите
все равно нан. Нота number. Что с деревом? Сейчас я вернусь, у меня даже специальный слайд для этого есть.
Вот, пожалуйста. Тут у нас попал объект, который попал признак, который неизвестен для нашего
объекта. На inference, например. Что мы делаем? Мы можем спустить объект по левому поддереву и по
правому поддереву, и мы же знаем вес левого и правого поддерева на обучение. Сколько объектов
пошло туда, сколько сюда. С этими весами мы можем усреднить предсказание. Вот все. Дерево еще и
устойчиво к пропускам данных в разумных пределах. Получается корреляция все равно разной шкалы,
все равно пропуски в данных до определенного предела, все равно переобучается, но можно это
побить тем, что мы их посредняем. Плошная радость. Собственно, ровно поэтому на самом деле деревья и
во многом поднились во весь рост, особенно с приходом градиентного бустинга, который мы с
вами разберем на следующих занятиях, который в 2001 году был Фридманом представлен, и градиентный
бустинг немного-немало привел к тому, что, как его называют, раз-два-три-четыре-пять, не только
своем умер, но еще и нейронные сети того времени, по сути, обратно ушли отдохнуть. Нейронные сети в
конце девяностых были очень популярны, в 2001-м выходит градиентный бустинг, все говорят, блин,
оно работает быстрее, эффективнее, вообще хорошо. Зачем нам ваши нейронки? Так что умение пользоваться
деревьями сейчас вам позволит делать что? Вот на табличных данных сейчас уже нейронки есть хорошие,
которые даже лучше деревьев работают. Но по простоте использования и по скорости, а еще
интерпретируемость, например, для деревьев и для ансамблей деревьев есть замечательный теоретически
обоснованный метод оценки значимости признаков SHAP, Shapely Editive Values, это работа, лучшая работа года
по версии НИПСа 2017 года, по-моему, возможно 18-го, могу ошибаться. Короче, деревья ваши друзья,
и более того, почти на любой задаче вы можете их в качестве бейслайна использовать. Ваша копилка
бейслайнов пополнилась. Собственно, если данных мало, признаков объектов это КНН, если это
классикация, можно добавить наивный базовый классикатор. Главное, правильно приорное распределение
выбирать. Линейно-логистическая регрессия всегда влет, может какие-нибудь там признаки еще
погенерить. Дерево туда же добавляется, если вот данные нелинейные, то у вас будет явно видно,
что логистическая регрессия и линейная регрессия сильно уступают рандомпоросту, потому что он
может нелинейные границы описывать, а соответственно линейные модели не могут. Это вам звоночек. Собственно,
и последнее. У меня это еще не все про рандомпорост. Он вообще как бы обалдел в своих плюсах.
Во-первых, он наиболее универсальный, у него есть куча различных доработок. Вот про extremely
randomized forest я вам говорил. Также есть классная версия isolation forest, про него как-нибудь в следующий
раз, если захотите, можно поговорить. Если коротко, random forest может использоваться как фильтр аномалий.
Вот у вас в данных бывают какие-то аномалии, то есть объекты, которые обладают именно каким-то
очень странным признаковым описанием. Вот у вас распределение ваших данных и вот тут торчит какая-то
точка. Как это можно сделать? Смотрите, мы с вами можем строить дерево, но преследовать другую цель,
не минимизировать какую-то энтропию или еще что-то. Мы можем пытаться на каждом шаге отделить один
объект. Хорошо? Тогда что получается? Мы на каждом шаге отделяем один объект. Те объекты, которые с
краю, их просто проще отделить будет. Поэтому когда дерево строится, оно будет сначала отделять тех,
кто с краю, а потом тех, кто ближе к центру, потому что там надо больше границ провести. Вы опять же можете
также построить n случайных деревьев, например, на буддстрапированных выборках и на случайных
признаках под множество, и посмотреть на каждый объект. Допустим, можете дерево строить до глубины
k, если вас там долго, и можете посмотреть на все объекты, которые до определенной глубины были
отделены в среднем. Значит, они находятся где-то с краев от вашего распределения, значит, скорее
всего это аномалия. Короче, деревья – плашная красота. И последняя, но тем не менее, ребят, вот
все, последняя формула, и я вас включу на перерыв, потом будет контрольная. Смотрите, out of back оценка.
Дерево, обученное на буддстрапированных выборках, позволяет вам получить оценку на отложенных
данных, не имея валидационной выборки. Бабаху. Что происходит? Мы же с вами когда буддстрапировали,
как fostered, ну наш же каждый объект попадал в буддстрапированную выборку случайно,NONindBROW,
правильно? Значит, для каждой буддстрапированной выборки есть под множество объектов,
которые в нее не попали из исходной. Да хватит звенеть, мы поняли.
интересно. Получается, для каждого объекта, обучающего выборки, мы можем найти под множество
моделей, которые этот объект не видели на обучении, правильно, потому что не обучено
на соответственность corespot. Но тогда мы можем взять и для каждого объекта получить предсказания
только в модели, которую вы не видели. Более того, мы тогда берем не все n-моделей, а лишь k из этих
моделей, их предсказания усредняем, и тогда это получается оценка сверху или снизу на нашу ошибку?
Сверху, потому что у нас меньше ансамбль, соответственно, мы меньше снижаем ошибку,
соответственно, у нас получается оценка сверху на нашу ошибку на отложенных данных, при этом нам
не надо валидацию отрубать, а валидация там 10-20-30 процентов данных, вообще говоря, это дорого.
Короче, деревья ваши друзья, и те, которые на улице кислород дают, как водоросли, и те,
которые здесь. Ну и последняя, наверное, красивая картинка, это уже вам так, подумайте, смотрите,
слева развиляющая поверхность для рандомпороста, справа для кнн. Вам не кажется, что они похожи?
Ну, немножко похожи. По факту, у вас деревья в некотором смысле могут быть приинтерпретированы,
как некоторая аппроксимация поведения кнн, потому что в пределе у вас что? У вас каждый объект
можно обвести своим квадратиком и предсказать ему нужную константу. То же самое делает метод
одного ближайшего соседа в какой-то норме. Но при этом дерево, в отличие от кнн, не будет
страдать, если у вас огромное признаковое пространство и огромное количество объектов,
потому что оно не квадратично по сложности, в отличие от кнн. Но если дерево переобучено,
у кнн, если дерево переобучилось, то ему тоже грустно, надо тогда ансамбль брать,
тогда у него все равно будет трейнер не ноль. У леса не будет нулевая трейнерная ошибка,
если будет, значит, лес вы тоже смогли переобучить. Ну хорошо, значит, дерево переобучили. Ну да,
здесь дерево переобучено и не очень хорошо. Окей. Ну, собственно, вот, что мы с вами сегодня разобрали.
Я вроде выделил все основные моменты. Мы с вами про деревья будем говорить еще минимум два занятия,
но деревья, пожалуйста, не списывайте со счетов, это классная вещь. Про категориальные фичи я
сказал буквально два слова. Категориальные фичи можно бинаризовать и с ними работать. В некоторых
случаях можно категориальные фичи использовать просто для биения. Кадбус, например, так умеет,
но там ничего особо не меняется.

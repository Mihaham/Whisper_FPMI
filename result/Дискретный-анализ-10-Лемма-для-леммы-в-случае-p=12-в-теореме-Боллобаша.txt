Но закончили мы тем, что в каком-то смысле доказали
теорему Балабаша про случай P равно 1-2, как ведет себя
хроматическое число случайного графа, когда P равно 1-2, то
есть когда все графы равновероятны.
Но дело упёрлось в мощную лему, которую можно кратко
охарактеризовать даже в маленьком кусочке, если
сно орех.
Помните такой факт?
В маленьком кусочке есть сно орех, это вот так лемма,
ее кодовое название, или в маленьком кусочке есть
сно орех.
Что утверждение я напоминаю, вероятность того, что для
любого множества s, входящего как под множество в множество
вершин нашего случайного графа, вот здесь n вершин,
а здесь я сейчас скажу, сколько мощность s равняется m, ну
либо больше, либо равняется m, это как раз неважно.
Выполнено, что аж, ограниченного на s, больше либо равняется
k1, вот эта вероятность стремится к единице при n, стремящемся
к бесконечности, где, напоминаю, здесь я думаю, что придется
это держать на доске, вот такое вот у нас m, это как раз
маленький кусочек, а лесной орех это k1, то есть картинка
то вот такая, у нас всего n вершин, маленький кусочек
это кусочек размера m, это произвольная s, и утверждается,
что в каждом таком произвольном s, какое бы мы ни взяли, обязательно
найдется лесной орех, то есть кусочек размера k1,
в котором нет ни одного ребра.
Вот мы этим чудом воспользовались в прошлый раз для того, чтобы
покрасить каждый граф нужное число цветов, почти каждый
граф.
Так, друзья, я не очень монотонно рассказываю, понятно все,
а то вроде как заладил, как пономарь, каждый граф,
каждый граф, каждый граф, нормально, ну хорошо.
Так, про k1 сейчас достаточно, наверное, помнить то, что
во-первых, k1 асимптотически 2 лог 2-ичный n или m, это одно
и то же с точки зрения асимптотики логарифма, и про k1 еще надо
помнить, что вот у нас есть такая функция fk от m, была
такая функция?
Она равна по определению мат ожидания Икс-Катова,
то есть среднему числу независимых множеств на k вершинах в случайном
графе.
Хорошая новость.
Так, которая напоминает c из m по k на 2 в степени
минус c из k по 2.
Так вот, k1 подбиралось таким образом, чтобы fk1 от m было
больше либо равно m в степени 3, плюс или малое от 1 что-ли,
вот так.
Было такое?
А то сейчас опять навру.
А, видите, вот наврал в обозначение, хорошо, да,
было fmt от k, соответственно, здесь будет fmt от k1, а в остальном
вроде все хорошо.
Да, друзья, спасибо, я не помню, это ж неважно как
обозначить, я уже немножко подзабыл.
Все, вот это вот все, что надо помнить про параметры,
но это тоже не сразу выстрелище ружье, оно выстрелит сильно
позже.
Вот, сейчас нам нужно доказать как-то вот эту лему.
Давайте, где бы мне начать, вот тут начну доказательство,
я напишу вероятность противоположного события, как всегда, существует
s из v мощности m, такое, что альпа от g на s, чего, меньше
чем k1, да, и я хочу доказать, что эта вероятность стремится
к нулю.
Ну, смысл утверждения я уже не комментирую, потому
что в прошлый раз он был максимально ярко прокомментирован,
сейчас надо просто вот взять и тупо это попытаться
доказать.
Оно не очень тупо, оно весьма тонко.
Так, ну, что можно сразу сказать, и с этим вряд ли
что-то удастся сделать, никакой формулы включения
и исключения мы здесь не напишем.
Так, друзья, вы помните, что если под знаком вероятности
стоит квантор существования, то это фактически объединение
событий, вероятность которого оценивается как сумма.
Ну, вот я так и напишу, это сумма по всем s, вот так
я выделяю, чтобы она большим казалась, мощности m, а чего
надо суммировать?
А, ну, что суммировать, вероятность того, что альфа вот g на s меньше
чем k1.
Так, друзья, согласны?
Ну, на самом деле, смотрите, что значит g, ограниченное
на какой-то свой кусок, у которого задан размер,
задано количество вершин, мы берем и случайный граф
ограничиваем на какую-то подсордельку.
Вы согласны, что ограничить его на эту подсордельку
и ограничить его на эту подсордельку, это ведь одно
и то же.
Вы понимаете, что за счет своей однородности случайный
граф, будучи ограниченным на кусок множества вершин,
превратится просто в случайный граф с этим количеством
вершин.
То есть, фактически, здесь написано просто c из n по
m, это количество способов выбрать s большой, мощности
m маленькое, количество слагаемых в этой сумме, а все слагаемые
одинаковые.
Это вероятность того, что, ну давайте, чтобы вам легче
было воспринимать альфа от h меньше чем k1, и вот здесь
вот вероятность уже берется на, давайте я вот так напишу,
h это случайный граф на множестве из m вершин с вероятностью
ребра 1 на 2.
Ну я для красоты вместо g написал h, хотя казалось
бы какая разница, но просто чтобы вам легче было потом
это воспринимать.
Вот это p, вот это p, оно на n вершинах, а вот это p, оно
уже на m вершинах, давайте это просто для себя запомним
и никаких лишних индексов рисовать не станем.
Хорошо так?
Нормально?
Так, ну что делать-то?
А, ну так что делать?
Я сейчас перепишу это.
Это равно c из n по m, вероятность того, что x с индексом k1 равняется
нулю.
Так, еще раз, что такое xk1?
Вот оно там есть.
По определению, xk1 – это количество независимых множеств
на k1 вершинах.
Ну согласитесь, что если максимальный размер независимого
множества меньше, чем k1, то это прямо в точности
то же самое, что независимых множеств, у которых k1 вершин
нет, нет таких множеств.
Их ноль.
Это одно и то же.
Альфа строго меньше, чем k1, и на k1 вершине нет ни
одного независимого множества.
Согласны?
Так, теперь, я и в прошлом году так читал, и вам так
прочитаю.
Вы не зря сюда пришли, вы хоть послушаете смысл
происходящего.
Потому что даже те, кто будут пересматривать, это место
могут при желании прокрутить.
Потому что я вам сейчас расскажу неправильное продолжение
рассуждения.
И вообще даже вот эта часть уже неправильная.
То есть она правильная в том смысле, что xk1 равно
0, действительно равносильно тому, что альфа-отаж меньше
чем k1.
Тут никакого подвоха нет, это правильно.
Другое дело, что если мы сейчас попробуем работать
именно с этой случайной величиной, то у нас ничего
не получится.
Можно я вот продолжу некое рассуждение, и вы увидите,
что ничего не получится.
Потом мы его зачерикаем и сделаем что-то ужасное.
Ну что, казалось бы, как, естественно, xk1.
Взять, записать.
Ну давайте, значит, действуем, как не так давно делали,
но я уже не помню, это пару лекций назад было.
Сначала делаем пустопорожные перегонки.
Во-первых, пишем xk1 не превосходит нуля, потом пишем вероятность
того, что xk1 со знаком минус больше либо равняется нуля,
ну то есть домножаем на минус единицу, просто неравенство
под знаком вероятности, и добавляем константу слева
и справа, равную математическому ожиданию.
p от мат ожидания xk1 минус xk1 больше либо равняется,
нет, не ноль, зараза, не лезет.
Конечно, мат ожидания е, xk, t, e1, оно не лезет, но ничего,
вот это вот я продолжил.
Так, друзья, все ж просто, да?
Я к чему хочу привести?
К неравенству Чебышова.
Потому что ну что мы знаем кроме неравенства Чебышова?
Ну кое-что знаем.
Ну давайте попробуем неравенство Чебышова оценить.
Получится c из n по m умножить на дисперсию xk1 и поделить
на мат ожидания xk1 в квадрате.
Так, надеюсь, что неравенство Чебышова все понимают,
как я применю.
Все понимают?
Ну как, разность, больше либо равна какого-то числа,
вероятность этого не больше, чем дисперсия, а поделить
на квадрат этого числа.
Стандартное неравенство Чебышова.
Теперь смотрите, квадрат этого числа, я пошел назад,
вот сюда вот, это вот эта штука.
М в шестой, а дисперсия тоже не нулевая уж, конечно.
То есть оценка, которая у нас получилась, вот эта
вот, ну это может быть что-то стремящееся к нулю, но даже
если оно и стремится к нулю, там это надо проверять,
дисперсию считать как-то, даже если оно и стремится
к нулю, то уж точно не быстрее, чем моногочлен
от м.
Друзья, это понятно?
А теперь смотрите, с из не по м.
Я не буду вас мучить, но мы ж с вами занимались
асимптотиками всякими.
М у нас, ну конечно, это не константа, помноженная
на не, а это не поделить на что-то медленно растущее,
но в общем это близко к экспоненте, товарищи, в общем это близко
к экспоненте.
То есть подставить вместо м, н деленное на что-то медленно
растущее, или там n поделенное на миллион, при маленьких
n это одно и то же, а если вы подставляете n поделенное
на миллион, то в самой первой лекции мы с вами доказали,
что такая c растет экспоненциально.
То есть, наверное, с из не по м это не экспонента,
а какая-то субэкспонента, но уж точно не моногочлен.
Моногочленом она не забьется.
Наша-то цель доказать, что все это стремится к нулю,
извините, я бегаю, поэтому за мной приходится вводить
камеру.
Должно стремиться к нулю, но в фигушке вы почти экспоненциально
растущую функцию укокаете до нуля, деля ее на какой-то
несчастный многочлен м в шестой степени.
Я понятно объяснил?
А с какими еще штуковинами мы имели дело, которые значительно
опережают действие неравенства Чебышова?
Это вот помните, да, неравенство Азумы?
Помните неравенство Азумы?
Забыли уже.
Ну, было такое, да, было неравенство Азумы, которое
говорило, что если вы возьмете какую-то функцию в случайном
графе, ну, то есть, в случайную величину, вычтите из нее
ее математическое ожидание.
Ну, собственно, как мы здесь и сделали.
И сравните это дело с какой-нибудь константой, то это будет не
больше, чем e в степени минуса квадрат поделить на нечто.
А вот на что поделить?
Может, вы там отлеснули уже, вспомнили?
Но может быть на 2 и на минус 1, да, так бывает, а еще бывает
по-другому.
Бывает, e в степени минуса квадрат поделить на 2 c
из n по 2.
И вот так лучше, а так хуже, но это уж как повезет.
Потому, что вот такое неравенство выполняется в случае Липшицевости
по вершинам, а такое в случае Липшицевости по ребрам.
Очень легко запоминать, вершин n, ребра полного графа
c из n под в.
Поэтому если f Липшицево по ребрам, то имеет место
такое неравенство, оно похуже, потому что в знаменателе
стоит большое число, а оно со знаком минус.
Вот.
И если Липшицево по вершинам, тогда в знаменателе стаять
относительно маленькое число и со знаком минус дает очень хорошую оценку, но даже эта оценка
хороша. Так, друзья, я сколько-нибудь понятен или я только для себя рассказываю? Понятно, да? Вот это
важно просто прочувствовать. Вот мы хотим этим воспользоваться, но первая трагедия, которая нас
подстерегает и у которой будет катарсис, все будет хорошо. Первая трагедия состоит в том, что х, она
Липшицева хоть по кому-нибудь, число независимых множеств на данном количестве вершин. Очевидно,
что она не является Липшицевой ни в каком смысле слова. Ну, потому что представьте себе, что у вас
есть какое-то ребро, например, и вот на нем сидит одно независимое множество, то есть куча вершин,
между которыми только вот это одно ребро и присутствует, другое независимое множество,
опять куча вершин, которые портятся только вот этим одним единственным ребром. Третье какое-то множество
вершин опять без ребер, чпок, и все стали независимыми. Одно ребро покоцали, а величина изменилась
катастрофически, не на единицу, а насколько хотите, на 3, на 10, на 100. Друзья, помните, что такое Липшицево
по вершинам, давайте я напомню, по ребрам. Что такое Липшицево по ребрам? Это значит, вы удаляете одно
ребро из графа или добавляете к нему одно ребро, и ваша величина не меняется или меняется на
единичку, но не больше того. Вот это Липшицевость по ребрам. Я в свое время говорил, даже количество
треугольников, очевидно, не Липшицево по ребрам. Ну, по вершинам тем более, потому что там
разрешается портить окрестность любой вершины. То есть мы не можем вот эту всю схемотехнику
провернуть, довести до сюда и сказать, давайте использовать неравенство Азумы. Хотелось бы
вот в этом месте вместо неравенства Чебышова использовать неравенство Азумы. Вместо вот этой
дроби будет экспонента от нее же, только от перевернутой и уже экспонента от отрицательного
многочлена. Ух ты, это круто. Понятно говорю, да? А не можем, потому что Xкатая первая, к сожалению,
не является Липшицевой отнюдь ни в каком смысле. Так, друзья, ну это все философия. То есть если бы я
хотел просто формально доказать теорему, то я бы этого всего не говорил, а сказал, давайте рассмотрим
какую-то ужасную случайную величину и будем с ней работать. Вот сейчас я это и сделаю. То есть вот была
философия-философия, теперь я это все стираю. Ну, Азуму не стираю, там пригодится. Так,
знаете, а тут не стираю пока. Давайте я введу случайную величину Yкатая вместо Xкатого.
Ну, она, естественно, случайная величина на множестве графов, поэтому аргумент ее это граф.
Какой? Я без модуля нарисовал, поэтому коэффициент 2 не нужен. Ну, имеется в виду,
что минуса будет с той же вероятностью, а с модулем удвоенной. Да-да-да, правильно.
Вот Yкатая, по определению, это вот что такое у нас будет. Это будет максимальное такое число,
давайте T, например, что существует набор множеств А1 и так далее Аt среди вершин нашего графа,
каждая из которых имеет мощность, равную K1, вот этому самому K1, Yкатая первая, равная K1. Так,
но это еще не все. Для любых Ij, я поясню смысл, все будет понятно, Аитая пересеченная с Ажитым
имеет мощность не большую единицы. Сейчас не большую единицу, а больше или равную двойке,
сейчас наоборот не больше единицы, правильно. Существует множество пересекающиеся не более
чем по одной вершине, но и Аитая независима. То есть картина вот такая, есть множество вершин,
в нем есть какие-то подмножества А1 размера К1, А2 размера К1 и так далее. Вот так вот нарисую
как-нибудь по одной вершине. Аn тоже размера К1, но извините, оно такое тоненькое, поэтому того же
размера. Но так что вот они все независимые, каждая из них независимая, нет ребер внутри,
и при этом каждые два пересекаются не больше чем по одной вершине. Вот нас интересует максимальная
мощность такой совокупности А1, Аt, что каждая Аитая независима нужной мощности К1, и при этом
каждые два из них, если пересекаются, то только по одной вершине. Жуть какая-то, да? Не, ничего, понятно.
Друзья, ну вот я хочу сказать, что вот можно так поправить. Нет, что-то Y у меня не было. Пусть будет
вот так. Если бы я сразу так написал, ну вы бы мне сказали, что я сумасшедший. Почему не X? Вот те из вас,
кто вдумывается и успевает просто так быстро сообразить, ну точно бы сказали, а почему не X-то?
Вот а потому не X, что с X-ом работать нельзя, он не Липшицев. А Y Липшицева, но она равна нулю тоже
тогда и только тогда, когда у нас просто нет ни одного независимого множества. Согласитесь?
Что в этом смысле онаrilky же самую роль, что X, она равна нулю уже только тогда, когда нет ни одного
независимого множества. Если есть хотя бы одно независимое множество, то t равно единице, мы уже
можем взять, вот взяли одно независимое множество. Все получилось, все хорошо. Поэтому это правильное
равенство, но теперь согласитесь, что yкт первая так хитро устроена, что она
липшится по кому? По ребрам, конечно, да. Именно ради этого требование, чтобы
мощность пересечения каждых двух была не больше единицы. Ни у каких двух нет
общего потенциального ребра, поэтому удаление одного ребра или добавление
одного ребра не поменяет эту цепочку больше чем на единицу максимально. Одно
множество может добавиться, но два множества добавиться не могут, потому что
тогда бы они имели общее ребро. Друзья, понятно говорю? Вот, то есть yкт первая
липшицева по ребрам. Вот, то есть мы такое неравенство будем применять, но только
с заменой на м. Липшицева, я очень люблю этот, как подпись, да, липшицева по ребрам. Поэтому
будем использовать вот такое неравенство. Ну какое? Давайте я здесь подотру, я специально
ничего не трогал. Я вместо х напишу y, все будет то же самое. Здесь тоже вместо
x и y. Здесь везде вместо x и y. Это уже стало противно немножко. Тут вместо x и y. А вот это вот
нужно переписать. На е в какой степени? Минус модуль еукт первая в квадрате, это
вот а наше, с которым мы сравниваем. А наше. Поделить на что? На два. С и зен по два. Так, друзья, тут
что-нибудь видно или оно смешалось? Как бы это так отделить? С и зен такую сейчас штучку надо
правильно отделить. Вот так. Вот так вот. Получилось? Видно? Ну вот тут вот еукт первая,
оно сверху. А здесь еукт первая в квадрате, показатели отрицательные экспоненты, поделенные на два
с и зен по два. Ну это, товарищи, меньше, конечно, чем два в н-й на е в степени. Той же самый еукт первая
в квадрате поделить на два с и зен по два. То есть я с и зен по м совсем халявно оцениваю. Я долго
рассуждал, что это почти экспонента, поэтому на самом деле уж очень большой халявы тут нет. А мы
сейчас так хорошо оценим вот эту отрицательную экспоненту, что нам станет наплевать. Понятно
говорю, да? Но это нам предстоит, потому что, с одной стороны, мы вроде преуспели, мы получили
экспоненциально убывающую величину, но мы замели пока что пыль под ковер. Мы же не понимаем,
насколько она реально убывает. Мы же не знаем мат ожидания y-катова первого, мы знаем мат ожидания
x-катова первого, и оно вот такое. Вот бы мат ожидания y-катова первого было, если не такое,
то хоть какое-нибудь похожее. Тогда было бы хорошо. Так, понятна цель? Давайте я напишу ее реализацию,
то есть еще одну лемму в лемме, которую мы сегодня докажем. Так, чего надо стирать? Вот это можно стирать.
Так, лемма в лемме. Вот так. Мы доказываем лемму, а для того, чтобы доказать, формулируем лемму.
Это лемма вот в этой лемме. Лемма утверждает, что мат ожидания y-катова первого больше либо равняется
с точностью до асимптотики вот такой величины m в квадрате поделить на 2k1 в четвертой степени.
Но прежде всего, давайте убедимся, что если это верно, то мы победили вот здесь, и это стремится к
нулю. То есть нам останется доказать только лемму в лемме. Мы убедимся, что это стремится к нулю,
и тогда останется доказать лемму в лемме. Ну, действительно, давайте я все-таки перепишу. У нас
2 в степени n, а тут мы вот эту штуку подставляем с квадратом, то есть будет какая-то асимптотика.
Так, дальше будет m в четвертой степени поделить на 4k1 в восьмой, и еще надо будет поделить на 2c из m
по 2. Это вот здесь у нас 2c из m по 2, правильно? Так, друзья, может вы сразу понимаете или не очевидно?
Ну ладно, не очевидно. Равно 2 в степени n на e в степени... Сейчас я чуть-чуть другую асимптотику
напишу. Ну какая разница? Все равно это будет асимптотика. Будет вот так. m в четвертой поделить
на 8k1 в восьмой и на m в квадрате. Я просто хочу сказать, что c из m по 2... А, надо было пополам,
ладно. c из m по 2 это m квадрат пополам. В асимптотике. А асимптотику я сюда загнал. То есть вот это
то малое от единицы, и вот эта суть разные вещи. Ну какая разница? И то, и другое стремится к ноль.
Ну мне кажется сегодня стало лучше, по крайней мере с теми временами, когда я здесь умирал. Друзья,
вам как? Хорошо? Вам нормально? Ну тогда значит нормально. Я-то понятно прыгаю, чем холоднее,
тем лучше. Вам комфортно? Друзья, понятно, что произошло? Ну то есть вот эту двойку,
конечно, надо было здесь написать, просто 4 и все. Вот так. Чпок-чпок. Тут двоечка. А, 2вн,
н потерялась. Тут двоечка. Ну смотрите, а k1, я специально ведь напомнил, что оно примерно
два лог двоечной н, да? Ну примерно в смысле вот с точностью размножения на 1 плюс о малое от единицы.
Ну то есть все можно переписать вот так. Это 2 в степени n на e в степени минус 1 плюс о малое,
даже не 1 плюс о малое, я вот так напишу. m в степени 2 плюс о малое от единицы. Мы уже так делали. Я
вот это m в квадрате делю на какую-то степень логарифма, то есть реально это не плюс, а минус,
но мне плевать там. Плюс-минус. Важно, что это функция, которая стремится к нулю. Степень логарифма,
она влияет только так. Ну и все. m. Смотрите, какое m. Это опять n поделенное на степень логарифма. То
есть я могу в свою очередь это вот так переписать. 2 в степени n на e в степени минус n квадрат и
к квадрату добавленного малой от единицы. m отличается от n делением на какую-то степень
логарифма. Она вся уходит вот в этого малой от единицы. Это стандартный анализ. Стандартный
анализ? Проходили ведь в прошлый раз? Мы с вами прям проверяли. Ну согласитесь,
что это с огромным свистом летит в ноль, то есть это докажем будем в героях. Вот. Так,
ну что? Вот это определение не хотелось бы стирать, а все остальное стереть можно.
Чтобы вам было еще, может быть, содержательно понятнее, давайте я сперва не докажу эту лему.
Опять будет такая врезочка чисто на понимание происходящего. Можно? Можете записывать,
можете не записывать, но вот эта штука запишет, наверное, если батарейки не сядут. Вот я хочу,
чтобы суть происходящего понимали. Что на самом деле такое вот это e, y, k, t первое? Как это
можно интерпретировать? Можно интерпретировать это вот как. Давайте k с индексом m, это полный
граф на ме вершинах, как всегда, просто стандартное обозначение. Так? Давайте,
если я над ним нарисую черту, то это, наоборот, пустой граф, то есть такое независимое множество
из отдельных вершин, которых мэш стук. k, m с чертой стерли все ребра. Ну как обычно,
j с чертой это инвертированный граф, ребра стерли, каких не было, наоборот, провели. Но тут все были,
поэтому если все ребра стерли, осталось пустое множество. Ребр, а вершина отдельная. Вот такой
k, m с чертой. Вот что такое y, k, t от вот этого графа? Давайте подумаем. Ну k, t, k, t первое там не
так важно, просто пусть какое-то k дано, например, k первое. Вот что такое y, k, t от пустого графа? Здесь
напоминается определение. Это фактически нам нужно просто взять максимальное количество k-элементных
подмножеств, попарные пересечения которых имеют мощности не больше, чем единица. Ну потому что
тут все множества независимые, и вот это условие про то, что они являются независимыми, никакого
смысла сейчас не имеет. Просто нас интересует максимальное количество k-элементных подмножеств,
сколько элементного множества, me-элементного множества, me-элементного множества. Так что они
попарно пересекаются не больше, чем по одному элементу. Может быть вы вспоминаете задачку
теоретагирования, которую я рассказывал. Я про пьяниц даже говорил, наверное, что там тройки
пьяниц какие-то. Вот это оно самое. Ну очевидно, ну или почти очевидно, сейчас я напишу очевидно,
мне кажется, что очевидно, но это обычно не очевидно. Людям почему-то не очевидно, что это не
больше, чем c из m по 2 поделить на c из k по 2. Это очень простое упражнение на дирихле. Значит,
как это получается? Если у вас сарделька из me-вершин, как угодно представлена вот в виде
таких k-вершинных подмножеств, пересечение каждых двух из которых либо пусто, либо по одной вершине,
то у них нет общих пар вершин, правильно? Но вот мы посчитаем количество пар вершин в каждой под
сардельке. Оно вот такое, c из k по 2. Просто пары вершин. Берем пару вершин, берем другую пару
вершин. Сколько всего пар вершин в множестве из k-вершин? Ну c из k по 2, очевидно. При этом общих пар
вершин у этих сарделек нет. Ну все, поэтому получается, что самих этих сарделек точно не больше,
чем количество всех пар вершин на огромную сардельку поделить на количество пар вершин,
подающих каждую. Друзья, я понятно объяснил? Это просто банальный принцип дирекле. Ну это примерно,
особенно если m и k стремятся к бесконечности, как это происходит у нас, тогда это можно писать тильда.
Ну можно по-разному, да, но это примерно m квадрат на k квадрат, друзья, это понятно? Так вот,
я утверждаю, что если в качестве k взять k1 и y брать не на k с чертой, то есть на графе,
в котором вообще ребер нет, а даже на случайном его подграфе, половину ребер провести в среднем,
то все равно оценка снизу будет почти такая же, как оценка сверху в самом лучшем случае.
Самый лучший случай, это когда ребер вообще нет, тогда yкт самое большое, конечно. На случайном
подграфе yкт скорее всего поменьше, но утверждается, что в среднем оно и снизу оценивается почти так,
как в лучшем случае оно оценивается сверху. Ну потерялся квадрат, да, вот тут k квадрат,
а тут k4. Сейчас, друзья, вот эту интуицию я объяснил, да, это не обязательно понимать,
но это полезно понимать, это не нужно будет знать на экзамене, но если вы это прочувствуете,
вы лучше поймете теорию. Такой вот происходит, да, гипотеза до сих пор не доказана, что здесь
можно заменить k4 на k в квадрате, но пожертвовав какой-то... Эту гипотезу до сих пор никто не
умеет доказывать. Открытая проблема. Ну ладно, нам-то все равно, видите, какой тут свист получился,
нам плевать в какую степень логарифом возводить в квадрат или в сотую, все пойдет вот в этого маленькой
от единицы, но а n в квадрате, конечно, забивает с большим запасом. Вот, поэтому это было такое
лирическое отступление врезко, теперь я ее стираю и формально аккуратно доказываю лему. Так,
давайте формально докажем то, что здесь написано, но это очень красиво на самом деле,
то есть если вы поймете сейчас прям вот сейчас доказательство, то оно вас тоже в какой-то
мере должно проканать. Потому что для того, чтобы доказать вот это неравенство, а неравенство
вроде как утверждает что-то про случайные объекты, то есть про случайные графы, мы сейчас применим
вероятностный метод, то есть добавим еще случайности, но это само по себе неожиданно. То есть мы
дополнительно усредним в каком-то смысле, но давайте сейчас я объясню о чем идет речь. Так,
что же мы сделаем? Давайте обозначим как-нибудь вот так k1 прямое и так далее,
но большое. Прямое, но большое или и большое. k большое с индексом виноват c из m по k1,
но обозначение, конечно, несколько громоздкие, но я хочу просто взять и перечислить все под
множество мощности k1 в графе нам и вершинах. Это просто все под множество. Что? Нет,
ну индикатор тут фиг получится с индикаторами, но фиг получится с индикаторами, потому что нас
интересуют не отдельные под множество, а их такие цепочки что ли совокупности. Тут вот как,
какие тут индикаторы-то брать для совокупности, а нас интересует же не мощность, а максимальная
мощность, то есть это явно не минейность мат ожидания, конечно, тут что-то будет похитрее. Пока
я просто ввожу обозначение для всех под множество мощности k1, ну на множество вершин,
которых у нас m штук. Все под множество мощности k1, а уж где понятно. У нашей большой сардельки,
которая все время символизирует множество из m вершин. Всего вершин в графе m штук,
и вот мы берем все k1 элементные под множество. Так, давайте скажем, что yk1 в этих черненах
можно вот так сказать, что такое yk1adj. Это максимальное количество множества отсюда,
которые пересекаются попарно не больше, чем по одному элементу. То есть просто можно вот так
написать, это максимальная t опять, такое, что существует там k и t первое, k и t с номером t.
А, нет, сейчас, подожди, не, не надо так писать, извините, вот это пусть будет, а так писать не надо,
все, так писать не надо, виноват. Так, сейчас, сейчас, секунду, я подумаю, как лучше сказать,
чуть-чуть я поторопился, значит, все под множество мощности k1, это, конечно, хорошо. Так, так, так, так, так, так.
Ага, ой, сейчас. Я же обещал вероятностный метод, вот давайте его сразу запустим, а зачем,
это потом будет понятно. Так, давайте возьмем какое-нибудь число, назовем его q, но q плохо,
потому что оно путается с p, правда у нас p нет, у нас p равно 1 второй, может и неплохо. Я не знаю, q,
можно взять число q или q со звездочкой лучше? Лучше q, да? А q с крышечкой не хотите? Шучу, шучу. Давайте
просто q, хорошо, но q это не один минус p, пожалуйста, только не путайте, это не один минус p, у нас p никакого
нет, у нас вероятность ребра одна-вторая, поэтому q это будет тоже какая-то вероятность, но какая,
я сейчас скажу. Сейчас, аааааааааааааааааааааааааааааааа, елки-палки, черт, я что-то путаю,
сейчас не так надо сделать, не так, во, q будет сейчас, q будет в секунду, что-то я отраможу,
извините, друзья, не знаю почему. Давайте еще k красивое возьмем. Ну, друзья, я вот и у тех,
кто потом слушать будет, прошу прощения, чуть путаюсь. Сейчас все расставим по полочкам,
все будет очень аккуратно. Значит, вот есть эти k прямые, это просто все k 1 элементные под
множество. А давайте у нас есть какой-то граф. Ну, случайный, не случайный. Давайте введем такое
множество k красивое от g. Это множество тех k прямое, даже некрасивое, потому что рука
сорвалась. Видите, какое некрасивое оно получилось и не совсем прямое. Ну ладно,
множество тех k прямое it вот отсюда, что k it является независимым в графе g. Вот так.
Значит, смотрите, пока у нас случайен граф, он случайен, конечно, мы каждое ребро проводим
независимо от остальных, с вероятностью одна вторая, но даже представим себе, что мы граф зафиксировали
и просто рассмотрели вот такое множество тех его k 1 элементных под множество, которые в нем
образуют независимое множество, то есть не соединены ребра внутри. Так понятно, что такое k от g, да?
Как красивое от g вам дан граф, вы просто смотрите все его k 1 элементные независимые под множество.
Ну, я как-то немножко нечетко просто выражался, но k от g, вот вроде понятно, что такое. А теперь я
возьму те независимые множества, которые попали в этот вот граф и их прорежу в некотором смысле,
а именно вот эта чиселка q, которая будет служить вероятностью, она будет служить вероятностью того,
что каждое отдельное множество, попавшее вот в это k красивое, мы сохраним независимо от всех остальных.
То есть вот есть какое-то k красивое от g, фиг знает, как обозначить его элементы, это какие-то там
не знаю, а1 и так далее. А, кстати, можно сказать, с каким последним индексом, какой последний индекс,
понимаете, как его написать? Сколько всего независимых множеств в графе g? Нет, альфа это максимальный размер.
А что такое, хе, это хроматическое число, что ли? Нет, это x с индексом k1 от g, ха-ха, ну xk1 от g это количество
независимых множеств размера k1 в графе g, просто по определению, вот тот самый, который мы забанили и
заменили на y. Сейчас узнали его, да? Это x, да, x это количество независимых, ну каждая аи-т принадлежит вот этому множеству k1, kc, см по k1.
Вот, то есть размер этого k от g это, конечно, xkt1 от g, а вот оно же, вот же оно. Друзья, я специально его берег.
Сейчас, друзья, я медленно рассказываю, но я надеюсь, что пока все понятно. Чуть-чуть я путался, но сейчас уже не путаюсь.
Теперь, смотрите, мы это множество начинаем прореживать, то есть мы бросаем монетку, которая с вероятностью q кокает нам очередное множество.
Бросаем монетку, если реализовалась вероятность q, мы это множество просто не вытаскиваем оттуда и похереваем.
Если, наоборот, реализовалась противоположная вероятность, мы его вытаскиваем и бережно сохраняем.
Так, я могу, конечно, заподозрить, что некоторые считают, что похерить это плохое слово. Это хорошее слово, это поставить крест.
Ну, поксерить, да, это не поставить крест. Вот смотрите, как интересно вы предложили.
Значит, если вероятность q, то мы похериваем, а если вероятность 1-q, то мы поксериваем.
Ну, в смысле, что мы его отправляем в некоторое новое строющееся множество, которое давайте назовем, например, c от g.
Нет, нет, не вершина, еще раз. k от g состоит из k1 элементных множеств вершин.
И мы каждое множество либо выбираем вот сюда, все множество, либо не выбираем. Понятно? Всем сейчас понятно?
Отлично. Вот это случайное подмножество в множестве независимых множеств вершин случайного графа g.
Простите меня за такой пафос, но это же мощно. Был случайный граф, в нем были случайные множества.
А мы еще из них случайным образом часть выбрали, а часть похерили. Я настаиваю на этом слове.
Ну, то есть, друзья, если вы хотите понять, как устроено вероятностное пространство в итоге, то устроено оно, конечно, очень просто.
У вас есть граф, вероятность которого считается стандартным образом, как 1 поделить на 2 в степени c из m по 2.
И к этому графу еще приляпан вот этот набор случайно выбранных из него независимых множеств.
Взяли и некоторым образом случайно его как бы проредили, прорешиваем.
То есть, вероятность такой пары, это вероятность графа умножить на вероятность вот этой штуки.
А вероятность этой штуки, ну, это q в степени на 1 минус q в степени. Ну, стандартная, биномиальная.
Так, понятно, как устроено пространство? То есть, мы добавили такой вот случайности.
Добавили случайности. Так, сейчас будет еще один смешной момент с точки зрения обозначений.
Вот, смотрите, здесь есть fmt от k, есть exk, есть его явная формула. Это уже три обозначения.
Давайте, если сюда подставить вместо k k1, назовем то, что получится для максимальной краткости.
Оно же вот такое еще получится, но это мы тоже пока не будем помнить.
Это случайный набор множеств. Он случайный, да.
Если мы граф зафиксировали, то c.adj тоже не фиксировано.
k красивое adj фиксировано, а c.adj все равно случайно.
И его вероятность это q в степени его мощность на 1 минус q в степени xkt первое adj минус его мощность.
Для конкретного графа xkt первое adj это конкретное число.
Поэтому вероятность вот этой штуки, вот этой штуки,
это еще раз повторяю q в степени ее мощности, вот на конкретном графе,
на 1 минус q в степени xkt первое adj минус ее мощность.
Вот так. Ну давайте я напишу, чтобы точно это можно было воспроизвести, не на слух, а так вот в записи.
То есть мы знаем кучу всего.
Если мы сюда подставим k1, k1, k1, k1, то это три одинаковых числа и плюс они еще все не меньше, чем вот это.
Это мы все знаем, но тем не менее я сейчас коротенько-коротенько все эти четыре штуки обозначу мю.
Буквой мю.
Можно?
мю
мю
Так.
Если я введу такое обозначение мю, ну я вот здесь напишу fm от k1,
это будет мю.
Ну мю просто так, определим.
Там с вероятностью q не брали.
Нет, с вероятностью q брали.
А, ладно.
А я неправильно сказал, да?
А, я сказал с вероятностью q похериваем, а 1 минус q поксериваем.
Ой, Господи.
Ну извините, да, я, конечно, может быть, оговорился.
Нет, это, может быть, мне так вот это q понравилось, что оно q, нет, q это вероятность успеха сейчас.
Если считать успехом, все-таки не похерить, а поксерить.
Да-да-да, q это вероятность того, что мы сохраняем жизнь каждому отдельно взятому независимому множеству.
Сейчас, друзья, нормально, не запутал?
Нормально.
Ну вот хорошо, что я это написал, а то потом были бы разночтения.
Ну, в общем, я ввожу четвертое обозначение для одного и того же, которое совсем коротенькое мю, но чем плохо.
Ну, хочу ввести, ввожу.
Мне так короче писать.
Ну, смотрите, давайте обсудим, каково математическое ожидание мощности c.
Это очень легко сказать.
Это вопрос к аудитории, на которой есть надежда, что она сумеет ответить.
Ну, я стараюсь последовательно рассказывать.
Я вначале чуть-чуть запутался, но дальше-то я очень последовательно рассказываю.
Ну, а мат ожидания x-то чему равно?
Только что говорил?
Мю.
Ку-мю, правильно.
Да, ответ ку-мю.
Ку на мю, конечно.
Ку на мю.
Ку это вероятность успеха, а мю это математическое ожидание вот этой штуки.
То есть мы как бы дважды усредняем.
Вот тут используется линейность, товарищи.
Вот тут используется линейность.
Ку умножить на мю.
Сейчас я виноват, введу еще два множества, но потерпите, они очень естественные.
Значит, w будет зависеть просто от графа.
И это будет множество таких пар.
k-i-t, k-j-t.
Видите, они прямые, это прямые k-i-t.
То есть они относятся под множеством мощности k1.
Такие, что...
Давайте только пары будем считать неупорядочными.
Вот так их напишем.
Неважно, в каком порядке их взять.
k-i-t, k-j-t.
Такие, что k-i-t, k-j-t принадлежит k красивому от g.
И k-i-t, пересеченная с k-j-t, имеет мощность больше либо равную двойке.
Вот количество таких пар возьмем в графе g.
Что это за пары, товарищи?
Вот я специально не стирал y-k-t первое.
Это пары, которых не должно быть в цепочке максимальную длину,
которой дает нам y.
Их не должно быть.
Пары должны пересекаться тут не больше, чем по одному элементу.
При этом мы говорим о парах как раз k1 вершинных множеств,
каждая из которых независима.
Здесь мы говорим тоже о парах k1 вершинных множеств,
каждая из которых независима.
Но при этом говорим о плохих парах.
То есть их бы изничтожить.
Понятно сказал, что такое w от g.
Это множество вредных пар.
Тех пар, которые не должны встречаться в определении y.
И точно также введем w штрих от g запятая c от g.
То есть на том вероятностном пространстве,
в котором появилась дополнительная случайность с прореживанием.
Абсолютно все то же самое.
Вот просто 1 в 1.
Только вот здесь вот c от g.
А дальше опять все то же самое.
То есть это снова множество вредных пар,
но уже попадающих обоими своими множествами в прорежанное c от g.
Прорежанную совокупность.
Услеживаете?
В самом графе было много независимых множеств.
Какие-то из них образовывали пары, имеющие неподобающие пересечения.
Эти пары не могут встречаться в цепочках, которые нас интересуют.
Но мы затем не только взяли граф,
а еще проредили, зачем-то вот так искусственно, проредили его множество независимых подмножеств.
И теперь мы смотрим на количество вредных пар,
только таких, у которых каждая каитая, кожитая попадает вот в эти прореженные.
Но формально я все написал, и смысл, надеюсь, уже понятен.
Вот надо от этих избавиться.
Тогда мы будем иметь какую-то оценку у.
Так, давайте я введу обозначение для математического ожидания мощности w.
Обозначение будет вот такое.
Дельта пополам.
Более идиотского обозначения не придумать, конечно,
но я поясню, в чем смысл несколько позже.
Пополам, потому что я в качестве дельта буду брать как раз мат ожидания для случая упорядоченных пар.
То есть у меня в w неупорядоченные пары,
но дельта это будет мат ожидания в случае, когда здесь были круглые скобки,
то есть пары были кортежами, упорядоченными парами.
Ну, естественно, такое дельта от искомого мат ожидания отличается в два раза.
Это, я надеюсь, понятно?
Что если фигурные скобки здесь заменить на круглые,
то новое мат ожидания отличается от искомого в два раза.
Вот это новое я обозначу дельта, но разделю пополам, чтобы получить то, которое нас интересует.
Теперь, друзья, хватит занудствовать.
Ну-ка, скажите мне, что такое мат ожидания мощности w штрих?
Дельта на два, q, но это близко к правильному, но неправильному.
Мю обозначение, да, мю, что за мю?
Нет, тут вот дельта, правильно, что дельта на два участвует, это правильно,
но только не на q, а на q квадрат, конечно.
Мы же интересуемся парами, и оба элемента пары должны попасть в c.
Вероятность того, что k и t попадают в c, это q.
Вероятность того, что k и jt попадают в c, это q.
А что они обе попадают в c, это q квадрат.
Согласитесь, что мат ожидания w, в котором нас интересуют все независимые множества графа,
отличается от мат ожидания w штрих ровно в q квадрат раз.
Просто вероятность того, что эти товарищи и сюда тоже попали.
Это опять линейность.
Ну, что такое дельта, я пока не знаю.
Сейчас, где бы мне стереть, даже уже и не знаю.
Ха-ха, где же мне стереть?
Ну, вот здесь, наверное, могу стереть, мне кажется, это понятно.
Это уже понятно.
Ничего-ничего, почти победа.
Ну, не совсем, конечно, но почти.
Так, давайте знаете, что сделаем.
Давайте, вот у нас есть c от g, а от него возьмем, это последнее будет действие,
не переживайте, и перейдем к c со звездочкой от g.
Сейчас скажу, по какому принципу, очень естественному.
Мы просто вот в этом множестве c от g найдем все пары, которые принадлежат множеству w штрих.
Так, смотрите на w штрих, это множество тех пар из c от g, которые пересекаются не так, как хотелось бы.
Вот возьмем все пары из этого w штрих, и у каждой выдернем по херям.
Один элемент.
Из каждой неупорядоченной пары, оба элемента, которые попали в c от g,
один, не важно какой, каитая, кожитая, она же неупорядоченная, по херям, удалим из c от g.
Ну может так случиться, что убивая какие-то две разных пары, мы кокнем только одно каитое.
Ну и слава богу, значит мы еще быстрее все расколкаем.
Короче, вот это вот c со звездочкой от g получается из множества c от g путем удаления по одному элементу из каждой пары, принадлежащей w штрих.
У нас такая идея уже случалась, как минимум один раз, когда мы доказывали теорему Эрдерша про обхват.
Когда мы там разрушали что-то лишнее.
Ну вот здесь тоже разрушаем вредные пары.
Слушайте, ну согласитесь, что yкт первое от g больше не поравняется, чем мощность c со звездочкой от g.
Так, yкт первое я не стер.
Это максимальное количество k1 элементных подмножеств, пересечение которых по парной имеют мощность не больше чем единица, и все они независимы.
Но c со звездочкой ведь так и строилось.
Оно строилось из совокупности независимых множеств, после чего эта независимая совокупность прореживалась, удалялись элементы из каждой вредной пары, из каждой пары, в которой пересечение большое.
Друзья, точно успеваете за мыслью?
То есть, конечно, мы не знаем, где yкт первое достигается, но мы точно знаем, что это нижняя оценка.
Мы предъявили некоторую вероятностную процедуру, которая позволяет эту оценку получить.
Ну то есть, математическое ожидание, которое нас интересует, больше либо равняется математическому ожиданию,
которое в свою очередь больше либо равняется, нежели математическое ожидание c мощности c, минус математическое ожидание мощности w штрих по линейности.
Именно больше либо равняется, потому что, как я уже говорил, в c со звездочкой может получиться и больше элементов, чем если из c вычитать w,
потому что одно и то же множество может разорвать сразу несколько вредных пар.
Поэтому здесь больше либо равно. Согласны?
Ну, это нас устраивает, только хорошо.
Так, это равно miu на ku.
Да-да-да, без мат ожидания тоже верно, надо было так и написать.
Да-да-да.
Так, miu на ku минус дельта ku квадрат пополам.
Верно?
Не, ну вроде все пока хорошо.
Слушайте, а что такое ku?
Ну это какое-то число, вот мы взяли какое-то число, мы же его никак не конкретизировали, правда?
То есть его выбор в каком-то смысле, это в нашей власти?
Ну какая оценка будет самой лучшей, если тут по ku, извините, парабола вот такого вида?
В вершине параболы, правильно?
То есть ku надо по-хорошему выбрать так, чтобы вот эта разность достигала своему максимума.
Miu делить на дельта, правильно?
В качестве ku надо взять, вот здесь давайте возьмем, miu делить на дельта.
Тогда что у нас тут получится, если подставить вместо ku miu делить на дельта?
Miu квадрат на 2 дельта, правильно?
Miu квадрат на 2 дельта.
Шикарно.
А наша цель доказать?
Вот такое вот неравенство, дорогие товарищи.
Такое?
Да, вот надо сейчас сопоставить.
Сейчас надо догадаться, чему равняется miu.
Нет, чему равняется miu мы знаем.
Miu вот оно.
Сейчас наконец вот это ружье непонятное.
Помните, мы в прошлый раз долго дискутировали, откуда там эта третья степень, как она там растет в ме примерно раз.
Ну разобрались в конце концов.
Вот это сейчас ружье как раз выстрелит.
Сейчас это знание о том, чему равняется miu нам очень сильно поможет.
Так.
Но что я хочу сказать?
Я-то хочу, чтобы это равнялось, ну или асимпатически равнялось miu в квадрате на 2 к1 в четвертой,
ну а асимпатически это значит тут 1 плюс о малой от единицы.
Это я хочу.
Значит, чего такое отсюда дельта?
Давайте сообразим, какое отсюда дельта получается, если мы хотим, чтобы получилось такое равенство.
Ну вернее, надо вот так писать, конечно, извините.
Дельта равно чему-то, следовательно, это равно.
Чему равно дельта?
Почему м в четвертой?
Нет, давайте в терминах miu, да?
Ну, наверное, miu в квадрат, чтобы miu в квадрат сократилось.
Наверное, пополам не надо, потому что двойки там общие, они сократятся.
Да?
Miu в квадрат.
И на что-то надо поделить.
Надо поделить на m в квадрате, да?
И умножить на k1 в четвертой.
Плохо.
Ну тут вот напишу k1 в четвертой.
Вот так.
Видно?
Ну k1 в четвертой на miu в квадрат на m в квадрате, да?
Так, согласны, что если я сюда вместо дельта...
Ну, а я вот сейчас сотру это и тильно напишу.
Тогда будет совсем правильно.
Если я вместо дельта подставлю асимпатическую вот такую штуковину,
то miu в квадрат у меня сократится,
m в квадрат прыгнет наверх, двойка останется,
а k1 в четвертой прыгнет вниз.
Ну, вроде все хорошо.
Значит, дельта должно быть вот таким.
Так.
Охреново знает, почему оно такое покажет.
Ну, должно же быть таким, иначе ничего не получится.
Правда, товарищи?
Должно быть таким.
Значит, будет.
Ну, значит, будет.
Сейчас, друзья, сейчас будет микрокатарсис или макро.
Я уж не знаю.
Сейчас смотрите, какая чудесная вещь сейчас произойдет.
Смотрите.
Смотрите.
Значит, мы выяснили, что дельта обязана таким быть.
Нам это, видимо, предстоит объяснить каким-то образом,
но, видимо, уже не сейчас.
А что будет катарсисом?
Вы смотрите.
Мы куположили равным mu поделить на дельта.
А мы вообще имели право так сделать?
Нет.
Не в том дело.
Дело в том, что эта вероятность, она должна быть меньше единицы.
Вам не кажется, что если это больше единицы, то все, что мы делаем, это хрень?
Вот, дайте посмотрим.
Значит, дельта обязана быть таким.
Поэтому mu, поделенная на дельта, асимпатически ведет себя следующим образом.
Это mu m в квадрате дважды...
А, как куда дважды? Двойки нет.
mu m в квадрате просто поделить на k1 в четвертый mu квадрат.
Согласны?
Чего?
Вот оно отправилось вниз, да.
Вот k1 в четвертый mu квадрат отправилось вниз,
а вверх прыгнуло m в квадрате, ну и mu осталось.
Сейчас, друзья, видно, нет?
Это равно m в квадрате поделить на k1 в четвертый mu.
И вот теперь мы вспоминаем, что mu не меньше, чем m в кубе.
С точностью до какой-то там логарифмической фигни.
То есть это все не больше, чем m в квадрате поделить на k1 в четвертый на m в степени 3 плюс о малой от единицы.
А это равно 1 поделить на m в степени 1 плюс о малой от единицы.
И это, конечно, не только меньше единицы, но даже стремится к нулю.
Ну, мы считаем, что mu достаточно большое, чтобы это стало меньше единицы.
Это не равно?
Нет, равно.
Потому что я k1 в четвертый вот в эту малую от единицы загнал, поменяв его там немножечко.
Друзья, я объяснил, зачем мы мучились-то с выбором параметра.
Зачем нам нужно было это не сейчас на m в кубе.
Вот сейчас это наконец выстрелило.
Ну слушайте, ну это круто.
Чтобы вот это вероятностью было, надо чтобы такая штучка росла хотя бы как m в кубе.
Ну можно m в десятый, это неважно.
Но я для красоты вот прям так впритык подобрал.
Еще раз, k1 в четвертый – это логарифм в четвертой степени.
Я его вот сюда загнал, как в известном ролике.
Ну извините, он сюда прыгает.
Ну так вот, надо к этому привыкнуть.
Логарифм – это m в степени о малой от единицы.
m в степени сумма двух о малых от единицы – это тоже m в степени о малой от единицы.
То есть вот это о малой и вот это о малой – они разные.
Ну какая разница, все равно все хорошо.
Значит, нам осталось теперь доказать, что дельта вот такая.
Ну это уже дел техники, только времени не хватает, по-моему.
Я сейчас напишу, что нам осталось сделать.
И в следующий раз мы быстро с этим расправимся.
Потому что совсем точную выкладку я писать не буду.
Это скучно.
На самом деле дельта, оно так вот, в принципе, естественным образом обозначено.
Это в каком-то смысле дисперсия.
Потому что мы считаем количество пар каких-то объектов.
А пары объектов – это всегда дисперсия или второй момент.
Это кавариация.
Ну кавариация, да-да, как хотите.
В общем, е от мощности w.
Ну, это действительно одна вторая.
Как считать?
Математическое ожидание числа пар.
Ну просто по линейности.
То есть надо перебрать все пары.
Оно уже тут не сохранилось, но вот здесь было перечисано все.
k1 и так далее, kcsm по k1.
Надо перебрать все-все-все пары.
Но только такие, которые имеют вот это пересечение.
И для каждой пары посчитать вероятность того, что она состоит из независимых множеств.
Давайте я напишу.
Это я успею написать, прокомментировать.
Смотрите, мы буковкой t теперь обозначим не то, что здесь обозначено, а размер пересечения.
Ну, естественно, он не больше, чем k1-1.
То есть давайте я сразу картину нарисую.
Вот мы вершин.
Мы выбираем одно множество мощности k1 и выбираем второе множество мощности k1 так, чтобы вот здесь было t общих элементов.
И t должно быть не меньше двойки.
Вот мы зафиксировали сначала само t.
Дальше мы выбираем из m k1-вершин для первого множества.
Потом из этих k1-вершин мы выбираем t-вершин, которые окажутся общими.
И, наконец, после того как эта общая часть двух сардельчик зафиксирована,
оставшуюся часть второй сардельки, естественно, выбираем вот здесь на множестве из m, минус k1-вершин.
Но выбрать нам остается k1-t-вершин.
Вот произведение трех таких цешек это способ фиксации какой-то пары с имеющей t общих элементов.
И теперь надо умножить это, ну как по линейности считают в мат ожидания,
на вероятность того, что и тут нет ни одного ребра, и тут нет ни одного ребра.
Что оба этих множества независимы.
Эта вероятность, это одна вторая в степени дважды csk1 по 2, минус cst по 2.
Ну, мы складываем, сколько есть ребер тут, которые должны отсутствовать, сколько есть ребер тут.
И, естественно, мы посчитали лишний раз те ребра, которые находятся целиком в пересечении.
Вот я их вычел.
Ну, следили?
Значит, смотрите, я сейчас не успеваю ни с комков нормально рассказать.
На следующей лекции, за первые 10 минут буквально, я расскажу, почему при t равном 2 мы получаем ровно ту самую асимптотику, которую ожидаем.
Вот эту. Она будет уже при t равном 2.
Она будет уже при t равном 2.
А дальше я с вашего позволения скажу без доказательства, потому что скучный анализ.
Я не хочу вас перегружать анализом, я хочу, чтобы вы усваивали идеи.
Скучный анализ показывает, что все остальные слагаемые до какого-то момента сильно меньше,
поэтому сумму можно оценить главным членом асимптотику, которого мы нашли, вот тем, который при t равном 2.
Умноженным там на сумму какой-то бесконечной убывающей геометрической прогрессии.
Настолько хорошо все последующие члены оцениваются по отношению к предыдущему,
что там получается убывающая геометрическая прогрессия, которая на асимптотику не влияет.
Но это до какого-то момента. Потом наступает перегиб, и там снова эти штуки начинают расти.
Но если посчитать правый конец, вот эту верхнюю границу суммирования, там все-таки будет сильно меньше, чем при t равном 2.
В общем, вот эта хрень могла бы занять всю следующую лекцию. Естественно, я это делать не собираюсь.
Я посчитаю только t равно 2, а про остальное оставлю без доказательств.
Ну, я тоже так думаю. Тут идей нет. Главное, чтобы вы понимали, что при t равном 2 максимум, дальше оно настолько маленькое,
что все доминируется строго этим t равном 2. И все это считать никто на экзамене, конечно, не потребует.
Это маразм, это неинтересно. Но главное, чтобы вся суть происходящего понятна. Все на сегодня. Спасибо.

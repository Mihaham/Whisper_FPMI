Ну как вы догадываетесь, да, в этом семестре это у нас
последнее занятие.
Как-то я, конечно, рискну опять накаркать, но у меня
есть подозрение, что мы сегодня закончим пораньше.
То есть в какой-то момент у меня было ощущение, что
может быть мы даже в две пары, если честно, уложимся,
а может даже и быстрее.
Впрочем, как говорится, я всегда так говорю, а потом
как выясняется, сколько всего надо еще рассказать.
Вот так.
Ну поговорим мы сегодня о миностовах в неориентированных
графах.
Да, почему я это выточаю?
Потому что вчера об ориентированных графах как раз активно
разговаривали.
Ну а об чем у нас вообще задачка?
Ну задачка у нас в том, что дан неориентированный
на этот раз граф, неориентированный, но при этом взвешенный.
И нам теперь жутко интересно, хочется найти минимальный
остов.
Что такое минимальный остов?
Что такое остов вообще?
Остов – это у нас какой-то под граф, который с одной
стороны стягивает, собственно, все вершины, как говорят
по крайней мере, и с другой стороны остается связано.
Ну, формальное определение тоже можно писать, но я
думаю, что нет смысла этого делать, думаю, так все
прекрасно понимают.
Хотя можно сразу на самом деле спросить, кто когда-нибудь
решал задачу о поиске минимального остова в неориентированном
графе?
Хоть за какую-нибудь симпатику.
Так, кто когда-нибудь решал эту задачу за симпатику
ВЕ?
Ну да, особенно если учесть, что Е заведомо не меньше
В, минус один, то да, окей, хорошо, а кто когда-нибудь
решал эту задачу за В квадрат?
Так, да ладно, так, хорошо, а что, вы все только расколы
пишете, что ли?
Хорошо, кто решал эту задачу за ЕЛУГВЕ?
А за ЕЛУГЕ?
Так, ребят, что-то вы это, что, все, вы, вы, что-то это
все, к концу семестра уже это, ручки и все не поднимаются,
да, совсем?
А то вы как-то поднимаетесь, что вы это, как, ну да, нету,
как сказать, хорошо, ладно, а кто решал задачу за ЕЛОГЛОГВ?
Три ЛОГА, ну хорошо, а за два?
А ЕЛОГЛОГ, хорошо, а за 4 ЛОГЛОГА?
А, ну там же можно за, о, от Е как будет делать?
Рандомом?
Рандомом?
Нет, сегодня нас не интересует рандомный алгоритм, нет,
конечно, да, и замечательный, конечно, алгоритм, который
работает с отожиданием от Е и худшим временем работы
как от те самые ЕЛОГВСВ квадратов, это да, но его мы,
естественно, будем обсуждать в следующем семестре, соответственно,
в числе вероятностных алгоритмов, вот, так, ну окей, но давайте
разбираться о чем-то, а кто-нибудь когда-нибудь писал
ноту?
Нет, ну есть еще вариант Е плюс ВЛОГВ, кстати.
Конечно.
Так, но давайте говорить, собственно, об чем у нас
вообще речь.
Так, ну речь у нас о том, что, так, ну прежде чем говорить
хоть какие-то алгоритмы, на самом деле введем, конечно,
хоть маленькую теоретическую базу, потому что может возникнуть
вообще такая маленькая интересная задача, а как
вообще понять, что остов вообще минимальный?
Ну кроме как, ну давайте переберем все остовы,
сравним их веса.
Давайте найдем минимальный остов и проверим, что сумма
равна.
Ага, ну так, да, нам же алгоритмы, у нас еще ни одного алгоритма
нету, ну доказательного, по крайней мере.
Ну домашних там критерий.
О, ДК.
Так, хотя, конечно, какие-то критерии у нас в домашних
заданиях уже были.
Вот, действительно.
Ну вот, ну основной у нас действительно такой, то есть
такое маленькое свойство остов, ну вот, минимального
заключается в чем?
То есть у нас заключается в том, что если у нас есть
какой-то, допустим, остов, и неожиданно нашлось какое-то
ребро, которое соединяет, которое не лежит в этом остове,
тогда заметим, что оно как бы стягивает в остове цикл.
Ну, потому что между, если взять это ребро и путь по
остову между двумя ребрами, то получается цикл.
И тогда оказывается важное свойство.
То есть можно заметить, что если вес этого ребра
х, то на стягиваемом цикле все веса меньше либо равны
х.
Ну, если это минимальный, конечно, остов.
Ну, доказательство очень простое, что если хоть одно
из этих ребер больше х, то, соответственно, мы тогда
могли бы просто выкинуть это ребро из остова и добавить
это, и, собственно, остов от этого бы уменьшился.
Вот.
Ну, возникает такая простая естественная идея.
Вот.
Это с одной стороны.
Но, правда, с другой стороны, в домашнем задании возникает,
конечно, и более продвинутая задача.
Или не возникает?
Да.
То есть оказывается верное обратное, что если существует
действительно какой-то остов, то тогда утверждается,
что этот остов в графе минимальный.
Да.
Вот.
Действительно есть такое базовое свойство.
Но, впрочем, все эти базовые свойства основываются,
на еще более какой-то базовой теории.
Вот.
Но, правда, базовая теория подразумевает, что минимальный
остов это штука, которую можно искать жадно.
В каком смысле?
Вот.
Ну, идея здесь будет такая.
Значит, впервые за курс мы ведем понятие разрез.
Что такое разрез?
Это в нашем случае это будет просто разбиение множества
вершин графа на два не пустых множества.
Вот.
Ну, и также будем говорить, что ребро пересекает разрез.
Если, то есть будем говорить, что там ребро, вот это UVW,
пересекает разрез.
На этот разрез.
Если там, вот так скажем, U лежит в S, XOR, V лежит в S.
Вот.
Я так немножко по-умному сформулирую вместе.
Вот.
Ну, хотя, если внимательно присмотреться, то есть я
хочу просто сказать, что ровно одна из этих вершин лежит в доле S,
а другая, соответственно, в доле T.
Вот.
Ну, такая симметрия позволяет, собственно, не заморачиваться
об то, кто конкретно.
Вот.
Да, это называется разрез.
Вот.
Вот.
Ну, и у нас возникает, конечно, такая идея.
Ну, тут, правда, всегда надо очень внимательно формулировать,
потому что она очень хочет ее сформулировать так быстро,
употребляя меньше слов, и, называется, попасть в формальный бред.
Вот.
Утверждение у нас говорит так.
То есть, ну, оказывается, что вот жадность может работать
следующим образом.
То есть, пусть у нас есть какая-то, так сказать,
уже часть разреза.
То есть, вот есть набор ребер.
Ну, точнее, часть ребер, которые уже являются там частью
какого-то миностого.
Вот.
Проведем какой-нибудь разрез, который такой, что
никакие ребра этого подостого не пересекают разрез.
Вот.
Ну, тогда заметим, что, ну, вот.
Ну, этот разрез пересекают какие-то ребра.
Там другие какие-то ребра разные.
И среди этих ребер, наверное, есть минимальное.
Ну, вот.
Тогда есть, действительно, мистическое свойство.
Какое у нас мистическое свойство?
Что, оказывается, это ребро можно добавить в миностов.
Вот.
Ну, есть такая вот классическая идея, в общем-то, на которой
у нас все алгоритмы, в общем-то, и основаны.
Правда, но вправивается.
А как теперь это нормально сформулировать?
Вот что нам конкретно надо доказать?
Что существует минимальное основное дерево, в которое
содержится все, все дерево, которое были уже выбраны,
и наше новое.
Да, совершенно верно.
Да.
Да, приходится, конечно, немножко громоздко формулировать,
к сожалению.
Но что делать?
Да.
Ну, можно, конечно, пытаться избавиться от громоздкости
путем введения.
Давайте скажем, что ребро безопасно, если и дальше
эту громоздкость прописать.
Ну, вот.
Ну, там кто-то так делает, на самом деле.
То есть ни у кого там не было этой леббы, там формулировки,
там что-то, что-то минимальное пересекающее разрез ребро
безопасно для...
Не, не слышали такого?
Вот есть в мои времена вот нет-нет, такое звучало.
А, да?
Окей.
Ну, лебба называется что-то на безопасном ребре.
Ну, да.
Нет, ну, да, нет.
Ну, она вот поэтому известна, как лебба о безопасном ребре.
Я еще называю ее теорема о разрезе.
Поэтому кто-то приходит на экзамен начинает что-то
про потоки говорить.
Вот.
Честно скажу, меня это очень сильно не радует.
Поэтому, пожалуйста, не делайте так.
Вот.
Как бы потоки у нас в следующем семестре будут, если что.
Там этих всех разрезов навалом будет.
Вот.
Значит, итак.
Значит, точная формулировка.
Итак, лемма...
Ладно, давайте этим.
Лемма о безопасном ребре.
Вот.
Лемма о безопасном ребре.
Что это такое?
Ну, вот.
Ну, лемма о безопасном ребре говорит так.
Итак.
Итак, пусть у нас, значит, ну, есть, соответственно,
граф.
И пусть у нас оказывается какое-нибудь T под множество
E.
Это под множество...
Это ребер.
Внимание.
Какого-то...
Мина 100.
Нет, ладно.
T это плохо.
Потому что у нас ST это разрез.
Так.
Какую бы букву взять?
Ладно.
Давайте следующую.
Ладно.
Не будем долго думать.
Будет буква O.
Значит, пусть будем говорить у нас действительно ST.
Это разрез.
Такой, что не одно из ребер.
У не пересекает разрез.
Вот.
Ну, не пересекает.
И вот.
СТ.
Вот.
Тогда оказывается, значит...
Значит, да.
То есть дальше теперь.
То есть пусть уже третий раз.
Пусть.
Но что делать?
Там пусть E.
Значит, ребро с минимальным весом.
Пересекающий разрез.
Вот.
Тогда.
Тогда у нас важно.
Оказывается, что U, объединенное с E, это тоже под множество
ребер какого-то мина 100.
Будьте здоровы.
Какого-то мина 100.
Вот.
То есть надежная формулировка будет скорее такая.
Ну, в принципе, заметьте, что да, надежность формулировки,
такая сложность окупается, конечно, простым доказательством.
Потому что, ну, как это доказать?
Потому что рассмотрим здесь какой-нибудь мина 100.
И, предположим, выяснилось, что у нас, выяснилось, что
вот это ребро, оно в этот мина 100 случайно не попало.
Что мы тогда сделаем?
Так, ну, возьмем какой-нибудь тут мина 100.
Вот.
И заметим, что в мина 100 тогда вот это ребро будет
стягивать цикл.
Значит, что у нас тогда получится?
То есть получится E и какой-то путь.
В общем, какие-то ребра у этого пути зелененькие,
какие-то красненькие.
Сколько, кстати, красненьких мы не знаем, потому что чисто
теоретически этот путь может зигзаром ходить туда-сюда.
Но мы гарантируем, что хотя бы одно, то есть хотя бы одно
ребро, которое не было еще в этом множестве, конечно,
найдется, потому что хоть кто-то на этом пути хотя
бы раз пересечь разрез да должен.
Вот.
Но тогда мы можем сделать вот что.
Тогда у нас, если у этого ребра вес W, то что мы тогда
можем, какой мы вывод можем сделать про эти красные
ребра?
Как красные ребра?
С одной стороны, они все меньше либо равны W, почему?
Потому что они пересекают разрез, а ребро E минимально
по весу среди пересекающих разрез.
С другой стороны, они все должны быть меньше либо
равно W, почему?
Потому что это минимальный остов.
Если бы тут… Нет, стоп, наоборот.
Все, наоборот.
Они меньше либо равно W, потому что это миностов,
но если бы тут кто-то был строго больше W, то мы бы
это ребро подменили бы на E и миностов уменьшился
бы.
Вот.
Но с другой стороны, все они больше либо равно
W, почему?
Потому что W там минимально среди всех пересекающих
разрез.
Но тогда вывод простой.
То есть, каждое красное ребро получается по весу
равно W, и мы на самом деле в этом минимальном остове
можем подменить это ребро на это и подменить просто
миностов на другой.
То есть, как видите, если у нас есть равные ребра
по весу, то в принципе может быть много миностов.
Ну, типичный пример, полный граф, где все веса равны,
скажем, единичке.
Тогда у нас там этих миностовов там просто страшное количество.
У тебя страшное количество это сколько?
О, классно.
Любое подносит, что прям это дерево?
Да, минус два, да.
Нет, я говорю, а у нас сам равной N квадрат.
Вот прям полный граф.
Все весоребер равны, сколько у нас миностовов?
Ой, чего?
У вас точно этого на экзамене не будет?
Ну, теперь думать надо.
Ты перебрался случайно.
Не, не, не.
Не, ну, я не знаю.
Слушайте, просто не знаю.
В мои времена как бы на экзамене возникало такое
словосочетание, как коды приуфера там, теория макейли,
там N в степени N минус два.
Не?
Так, ну ладно, может у вас...
Ну, значит это у вас ладно.
Если сейчас у вас этого дискрайне не было, значит будет.
А, ну вот.
Это было неделю назад.
А, ну, видимо, да.
А, ну да.
Неделю назад, видимо, никто не ходил в дискран.
Ну, окей.
Ладно.
У вас это все еще ждет.
Понятно.
Да.
Вот.
Ну, хорошо.
Ладно.
В нашем случае у нас есть, конечно, важный инструмент,
который уже позволяет нам там более-менее как-то
более-менее активно работать.
Значит, смотрите.
Значит, потому что просто уже на основе этой леммы,
в общем-то, там можно придумать кучу разных жадников.
Вот.
Ну, мы пойдем, как бы, в историческое там порядке,
в котором они появлялись исторически.
Поэтому...
Ну вот.
Поэтому начнем мы с алгоритма Борувки.
О.
То есть алгоритм Борувки.
Вот.
Такое вот красивое слово.
Да.
Редкий случай алгоритма датируется 1926-ым годом.
Да.
Компьютера вообще не было, а миностов уже искали.
Да.
Вопрос чиркафтников.
Как вы думаете, в каком контексте вообще Борувке
вообще могло потребоваться решать такую задачу?
Да нет.
Нет.
Там не трубы.
Там другое.
А дороги какие?
Нет.
Нет.
Если бы это все электросетчи в Словакии.
Да.
Вот задача.
Желательно...
А там было что-то типа соединить Словаки в единую сеть.
Какая-то электрическую.
Желательно подешевле.
Вот.
Ну или что-то в этом вроде.
Ну вот.
Ну вот.
И в принципе у Борувки и Вазикла вот действительно
такая интересная, такая мысль.
Как это можно делать?
Так.
Значит, вот рассмотрим.
У нас какие-то есть вершины.
И вот рассмотрим.
Рассмотрим какую-нибудь первую вершину.
Так.
Вот из нее торчат какие-то ребра.
Правда?
И какой-то из этих ребер минимальное.
Ну.
Так отлично.
Давайте его добавим в миностов.
Оно в каком-то миностове откровенно лежит.
Ну тирема у нас, тирему можно применить скажем вот
к такому разрезу.
Правда?
Понимаете, да?
Но заметим, что не только для этой вершины минимальное
ребро лежит в каком-то разрезе.
Вот там в миностове каком-то.
Ведь для любой вершины можно взять минимальное
ребро.
Просто за УАТЕ пробежаться и для каждой вершины найти
минимальное ребро.
Так вот.
Мистическое утверждение.
На самом деле давайте пробежимся и для каждой вершины найдем
минимальное ребро и честно добавим его в остов.
Вот.
Вот такая идея.
Правда нам надо вот.
Ну правда вот.
Вот действительно мистическое утверждение.
Но правда нам придется доказать для того чтобы доказать
что это валидно.
Нужно сказать что-то более сильное.
То есть там конечно каждый из этих ребер в каком-то
миностове откровенно лежит.
Но нужно доказать что оно лежит.
Что на самом деле все они одновременно лежат в каком-то
миностове.
Но сами понимаете это не одно и то же.
Вот.
Ну как это можно?
Ну можно свести к домашнему заданию.
Ну сказав скажем что пусть все ребра попарно различны.
А если они не попарно различны то легко выкрутиться скажем
что давайте прибавим к ним супер микроскопический
эпсилон умноженный на порядковый номер ребра скажем.
Вот.
И тогда у нас и тогда мы уже знаем что минимальный
остов один.
И тогда получается что если про какие-то ребра мы
знаем что они лежат в миностове то получается они все
в нем одновременно лежат.
Вот можно прям вот так в наглую выкручиваться.
Ну правда для этого на экзамен я вас потребую доказать
что миностов реально один при попарно различных ребрах.
Вот.
Поэтому мы лучше сделаем попроще.
А как сделать попроще?
Ну очень просто.
Смотрите.
Вот берем вершину.
Так.
Минимальное ребро у нее торчащие из нее это вот минимальное
ребро вот такого разреза.
Отлично.
Взяли ребро добавили в разрез.
Да.
Теперь рассмотрим вот для этой вершины.
Может быть так что для нее вот это ребро тоже минимальное.
Логично да?
Вот.
Ну если так то в этом месте останавливаемся типа идем
дальше.
Но если у нее минимальное ребро другое то оно минимальное
не просто как торчащие из нее но оно минимально вот
в таком разрезе.
Правда?
Потому что из этой вершины получается таких ребер не
торчит.
Если выяснилось что это ребро не минимально вот для
этой вершины то так собственно и идем.
То есть вот получается ну в какой-то момент цепочка
конечно закончится.
Ну правда тут если вы не аккуратно работаете с
одинаковыми ребрами то у вас конечно может произойти
зацикл.
Поэтому я и говорю что когда вы сравниваете ребра нужно
прям строго сравнивать.
Что если веса ребра одинаковые сравнивать прям по какому-нибудь
айдишнику или чему-нибудь еще в этом роде.
Так.
Ну хорошо.
Допустим вот цепочка закончилась.
Чего мы тогда.
Ну вот.
Вот спрашивается что можно сделать в этом случае.
Вот.
Ну можно на самом деле сделать дальше.
Берем вершинку.
И тоже из нее находим какой-нибудь там минимальное торчащее
из нее ребро.
Вот.
Ну можем же такое сделать.
Почему бы нет.
Так.
Тоже находим.
Пока вот разрез такой.
Потом разрез такой.
Ну и дальше то же самое.
Либо цепочка сама замкнется.
Либо что еще может произойти.
Ну да.
То есть на самом деле вот тут прикол такой, что мы что
как это ни странно.
Да.
Мы на самом деле можем упереться.
Упереться в то что мы уже нашли раньше.
Но ничего страшного в этом нет.
Вот.
То есть.
Но на самом деле я говорю, что суть в том что давайте.
Ну вот.
То есть это мы так связанным говорим, что давайте вот
так вот честно пойдём.
То есть это мы так.
Потому что на самом деле как бы алгоритм будет заключаться
в том.
То есть алгоритм будет делать следующее.
Он для каждой вершины найдет минимальное ребро.
как я сказал строго минимальное вот значит эти ребра образуют какие-то вот связанные наборы
вот про них можно только сказать что как бы в каждой компоненте связанности относительно
таких ребр хотя бы две вершины есть правда ну а теперь это давайте это все сожмем чем нет мы не
будем использовать s&m какой-то там если вам пришла в голову случайно какая-то аббревиатура от
которой вы случайно когда-то услышали нет зачем мы просто за оте сожмем эти компоненты в одну там
собственно там каждую в одну вершину вот и тогда заметим что у нас как бы количество вершин в
графе уменьшилось хотя бы в два раза потому что в каждой компоненте хотя бы две вершины ну потому
что каждая вершина хоть с кем-то ребром да связан понимаете да вот понимаете вот да нет наверное
есть кто живой вот да наверное окей и внимание вопрос что же теперь делать ну вот но идея
действительно такая да что просто давайте вот эту вот операцию будем повторять до тех пор пока
у нас не останется одна вершина кстати эта операция носит отдельное гордое название шаг борувки
да прям вот отдельно прям это вот отдельно лучше это помнить потому что в некоторых алгоритмах там
ну и сегодня у нас там еще будет и например буду и вот кстати в уже упоминавшем алгоритме за от
е вероятностном тоже на самом деле вот сам по себе шаг борувки будет играть важную роль
ну да логично вот ну да она дарит дарит дарит и стучи я рассказываю теорию по до после того
как вы поделали домашнее задание да но что делать к сожалению да то есть общий разговор тут надо
ввести потому что но что что чтобы от этого отталкиваться но вот как бы шаг борувки подчеркиваю
есть да вы уже с этим столкнулись да в некоторых алгоритмах действительно оказывается полезно не
делать это до конца хотя в принципе уже неплохой алгоритм на самом деле есть потому что количество
таких шагов будет не более чем логарифом и получается что это будет работать за о от е лог в вот это если
вы просто делаете вот это вот в тупую без лишних заморочек вот ну по написанию может не самые
приятный алгоритм но там по крайней мере по сравнению со всякими прим и микроскалами впрочем
как бы все но то но прочим ничего сложного тут особо нет вот то есть принципе вот такой вот
маленький приятный алгоритм вот но говорит пока что да есть приятность в том что да то есть под
е лог вы конечно это надо еще там какой-то тест подгонять потому что что было е лог в это надо
чтобы у вас там вот эти компоненты жимаемые вершины действительно оказывались каким-то вот
этим вот про сочетание вот потому что в реальности понятно что там на чем-то более рандомном
возможно вам будет вести компоненты будут сильно больше и тогда и шагов будет меньше вот но если
добавить что как бы еще этот алгоритм параллельцы и плохо но сами видите да но в том плане что ведь я то
есть мы говорим что потому что вот эти вот минимальные ребра из каждой вершины можно и параллельно
искать но я сейчас не буду вдаваться там в теорию там какой-то в то в чем там мало разбирались но как
бы теоретически вот в параллель там если вы там будете на каком-то очень большом графе института
пытаться строить миностов то на самом деле вот такой шаг борувки может вам сильно помочь вот на
самом деле вот и так но вот это вот у нас это один классический алгоритм вот так но хорошо как вы
тяну в 50-х годах есть когда появились конечно чуть другие алгоритмы какие но в первую очередь
сейчас будет интересовать алгоритм прима вот да прим это фамилия естественно они там знаете
типа типа алгоритм там алгоритм прима да потому что там потому что мы крутые ну или там любим
кукс кто мне не знаю что-нибудь еще в этом роде вот значится что еще можно значит алгоритм
прима да это уже где-то 50-е годы какие-то там вот ну и здесь предлагается такая красивая тоже
такая же там технология жадного набора в чем-то схожая с алгоритмом дэйкстре потому что мы
говорим так вот у нас есть какие-то вершины мы говорим так возьмем стартовую вершину так
ну-ка давайте-ка возьмем минимально это чаще из нее ребро вот так его может добавить во 100 так
отлично вот у нас есть такая компонента давайте рассмотрим вот такой разрез и попытаемся найти
ну то есть найдем минимальное ребро который можно прицепить к этой компоненте и соединить ее с
кем-то еще так ну нашли так ну дальше теперь рассмотрим вот такой разрез и прицепим кого-то
еще давайте для удобства вот это ну там потом вот это но и так далее ну прицеп понятен то есть
у нас каждый момент времени есть компонента связанности с эсом и мы хотим расширить ее на
нет мне не сказал я сказал что это жадный на что такой жадный экстенсивный набор в том
плане что мы от одной вершины стартуем вот от одной вершины как бы расширяем
мостов захватывая все новые новые территории нет вот нет вот это оно что вот так бы не
за вот мне как-то вот интуитивное ощущение принципиально другого ну да ну хорошо ладно
можно действительно сказать что мы от каждой вершины делаем просто такой шаг в баровке тут
мы просто делаем шаги только от одной вершины да хорошо наверно можно и так сказать вот
но теперь вот это про закройся точку такой алгоритм работает как напишут да нет ну господи какой
подвох есть я далек от я далек от мысли шоу как бы там есть ну да но вот давайте так вот
но вот ну смотрите нет просто заметил что вот потому что я рассказал самая тупая реализация
она вообще вот я почему за ве спрашивал потому что тупая реализация вообще завоевать будет
работать кто-нибудь когда-нибудь писал такой алгоритм за ве ну как-то да но видимо но не
знаю потому что вот ну видимо смотря какая интерпретация обычно конечно когда рассказывает
этот алгоритм конечно вам сразу быстро рассказывает что на самом деле его можно
реализовать а-ля до экстра а именно давайте для каждой вершины будем хранить минимальное
ребро к которым можно эту вершину подцепить к имеющейся компоненте то есть самое то есть
когда мы то есть на каждом шаге мы часто перебираем все вершины находим вершину с минимальным
ребром подцепляем за это ребро и из этой вершины делаем релаксации вот и это тогда уже начинает
работать за о от е плюс в квадрат так ну-ка кто писал за такую симпатику да ладно да ладно
никому не приходилось писать честного прима не всегда нет там просто все гораздо проще была
потому что есть классическая задача значит найдите миностов в графе на плоскости а именно вам даны
пять тысяч вершин вершины точки на плоскости значит вес каждого ребра это расстояние между
точками то есть в этой задачи вы краскала искуете просто не упихнуть или вы упихали
нет но но действительно тут как всегда да сразу как-то е плюс в квадрат да хорошо если краскал
нашу так хорошо ладно тогда смысл это вопрос а кто-то кто вообще писал этот алгоритм хоть
в какой-то интерпретации вместо прима да конечно ну ладно сейчас изучим ну вот так ну как сказать
да как говорится просто есть алгоритм с дэкстрой а есть алгоритм с сортировкой и это точно но вот
разница есть ладно но тут ну хорошо это вот е плюс в квадрат но на самом деле если е порядка
в квадрат то оптимальному пьем оптимальнее в общем-то вы не придумаете в принципе очевидно
но если у вас есть там меньше чем в квадрат то что можно сделать то можно конечно задумываться
на тему того что в квадрат нам нужно чисто для того чтобы искать минимум нельзя ли искать
минимум оптимальнее ну самый тупой вариант давайте не просто для каждой вершины поддерживать это
минимальное ребро но эти минимальные ребра хранить в какой-нибудь куче или даже самое тупое в
каком-то сете даже чтобы удобнее было так если поддерживать их сете то получится асимптотика
е лог в почему на е лог в потому что нам надо не только доставать минимум но и к сожалению еще
и релаксировать а по каждому реблу релаксация может стоить логарифом если это будет реально
релаксация так но кстати е лог в ой ну да вот мы уже достигли кстати асимптотики барвки вот
а можем здесь еще но вот но на самом деле ой там самый смешной что здесь мы еще можем развлечься
в чем весьма неожиданно первое развлечение достаточно простое
мягкая куча нам тут не по не хотя нет стоп стоп стоп нет вот тут к сожалению вот дайта это моя
мечта чтобы тут могли применить мягкую кучу нет а чей так потому что ради чего изобрели мягкую
кучу мягкую кучу изобрели ради того чтобы действительно построить миностоп за крутое
время это реально собственно практически вот я не знаю по моему это едва ли вот основной ее
применение асимптотика там кстати е там на обратную функцию кирмана и сэмэ вот там нет
вот нет но объяснение простое ладно правильно на самом деле так там на самом деле просто это
асимптотика такая м умножить на значит какую-то функцию бета какую-то экзотически растущую а потом
доказывается что эта штука больше обратной функции кирмана классической не более чем в 9 раз вот нет
слушайте это еще не самое смешное что там да ну конечно не оно не она потом из уважения это потому
что понятно что там чизель это явно наверно старья нам обсуждал и лично скорее всего поэтому
почему бы нет вот но жалко к сожалению пока этот алгоритм не вошел в наш курс вот потому что
ну честно скажем он не менее сложен чем сам софт хипп вот к сожалению как выяснилось то есть
там нет такого что вот если у вас есть в кармане софт хипп то как бы это алгоритм на халяву построили
нет там приличный такой сложности алгоритм но утверждает что для текущих момент это лучший
алгоритм со известной асимптотикой с детерминированной вот это ваше уточнение
потому что ладно если уж говорить о том что вообще есть потому что есть в этом смысле есть
вообще есть вообще вообще есть самый лучший алгоритм зова от нуля нет нет начитались алгоритм
который говорит что мы умеем доказывать что наш алгоритм это лучший по асимптотике алгоритм
которые вообще в природе можно искать минус то вы но вот маленькая беда что это за лучшая
асимптотика науки неизвестно да вот да вот такой шедевр да вот то есть вот такой шедевр я
вот то есть я не знаю кто-то тогда в какой-то версии курса даже по моему кто-то пытался начинать
на каких решающих деревьях собственно этот алгоритм основан и как он работает но вот
алгоритм известный и он фане можно реализовать ну реализовать можно что-то скорее всего нет
то есть все нормально то есть нормальный алгоритм но думает с прощем да хотя да и тут понимаешь
что тут уже мечта может это когда-нибудь это организовать это спецкурс какой-нибудь хотя
тут всегда можно хотя вот там можно спецкурсов я там видимо спецкурс все алгоритмы по мина столам
там все алгоритмы по потокам но у нас этот у нас там мини-курс у нас с saved скажу у нас алгоритмы
потока вообще там я думаю как минимум месяц займу но там потому что там действительно красивая
теория на самом деле то можно действительно хорошо и глубоко изучать но и рядом у нее там
соседи ну про сочетания конечно в общем про сочетание такой мелко связи то есть на самом
Теория про сочетание – это уже мелкое, относительно маленькое следствие теории Форда Фулкерсона, на самом деле, но это ладно.
Собственно, может, кого-то иногда удивляет, хотя уже никого не удивляет, почему у меня в этом курсе графов нет курса про сочетание.
Вот казалось бы, что бы мне сейчас не потратить 15 минут и не рассказать вам алгоритм Куна.
Так я же прям ровно обитаю Форда Фулкерсона.
Ну вот, да, именно.
И мы в этом, собственно, убедимся.
И более того, да.
То есть, на самом деле, можно его доказать просто, но это получится из цикла.
Есть алгоритм, вот доказательство, что он работает, но как бы к нему пришли – вот вопрос.
Хотя отдельный вопрос, как к нему пришли, может, реально эти цепочки придумали.
Так вот, ладно.
Так, в общем, есть такие развлекухи.
Ну ладно, у нас мы сегодня ограничимся, видимо, более простой развлекухой.
У нас будет развлекуха вида.
Ну вот.
Значит, ну сейчас вот подойдите, сейчас найдем.
Значит, пока у нас был геолог В, это у нас, напоминаю, прим с сетом.
Но если мы вставим вместо сета кучу фибоначи, вот.
Вот, то получится алгоритм Е плюс В лог В.
Ну, потому что, ну, собственно, аналогично Дэйстре.
Давайте, а вот это давайте вспомним, потому что это нам сейчас будет пригождаться.
Потому что за счет чего тут Е плюс В лог В?
Ну, за счет того, что, помните, чем приятна куча фибоначи?
Она приятна тем, что вы, если ваша задача не удалить элемента, только уменьшить его,
то мы это делаем не за алгоритмом, а за единицу.
Помните, да?
Вот, помните?
Вот, да.
А, ну да, у кого-то из вас это даже было в вопросах на ОТЛ.
Да.
Как это, как говорится, один из шести билетов кому-то повезло.
Потому что, по-моему, это был едва ли не самый халявный там билетик, по-моему.
Сейчас с кучей фибоначи.
Ну да.
Там очередь на шести стеках была самая.
Там уже на шести стеках, потом, типа, проехали два билета, кажется.
Да, что, очередь на шести стеках была легче?
Да.
Ну, значит, все субъективно.
Ну окей.
Нет, ну хорошо, хорошо.
Тут на вкус и цвет, там как бы.
Ну хотя ладно, там, в общем-то, что там.
Нет, в этом смысле, кто-то, наверное, скажет, что там и всякая вот эта фурятина тоже была не сильно убойная, на самом деле.
Уделение было неприятно.
Неприятно?
Ну, что делать?
Хотя вот философский вопрос.
Нет, ну сейчас, хотя, на самом деле, после курса мотонализа с рядами Тейлора, на самом деле, вам бы, наверное, не показалось это так страшно.
Нет?
Ну да.
Ну да, это деление чисел.
Деление многочленов, оно и не вопрос, но вот он стоял.
Потому что это как бы, да, там и несложно.
Ладно.
Но мы сейчас попытаемся это проапгрейдить.
Значит, наша ставочка.
Значит, у нас сейчас будет вообще неожиданная симптотика.
Значит, мы сейчас попробуем придумать алгоритм, который работает за О от Е, если Е больше чем В лог лог лог В.
Вот.
Ну, ну, как сказать, вы, конечно, скажите так, можете считать, что алгоритм работает за...
Ну, как сказать, можно, наверное, считать, что алгоритм работает за там...
Нет, хотя нет, так вы не скажете.
На самом деле, так, официальная симптотика этого алгоритма, прям полная.
О от Е лог звездочка В.
Так, не путать с обратной функцией актермана, она к ней никакого отношения не имеет.
И СНМ-а сейчас не будет.
А почему Лог звездочки часто относятся к ней как-то?
В смысле, относятся.
Не, ну, смотри, почему часто говорят, подрываются с собой звездочки, не путать, типа сорвать функцию актермана, это же разные вещи.
Ну, вот поэтому и...
Нет, нет, я это упоминаю просто чисто потому, что...
Потому что как бы про СНМ известно, что он работает за Лог звездочку и что он работает за обратную функцию актермана.
Поэтому как бы...
Ну, так как обычно доказательством очень редко кто заморачивается, вот.
То, соответственно, мы и понимаем, что Лог звездочки...
Ну, вот, поэтому как бы можно и перепутать.
Это чисто на всякий случай.
Потому что так-то, конечно, да, если копнуть, то мы понимаем, что это принципиально разные вещи.
Итак, но на всякий случай, кстати, а что такое вообще Лог звездочка?
Да, что это такое?
Да, значит, официальное определение у нас будет такое.
Лог звездочка.
Это...
Значит, Лог звездочка В.
Это минимальное такое K.
Такое что?
Лог два, лог два, лог два и так далее.
Лог два, вот так вот, в K раз.
Будет.
Ну, скажем, меньше единиц.
Вот.
Ну, на самом деле, там, как сказать, с точки зрения Олимпиада...
Обычно чем нравится обратная функция кирпада?
Тем что она очень, очень, очень, очень, очень, очень, очень, очень, очень, очень, очень...
медленно растет.
Вот.
Но этих очень настолько много, что и доказательства
там достаточно сложно, настолько сложно, что мы сегодня,
к сожалению, не доживем, хотя вот жалко, в прошлом
семестре дожили, если честно, и там выяснилось, что там
несильный убой, но, впрочем, тут я вас отсылаю либо к
лекциям прошлого года, либо к лекциям Павла Маврина,
кстати.
Да, если говорить еще об одной альтернативной литературе,
там кого можно почитать, послушать, то, как бы, на
самом деле, в 2021 году, на самом деле, Паша Маврин
выпустил там просто целый, просто тоже курс алгоритмов.
Ну правда, насколько я понял, это не был там, ну я не понял,
что это был за курс, это, по-моему, не совсем был там
какой-то внутриэтмошный курс алгоритмов, и более
того, и там еще часть лекций шли еще на английском языке.
Ну вот, ну я не знаю подробностей для кого, но, по крайней мере,
там, если в Гугле, там, в Ютубе наберете, вы там легко
найдете.
Вот.
И там в качестве одной, в качестве одной из лекций
было реально рассказано, на самом деле, то есть там
реально рассказывалось доказательство того, почему
эта работа, СНМ работает за обратную функцию ОКИРВАН.
Вот.
Но правда, сам по нот, правда, но правда, сам Паша активно
при этом отсылался к оригинальной статье, угадайте, кого?
Тарьяна?
Конечно, да.
Да.
Ну вот, собственно, к оригинальной статье Тарьяна очень советовал,
чтобы ввести статья замечательную.
Да.
Так что, если кому-то интересно.
Вот.
Так вот.
Но, с точки зрения Олимпиад, обычно такого не надо, потому
что, на самом деле, нам, ну, потому что обычно в СНМе
что нам надо?
Нам надо, что она работает, то там почти за от единицы.
То есть, логарифм звездочка может не так медленно растущая,
то есть она, то есть, логарифм звездочка, это, к сожалению,
всего лишь очень-очень-очень-очень-очень-очень-очень медленно растущая
функция.
Вот.
Но для жизни нам этого хватает.
Ну, как бы, уровень такой, что при реальных ограничениях,
ну давайте вот подумаем, сколько, к чему должно быть
равно В, чтобы у нас лог звездочка было равно, например,
там скажем два.
Шестнадцать.
А, не сто.
Где четыре?
Ну вот.
Ну давайте так.
Вот давайте для удобства я тут меньше либо равно
напишу.
Вот.
И тогда получится, что для того, чтобы у меня было
лог звездочка равно два, у меня должно быть два в
степени два в степени два.
А, ой.
Потому что я молодец.
Вот.
Да.
Итак, два в степени два.
Это тогда будет двоечка.
Чтобы лог звездочка была троечка, нужно, чтобы было
два в степени два в степени два.
Это сколько?
Шестнадцать.
Ну вот, это шестнадцать.
чтобы была четверочка нужно 2 в степени 2 в степени 2 в степени 2 это
да 65 тысяч 536
а чтобы было больше надо чтобы было два вот в этой вот степени
ну я не знаю там как бы там что произойдет раньше как бы мы начнем реально работать с такими объектов
количество с такими графами или у нас земля накроется наконец через 5 миллиардов лет но
соответственно
не
нету мало ли ну и есть еще темная материя мало
может
нету как сказать мало ли молекула шесть чего-то состоит молекула
ну да нет ну как-то говорят да как говорят ученые да как-то ученые мы открыли как-то мы открыли молекул
расщепили ее на атомы атомы на атомные ядра и электроны ядро на к варке а что дальше
из чего состоят к варке
ученые сходятся в одном из какой-то хрень
вот
нет ну да но как бы все научно популярный фильм до хрень называется да сенсационный фильм там а сенсационных открытиях сенсационологов да так что
вот помните да там
мало ли причем там куда ученые там важные говорят то есть там это как выясняется такий скинститут изучение хрень и других областей физики там например да
вот ну неважно ладно это отдельно там посмотрите замечательная вещь вот
соответственно
значит это у нас такое вот то есть принципе а то есть нас это тоже устраивает что как бы в реальной жизни
там меньше там как бы больше чем четыре вы вряд ли встретить
в обратной функции акирмана круче вы в реальной жизни вряд ли встретите больше чем два
да вот но как бы в реальной нам жизни в общем-то не сильно принципиально
но вроде еще да
ну да но я сейчас слету не воспроизведу определение
но там определение было такое что дата выяснялось что для того чтобы воду там
что там шо то но то есть для того чтобы вот этого но так и так для того чтобы
т stains должны быть там какие-то очень экзотическими большими чтобы там там какая-то функция
это обратная функция кирпада было чем-то вам двойкой какой-то
то есть там тоже какие-то таблицы там в какие-то степенные там возрастание только еще еще в более крутой версии
вот мне важно
Ладно, значит, мы пока перейдем к этой вот штуке, попробуем, на самом деле, найти и решить задачу за вот такую асимптотику.
Итак, вас приветствует алгоритм Фредмана Тарьяна.
Да.
Как говорится, да, Тарьян снова с нами, да.
Так, ну ладно, Тарьян, в общем-то, никуда не уходил.
Да, ну как бы, как же, на самом деле, да.
Как это, в общем-то, как эта теория алгоритмов без этого доброго дядечки, на самом деле.
На примере, ну, хочется мечтать, я, конечно, там, конечно, вообще не знаком с этим человеком, хотя, может быть, хотя, я не знаю, может, вы продвинуты, может, кто-то из вас слушал его лекции где-нибудь на ютубе, я не знаю.
Потому что, как бы, как мы уже, по-моему, выясняли на прошлой лекции три недели назад, что это вполне себе ныне здравствующий там дедушка, работающий в Принстоне, вполне себе.
Так что, да, есть подозрение, что он там, может быть, что-то и читает. Может, конечно, уже и нет, там у него, вон, пошло этих вот учеников, там творчество, которое, там, творчество, которых мы тоже, в общем-то, уже активно изучаем.
Наверное, тоже много, но, впрочем, вот.
Так вот. Итак, вас приветствует алгоритм Фредмана Тарьяна.
Фредман Тарчан.
Итак.
Значит, что мы сейчас будем делать?
Значит, нам хочется, конечно, вот у баровки есть эта красивая идея, что, действительно, мы делаем шаг в баровке, там, допустим, и уменьшаем там количество вершин в два раза.
Вот. Мы здесь попытаемся, конечно, чтобы у нас количество вершин, добрый день, уменьшалось в какое-нибудь более приличное количество раз.
Значит, смотрите. На первом шаге мы попытаемся сжимать следующим образом.
Значит, смотрите. Внимание. Так, внимание.
Так. Хотя нет, на всякий случай скажу.
А ну, поднимите руки, кто знает этот алгоритм.
А то мало ли, что там в пятнадцатой группе ЛКШ рассказывает, я ж не знаю.
Вот. Нет?
Нет.
В пятнадцатой группе ЛКШ рассказывает то же самое, что и в шестадцатой.
Логично.
Окей.
Но это да. Но шутки шутками, а на самом деле в продвинутой группе ЛКШ реально много что рассказывает, поэтому, как бы, поэтому мало ли.
Поэтому мало, ну вот. Поэтому мало ли.
Ну окей. Значит, смотрите. Так. Фредман Тарьян вас приветствует.
И говорят следующее. Так. Значит, алгоритм будет говорить так.
Значит. Возьмём стартовую вершину, и запустим честного прима.
Будем запускать честного прима. Прям вот работать-работать-работать-работать-работать-работать-работать-работать.
Но! Мы остановимся.
Когда мы остановимся?
работать, работать, работать, работать, работать, но мы остановимся, когда мы остановимся? Мы остановимся в
тот момент, когда у нас размер компоненты станет равен k. Понятно, да? Вот, значит, ну, если это k, конечно,
покрыло прям всё, то нам повезло, мы радуемся, мы нашли миностов. Если такого не произошло, что мы
делаем? Мы берём вершину, которая в это не попала, и начинаем из неё абсолютно ту же процедуру. Тоже
её расширяем, расширяем, расширяем, расширяем, расширяем, и опа, она достигла размера k. Мы
останавливаемся, ну вот, и делаем дальше. Что дальше? Если ещё какие-то вершины остались непокрыты, мы
ещё их берём, берём, берём, берём, берём, берём, опа, так, ладно, если произошло, ну, могло так произойти,
что, как бы, если делать честного прима, то произошёл подцеп к тому, что было раньше, да? Ну, ничего
страшного, ладно, если так произошло, ну ладно, объединим и пойдём дальше. Вот, удобно, правда? Вот,
удобно, да? Мы делаем, мы фактически делаем алгоритм прима, но при этом, если в текущей компоненте уже
размер k, то мы останавливаемся, берём незахваченную вершину и делаем из неё. Вот, то есть так делаем,
пока не покроем все вершины. Получилось несколько компонент, каждый из которых размера там, ну,
что-то типа k. Вот. Теперь, внимание, вопрос, за какую асимптотику от VEK вы можете реализовать такую штуку?
Ну, в принципе, да. Нет, нет, ну, нет, почему yellow k? Так, ну, VEK, да, заметим, что если мы гарантируем,
что мы ограничиваем k, то заметим, что в потенциальном сете, в котором минимум мы ищем,
мы можем хранить только k минимумов. То есть, если у нас размер ak плюс 1, то мы максимум просто можем выкидывать,
правда? И получается, что такой шаг мы можем сделать за асимптотику E log k. А могли бы и побыстрее.
Так, ну, да. Ну, VEK, да, VEK это идея такая, что у нас есть, что можно вместо сета использовать кучу фибоначи.
Ну, вот. То есть, просто кучу фибоначи следить. Ну, вот. И тогда, казалось бы, мы вершины достаем V раз,
а там как-нибудь добавляем и как бы релаксируем их, делаем декриски E раз. Получается E plus V log k.
Правда, тут есть маленький подвох. А для этого у нас была E log k. Почему такая была асимптотика?
Потому что сет. Ну, идея такая. Как мы делаем примо? Делаем примо сетом, да? То есть, храним в сете,
ну, хотелось бы хранить все вершины. Ну, в идеале, просто тупой алгоритм примо сетом как работает.
Храним все вершины, которые мы можем подцепить, и для каждой вершины храним, собственно, ребра,
минимальное ребро, да? Вершины сортируем по этому ребру, да? Вот. Но E log k более читерская вещь,
потому что мы говорим, что так как у нас размер не более чем k, то мы знаем, что в сете не имеет
смысла хранить не все, а только k минимум. То есть, мы говорим, что если мы что-то добавили,
у нас сет стал размера k плюс 1, давайте k плюс первый элемент тупо выкинем.
Так получался метод E log k. Мы, ну, мы попытались, теперь пытаемся делать то же самое с кучей фибоначи.
Ну, мы не можем держать k элемент. Но в куче фибоначи есть проблема,
как-то выкидывать максимальный элемент – это все-таки логарифм там.
Вот, внимание, вопрос. Как же нам выкрутиться?
Сейчас у нас проблема в том, что мы не можем выкидывать максимальный элемент.
Ну, типа, а мы… Завести вторую кучу не подойдет?
И чем это нам поможет? Ну, мы заводим одну кучу на
минимум, другую на максимум. Значит, на кучу на максимум.
А, нет, вторая куча тоже на минимум. Это будет удален. Одна куча – это все,
что мы доположили, а вторая куча – это удаленные вершины. Мы кладем, когда мы хотим дать элемент,
мы кладем удаленные. Теперь мы хотим посмотреть на верхний. Если у них верхний разный, значит,
тот, который верхний, исходный кучом правильный. Сейчас, ну, с Леотова есть какая-то вопрос. То есть,
мы из первой не особо что удаляем, и тогда log k превратится в log v?
Нет, а потом, когда они у нас… Мы удаляем максимальный, чтобы он нам не мешался. Но
смотреть в верх, он, кажется, так не мешается. А когда он все-таки мешается,
ну, тогда мы сделаем экстра. Хотя это в лучшем случае может быть всегда. Нет, может быть и не всегда.
Ну, не знаю. Стоп, а зачем мы вообще… Ну, вот что же делать?
Мы хотим в куче хранить log k элементов, но мы не можем выбрасывать это.
Ну, просто да. Потому что выбрасывание, если я буду каждый раз, когда там k плюс 1 элемент
выбрасывать максимум, то мне придет, то я как бы не смогу сделать за единицу. И придется это,
и тогда у меня будет е log k. А я хочу log k только на v.
А чем это поможет? Сейчас, почему как квадрат-то? Ну, а суть-то одна. Вы храните сейчас k минимум.
Потом вы попытались добавить еще что-то. Значит, это что-то лучше, чем нод. Поэтому вы что-то
добавили, теперь k плюс 1 элемент надо удалить. А делаете вы это постоянно. Просто каждый раз,
когда релаксируете по какому-то ребру. Так, и что это даст?
Это даст, что тогда у нас у k вершин максимум k квадрат в соседей. Подождите,
у вершин-то соседей у нас потенциально больше, чем k. Так нет, я говорю, что мы стеблин каждой
вершины обрезаем до k. И что? Ну, теперь у нас k вершин у каждой стеблины в k. И что е будет,
е будет? Вместо е вы напишите 8.vk. Нет, погодите, до k квадрат. Ну, тогда
откуда? Мы из каждой вершины положим в кучу не более, чем k других мужиков. И получается,
суммарно, а когда мы прошлись пока вершинкам, мы уже остановимся. Поэтому мы не более k раз
положим пока вершинок в кучу. Размер кучи не более, чем k. Нет, то-то и проблема в том,
фишка алгоритма в том, что мы много вершин можем положить в эту кучу. Лог k у нас в этом сете
заключался не потому, что мы в этот сет положим мало вершин, а потому, что мы их будем быстро
удалять. То есть, может быть, мы много вершин туда положим, но просто многие вершины просто оттуда
быстро выбудут. Да, но мы верю, что у нас суммарно, есть удаление даже не чувствует, у нас суммарно,
просто слишком мало положит, чтобы у нас алгоритм. Суммарно мы положим не более, чем k в квадрат
вершин, раз мы положим суммарно не более, чем k в квадрат вершин. А почему как квадрат? Я боюсь,
что мы каждую вершину можно вполне себе туда положить. Это будет нормально. Мы же обрубили степень
специально, чтобы у нас каждая вершина. Мы рассмотрим вершины. Мы не рассмотрим больше,
чем k вершин за шаг. Тогда, получается, в каждой вершине будет когда к, то есть, получается,
вк, к симпатику, что ли? Нет, мы стоим в вершине, она может нам вызвать не более, чем k операции
положить элемент в кучу. Да. Потому, что мы обрезали степень пока. Мы суммарно не более, чем
к вершинам вызовемся. Потому, что мы берем компоненту размера k. Потому, что когда мы взяли
уже, когда мы взяли катую, мы скажем, окей, мы закончили и сбросим всю кучу. Так. Поэтому у нас в
куче будет суммарно не более, чем k квадрат операции добавления элементов в кучу. Поэтому иметь
это на каждом этапе. На каждом шаге. Шаг это вот, мы хотим взять компоненту размера k. Ну.
Построить компоненту размера k. Заблуждается, что за один шаг суммарно добавлений, суммарно
добавлений будет не более, чем k квадрат. Так, и? Ну, поэтому мы говорили, что логарифм, поэтому если
до каждого шага считать, что время работает, это логарифм от максимального размера кучи,
это вообще будет за всю историю. Логарифм от k квадрата, логарифм от k квадрата. И что? А как вы
достигаете того, что у вас при шете добавлений декризы за единицу работает? Сейчас у нас insert
кучу. Ну, за единицу. Да, но да, но напоминаю, вы должны следить. Сейчас у нас единственная вещь,
которая работает не за единицу, это экстракт. Мы его запустим в раз и он у нас работает за лог
от размера кучи. И мы сделали так, чтобы куча всегда была размера не более, чем k квадрат,
поэтому все экстракты работают за лог. А, я понял. Аааа, понятно. Может глупый вопрос, а как обрезать степень?
Ну, ты просто идет, ты переиграешь всех ее соседей, а как только релаксировал хотя бы каста нам
ну да вообще вы релаксируешь видимо хотя нет погоди погодите погодите а вы откуда знаете
как по кому надо релаксировать ну юзды то да но а ну ну просто не придется ли вам сортировать там
нет канны нет смотрите ткан и меньше допустим можно за линию найти не проблема но если вам если
вам плевать в каком они порядке то найти но нет смотрите но у джейк в каждой вершине можно найти
к минимумов если вам плевать в каком они порядке идут за за линию потому что можно найти кату
порядковой статистикой просто взять да это не проблема но проблемом другом вам ко минимумов не
помогут потому что какие-то из этих камень им по каким-то из этих камень умы будете релаксировать
по каким-то нет потому что они вызовут пойдут уже в то что было раньше в чем причастен что я
в какой-то момент то есть там но так я но а пробежимся по ребрам вытянем плохие из оставшихся выберем
к минимальных но да ну ладно но можно и так да нет у меня просто была технология такая работаем с
кучей на самом деле с кучей фибоначи но просто если в ней стал размер 2к тогда мы объявляем
перестройку а именно ну достаем ну берем в эту кучу иначе встать чисто эти 2к элементов за от
кан находим в них ка максимальных и строим из них кучу заново ну там амортизировано получится
нормально а потому что давайте приклад давайте от добавления в кучу каждого элемента допустим мы
далиashma.ru
тогда идея такая пусть у вас встала 2 и но вот тогда допустим у вас там после того как у вас
то есть пока у вас то есть в то есть опустим мы не будем считать то есть допустим пуп aquellos а load
к а или менее элементы в том монетки мы не используем вот здесь но тогда д photograph 2 к у нас
и потратим, то есть эти вот монетки. И все, теперь у нас снова k-элемент. Вот так вот. И все.
То есть вот есть такой чит. Ну ваш чит, в принципе, тоже может работать. Хотя вот надо, может быть, нам придется уточнить алгоритм, но об этом мы подумаем.
Вот. Ну в принципе, да. То есть вот разными, да, в принципе.
Так. Ну да, в принципе. Ну да, заметим, действительно, что.
Сейчас хотя. Ну хотя, да, да, действительно. Да, можно и так.
Вот. Ну значит, в чем теперь фишка? Итак, мы научились тоже с помощью кого-то интересного выкрутаса работать за E плюс V log k.
Допустим. Но вправивается теперь. А теперь какое k подобрать?
Видимо log V. Почему?
Зачем?
Нет.
Сейчас. А из этой симпозиции, которую мы только что нашли, мы не хотим, типа, взять k как можно и меньше?
Потому что с логикой E плюс V log k, то, типа, кажется, уменьшая k, просто алгоритм еще работает быстрее.
Ну, с одной стороны, да. Но с другой стороны, тут на самом деле у нас степень свободы больше, чем кажется.
Потому что заметим, что если мы знаем, что, например, вот эта штука не будет превосходить E, то при этом условии, наоборот, хочется взять k как можно больше.
Ну вот. Поэтому возникает такая идея. Давайте рассмотрим такое k. k равно 2 в степени E поделить на V.
О.
Ну, кстати, да. Заметим, что во многих случаях, кстати, это вам может уже хватить для того, чтобы уже на первой фазе победить за U от E, кстати.
Вот. Видно, да?
Ведь, смотрите, действительно, давайте вот такое k. То есть, заметим, что если, кстати, вот тоже, это уже нам дает возможность, что если E больше, чем V log V, допустим, да, то этот алгоритм, в принципе, вот уже, уже вот за один этот шаг, давайте назовем эту операцию, там, шаг Фредбонатариана от k.
Ну вот. Ну, потому что, почему от k? Потому что k у нас все-таки параметр будет. Понимаете, да?
Вот. И тогда вот оказывается, что, то есть, такой вот шаг у нас работает за, при таком k, получается, шаг работает за U от E.
Ну, почему от E? Потому что log вот этой штуки, это E поделить на V, и тут у вас сократится будет тоже E. Понимаете, да?
Вот. Понимаете?
Вот. Значит, получается вот такая красота.
И вот.
Но, если V больше либо равно, чем V log V, то тогда легко убедиться, что за один этот шаг вы уже, в общем-то, алгоритмы закончите.
Понимаете, да?
Вот.
Ну, правда, заметим, что если E больше, чем V log V, то, в общем-то, за U от E вы и так уже, у вас уже есть алгоритм.
Это обычное, просто алгоритм с кучей Фибоначчи, но это, по сути, он и будет.
Ну, потому что, если k здесь V или больше, то, как вы уже понимаете, это просто ездится стандартный алгоритм Прима с кучей Фибоначчи.
Просто практически, вот буквально он и есть.
Но что делать, если E все-таки поменьше?
Так вот, щит такой.
Дело в том, что, значит, смотрите, мы будем делать много шагов.
На первом шаге мы выберем k2 в степени E поделить на V для того, чтобы шаг был за U от E.
Заверная вперед, мы и дальше будем говорить о том, что каждый шаг будет работать за U от E.
Вот. Но мы будем четырить.
Значит, смотрите.
Дело в том, что дальше все у нас будет просто.
Дальше у нас на каждом следующем шаге мы возьмем k как 2 в степени предыдущей.
Вот уже догадываетесь, да, откуда у меня лог звездочка взялась.
Ну и так пока не закончится.
Вот, понятно, да?
Вот, видно?
Чуть-чуть не видно?
А, видно, вот.
А теперь давайте думать, за какое всем точку теперь будет работать каждый шаг?
Ну, на каждом шаге, ладно, можем считать, что количество ребер остается неизвенным, допустим, да?
Ну, оно там, конечно, чуть-чуть уменьшается после каждого сжатия, но это нас не сильно колышит.
Вот.
Но давайте смотреть.
Но заметим, что если у нас k.
Но давайте смотреть.
Вот на каждом шаге у нас есть не только количество, не только собственно k, но и количество вершин.
Ну, допустим, вот давайте V1 сейчас у нас количество, это V.
А количество вершин на этом шаге, оно меньше либо равно, чем что?
Оно меньше либо равно, чем V предыдущее поделить на k предыдущее, согласны?
Ну, потому что в каждой компоненте связности, которая у нас получилась, размер хотя бы там k предыдущее был, правда?
Поэтому количество вершин уменьшилось.
Вот.
И дальше V3 меньше либо равно, чем V2 поделить на k2, да?
V4 там меньше либо равно, чем...
Ну и так далее.
Но теперь давайте смотреть.
Вот мы знаем, что у нас тут ахимточка на первом шаге была E плюс V логарифом там 2 в степени E поделить на V, да?
Равно E плюс там VE поделить на V равно от E.
Так, ну первый шаг, да.
Теперь давайте рассмотрим второй шаг.
У нас на втором шаге получается E плюс V1 поделить на k1 на логарифом 2 в степени k1.
То есть это равно E плюс V1 поделить на k1 на k1.
То есть шлеп-шлеп от E.
Удобно, правда?
Но легко убедиться, как вы уже понимаете, я думаю, что на каждом следующем шаге тоже будет.
Все от E, от E, от E ровно по этим же причинам, правда?
То есть отсюда получается, что нам нужно думать, сколько раз надо как бы возводить 2 в себя.
То есть 2 в себя, потом в то, что получилось 2, возветить и так далее.
То есть сколько эта башенка должна быть, чтобы у меня получилось V.
Вот, понимаете, да?
Чему нас это приведет?
Ну вот.
Ну в принципе отсюда получается, что количество шаров получается, ну как сказать, можно уже оценить, да, что пока k у нас, ну вот.
Что это, можно сказать, что это E лог звездочка V.
Можно прямо так оценить.
Понимаете, да?
Но мы пойдем, да.
Но мы пойдем, хитрее.
Вот.
Дело в том, что, обратите внимание, стартуем мы не с единички.
Там и не с двоечки.
Мы стартуем вообще с приличной такой штуки, правда?
То есть на самом деле, то есть более точная симпточка могла быть такая.
E умножить на лог звездочка V.
Ну, сколько шаров надо, чтобы достигнуть V, начиная отсюда?
Ну это не лог звездочка V.
Это лог звездочка V минус лог звездочка.
Получается два в степени, ну да, два в степени е делить на V.
Плюс один.
Чего?
Ну да, то же самое, что это, плюс один, конечно, да.
Но я напишу, чтобы тут, чтобы это совсем тут честно было, откуда я это взял.
Вот.
Вот.
Видите, да?
Ну вот.
То есть тут получается, видите, наоборот.
При этом вычитание получается, что чем E больше, тем лучше.
Почему нам принципальное это вычитание?
А потому что чем больше E, тем с более низкого старта мы начинаем.
И тем меньше шаров мы делаем.
То есть, видите, потому что, видите, поэтому не случайно.
Мы вот ужимаем K по максимуму так, чтобы первый шаг за O от E работал.
И оказывается, что у нас есть очень широкие возможности для этого.
Но результат, но тогда насколько они широкие?
Ну конечно, если E это прям вот V плюс там микроскопическое что-то, то от лог звездочка V вы никуда не денетесь.
Но если оказывается, давайте я сейчас какой-нибудь еще маркер возьму.
Если у вас при этом оказывается, что...
Но правда при этом оказывается, что если E у вас больше, чем допустим вот то самое V лог лог лог V.
Да?
То что тогда, тогда что получается?
Тогда из этого следует, что лог звездочка 2 в степени E делить на V это сколько?
Это получается больше, чем лог звездочка чего?
Там 2 в степени лог лог лог V.
Видно, да?
А что такое лог звездочка 2 в степени лог лог лог V?
Это лог звездочка V минус 2?
Ну да, это на самом деле да, равно фактически лог звездочка V минус 2, совершенно верно.
И тогда получается, что так как мы тут оцениваем это снизу, то при вычитании получается оценка сверху.
И получается, что здесь E умножить на константу.
Добрый день.
Отсюда в общем-то и получается, что если у вас E больше, чем V лог лог лог V,
то получается, что мы научились искать остов за O от E.
Видно, да?
Как выяснилось, даже это не сильно сложно.
Чего?
Нет, ну константа куча фибоначи, конечно.
Куда же деваться от нее любимой?
Ну ничего.
Ну ничего.
Вот.
Так что да.
Так что в принципе с точки зрения практики, видимо, этого уже хватит, хотя да.
Философский вопрос, что будет работать быстрее, вот это или нормальный прим с сетом.
Вот тут, конечно, вопрос философский.
Так ладно, есть ли тут еще какие-то вопросы?
Можете повторить, почему мы вычитаем вот это вот итерированное логарифм?
Ну потому что, смотрите, потому что фактически у нас тут, если мы хотим точно оценить,
то тут не лог звездочка V, а сколько раз нужно возводить 2 в степень вот это, то есть начиная отсюда, да?
Чтоб получить V.
Что такое логарифм звездочка, да?
Это мы берем единицу и сколько надо делать шаг, что х превращаем в 2 в степень х, да?
То есть сколько нужно вот эту башенку накручивать так, чтобы число получилось больше, чем V, да?
Но здесь, смотрите, мы стартуем не с единицы, а с вот этого числа.
Поэтому количество шагов, которые нужно сделать, на вот столько меньше.
Ну я тут плюс один на всяких случаях пишу. Вот и все.
И это нам оказывается принципиально вот в этом случае?
Да, и оказывается, чем больше е, тем более нам это принципиально.
Именно, именно, именно.
Так, ну что, еще вопросы?
Так, есть вопросы? Да нет, наверное.
Так, ну если вопросов нет, ну тогда переглянем.
Так, ну если вопросов нет, ну тогда переглянем.
Смотрите, в какую сторону еще можно чуть-чуть копнуть?
Ладно, давайте попробуем все-таки упомянуть.
Ну вот, упомянуть атомик хип.
Да, нет, мы сейчас не будем его там подробно обсуждать.
А атомики это кто?
Вот, это я скажу, естественно.
Значит ситуация такая.
Представьте себе, что мы, важно сделать оговорку,
представим себе, что мы живем в целых числах.
Ну в том плане, мы уже обсуждали эту модель,
то есть модель, что мы живем там в какой-то модели,
в которой у нас есть какие-то фиксированные какие-то
W-битные числа, и мы с ними все умеем делать.
Ну вот, но там еще правда какие-то оговорчики,
что мы их там еще что-то умножать за вот единицу умеем.
Ну там ладно, это какие-то уже глубокие оговорчики,
я, честно говоря, там слабо понимаю.
Но представим себе, что веса, с которыми мы работаем,
это целые числа.
Ну потому что представлять можно все что угодно.
Заметим, что мы здесь представлять можем даже едва ли не ситуацию,
что тут веса, это вот те самые камешки, помните?
Ну и сортировки там вот этого всего.
Так, а есть кто живой?
А то что-то все развалились, как будто я сейчас рассказываю то,
что и так уже давно все знают.
А это не так, я вас убираю.
Так что лучше вылезайте, а то знаете,
а то там сублимация суббирека будет очень резистентна
по отношению к коллективному бессознательному,
и вы уже не поймете, о чем я говорю, да.
Вот.
Значит, смотрите.
Ну так вот, просто фишка такая.
Но дело в том, что вот что такое,
просто по идее атомик хип.
Она говорит, что давайте его действительно запустим
абсолютно обычный хип,
может быть там в том числе и сливаемый,
хотя нет, по-моему, не сливаемый.
Вот.
Но который будет работать на этих,
в этой модели и с этими целыми числами.
То есть там ключи в этой куче,
это именно целые числа.
Это важно.
Так вот.
Ладно, если я не наврал,
то значит утверждается, что воспользоваться этим можно.
А воспользоваться этим можно так,
что дело в том, что этот атомик хип,
значит он все операции будет делать за от единицы,
там какой-нибудь там экстракт-инсерт, экстракт-мин,
ну не говоря уже о get-min.
Нет, хотя нет, вру.
Экстракт-мин будет, ну вот,
ну там все операции всякие, insert,
может быть даже, ну ладно,
за мэлт я рисковать не буду,
но вот get-min, конечно, точно.
Вот.
А экстракт-мин будет работать,
но обычно он работает за log-in,
и мы как бы знаем, что в какой-нибудь модели камешков
быстрее не получится, иначе у вас сортировка будет быстрее, да?
Но здесь мы с 4.
Мы неожиданно себе заявим,
а, ну да, еще, да, дикриски еще, конечно,
какой-нибудь.
Вот.
А экстракт-мин будет работать не за log-in,
а внезапно за симптотику log-in делить на log-log-in.
Вот такая неожиданная задача.
Вправивается.
Вот представьте себе, что нам с небес по факсу
прислали вот такой черный ящик.
Спрашивается,
за какую симптотику теперь мы можем реализовать алгоритм Фредмана Торьяна?
Нет, видите, да.
То есть чит заключается в том, что, да, то есть действительно,
то есть это вроде выглядит как оптимизация,
потому что у нас теперь вот этот шаг будет не за,
шаг будет вместо.
Чего?
Да, ну погодите.
Ну давайте, давайте вот аккуратненько.
Значит, e плюс v log-k у нас торжественно превращается
в e плюс сколько?
v.
Ну получается log-k делить на log-log-k.
Вот, получается такое.
Получается.
Вот.
Ну там, правда, те же оговорочки, да.
Но, видимо,
каким-то образом придется тоже предполагать,
что мы там, если у нас размер кучи станет 2k,
то мы сможем за log-k выписать хотя бы все элементы.
Но обычно это, понятно, не сильно большая проблема,
то есть в любой структуре данных, наверное,
если вы кладете элементы, то вы можете поддерживать
хотя бы список этих элементов, правда?
Ну есть подозрение, что так или иначе вы это сделаете, да?
Вот.
А теперь вот давайте внимательно подумаем.
Ну вот.
То есть если нам такое повезло,
ну-ка давайте посмотрим,
во-первых, какое k мы сюда можем подсунуть,
а главное, какие следующие k мы вообще тут можем,
там с какими следующими k мы вообще можем дальше развлекаться.
Ну-ка давайте вот, как вы думаете?
Как вы думаете?
Типа e log v делить на v.
Так.
Так, то есть k давайте примерно скажем,
сколько вы говорите?
2 в степени e log v делить на v, нет?
e делить на v log v, вот так, да?
Ну что-то такое, наверное.
А, ну кстати, это равно, я больше скажу,
это равно v в степени e делить на v.
А, e log v, наверное.
Нет, ну пройдите, пройдите.
Ну давай я это подставим, что проблем-то?
Давай.
Да ладно, ладно, кто это? Просыпайся.
А то это как-то...
А то знаешь, как-то раз в жизни сел на первую парту и спать сразу.
Вот.
Значит, давайте, значит, log k это что такое?
Так, log k у нас получается e делить на v.
О.
А тут делить, значит, log.
Ну тут еще поделить на какой-то log e делить на v.
Ну ладно.
Так, но это уже неплохо, да?
Да.
Да.
Так, это получается e.
Но возникает, правда, маленький вопрос.
Слушайте, правда, вот что-то даже...
Вопрос такой, что даже как-то подозрительно.
А нет ощущения, что это, по-моему, должно быть больше, чем v?
Возникает ощущение, что это почти всегда больше, чем v.
У нас тут было v, e делить на v, умножить на log v.
А мы, кажется, где-то здесь чуть-чуть просчитались.
Вы вспомните, мы написали v умножить на e делить на v.
Ну, ты сам сказал e log v делить на v.
Да, да.
А где log v, на который еще надо умножить?
Ну вот, ну смотри, 2 в степени e делить на v log v, да?
Да.
Так, я просто сказал, что это как бы 2 в степени log v, и все это в степени e делить на v, нет?
Ну, типа вот это x в степени z, я сказал.
Да, да, это правда, это правда.
Это правда, я про то, что где мы, в плане, в нижней штуке,
где мы описываем e, plus v, e, v, это делить на log v, мы, конечно, там забыли log v где-то, нет?
А-а-а.
А кто это делает?
А, вот так.
Нет, нет, внизу-то, ну внизу это неважно, это даже лучше, от этого только лучше становится.
От этого только лучше становится.
Сверху.
Сверху?
А-а-а, да, да, да, да, да, да.
Вот, хорошо.
Да, то есть k конечно кайфежная, но пока еще, но пить боржон пока еще, конечно, рано.
Вот.
Так, ну хорошо, да, потому что это была бы, конечно, миша.
Потому что, заметим, что пока, конечно, заявить, что там просто даже подставить k равно v, это еще не победа.
То есть подставить, если у вас k равно v, то симпточка получится какая?
e, plus v на log v, поделить на log log v.
Так, ну это уже лучше.
Ну тоже неплохо, но правда, не сильно нас спасает.
Ну хотя вот, за один для одного шага это уже неплохо.
Хорошо.
А давайте подумаем, о каком тут k еще может идти речь?
Вот в такой симптотике.
Ну давайте все-таки, понятно, что да, можно, конечно, брать ту же технологию и получить ту же самую симптотику,
но есть подозрение, что k, наверное, можно и получше взять.
Вот можно, ну можно взять 2 в степени e делить на v, и может получиться неплохо.
Ну вот, а можно вместо этого взять что?
Я не знаю ведь.
Я не знаю ведь.
Да, ну-ка, и что получится?
Хорошо, это совсем.
Так, k равно, сколько там?
Значит, e поделить на log v.
Ну да, тогда у нас v не сократится.
Так.
Нет, ну действительно надо, да.
А если вот так?
А если вот так?
2 в степени, а нет, вот это делать, да.
Сейчас, там же в школе была формула, когда лог a поделил лог b, это что?
Это лог v, 1 поделил.
Нет, это будет формула, там это будет лог k по основанию, по основанию лог k.
То есть вот так вот.
То есть не сильно.
Нет.
А чем нам это поможет?
Нет, просто это равно вот этому, это просто равенство.
Вот.
Так.
Вот, и нам надо, чтобы это было меньше или не равно, чем e поделить на log.
Но вот надо решить просто.
Так.
Нет, ну да, можно так сказать.
Так.
Нет, ну да, можно так сказать, да.
Давайте так.
Ну можно так сказать, t поделить на лог t, давайте пусть у меня t равно лог k для удобства, да.
Это 2 в степени, e поделить на v.
Так.
Нет, это просто e поделить на v, допустим, да.
Так, внимание, вопрос, какое t можно взять?
Я сейчас, а какое это имеет?
Ну потому что пусть у меня t равно лог k, да.
Ну смотрите, мы хотим, чтобы...
Вот, да.
Ну просто вот так сокращение.
Теперь давайте подумаем, чему может быть равно t?
Ну, само по себе e поделить на v, конечно же, подойдет.
Но с другой стороны, если я возьму e поделить на v, лог e поделить на v, подойдет ли это?
Это будет больше.
Да?
А почему?
Потому что t поделить на лог k?
Ну-ка, погоди.
e поделить на v, лог e поделить на v, да.
Тогда что у нас получается?
Тогда t делить на лог t, значит, получается, e v лог e делить на v, делить на лог e делить на v, лог e делить на v.
Ну, то есть, заметим, что да.
Вот.
И это выглядит как шлеп-шлеп.
Причем, более того, это выглядит как практически идеальное решение.
Ну, потому что в знаменателе заметим, что вот это вот, это t от лог e поделить на v, очевидно.
Да, но по факту это значит шлеп-шлеп, e делить на v. Отлично.
То есть, значит, пишем.
То есть, вывод очень...
То есть, значит, какой вывод получается?
То есть, на самом деле, да, это было действительно близко к правде.
Только давайте теперь скажем, что k у нас равно...
Так, давайте я тут еще постираю чуть-чуть.
Так.
Так.
Где тут постирать?
Ну ладно, давайте.
То есть, это я поделить на v в степени и тебе поделить на v?
Да.
Нет, почему? Нет, 2...
А, ну да.
Нет, ну это уже адекватно хотя бы.
Потому что не v в степени уже, потому что плевать какой, потому что e не меньше v.
Ну, бывает так, что e меньше, чем v, но в этом случае решение очевидно.
Вот.
При этом.
Значит, да, давайте скажем k равно 2 в степени e делить на v логарифом двоичный e поделить на v.
Так.
Ну, давайте, да, можем на всякий случай подставить.
e плюс v на, значит, e поделить на v логарифом двоичный e делить на v поделить на логарифом двоичный...
там, собственно, e делить на v логарифом двоичный e делить на v.
Ну, понятно, там шлеп-шлеп.
Ну, в общем, такой меньше либо равный шлеп-шлеп.
И равно, в общем, короче, от e.
Так, ура!
Так, это, конечно, неплохо.
Теперь возникает вопрос.
А как вы думаете, да?
Так.
Так.
Сейчас сразу...
Мы просто полагаем, что у нас больше либо равен чувствитель, да?
Потому что кажется, что e поделить на v в разрешенных графах у нас может быть чем-то близким к единице.
У тебя это лог e поделить на v плюс лог-лог e поделить на v.
Ну да, смотри.
Да, это равно лог 2e поделить на v плюс еще...
А, нет, то это просто неважно.
Смотри, вот это заведомо больше, чем это.
Я просто думал, что...
Смотрите, что у нас вот e поделить на v уже близким к единице.
Мы получаем что-то типа меньшее нуля.
А, ну я тебя понял, да.
Все нормально.
Нет, ну как сказать, нет.
Нет, ну давайте так.
Будем считать, что e поделить на v больше, чем 1.
Потому что...
Ну да, мы считаем, что он больше, чем 1.
Это хорошо.
Ну давайте так.
Ну меньше 2, допустим.
А-а-а, ой.
Ой.
Ой.
Ой, а ведь правда.
Ай-я-яй.
Нет, проблема в том, что...
Смотрите, мы здесь сокращали, предполагая, что вот это вот, наверное, больше, чем вот это.
Но это может быть просто неправда.
Почему?
Потому что вот этот двоичный логариф, как правильно заметил Вася, не обязан быть больше единицы.
То есть для этого нам нужно потребовать, чтобы e поделить на v было больше либо равно уже не единицы, а двойки.
Да, ай-яй-яй.
Действительно, что делать?
Нет, ну впрочем, конечно, конкретно этот вопрос, конечно, решается.
А, если e поделить на v меньше 2, по 2, то можно...
Ага, да.
Накидать лишние хребер, да.
Да.
Ну, кстати.
Нет, ну понятно, что k бы...
Нет, ну или можно накидать лишние хребер, или просто сказать, что k равно максимум из...
То есть вместо e поделить на v пишем максимум из e поделить на v и 2.
Ну, в принципе, да.
То есть понятно, в реальном коде как бы это не отразится, хотя можно и отразить, собственно, да.
Ну, хотя знаете, как бывает потом, кто да...
Как это да...
В реальном коде будет заряд кожды константа 10.
Ну да.
Ну всякое бывает, да.
Нет, ну бывает эти.
Нет, почему?
Я помню, я как-то контест какой-то там что-то писал.
Там была задача тоже, надо было написать динамику, но перед этим объекта надо было посортировать по какому-то непонятному компаратору.
Ну вот.
Как бы это был какой-то напрашивающийся компаратор, но как бы, если по нему отсортировать, почему-то получалось va.
Вот, я не знаю, то есть...
Ну, точнее, я сказал, ну просто вот один из альпиагиков, который там очень против...
Не понимал, что зачем вы там доказательства будете, за что это такое, непонятно было.
А я взял...
Ну вот, представляете, он там написал это и получил va и думал, что делать.
Ну вот, я сказал, а я просто сортировал вот так вот.
А почему так?
У меня так доказательства сходилось.
Вот.
И он почему-то был сильно удивлен.
Так, ну ладно, не важно.
Не, ну там порядок был принципиален.
Значит, хорошо.
Ладно, в любом случае, если мы вот этого так или иначе достигли, то теперь давайте думать.
Хорошо.
Так, то есть у нас v теперь превратилось в...
Ну вот, соответственно, v делить на k.
Так, но фишка теперь заключается в другом.
Так, а какое следующее k нам вообще взять?
Видимо, вот вы развили раньше 2 в степени.
Это как...
Вот сейчас будем сделать что-то чуть больше.
То есть вот эти 2 в степень еще умножить, допустим, на логрифт.
А и умножить на саму штуку вот.
Так.
Нет, то это, конечно, да.
Так, сколько, говорите, получается?
То есть, типа, v приходит в 2 в степени v умножить на v.
V квадрат?
2 в степени v и умножить на v просто.
Ну ладно, то это только не v давайте, а k.
А, 2 в степени k и он просто умножить на k.
Не при степени двойки, а просто умножить на k.
А, 2 в степени k умножить на k, вот так, да?
Да, вот это.
Ну вот теперь давайте думать, а чем это нам поможет?
В нашем случае это поможет...
Получается сколько?
То есть 2 в степени...
Нет, хорошо, k-то переходит.
И давайте смотреть.
У нас получается e плюс v делить на k.
По-любому v делить на k умножить на сколько?
Там логарифом 2 в степени к, то есть лог, там, 2 в степени k на k,
поделить на log log...
2 в степени k на k...
Внимание, вопрос...
А сильно нам это помогает?
По-моему, логарифом вот этого и логарифом тупо 2 в степени k,
и это, в общем-то, примерно одно и то же.
illy 2 в степени k может умножить на логарифом k.
Чего-почему умножить на логарифом k?
Логарифом 2 в степени k умножить на k это...
Это...
Это к плюс лог к, в общем-то, а это все равно к, так что, в общем-то, по-моему, да, можно...
А, ну, то есть 2 в степени к лог к.
Во, вот это уже интереснее, да. Ну-ка, давайте 2 в степени к лог к. Что получится?
Вот. Так, лог лог 2 в степени к лог к.
Ну, что получится? Ну, и здесь получается у нас... Ну, понятно, что тут получается...
Е плюс В делить на к, значит, умножить на... Действительно, к лог к.
Значит, поделить на... Ну, фактически, лог к плюс лог лог к.
Вот. Ну, по факту получается, да, шлеп-шлеп, шлеп-шлеп, ура.
Видимо, можно было еще позакручивать, наверное. В общем, получается Е.
Так. Ну, а просто фишка теперь заключается в топ-да, носка, нот.
А действительно, просто, теперь думаем, а насколько, как вы думаете, да, насколько быстро это будет расти?
Сейчас, то есть, носка переходит в как в степени к?
Ну, на самом деле, да. Да, то самое Е поделить на В в степени Е поделить на В получается, да.
Да, вот видите. Такая лесенка, это звучит как лог из лог-звездочки. Ага, да.
О, господи. Нет, в смысле, лог из лог-звездочки.
Это очень быстро. Ну, да. Но, правда, все равно плохо. Быстро.
Ну, вот видите, потому что я четко к чему мы должны прийти.
Дело в том, что утверждается, что с вот этим вот атомик-хипом на самом деле просто там этот алгоритм просто работает вообще за две итерации.
И алгоритм получается вообще за вот Е работает.
За две итерации на реальных данных или в плане всегда?
В плане всегда. Нет, ну, на реальных данных лог-звездочка у нас реально, ну не две, но четыре. Это не очень интересно.
И тогда после первой итерации у нас сколько вершин остается?
Так, давайте. После первой итерации у нас остается вершин...
В поделить на?
В поделить на, значит, там...
Да, Е поделить на В, там лог-2, Е поделить на В.
Вот, можно поперекидывать, сказать, что это В квадрат поделить на там...
Сейчас почему так?
А, ладно, потому что да. Да, не так.
У нас есть два в степени.
То есть это мы делим на Е в степени В, но еще в какой-то там...
В степени Е поделить на В.
Сейчас, если Е равняется В, то...
Но мы сказали, Е поделить на В это хотя бы два.
Так что допустим, да?
Ничего хорошего.
Мы можем В сделать там 1, Е 35, а я поделить на В сделать 2.
Кажется, мы за две итерации так...
Да, но так, по крайней мере, не получится.
Значит, надо камень.
Так, ну да.
Теперь возникает вопрос, каким же образом можно подменить К?
На нуде К увеличить.
Ну, либо увеличить, либо сделать его как-то, может быть, зависит, чем даже...
Не только от Е делить на В, хотя это тоже, но и, может быть, от самого В.
Ну-ка, давайте прям вот это интересно подумать.
Может быть, можно добиться точного понравительства?
Да... Нет, ну как?
Ну, не очень понятно.
Ну-ка, я вот, это же интересно вести.
Сейчас, тогда мы, наверное...
Да, может быть, нам не надо стремиться к тому, чтобы...
Ну, ВЛОК был хотя бы Е.
А, ну не ВЛОК, а ВЛОК-ЛОК-ЛОК.
Потому что мы типа знаем, что у нас все эти операции.
Ну-ка.
Ну-ка.
Ну-ка.
Ну-ка.
ЕЛОК Е поделить на В.
ЕЛОК Е поделить на В?
Давайте попробуем.
Или, в смысле, два в степени это?
Одну, два в степени.
Так, два в степени ЕЛОК Е поделить на В.
Ну, давайте попробуем.
И что это даст?
А мы же пробовали уже ЕЛОК В поделить на В.
А ЕЛОК Е, типа, больше в равно.
Но был слишком большим.
Ну, в смысле, у нас получилось, что за первую итерацию...
А, у нас К больше, чем все вершины практически всегда.
Да, у меня уже Е в степени.
Получается В в степени что-то больше единицы.
Ну да, да, да, да, да.
Ну да, сейчас вот.
Так, кажется, вот.
Нет, надо как-то свой пишу.
Вот как-то я вот.
ЕЛОК.
Ага.
Чувак?
Да.
Ты, это я...
В отран бил...
Уровенье Х при ритме ЛОК Х
сравняется...
И К.
Потому что, я...
Да, ты, это ты...
Ты, это ты...
Ты, это ты...
Ты, это ты...
Ты, это ты...
Ты, это ты...
ну а симпатически по-любому как бы да так там уже или там надо это как-то заметить что-то еще
может быть вы налог вы а потом налог лог а потом на лог тук лог
кила кила если перебираем рандомные функции давайте е делить на в
умножить налог е делить на в умножить налог лог е делить на в о господи ну и что это даст
у нас две итерации кто знает давайте спишем ассимпточку второй и и итерации
она у нас работает за е плюс в поделить на два в степени нет просто плюс в поделить на
к налог к нелит на лог лог к так а давайте у нас сейчас будет сейчас мы просто кажется слишком
сложно думаем мы даже по сути просто там системы из двух неравенств надо сказать что
корень из в
корень из токачилл корень из в это тот же лог вы получите все равно вы лог вы
поделить налог лог вы и как бы не факт что это как бы е может оказаться еще меньше до
вот тут какие то есть как бы нас интересует когда е меньше чем даже в лог лог лог в вот тут
мальчика
сейчас
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Слышен звук дыхания.
Сейчас, почему так? Почему v поделить на k?
А на сколько?
Просто же v поделить на k.
Сейчас.
У нас в вершин...
Сейчас, мы k превратили в v делить на k, да?
Вот этот коэффициент слева, это то сколько у нас в вершин сейчас.
После первой итерации у нас v поделить на k в вершин.
А, понял.
Да, у нас v поделить на k в вершин.
Так, отлично.
И...
Вот.
И вот нам нужно, чтобы вся эта штука была меньше, чем e.
Вот.
Ну, теперь давайте мы числитель заменим на просто log v.
Так.
Так, ладно, значит, меньше либо равно e плюс...
В общем, подставить надо.
Так, ну теперь давайте v делить на...
А, какое...
Так, какое k?
Вот это беремся еще, да?
Да.
Так, 2 в степени e поделить на v.
Log e поделить на v.
Умножить на сколько там log, право.
Так.
Так.
Так.
И так.
Ну, и здесь получается, домножаем на log v поделить на...
Сколько?
Так, поделите нот.
Так, а тут придется все-таки log v делить на k оставить.
Все-таки так оценить не получится.
И чего?
И утверждается, что вот это вот схлопнет вот это?
Ну, кажется, да.
Ну, не факт.
Ну, тогда, наверное, все-таки не надо забивать на log k.
Надо ему вычесть все.
Угу.
Неужели?
Нет.
Ну, как сказать?
А, там log v?
Нет, так просто не получится.
Ну, там не log v, а...
Ну, вот.
Ну, можно писать, что log v минус там...
Ну, сейчас нам же нужно показать, что log v меньше e.
Ну, это-то правда.
Log v меньше, чем...
Доказываем, что вся эта штука не превышает e.
Да.
Но для этого как бы...
Ну, тут вопрос, что кого схлопливает.
Потому что у нас еще есть вот это v, которое тоже надо убить.
А, это не единицы?
Это v.
То есть мы, да, мы тут поделили на k, но это все-таки v.
Вот.
То есть получается log v.
Ну, типа того, да.
Угу.
Ну-ка.
Угу.
У-у, черт.
Потому что, если я больше, чем v, то...
Вася, вставай.
Нет.
Нет, там просто, видимо, придется еще домыслить немножко.
Потому что, если мало ребер...
Угу.
Нет, они, да, они просто...
Не, просто немножко утверждается, что...
Да, что...
От ребер тоже.
Нет, они вообще просто...
То есть...
Ладно.
Они в этом смысле теряют.
То есть они в этом смысле говорят, что...
Да.
То есть говорят так.
Возьмем log v и, казалось бы, будет радость.
Да?
Не будет.
Окей.
V log v может быть больше, чем v.
Угу.
Ну, это, да, как тебе...
Картинка называется вам больше, чем я.
Угу.
Угу.
Угу.
Угу.
Угу.
Угу.
Угу.
Угу.
Угу.
Угу.
Ну, это...
Да, как тебе...
Картинка называется вам...
Да.
Ну, это да.
Вот.
Просто если v log v меньше, чем я, тогда непонятно, зачем
мы все это делаем.
У нас есть прием.
Ну, да.
Так.
Вот так.
Вот я не знаю точно, что они там, конечно...
Сейчас, может, это да.
Или там...
Нет, или там алгоритм.
Нет, может, конечно, Фрэд Монтальян делал чуть-чуть по-другому.
Угу.
Почти.
Ага.
Вот.
Что?
ну да, стартуем с m9, если к, то как бы...
сейчас мы используем...
так, нет, ну если k равно log n, то как бы ладно, тут конечно уже будет log log n поделить на log log log n,
то есть на самом деле это не прямо, как бы вполне себе уже еще лучше, но плохо.
что-то я уже путаюсь, ну как же, нет, log log, это делить на log log log, это конечно круто,
но е может быть еще меньше. как же они так читерски выкручивались-то?
м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м-м
ну да да все вот так
я тебе странно было что-то то шуту то есть очень простое
манки тут уже нам так
да нет причем да причем под анализ нет видимо какой-то еще дом замечания или
там этот фредман тарян действует не совсем так да нет или нет мы уже пытаемся такой нет ситуация
такая что это из этого мы больше не выжмем потому что мы уже тут порешали уравнение и тут выглядит
так что по моему вот вот это уравнение мы решили асимпатически точно вот по моему мы тут как бы
лучше чем как бы еделить новое лог еделить новое просто не выжмем вот так что то дело возможно
не в симпатике а дело да то есть дай дело дело еще возможно в том что в азам во сколько
раз там у нас еще к чину сократится то есть во сколько то есть это во сколько во сколько раз
у нас там еще то есть сколько у нас еще группа чек там будет
не но странно если критически размер кучи лог в это конечно дату получится лог
в и как бы неких понятно как от него избавляться выделить на что можно на
второй иктрации перейти к в поделить на к в любой степени но так что это да то
с проблем в том нас проблема не ва проблем в том что е слишком мелая но
вот слишком так нет а ну да нет вася все-таки не выдержал напряжение на
вася вася вася я конечно очень жалко тебя будить но как бы ты храпишь слишком громко
нет ничего страшного не ну понятно я я я вася прекрасно понимаю на самом деле но вот что-то
сам бы как бы если беда так не хотелось спать я бы на самом деле сейчас полчаса не висел ответ на
этот вопрос хотят которые там на самом деле там на самом деле право там с блеском решается за
три минуты называется главное просто понять как какими-то какими пятью словами то есть там
действительно я плюс в лог вот это или нет но казалось бы да так ну да может где-то там фишка
еще в том что у нас и конечно еще может нет просто если е совсем маленькая то на самом деле на каждом
шаге у нас е тоже уменьшается да то есть есть там е поделить на это в это прям прям совсем одним
да то помните что когда вы жали констар компоненту размера ката у вас количество ребер выкинулся но
это не работает ну не но не особо станутся если е все-таки а симпатически больше чем в но на
обратную функцию керман так например какой-нибудь то там вычитание как бы из е чего-то порядка того же
в общем-то не сильно даст нам погоду вот тут вот какая про тут вот собственно какая проблема есть
нет все-таки жутко не как же это неужели не ну просто да нет я прошу прощенько за зависть но
просто тут они не до но я не знаю просто как будто алга да нет просто честно скажем вот в статье
просто про атомик ип вот тут просто прямым текстом пишем даже вот у нас есть алгоритм да
то есть там вот перевожу тачит идея что растим значит такое там дерево растущие значит из
произвольной вершины как в алгоритме прима только растем пока но пока куча из соседних
элементов значит значит не достигнет значит критического момента потом мы начинаем вот потом
начинаем новую вершину и собственно растим другое дерево то есть тоже пока значит
останавливаемся вот потом там значит все сжимаем и собственно переходим за это мжатый граф так
значит в результате но вот значит но детали понятно но и нам говорят что да что ну и говорят
что если у нас на первой фазе у нас критический размер k, то количество вершин у нас оставшихся...
А, тут знаете просто фишка.
Просто дело в том, что нам тут утверждают, видимо надо просто не от v отталкиваться.
Нет, нам еще говорят, что хочется делать так, чтобы количество вершин от k не превосходило 2e делить на k.
Потому что на самом деле алгоритм Фредмонтарьяна, он на самом деле вот эти вот щиты с отсечкой k максимумов не делает.
Он делает немножко по-другому.
Он останавливается, когда у вас количество ребер, связанных с вашей компонентой, становится k.
Сейчас, ну если с нашей компонентой связано корабль, из этого не следует?
Из этого следует, что количество компонентов суммарных у вас не сильно большое.
Потому что суммарное количество ребер, связанных со всеми компонентами, у вас сколько? 2e?
Понятно, да?
Сейчас, подождите, ребер, связанных с компонентами, имеется в виду, что они внутри компонента тоже считаются?
Да.
Вот. Ну тогда получается, что у вас количество вершин не более чем 2e делить на k.
Так, ну правда...
Мы же все ухудшили, у нас было b подник на k, а стало e подник на k.
Ну вот, да.
Но я просто зачитывал, что тут.
Они говорят вот это.
Просто там такая версия была, может там этого тоже хватало.
Так, ну дальше говорить.
Значит, у нас есть, да, действительно есть двухвариантный проход.
Значит, заметим, что у нас, да.
Значит, используем Atomic Hip, который, значит, там...
А, тут еще.
Сейчас тут, кажется, просто надо...
Ладно, на Atomic Hip, тут, кажется...
Значит...
А, смотрите.
Такая.
Там про...
Нет, да.
Ой.
Все, мой тупняк.
Все.
Я все с ума сошел, да.
Ну я не с ума сошел, нет.
Я не с ума сошел.
Я не с ума сошел.
Я не с ума сошел.
Я не с ума сошел.
Я не с ума сошел.
А.
Если делать алгоритм предмана Тайяна, вот так, как написано там,
ему не будет ли проще писать?
Может.
Ну, кстати, будет.
Без вот этих глоток.
Ну да.
Ну, кстати, давайте уже...
Ну да, да, да, да, да.
Ну, пожалуй.
Нет, так-то будет.
Другой вопрос зайдет ли.
Ну, ладно.
Хотя там вообще говорят в наглую, что первая фаза,
давайте скажем, что k равно e поделить на v и не паримся.
Все.
Ну, максимум из e поделить на 2 и 2, например, да.
Понятно, что тут уже лог e поделить на v.
Соответственно, понятно.
Уже можно сказать, что это e поделить на что-то и не паримся.
А дальше пишем, что каждая следующая степень двойки предыдущего
и, соответственно, там все нормально.
Хотя, если мы оцениваем, что на каждой следующей фазе
у вас e поделить на k предыдущего,
у нас тут получалось e плюс v, так что, в принципе, нормально.
Нет, на самом деле я не сказал просто...
На самом деле я про atomic hip забыл сказать более читерскую вещь.
То есть, на самом деле она не просто работает за такую симптотику,
потому что на самом деле она основана на...
Ну, там на самом деле цепочка.
Atomic hip на самом деле основана на af hip и af hip основана на q hip.
А af hip говорит, что я умею делать все эти операции,
в том числе экстракт-мин, за от единицы.
А atomic hip замедляет?
Нет, он ничего не замедляет.
Фишка в другом.
Я просто здесь говорю, что у меня в каждый момент времени
размер кучи не превосходит лог квадрат n.
Но там просто фишка в том, что n у вас тоже не превосходит
каких-то чисел, которые вы можете у себя хранить.
То есть, на n есть какое-то естественное ограничение,
что n не более чем 2 в степени этой битности числа.
А то и, возможно, даже значительно меньше.
Поэтому, на самом деле, есть существенное ограничение,
что если n меньше не просто этого адеквата,
но еще и логарифма этого, то утверждается,
что чисел мало, и все эти операции можно делать за от единицы.
Как это делается?
Это сводится к q hip, в котором то же самое, что af по ограничению,
только тут лог в степени 1 четверть.
В реальных ограничениях w хотя бы 64,
то логарифм 6, корень в четвертой степени 6,
но там совсем по мелочи.
То есть, если вы подразумеваете,
что у вас количество элементов не более чем вот это,
то вы работаете за от единицы,
потому что там вот тут действительно начинается черная магия.
Если бы я не чувствовал себя, как это делается,
если бы я не чувствовал себя, как Вася,
я бы успел бы рассказать все эти сведения.
Тут действительно черная магический автомат.
Но так вот, фишка в чем?
Фишка еще заключается в том,
что, оказывается, если куча не сильно большого размера,
мы все за от единицы делаем.
Вывести из-за симптотики это невозможно,
просто структура еще круче.
Просто действительно заметим,
что нам вот это к не имеет смысла делать больше,
чем лог В, правда?
В том плане, что мы знаем,
что мы можем сделать к вот таким.
Но если оно оказалось больше,
чем лог В, давайте просто сделаем лог В.
И тогда получится, что вы все сделали
просто за е плюс В и не паримся.
Можете поподробнее?
Я не очень понимаю.
Нет, почему не имеет смысла делать меньше?
Потому что, напоминаю,
у нас теперь есть куча,
которая говорит, что если у меня в куче размер
не больше, чем лог квадрат,
лог квадрат Н,
то я тогда все эти операции
делаю теперь не за лог N делить на лог N,
а тоже за от единицы.
А, типа за счет того,
что у нас на самом деле операций супер много,
то есть утверждает, что
элементов на самом деле
в куче не сильно много.
А как, правда, ограничение серьезно,
то есть заметим,
что давайте представим себе,
что мы возьмем k,
а что мы паримся?
Давайте тупо по этой лодке возьмем k равно лог В.
Смотрите, тогда первую фазу
мы точно сделаем за е плюс В,
потому что мы все операции делаем за единицы.
Да.
Но теперь мы на второй фазе
можем делать даже просто едва ли
нечестную кучу фибоначи.
И у нас получится ассимтотика
е плюс,
помните, честно это вот это вот,
выделить на k, на лог k и не паримся.
Да.
Так, но теперь давайте
скажем,
но если у нас теперь
да, ну вот, ну точнее так,
но вторую фазу мы скажем,
что пусть k это у нас равно уже.
Нет, хотя нет, пока еще не с k.
Ну все, это просто меньше, чем В,
то есть меньше, чем е.
Ну да, так.
Сейчас.
А почему на лог k, когда на лог
вводим k?
Так, сейчас.
Нет, ну да, так, сейчас.
Так, как мы до этого делали?
Мы делали два степени k, да?
А, ну да, мы не заморачиваем,
потому что да, вы говорили, что это...
Ну да, все получается там.
Да, выделить на k, на лог два степени k,
это уже хорошо, да?
Вот, ну только теперь замечаем, что два
степени k-то у нас это теперь В.
То есть получается, что на
второй фазе мы просто...
Ну можно и не замечать, он просто
сократился.
Нет, это просто да, это было сокращение,
это из-за того, что так мы доказывали,
что каждая следующая фаза после первой
фазы, да?
Но тут фишка в том, что просто после вот
этого вот, вторая фаза уже как равно В,
и значит мы уже просто написали честного прима.
То есть, в чем обратить внимание?
То есть мы по сути только в первой фазе
реально использовали atomic hip.
Во второй фазе мы уже можем просто
взять абсолютно обычного прима с кучей фибоначи,
прям абсолютно честного, и это уже
победа.
То есть сокращение, видите, из-за чего
оно возникло, да? То есть на самом деле
важный момент, то есть помните, да? То есть
просто из-за симптотики оно не выжимается.
Важный момент именно, что тут
всего вот этого вот
очень мало.
Вот, но главное, что
видимо, что у нас есть afhip, который
значит умеет вот все операции делать
за единицы, включая
все декрески, все удаления,
все добавления, вот это вот все.
Вот.
Так что вот такой вот
нот. Так что получается вот такой вот
неожиданный чит.
Вот.
Да.
Просто log v для того,
чтобы у нас наша куча просто за вот
единицы работала.
Да.
А на второй итерации мы возьмем
просто k равно
2 в степени предыдущая k
и получим
e плюс v. Но при этом
2 в степени предыдущая k это 2 в степени
log v, то есть v.
Ну и все.
То есть это означает, что мы как бы разрешаем
себе расшириться до упора.
То есть мы просто
делаем все уже честно.
И все.
То есть на втором шаге мы
можем делать
мы можем делать e плюс log v.
Ну да, потому что
на втором шаге у нас тут уже не v, а v поделить
на log v.
Поэтому как бы тут...
Поэтому log v
нам тут уже
амалину...
Сетом будет
e log v.
Да, сетом тут e будет. Все-таки куча фибоначи
нужна. Но уже в смысле более
старой версии будет адекватно. Да.
Ура. Да будет радость.
Но правда
напоминаю, это работает только
в предположении, что вы живете,
что у вас там эти целые
числа, вы в них это
и получается так.
Ну то есть честно скажем,
автомат, как вы уже поняли,
на самом деле вот этот автомат очень сложно
написать. Ну вот так,
чтобы он хоть реально работал. Потому что
в нашей реальной жизни
вот этот лог это вообще 1,2.
Написать кучу, то есть куча,
в которой не более чем один
или два элемента.
Да, то есть для этого
там, ну там, главное теоретическое
там как же очень сильная гадость, на самом деле.
Честно, а вот как тут
вообще... А, или даже
там даже пятая степень, может быть
даже не четвертая, но это неважно.
Ассертическое
количество операций
сложения, сравнения целых чисел, да?
То есть, ну да.
Нет, ну как всегда, мы же как
ну одно действие, да, ну сложение,
ну там еще есть подразумевается,
что они умножение умеют делать.
То есть, что мы должны уметь делать?
Ну, обычно
можно считать, что просто
то же, что и обычно. Только
помним, что за у от единицы мы делаем
арифметическую операцию только
не со всеми числами, а только с
битными числами.
То есть, условно говоря, мы конечно часто
неявно предполагаем, что у нас
мы с произвольными числами работаем
за у от единицы.
Сейчас мы обычно
должны помнить, что мы только с битными числами
работаем за у от единицы, а с произвольной
битности числами уже длинная арифметика
с нормальной асимптотикой.
Интересно, там нужно, например,
делить или умножать?
Делить или какие делить?
Делить или умножать?
Ну, по-моему,
ну там надо как бы битные операции какие-то делать,
ссорики всякие делать, кстати, надо.
Нет, ну вот это
все делается за у от длинных,
а вот деление умножения
уже плохо.
Ну да, ну да.
Тут можно сказать так,
что явно, я
всех деталей не скажу, но
почему-то везде подчеркивается важность,
что умножение б битных чисел
делается за у от единицы, это базовая операция,
потому что.
Сложение вычитания за у от единицы
же не смущает, даже несмотря на то, что это
б битное число?
Ну, не смущает.
Ну, а почему тогда умножение смущает?
В общем, считается, что умножение тоже там
какой-то, видимо, схемой за у от единицы делается.
Вот.
Так.
Так, ух ты, сколько времени
мы на это потратили?
Ну, давайте, ладно, давайте потратим
еще, потому что.
О.
Ну ладно, вообще,
кстати, время уже следующего перерыва пришло.
Да,
я прошу прощения, мой, конечно, косяк,
ну ладно, ну ладно, давайте тогда,
давайте тогда немножко еще
отдохнем, надо, да?
Потому что алгоритм краскала говорит
вообще наглую вещь.
Он говорит, так, а давайте просто
отсортируем все ребра и будем добавлять их
в порядке возрастания.
Ну, по принципу, если, так,
вот действительно, потому что действительно
вот мы, есть у нас какой-то вот
граф, да?
Там вот уже какой-то набор ребер.
Так, а теперь давайте рассмотрим
минимальное ребро,
которое вообще сюда можно добавить
без зацикла.
Так вот.
Ну, достаточно очевидно, что это
ребро добавить можно, правда?
Вот, понятно, да?
Так, ну да,
все уже мертвы, ну ладно.
Да, ну какой-нибудь там разрез
придумать легко.
Поэтому автоматически возникает идея, давайте
просто отсортируем все ребра и для каждого ребра
будем говорить, так, можно его добавить?
Да, добавляем, нет, нельзя. Остается только,
то есть задача после этого сводится к тому,
что надо просто быстро определять
для ребра, можно ли его
добавить, да или нет?
Вот.
Как же это сделать?
Ну, да,
ну давайте, да, ну конечно, есть СНМ.
Так, ну ладно,
что такое СНМ, наверное, объяснять тоже
не надо, да?
Или надо?
Не, не надо, ну классики, ну вот.
Ну, давайте, ладно, не будем тратить время на формальное
объяснение, что такое СНМ.
Вот.
Ну, давайте так, ну вот,
поговорим только, ну вот, ну давайте подумаем,
за какую симпатику можно его реализовать?
Ну, конечно, если бы вы объединяли эти два множества
в тупую, это будет e лог e плюс
очевидно v квадрат.
Но важный момент.
Так, но важный момент,
давайте все-таки отметим этот важный момент, что
если бы вы объединяли два множества путем
переливания меньшего в большее,
то это бы работало, то
суммарное время объединения было бы
v лог v.
Ну вот, ну тоже причины понятные,
тут все впадает.
Вот.
Ну, СНМ отчасти это и предполагает.
Потому что мы помним, да, что у СНМ
есть две евристики, евристики сжатия
путей и ранковый евристика.
Так, ну начнем
с простого. Такой уровень, так сказать,
изи. Там изи при изи.
Если мы используем только ранковую
евристику, то тогда у нас
все работает за логарифом.
Почему? Да.
Потому что мы по индукции доказываем, что если у вершины
ранк x, значит у него в поддереве хотя бы
два степени x вершин.
А ранка мы как бы в высоту, или просто
размер поддерев будем брать?
Ну, например, давайте...
Просто же размер брать, тогда все очень...
Чего размер?
Ну не размера логарифа, а размера, видимо, да?
Нет, ну просто же можно размер...
А, ну по-разному. Дальше как реализуете?
Ну, все-таки нам будет
наверное проще все-таки для последующего
анализа сказать, что все-таки
там ранка вершины, это просто ее
высота в дереве без сжатия
путей.
Ну вот, и мы говорим, что когда мы объединяем
два дерева, мы просто подвешиваем
дерево с меньшей высотой и дерево
с большей высотой.
А если мы объединяем два дерева с одинаковой
высотой, то как бы
у одной вершины ранка увеличивается
на 1.
Но по индукции доказываем, что у
дерева высоты h хотя бы 2 в степени h вершин,
не обязательно сбалансированных, конечно,
но вот хотя бы 2 в степени h будет.
И отсюда следует, что
максимальный ранк вершины не более
чем логариф. Да, это изи,
тут обсуждать нечего.
Но более важная для нас вещь,
кстати, с точки зрения дерева
доминатора вчера.
Если вы не используете ранговую эвристику,
но используете эвристику сжатия
путей,
то оказывается,
в дело вступает амортизация,
но учетная стоимость
каждой операции
все еще логариф.
Если вы делаете только сжатие путей,
вот как бы
вчера эволинк апдейт,
тогда тоже будет работать с логарифом, сейчас мы это докажем.
То есть, представьте себе,
что объединяете так,
что вам просто жестко говорят,
кого куда вешать,
но вы имеете право делать
эвристику сжатия путей.
Как же доказать,
что мы все делаем
за логарифом?
Вот.
Как же нам такое доказать?
Ну, давайте посмотрим
вот на что.
Давайте рассмотрим каждое конкретное.
Ну, давайте так.
То есть, мы воображаем ранг вести...
Да. Давайте, да. У каждой вершины,
когда мы строим потенциально дерево,
у каждой вершины есть ранг.
Ну, ранг, который был бы,
если бы сжатия путей не было. Согласны?
Ну, заметим, что в корнях дерева
он, конечно, периодически увеличивается,
но если вершина перестала быть корнем,
то этот ранг остался фиксированным
раз и навсегда.
Тогда что мы делаем?
Тогда
на каждом сжатии,
когда мы вызываем какое-то вот это сжатие
путей от вершины,
то что у нас
действительно происходит?
Вот. Ну, нот происходит,
у нас следующее.
Ну, вот. То есть,
ну, заметим, что
нот... То есть, мы замечаем,
что у каждой вершины
увеличивая нот.
То есть, когда мы проходимся по либу, да,
то есть, как бы ранг,
ранг увеличивается у вершины.
Вот.
Ну, а теперь нот...
Ну, возникает такой естественный вопрос.
Соответственно, нот...
То есть, нот на сколько?
Вот.
Значит, что тут тогда надо сделать?
Что тут тогда надо сделать?
Вот. Ну, на самом деле, нам...
Нас будет интересно, конечно, не только то, что ранг
увеличивается, нас будет интересовать,
а насколько ранг увеличивается.
А именно. То есть, вот,
допустим, у нас есть ребро из вершины
х в вершину,
так сказать, пр от х, да?
Тогда мы вот этому ребру...
Тогда вот есть
ребро е и, так скажем,
ранг ребра
равен к чему? Он будет равен
просто разности
с з.
Да,
с з я беру
в истинном ребре.
То есть, помните, у нас есть два дерева.
Знаете, вот, кстати, в этом месте
вводится два, то есть, официальное понятие.
Есть реальное дерево, да?
То есть, если мы честно объединяли и ничего не сжимали,
было бы какое-то одно дерево, да?
А есть виртуальное дерево, которое у нас происходит,
когда мы делаем жатие
путей, да?
Поэтому с з я беру
по реальному размеру.
Так, понятно, о чем я говорю?
Так вот, я беру вот такую разность
с з,
но не просто разность,
а двоичный логарифм.
И не просто двоичный логарифм,
а двоичный логарифм,
округленный вниз.
Ну, заметим, что...
Так, ну, тут, конечно, скобчику нам поставить.
Так, ну, мы понимаем, что
ранг, он как минимум ноль,
а максимум логан.
Вот.
А теперь давайте посмотрим,
когда у нас идет такое
сжатие путей, то есть, сжатие вот всех
этих ребер.
Ну, заметим, что что у нас вообще может
происходить.
На самом деле,
то есть, ну, у нас есть, как бы, если рассмотреть
тут это как последовательность рангов, да?
То есть, какие тут ранги бывают? Они бывают рандомные.
Ноль, два, там один, два, там
какой-нибудь там три, там еще
какой-нибудь там один,
там два. Так вот, идея такая.
Давайте разделим
эти ребра на два множества.
Те, у которых вот соответствующая
чиселка встречается в последний раз,
и все остальные.
Ну, заметим, что тех ребер,
где вот чиселка встречается в последний раз,
это их логарифм, правда?
Что значит последний раз?
Ну, рассмотрим,
ну, когда мы сжимаем, делаем сжатие
путей, мы
рассмотрим, вот рассмотрим ребра,
рассмотрим последовательность рангов, возникающего на этих
ребрах, да?
Эти, ну, ранги бывают от нуля до лога.
В общем, все числа целые. И каждое число
сколько-то раз...
Но каждое число где-то встречается в последний раз.
Если встречается, конечно.
Так вот. Вот давайте рассмотрим
ребра, которые встречаются в последний раз.
Их логарифм.
Логично, да?
Есть остальные ребра, их не обязательно
логарифм. Но здесь возникает
другой важный момент.
Теперь замечаем, что у каждой вот этой вот
вершины, если вот это ребро,
это чиселка не последняя, то в результате
сжатия пути у этой вершины
ранг ребра увеличится.
Логично, да?
Логично, да?
Да.
Ну, теперь смотрите.
Вот предположим, что у вас тут
ранг ребра, допустим, к. То есть
это означает, что у вас тут
сж увеличилась, то есть
хотя бы на 2 в степени k, правда?
Понимаете, да?
Ну, там где-то 2 в степени k, да 2 в степени k
плюс 1 где-то.
А если у вас где-то там потом было еще
ребро, где тоже плюс 2 в степени k,
то в результате сжатия
пути отсюда-сюда ССС будет
увеличиваться хотя бы на 2 в степени k
плюс 1.
И тогда получается, что ранг этого ребра
увеличивается.
Понимаете, да?
Значит, еще раз.
Значит, смотрите, у каждой вершины есть
ребро виртуальное.
У этого ребра есть ранг.
Периодически это ребро меняется
в сжатии путей.
И оно увеличивается.
То есть периодически меняется, но периодически
нет. Ребро меняется, ранг иногда
остается прежним, иногда увеличивается.
У нас такая ситуация.
Значит, если
мы рассмотрим какое-то конкретное
сжатие какого-то пути.
Если выяснилось, что у вас есть
всего какого-то ранга и выше у вас тоже
есть ребро такого ранга,
то тогда я утверждаю, что ранг этого ребра
в результате сжатия увеличится.
Сейчас ранг ребра это что?
Ранг ребра, вот
определение, логарифом
разности сайзов между родителем и
мной.
Что такое ранг нижнего ребра?
У каждой вершины есть ребро
торчащий из него.
Оно одно определяется этой вершиной.
Поэтому ведущие вы меня
рассматривать нельзя, потому что в одну вершину
может вести много ребр.
А мы ранг считаем
у виртуальных ребр тоже?
Нет, я больше скажу.
Ранг мы считаем именно у виртуальных ребр.
То есть ранг это состояние
текущего дерева с текущими
ребрами? Да, с текущими ребрами.
Хотя СЗшки берутся по...
они фиксированы.
Фиксированы в том плане берутся по реалии.
То есть у корня они не фиксированы,
потому что корню могут кто-то подвешивать.
Но как только вершина перестала быть корнем,
это значит, что СЗшка в ней
всегда остановилась.
Это приводит к тому, что при некоторых подвешиваниях
ранг ребер
ведущих в корень мог увеличиться
автоматически.
Нет, не надо.
Нет, это не надо.
Потому что я хочу доказать,
я хочу продемонстрировать следующую идею.
Что у вас каждое сжатие пути
работает, во-первых,
за от логорифам
плюс количество
увеличений рангов.
Можно так оценить.
Но теперь заметим, что
суммарное количество увеличений
рангов у всех ребер, очевидно,
не более чем n логн.
Ребер у нас n, в смысле n-1.
Он увеличивается не более, чем логн раз.
Ну все.
Получается, что
суммарно мы все операции
сделали не более, чем за q
плюс n логн,
где q количество вызовов сжатия пути.
Ну и подвешивания.
Почему же за логн работает?
Ну не совсем.
Сжатие работает
за...
Рассмотрим, ребра бывают
ребра с последним таким рангом
и ребра с не последним
таким рангом.
Каким-то.
То есть для каждого числа
существует ребро,
последнее ребро с таким рангом.
Так вот.
Последних ребер их логорифом,
потому что у нас логорифом рангов.
Поэтому мы записываем,
что мы потратили на них
логорифом времени.
На остальные мы тратим
количество...
То есть некоторое количество
увеличений рангов.
Потому что у каждого из оставшихся ребер
ранг увеличивается.
Ну все.
Тогда получается суммарно q операций,
включая сжатие пути,
это будет работать за q логн
плюс количество увеличений рангов.
Но так как у нас
увеличение рангов суммарно не более,
чем n логн, то получается,
что амортизированная асимптотика
что-то типа лога.
Вот. Понятно?
Вот.
Вот так.
Может быть не совсем аккуратно, но нормально.
Вот.
Так что получается, что само по себе
сжатие путей уже дает вам логоризмическую
асимптотику. Это нормально.
Чем больше операции, тем ближе
эта асимптотика будет к единице.
Потому что можно делать более аккуратные
доказательства и сказать, что
учетная стоимость будет не более, чем
логорифом, скажем, n квадрат
поделить на q.
Вот какая-то такая фишка будет.
Вот.
Но там две оценки.
С одной стороны такая, с другой стороны такая.
Поэтому если q стремится к n квадрату,
то у вас эта учетная стоимость стремится к единице.
Вот.
Но нас устроит логориф.
Но оказывается, есть
более серьезная
асимптотика.
Так, ну-ка давайте сейчас попробуйте нод.
Так.
Ну а теперь уровень, так сказать, ну не хар, конечно,
но тоже приличный.
А теперь представьте себе, что мы иранговую
евристику вводим.
И евристику сжатия
путей.
Тогда мы сейчас обойдемся
простым утверждением.
То есть я утверждаю, что, по крайней мере,
амортизировано.
У нас каждая операция
будет работать за o от log
звездочка.
Да, до обратной функции
керманом, к сожалению, не наживем.
А вот лог звездочку
все-таки попробуем.
Да, лог звездочка, да, та самая,
что количество взятия логориф.
Ну, за счет чего мы будем
работать? Мы будем
работать за счет того, что
как бы мы помним, да, что
в ранговой евристики, да, напомним
того, что если у вас размер,
то есть ранг какой-то
вершины, как это,
ну, ранг у нас как называется, давайте
h от вершины
равен какому-то числу
x, то из этого следует,
что cz от v
больше либо равен
2 в степени x.
В реальном
исчислении это продолжает быть
верно, правда?
Но из этого можно сделать
очень приятный вывод.
Какой? Из этого
можно сделать вывод, что
количество вершин
таких, что h от v
равно x, их
меньше либо равно, чем n поделить
на 2 в степени x.
Так, если h от v
равно x,
то размер этого
реального поделим
2 в степени x, хотя бы.
Ну, также по индукции доказываем,
мы это уже делали, да? Но это
реального дерева, конечно, без сжатия пути.
Но заметим, что у каждой такой вершины
под дерево свое, это под дерево состоит из
вершин меньшего ранга, да?
Причем своих.
Из этого следует маленькая приятная вещь,
что количество вершин каждого ранга
не более, чем n поделить на 2 в степени
этот ранг.
И мы сейчас будем этим
пользоваться.
Потому что
смотрите, вот у нас есть ребро.
Давайте представим себе, что у нас опять
есть виртуальное вот это ребро,
которое ведет нас из вершины v в вершину
p от v.
Понятно, да?
И теперь ситуация такая.
Я буду говорить, что пусть у меня
ребро e тяжелое,
если
cz от
p от v
больше либо равно,
чем
ну, например,
1.9
в степени cz от v.
Ну, вы сейчас увидите, почему
1.9, а не 2.
Вот.
И легко иначе.
Тогда, смотрите,
когда вы делаете сжатие
путей, у вас
как бы cz от pr
всегда увеличивается, ну, кроме последнего ребра,
правда?
Вот.
Но на самом деле теперь вас интересует вот что.
Когда вы рассматриваете путь,
рассмотрим путь.
Так, ладно.
Рассмотрим путь, путь, путь, путь.
Тогда замечаем, что
ладно, это ребро не меняется,
а у этих всех cz от родителей явно
увеличивается, правда?
Ну, смотрите, некоторые из этих ребер
тяжелые,
тяжелые,
тяжелые. Но сколько,
ну, как вы думаете, сколько у нас тяжелых ребер?
Вот ваша ставка. Сколько у нас тяжелых ребер?
Ну, типа лог-свездочка. Конечно, лог-свездочка,
потому что каждый раз, когда вы скачете,
у вас тут x превращается в
2 степени x, да?
Ну, понятно. То есть тяжелых, их всего
там, лог-свездочка.
Сколько легких?
Лог и 100 звездочек.
Ну, кстати, да.
Но это нам не важно.
Важно другое,
что у каждого легкого ребра
cz родителя увеличивается,
правда?
Но заметим, что у каждого
ребра, торчащегось с какой-то
вершины, эта cz может увеличиваться
ограниченное число раз.
То есть если тут cz равно x,
да,
то на самом деле
увеличиваться это легкое ребро,
пока оно легкое, оно может
сколько раз?
Оно может увеличиваться не более, чем
1.9 в степени x раз,
правда? А теперь, сколько может быть
суммарно увеличений?
Ну, суммарно увеличений может быть,
то есть сумма по всем
вершинам получается 1.9
в степени
там
собственно
ранг от x.
Увеличение же может быть все-таки...
Чего?
Мы же каждый раз, когда
увеличиваем предка,
размер предка увеличивается
хотя бы в два раза, нет?
Нет.
А нет, нет, нет.
Не обязательно, вы же можете меньше ранг подвешивать,
сильно меньше ранг большему,
одну вершинку можно...
Так, ой, так, набрал ли я?
Так, нет, подождите,
у меня тут сумма 1.9
на, так сказать...
Cz.
А, нет,
подождите,
зачем нам Cz?
Почему бы нам тут h не сделать, а?
А с правой что?
Тоже h.
Подстанет ли от этого хуже?
Мы сейчас будем пользоваться тем,
что у нас ранг строго увеличивается.
Поэтому получается,
у каждой вершины
количество операций,
когда у нас будет сжиматься у нее легкое ребро,
это будет происходить не более, чем
1.9 в степени ее ранг раз.
Почему это?
Как только у нее ребро станет вести в ранг
вот такой или выше,
оно станет тяжелым.
То есть он продолжит
увеличиваться, но мы его просто уже не будем
учитывать. То есть оно перестанет быть легким.
Нам интересует,
сколько раз у легких ребер
увеличивается ранг конца.
Именно у легких.
Что?
Почему в квадрат?
Почему в квадрат?
Нет, ну на самом деле,
смотрите, на самом деле
это не превосходит,
то есть это что такое?
Это равно сумме по всем рангам,
то есть от нуля до вот этого h,
чего? 1.9 на h
на количество таких
вершин,
что h
от v равно h.
Логично, да?
А теперь мы неожиданно вспомним,
что у нас вообще этих вершин мало.
Два в степени.
Поэтому получается, что сумма
это 1 делит на 9
в степень h, делить на 2
в степень h, а и на n
внушить.
Ну что, поздравляю.
Мы получаем геометрическую
прогрессию, сходящуюся к константе.
Всё. Ну, кстати, можно даже
полторашку взять на самом деле, может быть
даже константа будет более адекватной.
Адекватной.
Вот.
Ну вот, собственно, и всё.
Этому, получается, показали, что
лёгкие ребра, они
перевешиваются не более, чем
столько раз.
Да.
А все остальные тяжёлые.
То есть, получается, лёгкие ребра суммарно
перевешиваются от n раз,
а тяжёлых ребр на каждом шаге
лог-звёздочка. Правда, с оговоркой,
что лог-звёздочка получится
логарифом берём не по основанию 2,
а по основанию 1.9.
Ну, оговорка в том, что
сколько тут тяжёлых ребр?
Смотрите, в определении
лог-звёздочка мы логарифом каждый раз
брали по основанию 2, да?
А здесь надо, как бы, получать при таком
доказательстве, надо брать по основанию 1.9.
Так.
Что мы сейчас вообще доказывали?
Мы доказывали, если мы комбинируем эти
левристики. Да. А где мы пользовались
вообще рангами? А пользовались мы тем,
чтобы утверждать, что каждый раз,
когда вы ведёте, что у вас ребро
идёт от вершины с каким-то рангом
к вершине с строго большим рангом.
Вот.
То есть, только в этом?
Нет, ну ещё с 55.
А ещё пользовались...
Но ещё мы этим пользовались,
когда доказывали, что если у вершины
есть какой-то ранг, то у неё очень много
вершин в подделье.
Потому что в общем случае это, конечно, не верно.
Хорошо. Так. Ещё
вопрос.
Почему мы можем...
Почему там такая сумма?
Почему лёгкие ребра мы можем
перевешивать такое количество раз?
Ну, потому что каждый раз,
когда вы перевешиваете ребро, ранг
конца строго увеличивается.
Да.
Это вот из-за...
Да, потому что мы следим за тем,
что у нас каждый ребро ведёт из вершины
какого-то ранга в вершину с большим рангом.
И выше-то тоже всё больше, больше, больше,
строго больше.
Поэтому получается, до того, как оно станет
тяжёлым, пройдёт не более чем вот столько
итераций.
Да.
Нет, ну мы так объявили, что
как только у нас ребро ведёт
в ранг 1.9 от нашего,
в степени нашего,
то мы объявляем, что ребро тяжёлое.
Вот просто берём и объявляем.
Но выясняется...
Ну вот.
Ну тогда получается, что
пока оно будет лёгким, значит
этих обновлений будет вот столько.
Вот.
Но только выясняется, что
вершин-то такого ранга, в общем-то,
не сильно много.
Точнее, наоборот, сильно много.
Точнее, наоборот, не сильно много.
Поэтому, если посуммировать
это по всем рангам, вообще получается
от n. Потому что n получается на какую-то
геометрическую прогрессию.
С коэффициентом меньше единиц.
Вот.
То есть, как я уже сказал, мелкая
оговорка заключается только в том, что
лог звёздочка берётся не по основанию 2,
а по основанию 1.9.
Так что остаётся только в качестве упражнения
доказать, что лог звёздочки
по двум разным константам основанием
отличаются в константу 1.
Понятно, да?
Вот. Так.
Но это вот что уж точно нужно было сказать
про СНМ.
Так, есть ли тут какие-то вопросы?
Да нет, наверное.
Отлично.
Значит, у нас осталось ещё чуть-чуть.
Так, что, ещё вопросы есть?
Нет, всё.
Ладно.
Так, ну хорошо, ладно, это я должен был
проговорить. Хотя, конечно, думаю, для многих
из вас это, конечно, не новость.
Поэтому мы всё-таки немножко
поговорим об атоме к себе.
Ну, как бы, я сейчас не буду говорить
о том, как это сделать в предположении,
что там лог, там
корень четвёртой степени из лога,
допустим, у вас чиселок в хипе,
как всё за единицу делать.
Вот.
Вот, ну сейчас вот
просыпаю. Так, вот сейчас будьте внимательны.
Потому что сейчас, конечно,
ждёт ещё одно эпическое упражнение
на амортизацию.
Там на амортизацию аля Кучи Фибоначчи.
Ну и софт-хип
тоже чуть-чуть. Вот.
Значит, смотрите.
Итак, ситуация. Представьте себе,
что у вас есть
мистический чёрный ящик,
который умеет делать
insert,
там, extract min
и чё-то, ну вот. А также
там всякие, ну понятно уже там
неважно, decrease key,
да и просто delete.
И всё это за от единицы.
Но при этом чёрный ящик
требует, что СЗшка
не имеет права быть
больше либо равна, чем лог квадрата.
Каким образом нам теперь
написать, теперь вот, то есть вот такая
куча, допустим, у нас есть.
Вот, понимаете, да?
Теперь задача.
Значит, задача такая.
Теперь.
Нам теперь требуется.
То есть нам теперь требуется
написать кучу на n элементов.
Но так, чтобы все операции работали
за у от единицы.
То есть теперь вот, когда
куча уже может быть размера n,
но хочется, чтобы теперь все insert
и компания
там insert были у от единицы.
А всякие deletes
с экстракт-минами, естественно,
были уже теперь
не log n, но
log log n.
Вот, хочется себе
как бы сократить
асимптотику в log log n раз.
Вот, делаем ставки.
Как же это можно вообще сделать?
Ну, первая идея
на самом деле
достаточно проста.
То есть основным действующим
лицом у нас будет
B-ичная куча.
Вот сейчас послушайте внимательно,
сейчас весело будет.
Значит, внимание.
У нас есть B-ичная куча,
где B
прям будет практически честная такая
B-ичная куча,
где, вот сейчас неожиданный чип будет,
где B равно,
угадайте,
чего?
57.
Ну, 57 это константа.
Log квадрат.
Нет, ну не квадрат, просто
log n.
Такая log н-ичная куча.
Вот, то есть такая log н-ичная
куча, но при этом, смотрите, как она устроена.
В каждой вершине
много ребер,
но, смотрите, устроено оно так.
Все эти ребра с вершинами,
которые их представляют,
они расположены
вот это вот. Вот давайте
этот черный ящик мы назовем AF HIP.
Вот, понимаете, да?
Так вот.
Идея будет такая, что мы будем
хранить не только вот этот вот элемент,
но и для каждого вот этого дерева
вот эти минимумы
мы будем, то есть эти вот минимумы
будут храниться в
тоже AF HIP.
Ну, потому что, понятно, размер кучи логарифом,
поэтому нормально, да?
Вот.
Ну, начнем с того, что мы можем
вообразить себе такого рода кучу, да?
Непонятно, почему все
нормально.
Чего?
Нам же нужен лог в подряд в предположении,
что мы сделаем хотя бы n запросов
к этой куче.
Почему
лог в квадрат запросов?
Нет, у нее размер
не больше, чем лог в квадрат.
Нет, у нее...
Ужинение, что мы сделаем хотя бы n запросов,
и тогда все работает. Нет, нет, нет,
нет, никаких таких
предположений нет. Просто вот лог в квадрат...
Нет, у нас есть n, где n
максимальный размер вот этой кучи, да?
Вот мы планируем реализовать кучу,
которая работает с n элементами.
Про n мы знаем только, что n
оно как бы влезает в
нашу битность.
Сейчас, тогда...
Какое условие
у AF кучи, что если...
Значит, да,
AF куча говорит, что я требую
как бы, чтобы размер был
не более, чем лог в квадрате.
Что такое количество?
Нет, максималика...
Ну, количество элементов.
Ну, можете считать, что n это
какой-то там...
Ну, можете считать, что n это
какое-нибудь там число,
влезающее в вашу битность. Ну ладно,
можете считать, что n это там где-нибудь 2 в степени...
Там, может быть, лог в квадрат
от, допустим, 2 в степени битности,
например. А, то есть n это не количество...
Ну, это сверху. На самом деле
количество.
Ну, я присоединил, что
непонятно, что такое n.
Ну, нет, n это количество
здесь. Нет, вот слева
что такое n? Нет, слева
оно тоже иницируется этим.
Я не понимаю, в чем
проблема, если честно.
Сейчас,
при каких условиях AF
хип работает за от 1?
Ну, при условии, что
размер у него не сильно большой.
Ну, ладно, можете считать...
Если вы хотите отвязаться от ограничений,
можете считать, что
только n, допустим,
это не более чем лог в квадрат
от 2 в степени w.
То есть, в общем,
это получится там...
То есть, лог
от 2 в степени битности это битность.
В общем, короче...
Не, ну просто это странно,
потому что можно для одних и тех же операций
сказать, вот у меня n равно
10 в сотый.
А можно потом сказать... А, понял вас.
В этом
тут и проявляется битность.
Потому что, напоминаю, что мы живем
не в рандомном мире. В рандомном
мире это было бы невозможно. Мы живем в
мире, где все числа, все ключи
это битные
числа.
Ну, где... Не вот это b,
ладно, там w, битные числа.
У нас есть великое число w,
которое фиксировано.
И мы работаем только с числами,
которые в эти битности
влезают. В частности, мы
вынуждены заключить, что n меньше,
чем 2 в степени w.
То есть, поэтому, видите, у нас на n
само есть ограничение.
Вот.
Ну, кстати, может там оно еще более сильное,
но давайте скажем, что оно такое.
Вот.
Так, поняли, да?
Вот.
То есть, в общем, ну, короче, ладно,
можете считать, что здесь cz меньше
льбрана, лог квадрат от вот этой
штуки.
То есть, это равно, ну, вот, ну, короче, там.
Вот.
И вот, предположим, что у нас
а оказалось, что
afhip у вас не сильно большой.
То есть, afhip небольшой,
и оказалось, что для такой небольшой
он все за 1 делает.
Вот.
Ну, тогда, на самом деле,
то есть, в общем-то,
можно теперь так сказать.
Если бы не было дикрийский,
то мы вообще, то есть, смотрите, какая
высота у этой кучи?
Лог, лог.
Ну, я бы так сказал, она лог
по основанию bn, правда?
Ну вот, это равно
логн поделить на логб.
Это равно
логн поделить на, получается,
лог логн.
То есть, в принципе, получается,
что если бы вы делали только честные
инсерты и, допустим,
экстракт мины или дилиты
путем свапы с последним элементом, да?
То, получается, работало бы за высоту.
Почему за высоту?
Ну, потому что, правда, тут надо оговориться,
как вы делаете
shift down, да?
Как делается shift down?
Ну вот, берем вершину, просматриваем детей,
если мы находим минимального ребенка,
если он меньше и нас, то ставим вместо себя
и идем дальше, да?
Но, как бы, зачем нам нужен afhip?
Для того, чтобы делать не за от количества детей,
а за от единиц.
Понимаете, да?
Все тут вставили нормально.
Что мы еще раз храним в вершине, кроме детей?
Храним, короче, детей.
Вы можете считать, что это
обычная быличная куча, да?
То есть, в каждой вершине мы еще
храним, как бы, берем на их детей
и этих детей храним в своей куче.
То есть, в каждой вершине хранится,
так сказать, куча детей.
Вот, ну в смысле hip дети.
А как это сказать? Хранится куча детей.
А, мы берем всех наших детей и запихиваем их в кучу.
Лучше не становится.
В общем, суть поняли, короче.
Но это все прекрасно работает.
То есть, сив даун сделали,
сив тап еще легче.
Но все прекрасно работало,
если бы у нас не было мистической операции
дикриски.
О которой мы тоже мечтаем,
что она от единиц.
Но просто так сделать дикриски
не получается, потому что
в обычной куче мы дикриски достигаем за счет сив тапа,
а сив тап у нас все-таки за столько работает.
Да и, кстати, с инсертом у нас
со вставкой в эту кучу у нас
общие проблемы, кстати.
Потому что мы
вставляем куда-то и делаем сив тап.
Чего?
У нас дилет есть?
Нет, погодите.
Как раз наоборот, если у нас дилет есть,
то не очень понятно, почему инсерт работает за.
Хотя вот возникает ощущение,
что если вы делаете N подряд,
ну хотя нет, погодите,
то смотря как создавать кучу.
Потому что, вообще говоря, не очень понятно.
Хотя, конечно, есть, действительно,
ощущение какой-то амортизации есть.
Почему?
Ну потому что, да, действительно,
хотя нет, если вы тут прям
построили честную кучу, то как бы там
будет честный сив тап.
Поэтому надо очень аккуратно думать.
Вот. Значит, как справиться?
Ну, правда, тут у вас высекал вопрос,
а зачем нам требуется, что в IFHP
не лог, а лог квадрат вообще?
Вот. А идея, на самом деле, такая.
Идея.
Значит, дело в том, что
мы объявляем, что у каждой вершины
как бы есть ранг, то есть та самая высота.
Так вот, на самом деле, идея
будет такая. У нас в системе
будет, на самом деле, несколько
вот таких вот деревьев.
И у каждой будет своя высота.
То есть, элементы будут храниться не в одном
дереве, не в одной куче,
а в нескольких деревьев.
И у каждой будет там своя
какая-то высота.
Тут h1, h2, h3 и так далее.
Понимаете, да?
Но у вас будет теперь
важное ограничение.
Важное ограничение. Какое у нас будет
важное ограничение?
Очень простое. Я буду требовать,
что у меня количество
деревьев с
h равно какому-нибудь
фиксированному h0.
Оно всегда должно
быть не превосходить.
Оно должно быть
строго меньше, чем b.
Но идея у меня будет такая.
У меня будут периодически появляться
новые деревья. Например, когда я буду делать
insert, у меня будет появляться дерево
высоты 0.
Но теперь у меня возникает следующая идея.
Какая у меня идея возникает?
Неожиданная.
Она у меня возникает так.
Она у меня возникает следующим образом.
Как только у меня будет появляться
две деревья,
я буду их пытаться объединить в
одно дерево.
Понимаете, да?
Понятно, что каждый раз, когда у вас
появляется дерево, вы можете класть монетку
и тогда, получается, этими монетками
это дерево оплатит.
Получается, эта операция будет бесплатная.
Как вы уже догадались,
как мне этих деревьев, так сказать,
мы будем тоже хранить в куче.
И размер этой кучи будет лог квадрат.
Поэтому нам тут
требуется лог квадрат.
А теперь давайте поподробнее.
Потому что как нам объединить
b деревьев в одну кучу?
Ну, как просто?
Дело в том, что
не совсем так.
Нам же этот огневой элемент желательно
еще откуда-то взять.
Нет?
А мы умеем
умелдить
эти два окипа
от 1?
Нет.
Просто так у нас молдов нет.
У нас задача есть b деревьев
от одинаковой высоты.
И превратить их в одну.
Совсем переливать не получится.
Каким образом?
Просто там...
Нет, ну не совсем.
У дерева фиксированы высоты,
количество элементов все-таки
имеет ограничение.
И потом хочется склеивать
все-таки как-то за единицу, что ли.
Или хотя бы за b.
Но амортизация нам должна
позволять склеивать за b.
Потому что если у нас с каждому
дереву дана монетка,
то для того, чтобы склеить
b деревьев в одну, у нас получается
б-1.
Одну монетку мы еще докинем.
Поэтому должно
получиться, что операции вроде 0.
Поэтому
должно хватить.
Или не должно?
Не должно?
Но как это сделать?
Для этого давайте
рассмотрим все-таки такое.
Давайте от этого хипа попробуем немножко уйти.
И в каком плане?
В каком плане?
Потому что мы здесь вот писали хип в предположении,
что в каждой вершинке находится
свой ключ. Правда?
Ну в каждой вершинке есть ключик
и хип на детей, да?
А теперь у нас какая-то другая идея.
А давайте посмотрим...
А давайте не будем в каждой...
А давайте только в листьях будем
хранить реальное значение.
То есть будет
такой бытичный деревоотреск?
Понятно о чем
речь, да?
Ну минимум
на поддире.
Ну может быть на подотреске,
но это будет не совсем подотрезок,
там будет немножко по-другому.
Сейчас вы увидите. Но давайте правда
подумаем. А вот эта вот оптимизация,
как она нас...
Впрочем, значит...
А, впрочем, сейчас мы вообще все по-другому
будем делать. То есть конечно
сифтапами, сифтдаунами у нас конечно
возникают проблемы.
Вот, понимаете, да?
Вот.
Вот.
Ну вот. Ну значит теперь
что нам теперь хочется
сделать?
Вот. Хочется сделать
нот...
Ну вот как
теперь нам это сделать?
Ну теперь понятно, как вы уже поняли, да?
Со вставкой у нас никаких проблем нет.
То есть взяли и сделали.
Но теперь у нас и дилит,
и дикризки, но на самом деле
могут быть сделаны так.
Как сделать дикризки?
Очень просто выпилить...
Ну на самом деле дикризки у нас как всегда
может сводиться к дилиту, правда?
То есть выпилим каким-то образом
этот элемент, а потом его просто добавим
к нам просто с уменьшенным, правда?
Хотя нет, мы же это не хотим...
А, подождите, мы этого не хотим делать.
Потому что, да,
у нас дилит хочется за адекватную асимптотику,
а дикризки все-таки за единиц.
Вот понятно, о чем я говорю, да?
Или непонятно?
Ау!
Вот, ну да, вроде понятно.
Но теперь возникает вопрос.
Как делать дикризки?
Ну, значит, идея возникает следующая.
То есть...
Ну, соответственно,
пытаемся делать дикризки.
Вот, но на самом деле
можно попробовать сделать так.
Вот у нас есть...
То есть какой-то элемент мы уменьшаем,
он в каком-то дереве находится, да?
Вот, но теперь вот отправляемся...
Хорошо, мы уменьшили это дерево, да?
И за вот единицу мы можем
полезть вот этот атомик СИП
и проверить...
В общем, поменять тут этот элемент, да?
За единицу и за одно проверить.
А...
Изменился ли у него
минимум?
Да или нет?
Он же может потом
дальше идти и еще
перемешаться.
Нет, на самом деле, как повезет.
Но первое, что нужно отметить.
Что нам нужно отметить?
Первое, что нужно отметить,
это то, что
минимум от этой штуки, в общем-то,
особо не поменялся.
То есть он мог и в принципе остался
прежним. Если он остался прежним,
то в общем-то делать ничего не надо, правда?
Потому что там сверху
ничего не...
Соответственно,
не поменялось.
Вот.
Пока мы делаем дикрески,
мы всегда делаем это в листе.
У нас все в листе.
Ну да, да, да.
Да.
Да, ну естественно.
Почему минимум не поменялось?
Потому что...
Могло казаться, что
минимум среди детей не поменялся,
потому что эта штука уменьшилась,
то есть какой-то соседний сосед, который еще меньше.
То есть в этом случае
действительно не особо все поменялось.
Что мы тогда дальше делаем?
Если минимум
поменялся,
ну идем еще...
Вот. Ну в идеале, конечно,
да, заметим, что
в идеале, если минимум поменялся,
то нам, конечно, придется так это прям честно-честно-честно
идти-идти-идти, да?
Вот.
Вот. Но это, конечно, не очень
хорошо. Поэтому мы пойдем другим
путем.
Вот.
Мы скажем, что, а давайте-ка мы этого ребенка
просто выпилим.
Что такое выпилим?
Это означает, что вот это под дерево высоты
ноль мы
отправим себе в
вот.
Собственно, в список деревьев.
То есть, видите, мы его не удаляем,
мы его отправляем в список
детей. Ну вот.
Но понятно, что раз оно
высоты ноль, то я теперь могу этот ключ
уже там написать, что у меня
что угодно.
Вот. Понимаете, да?
Вот.
Ну хорошо, говорим мы.
Но тогда мы замечаем,
что...
Ну вот. Но тогда... Хорошо.
Значит, хорошо. Но тогда
вот эта штука тоже
уменьшилась, да?
Так.
Но на самом деле нет.
Вот. Нет.
Но на самом деле тут ничего не сломалось.
Тут просто, как бы, мог
поменяться минимум.
Вот. Но от того, что вы...
Нет, смотрите тут. Какая проблема могла возникнуть?
От того, что вы удалили какой-то элемент,
минимум среди этих детей мог увеличиться.
Мог увеличиться.
Сейчас мог, но
у нас сразу нет проблемы.
Мы хотим, чтобы у нас была
маленькая глубина.
А для этого мы хотим, чтобы у всех
было плюс-минус 2 детей.
Если мы будем вот так вот втупую
удалять...
Ну да. Да.
Нет. Ну да. Эту проблему мы будем решать.
Но так... Хорошо.
Ладно. Вброшу тогда идею, как мы будем решать
этот вопрос. Значит,
по идее мы очень хотим, чтоб, конечно,
вырезать не очень много.
Вот помните, как мы в куче фибоначчи
делали каскадинг-кат?
Мы говорили, что если у вершины
отпилился один ребенок, она
получает монетку.
То есть она как бы напряжена.
Но если мы отпилили второго ребенка,
то мы выпиливаем ее саму.
Здесь будет примерно та же идея.
Только каждая вершина будет ждать,
когда у нее отпилят B пополам детей.
То есть как только отпилят B
пополам детей, значит все, мы теряем терпение
и выпиливаем B.
То есть тогда получается, что
вот...
То есть тогда мы ее как бы выпиливаем.
Может быть даже эту вершину мы даже вообще...
То есть мы даже
заявим, что может быть самой этой вершины
может у нас толком нету.
Вот.
А сами эти вот B пополам детей может есть.
Хотя вот идея какая-то
в эту сторону будет.
Вот.
Но правда, теперь выеси какая-то вопрос.
Так, хорошо. Но пока, если у нее не слишком много...
Пока еще много детей,
мы ее, наверное, совсем выпиливать не хотим.
Ну, потому что если мы прям...
Ну вот.
То есть что тогда можно было бы везде плать?
Вот, действительно, что тогда можно было бы сделать?
Ну, можно было бы...
Ну, можно как минимум хотя бы
посмотреть в кучу...
То есть посмотреть в родителей, посмотреть...
Хорошо, вот у меня тут минимум изменился, да?
Возникает вопрос.
А изменило ли это минимум сверху?
Вот.
Ну, соответственно, если не изменило,
то тоже ничего не надо делать. А если изменило?
То отпиливаемся.
Ну вот.
Нет, ну вот.
Ну да, то в принципе можно, наверное,
то в принципе тогда да, придется
эту вершину отпиливать.
Ну а дальше, если говорим, что у нас тут
детей осталось
б пополам, то значит эту вершину мы отпиливаем
по-любому.
Сейчас, то есть
у нас есть плохой случай,
когда та
вершина,
которую мы уменьшаем, она
и так была минимальна,
то есть при ее удалении минимум
увеличился.
Если это влияет на минимум в предке,
то мы отпиливаем всю вершину.
Ну да, ну вот давайте...
Давайте подумаем.
Ладно, если мы прям действительно вот в тупую...
Ну правда, действительно давайте думать.
Если мы действительно прям отпиливаем
минимум,
то насколько это хорошо?
Насколько это хорошо?
Ну может
изменится минимум прямо вообще
у всего?
Ну да, проблема, проблема.
Да, действительно,
у нас действительно
может отпилиться минимум у всего дерева.
Да,
есть какая-то проблема. Да, хорошо.
Получается, да,
у дерева отрезков свой недостаток
получается.
Хорошо.
А если вернуться к обычной куче?
Кстати, обычная куча решала бы этот вопрос.
Единственная разница,
только что мы стартовали бы, наверное,
иногда не с листа,
а с элементов в произвольном дереве.
Да?
В произвольной глубине, да?
Вот.
То есть ведь заметим, что у нас тогда
куча устроена так, что локально всё хорошо,
да? То есть как бы
поменяется только то, что вот в куче
из детей надо просто одного ребёнка убрать.
Но ничего
не поменяется, потому что тут всё равно был
элемент меньше и значит дальше там ещё меньше,
поэтому больше ничего делать не надо.
Но тогда, правда,
возникает всё равно естественный вопрос.
А что же всё-таки нам
делать, если у нас
образовалось тут B деревьев
одинаковой высоты, которые нам вот
очень-очень-очень хочется слить в одно?
Вот как бы вы сказали.
Как бы нам тогда выкрутиться?
Хочется взять,
не знаю,
через фейтную вершину
их всех подкисить за неё.
А потом как-нибудь её вниз
пропихнуть?
Ну, пропихнуть вниз,
к сожалению, не получится
за O от единицы или даже за O от B.
А, ну тогда
можно взять корень
минимального,
минимальный корень
и к нему всё.
Нет, тут теоретически можно,
но практически...
Да,
вот возникает вопрос.
Как же совместить это, чтобы
куча была и дерево отрезков было?
А нельзя, как мы
в B дереве делали?
А как мы в B дереве делали?
Получился
массив длины 2B.
Давайте возьмём
минимум, поднимём его
вверх.
Ну,
раз тянем его
пополам,
поднимем две вершины,
ещё одну.
Такую вот штуку
можно подкосить.
Ну, не знаю, нет,
такую штуку да, но там что-то пропихивать надо.
Понимаете, там же в B дереве
пропихлоны тоже не за единицу работали.
Мы же в корне.
Нет, ну мы работаем в корне,
что нам это даёт?
У нас есть B деревья,
надо объединить их как-то в одно.
То есть, да,
у нас есть O от B времени.
На это.
Ну вот, мы
берём
минимальную вершину,
ну, минимальный корень.
И всех
остальных приделываем к нему.
Так.
Вот у нас проблема, что теперь
он может быть размера больше,
чем 2B.
Ну, тогда
просто делим там его
на две части,
выделяем три вершины
и строим вот эту штуку.
Да, но при этом нам
надо ещё следить, но при этом нам
ещё важно следить за тем, чтобы у нас
количество детей было не сильно
мало.
Ну вот.
Ну-ка.
На самом деле первая идея, которая тут возникает,
на самом деле такая. А давайте введём фиктивный
элемент.
Ну, потому что можно там всегда
устроить что-нибудь, что если фиктивных элементов
стало слишком много, то давайте устроим
перестройку.
То есть, если там, скажем, фиктивных
элементов стало больше, чем неэффективных,
примерно равно,
то давайте просто...
То есть, тогда можно
всё
стереть, просто выписать все элементы
и честно из них построить, и это
учётная стоимость
такой операции будет ноль.
Ну, это понятная идея, да?
Это понятная идея?
Так, если кто-то ещё там...
Да, кто тут ещё жив-то вообще?
Как говорят в видео,
поднимите руки, кто ещё жив!
Во, да, честно.
Спасибо.
Ну, так просто уже нет.
Я понимаю, что все уже мёртвые,
но что делать?
Просто финализировать надо.
Так что хорошо.
Но тогда получается, что за О от Б
действительно можно вешать так, чтобы было у нас
действительно тут целых Б детей.
Но просто теперь мы
поддерживаем вариант, что
у каждой вершины детей
ну хотя бы Б пополам.
Да?
То есть, идея будет такая,
что мы честно всё отпиливаем-отпиливаем,
хипы меняем, в корень даже лезть не надо,
потому что и так всё в порядке.
Ну, понятно,
если нас попросили дикриски
от корня какой-то кучи, мы просто делаем дикриски
кучи. А, но не забываем,
что корни вот этих вот всех
куч на самом деле у нас всегда находятся
в какой-то
великой глобальной АФ куче.
Помним, да?
Но помним, что у нас ограничения нам гарантируют,
что у нас как бы там в этой куче всё не более,
чем лог квадрат элементов, поэтому это
нас устраивает.
Так что такая вот примерно
у нас будет такая штука.
Хотя... Нет, хотя нет,
погодите.
Вот.
А что делать-то
с фейковой вершиной?
Ну, а как мы реализуем
фейковую вершину?
Ну, так как мы создаём новый ключ
размера минус, то есть который равен
минус чё-нибудь. Минус бесконечность,
минус две бесконечности, минус три бесконечности
и так далее. Ну, или просто минус бесконечности,
не паримся. Ну, и у нас тогда
минимум вот по всей
куче минус бесконечности.
Максимум может быть.
Да, проблема.
А, ну да, тогда мы минимум не можем делать.
Стоп, ну мы кучу, на что хотим?
На минимум. На минимум, конечно.
Тогда делаем бесконечность.
Так, делаем бесконечность. Нет, вопрос... Правильный был
вопрос. А как нам потом
скажем GetMin сделать
или Extra там? GetMin
как нам сделать?
То есть GetMin нам скажет, вот он нам скажет
минус бесконечность, потому что вот в этом дереве
есть в корне минус бесконечность.
Нет, стоп.
А, блин.
Ну, можно пределить
эти фейковые
корни.
А GetMin мы на сколько хотим делать?
За единицу, естественно.
Естественно мы хотим всё за единицу.
Так.
Вот, вот давайте думать.
Как же тут можно сделать?
Так.
Так.
Так.
Так.
Так.
Так.
Так.
Так.
Так.
Так.
Так.
Так.
Так.
Так.
Так.
Так.
Так.
Ага.
Ну, можно, например, primer
не пласть.
Как у нас устроен IFHIP в вершину?
Это минимум всех детей, плюс она сама.
Но можно просто не класть в IFHIP фейковые вершины
и минимум термия брать как минимум в форме IFHIP?
Нет, ну тогда просто они...
тогда сразу возникает повреждение.
А что делать, если они глубоко проникнут?
То есть эти видосы бесконечности потом глубоко проникнут?
Нет, ну с глубинами у нас проблем нет.
Нет, в смысле нет.
Нет, проблема не с глубиной, а в том, что потом не только в корнях будут минусы бесконечности.
Ну, в плане...
ну вот...
во всем дереве просто не пойдем в IFHIP
фейковую вершину.
Она от этого только уменьшится может.
Не знаю.
Сейчас.
Ну да. Нет, тут вот...
Нет, тут вообще, знаете, как это так...
Как это ни странно, тут вообще предлагают мистическую операцию.
Предлагается, знаете, как?
Строим вот это дерево с минус бесконечности, да?
А в том из этой минус бесконечности неожиданно этот минимум...
то есть минимум достаем.
То есть идея такая.
Так, минимум где-то здесь.
Так, допустим, здесь.
Отлично.
Значит, этот минимум перетаскиваем сюда,
но тогда в этой вершине нет корня, да?
То есть нет значения.
Ну отлично.
Давайте его тоже достаем из детей и так далее.
Ну это за глубину работает?
Да, вот проблема, да.
Так что вот действительно...
Ну в худшем случае это работает за вот...
за глубину.
То есть придется дойти до листа,
может быть там какой-то...
Хотя...
Ну да.
То есть это...
Нет, по сути это означает, что мы тут просто
по какой-то веточке, получается, прогуляемся.
Вот.
И где-то тут еще какого-то ребенка выпилим.
Да?
Вот.
Ну и теперь...
Ну вот.
Ну вот.
Ну возникает такой тонкий момент.
Так.
Ну какой-то у нас тонкий момент возникает.
Тонкий момент, наверное, возникает то, что...
Так.
Хотя да, высота у нас какая-то нетриалиальная.
Да, мы же хотим это за единицу делать.
Ну вот.
Хотя с другой стороны мы и так то, что мы сейчас
говорим, мы делаем вот за столько, да?
Ну мы говорим...
Мы вот объединяли О от В, да?
А высота у нас по любасику не сильно большая.
Ладно.
Есть правда мелкая...
Ну правда тут, конечно, есть такая мелкая закрученная
оговорочка.
Мелкая закрученная оговорочка говорит о том, что там чисто
теоретически там могло так оказаться, что вы...
Ну хорошо.
Так что вы тут пилили, пилили, пилили.
Ну ладно.
Давайте вот для простоты тут лучше плюс бесконечность
повесим, да?
Ну потому что если мы тут отпилили ребеночка, и тут
оказалось В пополам, то придется опять это рекурсивно
вызывать, как бы не зациклиться, да?
Ну я предлагаю так.
Давайте, чтобы сейчас не будем зависать, просто скажем,
что ладно, в случае чего можно сделать глобальную перестройку
и не париться.
Вот.
И ну вот там каждый раз.
Поэтому пишем тут плюс бесконечности, это нормально.
Понятно, да?
То есть получается высота-то у нас меньше, чем вот это
В.
Видим, да?
Вот.
Вот.
Поэтому ничего страшного.
Так.
Так.
То есть получается хорошо.
С объединением, то есть объединением В деревья выкрутились.
Филипп Дмитриевич, можете, пожалуйста, повторить,
потому что вы сначала сказали одно, потом отменили,
потом опять.
Так.
Хорошо.
По итогу, мы сейчас сделаем дерево отрезков или мы просто...
Вот.
Теперь давайте все это соберем с самого начала, все, что
мы попытались придумать.
Да.
Это был семинар в процессе, да, у нас есть идеи, давайте
придумаем алгоритм, да?
Значит, алгоритм получился такой.
Нет, храним все-таки кучу.
Вот.
Значит, храним кучу.
Значит, при этом храним.
Значит, в каждой вершине есть какая-то высота H.
И при этом гарантируется, что у всех детей как минимум,
то есть не более чем В детей, но и не менее чем В пополам.
Даже строго более чем В пополам.
Вот.
И еще гарантируется, что у нас каждого дерева, дерева
каждой, у нас есть несколько деревьев, и деревья в каждой
высоты у нас строго меньше чем В, мы их как-то аккуратно
храним.
Ну там, допустим, у нас там, ну сколько, ну у нас там
сколько существует рангов, допустим, да, столько существует
там, скажем, двухсвязных списков.
Если где-то оказалось, что деревья в В стало, то мы
тут же обе превращаем эти деревья в, соответственно,
одно дерево.
Вот.
Так что вот такой чит.
Я утверждаю, что все эти операции делаются тогда
за О от единицы, кроме дилита.
Вот.
Ну как мы делаем дилит?
Ну дилит предлагается просто делать предельно честно,
то есть как бы там заменяем на плюс бесконечности,
делаем честный севдаун.
Да, у нас тут будет фиктивный элемент, но я предлагаю
его даже не удалить.
Ну можно его удалить, здесь вот эскорли, вот снизу
выпилить, а можно не удалять.
Как я сказал, у нас, то есть там в оригинальной структуре
такого нет, но как бы лайфхак вида, что храним фиктивные
элементы, их стало слишком много, давайте, то есть давайте
сделаем перестройку, он продолжит работать.
Это такие, это наши локальные чипы будут.
Вот.
Ну вот, соответственно, значит, если с дилитом
разобрались, insert это просто вставка нового дерева
высоты 0, и возможно вот это вот объединение, если там
на нулевом уровне образовалась бета, мы их объединяем
и получается новое дерево высоты 1.
Если на высоте 1 тоже оказалась бета, понятно, делаем, делаем,
делаем, в общем, пока не схлопнется.
А как мы еще раз объединяем?
Ну, нет, мы объединяем не два дерева, мы объединяем
б деревьев одинаковой высоты.
Да.
Как мы это делаем?
Мы это делаем вот как.
Мы создаем вершину высоты на единичку больше.
Вот.
После этого объявляем плюс бесконечность и честно
делаем, и честно ее опускаем.
Опускаем ее мы, пользуясь тем, что высота дерева меньше,
чем б.
Чтобы b у нас логорифом, а высота дерева log n поделить
на log log n.
Потому что, ну как бы, если у нас светвление, ну у нас
по крайней мере в идеале было, да, что у каждой вершины
значит светвление, то есть каждой вершине соответствует
вот большое количество листов.
По крайней мере теоретически виртуально.
Понимаете, да?
Вот.
И суммарное количество листов у нас типа n.
Вот.
Так что вот такой неожиданный чит получается.
Так, а вот поподробнее про перестройку.
Как мы это делаем?
Ну идея очень простая.
Если мы выяснили, что у нас количество эффективных
элементов становится больше либо равным, чем количество
не эффективных, тогда мы что делаем?
Мы просто все убираем.
То есть мы просто выписываем все элементы в ряд.
Просто выстраиваем все элементы в ряд.
И заново за линию строим эти деревья.
Ну как мы их строим?
Ну их в принципе...
Ну то есть просто честно.
То есть на самом деле мы делаем из них, соответственно,
сколько там было элементов нулевой высоты, а дальше
просто предельно честно, то есть их схлопываем,
вот по б штук одинаковых, так же как делали раньше.
То есть мы вот избавились от половин эффективных,
и мы опять начинаем их склеивать при помощи эффективных.
Ну вот.
Ну да.
Вот.
Ну вот.
Ну как бы да.
То есть остается только аккуратно убедиться в том,
что это все работает действительно за адекватную этим точку.
Перестройка будет нормально работать?
Ну потому что везде будут появляться разные монетки.
Перестройка оплачивается так.
Давайте каждый раз, когда вы удаляете элемент,
когда вы удаляете на каждый удаленно элемент кладем ну допустим,
пять монеток.
Пять красных монеток.
Да, у нас будут монетки разных цветов.
Вот в данной случае пять красных монеток.
Тогда значит, идея такая.
Предположим, что выяснилось, что у вас как бы
эффективных элементов не меньше чем неэффективных, да?
А неэффективных у вас допустим, L.
Тогда у вас получается что у вас есть пять L-монеток
на то, чтобы сделать перестройку.
То есть перестройку оплачиваем красными монетами, да?
Да, перестройку мы оплачиваем этими красными монетами.
Ну еще у нас же еще эффективные добавляются, когда мы их сливаем, да?
Ну да, есть такая, да.
Тогда мы тоже должны как у вас красными монетами.
Так, ой-ой-ой.
Но, смотрите, нет, ну тут идея такая.
На самом деле, когда вы уж прям в явном виде строите,
можно на самом деле эффективных элементов не делать.
Ну а вместо этого, так, ну вот.
Ну а вместо этого действительно, то есть действительно
уже делать на самом деле как-нибудь по-аккуратнее.
Сейчас, нет, подождите, я не понял, в чем проблема.
Вот вы говорили, что мы когда удаляем, складываем 5 красных монет.
Да.
А, ну хотя да, в общем неважно.
Потому что на самом деле, да, заметим, что действительно
удаляемых монеток, в общем-то, будет в b раз меньше, действительно.
То есть ладно, если мы будем сейчас честно делать, как мы сейчас делали, да?
То есть фиктивные элементы будут создаваться,
но у нас просто на этот момент этих фиктивных элементов
будет уже не l, а получается l поделить на b.
Сейчас, на этот момент, это на какой?
Ну, напрямую на этот момент.
На какой?
Ну, напрямую сейчас.
Сейчас, а в чем еще раз проблема?
Скажите, пожалуйста.
Ну, проблема в том, что как бы, вы правильно отметили,
что у нас проблема в том, что у нас фиктивные элементы прямо сейчас тоже есть.
Вот.
И нам как бы надо проследить, что нам как бы
старых 5l монеток хватит и на то, чтобы это все построить,
и на то, чтобы эти монетки сохранились.
Да, то есть нам нужно еще приберечь на будущее какие-то.
Ну да, получается так.
Или как-то более аккуратно построить, чтобы от них избавиться.
А мы не можем?
Чего?
Ничего-то не понятно.
Вот.
Нет, на самом деле чит очень простой.
Нет, ну с другой стороны, реально, у нас же новых фиктивных станет не l,
а станет l делить на b.
Поэтому можно как-то выделить столько монет, чтобы этим тоже хватило.
Сейчас, не l, а l делить на b.
Ну вот, когда начали перестройку, у нас было сколько-то плохих фиктивных.
Было, да.
Потом их сократилось в b раз после перестройки.
Так.
Ну вот, кажется, что если их сократилось в b раз, то все в принципе хорошо.
Но это правда соговорка, если их сократилось именно в b раз.
Ну а?
Потому что, ну не совсем в b,
потому что если у нас сейчас l элементов,
то как бы для того, чтобы получить деревья первого уровня,
у нас получится l делить на b фиксированных элементов.
Но, но эти, да, но эти корни теперь тоже надо объединять,
потому что l делить на b может оказаться больше, чем b.
А, ну правда не сильно страшно,
потому что получится монеток l делить на b,
плюс l делить на b в квадрате,
плюс и так далее.
Ну, короче, это...
Да, есть подозрение, что это меньше либо равно 2 l поделить на b.
Ладно, если b...
Ну, n хотя бы 2, давайте сделаем.
Ну хорошо, да, если...
Ну ладно.
Нет, ну даже, ну может хотя бы 4 тогда, да?
Вот, ну хорошо, да, если, да.
При достаточно большом...
Ну вот.
Нет, ну очень не так страшно,
учитывая, что у нас 5 l монеток, как вы помните, да?
А, ну хотя нет, там же по 5 монеток найти удаленные,
но это, ну очень понятно, что это меньше l в несколько раз,
и получается, что запас монеток есть.
Там все остальное лечится там, что не 5 или 10 l.
Вот это у нас будут на экзамене спрашивать?
О, как же!
То есть потом вот это надо будет рассказывать?
Ну я вам так скажу.
Ну да.
У нас всегда есть статья.
Это на УД, да?
Нет, ну нет.
Нет, да, я думаю тут.
Ну вот.
Ну хорошо.
В общем, таким образом, хорошо.
С перестройкой разобрались вроде живы, да?
Вроде да.
Так, теперь.
Что мы теперь, ладно.
С фиктивными элементами разобрались?
Значит, что у нас теперь?
Ну вот.
Замечаем, да, что с фиктивными...
Ну правда, и в чем важно, что все фиктивные элементы,
они равны плюс бесконечности,
и поэтому ни на что не влияют.
Помните, да?
Что теперь можно сделать?
Так, что нам теперь еще надо проговорить?
Ну, кстати, у нас же нет плюс бесконечности.
Нет, наоборот, почему?
У нас же фишка в том, что когда вы объединяли b деревья,
вы сказали, что это фиктивный элемент плюс бесконечность.
Потому что в результате чего после того,
как вы сделали объединение,
вы эту плюс бесконечность посевдаунили вниз?
Да.
Вот.
Просто севдаун вы делаете за высоту дерева,
а высота дерева все равно меньше, чем b,
поэтому вас это устраивает.
Что такое число в куче?
Обычно в этом месте подразумевается,
что просто у вас не так жестко подогнаны ограничения.
Там часто бывает даже более сильно,
потому что вы даже не два в степени w,
а, может быть, два в степени w минус 1 и минус 2,
и вообще там едва ли не w пополам даже.
Да.
Нет, бесконечность в константе вы не сделаете.
Нет, реально, в общем, это решаемо.
Можете хранить рядом с каждым элементом флаг
на тему того небесконечности я.
То есть я нормальный или ненормальный?
Вот так.
Теперь возникает вопрос,
когда вы делаете вот это вот каскадинг выпиливания?
Здесь, правда, технология тоже стандартная.
Там, конечно, важно, чтобы
во-первых, да, во-первых, мы замечаем,
что теперь давайте думать,
как нам теперь вот тут следить за тем,
чтобы у нас объединение вот эти суммарно работали адекватно.
Ну ладно, сейчас мы поверили,
что при просто вот перестройке
на самом деле мы деревья эти с нуля
действительно суммарно за линию построим.
Но я думаю, вы уже убедились,
что да, мы их построим за l
плюс l делить на b, плюс l делить на b квадрат,
а это что, это вот это?
Ну реально во времени, понятно, да?
И на это у нас монетки есть.
Но у нас же все время возникают
вот эти вот ситуации,
давайте в каждый момент времени
добавлять новое дерево или выпиливать откуда-то
и неожиданно обнаруживать,
что если у нас есть b деревья,
давайте их объедение.
Как тогда это сделать?
Ну сделаем тогда.
Ну тогда можно сделать действительно
ну как же нам следить?
Ну в идеале конечно хочется следить,
что допустим каждый раз,
когда у нас есть какое-то дерево,
на этом вот отдельное дерево
в глобальном списке,
мы храним там какие-нибудь зеленые монетки.
А какую проблему мы говорим?
Ну следующая проблема,
мы говорим, что мы хотим оплачивать,
ну что у нас периодически в списке
появляются новые деревья, да?
И причем они не просто появляются,
они не просто появляются,
а мы говорим, что если у таких деревьев
такой высоты в списке стала b,
то значит их надо объединить.
Мы хотим найти монетки,
за счет которых мы это хотим делать.
На каждое дерево кладем по монетке?
Да, ну теперь по новой монетке, по зеленой.
Это же какая-то проблема у уровня,
что у нас там была биномиальная куча?
Нет, ну там мы как-то выдали...
Нет, ну раньше...
В биномиальной куче мы эту проблему не решали.
Ну да, ну хорошо, в куче фибоначи.
В куче фибоначи мы решали проблему,
но там это решало...
Но там это как-то... там читерские это решали...
Ну вот.
Но там просто...
Там это никак не решалось,
потому что в куче фибоначи, к сожалению,
нам было плевать, сколько деревьев у нас
в корневом списке и какие они.
То есть там просто говорилось,
что мы от вершин не имеем права
много отпиливать.
Это нам нужно было для того, чтобы у нас...
Если у нас дерево высоты H,
реально высоты H,
то в нем не слишком мало вершин.
Вот.
Мы тут вообще хоть когда-то удаляем вершины.
Ну, получается...
Мы добавляем,
а если много фиктивных,
то делаем перестройку.
У нас как будто вообще как...
Короче, у каждой вершины степень В получается.
Ну да.
А, слушайте,
у нас тогда вообще чит.
Ведь мы же тут тоже выпиливать никого не обязаны, кстати.
А тут это где?
Ну, потому что вспомним.
Как мы делаем дикриски, давайте вспомним.
А, нет, хотя нет, обязаны.
Как мы делаем дикриски?
Мы берем элемент,
и просто выпиливаем это дерево,
отправляем его в корень.
То есть поэтому, к сожалению,
выпил необходим.
Вот.
Но, правда, сам по себе он еще страшен,
потому что один выпил, это не страшно.
Но тогда идея такая.
Ну вот.
Но тогда просто идея такая.
Как мы уже убедились, там достаточно,
что когда у нас появляется новое дерево,
на нем должно храниться какая-нибудь зеленая монетка
на его будущее объединение, правда?
А лучше две.
Тогда там легко убедиться,
что там даже каскадинг объявлений будут,
эти вот объединения будут нормальны.
Но просто фишка теперь такая.
Когда вы делаете дикриски,
значит, идея такая.
Вы кладете,
то есть вы выдаете
желтые ноты.
То есть вы на это выдаете, скажем,
три желтые монетки.
И тогда идея такая.
Одна из этих желтых монеток
отправляется вместе с этим деревом
с зеленой.
Вот.
Оставшиеся две желтые монетки,
мы тогда что делаем?
Мы говорим так, мы его выпилили, смотрим на родителя.
Если на родителя
желтые,
если у нас желтый нот,
то есть если у нас тут еще нот,
то есть от того, что мы выпилили здесь детей,
мы, значит, сюда кладем две желтые монетки.
Да?
Можете повторить.
Ну, еще раз.
Какое предположение? Я ничего не предположил.
Ну, я говорю, значит,
когда мы говорим дикриски
от вершины, мы даем на эту операцию
три желтые монетки.
Одна желтая монетка сразу превращается в зеленую
и кладется на вот это
дерево в корень.
Ну, мы поддерживаем вариант,
что у нас на каждом дереве
хранится монетка.
Две желтые монетки отправляются
в ту вершину, от которой мы только что
отпилились.
Тогда, в тот момент, когда тут станет
B пополам детей,
у нас уже в этой вершине накопится
B желтых монеток.
И получается, что
ну, можно сказать, тогда
действительно будет B желтых монеток,
и тогда можно действительно эту вершину
куда-нибудь перетащить.
Потому что мы...
Дело в том, что B пополам, это для нас
недопустимо, поэтому мы как бы эту вершину
удалим и добавим в корень уже
не одно дерево, а B пополам дереве.
Но в соответствующее
количество монеток у нас на это есть.
А вот такой вопрос, а вот когда мы делаем
дикриски, мы можем не выпиливать
ее, а заменять на фиктивную?
Заменять на фиктивную...
Понимаете, если бы она была листом,
пожалуйста.
А вот если она оказалась в середине,
а такое может быть, то увы.
Да, вот в чем проблема, да.
Дело в том, что у нас
как бы в дикриски запрещен синдал.
А вот когда мы делали с деревом отрезков,
какая у нас была проблема?
Почему мы отказались?
Ну...
Какая у нас проблема была?
Ну просто тогда непонятно было,
как дикриски без всяких аккуратных
сифтапов, как дикриски
без сифтапов делать.
Потому что сейчас у нас как бы локальная
структура, которая позволяет нам, если тут
что-то отпилилось, ну и ладно.
Не, ну можно сделать дерево отрезков,
и тогда просто если делать дикриски, то
заменяем просто на плюс бесконечность.
Да, но тогда придется прибежаться по всем родителям
и тут все минимумы обновить.
Вот, мы вот ровно из-за этого
мы вообще страдаем тут и делаем какие-то
адские амортизации.
Но общая технология такая.
Вот.
Так что вот такие вот веселые дела.
Значит ладно, давайте времени у нас уже много.
Хотя вот общую суть сказали,
значит теперь остается только так.
Ну или к AF-хипу.
Потому что теперь самое смешное,
как свести теперь AF-хип,
вот сам по себе это АС,
сводится к Q-хипу.
Он тоже все делает за О от единицы,
но у него еще более жесткие ограничения.
СЗ меньше либо равно
логарифом в степени
1 делить на 5.
Ладно, 4.
Ну сколько-нибудь.
Как это сделать?
Очень просто.
Технология абсолютно та же,
только B здесь будет равно
сколько?
Да, вот тут надо вот еще.
Ну давайте скажем, что B равно
лог 1 делить на 5.
А не на 10?
Не на 10.
Зачем на 10?
Тут мы взяли корень
из допустимого асфера.
Так, сейчас.
Ну погодите, значит тут B
лог 1 делить на вот.
Нет, ну там вот идея такая, ну давайте просто так вот возьмем.
Нет, просто тогда будет маленькая
приятная вещь.
Высота каждого дерева у нас будет какая?
Ну в идеале появляется
константная.
Филипп Дмитриевич,
такой вопрос.
А вот если мы делаем дикрески,
мы выпиливаем, да?
Что с детьми происходит?
Нет, погодите, мы же дерево
не выпиливаем. Мы дерево просто берем
эту вершину со всеми ее потомками,
прям как дерево, и вешаем сюда.
То есть с потомками ничего не происходит.
Так, у нас, видите, просто приятная локальная структура,
в которой как бы, да,
их низ с низами никак не связывает.
Вот.
Но теперь думаем,
вот если у нас B, лог 1 делить на 5n,
тогда заметим, что высота
каждого дерева, вообще говоря, константа.
Там что-то типа 11.
Вот.
Ну в принципе это означает,
что эту штуку вообще можно
полегче реализовывать, то есть можно там действительно никого
не удалять, все севтапы, севдауны
предельно честно делать и не париться.
Вот, по идее, да?
Ну вот, ну а можно сделать
вот просто примерно так же, просто обнаруживать,
что дилит у нас будет работать
за от тоже логарифом там
вот этого по основанию этого и получится
единица.
Так, тяжело что-то будет, может быть,
помедленнее.
То есть мы получается сделали то же самое?
Да, но только тут размер всегда не более,
чем лог квадрат n, а B мы сказали
теперь лог 1 делить на 5 от того же самого n.
Вот.
Тогда у нас что получилось?
Тогда утверждается, что во-первых,
высота каждого дерева константа.
Потому что высота дерева
это что-то типа логарифом
этого по основанию вот это.
Логично, да?
Ну, давайте написать это.
Ну, если это константа,
то...
Ну, давайте.
Вот.
Это равно
логарифом в квадрате n
поделить на
логарифом
в степени
одна пятая n.
Это равно 2 лог
лог n делить
на полтора.
Вот.
Вот, ура.
Ну, нет.
В принципе, это вообще вам дает возможность
вообще особо не заморачиваться,
а просто честно делать
предельно честные деревья
и в них честно делать севтап и севдаун
и не заморачиваться.
Вот.
А, ну и все.
Почему?
Как мы связали угол квадрата с этим?
А, сейчас.
Ну, n напоминаю у нас как всегда.
Можете считать, что там n не происходит
два в степени бипность.
То есть, тут как бы n тоже
это два в степени бипность.
То есть, видите, это более сильные условия.
Да, но мы к нему
смогли свести так, как мы смогли
разделить типа
кучу-кучу, короче, сделать и из-за этого у нас...
Ну, типа того, да.
Вот.
Ну, там вот одна пятая там.
Вот.
Ну, там действительно можно одна пятая,
но там...
Ну, то еще заметим, что
если у нас есть несколько деревьев,
то эти деревья тоже нам придется хранить
в какой-то куче, да?
А зачем нам несколько деревьев, если мы можем, по идее,
делать честную кучу?
Ну, если у нас высота конца.
А, ну да.
Да, согласен.
Да, все, вообще не заморачиваемся.
Да, честно скажем,
статья, конечно, написана немножко не так,
но как всегда,
это как всегда, это называется, да.
Мы поняли основные идеи, дальше добыслим.
Возможно, на экзамене вы придумаете что-то третье.
И это будет даже круто.
Да, в стрессовые ситуации.
Ну, почему в стрессовые?
Знаете, это даже на самом деле правильнее,
потому что, как бы, знаете,
не очень интересно, если вы просто выучите,
вообще отторабаните и все. Интереснее, если вы
как бы эти идеи освоите и на основе этого что-то придумаете,
потому что это, собственно,
ну, как я уже понял,
это гораздо лучше,
потому что, понятно, просто в самом по себе
вызубливание этого алгоритма смысла
действительно никакого.
Ну вот, то есть, как бы, я это хотел
рассказать именно потому, что вот действительно,
то есть рассказать, потому что есть вот такие вот
такого рода технологии,
которые позволяют вам тут сделать вот единицы
и при этом уменьшать ограничения.
Потому что, к сожалению,
я не могу вам рассказать полностью о Atomic Hip,
потому что мне придется рассказать, что будет здесь.
А ку-хип она вот
по объему насколько соотносится
с вот этим? Слушайте, расстрел.
Я думаю, как минимум одно...
Я подозреваю, что мне надо было бы недельки 2-3
на это потратить.
Потому что там надо и Fusion-хипы там всякие рассказывать,
там всякие x-y деревья, там это
подходить-подходить, и там еще получился
какие-то свойства изучать, и там бы получился
еще какой-то эпичный, обзубодробительный
автомат. Честно скажу.
Если вы, скажем так,
если хочется на это послушать,
то лекции прошлого года остались.
Нет, не хочется.
Нет, я не прошу вызубывать
к экзамену, хотя там...
В итоге дойти ку-хипа
так удалось, какие-то Fusion-штуки
удалось сделать,
но при этом
вот это вот мы там обсудили
какие-то общие положения, и в экзамен это уже не выносилось.
Потому что это уже действительно расстрел.
То есть может быть
в будущем когда-нибудь,
когда первое и восьмое мая не будут выпадать
на понедельник, мы сможем
когда-нибудь это сделать.
Но вам повезло.
Вот так.
Хотя не знаю, там лишние занятия иногда можно потратить
на то, чтобы доказать обратную функцию
Германова с Эдеми, конечно.
Интересно, кто-то общает эти решения,
эти статьи их отчитывает, потому что
кажется, что тут очень все такое
хлипкое, что если в каком-то месте
конец там не сойдется,
но смотрите, нет,
я вам так скажу,
как это работает. Во-первых, вычитывает сам автор,
потому что одно дело на пальтор, а другое дело,
когда вы это пишете, вы это сами думаете.
Во-вторых,
дальше безопасность такая. Во-вторых,
сам автор, это активно,
как бы понятно, что если вы первые, кто это придумал,
то естественно там вас тестируют по максимуму.
Поэтому вы там
рассказываете до конференция и прям вот очень
активно обсуждаете.
И ваши коллеги просто вас там тоже в этом месте
все эти вопросы задают, собственно,
активно и валят. В-третьих,
как устроена статья?
То есть статья устроена еще так, что вы написали этот
текст, прям подробно написали.
То есть не так, как я. Я еще иногда
рассказываю предположение, что вот есть такая
идея, давайте вместе подумаем и выкопаем.
Там расположено не так.
Вот можете просто сравнить, как я вот
дерево доминаторов рассказал вот вчера,
вот просто на записи, и что написано в оригинальной
статье Тарьяна.
В оригинальной статье Тарьяна там есть, конечно,
намеки на то, что все поселось из этой идеи
какой-либо примерно, но там будут
просто формально там 5 CRM
и несколько LEM, каждый из которых
скрупулезно доказано.
Вот, исходя из этого там
показан алгоритм.
Вот.
То есть здесь будет тоже вам там подробно напишут.
Вот, например, тут есть статья, кстати, выяснилось,
что она, оказывается, легко скачивается и даже
без заморочек. Там будет
сказано, что тут у этой кучи там есть
операция insert от X, delete
от X, decrys от XW,
ripple от X, там
popcue будет, например,
consolidate, prune.
Каждая операция будет прописана,
что она делает, и, собственно, будет дан
анализ. Правда, честно скажу,
анализ, к сожалению, тоже немножко на пальцах.
Вот я и говорю,
что у нас тут многие какие-то вещи, они там...
Да, но! Но как статья устроена?
Ну, например, статьи, математические статьи
просто так в журнал не попадают.
Потому что статья шлется
рецензенту. Рецензент
сидит, и у него там первая задача,
во-первых, действительно убедиться, что багов нет.
То есть если бага,
если баги есть или есть какие-то вопросы
непонятки, он просто присылает вопросы.
Или, может быть, напрямую общается.
Ну, дальше все зависит
от... Ну, дальше, конечно, там всякие
приколы есть, потому что у меня в научной работе
встретился прикол, что вот там есть
одна теорема, которая доказана тремя авторами,
точнее, тремя группами авторов в трех статьях
87, 89
и 91-х годов.
Более того, ну, ладно,
допустим, ну,
допустим, можно иногда объяснить, что там
статьи вышли в разных журналах, они
друг от друга не знали, потому что одна статья, на самом деле,
между ее написанием и выходом
иногда там несколько месяцев, а то и пару лет
проходит. Тем более, что одна вышла
в американском журнале, другая
в польском. Но
статьи 87 и 89
91-го года вышли в одном
и том же журнале.
Почему так произошло?
В статье 91-го года, кстати,
от нашего соотечественника по фамилии Гутки,
значит, там написано следующее.
Значит, да, вот есть такая красивая теорема.
Мы читали статью 87-го года
и ничего не поняли.
Вот, вообще ничего не поняли.
Но потом в какой-то момент мы узнали, что
редактор, то есть те, кто редактировал,
те, кто это вычитывал, тоже
ничего не поняли, но просто поверили умным
авторам.
А вторые там просто Франко Вивальди,
это там действительно серьезная величина.
Вот, поэтому как бы там, возможно,
немножко им не продавили, но просто
поняли, что ничего не поняли. Поэтому
редактура согласилась тем, что будет полезно,
если мы просто там
дадим ту же теорему,
но, на наш взгляд,
более простым доказательствам.
И дается просто другое, ну, по крайней мере, это понятно.
С 87-й год не знаю, но
доказательства 91-го года действительно понятны.
Вот, разобраться можно.
То есть вот есть еще такие ситуации.
Ну, это такая по мелочи. Но в целом,
на самом деле, все-таки подобные вещи там, на самом деле,
профессионалами активно вычитываются,
обсуждаются на конференциях там, так что вот тут
все достаточно... То есть в этом
смысле достаточно жесткенько.
Вот.
Ну и потом, не говоря уже о том, что
как бы там, как бы есть еще вариант,
что можно уесть. Потому что, как бы,
если вы вывести статью, то, конечно, пока
она будет считаться, что вы победили,
пока никто не нашел лажу.
Причинка в том, что кто-то лажу может и найти.
Если он найдет прям содержательную лажу,
ну вот, а потом, собственно, сам
подсунет там, собственно, решение, то
получится, что он отберет у вас, собственно,
как бы звание
решателя проблемы.
Вот.
Так что тут еще такие.
Но, впрочем, на тему сомнительности я говорю.
То есть, на самом деле, может быть, тут, конечно, быстровато немножко
прошли, но на самом деле эта идея не самая
страшная. То есть там автомат в кухе
пейс сильно сложнее, я вас уверяю.
Но мы его и не обсуждаем.
Так что вот,
ладненько. Так, думаю,
наверное, на этом все.

так давайте вернемся к той задачке которая была вот эта вот задачка передать
сообщение квантовая в котором есть суперпозиции нуля единиц зачем это нужно был вопрос прошлого
раз чтобы например связать два квантовых компьютера в сеть значит как общая процедура
строилась у нас у нас состояние с помощью кодирования переводилось в частичное состояние
кубитом каждый из этих кубит пересылался через канал на выходе получалось вот такое вот за
шумленное состояние видите n это число раз использование канал вот с этим агрегатом который
на выходе здесь есть нужно было провести операцию декодирование после декодирования вы получите
какой то бек который мало должен отличаться от си степени отличия можно например вот такой
вот нормы смотреть один норму а мы в прошлый раз с вами водили фиделить для этого различия
так сейчас я найду примеры тоже рассматривали вот если общая схема то пси пси матрица плотности
на входе после кодирования переводится в некоторое другое состояние пси большое здесь
n подчеркнуто это число кубитов который используется дальше каждый из этих кубитов пересылается
через канал потом после процедуры декодирования получаете ром вот заметьте что размерности всех
этих пространства они как бы отличаются то есть вот пси это из пространства размера стека вот
на выходе тоже вы получаете ро из пространства размерности к а вот промежуточная часть вот в
этом кодировании она другой размером все обладает это аж в тензорной степени н где
на число раз используем канал вот эта вот схема как раз это иллюстрирует то есть у вас здесь
пси это кубитное состояние размерности в 2 она выходе вот после кодирования у вас состояние
размерности 2 в степени 9 так ну вот степень совпадения здесь внизу написано это и есть как
раз фиделити упоминаем вот минимальная фиделити это будет там по всем си взять меня я посмотрю есть
на следующем слайде есть вот минимальная степень совпадения или как здесь написано минимальная
точность воспроизведения это вот такая вот величина берем си на входе кодируем пропускаем
через канал декодируем и результат должен мало отличаться от пси вот минимум по всем си возьмем
получим некоторые фиделити если фиделити близки единицы значит мы свою задачу выполнена вы
симптотики должны получить плянцами о чем секунду с конечностью должны получить и день ну вот
скорость передачи данных квантовых это такая величина что существует последовательность
пространств я вот это не говорил прошел раз говорил просто что отношения логарифма как n
таких, что выполняется. Эти последовательности нужны просто, чтобы был предел определенный.
Так, по этой части, это воспоминания предыдущей лекции. Есть вопросы или нет?
А кодирование – это у нас вот эта вот схема из конца прошлого семестра, да?
Например, такая схема. Для меня главное, чтобы вы поняли вот в этой схеме,
как она осуществляется. То есть вы хотите на самом деле передать вот это состояние
псималинка, но канал-то у вас шумит, вернее, в канале есть шум, поэтому вам нужно эту информацию
исправлять ошибки в результате действия этих шумов. Коды исправляющей ошибки мы с вами в конце
прошлого семестра смотрели. Они что позволяют эти коды исправления ошибки сделать? Вероятность
ошибки уменьшить, правильно? То есть вот если, например, без исправления ошибка была там П,
то после исправления она будет П квадрат. Вероятность ошибки падает. А тут смотрится
симпатический предел, когда n стынет в бесконечности уже. Но и вот эта ψ при этом,
его разменность тоже увеличивается. Вот давайте я сейчас напишу. Вот эта
ψ это элемент какого пространства? С, К. Вот эта ψn это элемент какого пространства? Это С,
например, 2 в степени n, если вы в кубитном состоянии кодируете. Видно, что я пишу?
Видно. Вот вы n устремляете к бесконечности. Это значит, что у вас число кубитов вот здесь
увеличивается. 2 в степени n, понятно, тоже увеличивается. К при этом тоже увеличивается.
Как посчитать сколько здесь кубитов? Это примерно 2 в степени логарифм К. И вот этот логарифм К
это число кубитов. Вот этот логарифм К разделим на n, и это будет отношение числа кубитов,
которые можно надежно передавать к числу кубитов, используемых при пересылке через канал.
Вот это отношение в пределе, когда n стынет к бесконечности. Видите, тут и верхняя часть
стремится к бесконечности, и нижняя часть стремится к бесконечности. Но их отношение
какой-то конечной. И вот это и будет ваша скорость передачи данных. При этом
конкретную схему вы позволяете некоторую свободу.
Вот это кодирование EN вы можете в принципе сами выбирать, какое хотите.
Например, вот эта схема. Какой изометрия? Это будет ваше кодирование EN.
Но понятно, что здесь EN зафиксировано. У вас EN равно, например, девятке, и если вы
посчитаете фиделити, вот эту минимальную степень совпадения, она будет меньше единиц.
Ну, например, там 0,9. Я точное значение не знаю, но будет какое-то.
И понятно, почему так. Потому что EN у вас конечная. Если бы вы EN стремили к бесконечности,
то есть эту схему как бы увеличивали, увеличивали еще, то вы бы эту степень совпадения сделали бы лучше.
Так, Эдуард ответил на вопрос или наоборот запутал? Ответили абсолютно полностью.
Хорошо. Вот последнее еще определение, вот здесь квантовая пропускная способность.
Это супремум вот этих достижимых скоростей, когда вы можете менять вот эти кодирования,
можете менять вот эти последовательства кодирования и так далее. Вот этого можете менять,
вот эта величина есть квантовая пропускная способность. В прошлый раз я под занавес уже
анонсировал верхнюю границу. Тут есть некоторые слайды с обоснованием этой верхней границы.
Давайте быстренько пробежимся по ним. Вот представьте, что у вас есть перепутанное
состояние. hk, hk это два подпространства. Ну, например, одно, второе. И у вас есть перепутанное состояние,
которое является на самом деле максимально перепутанным. Теперь, если вы одну из этих
частей закодируете, отправите через канал, декодируете, она вот здесь. И эта часть должна
быть очень похожа, ну вы симкотики воспроизводите, вот эту часть перепутанного состояния. То есть,
что вы на самом деле сделаете? Вы протягиваете эту перепутанность между частицей слева и вот
этой частицей справа. У вас получается перепутанность на расстоянии, потому что здесь слева это
подсистема справа от канала. Вы эту перепутанность в пространстве между лабораториями А и Б
распространили. И дальше вы можете, например, использовать протокол квантовой телепортации с
использованием классического канала. Таким образом, вы сможете передавать квантовые сообщения все
отсюда сюда, с помощью протокола квантовой телепортации. Так, вот этот слайд это воспроизводит. Так, дальше,
то есть, вы сможете в такой схеме передавать квантовые сообщения. То есть, это другая формулировка
с перепутанностью, которую в книжках используют, например, в книжке Вильде, Ватроуз и других,
для доказательства верхней грани. Поскольку у вас все-таки есть вот это вот асимпатическое
поведение, у вас какая-то ошибочка epsilon всегда есть. Ну и в стандартной нашей формулировке,
что для любого epsilon существует 0, такое, что для любых n больше, чем 0. Это мы кратко называем для
достаточно больших. Для достаточно больших n не превосходит эта ошибка epsilon. Эта epsilon
пропадет дальше в рассуждении, потому что мы n устремим в бесконечности, но для строгости давайте
так напишем. И вот теперь, как выразить этот логариф МК? Логариф МК – это есть энтеропия максимально
смешанного состояния, а максимально смешанное состояние – это есть подсистема максимально
перепутанного. Вот здесь они написаны, максимально смешанное состояние. И вы видите, что можно этот
логариф МК написать как энтропию просто подсистемы A или энтропию подсистемы R. А поскольку A и R в этой
схеме – это и есть ваше состояние C+, то тогда энтропия есть. Теперь в это выражение
можно переписать в другом виде. Вот картинку внизу надо посмотреть. Вот у вас S, A, R. После
прохождения через канал у вас будет... А, все. Немножко запутался. Вот смотрите. У вас
состояние ωRb мало отличается от максимально перепутанного. Вот если бы ε было равно 0, мы бы
просто здесь написали Sb. Если ε не нулевое, то тогда вот такая есть граница, которая называется
Алийский-Фанес-Винкер. Но вот эти вот приращения, которые здесь написаны, они при ε равно 0
зануляются. Мы потом перейдем к пределу, и поэтому они пропадут. Это понятно. А можно еще раз,
откуда у нас условная энтропия появилась? Вот, левее. Сейчас возьму другой цвет, чтобы
различался от того, что уже есть. Так, вот это вот. Да. Вот смотрите, тут некоторый трюк просто. S, A, R, A – это
вообще 0. Поэтому вы можете вычесть из любого величины 0, получите то же самое. Так, ну тут,
наверное, логично было бы написать S, A, но не суть важная. S, A или S, R – одна и та же. Тут логика
вот в чем. Вот смотрите на вот это вот неравенство. Это неравенство говорит вам, что Rb под система
почти такая же, как в максимально перепутанном состоянии. Поэтому, если вы замените вот здесь
вот букву A на букву B, то вы получите небольшую ошибку. Замена A на B не приведет к большой ошибке.
Эта ошибка как раз таки записана в виде вот красного квадратика справа. А когда вы устремите
n к бесконечности, в этом частном будет стоять n, и epsilon в пределении на n даст вам 0, потому что
для достаточно больших это означает вот именно вот эту фразу. Поэтому там у вас будет 0. А мы условия
такое уложили, что ω Rb недалеко от C+, лет откуда-то следует? Это мы хотим сделать, да. Вот представьте,
что, да, это мы условенно уложили. Вот кодирование, канал и декодирование – это почти тождественное
преобразование. В реанствамящемся к бесконечности должно быть тождественное. Это мы хотим. Вот смотрите,
да, еще раз, что хотим сделать. Предположим, что у вас есть вот эти кодирования и декодирования,
которые удовлетворяют всем вот этим вот свойствам, которые здесь перечислены. Мы теперь хотим верхнюю
границу на Q найти. Вот что мы хотим. Мы хотим ограничить ее сверху, потому что мы ищем
фундаментальные ограничения. Не превосходит она какой величины. Поэтому давайте предположим,
что существуют вот эти кодирования и декодирования, которые всем свойствам удовлетворяют. Значит,
это epsilon стремится к нулю, преанствуемящемся бесконечно. Вот. Когда у нас получится вот такое
выражение, которое внизу написано, видите, тут sb-sωrb делить на n. Сейчас, можно еще вопрос? А что такое h2 в красной применении? А, это бидарная тропия. Помните, что такое или напомнить?
Это, в смысле, для двух исходов или как? Да, да, да. Вот, смотрите, h2 от x, это и есть минус x логаритм x, минус 1 минус x логаритм 1 минус x.
Ну, вот еще просто напомню, что h2 от нуля есть 0, поэтому, когда вот это epsilon мало,
то у вас h2 тоже занулиться. То есть, вот эта вот величина в красной прямоугольнике, она при epsilon равна нулю точно равна 0.
А потом разделите на n и получите величину, которая в пределе даст тоже. Вот так вот.
Значит, остается вот эта вот величина, и ее надо ограничить. Для того, чтобы ее ограничить сверху,
ну, тут можно, давайте я кратко расскажу, как я понимаю. Что такое sb? sb, вот если бы кодирования
и декодирования не было, это было бы состояние на выходе. А что такое амега rb? Это было бы вот это
вот состояние на выходе вспомогательной системы r и b. А если у вас исходное состояние чистое,
оно чистое, то на выходе rb и окружение в представлении Stein-Spring тоже будет чисто.
Поэтому эта энтропия rb будет равняться энтропии e top. Поэтому энтропия окружения, это будет
энтропия выхода комплементарного канала. Когда у вас получится первая часть энтропии выхода
прямого канала, минус энтропии выхода комплементарного канала. Ну, та самая формула для
когерентной информации, которую в прошлый раз писали. Здесь немножко ситуация сложнее, потому что
у вас здесь, видите, есть e кодирование и d декодирование, и они немножко портят ситуацию, но не сильно.
То есть дальнейшие рассуждения направлены на то, чтобы избавиться от этого e, избавиться от этого d,
и тогда как раз будет та интерпретация, которую я только что сказал. А чтобы избавиться,
тут и свойства когерентной информации возникают. Видите, тут вот d и e пока присутствуют,
потом мы от них захотим избавиться. Избавляемся, поскольку когерентная информация...
Когерентная информация, то же самое, что квантовая взаимная информация.
Нет, смотрите, просто взаимная информация и какой-то канал. Это есть энтропия выхода этого канала,
плюс энтропия входа, минус энтропия выхода комплементарного канала. Это просто взаимная
информация. А когерентная информация, у нее буква c здесь будет. Наука молодая,
обозначения не устоялись. Используйте те, которые в книжке Холлива, чтобы вам было прочее тоже.
Поэтому вам, может, не нравится то, что там она называется когерентная? Ну, в общем,
то, что люди придумали и использовали, так же ему должно быть. Как я в прошлом семестре вам
рассказывал про Фейман, что он там свои обозначения придумал, его никто не понимал.
Точно так же и здесь. Надо использовать то, что там у них делают. Тогда у вас все будут...
Так, видите, разницу в чем? Разница заключается в том, что из взаимной информации мы вычитаем
ms.rho. Это немного другая величина. Хорошо. Вот что мы хотим с вами сделать. Избавиться от
вот этих кодирований и декодирований и в итоге получить величину, которая ниже представлен.
Ну, делается это с помощью там некоторых свойств. Когерентная информация удовлетворяет
такому свойству. То есть можно выкинуть второй канал ФИ2. Тогда у вас получается,
что можно выкинуть вот эти декодирования. Останется просто ФИ и кодирование на входе.
А кодирование на входе, это была аизометрия. Она энтропию не меняет. Поэтому у вас тоже будет
свойство то, которое нужно. Вот здесь написано, что EN это аизометрия. Энтропия не меняется под
действием аизометрии. Это аналог унитарного преобразования только с разными размерностями входа.
А изометрию мы не с двух сторон разве должны были бы умножать?
Да, это вот вопрос вот как раз задавался Тослов в прошлый раз, по-моему. Почему энтропия,
например, некоторого состояния РО равняется энтропии состояния W РО W КВЕСТИ, где W аизометрии.
Но аизометрическое отображение. Это понятно. В смысле, EN это и есть действие на РО,
это и есть по определению W РО W КВЕСТИ. А, хорошо. E это отображение. Это конконтинация
отображения. E отображение, F отображение, D отображение. Все вместе тоже какое-то отображение.
На экзамене этой части не будет, поэтому можете особо не вникать, если не хотите прям заниматься
этими вещами. Но для понимания, вот что получается, что из когерентной информации вы можете выкинуть
вот эти декодирования, декодирования. И вот тут у меня написано в правой части максимум еще поро,
но это и будет тогда верхняя граница для той величины, которую мы с вами смотрели. Вот здесь
СБ минус С омегарОБ РБ. Вот верхняя граница для нее, она на вот этом слайде и пристав. А как мы
еще раз правую часть убрали? То есть декодирование мы убрали по свойству, которое вверху. А кодирование
как убрали? Так, как мы убрали? Тут немножко хитрее у меня написано. Давайте, раз такой вопрос есть,
доведем до конца. Значит, смотрите, вот здесь вот РОА, это была матрица плотности какого размера? К на К.
Смотрите, это как раз таки в этой схеме, в исходной. А, ну, можно на эту схему смотреть, можно на
вот эту схему смотреть. Видите, тут была размерность К. Вот эта РОА, которая у меня там написана, это
есть матрица размером К на К. Дальше, что мы с вами сделаем? С того как мы обложим ее слева-справа
вот этим W, то есть применим кодирование, мы получим уже матрицу плотности размера 2 в степени N на 2 в степени N.
Эту матрицу плотности давайте назовем буквой РОА. И тогда мы можем избавиться от этого Е и написать
здесь просто РОА. Вы можете меня спросить, а почему так? Ведь в когерентной информации есть еще
комплементарный канал. Вот здесь пустое место есть. Смотрите, это энтропия выхода прямого канала,
РОА. Это значит, я вот это заменяю просто на РОА. Отнять энтропию выхода комплементарного канала. То есть
я должен написать ФИ тензор на НЕН, комплементарный действует на РОА. Вы можете спросить, а почему? Здесь-то
понятно, мы заменили на РОА, а тут как так можно сделать? Но вы видите, что есть такое свойство. Вы
оператор Крауса для ЕН. ЕН содержит только один оператор Краус. Если вы помните, как определяется
комплементарный канал, там с оператором Крауса он записывается. Вы увидите, что поскольку ЕН изометрия,
то вот эта величина, которая здесь написана, она в точность совпадает с ФИ тензор на Н комплементарное.
Так, а тут что мне надо написать? Тут мне нужно написать вот так вот ЕН РОА. Почему? Поскольку ЕН
содержит один оператор Краус. И снова получается РОА, ой, просто РОА. Снова получается РОА. А если кратко
тут так, и вот РОА у вас остается, и вы можете взять максимум по этому РОА, и это будет верхняя
граница для вашей когеретной информации. Вот так вот избавляемся от кодирования. Вот верхнюю
границу мы теперь получили. Дальше что мы можем сделать? Воспользоваться вот этим вот пределом,
ладарифм К делить на Н не превосходит, предел один делить на Н. Вот верхняя граница для этого
выражения. Это есть теперь вот этот вот голубенький рамочек. Все, вот эта вот верхняя граница,
мы ее с вами получили. А дальше то, что эта граница достигается при энтисеме и бесконечности,
это намного более сложное доказательство, которое было закончено девятаком только в 2005 году.
Окончательное доказательство было представлено, что эта граница достигается. Вот получается,
что с 2005 года мы знаем вот такое выражение для квантовой пропускной способности канала,
то есть Q-фи задается вот такой вот формулой. Еще говорят, что это формула регулиризованная,
поскольку есть один делить на Н и вот этот предел. Теперь здесь те же самые рассуждения про
аддитивность и так далее, которые стандартно возникают. В общем случае, аддитивности нет.
Примеры отсутствия аддитивности к эгерентной информации это последовательное применение
дефазирующего и стирающего каналов. Это вот недавняя работа Ледицкого в 2018 году. Есть вот
еще такой класс обобщенных стирающих каналов, это я вот сделал в прошлом году,
journal physics 8, у меня статья есть. То есть аддитивности нет для широкого класса каналов. В общем случае
получается, что, например, если вы возьмете здесь двойку в этом выражении, вы получите величину
больше, чем когда возьмете единичку. Ну это вот в этой рамке написано. Таких примеров много,
но есть случаи, когда все-таки аддитивность к эгерентной информации имеет место. Она имеет
место для деградируемых каналов. Эгерентная информация аддитивна. Доказательство есть в книжке Холева,
оно же есть и в статье Шора, по-моему, из девятак, как раз таки, 2008 года. Аддитивность для деградируемых
каналов-то оно несложное, просто я не хочу тратить время на это. А для антидеградируемых каналов у вас
получается нулевая пропускная способность. Так, это было у вас на семинарах. Я знаю, что я рассказывал
для 813 группы, а для других, для 11-12 были у вас деградируемые антидеградируемые каналы. Просто
название их и определение были точно. А вы хотите так чуть-чуть побольше обсудить их, да, немножко?
А, ну да, можно. Было бы неплохо. Ну, в общем, давайте на примере расскажу. Другой цвет ещё возьму.
Вот представьте, что у вас, ну давайте начнём с того, что такое прямой канал. Прямой канал
показывает, что будет на выходе, если вы какой-то вход пошлют. А комплементарный канал показывает,
что при этом растворится в окружении, то есть какая информация об исходном состоянии РО останется
в окружении. Вот теперь вы можете сказать, например, канал будет сильно шумящим, если в окружении
растворяется больше информации, чем проходит. Вот так вы скажете. Тогда канал сильно шумит.
Но что это означает с точки зрения математики? Это означает, что вы, например, можете из вот
этого состояния получить состояние на выходе прямого канала. То есть если вот эта вот стрелочка,
я сейчас её жирной сделаю, если вот эта стрелочка работает, то есть если вы из состояния окружения
можете получить выход канала своего прямого, то это означает, что в окружении больше информации
содержится. Это означает, что это вот будет как раз-таки антидеградированный случай, то есть что
выход прямого канала можно представить как конкатинацию выхода комплементарного и какого-то
вспомогательного канала тета. То есть вот эта стрелочка — это тет. То есть это сильно шумящий канал.
А как так получается, что мы из информации... Сейчас, вот у нас была информация какая-то, которую мы
послали, а потом мы говорим, что из информации, которая растворилась в environment, мы можем
восстановить информацию, которую получили на выходе, да? Это значит, что у нас информация дублируется
как-то. Вот-вот-вот-вот. Вы и пришли к тому, что я хочу сказать. Значит, получается, смотрите как.
Если, да, вы в этом случае можете эту информацию дублировать, то есть клонировать, правильно? Это что
означает? Вот смотрите внизу объяснение красным шрифтом. Это вот как раз тот сценарий, который вы
запрашиваете. Допустим, пропускная способность канала phi больше нуля. Это означает, что если я использую
его много раз в пределе энстеначности к бесконечности, я могу чистое состояние psi воспроизвести с
напереданной точностью. Вот здесь вот, правильно? Но поскольку в окружении растворяется больше
информации, чем проходит вперед, то я его могу и вот здесь вот восстановить. Понимаете?
И тогда у нас возникает противоречие с чем? С теоремой запрете клонирования. Нельзя произвольное
состояние psi воспроизвести в двух местах для любого psi. Поскольку клонирования у нас нет,
то значит пропускная способность равняется нулю квантовая. Видите, что получается? То есть не существуют
таких кодирований и декодирований, сколько бы раз вы не применяли канал, чтобы воспроизвести вот
эти вот состояния psi, как на выходе канала, так и в… Ну да, на выходе канала. Пример здесь такой,
например, стирающий канал. Если вы разбирали на… Там есть параметр p, вероятность того, что состояние
будет задетектировано. Так вот, график для q выглядит таким образом. Сначала это просто ноль,
при малых значениях p, вероятность успешного детектирования, а потом начинает расти.
Любое значение для пропускной способности q как раз-таки означает, что шум настолько сильный,
что вы не можете воспроизвести состояние… Ну, короче, не можете исправлять ошибки.
Шум очень сильный. Так, про антидеградируемые более-менее понятно? Это сильно шумящие каналы.
Антидеградируемые, сильно шумящие, квантовая пропускная способность для них равна нулю.
А правда, что мы выход комплемендарного канала никогда не видим и не фиксируем никак? Мы просто
используем его как модель. Да, тоже хорошо. Спасибо за вопрос. Сейчас скажу, почему это. Радуюсь.
Радуюсь потому, что в конце прошлого года я делал там доклад в Мяне, и как раз-таки Холливый говорит,
что окружение можно считать вообще по-разному, рассматривать окружение. Окружение – это не
обязательно вот та физическая среда, которая окружает оптоволокно, когда вы по нему пересылаете
сигнал. А в качестве окружения можно рассматривать еще и действие перехватчика, например. То есть
окружение – это такое растяжимое понятие. То есть вот эта вот фии с волной, комплементарный канал,
он показывает еще что? Возможные вмешательства в ваш канал связи и так далее. То есть вы же не
знаете, за счет чего у вас возникает шум в линии. Вот шум может возникать из-за того, что кто-то
вот пытается в ваш канал включиться, подсоединиться. Буква И – она не только environment может
обозначать, но и eavesdropper – тот, кто вас подслушит. То есть в принципе вы правы, если у вас окружение
представляет собой реальный физический объект, например, какие-то моды электромагнитного излучения,
которые связывают с оптоволокном и так далее, то вы к ним доступа не имеете, но и никто другой не
имеет доступа. Но в принципе шумы могут возникать из-за того, что кто-то в вашу линию связи внедряется.
Если у вас есть только вход и выход, вы не знаете, как устроена промежуточная часть, то вы в худшем
сценарии должны всегда предполагать, что это кто-то внедрился в ваш канал и пытается его как-то там
подслушать, еще что-то. Значит у него будет у этого человека вот такая вот информация фикомплементарная.
Так, про это был вопрос или не про это? Про это. Тут логика такая, для нас канал – это некоторый
blackbox. Мы знаем, что происходит на выходе из него, но как его внутренняя структура устроена, мы не знаем.
В худшем случае это кто-то нас там пытается подслушать. Так, а вот деградируемый канал – это мало
шумящее. В каком смысле? В том смысле, что вы из выхода прямого канала можете получить выход
комплементарно. Вот тут мне тоже написано, шумы не сильные, и в этом случае есть аддитивность
когерентной информации. И формула тогда упрощается. Формула тогда, вот этот предел, который
здесь написан, равняется просто максимуму по ρ и c, ρ, φ, пропадает тензорная степень, пропадает
вот это деление на n, если φ является деградируемым каналом. Так, ну в принципе, вот в этой первой
части я рассказал то, что хотел рассказать про квантовую пропускную способность. Так, дальше у
меня пойдут тензорные сети. Давайте вопросики еще. Да, можно еще вопрос тогда. Так, можно еще
раз про то, что вот мы сказали, если у нас канал деградируемый или антидеградируемый, то мы,
получается, можем клонировать состояние. Давайте вот разделим. Если, допустим, вот так, сейчас возьму
указку. Вот тут многоходовка. Я плохо рассказываю о многоходовке, сейчас постараюсь еще раз. Вот
представим, что выполнены два условия. Канал антидеградируемый, то есть сильно шумит, вот выполнено
вот такое свойство. И его пропускная способность больше нуля. То есть два условия выполнены. И одно и
другое. Он и антидеградируемый, и q больше нуля. Когда мы получим противоречие? В чем будет
заключаться это противоречие? Если канал сильно шумит, значит, у вас информации в окружении больше,
чем на выходе из канала. А если q больше нуля, то вот здесь, на выходе из канала, вы уже можете
восстанавливать psi с помощью некоторых операций кодирования деградитации. Но раз здесь phi rho содержит
меньше информации, чем phi-комплементарный ров, то здесь тоже можно восстановить psi.
Эти внизу тоже можно восстановить. Получается, что psi можно восстановить и там, и там. Значит,
мы можем клонировать. А клонировать нельзя противоречие. Какое из этих двух утверждений
неверно? Один или два? Или оба? Первое, это просто фиксировать свойство канала. Неправильно было
наше предположение о том, что q больше нуля. Значит q равно нулю. Поэтому квантовая пропускная
способность для антидеградированного канала равна нулю. Все эти рассуждения только для
антидеградированного. Все, хорошо, я понял. А если у вас он просто деградируемый, то никакого
противоречия нет. Если вы допустим, что q больше нуля, ну тогда что означает? Что вы берете psi,
здесь воспроизводите psi, а здесь меньше информации. Укружение растворяется. То есть,
может быть, вы psi не можете здесь получить. Ну как бы никакого противоречия нет. Но что хорошо
для деградируемых каналов? Для деградируемых каналов хорошо то, что аддитивность есть для
кигаретной информации. И тогда формула, вот эта вот сложная формула под середине слайда,
преобразуется в простую формулу, которая справа написана. И вот это вы используете при решении
домашней задачи для дефазирующего канала. Дефазирующий канал удовлетворяет такому свойству.
Он является деградируемым. Ответил или не очень? Да, ответили. Так, еще вопросики есть про квантовую
пропускную способность? Или все в предвкушении уже тензорных диаграмм и так далее. Видимо в
предвкушении. Поехали тогда. Сейчас будет много картинок, поэтому и оправдана наша лекция онлайн,
потому что рисовать эти картинки долго, проще тут уже показать на экране. Так, квантовые тензорные
сети. Значит, тензорные диаграммы просто осложны. То, что раньше вы изучали, можно представить в виде
простых картинок. Вот, например, тензор первого ранга или первого порядка, как говорят, это будем
изображать в виде объекта с одной палочкой. Индекс, который пробегает, это палочка. Вот здесь
нарисована у меня ситуация, когда вы различаете Contra и Co вариантные тензоры. Например, Contra это
будут ножки вверх, а Co это ножки вниз. Все зависит от метрики. У вас метрический тензор это единичная
матрица или не единичная. У нас потом, в нашем курсе, будет единичная метрическая, метрический
тензор и единичная матрица издаваться, поэтому нам будет без разницы, куда ножки будут. Я просто
хочу показать вам, как формулы из вашей теории поля, которую вы изучали год назад, можно записать
в виде тензорных диаграмм в простом виде. Так, вот этот слайд понятен. Графическое представление вот
такого тензора с компонентом. Пока просто картинка. Теперь вы можете Co вариантный тензор написать
вот так. Ножку вниз. Дальше что вы можете сделать? Вы можете их объединить. Если вы их объединяете в один
тензор C, то это будет тензор второго ранга. Ранг определяется количеством ножек, которые ни на
что не замкнуты. Свободные ножки это и есть. Количество свободных ножек это и есть ранг тензора.
Видите, раньше были у вас в курсе теории поля вот эти вот индексы И, Ж, вверху, внизу, путаница была,
как там поднимать индекс, как там опускать. Здесь пока все понятно. Вы те ножки, которые смотрят
вверх, это верхний индекс, те ножки, которые смотрят вниз, это вот тензор второго ранга. Если вы,
например, их соедините в другом порядке, то есть соедините ноги, то тогда у вас что получится? У
вас получится одна нога, но она замкнута на вот эти картинки А и В квадратики. Поэтому индекс
уведется с суммированием. То есть как только у вас возникает АИТ, БИТ, индекс вот этот И совпадающий,
по нему уведется суммирование правила Эйнштейна. Помните, что мы в результате получаем? Мы получаем
вот этот вот тензор D, у которого нет ножек, смотрящих наружу. А если нет ножек, смотрящих на
ружу, значит, тензор нулевого ранга. Пока понятно? Да. Да, только я забыл, что такое коврянтный, чем
означается коврянтный. У них правила преобразования разные, но сейчас не будем про это. Присмотрите. Если вы возьмете,
например, и замкнете вот эти ножки А и Б, то вы получите тот же самый объект, что вот справа на
нему нарисован. Но это вы просто взяли след у матрицы, которая тут была. Ну ладно. Вот тензор третьего ранга,
например, нарисован. Т, И, Ж, К. Если вы, например, замкнете ножки вот так вот, то это называется еще
свертка тензора по индексам ЖК. Значит, пишем Ж, Ж, суммирование по Ж, получаем тензор ранга 1. Понятно?
По-моему, все очень понятно. Так, дальше. Ну вот, например, символ левичьего вито E, I, G, K, L для
размеров с 4. Ну, это тензор четвертого ранга. Вы, когда его, например, соединяете с тензором F и G,
тензором электромагнитного воля, вы получаете с коэффициентом 1,2. Вы получаете вот такой вот объект F,
F, K, L. Видите? K, L, ножки смотрящие вверх. Все вместе, сейчас выделю. Все вместе. Это есть тензор второго ранга.
Видите, потому что ноги K, L наружу, а все остальные суммируются. Это будет у вас дуальный тензор, тот самый,
который в теории поля вы изучали год назад. То есть все те формулы, которые были год назад,
можно записать в виде таких простых диагнозов. Теперь вот метрический тензор. Что значит, например,
опустить индекс? Вот у вас был AIT, контравариантный тензор. Вы хотите сделать так, чтобы ножка
смотрела вниз. Тогда вы применяете вот этот G и Gt. Это будет у вас метрический тензор.
Если так напишете, то понятно. Свертка вот такая. У вас как раз таки переставляет верхний индекс
сделать его нижним. Если сделаете G и Gt, где и наверху, и внизу, то это будет символ экрана.
Все очень просто. В нашем случае нет. Вот это G будет задаваться единичной матрице. Поэтому
нам неважно будет, куда смотрят ножки. Вверх-вниз, это все без разницы. Поэтому мы выявили две метрики,
куда ножки смотрят нам без разницы. Правого вниз, влево. Теперь давайте поговорим,
какие свойства здесь есть. Вот как записать матрицу. Матрицу можно записать в виде тензора второго
ранга. M и Gt, это есть матричный элемент. Теперь, если у вас есть произведение матриц,
то тогда вы можете взять M на N произведение. И, тк, и, элемент. Это есть сумма по G. M и Gt,
N и Gt. Это первый курс формулы. Произведение матриц это будет просто соединение вот таких
вот диаграммах для M и для N. Справа, которая показывает. Это понятно? Да. С недавних пор я
начал немножко по-другому еще делать. Когда у вас есть вот такая вот картинка M с ножком,
то не очень понятно, какая нога отвечает за нумерацию строк, а какая нога отвечает за
нумерацию столбцов. Чтобы избежать этого, можно вот так вот написать, что это есть сумма по
G, M и Gt. Тут, например, у вас будет ket и braji. Вот эта вот запись пока понятна? Есть M, как оператор.
Понятно. Вот та ket ножка, которая выходит, я обычно теперь рисую стрелочкой исходящей. А та нога,
которая соответствует bra компонентам, стрелочкой входящей. Это вот bra, а вот это вот ket. Значит,
здесь будет i, а здесь будет j. Чем удобны эти стрелочки? Эти стрелочки удобны тем,
что они показывают еще порядок произведения матрицы. То есть, если мы вот здесь нарисуем
эти стрелочки, то они будут показывать сначала N, потом левее M. Вот так вот. Все очень просто.
Значит, здесь слева еще показано, что... Взятия следа. То есть, если вы вот так вот
соедините эти ноги, то вы получите след M, сумма по E, M и Gt. Очень хорошо. Это все произведение.
След тут я тоже же сказал. Что значит транспонировать матрицу? В левом нижнем углу написано. Это значит
ноги ее закрутить в другие стороны. Либо вот так вот сделать. Если у вас есть картинка M и вот эти
стрелочки, то вы хотите поменять направление стрелочек. Но если вы меняете направление стрелочек,
то внутри вы должны уже поставить M-транспонирование. След от произведения матриц. Здесь вот картинка.
Все очень просто. Вы можете перетащить эту матрицу M, например, по красной стрелке. Она станет справа
от матрицы N. И вы в диаграммном языке очень просто доказали вот это вот свойство. Что след
произведения матриц? Равняется след произведения матриц в обратном порядке. Теперь тензорное
произведение. Справа внизу картинка. Есть матрица M с элементами I, G. Есть матрица N с элементами KL.
Вот когда мы просто их эти элементы умножаем, это все равно, что мы на тензорном языке записываем
вот эти M, N рядом с другом. Получаем некоторую матрицу T, I, G, KL, где теперь I, K это один мультииндекс,
а G, L это другой мультииндекс. Что мы получаем? Так, про тензорное произведение, наверное,
надо вам немножко еще осмыслить. Давайте вопрос какой-нибудь. А мы обязательно рисуем прямоугольник
вокруг M, N? Нет, любую картинку, может треугольничек. Нет, в смысле, что-то рисуем обязательно вокруг.
Ну обычно да, очень да. Сейчас-то почему еще раз мы можем записать все внизу? За это все,
из-за единичной метрики. Да-да-да, теперь неважно какие индексы вверху, какие внизу. А почему
транспонирование вот так выглядит? Вот эти ноги менять местами или как? Или наверху? Нет,
вот эти ноги транспонирование матрицы. А, только вам нужно поменять направление с строчек и столбцов.
Давайте я стрелочками покажу. Вот раньше у вас было вот так, это была исходная матрица. Теперь
у вас ноги будут выходить в другую сторону. Я думал, это мы ее зациклили, взяли слет и еще полку сверху положили.
Нет-нет-нет, тут пересечение. Вот тут вот нет-нет-нет, это у меня ноги так пересекаются, когда рисовал.
Хорошо. Ну, поняли, да? Тут нет. Так, мне бы пустой слайд еще сделать. Давайте знаете,
как я сделал. Я сейчас ставлю пустой слайд, потому что хотел вам кое-что рассказать про
тензорное произведение еще. Сделал стекущего слайда. Вот, про тензорное произведение еще
чуть-чуть расскажу. Вот у вас, давайте в другую сторону нарисую, чтобы было более понятно. Вот,
например, матрица M, матрица N. Вот тот объект, который сейчас есть, это объект четвертого ранга.
Мы его с вами трактуем как тензорное произведение M на N. Видите, получается, что M это, грубо говоря,
пояс, который по рельсам двигается верхним, N это пояс, который по рельсам движется нижним.
Если вы их сдвинете, то ничего в принципе в плане картинки диаграммной не поменяется. Вот,
например, вот так вот я сделал. Это та же самая картинка. Мы же ничего не сделали, просто линии
продлили и сдвинули немножко тензор. Но при этом мы с вами получили некоторые свойства. Если я
посмотрю на вот этот объект, что я здесь вижу? Я вижу здесь единичную матрицу и вот здесь тоже
вижу внизу единичную. Получается, что M тензор на N равняется, ну, например, и тензор на N,
а вот M тензор на N. Видите, простые выражения можно доказать с помощью этих тензорных диаграмм
очень легко. Шок. Что еще? В такой момент у нас появилось произведение матриц. Вот на картинке
произведение, что соответствует. Ага, да, сейчас сделаем. Вот M, это у вас была матрица, у нее были
индексы IJ, да? Вот давайте я их пропишу. У N, например, KL. Вот когда мы два объекта вот так написали,
мы и записали вот этот тензор четвертого ранга. Там, по-моему, на предыдущем слайде это T было. T и JKL.
Так, где возникло произведение матрицы?
Не знаю, где оно возникло. Ну вот, вы сказали, что это единичный на N произведение матриц.
Так вот же оно, вот оно. Вот, смотрите, вот это уже было понятно. Вот когда мы соединяем M и N,
вот это отдельно две матрицы. Вот матрица M, матрица N. Когда я соединяю линии, то я получаю
произведение. Я соединяю их линиями, а линия тут и так есть. Вот я и получил произведение.
Так, и почему сейчас там одна линия была, и это было суммирование по индексу. А, здесь
суммирование по 2 индексам получается. Да, да, вот про это я и хотел сказать. Смотрите, у тензоров
есть, бывает, форма. Форма тензора. Это, грубо говоря, сколько ножек торчит и какие измерения для
каждой ножки возможны. То есть вот сколько значений индекса каждый пробегает. J, например, 1, 2, 3, 4, 5.
Это форма тензора. Что такое форма тензора? Более-менее понятно, сколько ножек и сколько
значений пробегает каждый ножок, индекс каждой ножки. Вот, теперь смотрите, вы эту форму тензора
можете менять. Каким образом? Например, вы можете объединить индексы в одну ногу, понимаете, и это
тогда даст вам мультииндекс. Мультииндекс. Вы изменили форму тензора. У вас теперь вот это все как одна
нога, но то значение индексов, которое пробегает эта нога, есть произведение измерений для каждой
ноги. Когда мы записали вот в таком виде, мы и подразумеваем, что мы объединили индексы ИК в один
мультииндекс, а JL в другой мультииндекс. Так же, как в прошлом семестре. Да, хорошо.
Смотрите, еще у нас появилось такое понятие, как форма тензора, и ее можно менять. Сейчас я
промотаю слайды. Вот объединение ЖК в один мультииндекс дает вам такую жирную ногу. Видите,
здесь Ж штрих. Это вы поменяли форму. Вы можете, например, местами поменять тут индексы. Тоже
форма поменяется у тензора. Про форму тензора можно еще так рассуждать. Вот пусть у вас есть
тензор третьего ранда, то есть три ноги и ЖК. Это получается, вы же информатики, математики. Это
массив многомерный, правильно? Вы можете элементы этого массива, тут они цветами обозначены черным,
один индекс синий, другой красный. По-другому перенумеровать. То есть вместо вот такой вот трехмерной
таблицы вы можете все эти данные записать в виде, например, двумерной таблицы такого типа. Но вся
информация та же самая, просто вы форму поменяли. Тут форма одна у тензора, а здесь другая. Понятно?
А вот то, что я хотел вам еще рассказать, это вот такое простое свойство, которое внизу написано,
вот это равенство. Помните, мы его доказывали с вами в прошлом семестре у доски. Там смотрели
мультииндекс, там что-то такое выводили. Давайте теперь докажем на языке тензорный диагноз. Значит,
давайте с правой части здесь начнем, на этом слайде. Что такое AC и BD? Вот зеленым выделено
произведение матрицы AC и другой зеленой рамкой выделено произведение матриц BD. Вот рельсы не
пересекаются для этих двух столбиков, поэтому здесь значок тензорного произведения между ними. Вот правая
часть, понятно? Да, понятно. Теперь вот сейчас переключу на другой слайд, там будет по-другому
разбито все это. AB будет вот такой вот формой, ACD вот этой вот нижней формы. Как только вы на желтые
блоки смотрите, желтый блок это AB, вот он верхний, а нижний желтый блок это C тензорным умножить на D,
а красные линии вертикальные в картинке это есть произведение. Оп, доказали с вами формулу.
Видите, как все просто. А наверху тут доказать того, что след от тензорного произведения есть
произведение следов. Короче, тензорный диаграмм может просто всякие матричные соотношения
доказывать. Первый месседж, форму тензора можно менять, второй месседж. Теперь что такое тензорная
сеть? Тензорная сеть это объединение тензоров некоторое, в котором у вас есть свертки. Видите,
вот здесь вот сворачивается, вот здесь вот сворачивается. Потому что эти ножки соединены,
соединяют разные объекты. Какие индексы свободными остаются? Вот одна свободная нога, другая, третья,
четвертая. Еще есть, вроде не вижу. Вот это значит все вместе. Это есть тензор четвертого раунда,
составленный из свертки других тензоров. Как выглядит вот это вот, как мы знаем,
какие свертки делать. Есть граф. Граф показывает, с какими тензоры нужно соединить. Этот граф это
есть структура тензорной сети. Граф сверток, плюс свободные ноги, это и есть архитектура
тензорной сети. Так более-менее понятно? Теперь ваш ключевой вопрос. Зачем это надо? Поясняю на
примере. Допустим, у вас есть многочастичное состояние. Допустим, из N убитых. Значит,
C, которая здесь записана, это есть, видите, и 1 принимает два значения, 0,1, и 2, 0,1, и так далее,
и N, 0,1, и C, и 1, и 2, и N. Это есть как раз таки тензор. Какого раунда? Раунда N. У него N ножик.
При этом сам C, этот объект, как ket-вектор, это есть элемент скалькимерного пространства 2 в
степени N. Размерность гигантская для больших N. Экспоненциально большая размерность. Экспоненциально
большая размерность. Это означает, что чтобы задать общее состояние N убитых, вам нужно задать вот
эти вот комплексные числа, C, и 1, и 2, и так далее, и N. То есть сколько вам нужно действительных
параметров? Это 2 умножить на 2 в степени N. Если комплексные параметры считаем, то это будет 2 в
степени N комплексных параметров. Чтобы задать вот этот объект C, вам нужно экспоненциально много
параметров. Если же вы его представляете в виде вот такой вот свертки, где каждый тензор обладает
малым рангом, то тогда у вас количество параметров необходимых может стать меньше. В этом как раз
таки выиграешь описание в виде тензорных сетей. Вы учитываете корреляции между частицами, например,
корреляция между этой частицей и этой частицей, учитываются посредством вот этого графа. Вот видите,
какие тут сложные корреляции. Однако при этом число параметров, которые используются, будет меньше.
Вот следующая картинка покажет. Тут правда я N заменил на N маленькое, но я думаю вы не запутаетесь.
Здесь вот 2 в степени N комплексных чисел задают общее состояние. Состояние общего
Общее состояние. А вот справа показана свертка таких вот тензорочков. Давайте смотреть. Вот здесь вот этот
тензор третьего ранга. Так, значит смотрите, в правой части нарисована свертка, где граф линейный
соединяет тензорочки маленькие. Каждый маленький тензорочек посерединке задается 2 умножить на R квадрат параметров.
Вот столько параметров для их задания. Это понятно? Нет. А что такое R? R это размерность связи, ну индекс, сколько
индекс, какие значения, сколько значений может принимать индекс в горизонтальной связи. 1, 3 точки R. Видите, вот написано.
1, 3 точки R это те значения, которые может принимать индекс в горизонтальной связи. Так, откуда тогда еще 2? Каблярка квадрата?
А 2, 2 внизу, вот она. А почему мы захотели, чтобы у нас тензор вот так вот выглядел, чтобы он у него горизонтальный?
Хотим, вот так вот. Самый простой граф, линейный граф. А почему на выходе, ну вот почему там, где 2 у нас не R значений?
А потому что это кубиты. Вот смотрите, вот здесь вот, например, смотрите, слева пишу сейчас. И2 это ножка.
Вторая. Она показывает вам возможные состояния второго кубита. Либо он в нулевом состоянии, либо он в состоянии 1.
Я смотрю, например, на ножку ИН. Она тоже может быть либо 0, либо 1.
Вот эти же ножки нарисованы здесь. И2, ИН.
И они пробегают два значения индекса соответствующих ножек.
Хорошо, да. То есть получается, смотрите, слева представлено огромный массив. Это будет N-мерная таблица слева.
Справа представлено много маленьких табличек, по которым ведется свёртка.
Теперь оказывается, оказывается, что представление вот это вот слева, где экспоненциально много параметров,
может замениться вот этим представлением справа, где линейное с ростом N число параметров.
Для некоторых ситуаций, например, для основных состояний локальных гамильтиньянов спина в цепочек или других отрывов.
Сейчас покажу пример, где вот это вот представление имеет место.
Вот тот линейный граф, который мы рассмотрели, он называется, он приводит к состоянию матричного произведения.
По-английски это Matrix Product State, MPS. А это название придумали люди, которые физикой занимаются.
А те люди, которые занимаются сжатием данных, а сжатие здесь видно, как это происходит.
Те люди, которые занимаются сжатием данных, это вот, например, прикладными вещами занимается оселедец в Румскалтехе.
Он ученик тортышников. Они называют это Tensor Train.
То есть те люди, которые занимаются сжатием данных, они называют это Tensor Train.
Из картинки более-менее понятно, почему они так назвали. Поезд из тензоров.
Смотрим еще раз на описание. Вектор ψ задается вот такой вот формулой.
Коэффициенты С здесь записаны. Эти коэффициенты С, как тензоры Н того ранга, могут быть представлены как произведение некоторых матриц.
А откуда берутся эти матрицы? Возьмите желтую область вот здесь и зафиксируйте ножку и зафиксируем значение И.
Если я значение Икатой зафиксировал, ну, например, ноль, то тогда я получу уже матрицу.
Видите, тогда я получаю вот здесь матрицу. Как я назову эту матрицу? Она стоит на окатом месте.
У меня число букв алфавита может быть меньше, чем число кубитов, например, число кубитов тысячи.
Поэтому я буду использовать букву А и здесь ставить К. Это будет как бы мое обозначение для буквы.
И что я еще сделал? Я зафиксировал ножку Икатой, поэтому я запишу вот здесь вот Икатой.
И все вместе это получилась матрица, у которой есть ножка влево, ножка вправо.
Ногу левую обозначим альфа. Ногу правую обозначим альфакат.
Тогда вот этот вот объект, вот он и получится, который здесь внизу представлен.
Это матричные элементы. Матричные элементы объекта желтого.
Так, ну много обозначений, вы могли запутаться. Давайте вопрос.
Короче, это матрица R на R. Да, матрица R на R.
Если вы возьмете крайний элемент, вот здесь вот, и зафиксируете И1, то вы получите матрицу какого размера?
1 на R. Это что такое будет? Это будет строчка. Это будет строка.
Если возьмете правую, то здесь будет матрица размера R на 1. Это будет столбец.
Значит, смотрите, вот эта строка, это есть вот эта матрица. Вот этот столбец, это есть последняя матрица.
Строчка, матрица, матрица и так далее, пока не получим столбец. Это все даст вам просто число.
А потом обучаем на эту цепочку, чтобы она выучила представление?
Да-да-да, это вариационный метод называется. То есть вы можете варьировать параметры вот этих маленьких табличек,
чтобы она представляла ваше состояние ПСИ, которое слева написано.
Она будет точной? Точной мы можем добиться представления? Или все равно приближение?
Нет-нет-нет. Сейчас скажу. Все зависит от степени перепутанности.
Если у вас перепутанность левой части с правой частью мала, то тогда вы можете с наберет заданной точностью это все сделать.
Если перепутанность большая, в смысле этой энтропии перепутанности, то тогда вам нужно R увеличивать для увеличения точности.
То есть, короче, буквовка R подстраивается под состояние ПСИ.
Для некоторых состояний ПСИ R мало, для некоторых состояний ПСИ R большое.
Вот мы будем интересоваться теми состояниями R, для которых R мало.
Что такое R? Давайте вот про это поговорим.
Альфа-ката у вас – это те значения, которые индексы горизонтальных связей принимают.
Эта размерность связей еще называется по-английски bond dimension.
Это R-ката называется bond dimension.
Максимум из этих R-катах – это есть rank MPS.
Это не rank матрицы, а вот так вот называется rank MPS.
Пример.
Если rank MPS равен 1, это означает, что размеры всех этих горизонтальных связей есть единичка,
то тогда это означает, что у вас этих горизонтальных связей вовсе нет.
Потому что суммы нет.
Сумма есть по индексам. Если все индексы зафиксированы,
если все альфы принимают одно значение, то тогда суммы нет.
У вас факторизованное состояние – нет перепутанности.
То есть rank MPS равен единице – это все равно, что состояние не перепутано.
Если rank MPS равен 2, то уже можно описывать перепутанное состояние.
Вот у вас в задании есть задачка про GZ-состояние.
GZ-состояние N-кубитов.
Давайте ее решим.
Значит, нужно… Что это за GZ-состояние N-кубитов?
Оно записывается таким образом.
Один делить на корень из двух. Все нули плюс все единицы.
В прошлом семестре, помните, были в задании 3 нуля, 3 единицы.
Это было GZ-состояние для 3-х кубитов.
А здесь GZ-состояние для N-кубитов.
Вот это состояние перепутанное состояние.
Вот давайте его представим в виде тензорной сети MPS.
Как это сделать?
А вот так вот очень просто.
Вот эту строчку первую задайте таким образом.
Матрицы задайте вот таким образом.
1 0 0 0 или 0 0 0 1.
Ну и столцы окончательно задайте таким образом.
Почему так нужно делать?
Потому что если вы возьмете, например, матрицу какую-то карту с индексом 0,
то она будет у вас 1 0 0.
Если вы возьмете матрицу АК плюс 1 с индексом 1,
то она будет выглядеть вот так вот. 0 0 0 1.
Если вы их перемножите, эти две матрицы, то вы получите нулевую матрицу.
Что означает нулевая матрица?
Это означает, что в состоянии вот таком вот состоянии C и 1 и так далее,
вот дошли до катового места.
На катом месте стоит 0, катая позиция.
На АК плюс 1 месте стоит единичка.
И дальше снова какие-то там индексы.
Но нам не важно. Если встречается 0 и единичка, то коэффициент зануляется.
Теперь поскольку матрица коммутирует, а не диагонально,
то не важно, встретятся ли эти 0 и единичка рядом друг с другом
или вообще будут представлены цепочки.
С какие-то элементы, потом 0, потом какие-то элементы,
потом 1, какие-то элементы, будет равно 0.
А это как раз то, что нам нужно.
Нам нужно, чтобы или все были нулями, или все были единичками, индексы.
Понимаете?
Но это сложно с первого раза въехать.
Тут нам потребуется два значения.
Один на корень из 2 и один на корень из 2.
Вот на самом деле-то нет. Смотрите.
Сколько параметров вам здесь нужно?
Я утверждаю, что 2 в степени n.
Вы правильно говорите.
С 0, 0 и так далее, 0 это 1 делить на корень из 2.
С 1, 1 и так далее, 1 это 1 делить на корень из 2.
А вот С 0, 0, 1 вам же тоже нужно задать?
Должны вы выбрать его равным нулю.
То есть вот в этой таблице n-мерной,
в n-мерной таблице у вас много нулей
и местами есть вот эти вот элементы 1 делить на корень из 2.
Но вы же должны прописать нули, где они должны быть.
Поэтому в этой n-мерной таблице она sparse.
Разреженная.
Да, разреженная получается таблица.
Но, извините, нули там тоже надо прописать.
Когда вы задаете gz-состояние в виде вот такого вот тензорного поезда,
то тогда у вас r равно 2.
И сколько вам нужно здесь задать параметров?
Значит, 2 на r в квадрате и на n.
То есть получается 8n.
Сейчас я отвечу на r2.
А потому что размер матрицы 2 на 2.
Вот эта матрица, это есть r на n.
Да, все понял.
Ну, смотрите, на самом деле мы только что решили
под задачку, домашней задачей.
Одной из задачки задания.
Ну, на семинарах подробнее расскажете.
Для меня главное, чтобы вы понимали общую структуру.
Так, теперь давайте я покажу вам другие тензорные сети
и на этом закончу, потому что 2 минуты и все осталось.
Вот вы могли, например, такую тензорную сеть посмотреть,
где первая соединяется с последней.
Это будут периодически граничные условия,
когда есть такое вот замыкание.
Ну, для физических задач обычно ставится,
где есть периодически граничные условия.
Ну, gz-состояние тоже можно записать в таком виде.
Нормировка матрицы плотности, это все объекты,
которые я рассказываю, например, в своем семестровом курсе
квантовой тензорной сети в Мияне.
Там тоже какое-то видео есть, поэтому...
Так, вот здесь задачка из домашнего задания.
Ну, понятно, что на семинарах уже будете разбирать,
как записать операторы в виде тензоров.
Вот там будут вот такие объекты встречаться.
Видите, ножки вверх-вниз, это означает,
что у вас теперь есть преобразование
для физических ваших систем.
То есть это уже операторы, не сами состояния,
а можно еще операторы записывать в виде произведения матриц
MetaX Product Operator.
Такие есть тензорные сети.
Я сейчас покажу вам еще другие тензорные сети.
Италии, которые не нужны.
Вот есть древесные тензорные сети.
MPS является частным случаем этой древесной тензорной сети.
То есть линейный граф.
Вот видите, вот так вот, если пройдете,
то это есть частный случай этой древесной тензорной сети.
Значит, есть квантовая ограниченная машина Больсона,
где есть два уровня.
Вот V – это visible layer.
Видите, тут есть физические индексы.
Например, ваши кубиты, индексы кубит.
А есть hidden layer.
То, что вы не видите.
Но каждый из кубитов осуществляет корреляцию с другим кубитом
посредством этого hidden layer.
Вот такие тензорные сети есть.
Активно изучаются.
Потому что они похожи на нейронные.
Значит, есть состояние проецированных перепутных пар.
По-английски projected entangled pass states.
Это вот такая решетка,
где физические индексы вот здесь находятся.
Это для двумерных всяких физических систем с корреляцией.
Есть еще другая тензорная сеть,
которая называется Mera.
Сейчас я вам ее расскажу.
И это будет конец.
Давайте с этого слайда начнем.
Вот здесь W – это изометрия.
Как сделать изометрическую матрицу?
Вы можете взять унитарную матрицу.
Вот эту W унитарную.
Ой, просто W унитарную.
И вырезать из нее столбцы или строчки.
Соединяя с вот такими вот
фиксированными значениями кубитов 0,
вы получаете изометрическую.
Вот это первое. Понятно, Ильич?
Когда-то мы с вами это делаем,
когда представление ставится?
Смотрите, что вы можете на квантовом компьютере реализовать.
Вы можете на квантовом компьютере реализовать такую схему.
Вот у вас 0, 0, 0 – это вспомогательные кубиты.
И вы дальше можете начать их перепутывать.
То есть зеленые – это унитарные некоторые вентели W,
а вот это синенькие – это какие-то унитарные вентели Q.
То есть вот эту вот схему можно на квантовом компьютере реализовать.
Квантовый компьютер.
Какую вы в итоге получите тензорную диаграмму?
Вы получите…
А где же она? Я не вижу.
Я не нарисовал.
Ну ладно, тогда здесь я остановлюсь.
Вот такую вот тензорную диаграмму получите.
Чем она замечательна?
Она замечательна тем, что, видите, внизу у вас много кубитов.
Много кубитов.
Вот тут, вот на этом срезе у вас число кубитов во много раз меньше.
Потом на следующем срезе у вас число кубитов еще меньше.
На самом деле их становится экспоненциально меньше.
То есть надо делить, например.
Разделите число на три – получите здесь.
Разделите еще на три – получите число тут.
И вот так вот.
Некоторое грубленное описание.
То есть получается, что с помощью вот такой схемы можно описывать много кубитов.
Много кубитов описывать.
Используя логарифм от этого числа кубитов вентиля.
И такое представление называется multi-scale многомасштабное.
Почему?
Потому что вот эти стрелочки показывают вам переход к грубленному описанию на другом масштабе.
Ну вот такая еще тензорная сеть есть, тоже сейчас популярная.
Больше ничего не могу сказать.
Время вышло.
В общем, тензорная сеть – это интересная штука.

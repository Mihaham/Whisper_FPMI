Давайте начинать. Я напоминаю, что в прошлый раз я начал доказывать теорему о явной
конструкции пласоновского процесса. Давайте мы ее завершим как следует. Я напоминаю,
что это теорема о том, что вот такой процесс премум n таких, что sn или сумма k равно от единицы
до n ксикатах больше либо равно t, что вот такой процесс как функция t, случайная функция t,
где ксикаты показательные, независимые в совокупности с одним и теми же параметром лямбда,
что этот процесс является пласоновским процессом. И что мы делали, чтобы это доказать? Мы
проверяли свойство пласоновского процесса просто по определению. В 0 яо 0, ну просто по
построению мы считаем, что если t равно 0 пусть t будет равно 0. Почему? А, меньше либо равно, да?
Меньше либо равно, спасибо. Вот x от 0 равно 0. Вот и нам нужно проверить эти два других сложных
свойства про независимость в совокупности и то, что пласоновское распределение имеет при
рощении этого процесса. Что мы делали? Мы с вами записали вектор из вот этих сумм, выяснили как
он распределен, получили функцию плотности для него и выразили вероятности при рощении x через
эти s. Ну и получили там огромный интеграл. Давайте я снова получу, значит как там было x от t равняется
k1, x от t2 минус x от t1 равняется k2 минус k1. Вот, ну и так далее e до n, давайте напишу, значит до
xtn минус xtn минус 1 равняется kn минус kn минус 1, где k не отрицательные tl и e такие, что k1 меньше
либо равен чем k2, меньше либо равен чем k3 и так далее. Вот такую вещь мы записали и мы выяснили,
значит выразив эти при рощении через s, что это будет вот что. Ну я не буду там все прям повторять,
значит мы дошли вот до чего? До интеграла tn да плюс бесконечности, значит мы выразили там самую
последнюю из x, e в степени x kn плюс 1, плюс 1 прибавляется kn. Вот это наше переменное интегрирование
x kn плюс 1. Там еще у нас было лямда в степени kn. И дальше был такой интеграл многократный
от следующего, значит мы интегрируем по x, по множеству каком x1 и так далее. Давайте
сейчас я сначала запишу, а потом вы можете записать, ну или проверьте как я там писал
на прошлой лекции. По-моему вот это принадлежит значит от 0 до t1, вот это множество, потом пересечь,
как это написать t. Ну можно кстати фигурные скобки не рисовать, не важно. Иксы эти принадлежат вот
этому интервалу xk1 плюс 1 и так далее до xk2 принадлежат интервалу от t1 до t2. Ну и так далее
до xk наверное n минус 1 плюс 1 до xkn принадлежит от tn минус 1 до tn. Скорее всего так,
проверьте пожалуйста. Ну вроде так. Ну у нас там была еще одна, но вот она выразилась вот в этот
интеграл отдельный. И здесь у нас стоит индикатор того, что x, ну можно так, не важно, 0 меньше x1,
меньше x2, меньше и так далее и вот вообще все они xkn. Вот такой индикатор, где все x упорядочены.
Здесь мы просто пишем dx1 и так далее по dxkn. Вот такой интеграл. Ну и я много говорил на прошлой
лекции по поводу того, как это вычисляется. Страшно выглядит, но тут все очень тривиально на самом
деле, потому что нам нужно посмотреть просто на то, что из себя представляет эта область и так
получается, что если эти x лежат здесь, вот все они больше чем tn минус 1, значит все эти x подавны
меньше, чем эти x. Поэтому соответствующие знаки меньше из этого индикатора можно убрать просто
потому что это ничего не изменит. Мы интегрируем по такой области. Поэтому вот этот индикатор мы
представим как произведение индикаторов, что эти упорядочены, умножить на индикатор,
что эти упорядочены, умножить на индикатор и так далее, что эти упорядочены. Вот и все. Так что
давайте равно, вот здесь я продолжу. Первый интеграл здесь у нас получается не забудьте,
e в степени минус лямбда tn на лямбда в степени kn. Вот это я первый интеграл посчитал. И умножить
на произведение интегралов, потому что все индикаторы они расщепились, x все независимы,
поэтому этот многократный интеграл будет уже произведением интегралов по каждой из этих
областей. Ну вот сколько у нас этих областей? Видите 1, 2 и так далее n, n штук областей у нас.
Пусть будет g от единицы до n. Вот интеграл, вот тоже многократный интеграл, но вот это какой-то
вот такой значит x. Так сейчас мы посмотрим g и тогда нет x. Вот какой тут x к g. Сейчас напишем x
к g минус 1 плюс 1 до x к g. От чего? От индикатора того, что x к g минус 1 меньше и так далее,
меньше чем x к g. По-моему что-то такое должно получиться, x к g минус 1 и x к g. Ну вот и мы
значит теперь свели всю задачу к расчету вот такого интеграла. Значит если здесь что у нас
принадлежит t g минус 1 до t g. Значит если у нас нет этого интеграла, то это просто, если у нас нет
этого индикатора, то этот интеграл это просто объем вот этой области. А это прямоугольник,
то есть вот это минус вот это в степени получается k g минус k g минус 1. Ну а здесь у нас x упорядоченные,
а это множество в этом параллелепипеде, значит многомерном, это симплекс и объем его известен.
Это нужно взять объем всего пространства, то есть всего этого прямоугольника или параллелепипеде,
не знаю как это назвать, и разделить на факториал размерности пространства. То есть это будет t g
t минус t g минус 1 в степени k g минус k g минус 1 разделить на факториал размерности пространства k g
минус k g минус 1 факториал. Вот и все. Получаем e в степени минус лямбда t n лямбда в степени k n произведение g
равняется к единице до n. Значит от t g минус t g минус 1 в степени k g минус k g минус 1 и разделить на
k g минус k g минус 1 факториал. Вот объем параллелепипеда разделить на факториал размерности
пространства. Вот этой степени, которая здесь стоит. Вот. Ну, по-моему, вот так. Так, хорошо. Ну,
теперь нужно просто это выражение немножко переписать, потому что мы-то чего хотим? Мы хотим
показать, что вот эта вероятность, которая здесь стоит, она расщепляется на произведение
вероятностей. Вот. То есть надо представить все вот это в виде одного произведения. Вот давайте мы
попробуем это сделать. Произведение по g равном от единицы до n. Вот. И как мы это сделаем? Ну,
смотрите. t n это на самом деле t n минус t n минус 1 плюс t n минус 1 минус t n минус 2 и так далее.
То есть телескопическая сумма. Так далее до t 0. А t 0 у нас равно 0. Так что мы можем записать это
так. Е в степени минус лямбда. А t. Вот. И мне надо, чтобы как здесь было. t g, t минус t, g минус 1. Так. Вот. Так что,
если я просуммирую, t 0 я считаю нулем, то я получу в точности вот это. Так. Вот. Хорошо. Теперь насчет
лямбда. Тут мы сделаем то же самое. Значит k и n. А то тоже k и n минус k и n минус 1. Плюс k и n минус 1
минус k и n минус 2 и так далее. До k 0. А k 0 у нас тоже равно нулю. Так что вот эту лямбду давайте
мы внесем с вами вот в эту скобку. Чтобы все было красиво и симметрично. Значит у нас получается
лямбда умножить на t g минус t g минус 1. Вот такая вот скобка в степени k g минус k g минус 1. Вот видите,
если мы отсюда лямбду будем выносить, она вынестся вот этой степенью, возьмется произведение там
телескопическая сумма и будет лямбда в степени k и n. Ну а вот это оставим без изменения. Ну вот и все.
Вот наше выражение. Вот наше произведение. И теперь просто по индукции мы можем начать с n
равная единице. Значит все, что я здесь написал верно для любых n, больше либо равно 1. Если мы
возьмем n равно 1, мы получим, что вероятность xt1 равняется k1. Давайте я прямо ее напишу,
чтобы вы явно увидели. Вероятность kt1 равняется k1. Ну берем n равняется единице. Что мы получаем?
Надо взять, давайте сюда посмотрим лучше, g равно 1. Здесь получается e в степени минус лямбда t1,
а это будет t0 равно 0. Здесь лямбда на t1 в степени k1 разделить на k1 факториал. Вот это
Пуассоновское распределение. То есть k от t1 имеет Пуассоновское распределение с параметром
лямбда t1. Как и надо, пожалуйста. Ну и теперь если мы возьмем n равняется 2, то мы получим,
что вот эта вероятность равна произведению двух вещей. Первая из которых это вот это, но мы
выяснили, что она равна вот этому. А вторая это некое выражение, некая вероятность,
которая представляет собой Пуассоновское распределение для лямбда от t2 минус t1.
Вот. И это будет равна вероятности вот этого события. Так что просто по индукции мы получаем,
что вот эта вещь равна вероятности каждой из этих событий по отдельности. Ну просто начиная с n
равная единице. Для n равно 1 мы нашли вероятность этого события, мы его знаем. Рассмотрим, n равняется 2,
у нас два множителя. Первый из которых мы выяснили, чему он равен. Значит, второй равно вот этому.
Вот. Так что по индукции мы получаем, что действительно эти события независимы в совокупности. Это первое.
А второе, они распределены по Пуассону. Вот потому что мы видим все эти выражения. Они
имеют Пуассоновское распределение. Вот. Ну еще можно так на это посмотреть, если вы хотите найти
вероятность каждого из этих превращений по отдельности. Но это означает, что вам нужно по
остальным аргументам суммировать. Вот. От нуля до плюс бесконечности. Вот. И тогда у вас получится
вероятность только одного превращения. Если вы это примените вот к этой формуле, то вы получите
снова Пуассоновское распределение. В общем, тут по-всякому можно об этом думать. Вот. В общем-то,
на этом все. Теорема доказана. Вот. Мы получили. Действительно, что это есть. И смотрите,
вот еще один важный момент и в задачу, когда решаете, когда от вас просят доказать, что какие-то
случайные величины независимы в совокупности, то это значит, что вы должны доказать, что вот такие
вероятности расщепляются на произведение вероятностей. Вот. Именно это вы должны доказывать
честно, аккуратно, формально, а не обращаться там какой-то интуиции, что-то. Давайте посмотрим,
могут ли они быть взависимыми. Да. Вот. Рассуждения пространные и философские начинаются. Нет. Это
не есть доказательство независимости. Независимость — это термин в теории вероятности вполне конкретный,
вполне определенный. Он не связан с функциональной независимостью, он не связан с причинно-следственными
связями. Никак. Вот. Это вполне себе определенная вещь. Так что и в этой теореме я вам показал,
как это доказывается. Ну, сложно, да, но тем не менее. Зато мы все сделали как надо. Взяли эту
вероятность и показали, что она расщепляется. Вот. Хорошо. Теорема о явной конструкции доказана.
Теперь для нас плацоновский процесс — это неважно, что, неважно, как мы его определяем. Мы его
могли определить как оксиоматически, а мы могли, в принципе, определить случайно плацоновский
процесс вот так. То есть изначально говорить будем называть вот этот процесс плацоновским. Это
неважно, потому что мы показали, что у них семейства, конечно, мерных распределений совпадают,
что одно есть другое. Кстати говоря, можно было бы доказывать по другому теорему. Скажем,
скажем, скажем, взять за основу определение, которое у нас было там, оксиоматическое,
и доказывать, что тогда процесс имеет вот такой вид. По-моему, можно было и так доказывать — это
другая теорема, но мы бы пришли к тому же самому. Вот. Но я выбрал вот такой подход — взять вот
этой и доказать, что это то. Это неважно, неважно, как ты доказываешь, потому что в итоге ты
приходишь все равно к одному и тому же семейству конечномерных распределений. Так что не нужно
доказывать в обе стороны, достаточно доказать только в одну сторону. Как только семейство
конечномерных распределений получено — все, тебе не важно изначальная конструкция процесса.
Так, ладно. Какие следствия из этой теоремы мы можем получить мгновенные? А очень много
следствий. Во-первых, во-первых, насчет следствия. Во-первых, мы понимаем, что скачки происходят в моменты
t. В моменты времени случайные tau1 равняется s1, tau2 равняется s2 и так далее. Вот. Потому что когда
t, если, ну давайте снова я напишу, xt, н такое, что sn меньше либо равно t. Вот, когда у нас t
увеличивается, вот оно превзошел через какой-то sn, у нас n сразу поднялось, да, и xt поднялся на
единичку. Так что вот эти вот s — это наши моменты времени, когда процесс испытывает скачок. А как
распределены эти моменты времени мы знаем, потому что sn — это есть сумма показательных независимых
случайных величин с одним и тем же параметром. Так что вот эти tau, n — это есть распределение
рланга с параметрами n лямбда. Сумма показательных независимых случайных величин, n штук с
параметром лямбда. Так что мы получаем из этой теоремы сразу же, как распределены моменты
времени, когда происходят скачки. Обратите внимание, не случайные моменты времени. Вот,
дальше что еще можно заметить, что между скачками какое время проходит? tau, n – tau, n – 1. Вот это
время между скачками. Оно случайное, это случайная величина. Но это sn, а это sn – 1,
так что это есть xn, и это показательное распределение. Значит, интервалы между
случайными скачками имеют показательное распределение, и эти интервалы не зависят
друг от друга в смысле стахастической независимости. Вот что мы с вами получаем.
Третье. Насколько может происходить скачок? Ну, давайте так вот. Возможно ли скачок на
величину 2 или 3, то есть больше чем на единицу? Вероятность того, что существует скачок,
который больше либо равен чем 2, это то же самое, что какие-то эски совпали. То есть существует n,
такие, что sn равняется sn плюс 1. То есть какие-то моменты времени совпали, и у тебя как бы на один
произошел скачок, и сразу же на другой произошел скачок одновременно. Вот. А это означает,
что существует n такое, что xn, какой-то xn, ну, xn плюс 1 получается равно нулю. Но вероятность
что xn равно нулю, оно равно нулю просто потому что xy это непрерывная случайная величина. А вероятность
вот этого события, она не превосходит, что получается, суммы вероятности событий вот таких,
а они все равны нулю. Вот. Ну, здесь счетное число событий, вероятности все равны нулю,
так что это тоже равно нулю. Здесь получается, что с вероятностью единицы скачок происходит на
единицу, он не происходит сразу на два или на три или на сколько-то. Это может произойти, то есть в
принципе это возможно, но вероятность этого равна нулю. Вот так. В принципе это возможно,
но вероятность этого равна нулю. Вот такие мгновенные свойства мы получаем для пуласоновского процесса.
Давайте я картинку такую нарисую. Что такое пуласоновский процесс? Значит, он стартует из нуля, мы выяснили.
Вот это tau1, вот это tau2, вот это tau3, вот это Erlang 1 лямбда, это экспонента от лямбда, вот это
Erlang 2 лямбда и так далее. Вот это показательное распределение, вот это показательное распределение.
Они независимы. Все вот эти вот скачки в совокупности независимы. И вот это на единицу с вероятностью единицы
происходит. Вот кусочно постоянное, не убывающее. Вот такая вот штука. Наш пуласоновский процесс.
Пуласоновский процесс можно еще записать вот в таком виде.
КАТ равняется сумма и равно от единиц до КАТ, от единиц. То есть КАТ единиц, где КАТ, это вот случайная величина с пуласоновским распределением,
наш процесс. Можно записать процесс вот таким образом. И отсюда можно получить обобщение пуласоновского процесса
То есть можно рассмотреть вот такие, давайте КАС чертуем, допустим назовем для простоты. Нет, давайте КАС.
Вот. Сумма и равна от единицы до КАТ в сиитых. То есть здесь у нас как бы единица тождественная, а здесь мы можем поставить вместо этой единицы случайные величины, какие-то независимые к сиитые.
Вот. Тогда вот такие вот процессы называются сложными пуласоновскими процессами.
Ложный пуласоновский процесс. По-английски это называется compound.
Будете где-то читать? Compound. Пуласовский процесс. Поэтому КАС. Вот. И, допустим, если взять ксиитую, то есть могут быть самые разные здесь случайные величины. Разные там они могут быть дискретны, непрерывны.
Вот здесь вот стоять. Это будет все сложный пуласоновский процесс. Ну вот, допустим, ксиитая у нас принадлежит бернулевской случайной величине от П.
Давайте мы посмотрим, что это такое. То есть это мы складываем нолики и единички. Единичка здесь будет с вероятностью П, нолик с вероятностью 1 минус П.
Это все равно, что если бы мы взяли вот этот процесс, исходный пуласоновский, и каждый из скачков мы бы оставили с вероятностью П и удалили с вероятностью 1 минус П.
Такой разреженный или просеянный получается пуласоновский процесс. Вот если мы запишем вот такую конструкцию, где-то обычный пуласоновский процесс,
оксиды независимые распределены вот так, то можно доказать, что вот этот процесс тоже пуласоновский, но у него параметр не лямбда, как у этого, а лямбда П.
То есть это тоже пуласоновский процесс с параметром лямбда П. Вот где П, это вот эта П, которая здесь стоит, а лямбда это параметр вот этого пуласоновского процесса,
исходного, какой мы только что разбирали. Вот это можно доказать. Есть одно из обобщений пуласоновского процесса, сложный пуласоновский процесс.
Ну и еще одно обобщение пуласоновского процесса, то, что называется неоднородный пуласоновский процесс, давайте я тоже это запишу.
Он определяется точно так же, как исходный, но отличие только в одном пункте.
Значит, когда мы пишем в третьем пункте kt-ks, любых t больше s, определяется как пуласоновская случайная величина.
Вот с таким параметром от s до t некоторые функции, скажем от tau до tau, где это некоторая неотрицательная функция.
Вот, ну и тогда вот получается такой неоднородный, он называется неоднородным пуласоновским процессом k и, давайте мы его обозначим.
Сейчас скажу почему. Вот, неоднородный пуласоновский процесс уже не с параметром, а с функцией лямда, а tau.
Вот простой процесс пуласона, там это функция константа, для любых tau от 0 до бесконечности равно некоторой лямбде.
Но вот можно еще рассматривать такие конструкции, где это функция.
То есть интенсивность, когда интенсивность процесса зависит от времени. Неоднородный, да, процесс получается.
Это неоднородный пуласоновский процесс.
Ниус, так что ли пишется? Или там буква o?
Вот такие два обобщения.
Так, ну вот, на этом материал, касающийся пуласоновского процесса, такой предварительный заканчивается.
Мы потом еще будем возвращаться к пуласоновскому процессу много раз, когда будем о чем-то говорить и будем развивать дальше его свойства, изучать.
И еще во втором задании тоже мы вспомним про пуласоновский процесс и еще больше свойства изучим.
А пока на этом такая логическая мысль завершается.
Итак, мы вели с вами некий процесс, задали его оксиоматически. Его математическое ожидание дисперсии равняется лямдо t.
Он не убывает кусочно-постоянно, прыжки только на единицу, интервалы распылены показательно, независимы.
Случайные моменты времени он скачет, они распылены по иерлангу.
Вот, значит, доказали явную конструкцию, вывели, выяснили, что можно было сдавать так, можно было сдавать так, как угодно.
И я вам дал вот два таких обобщения пуласоновского процесса.
На этом мы эту линию заканчиваем и переходим к новой большой теме, которая называется нормальные процессы или гауссовские процессы.
Гауссовские или нормальные
процессы.
Значит, по определению гауссовским или нормальным процессом называется случайный
процесс, у которого все конечномерные распределения являются нормальными.
Вот, одномерное распределение, двумерное распределение и так далее, все они являются нормальными.
Вот, это означает, что если вам дан гауссовский процесс x от t, то тогда вектор, который составлен из его сечений,
xtn, вот такой вектор, он имеет нормальное распределение.
Нормально распределен. Вот такие процессы называются нормальными или гауссовскими.
Значит, когда мы будем работать с нормальными процессами, нам придется
использовать множество утверждений из теории вероятностей о нормальных векторах.
Потому что, видите, мы берем сечение, это нормальный вектор, и часто нам приходится работать с векторами из сечений, то есть работать с нормальными векторами.
И я хотел вам напомнить некоторые сведения полезные из теории вероятностей, касающиеся нормальных векторов, которые нам пригодятся дальше.
И для доказательства теорем, и для решения задач.
Но первое, что я сделаю, я напомню, что называется нормальным вектором.
Значит, вектор x1, xn называется нормальным,
если его характеристическая функция имеет определенный вид.
И, ну, давайте x его большой обозначим, fix большое от s, это есть экспонента
от e, минимум единица, на μ транспонированная s, mu это вектор, s это вектор.
У s вектор столько же компонент, сколько у x здесь, n.
И минус одна вторая, s транспонированная, r, s.
Вот. Вектор называется нормальным, если его характеристическая функция имеет вот такой вид.
Mu это вектор с n компонентами, r это матрица n на n, квадратная.
Mu, это вектор математических ожиданий, как можно доказать mu,
равняется математическому ожиданию x, x это вектор.
Мат ожидание вектора по определению, это вектор математических ожиданий.
корреляционная матрица, это матрица n на n, и ее можно записать так,
это x центрированная на x центрированная транспонированная.
Вот, x центрированная это вектор, столбец,
x транспонирован это вектор строка столбец умножается на строку получается матрица не путайте с x транспонированная x которая скаляр
вот то есть это математическое ожидание что такое центрированная это вычитание мю мю на x минус мю
транспонированная
вот
ну давайте
несколько свойств
нормальных векторов вспомним
свойства
нормальных векторов
первое свойство которое говорит о следующем если вектор
x нормальный с распределением давайте мы будем так обозначать мю и r потому что видите
распределение вектора однозначно определяется
вектором мю и матрица r если ты их задал все у тебя однозначно задана
характеристическая функция вектора а значит и его распределение
ну и вот такое обозначение будем использовать для
нормальных векторов пусть вот так тогда то есть если вектор
нормальный то тогда все его компоненты являются нормальными случайными величинами
то тогда x это это есть нормальная случайная величина с
математическим ожиданием мю и то я и то я компонента вот этого вектора мю
запятая r и то я и то я и ты диагональный элемент матрицы r
вот если вектор нормальный то все его компоненты имеют нормальное распределение
обратное неверно
если случайный вектор
если у случайного вектора все компоненты нормальные это не значит что вектор нормальный
вот есть контр примеры к этому можете найти в книжке
натан гус гробачев по теории вероятности пример там такой рассмотрен но
если все компоненты вектора
независимы
совокупности и
являются нормальными то и тогда вектор из них
является нормальным случайным вектором
вот
если вектор нормальный все компоненты нормальные если компоненты нормальные то не обязательно вектор нормальный вы если они независимы совокупность тогда вектор норм ay
все
едем дальше второе
если у
нормального вектора вот это матрица r
обратима
она может быть необратима, но если она обратима, то
тогда у случайного вектора x существует плотность.
Если существует r-1, то существует плотность вектора x.
fx от x, это есть единицы разделить на 2p в степени n пополам
корень детерминант r на экспонентал от минус, здесь
2, здесь получается x-μ транспонированная, r в минус 1 на x-μ.
Для любого x из r, n.
Вот так, по-моему.
Вот, если она обратима.
В общем случае корреляционная матрица не отрицательно
определена, то есть у нее детерминант может быть
равен нулю, но если она положительно определена
или это равносильно тому, что если существует обратная
у корреляционной матрицы, то тогда существует плотность
и она выражается вот по такой формуле.
Вот, но если эта матрица не обратима, если не существует
r в минус 1, то тогда существует некоторый вектор c, не нулевой,
такой, что rc равно нулю.
И отсюда можно доказать, что тогда, что здесь это
означает, что у нас столбцы или строки линейно-зависимы,
можно доказать, что тогда и компоненты x-а линейно-зависимы.
Вот давайте мы это покажем.
Да, c это вектор.
Да, да, по компонентам каждая.
Или норма его не равна нулю, ну, собственный вектор.
Вектор не равен нулю, это означает, что норма c не равна нулю.
Вот так.
Вектор равен нулю, если все его компоненты равны нулю.
Так, значит существует c не равен нулю, такое что?
Значит, что отсюда следует?
Если rc равняется нулю, давайте мы слева умножим
на c транспонированное.
Мы получим ноль, да?
Ничего не изменится.
星 транспонированное cH 같ается нулю.
Мы просто c транспонированного множили на нулевой вектор.
Получим опять же ноль.
Теперь распишем такое r, c транспонированное, мат
ожидания x центрированная, x центрированная, транспонированная
на c, равно нулю.
Вот мы r расписали.
можем это делать, потому что c это константа, это не случайная величина.
Получаем c транспонированная, x центрированная, x центрированная
транспонированная c равно нулю. А здесь написано нечто транспонированное умножить
на это нечто. То есть получается мат ожидания x центрированная
c транспонированная на x центрированная c равно нулю.
То есть мы транспонирование берем, c транспонирования на вот это транспонирование это x центрированная.
Это мы просто переписали. Ну а это означает что математическое ожидание
x центрированная t ц в квадрате равно нулю. Отсюда следует что мы имеем дело
случайной величиной, не отрицательной она возводится в квадрат, но мат ожидания,
которая равно нулю, ну некуда деваться, тогда она почти наверно должна быть равна нулю.
х-центрированная Тc равно нулю почти наверно.
Вот. Это означает, что линейно зависимая компонент у х. С вероятностью единица
линейно зависимая компонент.
с вектором от ожидания мил и корреляционной матрицы r. Тогда для любой матрицы а,
значит, к на n, не обязательно невырожденный, даже не обязательно квадратный. А х это тоже будет
нормальный вектор, но вот с такими компонентами. Математическое ожидание умножается на а,
а корреляционная матрица будет а, р, а транспонированная. Вот видите, если мы
линейно подействуем на х, мы снова получим нормальный вектор. То есть,
линейные преобразования не выводят нас из нормальности, даже если они вырождены,
даже если они не квадратные. Вот так. Для того, чтобы запомнить эту формулу,
где тут транспонирование, здесь или там, предлагается такое смешное манемоническое правило.
Марат. Видите, здесь. Когда ты помнишь марат, ты помнишь, что t на конце, поэтому t должна быть
здесь, а не где-нибудь тут. Марат. Или а марат. Вот так, чтобы запомнить эту формулу.
Так, дальше едем. Еще один вектор. Значит, вот это свойство, оно доказывается просто по
определению. То есть, у нас есть вектор х, значит, у него есть характеристическая функция некоторая,
и мы рассматриваем вот этот вектор ax. И если мы запишем его характеристическую функцию,
мы увидим, что это характеристическая функция случайного вектора, нормального,
вот с такими параметрами. Все. То есть, это чисто дело техники. Доказывать я это не буду,
но вы можете этим пользоваться всюду. Так, четвертое. Еще одно очень важное свойство,
в решении задач тоже очень помогает. Вектор х является нормальным тогда и только тогда,
когда любая линейная комбинация его компонент является нормальной случайной величиной или
константой. Вот как. То есть, х нормальный вектор, когда и только тогда, когда для любых ситх из r,
сумма ситая и кситая это нормальная случайная величина, ну или константа. Ну, кстати говоря,
вот эту добавку или константа иногда не произносит, подразумевая под этим то,
что если это константа, то ее можно интерпретировать как нормальную случайную величину с нулевой
дисперсией. Вот. Так что иногда вы можете встретить такие утверждения, где не говорится или константа,
но как бы мы все понимаем, что это значит, что если это константа, значит, что как будто
нормальная, но с нулевой дисперсией. Вот. А так вот, если быть до конца строгим, то надо
добавлять или константа. Ну, кстати говоря, и здесь тоже, когда мы делаем какие-то линейные
комбинации, некоторые из этих компонент айкса, они могут быть равны нулю, это не совсем нормальная
случайная величина, но если ты под константами понимаешь случаи, когда дисперсия равна нулю,
то как бы правило вот такое, оно общее остается. То есть, это очень удобно в этих случаях так считать.
Значит, это доказательство я тоже не привожу, и оно тоже несложное через характеристическую
функцию. Вот делается. Можете Ширяева, например, посмотреть, как доказывается, но этим вы тоже
можете пользоваться. Так. Дальше идем. Еще одно важное свойство. Пять. Что компоненты нормального
случайного вектора некоррелируемы тогда и только тогда, когда они независимы. Вот. Это очень важное
свойство, но при этом студенты допускают здесь ошибки. Смотрите, здесь очень важно то, что если
компоненты нормального случайного вектора некоррелируемы, тогда они независимы. Вот если
просто случайные величины нормальные и некоррелируемые, то тогда они могут быть
зависимыми. А здесь речь идет про компоненты. Быть компонентой нормального вектора это более
сильное свойство, чем просто быть нормальной случайной величиной. Обратите на это внимание.
То есть, если компоненты нормального вектора некоррелированы,
следовательно, они независимы.
Но из независимости следует некоррелируемость для любых случайных величин,
не только для нормальных векторов, компонентов и величин.
То есть, из независимости всегда следует некоррелируемость.
А из некоррелируемости независимость следует не всегда.
Вот оказывается, что для компонент нормального вектора это верно.
Вот это вот следствие.
Так, дальше едем.
Еще одно свойство – проусловное распределение.
Ну, это вам должно было быть доказано в теории вероятности даже.
В принципе, это тоже чисто дело техники.
Идейно там практически ничего нет.
Шестое.
Значит, пусть дан вектор Xi это.
То есть, это вектор.
И вот это тоже вектор.
Этот с n компонентами, этот с m компонентами.
Это нормальный вектор.
Пусть это нормальный вектор.
Вот.
Тогда условное распределение Xi при условии на эту.
Тоже имеет нормальное распределение.
Сейчас запишу, какое именно.
Ну, вот пусть Xi это нормальный вектор.
С математическим ожиданием mu Xi.
И корреляционной матрицей r Xi Xi.
Пусть это нормальное распределение имеет с мат ожиданием mu это и r это это.
И пусть существует r это это в минус первой.
Пусть матрица вот эта обратима.
На r Xi все равно.
Но пусть вот эта матрица обратима.
Вот.
Тогда вот такое выражение известно.
Но оно большое, я его отсюда перепишу.
Что условное распределение вот при таком условии.
Это тоже нормальный случайный вектор.
С математическим ожиданием mu Xi.
Плюс r Xi это.
Это корреляционная матрица взаимная для векторов Xi это.
Ну, я сейчас напишу.
Вот.
На r это это в минус первой.
На x минус.
Mu это.
Запятая.
Здесь будет у нас стоять r Xi Xi.
Минус r Xi это.
На r это это в минус первой.
На r это Xi.
Вот.
А что такое r Xi это?
Это значит у нас вот этот вектор.
Он имеет пусть распределение нормальное.
Вектора математических ожиданий mu Xi mu это.
И матрицей корреляционной вот такой.
Она блочная.
Значит здесь r Xi Xi.
Здесь r это это.
Здесь у нас будет стоять r Xi это.
r это Xi.
Вот.
Вот тогда это вот эта штука.
Так.
Правильно я там расставил?
Да, правильно.
Ну вот.
Ну и выводить я эту формулу для векторов не буду.
Я просто скажу вкратце примерно откуда она берется.
Вот смотрите.
Допустим для простоты у нас Xi и это сколяры.
Не вектора, а сколяры.
Тогда они являются компонентами нормального вектора.
А это означает, а мы знаем свойства.
Пятое, что если компоненты нормального вектора не каллерируемые, значит они независимы.
Таким образом мы можем посмотреть вот на эту вероятность следующим образом.
Вероятность того, что Xi, допустим, меньше какого-то y при условии, что это равняется x.
Вот как вычислить такую условную вероятность?
Давайте мы заметим, что это равно вероятности того, что Xi минус a это меньше, чем y меньше a это.
А это равна x.
Ой, меньше y минус ax при условии, что это равняется x.
Вот мы можем записать вот так.
В итоге здесь стоит некая величина меньше некоторого числа при условии вот этого события.
Это зависит только от это, а эта штука зависит от такой ленинной комбинации.
Xi минус a это.
Так вот, мы можем подобрать такую a, чтобы вот эта величина и вот эта величина были не коррелируемыми.
Просто к авариации их равна нулю и оттуда найти a.
Линейное уравнение относить на просто решить.
Так как они являются компонентами некоторого нормального случайного вектора, вот такого.
Это нормальный случайный вектор, потому что он получен линейной комбинацией от исходного случайного вектора Xi.
Так как они являются компонентами некоторого нормального случайного вектора, то из их некоррелируемости будет следовать их независимость.
Так что нам остается найти a так, чтобы эти штуки были не коррелируемыми, тогда они будут независимыми.
А раз они не независимы, то вот это условие можно убрать условное.
Тогда для такого a получится вероятность вот этого меньше вот этого.
А это уже нормальная случайная величина с известными там мат. ожиданием и дисперсией и все находится.
И эти рассуждения можно провести для векторов Xi.
Ну так вот, примерно, чтобы вы себе представляли откуда это берется.
Так, хорошо.
Это я вам рассказал.
И, наконец, все вот эти свойства так или иначе у вас были.
Но сейчас я вам напишу еще одну теорему без доказательства,
которая у вас не была, но она супер полезная и в случайных процессах она очень сильно помогает.
Она называется теоремой Вика.
Или еще ее называют теоремой ИС Эрлиса.
Это какой там? Седьмой, наверное, получается, пункт.
Теорема Вика.
Она вот о чем.
Значит, пусть дан нормальный случайный вектор X с нулевой мат. ожиданием.
Это вектор. X-то тоже вектор.
Нулевое мат. ожидание и некоторая корреляционная матрица R.
Она может быть выражена, вообще говоря.
Вот.
То есть X у нас из Rn.
N-компонент.
Тогда, если n нечетно, то мат. ожидание x1, x2 и т.д. xn равно нулю.
Вот.
Даже если x-ы зависимые, мы не можем расщепить,
то мы не можем расщепить их.
То есть x1 равно нулю.
Вот.
Даже если x-ы зависимые, мы не можем расщепить
мат. ожидания произведения на произведение мат. ожидания.
Это все равно будет ноль.
Если n-четно, то мат. ожидание вот этого произведения.
Сумма произведений компонент R.
Q1 и т.д.
Здесь n пополам множителей.
Сумма ведутся по P и по Q.
Значит, где сумма берется по всем разбиениям множества.
Сумма берется по всем разбиениям множества.
1n на n пополам пар.
Звучит сложно, но сейчас я вам покажу несколько примеров,
и сразу все поймете.
И почему здесь написано ровно так, как написано.
Значит, это со звездочкой.
Я не буду доказывать эту теорему.
Она очень техническая и безыдейная.
Практически.
Смотрите.
Вот пусть нам дал вектор с четырьмя компонентами.
И как мы можем это сделать?
Смотрите.
Вот пусть нам дал вектор с четырьмя компонентами.
x1, x2, x3, x4.
Любой его подвектор это и есть нормальный вектор,
потому что он может быть получен как линейная комбинация.
Вот.
Тогда смотрите.
Математическое ожидание x1 равно 0.
Но мы это и так понимаем, потому что нам дали такой вектор x.
Ну мы от ожидания x1, x2, x3 тоже равно 0.
Даже если они зависимы,
эти x все равно будет равно 0.
А почему мы пишем равно 0?
Потому что их здесь нечетное число стоит.
1, 2, 3. Их нечетное число.
Вот.
Теперь рассмотрим от ожидания x1, x2, x3, x4.
Что нам надо сделать?
Нам надо разбить множество 1, 2, 3, 4
неупорядочным образом на пары неупорядочные.
Как мы можем это сделать?
1, 2, 3, 4.
1, 3, 2, 4.
1, 4, 2, 3.
Пишем r1, 2, 3, 4.
1, 3, 2, 4.
1, 4, 2, 3.
Все.
Вот.
Берем вот это множество и делим его на пары.
Понятно?
Вот так вот на примере.
В чем смысл говорить неупорядочные?
Потому что мы не различаем
разбиение.
Мы разбиваем вот это множество.
Мы, например,
не различаем вот такое разбиение
и вот такое разбиение.
Здесь мы можем написать не 1, 2, а 2, 1.
Но сюда не входит слагаемое,
кроме вот этого,
сюда не входит слагаемое r2, 1, r3, 4.
Нам не нужны упорядочные разбиения.
Их было бы больше.
Больше слагаемых было бы.
Нас интересует только неупорядочное.
Один раз получили 1, 2, 3, 4.
Все. Это мы считаем, что разбили.
То есть вот такими мы считаем,
что это одно и то же.
И точно так же, например,
3, 4, 1, 2.
Вот.
Мы считаем, что это тоже ничего
нам нового не дает.
В этом смысл вот этих неупорядоченностей.
Да.
Вот.
Это тоже можно,
потому что сейчас я вам продемонстрирую.
Я же никак не сказал о том,
что эти иксы обязательно разные.
Я даже сказал,
что они могут быть зависимыми между собой.
И равенство вполне себе допускается.
Потому что, например,
если вы хотите вычислить
вот такую величину,
ну, как на это можно формально
так вот посмотреть?
Как будто у вас есть вектор
x1, x2, x3, x4.
Он нормальный.
Да. Допустим.
Но вы же можете из него
линейной комбинацией получить
x1, x1, x1, x1.
Матрицу можно такую придумать,
что при умножении на это получится
вот такой вектор. Значит, это тоже
нормальный случайный вектор.
Вот. Так что вы можете
посмотреть на него как на y1, y2,
y3, y4.
У него нулевые мат ожидания
и некоторая матрица корреляции.
Так что вы можете применять
эту теорему для вектора y,
но потом от y перейти от x к вот этим
штукам. Так что
равенство каких-то компонентов вполне
себе допускается.
И что это будет? Смотрите, как можно
написать. Мат ожидания x1,
x1, x1,
x1. То есть мы должны
разбить. То есть мы смотрим на это как бы
на 1, 2, 3, 4.
И мы пытаемся разбить это множество на пары.
Мы должны взять вот эти и вот эти.
Вот эти, вот эти, вот эти,
вот эти. Так что получается,
когда мы возьмем эти, мы получим
r11
и умножить на r11.
Плюс вот эти теперь.
Это тоже r11. Вот на эти
r11. Плюс
теперь вот эти две.
r11 и внутренние две r11.
Ну что, теперь бывает.
Ну и получается это 3
r11 в квадрате.
Вот. А r11
это
диагональный элемент матрицы
корреляции. Это дисперсия
первой компоненты. И она еще
в квадрат возводится. Значит получается
3 сигма 1 в четвертой степени.
Вот.
Вот вычислили.
Вот такая теорема замечательная.
Несколько слов скажу примерно откуда
это все берется.
Значит, без доказательства.
На самом деле все это, все это очень
легко. Может вообще показаться очень
странным, да? Почему?
Моменты, произвольные
моменты.
Разной степени. Если там повторяются
какие-то тексты, то можно там вычислять
типа мат ожидания x1 в пятый
на x2 в шестой,
на x3 в седьмой.
Если сумма всех степеней нечетная, сразу
вычислишь ноль все. Вот.
А если четная, ну тогда вот по этим формулам
все вычисляется. И все выражается через
вторые моменты, через корреляционные
моменты. А почему? А это очень просто.
Потому что характеристическая
функция нормального вектора,
давайте мы ее вспомним,
это что такое? Когда mu равно нулю.
Это экспонента
от минус одна вторая, транспонированная
rs. Вот.
И
произвольные моменты,
как вы знаете из теории вероятности, они
связаны с частными производными вот этой
функции в нуле. Например, вот эта штука,
она там выражается как что-то
там, я не помню какой коэффициент,
но это неважно, d в степени
n на phi
x в нуле
по ds1, ds2
и так далее, по dsn.
Ну и за счет того, что здесь
стоит квадратичная форма,
то есть она как бы степень,
у s здесь четная, то когда ты берешь
нечетную, нечетного порядка
производную, при s равно ноль,
то ты получишь ноль.
Вот. А когда у тебя
s четная, то вот эти
производные могут дать по
четному числу s, могут дать в принципе
не нули. Ну вот они и дают как раз вот такие
вот произведения, которые
зависят от r. А почему они только от r зависят?
Потому что функция характеристическая только от r зависит.
Видите?
Функция вектора однозначно
определяется r-кой. У
гауссского векторов однозначно определяется r-кой.
Значит и все производные определяются каким-то образом
через эту r-ку. Вот и все.
Вот и получается, что произвольный момент
зависит от этого r.
Вот. То есть вот
это полезно понимать,
откуда берется. Ну эта теория, она
очень техническая, и доказывать я ее не буду.
Вы можете ей пользоваться.
Так, все. На этом
я заканчиваю
разговор про свойства нормальных
векторов.
У меня еще есть
немножко времени.
Давайте я введу
новый для вас процесс
очень важный,
который мы будем тоже работать потом
очень долгое время.
Винеровский процесс.
Это очень важный представитель
гауссских процессов.
То есть перейдем уже непосредственно к процессам
от нормальных
векторов.
Значит, винеровский
процесс
он определяется
следующим образом.
Очень похожим образом, как
определяется полосунницкий процесс.
Определение
винеровским
процессом.
Его еще иногда называют процессом
Броуновского движения.
Винеровским процессом
W A T
называется
случайный процесс
случайный процесс
со свойствами.
Тоже три свойства.
Как было для
полосунницкого процесса.
В нуле он ноль, почти
наверное.
Второе W A T
это процесс
независимыми
приращениями
так же, как и полосунницкий процесс.
А третье
для любых T и S
больше либо равных нулю
не обязательно упорядоченных между собой,
как для полосунницкого процесса.
W A T
имеет нормальное распределение
с нулевым от ожидания
и дисперсии равные модуль T-S.
Иногда здесь пишут
сигму в квадрате.
Тогда это будет винеровский процесс
с параметром сигмы в квадрат.
Нам это нигде не пригодится, поэтому я его опускаю.
Хотя можно было бы
определить такой процесс.
Вот, это определение
винеровского процесса.
Откуда это взялось?
Значит,
где-то в XIX веке
был такой ботаник
английский Brown.
Значит, он
обнаружил, что
в жидкостей
какие-то маленькие частички, они
трясутся под микроскопом.
И потом, в начале
XX века, Эйнштейн это связал
с тем, что
вот эта маленькая частица,
подвешенная в жидкости, она трясется,
потому что о нее ударяются молекулы
с разных сторон.
И впоследствии,
и координаты вот этой частицы,
которая трясется в жидкости,
это можно описывать
как случайный процесс.
И вот Винер был одним из тех,
кто глубоко изучал свойства
этого процесса.
Математически это все описывал, изучал, исследовал.
И какая тут мотивация
стоит для того, чтобы записать
все вот эти свойства.
Вот представьте себе жидкость, правда,
не трехмерную, а одномерную.
И мы будем
под X от T
понимать координату
частицы
на этой
координатной оси.
В зависимости
от времени T. Время идет, частицы
как-то движутся по
этой оси.
Ну и мы будем
считать, что
положение
этой координаты
определяется
очень большим
множеством соударений
этой частицы с какими-то
другими мелкими частицами, которые с
равной вероятностью ее толкают то влево, то вправо.
И если мы будем считать, что в начальный момент
времени частица находится в нуле,
вот оно, видите,
наше первое свойство,
если мы будем считать, что
то, насколько
сместилась частица
на этом интервале, не зависит от того,
насколько она сместилась
на других интервалах, потому что
где бы она ни была, в любой момент времени
на нее одинаково давят как слева, так и справа,
и ситуация не меняется.
Так что это
разумное предположение, считать, что
вот эти тоже независимы в совокупности.
И если мы будем считать, что
вот это приращение нормальное,
а почему мы считаем, что это
нормально? Потому что это приращение,
то, насколько частица сдвинула за этот интервал
времени, определяется большим числом
взаимодействия с ней, суммой большого числа
случайных величин.
А по центральной и предельной теоремии
хорошее приближение для нее
это нормальное приближение.
И если наконец мы будем считать, что
это приращение зависит
только от разности момента времени,
если ты прибавишь сюда h
и сюда прибавишь h, ничего не поменяется,
что она на этом интервале?
Случайная величина, насколько
она сместилась, не зависит от момента времени.
Главное, она зависит от величины
этого интервала времени, но не от того,
в какой момент времени ты рассматриваешь это приращение,
то есть стационарность,
называется, то тогда ты придешь просто вот к этому определению. То есть опять же
очень естественные свойства. Начинаем из нуля и максимальная симметрия во
всем. Значит, независимость от интервала времени и стационарность, однородность,
все. То ты приходишь просто вот к этому определению. Потому что он тоже очень
естественный, что для Пуассоновского процесса, что здесь. И, кстати, обратите
внимание, что этот процесс, что Пуассоновский процесс, они получаются как
результат взаимодействия чего-то большого в малом интервале времени. Когда мы раз
говорили о Пуассоновском процессе, там, значит, было много редких событий, которые
редко происходят, но их много, да. Здесь тоже мы рассмотрим частицу, на которую
давят слева-справа другие какие-то частицы в большом количестве. И тоже мы
пользуемся какой-то предельной теоремой.
Является Гауссовским процессом. Доказательства. Берем произвольное n,
произвольные t1. Давайте мы их упорядочим, хотя это будет не очень принципиально.
Давайте мы их упорядочим и рассмотрим вот такой вектор v от t1, v от t2 и так далее v от tn.
Вот. Если мы докажем, что он нормальный, этот вектор, то тогда это будет Гауссовский
процесс. Потому что Гауссовский процесс это процессы, у которых любые
конечномерные распределения являются нормальными. То есть, по определению
конечномерных распределений, любые векторы с течением являются нормальными.
Попробуем доказать, что это нормальный случайный вектор. Ну как мы будем это
делать? Например, вот так. Нужно показать, что он является, например, линейной
комбинацией какого-то вектора, про который мы точно знаем, что он нормальный. Вот. И в
качестве такого вектора мы возьмем просто превращение v от t1 минус v от нуля, ну v от нуля
равно нулю. Вот v от t2 минус v от t1 и так далее v от tn минус v tn минус 1. Вот. Ну и какую надо
взять матрицу? Значит, единичка, нолик. А у нас даже что-то такое было уже, да? Когда мы
доказывали теорему о явной конструкции Гауссовского процесса. Но это как бы вот
эта конструкция, которая возникает, она естественна, потому что мы через
превращение пытаемся найти распределение самих сечений. Здесь у нас будут единицы, единицы,
нолик, псинолики, ну и так далее. В конце у нас будут одни единички вот тут стоять. Вот. У этого
вектора что? У него все компоненты нормальные по третьему свойству, но они все независимые эти
приращения по второму свойству. Значит, они все нормальные и все независимые. Значит, это нормальный
вектор и он умножается на какую-то матрицу, неважно какую. Значит, это тоже вектор нормальный. Конец
доказательства. Все. Классная теорема, да? Не, не, не. В том-то и дело, что неважно. Любая матрица,
хоть прямоувольная, там, не обязательно даже квадратная, чтобы была. Все равно. Вот. Ну,
а видите, мы тут взяли только упорядочные Т, но если они не упорядочные, ну переставьте сечение
здесь местами. Это все равно какая-то линейная комбинация над этими компонентами, так что никуда
ты тут не денешься. Вот. Ну, вот такая замечательная теорема. Ну, давайте я еще для порядка выпишу
некоторые числовые характеристики для Винерского процесса. Значит, математическое ожидание
Винерского процесса. Значит, w это одно сечение. w от t распредлена так же, как w от t минус w от 0,
потому что w от 0 равно 0. А это нормальное распределение с параметрами 0 модуль t.
Ну, раз там 0, значит, мат. ожидание равно 0. Дисперсия w от t равняется модулю t. Ну,
так как у нас t больше либо равен 0, то модуль можно здесь не писать, поэтому это просто t. Потому
что t больше либо равно 0 для процесса. Значит, абсолютно аналогично, как формула выводилась для
плацсонского процесса, здесь тоже корреляционная функция ts. Это есть минимум t и s. У плацсона здесь
был лямбда, а здесь нет. То, что формула похожа, это просто следствие того, что и то,
и другой процесс. Это процессы с независимыми превращениями. Вот и все. Поэтому формула похожа
и удивляться тут не стоит. Ну, что еще может пригодиться? Давайте мы знаете еще какую вычислим
штуку. Вот такую. Мод. ожидание v в четвертой от t. Нам это пригодится потом. Мод. ожидание
v в четвертой от t. Значит, смотрите, мы можем рассмотреть вектор вот такой v от t, v от t, v от t,
v от t. В общем-то, он получается как умножение некой матрицы 1, 1, 1, 1 на v от t. Так как вектор
с одной компонентой. Так что это тоже нормальный вектор. И мы для вычисления вот этой штуки можем
применить теорему вика, чтобы от ожидания у него все равно нулю. А мы, кстати, вычисляли v в четвертой.
Что мы там получали? 3r в квадрате t, t. Там у нас было r1,1 в квадрате. 3r1,1 в квадрате.
Ну вот. То есть 3t в четвертой. Вот вычислил математическое ожидание от этой вещи.
А может и t в квадрате? А, дисперсия равна t. А мы дисперсию возводим в квадрат. Да,
спасибо. В квадрате все правильно. Вот. Там модуль t, там без квадрата, да. И вот тут тоже,
видите, хоть это дисперсия, но здесь квадрата нет. Вот. А вот возводится в квадрат. Вот так вот.
Но вот это наблюдение, оно нам потом пригодится. А где именно? Ну, в принципе,
могу уже сразу даже, наверное, сказать. Чем со временем? Четыре минуты. Ну, смотрите. Да,
давайте прям сейчас сразу скажу. Это тривиально. Вот это вот я не просто так-то писал. Вот для нас
будет важно то, что Винеровского процесса, у него траектории вообще очень сложно устроены. Это
не как вот эти вот кусочно постоянные. Траектория выглядит вот так. Вот такие вот. И когда ты
увеличишь какую-нибудь ее часть, то ты там увидишь снова что-то такое вот. И как ты не увеличивай,
там все равно будет какая-то вот такая гадость. Ну, так это на бесконечности как бы. Нет,
а что такое на любом? Это ждание. Если я повторю реализацию, то она пойдет вот так вот. И когда ты
много реализаций рассмотришь, половина пойдет вверх, половина пойдет вниз, грубо говоря, да. И
оно устаканится. А то, что оно на бесконечности куда-то уходит от нуля, так это естественно.
Дисперсия растет. Хоть мы от ожидания равно нулю в любой момент времени, но дисперсия растет.
Поэтому это естественно ожидать, что она куда-то уйдет. Так вот, вот эти траектории у процесса,
они, можно доказать, что они всюду непрерывны. Почти все траектории всюду непрерывны, но нигде
не дифференцируемо. Вот. То есть это такая вот гадость. И в связи с этим такие процессы довольно
тяжело исследовать. А вот это нам нужно будет, ну ладно, это я в следующий раз, чтобы аккуратно это
все выписать, понадобится доказать, что у Винеровского процесса существует непрерывная модификация.
Помните, у нас была теорема Колмогорова на первой лекции. Есть математическое ожидание, там,
модуля некой разности, что-то там не превосходит. Вот, можно воспользоваться вот этим вот наблюдением
и доказать, что у Винеровского процесса существует непрерывная модификация. А это означает, что мы
можем работать, когда мы работаем с этим процессом, мы можем считать, что все его траектории хоть и
выглядят сложно, но все они являются непрерывными функциями. Вот. То есть, что у нас задано такое
вероятностное пространство, и на нем задан Винеровский процесс, что все его траектории непрерывные.
Вот. То есть, это прямое следствие той теоремы Колмогорова. Это будет очень удобно так считать,
что все его траектории непрерывные. То есть, существуют такие как бы, можно так определить
Винеровский процесс, что у него будут выполнять три свойства, но будут траектории разрывными
какие-то. Их будет множество мир и нуль, но как бы они могут существовать. Так вот, чисто формально
могут существовать. Но благодаря вот этой теореме Колмогорова мы можем задать вероятностное
пространство другое на множестве функций, только непрерывных. И на нем определить уже вот этот
процесс. И там уже не просто почти все, буквально все траектории будут непрерывными. И дальше,
при решении задач, мы будем всегда учитывать этот факт, что все его траектории непрерывные. Вот.
Иногда, кстати говоря, это добавляют четвертым пунктом туда. Траектории непрерывные. Но это некое
следствие вот этих уже трех пунктов. То есть, это не получается независимость как бы этой аксиоматики.
В общем, я не добавляю, а просто говорю о том, что можно определить Винеровский процесс и так и сяк.
Для удобства, для практики удобно считать, что все его траектории непрерывные. Ну все.

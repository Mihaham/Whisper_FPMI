В чем суть? Мы хотим построить какую-то структуру данных, которая будет уметь отвечать на три запроса.
Это find, то есть проверить есть ли элемент.
Insert, ставить элемент и erase, удалить.
Мы с вами уже делали похоже, это были всякие деревья поиска.
И там получили разные варианты.
Средним логорифом, амортизированный логорифом, обычный логорифом, логориф по основанию T.
А здесь мы будем целиться в еще более лучшую вещь, а именно хотим за от единицы в среднем.
В среднем, то есть в ожидании будет от единицы.
Возможно мы сегодня с вами познакомимся с штуком от ожидания, с этим словом страшно.
Вот, это наша цель великая.
И первая идея, которая предлагается вообще, раз у нас салат единицы, значит мы должны как-то сравнивать объекты с салат единицы.
Какие объекты мы умеем сравнивать с салат единицы? На равенство.
Все, только числа.
Поэтому мы будем считать, что определение, следующая пусть, у, это множество всех объектов.
Ну там не знаю, например, это строки на алфавите ABC длины до 50.
Ну допустим, какое-то такое странное множество.
Вот, тогда функция h, которая будет переводить множество u в n-1, это hash функция.
Вот, мы не знаем как она действует, мы знаем лишь, что она берет, может брать любой объект из рассматриваемых и вытащить чиселку.
Дальше, следующее определение, что пара x и y, где x не равен y, создает коллизию,
а если h от x равна h от y?
То есть, если мы нашли пару неравных ключей, у которых hash функция одинаковая, то мы считаем, что это коллизия.
Как вы можете догадаться, коллизия это плохо.
Почему это плохо? Потому что если у вас hash от x не равно hash от y, вы сразу же делаете вывод, что x не равно y.
В обратную сторону это неверно.
Собственно, за счет вот этой вот такой идеи мы будем быстро сравнивать все.
Окей, дальше что мы будем делать?
Вопрос, почему hash переводит значение от 0 до n-1?
Потому что мы умеем только числа быстро сравнивать.
Почему не?
Потому что n-чиселок всего должно быть.
Ну, программисты с 0 начинают.
Хорошо, коллизию определили. Что дальше мы хотим?
Наверное, создать какую-то hash таблицу надо.
Ну вот, какие будут идеи?
Как переводит hash?
Как hash?
Массив из n-элементов.
Да, давайте бахнем массив из n-элементов. Почему нет?
Сделаем массив из n-элементов.
Ой, зачем он то? В гулей давайте.
Наш hash таблица.
И она будет иметь размер n.
И у всех false означает.
Ну тогда понятно, что при insert можно будто бы просто сказать,
посчитай-ка мне hash функцию от объекта и что-то сделай дальше.
Ну вот, пойди по нужному индексу, поставь true, что элемент есть.
Чем такая схема плоха?
У коллизии, во-первых, мы не умеем с ними справляться.
Это раз. Два.
Но все-таки много памяти, так или иначе.
Все-таки от n-элементов неприятно.
Будем хотеть чуть меньше, обычно.
В частности, мы, наверное, хотим памяти от числа добавленных объектов,
а не от какого-то абстрактного m.
Мы бахнули вектором, какая разница?
У вас уже n-памяти выделено, это плохо.
Вы хотите, чтобы у вас, наверное, если hash таблица содержит один элемент,
она там не очень большего размера была.
Такая схема называется прямая десенсия.
Директ адресинг по-английски.
Проблемы мы уже с вами выяснили.
Первый – это коллизия.
Второй – это o от n-памяти.
Всегда.
Давайте такую модель посмотрим.
Здесь у нас все плохо.
Давайте посмотрим вторую модель.
Это вот такая.
Simple Uniform Cache.
Или просто равномерное хаширование по-русски.
Simple – это простой, Uniform – равномерный.
В чем будет суть?
Смотрите.
Мы хотим сделать что?
Нам нужно как-то разобраться с коллизиями.
Мы хотим сделать что?
Нам нужно как-то разобраться с коллизиями.
Мы хотим сделать что?
Мы хотим сделать что?
Мы хотим сделать что?
Нам нужно как-то разобраться с коллизиями.
Давайте сделаем следующее.
Будто бы у нас есть массивчик размера m.
Будто бы у нас есть массивчик размера m.
И мы будем брать, что наша h' от x – это h от x по модулю m.
Здесь мы себе позовем хрень не m элементов, а какое-то m.
Оно будет сильно меньше, чем m.
И там при необходимости мы можем как-то менять.
Если добавилось слишком много элементов, то увеличить.
Если убрали слишком много элементов, то как-то уменьшить.
Как в векторе мы делали с вами.
Здесь можно как-то динамически пытаться расширять.
И что делать с коллизиями?
Тем более их стало у вас сильно больше.
Потому что по модулю m.
М меньше m, очевидно.
Считайте.
Делать следующее.
Храним.
Здесь какие-то контейнеры.
То есть храним цепочки.
Так.
Давайте симплу сотрем.
Он нам позже понадобится в виду архитектуры.
Немного не прото.
То есть храним.
Массив размера m меньше m.
Это будет массив цепочек.
Еще их называют бакетами.
То есть мы будем называть термином бакет.
Нечто, что хранит элементы, образующие коллизию.
Или бакетов.
По-английски бакет.
То есть здесь у всех хэши равные.
И тогда, по идее, что такое поиск?
Поиск – это посчитать хэш-функцию,
пройти в нужный массивчик
и пройтись под цепочки полностью.
Аналогично вставка – это
найти нужный индекс,
пройтись в под цепочки,
если такого элемента нет, давайте конец.
Что такое удаление?
Что такое удаление?
Что такое удаление?
Что такое удаление?
Что такое удаление?
Что такое удаление?
Идея та же.
Так, допустим, вы где-то остановитесь посередине
и вам нужно удалять.
Ну где-то из середины. Вот, допустим,
элемент, которого надо сделать.
Тогда какой контейнер нам
подойдет для хранения вот таких вот цепочек?
Ну да, потому что нам нужно
из середины быстро удалять.
Так, как надо.
За от единицы
удалять
из
бакета.
А зачем нам за от единицы
удалять от бакета?
Мы же хотим вот здесь вот за от единицы
в середине все операции.
Ну это правда.
Это правда.
Ну хочется как можно быстрее
удалять, скажем так. Если мы можем
за от единицы, почему нет?
Ну чтобы лишний раз не нагружать.
Ну ладно, давайте напишем.
Не надо, хочется.
Сейчас в бакета
будем хранить
качество
списка.
Ну как вы смотрите, у вас здесь,
если вы хрените вектор, у вас синтетически
не сильно изменится константу все равно,
но вам не все все равно не нужна
индексация от этого,
которые предоставляет вам вектор.
Вот такая вот идея, что
если вы можете обойтись меньшим
множеством операций, берите наиболее
удобное.
Окей, то есть тогда мы получаем,
что по сути
время работы операции
это что-то типа
от длины цепочки.
Согласны?
Ну тогда мы наверное
хотим оценивать
в худшем случае, если мы
смотрим от длины цепочки, мы хотим, чтобы
они все были плюс-минус равные длины.
То есть чтобы наша хреш-функция раскидывала
равномерно элегента.
Логично?
То есть что у нас есть?
Наш в рецепте
лежит h
действующий из u
0 tra-ta-ta
n-1
ну только
m-1 все-таки уже.
Потому что мы берем по моду Lamp, поэтому
m-1.
И мы хотим, чтобы она равномерно раскидывала.
Ну пусть.
Мы даже не будем называть
переменную.
Теперь собственно говоря наш simple uniform
hashing прекрасный.
А в чем была суть?
Ну давайте мы для каждого объекта из u
возьмем случайно и независимо выберем отсюда
число. Из 0 tra-ta-ta
n-1.
Создадим массив размера u.
Как-то проиндексируем его элементы
может 100 u.
И поставим
каждому
уиты из u
независимо
и равновероятно.
Число
из множества
Вот такая вот идея.
Ну тогда
будто бы мы все сами построили, да?
Подождите.
Будто бы мы сами построили, но мы еще
про время не знаем что там. Мы еще не знаем
что в последнем 12 почке.
Видите, мы написали это
написали это и это вообще
никак не связано.
Пока что. Давайте цементировать.
Окей.
Пусть
l-q
это 12 почки
для ключа q.
И соответственно
что еще?
В текущий момент
в хэштаблице
ключи
k-1
k-t.
k-t.
t ключей.
Ну тогда как посчитать среднюю длину цепочки?
Ну можно и записать это так
что l-q
это просто сумма по i
от единички до t
индикаторов того, что h
от
x
там не знаю
давайте h от q
равно h от
k-i
Ну это понятно почему так?
Ну индикатор, например,
1 если true и 0 если false
условия в нем.
Тогда вы рассмотрите просто число
коллизий, то есть число элементов, которые дают вам
коллизию с вашим q.
Ну это и есть как раз 12 почки
по сути.
Все каиты различны.
Ну теперь страшное слово.
Вот ожидание
или expectation.
Скажем так, пока что я оставлю это без комментариев
вот
для первого курса особенно достойно.
Я вам пришлю
в чат
методику по тому, как понимать
что такое мотождание.
Пока что остановимся лишь на интуитивном
уровне, что
вообще не так, я просто распишу сначала формулу
потом поясню все переходы в ней.
Вот.
Это просто мотождание от такой вот интересной суммы.
Мотождание обладает интересным
свойством, что
оно линейно, то есть можно
менять сумму и мотождание местами.
В частности
ну
окей, мы сами говорили про мотождание в контексте
что если у нас есть кубик один, то он выбрасывает
в среднем 3,5, да?
Типа у вас там кубик
от 1 до 6, там
грани у него пронумерованы, тогда средний номер,
который упадет в грани, 3,5.
Вот, такое у нас же было.
Тогда если в среднем вы выбрете 2 кубика
наверное у вас в среднем 3,5
плюс 3,5 будет, то есть 7.
Что вроде бы логично.
Вот.
Детское объяснение,
то почему так можно делать.
Подробные объяснения вам дадут
это наверное
в третьем семестре или в четвертом, где у вас
мера либега будет.
Мера либега.
Ну ладно, я в мотождании напишу что такое.
Не расстраивайтесь.
Сейчас я допишу формулу.
И есть такое интересное свойство, что
мотождание индикатора это его вероятность.
Давайте оценим эту вероятность.
Почему она равна?
Ну, наверное она равна следующему.
Будто бы хочется сказать, что раз мы
убираем независимо равномерно
из вот того прекрасного множества
ноль тра-та-та м минус 1,
то наверное 1 делить на m просто, да?
Ну вот, это неправильно.
Это единичка, если
q равно k и t,
и 1 делить на m
иначе.
Ну потому что если у вас
как бы они совпали, то понятно, что
вероятность 1.
Точно будет это верно. Если они не совпали,
то тогда действительно идет
наша логика, что мы выбирали
независимо и равномерно, поэтому здесь
будет 1 делить на m.
Но это как не знаю.
Если вы бросите два кубика,
какая вероятность того, что у них будут
одинаковые значения?
Вот одна шестая будет, если что.
Можете посчитать.
Наверное.
Ну да, 6 пар, что у вас совпадет,
6 квадрат пар, что всего.
6 делить на 6 квадрат,
1 делить на m получаем.
Окей.
Ну тогда можно оценить эту штуку.
1 плюс
t минус 1 делить на m.
Понятно, откуда эта оценка берется?
То есть у вас, допустим,
здесь есть одно равенство,
это единичка, и тогда у вас
t минус 1 не равенство,
в любом случае.
Потому что у вас все каиты различны.
А почему мы равенство не поставим?
Ну...
Нам достаточно неравенство здесь,
скажем так.
Можно равенство поставить.
А, равенство здесь мы не можем поставить.
Знаете почему? Потому что вдруг у нас q не равно
никакому каитам.
Тогда это было бы просто t делить на m,
а это меньше.
Поэтому здесь неравенство стоит.
И для красты
минус 1 уберем.
Окей.
Так-так-так.
Там стиратка.
Так сказать,
наша цель перед нами,
как видите,
мы уже близки к успеху на самом деле.
Мы уже близки к успеху на самом деле.
Почему мы близки к успеху?
Потому что мы оценили,
что для на цепочке,
в среднем, она меньше
1 плюс t делить на m. Согласны?
Определение.
t деленное на m,
где
t это
размер х-таблицы
Развер в плане
так напишем,
число элементов х-таблицы.
То есть то, что вам вернуло метод size
в х-таблице
а m
это число бакетов
макетов
называется
коэффициентом
загрузки
ну или загруженности.
Тут как бы
в этой теории обычно все говорят на английском.
На английском это load factor называется.
Ну тогда
смотри, что мы делаем.
Зафиксируем
c больше 0
и будем поддерживать
m так,
чтобы
alpha
t деленное на m
было столько меньше c.
То есть когда вы
подобавляли туда элементов кучу,
вам рано или поздно придется
расширяться,
чтобы вы сохраняли вот это вот свойство.
То есть как бы у нас будет такая
х-таблица, что когда у нас
поднакидали столько элементов, что мы превысили это
отношение, мы берем
исправим полностью перестройку.
То есть мы берем, создаем новую х-таблицу
в два раза большего размера
и
что мы с ней делаем?
Заново сгибаем в нее элементы.
Идея такая, в общем-то.
Ну тогда если мы сказали, что это
меньше c,
тогда мы можем сказать, что это столько меньше,
чем 1 х c.
Ну откуда вас следует?
Потому что
мы от ожидания длины цепочки
это от 1.
Значит все операции
выполняются за от 1 в среднем.
Победа как бы, да?
Теперь давайте поймем, что это нифига не победа.
Мы по сути не приблизились к успеху
вообще ни разу.
Почему?
Потому что в этой модели
нужно хранить для каждого
ключа из u
какую-то случайную
числу.
Допустим, мы умеем случайно
убирать числа, да?
Но проблема в том, что мы должны хранить
массив размера модуль u.
Что совсем неприемлемо, в общем-то.
Вот вы работаете со строками,
строки на алфавите длины 2,
бинарные строки, да?
И они все, рассматривают
все строки длины до 50.
Два в 0, плюс два в первой,
плюс и так далее.
Два в пятидесятой минус один.
Много достаточно.
И как бы нам с таким
иметь дело не хочется.
Поэтому simple uniform hashing
это не алгоритм никакой.
Это модель теоретическая, в которой мы смогли
что-то посчитать.
И действительно, если это верно,
то есть если у вас верно тот принцип,
то если мы равномерно выбираем,
то действительно это круто.
Мы смогли доказать,
что все-таки единица работает.
Но мы только что поняли, что на практике
это ни разу не применимо.
Теперь будем подгонять модель,
чтобы она была применима на практике.
Давайте еще раз
вернемся к этим вычислениям.
Вот это просто по определению цепочки.
Это свойством от ожидания,
этот переход.
Этот переход тоже свойством от ожидания.
Смотрите, вот отсюда
до сюда
мы ни разу
не пользовались моделью теоретической
как таковой.
Мы пользуемся тем, что у нас есть какие-то бакеты.
Я ценю их размер.
Что теперь будем делать?
Проблема в том,
что у нас вот такое неравенство есть.
И вот оно вытекает
из нашей модели, которую мы построили.
Давайте мы ослабим требования
и не будем требовать равномерности, независимости.
Будем требовать только вот
такое вот интересное ограничение.
Определение.
Отдельное следующее.
Что
назовем
семейство
аж красивое.
Хэш функции
лямбда универсальный
если
для любых x и y
рассматривается вероятность по хэш функциям
из семейства.
То есть, да, вы хотите брать случайную
хэш функцию.
У вас зачем-то возникло такое желание.
аж от x
равно аж от y
не превосходит
лямбда делить на
сем.
То есть, смотрите,
в чем суть?
Вы такие, окей, хорошо.
Это все, конечно, круто.
Я не могу брать бассейн размера u
и каждому из них, независимо, проставлять
это долго.
Это много по памяти.
Но я могу что сделать?
Я могу как-то параметризовать мою
семейство хэш функций.
То есть, сделать, чтобы оно зависело
от каких-то параметров.
И дальше просто перебирать вот эту
вероятность.
Потому что вдруг хэш функции будет удовлетворять
вот этому соотношению.
Вот тут вот.
Это вот оно, по сути.
Лямбда.
Либо с неравных игр,
имеется в виду, конечно.
Окей.
Теперь, что мы сами сделаем?
Мы будем считать, что u
это множество.
То есть, это какое-то множество чечелок.
Оно будет большим.
Достаточно.
Это будет uint64t.
То есть, их будет 2 в 64
в реализацию у вас.
Но при этом это не все объекты
возможны, типа там строк, например.
Как со строками работать,
потом поясним.
Это нам будет важно для анализа.
Тогда, если у нас есть такое
семейство хэш функций,
то все. У нас есть вот такое
вот свойство.
У нас есть такое свойство, у нас есть оценка.
Нужная нам.
Но осталось
предвить h.
Согласны?
Если я построю вам такую h красивую,
то мы победим.
Определение.
В плане.
У вас вот здесь вот
один делить на m.
Но нам достаточно лямбда делить на m здесь взять.
Все равно оценки все не изменятся.
Просто там
констант циприс побольше взять просто.
Определение следующее, что
h красивая сильно универсальная,
ну или просто универсальная.
Если оно
один универсальное.
То есть, если вы смогли
построить лямбда равна единице, то оно называется у вас
сильно универсальным.
Или просто универсальным.
Или один универсальным.
И еще тысяча других вариантов, которые вы только
придумаете, потому что здесь нет фиксированной терминологии.
Мы убьем его строить.
Они один универсальная.
То есть, лямбда равна единице у вас в этом соотношении.
Окей.
Рассмотрим
h параметризованная параметрами
a и b.
А b принадлежит zp,
а не равно 0.
Давайте так
допишем.
h ab
так
h ab
от x
равно
ax
плюс b
по модулю p,
по модулю m.
Где m, это у вас
вот отсюда берется чиселка.
А эта чиселка берется
отсюда.
Эта чиселка берется отсюда,
а m это число бакетов.
Я хочу доказать, что у такого семейства будет один универсальный.
Сильно универсальный.
Что вообще не очевидно.
Что?
p простое.
p простое.
p больше,
чем
модулю.
Чтобы у вас любой x,
который вы могли подать хэш-функции,
он у вас всегда был
меньше, чем p.
Чтобы он тоже жил в zp.
Теорема.
Семейство выше.
Сильно универсальное.
Универсальное.
Будут идеи, как доказывать?
Ну окей, ладно, давайте я начну.
Это вот 0,
1, 2,
и минус 1.
То есть, арифметика по модулю p.
Окей, здесь будет 2 этапа.
Первый этап это,
что значит, что
ax плюс b,
давайте зафиксируем x не равно y.
Зафиксируем
x не равно
y.
Введем g от x
равную ax плюс b
по модулю p.
То есть, вот это вот первый кусочек
внутренний.
Давайте рассмотрим, что значит g от x
равно g от y.
Это то же самое, что у вас
ax плюс b
равно
ax плюс b
по модулю p.
Согласны?
Просто по определению уже.
Это то же самое,
что у вас
ax
сравнимо с ay
по модулю p.
Так как zp это поле,
там можно делить
мы можем домножить на a в минус
1 здесь и здесь,
и у вас не возникнет
многих проблем.
Что x сравним с y по модулю p?
Так как у нас
x и y из zp заведомо,
потому что p больше, чем размер множества,
размер u,
равесенен тому, что x равно y.
То есть, мы вам показали
равносильность
того, что у вас
g от x равно g от y,
это то же самое, что x равно y.
Ну что из этого следует в частности?
Из этого следует в частности,
что g
из zp,
ну x и y вообще лежат вот здесь, вот в u.
x и y в u.
Так как у нас
p заведомо больше, чем u,
у нас x и y
равносильно этому,
просто-напросто.
Окей.
Мы показали, что g коллизии вообще не дает.
g не дает коллизий.
Второй этап доказательства.
Докажем, что g биекция.
То есть, смотрите, у вас
была какая-то точка.
Вот у вас шкарнатная плоскость.
Ось x, ось y.
Это g от x,
это ось g от y.
И вы как-то сюда переносите
по действиям функции g.
То есть,
у вас была какая-то точка.
Вот у вас шкарнатная плоскость.
Ось x, ось y.
Ваш точку x, y.
x, y приходит
g от x, g от y.
То есть, мы действительно показали,
что каждую точку мы переносим в какую-то одну.
Отсюда-сюда.
Но правда ли, что из-за этого следует
что g биекция?
Вот мы доказали
с вами, что g от x равно g от y,
а тогда это x равно y.
Правда ли, что g биекция?
Ну да, x, y из z, p лежат.
g от y будто бы тоже
из z, p лежат, потому что берете по моделю
p в конце.
Ну как-то можно явно показать.
Можно показать, на самом деле, что
если у вас
есть пара x, y
и пара g от x, g от y,
вы можете однозначно a, b восстановить
по ним просто-напросто.
Согласны ли вы с этим утверждением?
То есть, если вы знаете, что у вас
ax плюс b равно u,
ay плюс b
равно v,
то у вас есть такая система, да?
То вы a и b однозначно
из нее находите.
Ну давайте посмотрим, почему это верно.
Рассмотрим матричную запись.
Что это такое?
Это будто бы вы рассматриваете
матрицу A.
Не, не так. Не так хочу
написать. Наоборот, хочу x, y написать.
У, в.
Правда? Вроде правда, да?
Правда, да?
Тогда у меня a, b однозначно
останавливаются по u и v,
тогда и только тогда, когда этот
определитель не выражен.
У вас определитель выражен только тогда,
когда x равно y.
Все, победа. То есть, мы с вами
получили что-то биекция, действительно.
Вот. Зачем я это делаю?
Зачем я это сделал?
Затем, что иногда вам нужно будет,
если вы будете заниматься алгоритмами,
у вас будут не только вот такие вот семейства,
но вы будете
хотеть семейства больше,
что у вас был h от x, там равно h от y,
h от y равно h от z и так далее.
То есть, больше сюда засовывать условий.
Да, такие случаи
не нужны. Есть такие хэш-функции.
Они там называются
два независимые,
три независимые. В некоторых алгоритмах
нужно пять независимые хэш-функции.
И у вас получается здесь матрица
Вандермонда, по сути.
И вы считаете определитель.
А определитель матрицы Вандермонда у вас
не выражен, если у вас нет равных элементов
среди x, y и так далее.
Поэтому я это написал, чтобы было красиво.
Вот.
Ну, давайте это
дельта обозначим.
death от
x, y.
Вот.
Ну, давайте это дельта
обозначим.
death от
x, y, 1, 1
дельта не равен нулю при
x, не равных y.
Ну все, доказали, что у нас есть объекция.
Я говорю следующее,
что
вероятность вот этих вот
это то же самое, что вероятность
вот эта вот.
То есть, вероятность третьей равной x, y
такова же,
что вы возьмете равные a, b.
Поэтому у вас семейство хэш-функций
параметризуется так,
а с помощью этого хода вы можете
перейти к x, y от a, b.
Потому что там у вас
на самом деле записана вероятность
и для x, y.
Для x, не равных y вы рассматриваете.
А здесь у вас написана
вероятность по хэш-функциям.
И вообще связь не очевидна, откуда она берется.
Это же разные вероятности, по сути.
И когда он сказал, что отсюда следует это,
это вообще неправда на самом деле.
Это следует тогда и только когда,
когда у вас по x и y
однозначно восстанавливается хэш-функция.
Что мы здесь доказали с вами.
По модулю m мы будем брать только для того,
чтобы получить вот эту вот оценку
на самом деле.
Это просто вероятность того,
что берете случайную хэш-функцию.
То есть вы считаете здесь
число хэш-функций, дающих коллизию
на конкретных x, y.
И делите на размерность семейства.
Классическое комбинаторное
определение вероятности,
это число успехов делить на число
всевозможных вариантов.
Число успехов это число хэш-функций,
которые вам дадут коллизию.
Всего вариантов, это сколько у вас
элементов семейства.
И здесь это вообще не очевидно как-то связано
с числом пар.
А если мы показали, что это биекция
на этапе g,
на этапе функции g,
это биекция.
И вы можете говорить в терминах
x, y о вероятности в терминах h.
Отлично.
Теперь осталось,
раз у нас g биекция,
осталось по модулю m разобраться,
что происходит.
Наверное, сатрусы этой доски
все-таки, потому что там
все еще важная вещь.
Пункт третий.
Соотношение
на вероятность, так называемое.
Как я вам сказал,
мы сейчас будем считать число
хэш-функций, которые дадут вам коллизию.
То есть зафиксируем x неравно y.
Если мы зафиксировали x неравно y,
мы можем однозначно восстановить a, b.
Поэтому мы однозначно можем восстановить
не так сейчас, не x неравно y.
И все, просто зафиксировали их.
Остановились на этом этапе.
Что дальше будем делать?
Будем считать вероятность того,
что h от x равна h от y.
Которую нам сказано посчитать.
Как ее оценить?
Зафиксируем x.
Мы зафиксировали x неравно y.
Даже не так, просто зафиксируем какой-то x.
Тогда что это такое?
Как это можно посчитать?
Сколько у вас для заданного x
хэш-функций,
которым дадут коллизию?
Тогда h от x
равно h от y
равна сильно тому,
что g от x
равно g от y
по модулю m.
То есть вот у вас
какая-то числа прямая, да?
Вот ваш g от x.
Он лежит где-то в отрезке
от 0 до p-1.
Тогда как найти число таких y,
что g от y,
будучи биекцией,
вам даст
равное число?
g от x
плюс m,
g от x
плюс 2m,
g от x минус m.
Сколько таких отрезков длины m влезет?
Ну я утверждаю что-то типа
что вот столько отрезков будет.
То есть я при фиксированном x
из двух вариантов получу коллизии.
Почему это так?
Потому что у меня есть биекция
между x и y, которые я здесь
рассматриваю напрямой, и хэш-функциями,
которые я здесь рассматриваю.
У меня же биекция как в одну сторону,
так и в другую биекцию будет.
Поэтому если я посчитал число равных пар
x и y при фиксированном x,
это значит, что я нашел число
нужных нам хэш-функций.
То есть это равно
p на вот эту вот штуку.
Ну почему это так?
Потому что у вас x в p штук.
А для каждого x вот столько y.
Поделить на
размерность h большого,
h красивого.
Сколько у нас хэш-функций всего существует?
Ну давайте они параметризуются a и b буквами.
Буквок a сколько разных может быть?
А лежит zp, а не равно 0.
p-1, а b разных сколько?
В смысле плюс 1.
Почему b-то p-1?
А ну любое может быть.
Их p.
Ну 0, 1, 2, 3.
Их p чисел разных будет.
То есть b класс p, h к p-1.
Поднимите руки,
кто верит, что это
1 делик на m будет.
Не превосходить.
Ну давайте докажем.
Изня.
Ну я утверждаю,
что это верно на самом деле.
Просто мне немножко лень сейчас с этим возиться.
Будем честны.
Вот.
То есть мы с вами построили сильно универсальное
семейство хэш-функций.
А раз мы смогли его построить,
и мы доказали, что
в данном семействе хэш-функций,
данной вероятности коверентам на той доске,
мы с вами построили нужное нам
семейство хэш-функций с таким свойством.
Значит мы построили наконец-то
хэштаблицу с заданными
операциями.
Все, теперь вы можете пользоваться
в следующем контесте, можно будет пользоваться
всем, если что.
Типа нордовцитами тоже можно будет
теперь пользоваться.
Потому что мы обсуждали, как они реализованы,
почему они работают.
Все окей, мы закончили с хэштаблицами, и
я вас поздравляю, мы закончили с структурами данных
на этот семестр. Теперь алгоритмы.
Динамическое программирование.
Шутка
и простатическое программирование не уместно
из зала.
Кто-то пошутил, я слышал.
Окей, вот, динамическое программирование,
что это такое? Это
такой подход, когда мы
хотим решить какую-то одну большую задачу,
и мы знаем, как решить задачу
гораздо проще.
Пример. Тут
легче сразу сразу с примера начать.
Вспомним наше любимое число
Fibonacci.
F1
равно F2
равно
1.
Как
напишут
вычисление числа Fibonacci
человек, который
мало об этом программирует,
нажимается так, что
int функция Fib
от n
типа if
n
больше либо равно 2
в return
Fib от
n-1
plus Fib
от n-2.
Иначе
return
0.
Это всегда код,
вы не понимаете, это другое.
Есть такой код,
чем он плох,
чем он хорош, давайте так.
Он с полностью соответствующей математической модели,
но при этом он очень плох
с точки зрения реализации.
Можете попробовать по приколу
оценить, сколько времени это работает.
Ну, примерно, да.
Ну, от числа Fibonacci.
Оно не растет
потенциально быстро, мы это сами уже выясняли.
То есть, смотрите, здесь явно принцип
того, что мы хотим посчитать Fn,
но если мы знаем n-1 и n-2,
то мы как бы явно можем
посчитать Fn.
То есть, как бы у нас какая-то сложная задача, мы ее делим
под задачу попроще.
Поэтому давайте мы будем
считать не так, а заведем массив dp
так называемый.
Пусть dpi
равно
F it
просто.
Тогда у нас есть база индукции,
база динамики, так сказать.
У нас есть с вами переход.
Это что dp it
это dp i-1
плюс dp
i-2.
Мы знаем, где лежит ответ.
После того, как мы посчитали dp,
ответ от dp n.
Осталось понять, как порядок
пересчета делать.
В каком порядке вы будете
высчитывать значения dp it?
Можно вот так вот высчитывать, да?
Можно написать
i равно 1,
i равно 3,
n.
Вот.
То есть, можно идти вниз,
так сказать, нисходящую динамику делать,
от n уменьшать.
А можно, наоборот, вверх,
от маленьких в больших.
Вот.
Собственно, как-то так.
То есть, смотрите, чем прикол.
Вы могли бы здесь действительно заполнить
мотив dp, запомните его.
И здесь перекрутивно вызываться,
и просто, если вы взялись рекурсивно,
то возвращать dp it,
если он уже посчитан.
Иначе,
надо посчитать.
Как бы, действительно,
это будет работать тоже за рению
суммарно, но
эта рекурсия, она не очень
просмотрится все-таки.
Вот.
То есть, как бы,
как можно рассматривать динамику?
Динамику в некотором плане к ней можно посмотреть
как какая-то индукция.
То есть, индукция по номеру
часа фибоначи, например.
Мы сейчас посмотрим часам пару задач,
чтобы было понятнее.
У нас есть база, у нас есть переход, у нас есть местный гляд,
у нас есть порядок пересчета.
То есть, и того, чтобы,
так сказать, пять шагов к успеху в задаче на dp,
это вам нужно определить пять
из этих параметров.
Это
собственно, что такое
dp it?
Ну, быть может, здесь будут у вас больше измерений.
Просто, что такое состояние динамики?
Дальше, какие базы в состоянии вам заведомо
известны, и вы считаете
относительно быстро?
Какие у вас есть формулы перехода?
Где у вас будет лежать ответ, когда вы посчитаете?
И в каком порядке вы будете считать ответ?
Здесь желательно четвертый и пятый местами поменять.
Но это неважно особо.
То есть,
не то чтобы формальность,
понятное дело, что
никто, когда решает задачу на динамику,
не сидит такой, блин,
где, какой у меня порядок пересчета?
И не выписывает на листочке где-то себя.
Просто берет и пишет.
И он не особо задумывается о том,
что здесь действительно есть пять шагов.
Итак,
что можно об этом сказать?
Короче, если вы будете представить голове как-то так,
то он будет жить проще, просто,
решать задачу на динамику.
Помимо того, есть еще принцип
перекрывающихся под задач,
это когда вы много раз должны посчитать одно и то же.
То есть, если вы рассматриваете
этот код,
то у вас там в часах манача 10,
когда вы его вызовете, ФИБА 10,
где-нибудь в мейне вы здесь ФИБА 10 посчитаете, да?
У вас кучу раз ФИБА 5
будут вызываться.
То есть, перескачиваешься
под задачу, посчитать ФИБА 5
и кучу раз. Давайте просто один раз
сферим ответы и все.
То есть, подход, когда у вас есть рекурсия
и вы запоминаете массив ответов,
это называется рекурсия
с мемуизацией, так называют.
А когда вы отказываетесь от рекурсии,
то это называется ДБА.
Как много в этом словосочетании?
Ну, и по сути дела,
мы сейчас будем просто рассматривать
разные задачи, какие есть.
Ну, и такая первая задача,
помимо чисел ФИБА,
легендарных, конечно же,
это задача про кузнечика.
Ну да, прикиньте,
вот мы с вами проходили
Б деревья с Б деревья.
Тут, кажется, таблицу рядом с ними считали.
Мы с вами спустились на такое дно.
Это кузнечик. Вот, есть кузнечик.
Знакомьтесь к кузнечику.
Он не очень похож на кузнечика,
но были вот такие кузнечики,
откуда я родился.
Будем считать.
Может, это Козлик, не знаю.
У вас вот написано там
числа, которые...
Вот есть вот, так сказать,
он умеет у вас прыгать как-то,
тут написано число.
Допустим, не знаю,
он упал на клетку с А1,
там, не знаю, потеряешь,
но что может кузнечик потерять?
Не надо так.
У нас все-таки ХП.
Очки ХП он теряет.
Вот, там, не знаю, если он упал во 2,
типа тут, не знаю, была там
схватка с гусеницей, например, какая-нибудь.
Если он упал во 2, то он там
поел, что идет к кузнечике.
Ну траву, вот он поел траву,
плюс есть очки ХП,
что-нибудь такое.
Вот кузнечик умеет прыгать на 1 вперед
и на 2 вперед.
Вопрос в том, какая траектория
должна быть к кузнечику, чтобы
собрать максимальное
количество очков ХП
к А1.
Давайте здесь у него
будет, не знаю, домик.
А как он умеет прыгать?
На 1 вперед, а на 2 вперед.
Вот, ну давайте
здесь жадный алгоритм, который
все время прыгает в максимальную
возможность, что он не проходит.
Вы согласны с этим или нет?
Да, да.
Ну давайте построим пример,
когда он не работает.
Что-нибудь типа минус 1.
Десять.
Минус 100 и минус 1000.
Вот ваш балдежный кузнечик.
Вот его тропа успеха.
И как ваш алгоритм будет
прыгать?
Что?
Не, почему?
Он у вас прыгнет сюда?
Ну вот что такое жадный алгоритм?
Вы будете прыгать, а он
в наименьше из зол
возможных на данный момент.
Ну минус 1 и 10.
Вы прыгнете в 10, понятное дело.
Ну вам нужно всегда в ноль добраться, допустим.
Хорошо, дальше прыгать вперед.
Либо сюда, либо сюда.
Видимо сюда, да?
Все, наждано.
Да, это
минус 3.
Сейчас.
Это обман.
Наверное 1000.
Ну да, если минус 100 и 1000.
Если он отработает,
в любом случае.
Десять и не минус.
Наверное, у вас есть число
для вытягивающих.
Вот так вот.
Он не сможет прыгнуть.
Минус 100, минус 50.
Он тогда в минус 100 прыгнет.
Да я хочу вас байтить на прыжок.
В какое место?
Он не байтит.
Помогите.
Ну типа, мы стоим, окей.
Мы прыгаем в лучшее из возможных.
И вот Коки хорошо стоит и дальше смотрит.
Я прыгаю в минус 100, наверное.
Смотрит и дальше прыгает.
Лучше из двух, наверное, минус 1000.
Прыгнет 0.
А оптимальный алгоритм
был бы такой.
Раз, два, три.
То есть не фармит лишние минус 100.
Валдежно мы смотрели.
Это было сложно.
Вот такой у нас кузнечик классный есть.
Вопрос, как решать задачу.
Жадную гриф не подходит.
Можно пробовать другую жадность какую-нибудь.
Не факт, что это получится.
Поэтому предлагается следующее решение.
Рассмотрим DPIT.
Рассмотрим DPIT.
Ответ.
Если
массив имеет вид.
A1, A2.
То есть первый и элемент.
Это прекрасный
naming, а нам как-то
именно надо назвать.
Вот, это самое сложное
в динамическом программировании.
Но помимо того, чтобы понять, как решать задачу,
правильно назвать DP.
Что такое DP?
Это
не знаю.
Max result
on prefix.
То есть длинные имя писать.
Да.
Ну я не знаю, может у вас там
семинаристы примут короткое имя, я не буду понимать.
Окей.
Хорошо.
Мы сами определили базы где.
Где базы, ребят?
Ну такой отбеляет 0.
Отбеляет 0, да.
Что еще может быть?
Мы можем сказать четко DP от единички.
Это что такое?
Это A1 просто.
Мы можем еще DP2 задать,
например.
Это максимум из
A2,
A1 плюс A2.
Ну или можно перестать
так как A2 плюс
A1 на индикатор того,
что A1 больше 0.
Да чего так сказать просто A2
плюс максимум A1
0? Можно.
Можно как угодно.
Типа здесь
it's up to you, как говорится.
Где-то считать.
Окей.
С этим справились.
Уже хорошо.
И переходы есть.
Ну,
DP iti
это A iti
плюс
максимум из
DP
и минус 1, DP
и минус 2.
Почему?
Вы обязательно придете
и выберете максимум
из двух величин, откуда вы могли попасть.
Но ответ лежит в DPN.
Порядок пересчета
вот.
Окей, знаете, справились.
У нас времени.
Вам уже опять надо сказать.

да давайте начинать здравствуйте друзья значит в общем ситуация такова что мы
теперь с вами будем до конца семестра видимо проводить лекции вам они потому что я уехал
и видимо уж как минимум до конца семестра не вернусь но надеюсь на качестве лекции это не
скажется так значит теперь давайте к делу начнем мы с вами фроша остановились меня вообще нормально
так но по поводу экзамена знаете сейчас вообще сложно о чем-то конкретном говорить как будет
устроена жизнь там у нас в июне я тоже не умею это предсказывать но пока никаких официальных
распоряжений на эту тему не было и поэтому на сегодняшний день видение такое что экзамен
будет в обычном формате то есть он будет в аудитории если я не приеду то принимать просто
буду не я есть много людей и семинаристов которые и других преподаватели которые в этом
семестре не участвует в давание вероятности которые могут принять экзамен проблем с этим
никаких нет но может быть ситуация как-то изменится но на текущий момент видение такое что экзамен будет
в обычном формате меня нормально слышно видно да да да да прекрасно просто не очень хорошая
интернет поэтому если какие-то сбои пожалуйста сразу говорить так ну хорошо значит мой фрош
раз с вами остановились на понятии математического ожидания и в общем-то довольно полно о нем
поговорили подробно давайте теперь двинемся дальше говорим про дисперсию
слишком жирно
красно так дисперсия
но прежде чем я определю что такое дисперсия
давайте я поясню какая интуиция этого понятия зачем она вообще нужна но опять же важна
статистическая составляющая то есть статистическая страна дело такой параметр распределения который
важен со статистической точки зрения в том смысле что если у вас есть представить себя
обычную ситуацию чтобы вы смогли один тоже эксперимент произвести независимо много раз и
есть у вас какие-то результаты эксперимента до которой мы воспринимаем независимо случайная
величина вот на практике это просто там миллион чисел или тысяча чисел и вы хотите сделать вывод
о том как распределена распределены ваша случайная величина по вот этим числам ну один
из варианта писать средняя до узнать какое математическое ожидание случайной величины
то есть если в эти величины творции подельное количество то вы получите приблизительно да
если ваш крики до довольно большим то это будет высокая точность вы получите приблизительно
математическое ожидание, случайно, речное. Вот, но понятно, что среднее значение, но оно мало
о чем говорит, оно говорит о том, что, ну, если у вас нет никакой другой информации, то среднее
значение просто говорит о том, что вы ожидаете в будущем вот такое вот значение, да, что вот, что
вот вокруг него будут сосредоточены все ваши числа. А вот насколько сильно вокруг него они
будут сосредоточены? На этот вопрос отвечает значение дисперсии. Вот дисперсия это как раз мера
отклонения случайного лично от ее математического ожидания. Иными словами, разброс может быть очень
большой. От того, что вы нашли ожидаемое значение, еще рано радоваться, из этого не следует, что вы
поймете, как ваша случайная величина будет вести себя в будущем, потому что она относительно этого
ожидаемого значения может отклоняться очень сильно. И вот эта вот мера отклонения, это есть дисперсия,
если дисперсия маленькая, то математическое ожидание является хорошим приближением в том смысле,
что вокруг него все будет сосредоточено. Если дисперсия большая, то вам нужно это отклонение,
конечно, учитывать при каком-то прогнозировании. Значит, теперь что же это такое с теоретической
точки зрения? Дисперсия случайного лично кси — это по определению математическое ожидание квадрата
разности между случайной величиной и ее математическим ожиданием. То есть квадрат разности — это
способ посчитать отклонение, то есть это такое квадратичное отклонение случайной величины
от ее среднего, а потом вы еще раз берете среднее, получаете среднее квадратическое отклонение,
ну или иными самими дисперсиями. Дисперсию можно посчитать иначе, и зачастую, как правило,
так делать проще. Считать вот не так, как здесь написано, не по определению, а по эквивалентной
формуле, которая легко доказывается. Значит, утверждение стоит в том, что дисперсия — это
мат ожидания квадрата случайной величины минус квадрат мат ожидания случайной величины.
Ну почему так? Просто надо формулой линейности, свойством линейности математического ожидания
воспользоваться. Смотрите, берем нашу дисперсию по определению. Это есть мат ожидания от квадрата
разности, но этот квадрат разности можно раскрыть. Что такое квадрат разности? Это
кси квадрат минус удвоенное произведение кси и математическое ожидание кси и плюс
квадрат математического ожидания. Дальше как раз линейность, которая говорит о том, что вы
можете вот эту всю сумму представить в виде суммы математического ожидания. Ну еще константная
длинность за знак математического ожидания. У вас получается от ожидания от кси квадрата
минус. Давайте посмотрим на вот это второе слагаемое. В нём есть константы. Это двойка и мат ожидания
кси. Это просто число, это не случайная величина, поэтому их за знак математического ожидания
можно вывести. У вас получится два мат ожидания кси. Умножить на мат ожидания того, что осталось,
но осталось просто кси. И плюс мат ожидания кси в квадрате.
Вот, ну теперь просто мы уничтожаем от мат ожидания кси в квадрате. У нас
остаётся мат ожидания кси в квадрате, и у нас остаётся мат ожидания от кси квадрата минус мат
ожидания кси в квадрате. Что это требовалось?
Ну давайте теперь разберём какие-то примеры, а потом поговорим о свойствах дисперсии. То есть
давайте посчитаем просто дисперсию для основных распределений, о которых мы с вами говорили и
когда мы мат ожидания тоже считали. Начнём в дискретных.
Так, в какой мы там последовательности рассматриваем? Давайте в той же самой
последовательности будем рассматривать дисперсию.
Ага, значит, Бернольевская, биномиальная, равномерная по освобождению. Прекрасно.
Итак, сначала Бернольевское распределение.
Ну, чтобы найти дисперсию, мы будем в основном читать вот эту вторую формулю,
которая в утверждении написана. Значит, поэтому, чтобы найти дисперсию, нам было неплохо найти
мат ожидания квадрата случайно-речным. От того, что мы вернулись к случайно-речному
изведению в квадрат, она, конечно, не поменяется, потому что у неё два значения, 0 и 1. От изведения
нуля в квадрат, единицы в квадрат, они не меняются. Поэтому кси квадрат просто равно кси, а значит,
мат ожидания кси квадрат равно мат ожидания кси. Как мы знаем, это P. А дисперсия – это значит
мат ожидания кси квадрата минус квадрат мат ожидания кси. Ну, то есть, это P минус P в квадрат.
Окей, двигаемся дальше. Равномерная на множестве чисел от 1 до n.
Опять, сперва, найдем мат ожидания кси квадрата. Ну, смотрите, значит, если кси принимает значение 1
и тогда m, равномерность восприятия означает, что все эти значения имеют одинаковую вероятность 1n.
То кси квадрат принимает значение, равное квадратам этих чисел. То есть, 1, 4, 9, 16, 25 и так далее.
Их вероятность – это 1n. То есть, на самом деле, мат ожидания кси квадрата – это будет просто сумма
значений нашей случайночной кси квадрат. Сумма платья от 1 до n и квадрат. И умножительная вероятность,
которая равна 1n. То есть, квадрат по 9 на n. Что равно… Сумма квадратов – это n на n плюс 1 на 2
и 2n плюс 1 поделить на 6. Значит, получится n плюс 1 умножить на 2n плюс 1 и поделить на 6.
Давайте еще сразу вспомним, что у нас была теорема о замене переменных в интеграле Лебеда,
которая говорит, что если мы считаем от ожидания от функции случайного вектора или случайной
то есть интеграл от этой функции по мере равной распределению naszej случайной верчины или firms.
Соответственно, если у вас есть дискрепная случайная верчина и вы считаете мат ожидания
от функции, ну например theeб怎么 ты fem, вот как мы только что делали. То есть интеграл от
хsquad по мере равной распредел taking hits поrenched virtuality. Соверш memang disk этот будет просто
сумма квадратов значений случайного величины uplift на вероятности этих значений. Это точно
то, что мы с вами написали. Для дискретного распределения это и так понятно, потому что
когда вы возводите случайную смотрему в квадрат, в квадрат возводится ее значение. То есть вы
получаете случайную смотрему, значение которого равно квадратам, и вероятности те же самые
остаются. В точности т albums об замене переменных в интегронAnnibale. Вы можете найти дисперсию.
Вычитаем из квадрата мат ожидания, который равен, как мы только что посчитали, n
плюс один умножить на 2, n плюс один поделить на 6.
Вычитаем в квадрат математическое ожидание. Мат ожидания для равноверного мы считали это
n плюс один пополам, значит вычитаем n плюс один пополам в квадрате.
Так, ну если это аккуратненько посчитать, общий знаменатель будет 12, будет общий множественный
n плюс один, и в числителе еще будет 4n плюс два минус 3n минус три, то есть получится на самом
деле n минус один, и в итоге получается n квадрат минус один длить на 12.
Есть какие-то вопросы?
Вопросов нет, окей. У меня есть вопрос. Да, пожалуйста. Как долго мы будем повторять тот курс,
который мы уже прослушали? То есть это совсем очень маленькие изменения, как-то по два раза
одно и то же. Вы имеете в виду, что у вас уже были дискретные распределения, но дискретным
распределением мы уделяем там 10 процентов лекций. Если это проблема, мы можем не уделять, но мне
кажется, что за счет того, что я провожу там параллели с какими-то вещами, которые я вам
рассказываю в первый раз, то это полезно. Но может быть кому-то кажется, что это слишком просто,
но прекрасно на самом деле, что вы понимаете. Не переживайте, у нас всего 4 примера дискретных
распределений. Остальные будут абсолютно непрерывные. Я так понимаю, про них Иван Георгиевич
не говорил. Ничего страшного, еще пять минут и перейдем к абсолютно непрерывным. Дайте возможность
тем людям, которые либо это подзабыли, либо хотят повторить. Бенмяльное распределение.
Здесь можно читать в лоб. Я не буду этого делать, так же, как мы делали, когда читали математическое
ожидание. Можно посчитать в лоб от ожидания касси квадрата, сделать какие-то вычисления и после
этого найти дисперсию, которая окажется равной NP умножить на 1-P. Я сейчас напомню свойства дисперсии,
и потом вернемся к этому вопросу. Видим, что дисперсию можно посчитать просто как сумму
независимых, как дисперсию суммы независимых вернувских случайных увеличений. Ну и наконец
полосоновское распределение. Опять же, если вы считаете мат ожидания квадрат случайной величины
для полосоновского распределения, то вы либо потеряете о замене переменных, да либо понимаете,
что значение случайной величины просто называется в квадрат, поэтому у вас получается сумма значений,
то есть и в квадрате по и от 0 до бесконечности, умножить на вероятность, то есть на e в степени
минус лямбда, на лямбда в степени и и потерять на e факториал. Ну опять, значит, слагаемая при
и равном нулю исчезает, потому что она равна нулю, поэтому можно начинать суммировать с единицы,
суммировать с единицы, и получится что? Получится и на e в степени минус лямбда,
на лямбда в степени и и поделить на e минус 1 факториал. Ну а дальше делаем следующее. Мы первые
множители и развиваем на сумму e минус 1 и плюс 1, и получается вот такая вот вещь. Ну и соответственно
перепишем это дело в виде двух сумм. Первая будет сумма, значит, первая, вот когда мы возьмем слагаемая,
в котором есть множитель e минус 1, он сократится с e минус 1 факториал. Более того, при e равно 1 вы
получите 0, поэтому суммировать можно начинать с e равно 2 до бесконечности, e в степени минус
лямбда на лямбда в степени и поделить на e минус 2 факториал. Ну и плюс то же самое, только сумма
по e, начиная с единицы, и в знаменателе будет e минус 1 факториал. Далее, если вы из первого слагаемого
вынести лямбда в квадрате, вы получите, конечно, просто ряд пейлора для экспонента, и экспонент
сократится и останется просто лямбда в квадрате. То же самое со вторым слагаемым, который даст просто
лямбда. Ну и значит дисперсия. Это есть мат ожидания x2 минус квадрат мат ожидания, что равно просто лямбда.
Вот. Ну и теперь давайте перейдём к абсолютно непрерывным распределениям, тоже в той же
последности, которые были для математического ожидания. Что мы там считали? Мы сначала считали
равномерное. Да, нет, не здесь. Мы сначала считали вот равномерное, потом экспоненциальное,
а потом нормальное. Давайте также сделаем равномерное распределение на отрезке от АДВМ.
По тому же принципу сначала найдем от ожидания x2, но здесь мы в точности применяем теорему
Либега о замене переменных. То есть интегрируем x2. Интегрировать мы должны по мере
соответствующей равной распределению случайно вещества x. Она абсолютно непрерывная, поэтому интеграл
Либега по этой мере превратится в интеграл Либега по классической мере Либега. Надо просто вынести
из-под дифференциала плотность, которая равна единице поделить на b-а. Ну и на индикатор, но этот
индикатор можно занести в пределы интегрирования и написать интеграл тадабэ. Началось вот такой
интеграл, который равен там b в кубе минус a в кубе поделить на 3 b-а. Ну или b квадрат плюс ab
плюс a квадрат поделить на 3. Далее дисперсия. Это мат ожидания x2 и минус квадрат мат ожидания
минус a плюс b в квадрате поделить на 4.
Опять, если привести к общему знаменателю, то что получится? Значит общий знаменателем будет
12 и несложно видеть, что получится a минус b в квадрате. Вот это дисперсия с равномерным
Хорошо, теперь экспоненциально.
Значит, читаем мат ожидания x2. Это опять по тяреме замене перемен в интеграле бега вы просто
интегрируете x2 в нужной на плотность случайно вечной x. То есть будет интеграл от 0 до бесконечности
x2 лямдо e в степени минус лямдо x dx. Если вдруг кто-то не понимает, откуда берутся интегралы,
которые я пишу, пожалуйста спрашивайте. Заносим экспоненту по дифференциал, получается минус xd
от e в степени минус лямдо x. Дальше интегрируя по частям, первая слагаемая очевидно будет равно нулю,
останется интеграл от 0 до бесконечности e в степени минус лямдо x dx, который равен 1 дриет на лямдо.
Ну и следовательно дисперсия. Это есть мат ожидания x2, то есть 1 дриет на лямдо,
минус квадрат мат ожидания, то есть 1 дриет на лямдо квадрат.
Можно вопрос? Второй знак равенства в первой строчке. Там было слева x2, а справа x2 уже нет.
Потому что я написал, конечно глупость, сейчас я это исправлю, спасибо большое.
Значит да, здесь должен быть x2 и здесь должно быть 2x.
Здесь должно быть 2x. Ну еще раз интегрируя по частям,
мы получим интеграл от 0 до бесконечности минус 2x, дифференциал от e в степени минус лямдо x,
еще на лямдо надо поделить. И это есть интеграл от 0 до бесконечности, 2 делить на лямдо.
E в степени минус лямдо x dx, что равно 2 делить на лямдо квадрат. А значит дисперсия,
это есть мат ожидания x2, то есть 2 делить на лямдо квадрат, минус квадрат мат ожидания x,
минус 1 делить на лямдо квадрат, что равно 1 делить на лямдо квадрат.
Еще раз почитать, нигде не ошибся.
Дифференциал от e в степени минус лямдо x, 2x в степени минус лямдо x,
еще раз. И нормальное распределение. Давайте посчитаем для стандартного нормального,
а потом после того, как напишем свойства дисперсии, вернемся к нормальному распределению
параметра мат. Ну опять нам нужен так называемый второй момент, то есть мат ожидания x2.
Потеряемое замене переменных, это есть интеграл от x2 умножить на плотность, то есть 1 делить на
корень из 2p, e в степени минус x квадрат пополам dx. Ну и заносим экспоненту вместе с x как множителем
внутрь дифференциала, то есть получится 1 поделить на корень из 2px дифференциал, а еще минус
перед этим делом дифференциал от e в степени минус x квадрат пополам. Интегрируя по частям,
мы получим интеграл от минус бесконечности до бесконечности, 1 поделить на корень из 2p,
e в степени минус x квадрат пополам dx. Это просто интеграл от плотности нормального распределения,
который n единицы. А дисперсия это тогда тоже единица, потому что мат ожидания равнули,
то есть квадрат мат ожидания минус мат ожидания в квадрате, мат ожидания в квадрате ноль,
получаем единицу. Так, прекрасно, теперь давайте поговорим про свойства,
после чего вернемся к двум примерам, которые мы не досчитали.
Во-первых, для свойств мне будет полезно понятие кавариации случайных величин.
Когда я буду считать дисперсию суммы, я ее сведу к кавариации. Кавариация это некоторая
мера зависимости случайных величин, то есть, по сути, чем случайная величина более зависимая,
тем больше у них кавариация. Почему эта мера зависимости, я сейчас тоже объясню чуть позже,
когда поговорю про неравенство, потому что в униковском. Что такое кавариация? Кавариация
случайных величин x и e, это есть мат ожидания вот такого вот произведения, x минус e x,
это минус e x. В частности, понятно, что дисперсия это кавариация случайной величины самой собой.
Просто по определению дисперсии и кавариации. Дисперсия это мат ожидания.
Наверное, для нас мат ожидания это. Да, спасибо, конечно.
Во-вторых, дисперсия, конечно, не отрицательна. Это следует из определений и свойств математических
дисперсий, этому мат ожидания от квадрата случайной величины. То есть от не отрицательной
случайной величины. А мат ожиданий от не lautей случайной величины,shelfvin, самому по
себе не отрицательный. но некоторые аналог свойства линейности если вы берете дисперсию от
константа умноженной на случайная вечноénNEAR, вот эта константа выносится из-под дисперсии в
в квадрате, это тоже очевидным свойством следует изобреление дисперсии, если вы берете дисперсию
от константа на кси, то, во-первых, здесь появится квадрат внутри мат ожидания первого, да и с того,
что случайно вы считаете квадратом, во-вторых, второе тоже возойдется в квадрат, поэтому у вас будет
ц квадрат выестся впереди дисперсии. так хорошо далее далее понятно, что к вариации можно
посчитать иначе, значит, можно посчитать как мат ожидания произведения минус произведение мат ожидания
он оказывается в полной аналогии с формулой дисперсии, давайте я быстренько это напишу,
значит, квадрат от кси это, это есть мат ожидания по определению от кси минус е кси на это минус
вы считаете это произведение и получаете мат ожидания от кси это минус кси умножить на е это
минус это умножить на мат ожидания кси и плюс произведение мат ожиданий. но дальше раскрываете
по линейности, раскрываете по линейности и получается очевидным образом то,
что требуется. у вас есть три одинаковых слагаемых, у двух стоит знак минус, у третьего
Поэтому получается, действительно, мат ожидания произведения, минус произведение мат ожидания.
Дальше, ну теперь давайте с суммой. И что будет, если мы считаем дисперсию от суммы?
Ну понятно, что у нас даже с сумможением на константу тут линейность не получилась с суммой,
тем более, значит, верна следующая формула. Дисперсия суммы, n случайных величин для
общности, это есть сумма дисперсий, плюс сумма по всем различным i и g к вариации x и t, x и g t.
Вот, ну это тоже, в общем-то, очевидное следствие того, что первое свойство,
что дисперсия, сейчас давайте я вернусь к доказательству этого свойства по столке,
я еще скажу, что кавриация белинейна. Значит, это очевидное следствие того,
что дисперсия это кавриация случайно величины самой собой, но еще надо использовать то,
что кавриация белинейна, то есть, а именно, во-первых, асимметрично, то есть кавриация
x и t равна кавриации x и t, а во-вторых, кавриация от a1x1 плюс a2x2t равна a1 умножить на кавриация
x и 1t плюс a2 на кавриация x и 2t. То есть, иными словами, она линейна как по первому аргументу,
так по второму. В силу симметричности, то же самое верно и для второго аргумента.
Ну, с симметричностью все очевидно, просто определение симметричное, того,
что вы перестаете две случайных величины, и у вас правая часть не изменится с того,
что умножение коммутативно. То есть, с симметричностью понятно, с белинейностью тоже понятно,
в силу свойства линейности математического ожидания. Если вы смотрите на определение кавриации,
если вы вместо x напишете сумму x1 плюс x2 или a1x1 плюс a2x2t2, то все это по линейности
математического ожидания вынесется и получится a1 кавриация x1t плюс a2 кавриация x2t. Ну, вы либо с
Ваном Генриховичем это проделали, либо, если нет, можете поупражняться и доказать это самостоятельно,
это очевидное упражнение. Вот. Значит, как из этого следует, что дисперсия суммы равна сумме дисперсии
плюс сумму кавриации? Ну, кстати, сумма дисперсии, это, конечно, та же самая сумма кавриации. То есть,
можно было бы написать просто сумму кавриации по всем, вообще, возможным и ежели. Это было бы
то же самое. Но мне удобнее такая формула, потому что я для независимых случайных величин из этого
сделал некоторый вывод чуть позже. Поэтому давайте вот в таком виде эту формула оставим. Значит,
доказываем. Итак, давайте дисперсию просто перепишем как кавриацию случайной величины самой
собой. Дисперсия суммы, это будет кавриация этой суммы самой собой.
И дальше победленность кавриации. Это просто сумма всевозможных кавриаций. По всем возможным
и ежи кавриации х и т, что, в общем-то, и требуется. В этой формуле обратите внимание,
что здесь имеется в виду всевозможные пары и ежи. То есть, каждая пара считается дважды. И может
быть меньше, чем ж, и наоборот, и может быть больше, чем ж, при этом значения и ежи будут
одинаковые. И кавриации в силу симметричности тоже будут одинаковые. То есть, каждая кавриация,
на самом деле, в этой сумме считается дважды. Ну и наконец, следствие, которое я обещал, есть
из чайной величины попарно-независимая, то и дисперсия суммы равна сумме дисперсии.
Понятно, что независимость в совокупности – это более сильное
свойство, чем попарная независимость, поэтому для независимости в совокупности это тоже верно.
То дисперсия суммы равна сумме дисперсии. Это следует из-за того, что кавриация для
независимого случайных величин равна нулю. Да, то есть это очевидное следствие. То есть,
слагаем, сразу обнуляется. Если кси независимо с это, то кавриация кси-эта равна нулю.
В свою очередь, это следует из-за того, что для независимого случайных величин
отжидание произведения равно произведением от ожидания. А кавриация, как я написал выше,
это отжидание произведения минус произведением от ожидания. То есть, это будет ноль, конечно,
независимо от случайных величин. Надеюсь, что тут всё очевидно. Если есть какие-то вопросы,
пожалуйста, задавайте. Теперь мы можем вернуться к нашим двум примерам с бенемиальным
распределением и с нормальным и действительно доказать, что из этих свойств следует то,
что нам нужно. Во-первых, с бенемиальным распределением я уже говорил, что вы можете
его воспринимать как просто сумму независимого бенемиального случайных величин. То есть,
иными словами, если вы возьмёте кси-1 и кси-эн независимые в совокупности, это важно, иначе у вас
не получится в сумме бенемиальное распределение. С точки зрения дисперсии суммы это неважно,
даже если они попарно независимые, то всё равно всё получится. Но вот чтобы получить именно
бенемиальное распределение, вам нужна независимость в совокупности. Пусть кси-1 и кси-эн это
Бернулевские с параметром p независимые в совокупности. А тогда их сумма тоже имеет
бенемиальное распределение с параметром p, то есть то же самое распределение, которое у случайной
величины кси, а значит у них одинаковая дисперсия. То есть дисперсия кси будет равна дисперсии суммы
этих случайных величин. Так они независимы попарно, тем более в совокупности, то дисперсия
суммы это сумма дисперсий. Дисперсию для Бернулевских мы считали, получаем np1-p.
Ну и нормальное распределение с параметром i7². Здесь тот же трюк, который мы делали для
математического ожидания. То есть возьмем стандартное нормальное и преобразуем его так,
чтобы получилась наша нормальная с параметрами i7². Как это делается? Если взять стандартное
нормальное, то я напоминаю, что если мы умножим его, эту случайную величину, на корень i7² и
прибавим a, то мы получим нормальное распределение с параметрами i7².
Ну а значит дисперсия кси совпадает с дисперсией корень i7² это плюс а.
Далее дисперсия суммы. У нас для неё есть формула. И в этой сумме второе слагаем это
константа. А константа, она независима с чем угодно. Поэтому это есть дисперсия от первой
случайной величины, умножить на дисперсию от второй случайной величины. Но дисперсия
константа это конечно ноль, потому что отклонение константа от самой себя это ноль. У константа
ожидание тоже равно этой самой константе, поэтому это просто ноль. А есть второе, из первого слагаемого,
корень сима в квадрате выносится и возводится в квадрат. То есть получается сима в квадрате
на дисперсию это, который мы считали, для стандартного нормального распределения дисперсия равна
единице. Получается сима в квадрате. Итак, у нормального распределения первые параметры равны
математическому ожиданию, а второй равен дисперсию. Есть ли какие-то вопросы?
Есть какая-то формула для дисперсии в виде одного интеграла?
Или там придется считать два и потом вычитать?
В любом случае придется считать два интеграла, какой бы вы формулой не пользовались. Если вам
известно мат ожидания, то вы считаете один интеграл. Каким вы определением не воспользовались,
первым или вторым, у вас везде фигурирует математическое ожидание. Если будете считать,
вот так у вас будет просто один интеграл внутри, будет другой интеграл. Если будет считать вторым
образом, у вас будет разность двух интегралов. Никак по-другому вы не посчитаете.
В общем случае.
Хорошо. Теперь мы поговорим про важные неравенства, связанные с понятием математического
ожидания дисперсии. И первое из них, чтобы не убегать далеко от понятия ковариации, я вам
обещал, что я объясню, почему ковариация это мера зависимости случайных величин. Но уже в
некотором смысле мы это поняли. Если случайная величина независима, то ковариация равна нулю.
А что если независимая? Ковариация тогда может быть сколько угодно большой по модулю. Она может
быть положительная, может быть отрицательная, может быть сколько угодно большой. Поэтому вот прямо в
точности ковариацию как меру зависимости использовать не очень удобно, ее бы удобней было бы отнормировать.
Инормирует ее вот чем. Значит рассматривает так называемый коэффициент корреляции случайных
величин или просто корреляцию случайных величин. Это есть ковариация, деленная на корень из
произведения дисперсии. Вот. И утверждается, что эта величина, какие бы ни были случайные
величины, если у них есть дисперсия, то эта величина находится в отрезке от минус единицы
до единицы. И вот это уже гораздо более адекватная мера зависимости, потому что если вы
достигли экстремальных значений на границах интервалов, если это минус один или один,
это означает, что случайные величины очень сильно зависимы, а именно они линейно зависимы.
И это практически следствие, так называемое неравенство Коши Буниковского,
который вы прекрасно знаете, но давайте его напомним.
Которая можно для случайных величин сформулировать следующим образом. Пусть есть две случайные
величины такие, что у них конечный второй момент, то есть мат ожидания к си квадрата меньше
бесконечности, мат ожидания в квадрате меньше бесконечности. Тогда математическое ожидание
модуля произведения, во-первых, конечна, а во-вторых, оно меньше ли бы равно, чем корень из произведения
квадратов математических ожиданий. Из этого, конечно, сразу следует то, что коэффициент
корреляции находится в отрезке от минусы единицы до единицы, как это увидеть, но вместо кси это нужно
поставить разность между кси и мат ожиданием и этой мат ожиданием, тогда вы здесь получите,
вот здесь вы как раз получите то, что у вас в определении к вариации кси минус е кси на это
минус е это, а здесь у вас будет корень из произведения дисперсии. Вот, поэтому утверждение о том,
что коэффициент корреляции находится в отрезке от минусы единицы до единицы, сразу следует из
неравенства Кошмарниковского. Давайте, неравенство Кошмарниковского доказывается в две строчки,
поэтому давайте я его быстренько напомню, как его можно доказать. Есть разные способы.
Мода один из, на мой взгляд, простых. Ещё я как я сказал, что если вы достигли экстремального
значения, то есть если корреляция это минус е и е, то из этого следует, что случайная
причина линии независима с вероятностью 1. Мы сейчас увидим в процессе доказательств, что действительно так.
Значит, первый случай, который мы рассмотрим отдельно, это если мат ожидания 0, кси в квадрате,
в квадрате 0. Значит, в этом случае, так как кси квадрат это не отрицательная случайная личина и
свойство математического ожидания, мы знаем, что с вероятностью 1 случайная личина равна 0.
Раз она равна 0, то просто обе части этого неравенства равны 0, то есть им от ожидания
модуля произведения равна 0. Ну, кси это 0, от умножения на это она останется нулём, поэтому
мат ожидания будет 0. И корень из е кси квадрат на е квадрат тоже 0.
Ну, в силу симметрии мы тем самым разобрали из случаев, когда мат ожидания в квадрат 0. То есть
мы можем теперь считать, что оба мат ожидания и кси квадрата, и это квадраты минули.
Значит, давайте теперь рассмотрим, отнормируем нашу случайную личину для удобства. Рассмотрим
кси с волной равное кси поделить на корень из е кси квадрат, и это с волной равно это поделить на
корень из е квадрата. Понятно, что если мы сейчас рассмотрим, если мы рассмотрим мат
ожидания от кси с волной в квадрате, то мы получим единицу. Правда же, если мы сделаем кси с волной в
квадрат, мы получим просто кси квадрат поделить на корень из е кси квадрат в квадрате, то есть
просто е кси квадрат. И мат ожидания с волной в квадрате это тоже единица. Поэтому мат ожидания от
кси с волной минус эт с волной в квадрате, это есть два, ну я возложу разность в квадрат,
я получаю квадрат первого слагаемого минус двойное произведение плюс квадрат второго слагаемого.
Два мат ожидания равны единице, то есть это будет два минус два мат ожидания кси с волной эт с волной.
Давайте модули напишем, чтобы доказать именно то, что я хотел, то есть здесь будет тоже модуль
стоять. Ну вот, это мат ожидания не отрицательные случайно уличны, поэтому эту штуку не отрицать.
Откуда следует, что действительно мат ожидания модуля кси с волной эт с волной меньше равно единицы.
На самом деле это точно то, что мы хотели, потому что если мы теперь вспомним, что такое кси с волной
эт с волной, то мы получим мат ожидания от модуля кси эт, деленное на корень из произведения
кси, мат ожидания кси квадрата, мат ожидания в квадрате. И эта штука меньше, но единица по
линейностям мат ожидания это точно то, что мы хотели доказать. В каком случае достигается равенство
единицы? Равенство единицы достигается в том случае, когда вот здесь вот стоит тоже равенство. То есть
нулю должно быть равно мат ожидания квадраты разности модуля кси с волной минус эт с волной.
Значит равенство достигается тогда и только тогда, когда мат ожидания от модуля кси с волной
минус эт с волной в квадрате равно нулю. А так как случайная вещественная не отрицательна, то это
верно тогда и только тогда, когда с вероятностью 1 модуль кси с волной равен модулю эт с волной.
Это и есть на самом деле линейная зависимость. Почему это линейная зависимость? Ну потому,
что т с волной это кси поделить на константу, а эт с волной это эт поделить на константу. То есть
получается, что одна случайная вещественная линия не зависит от другой. Есть ли какие-то вопросы?
Да, это правда, но мы просто доказывали здесь нечто большее, чем то, что нам нужно. Мы доказывали
если мы доказывали просто мат ожидания произведения, то то, что нам надо. Допустим,
мы хотим рассмотреть ситуацию, когда кавариация равна единице. Что это означает? Это означает,
что в неравенстве к Кашибуниковскому, во-первых, надо модуль урать, во-вторых,
поставить равенство. И тогда мы в том месте, где я написал модуль случайных величин,
писали бы сами случайные величины без модуля и получили бы то же самое для самих случайных
величин без модуля. Просто надо аналогичное доказательство проделать для случайных
величин без модуля. Так, хорошо. Полезный неравенство к Кашибуниковскому часто применяется. В частности,
я промотивировал рассмотрение этой меры зависимости от случайных величин. Дальнейшие два
неравенства, о которых я поговорю, так называемые неравенства Маркова и Неравенства Чебышова,
это так называемые концентрационные неравенства. Отдельный очень важный раздел теории вероятности
он о том, насколько хорошо случайные величины сконцентрированы вокруг своего математического
ожидания. Вот я уже вам говорил, что дисперсию можно воспринимать как меру такой концентрации.
Чем больше дисперсии, тем сильнее случайные величины разбросаны вокруг математического
ожидания. И строгое утверждение о том, что это действительно так, называется неравенство Чебышова.
А неравенство Маркова, это тоже важный неравенство, из которого неравенство Чебышова просто явно следует.
Давайте я напомню, что такое неравенство Маркова. Так, друзья, 10 секунд перерыв,
мне нужно сходить за зарядкой для ноутбука.
Так, продолжаем. Нет, как я обещал, неравенство Маркова. Неравенство Маркова. Неравенство Маркова
звучит следующим образом. Представьте, что у вас есть случайная величина, которая не отрицательна.
Да, прошу прощения за переполох. Значит, итак, пусть кси не отрицательная случайная величина,
не отрицательная случайная величина, и пусть есть какое-то положительное число А. Ещё заодно
предположено, что мотождание кси меньше бесконечности. Тогда вероятность того,
что кси больше забрано, чем А, меньше забрано, чем мотождание кси поделительно А. Ну, я думаю,
что для многих интенсивно понятно, что действительно так, и доказательства, вообще говоря,
очень простое. Заметим, что в силу того, что кси не отрицательна, она больше либо равна,
чем А, умножительный индикатор того, что кси больше чем А. То есть, что я сделал здесь? Я взял
значение случайной величины кси, и в той ситуации, когда оно меньше, чем А, я его занулил. От этого оно
могло только уменьшиться, не могло никак увеличиться, потому что оно не отрицательное.
Я вместо отрицательного числа написал ноль. А в случае, если значение случайной величины хотя бы А,
я его тоже уменьшил, я его превратил в А. Поэтому это неравенство, конечно, верно. Если у нас есть
неравенство между двумя случайными величинами, то неравенство математического ожидания сохраняется.
Из этого следует, что мат ожидания кси больше либо равно, чем мат ожидания от А, умножительный
индикатор того, что кси больше чем А. Теперь это А можно вынести за знак математического ожидания,
и получается А, умножительная вероятность того, что кси больше чем А. Ну, чтобы получить искомое
Ну и не нравится, что осталось просто на А поделить обе части. Действительно получается, что вероятность того, что КС больше на ЧМА меньше собрана, чем в ожидании КС поделить на А.
Ну и не нравится Чебышова.
Оно звучит следующим образом. Теперь нам не важно, какой знак случайной величины, нам важно только, чтобы не был второй момент.
Значит пусть мы от ожидания КС в квадрате меньше бесконечности, и пусть ε это какое-то положительное число.
Тогда вероятность того, что модуль КС минус мы от ожидания КС, хотя бы ε, меньше либо равна, чем дисперсия КС поделить на ε в квадрате.
И это как раз о том, что дисперсия является мерой отклонения, извините, случаемой.
Вероятность того, что модуль КС минус мы от ожидания КС, хотя бы ε, меньше либо равна, чем дисперсия КС поделить на ε в квадрате.
И это как раз о том, что дисперсия является мерой отклонения, извините, случаемой.
То есть чем меньше дисперсия, тем меньше правая часть этого неравенства.
И соответственно, тем меньше вероятность того, что отклонение, вероятность отклонения, случаемой, мы от ожидания на ε.
Это очевидное следствие неравенства Маркова. Достаточно свести просто одно неравенство к другому.
Как это сделать? Ну, давайте в качестве вот этого случайного числа КС в неравенстве Маркова возьмем модуль КС минус ЕКС.
То есть возьмем случайную числу. Это равный модуль КС минус ЕКС.
Это будет в квадрате, извиняюсь, КС минус ЕКС в квадрате.
Это будет случайная величина из неравенства Маркова, которая в неравенстве Маркова обозначена КС.
А в качестве А мы возьмем просто Экс. Значит, по неравенству Маркова,
по неравенству Маркова,
так как это не отрицательно, мы имеем право неравенство применить,
то есть мы получаем вероятность того, что это больше либо равна, чем Эксимум в квадрат,
меньше либо равна, чем мотождание Экса поделить на Эксимум в квадрате.
А мотождание Экса, это есть дисперсия.
Это есть дисперсия.
Тем самым неравенство к бушову доказано.
Есть какие-то вопросы.
Ну и как пример неравенства применения и неравенства к бушову,
давайте докажем очень важное утверждение, которое называется закон больших чисел в форме к бушову.
Вот наконец-то у нас первая предельная теория.
Закон больших чисел.
Чисел в форме к бушову.
Ну это утверждение о том, что если вы будете тысячу раз подбрасывать монетку
и поделите количество решек на тысячу, то будет примерно 0,5.
Это формально звучит следующим образом.
Это важный момент.
Это тот момент, когда мы наше теоретическое определение вероятности свели к физическому определению вероятности.
Как я уже говорил, исторически сначала вероятностью считалась частотность появления события,
а потом после того, как появилась Эксиматика Калмогорова,
наоборот, из такого теоретического определения вероятности, как вероятности меры,
мы получаем физическое определение вероятности,
что действительно просто совпадает в пределе,
совпадает с частотностью происхождения события, когда вы его независимо много раз повторяете.
Итак, пусть X1, X2 и так далее – это бесконечная последовательность независимых случайных величин.
Независимых совокупностей?
Это хороший вопрос, да, на самом деле попарно независимости достаточно,
но когда я пишу слово «независимость» и не уточняю, что он попарно, я имею в виду независимость совокупности.
Почему я здесь пишу более сильное условие, не ослабляя его?
Потому что я хочу продемонстрировать именно такой закон больших чисел,
который лежит в основе физического определения вероятности.
Я его докажу, потом сразу сформулирую обобщение, и все места, которые можно ослабить, я ослаблю.
Здесь я имею в виду независимость совокупности.
Пусть X1, X2 – это независимые случайные величины.
Давайте опять слишком сильное утверждение по требованию,
что они одинаково распределены. Это тоже не обязательно, можно это ослабить.
Одинаково распределенные.
Давайте еще считать, что у них есть мотождание дисперсия.
Мотождание кситы равно a. Дисперсия кситы равнеет c2.
И давайте еще введем какое-то положительное число.
Тогда вероятность того, что кси1 и талия плюс кси n поделить на n
минус a по модулю больше, чем епсилон,
да, то есть вероятность того, что империческое среднее
отличается от теоретического среднего, больше, чем на епсилон,
стремится к нулю, примерно к стремящемуся бесконечности.
Давайте это дважды поделим.
Вот здесь у нас есть мотождание дисперсии.
И вот здесь у нас есть мотождание дисперсии.
Мы будем показывать об обобщении без конечности.
Давайте это дваждение докажем.
Или давайте проще поставим.
Я сразу сформулирую об общении,
и будем сразу доказывать об обобщении.
Еще раз, значит, вот этот закон, который я здесь формулировал,
чем обобщение. Во-вторых, он более прикладной. То есть вот это физическое
понятие вероятности о том, что вы вероятность воспринимаете как
частотность появления события для именно независимых испытаний. Это как раз
вот закон больших чисел в форме ЧБ-шоу в таком оригинальном виде. Но он очень
легко обобщается до следующего. Это как-то связано с сходимостью по мере?
Это в точности сходимость по мере, да, это в точности сходимость по мере, которая в теории вероятности
называется сходимость по вероятности. Мы об этом еще поговорим чуть позже.
То есть это сходимость на самом деле по вероятностной мере, вот этого отношения
к сиадинку стали, плюс к сиадинку поделить на m, к константе a. Так, значит обобщение. Пусть есть
попарная независимость. Знаете, даже не нужно попарной независимости, можно еще ослабить попарной
некоррелируемость, так называемая. То есть попарная кавариация равна нулю. Это еще более слабое
свойство, чем попарная независимость. Потому что из того, что кавариация равна нулю, следует
независимость. Прошу прощения, из того, что есть независимость следующая кавариация равна нулю,
но обратная вообще говоря не нет. То есть может такое быть, что кавариация ноль, а при этом никакой
независимости даже по парной нет. Значит пусть есть последствия случайных величин таких, что для
всех различных i и g кавариация x и t, x и g, t равна нулю.
Что еще надо сказать? Надо еще сказать, что все дисперсии ограничены. То есть нам одинаковая
распределенность на самом деле нужна была для того, чтобы дисперсии были одинаковые,
но одинаковые дисперсии тоже не обязательно, достаточно чтобы они были ограничены. Пусть
существует такая константа c, что для любого i дисперсия x и t меньше чем c.
Ну и на самом деле этого достаточно. Давайте еще для удобства обозначим сумму первых
m случайных величин за s, и рассмотрим опять какое-то epsilon больше нуля,
и рассмотрим произвольную последовательность w, m, которая стремится к бесконечности с ростом m.
Тогда вероятность того, что модуль sn минус мотождание sn поделить на корень из n умножить
на w, m больше чем epsilon стремится к нулю при настримящемся бесконечности. Как увидеть,
что это действительно обобщение закона больших чисел в форме Чебушева. Ну смотрите,
надо в качестве w, m взять корень из n, и вы получите в точности первый вариант,
то есть более слабый вариант, вы получите в точности zbch в форме Чебушева. Ну смотрите,
действительно, если я возьму в качестве w, m корень из n, я узнаю, что я получу n. А мотождание
суммы, если случайные величины одинаково распределены, у них у всех мотождания одинаковые,
то мотождание суммы это n умножить на 1 мотождание, nA. Поэтому nA с n сократится,
останется просто A, и вы в точности получите вот это выражение из просто закона больших
чисел в форме Чебушева. То есть закон больших чисел в форме Чебушева далек от оптимальности,
вот этот n знаменатель, это очень грубо, это n можно заменить на любую функцию,
которая растет в бесконечности быстрее, чем корень из n. Вот, а доказательство идентичное,
то есть это обобщение просто рождается из того, что вы берете доказательство закона больших
чисел и смотрите, с чего это доказательство можно ослабить. Поэтому давайте сразу этот
усиленный вариант доказывать. Значит, дальше у нас в курсе еще будет усиленный закон больших
чисел, его не надо путать с обобщением закона больших чисел, о котором я вас говорил. Значит,
если в обычном законе больших чисел, как правильно было замечено, речь идет про сходимость по мере,
то в усиленном законе больших чисел будет идти речь про сходимость почти всюду. Так, это все один
и тот же закон больших чисел, то есть это не то чтобы какое-то супермощное обобщение. Хорошо. Так,
что делаем? Пользуемся неравностью Чебушева, смотрим на нашу вероятность, которую мы должны
с вами оценить. Да, конечно, она стремится к нулю, вероятность того, что модуль СН минус ЕСН
поделить на корень ЕЗН ВН больше чем ЭСН. И умножаем обе части неравенства на знаменатель,
получим вероятность того, что модуль СН минус ЕСН больше чем ЭПСИВАН корень ЕЗН ВН.
По неравенству Чебушева это меньше ебраночьим дисперсия СН
поделить на квадрат правой части, то есть ЭПСИВАН квадрат НВН квадрат. Дальше, в силу того,
что все кавариации ноль, я напомню, что дисперсия сумма, это сумма дисперсии плюс все возможные
кавариации разных случайных величин. Они все ноль, поэтому дисперсия сумма, это просто сумма
дисперсии. Ну и вы уже видите наверняка, почему это сумма дисперсии с нулю, потому что мы
предположили, что все дисперсии ограничены одной и той же константой С, поэтому числитель не
превосходит С умножить на Н ЭПСИВАН в квадрате НВН в квадрат, Н сокращается, и вот то место,
где мы понимаем, что нам корень ЕЗН нужно домножить на растущую функцию, потому что корень
ЕЗН возвелся в квадрат, с числителем сократился, и вот именно эта растущая функция осталась
в номинации. Что и требовалось, есть ли какие-то вопросы?
Ну и последнее неравенство, последнее утверждение на сегодня, которое нам в курсе тоже потребуется,
я сразу продемонстрирую пару очевидных применений, это неравенство Йенцина.
Неравенство Йенцина. Ну в частности, из этого неравенства следует, что дисперсия не отрицательна,
хотя это так очевидно, да, и следует то, что мотождание модуля больше собрано, чем модуль
мотождания. Неравенство Йенцина, это некоторые утверждения, которые это все обобщают. Значит, пусть есть
некоторая выпуклая бареллевская функция, некоторая выпуклая бареллевская функция,
ну вот, например, квадрат или модуль, да, пусть Ж, ну бареллевская, да, когда мы говорим выпуклая
функция, мы, наверное, считаем, что она непрерывная. Тогда, наверное, про бареллевость говорится
необязательно, пусть просто уже выпуклая, она автоматически бареллевская, которая действует из РВР.
И пусть мотождание модуля ФС меньше бесконечности, если это случайно у ЧНАС в конечном первом моменте.
Тогда
мотождание Ж от Си больше собрано, чем Ж от мотождания КСи.
Вот, ну в частности, если вы вместо Ж подставите квадрат или модуль, вы получите, собственно,
известные вам свойства математического ожидания. Во-первых, то, что мотождание модуля больше
вон очень модным от ожидания, во-вторых, то, что беспрерывный интерцептор. Значит,
как это утверждение доказать в общем случае? Ну, довольно просто, надо просто воспользоваться
определением выпуклости. Значит, что означает функция выпуклая? Ну, это означает, что какой бы вы
не взяли, какой бы вы не взяли аргумент, вы через точку, если вы возьмете график функции,
какой бы вы не взяли аргумент, через соответствующую точку на графике функции вы можете провести прямую,
так что у вас целиком вся кривая окажется выше этой прямой. То есть, иными словами,
любого х0, давайте прямо запишем то, что я сказал, любого х0 вы можете выбрать прямую,
но на самом деле у вас точка фиксирована, через которую вы эту прямую проводите,
поэтому на самом деле вы можете выбрать наклон. Существует некоторая гамма, лямбда от х0 такое,
что g от x, это такое, что для всех вообще x, g от x минус g от x0 больше всего равно, чем лямбда от x0 умножить на x минус x0.
Это точно то, что я сказал, потому что прямая, которая проходит через эту точку,
она лежит полностью ниже, чем весь график функции, только не единственная в этой точке x0,
его пересекательность касательная. Теперь давайте сделаем следующее, давайте в качестве x0 возьмем
мотождание x, а в качестве x возьмем x. Что мы получим? Мы получим, что g от x минус g от мотождания x
больше равно, чем лямбда от мотождания x умножить на x минус мотождание x.
А теперь возьмем мотождание от обеих частей этого неравенства.
Да, имеем право, потому что мы знаем, что от правой части мы мотождание можем взять из-за того,
что мотождание может быть конечное. То есть у меня в правой части все не случайно, кроме x.
Да, в этой правой части только x случайно, в состоянии не случайно, поэтому мотождание от правой части
существует. И значит верно следующее неравенство, что мотождание от g от x минус g от мотождания x
больше либо равно, чем мотождание от правой части. По линейности это есть лямбда от x, это константа.
Умножаем отдел на мотождание x минус мотождание x, то есть на ноль. И по линейности в левой части
мы получаем, что мотождание от g от x минус g от мотождания x это тоже какая-то константа,
больше нулю, что и требуется. Тем самым неравенство янса тоже доказано.
Есть ли какие-то вопросы?
Ну сразу примеры, как я обещал, я это устно произнес, давайте еще пропишу на всякий случай.
Для тех, кому будет лень пересматривать это видео, кто будет смотреть только конспект,
вдруг такие люди найдутся. Те примеры, которые мы с вами уже разбирали. Во-первых, если в качестве g
выбрать с модуль x, вы получите, что мотождание модуля x больше либо равно, чем мотождание от
x. Во-вторых, если в качестве g от x выбрать x квадрат, то вы получите известное вам
не отрицательное с дисперсиумом. Отжидание x квадрата больше равно, чем квадрат мотождания.
Значит, дальнейшая тема, о которой мы с вами будем говорить, это условное математическое
ожидание. Это, наверное, одна из самых сложных тем этого курса, если не самая сложная. Я не хочу
разрывать эту тему на две части. Я по-любому придется разрывать. Мы за одну лекцию целиком
успеем полностью обсудить все, что касается условного мотождания. Но я оставшиеся восемь минут
не хочу тратить на то, чтобы давать определение, потому что я боюсь, что ухудшится понимание. На
следующей лекции мы целиком с вами разберем большой блок, касающийся мотождания, чтобы было
понятно условное мотождание. Давайте я пару минут потрачу на то, чтобы объяснить, о чем там
пойдет речь и что это вообще с условным мотожданием такое с точки зрения интуиции. Я когда
говорил про математическое ожидание, я на следующей лекции еще это повторю, но просто чтобы
анонсировать. Когда я говорил про математическое ожидание, одна из мотиваций к его рассмотрению
была статистическая. То есть мы говорили о том, что если мы хотим предсказать, например, в будущем
значение случайной величины, мы проведем много раз эксперимент, посмотрим, чему равно значение
в среднем. Ну и скажем, значит где-то в будущем мы ожидаем вот это значение, ну или примерно его.
Представьте, что у вас есть некоторый процесс. Это не просто одна и та же случайная величина,
которую вы много раз реализуете, а стоимость цена какой-нибудь акции, валюты, там еще что-то,
что изменяется непрерывно во время, в общем изменяется. И у вас на самом деле
сегодня одно распределение, а завтра уже какое-то другое. И довольно сложно делать прогнозы. В принципе,
если вы знаете какое распределение там вашей случайной величины, там вашей стоимости акции,
еще чего-то будет через год, вот вам интересно на самом деле, допустим, что там будет через год
с вашим случайным процессом. Вот вы знаете, скажем, средний через год, ну например. Ну вы скажете,
ну окей, я ожидаю, что значение будет равно 100, потому что 100 это ваше среднее. А теперь представьте,
что прошло полгода, и за эти полгода появилось очень много информации относительно того,
как ваш процесс был устроен. И вы уже, конечно, будете предсказывать нечто другое, потому что за
счет того, что вы стали обладать большим объемом информации, ваше предсказание может очень сильно
изменить. Вы снова в качестве предсказания будете использовать среднее, но это среднее уже будет
функцией от истории. То есть вот за эти полгода, как изменялось значение вашего случайного
процесса, будет влиять на ваше среднее. То есть условное математическое ожидание — это функция от
условия. В качестве условия мы подразумеваем, вот как раз историю, вот у вас есть какая-то
история вашего случайного процесса в течение полгода. В зависимости от того, какая, какая
у вас история есть, вы получаете то или иное значение от ожидания. Вот это и есть условный
от ожидания, о котором мы с вами в следующий раз будем говорить. В общем-то, на этом все.
Если есть какие-то вопросы, пожалуйста, задавайте. Если вопросов нет, всем спасибо.
До встречи в следующую субботу. Хорошей недели. До свидания.

Так, всем добрый вечер. В прошлый раз мы разошлись на довольно печальной ноте. Мы построили алгоритм,
который не работает. Я напомню, у нас было простое равномерное хеширование. В чем его идея?
У нас есть множество всевозможных хэш-функций, где к – это соответственно множество ключей,
которые теоретически может сохранить наш хэштаблицу данную, и хэштаблицу, соответственно,
как у нас было устроено. Это был просто массивчик размера m. И в каждой ячейке,
которого у нас хранится список элементов, которые попадают в эту ячейку. Грубо говоря,
список значений, обладающих одним и тем же хэшом. В данном случае у нас есть h от x,
значение h от x совпадает с значением h от y, совпадает с значением h от z,
и все они равны значению 2. Такая ситуация называется коллизией. Мы сказали, что хорошо,
у нас есть коллизия, давайте просто-напросто все элементы с одинаковыми хешами хранить в списке,
а соответственно поиск будет рассуждаться следующим образом. Мы идем в соответствующий
элемент массива, и дальше обычным линейным поиском в этом списке ищем элемент. И соответственно
вставка, удаление происходит абсолютно точно так же. У нас была с вами теорема,
которая говорила о том, что если из этого множества выбирать хэш-функцию абсолютно случайно,
равновероятно, то есть каждая функция имеет одинаковую вероятность быть взятой,
то в этом случае у нас достигает следующая оценка, что на среднее время работа find
являлась о большим от 1 плюс n деленное на m. Дальше мы сказали, что если мы вот этот
коэффициент альфа n на m поддерживаем, скажем так, меньше либо равным единице,
как мы это делаем, просто при необходимости расширяем нашу таблицу, вот эту m соответственно
увеличиваем вместе с n. В этом случае соответственно у нас получается, что время работы тех операций от
единицы, но при этом в среднем. Опять же повторюсь, к сожалению, просто равномерно
хэш-функции на практике недостижимо, его практически невозможно реализовать на практике,
поэтому приходится выдумывать нечто иное. Сегодня мы как раз-таки, во-первых, разберемся с
этой проблемой, построим все-таки множество хэш-функций, которые будут тультворять вот этим
всем соотношением, то есть мы эти оценки на самом деле, мы добьемся этих самых оценок. Кроме того,
мы построим структуру данных, которая позволит нам хэшировать так, что у нас в худшем случае
поезд будет отчисляться за единицу, то есть единица будет не в среднем, а в худшем случае.
Но давайте обо всем порядке. Сначала давайте поговорим про так называемые универсальные
семейства хэш-функций. Тут повторена проблема, которая была в прошлый раз. Действительно,
мы рассмотрели просто равномерное хэширование и поняли, что на практике его сложно добиться по
нескольким причинам. Во-первых, как минимум, его достаточно сложно реализовать, в том смысле,
что если мы хотим сохранить какую-нибудь функцию из вот этого множества, то нам, соответственно,
понадобится очень много памяти. Но, соответственно, в этом месте, где много памяти,
нам требуется много времени просто на то, чтобы создать единственную функцию,
которая не подойдет. Ну, и, вообще говоря, если мы воспользуемся тем планом, который у нас был
до этого, то скажем, нам приходит какой-то элемент, мы его запоминаем и запоминаем его
хэш-взначение, а если нам приходит какой-то новый элемент, то соответственно мы генерируем для него
новое хэш-взначение. Но как мы приходим к тому, что нам в принципе для того, чтобы реализовать
хэш-функцию, нужно реализовать хэш-таблицу. Да, то есть такой парадокс. В общем, проблемы вспомнили.
Как мы будем решать? В общем-то, идея на самом деле очень простая. Идея, которую мы будем следовать на
сегодняшней лекции, будет такая. Ну хорошо, смотрите, вот это вот множество множества всех хэш-функций,
вот оно, множество хэш-функций, оно довольно большое. Да, его довольно сложно представить в памяти,
то есть как бы его довольно сложно хранить в памяти, к минимуму, да. То есть, если все-таки у вас
функции случайные, то, наверное, это означает, что для каждого элемента, для каждого входного данного,
вам нужно хранить его соответствующие, ну соответствующие отображения. Да, вот. Ну а давайте
воспользуемся следующей идеей. Ну хорошо, вот это множество хранить очень сложно, но принципе его
описать довольно сложно. А давайте из этого множества выделим какое-нибудь небольшое
подмножество, маленькое подмножество, которое, во-первых, будет просто описано, ну допустим,
это множество всех линейных функций или множество всех квадратичных функций. Да, короче говоря,
просто выделим какое-то подможество функций из огромного, ну из всевозможных
функций. Это во-первых. А во-вторых, давайте скажем, что вот это вот маленькое множество
пусть обладает тем же самым свойством, что и наше исходное большое множество.
Ну а каким приятным свойством обладает наше исходное большое множество? Ну она
обладает тем свойством, что если мы случайно из него выбираем хэш-функцию,
то вероятность того, что у двух произвольных элементов, не равных друг
другу, будет коллизия, она равна 1 на m. Ну собственно, по сути, из этого следовала
вот эта самая оценка в теореме. Мы говорили, что так как вероятность совпадения
двух хэш-взначений на двух разных элементов равна 1 на m, то
соответственно отсюда следовала вот эта оценка. Ну и скажем, давайте следующую
вещь. Давайте тоже возьмем какое-нибудь маленькое подможество из этого большого
множества и скажем, что если вероятность вероятность совпадения хэш-взначений
вот в этом маленьком множестве будет меньше равной чем 1 на m, то
соответственно, мы победили. У нас будет та же самая оценка. Согласны? Давайте формально
опишем, что я хочу сказать. Пусть, как обычно, у нас есть множество k. Вот это множество
всех ключей. Ну и размер каштаблицы m. Ну что такое размер каштаблицы? Это просто
размер вот этого массива списков. Далее, m-1. Вот. Тогда скажем следующее, что семейство
функций h, которое является под множеством всевозможных хэш-функций, называется
универсальным. Вот это семейство. В том случае, если для любых x и y, таких, что x не равно y,
при равновероятном выборе h из этого множества будет выполнено следующее. Вероятность того,
что h от x будет равен h от y меньше вероятно, чем 1 на m. Окей? То есть мы взяли какое-то
под множество из исходного большого множества, ну грубо говоря, мешок функций. Даже случайно
взяли. Дальше проводим следующий эксперимент. Случайно выбираем функцию из этого мешка,
да, и смотрим, соответственно, совпало ли у меня h от x с h от y. При этом x и y они фиксированы,
да, какие-то. Ну абсолютно произвольные, фиксированные. Вот берем произвольные,
фиксированные x и y и вытаскиваем оттуда хэш-функции. Вот. И смотрим, в каких случаях у нас эти
хэш-взначения совпали. И вот если доля случаев, в которых у меня хэш-взначения для x и y
совпали меньше, чем 1 на m, то мы будем говорить, что вот этот мешок с функциями, то есть вот этот
семейство функций, оно будет в универсальном. Окей? Понятно определение? Какой вопрос?
Окей. Да, ну то есть смысл такой, что какую бы вы пару x и y не взяли, доля функций вот в этом множестве
функций h, на которых они дают коллизию, не превосходит 1 делить на m. То есть, грубо говоря,
для любой пары x и y у вас количество функций, на которых вот на этой конкретной паре будет
достигаться коллизия, очень мало. Ну очень мало, это 1 деленное на m в наших терминах. Окей? Вот. Ну
соответственно, я думаю, пометую о прошлой лекции, вы можете задать естественный вопрос. А такое
существует? Если простого равномерного кшера нет, может и такого не существует в принципе. Вот. Но
сейчас я покажу, что на самом деле пример вот такого семейства привести можно. И, собственно,
вот он. Вот он пример. Давайте о нем, давайте просто смотрим на пример, а вывод будем
параллельно делать. Значит, давайте рассмотрим следующую задачу. Вот пусть у меня множество
ключей, в конкретном случае, пусть у меня множество ключей. Это система вычетов по модулю p. Ну то есть,
грубо говоря, просто значение 0, 1, 2 и так далее. p-1. Ну где p это некоторое простое число.
Вот. И, значит, давайте я скажу следующее. Давайте в качестве универсального семейства хэш-функции,
точнее в качестве множества хэш-функций допустимых, возьму вот такое множество. Значит,
это будут все такие функции h, которые сдаются следующим уравнением. Значит, h от x у нас равно
ax плюс b, остаток отделения на p. И от этого всего еще берем остаток отделения на. Вот. При этом
a и b пробегают в следующее значение. Значит, a лежит в том же самом zp, но без нуля, а b абсолютно
произвольная из zp. Вот. Вот. Обратите внимание, смотрите, согласны ли вы, что не любая функция,
не любая функция, отображающая k в 0 и так далее, m-1, описана вот здесь. Согласны, что это
некоторая подножество все-таки всевозможных функций. Это, во-первых. А, во-вторых, это множество
очень легко описать. Точнее произвольную функцию h отсюда очень легко описать. Ну давайте там в
терминах вашего любимой c++. Вот как сохранить функцию h, которая сдается вот таким вот уравнением.
Да, мы просто храним на самом деле, то есть тут по сути есть такое отражесление между функцией h и
парой чисел ab. То есть, грубо говоря, вы просто храните 2 int. Все. То есть мы избавились от этой проблемы,
что на то, чтобы хранить отдельную хэш-функцию, вам требуется много памяти. Вот чтобы сохранить
вот любую хэш-функцию из этого множества, вам достаточно всего лишь сохранить 2 int и все. На самом
деле, да, 2 unsigned int. Вот. Понятно? Вот. Но осталось лишь убедиться в том, что действительно, если я
буду выбирать случайно хэш-функцию из вот этого множества, а не из всего большого множества, то у
меня будут очень маленькие вероятности коллизий. Вот это осталось доказать. Давайте, собственно,
это проведем. Значит, как мы это будем доказывать? Ну, во-первых, по определению универсального
семейства хэш-функции, что мы должны сказать? Мы должны сказать, что для любой пары x и y мы должны
показать, что вероятность того, что хэш значение от x совпадает с хэш значением от y с очень малой
вероятностью. Ну, давайте возьмем просто произвольные. Возьмем и зафиксируем произвольные
x и y, принадлежащие к, ну, при этом x не равны y, естественно. Вот. Давайте для конкретной
произвольной пары докажем, что вероятность коллизий на них будет мала. Значит, ну, давайте еще
отдельно рассмотрим значение х штрих, которое будет равно ax плюс b процент p, и y штрих равный
ay плюс b процент p. Ну, просто это х-начение, да, без вот этого последнего взятия остатка.
Вот. Значит, чего начнем? Давайте первым делом покажем, что х штрих не равно y штрих. Пока
мы забудем про него. Пока они меня интересуют о х штрих и y штрих. Понятно ли, что х штрих не
равен y штрих 100% без всяких вероятностей? То есть, какую бы я хэш функцию не взял, у меня
гарантированно х штрих не будет совпадать с y штрих. Ну, почему это так? Ну, давайте, хорошо,
давайте просто наоборот рассмотрим х штрих минус y штрих и покажем, что оно не равно нулю.
Значит, вот если я вычитаю два остатка, то все рифметику остатка знают, что разность двух
остатков это просто остаток разности. Давайте, собственно, просто возьмем разность вот этих
штук и получим ax-y%p. Согласны? Ну, бшки ушли, осталось ax-y%p. Ну, а вы верите, что вот эта штука не
равна нулю? Ну, не равна нулю. Ну, действительно, почему? Давайте вот тут. Можно уже спойлер посмотреть.
Ну, действительно, вот эта штука не равна нулю. Почему? Потому что х, во-первых, не совпадает с
y, а это два вычта. То есть, если вы один вычет вычитаете другого, при этом они не равны другому,
то, естественно, ноль вы не получите. Ну, и при этом a, заметьте, а у нас вот такое,
а не равно нулю, да, а гарантирован не делитель p. Соответственно, а не делитель p, x-y, простите,
а p не делит a, и также p не делит x-y, и при этом p простое, поэтому, соответственно, произведение
тоже не делится на p. Окей? Что еще раз? А у нас гарантирован вот отсюда. А это какой-то
остаток пределения на p, но при этом не нулевой. То есть, ну, грубо говоря, а не делится на p. Вот все.
А не делится на p, x-y не делится на p. Так как p простое, то и произведение тоже не делится на p.
Логично? Все, следовательно, разность х' и y' не равна нулю, соответственно, х не равен y'. Все.
Вопрос?
А как x-y может быть равен p, если x и y лежат в остаком множестве?
x и y это два вычета, это два вычета, причем различные. Они не могут быть равны нулю априори.
Второй пункт. Неочевидный. Вот, смотри, давайте посмотрим вот на эти два уравнения. И вот я утверждаю,
что, на самом деле, вот эти два уравнения, они задают мне объекцию между, давайте так скажу,
парами x' и y' всевозможными и AB. Ну, естественно, прификсированным x и y.
Ну, короче, грубо говоря, вот у меня есть какие-то x и y фиксированные, и если я буду перебирать
всевозможные пары A и B, то это будет означать, что я на самом деле перебираю всевозможные пары
x' и y'. Понятно? То есть, любому заранее заданному x' и y', я могу подобрать такие A и B. И наоборот.
Собственно, произвольные A и B, ну это естественно, произвольные AB задаст мне какую-то пару x' и y'.
Понятно? То есть, грубо говоря, мне даны какие-то произвольные абсолютно x и y, а я всегда могу
подобрать такие A и B. Ну, мне даны произвольные x и y, вот. И дальше я хочу получить какие-то
x' и y'. И вот утверждается, что я всегда смогу подобрать такие A и B, что у меня такое получится
преобразование. Переобразование из x и y в x' и y'. Вот, мне дают фиксированный x и y. Вот вы даете
мне произвольный x и y, 5-10. Вот. А я хочу, чтобы они в итоге отображались у меня в, не знаю, там,
7 и 8. Я всегда смогу подобрать такие A и B, что у меня такое получится. Вот. Вот. Ну, грубо
говоря, вот это вот, то есть, грубо говоря, вот это вот соотношение задает необъекцию между парами A и B
и x' и y'. Ну, естественно, при условии, что у меня x, y и p, они фиксированы. Но они у меня
фиксированы, потому что я их заранее выбрал и зафиксировал. Окей? Вот. Давайте вот покажем. Ну,
на самом деле, показать это несложно. Давайте просто насмотрим систему уравнений вот эту.
AX плюс B процент P. AY плюс B процент P равно x' и y'. Ну, и тут можно рассуждать по-разному.
Можно сказать, что, на самом деле, система вычетов, там, она образует поле, поэтому тут можно применить
ваш любимый метод и решение системы линейных уравнений, там, неважно, через детерминат или
через что-то еще. Вот. И показать, что эта система имеет единственное решение. А можно просто просто
в тупую выразить, не знаю, там, либо B, либо A из первого уравнения и поставить его во второе. Вот. То есть,
грубо говоря, ну, как бы вы не рассуждали, да, смотрите, что вот отсюда можно сказать. Ну,
давайте через матрицу пойдем. Вот тут чему равна матрица? Тут матрица у меня x1 и y1. Да? Вот. Если
A и B, то у меня, на самом деле, эти самые переменные. Вот. То перед A стоит коэффициент x, перед B стоит
коэффициент 1, перед Y стоит коэффициент A, перед B стоит коэффициент 1. Вот. В итоге получается такая
матрица. Эта матрица, естественно, детерминат этой матрицы, естественно, не равен нулю.
Прогласны? Почему? Потому что у меня x не равен y. Не шокируйтесь. Вот. Ну все, из-за этого, собственно,
следует, что вот эта система имеет ровно одно решение. Окей? Для любого x' и y' я могу подобрать
только единственный A и B, который даст к нему такое преобразование. Окей? Нормально?
Так, это что мы сейчас таким образом доказали?
Мы доказали, что у вас есть множество x' и y', и у вас есть множество пар A, B. И мы показали,
что мы, на самом деле, показали вот так, что любому элементу отсюда соответствует единственный
элемент вот отсюда. Да? Вот. Но осталось нам показать, что действительно все элементы из A, B,
они будут соответствовать каким-то элементам из x' и y'. Окей? Но это на самом легко видеть. Почему?
Потому что если мы посмотрим на мощность множества всех пар A, B, то почему она равна? Ну не совсем.
P на P-1. Почему? Ну, правила произведены, во-первых, во-вторых, потому что у A не
может равняться нулю, а B может быть произвольным вычетом по модулю P. Окей? Что? Нет, панофиксировано.
Вот нам просто дано такое множество K. Вот нам известно, что все элементы, которые будут
переходить к нам в кэш-таблицу, они вот лежат вот в таком множестве, где P это некоторое простое число.
Ну оно по условию задачи нам дано. Вам сказано, что, не знаю, вам подаются на вход все числа от 0 до 16.
Вот. Значит, в этом случае P равно 17. Короче говоря, можете считать, что если вы обработаетесь
на то, то можете считать, что P это какое-нибудь очень большое число, простое. Вот мы сейчас, соответственно,
тут помещаются все инты и так далее. Но P заранее фиксированное число, мы его не подбираем. Оно вот в условии
задачи нам дано. Вот это это условие задачи. Вот. А чему равно множество всевозможных пар х' и х'?
Вот. Вот это х'. Какое количество пар х' и х'? Почему P-1? Да, потому что мы в первом боксе доказали,
что х' не может быть равно х'. Это единственное ограничение, окей? Все, а всего количество пар,
всего количества несовпадающих пар х' и х', где х' принадлежит от 0 до P-1, и х' принадлежит от 0 до P-1,
и при этом они совпадают вот в точно все равно вот это. То есть, на самом деле, вот эти множества,
они совпадают, да, и у вас для каждого, для произвольного х' и х' существует ровно один Аб,
в который он отворажается. Окей? Вот. Соответственно, доказали биективность.
Окей, нормально. Смотрите, что таким образом получилось.
Что мы сказали? Мы с вами сказали, что вот у нас есть вот это множество, вот это универсальное множество,
пока оно универсальное, да, просто какое это множество. Как я генерирую случайную функцию из него,
из этого множества? Ну, как загенерирую случайную функцию из этого множества?
Да, просто случайно генерируем А и случайно генерируем evenly нужен,
А просто случайно генерируем a, и случайно генерируем b, вот.
А теперь смотрите, что мы показали во втором пункте.
А во втором пункте мы показали следующее.
Смотрите, вот мне приходит какой-то x и какой-то y, абсолютно произвольно, я их не знаю.
Они приходят и так далее.
И я случайно выбираю a и b.
Что это на самом деле означает?
Если я случайно выбираю a и b, то я на самом деле абсолютно случайно выбираю их образы, согласны?
Случайно выбираю образы x' и y'.
То есть что получается, вы мне подавите на вход x и y,
а я абсолютно случайным образом перевожу их в какие-то другие два числа.
Понятно?
Абсолютно рандомно.
Согласны?
Но так как a и b я выбираю случайно, а между a и b и x' и y' у меня существует би-эксия,
то это означает, что по сути я x' и y' выбираю случайно.
Окей?
Все, то есть теперь я на самом деле могу просто анализировать тот случай,
в котором у меня x' и y' абсолютно случайно.
Ну вот ровно то, о чем мы говорили в прошлый раз, да?
То есть мы, вообще говоря, не можем говорить о том, что у нас данные какие-то случайные.
Потому что нам обязательно кто-нибудь подсунет какие-нибудь, в общем, стремные случаи,
на которых у нас, конечно, таблица будет падать.
Ну, будет работать долго.
А тут я говорю, что мне, вообще говоря, неважно, какие вы мне x' и y' не дали,
я все равно подкину монетку и переведу их в абсолютно два случайных числа.
И вот тут я уже могу анализировать средний случай.
Могу анализировать, что у меня происходит там, с какой вероятностью.
Ну, соответственно, давайте третий пункт, последний.
Наконец, докажем то, чего мы хотим.
А хотим мы следующего.
Мы хотим, чтобы вероятность...
Ну, мы хотим проверить, что вероятность того,
что h от x будет равно h от y,
будет меньше вероятно, чем 1 на m.
Вот это мы должны доказать.
Ну, давайте начнем.
h от x равно h от y.
Ну, это мы перепишем следующим образом.
x' процент m равно y' процент m.
Согласны?
Ну, вот, определение x',
вот такое.
А функция h, она вот такая.
То есть, по сути, x' процент m.
Вот.
Ну, и тут, смотрите, у меня x' абсолютно случайное,
y' абсолютно случайное, при этом они не равны друг другу.
С кой вероятностью совпадают
остатки случайного числа x'
и случайного числа y'?
Ну, 1 на m пока слишком сильно.
Так.
Давайте порисуем.
Вот у меня есть числовая прямая.
Ну, пусть тут.
m, m, 3m, и так далее.
Тут какой-то km, и тут значение p.
Вот, ну давайте еще вот так.
Ну, смотрите, тут давайте снова воспользуемся
не меньшим, а давайте просто равно пока напишем.
Воспользуемся обычным школьным определением вероятости.
Количество благоприятных сходов
деленное на общее количество сходов.
Что у нас тут будет являться исходом?
Ну, исходом будет являться
просто какая-то пара x' и y', окей?
Какое количество пары x' и y' у меня существует?
Уже считали.
Да, то есть p на p-1.
А теперь давайте посчитаем количество пар x' и y',
у которых будут одинаковые остатки определений на m.
Ну, смотрите, ну тут все просто.
Сколькими способами можно выбрать x?
p.
То есть выбираем произвольный x.
Но, допустим, он попал куда-то вот сюда.
Вот где-то здесь x.
Вот для этого конкретного x
сколько существует y'
которые совпадают с ним по остатку?
Сколько?
Ну, давайте просто нарисуем.
Да, вот этот y подходит.
А вот этот y подходит.
Нет, x' не совпадает с x'.
Еще подходит вот этот y,
ну и, возможно, вот этот y, да?
Ну, тут не факт,
но теоретически может быть.
Так вот сколько тут деленных точек всего?
Да, целая часть
p деленная на m
окурленная вверх.
Это если вот эта точка попадается.
Ну, еще, на самом деле, надо вычислить
x1, почему?
Потому что эта точка у нас не устраивает.
Да?
Вот.
Так.
Сейчас напишу вот такую вещь.
Целую часть мне неудобна.
Кто знает, как целую часть можно ограничить?
Ну, точнее,
p деленная на m, округленная вверх.
Как можно ограничить сверху?
Нет, это слишком грубо.
Давайте так.
Вот так, согласны?
Вот.
Вы уже видите, что тут получается, нет?
Нет?
Хорошо.
Минус один.
Минус один это m деленно на m.
p плюс m
минус один минус m
деленно на m.
Приводим к общему знаменателю.
Теперь намек понятен.
Что мы делаем?
p сократить?
Ну, давайте сократим.
Раз, раз.
И на самом деле, вот эта штука тоже сократится.
Согласны?
И что вот так получится?
Ну, кажется, не обманул.
Один на m.
Вот.
Вот такая история.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Вот.
Мы выбираем произвольное b
Какое? Мы не выбираем b
Мы случайно выбираем h
Сейчас, что значит произвольное hb?
А b – некоторые паралии, с которыми мы барьируем
То есть это параметрическая стимейсия
Ну, p мы фиксируем, x или y мы фиксируем
И дальше у нас есть мешок вот этих функций
И мы должны сказать, что для любых наборов
что для любого b, любого m, любого x, любого y
у нас вероятность, что это будет меньше, чем одна вторая
А тут я привел пример конкретного x и y
на которых, соответственно, у меня вероятность большая
Пример ящик?
Итак, ну и итог
Что по итогу?
А по итогу у нас получается следующее
Смотрите, у нас просто было время
У нас было время, чтобы при простом равномерном пушировании
в тремя время работы
Ваин
Поставлял
Да?
И было же такое
Вот я тут знаю, что если делать вот простое
равномерное пуширование, заменим на универсалитке
То есть при равновероятном пушире h из h большого
в среднем равномерном пушире h из h большого
в среднем равномерном пушире h из h большого
в среднем равномерном пушире h из h большого
То есть утверждение теоремы полностью справится
Потому что единственное место, где нам требовалось
самая вероятность
Соответственно, где нам требовались
какие-то свойства расширения
Это была последняя сторона
Потому что там начисляли что?
Вот там, где мы делали почки
Да, было платье
В итоге приходили к свередине соотношения
То есть это просто основа
Твоя делится до n
Роя, x
Ну, было такое, да?
Да
И вот тут раньше
Вот эта штука
Она раз в сумме
Вот n
1 на n, где x и y
1 на n, где x и y
1 на n, где x и y
1 на n, где x и y
1 на n, где x и y
Я что-то про то разомерно кширил
Но нам показывают, что
вероятность, что 2 случайные
что вероятность попадения в 2 случайных чисел
Который поднимает значение
1 на n, 2 на n
1 на n
А тут, поскольку мы просто те ли
изменили
Меньше неправда
Меньше неправда, чем от 1 на n
Понятно?
Ну, минус, что вам нужно для
для каждого прежнего кота
придумать свой аж
Сейчас я привел пример
уникальных смесей
для аж пушей, когда у меня
ключи
и целые числа вот с такого множества
Если у вас ключи это трое
то, соответственно, для троих нужно придумать
свое универсальное смесь
У нас более этого делать не нужно
в целом не нужно целые числа реализовать
Все
Нет
Вот он
Это подтверждение понятно?
По сути получить другую теорию
просто на которую отличается вот в одном символе
На вот здесь
Все
То есть в условии заменяя просто равномерное
кширование на универсальное смесь этой функции
и меняем два краевеса
У нас два краевеса здесь все
Мы получили рабочую схему
Полностью рабочую схему
Давайте ее проговорим
Что мы делаем?
Нам нужно построить пару данных
Которые умеют делать
Inter, Raise
и Fine
Что мы делаем?
Мы заводим массив списков
Мы случайно выбираем
кэш-функцию из универсального смеси
То есть нам известно много ключей
которые теоретически могут к нам проходить
Мы соответственно строим
универсальные смеси такой шнукции
для этого множества ключей
Так что согласно этой шнукции
мы распределяем наши элементы
по картинам
Их два элемента попали в эту картинку
если там архитектурный список
Списки элементов, которые туда попали
И соответственно чтобы удерживать
вот это вот соотношение
небольшим
чтобы оно не раствовало
Мы говорим следующую вещь
Если кэш-таблица стала равно
в ддн-ке
то мы увеличиваем
кэш-таблицу два раза
и производим перепишивание
Мы выбираем другую кэш-функцию
которая уже отображает
во множество окулятых 2м
Ну и даже продолжаем оставлять
пока не найдем того размера 2m
У нас остались элементы
кэш-таблицы ровно 2m
чтобы увеличивать два раза
У нас обучение включает
что может быть ограниченным
Ну я здесь думал
что может быть бесконечным
Но на практике у нас ничего бесконечного нет
Если вы знаете, например
вам нужно хранить имплей
Или вы знаете, что вам нужно
хранить комплектные вещи
Но если вы когда-то продаёте
то не знаю, берите
Кэш-таблица
Я буду хранить целые кэш-таблицы
Вот и всё
То есть ваш атак
это целые кэш-таблицы
Понятно?
Сколько это?
Внимание 20 секунд
Ну...
Я в своем веке
Ну...
Я в своем веке
Я в своем веке
Я в своем веке
Я в своем веке
Доли этой строго можно придумать
свою специальную интервью
Так, что остается вопроса?
Когда берём?
Так, на второй части мы поговорим про
идеальное хэширование статического множества
Это немного другой урод задачи
Которую мы решим
соответственно более эффективно, чем
исходную
Значит, задача ставится следующим образом
Дано некоторое важно
фиксированное множество х
Ну...
Ну...
Ну...
Фиксированное множество х
То есть вот до этого мы рассматривали
кэш-таблицы, в которые мы
периодически появлялись новые элементы
из которых удалялись старые элементы
Тут нам заранее говорят
что вот есть такое множество
и оно фиксировано
Ничего в него добавляться, ничего из него удаляться не будет
Оно такое множество
И всё, что вам нужно сделать, это быстро отвечать на вопрос
на findX
То есть быстро отвечать на вопросы, типа
есть ли ваше хэш-таблице это значение, или нет его
Никаких вставок, удалений нет
То есть естественно
какая-то задача тоже можно применить в тот метод
который мы обсуждали до этого
То есть просто построить хэш-таблицу цепочками
то есть выбрать случайную хэш-функцию
построить хэш-таблицу, ну и всё
То есть теорема нам говорит, что в среднем у нас будет
соответственно поиск работать за единицу
Но проблема в том, что в худшем случае
возможно такая ситуация, при которой у вас
поиск, при которой у вас поиск будет работать
за линейное время
То есть почти все элементы грубо говоря попали в одну корзину
От кого никто не застрахован
То есть вы всё-таки хэш-функцию выбираете абсолютно случайно
Этого бы хотелось избежать
Всё-таки хочется, наверное, если вам много что известно заранее
наверное хочется его как-то просмотреть
и как-то подобрать специальную хэш-функцию
ну или как-то подобрать структуру данных
которая бы гарантированно сдавала от единицы
в худшем случае, то есть за какое-то константное время
давала вам чёткий ответ, либо да, либо нет
за какое-то константное
разумное время
То есть грубо говоря
мы хотим построить кэш-таблицу
в которой бы не было совсем коллизий
Ну, мне кажется, цель благородная
Давайте обсудим, как её можно
как её можно достичь
Тут сейчас будет много теории
но в конце, я думаю, будет катарсис
построим интересную
полезную структуру данных
Вот, начну давайте
несколько поделений
несколько утверждений
Взря ты спойлер дал
Давайте вместе как-то разбираться
Давайте я за С
Давайте я за С обозначу
количество
коллизий
кэш-таблицы
Вот, то есть
вот у меня есть множество х
у меня есть множество х
х1, х2 и так далее
хн
и какая-то хэш-функция h, которая
отображает множество k
2 множество 0
1 и так далее
и минус 1
Вот, ну и на самом деле
c
c это просто количество пара
x и g, на которых у меня h
даёт один и тот же ответ
просто наоборот определение коллизий
Начну, утверждение 1
заключается в том, что
нужно расписать так
h от x и
равно h от x и g
и меньше, чем g
Ну, я пояснять думаю не буду
Ну, я надеюсь, это понятно
Это просто по определению
коллизий. Вы переврать все возможные пары
и g и смотрите, есть у вас коллизия или нет
Ну, с индикаторной функцией в прошлый раз знакомились
То есть она равна 1 если true
и 0 если false
С этим согласны
Второй пункт
Второе утверждение
Вот, смотрите, давайте я теперь h
пусть теперь у меня h
выбрана случайно
из универсального семейства
Вот сейчас, начиная с текущего момента
мы, когда я буду говорить про случайную хэш-функцию
всегда подразумеваем универсальное семейство
Потому что простого равномерокшери не существует
Всё, что у нас есть, это универсальное семейство
Пусть h
выбрана
равновероятно
из универсального семейства
h большое
Вот
Тогда
среднее значение
среднее количество коллизий в хэштаблице
будет равно
n на m-1
зеленая на 2m
Ну, доказательство просто следует
из утверждения 1
то есть среднее значение c
это есть среднее значение этой суммы
по утверждению 1
Дальше я знак среднего
могу внести под знак суммы
это мы в прошлый раз обсуждали
средняя сумма и то же самое, что сумма средних
Ну а среднее значение, величины, который
среднее значение либо единицы, либо ноль, чему равно
Тоже в прошлый раз обсуждали
Просто вероятность вот этого события
Это есть не что иное
Просто сумма по имени с чем g
вероятность h от x
равно
h от x g
Ну а это есть не что иное
Это не превосходит n-1
пополам умножить
1 на m
Понятно, откуда последнее взялось?
Но вот эта штука, каждый из слагаемых
не превосходит 1 на m. Почему?
Потому что h у меня из универсального семейства
Просто по пределам универсального семейства
у нас вероятность совпадения
двух элементов,
двух значений,
аргументов, которых не равны друг другу
не превосходит 1 на m
Но всего членов в этой сумме
н-1 пополам
Общее количество пар
по множеству x1, тогда xn
и n-1 пополам
Понятно?
Поэтому есть вопросы?
Что можно?
Вот
Давайте проанализируем этот результат
Что мы на самом деле получаем?
Мы получаем следующую вещь
Вот вы взяли кэш-таблицу
и при этом кэш-таблицу размера m
То есть у вас есть
множество из n элементов
и вы взяли кэш-таблицу размера m
То есть массив размера m
И дальше случайным образом
выбираете хэш-функцию и смотрите
сколько эта хэш-функция дает вам коллизий
То есть мы во что целимся?
Мы хотим построить кэш-таблицу
которая бы вообще не давала коллизий
А тут мы видим, что среднее значение коллизий
примерно равно
n на n-1 делено n2m
То есть кажется, что из этого следует
что m должно быть достаточно большим
чтобы у вас среднее количество коллизий
было достаточно маленьким
То есть m должна быть как минимум n2
Наверное
Понятно?
Но интерпретация, понятно?
Чтобы у вас количества коллизий было мало
Размер самой кэш-таблицы был достаточно большим
Ну ладно, пока запомню этот результат
Пойдем дальше
Следующий результат
будет не совсем из алгоритмов
а будет из теории вероятности
Но нам понадобится
Лемма Маркова
Пусть x
из случайной величины
Тут я делаю оговорку
что я не вожу формально понять
что такое случайная величина
Я просто обращаюсь к вашей интуиции
То есть это просто некоторая величина
которая принимает случайные значения
Из случайной величины
принимающее
значение
из
Ну давайте еще ноль включим
Грубо говоря, случайная величина
это дать некоторого случайного эксперимента
Ну, например, подбрасывание монетки
выбор какой-нибудь случайной функции
измерение силы тока на лабах и так далее
Понятно?
Вот, понимаешь, что значение
из n0
Что я хочу сказать?
Хочу сказать, что тогда
для любого епсилон больше нуля
вероятность того, что x
будет больше чем епсилон
ну, больше любра равно чем епсилон
Больше любра равно чем
среднее значение величины x
зеленой епсилон
Вот, можно сразу посмотреть на эту формулировку
и проинтерпретировать этот результат
Что этот результат нам говорит?
Зачем он нам нужен?
А он нам нужен для того, чтобы мы по-среднему могли
как-то оценивать вероятности
То есть что нам эта штука говорит?
Что говорит о том, что если у вас вдруг
среднее значение какой-то величины мало
если вы понимаете, что вы проводите эксперимент
и у вас в среднем получается какое-то маленькое значение
то же будет мало
Вы на самом деле не явно используете этот результат
например, когда говорили про
не знаю, быструю сортировку
То есть вы говорили, что быстрая сортировка в среднем работает за n log n
Ну и в целом говорили, что вероятность больших значений
вероятность того, что сортировка будет работать за n квадрат
крайне мала
Вот, собственно, это некоторые следствия из этого
Понятно?
Окей?
Ну, доказательство тоже простое
Давайте распишем среднее
Среднее x это
есть не что иное, как сумма по x
от 0 до бесконечности
То есть у меня x принимает значение из натуральных чисел
с 0
Поэтому я просуммирую по всем значениям
И, соответственно
умножу конкретное значение
на вероятность всего появления
Это в чисто виде среднее
Тоже в прошлый раз обсуждали
Вот
Так, вы давайте из этой суммы
я выкину лишнее
Лишнее это те значения x, которые меньше, чем епсилон
Просто из этой суммы выкинули какие-то маленькие значения
Понятное дело, что я ее не уменьшил
Почему? Потому что у меня x от приории больше, чем 0
Ну и сейчас я еще сильнее
уменьшу эту сумму
Посмотрите, если у меня все x больше, чем епсилон
Согласны ли вы, что я могу x заменить на епсилон
Тем самым я сумму только уменьшу
Да?
Давайте я просто все x заменю на епсилон
Ну и сразу вытащу эту константу за скоб
Так получу сумма
по всем x больше, чем епсилон
x большое
равно конкретному x малому
Так
А вот это что?
Чему равна сумма вероятности
x большое, случайная величина
равна значению x
Если я буду суммировать по всем x больше,
то я получу сумму
Ну что такое вот эта сумма?
Да, ну это просто
вероятность того, что у меня x большое
будет больше, чем епсилон, согласны?
Ну все
Есть вопросы?
Все нормально?
Окей
Ну да, тут нужно сказать, что
лемма на самом деле верна
и в произвольной случае, когда у вас
есть произвольная, неотрицательная, случайная величина
То есть она не обязательно должна быть дискретной
и так далее, но это уже
на теорию вероятности вам останется
Все? Можем дальше идти?
Так
Давайте формулировку оставим
Теперь я, значит,
готов сделать следующее заявление
Заявление будет такое
Если возьмете хэш-таблицу
размера n квадрат
то с очень маленькой вероятностью
в ней будет хоть одна коллизия
Точнее, давайте так
с очень большой вероятностью
в ней вообще не будет коллизий
Нормально?
Теорема
1
Ну, снова традиционно
пусть h
равновероятно
выбрана
из универсального смеси
большое
и
соответственно
и размер хэш-таблицы
равен квадрату количества элементов
которые в ней хранится
Тогда вероятность того, что
число коллизий будет
больше равно, чем единица
то есть вероятность того, что будет хотя бы одна коллизия
меньше, чем
одна вторая
То есть можно
прочитать наоборот
что больше, чем одна вторая
у вас в хэш-таблице вообще не будет коллизий
Понятно? Вот выбираете случайную функцию
случайную хэш-функцию
и вот собирается больше чем 50%
вы говорите, что в моей хэш-таблице коллизий вообще не будет
Нормально?
Доказательство
Ну, доказательство простое
Тут мы просто применяем левую маркову
В поле марковы
В поле марковы
такую вероятность можем оценить как?
как среднее значение
С, деленное единицу
Эпилон, парной единицы
Согласны?
Только что доказали
А по утверждению 2
Мы доказывали, как можно ограничить среднее значение
среднее число коллизий
Даже не ограничить, по-моему там равенство было, да?
Нет, неравенство
Почему оно было равно?
n на n-1 на 2m
Все, теперь вспоминаем, что
у нас m это n квадрат
Получаем n на n-1
деленное на 2n квадрат
меньше, чем одна вторая
Последний переход, упражнение
Окей?
Вот
Да, смысл обсудили
Если возьмете кэш-таблицу
у которой размер равен квадрату
количеству ее элементов, то все
у вас гарантированно коллизий не будет
Вы можете сказать следующую вещь
Это все равно случайный процесс
Вы взяли кэш-функцию, и вам не повезло
в ней есть коллизий, что делать?
Давайте я буду убрать хэш-функцию
если получившись хэш-таблицы, у меня есть коллизии
я буду строить хэш-таблицу заново
возьму другую хэш-функцию
и, соответственно, построю theyол к хэш-таблицу заново
Если и в ней есть коллизии, то снова возьму хэш-функцию
и построю заново
Понятное дело, что
теоретически такой процесс может Mercedes бесконечно
но с учетом того, что у вас
с предвир ¿
рацией, согласны? План понятен? Конечно, таблица размерами квадрат. Вытаскиваем из мешка
случайный кэш-функт до тех пор, пока вам не повезет. А повезти вам, согласно этой теории,
должно быть достаточно быстро, потому что вам не везет с вероятностью умеющего 1 на 2.
Нет, ну нет, подождите, у вас, не знаю, у вас количество интов 4 миллиарда, а за хэш-функт вам нужно всего лишь
тысячу из них. Тогда m равно миллиону, нормально. Ну, m может быть больше, чем n? Давайте напишем такое
отношение. m больше, чем n, но меньше, чем мощность k. Даже много меньше можно написать.
У вас наверняка тут что-то смущает, что?
Разве вот это не смущает, нет? Ну хорошо. Сколько элементов вы таким образом можете сохранить в хэш-таблице,
не попадая под мл на 27-м тесте? Это спойлер. Ну сколько? Ну, n равное тысяче, m равное миллион,
ну терпимо. n равно 10 тысяч, ну 100 миллионов, ну терпимо. n равно 10 пятый, все, без шансов.
Логично. То есть, смотрите, на самом деле n у вас, вы не можете захэшировать достаточно
большое множество. То есть, даже множество размера миллион или 100 тысяч, это уже все, конец. То есть,
это уже слишком сложно. Вообще, вы должны, наверное, еще с первого семестра понимать, что квадратичная
сложность это как-то ну тумач, вот чего-то быстрее. Вот. Поэтому это все хорошо, но у нас будет другой
план. В смысле, это нам пригодится, вот, но мы будем действовать по-другому. Значит, если мы будем
строить одну большую кэш-таблицу размера n квадрат, будет очень плохо. Я предложу следующую вещь.
Давайте построим обычную маленькую кэш-таблицу. Ну вот, для тех элементов, которые попали вот
в каждый конкретный элемент, в каждую конкретную корзину, я построю свою кэш-таблицу уже размера
n квадрат. То есть, у меня будет кэш-таблица кэш-таблиц. То есть, у меня будет не одна большая
кэш-таблица размера n квадрат, а будет много маленьких кэш-таблиц, каждый из которых будет
иметь размер квадратик от количества элементов, которые у него попало. Окей? Вот. Нет, нам ничего не
попадает. Нам множество уже дано, куда-нибудь добавляется. Какой вопрос еще был?
Все, будем действовать так. Построим маленькую кэш-таблицу, а внутри каждой ячейки построим
такую кэш-таблицу, которая будет иметь размер квадрата от количества элементов, которые
у него попали. Возникнет естественный вопрос. А почему эта кэш-таблица не может иметь большой
размер? Ну окей, мы понимаем, что n квадрат может быть большим. А если я просумирую квадраты,
это число же тоже теоретически может быть большим. Вот. Вот следующее утверждение. 1 квадрат
хуже, чем много маленьких квадратчиков. Утверждение такое. Теорема 2. Ну да, как обычно,
давайте я преамбулу опущу. Ну про h вы все понимаете. h берется случайно, равновероятно из
универсального семейства хэш-функций. Вот. Дальше. Пусть у меня n равно n. То есть я беру обычную
кэш-таблицу, которая имеет размер соответствующий ровно количеству элементов, которые нам нужно
захэшировать. Вот. Значит, пусть в эту ячейку попало n и элементов. Вот мне хочется понять,
если я по всем ячейкам просумирую квадраты их размеров, получится у меня большое число или
небольшое число? Значит, утверждение, что будет небольшое число, а именно среднее значение суммы
квадратов по i от единицы до n будет меньше, чем 2n. Вот. Если вы просумируете квадраты всех размеров
ячеек, ну и усредните по нескольким экспериментам, по нескольким запускам, то вы на самом деле всего лишь
получите значение, не превосходящее 2n. То есть размер на самом деле будет линейный относительно
количества элементов, которые хранится в хэш-таблице. Доказательства. Давай для начала напишем вот так.
Давайте вот эту сумму я распишу по-другому. 2 сумма i единицы до m, ni на ni минус 1 пополам
плюс 1 на единицы. Так, вот это всем понятно. На двоих я может не обращать внимания, они там
сокращаются. Тут стоит сумма ni на ni минус 1, и я из нее, из этой суммы ni квадрат, вытащил просто ni.
Понятно? Понятно, что вот эта вот штука и вот эта штука сумме дает ni квадрате. Не сложно.
Тут есть два слагаммера. Вот это и вот это. Давайте к самой сообразительной. Кто скажет мне,
чему равна вот эта штука? Вот эта. Чему равна сумма размеров каждой ячейки? n. Всего у меня элементов
n, поэтому если я просуммирую размеры каждой ячейки, то получу n. Отлично. Замечательно. Так,
теперь для более сообразительных. Нет, давайте вот так. Чему равна вот эта штука?
Давайте так. В чем смысл вот этой штуки? Как вы можете проинтерпретировать это значение?
Отлично. Замечательно. А почему-то более конкретно. Давайте в терминах коллизий. В терминах
коллизий. Чему это равно? Хорошо. Вот у вас есть ячейка с кодовым названием i. И в ней есть,
давайте вот так изброшу, мешочек. В этом мешочке и элементов. Давай так. Комминаторика была? Какой
комминаторный смысл вот этой штуки? Число сочетаний, число двоек. Число двоек, число пар. Неупорядочно.
А чему равно число неупорядочных пар вот в этом мешочке? Согласны, что это количество коллизий,
которое образует вот эти элементы между собой? Взял произвольный элемент отсюда, взял произвольные два
элемента. Согласны ли вы, что они образуют коллизию? Сейчас, подождите. Они образуют коллизию,
согласны? Взял еще пару элементов, они образуют коллизию. Если я просуммирую все возможные пары
вот здесь, то я получу количество коллизий вот в этом мешочке. Согласны? А если я просуммирую число
коллизий по всем мешочкам? Это общее количество коллизий. Согласны? То есть, это ничто иное как два на
С. Так, это понятно? Смотрите. Давайте я возьму два мешочка. Вот я возьму произвольный элемент отсюда
и произвольный элемент отсюда. Образуют ли они коллизию между собой или нет? Нет, потому что они
у них разные херозначения, они в разных мешках. Соответственно, коллизии могут образовываться
только для элементов в одном мешке. Согласны? А сколько всего тут коллизий? Всего тут коллизий — это
ровно количество пар. Количество пар элементов отсюда. А количество пар элементов отсюда — это вот краски
вот эта штука. Число сочетаний по два. Если я просуммировал это по всем, по каждой ячейке, по каждому
мешочку, то я получу краски С. Ну, двойка она вот отсюда возникла. Понятно? Допустим. Так, теперь вот от этого всего
нужно взять среднее. Так, вот эту сумму я расписал вот так. Соответственно, это среднее от 2c плюс
среднее значение n. Так, снова проявляем смекалку. Чему равно среднее значение n? n — отлично. n — это
константа. Это не случайная величина. Она, как ни крути, всегда будет n. Как бы вы х-функцию не
убирали. Так, а среднее значение, а среднее значение, среднее удвоенное значение числа коллизий? Мы считали.
Но это n на n минус 1, деленное на 2m, но двойка убивается, поэтому тут получается просто m. А m у нас равно n?
m равно n. Соответственно, я получаю 2n минус 1, что меньше, чем 2n. Это тоже упражнение.
Получилось?
Окей, окей, окей. Так, слайды закончились.
Ну все, смотрите, смотрите, что мы получили. Еще раз напоминаю наш план. Мы берем х-таблицу размера
равную n. Распределяем все элементы по этой самой х-таблице. А дальше внутри каждой ячейки, в каждом
вот этом мешочке, трое свою х-таблицу, но размера n квадрат. Вот. Почему это хорошо? Ну то есть
почему это сработает? Потому что, если вы просуммируете размеры всех х-таблиц суммарно, то
у вас получится не n квадрат, а получится что-то типа 2n. Ну даже меньше, чем 2n. Понятно? Ну давайте
тут еще надо следствие на самом деле показать. Этого не совсем достаточно. Следствие, вероятность
того, что сумма ni в квадрате от 1 до n будет больше либо равна, чем 4n, меньше, чем вот.
То есть, грубо говоря, с очень большой вероятностью у вас сумма ni в квадрате будет меньше, чем 4n. Ну со
средним все-таки непонятно, как работает, да? То есть, я говорю, что, ну в среднем у меня размер
х-таблицы меньше, чем 2n. На, ну у вас на самом деле возможный случай, когда у меня размер х-таблицы
будет больше, чем 2n и так далее. Ну вот, вот это следствие говорит о том, что у вас с очень маленькой
вероятностью размер х-таблицы будет превосходить чем, будет, будет, будет превосходить 4n. Вот. Ну тут на самом
деле все просто. Тут просто лему Маркова тоже применяем, и все. Сумма ni в квадрате больше
либо равна, чем 4n. Меньше либо равна, чем это полемя Маркова. Среднее значение суммы
ni в квадрате, деленное на 4n. Вот. А по теореме эта штука меньше, чем 2n на 4n. Ну то есть, вторая.
Так, по теореме есть вопрос? Все, это была последняя теорема, последнее следствие. Сейчас алгоритм выпишем и все.
По теории есть вопросы? Окей. Так. Алгоритм построения.
Я помню, что он называется FKS. По именам авторов. Авторов, честно говоря, я не помню. Вот. Короче, FKS.
Первый шаг. Выбираем случайную h и справляем х-таблицу с m равную n.
Второй пункт. Если сумма ni в квадрате оказалась больше, чем 4n, то повторяем пункт 1.
Смотрите, мы хотим, чтобы х-таблица была маленькой. При этом мы знаем, что х-таблица будет маленькой с очень большой вероятностью.
Вот. Вы выбрали случайную х-таблицу и распроделили свои элементы по бакетам. Вот по этим корзинам.
И в итоге обнаружили, что х-таблица внезапно большая. Сумма ni в квадрате больше, чем 4n.
Далее мы говорим, что мы отказываемся от этой хэш-функции, берем новую хэш-функцию и строим хэш-таблицу заново.
Первый пункт. За сколько работает? За 5n, согласна. А сколько раз мне в худшем случае, не в худшем случае,
сколько раз средними придется перестраивать мою хэш-таблицу? Примерно два раза, да?
Потому что мне не везет с вероятностью меньше, чем одна вторая.
Но соответственно, мне везет с вероятностью больше, чем одна вторая.
Но примерно на второй попытке я уже найду хорошую хэш-функцию. Согласны?
Примерно со второй попытки. Средней давайте напишем.
Так, дальше третий пункт. Для любого i, принадлежащего от 0 до m-1, строим хэш-таблицу, нет, давайте так.
Случайно выбираем h-i и строим i-ты ячейки, хэш-таблицу размера m.
m-i равная m-i в квадрате. Давайте еще раз поясню, что n-i это число элементов i-той корзине.
Ну то есть я распределил мои элементы по корзинам, и теперь я хочу, чтобы внутри каждой корзины у меня не было коллизий.
То есть хочу их закашировать так, чтобы каждый из них отправлялся в свою ячейку.
Ну для этого по теореме 1 мне утверждает, чтобы гарантировать, что с большой вероятностью не будет коллизий, мне нужно взять хэш-таблицу размер n-квадрат.
Ну все, раз вытую ячейку, у меня попало n и t элементов, соответственно, я должен взять хэш-таблицу размера n и t в квадрате.
Ну и четвертый пункт. Если для какого-то и получились коллизии, то что делаем?
Что делаем? Да, то переделаем вот эту хэш-табличку маленькую, вот то, перестраиваем, ну давайте то повторяем.
Повторяем пункт 3, для этого и.
Давайте определимся, за сколько работает пункт 3 и пункт 4. Пункт 3 за сколько работает?
За тета от n, а почему?
Да, потому что после второго пункта у меня гарантирована размер хэш-таблицы не больше чем 4n,
поэтому сумма n и t в квадрате гарантирована не больше чем 4n.
Соответственно, все хэш-таблицы я суммарно построю не дольше, чем за 4n, согласны?
Ну и пункт 4 у меня выполнится примерно со второй итерацией.
Но здесь та же самая история. Я повторяю пункт 3 до тех пор, пока мне не попадется хорошая хэш-таблица.
А хорошая хэш-таблица мне попадается с большой вероятностью,
поэтому уже с первого раза с большой вероятностью я получу хорошую хэш-таблицу.
Но если на первом и в первом, то я получу хорошую хэш-таблицу.
на самом деле уже с первого раза, с большой вероятностью, я получу хорошую хэштаблицу.
Но если на первый раз не повезло, то со второго повезет.
Так или иначе, меня достаточно быстро должно повезти, потому что у меня вероятность большая.
Итак, итог. Так, алгоритм весь понятен?
Ну, вот так получается здесь.
Здесь 4хn, мне гарантирована вероятность плохой хэштаблицы будет меньше, чем одна вторая.
В принципе, можно подобрать, не знаю, там 8n, 10n, тогда вероятность будет еще больше.
Ну, хэштаблица тогда тоже будет еще больше.
Ну, 4n не из потолка взялась, а вот отсюда из следствия.
Так, алгоритма, есть вопросы?
Ну, окей, тогда давайте анализ, давайте с простого начнем.
Время работы файнк.
За сколько у меня будет работать файнк в такой хэштаблице?
Почему в среднем?
У меня теперь пояс гарантированно работает за единицу, согласна?
Ну, почему? Потому что как работает поиск?
Вот у меня есть внешняя хэштаблица, и вот в каждой ячейке, давайте вот эту изображу,
у нее хранится еще одна хэштаблица размера mi5 в квадрате.
Как у меня происходит поиск хэштаблицы?
Я сначала беру исходную кэшфункцию h, она меня введет в какую-то ячейку i.
А дальше в этой ячейке i я беру кэшфункцию h, i, t и выпадаю в какую-то ячейку вот здесь.
И соответственно там либо лежит, может быть, или нет, или нет, или есть, или нет.
А дальше в этой ячейке i я беру кэшфункцию h, i, t и выпадаю в какую-то ячейку вот здесь.
И соответственно там либо лежит мое значение, либо не лежит, согласна?
То есть, по сути, за два шага я гарантированно узнаю, есть ли элемент в хэштаблице или нет его.
Понятно?
Понятно, что я делаю не больше чем два шага.
То есть у меня тут никаких цепочек нет.
Почему? Потому что вот в этой внутренней хэштаблице гарантированно нет коллизий.
А если там гарантированно нет коллизий, то я по цепочкам никаким не прохожусь.
Всё.
А зачем, если у вас уже и так успех?
Можете сделать.
Можете.
Так, в худшем случае.
Так, теперь память.
Сколько память занимает эта структура данных?
Ну сколько? n², n log n.
Так, это от n, почему в среднем?
Так, у нас была сумма.
А если посмотреть на алгоритм?
Я согласен, что у нас была оценка,
что среднее значение n² меньше, чем 2n.
А могу ли я что-то гарантировать в худшем случае?
В худшем случае, да.
Смотрите, я же гарантировал уже тут, на втором пункте,
что у меня сумма n² точно не больше, чем 4n.
То есть второй пункт,
это от n в худшем случае.
Это гарантирует
пункт два.
Почему?
Если вдруг мне попалась большая х-таблица,
то я просто ее заново перестраиваю.
До тех пор, пока у меня х-таблица не будет маленькой.
То есть уже в втором пункте
мне точно известно, что сумма n²,
то есть размер, общий размер х-таблицы,
точно не больше, чем 4n.
Но если он не больше, чем 4n, то ну все.
Победа.
Так, ну и наконец.
Время построения.
За сколько строится эта х-таблица?
Это от n тоже.
А вот тут я могу сказать, что в худшем случае?
Могу ли я гарантировать, что вот точно
за какое-то разумное линейное время она построится?
Почему?
Да, потому что
тут, к сожалению, есть
все-таки элемент случайности.
Элемент случайности, он есть
в пункте 2 и в пункте 4.
То есть теоретически, теоретически
она может катастрофически не вести.
То есть это происходит с маленькой, вероятно,
все, но теоретически, возможно, такое,
что вы берете хэш-функции, берете, берете, берете,
и постоянно вам подаются плохие хэш-функции,
которые дают вам большое значение здесь.
И поэтому пункт два теоретически
может продолжаться бесконечно.
Ну, не бесконечно, но вы можете
достаточно долго перебирать все
эти хэш-функции.
Поэтому мы понимаем, что в среднем
со второй попытки, ну, с первой со второй попытки
вам уже повезет.
Да, потому что вероятность везения довольно большая.
Ну, а теоретически?
Теоретически, к сожалению, гарантировать
быструю исходимость не получится.
Поэтому вот тут мы пишем
только в среднем.
Вот.
Ну, я думаю, это уже неплохо.
Хотя бы
find за единицу в худшем случае мы сделали.
То есть эта хэш-таблица гарантированно
позволяет вам искать
за единицу.
То есть никаких худших линейных случаев и так далее нет.
Но в постановке у вас
множество известно заранее.
Ну, тут надо сказать что замечание, что есть модификация
алгоритма FKS, которая работает и
в динамическом случае, то есть
если вам все-таки нужно добавлять элементы,
удалять элементы, то, ну, примерно похожий
алгоритм, он работает и
в динамическом случае.
Только там надо динамически там переширять, перехешировать.
Но об этом можете почитать отдельно.
Ну, а пока все.

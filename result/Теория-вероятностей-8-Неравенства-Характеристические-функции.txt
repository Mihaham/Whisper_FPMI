Так, коллеги, значит, с того места, на котором мы остановились, мы с вами получили условия
независимости, точнее говоря, получили, что если две случайные причины независимые,
то математическое ожидание произведения равно произведению математических ожиданий. Правда,
отметили, что это также имеет место, когда ковыриация между этими случайными величинами равна нулю.
Ну и там нарисовали некую диаграмму. Так, ну давайте теперь воспользуемся этими знаниями,
ну, например, вот для чего. Давайте посчитаем дисперсию суммы случайных величин. Просто
получим формулу. Это есть математическое ожидание кси центрированное плюс это
центрированное в квадрате. Правильно, да? Раскрываем скобочки, пользуемся линейностью.
Математическое ожидание кси центрированного в квадрате это дисперсия кси. Математическое
ожидание это центрированное в квадрате, это дисперсия это. И плюс два математических ожидания,
их произведение. Но произведение, математическое ожидание произведения центрированных случайных
величин, как раз и есть ковыриация. Что мы видим из этой формулы? Из этой формулы мы,
выводы какие можем сделать? Ну, во-первых, если случайные величины не коррелированы,
то дисперсия суммы равна сумме дисперсии, потому что эта ковыриация равна нулю. Если
случайные величины независимы, то дисперсия суммы равна сумме дисперсии. Ну, чаще всего вот в
такой формулировке это свойство и приводит. Ну, еще раз обращаю ваше внимание, что совсем
не обязательно независимы случайных величин, чтобы дисперсия была аддитивной. В принципе,
достаточно вот того, что ковыриация их равна нулю. Ну и теперь мы можем, хочу вернуться к анонсу того,
что я делал в прошлый раз. Почему все-таки у мат ожидания кси два обозначения? E кси и M кси.
Значит, ну и повторюсь, значит, E кси это expected value. Ожидание, ну, собственно, это ожидание,
а вот это мин, среднее. Почему такие две буквы, как бы, так сказать, возникли? Чтобы понять это,
давайте рассмотрим n штук случайных величин, которые независимы, одинаково распределенные
случайные величины. NURS-V такое стандартное обозначение. Ну, заметьте, я не говорю независимую
совокупности, поскольку, ну, это излишнее свойство, просто кси n независимы, одинаково распределенные
случайные величины. И введу такую, в общем-то, известную вам функцию среднее значение. Среднее
значение. Ну а теперь давайте попробуем применить наше знание. Чему равно математическое ожидание
кси среднего? А где минус? Вот это? Так, значит, ну, поскольку это одинаково
распределенные случайные величины, то у них у всех одинаково от ожидания дисперсия существования,
которые предполагаем. Итак, чему равно от ожидания кси среднего? Ну, давайте примените.
Коллеги, согласны? А чему равна дисперсия кси среднего? Напомню, что это независимые одинаково
распределенные случайные величины. Из-под знака дисперсии константа как выносится? В квадрате,
1 на n квадрат, получается дисперсия суммы. Дисперсия суммы для независимых случайных
величин равна сумме дисперсий. Получается n дисперсий делить на n квадрат. То есть дисперсия
кси среднего равна дисперсии кси делить на n. Понятно, да? Ну, вообще, я как бы исхожу из того,
что вы такие вещи, ну, легко делаете. Вот, ну и давайте воспользуемся, собственно, теперь неравенством
Чебышева для случайной величины кси средняя и получим. Вероятность того, модуль кси средняя
отклоняется от своего мотожидания, а мотожидание кси среднего это екси, на величину больше или
равную епсилон меньше или равна дисперсии кси среднего, то есть дисперсии кси делить на n,
епсилон квадрат. Вот что получается. Ну и как мы это интерпретируем? Среднее значение
при достаточно большом n со сколь угодно большой вероятностью находится в сколь угодно малой
окрестности математического ожидания. Видите, да? То есть при n стремящейся к бесконечности кси средняя,
то есть средняя, и мотожидание это одно и то же. Но в чем разница? Екси это интеграл либега от
случайной величины, определенный на неком вероятностном пространстве, то есть элемент
абстрактной модели. А что такое кси средняя? Это эксперимент, это экспериментальные данные,
например монет мы можем подбрасывать или кубик. И таким образом, ну я возвращаюсь к тому, о чем
говорил в самом начале, ну вот можно сказать, что, например, вот это неравенство, одно из, да, говорит
о том, что теория вероятностей это не абстрактная дисциплина, а естественно научная дисциплина,
потому что, значит, характеристики, которые, так сказать, вычислимы в модели, могут быть
экспериментально подтверждены, ну как законома, например. Есть у вас абстрактная модель, как
электрончики там движутся по решетке металла под воздействием электрического поля, а есть эксперимент,
вы просто меряете напряжение, силу тока, вот сопротивление либо получаете, либо так светя,
но в системе, то есть вот эту величину можно подтвердить экспериментально, потому что она
приближается сколь угодно высокой степенью величиной кси средняя. Поэтому это, как я уже
говорил, ну причем это, конечно, не общепризнанный термин, там это шутка такая, да, это, можно сказать,
второй закон Ньютона для теории вероятностей. Вот, значит, или одна из формулировок его. Ну вот,
давайте на этом зафиксируемся, это очень важно, но для того, чтобы это как бы осмыслить, надо самим,
как называется, покрутить в голове, вот я зафиксировал этот факт, комментарии дал, и давайте
двинемся дальше. Значит, мы с вами изучили ковариацию, надо сказать, что еще есть такая величина вводится,
можно с ней столкнуться, особенно во всякого рода приложениях, это корреляция или коэффициент
корреляции, которая определяется вот так для двух случайных величинам.
Корреляция, это ее определение. Какими свойствами она обладает? Ну, во-первых,
Ро по модулю меньше чего? Единицы, потому что в прошлый раз, когда мы свойства дисперсии
выписывали, мы показали на основании неравенства Кашибуниковска, что ковариация кси-эта по модулю
меньше или равна корне квадратного из произведения дисперсии, поэтому Ро меньше равно единице. Второе
свойство, если кси-эта независима, то чему равна корреляция или коэффициент корреляции? Ро равно нулю.
Ну и третье свойство, которое выделяют в корреляции, я вот здесь напишу, давайте считать, что это
линейно связано с кси и посчитаем коэффициент корреляции таких случайных величин. Ну, прежде
всего, надо ка вариацию посчитать. Кси-эта равно математическому ожиданию кси-эта минус мат
ожидание кси умножить на мат ожидание это равно математическое ожидание axi плюс b умножить на кси
минус мат ожидания кси умножить, ну сразу мат ожидания это напишу, а мат ожидания кси плюс b
равно. Ну, собственно, пользуемся линейностью. От этого остается а на мат ожидания кси квадрат,
член с b сокращается, минус а на мат ожидания кси в квадрате. А это что такое? Это а умножить на
дисперсию кси. Правильно, да? Ну и вот здесь, вот здесь напишу, чтобы какое-то время пожило.
Ну и тогда получается, что коэффициент ρ равен в числителе а дисперсия кси, в знаменателе,
во-первых, дисперсия кси. Чему равна дисперсия эта через дисперсию кси, если вот такая связь?
А квадрат дисперсии кси. Ну и получается, что это равно знак а, то есть плюс или минус единица.
В какой строчке? Здесь, вот в этой строчке. Минус а екси в квадрате, минус а дисперсии,
значит равно. Вот это, да? То есть получается коэффициент корреляции, если случайные величины
линейно связаны. Ну достигает своего максимального по модулю значения 1 или минус 1, а если они
независимы, то он равен нулю. Это, так сказать, приводит к тому, что иногда в технических приложениях
коэффициент корреляции считают мерой зависимости случайных величин. Это такое качественное
утверждение. Количественно оно, по большому счету, ничего не значит. Существует большое количество
примеров, когда две случайные величины, которые являются некоррелированными, то есть кавариация равна
нулю, при этом являются зависимыми. Поэтому в обратную сторону не работает. Но есть, как бы,
важный случай, в котором это вполне работает. Ну вот нам настало время познакомиться с
нормальным случайным вектором. Значит корреляция не превосходит единицей, для независимых равна нулю,
для линейной связи равна либо плюс, либо минус 1, зависит от знака А. И еще раз повторюсь из того,
что корреляция равна нулю, то есть кавариация равна нулю из определения, не следует, что случайные
величины кси и это независимы, за исключением выделенных случаев. И вот один выделенный
важный случай мы сейчас рассмотрим. Так, ну давайте введем вектор кси, кси1, ксик, как компонент у него.
Математическое ожидание кси равно математическому ожиданию кси1, математическое ожидание ксик и
обозначим это вектором m. Также введем матрицу, которую обозначим r, которая равна математическое
ожидание кси, центрированное на кси, центрированное, транспонированное. Это что за объект? Это матрица
k на k, у которой элементами является кавариация ксиитая, ксижитая и ж1к. Вот этот объект, естественно,
называется матрицей кавариации. По диагонали этой матрицы стоит что? Дисперсии, потому что кавариация
самой на себя случайно влечены, это дисперсия. Напоминаю, закрепляем знания, полученные на прошлой
лекции. Вот и вводится нормальный случайный вектор, который зависит от двух параметров m и r. Вводится
это нормальный случайный вектор своей плотностью, значит f это будет уже векторная функция. Ну вот
тут я напишу вот так mr и плотность это выглядит таким образом. Единицы делить на 2p в степени n
пополам, корень квадратный из детермината r, e в степени минус x вектор минус m вектор транспонированная,
r в минус 1 и x транспонированная минус m делить пополам. Но смотрите, у нас появились объекты в
знаменателе детерминат и в показателе степени r минус 1, значит мы предполагаем, что эти объекты
существуют. Но вообще просто по построению матрица r больше или равно нуля, с точки зрения положительной
определенности. Мы пока требуем r строго больше нуля для того, что детерминат не равнялся нулю и
обратная матрица существовала. И вот такой нормальный вектор называется невыраженным нормальным
вектором, в отличие от выраженного нормального вектора, у которого r может равняться нулю, но это
чуть позже. Пока считаем r больше нуля строго. 2p в степени n пополам, n, k, прошу прощения. Вот тут,
коллеги, справьте, у нас размерность k. Спасибо. Вот, ну давайте вот это сотру.
Так.
Давайте предположим, что у нас компоненты этого вектора не коррелированы. То есть вот
тот самый случай, когда ксиитая, ксижитая равно нулю для любого i неравного j. Коэффициент
корреляции равен нулю, ковариация равна нулю. Вот рассматриваем такой случай. В этом случае,
значит, тогда матрица выглядит так. Дисперсия кси-1, дисперсия кси-k, 0, 0. Правильно, да?
Детерминат тогда ее, это произведение дисперсии ксиджитых j от единицы до k, а r в минус 1, в этом
простом случае, это единица на дисперсии кси-1, единица на дисперсии кси-k, 0, 0. Ну и давайте это
теперь просто подставим в нашу функцию плотности и посмотрим, что получится. Стираем вот здесь.
Я без индекса буду писать, с вашего позволения. Вот об этой функции идет речь. Равно единица
на 2π в степени k пополам. Детерминат у нас превратится, вот такое произведение дисперсия кси-1, дисперсия
дисперсия кси-k. Экспонент у нас превратится вот в такую штуку, может так напишу, эксп, минус сумма
x житая в квадрате, x житая минус m житая в квадрате делить на два дисперсия кси-житая. Вот посмотрите,
правильно? Да, спасибо. Равно ж от единицы до k, единица на корень квадратной 2π дисперсии
кси-житая, е в степени минус x житая минус m житая в квадрате делить на две дисперсии кси-житая.
То есть функция плотности вот этого вектора, вот с такой плотностью, если компоненты вектора
не коррелированы, развалилось вот с такое произведение. А что здесь каждый из сомножителей
представляет? Функция плотности нормальной случайной величины. Функция плотности вектора равна
произведению плотностей компонент. Это и означает, что кси-1, кси-n не только не коррелированы, но и
независима. То есть нормальный случайный вектор – это тот самый случай, когда из некоррелированности
следует независимость. Мы привели именно этот пример, потому что само по себе нормальное
распределение и нормальный вектор – очень важный объект в нашей дисциплине. И вот для него есть
такое замечательное свойство. То есть из независимости, из некоррелированности компонент
нормального случайного вектора следует их независимость. Вот отметите про себя этот факт.
Так, дальше. Ну вот эти две доски тогда начну занимать.
Напомню, что мы с вами обсуждаем мат ожидания, а нормальный случайный вектор мы ввели в связи
с использованием свойств к авариации. Ну давайте еще там про мат ожидания чуть-чуть поговорим.
Ну, помним про то, что случайная величина – это на самом деле измеримая функция на множество
элементарных исходов, а математическое ожидание – интеграл Либега. Поэтому и существует достаточно
большое количество всяких неравенств, ну как там в метрических пространствах, например. Для
случайных величин или, точнее говоря, их мат ожиданий тоже существует всякие неравенства. Их много,
но мы сейчас познакомимся или я напомню вам где-то такой минимум. Итак, имеет место такое неравенство.
Математическое ожидание кси1 умножить на кси2 меньше или равно, чем математическое ожидание кси1 в
степени r в степени единица на r умножить на математическое ожидание модуль кси2 в степени s,
в степени единица на s, где единица на r плюс единица на s равно единице и r больше единицы. Что это
за неравенство? Это неравенство Гольдера, просто вместо интегралов написал мат ожиданий. Еще одно
неравенство. Математическое ожидание кси1 плюс кси2 в степени r, все это единица на r меньше или равно
математическое ожидание кси1 в степени r в степени единицы на r плюс математическое ожидание кси2 в
степени r в степень единицы на r. Это что за неравенство Минковского? Опять же, вместо функции написал
случайные илличины, а вместо интегралов написал мат ожидания. А вот следующее неравенство вряд ли
вам знакомо. Выглядит оно вот так. Пусть у больше v больше нуля, то есть заданы два числа, тогда
математическое ожидание модуль это в степени u корень, так сказать, у той степени, больше ли равно
математическому ожиданию это в степени v корень в этой степени. Знаете, да, это неравенство Липунова.
Значит, какой факт из него сразу следует? Что если у вас есть, существует мат ожидания это в степени u,
то существует мат ожидания u в степени v для всех v меньше u, но больше нуля. Или если u и v целые,
то часто вот как это, так сказать, апеллируют какому факту. Если у случайной влечены существует
начальный момент катого порядка, то существуют все начальные моменты более низших порядков. Мы
сегодня этим, кстати, воспользуемся. Так, значит, неравенство Липунова мы доказывать не будем,
ну, по крайней мере, так сказать, доказательства Липунова. Мы сейчас получим еще одно неравенство,
которое тоже вам по названию, по крайней мере, знакомо, но, может быть, в такой формулировке не
сталкивались. Значит, давайте рассмотрим g от x, выпуклую функцию, ну так, для определенности выпуклую
вниз, уточнимся. И тогда можно утверждать следующее, что для любых x и y g от x больше или равно,
чем g от y, плюс некоторое число g' от y, которое совершенно случайно совпадает с обозначением
производной, на x-y. Это для выпуклой вниз функции. Ну, если функция дифференцируемая, то это
действительно просто производная, а если не дифференцируемая, то это танго с наклона опорной
плоскости, опорной племой. Теперь давайте, так сказать, придадим вероятностный шарм. Давайте в
качестве x возьмем некую случайную личину x, а в качестве y возьмем математическое ожидание x.
Ну, естественно, предполагаемое существование. Ну и что тогда получается? Тогда получается g от
x больше или равно g от мат ожидания x, плюс g' от мат ожидания x на x минус мат ожидания x.
Для всех Омега из исходного вероятностного пространства имеет место вот такое неравенство.
По свойству интеграла Либега, аналогичное ранжирование имеет место и для мат ожиданий.
Поэтому пишем. Берем мат ожидания, так сказать, с двух сторон, если на сленге говорить. Мат ожидания
g от x больше или равно. Но вот это уже число, поэтому вот мат ожидания самому себе и равно. А
мат ожидания вот этой величины чему равно? Нулю, потому что константы выносятся. Мат ожидания x минус
мат ожидания x равно нулю. Поэтому получается, собственно, вот такое неравенство, которое называется
неравенство Янсона. Неравенство Янсона для случайных величин. Ну, кстати, не знаю, стоит ли тратить на это время.
Давайте, видимо, напишу. Значит, если теперь взять в качестве кси случайную величину модуль
это в степени v, а в качестве g от x взять модуль x в степени u на v, поскольку это больше единицы,
то это выпукло вниз функция. Подставить неравенство Янсона и как бы аккуратно две строчки провести
выкладки, то вы получите неравенство Липунова. Но с точки зрения научных приоритетов неравенство
Липунова появилось раньше, чем неравенство Янсона. Поэтому, так сказать, неравенство Липунова по праву
носит имя великого российского математика Липунова, а не является просто следствием неравенства Янсона.
Вот так. Ну, это вот давайте тот минимум про вероятностные неравенства, которые полезно знать,
хотя еще раз повторю, в каких-то конкретных задачах могут быть всякие другие.
Так, и значит, по-моему, последний объект, который нам нужно изучить из, так сказать,
математических ожиданий, это условное математическое ожидание. Я напомню,
что мы с вами вводили условную функцию распределения, ну, которая мы понимаем так,
кси меньше х при условии, что это равно у. Значит, случай, когда вероятность вот этого события не равна нулю,
не представляет никаких проблем, пока ограничимся им. Более того, будем считать,
что это дискретная случайная величина. И тогда условным от ожидания мы вводим самым естественным образом.
Условное от ожидания кси при условии, что это равно у, ну, иногда пишут равно у, иногда как бы нет,
это есть интеграл х на условную функцию распределения, то есть простым от ожидания это xdf функция распределения,
а условным от ожидания это х на d условную функцию распределения. Вот так мы ее определим.
Скажу попозже пару слов на эту тему. Так, теперь давайте вот какую последствию проделаем.
Выпишем функцию распределения случайной величины кси. Это по определению вероятность того,
что кси меньше х и по формуле полной вероятности это сумма вероятности того, что кси меньше х при
условии, что это равно некому y житому. Здесь я уже пользую то, что это дискретная случайная величина,
сумма по ж. И это равно в наших терминах сумма f кси это х при условии y на вероятность того,
что это равно y житое. Я бы здесь не стал выписывать отдельно непрерывный дискретный случай из общего
интеграла Либега, но мы помним, что это всего лишь такая мнимоническая запись. Просто чтобы не
выписывать каждый раз непрерывное через плотность, а дискретное через суммы, мы вот пользуемся
записью интеграла Либега Стилтеса. Так, значит, вот и кси такая. Пойду вот сюда по кругу.
Давайте выпишем теперь мат ожидания кси. Это есть интеграл df кси от х. Вместо dfc подставляем
вот эту формулу и получаем сумма вероятностей того, что это равно y житое на интеграл х df
кси это х при условии y житое. Да, вроде так. Теперь обращаем внимание, что вот это как раз наша
екси при условии это равно y житое с одной стороны, а с другой стороны это просто некая
функция от y житого. Функция от y житого. И что мы здесь видим в этой сумме? Значение
функции в точке y житое умножено вероятность того, что случайно чина примет это значение. По
нашим определениям, по всем, это есть математическое ожидание фи вот это. Правильно, да? Вот функция
на вероятность. Ну или перепишем это в такой общей как бы так принятой форме. То есть сначала
берется условное мат ожидания, а потом берется, так сказать, мат ожидания по той случайной
величине, которая была условием, потому что условным от ожидания это случайная величина. Вот фи от y житого,
ну фи от этого. Тут, конечно, возникают, могут, ну есть вопросы, а будет ли эта функция измеримая? То
есть, ну давайте мы это просто постулируем. И вот, собственно, как бы сказать, основное уравнение
условных математических ожиданий. Я иногда здесь ставлю внизу это, чтобы было понятно, о чем идет
речь с точки зрения внешнего от ожидания. Ну это необщепринято. Вот, иногда ставят, иногда не ставят.
И теперь как бы пару слов. На самом деле, наше определение условно-математическое ожидание
через условную функцию распределения имеет ряд родовых как раз травм, связанных с тем, что вот
эта вероятность может равна нулю. Точнее говоря, нас интересуют такие случаи, когда эта вероятность
равна нулю. И мы с вами на прошлой лекции получили, что в качестве условной функции плотности надо
взять вот такую. И вот если такую взять, то в смысле не там предельного перехода, который мы сделали,
можно считать, что это условная плотность. Вот. Ну вот, это родовая травма переносится и на
условные математические ожидания. Поэтому более такой глубокий и правильный подход состоит в том,
что вот это берется за определение условно-математического ожидания. Условное
математическое ожидание — это такая функция, одно из ее свойств, что когда вы берете по ней
мат ожидания по условию, вы получаете мат ожиданий с одной случайной влечены. Вот. Этот подход,
ну там, теоретически безупречен, но у нас, к сожалению, нет на него времени. Тем не менее, так как мы это
сделали, он более наглядный. Здесь как бы, ну так все естественно. Условная функция распределения есть,
вот и определяем условное мат ожидания. Здесь это, так сказать, выглядит гораздо более абстрактно,
но, тем не менее, такой подход существует, и он, с теоретической точки зрения, там более общий,
более гармоничный. Но мы с вами, еще раз повторю, определим вот так, в виду экономии времени. Так,
вроде по мат ожиданиям и свойствам все, что я хотел сказать, поэтому мы с вами приступаем к следующей теме.
Ага, ну сейчас будет звонок, я так понимаю, и мы приступим к ней после перемены.
Следующая наша тема, это характеристические функции. То есть, для любой случайной влечены,
мы можем определить такую функцию, фи, кси, акте, которая есть математическое ожидание,
е в степени, и, т, кси. Не слышу звонка, да? А был, да? Ну все, дыхайте тогда, что-то.
Так, прошу прощения, я все-таки в части вот этой формулы приведу один пример, существенный,
как бы важный, но простой. Давайте рассмотрим, какую случайную влечину с, которая равна сумме
неких ксиджитов, но в случайном количестве. То есть, ксиджиты – это независимо одинаково
распределенные случайные влечины, ну а n – дискретная случайная влечина некоторая. И мы хотим
найти мат ожидания s. Воспользуемся вот этой формулой и напишем, что это мат ожидания по n,
на математическое ожидание суммы ксиджитых, g от единицы до n, при условии, что n равно некому n
малому. Это равно сумма. Если n большое равно n малому, то это мат ожидания суммы независимых
одинаково распределенных величин и равна на n на мат ожидания кси. А мат ожидания, чтобы взять по n,
надо умножить вероятность того, что n равно n малое. И по всем n взять сумму. Равно, равно, мат
ожидания кси выносим за скобки, а то, что остается в скобках, это мат ожидания n. И получаем такую,
в этом смысле, естественную формулу. Мат ожидания s равно мат ожидания кси умножить на мат
ожидания n. Если повозиться аккуратно по такой же технологии, то можно получить дисперсию s,
которая окажется равна дисперсии кси умножить на мат ожидания n. Такой вполне ожидаемый член.
Но это не все. А еще есть дисперсия n, умноженная на мат ожидания кси в квадрате.
Итак, в практическом применении вот этот, конечно, член много чего портит. Эти формулы
довольно широко применяются, потому что существует довольно много моделей, в которых,
вот рассматриваются такие случайные величины. Сумма независимых случайных величин, но взятая
в случайном количестве. Вот, тогда давайте теперь уже все и перейдем к характеристическим функциям.
Значит, характеристическая функция определяется для любой случайной величины. Это есть
математическое ожидание e в степени i. Это корень из минус единицы, комплексная i,
текси. Через интеграл Лебега-Стилтеса записывается вот так. Для дискретного
случая это будет вот такая сумма, для неправильного случая это будет вот такой интеграл. Что это такое
с точки зрения анализа математического общего? Что это за объекты? Знакомы вам? Это фурье
преобразования, фурье образ. То есть, на самом деле, характеристическая функция это фурье образ
функции распределения. То есть, как математический объект, он из себя ничего нового не представляет.
Ну, надо сказать, что для теории вероятности это всего лишь, по большей части, всего лишь такой
удобный инструмент, техника. Какого-то глубокого теоретического смысла в нём нет, но техника очень
удобная. Давайте в этом убедимся. Но перед тем, как мы это убедимся, давайте рассмотрим основные
свойства. Первое свойство, значит, это функция fx от t по модулю всегда меньше или равна единице,
а вот fx от 0 равно единице для любой x. То есть, это комплекснозначная функция, модуль которой меньше
единицы и максимума она достигает в точке t равно 0. По крайней мере, это очевидно. По крайней мере,
это говорит о том, что характеристическая функция любой случайной влечины существует. У любой
случайной влечены есть фурье образ. Второе не бесполезное свойство, эта функция равномерно
непрерывна. То есть, для любого epsilon существует h, но больше нуля не пишу для краткости. Такой,
что модуль xt плюс h минус fx от t меньше или равно epsilon для любого t. Равномерно непрерывная
функция. Значит, третье свойство может быть самое важное или, так сказать, наиболее продуктивное
с точки зрения применения аппарата. Если xi и это независимые случайные влечины, то характеристическая
функция их суммы равна чему? Произведению характеристических функций. Ну, здесь пару слов скажем.
Значит, характеристическая функция суммы это математическое ожидание e в степени it xi плюс
это равно математическому ожиданию e в степени it xi умножить на математическое ожидание e в степени it это.
Это понятный переход. Скажу два слова, потому что важно. Есть такие факты, которые не сложные,
если их не усвоишь, потом бывает сложно. Вот пусть у нас xi и это независимые случайные влечины,
тогда случайные влечины phi от xi и psi от it тоже независимые случайные влечины для любых
баррельских естественных функций. Понятно, почему? Всем понятно? Здесь комплексная,
но если расписать там будет phi, грубо говоря, sin, a, psi, cos. Значит, вот так вот это разложится в
такое произведение, а это по определению есть. Вот, значит, следующее свойство.
Следующее свойство четвертому назовем. Они не по важности, я их перечисляю. Значит,
пусть у нас это равно a xi плюс b, тогда характеристическая функция это равна e в степени it b на
характеристическую функцию xi, взятую в точке a t. Не вызвать затруднение у вас расписать.
Коллеги, если кому-то что-то непонятно, вы говорите, я чуть поподробнее скажу,
просто здесь дольше писать. Вот, точнее говоря, если вы это сделаете, то у вас все получится. И
еще свойства выделим. Ну, выпишем еще раз определение. Давайте к раз эту функцию по t продифференцируем.
Вот так. И возьмем в точке t равно нулю. Если t равно нулю, то это у нас превратится,
вот здесь напишу, и в степени k на математическое ожидание xi в степени k. Правда, да? Но при
условии, что этот момент существует. Если он существует, то только тогда можно дифференцировать.
Согласны, да? Ну, и отсюда вспоминаем неравенство Липунова. Следует, что если каты,
производные в нуле, точнее говоря, если существует каты, начальный момент случайно
влечены, то все производные в нуле характеристической функции до катова, первая, вторая, третья каты будут
определяться вот такой формулой. Будут определяться вот такой формулой. Вроде ничего не забыл. Если
чего забыл, по ходу вспомним. Значит, все свойства, кроме третьего, носят абсолютно технический
характер. Третье свойство несет некую идеологию. То есть, вот тот факт, что если независимые
случайно влечены, то любые функции от них тоже независимы. А все остальное, ну, просто так,
технические, технические свойства выписали для удобства. Не потому, что это какие-то серьезные
результаты. Это вообще, говоря как бы, общие свойства интеграла Фурье. Вот. Ну и, собственно, из этих
свойств уже видно, какой практически прог с этой характеристической функцией. Во-первых,
можно моменты считать легко, так сказать, если у вас есть характеристическая функция. И можно
характеристические функции суммы независимых случайных величин тоже легко выписывать.
Так, ну давайте для примера просто, значит, для примера просто возьмем распределение
Бернули и посчитаем его характеристическую функцию. Это дискретная случайная величина,
поэтому фи B от P в точке T мы должны вон по формуле взять сумму. У нас случайно влечена,
принимает значение 0 и 1, значит это будет E в степени IT 0, 0 приняла значение случайно влечена,
на вероятность того, что она равна 0, это мы Q обычно обозначаем, плюс E в степени IT на 1,
номератность того, что она равнилась, это P, то есть это получается Q плюс P на E в степени IT.
Теперь давайте возьмем биномиальную величину с параметрами N и P. Кто мне с ходу скажет,
почему ее равна характеристическая функция? Вспоминаем, что это сумма независимых Бернули,
а характеристическая сумма функции независимых Бернули равна произведению, поэтому ее характеристическая
функция Q плюс P на E в степени T в степени N. Для примера. Теперь давайте еще один важный
пример, это с вашего позволения Сатру. Теперь давайте рассмотрим нормальное,
стандартное нормальное распределение N0,1 и выпишем о характеристическую функцию. Это по
определению интеграл от минус бесконечности до бесконечности E в степени ITх единица на корень
из 2P е в степени минус x квадрат пополам dx. По определению, е в степени Tx комплексная
часть, это будет sin Tx, начетную функцию 0 уйдет, останется только единица на корень из 2P интеграл
cos Tx е в степени минус x квадрат пополам dx. Теперь давайте посчитаем производную по Т этой
функции. Производная по Т, это будет равно единица на корень из 2P, cos это sin, я напишу sin Tx,
минус запихну в минус x, это я по Т продиференсировал, но здесь е в степени минус x квадрат пополам dx.
Так вот, давайте теперь этот интеграл возьмем по частям. Этот фокус Феймана,
знаком вам такого выражения? Итак, sin Tx это будет у, а dv будет минус x на е в степени минус x квадрат
пополам dx, то есть d от е в степени минус x квадрат пополам, равно, вот сюда перейду,
равно единица на корень из 2P остаётся, тут будет у на v sin Tx, это не x квадрат пополам,
минус бесконечность до бесконечность, минус u dv, значит, минус v du, du это T, cos Tx, потому что
теперь дифференцируем по x, значит, на е в степени минус x квадрат пополам dx. Так вот,
это вот с этим сравниваем и видим, что а, и прошу прощения, а вот этот корень ещё, вот он здесь,
значит, и видим, что вот это на самом деле phi от T, и мы получаем, идём в начало и получаем вот такое
уравнение, phi штрих от T равно минус T на phi от T, граничное условие phi от нуля, как мы знаем, равна
единице, характеристическая функция в нуле всегда равна единице, правильно, да? Но вот это 0, когда мы
предел подставим, вот. Решение от уравнения, кто подскажет, е в степени минус T квадрат пополам,
решение от уравнения, и это и есть характеристическая функция стандартного нормального распределения. Ну,
сейчас давайте что-нибудь из этого факта постараемся выжать.
Перво-наперво такая техническая деталь. Давайте введём величину x по правилу,
это a умножить, не a, а m, ой, прошу прощения, не m, sigma, sigma умножить на n 0,1 плюс m, sigma больше нуля.
Так, поставим себе такое ограничение. Ну и рассмотрим функцию вероятности этой случайной
величины. Это вероятность того, что xi меньше x, это равно вероятности того, что n 0,1 меньше,
чем x минус m, делить на sigma, помним sigma больше нуля, правильно, да? А это у нас по определению,
есть функция распределения n 0,1, взятая в точке x минус m, делить на sigma. Ну и давайте плотность
f xi от x получим, продиференцируем. Значит, производная вот этой функции, это плотность стандарта
равного распределения и на производную аргумента. То есть получаем единица делить на sigma корень
из 2p, e в степени минус, вместо x подставляем x минус m в квадрате, делить на 2 sigma квадрат.
Что это такое? Это функция плотности произвольного нормального распределения. Какое мы вот отсюда
делаем? Линейное преобразование нормальной случайной величины оставляет ее в том же классе.
Но здесь не существенно, что я с n 0,1 начал, да? Вот, значит, итак, первый вывод. Нормальная
величина остается в том же классе. Теперь давайте рассмотрим характеристичку.
Пока не пользуемся, сейчас попользуемся. А теперь давайте выпишем характеристическую
функцию вот этой вот x. В соответствии со свойством, по-моему, которое было 4,
которое я стер, когда линейная связь между случайными величинами, это будет равно е в степени
и t, m на характеристическую функцию стандартной равновечины, взятая в точке sigma t, на e в степени
минус sigma квадрат t квадрат, делить пополам. Вот характеристическая функция, ну, нормальной
случайной величины с произвольными параметрами m и sigma. И, наконец, еще один приятный факт.
Пусть у нас есть набор вектор из нормальных случайных величин, каждый со своим параметром
м от ожидания дисперсии. Ну, и их там каштук. Рассмотрим случайную величину, которая равна
их сумме. Характеристическая функция ее, поскольку они это сумма независимых случайных величин,
по свойству это произведение характеристических функций. И это получается, вот сюда смотрим,
e в степени и t, сумма м житых, минус t квадрат, сумма sigma житых в квадрате, делить пополам.
А это что за характеристическая функция чего? Это нормальное распределение
параметрами с суммой м от ожидания и суммой дисперсии. Я что-то запамятовал. Тут мы неявно
пользуемся взаимооднозначным отображением между функциями распределения и характеристическими
функциями. Но это свойство интеграла Fourier. Между образами и прообразами существует
взаимооднозначное соответствие. Поэтому из того факта, что я получил характеристическую функцию s,
я отсюда делаю вывод, что функция распределения s вот такая. Ну вот такой факт интересный и
любопытный, что сумма нормальных случайных величин, независимых пока, на самом деле любых,
но пока независимых, сумма нормальных случайных величин независимых имеет нормальное распределение
вот с такими параметрами. Ну и давайте дальше. Что еще нам нужно знать про характеристические
функции? Для того, чтобы это стало у нас еще более серьезным инструментом, помимо того,
что существует взаимооднозначное соответствие, давайте вот какую рассмотрим постановку. Пусть у
нас есть последовательность функции распределения. Ну какая-то последовательность функции
распределения. Ну точнее говоря, есть случайные величины, им соответствует функция распределения и
им соответствует последовательность характеристических функций. В достаточно широком классе,
если вот эта последовательность образов поточительно сходится к какой-то функции,
то и последовательность прообразов тоже сходится к какой-то функции, и фурье образом предела
является вот этот предел. Ну как бы там есть ограничение, но класс достаточно широк. Но у
нас есть одна особенность. Нас не интересует любой предел вот этой последовательности. Мы
хотим, чтобы эта последовательность была функцией распределения. И здесь существует тонкость. Как
говорят, есть нюанс. Для того, чтобы с ним разобраться, давайте еще одну сделаем полезную упражнение.
Пусть у нас случайная величина равномерно распределена на минус аа. Найдем мы характеристическую
функцию. Значит, это единица делить на 2а, это плотность интегрирования от минус а до а,
а не от минуса бесконечности до бесконечности, е в степени itx dx. Ну опять комплексная часть
обнуляется и получается интеграл cos tx от минус а до а единица делить на 2а dx. Это у нас
что такое получается. Получается sin tx делить на 2at от минус а до а. Ну и получается это sin at
делить на at. Правда, мы тут незаметно воспомнились тем, что t не равно 0. Ну а когда t равно 0,
то всегда единица. Ну а теперь можем примером проиллюстрировать особенности, которые могут
возникать при предельных переходах в пространстве функций распределения и в пространстве характеристических
функций. Значит, давайте рассмотрим как раз для примера. Значит, такой случай, ну ксиен имеет
равномерное распределение на минус нn. Тогда характеристическая функция phi на t будет иметь
вид sin nt делить на nt. При t не равно 0. При n, стремящемся к бесконечности, смотрим вот этот предельный
переход в пространстве образов. Это стремится к чему? К нулю. Для любого t не равно 0. Ну а когда t
равно 0, это единица. Давайте посмотрим, как ведет себя последовательность функции распределения.
Я нарисую одну минус нn, единица, одна вторая. Вот функции распределения, случайно влечены равномерно
распределенные на минус нn. При n, стремясь к бесконечности, поточнее это к чему сходится?
Функции f от x. Поточечный предел, прям стремясь к бесконечности такой функции. Что? Поточечный
предел. В каждой точке к чему эта функция стремится, если t начинает разъезжаться? К 1 и 2. И это не
функция распределения. Видите? Сходимость-то есть, но не к функции распределения. Это нас не устраивает,
и поэтому мы хотели бы знать, откуда это возникло. Ну собственно, почему? А вот ровно поэтому,
потому что характеристическая функция у вас разрывна в нуле. Вот если этого нет,
то это соответствующий теоретический результат. То есть если сходимость к функции непрерывной в нуле,
то гарантируется, что функции распределения имеют пределом в функцию распределения,
по крайней мере, во всех точках непрерывности последней.
Итак, значит, если мы хотим воспользоваться аппаратом характеристических функций для
исследования симптотики случайных величин функции распределения, то мы должны смотреть, чтобы предел
у нас здесь был непрерывный в нуле. Вот если он непрерывный в нуле, тогда можно и обратное
преобразование взять, и станет понятно, куда поточно сходится функция распределения.
Вот после этого последнего замечания давайте перейдем к доказательству. Такого тоже вполне
уже результата, хотя мы можем этого не делать. Мы с вами сейчас докажем теорему мавролапласа,
она будет следовать из десятка результатов, которые мы получим позже, но для того,
чтобы продемонстрировать, как работает аппарат характеристических функций,
давайте это доказательство проведем сейчас с помощью аппарата характеристических функций.
Так, пусть мюен принадлежит биномиальному распределению с параметрами n и p. Ну,
например, подбрасывание монеты, любая схема подойдет. Значит, мотожадание мюен чему равно?
n на p. А дисперсия мюен? n, p, q. Теперь давайте введем случайную величину,
ну так наша любимая буква это n, которая есть мюен минус мотожадание мюен и делить на корень
квадратный из дисперсии мюен, что на самом деле означает мюен минус np делить на корень квадратный
из npq. Мотожадание это n, чему равно? 0. Вычли из случайной величины и мотожадания. Дисперсия
это n, чему равно? 1. Совершенно справедливо. Вот такие случайные величины, иногда называют
нормированными. То есть, если из любой случайной величины вычислить все мотожадание и поделить на
корень дисперсии, то получившаяся случайная величина будет иметь нулевое мотожадание и единичную дисперсию.
Ну и давайте найдем характеристическую функцию это n, которую я обозначу.
По определению, это математическое ожидание e в степени it мюен минус e мюен делить на корень
квадратный npq. По свойствам характеристической функции константу можно вынести e в степени
минус it. Тут я поставил значение, а тут поставил мат ожидания. Тут p, np и tpn на корень
квадратный из npq. И на характеристическую функцию мюенная, умноженную на константу
единиц на корень из npq. По свойствам характеристическая функция, это значение
характеристической функции мюенная, взятое в точке t делить на корень из npq.
Правильно, да?
Равно.
Равно.
E в степени i минус it делить на корень квадратный npq в степени n. А характеристическая функция,
вот это, я ее стерано здесь было написано, это q плюс p на e в степени it. То есть на e в степени
it на корень из npq. Так, а вот здесь, извините, я забыл b.
И в степени n. Пока мы просто пользуемся тем, на что обратили внимание, когда выписывали
свойства. Ну что, одинаковые степени. Вношу под скобки. Получаю q на e в степени минус itp
на корень из npq плюс p. А здесь смотрим it минус itp. Это it единицы минус p. Это q. Значит itq делить
на корень квадратный из npq. Все это в степени n. Равно. Равно. Разлагаем экспоненту комплексный
в ряд. q единица минус itp на корень из npq. t в квадрате пополам минус t квадрат пополам
на p квадрат на npq. И плюс о малый от т квадрата на 2, а на npq. Это и плюс о малый от единицы на
e, но я чуть попозже, ну в смысле в конце напишу. Плюс p то же самое. Единица плюс itq корень
npq тот же самый минус t квадрат пополам только q квадрат на npq. Плюс о мало единицы на n и все
это в степени n. Ну осталось две строчки у нас буквально. Значит раскрываем скобки p плюс q это
единица. Вот это член смотри чего будет представлять со знаком минус itpq делить на npq корень, а вот это
член со знаком плюс itqp на корень из pq. То есть они сокращаются. Значит здесь вот это q с этим q
сокращается, это p с этим p сокращается. Получается t квадрат на 2np со знаком минус,
здесь pp сокращается, qq сокращается. t квадрат делить на 2nq со знаком плюс. p
плюс q равно единицы значит это просто минус t квадрат на 2n плюс о мало единицы на n в степени n.
Придел. Просто чудесным образом совпало с характеристической функцией стандартного
нормального распределения. Она непрерывна в нуле и поэтому мы отсюда делаем вывод,
что вероятность того, что это n будет меньше x стремится к функции распределения стандартного
нормального распределения. То есть вот к такому интегралу минус бесконечности до x единица на
корень из 2p есть степень минус u квадрат пополам на du. Причем поскольку предельная функция не
убывающая ограниченная, то поточечная сходимость стремится поточечна. Означает равномерную
сходимость. Знаете такой факт. Ну вот собственно мы с вами доказали результат вот этот вот,
который называется теоремы муавролапласа и показывает, что усреднение с нормировкой
результатов экспериментов в схеме Бернуле при большом n по сути дела совпадает со стандартной
нормальной случайной величиной. Это первый наш пример, когда мы сталкиваемся с тем,
что концентрация меры в окрестности нормального распределения. То есть сходится к нормальному
распределению. Так, ну тогда все. Давайте так как-то не успеваем, ну будем стараться.
Спасибо коллеге, давайте отдыхайте. Благодарю вас.
С вами был Игорь Негода.

Теория Рамсе
Я хочу вам сказать очень важный инструмент, который только для теории Рамсе не нужен, но я расскажу и другие.
Это называется локальный леммо ЛООС. Это очень мощный, вероятностный инструмент.
ЛООС мне кажется уже упоминался. Здесь, конечно, потому что я рассказывал, что это пологический метод оценивания хроматических чисел петерского грамма.
Было так? Ну, те, кто ходил в трехдесятих концов прошлого семестра, знают, но их было еще немного.
Я вообще исключительно рад, что у нас произошел этот средство. Мы видим, опять будет какое-то обывание концов семестра, вызванное объективными причинами.
Там на главе какие-то появляются, люди уже не успевают. Локальный леммо ЛООС это очень классная история.
Давайте я просто какую-то затрату сначала сделаю, чтобы было понятно, о чем идет речь.
Мы когда доказываем какие-то вероятностные утверждения, в частности, в прошлый раз доказывали, мы чего делаем?
Мы говорим, есть объединение каких-то событий, ну, знаете, по ях, единицы, двадлях, н событий.
Мы оцениваем это как сумму по ях, единицы, двадлях, вероятностей аиды, потом оказывается, что это меньше единицы.
И из этого мы делаем вывод, что с положительной вероятностью не выполняется ни одно из событий, а они для нашей задачи как раз вот так, по предоставлению.
То есть вывод такой, вероятность того, что не произойдет ни одно из этих событий больше нуля.
Правда же, мы так рассуждали, например, когда оцениваем числа рапс.
У нас была цель из NPS событий, каждая из которых состояла в том, что какое-то асилементное множество то ли образует клип, то ли является независимым.
Это были вредные события, их была цель из NPS штук, и фактически для нас радость наступала в тот момент, когда оказывалось, что вероятность их совместного не выполнения больше нуля.
Локальная лямба Лобаса позволяет существенно улучшать вот этот подход, в каком-то смысле продолжая позволять нам избегать ужасных формул и включений исключений.
Красиво сказано, понятно?
Формулируется она все равно не очень приятно.
Локальная лямба?
Нет, ну сейчас она будет, вы посмотрите приятно.
Нет, она иногда применяется не очень приятно в том смысле, что для задачи Рамсея там будет довольно громоздкая выводка, ну что ж поделать.
Формулируется она просто очень приятно в каком-то смысле, потому что нам не нужно следить, сейчас я расскажу как она формулируется, тогда вы поймете.
Нет, нам не нужно следить за какими-то очень сложными пересечениями этих событий, как-то там они могут по-разному пересекаться, вероятность этих пересечений могут быть какими угодно, а там очень понятная характеристика связанная с их зависимостью.
Есть такая локальная лямба, вот я ее сейчас попробую сформулировать сначала в симметричной форме, ну а вы уж посмотрите, сложно это формулировать или нет, по-моему нет, но главное, что ее можно применять.
Это будет называться локальная лямба новоса, это локальная лямба новоса, симметричный случай.
Формулируется она так, есть n sub t, как и раньше, в каком-то там вероятностном пространстве, это нас не очень беспокоит, в каком есть n sub t,
мы знаем, что для этого и вероятность Аитова ограничена сверху какой-то чиселкой p, это нам дано, p общая для всех, не, ну понятно, что всегда можно ограничить вероятность всех событий единицы, но это наверное не та чиселка, которую хочется здесь заиметь, так чтобы вы сразу понимали,
p конечно меньше единицы, и хорошо бы в наших вот асимпатических задачах, чтобы p еще в каком-нибудь смысле стремилась к 0, ладно, есть события, и мы единообразно оцениваем их вероятности, наверное все-таки предполагается, что они просто примерно одинаковые или вообще равные, то есть вот в случае с Рамсейем оно так и было, вот здесь вот в Рамсейе вероятности Аитов у нас получались разными 2 в степени 1, минус c из-за спорта,
если вы посмотрите на прошлые лекции, то те события, которые отвечали числом Рамсея, имели вот такую вероятность, но продолжаем формулировку, сейчас будет чуть менее принятно, но ничего страшного, для любого не И, а ИТ это еще одно условие, теория, вероятность параметра, условия пока особо нет, условия будут в самом конце, не зависит
от совокупности всех остальных событий, кроме не более D, опять D общая для всех ИТ,
ну давайте я пока не буду комментировать, последнее условие, пусть выполнено такое неравенство, E на B на D плюс 1 меньше либо равно ИТ, не знаю, что такого страшного, кроме того, что такое E,
ну люди не ожидают, но это то самое E, то есть это просто 2,71, это не какое-то новое обозначение, это просто число E, здесь неожиданно так вот вылезает, вы потом поймете,
ну вот такие вещи тогда, а тогда вот то, что нужно, вероятность того, что ни одно из событий не произойдет больше 0,
ну давайте попробуем понять точно совсем, что здесь сформулировано, в чем смысл этой теоремы, еще раз, что значит АИТ не зависит от совокупности всех остальных событий, кроме не более D,
ну вообще говоря, вас должны, вас должны хорошо учить сейчас вопросом зависимости, независимости,
то есть есть всего D, есть всего D, не более чем D событий, с которым пересечение 0, с которым не 0 пересечение,
нет, нет, не зависит, это уже не про пересечение, подождите, подождите, подождите, я подробнее скажу, есть не более чем D событий,
ну кроме АИТ, есть АИТ, и из оставшихся N-1 событий можно убрать не более чем D штук, убрать не более чем D штук, так что от всех оставшихся событий АИТ зависеть не будет,
что значит АИТ не зависит от совокупности каких-то событий, ну страшно можно сказать, что не зависит от сигма алгебры, которую имя порождено,
и вообще говоря, я думаю, что все такое слышали и даже учились это понимать, нет, я не прав, еще не успели такого слышать,
ну ничего страшного, мне-то это не нужно, мне-то это не нужно, я просто думал, что вы уже это слышали, и поэтому я могу сказать,
а я вообще тогда ничего объяснять не буду, но я конечно буду в любом случае объяснять, потому что для меня это очень простая вещь,
а подверждается просто следующее, для любого G, ну давайте я так скажу, А не зависит просто определение там,
от В1 и так далее ВК, если для любого G из 1, 2 и так далее К, вероятность, значит здесь напишу, А при условии пересечения пажи маленьких,
пажи большой, пажи так, равняется вероятности А, от совокупности, да, это именно независимость под совокупность событий,
вот так, ну условная-то вероятность была, подождите, если условной вероятности не было, тогда конечно мне придется больше напоминать, но это довольно странно было,
условная вероятность-то была, бывает такая условная вероятность, но очень естественное определение, вероятность того, что произойдет А и вероятность того, что А произойдет при условии любого набора этих событий,
это одно и то же, ну конечно тогда мы будем говорить, что А не зависит от их совокупности, сейчас понятно, очень просто, очень естественное определение,
и здесь мы просто говорим, вот пусть нам известно откуда-то, что эти события по возможности не слишком зависимы локально, почему она локальная,
потому что зависимости в каком-то смысле локализуются, вот у нас выделено какое-то конкретное событие АИТ, и локально, если на него и влияют,
вот на него если влияют какие-то еще события, то их не более чем D штук, а то всех остальных оно в этом смысле не зависит, мне кажется это совершенно понятно,
чем меньше D, тем естественно меньше вот это выражение и тем больше шансов, что оно окажется меньше 1 и все получится,
но самое главное, что вот здесь, вот в этой части как получается меньше 1, если считать, что у нас как раз как в локальной ремме вероятности всех событий не больше чем P,
здесь это получается не больше чем NP, правильно, это все что мы знаем, и нам нужно чтобы N умноженное на P было меньше единицы,
а здесь нам нужно чтобы D умноженное на P было меньше единицы, но с точностью какого-то E, которое в общем как правило в приложениях и какого большого смысла не несет,
но тем не менее его надо писать, но грубо говоря, здесь 5P должно быть меньше единицы, а здесь NP должно быть меньше единицы, N это количество всех событий,
а если вот этих локальных зависимости совсем мало, то это дает конечно колоссальное улучшение по сравнению с банальным методом.
Вот так сложилось, заказали, что будет в каком-то смысле следовать,
ой сейчас уже не могу, я только написал, что в понедельник в этом-то я буду здесь, а через две недели я уже буду отсутствовать.
Не повезло.
Я вроде взял его, а он не принял.
Говорю чего-то, а он не принял.
Нет, почему, это сейчас будет видно из доказательства, что браться того метода, который мы будем применять, его выкинуть нельзя.
Другое дело, почему действительно оно посуществует?
Даже это известно, только я никогда этого не рассказываю, но довольно долго приводить соответствующий пример.
Есть изысканные примеры, которые показывают, что е здесь обязательно нужно.
Работы на обычные 80-х годов, которые показывают, что е убрать действительно нельзя.
Это недавнее что-то, Лобус это придумал в 70-е годы, это очень много чего дало.
Конкретно для рамсе гора как раз родит мышь, но помимо рамсе есть масса других приложений, в которых это офигенно работает.
Но и весь смысл заложен в том, о чем я сейчас говорю.
Значит, смотрите, давайте подумаем, как лучше построить нам домейшую деятельность.
Наверное, я рожу мышь сначала.
Но все-таки мы рамсеем занимались.
Но нам потом надо будет это наказать.
Давайте сделаем так.
Первое действие.
Первое действие.
Теорема 1.
Р от СС.
Больше либо равняется чего-то.
Но вот это что-то, это будет мышь.
То есть мы не сильно улучшим результат.
Но ничего страшного.
Это не все.
Теория рамсе просто сложная, а локальные леммы все равно хорошие очень.
Значит, мы ее дальше захотим доказывать.
Но доказывать я никогда не доказываю напрямую, потому что я хочу рассказать и несимметричный случай тоже.
Поэтому второй пункт программы будет формулировка.
Потребует времени.
Несимметричного, то есть общего, в том числе несимметричного, случая локальной леммы.
Более общего, случая локальной леммы лобаса, из которого в частности будет вытекать теория матины.
Соответственно, третий пункт будет вывод.
Просто чтобы вам было понятно, что происходит в каждый момент времени.
Вывод теоремы 1 из несимметричного случая.
Вот там будет видно, откуда взялась Е.
Потом будет доказательство несимметричного случая.
То есть мы окончательно замкнем все.
Только вот в этот момент.
И общий случай, и частный случай симметричный будет выведен из него.
И мы же куратим.
Тут все в этом порядке будет двигаться в конечном счете.
Доказательство несимметричного случая.
И пятый, дальнейший пункт, это еще более интересные приложения.
Еще приложения LLL.
Причем как в симметричном, так и в несимметричном случае.
Потому что иначе вот этот пункт 4 подвиснет в воздухе.
Вы мне скажете, а нафига я это делал?
Вот эти два пункта.
Ну доказал бы сразу.
И все.
Но есть масса интересных результатов, которые получаются именно из несимметричного случая.
В частности, друзья, чтобы было совсем понятно, вот здесь в пункте 5.
Будет присутствовать число Рамсея Р от 3, запятая Т.
Про которое я в прошлый раз говорил, что оно в общем случае неизвестно.
Вот.
То есть вот сюда пойдет из несимметричного случая Р от 3.
Вот такой план на сегодня и может быть на через неделю тоже.
Наверняка это все невозможно за сегодня рассказать.
Только начнем.
Так, ну рожаем мышь.
Ладно.
Так.
Теорема мыши.
Ну ладно, она 2 конечно.
Не знаю.
Зачем здесь эта номерация?
Зря.
Теорема мыши.
Значит, утверждение такое.
Р от СС больше либо равняется 1 плюс о малой от единицы.
Так.
Сейчас скажу.
На корень С2 поделить на Е.
На С.
На 2 в степени С.
Вообще, если вы помните прошлые лекции, то это должно произвести впечатление.
Потому что у нас сначала самый простой вероятностный метод давал оценку,
в которой в знаменателе стоял корень С2 вместе с Е.
Потом этот корень С2 просто пропал, тут была единичка.
А сейчас он снова появился, но уже в числителе.
То есть все последовательные улучшения нижних оценок числа Ромсэ,
которые у нас с вами получались,
это каждый раз в корень С2 кирас.
Два раза улучшали, оба раза в корень С2 кирас.
Но я могу вас порадовать, что это мышь, но это мышь золотая.
Потому что никто лучше не умеет.
Это самый лучший известный результат, который получен в настоящее время.
То есть диагональные числа Ромсэ, в общем случае, 8.
Лучше, чем вот так, никто оценивать не умеет.
По всю пору, как только придумал Лобас локальную лему,
в частности, как следствие, получили эту.
Ну, конечно, мышь, но лучше, а лучше еще не могут.
То есть заменить корень С2 на полтора, например, не получается.
Так, ну давайте доказательствам.
Ну, мы в очередной раз вспоминаем то, что было на прошлой лекции, то есть вот то, что я здесь напоминал.
Нужно, наверное, напомнить, да?
Чуть более подробно, да иначе просто трудно воспринимать на слухах.
Ну, я не знаю, можно не записывать, вы это все писали в прошлый раз,
вы можете отошлать в себя к лекции, которая была недели назад, но картина такая.
Вот у нас есть какое-то количество вершин, мы пока не знаем, что такое N, но оно в итоге должно вот таким оказаться.
Вот мы берем какое-то количество вершин.
Дальше мы выбираем из этого количества S вершин, вот это самое S, которое здесь.
И рассматриваем вот такие события.
А1, и так далее, а с индексом заизн пайс.
Это событие, состоящее в том, что ИТ, S элементное множество является кликой или независимым.
Кликой или независимым.
И вот я как раз там написал, что вероятность каждого такого события это, конечно, 2 в степени 1 минус C из-за сподв.
Одна вторая степень C из-за сподв, это вероятность, хоть отсутствие всех реберных вершин, хоть присутствие.
Поэтому 2 и вы получаете.
Знаете, все было. Это все было.
Так, то есть вот это P, вот это P из формулировки локальный леммелопис. Они просто все одинаковые.
Так, вот я специально напомнил определение события, чтобы можно было подумать, а что такое D.
То есть нам дано какое-то множество из S вершин.
И событие состоит из графов, для которых это множество то ли клика, то ли антиклика, независимо.
Вот от каких событий может зависеть данное событие.
Пересекаются хотя бы по 2 вершинам.
Да, совершенно верно, которые пересекаются хотя бы по 2 вершинам.
Если одна вершина общая, там просто ребер общих не будет, поэтому никак не повлияет.
Но можно D в этом смысле написать прямо в точности.
То есть это количество таких множеств в мощности S, которые вот это фиксированное множество пересекает и менее чем по 2 вершин.
Сейчас всем понятно? Тихо, я понял, что понял. Всем понятно, да?
Хорошо.
Ну как это посчитать? Ну можно сумму написать большую, можно разность какую-нибудь написать.
А я сумму напишу.
Мне кажется, что так понятнее.
По K от 2 до S-1 С из S по K умножить на С из N-S по S-K.
Правильно?
Ну то есть K это как раз количество общих вершин.
Я сначала из фиксированной сардельки выбираю вот эти K вершин, а потом мне надо еще до выбора S-K из оставшихся N-S.
Вот отсюда выбираю K общих, а из оставшегося выбираю S-K.
Вот D будет таким.
Это шутка какая.
Ну да, вы можете сказать, что если суммировать по K от 0 до S, то получится.
Понятно, что такое, ну можно вычитанием это написать.
Но я сейчас вот чего скажу.
Давайте я скажу, что D уж точно не превосходит следующей величины.
Я не буду объяснять, что это огрубление дает прям вот оптимальный результат в итоге.
Это если хотите проверить победителей, что называется, не судят.
Улучшить вы меня не сможете на этом пути.
Значит, я напишу не такую страшную сумму, а более простую оценку.
Смотрите, С из S по 2 умножить на С из N-2 по S-2.
Вот так, по S-2.
Это грубая оценка, это меньше строго, конечно.
Не пытайтесь ее получить как непосредственное следствие отсюда.
Это не так очевидно, хотя черт вас знает.
Может и получите.
Я по-другому рассуждаю, смотрите, вот почему-то это не сразу доходит, но давайте попробуем понять.
Я что вот здесь делаю, когда так оцениваю D?
Я сначала фиксирую две общих вершины, какие угодно две.
А потом оставшиеся S-2 выбираю, видите, откуда, не из N-S, а из N-2.
То есть вот я нарисовал сардельку, зафиксировал две общих вершины,
а вторую сардельку к ней приляпал, в том числе выбирая ее вершины отсюда тоже.
Конечно, это ограбление, потому что я мог многое посчитать миллион раз.
Жаль.
Понятно, да, чего я говорю?
Оценить количество, множество мощностей, которые данное множество пересекают по двум и более элементам,
можно вот так взять какие-то два элемента, остальные добраться откуда угодно, хоть снаружи, хоть внутри.
Вот будет такая оценка.
Я утверждаю, что для наших асимпатических целей плевать, вот так написать или вот так.
Но помните, я в прошлый раз какие-то там издевательские обозначения вводил, теорема штрих, теорема два штриха,
я, честно говоря, все позабыл, но я шутку это делал.
Можете мне напомнить, то есть, конечно, вот такая форма, более точная форма,
если доказывать не асимптотику, а пытаться вот это компьютерное решение находить,
но она лучше, конечно, то есть, вот я тут не помню, с одним штрихом была теорема, с двумя.
Так.
Та, которая про, как называется, про точные вычисления.
Ну ладно, Лева, сейчас она будет просто штрихом, раз вы не хотите меня доказать.
Теорема два штрих.
Сегодня она будет просто со штрихом.
Теорема два штрих.
Я нас не считаю.
У меня 1 на 2с.
Н таково, что е умножить на 2 в степени 1 минус цс по 2 умножить на сумму,
пока будет это с минус 1 цс по к цс минус с по с минус к плюс 1 меньше либо равно 1,
тогда r от сс больше чем n.
Но это уж точно не мышь, потому что это дает там какие-то конкретные улучшения для конкретных с.
То есть вы берете просто для данного с в цикле по n и добегаетесь до максимального значения, при котором все еще выполнено вот это нерадкость.
Тут никаких ассептотик, может просто явно все сосчитать, но у него вот самая лучшая n будет самая лучшая цепь.
Это у нас такая же теорема должна была быть на прошлой лекции.
В смысле?
Или это какая-то новая?
Это новая, потому что она вытекает из логальной реммы вот эти вот вычисления, которые вы только что провели.
Вы знаете, что мы сделали?
Мы вычислили вероятность каждого из вредоносных событий.
Оно вредоносное, потому что образуется в графе то ли клика, то ли независимое множество.
Мы вычислили точное значение d, ну как смогли, вычислили.
И тогда вот локальная лемма Ловаса нам говорит, что если взять такое p, такое d и удостовериться в том, что справедливо вот это неравенство,
то вероятность пересечения отрица не больше нуля.
А значит не выполнено ни одно из вредных событий, то есть не пусто множество графов, в которых нет ни одной клики и ни одного независимого множества на s-вершинах.
А нам ровно этого и хотелось.
То есть если мы утверждаем, что r от ss больше, чем n, это мы утверждаем, что существует граф на n-вершинах, у которого нет ни одной s-крики и ни одного s-независимого множества.
Вот это пересечение отрицаний, оно из таких графов и состоит.
Вредное множество, это что там есть клика или нет.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
То есть это не вредное множество.
Это новая теорема, которая вытекает в напрямую из латальи ремы Лобсона.
Но из нее, из этой теоремы 2-3 мы сейчас получим как следствие вот это синтетическое неравенство, которое я назвал мышью.
Я просто сейчас возьму в качестве N вот это выражение,
заменю вот эту мерзкую вяру, которая тут написана,
на более гораздо компактную оценку и проверю, что все получится.
Сейчас, друзья, логика понятна.
Значит наша цель подставить вот такое D вместо того, которое там написано,
и убедиться в том, что с таким N все получается.
Сейчас я это сделаю.
Значит я пишу вот так.
N на 2 в степени 1 минус C из-за сподва.
Так, на C из-за сподва.
C из-за N минус 2.
ПС минус 2.
Значит, дорогие друзья, я не хочу писать плюс один.
Я не хороший человек.
Я вот так.
За вас.
Ну чем рисовать плюс один какое-то омерзительное,
которое как-то хорошо с точки зрения вот этого,
как бы не было что-то в теории с штрихом.
Для синтетики, ну это просто пояс в горме.
Как я это плюс один буду рисовать?
Я думаю, что здесь умножим.
Его можно вообще не рисовать и так, потому что мы посчитали в том числе наше множество.
В нашей грубой оценке мы в том числе посчитали ты сам плюс один.
Слушайте, круто, да, согласен, согласен.
Можно не писать правильно.
Ну я уже нарисовал три, не надо менять.
Ничего не испортится.
Знаете, как вот принято считать P в какое-то там время равно тройке.
Ну вот ИГЕ тоже можно считать равно тройке.
P равно N.
Пик вот это равно ускорение свободного пыли.
Во всяком случае 6.
Так.
Ну, вы поймите, у меня сейчас получится, что это не просто меньше равно единица, а что это там стремится к нулю.
Ага.
Со свистом причем.
Ну, потому что я, да, вот принялся за самим.
Я же волен выбирать вот этого малая от единицы как угодно.
Там такой запас прочности.
Все у меня получится, конечно.
Сейчас главное, что констант не забудется.
И все будет хорошо.
Кройка их ни на что не влияли.
Так.
Ну, давайте писать.
Так, мы уже в прошлый раз писали.
Ничего не поделать.
Это для вас самое учительное, поэтому я честно это проделаю.
3 на 2 в степени 1 емус s квадрат пополам плюс s пополам.
Я раздумал, что скобка в цежке с и за сподровать.
Ясно, если мы садим пополам.
Так.
На, умножить надо, s квадрат пополам.
Это с и за сподровать.
Но, смотрите, то, что s стремится к бесконечности, это просто понятно.
У нас n стремится к бесконечности, а s будет двойной, двойечной алгоритм.
Поэтому я спокойно могу s умноженное на s минус 1 представлять как s квадрат.
Это корректно.
Не то, что это халтует.
Вот.
А с из n минус 2 по s минус 2, это что такое?
Это n минус 2 в степени s минус 2 поделить на s минус 2 квадрата.
Ну, я просто пользуюсь тем, как обычно, что c из n по k, t из k поделить на k квадрата симпатически.
Конечно, если k меньше и меньше корни из n, но я повторяю, k это алгоритм.
Меньше и меньше, чем корень из n, поэтому я могу.
Так, ну, слушайте, n минус 2 в степени s минус 2, это, наверное, с хорошим запасом n в степени s минус 2, нет?
Я так неуверенно говорю, потому что я не знаю.
Хочу вас немножко расширить.
Поскольку все обосходится в алгоритмическую степень, то, конечно, разницы никакой.
Если можно, вы уже это как-нибудь сами, мне, честно говоря, в такой-то степени неохота.
Значит, это асимпатически равно, пишу, 3 на 2 в степени 1 минус s квадрат пополам,
плюс s пополам, так, s квадрат пополам, тут s в степени s минус 2.
Так, а тут я бы написал как s факториал, вот это s минус 2 факториал, я бы написал как s факториал.
Но это неправильно, это s факториал поделить на s минус 2, на s минус 1 и на s.
Вообще, я зря здесь.
Тут я раз ас ас минус 1.
Ну, я не подумал об этом, дорогие друзья, ну, я не знаю, ну, я не могу.
Ну, я здесь тоже напишу s квадратов, тоже асимпатически правильно.
Ну, вы поймите, да, если бы я заранее сообразил, я бы здесь не поросил голову про асимпатику,
а просто вот с этим с умножить на с минус 1 мило сократил.
Ну, правильно же, это асимпатический s квадратик, тут тоже асимпатический s квадратик.
Ну, вот они сейчас сократятся.
Кажется, наоборот, s четвертый должно вынести.
Правда, да?
Ну, это тоже не страшно.
Нет, ну, ветер-то в поле появился нам так очень.
Ладно, я понял, да, я заранее повторялся.
Ну, тогда ничего страшного.
Ну, я обучил голову асимпатически правильно, да.
Правильно здесь делить на s квадрате.
Поэтому будет с четвертым.
Ну, сейчас перепишу, что это равно.
Нет, давайте сразу тигу нарисуем, потому что я с факториалом самим участком.
Сделан буканик.
Так, где бы мне стирать?
Наверное, вот здесь.
Мы ее применили.
А когда мы ее еще будем наказать?
Может, нисилку.
У нас сейчас звонок будет.
Ну?
Ну, сейчас.
Так, начинает идти, да.
Я еще l сейчас подставлю явно.
Ведь разница между тем, что было в прошлый раз, и тем, что сегодня очень маленькая, к сожалению.
Зависимости много.
Разница, там было n в степени s, а у нас теперь n в степени s-2.
Вот это деление на n в квадрате, оно, к сожалению, дает только муже.
Ну, вот, к сожалению.
Ладно, пишу.
На счет 3, на 2 в степени 1-s квадрат пополам, xs пополам.
Так, ну, правого, да.
На s-4, к сожалению, пополам.
Так.
Здесь будет корень s-2, bs.
S на d.
Ws-t.
И n в степени s-2.
Это я вот этот звук переписываю.
Я пишу 1-1.
Ws-2.
Вот здесь как раз моя сила.
Я могу подбирать любую нужную мне функцию.
В прошлый раз мы очень весело ее подбирали, я помню.
Так, первые квадриллионы значения были минус 1.
Ну, может, не помните.
Ладно.
Так, дальше вот это возвожу.
Значит, у меня получается...
Ну, сейчас давайте я допишу.
2 в степени s-2 пополам.
Корень из 2 в s-2 степени.
2 в степени s-2 пополам.
s в степени s-2.
И 2 в степени...
Ну, давайте сразу раскроем столбки.
Значит, s пополам и умножим на s-2.
Это будет s-2 пополам.
Ничего.
Минус s, да?
Я тут немножко ложаюсь, следите внимательно.
s на s-2 это s-2-2s.
Но пополам это s-2 пополам минус s.
Ну, вроде бы правильно.
Вроде бы ничего не забыл.
Ну, давайте я доведу до конца потом перерыв.
Ну, что тут осталось?
Там уж много.
Противно, конечно.
Вот это s.
И вот как-то так надо писать.
С этим s сокращается.
Так, а где у меня e потерялось?
У меня где-то e еще потерялось.
У меня вот это e потерялось.
Все, я понял.
Я еще вот здесь где-то писал.
e в степени какой?
s-2.
Нет, 2 минус s-3.
2 минус s-3.
Вот у меня еще вот это e в знаменателе.
Ну, с второй степень.
Ну, если я умножаю на него, то 2 минус s.
Потерял.
Видите сейчас, что потерял?
Забыл.
Ну, вот оно должно вот так сократиться.
Чпок.
Это тоже тогда сразу сокращается.
И вот этот вот чпок.
Остаётся e в квадрате.
Ну, было бы e в кубе, если бы я не настоял на том, чтобы e превратить в 3.
Ну, какая разница?
Значит, давайте я константы напишу, какие константы остались.
Вот эта единица вот с этой сокращается.
Остаётся 3.
Но тут еще будет 2 в минус 1.
Вот это 2 в минус 1.
Это 3 в вторых.
Всякая чпуха.
На e в квадрат.
3 в вторых.
e в квадрат.
Дальше 2 в степени.
Так, слушайте, мне это надоело.
2 в степени минус 2 в квадрат пополам сокращается.
Чпок.
Чпок.
Слушайте, а смотрите.
2 в степени s пополам.
И тут 2 в степени s пополам.
Какое совпадение?
Это же 2 в степени s.
А тут 2 в степени минус s.
Так, я вот так нарисую.
Шлёп.
Шлёп.
И вот это тоже шлёп вместе с этой двой.
Так, у меня ещё так.
2 в минус 2 у меня было.
Я его уже записал.
2 в минус 2 пополам.
2 в минус 1.
Так, слушайте, меня в стопы сокращалось.
О, смотрите.
У меня осталось s в минус 2.
s в четвёртый.
По-моему это s в квадрате.
Ещё 9 на корень из s.
Да, ещё 9 на...
Ну давайте на корень из 2 без.
Я сплевать вот так.
Так, чего ещё осталось?
Всё, что ли?
Не всё!
Самое главное, что...
Самое главное, конечно, друзья.
Самое главное, это вот это.
Нет, понимаете, вот если этого нет,
то я вообще облажался.
Вместе с перерывом.
Это всё, конец.
Вот если бы этого не было, друзья,
получилось бы s в квадрате
поделить на корень из s.
Это ни фига ни меньше единицы.
Вся моя сила тут.
Я могу выбрать у ад единицы так, как пожелаю.
Понимаете?
Ну, конечно, тут надо было ещё вот здесь написать
один кусок у ад единицы.
Это уж совсем аккуратно.
Это в честь коронной стиры.
Я забыл сделать, но сейчас сделаю.
Это важно.
Важно и не важно, но хотя бы как.
Так, друзья, вы понимаете,
что это при правильном подполье
забьёт эту штуку с запасом или нет?
Или надо это повторять?
Ну, мы это уже делали, конечно.
Возьмите там 1-1 по 3 на корень из s
и возведите это в s-2 степени.
В s-2 степени.
Возьмите такую функцию, стремящуюся к 0.
Это будет е в степени минус корень из s
по там на что-то, на 1 плюс в ад единицы.
Ну, конечно, е в степени минус корень из s
забивает вот это с запасом.
Всё.
Три маленьких s, да.
Возьмите минус 1, чтобы это было 0.
Да вообще вопрос.
Всё, я доказал.
Я доказал вот это как следствие.
Я доказал вот это.
Давайте сделаем это.
Так, ну, слушайте, у нас на повестке
формулировка несимметричного случая
вполне возможно, что на этом всё и закончится.
На сегодня мы, может, успеем вывод, хотя вряд ли.
Так, значит, для несимметричного случая
надо вести такой асцеп, который называется
орг-граф зависимости.
То есть ориентированный граф
в зависимости от некоторых событий.
Я подозреваю, что на планах хватит настатка
всё время, рабоческий целиком.
Значит, о чём идёт речь?
Вот у нас есть какие-то события
А1 и так далее, АН.
Ну, можно, например, сказать так.
Вот помните, в симметричном случае,
который, к сожалению, пришлось стереть,
там говорилось, что каждое событие
не зависит от совокупности всех,
кроме ни более, чем Д.
Ну, как-то это можно же вот так изобразить.
Вот есть событие А1,
а есть ещё какие-то Д события,
ну, ни более, чем Д,
состоящее из ни более, чем Д событий,
от которых АИТ может теоретически зависеть.
Ну, вот ткнём в них направленными стрелками
во все такие события.
Получится некоторая ориентированная игра.
Ну, потом возьмём событие А2,
посмотрим на то, наоборот,
какие ему события вредят,
в смысле зависимости от них есть.
Может быть, ну, АТ1, например,
где есть зависимость,
тогда стрелка пойдёт в обратную сторону,
там ещё какие-то будут, тут какие-то будут,
и так далее.
Получится какой-то орграф.
Но это не строкое определение,
это просто некоторый намёк
на то, зачем нужны орграфы,
откуда они могут появиться.
И на самом деле, когда мы будем выводить
то, что появится,
симметричный случай, который мы стёрли,
там вот этот орграф зависимости,
который я сейчас нарисовал,
нам надо дать строкое определение
того, что мы считаем орграфом зависимости
в общем случае.
Значит, давайте определение дадим так.
Он будет не единственным,
а его можно будет выбирать из целого множества.
То есть мы скажем, что
ориентированная игра в G
с множеством держит А1,
так далее АН
и множеством направленных ребер
не называется даже,
а является орграфом зависимости
для этих событий,
орграфом зависимости,
если для любого И
АИТ не зависит
не зависит
не зависит
от совокупности
тех,
ожито,
что АИТ
ожитое
не принадлежит множеству ребер,
в которые не идёт
направленная из АИТово наружу стрелка.
Ну вот, смотрите на этот пример.
Сарденко состоит из тех Д
или не более чем Д событий,
которые могут оказать влияние на АИТ.
А до всех остальных,
как это было в симметричном времени,
АИТ не зависит.
В общем случае,
мы скажем,
там какой-то ориентированный граф
в другой совершенно покажем
на множестве вершин А1,
мы будем считать,
что он является орграфом зависимости,
и мы проверим его
на то, что является он или нет.
Мы будем считать,
что он в проверку прошёл,
он является орграфом зависимости,
если для каждого АИТово
выполнено то,
что здесь написано,
оно не зависит от совокупности тех событий,
куда не идёт направленная из АИТово стрелка.
Ну без дополнительных примеров
и пояснения,
хотя примерно понятно,
чего я хотел сказать.
Дополнительные примеры
и пояснения сейчас будут.
Во-первых,
давайте попробуем понять,
какой ориентированный граф
множества вершин А1 на N
заведомо является орграфом зависимости
вне зависимости от того,
какие зависимости там есть.
Конечно,
полный ориентированный граф
всегда является орграфом зависимости,
согласно этому определению.
Не зависимость от пустоты.
Но потому что от пустоты
любое событие не зависит.
Если ребра проведены все вообще,
значит ориентированный граф
не зависит от пустоты.
Понятно,
что обычно нас интересует
минимизировать количество ребер.
Вот этот пример как раз.
Он показывает, как их минимизировать
в рамках условий стеоризма.
Но давайте другой пример.
Вот у нас есть три события.
Два, три.
Какие-нибудь, которые попарно
независимы,
но зависимы в свободности.
Достаточно с каждой вершиной
провести любое?
Да, в общем, именно так.
Ну давайте я для всех поподробнее скажу.
Я просто приведу ровно два
незаморженных
графа зависимости
с минимальным количеством ребра.
Значит один пример вот такой
цикл.
Действительно, а первое
не зависит
от свободности тех событий,
в которые не идет направленное ребро.
А именно первое не зависит
от третьего.
А второе не зависит от первого.
А третье
не зависит от второго.
Все соблюдено.
Понимаете?
Но при этом убрать
любое из этих ребр я не могу,
потому что если я, например, вот это
ребро уберу, то тогда потребуется, чтобы
А3 не зависела от вот этой
свободности.
А это не так, потому что все эти
три события зависимы вместе.
Ни одной из ребр я убрать не
могу.
Это минимальный граф.
Вот так.
Тоже три ребра,
и тоже все получается.
Так нужно.
Ну и тоже не одно ребро убрать нельзя.
Понятно, да?
То есть они могут быть разными.
Тут есть опять вот вольница некоторая.
Мы свободны в выборе.
На самом деле они могут быть...
Если вы понимаете определение,
то я готов сформулировать общую теорему.
Общая теорема, наверное,
сюда отнести,
она звучит так.
Я хочу симметрично успеть вывести.
А общую мы докажем в следующий раз.
Ну сейчас мы ее сформулируем
сначала.
Главное.
Значит, теорема 3.
Раз уж я начал
нумераться зачем-то.
Общая
ну как бы не симметричная,
локальная
теорема 3.
Ну вот.
Ну как бы не симметричная,
локальная тема Лооса.
А, 1, А, М.
Опять какие-то события.
Не важно на каком пространстве.
Пусть
Ж
такой
орграф зависимости,
я подчеркиваю слово, не такой,
что у нас есть выбор.
Мы можем выбрать еще орграф зависимости
с такой орграф зависимости,
что найдется набор чисел
х1 и так далее, хn
по числу событий,
находящихся в полуинтервалах
от 0 до единицы, единицу не включая.
Найдется
значит такое, что
найдется набор чисел,
с которыми
для любого и
то есть аитово
не превосходит иксытое
по множеству произведения
по всем Ж, таким, что
аитое, ажитое
принадлежит Е,
единица минус иксытое,
а какая,
не симметричная форма.
Тогда,
то, что нам нужно,
тогда
вероятность пересечения отрезаний
ну да,
не просто больше нуля,
а больше перебравна произведения
пое от единицы до n,
единицы минус иксытое,
но поскольку иксытое строго меньше одного,
то это больше нуля.
Просто такой
нормально то, что у нас термин
будет называться общая локальная лемма локуса?
Ну общий случай локальная лемма локуса.
Она локальная, потому что
зависимости мало,
а не потому, что она
локальная.
Ну да,
ну да.
В этом контексте почему-то приходит в голову
название известного парижского университета
высшая нормальная школа.
Ну хоть не актогональ.
Что значит, что она нормальная,
при этом высшая?
Репендикуляр, конечно.
Так, друзья,
ну вот это вот самое страшное здесь,
конечно, что-то написано,
и чтобы лучше понимать,
что здесь происходит,
и заодно понять откуда берется е
в симметричном случае,
давайте я напомню симметричную формулировку
и за оставшееся время выведу результат
симметричной леммы из вот этого общего.
И он станет понятнее,
и ею докажем.
То есть вот этот третий куб программы
мы реализуем.
А больше мы сразу что-то успеем.
Может быть, вот здесь знаете,
еще что стоит добавить,
если два события сами между собой зависимые,
то, конечно, стрелки пойдут туда-обрату.
Вот это важно понимать,
что если два события зависимые,
то в любом украще зависимости
тут будут стрелки туда и обрату.
А вот этот пример как раз показывает
что они-то могут быть попарно независимыми,
тогда стрелки туда-обрату не обязаны идти.
Но возникают более сложные зависимости,
и отсюда вылезают вот такие конструкции.
Так, симметричный случай,
это была теория М1, да?
Симметричная,
ла-ла-ла,
там были тоже события,
для любого И
вероятность Аитова
не превосходила П,
для любого И
АИТ
не зависело
ото всех,
кроме ни более Д штук,
и Е на П
на Д плюс один
не превосходило единицы,
ну а как следствие получалось
ровно то, что нужно.
Е от единицы до Н,
АИТ в чертовой больше 0.
То есть смотрите,
мы находимся сейчас,
мы считаем,
вот в этих условиях,
нам нужно доказать эту теорию.
То есть нам дано,
что вероятности не больше чем П,
не более чем Д,
и что выполнена вот эта хитрая стратегия.
Это нам дано.
Мы хотим вот это вывести,
воспользовавшись
вот этим результатом,
который мы докажем через нить.
Понятно, да?
Так, слушайте, я смог объяснить,
какой аминет подождите на В,
рассмотрим два случаев.
Первый случай,
это когда Д равняется нулю.
Он идиотский,
потому что в этом случае
вероятность пересечения бои
от единицы до Н,
АИТ в чертовой, это знаете что?
Произведение.
Потому что Д равно нулю,
это значит они независимы в свободности.
Это просто произведение
бои от единицы до Н,
вероятности АИТ
с чертовой,
которые равны единице одни равны,
а больше либо равны единице минус П,
ну это все стало быть
больше либо равно один минус П
в Н-ой степени,
то есть конечно больше нуля,
все получается.
Так, друзья, это не очень погнало.
Так, нормально?
Это тривиальный случай,
здесь не надо ничего использовать.
Давайте стал бы считать,
что Д больше либо равно единице.
Вот теперь я хочу действительно
воспользоваться этой темникой.
Прежде всего мне нужно выбрать
орграф зависимости.
Вот я беру этот самый
орграф зависимости.
Вот этот самый.
Какой еще раз?
Я протыкаю направленными стрелками
те не более чем Д события,
от которых данная АИТ зависит.
Вы согласны, что получается
орграф зависимости?
Просто по условию теорем АИТ
не зависит от всех остальных.
Если мы Д-стрелками проткнули
те Д события, которые могут повредить,
то от всех остальных оно не зависит.
Все прекрасно, это орграф зависимости.
У него степень каждой вершины
исходящей не больше чем Д, правильно?
Сходящая степень каждой вершины
не больше чем Д.
Взяли такой орграф,
выбрал чисел к ним от этих.
Я их выберу, естественно,
все одинаковыми,
симметричные же случаи.
Я возьму Х1, Хn, все равными 1,
потереть на D плюс 1.
Видите, тут важно было
ограничиться от D равного нулю,
потому что при D равному нулю
получилось по единице,
которую мы не хотим в этом месте получать.
Не имеем права.
Ну, хорошо.
Тогда, смотрите,
нам, зная, что вероятность
не больше чем D,
и что выполнено вот это неравенство,
нужно проверить вот эту систему неравенства.
Так, вы согласны?
Если мы хотим воспользоваться этой теоремой,
нам надо удостовериться,
что вероятность Аитова вот так оценивается.
Нужно проверить, нужно,
давайте я напишу.
Нужно проверить,
что для любого D
вероятность Аитова
не превосходит 1,
потереть на D плюс 1,
произведение величин 1,
минус 1, на D плюс 1.
Ну, я здесь многоточие напишу,
вот по этим самым товарищам.
Так, сейчас я постараюсь не торопиться.
Сколько здесь сомножить?
Не больше D.
Не больше D, правильно?
Ну, из Аитова выходит не больше, чем D стрелок.
Это точно здесь написано,
Аитово-Аитово-Аитово.
Не больше, чем D раз.
То есть тут вот не больше, чем D сомножить.
Каждая из которых меньше единицы.
Вы согласитесь,
что вам достаточно доказать стрелку снизу вверх,
для того, чтобы убедиться в этом,
достаточно доказать,
что P не превосходит 1,
потереть на D плюс 1,
1, минус 1, на D плюс 1,
этой степени.
Потому что здесь мы заменили то,
что реально было,
на меньшее число.
Мы возвели в большую,
вообще говоря, степень,
чем здесь сомножить не было.
То есть мы вот эту штуку заменили
меньшим числом.
Ну, меньше не правильно.
А слева мы, наоборот,
заменили большим числом.
Потому что у нас вероятность Аитова
не превосходит P.
Ну, то есть этого достаточно.
Если большее число не превосходит
меньшего числа,
ну, значит,
меньшее число не превосходит большим.
Понятно, да, что я сказал?
Смешно звучит, но понятно.
Вот.
Слушайте,
какая-то же пределья дает
1 поделить на E.
Причем, как оно соотносится
с 1 поделить на E?
Оно больше, опять, чем 1 поделить на E.
Всегда.
Поэтому можно еще раз
нарисовать вот такую стрелку
и написать вот так.
1 на D плюс 1,
не 1 поделить на E.
А это просто тоже самое,
что E на P
на D плюс 1
меньшее правное 1,
которое у нас тут есть.
Я прояснил,
откуда E взялась?
В принципе, конечно, я мог
здесь писать не так, наверное,
а мог записать вот так.
Ну, как-то это слишком громоздко.
Ешки вполне хватает.
А вот заменить E на какую-либо константу
отлично от E.
В общем случае нельзя.
То есть можно подобрать примеры
таких событий, когда вот прямо
сколь угодно близко к E
я, конечно, вам рассказывать не буду.
Слушайте, ну я, конечно,
нехороший человек.
Я закончил, что ли,
на 10 минут раньше.
Но начинать доказывать эту тярему
это издевательство,
потому что за 10 минут я точно
ее не докажу, это полчаса минимум.
А вы все забудете через неделю.
Ну что делать?
Не рассчитал время.
Все, давайте.
Аплодисменты.

Так, добрый день, ну давайте продолжим нашу деятельность. Я обещал немножко рассказать
про то, как связан вапник черванинкес и задачи, так сказать, теории вероятности или статистики.
Вы знаете, конечно, что такое закон больших чисел. А усиленный закон больших чисел?
Ну, в анализе данных это важно, конечно, и то, что я сейчас буду рассказывать, это тоже важно.
Ну, я так буду просто обзор небольшой делать. Но, тем не менее, прямое следствие конечности
размерности вапника черванинкеса, как и в прошлый раз для задачи по треугольнике, так и в этот раз
вот для задачи статистики, собственно, вапника черванинкеса с этой задачи начинали. Ну, давайте
я напомню, наверное, что это такое. Вот есть закон больших чисел. Давайте, наверное, в какой-нибудь
самой простой формулировке, ну, условно говоря, для схемы испытаний Бернуль, то есть не вообще
для произвольной последовательности каких-то случайных величин, которые, может быть, Бог знает какой,
там совершенно разнообразный, а для случайных величин, которые являются индикаторами некоторых
событий. Ну, то есть, как это должно выглядеть? У нас есть какие-то события А1, АН и так до
бесконечности, которые, ну, как сказать, отвечают успеху в очередном испытании Бернуль. Мы бросаем
монетку, если она выпала решкой кверху, говорим, что успех, но это и будет событие А1. То есть,
если случилось событие А1, то говорим, что успех, если не случилось, то говорим, что неудача. Чтобы
это была такая настоящая схема испытаний Бернуль, что нужно сказать про эти события? Конечно,
они должны быть независимыми в совокупности. Ну, то есть, живут они, по-видимому, на каком-то
бесконечном вероятностном пространстве, потому что иначе вы, конечно, не сможете организовать
бесконечную последовательность событий независимых в совокупности. Но это вас тоже учили в курсе теории
вероятностей, я это напоминать не буду. Учили же. Бесконечную последовательность случайных величин
независимых можно задать только на бесконечном пространстве. Я думаю, что учили. Ну вот,
независимые в совокупности события. Тогда, если обозначить... Да, давайте еще считать,
что для любого И вероятность Аитова, конечно, равняется некоторому П. То есть, иначе это не
будет схема Бернуль. Надо, чтобы вероятности все были одинаковые. Равно вероятные события и
эти события независимых в совокупности. Тогда закон больших чисел, это что такое? Мы просто берем
индикаторы этих событий, первых N штук, вкладываем их, делим на N и говорим, что куда это стремится,
и как это стремится, как и куда это стремится. В обычном законе больших чисел, как и куда это
стремится. Ну, к мат ожиданию, которое, конечно, равно вероятности Аитова, то есть П. Сама сумма
стремится к N, П, но не стремится, а эквивалент на N, П. Когда мы делим на N, да, получается в
пределе П. Так, ну и как устроена эта сходимость? Какого вида сходимости тут присутствует? Ну вот,
слушайте, закон больших чисел, от усиленного закона больших чисел, отличается тем, что вид сходимости
здесь в одном случае по вероятности, это просто закон больших чисел, ну или по мере, как еще говорят,
по вероятности. А усиленный закон больших чисел, это когда вот здесь вот вероятность заменяется на
почти наверно. Это У, З, Б, Ч. И вам обязательно должны в основном курсе теории вероятности
доказать и закон больших чисел в куче разных форм, и усиленный закон больших чисел, как минимум,
в двух формах. Но для схемы испытаний Бернули он, конечно, верен, в какой форме его не доказан,
но это факт. Так, слушайте, вы помните, в чем отличие сходимости почти наверно от сходимости по
вероятности? И помните, что сходимость по вероятности слабее, строго слабее, чем сходимость почти
наверно. Потому что из почти наверно и по вероятности следует, но есть пример с бегунком с таким,
который показывает, что по мере может сходиться к нулю, например, а почти наверно вообще не сходится
просто вообще ни в одной точке не сходится. Не то, что нет почти наверной исходимости, а нет исходимости ни в одной точке.
То есть почти наверная гораздо сильнее, поэтому соответствующие законы больших чисел называются усиленными.
И именно они нужны на самом деле в анализе данных. Вот уж так сложилось.
Например, если вы что-то хотите приблизить с помощью какого-нибудь метода типа Монта-Карла,
то, конечно, без усиленного закона больших чисел вам не обойтись.
Но здесь он работает.
Так вот, каким вопросом задавались товарищи Вапник и Черваненкес,
если так совсем поверхностно это сформулировать в твоем 1970 году?
Они задавались вопросом, насколько далеко можно это дело обобщить?
В каком смысле обобщить?
Ну, смотрите, статистики у вас еще не было, конечно.
Это я знаю. Вы не думайте, что я не знаю, что у вас статистики не было.
Но в статистике есть значимое усиление вот этого усиленного закона больших чисел, дальнейшее,
которое на самом деле и позволяет очень многие факты из статистики доказывать.
Вот.
Значит, называется оно теорема Гливенко-Кантелли.
Вам ее обязательно докажут в курсе статистики, какой бы из наших курсов статистики вы не слушали.
То есть, неважно вы на потоке условном общем или на потоке, как он там называется.
ДС-поток — это как бы ДС-поток.
Ну да, да, ДС-поток, да, да, да.
Все равно Гливенко-Кантелли — это такая основа основ.
Как бы это нам попроще сформулировать?
Ну, давайте я как бы попроще сформулировать.
Ну, наверное, можно вот так сказать.
Сейчас я соображу как лучше.
Ну, давайте я сначала не формулировку, а все-таки немножко основу статистики какую-то скажу.
Значит, у нас есть выборка.
Ну, чтобы было понятно, о чем вообще идет речь, давайте я немножко расскажу про статистику.
Есть выборка. Это числа, которые мы просто пронаблюдали в каком-то эксперименте.
Ну, какой-то набор вещественных чисел.
Не знаю, следили за температурой воздуха каждый день в 12 часов.
И вот в первый день мы увидели температуру минус 1, во второй там плюс 2 и так далее.
Ну, получилось какая-то последовательность чисел.
А мы считаем, что за этими числами стоят какие-то случайные величины.
И эти числа служат просто их реализации.
То есть у Господа Бога есть такая монетка, которую он подбрасывает, засовывает куда-то в какую-то случайную величину.
Батс получается температура сегодня, потом температура завтра.
И так в течение месяца.
Про эти случайные величины обычно для простоты предполагают, что они не обязательно, это не вся статистика такая,
это ее классическая основа независимая и одинаково распределены.
Но только распределение мы этого не знаем.
И вот это основная проблема статистики.
Как оценить распределение вот этих случайных величин, зная только элементы выборки, которые мы пронаблюдали.
Ну, как-то вот попробовать, апроксимировать, оценить максимально правдоподобным образом параметры этого распределения
или просто все это распределение в целом.
Вот мы не знаем.
Ну, есть какая-то функция.
f, xi, it от x.
Это функция распределения каждой из этих случайных величин.
Они одинаково распределены, значит, функция одна и та же для всех.
Можно f, xi, 1 написать.
Но мы ее не знаем, мы хотим ее как-то апроксимировать.
Но такой самый тупой способ апроксимировать функцию оказывается при этом гениальным и хорошим.
И вот об этом теорема Гливенко-Кантельли сейчас я про это скажу.
Значит, как ведет тебя обычная функция распределения?
Ну, грубо говоря, как-то так. Правда же?
Это вот f, xi, it от x.
Конечно, там где-то могут быть ступенечки, какие-то более хитрые, изломы.
Но она монотонно не возрастает на всем вот этом промежутке.
От нуля до единицы.
Непрерывно справа.
Ну, здесь я нарисовал просто непрерывно.
Ну, давайте попробуем ее апроксимировать просто вот такими ступенечками.
Ну, какими ступенечками?
Возьмем элементы выборки и просто вот такую функцию рассмотрим.
fn, давайте с крышкой ее обычно обозначают, от x1, xn и от x.
То есть мы ее по выборке построим, а x это будет обычный вещественный аргумент, такой же, как у теоретической функции распределения.
Что определяется, повторяю, это просто ступенчатая функция.
Значит, это будет 1n, сумма по i от единицы до n.
Индикаторов сравнивайте с тем, что я написал на левой части доски.
Индикаторов того, что x, it не превосходит x.
Вот такая вот простая функция.
Понятно ли, что это за ступенчатая функция, что она действительно выглядит так, как на картинке?
Ну, то есть тут есть какие-то специфические события аиты, которые между собой, очевидно, независимы.
Вот события аиты, вот это вот. Это события аиты.
Сравнивайте с тем, что написано на левой части доски.
Они, очевидно, взаимно независимы, эти события.
Потому что случайная величина x, it независима по условию.
Понимаете за мыслью?
Вот эти события независимы по i.
Потому что x, it независима и случайная величина, значит, эти события тоже независимы.
Ну, такая вот функция.
Теорема Гливенко-Кантелли говорит следующее.
Давайте так, подождите, теорема Гливенко-Кантелли.
ЗБЧ, стало быть, что говорит, или усиленный закон больших чисел,
он говорит, что это сходится хоть по вероятности, хоть почти на верное,
потому что и усиленный закон больших чисел тоже выполняется.
К чему это сходится? Одна n-ная сумма индикатора.
К вероятности того, что x, it не превосходит x.
К вероятности того, нужно писать, x, it не превосходит x,
потому что x, it – это конкретная чиселка, а x, it – это та случайная величина,
которая ее породила.
Я точно понятно выражаюсь? Не очень быстро. Нормально.
Но это что? f x, it от x.
Но что здесь утверждается? Здесь утверждается, что если число x вещественное вы зафиксировали,
тогда вот эта опроксимирующая ступенчатая функция, взятая в этой точке,
ростом объема выборки входит как раз к той функции, которую мы ищем,
но опять же, взятой в этой точке.
То есть, к каждой конкретной точке есть сходимость числовая.
Понятно говорю?
А вот теорема Гливенко-Кантелли говорит большее.
Она говорит следующее, что вероятность,
которой supremum по всем x на вещественной прямой
модуля разности fn с крышкой от x1 к xn x – f x1 от x, не важно, они все одинаковые,
стремится к нулю, fn стремящимся к бесконечности, равна единице.
То есть, это не просто усиленный закон больших чисел,
а это равномерная сходимость в усиленном законе больших чисел.
Это именно усиленный закон, потому что сходимость почти наверная,
вероятность, где сходимость есть, равна единице.
Но мы не просто берем для конкретной точки x модуль вот этой разности,
как в усиленном законе больших чисел,
а мы берем supremum этого модуля по всем x,
то есть как бы всю трубку между реальной функцией и той, которая ее приближает.
Вот берем всю эту бесконечную трубку,
на ней смотрим supremum разности,
и даже этот supremum стремится к нулю почти наверно,
а не просто модуль разности.
Понятно, да?
Это не сложная на самом деле теория, мы вам ее обязательно докажем.
Но Вапника и Черванинки задались вопросом,
а можно ли это еще как-то дальше обобщить?
Ну, давайте это, наверное, переформулируем на языке вот этих индикаторов еще разок.
Значит, у нас получается вот так.
У нас есть последовательности a1х, a2x и так далее, anx и так далее.
В которых a, it, x – это есть событие, состоящее в том, что xi, it не превосходит x.
При этом it у нас пробегает по счетному множеству чисел от единицы до бесконечности,
а x – это любое вещественное число.
То есть как бы есть целый континуум последовательностей.
Для каждого x своя бесконечная последовательность.
При этом внутри каждой последовательности выполняется все то же самое, что написано слева.
То есть для любого фиксированного x, a, it и x, взаимно независимое.
И вероятность a, it и x равняется просто вот этому числу.
Давайте его обозначим px.
Это не p в степени x, а p с индексом x.
Ну, с верхним.
Сейчас пока понятно, что происходит?
Ну да, видите, я же написал, для любого x, a, it и x взаимно независимые, не наоборот.
То есть если я фиксирую x, то внутри такой последовательности они взаимно независимы.
Здесь как раз так и сказано.
Они там взаимно независимы, и для этого фиксированного конкретного x вероятность каждого из них,
Ну тут, наверное, надо написать вот так.
Тут не влезает.
Для любого i вероятность a, it и x равняется одному и тому же px.
Но от x, конечно, эта вероятность зависит.
Вот она.
Такая.
Замечательная.
Каким вопросом задались еще раз товарищи Вапник и Черваненко?
А что если у нас не вот так вот порожденные события, а как-нибудь там, бог знает как.
Просто есть огромное множество каких-то последовательностей событий в общей случае.
У нас есть, опять же, последовательность a, it и x,
где i меняется от единицы до бесконечности, а x принадлежит какому-нибудь множеству.
x красивое, не обязательно вещественное и прямое, там бог знает чему.
И опять известно, что для любого конкретного x, a, it и x взаимно независимы, все то же самое.
Просто вот этот x большой может быть очень сложный.
Он может там, я не знаю, Rn заполнять или вообще какое-нибудь пространство функций, бог знает что.
Вот эти ax взаимно независимы и для любого i опять же все то же самое.
Вероятность a, it и x равняется px.
Каким индекс?
px.
Вот при каких условиях, вопрос Вапника и Черваненко,
при каких условиях вероятность того, что supremum по x маленькое уже просто из этого x большое,
как модуля разности i, a, 1, x+, и т.д.
плюс i, a, n, t, x, поделить на n,
минус px.
Травнивайте с теоремы Гливенко-Контельни, но там буквально то же самое написано.
Но только в конкретном случае.
Как тремиться к нулю? Вероятностью единицы.
То есть какие условия гарантируют наличие равномерной исходимости в усиленном законе больших чисел?
На классе последовательств.
Я понятно спробовал вопрос?
Ну вот, собственно, оказалось, что если вы возьмете вероятностное пространство
и в нем рассмотрите все возможные вот такие вот последовательности,
ну события там живут, вот ваша омега, в котором живут все эти события.
И вот есть такое r, которое состоит из а и тх,
по i от единицы до бесконечности, и по x из x красиво.
Если вы рассмотрите такое ранжированное пространство,
то вопрос о том, конечная или бесконечная его размерность вапника Червоненкеса,
равносилен положительному ответу на вот этот вопрос.
Если размерность вапника Червоненкеса конечна,
то усиленный закон больших чисел работает в равномерной форме.
Если бесконечно, то не работает.
Ну это доказывается там примерно так же, как мы доказывали теорему про треугольники и более общий случай.
Но там чуть по-другому надо рассуждать.
Я, конечно, этого делать не буду, никаким упражнениям я это не считаю,
это довольно сложно, но это такой вот очень мощный факт,
который позволяет в том числе делать некоторые оценки,
связанные с качеством машинного обучения, со скоростью.
Есть в этом некоторая проблема, когда товарищи стояли у основ очень многих методов машинного обучения,
вапника Червоненкеса очень много усилий в это вложили,
и действительно цитируются по всему миру лучшими специалистами в этой области.
Они получили эту оценку, и она дала теоретический результат, который позволил обосновать кучу методов.
Но когда мы смотрим на какие-то конкретные реализации, то мы понимаем, что эти оценки очень завышены, это правда.
И куча народу после вапника Червоненкеса, например даже наш Воронцов,
знаете Константин Вячеславович, нет, не слышали?
Такой очень известный лектор по машинному обучению, мне казалось, один из самых популярных в нашей стране.
Ну и в ШАДе он преподает, и здесь на Фистехе тоже, сам выпускник Фистеха.
Не слышали, не знаю, почему не слышали, но в общем один из самых таких ярких лекторов по машинному обучению,
он как ученый занимался именно уточнением оценок вапника Червоненкеса.
Ну не только он там много людей этим занимается,
но действительно с практической точки зрения это все-таки только затравка такая,
с практической точки зрения там дальше надо возиться.
Но тем не менее вот с этого все стартовало, когда в 70-м году это было доказано, это был очень значимый шаг.
Ну ладно, наверное на этом я завершу эту тематику, все, в общем, системы представителей покрытия,
это все я закончу на этом, и наверное перейду сейчас к еще одной теме,
чего у меня по времени-то, а я еще только полчаса рассуждаю, и что-то как я, какой я шустрый.
Вот, одна тема.
В прошлом году я в этом месте рассказывал немножко такой переход, конечно,
наверное мне его надо перенести в какую-то другую часть курса.
Тема-то красивая, но она просто немножко, наверное, хотя может и нормально,
что она после Вапника-Червоненкеса.
Помните когда-то на первом курсе все начиналось с вопроса о том,
что будет если у нас 15 пятиэлементных подмножис, 30-элементного множества,
нет, уже не помните, ну значит у вас здоровый мозг,
потому что он отбрасывает то, что вам хотелось забыть, а я сейчас напомню.
На самом деле, ну просто в тот момент вы могли не осознать, насколько это полезная штука,
но я этим демонстрировал силу принципа Дерехли.
Это прям первая лекция была, и я считаю, что тогда это прям очень важно.
Если бы я мог рассказывать прямо тогда то, что я могу рассказывать теперь,
я бы прямо тогда рассказал, но потому что это действительно такая вещь,
тоже связанная с вероятностными методами, и так просто на первом курсе она не получается.
Ну давайте я напомню ту задачу, там было две темы про это, я обе темы сейчас напомню, обе разовью.
И гиперграф мы с вами знаем, что такое, сейчас конечно легче это все делать,
но давайте я напомню ту задачу, которую вы как выяснилось забыли.
Значит задача такая, есть множество, состоящее из 30 элементов,
30 чисел от 1 до 30, в нем есть подмножество m1 и так далее m15,
такие, что мощность каждого из них равняется 5,
и утверждение состояло в том, что как бы это ни было расположено,
как бы ни было устроено этот 5 однородный гиперграф на 30 вершинах,
его хроматическое число не больше двойки.
Ну то есть можно так покрасить в два цвета числа от 1 до 30, чтобы каждая мытая была не одноцветна.
Не одноцветным это значит, что содержало элементы хотя бы двух различных цветов,
но если цветов всего два, значит вот тех самых двух, которые красили.
Что, не помните такое?
Помнили, да, что была такая задачка?
Я говорил, что это как бы иллюстрация на принцип Дерехле, но скорее это все-таки иллюстрация на вероятностный метод.
То есть мы чего делаем? Мы каждый из этих чисел с вероятностью 1 вторая красим в красный цвет,
с вероятностью 1 вторая в синий, то есть каждое число вот отсюда присваиваемому цвет К,
с вероятностью 1 вторая цвет С, с вероятностью 1 вторая.
У нас получается 15 событий, события, в которых каждая аитая состоит в том, что митая одноцветна.
Это бредные события, как обычно, вероятность которых хочется минимизировать.
Нам-то хочется, чтобы все миты были не одноцветны.
Вероятность аитого, это понятное дело, сколько?
Скажите мне, пожалуйста, присутствующие здесь слушатели,
с какой вероятностью данное конкретное множество из пяти элементов при такой случайной бирнуликской покраске будет целиком одного цвета?
2 в минус четвертый, правильно.
Ну, я напишу 1 шестнадцатая, хотя 2 в минус четвертая это системнее, если бы там n было, 2 в минус n красиво выглядит.
Я напишу 1 шестнадцатая, да.
Соответственно, вероятность объединения аитах по i от единицы до 15 меньше или равна 15 шестнадцатых, то есть меньше единицы,
ну и значит с положительной вероятностью ни одной из этих событий не выполнена,
а, стало быть, существует раскраска, в которой все множества не одноцветны.
Я не очень быстро это рассказал, вроде эта идеология уже должна, вот уж она должна сидеть просто в мозгу постоянно.
Мы с таким сталкивались. Вы даже можете мне сказать, но у нас же есть локальная лемма ловоса.
Ну, проблема в том, что тут очень много зависимости априорий, непонятно, что с этим делать.
Ну и даже когда мы иллюстрировали локальную лему ловоса, мы вспоминали эту задачу и говорили, что при этом подходе так ее примить-то не получится,
а вот если как-то ограничить степень вершины гиперграфа, тогда получится.
Помните, я такой пример приводил? Но кто внимательно слушал, тот помнит.
Это было прямо в этом семестре на локальной лемме ловоса. Похожую задачу я рассматривал.
Вот. Ну вот я сейчас ее еще глубже хочу изучить вместе с вами, потому что она таким рефреном появляется у нас на протяжении всех этих двух лет.
Я вам напомню еще одну историю про эту задачу. Может вы вспомните?
Значит, смотрите, на первой лекции ОКТЧ она была вот именно в том виде, как сейчас написано.
Там я, правда, объяснял ее не так. Я говорил, давайте просто рассмотрим множество тех раскрасок. Ну какая разница?
А сейчас я могу оперировать вероятностным языком.
Ну хорошо. Потом она действительно возникала в контексте локальной леммы ловоса, когда я рассматривал тоже однородные гиперграфы с ограниченной степенью вершины.
Это второй случай, когда возникала эта задача. Но был еще третий случай, когда эта задача возникала.
Может кто-нибудь вспомнит?
Матрица Адамара, гениально!
Которая у нас была на ОКТЧ.
На ОКТЧ, да. У вас это уже было на ОКТЧ, конечно.
Да, там была задача про дискрепанс, про уклонение.
Ну давайте я сформулирую сначала вот эту задачу в общем виде, а потом в общем виде напомню, что такое дискрепанс.
Значит, в общем виде задача, с которой мы столкнулись, понятное дело, как звучит.
Давайте рассмотрим произвольный эноднородный гиперграф.
Так, из всякой очередной мелок. Ладно, я все-таки не буду больше экономить. Не возьму большой. Тут есть.
Это я просто пытался сэкономить для института мел.
Так, рассмотрим произвольный эноднородный гиперграф.
Давайте назовем его хроматическим числом. Понятно, что? Я, наверное, уже это говорил, да?
Я давал это определение?
Мне кажется, что давал. Давайте я еще раз повторю, потому что люди забыли.
Хроматическое число гиперграфа, ну это как обычно минимальное число цветов, в которые красятся его вершины,
все то же самое, как у графа. Но с условием, что в каждом ребре есть вершины хотя бы двух различных цветов.
То есть даже если вы краете в десять цветов, не нужно, чтобы в каждом ребре присутствовали все десять цветов.
Нужно, чтобы каждое ребро было просто не одноцветным.
Я не знаю, нужно что-то писать на доске?
Минимальное число цветов, которые можно так покрасить вершины, чтобы каждое ребро получилось не одноцветным.
Вот рассмотрим произвольный эноднородный гиперграф H, ну там с каким-то множеством вершин,
с каким-то множеством вершин, с каким-то множеством вершин.
Вот рассмотрим произвольный эноднородный гиперграф H, там с каким-то множеством вершин,
с каким-то множеством ребер. Такой, что его хроматическое число равняется двойке.
Он называется такой двудольный эноднородный гиперграф.
Прям в классическом смысле слова, двудольный граф, граф, у которого хроматическое число два.
два. Ну и здесь тоже двудольный гиперграф, у него хроматическое число два. Всего вершины можно
покрасить таким образом в два цвета, чтобы каждое ребро было не одноцветно. Нас интересует вопрос
от искания величины наименьшего количества ребер в таком гиперграфе. Наименьшего количества
ребер в таком гиперграфе. Нет, сейчас, глупость сказал. Мы говорим, рассмотрим произволенный
однородный гиперграф с хроматическим числом два. Наоборот тогда нас интересует, видимо,
максимальное количество ребер в этом графе. Насколько большим оно может быть. Ну давайте
я по-другому все-таки скажу, просто напишу формальное определение и все станет совершенно
понятно. Я только запутаю слушателей. Давайте m от n. Ведем такое обозначение. Это будет
все-таки минимум. Треди всех таких k, то существует n однородный гиперграф
к ребрами и х больше двойки. Наоборот, больше. Ну то есть найти максимальное количество ребер в
гиперграфе, который красятся в два цвета, это с точностью до разницы в единичку, найти минимальное
k, при котором существует однородный гиперграф с ребрами и не красящийся в два цвета.
Так, я уверен, что не все понимают определение, потому что много кванторов. Это надо осознать. Вот,
смотрите, здесь есть конкретное утверждение. Любой 5 однородный гиперграф с 15 ребрами можно
покрасить в два цвета. Правда, это утверждение на 30 вершинах, но, товарищи, согласитесь,
что никакой же разницы нет. Было бы 50 вершин, что-то бы в том рассуждении, которое я стер,
изменилось, да ничего бы не изменилось. Размеры ребер те же самые, 5. Поэтому вероятность как была,
одна шестнадцатая, так и останется. Ребер самих 15 штук, 15-16 меньше единицы. Все хорошо.
То есть, 30 здесь никакой роли не играет. Ну, короче, что мы можем сказать про m от 5,
исходя вот из этого утверждения? Полуторгодичной давности. Не больше? Больше 15, правильно? Больше 15.
Любой 5 однородный гиперграф с 15 и менее ребрами красится в два цвета. А чтобы существовал
5 однородный гиперграф с к ребрами, который не красится в два цвета,
стало быть, требуется больше, чем 15 ребер. Нормально сейчас объяснил? Сейчас понятно,
что такое m от n. Так, ну, наверное, теорема 1, это обобщение того, что мы вот здесь написали,
можно написать вот так. Больше, чем 2 в степени n-1. Так, плюс один, что ли? Или минус один? Это как-то
вот так. 2 в степени n-1-1. Так, дорогие товарищи, понятно, что для произвольного
однородного гиперграфа все то же самое рассуждение проходит. Там просто вероятность события будет
2 в степени 1-n. И поэтому надо умножать на 2 в степени n-1, чтобы получить меньше единицы.
Ну, на что-то меньшее, чем 2 в степени n-1. Вот хотя бы так. Ну, или что то же самое. Мне
приятнее писать вот так. m от n больше либо равняется 2 в n-1. Ну, это то же самое. Так,
давайте устраивать перерыв, наверное, а потом… Да, для малых n это очень хорошо, действительно,
прокомментировать, что будет для малых n с этой величиной. С табличкой все плохо, потому что
неизвестно почти ничего. Но для каких-то малых n это полезно посмотреть. Давайте, действительно,
посмотрим. Сейчас я все сотру здесь. Так. Ну, смотрите, m от 2 – это, конечно, первое,
потому что если n равно единице, то непонятно, как красить два цвета, чтобы ребро получилось
неодноцветным. Если n равно единице, то в каждом ребре одна вершина, но это бессмыслится какая-то.
Вот. m от 2 – это когда у нас не гиперграф, а обычный граф. В каждом ребре две вершины. Ну,
и чему равно m от 2 – это очень просто. Минимальное число ребер в графе,
которые не красятся в два цвета. Ребер два, красят же всегда. Три, конечно, треугольник,
просто возьмите. И это тот самый пример графа с тремя ребрами, который не красятся в два цвета,
хотя бывают, конечно, графы с тремя ребрами, которые и красятся. Но здесь квандросуществование
стоит, поэтому m от 2 равняется 3. Уже m от 3 – это нетривиальная задача. Ну, равняется это 7.
Можете попробовать самостоятельно. Я не буду сейчас на это время тратить.
Но, в общем, это уже не очевидно. m от 4 – боюсь соврать. Посчитали совсем недавно, буквально лет
пять, может быть, назад с участием компьютера. По-моему, 2 от 1 получилось, но я напишу под
вопросом, потому что могу ошибаться. Вот. Ну, вообще видно, что растет, наверное, все-таки
экспоненциально. Ну, так понятно, m от n больше, чем 2 в степени на минус 1, растет экспоненциально.
Но как в точности – это уже другой вопрос. m от 5 неизвестно. То есть вот мы с вами знаем,
что m от 5 больше либо равняется 16, но можно на это еще посмотреть чуть подробнее потом, попозже.
Хорошо. Ну, давайте зададимся вопросом, чему его точно не превосходит m от n. Пусть это будет
теорема 2, которая простая совсем. Его m от n не превосходит. Есть очень простой пример n однородного
гиперграфа, который нельзя покрасить в два цвета, так чтобы каждый ревро был не одноцветным.
Это прямое обобщение треугольника? Нет, полный, да, полный однородный гиперграф на
скольких-то вершинах. Конечно, да, треугольник – это полный граф на трех вершинах тривиальным
образом. Но я, в общем, утверждаю, что это не больше, чем c из 2n-1 по n, потому что можно взять вот
столько вершин и рассмотреть полный однородный гиперграф. Если вы 2n-1 вершину покрасите в
два цвета, то какие-то n вершин будут обязательно покрашены в один цвет. Согласны? Если вы 2n-1 вершины
тоже принадлежит директе, покрасили в два цвета, то какие-то n из них покрашены в один цвет. А у нас
все n-элементные подмножства служат ребрами. Значит, невозможно так покрасить в два цвета,
чтобы каждое ребро было не одноцветным. Какое это выбьется? Вот пример с 2n-1 вершинами,
с таким количеством ребр. Ну как мы все с вами хорошо знаем, это 2 в степени 2n-1 поделить на
корень из pn. Это же прямо про числа Рамсея, можно сказать, оценка. Мы сто раз это писали,
поэтому это можно уже наизусть запоминать. Ну в общем, грубо говоря, это 4 в н степени,
а тут 2 в н степени. Хреново как-то. Ну давайте теорема 3. Против скрипанс я тогда в другой раз
напомню. Сейчас уже так хорошо как-то пошло вот в эту стезю про уклонение напомню в следующий
раз. Не успи. Значит, можно доказать вот такую штуку. m от n не превосходит 1 плюс о малой от единицы
елогарифом двойки на 4, n квадрат на 2 в степени. То есть на самом деле m от n зажато довольно тесно,
а симпатически, в отличие от тех же чисел Рамсея, здесь видите нижняя оценка 2 в н минус 1,
ну то есть грубо говоря 2 в н. А верхняя оценка с точностью до константы это n квадрат на 2 в н.
Зазор неэкспоненциальный. Но, конечно, здесь это явный пример, а это мы сейчас будем доказывать
тоже вероятностным методом. Но что значит доказать верхнюю оценку вероятностным методом? Это значит
доказать, что существует гиперграф. Ну значит, гиперграф надо брать случайно. Ну сейчас сделаем.
Давайте будем считать для простоты, что n четно. Для нечетного там все так же делается,
не хочу морочить голову, пусть n будет четным. Просто упрощаю немножко рассуждение, чтобы лишнего
не писать. Возьмем v равное n квадрат пополам. Ну, как до этого додумались, это уже вопрос другой.
Ну так вот, додумались. Я вам напишу формулировку в духе числа Рамсея, такую компьютерную,
помните? Как перебором там что-то найти. Ну вот люди так и догадались, что это оптимальная ситуация.
Возьмем v равное n квадрат пополам и в качестве множества вершин возьмем числа от единицы до v.
Наша задача построить гиперграф, у которого вот столько ребер и который в два цвета не красится.
Правильно? Такой же нам нужен гиперграф. Множество вершин мы вольны выбирать как угодно,
вот выбрали таким загадочным способом. Я утверждаю, что оптимально в качестве количества вершин взять
почему-то n квадрат пополам. Но n квадрат пополам делится, поскольку мы предположили, что n четно.
А ребра на этом множестве вершин уже будем выбирать случайно. Давайте обозначим просто
буквой m количество ребер. Потом наша цель доказать, что m вот такое можно взять.
m количество ребер. Цель доказать, что можно взять m асимпатически равным елогарифом
двойки на 4n квадрат на 2v. Я думаю, что мы даже с подвигнем сейчас это сделать,
потому что это не очень сложно. Вы настолько уже должны были привыкнуть к этим асимпатикам,
что кажется это не очень сложно. Ну или как та лошадь цыганская, конечно, привыкли, но сдохли.
Будем надеяться на лучшее. Смотрите, давайте ребра выбирать взаимно-независимо. Выбираем
ребра взаимно-независимо, то есть с возвращением. Давайте так я скажу. Я, по-моему, уже это употреблял
термин, можно не комментировать снова. Выбираем ребра с возвращением, ну согласно классическому
определению вероятности. Ну то есть что это значит? Мы берем какое-то ребро, как его обозначить,
а1, например, и вероятность того, что мы выбираем именно это конкретное а1, это 1 поделить на c из
v по n. Нам же нужен n однородный гиперграф, просто возле этих v вершин выбираем произвольное n
случайным образом. Классическое определение вероятности. Потом запоминаем, какое выбрали,
возвращаем на место и выбираем еще одно, может быть снова его же. Как повезет. Ну процесс,
процесс прямо такой же, как в доказательстве теоремы в аптеке червонемца из прошлой лекции,
только там не ребра выбирали, а элементы выбирали. Берем ребро, возвращаем на место,
но выбираем еще одно случайное и так далее. Ам тоже выбираем с вероятностью 1 на c из v по n.
Ну то есть теоретически они могут все совпасть, но для нас это только хорошо,
потому что если у нас в гиперграфе будет мало ребер, нам естественно от этого тем лучше. Понятно
говорю? То есть ребра, которые не дай бог совпадут, мы просто отреждествуем. Если мы докажем,
что с вероятностью положительной вот такой вот случайный гиперграф не красятся в два цвета,
ну все, мы в героях. При вот таком выборе мы. Наша цель доказать, что с положительной вероятностью
вот эта штуковина после отреждествления совпадающих ребер не красятся в два цвета.
Никак. Ну давайте вот попробуем еще тут что-нибудь пописать. Нам нет мало места. Мало. Поехали стирать.
Так, а это тоже. Да, слушайте, пока я не продолжил, я вот здесь вот остановлюсь на секунду. Смотрите,
если вот в эту теорему 2 подставить вместо n пятерку, то получится t из 9 по 5, то есть 126,
это я помню. То есть для m от пятерки вот здесь мы получаем оценку не больше, чем 126. А как вам
зазорчик от 16 до 126? Из этой теоремы в том виде, как она сейчас сформулирована, непонятно,
что следует про m от 5, потому что что такое у малой от единицы, я же не конкретизировал. Какая тут
функция стоит, тремящееся к нулю. Тем не менее, сейчас вот мы по ходу доказательства вероятности
пооцениваем, и я переформулирую теорему 3 не вот в таком асимпатическом виде, а в компьютерном,
и будет вам задача найти для пяти самый лучший оценку. Ну просто вот чтобы осознать,
что происходит. Прямо на компьютере. Я не знаю, я просто никогда не читал. Мне,
например, интересно. Всё, возвращаюсь к доказательству. Так, давайте зафиксируем раскраску.
Множество 1, 2 и так далее. В маленькое, в два цвета. Она не случайная. Раскраска какая-то,
просто конкретная. Но вы можете, конечно, как принято у молодых людей, сказать рандомная,
но это не в том смысле, как мы понимаем, случайность. Случайный у нас, товарищи,
гиперграф. Вероятностное пространство состоит из гиперграфов. И мы сейчас просто временно
фиксируем какую-то раскраску, чтобы определить события, состоящие из гиперграфов. Какое-то
свойство гиперграфа. Ну давайте я эту раскраску как-то в буквы Х обозначу. На всякий случай
напомню вам, что всего этих раскрасок, конечно же, 2 в степени В. Но это очевидно,
сколько раскрасок в два цвета на множестве из В элементов. 2 в степени В. Вот какую-то одну из
этих 2 в степени В раскрасок мы сейчас фиксируем. Давайте попробуем найти вероятность того,
что А1, прямо А1, первое ребро одноцветно, именно одноцветно в раскраске Х. Ну что может,
здесь вы мне какой-нибудь мудрый вопрос зададите или нет. Вот дана конкретная раскраска, то есть
разбиение на две какие-то непересекающиеся части, может быть, разных мощностей вот этого множества.
Раскрасили в два цвета, скажем, первый элемент красный, все остальные синие. Там 10 элементов
красных каких-то, остальные синие. Вот она дана уже, эта раскраска. Теперь берем случайное подмножество
и хотим посчитать, что это подмножество одноцветно вот в этой конкретной раскраске.
Чего нам не хватает, чтобы эту вероятность посчитать? Совершенно верно! Надо знать,
сколько вершин, например, красного цвета. Одного достаточно, потому что второго В минус первого.
Ну, можно, конечно, для симметрии сказать, что пусть К вершин красного, С вершин синего,
но понятно, что К плюс С равняется В. Нормально? Буквы ничем не путаются. К, С,
вроде тут таких не было. Так, ну вот если нам все-таки дано, что в этой раскраске ХК красных вершин
и С синих, то как вероятность устроена? Так, ну в знаменателе очевидно С из В по Н, а в числителе
правильно С из К по Н плюс С из С по Н. С из С, понимаете, тут С и С в разных языках. Так,
ну, дорогие товарищи, у нас с вами уже было такое свойство в этом году, как выпуклость
биномиального коэффициента. Она упоминалась, когда мы доказывали оценки для двудольных чисел
рамсе. И она нам говорит, что вот это все не меньше, чем два С из В пополам по Н поделить на С из В по Н.
Ну это выпуклость. К плюс С равняется В, поэтому можем вот так написать. А это уже, слава богу,
не зависит ни от К, ни от С. Это общее для всех ХИ. Давайте временно вот эту штучку обозначим
буквой П маленькая, которую у нас вроде пока не было. Да, буквой П маленькая обозначим.
Прекрасно, тогда вероятность того, что А1 не одноцвет на ХИ, ну просто берем отрицание события,
не больше, чем один минус П. Согласны? Тогда вероятность того, что для любого И АИТ не одноцветна
вот в этой конкретной ХИ, как оценивается? Какой? Сколько ребер, правильно? А ребер М-штук,
вот здесь написано. Ребер М-штук. Значит, ввиду независимости их выбора взаимной,
получаем один минус П в МТ-степени. Ну тогда вероятность того, что существует раскраска такая,
что для любого И АИТ не одноцветна в этой раскраске, которая существует, не больше чего, товарищи,
два в степени В, вот здесь написано не в МТ, а в этой, два в степени В на один минус П в степени М.
Ну как всегда существует это объединение, перебор в случае. Так, ну и все. Значит, вероятность
того, что для любого ХИ найдется и такое, что АИТ одноцветна, снова переворачиваем, рассматриваем
отрицание, больше либо равна один минус два в степени В на один минус П в МТ-й. Ну а что такое
написано здесь в подкопках? Какова бы ни была раскраска, найдется одноцветное ребро. Вероятность
на множество гиперграфов берется, то есть здесь вот здесь в подкопках написано множество таких
гиперграфов, для которых любая раскраска находит хреновое ребро, натыкается на хреновое ребро.
Если вот здесь меньше единицы, то здесь больше нуля и мы победили. То есть компьютерная формулировка
такая. Теорема... Какая она у нас по счету? Три. Вот давайте, теорема три штрих. Компьютерный
вариант формулировки. Пусть для данного N, для данного N число В таково... Не понял?
Наверное числа М и В таковы. То что? Нет, мы тут его зафиксировали, потому что это я такой умный,
а я-то вам хочу оптимальную компьютерную формулировку написать. Не такую, из которой вот эта
асимптотика получается, а такую, которую надо применять к М от пяти, например. Там же нет никакой
асимптотики. Наверное, да, вы правы. Числа В и М таковы, числа В и М таковы, то существует... А что
существует? Что-что? Да-да-да, тут ничего не надо. Существует глупость. Скал таковы, что выполняется
неравенство до 2 в степени В на единица минус П в степени М меньше одного, где П вот это вот.
Всё. Тогда М от N меньше либо равняется... Вот. Вы берете N равное пяти, и ни в коем случае не берете
В равное именно там 25 вторых или целая часть от 25 вторых. А просто бежите в цикле по всем В,
там начиная от пяти, наверное, потому что до пяти вообще бессмысленно, но всё-таки на этих вершинах
должно умещаться хотя бы одно ребро. Вот. Бежите в цикле по В. Вы там убедитесь в том, что при
достаточно больших N, N квадрат пополам примерно и надо брать. Вы это просто увидите, если будете
считать на компьютере. Но так просто тупо запускаете цикл по В, и для каждого В ищите
самое маленькое М, при котором это выполняется. То есть внутренний цикл, он по М, и вы хотите
найти такое минимальное М, что при данном В выполнено вот это неравенство. И потом
ищете глобальный минимум по всем В. Ламэ. Так, я понятно сформулировал? Ну, это, знаете,
это алгоритм-то не огорвало каяло с аксены, правда? Что-то очень простое. Да-да-да, вот огорвал,
каял с аксены, это действительно торжество человеческой мысли. А тут-то чё? Тяп-ляп.
Ну, можете попробовать просто для МАТ-5 пооптимизировать, понять при каком В находится самое маленькое М. Какое это
будет самое маленькое М? Ну, я уверен, что оно будет меньше, чем 126. Но насколько меньше? Это
интересный вопрос. Я не знаю, я не считал никогда. Может кто-то и считал, но я не знаю даже, где это
посмотреть. В следующем году расскажу студентам, что вот мои студенты прошлого года, третьекурсники
нынешние, нашли это. Вот я вам теперь показываю. Потом они мне расскажут, скажут, что это ошибка была. Так,
ну ладно, в общем, вот такая формулировка вполне понятная, обозримая. Но наша задача теперь убедиться
в том за оставшееся время, что действительно, если взять В равное n квадрат пополам, как мы и сделали,
и М вот такого вот вида, с каким-то подходящему маленьким от единицы, то неравенство будет
выполнено. Это довольно такая все-таки муторная задача, потому что смотрите, какие тут цешечки
прекрасные. Но мы умеем считать такие цешечки. Можно я посчитаю честно? Ну что ж, позориться-то.
То есть я мог бы на этом остановиться в рамках доказательств и сказать, ну все, все получилось.
Но все-таки тут не так сложно. А у нас анализ с вами дискретный.
Вот тут есть это М.
Так, ой, c из n квадрат пополам по n. Ну давайте c из v по m. Это v на v минус 1.
Стандартная вещь на v минус n плюс 1 на n факториал. Стандартная в том смысле, что мы много раз такие
штуки писали. Они уже приестся даже должны. Это v в степени n поделить на n факториал, 1 минус 1 на v,
1 минус n минус 1 на v. Догоняем в экспоненту v в степени n, n факториал, e в степени,
и раскрываем по Тейлору. Уже делали такое много раз. Не хочу на это подробно заострять внимание.
1 на v квадрат минус 1 квадрат на v квадрат. Вот так. Ну узнаете, конечно. Да, логариф мы просто
раскрывали. Точно это было множество раз. Так, это равно v в степени n на n факториал,
e в степени минус n, n минус 1 на 2v. Просуммировала ритмическую прогрессию. Плюс о большое от n в
кубе на v в квадрате. Ну v тоже можно здесь выписать, равно n квадрат пополам. Здесь стоит пункция
квадратичная, а нет, вот здесь стоит пункция кубическая, а здесь четвертая степень. v в квадрате
это n в четвертый. Поэтому вот эта вся штука стремится к нулю. Мы можем писать это асимптатически
равно v в степени n поделить на n факториал, умножить на e в степени минус n квадрат на 2v
плюс n на 2v. Ну n на 2v снова, конечно, стремится к нулю. Асимптатически равно v в степени n на n
факториал, e в степени просто минус n квадрат на v. Ну и вот так примерно угадывается, конечно,
что v должно быть порядка n в квадрате. Значит, если вы берете v равное n квадрат пополам,
то здесь получается в итоге v в n на n факториал, e в минус 2. Видите, как легко? Ну легко же,
правда. Привычно, обычно, ничего тут такого страшного нет. Совершенно аналогично c из v пополам
по n. Оно же тоже тут присутствует в определении p маленького. Что, потерял где-то двоечку? А вот
весь 2v. Ой, да, это e в минус 1. Обычно, привычно наопечатался. Спасибо большое. Обсчитался.
Так, ну совершенно аналогично. Это асимптатически равно v пополам v на n факториал. Вот на e в какой
степени? Можете сообразить. Но на самом деле легко. Вот здесь просто вместо v будет v пополам. И вот
здесь будет e в минус 2. Вот здесь будет e в минус 2. Совершенно так же считается. Соответственно,
p, который у нас 2c из v пополам по n поделить на c из v по n, это будет что такое? Значит,
v в n сократится, n факториалы сократятся. Останется 1 поделить на 2 в n, но умножить на 2. То есть
это будет 2 в степени 1 минус n. Услеживаете? Не ошибся я нигде. Еще раз, c из v пополам по n. Тут
одна вторая в n, это 2 в минус n. Нет, e сейчас будет. Да, я не говорю, что это все. Еще надо будет
действительно e в минус 2 поделить на e в минус 1. И это будет e в минус 1. Правильно, еще e. Теперь
правильно все? Да, теперь получается e в минус 1, конечно. Теперь возвращаемся к 2 в степени v на 1
минус p в n-ты степени. Это, конечно, не больше, чем 2 в степени v на e в степени минус pm, но тоже
стандартное неравенство. Опять загоняем 1 минус p в показатель экспоненты, там будет логарифма от
1 минус p. Он не больше, чем минус p. Это много раз тоже использовали. Так, ну что, p у нас посчитано.
Сейчас все увидим. Будет катапсис. 2 в степени n квадрат пополам. Это я v переписал. Так, мы,
наверное, сведем все к показателю e. Значит, будет e в степени n квадрат пополам на логарифм
натуральной двойки. Это я 2 в степени v переписал. Видите, да? 2 в степени v это e в степени логарифм
двойки, умноженный на v, а v это n квадрат пополам. Теперь я из этого вычитаю. Где тут у меня p? 2 в
степени 1 минус n на e в минус 1, это минус p. И вот надо умножить на m. Ну что, по-моему,
сейчас все сократится, смотрите. e логарифм 2. Не видно? Ну давайте я вот так перепишу. 2 в степени
1 минус n на e в минус 1, на m это равно 1 плюс о малой от 1. Так, на 2 в степени 1 минус n на e в
минус 1, на e на логарифм 2, о боже мой, на 4 и на 2 в степени n. Хлёп! Хлёп! Хлёп! Хлёп!
Что у меня получилось? А, еще! То есть осталось только логарифм двойки пополам, а n квадрат я
потерял. Осталось логарифм двойки пополам и n квадрат, ну и тут тоже логарифм двойки пополам
и n квадрат. Но просто вот здесь за счет этого 1 плюс о малой от 1 мы можем вычесть так,
чтобы получилось минус что-то, стремящееся к бесконечности. Ну хотите, чтобы было понятнее,
здесь напишу 1 плюс епсилон, как всегда, но так всегда понятнее. Берем просто какое-то
епсилон, большая нуля и вот тут у нас просто n квадрат пополам логарифм двойки, а тут у нас
1 плюс епсилон n квадрат логарифм двойки пополам. Ну ясно, что эта разность это епсилон,
умноженная на n квадрат, умноженная на константу, причем со знаком минус. Ну все,
вероятность меньше единицы, она стремится к нулю даже, довольно порядочным свистом. Вероятность даже
стремится к нулю, ну конечно она меньше единицы, стало быть. Ну а поскольку и епсилон берется
любое, то его можно заменить на у малое от единицы. Стандартное тоже рассуждение, а симпатическое.
Если пугает у малое от единицы сразу, то напишите вместо него епсилон и скажите,
что для этого произвольного епсилона найдется свое n маленькое, начиная с которого это будет
меньше единицы. Ну значит, епсилон можно заменить на у малое от у. Все. Ну смотрите,
как оно прямо впридык-впридык подобное. Значит, последнее, что я скажу, вот эту оценку,
а симпатическую, которую мы сейчас доказали, но стер-то уже это утверждение, вот эту. Вот
оно здесь. Вот эту оценку никто в настоящее время не умеет улучшить. Все, что люди умеют улучшить,
это величину епсилон. Ну малое от единицы вот это. А вот эти константы, даже их не удается
улучшить никакими силами. Это прямо проблема. Верхняя оценка никому не дается. Такое удивительное
дело. Ну в следующий раз я еще немножко продолжу про нижние оценки и поговорю про дискрепанс,
который был в прошлом году и который там был с матрицами Адамара. Я напомню, что это такое,
причем там были матрицы Адамара, и докажу что-то комплементарное к этому. Ну а на сегодня все.

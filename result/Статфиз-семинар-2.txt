с кусочками понятнее да но а кто сказал вот кстати давайте попробуем вот а
встречается чаще на него 1 бит да будем тратить допустим если мы будем 2 закодируем
ноликом а bba как сделаем ну 1 0 1 1 да не будем путаться а bb или 0 1 будьте здоровы
3 тогда что ли сделаем а да 3 его вообще порог плохо я просто хотел с
вами посчитать сколько в среднем будет тратится понимаете а что не знаю вы же вы же
прикладники то как как сжимать а можно куда-то один бит что ли нам а 3 3 бита это
что у нас такое будет да а раньше как мы делали с вами там хватало почему-то
начинали с двух дальше по три было да
а ну вероятность мы могли сами написать как хотим вот эти буквы е или мотэ там которые
были мы могли там вероятность переделать как двумя битами да ну понятно вот так вот да ну
допустим так ничего мы сэкономим получится меньше чем один бит на символ или нет но давайте
посчитаем интересно мне я такое раньше не считал 4 девятых это вероятность появления одного да в
среднем сколько мы тратить будем плюс 2 девятых на 2 и еще плюс 3 правильно вот так вот и плюс
одна девятая на 3 так что получится владимир считаем девятку общий знаменатель 4 плюс 10
плюс 3 так 17 девятых смотрите что получилось на сколько символа на два символа получается что
мы здесь тратим о молодцы 17 18 бит на символ что есть меньше единицы то есть такое кодирование лучше
для хранения это значит что если у вас был текст из 18 тысяч букв вы его при таком кодировании
замените на 17 тысяч битов лучше получилось теперь а какое оптимальное соотношение вот
какое минимальное можно сделать количество бит на символ это надо посчитать энтропию вот этого
распределения вы его считали там что получилось 0 092 но значит 092 меньше чем 17 18
потому что то вот это кодирование оно лучше чем наивное но хуже чем оптимальное оптимальное
почему тут все это иррациональные какие-то выражения получаются потому что оно в
симптотике получено то есть вы можете не двойными вот этими блоками делать от районами четверными и
тогда про сжатие теперь понятнее немножко стало как сжимать хорошо теперь как передавать
когда у вас есть канал какой-то связи в нем могут быть шумы вот давайте тот самый пример
который был на лекции здесь буква а или б приходит на вход она выходе может
получиться а или б или пробел когда стерлась в моем алфавите в моем
исходном алфавите не было пробелов может и другая ситуация быть что на входе там
33 она выходит там 26 транслитерация а вот здесь вот коммуникация подразумевается вот таким
образом да спасибо за вопрос хороший тут все разбито на некоторый тамбин по-английски
вы знаете что как бы разбивается на слоты все что-то приходит если вы на слоты не
разбиваете а что-то может прийти или не прийти это совершенно другая ситуация
чуть-чуть погромче немножко
нет я писал шо кстати в следующий раз скорее всего не будет так что можете отдохнуть и не
приходить и не приходить ну можете не писать это вы на четвертом курсе вам осталось учиться тут
я не знаю надо получать удовольствие от образования чем учится-то уже все спортивные интересы все
свои удовлетворили на первых там первом втором курсе так ну чего возвращаемся к этой задачке да
могут буквы да алфавит на входе на выходе не обязательно впадаю значит пример который был
в тесте а б может перейти в а б ц в а там с какой-то играет и все одна вторая в б с одной
четвертой в с с одной четвертой ну и аналогично б поскольку любая буква может перейти в любую
другую букву то любое вот это слово которое написано или вообще вся книжка которую вы
написали из а и б может в любую вообще последовательность преобразоваться могут
все буквы ц получится на выходе могут может метеорит сейчас упасть на может
но вероятность этого события мало понимаете точно также и здесь игра на вероятностях построена
то есть несмотря на то что весь текст может преобразоваться в полную ерунду то есть в одну
и ту же букву например но вероятность этого события мало и поэтому передавать информацию можно
в каком смысле вот в том смысле математическом что вероятность ошибки есть есть вероятность
ошибки но эта ошибка стремится к нулю при n стремящимся к бесконечности то есть она как
бы показывает что становится не очень маловероятным это надо чтобы прям совсем неудачником быть чтобы
вот это все преобразовалось полную в одну букву ц понятно теперь смотрите как построена схема
значит вы можете выбрать несколько последовательности большой длины ну например вот это будет одна из
них другая будет например вся состоятельств б вот это вторая вот длина их и выбрать таких
n большой штук вот таких вот понятно что каждый из этих слов я просто точками здесь обозначал но
может перейти вообще в любое другое слово более вероятно что оно будет переходить в какую-то
какое-то подмножество которое называется условно типичные слова понимаете и тогда вероятность
ошибки что вы например взяли это слово и думали что оно попадет вот это условно типичная область
а она попала вот в этом эта вероятность будет стремиться к нулю принц мяч бесконечно значит
если у вас вот эти области все не пересекаются то замечательно вы эти n слов можете передавать
вот вы меня мучили все вопрос вот-вот-вот значит векторе спрашивает что мы передаем если наш
канал связи вот этот вот может брать на входе буквы а и б она выходе выдавать а боится то все
мы тут зажаты условиями задачи мы можем только на вход принимать а и б понимаете
пример тут у вас один корабль здесь флажки вывешиваем этим там пиратский флажок с крестика
но знаете эту коммуникацию морскую но видели всякие ленточки а на них флажки вот такие вот разные
они что-то означают понимаете представим что здесь туман туман плохо видно здесь вот там другой
корабль например там тоже есть какие а ну ладно тут смотрят в общем подзорную трубу что ж тут
какой им сигнал хотят передать в этой азбуке их морской есть определенный набор флажков вот
эти флажки это а и б пусть будет две всего понимаете будьте здоровы но вы-то хотите какое-то
что-то осмысленное передать правильно поэтому что вы можете сделать вы можете вот эти n
слов поставить им соответствие какие-то биты понимаете вы можете сделать сказать вот это
слово которое вот это вот а да оно будет у вас означать все нули например а следующее где все
б будет обозначать нули а в конце стоит единичка а вот это последнее слово из n будет обозначать
все единички но это подразумевает что это n есть степень двойки да такая запись вот в прошлый раз
я неправильно сказал потом екатерина под правила что вот это количество битов вот здесь вот количество
бит это и определяет это n по формуле n равняется 2 в степени пересылаемых бит биты понятно то есть
отправляя вот это слово а-ба оно может преобразоваться например в c-c-b-a-c-c но это
c-c-a-b-c оно вот здесь вот я тогда понимаю что было послано мне вот эти вот нули и
и и делаю область принятия решения то есть вот это все это означает что мне были отправлены нули
понятно что вот это битовое кодирование оно не обязательно битых все делать я мог здесь
нарисовать там чебурашку крокодил гену там еще чего-то с помощью них кодировать теперь вот
это взаимная информация и x и y она показывает вам количество бит вот этих вот бит вот этих
делённое на длину слова а слово у вас вот такое вот а-б-б-б-а
прием стремящимся к бесконечности
это понятно теча n маленькая это была длина слова как раз таки вот это да
вас как зовут артем вот если бы шума не было вот представьте вот этот вот канал без шума то
что на входе вложили на выходе получили то есть буквы цени реализуются чему тогда равняется
вот это вот взаимная информация так я пока правую доску освобожу пока вы думаете а хорошо да
да но мы мы здесь собрались чтобы обсудить тему мы не привязаны к задачкам
смотрите у вас подход со стороны практики как это реализовать на практике правильно
у шэннона был подход который показывал сколько вообще в принципе вот возможно вот так же как
в начале занятия мы с вами взяли конкретную схему взяли и получилось 1718 прекрасно молодцы
а шэннон сказал что вы как не старайтесь да две штучки ну давайте возьмите да да
слово является носителем какой-то информации а какой вы сами определяете
ну да но если букв много в входном то да то да да да да то есть например вот в примере
пусть у вас русский алфавит там 33 буквы если шума нет сейчас мы вернемся к вашему если шума
нет то у вас эти каждый из букв приходит в нетронутом виде правильно сколько битвы
логарифма 33 столько бит информации можете передавать то есть в этом случае скорость
будет больше единицы понимаете скорость r мне запретили скорость это будет логарифм 33
один символ содержит себе столько бит информации если все буквы встречаются равновероятно
ну ваша интерпретация такая да да ну понятно а если буквы встречаются не равно вероятно
меньше чего быстрее будет меньше хуже будет почему лучше-то вот представьте себе выраженный
случай буква например c встречается с вероятностью 1 а другие с 0 это значит что у вас всегда на входе
вот здесь вот источник производится все но вы же не можете передавать информацию то есть скорость
еще наблюдения не обязательно меньше единицы как будет в наших задачах если размерность
алфавита большая а шума мало то вот скорость может быть больше скорость она по отношению
к битам считается у вас 33 уровневая система грубо говоря когда вы берете русский алфавит
один символ сразу содержит себе столько бит информации так еще раз как артем а ну и
чего будет если мы значит без ошибок передаем буквы а б распределение пусть вот такое две
третьих одна треть но если шума нет то задачка сводится просто хранение информации правильно
ну то есть то что вот мы считали в тесте 092 бит вот тогда ответ для этой взаимной информации
будет 092 так смотрите а если бы буквы артем были бы с вероятностью
одна вторая каждая то чему бы тогда вот это взаимная информация равнялась без шума
единиц правильно все поняли почему если вероятности букв а и б одинаковые получим один вот здесь
это нам ставят теперь задачку общую в таком виде сейчас сотру кусочек если вам задан теперь
только канал передачи информации и входной алфавит то есть смотрите вам задана вот такая
вот картинка но буквы а и б вы можете выбирать частоту их появления сами тогда что у вас
получается вы должны адаптировать вероятности этих букв под свойство вашего канала и тогда
вот эта взаимная информация при максимизации по вот этому распределению букв на входе даст
вам классическую пропускную способность еще вот здесь вот индекс ш ставят шеннад
понятно распределение букв на входе влияет на взаимную информацию поэтому оптимизируя
можно получить значение самое большое задачка 3 из задания смотрим
от ее размера да зависит
да
вот это не очень понял зависит от чего
условно типичных слов да да но сейчас скажу вот когда мы пишем количество
типичных слов мы пишем что их 2 в степени n энтропия ну соответствующего там либо
условно энтропия либо просто энтропия где здесь дельта потому что количество сейчас
количество этих слов типичных слов оно смотрите как да появляется слово симптомика
правильное здесь пишу плюс дельта ой минус дельта да а здесь пишу 2 в степени n аж плюс дельта теперь
я могу устремить и эпсилон к нулю и дельта к нулю и тогда найдутся найдется смотрите
для любых эпсилон дельта больше нуля существует некоторое число н 0 такое что для любого n больше
да да да вероятность зажата должна быть в коридоре в некотором да но когда n стремится к
бесконечности от этой дельта мало что влияет дельта делается для конечности понимаете чтобы
чтобы учесть возможное отклонение и законечность
для практики это не очень актуально а для теории актуально так третья задачка открываем
найдите пропускную способность классического канала связи то есть будем вот эту формулу и с
вами использовать в котором алфавит на входе состоит из пяти букв вот этих ел м от а шум
в линии описывается стахастической матрицы с условными вероятностями ну тут написано
какими кто-то может хочет у доски порешать есть такие смельчаки
выходите выходите а мы будем на центральной доске уже настроена она а я не знаю ну формулы
давайте нарисуем вот эти буквы ел м от и во что они могут переходить мы это делали в
прошлый раз ну вспомним третью задачу сейчас решаем
да с вероятностью одна вторая в себя либо с вероятностью одна вторая сдвиг да осуществляется
все-все правильно теперь
л в себя и в м может переходить все правильно тут я уже не силен
в е может все да каждая стрелочка имеет вес одна вторая видите это нам упрощает жизнь облегчает
ее да смотрите правильно все говорит екатерина правильно нужно подобрать вероятности этих букв
чтобы взаимная информация приняла максимальное значение да но в прошлый раз у нас были зафиксированы
вероятность е было с вероятностью 1 четвертая это 1 восьмая 1 восьмая да мы напишем п
да да да у нас есть вероятность каждой буквы
а шикс это будет энтропия того распределения которое вы сделаете
это надо считать и это надо считать давайте с этого начнем давайте стахосеческую матрицу запишем
А
м
Да, хорошо, это стахастическая матрица,
которая задает канал связи, понимаете? Это в наших формулах было PY при условии x,
вот что это такое. Теперь нам нужна вот эта энтропия, да? А это есть энтропия какого распределения?
PXY, а это есть произведение вот такое. PX мы не знаем, зато PYX мы знаем, а где-то нули.
Поэтому давайте, как в прошлый раз сделали, сделаем матрицу XY. Но у нас доска, мы пользуемся
преимуществом доски и просто эту матрицу сейчас переделаем в матрицу PXY. Как это сделать?
Да, но что домножать?
В строчках или в столбцах будем делать?
Ну от XR же только зависит. Если их зафиксируем, значит там одно будет значение. На PY, PYL, PYM, PYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYP
Слева на диагональную матрицу по диагонали стоят PY, PYL, PYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPYPY
И он умножит каждую строчку на PY
Смотрите, сейчас
Короче, мне здесь сверху надо написать PY
У нас здесь единички, да?
Да, PY
Тут PY
Дальше что там?
PY
PO
И PT, да?
Так, ну
Минимальными усилиями это мы сделали
Мы с вами теперь знаем вероятность появления букв XY, да?
Да
Вход-выход
Ну, например, вероятность того, что на выходе будет T на входе L совместно, равна 0
Короче, мы хотим вот эту штуковую
Вот эту вот
Все, сейчас
Нам ее нужно
Минимизировать, а это максимизировать
Мы все выражение сначала запишем
А вот так можем неправильно получить
Сейчас мы все напихаем
hxy давайте считать
Формулу-то какая?
Пишите, пишите, минус, сумма
А тут смотрите, тут у вас случайный велич XY, да?
Да, правильно
Правильно
Так, тут стоят нолики, ноль логарифа нуля
Да, выкидываем
Теперь смотрите, PY пополам, PY пополам
Два раза встречаются, правильно?
Один
А еще под логарифом тоже PY ставим
Давайте честно сделаем, минус 2
А, пополам тогда убираем
Логарифом PY на 2
Чем вам не нравится?
Дальше будет минус PY и так далее
Понимаете? По всем буквам так
Теперь давайте преобразовывать
Сверху дотянетесь, не дотянетесь?
Кружочек поставим, чтобы мы там продолжим
Логариф
А это есть?
Выносим
Так
Логариф
На единицу
Да, это тоже единица
Да, только знак какой, плюс или минус?
Плюс
Мне не разделились
Голосуем
Плюс пишите, плюс один
Голосу
Ну
Значит, видите, что произойдет, когда мы будем вычитать HXY?
HX уйдет
Все, нам Y теперь нужно
Для того, чтобы Y, нужно маргинальное распределение по Y знать, да?
Суммируем по X
Это что будет?
В столбцах
Ну, куда хотите
Понятно же, что все получится
В столбцах просуммировали
В столбцах или в строчках?
В строчках
Что такое равномерное?
Что такое равномерное?
Если была бы другая
То есть у нас очень простая матка
Очень простая, поэтому у нас все жизнь упростилась, да
Если была бы сложная, было бы сложнее
Да, и видите, какие выражения
ПЕ плюс ПТ и так далее
Правильно?
Вот энтропия такого распределения останется
И будет минус один
Правильно?
Значит, мы с вами получили вот такую величину
Сейчас я здесь ее напишу
И XY
А, там не видно
Не торопитесь
Буду еще какое-то время писать
В нашей задачке
Вот такая формула получается
Где XY
Это есть
Энтропия
Распределение
Какого?
Ну, там вот эти вот ПЕ плюс
Что там ПТ пополам
И так далее
Вот такого сложного распределения
И нам нужно максимизировать эту величину теперь
По вот этим буковкам ПЕ, ПТ
Но они должны все одинаковые быть
Почему?
Правильно
Энтропия максимальна
Когда распределение равновероятно
Равновероятно
Правильно
Правильно
И чему равно это максимальное значение?
Логариф ПТ
Да, а все вероятности ПЕ равняется ПН
равняется так далее
равно одной пятой
Так, у кого есть калькулятор?
Ну-ка, посчитаем вот эту величину
Все, спасибо большое
1,2
1,32
Хорошо
А кто помнит ответ в задаче 2?
Ну, чуть-чуть поменьше
В задаче 2
Ответ был чуть-чуть поменьше, чем в задаче 3
Понимаете?
Но, кажется, какая разница?
1,32 или 1,28
Разница проявляется в том, что
это все умножается на число битов
Понимаете?
Поэтому разница имеет значение
как раз-таки в этой асимптотике
Так, делаем перерыв
После перерыва у нас с вами уже кодирование в квантовые состояния
Теперь носителем информации будет квантовый объект
И там разные сценарии, возможно
Вот про это поговорим после перерывчика
Рекап
Рекапилот
Повторить
R это показатель
степени 2
в степени n
который показывает
число надежно передаваемых слов
длины n
Вот
Да
Лучше которой никак не сделать
А если вы попробуете передавать со скоростью больше, чем c
то обязательно будет ошибка
не исчезающее при увеличении длины
Зависит от задачи
Supremum R это c
Так, c
Если у меня канал без шума, почему не достижимо?
То есть когда-то достижимо
А в общем случае теория ставит вам водораздел
Вот эту картинку снова нарисую
Вероятность ошибки в зависимости от скорости передачи данных
Если скорость меньше c, то вероятность стремится к пределу p
то ошибка стремится к нулю при энстимящемся их бесконечности
Если же скорость больше, чем c, то ошибка стремится к единиц
Вот
Фундаментальную границу установили
пропускная способность
это делает
Ничего страшного
N
R
может
Не понял
Это же показатель
Показатель
Это же мы не умножаем
Не это число возводим в степень, а это показатель степени
Поэтому все нормально
Если, например, русский алфавит без ошибок
R равняется чему?
Логарифм 33 по основанию 2
2 в этой степени есть 33 в степени n
Передаете n букв
33 в степени n
Это и будет ваше количество надежно передаваемых слов длины n
Все сходится
R может быть меньше единиц
В каком примере?
Который в тесте был
Там еще что-то такое
Так
Переходим к четвертой задаче
А по этому листику не надо отмечать?
Нет, не надо
Кажется, все записается
Очень хорошо
Вы не записались?
Четвертая задачка
Квантовый источник равновероятно производит кубиты
в одном из двух чистых состояний
P0 или P1
Смотрите, какая подоплека стоит
Если вам нужно передать bit 0
Вы нажимаете кнопку, которая вам производит, например, фотон с одной поляризацией
Если вам нужно передать bit 1
Вы нажимаете кнопку и производите фотон с другой поляризацией
Эти два состояния обозначаются P0 и P1
Чистое состояние
Пока начало понятно?
Понятно
Теперь спрашивается, какое количество классической информации можно извлечь
при индивидуальном измерении каждого кубита?
Это означает, что потерь нет в задаче
У нас стоит детектор
Этот детектор
Что делает?
Приходят к нему фотоны, у него срабатывает аппаратура измерительная
Здесь имеется в виду, что у вас загорается одна лампочка или другая лампочка
Когда приходит фотон
Если загорается верхняя, мы интерпретируем это как ноль
Если загорается нижняя, мы интерпретируем это как единичка
Вот эти P0 и P1?
В общем случае нет, на этом вся игра задачи построена
То есть состояние P0 и P1 не артегональное
На входе 0 единичка, классический
На выходе получаем 0 единичка, классический
Но промежуточная часть квантовая
Это первый подход, когда мы индивидуальное измерение каждого фотона делаем
Каждого кубита
Индивидуальное измерение каждого кубита
Давайте такую схему тогда сделаем
У нас было какое-то слово x1 и так далее xn
Мы его преобразовали в квантовые состояния
Вот этими шариками я обозначу
Здесь было rho x1, rho x2 и так далее rho xn
В нашем случае вот это rho 0 это Psi 0 Psi 0
А rho 1 это есть Psi 1 Psi 1
Видно вам внизу или плохо видно?
Пси 1, пси 1
Пси 1, пси 1, да, спасибо, извините
Пси 1, пси 1
Дальше вы что делаете?
Что значит индивидуальное?
Вам приходят вот эти шарики
И вы каждое измеряете
Первый шарик измерили, второй шарик измерили, третий измерили
Здесь при измерении первого шарика получите 0 или 1
Ну и аналогично во втором и в последнем
Вот что значит индивидуальное измерение
Это первый вопрос задачи, попробуем на него ответить
Ну давайте сначала поймем, что нас потом будут спрашивать
А потом нас будут спрашивать про коллективные измерения
Что это означает?
Во втором вопросе вы ждете, когда к вам придут эти n фотонов
И все их сразу, вот эти все фотоны измеряете за один раз
Понимаете? Здесь более сложное первее
Вот это и исходов измерений у этого измерения больше, чем исходов измерений у каждого
Ну здесь 2, например, исхода 0 или 1
А здесь будет 2 в степени n исходов
Понятно? Вот это это коллективное измерение
А внизу индивидуальное
Да
А, равновероятно это означает, что сами вот эти 0 и единички
С вероятностью одна вторая возникают на входе
Понятно?
Исходный классический текст содержит равновероятно 0 и единички
Вот окажется в итоге, что ответы будут разные
То есть если мы будем индивидуально измерять каждый кубит
То получим один ответ, одну скорость передающую информацию
А если коллективно будем измерять, то получим другой ответ
Но поскольку коллективное это общее описание, которое включает как частный случай
Индивидуальное
Понятно?
То во втором случае ответ будет больше
Общая постановка понятна?
Теперь переходим к деталям
Если я измеряю каждый кубит индивидуально
Индивидуальное измерение
Оно описывается некоторым киовием
Оно содержит два элемента
e0, e1
Которые дают нам вероятность получить y при условии, что на входе был x
По такой формуле
След e, y, rho, x
Максим, откуда эта формула вообще?
В прошлом семе, правильно?
Правильно, правильно
Стахастическая матрица задается этой формулой
Но эта стахастическая матрица
Почему она, кстати, стахастическая?
Сумма по y
Антон нам сейчас скажет
Потому что у нас POE в сумме дает единичный оператор
Да, правильно
Вот эта сумма по y, e, y
Это есть единичный оператор
След от любого оператора плотности есть единичек
Стахастическая матрица
Она, фактически эта матрица
Определяет эффективный классический канал, правильно?
То есть теперь, если я вот так вот в рамочку все это обведу
И помещу это в некоторый черный ящик
То я не буду знать устройство
И не буду думать об устройстве этой начинки
То будет классический вход, классический выход
Стахастическая матрица задается этой формулой
Что я могу делать в этой задаче?
Закрыть?
Что я могу делать в этой задаче?
Я могу менять z 0, e 1
Измерительные устройства-то я могу менять?
Могу измерить в одном байсе, в другом байсе делать измерение
Понятно?
То есть эта стахастическая матрица зависит от моего выбора POVM
То есть, смотрите, к чему свелась задача
Скорость передачи информации в этом случае
Ну или давайте не так
Давайте уже прям максимум брать
Максимальная скорость передачи информации в этом случае обозначается вот так
c 1 1
Что означает этой единичке?
Это означает, что вы каждую букву кодируете в отдельный кубит
Это первая единичка
Вторая единичка означает, что вы каждый кубит измеряете индивидуально
Ну ладно
Чего хочу написать?
Хочу написать, чтобы вы вот это POVM можете менять
Давайте вот так напишу
e x y supremo
Что вы можете здесь менять?
Вы можете менять POVM-элементы
Вот эта величина будет ответ на первый вопрос задач
Здесь вы распределение менять не можете, потому что по условию распределение зафиксировано
равновероятно 0 к единичке
Теперь мне нужно вам рассказать, как сделать оптимизацию по POVM-элементам
Ну да, это сложная задача, но мы ее с вами успеем решить из физических соображений
c 1 1 показывает максимальную скорость передачи при кодировании битов в отдельные кубиты
и индивидуальном измерении этих кубитов
Да, вот если было бы c 1 бесконечность, это значит, что вы коллективное измерение производите
А c бесконечность 1 это значит, что вы не каждый бит кодируете, а ждете слова
и это слово целиком кодируете в последовательстве бит
А что такое c бесконечность? Бесконечность – это когда вы кодируете большие слова
и измеряете эти большие слова
И вот это будет истинная пропускная способность
Смотрите, у нас есть багаж знаний с прошлого семестра
В чем он заключается? Там была такая задача
Различение квантовых состояний с минимальной ошибкой
Там у вас были состояния rho 1, rho 2
Которые, кстати, тоже с вероятностями 1 и 2 появлялись
И вы их отправляли на некоторое измерительное устройство, у которого было две лампочки
Безошибочно там было три лампочки
А тут с минимальной ошибкой
Вот это измерение, которое минимизирует эту ошибку, является оптимальным
Напомните, как зовут вас? Артур? Артур
Так, вспоминайте эту задачку
И то, и другое нам надо, на самом деле
Так, напоминаю из прошлого семестра
Называлась эта задача minimum error discrimination
Дискриминация
Это была одна вторая, здесь была единичка, плюс одна вторая
rho 1 – rho 2, 1 норма какая-то
Помните такое? Что играет роль rho 1 в нашем случае?
Ну вот это вот psi 0 на psi 0, например
А что играет роль rho 2? Это psi 1 на psi 1
Как были устроены оптимальные POVM-эффекты?
E1 – оптимальная, сверху напишу opt
Это была, что такое? Это была сумма
некоторых псикатых-псикатых
по K таким, что λкт
там больше 0
где λкт, в свою очередь, входит в разложение rho 1 – rho 2
Это была сумма по K, λкт, псикатых-псикатых
Можно и так, но поскольку в сумме должно сдавать единичный оператор, то где-то надо будет поставить
Ну куда эти проекторы включить? В E1 или в E2? Мне без разницы
Вот здесь вот 0 выслагаем?
Ну давайте для E2 напишу
То же самое, только здесь тогда λкт меньше либо равно 0
Если вы здесь делаете больше либо равно 0, то здесь просто меньше пишете
Потому что E1 плюс E2 должно давать единичный оператор
В rho, а здесь нет
Чувствуете? Тут умножаем, тут не умножаем
Так, вы вспомнили, Антон, более меня?
Что-то помнится
Что-то помнится
Так, картинку геометрическую рисуем для лучшего понимания
У нас кубит
Состояния чистые
Где лежат чистые состояния на шаре блуха, Алексей?
На сфере
Пусть у нас ψ0 вот тут вот
Вот его матрица плотность чистая
ψ1 вот тут вот
Как выглядят вот эти E1 и E2?
Это сумма проекторов
Видите?
Потому что ψ это собственные векторы в спектральном разложении rho1-rho2
У вас всего пространство двумерное
Значит, проекторы какие у вас?
Ранга 1
Значит, это сами чистые состояния E1 и E2
Вообще, да?
Картинка вот так вот выглядит
Проведем центральный диаметр параллельный этой хорде
Вот это будет ваша E1
Которая есть, с одной стороны, чистые состояния
С другой стороны POV-м элемент
А это будет E2
Ну или E0-E1
Это прошлый семестр
Вроде это не рисование
Мы тоже не рисовали
Это мы не проходили
Это нам не задавали
Это мы не рисовали, а теперь вот знаем
А это случайно вышло?
Потому что она двумерная
В прошлом семестре у нас было в общем случае, или нет?
В прошлом семестре было в общем случае, да
Поэтому мы не рисовали
Вот смотрите, rho каждое задается вектором Bloch'а
Помните?
r такое было
Здесь r, r
Вычитаем одно из другого
Получится r-r
Вот оно, r1-r2
Понимаете?
Вот они
Ну значит, как выглядят оптимальные измерения мы знаем
Теперь нам нужно саму вот эту величину узнать
Какую скорость передачи информации
Надо узнать в первом вопросе
Смотрите, p-success это что такое?
Это вероятность того, что мы отправляли нолик и получим нолик
Правильно?
А p-fail это вероятность того, что мы отправляли нолик, а получим единичку
Следовательно
Мы с вами получили так называемый бинарный симметричный канал
Ноль переходит в ноль с вот этой вероятностью p-success
Один минус эта вероятность переходит в единицу
Ну и аналогично единичка
Тоже с этой вероятностью p-success
Нолик и единичку
Бинарный симметричный канал называется
Почему симметричный?
Потому что 0 и 1 ведутся одинаковым образом
Но давайте вероятность успеха p просто обозначим
Неуспех это 1-p будет
Так, для такого бинарного симметричного канала в силу его симметрии
Как вы думаете, какое оптимальное распределение?
Ну вот как раз-таки одна вторая, одна вторая
А чему равна скорость взаимная информация x и y в такой постановке?
Если на входе 0 и единичка с вероятностями одна вторая возникает
Взаимная информация?
Взаимная информация это есть hx плюс hy минус hxy
Вот точно так же, как Екатерина считала
У вас hx чему равно?
Если равновероятны 0 и единичка?
Единица
Если они равновероятны, то и 0 и единица на выходе тоже равновероятны
hxy тоже равно единице, а hxy осталось нам определить
Стахастическая матрица является в этом случае b
Умножаем по x на одну вторую
Вот получили такую матрицу вероятностей совместного распределения
Это матрица pxy в виде матрицы
Это понятно? Успеваете?
Ну ее энтропию надо посчитать
Считаем минус
Как записать энтропию?
Энтропия это минус одна вторая p логарифом этого p пополам
И все это два раза
Ну и аналогично, минус 1 минус p логарифм 1 минус p делить на 2
Да, два раза, поэтому я уже одну вторую не пишу
Значит смотрите, логарифм частного раскрываем
Получится тут минус логарифм двойки
С коэффициентами 1 минус p и p они сложатся
Получится единичка, одна из единичек пропадет
Останется такой ответ
Единица минус так называемая бинарная энтропия
Бинарная энтропия задается формулой
Минус p логарифм p, минус 1 минус p логарифм 1 минус p
h2p это бинарная энтропия
Получается мы ответили на первый вопрос задачи
Ответили, подставить вместо p вот это выражение и получим ответ
Но это такое, знаете, как вам сказать
Через psi 0, psi 1 вот это выражение упрощается
Для чистых состояний вот это выражение
Можно еще к более красивому виду привести
Но в силу недостатка времени сколько у нас осталось?
Значит понятно, что надо сделать
Это подставляем в формулу 1 минус h2p
И упрощаем
Через psi 0 и psi 1
И получаем ответ
Давайте теперь второй вопрос задачи осветим
Почему там может получиться что-то лучше?
Вот допустим вы взяли, закодировали буквы в отдельные кубиты
Вот эти шарики
А потом взяли и измерили их коллективным образом
Коллективное измерение
Оказывается, что в этом случае вы можете выжать вот ту самую границу Холева
Вот тот максимум, который мы сегодня на лекции обсуждали
Вы можете в этом случае получить
То есть вот это и x и y тогда в этой задачке
Где y это теперь уже не отдельные буквы выходного алфавита
А вот эти коллективные длинные буквы
То есть наверное тут
Не буду писать тогда и x и y, напишу просто r
Вот в этом случае можно добиться достижения этой границы Холева
Подробно, конечно, в следующий раз
Какая формула будет тогда?
Формула будет такая
S rho средняя минус сумма по x' Px S rho x
Что такое x?
Это либо 0 либо 1
То есть в нашей задачке это будет
Энтропия среднего состояния ансамбля
Среднее состояние ансамбля это Psi 0 Psi 0
Плюс Psi 1 Psi 1 делить на 2
Отнять
Среднюю энтропию
Что такое средняя энтропия?
Это минус 1 вторая S Psi 0 Psi 0
Минус 1 вторая S Psi 1 Psi 1
Чему равна энтропия чистого состояния?
Какого кубитного?
Просто чистого
Кубитный, не кубитный, не важно
Потому что его спектральное распределение какое?
Выраженное
Это энтропия 0, это энтропия 0
Получается энтропия вот этого состояния
Что-то не слышу я звонка-то
Был?
Отманули
Геометрически где находится вот эта точка?
На картинке которая была
Сейчас я ее восстановлю
А где на хорде, правильно?
Середина той самой хорды
Вы поняли, да?
Оно смешанное, а не перепутанное
Вот где среднее состояние ансамбля сидит
Смешанное
Значит смотрите
В чем логика этой задачи?
Оставшаяся часть
Сыразить и вот эту величину
Которая была в пункте 1
И вот эту величину, которая в пункте 2
Через
Через что?
Сейчас объясню под оплеку
Вот понятно, что если я эту картинку завращаю как-то
Поверну
Вся картинка повернется, но энтропия не изменится
Почему? Потому что энтропия определяется расстоянием до центра
Канитарные преобразования не меняют энтропию
Знаете вы это свойство?
S у ро у крестик равняется S ро
Почему? Потому что собственные значения не меняются
Поэтому в этой задаче не важно, как вы конкретно выберете PSI 0
Как вы конкретно выберете PSI 1
Не важно
Важно только будет
Их скалярное произведение
Которое показывает близко они находятся друг к другу или далеко
Понимаете?
И зависит только от модуля этого скалярного произведения
Потому что в комплексности все вращения, все уходят
Получается, что все ответы нужно выразить через эту величину
Через эту величину выразить все ответы нужно
А тогда можно что сделать?
Тогда можно построить вот такой график
По оси OPSIS отложить вот это скалярное произведение PSI 0 на PSI 1
А по оси... что там оно?
Ординат отложить вот эту скорость
И вы увидите такую картинку
Если состояния совпадают...
А, и 0 давайте 1 минус эту величину, то будет некрасиво
Вот давайте 1 минус эту величину по оси OPSIS отложим
Если эта величина равна 1, то есть состояние артагонально
0 и 1 артагонально
То вы можете делать измерения прямо в этом базе 0 и 1
Правильно?
Устали уже, не соображайте, и все нормально
И получаете 1 бит информации в этом случае
То есть точка 1,1 у вас есть
Если же эти состояния совпадают
То это все равно, что вы все время одну и ту же букву отправляете
0
Звонок был
Рисую, как выглядят схематичные картинки
В случае 1, первый вопрос задачки будет вот так как-то себя вести
А в втором случае вот так вот
Ну, я уж не помню, как туда выпуклась какая
Значит, по вашему запросу в следующий раз
Мы можем либо доделать эту задачку, либо дальше пойдем
В общем, поговорим с вами
В общем, до встречи через неделю

Мы снова говорим на семинаре про какие-то более практические аспекты всех этих
распределенных систем, но прежде чем мы перейдем к теме нашей сегодняшней, это вызов ударенных
процедур или ударенный вызов процедур. Я так до сих пор и не узнал, как правильно. Есть ли у вас
вопросы ко мне, какие-то пожелания, предложения по поводу того симулятора, с которым мы работаем?
У нас есть инструменты, их довольно много, они довольно сложно устроены, они еще и меняются,
и у них что-то добавляется. Появился визуализатор, даже два. Вот выберите,
который вам больше нравится, и в нем еще много чего появится. Но, возможно, вдруг вам что-то
непонятно, что-то неудобно, что-то неинтуитивно, документация чего-то не покрывает. Вот, пожалуйста,
не держите это в себе, говорите сразу. Не товарищу, а в чат. Если вы ждете час, пока бегают
симуляции, а потом через сутки пишете об этом, то лучше говорить сразу, пока этот час идет,
чтобы я знал об этом, ну в смысле мог на это повлиять. Если вдруг вы встречаетесь с чем-то,
что вам неудобно, то вместо того, чтобы принимать такое должное, можно это обсудить.
Я, со своей стороны, вам рекомендую прочесть документацию, попробовать обязательно визуализатор
и почитать логи. Без этого, кажется, ничего не выйдет. И у меня есть такой мета-совет,
когда вы пишете код, не думайте про симулятор. Чем меньше вы про симулятор думаете, тем вам
лучше. В документации есть пара пунктов, где о нем нужно думать. Вот там, мне надо
назвать глобальные перемены, и нужно использовать интерфейсы, а не там хроны, рэндомы, все такое.
Этих исключений довольно мало. Во всем остальном вы можете думать про физический мир, где что-то
параллельно исполняется, где узлы действительно могут отказывать, где все занимает время. В общем,
если вы будете жить в такой модели мира, то, скорее всего, и в симуляторе с вами все будет
хорошо. А если вы будете думать про то, что в симуляторе есть и чего там нет, то, возможно,
вы сами в ногу выстрелите случайно. По поводу первой задачи. Второй задачи. Первый в симуляторе.
Я вам очень рекомендую сделать сначала как-нибудь, а потом сделать через стру тайм. А потом сделать
хорошо через стру тайм. Там есть два способа, два шага в решении стру тайма. Первый наивный,
второй можно немножко подумать и что-то там распролилить, попробовать. Или вы можете прочесть
статью про спаннеры. Довольно сложно, потому что вы пока не сможете ее прочесть. Но если вы вдруг
найдете, как там используется стру тайм, то там, по-моему, замечания про это точно есть.
И, ну вот, в общем, вы можете применить у себя все идеи, которые там есть, и споткнуться
об какие-то грабли интересные. Мы это потом обсудим. Опять же, пишите, делитесь этим со мной.
Мне любопытно. Ну а сегодня я хочу поговорить с вами немного про код, который мы пишем,
точнее, который вы уже умеете писать сами и который вы умеете использовать. Вот кусочек,
где делается рпс-вызов. И я бы хотел с вами сегодня обсудить, как этот код работает и как
вообще... Мы живем с некоторым фреймворком для удаленных вызовов. Я хочу сегодня эту задачу
обсудить подробнее. То есть, как такой фреймворк в принципе должен быть, как он может выглядеть,
какие в нем есть подзадачи, какие можно в архитектуре этого фреймворка выделить слои и
где какая задача решается. То есть, чтобы у вас не было ощущения, что у нас некоторый произвол,
некоторый произвольный интерфейс, и он как-то не похож на все остальное или похож на все остальное,
нужно выяснить, где он похож, где он расходится и какие вообще у нас варианты. Ну, примерно,
как мы в прошлом семестре обсуждали, как мы можем сделать конкарнси, и у нас было много
способов разных. Вот у нас другая задача, где тоже можно все делать по-разному и в жизни вы можете
работать с разными фреймворками. Мы сегодня хотим разобраться, какие там есть трейд-оффы и какие
там есть альтернативы в смысле дизайна. Ну, а для начала нужно вспомнить, как вообще мы к этому
пришли. Итак, мы пишем с вами алгоритм. В алгоритме есть алгоритм репликации ячейки памяти,
но или наивного репликации кива-реохранилища, и там есть узлы и клиенты. Я коротко напомню,
как мы вообще пришли к понять этих ударенных вызовов. Мы, конечно, оттолкнулись от алгоритма,
который у нас был. У нас были узлы системы и были клиенты, которые с этой системой работают,
ставят запрос туда. Ну а дальше, как мы про этот алгоритм рассуждали? Вот у нас есть клиент,
он посылает свою команду, допустим, сет ключ значения на какой-то узел случайной этой системы,
и этот узел начинает собирать кворумы. Сначала он собирает кворум на чтение,
чтобы выбрать временную метку для записи, потом он эту запись с этой временной меткой записывает
на какой-то кворум. Ну и другой клиент делает что-то похожее, можно его изобразить синим цветом.
Дальше, что мы сказали? Посмотрим на какую-то машину, вот на эту машину или на эту машину,
или на эту машину, как они в этом алгоритме участвуют. С одной стороны, вот эта реплика,
Р1, Р2, Р3, Р4, Р5, напоминаю, их всегда нечетное количество. Вот реплика Р4, она получает
команду от клиента и начинает выбирать timestamp, собирать кворумы, чем-то вот таким вот занимается.
Реплика Р2 занимается немного другим, она сама ничего по себе не делает,
не обслуживает клиентов в данном исполнении, она получает команды от этой реплики и записывает у себя
что-то, сравнивает свою версию с той, что я отправил этот узел, ну и либо отвергает его предложение,
либо обновляет свое локальное значение. И вроде бы это все можно было бы написать буквально на кулбеках,
где узел получает сообщение, на нем дергается какой-то кулбек для команды Set или кулбек для команды
Запиши, обнови локальное значение. Но все же можно заметить, что у этих узлов, хоть они все равноправны
относительно того или иного запроса, есть разные поведения. Вот этот узел, он координирует запрос,
он выбирает timestamp, собирает кворумы, дожидается их, какую-то активную роль выполняет. А вот этот узел
относительно этого красного запроса, красной записи, он пассивен, он просто принимает запросы
от этого координатора и обновляет свое значение, либо не обновляет, если оно устарело уже.
И довольно разумно, что вот эти разные поведения, эти разные реакции на разные наборы сообщений
можно было бы как-то отделить друг от друга. Мы сказали, что у нас в алгоритме есть роль координатора
и роль реплики. Вот координатор, это активная роль, она бдрайвит протокол записи, а реплика,
она пассивная, она просто отвечает на запросы, никогда сама коммуникацию не инициирует. И довольно
разумно, если вот эти две роли, которые логический алгоритм, конечно же, есть, были бы выделены как-то
в коде. То есть это два различных поведения узла. И относительно каждого запроса он может быть либо
координатором, либо репликой, может быть всем и тем. В данном случае вот R4, он и координатор
этого запроса, и реплика, которая на команду координатора отвечает. То есть вот эти роли
существуют по одной, существуют вместе, и разумно было бы их в коде выделить в какие-то
отдельные сущности, в отдельные классы. И у этого класса был бы какой-то набор методов, в зависимости
от того, какие команды этот участник, эту роль может принимать. Для координатора эти команды,
это set и get, это внешние команды от клиентов. Для реплики это команды, ну вот у нас можно
называть local read, local write, это команды записи или чтения локального значения. Это вот первая мысль,
которая нас посетила. Вторая мысль, параллельная первая, заключалась в том, что с одной стороны мы
здесь рисуем стрелочки, и вот координатор получает этот красный запрос от клиента, а дальше его раздает
другим, но в том числе и себе. И на уровне сети это, конечно, асинхронная отправка сообщения,
то есть мы пишем, ну мы пользуемся, конечно, каким-то транспортом сообщения, но в конечном итоге мы
кидаем в канал байты, и они улетают, и после этого мы ждем, что в обратную сторону однажды что-то
прилетит. Главное, что вот на этом уровне и в нашей модели, которую мы построили на первой лекции,
были только асинхронные сообщения. Но если мы смотрим на поведение нашего владелеца, на его устройство,
то мы там рисуем такие диаграммы, где есть координатор и есть реплика, и они все-таки
общаются структурировано. Координатор задает реплике какой-то запрос, и она на него отвечает.
И вот на стрелку в одну сторону есть стрелка в обратную сторону с ответом, и они логически связаны.
Поэтому с одной стороны, да, мы можем писать код на колбэках, и да, мы можем писать код на асинхронных
сообщениях, но по факту нам и колбэки не очень удобны, нам нужно роли выделять, нам удобно было бы
роли выделить. А с другой стороны, у нас все-таки коммуникация не в модели асинхронных сообщений,
а в модели, где у нас есть какое-то сообщение-реквест и какой-то ответ-респонс. То есть координатор в этой
коммуникации является клиентом, а реплика является сервером, который отвечает на запросы клиента.
И в свою очередь, координатор является сервером для клиента внешнего, а клиент... Этот узел R4, он
с одной стороны сервер для запроса клиента, и с другой стороны он клиент, когда он обращается к серверу
другой реплики для записи своего значения.
Если мы это в голове держим, то мы, наверное, понимаем, как нам удобнее было бы писать код, чтобы
и вот это было в коде выражено, и вот эти роли как-то были явно присутствовали, и коммуникация клиент-сервер
тоже была бы явной в коде, потому что так наши алгоритмы устроены. И тут мы приходим к понятию,
который называется, естественным образом приходим к понятию, который называется RPC.
Это вызов remote просить G-call.
В чем смысл? В том, что мы не думаем про сеть, мы пытаемся сеть скрыть. Мы пытаемся
рассуждать в терминах объектов. Да, мы не живем в модели разбираемой памяти, он между нами сеть,
но в то же время как будто бы здесь координатор берет и вызывает на каком-то объекте реплика
какой-то метод типа local write. Да, этот объект живет далеко, этот объект существует вот здесь,
на другой машине, но даже не то, чтобы он в памяти живет, у него может быть состояние персистентное
на диске, как у нашей реплики. Но тем не менее мы хотим как будто бы взять и на месте координатора
вызвать методы на себе и на трех других репликах. То есть мы буквально хотим локально сделать
вызов функции так, чтобы на самом деле эта функция была вызвана где-то на другой машине.
И когда этот метод вызванный на каком-то объекте на другой машине вернет ответ, то мы его получили
из своего вызова локально. Не то чтобы прямо так вот нам удобно будет делать, но вот такова задумка
в RPC. И давайте мы сейчас наверное переместимся на некоторое время на экран. В ходе, который мы пишем,
все так и происходит. У нас есть координатор и у нас есть реплика, и это отдельные классы.
Они отвечают за какое-то поведение свое. Реплика имеет методы localWrite и localRead, а координатор
имеет методы клиентские set и get. И в методе set мы выбираем каким-то образом неправильную здесь
временную метку, а дальше как будто бы вызываем на объекте реплика на других машинах методы
localWrite. Передаем туда аргументы. Ну вот буквально объекты языка C++. И на выходе получаем тоже
какой-то объект. Но здесь получаем void, это неинтересно. Где-то в get мы получаем наверное что-то другое.
Мы получаем stamped value. Если от деталей алгоритма абстрагироваться и поговорить про сам RPC,
сам базовый механизм, то он для клиента выглядит так. У нас есть некоторый сервис. Предлагаю
говорить про калькулятор. Это такой стандартный Hello World. Вы хотите как клиент иметь доступ к
калькулятору, который находится на другой машине. И вы хотите вызывать на нем методы буквально так.
Умножить 3 на 7 и получите ответ. Буквально вы делаете вызов на каком-то локальном объекте.
Но на деле вы хотите, чтобы этот вызов происходил удаленно на другой машине. Так что вам нужно
сам объект, который живет на другой машине и умеет умножать два числа, перенажать два числа,
раскладывать. И дальше получается, что вам в такой конструкции, если вы хотите такой абстракции
достичь, вам нужно два представления для объекта. С одной стороны, вам нужен реальный объект,
который выполняет эти операции на удаленной машине. И он в RPC называется сервисом. И вам нужно
какое-то локальное представление этого удаленного сервиса. Он называется, это локальное представление,
такой объект называется стап. И вот у клиента есть стап, а у другой машины, которая находится
далеко от вас, есть сервис. И стапы сервиса они реализуют, конечно же, один вот тот же интерфейс.
Умеет умножать и складывать. Ну а дальше вопрос, как это все сделать?
Да. Вот стап – это такое локальное представление для удаленного сервиса.
В чем смысл этого стаба? Что он нам дает? Ну вот он нам дает такую иллюзию, что мы работаем с
локальным объектом, как будто бы. Мы не думаем ничего про сеть здесь. Мы буквально вызываем метод
на объекте, передаем туда аргументы. Какими сущностями мы на этом уровне оперируем? У нас есть
стап сервис, это такие вот концы этой всей конструкции. У нас стап, с другой стороны сервис.
И там есть методы, там есть типизированные аргументы, там есть результат. Вот так мы о этом говорим.
Ненадолго мы были на проекторе, теперь возвращаемся к доске. Итак, вот есть клиент,
и есть с другой стороны сервер. И что делает клиент? Клиент вызывает метод на стабе.
И с другой стороны, где-то спустя какое-то время, на каком-то сервисе конкретном, вызывается этот же метод foo.
Это два вот крайних уровня этой конструкции. Заметим, что, наверное, у нас же машина,
удаленная, допустим, даже может быть не одна, но так или иначе она хочет готово эти запросы
обслуживать конкурентно, и мы тоже готовы на этом стабе функцизовать конкурентно из разных потоков.
И ничего этого в стабе и в сервисе в этих понятиях нет. Этот уровень про это не думает.
Как бы мы могли сделать этот удаленный вызов? В конце концов, мы не можем сделать это сразу.
У нас между клиентом и сервером есть сеть.
И единственный способ, по которому этот вызов может удаленно состояться, единственный способ это сделать – это отправить сообщение.
Давайте я аккуратно нарисую сейчас заготовку себе на будущее.
Вот посередине находится сеть. И все, что вы можете делать здесь – это отправлять в одну сторону сообщения и в обратную сторону сообщения.
При этом никакой семантики у этого уровня нет. Это уровень, который называется транспорт.
И вот любой фреймворк для RPC без слоя транспорта, без этого уровня представить себе невозможно.
Это в конце концов единственный способ, через который происходит вся коммуникация между клиентами и серверами.
Этот уровень работает не с методами. Вот здесь есть методы, здесь есть аргументы, здесь есть типы.
Вот на уровне транспорта ничего этого нет. На уровне транспорта есть просто сообщение.
Не типизированные, просто строчки byte, про которые мы ничего не понимаем.
Если мы говорим про RPC framework, то в нем обязательно должен быть уровень транспорта.
Этот транспорт работает с сообщениями, в смысле не типизированными строчками byte.
И транспорт про эти строчки ничего не понимает, какие там сообщения внутри уложены.
Транспорт дает вам два механизма – для клиента и для сервера.
Клиент вызывает Connect и передает некоторый адрес.
Сервер вызывает Surf и слушает на каком-то порту.
Это даже не какие-то конкретные адреса, и порт – это не число, потому что как именно реализован транспорт совершенно неважно.
Важно, что он не знает, как устроены адреса. Они могут быть устроены произвольным образом, в зависимости от того, как этот интерфейс реализуется.
Когда вы коннектитесь к серверу на стороне клиента, то вы получаете себе, во-первых, socket, в который можно асинхронно отправлять сообщения.
А кроме того, вы в Connect регистрируете обработчик, чтобы реагировать на сообщения, которые прилетают вам по обратную сторону.
Со стороны сервера происходит примерно то же самое. Вы указываете порт и регистрируете обработчик, через который будут вызываться вызовы HandleMessage и HandleConnect.
HandleMessage – это обработчик, который вызывается при доставке сообщения.
И вот вы получаете себе socket для того, чтобы можно было ответить.
И вызов HandleDisconnect для того, чтобы узнать, что соединение порвано.
То есть здесь мы предполагаем, что есть какая-то абстракция соединения.
И вот этот уровень, он и отвечает на вопрос нам, какие гарантии мы ожидаем от сети.
Мы никакие серьезные гарантии не ожидаем. Мы ожидаем, что мы можем отправить сообщения в сеть.
И оно будет либо доставлено Eventually, либо на нашем обработчике, который мы зарегистрировали в Connect, будет вызван Callback HandleDisconnect.
И тогда мы всякие ожидания относительно доставки должны потерять для всех сообщений, для которых мы уже сделали send.
Так что вы как бы в теории, когда вам говорят, что сообщение будет доставлено, это ничего не значит, конечно, потому что может быть Disconnect.
И тогда вы уже ничего не понимаете про состояние этого соединения.
Здесь вот между этим уровнем, где есть стабы и сервисы, и типы, и аргументы,
и уровнем, где у нас есть транспорт таких вот не типизированных сообщений, должен быть еще какой-то слой.
Понимаем ли мы, как он должен быть устроен? Какую задачу он должен решать?
Вот этот слой транспорта он решает задачу доставки сообщений.
Нет, не сериализация. Но сериализация, конечно же, чтобы спуститься от уровня стаб до уровня транспорта
или подняться на стороне сервера от уровня транспорта к уровню сервиса, нужно уже иметь типы, нужно восстановить эти типы.
Поэтому нам, да, само собой, нам нужно решать задачу, которая называется задача сериализации и десериализации.
То есть как трансформировать объект в нашем любимом языке в строчку из Byte.
А потом как ее в обратную сторону восстановить. Это одна задача.
Но да, она решается и даже нужно дважды применять.
Но вот между понятием стаба и транспортом есть некоторый зазор.
Вот что в этом зазоре находится?
Мне кажется, что вы знаете, что находится, потому что если вы были в сознании, когда вы писали первую домашнюю работу, вы вряд ли забыли.
Смотри, мы...
Смотри, мы...
Цикловайл не может в архитектуре находиться.
Ну нет, так не получается, значит нужно на картинку возвращаться.
Я надеялся этого избежать, но мы будем страдать.
Вот смотрите, на этом уровне у нас есть вызов.
Вызов функции на некотором сервисе.
Сколько этих сервисов может быть здесь?
Ну вообще говоря, сколько угодно.
У нас больше два сервиса. Каждая реплика сочетает все два поведения.
И координатор, и каждый узел.
И реплик, и координатор.
И вот мы вызываем на конкретном сервисе конкретный метод.
И с другой стороны, он должен вызваться.
И мы получаем ответ.
А с другой стороны, на уровне транспорта у нас типов нет.
Мы это уже выяснили.
А еще и семантики запроса ответа тоже нет.
У нас есть просто сообщение.
Мы вот здесь умеем сказать send, и сообщение улетает в сеть.
А потом, если вдруг к нам прилетает другое сообщение,
то у нас вызывается обработчик, candle message.
Этот месседж относится к чему угодно.
Может быть, вы сделали три вызова, и вот вы отправили один реквест,
и получили респонс на какой-то другой вызов, который вы сделали ранее.
Вот связи между ними на уровне транспорта никакой нет.
Эта абстракция максимально простая.
Она занимается доставкой сообщений.
Она не знает про семантику, про типы сообщения.
Она не знает про то, что есть какие-то вызовы, есть понятия клиента,
и вот этого колера и коле.
У нас это в первой домашней работе называлось каналами.
Вот здесь был канал.
А здесь симметричная ему сущность называется RPC-сервер.
Вот и давайте подумаем, зачем и то, и другое нужно.
А заодно, где мой хороший черный маркер?
Вот он.
Вот как распределены обязанности между всем этим?
Здесь есть стаб, здесь есть канал, а ниже этого находится socket.
Ну, транспортный socket и некоторый интерфейс.
А здесь уровень транспорта представлен в виде I-сервера.
Над ним находится RPC-сервер, а над ним в обратную сторону находится уже конкретный сервис.
Вот понимаете ли вы, как в этой конструкции распределены обязанности?
Вот я предлагал ключевые слова писать.
Вот здесь есть методы, аргументы, типы.
Здесь есть просто сообщения.
Асинхронные сообщения при чем?
А вот какие ключевые слова, какие buzzword я должен написать здесь?
Это хорошее, на самом деле, замечание.
Вот здесь у нас есть один объект, и мы его называем методом.
Но очевидно, нужна какая-то синхронизация, если мы конкурентно это все делаем.
Здесь нужна просто конкарнсия какая-то, то есть в том или ином виде.
Когда мы делаем запрос, хотим ли мы дождаться его синхронно,
или мы хотим получить какое-то будущее представление для результата этого запроса, фьючу?
Вот мы вроде бы решили, что мы хотим всегда получить фьючу,
потому что фьючу всегда можно дождаться синхронно,
но если мы захотим, мы можем из них комбинатором какую-то другую фьючу сплепить для ожидания.
Вот конкарнсия с этой стороны, это фьюча.
Ну плюс что еще?
Плюс овей для того, чтобы ее дождаться можно было синхронно.
А с этой стороны конкарнсия в каком виде реализуется?
В каком виде она появляется?
Сейчас нет.
Ну ты же писал канал.
Промесс не работает по сети все же.
Фьючу отдается пользователю, промесс остается внутри конструкции.
Это всегда так. На всякий случай, еще маленькая мораль с весны.
Промессы в коде пользователей не торчат нигде.
Промессы всегда скрыты, фьючи торчат наружу.
Конкарнсия появляется здесь на уровне интерфейсов.
А что происходит вот здесь, симметрично с другой стороны?
Конечно есть. У нас есть сервер, машинка.
На ней есть сервисы.
Есть много клиентов, которые вызывают вызовы на них.
Конечно же, мы не хотим обслуживать клиентов по одному.
Нам нужны конкурентные обработчики.
Для этого нужен какой-то предпул, в котором запускаются файберы обработчики.
Мы говорили, что в RPC каждый обработчик, который запускается,
если он будет файбером, то это будет удобно,
потому что файбер может заснуть и собрать квором, дождаться его.
Эти обработчики, конечно же, должны обслуживать запросы к системе
конкурентно и по возможности параллельно.
Поэтому нам нужен какой-то инструмент для этой параллельности и конкурентности.
У нас это файберы, трэдпулы и вся эта конструкция, которую мы изучали.
Это симметричная сторона.
Вот здесь есть конкарнсия.
А какую еще задачу решает этот канал и RPC-сервер?
Какие слова еще были мне сказаны в самом начале?
И на которых не нарисованы на картинке.
Вот здесь еще появляется понятие request-response.
Но вот именно здесь эти сообщения,
отправляемые через транспорт и доставляемые в обратную сторону,
обретают такую семантику.
Транспорт просто отправляет сообщения на узел.
Он даже не знает, что это какой-то запрос для RPC.
А чем занимается канал?
Вот он, с одной стороны, получает от этого стаба какие-то уже вызовы,
трансформирует их в сообщения, запросы.
И когда он получает сообщения в обратную сторону, через ScandlerSocket,
то канал понимает, к какому запросу это сообщение в обратную сторону относится.
На какой запрос это сообщение отвечает?
Этот уровень решает такие задачи.
Этот уровень решает просто задачу транспорта синхронного сообщения.
Этот уровень занимается сериализацией, типами, аргументами.
Итак, вот уже выстроилась какая-то архитектура.
Я утверждаю, что более-менее любой фрейморк RPC должен в себе содержать такие слои,
потому что здесь просто границы между задачами проходят очень естественные.
Когда мы переходим сюда, то какой интерфейс у этого канала?
Вот этот месседж, который был в коле у канала, это была просто строчка.
Вот канал уже не понимал, как именно, что он за запрос задает.
Он получал себе некоторый дескриптор, то есть имя сервиса с другой стороны,
имя метода, которого на него можно вызвать, и сериализованные аргументы.
Что это за аргументы, мы не понимаем, типов уже здесь нет.
А дальше мы отправляем через транспорт, упаковываем этот запрос в сообщение,
оно прилетает с этой стороны, и вот уже здесь, когда мы переходим сюда,
типы стираются, типы аргументов и типы значения.
Этот слой ни про то, ни про другое не знает.
И когда нам возвращается ответ из этого канала, он нам возвращает фьюч, но фьюч у чего?
Фьюч, это называлось там месседж.
То есть просто строчка.
На входе строчка, на выходе строчка.
Как ее интерпретировать, пусть об этом думает стаб.
И то же самое здесь.
Это сообщение прилетает сюда, RPC Server понимает, что это запрос
к какому-то методу какого-то объекта, к какому-то методу какого-то сервиса,
и вызывает на нем метод, но при этом как именно интерпретировать аргументы,
каких они типов, сам Server не знает, это не его проблема.
Она решается вот на этом уровне.
То есть здесь мы сериализуем, здесь мы десериализуем.
Но на самом деле сериализуем мы дважды.
Понятно ли почему?
Ну, с одной стороны, мы сериализуем аргументы.
Но дальше нам же нужно, при переходе отсюда-сюда, у нас здесь есть логический реквест,
а на этом уровне есть просто строчка byte сообщения.
Вот нужно же, чтобы с другой стороны эту строчку byte прочитали и интерпретировали,
что это запрос к какому-то методу какого-то сервиса.
Так что вот здесь задача сериализации решается еще раз.
Но уже если здесь мы сериализовали аргументы, то здесь мы сериализуем сам запрос.
То есть структуру, где есть поля, какой сервис нужно выдать,
какой аргумент нужно выдать с какими аргументами.
Это все улетает на другую сторону.
И здесь сначала срабатывает обработчик сообщения я.
Это сообщение десериализуется в структуру response.
Из этого response извлекается, какой именно метод какого сервиса нужно вызвать.
Ну что, слои понятны?
Вот здесь типы сериализации, здесь concurrency и request-response,
здесь просто доставка сообщений.
Вот границы очень четкие, ну и теперь можно показать,
как это все матчится в конкретный код, чтобы вас ничего не удивляло.
Итак, есть транспорт.
И здесь интерфейс максимально простой,
и он должен быть настолько простой, чтобы его можно было реализовать
более-менее для любого протокола под капотом,
можно было бы использовать HTTP, можно использовать TCP.
И при этом, поверх этого всего, можно было бы реализовать вот эту смантинскую RPC.
Почему еще удобно транспорт абстрагировать?
Понятно ли это?
То есть, почему здесь не написано конкретно TCP, а почему здесь написан интерфейс?
Потому что, по крайней мере, тесты удобно писать.
Вы тестируете свой фреймворк, там, где есть реализация, там, где есть concurrency,
и удобно подставить реализацию, которая по сети вообще ничего не отправляет.
Собственно, в примерах к фреймворку здесь реализован транспорт,
который реализует как будто бы локальную доставку сообщений,
просто хранит очередь пакетов и доставляет их, эмулирует TCP в какой-то степени.
Ну и если вы захотите сделать что-то более разумное, чем эмуляцию на одной машине,
вы можете подставить какой-то произвольный фреймворк, который реализует синхронную отправку.
Например, есть 0MQ.
Ну и вот для C++ там тоже есть реализация, причем, кажется, даже не одна.
Ну вот видите, обилие языков, которые поддерживает.
Про языки мы сейчас поговорим еще.
Что?
Здорово. Я рад, что ты что-то помнишь.
Хорошо.
Вот над этим уровнем находится уровень канала.
Это уже уровень RPC.
И здесь мы имеем такой интерфейс, мы вызываем метод.
Метод – это приобретение канала.
И вот здесь у нас есть репрессор.
И вот здесь у нас есть репрессор.
И вот здесь у нас есть репрессор.
И здесь мы имеем такой интерфейс, мы вызываем метод.
Метод – это пара из имени сервиса и имени метода.
И мы передадим туда сообщение.
Сообщение – это просто строчка byte.
И поверх этого всего, да, мы возвращаем фьюч,
потому что мы хотим, чтобы мы могли вызывать дожидаться либо синхронно, либо асинхронно.
И канал – это то место, где будет реализована какая-то конкарнсия.
Потому что когда вы вызываете методы через стаб, вот здесь вот,
ну здесь в примере это делается в один поток,
вы могли бы делать это параллельно из разных потоков, из разных файберов на своем узле.
И, конечно же, канал должен как-то эти запросы сервализовать,
ну и отправлять их в сети, отправлять их в транспорт.
То есть канал трансформирует вот это все, вот эти аргументы в какие-то сообщения.
И я сказал, что вот эти сообщения должны как-то получаться.
Сейчас, это не туда.
Вот такие вот сообщения, которые просто строчки.
Как из вот такой сигнатуры получить строчку?
Из такой сигнатуры получить строчку?
Ну, нужно какое-то представление объекта Request.
И вот есть объект Request, и он умеет сервализоваться.
Вот когда мы вызываем Call, то, видимо, канал эти свои аргументы Call трансформирует в такой Request,
сервализует его и отправляет в сеть.
Вот можно что-то поискать про это.
Ну вот, у нас есть Request.
Вот, мы...
Сейчас...
Сейчас...
Мы конструируем такую вот структуру,
сервализуем ее в поток Byte и бросаем в сеть.
Она улетает, и с другой стороны, на RPC-сервере
вызывается обработчик HandleMessage.
Этот обработчик HandleMessage, ну, этот message, который мы получили транспортного уровня,
на нем вызывается deserialize, и мы этот Request парсим.
Получаем у себя на другой стороне вот этот самый объект.
Вот здесь...
Ну, здесь аргументы вызова по-прежнему сервализованы.
То есть мы в сервализованном сообщении храним сервализованное сообщение.
Ну, потому что вот это сообщение с аргументами будет десервализовано на уровне выше, получается, над нами.
А дальше мы с помощью этого реквеста понимаем, какому сервису он относится, находим этот сервис,
и на нем вызываем метод с нужными аргументами.
Но, опять же, мы на уровне сервера не понимаем, как эти аргументы интерпретировать.
Для нас на нашем уровне это пока строчка Byte.
Видимо, интерпретация аргументов будет заниматься реализацией вот этого I-сервиса.
Ну, и я уже сказал, что RPC-сервер – это место, где появляется конкуренция,
где появляется конкурентная обработка запросов.
Ну, и вот тут, смотрите, что написано.
Когда у нас на сервере дергается обработчик сообщения, то мы стартуем новый файбер.
И в нем выполняем функцию процесс-реквест.
Откуда же берется вот этот самый интерфейс, реализация интерфейса сервис?
Ну, мы ее пишем сами. Вот здесь вот.
Вот мы написали этот сервис и унаследовали его от специального класса ServiceBase.
Зачем мы так сделали?
Ну, потому что этот ServiceBase позволяет нам регистрировать обработчики с конкретными типами
с помощью вот такого макросса.
А этот макросс вызывает мне RegisterMethod, где мы получаем поинтерна функцию,
ну, во-первых, мы наследуемся, конечно же, от iService, и мы реализуем Invoke.
Каким образом? Мы находим Invoker в нашем мэйпе со списком методов,
со снабором методов, и его вызываем.
А этот Invoker, он строится с помощью этого макросса, с помощью вызова RegisterMethod.
Там мы знаем типы, и там мы можем построить вот такой макросс.
Который, получив строчку и зная свои настоящие аргументы, может ее десерилизовать,
вызвать функцию с этими аргументами, и потом сервизовать ответ обратно.
В итоге типы на уровне сервиса возникают только вот в этом маленьком месте.
А сам RPC Server работает с этим сервисом,
передавая строчки и возвращая строчки, и получает в него обратные строчки.
Понятно?
Стап делает симметричные вещи. Если мы говорим про стап, то в примере вот в этом.
Когда мы вызываем Multiply, то мы вызываем RPC Call,
передаем туда канал, через который... Стап вообще строится по каналу,
который, собственно, и умеет отправлять сообщения.
И мы сначала говорим Call, Arcs, и вот здесь происходит сервизация.
А дальше мы уже говорим Via Channel, Start, и в этом Start мы уже говорим,
а дальше мы уже говорим Via Channel, Start, и в этом Start вызывается как раз Call на канале.
У нас уже есть описание метода, у нас уже есть сервизованный вход,
и канал занимается тем, что трансформирует этот запрос в какое-то сообщение для socket.
Я уже показывал. Вот здесь.
Но, я уже сказал, канал реализует конкуренцию на стороне клиента.
А конкуренция на стороне клиента в двух проявлениях существует.
Во-первых, в канале вызываются Call-ы конкурентно, из разных потоков потенциально.
Потому что у вас стабов может быть много, вызовов может быть много конкурентных,
а канал, через который вы делаете эти вызовы, он один.
И вот на нем конкурентно зовутся Call-ы, их нужно как-то упорядочивать.
Кроме того, он вниз на транспорт спускает сообщение,
и обратно же к нему поднимаются обработчики HandleMessage и HandleDisconnect.
То есть конкретный канал является с одной стороны, реализует интерфейс Channel,
и позволяет сверху развивать Call-ы,
а с другой стороны, он реагирует на обработчики снизу.
Он Handler транспорта.
И к нему конкурентно приходят разные события.
С одной стороны вызывы, а с другой стороны сообщения из сети.
И возникает следующая задача.
А как этот канал написать?
Потому что на нем зовутся конкурентно очень разные действия, очень разные обработчики.
Call-ы, HandleMessage, HandleDisconnect.
Из разных потоков, без синхронизации.
Что делать к каналу, если он хочет оставаться простым?
Что?
Ему нужно использовать Strand.
То есть все обработчики, которые в нем запускаются, в нем запускаются через Strand.
Это не единственное.
Для такой задачи Strand и нужен.
У нас есть пул потоков, допустим.
В нем четыре потока.
И у нас есть очень много каналов в разные машины.
И выполнится очень много вызовов.
И нужно много обработчиков для многих каналов,
чтобы эффективно упаковывать маленькое количество потоков.
И при этом, чтобы каждый канал работал просто однопоточно.
Ну вот давайте все операции над одним каналом упорядочим через Strand.
А все эти Strand-ы будут декорировать один и тот же threadpool.
Как мы уже знаем, с вами с прошлого семестра,
поверх одного threadpool-а Strand-ов может быть сколько угодно.
Так что, когда вызывается обработчик HandleMessage,
то этот обработчик на самом деле кладется в Strand.
Когда вызывается HandleDisconnect, тоже в Strand.
Когда мы говорим Call,
то мы строим request и снова его бросаем в Strand.
И в итоге все операции, которые меняют состояние канала,
а оно не очень-то простое.
Ну это не очень-то и сложно, но с другой стороны.
Вот все обработчики, которые меняют его состояние, они упорядочены.
А состояние канала – это вот мэпа
из requestId в структуру реквеста.
Канал занимается тем, что он мачит request и response.
Когда к нам из сети прилетает сообщение,
мы знаем, что это response.
Декодируем его вот такую структуру.
И по requestId, которую мы там нашли,
ищем запрос в мэпе.
Находим его.
И если находим, то говорим на промесе этого запроса setValue.
И отдаем туда строчку, которая прилетела с другой стороны.
А эта строчка интерпретируется снова на уровне выше, на уровне стаба.
Вот здесь вот.
То есть мы сначала по картинке, которая была под слайдом,
проходимся в одну сторону вниз,
а потом проходимся целиком в другую сторону наверх.
И в одну сторону здесь реализуем, в другую сторону здесь реализуем.
И конкарнси появляется здесь на уровне канала
в виде интерфейсов с фьючами
и в виде стренда, который реализует все операции над этим каналом,
над его состоянием.
А с другой стороны, конкарнси появляется в RPC-сервере
в виде набора файберов, в которых запускаются обработчики.
Хорошо.
А теперь, я бы сказал, довольно общая структура любого RPC-фреймворка.
В любом RPC-фреймворке должен быть слой транспорта,
чтобы абстрагировать, как именно вы доставляете сообщение.
В любом фреймворке RPC должна быть поддержка конкарнси.
Вот RPC почти никогда не бывает без конкарнси.
Если язык Go, почему он такой создан?
Почему там конкарнси есть?
Потому что он предназначен для того, чтобы выписать свои предложения.
Через RPC, например.
Конкарнси просто необходим.
И он там сделан чрезвычайно разумно.
Я, может быть, сегодня не успею что сказать,
но, может быть, потом поговорю с кем-нибудь, кто захочет.
Конкарнси необходимы,
и если вы пишете на Go у вас на месте конкарнси,
если вы используете какие-то фреймворки,
ну вот Cup'n'Pro, сейчас я вам покажу, фреймворк.
Вот вместе с ним в догон идет библиотека конкарнси.
Потому что без нее все это,
обработку запросов и вообще интерфейса представить невозможно.
А я сейчас это объясню, это очень разумная вещь.
Я как бы к этому клоню.
И, кстати, в Cup'n'Pro-то сделано все очень классно.
Там очень хитро сделано.
Там future используются ленивые.
Если вы немножко в последнем семестре делали,
то вы знаете, что обычно, когда у вас future,
то чтобы получить future,
вы должны что-то отправить в другую сторону,
какую-то операцию начать.
А если у вас ленивые future,
как в лекции про Unified Executor,
то вы можете сначала выстроить pipeline,
а потом его разумно запустить весь.
Вот Cup'n'Pro-то он с помощью своих собственных future,
которые на самом деле таски из прошлого семестра,
реализует pipeline.
То есть если вы делаете запрос к другой машине,
то можно это все упаковать в такой конвейер и запустить разум.
Ну вот такой интересный спецэффект.
Итак, значит, есть слои транспорта, конкарнти и стап.
Вот давайте сверху вниз поговорим,
насколько это можно сделать универсально.
У нас реализация очень простая.
Мы для того, чтобы отправлять сообщения
с какими-то аргументами,
серилизуем их с помощью
библиотеки серилизации,
которая называется Serial.
Это библиотека...
Ну давайте откроем просто GitHub.
Это библиотека, которая умеет
серилизовать объекты на C++.
Мы можем в каждом из них написать
такой метод вспомогательный,
в нем перечислить поля.
Тут уже в канале писали про это,
что это можно альтернативным образом делать рефлексии,
compile-time-рефлексии
или динамической рефлексии,
что совсем неэффективно.
В общем, это сейчас не важно.
В C++ вы пишете это либо руками,
либо вы пишете это в виде макроса,
как мы делаем.
Вот, например, задача номер один.
Ну и дальше эта строчка может...
В смысле, этот объект на C++
трансформируется в строчку из Byte.
Мы его передаем дальше через канал.
В любом языке,
ну как бы в экосистеме любого языка
есть какой-то дефолтный способ
серилизации часто.
Ну он в Rust'е есть, есть в Python,
знаете, в Pickle все, наверное.
Зачем это нужно?
Чтобы либо отправить объекты по сети,
либо на диске их хранить.
Значит, как-то решается.
Но смотрите, какое дело.
Возможно, мы бы хотели чуть более
универсальный фреймворк иметь,
который позволял бы нам вызывать методы,
писать клиента, которые вызывают методы,
и писать сервер, который реализует
этот метод на разных языках.
То есть мы бы хотели написать клиента
на Python, а сервер с сервисом
написать на C++.
И вот здесь уже становится сложно,
потому что...
В чем преимущество фреймворка
в библятике серилизации
проток accordance с серилизацией для конкретного языка?
В том что он может свободно
серилизовать структуры данных
из библиотеки вашей,
из Вектор, Terry разllo и Wariant.
可能 серилизовать таблы,
все, что в вашем языке есть
в библиотеке стандартной,
можно серилизовать.
Но очень сложно серилизовать wariant,
а с другой стороны, на другом языке,
где этого wariant нет, его серилизовать.
И с одной стороны,
универсальность. И если вы хотите способ сервизации кроссплатформенной, то вы должны
сильно ограничить множество типов, которые вы поддерживаете. Это во-первых. А во-вторых, вы
должны иметь какой-то универсальный способ, независящий от языка, эти типы, которые вы
сервизуете, описывать. То есть сами объекты описывать. И вот так мы приходим к альтернативному
универсальному механизму, который называется протобав. Знаете ли вы про него?
У нас в RPC framework есть ветка, и мы вряд ли на нее перейдем в этом семестре, но могли бы.
Там смотрите. Во-первых, у нас есть калькулятор, и мы в нем отправляем сообщение.
Вот сообщение мы описываем так. Мы создаем файл, который называется
арифметика прота, и в нем описываем такие объекты. messageMultiply. Это request. Там есть два поля,
типа inch64, там первое и второе. И в качестве ответа мы получаем сообщение product, в нем есть
одно поле inch64. Когда мы пишем рпс-сервис, то мы в обработчиках получаем request и заполняем
response. Это прото-описание, это и есть такой язык DSL, на котором можно описывать сервизуемые
объекты. А дальше специальный компилиатор, который запускается на фазе генерации файлов
сборки, по этим мета-описаниям компилирует, ну просто генерирует код на C++. Довольно
нечитаемый, но это ему и не нужно. То есть он по этому мета-описанию генерирует классы с полями
классы с методами, например, setValue или просто value getter для наших прото-описаний. Вот эти
объекты уже сгенерированы компилиатором портабуфа за нас. И компилиатор этот пишется для каждого
языка, и клиент на питоне вызывает этот компилиатор для того, чтобы сгенировать объекты
диссерилизации для клиента, для стаба своего, вот здесь вот. И вот заполняет эти поля. А сервер
генерирует с помощью компилиатора объекты, чтобы диссерилизовать их и читать. Вот он их читает.
Идея понятна? Да, вот здесь поэтому портабуф ограничивает набор типов, из которых можно
строить сообщения. Смотрим в описании этого языка, там есть типы, которые есть практически везде.
Вещественные числа, целые числа, булл, строчка, ну и набор байт, просто произвольный байт. Вот в
общем-то больше ничего и нет. Ну вы из них можете строить сообщения и можете строить, ну делать поля
своих месседжей другими месседжами. И вот так реализован, собственно, сам RPC. Для уровня RPC
ведь тоже нужны объекты серилизуемые? Нужна структура request, структура response. И вот сама RPC
библиотека имеет в себе message request, message response, и в message request есть message method, у которого
есть сервис, имя сервиса и имя метода. Но здесь уже есть байты, и вот байты — это серилизованный
request, который был серилизован самим стабом. Когда мы пишем стаб, то что мы делаем? Мы строим
этот request и вот здесь его серилизуем. Получаются произвольные байты, вот эти байты дальше через
уровень каналов улетают куда-то дальше. Понятна ли конструкция? Вот мы просто поменяли протокол
серилизации, и теперь в принципе мы можем сделать поддержку для этого протокола с другой стороны,
на другом языке. Нужно лишь договориться, чтобы и клиент, и сервер одинаково могли реализовать
транспорт-сообщение. Вот если на уровне транспорта они договорятся между собой, то дальше на уровне
серилизации они могут декодировать объекты запроса и объекты аргументов и ответа пользователя.
Понятно ли это? Хорошо. Тогда пару слов про то, как это реализовано на уровне провода,
потому что вообще вопрос интересный. Вы здесь уже видели картинку? Ну во-первых, есть разные
мнения по поводу про того, что все очень плохо. И вот вы видели картинку, где же она была?
Хорошо, заново. Есть разные механизмы серилизации, и вот есть протокол, который, ждем его, сеть нас в
неудачный момент решило подвести. Ну что ж, попробуем еще раз. Но нам интернет нужен,
без него мы не справимся. Вот, что можно построить протокол серилизации бесконечно раз быстрее,
чем протобуф. Но протобуф вообще он реализован-то разумно. То есть, в самом деле,
как можно написать с код серилизации эффективнее, чем сгенерировать его статически? Мы знаем,
какие поля будут, мы знаем, каких типов они будут, нам не нужно в рантайме ничего обходить,
мы можем просто сгенерировать код. Как именно протобуф все ваши структуры укладывает в байты в
проводе? Тут у него типов немного, у него есть строчки и целые числа. Вот как можно серилизовать
целые числа эффективно? Ну, во-первых, вы знаете, что есть процессоры, у которых порядок байт разный,
нужно это учесть, но это легко. Дальше, представьте, что вы серилизуете IN64, 8 байт, а значение у него
один. Вот довольно глупо тратите 8 байт на проводе. Что нужно сделать? Нет, так делать не нужно. Нужно
использовать технику, которая называется warrent-encoding. Warrent — это техника следующая. Вы пишете байты
от младшего к старшему, и в каждом байте вы старший бит, в старшем бите помечаете, это последний
байт или еще не последний. Вот если там старший бит единица, то этот байт последний, и нужно
заканчивать. Если же этот байт, если сейчас есть единица, то байт не последний, если ноль, то байт
последний. В итоге, чтобы закодировать, например, число один, вы тратите ровно один байт, в котором
в старшем бите написано, что это единственный, но это последний байт этого warrent. Ну вот так вы можете
компактно кодировать не отрицательные числа. Ты сомневаешься, что можно выложить... не очень
понимаю, в чем проблема-то? Смотри, ты никогда не работаешь, не отправляешь никому 8 байт. Вообще
так вопрос не стоит. Ты когда-нибудь видел TCP? Ты же знаешь, что по нему нельзя отправлять 8 байт,
по нему нужно гигабайт отправить, потому что сначала TCP медленно, он разгоняется, он не понимает,
насколько нагружена сеть, и насколько нагружен клиент, поэтому там есть flow control и congestion
control, чтобы не нагружать сеть и ту сторону. И вот ровно поэтому в прошлый раз мы говорили,
что для файловой системы разумно брать большие чанки, чтобы сеть нагружать большими чанками,
чтобы TCP быстро работал. Поэтому когда ты посылаешь сообщение через канал, у тебя сообщение
отправляется не одно, а вот много разных сообщений, и TCP загружен, и у тебя потом байт большой,
и разумно, чтобы в нем было меньше байт, ну в смысле, чтобы просто экономить на времени.
То есть мы отправляем много данных, мы отправляем большие наборы данных, которых очень много
маленьких портабуфов, много маленьких сообщений, поэтому... или пишем их на диск. Поэтому чем меньше
этих байт будет, тем лучше. Это мы с этим и справились. Почему так нельзя кодировать отрицательные числа?
Потому что у них есть... если мы храним минус один, то у него будет в старшем бите, 63-ем, будет
написано единичка, и мы должны будем закодировать все бесполезные нули. И тут как нужно поступить?
Нужно воспользоваться знаниями математики. Вот вы же умеете доказывать, что множество целых и
натуральных чисел равномощны. Как вы это делаете? Вы их зигзагом обходите, наверное. Ну вот поэтому
вы с помощью нехитрого преобразования конвертируете отрицательное число в положительное, то есть его
порядка и номер в обходе зигзагом, и маленькое отрицательное число становится маленьким
положительным числом. А дальше оно кодируется уже варентом. Ну строчки понятны. Строчки кодируются
как длина плюс байты. А больше ничего в протовуфе-то особенного нет. Ну ладно, там есть кое-что, но это
уже неважно. Так вот, как же это можно ускорить в бесконечное число раз? Спрашивается на этой карте.
Как вот эту процедуру будут буквально ускорить в бесконечном много раз? Нужно сделать процедуру
серилизации и десерилизации к тождественному преобразованию. Как этого добиться? Кто понимает,
о чем я вам говорю сейчас? Ну вот смотрите, что такое int в памяти? Это 8 байт. А что такое int в
проводе в протовуфе? Это какое-то переменное количество байт в кодировке варент. А что если мы
научимся концентрировать свои сообщения так, чтобы их представление в памяти и представление
в проводе было одинаковым? Ну тогда серилизация не будет ничего делать. Вот объект памяти,
его байты, это и есть серилизованная версия. Ну скажем, в протовуфе тут возникает такая штука,
которая называется zero-copy-input. Слышали вы о ней или нет? Сейчас я найду пример. Смотрите,
посторонний вопрос вообще. Есть интерфейс ридера, который умеет откуда-то читать что-то. Из файла,
из строки, из космоса. Вот как он устроен? Вы говорите readSum и передаете ему буфер. То есть
стартовую позицию в памяти и размер буфера. А этот read его заполняет. Этот readSum его заполняет.
Хорошо это или плохо? Это нормально. Но есть некоторая проблема. Допустим, вы пишете ридер для
памяти. То есть у вас есть диапазон памяти уже и вы хотите его адаптировать под интерфейс
ридера. И вам говорят readSum. Что вы делаете? Вы просто берете и из своей памяти копируете в
буфер того, что вам прислали. В тот буфер, который вам дали. Смысл в этом мало в данном случае.
То есть можно было бы просто выдать диапазон памяти из своего буфера. Вот это альтернативный
интерфейс, который называется Zerocopy. То есть вместо того, чтобы отдавать буфер в ридер,
вы наоборот просто получаете диапазон байт из ридера. Да, теперь этот ридер отвечает за то,
чтобы эти байты прожили некоторое время. Это некоторая проблема. Ровно поэтому так не стоит
делать всегда. Но с другой стороны, так вы можете избежать копий. И скажем, понятно, что если у вас
сервизованный протобув сообщения есть, то в нем в конце концов байты каждой строчки уложены подряд.
Поэтому их можно было бы прочесть, ничего не декодируя. Ну вот, протокол CapnProto это протокол,
который реализован разработчиком, который делал в Google Protobuf 2. Он ушел оттуда и делает
свой framework теперь. Вот в нем как раз такая идея. Zerocopy – сервизация. То есть представление байта,
представление объекта сообщения в памяти и на проводе одно и то же. Ровно поэтому у вас процедура
сервизации не сервизации нулевая. Она ничего не делает. Но есть альтернативная библиотека,
она называется flatbuffers. Она про то же, про то, чтобы представление в памяти и в проводе,
или на диске было одинаковым. Говорят, это для геймдеву полезно, я ничего не знаю, спросите у Рома
Сандука, если встретите. Так можно, скажем, сделать ммэп из памяти, из дисков память, и все,
у вас уже готовый объект в памяти, можно их читать. Ничего больше не делая.
Так варинтом не надо пользоваться, это протокол в проводе. Ты говоришь serialize,
получаешь байты, и в этих байтах зашифрованы варинтом числа. Потом ты говоришь deserialize,
вот эти байты преобразуются в твои числа, в твою структуру с методами. Вот я вызвал deserialize,
где-то здесь сейчас покажу. Пример у меня был на протобуфе, но пользоваться как? Скорее,
на что это влияет? Есть некоторые нюансы, то есть тебе не могут STD стринг отдать теперь,
потому что он владеет памятью, а в зерокопе ты не владеешь памятью. А еще это имеет большие
последствия для объема данных в проводе, потому что теперь ты не можешь использовать варинт,
ты не можешь с варинтами работать в своем компьютере, в своем процессоре. Поэтому,
если у тебя в протобуфе есть незаполненное поле, оно, скажем, может быть просто опущено или сжато
до одного байта, целое число. Но если ты серилизуешь через capnproto или flatbuffers 64,
то он будет занимать честно 8 байт в проводе. То есть он более вербозный получается, то есть ты
ничего здесь не можешь компактить, но с другой стороны ты экономишь на серилизации, десерилизации.
Ну у тебя такой trade-off есть, что тебе больше нравится. Ладно, последнее, о чем я хочу поговорить,
вот этот протокол мы еще не закончим, ну простите, я никого не держу, вы можете бежать. Тема бесконечная,
но она кажется очень полезна для жизни, потому что вы неизбежно столкнетесь с чем-то либо протобуфа,
либо протокол. Вот мы знаем, как делать concurrency, ну разными способами, вы знаете,
что разные библиотеки есть. Вы знаете, что есть protobuf, flatbuffers, capnproto, разные протоколы
серилизации. Это разные слои, на которых вы можете что-то свое делать, что-то менять в этой
архитектуре. А есть еще транспорт. И вот транспорт пока нам в принципе, ну мы не обсудили,
как его сделать. Какие у нас от него ожидания? С одной стороны, этот транспорт должен быть
производительным, то есть мы должны упаковать очень много сообщений туда-сюда в там малое количество
TCP-соединений, чтобы они были нагруженными. А кроме этого мы хотим кроссплатформенность,
чтобы мы могли сделать send на C++ и получить потом с другой стороны на Python что-то. Конечно же,
можно упаковать вот все эти сообщения в TCP с помощью какого-то кастомного протокола. Но если вы
хотите сделать что-то кроссплатформенное, то получается, что у вас протобуф уже есть,
у вас concurrency там уже разные в языках есть, написано все это, а протокол отправки получения
сообщения вы будете писать свой. Как быть? Вот оказывается, что задача тоже уже решена,
решена в смысле универсально решена. Решена с помощью протокола HTTP. Вот вообще HTTP это
супернеэффективный протокол, ну в смысле первая версия протокола HTTP. Там, ну это текстовый
формат и кроме того он не позволяет вам клиенту работать конкурентно. То есть если вы отправляете,
ну вы можете отправить несколько запросов в HTTP, но не вот так вот делать, а вот сразу все
отправить. Но сервер обязан отвечать на них ровно в таком же порядке. Если первый запрос залипнет,
он будет медленным, то вы на два других ответа тоже не получите. Почему это плохо? Потому что
веб, ну то есть интернет медленно работает. Вот вы приходите на сайт, и чтобы его загрузить,
вам нужно загрузить миллион картинок. И конечно же браузеру нужно грузить их параллельно. Он
не может эти параллельные запросы логически упаковать в одно соединение, потому что HTTP так
не работает. Поэтому браузер открывает много соединений. Но это много TCP соединений, и по ним
отправятся маленькие запросы. А это неэффективно, это неправильное использование TCP. По нему нужно
гигабайт отправить. Поэтому сделать и конкурентно, и быстро HTTP вам не помогает, и поэтому все
тормозит, интернет тормозит. Что делали люди? Они решили, что интернет не годится такой, нужно
новый делать. Но и сделали это в Google. Они разработали новый протокол HTTP, он звался speedy, и он стал
основой для протокола HTTP версии 2. И вот протокол HTTP версии 2, это вот то же самое, что вот HTTP 2
для TCP, это то же самое, что файберы для потоков. Вот буквально. У вас есть ограниченные физические
ресурсы, потоки, но они виртуальны, но все равно как будто физические. И планировщик операционной
системы умеет эффективно планировать их, только если они занимаются тяжелой работой. А у нас,
например, задача чтения сети, запись в сеть, она постоянно засыпает, это плохо. Поэтому что мы делаем,
мы пишем файберы и упаковываем их плотно в эти потоки. И файбер блокируется, на его
месте становится другой, поток продолжает работать. Что сделано в HTTP 2? В HTTP 2 поверх
TCP соединения, в логическом вот этом HTTP соединении, есть очень много независимых потоков данных.
То есть очень просто много независимых логических стримов, которые состоят из отдельных фреймов.
Это вот буквально вы в одно HTTP соединение логическое, в один поток byte, записываете много таких
вот виртуальных потоков уровня HTTP. И каждый независимый поток, это может быть отдельный
запрос, отдельные картинки. А сервер уже может их поработать конкурентно. У него нет обязанностей
выполнять вот такой вот, в очереди их ставить. Правда, за счет этого появляются сложности, потому
что у вас в TCP есть flow control, чтобы не загружать ресивера, а теперь у вас много потоков в одном
TCP. И в итоге вы реализуете flow control в HTTP 2 для каждого логического потока, для каждого такого
стрима. В общем, вы еще на уровне абстракции выше воспроизводите все то же самое, что есть ниже,
так как мы писали собственные планировщики для файберов. Но вы получаете универсальный протокол,
и вот этот протокол создан для вообще для интернета, чтобы сайты быстро грузились в браузерах.
Но с другой стороны, он же отлично подходит для RPC, потому что у вас как бы TCP соединение это
канал в RPC, вот эти вот стримы это независимые RPC запросы, а фреймы это вот их маленькие кусочки.
Это вот request и response. И вот одно на другое мачится. Ну и а в чем бонус-то? В том,
что HTTP 2 это штука полезная, она должна быть реализована на каждом языке, на котором вообще
люди собираются писать серверы. Если вы пишете сервер на Python, то должен быть HTTP 2. Если у вас
есть сервер на C++, если вы планируете где-то поднять сервер на C++, у вас должен быть HTTP 2 для C++,
для Rast, для God, для чего угодно. И вот можно ожидать, что для вашего любимого языка будет
поддержан протокол HTTP 2. И вот его, положив в абстракцию транспорта, вы можете получить
кроссплатформенный способ общаться к клиенту и серверу с помощью сахронных сообщений. Итого,
у вас есть на этом уровне HTTP 2, на уровне транспорта кроссплатформенный, на уровне выше у вас есть
конкарнси, которая более-менее стандартная в каждом языке, и на уровне там, где стаб и сервис,
у вас есть протобув, ну или что-то такое же универсальное, как протобув, для того, чтобы делать
реализацию. И вот вы в таких слоях, из таких кубиков готовых можете построить свой фрейнворк. И вот он
более-менее везде будет выглядеть одинаковым образом, то есть в любом языке, который бы вы не
выбрали. То есть понятно, что детали будут отличаться, интерфейсы будут отличаться, но в конце концов все
эти слои будут, и кто их объединяет, в конце концов объединяет их сам Гугл. Ну потому что Гугл придумал
протобув, и Гугл придумал HTTP 2, и Гугл написал Go. Ну как бы, вот у них есть GRPC. GRPC — это реализация
RPC, которая, ну вот как бы наиболее нативная, наверное, для Go. Ну и понятно, что она может
работать с P2, который придумал Гугл, использовать протобув, который придумал Гугл, и вы используете
Go с каналами ISFA и с GRUTIN, которые тоже придумал Гугл. Вот такой вот конец этой истории. Так что вот,
надеюсь, что вы, когда будете писать код, будете понимать, чем наш фрейнворк отличается, и тем,
что он, в принципе, отличается вон деталями, а его организация внутренняя, она абсолютно
универсальна. На самом деле, я могу и продолжать, так что просто заканчивать нам. Давайте,
если кто-то хочет о чем-то поговорить еще, мы останемся, а так можно разбегаться. Да я и сам устал.
Наверное, стоило бы поговорить про контексты, которые в Домашке есть. Я вроде бы говорил про
их немножко, но, может быть, сейчас нужно поговорить еще раз, чтобы было понятно,
что это значит. И про дедлайны, про отмену, может быть, пару нюансов еще можно было обсудить.
Да не эти дедлайны, дедлайны, которые в RPC. Ну нет, потому что я хочу сказать,
мы не обсуждали, поэтому. Ну давайте мы отпустим тех, кто изнимогает уже. Ну еще раз,
можно всем разойтись, я не против. Невозможно же все успеть в конце концов. Счастливо.
Вопрос, вот маленькое упражнение для ума. Вот у вас есть запрос, и в наших алгоритмах этого нет,
но если вы пишете RPC чуть более высокоуровневой, не то что там, где хворомы собираются какие-то,
а вы пишете RPC-сервис в какой-то большой репетированной системе, то разумно ставят
на него дедлайн. То есть вы вечно ждать не будете. Вот как именно этот дедлайн реализовать? Ну и дедлайн или тайм-аут.
Ну можно не так делать, так наоборот делать не стоит. Вот, я уже давал вам ссылку, она про
то, что вообще тайм-аут использовать не нужно. Нужно использовать дедлайны, потому что тайм-аут
это вещь относительная, и непонятно от чего его отсчитывать. А дедлайн на общей оси находится,
и всем понятен. Так вот, если у тебя есть дедлайн к запросу, то возьми и вместе добавь в свой
реквест, вот в структуру которую ты отправляешь, которую канал отправляет серверу, заведи здесь
поле дедлайн. И пусть сам сервер у себя отменит операцию, когда дедлайн на его месте, по его
часам истечет. Вообще говоря, его часы своими могут быть не синхронизированы, у него они могут
там, не знаю, бежать чуть быстрее, у тебя чуть медленнее, но в целом тебе сойдет.

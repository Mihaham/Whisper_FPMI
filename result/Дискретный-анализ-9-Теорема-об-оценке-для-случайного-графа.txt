Так, товарищи, ну всё, наверное, пора начинать, да?
Я написал формулировку теоремы, которой мы, кажется,
закончили прошлую лекцию.
Я вроде считал, что нам осталось её доказать.
Или так не было?
Не было такой?
Было?
Было, конечно.
Ну, я вроде ещё не совсем в маразме.
Это вы скоро узнаете.
Я развлекал первокурсников, но на самом деле вас-то
я скоро, прям совсем скоро этим буду развлекать.
Есть такое понятие липшитового функции на графе.
Вот это написано «липшитс», но я их развлекал как раз
именно этим.
Это мой почерк такой.
И по этому всё одинаково.
Потом я сказал, что я могу, конечно, постараться и написать
как надо, но вот обычно я так не делаю.
Так, друзья, давайте попробуем сразу так собраться и доказать
вот эту замечательную теорему.
Пафос этой теоремы я произнёс ещё в прошлый раз.
Всё моё величие, так сказать.
Ну, представляете, абсолютно дырявый граф, у него нет
треугольников, нет циклов длины четыре.
Самый короткий цикл имеет длину как минимум L плюс
один.
У него длины L циклов нету, и, тем не менее, он не красится
в к цветов.
То есть отсутствие раскраски – это не свидетельство тому,
что там обязательно большая клика.
Там ничего нет, никаких клик, кроме ребер.
Ну, ребратохроматическое число два.
Откуда?
Как?
Ну, это вероятностный метод.
То есть я же не предъявлю вам граф, а докажу, что он
существует.
Вот давайте возьмём такую вероятность ребра.
Вот такую.
Ну, вот нам в формулировке дано число L, ограничение
нижнее на длину самого короткого цикла.
И давайте через T обозначим 1 поделить на 2 L, но это какое-то
строго меньше единица положительное число.
L у нас фиксировано.
Друзья, вот с самого начала вы должны понимать, что,
ещё раз, вот я это говорил в прошлый раз, я это помню,
мы вольны выбирать количество вершин.
То есть понятно, что чем больше K, чем больше L, тем больше
будет то количество вершин, на котором нам удастся доказать
существование вот такого графа.
То есть понятно, что такой граф будет иметь огромное
количество вершин.
N это параметр, который мы там подберём, N можно стремить
к бесконечности.
А T это фиксированная величина равная, может быть, чему-то
очень маленькому, если L большое, но это всё-таки положительное
число больше единиц, меньше единицы.
И вот если вы вот эту единичку вычитаете, получается нечто
отрицательное, ну и, стало быть, это можно использовать
как вероятность.
Так, друзья, я надеюсь, все понимают, что вероятность
должна жить на отрезке от 0 до 1.
Поэтому мне важно было убедиться в том, что T это минус 1, отрицательное
число.
Брать вероятность, равная чему-то большему единице,
это позорище какое-то.
Так, рассмотрим получайный граф G от NP, вот с такой вероятностью
Так, давайте на этом случайном графе ведем случайную величину.
Кстати, как у вас продвигается история с теорией вероятностей?
Как так, никак?
Пока еще нету, да, понятно.
Ну ладно, я же вам рассказал все, что необходимо, все понятно,
что происходит.
Ну и хорошо, значит, XL это случайная величина.
Ну, раз она случайная величина, значит ее аргументом служит
граф.
У нас элементарные исходы – это графы, правильно?
Значит, давайте ее определим.
XLT от G – это количество простых циклов.
Нас интересуют простые циклы, когда мы определяем G маленькое
вот этот герс, обхват, количество простых циклов длины непревосходящей
L в графе G.
Ну, то есть, если неформально говорить, это количество
тех циклов, которые вредят нашей цели.
У нас цель – доказать, что существует граф с обхватом
больше, чем L, и хроматическим числом больше, чем K.
Ну, то есть, там две цели, как бы, он должен одновременно
иметь большой обхват и большое хроматическое число.
Но вот такие графы, в которых есть короткие циклы, они
вредят нашей цели.
И сами циклы эти тоже вредят.
Понятно, я говорю, да, неформально?
Количество тех циклов, которые вредят нашей цели.
Вредные циклы.
Да, да, да, конечно, и вот это L, это то L, и вот это L, это то L.
Да, да, да.
Это именно вредные циклы.
Ну, еще раз.
Я хочу, чтобы граф имел обхват больше, чем L, то есть,
чтобы в нем не было простых циклов длины не больше, чем L.
А это вот количество таких циклов, которых хотелось бы, чтобы не было.
Вот тут их нет, а тут они, может быть, есть.
Вот мы меряем, сколько их, и они, естественно, все вредны.
Кем больше их, тем хуже граф.
Но граф нам сильно не подходит.
Ну, хитрее, на самом деле, то есть, ваша идея правильная.
Да, нам нужно доказать, что...
Нет, ну, понимаете, какое дело?
То, что графы с обхватом больше, чем L существуют,
ну это же очевидно.
Нарисуйте цикл длины больше, чем L.
Просто вот один цикл, что?
Но надо, чтобы еще хроматическое число было больше, чем K.
Вот как пересечь два события, да?
Ну, там хитрее чуть-чуть, там будет более хитрый ход.
Но идея, в общем, такая, да, то есть, что...
Сейчас я докажу, что такие графы не просто существуют,
а что их много.
Да.
Ж от графа без циклов равно бесконечности.
Ну, давайте считать, что равно бесконечности.
Это вроде ничему не противоречит.
Ну, то есть, можно так это определить,
и это не противоречит сложности нашей теории,
потому что, если в графе нет циклов, то он двудолий.
Нам же надо доказать, что бывает граф с хроматическим
числом больше, чем K.
Да, давайте считать, что бесконечности.
Да?
Ну, да, да, да, то есть, ну...
Ну, бесконечности, бесконечность, я согласен, можно тогда
определить.
Так, ну, возвращаемся сюда.
Значит, вот просто идейно, чтобы вы понимали, что сейчас
будет происходить.
Я докажу, что не просто существует граф с обхватом
больше, чем L, но он существует.
Цикл большой, возьмите, и все.
А что таких графов полно.
Что мера множества этих графов, вероятность, больше
одной-второй.
Ну, по крайней мере, начиная с какого-то N, в предостаточно
большом количестве вершин.
Вот давайте это прежде всего докажем.
Если их много, то есть хороший шанс найти среди них и такие,
у которых хи большое.
Вы идею почувствуете, да?
Если окажется, что таких не просто один, а много,
то есть, откуда выбирать такие, у которых хи большое.
В общем, мы все это проделали.
Если бы мы просто могли это доказать, это было бы
чуть проще, чем то, что мы сделаем.
Но мы почти это сделаем, сейчас увидите.
Почти это.
В каком-то смысле, да, и тех, и тех больше одной-второй,
но не совсем.
Все.
Чуть-чуть, вот не то, что...
Ну, в общем, сейчас вы увидите, я не могу выяснить, что
В общем, сейчас вы увидите. Я идею произнес, а сейчас давайте ее аккуратно реализовывать.
Ну, во-первых, мат ожидания Excel это, по идее, вопрос к аудитории, потому что это просто
линейность, которую мы вроде как заживали в каком-то смысле. И раз вы тут присутствуете,
то вы должны уже понимать, как такие штуки считаются. Сколько в среднем есть графов?
Но там не просто c, потому что надо перебрать еще вот эти длины. Надо, конечно, просуммировать,
во-первых, по r от тройки до l. Мы же не знаем, какой именно длины будет простой цикл. Вот давайте
я обозначим r, тогда действительно тут будет c из n, но по r. Умножить на r-1 факториал пополам,
и умножить на p в степени r. Друзья, если хоть кому-то что-то здесь непонятно,
я, конечно, произнесу подробнее. Значит, c из n, по r я все-таки чуть-чуть произнесу подробнее. Это что?
Это количество способов выбрать r-вершин для цикла. Но когда вы зафиксировали r-вершин,
циклический порядок на этом множестве фиксируется вот столькими способами. И вот только после этого
вы уже считаете мат ожидания индикатора, что вот этот конкретный цикл с этим конкретным
порядком вершин в нем присутствует в графе. Это p в степени r. Нормальный темп, все успевают.
Так, ну, на самом деле l у нас константа. Я это не устаю напоминать, потому что кажется,
может быть, это там Google какой-то или Google в степени Google. Google – раз, что-то жуткое может
быть. Но это константа. От n она не зависит. Поэтому я оценивать буду по-идиотски. Я скажу,
что это меньше. Ну, давайте пока r от тройки до l. Тут напишу n в степени r на p в степени r. То есть
я c из n по r оценю просто как n в r-те поделить на r-факториал. И вот этот вот r-1-факториал,
деленный на r-факториал, оценю тупо единицей сверху. То есть я забуду про все эти r-ки,
которые там есть. Вспеваете? Вы думаете, я буду суммировать геометрическую прогрессию? Нет.
Но я все-таки перепишу. r от тройки до l, np в r-те. Не буду я суммировать геометрическую прогрессию,
я еще раз перепишу. Что такое np? Смотрите сюда. Вот p это n в степени θ-1. Поэтому np это n в
степени θ, правильно? np это n в степени θ. То есть получается n в степени θ, умноженное на r.
Ну, это, конечно, геометрическая прогрессия, да. Но зачем я суммировать, если l это константа?
Я вам напишу вот так. Это меньше, чем l на n в степени θ-l. Потому что l это константа,
правильно вы смеетесь? Это весело, да. Я вот люблю так, как грублять. Работает, да. Самое
большое, слагаемое в этой сумме будет, например, равным l. Это возрастающая геометрическая прогрессия.
Ну и ладно. l там, l-3. Да, у меня l-3. Знаете, если это гугл в степени гугл, гугл раз,
из него тройку вычитать еще. Но это какое-то просто. Да, правильно. θ-l это 1-2. Я специально так
выбрал θ, чтобы было удобно. Это константа корни из n. Правильно, да, это l умножить на корень из n.
И теперь пользуемся неравенством Маркова. А именно мы скажем, что xl больше либо равняется
n пополам. Ну, например, n пополам. Вы потом поймете, почему это полезно. Меньше либо равно
мат ожидания xl. А, по поводу xl, tl и xl. Вот звучит, да, xl extra large. Чувствуете, да, xl. Или вам
xl больше напоминает xl как программу. Знаете, xl такой, таблички рисовать, язык такой есть. Вот. Я
просто, ну, я люблю пошутить, да. Я вот тут смотрю на экраны, какая-то новость пробегает про фистех
экрана с новостями. Вот тут развешано. Там написано, что кто-то там фистехи, что ли, будут
участвовать в каком-то мероприятии. И дальше написано xl. Я думаю, что за xl мероприятие? И тут
до меня доходит, что это сороковое мероприятие. Латинские цифры, понимаете, вот так вот.
xl мероприятие. Какое крутое мероприятие xl. Ну, там мероприятие как-то названо. Я просто не
помню какое. Ну, там xl конференция, например. Xl конференция. Она не xl, она сороковая. Так,
ладно. Возвращаюсь сюда. О, смотрите. Мат ожиданий xl-того поделить на n на 2, это повторяет
просто неравенство Маркова. Значит, это у нас меньше, чем 2l корней из n поделить на n. Ну,
и, в общем, это стремится к нулю, примерно, стремящимся к бесконечности. Например,
можно сказать, что это меньше, чем 1 вторая, начиная с какого-то n. Давайте я вот здесь вот
зафиксирую, что у меня получилось. Значит, я зафиксирую вот что. Существует n1 такое,
что для любого n, больше либо равного n1, вероятность того, что xl-т меньше, чем n пополам,
больше, чем 1 вторая. Я вот так напишу. То есть, вероятность того, что xl в каком-то смысле
большая меньше 1 второй, значит, вероятность отрицания больше 1 второй. Так, друзья,
поняли, что я сделал? Ну, это как бы чуть-чуть не то, что я обещал. Не то, что много графов,
у которых вообще нет коротких циклов, а много графов, у которых коротких циклов мало. Ну,
что значит xl меньше n пополам? Это значит, у графа мало коротких циклов. Короткие — это вот
вредные, которые длины не больше, чем l. Вот если у графа xl-т меньше, чем n пополам, значит,
в этом смысле меньше, чем n пополам, мало коротких вредных циклов, вот таких. Увидите? Не, ну, хотите
я раскрою карты, но это катарсис. Это обалденная, конечно, теорема. Там действительно катарсис. Хотите,
сразу скажу, в чем катарсис будет. Ну, может, вы дождетесь его. Много графов, у которых мало
плохих циклов. Ну, давайте, наверное, докажем, что много графов, у которых хи большое. Это
ожидаемо. Нужно было доказать. Давайте. Я катарсис чуть-чуть отложу, хорошо? Не, ну,
пока вы поняли, ведь, что произошло, ничего страшного. Что понятно? Мы доказали, что много графов,
которые, если и не такие, как хотелось бы, ну, близки к таким, у них мало коротких циклов. Они
есть там, наверное, но их мало. Так, теперь давайте введем такую буковку х, которая будет
верхняя целая часть от 3 логарифмен поделить на p. Кохнул сразу. Не, ну ладно, друзья, во-первых,
я могу это в шпаргалку записывать, и кажется, мы это и записываем в шпаргалку, то есть запоминать
такие вещи не нужно. Вы, главное, поймите, в чем смысл. Но, во-первых, все-таки, как устроена эта
величина? Это верхняя целая часть от 3 логарифмен поделить на n в степени theta-1. Ну, потому что p
у нас от n в степени theta-1. Так, друзья, куда стремится вот эта функция при n, стремящемся к
бесконечности? Конечно, к бесконечности, потому что n в theta-1 меньше единицы. Ну, причем сильно,
потому что theta-1 это что-то меньшее нуля. Да, на самом деле, можно перекинуть в числитель. Это
стремится к бесконечности. Так, всем понятно, что стремится к бесконечности. Поэтому в частности
я могу, например, писать вот так, что x tilde 3 логарифмен на n theta-1. Ну, просто потому,
что аргумент стремится к бесконечности, значит, целая часть, которая от него отличается не больше,
чем на один, тоже стремится к бесконечности с такой же примерно скоростью. То есть, а симптатика,
конечно, имеет место. Так, зачем я ввел x? Сейчас вы увидите. Давайте рассмотрим случайную величину y
с индексом x. Y с индексом x от g, это тоже случайная величина, которая определяется,
как количество независимых множеств в g мощности x каждая. Мощности x. Сколько в графе g есть
независимых множеств на x вершинах? Так, ну, чтобы вам было поспокойнее на душе, а то вы думаете,
что вы не понимаете, к чему я клоню, я вот здесь вот напомню вам, есть прекрасное неравенство.
И от g больше либо равняется n поделить на альфа от g. Я его доказывал в прошлый раз и комментировал,
что оно лучше, чем амега от g, помните, да? То есть, альфа от g это число независимости,
количество вершин в самом большом независимом множестве. А тут мы берем количество независимых
множеств, у которых дано количество вершин. Сколько независимых множеств в мощности x?
Ну, дело в том, что альфа от g меньше, чем x тогда и только тогда, или давайте больше,
чем x, тогда это больше либо равно, тогда и только тогда, когда у с индексом x от g больше либо
равен единице. Согласны, товарищи? Ну, что это означает? Это означает, что вероятность,
с которой альфа от g больше либо равняется x, равна вероятности того, что ух от g больше либо равен
единице. Прыгай сюда, чтобы снизу не писать, наверх перемещайся. А это опять, согласно неравенству
Маркова, не больше, чем мот ожидания ух. Ну, неравенство Маркова применяю. Так,
снова линейность какая теперь? Сколько в среднем независимых множеств на x вершинах, товарищи?
Ну, тут совсем просто. c из n по x на 1 минус p пепенит c из x по 2. Так, ну, я, может, много пафоса
навел, но это вроде очевидно. Очевидно, да? Независимое множество это значит на x вершинах должны
пропасть все ребра. Рёбер на x вершинах c из x по 2 и каждая пропадает с вероятностью 1
минус p, а затем суммируем индикатор. c из n по x. Точно не очень быстро, все успеваем. Так,
ну, оцениваем стандартным образом. Это меньше либо равно. Слушайте, я совсем расхалявлюсь,
c из n по x меньше, чем n в степени x. Но я даже x факториал не буду писать. Почему не буду?
Потому что он очень маленький, там логарифом на какую-то фигню. Ну, т.е. этот x факториал нам
сильно не поможет. Друзья, я понимаю, что вам с кодов все-таки настолько непривычно еще асимптотики,
сколько бы вас ими не грузили. Но победители же не судят, пренебрег с знаменателем, значит,
так можно. Если вдруг поняли, почему я решил пренебречь, ну, отлично, не поняли, ничего страшного.
Так, а как 1 минус p в степени оценивается? Вот так. E в степени минус p на c из x по 2.
Но как обычно, логарифм от 1 минус p не больше, чем минус p. Много-много раз у нас уже было. Хотелось
бы, чтобы вы это запомнили. Так, это равняется E в степени x логарифмен, x логарифмен, минус p x,
x минус 1 по полов. Ну, все, что я хочу сделать, это доказать, что это дело стремится к нулю. Я думаю,
вы догадались. Догадались, что я хочу стремиться к нулю? Что вероятность, с которой независимое
множество есть большие, эта вероятность должна стремиться к нулю. Потому что нам нужно доказать,
что альфа дже маленькая, тогда и хя дже будет большое. Поняли, да, мысль? Вот, поэтому большое
оно должно быть с маленькой вероятностью. Ну, то есть нас интересует на самом деле вот такая разность.
Я х выношу за скобку в показателе экспонента x, там общий. В скобках остается логарифм минус
p умножить на x минус 1 по полов. Так, то есть получается, смотрите, логарифмен минус, я так напишу,
1 плюс о малое от единицы. Ну, потому что x минус 1 асимпатически равен x, а это, в свою очередь,
асимпатически равно вот тому, что написано. Я пишу 3 логарифм n на... Что? Что я пишу? Правильно
пишу или неправильно? Что? А, ну да, n в степени theta минус 1, это по сути p. Зачем я буду переписывать-то?
Действительно, не надо писать n в степени theta минус 1, лучше написать p. Вот это вот, это же p,
вот это вот. Все p. Я лучше p здесь напишу, чтобы умножить на p и сделать вот так. Я, конечно,
могу и тут, и тут n в theta минус 1 написать, но они чпокнут друг друга все равно. На два разделить
нужно. Да, вот на это два, конечно, нужно разделить. Ну, видите, я с запасом взял. Понятно,
что если вы из логарифма вы читаете нечто асимпатически равное 3 вторым логарифма,
то это стремится к минус бесконечности. Потом умножается на x, который растет,
то есть продолжает стремиться к минус бесконечности еще быстрее. А е в этой степени,
стало быть, стремится к нулю, как я и хотел. Правильно, да? Вот, поэтому у меня получается в итоге,
давайте вывод. Такую же рамочку, как и там, сделаю. Значит, вывод такой, существует n2 такое,
что для любого n больше либо равного n2, вероятность которой альфа g меньше x больше 1 второй.
Но у меня, конечно, рамочки получились в разных концах света. Одна в Калининграде,
другая во Владивостоке. Неудобно на видео показывать, я понимаю, но у меня мертвая зона тут
просто. Вот один вывод, это что начиная с некоторого n1, вот эта вероятность больше 1 второй,
xl-t меньше, чем n пополам. А с другой стороны, начиная с некоторого n2, альфа маленькая,
с большой вероятностью. Ну, значит, начиная с максимума из n1 и n2, события пересекаются,
правильно? Так, ну это уже надо где-то написать. Давайте сюда пойдем. Не, ну строго же больше 1
и 2, значит, пересекаются. Но больше нуля, понимаете? Больше нуля вероятность, значит,
множество не пустое. У нас конечное вероятностное пространство, этого достаточно. Хотите,
замените на две трети, никто не будет возражать. Тогда вероятность будет там больше, чем одна
треть типа. Кстати, все умеют это доказывать. Простое упражнение на формулы типа Моргана
там или чего-то. Как переписать А, объединенное Б с чертой? У нас получается, что для любого n,
больше либо равного максимума из n1 и n2, существует граф на n вершинах, существует g
на вот этих вот n вершинах. Такое, что ХLT от g меньше, чем n пополам и альфа g меньше, чем х. И вот
теперь я обещал катарсис. Вот я не стал уж заранее объяснять. Вот сейчас будет объяснение того,
как я с этим борюсь. Вот берем этот граф G, в нем мало плохих циклов вредных. Давайте из каждого
вредного цикла, каким бы он ни был, удалим любую его вершину вместе с ребрами, которые к ней
примыкают. Удалим из каждого цикла любую вершину. Можно я это не буду писать. Мы удалим сколько
вершин в итоге? Меньше, чем n пополам, правда? Но циклов-то меньше, чем n пополам. Не важно,
как мы их удаляем, просто тупо удаляем и все. У нас останется граф G штрих. У него количество
вершин больше, чем n пополам. Потому что из исходного графа, имевшего n вершин,
мы удалили меньше, чем n пополам вершин. Сейчас, секунду, извините. Алло. Не вполне. Я лучше
перезвоню минут через десять. Да. Так, слышите, это понятно, да? Теперь, на самом деле, понятно,
ну совсем очевидно, по-моему, что ХЛ от G штрих равняется нулю. Ну как могли появиться новые циклы?
Мы все циклы, какие были, разорвали. Каждого удалили вершины. И почти очевидно, ну я думаю,
что просто очевидно, что альфа тоже не могла вырасти. Потому что, если бы у уменьшенного графа
была больше альфа, то у исходного графа она была бы больше тоже. Ребра удаляли вместе с вершинами,
конечно, да. Только те ребра, которые примыкают к удаленным вершинам, естественно, имеют их
своими конца. Конечно, да. Понятно, что альфа не могла вырасти, да? Всем понятно. Ну что мы можем
сказать про этот граф, пользуясь вот этой замечательной формулой, оценкой? Хи от G штрих
больше либо равняется N поделить на альфа от G, N пополам. Виноват, N пополам, потому что вершина
N пополам. Даже можно больше написать. Это неважно. Больше чем N пополам поделить на альфа от G. Это
больше чем N поделить на 2х, что альфа от G штрих меньше чем х. Вот, поэтому получается больше чем N
на 2х. Это асимпатически равно N поделить на 2, на 3 логарифм N умножить на N в степени
θ-1. Перевернула эту дробь. Видите, попал в знаменатель, знаменатель в числитель.
Так, получается N в степени θ поделить на 6 логарифмов N. Ну и, конечно, N в степени θ растет
быстрее, чем логарифм, каким бы малым θ это ни было. Оно же фиксировано 1 на 2. Всё, это больше
чем K. Ну, формально при N больше либо равном N3. То есть, формально надо взять максимум из N1,
N2, N3. Вот такой вот N. И для него уже всё хорошо. ХLT равно нулю, а хроматическое число больше,
чем K. Вот так. Поняли, да? Тут, видите, ещё дополнительная хитрость. Такое удаление вершин
произошло, по ходу дела. Вот оно создаёт прям катарсис. Не просто мы пересекли два события и
получилось мера больше нуля, а ещё надо было подправить что-то. Да, да, да. Тут ещё надо разочек
применить вот эту максимизацию, потому что Фик его знает, а вдруг это не больше, чем K. Но мы вольны
выбирать N. Чем больше N, тем больше это величина. Да, нам повезло, что настолько мало этих коротких
циклов, что тупо удаляя вершину просто из каждого изника, даже не следя за тем, что удаление одной
вершины может там привести к удалению нескольких циклов. Плевать. Любое удаляемое всё равно их
настолько мало, что количество остающихся вершин всё равно большое. Вот так вот. И хроматическое число
в два раза оно хуже, но какая разница? Главное, что вот эта дробь растёт. Вот такой вот совершенно
удивительный результат, очень красивый на самом деле. Но я уже говорил, что потом конкретизировали
этот граф, сумели его построить явно, но там очень сложная конструкция. Это сделал Ловос,
такой совершенно выдающийся человек, который много чего сделал. Я в этом курсе ещё буду доказывать
две его теоремы очень важных, ну а пример я его приводить не буду. Всё, значит, эту теорему-то
мы доказали. Слушайте, как здорово, как быстро. Точно все всё поняли. Но давайте я теперь дальше
буду двигаться. Я ещё вам такое вкрапление важное сделаю, чтобы максимально вас замотивировать
заниматься хроматическими числами. Но я уже говорил о том, что эта штука важная с практической
точки зрения, и вы сами в какой-то момент включились и сказали, ну хорошо, важная, а задача-то НП трудная.
То есть, скорее всего, она за экспоненту только решается и вряд ли существует какой-то алгоритм,
который быстро бы находил хроматическое число. Это действительно огромная проблема, которая
приводит прямо к вызовам в области, знаете, как говорят, ивристик, то есть придумывают
ивристические алгоритмы, которые там как-то приближаются к хроматическому числу. Но оказывается,
внимание, товарищи, что даже банальный шадный алгоритм совсем тупой в некотором смысле,
что он в каком-то смысле действительно едва ли не лучше всех, но по крайней мере не сильно хуже,
чем самые лучшие. Вот про это я сейчас строго математически расскажу. Это вот тема, я не знаю,
успею я сегодня все или нет, но это важная тема, в очередной раз свидетельствующая о том, что даже
банальный шадный алгоритм для покраски графа хорошо работает. В каком смысле хорошо, это я вам сейчас скажу.
Там некий пафос разведу, будет нерешенная проблема. Вдруг кто-нибудь решит, она сложная совсем.
Ну типа того, но сейчас вот я все аккуратно скажу. Я не буду рассказывать для простоты про
хроматическое число, чуть проще разбираться с числом независимости. Ну понятно, что это связанные
величины, то есть если мы разберемся с одним, то наверное и с другим тоже. Так, друзья, что такое
шадный алгоритм для покраски графа? Ну я совсем банальную вещь скажу. Представьте себе, что вершины
занумерованы. Мы двигаемся по этой нумерации и очередную вершину пытаемся покрасить в цвет
маленьким номером, в который ее можно покрасить без противоречия с уже покрашенными вершинами.
Если это не удается, присваиваем новый цвет. Так, друзья, понятно? Я думаю, что вы как люди,
которые у нас учитесь, быстро это схватываете. Банальный, совершенно простой жадный алгоритм.
Вот давайте вот так обозначим соответствующий результат, то есть это же русское написано,
жадное. И же это то количество цветов, которые он выдаст. На выходе, если его применить к графу
же, вы скажете в какой нумерации вершин, но вот я уже дал какую-то, вот она пусть будет раз и навсегда
одна и та же. Неважно, в рандомной, как вы любите говорить, но она не случайная в смысле теории вероятности,
просто выбрали какую-то, зафиксировали и больше об этом не думаем. Какую бы ни взяли, вот в ней
будем работать. Так, ну то есть, я надеюсь, вы понимаете, что вот это х с индексом же, скорее всего,
его больше, чем реальное хроматическое число, правда? Оно, конечно, больше либо равно х и,
скорее всего, строго больше. Ну а здесь наоборот. А, кстати, я не сказал, что такое альфа с индексом же,
это может быть непонятно. Значит, если вы покрасили жадным образом вершины, на это вы поняли как,
да, то там есть вершины покрашенные в один цвет, в другой цвет, в третий цвет, но выберем просто то
множество, которое имеет самую большую мощность. И эту мощность обозначим альфа с индексом же.
То есть, жадно красим и потом находим цвет, в который покрашено больше всего вершин. Вот это
количество вершин покрашенных в данный цвет обозначаем альфа ж от ж. Значит, теорема,
которую я докажу, и она, в общем, совершенно элементарная, то есть там тоже не больше,
чем неравенство Маркова, но там такая просто рассуждение будет красивое. Теорема утверждает
следующее. Если вот такой, ну, то есть мы рассматриваем случайный граф на n вершинах с
вероятностью ребра 1 на 2. Фактически это значит, что все графы просто равновероятны. Если мы считаем,
что все графы равновероятны, то вероятность, с которой альфа от ж поделить на альфа ж от ж меньше
или равняется 2 плюс эпсилон, вот такая вот вероятность, давайте так, для любого эпсилон большего
нуля, вероятность того, что отношение реального числа независимости к тому, которое дает жадный
алгоритм, что вот это отношение не больше, чем 2 плюс эпсилон, стремится к енице, к примеру,
стремящимся к бесконечности. Это мы докажем. Но, то есть, если вы можете считать априорно,
что все графы равновозможны, равновероятны, из каких-то там соображений вашей реальности,
в которой вы существуете, если у вас нет какого-то априорного распределения, а просто вот все
графы равновозможны, и вот капает на вас такой граф, запускаете на нем жадник, скорее всего,
если вы ошибетесь, то не больше, чем примерно в два раза, и неважно, сколько у него вершин,
миллион, миллиард, квадриллион, а на квадриллионе вершин вы фиг посчитаете число независимости,
а жадный алгоритм у вас сработает очень даже. Что, на квадриллионе не сработает? А памяти не
хватит? Памяти не хватит? Ну, за полдня посчитает, конечно, то есть, это нормально совершенно на
квадриллионе вершин, но на квадриллионе вершин ошибиться в два раза это, в общем, уже не очень
страшно. Это я вас агитирую за жадник пока, что я потом буду вам рассказывать, чем это все-таки не
так хорошо, но пока агитирую. Сейчас прозвенел звонок, давайте пять минут перерыв, потом я
продолжу комментировать. Сейчас я еще прокомментирую, видите, оказывается, что-то не всегда слышно. Я еще
раз повторю, чтобы пафот был максимальным. Так, друзья, все тихо, рассаживайтесь, пожалуйста.
Значит, смотрите еще раз, вот мне было сделано замечание, что я как-то не максимально четко
распараллелил вот эти два факта. Оказывается, это не сразу доходит. Вот видите, ну, конечно,
очевидно, что жадное не больше, чем реальное, но мы ищем максимум, как-то его опроксимируем.
Естественно, то, что мы нашли, оно может быть максимальным, но может быть и меньше. Вот, а с
другой стороны, мы доказываем, что выполнено обратное неравенство. Вот смотрите, здесь альфа ж поделить
на альфа ж от ж, оно не больше, чем два плюс эпсимум, а здесь эта дробь, она, наоборот, не меньше единицы,
то есть вот если альфа ж разделить на альфа ж, то это больше либо равно единицы. Для любого графа,
конечно, вот эта дробь не меньше единицы, а у нас получается, что для почти всех графов она еще и не
больше, чем два плюс эпсимум. Вот в этом пафос. А вершина неважно сколько, в сравнении вероятность будет
чем больше вершины, тем ближе к единице, но неважно сколько в каком-то смысле. Так, друзья, вот
сейчас пафос этого результата понятен, если вы работаете не на всех графах, но на почти всех графах,
на почти всех, то очень неплохо апроксимирует жадный алгоритм. Теперь вот я обещал вам нерешенную
проблему и дополнительный пафос относительно жадного алгоритма. Друзья, значит вы просто послушайте,
ничего сейчас доказывать не буду, доказывать я потом буду вот это, но очень важно почувствовать
пафос, вот очень важно в это как-то вникнуть. Смотрите, смотрите. Чем меньше эпсилон, это вы увидите из
доказательства даже, тем медленнее стремится вероятность к единице. То есть если вы хотите
здесь написать 2,1 квадриллионная, то вот эта вероятность стремится к единице, но очень-очень
медленно. Если вы хотите написать 2,1 угольная, 1 поделить на Google, то вероятность тоже будет стремиться
к единице, но совсем медленно, еще гораздо медленнее. Поэтому вы увидите это из доказательства. Заменить
эпсилон просто нулем нельзя. Квантор стоит извне значка вероятности. Для каждого эпсилон вероятность
стремится к единице, но скорость стремления тем ниже, чем ближе к нулю этот эпсилон. И прямо
из доказательства вы это увидите. Да, это будет следовать из нашего доказательства. Нет,
мы докажем, что он стремится к единице достаточно быстро на самом деле, но заменить эпсилон на 0
не получится. Правильно, вот я про это дальше хочу сказать. Значит, нет, к сожалению, продвинутого
доказательства. То есть, ну я не буду этого доказывать, но можно показать, что жадный алгоритм в
принципе вот здесь эпсилон на 0 не заменяет. Вот ну невозможно. Перевнимание, слушаем дальше.
Существуют алгоритмы, не жадные, более сложные. Такие, что вероятность альфа дж поделить на альфа
а, а это какой-то там существующий алгоритм просто не превосходит двойке с вероятностью стремящейся к
единице. Такие алгоритмы есть. Причем они полинамиальные. Они хуже, конечно, чем жадник,
но они полинамиальные. Так понятно говорю, да? А симптотику какую? Не, ну а какая там асимптотика?
Ну сколько ребер, столько и работает он. Ну то есть это вот n в квадрате, если считать,
что это число вершина. А если ребер мало, то и того лучше. Ну почему про квадриллион было сказано,
что это все-таки полдня, потому что квадриллион в квадрате, это так, это много. Да, это уже не
полдня, я боюсь, что это я тоже хватил. Вот, квадриллион, наверное, это все-таки много. Ну ладно,
но существуют полинамиальные алгоритмы, которые позволяют убрать епсилон. Это, конечно,
великое достижение. Сами понимаете, что я это доказывать не буду. Но у нас же достижение
совсем ничтожное, такая уже математика-математика. Это скорее подтверждение, сейчас будет великое
подтверждение тому, что надо заниматься жадностью, надо жадничать. Утверждение такое,
гипотеза, не доказанная никем. Помните, вот я сказал, будет нерешенная проблема. По-видимому,
по-видимому не существует полинамиального алгоритма, который вот в этой вот формулировке
позволил бы двойку заменить на что-либо меньшее. Нет, я не говорю на 2 минус епсилон, где епсилон
стремится к нулю. А я говорю 1 и 99, например. Вот поставить здесь 1, 9, 9, 9, 9. Вот по-видимому
нет такого полинамиального алгоритма, чтобы это выполнялось. Нет, в периоде можно. Но если вы
зафиксируете любое количество девяток, то по-видимому нет такого. И никто это не может доказать. Но это
ладно то, что никто не может доказать. Но если это верно, это означает, что действительно то, что вы
говорили в каком-то смысле так и есть. Жадный алгоритм дает практически оптимальный результат.
То есть дальше начинаются какие-то изыски, а еще лучше сделать нельзя. Такой вот удивительный факт.
Да, совершенно очевидно, но понятно. Поэтому на практике, как правило, конечно, добавляют
какие-то евристики. То есть не пользуются только жадными алгоритмами. Практика отличается от этой
теории, конечно. Но тем не менее вы должны понимать, что вот такая вот есть очень красивая вещь,
которая свидетельствует о том, что жадность можно использовать. Ну чего, я не успею доказать
то все за оставшиеся... Время полчаса, да? Давайте начнем. Ну куда деваться-то? Времени-то еще полно,
а доказать за оставшуюся часть лекции я вряд ли успею. Оно несложно. Смотрите, на самом деле тут
есть элемент лукавства в этом утверждении, конечно. А именно мы уже с вами знаем. Мы знаем. Я начал
доказывать, что вероятность вот такого события стремится к единице. Я в прошлый раз доказывал.
Помните, когда говорил, что омега и альфа одновременно маленькие, поэтому почти всегда
лучше использовать мощность вн альфа в качестве оценки кром-числа, а не омегу. Это теория из прошлого
раза. Ну раз мы это уже знаем, значит нам достаточно теперь доказать что? Что достаточно доказать?
Что для любого иапселон большего нуля вероятность того, что альфа g от g,
пум, пум-пум. Чего? Большая или маленькая? Чего-то туплю. Альфа от g мы знаем, что меньше,
чем 2 лог 2 х н. Значит, надо доказать, что она больше. Больше, чем 1 минус
епселон. На лог 2 х н. Вот это вероятность стремится к единице.
Ну, кстати, да. Я как-то не догадывался так откомментировать. Действительно,
это правильное совершенно замечание. Заодно получим. Ну, естественно, да. Но как-то мне не
приходило в голову сказать, что вот в этом месте мы уже это делали. Да, действительно. Так,
ну вы понимаете, да, почему этого достаточно? Ну как? Альфа от g поделить на альфа g от g,
почти всегда числитель меньше, чем 2 лог 2 х н, а знаменатель больше, чем 1 минус
епселон на лог двоичный. Что? В смысле, если бы здесь было 2 лога рифма, это было бы хорошо.
Не понял. Почему ломается-то?
А, ну это тоже да. Ну это я понял, да, из того, что проблема не решенная следует, конечно. Да, да,
да. Но зачем сейчас об этом говорить? Понятно, что это осталось доказать? Короче. Сейчас, товарищи,
я что-то не пойму. Поднимите руки, кто понимает, почему это осталось доказать? Никто не понимает,
да? Или все понимают? Ну буквально дает. Ну как? Ну альфа от g поделить на альфа g от g. Ну дайте,
я все-таки произнесу. Меньше, чем 2 лог 2 х н, потому что альфа от g меньше. Поделить на альфа g,
которая больше, на 1 минус, минус епселон на лог двоичный н шлеп. Шлеп это 2 поделить на 1
минус епселон и это 2 плюс епселон штрих. Но поскольку по епселон стоит квантор для
любого, то и по епселон штрих тоже. Да, то есть этого достаточно. Все. Если мы это докажем,
мы завершим теорию. Дайте это докажем. Ну тут поступали очень разумные комментарии,
что это в свою очередь доказывает действительно, что не только вот такая цена верна,
но и почти такая же снизу. Это правда, да.
Так, 2 плюс епселон. А вот сюда смотри. А кстати, я говорил, да, слова симпатические,
почти наверное. То есть я не обязательно должен писать вероятность стремиться к единице,
я могу писать а, п, н, там выполнено вот это. Все ж помните такое. На всякий случай,
это может не понадобится. Так, ну давайте как-то двигаться к этому доказательству. Я, естественно,
буду доказывать, что наоборот вероятность того, что альфа ж от ж меньше либо равняется,
там меньше, строго, неважно, чем 1 минус епселон на лог 2 х н, стремится к нулю. Понятно дело,
равносильное утверждение, но обычно как-то удобнее оценивать сверху, чем снизу, поэтому я и буду это
доказывать. Вот если хотите, давайте я вот это обозначу сейчас буквой а. Это не алгоритм,
это событие. Я обозначу буквой а, событие, которое нас интересует и которое должно иметь меру
стремящуюся к нулю. Так, сейчас я нарисую некое довольно страшное следствие из этого. И мне главное,
чтобы за оставшееся время вы поняли, что это следствие верно, а оценки можем делать в следующий раз.
Смотрите, я хочу сказать, что из а следует нечто. Нечто. Ну нечто будет очень длинным,
я его буду долго писать. Я сейчас нарисую картинку, чтобы было гораздо понятнее. Если я докажу это,
то я надеюсь все понимают, что чем бы оно ни оказалось, что бы ни было вот это нечто,
чем бы оно ни было, то вероятность а, конечно, не больше, чем вероятность этого нечто. А вот
и ее мы уже оценим там как-то. Ну если из а следует что-то, значит что-то большее. Правда
а следует, значит а вложено в т. Что-то не меньшее, если строго говорить, то что-то не меньшее.
Но там будет наверняка большее, но неважно. Нам нужна оценка просто вот такая, меньше либо
равно. Нечто может и совпасть с а теоретически, это не жалко. Главное, чтобы импликация была
именно в эту сторону. Так, ну я рисую картинку. Есть теновершин, сарделька это как обычно множество
вершин нашего случайного графа. Вот смотрите, в чем состоит а. А состоит в том, что жадный
алгоритм в каком-то смысле обломался. Ну ладно, обломался, но не нашел большого независимого
множества. Но это я говорю банальность, это ну понятно. Все независимые множества, которые
ему довелось найти этому жадному алгоритму, они имеют размер не больше, чем вот эта величина.
Ну это я произношу тривиальность, правда? Давайте я введу вот такое обозначение. Вот здесь
введу m. Это будет целая часть от m поделить на дважды 1-эпсилон налог двоичный. Ну ничего тут
такого страшного нет. Это вот величина облома, так сказать, где обломался жадный алгоритм,
на чем он остановился, на каком размере. И вот эту величину я ставлю в знаменатель и еще удваиваю.
Ну такое вот число. Давайте я нарисую дальше картинку. T1, Tm. Что это такое? Это независимое
множество, которое нашел наш жадный алгоритм. Он не нашел больших независимых множеств,
но он точно нашел столько маленьких. Если он обломался, вот видите, я же импликацию строю,
следствие. Если алгоритм обломался, то есть в целом не нашел ни одного большого независимого
множества, то он точно совершенно нашел m штук маленьких независимых множеств. Ну вот,
а мне хватит m, понимаете? Я же импликацию строю, вот я в очередной раз повторяю. Мне
удобнее сейчас будет оценивать, если я скажу, что их m. Да, наверное, даже 2m, может даже больше.
Мы же не знаем, насколько они маленькие, может они вообще по одной вершине кажут. Может же такой
быть полный граф? Был бы полный граф, он бы нашел навершин просто отдельных, изолированных,
больше ничего. То есть я не знаю, сколько он в точности нашел, но уж железно я понимаю,
что не меньше, чем столько. Вот столько нашел. То есть вот это нечто, вот я его сейчас буду записывать,
существуют a1 и так далее, а с индексом m, такие, что для любого i аитоя не превосходит 1-эпсилон на
лог 2-ичный n. И существуют c1 и так далее, cm, такие, что для любого i жи, уж аккуратно напишу,
ситое пересеченное с цежитом пусто. А, ну уж сначала написать, для любого i мощность ситое
равняется аитому, и потом уже писать, что они не пересекаются. Которые являются независимыми,
но я не буду даже этого писать. Просто существует непересекающееся множество, каждый из которых
имеет мощность не большую, чем 1-эпсилон на лог 2-ичный n. То, что они независимы, это правда,
но я же пишу импликацию, давайте я не буду упоминать, что они независимые. Вот мне это не нужно.
С точки зрения будущей оценки мне плевать, что они независимые, хотя они, конечно, ковыри. Хотите
напишите себе вот это те самые маленькие независимые множества, которые точно нашел
жадный алгоритм? Точно нашел. Он не мог найти большие. Мы знаем, что он не мог найти большие,
следовательно он точно нашел много маленьких. Много, но вот настолько много. Так, друзья,
сейчас понятно, что такое мэ, что половина примерно от общего числа вершин. Маленькие
сардельчики, они занимают примерно половину от большой. Ну не больше, чем половину от большой.
Так, друзья, все уследили, да? Понятно, что происходит? Так, смотрите, для меня важно,
вот сейчас вы должны осознать, для меня важно не то, что они независимые, хотя повторяю в третий
раз это правда, для меня важно, что жадный алгоритм ни одной из них не смог нарастить.
Я напишу вот так, для любого X, вот он X, принадлежащего сардельки без подсарделек,
я его специально нарисовал вне вот этих подсарделечек и здесь то же самое написал,
для любой вершины, которая не попала ни в один из этих независимых кусочков, для любой вершины
X, не попавшей ни в один из этих независимых кусочков, для любого Y, существует Y, принадлежащий
циитому, такой, что пара XY является ребром. То есть, какое бы циитое вы не взяли, есть и такое
ребро, и такое, и такое, и такое, хотя бы одно. Мы не смогли ни к одному из этих кусочков добавить
ни одной вершины, потому что жадный алгоритм так и работает, он двигается по нумерации вершин и
выбирая очередную вершину, он не может ее добавить ни к одному из этих маленьких независимых, поэтому
они маленькие. Как мы красим? Мы выбираем очередную вершину, мы пытаемся покрасить ее в какой-то из
предшествующих цветов. Вот эти независимые множества при раскраске это цвета, разные цвета,
и вот мы не можем эту вершину покрасить ни в цвет C1, ни в цвет C2, ни в цвет Cm, не можем
ее добавить. Почему? Потому что есть хотя бы одно такое ребро, хотя бы одно такое, и так далее.
Ну вы можете, знаете как, совсем, чтобы было максимально в голове укладывалось, максимально
понятно. Ведь что такое A? A это просто множество графов, правильно? И тут написано множество
графов, на самом деле событие это множество графов. Вот если граф принадлежит A, это значит,
что на нем жадный алгоритм на этом конкретном графе. Он уже никакой не случайный, просто вот есть
граф, который принадлежит множеству A. Вы на нем, на этом конкретном графе запускаете жадный алгоритм,
и он не может построить большего, чем вот такого размера независимого множества. Но это в
точности означает, что на этом графе найдется как минимум столько независимых кусочков, к которым
жадный алгоритм ничего не сможет прибавить. Мне кажется, вот в таком вот рассуждении совсем
понятно, о чем я сказал. Понятно, да, товарищи? О, как здорово! И у меня остается 15 минут. Не,
ну что-то я могу посчитать. Начать считать? Я думаю, надо начать, иначе... Так, то есть я буду
пользоваться вот этим. Вероятность A не превосходит вероятности того, что я написал в две строчки,
плюс еще чуть не уместилось. Ну что поделать? Вот такая жизнь. Давайте считать с конца. Ну что
значит считать с конца? Это мы зафиксируем какое-нибудь СИТ, вот мы зафиксируем какое-нибудь
СИТ, зафиксируем какую-нибудь вершину Х и попробуем найти вероятность того, что существует
Y из СИТово такой, что пара XY является ребром. Вот самый конец существует Y из СИТово такой,
что XY образует ребро. Так, какова вероятность этого замечательного события? Какой вероятности
данная конкретная вершина соединена хотя бы одним ребром с данным конкретным множеством?
Ну ладно, я вам скажу. Ну, ага. Какое? Три строчки?
Не, никакого потом. Вот нам дан уже совершенно актуальный граф, конкретный граф, принадлежащий множеству
А. Он же не случайный, просто вот есть граф, лежащий этому множеству. Мы знаем, что он ему
принадлежит. Это значит, когда мы запускаем жадный алгоритм, он найдет только маленькие независимые
множества. Вот возьмем какие-то мы из них, почему он их не смог увеличить? Потому что каждая вершина
это необходимое условие, конечно, да. Ну, я не знаю, что значит является достаточным. Я не очень понимаю,
в чем вопрос или комментарий. То есть, если вы взяли конкретный граф, то для него это свойство
выполнено просто. Не то, что необходимо или достаточно, а для него это свойство выполнено.
Для него это свойство выполнено. Вот в нем те независимые множества, которые нашел жадный
алгоритм, любые мы из них, таковы, что если вы возьмете любую вершину извне, то она не сможет быть
прибавлена ни к одному из них. Может быть, но это не помогает улучшить оценку в итоге. Да-да-да,
я не говорю, что это оптимальное следствие, конечно, можно немножко улучшить. Что понятно,
что вот эти вот кванторосуществования, которые написаны тут, но они как бы тоже избыточные.
Ясно, что это объединение фактически каких-то множеств графов. Когда мы берем кванторосуществует,
это значит, что мы объединяем множество графов. Но они же пересекаются между собой. Наверное,
это тоже надо как-то учитывать. Потом мы наплевали на то, что 3-ты независимы, хотя это так. Но это
значит, что это просто не помогает оценке. Но аналитика так устроена, что это не влияет на итоговый
результат. Это дает какое-нибудь там деление на n. Ну, а что с того, что вы экспоненту поделили на n?
От этого ей ни тепло, ни холодно. Вот там смысл такой. Давайте сюда вернемся. Вероятность вот такая.
Вот. 1 минус 1 вторая в степени аи-т. Понятно почему, да? Потому что с вероятностью 1 вторая в степени аи-т
не проведено ни одно ребро. Не проведено ни одно. А хотя бы одно проведено, ну, с противоположной
вероятностью. Ни одно ребро не проведено. Отсутствуют ребра с вероятностью 1 вторая. У нас p равно 1
второй. 1 вторая в степени аи-т это, что все ребра из иксавца и ты отсутствует. Вообще все отсутствуют.
Одно хотя бы присутствует. Это 1 минус вот эта вот штука. Так, чего я еще успею? Слушайте,
я, наверное, успею добавить для любого и. Вот так вот. Если я напишу здесь, смотрите,
я вот буду издеваться. Что изменится, товарищи? Так, друзья, я не хочу издеваться на самом деле.
Я шучу, что я издеваюсь. Если я на самом деле издеваюсь, то срочно меня останавливайте и
переспрашивайте. Ну, я просто не хочу 10 раз одно и то же переписывать. Но вот если не было этой
вставочки, то вероятность мы посчитали. А с какой вероятностью выполнено, что для любого и вот такое?
Что значит для любого? Это пересечение событий, правильно, по всем и. Произведение, потому что они
по и независимы. У нас цииты и не пересекаются. Нам важно, что цииты и цежиты и не пересекаются.
Поэтому вот эти вероятности надо просто перемножить. Перемножить по всем и от единицы до м. Вот эти
вот штуковины. Вот эти штуковины надо перемножить. Так, друзья, поняли? Ну, слушайте, а тут еще есть для любого и.
Ну, я постепенно вот сюда двигаюсь, справа налево. Но опять, а по и они тоже независимы,
правда? Если я для вот этого и посчитал вероятность того, что он с каждой
сарделькой соединен, вот она, произведение, и потом возьму какой-то другой и и посчитаю
для него ту же самую вероятность, то дальше надо эти вероятности снова перемножить.
Согласны? В каком количестве? Надо вот так написать скобка, а тут n-a1- и так далее,
достаточно все успеваю? Это вот сколько вершин x находится вне подсардельщик,
сколько x-ов находится вот в этой разности. Каждая c и t имеет мощность а и t, то есть мощность
вот этого объединения это а1+, а2+, аm, а разность имеет мощность n-a1- и так далее,
может, я успею, но только чтобы понятно было, товарищи. Не, наверное, не успею. Ну ладно,
вот это оценить я успею. Смотрите, а и t, так, а и t, оно не больше вот этой величины.
Одна вторая в степени аи значит не меньше, чем если я вместо аитова напишу вот это. С
минусом снова не больше, а мне нужен знак именно в эту сторону, я хочу устремить к нулю вероятно.
Я пишу не больше, продолжаю вот здесь не больше. Произведение по i единицы до m,
1 минус 1 вторая, тут 1 минус эпсилон на лог двоичный. Так, ну до сюда понятно,
да, что произошло. И теперь мне еще надо вот эту штуку как-то оценить. n пополам. Тумма вот этих
аитов не больше, чем n пополам, но значит разность не меньше, чем n пополам, а поскольку она стоит в
показателе числа меньшего единицы, то значок неравенства в нужную сторону. То есть вот здесь
стоит n попал. Понятно откуда n пополам, да? Потому что каждая аито не больше, чем вот это, значит
надо вот это умножить на m, а мы вот это умножаем на m не больше, чем n пополам.
Так, ну смотрите, у нас от m сейчас вообще ничего не зависит, то есть надо просто в мэтую степень
возвести. Никакого произведения уже по сути нет. Надо вот эту штуку просто всю возвести в мэтую
степень. У нас получается вот так. Т1, я сразу вот это преобразую, 2 в степени лог двоичный,
это n. У меня получается 1 поделить на n в степени 1 минус эпсилон, а тут будет m не пополам. И это
я еще оценю стандартным образом как e в степени минус m не пополам умножить на 1 деленное на n в
степени 1 минус эпсилон. Стандартная оценка 1 минус p не больше, чем e в степени минус p. Сегодня
уже была. Так, ну давайте я еще это перепишу. Это e в степени минус m на n в степени эпсилон пополам.
Ну и давайте еще m как-нибудь внизу оценю. Ну это скажем больше либо равно, нежели n поделить на
4 лог двоичный. Целая часть оценивается половиной своего аргумента. Х не меньше, чем х попало.
Целая часть х не меньше, чем х попало. Тупо оцениваю. У меня получается вот это вот не больше,
чем e в степени минус. Так n в степени 1 плюс эпсилон поделить на 4. Логарифм двоичный.
Вот так. Слушайте, ну это компактненько. Чуть-чуть страшноватенько, но в общем не смертельно.
Согласны?
Нет, оно было на 2. А, еще на 2. Да-да-да, на 8. Правильно, да. Спасибо большое. На 8. Тут было 2,
вот тут было 2. И еще в оценке me у меня 4. Поэтому на 8. Ну это, конечно, никакой роли не играет,
но все-таки давайте. Да, аккуратно, поэтому следите. Спасибо большое. На 8. Это мы не все,
конечно, оценили. Знаете, откуда оценили? Мы оценили вот отсюда. Но нам еще надо
кванторосуществование как-то учесть. Так, дорогие друзья, вот я уже об этом говорил,
я еще раз, помедленнее это повторю. Вы понимаете или нет, что если появляется кванторосуществование,
то это фактически объединение. Или-или-или существует, это значит или-или-или. Существует
С1, это значит хотя бы одно С1 подойдет под вот это условие. То есть надо взять объединение по
всем циитам, которые обладают вот этими свойствами. Вот эта вот вероятность нечто,
она на самом деле не превосходит суммы по А1, суммы по Ам. Ну, естественно, таким,
что они не больше, чем, ну давайте я напишу, 2t не больше, чем 1-эпсилон налог двоичный. И дальше
будет сумма по С1, сумма по См. И здесь будет то, что циитая, пересеченная с циитам, пусто. Что
мощность каждого циитого равняется аитому. А вот тут будет вот эта вот ешечка в отрицательной
степени. Вот это вот вероятность А так оценивается. Так, сумма откуда взялась, но вероятность объединения не
больше, чем сумма вероятности. Непонятно насчет полинома, понимаете? Дело в том,
что каждая циита это выбирается сколькими способами. С из Н по аитому. Так, друзья, до сюда понятно,
что произошло. Все раз, вот эти кванторы существования, они под знаком вероятности
превратились в объединение. А вероятность объединения, она не больше, чем сумма вероятностей. Вот вероятности,
которые суммируются, мы оценили, они не превосходят отрицательные экспоненты. Но теперь они суммируются
по всем возможным способам выбрать вот эти числа А и Т и по всем возможным способам выбрать цииты
соответствующими свойствами. До сюда понятно? Понятно? Нет вопросов. Значит, я вынужден сейчас оборвать
свою речь. Осталось очень немного. Вы видите, что вот эта вот экспонента, она вообще-то от циитового
вообще никак не зависит. То есть, практически нам надо просто посчитать количество слагаемых и
умножить на эту экспоненту. Ну, чуть-чуть я не успеваю, мне надо бежать на защиту, на кандидатскую.
Давайте в следующий раз я минут за 10 это дожму. Запомните, что мы на этом остановились, вдруг я
что-то забуду. Еще раз повторю. Все, спасибо.

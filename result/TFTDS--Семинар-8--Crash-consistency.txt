Смотрите, сегодня семинар, которого, мне кажется, нам не хватало для полноты картины.
Мы в этом курсе говорим про распределенные отказоустойчивые системы. И отказоустойчивость,
с отказоустойчивостью мы справляемся по-разному, мы добиваемся ее по-разному. Смотрите,
с одной стороны, отказоустойчивость про то, что компьютер может умереть. Какая-то машина может
пропасть из нашего кластера, и система должна продолжить работу. Отказоустойчивость по модулю
таких проблем мы добивались с помощью алгоритмов консенсуса и хворумов. Хворумы позволяли нам
избавить пользователя от знания того, что машина сейчас умерла, от необходимости вообще ее
наличия в кластере в данный момент. Мы можем записать что-то на хворум, какие-то машины могут
быть недоступны, отрезаны партишеном или просто быть выключенными. То есть с крешем узла мы
справляемся. В будущем мы должны будем научиться справляться не только с крешами узлов, но еще и с
тем, что узел конкретный может лгать или даже некоторая группа узлов может лгать согласованно
другим. Это разговор про византийские отказы. Но это пока мы еще не проходили, пока мы про это ничего
не знаем и я думаю уже через две недели начнем этому учиться. Но у нас есть еще один вот
проблем, который связан с отказоустойчивостью. Это рестарты узлов. Рестарты узлов, а проблема
называется crash consistency. Как гарантировать, что наше состояние, которое мы храним видимо
персистентно на жестком диске, переживет этот рестарт? Мы с crash consistency должны обеспечивать,
мы с ней сталкивались в двух задачах. Во-первых, мы использовали levelDB, интерфейс базы данных,
более-менее levelDB и levelDB должна гарантировать, что если она подтвердила нам пут, то даже если
машина перезагрузится, то данные будут после этого путу доступны. И levelDB это обеспечивало
хранение данной на жестком диске в виде логов и sstable. То есть в памяти хранился memtable,
на memtable не страшно потерять, потому что по writeaheadlog его всегда можно восстановить. Любая
вставка проходит сначала через writeaheadlog, потом уже через memtable, потом подает memtable. Ну,
конечно, если мы в levelDB используем довольно сильные гарантии, там их можно ослабить,
но для наших целей важно, чтобы гарантии были сильными. Другой пример, где нам важна crash
consistency, это репликация автомата, это rsm. Вот у нас rsm может жить в памяти, но мы его храним в
виде лога команд на жестком диске. И если мы говорим про multiprocess пусть, то в текущей
домашней, вы в каждой ячейке этого лога храните состояние acceptor. Вот очень важно состояние
acceptor фиксировать на жестком диске. Если acceptor отвечает пропозеру, что он проголосовал за
предложение, проголосовал за команду для данного слота лога, то мы не имеем права ее потерять.
Вот сегодня вроде бы и лог на диске, и writeaheadlog, и sstable в lsm решают проблемы crash consistency. Если
узел перезагрузится, то мы данные не потеряем. Но очень наивно считать, что мы на самом деле
задачу решили, потому что внутри этой задачи спрятано очень много сложностей,
очень много хитростей, которые мы пока игнорируем. Вот наша задача сегодня разобраться, а как
именно обеспечить crash consistency, как именно работать с файловой системой. Чуть конкретнее мы хотим
выполнение двух свойств, двух букв из acid. Мы хотим свойства атомарности и хотим свойства надежности,
durability. Эти два свойства про разные. Они про crash consistency, но про разные. Durability — оно про то,
что если вы подтвердили запись данных, если вы подтвердили put в leveldb или если вы подтверждаете
промессом или сообщением accept пропозеру, что его команда успешно положилась в лог, то вы не имеете
права после рестарта машины об этом обещании забыть. Если вы подтвердили запись, то вы должны
гарантировать, что после рестарта вы сможете эту запись прочесть. Это durability. Вторая гарантия —
атомарность. Она про то, что происходит, если выключение питания вас застало в середине какой-то
операции, то есть в середине записи в лог, в середине пута. В этом случае мы должны гарантировать,
что наша запись либо целиком откатится, либо будет полностью применена после рестарта.
Мы не можем оставить лог или наш хранилищ в каком-то несогласованном состоянии. Вот две такие
задачи нас сегодня будут волновать. Нас сегодня будут волновать детали, как именно мы этого
достигаем. Вот давайте вспомним про... Давайте сначала начнем с RSM, которая нам ближе, которая в
домашней работе есть. В RSM у вас есть лог, в котором фиксируются все команды и голоса всех
acceptors. Лог нужен для того, чтобы в случае рестарта системы иметь возможность восстановиться и
продолжить работу с того места, где вы закончили, не потерять никакие обещания, не потерять
закоммиченные команды. Но с логом есть некоторая проблема, а именно вы не можете жить этим логом
вечно. Если вы читаете статью про multiplexus или про raft, то там предполагается, что лог растет
бесконечно. Но это предположение совсем не практично, потому что рано или поздно, во-первых, лог
переполнит диск, а во-вторых, длинный лог, даже если он пока помещается в диск, просто замедляет нам
восстановление реплики после рестарта узла. Мы должны, если бы, допустим, писали в лог неделю,
а потом перезагрузились, мы должны все недельные изменения, все недельные команды накатить на
состояние автомата. Вряд ли мы хотим делать именно так. Поэтому, когда мы говорим про персистентность,
про дюрабельность нашего RSM, мы должны думать о состоянии на диске не только как о логе,
мы должны лог периодически обрезать, его компактифицировать и закоммиченный и
примененный к автомату префикс лога заменять на снимок состояния автомата. Какой бы длинный
лог ни был, у него есть более компактное представление. Это просто мгновенное состояние
автомата, которое получается после применения к автомату в пустом состоянии всего префикса.
Если мы, допустим, храним какое-то дерево, например, дерево файловой системы, потому что мы метастор
распределенной файловой системы, то вместо того, чтобы хранить очень длинную историю пралок
этого дерева, мы можем просто сделать снимок состояния всего дерева, записать его на диск и
от лога команд отрезать некоторые закоммичные префиксы, которые соответствуют вот этому данному
снимку. Таким образом, у нас персистентное состояние RSM образовано суффиксом лога команд и снимком
состояния автомата для обрезанного префикса. Вот давайте подумаем, какие трудности у нас возникают,
когда мы начинаем хранить префикс лога в виде снимка состояния. Во-первых, нам нужно каким-то
образом от лога откусывать префикс. Звучит довольно просто, но, с другой стороны, не совсем понятно,
как именно мы собираемся откусывать префикс от файла на диске, потому что API файловой системы
нам такой возможности не предоставляет. Это, кстати, довольно странно, потому что в конце концов в
файловой системе файл хранится в iNode где-то в виде списка блоков, так что вполне можно было
бы лишние блоки из префикса отсечь. Но у нас есть только файл, у нас есть API файловой системы и
ничего мы с этим сделать уже не можем. Мы должны моделировать truncate каким-то образом
самостоятельно. Вот как мы с этим справляемся, понимаете ли вы? Такая операция нужна любому
RSM-у, будь это multiprocess, будь это raft, все равно префиксы откусывать нужно. Вот понимаем ли мы,
как это делать? Но вот для того, чтобы откусывать префикс, нужно хранить лог, видимо, не в одном
файле. Нужно использовать то, что называется сегментированный лог. Сегментированный лог — это лог,
который состоит из нескольких фрагментов, из нескольких файлов. Вот, скажем, в домашней
работе pro multiprocess вам дана реализация лога. Сейчас найду ее. Это интерфейс лога, у него есть
операция truncate. Мы это уже видим. И есть разные реализации. В частности, есть сегментированная
реализация, в которой лог представлен в виде набора файлов. Каждый сегмент в этой реализации
отвечает за некоторый диапазон индексов. И когда этому логу приходит операция truncate,
то, видимо, он должен просто пойти и лишние файлы удалить. То есть файлы, которые целиком
накрываются закоммиченным и примененным к автомату префиксом. Нет, я не туда смотрю, простите.
Это код сегмента, а мне нужен код самого лога. Вот, мы идем по сегментам, и если последний индекс,
за который отвечает сегмент, не больше, чем индекс, до которого мы хотим все удалить,
то мы удаляем сам файл с файловой системы. Ну вот, такая нехитрая идея, но почему-то
она не является совсем уж тривиальной, и мы чуть позже увидим, в чем там сложность.
Ну, это, как бы, одна половина задачи. Отрезать лог, наверное, понятно, что это можно сделать. Вот,
мы примерно так эту задачу решаем. А что делать со снимком состояния автомата?
В домашней работе мы предполагаем, что все очень тривиально. У нас есть интерфейс
State Machine, и в нем есть метод, который пишет снимок в виде какого-то байта.
Вот кажется, что так делать не практично, если мы говорим про промышленную реализацию,
потому что ваш автомат, например, дерево-файловая система, может быть огромным. Этому какие-то
десятки-сотни гигабайт в памяти. И мы, конечно, не можем себе позволить на лидере RSM остановиться
на некоторое время и ждать, пока все это дерево запишется из памяти на диск. Как тут можно поступить?
Ну, наверное, можно было бы скопировать его в памяти сначала, а потом уже в отдельном потоке
писать на диск, чтобы не мешать конкурирующим апдейтам этого дерева, чтобы не мешать автомату
работать, реплике RSM работать. Конечно, нам уже нужен снимок, который бы согласованный,
если мы берем, вот фиксируем состояние автомата для некоторого префикса command,
префикса log command, и вот ровно это состояние пишем на диск. Мы не можем позволить себе,
чтобы какие-то части дерева соответствовали разным языкомичным префиксам лога.
Так вот, останавливать автомат на время записи на диск мы не хотим, разумеется,
и мы даже не можем позволить себе останавливать автомат для того, чтобы скопировать все его
состояние в памяти, потому что это в два раза больше памяти, а памяти у нас может столько не
быть, потому что и так RSM, который хранит дерево файловой системы, это уже узкое место в дизайне
файловой системы. Как можно было бы сделать эффективнее, как можно было бы сделать более ловко,
как можно сделать мгновенный снимок большого состояния сложной структуры данных и при этом
не останавливать работу реплики RSM. Вот для решения такой задачи есть такой, есть академический
способ, он называется персистентная структура данных. Допустим, вы хотите поддерживать дерево,
его редактировать, добавлять к него некоторые узлы, добавлять к него новые узлы и при этом вы
хотите получать мгновенное состояние всего дерева. Для этого используется механизм Copy on Write. Вот
допустим, у вас есть какое-то дерево и вы хотите к нему вот к узлу F прицепить узел E. Вы хотите
к какой-то директории прицепить новый лист, новый файл. Вот вы его, конечно, создаете в памяти,
а дальше из директории ставите на него ссылку. Рождается новая директория. Это директория,
ну в смысле новая версия старой директории. Но на эту новую версию старой директории должна
ссылаться новая версия родительской директории, ну и вот так вплоть до корня. То есть мы меняем
какую-то часть дерева в нашей операции, в данном случае мы цепляем лист и вместе с этим листом
меняется путь к этому листу в дереве. И мы это честно копируем. Мы создаем новые версии этих листов,
штрихованные на этой рисунке. Но при этом все остальные узлы дерева остаются неизменными. Они
между старой версией дерева и новой версией дерева общие. Их можно переиспользовать. Поэтому
новый путь мы выстраиваем в памяти из новых узлов, путь мы устраиваем из новых узлов и ставим
ссылки на старые вершины, которые не поменялись. Вот под дерево B оно используется и в старой версии,
и в новой версии, и можно его разделить между двумя этими версиями. Но это такой академический
способ. То есть таким образом можно снэпшотить состояние, скажем, дерева. И для нас снэпшотить
дерево довольно полезно. Но все же нам требуется таким образом написать весь наш код в виде вот
таких вот персистентных структур данных. Тогда бы мы могли делать снэпшоты, не блокируя работу
автоматом, не блокируя работу реплики RSA. Но все же мы пишем какой-то промышленный сложный код,
и в нем может быть, могут быть не только деревья, могут быть, не знаю, все что вы в коде можете
написать. Очень сложное и разнообразное состояние. И делать его целиком из персистентных структур
данных, это довольно сложно, довольно громоздко. Но оказывается, что можно добиться примерно
такого же эффекта, такого же механизма, никаким образом не меняя весь наш остальной код, весь
код и все состояние, которое у нас уже есть. Вот идея за персистентностью на этом рисунке,
это идея Copy-on-Write. Мы копируем то, что мы собираемся перезаписать. Но механизм Copy-on-Write
за нас уже реализован в самой операционной системе, в которой исполняется наш код. Этот
механизм Copy-on-Write реализован в механизме виртуальной памяти. У вас есть такой волшебный
системный вызов, системный вызов Fork. Системный вызов Fork порождает дочерний процесс,
который получает копию памяти родительского процесса. Но копия, она ленивая. То есть не то,
чтобы вы копируете все состояние. Мы, собственно, от этого и хотим уйти. Мы не хотим копировать все
состояние в памяти, потому что это большая пауза. Вместо этого механизм Copy-on-Write в виртуальной
памяти, когда вы Fork-ите процесс, копируют только таблицу страниц. То есть теперь родительский
процесс и дочерний ссылаются на одни и те же страницы памяти. Но при этом в обеих процессах
эти страницы помечены как ридонли. То есть программа не может их перезаписывать. Вернее,
может, конечно, но это ей необходимо делать, потому что автомат хочет продолжить работу.
Всем я говорю автомат. Реплика RSM. Но когда реплика RSM будет в этот автомат что-то писать,
то поскольку эта запись в ридонли страницу будет случаться, будет происходить прерывание,
управление будет переходить в операционной системе, и операционная система уже по мере
необходимости будет копировать старые ридонли страницы в новые страницы, в которые уже можно
писать. Собственно, механизм Copy-on-Write. То есть мы копируем при Fork-е только таблицу
страницы, дальше уже по мере необходимости копируем сами страницы памяти. Как это все относится
к нашей задаче, а именно к задаче реализации, эффективной реализации метода, куда он от нас
уехал. Метода Get Snapshot автомата. Make Snapshot. Теперь, если мы хотим сделать снапшот, мы Fork-ем
процесс, после чего в дочернем процессе мы спокойно обходим все состояние автомата и пишем его
на диск. Почему мы там можем так сделать? Потому что вот там никакой конкуренции уже не будет. Там
не будет конкурирующих апдейтов, потому что Fork обладает очень странным свойством, а именно Fork
копирует память, но теряет все потоки, которые были в родителе. Если вы Fork-ите процесс, то в
дочернем процессе, но, конечно, у вас в вашем родительском процессе, который являлся репликой
RSM, было очень много потоков. Ну ладно, какое это количество потоков? То в дочернем процессе у вас
останется только один поток. Тот поток, который непосредственно вызывал Fork. Поэтому он может в
одиночестве спокойно обойти текущее состояние структуры данных и сдамить ее на диск. Это,
конечно же, не бесплатная операция, потому что и у ребенка непонятно в какой степени, тут уже как
код напишете, но в родителе реплика RSM продолжает работать. И это означает, что она делает много
записей в память. И это означает, что там случается много першфалтов и много копирований
страниц. И это означает, что все-таки родитель у нас тормозит немножко. Но, по крайней мере,
в нем нет гигантской паузы. Да, он работает хуже, он работает медленнее, а пользователи это замечают.
Но большой паузы у нас все же нет. Ну и нам не потребовалось никакой сложной работы для того,
чтобы научиться снапшотить более-менее произвольное состояние в памяти, как бы оно не было организован.
Вот так делает, кажется, любая эффективная реализация RSM. Но, конечно, это еще не все. Это
не все, что нужно сделать для снапшотов, потому что у вас есть некоторая очень нелепая,
но, тем не менее, трудность. Вот смотрите, вы делаете форк для того, чтобы в потомке
обойти структуру данных в памяти и записать ее на диск. Вы так делаете, потому что все остальные
потоки не выжили. Но эти потоки, они, конечно, потерялись, но, с другой стороны, остались
эффекты, их воздействие на память сохранилось, потому что память вы скопировали. А вот теперь
представим, что вы какой-то поток, вы родители делали форк, а другой поток в вашем же процессе
сейчас алоцировал память. Разумеется, такое могло происходить. Так вот, этот поток пришел в
алокатор. Алокатор, разумеется, у вас, скорее всего, алокатор у вас общий, у вашего потока и у того
потока, который занимается какой-то другой работой сейчас. Алокатор — это разделяемый сервис. Да,
алокаторы современные, хорошие алокаторы шардированы. То есть, как правило, между потоками,
которые алоцируют память, синхронизация не требуется, по крайней мере, на быстром пути уж точно.
Но все же у алокатора есть некоторое общее состояние, какие-то общие арены, общие чанки памяти,
которые он запрашивает у операционной системы. И если долго спускаться вглубь алокатора, то
рано или поздно вы встретите, спустя все кэши для быстрого доступа, все это локальные кэши,
рано или поздно вы встретите какие-то мютоксы. А теперь возникает следующая проблема. Вы форкаетесь,
а другой поток в это время находится внутри алокатора и взял там блокировку. И в результате,
после форка в дочернем процессе у вас поток, который был в критической секции в этой блокировке,
в алокаторе исчезнет, а вот блокировка сама сохранится, потому что в конце концов это
какой-то битик в памяти. В итоге алокатор сейчас залочен, вы к нему обращаться не можете в
дочернем процессе, а это значит, что вы вряд ли сможете сделать что-то полезное. Вот такая неприятность.
И эта проблема не то чтобы надуманная, это проблема, которая касается более-менее любого
алокатора. Вот потоки и форки — это вещи не слишком ортогональные, это две совершенно разные фичи,
которые плохо сочетаются друг с другом. Поэтому алокатор, хороший алокатор, ну скажем, jmalloc,
такой стандартный, хороший алокатор для C++. Вот там эту проблему требуется решать,
она решена. И она решается, ну я бы сказал, что некоторым костырем у вас есть такой библиотечный
вызов, который называется Petroid at fork. Ну 3D — это библиотека, поэтому вызов библиотечный,
это не системный вызов. Он позволяет вам зарегистрировать для форка три обработчика,
три кулбека. Первый будет вызван перед форком, а два других будут вызваны после форка, один
соответственно в родителе, другой в ребенке. Как можно этот пистолет, этот форк использовать?
Ну посмотрим на jmalloc. В нем этот вызов должен быть использован, потому что jmalloc может быть
использован в программе с форком. Ну вот, мы регистрируем там какие-то обработчики, и что эти
обработчики собираются делать? Смотрите, с одной стороны это костырь, вот глобально,
все это костырь, а с другой стороны довольно остроумно. Если мы делаем форк, то мы сначала
берем все мютоксы в нашем локаторе. То есть, грубо говоря, мы перед форком дожидаемся,
что все другие потоки, которые с этими мютоксами работают, отпустят эти мютоксы, и мы эти мютоксы
залочим. После этого мы сделаем форк, и после форка в дочернем процессе мы будем уверены,
что сейчас все мютоксы локаторы взяты и взяты нами. И после этого мы можем, в смысле контролируемо
захваченные, и после этого можно в дочернем процессе их обойти и отпустить. Вот мы отпускаем все
эти мютоксы. Вот такая, не знаю, то ли остроумная, то ли какая-то... Но мне кажется, что в этом есть
что-то изящное. Но, тем не менее, если вы пишете RSM и если вы хотите делать снепшоты эффективно,
то вам нужен форк, а если вам нужен форк, то вам нужно каким-то образом гарантировать,
что ваша синхронизация и потоки с этим форком будут совместимы, чтобы вы не оставили автомат
заблокированным навсегда. Автомат и локатор, разумеется. Ну вот, таким образом можно делать
снимать состояние и лог обрезать с помощью вот таких транкейтов, с помощью транкейта сегментов.
Ну и, конечно, все это будет задевать реализацию рафта или реализацию паксиса, который мы пишем.
Вот давайте...
Слишком много ссылок.
Если вы возьмете писать снепшоты в реализации мультипаксиса, то вас ждет не то чтобы проблема,
но некоторая дополнительная трудность на уровне самого консенсуса, потому что в консенсусе у вас
общаются пропозеры и аксепторы или лидеры фолловер, и если вдруг вы, аксептор, получаете...
если, точнее, не вдруг, а если просто вы, аксептор, получаете какую-то команду от пропозера,
то вы смотрите соответствующий слот лога и отвечаете ему. Но может быть так, что вы аксептор,
вы реплика, и вы уже у себя обрезали префикс лога, вы заменили его снепшотом,
а вам приходит какой-то пропоз или препер для этого отрезанного префикса, и вы уже не можете
ничего ответить. Ровно поэтому в ваш протокол вмешиваются вот такие служебные команды.
Вот, пожалуйста, в RAFT-е лидер отправляет Append Entries, отправляет репликам какие-то команды,
но если так оказывается, что реплика сильно отстала от лидера, то лидер уже не может
отправить ей префикс лога, он отправляет ей снепшот. Или в Multipax все то же самое,
может быть вы, аксептор, получили препер для старого слота, у вас его уже нет, и вы отвечаете
ему не промессом, вы отвечаете ему снепшотом, который этот пропозер должен на свои реплики
применить, установить для того, чтобы нагнать отставание. Собственно, реплика, которая отстала,
скажем, на час от других, она не будет, конечно, нагонять весь лог, она просто догонит состояние
других реплик, установив снепшот. Хорошо, вот это все, допустим, понятно, что это все можно сделать,
но а теперь поговорим, как... Мы сейчас к этому моменту разобрали, какие данные нужно хранить
на репликах, на диске. В случае RSM это снепшот и лог, в случае LSM это right-ahead-log и sustainable.
А теперь следующий шаг, разобраться как с файловой системой, это все совместимо, как мы с файловой
системой работаем, как мы сохраняем эти все данные на диск, как мы что-то пишем в лог. Задача
вообще-то не простая. Вот в общем случае ее можно поставить так, у вас есть некоторое сложное
состояние, которое лежит на диске, но оно может быть представлено совершенно по-разному, но в
общем случае у вас какой-то набор файлов, что у вас еще может быть в файловой системе, и вы в этих
файлах что-то меняете. Может быть вы дописываете в конец, а может быть вы прямо внутри файла что-то
перезаписываете, скажем вы пишете B дерево, или у вас Google File System и вы перезаписываете часть
вашего чанка. Можно представить совершенно разные варианты, ну вот давайте посмотрим на такую вроде
бы простую подзадачу. Вот она решается точно в базах данных. У вас есть файл, и вы должны
зафиксировать в этом файле какие-то свои изменения, перезаписать какой-то его фрагмент. Вот задача,
которую мы сейчас разберем, выглядит так. Как перезаписать фрагмент файла? Задача очень простая,
но выглядит просто. У вас для перезаписи фрагмента файла, например, вы реплика chunk
server GFS, хотите перезаписать какой-то кусочек, который вам прислал клиент. GFS это умеет. У вас
есть вызов per write. У вас есть открытый файловый дескриптор, у вас есть буфер, и вы хотите
записать из него count byte по вот такому оффсету файла. Ну и отлично, начинаем файл писать.
Давайте я буду писать некоторые псевдокод. Эта суть не поменяет совсем. Я хочу записать в
файл в директории D по оффсету 2, 3 символа как будто бы. Вот такие вот. В файле при этом изначально
написано фу. Ну точнее написано что-то, потом фу, потом что-то. Вот я это хочу, это фу хочу
перезаписать на бар. Вот давайте подумаем в терминах отомарности и в терминах дюрабельности,
надежности. Соблюдаются ли здесь два этих свойства? Да, маленькое техническое замечание, конечно,
когда я говорю про запись бар или перезапись фу, то здесь я не думаю про отдельные символы,
а скорее это блоки. Потому что для блочного устройства, над которым работает файловая система,
единицей чтения записи является блок. Вот у меня здесь три блока, которые я условно обозначил как
вот три буквы. И здесь тоже я перезаписываю три блока, а не просто три символа. Вот я должен,
я собираюсь эти три блока перезаписать. Вот сначала поговорим про надежность. Верно ли,
что если у меня завершится этот самый вызов write, то после этого я буду уверен, что я могу
спокойно перезагрузиться, в смысле моя машина может перезагрузиться, и я данные не потеряю. Вот,
к сожалению, это не так. Сам вызов write, ну никакой вызов write файловую систему не гарантирует,
что данные достигли диска. Потому что, когда вы пишете что-то файловую систему, вы правило
пишете в кэш-операционной системы. Ваши записи пока оседают в оперативной памяти,
и для того, чтобы гарантировать, что эти данные были сброшены на диск, вам нужно сделать отдельный
специальный вызов, который называется fsync. Вот он сбрасывает все ваши записи, которые накопились
в кэш-операционной системы на устройство. То есть fsync – это тот способ, с помощью которого в
файловых системах обеспечивается дюрабленность. Вы должны сделать fsync на файле. Вот вроде бы это
обеспечивает нам дюрабилити. Но у нас есть второе свойство. Второе свойство – это атомарность
записи файл. И тут уже сложнее. Вот мы говорим про атомарность записи вот сразу трех блоков. Мы
перезаписываем сразу три блока. Гарантирует ли нам файловая система атомарность? Ну, нужно
изучать документацию. К сожалению, файловая система – это такое место, где документация не очень
строгая. И, как правило, сложно понять, какие именно гарантии та или иная файловая система дает.
Вот некоторое время назад люди написали даже целую статью про ток. Они взяли файловые системы и
исследовали, какими же свойствами эти системы обладают. Что они позволяют делать атомарно,
а что не позволяют. И, скажем, файловые системы, ну, кажется, что все, позволяют вам атомарно
перезаписать очень небольшую порцию данных, именно один сектор на блоке устройстве. Ну да,
я говорил про блоки. Диск работает все-таки не с блоками, а работает с секторами. Они могут
быть меньше. Но вот перезапись одного сектора – она точно всегда атомарная, потому что такими
порциями пишет жесткий диск. Так он устроен. А вот, скажем, append одного сектора к файлу уже не
атомарен. Хотя казалось бы, тот же самый объем данных. Запись нового сектора действительно атомарна.
Но если вы хотите добавить что-то в файл, то вы должны, по крайней мере, увеличить его длину. А
это запись в структуре iNode файла. А iNode – это часть методанных. А методанные и данные хранятся
вообще-то в файловой системе в разных местах. Это разные блоки, разные секторы. Поэтому append
от файловой системы требует двух записей в разные места, и поэтому атомарность вы здесь,
по умолчанию, теряете. Но если мы говорим про перезапись сразу нескольких блоков, то тут можно
не волноваться. В любой файловой системе такие операции будут не атомарными. Вот здесь крестик – это
значит не атомарность. Так что мы должны каким-то образом, чтобы гарантировать crash consistency,
мы должны позаботиться об атомарности перезаписи вот этого фрагмента. Каким образом мы это сделаем?
Вот есть такой стандартный способ, который мы используем в реализации Logo RSM в текущей
домашней системе и который использует любая база данных и использует levelDB. В общем,
нам нужно залогировать наши действия. Мы не можем прямо писать bar, потому что если мы перезагрузимся
в середине этой записи, то мы можем получить файл, в котором будет написано не фу, не bar,
а boo, или far, или что-нибудь еще. Все эти варианты окажутся возможными. Так что если это случится,
такое может случиться. Мы должны уметь видимо откатить наши неполноценные или откатить то,
что мы не дописали, либо накатить то, что мы собирались написать. Ну вот давайте мы для
разнообразия откатывать научимся. Собственно, это вопрос про транзакции, который сегодня был на
реакции. Что делать, если мы хотим транзакцию откатить? Нам нужно уметь поддерживать андулог.
То есть мы создадим новый файл перед тем, как перезаписывать существующий. И в этот файл
запишем такую служебную запись, что мы собираемся в нашем файле файл перезаписать софтсеты 2,
3 блока, в которых раньше было написано foo. Вот мы поместили туда такую служебную запись.
После этого сделали запись файл, потом сделали fsync, а потом сделали unlink для лога. Он нам больше
не нужен. Будет ли это работать? Ну, как сказать? На самом деле мы... То есть у нас раньше была
проблема с неотомарностью вот этой записи, а теперь у нас другая проблема с неотомарностью уже
этой записи. То есть мы, казалось бы, передвинули проблему просто в другое место. Но теперь нам
легче ее решить. Вот если мы записали все в лог и поломались где-то на этом этапе и
получили в файле boo, то нам не сложно это понять, потому что после рестарта мы увидим,
что наш лог еще находится в файловой системе. Видимо, сама запись была неуспешна и нужно
ее откатить. Для этого мы перечитаем лог. Мы, видимо, ожидаем, что в нем написаны все байты,
которые были исходно в файле. Байты foo. Но что если мы сам лог не дописали? Точнее,
мы перезагрузились в тот момент, когда мы записывали в лог эту служебную запись. И в итоге
файл есть, файл с логом есть, но в нем написано такое. Какой-то мусор вместо наших блоков. Почему
такое могло случиться? Потому что запись файл требует изменения, то есть записи блоков с данными и
изменения iNode, то есть изменения длины файла. И пусть файл стал нужной длины уже, то есть там
действительно есть заголовок и есть вот три блока с данными. Но сами эти блоки с данными,
они успели только лоцироваться, но еще не успели перезаписаться, поэтому в них какой-то мусор.
И в итоге мы, если мы перезагрузились в середине этого райта, то он не отомарен и мы можем из этого
лога откатить нашу запись bar, заменив ее на мусор. Ну то есть раз уж запись bar не начался,
не началось еще, то в файле все еще фу, а мы перезапишем этот фу на мусор. Очень неприятная
ситуация. Нужно как-то ее починить. Ну вот для этого тоже есть универсальное решение. Мы должны
убедиться, что наш лог вообще-то находится в целостном состоянии. То есть вот эта запись в логе,
она действительно валидная. Поэтому мы к такой служебной записи добавляем чексумму.
То есть мы считаем чексумму от записанных дан, от данных, которые мы собираемся поместить в
анду лог и эту чексумму обязательно добавляем в запись в этом логе. Если вдруг запись в лог
покарабтилась, то есть она не была завершена, то видимо, когда мы будем читать лог, то мы увидим,
что чексумма, вычисленная по данным, не сходится с чексуммой, которая записана в заголовке этой
записи. И после этого, видимо, поймем, что запись в лог не была успешна. То есть мы даже не смогли
записать старое состояние файла. Это значит, что мы, видимо, еще не приступили к перезаписи самого
файла. Так что ничего делать не нужно. Вот чексумма, это на самом деле безумно важная и такая
фундаментальная идея для корректности систем самого разного уровня в самых разных контекстах.
Мы говорили с вами про LevelDB и RocksDB, про хранилища локальные, и вот RocksDB в своей статье последней
описывает, я кидал ее к нам в чат, говорит, что они используют чексуммы вот буквально для всего,
то есть они используют чексуммы для отдельных блоков в Isostable, они используют чексуммы для
самих Isostable, потому что в случае, когда там, не знаю, нужно запекапить данные, они копируют сами
Isostable, они не то что там делают те же самые путы, они просто берут представление LSM на диске,
представление RocksDB на диске, то есть сами Isostable и копируют их на другие машины. И нужно,
чтобы на другой машине потом можно было чексумму проверить. Ну вообще, стоит об этом рассуждать так.
Вот у нас есть компьютеры, и компьютеры нам нужны для двух вещей, чтобы что-то вычислять и чтобы
что-то хранить. И вот мы можем хранить в компьютере что-то на диске, можем хранить что-то в памяти,
и мы можем, ну так, условно хранить в проводах, то есть мы отправляем с одной стороны конца
с одного конца провода и читаем из другого конца провода. И вот эти три вида хранилища,
они работают с нашими машинными словами, с битами, с байтами, ну короче, с какими-то
вот дискретными единицами информации. При этом, понятно дело, в физическом мире ничего дискретного
нет. У нас нет никакого представления для нуля и для единицы вот такого вот четкого. У нас,
не знаю, какие-то заряды хранятся. В итоге, чему это может привести? К тому, что вы с одной стороны
записали в провод одни данные, а получили другие. Или вы записали что-то на диск, а на диске
покарабчивался сектор. Или вы записали что-то в память, а потом читаете, и оказывается,
что вы прочли не то, что, точнее не то, что бы оказывается, вы как раз об этом не знаете. Но
память внутри себя данные испортила. Вот вы от любого хранилища, будь то файя, будь то память,
будь то диск или будь то сеть, ожидаете, что это хранилище реализует тождественную функцию. Вы
что-то туда записали, а потом что-то прочитали, и это будет то же самое. Но в силу непрерывности
физического мира такое обеспечение всегда удается. В случае с диском, наверное, понятно,
в случае с сетью я прокомментирую сейчас. Но в случае с памятью это может выглядеть довольно
неожиданно, потому что в памяти мы храним, не знаю, какие-то указатели, и если в них
будут флипаться биты, то совершенно непонятно, чего мы от нашей системы ожидаем. В смысле от
нашего компьютера вообще, как он будет работать. Так вот, с памятью все как раз более-менее хорошо,
потому что, во-первых, проблемы случаются. Сейчас я на этой статье про RopsDB. Во-первых,
проблемы случаются, то есть действительно память может переворачивать биты. Вот буквально эта
статья довольно старая уже девятого года от Google про исследование ошибок в планках памяти,
которые возникают в их кластерах. Ну и ответ такой проблемы возникают. Вот,
проблемы возникают, но их существенно больше нуля, но, к счастью, они не оказывают прямого
эффекта на наши программы, потому что планки памяти, которые используются в датацентрах,
они надежны, в том смысле, что внутри них реализованы коды коллекции ошибок. Вот такие
планки способны исправить перевернутый бит в машинном слове. А почему он вообще перевернулся?
Ну причины могут быть разными. Ну, скажем, возможно такая, что из космоса прилетел луч и
зарядил какой-то маленький конденсатор у нас в памяти. Вот это вполне реальная проблема. Какие-нибудь
там альфа-частицы, вот все они могут переворачивать наши биты. И это случается не то, чтобы часто,
но это вообще в принципе случается. Вот в одной планке памяти можно за год насчитать
четыре тысячи корректируемых ошибок. Может быть не очень много, смотря относительно того,
с какой интенсивностью с этой планкой работают, но любой перевернутый бит может очень сильно
навредить, он может иметь такой каскадный эффект. В итоге хорошо бы после каждой записи куда угодно
проверять, что вы читаете после этого те данные, которые вы записали. В памяти, планка памяти
делает это за нас. С диском за нас это не делают. В случае с диском, в случае с диском мы должны
сами использовать чексуммы. И не просто один раз записать на диск, а потом при чтении чексумму
проверять. А лучше вообще периодически просто в фоне перечитывать данные и проверять, что они
все еще целы, то есть они сходятся с чексуммой, которую мы ожидаем. Скажем, вы пишете чанк сервера,
который хранит чанки для Google File System. Вот к вам пришли, записали чанк, вы положили его себе на
диск. А потом спустя год вы его читаете, оказывается что чексумма не сходится. А вообще-то мастер считал,
что вы храните реплику, а вы на самом деле уже не являетесь репликой. Поэтому разумно в фоне
перечитывать ваши локальные чанки, и если вдруг вы увидите, что какие-то из них попортились,
у них не сходятся чексуммы, то вы должны отправить информацию об этом мастеру, чтобы он читал,
что вы больше не реплика чанка и дореплицировал этот чанк на другие машины.
Значит, с памятью за нас проблему решают сами планки памяти с помощью кодокоррекции
ошибок. В случае с файловыми системами мы чексуммим все самостоятельно. Ну и кстати, раз уж мы недалеко,
то write ahead логи, которые мы используем в нашем фреймворке, они конечно же работают с чексуммой.
Когда мы пишем что-то на диск, то мы сначала пишем для любой записи заголовок, и в этой записи
есть чексумма, которую мы ожидаем от данных. А кроме того, у каждого заголовка есть еще и
некоторые magic number. Давайте я покажу структуру хеддер. Некоторые magic number просто для того,
чтобы понимать, что те байты, которые мы читаем с самого начала write ahead логи, это вообще-то
байты заголовка или просто мусор. А то мы можем прочесть заголовок, потому что у нас есть нужное
количество байт, но заголовком он конечно не является при этом. Скорее всего, там чексумма дальше
разойдется, но все же мы можем быстро понять, что перед нами даже не заголовок. С файлами и
с памятью мы более-менее умеем безопасно работать, но нам нужно работать безопасно и с сетью.
И работать с сетью, казалось бы, тут уж точно все безопасно, потому что когда мы отправляем,
скажем, сообщение с РПС-вызовом, служебное сообщение, в котором описано, какой метод мы
вызываем на комсервисе, с какими аргументами, то, казалось бы, вот это сообщение на нашем уровне,
на уровне пользователя упаковывается сначала в сегмент DCP, у которого есть чексумма. Этот сегмент
DCP упаковывается в IP-дотограмму для роутинга запроса, для роутинга нашего пакета. И
эта IP-дотограмма, в свою очередь, по проводам двигается в виде Ethernet-фрейма. То есть у вас
есть разные уровни с текстовых протоколов, и на каждом уровне у вас есть свои собственные
заголовки, и на каждом уровне у нас есть чексумма. Она есть в Ethernet, она есть в протоколе IP,
вот здесь вот, сейчас, нет, вот она, и она есть в протоколе DCP. Но при этом оказывается, что этого
недостаточно. Это очень странно, что этого недостаточно, но, тем не менее, этого оказывается
недостаточно. И вот, скажем, инженер Twitter советует нам, что если вы пишете свой собственный протокол,
который двигает данные по сети, поверх DCP, вы все равно должны на уровне своего приложения,
на уровне своего протокола уже прикладного, все равно использовать чексумму, несмотря на то,
что на трех уровнях под вами они есть. И вот в нашем RPC-фреймворке тоже есть чексуммы, и сейчас я
попытаюсь объяснить, почему же они требуются, почему всего того, что уже есть в сетевых протоколах,
оказывается недостаточно. Вот если мы отправляем сообщения, смотрим на транспортный канал,
смотрим на...кажется, метод назывался...секундочку...это мы получаем реквест, вот
отправляем респонс. И вот мы формируем пакет, реквест, содержимое...содержим нашего вызова,
и высчитываю чексумму по данным и всем...ладно, я почему-то не хочу приходить к предзрению...высчитываем
чексумму...давайте так покажу вручную...по всем полям, которые в этом реквесте есть.
Почему же это необходимо? Ну, давайте начнем из глубины. Вот у нас есть на самой глубине протокол
Ethernet, и на этом уровне работают очень сильные, очень мощные чексуммы CRC 32-битный, и не так-то
просто его поломать. Но проблема очень странная. Сейчас найду ссылку про это, к сожалению их...и их
много. Так вот, CRC само по себе надежная, то есть чексумма в Ethernet frame их надежные, но беда в том,
что когда какая-то сетевая коробка получает frame и отправляет его дальше по маршруту, то она
внутри не то чтобы сохраняет frame в неизменном состоянии, она в нем может что-то поменять и
пересчитать чексумму. То есть, короче говоря, какие-то свечи могут суммы пересчитывать,
а это означает, что эти чексуммы защищают нас от ошибок, которые возникают именно в проводах,
но они не защищают нас от ошибок, которые происходят в самих свечах. Но они бывают
ломаются, в них бывают баги, в них бывает, ну, не знаю, что-то идет не так. Они могут корраптить
пакет, и в итоге коробка получает пакет по сети, внутри этой коробки он каким-то образом портится,
в нем портится данные вашей там IP-дотограммы, вашего TCP-сегмента, а после этого перед отправкой
коробка пересчитывает чексумму по уже испорченным данным, и дальше эта чексумма с этими испорченными
данными отправляется до следующей коробки. И, разумеется, вот такую ошибку никто уже не обнаружит.
Так что такие ошибки могут возникать. Ну хорошо, если даже у нас покораптились данные, то в конце концов
у нас есть протокол IP уровнем выше, и вот он-то заметит. Протокол IP не заметит, потому что в
протоколе IP-чексуммы рассчитывается только по полям заголовка. То есть протокол IP не
пытается обеспечивать целостность данных, которую он двигает. Он про маршрутизацию,
он не про транспорт. Ну ладно, IP нам не поможет, интернет тоже может не помочь, но у нас же есть TCP
с чексуммой. Так вот, оказывается, что и на TCP тоже надежды нет, потому что чексумма в TCP
безумно слабая. Чексумма в TCP это всего лишь, ну с некоторыми оговорками, это сумма 16 битных
фрагментов данных. Вот просто сумма. И если вдруг в двух вот таких вот блоках по 16 бит есть два
перевернутых бита в одних и тех же позициях, то чексумма такого TCP сегмента не поменяется.
То есть если у вас всего лишь перевернулись два бита в одних и тех же аффсетах, то чексумма
TCP ошибки не заметит. Стэк TCP ошибки не заметит. IP вообще это проверять не собирается, в чексумме
только заголовок. А в интернете просто ошибка была пригналирована, потому что она была допущена
в какой-то коробке, в каком-то свече и просто чексумма была пересчитана. Вот абсолютно безумная
история, но тем не менее эти проблемы в реальности могут возникать, поэтому любой RPC framework,
любой протокол коммуникации, который вы реализуете на прикладном уровне, должен чексумме данные
самостоятельно. Ну скажем об этом пишу даже в статье Pro Bigtable. Вот у них есть там где-то пункт
про, сейчас найду, выученные уроки. Ну вот, урок, который они извлекли, что у них чексумма есть
и в RPC тоже, потому что стандартных сетевых протоколов оказывается недостаточно. Абсолютно
странно, абсолютно жуткая история, если вы инженерно, то есть очень трудно на что-то полагаться. Так вот,
полагаться нужно только на себя. Вот есть такой принцип в дизайне. Он про то, что вам нужно
обеспечивать надежность передачи, вам нужно обеспечивать целостность ваших данных при передаче
с одного компьютера, допустим, на другой или при работе с файловой системой. Так вот, между вашим
компьютером и другим находится, ну, невероятное количество сетевого оборудования по пути и
невероятное количество протоколов под капотом. Вот такой целый стэк сложный. И вы, конечно,
можете надеяться, что на каждом уровне и на каждом переходе внутри все будет корректно и все будет
проверено. Но гораздо надежнее просто делать проверки на концах, в двух конечных точках
коммуникации. То есть, как бы не были надежные сетевые протоколы, можно об этом просто не думать,
и стоит об этом не думать для надежности, и проверять чек-сумы, писать чек-суму при отправке
и проверять при получении. Писать чек-суму при записи на диск и проверять ее после чтения.
Пусть лучше, пусть каждый, пусть эти чек-сумы будут в некотором смысле избыточны, но в
худшем случае вы получаете дополнительную безопасность для своих вариантов. Ну хорошо,
вот значит с чек-сумами мы разобрались. Мы можем, вернемся к нашему примеру, записать ванду лог
вот такую вот чек-суму, записать ванду лог запись с чек-сумой, и если вдруг после рестарта мы видим,
что лог-файл все еще не удален, то мы можем пойти в этот лог-файл, вычитать из него данные,
проверить, что чек-сума сходится, и если чек-сума сходится, то откатить запись. А если чек-сума не
сходится, значит этой записи не было. Ну как будто бы вот такой протокол уже безопасен, но на самом
деле тоже нет. На самом деле после выполнения такого протокола вы можете закончить с файловой
системой, в которой в файле написано, скажем, boo, а в логе написано 2-3 чек-сум, а дальше снова мусор.
То есть вы и откатить ничего не можете, и то есть ваша запись частично применилась,
но откатить вы ее не можете, потому что у вас и лог покарабчен. Почему такое могло быть?
Тут на самом деле ситуация еще печальней, потому что когда вы думаете про файловые системы,
вы на самом деле можете о них думать как про, ну в каком-то смысле модели памяти. В моделях
памяти у нас были две проблемы – атомарность и упорядочивание операций. Так вот с файловыми
системами то же самое. Вы должны думать и про атомарность, и про упорядочивание. Вот в
определенных режимах нет гарантий, что записи в этот лог и записи в файл будут упорядочены
друг относительно друга. Может быть вы успеете записать какие-то блоки из этой записи и какие-то
блоки из этой, но не все. То есть то, что вы увидели хотя бы один блок из этой записи, не означает,
что выполнилась запись предыдущей. Вам нужно каким-то образом явно упорядочить две эти записи.
И для этого, для того чтобы это сделать, ну в моделях памяти у нас были специальные инструкции
барьеры. Здесь у нас есть тот же самый fsync. Мы должны добавить еще вот такой вызов,
который гарантирует, что запись в лог будет упорядочена до перезаписи блока в файле.
Да, я не рассказал с самого начала, кое-что еще забыл про fsync. fsync он не такой простой,
каким кажется. Вот он с одной стороны должен сбрасывать данные из кэша операционной системы
на диск, но может случиться так, что fsync вернет ошибку. И вот оказывается, что это совершенно
чудовищная история. Сейчас не знаю, найду я быстро ее или нет. Где-то она у меня была открыта.
Вот оказывается Postgres использовал fsync, но у них про это есть доклад. Они 20 лет неправильно
использовали fsync. Ну как неправильно? Они ожидали разумного, что... не умею переходить по ссылкам.
Они ожидали разумного, что если они делают fsync, а он завершается ошибкой, то это означает,
что оно, видимо, нужно fsync повторить когда-нибудь в будущем. И если fsync завершился,
то он вместе с собой записал... он гарантированно сбросил все данные,
которые предшествовали этому fsync в этот файл. Вот оказывается, fsync реализован не так. Это
абсолютно жуткая история, но тем не менее. Почему fsync может вернуть ошибку? Ну потому,
что вы могли писать данные на флешку. И вот вы писали данные на флешку, а потом флешку вытащили,
а потом сказали fsync. И в итоге вы не можете больше сбросить данные на диск, ну данные на эту флешку
из оперативной памяти. Ну что в этом случае делает операционная система? Ну раз флешку вытащили,
то, видимо, данные нужно из оперативной памяти выбросить, потому что они уже не могут быть
записаны на устройство. Поэтому операционная система могла делать и делала так. А именно,
она просто дропала данные, то есть на Linux конкретно, дропала данные в случае ошибки fsync. Поэтому
если у вас первый fsync проваливается, то он вместе с собой дропает страницы, которые были помечены
как грязные, и в итоге последующий fsync уже ничего на диск не сбросит. А в поздросе,
на этом хрупком основании, что fsync заперсистит все ваши данные, которые предшествовали этому
fsync, держалась логика чекпоинтов и логика обновлений хранилища. Вот, абсолютно чудовищная история.
Ну вот так fsync реализован, такие у него странные гарантии. Если вы видите ошибку от fsync,
то нужно падать. Ну а почему вы не с флешками работаете, то есть почему это делает fsync,
более-менее понятно. Но с другой стороны, у вас не флешка. И почему бы вы вдруг увидели такую ошибку?
Ну почему бы вы вообще увидели ошибку в fsync? Ну потому что сейчас вы тоже не то, чтобы с
локальным диском работаете. Скорее всего, если вы работаете в облаке, то под вами не настоящий диск
локальный, а под вами некое виртуальное диск, виртуальное блочное устройство, за которым стоит
некоторая большая распределённая система. То есть когда вы пишете на диск, вы ходите по сети куда-то.
Ну и понятно, что в этом смысл распределённых систем, что у вас где-то в другом месте ломается
компьютер, и в итоге у вас, в вашей виртуальной машине почему-то какие-то странности с диском,
и в итоге ваш fsync возвращает ошибку и дропает данные. Поэтому кажется, что разумная стратегия при
получении ошибки от fsync это поскорее упасть. То есть полагаться, что следующий fsync узнает,
ну починит, всё, нельзя. Да, ещё вспомнил про баги. Это такой не то чтобы баг Linux, они не знают.
Очень странное его поведение. Ну вот с чексумами, и почему мы ещё не можем доверять протоколам и их
чексумам? Ну потому что бывают баги в сетевом стеке. Вот, ну я вам могу кинуть дальше ссылку,
сейчас я найду её где-то. Так вот, в случае там какой-то комбинации виртуализации и TCP,
операционная система могла просто скипать проверку чексумы. Вот в твиттере где-то на
это нарвались и там долго-долго отлаживали это всё искали. Вот так. Очень страшно жить в таком
мире. Но кажется, что мы починили теперь это. То есть мы используем андулог, мы в нём используем
чексумы, чтобы проверять что андулог в целостном состоянии. Мы эфсинкаем его, чтобы гарантировать,
что запись в андулог предшествует write файл. Мы эфсинкаем файл для durability. Ну вот вроде бы
всё. Мы достигли durability, наконец-то тамарность достигли. Так вот, оказывается, что и это ещё не всё.
Потому что даже после такого протокола мы можем увидеть в файле строчку boo и увидеть,
что у нас нет лога. Лога нет, boo есть, данные в несогласованном состоянии, восстановить их
невозможно. Почему такое может произойти? Потому что на самом деле запись лога, ну вот создание и
запись лога это не только запись в файл с логом. Ну да, вот у нас есть запись в файл с логом,
то есть мы должны зафиксировать с помощью эфсинка изменения в файле, то есть в его данных и в его
метод данных. То есть мы здесь зафиксируем, что длина файла изменится и страницы, блоки,
которые мы записали в файл, будут сброшены на диск. Но вот этот протокол требует вообще-то
создания файла. То есть он требует ещё одной записи, а именно в iNode директории. Директория
это тоже файл и в неё тоже нужно записать, что вообще-то лог появился в файловой системе.
Вот сама, сам эфсинг файла это не гарантирует. Эфсинг файла гарантирует, что будут сброшены
изменения для данного файла. Но вообще-то сам по себе файл не знает про то, где он в
дереве файловой системы находится. Ну просто потому, что на один и тот же файл могут ссылаться
разные пути в файловой системе. Так что это внешние ссылки. И сам файл их не хранит,
он хранит максимум счётчик ссылок. В итоге вы действительно заперсистили содержимое лога,
но вы не гарантировали здесь, что вы надёжно сохранили запись в директорию, в которой вы
создали этот лог. Так что вам нужен ещё один эфсинг на самой директории D. Вот любая надёжная
реализация райта хэт логгинга должна использовать эфсинг ещё и на директории. И скажем, почему это
важно для нас, почему это важно для людей, которые пишут, скажем, строят, скажем,
RSM и там используют сегментированные логи. Если вы создаете сегмент лога на диске и пишете в
него там очередные записи, которые прислал лидер, то вы должны гарантировать, что этот сегмент не
потеряете. Но для самого лога вы используете, конечно, райта хэт логгинг с чексумами. Вы
пишете чексуму, вы пишете magic number для заголовка перед собственной записью на диск. Но вам ещё
нужно гарантировать, что если вы вообще строите, открываете новый файл, создаете новый файл,
то действительно в директории этот файл тоже будет виден. Иначе вы можете потерять сегмент
лога и потерять обещание, которое вы дали лидеру. Ну и в конце концов,
чтобы убедиться, что лог не нужен, нужно ещё, кажется, сделать это. Чтобы убедиться,
что лог удалён. Ну вот, протокол перезаписи фрагмента файла выглядит вот так. Абсолютно
жуткая история, ещё раз повторю. Вы не можете просто так пойти перезаписать несколько блоков
файла. Вы должны устроить довольно сложный протокол и сделать сразу 3-4 fsync, чтобы гарантировать,
что вы можете после рестарта откатиться к старому состоянию или накатить новое,
то есть обеспечить стомарность. Крэш консистенции достигается сложно. Крэш консистенции достигается
конечно же логами и плюс большим количеством вот таких вот fsync на директориях и на файлах. Ну
и чексумами. То есть весь этот аппарат необходим, чтобы просто иметь персидентное состояние на диске.
Почему так? Ну, трудно сказать, почему так. Вот абстракция файловой системы такова. Вообще вот
можно понять, что абстракция файловой системы это довольно, ну не знаю, почему она такая сложная,
но она довольно скверная, потому что вот в ней очень много свойств, которые плохо документированы,
и которые вот очень легко опираться на какое-то неверное ожидание, на какую-то гарантию,
которая на самом деле не соблюдается. А комбинации вот таких гарантий может быть очень много,
просто потому что API файловая система очень большая. Там слишком много методов. Вот сравните это с API
Executor. В Executor у вас admin.execute и больше ничего нет. Это очень удачная абстракция, она очень емкая,
и в ней очень мало, то есть очень маленькая граница с пользователем. А здесь граница
с пользователем гигантская. Тут там, не знаю, десятки уже методов, кажется. И неудивительно,
что получаются очень сложные комбинации, которые могут возникать и которые работают не так,
как мы ожидаем. Можно ли не использовать файловые системы? Ну в какой-то степени можно. Скажем,
в Яндексе когда-то давно отдельные системы не использовали файловую систему, описали свое
хранилище прямо поверх блок девайса. С одной стороны, это позволяло добиться какой-то
эффективности дополнительной за счет того, что у вас меньше абстракций между вашей системой и
хранилищем. А с другой стороны, была очень скверная ситуация, когда была допущена какая-то ошибка
вот в этом слое программным, который обеспечивал доступ к блоковому устройству. И вот после ошибки
в этом месте очень сложно понять, что с вашими данными происходит. Вот, скажем, Google File System,
если я не ошибаюсь, они там явно пишут, что они хранят данные просто в виде файлов в файловой
системе. Ну потому что для файловой системы очень много инструментов и легко понять,
что случилось. Можно пойти на машину и из командной строки посмотреть какие чанки там есть,
посчитать их чех суммы. Если вы работаете с блоковым устройством напрямую без файловой системы,
то вы такой возможности решаетесь. Вам просто сложнее жить в случае проблем. Поэтому мне кажется,
что так прямо сейчас делать не нужно. То есть все-таки нужно работать с файловой системой,
но с файловой системой нужно работать аккуратно. Ну что ж, таков итог сегодняшнего семинара.
Перезаписать кусочек файлов сложно, писать что-то в Write Ahead Log сложно и требует большой аккуратности.
Ну, в наших задачах логи необходимы, в RSM их они необходимы. И напоследок я расскажу два
мне кажется забавных, ну не то что наблюдения, а оптимизации. А именно, если вы храните,
если вы пишете реплицированные кивали у хранилища, то мы это уже обсуждали, в нем каждый
диапазон, в нем данные ваших гигантских таблиц бьются на эти диапазоны и каждый диапазон хранится
в виде отдельного RSM на наборах реплик. И мы обсуждали позапрошлый раз, кажется,
что вот эти диапазоны, они довольно маленькие, там какие-то десятки-сотни мегабайт, потому что их
можно легко балансировать. Ну для того, чтобы их можно было легко балансировать. Так вот,
получается, что на одной реплике у вас очень много разных RSM, и в этом случае будет очень
накладно, очень неэффективно для каждого RSM, для каждого рейнджа иметь свой собственный
лог на диске, потому что мы тогда будем делать очень-очень много параллельной записи на диск,
он не сможет так эффективно работать. Поэтому как RoachDB, но любая современная подобная система,
да и даже Bigtable на заре времен, если вы читали статью, то они пишут, что они не делают для каждого
таблета отдельный лог. Они для таблетов, которые обслуживаются одной машиной, одним таблет-сервером,
хранят все их логи в одном файле, просто для того, чтобы оптимизировать нагрузку. Так вот,
можно пойти еще дальше. Я вам говорил в лекции про RSM как-то давным-давно, про систему Яндекс.ДБ,
про то, что там вся система построена поверх RSM. Ну просто, грубо говоря, построена вокруг
отказоустойчивых машин. Машин, которые не ломаются, потому что каждая такая машина виртуальная,
но каждый такой актор — это отдельный реплицированный автомат. Так вот, в этой системе пошли еще дальше,
потому что у них одни RSM-ы, и они решили, что... Сейчас найду подходящую картинку или нет.
Вот, они решили, что они будут строить свою систему из этих RSM-ов поверх распределенного хранилища,
которое, по сути, будет реализовывать хранилище для логов. То есть хранилище для логов RSM-ов — это целая
отдельная подсистема. И уже поверх нее строятся вот такие отказоустойчивые реплицированные акторы.
Вот эта система про имутабельные данные, а поверх этой подсистемы, которая хранит логи и снапшоты,
строятся уже акторы RSM-ы, а из них выстраиваются уже в более сложные уровни. Ну и почему я сейчас об этом
вспоминаю — потому что через неделю мы будем говорить про распределенные транзакции, и там у нас
будет три примера — Bigtable, Spanner, ну и как раз Yandex.DB, как там устроены транзакции по типу кэрви.
Ну а с логами из файловой системы, наверное, на сегодня все. Кажется, что вот таких чудес
с нас хватит. Все, спасибо большое вам, тогда до встречи через неделю.

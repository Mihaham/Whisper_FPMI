Так, всем доброго дня. Нам надо перестраиваться. У нас в этот раз лекции в 12.20, а не в 11.30,
поэтому доброе утро уже не скажешь. Давайте еще раз познакомимся, особенно для тех,
кто будет смотреть меня в YouTube внезапно. Меня зовут Ахтямов Павел. И, как ни странно,
мы продолжаем с вами изучать всякие разные технические вещи. Формально этот курс называется
«Языки программирования и теории компиляции». Еще одно формальное название этого курса – «Теории
языков программирования и компиляции». Тут могут быть люди, которые записаны на два курса,
разных совсем. По факту это курс по компиляторам. Что мы будем с вами делать? Мы будем на лекциях
изучать то, как пишутся компиляторы. На практике мы будем смотреть практически кейсы того,
как эти компиляторы писать, основные аспекты. Лекции у нас здесь. Семинары 15.30 будут где-то
там наверху. Сегодня? Да, сегодня. То есть у нас все занятия будут в один день. В 15.30 что у
нас в расписании? Да ладно. Да, через пару. Там тоже типа должно быть. А, и вот так, но там
стоит? Понятно. Ладно, будем что-нибудь думать с этим. Там блоки по выбору скорее всего, если дальше
промотать в этих блоках по выбору, там скорее всего будут тоже языки программирования и компиляции.
Этот курс является курсом по выбору одним весом? Нет, странно. Ну ладно. Понятно. Ладно,
короче будем разбираться. Значит, курс будет о теории компиляторов и про практику написания
компиляторов. Сразу скажу, что этот курс совместный. Я предполагаю, что все так или
иначе проходили курс формальных языков. Все проходили? Да, хорошо. Я пишу скринкаст. Да,
нормально, да. Вот, собственно нам это понадобится, но понадобится на самом деле не в большом количестве
случаев. Давайте тогда я спрошу следующие вещи. Там на самом деле будет некоторая сведения из курсов,
которые нужны. Этот курс на самом деле является такой квинтэссенцией нескольких курсов. Наверное,
люди с третьего курса знают, что есть такое понятие как функкан, функциональный анализ,
который требует сведения из разных курсов. Здесь как раз такая же история. Нам нужно будут
формальные языки и трансляции для того, чтобы понимать, как работают регулярные выражения,
конечные автоматы и кс-громантики. И как ни странно, нам нужно будет понимание того,
как работает алгоритм парсинга. Хорошо работает. Не, а как именно конфликты разрешаете и так далее.
Этот кейс мы с вами продвинемся и научимся как раз пробовать разрешать эти конфликты на уровне
не того, что типа есть конфликт этот или нету этого конфликта, а на уровне того, собственно,
каким образом грамматики перестраивать, чтобы не было этих конфликт. Тема с однозначными
грамматиками, если кто ее не забыл, она будет крайне полезной. Заодно мы рассмотрим с вами
еще один алгоритм парсинга, который сильно проще, чем LR0, это алгоритм LL. Мы тоже его
рассмотрим и, наверное, с этого года будут возможны разные реализации языков программирования,
поэтому попробуем такой эксперимент. В принципе, можно. Кстати, про пеги можно
тоже поговорить. Это, на самом деле, базовая вещь, на которой мы будем заострять первые два
занятия. Дальше нам внезапно понадобится курс по технологиям программирования или, на самом
деле, того, как грамотно организовывать код, потому что кода будет много, сразу предупреждаю.
С учетом того, сколько ты мне уже сдал практиками по интерпретаторам, может быть и нет.
Наверное, не побьет, но если только ты компилятор до конца не доведешь.
Вот, значит, вводные сведения о компиляции проектов нам тоже понадобятся, потому что нам
нужно будет понимать, как работают системы сборки. Дальше, как ни странно, как мы все
преобразуем, нам понадобятся алгоритмы. Все знают алгоритмы? Наверное, не один экзамен уже сдали по алгоритмам.
А, понятно. Ладно. Хорошо. Алгоритмы, преимущественно, нам понадобится алгоритмограф, как ни странно.
Причем, кажется, что компиляторы, какие тут алгоритмы на графах. На самом деле, они есть.
Мы будем говорить про статистические методы оптимизации, но машинку мы прикручивать туда не будем.
И последняя вещь. Наверное, у кого-то этот курс уже прошел. Это курс по операционным
системам. У кого-то он еще идет. Наверное, я не знаю, сейчас на АКОСе до сих пор на ассемблере пишут?
Это, как ни странно, окажется полезным. Мы понятно, что на уровне ассемблера спускаться не будем.
Коллинг-конвеншнс было бы неплохо знать. На самом деле, мы его писать не будем, скорее всего, в базовом варианте.
Здесь есть такая слайд, для тех, кто раньше формальные языки были не у всех. Что делать, если я внезапно все забыл?
На самом деле, раньше был ответ страдать. Но, как ни странно, основные сведения мы будем разбирать на лекциях в семинарах.
И будет список литературы, с которым можно будет подчеркнуть эту всю информацию. Сразу скажу, курс в лекциях чуть более теоретический.
На практике он будет, конечно же, практический. Большая часть будет явно практической.
Поэтому можно будет подчеркнуть необходимую информацию. Так, этот слайд я пропущу, чтобы я его на ютубе не полил.
Давайте поговорим про литературу. На курсе по формальным языкам мы это опустили, но на самом деле здесь это будет крайне полезным.
Есть, так или иначе, три-четыре фундаментальных книги, которые можно что-то исследовать.
Первая книга называется Engineering Compiler. Это, как мне кажется, одна из самых современно прикладных книг по компиляторам.
Причем в ней есть еще и теоретические основы. Причем хорошо достаточно описаны.
Я не знаю, сейчас можно ее достать или нет. Честно, потому что раньше я ее доставал в библиотеке Урэйле.
Наверное, слышали такой. Слышали издателя Урэйле? Они раньше еще любили на заставке всяких животных закидывать.
Собственно, книга достаточно популярная. Теория с практическими примерами.
Вторая книга, которая здесь бывает, это вот такая вот.
Это вот, собственно, если кто-то пытался каким-то образом когда-то проходить курс по компиляторам, то обычно по ней читается большее количество книжек, большее количество компиляторов.
По крайней мере, раньше на кафедре Абби, которая я сейчас не знаю, как называется, у нее есть сейчас же какое-то название у кафедры Абби, или они?
Идиоты.
Вот. Собственно, там раньше был курс по компиляторам, он был годовой, и как раз в нем все разбиралось по этой книге.
Просто такая вот. Мостановская книга, 400 страниц. Причем эта книга, образно говоря, 2002 года. Садитесь, делайте, так сказать.
Мы, понятно, что не все время будем опираться на нее.
Так, собственно, эту книгу я, наверное, покажу. Скажу, что она существует. Она называется Dragon Book, потому что на заголовке написан нарисован дракон.
Да, но на самом деле книга хорошая, и книга именно хорошая, теоретическая книга. Но для практики, к сожалению, она сейчас уже особо не подходит.
Вот. Я не знаю. Кто-нибудь пытался читать ее, может быть?
Нет, я не буду знать, кто пытался читать.
Да, причем хочу сказать, что у Аха Уйма бывают две книги. То есть первая про формальные языки и про парсер, а эта чисто для про написание компиляторов.
Ну, она вот такая вот. То есть она толще, чем мой ноутбук, в три раза где-то. Вот такая постодонская история.
Четвертая книга, которая здесь есть. Значит, здесь я буду, опять же, альтернативно предлагать две книги.
Значит, первая это Flexibizon. Это для тех любителей, кто хочет вот прямо пострадать, так сказать.
А?
Флексить.
Да. И вторая книга, которая будет, это будет книга Panteler.
Да. Главное, хочу сказать следующее, что несмотря на то, что язык, который написан, называется Anteler, собственно, там реализовывается LR-алгоритм.
А расшифровывается это, как и Nazar Language Cognition Tool.
Вот.
Да. Значит, смотрите. Теперь давайте расскажу, что мы будем стараться делать.
Значит, у нас цель на выходе будет реализовать компилятор упрощенного языка программирования.
Значит, будет два пути. Первый будет более продвинутый, и, значит, нам нужно будет пройти все стадии сложного пути, как минимум, один раз.
И путь проще, это сделать строивание языка в язык промежуточного представления.
Промежуточного представления, ну, любого промежуточного представления я беру в качестве примера, это LLVM.
Теперь по формату выполнения заданий. Значит, задания будут идти итеративно.
То есть мы будем, первая наша цель будет все время это довести образно говоря какой-то базовый интерпретатор до трансляции в LLVM, а потом его обрастать мясом.
То есть сначала нам нужно просто пройти всю эту цепочку для того, чтобы быстро продвинуться в наши задачи, потом мы будем уже собственно внедрять системы типов и так далее.
Вот, сразу скажу, что проект можно будет выполнять в командах. Я еще подумаю по командам, по сколько человек это будет.
Потому что иногда одного этого сделать практически нереально.
Да, ну, у команды два-три человека, но сразу скажу, что для двух человек еще можно будет делать строивание в LLVM.
Если вы подписываетесь на три человека в команде, то придется пройти полный путь.
Ну, это да.
Вот. Наверное, чтобы вы понимали, как вам идея такая, что я на ахчеке сразу выложу формулировки заданий, тем более они с прошлого года остались.
Сразу выложу формулировки, чтобы вы поняли, куда мы в итоге движемся.
Хорошо. Так, давайте, наверное, организационные вопросы.
Так, давайте про пары этого не будем говорить. Давайте в тройках нельзя будет свернуть в LLVM.
С двойками посмотрим. То есть, готовьтесь к тому, что если вы делаете в одного из двух пар, то вы будете в тройках.
Если вы делаете в одного из двух пар, то вы будете в тройках.
Если вы делаете в одного, то у вас будут какие-то простые вещи.
Если вы работаете вдвоем, то у вас будут некоторые усложнения к заданиям.
Если вы работаете втроем, то мы движемся по полному пути.
Давайте примеры усложнений, которые будут в парных проектах.
Допустим, первое задание, которое у нас будет, это будет прикрутить сканеры и парсеры.
Прикрутить парсеры. Так вот, в чем отличия будут?
Отличие будет в том, что для первых и одиночных проектов будет достаточно прикрутить парсер.
А для парных проектов нужно будет прикрутить в ошибках компиляции, в каком месте она происходит.
То есть, такой механизм под названием locations.
То есть, нужно будет отслеживать, где именно происходит ошибка компиляции.
Это достаточно популярная практика. Вы видели языки программирования.
Особенно, кстати, современные языки, они подсвечивают даже участок кода.
Выдирают от участка кода и его подсвечивают.
Поэтому можно будет делать проект вдвоем.
По-моему, это все, что касается организации организационных вопросов.
Поэтому давайте, наверное, остановимся здесь.
Есть вопрос по организации?
Семинары.
Семинары, что с ними? Да, надо будет решить этот вопрос.
Чуть раньше закончим сегодня и порешаем этот вопрос.
Возможно, найдем слот.
Так, давайте тогда двигаться дальше.
Какой-нибудь тулу, конечно, мы помним.
Типа антеллера или типа пигена?
Я сейчас как раз про цепочку буду эту говорить.
Так, давайте еще организационные вопросы.
Есть вариант оутпут в виде ассемлера?
Ну, для отладки можно.
Нет, аутпут в виде ассемлера?
Ну, аутпут в виде ассемлера это как раз сложный путь.
В сложном пути как раз наша цель дойти будет до ассемлерного...
Еще тебе прикол в искусстве детинского.
Написать контент минус первый, который, наоборот, из-за Яры тебе язык гнели.
Ну да, то, что...
Ой, можно, это как декомпилятор называется.
Так, организационные вопросы.
Потом ты берешь все интерлоты.
Так, ладно.
Так, давайте продолжим все-таки.
Так, мы пришли на курс по компиляторам.
И, наверное, хотелось бы понять, а что же такое компилятор?
Я не знаю, я пишу не в кофе.
Это хороший ответ.
Это функция.
Да, это функция.
Это смартфон, который переводит в программную язык, а программирование другой.
Да, это...
Скажешь, что X86 это тоже язык программирования?
Нет, это ассемлерный диалект X86, это язык программирования.
А ассемлерный диалект, который подчеркнет ассемлеру байк-код, это не компилятор что ли?
Ну, тоже компилятор.
А список языков программирования языков X86?
Ну да.
Так, давайте все-таки, чтобы это...
Я понимаю, что у нас уже этот лекция практически в диалог превращается.
Но давайте попробуем это.
Компилятор это компьютерная программа.
Определение, кстати, я взял с Википедии,
которое транслирует код, написан на одном языке программирования, в код на другом языке программирования.
Ну, не обязательно на языке программирования.
Можно сказать, что и в машинный код, при желании.
Так, давайте примеры, ГЦЦ.
Откуда-куда?
А там ахзимонога-куда, а монога-куда?
Нет.
Именно, да, именно.
Плюс это...
ГЦЦ у тебя в ГЦЦ.
ГЦЦ у тебя в ГЦЦ.
Си, плюс, плюс.
Ну да.
Ну, в общем, да, из Java даже умеет.
А в 13 версии из Rasta даже научился.
Ну, давайте остановимся на ассемблере.
Ну да, пока не будем.
GDK.
Да, откуда-куда?
Да, из Java в Java Bytecode.
Нет, jar это архив.
Jar это, на самом деле, аналог DLL-ки в Windows.
Вот, если вы реально в ВИМе откроете jar-файл, то вы увидите, что там набор классов.
Есть.
Так, ладно.
CoffeeScript, блин, я забываю каждый раз подправить презентацию,
поэтому давайте будем считать, что здесь не CoffeeScript, а TypeScript.
Кто знаком, что такое TypeScript?
TypeScript со строгой интерфейсации.
Да, из TypeScript в JavaScript.
Есть еще всякие ВИМ-компиляторы, которые компилируют Rasta в WebAssembly.
Открыли черную дуру.
Так.
Ну, по-хорошему да.
Python умеет на стадии препроцессинга делать трансляцию в Python Bytecode,
который он потом интерпретирует.
Да, в PyC.
Но давайте я скажу проще, чтобы все воспринимали,
что по умолчанию Python все-таки считается интерпретатором.
Хотя, по-моему, уже...
Ну да, если мы в Python говорим Python, там сначала Python Bytecode,
потом он уже интерпретируется.
Вот, поэтому вот так.
Что такое интерпретатор?
Тоже заговорим, что это компьютерная программа,
которая выполняет инструкции без компиляции в машины язык.
Вот, ну давайте будем считать так.
Так, а теперь, наконец-таки, начнем с того, собственно,
каким образом выглядят все компиляторы.
Значит, у нас есть обычно входная программа,
которую мы подаем код в frontend.
Значит, это часть кода, часть компилятора,
которая должна код на нашем языке программирования
превратить в некоторое промежуточное представление,
которое будет одинаковое для всех языков программирования.
Независимо от того, какой код это вставил.
Дальше, значит, у нас с вами есть оптимизатор,
который берет код на языке промежуточного представления
и оптимизирует его.
То есть обычно оптимизация происходит не на стадии того,
что вы читаете код, и у вас написано там int x равно a умножить на 2.
В этом моменте очень сложно преобразовать.
А проще, когда мы уже переходим в набор каких-то более простых инструкций.
После этого у нас получается на выходе результат промежуточного представления
с оптимизированным кодом.
И дальше у нас начинает срабатывать стадия back-end,
которая берет и транслирует этот код уже под, так сказать,
определенную архитектуру процессора.
То есть именно мы превращаем все это в диалект ассемблера.
Еще раз front-end. Мы транслируем промежуточное представление.
Оптимизер мы оптимизируем промежуточное представление.
И back-end это транслируется промежуточного представления в исполняемый код.
Сразу такой вопрос, который я хочу задать.
Кто здесь вставил пакеты из PIP?
Это Python Package Manager.
Хорошо. Есть ли там особенность как раз от того,
откуда у вас качается пакет?
Там есть дополнительный чемодан под своей указательностью.
Смотрите.
Кстати, у меня вообще PIP-пакеты унесены в обычной репетитории.
Да, смотрите. Вот у вас есть пакет.
И давайте какой-нибудь пакет откроем.
Json это системный пакет.
Давайте NumPy.
Кстати, NumPy мне кажется будет прямо визуально.
О, господи. Там же второй NumPy скоро выйдет.
Обратной совместимости не будет.
Так. И смотрите. Здесь есть вот такая вещь.
Download.
И смотрите, что здесь есть.
Здесь есть вилпакеты.
И здесь есть так называемая тройка.
Собственно, это операционная система,
в которой вы это все запускаете.
Архитектура процессора.
И в Python еще используется такое понятие, как этот.
Как правильно сказать? Backend, под который написан этот питон.
То есть, видите, здесь CP312.
Это значит, что этот код, вот этот установочник
работает в CPython 312.
Опять же, есть Pypy.
Вот это вот питоновские, которые.
То есть, видите, в зависимости от того,
есть ли у вас Python 312,
есть ли у вас под капотом код на сях,
у вас будут устанавливаться разные версии пакет.
И, собственно, в чем цель pipa?
Цель pipa, когда вы пишете pip install,
это обратиться в этот пакетный менеджер,
узнать, какая версия у вас операционной системы,
какая версия питона,
какая версия под капотом CPython либо Pypy
и выбрать правильный дистрибутив.
То есть, видите, в зависимости от бэкэнда
все может сильно отличаться.
Значит, да.
Ну, собственно, если это написано в проекте.
Обычно все-таки стараются готовить
правильный установочник, который не надо компилизовать
в процессе установки.
Потому что сидеть, значит, мы пишем pip install,
при этом указываем какие-нибудь переменные
окружения сборки под CMake перед этим.
Я, кстати, с этим столкнулся недавно,
когда я запускал большую языковую модель
на своем компьютере.
Да, что такое большая языковая?
Оно самое.
Что такое большая языковая модель?
Это то, что используются аналоги модели чат ГПТ.
Пришлось.
Собственно, там вот этот миндинг,
который был LAM и CPTPython, нам пришлось
export use force CMake единичка,
export CMake args равно токенная строка.
Потом только pip install.
То есть через переменные окружения все,
пришлось прокидывать.
Ну ладно, мы уже немножко отклоняемся от темы.
Чем удобна такая структура?
Кто помнит технологию программирования?
Паттерн-медиатор или паттерн-мост?
Что такое медиатор?
Это посредник, который перекидывает
информацию с одной части на вторую часть.
Что такое паттерн-мост?
Кто помнит?
Да, технологии программирования пошли мимо.
Да, ну у вас-то понятно.
Это паттерн, в котором у нас есть управляющая часть
и управляемая часть.
У управляющей части одна иерархия,
у управляемой части другая иерархия классов.
То есть связываются они через API-интерфейс.
Кстати, тут сразу скажу, что здесь не совсем
правильная презентация.
У нас есть frontend.
Аналог GCC взят.
У нас есть cfront, у нас есть c++front,
у нас есть fortranfront.
Дальше мы это закидываем в промежуточное представление.
У нас происходит оптимизатор.
И дальше мы компилируем под определенную архитектуру.
Собственно, это либо армовская архитектура,
либо какой-нибудь мингвэшный компилятор.
Я думаю, что все слышали про мингвэ
свое собственное время, когда писали на плюсах,
там, не знаю, свое время.
Было такое?
Вот. И линуксбэк, который называется
обычный x8664-компилятор,
который обычно ставится,
когда вы делаете pip install g++.
Вот. То есть чем удобна такая структура?
Она удобна тем, что нам на самом деле достаточно
написать какой-нибудь frontend дополнительно
и правильно встроить в структуру Яра.
Затем мы будем с вами и заниматься.
А дальше оптимизаторы, если правильно
организованная структура кода,
то мы можем сами прикрутить определенную стадию оптимизации.
Собственно, здравствуйте, еще один паттерн.
Значит, это либо паттерн-билдер,
который проходит по всем стадиям и выполняет его,
создает нам проект,
либо паттерн под названием шаблонный метод.
Собственно, где у нас что-то выполняется по шаблону.
Так.
А мы сами будем выбирать?
Мы работаем, если мы выбираем...
Да.
Да, тут можно счетерить и выбрать,
написать все под ARM.
Может под ARM писать все проще.
Так, вот так не надо на мной издеваться.
У меня тоже есть где тестить ARM.
Да, у меня ARM процессор.
Да, да, согласен.
Кему решают все вопросы.
Ну да.
Да, в общем, давайте все-таки рассмотрим,
из чего же обычно состоят все эти части.
Значит, собственно, цель наших занятий
будет это постепенно проходиться по каждой из этих частей
и разбирать, каким образом они работают.
Начнем с тех частей, с которыми мы с вами работали.
Как вы думаете, какие из частей мы с вами разбирали
на курс формальных языков?
Первые две.
Первые две.
На остальные...
На остальные...
Ну да, да, да.
Да.
Так, давайте как раз мы начнем с первой части, это сканер.
Вот представьте себе, что у нас есть такой псевдокод.
Это псевдокод.
То есть у нас есть переменная int a, int b, int c, int d,
потом читаем a, b, c, d,
делаем такой температур присваивания и пишем print a.
Мы понимаем с вами, что код работает не таким образом.
То, что вот он существует, отлично.
Что мы можем с вами сделать?
Курс формальных языков вспоминаем.
Не-не-не, рано.
Нам нужно выделить все ключевые слова, которые здесь есть.
То есть все токен...
Да, нам нужно это все токенизировать.
Собственно, первая стадия front-end называется сканером.
В чем она заключается?
Каждую из составных частей разбить на так называемые токены.
Собственно, что такое токен?
Токен это как раз такая структура,
внутри которой есть какая-то встроенная часть в виде текста.
То есть токен это по факту тип.
Вот, типа int, var, semicolon и так далее.
Алексема это то, что как раз парсится этим токеном.
То есть давайте мы сделаем в следующем.
Мы понимаем, что int это скорее всего тип int,
abcd это на самом деле var,
точку запятой это semicolon,
read это read, print-print,
скобки заменяем, assign,
двойку мы заменяем на нам и звездочку заменяем на mu.
И смотрим, что получается.
На самом деле у нас получается вот такая вот вещь.
Да, причем еще и без перевода строки.
То есть перевод строки просто для визуальной трансляции того, как это работает.
То есть вот что мы хотим получить на выходе сканера.
Причем я замечу, что некоторые токены, которые здесь есть, они именованные.
То есть внутри них есть название имени.
Вот, допустим, двойка это нам, var это b.
Следующая часть.
Вот у нас есть набор токенов.
Следующая часть это парсер.
Парсер в качестве входного алфавита, вот это важно,
принимает токены.
Да, то есть она принимает токены, она не принимает слова из этого алфавита.
Ну, из входного алфавита, то есть это выход сканера.
Вот, ну и здесь мы что должны построить, чтобы построить парсер?
Грамматику, да.
Причем контекст свободный.
Что делает парсер?
Парсер разбирает полученную грамматику, программу при помощи грамматики
чтобы выстраивать дерево разбора.
И здесь нужно будет специфицировать два разных видов дерева разбора.
То дерево разбора, которое мы с вами разбирали на курсе формальных языков,
оно никаким образом не обозначалось.
А здесь мы для каждого правила разбора будем специфицировать его конкретный тип.
И вот это будет называться абстрактным синтоксическим деревом разбора, ast.
Вот пример грамматики, который парсит эти выражения.
Программа это statement семиколон либо list statement.
Понятно, что эта грамматика может быть неоднозначной сейчас, хотя она по идее должна быть быть однозначной.
А list statement, дальше statement это read, assign, print и так далее.
И здесь мы явно сразу видим структуру, что обычно программа у нас разбивается сначала на наборы стейтментов,
а внутри стейтментов есть какие-то экспрессии.
И в принципе это будет сохраняться еще до долгих стадий компиляции.
То есть там в VR такая же абстракция есть.
Lparen и rparen у нас есть.
Понятно, что когда мы будем упрощать это все дело, у нас это будут не скобки уже, а какие-то...
Там можно обратный маппинг сделать.
То есть сказать, что lparen это скобки.
Это делается в том же самом бизоне.
Ну и вот, у нас получается дерево разбора.
Давайте... я в какой-то момент времени рисовал, вы полюбуйтесь как...
Вот, получаем дерево разбора грамматики.
А дальше, когда у нас есть дерево абстракции тактического дерева разбора,
на самом деле для того, чтобы довести его до промежуточного представления,
нам нужно на самом деле пройти большое количество стадий.
Нам нужно создать таблицу символов.
То есть это такая структура данных, которая позволит по переменной понять, какой у нее тип.
И что это такое? Класс, переменная, число и так далее.
Дальше нам нужно будет построить систему выводов типов.
То есть у нас ошибки компиляции могут быть на уровне сканера обнаружены,
чтобы какой-то токен не проанализируем.
Больше количество ошибок это синтоксических ошибок.
Но более того, у нас есть семантические ошибки, когда у нас типы не совпали.
И, кстати, такой вопрос.
Переменную объявили дважды. Это какой тип ошибки?
Это уже семантическая ошибка.
Да, то есть нам нужно почитать...
И дальше нам нужно будет проверить преобразование над деревом для получения промежутинного представления.
И здесь нам как раз уже понадобится набор так называемых виртуальных регистров.
Это более простой пример.
На самом деле я просто взял аналог того кода, скомпилировал в VR и заменил...
Да, только это на самом деле не чистый ЯР, а вот такой вот промежуточный.
То есть я из него просто выкинул несколько лишних слов, чтобы было проще.
То есть смотрите, когда мы создаем переменную, на самом деле мы ее алоцируем на стеке.
Да, да, да.
Смотрите, это не переменная, это виртуальный регистр называется.
И здесь видно, что каждый раз, когда мы объявляем новый регистр, мы не можем ничего присвоить другому.
То есть мы создаем виртуальный регистр и все.
Да, если нам нужно записать ячейку в какой-нибудь другой виртуальный регистр, здесь есть операция Store, которая это делает.
Я вижу, что это старая версия VR, потому что поинтер непрозрачный.
Ну да.
Тут тоже по усмотрению. Обычно берется мини-версия Джавы, либо какой-нибудь C.
Да, C++ с простыми классами.
C++?
Да. Я бы так сказал, да, C++.
Видите, кстати, вопрос такой, который я хочу задать.
Чем более удобно это представление?
Это не дерево.
Это не дерево, это линейная структура кода, которую можно выполнить.
Более того, у нее есть заранее определенный набор правил.
То есть в принципе, насколько я понимаю, если мы забиваем на вот эти вот команды, которые у нас есть, то в принципе это даже что-то похожее на автоматность.
То есть их легко распознать именно последовательно.
То есть по факту мы получаем с вами на халяву правилинейную грамотику.
Ну да.
Да.
Вот.
Значит, дальше, после того, как мы это пройдем, мы будем проходить про оптимизацию.
Ну здесь, конечно, простая оптимизация.
И здесь видно, что это оптимизация уже на уровне ассамблярных команд.
То есть видите, мы вычисляем A на 2 на B на C на D в большом количестве циклов.
То в принципе мы можем попробовать вынести эту переменную до и попробовать вычислять.
Там прямо можно будет посмотреть, собственно.
Видите, A на 2 на B на C на D на самом деле раскладывается вот в такую портянку.
Ну, это не те методы оптимизации.
Да.
И последние части компиляторов, которые есть, это выбор инструкций,
аллокация регистров и составление расписания инструкций.
Давайте тоже поговорим про эти части.
Мы, опять же, каждый случай будем разбирать отдельно.
В выбор инструкций задача данного этапа будет взять набор инструкций,
которые у нас имеется на входе, и перестроить ее под определенную архитектуру.
Потому что в разных архитектурах процессора бывают разные наборы ассамблярных команд.
То есть здесь мы, опять же, при выборе инструкций говорим,
что число регистров может быть бесконечным.
Ну и тут возможна базовая трансформация,
к которой, конечно, уже заточено, смотрите, под определенную версию компилятора,
потому что где-то операция умножения на два работает быстро,
где-то это надо трансформировать в операцию плюс,
а где-то это нужно трансформировать во что?
В битовый сдвиг.
То есть, опять же, тут уже возникает задача, кстати, как ни странно, оптимизации.
Причем дискретной оптимизации.
То есть нужно будет покрыть дерево таким образом, чтобы оно было меньше...
Кажем, мало на физики инструкций.
Да.
Следующая стадия как раз понадобит ценам здесь алгоритма.
Это локация регистров.
Что делает этот этап?
Задача этого этапа заключается в том, чтобы виртуальные регистры перевести в реальные.
Вопрос к вам, как людям, которые прошли курс по аквозу,
по крайней мере, первый семестр.
Сколько регистров есть в разных архитектурах, которые можно использовать?
Конечное количество.
16, 20 до 2.
Ну, это в армии.
На 86, а потом на 6.
Ну, а сколько из них реально можно использовать нормально?
Ну, 6-6.
Ну, там же Колинка меньше сильно режет.
Да-да-да.
6.
Ну, в лучшем случае очень странные операции.
Вот.
А теперь знатоки.
Знаете такое понятие, как хроматическое число графа?
Это количество цветов, которые нужно покрасить граф,
для того, чтобы никакие два ребра не были покрашены в один цвет.
Ну вот, как раз наша задача будет оценить, можем ли мы покрасить граф разные цвета.
Собственно, вершины будут это регистры виртуальные, ребра, это могут ли они одновременно существовать.
И нужно будет как раз разбирать, можем ли мы этот граф покрасить.
Сразу скажу, что Uber-мега-жёстких алгоритмов не будет, там будет тупой жадный алгоритм.
Но в целом нужно будет понять, каким образом это решать.
Если у нас алгоритм не справляется успешно, мы тратим наши ресурсы на то, чтобы скинуть эту переменную виртуальную регистр в память.
Но это уменьшает скорость нашей программы.
Увеличивает задержку.
А?
Не, алгоритма достаточно.
Вот, кстати, пример превращения этого кода.
То есть вот тут был один код, и здесь видите, всего вместо шести регистров, которые были отведены под каждую перемену ABCD,
ARP это, по-моему, что-то типа фреймпоинтера, то есть это указатель на начало стека.
Их, оказывается, можно превратить в всего три регистра.
Если посмотреть код. Ну, собственно, презентацию можно сделать.
Ну и, собственно, последняя вещь, это про конвейерность.
Обычно про нее не говорят, но желательно правильно выстроить инструкцию таким образом, чтобы они правильно встали в конвейер процессора.
То есть какие-то для чтения, какие-то для запись, какие-то для вычислений.
Но мы как раз про это детально говорить не будем.
Так, это что касается стадии компиляции. Есть ли вопросы?
Хорошо, тогда давайте мы сегодня начнем с... Закончим нашу лекцию первой темой, такой фундаментальной.
И вам тут сейчас настанут некоторые флешбеки, из предыдущего семестра, во что мы будем говорить про сканеры.
Но на самом деле мы немножко их адаптируем к нашим вещам.
Да, это, кстати, основные пункты поведения.
Слушайте, еще раз мы с вами...
Сейчас параллельно курс идет по хайлоду.
Так, там более правильное определение фронтэнта.
Там фронтэнту даже относится NGINX в веб-сервер.
То есть все, что на уровне до веб-серверов, это прямо фронтэнтом считается.
NGINX написано не на JavaScript, явно уж точно.
Более того, на фронтэнде можно сделать авторизацию.
Внешние проверки.
Так, давайте перейдем тогда к сканерам.
И поймем...
Да, у нас будет каждый раз рисоваться такая картинка.
Стоит в компилер.
Да, то есть мы находимся сейчас здесь.
И давайте еще раз я проговорю те определения, которые я уже говорил.
Что такое Алексема?
Это некоторая последняя символов, шаблон которой ультавряет некоторому требованию.
А токен – это строка, имеющая некоторый сакральный смысл, некоторые Алексемы.
То есть токен – это типизированная Алексема.
Сканер – это по факту транслятор, который приводит входной текст в некоторую последовательность токенов.
Значит, примеры возможных Алексем токенов.
Int – это type.
Причем Int – это Алексема, а type – это будет токен.
42 – это...
Ну, кто-то считает, что это число.
Можно написать, что это магическое число.
Можно написать, что это void-звездочка.
Ну, понятно, что это плашутка.
Сканер переводит в из Алексема токена.
Ну да.
То есть он знает, что это type, что это int.
Да, да, да.
Но токен обычно внутри себя содержит Алексему.
Подчеркивание Hello – это переменная.
Hello word – это строка.
И плюс-минус – это может быть либо бинарная операция с Алексемой заложенной в эту бинарную операцию,
либо это может быть отдельный токен, вида плюс и минус.
При желании, да.
Да, правила грамматики какие-то там должны быть громоздкие.
Да.
Ну...
Типа я пробил его, да я вам правила грамматики.
Ну, бывают разные подходы.
Такой неправильно мне парить.
Да.
Так.
Ладно.
Что означает, что Алексема заложен в токен?
А это означает, что, допустим, если у нас есть Алексе, образно говоря,
у нас под токеном type,
может скрываться как int, так и float.
В принципе, произвольный тип.
Вот.
И мы будем это обозначать следующим образом.
Что у нас есть тип.
Вот, тут доску поменяли, что ли.
А внутри нее будет заложена Алексема, которая ее раскрывает.
То есть, как бы у нас в коде это float,
по факту это тип.
Вот.
Хорошо.
Задание токенов.
И здесь начинается курс формальных языков,
потому что для нас обычно токены проще задавать регулярными выражениями.
То есть, шаблонами, которые это все парсят.
Напоминаем, собственно, определение регулярных выражений.
Они задаются рекурсивным образом.
0 это пустой язык,
1 это пустое слово,
однобуквенные символы этой языка за этой буквы,
есть конкатинация языков.
В нашем случае регулярные выражения есть объединение языков
и есть итерация к линии.
Мы это все прекрасно помним.
И это формальное определение.
А теперь давайте посмотрим, как это дело пишет о практике.
Практике это пишет вот таким вот образом.
То есть, у нас появляются дополнительные символы,
которые раскрываются в базовую.
Первый токен, который есть,
это то, что начинается с определенного слова
и заканчивается им.
То есть, это оператор или на уровне одного токена.
То есть, допустим, каким образом задаются все большие буквы,
они задаются вот таким образом.
Как задаются все буквы?
Как задаются все буквы?
Вот так они задаются.
Да, да, да.
Тут опять же плюс имеет...
В регулярных он был вверх и полеуза, а теперь под стеной.
Да, это серединный плюс.
То есть, здесь есть еще вот такая вещь.
Вот так вот.
То есть, когда вы пишете о плюс, это на самом деле а с символом плюс.
И важный еще момент, что круглые скобки здесь тоже имеют смысл.
Это называется группирование в регулярных выражениях.
Ну, это уже совсем продвинутые вещи.
Значит, и при этом есть некоторые сокращения.
Здесь важное следующее, что мы с вами решаем все-таки...
Смотрите, важный момент.
Мы здесь решаем задачи не распознавания, а задачи трансляции.
Поэтому у нас будет поток символов,
которые нам нужно будет транслировать набор токенов.
Поэтому здесь еще появляются дополнительные токены
в виде начала строки и конца строки.
Которые бывают полезны.
Backslash-S плюс.
Backslash-S это пробельные символы,
а Backslash-V это непробельные символы.
Так, как говорится, сложно.
Если вы не пользовались.
И здесь есть реклама, к сожалению.
Твоя банковская система, которая ушла из нашей страны.
Это, наверное, мем нашего поколения.
Если что, видео покажу в конце.
Но для построения регулярок есть сайт regig101.com
Я не знаю.
Коллеги с третьего курса точно знают.
Потому что были параллельные распределенные вычисления.
В общем, сайт выглядит вот таким вот образом.
И давайте что-нибудь напишу.
Видите?
Сколько сегодня?
Вот, видите, что он сделал?
Он выделил, собственно, все вхождения вот этого шаблона.
0,9 плюс.
То есть первое вхождение 20,24.
Второе вхождение 0,2.
То есть при помощи него можно взять,
загнать регулярные выражения
и увидеть, что он конкретно разбивает в данном случае на токенах.
Крайне удобно.
Просто заходим, вбиваем, смотрим, тестируем.
В принципе, здесь даже есть кодеген.
Для парсинга этих регулярных выражений.
Да.
Да.
Того шаблона, который...
Здесь есть такая кнопочка.
Вот.
Собственно, здесь даже можно эти,
юнит-тест писать.
А? При желании.
А?
Последний символов.
Да.
Да.
В ASCII, да.
Ну, да.
Да, тут есть, кстати, разные форматы.
Match.
Вот.
Ну, здесь видно от 0 до A.
Собственно, от 48 до 65 символа.
В ASCII кодировки.
Так, это что касается регулярных выражений?
Так, что было по формальным языкам?
А, я имею в виду, вот такая регулярная та,
которая в формалку была.
Ну, а это те же самые регулярки, если что.
Нет, там есть...
Куча, куча...
А.
Ну, да, это продвинутые вещи.
В первую очередь.
Ну, мне кажется, это будет сложно,
потому что, собственно,
язык AFP это и не контексту свободный язык.
Поэтому с этим будут проблемы.
Ну, блин, давайте уже не закапываться.
Так, давайте как раз попробуем построить регулянки
на некоторых примерах.
Не отрицательные целые числа.
Ну, желательно, чтобы еще...
Ну, с двумя цифрами все-таки мы пропускали.
Да, тут есть еще операторы или,
вот, которые я указал, это палка.
Ну, да, то есть это...
Ну, да, да, да.
Ну, мы все-таки не восьмеричную запись.
То есть у нас число либо неоднозначное.
Начинается оно с 1 или 9.
Вот, дальше 0,9 плюс.
Либо 0,9. Имена переменных?
Ну, вот, понятно, что это некоторые подношения.
Я в качестве примера показываю.
Ну, это да.
Ну, да, да, да.
Ну, тут можно по-разному уже задавать.
Это просто для того, чтобы мы обознаковались.
Строки, кстати.
Да, тут я, к сожалению, все кроме кавычки опустил.
Но по-хорошему, кавычка, все кроме кавычки.
Кстати, как в этой штуке сделать отрицание?
В этой штуке сделать отрицание,
тут нужно крышечку добавить.
Это будет отрицание.
Мы с вами понимаем, что регулярные языки
в замке относятся к отрицанию,
поэтому здесь у нас никаких проблем не было.
Все кроме, смотри, кавычки и бэкс-лэш-кавычки.
Ну...
А как распознавать экранирование?
Или экранирование, которое, скажем,
форматирована строка?
Все кроме кавычки, но бэкс-лэш-кавычка...
Типа я, когда писал по интерпретатору,
там тоже часть языка это экранирование и форматирование.
Экранирование, это вообще первый прогон?
Да, кстати, несколько прогонов тоже разрешается.
То есть мы можем взять наш прогон
и его трансформировать в первый раз.
То есть, когда ты считываешь буквы,
перед тем, как кидать экранизаторы,
ты перекорректируешь?
Да, но поскольку люди очень решили
сильно забить на это все,
для этих людей как раз и придумал
был формат B6C4,
который берет и переводит это
в набор символов и слашей.
Собственно, даже если вы посмотрите
SSH-ключ свой,
там строка написана в формате B6C4.
Специально чтобы можно было
подобрать эту строку?
Да.
Так, вопрос.
Мы с вами разобрали, каким образом
мы можем стряпать эти шаблоны.
Каким образом их распознавать?
Готовыми парсерами или регулярными?
Так, ладно, давайте мы
в теорию перейдем.
Что мы сопоставляли регулярным выражением?
Конечные автоматы, конечно же.
Для того, чтобы распознать шаблоны,
нам нужны конечные автоматы.
Сразу скажу, что у нас
курс уже более ламповый,
поэтому здесь не будет
строгих определений.
Но если мы говорим про строгие определения,
то, конечно, автоматы это
пятидесять.
Можно только так в экзамене сказать?
Теперь можно.
Ты в экзамене дал, как ты тебе что правил?
Да.
Множество состояний,
афавит, стартовые состояний,
множество переходов, откуда-куда,
и здесь можно сказать, что по какой букве
или по какому слову,
и множество завершающих состояний.
Слово распознается тогда,
когда существует путь от старта до финишена,
и это очень важно.
Когда существует путь от старта до финишена,
на ребрах которого написано слово.
Понятно, что здесь мы про штопоры
говорить не будем.
Потому что наша цель, на самом деле,
другая.
Вот пример автомат для распознавания этих чисел.
Тут он немножко
преобразовал.
У нас есть числа от 0 до 9,
есть числа от 0 до 9 и 0.
Есть недетерминированность.
Мы с вами понимаем, что мы можем с вами
построить детерминированный автомат.
Собственно, для этого нужно выполнить три стадия.
Первое, это сделать все переходы
не более чем однобуквенными.
Дальше убрать пустые переходы и построить
детерминированный автомат.
Мы по факту это инклудим.
Я просто напоминаю,
как это инклудится.
Ну как, хедр, почти.
Мы показываем абстрактный класс,
можно сказать.
Без реализации.
Вот оно реализация.
Наверное,
все это помнят.
Все помнят?
Хорошо.
Детерминированный автомат.
Когда мы детерминируем автомат, мы собираем по множеству.
Тоже алгоритм Тобсона у нас был.
Дальше нам нужно будет построить
минимальный ДК.
Тоже делается различение по буквы.
Синий мы в одну часть отправляем.
В одном классе другие и в другой класс.
И делаем это до тех пор,
пока это повторяется.
Все понимают, что симпточка этого алгоритма
экспоненциальная.
Но почему мы его можем делать?
Да.
Мы делаем на один раз.
Но он даже компилироваться не долго будет, на самом деле.
Ну да.
Так.
И вот, смотрите,
один важный момент,
который я хочу обратить внимание,
что в определении
нужно допустить
одно важное изменение.
Я хочу подчеркнуть, где.
Здесь.
Множество завершает
состояние.
Для распознавания.
Мы все-таки делаем
не распознавание,
мы делаем транслятор.
И цель наша такая, что по входному слову
мы должны транслировать его в один из типов.
В один из типов токена.
Вопрос.
Каким образом мы можем трансформировать это определение,
чтобы у нас все получилось?
Да.
Мы можем сказать, что завершающее состояние
у каждого завершающего состояния
у нас есть определенный цвет.
Ну, можно так.
То есть, смотрите,
мы красим наше завершающее состояние
в несколько типов.
Важно, главное,
чтобы у вас
два завершающие состояния
не покрасилось в один и тот же цвет.
Но обычно это решают таким образом,
что первое правило,
которое соответствует
вот этому шаблону,
в тот цвет оно и красится.
То есть, если у вас есть правило
на пятой строке,
есть правило на восьмой строке
с разными типами токенов,
то красятся в цвет пятой строки.
Потому что чтобы было
по методу исключения все работало.
Потому что обычно,
как ни странно, последний токен,
который задают в сканере,
это вот такой. Это точка-звездочка.
Это означает
произвольная строка.
Мы на ней говорим,
что у нас ошибка сканера.
Чего там шумит?
Ноль шумит, что ли?
Ну все, проектор это.
Так, и давайте как раз мы сейчас
поговорим про алгоритм построения сканера.
Может быть, успеем один разобрать.
Потому что сканер – это транслятор.
Он принимает на вход всю строку
и должен получать
какой-то выход.
Здесь есть другой подход.
Второй подход – это прямой поиск.
Это по факту то, что делал
Гера у нас.
То есть, он берет
и пишет сам код, который берет
и делает сканер.
Вот.
Я делал все, чтобы издвигать
и избирать от состояния кода.
Проделал дарить гераматику.
И есть еще такой код.
Подход называется ручной подход.
То есть, это всякие евристические правила,
которые мы можем докинуть
для того, чтобы построить сканер.
То есть, у нас есть классический сканер,
но, допонятно, мы делаем еще какие-то действия.
Итак, табличный подход. Давайте расскажу,
в чем он состоит.
Цель будет состоять в том, чтобы
на выходе получить токен.
Набор токенов.
Но здесь есть некоторый подвох.
Мы можем сказать,
ну, какой простой пример наивный алгоритм.
Мы берем, значит,
идем по нашему слову до тех пор,
пока мы не зашли в завершающее состояние.
Как только мы заходим в завершающее состояние,
мы выдаем токен.
Найдите ошибку.
Нет, нет, нет.
Это не вот этот алгоритм.
Смотрите, здесь есть такой подвох,
что давайте рассмотрим два слова.
Первое слово это new,
а второе слово это new-element.
Это ключевое слово.
Это название нашей переменной.
Если мы будем парсить
последовательно
и будем работать new,
здесь мы выдаем наш токен
и обрываем наш контекст.
А дальше у нас получается какая-то ерунда.
Как-то некрасиво.
Мы хотим парсить все-таки
наибольшее по включению слова.
Обычно при сканики
говорится, что
пытаемся максимально
по включению префикс делать.
Это тоже есть.
Поэтому наша цель
будет стать следующим,
что когда мы парсим наше слово,
мы доходим до состояния
состояния error.
Подумайте, в каком алгоритме
у нас возникало состояние error.
Кто может сказать?
Мы с вами его рассматривали
алгоритм на курс формальных языков.
Не конфликт.
Давайте я назову его
другое название, stock.
Когда мы строили с вами
PDKA,
у нас было стоковое состояние,
в которое приходят все слова,
в которые у меня было перехода.
Здесь это состояние мы назовем
состояние ошибкой.
Мы пошли не по корректному пути,
а пошли в какое-то другое место.
Это ошибка error.
Если в задаче распознавания
это состояние было для нас совсем неважно,
то здесь оно играет ключевую роль.
Мы относительно него начинаем
отматывать наш сканер назад.
То есть мы построили с вами
несколько завершающих состояний.
И потом пошли в состояние ошибку.
Дальше наша цель
будет сделать обратный проход
по этому моменту до тех пор,
пока мы не вернемся в завершающее состояние.
Именно в нем отсечь наше слово.
Здесь у нас получается new элемент.
После наступления
с пробела мы идем в ошибку.
Скорее всего,
если там
не какая-то другая грамотика.
И дальше мы отматываемся назад
и попадаем в наш сканер.
С высокой степенью вероятности
мы сделаем один шаг назад.
Но это не гарантируется.
Дальше мы очищаем стэк.
Тут именно стэк нужен для того,
чтобы все это делать.
Если мы внезапно дошли
до корня стэка,
а такое вполне возможно.
То есть мы шли-шли-шли,
и на нашем пути не было ни одного принимающего состояния.
Хотя это навряд ли.
То мы говорим, что у нас
синтоксическая ошибка.
Ой, не синтоксическая, а лексическая ошибка.
В нашем примере.
Хороший кейс.
У нас
нам нужно сделать следующую вещь.
Если мы бесконечно будем
ходить до конца программы.
Ну, end of file
мы можем всегда ввести перемен
строку.
И именно туда тоже закидываем
переход в ошибку.
Да, мы построили для всех
токенов, которые мы хотим располонить
по ДК.
Да, как только мы дошли в ошибку.
Дальше мы делаем следующее.
Мы делаем откат по нашему пути
до тех пор, пока мы не зашли в завершающее
состояние.
И дальше то слово, которое у нас было
до завершающего состояния, мы отсекаем
и повторяем все по новой.
Да, да, снова в стартовую вершину.
Здесь нужно отдельно рассмотреть случай
конца файла.
Собственно, этот пример мы уже
с вами разобрали.
И, кстати, проблема этого алгоритма, что он работает
в квадратичное время для произвольных
сканеров.
Здесь есть пример, зачастую
он будет работать за линию.
У нас есть строка
ab, ab, ab, ab...
Здесь видно, что
можно подумать, что если мы принимаем
слово только ab, то у нас будет
квадратичная сложность на отматывание.
То есть мы промотали все слово, смотрите.
ab, ab, ab,
дошли до конца файла бесконечный цикл,
и все-таки нам нужно отмотаться до ab.
То есть мы по факту сделали цикл
по всему слову,
проход по всему слову, а приняли только ab.
Поэтому здесь
это можно лечить. Мы можем сказать,
что давайте заведем
массив пар,
позицию состояния в сканере
и будем при помощи него
отслеживать.
Допустим, если мы один раз зашли в какую-то
ложную ситуацию, в ложный путь,
то мы можем сделать маркировку
того, что мы уже пошли по ложному пути.
Ну, в принципе, да.
Почему здесь стек?
Потому что мы считаем,
что мы работаем с водным потоком,
поэтому все-таки, как только мы
прошли что-то из входного потока,
нам желательно сохранять это все дело.
Ровно по этой причине.
Вот и все.
Вот и все.
Вот и все.
Вот и все.
Вот и все.
Вот и все.
Ровно по этой причине.
Собственно, давайте попробуем
подумать, какие недостатки у этого подхода есть.
Тут проблем
будет в памяти.
Потому что, чтобы хранить DKA,
нам нужно большое количество
состояний.
Сейчас это не проблема,
потому что у нас компьютеры все мощные.
В 80-90-х годах это была проблема.
И, во-вторых, у нас есть долгий поиск по таблиц.
То есть нам нужно
отмотать какое-то состояние DKA,
проверить в нем переходы, найти его,
и так далее. С этим есть проблема.
Вот.
Собственно,
есть идея,
каким образом еще попытаться
обойти
вот этот подход.
С тем, что мы можем очень долго
плутать.
Он, кстати,
завязан как на особенностях
операционной системы,
так и на особенностях самого языка.
Вот, допустим, мы с вами,
если у нас распознается строка
какая-нибудь, то мы понимаем сразу,
что у нас первый символ
это кавычка, и явно нам нужно ждать следующую
кавычку.
Да?
Вопрос к вам.
Вы когда-нибудь пытались объявлять очень длинные
кавычки в программах?
Практически во всех языках
программирования есть ограничение
на длину переменной.
То есть тем самым мы можем
на этапе сканера сказать, что если
мы находимся в определенном
наборе ситуаций,
проевристический подход,
и, собственно, мы прошли какое-то количество
символов, очень большое,
и мы не дошли до ошибки,
или мы не приходили
ни через какое завершающее состояние,
то, значит, мы
останавливаем наш алгоритм.
Говорим, что у нас лексическая ошибка.
Понятно, да? То есть...
А?
Ну, это
совсем плохая ошибка компилятора.
Так. Да.
Тут, кстати, есть еще прямой подход.
То есть мы по факту, как только
заходим в определенное
состояние в ДК,
просто выводим код,
который у нас
имеется. То есть в первый раз, как только
мы зашли в определенное состояние ДК,
мы генерируем код.
То есть вы представляете себе ситуацию,
у вас есть код, у вас есть какой-то пример,
и вы понимаете, в принципе,
какой у вас маленький НК есть,
и вы понимаете, что, скорее всего,
на стадии прохода нашей программы
вам не нужен этот весь код.
Вам не нужна эта вся таблица.
Тогда идея какая?
Мы идем, и
как только попадаем в определенное состояние
в НК, то есть запускаем алгоритм Томсона
на этапе прохода самого языка.
На этапе прохода компилятора.
В итоге у вас код автоматически генерироваться
будет
для парсинга,
для распознавания Алексея.
Получается вот такая вот страшная вещь.
Да, это фотография из книжки.
Собственно, вот так
она будет выглядеть.
Так, хорошо.
Тут есть еще евристические подходы.
Собственно, про ограниченность
названий переменных мы с вами поговорили.
Тут есть решение.
И есть еще одна чтенька евристика
используется, это обработка чисел.
Кстати, мы ее тоже посмотрим.
То есть мы понимаем с вами,
что конвертировать строки в числа
это зачастую оверхед,
потому что вы и так уже прочли
строку,
и вам еще раз эту строку конвертировать число
это бред.
Поэтому вспоминаем олимпиадное
программирование, каким образом
там строки в числа конвертируются.
Омтикота, мы идем,
считываем символ,
его автоматически считаем
текущее число,
которое у нас имеется.
То есть вот такие подходы к трансформации
в число, чтобы как только вы зашли
в завершающее состояние, связанное
с числом, вы сразу выдавали число,
а не делали дополнительную
конвертацию на это дело.
И такие мелкие обработки
еще есть.
Еще один последний кейс
это обработка ключевых слов.
У вас количество
ключевых слов в языке
ограничено, поэтому можно сделать
следующее. А давайте мы с вами
создадим универсальную
хэш-функцию для этих ключевых слов
и будем просто каждый раз, когда у нас
накапливается определенное слово, автоматически
проверять,
является ли оно
ключевым типом.
Для того, чтобы если оно является ключевым типом, мы сразу сказали,
что это токен ключевого типа.
И не выполняли лишний проход
по автоматам.
Нет, нет, конечно, это вот такие
теоретические вещи, которые я хотел рассказать.
Вот, собственно,
это еще раз суммирование
того, что мы прошли за сегодня.
Мы посмотрели,
из чего состоит компилятор,
и начали работать как раз
с разными типами сканеров.
Возможно, какие-то части мы добьем
на следующей лекции.
А сейчас мы будем решать
вопрос с семинаром.
Тем, кто смотрел на YouTube,
всем спасибо.
Всем до следующего раза.

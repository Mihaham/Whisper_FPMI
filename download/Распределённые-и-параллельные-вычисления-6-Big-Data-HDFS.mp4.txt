[00:00.000 --> 00:09.000]  Мы с вами начинаем модуль, посвященный обработке больших данных.
[00:09.000 --> 00:12.000]  Вести его буду я, четыре лекции.
[00:12.000 --> 00:16.000]  Сначала мы поговорим про охранение больших данных,
[00:16.000 --> 00:18.000]  и потом еще три лекции про вычисление.
[00:18.000 --> 00:21.000]  После чего будет одна приглашенная лекция.
[00:21.000 --> 00:26.000]  Там вам расскажут про работу со Spark.
[00:26.000 --> 00:36.000]  Сначала начнем с того, что вообще такое большие данные.
[00:36.000 --> 00:43.000]  Большие данные – это какая-то отметка, после которой мы вдруг начали называть данные большими.
[00:43.000 --> 00:47.000]  А это данные, которые характеризуются тремя вот такими свойствами.
[00:47.000 --> 00:51.000]  Это volume, это, конечно же, объем, но он характеризуется не тем,
[00:51.000 --> 00:54.000]  что данных больше, чем пять терабайт, например,
[00:54.000 --> 00:58.000]  а тем, что мы просто не можем обработать эти данные на одном компьютере.
[00:58.000 --> 01:03.000]  У нас не хватает оперативки, либо працов, либо дисков, либо всего вместе.
[01:03.000 --> 01:08.000]  Второе свойство – это variety, то есть данные поступают из разных источников,
[01:08.000 --> 01:11.000]  с разной периодичностью, с разной структурой,
[01:11.000 --> 01:14.000]  и все это нужно как-то приводить к общему знаменателю.
[01:14.000 --> 01:20.000]  И третий – это velocity, то есть способ, каким мы обрабатываем большие данные.
[01:20.000 --> 01:25.000]  Либо это batch обработка, то есть с какой-то периодичностью мы
[01:25.000 --> 01:30.000]  процессим то, что у нас накопилось, получаем какой-то отчет,
[01:30.000 --> 01:33.000]  либо это обработка данных в реальном времени.
[01:35.000 --> 01:37.000]  Ну и по поводу хранения данных.
[01:37.000 --> 01:40.000]  Если у нас с вами не хватает одного сервера,
[01:40.000 --> 01:47.000]  то можно отмасштабироваться вертикально, то есть купить еще один сервер побольше,
[01:47.000 --> 01:51.000]  либо горизонтально и купить несколько серверов.
[01:51.000 --> 01:56.000]  Ну и понятно, что первый способ он рано или поздно себя изживет,
[01:56.000 --> 02:00.000]  у нас когда-нибудь этот огромный сервер ему все-таки перестанет хватать
[02:00.000 --> 02:02.000]  и придется покупать несколько.
[02:04.000 --> 02:07.000]  Вот, если несколько, то суммарная мощность увеличивается,
[02:07.000 --> 02:12.000]  но надо как-то распределять задачи между этими серверами.
[02:18.000 --> 02:23.000]  Надо как-то контролировать то, что у нас иногда сервера выходит из строя
[02:23.000 --> 02:26.000]  и чтобы система все равно работала.
[02:30.000 --> 02:32.000]  Ну и вообще нам нужна какая-то файловая система,
[02:32.000 --> 02:36.000]  которая все вот эти новые проблемы, которые у нас появились,
[02:36.000 --> 02:38.000]  она от нас это скроет.
[02:38.000 --> 02:44.000]  И хочется, чтобы мы работали с вот этой системой
[02:44.000 --> 02:47.000]  так же, как если бы мы работали у себя на компьютере.
[02:49.000 --> 02:51.000]  В принципе, это и есть распределенная система,
[02:51.000 --> 02:55.000]  когда с точки зрения пользователя это просто одна машинка,
[02:55.000 --> 02:58.000]  а под капотом работают несколько серверов с разными ролями
[02:58.000 --> 03:00.000]  и выполняют разные задачи.
[03:02.000 --> 03:06.000]  Такая система есть, она называется Google File System.
[03:07.000 --> 03:12.000]  Понятно, что первая компания, которая с большими данными столкнулась в реальной жизни,
[03:12.000 --> 03:13.000]  это Google.
[03:13.000 --> 03:17.000]  И вот еще в 2003 году, получается больше 15 лет,
[03:18.000 --> 03:21.000]  вышла статья, которая называется Google File System.
[03:21.000 --> 03:23.000]  Эта файловая система была закрыта,
[03:24.000 --> 03:27.000]  но по этой статье и по некоторым другим статьям
[03:28.000 --> 03:32.000]  появилась своя файловая система уже с открытым исходным кодом,
[03:32.000 --> 03:35.000]  который называется Hadoop Distributed File System.
[03:36.000 --> 03:39.000]  Hadoop – это фреймворк для вычисления поверх больших данных,
[03:39.000 --> 03:41.000]  поверх этой системы.
[03:41.000 --> 03:44.000]  И мы его тоже будем разбирать на следующих лекциях.
[03:47.000 --> 03:51.000]  Вот как у нас устроена GFS и как вслед за ней устроена HDFS.
[03:52.000 --> 03:56.000]  У нас есть три основных роли в этой системе.
[03:56.000 --> 03:58.000]  Первая роль – это клиент.
[03:58.000 --> 04:01.000]  На клиенте мы запускаем программы,
[04:02.000 --> 04:04.000]  используем файловую систему,
[04:04.000 --> 04:08.000]  но никаких содержательных данных мы тут не храним.
[04:08.000 --> 04:10.000]  Это как такая веб-морда,
[04:10.000 --> 04:12.000]  но на самом деле это не веб, а это сервер.
[04:14.000 --> 04:18.000]  Дальше у нас есть еще одна роль – это датонода,
[04:18.000 --> 04:20.000]  то есть узел данных.
[04:20.000 --> 04:22.000]  Здесь хранятся данные,
[04:22.000 --> 04:24.000]  они разбиваются на блоки большого размера,
[04:24.000 --> 04:26.000]  например, 64 мегабайт, 128,
[04:26.000 --> 04:28.000]  но это все настраивается.
[04:29.000 --> 04:31.000]  Кстати, кто помнит,
[04:31.000 --> 04:34.000]  какой размер блока в обычной файловой системе?
[04:34.000 --> 04:36.000]  Можете посмотреть размерчик?
[04:36.000 --> 04:38.000]  Размер блока.
[04:38.000 --> 04:40.000]  У нас файловая система не только HDFS,
[04:40.000 --> 04:42.000]  а любая файловая система,
[04:42.000 --> 04:44.000]  она имеет, не то что любая,
[04:44.000 --> 04:46.000]  но самая популярная,
[04:46.000 --> 04:48.000]  они имеют блокную структуру
[04:48.000 --> 04:50.000]  и там тоже есть размер блока.
[04:52.000 --> 04:54.000]  32?
[04:54.000 --> 04:56.000]  Нет, 32.
[04:56.000 --> 04:58.000]  32.
[04:58.000 --> 05:00.000]  32.
[05:00.000 --> 05:02.000]  32.
[05:02.000 --> 05:04.000]  Нет.
[05:04.000 --> 05:06.000]  Ну, бывает и 32, конечно,
[05:06.000 --> 05:08.000]  но на деле меньше, конечно.
[05:10.000 --> 05:12.000]  Значит, либо 8, либо 16.
[05:12.000 --> 05:14.000]  4 килобайта.
[05:14.000 --> 05:16.000]  Это легко проверить.
[05:16.000 --> 05:18.000]  Берете флешку, форматируете ее,
[05:18.000 --> 05:20.000]  например, под MTFS
[05:20.000 --> 05:22.000]  или под FAT32, не важно,
[05:22.000 --> 05:24.000]  FAT32 – это уже совсем старье.
[05:24.000 --> 05:26.000]  Закачиваете туда маленький файл,
[05:26.000 --> 05:28.000]  например, TXT,
[05:28.000 --> 05:30.000]  в котором хранится один символ.
[05:30.000 --> 05:32.000]  И если посмотрите на свойства флешки,
[05:32.000 --> 05:34.000]  сколько места там занято,
[05:34.000 --> 05:36.000]  вы увидите 4 килобайта.
[05:38.000 --> 05:40.000]  Так.
[05:40.000 --> 05:42.000]  Это что касается датонода,
[05:42.000 --> 05:44.000]  то есть устроено в принципе так же,
[05:44.000 --> 05:46.000]  только блоки большие.
[05:46.000 --> 05:48.000]  Есть еще name-нода.
[05:48.000 --> 05:50.000]  Это узел имен, и он хранит имена,
[05:50.000 --> 05:52.000]  он хранит пути к этим данным.
[05:52.000 --> 05:54.000]  То есть сервера разные
[05:54.000 --> 05:56.000]  и может быть такое,
[05:56.000 --> 05:58.000]  что файл разбит на блоке,
[05:58.000 --> 06:00.000]  блоки хранятся здесь, какие-то здесь,
[06:00.000 --> 06:02.000]  какие-то где-то еще в другом месте.
[06:02.000 --> 06:04.000]  И нужен какой-то единый мастер,
[06:04.000 --> 06:06.000]  который знает о том,
[06:06.000 --> 06:08.000]  где что хранится, и к нему мы приходим,
[06:08.000 --> 06:10.000]  когда мы хотим прочитать файл.
[06:10.000 --> 06:12.000]  Это и есть name-нода.
[06:12.000 --> 06:14.000]  Она хранит такой слепок
[06:14.000 --> 06:16.000]  файловой системы,
[06:16.000 --> 06:18.000]  его структура похожа на дерево.
[06:22.000 --> 06:24.000]  И это дерево все хранится
[06:24.000 --> 06:26.000]  в оперативной памяти.
[06:26.000 --> 06:28.000]  У кого есть идеи, почему?
[06:28.000 --> 06:30.000]  Зачем так сделали разработчики
[06:30.000 --> 06:32.000]  файловой системы,
[06:32.000 --> 06:34.000]  если оперативка это
[06:34.000 --> 06:36.000]  совсем ненадежная вещь,
[06:36.000 --> 06:38.000]  у нас любой ребут оперативку сбрасывает в ноль.
[06:40.000 --> 06:42.000]  Для быстроты?
[06:42.000 --> 06:44.000]  Да, для скорости доступа.
[06:44.000 --> 06:46.000]  Конечно, как мы увидим на практике,
[06:46.000 --> 06:48.000]  файловая система HDFS
[06:48.000 --> 06:50.000]  она вообще не про скорость,
[06:50.000 --> 06:52.000]  потому что нам надо
[06:52.000 --> 06:54.000]  на каждую команду идти
[06:54.000 --> 06:56.000]  по сети, сюда обращаться,
[06:56.000 --> 06:58.000]  потом сюда обращаться,
[06:58.000 --> 07:00.000]  но мы это все более подробно разберем.
[07:00.000 --> 07:02.000]  Пока только скажу, что HDFS
[07:02.000 --> 07:04.000]  действительно медленная,
[07:04.000 --> 07:06.000]  потому что там A. сеть и B. джава.
[07:06.000 --> 07:08.000]  HDFS написан на джаве
[07:08.000 --> 07:10.000]  и сам ходу план написан на джаве.
[07:16.000 --> 07:18.000]  Давайте посмотрим на архитектуру HDFS.
[07:18.000 --> 07:20.000]  Вот у нас есть
[07:20.000 --> 07:22.000]  name-нода.
[07:22.000 --> 07:24.000]  Это нанит вот это дерево файловой системы
[07:24.000 --> 07:26.000]  и когда мы работаем с файловой системой,
[07:26.000 --> 07:28.000]  мы постоянно
[07:28.000 --> 07:30.000]  какие-то файлы добавляем,
[07:30.000 --> 07:32.000]  какие-то удаляем, какие-то перемещаем.
[07:32.000 --> 07:34.000]  В общем, это дерево находится в таком динамическом изменении.
[07:36.000 --> 07:38.000]  Его нужно часто обновлять
[07:38.000 --> 07:40.000]  и вот это обновление
[07:40.000 --> 07:42.000]  занимает много ресурсов
[07:42.000 --> 07:44.000]  процессора. Пока просто держим это
[07:44.000 --> 07:46.000]  в голове, потом будем думать,
[07:46.000 --> 07:48.000]  что с этим делать.
[07:48.000 --> 07:50.000]  Какие проблемы в архитектуре
[07:50.000 --> 07:52.000]  вот в такой вы видите?
[07:58.000 --> 08:00.000]  Кто полегло,
[08:00.000 --> 08:02.000]  ничего не слышно было.
[08:02.000 --> 08:04.000]  Если проблема
[08:04.000 --> 08:06.000]  с name-нодой, то все
[08:06.000 --> 08:08.000]  недоступно. Да, ну и
[08:08.000 --> 08:10.000]  SPOV, единая точка отказа.
[08:12.000 --> 08:14.000]  Single Point of Failure.
[08:14.000 --> 08:16.000]  Что с этим делать?
[08:16.000 --> 08:18.000]  Ну, первое такое лобовое решение
[08:18.000 --> 08:20.000]  проблем, это давайте сделаем бэкап,
[08:20.000 --> 08:22.000]  чтобы у нас была не одна машинка,
[08:22.000 --> 08:24.000]  а несколько, ну две хотя бы.
[08:24.000 --> 08:26.000]  Чем это плохо?
[08:26.000 --> 08:28.000]  Вот эта name-нода, она большая,
[08:28.000 --> 08:30.000]  то есть там много оперативки,
[08:30.000 --> 08:32.000]  там много процессоров, она стоит дорого
[08:32.000 --> 08:34.000]  и нам придется держать еще
[08:34.000 --> 08:36.000]  одну такую же и плюс
[08:36.000 --> 08:38.000]  это большая нагрузка на сеть,
[08:38.000 --> 08:40.000]  потому что нам нужно синхронизироваться
[08:40.000 --> 08:42.000]  теперь с обеими name-нодами.
[08:42.000 --> 08:44.000]  То есть у нас
[08:44.000 --> 08:46.000]  у нас датоноды и клиент стучатся
[08:46.000 --> 08:48.000]  и в эту, и в какую-нибудь
[08:48.000 --> 08:50.000]  ее бэкап
[08:50.000 --> 08:52.000]  тоже.
[08:52.000 --> 08:54.000]  Не очень хорошо.
[08:54.000 --> 08:56.000]  Вторая такая
[08:56.000 --> 08:58.000]  попытка справиться
[08:58.000 --> 09:00.000]  с SPOV, это secondary name-нода.
[09:00.000 --> 09:02.000]  Следует сказать, что
[09:02.000 --> 09:04.000]  в литературе по ходу
[09:04.000 --> 09:06.000]  это одно из самых таких
[09:06.000 --> 09:08.000]  запутывающих названий, потому что
[09:08.000 --> 09:10.000]  secondary name-нода
[09:10.000 --> 09:12.000]  это вторичная name-нода, и кажется, что
[09:12.000 --> 09:14.000]  она должна быть заменой первичной,
[09:14.000 --> 09:16.000]  но на самом деле это не так.
[09:16.000 --> 09:18.000]  Все, что делает
[09:18.000 --> 09:20.000]  secondary name-нода, это
[09:20.000 --> 09:22.000]  вот эта функция,
[09:22.000 --> 09:24.000]  то есть у нее функция одна, по сути.
[09:24.000 --> 09:26.000]  Как мы знаем,
[09:26.000 --> 09:28.000]  хранится слепок файловой системы
[09:28.000 --> 09:30.000]  в памяти, и постоянно
[09:30.000 --> 09:32.000]  какие-то происходят изменения
[09:32.000 --> 09:34.000]  с этим слепком, и чтобы
[09:34.000 --> 09:36.000]  не заниматься постоянным,
[09:36.000 --> 09:38.000]  непрерывным обновлением
[09:38.000 --> 09:40.000]  этого слепка,
[09:40.000 --> 09:42.000]  оно происходит с каким-то периодом,
[09:42.000 --> 09:44.000]  и пока у нас обновление
[09:44.000 --> 09:46.000]  не произошло, все изменения
[09:46.000 --> 09:48.000]  мы записываем в edit log.
[09:48.000 --> 09:50.000]  То есть есть слепок файловой системы,
[09:50.000 --> 09:52.000]  и есть логи того,
[09:52.000 --> 09:54.000]  что мы с ним хотим сделать,
[09:54.000 --> 09:56.000]  как мы хотим его обновить.
[09:56.000 --> 09:58.000]  И вот это вот обновление, как я уже сказал,
[09:58.000 --> 10:00.000]  оно занимает много ресурсов процессора,
[10:00.000 --> 10:02.000]  поэтому name-нода отдает его
[10:02.000 --> 10:04.000]  на outsource.
[10:04.000 --> 10:06.000]  И вот secondary name-нода, SNN,
[10:06.000 --> 10:08.000]  она занимается тем, что получает
[10:08.000 --> 10:10.000]  FSImage, и она его хранит здесь,
[10:10.000 --> 10:12.000]  правда он устаревший,
[10:14.000 --> 10:16.000]  потому что логи изменений
[10:16.000 --> 10:18.000]  она получает с каким-то периодом,
[10:18.000 --> 10:20.000]  то есть она здесь хранит устаревший
[10:20.000 --> 10:22.000]  FSImage, получает логи,
[10:22.000 --> 10:24.000]  обновляет FSImage, возвращает обратно.
[10:24.000 --> 10:26.000]  Через какое-то время она снова запрашивает
[10:26.000 --> 10:28.000]  логи, вот query for edit logs
[10:28.000 --> 10:30.000]  in regular intervals,
[10:30.000 --> 10:32.000]  то есть с каким-то интервалом, достаточно
[10:32.000 --> 10:34.000]  небольшим интервалом, она запрашивает
[10:34.000 --> 10:36.000]  новые логи, мержит
[10:36.000 --> 10:38.000]  и возвращает на выход.
[10:38.000 --> 10:40.000]  Это все, что делает secondary name-нода.
[10:42.000 --> 10:44.000]  Вот добавили мы здесь SNN, но
[10:44.000 --> 10:46.000]  с единой точкой отказа мы не справились
[10:46.000 --> 10:48.000]  со всем, мы немного разгрузили, конечно,
[10:48.000 --> 10:50.000]  но вот
[10:50.000 --> 10:52.000]  надо тоже понимать, об этом
[10:52.000 --> 10:54.000]  спрашивают на собеседованиях, что
[10:54.000 --> 10:56.000]  secondary name-нода,
[10:56.000 --> 10:58.000]  она не является заменой name-ноды,
[10:58.000 --> 11:00.000]  она, конечно, хранит
[11:00.000 --> 11:02.000]  устаревший слепок файловой системы,
[11:02.000 --> 11:04.000]  если name-нода все,
[11:04.000 --> 11:06.000]  то с secondary можно
[11:06.000 --> 11:08.000]  восстановиться руками
[11:08.000 --> 11:10.000]  хотя бы часть информации, но это не заменой
[11:10.000 --> 11:12.000]  и не backup.
[11:14.000 --> 11:16.000]  Вот, ну и теперь еще попытки
[11:16.000 --> 11:18.000]  решить проблему со споффом,
[11:18.000 --> 11:20.000]  это HDFS Federation.
[11:24.000 --> 11:26.000]  То есть у нас...
[11:26.000 --> 11:28.000]  Сейчас можете повторить, пожалуйста, какие
[11:28.000 --> 11:30.000]  ну вот
[11:30.000 --> 11:32.000]  почему
[11:32.000 --> 11:34.000]  не как backup, то есть что она
[11:34.000 --> 11:36.000]  делает индивидуально?
[11:38.000 --> 11:40.000]  Ну то есть name-нода, помимо того,
[11:40.000 --> 11:42.000]  что она хранит слепок
[11:42.000 --> 11:44.000]  и хранит вот эти логи,
[11:44.000 --> 11:46.000]  она должна уметь отвечать на запросы.
[11:46.000 --> 11:48.000]  Вот, посмотрим еще раз на эту схему.
[11:48.000 --> 11:50.000]  Мы разберем отдельно,
[11:50.000 --> 11:52.000]  как происходит чтение в этой HDFS,
[11:52.000 --> 11:54.000]  как происходит запись,
[11:54.000 --> 11:56.000]  но главное, что name-нода, она должна
[11:56.000 --> 11:58.000]  уметь общаться
[11:58.000 --> 12:00.000]  с остальными серверами в этой системе,
[12:00.000 --> 12:02.000]  иметь для этого
[12:02.000 --> 12:04.000]  API специальное, по которому
[12:04.000 --> 12:06.000]  происходит взаимодействие. На это API мы
[12:06.000 --> 12:08.000]  тоже посмотрим.
[12:08.000 --> 12:10.000]  Вот, у secondary name-ноды такого API нет.
[12:10.000 --> 12:12.000]  То есть она не умеет
[12:12.000 --> 12:14.000]  быть name-нодой, она умеет
[12:14.000 --> 12:16.000]  просто заниматься слиянием
[12:16.000 --> 12:18.000]  слепкофайловой системы
[12:18.000 --> 12:20.000]  с логами и отдавать обратно.
[12:20.000 --> 12:22.000]  Все.
[12:24.000 --> 12:26.000]  Сейчас стало понять? Нет.
[12:30.000 --> 12:32.000]  Да, спасибо.
[12:32.000 --> 12:34.000]  Окей.
[12:34.000 --> 12:36.000]  Идем тогда дальше.
[12:36.000 --> 12:38.000]  Вот, HDFS Federation
[12:38.000 --> 12:40.000]  это несколько name-нод.
[12:40.000 --> 12:42.000]  То есть у нас есть файловая система.
[12:42.000 --> 12:44.000]  У нее есть там директории
[12:44.000 --> 12:46.000]  верхнего уровня, условно, там
[12:46.000 --> 12:48.000]  home, etc, bin и так далее.
[12:48.000 --> 12:50.000]  User.
[12:50.000 --> 12:52.000]  За каждую поддиректорию отвечает
[12:52.000 --> 12:54.000]  своя name-нода. Если у нас
[12:54.000 --> 12:56.000]  грохнется одна, мы хотя бы не потеряем все.
[12:56.000 --> 12:58.000]  Второе, это
[12:58.000 --> 13:00.000]  high availability name-нод.
[13:00.000 --> 13:02.000]  Можно подумать, что это как
[13:02.000 --> 13:04.000]  backup name-ноды.
[13:04.000 --> 13:06.000]  Но на самом деле это не так.
[13:06.000 --> 13:08.000]  Потому что у нас есть одна активная
[13:08.000 --> 13:10.000]  name-нода, остальные stand-by
[13:10.000 --> 13:12.000]  это значит, что они
[13:14.000 --> 13:16.000]  не подвергаются такой большой
[13:16.000 --> 13:18.000]  нагрузке со стороны
[13:18.000 --> 13:20.000]  клиента и датонод.
[13:20.000 --> 13:22.000]  Они могут хранить вот эти вот слепкие
[13:22.000 --> 13:24.000]  на диске. Не обязательно для
[13:24.000 --> 13:26.000]  этого иметь кучу памяти.
[13:26.000 --> 13:28.000]  Они конечно синхронизируются
[13:28.000 --> 13:30.000]  с основной name-нодой,
[13:30.000 --> 13:32.000]  но они не так нагружены и в принципе
[13:32.000 --> 13:34.000]  не обязаны быть таким же сильными, мощными.
[13:34.000 --> 13:36.000]  Если основная
[13:36.000 --> 13:38.000]  name-нода падает,
[13:38.000 --> 13:40.000]  то одна из вот этих stand-by
[13:40.000 --> 13:42.000]  подхватывает, как выбирается
[13:42.000 --> 13:44.000]  одна из этих stand-by.
[13:44.000 --> 13:46.000]  Это происходит алгоритм консенсуса,
[13:46.000 --> 13:48.000]  выбор лидера.
[13:48.000 --> 13:50.000]  То есть как бы
[13:50.000 --> 13:52.000]  каждый из name-нод голосует
[13:52.000 --> 13:54.000]  за другие.
[13:54.000 --> 13:56.000]  И выбирается какая-то одна.
[13:56.000 --> 13:58.000]  Она временно становится активной,
[13:58.000 --> 14:00.000]  она выгружает с дисков память
[14:00.000 --> 14:02.000]  слепок файловой системы.
[14:02.000 --> 14:04.000]  Если
[14:04.000 --> 14:06.000]  не хватает памяти,
[14:06.000 --> 14:08.000]  то в принципе мы можем работать
[14:08.000 --> 14:10.000]  прямо с диском, правда это будет долго.
[14:18.000 --> 14:20.000]  Но это что касается name-ноды
[14:20.000 --> 14:22.000]  и единой точки отказа.
[14:22.000 --> 14:24.000]  Скажите, есть какие-нибудь сейчас вопросы
[14:24.000 --> 14:26.000]  по работе name-ноды?
[14:30.000 --> 14:32.000]  А вот этот алгоритм консенсуса
[14:32.000 --> 14:34.000]  мы будем подробнее обсуждать?
[14:36.000 --> 14:38.000]  В разделе ходу
[14:38.000 --> 14:40.000]  не будем.
[14:40.000 --> 14:42.000]  Но когда у нас закончится вот этот блок,
[14:42.000 --> 14:44.000]  который я сказал,
[14:44.000 --> 14:46.000]  после моих лекций начинается у вас Роман Липовский,
[14:46.000 --> 14:48.000]  который там будет рассказывать
[14:48.000 --> 14:50.000]  про консенсус,
[14:50.000 --> 14:52.000]  про Каптиорему,
[14:52.000 --> 14:54.000]  в применении уже скорее
[14:54.000 --> 14:56.000]  не к ходу, а к Кавке.
[15:00.000 --> 15:02.000]  То есть вопрос,
[15:02.000 --> 15:04.000]  по какому принципу они голосуют
[15:04.000 --> 15:06.000]  будет
[15:06.000 --> 15:08.000]  на лекциях освещаться?
[15:10.000 --> 15:12.000]  Если кратко,
[15:12.000 --> 15:14.000]  то в принципе
[15:14.000 --> 15:16.000]  по крайней мере в первых версиях
[15:16.000 --> 15:18.000]  ходу голосование происходило
[15:18.000 --> 15:20.000]  просто рандомно.
[15:22.000 --> 15:24.000]  То есть кто больше набрал голосов,
[15:24.000 --> 15:26.000]  тот и становится лидером.
[15:30.000 --> 15:32.000]  Идем дальше тогда.
[15:32.000 --> 15:34.000]  Датоноды.
[15:34.000 --> 15:36.000]  Что такое датоноды?
[15:36.000 --> 15:38.000]  Это обычные сервера под линуксом.
[15:38.000 --> 15:40.000]  Нейнод тоже обычный сервер под линуксом.
[15:42.000 --> 15:44.000]  И на этих серверах есть специальные папки,
[15:44.000 --> 15:46.000]  в которых хранятся блоки.
[15:48.000 --> 15:50.000]  Если им нод много,
[15:50.000 --> 15:52.000]  и блоков тоже много,
[15:52.000 --> 15:54.000]  при падении одной датоноды
[15:54.000 --> 15:56.000]  у нас получится,
[15:56.000 --> 15:58.000]  что часть блоков пропадет.
[15:58.000 --> 16:00.000]  Это разрешать нельзя,
[16:00.000 --> 16:02.000]  поэтому у нас есть репликация.
[16:02.000 --> 16:04.000]  Каждый блок копируется
[16:04.000 --> 16:06.000]  на несколько датонод.
[16:08.000 --> 16:10.000]  И если у нас выпадет
[16:10.000 --> 16:12.000]  одна датонода или несколько,
[16:12.000 --> 16:14.000]  мы этого даже не заметим.
[16:14.000 --> 16:16.000]  У нас будут копии блоков,
[16:16.000 --> 16:18.000]  которые у нас есть.
[16:18.000 --> 16:20.000]  Помимо этого,
[16:20.000 --> 16:22.000]  нейнод хранит информацию
[16:22.000 --> 16:24.000]  о расположении датоноды.
[16:24.000 --> 16:26.000]  Вот это, кстати, к слову,
[16:26.000 --> 16:28.000]  о том, почему секондрией нейнода
[16:28.000 --> 16:30.000]  не может заменить нейнод.
[16:30.000 --> 16:32.000]  Но секондрией нейнода
[16:32.000 --> 16:34.000]  вот этого всего она не хранит.
[16:34.000 --> 16:36.000]  Нейнод хранит данные о топологии сети,
[16:36.000 --> 16:38.000]  где какая стойка,
[16:38.000 --> 16:40.000]  где какой дата-центр.
[16:40.000 --> 16:42.000]  И при записи блоков в HDFS
[16:42.000 --> 16:44.000]  она старается эти блоки
[16:44.000 --> 16:46.000]  Кстати, скажите, чем это хорошо
[16:46.000 --> 16:48.000]  и чем это плохо?
[16:54.000 --> 16:56.000]  Если какой-то дата-центр
[16:56.000 --> 16:58.000]  потом произошел,
[16:58.000 --> 17:00.000]  то у нас все не будет
[17:00.000 --> 17:02.000]  в одном дата-центре,
[17:02.000 --> 17:04.000]  хоть там и несколько серверов.
[17:06.000 --> 17:08.000]  Хорошо, а чем это плохо?
[17:14.000 --> 17:16.000]  Вот у нас дата-локалити.
[17:16.000 --> 17:18.000]  Если произошло что-то плохое,
[17:18.000 --> 17:20.000]  мы хотя бы не потеряем все.
[17:22.000 --> 17:24.000]  А чем это плохо?
[17:40.000 --> 17:42.000]  Какие-нибудь идеи есть?
[17:44.000 --> 17:46.000]  Нагрузка на сеть
[17:46.000 --> 17:48.000]  далеко пересылать?
[17:48.000 --> 17:50.000]  Да.
[17:50.000 --> 17:52.000]  Даже дело не в самой нагрузке на сеть,
[17:52.000 --> 17:54.000]  а в том, что сеть просто
[17:54.000 --> 17:56.000]  длинная, медленная,
[17:56.000 --> 17:58.000]  через много серверов нужно ходить
[17:58.000 --> 18:00.000]  и это будет долго.
[18:02.000 --> 18:04.000]  Вот картинка, где мы видим,
[18:04.000 --> 18:06.000]  как у нас разносятся блоки
[18:06.000 --> 18:08.000]  по разным нодам.
[18:08.000 --> 18:10.000]  То есть мы пишем файлик
[18:10.000 --> 18:12.000]  вот этого,
[18:12.000 --> 18:14.000]  вот этот и вот этот.
[18:14.000 --> 18:16.000]  И вот у нас его блоки.
[18:16.000 --> 18:18.000]  С0 находится здесь и здесь,
[18:18.000 --> 18:20.000]  как можно дальше.
[18:20.000 --> 18:22.000]  С1 находится рядом,
[18:22.000 --> 18:24.000]  С3 находится здесь
[18:24.000 --> 18:26.000]  и где-то еще.
[18:26.000 --> 18:28.000]  Почему вот такой разброс
[18:28.000 --> 18:30.000]  и почему некоторые блоки
[18:30.000 --> 18:32.000]  все же находятся рядом?
[18:32.000 --> 18:34.000]  Потому что у HDFS есть еще такой
[18:34.000 --> 18:36.000]  процесс, который запускается
[18:36.000 --> 18:38.000]  тоже с некой периодичностью,
[18:38.000 --> 18:40.000]  это балансер.
[18:40.000 --> 18:42.000]  И смотрит, если какая-то дата нода
[18:42.000 --> 18:44.000]  перегружена,
[18:44.000 --> 18:46.000]  то он блоки с нее перекидывает
[18:46.000 --> 18:48.000]  на другие дата ноды.
[18:48.000 --> 18:50.000]  Сообщает, конечно, об этом
[18:50.000 --> 18:52.000]  в нейм ноде, чтобы она понимала, где лежат блоки.
[18:52.000 --> 18:54.000]  Помимо этого, если какая-то
[18:54.000 --> 18:56.000]  дата нода у нас недоступна
[18:56.000 --> 18:58.000]  и пропала часть реплик,
[18:58.000 --> 19:00.000]  часть копий блоков,
[19:00.000 --> 19:02.000]  то балансер докопирует эти блоки
[19:02.000 --> 19:04.000]  и чтобы коэффициент репликации
[19:04.000 --> 19:06.000]  был такой, какой он должен быть.
[19:10.000 --> 19:12.000]  Вот.
[19:20.000 --> 19:22.000]  То есть так у нас выглядят
[19:22.000 --> 19:24.000]  ноды.
[19:24.000 --> 19:26.000]  Вот у нас блок 1,
[19:26.000 --> 19:28.000]  он хранится здесь, здесь и здесь,
[19:28.000 --> 19:30.000]  и вот у него реплика 1, реплика 3, реплика 2,
[19:30.000 --> 19:32.000]  то есть вот так хранятся копии,
[19:32.000 --> 19:34.000]  они хранятся на разных нодах.
[19:40.000 --> 19:42.000]  Вот.
[19:42.000 --> 19:44.000]  Еще я вам сказал, что блоки
[19:44.000 --> 19:46.000]  одинакового размера,
[19:46.000 --> 19:48.000]  но на самом деле это не всегда так.
[19:48.000 --> 19:50.000]  Дело в том, что у нас
[19:50.000 --> 19:52.000]  есть такая проблема,
[19:52.000 --> 19:54.000]  и она известная проблема в ходу,
[19:54.000 --> 19:56.000]  называется проблема маленьких файлов.
[19:56.000 --> 19:58.000]  Что это значит?
[19:58.000 --> 20:00.000]  Если мы захотим в HDFS
[20:00.000 --> 20:02.000]  залить большущий файл,
[20:02.000 --> 20:04.000]  то HDFS сам разобьет его на блоки,
[20:04.000 --> 20:06.000]  сделает реплики,
[20:06.000 --> 20:08.000]  разнесет по нодам
[20:08.000 --> 20:10.000]  и с этим справится.
[20:10.000 --> 20:12.000]  То есть для сколь угодно больших данных
[20:12.000 --> 20:14.000]  HDFS будет работать в порядке.
[20:14.000 --> 20:16.000]  Но главное, чтоб места хватило.
[20:16.000 --> 20:18.000]  А что делать, если файлики маленькие?
[20:18.000 --> 20:20.000]  Вот слепить два
[20:20.000 --> 20:22.000]  или три файла в один блок HDFS
[20:22.000 --> 20:24.000]  не может.
[20:24.000 --> 20:26.000]  Ну просто потому, что мы потеряем
[20:26.000 --> 20:28.000]  разделители между файлами.
[20:28.000 --> 20:30.000]  Поэтому что у нас получается?
[20:30.000 --> 20:32.000]  У нас получается,
[20:32.000 --> 20:34.000]  если файлик маленький,
[20:34.000 --> 20:36.000]  то на него все равно создается блок.
[20:36.000 --> 20:38.000]  Блок меньшего размера.
[20:38.000 --> 20:40.000]  Но проблема не в том, что блок
[20:40.000 --> 20:42.000]  меньшего размера, а проблема в том,
[20:42.000 --> 20:44.000]  что на каждый такой блок у нас хранится
[20:44.000 --> 20:46.000]  метаинформация вот здесь,
[20:46.000 --> 20:48.000]  в нейм-ноде.
[20:48.000 --> 20:50.000]  Вот здесь.
[20:50.000 --> 20:52.000]  И метаинформация хранится точно такая же.
[20:52.000 --> 20:54.000]  Вот что блок полноценный,
[20:54.000 --> 20:56.000]  что он маленький.
[20:56.000 --> 20:58.000]  Поэтому получается, если у нас
[20:58.000 --> 21:00.000]  много маленьких блоков,
[21:00.000 --> 21:02.000]  то нейм-ноду мы загрузим,
[21:02.000 --> 21:04.000]  а на диске останется много места.
[21:04.000 --> 21:06.000]  И вот это вот, конечно, плохо.
[21:08.000 --> 21:10.000]  Почему еще бывают
[21:10.000 --> 21:12.000]  блоки не всегда одинакового размера?
[21:12.000 --> 21:14.000]  Вот у нас есть какой-то
[21:14.000 --> 21:16.000]  большой файл. Мы его разбили
[21:16.000 --> 21:18.000]  на блоки, но понятно, что
[21:18.000 --> 21:20.000]  кратно на размер
[21:20.000 --> 21:22.000]  блока вряд ли мы его разобьем.
[21:22.000 --> 21:24.000]  Последний блок будет меньше.
[21:24.000 --> 21:26.000]  И это вот тоже тот случай,
[21:26.000 --> 21:28.000]  последний блок может быть меньше.
[21:30.000 --> 21:32.000]  Сейчас я вам даже покажу это на примере.
[21:34.000 --> 21:36.000]  Так.
[21:42.000 --> 21:44.000]  Скоро вам придут доступы
[21:44.000 --> 21:46.000]  на вот этот квантор,
[21:46.000 --> 21:48.000]  на котором вы будете делать домашки
[21:48.000 --> 21:50.000]  все после куба.
[21:58.000 --> 22:00.000]  Так.
[22:00.000 --> 22:02.000]  Хорошо.
[22:04.000 --> 22:06.000]  Ну и вот давайте посмотрим
[22:06.000 --> 22:08.000]  на...
[22:08.000 --> 22:10.000]  Это у нас юзер-интерфейс нейм-ноды.
[22:10.000 --> 22:12.000]  И пойдем побродим
[22:12.000 --> 22:14.000]  по файловой системе.
[22:14.000 --> 22:16.000]  Найдем какой-нибудь файлик побольше.
[22:18.000 --> 22:20.000]  Вот, например,
[22:20.000 --> 22:22.000]  файл, который содержит
[22:22.000 --> 22:24.000]  12 гигов.
[22:24.000 --> 22:26.000]  И посмотрим,
[22:26.000 --> 22:28.000]  какой он будет.
[22:28.000 --> 22:30.000]  Вот, например,
[22:30.000 --> 22:32.000]  файл, который содержит
[22:32.000 --> 22:34.000]  12 гигов.
[22:34.000 --> 22:36.000]  И посмотрим на его блоки.
[22:36.000 --> 22:38.000]  Давайте посмотрим, как у нас вообще
[22:38.000 --> 22:40.000]  хранятся блоки, какого они размера.
[22:40.000 --> 22:42.000]  Вот первый блок, второй, третий.
[22:42.000 --> 22:44.000]  Что мы видим? Что меняется?
[22:44.000 --> 22:46.000]  ID-блока. Оно у каждого свое.
[22:46.000 --> 22:48.000]  Blockpool — это
[22:48.000 --> 22:50.000]  такая вот структура.
[22:50.000 --> 22:52.000]  Она относится
[22:52.000 --> 22:54.000]  к топологии сети.
[22:54.000 --> 22:56.000]  То есть, если у нас
[22:56.000 --> 22:58.000]  много дата-центров,
[22:58.000 --> 23:00.000]  много стоек, мы об этом сообщаем
[23:00.000 --> 23:02.000]  в специальном конфиге-нейм-ноде,
[23:02.000 --> 23:04.000]  то она делает несколько блок-пулов.
[23:04.000 --> 23:06.000]  В данном случае
[23:06.000 --> 23:08.000]  на нашем ущельном кластере
[23:08.000 --> 23:10.000]  фактически стойка одна, дата-центр один
[23:10.000 --> 23:12.000]  и блок-пул тоже один.
[23:12.000 --> 23:14.000]  Вот это вот некая константа,
[23:14.000 --> 23:16.000]  на которую можно просто не обращать внимания.
[23:16.000 --> 23:18.000]  Дальше, generation-stamp —
[23:18.000 --> 23:20.000]  это версия.
[23:20.000 --> 23:22.000]  То есть, мы работаем
[23:22.000 --> 23:24.000]  с распределенной системой,
[23:24.000 --> 23:26.000]  где машинки могут отваливаться.
[23:26.000 --> 23:28.000]  И может быть такое,
[23:28.000 --> 23:30.000]  что мы начали запись в HDFS,
[23:30.000 --> 23:32.000]  машинка отвалилась,
[23:32.000 --> 23:34.000]  потом она поднялась,
[23:34.000 --> 23:36.000]  но на ней содержится более старая версия
[23:36.000 --> 23:38.000]  этого же блока.
[23:38.000 --> 23:40.000]  Как система понимает, что версия более старая,
[23:40.000 --> 23:42.000]  вот как раз поэтому generation-stamp.
[23:42.000 --> 23:44.000]  Дальше — размер.
[23:44.000 --> 23:46.000]  И после размера
[23:46.000 --> 23:48.000]  список узлов,
[23:48.000 --> 23:50.000]  на которых хранятся
[23:50.000 --> 23:52.000]  реплики этого блока.
[23:54.000 --> 23:56.000]  Вот давайте покликаем по разным блокам,
[23:56.000 --> 23:58.000]  потому что меняются ноты,
[23:58.000 --> 24:00.000]  меняется ID, меняется stamp,
[24:00.000 --> 24:02.000]  а вот размер не меняется нигде.
[24:02.000 --> 24:04.000]  Опять не поменялся.
[24:04.000 --> 24:06.000]  Опять не поменялся.
[24:06.000 --> 24:08.000]  Идем до самого конца.
[24:08.000 --> 24:10.000]  Вот 90-й блок. Опять не поменялся.
[24:10.000 --> 24:12.000]  91-й.
[24:12.000 --> 24:14.000]  Поменялся.
[24:14.000 --> 24:16.000]  Последний блок чуть-чуть меньше.
[24:16.000 --> 24:18.000]  Есть еще
[24:18.000 --> 24:20.000]  один случай,
[24:20.000 --> 24:22.000]  когда блоки другого размера —
[24:22.000 --> 24:24.000]  это когда последний блок
[24:24.000 --> 24:26.000]  совсем маленький,
[24:26.000 --> 24:28.000]  ну и, как я уже сказал,
[24:28.000 --> 24:30.000]  нам очень невыгодно хранить маленькие блоки в HDFS,
[24:30.000 --> 24:32.000]  поэтому он прикрепляется
[24:32.000 --> 24:34.000]  к предпоследнему, и предпоследний
[24:34.000 --> 24:36.000]  становится чуть больше.
[24:36.000 --> 24:38.000]  Совсем маленький — это там несколько процентов
[24:38.000 --> 24:40.000]  от размера блоков.
[24:48.000 --> 24:50.000]  Вот, скоро вам придут доступы
[24:50.000 --> 24:52.000]  такого типа, правда,
[24:52.000 --> 24:54.000]  будет по-другому называться хвост,
[24:54.000 --> 24:56.000]  но примерно такие письма вам придут.
[25:00.000 --> 25:02.000]  Дальше. Теперь поговорим
[25:02.000 --> 25:04.000]  про чтение и запись. Пока мы к нему
[25:04.000 --> 25:06.000]  не перешли, по поводу хранения
[25:06.000 --> 25:08.000]  блоков какой-нибудь вопрос есть?
[25:16.000 --> 25:18.000]  Где все? Почему-то у нас
[25:18.000 --> 25:20.000]  аж 9
[25:20.000 --> 25:22.000]  и 100.
[25:26.000 --> 25:28.000]  На предыдущей лекции
[25:28.000 --> 25:30.000]  примерно так и ходило,
[25:30.000 --> 25:32.000]  может, чуть-чуть побольше.
[25:32.000 --> 25:34.000]  Ну окей, вы все-таки
[25:34.000 --> 25:36.000]  старайтесь ходить, тем более
[25:36.000 --> 25:38.000]  лекции сейчас удаленные,
[25:38.000 --> 25:40.000]  и, возможно, у нас получится пригласить
[25:40.000 --> 25:42.000]  очень крутого специалиста
[25:42.000 --> 25:44.000]  из заграничной компании,
[25:44.000 --> 25:46.000]  но он будет вести лекцию на русском,
[25:46.000 --> 25:48.000]  но это прям крутой специалист
[25:48.000 --> 25:50.000]  в Spark.
[25:52.000 --> 25:54.000]  Давайте возвращаемся к блокам.
[25:54.000 --> 25:56.000]  Я так понимаю, что пока вопросов нет.
[25:56.000 --> 25:58.000]  Тогда идем дальше.
[26:00.000 --> 26:02.000]  И поговорим про чтение и запись.
[26:02.000 --> 26:04.000]  Вообще ходу
[26:04.000 --> 26:06.000]  как бы работает по
[26:06.000 --> 26:08.000]  парадигме
[26:08.000 --> 26:10.000]  write once, read many.
[26:10.000 --> 26:12.000]  То есть пишем редко,
[26:12.000 --> 26:14.000]  читаем часто. Сейчас мы поймем почему.
[26:14.000 --> 26:16.000]  Когда мы читаем, вот что происходит.
[26:16.000 --> 26:18.000]  Мы запрашиваем
[26:18.000 --> 26:20.000]  у NameNode
[26:20.000 --> 26:22.000]  какой-нибудь блок, который хотим прочитать.
[26:22.000 --> 26:24.000]  Точнее, мы хотим прочитать
[26:24.000 --> 26:26.000]  не блок, а файл обычно.
[26:26.000 --> 26:28.000]  Мы говорим, мы хотим прочитать такой-то файл.
[26:28.000 --> 26:30.000]  NameNode говорит,
[26:30.000 --> 26:32.000]  что он находится там-то, на каких-то блоках.
[26:32.000 --> 26:34.000]  Вот он отправляет назад блок ID,
[26:34.000 --> 26:36.000]  блок location.
[26:36.000 --> 26:38.000]  И дальше клиент уже без участия
[26:38.000 --> 26:40.000]  NameNode идет напрямую к datanode,
[26:40.000 --> 26:42.000]  причем к ближайшей datanode.
[26:42.000 --> 26:44.000]  То есть у нас блоки
[26:44.000 --> 26:46.000]  сфотографированы, их копии хранятся
[26:46.000 --> 26:48.000]  на разных машинках.
[26:48.000 --> 26:50.000]  Клиент выбирает ближайшую из этих копий
[26:50.000 --> 26:52.000]  и ее выкачивает.
[26:52.000 --> 26:54.000]  И назад получает блок дейта.
[26:54.000 --> 26:56.000]  Жирная стрелочка.
[26:56.000 --> 26:58.000]  Ну и
[26:58.000 --> 27:00.000]  NameNode дает инструкции
[27:00.000 --> 27:02.000]  к datanode.
[27:02.000 --> 27:04.000]  Например, балансер, который тут работает,
[27:04.000 --> 27:06.000]  он может перемещать блоки.
[27:06.000 --> 27:08.000]  В свою очередь datanode отправляют
[27:08.000 --> 27:10.000]  к NameNode hard биты.
[27:10.000 --> 27:12.000]  Типа, у нас все хорошо, у нас все хорошо,
[27:12.000 --> 27:14.000]  мы готовы работать.
[27:14.000 --> 27:16.000]  Если они hard биты отправлять перестают,
[27:16.000 --> 27:18.000]  то NameNode думает,
[27:18.000 --> 27:20.000]  что datanode умерла.
[27:20.000 --> 27:22.000]  Бывают еще
[27:22.000 --> 27:24.000]  более выраженные случаи чтения,
[27:24.000 --> 27:26.000]  когда мы, например,
[27:26.000 --> 27:28.000]  не читаем файл какой-то,
[27:28.000 --> 27:30.000]  а мы выполняем команду типа ls.
[27:34.000 --> 27:36.000]  И вот для того, чтобы сделать ls,
[27:36.000 --> 27:38.000]  нам не нужно идти в datanode,
[27:38.000 --> 27:40.000]  нам нужно пойти только в NameNode,
[27:40.000 --> 27:42.000]  и какие файлы по названиям
[27:42.000 --> 27:44.000]  в этой папке хранятся.
[27:46.000 --> 27:48.000]  Что касается записи.
[27:48.000 --> 27:50.000]  Запись происходит медленно,
[27:50.000 --> 27:52.000]  потому что она происходит вот так.
[27:52.000 --> 27:54.000]  Мы хотим добавить файл,
[27:54.000 --> 27:56.000]  идем с этим к NameNode.
[27:56.000 --> 27:58.000]  NameNode нам выдает
[27:58.000 --> 28:00.000]  место в datanode,
[28:00.000 --> 28:02.000]  куда мы можем записать наши блоки.
[28:02.000 --> 28:04.000]  И мы начинаем писать тоже напрямую
[28:04.000 --> 28:06.000]  в datanode.
[28:06.000 --> 28:08.000]  Записали на первую,
[28:08.000 --> 28:10.000]  записали во вторую,
[28:10.000 --> 28:12.000]  во вторую мы пишем уже
[28:12.000 --> 28:14.000]  не с клиента, а с datanode,
[28:14.000 --> 28:16.000]  то есть тут такой вот каскад идет
[28:16.000 --> 28:18.000]  datanode, друг другу
[28:18.000 --> 28:20.000]  передает вот этот новый блок.
[28:20.000 --> 28:22.000]  Мы его пишем,
[28:22.000 --> 28:24.000]  каждая datanode отчитывается в NameNode,
[28:24.000 --> 28:26.000]  что запись завершена,
[28:26.000 --> 28:28.000]  и сама команда завершится только тогда,
[28:28.000 --> 28:30.000]  когда мы закончим всю эту запись.
[28:30.000 --> 28:32.000]  То есть когда на всех
[28:32.000 --> 28:34.000]  datanode появится блок.
[28:34.000 --> 28:36.000]  Такой способ записи
[28:36.000 --> 28:38.000]  называется синхронная репликация,
[28:38.000 --> 28:40.000]  то есть пока мы не завершили
[28:40.000 --> 28:42.000]  все, мы OK не получаем.
[28:42.000 --> 28:44.000]  Есть еще асинхронная репликация,
[28:44.000 --> 28:46.000]  она у нас будет с вами в кавке.
[28:46.000 --> 28:48.000]  Когда мы записываем
[28:50.000 --> 28:52.000]  информацию только
[28:52.000 --> 28:54.000]  на какое-то подмножество серверов,
[28:54.000 --> 28:56.000]  получаем OK, а остальное
[28:56.000 --> 28:58.000]  доделываем уже в фоне,
[28:58.000 --> 29:00.000]  в background.
[29:00.000 --> 29:02.000]  Хорошо, есть ли какие-то вопросы
[29:02.000 --> 29:04.000]  по поводу учтения записи?
[29:30.000 --> 29:32.000]  Хорошо, идем дальше.
[29:32.000 --> 29:34.000]  Посмотрим на UI HDFS,
[29:34.000 --> 29:36.000]  что у нас тут вообще есть.
[29:40.000 --> 29:42.000]  Вот здесь выдается
[29:42.000 --> 29:44.000]  информация о том,
[29:44.000 --> 29:46.000]  что какие-то блоки могут быть потеряны.
[29:46.000 --> 29:48.000]  Например, у нас упало
[29:48.000 --> 29:50.000]  сразу несколько машинок
[29:50.000 --> 29:52.000]  или произошло что-то с диском.
[29:52.000 --> 29:54.000]  В общем, все реплики у нас
[29:54.000 --> 29:56.000]  пропали или недоступны.
[29:56.000 --> 29:58.000]  Давайте посмотрим, что у нас здесь.
[29:58.000 --> 30:00.000]  Здесь сказано, сколько мы
[30:00.000 --> 30:02.000]  можем хранить данных в HDFS,
[30:02.000 --> 30:04.000]  сколько у нас занято.
[30:04.000 --> 30:06.000]  Non-DFS used
[30:06.000 --> 30:08.000]  это сколько у нас используется
[30:08.000 --> 30:10.000]  не для нужды HDFS.
[30:10.000 --> 30:12.000]  Кстати, давайте подумаем,
[30:12.000 --> 30:14.000]  а какие могут быть нужды помимо HDFS?
[30:14.000 --> 30:16.000]  Вот представим, что у нас
[30:16.000 --> 30:18.000]  на сервере есть
[30:18.000 --> 30:20.000]  только HDFS,
[30:20.000 --> 30:22.000]  нет там никаких других систем,
[30:22.000 --> 30:24.000]  нет Hadoop, нет Spark,
[30:24.000 --> 30:26.000]  только хранилка, все.
[30:28.000 --> 30:30.000]  Куда могут идти вот эти вот
[30:30.000 --> 30:32.000]  вот эта вот часть,
[30:32.000 --> 30:34.000]  на что она может расходоваться?
[30:38.000 --> 30:40.000]  Имперационная система?
[30:40.000 --> 30:42.000]  Да.
[30:42.000 --> 30:44.000]  Все сервера, на которых работает Hadoop,
[30:44.000 --> 30:46.000]  это прежде всего обычные машинки,
[30:46.000 --> 30:48.000]  на которых стоит
[30:48.000 --> 30:50.000]  почти всегда Windows,
[30:50.000 --> 30:52.000]  очень редко какие-то другие оси.
[30:52.000 --> 30:54.000]  И на то, чтобы обслуживать этот Linux,
[30:54.000 --> 30:56.000]  конечно, нужно место.
[30:58.000 --> 31:00.000]  Дальше можем увидеть, сколько у нас
[31:00.000 --> 31:02.000]  живых нод, как они между собой
[31:02.000 --> 31:04.000]  распределены, то есть видите,
[31:04.000 --> 31:06.000]  балансер
[31:06.000 --> 31:08.000]  отработал, и в принципе
[31:08.000 --> 31:10.000]  все ноды, на всех нодах
[31:10.000 --> 31:12.000]  информации по поровну,
[31:12.000 --> 31:14.000]  кроме пятой. Почему на пятой информации мало?
[31:14.000 --> 31:16.000]  Потому что
[31:16.000 --> 31:18.000]  она лежала, и только
[31:18.000 --> 31:20.000]  недавно поднялась.
[31:22.000 --> 31:24.000]  И что-то у нас
[31:24.000 --> 31:26.000]  уже с вами семя.
[31:34.000 --> 31:36.000]  Есть Rest API,
[31:36.000 --> 31:38.000]  можно по нему
[31:38.000 --> 31:40.000]  обращаться к файловой системе
[31:40.000 --> 31:42.000]  и делать всякие операции
[31:42.000 --> 31:44.000]  с файлами, и именно поэтому
[31:44.000 --> 31:46.000]  порт вот этот закрыт.
[31:46.000 --> 31:48.000]  То есть, когда я заходил
[31:48.000 --> 31:50.000]  на кластер, вы видели,
[31:50.000 --> 31:52.000]  что я использовал вот такую вот
[31:52.000 --> 31:54.000]  команду
[31:54.000 --> 31:56.000]  с пробросом портов.
[31:58.000 --> 32:00.000]  То есть, порт наружу закрыт, мы делаем
[32:00.000 --> 32:02.000]  SSH-туннель. Почему мы его делаем?
[32:02.000 --> 32:04.000]  Потому что в принципе HDFS
[32:04.000 --> 32:06.000]  не очень хорош в плане безопасности.
[32:06.000 --> 32:08.000]  Если какая-то вебмонта торчит наружу,
[32:08.000 --> 32:10.000]  то любой желающий
[32:10.000 --> 32:12.000]  сможет подключиться
[32:12.000 --> 32:14.000]  к ней по Rest API, например,
[32:14.000 --> 32:16.000]  и что-нибудь нехорошее
[32:16.000 --> 32:18.000]  сделать в файловой системе.
[32:20.000 --> 32:22.000]  Вот.
[32:22.000 --> 32:24.000]  Какой есть API у HDFS
[32:24.000 --> 32:26.000]  и как мы можем с ним
[32:26.000 --> 32:28.000]  взаимодействовать? Можно взаимодействовать
[32:28.000 --> 32:30.000]  через HDFS Shell.
[32:30.000 --> 32:32.000]  Это просто набор команд,
[32:32.000 --> 32:34.000]  похожих на линуксовые.
[32:38.000 --> 32:40.000]  То есть, вот мы пишем
[32:40.000 --> 32:42.000]  префикс HadoopFS
[32:42.000 --> 32:44.000]  либо HDFSDFS.
[32:44.000 --> 32:46.000]  И так и так можно.
[32:46.000 --> 32:48.000]  Но правда, HadoopFS это уже
[32:48.000 --> 32:50.000]  устаревшая
[32:50.000 --> 32:52.000]  как бы
[32:52.000 --> 32:54.000]  устаревшая запись,
[32:54.000 --> 32:56.000]  лучше использовать
[32:56.000 --> 32:58.000]  HDFSDFS.
[33:00.000 --> 33:02.000]  Вот, например,
[33:02.000 --> 33:04.000]  HDFSDFS
[33:04.000 --> 33:06.000]  MinusPlusVib
[33:08.000 --> 33:10.000]  И вот мы вывели всех пользователей,
[33:10.000 --> 33:12.000]  которые сейчас у нас есть на класты.
[33:18.000 --> 33:20.000]  Поинма Shell
[33:20.000 --> 33:22.000]  есть еще JavaP.
[33:22.000 --> 33:24.000]  Как я уже сказал, Hadoop написан на Java
[33:24.000 --> 33:26.000]  и у него есть
[33:26.000 --> 33:28.000]  нативное JavaP.
[33:28.000 --> 33:30.000]  Есть еще Rest API.
[33:30.000 --> 33:32.000]  Мы на него тоже посмотрим чуть позже.
[33:34.000 --> 33:36.000]  Ну и есть еще всякие обертки.
[33:36.000 --> 33:38.000]  На семинарах их подробнее
[33:38.000 --> 33:40.000]  обсудим.
[33:40.000 --> 33:42.000]  Обертки на Python.
[33:42.000 --> 33:44.000]  Есть библиотека HadoopPipes
[33:44.000 --> 33:46.000]  на плюсах.
[33:48.000 --> 33:50.000]  Вот.
[33:50.000 --> 33:52.000]  Ну и давайте теперь разберем
[33:52.000 --> 33:54.000]  немного кода.
[34:02.000 --> 34:04.000]  Сейчас я выполню вот эту команду
[34:04.000 --> 34:06.000]  и посмотрим,
[34:06.000 --> 34:08.000]  что будет.
[34:08.000 --> 34:10.000]  То есть это
[34:10.000 --> 34:12.000]  Rest API.
[34:14.000 --> 34:16.000]  Все знакомы с командой Curl
[34:16.000 --> 34:18.000]  с такой утилитой.
[34:22.000 --> 34:24.000]  Ну, вроде на косе было,
[34:24.000 --> 34:26.000]  но не очень.
[34:26.000 --> 34:28.000]  Нет.
[34:28.000 --> 34:30.000]  На косе должно было быть, конечно.
[34:32.000 --> 34:34.000]  Ну, может быть, подпустили.
[34:34.000 --> 34:36.000]  По сути,
[34:36.000 --> 34:38.000]  Curl выкачивает веб-страницу
[34:40.000 --> 34:42.000]  локально.
[34:42.000 --> 34:44.000]  Все, что нам от него сейчас нужно.
[34:44.000 --> 34:46.000]  То есть,
[34:46.000 --> 34:48.000]  как вы знаете, Rest API это, в принципе,
[34:48.000 --> 34:50.000]  оно устроено так,
[34:50.000 --> 34:52.000]  что у нас есть веб-страница.
[34:52.000 --> 34:54.000]  По каждому запросу
[34:54.000 --> 34:56.000]  у нас выдается некая веб-страница.
[34:56.000 --> 34:58.000]  На ней вместо
[34:58.000 --> 35:00.000]  какой-то красивой
[35:00.000 --> 35:02.000]  веб-морды находится
[35:02.000 --> 35:04.000]  JSON, который мы можем скачать
[35:04.000 --> 35:06.000]  и как-то проанализировать.
[35:06.000 --> 35:08.000]  Ну и в данном случае
[35:08.000 --> 35:10.000]  мы хотим скачать, мы хотим посмотреть
[35:10.000 --> 35:12.000]  первые 10 символов вот этого файла.
[35:12.000 --> 35:14.000]  Что мы делаем?
[35:14.000 --> 35:16.000]  Мы подключаемся к нейм-ноде,
[35:16.000 --> 35:18.000]  идем по Rest API,
[35:18.000 --> 35:20.000]  вводим путь к файлу
[35:20.000 --> 35:22.000]  и вот дальше
[35:26.000 --> 35:28.000]  дальше с помощью вот такого синтакса
[35:28.000 --> 35:30.000]  вводим параметры.
[35:30.000 --> 35:32.000]  Что вот это за вопрос,
[35:32.000 --> 35:38.000]  это Get HTTP запрос.
[35:38.000 --> 35:40.000]  То есть,
[35:40.000 --> 35:42.000]  в Get запросах к веб-страницам
[35:42.000 --> 35:44.000]  мы можем
[35:44.000 --> 35:46.000]  через вот такой синтаксе
[35:46.000 --> 35:48.000]  передавать параметры gay-value.
[35:48.000 --> 35:50.000]  Вот первый параметр
[35:50.000 --> 35:52.000]  и через ampersand второй параметр.
[36:02.000 --> 36:04.000]  Вот, читаем
[36:04.000 --> 36:06.000]  первые 10 символов,
[36:06.000 --> 36:08.000]  Open открыть, 10 символов прочитать.
[36:08.000 --> 36:10.000]  Видим вот такой результат.
[36:10.000 --> 36:12.000]  Никаких символов нету,
[36:12.000 --> 36:14.000]  кто может
[36:14.000 --> 36:16.000]  рассказать почему, исходя
[36:16.000 --> 36:18.000]  из того, что вы сегодня услышали.
[36:36.000 --> 36:38.000]  Где все?
[36:38.000 --> 36:40.000]  Есть ли какие-то идеи,
[36:40.000 --> 36:42.000]  почему вы вместо символов
[36:42.000 --> 36:44.000]  получили вот это
[36:44.000 --> 36:46.000]  и что вот это все означает?
[36:48.000 --> 36:50.000]  Но он тут указал
[36:50.000 --> 36:52.000]  на какую-то другую локацию,
[36:52.000 --> 36:54.000]  возможно, там надо смотреть.
[36:54.000 --> 36:56.000]  Ну да, а почему
[36:56.000 --> 36:58.000]  он нас куда-то перебросил?
[36:58.000 --> 37:00.000]  Вот почему такое случилось?
[37:00.000 --> 37:02.000]  Ну да, я не знаю.
[37:02.000 --> 37:04.000]  Ну да, я не знаю.
[37:04.000 --> 37:06.000]  Почему такое случилось?
[37:06.000 --> 37:08.000]  Это какой-то баг системы
[37:08.000 --> 37:10.000]  или что это?
[37:10.000 --> 37:12.000]  Ну, вроде клиент
[37:12.000 --> 37:14.000]  запрашивает на имноду
[37:14.000 --> 37:16.000]  реплику блоков, а чтение нужно именно
[37:16.000 --> 37:18.000]  с сервера, с ноды делать.
[37:18.000 --> 37:20.000]  Да, так и есть.
[37:20.000 --> 37:22.000]  То есть мы запросили у ноды
[37:22.000 --> 37:24.000]  и нода нам сказала temporary redirect
[37:24.000 --> 37:26.000]  и редиректную она вот сюда.
[37:28.000 --> 37:30.000]  Вот это уже, как видите, написано
[37:30.000 --> 37:32.000]  nip-node-03
[37:32.000 --> 37:34.000]  и далее
[37:34.000 --> 37:36.000]  и вот путь.
[37:40.000 --> 37:42.000]  Давайте заменим этот путь и посмотрим, что будет.
[37:46.000 --> 37:48.000]  Да, действительно мы видим первые 10 символов,
[37:48.000 --> 37:50.000]  вот они у нас есть, первая строчка.
[37:54.000 --> 37:56.000]  На самом деле можно это делать сразу,
[37:56.000 --> 37:58.000]  то есть вот у нас есть
[37:58.000 --> 38:00.000]  редирект вот такой,
[38:00.000 --> 38:02.000]  можно это делать сразу.
[38:04.000 --> 38:06.000]  Для этого нужно добавить
[38:06.000 --> 38:08.000]  параметр
[38:08.000 --> 38:10.000]  "-l",
[38:10.000 --> 38:12.000]  это автоматическая
[38:12.000 --> 38:14.000]  поддержка редиректов, то есть мы
[38:14.000 --> 38:16.000]  пошли на страничку, если мы встретили
[38:16.000 --> 38:18.000]  редирект, мы сразу по нему тоже пошли.
[38:20.000 --> 38:22.000]  И вот у нас результат.
[38:30.000 --> 38:32.000]  Вот еще примеры
[38:32.000 --> 38:34.000]  команд SDFS.
[38:34.000 --> 38:36.000]  Как я уже сказал,
[38:36.000 --> 38:38.000]  они похожи на обычный Linux, правда
[38:38.000 --> 38:40.000]  работают намного дольше.
[38:42.000 --> 38:44.000]  Ну и вот
[38:44.000 --> 38:46.000]  так.
[38:56.000 --> 38:58.000]  Пока мы не перешли
[38:58.000 --> 39:00.000]  к SCK, давайте я вот так
[39:00.000 --> 39:02.000]  числами передвину и
[39:02.000 --> 39:04.000]  пойдем сюда, то есть помимо
[39:04.000 --> 39:06.000]  API Java, как я уже сказал,
[39:06.000 --> 39:08.000]  есть обвертки на Python, есть
[39:08.000 --> 39:10.000]  на C++, вот эти две,
[39:10.000 --> 39:12.000]  которые выделены курсивом,
[39:12.000 --> 39:14.000]  это те, с которыми я больше всего работал
[39:14.000 --> 39:16.000]  и могу сказать, что да,
[39:16.000 --> 39:18.000]  пойду бы она довольно удобная,
[39:18.000 --> 39:20.000]  но она уже сейчас не обновляется
[39:20.000 --> 39:22.000]  и она до этих пор сидит на втором
[39:22.000 --> 39:24.000]  Python. SDFS CLI
[39:24.000 --> 39:26.000]  это тоже удобная библиотека,
[39:26.000 --> 39:28.000]  она обновляется, с ней все в порядке,
[39:28.000 --> 39:30.000]  но она умеет работать
[39:30.000 --> 39:32.000]  только с HDFS, то есть если
[39:32.000 --> 39:34.000]  вот эти все библиотеки они умеют
[39:34.000 --> 39:36.000]  производить и вычисления,
[39:36.000 --> 39:38.000]  о том, как эти вычисления
[39:38.000 --> 39:40.000]  работают, мы поговорим в следующий раз,
[39:40.000 --> 39:42.000]  то SDFS CLI умеет
[39:42.000 --> 39:44.000]  только работать с
[39:44.000 --> 39:46.000]  файловой системы HDFS.
[39:46.000 --> 39:48.000]  Вот, я вам
[39:48.000 --> 39:50.000]  говорил, что вот есть
[39:50.000 --> 39:52.000]  файловая система HDFS,
[39:52.000 --> 39:54.000]  как ее вообще поймать,
[39:54.000 --> 39:56.000]  вот мы заходим на сервер, тут у нас есть
[39:56.000 --> 39:58.000]  обычная файловая система, мы
[39:58.000 --> 40:00.000]  можем смотреть на какие-то файлы,
[40:00.000 --> 40:02.000]  а можем выполнить вот такую
[40:02.000 --> 40:04.000]  команду.
[40:08.000 --> 40:10.000]  И вот это у нас будет другая файловая
[40:10.000 --> 40:12.000]  система HDFS.
[40:12.000 --> 40:14.000]  И вот это у нас будет другая файловая система,
[40:14.000 --> 40:16.000]  то есть по сути, когда вы будете
[40:16.000 --> 40:18.000]  работать на этом
[40:18.000 --> 40:20.000]  пластере, у вас будет
[40:20.000 --> 40:22.000]  две файловые системы, находящиеся
[40:22.000 --> 40:24.000]  на одном сервере,
[40:24.000 --> 40:26.000]  Локальная и HDFS.
[40:26.000 --> 40:28.000]  Легко поверить, что они разные,
[40:28.000 --> 40:30.000]  давайте вот выполним такую команду.
[40:30.000 --> 40:32.000]  ls home
[40:32.000 --> 40:34.000]  и вот в
[40:34.000 --> 40:36.000]  home вы увидели
[40:36.000 --> 40:38.000]  всех ваших коллег, которые сидят
[40:38.000 --> 40:40.000]  на пластере, плюс там
[40:40.000 --> 40:42.000]  студенты других курсов.
[40:44.000 --> 40:46.000]  А если мы сделаем вот так,
[40:46.000 --> 40:48.000]  HDFS,
[40:48.000 --> 40:50.000]  HDFS-ls,
[40:50.000 --> 40:52.000]  то home мы тут
[40:52.000 --> 40:54.000]  не найдем, потому что в HDFS
[40:54.000 --> 40:56.000]  пользователи хранятся
[40:56.000 --> 40:58.000]  по умолчанию в директории user, вот здесь.
[41:02.000 --> 41:04.000]  Ну и давайте еще выполним вот эту
[41:04.000 --> 41:06.000]  команду.
[41:06.000 --> 41:08.000]  И посмотрим
[41:08.000 --> 41:10.000]  вот на что.
[41:12.000 --> 41:14.000]  Все знают, что такое команда du,
[41:14.000 --> 41:16.000]  что она меряет.
[41:20.000 --> 41:22.000]  Видимо нет.
[41:22.000 --> 41:24.000]  Это диск usage.
[41:24.000 --> 41:26.000]  То есть мы
[41:26.000 --> 41:28.000]  измеряем, сколько места занимает
[41:28.000 --> 41:30.000]  папка в файловой системе, вот и все.
[41:32.000 --> 41:34.000]  И в HDFS тоже есть диск usage,
[41:34.000 --> 41:36.000]  но он почему-то вывел два числа.
[41:36.000 --> 41:38.000]  Должен же быть один размер
[41:38.000 --> 41:40.000]  папки или файла. Почему два?
[41:40.000 --> 41:42.000]  Есть ли какие-то идеи?
[41:50.000 --> 41:52.000]  Может с метаданами и без метада?
[41:54.000 --> 41:56.000]  Метаданных у нас мало,
[41:56.000 --> 41:58.000]  то есть на каждый блок,
[41:58.000 --> 42:00.000]  а блок у нас там 6,4 мегабайта
[42:00.000 --> 42:02.000]  или 128, мы храним
[42:02.000 --> 42:04.000]  небольшую метаинформацию,
[42:04.000 --> 42:06.000]  там пожалуй 600 байк
[42:06.000 --> 42:08.000]  или что-то типа того на каждый блок.
[42:08.000 --> 42:10.000]  Поэтому странно, чтобы у нас
[42:10.000 --> 42:12.000]  метаинформация давала прирост в разы.
[42:14.000 --> 42:16.000]  Ну там скорее всего еще
[42:16.000 --> 42:18.000]  суммарный объем всех трех файлов,
[42:18.000 --> 42:20.000]  всех копий, со всех реплик
[42:20.000 --> 42:22.000]  с метаданами.
[42:22.000 --> 42:24.000]  Так и есть. То есть вот этот размер,
[42:24.000 --> 42:26.000]  это размер, который файл будет
[42:26.000 --> 42:28.000]  занимать, если мы его выкачаем локально.
[42:28.000 --> 42:30.000]  А это
[42:30.000 --> 42:32.000]  с учетом всех его реплик,
[42:32.000 --> 42:34.000]  то есть сколько места он
[42:34.000 --> 42:36.000]  занимает реально в файловой системе.
[42:42.000 --> 42:44.000]  Ну а теперь посмотрим на такую
[42:44.000 --> 42:46.000]  вот команду FSTK.
[42:48.000 --> 42:50.000]  Вообще опять вопрос к вам,
[42:50.000 --> 42:52.000]  кто знает, что такое FSTK?
[42:52.000 --> 42:54.000]  Что эта команда вообще делает?
[42:54.000 --> 42:56.000]  Она есть не только в ходу,
[42:56.000 --> 42:58.000]  она есть в любой файловой системе.
[43:10.000 --> 43:12.000]  Это же тоже должно было быть на косе?
[43:14.000 --> 43:16.000]  Нет, этого не было.
[43:16.000 --> 43:18.000]  Окей.
[43:18.000 --> 43:20.000]  Но возможно вы когда-нибудь исследовали
[43:20.000 --> 43:22.000]  жесткий диск
[43:22.000 --> 43:24.000]  у себя на компьютере,
[43:24.000 --> 43:26.000]  на винде, например.
[43:30.000 --> 43:32.000]  То есть, наверное, вам приходилось делать проверку
[43:32.000 --> 43:34.000]  диска на предмет битых блоков,
[43:34.000 --> 43:36.000]  битых секторов каких-нибудь,
[43:36.000 --> 43:38.000]  если вы начали замечать,
[43:38.000 --> 43:40.000]  что какие-то папки недоступны.
[43:40.000 --> 43:42.000]  Да, команда FSTK
[43:42.000 --> 43:44.000]  это File System Check,
[43:44.000 --> 43:46.000]  проверка файловой системы.
[43:46.000 --> 43:48.000]  Если в обычной файловой системе
[43:48.000 --> 43:50.000]  FSTK нам выдает что-то нехорошее,
[43:50.000 --> 43:52.000]  то это
[43:52.000 --> 43:54.000]  действительно плохо,
[43:54.000 --> 43:56.000]  потому что обычно у нас в компьютере один диск,
[43:56.000 --> 43:58.000]  если он начал фрипаться,
[43:58.000 --> 44:00.000]  то его надо менять, причем срочно.
[44:00.000 --> 44:02.000]  В реальных системах FSTK
[44:02.000 --> 44:04.000]  это скорее такая диагностика,
[44:04.000 --> 44:06.000]  если у нас вылетела какая-то нода
[44:06.000 --> 44:08.000]  или вылетел диск,
[44:08.000 --> 44:10.000]  это не так критично.
[44:10.000 --> 44:12.000]  Но зато FSTK позволяет поисследовать
[44:12.000 --> 44:14.000]  файловую систему и понять,
[44:14.000 --> 44:16.000]  где лежат блоки,
[44:16.000 --> 44:18.000]  какого они размера, как они хранятся.
[44:18.000 --> 44:20.000]  Вот давайте посмотрим.
[44:22.000 --> 44:24.000]  На вот такую вот команду.
[44:42.000 --> 44:44.000]  То есть мы прошли по папке,
[44:44.000 --> 44:46.000]  вот такая у нас папка data.zps,
[44:46.000 --> 44:48.000]  например, здесь мы
[44:48.000 --> 44:50.000]  подключились к нейм-ноде
[44:50.000 --> 44:52.000]  и выяснили статус, статус OK.
[44:52.000 --> 44:54.000]  Смотрим на
[44:54.000 --> 44:56.000]  количество подпапок,
[44:56.000 --> 44:58.000]  количество файлов,
[44:58.000 --> 45:00.000]  общий размер,
[45:00.000 --> 45:02.000]  такой summary.
[45:02.000 --> 45:04.000]  Смотрим на
[45:04.000 --> 45:06.000]  репликацию блоков, вот кстати
[45:06.000 --> 45:08.000]  default replication factor
[45:08.000 --> 45:10.000]  и average block replication,
[45:10.000 --> 45:12.000]  если они совпадают, это хорошо,
[45:12.000 --> 45:14.000]  потому что если они не совпадают,
[45:14.000 --> 45:16.000]  значит у нас где-то не хватает реплики
[45:16.000 --> 45:18.000]  или какие-то лишние реплики
[45:18.000 --> 45:20.000]  появились.
[45:20.000 --> 45:22.000]  Но, кстати, про реплики.
[45:22.000 --> 45:24.000]  У нас есть у реплик несколько состояний.
[45:24.000 --> 45:26.000]  Есть misreplicated,
[45:26.000 --> 45:28.000]  когда у нас нету реплики,
[45:28.000 --> 45:30.000]  есть underreplicated,
[45:30.000 --> 45:32.000]  когда у нас у блока меньше
[45:32.000 --> 45:34.000]  реплик, меньше копий, чем мы хотели,
[45:34.000 --> 45:36.000]  есть overreplicated.
[45:36.000 --> 45:38.000]  Ну вот с mis и under все понятно,
[45:38.000 --> 45:40.000]  у нас упала одна или несколько нот,
[45:40.000 --> 45:42.000]  пропали реплики, поэтому under.
[45:42.000 --> 45:44.000]  А в каких случаях может быть
[45:44.000 --> 45:46.000]  overreplicated? То есть overreplicated
[45:46.000 --> 45:48.000]  это больше реплик, чем мы
[45:48.000 --> 45:50.000]  сконфигурировали, чем мы сказали.
[45:50.000 --> 45:52.000]  Как такое может быть?
[45:54.000 --> 45:56.000]  Может, например, какая-то нода
[45:56.000 --> 45:58.000]  отвалилась, долго не отвечала,
[45:58.000 --> 46:00.000]  мы реплицировали что-то, а потом
[46:00.000 --> 46:02.000]  та нода проснулась.
[46:02.000 --> 46:04.000]  Именно так. Все правильно.
[46:08.000 --> 46:10.000]  Вот давайте я открою
[46:10.000 --> 46:12.000]  другой файл, хотя можно и этот.
[46:12.000 --> 46:14.000]  Просто возьмем вот такие
[46:14.000 --> 46:16.000]  вот ключики.
[46:22.000 --> 46:24.000]  Minus files, minus blocks, minus locations.
[46:24.000 --> 46:26.000]  Вот таким образом мы узнаем,
[46:26.000 --> 46:28.000]  где у нас хранятся блоки.
[46:30.000 --> 46:32.000]  И мы видим, что
[46:32.000 --> 46:34.000]  вот в этой папке у нас хранится
[46:34.000 --> 46:36.000]  еще под папка, и здесь
[46:36.000 --> 46:38.000]  хранится один блок. В этой папке
[46:38.000 --> 46:40.000]  у нас хранится файл,
[46:40.000 --> 46:42.000]  и в нем хранится один блок.
[46:42.000 --> 46:44.000]  Что мы знаем про блок?
[46:44.000 --> 46:46.000]  Мы знаем его блок pool, но, как я уже сказал,
[46:46.000 --> 46:48.000]  в нашем случае это константа.
[46:48.000 --> 46:50.000]  Мы знаем его ID, знаем его версию,
[46:50.000 --> 46:52.000]  его длину.
[46:52.000 --> 46:54.000]  Видите, у нас здесь блок вот такого размера,
[46:54.000 --> 46:56.000]  а здесь блок вообще маленький.
[46:56.000 --> 46:58.000]  Ну, файл просто маленький.
[46:58.000 --> 47:00.000]  LiveReplix2, две реплики,
[47:00.000 --> 47:02.000]  и про каждую реплику мы видим,
[47:02.000 --> 47:04.000]  на какой ноде
[47:04.000 --> 47:06.000]  она хранится.
[47:08.000 --> 47:10.000]  IP-шник ноды, port.
[47:10.000 --> 47:12.000]  Это ID-шник ноды внутренней.
[47:12.000 --> 47:14.000]  То есть,
[47:14.000 --> 47:16.000]  когда мы делаем кластер, понятно, что
[47:16.000 --> 47:18.000]  в процессе работы
[47:18.000 --> 47:20.000]  могут быть какие-то сетевые
[47:20.000 --> 47:22.000]  перестройки, могут меняться
[47:22.000 --> 47:24.000]  хосты, могут меняться
[47:24.000 --> 47:26.000]  IP-шники, но за счет
[47:26.000 --> 47:28.000]  внутреннего ID система
[47:28.000 --> 47:30.000]  все-таки будет связываться с этими нодами.
[47:30.000 --> 47:32.000]  Ну, и здесь еще указан диск.
[47:34.000 --> 47:36.000]  То есть, в современных версиях Hadoop
[47:36.000 --> 47:38.000]  датоноды могут
[47:38.000 --> 47:40.000]  хранить блоки не только на диске,
[47:40.000 --> 47:42.000]  а могут, например, хранить
[47:42.000 --> 47:44.000]  в каком-нибудь облаке.
[47:48.000 --> 47:50.000]  Пожалуй, облако
[47:50.000 --> 47:52.000]  это самый яркий пример,
[47:52.000 --> 47:54.000]  потому что сейчас, в принципе,
[47:54.000 --> 47:56.000]  идет тренд
[47:56.000 --> 47:58.000]  замены on-premises
[47:58.000 --> 48:00.000]  систем, которые мы сами разворачиваем
[48:00.000 --> 48:02.000]  и поддерживаем на какие-то
[48:02.000 --> 48:04.000]  облачные вычисления.
[48:04.000 --> 48:06.000]  Ну, и вот второй блок тоже.
[48:08.000 --> 48:10.000]  Давайте я для примера
[48:12.000 --> 48:14.000]  возьму другой файл, побольше размером,
[48:14.000 --> 48:16.000]  вот, например, отсюда.
[48:34.000 --> 48:36.000]  Вот, мы видим целую пучу блоков.
[48:40.000 --> 48:42.000]  Вот, 0, 1, 2, 3 и так далее.
[48:42.000 --> 48:44.000]  Вот они пронумерованы.
[48:44.000 --> 48:46.000]  У каждого фактора репликации 3,
[48:46.000 --> 48:48.000]  размер одинаковый.
[48:48.000 --> 48:50.000]  Вот три датоноды,
[48:50.000 --> 48:52.000]  где это все хранится.
[48:56.000 --> 48:58.000]  Сколько блоков? 92.
[48:58.000 --> 49:00.000]  Последний у нас чуть меньше
[49:00.000 --> 49:02.000]  по размеру.
[49:02.000 --> 49:04.000]  Ну, и в конце есть summary.
[49:04.000 --> 49:06.000]  Мы действительно видим, что 92
[49:06.000 --> 49:08.000]  блока.
[49:08.000 --> 49:10.000]  Я это почему все рассказываю.
[49:10.000 --> 49:12.000]  У вас будет, мы, конечно, посмотрим это
[49:12.000 --> 49:14.000]  еще на семинарах, но у вас будет
[49:14.000 --> 49:16.000]  домашка, в которой вы будете
[49:16.000 --> 49:18.000]  исследовать файловую систему.
[49:18.000 --> 49:20.000]  Вот, именно
[49:20.000 --> 49:22.000]  отвечать
[49:22.000 --> 49:24.000]  на вопросы типа
[49:24.000 --> 49:26.000]  где, на какой ноде
[49:26.000 --> 49:28.000]  и в какой бабке лежит первый блок
[49:28.000 --> 49:30.000]  данного файла.
[49:30.000 --> 49:32.000]  Вот, можно вывести информацию
[49:32.000 --> 49:34.000]  и про отдельный блок.
[49:38.000 --> 49:40.000]  Вот, возьмем блок.
[49:40.000 --> 49:42.000]  Вот эту всю версию не берем.
[49:42.000 --> 49:44.000]  Берем только блок.
[49:48.000 --> 49:50.000]  Ну, и вот мы видим, что
[49:50.000 --> 49:52.000]  у нас есть
[49:52.000 --> 49:54.000]  один блок.
[49:54.000 --> 49:56.000]  Ну, и мы видим, что
[49:56.000 --> 49:58.000]  у нас есть один блок.
[49:58.000 --> 50:00.000]  Ну, и вот мы видим, что
[50:00.000 --> 50:02.000]  этот блок хранится на
[50:02.000 --> 50:04.000]  трех нодах.
[50:04.000 --> 50:06.000]  У него все в порядке с
[50:06.000 --> 50:08.000]  репликацией, то есть
[50:08.000 --> 50:10.000]  три expected реплика, три live реплика.
[50:10.000 --> 50:12.000]  Никто не упал, все в порядке.
[50:18.000 --> 50:20.000]  Хорошо, есть ли
[50:20.000 --> 50:22.000]  какие-нибудь вопросы по вот этим вот
[50:22.000 --> 50:24.000]  командам?
[50:24.000 --> 50:26.000]  Как-то все очень тихо
[50:26.000 --> 50:28.000]  и нас все меньше.
[50:44.000 --> 50:46.000]  Видимо, нет вопросов.
[50:46.000 --> 50:48.000]  Ну, окей, тогда идем дальше.
[50:48.000 --> 50:50.000]  Давайте попробуем решить такую задачку.
[50:50.000 --> 50:52.000]  Это...
[50:52.000 --> 50:54.000]  Задача приближена к реальной,
[50:54.000 --> 50:56.000]  когда вот
[50:56.000 --> 50:58.000]  приходит заказчик и просит
[50:58.000 --> 51:00.000]  спланировать кластер, или мы сами
[51:00.000 --> 51:02.000]  планируем кластер для какого-то проекта.
[51:02.000 --> 51:04.000]  Вот, у нас есть объем
[51:04.000 --> 51:06.000]  всех дисков кластера.
[51:06.000 --> 51:08.000]  Есть вот такой вот размер блока.
[51:08.000 --> 51:10.000]  И вот
[51:10.000 --> 51:12.000]  у нас есть
[51:12.000 --> 51:14.000]  объем всех дисков кластера.
[51:14.000 --> 51:16.000]  Есть вот такой вот размер блока.
[51:16.000 --> 51:18.000]  Вот размер метаинформации.
[51:18.000 --> 51:20.000]  То есть сколько памяти...
[51:20.000 --> 51:22.000]  Сколько оперативной памяти в
[51:22.000 --> 51:24.000]  нейминоде затрачивается на один
[51:24.000 --> 51:26.000]  такой объем?
[51:26.000 --> 51:28.000]  И количество реплик.
[51:28.000 --> 51:30.000]  Нас поможет оценить
[51:30.000 --> 51:32.000]  минимальный объем оперативки в нейминоде.
[51:32.000 --> 51:34.000]  Сначала вопрос, а почему оценить?
[51:34.000 --> 51:36.000]  Почему не посчитать точно?
[51:38.000 --> 51:40.000]  И второе, как именно мы это будем считать?
[51:42.000 --> 51:44.000]  Оценить, наверное, потому что
[51:44.000 --> 51:46.000]  нацило не разделится?
[51:46.000 --> 51:48.000]  Или может быть...
[51:48.000 --> 51:50.000]  Да, верно.
[51:50.000 --> 51:52.000]  Во-первых, нацило не разделится.
[51:52.000 --> 51:54.000]  Во-вторых, мы вообще не знаем ничего про файлы.
[51:54.000 --> 51:56.000]  То есть, да,
[51:56.000 --> 51:58.000]  мы хотим хранить два битабайта.
[51:58.000 --> 52:00.000]  Хорошо, а сколько у нас будет файлов?
[52:00.000 --> 52:02.000]  Какого они будут размеры?
[52:02.000 --> 52:04.000]  Или это будет один огромный файл величины 2 битабайта?
[52:04.000 --> 52:06.000]  Или это будет
[52:06.000 --> 52:08.000]  миллион файлов по несколько килобайт?
[52:08.000 --> 52:10.000]  Это совершенно разные
[52:10.000 --> 52:12.000]  затраты на нейминоду.
[52:12.000 --> 52:14.000]  Ну и давайте для начала подумаем,
[52:14.000 --> 52:16.000]  а в каких случаях
[52:16.000 --> 52:18.000]  как нам хранить файлы,
[52:18.000 --> 52:20.000]  чтобы затраты на нейминоду были минимальны?
[52:28.000 --> 52:30.000]  Нужно, чтобы блоки были максимально большие.
[52:32.000 --> 52:34.000]  Значит, все файлы должны быть
[52:34.000 --> 52:36.000]  кратны, 64 мегабайт.
[52:36.000 --> 52:38.000]  Да, или равны, или кратны.
[52:38.000 --> 52:40.000]  Ну или вообще такой выраженный случай,
[52:40.000 --> 52:42.000]  когда у нас будет один огромный файл
[52:42.000 --> 52:44.000]  величиной 2 битабайта.
[52:44.000 --> 52:46.000]  Но, правда, вряд ли такое можно представить.
[52:48.000 --> 52:50.000]  Ну а теперь давайте оценим,
[52:50.000 --> 52:52.000]  давайте подумаем, как мы можем это посчитать.
[53:00.000 --> 53:02.000]  Тут я все-таки жду каких-то идей от вас.
[53:04.000 --> 53:06.000]  Ну, давайте посчитаем, сколько всего
[53:06.000 --> 53:08.000]  блоков нужно.
[53:08.000 --> 53:10.000]  Ну это, видимо, где-то
[53:10.000 --> 53:12.000]  2 в 21 выделить на 64.
[53:14.000 --> 53:16.000]  Потом это нужно будет умножить на
[53:16.000 --> 53:18.000]  600 байт.
[53:18.000 --> 53:20.000]  Это то, сколько байт нам потребуется
[53:20.000 --> 53:22.000]  суммарно.
[53:22.000 --> 53:24.000]  Ну и, видимо, умножить на 3, потому что
[53:24.000 --> 53:26.000]  для каждой реплики нам
[53:26.000 --> 53:28.000]  тоже нужно вот эту метаинформацию хранить.
[53:30.000 --> 53:32.000]  Тут метаинформация хранится
[53:32.000 --> 53:34.000]  все-таки не для реплики, а для блока.
[53:34.000 --> 53:36.000]  То есть мы берем 2 битабайта,
[53:36.000 --> 53:38.000]  делим на 64,
[53:38.000 --> 53:40.000]  а потом еще делим на 3,
[53:40.000 --> 53:42.000]  потому что в этих 2 битабайтах
[53:42.000 --> 53:44.000]  мы должны все реплики наши уместить.
[53:48.000 --> 53:50.000]  Все блоки.
[53:50.000 --> 53:52.000]  И вот если мы так делаем,
[53:52.000 --> 53:54.000]  2 делить на 64, делить на 3,
[53:54.000 --> 53:56.000]  у нас будет количество блоков.
[53:56.000 --> 53:58.000]  И потом на каждый блок мы
[53:58.000 --> 54:00.000]  даем 600 мегабайт.
[54:00.000 --> 54:02.000]  Вот получается вот такая штука.
[54:04.000 --> 54:06.000]  Ну и последнее,
[54:06.000 --> 54:08.000]  что мне осталось тут вам рассказать,
[54:08.000 --> 54:10.000]  это несколько статей,
[54:10.000 --> 54:12.000]  которые вам понадобятся,
[54:12.000 --> 54:14.000]  если будете погружаться
[54:14.000 --> 54:16.000]  в HDFS
[54:16.000 --> 54:18.000]  и вообще в ходу сильнее.
[54:18.000 --> 54:20.000]  Статья, что в действительности
[54:20.000 --> 54:22.000]  делает secondary node,
[54:22.000 --> 54:24.000]  потому что, ну это
[54:24.000 --> 54:26.000]  часто спрашивают нас,
[54:26.000 --> 54:28.000]  что мы делаем в HDFS,
[54:28.000 --> 54:30.000]  что мы делаем в HDFS,
[54:30.000 --> 54:32.000]  что мы делаем в HDFS,
[54:32.000 --> 54:34.000]  потому что это часто спрашивают
[54:34.000 --> 54:36.000]  на собеседованиях. То, что я вам сказал,
[54:36.000 --> 54:38.000]  она занимается слиянием,
[54:38.000 --> 54:40.000]  но просто здесь подробно про это написано.
[54:42.000 --> 54:44.000]  Дальше статья про архитектуру HDFS.
[54:44.000 --> 54:46.000]  Была раньше компания Hortonworks,
[54:46.000 --> 54:48.000]  она занималась разработкой
[54:48.000 --> 54:50.000]  и поставкой
[54:52.000 --> 54:54.000]  сборок Hadoop.
[54:54.000 --> 54:56.000]  Потом не так давно,
[54:56.000 --> 54:58.000]  пару лет назад, ее купила
[54:58.000 --> 55:00.000]  другая компания Cloudair,
[55:00.000 --> 55:02.000]  которая занималась
[55:02.000 --> 55:04.000]  сборкой Hadoop.
[55:04.000 --> 55:06.000]  И Hortonworks
[55:06.000 --> 55:08.000]  перестал существовать,
[55:08.000 --> 55:10.000]  по сути.
[55:10.000 --> 55:12.000]  Сохранилась их статья
[55:12.000 --> 55:14.000]  в китайском,
[55:14.000 --> 55:16.000]  китайская веплика,
[55:16.000 --> 55:18.000]  и здесь можно посмотреть,
[55:18.000 --> 55:20.000]  как реально хранятся
[55:20.000 --> 55:22.000]  блоки, файлы,
[55:22.000 --> 55:24.000]  вот эти все слепки,
[55:24.000 --> 55:26.000]  то есть действительно слепки
[55:26.000 --> 55:28.000]  и эдитлоги, они хранятся в оперативке,
[55:28.000 --> 55:30.000]  и имя нода их также вытапит на диск.
[55:32.000 --> 55:34.000]  И вот как именно они хранятся,
[55:34.000 --> 55:36.000]  мы можем это посмотреть.
[55:40.000 --> 55:42.000]  То есть, по сути,
[55:42.000 --> 55:44.000]  на каждом сервере, будь то имя нода
[55:44.000 --> 55:46.000]  или дата нода у нас есть
[55:46.000 --> 55:48.000]  просто папка,
[55:48.000 --> 55:50.000]  там slash.dfs, она лежит в корне,
[55:50.000 --> 55:52.000]  и внутри slash.dfs есть
[55:52.000 --> 55:54.000]  папка для дата ноды
[55:54.000 --> 55:56.000]  или папка для имя ноды,
[55:56.000 --> 55:58.000]  все эти вещи хранятся.
[56:00.000 --> 56:02.000]  Можно почитать про это подробнее,
[56:02.000 --> 56:04.000]  есть также статья Константина Швачко
[56:04.000 --> 56:06.000]  про устройство
[56:06.000 --> 56:08.000]  файловой системы HDFS,
[56:08.000 --> 56:10.000]  она уже такая более научная,
[56:10.000 --> 56:12.000]  теоретическая, без каких-то примеров кода,
[56:12.000 --> 56:14.000]  без файлов, но зато
[56:14.000 --> 56:16.000]  с формулами и схемами.
[56:18.000 --> 56:20.000]  Есть также вот этот вот пост,
[56:22.000 --> 56:24.000]  это конспект по яндексовому
[56:24.000 --> 56:26.000]  курсу на курсере по ходу,
[56:28.000 --> 56:30.000]  и здесь тоже подробно
[56:30.000 --> 56:32.000]  вот эти все схемы
[56:34.000 --> 56:36.000]  нарисованы и рассказаны,
[56:36.000 --> 56:38.000]  как, например, происходит репликация,
[56:38.000 --> 56:40.000]  что сначала мы отреплицировали
[56:40.000 --> 56:42.000]  на ближайшую ноду,
[56:42.000 --> 56:44.000]  может быть даже на эту, на которой мы находимся,
[56:44.000 --> 56:46.000]  но это не наш случай,
[56:46.000 --> 56:48.000]  потому что у нас с вами есть клиентская машинка,
[56:48.000 --> 56:50.000]  вот эта, на которой я сейчас
[56:50.000 --> 56:52.000]  сижу, здесь нет
[56:52.000 --> 56:54.000]  дата ноды,
[56:54.000 --> 56:56.000]  но бывает так,
[56:56.000 --> 56:58.000]  что клиент, он сам нодой и является,
[56:58.000 --> 57:00.000]  такое тоже часто бывает,
[57:00.000 --> 57:02.000]  поэтому мы реплицируем на него,
[57:02.000 --> 57:04.000]  потом реплицируем на эту же стойку,
[57:04.000 --> 57:06.000]  потом на этот же дата-центр и так далее.
[57:14.000 --> 57:16.000]  И последнее, это глава третья
[57:16.000 --> 57:18.000]  из книжки ходу подробное руководство,
[57:22.000 --> 57:24.000]  у этой книжки есть
[57:24.000 --> 57:26.000]  последнее издание, четвертое,
[57:26.000 --> 57:28.000]  оно не переведено на русский,
[57:28.000 --> 57:30.000]  а третье переведено,
[57:30.000 --> 57:32.000]  и у него даже хороший перевод.
[57:38.000 --> 57:42.000]  Окей, есть ли сейчас какие-нибудь вопросы?
[57:52.000 --> 57:54.000]  Скоро домашка по пути появится?
[57:58.000 --> 58:02.000]  На выходных ее Паша допилит,
[58:02.000 --> 58:06.000]  я думаю, что формулировку мы выложим даже раньше,
[58:06.000 --> 58:10.000]  а потом просто на выходных появятся тесты к ней.
[58:22.000 --> 58:24.000]  В принципе, сейчас
[58:24.000 --> 58:26.000]  с такой теоретической частью
[58:26.000 --> 58:28.000]  по хранению данных
[58:28.000 --> 58:30.000]  все, в следующий раз
[58:30.000 --> 58:32.000]  на лекции мы начнем разбирать
[58:32.000 --> 58:34.000]  MapReduce, то есть
[58:34.000 --> 58:36.000]  вычисление поверх больших данных.
[58:52.000 --> 58:54.000]  Окей, если вопросов нет,
[58:54.000 --> 58:56.000]  то на этом все,
[58:56.000 --> 58:58.000]  и тогда всем пока.

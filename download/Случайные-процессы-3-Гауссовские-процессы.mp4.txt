[00:00.000 --> 00:11.600]  Давайте начинать. Я напоминаю, что в прошлый раз я начал доказывать теорему о явной
[00:11.600 --> 00:19.200]  конструкции пласоновского процесса. Давайте мы ее завершим как следует. Я напоминаю,
[00:19.200 --> 00:32.680]  что это теорема о том, что вот такой процесс премум n таких, что sn или сумма k равно от единицы
[00:32.680 --> 00:41.000]  до n ксикатах больше либо равно t, что вот такой процесс как функция t, случайная функция t,
[00:41.000 --> 00:47.280]  где ксикаты показательные, независимые в совокупности с одним и теми же параметром лямбда,
[00:47.280 --> 00:56.040]  что этот процесс является пласоновским процессом. И что мы делали, чтобы это доказать? Мы
[00:56.040 --> 01:01.800]  проверяли свойство пласоновского процесса просто по определению. В 0 яо 0, ну просто по
[01:01.800 --> 01:11.440]  построению мы считаем, что если t равно 0 пусть t будет равно 0. Почему? А, меньше либо равно, да?
[01:11.440 --> 01:21.000]  Меньше либо равно, спасибо. Вот x от 0 равно 0. Вот и нам нужно проверить эти два других сложных
[01:21.000 --> 01:28.920]  свойства про независимость в совокупности и то, что пласоновское распределение имеет при
[01:28.920 --> 01:37.840]  рощении этого процесса. Что мы делали? Мы с вами записали вектор из вот этих сумм, выяснили как
[01:37.840 --> 01:44.080]  он распределен, получили функцию плотности для него и выразили вероятности при рощении x через
[01:44.080 --> 01:55.080]  эти s. Ну и получили там огромный интеграл. Давайте я снова получу, значит как там было x от t равняется
[01:55.080 --> 02:09.040]  k1, x от t2 минус x от t1 равняется k2 минус k1. Вот, ну и так далее e до n, давайте напишу, значит до
[02:09.040 --> 02:23.680]  xtn минус xtn минус 1 равняется kn минус kn минус 1, где k не отрицательные tl и e такие, что k1 меньше
[02:23.680 --> 02:29.920]  либо равен чем k2, меньше либо равен чем k3 и так далее. Вот такую вещь мы записали и мы выяснили,
[02:29.920 --> 02:37.440]  значит выразив эти при рощении через s, что это будет вот что. Ну я не буду там все прям повторять,
[02:37.440 --> 02:47.080]  значит мы дошли вот до чего? До интеграла tn да плюс бесконечности, значит мы выразили там самую
[02:47.080 --> 03:01.280]  последнюю из x, e в степени x kn плюс 1, плюс 1 прибавляется kn. Вот это наше переменное интегрирование
[03:01.280 --> 03:26.880]  x kn плюс 1. Там еще у нас было лямда в степени kn. И дальше был такой интеграл многократный
[03:26.880 --> 03:38.200]  от следующего, значит мы интегрируем по x, по множеству каком x1 и так далее. Давайте
[03:38.200 --> 03:42.040]  сейчас я сначала запишу, а потом вы можете записать, ну или проверьте как я там писал
[03:42.040 --> 03:53.680]  на прошлой лекции. По-моему вот это принадлежит значит от 0 до t1, вот это множество, потом пересечь,
[03:53.680 --> 04:04.400]  как это написать t. Ну можно кстати фигурные скобки не рисовать, не важно. Иксы эти принадлежат вот
[04:04.400 --> 04:19.440]  этому интервалу xk1 плюс 1 и так далее до xk2 принадлежат интервалу от t1 до t2. Ну и так далее
[04:19.440 --> 04:44.880]  до xk наверное n минус 1 плюс 1 до xkn принадлежит от tn минус 1 до tn. Скорее всего так,
[04:44.880 --> 04:55.160]  проверьте пожалуйста. Ну вроде так. Ну у нас там была еще одна, но вот она выразилась вот в этот
[04:55.160 --> 05:03.760]  интеграл отдельный. И здесь у нас стоит индикатор того, что x, ну можно так, не важно, 0 меньше x1,
[05:03.760 --> 05:11.800]  меньше x2, меньше и так далее и вот вообще все они xkn. Вот такой индикатор, где все x упорядочены.
[05:11.800 --> 05:22.320]  Здесь мы просто пишем dx1 и так далее по dxkn. Вот такой интеграл. Ну и я много говорил на прошлой
[05:22.320 --> 05:28.120]  лекции по поводу того, как это вычисляется. Страшно выглядит, но тут все очень тривиально на самом
[05:28.120 --> 05:34.920]  деле, потому что нам нужно посмотреть просто на то, что из себя представляет эта область и так
[05:34.920 --> 05:41.960]  получается, что если эти x лежат здесь, вот все они больше чем tn минус 1, значит все эти x подавны
[05:41.960 --> 05:47.600]  меньше, чем эти x. Поэтому соответствующие знаки меньше из этого индикатора можно убрать просто
[05:47.600 --> 05:53.440]  потому что это ничего не изменит. Мы интегрируем по такой области. Поэтому вот этот индикатор мы
[05:53.440 --> 05:58.720]  представим как произведение индикаторов, что эти упорядочены, умножить на индикатор,
[05:58.720 --> 06:06.280]  что эти упорядочены, умножить на индикатор и так далее, что эти упорядочены. Вот и все. Так что
[06:06.280 --> 06:15.560]  давайте равно, вот здесь я продолжу. Первый интеграл здесь у нас получается не забудьте,
[06:15.560 --> 06:28.520]  e в степени минус лямбда tn на лямбда в степени kn. Вот это я первый интеграл посчитал. И умножить
[06:28.520 --> 06:37.160]  на произведение интегралов, потому что все индикаторы они расщепились, x все независимы,
[06:37.160 --> 06:41.680]  поэтому этот многократный интеграл будет уже произведением интегралов по каждой из этих
[06:41.680 --> 06:49.120]  областей. Ну вот сколько у нас этих областей? Видите 1, 2 и так далее n, n штук областей у нас.
[06:49.120 --> 06:58.760]  Пусть будет g от единицы до n. Вот интеграл, вот тоже многократный интеграл, но вот это какой-то
[06:58.760 --> 07:15.240]  вот такой значит x. Так сейчас мы посмотрим g и тогда нет x. Вот какой тут x к g. Сейчас напишем x
[07:15.240 --> 07:33.440]  к g минус 1 плюс 1 до x к g. От чего? От индикатора того, что x к g минус 1 меньше и так далее,
[07:33.440 --> 07:46.040]  меньше чем x к g. По-моему что-то такое должно получиться, x к g минус 1 и x к g. Ну вот и мы
[07:46.040 --> 07:52.160]  значит теперь свели всю задачу к расчету вот такого интеграла. Значит если здесь что у нас
[07:52.160 --> 08:02.120]  принадлежит t g минус 1 до t g. Значит если у нас нет этого интеграла, то это просто, если у нас нет
[08:02.120 --> 08:08.000]  этого индикатора, то этот интеграл это просто объем вот этой области. А это прямоугольник,
[08:08.000 --> 08:17.680]  то есть вот это минус вот это в степени получается k g минус k g минус 1. Ну а здесь у нас x упорядоченные,
[08:17.680 --> 08:26.920]  а это множество в этом параллелепипеде, значит многомерном, это симплекс и объем его известен.
[08:26.920 --> 08:32.040]  Это нужно взять объем всего пространства, то есть всего этого прямоугольника или параллелепипеде,
[08:32.040 --> 08:37.920]  не знаю как это назвать, и разделить на факториал размерности пространства. То есть это будет t g
[08:37.920 --> 08:47.560]  t минус t g минус 1 в степени k g минус k g минус 1 разделить на факториал размерности пространства k g
[08:47.560 --> 09:00.080]  минус k g минус 1 факториал. Вот и все. Получаем e в степени минус лямбда t n лямбда в степени k n произведение g
[09:00.080 --> 09:13.480]  равняется к единице до n. Значит от t g минус t g минус 1 в степени k g минус k g минус 1 и разделить на
[09:13.480 --> 09:24.520]  k g минус k g минус 1 факториал. Вот объем параллелепипеда разделить на факториал размерности
[09:24.520 --> 09:36.120]  пространства. Вот этой степени, которая здесь стоит. Вот. Ну, по-моему, вот так. Так, хорошо. Ну,
[09:36.120 --> 09:42.040]  теперь нужно просто это выражение немножко переписать, потому что мы-то чего хотим? Мы хотим
[09:42.040 --> 09:46.200]  показать, что вот эта вероятность, которая здесь стоит, она расщепляется на произведение
[09:46.200 --> 09:52.760]  вероятностей. Вот. То есть надо представить все вот это в виде одного произведения. Вот давайте мы
[09:52.760 --> 10:01.960]  попробуем это сделать. Произведение по g равном от единицы до n. Вот. И как мы это сделаем? Ну,
[10:01.960 --> 10:13.800]  смотрите. t n это на самом деле t n минус t n минус 1 плюс t n минус 1 минус t n минус 2 и так далее.
[10:13.800 --> 10:20.920]  То есть телескопическая сумма. Так далее до t 0. А t 0 у нас равно 0. Так что мы можем записать это
[10:20.920 --> 10:33.160]  так. Е в степени минус лямбда. А t. Вот. И мне надо, чтобы как здесь было. t g, t минус t, g минус 1. Так. Вот. Так что,
[10:33.160 --> 10:44.680]  если я просуммирую, t 0 я считаю нулем, то я получу в точности вот это. Так. Вот. Хорошо. Теперь насчет
[10:44.680 --> 10:51.720]  лямбда. Тут мы сделаем то же самое. Значит k и n. А то тоже k и n минус k и n минус 1. Плюс k и n минус 1
[10:51.720 --> 10:57.800]  минус k и n минус 2 и так далее. До k 0. А k 0 у нас тоже равно нулю. Так что вот эту лямбду давайте
[10:57.800 --> 11:04.120]  мы внесем с вами вот в эту скобку. Чтобы все было красиво и симметрично. Значит у нас получается
[11:04.120 --> 11:17.480]  лямбда умножить на t g минус t g минус 1. Вот такая вот скобка в степени k g минус k g минус 1. Вот видите,
[11:17.480 --> 11:22.640]  если мы отсюда лямбду будем выносить, она вынестся вот этой степенью, возьмется произведение там
[11:22.640 --> 11:30.000]  телескопическая сумма и будет лямбда в степени k и n. Ну а вот это оставим без изменения. Ну вот и все.
[11:30.000 --> 11:38.800]  Вот наше выражение. Вот наше произведение. И теперь просто по индукции мы можем начать с n
[11:38.800 --> 11:46.760]  равная единице. Значит все, что я здесь написал верно для любых n, больше либо равно 1. Если мы
[11:46.760 --> 11:52.680]  возьмем n равно 1, мы получим, что вероятность xt1 равняется k1. Давайте я прямо ее напишу,
[11:52.680 --> 12:02.520]  чтобы вы явно увидели. Вероятность kt1 равняется k1. Ну берем n равняется единице. Что мы получаем?
[12:02.520 --> 12:13.080]  Надо взять, давайте сюда посмотрим лучше, g равно 1. Здесь получается e в степени минус лямбда t1,
[12:13.080 --> 12:27.040]  а это будет t0 равно 0. Здесь лямбда на t1 в степени k1 разделить на k1 факториал. Вот это
[12:27.040 --> 12:34.080]  Пуассоновское распределение. То есть k от t1 имеет Пуассоновское распределение с параметром
[12:34.080 --> 12:42.880]  лямбда t1. Как и надо, пожалуйста. Ну и теперь если мы возьмем n равняется 2, то мы получим,
[12:42.880 --> 12:50.520]  что вот эта вероятность равна произведению двух вещей. Первая из которых это вот это, но мы
[12:50.520 --> 12:57.640]  выяснили, что она равна вот этому. А вторая это некое выражение, некая вероятность,
[12:57.640 --> 13:06.800]  которая представляет собой Пуассоновское распределение для лямбда от t2 минус t1.
[13:06.800 --> 13:16.400]  Вот. И это будет равна вероятности вот этого события. Так что просто по индукции мы получаем,
[13:16.400 --> 13:24.640]  что вот эта вещь равна вероятности каждой из этих событий по отдельности. Ну просто начиная с n
[13:24.640 --> 13:30.800]  равная единице. Для n равно 1 мы нашли вероятность этого события, мы его знаем. Рассмотрим, n равняется 2,
[13:30.800 --> 13:35.680]  у нас два множителя. Первый из которых мы выяснили, чему он равен. Значит, второй равно вот этому.
[13:35.680 --> 13:43.960]  Вот. Так что по индукции мы получаем, что действительно эти события независимы в совокупности. Это первое.
[13:43.960 --> 13:49.240]  А второе, они распределены по Пуассону. Вот потому что мы видим все эти выражения. Они
[13:49.240 --> 13:56.000]  имеют Пуассоновское распределение. Вот. Ну еще можно так на это посмотреть, если вы хотите найти
[13:56.000 --> 14:03.400]  вероятность каждого из этих превращений по отдельности. Но это означает, что вам нужно по
[14:03.400 --> 14:11.760]  остальным аргументам суммировать. Вот. От нуля до плюс бесконечности. Вот. И тогда у вас получится
[14:11.760 --> 14:16.760]  вероятность только одного превращения. Если вы это примените вот к этой формуле, то вы получите
[14:16.760 --> 14:22.440]  снова Пуассоновское распределение. В общем, тут по-всякому можно об этом думать. Вот. В общем-то,
[14:22.440 --> 14:29.120]  на этом все. Теорема доказана. Вот. Мы получили. Действительно, что это есть. И смотрите,
[14:29.120 --> 14:35.040]  вот еще один важный момент и в задачу, когда решаете, когда от вас просят доказать, что какие-то
[14:35.040 --> 14:44.360]  случайные величины независимы в совокупности, то это значит, что вы должны доказать, что вот такие
[14:44.360 --> 14:49.880]  вероятности расщепляются на произведение вероятностей. Вот. Именно это вы должны доказывать
[14:49.880 --> 14:55.840]  честно, аккуратно, формально, а не обращаться там какой-то интуиции, что-то. Давайте посмотрим,
[14:55.840 --> 15:02.200]  могут ли они быть взависимыми. Да. Вот. Рассуждения пространные и философские начинаются. Нет. Это
[15:02.200 --> 15:08.400]  не есть доказательство независимости. Независимость — это термин в теории вероятности вполне конкретный,
[15:08.400 --> 15:14.400]  вполне определенный. Он не связан с функциональной независимостью, он не связан с причинно-следственными
[15:14.400 --> 15:24.120]  связями. Никак. Вот. Это вполне себе определенная вещь. Так что и в этой теореме я вам показал,
[15:24.120 --> 15:30.760]  как это доказывается. Ну, сложно, да, но тем не менее. Зато мы все сделали как надо. Взяли эту
[15:30.760 --> 15:37.720]  вероятность и показали, что она расщепляется. Вот. Хорошо. Теорема о явной конструкции доказана.
[15:37.720 --> 15:43.400]  Теперь для нас плацоновский процесс — это неважно, что, неважно, как мы его определяем. Мы его
[15:43.400 --> 15:50.640]  могли определить как оксиоматически, а мы могли, в принципе, определить случайно плацоновский
[15:50.640 --> 15:56.480]  процесс вот так. То есть изначально говорить будем называть вот этот процесс плацоновским. Это
[15:56.480 --> 16:01.400]  неважно, потому что мы показали, что у них семейства, конечно, мерных распределений совпадают,
[16:01.400 --> 16:08.480]  что одно есть другое. Кстати говоря, можно было бы доказывать по другому теорему. Скажем,
[16:08.480 --> 16:18.320]  скажем, скажем, взять за основу определение, которое у нас было там, оксиоматическое,
[16:18.320 --> 16:24.560]  и доказывать, что тогда процесс имеет вот такой вид. По-моему, можно было и так доказывать — это
[16:24.560 --> 16:30.640]  другая теорема, но мы бы пришли к тому же самому. Вот. Но я выбрал вот такой подход — взять вот
[16:30.640 --> 16:36.280]  этой и доказать, что это то. Это неважно, неважно, как ты доказываешь, потому что в итоге ты
[16:36.280 --> 16:41.440]  приходишь все равно к одному и тому же семейству конечномерных распределений. Так что не нужно
[16:41.440 --> 16:45.360]  доказывать в обе стороны, достаточно доказать только в одну сторону. Как только семейство
[16:45.360 --> 16:50.400]  конечномерных распределений получено — все, тебе не важно изначальная конструкция процесса.
[16:50.400 --> 16:58.760]  Так, ладно. Какие следствия из этой теоремы мы можем получить мгновенные? А очень много
[16:58.760 --> 17:23.200]  следствий. Во-первых, во-первых, насчет следствия. Во-первых, мы понимаем, что скачки происходят в моменты
[17:23.200 --> 17:40.120]  t. В моменты времени случайные tau1 равняется s1, tau2 равняется s2 и так далее. Вот. Потому что когда
[17:40.120 --> 17:52.680]  t, если, ну давайте снова я напишу, xt, н такое, что sn меньше либо равно t. Вот, когда у нас t
[17:52.680 --> 18:00.800]  увеличивается, вот оно превзошел через какой-то sn, у нас n сразу поднялось, да, и xt поднялся на
[18:00.800 --> 18:09.680]  единичку. Так что вот эти вот s — это наши моменты времени, когда процесс испытывает скачок. А как
[18:09.680 --> 18:17.760]  распределены эти моменты времени мы знаем, потому что sn — это есть сумма показательных независимых
[18:17.760 --> 18:24.280]  случайных величин с одним и тем же параметром. Так что вот эти tau, n — это есть распределение
[18:24.280 --> 18:30.200]  рланга с параметрами n лямбда. Сумма показательных независимых случайных величин, n штук с
[18:30.200 --> 18:35.760]  параметром лямбда. Так что мы получаем из этой теоремы сразу же, как распределены моменты
[18:35.760 --> 18:41.880]  времени, когда происходят скачки. Обратите внимание, не случайные моменты времени. Вот,
[18:41.880 --> 18:54.160]  дальше что еще можно заметить, что между скачками какое время проходит? tau, n – tau, n – 1. Вот это
[18:54.160 --> 19:00.640]  время между скачками. Оно случайное, это случайная величина. Но это sn, а это sn – 1,
[19:00.640 --> 19:07.960]  так что это есть xn, и это показательное распределение. Значит, интервалы между
[19:07.960 --> 19:14.960]  случайными скачками имеют показательное распределение, и эти интервалы не зависят
[19:14.960 --> 19:20.960]  друг от друга в смысле стахастической независимости. Вот что мы с вами получаем.
[19:20.960 --> 19:30.680]  Третье. Насколько может происходить скачок? Ну, давайте так вот. Возможно ли скачок на
[19:30.680 --> 19:39.680]  величину 2 или 3, то есть больше чем на единицу? Вероятность того, что существует скачок,
[19:39.680 --> 19:50.400]  который больше либо равен чем 2, это то же самое, что какие-то эски совпали. То есть существует n,
[19:50.400 --> 19:58.520]  такие, что sn равняется sn плюс 1. То есть какие-то моменты времени совпали, и у тебя как бы на один
[19:58.520 --> 20:04.440]  произошел скачок, и сразу же на другой произошел скачок одновременно. Вот. А это означает,
[20:04.440 --> 20:15.960]  что существует n такое, что xn, какой-то xn, ну, xn плюс 1 получается равно нулю. Но вероятность
[20:16.680 --> 20:23.720]  что xn равно нулю, оно равно нулю просто потому что xy это непрерывная случайная величина. А вероятность
[20:23.720 --> 20:31.200]  вот этого события, она не превосходит, что получается, суммы вероятности событий вот таких,
[20:31.200 --> 20:37.840]  а они все равны нулю. Вот. Ну, здесь счетное число событий, вероятности все равны нулю,
[20:37.840 --> 20:44.200]  так что это тоже равно нулю. Здесь получается, что с вероятностью единицы скачок происходит на
[20:44.200 --> 20:50.120]  единицу, он не происходит сразу на два или на три или на сколько-то. Это может произойти, то есть в
[20:50.120 --> 20:57.400]  принципе это возможно, но вероятность этого равна нулю. Вот так. В принципе это возможно,
[20:57.400 --> 21:08.920]  но вероятность этого равна нулю. Вот такие мгновенные свойства мы получаем для пуласоновского процесса.
[21:08.920 --> 21:18.200]  Давайте я картинку такую нарисую. Что такое пуласоновский процесс? Значит, он стартует из нуля, мы выяснили.
[21:18.200 --> 21:37.560]  Вот это tau1, вот это tau2, вот это tau3, вот это Erlang 1 лямбда, это экспонента от лямбда, вот это
[21:37.560 --> 21:48.000]  Erlang 2 лямбда и так далее. Вот это показательное распределение, вот это показательное распределение.
[21:48.000 --> 22:00.760]  Они независимы. Все вот эти вот скачки в совокупности независимы. И вот это на единицу с вероятностью единицы
[22:00.760 --> 22:10.280]  происходит. Вот кусочно постоянное, не убывающее. Вот такая вот штука. Наш пуласоновский процесс.
[22:10.280 --> 22:17.680]  Пуласоновский процесс можно еще записать вот в таком виде.
[22:17.680 --> 22:37.440]  КАТ равняется сумма и равно от единиц до КАТ, от единиц. То есть КАТ единиц, где КАТ, это вот случайная величина с пуласоновским распределением,
[22:37.440 --> 22:47.440]  наш процесс. Можно записать процесс вот таким образом. И отсюда можно получить обобщение пуласоновского процесса
[22:47.440 --> 22:54.440]  То есть можно рассмотреть вот такие, давайте КАС чертуем, допустим назовем для простоты. Нет, давайте КАС.
[22:54.440 --> 23:11.440]  Вот. Сумма и равна от единицы до КАТ в сиитых. То есть здесь у нас как бы единица тождественная, а здесь мы можем поставить вместо этой единицы случайные величины, какие-то независимые к сиитые.
[23:11.440 --> 23:17.440]  Вот. Тогда вот такие вот процессы называются сложными пуласоновскими процессами.
[23:17.440 --> 23:27.440]  Ложный пуласоновский процесс. По-английски это называется compound.
[23:28.440 --> 23:46.440]  Будете где-то читать? Compound. Пуласовский процесс. Поэтому КАС. Вот. И, допустим, если взять ксиитую, то есть могут быть самые разные здесь случайные величины. Разные там они могут быть дискретны, непрерывны.
[23:46.440 --> 23:54.440]  Вот здесь вот стоять. Это будет все сложный пуласоновский процесс. Ну вот, допустим, ксиитая у нас принадлежит бернулевской случайной величине от П.
[23:54.440 --> 24:06.440]  Давайте мы посмотрим, что это такое. То есть это мы складываем нолики и единички. Единичка здесь будет с вероятностью П, нолик с вероятностью 1 минус П.
[24:06.440 --> 24:20.440]  Это все равно, что если бы мы взяли вот этот процесс, исходный пуласоновский, и каждый из скачков мы бы оставили с вероятностью П и удалили с вероятностью 1 минус П.
[24:21.440 --> 24:30.440]  Такой разреженный или просеянный получается пуласоновский процесс. Вот если мы запишем вот такую конструкцию, где-то обычный пуласоновский процесс,
[24:30.440 --> 24:44.440]  оксиды независимые распределены вот так, то можно доказать, что вот этот процесс тоже пуласоновский, но у него параметр не лямбда, как у этого, а лямбда П.
[24:44.440 --> 24:58.440]  То есть это тоже пуласоновский процесс с параметром лямбда П. Вот где П, это вот эта П, которая здесь стоит, а лямбда это параметр вот этого пуласоновского процесса,
[24:58.440 --> 25:09.440]  исходного, какой мы только что разбирали. Вот это можно доказать. Есть одно из обобщений пуласоновского процесса, сложный пуласоновский процесс.
[25:09.440 --> 25:19.440]  Ну и еще одно обобщение пуласоновского процесса, то, что называется неоднородный пуласоновский процесс, давайте я тоже это запишу.
[25:19.440 --> 25:24.440]  Он определяется точно так же, как исходный, но отличие только в одном пункте.
[25:24.440 --> 25:34.440]  Значит, когда мы пишем в третьем пункте kt-ks, любых t больше s, определяется как пуласоновская случайная величина.
[25:34.440 --> 25:44.440]  Вот с таким параметром от s до t некоторые функции, скажем от tau до tau, где это некоторая неотрицательная функция.
[25:45.440 --> 25:55.440]  Вот, ну и тогда вот получается такой неоднородный, он называется неоднородным пуласоновским процессом k и, давайте мы его обозначим.
[25:55.440 --> 26:04.440]  Сейчас скажу почему. Вот, неоднородный пуласоновский процесс уже не с параметром, а с функцией лямда, а tau.
[26:04.440 --> 26:13.440]  Вот простой процесс пуласона, там это функция константа, для любых tau от 0 до бесконечности равно некоторой лямбде.
[26:13.440 --> 26:17.440]  Но вот можно еще рассматривать такие конструкции, где это функция.
[26:17.440 --> 26:24.440]  То есть интенсивность, когда интенсивность процесса зависит от времени. Неоднородный, да, процесс получается.
[26:24.440 --> 26:35.440]  Это неоднородный пуласоновский процесс.
[26:35.440 --> 26:45.440]  Ниус, так что ли пишется? Или там буква o?
[26:45.440 --> 27:05.440]  Вот такие два обобщения.
[27:05.440 --> 27:17.440]  Так, ну вот, на этом материал, касающийся пуласоновского процесса, такой предварительный заканчивается.
[27:17.440 --> 27:27.440]  Мы потом еще будем возвращаться к пуласоновскому процессу много раз, когда будем о чем-то говорить и будем развивать дальше его свойства, изучать.
[27:27.440 --> 27:33.440]  И еще во втором задании тоже мы вспомним про пуласоновский процесс и еще больше свойства изучим.
[27:33.440 --> 27:39.440]  А пока на этом такая логическая мысль завершается.
[27:39.440 --> 27:46.440]  Итак, мы вели с вами некий процесс, задали его оксиоматически. Его математическое ожидание дисперсии равняется лямдо t.
[27:46.440 --> 27:52.440]  Он не убывает кусочно-постоянно, прыжки только на единицу, интервалы распылены показательно, независимы.
[27:52.440 --> 27:57.440]  Случайные моменты времени он скачет, они распылены по иерлангу.
[27:57.440 --> 28:06.440]  Вот, значит, доказали явную конструкцию, вывели, выяснили, что можно было сдавать так, можно было сдавать так, как угодно.
[28:06.440 --> 28:16.440]  И я вам дал вот два таких обобщения пуласоновского процесса.
[28:16.440 --> 28:29.440]  На этом мы эту линию заканчиваем и переходим к новой большой теме, которая называется нормальные процессы или гауссовские процессы.
[28:46.440 --> 29:05.440]  Гауссовские или нормальные
[29:05.440 --> 29:13.440]  процессы.
[29:14.440 --> 29:22.440]  Значит, по определению гауссовским или нормальным процессом называется случайный
[29:22.440 --> 29:32.440]  процесс, у которого все конечномерные распределения являются нормальными.
[29:32.440 --> 29:38.440]  Вот, одномерное распределение, двумерное распределение и так далее, все они являются нормальными.
[29:38.440 --> 29:48.440]  Вот, это означает, что если вам дан гауссовский процесс x от t, то тогда вектор, который составлен из его сечений,
[29:48.440 --> 29:56.440]  xtn, вот такой вектор, он имеет нормальное распределение.
[29:56.440 --> 30:04.440]  Нормально распределен. Вот такие процессы называются нормальными или гауссовскими.
[30:04.440 --> 30:12.440]  Значит, когда мы будем работать с нормальными процессами, нам придется
[30:12.440 --> 30:18.440]  использовать множество утверждений из теории вероятностей о нормальных векторах.
[30:18.440 --> 30:26.440]  Потому что, видите, мы берем сечение, это нормальный вектор, и часто нам приходится работать с векторами из сечений, то есть работать с нормальными векторами.
[30:26.440 --> 30:36.440]  И я хотел вам напомнить некоторые сведения полезные из теории вероятностей, касающиеся нормальных векторов, которые нам пригодятся дальше.
[30:36.440 --> 30:40.440]  И для доказательства теорем, и для решения задач.
[30:40.440 --> 30:46.440]  Но первое, что я сделаю, я напомню, что называется нормальным вектором.
[30:46.440 --> 30:52.440]  Значит, вектор x1, xn называется нормальным,
[30:52.440 --> 30:58.440]  если его характеристическая функция имеет определенный вид.
[30:58.440 --> 31:06.440]  И, ну, давайте x его большой обозначим, fix большое от s, это есть экспонента
[31:06.440 --> 31:14.440]  от e, минимум единица, на μ транспонированная s, mu это вектор, s это вектор.
[31:14.440 --> 31:19.440]  У s вектор столько же компонент, сколько у x здесь, n.
[31:19.440 --> 31:26.440]  И минус одна вторая, s транспонированная, r, s.
[31:26.440 --> 31:34.440]  Вот. Вектор называется нормальным, если его характеристическая функция имеет вот такой вид.
[31:34.440 --> 31:40.440]  Mu это вектор с n компонентами, r это матрица n на n, квадратная.
[31:40.440 --> 31:43.880]  Mu, это вектор математических ожиданий, как можно доказать mu,
[31:43.880 --> 31:46.880]  равняется математическому ожиданию x, x это вектор.
[31:46.880 --> 31:55.880]  Мат ожидание вектора по определению, это вектор математических ожиданий.
[31:55.880 --> 32:01.840]  корреляционная матрица, это матрица n на n, и ее можно записать так,
[32:01.840 --> 32:04.840]  это x центрированная на x центрированная транспонированная.
[32:04.840 --> 32:09.840]  Вот, x центрированная это вектор, столбец,
[32:09.840 --> 32:18.740]  x транспонирован это вектор строка столбец умножается на строку получается матрица не путайте с x транспонированная x которая скаляр
[32:20.040 --> 32:27.040]  вот то есть это математическое ожидание что такое центрированная это вычитание мю мю на x минус мю
[32:28.080 --> 32:30.080]  транспонированная
[32:30.080 --> 32:32.080]  вот
[32:33.160 --> 32:35.160]  ну давайте
[32:35.360 --> 32:37.360]  несколько свойств
[32:37.360 --> 32:39.360]  нормальных векторов вспомним
[32:40.840 --> 32:42.840]  свойства
[32:46.200 --> 32:48.200]  нормальных векторов
[32:52.840 --> 32:56.560]  первое свойство которое говорит о следующем если вектор
[32:57.480 --> 33:03.200]  x нормальный с распределением давайте мы будем так обозначать мю и r потому что видите
[33:03.920 --> 33:06.040]  распределение вектора однозначно определяется
[33:06.720 --> 33:11.320]  вектором мю и матрица r если ты их задал все у тебя однозначно задана
[33:12.040 --> 33:14.520]  характеристическая функция вектора а значит и его распределение
[33:15.560 --> 33:18.400]  ну и вот такое обозначение будем использовать для
[33:20.120 --> 33:24.040]  нормальных векторов пусть вот так тогда то есть если вектор
[33:24.600 --> 33:30.280]  нормальный то тогда все его компоненты являются нормальными случайными величинами
[33:31.040 --> 33:35.800]  то тогда x это это есть нормальная случайная величина с
[33:36.080 --> 33:39.880]  математическим ожиданием мю и то я и то я компонента вот этого вектора мю
[33:40.600 --> 33:46.000]  запятая r и то я и то я и ты диагональный элемент матрицы r
[33:47.120 --> 33:53.680]  вот если вектор нормальный то все его компоненты имеют нормальное распределение
[33:54.400 --> 33:56.240]  обратное неверно
[33:56.240 --> 33:58.240]  если случайный вектор
[33:58.600 --> 34:05.600]  если у случайного вектора все компоненты нормальные это не значит что вектор нормальный
[34:06.320 --> 34:09.840]  вот есть контр примеры к этому можете найти в книжке
[34:10.360 --> 34:14.480]  натан гус гробачев по теории вероятности пример там такой рассмотрен но
[34:15.120 --> 34:17.520]  если все компоненты вектора
[34:18.800 --> 34:20.800]  независимы
[34:20.800 --> 34:22.640]  совокупности и
[34:22.640 --> 34:26.440]  являются нормальными то и тогда вектор из них
[34:27.200 --> 34:29.680]  является нормальным случайным вектором
[34:30.880 --> 34:32.880]  вот
[34:35.440 --> 34:43.600]  если вектор нормальный все компоненты нормальные если компоненты нормальные то не обязательно вектор нормальный вы если они независимы совокупность тогда вектор норм ay
[34:44.180 --> 34:46.180]  все
[34:46.420 --> 34:48.420]  едем дальше второе
[34:49.200 --> 34:51.200]  если у
[34:51.460 --> 34:53.760]  нормального вектора вот это матрица r
[34:54.340 --> 34:56.360]  обратима
[34:56.360 --> 34:59.600]  она может быть необратима, но если она обратима, то
[34:59.600 --> 35:05.360]  тогда у случайного вектора x существует плотность.
[35:05.360 --> 35:14.600]  Если существует r-1, то существует плотность вектора x.
[35:14.600 --> 35:22.960]  fx от x, это есть единицы разделить на 2p в степени n пополам
[35:22.960 --> 35:30.680]  корень детерминант r на экспонентал от минус, здесь
[35:30.680 --> 35:41.000]  2, здесь получается x-μ транспонированная, r в минус 1 на x-μ.
[35:41.000 --> 35:44.400]  Для любого x из r, n.
[35:44.400 --> 35:47.400]  Вот так, по-моему.
[35:47.400 --> 35:51.640]  Вот, если она обратима.
[35:51.680 --> 35:56.000]  В общем случае корреляционная матрица не отрицательно
[35:56.000 --> 35:58.800]  определена, то есть у нее детерминант может быть
[35:58.800 --> 36:03.520]  равен нулю, но если она положительно определена
[36:03.520 --> 36:06.880]  или это равносильно тому, что если существует обратная
[36:06.880 --> 36:11.360]  у корреляционной матрицы, то тогда существует плотность
[36:11.360 --> 36:13.320]  и она выражается вот по такой формуле.
[36:13.320 --> 36:22.960]  Вот, но если эта матрица не обратима, если не существует
[36:22.960 --> 36:30.920]  r в минус 1, то тогда существует некоторый вектор c, не нулевой,
[36:30.920 --> 36:33.600]  такой, что rc равно нулю.
[36:33.600 --> 36:39.440]  И отсюда можно доказать, что тогда, что здесь это
[36:39.440 --> 36:45.000]  означает, что у нас столбцы или строки линейно-зависимы,
[36:45.000 --> 36:49.480]  можно доказать, что тогда и компоненты x-а линейно-зависимы.
[36:49.480 --> 36:50.880]  Вот давайте мы это покажем.
[36:50.880 --> 36:54.120]  Да, c это вектор.
[36:54.120 --> 36:57.920]  Да, да, по компонентам каждая.
[36:57.920 --> 37:00.640]  Или норма его не равна нулю, ну, собственный вектор.
[37:00.640 --> 37:16.440]  Вектор не равен нулю, это означает, что норма c не равна нулю.
[37:16.440 --> 37:19.800]  Вот так.
[37:19.800 --> 37:22.200]  Вектор равен нулю, если все его компоненты равны нулю.
[37:22.200 --> 37:27.480]  Так, значит существует c не равен нулю, такое что?
[37:27.480 --> 37:29.200]  Значит, что отсюда следует?
[37:29.960 --> 37:34.160]  Если rc равняется нулю, давайте мы слева умножим
[37:34.160 --> 37:35.180]  на c транспонированное.
[37:35.180 --> 37:36.240]  Мы получим ноль, да?
[37:36.240 --> 37:37.240]  Ничего не изменится.
[37:37.240 --> 37:38.880] 星 транспонированное cH 같ается нулю.
[37:38.880 --> 37:43.680]  Мы просто c транспонированного множили на нулевой вектор.
[37:43.680 --> 37:44.760]  Получим опять же ноль.
[37:44.760 --> 37:48.720]  Теперь распишем такое r, c транспонированное, мат
[37:48.720 --> 37:52.600]  ожидания x центрированная, x центрированная, транспонированная
[37:52.600 --> 37:55.280]  на c, равно нулю.
[37:55.280 --> 37:56.400]  Вот мы r расписали.
[37:56.400 --> 38:01.200]  можем это делать, потому что c это константа, это не случайная величина.
[38:01.200 --> 38:05.160]  Получаем c транспонированная, x центрированная, x центрированная
[38:05.160 --> 38:11.240]  транспонированная c равно нулю. А здесь написано нечто транспонированное умножить
[38:11.240 --> 38:18.360]  на это нечто. То есть получается мат ожидания x центрированная
[38:18.700 --> 38:24.440]  c транспонированная на x центрированная c равно нулю.
[38:24.440 --> 38:29.520]  То есть мы транспонирование берем, c транспонирования на вот это транспонирование это x центрированная.
[38:29.520 --> 38:36.760]  Это мы просто переписали. Ну а это означает что математическое ожидание
[38:36.760 --> 38:45.900]  x центрированная t ц в квадрате равно нулю. Отсюда следует что мы имеем дело
[38:45.900 --> 38:49.940]  случайной величиной, не отрицательной она возводится в квадрат, но мат ожидания,
[38:49.940 --> 38:53.460]  которая равно нулю, ну некуда деваться, тогда она почти наверно должна быть равна нулю.
[38:53.460 --> 38:58.340]  х-центрированная Тc равно нулю почти наверно.
[38:58.340 --> 39:06.380]  Вот. Это означает, что линейно зависимая компонент у х. С вероятностью единица
[39:06.380 --> 39:09.340]  линейно зависимая компонент.
[39:09.860 --> 39:17.020]  с вектором от ожидания мил и корреляционной матрицы r. Тогда для любой матрицы а,
[39:17.020 --> 39:32.300]  значит, к на n, не обязательно невырожденный, даже не обязательно квадратный. А х это тоже будет
[39:32.300 --> 39:42.140]  нормальный вектор, но вот с такими компонентами. Математическое ожидание умножается на а,
[39:42.140 --> 39:51.380]  а корреляционная матрица будет а, р, а транспонированная. Вот видите, если мы
[39:51.380 --> 39:55.820]  линейно подействуем на х, мы снова получим нормальный вектор. То есть,
[39:55.820 --> 40:01.820]  линейные преобразования не выводят нас из нормальности, даже если они вырождены,
[40:01.820 --> 40:09.660]  даже если они не квадратные. Вот так. Для того, чтобы запомнить эту формулу,
[40:09.660 --> 40:16.140]  где тут транспонирование, здесь или там, предлагается такое смешное манемоническое правило.
[40:16.140 --> 40:22.780]  Марат. Видите, здесь. Когда ты помнишь марат, ты помнишь, что t на конце, поэтому t должна быть
[40:22.780 --> 40:33.020]  здесь, а не где-нибудь тут. Марат. Или а марат. Вот так, чтобы запомнить эту формулу.
[40:33.020 --> 40:45.380]  Так, дальше едем. Еще один вектор. Значит, вот это свойство, оно доказывается просто по
[40:45.380 --> 40:51.380]  определению. То есть, у нас есть вектор х, значит, у него есть характеристическая функция некоторая,
[40:51.620 --> 40:57.260]  и мы рассматриваем вот этот вектор ax. И если мы запишем его характеристическую функцию,
[40:57.260 --> 41:01.620]  мы увидим, что это характеристическая функция случайного вектора, нормального,
[41:01.620 --> 41:07.180]  вот с такими параметрами. Все. То есть, это чисто дело техники. Доказывать я это не буду,
[41:07.180 --> 41:14.340]  но вы можете этим пользоваться всюду. Так, четвертое. Еще одно очень важное свойство,
[41:14.740 --> 41:23.180]  в решении задач тоже очень помогает. Вектор х является нормальным тогда и только тогда,
[41:23.180 --> 41:31.580]  когда любая линейная комбинация его компонент является нормальной случайной величиной или
[41:31.580 --> 41:47.500]  константой. Вот как. То есть, х нормальный вектор, когда и только тогда, когда для любых ситх из r,
[41:47.500 --> 42:00.300]  сумма ситая и кситая это нормальная случайная величина, ну или константа. Ну, кстати говоря,
[42:00.300 --> 42:07.060]  вот эту добавку или константа иногда не произносит, подразумевая под этим то,
[42:07.060 --> 42:12.900]  что если это константа, то ее можно интерпретировать как нормальную случайную величину с нулевой
[42:12.900 --> 42:18.820]  дисперсией. Вот. Так что иногда вы можете встретить такие утверждения, где не говорится или константа,
[42:18.820 --> 42:26.140]  но как бы мы все понимаем, что это значит, что если это константа, значит, что как будто
[42:26.140 --> 42:31.140]  нормальная, но с нулевой дисперсией. Вот. А так вот, если быть до конца строгим, то надо
[42:31.140 --> 42:35.220]  добавлять или константа. Ну, кстати говоря, и здесь тоже, когда мы делаем какие-то линейные
[42:35.220 --> 42:42.340]  комбинации, некоторые из этих компонент айкса, они могут быть равны нулю, это не совсем нормальная
[42:42.340 --> 42:47.540]  случайная величина, но если ты под константами понимаешь случаи, когда дисперсия равна нулю,
[42:47.540 --> 42:55.260]  то как бы правило вот такое, оно общее остается. То есть, это очень удобно в этих случаях так считать.
[42:55.260 --> 43:00.860]  Значит, это доказательство я тоже не привожу, и оно тоже несложное через характеристическую
[43:00.860 --> 43:08.220]  функцию. Вот делается. Можете Ширяева, например, посмотреть, как доказывается, но этим вы тоже
[43:08.220 --> 43:18.860]  можете пользоваться. Так. Дальше идем. Еще одно важное свойство. Пять. Что компоненты нормального
[43:18.860 --> 43:30.140]  случайного вектора некоррелируемы тогда и только тогда, когда они независимы. Вот. Это очень важное
[43:30.140 --> 43:42.300]  свойство, но при этом студенты допускают здесь ошибки. Смотрите, здесь очень важно то, что если
[43:42.300 --> 43:48.660]  компоненты нормального случайного вектора некоррелируемы, тогда они независимы. Вот если
[43:48.660 --> 43:57.720]  просто случайные величины нормальные и некоррелируемые, то тогда они могут быть
[43:57.720 --> 44:04.420]  зависимыми. А здесь речь идет про компоненты. Быть компонентой нормального вектора это более
[44:04.420 --> 44:09.260]  сильное свойство, чем просто быть нормальной случайной величиной. Обратите на это внимание.
[44:09.260 --> 44:25.260]  То есть, если компоненты нормального вектора некоррелированы,
[44:25.260 --> 44:27.260]  следовательно, они независимы.
[44:27.260 --> 44:32.260]  Но из независимости следует некоррелируемость для любых случайных величин,
[44:32.260 --> 44:35.260]  не только для нормальных векторов, компонентов и величин.
[44:35.260 --> 44:38.260]  То есть, из независимости всегда следует некоррелируемость.
[44:38.260 --> 44:43.260]  А из некоррелируемости независимость следует не всегда.
[44:43.260 --> 44:47.260]  Вот оказывается, что для компонент нормального вектора это верно.
[44:47.260 --> 44:50.260]  Вот это вот следствие.
[44:50.260 --> 44:55.260]  Так, дальше едем.
[44:55.260 --> 44:59.260]  Еще одно свойство – проусловное распределение.
[44:59.260 --> 45:03.260]  Ну, это вам должно было быть доказано в теории вероятности даже.
[45:03.260 --> 45:06.260]  В принципе, это тоже чисто дело техники.
[45:06.260 --> 45:09.260]  Идейно там практически ничего нет.
[45:09.260 --> 45:10.260]  Шестое.
[45:10.260 --> 45:13.260]  Значит, пусть дан вектор Xi это.
[45:13.260 --> 45:15.260]  То есть, это вектор.
[45:15.260 --> 45:17.260]  И вот это тоже вектор.
[45:17.260 --> 45:20.260]  Этот с n компонентами, этот с m компонентами.
[45:20.260 --> 45:22.260]  Это нормальный вектор.
[45:22.260 --> 45:28.260]  Пусть это нормальный вектор.
[45:28.260 --> 45:29.260]  Вот.
[45:29.260 --> 45:35.260]  Тогда условное распределение Xi при условии на эту.
[45:35.260 --> 45:39.260]  Тоже имеет нормальное распределение.
[45:39.260 --> 45:41.260]  Сейчас запишу, какое именно.
[45:41.260 --> 45:44.260]  Ну, вот пусть Xi это нормальный вектор.
[45:44.260 --> 45:47.260]  С математическим ожиданием mu Xi.
[45:47.260 --> 45:51.260]  И корреляционной матрицей r Xi Xi.
[45:51.260 --> 45:59.260]  Пусть это нормальное распределение имеет с мат ожиданием mu это и r это это.
[45:59.260 --> 46:03.260]  И пусть существует r это это в минус первой.
[46:03.260 --> 46:05.260]  Пусть матрица вот эта обратима.
[46:05.260 --> 46:07.260]  На r Xi все равно.
[46:07.260 --> 46:10.260]  Но пусть вот эта матрица обратима.
[46:12.260 --> 46:13.260]  Вот.
[46:13.260 --> 46:16.260]  Тогда вот такое выражение известно.
[46:16.260 --> 46:19.260]  Но оно большое, я его отсюда перепишу.
[46:19.260 --> 46:24.260]  Что условное распределение вот при таком условии.
[46:24.260 --> 46:27.260]  Это тоже нормальный случайный вектор.
[46:27.260 --> 46:32.260]  С математическим ожиданием mu Xi.
[46:32.260 --> 46:34.260]  Плюс r Xi это.
[46:34.260 --> 46:38.260]  Это корреляционная матрица взаимная для векторов Xi это.
[46:38.260 --> 46:39.260]  Ну, я сейчас напишу.
[46:39.260 --> 46:40.260]  Вот.
[46:40.260 --> 46:44.260]  На r это это в минус первой.
[46:44.260 --> 46:47.260]  На x минус.
[46:47.260 --> 46:49.260]  Mu это.
[46:49.260 --> 46:51.260]  Запятая.
[46:51.260 --> 46:53.260]  Здесь будет у нас стоять r Xi Xi.
[46:53.260 --> 46:57.260]  Минус r Xi это.
[46:57.260 --> 47:01.260]  На r это это в минус первой.
[47:01.260 --> 47:03.260]  На r это Xi.
[47:03.260 --> 47:04.260]  Вот.
[47:04.260 --> 47:05.260]  А что такое r Xi это?
[47:05.260 --> 47:08.260]  Это значит у нас вот этот вектор.
[47:08.260 --> 47:11.260]  Он имеет пусть распределение нормальное.
[47:11.260 --> 47:15.260]  Вектора математических ожиданий mu Xi mu это.
[47:15.260 --> 47:18.260]  И матрицей корреляционной вот такой.
[47:18.260 --> 47:19.260]  Она блочная.
[47:19.260 --> 47:20.260]  Значит здесь r Xi Xi.
[47:20.260 --> 47:22.260]  Здесь r это это.
[47:22.260 --> 47:25.260]  Здесь у нас будет стоять r Xi это.
[47:25.260 --> 47:28.260]  r это Xi.
[47:28.260 --> 47:31.260]  Вот.
[47:31.260 --> 47:35.260]  Вот тогда это вот эта штука.
[47:35.260 --> 47:36.260]  Так.
[47:36.260 --> 47:38.260]  Правильно я там расставил?
[47:38.260 --> 47:40.260]  Да, правильно.
[47:40.260 --> 47:41.260]  Ну вот.
[47:41.260 --> 47:43.260]  Ну и выводить я эту формулу для векторов не буду.
[47:43.260 --> 47:46.260]  Я просто скажу вкратце примерно откуда она берется.
[47:46.260 --> 47:47.260]  Вот смотрите.
[47:47.260 --> 47:51.260]  Допустим для простоты у нас Xi и это сколяры.
[47:51.260 --> 47:53.260]  Не вектора, а сколяры.
[47:53.260 --> 47:58.260]  Тогда они являются компонентами нормального вектора.
[47:58.260 --> 48:01.260]  А это означает, а мы знаем свойства.
[48:01.260 --> 48:07.260]  Пятое, что если компоненты нормального вектора не каллерируемые, значит они независимы.
[48:07.260 --> 48:10.260]  Таким образом мы можем посмотреть вот на эту вероятность следующим образом.
[48:10.260 --> 48:22.260]  Вероятность того, что Xi, допустим, меньше какого-то y при условии, что это равняется x.
[48:22.260 --> 48:25.260]  Вот как вычислить такую условную вероятность?
[48:25.260 --> 48:35.260]  Давайте мы заметим, что это равно вероятности того, что Xi минус a это меньше, чем y меньше a это.
[48:35.260 --> 48:37.260]  А это равна x.
[48:37.260 --> 48:44.260]  Ой, меньше y минус ax при условии, что это равняется x.
[48:44.260 --> 48:46.260]  Вот мы можем записать вот так.
[48:46.260 --> 48:53.260]  В итоге здесь стоит некая величина меньше некоторого числа при условии вот этого события.
[48:53.260 --> 48:57.260]  Это зависит только от это, а эта штука зависит от такой ленинной комбинации.
[48:57.260 --> 48:59.260]  Xi минус a это.
[48:59.260 --> 49:10.260]  Так вот, мы можем подобрать такую a, чтобы вот эта величина и вот эта величина были не коррелируемыми.
[49:10.260 --> 49:13.260]  Просто к авариации их равна нулю и оттуда найти a.
[49:13.260 --> 49:16.260]  Линейное уравнение относить на просто решить.
[49:16.260 --> 49:25.260]  Так как они являются компонентами некоторого нормального случайного вектора, вот такого.
[49:25.260 --> 49:31.260]  Это нормальный случайный вектор, потому что он получен линейной комбинацией от исходного случайного вектора Xi.
[49:31.260 --> 49:39.260]  Так как они являются компонентами некоторого нормального случайного вектора, то из их некоррелируемости будет следовать их независимость.
[49:39.260 --> 49:45.260]  Так что нам остается найти a так, чтобы эти штуки были не коррелируемыми, тогда они будут независимыми.
[49:45.260 --> 49:49.260]  А раз они не независимы, то вот это условие можно убрать условное.
[49:49.260 --> 49:53.260]  Тогда для такого a получится вероятность вот этого меньше вот этого.
[49:53.260 --> 49:59.260]  А это уже нормальная случайная величина с известными там мат. ожиданием и дисперсией и все находится.
[49:59.260 --> 50:03.260]  И эти рассуждения можно провести для векторов Xi.
[50:03.260 --> 50:07.260]  Ну так вот, примерно, чтобы вы себе представляли откуда это берется.
[50:10.260 --> 50:12.260]  Так, хорошо.
[50:15.260 --> 50:17.260]  Это я вам рассказал.
[50:19.260 --> 50:23.260]  И, наконец, все вот эти свойства так или иначе у вас были.
[50:23.260 --> 50:28.260]  Но сейчас я вам напишу еще одну теорему без доказательства,
[50:28.260 --> 50:35.260]  которая у вас не была, но она супер полезная и в случайных процессах она очень сильно помогает.
[50:35.260 --> 50:39.260]  Она называется теоремой Вика.
[50:39.260 --> 50:42.260]  Или еще ее называют теоремой ИС Эрлиса.
[50:50.260 --> 50:53.260]  Это какой там? Седьмой, наверное, получается, пункт.
[51:02.260 --> 51:04.260]  Теорема Вика.
[51:10.260 --> 51:12.260]  Она вот о чем.
[51:12.260 --> 51:22.260]  Значит, пусть дан нормальный случайный вектор X с нулевой мат. ожиданием.
[51:22.260 --> 51:24.260]  Это вектор. X-то тоже вектор.
[51:24.260 --> 51:30.260]  Нулевое мат. ожидание и некоторая корреляционная матрица R.
[51:30.260 --> 51:34.260]  Она может быть выражена, вообще говоря.
[51:34.260 --> 51:36.260]  Вот.
[51:36.260 --> 51:39.260]  То есть X у нас из Rn.
[51:39.260 --> 51:41.260]  N-компонент.
[51:41.260 --> 51:51.260]  Тогда, если n нечетно, то мат. ожидание x1, x2 и т.д. xn равно нулю.
[51:51.260 --> 51:53.260]  Вот.
[51:53.260 --> 51:57.260]  Даже если x-ы зависимые, мы не можем расщепить,
[51:57.260 --> 51:59.260]  то мы не можем расщепить их.
[51:59.260 --> 52:01.260]  То есть x1 равно нулю.
[52:05.260 --> 52:07.260]  Вот.
[52:07.260 --> 52:11.260]  Даже если x-ы зависимые, мы не можем расщепить
[52:11.260 --> 52:14.260]  мат. ожидания произведения на произведение мат. ожидания.
[52:14.260 --> 52:16.260]  Это все равно будет ноль.
[52:18.260 --> 52:27.260]  Если n-четно, то мат. ожидание вот этого произведения.
[52:27.260 --> 52:35.260]  Сумма произведений компонент R.
[52:35.260 --> 52:37.260]  Q1 и т.д.
[52:37.260 --> 52:41.260]  Здесь n пополам множителей.
[52:41.260 --> 52:46.260]  Сумма ведутся по P и по Q.
[52:46.260 --> 52:54.260]  Значит, где сумма берется по всем разбиениям множества.
[52:54.260 --> 53:06.260]  Сумма берется по всем разбиениям множества.
[53:06.260 --> 53:12.260]  1n на n пополам пар.
[53:12.260 --> 53:16.260]  Звучит сложно, но сейчас я вам покажу несколько примеров,
[53:16.260 --> 53:18.260]  и сразу все поймете.
[53:18.260 --> 53:20.260]  И почему здесь написано ровно так, как написано.
[53:20.260 --> 53:22.260]  Значит, это со звездочкой.
[53:22.260 --> 53:24.260]  Я не буду доказывать эту теорему.
[53:24.260 --> 53:26.260]  Она очень техническая и безыдейная.
[53:26.260 --> 53:28.260]  Практически.
[53:28.260 --> 53:30.260]  Смотрите.
[53:30.260 --> 53:34.260]  Вот пусть нам дал вектор с четырьмя компонентами.
[53:34.260 --> 53:36.260]  И как мы можем это сделать?
[53:36.260 --> 53:38.260]  Смотрите.
[53:38.260 --> 53:42.260]  Вот пусть нам дал вектор с четырьмя компонентами.
[53:42.260 --> 53:48.260]  x1, x2, x3, x4.
[53:48.260 --> 53:52.260]  Любой его подвектор это и есть нормальный вектор,
[53:52.260 --> 53:54.260]  потому что он может быть получен как линейная комбинация.
[53:54.260 --> 53:56.260]  Вот.
[53:56.260 --> 53:58.260]  Тогда смотрите.
[53:58.260 --> 54:00.260]  Математическое ожидание x1 равно 0.
[54:00.260 --> 54:02.260]  Но мы это и так понимаем, потому что нам дали такой вектор x.
[54:02.260 --> 54:08.260]  Ну мы от ожидания x1, x2, x3 тоже равно 0.
[54:08.260 --> 54:10.260]  Даже если они зависимы,
[54:10.260 --> 54:12.260]  эти x все равно будет равно 0.
[54:12.260 --> 54:14.260]  А почему мы пишем равно 0?
[54:14.260 --> 54:16.260]  Потому что их здесь нечетное число стоит.
[54:16.260 --> 54:18.260]  1, 2, 3. Их нечетное число.
[54:18.260 --> 54:20.260]  Вот.
[54:20.260 --> 54:24.260]  Теперь рассмотрим от ожидания x1, x2, x3, x4.
[54:24.260 --> 54:26.260]  Что нам надо сделать?
[54:26.260 --> 54:30.260]  Нам надо разбить множество 1, 2, 3, 4
[54:30.260 --> 54:32.260]  неупорядочным образом на пары неупорядочные.
[54:32.260 --> 54:34.260]  Как мы можем это сделать?
[54:34.260 --> 54:36.260]  1, 2, 3, 4.
[54:36.260 --> 54:38.260]  1, 3, 2, 4.
[54:38.260 --> 54:40.260]  1, 4, 2, 3.
[54:40.260 --> 54:44.260]  Пишем r1, 2, 3, 4.
[54:44.260 --> 54:48.260]  1, 3, 2, 4.
[54:48.260 --> 54:50.260]  1, 4, 2, 3.
[54:50.260 --> 54:52.260]  Все.
[54:52.260 --> 54:54.260]  Вот.
[54:54.260 --> 54:56.260]  Берем вот это множество и делим его на пары.
[54:56.260 --> 54:58.260]  Понятно?
[54:58.260 --> 55:00.260]  Вот так вот на примере.
[55:00.260 --> 55:02.260]  В чем смысл говорить неупорядочные?
[55:02.260 --> 55:04.260]  Потому что мы не различаем
[55:04.260 --> 55:06.260]  разбиение.
[55:06.260 --> 55:08.260]  Мы разбиваем вот это множество.
[55:08.260 --> 55:10.260]  Мы, например,
[55:10.260 --> 55:12.260]  не различаем вот такое разбиение
[55:12.260 --> 55:14.260]  и вот такое разбиение.
[55:14.260 --> 55:16.260]  Здесь мы можем написать не 1, 2, а 2, 1.
[55:16.260 --> 55:18.260]  Но сюда не входит слагаемое,
[55:18.260 --> 55:20.260]  кроме вот этого,
[55:20.260 --> 55:22.260]  сюда не входит слагаемое r2, 1, r3, 4.
[55:22.260 --> 55:24.260]  Нам не нужны упорядочные разбиения.
[55:24.260 --> 55:26.260]  Их было бы больше.
[55:26.260 --> 55:28.260]  Больше слагаемых было бы.
[55:28.260 --> 55:30.260]  Нас интересует только неупорядочное.
[55:30.260 --> 55:32.260]  Один раз получили 1, 2, 3, 4.
[55:32.260 --> 55:34.260]  Все. Это мы считаем, что разбили.
[55:34.260 --> 55:36.260]  То есть вот такими мы считаем,
[55:36.260 --> 55:38.260]  что это одно и то же.
[55:38.260 --> 55:40.260]  И точно так же, например,
[55:40.260 --> 55:42.260]  3, 4, 1, 2.
[55:42.260 --> 55:44.260]  Вот.
[55:44.260 --> 55:46.260]  Мы считаем, что это тоже ничего
[55:46.260 --> 55:48.260]  нам нового не дает.
[55:48.260 --> 55:50.260]  В этом смысл вот этих неупорядоченностей.
[55:50.260 --> 55:52.260]  Да.
[55:54.260 --> 55:56.260]  Вот.
[55:56.260 --> 55:58.260]  Это тоже можно,
[55:58.260 --> 56:00.260]  потому что сейчас я вам продемонстрирую.
[56:04.260 --> 56:06.260]  Я же никак не сказал о том,
[56:06.260 --> 56:08.260]  что эти иксы обязательно разные.
[56:08.260 --> 56:10.260]  Я даже сказал,
[56:10.260 --> 56:12.260]  что они могут быть зависимыми между собой.
[56:12.260 --> 56:14.260]  И равенство вполне себе допускается.
[56:14.260 --> 56:16.260]  Потому что, например,
[56:16.260 --> 56:18.260]  если вы хотите вычислить
[56:20.260 --> 56:22.260]  вот такую величину,
[56:22.260 --> 56:24.260]  ну, как на это можно формально
[56:24.260 --> 56:26.260]  так вот посмотреть?
[56:26.260 --> 56:28.260]  Как будто у вас есть вектор
[56:28.260 --> 56:30.260]  x1, x2, x3, x4.
[56:30.260 --> 56:32.260]  Он нормальный.
[56:32.260 --> 56:34.260]  Да. Допустим.
[56:34.260 --> 56:36.260]  Но вы же можете из него
[56:36.260 --> 56:38.260]  линейной комбинацией получить
[56:38.260 --> 56:40.260]  x1, x1, x1, x1.
[56:40.260 --> 56:42.260]  Матрицу можно такую придумать,
[56:42.260 --> 56:44.260]  что при умножении на это получится
[56:44.260 --> 56:46.260]  вот такой вектор. Значит, это тоже
[56:46.260 --> 56:48.260]  нормальный случайный вектор.
[56:48.260 --> 56:50.260]  Вот. Так что вы можете
[56:50.260 --> 56:52.260]  посмотреть на него как на y1, y2,
[56:52.260 --> 56:54.260]  y3, y4.
[56:54.260 --> 56:56.260]  У него нулевые мат ожидания
[56:56.260 --> 56:58.260]  и некоторая матрица корреляции.
[56:58.260 --> 57:00.260]  Так что вы можете применять
[57:00.260 --> 57:02.260]  эту теорему для вектора y,
[57:02.260 --> 57:04.260]  но потом от y перейти от x к вот этим
[57:04.260 --> 57:06.260]  штукам. Так что
[57:06.260 --> 57:08.260]  равенство каких-то компонентов вполне
[57:08.260 --> 57:10.260]  себе допускается.
[57:10.260 --> 57:12.260]  И что это будет? Смотрите, как можно
[57:12.260 --> 57:14.260]  написать. Мат ожидания x1,
[57:14.260 --> 57:16.260]  x1, x1,
[57:16.260 --> 57:18.260]  x1. То есть мы должны
[57:18.260 --> 57:20.260]  разбить. То есть мы смотрим на это как бы
[57:20.260 --> 57:22.260]  на 1, 2, 3, 4.
[57:22.260 --> 57:24.260]  И мы пытаемся разбить это множество на пары.
[57:24.260 --> 57:26.260]  Мы должны взять вот эти и вот эти.
[57:26.260 --> 57:28.260]  Вот эти, вот эти, вот эти,
[57:28.260 --> 57:30.260]  вот эти. Так что получается,
[57:30.260 --> 57:32.260]  когда мы возьмем эти, мы получим
[57:32.260 --> 57:34.260]  r11
[57:34.260 --> 57:36.260]  и умножить на r11.
[57:36.260 --> 57:38.260]  Плюс вот эти теперь.
[57:38.260 --> 57:40.260]  Это тоже r11. Вот на эти
[57:40.260 --> 57:42.260]  r11. Плюс
[57:42.260 --> 57:44.260]  теперь вот эти две.
[57:44.260 --> 57:46.260]  r11 и внутренние две r11.
[57:46.260 --> 57:48.260]  Ну что, теперь бывает.
[57:48.260 --> 57:50.260]  Ну и получается это 3
[57:50.260 --> 57:52.260]  r11 в квадрате.
[57:52.260 --> 57:54.260]  Вот. А r11
[57:54.260 --> 57:56.260]  это
[57:56.260 --> 57:58.260]  диагональный элемент матрицы
[57:58.260 --> 58:00.260]  корреляции. Это дисперсия
[58:00.260 --> 58:02.260]  первой компоненты. И она еще
[58:02.260 --> 58:04.260]  в квадрат возводится. Значит получается
[58:04.260 --> 58:06.260]  3 сигма 1 в четвертой степени.
[58:06.260 --> 58:08.260]  Вот.
[58:08.260 --> 58:10.260]  Вот вычислили.
[58:10.260 --> 58:12.260]  Вот такая теорема замечательная.
[58:12.260 --> 58:14.260]  Несколько слов скажу примерно откуда
[58:14.260 --> 58:16.260]  это все берется.
[58:16.260 --> 58:18.260]  Значит, без доказательства.
[58:18.260 --> 58:20.260]  На самом деле все это, все это очень
[58:20.260 --> 58:22.260]  легко. Может вообще показаться очень
[58:22.260 --> 58:24.260]  странным, да? Почему?
[58:24.260 --> 58:26.260]  Моменты, произвольные
[58:26.260 --> 58:28.260]  моменты.
[58:28.260 --> 58:30.260]  Разной степени. Если там повторяются
[58:30.260 --> 58:32.260]  какие-то тексты, то можно там вычислять
[58:32.260 --> 58:34.260]  типа мат ожидания x1 в пятый
[58:34.260 --> 58:36.260]  на x2 в шестой,
[58:36.260 --> 58:38.260]  на x3 в седьмой.
[58:38.260 --> 58:40.260]  Если сумма всех степеней нечетная, сразу
[58:40.260 --> 58:42.260]  вычислишь ноль все. Вот.
[58:42.260 --> 58:44.260]  А если четная, ну тогда вот по этим формулам
[58:44.260 --> 58:46.260]  все вычисляется. И все выражается через
[58:46.260 --> 58:48.260]  вторые моменты, через корреляционные
[58:48.260 --> 58:50.260]  моменты. А почему? А это очень просто.
[58:50.260 --> 58:52.260]  Потому что характеристическая
[58:52.260 --> 58:54.260]  функция нормального вектора,
[58:54.260 --> 58:56.260]  давайте мы ее вспомним,
[58:56.260 --> 58:58.260]  это что такое? Когда mu равно нулю.
[58:58.260 --> 59:00.260]  Это экспонента
[59:00.260 --> 59:02.260]  от минус одна вторая, транспонированная
[59:02.260 --> 59:04.260]  rs. Вот.
[59:04.260 --> 59:06.260]  И
[59:08.260 --> 59:10.260]  произвольные моменты,
[59:12.260 --> 59:14.260]  как вы знаете из теории вероятности, они
[59:14.260 --> 59:16.260]  связаны с частными производными вот этой
[59:16.260 --> 59:18.260]  функции в нуле. Например, вот эта штука,
[59:18.260 --> 59:20.260]  она там выражается как что-то
[59:20.260 --> 59:22.260]  там, я не помню какой коэффициент,
[59:22.260 --> 59:24.260]  но это неважно, d в степени
[59:24.260 --> 59:26.260]  n на phi
[59:26.260 --> 59:28.260]  x в нуле
[59:28.260 --> 59:30.260]  по ds1, ds2
[59:30.260 --> 59:32.260]  и так далее, по dsn.
[59:32.260 --> 59:34.260]  Ну и за счет того, что здесь
[59:34.260 --> 59:36.260]  стоит квадратичная форма,
[59:36.260 --> 59:38.260]  то есть она как бы степень,
[59:38.260 --> 59:40.260]  у s здесь четная, то когда ты берешь
[59:40.260 --> 59:42.260]  нечетную, нечетного порядка
[59:42.260 --> 59:44.260]  производную, при s равно ноль,
[59:44.260 --> 59:46.260]  то ты получишь ноль.
[59:46.260 --> 59:48.260]  Вот. А когда у тебя
[59:48.260 --> 59:50.260]  s четная, то вот эти
[59:50.260 --> 59:52.260]  производные могут дать по
[59:52.260 --> 59:54.260]  четному числу s, могут дать в принципе
[59:54.260 --> 59:56.260]  не нули. Ну вот они и дают как раз вот такие
[59:56.260 --> 59:58.260]  вот произведения, которые
[59:58.260 --> 01:00:00.260]  зависят от r. А почему они только от r зависят?
[01:00:00.260 --> 01:00:02.260]  Потому что функция характеристическая только от r зависит.
[01:00:02.260 --> 01:00:04.260]  Видите?
[01:00:04.260 --> 01:00:06.260]  Функция вектора однозначно
[01:00:06.260 --> 01:00:08.260]  определяется r-кой. У
[01:00:08.260 --> 01:00:10.260]  гауссского векторов однозначно определяется r-кой.
[01:00:10.260 --> 01:00:12.260]  Значит и все производные определяются каким-то образом
[01:00:12.260 --> 01:00:14.260]  через эту r-ку. Вот и все.
[01:00:14.260 --> 01:00:16.260]  Вот и получается, что произвольный момент
[01:00:16.260 --> 01:00:18.260]  зависит от этого r.
[01:00:18.260 --> 01:00:20.260]  Вот. То есть вот
[01:00:20.260 --> 01:00:22.260]  это полезно понимать,
[01:00:22.260 --> 01:00:24.260]  откуда берется. Ну эта теория, она
[01:00:24.260 --> 01:00:26.260]  очень техническая, и доказывать я ее не буду.
[01:00:26.260 --> 01:00:28.260]  Вы можете ей пользоваться.
[01:00:30.260 --> 01:00:32.260]  Так, все. На этом
[01:00:32.260 --> 01:00:34.260]  я заканчиваю
[01:00:34.260 --> 01:00:36.260]  разговор про свойства нормальных
[01:00:36.260 --> 01:00:38.260]  векторов.
[01:00:38.260 --> 01:00:40.260]  У меня еще есть
[01:00:40.260 --> 01:00:42.260]  немножко времени.
[01:00:42.260 --> 01:00:44.260]  Давайте я введу
[01:00:44.260 --> 01:00:46.260]  новый для вас процесс
[01:00:46.260 --> 01:00:48.260]  очень важный,
[01:00:48.260 --> 01:00:50.260]  который мы будем тоже работать потом
[01:00:50.260 --> 01:00:52.260]  очень долгое время.
[01:00:52.260 --> 01:00:54.260]  Винеровский процесс.
[01:00:54.260 --> 01:00:56.260]  Это очень важный представитель
[01:00:56.260 --> 01:00:58.260]  гауссских процессов.
[01:00:58.260 --> 01:01:00.260]  То есть перейдем уже непосредственно к процессам
[01:01:00.260 --> 01:01:02.260]  от нормальных
[01:01:02.260 --> 01:01:04.260]  векторов.
[01:01:10.260 --> 01:01:12.260]  Значит, винеровский
[01:01:12.260 --> 01:01:14.260]  процесс
[01:01:16.260 --> 01:01:18.260]  он определяется
[01:01:18.260 --> 01:01:20.260]  следующим образом.
[01:01:22.260 --> 01:01:24.260]  Очень похожим образом, как
[01:01:24.260 --> 01:01:26.260]  определяется полосунницкий процесс.
[01:01:26.260 --> 01:01:28.260]  Определение
[01:01:30.260 --> 01:01:32.260]  винеровским
[01:01:34.260 --> 01:01:36.260]  процессом.
[01:01:36.260 --> 01:01:38.260]  Его еще иногда называют процессом
[01:01:38.260 --> 01:01:40.260]  Броуновского движения.
[01:01:40.260 --> 01:01:42.260]  Винеровским процессом
[01:01:42.260 --> 01:01:44.260]  W A T
[01:01:44.260 --> 01:01:46.260]  называется
[01:01:50.260 --> 01:01:52.260]  случайный процесс
[01:01:52.260 --> 01:01:54.260]  случайный процесс
[01:01:54.260 --> 01:01:56.260]  со свойствами.
[01:01:56.260 --> 01:01:58.260]  Тоже три свойства.
[01:01:58.260 --> 01:02:00.260]  Как было для
[01:02:00.260 --> 01:02:02.260]  полосунницкого процесса.
[01:02:02.260 --> 01:02:04.260]  В нуле он ноль, почти
[01:02:04.260 --> 01:02:06.260]  наверное.
[01:02:06.260 --> 01:02:08.260]  Второе W A T
[01:02:08.260 --> 01:02:10.260]  это процесс
[01:02:12.260 --> 01:02:14.260]  независимыми
[01:02:14.260 --> 01:02:16.260]  приращениями
[01:02:16.260 --> 01:02:18.260]  так же, как и полосунницкий процесс.
[01:02:18.260 --> 01:02:20.260]  А третье
[01:02:20.260 --> 01:02:22.260]  для любых T и S
[01:02:22.260 --> 01:02:24.260]  больше либо равных нулю
[01:02:24.260 --> 01:02:26.260]  не обязательно упорядоченных между собой,
[01:02:26.260 --> 01:02:28.260]  как для полосунницкого процесса.
[01:02:28.260 --> 01:02:30.260]  W A T
[01:02:30.260 --> 01:02:32.260]  имеет нормальное распределение
[01:02:32.260 --> 01:02:34.260]  с нулевым от ожидания
[01:02:34.260 --> 01:02:36.260]  и дисперсии равные модуль T-S.
[01:02:36.260 --> 01:02:38.260]  Иногда здесь пишут
[01:02:38.260 --> 01:02:40.260]  сигму в квадрате.
[01:02:40.260 --> 01:02:42.260]  Тогда это будет винеровский процесс
[01:02:42.260 --> 01:02:44.260]  с параметром сигмы в квадрат.
[01:02:44.260 --> 01:02:46.260]  Нам это нигде не пригодится, поэтому я его опускаю.
[01:02:46.260 --> 01:02:48.260]  Хотя можно было бы
[01:02:48.260 --> 01:02:50.260]  определить такой процесс.
[01:02:50.260 --> 01:02:52.260]  Вот, это определение
[01:02:52.260 --> 01:02:54.260]  винеровского процесса.
[01:02:54.260 --> 01:02:56.260]  Откуда это взялось?
[01:02:56.260 --> 01:02:58.260]  Значит,
[01:02:58.260 --> 01:03:00.260]  где-то в XIX веке
[01:03:00.260 --> 01:03:02.260]  был такой ботаник
[01:03:02.260 --> 01:03:04.260]  английский Brown.
[01:03:04.260 --> 01:03:06.260]  Значит, он
[01:03:06.260 --> 01:03:08.260]  обнаружил, что
[01:03:08.260 --> 01:03:10.260]  в жидкостей
[01:03:10.260 --> 01:03:12.260]  какие-то маленькие частички, они
[01:03:12.260 --> 01:03:14.260]  трясутся под микроскопом.
[01:03:14.260 --> 01:03:16.260]  И потом, в начале
[01:03:16.260 --> 01:03:18.260]  XX века, Эйнштейн это связал
[01:03:18.260 --> 01:03:20.260]  с тем, что
[01:03:20.260 --> 01:03:22.260]  вот эта маленькая частица,
[01:03:22.260 --> 01:03:24.260]  подвешенная в жидкости, она трясется,
[01:03:24.260 --> 01:03:26.260]  потому что о нее ударяются молекулы
[01:03:26.260 --> 01:03:28.260]  с разных сторон.
[01:03:28.260 --> 01:03:30.260]  И впоследствии,
[01:03:30.260 --> 01:03:32.260]  и координаты вот этой частицы,
[01:03:32.260 --> 01:03:34.260]  которая трясется в жидкости,
[01:03:34.260 --> 01:03:36.260]  это можно описывать
[01:03:36.260 --> 01:03:38.260]  как случайный процесс.
[01:03:38.260 --> 01:03:40.260]  И вот Винер был одним из тех,
[01:03:40.260 --> 01:03:42.260]  кто глубоко изучал свойства
[01:03:42.260 --> 01:03:44.260]  этого процесса.
[01:03:44.260 --> 01:03:46.260]  Математически это все описывал, изучал, исследовал.
[01:03:46.260 --> 01:03:48.260]  И какая тут мотивация
[01:03:48.260 --> 01:03:50.260]  стоит для того, чтобы записать
[01:03:50.260 --> 01:03:52.260]  все вот эти свойства.
[01:03:52.260 --> 01:03:54.260]  Вот представьте себе жидкость, правда,
[01:03:54.260 --> 01:03:56.260]  не трехмерную, а одномерную.
[01:03:56.260 --> 01:03:58.260]  И мы будем
[01:03:58.260 --> 01:04:00.260]  под X от T
[01:04:00.260 --> 01:04:02.260]  понимать координату
[01:04:02.260 --> 01:04:04.260]  частицы
[01:04:04.260 --> 01:04:06.260]  на этой
[01:04:06.260 --> 01:04:08.260]  координатной оси.
[01:04:08.260 --> 01:04:10.260]  В зависимости
[01:04:10.260 --> 01:04:12.260]  от времени T. Время идет, частицы
[01:04:12.260 --> 01:04:14.260]  как-то движутся по
[01:04:14.260 --> 01:04:16.260]  этой оси.
[01:04:16.260 --> 01:04:18.260]  Ну и мы будем
[01:04:18.260 --> 01:04:20.260]  считать, что
[01:04:20.260 --> 01:04:22.260]  положение
[01:04:22.260 --> 01:04:24.260]  этой координаты
[01:04:24.260 --> 01:04:26.260]  определяется
[01:04:26.260 --> 01:04:28.260]  очень большим
[01:04:28.260 --> 01:04:30.260]  множеством соударений
[01:04:30.260 --> 01:04:32.260]  этой частицы с какими-то
[01:04:32.260 --> 01:04:34.260]  другими мелкими частицами, которые с
[01:04:34.260 --> 01:04:36.260]  равной вероятностью ее толкают то влево, то вправо.
[01:04:38.260 --> 01:04:40.260]  И если мы будем считать, что в начальный момент
[01:04:40.260 --> 01:04:42.260]  времени частица находится в нуле,
[01:04:42.260 --> 01:04:44.260]  вот оно, видите,
[01:04:44.260 --> 01:04:46.260]  наше первое свойство,
[01:04:46.260 --> 01:04:48.260]  если мы будем считать, что
[01:04:48.260 --> 01:04:50.260]  то, насколько
[01:04:50.260 --> 01:04:52.260]  сместилась частица
[01:04:54.260 --> 01:04:56.260]  на этом интервале, не зависит от того,
[01:04:56.260 --> 01:04:58.260]  насколько она сместилась
[01:04:58.260 --> 01:05:00.260]  на других интервалах, потому что
[01:05:00.260 --> 01:05:02.260]  где бы она ни была, в любой момент времени
[01:05:02.260 --> 01:05:04.260]  на нее одинаково давят как слева, так и справа,
[01:05:04.260 --> 01:05:06.260]  и ситуация не меняется.
[01:05:06.260 --> 01:05:08.260]  Так что это
[01:05:08.260 --> 01:05:10.260]  разумное предположение, считать, что
[01:05:10.260 --> 01:05:12.260]  вот эти тоже независимы в совокупности.
[01:05:14.260 --> 01:05:16.260]  И если мы будем считать, что
[01:05:16.260 --> 01:05:18.260]  вот это приращение нормальное,
[01:05:18.260 --> 01:05:20.260]  а почему мы считаем, что это
[01:05:20.260 --> 01:05:22.260]  нормально? Потому что это приращение,
[01:05:22.260 --> 01:05:24.260]  то, насколько частица сдвинула за этот интервал
[01:05:24.260 --> 01:05:26.260]  времени, определяется большим числом
[01:05:26.260 --> 01:05:28.260]  взаимодействия с ней, суммой большого числа
[01:05:30.260 --> 01:05:32.260]  случайных величин.
[01:05:32.260 --> 01:05:34.260]  А по центральной и предельной теоремии
[01:05:34.260 --> 01:05:36.260]  хорошее приближение для нее
[01:05:36.260 --> 01:05:38.260]  это нормальное приближение.
[01:05:38.260 --> 01:05:40.260]  И если наконец мы будем считать, что
[01:05:40.260 --> 01:05:42.260]  это приращение зависит
[01:05:42.260 --> 01:05:44.260]  только от разности момента времени,
[01:05:44.260 --> 01:05:46.260]  если ты прибавишь сюда h
[01:05:46.260 --> 01:05:48.260]  и сюда прибавишь h, ничего не поменяется,
[01:05:48.260 --> 01:05:50.260]  что она на этом интервале?
[01:05:52.260 --> 01:05:54.260]  Случайная величина, насколько
[01:05:54.260 --> 01:05:56.260]  она сместилась, не зависит от момента времени.
[01:05:58.260 --> 01:06:00.260]  Главное, она зависит от величины
[01:06:00.260 --> 01:06:02.260]  этого интервала времени, но не от того,
[01:06:02.260 --> 01:06:04.260]  в какой момент времени ты рассматриваешь это приращение,
[01:06:04.260 --> 01:06:06.260]  то есть стационарность,
[01:06:06.260 --> 01:06:09.740]  называется, то тогда ты придешь просто вот к этому определению. То есть опять же
[01:06:09.740 --> 01:06:14.740]  очень естественные свойства. Начинаем из нуля и максимальная симметрия во
[01:06:14.740 --> 01:06:20.260]  всем. Значит, независимость от интервала времени и стационарность, однородность,
[01:06:20.260 --> 01:06:24.140]  все. То ты приходишь просто вот к этому определению. Потому что он тоже очень
[01:06:24.140 --> 01:06:27.580]  естественный, что для Пуассоновского процесса, что здесь. И, кстати, обратите
[01:06:27.580 --> 01:06:35.460]  внимание, что этот процесс, что Пуассоновский процесс, они получаются как
[01:06:35.460 --> 01:06:40.260]  результат взаимодействия чего-то большого в малом интервале времени. Когда мы раз
[01:06:40.260 --> 01:06:46.380]  говорили о Пуассоновском процессе, там, значит, было много редких событий, которые
[01:06:46.380 --> 01:06:51.300]  редко происходят, но их много, да. Здесь тоже мы рассмотрим частицу, на которую
[01:06:51.300 --> 01:06:55.380]  давят слева-справа другие какие-то частицы в большом количестве. И тоже мы
[01:06:55.380 --> 01:06:59.100]  пользуемся какой-то предельной теоремой.
[01:07:06.580 --> 01:07:16.940]  Является Гауссовским процессом. Доказательства. Берем произвольное n,
[01:07:16.940 --> 01:07:23.740]  произвольные t1. Давайте мы их упорядочим, хотя это будет не очень принципиально.
[01:07:23.780 --> 01:07:38.140]  Давайте мы их упорядочим и рассмотрим вот такой вектор v от t1, v от t2 и так далее v от tn.
[01:07:38.140 --> 01:07:46.300]  Вот. Если мы докажем, что он нормальный, этот вектор, то тогда это будет Гауссовский
[01:07:46.300 --> 01:07:50.760]  процесс. Потому что Гауссовский процесс это процессы, у которых любые
[01:07:50.760 --> 01:07:54.560]  конечномерные распределения являются нормальными. То есть, по определению
[01:07:54.560 --> 01:07:58.880]  конечномерных распределений, любые векторы с течением являются нормальными.
[01:07:58.880 --> 01:08:04.440]  Попробуем доказать, что это нормальный случайный вектор. Ну как мы будем это
[01:08:04.440 --> 01:08:08.760]  делать? Например, вот так. Нужно показать, что он является, например, линейной
[01:08:08.760 --> 01:08:14.560]  комбинацией какого-то вектора, про который мы точно знаем, что он нормальный. Вот. И в
[01:08:14.560 --> 01:08:20.920]  качестве такого вектора мы возьмем просто превращение v от t1 минус v от нуля, ну v от нуля
[01:08:20.920 --> 01:08:34.120]  равно нулю. Вот v от t2 минус v от t1 и так далее v от tn минус v tn минус 1. Вот. Ну и какую надо
[01:08:34.120 --> 01:08:39.040]  взять матрицу? Значит, единичка, нолик. А у нас даже что-то такое было уже, да? Когда мы
[01:08:39.040 --> 01:08:44.080]  доказывали теорему о явной конструкции Гауссовского процесса. Но это как бы вот
[01:08:44.080 --> 01:08:47.000]  эта конструкция, которая возникает, она естественна, потому что мы через
[01:08:47.000 --> 01:08:53.200]  превращение пытаемся найти распределение самих сечений. Здесь у нас будут единицы, единицы,
[01:08:53.200 --> 01:09:01.240]  нолик, псинолики, ну и так далее. В конце у нас будут одни единички вот тут стоять. Вот. У этого
[01:09:01.240 --> 01:09:09.320]  вектора что? У него все компоненты нормальные по третьему свойству, но они все независимые эти
[01:09:09.320 --> 01:09:15.280]  приращения по второму свойству. Значит, они все нормальные и все независимые. Значит, это нормальный
[01:09:15.280 --> 01:09:21.600]  вектор и он умножается на какую-то матрицу, неважно какую. Значит, это тоже вектор нормальный. Конец
[01:09:21.600 --> 01:09:36.200]  доказательства. Все. Классная теорема, да? Не, не, не. В том-то и дело, что неважно. Любая матрица,
[01:09:36.200 --> 01:09:44.080]  хоть прямоувольная, там, не обязательно даже квадратная, чтобы была. Все равно. Вот. Ну,
[01:09:44.080 --> 01:09:50.880]  а видите, мы тут взяли только упорядочные Т, но если они не упорядочные, ну переставьте сечение
[01:09:50.880 --> 01:09:55.880]  здесь местами. Это все равно какая-то линейная комбинация над этими компонентами, так что никуда
[01:09:55.880 --> 01:10:06.280]  ты тут не денешься. Вот. Ну, вот такая замечательная теорема. Ну, давайте я еще для порядка выпишу
[01:10:06.280 --> 01:10:11.000]  некоторые числовые характеристики для Винерского процесса. Значит, математическое ожидание
[01:10:11.000 --> 01:10:18.160]  Винерского процесса. Значит, w это одно сечение. w от t распредлена так же, как w от t минус w от 0,
[01:10:18.160 --> 01:10:22.960]  потому что w от 0 равно 0. А это нормальное распределение с параметрами 0 модуль t.
[01:10:22.960 --> 01:10:30.640]  Ну, раз там 0, значит, мат. ожидание равно 0. Дисперсия w от t равняется модулю t. Ну,
[01:10:30.640 --> 01:10:35.680]  так как у нас t больше либо равен 0, то модуль можно здесь не писать, поэтому это просто t. Потому
[01:10:35.680 --> 01:10:42.000]  что t больше либо равно 0 для процесса. Значит, абсолютно аналогично, как формула выводилась для
[01:10:42.000 --> 01:10:52.600]  плацсонского процесса, здесь тоже корреляционная функция ts. Это есть минимум t и s. У плацсона здесь
[01:10:52.600 --> 01:10:57.200]  был лямбда, а здесь нет. То, что формула похожа, это просто следствие того, что и то,
[01:10:57.200 --> 01:11:03.760]  и другой процесс. Это процессы с независимыми превращениями. Вот и все. Поэтому формула похожа
[01:11:03.760 --> 01:11:10.600]  и удивляться тут не стоит. Ну, что еще может пригодиться? Давайте мы знаете еще какую вычислим
[01:11:10.600 --> 01:11:22.800]  штуку. Вот такую. Мод. ожидание v в четвертой от t. Нам это пригодится потом. Мод. ожидание
[01:11:22.800 --> 01:11:30.520]  v в четвертой от t. Значит, смотрите, мы можем рассмотреть вектор вот такой v от t, v от t, v от t,
[01:11:30.520 --> 01:11:39.640]  v от t. В общем-то, он получается как умножение некой матрицы 1, 1, 1, 1 на v от t. Так как вектор
[01:11:39.640 --> 01:11:46.640]  с одной компонентой. Так что это тоже нормальный вектор. И мы для вычисления вот этой штуки можем
[01:11:46.640 --> 01:11:53.760]  применить теорему вика, чтобы от ожидания у него все равно нулю. А мы, кстати, вычисляли v в четвертой.
[01:11:53.760 --> 01:12:06.960]  Что мы там получали? 3r в квадрате t, t. Там у нас было r1,1 в квадрате. 3r1,1 в квадрате.
[01:12:06.960 --> 01:12:19.120]  Ну вот. То есть 3t в четвертой. Вот вычислил математическое ожидание от этой вещи.
[01:12:19.120 --> 01:12:30.200]  А может и t в квадрате? А, дисперсия равна t. А мы дисперсию возводим в квадрат. Да,
[01:12:30.200 --> 01:12:44.240]  спасибо. В квадрате все правильно. Вот. Там модуль t, там без квадрата, да. И вот тут тоже,
[01:12:44.240 --> 01:12:53.560]  видите, хоть это дисперсия, но здесь квадрата нет. Вот. А вот возводится в квадрат. Вот так вот.
[01:12:53.560 --> 01:13:03.600]  Но вот это наблюдение, оно нам потом пригодится. А где именно? Ну, в принципе,
[01:13:03.600 --> 01:13:15.400]  могу уже сразу даже, наверное, сказать. Чем со временем? Четыре минуты. Ну, смотрите. Да,
[01:13:15.400 --> 01:13:22.160]  давайте прям сейчас сразу скажу. Это тривиально. Вот это вот я не просто так-то писал. Вот для нас
[01:13:22.160 --> 01:13:28.120]  будет важно то, что Винеровского процесса, у него траектории вообще очень сложно устроены. Это
[01:13:28.120 --> 01:13:39.320]  не как вот эти вот кусочно постоянные. Траектория выглядит вот так. Вот такие вот. И когда ты
[01:13:39.320 --> 01:13:46.880]  увеличишь какую-нибудь ее часть, то ты там увидишь снова что-то такое вот. И как ты не увеличивай,
[01:13:46.880 --> 01:13:54.920]  там все равно будет какая-то вот такая гадость. Ну, так это на бесконечности как бы. Нет,
[01:13:54.920 --> 01:14:03.560]  а что такое на любом? Это ждание. Если я повторю реализацию, то она пойдет вот так вот. И когда ты
[01:14:03.560 --> 01:14:08.840]  много реализаций рассмотришь, половина пойдет вверх, половина пойдет вниз, грубо говоря, да. И
[01:14:08.840 --> 01:14:16.080]  оно устаканится. А то, что оно на бесконечности куда-то уходит от нуля, так это естественно.
[01:14:16.080 --> 01:14:21.320]  Дисперсия растет. Хоть мы от ожидания равно нулю в любой момент времени, но дисперсия растет.
[01:14:21.320 --> 01:14:27.080]  Поэтому это естественно ожидать, что она куда-то уйдет. Так вот, вот эти траектории у процесса,
[01:14:27.080 --> 01:14:34.840]  они, можно доказать, что они всюду непрерывны. Почти все траектории всюду непрерывны, но нигде
[01:14:34.840 --> 01:14:42.400]  не дифференцируемо. Вот. То есть это такая вот гадость. И в связи с этим такие процессы довольно
[01:14:42.400 --> 01:14:46.840]  тяжело исследовать. А вот это нам нужно будет, ну ладно, это я в следующий раз, чтобы аккуратно это
[01:14:46.840 --> 01:14:52.920]  все выписать, понадобится доказать, что у Винеровского процесса существует непрерывная модификация.
[01:14:52.920 --> 01:14:57.360]  Помните, у нас была теорема Колмогорова на первой лекции. Есть математическое ожидание, там,
[01:14:57.360 --> 01:15:03.920]  модуля некой разности, что-то там не превосходит. Вот, можно воспользоваться вот этим вот наблюдением
[01:15:03.920 --> 01:15:09.760]  и доказать, что у Винеровского процесса существует непрерывная модификация. А это означает, что мы
[01:15:09.760 --> 01:15:14.320]  можем работать, когда мы работаем с этим процессом, мы можем считать, что все его траектории хоть и
[01:15:14.320 --> 01:15:23.400]  выглядят сложно, но все они являются непрерывными функциями. Вот. То есть, что у нас задано такое
[01:15:23.400 --> 01:15:27.920]  вероятностное пространство, и на нем задан Винеровский процесс, что все его траектории непрерывные.
[01:15:27.920 --> 01:15:34.920]  Вот. То есть, это прямое следствие той теоремы Колмогорова. Это будет очень удобно так считать,
[01:15:34.920 --> 01:15:41.600]  что все его траектории непрерывные. То есть, существуют такие как бы, можно так определить
[01:15:41.600 --> 01:15:45.840]  Винеровский процесс, что у него будут выполнять три свойства, но будут траектории разрывными
[01:15:45.840 --> 01:15:53.880]  какие-то. Их будет множество мир и нуль, но как бы они могут существовать. Так вот, чисто формально
[01:15:53.880 --> 01:15:59.760]  могут существовать. Но благодаря вот этой теореме Колмогорова мы можем задать вероятностное
[01:15:59.760 --> 01:16:06.960]  пространство другое на множестве функций, только непрерывных. И на нем определить уже вот этот
[01:16:06.960 --> 01:16:13.840]  процесс. И там уже не просто почти все, буквально все траектории будут непрерывными. И дальше,
[01:16:13.840 --> 01:16:21.480]  при решении задач, мы будем всегда учитывать этот факт, что все его траектории непрерывные. Вот.
[01:16:21.480 --> 01:16:26.640]  Иногда, кстати говоря, это добавляют четвертым пунктом туда. Траектории непрерывные. Но это некое
[01:16:26.640 --> 01:16:31.960]  следствие вот этих уже трех пунктов. То есть, это не получается независимость как бы этой аксиоматики.
[01:16:31.960 --> 01:16:36.600]  В общем, я не добавляю, а просто говорю о том, что можно определить Винеровский процесс и так и сяк.
[01:16:36.600 --> 01:16:42.240]  Для удобства, для практики удобно считать, что все его траектории непрерывные. Ну все.

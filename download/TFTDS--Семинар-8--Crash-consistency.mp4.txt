[00:00.000 --> 00:09.600]  Смотрите, сегодня семинар, которого, мне кажется, нам не хватало для полноты картины.
[00:09.600 --> 00:16.640]  Мы в этом курсе говорим про распределенные отказоустойчивые системы. И отказоустойчивость,
[00:16.640 --> 00:22.800]  с отказоустойчивостью мы справляемся по-разному, мы добиваемся ее по-разному. Смотрите,
[00:22.800 --> 00:28.800]  с одной стороны, отказоустойчивость про то, что компьютер может умереть. Какая-то машина может
[00:28.800 --> 00:35.920]  пропасть из нашего кластера, и система должна продолжить работу. Отказоустойчивость по модулю
[00:35.920 --> 00:41.920]  таких проблем мы добивались с помощью алгоритмов консенсуса и хворумов. Хворумы позволяли нам
[00:41.920 --> 00:49.920]  избавить пользователя от знания того, что машина сейчас умерла, от необходимости вообще ее
[00:49.920 --> 00:54.720]  наличия в кластере в данный момент. Мы можем записать что-то на хворум, какие-то машины могут
[00:54.800 --> 01:01.880]  быть недоступны, отрезаны партишеном или просто быть выключенными. То есть с крешем узла мы
[01:01.880 --> 01:09.520]  справляемся. В будущем мы должны будем научиться справляться не только с крешами узлов, но еще и с
[01:09.520 --> 01:14.840]  тем, что узел конкретный может лгать или даже некоторая группа узлов может лгать согласованно
[01:14.840 --> 01:24.000]  другим. Это разговор про византийские отказы. Но это пока мы еще не проходили, пока мы про это ничего
[01:24.000 --> 01:30.600]  не знаем и я думаю уже через две недели начнем этому учиться. Но у нас есть еще один вот
[01:30.600 --> 01:38.200]  проблем, который связан с отказоустойчивостью. Это рестарты узлов. Рестарты узлов, а проблема
[01:38.200 --> 01:44.600]  называется crash consistency. Как гарантировать, что наше состояние, которое мы храним видимо
[01:44.600 --> 01:53.040]  персистентно на жестком диске, переживет этот рестарт? Мы с crash consistency должны обеспечивать,
[01:53.040 --> 02:00.360]  мы с ней сталкивались в двух задачах. Во-первых, мы использовали levelDB, интерфейс базы данных,
[02:00.360 --> 02:08.000]  более-менее levelDB и levelDB должна гарантировать, что если она подтвердила нам пут, то даже если
[02:08.000 --> 02:15.680]  машина перезагрузится, то данные будут после этого путу доступны. И levelDB это обеспечивало
[02:15.680 --> 02:22.160]  хранение данной на жестком диске в виде логов и sstable. То есть в памяти хранился memtable,
[02:22.160 --> 02:28.160]  на memtable не страшно потерять, потому что по writeaheadlog его всегда можно восстановить. Любая
[02:28.160 --> 02:33.320]  вставка проходит сначала через writeaheadlog, потом уже через memtable, потом подает memtable. Ну,
[02:33.320 --> 02:39.400]  конечно, если мы в levelDB используем довольно сильные гарантии, там их можно ослабить,
[02:39.400 --> 02:46.880]  но для наших целей важно, чтобы гарантии были сильными. Другой пример, где нам важна crash
[02:46.880 --> 02:56.160]  consistency, это репликация автомата, это rsm. Вот у нас rsm может жить в памяти, но мы его храним в
[02:56.160 --> 03:04.880]  виде лога команд на жестком диске. И если мы говорим про multiprocess пусть, то в текущей
[03:04.880 --> 03:10.200]  домашней, вы в каждой ячейке этого лога храните состояние acceptor. Вот очень важно состояние
[03:10.200 --> 03:15.520]  acceptor фиксировать на жестком диске. Если acceptor отвечает пропозеру, что он проголосовал за
[03:15.520 --> 03:20.320]  предложение, проголосовал за команду для данного слота лога, то мы не имеем права ее потерять.
[03:20.320 --> 03:33.440]  Вот сегодня вроде бы и лог на диске, и writeaheadlog, и sstable в lsm решают проблемы crash consistency. Если
[03:33.440 --> 03:38.960]  узел перезагрузится, то мы данные не потеряем. Но очень наивно считать, что мы на самом деле
[03:38.960 --> 03:45.000]  задачу решили, потому что внутри этой задачи спрятано очень много сложностей,
[03:45.000 --> 03:49.880]  очень много хитростей, которые мы пока игнорируем. Вот наша задача сегодня разобраться, а как
[03:49.880 --> 03:57.640]  именно обеспечить crash consistency, как именно работать с файловой системой. Чуть конкретнее мы хотим
[03:57.640 --> 04:06.000]  выполнение двух свойств, двух букв из acid. Мы хотим свойства атомарности и хотим свойства надежности,
[04:06.000 --> 04:12.920]  durability. Эти два свойства про разные. Они про crash consistency, но про разные. Durability — оно про то,
[04:12.920 --> 04:19.960]  что если вы подтвердили запись данных, если вы подтвердили put в leveldb или если вы подтверждаете
[04:19.960 --> 04:30.280]  промессом или сообщением accept пропозеру, что его команда успешно положилась в лог, то вы не имеете
[04:30.280 --> 04:36.280]  права после рестарта машины об этом обещании забыть. Если вы подтвердили запись, то вы должны
[04:36.280 --> 04:43.520]  гарантировать, что после рестарта вы сможете эту запись прочесть. Это durability. Вторая гарантия —
[04:43.520 --> 04:50.440]  атомарность. Она про то, что происходит, если выключение питания вас застало в середине какой-то
[04:50.440 --> 04:56.800]  операции, то есть в середине записи в лог, в середине пута. В этом случае мы должны гарантировать,
[04:56.800 --> 05:05.120]  что наша запись либо целиком откатится, либо будет полностью применена после рестарта.
[05:05.120 --> 05:15.680]  Мы не можем оставить лог или наш хранилищ в каком-то несогласованном состоянии. Вот две такие
[05:15.680 --> 05:19.120]  задачи нас сегодня будут волновать. Нас сегодня будут волновать детали, как именно мы этого
[05:19.120 --> 05:25.840]  достигаем. Вот давайте вспомним про... Давайте сначала начнем с RSM, которая нам ближе, которая в
[05:25.840 --> 05:32.920]  домашней работе есть. В RSM у вас есть лог, в котором фиксируются все команды и голоса всех
[05:32.920 --> 05:42.640]  acceptors. Лог нужен для того, чтобы в случае рестарта системы иметь возможность восстановиться и
[05:42.640 --> 05:46.680]  продолжить работу с того места, где вы закончили, не потерять никакие обещания, не потерять
[05:46.680 --> 05:54.760]  закоммиченные команды. Но с логом есть некоторая проблема, а именно вы не можете жить этим логом
[05:54.760 --> 06:02.680]  вечно. Если вы читаете статью про multiplexus или про raft, то там предполагается, что лог растет
[06:02.680 --> 06:07.920]  бесконечно. Но это предположение совсем не практично, потому что рано или поздно, во-первых, лог
[06:07.920 --> 06:14.080]  переполнит диск, а во-вторых, длинный лог, даже если он пока помещается в диск, просто замедляет нам
[06:14.080 --> 06:21.520]  восстановление реплики после рестарта узла. Мы должны, если бы, допустим, писали в лог неделю,
[06:21.520 --> 06:26.600]  а потом перезагрузились, мы должны все недельные изменения, все недельные команды накатить на
[06:26.600 --> 06:33.440]  состояние автомата. Вряд ли мы хотим делать именно так. Поэтому, когда мы говорим про персистентность,
[06:33.440 --> 06:41.360]  про дюрабельность нашего RSM, мы должны думать о состоянии на диске не только как о логе,
[06:41.360 --> 06:47.000]  мы должны лог периодически обрезать, его компактифицировать и закоммиченный и
[06:47.000 --> 06:54.360]  примененный к автомату префикс лога заменять на снимок состояния автомата. Какой бы длинный
[06:54.360 --> 06:58.280]  лог ни был, у него есть более компактное представление. Это просто мгновенное состояние
[06:58.280 --> 07:04.840]  автомата, которое получается после применения к автомату в пустом состоянии всего префикса.
[07:04.840 --> 07:12.840]  Если мы, допустим, храним какое-то дерево, например, дерево файловой системы, потому что мы метастор
[07:12.840 --> 07:18.200]  распределенной файловой системы, то вместо того, чтобы хранить очень длинную историю пралок
[07:18.200 --> 07:22.480]  этого дерева, мы можем просто сделать снимок состояния всего дерева, записать его на диск и
[07:22.480 --> 07:29.640]  от лога команд отрезать некоторые закоммичные префиксы, которые соответствуют вот этому данному
[07:29.640 --> 07:37.280]  снимку. Таким образом, у нас персистентное состояние RSM образовано суффиксом лога команд и снимком
[07:37.280 --> 07:45.960]  состояния автомата для обрезанного префикса. Вот давайте подумаем, какие трудности у нас возникают,
[07:45.960 --> 07:55.160]  когда мы начинаем хранить префикс лога в виде снимка состояния. Во-первых, нам нужно каким-то
[07:55.160 --> 08:06.480]  образом от лога откусывать префикс. Звучит довольно просто, но, с другой стороны, не совсем понятно,
[08:06.480 --> 08:13.000]  как именно мы собираемся откусывать префикс от файла на диске, потому что API файловой системы
[08:13.000 --> 08:19.240]  нам такой возможности не предоставляет. Это, кстати, довольно странно, потому что в конце концов в
[08:19.240 --> 08:26.560]  файловой системе файл хранится в iNode где-то в виде списка блоков, так что вполне можно было
[08:26.560 --> 08:33.920]  бы лишние блоки из префикса отсечь. Но у нас есть только файл, у нас есть API файловой системы и
[08:33.920 --> 08:38.960]  ничего мы с этим сделать уже не можем. Мы должны моделировать truncate каким-то образом
[08:38.960 --> 08:46.760]  самостоятельно. Вот как мы с этим справляемся, понимаете ли вы? Такая операция нужна любому
[08:46.760 --> 08:56.720]  RSM-у, будь это multiprocess, будь это raft, все равно префиксы откусывать нужно. Вот понимаем ли мы,
[08:56.720 --> 09:05.320]  как это делать? Но вот для того, чтобы откусывать префикс, нужно хранить лог, видимо, не в одном
[09:05.320 --> 09:12.400]  файле. Нужно использовать то, что называется сегментированный лог. Сегментированный лог — это лог,
[09:12.400 --> 09:18.400]  который состоит из нескольких фрагментов, из нескольких файлов. Вот, скажем, в домашней
[09:18.400 --> 09:29.840]  работе pro multiprocess вам дана реализация лога. Сейчас найду ее. Это интерфейс лога, у него есть
[09:29.840 --> 09:39.080]  операция truncate. Мы это уже видим. И есть разные реализации. В частности, есть сегментированная
[09:39.080 --> 09:54.720]  реализация, в которой лог представлен в виде набора файлов. Каждый сегмент в этой реализации
[09:54.720 --> 10:01.440]  отвечает за некоторый диапазон индексов. И когда этому логу приходит операция truncate,
[10:01.440 --> 10:07.520]  то, видимо, он должен просто пойти и лишние файлы удалить. То есть файлы, которые целиком
[10:07.520 --> 10:28.360]  накрываются закоммиченным и примененным к автомату префиксом. Нет, я не туда смотрю, простите.
[10:28.360 --> 10:45.560]  Это код сегмента, а мне нужен код самого лога. Вот, мы идем по сегментам, и если последний индекс,
[10:45.560 --> 10:52.760]  за который отвечает сегмент, не больше, чем индекс, до которого мы хотим все удалить,
[10:52.760 --> 11:02.240]  то мы удаляем сам файл с файловой системы. Ну вот, такая нехитрая идея, но почему-то
[11:02.240 --> 11:07.400]  она не является совсем уж тривиальной, и мы чуть позже увидим, в чем там сложность.
[11:07.400 --> 11:15.160]  Ну, это, как бы, одна половина задачи. Отрезать лог, наверное, понятно, что это можно сделать. Вот,
[11:15.160 --> 11:19.920]  мы примерно так эту задачу решаем. А что делать со снимком состояния автомата?
[11:19.920 --> 11:26.960]  В домашней работе мы предполагаем, что все очень тривиально. У нас есть интерфейс
[11:26.960 --> 11:36.160]  State Machine, и в нем есть метод, который пишет снимок в виде какого-то байта.
[11:36.160 --> 11:42.960]  Вот кажется, что так делать не практично, если мы говорим про промышленную реализацию,
[11:42.960 --> 11:49.160]  потому что ваш автомат, например, дерево-файловая система, может быть огромным. Этому какие-то
[11:49.200 --> 11:56.680]  десятки-сотни гигабайт в памяти. И мы, конечно, не можем себе позволить на лидере RSM остановиться
[11:56.680 --> 12:09.440]  на некоторое время и ждать, пока все это дерево запишется из памяти на диск. Как тут можно поступить?
[12:09.440 --> 12:17.960]  Ну, наверное, можно было бы скопировать его в памяти сначала, а потом уже в отдельном потоке
[12:17.960 --> 12:23.840]  писать на диск, чтобы не мешать конкурирующим апдейтам этого дерева, чтобы не мешать автомату
[12:23.840 --> 12:32.440]  работать, реплике RSM работать. Конечно, нам уже нужен снимок, который бы согласованный,
[12:32.440 --> 12:36.360]  если мы берем, вот фиксируем состояние автомата для некоторого префикса command,
[12:36.360 --> 12:41.560]  префикса log command, и вот ровно это состояние пишем на диск. Мы не можем позволить себе,
[12:41.560 --> 12:46.720]  чтобы какие-то части дерева соответствовали разным языкомичным префиксам лога.
[12:46.720 --> 12:53.680]  Так вот, останавливать автомат на время записи на диск мы не хотим, разумеется,
[12:53.680 --> 12:59.240]  и мы даже не можем позволить себе останавливать автомат для того, чтобы скопировать все его
[12:59.240 --> 13:04.800]  состояние в памяти, потому что это в два раза больше памяти, а памяти у нас может столько не
[13:04.800 --> 13:11.520]  быть, потому что и так RSM, который хранит дерево файловой системы, это уже узкое место в дизайне
[13:11.520 --> 13:19.040]  файловой системы. Как можно было бы сделать эффективнее, как можно было бы сделать более ловко,
[13:19.040 --> 13:27.040]  как можно сделать мгновенный снимок большого состояния сложной структуры данных и при этом
[13:27.040 --> 13:41.680]  не останавливать работу реплики RSM. Вот для решения такой задачи есть такой, есть академический
[13:41.680 --> 13:49.920]  способ, он называется персистентная структура данных. Допустим, вы хотите поддерживать дерево,
[13:49.920 --> 13:57.160]  его редактировать, добавлять к него некоторые узлы, добавлять к него новые узлы и при этом вы
[13:57.160 --> 14:04.600]  хотите получать мгновенное состояние всего дерева. Для этого используется механизм Copy on Write. Вот
[14:04.600 --> 14:11.160]  допустим, у вас есть какое-то дерево и вы хотите к нему вот к узлу F прицепить узел E. Вы хотите
[14:11.160 --> 14:16.440]  к какой-то директории прицепить новый лист, новый файл. Вот вы его, конечно, создаете в памяти,
[14:16.440 --> 14:26.080]  а дальше из директории ставите на него ссылку. Рождается новая директория. Это директория,
[14:26.080 --> 14:34.480]  ну в смысле новая версия старой директории. Но на эту новую версию старой директории должна
[14:34.480 --> 14:40.480]  ссылаться новая версия родительской директории, ну и вот так вплоть до корня. То есть мы меняем
[14:40.480 --> 14:45.640]  какую-то часть дерева в нашей операции, в данном случае мы цепляем лист и вместе с этим листом
[14:45.640 --> 14:51.320]  меняется путь к этому листу в дереве. И мы это честно копируем. Мы создаем новые версии этих листов,
[14:51.320 --> 14:58.000]  штрихованные на этой рисунке. Но при этом все остальные узлы дерева остаются неизменными. Они
[14:58.000 --> 15:04.800]  между старой версией дерева и новой версией дерева общие. Их можно переиспользовать. Поэтому
[15:04.800 --> 15:11.720]  новый путь мы выстраиваем в памяти из новых узлов, путь мы устраиваем из новых узлов и ставим
[15:11.720 --> 15:21.240]  ссылки на старые вершины, которые не поменялись. Вот под дерево B оно используется и в старой версии,
[15:21.240 --> 15:27.760]  и в новой версии, и можно его разделить между двумя этими версиями. Но это такой академический
[15:27.760 --> 15:35.040]  способ. То есть таким образом можно снэпшотить состояние, скажем, дерева. И для нас снэпшотить
[15:35.040 --> 15:43.520]  дерево довольно полезно. Но все же нам требуется таким образом написать весь наш код в виде вот
[15:43.520 --> 15:48.280]  таких вот персистентных структур данных. Тогда бы мы могли делать снэпшоты, не блокируя работу
[15:48.280 --> 15:55.120]  автоматом, не блокируя работу реплики RSA. Но все же мы пишем какой-то промышленный сложный код,
[15:55.120 --> 16:01.680]  и в нем может быть, могут быть не только деревья, могут быть, не знаю, все что вы в коде можете
[16:01.680 --> 16:08.320]  написать. Очень сложное и разнообразное состояние. И делать его целиком из персистентных структур
[16:08.320 --> 16:16.280]  данных, это довольно сложно, довольно громоздко. Но оказывается, что можно добиться примерно
[16:16.280 --> 16:23.120]  такого же эффекта, такого же механизма, никаким образом не меняя весь наш остальной код, весь
[16:23.120 --> 16:29.760]  код и все состояние, которое у нас уже есть. Вот идея за персистентностью на этом рисунке,
[16:29.760 --> 16:36.200]  это идея Copy-on-Write. Мы копируем то, что мы собираемся перезаписать. Но механизм Copy-on-Write
[16:36.200 --> 16:43.440]  за нас уже реализован в самой операционной системе, в которой исполняется наш код. Этот
[16:43.440 --> 16:52.040]  механизм Copy-on-Write реализован в механизме виртуальной памяти. У вас есть такой волшебный
[16:52.040 --> 16:58.120]  системный вызов, системный вызов Fork. Системный вызов Fork порождает дочерний процесс,
[16:58.120 --> 17:07.600]  который получает копию памяти родительского процесса. Но копия, она ленивая. То есть не то,
[17:07.600 --> 17:11.800]  чтобы вы копируете все состояние. Мы, собственно, от этого и хотим уйти. Мы не хотим копировать все
[17:11.800 --> 17:17.760]  состояние в памяти, потому что это большая пауза. Вместо этого механизм Copy-on-Write в виртуальной
[17:17.760 --> 17:24.520]  памяти, когда вы Fork-ите процесс, копируют только таблицу страниц. То есть теперь родительский
[17:24.520 --> 17:31.480]  процесс и дочерний ссылаются на одни и те же страницы памяти. Но при этом в обеих процессах
[17:31.480 --> 17:39.880]  эти страницы помечены как ридонли. То есть программа не может их перезаписывать. Вернее,
[17:39.880 --> 17:44.800]  может, конечно, но это ей необходимо делать, потому что автомат хочет продолжить работу.
[17:44.800 --> 17:56.080]  Всем я говорю автомат. Реплика RSM. Но когда реплика RSM будет в этот автомат что-то писать,
[17:56.080 --> 18:03.600]  то поскольку эта запись в ридонли страницу будет случаться, будет происходить прерывание,
[18:03.600 --> 18:08.640]  управление будет переходить в операционной системе, и операционная система уже по мере
[18:08.640 --> 18:14.960]  необходимости будет копировать старые ридонли страницы в новые страницы, в которые уже можно
[18:14.960 --> 18:22.240]  писать. Собственно, механизм Copy-on-Write. То есть мы копируем при Fork-е только таблицу
[18:22.240 --> 18:28.880]  страницы, дальше уже по мере необходимости копируем сами страницы памяти. Как это все относится
[18:28.880 --> 18:37.920]  к нашей задаче, а именно к задаче реализации, эффективной реализации метода, куда он от нас
[18:37.920 --> 18:49.400]  уехал. Метода Get Snapshot автомата. Make Snapshot. Теперь, если мы хотим сделать снапшот, мы Fork-ем
[18:49.400 --> 19:00.280]  процесс, после чего в дочернем процессе мы спокойно обходим все состояние автомата и пишем его
[19:00.280 --> 19:07.240]  на диск. Почему мы там можем так сделать? Потому что вот там никакой конкуренции уже не будет. Там
[19:07.240 --> 19:14.680]  не будет конкурирующих апдейтов, потому что Fork обладает очень странным свойством, а именно Fork
[19:14.680 --> 19:22.880]  копирует память, но теряет все потоки, которые были в родителе. Если вы Fork-ите процесс, то в
[19:22.880 --> 19:28.920]  дочернем процессе, но, конечно, у вас в вашем родительском процессе, который являлся репликой
[19:28.920 --> 19:35.960]  RSM, было очень много потоков. Ну ладно, какое это количество потоков? То в дочернем процессе у вас
[19:35.960 --> 19:42.680]  останется только один поток. Тот поток, который непосредственно вызывал Fork. Поэтому он может в
[19:42.680 --> 19:49.680]  одиночестве спокойно обойти текущее состояние структуры данных и сдамить ее на диск. Это,
[19:49.680 --> 19:56.040]  конечно же, не бесплатная операция, потому что и у ребенка непонятно в какой степени, тут уже как
[19:56.040 --> 20:02.760]  код напишете, но в родителе реплика RSM продолжает работать. И это означает, что она делает много
[20:02.760 --> 20:08.240]  записей в память. И это означает, что там случается много першфалтов и много копирований
[20:08.240 --> 20:14.640]  страниц. И это означает, что все-таки родитель у нас тормозит немножко. Но, по крайней мере,
[20:14.640 --> 20:23.200]  в нем нет гигантской паузы. Да, он работает хуже, он работает медленнее, а пользователи это замечают.
[20:23.200 --> 20:31.520]  Но большой паузы у нас все же нет. Ну и нам не потребовалось никакой сложной работы для того,
[20:31.520 --> 20:36.960]  чтобы научиться снапшотить более-менее произвольное состояние в памяти, как бы оно не было организован.
[20:36.960 --> 20:50.320]  Вот так делает, кажется, любая эффективная реализация RSM. Но, конечно, это еще не все. Это
[20:50.320 --> 20:56.080]  не все, что нужно сделать для снапшотов, потому что у вас есть некоторая очень нелепая,
[20:56.080 --> 21:04.320]  но, тем не менее, трудность. Вот смотрите, вы делаете форк для того, чтобы в потомке
[21:04.320 --> 21:12.720]  обойти структуру данных в памяти и записать ее на диск. Вы так делаете, потому что все остальные
[21:12.720 --> 21:20.760]  потоки не выжили. Но эти потоки, они, конечно, потерялись, но, с другой стороны, остались
[21:20.760 --> 21:28.960]  эффекты, их воздействие на память сохранилось, потому что память вы скопировали. А вот теперь
[21:28.960 --> 21:38.160]  представим, что вы какой-то поток, вы родители делали форк, а другой поток в вашем же процессе
[21:38.160 --> 21:46.360]  сейчас алоцировал память. Разумеется, такое могло происходить. Так вот, этот поток пришел в
[21:46.360 --> 21:54.280]  алокатор. Алокатор, разумеется, у вас, скорее всего, алокатор у вас общий, у вашего потока и у того
[21:54.280 --> 21:59.320]  потока, который занимается какой-то другой работой сейчас. Алокатор — это разделяемый сервис. Да,
[21:59.320 --> 22:05.000]  алокаторы современные, хорошие алокаторы шардированы. То есть, как правило, между потоками,
[22:05.000 --> 22:09.880]  которые алоцируют память, синхронизация не требуется, по крайней мере, на быстром пути уж точно.
[22:09.880 --> 22:16.760]  Но все же у алокатора есть некоторое общее состояние, какие-то общие арены, общие чанки памяти,
[22:16.760 --> 22:22.720]  которые он запрашивает у операционной системы. И если долго спускаться вглубь алокатора, то
[22:22.720 --> 22:28.880]  рано или поздно вы встретите, спустя все кэши для быстрого доступа, все это локальные кэши,
[22:28.880 --> 22:37.560]  рано или поздно вы встретите какие-то мютоксы. А теперь возникает следующая проблема. Вы форкаетесь,
[22:37.560 --> 22:45.000]  а другой поток в это время находится внутри алокатора и взял там блокировку. И в результате,
[22:45.000 --> 22:50.880]  после форка в дочернем процессе у вас поток, который был в критической секции в этой блокировке,
[22:50.880 --> 22:56.400]  в алокаторе исчезнет, а вот блокировка сама сохранится, потому что в конце концов это
[22:56.400 --> 23:05.920]  какой-то битик в памяти. В итоге алокатор сейчас залочен, вы к нему обращаться не можете в
[23:05.920 --> 23:13.600]  дочернем процессе, а это значит, что вы вряд ли сможете сделать что-то полезное. Вот такая неприятность.
[23:13.600 --> 23:21.760]  И эта проблема не то чтобы надуманная, это проблема, которая касается более-менее любого
[23:21.760 --> 23:29.320]  алокатора. Вот потоки и форки — это вещи не слишком ортогональные, это две совершенно разные фичи,
[23:29.320 --> 23:34.800]  которые плохо сочетаются друг с другом. Поэтому алокатор, хороший алокатор, ну скажем, jmalloc,
[23:34.800 --> 23:47.600]  такой стандартный, хороший алокатор для C++. Вот там эту проблему требуется решать,
[23:47.600 --> 23:57.920]  она решена. И она решается, ну я бы сказал, что некоторым костырем у вас есть такой библиотечный
[23:57.920 --> 24:02.560]  вызов, который называется Petroid at fork. Ну 3D — это библиотека, поэтому вызов библиотечный,
[24:02.560 --> 24:09.920]  это не системный вызов. Он позволяет вам зарегистрировать для форка три обработчика,
[24:09.920 --> 24:18.400]  три кулбека. Первый будет вызван перед форком, а два других будут вызваны после форка, один
[24:18.400 --> 24:26.720]  соответственно в родителе, другой в ребенке. Как можно этот пистолет, этот форк использовать?
[24:26.720 --> 24:34.960]  Ну посмотрим на jmalloc. В нем этот вызов должен быть использован, потому что jmalloc может быть
[24:34.960 --> 24:54.240]  использован в программе с форком. Ну вот, мы регистрируем там какие-то обработчики, и что эти
[24:54.240 --> 25:03.640]  обработчики собираются делать? Смотрите, с одной стороны это костырь, вот глобально,
[25:03.640 --> 25:10.080]  все это костырь, а с другой стороны довольно остроумно. Если мы делаем форк, то мы сначала
[25:10.080 --> 25:17.680]  берем все мютоксы в нашем локаторе. То есть, грубо говоря, мы перед форком дожидаемся,
[25:17.680 --> 25:25.920]  что все другие потоки, которые с этими мютоксами работают, отпустят эти мютоксы, и мы эти мютоксы
[25:25.920 --> 25:33.280]  залочим. После этого мы сделаем форк, и после форка в дочернем процессе мы будем уверены,
[25:33.280 --> 25:39.600]  что сейчас все мютоксы локаторы взяты и взяты нами. И после этого мы можем, в смысле контролируемо
[25:39.600 --> 25:56.080]  захваченные, и после этого можно в дочернем процессе их обойти и отпустить. Вот мы отпускаем все
[25:56.080 --> 26:04.880]  эти мютоксы. Вот такая, не знаю, то ли остроумная, то ли какая-то... Но мне кажется, что в этом есть
[26:04.880 --> 26:10.400]  что-то изящное. Но, тем не менее, если вы пишете RSM и если вы хотите делать снепшоты эффективно,
[26:10.400 --> 26:16.680]  то вам нужен форк, а если вам нужен форк, то вам нужно каким-то образом гарантировать,
[26:16.680 --> 26:24.400]  что ваша синхронизация и потоки с этим форком будут совместимы, чтобы вы не оставили автомат
[26:24.400 --> 26:34.800]  заблокированным навсегда. Автомат и локатор, разумеется. Ну вот, таким образом можно делать
[26:34.800 --> 26:40.720]  снимать состояние и лог обрезать с помощью вот таких транкейтов, с помощью транкейта сегментов.
[26:40.720 --> 26:49.320]  Ну и, конечно, все это будет задевать реализацию рафта или реализацию паксиса, который мы пишем.
[26:49.320 --> 26:52.080]  Вот давайте...
[26:57.920 --> 26:59.080]  Слишком много ссылок.
[27:04.800 --> 27:13.200]  Если вы возьмете писать снепшоты в реализации мультипаксиса, то вас ждет не то чтобы проблема,
[27:13.200 --> 27:22.040]  но некоторая дополнительная трудность на уровне самого консенсуса, потому что в консенсусе у вас
[27:22.040 --> 27:34.960]  общаются пропозеры и аксепторы или лидеры фолловер, и если вдруг вы, аксептор, получаете...
[27:34.960 --> 27:40.360]  если, точнее, не вдруг, а если просто вы, аксептор, получаете какую-то команду от пропозера,
[27:40.360 --> 27:46.520]  то вы смотрите соответствующий слот лога и отвечаете ему. Но может быть так, что вы аксептор,
[27:46.520 --> 27:53.560]  вы реплика, и вы уже у себя обрезали префикс лога, вы заменили его снепшотом,
[27:53.560 --> 28:02.760]  а вам приходит какой-то пропоз или препер для этого отрезанного префикса, и вы уже не можете
[28:02.760 --> 28:12.640]  ничего ответить. Ровно поэтому в ваш протокол вмешиваются вот такие служебные команды.
[28:13.360 --> 28:20.840]  Вот, пожалуйста, в RAFT-е лидер отправляет Append Entries, отправляет репликам какие-то команды,
[28:20.840 --> 28:25.840]  но если так оказывается, что реплика сильно отстала от лидера, то лидер уже не может
[28:25.840 --> 28:32.600]  отправить ей префикс лога, он отправляет ей снепшот. Или в Multipax все то же самое,
[28:32.600 --> 28:37.960]  может быть вы, аксептор, получили препер для старого слота, у вас его уже нет, и вы отвечаете
[28:37.960 --> 28:45.080]  ему не промессом, вы отвечаете ему снепшотом, который этот пропозер должен на свои реплики
[28:45.080 --> 28:56.080]  применить, установить для того, чтобы нагнать отставание. Собственно, реплика, которая отстала,
[28:56.080 --> 29:02.760]  скажем, на час от других, она не будет, конечно, нагонять весь лог, она просто догонит состояние
[29:02.760 --> 29:15.720]  других реплик, установив снепшот. Хорошо, вот это все, допустим, понятно, что это все можно сделать,
[29:15.720 --> 29:27.760]  но а теперь поговорим, как... Мы сейчас к этому моменту разобрали, какие данные нужно хранить
[29:27.760 --> 29:38.040]  на репликах, на диске. В случае RSM это снепшот и лог, в случае LSM это right-ahead-log и sustainable.
[29:38.040 --> 29:45.760]  А теперь следующий шаг, разобраться как с файловой системой, это все совместимо, как мы с файловой
[29:45.760 --> 29:51.680]  системой работаем, как мы сохраняем эти все данные на диск, как мы что-то пишем в лог. Задача
[29:51.680 --> 30:02.320]  вообще-то не простая. Вот в общем случае ее можно поставить так, у вас есть некоторое сложное
[30:02.320 --> 30:09.920]  состояние, которое лежит на диске, но оно может быть представлено совершенно по-разному, но в
[30:09.920 --> 30:16.200]  общем случае у вас какой-то набор файлов, что у вас еще может быть в файловой системе, и вы в этих
[30:16.200 --> 30:22.440]  файлах что-то меняете. Может быть вы дописываете в конец, а может быть вы прямо внутри файла что-то
[30:22.440 --> 30:31.560]  перезаписываете, скажем вы пишете B дерево, или у вас Google File System и вы перезаписываете часть
[30:31.560 --> 30:40.480]  вашего чанка. Можно представить совершенно разные варианты, ну вот давайте посмотрим на такую вроде
[30:40.480 --> 30:48.600]  бы простую подзадачу. Вот она решается точно в базах данных. У вас есть файл, и вы должны
[30:48.600 --> 30:58.560]  зафиксировать в этом файле какие-то свои изменения, перезаписать какой-то его фрагмент. Вот задача,
[30:58.560 --> 31:05.320]  которую мы сейчас разберем, выглядит так. Как перезаписать фрагмент файла? Задача очень простая,
[31:05.320 --> 31:10.840]  но выглядит просто. У вас для перезаписи фрагмента файла, например, вы реплика chunk
[31:10.840 --> 31:17.520]  server GFS, хотите перезаписать какой-то кусочек, который вам прислал клиент. GFS это умеет. У вас
[31:17.520 --> 31:24.960]  есть вызов per write. У вас есть открытый файловый дескриптор, у вас есть буфер, и вы хотите
[31:24.960 --> 31:35.080]  записать из него count byte по вот такому оффсету файла. Ну и отлично, начинаем файл писать.
[31:35.080 --> 31:46.040]  Давайте я буду писать некоторые псевдокод. Эта суть не поменяет совсем. Я хочу записать в
[31:46.040 --> 32:01.480]  файл в директории D по оффсету 2, 3 символа как будто бы. Вот такие вот. В файле при этом изначально
[32:01.480 --> 32:17.240]  написано фу. Ну точнее написано что-то, потом фу, потом что-то. Вот я это хочу, это фу хочу
[32:17.240 --> 32:29.480]  перезаписать на бар. Вот давайте подумаем в терминах отомарности и в терминах дюрабельности,
[32:29.480 --> 32:40.480]  надежности. Соблюдаются ли здесь два этих свойства? Да, маленькое техническое замечание, конечно,
[32:40.480 --> 32:47.440]  когда я говорю про запись бар или перезапись фу, то здесь я не думаю про отдельные символы,
[32:47.440 --> 32:53.560]  а скорее это блоки. Потому что для блочного устройства, над которым работает файловая система,
[32:53.560 --> 33:00.520]  единицей чтения записи является блок. Вот у меня здесь три блока, которые я условно обозначил как
[33:00.520 --> 33:08.720]  вот три буквы. И здесь тоже я перезаписываю три блока, а не просто три символа. Вот я должен,
[33:08.720 --> 33:17.640]  я собираюсь эти три блока перезаписать. Вот сначала поговорим про надежность. Верно ли,
[33:17.640 --> 33:23.920]  что если у меня завершится этот самый вызов write, то после этого я буду уверен, что я могу
[33:23.920 --> 33:29.960]  спокойно перезагрузиться, в смысле моя машина может перезагрузиться, и я данные не потеряю. Вот,
[33:29.960 --> 33:37.440]  к сожалению, это не так. Сам вызов write, ну никакой вызов write файловую систему не гарантирует,
[33:37.440 --> 33:42.280]  что данные достигли диска. Потому что, когда вы пишете что-то файловую систему, вы правило
[33:42.280 --> 33:49.840]  пишете в кэш-операционной системы. Ваши записи пока оседают в оперативной памяти,
[33:49.840 --> 33:57.640]  и для того, чтобы гарантировать, что эти данные были сброшены на диск, вам нужно сделать отдельный
[33:57.640 --> 34:07.640]  специальный вызов, который называется fsync. Вот он сбрасывает все ваши записи, которые накопились
[34:07.640 --> 34:17.360]  в кэш-операционной системы на устройство. То есть fsync – это тот способ, с помощью которого в
[34:17.360 --> 34:28.720]  файловых системах обеспечивается дюрабленность. Вы должны сделать fsync на файле. Вот вроде бы это
[34:28.720 --> 34:36.200]  обеспечивает нам дюрабилити. Но у нас есть второе свойство. Второе свойство – это атомарность
[34:36.200 --> 34:47.600]  записи файл. И тут уже сложнее. Вот мы говорим про атомарность записи вот сразу трех блоков. Мы
[34:47.600 --> 34:55.280]  перезаписываем сразу три блока. Гарантирует ли нам файловая система атомарность? Ну, нужно
[34:55.280 --> 35:01.160]  изучать документацию. К сожалению, файловая система – это такое место, где документация не очень
[35:01.160 --> 35:06.320]  строгая. И, как правило, сложно понять, какие именно гарантии та или иная файловая система дает.
[35:06.320 --> 35:13.680]  Вот некоторое время назад люди написали даже целую статью про ток. Они взяли файловые системы и
[35:13.680 --> 35:20.000]  исследовали, какими же свойствами эти системы обладают. Что они позволяют делать атомарно,
[35:20.000 --> 35:26.480]  а что не позволяют. И, скажем, файловые системы, ну, кажется, что все, позволяют вам атомарно
[35:26.480 --> 35:34.240]  перезаписать очень небольшую порцию данных, именно один сектор на блоке устройстве. Ну да,
[35:34.240 --> 35:39.480]  я говорил про блоки. Диск работает все-таки не с блоками, а работает с секторами. Они могут
[35:39.480 --> 35:46.400]  быть меньше. Но вот перезапись одного сектора – она точно всегда атомарная, потому что такими
[35:46.400 --> 35:53.760]  порциями пишет жесткий диск. Так он устроен. А вот, скажем, append одного сектора к файлу уже не
[35:53.760 --> 36:02.520]  атомарен. Хотя казалось бы, тот же самый объем данных. Запись нового сектора действительно атомарна.
[36:02.520 --> 36:07.760]  Но если вы хотите добавить что-то в файл, то вы должны, по крайней мере, увеличить его длину. А
[36:07.760 --> 36:16.840]  это запись в структуре iNode файла. А iNode – это часть методанных. А методанные и данные хранятся
[36:16.840 --> 36:24.000]  вообще-то в файловой системе в разных местах. Это разные блоки, разные секторы. Поэтому append
[36:24.000 --> 36:29.360]  от файловой системы требует двух записей в разные места, и поэтому атомарность вы здесь,
[36:29.360 --> 36:40.000]  по умолчанию, теряете. Но если мы говорим про перезапись сразу нескольких блоков, то тут можно
[36:40.000 --> 36:47.400]  не волноваться. В любой файловой системе такие операции будут не атомарными. Вот здесь крестик – это
[36:47.400 --> 36:55.920]  значит не атомарность. Так что мы должны каким-то образом, чтобы гарантировать crash consistency,
[36:55.920 --> 37:04.840]  мы должны позаботиться об атомарности перезаписи вот этого фрагмента. Каким образом мы это сделаем?
[37:04.840 --> 37:12.840]  Вот есть такой стандартный способ, который мы используем в реализации Logo RSM в текущей
[37:12.840 --> 37:21.800]  домашней системе и который использует любая база данных и использует levelDB. В общем,
[37:21.800 --> 37:30.400]  нам нужно залогировать наши действия. Мы не можем прямо писать bar, потому что если мы перезагрузимся
[37:30.400 --> 37:39.400]  в середине этой записи, то мы можем получить файл, в котором будет написано не фу, не bar,
[37:39.400 --> 37:49.040]  а boo, или far, или что-нибудь еще. Все эти варианты окажутся возможными. Так что если это случится,
[37:49.040 --> 37:59.960]  такое может случиться. Мы должны уметь видимо откатить наши неполноценные или откатить то,
[37:59.960 --> 38:06.080]  что мы не дописали, либо накатить то, что мы собирались написать. Ну вот давайте мы для
[38:06.080 --> 38:11.640]  разнообразия откатывать научимся. Собственно, это вопрос про транзакции, который сегодня был на
[38:11.640 --> 38:17.840]  реакции. Что делать, если мы хотим транзакцию откатить? Нам нужно уметь поддерживать андулог.
[38:17.840 --> 38:29.360]  То есть мы создадим новый файл перед тем, как перезаписывать существующий. И в этот файл
[38:29.360 --> 38:40.240]  запишем такую служебную запись, что мы собираемся в нашем файле файл перезаписать софтсеты 2,
[38:40.320 --> 38:49.680]  3 блока, в которых раньше было написано foo. Вот мы поместили туда такую служебную запись.
[38:49.680 --> 38:59.680]  После этого сделали запись файл, потом сделали fsync, а потом сделали unlink для лога. Он нам больше
[38:59.680 --> 39:15.400]  не нужен. Будет ли это работать? Ну, как сказать? На самом деле мы... То есть у нас раньше была
[39:15.400 --> 39:20.640]  проблема с неотомарностью вот этой записи, а теперь у нас другая проблема с неотомарностью уже
[39:20.640 --> 39:27.600]  этой записи. То есть мы, казалось бы, передвинули проблему просто в другое место. Но теперь нам
[39:27.600 --> 39:38.480]  легче ее решить. Вот если мы записали все в лог и поломались где-то на этом этапе и
[39:38.480 --> 39:46.440]  получили в файле boo, то нам не сложно это понять, потому что после рестарта мы увидим,
[39:46.440 --> 39:54.320]  что наш лог еще находится в файловой системе. Видимо, сама запись была неуспешна и нужно
[39:54.320 --> 40:00.720]  ее откатить. Для этого мы перечитаем лог. Мы, видимо, ожидаем, что в нем написаны все байты,
[40:00.720 --> 40:12.200]  которые были исходно в файле. Байты foo. Но что если мы сам лог не дописали? Точнее,
[40:12.200 --> 40:20.040]  мы перезагрузились в тот момент, когда мы записывали в лог эту служебную запись. И в итоге
[40:20.040 --> 40:30.240]  файл есть, файл с логом есть, но в нем написано такое. Какой-то мусор вместо наших блоков. Почему
[40:30.240 --> 40:39.400]  такое могло случиться? Потому что запись файл требует изменения, то есть записи блоков с данными и
[40:39.400 --> 40:48.560]  изменения iNode, то есть изменения длины файла. И пусть файл стал нужной длины уже, то есть там
[40:48.560 --> 40:54.720]  действительно есть заголовок и есть вот три блока с данными. Но сами эти блоки с данными,
[40:54.720 --> 40:59.000]  они успели только лоцироваться, но еще не успели перезаписаться, поэтому в них какой-то мусор.
[40:59.000 --> 41:07.520]  И в итоге мы, если мы перезагрузились в середине этого райта, то он не отомарен и мы можем из этого
[41:07.520 --> 41:18.680]  лога откатить нашу запись bar, заменив ее на мусор. Ну то есть раз уж запись bar не начался,
[41:18.680 --> 41:27.960]  не началось еще, то в файле все еще фу, а мы перезапишем этот фу на мусор. Очень неприятная
[41:27.960 --> 41:35.640]  ситуация. Нужно как-то ее починить. Ну вот для этого тоже есть универсальное решение. Мы должны
[41:35.680 --> 41:41.080]  убедиться, что наш лог вообще-то находится в целостном состоянии. То есть вот эта запись в логе,
[41:41.080 --> 41:48.920]  она действительно валидная. Поэтому мы к такой служебной записи добавляем чексумму.
[41:48.920 --> 41:59.400]  То есть мы считаем чексумму от записанных дан, от данных, которые мы собираемся поместить в
[41:59.400 --> 42:11.400]  анду лог и эту чексумму обязательно добавляем в запись в этом логе. Если вдруг запись в лог
[42:11.400 --> 42:18.440]  покарабтилась, то есть она не была завершена, то видимо, когда мы будем читать лог, то мы увидим,
[42:18.440 --> 42:26.640]  что чексумма, вычисленная по данным, не сходится с чексуммой, которая записана в заголовке этой
[42:26.640 --> 42:34.600]  записи. И после этого, видимо, поймем, что запись в лог не была успешна. То есть мы даже не смогли
[42:34.600 --> 42:41.560]  записать старое состояние файла. Это значит, что мы, видимо, еще не приступили к перезаписи самого
[42:41.560 --> 42:52.720]  файла. Так что ничего делать не нужно. Вот чексумма, это на самом деле безумно важная и такая
[42:52.720 --> 43:00.400]  фундаментальная идея для корректности систем самого разного уровня в самых разных контекстах.
[43:00.400 --> 43:07.760]  Мы говорили с вами про LevelDB и RocksDB, про хранилища локальные, и вот RocksDB в своей статье последней
[43:07.760 --> 43:14.920]  описывает, я кидал ее к нам в чат, говорит, что они используют чексуммы вот буквально для всего,
[43:14.920 --> 43:19.880]  то есть они используют чексуммы для отдельных блоков в Isostable, они используют чексуммы для
[43:19.880 --> 43:25.640]  самих Isostable, потому что в случае, когда там, не знаю, нужно запекапить данные, они копируют сами
[43:25.640 --> 43:31.920]  Isostable, они не то что там делают те же самые путы, они просто берут представление LSM на диске,
[43:31.920 --> 43:37.680]  представление RocksDB на диске, то есть сами Isostable и копируют их на другие машины. И нужно,
[43:37.680 --> 43:46.040]  чтобы на другой машине потом можно было чексумму проверить. Ну вообще, стоит об этом рассуждать так.
[43:46.040 --> 43:52.520]  Вот у нас есть компьютеры, и компьютеры нам нужны для двух вещей, чтобы что-то вычислять и чтобы
[43:52.520 --> 44:00.480]  что-то хранить. И вот мы можем хранить в компьютере что-то на диске, можем хранить что-то в памяти,
[44:00.480 --> 44:06.480]  и мы можем, ну так, условно хранить в проводах, то есть мы отправляем с одной стороны конца
[44:06.920 --> 44:17.120]  с одного конца провода и читаем из другого конца провода. И вот эти три вида хранилища,
[44:17.120 --> 44:23.280]  они работают с нашими машинными словами, с битами, с байтами, ну короче, с какими-то
[44:23.280 --> 44:30.920]  вот дискретными единицами информации. При этом, понятно дело, в физическом мире ничего дискретного
[44:30.920 --> 44:36.520]  нет. У нас нет никакого представления для нуля и для единицы вот такого вот четкого. У нас,
[44:36.520 --> 44:45.320]  не знаю, какие-то заряды хранятся. В итоге, чему это может привести? К тому, что вы с одной стороны
[44:45.320 --> 44:51.480]  записали в провод одни данные, а получили другие. Или вы записали что-то на диск, а на диске
[44:51.480 --> 45:00.840]  покарабчивался сектор. Или вы записали что-то в память, а потом читаете, и оказывается,
[45:00.840 --> 45:05.640]  что вы прочли не то, что, точнее не то, что бы оказывается, вы как раз об этом не знаете. Но
[45:05.640 --> 45:11.560]  память внутри себя данные испортила. Вот вы от любого хранилища, будь то файя, будь то память,
[45:11.560 --> 45:16.720]  будь то диск или будь то сеть, ожидаете, что это хранилище реализует тождественную функцию. Вы
[45:16.720 --> 45:23.640]  что-то туда записали, а потом что-то прочитали, и это будет то же самое. Но в силу непрерывности
[45:23.640 --> 45:30.360]  физического мира такое обеспечение всегда удается. В случае с диском, наверное, понятно,
[45:30.360 --> 45:34.840]  в случае с сетью я прокомментирую сейчас. Но в случае с памятью это может выглядеть довольно
[45:34.840 --> 45:39.440]  неожиданно, потому что в памяти мы храним, не знаю, какие-то указатели, и если в них
[45:39.440 --> 45:45.600]  будут флипаться биты, то совершенно непонятно, чего мы от нашей системы ожидаем. В смысле от
[45:45.600 --> 45:52.600]  нашего компьютера вообще, как он будет работать. Так вот, с памятью все как раз более-менее хорошо,
[45:52.600 --> 45:59.960]  потому что, во-первых, проблемы случаются. Сейчас я на этой статье про RopsDB. Во-первых,
[45:59.960 --> 46:06.880]  проблемы случаются, то есть действительно память может переворачивать биты. Вот буквально эта
[46:06.880 --> 46:14.240]  статья довольно старая уже девятого года от Google про исследование ошибок в планках памяти,
[46:14.400 --> 46:21.200]  которые возникают в их кластерах. Ну и ответ такой проблемы возникают. Вот,
[46:21.200 --> 46:31.680]  проблемы возникают, но их существенно больше нуля, но, к счастью, они не оказывают прямого
[46:31.680 --> 46:36.680]  эффекта на наши программы, потому что планки памяти, которые используются в датацентрах,
[46:36.680 --> 46:43.200]  они надежны, в том смысле, что внутри них реализованы коды коллекции ошибок. Вот такие
[46:43.200 --> 46:50.720]  планки способны исправить перевернутый бит в машинном слове. А почему он вообще перевернулся?
[46:50.720 --> 46:56.720]  Ну причины могут быть разными. Ну, скажем, возможно такая, что из космоса прилетел луч и
[46:56.720 --> 47:03.120]  зарядил какой-то маленький конденсатор у нас в памяти. Вот это вполне реальная проблема. Какие-нибудь
[47:03.120 --> 47:08.760]  там альфа-частицы, вот все они могут переворачивать наши биты. И это случается не то, чтобы часто,
[47:08.840 --> 47:14.640]  но это вообще в принципе случается. Вот в одной планке памяти можно за год насчитать
[47:14.640 --> 47:20.840]  четыре тысячи корректируемых ошибок. Может быть не очень много, смотря относительно того,
[47:20.840 --> 47:26.240]  с какой интенсивностью с этой планкой работают, но любой перевернутый бит может очень сильно
[47:26.240 --> 47:34.520]  навредить, он может иметь такой каскадный эффект. В итоге хорошо бы после каждой записи куда угодно
[47:34.520 --> 47:41.960]  проверять, что вы читаете после этого те данные, которые вы записали. В памяти, планка памяти
[47:41.960 --> 47:52.400]  делает это за нас. С диском за нас это не делают. В случае с диском, в случае с диском мы должны
[47:52.400 --> 47:58.760]  сами использовать чексуммы. И не просто один раз записать на диск, а потом при чтении чексумму
[47:58.760 --> 48:05.520]  проверять. А лучше вообще периодически просто в фоне перечитывать данные и проверять, что они
[48:05.520 --> 48:11.000]  все еще целы, то есть они сходятся с чексуммой, которую мы ожидаем. Скажем, вы пишете чанк сервера,
[48:11.000 --> 48:16.880]  который хранит чанки для Google File System. Вот к вам пришли, записали чанк, вы положили его себе на
[48:16.880 --> 48:25.200]  диск. А потом спустя год вы его читаете, оказывается что чексумма не сходится. А вообще-то мастер считал,
[48:25.200 --> 48:32.880]  что вы храните реплику, а вы на самом деле уже не являетесь репликой. Поэтому разумно в фоне
[48:32.880 --> 48:38.080]  перечитывать ваши локальные чанки, и если вдруг вы увидите, что какие-то из них попортились,
[48:38.080 --> 48:44.280]  у них не сходятся чексуммы, то вы должны отправить информацию об этом мастеру, чтобы он читал,
[48:44.280 --> 48:49.080]  что вы больше не реплика чанка и дореплицировал этот чанк на другие машины.
[48:49.080 --> 48:59.320]  Значит, с памятью за нас проблему решают сами планки памяти с помощью кодокоррекции
[48:59.320 --> 49:08.440]  ошибок. В случае с файловыми системами мы чексуммим все самостоятельно. Ну и кстати, раз уж мы недалеко,
[49:08.440 --> 49:20.800]  то write ahead логи, которые мы используем в нашем фреймворке, они конечно же работают с чексуммой.
[49:20.800 --> 49:29.160]  Когда мы пишем что-то на диск, то мы сначала пишем для любой записи заголовок, и в этой записи
[49:29.160 --> 49:38.040]  есть чексумма, которую мы ожидаем от данных. А кроме того, у каждого заголовка есть еще и
[49:38.040 --> 49:44.760]  некоторые magic number. Давайте я покажу структуру хеддер. Некоторые magic number просто для того,
[49:44.760 --> 49:50.520]  чтобы понимать, что те байты, которые мы читаем с самого начала write ahead логи, это вообще-то
[49:50.520 --> 49:57.200]  байты заголовка или просто мусор. А то мы можем прочесть заголовок, потому что у нас есть нужное
[49:57.200 --> 50:03.360]  количество байт, но заголовком он конечно не является при этом. Скорее всего, там чексумма дальше
[50:03.360 --> 50:11.520]  разойдется, но все же мы можем быстро понять, что перед нами даже не заголовок. С файлами и
[50:11.520 --> 50:18.680]  с памятью мы более-менее умеем безопасно работать, но нам нужно работать безопасно и с сетью.
[50:18.680 --> 50:27.920]  И работать с сетью, казалось бы, тут уж точно все безопасно, потому что когда мы отправляем,
[50:27.920 --> 50:33.360]  скажем, сообщение с РПС-вызовом, служебное сообщение, в котором описано, какой метод мы
[50:33.360 --> 50:39.840]  вызываем на комсервисе, с какими аргументами, то, казалось бы, вот это сообщение на нашем уровне,
[50:39.840 --> 50:49.760]  на уровне пользователя упаковывается сначала в сегмент DCP, у которого есть чексумма. Этот сегмент
[50:49.760 --> 50:57.160]  DCP упаковывается в IP-дотограмму для роутинга запроса, для роутинга нашего пакета. И
[50:57.160 --> 51:05.600]  эта IP-дотограмма, в свою очередь, по проводам двигается в виде Ethernet-фрейма. То есть у вас
[51:05.600 --> 51:13.360]  есть разные уровни с текстовых протоколов, и на каждом уровне у вас есть свои собственные
[51:13.360 --> 51:22.360]  заголовки, и на каждом уровне у нас есть чексумма. Она есть в Ethernet, она есть в протоколе IP,
[51:22.360 --> 51:36.680]  вот здесь вот, сейчас, нет, вот она, и она есть в протоколе DCP. Но при этом оказывается, что этого
[51:37.160 --> 51:48.080]  недостаточно. Это очень странно, что этого недостаточно, но, тем не менее, этого оказывается
[51:48.080 --> 51:53.960]  недостаточно. И вот, скажем, инженер Twitter советует нам, что если вы пишете свой собственный протокол,
[51:53.960 --> 52:00.640]  который двигает данные по сети, поверх DCP, вы все равно должны на уровне своего приложения,
[52:00.640 --> 52:05.600]  на уровне своего протокола уже прикладного, все равно использовать чексумму, несмотря на то,
[52:05.640 --> 52:16.400]  что на трех уровнях под вами они есть. И вот в нашем RPC-фреймворке тоже есть чексуммы, и сейчас я
[52:16.400 --> 52:22.520]  попытаюсь объяснить, почему же они требуются, почему всего того, что уже есть в сетевых протоколах,
[52:22.520 --> 52:30.720]  оказывается недостаточно. Вот если мы отправляем сообщения, смотрим на транспортный канал,
[52:30.720 --> 52:50.000]  смотрим на...кажется, метод назывался...секундочку...это мы получаем реквест, вот
[52:50.000 --> 52:58.600]  отправляем респонс. И вот мы формируем пакет, реквест, содержимое...содержим нашего вызова,
[52:58.720 --> 53:11.640]  и высчитываю чексумму по данным и всем...ладно, я почему-то не хочу приходить к предзрению...высчитываем
[53:11.640 --> 53:18.720]  чексумму...давайте так покажу вручную...по всем полям, которые в этом реквесте есть.
[53:18.720 --> 53:29.040]  Почему же это необходимо? Ну, давайте начнем из глубины. Вот у нас есть на самой глубине протокол
[53:29.040 --> 53:43.800]  Ethernet, и на этом уровне работают очень сильные, очень мощные чексуммы CRC 32-битный, и не так-то
[53:43.800 --> 54:01.120]  просто его поломать. Но проблема очень странная. Сейчас найду ссылку про это, к сожалению их...и их
[54:01.120 --> 54:10.400]  много. Так вот, CRC само по себе надежная, то есть чексумма в Ethernet frame их надежные, но беда в том,
[54:10.400 --> 54:18.520]  что когда какая-то сетевая коробка получает frame и отправляет его дальше по маршруту, то она
[54:18.520 --> 54:25.640]  внутри не то чтобы сохраняет frame в неизменном состоянии, она в нем может что-то поменять и
[54:25.640 --> 54:34.320]  пересчитать чексумму. То есть, короче говоря, какие-то свечи могут суммы пересчитывать,
[54:34.320 --> 54:41.880]  а это означает, что эти чексуммы защищают нас от ошибок, которые возникают именно в проводах,
[54:41.880 --> 54:50.440]  но они не защищают нас от ошибок, которые происходят в самих свечах. Но они бывают
[54:50.440 --> 54:55.800]  ломаются, в них бывают баги, в них бывает, ну, не знаю, что-то идет не так. Они могут корраптить
[54:55.800 --> 55:05.080]  пакет, и в итоге коробка получает пакет по сети, внутри этой коробки он каким-то образом портится,
[55:05.080 --> 55:11.880]  в нем портится данные вашей там IP-дотограммы, вашего TCP-сегмента, а после этого перед отправкой
[55:11.880 --> 55:18.400]  коробка пересчитывает чексумму по уже испорченным данным, и дальше эта чексумма с этими испорченными
[55:18.400 --> 55:23.720]  данными отправляется до следующей коробки. И, разумеется, вот такую ошибку никто уже не обнаружит.
[55:23.720 --> 55:31.480]  Так что такие ошибки могут возникать. Ну хорошо, если даже у нас покораптились данные, то в конце концов
[55:31.480 --> 55:40.120]  у нас есть протокол IP уровнем выше, и вот он-то заметит. Протокол IP не заметит, потому что в
[55:40.120 --> 55:45.920]  протоколе IP-чексуммы рассчитывается только по полям заголовка. То есть протокол IP не
[55:45.920 --> 55:52.240]  пытается обеспечивать целостность данных, которую он двигает. Он про маршрутизацию,
[55:52.240 --> 56:02.040]  он не про транспорт. Ну ладно, IP нам не поможет, интернет тоже может не помочь, но у нас же есть TCP
[56:02.040 --> 56:13.120]  с чексуммой. Так вот, оказывается, что и на TCP тоже надежды нет, потому что чексумма в TCP
[56:13.120 --> 56:23.160]  безумно слабая. Чексумма в TCP это всего лишь, ну с некоторыми оговорками, это сумма 16 битных
[56:23.160 --> 56:37.000]  фрагментов данных. Вот просто сумма. И если вдруг в двух вот таких вот блоках по 16 бит есть два
[56:37.040 --> 56:44.200]  перевернутых бита в одних и тех же позициях, то чексумма такого TCP сегмента не поменяется.
[56:44.200 --> 56:53.480]  То есть если у вас всего лишь перевернулись два бита в одних и тех же аффсетах, то чексумма
[56:53.480 --> 57:02.640]  TCP ошибки не заметит. Стэк TCP ошибки не заметит. IP вообще это проверять не собирается, в чексумме
[57:02.640 --> 57:08.760]  только заголовок. А в интернете просто ошибка была пригналирована, потому что она была допущена
[57:08.760 --> 57:16.000]  в какой-то коробке, в каком-то свече и просто чексумма была пересчитана. Вот абсолютно безумная
[57:16.000 --> 57:22.640]  история, но тем не менее эти проблемы в реальности могут возникать, поэтому любой RPC framework,
[57:22.640 --> 57:29.440]  любой протокол коммуникации, который вы реализуете на прикладном уровне, должен чексумме данные
[57:29.440 --> 57:46.880]  самостоятельно. Ну скажем об этом пишу даже в статье Pro Bigtable. Вот у них есть там где-то пункт
[57:46.880 --> 57:58.440]  про, сейчас найду, выученные уроки. Ну вот, урок, который они извлекли, что у них чексумма есть
[57:58.440 --> 58:04.560]  и в RPC тоже, потому что стандартных сетевых протоколов оказывается недостаточно. Абсолютно
[58:04.560 --> 58:11.000]  странно, абсолютно жуткая история, если вы инженерно, то есть очень трудно на что-то полагаться. Так вот,
[58:11.000 --> 58:23.280]  полагаться нужно только на себя. Вот есть такой принцип в дизайне. Он про то, что вам нужно
[58:23.280 --> 58:31.000]  обеспечивать надежность передачи, вам нужно обеспечивать целостность ваших данных при передаче
[58:31.000 --> 58:35.880]  с одного компьютера, допустим, на другой или при работе с файловой системой. Так вот, между вашим
[58:35.880 --> 58:40.800]  компьютером и другим находится, ну, невероятное количество сетевого оборудования по пути и
[58:40.800 --> 58:46.440]  невероятное количество протоколов под капотом. Вот такой целый стэк сложный. И вы, конечно,
[58:46.440 --> 58:51.880]  можете надеяться, что на каждом уровне и на каждом переходе внутри все будет корректно и все будет
[58:51.880 --> 58:59.280]  проверено. Но гораздо надежнее просто делать проверки на концах, в двух конечных точках
[58:59.280 --> 59:09.240]  коммуникации. То есть, как бы не были надежные сетевые протоколы, можно об этом просто не думать,
[59:09.240 --> 59:16.520]  и стоит об этом не думать для надежности, и проверять чек-сумы, писать чек-суму при отправке
[59:16.520 --> 59:21.800]  и проверять при получении. Писать чек-суму при записи на диск и проверять ее после чтения.
[59:25.800 --> 59:36.360]  Пусть лучше, пусть каждый, пусть эти чек-сумы будут в некотором смысле избыточны, но в
[59:36.360 --> 59:44.240]  худшем случае вы получаете дополнительную безопасность для своих вариантов. Ну хорошо,
[59:44.240 --> 59:53.480]  вот значит с чек-сумами мы разобрались. Мы можем, вернемся к нашему примеру, записать ванду лог
[59:53.480 --> 01:00:00.400]  вот такую вот чек-суму, записать ванду лог запись с чек-сумой, и если вдруг после рестарта мы видим,
[01:00:00.400 --> 01:00:08.000]  что лог-файл все еще не удален, то мы можем пойти в этот лог-файл, вычитать из него данные,
[01:00:08.000 --> 01:00:13.640]  проверить, что чек-сума сходится, и если чек-сума сходится, то откатить запись. А если чек-сума не
[01:00:13.640 --> 01:00:21.040]  сходится, значит этой записи не было. Ну как будто бы вот такой протокол уже безопасен, но на самом
[01:00:21.040 --> 01:00:27.360]  деле тоже нет. На самом деле после выполнения такого протокола вы можете закончить с файловой
[01:00:27.360 --> 01:00:40.880]  системой, в которой в файле написано, скажем, boo, а в логе написано 2-3 чек-сум, а дальше снова мусор.
[01:00:40.880 --> 01:00:49.080]  То есть вы и откатить ничего не можете, и то есть ваша запись частично применилась,
[01:00:49.080 --> 01:00:55.920]  но откатить вы ее не можете, потому что у вас и лог покарабчен. Почему такое могло быть?
[01:00:55.920 --> 01:01:03.720]  Тут на самом деле ситуация еще печальней, потому что когда вы думаете про файловые системы,
[01:01:03.720 --> 01:01:10.440]  вы на самом деле можете о них думать как про, ну в каком-то смысле модели памяти. В моделях
[01:01:10.440 --> 01:01:15.280]  памяти у нас были две проблемы – атомарность и упорядочивание операций. Так вот с файловыми
[01:01:15.280 --> 01:01:22.200]  системами то же самое. Вы должны думать и про атомарность, и про упорядочивание. Вот в
[01:01:22.200 --> 01:01:31.840]  определенных режимах нет гарантий, что записи в этот лог и записи в файл будут упорядочены
[01:01:31.840 --> 01:01:39.840]  друг относительно друга. Может быть вы успеете записать какие-то блоки из этой записи и какие-то
[01:01:39.840 --> 01:01:45.040]  блоки из этой, но не все. То есть то, что вы увидели хотя бы один блок из этой записи, не означает,
[01:01:45.040 --> 01:01:52.600]  что выполнилась запись предыдущей. Вам нужно каким-то образом явно упорядочить две эти записи.
[01:01:52.600 --> 01:02:00.440]  И для этого, для того чтобы это сделать, ну в моделях памяти у нас были специальные инструкции
[01:02:00.440 --> 01:02:07.480]  барьеры. Здесь у нас есть тот же самый fsync. Мы должны добавить еще вот такой вызов,
[01:02:07.480 --> 01:02:14.560]  который гарантирует, что запись в лог будет упорядочена до перезаписи блока в файле.
[01:02:14.560 --> 01:02:26.360]  Да, я не рассказал с самого начала, кое-что еще забыл про fsync. fsync он не такой простой,
[01:02:26.360 --> 01:02:35.160]  каким кажется. Вот он с одной стороны должен сбрасывать данные из кэша операционной системы
[01:02:35.160 --> 01:02:43.720]  на диск, но может случиться так, что fsync вернет ошибку. И вот оказывается, что это совершенно
[01:02:43.720 --> 01:02:48.640]  чудовищная история. Сейчас не знаю, найду я быстро ее или нет. Где-то она у меня была открыта.
[01:02:48.640 --> 01:02:58.880]  Вот оказывается Postgres использовал fsync, но у них про это есть доклад. Они 20 лет неправильно
[01:02:58.880 --> 01:03:07.760]  использовали fsync. Ну как неправильно? Они ожидали разумного, что... не умею переходить по ссылкам.
[01:03:07.760 --> 01:03:15.800]  Они ожидали разумного, что если они делают fsync, а он завершается ошибкой, то это означает,
[01:03:15.800 --> 01:03:21.600]  что оно, видимо, нужно fsync повторить когда-нибудь в будущем. И если fsync завершился,
[01:03:21.600 --> 01:03:28.080]  то он вместе с собой записал... он гарантированно сбросил все данные,
[01:03:28.080 --> 01:03:35.320]  которые предшествовали этому fsync в этот файл. Вот оказывается, fsync реализован не так. Это
[01:03:35.320 --> 01:03:42.720]  абсолютно жуткая история, но тем не менее. Почему fsync может вернуть ошибку? Ну потому,
[01:03:42.720 --> 01:03:49.160]  что вы могли писать данные на флешку. И вот вы писали данные на флешку, а потом флешку вытащили,
[01:03:49.160 --> 01:03:56.880]  а потом сказали fsync. И в итоге вы не можете больше сбросить данные на диск, ну данные на эту флешку
[01:03:56.880 --> 01:04:03.600]  из оперативной памяти. Ну что в этом случае делает операционная система? Ну раз флешку вытащили,
[01:04:03.600 --> 01:04:08.800]  то, видимо, данные нужно из оперативной памяти выбросить, потому что они уже не могут быть
[01:04:08.800 --> 01:04:16.080]  записаны на устройство. Поэтому операционная система могла делать и делала так. А именно,
[01:04:16.080 --> 01:04:23.840]  она просто дропала данные, то есть на Linux конкретно, дропала данные в случае ошибки fsync. Поэтому
[01:04:23.840 --> 01:04:29.600]  если у вас первый fsync проваливается, то он вместе с собой дропает страницы, которые были помечены
[01:04:29.600 --> 01:04:36.040]  как грязные, и в итоге последующий fsync уже ничего на диск не сбросит. А в поздросе,
[01:04:36.040 --> 01:04:43.520]  на этом хрупком основании, что fsync заперсистит все ваши данные, которые предшествовали этому
[01:04:43.520 --> 01:04:55.040]  fsync, держалась логика чекпоинтов и логика обновлений хранилища. Вот, абсолютно чудовищная история.
[01:04:55.040 --> 01:05:00.920]  Ну вот так fsync реализован, такие у него странные гарантии. Если вы видите ошибку от fsync,
[01:05:00.920 --> 01:05:05.360]  то нужно падать. Ну а почему вы не с флешками работаете, то есть почему это делает fsync,
[01:05:05.360 --> 01:05:10.240]  более-менее понятно. Но с другой стороны, у вас не флешка. И почему бы вы вдруг увидели такую ошибку?
[01:05:10.240 --> 01:05:17.120]  Ну почему бы вы вообще увидели ошибку в fsync? Ну потому что сейчас вы тоже не то, чтобы с
[01:05:17.120 --> 01:05:25.200]  локальным диском работаете. Скорее всего, если вы работаете в облаке, то под вами не настоящий диск
[01:05:25.200 --> 01:05:32.080]  локальный, а под вами некое виртуальное диск, виртуальное блочное устройство, за которым стоит
[01:05:32.080 --> 01:05:36.000]  некоторая большая распределённая система. То есть когда вы пишете на диск, вы ходите по сети куда-то.
[01:05:36.000 --> 01:05:42.320]  Ну и понятно, что в этом смысл распределённых систем, что у вас где-то в другом месте ломается
[01:05:42.320 --> 01:05:48.120]  компьютер, и в итоге у вас, в вашей виртуальной машине почему-то какие-то странности с диском,
[01:05:48.120 --> 01:05:53.920]  и в итоге ваш fsync возвращает ошибку и дропает данные. Поэтому кажется, что разумная стратегия при
[01:05:53.920 --> 01:06:01.520]  получении ошибки от fsync это поскорее упасть. То есть полагаться, что следующий fsync узнает,
[01:06:01.520 --> 01:06:13.640]  ну починит, всё, нельзя. Да, ещё вспомнил про баги. Это такой не то чтобы баг Linux, они не знают.
[01:06:13.640 --> 01:06:21.320]  Очень странное его поведение. Ну вот с чексумами, и почему мы ещё не можем доверять протоколам и их
[01:06:21.320 --> 01:06:28.880]  чексумам? Ну потому что бывают баги в сетевом стеке. Вот, ну я вам могу кинуть дальше ссылку,
[01:06:28.880 --> 01:06:36.760]  сейчас я найду её где-то. Так вот, в случае там какой-то комбинации виртуализации и TCP,
[01:06:36.760 --> 01:06:42.280]  операционная система могла просто скипать проверку чексумы. Вот в твиттере где-то на
[01:06:42.280 --> 01:06:51.520]  это нарвались и там долго-долго отлаживали это всё искали. Вот так. Очень страшно жить в таком
[01:06:51.520 --> 01:06:58.000]  мире. Но кажется, что мы починили теперь это. То есть мы используем андулог, мы в нём используем
[01:06:58.000 --> 01:07:03.440]  чексумы, чтобы проверять что андулог в целостном состоянии. Мы эфсинкаем его, чтобы гарантировать,
[01:07:03.440 --> 01:07:13.040]  что запись в андулог предшествует write файл. Мы эфсинкаем файл для durability. Ну вот вроде бы
[01:07:13.040 --> 01:07:18.000]  всё. Мы достигли durability, наконец-то тамарность достигли. Так вот, оказывается, что и это ещё не всё.
[01:07:18.000 --> 01:07:29.240]  Потому что даже после такого протокола мы можем увидеть в файле строчку boo и увидеть,
[01:07:29.240 --> 01:07:35.000]  что у нас нет лога. Лога нет, boo есть, данные в несогласованном состоянии, восстановить их
[01:07:35.000 --> 01:07:45.080]  невозможно. Почему такое может произойти? Потому что на самом деле запись лога, ну вот создание и
[01:07:45.080 --> 01:07:53.160]  запись лога это не только запись в файл с логом. Ну да, вот у нас есть запись в файл с логом,
[01:07:53.160 --> 01:08:02.200]  то есть мы должны зафиксировать с помощью эфсинка изменения в файле, то есть в его данных и в его
[01:08:02.200 --> 01:08:09.800]  метод данных. То есть мы здесь зафиксируем, что длина файла изменится и страницы, блоки,
[01:08:09.800 --> 01:08:18.440]  которые мы записали в файл, будут сброшены на диск. Но вот этот протокол требует вообще-то
[01:08:18.440 --> 01:08:27.720]  создания файла. То есть он требует ещё одной записи, а именно в iNode директории. Директория
[01:08:27.720 --> 01:08:32.760]  это тоже файл и в неё тоже нужно записать, что вообще-то лог появился в файловой системе.
[01:08:32.760 --> 01:08:44.040]  Вот сама, сам эфсинг файла это не гарантирует. Эфсинг файла гарантирует, что будут сброшены
[01:08:44.040 --> 01:08:50.560]  изменения для данного файла. Но вообще-то сам по себе файл не знает про то, где он в
[01:08:50.560 --> 01:08:56.920]  дереве файловой системы находится. Ну просто потому, что на один и тот же файл могут ссылаться
[01:08:56.920 --> 01:09:04.720]  разные пути в файловой системе. Так что это внешние ссылки. И сам файл их не хранит,
[01:09:04.720 --> 01:09:13.120]  он хранит максимум счётчик ссылок. В итоге вы действительно заперсистили содержимое лога,
[01:09:13.120 --> 01:09:20.440]  но вы не гарантировали здесь, что вы надёжно сохранили запись в директорию, в которой вы
[01:09:20.440 --> 01:09:34.720]  создали этот лог. Так что вам нужен ещё один эфсинг на самой директории D. Вот любая надёжная
[01:09:34.720 --> 01:09:44.800]  реализация райта хэт логгинга должна использовать эфсинг ещё и на директории. И скажем, почему это
[01:09:44.800 --> 01:09:48.840]  важно для нас, почему это важно для людей, которые пишут, скажем, строят, скажем,
[01:09:48.840 --> 01:09:56.560]  RSM и там используют сегментированные логи. Если вы создаете сегмент лога на диске и пишете в
[01:09:56.560 --> 01:10:02.160]  него там очередные записи, которые прислал лидер, то вы должны гарантировать, что этот сегмент не
[01:10:02.160 --> 01:10:09.920]  потеряете. Но для самого лога вы используете, конечно, райта хэт логгинг с чексумами. Вы
[01:10:09.920 --> 01:10:17.920]  пишете чексуму, вы пишете magic number для заголовка перед собственной записью на диск. Но вам ещё
[01:10:17.920 --> 01:10:24.800]  нужно гарантировать, что если вы вообще строите, открываете новый файл, создаете новый файл,
[01:10:24.800 --> 01:10:34.280]  то действительно в директории этот файл тоже будет виден. Иначе вы можете потерять сегмент
[01:10:34.280 --> 01:10:42.320]  лога и потерять обещание, которое вы дали лидеру. Ну и в конце концов,
[01:10:42.320 --> 01:10:56.840]  чтобы убедиться, что лог не нужен, нужно ещё, кажется, сделать это. Чтобы убедиться,
[01:10:56.840 --> 01:11:06.880]  что лог удалён. Ну вот, протокол перезаписи фрагмента файла выглядит вот так. Абсолютно
[01:11:06.880 --> 01:11:14.520]  жуткая история, ещё раз повторю. Вы не можете просто так пойти перезаписать несколько блоков
[01:11:14.520 --> 01:11:21.000]  файла. Вы должны устроить довольно сложный протокол и сделать сразу 3-4 fsync, чтобы гарантировать,
[01:11:21.000 --> 01:11:28.240]  что вы можете после рестарта откатиться к старому состоянию или накатить новое,
[01:11:28.240 --> 01:11:35.040]  то есть обеспечить стомарность. Крэш консистенции достигается сложно. Крэш консистенции достигается
[01:11:35.040 --> 01:11:41.080]  конечно же логами и плюс большим количеством вот таких вот fsync на директориях и на файлах. Ну
[01:11:41.080 --> 01:11:47.880]  и чексумами. То есть весь этот аппарат необходим, чтобы просто иметь персидентное состояние на диске.
[01:11:47.880 --> 01:11:57.000]  Почему так? Ну, трудно сказать, почему так. Вот абстракция файловой системы такова. Вообще вот
[01:11:57.000 --> 01:12:02.360]  можно понять, что абстракция файловой системы это довольно, ну не знаю, почему она такая сложная,
[01:12:02.360 --> 01:12:09.200]  но она довольно скверная, потому что вот в ней очень много свойств, которые плохо документированы,
[01:12:09.200 --> 01:12:15.520]  и которые вот очень легко опираться на какое-то неверное ожидание, на какую-то гарантию,
[01:12:15.680 --> 01:12:22.200]  которая на самом деле не соблюдается. А комбинации вот таких гарантий может быть очень много,
[01:12:22.200 --> 01:12:28.800]  просто потому что API файловая система очень большая. Там слишком много методов. Вот сравните это с API
[01:12:28.800 --> 01:12:35.680]  Executor. В Executor у вас admin.execute и больше ничего нет. Это очень удачная абстракция, она очень емкая,
[01:12:35.680 --> 01:12:41.040]  и в ней очень мало, то есть очень маленькая граница с пользователем. А здесь граница
[01:12:41.040 --> 01:12:47.040]  с пользователем гигантская. Тут там, не знаю, десятки уже методов, кажется. И неудивительно,
[01:12:47.040 --> 01:12:53.280]  что получаются очень сложные комбинации, которые могут возникать и которые работают не так,
[01:12:53.280 --> 01:13:01.640]  как мы ожидаем. Можно ли не использовать файловые системы? Ну в какой-то степени можно. Скажем,
[01:13:01.640 --> 01:13:09.240]  в Яндексе когда-то давно отдельные системы не использовали файловую систему, описали свое
[01:13:09.240 --> 01:13:18.800]  хранилище прямо поверх блок девайса. С одной стороны, это позволяло добиться какой-то
[01:13:18.800 --> 01:13:22.920]  эффективности дополнительной за счет того, что у вас меньше абстракций между вашей системой и
[01:13:22.920 --> 01:13:31.640]  хранилищем. А с другой стороны, была очень скверная ситуация, когда была допущена какая-то ошибка
[01:13:31.640 --> 01:13:40.040]  вот в этом слое программным, который обеспечивал доступ к блоковому устройству. И вот после ошибки
[01:13:40.040 --> 01:13:47.760]  в этом месте очень сложно понять, что с вашими данными происходит. Вот, скажем, Google File System,
[01:13:47.760 --> 01:13:52.160]  если я не ошибаюсь, они там явно пишут, что они хранят данные просто в виде файлов в файловой
[01:13:52.160 --> 01:13:57.480]  системе. Ну потому что для файловой системы очень много инструментов и легко понять,
[01:13:57.480 --> 01:14:03.160]  что случилось. Можно пойти на машину и из командной строки посмотреть какие чанки там есть,
[01:14:03.160 --> 01:14:07.920]  посчитать их чех суммы. Если вы работаете с блоковым устройством напрямую без файловой системы,
[01:14:07.920 --> 01:14:15.280]  то вы такой возможности решаетесь. Вам просто сложнее жить в случае проблем. Поэтому мне кажется,
[01:14:15.280 --> 01:14:23.200]  что так прямо сейчас делать не нужно. То есть все-таки нужно работать с файловой системой,
[01:14:23.600 --> 01:14:35.680]  но с файловой системой нужно работать аккуратно. Ну что ж, таков итог сегодняшнего семинара.
[01:14:35.680 --> 01:14:43.360]  Перезаписать кусочек файлов сложно, писать что-то в Write Ahead Log сложно и требует большой аккуратности.
[01:14:43.360 --> 01:14:52.680]  Ну, в наших задачах логи необходимы, в RSM их они необходимы. И напоследок я расскажу два
[01:14:52.680 --> 01:15:03.520]  мне кажется забавных, ну не то что наблюдения, а оптимизации. А именно, если вы храните,
[01:15:03.520 --> 01:15:11.280]  если вы пишете реплицированные кивали у хранилища, то мы это уже обсуждали, в нем каждый
[01:15:11.280 --> 01:15:20.760]  диапазон, в нем данные ваших гигантских таблиц бьются на эти диапазоны и каждый диапазон хранится
[01:15:20.760 --> 01:15:27.480]  в виде отдельного RSM на наборах реплик. И мы обсуждали позапрошлый раз, кажется,
[01:15:27.480 --> 01:15:34.800]  что вот эти диапазоны, они довольно маленькие, там какие-то десятки-сотни мегабайт, потому что их
[01:15:34.800 --> 01:15:42.920]  можно легко балансировать. Ну для того, чтобы их можно было легко балансировать. Так вот,
[01:15:42.920 --> 01:15:49.000]  получается, что на одной реплике у вас очень много разных RSM, и в этом случае будет очень
[01:15:49.000 --> 01:15:57.000]  накладно, очень неэффективно для каждого RSM, для каждого рейнджа иметь свой собственный
[01:15:57.000 --> 01:16:01.280]  лог на диске, потому что мы тогда будем делать очень-очень много параллельной записи на диск,
[01:16:01.280 --> 01:16:08.600]  он не сможет так эффективно работать. Поэтому как RoachDB, но любая современная подобная система,
[01:16:08.600 --> 01:16:17.440]  да и даже Bigtable на заре времен, если вы читали статью, то они пишут, что они не делают для каждого
[01:16:17.440 --> 01:16:25.080]  таблета отдельный лог. Они для таблетов, которые обслуживаются одной машиной, одним таблет-сервером,
[01:16:25.080 --> 01:16:34.840]  хранят все их логи в одном файле, просто для того, чтобы оптимизировать нагрузку. Так вот,
[01:16:34.840 --> 01:16:41.240]  можно пойти еще дальше. Я вам говорил в лекции про RSM как-то давным-давно, про систему Яндекс.ДБ,
[01:16:41.240 --> 01:16:49.800]  про то, что там вся система построена поверх RSM. Ну просто, грубо говоря, построена вокруг
[01:16:49.800 --> 01:16:55.720]  отказоустойчивых машин. Машин, которые не ломаются, потому что каждая такая машина виртуальная,
[01:16:55.720 --> 01:17:02.600]  но каждый такой актор — это отдельный реплицированный автомат. Так вот, в этой системе пошли еще дальше,
[01:17:02.600 --> 01:17:11.760]  потому что у них одни RSM-ы, и они решили, что... Сейчас найду подходящую картинку или нет.
[01:17:20.200 --> 01:17:26.040]  Вот, они решили, что они будут строить свою систему из этих RSM-ов поверх распределенного хранилища,
[01:17:26.040 --> 01:17:35.640]  которое, по сути, будет реализовывать хранилище для логов. То есть хранилище для логов RSM-ов — это целая
[01:17:35.640 --> 01:17:44.080]  отдельная подсистема. И уже поверх нее строятся вот такие отказоустойчивые реплицированные акторы.
[01:17:44.080 --> 01:17:55.040]  Вот эта система про имутабельные данные, а поверх этой подсистемы, которая хранит логи и снапшоты,
[01:17:55.040 --> 01:18:04.240]  строятся уже акторы RSM-ы, а из них выстраиваются уже в более сложные уровни. Ну и почему я сейчас об этом
[01:18:04.240 --> 01:18:09.120]  вспоминаю — потому что через неделю мы будем говорить про распределенные транзакции, и там у нас
[01:18:09.120 --> 01:18:15.800]  будет три примера — Bigtable, Spanner, ну и как раз Yandex.DB, как там устроены транзакции по типу кэрви.
[01:18:15.800 --> 01:18:24.640]  Ну а с логами из файловой системы, наверное, на сегодня все. Кажется, что вот таких чудес
[01:18:24.640 --> 01:18:31.200]  с нас хватит. Все, спасибо большое вам, тогда до встречи через неделю.

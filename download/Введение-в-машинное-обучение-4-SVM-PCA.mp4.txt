[00:00.000 --> 00:08.740]  Итак, сегодня мы с вами продолжаем беседу про
[00:08.740 --> 00:12.700]  линейные модели и поговорим про еще две любопытные линейные
[00:12.700 --> 00:13.700]  модели.
[00:13.700 --> 00:15.960]  Забавно, но почему-то вот эти два подхода регулярно
[00:15.960 --> 00:16.960]  люди путают.
[00:16.960 --> 00:19.040]  Не говорю про всех, но некоторые люди, приходя
[00:19.040 --> 00:21.680]  на беседу, могут перепут ПСЕ и СВМ, хотя по факту
[00:21.680 --> 00:24.760]  один мет топорных векторов, второй мет главных компонентов.
[00:24.760 --> 00:28.600]  Наверное, потому что по-русски звучит похоже, это путают.
[00:28.600 --> 00:30.480]  К тому сегодня сразу две задачи.
[00:30.480 --> 00:33.040]  Потому что, во-первых, эти методы во многом связаны
[00:33.040 --> 00:34.960]  со всеми теми, что мы разбирали с вами ранее.
[00:34.960 --> 00:37.240]  Во-вторых, на сегодняшнем занятии мы заканчиваем
[00:37.240 --> 00:39.720]  свое путешествие сквозь чисто линейные модели.
[00:39.720 --> 00:41.800]  Со следствием мы начнем уже говорить про деревья,
[00:41.800 --> 00:44.960]  то есть про нелинейное преобразование, про их ансамбли и постепенно
[00:44.960 --> 00:48.280]  подберемся к современным, относительно современным
[00:48.280 --> 00:50.080]  сначала, потом современным нейронным сетям.
[00:50.080 --> 00:53.480]  Опять-таки, сегодня у нас план очень простой.
[00:53.480 --> 00:55.560]  Во-первых, поговорить про то, что такое мет топорных
[00:55.560 --> 00:57.640]  векторов, никаких страшных двойственных задач и те
[00:57.640 --> 00:59.880]  ремы хороших кунтаккеров у нас сегодня не будет,
[00:59.880 --> 01:03.440]  хотя, по сути, именно оттуда все это дело было выведено
[01:03.440 --> 01:04.560]  и замечательно выводится.
[01:04.560 --> 01:06.960]  Я про это скажу лишь отчасти, потому что у кого-то из
[01:06.960 --> 01:09.280]  вас метоптов вообще еще не было, у кого-то из вас
[01:09.280 --> 01:12.240]  они были, но ККТ, кто сходу может сформулировать те
[01:12.240 --> 01:15.240]  ремы хороших кунтаккеров?
[01:15.240 --> 01:19.800]  Ну вот, стоит революции доказать.
[01:19.800 --> 01:23.400]  По идее, он на третьем курсе, метопты на фупне, да и на
[01:23.400 --> 01:25.200]  фифте, по-моему, читаются, раньше, по крайней мере,
[01:25.200 --> 01:26.200]  так было правильно.
[01:26.560 --> 01:31.400]  Ну ККТ должен быть в курсе метоптов, тут как минимум
[01:31.400 --> 01:34.000]  ПМФ четвертого курса, значит вы, по идее, должны знать
[01:34.000 --> 01:35.000]  что-то такое.
[01:35.000 --> 01:38.000]  Ну вот, пожалуйста.
[01:38.000 --> 01:40.000]  Ну, короче, не случилось.
[01:40.000 --> 01:41.840]  Ладно, иногда, на всякий случай, что такой метод может
[01:41.840 --> 01:43.840]  или лагранжа, по крайней мере, помните?
[01:43.840 --> 01:47.840]  Это в Матане там было, решение задач с ограничениями, я
[01:47.840 --> 01:48.840]  понял.
[01:48.840 --> 01:52.680]  Ну, я чуть-чуть про это напомню, но глобально мы в это
[01:52.680 --> 01:54.400]  с вами погружаться сегодня не будем, просто потому
[01:54.400 --> 01:57.200]  что, ну, это, грубо говоря, стезя других коллег.
[01:57.200 --> 02:00.680]  Ладно, поговорим с вами про СВМ, краски постараемся
[02:00.680 --> 02:02.520]  ее вывести исходя из здравого смысла.
[02:02.520 --> 02:05.040]  Сегодня нам краски понадобится вспомнить, что такое отступ
[02:05.040 --> 02:06.040]  он же margin.
[02:06.040 --> 02:07.040]  Все помнят?
[02:07.040 --> 02:10.040]  Кто не помнит, что такое отступ?
[02:10.040 --> 02:11.040]  Замечательно.
[02:11.040 --> 02:12.040]  Огонь.
[02:12.040 --> 02:14.200]  Я еще раз это проговорю, но практически вся аудитория
[02:14.200 --> 02:15.200]  не подняла рук.
[02:15.200 --> 02:17.800]  А потом вспомним с вами, что у нас есть не только
[02:17.800 --> 02:21.600]  задачи обучения с учителем, есть и без учителя, и поснижаем
[02:21.600 --> 02:22.880]  размер пространства.
[02:22.880 --> 02:25.160]  Заодно после сегодняшнего занятия у вас появится
[02:25.160 --> 02:26.640]  аж две домашки.
[02:26.640 --> 02:27.640]  Ужас.
[02:27.640 --> 02:30.520]  Во-первых, у вас появится третья домашка на автопроверку.
[02:30.520 --> 02:32.560]  Вот тут уже, как я говорил, предложили некоторые коллеги
[02:32.560 --> 02:34.080]  объюзить проверочную систему.
[02:34.080 --> 02:36.720]  Ну, в принципе, объюзьте, главное нам потом скажите,
[02:36.720 --> 02:37.720]  как вы ее поломали.
[02:37.720 --> 02:39.800]  Это в принципе дело крайне хорошее.
[02:39.800 --> 02:42.280]  А во-вторых, у вас, собственно, появится третья домашка,
[02:42.280 --> 02:43.280]  она будет на пару недель.
[02:43.280 --> 02:47.120]  Да, кстати, на всякий случай, так как многие оказались
[02:47.120 --> 02:50.080]  крайне нерастыробно, я думаю, все те, кто в аудитории,
[02:50.080 --> 02:53.160]  все слушатели, кто здесь, заполнили форму своевременно,
[02:53.160 --> 02:55.560]  но некоторые люди, вот сейчас я буквально перед лекцией
[02:55.560 --> 02:57.600]  докинул право всем тем, кто заполнил, еще двадцати
[02:57.600 --> 02:58.600]  человек.
[02:58.600 --> 03:01.360]  Но, тем не менее, коль уж такое дело, мы до пятого
[03:01.360 --> 03:03.200]  числа дедлайн продлим, чтобы все могли нормально
[03:03.200 --> 03:06.960]  первую домашку дослать, потому что, ну, людей жалко,
[03:06.960 --> 03:10.120]  скажем так, вот маленький реверанс в сторону слушателей.
[03:10.120 --> 03:12.800]  На всякий случай, это скорее исключение чем правило,
[03:12.800 --> 03:15.480]  то есть не надо, пожалуйста, каждый раз говорить, давайте
[03:15.480 --> 03:18.200]  продлим дедлайн, дедлайн известен второй до десятого.
[03:18.200 --> 03:22.400]  Все дедлайны обычно стоят на понедельник, в общем
[03:22.400 --> 03:25.120]  говоря, хорошо постоять на воскресенье, но практика
[03:25.120 --> 03:27.440]  показывает, что на понедельник как-то работает лучше то,
[03:27.440 --> 03:29.480]  что люди в ночь с воскресенья на понедельник любят что-то
[03:29.480 --> 03:30.960]  судорожно доделывать.
[03:30.960 --> 03:35.480]  Нам количество отправок растет очень значительно.
[03:35.480 --> 03:37.080]  Хорошо, так вот, почему две домашки?
[03:37.080 --> 03:38.960]  Вторая домашка, это будет, как раз, первая лабораторная
[03:38.960 --> 03:41.840]  работа, которую вы можете начать уже сейчас.
[03:41.840 --> 03:44.560]  Помните, кстати, в первой, да и во второй домашке обычной,
[03:44.560 --> 03:46.440]  у вас были всякие теории вопроса, было непонятно,
[03:46.440 --> 03:47.440]  куда их девать.
[03:47.560 --> 03:49.960]  На самом деле, сделано исключительно для вашего удобства, у вас
[03:49.960 --> 03:52.640]  в лабе будут места, куда вставить ответы на теории
[03:52.640 --> 03:53.640]  вопросов.
[03:53.640 --> 03:55.520]  По сути, они там будут те же самые.
[03:55.520 --> 03:57.520]  Мы по факту просто разнесли некоторые теории вопроса
[03:57.520 --> 04:00.080]  из лабы, потому что их надо проверять руками, по домашкам,
[04:00.080 --> 04:01.080]  чтобы вы заранее подумали.
[04:01.080 --> 04:02.080]  Только и всего.
[04:02.080 --> 04:05.000]  То есть, вам там придется их вставить, если вы их
[04:05.000 --> 04:08.280]  не отвечали, придется ответить, если отвечали, Control-C, Control-V,
[04:08.280 --> 04:09.280]  все замечательно.
[04:09.280 --> 04:12.120]  Так вот, первая лаба, она у вас будет уже аж на три,
[04:12.120 --> 04:14.920]  наверное, с лишним недели выдана, она достаточно объемная,
[04:14.920 --> 04:17.520]  она будет включать в себя кусочек по деревьям, то
[04:17.520 --> 04:19.720]  есть, по идее, после сегодняшнего занятия вы еще не целиком
[04:19.720 --> 04:21.920]  можете ее решить, но в ней там, по-моему, аж восемь
[04:21.920 --> 04:22.920]  шагов, поэтому решать ее можно.
[04:22.920 --> 04:23.920]  Да.
[04:23.920 --> 04:29.480]  Ну, это у нас скорее такое следствие.
[04:29.480 --> 04:31.520]  Она когда-то была на русском, в той переводе на английский
[04:31.520 --> 04:33.560]  стало гораздо понятнее, и ее так оставили.
[04:33.560 --> 04:34.560]  А?
[04:34.560 --> 04:35.560]  В среднем.
[04:35.560 --> 04:41.560]  Ну, смотрите, у вас иначе получается, воспользуйтесь
[04:41.560 --> 04:45.120]  методом вот таким-то, обучите модель такую-то, потому
[04:45.120 --> 04:47.800]  что используйте главные компоненты, но метод называется
[04:47.800 --> 04:52.240]  PCA, потому что Principle Component Analysis, тут все в тех же терминах
[04:52.240 --> 04:53.240]  используется.
[04:53.240 --> 04:54.240]  Вот.
[04:54.240 --> 04:56.680]  Ну и плюс, опять же, везде ссылки на доки, доки все
[04:56.680 --> 05:00.120]  на английском, ну, мы в IT-шке сидим, тут вроде как английский
[05:00.120 --> 05:01.120]  язык доминирует.
[05:01.120 --> 05:02.120]  Ладно, давайте начинать.
[05:02.120 --> 05:03.120]  Короче, что происходит?
[05:03.120 --> 05:05.280]  На всякий случай, вот еще маленькие такие связи
[05:05.280 --> 05:08.480]  будут с SVD, он же и сингулярное разложение.
[05:08.480 --> 05:15.680]  Сингулярное разложение кто-нибудь помнит?
[05:15.680 --> 05:16.680]  Хорошо.
[05:16.680 --> 05:20.320]  А разложение по собственным векторам?
[05:20.320 --> 05:25.040]  О, уже лучше, ну классно, тогда с ним краски и свяжемся.
[05:25.040 --> 05:27.920]  Ладно, давайте посмотрим на задачку линейной классикации,
[05:27.920 --> 05:29.680]  как она у нас была на прошлой неделе.
[05:29.680 --> 05:30.680]  И как она была вообще.
[05:30.680 --> 05:34.000]  У нас есть множество пар XY, X какой-то вектор в каком-то
[05:34.000 --> 05:35.000]  линейном пространстве.
[05:35.000 --> 05:38.120]  Опять же, все категориальные признаки в самом простом
[05:38.120 --> 05:41.360]  случае берем и ванхотом просто кодируем и все.
[05:41.360 --> 05:44.680]  То есть для каждого категориального признака с P значениями
[05:44.680 --> 05:48.360]  создаем P-мерный вектор, где на, короче, нужном месте
[05:48.360 --> 05:50.400]  стоит единичка все остальные нули.
[05:50.400 --> 05:53.000]  И, соответственно, пусть у нас бинарная классикация,
[05:53.000 --> 05:56.120]  тогда у нас Y принадлежит плюс-минус один, например.
[05:56.120 --> 05:58.640]  Опять же, повторюсь, как вы помните, можно абсолютно
[05:58.640 --> 06:02.320]  его переобозначить красный, зеленый, один-ноль, котики-собачки,
[06:02.320 --> 06:03.320]  как угодно.
[06:03.320 --> 06:05.680]  Котики, реальные величины и все.
[06:05.680 --> 06:08.080]  Мы строим модель линейной классикации, которая выглядит
[06:08.080 --> 06:09.080]  следующим образом.
[06:09.080 --> 06:12.040]  Наш классикатор, пусть он будет A от X, плюс у нас
[06:12.040 --> 06:14.240]  есть параметр омега, веса и омега ноль свободных
[06:14.240 --> 06:15.240]  членов.
[06:15.240 --> 06:16.240]  Это, краски, знак вот этой штуки.
[06:16.240 --> 06:19.600]  На всякий случай можете сразу обратить внимание,
[06:19.600 --> 06:22.520]  что, погодите, омега X плюс B, а у нас тут минус омега
[06:22.520 --> 06:23.520]  ноль, то есть минус B.
[06:23.520 --> 06:24.520]  Внимание, вопрос.
[06:24.520 --> 06:28.520]  Имеет ли это какую-то разницу?
[06:28.520 --> 06:31.040]  Никакой разницы нет, потому что омега все равно выучивается.
[06:31.040 --> 06:33.280]  Если у вас минус омега, значит вы будете просто выучивать
[06:33.280 --> 06:35.240]  то же самое, но с отрицательным знаком.
[06:35.240 --> 06:37.240]  Все, ничего не меняется.
[06:37.240 --> 06:39.920]  И, соответственно, наша функция потерь, именно в
[06:39.920 --> 06:42.240]  общем случае, в классикации она всегда лишь одна.
[06:42.240 --> 06:44.760]  Мы просто можем сказать, правильно или неправильно.
[06:44.760 --> 06:47.160]  Поэтому наш эмпирический риск – это количество ошибок
[06:47.160 --> 06:48.160]  классикации.
[06:48.160 --> 06:50.320]  Сколько предсказаний не совпадает с истинной меткой
[06:50.320 --> 06:51.320]  класса.
[06:51.320 --> 06:54.240]  Или что то же самое, на скольких объектах у нас отступ отрицательный.
[06:54.240 --> 06:55.240]  Хорошо?
[06:55.240 --> 06:56.240]  Все.
[06:56.240 --> 07:00.120]  Сколько объектов у нас получили отрицательный отступ,
[07:00.120 --> 07:02.120]  значит не в чужом классе, значит все плохо.
[07:02.120 --> 07:04.560]  Ну и, соответственно, про отступ мы с вами помним,
[07:04.560 --> 07:10.160]  что это по сути проекция краски вектора на наш вектор,
[07:10.160 --> 07:13.280]  который указывает на объект на нормальной плоскости.
[07:13.280 --> 07:15.680]  Вот здесь краска минус омега 0 позволяет нам перетащить
[07:15.680 --> 07:18.480]  все это дело в ноль, поэтому все спокойно.
[07:18.480 --> 07:21.040]  На у, чтобы мы учитывали, по сути, в глубине своего
[07:21.040 --> 07:22.200]  класса или чужого.
[07:22.200 --> 07:24.360]  Именно поэтому мы здесь учитываем, грубо говоря,
[07:24.360 --> 07:26.000]  плюс-минус один, нам так просто удобнее.
[07:26.000 --> 07:28.360]  Потому что у нас тогда знак, если сонаправленный, будет
[07:28.360 --> 07:30.760]  положительный, если разнонаправленный, будет отрицательный.
[07:30.760 --> 07:34.160]  Хорошо, с этим все понятно, правильно?
[07:34.160 --> 07:36.200]  Класс, не едем дальше.
[07:36.200 --> 07:37.200]  А теперь давайте подумаем.
[07:37.200 --> 07:40.460]  Вот у нас с вами задача кружочки против квадратиков
[07:40.460 --> 07:43.360]  и вот вам три гиперплоскости, к которым можно провести
[07:43.360 --> 07:44.600]  результат одинаковый.
[07:44.600 --> 07:45.900]  Ошибок ноль.
[07:45.900 --> 07:47.160]  Внимание, вопрос.
[07:47.160 --> 07:50.160]  Какая плоскость лучше?
[07:50.160 --> 07:51.880]  Вы выбираете вторую.
[07:51.880 --> 07:54.760]  Вы это интуитивно сделали на основании каких предположений?
[07:54.760 --> 08:01.520]  Максимальная уверенность во всех, чтобы было симметрично.
[08:01.520 --> 08:02.520]  Классно.
[08:02.520 --> 08:05.520]  Еще какие предложения?
[08:05.520 --> 08:09.240]  Через ноль проходит тоже, на самом деле, классно.
[08:09.240 --> 08:10.680]  Максимальная удаленность от всех.
[08:10.680 --> 08:11.680]  Классно.
[08:11.680 --> 08:14.000]  То есть интуитивно многие из вас понимают, что вторая
[08:14.000 --> 08:15.000]  как-то получше.
[08:15.000 --> 08:18.680]  Здесь, на самом деле, можно опять же вернуться к такому
[08:18.680 --> 08:19.680]  определению.
[08:19.680 --> 08:21.200]  Оно не совсем формальное, но тем не менее мы его с
[08:21.200 --> 08:23.840]  вами обсуждали, кажется, две недели назад.
[08:23.920 --> 08:24.920]  Устойчивость модели.
[08:24.920 --> 08:25.920]  Помните?
[08:25.920 --> 08:28.720]  У нас гиперплоскость определяется вектором нормали и свободным
[08:28.720 --> 08:29.720]  членам.
[08:29.720 --> 08:32.360]  Мы говорим, что модель является устойчивой, если
[08:32.360 --> 08:36.160]  малое изменение обучающей выборке не приводит к значительному
[08:36.160 --> 08:37.160]  изменению параметров.
[08:37.160 --> 08:42.160]  В данном случае или предсказание модели, что тоже так бывает.
[08:42.160 --> 08:45.200]  Смотрите, если у нас вот эта модель, вот L1, если
[08:45.200 --> 08:47.840]  ее чуть-чуть подвигать буквально на, еще не знаю,
[08:47.840 --> 08:50.160]  мэпсил малую углу, она, грубо говоря, может начать
[08:50.160 --> 08:53.080]  захватывать кружочек с другой стороны или квадрат
[08:53.080 --> 08:54.080]  опять же с другой стороны.
[08:54.080 --> 08:57.440]  Получается, что при небольшом изменении параметров модели
[08:57.440 --> 08:59.480]  у нас с вами резко меняется предсказание модели на
[08:59.480 --> 09:00.480]  некоторых объектах.
[09:00.480 --> 09:03.480]  Получается, у нас некоторые объекты неуверенно классифицируются,
[09:03.480 --> 09:04.480]  о чем вы как раз тоже говорили.
[09:04.480 --> 09:07.320]  С L2 такой проблемы нет, с L3 опять такая же проблема
[09:07.320 --> 09:08.320]  есть.
[09:08.320 --> 09:11.200]  Но мы это с вами описали вроде как словами.
[09:11.200 --> 09:12.200]  Как это формализовать?
[09:12.200 --> 09:13.200]  У вас был вопрос?
[09:13.200 --> 09:14.200]  Нет?
[09:14.200 --> 09:15.200]  Хорошо.
[09:15.200 --> 09:16.200]  Как это формализовать?
[09:16.200 --> 09:18.640]  Мы можем как раз-таки об этом вспомнить ровно в
[09:18.640 --> 09:21.360]  терминах отступа, о котором мы с вами говорили.
[09:21.360 --> 09:25.560]  Давайте выбирать такую гиперплоскость, которая
[09:25.560 --> 09:29.160]  максимизирует отступ до всех точек.
[09:29.160 --> 09:32.520]  Более того, опять давайте сходим назад на один шужочек.
[09:32.520 --> 09:34.920]  Мы вычли свободный член, поэтому наша гиперплоскость
[09:34.920 --> 09:35.920]  проходит через ноль.
[09:35.920 --> 09:36.920]  Договорились?
[09:36.920 --> 09:39.280]  Просто для удобства нам так гораздо проще будет.
[09:39.280 --> 09:40.560]  Теперь внимание вопрос.
[09:40.560 --> 09:43.880]  Если у нас с вами гиперплоскость проходит через ноль, скажите,
[09:43.880 --> 09:48.760]  мы можем с вами утверждать, сделать вот такое утверждение,
[09:48.760 --> 09:51.120]  если выбракой линии наразделим, выдвинуть такое утверждение.
[09:51.440 --> 09:54.520]  Что для всех объектов одного класса отступ больше
[09:54.520 --> 09:57.080]  или равен единице, для всех объектов другого класса
[09:57.080 --> 09:58.720]  отступ меньше или равен минус единице.
[09:58.720 --> 10:03.120]  Ой, простите, не отступ, а именно вот это нормирование,
[10:03.120 --> 10:06.520]  как скажем так, расстояние со знаком, короче, глубина
[10:06.520 --> 10:07.520]  в классе.
[10:07.520 --> 10:11.320]  Понятно, почему здесь именно плюс-минус один, а не плюс-минус
[10:11.320 --> 10:12.320]  ноль-пять.
[10:12.320 --> 10:17.440]  Ну смотрите, еще раз, омега ноль, во-первых, здесь у
[10:17.440 --> 10:21.040]  нас омега ноль, и омега обучается, правильно?
[10:21.600 --> 10:25.480]  По сути, вы, используя свои значения параметров,
[10:25.480 --> 10:28.560]  можете таким образом перенормировать омега, чтобы всегда выполнялась
[10:28.560 --> 10:31.120]  вот это неравенство, если у вас выбрака линиейно-разделима.
[10:31.120 --> 10:32.640]  Что происходит?
[10:32.640 --> 10:35.480]  Вы, по сути, говорите, вот у меня разделяющая гиперплоскость,
[10:35.480 --> 10:38.760]  вот это расстояние теперь должно быть равно единице.
[10:38.760 --> 10:40.560]  Что такое это расстояние?
[10:40.560 --> 10:41.560]  Это омега и икс.
[10:41.560 --> 10:44.440]  Все, если вы перенормируете омегу, потому что до икса
[10:44.440 --> 10:46.560]  вы не можете нормируй все-таки это пространство, не надо
[10:46.560 --> 10:49.320]  менять просто так значение признаков, в омегу можете
[10:49.320 --> 10:51.560]  перенормировать так, чтобы неравенство выполнялось.
[10:51.560 --> 10:52.560]  Согласны?
[10:52.560 --> 10:53.560]  Теперь.
[10:53.560 --> 10:55.000]  Опять же, мы это делаем чисто для удобства, потому
[10:55.000 --> 10:57.320]  что плюс-минус один, ну, с ним понятия.
[10:57.320 --> 10:59.560]  По факту я здесь могу написать плюс-минус 100, и ничего не
[10:59.560 --> 11:00.560]  поменяется.
[11:00.560 --> 11:01.560]  Да?
[11:01.560 --> 11:04.280]  Сейчас увидите.
[11:04.280 --> 11:07.160]  Вообще говоря, один, у единицы есть как минимум одно классное
[11:07.160 --> 11:10.080]  свойство, она ограничивает сверху-снизу всякие наши
[11:10.080 --> 11:11.080]  периодические функции.
[11:11.080 --> 11:12.080]  Вот.
[11:12.080 --> 11:13.080]  Хорошо.
[11:13.080 --> 11:16.200]  Собственно, мы с вами можем это выписать, правильно?
[11:16.200 --> 11:18.200]  С этими двумя неравенствами теперь все согласны.
[11:18.200 --> 11:19.200]  Хорошо.
[11:19.200 --> 11:20.200]  Ну и едем дальше, соответственно.
[11:20.200 --> 11:23.320]  Вот опять они у нас переписаны, и мы теперь хотим сделать
[11:23.320 --> 11:24.320]  что?
[11:24.320 --> 11:27.320]  Мы с вами говорим, что у нас таким образом все выполняется.
[11:27.320 --> 11:31.320]  Давайте-ка теперь попробуем из этого поставить задачу
[11:31.320 --> 11:33.320]  максимизации какую-то, или минимизации, короче,
[11:33.320 --> 11:35.320]  оптимизационную задачу прямо отсюда вытащить.
[11:35.320 --> 11:36.320]  Каким образом?
[11:36.320 --> 11:39.400]  Ну, мы с вами можем посмотреть сюда, домножить второй
[11:39.400 --> 11:42.760]  неравенствов допустим на минус один, и что у нас тогда
[11:42.760 --> 11:43.760]  получается?
[11:43.760 --> 11:47.480]  Омега х минус, минус один на омега х минус минус
[11:47.480 --> 11:51.480]  омега ноль, больше или равен опять же единице, правильно?
[11:51.480 --> 11:54.480]  И если вы внимательно посмотрите, если сюда домножить на
[11:54.480 --> 11:57.480]  минус единицу, то это чистой воды будет отступ для отрицательного
[11:57.480 --> 11:58.480]  класса.
[11:58.480 --> 12:00.480]  А это чистая вода-оступ для положительного, потому
[12:00.480 --> 12:01.480]  что у него плюс один.
[12:01.480 --> 12:02.480]  Верно?
[12:02.480 --> 12:06.480]  Ну, если у нас два неравенства типа большего, мы их можем
[12:06.480 --> 12:07.480]  сложить.
[12:07.480 --> 12:09.480]  Левая часть, сумма левых частей все равно, большая равна
[12:09.480 --> 12:10.480]  сумме правых частей.
[12:10.480 --> 12:11.480]  Вот мы с вами можем сделать это.
[12:12.200 --> 12:15.200]  Вот мы с вами ее, собственно, и получаем.
[12:15.200 --> 12:19.200]  Х плюс минус х минус, у нас эта штука симметричная,
[12:19.200 --> 12:20.200]  все нормально.
[12:20.200 --> 12:23.200]  Краски, больше ли равен, чем два, здесь норма омеги.
[12:23.200 --> 12:28.200]  Откуда здесь взяла норму омеги, скажите мне, пожалуйста.
[12:28.200 --> 12:30.200]  Смотрите, мы с вами только что сказали, что мы можем
[12:30.200 --> 12:32.200]  с вами перенормировать омегу так, чтобы у нас выполнялась
[12:32.200 --> 12:33.200]  неравенство, правильно?
[12:33.200 --> 12:36.680]  Поэтому мы можем сказать, что х плюс минус х минус
[12:36.680 --> 12:39.880]  умноженное на омега скалярно, минус омега ноль, и опять
[12:40.280 --> 12:43.280]  ж плюс омега ноль, они поэтому и потерялись, свободные
[12:43.280 --> 12:44.280]  селены.
[12:44.280 --> 12:45.280]  Больше ли равно двум, согласны?
[12:45.280 --> 12:48.280]  Но здесь у нас эта неравенство зависит от обеих.
[12:48.280 --> 12:51.280]  Давайте опять перенормируем ее теперь обратно в единичный
[12:51.280 --> 12:52.280]  вектор.
[12:52.280 --> 12:53.280]  Каким образом?
[12:53.280 --> 12:54.280]  Поделимся на ее норму.
[12:54.280 --> 12:55.280]  Все.
[12:55.280 --> 12:57.280]  Поэтому теперь у нас омега делить на норму омеги, это
[12:57.280 --> 12:59.280]  единичный вектор, направленный в сторону нормали.
[12:59.280 --> 13:00.280]  И все в порядке.
[13:00.280 --> 13:03.280]  И получается, что у нас вот эта штука всегда больше
[13:03.280 --> 13:05.280]  равна, чем два, делить на норму омеги.
[13:05.280 --> 13:06.280]  Согласны?
[13:06.280 --> 13:09.280]  А теперь внимательно посмотрите, что такое х плюс минус
[13:09.680 --> 13:10.680]  х минус?
[13:10.680 --> 13:17.680]  У нас один вектор, второй вектор, и, соответственно,
[13:17.680 --> 13:21.680]  мы вычили их и проецировали на вектор нормали.
[13:21.680 --> 13:25.480]  Получается, это расстояние по нормали, кратко, между
[13:25.480 --> 13:28.680]  ближайшим объектом х плюс и х минус.
[13:28.680 --> 13:29.680]  Верно?
[13:29.680 --> 13:30.680]  Ну вот оно у нас нарисовано.
[13:30.680 --> 13:31.680]  Вот у нас два вектора.
[13:31.680 --> 13:35.680]  Вот х плюс минус х минус, проецированный на направлении
[13:35.680 --> 13:36.680]  нормали.
[13:36.680 --> 13:37.680]  Согласны?
[13:38.080 --> 13:39.080]  Ну получается что?
[13:39.080 --> 13:42.480]  Мы здесь, по сути, утверждаем, что вот эта штука является
[13:42.480 --> 13:46.480]  верхней оценкой на двойку всегда, и делим на норму
[13:46.480 --> 13:48.080]  омеги краски, чтобы это вообще не зависело от омеги
[13:48.080 --> 13:49.080]  и зависело только от наших точек.
[13:49.080 --> 13:53.080]  Все, пожалуйста, вы придумали себе новую оптимационную
[13:53.080 --> 13:54.080]  задачу.
[13:54.080 --> 13:56.380]  Теперь мы говорим, давайте у нас для всех наших точек
[13:56.380 --> 13:58.480]  будет выполняться вот эта пара неравенств.
[13:58.480 --> 14:00.680]  По сути, мы говорим, что отступ всегда больше равен
[14:00.680 --> 14:01.680]  днице.
[14:01.680 --> 14:02.680]  Хорошо.
[14:02.680 --> 14:06.480]  Тогда, максимизируя 2 делить на норму омеги, мы будем
[14:06.480 --> 14:08.880]  максимизировать зазор между классами.
[14:08.880 --> 14:11.880]  Потому что зазор — это как раз таки оценка снизу
[14:11.880 --> 14:13.880]  на расстоянии между ближайшими точками.
[14:13.880 --> 14:14.880]  Согласны?
[14:14.880 --> 14:17.880]  Все, мы с вами только что придумали себе новую оптимизационную
[14:17.880 --> 14:18.880]  задачу.
[14:18.880 --> 14:19.880]  Да?
[14:23.880 --> 14:25.880]  Да, омега 0 у нас явно прямо уходит при вычитании.
[14:25.880 --> 14:26.880]  Смотрите.
[14:26.880 --> 14:32.680]  Омега х минус омега 0 минус омега х минус минус минус
[14:32.680 --> 14:33.680]  плюс омега 0.
[14:33.680 --> 14:34.680]  Все, она ушла вообще.
[14:36.680 --> 14:37.680]  Согласны?
[14:37.680 --> 14:38.680]  Все.
[14:38.680 --> 14:41.180]  И, соответственно, получается, что это у нас оценка сверх.
[14:41.180 --> 14:42.480]  А теперь получается что?
[14:42.480 --> 14:47.380]  Если наша выборка линейно-разделима, то среди всех точек, простите,
[14:47.380 --> 14:50.740]  среди всех гиперплоскостей, которые у нас есть, нас
[14:50.740 --> 14:54.800]  удовлетворяет та гиперплоскость, которая доставляет максимум
[14:54.800 --> 14:56.280]  вот этого функционала.
[14:56.280 --> 14:57.280]  Согласны?
[14:57.280 --> 15:00.680]  А это вам, кстати, сразу овтоп, но сразу задам вопрос.
[15:00.680 --> 15:04.280]  Это вам ничего не напоминает?
[15:04.280 --> 15:05.680]  Норма у нас здесь какая, во-первых?
[15:05.880 --> 15:08.480]  Если у нас просто обычный скалярный произведение,
[15:08.480 --> 15:09.480]  какая здесь норма?
[15:09.480 --> 15:10.480]  Евклидова, правильно?
[15:10.480 --> 15:11.480]  Вторая норма.
[15:11.480 --> 15:15.080]  Два делеть на вторую норму нужно максимизировать.
[15:15.080 --> 15:19.680]  Если мы максимизируем один делеть на вторую норму,
[15:19.680 --> 15:23.080]  значит мы минимизируем что?
[15:23.080 --> 15:24.080]  Вторую норму.
[15:24.080 --> 15:25.080]  Согласны?
[15:25.080 --> 15:28.480]  Это то же самое ограничение, что у нас было раньше.
[15:28.480 --> 15:30.520]  Мы с вами только что обнаружили, что л2 регуляризация
[15:30.520 --> 15:32.120]  распширяет нам разделяющую полосу.
[15:32.120 --> 15:36.520]  Это так, к слову.
[15:36.520 --> 15:37.520]  Хорошо.
[15:37.520 --> 15:39.880]  Вот мы с вами его придумали, и вот, по сути, у нас с вами
[15:39.880 --> 15:42.320]  теперь решается следующая задача.
[15:42.320 --> 15:45.920]  По сути, для задачи, где у нас выборки линейно разделимы,
[15:45.920 --> 15:47.640]  то есть у нас нет никакого шума, мы можем найти какую-то
[15:47.640 --> 15:50.400]  гиперплоскость, которая подходит, мы решаем с вами
[15:50.400 --> 15:51.600]  следующую задачу.
[15:51.600 --> 15:54.360]  Необходимо у нас есть ограничение, чтобы все отступы были
[15:54.360 --> 15:59.440]  больше или равны единице, и при этом мы должны минимизировать
[15:59.600 --> 16:00.600]  вторую норму векторовисов.
[16:00.600 --> 16:03.480]  Ну, тут одна вторая, но, как вы понимаете, константа
[16:03.480 --> 16:04.480]  никоим образом не влияет.
[16:04.480 --> 16:05.480]  Все.
[16:05.480 --> 16:06.480]  Вот раз.
[16:06.480 --> 16:09.080]  Возникает сразу два вопроса.
[16:09.080 --> 16:13.480]  Что делать, если у нас с вами задача нелинейно
[16:13.480 --> 16:15.360]  разделимая, у нас правило шум есть, поэтому мы не
[16:15.360 --> 16:17.240]  можем прямую провести так, чтобы у нас не было ошибок
[16:17.240 --> 16:18.240]  или гиперплоскости.
[16:18.240 --> 16:21.280]  И второе, а что делать с двойственной задачей?
[16:21.280 --> 16:23.400]  Тут краски можно вспомнить метапты, что мы умеем решать
[16:23.400 --> 16:26.360]  двойственные задачи и так далее, но что-то ККТ
[16:26.360 --> 16:28.560]  вызвало некоторые вопросы у аудитории, поэтому будем
[16:28.560 --> 16:31.000]  решать по старинке методом градиентного спуска.
[16:31.000 --> 16:32.000]  Так тоже можно.
[16:32.000 --> 16:33.000]  Хорошо?
[16:33.000 --> 16:34.000]  Собственно, давайте.
[16:34.000 --> 16:36.280]  Давайте начнем с первого вопроса, что делать, если
[16:36.280 --> 16:37.680]  выборка нелинейно разделима.
[16:37.680 --> 16:42.920]  Может кто-то из вас сразу может предложить, что делать?
[16:42.920 --> 16:43.920]  Проигнорируем.
[16:43.920 --> 16:44.920]  Хорошо.
[16:44.920 --> 16:47.440]  Но как-то штраф вести.
[16:47.440 --> 16:50.440]  А в какой штраф?
[16:50.440 --> 16:54.560]  Ну вот, хорошая идея от вашего коллеги как раз.
[16:54.560 --> 16:57.880]  А давайте за каждую пару, даже не за каждую пару,
[16:57.880 --> 17:00.600]  за каждый объект, который нарушает вот это ограничение,
[17:00.600 --> 17:03.040]  будем добавлять какой-то штраф нашей модели.
[17:03.040 --> 17:04.040]  Классно.
[17:04.040 --> 17:06.040]  Давайте ему краски введем.
[17:06.040 --> 17:08.600]  Пусть у нас теперь ограничения отлабляются, и теперь у нас
[17:08.600 --> 17:11.960]  маржин для каждого объекта, больший равен 1 минус кси
[17:11.960 --> 17:12.960]  ит.
[17:12.960 --> 17:15.200]  Кси ита это такая свободная переменная, латентная,
[17:15.200 --> 17:17.800]  к которой мы просто ввели, она ничему реально не соответствует.
[17:17.800 --> 17:19.680]  Мы говорим, что для каждого объекта теперь существует
[17:19.680 --> 17:20.680]  некоторая ксишка.
[17:20.680 --> 17:22.880]  Ит объект, ит иксишка.
[17:22.880 --> 17:26.240]  И соответственно, мы говорим, что кси иты всегда положительны,
[17:26.320 --> 17:29.440]  и теперь мы будем решать соответственно две вещи.
[17:29.440 --> 17:34.360]  У нас есть первая штуковина, собственно, первое ограничение,
[17:34.360 --> 17:37.280]  что у нас маржин больше или равен 1 минус кси итой.
[17:37.280 --> 17:40.040]  Второй, что все кси больше или равны нулю, но потому
[17:40.040 --> 17:42.800]  что иначе бред какой-то, мы увеличиваем маржин,
[17:42.800 --> 17:44.160]  зачем-то в ограничении нам это не надо.
[17:44.160 --> 17:48.360]  И собственно теперь мы добавляем сумму всех этих наших штрафов
[17:48.360 --> 17:50.480]  в наш оптимизируемый функционал.
[17:50.480 --> 17:55.760]  Понятно, что произошло?
[17:55.760 --> 17:57.920]  Логично взять нелинейную, в простейшем случае это
[17:57.920 --> 17:58.920]  линейная зависимость.
[17:58.920 --> 18:01.840]  То есть здесь вы на самом деле можете какую-то и другую
[18:01.840 --> 18:04.080]  ввести, и это приведет вас к другому решению, но
[18:04.080 --> 18:06.160]  как правило, смотрите, что такое маржин, это насколько
[18:06.160 --> 18:09.240]  глубоко вы находитесь внутри своего класса.
[18:09.240 --> 18:12.320]  По сути мы говорим, вот на самом деле мы меньше, чем
[18:12.320 --> 18:14.520]  на единицах глубине своего класса, например, оно вообще
[18:14.520 --> 18:17.240]  отрицательное, мы в чужом классе сидим, но расстояние
[18:17.240 --> 18:20.640]  это у нас линейно суммируется в данном случае, поэтому
[18:20.640 --> 18:22.520]  логично взять линейную, почему оно по квадрату
[18:22.520 --> 18:24.480]  должно расти или по любому другому.
[18:24.480 --> 18:31.840]  Но норма-то у нас краски в квадрате, но у нас это
[18:31.840 --> 18:35.200]  сумма квадрата под квадратным корнем, степень все равно
[18:35.200 --> 18:36.200]  первая.
[18:36.200 --> 18:37.200]  Тут степень тоже первая.
[18:37.200 --> 18:38.200]  Хорошо?
[18:38.200 --> 18:39.200]  Окей.
[18:39.200 --> 18:40.200]  Смотрите.
[18:40.200 --> 18:43.880]  И получается вот наша теперь с вами оригинальная
[18:43.880 --> 18:44.880]  задача.
[18:44.880 --> 18:46.520]  У нас с вами есть, собственно, оригинальная оптимизированная
[18:46.520 --> 18:50.960]  задача, плюс все наши ограничения, которые породили нам штрафы,
[18:50.960 --> 18:52.600]  это надо отправить опять джинаминием.
[18:52.600 --> 18:54.440]  Теперь еще и по всем ксишкам.
[18:54.440 --> 18:57.000]  Но решать задачу аж двумя ограничениями, по сути у
[18:57.000 --> 19:01.120]  нас с вами есть краски 1 и 2 ограничения, не жесткие.
[19:01.120 --> 19:04.560]  Мы можем это все перевести в задачу, скажем так, безусловной
[19:04.560 --> 19:05.560]  оптимизации.
[19:05.560 --> 19:07.320]  Краски вспомним, как там двоистные задачи решаются
[19:07.320 --> 19:08.320]  и так далее.
[19:08.320 --> 19:09.320]  Смотрите.
[19:09.320 --> 19:10.320]  Что происходит?
[19:10.320 --> 19:13.480]  Вот у нас с вами оригинальный оптимизируемый функционал,
[19:13.480 --> 19:14.480]  правильно?
[19:14.480 --> 19:17.160]  У нас одна вторая норма омеге в квадрате, с ней мы ничего
[19:17.160 --> 19:19.400]  сделать не можем, она существует, она отвечает за ширину
[19:19.400 --> 19:20.400]  полосы.
[19:20.400 --> 19:22.440]  Все, отложили, она существует.
[19:22.440 --> 19:23.440]  Дальше.
[19:23.440 --> 19:26.400]  С на сумму всех ксишек, правильно?
[19:26.400 --> 19:28.000]  Что такое сумма всех ксишек?
[19:28.000 --> 19:30.040]  На самом деле, что такое кси?
[19:30.040 --> 19:33.640]  Ну, кси у нас можно перекинуть сюда, это сюда.
[19:33.640 --> 19:35.760]  Кси у нас всегда больше или равна, чем один минус
[19:35.760 --> 19:36.760]  марджин.
[19:36.760 --> 19:37.760]  Согласны?
[19:37.760 --> 19:39.840]  И причем мы помним, что кси только больше или равна
[19:39.840 --> 19:40.840]  нулю.
[19:40.840 --> 19:42.240]  Если кси меньше нуля, то она нас не интересует.
[19:42.240 --> 19:43.240]  Все.
[19:43.240 --> 19:47.240]  У нас не должно быть кси меньше нуля, потому что
[19:47.240 --> 19:48.240]  иначе это...
[19:48.240 --> 19:51.280]  Ну короче, если кси меньше нуля, это значит, что у нас
[19:51.280 --> 19:54.600]  марджин еще более, еще больше, чем единица.
[19:54.600 --> 19:56.480]  А нам не надо, чтобы он был больше единицы, мы на
[19:56.480 --> 19:57.480]  это ограничение накладываем.
[19:57.480 --> 20:00.320]  Ну, получается что?
[20:00.320 --> 20:02.800]  Получается у нас вот это неравенство, можно тем
[20:02.800 --> 20:03.800]  образом сказать.
[20:03.800 --> 20:07.080]  У нас кси всегда больше или равна один минус марджин,
[20:07.080 --> 20:10.320]  но при этом мы с вами помним, что краски марджин больше
[20:10.320 --> 20:12.040]  единицы нас вообще не интересует.
[20:12.040 --> 20:13.040]  Правильно?
[20:13.040 --> 20:14.040]  Хорошо.
[20:14.040 --> 20:17.040]  Ну и тогда мы с вами можем переписать в каком виде.
[20:17.280 --> 20:20.600]  Когда сумма ксишек, она у нас больше или равна чем
[20:20.600 --> 20:21.600]  что?
[20:21.600 --> 20:24.080]  Чем сумма один минус марджин?
[20:24.080 --> 20:25.080]  Правильно?
[20:25.080 --> 20:28.640]  И эта штука всегда должна быть не отрицательна.
[20:28.640 --> 20:33.840]  Потому что если она отрицательна, значит один минус марджин
[20:33.840 --> 20:34.840]  получился каким?
[20:34.840 --> 20:35.840]  Простите.
[20:35.840 --> 20:37.520]  Раз, два, три, четыре, пять.
[20:37.520 --> 20:40.760]  Если она отрицательна, то один минус марджин получился
[20:40.760 --> 20:41.760]  как раз таки...
[20:41.760 --> 20:42.760]  Господи.
[20:42.760 --> 20:46.160]  Отрицательным, значит у нас все отрицательны, чего
[20:46.160 --> 20:47.160]  она не должна.
[20:47.160 --> 20:48.160]  Вот.
[20:48.160 --> 20:50.720]  Короче, мы с вами из двух ограничений, по сути, переформулируем
[20:50.720 --> 20:53.240]  вот эту часть уже в безусловной задаче оптимизации.
[20:53.240 --> 20:57.640]  Один минус марджин, по сути, это верхняя нижняя оценка
[20:57.640 --> 21:01.480]  на кси, куда пропала, и берем только положительную
[21:01.480 --> 21:02.480]  часть.
[21:02.480 --> 21:03.480]  То есть если она отрицательна, она равна нулю.
[21:03.480 --> 21:05.480]  Если положительная, то она равна тому, что есть.
[21:05.480 --> 21:06.480]  Да?
[21:06.480 --> 21:09.480]  Зачем нам ограничивать кси с нижним нулем?
[21:09.480 --> 21:12.480]  Если все отрицательные, то это означает, что у нас
[21:12.480 --> 21:14.480]  же как-то большая уверенность в запрете?
[21:14.480 --> 21:17.400]  Да, но нам не нужна большая уверенность, это ограничение.
[21:17.400 --> 21:18.760]  Смотрите, это не исходная задача, это ограничение
[21:18.760 --> 21:19.760]  на задачу.
[21:19.760 --> 21:23.480]  Мы говорим, что наш марджин должен быть не меньше, чем
[21:23.480 --> 21:24.480]  1 минус кси.
[21:24.480 --> 21:29.400]  А сверху, если марджин больше единицы, мы точно довольны.
[21:29.400 --> 21:31.440]  Поэтому нам кси отрицательны, смысла не имеют в том, что
[21:31.440 --> 21:34.320]  иначе мы получим марджин ограничен снизу еще большим,
[21:34.320 --> 21:35.320]  чем единицей.
[21:35.320 --> 21:37.960]  У нас для самых близких объектов марджин должен быть равен
[21:37.960 --> 21:41.960]  единице в разделимой задаче, а в неразделимой единице
[21:41.960 --> 21:42.960]  минус кси.
[21:42.960 --> 21:43.960]  Все.
[21:44.960 --> 21:45.960]  Уловили?
[21:45.960 --> 21:46.960]  Нет?
[21:46.960 --> 21:51.360]  А с – это, как раз таки, некоторая константа, которая
[21:51.360 --> 21:55.160]  учитывает, насколько у нас ширина полосы по важности
[21:55.160 --> 22:00.000]  соотносится с нашими нарушенными ограничениями на марджин.
[22:00.000 --> 22:01.000]  С – это гиперпараметр.
[22:01.000 --> 22:04.400]  Ну и теперь вы можете, по сути, увидеть итоговую
[22:04.400 --> 22:05.400]  модель.
[22:05.400 --> 22:09.360]  У нас с на сумму 1 минус марджин положительный, плюс 1
[22:09.360 --> 22:10.960]  вторая норма омеги.
[22:10.960 --> 22:13.480]  Ну, допустим, в квадрате в данном случае, пожалуйста.
[22:13.480 --> 22:15.720]  Опять же, откуда взялся квадрат, на всякий случай?
[22:15.720 --> 22:18.520]  Вот тут у нас вроде квадрата не было, потом у нас квадрат
[22:18.520 --> 22:19.520]  появился.
[22:19.520 --> 22:21.920]  Квадрат на самом деле появился, исходя из простых соображений.
[22:21.920 --> 22:26.160]  Во-первых, когда у вас стоит норма вторая, сама по себе
[22:26.160 --> 22:28.200]  и не в квадрате, ее считать дороже.
[22:28.200 --> 22:29.840]  Вам нужно сначала посчитать своему квадратов, потом
[22:29.840 --> 22:31.440]  еще считать квадратный корень.
[22:31.440 --> 22:34.520]  Так как у нас парабола… парабола, господи… квадратный
[22:34.520 --> 22:37.600]  корень функции монотонная, правильно?
[22:37.600 --> 22:39.640]  Соответственно, argmax у нас будет одинаковый.
[22:39.640 --> 22:41.840]  Поэтому вы завели в квадрат или парабола, опять же, справа
[22:41.840 --> 22:42.840]  от нуля.
[22:43.240 --> 22:46.320]  У нас задача не поменяла своего решения, именно точки
[22:46.320 --> 22:49.040]  аргумента, где достигается максимум, но при этом считать
[22:49.040 --> 22:50.040]  нам выгиб.
[22:50.040 --> 22:51.040]  А?
[22:51.040 --> 22:55.920]  Одна вторая просто остается исходно оттуда, вот она
[22:55.920 --> 22:56.920]  здесь стояла.
[22:56.920 --> 23:02.920]  Смотрите, я подеваю, ладно, согласен, должна быть одна
[23:02.920 --> 23:03.920]  четвертая.
[23:03.920 --> 23:08.080]  На самом деле, абсолютно, я тогда поправлю, это здесь,
[23:08.080 --> 23:09.080]  это видимо просто опечатка.
[23:09.080 --> 23:11.240]  Абсолютно неважно почему, у вас есть константа.
[23:11.240 --> 23:14.560]  Давайте все просто, короче, вот это все еще домножим
[23:14.560 --> 23:17.360]  условно на двойку, тогда здесь будет C со звездой
[23:17.360 --> 23:18.360]  новое.
[23:18.360 --> 23:20.600]  C со звездой, как раз, отвечает за, по сути, соотношение
[23:20.600 --> 23:23.400]  регуляризации, не регуляризация, простите, ограничений и
[23:23.400 --> 23:24.400]  шириной полосы.
[23:24.400 --> 23:26.840]  Все константы, они, по сути, вырождаются в одну.
[23:26.840 --> 23:32.000]  Так, ну вот страшные всякие ботан, точнее метапты для
[23:32.000 --> 23:34.800]  тех, кто знаком с метаптами, я вам их лишний раз показывать
[23:34.800 --> 23:35.800]  не буду.
[23:35.800 --> 23:36.800]  Вот.
[23:36.800 --> 23:39.800]  Я на самом деле хотел бы с вами сейчас внимательно
[23:39.800 --> 23:44.680]  посмотреть вот на эту формулу, снизу, ККТ показывать не
[23:44.680 --> 23:45.680]  надо.
[23:45.680 --> 23:48.560]  Если вы покажете это большой плюс, это плюс балл, молодцы,
[23:48.560 --> 23:51.360]  но, скажем так, если вас на метаптах этому не научили,
[23:51.360 --> 23:52.360]  мои полномочия на этом все.
[23:52.360 --> 23:53.360]  К сожалению.
[23:53.360 --> 23:58.160]  Я, так скажу, по опыту процентов 80 не может это воспроизвести
[23:58.160 --> 24:00.960]  на экзамене, так что я к этому отношусь уже по-философски.
[24:00.960 --> 24:01.960]  Вот.
[24:01.960 --> 24:06.200]  Ладно, давайте сюда внимательно посмотрим, пожалуйста, посмотрите
[24:06.200 --> 24:08.680]  внимательно на эту формулу и скажите мне, она вам ничего
[24:08.680 --> 24:14.240]  не напоминает, классически, если я пока дверь закрою.
[24:14.240 --> 24:33.680]  Похоже на регуляризацию, но теперь давайте посмотрим
[24:33.680 --> 24:34.680]  внимательнее.
[24:34.680 --> 24:36.320]  Это на самом деле я вас туда и подвожу.
[24:36.320 --> 24:39.600]  У вас есть два члена в вашей функции, скажем так,
[24:39.600 --> 24:41.760]  эмпирического риска, который вы минимизируете.
[24:41.760 --> 24:42.760]  Правильно?
[24:42.760 --> 24:43.760]  Первое.
[24:43.760 --> 24:47.960]  У вас есть какой-то член, который отвечает за величину
[24:47.960 --> 24:50.520]  на каждом объекте по отдельности, вот же у вас здесь margin на
[24:50.520 --> 24:52.560]  каждом объекте стоит, правильно?
[24:52.560 --> 24:54.960]  1 минус margin положительная часть.
[24:54.960 --> 24:57.920]  Вторая часть у вас зависит только от вектора весов,
[24:57.920 --> 24:59.440]  от параметров вашей модели, вообще не зависит ни от
[24:59.440 --> 25:02.520]  каких объектов по отдельности, только целиком.
[25:02.520 --> 25:04.640]  Мы с вами раньше видели то же самое.
[25:04.640 --> 25:07.600]  Вся потеря плюс регуляризатор.
[25:07.600 --> 25:11.840]  Здесь у нас 1 член, 2 член, давайте я до него сейчас дойду,
[25:11.840 --> 25:17.200]  по сути вот оно у нас и есть, но почему очень важна
[25:17.200 --> 25:19.880]  эта часть лекции и почему на этом занятии такое пристальное
[25:19.880 --> 25:22.240]  внимание уделяется тому, откуда мы это вывели.
[25:22.240 --> 25:24.880]  Помните нашу изначальную постановку задачи.
[25:24.880 --> 25:27.720]  Мы сказали, пусть у нас выбор коленина разделимая,
[25:27.720 --> 25:31.080]  и тогда мы максимизируем ширину полосы, а значит
[25:31.120 --> 25:36.040]  мы минимизируем вторую норму вектора весов, согласны?
[25:36.040 --> 25:39.160]  Наша изначальная оптимизационная задача была именно минимизировать
[25:39.160 --> 25:42.800]  вторую норму вектора весов, плюс у нас были дополнительные
[25:42.800 --> 25:45.400]  ограничения, что у нас margin везде больше или равен
[25:45.400 --> 25:46.400]  1.
[25:46.400 --> 25:49.280]  В классической постановке, если мы сюда посмотрим, у
[25:49.280 --> 25:52.960]  нас какая-то функция ошибки, плюс регуляризатор в качестве
[25:52.960 --> 25:54.840]  второй нормы вектора весов, мы это с вами уже раньше
[25:54.840 --> 25:57.840]  видели, регуляризация потихоньку.
[25:57.840 --> 25:58.840]  К чему я это веду?
[25:59.000 --> 26:02.720]  К тому, что зависит от вашей формулировки задачи, что
[26:02.720 --> 26:04.920]  вы назовете регуляризатором, что вы назовете на самом
[26:04.920 --> 26:07.480]  деле функцией потерь, можете хоть горшком все назвать,
[26:07.480 --> 26:09.880]  у вас всегда оптимизационная задача решается и в нее
[26:09.880 --> 26:13.360]  входят, прям так, у вас либо один функционал оптимизируется,
[26:13.360 --> 26:15.480]  либо сразу несколько их линейных комбинаций.
[26:15.480 --> 26:18.680]  Как вы их называете, это ваше дело, это зависит исключительно
[26:18.680 --> 26:21.160]  от того, какой у вас бэкграунд, какие требования заказчики
[26:21.160 --> 26:22.160]  и так далее.
[26:22.160 --> 26:23.600]  Суть в том, что все равно вот эту штуку вам придется
[26:23.600 --> 26:24.600]  минимизировать.
[26:24.600 --> 26:27.480]  То, что только что у нас с вами оригинальный функционал
[26:27.720 --> 26:28.720]  был.
[26:28.720 --> 26:31.360]  Вот, который мы оптимизируем, а вот на него ограничения.
[26:31.360 --> 26:33.520]  По факту мы раньше говорили, что у нас оригинальная функция
[26:33.520 --> 26:36.080]  потерья то, что мы минимизируем, плюс ограничение второй
[26:36.080 --> 26:37.560]  нормы векторов весов, например.
[26:37.560 --> 26:40.120]  Что вы назовете ограничением зависит от того, как вы задачу
[26:40.120 --> 26:41.120]  формулировали.
[26:41.120 --> 26:42.760]  Здесь у нас получилось по факту наоборот.
[26:42.760 --> 26:45.080]  Вот наша функция потерь, в кавычке, то, что нам надо
[26:45.080 --> 26:46.080]  минимизировать.
[26:46.080 --> 26:47.080]  А вот наши ограничения.
[26:47.080 --> 26:49.680]  Как видите, я только что по сами руками их поменял
[26:49.680 --> 26:50.680]  насталь.
[26:50.680 --> 26:53.280]  Так что, пожалуйста, не привязывайтесь намертвость
[26:53.280 --> 26:55.780]  тем, что вот функция потерь это функция потерь, регуляризация
[26:55.780 --> 27:00.780]  ограничения, по факту у вас все эти члены составляют вместе ваш функционал,
[27:00.780 --> 27:04.400]  который вы оптимизируете. И важно понимать, что откуда пришло, а не что каким
[27:04.400 --> 27:08.860]  словом назвать. Можешь назвать как угодно просто, не знаю, там, r1, r2. Первый
[27:08.860 --> 27:14.660]  член, второй член. Все. Хорошо? Тут вопрос есть?
[27:20.620 --> 27:25.700]  Вот смотрите, регуляризация в классическом смысле, откуда взялось один делить на
[27:25.700 --> 27:30.820]  2c. Ну, все поделили на c. Это просто исходя из классического, скажем так, вывода,
[27:30.820 --> 27:34.460]  который только что здесь был, один делить на 2c. Логично, что один делить на 2c
[27:34.460 --> 27:39.020]  можно переобозначить другой констант и с ней работать. Это просто, скажем так,
[27:39.020 --> 27:43.180]  реверанс в сторону оригинального представления, которое было представлено
[27:43.180 --> 27:48.260]  Вапником и Червоненкесом в их работе чуть ли не 53 года. Я могу наврать с датой.
[27:48.260 --> 27:53.660]  Короче, метапорных витеров это, собственно, наследие советских математиков Вапник
[27:53.660 --> 27:57.460]  и Червоненкес, которые создали метапорных витеров, потом обобщили его на линии
[27:57.460 --> 28:01.420]  неразделимую выборку и так далее. Вапник все еще выступает в роли
[28:01.420 --> 28:05.860]  профессора. Некоторое время назад он был в ныне запрещенной у нас организации
[28:05.860 --> 28:09.580]  на западе, у них в лабораториях там по искусственному интеллекту.
[28:09.580 --> 28:15.180]  Червоненкес, к сожалению, по-моему, году в 2014-м отошел в мир иной, по-моему,
[28:15.180 --> 28:19.700]  гулял в лосе на островском парке и заблудился. Так что, пожалуйста, это. Не теряйте
[28:19.700 --> 28:24.980]  телефоны. Классный очень мужик. Отвечал, в том числе, помогал запускать шат, который,
[28:24.980 --> 28:27.300]  я думаю, многие из вас знают, но вот так случилось.
[28:32.740 --> 28:37.460]  Вот, смотрите, что здесь имеется в виду. Во-первых, исторически для SVM-а
[28:37.460 --> 28:40.780]  конкретно записывают именно вот эту константу, которая отвечает за баланс
[28:40.780 --> 28:46.980]  между нормой вектор весов и марджином, именно как 1 делить на 2c. И если у нас
[28:46.980 --> 28:51.700]  1 делить на 2c, соответственно, если ц большая, значит, нас множит 1 делить на 2c,
[28:51.700 --> 28:58.020]  эта штука маленькая, и значит, у нас ограничение на ширину полосы тоже небольшое. Мы говорим,
[28:58.020 --> 29:02.060]  что нам не столь важно, насколько широкая полоса, то есть, по сути, она может больше крутиться,
[29:02.060 --> 29:06.900]  но при этом нам важно, чтобы у нас ошибка аппроксимации была наименьшая, то есть как
[29:06.900 --> 29:12.020]  можно меньше объектов должны иметь марджин меньше единиц. По сути, когда мы с вами снижаем ширину
[29:12.020 --> 29:16.820]  полосы, логично, что внутрь полосы попадает меньше объектов и меньше краткие вот этот член.
[29:16.820 --> 29:23.580]  Согласны? Вот. Если ц опять же маленькая, то 1 делить на 2c большая величина, значит,
[29:23.580 --> 29:29.380]  нам нужна широкая полоса, у нее меньше свободы крутиться, но при этом она заметает больше объектов
[29:29.380 --> 29:34.660]  и этот член становится больше. Все. То есть, просто интерпретация того, что если ц большая, значит,
[29:34.660 --> 29:39.980]  у нас полоса узкая, объектов в нее попало мало, если ц маленькая, полоса широкая, объектов в нее
[29:39.980 --> 29:45.140]  попало много. И теперь смотрите, в чем еще магия. Мы же с вами помните, на прошлом занятии выводили
[29:45.140 --> 29:50.820]  как раз-таки log-loss и обнаружили, что это верхняя оценка на нашу функцию потерь, именно истинную,
[29:50.820 --> 29:55.820]  которая просто корречит ошибку классификации, ступеньку. Вот она. Вот ваша ступенька марджин
[29:55.820 --> 30:01.700]  меньше нуля. Что мы здесь видим? 1 минус марджин плюс положительная часть. Вот ваша функция
[30:01.700 --> 30:06.660]  потерь. Это опять верхняя оценка внезапно, потому что на самом деле она выведена исходя из того,
[30:06.660 --> 30:14.140]  что мы решаем задачи классификации. И эта функция потерь называется hinge-loss. Не знаю,
[30:14.140 --> 30:18.980]  функция потери менее hinge-loss. Не имею hinge-loss, нам ее всю жизнь все кличут, как она называется по-русски.
[30:18.980 --> 30:24.580]  Я даже боюсь вам наврать. Ну вот, собственно, как она у нас выглядит. Хорошо?
[30:28.580 --> 30:36.460]  Ширина полосы. Еще раз, смотрите. Что означает ширина полосы? Мы говорим, что у нас ширина
[30:36.460 --> 30:41.540]  полосы — это расстояние между ближайшими к разделяющей гиперплоскости объектами одного
[30:41.540 --> 30:47.060]  класса и другого класса. И в зависимости от того, как вы проведете гиперплоскость, у вас она может
[30:47.060 --> 30:51.140]  быть либо больше, либо меньше. Потому что, смотрите, вот для этого, например, видите, у вас раз и два.
[30:51.140 --> 30:57.620]  По направлению проекции у вас там совсем маленькое расстояние. Для второй прямой у вас расстояние по
[30:57.620 --> 31:02.060]  направлению проекции сильно больше. Чем больше вот это расстояние, что то же самое, что ширина
[31:02.060 --> 31:05.900]  полосы, тем более устойчива наша модель с точки зрения того, что небольшое изменение весов не
[31:05.900 --> 31:12.620]  приведет к тому, что у нас ответы классикации поменяются. Вот. Хорошо. Ну и, собственно, вот это
[31:12.620 --> 31:20.060]  самое ограничение на наш, где он, вернись, на наш маржин, один минус маржин как раз-таки, это есть
[31:20.060 --> 31:24.740]  наша часть кинжалоса, причем на нее глядя можно очень просто понять, что на самом деле происходит.
[31:24.740 --> 31:30.020]  И почему мета называется метапорных векторов? Посмотрите, если мы вот эту штуку будем
[31:30.020 --> 31:34.580]  градиентным способом оптимизировать, а че, все нормально, дифференцируемо, дифференцируемо,
[31:34.580 --> 31:39.460]  все дифференцируемо. Если вы вспомните, что кинжалос у нас в точке 1 не дифференцируемый,
[31:39.460 --> 31:44.740]  ну давайте определим производную в единице нулем, и все. Нам не нужна гладкость, нам достаточно знать
[31:44.740 --> 31:51.100]  производную в каждой точке. Что здесь происходит? Смотрите, какие объекты являются опорными? Вот
[31:51.100 --> 31:57.540]  интуитивно, подумайте, глядя на этот функционал. Что такое опорные объекты, как вы думаете?
[31:57.540 --> 32:08.300]  Классно. Ну сейчас я покажу, откуда это выводится из оригинальной постановки. Смотрите, какие объекты
[32:08.300 --> 32:13.140]  вообще влияют на решение с точки зрения данного функционала? Если объект находится в глубине своего
[32:13.140 --> 32:18.900]  класса, он как-то влияет вообще на решение? У него margin больше единицы, 1 минус margin отрицательный,
[32:18.900 --> 32:23.500]  у нас все отрицательное отбрасывается, настолько положительное, поэтому в этом члене у нас клад
[32:23.500 --> 32:29.820]  будет 0, на вес он напрямую тоже не влияет 0. Все, объект вообще не влияет на решение. Он
[32:29.820 --> 32:34.060]  абсолютно не интересен. Именно поэтому это называется опорными хакторов. Опорными являются только те
[32:34.060 --> 32:39.580]  объекты, у которых вот эта величина меньше или равна меньше единиц. Они опорны потому что они
[32:39.580 --> 32:43.220]  вообще влияют на решение. Все остальные объекты вообще игнорируются, они могут быть где угодно,
[32:43.220 --> 32:49.740]  как угодно, нам не важно. Опорны только те объекты, которые оказались внутри полосы или, соответственно,
[32:49.740 --> 32:55.060]  вообще в чужом классе. Вот они опорные. И опять же, здесь есть, грубо говоря, два типа объектов.
[32:55.060 --> 33:00.180]  Первое, те, которые сидят внутри полосы, это именно что опорные объекты, на них опирается наша
[33:00.180 --> 33:04.700]  гиперплоскость. Второе, это шумовые объекты. Мы, к сожалению, ничего с ними сделать не можем,
[33:04.700 --> 33:08.820]  если объект сидит вообще внутри чужого класса, где-нибудь вот, не знаю, синяя точка будет вот
[33:08.820 --> 33:15.020]  здесь сидеть, то у него точно также будет 1 минус margin большая величина, и за это мы будем,
[33:15.020 --> 33:33.780]  грубо говоря, платить. Вот. Понятно? И да, и нет. Смотрите, фильтрация выбросов тема классная,
[33:33.780 --> 33:40.860]  но у нее есть одна маленькая проблема. Если у вас выборка очень многомерная и у вас там существенный
[33:40.860 --> 33:46.460]  нелиней на зависимости, то у вас выбросами, грубо говоря, выбросом вы можете обозначить объект,
[33:46.460 --> 33:51.780]  если у вас либо уже существует модель, и он сильно ей не подчиняется на вопрос ваша модель хорошая
[33:51.780 --> 33:57.580]  или плохая, или если вы можете это явно увидеть. Например, у вас совершенно спокойно, может быть,
[33:57.580 --> 34:01.500]  я пока рассказываю, пускай эта штука включится, у вас совершенно спокойно может быть какая-нибудь
[34:01.500 --> 34:06.900]  разделяющая гиперплоскость, не гиперплоскость, а гиперповерхность, вот такая вот. Синусоида,
[34:06.900 --> 34:12.860]  представьте себе, и, соответственно, внутри сверху от синусоида один класс, снизу другой. У вас
[34:12.860 --> 34:16.900]  никакого эвористического правила, который позволит одно от другого отделить, если вы не знаете
[34:16.900 --> 34:21.780]  истинную зависимость, нет. Поэтому вы эвористически можете обозвать выбросами, например, те точки,
[34:21.780 --> 34:27.100]  которые сидят на вершинах краски этих вот синусоиды или ксинусоиды. Являются ли они выбросами? Нет,
[34:27.100 --> 34:32.300]  у вас модель плохая. Так что в общем случае отбора нет, но есть различные способы, один из них,
[34:32.300 --> 34:36.660]  краски мы разберем с вами на следующем занятии, как фильтровать выброс, который прям явно выброс.
[34:36.660 --> 34:45.340]  Но в общем случае отбора бошечки, оно на венде, отбор выбросов у нас как такового не эвористического
[34:45.340 --> 34:49.500]  нет, потому что, чтобы знать, какие точки не подчиняются зависимости, нам надо знать саму
[34:49.500 --> 34:54.060]  зависимость, а наша задача найти зависимость. Пока мы ее не нашли, для нас точки просто какие-то
[34:54.060 --> 34:58.740]  странные. Так, ладно, спасибо, можно тебя выключить? Я уже передумал, я уже в воздухе все нарисовал.
[34:58.740 --> 35:05.620]  Хорошо, но собственно вот, на всякий случай, что здесь написано. Смотрите, вот, ксишка равна нулю,
[35:05.620 --> 35:11.420]  это те объекты, на которых у нас ограничения не нарушаются, с ним все понятно, они не опорные.
[35:11.420 --> 35:18.180]  Ксишка равна нулю, соответственно, те объекты, которые находятся на границе, они краски уже на что-то
[35:18.180 --> 35:22.300]  повлияли, потому что до этого ксишка была не равна нулю. Те объекты, для которых ксишка больше нуля,
[35:22.300 --> 35:27.340]  значит, не попали внутрь полосы или вообще наружу, это объекты, которые тоже влияют на решение. Вот эта
[35:27.340 --> 35:32.740]  пара влияет на решение, эти никоим образом не влияют. Опять же, откуда здесь какие-то лямбды,
[35:32.740 --> 35:38.420]  это собственно из двойственной задачи. Вот, мы можем посмотреть, вот мы вводим одни ограничения
[35:38.420 --> 35:42.420]  нежесткие, вторые жесткие, типы неравенства и неправенства, переписываем, соответственно,
[35:42.420 --> 35:47.860]  условия для локального минимума необходимые, решаем, вспоминаем там мед, может ли, ларанжа,
[35:47.860 --> 35:54.460]  получаем результат. Но это, скажем так, передам Александру Катруце, наверное, он сейчас читает
[35:54.460 --> 36:01.300]  опты. Знамя, чтобы он вам про это рассказал. Хорошо, ну что, со своим понятно, что происходит? А
[36:01.300 --> 36:08.860]  теперь, как раз, давайте вспомним, что мы с вами делали на лекции номер два классической нумерации
[36:08.860 --> 36:12.940]  с начала семестра или на лекции номер ноль, если посмотреть в репозитории, я просто вынес, чтобы
[36:12.940 --> 36:16.460]  классическую нумерацию иметь. Помните, мы там с вами говорили про скалярное произведение,
[36:16.460 --> 36:22.380]  правильно? Мы же можем с вами разные скалярные произведения вводить, согласны? И более того,
[36:22.380 --> 36:26.180]  здесь мы с вами можем видеть краски, здесь это классическое скалярное произведение, когда мы
[36:26.180 --> 36:35.540]  считаем margin, у нас там ωх плюс b, все понятно. А давайте-ка попробуем выбрать что-то более
[36:35.540 --> 36:41.060]  подходящее для задачи и ввести саму понятие более подходящее, более формально. Мы, по сути,
[36:41.060 --> 36:46.420]  с вами, когда считали margin, мы внутри него считали скалярное произведение между вектором весов и
[36:46.420 --> 36:51.580]  вектором нашего объекта, правильно? А почему бы нам не взять какое-нибудь другое скалярное
[36:51.580 --> 36:55.460]  произведение, какого-нибудь другого пространства, которое все еще удовлетворяет всем тем же
[36:55.460 --> 37:01.340]  ограничениям, которые у нас на него есть, чтобы не получить другие виды правильства? Ну, например,
[37:01.340 --> 37:04.620]  давайте перейдем в какое-нибудь другое гибель этого пространства и введем там какую-нибудь функцию
[37:04.620 --> 37:10.460]  ядра, по факту переопределим скалярное произведение, которое было изначально, вместо дотпродукта будем
[37:10.460 --> 37:17.660]  использовать вот такое ядро, которое удовлетворяет следующим ограничениям. Хорошо? Понятно,
[37:17.660 --> 37:22.660]  что здесь написано, на всякий случай? Прочитайте, пожалуйста, вот здесь. Мне надо, чтобы вы
[37:22.660 --> 37:28.980]  прочитали эти формулы, правда. Это проще языком математики сказать, чем долго пересказывать
[37:28.980 --> 37:41.780]  языком обычным, я и так тоже сказал, на самом деле. Смотрите, у вас что написано? Ядро для двух точек,
[37:41.780 --> 37:46.220]  вот, к от х штрих, это что? Это отображение из декартового произведения пространства х на
[37:46.220 --> 37:56.700]  пространство х в R. Это просто две точки. К от х задает функционал, который отображает декартовое
[37:56.700 --> 38:04.900]  произведение n-мерного пространства линейного, в котором мы были, в R. Собственно, х и х штрих это
[38:04.900 --> 38:09.420]  два вектора. В нашем случае у нас что умножается друг на друга? Вектор, который указывает на точку,
[38:09.420 --> 38:14.300]  и вектор нормалик нашей гиперплокости. Они же все равно в одном линейном пространстве, правильно?
[38:14.300 --> 38:22.140]  Раз мы их перемножим, можем скалярно. Вот, а теперь, собственно, что за новое пространство? А мы можем
[38:22.140 --> 38:25.540]  даже явно его не задавать, мы вместо этого можем сказать, а давайте-ка мы зададим какое-нибудь
[38:26.040 --> 38:30.300]  ядро ему будет соответствовать какой-то гейлер этого пространства. И более того, за счет того,
[38:30.300 --> 38:34.900]  что мы поменяли ядро, по сути, мы подмена ядра, поменяли скалярное произведение пространстве. Мы
[38:34.900 --> 38:38.540]  из скалярном произведении можем индуцировать норму, по сути, мы поменяли наше пространство,
[38:38.540 --> 38:43.180]  там теперь норма другая стала. Соответственно, расстояния поменялись. А если мы это сделаем,
[38:43.180 --> 38:49.580]  то мы с вами не явно преобразовали наши признаки пространства, сделав его нелинейным относительно
[38:49.580 --> 38:54.240]  исходных признаков. Я вам сейчас пример покажу. Вот смотрите, вот наша исходная выборка,
[38:54.240 --> 39:00.560]  например. Вот у нас с вами линейная гиперплоскость. Вот мы с вами берем и вместо этого вводим
[39:00.560 --> 39:06.160]  полиномиальное ядро краски степени D. Я не знаю какая там D, я не помню, кажется 2 или 3 степени.
[39:06.160 --> 39:10.960]  И теперь у нас с вами скалярная произведение считается вот таким образом. D, наверное,
[39:10.960 --> 39:16.040]  равно 3. Все остальное абсолютно так же. Вы решаете ту же самую оптимизационную задачу.
[39:16.040 --> 39:19.900]  У вас только скалярное произведение по-другому считается. Итоговая гиперплоскость в исходном
[39:19.900 --> 39:32.420]  пространстве вот так выглядит. У вас поверхность теперь нелинейно-разделяющая. Да, это ваше ядро,
[39:32.420 --> 39:35.700]  то есть здесь это именно классическое скалярное произведение, там специально треугольные скобки,
[39:35.700 --> 39:40.460]  а потом мы это возводим в третьей степени. Вот экспоненциальное ядро, пожалуйста. Взяли,
[39:40.460 --> 39:46.360]  посчитали норму, возвели в квадрат минус гамма-экспонент. Еще раз, это линейная, по сути,
[39:46.360 --> 39:51.120]  гиперплоскость, в том новом гильбертом пространстве, куда мы попали. В исходном
[39:51.120 --> 39:55.200]  пространстве логично, у нас скалярное произведение другое, норма другая, оно выглядит нелинейно.
[39:55.200 --> 40:01.360]  Плюс в чем? Вы, подобрав правильное ядро, по сути это называется спрямляющее пространство по-другому,
[40:01.360 --> 40:06.680]  вы спрямляете пространство, в нем ваша выборка линейно-разделима становится. То есть вы,
[40:06.740 --> 40:12.900]  подобрав правильное ядро, можете решить задачу уже линейным образом. Проблема этого заключается
[40:12.900 --> 40:17.460]  лишь в одном. Как вы думаете, в чем проблема этого трюка с подменой ядра или кернел-трик, как его
[40:17.460 --> 40:28.260]  называют? Переобучаемся раз, сложно считать два, еще варианты. Бинго, откуда нам взять ядро? Мы не
[40:28.260 --> 40:32.460]  знаем, какое ядро подходит. В общем случае, у нас с вами есть вот пачка, грубо говоря, общепринтых
[40:32.460 --> 40:36.840]  ягер, там штук 20, еще у некоторых из них еще и параметры есть, у полинамиального вот этого
[40:36.840 --> 40:42.600]  тут D, вы можете варьировать 2, 3, 4, 5, 10. Все. Какое ядро выбрать, это вопрос.
[41:06.840 --> 41:13.080]  Вообще да, может быть. Я, честно говоря, ваш вопрос не понял, может мне потом в перерыве
[41:13.080 --> 41:25.240]  тогда сказать конкретно, в чем беда, ладно? Вот этого конкретно? Вот это?
[41:25.240 --> 41:44.960]  Я ничего не понимаю, какой термин вам? Да. Ну слушайте, самое простое, наверное, взять методичку
[41:44.960 --> 41:50.160]  Воронцова, там стоит ссылка, скорее всего, на книжку, господи, как она называется, 11 года,
[41:50.160 --> 41:57.560]  наш Толмут. Все, позор моим сиделом, я забыл, как книжка называется.
[42:20.160 --> 42:43.440]  Не, погодите, что здесь написано? Здесь мы все лишь говорим, что для нашего ядра должно
[42:43.440 --> 42:47.960]  выполняться следующее, что существует такое отображение из X в новое пространство H,
[42:47.960 --> 42:56.560]  что именно про скалярное произведение от образа X и образа X' есть как раз таки результат
[42:56.560 --> 43:04.160]  применения к исходным их значениям ядра. Все. Мы утверждаем, что существует отображение из
[43:04.160 --> 43:08.960]  исходного пространства X в целевое спрямляющее пространство H такое, что скалярное произведение
[43:08.960 --> 43:15.480]  образов есть результат применения ядра к исходным значениям, как они там полностью называются? Есть
[43:15.480 --> 43:30.800]  образ, а есть что? Прообразов. Можно его найти. Я вам его по памяти не назову, я честно скажу.
[43:38.800 --> 43:43.000]  Не, как все отексы выглядят, я думаю, надо в каждом коментном случае это вводить. На самом
[43:43.080 --> 43:47.280]  деле на ядро есть несколько ограничений. Я, кажется, не включил слайд, давайте я потом покажу вам.
[43:47.280 --> 43:53.120]  Вот. Еще вопрос.
[43:57.120 --> 44:02.720]  Кажется, все немножко загрустили. По факту, что происходит с ядрами? Во-первых, нужны ли они вам
[44:02.720 --> 44:08.880]  сейчас вот прям на практике? На практике kernel trick применяют скорее реже, чем чаще. Это происходит
[44:08.880 --> 44:13.640]  в каких-то задачах, где вам вот SWM очень хорошо нравится, или если вы очень сильно нравится,
[44:13.640 --> 44:17.880]  или если вы имеете какое-то прям явное предположение, какое пространство спрямляющее вам может
[44:17.880 --> 44:22.160]  подходить. Ну, например, такое бывает в различных биржевых задачах, там SWM все еще популярен.
[44:22.160 --> 44:29.320]  По факту, зачем нам это надо? Затем, чтобы первый раз выйти за границы линейных моделей, потому что мы
[44:29.320 --> 44:34.080]  с вами первый раз говорим, а что сделать, если у нас линейной модели явно не хватает, а что делать мы
[44:34.080 --> 44:38.480]  не знаем. На самом деле, есть еще шаг номер ноль, который, в принципе, очевиден. Вы можете просто
[44:38.480 --> 44:42.800]  руками преобразовать ваши признаки, любым образом. Например, добавить там квадратичные признаки,
[44:42.800 --> 44:47.240]  квадраты всех, попарные произведения, добавить какие-нибудь функции над ними, не знаю, там sinx
[44:47.240 --> 44:52.280]  сквозь sinx, и так далее. Все это можно сделать для каждого элемента и получить новый признак.
[44:52.280 --> 44:58.440]  SWM краски был хорош в свое время, потому что тогда выборки были маленькие, задач было не так много.
[44:58.440 --> 45:03.760]  SWM позволяло найти решение до всей линей разрешимых задач. Сейчас по факту выбирать ядро вручную,
[45:03.760 --> 45:09.400]  конечно же, боль, страдание и на огромных выборках не работает, потому что вам для этого ядра еще и
[45:09.400 --> 45:13.160]  придется постоянно его считать под капотом. Скалярный произведение, как вы понимаете, обычное, два вектора
[45:13.160 --> 45:18.120]  перемножить сильно быстрее работает, чем посчитать там экспоненту от какого-нибудь, не знаю там, нормы
[45:18.120 --> 45:24.200]  разности. Поэтому здесь мы скорее с вами хотим обратить внимание на то, что мы с вами можем менять
[45:24.200 --> 45:30.600]  наше признаковое пространство различными способами. Например, используя краски вот это самое новое ядро,
[45:30.600 --> 45:34.760]  которое позволяет нам не само пространство поменять, мы даже явно не знаем, как оно выглядит, теперь новое
[45:34.760 --> 45:39.280]  пространство. Мы говорим, что у нас новое ядро, определяя скалярное произведение новое, оно логично
[45:39.280 --> 45:43.640]  его свойственно должно обладать. И тогда мы, соответственно, с вами получаем новое пространство,
[45:43.640 --> 45:47.400]  которое как-то там может быть выражено. Мы об этом не задумываемся, мы говорим, вот,
[45:47.400 --> 45:52.520]  скалярный произведение заменили, все остальное там пусть как-то индуцируется оттуда. Тут вопросы
[45:52.520 --> 45:57.280]  пожелания есть? Живы, целы или совсем запутались с этими?
[46:00.600 --> 46:04.760]  В новом пространстве будут.
[46:10.080 --> 46:20.680]  Так, хорошо, еще вопросы? Как выбрать ядро? По-навучному. Изучаете структуру ваших данных, смотрите на
[46:20.680 --> 46:26.440]  зависимость, пытаетесь положить их на какие-нибудь гиперповерсии, 1, 2, 3, 4, 5 порядка. На практике,
[46:26.440 --> 46:33.320]  у вас есть выборка из семи ядер кросс-валидацией. Если у вас датсет размером миллион на миллион,
[46:33.320 --> 46:38.160]  то берете под выборок штук 10, размером, не знаю, там, один процент, и на них выбираете,
[46:38.160 --> 46:41.360]  потому что вы на миллион на миллион будете еще все это обучать достаточно долго и дорого.
[46:41.360 --> 46:48.680]  Ну и на практике, как правило, берут просто-напросто модельку попроще, но при этом способную к
[46:48.680 --> 46:53.480]  нелинейностям адаптируется самостоятельно, то же самое дерево или их ансамбль, та же самая
[46:53.480 --> 46:58.520]  сеточка, какая-нибудь трех-четырехслойная, она вам каким-то неявным образом париметризует
[46:58.520 --> 47:02.640]  гораздо более сложную гиперповерхность, но при этом минус в том, что вы не явно это делаете,
[47:02.640 --> 47:06.480]  вы не можете сказать, что вот мы выбрали из таких соображений. Вот что там градиентный спуск нашел,
[47:06.480 --> 47:12.240]  то и нашел. Свобода, но за нее мы расплачиваемся не интерпретируемой ситуацией. Это сейчас основная
[47:12.240 --> 47:18.280]  проблема диплерринга. Ладно, собственно, это была во многом такая историческая справка, чтобы
[47:18.280 --> 47:23.760]  показать вам, что разными путями шло развитие методов глубокого обучения и вообще машинного
[47:23.760 --> 47:29.560]  обучения, и, во-вторых, что, на самом деле, машинное обучение, оно не стоит вот где-то вот особняком.
[47:29.560 --> 47:34.120]  Мы, на самом деле, раньше показывали прям весь вот этот вывод, краски показывали, все свойства этих
[47:34.120 --> 47:38.840]  ядер и так далее. Почему оно сейчас достаточно оперативно? То, что запоминало это обычно,
[47:38.840 --> 47:44.760]  процента 2 понимало процента 1 от слушателей, а необходимость на практике это делать у процента
[47:44.760 --> 47:49.600]  0, наверное, 0,05 присутствует, потому что СВМ, к сожалению, сейчас это красивая историческая
[47:49.600 --> 47:54.240]  модель, которая была реально создана, была очень популярна и в том числе показала, что советская
[47:54.240 --> 48:00.240]  российская школа в области машинного обучения ОГОГО. Долгие годы СВМ на всяких там хог гистограмм
[48:00.240 --> 48:05.220]  ориентированных градиентов был одним из основных способов работать с изображениями, но времена
[48:05.220 --> 48:09.600]  меняются. Сейчас СВМ это уже скорее красивый подход, который в некоторых задачах подходит, но
[48:09.600 --> 48:14.840]  зачастую он бьется другими подходами, которые проще, понятнее и так далее. Но тем же самым
[48:14.840 --> 48:19.560]  деревьями, собственно, закат СВМ в некотором смысле произошел в 2001 году, я опять же забегаю
[48:19.560 --> 48:24.480]  немного вперед, когда Фридман представил свою GBM Gradient Boosting Machine, метод градиентного
[48:24.480 --> 48:29.080]  бустинга. Когда градиентный бустинг появился, он без всякого подбора ядра стал нелинейной
[48:29.080 --> 48:34.800]  гиперповерности опроксимировать гораздо лучше и без всяких там страданий и приседаний. СВМ сразу
[48:34.800 --> 48:41.120]  же начал вдавать позиции. Ладно, и второй, собственно, момент, который хочется тоже сегодня разобрать
[48:41.120 --> 48:46.640]  коротенько, это метод главных компонент. Сейчас у ужаса у нас будет аж вторая, по-моему, теорема,
[48:46.640 --> 48:51.280]  которая есть в этом курсе. Первая была теория Магаусса Маркова. Кто помнит теория Магаусса Маркова?
[48:51.280 --> 49:01.000]  А остальные, где были на лекции? Уже забыли? Ладно. Оптимальную средине смещенных оценку дает
[49:01.000 --> 49:06.960]  вам минимизация МСЕ при условии, что у вас ошибка не смещенная, имеет конечную дисперсию и ошибки
[49:06.960 --> 49:14.160]  между собой неискоррелированы. Вот. Классно. Ну а теперь давайте чуть-чуть поговорим про задачу снижения
[49:14.160 --> 49:19.840]  размерности. Эта задача обучения без учителя, но, тем не менее, она очень широко применяется. И почему
[49:19.840 --> 49:25.240]  PCA здесь вообще стоит, хотя мы вроде с учителем пока работаем, потому что PCA это линейный метод раз
[49:25.240 --> 49:30.360]  и снижение размерности, в принципе, штука крайне важная и PCA, несмотря на то, что он простой линейный,
[49:30.360 --> 49:35.480]  все еще один из наиболее широко употребимых методов снижения размерности, который вообще в
[49:35.480 --> 49:40.920]  мире используется. Есть всякие там отэнкодеры, неявное снижение размерности и так далее. PCA простой,
[49:40.920 --> 49:46.360]  прямолинейный и работает. За это его все любят. Более того, он как раз и полностью интерпретируем,
[49:46.360 --> 49:52.600]  понятно, что происходит. Классно. Поехали. Зачастую у нас там десятки, сотни тысяч признаков есть,
[49:52.600 --> 49:57.840]  нам с ними работать банально неудобно. Во-первых, потому что у нас скорость подсчетов все-таки
[49:57.980 --> 50:01.420]  линейно к минимуму растет при увеличении размерности пространства. во-вторых,
[50:01.420 --> 50:06.100]  потому что есть то самое проклятие размерности, где у вас количество равноудаленных точек растет
[50:06.100 --> 50:11.380]  очень быстро при увеличении размерности пространства. Поэтому многомерных пространств у нас проблемы с
[50:11.380 --> 50:18.340]  метрическими алгоритмами, например. Плюс у нас с вами и достаточно непонятно, как визуализировать
[50:18.340 --> 50:22.620]  10-тыщмерное пространство. Вот нам хочется週тами на картинку посмотреть, что с ним делать, неясно. И
[50:22.620 --> 50:27.320]  некоторые модельки на больших размерностях вообще не работают, а на малых достаточно неплохо debates
[50:27.320 --> 50:29.940]  Но тот же самый КНН, если у вас размер пространства
[50:29.940 --> 50:33.040]  миллион, скорее всего не заработает вообще.
[50:33.040 --> 50:34.840]  На 10-мерном пространстве уже заработает.
[50:34.840 --> 50:37.680]  Можем попытаться снижать.
[50:37.680 --> 50:40.480]  Здесь можно вспомнить чуть-чуть, совсем чуть-чуть линал.
[50:40.480 --> 50:43.000]  Вот помните, были разные матричные разложения.
[50:43.000 --> 50:46.000]  Там УВ-разложения, разложения Халецкого.
[50:46.000 --> 50:48.120]  Нет, сингулярное разложение.
[50:48.120 --> 50:50.680]  Вот сингулярное, хотя бы помните, слава богу.
[50:50.680 --> 50:52.320]  То есть, в общем случае, мы что хотим?
[50:52.320 --> 50:55.040]  У нас есть матрица размером L на D.
[50:55.160 --> 50:57.160]  В общем случае, это любая матрица.
[50:57.160 --> 50:59.160]  В нашем конкретном случае, как вы понимаете,
[50:59.160 --> 51:01.160]  это матрица объект-признак, она же матрица плана.
[51:01.160 --> 51:04.160]  Мы хотим снизить размерность каким образом?
[51:04.160 --> 51:09.160]  Мы хотим ее перевести к матрице L на K, на K на D.
[51:09.160 --> 51:11.160]  Смотрите, у вас L на D и K на D.
[51:11.160 --> 51:13.160]  У вас два члена.
[51:13.160 --> 51:16.160]  Первая из них, это, собственно, теперь К-мерная матрица,
[51:16.160 --> 51:19.160]  которая у вас исключительно, точнее, количество столбцов K.
[51:19.160 --> 51:22.160]  Только K-признаков у вас осталось из D.
[51:22.160 --> 51:24.160]  А вторая это уже матрица перехода.
[51:24.280 --> 51:26.280]  Я ее, по сути, может потом выкину.
[51:36.280 --> 51:40.280]  Потому что у вас матрица V изначально, она тоже скорее...
[51:40.280 --> 51:43.280]  Короче, это уже транспонированная матрица нарисована.
[51:47.280 --> 51:50.280]  Да, V это D на K, а вы транспонированный, это K на D.
[51:50.400 --> 51:53.400]  Да и, в принципе, можете как угодно, главное,
[51:53.400 --> 51:56.400]  чтобы у вас размерности впадали, у вас L на K,
[51:56.400 --> 51:59.400]  на K на D должно быть, чтобы L на D получился.
[51:59.400 --> 52:02.400]  Просто оригинальная запись, опять же, это U на V транспонированный.
[52:04.400 --> 52:05.400]  Хорошо?
[52:05.400 --> 52:07.400]  И, соответственно, задача, как правило,
[52:07.400 --> 52:10.400]  когда мы ищем матричное разложение, матричную декомпозицию,
[52:10.400 --> 52:12.400]  минимизировать какую-нибудь невязку.
[52:12.400 --> 52:14.400]  Ну, как правило, это норма Фробениуса.
[52:14.400 --> 52:16.400]  Что такое норма Фробениуса, все помнят?
[52:16.400 --> 52:19.400]  Сумма квадратов отклонений по всем элементам.
[52:19.520 --> 52:21.520]  По сути, мосье, но на матричке.
[52:23.520 --> 52:25.520]  Вот, все, расписали.
[52:25.520 --> 52:28.520]  Но, в общем случае, скажем так, разложений бывает много разных.
[52:28.520 --> 52:30.520]  Сюда краски и разложений Халецкого,
[52:30.520 --> 52:33.520]  и UV-разложений относятся, и сингулярные разложения,
[52:33.520 --> 52:36.520]  и там еще десяток, наверное, вы в каких-нибудь учебниках
[52:36.520 --> 52:39.520]  по линалу и по внезапно финансовому моделированию найдете.
[52:39.520 --> 52:41.520]  Финансисты любят все эти разложения.
[52:41.520 --> 52:44.520]  Мы же с вами давайте поговорим про сингулярное разложение.
[52:44.520 --> 52:46.520]  Что такое сингулярное разложение?
[52:46.520 --> 52:47.520]  Вот в двух словах.
[52:47.640 --> 52:49.640]  Все помнят, или никто не помнит, или как?
[52:52.640 --> 52:53.640]  А кто помнит?
[52:55.640 --> 52:57.640]  К бою собственных векторов, в какой матрице?
[53:01.640 --> 53:02.640]  Хорошо.
[53:02.640 --> 53:05.640]  Какой вот там что-то про сопряженное было?
[53:05.640 --> 53:07.640]  Какой, какой, самый сопряженный?
[53:10.640 --> 53:11.640]  О, супер!
[53:11.640 --> 53:12.640]  Классно.
[53:12.640 --> 53:14.640]  А зачем нам А на сопряженную отмножать?
[53:17.640 --> 53:18.640]  Бинго.
[53:18.640 --> 53:19.640]  Смотрите.
[53:19.640 --> 53:21.640]  Давайте тогда вспомним для начала.
[53:21.640 --> 53:22.640]  Первое.
[53:22.640 --> 53:25.640]  Мы с вами про разложение по собственным векторам.
[53:25.640 --> 53:27.640]  Можем говорить для любой матрицы,
[53:27.640 --> 53:30.640]  или у нас все-таки переход к байсу собственных векторов
[53:30.640 --> 53:32.640]  не для всех матриц доступен?
[53:35.640 --> 53:36.640]  Давайте так скажем.
[53:36.640 --> 53:40.640]  У вас в байсе, который состоит из собственных векторов,
[53:40.640 --> 53:42.640]  матрица какой вид имеет?
[53:42.640 --> 53:43.640]  Диагональный.
[53:43.640 --> 53:46.640]  Все ли матрицы можно привести к диагональному виду?
[53:47.640 --> 53:48.640]  Какие нельзя?
[53:50.640 --> 53:52.640]  Выраженные раз, а еще-таки?
[53:53.640 --> 53:54.640]  Какие?
[53:55.640 --> 53:57.640]  Слушайте, я вас не слышу.
[53:57.640 --> 53:58.640]  Неквадратные.
[53:58.640 --> 53:59.640]  Неквадратные, супер.
[53:59.640 --> 54:01.640]  Если у вас матрица неквадратная,
[54:01.640 --> 54:03.640]  то вы априори не сможете ее в диагональный вид привести
[54:03.640 --> 54:05.640]  в то, что вас непонятно, что здесь нет.
[54:05.640 --> 54:07.640]  На самом деле у вас там есть еще ограничение,
[54:07.640 --> 54:10.640]  что она должна быть неотрицательной квадратической формой.
[54:10.640 --> 54:12.640]  Короче, можно умножить матрицу на сопряженную.
[54:12.640 --> 54:14.640]  Ту же самую матрицу.
[54:14.640 --> 54:16.640]  Х, ТХ, например.
[54:16.760 --> 54:17.760]  И краски.
[54:17.760 --> 54:19.760]  Тогда у вас получится уже матрица,
[54:19.760 --> 54:21.760]  которая явно диагональная,
[54:21.760 --> 54:23.760]  соответственно к некоторой квадратической форме,
[54:23.760 --> 54:24.760]  неотрицательной.
[54:24.760 --> 54:26.760]  Поэтому ее можно разложить по собственным векторам.
[54:26.760 --> 54:28.760]  Шаг номер два.
[54:28.760 --> 54:30.760]  Как вы уже сказали, в байсе из собственных векторов
[54:30.760 --> 54:32.760]  у нас матрица имеет диагональный вид, правильно?
[54:32.760 --> 54:34.760]  А теперь тогда давайте вопрос.
[54:34.760 --> 54:36.760]  Пусть у нас все собственные вектора теперь отнормированы,
[54:36.760 --> 54:38.760]  мы же их можем отнормировать,
[54:38.760 --> 54:40.760]  чтобы у них была одничная норма.
[54:40.760 --> 54:42.760]  Тогда, соответственно, что у нас будет означать
[54:42.760 --> 54:44.760]  каждый член вот на диагонале
[54:44.760 --> 54:46.760]  этой самой матрицы
[54:46.760 --> 54:48.760]  в байсе собственных векторов?
[54:48.760 --> 54:50.760]  Чего?
[54:50.760 --> 54:52.760]  Собственные значения. Классно.
[54:52.760 --> 54:54.760]  Там диагональ ставит собственные значения.
[54:54.760 --> 54:56.760]  Это вообще понятно? Нет? А то, кажется, у меня тут уже
[54:56.760 --> 54:58.760]  выразилась какая-то группа из девяти человек,
[54:58.760 --> 55:00.760]  с которыми я веду беседу. Я не хочу так делать,
[55:00.760 --> 55:02.760]  я хочу всю аудиторию не терять.
[55:02.760 --> 55:04.760]  Коллеги, вам там сзади понятно?
[55:04.760 --> 55:06.760]  Хорошо.
[55:06.760 --> 55:08.760]  Классно. Собственные значения, они
[55:08.760 --> 55:10.760]  что-нибудь вам показывают, или это просто какие-то
[55:10.760 --> 55:12.760]  волшебные числа, которые ни о чем нам больше не говорят?
[55:14.760 --> 55:16.760]  Вот, еще раз погромче.
[55:18.760 --> 55:20.760]  О, классно.
[55:20.760 --> 55:22.760]  Классно.
[55:22.760 --> 55:24.760]  Супер.
[55:24.760 --> 55:26.760]  А это что не значит физически?
[55:30.760 --> 55:32.760]  О, вот, замечательно.
[55:32.760 --> 55:34.760]  Смотрите, мы с вами
[55:34.760 --> 55:36.760]  можем вспомнить или разложение по собственным векторам,
[55:36.760 --> 55:38.760]  или в принципе подумать,
[55:38.760 --> 55:40.760]  если у нас матрица с вами
[55:40.760 --> 55:42.760]  в байсе собственных векторов переходит к виду
[55:42.760 --> 55:44.760]  диагональному, правильно?
[55:44.760 --> 55:46.760]  А каждый вектор
[55:46.760 --> 55:48.760]  теперь нормированный,
[55:48.760 --> 55:50.760]  собственные числа,
[55:50.760 --> 55:52.760]  которые стоят на диагонали,
[55:52.760 --> 55:54.760]  должны явно показывать, насколько далеко
[55:54.760 --> 55:56.760]  вдоль каждого из направлений у нас вытянута
[55:56.760 --> 55:58.760]  наша матрица. Согласны?
[55:58.760 --> 56:00.760]  Собственно, это на самом деле для нас очень важно.
[56:00.760 --> 56:02.760]  Так вот, давайте-ка теперь
[56:02.760 --> 56:04.760]  возьмем матричку и перейдем
[56:04.760 --> 56:06.760]  к ее скалярному
[56:06.760 --> 56:08.760]  разложению, ой, сингулярному,
[56:08.760 --> 56:10.760]  разложению каким образом? Давайте матрицу
[56:10.760 --> 56:12.760]  разложим на три вот таких вот
[56:12.760 --> 56:14.760]  матрицы.
[56:14.760 --> 56:16.760]  У нас будет У, Сигма и В транспонированы.
[56:16.760 --> 56:18.760]  Что такое У, Сигма и В?
[56:18.760 --> 56:20.760]  На самом деле У это будет
[56:20.760 --> 56:22.760]  матрица артагональная,
[56:22.760 --> 56:24.760]  Сигма это будет матрица диагональная,
[56:24.760 --> 56:26.760]  а В опять же будет матрица
[56:26.760 --> 56:28.760]  артагональная. Во-первых, что такое
[56:28.760 --> 56:30.760]  артагональная матрица? Все помнят?
[56:30.760 --> 56:32.760]  Что?
[56:32.760 --> 56:34.760]  Не, У-то не квадрат.
[56:34.760 --> 56:36.760]  Почему?
[56:36.760 --> 56:38.760]  Что с ней не так?
[56:38.760 --> 56:40.760]  Артагональная матрица не квадратная?
[56:44.760 --> 56:46.760]  Да.
[56:48.760 --> 56:50.760]  У вас всего лишь ограничение, что
[56:50.760 --> 56:52.760]  У на У транспонированная должна
[56:52.760 --> 56:54.760]  быть
[56:54.760 --> 56:56.760]  единичной матрицей.
[56:56.760 --> 56:58.760]  Ну, единичной в смысле
[56:58.760 --> 57:00.760]  айдент и трансформ на диагонале.
[57:00.760 --> 57:02.760]  Короче, ладно, кажется,
[57:02.760 --> 57:04.760]  мы начинаем вас терять.
[57:04.760 --> 57:06.760]  Давайте разложим матрицу на произведение трех.
[57:06.760 --> 57:08.760]  Артагональная, диагональная, еще раз артагональная.
[57:08.760 --> 57:10.760]  Хорошо?
[57:10.760 --> 57:12.760]  В принципе, можем так сделать.
[57:12.760 --> 57:14.760]  Тогда давайте сразу зададимся двумя вопросами.
[57:14.760 --> 57:16.760]  Первое, раз артагональная матрица
[57:16.760 --> 57:18.760]  артагональная, у нее детерминат какой?
[57:18.760 --> 57:20.760]  Один. Все это понимают?
[57:20.760 --> 57:22.760]  Точно?
[57:24.760 --> 57:26.760]  Окей, согласен.
[57:26.760 --> 57:28.760]  Если у нас ориентация не имеет,
[57:28.760 --> 57:30.760]  всегда должен быть один.
[57:30.760 --> 57:32.760]  Норм детермината точно единица.
[57:32.760 --> 57:34.760]  А теперь можем вспомнить абсолютно простую
[57:34.760 --> 57:36.760]  базовую вещь. Если у нас с вами матрица
[57:36.760 --> 57:38.760]  имеет детерминат
[57:38.760 --> 57:40.760]  равный единице,
[57:40.760 --> 57:42.760]  то какое преобразование линейного пространства оно
[57:42.760 --> 57:44.760]  сдает? Матрица это же при этом линейный оператор,
[57:44.760 --> 57:46.760]  правильно? Поворот.
[57:46.760 --> 57:48.760]  Только поворот, правильно?
[57:48.760 --> 57:50.760]  Все с этим согласны?
[57:50.760 --> 57:52.760]  Получается у нас с вами
[57:52.760 --> 57:54.760]  линейное преобразование любое,
[57:54.760 --> 57:56.760]  это поворот, растяжение, сжатие.
[57:56.760 --> 57:58.760]  Если матрица артагональная, то она
[57:58.760 --> 58:00.760]  сдает только поворот.
[58:00.760 --> 58:02.760]  У нас две артагональных матрицы раз-два,
[58:02.760 --> 58:04.760]  они сдают повороты.
[58:04.760 --> 58:06.760]  Сигма у нас будет краской отвечать за растяжение сжатия.
[58:06.760 --> 58:08.760]  В общем случае. Согласны?
[58:08.760 --> 58:10.760]  Супер.
[58:10.760 --> 58:12.760]  Ну и теперь давайте краске
[58:12.760 --> 58:14.760]  попробуем понять, что происходит.
[58:14.760 --> 58:16.760]  Что по факту мы пытаемся сделать?
[58:16.760 --> 58:18.760]  Мы пытаемся найти такое
[58:18.760 --> 58:20.760]  линейное отображение,
[58:20.760 --> 58:22.760]  которое позволяет нам выразить
[58:22.760 --> 58:24.760]  теперь все наши точки
[58:24.760 --> 58:26.760]  в каком-то новом байсе
[58:26.760 --> 58:28.760]  и при этом наша исходная матрица
[58:28.760 --> 58:30.760]  может быть представлена как произведение трех.
[58:30.760 --> 58:32.760]  Артагональная У, диагональная Сигма
[58:32.760 --> 58:34.760]  и опять же артагональная В.
[58:34.760 --> 58:36.760]  Мы можем это сделать
[58:36.760 --> 58:38.760]  с помощью рингулярного разложения,
[58:38.760 --> 58:40.760]  и тогда у нас матрицы У и В краски
[58:40.760 --> 58:42.760]  порождаются откуда. Мы берем матрицу Х,
[58:42.760 --> 58:44.760]  умножаем на сопряженную к самой себе,
[58:44.760 --> 58:46.760]  получаем квадратичную форму,
[58:46.760 --> 58:48.760]  ищем там собственные вектора
[58:48.760 --> 58:50.760]  и собственные соответствия значения.
[58:50.760 --> 58:52.760]  И оттуда мы к краске получаем
[58:52.760 --> 58:54.760]  данное разложение.
[58:54.760 --> 58:56.760]  В чем плюс на самом деле такого разложения?
[58:56.760 --> 58:58.760]  Во-первых, есть замечательная
[58:58.760 --> 59:00.760]  теорема. Сначала я вам скажу теорему,
[59:00.760 --> 59:02.760]  потому что она красивая,
[59:02.760 --> 59:04.760]  потом я скажу дровой смык, который под ней лежит.
[59:04.760 --> 59:06.760]  Есть замечательная теорема Экарта Янга,
[59:06.760 --> 59:08.760]  которая говорит, что вот такое разложение
[59:08.760 --> 59:10.760]  краски СВД.
[59:10.760 --> 59:12.760]  Если вы посмотрите на него внимательно,
[59:12.760 --> 59:14.760]  вы поймете, что у вас первый к столбцов
[59:14.760 --> 59:16.760]  из матрицы У, они соответствуют
[59:16.760 --> 59:18.760]  только первым к элементам из матрицы Сигма
[59:18.760 --> 59:20.760]  и первым к строкам
[59:20.760 --> 59:22.760]  из матрицы В. Согласны?
[59:22.760 --> 59:24.760]  Потому что у нас всегда умножается
[59:24.760 --> 59:26.760]  строка на столбец.
[59:26.760 --> 59:28.760]  Если вы хотите снизить размерность
[59:28.760 --> 59:30.760]  вашего пространства, так как у вас
[59:30.760 --> 59:32.760]  это исходная матрица, а это
[59:32.760 --> 59:34.760]  ее представление уже
[59:34.760 --> 59:36.760]  в коммерном подпространстве,
[59:36.760 --> 59:38.760]  вы можете взять
[59:38.760 --> 59:40.760]  миноры ранга К
[59:40.760 --> 59:42.760]  из этих всех трех матриц, первый к столбцов,
[59:42.760 --> 59:44.760]  первые к диагональных элементов
[59:44.760 --> 59:46.760]  и первый к строк, и получить
[59:46.760 --> 59:48.760]  коммерную аппроксимацию вашей исходной матрицы.
[59:48.760 --> 59:50.760]  Согласились?
[59:50.760 --> 59:52.760]  Это называют truncated SVD,
[59:52.760 --> 59:54.760]  обрезанные SVD, по-русски, видимо.
[59:54.760 --> 59:56.760]  И выкидываем все, кроме первых к членов.
[59:58.760 --> 01:00:00.760]  А вот это я сейчас поправлю.
[01:00:00.760 --> 01:00:02.760]  А теперь давайте вспомним.
[01:00:02.760 --> 01:00:04.760]  Раз матрица артагональная, у нее все
[01:00:04.760 --> 01:00:06.760]  векторы что?
[01:00:06.760 --> 01:00:08.760]  Артагональны друг к другу.
[01:00:08.760 --> 01:00:10.760]  Согласны? Все строки, которые у меня есть.
[01:00:10.760 --> 01:00:12.760]  Или в данном случае все столбцы.
[01:00:12.760 --> 01:00:14.760]  Раз так, значит
[01:00:14.760 --> 01:00:16.760]  диагональная матрица. У нее тоже все
[01:00:16.760 --> 01:00:18.760]  векторы артагональны друг к другу
[01:00:18.760 --> 01:00:20.760]  по определению. Соответственно, третья матрица
[01:00:20.760 --> 01:00:22.760]  опять-таки артагональна, все векторы опять-таки
[01:00:22.760 --> 01:00:24.760]  артагональны друг к другу. Верно ли, что мы с вами
[01:00:24.760 --> 01:00:26.760]  можем перетасовать векторы местами
[01:00:26.760 --> 01:00:28.760]  в любом формате? В любом порядке,
[01:00:28.760 --> 01:00:30.760]  лишь бы это во всех трех матрицах одновременно
[01:00:30.760 --> 01:00:32.760]  происходило. Результат не поменяется,
[01:00:32.760 --> 01:00:34.760]  правильно?
[01:00:34.760 --> 01:00:36.760]  Все согласны?
[01:00:38.760 --> 01:00:40.760]  Если я в матрице У
[01:00:40.760 --> 01:00:42.760]  поменяю первый-второй вектор местами,
[01:00:42.760 --> 01:00:44.760]  в матрице Сигма поменяю
[01:00:44.760 --> 01:00:46.760]  первый-вторую элементу местами,
[01:00:46.760 --> 01:00:48.760]  и здесь тоже поменяю местами, ничего не
[01:00:48.760 --> 01:00:50.760]  поменяется. Верно?
[01:00:50.760 --> 01:00:52.760]  Да, в исходное ничего не поменяется, конечно же,
[01:00:52.760 --> 01:00:54.760]  речами на ней.
[01:00:54.760 --> 01:00:56.760]  Классно.
[01:00:56.760 --> 01:00:58.760]  А теперь давайте
[01:00:58.760 --> 01:01:00.760]  пересортируем все наши
[01:01:00.760 --> 01:01:02.760]  объекты каким образом? Все наши точки.
[01:01:02.760 --> 01:01:04.760]  Так, чтобы у нас на диагонали
[01:01:04.760 --> 01:01:06.760]  матрицы Сигма они строго не
[01:01:06.760 --> 01:01:08.760]  возрастали.
[01:01:08.760 --> 01:01:10.760]  То есть у нас слева сверху самые
[01:01:10.760 --> 01:01:12.760]  большие элементы, справа снизу самые
[01:01:12.760 --> 01:01:14.760]  маленькие. Хорошо?
[01:01:14.760 --> 01:01:16.760]  Тогда мы с вами, соответственно, можем что сделать?
[01:01:16.760 --> 01:01:18.760]  Чего?
[01:01:20.760 --> 01:01:22.760]  По модулю, да. По абсолютному значению.
[01:01:22.760 --> 01:01:24.760]  Вот. И тогда мы, соответственно,
[01:01:24.760 --> 01:01:26.760]  видим, что у нас чем дальше
[01:01:26.760 --> 01:01:28.760]  влез, чем ниже мы сюда переходим,
[01:01:28.760 --> 01:01:30.760]  тем меньше у нас вот это самое
[01:01:30.760 --> 01:01:32.760]  сингулярное значение находится.
[01:01:32.760 --> 01:01:34.760]  Почему нам это важно? Но теперь мы с вами можем
[01:01:34.760 --> 01:01:36.760]  отбросить все кроме первых
[01:01:36.760 --> 01:01:38.760]  к, которые соответствуют максимальным
[01:01:38.760 --> 01:01:40.760]  элементам. Правильно?
[01:01:40.760 --> 01:01:42.760]  Здесь только что тут вспоминали про собственные
[01:01:42.760 --> 01:01:44.760]  векторы, собственные числа, показывают нам, насколько
[01:01:44.760 --> 01:01:46.760]  вдоль соответствующего вектора мы растянуты.
[01:01:46.760 --> 01:01:48.760]  На самом деле, эти направления
[01:01:48.760 --> 01:01:50.760]  не просто так там выбраны, они соответствуют максимальной
[01:01:50.760 --> 01:01:52.760]  дисперсии в исходном презинговом пространстве.
[01:01:52.760 --> 01:01:54.760]  А теперь давайте я вам покажу,
[01:01:54.760 --> 01:01:56.760]  как это все на практике работает.
[01:01:56.760 --> 01:01:58.760]  Ну, в смысле, что это такое в реальности.
[01:01:58.760 --> 01:02:00.760]  Вот вам облакоточка. Логично, что все эти
[01:02:00.760 --> 01:02:02.760]  точки мы можем с вами в матричку уложить.
[01:02:02.760 --> 01:02:04.760]  Надо будет матричка там, не знаю, 100 на 2.
[01:02:04.760 --> 01:02:06.760]  Я не знаю, сколько тут точек.
[01:02:06.760 --> 01:02:08.760]  Мы с вами хотим описать теперь
[01:02:08.760 --> 01:02:10.760]  все точки с помощью одной координаты.
[01:02:10.760 --> 01:02:12.760]  И при этом минимизировать
[01:02:12.760 --> 01:02:14.760]  норму фробениуса невязкие, то есть
[01:02:14.760 --> 01:02:16.760]  сумму квадратов отклонений.
[01:02:16.760 --> 01:02:18.760]  Теперь вы все точки
[01:02:18.760 --> 01:02:20.760]  опроксимируете только одной чиселкой вместо двух.
[01:02:20.760 --> 01:02:22.760]  И пытайтесь каким-то образом...
[01:02:22.760 --> 01:02:24.760]  Опять же, эта одна
[01:02:24.760 --> 01:02:26.760]  чиселка, она не обязана быть в исходных
[01:02:26.760 --> 01:02:28.760]  координатах. Вы можете выбрать любое
[01:02:28.760 --> 01:02:30.760]  направление, на которое вы это спроецируете.
[01:02:30.760 --> 01:02:32.760]  И теперь у вас все точки лежат на этой прямой.
[01:02:32.760 --> 01:02:34.760]  Вы должны это сделать так, чтобы ошибка была у вас
[01:02:34.760 --> 01:02:36.760]  наименьшей. Вот глядя вот сюда,
[01:02:36.760 --> 01:02:38.760]  где у вас будет прямая, на которую
[01:02:38.760 --> 01:02:40.760]  надо все точки спроецировать, чтобы была наименьшая ошибка.
[01:02:40.760 --> 01:02:42.760]  Чёрная дрель на стрелочке.
[01:02:42.760 --> 01:02:44.760]  Большая полуость нашего эллипса, правильно?
[01:02:44.760 --> 01:02:46.760]  Вы это сказали интуитивно.
[01:02:46.760 --> 01:02:48.760]  Почему это так происходит?
[01:02:48.760 --> 01:02:50.760]  Потому что у вас эллипс в эту сторону
[01:02:50.760 --> 01:02:52.760]  растянут сильно, в эту сторону слабо.
[01:02:52.760 --> 01:02:54.760]  Здесь у нас отклонения от
[01:02:54.760 --> 01:02:56.760]  полуоси поменьше, значит должно быть лучше.
[01:02:56.760 --> 01:02:58.760]  Правильно?
[01:02:58.760 --> 01:03:00.760]  А теперь давайте это скажем более ямко.
[01:03:00.760 --> 01:03:02.760]  Что такое норма фробениус вообще?
[01:03:02.760 --> 01:03:04.760]  Это сумма квадратов отклонений, правильно?
[01:03:04.760 --> 01:03:06.760]  Причём квадратов отклонений,
[01:03:06.760 --> 01:03:08.760]  которые у нас есть.
[01:03:08.760 --> 01:03:10.760]  Квадратов отклонений, правильно?
[01:03:10.760 --> 01:03:12.760]  Причём квадратов отклонений от среднего.
[01:03:12.760 --> 01:03:14.760]  А что такое квадрат отклонений
[01:03:14.760 --> 01:03:16.760]  от среднего в общем случае?
[01:03:16.760 --> 01:03:18.760]  На дисперсию очень похоже.
[01:03:18.760 --> 01:03:20.760]  Это и есть дисперсия.
[01:03:20.760 --> 01:03:22.760]  Вы берёте это, минус это
[01:03:22.760 --> 01:03:24.760]  в квадрате. Это минус среднее в квадрате.
[01:03:24.760 --> 01:03:26.760]  По сути у вас
[01:03:26.760 --> 01:03:28.760]  главные компоненты всегда
[01:03:28.760 --> 01:03:30.760]  являются направлением наибольшей
[01:03:30.760 --> 01:03:32.760]  дисперсии в вашем облаке точек.
[01:03:32.760 --> 01:03:34.760]  Это может быть абсолютно сложно, непонятно
[01:03:34.760 --> 01:03:36.760]  облако точек, но во-первых, вы всегда
[01:03:36.760 --> 01:03:38.760]  можете найти, в любом случае.
[01:03:38.760 --> 01:03:40.760]  А во-вторых, вы можете найти вдоль какого направления у вас дисперсия
[01:03:40.760 --> 01:03:42.760]  будет наибольшей.
[01:03:42.760 --> 01:03:44.760]  Всё, раз вы нашли направление наибольшей дисперсии,
[01:03:44.760 --> 01:03:46.760]  это ваш главный компонент.
[01:03:46.760 --> 01:03:48.760]  После чего происходит что?
[01:03:48.760 --> 01:03:50.760]  На самом деле, этому и соответствует
[01:03:50.760 --> 01:03:52.760]  первый вектор вот из этой матрицы.
[01:03:52.760 --> 01:03:54.760]  Он, кстати, отвечает за поворот в этом направлении.
[01:03:54.760 --> 01:03:56.760]  А первая чиселка говорит, насколько большая дисперсия
[01:03:56.760 --> 01:03:58.760]  у вас вдоль этого направления.
[01:03:58.760 --> 01:04:00.760]  Всё.
[01:04:00.760 --> 01:04:02.760]  Теперь вы говорите, что первый член у вас
[01:04:02.760 --> 01:04:04.760]  вашего разложения отвечает
[01:04:04.760 --> 01:04:06.760]  на направление наибольшей дисперсии.
[01:04:06.760 --> 01:04:08.760]  Вы уже его полностью описали,
[01:04:08.760 --> 01:04:10.760]  и теперь у вас остаются только невязки с этим представлением.
[01:04:10.760 --> 01:04:12.760]  Что получается?
[01:04:12.760 --> 01:04:14.760]  У вас остается какое-то ваше представление точек,
[01:04:14.760 --> 01:04:16.760]  вы можете выбрать второе направление
[01:04:16.760 --> 01:04:18.760]  наибольшей дисперсии.
[01:04:18.760 --> 01:04:20.760]  Логично, что он должен быть ордигонально первому.
[01:04:20.760 --> 01:04:22.760]  Опять же, описать все точки в его пространстве
[01:04:22.760 --> 01:04:24.760]  это второй столбец, второе значение.
[01:04:24.760 --> 01:04:26.760]  И так далее.
[01:04:26.760 --> 01:04:28.760]  По сути, мед главный компонент говорит,
[01:04:28.760 --> 01:04:30.760]  первые к главных компонент описывают первые к направлению
[01:04:30.760 --> 01:04:32.760]  наибольшей дисперсии,
[01:04:32.760 --> 01:04:34.760]  поэтому он дает вам наименьшую ошибку
[01:04:34.760 --> 01:04:36.760]  в камерном пространстве
[01:04:36.760 --> 01:04:38.760]  по нормам Фрабидиуса. Логично.
[01:04:38.760 --> 01:04:40.760]  Мы первые к компонент использовали, чтобы
[01:04:40.760 --> 01:04:42.760]  забрать первые к направлению, которые дают
[01:04:42.760 --> 01:04:44.760]  наибольшую ошибку при восстановлении.
[01:04:44.760 --> 01:04:46.760]  Логично, что если мы наибольшую ошибку покрыли,
[01:04:46.760 --> 01:04:48.760]  все остальные представления будут давать ошибку
[01:04:48.760 --> 01:04:50.760]  как минимум...
[01:04:50.760 --> 01:04:52.760]  Скажем так, наибольшую ошибку мы покрыли,
[01:04:52.760 --> 01:04:54.760]  значит, если бы мы кого-то из них не использовали,
[01:04:54.760 --> 01:04:56.760]  ошибка была бы еще больше.
[01:04:56.760 --> 01:04:58.760]  Вот.
[01:04:58.760 --> 01:05:00.760]  Никаких.
[01:05:00.760 --> 01:05:02.760]  Они...
[01:05:10.760 --> 01:05:12.760]  Ну, смотрите, тут возникает два вопроса.
[01:05:12.760 --> 01:05:14.760]  Первый, когда у вас есть направление наибольшей дисперсии
[01:05:14.760 --> 01:05:16.760]  единственное, оно всегда,
[01:05:16.760 --> 01:05:18.760]  если оно единственное, то это есть главный компонент.
[01:05:18.760 --> 01:05:20.760]  Второй, если у вас есть несколько
[01:05:20.760 --> 01:05:22.760]  одинаковых направлений, ну, простейший случай
[01:05:22.760 --> 01:05:24.760]  это центральная симметрия, представь себе
[01:05:24.760 --> 01:05:26.760]  равномерно заполненный точками шар.
[01:05:26.760 --> 01:05:28.760]  У вас любое направление наибольшей дисперсии.
[01:05:28.760 --> 01:05:30.760]  Ну, в таком случае вы можете
[01:05:30.760 --> 01:05:32.760]  выбрать любое из этих направлений,
[01:05:32.760 --> 01:05:34.760]  а все остальные должны быть не мордбинальны.
[01:05:34.760 --> 01:05:36.760]  То есть, у вас нет никакого ограничения,
[01:05:36.760 --> 01:05:38.760]  если у вас несколько направлений
[01:05:38.760 --> 01:05:40.760]  наибольшей дисперсии,
[01:05:40.760 --> 01:05:42.760]  то вас устраивает любое.
[01:05:42.760 --> 01:05:44.760]  Если они все одинаковые дисперсии имеют,
[01:05:44.760 --> 01:05:46.760]  значит, любое не лучше, чем остальные.
[01:05:46.760 --> 01:05:48.760]  Все.
[01:05:48.760 --> 01:05:50.760]  Вот.
[01:05:50.760 --> 01:05:52.760]  Ну и, собственно, эта теорема, на самом деле, называется
[01:05:52.760 --> 01:05:54.760]  теорема Экарта-Янга и говорит на простое.
[01:05:54.760 --> 01:05:56.760]  Если вы хотите построить
[01:05:56.760 --> 01:05:58.760]  аппроксимацию матрицы
[01:05:58.760 --> 01:06:00.760]  другой матрицы, но ранга К,
[01:06:00.760 --> 01:06:02.760]  то оптимальную
[01:06:02.760 --> 01:06:04.760]  аппроксимацию, с точки зрения
[01:06:04.760 --> 01:06:06.760]  наименьшей нормы Фрагениуса,
[01:06:06.760 --> 01:06:08.760]  среди линейной краски дает вам
[01:06:08.760 --> 01:06:10.760]  вот это вот самое PCA,
[01:06:10.760 --> 01:06:12.760]  которое строится через скалярное продолжение.
[01:06:12.760 --> 01:06:14.760]  Все.
[01:06:16.760 --> 01:06:18.760]  Смотрите, у вас есть исходная матрица А,
[01:06:18.760 --> 01:06:20.760]  вот, исходная матрица А.
[01:06:20.760 --> 01:06:22.760]  Да, просто матрица объект Призник.
[01:06:22.760 --> 01:06:24.760]  Вы говорите, я теперь хочу описать
[01:06:24.760 --> 01:06:26.760]  свои данные только камерным
[01:06:26.760 --> 01:06:28.760]  представлением, то есть не L признаков,
[01:06:28.760 --> 01:06:30.760]  а K признаков.
[01:06:30.760 --> 01:06:32.760]  Отверждение, что если вы эту новую матрицу
[01:06:32.760 --> 01:06:34.760]  построите вот таким образом, как
[01:06:34.760 --> 01:06:36.760]  построите U, Сигма,
[01:06:36.760 --> 01:06:38.760]  СВД-разложение оригинальной матрицы,
[01:06:38.760 --> 01:06:40.760]  а потом возьмете только первые K
[01:06:40.760 --> 01:06:42.760]  столбцов из У, первые
[01:06:42.760 --> 01:06:44.760]  коэлементов из Сигмы, короче, обрежете
[01:06:44.760 --> 01:06:46.760]  все кроме матрицы ранга К
[01:06:46.760 --> 01:06:48.760]  и их между собой точно так же перемянуете.
[01:06:48.760 --> 01:06:50.760]  Что такое обрежете? Вы их, на самом деле,
[01:06:50.760 --> 01:06:52.760]  можете оставить, просто-напросто
[01:06:52.760 --> 01:06:54.760]  и все остальное нулями забить, и все.
[01:06:54.760 --> 01:06:56.760]  То же самое получится.
[01:06:56.760 --> 01:06:58.760]  То у вас получится матрица АК,
[01:06:58.760 --> 01:07:00.760]  которая уже будет ранга К, она будет только камерная.
[01:07:00.760 --> 01:07:02.760]  И при этом вы потеряете
[01:07:02.760 --> 01:07:04.760]  наименьшее количество информации относительно
[01:07:04.760 --> 01:07:06.760]  оригинальной вашей матрики.
[01:07:12.760 --> 01:07:14.760]  Вот, смотрите, что такое. АК – это ваша
[01:07:14.760 --> 01:07:16.760]  аппроксимация ранга К через СВД.
[01:07:16.760 --> 01:07:18.760]  Для любой другой
[01:07:18.760 --> 01:07:20.760]  аппроксимации ранга К
[01:07:20.760 --> 01:07:22.760]  верно, что разница
[01:07:22.760 --> 01:07:24.760]  между А и БК всегда
[01:07:24.760 --> 01:07:26.760]  не меньше, чем разница между
[01:07:26.760 --> 01:07:28.760]  А и АК. То есть АК – это
[01:07:28.760 --> 01:07:30.760]  наилучшая аппроксимация с точки зрения нормы фробениз.
[01:07:30.760 --> 01:07:32.760]  Нормы фробениз – сумма квадратов отклонений.
[01:07:36.760 --> 01:07:38.760]  Ну, у вас две матрицы взяли по элементам вычисления.
[01:07:38.760 --> 01:07:40.760]  Теперь у вас каждая есть невязка,
[01:07:40.760 --> 01:07:42.760]  вы завели в квадрат, потому что вас абсолютное значение
[01:07:42.760 --> 01:07:44.760]  волнует, но в квадрате. Вот.
[01:07:44.760 --> 01:07:46.760]  Вот, все.
[01:07:50.760 --> 01:07:52.760]  Ну, смотрите, вы просто можете, если у вас
[01:07:52.760 --> 01:07:54.760]  упорядочно все, то первые K
[01:07:54.760 --> 01:07:56.760]  сингулярно значения им соответствуют
[01:07:56.760 --> 01:07:58.760]  то, что вам надо. Все остальное вы просто заменяете нулями.
[01:07:58.760 --> 01:08:00.760]  Ну, заменяете нулями, потому что иначе
[01:08:00.760 --> 01:08:02.760]  у вас размеры с матриц будут не совпадать, и у вас
[01:08:02.760 --> 01:08:04.760]  начнется проблема. По факту мы понимаем,
[01:08:04.760 --> 01:08:06.760]  что если у нас там стоит 0, все артагонально,
[01:08:06.760 --> 01:08:08.760]  мы можем ничего дальше не считать, дальше
[01:08:08.760 --> 01:08:10.760]  просто будут нулевые столцы. Логично?
[01:08:10.760 --> 01:08:12.760]  Хорошо.
[01:08:12.760 --> 01:08:14.760]  И опять же, это
[01:08:14.760 --> 01:08:16.760]  на практике через год, два, три
[01:08:16.760 --> 01:08:18.760]  многие начинают забывать, как это формально звучит,
[01:08:18.760 --> 01:08:20.760]  неформально. Мед главных компонент
[01:08:20.760 --> 01:08:22.760]  ищет направление наибольшей дисперсии.
[01:08:22.760 --> 01:08:24.760]  Я думаю, это понятно. Вот где у вас больше всего
[01:08:24.760 --> 01:08:26.760]  точки шумят, это то направление,
[01:08:26.760 --> 01:08:28.760]  вдоль которого вам нужно явно больше сил,
[01:08:28.760 --> 01:08:30.760]  чтобы их различать между собой.
[01:08:30.760 --> 01:08:32.760]  Грубо говоря, если вы замените всю координату
[01:08:32.760 --> 01:08:34.760]  на одно значение, вы получите больше штраф
[01:08:34.760 --> 01:08:36.760]  за это. Вот здесь, допустим,
[01:08:36.760 --> 01:08:38.760]  вот оно направление, вдоль него
[01:08:38.760 --> 01:08:40.760]  у нас больше всего шум, это и есть направление
[01:08:40.760 --> 01:08:42.760]  наибольшей дисперсии. И опять же,
[01:08:42.760 --> 01:08:44.760]  рубрика вопроса собеседования, если таких
[01:08:44.760 --> 01:08:46.760]  направлений несколько, это нормально,
[01:08:46.760 --> 01:08:48.760]  так бывает, тогда у вас главные компоненты
[01:08:48.760 --> 01:08:50.760]  неоднозначно определены.
[01:08:50.760 --> 01:08:52.760]  На самом деле, даже здесь они у вас неоднозначно
[01:08:52.760 --> 01:08:54.760]  определены, вы можете совершенно спокойно
[01:08:54.760 --> 01:08:56.760]  все повернуть на 180 градусов,
[01:08:56.760 --> 01:08:58.760]  вот у вас будут две главные компоненты абсолютно
[01:08:58.760 --> 01:09:00.760]  такие же, только направлены в другую сторону.
[01:09:00.760 --> 01:09:02.760]  Ничего не меняйте. Но с точностью до
[01:09:02.760 --> 01:09:04.760]  смены направлений, главные компоненты
[01:09:04.760 --> 01:09:06.760]  определены, если у вас все направления имеют
[01:09:06.760 --> 01:09:08.760]  разную дисперсию. Тогда все окей.
[01:09:10.760 --> 01:09:12.760]  А?
[01:09:16.760 --> 01:09:18.760]  Погодите, у вас матрица
[01:09:18.760 --> 01:09:20.760]  это краски множества
[01:09:20.760 --> 01:09:22.760]  векторов для точек.
[01:09:22.760 --> 01:09:24.760]  Для одной точки оно не имеет
[01:09:24.760 --> 01:09:26.760]  смысла.
[01:09:30.760 --> 01:09:32.760]  Это матрица
[01:09:32.760 --> 01:09:34.760]  сотни на два, потому что у вас две координаты.
[01:09:36.760 --> 01:09:38.760]  То есть у вас была матрица
[01:09:38.760 --> 01:09:40.760]  100 на два, вы говорите
[01:09:40.760 --> 01:09:42.760]  хочу одну координату, тогда у вас
[01:09:42.760 --> 01:09:44.760]  останется матрица 100 на один,
[01:09:44.760 --> 01:09:46.760]  где у вас краски будет известно, что вдоль вот этого
[01:09:46.760 --> 01:09:48.760]  вектора координата нужная вам
[01:09:48.760 --> 01:09:50.760]  лежит, и вы знаете чему оно равно.
[01:09:52.760 --> 01:09:54.760]  На всякий случай еще раз.
[01:09:54.760 --> 01:09:56.760]  И с ней, по сути, говорить вам,
[01:09:56.760 --> 01:09:58.760]  что вы проецируетесь на некоторое
[01:09:58.760 --> 01:10:00.760]  линейное подпространство. У вас было
[01:10:00.760 --> 01:10:02.760]  изначальное пространство размером l, вы выбрали
[01:10:02.760 --> 01:10:04.760]  k признаков, на камерное подпространство
[01:10:04.760 --> 01:10:06.760]  спроецировались.
[01:10:06.760 --> 01:10:08.760]  Причем краски вам и показывают, что
[01:10:08.760 --> 01:10:10.760]  главное число, точнее
[01:10:10.760 --> 01:10:12.760]  главная компонента обладает соответствующим
[01:10:12.760 --> 01:10:14.760]  вектором, который говорит куда, и
[01:10:14.760 --> 01:10:16.760]  числом, который говорит насколько там большая дисперсия.
[01:10:24.760 --> 01:10:26.760]  Так как все матрицы u и v артагональны,
[01:10:26.760 --> 01:10:28.760]  вы их можете совершенно спокойно менять местами,
[01:10:28.760 --> 01:10:30.760]  также меняя местами в том же порядке
[01:10:30.760 --> 01:10:32.760]  векторы в матрице sigma и v.
[01:10:32.760 --> 01:10:34.760]  Потому что если мы упрячим матрицу sigma
[01:10:34.760 --> 01:10:36.760]  по убыванию, по невозрастанию
[01:10:36.760 --> 01:10:38.760]  сингулярных значений,
[01:10:38.760 --> 01:10:40.760]  то первые k значения соответствуют
[01:10:40.760 --> 01:10:42.760]  первым k направлениям с максимальной дисперсией.
[01:10:42.760 --> 01:10:44.760]  Нам просто так удобно.
[01:10:44.760 --> 01:10:46.760]  Смотрите,
[01:10:46.760 --> 01:10:48.760]  сингулярное значение говорит насколько
[01:10:48.760 --> 01:10:50.760]  большая дисперсия.
[01:10:50.760 --> 01:10:52.760]  Чем больше сингулярное значение, тем больше дисперсия.
[01:10:52.760 --> 01:10:54.760]  Мы хотим
[01:10:54.760 --> 01:10:56.760]  в учебнике линалу
[01:10:56.760 --> 01:10:58.760]  за первый курс.
[01:10:58.760 --> 01:11:00.760]  Как это дело выбирать?
[01:11:00.760 --> 01:11:02.760]  На практике вы можете взять
[01:11:02.760 --> 01:11:04.760]  и построить вот такой вот график.
[01:11:04.760 --> 01:11:06.760]  Это либо дисперсия для
[01:11:06.760 --> 01:11:08.760]  каждой из компонент.
[01:11:08.760 --> 01:11:10.760]  Как правило, там есть какой-нибудь редкий переход.
[01:11:10.760 --> 01:11:12.760]  Называют метод складного ножа
[01:11:12.760 --> 01:11:14.760]  иногда почему-то. Вы смотрите,
[01:11:14.760 --> 01:11:16.760]  у вас дисперсия для каждой из компонент,
[01:11:16.760 --> 01:11:18.760]  потом в какой-то момент она остается слишком маленькой.
[01:11:18.760 --> 01:11:20.760]  Допустим, здесь мы видим, что
[01:11:20.760 --> 01:11:22.760]  первые m компонент покрывают
[01:11:22.760 --> 01:11:24.760]  почти всю дисперсию в нашей выборке,
[01:11:24.760 --> 01:11:26.760]  все остальные имеют крайне
[01:11:26.760 --> 01:11:28.760]  малое значение дисперсии
[01:11:28.760 --> 01:11:30.760]  или относительно дисперсии.
[01:11:30.760 --> 01:11:32.760]  Можно всю дисперсию посчитать
[01:11:32.760 --> 01:11:34.760]  и потом в долях от нее считать.
[01:11:34.760 --> 01:11:36.760]  Соответственно, как выбирать
[01:11:36.760 --> 01:11:38.760]  количество компонентов?
[01:11:38.760 --> 01:11:40.760]  Либо оно у вас сверху зафиксировано
[01:11:40.760 --> 01:11:42.760]  кем-то там, либо вы выбираете
[01:11:42.760 --> 01:11:44.760]  то количество компонентов, которое оптимально
[01:11:44.760 --> 01:11:46.760]  для вашей конкретной задачи.
[01:11:46.760 --> 01:11:48.760]  Но опять же стоит помнить, что
[01:11:48.760 --> 01:11:50.760]  ваше количество компонентов,
[01:11:50.760 --> 01:11:52.760]  независимо от того, сколько оно,
[01:11:52.760 --> 01:11:54.760]  вы по-другому.
[01:11:54.760 --> 01:11:56.760]  Несмотря на то, сколько компонентов вы выбираете,
[01:11:56.760 --> 01:11:58.760]  если вы взяли не все компоненты,
[01:11:58.760 --> 01:12:00.760]  значит вы уже взяли какое-то
[01:12:00.760 --> 01:12:02.760]  линейное подпространство, которое все
[01:12:02.760 --> 01:12:04.760]  спроецировали, но при этом вы учили
[01:12:04.760 --> 01:12:06.760]  только линейные, грубо говоря, взаимосвязи
[01:12:06.760 --> 01:12:08.760]  между признаками. Вы часть информации
[01:12:08.760 --> 01:12:10.760]  могли совершенно спокойно потерять.
[01:12:10.760 --> 01:12:12.760]  Вас от этого никто не застрахует.
[01:12:12.760 --> 01:12:14.760]  Если вы теряете, выкидывает часть информации,
[01:12:14.760 --> 01:12:16.760]  вы выкидываете. Но эта штука
[01:12:16.760 --> 01:12:18.760]  снижает пространство размером с
[01:12:18.760 --> 01:12:20.760]  линейным образом. Например, есть
[01:12:20.760 --> 01:12:22.760]  простенький вариант,
[01:12:22.760 --> 01:12:24.760]  где это не будет работать.
[01:12:28.760 --> 01:12:30.760]  Давайте я вам нарисую.
[01:12:30.760 --> 01:12:32.760]  Нет, спасибо.
[01:12:32.760 --> 01:12:34.760]  Условно, у вас данные могут лежать, например,
[01:12:34.760 --> 01:12:36.760]  на какой-нибудь вот такой вот
[01:12:36.760 --> 01:12:38.760]  поверхности.
[01:12:38.760 --> 01:12:40.760]  Но можете себе еще
[01:12:40.760 --> 01:12:42.760]  в трехмерном пространстве.
[01:12:42.760 --> 01:12:44.760]  Согласитесь, что у нас данные лежат
[01:12:44.760 --> 01:12:46.760]  на одной гиперповерхности,
[01:12:46.760 --> 01:12:48.760]  которая просто вот так рулоном закручена.
[01:12:48.760 --> 01:12:50.760]  На одной кривой в данном случае.
[01:12:50.760 --> 01:12:52.760]  И вам одной координаты по этой кривой
[01:12:52.760 --> 01:12:54.760]  уже достаточно было бы, чтобы все точки отделить.
[01:12:54.760 --> 01:12:56.760]  PCA не способен это сделать.
[01:12:56.760 --> 01:12:58.760]  PCA вам найдет что-нибудь типа
[01:12:58.760 --> 01:13:00.760]  вот так вот тогда. Давай всех спроецируем.
[01:13:00.760 --> 01:13:02.760]  И вот куча ошибок у вас будет
[01:13:02.760 --> 01:13:04.760]  вот здесь везде понатыкана.
[01:13:04.760 --> 01:13:06.760]  Согласны?
[01:13:06.760 --> 01:13:08.760]  Помните, что PCA штука линейная.
[01:13:08.760 --> 01:13:10.760]  И теперь самый важный
[01:13:10.760 --> 01:13:12.760]  комментарий. Он там уже проскакивал.
[01:13:12.760 --> 01:13:14.760]  Но как вы думаете, можно просто взять выборку
[01:13:14.760 --> 01:13:16.760]  и ее засунуть в PCA?
[01:13:16.760 --> 01:13:18.760]  Можно попробовать, согласен?
[01:13:18.760 --> 01:13:20.760]  Отнормировать, конечно.
[01:13:20.760 --> 01:13:22.760]  Если у вас данные не отнормированные,
[01:13:22.760 --> 01:13:24.760]  то у вас точно также будут считаться
[01:13:24.760 --> 01:13:26.760]  расстояния в вашем исходном пространстве,
[01:13:26.760 --> 01:13:28.760]  как и в КНН-е том же самом.
[01:13:28.760 --> 01:13:30.760]  Если у вас разные признаки
[01:13:30.760 --> 01:13:32.760]  в разных шкалах,
[01:13:32.760 --> 01:13:34.760]  то вы получите, собственно,
[01:13:34.760 --> 01:13:36.760]  чем больше шкала, тем больше дисперсии
[01:13:36.760 --> 01:13:38.760]  по этому направлению.
[01:13:38.760 --> 01:13:40.760]  Поэтому если вы не отнормируете ваши данные,
[01:13:40.760 --> 01:13:42.760]  вы можете либо это сделать, если вы четко понимаете,
[01:13:42.760 --> 01:13:44.760]  что данные нормировать не надо,
[01:13:44.760 --> 01:13:46.760]  и вы не представляют каким-то смыслом важным,
[01:13:46.760 --> 01:13:48.760]  его надо учесть именно в PCA,
[01:13:48.760 --> 01:13:50.760]  либо вы себе все сломаете.
[01:13:50.760 --> 01:13:52.760]  Я вас неспроста об этом предупреждаю,
[01:13:52.760 --> 01:13:54.760]  я не буду говорить прямо, но, пожалуйста,
[01:13:54.760 --> 01:13:56.760]  нормируйте данные перед PCA, вам скоро это очень сильно понадобится.
[01:13:58.760 --> 01:14:00.760]  Надеюсь, в этом году это услышат больше,
[01:14:00.760 --> 01:14:02.760]  чем обычно людей,
[01:14:02.760 --> 01:14:04.760]  потому что регулярно при
[01:14:04.760 --> 01:14:06.760]  рассмотрении того, что вы там
[01:14:06.760 --> 01:14:08.760]  понасчитали, не обращаясь мне к кому конкретно напрямую,
[01:14:08.760 --> 01:14:10.760]  но регулярно это ошибка номер
[01:14:10.760 --> 01:14:12.760]  один, наверное, который мы наблюдаем
[01:14:12.760 --> 01:14:14.760]  в первой половине курса.
[01:14:14.760 --> 01:14:16.760]  Вот, собственно,
[01:14:16.760 --> 01:14:18.760]  нормируйте данные перед PCA,
[01:14:18.760 --> 01:14:20.760]  пожалуйста. И, соответственно, снижаем
[01:14:20.760 --> 01:14:22.760]  размерность. Сигма K это наше
[01:14:22.760 --> 01:14:24.760]  камерное представление.
[01:14:24.760 --> 01:14:26.760]  Если мы хотим вернуться обратно,
[01:14:26.760 --> 01:14:28.760]  то вот наш будет х чертой,
[01:14:28.760 --> 01:14:30.760]  у на сигма, на v это
[01:14:30.760 --> 01:14:32.760]  матрица обратного перехода.
[01:14:32.760 --> 01:14:34.760]  Ну и вот вам маленький пример,
[01:14:34.760 --> 01:14:36.760]  допустим, библиотека Eigenfaces,
[01:14:36.760 --> 01:14:38.760]  просто различные изображения,
[01:14:38.760 --> 01:14:40.760]  я не знаю, видно ли вам там подписи,
[01:14:40.760 --> 01:14:42.760]  на всякий случай, вы на этих фотографиях
[01:14:42.760 --> 01:14:44.760]  видите хоть кого-то знакомого?
[01:14:44.760 --> 01:14:46.760]  Кого?
[01:14:48.760 --> 01:14:50.760]  О, ну слушайте, классно!
[01:14:52.760 --> 01:14:54.760]  Огонь!
[01:14:54.760 --> 01:14:56.760]  Ну, собственно, да, вон там еще Шварценеггер сидит,
[01:14:56.760 --> 01:14:58.760]  Билл Гейс, по идее,
[01:14:58.760 --> 01:15:00.760]  и так далее.
[01:15:00.760 --> 01:15:02.760]  Логично, что мы
[01:15:02.760 --> 01:15:04.760]  же с вами можем матричку...
[01:15:04.760 --> 01:15:06.760]  Это же матричка, правильно?
[01:15:06.760 --> 01:15:08.760]  Причем черно-белая изображение,
[01:15:08.760 --> 01:15:10.760]  и эта матричка просто-напросто
[01:15:10.760 --> 01:15:12.760]  размером там, не знаю, 64 на 64.
[01:15:12.760 --> 01:15:14.760]  Можем матричку через
[01:15:14.760 --> 01:15:16.760]  SVD тоже самое пытаться разложить?
[01:15:16.760 --> 01:15:18.760]  Можем. Ну вот берем
[01:15:18.760 --> 01:15:20.760]  топ-16 компонент, получаем вот такое.
[01:15:20.760 --> 01:15:22.760]  Страшновато.
[01:15:22.760 --> 01:15:24.760]  Вот берем 50 компонент,
[01:15:24.760 --> 01:15:26.760]  заметьте, уже начинают прорисовываться
[01:15:26.760 --> 01:15:28.760]  различные, скажем так,
[01:15:28.760 --> 01:15:30.760]  черты лица, но они в среднем все равно
[01:15:30.760 --> 01:15:32.760]  ориентированы как-то по центру.
[01:15:34.760 --> 01:15:36.760]  Ой, слушайте, я боюсь наврать,
[01:15:36.760 --> 01:15:38.760]  но раз у нас 250 компонентов,
[01:15:38.760 --> 01:15:40.760]  так что минимум 260 должно быть сверху.
[01:15:44.760 --> 01:15:46.760]  Ну, типа того, да.
[01:15:46.760 --> 01:15:48.760]  На самом деле, в вашем случае
[01:15:48.760 --> 01:15:50.760]  абсолютно неважно, у вас будет
[01:15:50.760 --> 01:15:52.760]  по строкам или полстопцам в данном случае,
[01:15:52.760 --> 01:15:54.760]  потому что матрица, она и есть матрица.
[01:15:54.760 --> 01:15:56.760]  У вас ранг матрицы от транспонирования
[01:15:56.760 --> 01:15:58.760]  не меняется.
[01:16:00.760 --> 01:16:02.760]  Еще раз, у вас есть изначальная матрица,
[01:16:02.760 --> 01:16:04.760]  вы говорите, я хочу
[01:16:04.760 --> 01:16:06.760]  взять эту матрицу, описать ее
[01:16:06.760 --> 01:16:08.760]  в каком-то линейном подпространстве,
[01:16:08.760 --> 01:16:10.760]  допустим, в терминах всего 18-16 компонент,
[01:16:10.760 --> 01:16:12.760]  а потом на основании этих 16 компонентов
[01:16:12.760 --> 01:16:14.760]  вернуться в исходное пространство.
[01:16:14.760 --> 01:16:16.760]  Да, то есть мы аппроксимируем матрицу
[01:16:16.760 --> 01:16:18.760]  в виде 16, у нас была матрица
[01:16:18.760 --> 01:16:20.760]  условно 250 на 250,
[01:16:20.760 --> 01:16:22.760]  мы говорим, на самом деле у меня будет
[01:16:22.760 --> 01:16:24.760]  матрица 250 на 16,
[01:16:24.760 --> 01:16:26.760]  а потом я вернусь обратно 250 на 250.
[01:16:30.760 --> 01:16:32.760]  Нет, да?
[01:16:34.760 --> 01:16:36.760]  Мне кажется,
[01:16:36.760 --> 01:16:38.760]  да, хотя
[01:16:38.760 --> 01:16:40.760]  глядя на вот эту картинку,
[01:16:40.760 --> 01:16:42.760]  у меня ощущение, что их
[01:16:42.760 --> 01:16:44.760]  всех между собой правда перемешали,
[01:16:44.760 --> 01:16:46.760]  тогда у вас была матрица размером
[01:16:46.760 --> 01:16:48.760]  количества картинок на изображение.
[01:16:48.760 --> 01:16:50.760]  Честно, конкретно в данном случае
[01:16:50.760 --> 01:16:52.760]  не знаю, в семинаре мы будем
[01:16:52.760 --> 01:16:54.760]  с одним изображением по отдельности работать.
[01:17:04.760 --> 01:17:06.760]  Вот.
[01:17:06.760 --> 01:17:08.760]  Смотрите, у вас
[01:17:08.760 --> 01:17:10.760]  х изначально это у на сигма
[01:17:10.760 --> 01:17:12.760]  на в транспонированное,
[01:17:12.760 --> 01:17:14.760]  если хотите взять первые всего лишь
[01:17:14.760 --> 01:17:16.760]  к штук, вы берете у к сигма к
[01:17:16.760 --> 01:17:18.760]  в транспонированное к,
[01:17:18.760 --> 01:17:20.760]  то есть первые к элементов из каждой матрицы соответственно.
[01:17:24.760 --> 01:17:26.760]  В данном случае
[01:17:26.760 --> 01:17:28.760]  наверное да.
[01:17:28.760 --> 01:17:30.760]  Я почему говорю наверное, потому что у меня
[01:17:30.760 --> 01:17:32.760]  в голове уже перепутано с этим семинаром,
[01:17:32.760 --> 01:17:34.760]  по отдельности как матрицу рассматривать,
[01:17:34.760 --> 01:17:36.760]  а здесь судя по тому, что у нас
[01:17:36.760 --> 01:17:38.760]  ориентация, видите, у нас все лица почему-то
[01:17:38.760 --> 01:17:40.760]  ориентированные по центру в среднем,
[01:17:40.760 --> 01:17:42.760]  так что видимо их правда вытянули в вектор
[01:17:42.760 --> 01:17:44.760]  и уложили просто в одну матрицу.
[01:17:44.760 --> 01:17:46.760]  Вот.
[01:17:46.760 --> 01:17:48.760]  Хорошо.
[01:17:48.760 --> 01:17:50.760]  Ну что ж.
[01:17:50.760 --> 01:17:52.760]  Ну и в принципе
[01:17:52.760 --> 01:17:54.760]  на этом
[01:17:54.760 --> 01:17:56.760]  лекция у нас закончилась.
[01:17:56.760 --> 01:17:58.760]  Перерыв 15 минут.

[00:00.000 --> 00:13.000]  Мы с вами в прошлый раз начали большие данные, и даже у кого-то они уже были на семинарах.
[00:13.000 --> 00:17.000]  Мы начали пока обсуждать только хранение больших данных.
[00:17.000 --> 00:22.000]  Сегодня и еще следующие три лекции у нас будет обработка больших данных.
[00:22.000 --> 00:29.000]  И перед тем как перейти к обработке, давайте вспомним про основные особенности вот этой файловой системы,
[00:29.000 --> 00:34.000]  которая мы обсуждали. Она называется ходу distributed file system, или HDFS.
[00:34.000 --> 00:43.000]  HDFS заточены под хранение больших файлов.
[00:43.000 --> 00:49.000]  Они разбиваются на блоки, и эти блоки хранятся на большом количестве серверов.
[00:49.000 --> 00:57.000]  Так как серверов много, то они будут чаще падать, поэтому система должна быть устойчива к отказам.
[00:57.000 --> 01:06.000]  Поэтому каждый блок у нас имеет несколько дублей, так называемые реплики, которые хранятся на каких-то других машинах.
[01:06.000 --> 01:10.000]  Если одна упадет, то у нас все равно реплики останутся.
[01:10.000 --> 01:26.000]  Ну и HDFS ориентирован на подход write once read main, не просто потому что во время записей нам нужно записать все реплики на n datanode.
[01:26.000 --> 01:39.000]  Это происходит долго, а чтение у нас происходит гораздо быстрее, потому что мы выкачиваем данные напрямую из ближайшей реплики.
[01:39.000 --> 01:41.000]  Ну это что касается хранения.
[01:41.000 --> 01:52.000]  Теперь нам надо бы как-то научиться делать вычисления поверх этой распределенной системы.
[01:52.000 --> 01:59.000]  Первая компания, которая столкнулась с проблемой в вычислении больших данных, это, конечно же, Google.
[01:59.000 --> 02:07.000]  И в 2004 году появилась статья, которая называется Google MapReduce.
[02:07.000 --> 02:12.000]  Система Google MapReduce была закрытой, как и система Google File System.
[02:12.000 --> 02:20.000]  Но на основе статей появилась, собственно, система Hadoop, и с ней мы и будем работать.
[02:20.000 --> 02:28.000]  У вас будет по ней семинары, два семинара даже, у вас будет по ней домашка.
[02:28.000 --> 02:33.000]  И как вообще обрабатываются данные в Hadoop?
[02:33.000 --> 02:38.000]  Используется вот такой вот подход, который называется MapReduce.
[02:38.000 --> 02:43.000]  Мы все время работаем с парами типа ключ начения, K1, V1.
[02:43.000 --> 02:48.000]  И вот эти пары у нас проходят несколько стадий.
[02:48.000 --> 02:52.000]  Первая стадия – это мап, когда данные обрабатываются поэлементно.
[02:52.000 --> 02:56.000]  Вот у нас была пара, и получилось одна или несколько новых пар.
[02:56.000 --> 03:00.000]  Это происходит на стадии вот этого мапера.
[03:00.000 --> 03:07.000]  Дальше идет стадия сортировки, где у нас полученные пары сортируются и группируются по ключам.
[03:08.000 --> 03:11.000]  И выдаются на выход уже вот такие пары.
[03:11.000 --> 03:18.000]  То есть ключ и не одно значение, а целая группа значений для этого ключа.
[03:21.000 --> 03:24.000]  После этого у нас происходит стадия reduce.
[03:24.000 --> 03:31.000]  Это когда мы берем эту группу, что-нибудь с ней делаем, например, какую-то агрегацию.
[03:31.000 --> 03:34.000]  Например, считаем сумму, считаем среднее.
[03:34.000 --> 03:36.000]  И у нас получается уже новая пара.
[03:36.000 --> 03:40.000]  Может попутно ключ поменяться, и будет новая пара.
[03:42.000 --> 03:45.000]  Чтобы было понятнее, как работает мап и как работает reduce,
[03:45.000 --> 03:49.000]  скажите, что будет в результате вот этих вот кусочков кода?
[04:01.000 --> 04:02.000]  Где все?
[04:03.000 --> 04:07.000]  По-моему, мап вернет массив 1 до 5.
[04:07.000 --> 04:14.000]  А reduce просуммирует все элементы от 0 до 4.
[04:14.000 --> 04:16.000]  Да, range 5 это от 0 до 4.
[04:16.000 --> 04:21.000]  x плюс 1 это мы просто независимо взяли каждый элемент и прибавили значку.
[04:21.000 --> 04:25.000]  А reduce посчитал сумму от 0 до 4, и вот что получилось.
[04:25.000 --> 04:27.000]  Да, все правильно.
[04:28.000 --> 04:35.000]  Ну и разберем первый пример, с которого обычно начинают изучения мап и reduce.
[04:35.000 --> 04:37.000]  Это word count.
[04:37.000 --> 04:41.000]  То есть задача подсчета частоты слов в тексте.
[04:42.000 --> 04:44.000]  Вот у нас есть такое предложение.
[04:44.000 --> 04:47.000]  В реальной жизни, конечно, текста у нас намного больше.
[04:47.000 --> 04:52.000]  И мы хотим с помощью мап и reduce понять, сколько раз каждое слово встречается в тексте.
[04:52.000 --> 04:56.000]  Давайте подумаем, что мы будем делать на каждой из этих стадий.
[04:56.000 --> 04:57.000]  Вот так вы думаете.
[04:57.000 --> 05:24.000]  Мап вернет превратить в пары слова единичка.
[05:24.000 --> 05:25.000]  Да, все так.
[05:25.000 --> 05:34.000]  То есть сначала мы вот этот текст разбиваем по словам, формируем вот эти пары, сортировка и сложение группы.
[05:34.000 --> 05:39.000]  Теперь давайте посмотрим немного на код.
[05:39.000 --> 05:42.000]  То есть вот пример нашего мапера.
[05:42.000 --> 05:51.000]  Все тот же word count, но, например, мы хотим по какой-то конкретной книге, которая лежит в текст-технике, посчитать word count.
[05:51.000 --> 05:52.000]  Что мы делаем?
[05:52.000 --> 05:57.000]  Мы ее выводим обычным кэтом или эхо или как-нибудь еще.
[05:57.000 --> 05:59.000]  Она у нас выводится в стандартный поток угода.
[05:59.000 --> 06:02.000]  Мы стандартные кэты выводим в стандартный поток угода.
[06:02.000 --> 06:04.000]  И мы выводим в стандартный поток угода.
[06:04.000 --> 06:05.000]  Что мы делаем?
[06:05.000 --> 06:09.000]  Мы ее выводим обычным кэтом или эхо или как-нибудь еще.
[06:09.000 --> 06:12.000]  Она у нас выводится в стандартный поток угода.
[06:12.000 --> 06:15.000]  Мы стандартный поток парсим.
[06:15.000 --> 06:18.000]  И получаем слова типа слова единичка.
[06:18.000 --> 06:20.000]  Это вот такой мапер.
[06:20.000 --> 06:24.000]  Теперь редьюсер.
[06:24.000 --> 06:29.000]  В редьюсере у нас уже имеются пары типа слова единичка.
[06:29.000 --> 06:33.000]  То есть они тоже в виде строки, но вот эти пары мы их уже можем вычислить.
[06:33.000 --> 06:36.000]  То есть мы можем получить кей-каунт.
[06:36.000 --> 06:45.000]  Ну и дальше мы считаем каунт для каждого кей.
[06:45.000 --> 06:47.000]  То есть смотрите, что мы сделаем.
[06:47.000 --> 06:49.000]  Мы берем входные строчки.
[06:49.000 --> 06:51.000]  Получаем из них кей-каунт.
[06:51.000 --> 06:57.000]  Мы знаем, что у нас прошел сорт.
[06:57.000 --> 07:03.000]  То есть вот эта вот стадия сорт у нас уже прошла.
[07:03.000 --> 07:06.000]  И поэтому кей у нас всегда одинаковый.
[07:06.000 --> 07:09.000]  То есть идут постоянные кей-1, кей-1, кей-1.
[07:09.000 --> 07:11.000]  В какой-то момент начинается кей-2.
[07:11.000 --> 07:21.000]  И когда у нас уже кей-2 начался, кей-1 уже больше нигде не встретится.
[07:21.000 --> 07:25.000]  Поэтому все, что нам нужно, нам нужно отслеживать то, что кей поменялся.
[07:25.000 --> 07:30.000]  Если кей поменялся, мы выводим результат для первого кей и считаем следующий.
[07:34.000 --> 07:36.000]  Вот как это все вместе выполнить?
[07:36.000 --> 07:40.000]  Пока нет никакого ходу, мы не используем никакой фреймборд.
[07:40.000 --> 07:44.000]  Мы просто пишем код и его выполняем.
[07:44.000 --> 07:46.000]  Вот у нас есть файлик.
[07:46.000 --> 07:47.000]  Мы его выводим.
[07:47.000 --> 07:49.000]  Мы применяем к нему маппер.
[07:49.000 --> 07:50.000]  Потом сортируем.
[07:50.000 --> 07:52.000]  Потом применяем редьюсер.
[07:52.000 --> 07:54.000]  Ну и потом еще идет одна сортировка.
[07:54.000 --> 07:56.000]  Но это уже для красоты.
[07:56.000 --> 07:59.000]  То есть мапр редьюсер весь заканчивается вот здесь.
[08:03.000 --> 08:11.000]  Это пока без всякого ходу, но просто как иллюстрация того, какие части соответствуют этим же частям в ходу.
[08:18.000 --> 08:21.000]  Ну и понятно, что у такого подхода у нас не будет хватать памяти.
[08:21.000 --> 08:26.000]  То есть если вот эта дата вдруг окажется большой, мы не сможем ей сделать кед.
[08:26.000 --> 08:28.000]  Она не поместится в память.
[08:28.000 --> 08:31.000]  И тогда придется думать что-нибудь другое.
[08:35.000 --> 08:37.000]  Вот давайте посмотрим, что именно.
[08:39.000 --> 08:42.000]  Вот это уже то, как мап редьюс реализован в ходу.
[08:42.000 --> 08:48.000]  То есть мы помним, что у нас есть HDFS, что данные хранятся там разбитые на блоке.
[08:48.000 --> 08:57.000]  И Hadoop, как фреймворк для вычислений, он стартует процессы мапперов и редьюсеров там же, где лежат данные.
[08:59.000 --> 09:03.000]  То есть на каких нодах данные лежат, на тех нодах и стартует маппер.
[09:03.000 --> 09:04.000]  Идет обработка.
[09:06.000 --> 09:07.000]  Потом идет сортировка.
[09:07.000 --> 09:14.000]  И на редьюсере у нас есть гарантия того, что на одну ноду, на один редьюсер у нас...
[09:14.000 --> 09:21.000]  То есть если сюда попал ключ один на одну ноду, то больше его уже нигде не будет.
[09:23.000 --> 09:28.000]  То есть мы вот эти группы по разным процессам дробить не можем.
[09:34.000 --> 09:37.000]  Вот сейчас есть какие-нибудь вопросы по этой схеме?
[09:37.000 --> 09:41.000]  Эти блоки все еще на нодах хранятся или уже на...
[09:42.000 --> 09:44.000]  Эти блоки хранятся на нодах, да.
[09:44.000 --> 09:47.000]  Маппер их считывает себе в оперативную память.
[09:47.000 --> 09:49.000]  Но сами блоки хранятся на нодах.
[09:51.000 --> 10:01.000]  И тут у нас сами ноды уже отменялись данными или нет, когда мод сортировали тут?
[10:01.000 --> 10:04.000]  Вот тут немного по-другому происходит.
[10:04.000 --> 10:06.000]  У нас вычитался блок.
[10:06.000 --> 10:10.000]  Мы сделали для него мап и положили данные на диск.
[10:12.000 --> 10:16.000]  То есть мы не вот этот блок уже не с ним работаем, а с вот результатом маппер.
[10:16.000 --> 10:19.000]  И его мы сортируем и переносим на другие машинки, да.
[10:19.000 --> 10:21.000]  Есть ли еще какие-нибудь вопросы по этой схеме?
[10:24.000 --> 10:30.000]  Не оченьerve.
[10:38.000 --> 10:43.000]  Я 1982 год был в disappeared's apartment.
[10:43.000 --> 10:55.720]  Это как бы функция маппера, функция редьюсера.
[10:55.720 --> 11:08.720]  Хорошо, если вопросов нет, тогда у меня к вам вопрос.
[11:08.720 --> 11:10.320]  Вот здесь указана такая штука.
[11:10.320 --> 11:12.960]  Блок не равен блоковым HDFS.
[11:12.960 --> 11:15.240]  Блоки, которые изображены здесь, они даже называются
[11:15.240 --> 11:18.480]  на самом деле не блоки, а сплиты, то есть их размер
[11:18.480 --> 11:21.480]  немного отличается.
[11:21.480 --> 11:27.280]  Зачем это нужно вообще ходуку, зачем нужны эти отличия?
[11:27.280 --> 11:38.280]  Как вы думаете?
[11:38.280 --> 11:41.440]  Возможно блоки, которые хранятся слишком большие,
[11:41.520 --> 11:43.520]  с ними неудобно работать, например?
[11:43.520 --> 11:51.520]  В целом, МАГР работает в специальной такой абстракции,
[11:51.520 --> 11:54.040]  которая называется контейнер, мы про это еще будем говорить
[11:54.040 --> 11:55.040]  в следующий раз.
[11:55.040 --> 11:59.120]  И у этого контейнера памяти где-то там 1-2 гигабайта,
[11:59.120 --> 12:00.120]  может быть и больше.
[12:00.120 --> 12:04.320]  А один блок у нас небольшой, ну 60 мегабайт, например.
[12:04.320 --> 12:07.080]  Поэтому вряд ли что проблема была связана с тем, что
[12:07.080 --> 12:08.080]  блоки большие.
[12:08.720 --> 12:12.200]  И к тому же я говорю, что блок примерно равен, то
[12:12.200 --> 12:15.760]  есть он не в два раза меньше или в пять раз, а он почти
[12:15.760 --> 12:16.760]  такой же.
[12:32.760 --> 12:34.760]  Есть ли какие-нибудь идеи еще?
[12:38.760 --> 13:02.760]  Хорошо, дело в том, что данные, которые подаются на вход
[13:02.760 --> 13:04.860]  на продюсер, они могут быть совершенно разные, это
[13:04.860 --> 13:05.860]  не только текст.
[13:05.860 --> 13:08.860]  А это, например, могут быть видео.
[13:08.860 --> 13:12.860]  И мы с вами видели в прошлый раз, как работает HDFS.
[13:12.860 --> 13:15.860]  Он не смотрит, какие там файлы, какой формат.
[13:15.860 --> 13:19.860]  Он просто их бьет с точностью до бита на равные части.
[13:19.860 --> 13:22.860]  И эти части улетают на разные ноды.
[13:22.860 --> 13:25.860]  Файл таким образом не корраптится, не портится,
[13:25.860 --> 13:30.860]  потому что когда мы делаем HDFS dfs-g, вычитываем его,
[13:30.860 --> 13:34.860]  у нас получается, что опять целиком файл, вот он целый собрался со всех кусков.
[13:34.860 --> 13:36.860]  Все хорошо.
[13:36.860 --> 13:39.860]  А здесь-то мы не делаем get, а у нас здесь вот эти блоки,
[13:39.860 --> 13:42.860]  они идут на мапперы, обрабатываются независимо,
[13:42.860 --> 13:45.860]  потом перемешиваются, потом опять обрабатываются.
[13:45.860 --> 13:49.860]  То есть никогда они уже не встретятся, кроме как в конце джобы.
[13:49.860 --> 13:52.860]  В конце вот этой задачи.
[13:52.860 --> 13:56.860]  Поэтому получается, что если у нас есть видео, например,
[13:56.860 --> 13:59.860]  вот какие-то видео-файлы, то может оказаться,
[13:59.860 --> 14:03.860]  что часть видео попадет в первый блок в HDFS, часть во второй.
[14:03.860 --> 14:06.860]  У нас будут незаконченные фрагменты видео,
[14:06.860 --> 14:09.860]  мы их не сможем никак ни декодировать, ни прочитать,
[14:09.860 --> 14:13.860]  и придется вот эти крайние кусочки просто выкинуть.
[14:13.860 --> 14:17.860]  Чтоб такого не происходило, ходу действует более умно.
[14:17.860 --> 14:20.860]  А именно вот так.
[14:20.860 --> 14:24.860]  То есть представим, что у нас вот тут вообще важно не запутаться,
[14:24.860 --> 14:29.860]  потому что вот это у нас блоки, эти кусочки 1, 2, 3, 4, 5,
[14:29.860 --> 14:34.860]  вот это внизу блоки в HDFS, то есть вот эти кусочки маленькие,
[14:34.860 --> 14:38.860]  это блоки, относящиеся к формату хранения данных,
[14:38.860 --> 14:41.860]  какие бывают блокчные форматы хранения.
[14:41.860 --> 14:44.860]  Например, какие-нибудь архивы.
[14:44.860 --> 14:48.860]  Или какие-нибудь видео-файлы, опять же.
[14:48.860 --> 14:51.860]  То есть здесь вот какой-то файл лежит.
[14:51.860 --> 14:54.860]  Внизу блок-баунды это граница блока в HDFS,
[14:54.860 --> 14:57.860]  а вверху это граница сплита.
[14:57.860 --> 15:02.860]  Сплит это то, что берет себе на вход MapReduce.
[15:02.860 --> 15:05.860]  То есть как происходит чтение?
[15:05.860 --> 15:08.860]  Мы вычитываем блок.
[15:08.860 --> 15:11.860]  Дальше видим, вычитываем блок из HDFS.
[15:11.860 --> 15:15.860]  Дальше видим, что блок из HDFS мы прочитали,
[15:15.860 --> 15:20.860]  но у нас не завершенный вот этот кусочек.
[15:20.860 --> 15:24.860]  То есть мы ничего с ним сделать не сможем, мы его не прочитаем.
[15:24.860 --> 15:27.860]  Поэтому мы еще идем в соседний блок
[15:27.860 --> 15:31.860]  и дочитываем вот этот оставшийся кусочек.
[15:31.860 --> 15:36.860]  Получается сплит чуть больше, а здесь будет сплит чуть меньше.
[15:38.860 --> 15:44.860]  Вот эти границы блоков, например, границы между 5 и 6 или 6 и 7,
[15:44.860 --> 15:46.860]  их Hadoop распознает сам.
[15:46.860 --> 15:49.860]  Мы можем ему подсказать, как распознавать.
[15:49.860 --> 15:53.860]  То есть в Hadoop есть много встроенных форматов данных.
[15:53.860 --> 16:00.860]  Можно указать, например, формат VZIP или видео MP4 какой-нибудь.
[16:00.860 --> 16:04.860]  Точно также можно указать разделитель.
[16:04.860 --> 16:06.860]  То есть если мы имеем дело с текстом,
[16:06.860 --> 16:10.860]  то можно, например, указать разделитель символ конца строки.
[16:10.860 --> 16:14.860]  Тогда у нас вся строка пойдет или сюда, или сюда.
[16:14.860 --> 16:16.860]  Мы ее не разрежем пополам.
[16:16.860 --> 16:19.860]  Или указать разделителем символ пробела.
[16:19.860 --> 16:22.860]  Вот эти блоки маленькие, это будут слова.
[16:34.860 --> 16:37.860]  А сейчас какие-нибудь вопросы появились.
[16:41.860 --> 16:46.860]  Вот там на предыдущем слайде у нас для K2 и K3 принялась функция G.
[16:46.860 --> 16:53.860]  Почему разве не к каждому G применяется отдельно K2 и отдельно K3?
[16:53.860 --> 16:54.860]  Да, по идее?
[16:54.860 --> 16:56.860]  Да, да.
[17:01.860 --> 17:04.860]  Сейчас я обновлю страницу.
[17:04.860 --> 17:06.860]  Изменил повинтажку.
[17:16.860 --> 17:19.860]  Хорошо, давайте тогда идем дальше.
[17:19.860 --> 17:23.860]  И посмотрим, как работает MapReduce уже более технически.
[17:23.860 --> 17:25.860]  Вот у нас есть InputSplit.
[17:25.860 --> 17:29.860]  Это вот этот блок с поправкой на формат файла.
[17:29.860 --> 17:34.860]  Этот блок мы вычитываем в МАП, и у нас тут происходит МАП.
[17:36.860 --> 17:38.860]  То есть блок целиком попадает в МАП,
[17:38.860 --> 17:43.860]  а дальше внутри МАПа он может разделять и вычитывать.
[17:43.860 --> 17:48.860]  А дальше внутри МАПа он может разделяться на какие-то более мелкие структуры.
[17:48.860 --> 17:52.860]  И по сути МАП у нас работает вот так.
[18:00.860 --> 18:03.860]  Вот, можно сказать, что наш МАП это функция Run.
[18:03.860 --> 18:06.860]  И в принципе в ходу-то она такая и есть функция Run.
[18:06.860 --> 18:11.860]  Внутри нее у нас есть Setup.
[18:13.860 --> 18:16.860]  Есть Cleanup.
[18:16.860 --> 18:20.860]  Setup и Cleanup это такие парные функции,
[18:20.860 --> 18:25.860]  одна из которых инициализирует всякие переменные в начале МАПа.
[18:25.860 --> 18:27.860]  Какие-то файлы может открывать.
[18:27.860 --> 18:29.860]  В общем, делает подготовку.
[18:29.860 --> 18:32.860]  А Cleanup все, что было в рамках этой подготовки сделано,
[18:32.860 --> 18:35.860]  удаляет, пишет на диске и так далее.
[18:35.860 --> 18:37.860]  Или просто удаляет.
[18:37.860 --> 18:41.860]  Вот между Setup и Cleanup у нас есть самое главное, а это МАП.
[18:41.860 --> 18:44.860]  Он работает в цикле.
[18:49.860 --> 18:53.860]  То есть пока у нас существуют, например, строчки или слова,
[18:53.860 --> 18:56.860]  в зависимости от того, как мы этот блок разбили,
[18:56.860 --> 18:58.860]  по какому разделить его,
[18:58.860 --> 19:03.860]  пока оно существует, мы выполняем МАПы последовательно.
[19:05.860 --> 19:07.860]  То есть получается, блоки мы разбили параллельно,
[19:07.860 --> 19:10.860]  а внутри блока мы последовательно выполняем несколько раз МАП.
[19:10.860 --> 19:13.860]  Ну и на самом деле Reduce устроен точно так же.
[19:13.860 --> 19:15.860]  То есть на вход подается вот эта штука.
[19:15.860 --> 19:18.860]  Как она получается, мы сейчас разберемся.
[19:18.860 --> 19:21.860]  Ну то есть это тоже некий блок, он подается на вход,
[19:21.860 --> 19:24.860]  а в этом блоке может быть несколько групп.
[19:27.860 --> 19:30.860]  Вот таких вот K2, K3 может быть групп несколько.
[19:30.860 --> 19:34.860]  Поэтому мы тоже делаем Setup, потом While,
[19:34.860 --> 19:37.860]  и вместо МАПа у нас применяется Reduce.
[19:40.860 --> 19:43.860]  Давайте посмотрим, что происходит здесь.
[19:43.860 --> 19:46.860]  То есть мы прочитали МАПу, что там,
[19:48.860 --> 19:52.860]  обработали, и результат МАПа записали в память.
[19:54.860 --> 19:56.860]  Если памяти у нас не хватает,
[19:56.860 --> 20:00.860]  то мы это можем слить на диск и писать в памяти еще раз.
[20:00.860 --> 20:03.860]  В общем, в конечном итоге весь вот этот буфер,
[20:03.860 --> 20:06.860]  весь результат МАПера, он улетит на диск.
[20:06.860 --> 20:09.860]  Видите, тут написано, что он улетит на диск.
[20:09.860 --> 20:12.860]  Видите, тут написано Partition, Sort and Spill to Disk.
[20:15.860 --> 20:18.860]  Что такое partition? Мы разберемся через два слайда.
[20:18.860 --> 20:23.860]  Что такое sort? Мы понимаем, что это идет сортировка внутри файла.
[20:25.860 --> 20:28.860]  Но и Spill to Disk это как бы бэкап на диск.
[20:29.860 --> 20:32.860]  То есть вот эти у нас бэкапы на диск остаются,
[20:32.860 --> 20:34.860]  их может быть несколько.
[20:35.860 --> 20:38.860]  Потом мы это все мержим в один файл,
[20:38.860 --> 20:41.860]  и у него получаются вот такие вот группы.
[20:41.860 --> 20:43.860]  Это K1, это K2, это K3.
[20:45.860 --> 20:49.860]  И дальше K1, например, у нас летит на первый редьюсер,
[20:49.860 --> 20:53.860]  K2 на второй редьюсер, K3 на третий, и так далее.
[20:53.860 --> 20:56.860]  Вот согласно этой вот схеме.
[20:57.860 --> 21:01.860]  Что-то прилетает на один редьюсер, что-то прилетает на разные.
[21:02.860 --> 21:04.860]  То есть K1 прилетел сюда,
[21:04.860 --> 21:09.860]  и K1 прилетел еще с других мапперов, которые на других нодах считались.
[21:14.860 --> 21:19.860]  Вот у нас, например, K1, вот здесь у нас K2, другая группа.
[21:19.860 --> 21:22.860]  И дальше, когда у нас группы собрались, мы делаем мерж.
[21:22.860 --> 21:26.860]  То есть все кусочки, относящиеся к одной группе, мы сливаем в один файл.
[21:26.860 --> 21:29.860]  Слили здесь, слили здесь, вот у нас две группы,
[21:29.860 --> 21:32.860]  и мы идем работать с редьюсером.
[21:46.860 --> 21:48.860]  Вот, то есть у нас отработал маппер.
[21:51.860 --> 21:53.860]  Вот здесь мы заполнили данные.
[21:53.860 --> 21:58.860]  Дальше они объединились вот в этом кусочке K1.
[22:01.860 --> 22:04.860]  Ну и потом этот K1 улетел на редьюсер.
[22:07.860 --> 22:10.860]  Скажите, по этой схеме какие-нибудь вопросы есть?
[22:10.860 --> 22:12.860]  Нет?
[22:24.860 --> 22:26.860]  Ну тогда идем дальше.
[22:26.860 --> 22:29.860]  Вот я говорил, что есть слово partition.
[22:31.860 --> 22:33.860]  Что это такое?
[22:35.860 --> 22:38.860]  В ходу есть инструмент, который решает,
[22:38.860 --> 22:42.860]  по какому именно принципу вот эти ключи будут улетать на редьюсеры.
[22:42.860 --> 22:46.860]  Ведь может быть такое, что все ключи улетели на один редьюсер,
[22:46.860 --> 22:48.860]  а остальные простаивают.
[22:50.860 --> 22:54.860]  Или, например, как нам гарантировать вот это требование,
[22:54.860 --> 22:58.860]  что K1 есть только здесь и больше чего нигде нету.
[22:58.860 --> 23:05.860]  За это отвечает специальный элемент ходу.
[23:05.860 --> 23:07.860]  Вот, вот я сейчас расскажу вам про группу.
[23:07.860 --> 23:09.860]  Более подробно я про него расскажу в следующий раз,
[23:09.860 --> 23:11.860]  которая называется partition.
[23:11.860 --> 23:14.860]  И по умолчанию он делает следующее.
[23:14.860 --> 23:18.860]  Он берет хэш от ключа, по которому мы разбиваем на группы,
[23:18.860 --> 23:22.860]  и берет остаток отделения на количество редьюсеров.
[23:22.860 --> 23:26.860]  Вот, на выходе у нас получается число от 0 до R,
[23:26.860 --> 23:29.860]  и в зависимости от этого числа данная конкретная пара
[23:29.860 --> 23:32.860]  пойдет на один или другой редьюсер.
[23:35.860 --> 23:45.860]  Вот, теперь поговорим немного про термины.
[23:51.860 --> 23:54.860]  То есть, в общем говоря, в одной программе,
[23:54.860 --> 23:56.860]  в которой мы пишем на ходу,
[23:56.860 --> 23:59.860]  чаще всего будет несколько mappreduce задач.
[23:59.860 --> 24:02.860]  Вот у вас в домашке будет две задачи.
[24:02.860 --> 24:05.860]  В первой задаче будет один mappreduce,
[24:05.860 --> 24:08.860]  одна цепочка, один такт mapp и reduce.
[24:08.860 --> 24:11.860]  Во второй задаче у кого-то будет два друг за другом,
[24:11.860 --> 24:14.860]  нужно будет закодить у кого-то три даже.
[24:14.860 --> 24:17.860]  Ну а в реальной жизни там десятки бывают этих job.
[24:17.860 --> 24:20.860]  Вот, ну и давайте посмотрим на термины.
[24:20.860 --> 24:22.860]  То есть, вот эта вся программа,
[24:22.860 --> 24:25.860]  которая использует Hadoop, называется application.
[24:25.860 --> 24:30.860]  Внутри application один вот этот такт mappreduce называется job.
[24:30.860 --> 24:34.860]  То есть, вот это все, что мы видим вот тут на схеме,
[24:34.860 --> 24:37.860]  это называется job.
[24:37.860 --> 24:42.860]  Один вот этот голубой прямоугольничек mapp или reduce
[24:42.860 --> 24:45.860]  называется task.
[24:45.860 --> 24:49.860]  И у каждой task может быть несколько попыток.
[24:49.860 --> 24:53.860]  Они называются attempts, но попытки так и называются.
[24:53.860 --> 24:56.860]  Откуда может взяться несколько попыток?
[24:56.860 --> 24:59.860]  Во-первых, из-за падений.
[24:59.860 --> 25:01.860]  Из-за падений ноды, из-за падений сети.
[25:01.860 --> 25:04.860]  То есть, если у нас в процессе вычисления нода упала,
[25:04.860 --> 25:09.860]  то Hadoop автоматически, даже никак нас не используя,
[25:09.860 --> 25:12.860]  он перенесет вычисление на другую ноду,
[25:12.860 --> 25:14.860]  и они досчитаются там.
[25:14.860 --> 25:17.860]  Это первый случай, когда может быть много попыток.
[25:17.860 --> 25:20.860]  Второй случай, когда может быть много попыток,
[25:20.860 --> 25:22.860]  это когда падает наш код.
[25:22.860 --> 25:26.860]  Мы будем разбирать чуть позже, как писать код на Hadoop.
[25:26.860 --> 25:30.860]  И может быть такое, ну, логично, что наш код падает.
[25:30.860 --> 25:33.860]  В таком случае система не понимает, что наш код падает,
[25:33.860 --> 25:37.860]  она думает, что проблема на ее стороне пытается чинить.
[25:37.860 --> 25:39.860]  Как она пытается чинить?
[25:39.860 --> 25:43.860]  Она пытается перезапускать этот упавший редьюсер еще раз.
[25:43.860 --> 25:45.860]  То есть, код падает, система перезапускает.
[25:45.860 --> 25:48.860]  Он опять падает, она еще раз перезапускает.
[25:48.860 --> 25:54.860]  Так может быть много раз, в зависимости от настроек данного кластера.
[25:54.860 --> 25:57.860]  У нас так будет происходить всего три раза.
[25:57.860 --> 26:00.860]  Если за три раза система не смогла выполнить ваш код,
[26:00.860 --> 26:03.860]  она сдается и говорит все.
[26:03.860 --> 26:06.860]  Я выполнять ничего не буду, и все падает.
[26:06.860 --> 26:09.860]  И третий случай, когда может быть много попыток,
[26:09.860 --> 26:12.860]  на этот раз у совершенно здоровых Hadoop,
[26:12.860 --> 26:16.860]  это такой подход, как спекулятивное выполнение,
[26:16.860 --> 26:20.860]  speculative execution.
[26:20.860 --> 26:23.860]  Здесь имеется в виду, что на нашем кластере
[26:23.860 --> 26:26.860]  очень много нот, но они все разные.
[26:26.860 --> 26:28.860]  Разные характеристики, разные процессы,
[26:28.860 --> 26:32.860]  разная надежность, разная сеть на этих машинках может быть.
[26:32.860 --> 26:34.860]  И чтобы не заниматься исследованием
[26:34.860 --> 26:37.860]  и не думать, на какой машинке что у нас быстрее посчитается,
[26:37.860 --> 26:40.860]  мы просто запускаем сразу много попыток,
[26:40.860 --> 26:42.860]  одной дольше задачи.
[26:42.860 --> 26:46.860]  Какая первая посчиталась, тот результат мы и взяли.
[26:53.860 --> 26:56.860]  Ну и давайте подведем такие промежуточные итоги.
[26:56.860 --> 26:59.860]  То есть map работает с парами ключ значения.
[27:05.860 --> 27:10.860]  Shaft and sort стадия сортирует данные по ключу и группирует,
[27:10.860 --> 27:13.860]  или диагонально ссортирует данные по ключу,
[27:13.860 --> 27:16.860]  или диагонально ссортирует данные по ключу.
[27:16.860 --> 27:19.860]  И в этом стадии мы не будем делать много шансов,
[27:19.860 --> 27:23.860]  но стадия сортирует данные по ключу и группирует,
[27:23.860 --> 27:29.860]  и вот эти вот группы обрабатывает тоже независимо.
[27:29.860 --> 27:33.860]  То есть так же, как map обрабатывал независимые пары,
[27:33.860 --> 27:35.860]  редьюсер обрабатывает вот эти группы.
[27:39.860 --> 27:42.860]  Ну и если проводить такие параллели с SQL,
[27:42.860 --> 27:47.860]  то map это аналог селекта, аналог UBR,
[27:47.860 --> 27:50.860]  то есть когда мы взяли какую-то одну строчку
[27:50.860 --> 27:54.860]  и сказали селект, нам нужно 2 первых поля в этой строчке.
[27:54.860 --> 27:59.860]  Или взяли where, когда сверили какое-то поле с условием.
[27:59.860 --> 28:04.860]  Никакой зависимости от других строчек нет.
[28:04.860 --> 28:07.860]  То есть селект и where это map,
[28:07.860 --> 28:11.860]  а все остальное, например, joining, gobuy,
[28:11.860 --> 28:14.860]  всякие аналитические функции, это все уже редьюсер.
[28:18.860 --> 28:22.860]  То есть маппер это обычный фильтр, парсинг, форматирование,
[28:22.860 --> 28:26.860]  редьюсер это обычная группировка агрегации.
[28:36.860 --> 28:39.860]  Ну и давайте посмотрим, как это все работает
[28:39.860 --> 28:41.860]  с точки зрения процессов.
[28:41.860 --> 28:43.860]  То есть вот есть наша программа,
[28:43.860 --> 28:45.860]  она форкается на мастер, на нейм-ноду,
[28:45.860 --> 28:50.860]  она форкается на воркеры, на датоноды.
[28:50.860 --> 28:53.860]  И вот input data это у нас HDFS,
[28:53.860 --> 28:55.860]  мы бьем ее на сплиты, как я уже сказал,
[28:55.860 --> 28:58.860]  каждый сплит обрабатывается одним маппером,
[28:58.860 --> 29:00.860]  а результаты мапперов пишутся не в HDFS,
[29:00.860 --> 29:03.860]  а пишутся в локальную файловую систему.
[29:03.860 --> 29:06.860]  То есть если мы вернемся к этой схеме,
[29:06.860 --> 29:09.860]  то вот тут у нас будет локальная файловая система,
[29:09.860 --> 29:12.860]  вот это все хранится в локальной, на ноде.
[29:12.860 --> 29:17.860]  Потом произойдет вот этот вот partitioning,
[29:17.860 --> 29:20.860]  и данные улетят на другую ноду,
[29:20.860 --> 29:22.860]  но это тоже локальная файловая система,
[29:22.860 --> 29:24.860]  это не HDFS.
[29:24.860 --> 29:28.860]  У нас просто HDFS медленнее даже чем обычный диск,
[29:28.860 --> 29:31.860]  поэтому нет смысла на него каждый раз писать.
[29:31.860 --> 29:35.860]  HDFS у нас только вот здесь и вот здесь в конце.
[29:42.860 --> 29:55.860]  Еще есть такой момент,
[29:55.860 --> 30:00.860]  это сам ходук написан на джаве.
[30:00.860 --> 30:04.860]  То есть вот эти все вещи, мапы, редьюсы,
[30:04.860 --> 30:06.860]  все, что мы сейчас разбирали,
[30:06.860 --> 30:09.860]  как это написать в поди, лучше всего написать на джаве,
[30:09.860 --> 30:12.860]  и ходук для этого больше всего приспособлен,
[30:12.860 --> 30:14.860]  но джаву знают не все.
[30:14.860 --> 30:19.860]  Поэтому есть такая технология, как ходук стриминг.
[30:19.860 --> 30:21.860]  Что такое ходук стриминг?
[30:21.860 --> 30:23.860]  Это по сути мы берем уже готовое,
[30:23.860 --> 30:26.860]  написанное не нами приложение на ходуке,
[30:26.860 --> 30:28.860]  вот оно верхняя строчка,
[30:28.860 --> 30:30.860]  то есть тут указан и input формат,
[30:30.860 --> 30:34.860]  то есть парсинг данных автоматический,
[30:34.860 --> 30:38.860]  сортировка автоматическая, запись, все это есть.
[30:38.860 --> 30:41.860]  То, что нам остается сделать, это мапер и редьюсер.
[30:41.860 --> 30:44.860]  А вот мапер и редьюсер мы категорически не хотим писать на джаве,
[30:44.860 --> 30:46.860]  потому что пишем его мы.
[30:46.860 --> 30:49.860]  Мы хотим написать на питоне, например.
[30:49.860 --> 30:51.860]  Мы можем написать на питоне,
[30:51.860 --> 30:53.860]  мы можем использовать вот эту программу
[30:53.860 --> 30:58.860]  и подставить туда свой скрип для мапера и для редьюсера,
[30:58.860 --> 31:01.860]  но для этого надо, чтобы код на питоне, который мы напишем,
[31:01.860 --> 31:03.860]  он соответствовал определенным требованиям,
[31:03.860 --> 31:07.860]  а именно читал данные из STDI
[31:07.860 --> 31:09.860]  Ну, то есть принты обычные делал.
[31:09.860 --> 31:13.860]  Вот давайте посмотрим на вот эти примеры кода.
[31:13.860 --> 31:15.860]  Действительно, вот мапер.
[31:15.860 --> 31:19.860]  Он читает данные из STDI и пишет данные в STDI.
[31:19.860 --> 31:21.860]  Редьюсер точно так же.
[31:21.860 --> 31:24.860]  Читает отсюда, пишет принтами.
[31:37.860 --> 31:40.860]  Вот, то есть мы можем писать программы на Hadoop
[31:40.860 --> 31:43.860]  на самом деле на любом языке программируем.
[31:48.860 --> 31:51.860]  Ну и давайте посмотрим, как вообще это все устроено.
[31:51.860 --> 31:53.860]  Если мы берем Hadoop Streaming,
[31:53.860 --> 31:55.860]  джаву я сейчас показывать не буду,
[31:55.860 --> 31:57.860]  потому что не все ее знают.
[31:57.860 --> 31:59.860]  Если мы берем Hadoop Streaming,
[31:59.860 --> 32:01.860]  то по сути, чтобы запустить нашу программу на Hadoop,
[32:01.860 --> 32:03.860]  мы пишем большую вот такую команду.
[32:03.860 --> 32:06.860]  Вот это вот одна команда, посмотрите.
[32:06.860 --> 32:11.860]  Мы указываем ajar files написанной не нами задачей.
[32:11.860 --> 32:13.860]  Ну и дальше всякие настройки.
[32:13.860 --> 32:15.860]  Как мы ее назовем?
[32:15.860 --> 32:17.860]  Сколько будет редьюсеров?
[32:17.860 --> 32:19.860]  Какие будут коды для мапера?
[32:19.860 --> 32:20.860]  Какие для редьюсеров?
[32:20.860 --> 32:22.860]  Входные и выходные данные.
[32:31.860 --> 32:33.860]  Вот, ну если говорить про Hadoop Streaming в целом,
[32:33.860 --> 32:38.860]  то мапер или юсер – это программа на любом языке программирования.
[32:38.860 --> 32:40.860]  Тут даже написано или Java Class,
[32:40.860 --> 32:42.860]  но если вы знаете джаву,
[32:42.860 --> 32:44.860]  то лучше тогда Streaming не использовать.
[32:44.860 --> 32:47.860]  Читают и стдин, пишут и стдраут.
[32:47.860 --> 32:51.860]  Shaffron Sort – вот этот вот обеспечивает Hadoop,
[32:51.860 --> 32:53.860]  и мы его никак не меняем.
[32:53.860 --> 32:55.860]  То есть можем, конечно, на это повлиять,
[32:55.860 --> 32:57.860]  но из коробки нет.
[32:57.860 --> 33:00.860]  Нужно будет еще там
[33:01.860 --> 33:04.860]  ставить в эту команду дополнительные конфиги,
[33:04.860 --> 33:07.860]  которые будут влиять на Shaffron Sort.
[33:15.860 --> 33:18.860]  Хорошо, есть ли какие-то вопросы по этой части?
[33:31.860 --> 33:34.860]  Давайте я сейчас запущу код.
[33:34.860 --> 33:37.860]  То есть мы уже с вами знаем мапер, знаем редьюсер.
[33:37.860 --> 33:41.860]  Мы его видели вот здесь.
[33:46.860 --> 33:50.860]  И знаем вот этот файл, который называется еще Streaming Driver,
[33:50.860 --> 33:55.860]  и он именно запускает команду в Hadoop.
[34:00.860 --> 34:04.860]  Вот, давайте сделаем run.
[34:12.860 --> 34:14.860]  Говорит, что нету.
[34:18.860 --> 34:20.860]  Сейчас проверим.
[34:21.860 --> 34:24.860]  Говорит, что нету.
[34:24.860 --> 34:27.860]  Говорит, что нету.
[34:28.860 --> 34:30.860]  Сейчас проверим.
[34:49.860 --> 34:52.860]  Ошибка в пути к данным.
[34:57.860 --> 34:59.860]  Вот.
[35:21.860 --> 35:23.860]  Вот что мы видим.
[35:23.860 --> 35:25.860]  Мы подключаемся к менеджеру ресурсов.
[35:25.860 --> 35:36.780]  раз. И видите, что мы тут видим? Мы тут видим total input pass 1, number of splits 2,
[35:36.780 --> 35:44.540]  то есть у нас два сплита, два блока в этом объеме данных. Дальше мы видим,
[35:44.540 --> 35:51.020]  что началось джоба, вот ее progress bar и счетчики. Это всякие системные счетчики,
[35:51.020 --> 35:56.780]  сколько чего прочитано, сколько чего записано, где были ошибки. И на семинарах я вам покажу,
[35:56.780 --> 36:03.100]  как эти счетчики делать самим, как можно добавлять сюда свои счетчики, это бывает удобно для отладки.
[36:03.100 --> 36:08.460]  Ну и вот результат работы программы, то есть мы с вами хотели посчитать вот count,
[36:08.460 --> 36:15.260]  сколько раз каждое слово встретилось, и мы это же и видим. Вот слово и число встречаемости.
[36:21.020 --> 36:44.820]  Давайте для интереса, я зайду на кластик с вот таким пробросом, то есть вот это
[36:44.820 --> 36:51.300]  1988, вот это порт для job history.
[37:44.820 --> 38:05.780]  Есть, то есть вот это вот job history. И давайте, чтобы было понятно,
[38:05.780 --> 38:11.900]  что тут вообще происходит и что можно посмотреть, давайте сделаем баг в задаче.
[38:11.900 --> 38:23.540]  Я сейчас найду задачу, которую я вам показываю. И, например, в маппере, давайте лучше в редьюсере,
[38:23.540 --> 38:38.140]  мы сделаем баг. Вот у нас нет функции вот такой form add, поэтому код будет падать,
[38:38.300 --> 38:54.820]  но давайте мы посмотрим, как он это будет делать. Вот маппер, с маппером все в порядке,
[38:54.820 --> 39:07.340]  а вот на редьюсере мы увидим, какие ошибки. Что это вообще за ошибки? По этим ошибкам сложно
[39:07.340 --> 39:15.100]  что-то сказать, кроме того, что pipe map right output threads, то есть что это значит в переводе на русский.
[39:15.100 --> 39:22.900]  Посмотрим еще раз на вот эту схему стриминга, как происходит передача данных. Вот работает
[39:22.900 --> 39:27.780]  ходуб задач, которую не мы писали, она выдает какой-то результат, и он через pipe, через
[39:27.780 --> 39:33.700]  конвейер, обычно линуксовый, передается в маппер. Маппер работает, снова через линуксовый
[39:33.700 --> 39:39.380]  pipe передается в ходуб на сортировку, и также с редьюсером. В случае нашей ошибки редьюс
[39:39.380 --> 39:45.380]  получает данные, но ничего не выдает на выход, потому что падает, и exception говорит, я не могу
[39:45.380 --> 39:53.780]  дождаться данных. Но кроме того, что он не может дождаться данных, мы ничего совершенно определить
[39:53.780 --> 40:01.940]  вот этим вот странным логом не можем. А как можем? Идем опять в job history, видим, что моя последняя
[40:01.940 --> 40:12.420]  задачка, она failed, вот, и видим, что у нас тут два маппера, и 16 редьюсеров. Ну к редьюсерам мы
[40:12.420 --> 40:19.340]  сейчас вернемся, мапперы у нас отработали правильно. Вопрос, почему на маппера два? Вот, от чего это
[40:19.340 --> 40:31.140]  зависит? Два сегмента? Два сплита, да, если мы посмотрим на вот эти вот логи, то мы увидим
[40:31.140 --> 40:39.420]  number of spilt 2. Все это железно говорит о том, что будет два маппер. То есть задать явно количество
[40:39.420 --> 40:46.700]  мапперов мы вообще не можем. Мы можем задать количество редьюсеров, это да. Вот, а маппер мы можем
[40:46.700 --> 40:57.060]  задать только через размер сплита. Дальше, вот мы видим количество редьюсеров 16, а здесь количество
[40:57.060 --> 41:05.540]  редьюсеров 49. Попыток больше, чем, ну, чем редьюсеров, это уже говорит о том, что у нас что-то падает.
[41:05.540 --> 41:11.700]  Тем более, если попыток у нас в три раза больше. Не на одну, на две, то есть вполне может быть такое,
[41:11.700 --> 41:17.500]  что, ну, по каким-то причинам редьюсер где-то не сработал, может быть ноды что-то, мы переключились
[41:17.500 --> 41:22.700]  на другую ноду, и все в порядке. А когда вот в три раза больше, то мы сразу вспоминаем, что я вам
[41:22.700 --> 41:29.660]  рассказывал. Количество попыток на нашем кластере, три штуки. Если за три раза система не смогла
[41:29.660 --> 41:38.260]  выполнить этот редьюсер, то она просто сдается и падает. Идем в эти попытки, выбираем любую,
[41:38.260 --> 41:46.260]  мы-то знаем с вами, что у нас код неверный и он падает. Поэтому выбираем любую, идем в логи и видим
[41:46.260 --> 42:08.540]  вот это. То есть, да, он отругался на form-add, как мы и хотели. Сейчас я верну это на место. То есть,
[42:08.540 --> 42:14.900]  вот, чтобы смотреть логи, вам нужно пропросить порт JobHistory, зайти в джобу, зайти в попытки и
[42:14.900 --> 42:20.740]  посмотреть логи. По вот этим ничего, кроме того, что падает ваш код, сказать нельзя.
[42:20.740 --> 42:36.140]  Теперь у меня еще один вопрос к вам. Кто помнит, чем плохо, если у нас в HDFS хранятся маленькие файлы?
[42:36.140 --> 42:54.500]  На каждый файл нужен свой блок. Хорошо, ну блок будет и что? У каждого блока есть мета. Да,
[42:54.500 --> 43:01.220]  у каждого блока есть мета, которая хранится на нейм-ноде, которая, ну если не одна, то их какое-то
[43:01.220 --> 43:07.620]  ограниченное количество и плюс это оперативка, которая тоже ограниченное количество. Поэтому с
[43:07.620 --> 43:16.200]  маленькими файлами, с маленькими блоками надо быть осторожным. Вот есть много статей, про это
[43:16.200 --> 43:25.900]  гуглите HDFS small files problem или Hadoop small files problem. Но главное это что? Вот у нас есть рам и саж.
[43:25.900 --> 43:38.100]  Наш код, с помощью которого мы запускаем задачу и у него есть коды мапера и коды редьюсеров. Это
[43:38.100 --> 43:45.300]  маленькие файлы и нам надо как-то их показать всем нодам, потому что мы же не знаем, где запустится,
[43:45.300 --> 43:54.460]  на какой именно ноде запустится мапер или редьюсер. То есть вот тут мы не знаем,
[43:54.460 --> 44:01.060]  где будет K1 обрабатываться, на первой ноде, на второй, на третий. И нам нужно, чтобы на всех
[44:01.060 --> 44:07.260]  нодах где будет запускаться редьюсер вместе с самим редьюсером, чтобы к нам доехал код,
[44:07.260 --> 44:14.700]  что собственно мы хотим выполнять. То есть вот эти файлы MapperPy и ReducerPy, чтобы они приехали
[44:14.700 --> 44:22.220]  на ноды. Загружать их в HDFS плохо, потому что вряд ли у нас будет 64 мегабайта чистого кода.
[44:22.220 --> 44:30.060]  Поэтому что приходится делать? В Hadoop есть такая структура, которая называется распределенный cache,
[44:30.060 --> 44:43.820]  distributed cache. То есть что это за структура такая? Пользоваться ей достаточно легко,
[44:43.980 --> 44:52.100]  достаточно указать минус files. И все, что мы сюда указали, это то, что добавится в этот распределенный
[44:52.100 --> 44:58.620]  cache. Распределенный cache это определенная папка, которая находится на каждой ноде,
[44:58.620 --> 45:06.300]  и именно туда помещаются вот эти файлы. Они не загружаются в HDFS, никак с ней не связаны,
[45:06.300 --> 45:13.580]  но они редованы. То есть положить файлы сюда можно в момент запуска задачи, а когда задача
[45:13.580 --> 45:20.500]  работает, изменять эти файлы уже не получится. Такое часто приходится делать, если у вас
[45:20.500 --> 45:27.180]  какой-нибудь справочник. Вот, например, вам нужно выполнить простую задачу фильтр, например,
[45:27.180 --> 45:34.140]  у вас есть куча данных, они лежат в HDFS, вы считаете их Hadoop, но попутно хотите отфильтровать те
[45:34.140 --> 45:40.500]  данные, которые в каком-нибудь черном списке находятся. Вот, и этот черный список, он небольшой,
[45:40.740 --> 45:48.580]  и вы его подаете в distributed cache вот сюда как файлик, и из маппера, из редьюзера можете это читать.
[45:48.580 --> 45:53.620]  На семинарах я покажу примеры про это.
[46:11.020 --> 46:19.380]  Вот, добавляем файлы в distributed cache, каждая нода будет иметь к ним доступ. Как только
[46:19.380 --> 46:33.740]  задача закончит работать, этот cache уничтожится. Хорошо, какие вопросы сейчас по маппредьюзу в целом?
[46:33.740 --> 46:42.380]  Это такое базовое понимание про маппредьюз. Дальше, но уже, наверное, в следующий раз мы
[46:42.380 --> 46:49.140]  обсудим, как можно настраивать вот этот шахландсорт, то есть пока у нас есть маппер, есть редьюзер,
[46:49.140 --> 46:54.140]  между ними какой-то шахландсорт, который как-то абсортирует данные, но мы не знаем, как на это
[46:54.140 --> 47:00.380]  повлиять. Вот узнаем мы об этом в следующий раз. А какие вопросы сейчас по мапперу редьюзеру?
[47:00.380 --> 47:09.740]  Я слышал, что маппредьюз позволяет очень сильно ускорять многие процессы, которые в обычных
[47:09.740 --> 47:17.260]  ситуациях очень долго бы длились. Это из-за того, что в самом ходу эффективная реализация для чего-то есть?
[47:17.260 --> 47:25.420]  Скорее мы ускоряем за счет того, что у нас много машинок, то есть мы не на одной сидим и
[47:25.500 --> 47:31.700]  последовательно процессим процессик, а мы раскинули, посчитали параллельно, собрали,
[47:31.700 --> 47:39.220]  получили результат. Вот за счет этого мы ускоряем, но в принципе я бы не сказал, что ходу быстрый,
[47:39.220 --> 47:49.420]  то есть как раз он не быстрый. Если мы посмотрим на недостатки, они у ходу поесть, то во-первых,
[47:49.420 --> 47:55.620]  ходу не умеет обрабатывать данные в реальном времени. Почему? Потому что смотрим на эту схему,
[47:55.620 --> 48:04.700]  вот у нас сплиты, мы прочитали сплит, пошли его обрабатывать мапом, дальше пошли-пошли его как-то
[48:04.700 --> 48:11.340]  сортировать, процессить. Если в процессе этой задачи она может несколько минут работать,
[48:11.340 --> 48:19.660]  в ее процессе мы что-то дописали в сплит, то вот эти все стадии никак про этот новый сплит не
[48:19.660 --> 48:29.220]  узнают. Вот это первый недостаток. Второе активное использование дисков, поэтому нельзя сказать,
[48:29.220 --> 48:36.860]  что ходу такой быстрый. Мы данные пишем в HDFS, мы данные пишем на диск, мы читаем с диска,
[48:36.860 --> 48:47.060]  все время идет взаимодействие с диском. Поэтому да, ходу позволяет просто обрабатывать большие
[48:47.060 --> 49:00.900]  данные, без них вы не обработаете, но не так, чтобы это делать быстро. Я сейчас даже найду статью.
[49:00.900 --> 49:18.100]  Вот, скину сейчас в чат зума. Называется common life tools can be 235 times faster than your ходу пластыр.
[49:18.100 --> 49:29.180]  То есть в этой статье говорится, что в принципе, если у вас не очень много данных и вы можете
[49:29.180 --> 49:36.780]  обработать их на одной машине, ходу вам не нужен, потому что ходу это джава, это постоянный старт и
[49:36.780 --> 49:42.540]  стоп джава-машин, это постоянная работа с диском, работа с сетью на разных нодах.
[49:42.540 --> 49:51.140]  И еще один недостаток, это то, что нужно писать много кода.
[49:51.140 --> 50:04.500]  Например, чтобы сделать такую простую задачку на wordcount, нам нужно написать маппер, написать
[50:04.500 --> 50:13.660]  редьюсер, написать еще вот этот вот стриминг драйвер. И конечно, хотя бы такие простые задачи
[50:13.980 --> 50:21.460]  хотелось бы решать быстрее. Мы будем обсуждать, как это все ускорять, как это ускорять по времени,
[50:21.460 --> 50:30.500]  потом как это ускорять по коду, но в базовой реализации это вот так. Хорошо, еще какие-нибудь
[50:30.500 --> 50:31.260]  вопросы есть?
[51:01.100 --> 51:11.500]  Хорошо, давайте тогда посмотрим на вот эту схему. Мы к ней вернемся более подробно в следующий раз,
[51:11.500 --> 51:18.100]  это скорее такая максимально подробная схема, которую вообще удалось найти про то,
[51:18.100 --> 51:28.620]  как работает MapReduce. Тут понятно, что у нас есть маппер, есть partitioner, который потом распорядляет
[51:28.620 --> 51:34.500]  вот эти пары по редьюсерам. Что происходит вот здесь, это мы разберем в следующий раз.
[51:50.020 --> 51:57.260]  Ну и потом кусочки данных из разных нод приходят на редьюс, и мы их начинаем мержить,
[51:57.260 --> 52:02.900]  мержим в несколько этапов, вот final merge, и потом это все попадает на редьюсер.
[52:02.900 --> 52:16.260]  Вот видите, здесь написана память и редьюсер input file, и точно так же в маппере.
[52:16.260 --> 52:29.860]  Здесь буфер, но он может быть переполнен, и тогда будет spill to disk. То есть в принципе
[52:29.860 --> 52:35.660]  ходуб нормально относится к тому, что у нас на выходе будет больше данных, чем на входе.
[52:35.660 --> 52:41.460]  А можете привести какой-нибудь пример, вот когда маппер получил на вход, он же не так-то много
[52:41.460 --> 52:48.180]  данных на вход получает, он получает один вот этот split, то есть один блок, и один блок это не так
[52:48.180 --> 52:54.380]  уж много, ну 60 мегабайт, где-то даже 30. Как может быть так, что на выходе маппер у нас появилось
[52:54.380 --> 53:10.580]  столько данных, что они в оперативную память на сервере не влазят? Сейчас понятен вопрос.
[53:11.460 --> 53:24.820]  Да, вопрос понятен. Ну вот есть ли какой-нибудь пример, как такое может быть?
[53:24.820 --> 53:32.100]  А мы должны им начисло сопоставлять каждому элементу или не обязательно?
[53:32.100 --> 53:35.580]  Ты имеешь в виду ключ значения каждому ключу?
[53:35.580 --> 53:38.740]  Да, значение может быть обязательно.
[53:38.740 --> 53:43.300]  Не обязательно, все что угодно, могут быть любые объекты, мы это на семинарах будем разбирать,
[53:43.300 --> 53:49.220]  это пара просто условно object-object, что там внутри, это уже в MapReduce не столь важно.
[53:49.220 --> 53:56.100]  Главное, чтобы MapReduce умел это сравнивать, потому что ему сортировать надо будет. То есть какие-то
[53:56.100 --> 54:04.700]  сравнимые объекты должны быть. Ну, например, мы будем каждый в подстроке что-то задавать,
[54:04.700 --> 54:14.780]  а подстрок может быть сильно, когда мы храним подстроку, сумма всех подстрок больше, чем строка.
[54:14.780 --> 54:23.540]  Сумма всех подстрок? А, ну, логично, да. То есть у тебя пришла строка, ты генерируешь все возможные подстроки,
[54:23.540 --> 54:33.700]  или еще лучше, у тебя на вход приходят пары left-right, числовые промежутки,
[54:33.700 --> 54:40.020]  а ты на маппере, ну, вот такая у тебя стоит задача, что ты на маппере генерируешь все числа от left-to-right,
[54:40.020 --> 54:47.500]  и получается у тебя на вход пришло, например, два числа, один и пять миллионов, на выходе пять
[54:47.500 --> 54:53.460]  миллионов чисел. Поэтому вполне себе можно придумать такой маппер, можно даже придумать маппер,
[54:53.460 --> 54:59.900]  который вообще не берет входных данных никаких, это просто генерация чего-нибудь, на вход ничего,
[54:59.900 --> 55:08.620]  на выход сколь угодно большая куча данных. Вот, и ходу, он умеет из коробки ничего для этого делать не надо,
[55:08.620 --> 55:13.820]  он умеет справляться с тем, что у нас очень много данных, в результате получилось.
[55:13.820 --> 55:24.300]  Вот, ну, схема страшная, не даром здесь стоит вот этот чувачок, но, в принципе,
[55:24.300 --> 55:31.660]  следующий раз мы ее рассмотрим более подробно. Сейчас есть еще одна ссылка, это тьюториал от
[55:31.660 --> 55:37.460]  Майкла Нола о том, как писать правильно программы на ходу к стриминге, но это тоже скорее программа
[55:37.460 --> 55:43.140]  семинара уже, вам семинаристы рассказывают подробнее, будете разбирать разные задачки.
[55:43.140 --> 55:54.860]  Вот, с таким базовым маппредьюсом сейчас все, в следующий раз мы разберем внутренность маппредьюса,
[55:54.860 --> 56:01.540]  то есть как устроен шафлед-сорт, как можно его настраивать, и еще разберем как в маппредьюсе
[56:01.540 --> 56:04.700]  делать джойны, потому что джойны в маппредьюсе это отдельное поле.
[56:13.740 --> 56:18.460]  Даже раньше все это мы с вами завершили. Есть ли какие-то сейчас вопросы еще?
[56:22.460 --> 56:28.620]  Почему именно маппредьюс, почему именно комбинация двух таких операций так изучается и применяется?
[56:28.620 --> 56:39.780]  Ну, потому что это достаточно универсальный набор операций, то есть все, что мы делаем,
[56:40.620 --> 56:47.220]  мы распределяем, считаем независимо, то есть получается мы можем или считать независимо,
[56:47.220 --> 56:52.380]  или считать зависимо. Вот там, где мы независимо считаем, так называемые bites,itely operations,
[56:52.380 --> 56:59.380]  мы не храним состояние, мы просто взяли элемент, посчитаем и написали куда-то, или state from
[56:59.380 --> 57:07.260]  операцией, когда мы храним какое-то state состояние, и вот тут уже, где мы state храним,
[57:07.260 --> 57:14.500]  то есть в принципе большая часть разных математических операций она ложительна на
[57:14.500 --> 57:20.940]  предьюс. Вот чуть позже мы с вами будем разбирать SQL по верхам а предьюса,
[57:20.940 --> 57:23.100]  то есть SQL тоже ложительна на предьюс.
[57:37.260 --> 57:48.020]  Хорошо, есть ли еще какие-нибудь вопросы?
[58:01.780 --> 58:05.780]  Если вопросов нет, тогда на этом все и всем спасибо.

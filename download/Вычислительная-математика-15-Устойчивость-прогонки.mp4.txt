[00:00.000 --> 00:10.840]  Мы продолжаем тему численно-медленно решение краевых задач для обыкновенных дифференциальных
[00:10.840 --> 00:18.560]  уравнений. Я напомню, на чем мы остановились в прошлый раз и продолжим. Мы начали говорить о
[00:18.560 --> 00:28.320]  задаче штурма Нью Вилле, которая имеет в дифференциальном варианте вот такой вид. Это уравнение
[00:28.320 --> 00:39.420]  второго порядка. С точки зрения физики она описывает либо диффузионные процессы, либо
[00:39.420 --> 00:46.080]  типопроводные, либо вообще процессы расстостранения тех или иных факторов. Оно используется, например,
[00:46.080 --> 00:53.840]  и в экологии, и в социологии, и даже для описания расстостранения эпидемии. Здесь, правда,
[00:53.840 --> 01:02.320]  социональный вариант. Обычно чаще используется в несоциональном варианте это уравнение.
[01:02.320 --> 01:09.080]  Сначала начнем с обыкновенного дифференциального уравнения.
[01:09.080 --> 01:22.400]  Х у нас меняется от 0 до L. Краевые условия, давайте еще раз повторим, важный момент,
[01:22.400 --> 01:35.280]  ну вот так вот такие у нас будут. У альфа-1 у штрих по х от 0 плюс бета-1 от 0 равняется гамма-1.
[01:35.280 --> 01:51.440]  Здесь х равняется 0. Левые ограниченные условия и правые. Альфа-2 у штрих по х от L плюс бета-2
[01:51.440 --> 02:06.880]  у от L равняется гамма-2. Х равняется L. То есть у нас вот такая краевая задача рассматривается.
[02:06.880 --> 02:14.560]  Что мы делаем далее? Далее мы вводим, как обычно, расчетную сетку,
[02:14.560 --> 02:29.200]  омега-n. Ну она имеет такой вид. mH равняется x, n. n это 0, 1 и так далее до n большого.
[02:29.200 --> 02:36.800]  h это шаг интегрирования L, то есть длино отрезка интегрирования делим на n.
[02:36.800 --> 02:48.400]  Ну x0 у нас 0 и x с индексом n большой у нас есть L. Вот это наша расчетная сетка.
[02:48.400 --> 02:54.400]  В данном случае она равномерная, но я вам говорил, что вообще говоря,
[02:54.400 --> 02:58.800]  сетка совсем не обязательно должна быть равномерной. Более того, чаще ее делает
[02:58.800 --> 03:04.160]  неравномерной в зависимости от разных причин. Ну одна из причин, например,
[03:04.640 --> 03:12.080]  сетку можно сгущать в области градиентов решений. Ну и кроме того, не всегда область интегрирования
[03:12.080 --> 03:18.560]  бывает такой хорошей, как в нашем случае отрезок. В многомерном случае это могут быть такие довольно
[03:18.560 --> 03:24.160]  сложные области интегрирования, и так равномерные сетки там просто принципиально невозможны.
[03:24.160 --> 03:34.160]  Так, и далее мы апроксимируем наши дифференциальные уравнения разностными соотношениями.
[03:34.160 --> 03:40.160]  Давайте это сделаем.
[03:40.160 --> 03:46.160]  Для внутренней точки у нас что получится?
[03:46.160 --> 03:56.160]  1 на h, коэффициент диффузии или типа проводности в зависимости от физики задачи.
[03:56.160 --> 04:08.160]  1 вторая, здесь у нас уn плюс 1 минус уn делим на h. Здесь пусть будет kn минус 1 вторая,
[04:08.160 --> 04:24.160]  уn минус 1 делим на h и плюс k1n. Давайте первую производную тоже довольно простым соотношением приблизим.
[04:24.160 --> 04:34.160]  Это не единственный способ дискотизации и апроксимации. Я пока довольно простой вариант выбрал.
[04:34.160 --> 04:44.160]  Равняется g от n, где n меняется от 1 до n. Это дискотизация первого дифференциального уравнения.
[04:44.160 --> 04:58.160]  Здесь можно задать вопрос, почему я первую слагаемую, это вторая производная решила апроксимировать следующим образом.
[04:58.160 --> 05:08.160]  Представим себе, что у нас это узлы сетки n, n минус 1, n плюс 1.
[05:08.160 --> 05:14.160]  Положим, что в точке n у нас есть разрыв каких-то механических свойств.
[05:14.160 --> 05:26.160]  Например, биометаллическая пластинка нагревается, алюминий, какой-нибудь быстрый нагреваемый металл, трудно нагреваемый материал.
[05:26.160 --> 05:34.160]  Как нам выбирать тогда коэффициенты типа проводности? Лучше всего выбирать в центре очерек.
[05:34.160 --> 05:49.160]  То есть здесь n минус 1, n минус 1 и n минус 2. То есть в центре выбираем коэффициенты в центре очерек.
[05:49.160 --> 05:55.160]  Тогда ничего страшного, если в точке n будет разрыв механических свойств.
[05:55.160 --> 06:02.160]  Это внутренняя точка, теперь краевые условия.
[06:02.160 --> 06:08.160]  Ну их давайте тоже я пока довольно простыми соотношениями апроксимирую.
[06:08.160 --> 06:16.160]  Для примера можно их апроксимировать и более сложными соотношениями.
[06:16.160 --> 06:18.160]  Мы об этом попозже будем говорить.
[06:18.160 --> 06:23.160]  Так здесь у нас x равняется 0. Это левая граница и правая граница.
[06:23.160 --> 06:42.160]  Альфа 2 у n минус 1, у n большое это крайняя точка, делим наш и плюс бета 2 у n большое, гамма n, здесь x равняется l.
[06:42.160 --> 06:50.160]  То есть правая граница и так мы получили следующее разное соотношение.
[06:50.160 --> 07:01.160]  Если вы повнимательнее поглядитесь, вы увидите, что то, что я выписал, это есть не что иное, как система линейных алгебраических уравнений.
[07:01.160 --> 07:07.160]  Хотя есть весьма специфическая матрица, есть матрица такого специфического вида.
[07:07.160 --> 07:16.160]  Давайте эту систему представим в более понятном виде для ее решения.
[07:16.160 --> 07:31.160]  Уравнение первое я представлю в следующем виде.
[07:31.160 --> 07:47.160]  Аn у n минус 1 минус bn у n плюс cn у n плюс 1 равняется dn.
[07:47.160 --> 07:56.160]  Здесь n у нас будет меняться от 1 до n большого. Это внутренняя точка.
[07:56.160 --> 08:03.160]  Левая граница. Условенно левая граница давайте мы вы пишем так.
[08:03.160 --> 08:18.160]  b0 у 0 плюс c0 у 1 равняется d0. Здесь n равняется 0.
[08:18.160 --> 08:23.160]  И условия на правой границе представим в таком удобном для нас виде.
[08:23.160 --> 08:38.160]  a н большое у n минус 1 минус b н большое у n большое равняется dn.
[08:38.160 --> 08:44.160]  То есть здесь n равняется n.
[08:44.160 --> 08:54.160]  Вот такая система уравнения. Разумеется между коэффициентами a, b, c и k, k1, k2 имеется связь.
[08:54.160 --> 09:06.160]  Ее можно вычислить. В этом уголке мы эту связь выпишем.
[09:06.160 --> 09:08.160]  Она нам потребуется чуть попозже.
[09:08.160 --> 09:15.160]  Ну, например, а н. Как ее можно представить через коэффициенты исходного дифференциального уравнения?
[09:15.160 --> 09:31.160]  Следующим образом, а n это будет k малое, n минус 1 второе делим на h квадрат и минус k1n делим на 2h.
[09:31.160 --> 09:36.160]  Ну, h всегда мы считаем малым параметром. То есть шаг сетки это всегда малый параметр.
[09:36.160 --> 09:41.160]  Мы считаем он, разумеется, всегда положительный, но он всегда много меньше единицы.
[09:41.160 --> 09:45.160]  Это малый параметр. Поэтому первое слагаемое много больше второго.
[09:45.160 --> 09:52.160]  Поэтому а н мы можем представить как... То есть второе слагаемое просто мы можем пренебречь.
[09:52.160 --> 10:02.160]  k здесь что у нас? xn минус h пополам на h квадрат.
[10:03.160 --> 10:10.160]  Это вот а n. Точно так же мы поступаем с коэффициентом cn.
[10:10.160 --> 10:27.160]  Он будет тоже приближенный равен k малое xn минус h пополам делим на h квадрат.
[10:27.160 --> 10:39.160]  И, что касается правой части dn, это просто будет fn. Она практически не поменялась.
[10:39.160 --> 10:46.160]  В дальнейшем нам потребуется еще оценка отношений вот этих двух коэффициентов а n и cn.
[10:46.160 --> 10:55.160]  Скоро она нам потребуется, поэтому я ее выпишу заранее, чтобы два раза не делать еще одну и ту же работу.
[10:55.160 --> 11:12.160]  Значит а н делим на cn. Приблизительно это есть k малое xn минус h пополам на k малое тоже xn плюс h пополам.
[11:12.160 --> 11:22.160]  И если эти выражения раскрыть по формуле Тейлора до первого члена линейного, то их можно представить как единица плюс ch.
[11:22.160 --> 11:28.160]  Ch тоже приблизительно, где ch много меньше единицы и больше нуля.
[11:28.160 --> 11:35.160]  c это обычный коэффициент, мы его считаем о большой вот единице, а ch всегда это малое число.
[11:35.160 --> 11:40.160]  Вот это сообношение нам чуть позже потребуется.
[11:40.160 --> 11:46.160]  Так, это мы представили нашу систему в таком...
[11:47.160 --> 11:56.160]  k малое, сейчас секундочку, k малое это вот этот коэффициент, а k малое вы это имеете в виду?
[11:56.160 --> 12:00.160]  Это этот и есть этот коэффициент.
[12:00.160 --> 12:09.160]  То есть здесь kn плюс 1 вторая, это есть xn минус h пополам, это есть n минус 1 вторая.
[12:09.160 --> 12:18.160]  Да, давайте, нет-нет, если вопрос задан, значит он имеет...
[12:18.160 --> 12:22.160]  Давайте его напишем тогда.
[12:22.160 --> 12:33.160]  kn плюс 1 вторая, это есть k от x, если помните, в официальном уравнении k зависит от x.
[12:33.160 --> 12:44.160]  Значит, это есть xn плюс h пополам, то есть это значение k в промежуточной точке.
[12:44.160 --> 12:51.160]  То есть если я пишу вот такой коэффициент, то мы это просто писали, надо напомнить, в прошлой семестре.
[12:51.160 --> 12:56.160]  Это означает, что значение функции берется в промежуточной точке между узлами.
[12:56.160 --> 13:04.160]  Центральной точке между узлами m и n плюс 1.
[13:04.160 --> 13:06.160]  Тогда это будет точка 1 плюс 1 вторая.
[13:06.160 --> 13:13.160]  Это довольно часто используется в численных методах, когда точка берется в промежуточной.
[13:13.160 --> 13:17.160]  Так, еще посмотрите, что еще непонятно.
[13:17.160 --> 13:22.160]  Так, ладно, думайте, какой вопрос задать.
[13:22.160 --> 13:27.160]  Так, теперь, значит, мы уже пришли к данной системе уравнений.
[13:27.160 --> 13:38.160]  Если ее представить уже в нормальной матричной форме, то она будет иметь следующий вид.
[13:38.160 --> 13:45.160]  Это некая матрица A, давайте A с волной, без волны потом потребуется, равняется D.
[13:45.160 --> 13:50.160]  Значит, матрица A с волной имеет вот такой вид.
[13:50.160 --> 13:54.160]  Значит, только три главные диагонали у нее не нулевые.
[13:54.160 --> 13:57.160]  Остальные все элементы нулевые.
[13:57.160 --> 14:03.160]  Значит, B0, C0 это левое ограниченное условие фактически.
[14:03.160 --> 14:09.160]  Далее A1 минус B1, C1 многоточек.
[14:09.160 --> 14:16.160]  Далее AN минус BN, CN это некая N строка.
[14:16.160 --> 14:26.160]  Ну и нижняя часть AN минус 1, минус BN минус 1, CN минус 1.
[14:26.160 --> 14:35.160]  Ну и правый ограниченный слой нам дает так, что AN большой минус BN большой.
[14:35.160 --> 14:37.160]  То есть матрица вот такого вида.
[14:37.160 --> 14:39.160]  В основном она нулевая.
[14:39.160 --> 14:46.160]  В основном ее элемент равен нулю, но три главные диагонали не нулевые.
[14:46.160 --> 14:48.160]  Что касается...
[14:48.160 --> 14:51.160]  Ой, D это векторы.
[14:51.160 --> 14:55.160]  Это по своему смыслу векторы.
[14:55.160 --> 14:58.160]  Так, давайте я это затру.
[15:07.160 --> 15:14.160]  Так, U это вектор U0 до Un.
[15:14.160 --> 15:16.160]  Это как раз вектор русского функции.
[15:16.160 --> 15:18.160]  Это наши коэффициенты.
[15:18.160 --> 15:22.160]  И D это вектор совмест правой частей.
[15:22.160 --> 15:31.160]  Вот D1 до Dn большого.
[15:31.160 --> 15:35.160]  То есть имеем такую вот систему уравнений.
[15:35.160 --> 15:38.160]  Такой специфической матрицы.
[15:38.160 --> 15:41.160]  Так, как мы ее решаем?
[15:41.160 --> 15:45.160]  Решаем ее следующим образом.
[15:55.160 --> 15:59.160]  Так, ну пусть эта система слева у нас пока останется.
[15:59.160 --> 16:01.160]  Пока останется.
[16:01.160 --> 16:05.160]  Решение этой системы вот в таком виде.
[16:05.160 --> 16:16.160]  Un-1 равняется Pn на Un плюс Qn.
[16:16.160 --> 16:24.160]  Где Pn и Qn это так называемые прогоночные коэффициенты, которые нам нужно найти.
[16:24.160 --> 16:27.160]  Ну и мы их естественно найдем.
[16:27.160 --> 16:32.160]  Опираясь на краевые условия.
[16:32.160 --> 16:41.160]  Ну а само это соотношение называется традиционно прогоночным соотношением.
[16:41.160 --> 16:48.160]  Если мы это соотношение поставим в уравнение для внутренней точки,
[16:48.160 --> 16:52.160]  то получим немного другое уравнение.
[16:52.160 --> 17:04.160]  Уn равняется Pn плюс 1 на Un плюс 1 плюс Qn плюс 1.
[17:04.160 --> 17:11.160]  Причем из этих двух уравнений мы можем вычислить значения коэффициентов.
[17:11.160 --> 17:16.160]  Pn плюс 1 и Qn плюс 1 так называемые прогоночные коэффициенты.
[17:16.160 --> 17:19.160]  Ну, они легко вычисляются.
[17:19.160 --> 17:29.160]  Pn плюс 1 это есть Cn делим на Bn минус An Pn.
[17:29.160 --> 17:41.160]  Qn плюс 1 это есть An на Cn Bn минус An Pn.
[17:41.160 --> 17:44.160]  Это вот вид прогоночных коэффициентов.
[17:44.160 --> 17:46.160]  Обратите внимание, что они вычисляются,
[17:46.160 --> 17:50.160]  потому что только в правых частях стоят коэффициенты, которые нам известны.
[17:50.160 --> 17:54.160]  То есть A it и B it и C it нам известны.
[17:54.160 --> 18:03.160]  Следовательно, мы можем пройтись от n равного 0 до n равного n
[18:03.160 --> 18:06.160]  и вычислить все прогоночные коэффициенты.
[18:06.160 --> 18:10.160]  Вот это вычисление носит название прямой прогонки.
[18:10.160 --> 18:12.160]  Это вычисление прогоночных коэффициентов.
[18:12.160 --> 18:18.160]  Далее, если мы их вычислили, мы их вычисляем, естественно.
[18:18.160 --> 18:25.160]  Идем с левой верхней части матрицы вправую нижнюю,
[18:25.160 --> 18:31.160]  то дальше нам необходимо вычислить само решение.
[18:31.160 --> 18:36.160]  Само решение мы, конечно же, будем вычислять, исходя из прогоночного соотношения.
[18:36.160 --> 18:40.160]  Давайте возьмем последнее уравнение.
[18:40.160 --> 18:50.160]  Уn-1 равняется pn на u, n плюс qn.
[18:50.160 --> 18:52.160]  Последнее уравнение.
[18:52.160 --> 18:56.160]  Уn мы считаем заданным в правом краевом условии.
[18:56.160 --> 19:01.160]  То есть мы можем найти решение в точке ун-1.
[19:01.160 --> 19:06.160]  Точно так же мы поступаем и с решением в точке ун-2.
[19:06.160 --> 19:15.160]  Это будет pn-1 на u, n-1 и плюс qn-1.
[19:15.160 --> 19:16.160]  Ну и так далее.
[19:16.160 --> 19:22.160]  Дальше ун-3, ун-4 и так до u1.
[19:22.160 --> 19:27.160]  То есть мы находим таким образом решение нашей задачи.
[19:27.160 --> 19:32.160]  Обратите внимание, что сам алгоритм простой.
[19:32.160 --> 19:35.160]  Два цикла.
[19:35.160 --> 19:39.160]  Слева направо и справа налево.
[19:39.160 --> 19:45.160]  И что особо важно, он имеет два важнейших свойства.
[19:45.160 --> 19:46.160]  Первое.
[19:46.160 --> 19:52.160]  В отличие от методогауса, легко оценить количество аризметических действий.
[19:52.160 --> 19:54.160]  Это всего-то обольшое от n.
[19:54.160 --> 19:58.160]  Когда мы имеем такую оценку количества аризметических действий,
[19:58.160 --> 20:02.160]  такие алгоритмы принято называть эффективными.
[20:02.160 --> 20:08.160]  Если n², n³, в математике начинают думать,
[20:08.160 --> 20:12.160]  как бы сделать так, чтобы не было обольшого от n³ или n².
[20:12.160 --> 20:15.160]  Нам нужно уменьшить порядок.
[20:15.160 --> 20:17.160]  Ну и в основном это получается.
[20:17.160 --> 20:21.160]  Есть даже оценки обольшого от логарифма n.
[20:22.160 --> 20:28.160]  И второе, что алгоритм оказывается действительно удивительно устойчив.
[20:28.160 --> 20:33.160]  Правда, для линейных задач и задач с переменными коэффициентами,
[20:33.160 --> 20:37.160]  то есть с коэффициентами, зависящей от независимой переменной.
[20:37.160 --> 20:45.160]  Для нелинейных задач в явном виде алгоритм прогонки не работает.
[20:45.160 --> 20:52.160]  Но можно не линейные задачи�ry и мы об этом будем говорить редуцировать к алгоритмам прогонки,
[20:52.160 --> 20:54.160]  к нескольким алгоритмам прогонки.
[20:54.160 --> 20:55.160]  об этом мы будем говорили.
[20:55.160 --> 21:00.160]  Это уже так называют итерационные процессы в функциональных пространствах.
[21:00.160 --> 21:04.160]  Мы знаем, что есть иерационные процессы для нахождения.
[21:04.160 --> 21:09.160]  Корни нелинейных уравнений, системы линейных уравнениях.
[21:09.160 --> 21:11.860]  Но оказывается и в функциональных пространствах
[21:11.860 --> 21:16.860]  можно с помощью итерационных процессов искать целые функции
[21:16.860 --> 21:19.860]  и одномерные, и двумерные, и многомерные и так далее
[21:19.860 --> 21:23.860]  об этом сейчас мы и будем говорить
[21:23.860 --> 21:29.860]  а теперь вот какую я еще хотел сделать
[21:29.860 --> 21:33.860]  немаловажную ремарку по поводу устойчивости
[21:33.860 --> 21:39.860]  давайте все-таки поговорим об устойчивости этого процесса
[21:42.860 --> 21:46.860]  об устойчивости процесса прогонки
[21:55.860 --> 21:59.860]  так, ну вот давайте сделаем так
[21:59.860 --> 22:01.860]  докажем такую теорию
[22:01.860 --> 22:04.860]  по поводу устойчивости процесса прогонки
[22:04.860 --> 22:09.860]  алгоритм прогонки устойчив при выполнении следующих условий
[22:09.860 --> 22:13.860]  во-первых, это условия диагонального преумладания
[22:13.860 --> 22:17.860]  с которыми мы с вами хорошо знакомы
[22:17.860 --> 22:24.860]  и во-вторых, условия следующие
[22:24.860 --> 22:28.860]  ну, p1, который вычисляется из краевых условий
[22:28.860 --> 22:31.860]  он должен быть от 0 до 1
[22:31.860 --> 22:35.860]  но на самом деле, если это так, то
[22:35.860 --> 22:38.860]  в очень простом методоматематической индукции
[22:39.860 --> 22:47.860]  доказать, что если это так, то и все pn будут лежать в интервале от 0 до 1
[22:47.860 --> 22:51.860]  я это простую не буду доказывать
[22:51.860 --> 22:53.860]  вы ее докажете без проблем без меня
[22:53.860 --> 22:57.860]  мы будем на это опираться
[22:57.860 --> 23:04.860]  разумеется, это так, если выполним условия диагонального преумладания
[23:04.860 --> 23:07.860]  условия диагонального преумладения здесь необходимы
[23:07.860 --> 23:10.860]  так, теперь давайте вот что сделаем
[23:10.860 --> 23:13.860]  докажем вот эту теорию
[23:13.860 --> 23:17.860]  давайте рассмотрим пока какой-то один коэффициент
[23:17.860 --> 23:19.860]  то есть прямой процесс прямой прогонки
[23:19.860 --> 23:21.860]  потом рассмотрим и обратную прогонку
[23:21.860 --> 23:23.860]  да, давайте
[23:31.860 --> 23:34.860]  сейчас мы говорим про алгоритм прогонки
[23:34.860 --> 23:36.860]  вопрос в чем?
[23:55.860 --> 23:59.860]  сейчас мы говорим о устойчивости метод прогонки
[24:04.860 --> 24:13.860]  мы рассмотрим его на примере вычисления коэффициента pn плюс 1
[24:13.860 --> 24:24.860]  как я выписывал, это выражение относится к pn плюс 1
[24:24.860 --> 24:27.860]  это теоретически
[24:27.860 --> 24:31.860]  в реальном компьютере, разумеется, есть погрешности
[24:31.860 --> 24:35.860]  поэтому в реальном компьютере нужно написать так
[24:35.860 --> 24:38.860]  pn плюс 1 плюс некая погрешность
[24:38.860 --> 24:40.860]  delta n плюс 1
[24:40.860 --> 24:43.860]  равняется cn
[24:43.860 --> 24:45.860]  а здесь bn
[24:45.860 --> 24:47.860]  минус am
[24:47.860 --> 24:54.860]  на pn плюс delta n
[24:54.860 --> 24:58.860]  и плюс некая погрешность epsilon n
[24:58.860 --> 25:00.860]  что здесь что?
[25:00.860 --> 25:10.860]  epsilon n это все ошибки, которые накапливаются при вычислении pn плюс 1 на одном шаге
[25:10.860 --> 25:14.860]  то есть все есть ошибки в cн, bn, am
[25:14.860 --> 25:18.860]  все мы их сюда вот вносим
[25:18.860 --> 25:20.860]  в epsilon n
[25:20.860 --> 25:22.860]  что касается delta n плюс 1
[25:22.860 --> 25:25.860]  delta n это так называемая наследственная погрешность
[25:25.860 --> 25:28.860]  которая накапливается при вычислении коэффициента pn
[25:28.860 --> 25:32.860]  то есть при прямой прогонке
[25:32.860 --> 25:35.860]  идем дальше
[25:35.860 --> 25:41.860]  нас интересует как соотносятся pn плюс 1
[25:41.860 --> 25:44.860]  delta n плюс 1 и delta n
[25:44.860 --> 25:46.860]  нас интересует эволюция погрешности
[25:46.860 --> 25:48.860]  будет ли погрешность возрастать
[25:48.860 --> 25:50.860]  будет ли она убывать
[25:50.860 --> 25:53.860]  либо она будет находиться на каком-то таком уровне
[25:53.860 --> 25:55.860]  который нам не мешает решать задачу
[25:55.860 --> 25:57.860]  это важнейший момент
[25:57.860 --> 25:59.860]  если она будет возрастать экспоненциально
[25:59.860 --> 26:03.860]  то надо нам выбирать другой алгоритм, другой метод
[26:03.860 --> 26:07.860]  иногда говорят даже в крайних случаях другую модель
[26:07.860 --> 26:09.860]  по процессу
[26:09.860 --> 26:12.860]  давайте первое слагаемое разложим
[26:12.860 --> 26:17.860]  в вариант Эвера до первого линейного члена
[26:17.860 --> 26:19.860]  это будет cн
[26:19.860 --> 26:21.860]  здесь у нас будет
[26:21.860 --> 26:23.860]  bn минус
[26:23.860 --> 26:26.860]  an pn
[26:26.860 --> 26:28.860]  далее плюс
[26:28.860 --> 26:31.860]  второй слагаемо
[26:31.860 --> 26:33.860]  an cn
[26:33.860 --> 26:35.860]  делим на
[26:35.860 --> 26:37.860]  bn минус
[26:37.860 --> 26:40.860]  an pn
[26:40.860 --> 26:42.860]  в квадрате
[26:42.860 --> 26:46.860]  здесь у нас будет малая величина delta n
[26:46.860 --> 26:49.860]  и плюс
[26:49.860 --> 26:52.860]  epsilon n
[26:52.860 --> 26:56.860]  разложили до первого члена
[26:56.860 --> 26:59.860]  теперь смотрим
[26:59.860 --> 27:02.860]  давайте я сотру сверху
[27:02.860 --> 27:05.860]  смотрим, что у нас на самом деле получилось
[27:05.860 --> 27:08.860]  получилось на самом деле хорошее выражение
[27:08.860 --> 27:11.860]  которое очень быстро упрощается
[27:11.860 --> 27:14.860]  что такое первое слагаемое?
[27:14.860 --> 27:18.860]  это есть не что ни было как pn плюс 1
[27:18.860 --> 27:20.860]  вот оно у нас pn плюс 1
[27:20.860 --> 27:23.860]  то есть мы их просто сокращаем
[27:23.860 --> 27:26.860]  в левой и в правой части
[27:26.860 --> 27:29.860]  у нас получается следующее
[27:34.860 --> 27:37.860]  у нас получается следующее
[27:37.860 --> 27:39.860]  delta n плюс 1
[27:39.860 --> 27:42.860]  это наша погрешность на n плюс первом шаге
[27:42.860 --> 27:45.860]  равна
[27:46.860 --> 27:50.860]  an cn
[27:50.860 --> 27:53.860]  здесь у нас bn минус
[27:53.860 --> 27:56.860]  an pn в квадрате
[27:56.860 --> 27:59.860]  на delta n и плюс
[27:59.860 --> 28:02.860]  epsilon n
[28:02.860 --> 28:05.860]  плюс epsilon n
[28:05.860 --> 28:08.860]  здесь трудно не рассмотреть
[28:08.860 --> 28:11.860]  если здесь знаменатель в квадрате
[28:11.860 --> 28:14.860]  тот же, что и выражение для pn плюс 1
[28:14.860 --> 28:16.860]  нужно возвести cn в квадрат
[28:16.860 --> 28:19.860]  и это будет просто pn плюс 1 в квадрате
[28:19.860 --> 28:22.860]  то есть это будет что у нас?
[28:22.860 --> 28:25.860]  an делим на cn
[28:25.860 --> 28:28.860]  а здесь cn в квадрате
[28:31.860 --> 28:34.860]  и наш знаменатель
[28:34.860 --> 28:37.860]  bn минус an
[28:37.860 --> 28:42.860]  и pn в квадрате
[28:42.860 --> 28:44.860]  ну плюс
[28:44.860 --> 28:46.860]  давайте вместо epsilon n
[28:46.860 --> 28:48.860]  оставим просто epsilon
[28:48.860 --> 28:50.860]  имея ввиду, что epsilon n
[28:50.860 --> 28:53.860]  меньше равняется epsilon и больше 0
[28:53.860 --> 28:56.860]  то есть некий максимальный такой погрешность
[28:56.860 --> 28:58.860]  оставим epsilon
[28:58.860 --> 29:01.860]  то есть индекс m нам не очень принципиальный
[29:01.860 --> 29:03.860]  для этой погрешности
[29:03.860 --> 29:05.860]  ну а что это такое?
[29:05.860 --> 29:07.860]  здесь очень просто разглядеть
[29:07.860 --> 29:10.860]  выражение очень сильно упрощается
[29:10.860 --> 29:14.860]  для эволюции погрешности вычислений
[29:14.860 --> 29:17.860]  вот эта эволюция погрешности вычислений
[29:17.860 --> 29:20.860]  она очень важна в любом вычислительном процессе
[29:20.860 --> 29:24.860]  даже если вы работаете с дабл пресиженной
[29:24.860 --> 29:27.860]  с какой угодноточностью
[29:27.860 --> 29:30.860]  вы обязательно делаете ошибки
[29:30.860 --> 29:33.860]  в многократных вычлениях
[29:33.860 --> 29:36.860]  разумеется, если вы там 10 раз одно и то же
[29:36.860 --> 29:39.860]  действия подряд делать, ничего страшного
[29:39.860 --> 29:41.860]  чаще всего приходится
[29:41.860 --> 29:44.860]  одну и ту же группу в аэспетических действиях
[29:44.860 --> 29:46.860]  делать миллионы раз
[29:46.860 --> 29:49.860]  и вот здесь вы можете
[29:49.860 --> 29:51.860]  наступить на
[29:51.860 --> 29:53.860]  острый подводный камень
[29:53.860 --> 29:55.860]  то есть на большие погрешности
[29:55.860 --> 29:58.860]  это означает, что dn плюс 1
[29:58.860 --> 30:00.860]  меньше равняется, чем у нас
[30:00.860 --> 30:02.860]  давайте я в скобках отмечу
[30:02.860 --> 30:05.860]  an на cn, поскольку мы оценивали его
[30:05.860 --> 30:07.860]  а здесь pn
[30:07.860 --> 30:09.860]  плюс 1 в квадрате
[30:09.860 --> 30:11.860]  здесь вот я
[30:11.860 --> 30:13.860]  дельта n упустил
[30:13.860 --> 30:15.860]  на тоже
[30:15.860 --> 30:17.860]  погрешность на n
[30:19.860 --> 30:21.860]  на n-м слой и плюс эпсилон
[30:21.860 --> 30:23.860]  ну вот
[30:23.860 --> 30:26.860]  теперь, посмотрите внимательно на это выражение
[30:26.860 --> 30:28.860]  мы его, оказывается, можем очень быстро
[30:28.860 --> 30:30.860]  и очень резко упростить
[30:30.860 --> 30:32.860]  каким образом?
[30:32.860 --> 30:34.860]  pn плюс 1 в квадрате
[30:34.860 --> 30:38.860]  это величина, которая меняется от 0 до 1
[30:38.860 --> 30:40.860]  an к cn
[30:40.860 --> 30:42.860]  если вы помните, я совсем недавно
[30:42.860 --> 30:44.860]  проводил оценки этой величины
[30:44.860 --> 30:46.860]  и, как видимо, не зря
[30:46.860 --> 30:48.860]  тогда мы можем
[30:48.860 --> 30:50.860]  написать, что это есть
[30:50.860 --> 30:52.860]  единица плюс ch
[30:54.860 --> 30:56.860]  на дельта n
[30:56.860 --> 30:58.860]  и плюс
[30:58.860 --> 31:00.860]  и плюс эпсилон
[31:00.860 --> 31:02.860]  дельта n и плюс эпсилон
[31:02.860 --> 31:04.860]  и вот так мы упростили наше выражение
[31:04.860 --> 31:06.860]  для эволюции погрешности
[31:08.860 --> 31:10.860]  ну теперь давайте вот что сделать
[31:10.860 --> 31:12.860]  нам бы хотелось, конечно, знать
[31:12.860 --> 31:14.860]  как зависит погрешность на n-м шаге
[31:14.860 --> 31:16.860]  от нуевой погрешности
[31:16.860 --> 31:18.860]  от дельта 0
[31:18.860 --> 31:20.860]  это тоже можно сделать
[31:20.860 --> 31:22.860]  совсем несложно
[31:22.860 --> 31:24.860]  ну давайте я уже сотру
[31:24.860 --> 31:26.860]  исходные расслабления
[31:26.860 --> 31:28.860]  мы их уже знаем наизусть, наверное
[31:32.860 --> 31:34.860]  так
[31:36.860 --> 31:38.860]  что у нас получится
[31:38.860 --> 31:40.860]  ну давайте вот такую
[31:40.860 --> 31:42.860]  лестнику сделаем
[31:42.860 --> 31:44.860]  дельта 1 меньше не равняется
[31:44.860 --> 31:46.860]  единице плюс ch
[31:46.860 --> 31:48.860]  ch, допоминаю
[31:48.860 --> 31:50.860]  величина много меньше единицы
[31:50.860 --> 31:52.860]  и больше 0
[31:52.860 --> 31:54.860]  значит, на дельта n
[31:54.860 --> 31:56.860]  на дельта n
[31:58.860 --> 32:00.860]  плюс
[32:00.860 --> 32:02.860]  ну здесь уже дельта 0 будет
[32:02.860 --> 32:04.860]  и плюс эпсилон
[32:04.860 --> 32:06.860]  идем дальше
[32:06.860 --> 32:08.860]  дельта 2
[32:08.860 --> 32:10.860]  меньше не равняется единице плюс
[32:10.860 --> 32:12.860]  ch
[32:12.860 --> 32:14.860]  в квадрате на дельта 0
[32:16.860 --> 32:18.860]  здесь будет эпсилон
[32:18.860 --> 32:20.860]  единица плюс
[32:20.860 --> 32:22.860]  ch
[32:22.860 --> 32:24.860]  ну дельта 0
[32:24.860 --> 32:26.860]  то есть я поставил дельта 2
[32:26.860 --> 32:28.860]  меньше равняется единица плюс ch
[32:28.860 --> 32:30.860]  дельта 1
[32:30.860 --> 32:32.860]  ну а вместо дельта 1 я дал верхнюю цинку
[32:32.860 --> 32:34.860]  тогда у меня получается здесь квадрат
[32:34.860 --> 32:36.860]  а здесь вот накапливается некая сумма
[32:36.860 --> 32:38.860]  ну давайте
[32:38.860 --> 32:40.860]  чтобы было до конца все понятно
[32:40.860 --> 32:42.860]  еще дельта 3 допишу
[32:42.860 --> 32:44.860]  дельта 3 меньше не равно
[32:44.860 --> 32:46.860]  единица плюс ch
[32:46.860 --> 32:48.860]  в кубе на дельта 0
[32:48.860 --> 32:50.860]  плюс эпсилон
[32:50.860 --> 32:52.860]  а здесь у нас накапливается вот такая сумма
[32:52.860 --> 32:54.860]  единица
[32:54.860 --> 32:56.860]  единица плюс ch
[32:56.860 --> 32:58.860]  единица плюс
[32:58.860 --> 33:00.860]  ch в квадрате
[33:00.860 --> 33:02.860]  так ну и так далее
[33:02.860 --> 33:04.860]  и так далее до
[33:04.860 --> 33:06.860]  дельта n
[33:06.860 --> 33:08.860]  значит дельта n будет оцениваться следующим образом
[33:08.860 --> 33:10.860]  единица плюс
[33:10.860 --> 33:12.860]  ch
[33:12.860 --> 33:14.860]  в степени n
[33:14.860 --> 33:16.860]  на дельта 0
[33:16.860 --> 33:18.860]  плюс эпсилон
[33:18.860 --> 33:20.860]  и здесь накапливается как вы видите
[33:20.860 --> 33:22.860]  геометрической
[33:22.860 --> 33:24.860]  прогрессии
[33:24.860 --> 33:26.860]  и он прекрасно известен
[33:26.860 --> 33:28.860]  единица плюс
[33:28.860 --> 33:30.860]  ch в степени n
[33:30.860 --> 33:32.860]  минус 1
[33:34.860 --> 33:36.860]  ну если есть
[33:36.860 --> 33:38.860]  геометрическая прогрессия
[33:38.860 --> 33:40.860]  то ее всегда можно просуммировать
[33:40.860 --> 33:42.860]  как вас учили в школе
[33:42.860 --> 33:44.860]  и получить
[33:44.860 --> 33:46.860]  окончательную оценку
[33:46.860 --> 33:48.860]  которая нам очень важна
[33:48.860 --> 33:50.860]  с точки зрения
[33:50.860 --> 33:52.860]  оценки устойчивости
[33:52.860 --> 33:54.860]  процесса прогонки
[33:54.860 --> 33:56.860]  или как
[33:56.860 --> 33:58.860]  в американской терминологии
[33:58.860 --> 34:00.860]  алгоритмом Томпсона
[34:00.860 --> 34:02.860]  так значит что это будет
[34:02.860 --> 34:04.860]  ну давайте я сюда
[34:04.860 --> 34:06.860]  принесу наверх
[34:06.860 --> 34:08.860]  значит
[34:08.860 --> 34:10.860]  единица плюс
[34:10.860 --> 34:12.860]  ch в степени n
[34:12.860 --> 34:14.860]  на дельта 0
[34:14.860 --> 34:16.860]  ну а здесь эпсилон
[34:16.860 --> 34:18.860]  на сумме геометрической прогрессии
[34:18.860 --> 34:20.860]  ch в степени n
[34:20.860 --> 34:22.860]  минус 1
[34:22.860 --> 34:24.860]  и внизу единица плюс ch
[34:24.860 --> 34:26.860]  минус 1
[34:26.860 --> 34:28.860]  меньшее равне с единичкой
[34:28.860 --> 34:30.860]  мы здесь предъебрежем
[34:30.860 --> 34:32.860]  поскольку здесь величина положительная
[34:32.860 --> 34:34.860]  получим единица плюс ch
[34:34.860 --> 34:36.860]  в степени n
[34:36.860 --> 34:38.860]  на дельта 0
[34:38.860 --> 34:40.860]  ну и здесь видно
[34:40.860 --> 34:42.860]  эпсилон на ch
[34:42.860 --> 34:44.860]  на единица плюс ch
[34:44.860 --> 34:46.860]  в степени n
[34:46.860 --> 34:48.860]  что это такое
[34:48.860 --> 34:50.860]  это есть
[34:50.860 --> 34:52.860]  давайте единица плюс ch
[34:52.860 --> 34:54.860]  вынесем за скобки
[34:54.860 --> 34:56.860]  в степени n
[34:56.860 --> 34:58.860]  в скобках остается дельта 0
[34:58.860 --> 35:00.860]  плюс эпсилон
[35:00.860 --> 35:02.860]  на ch
[35:02.860 --> 35:04.860]  ну и поскольку ch величина малая
[35:04.860 --> 35:06.860]  мы просто эту скобку
[35:06.860 --> 35:08.860]  степенной функцию можем представить в виде экспонента
[35:08.860 --> 35:10.860]  е в степени ch
[35:10.860 --> 35:12.860]  n
[35:12.860 --> 35:14.860]  а здесь дельта 0
[35:14.860 --> 35:16.860]  плюс эпсилон
[35:16.860 --> 35:18.860]  хорошо
[35:18.860 --> 35:20.860]  вот это окончательный
[35:20.860 --> 35:22.860]  итог наших всех
[35:22.860 --> 35:24.860]  рассуждений
[35:24.860 --> 35:26.860]  что отсюда мы видим
[35:26.860 --> 35:28.860]  то есть как эволюционирует
[35:28.860 --> 35:30.860]  наша погрешность
[35:30.860 --> 35:32.860]  так здесь у нас давайте
[35:32.860 --> 35:34.860]  напишем дельта
[35:34.860 --> 35:36.860]  n плюс 1
[35:36.860 --> 35:38.860]  чтобы было понятнее
[35:38.860 --> 35:40.860]  ну в скобках дельта 0
[35:40.860 --> 35:42.860]  величина малая
[35:42.860 --> 35:44.860]  эпсилон это величина порядка машинной ошибки
[35:44.860 --> 35:46.860]  ch всегда много больше
[35:46.860 --> 35:48.860]  величины
[35:48.860 --> 35:50.860]  машинной ошибки
[35:50.860 --> 35:52.860]  по крайней мере h нужно выбирать так чтобы
[35:52.860 --> 35:54.860]  эти величины были много больше машинной ошибки
[35:54.860 --> 35:56.860]  ну если вы помните еще где-то
[35:56.860 --> 35:58.860]  в сентябре мы с вами говорили
[35:58.860 --> 36:00.860]  и до семинарх видимо вы в лабораторной делали
[36:00.860 --> 36:02.860]  что например если мы
[36:02.860 --> 36:04.860]  чисто дифференцируем какую-то функцию
[36:04.860 --> 36:06.860]  ну например в простейшем образом
[36:06.860 --> 36:08.860]  с первым порядком процемации
[36:08.860 --> 36:10.860]  то шаг выбирать близко
[36:10.860 --> 36:12.860]  к машинной точности
[36:12.860 --> 36:14.860]  ошибка будет накапливаться
[36:14.860 --> 36:16.860]  здесь аналогичная ситуация
[36:16.860 --> 36:18.860]  то есть всегда нужно чтобы шаг
[36:18.860 --> 36:20.860]  был много больше
[36:20.860 --> 36:22.860]  машинной погрешности
[36:22.860 --> 36:24.860]  то есть нельзя убирать
[36:24.860 --> 36:26.860]  там скажем так бесконечно мало
[36:26.860 --> 36:28.860]  ну бесконечно мало
[36:28.860 --> 36:30.860]  теперь экспонента
[36:30.860 --> 36:32.860]  да экспонента всегда смущает
[36:32.860 --> 36:34.860]  потому что экспонент означает
[36:34.860 --> 36:36.860]  накопление погрешности
[36:36.860 --> 36:38.860]  но здесь у нас какая ситуация
[36:38.860 --> 36:40.860]  c h n
[36:40.860 --> 36:42.860]  c x n
[36:42.860 --> 36:44.860]  c это величина всегда порядка
[36:44.860 --> 36:46.860]  у большого от единицы
[36:46.860 --> 36:48.860]  x n ну для краевых задач
[36:48.860 --> 36:50.860]  в отличие от задач каши
[36:50.860 --> 36:52.860]  все-таки всегда-всегда этот интеграл
[36:52.860 --> 36:54.860]  интервал интегрирования ограничен
[36:54.860 --> 36:56.860]  обычно тоже в больших случаях
[36:56.860 --> 36:58.860]  порядка у большого от единицы
[36:58.860 --> 37:00.860]  но не всегда
[37:00.860 --> 37:02.860]  когда я говорю о жестких краевых задачах
[37:02.860 --> 37:04.860]  некорректных то это вот как раз
[37:04.860 --> 37:06.860]  задачи да их не очень много
[37:06.860 --> 37:08.860]  они не часто встречаются
[37:08.860 --> 37:10.860]  задачи вычислительно некорректные
[37:10.860 --> 37:12.860]  то есть такие когда вот эта
[37:12.860 --> 37:14.860]  экспонента может
[37:14.860 --> 37:16.860]  расти может расти
[37:16.860 --> 37:18.860]  таких задач немного но они
[37:18.860 --> 37:20.860]  встречаются подавляющее в большинстве
[37:20.860 --> 37:22.860]  вот оценка эволюции
[37:22.860 --> 37:24.860]  погрешности имеет такой вид
[37:24.860 --> 37:26.860]  ну в общем она где-то
[37:26.860 --> 37:28.860]  порядка у большого от единицы
[37:28.860 --> 37:30.860]  то есть это вот нормальная погрешность
[37:30.860 --> 37:32.860]  она растет
[37:32.860 --> 37:34.860]  не сильно либо вообще
[37:34.860 --> 37:36.860]  не растет
[37:36.860 --> 37:38.860]  есть доказательства
[37:38.860 --> 37:40.860]  прямой прогонки разумеется то же самое
[37:40.860 --> 37:42.860]  я могу написать
[37:42.860 --> 37:44.860]  для второго коэффициента прогоночного
[37:44.860 --> 37:46.860]  это q
[37:46.860 --> 37:48.860]  n плюс 1
[37:48.860 --> 37:50.860]  это a n
[37:50.860 --> 37:52.860]  на q m
[37:52.860 --> 37:54.860]  на q m минус
[37:54.860 --> 37:56.860]  d n
[37:56.860 --> 37:58.860]  делим на знаменатель
[37:58.860 --> 38:00.860]  b n минус a n
[38:00.860 --> 38:02.860]  b n
[38:02.860 --> 38:04.860]  те же самые
[38:04.860 --> 38:06.860]  в рассуждении можно проявить и тоже
[38:06.860 --> 38:08.860]  оценить что
[38:08.860 --> 38:10.860]  будет вот какая
[38:10.860 --> 38:12.860]  погрешность для этого коэффициента
[38:12.860 --> 38:14.860]  но это не буду делать это все аналогично
[38:18.860 --> 38:20.860]  то есть прямая прогонка в общем-то
[38:20.860 --> 38:22.860]  мы видим процесс устойчивый
[38:22.860 --> 38:24.860]  что касается обратной
[38:24.860 --> 38:26.860]  прогонки
[38:26.860 --> 38:28.860]  что касается обратной прогонки
[38:28.860 --> 38:30.860]  каким будет
[38:30.860 --> 38:32.860]  этот процесс
[38:32.860 --> 38:34.860]  будет ли он устойчив
[38:38.860 --> 38:40.860]  ну это просто следует
[38:40.860 --> 38:42.860]  из прогоночного соотношения
[38:46.860 --> 38:48.860]  еще не закончили
[38:48.860 --> 38:50.860]  я сказал
[38:50.860 --> 38:52.860]  про устойчивость прямой прогонки
[38:52.860 --> 38:54.860]  это половина
[38:54.860 --> 38:56.860]  теоремы но очень важная
[38:56.860 --> 38:58.860]  что касается обратной прогонки
[38:58.860 --> 39:00.860]  то ее устойчивость следует
[39:00.860 --> 39:02.860]  следует прямо из прогоночного соотношения
[39:02.860 --> 39:04.860]  я его напишу и сразу будет понятно
[39:06.860 --> 39:08.860]  у
[39:08.860 --> 39:10.860]  n это что у нас
[39:10.860 --> 39:12.860]  это есть p n плюс 1
[39:12.860 --> 39:14.860]  на дельта
[39:14.860 --> 39:16.860]  n плюс 1
[39:16.860 --> 39:18.860]  и плюс q
[39:18.860 --> 39:20.860]  n плюс 1
[39:20.860 --> 39:22.860]  это у нас
[39:22.860 --> 39:24.860]  так описывается обратная прогонка
[39:26.860 --> 39:28.860]  в реальном компьютере это u n плюс
[39:28.860 --> 39:30.860]  delta n
[39:30.860 --> 39:32.860]  равняется
[39:32.860 --> 39:34.860]  p n плюс 1
[39:34.860 --> 39:36.860]  а здесь delta n
[39:36.860 --> 39:38.860]  плюс 1
[39:38.860 --> 39:40.860]  так
[39:40.860 --> 39:42.860]  простите
[39:42.860 --> 39:44.860]  здесь у нас u n плюс 1
[39:44.860 --> 39:46.860]  u n плюс 1
[39:46.860 --> 39:48.860]  давайте
[39:48.860 --> 39:50.860]  последнее будем делать
[39:50.860 --> 39:52.860]  u n плюс 1
[39:52.860 --> 39:54.860]  плюс delta
[39:54.860 --> 39:56.860]  n плюс 1
[39:56.860 --> 39:58.860]  плюс q n плюс 1
[39:58.860 --> 40:00.860]  и
[40:00.860 --> 40:02.860]  суммарная погрешность
[40:02.860 --> 40:04.860]  я описывал
[40:04.860 --> 40:06.860]  n малая
[40:06.860 --> 40:08.860]  суммарная погрешность тех вычислений
[40:08.860 --> 40:10.860]  на n шаге
[40:10.860 --> 40:12.860]  ну а delta n это у нас
[40:12.860 --> 40:14.860]  как я говорил у нас следственная погрешность
[40:14.860 --> 40:16.860]  опять смотрим
[40:16.860 --> 40:18.860]  u n это есть
[40:18.860 --> 40:20.860]  p n плюс 1
[40:20.860 --> 40:22.860]  на u n плюс 1 и плюс q n плюс 1
[40:22.860 --> 40:24.860]  сразу все сокращается практически
[40:24.860 --> 40:26.860]  остается
[40:26.860 --> 40:28.860]  delta n
[40:28.860 --> 40:30.860]  равняется
[40:30.860 --> 40:32.860]  p n плюс 1
[40:32.860 --> 40:34.860]  на delta n плюс
[40:34.860 --> 40:36.860]  1 и плюс
[40:36.860 --> 40:38.860]  q n
[40:38.860 --> 40:40.860]  вот что у нас остается
[40:40.860 --> 40:42.860]  при этом вы хорошо знаете что
[40:42.860 --> 40:44.860]  p n плюс 1 лежит на интервале
[40:44.860 --> 40:46.860]  от 0 до 1
[40:46.860 --> 40:48.860]  все доказательства закончены
[40:48.860 --> 40:50.860]  вы конечно скажете
[40:50.860 --> 40:52.860]  а как влияет q n
[40:52.860 --> 40:54.860]  и t слова
[40:54.860 --> 40:56.860]  мы только что показывали
[40:56.860 --> 40:58.860]  предоказательств устойчивости
[40:58.860 --> 41:00.860]  по είому прогонки как она влияет
[41:00.860 --> 41:08.860]  то есть прагонка также
[41:08.860 --> 41:10.860]  будут устойчивы
[41:10.860 --> 41:12.860]  прогонка устойчива
[41:12.860 --> 41:14.860]  при выполнении
[41:14.860 --> 41:16.860]  naw discretion
[41:16.860 --> 41:18.860]  зачем я говорю
[41:18.860 --> 41:20.860]  address
[41:20.860 --> 41:27.580]  только трехточечные и не только вот такого вида, о котором я говорил. Сейчас я
[41:27.580 --> 41:36.780]  говорил о прогулке, которая идет как бы слева направо, с левой межней части
[41:36.780 --> 41:43.460]  матрицы в правую нижнюю, на самом деле можно делать обратную прогулку, то есть
[41:43.460 --> 41:52.660]  вычислять коэффициенты прогуточные справа-налево. Это тоже будет
[41:52.660 --> 41:57.860]  совершенно законная прогулка. Более того, можно делать процесс встречных прогулок,
[41:57.860 --> 42:03.620]  то есть одновременно вычислять коэффициенты и слева сверху, и справа внизу, и в
[42:03.620 --> 42:09.500]  Тайчаке где-то в центре матрицы. Называется процесс встречных прогулок.
[42:09.500 --> 42:15.500]  Другой вопрос и очень важный, что и когда нужно делать. В основном, конечно,
[42:15.500 --> 42:22.380]  делается именно прогулка в таком виде, в котором я вам ее представил.
[42:22.380 --> 42:35.420]  Это суммарная погрешность, которая допускается при всех анимитических вычислениях на ином шаге.
[42:35.420 --> 42:41.900]  То есть у нас же погрешность везде есть. И когда у n, и по n, и коэн вычисляем, везде у нас есть погрешность.
[42:41.900 --> 42:47.620]  Сумма в сумме мы берем эту погрешность и объединяем в epsilon n, чтобы не возиться...
[42:47.620 --> 43:02.220]  Нет, это вот правильный коэффициент, а погрешность, которую мы допустили при его вычислении, вот здесь сидит.
[43:02.220 --> 43:10.580]  Мы все эти погрешности для при вычлении qn плюс 1 и pn плюс 1 загнали вот сюда, в epsilon n,
[43:10.580 --> 43:15.380]  чтобы не возиться с лишними обозначениями.
[43:15.380 --> 43:22.740]  Все понятно? Просто это обличение нашей жизни.
[43:23.740 --> 43:34.180]  Как вы предполагали, написать qn плюс 1 и pn плюс 1 плюс epsilon, здесь можно написать и так далее.
[43:34.180 --> 43:45.180]  В принципе, мы получим точно тот же результат, но здесь я просто пишу гораздо меньше зачков, чем в первом случае, это экономит время.
[43:45.620 --> 44:00.820]  Могу сказать, что прогонка может быть прямая, обратная, может быть встречная, но могут быть прогонки не трехточечные, но, например, пятиточечные.
[44:00.820 --> 44:07.580]  При решении дифференциально уравнений четвертого порядка... Нет, не точность, четвертого порядка.
[44:07.980 --> 44:13.980]  Об этом чуть позже. Сейчас важный момент следующий.
[44:27.980 --> 44:31.980]  Теперь перейдем к задачам нелинейным.
[44:32.380 --> 44:41.380]  Прогонка прекрасного алгоритма хороша для нелинейных задач либо для задач с переменными коэффициентами.
[44:41.380 --> 44:53.380]  Конечно, есть большое желание свести решение нелинейных задач к этим простым и устойчивым алгоритмам.
[44:53.380 --> 44:57.380]  Это, в общем-то, удается сделать.
[44:57.780 --> 45:01.780]  Давайте посмотрим, как.
[45:05.780 --> 45:09.780]  Давайте я упрощу немного наше уравнение.
[45:09.780 --> 45:24.780]  Напишу так, у, вторая производная, неизвестная переменная, равняется f у, где у это уже правая нелинейная часть.
[45:25.180 --> 45:29.180]  Это, в принципе, она тут же нам портит всю жизнь.
[45:29.180 --> 45:39.180]  Нелинейность, но самый интересный, как я говорил, процессы, и физики, и механики, и медицине, и в экономике, как правило, нелинейные.
[45:39.180 --> 45:45.180]  Ну, не как правило, есть, конечно, линейный процесс, но, в основном, интерес представляет нелинейный процесс.
[45:45.580 --> 45:59.580]  Ну, и условия на границах пусть будет у от 0, пусть, например, равно d малое, а у от l пусть будет d большое.
[45:59.580 --> 46:03.580]  Давайте аппроцимируем нашу производную вторую.
[46:03.580 --> 46:06.580]  Мы научились это сделать в прошлом семестре.
[46:06.980 --> 46:18.980]  И будет следующий вид иметь, om-1-2, om-n-1 делим на h квадрат, и это равняется fn.
[46:18.980 --> 46:25.980]  fn меняется от единицы до m-1.
[46:26.380 --> 46:36.380]  Так, и у 0 внизу, это есть у нас d малое, у l внизу, это есть d большое.
[46:36.380 --> 46:38.380]  Вот так.
[46:38.380 --> 46:44.380]  Ну, и в операторном виде, вот этот оператор, аппроцимирующий вторую производную,
[46:44.380 --> 46:52.380]  обычно такое уже каноническое представление, это лямбда xx, а у лямбда равняется fn.
[46:52.780 --> 46:56.780]  То есть, этот оператор, разница, представляется, обычно обозначается следующим образом,
[46:56.780 --> 46:57.780]  лямбда xx.
[46:57.780 --> 47:01.780]  Это стандартное такое обозначение принятое.
[47:03.780 --> 47:05.780]  И вот что мы хотим сделать.
[47:05.780 --> 47:11.780]  Давайте сначала рассмотрим первый метод стрельбы.
[47:11.780 --> 47:18.780]  Он использовался активно, где-то еще 100 назад, например, в задачах в баллистике.
[47:19.180 --> 47:28.180]  Есть такая задача, сейчас, конечно, все гораздо более точнее, и проще стрельба по закрытым мишеням.
[47:28.180 --> 47:36.180]  То есть, есть пушка, перед ней гора, и нужно попасть в мишень, которая находится за горой.
[47:36.180 --> 47:40.180]  Эта задача решалась как-нибудь странным методом стрельбы.
[47:40.180 --> 47:42.180]  Правда, конечно, это не только это.
[47:42.180 --> 47:44.180]  Но вот как это делается?
[47:44.180 --> 47:48.180]  Как нам свести задачу краевую к решению f задачи каши?
[47:48.580 --> 47:50.580]  Ну, а как решается задача каши, мы знаем.
[47:50.580 --> 47:52.580]  Например, медленный ангел кота.
[47:52.580 --> 47:56.580]  Это уже, так сказать, проблем не вызывает решение такой задачи.
[47:56.580 --> 48:00.580]  Однако у нас чего не хватает, чтобы мы задачу каши решили.
[48:00.580 --> 48:05.580]  У нас вторая производная, то есть три неизвестных, и только одно начальное данное.
[48:05.580 --> 48:11.580]  Поэтому нам нужно добавить еще какое-то данное, но мы его не знаем.
[48:11.580 --> 48:16.580]  То есть, если мы вот такую графику нарисуем, здесь будет у нас x,
[48:16.980 --> 48:21.980]  здесь у нас скажем 0, здесь у нас 0, где-то здесь и правая граница.
[48:21.980 --> 48:27.980]  Пусть у нас здесь вот начальное условие на левой границе, а здесь направое.
[48:27.980 --> 48:29.980]  И да малое, и да большое.
[48:29.980 --> 48:35.980]  Положим, что вот такая кривая, это наше точное решение, которое мы не знаем, которое нам нужно найти.
[48:37.980 --> 48:42.980]  Нам хотелось бы начать эту задачу, решение задачи, задачи каши.
[48:43.380 --> 48:48.380]  Оттолкнуться от точки до 0 и попасть в какую-то другую точку.
[48:48.380 --> 48:51.380]  И потом таких кривых нарисовать много.
[48:51.380 --> 48:55.380]  А потом из них уже получить наше решение.
[48:55.380 --> 48:57.380]  У нас это не получается.
[48:57.380 --> 49:01.380]  Поэтому мы вводим такую функцию, как α1, например,
[49:01.380 --> 49:06.380]  которая равняется ау1-у0 и 9 а.
[49:06.380 --> 49:12.380]  То есть, это аппроксимация первой производной с первым порядком точности.
[49:12.780 --> 49:15.780]  Но ау1 мы не знаем, поэтому а1 мы тоже не знаем.
[49:15.780 --> 49:20.780]  Мы просто берем какое-то значение а1.
[49:20.780 --> 49:26.780]  Но, разумеется, как правило, когда вы решаете задачу, то в ней есть какой-то физический смысл.
[49:26.780 --> 49:32.780]  Поэтому а1 с какой-то степенью точности всегда можно определить ее пределы.
[49:32.780 --> 49:39.780]  Вот мы берем, если а1, то мы можем уже эту задачу начать решать.
[49:40.180 --> 49:44.180]  То есть, у нас есть у0, есть у1.
[49:44.180 --> 49:47.180]  Можно решать задачу каши.
[49:47.180 --> 49:52.180]  Ну, положим, мы ее решили и попали вот сюда.
[49:52.180 --> 49:57.180]  У, давайте так обозначаем, у эльфа, у эльфа вот а1.
[49:57.180 --> 50:00.180]  Вот а1 в эту точку опали.
[50:00.180 --> 50:03.180]  Это задача каши.
[50:03.180 --> 50:04.180]  Куда опадаем?
[50:04.180 --> 50:07.180]  Далее берем какое-то другое значение а2.
[50:07.580 --> 50:13.580]  Из того диапазода значение а, которое мы оценили, априорно до решения задачи.
[50:13.580 --> 50:19.580]  Всегда перед решением задачи делаются априорные оценки и решения, и параметров, и так далее,
[50:19.580 --> 50:23.580]  чтобы не попасть в трудную ситуацию.
[50:23.580 --> 50:24.580]  До а2.
[50:24.580 --> 50:27.580]  Вот положим, мы с помощью а2 попали сюда.
[50:27.580 --> 50:30.580]  Это будет у эль, вот а2.
[50:30.580 --> 50:34.580]  Ну, дальше и так далее до некого ак.
[50:34.980 --> 50:40.980]  Вот у эль, вот ак.
[50:40.980 --> 50:43.980]  И так мы достреляли, например, ну и так далее.
[50:43.980 --> 50:46.980]  Последний давайте сделаем, ак.
[50:46.980 --> 50:47.980]  Ак.
[50:47.980 --> 50:50.980]  Вот мы достреляли, ка.
[50:50.980 --> 50:55.980]  Задачу каши до решали и получили ка.
[51:04.980 --> 51:07.980]  Нет, это...
[51:07.980 --> 51:15.980]  Ну, положим, вы, смотрите, оценили, что все альфа-каты у вас лежат в диапазоне от 0 до 1, да?
[51:15.980 --> 51:23.980]  Ну, вы берете а1, например, там 0,1, а2, 0,2, ну и так далее.
[51:23.980 --> 51:26.980]  А там, скажем, там 9, да?
[51:26.980 --> 51:28.980]  Ну, 0,9 и так далее.
[51:28.980 --> 51:33.980]  То есть вы выбираете вот альфа с каким-то шагом в вашем диапазоне.
[51:34.380 --> 51:38.380]  Диапазон, ну, как правило, в реальных задачах вы можете оценивать.
[51:38.380 --> 51:42.380]  То есть он, как правило, от минус до плюс без кодичности не лежит в диапазоне.
[51:42.380 --> 51:47.380]  Как правило, он все-таки ограниченный, вы можете его хоть грубо, но оценить.
[51:47.380 --> 51:52.380]  Поэтому а1, а2, ак, это не зависит друг от друга.
[51:52.380 --> 51:55.380]  Это просто числа, которые вот в вашем диапазоне лежат.
[51:55.380 --> 51:58.380]  Дальше мы что делаем?
[51:58.780 --> 52:00.780]  Что мы получили, на самом деле?
[52:00.780 --> 52:03.780]  Так, сейчас пундочку, где у нас?
[52:03.780 --> 52:06.780]  А вот три, да?
[52:06.780 --> 52:11.780]  Так, что мы получили?
[52:11.780 --> 52:14.780]  Что мы делаем дальше?
[52:14.780 --> 52:17.780]  Из этих n-кривых.
[52:21.780 --> 52:23.780]  Из этих n-кривых.
[52:23.780 --> 52:26.780]  Вот что мы получили.
[52:27.180 --> 52:31.180]  Мы получили функцию ul от αк.
[52:31.180 --> 52:37.180]  И нам бы хотелось, чтобы, если мы из нее вычитаем правое условие d,
[52:37.180 --> 52:41.180]  чтобы оно было с какой-то точностью равно 0.
[52:41.180 --> 52:45.180]  Ну, разумеется, с какой-то точностью.
[52:45.180 --> 52:48.180]  С какой-то задной точностью, не обязательно с машиной.
[52:48.180 --> 52:51.180]  Точность вы задаете сами.
[52:51.180 --> 52:54.180]  Но я здесь, конечно, очень смело написал знак ранчества.
[52:54.580 --> 52:56.580]  Правильнее написать знак приближенного ранницы,
[52:56.580 --> 52:59.580]  поскольку всегда будет эта ранница выполняться приближенно.
[52:59.580 --> 53:01.580]  Нам бы этого хотелось.
[53:01.580 --> 53:04.580]  Ну, либо можно написать даже так.
[53:04.580 --> 53:11.580]  Функция от α, это есть ul от α минус d.
[53:11.580 --> 53:14.580]  Нам бы хотелось, чтобы она была равна 0.
[53:14.580 --> 53:17.580]  А что я написал, на самом деле, вот здесь?
[53:17.580 --> 53:22.580]  Я просто написал нелинейное уравнение.
[53:22.980 --> 53:27.980]  Нелинейное уравнение вы решать умеете.
[53:27.980 --> 53:36.980]  Например, какие методы есть для секущей?
[53:36.980 --> 53:40.980]  Есть такой метод еще?
[53:40.980 --> 53:43.980]  Ну, очень знаменитый метод.
[53:43.980 --> 53:46.980]  Как?
[53:46.980 --> 53:49.980]  Newton, конечно.
[53:49.980 --> 53:50.980]  Поразительный метод.
[53:51.380 --> 53:54.380]  Это один из самых поразительных методов,
[53:54.380 --> 53:56.380]  наверное, во всей математике.
[53:56.380 --> 53:58.380]  Может, не только в математике.
[53:58.380 --> 54:00.380]  Как, например, Newton есть в механике,
[54:00.380 --> 54:01.380]  поразительная вещь.
[54:01.380 --> 54:04.380]  Когда Newton его предложил в конце XVII века,
[54:04.380 --> 54:07.380]  примерно тогда же он предложил свой метод решения
[54:07.380 --> 54:08.380]  нелинейных уравнений.
[54:08.380 --> 54:10.380]  И он до сих пор не просто живой,
[54:10.380 --> 54:12.380]  он активно живой, этот метод.
[54:12.380 --> 54:14.380]  Сколько ему лет, там уже юбилей нужно праздновать,
[54:14.380 --> 54:16.380]  ставь, много столетний,
[54:16.380 --> 54:18.380]  он до сих пор живой.
[54:18.380 --> 54:20.380]  Его совершенствуют, его ускоряют.
[54:20.380 --> 54:22.380]  Но он остается методом Ньютона.
[54:22.380 --> 54:24.380]  Как и второй закон Ньютона.
[54:24.380 --> 54:26.380]  Поразительная вещь совершенно.
[54:26.380 --> 54:28.380]  Но если вы забыли, я просто напомню,
[54:28.380 --> 54:30.380]  как он выглядит.
[54:30.380 --> 54:32.380]  В данном случае, да.
[54:36.380 --> 54:39.380]  Сейчас будет тряпка более мокрая.
[54:39.380 --> 54:54.380]  Альфа К плюс один.
[54:54.380 --> 54:57.380]  Это есть альфа К минус
[54:57.380 --> 55:01.380]  f от альфа К делено
[55:01.380 --> 55:05.380]  f' по альфа К.
[55:05.380 --> 55:07.380]  Это метод Ньютона.
[55:07.380 --> 55:09.380]  Разумеется, вот вы сказали правильно,
[55:09.380 --> 55:11.380]  метасекущий как раз означает,
[55:11.380 --> 55:13.380]  что производная у нас
[55:13.380 --> 55:15.380]  не анонтически вычисляется численно.
[55:15.380 --> 55:17.380]  Поскольку альфа К у нас задана
[55:17.380 --> 55:19.380]  по точке этой точки.
[55:19.380 --> 55:21.380]  Поэтому здесь нужно вычислять ее численно.
[55:21.380 --> 55:23.380]  Производную.
[55:23.380 --> 55:25.380]  Например, вот по такому простому
[55:25.380 --> 55:27.380]  соотношению.
[55:27.380 --> 55:29.380]  Это вот что касается метод Ньютона.
[55:29.380 --> 55:31.380]  Но можно подойти к этой же даче
[55:31.380 --> 55:33.380]  и немного по-другому.
[55:33.380 --> 55:35.380]  Из другого, как говорится,
[55:35.380 --> 55:37.380]  до конца.
[55:37.380 --> 55:41.380]  Что мы имеем в краевой задаче?
[55:41.380 --> 55:43.380]  Мы имеем, например,
[55:43.380 --> 55:45.380]  ряд прецедентов.
[55:45.380 --> 55:47.380]  У ноль,
[55:47.380 --> 55:49.380]  это левое граничное условие,
[55:49.380 --> 55:55.380]  и у ль вот альфа К.
[55:55.380 --> 55:57.380]  Это то, что мы настреляли
[55:57.380 --> 55:59.380]  на правой границе получили.
[55:59.380 --> 56:01.380]  С К меняется от единицы там
[56:01.380 --> 56:03.380]  до К большого.
[56:03.380 --> 56:05.380]  Одна вот такая пара
[56:05.380 --> 56:07.380]  называется прецедентом.
[56:07.380 --> 56:09.380]  Вот совокупность пар
[56:09.380 --> 56:11.380]  таких, как называется,
[56:11.380 --> 56:13.380]  не помните, я как-то вам говорил?
[56:13.380 --> 56:15.380]  В терминологии машинного обучения
[56:15.380 --> 56:17.380]  это называется
[56:17.380 --> 56:19.380]  обучающая выборка.
[56:19.380 --> 56:21.380]  Совокупность вот таких прецедентов.
[56:21.380 --> 56:23.380]  Сама машинное обучение,
[56:23.380 --> 56:25.380]  если говорить о более
[56:25.380 --> 56:27.380]  правильных названиях,
[56:27.380 --> 56:29.380]  это есть обучение
[56:29.380 --> 56:31.380]  по прецедентам.
[56:31.380 --> 56:33.620]  то есть у нас есть много прецедентов
[56:33.620 --> 56:37.440]  и на основании их мы находим решение, разумеется, с некой
[56:37.440 --> 56:38.980]  степенью точности
[56:38.980 --> 56:43.460]  ну а если мы имеем обучающую выборку, то нам нужно составить целевую функцию
[56:43.460 --> 56:46.460]  целевая функция, естественно, просится
[56:46.460 --> 56:48.440]  мы с вами об этом говорили
[56:48.440 --> 56:50.140]  в виде f квадрат альфа
[56:50.140 --> 56:52.260]  почему f квадрат?
[56:52.260 --> 56:54.500]  потому что минимум этой функции 0
[56:54.500 --> 56:55.820]  и мы хотим
[56:55.820 --> 56:59.460]  найти альфа, при котором эта функция
[56:59.500 --> 57:01.700]  минимальна
[57:01.700 --> 57:04.700]  то есть 0
[57:04.700 --> 57:08.340]  для этого мы можем использовать
[57:08.340 --> 57:12.100]  ну любой есть метод оптимизации, о которых мы с вами говорили
[57:12.100 --> 57:14.340]  альфа-к в данном случае это есть
[57:14.340 --> 57:16.260]  аргумент
[57:16.260 --> 57:17.860]  минимума
[57:17.860 --> 57:21.340]  функции f квадрат по альфу
[57:21.340 --> 57:23.380]  ну если вы помните эти названия
[57:23.380 --> 57:27.060]  метод наискорейшего спуска, метод градетного спуска
[57:27.060 --> 57:28.460]  и так далее
[57:28.540 --> 57:31.340]  мы с вами говорили о методах оптимизации
[57:31.340 --> 57:32.780]  аппарат
[57:32.780 --> 57:35.600]  машинного обучения, это есть
[57:35.600 --> 57:39.180]  два аппарата, о которых мы с вами проходили. Первый аппарат это теория
[57:39.180 --> 57:40.380]  приближения функций
[57:40.380 --> 57:43.880]  мы с вами говорили, второй- это метод оптимизации, о которых мы
[57:43.880 --> 57:47.140]  с вами тоже говорили. То есть, я свел нашу задачу краевую
[57:47.140 --> 57:49.380]  к задаче машинного обучения
[57:50.180 --> 57:52.960]  на самом деле мы с ним периодически будем встречаться
[57:52.960 --> 57:55.260]  всё новое хорошо забыто с той стороны
[57:55.260 --> 57:57.180]  интерполяция и экстраполяция
[57:57.180 --> 58:03.180]  мет на имеющих квадратах это все было переименовано в методы машинного обучения
[58:03.180 --> 58:08.980]  на самом деле и даже краевые задачи тоже можно представить как метод машинного обучения
[58:08.980 --> 58:17.440]  фактически это метод простой итерации
[58:17.440 --> 58:21.040]  который можно представить как метод машинного обучения
[58:21.040 --> 58:29.640]  но есть конечно и более продвинутые по скорости методы мы обращаемся опять же к великому дилтону
[58:29.640 --> 58:38.400]  вот здесь у нас будет тоже уравнение второго порядка нелинейного
[58:38.400 --> 58:42.440]  в котором мы начинали и когда я рассказал о методе стрельбы
[58:42.440 --> 58:54.120]  лямбда хх ун равняется f от ун
[58:54.120 --> 59:01.080]  давайте распишем производную ун минус единица минус 2 ун плюс
[59:01.080 --> 59:08.420]  этот оператор это уже не производный это уже оператор разосный ун плюс 1 делим наш квадрат
[59:08.420 --> 59:20.260]  и равняется f под ун ну разумеется начальные данные нам известны но смотрите здесь
[59:20.260 --> 59:25.340]  три неизвестных да то есть нам очень хотелось бы очень свободительно использовать метод прогулки
[59:25.340 --> 59:31.460]  но действительно три неизвестных да то есть трех диагональный матрица все здорово что нам
[59:32.460 --> 59:40.120]  правая часть она нелинейная поэтому не можем применить метод прогулки но несколько секунд до
[59:40.120 --> 59:46.820]  размышления что можно придумать как все-таки можно использовать алгоритм прогулки для решения этой задачи
[59:52.820 --> 59:57.460]  итерационные методы молодец конечно конечно методы последовательных
[59:57.460 --> 01:00:04.100]  приближения то есть нам нужно задать некую начальную функцию которая назовем начальное
[01:00:04.100 --> 01:00:11.620]  приближение и от нее отталкиваться я рисовал эту картинку да я не буду ее повторять вот скажем
[01:00:11.620 --> 01:00:18.820]  наше точное решение и положим мы из каких-то соображений физических оценили начальное приближение
[01:00:18.820 --> 01:00:26.980]  этой функции ну в данном случае конечно напрашивается прямая самая простая начальное приближение вот
[01:00:26.980 --> 01:00:33.980]  то есть мы уже переходим чему и в медле стрельбы и в этом методе о котором сейчас будем говорить это
[01:00:33.980 --> 01:00:42.180]  же мы находим в функциональных пространствах функцию функцию ну разумеется в точную виде то есть
[01:00:42.180 --> 01:00:47.620]  мы уже работаем функциональных пространствах ну поэтому вообще говоря вот один из главных
[01:00:47.620 --> 01:00:54.020]  математических аппаратов в учительнике математики это функциональный анализ вот ну я конечно упрощает
[01:00:54.020 --> 01:01:01.700]  его определение чтобы было проще ну положим вот взяли некую начальную функцию давайте обозначим
[01:01:01.700 --> 01:01:14.260]  ее фи от о ну скажем вот ну от у 0 вот пусть она будет такая и мы хотим найти начальное приближение
[01:01:14.260 --> 01:01:20.700]  как мы это будем делать вот давайте это разнос со соотношения напишем так вверху будем вставить
[01:01:20.700 --> 01:01:31.340]  значок один у н минус один один где один это будет следующее приближение так далее
[01:01:31.340 --> 01:01:45.220]  у н плюс один один на аш квадрат а здесь будет у нас что ф от у н 0 а у н 0 это наша функция фи от
[01:01:45.220 --> 01:01:52.020]  другой против прощения я написал x и это это функция фи от икс азумеется
[01:01:52.020 --> 01:02:03.060]  фи от икс так это наше приближение первое пожалуйста здесь мы можем использовать прогулку для
[01:02:03.060 --> 01:02:10.660]  нахождения первого приближения у первого все нам здесь все известно поскольку первое приближение
[01:02:10.660 --> 01:02:19.660]  это функ? и дальше мы тоже самое делаем зная первое приближение здесь мы уже ставим
[01:02:19.660 --> 01:02:31.260]  давайте я вам перператором видно пишу у 2 второе приближение находим здесь будет f от у 1 ну и так далее
[01:02:31.260 --> 01:02:39.340]  и в конечном итоге у нас получается такой-то рационный процесс лямбда x икс у и плюс один
[01:02:39.340 --> 01:02:51.820]  н равняется f от у и который заканчивается например за одну точность например у и плюс один минус у и по
[01:02:51.820 --> 01:02:58.020]  норме меньше н равняется некой заданной точностью т.е. мы решаем эту задачу
[01:02:58.020 --> 01:03:12.900]  а что чтобы стал линейным мы всего лишь задали начальное приближение некую функцию например
[01:03:12.900 --> 01:03:19.460]  линейную я сказал например можно в любую функцию узнаете известную в данном случае
[01:03:20.300 --> 01:03:27.700]  она напрашивает само собой и тогда вы решаете значит альгаритм прогонки доходит и первое
[01:03:27.700 --> 01:03:34.100]  приближение ну например оно будет каким-то таким таким-то таким
[01:03:34.100 --> 01:03:49.420]  в первом приближении мы пишем уже вот эти самые краевые точки которые нам заданы в
[01:03:49.420 --> 01:03:57.580]  краевых условиях все мы уже не задачу каше решаем мы решаем задачу на основе алгоритма прогонки
[01:03:58.140 --> 01:04:03.900]  welder
[01:04:03.900 --> 01:04:09.420]  вот мы написали эту функцию нам известно вот эта функция у 0 от x это наша fluorite
[01:04:09.420 --> 01:04:16.120] agon function ciyat x то есть правая часть нам известно это приближLink это
[01:04:16.120 --> 01:04:21.740]  начальное приближение оно неправильное это не точное решение это начальное приближение
[01:04:21.740 --> 01:04:27.140]  но мы нам известно мы его задача а здесь у нас три известных это что у нас получается
[01:04:27.140 --> 01:04:35.140]  Это система уравнений линейных с трехдевинальной матрицей, то есть та же прогулка.
[01:04:35.140 --> 01:04:47.140]  А это правая часть прогулки. То есть алгоритм прогулки, который мы сейчас подробно разбирали,
[01:04:47.140 --> 01:04:52.140]  мы просто здесь используем для нахождения первого приближения.
[01:04:52.140 --> 01:05:02.140]  Да, это та функция, которую вы задали в качестве начального приближения нашего исковного решения.
[01:05:07.140 --> 01:05:12.140]  Потом уже у1 вы поставляете правую часть, находите у2, это второе приближение,
[01:05:12.140 --> 01:05:19.140]  и дальше вы получаете такой трорационный процесс.
[01:05:19.140 --> 01:05:25.140]  То есть вы делаете n прогонок. Н прогонок пока не выполнится в условиях вашей точности.
[01:05:25.140 --> 01:05:36.140]  Это будет метод простой итерации в функциональных пространствах.
[01:05:36.140 --> 01:05:45.140]  Вот так сейчас. Но его, конечно, нужно ускорить. Его можно ускорить.
[01:06:06.140 --> 01:06:13.140]  Вот здесь я уже напишу этот итерационный процесс в явную форму.
[01:06:13.140 --> 01:06:20.140]  Вот здесь поставим у1, а здесь у2. То, что вы как раз писали.
[01:06:20.140 --> 01:06:28.140]  Ну и задается обязательно начальное данное, начальное приближение.
[01:06:28.140 --> 01:06:31.140]  Вот это и есть итерационный процесс в функциональном пространстве.
[01:06:31.140 --> 01:06:37.140]  То есть мы итерационным образом и в медном последовательном приближении ищем уже функцию.
[01:06:37.140 --> 01:06:41.140]  Посмотрите еще раз, это понятно или нет?
[01:06:41.140 --> 01:06:47.140]  Вообще процесс аналогичен процессу поиска корня нелинейного равнения, но не совсем.
[01:06:47.140 --> 01:06:51.140]  Здесь мы находим n точек. Не одну точку, а n точек.
[01:06:51.140 --> 01:06:55.140]  Ну или там ка большой точек я обозначил.
[01:06:55.140 --> 01:06:57.140]  Мы находим функцию по точкам.
[01:06:57.140 --> 01:07:02.140]  Дальше мы ее можем делать непрерывно, например, с помощью оператора интерполирования.
[01:07:02.140 --> 01:07:09.140]  То есть любая точная функция делается непрерывно с помощью оператора интерполирования.
[01:07:09.140 --> 01:07:12.140]  Которую вы выбираете тоже с необходимой вам точностью.
[01:07:12.140 --> 01:07:15.140]  1, 2, 3 порядка и так далее.
[01:07:15.140 --> 01:07:17.140]  Лучше всего с планом интерполирования.
[01:07:17.140 --> 01:07:19.140]  Оно самое надежное.
[01:07:20.140 --> 01:07:22.140]  Хорошо.
[01:07:22.140 --> 01:07:24.140]  Теперь смотрите.
[01:07:24.140 --> 01:07:29.140]  На самом деле есть некоторые мелкие недостатки.
[01:07:29.140 --> 01:07:33.140]  Помните условия, которые нужно хорошо помнить?
[01:07:33.140 --> 01:07:41.140]  Выбора шара интегрирования для функции средней или правой части.
[01:07:41.140 --> 01:07:44.140]  То есть мы будем ограничены.
[01:07:44.140 --> 01:07:52.140]  Если, например, у нас будет норма этой мальцы с якоби большая, то шар по времени тоже будет ограничен.
[01:07:52.140 --> 01:07:55.140]  Как с этим справиться?
[01:07:55.140 --> 01:07:59.140]  В общем, на самом деле так же, как мы справлялись и ранее.
[01:07:59.140 --> 01:08:02.140]  Смотрите, что мы можем сделать на самом деле.
[01:08:02.140 --> 01:08:04.140]  Здесь я поставил значок i.
[01:08:04.140 --> 01:08:09.140]  Вот если бы я поставил значок i плюс 1, у нас была бы явная схема.
[01:08:09.140 --> 01:08:13.140]  И проблемы с выбором шара h не было.
[01:08:13.140 --> 01:08:15.140]  Но что у нас получилось?
[01:08:15.140 --> 01:08:18.140]  Я здесь написал ничто иное, как функцию нелинейного равнения.
[01:08:18.140 --> 01:08:23.140]  То есть нам придется решать итерационным образом нелинейного равнения на каждом шане.
[01:08:23.140 --> 01:08:29.140]  Это усложнит вычисление, то есть увеличит количество алимптических действий.
[01:08:29.140 --> 01:08:35.140]  Как можно сделать так, чтобы количество алимптических действий все-таки уменьшить, а не увеличить?
[01:08:35.140 --> 01:08:37.140]  Три секунды для размышления.
[01:08:37.140 --> 01:08:39.140]  Какие идеи будут?
[01:08:40.140 --> 01:08:42.140]  Погромче.
[01:08:44.140 --> 01:08:52.140]  Вопрос, как вот это нелинейное уравнение решить так, чтобы не увеличивать количество алимптических действий, а уменьшить?
[01:08:54.140 --> 01:08:56.140]  Еще раз?
[01:08:57.140 --> 01:09:00.140]  Хорошая идея, но пока подождем.
[01:09:00.140 --> 01:09:03.140]  Пусть будет пока сетка неравномерная.
[01:09:04.140 --> 01:09:07.140]  Помните, как называется метод Ньютона?
[01:09:07.140 --> 01:09:09.140]  По-другому.
[01:09:09.140 --> 01:09:13.140]  Касательных или метод линейно-оризации?
[01:09:13.140 --> 01:09:22.140]  Собственно говоря, Ньютон всего лишь линейно-оризовал функцию и получил совершенно гениальный метод по скорости сходимости.
[01:09:22.140 --> 01:09:24.140]  Что мы можем делать?
[01:09:24.140 --> 01:09:27.140]  Мы же эту функцию тоже можем линейно-оризовать.
[01:09:28.140 --> 01:09:39.140]  Как f от u и плюс 1 мы представим, давайте оставим не только линейный член,
[01:09:39.140 --> 01:09:53.140]  f от u и t плюс f' по u от u и t на u и плюс 1 минус u и.
[01:09:53.140 --> 01:09:56.140]  Вот я линейно-оризовал эту функцию.
[01:09:56.140 --> 01:10:02.140]  И смотрите, если я эту линейно-оризацию функцию ставлю в правую часть, у меня i плюс 1.
[01:10:02.140 --> 01:10:04.140]  Где остается? Вот она остается.
[01:10:04.140 --> 01:10:09.140]  Но это уже слагаемо, мы можем перенести в левую часть.
[01:10:09.140 --> 01:10:13.140]  Оно нам уже не испортит прогонкой, это линейное слагаемое.
[01:10:13.140 --> 01:10:15.140]  И мы спокойно делаем прогонку.
[01:10:16.140 --> 01:10:18.140]  То есть найти первую террацию.
[01:10:18.140 --> 01:10:23.140]  Но уже в правой части у нас будет стоять линейно-оризованная функция.
[01:10:23.140 --> 01:10:29.140]  Этот метод называется метод Ньютона или метод квази-линиоризации в функциональных пространствах.
[01:10:31.140 --> 01:10:36.140]  Идея Ньютона используется, но тот-то метод Ньютон точно сделал.
[01:10:36.140 --> 01:10:41.140]  Предложили он этот метод, я не берусь утверждать, вполне возможно.
[01:10:41.140 --> 01:10:48.140]  Но идея Ньютона, идея линиоризации не в линейной правой части, она оказалась очень плавотворной.
[01:10:48.140 --> 01:10:52.140]  Она используется до сих пор очень активно при решении очень новых задач.
[01:10:53.140 --> 01:10:57.140]  А в ненейности в подавляющей большинстве задач присутствует.
[01:10:57.140 --> 01:11:01.140]  Так что вот Витя Ньютон, он не только физик, он...
[01:11:01.140 --> 01:11:04.140]  ...эф-штрих?
[01:11:04.140 --> 01:11:06.140]  Эф-штрих?
[01:11:06.140 --> 01:11:09.140]  Ну, эф-штрих тоже нам известна на этой террации.
[01:11:09.140 --> 01:11:11.140]  Мы считаем, что она нам известна.
[01:11:11.140 --> 01:11:17.140]  Она может быть известна не аналитически, а по точкам ее может приблизить.
[01:11:17.140 --> 01:11:20.140]  Но она нам известна, эф-штрих.
[01:11:20.140 --> 01:11:24.140]  То есть она не будет там какая-нибудь волшебная функция?
[01:11:24.140 --> 01:11:26.140]  Ну, она нам известна.
[01:11:26.140 --> 01:11:28.140]  Поэтому мы вот решаем.
[01:11:28.140 --> 01:11:30.140]  Вот этот...
[01:11:30.140 --> 01:11:36.140]  Почему эф-штрих по У будет проще, чем...
[01:11:36.140 --> 01:11:43.140]  Почему эф-штрих по У будет проще, чем решение вот этого ненейного уравнения?
[01:11:43.140 --> 01:11:45.140]  Этот вопрос?
[01:11:45.140 --> 01:11:47.140]  Ну, вопрос правильный.
[01:11:47.140 --> 01:11:50.140]  Но я хотел бы его ополирать вам.
[01:11:50.140 --> 01:11:55.140]  Возьмите какую-нибудь простую задачу с кадровым уравнением и решите.
[01:11:55.140 --> 01:12:00.140]  Вот методом простой итерации левое уравнение и методом Ньютона его же.
[01:12:00.140 --> 01:12:03.140]  И сколько итераций будет в первом случае и во втором?
[01:12:03.140 --> 01:12:09.140]  Вы увидите, что в случае метода Ньютона, ну, если функция не какая-нибудь очень хитрая, экзотическая,
[01:12:09.140 --> 01:12:14.140]  то вы увидите, что метод Ньютона, конечно, очень экономичный метод.
[01:12:14.140 --> 01:12:18.140]  Но он тоже, как правило, вот забирает буквально несколько итераций.
[01:12:18.140 --> 01:12:22.140]  В несколько итераций вы находите решение вашей нелинейной задачи.
[01:12:22.140 --> 01:12:26.140]  Вот, то есть, этот метод линеризации.
[01:12:26.140 --> 01:12:28.140]  Так.
[01:12:28.140 --> 01:12:32.140]  Теперь сколько у нас осталось времени?
[01:12:32.140 --> 01:12:35.140]  Время осталось немного.
[01:12:35.140 --> 01:12:38.140]  Так, ну и вот тогда остался время.
[01:12:38.140 --> 01:12:41.140]  Ещё один метод вам интересный расскажу.
[01:12:41.140 --> 01:12:45.140]  Его вообще-то относится к методам приближённым.
[01:12:45.140 --> 01:12:48.140]  Нечисленно приближённым.
[01:12:48.140 --> 01:12:51.140]  Но это не очень важно в термологии.
[01:12:51.140 --> 01:12:53.140]  Метод в хуне.
[01:12:53.140 --> 01:13:01.140]  У нас есть вот аппроксимация нашего уравнения второго порядка.
[01:13:01.140 --> 01:13:07.140]  Ну, давайте для простоты UL возим в равном UL.
[01:13:07.140 --> 01:13:09.140]  Понял?
[01:13:09.140 --> 01:13:12.140]  И вот какая интересная штука получается.
[01:13:12.140 --> 01:13:17.140]  Оказывается, вот этот оператор λх имеет собственные значения, собственные функции.
[01:13:17.140 --> 01:13:22.140]  Я сейчас не буду делать примерно то, что вы получаете.
[01:13:22.140 --> 01:13:24.140]  Что это означает?
[01:13:24.140 --> 01:13:28.140]  Это означает, что λх можно на некоторый диаметр ω.
[01:13:28.140 --> 01:13:30.140]  Это есть λ.
[01:13:30.140 --> 01:13:32.140]  Тоже на этот диаметр ω.
[01:13:32.140 --> 01:13:35.140]  Прекрасное вам известное соотношение с первого курса.
[01:13:35.140 --> 01:13:39.140]  Оказывается, имеет этот оператор собственные числа.
[01:13:39.140 --> 01:13:41.140]  И они известны.
[01:13:41.140 --> 01:13:43.140]  Ну, как это можно поверить?
[01:13:43.140 --> 01:13:45.140]  Прямо поставим непосредственную постановку в это уравнение.
[01:13:45.140 --> 01:13:55.140]  4 на h квадрат на sin квадрат π к h на 2L.
[01:13:55.140 --> 01:14:00.140]  Это собственные значения и собственные функции.
[01:14:00.140 --> 01:14:08.140]  Кроме квадрата k от xl, это будет у нас в нормированном виде 2d9 квадрат плюс 2dL.
[01:14:08.140 --> 01:14:18.140]  А здесь sin pknh на L.
[01:14:18.140 --> 01:14:20.140]  Ну вот.
[01:14:20.140 --> 01:14:25.140]  Ну а что мы делаем, если у нас есть базис из собственных векторов?
[01:14:25.140 --> 01:14:28.140]  Зачем нам нужен вообще базис из собственных векторов?
[01:14:28.140 --> 01:14:30.140]  Это мы говорили в конце прошлого семеста.
[01:14:30.140 --> 01:14:32.140]  Для того, чтобы приближать функции.
[01:14:32.140 --> 01:14:38.140]  И получать приближенные решения в виде разложений по базе собственных функций.
[01:14:38.140 --> 01:14:42.140]  Вот. Мы это и можем сделать.
[01:14:42.140 --> 01:14:44.140]  И что у нас получится?
[01:14:44.140 --> 01:14:46.140]  Получится вот что у нас.
[01:14:46.140 --> 01:15:01.140]  Решение мы представляем у КТ в виде разложений по базису из собственных функций.
[01:15:01.140 --> 01:15:05.140]  Коэффициенты СКТ нам пока не известны.
[01:15:05.140 --> 01:15:08.140]  Их мы называем коэффициентами Fourier.
[01:15:08.140 --> 01:15:11.140]  Ну к меняется от 1 до n-1.
[01:15:11.140 --> 01:15:14.140]  То же самое мы сделаем с правильной частью.
[01:15:14.140 --> 01:15:18.140]  fKT мы тоже разложаем в таковый вариант.
[01:15:18.140 --> 01:15:21.140]  Ну коэффициенты Fourier давайте так ободначим по-другому.
[01:15:21.140 --> 01:15:23.140]  f с крышкой.
[01:15:23.140 --> 01:15:25.140]  Чтобы fKT.
[01:15:25.140 --> 01:15:33.140]  Ну а это ωKT это наши собственные векторы от xn.
[01:15:33.140 --> 01:15:36.140]  Так если это мы так делаем.
[01:15:36.140 --> 01:15:42.140]  Подставляем эти уn и правую часть вот в это уравнение.
[01:15:42.140 --> 01:15:46.140]  Подставляем. Что получаем?
[01:15:46.140 --> 01:15:50.140]  Так. Подставляем. И что мы получаем?
[01:15:56.140 --> 01:15:58.140]  Получаем следующее.
[01:15:58.140 --> 01:16:00.140]  λх.
[01:16:00.140 --> 01:16:02.140]  Здесь у нас что будет?
[01:16:02.140 --> 01:16:04.140]  Сумма СКТ на ω.
[01:16:04.140 --> 01:16:08.140]  Я уже немного сокращу и допишу.
[01:16:09.140 --> 01:16:13.140]  Что?
[01:16:13.140 --> 01:16:16.140]  А, собственные функции?
[01:16:16.140 --> 01:16:21.140]  Пока давайте так. Вместе они берутся откуда-то.
[01:16:21.140 --> 01:16:26.140]  Просто если я уйду в Волгебург, мы с вами будем заниматься немного другой наукой.
[01:16:26.140 --> 01:16:32.140]  Значит смотрите, я написал решение задачи собственные значения и собственные функции.
[01:16:32.140 --> 01:16:34.140]  Вы можете их поставить в это уравнение и проверить.
[01:16:34.140 --> 01:16:36.140]  Они будут сопроводять.
[01:16:36.140 --> 01:16:40.140]  Пока будем считать так, что я их угадал и можно проверить.
[01:16:40.140 --> 01:16:42.140]  Разумеется, они находятся.
[01:16:42.140 --> 01:16:45.140]  Это просто бы выдел в другой область немного.
[01:16:45.140 --> 01:16:49.140]  Поэтому пока поверьте и можете проверить.
[01:16:49.140 --> 01:16:53.140]  И справа у нас будут что?
[01:16:53.140 --> 01:16:57.140]  Правая часть FКТ на ω.
[01:16:57.140 --> 01:17:03.140]  Теперь давайте этот знак оператора я внесу под знак SUM.
[01:17:03.140 --> 01:17:07.140]  Под знак SUM.
[01:17:07.140 --> 01:17:11.140]  Вообще-то этот прием в математике существует.
[01:17:11.140 --> 01:17:15.140]  Когда угадывается решение, потом ставится в уравнение оператора
[01:17:15.140 --> 01:17:18.140]  и тем самым доказывается его справедливость.
[01:17:18.140 --> 01:17:21.140]  Хотя это не часто применяется.
[01:17:21.140 --> 01:17:24.140]  Под знак SUM что получится?
[01:17:24.140 --> 01:17:31.140]  Получится СКТ здесь λхх на ω КТ.
[01:17:31.140 --> 01:17:37.140]  А здесь SUM FКТ на ω КТ.
[01:17:37.140 --> 01:17:41.140]  Что такое λхх на ω КТ?
[01:17:41.140 --> 01:17:44.140]  Говорите мне.
[01:17:44.140 --> 01:17:46.140]  Я уже пишу.
[01:17:46.140 --> 01:17:49.140]  Это есть λКТ на ω КТ.
[01:17:49.140 --> 01:17:53.140]  Просто значение на собственные функции.
[01:17:53.140 --> 01:18:02.140]  И равняется FКТ на ω КТ.
[01:18:02.140 --> 01:18:06.140]  Отсюда мы находим коэффициент фурье.
[01:18:06.140 --> 01:18:12.140]  СКТ это будет FКТ на λКТ.
[01:18:12.140 --> 01:18:22.140]  Коэффициент фурье мы решаем задачу.
[01:18:22.140 --> 01:18:27.140]  Но разумеется, я не слышал вопроса, а сколько же чайных ряда нужно брать.
[01:18:27.140 --> 01:18:30.140]  Это вопрос важный.
[01:18:30.140 --> 01:18:33.140]  Зависит от вашей точности.
[01:18:33.140 --> 01:18:38.140]  Может оказаться так, что количество чайных ряда нужно брать большое или даже очень большое.
[01:18:38.140 --> 01:18:47.140]  Но этот метод, его обычно приближенным, нечисленным, очень популярен во многих областях.
[01:18:47.140 --> 01:18:50.140]  Особенно физических.
[01:18:50.140 --> 01:18:52.140]  На сегодня давайте закончим.
[01:18:52.140 --> 01:18:54.140]  До свидания.

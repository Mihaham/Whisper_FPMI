[00:00.000 --> 00:16.880]  так ну что давайте начнем времени у нас не так чтобы много как уже предыдущий ратор говорил я
[00:16.880 --> 00:22.640]  закончу эту это занятие эту лекцию и проведу еще одну поэтому здесь у нас осталось чуть
[00:22.640 --> 00:28.560]  больше половинки давайте сосредоточимся и пожалуй я думаю что сейчас вы уже немножко
[00:28.560 --> 00:36.440]  отдохнули пока мы здесь настраивали все поэтому давайте без перерыва хотелось бы там что-то
[00:36.440 --> 00:45.400]  успеть и так вот олег уже сказал что первое два занятия и несколько семинаров у вас будет
[00:45.400 --> 00:55.600]  посвящены параллельным вычислением да называется параллельные вычисления то есть там будет
[00:55.600 --> 01:03.680]  mpi и куда еще но в целом хотелось бы начать вообще с того откуда это все идет почему нужно бы
[01:03.680 --> 01:12.360]  рассматривать как mpi куда там еще были такие слова как open mp возможно вы будете что-то успевать по
[01:12.760 --> 01:19.200]  но я в этом не очень уверен по-моему успевает ну значит отлично значит успеете но здесь за
[01:19.200 --> 01:25.520]  скажем так чуть больше чем полторы лекции трудно осветить весь такой объем материала который
[01:25.520 --> 01:32.720]  посвящен какой mpi так и open mp поэтому сначала мы вообще коснемся того почему это все изучается
[01:32.720 --> 01:39.160]  почему изучается mpi open mp в дальнейшем куда ну и так далее почему в том числе в дальнейшем вы
[01:39.160 --> 01:45.040]  будете проходить курс о распределенных вычислениях работать с большими данными то есть откуда
[01:45.040 --> 01:52.640]  вообще все это идет и почему это нужно изучать вот поэтому давайте начнем значит вообще нужно
[01:52.640 --> 01:59.000]  определиться с тем что такое суперкомпьютер ну есть если так для людей не посвященных это
[01:59.000 --> 02:05.400]  можно показаться в общем-то банальным таким вопросом но суперкомпьютер это там где производит
[02:05.400 --> 02:11.280]  супер вычисление но нас это не очень устраивает на самом деле поэтому здесь давайте как-то
[02:11.280 --> 02:19.640]  формули формализовать хоть как-то о том что это такое например вот попытались это сделать в 86
[02:19.640 --> 02:26.160]  году например воксфордском словаре было такое определение что суперкомпьютер это очень мощная
[02:26.160 --> 02:32.680]  мвм который обладает производительностью в 10 миллионов операций в секунду то есть плавающей
[02:32.680 --> 02:41.120]  точкой получается 10 мегафлопс но проходит какое-то время и уже в начале 90-х это нужно
[02:41.120 --> 02:47.240]  было зачеркивать и переписывать словарь значит что будет сказано что у нас в начале 90-х уже
[02:47.240 --> 02:52.600]  суперкомпьютер является таевым которая обладает производительностью производительностью 300
[02:52.600 --> 03:00.160]  мегафлопс проходит еще несколько лет и производительность доходит до некой такой
[03:00.160 --> 03:07.080]  магической цифры цифры в 5 гигафлопс и так далее но в общем-то возникает вопрос если каждый раз
[03:07.080 --> 03:13.440]  приходится переписывать определение то может быть самим определением что-то не так потом
[03:13.440 --> 03:19.000]  наверное здесь стоит отойти от этого потому что у нас есть прогресс вычислительной техники и
[03:19.000 --> 03:25.520]  вообще говоря производительность так или иначе но постоянно растет производительсти в м растет но
[03:25.520 --> 03:32.200]  это может быть и нельзя оставить как 100 процент на определение но в голове держать нужно
[03:32.200 --> 03:41.080]  естественно дальше что еще можно взять за определение суперкомпьютера но вообще говоря
[03:41.080 --> 03:47.760]  суперкомпьютер это такие мм которые обладает большой мощностью значит они обладают всеми
[03:47.760 --> 03:54.760]  например большим количеством современные суперкомпьютеры большим количеством процессоров
[03:54.760 --> 04:02.560]  здесь разные системы пожаротушение система охлаждения подвода энергия то есть электро энергия
[04:02.560 --> 04:07.360]  поэтому вообще говоря это системы довольно-таки дорогие вот поэтому здесь тоже можно некоторым
[04:07.360 --> 04:14.160]  смысле операция и на стоимость в какой-то мере это действительно дорогие электроны вычислительной
[04:14.160 --> 04:23.640]  машины но опять же это нельзя ставить во во главе угла потому что что может быть например часто
[04:23.640 --> 04:29.400]  или ну не часто в сети можно найти что у состоятельных у людей там есть мышки из золота или там
[04:29.400 --> 04:35.920]  полированные кости древних животных инкрустировано бирлиантами и так далее это будет является дорогом
[04:35.920 --> 04:43.440]  дорогим аж дорогой машины дорогим компьютером но будет ли являться суперкомпьютером ну ответ
[04:43.440 --> 04:54.080]  очевиден или майнер может прийти или так значит да в какой-то мере естественно дорогие системы
[04:54.080 --> 05:02.560]  но это опять же нельзя ставить как определение в конце 80 шутку было сказано горненом белом
[05:02.560 --> 05:10.720]  и доном нельсеном и том что суперкомпьютер это тяжелая машина большая действительно это так и
[05:10.720 --> 05:17.480]  всегда суперкомпьютеры занимали там одну или две комнаты действительно это тяжелые машины и
[05:17.480 --> 05:22.120]  сейчас естественно большие суперкомпьютер занимает тоже не одну комнату на самом деле со всеми своими
[05:22.120 --> 05:30.200]  системами но это конечно было сказано в шутку тем не менее тоже логично и в общем-то правильно так
[05:30.200 --> 05:36.800]  что же на самом деле или что можно подразумевать под суперкомпьютером но в целом можно сказать что
[05:36.800 --> 05:45.080]  или можно считать что суперкомпьютер это такая в м которая по своей мощности вычислительной
[05:45.080 --> 05:53.120]  мощности на несколько порядков сильнее мощнее чем современно доступный персональный в м опять же
[05:53.120 --> 06:00.920]  это не точное определение но хоть на что-то где-то можно ориентироваться итак это все что пока вот
[06:00.920 --> 06:07.400]  говорится о том что такое да супер компьютер и о чем идет речь во что идет игра теперь
[06:07.400 --> 06:15.720]  почему давайте коснемся того почему нужно изучать параллельно программируем не в том или ином виде
[06:15.720 --> 06:23.720]  на самом деле можно сказать что вообще на в течение развития вычислительной техники и
[06:23.720 --> 06:30.200]  программного обеспечения можно отметить скажем так три кризиса программного обеспечения и
[06:30.200 --> 06:38.880]  связанного с развитием вычислительной техники значит сейчас считается что есть ну как не то
[06:38.880 --> 06:44.680]  что читается все языки программируем там делится условно на две большие ветки это языки низкого
[06:44.680 --> 06:52.600]  уровня и высокого уровня высокого уровня это когда тогда когда мы ну то есть когда при написании
[06:52.600 --> 06:59.560]  программы мы не учитываем какие-то архитектурные тонкости или вообще архитектуру компьютера вот
[06:59.560 --> 07:08.800]  низкого уровня это машины коды либо языки очень приближенные к машинам кадам это ассемблеры
[07:08.800 --> 07:14.120]  всевозможные и в общем-то на заре вычислительной техники именно низкое программирование в общем-то
[07:14.120 --> 07:22.720]  было единственно возможным и на это на основе низких но вернее языков низкого уровня можно
[07:22.720 --> 07:31.600]  было писать программы и конечно же они писались но потом прошел путь там от ламп до транзисторов и
[07:31.600 --> 07:37.960]  в итоге вычислительная техника стало более компактной более мощной и это все произошло вот
[07:37.960 --> 07:46.680]  на рубеже там 50 и 60 годов и продолжалось 60 и оказалось что компьютеры позволяли писать уже
[07:46.680 --> 07:53.760]  большие программы на ассемблерных языках это можно было делать но это было в общем-то довольно
[07:53.760 --> 08:03.080]  проблематично писать большие программы но это было просто сложно плюс к тому же здесь есть вопрос
[08:03.080 --> 08:08.800]  о переносимости программ с одной системы на другой и вот без существенной переработки кода вообще
[08:08.800 --> 08:15.640]  говоря это было невозможно поэтому естественным был переход к языкам высокого уровня появились
[08:15.640 --> 08:24.680]  языки алгол фортран си вы все должны знать по крайней мере слышали на чем-то даже и писали то есть
[08:24.680 --> 08:34.920]  это модульное программирование который позволял писать больше все то есть программы которые
[08:34.920 --> 08:41.240]  являются большими их можно было писать легче дальше прошел переход к большим интегральным схемам
[08:41.240 --> 08:50.040]  появились процессоры вот опять же на самом деле получается как обычно тем людям которые
[08:50.040 --> 08:54.720]  разрабатывают числительной системы в некотором смысле проще потому что программистам и тем кто
[08:54.720 --> 08:59.840]  связан с софтвэ приходится их все время догонять но был переход на большие интегральные схемы и
[08:59.840 --> 09:06.760]  что оказалось оказалось что да опять же на модульных языках скажем так можно писать большие
[09:06.760 --> 09:12.600]  программы но здесь если говорить о каких-то больших проектов которые задействует большое число людей
[09:12.600 --> 09:19.640]  сотни людей и большое количество строк кода там сотни тысяч миллион то опять же эти языки не
[09:19.640 --> 09:25.080]  совсем подходят для этих вещей но в общем-то естественно опять же по требованию времени скажем
[09:25.080 --> 09:32.400]  так выросла и объектно ориентирование программирования соответствующими языками все опять стало
[09:32.400 --> 09:39.800]  нормальным программное обеспечение в некотором смысле догнала развитие железа и позволила
[09:39.800 --> 09:46.440]  использовать себе возможности но это все было хорошо до каких пор примерно до рубежов до рубежа
[09:46.440 --> 09:52.920]  веков наших почему потому что здесь вычислительная техника столкнулась с определенными проблемами
[09:52.920 --> 10:00.080]  проблемами вдаль в плане дальнейшего своего развития повышение производительности в чем
[10:00.080 --> 10:07.720]  же здесь дело до этого все развитие практически по крайней мере вот если представлять себе
[10:07.720 --> 10:14.480]  персоналки то как было как происходило развитие все больше количества транзисторов умещалась на
[10:14.480 --> 10:22.120]  одном кристалле что позволяла увеличить частоты и таким образом вы используя одну и ту же программу
[10:22.120 --> 10:28.640]  подождав какое-то количество времени вы просто могли пользоваться той же программой с увеличенным
[10:28.640 --> 10:35.480]  быстродействием почему то есть она считала быстрее потому что новый компьютер считал быстрее но
[10:35.480 --> 10:43.680]  уменьшение транзисторов и увеличение плотности их нахождения на кристалле соответственно порождает
[10:43.680 --> 10:51.240]  проблему проблему тепловыделения и уже наконец 90-х годов это на тепловыделение составляла там
[10:51.240 --> 10:57.080]  больше чем 10 ватт на квадратный сантиметр если бы такими темпами пришло увеличение
[10:57.080 --> 11:04.800]  тепловыделения то там в компании intel посчитали что уже ближе к концу нулевых годов тепловыделение
[11:04.800 --> 11:12.760]  на кристалле было бы как в ядерном реакторе естественно что это невозможно правильно уже
[11:12.760 --> 11:19.760]  тогда стали устанавливаться естественно охлаждающие системы кулеры и нужно в итоге
[11:19.760 --> 11:28.720]  упёрлись и в частоту около 3 гигагерц и компоновка уже не позволяла увеличить не количество
[11:28.720 --> 11:36.040]  процессора не количество транзисторов не частоту но здесь опять же стоит что сказать что вот развитие
[11:36.040 --> 11:42.920]  процессор фоно шло двумя путями даже до двухтысячных годов первое это как раз
[11:42.920 --> 11:48.800]  увеличение количества транзисторов это один путь но этого было мало потому что если сравнить
[11:48.800 --> 11:56.880]  например частоты тактовые частоты компьютеров на заре то есть там 50 60 лет назад и тех которые
[11:56.880 --> 12:04.360]  были около нулевых годов то их отличие в частотах было несколько тысяч раз ну скажем так 5000 5
[12:04.360 --> 12:13.160]  мегагерца и там 2 с половиной гигагерца вот 5000 получается но производительность машин выросла
[12:13.160 --> 12:20.160]  не в 5000 раз она выросла значительно больше и дело здесь не только в частотах тактовых частотах но
[12:20.160 --> 12:26.440]  и во внутреннем устройстве процессор они стали сложнее у них так или иначе внутри был уже за
[12:26.440 --> 12:33.880]  шит невидимый скажем так для обыкновенных людей какая-то степень парализма была параллельная
[12:33.880 --> 12:41.800]  выборка данных из регистров либо конвертного устройства но так или иначе был еще второй путь
[12:41.800 --> 12:47.800]  повышения производительности не только увеличение тактовой частоты но и внутренний парализм какой-то
[12:47.800 --> 12:56.000]  внутри самих ядер но здесь все упёрлась физические принципы в определенные нужно было что-то
[12:56.000 --> 13:04.040]  делать и в общем-то взяли опять решение которое уже было сделано но раньше использование много
[13:04.040 --> 13:10.640]  ядерные машины взять несколько одинаковых процессоров это еще самый первый компьютер на
[13:10.640 --> 13:16.480]  самом деле были уже многоядерные скажем так по современным слов современными словами просто
[13:16.480 --> 13:22.840]  взяли несколько одинаковых процессоров соединили между собой вот получили компьютер который обладает
[13:22.840 --> 13:32.680]  более высокой мощностью так появились многоядерные процессоры это произошло где-то в 2005 году то есть
[13:32.680 --> 13:41.280]  ну я имею в виду бытовые бытовые процессоры и что оказалось что если вы используете старую свою
[13:41.280 --> 13:46.800]  последовательную программу и запускайте ее на новом процессоре которым например с начала была два
[13:46.800 --> 13:52.520]  ядра у нее производительность точно такая же как и на одноядерном процессоре ну потому что
[13:52.520 --> 13:58.480]  частот у них одинаково и внутренней архитектуры очень похоже к примеру проходит еще какое-то
[13:58.480 --> 14:07.560]  время еще 5 лет 4 8 ядер вы запускайте вашу программу и ее время работы не сильно уменьшается по той
[14:07.560 --> 14:15.720]  же самой причине нужно значит каким-то образом воспользоваться тем что у вас есть многоядер
[14:15.720 --> 14:22.000]  здесь возникает вопрос параллельного программирования как написать такую программу
[14:22.000 --> 14:27.640]  которая будет пользовать использовать все возможности все потенциальные вычислительные
[14:27.640 --> 14:33.400]  возможности вашей машины ну и здесь стоит отметить что все современные в общем-то машины
[14:33.400 --> 14:40.880]  состоят из типичных ну более менее типичных процессоров которые ставится на обыкновенные
[14:40.880 --> 14:46.360]  компьютеры или на ноутбуке то есть принципы принцип принципы распараллеливание как для
[14:46.360 --> 14:51.920]  вашего ноутбука так и например для суперкомпьютера они будут одни и те же на самом деле но опять же
[14:51.920 --> 15:00.560]  здесь стоит еще что отметить что все современные компьютеры ноутбуки даже телефоны они многоядерные
[15:00.560 --> 15:06.440]  то есть многоядерность она вообще вокруг нас и вообще говоря не знать как использовать эту
[15:06.440 --> 15:16.000]  многоядерность ну было бы плохо однако здесь какая проблема то возникает что автоматического
[15:16.000 --> 15:22.880]  распараллельно то не существует почему по многим причинам во первых компьютер не может до конца
[15:22.880 --> 15:30.000]  анализировать алгоритм второе не всякий хороший последовательный алгоритм можно хорошо распараллелить
[15:30.000 --> 15:37.120]  и наоборот бывает так что хорошо распараллельный алгоритм он как бы вырастает из не самого
[15:37.120 --> 15:41.800]  лучшего последовательного алгоритма поэтому здесь не все так очевидно и в некотором смысле здесь
[15:41.800 --> 15:48.720]  нет какой-то автоматизации процесса здесь этому нужно научиться почувствовать это и вот с этим и
[15:48.720 --> 15:58.240]  связано наше обучение итак какие есть вопросы примерно потому о чем я сказал есть ли они
[15:58.240 --> 16:07.120]  теперь давайте вот коснемся тогда для чего нужны супер компьютер мы разобрали разобрали
[16:07.120 --> 16:13.480]  с тем что это такое ну или примерно и теперь для чего они существует значит они существует
[16:13.480 --> 16:18.840]  для чего для уменьшения времени каких-то расчетов то есть решение задачи временной
[16:18.840 --> 16:28.200]  сложности обычно это очень часто какие-то научные расчеты которые мне требует определенного скажем
[16:28.200 --> 16:33.760]  так времени окончания расчета но хотелось бы чтобы эти расчеты длились поменьше но таким
[16:33.760 --> 16:39.520]  типичным опять же примером который у всех на слуху это прогноз погода вы все этим пользуетесь но
[16:39.520 --> 16:45.440]  что за этим стоит на самом деле за этим стоит использование каких-то математических моделей они
[16:45.440 --> 16:52.400]  бывают разные их много но их запускает на супер компьютерах на самом деле это просто программа
[16:52.400 --> 16:58.680]  которые работают на супер компьютере входных данных у них данные со всего мира или там с
[16:58.680 --> 17:04.400]  какой-то области земного шара данные температуру давление направление ветра там влажности и так
[17:04.400 --> 17:10.920]  далее входные данные есть есть модель она запускается на супер компьютере для чего потому
[17:10.920 --> 17:17.960]  что если взять типичную сетку для там атмосферы там километра на километр то есть кубический
[17:17.960 --> 17:24.640]  километр и высотой там 30 25 30 километров то вообще чтобы покрыть какую-то область даже
[17:24.640 --> 17:31.280]  всего шарика это будет например один миллиард ячеек таких если запустить это расчет на персоналке
[17:31.280 --> 17:38.800]  то вообще говоря можно ожидать того что вы получите результат вашего расчета в момент когда вы
[17:38.800 --> 17:45.360]  можете просто открыть окно и посмотреть эту погоду то есть это в общем-то не будет являться
[17:45.360 --> 17:51.280]  прогнозом погода проще будет действительно убедиться этого очью вот поэтому нужно использовать
[17:51.280 --> 17:55.680]  естественно супер компьютеры у них есть разные модели и вот разные модели они показывают разную
[17:55.680 --> 18:02.360]  достоверность своего расчета или например там всевозможные расчеты начинает проблема
[18:02.360 --> 18:09.000]  турбулентности до расчета движения динамики звезд в галактиках если например в одной галактике
[18:09.000 --> 18:17.240]  11 миллиардов звезд например или там 100 миллиардов звезд и вот посчитать один временной
[18:17.240 --> 18:23.320]  шаг или один шаг интегрирования для всего эту скопища звезд может потребоваться тоже один
[18:23.320 --> 18:29.960]  год или порядка этого а нужно чтобы целом оценить динамику развития этой галактики нужны опять же
[18:29.960 --> 18:39.080]  миллиарды шагов ну в общем-то время трудно просто представить даже так или иначе нужно
[18:39.080 --> 18:46.520]  уменьшить время расчета в общем дни для всех задач это есть не для всех задач сейчас подходят
[18:46.520 --> 18:53.200]  мощности но всегда для того или иного суперкомпьютера найдется задача которая ему будет по зубам и
[18:53.200 --> 19:01.960]  которая будет успешно нем решатся вторая задача то как раз и вот да первая часть вашего курса она
[19:01.960 --> 19:06.160]  вот будет именно с этим связано написание какой-то программы которая будет обладать высокой
[19:06.160 --> 19:14.040]  эффективности высоким ускорением но не требует какого-то ну скажем так времени выполнения
[19:14.040 --> 19:20.360]  определенного оно будет меньше чем при последовательном алгоритме вторая задача связанного
[19:20.360 --> 19:25.640]  с обработкой большим объемом данных это может быть визуализация какая-то либо это просто
[19:25.640 --> 19:33.720]  действительно обработка информации которая находится в интернет поисковики здесь определяющим
[19:33.720 --> 19:39.960]  является то что человек является в общем-то медленным устройством и нужно за момент реакции
[19:39.960 --> 19:45.600]  человека выдать какой-то результат то есть здесь есть уже определенная временные рамки ограничения
[19:45.600 --> 19:52.360]  то есть некая интерактивность либо поиск либо визуализация то есть это одна примерно 1-2 секунды
[19:52.360 --> 19:58.320]  или доли секунд но это вот с этим будет связано как раз вторая часть курса ну не только с этим но
[19:58.320 --> 20:05.360]  примерно в этой плоскости лежит еще есть задачи управления там промышленными установками то есть
[20:05.360 --> 20:10.600]  то что нужно компьютер на многопроцессорный компьютер управляет различными роботами на каком-то
[20:10.600 --> 20:20.680]  предприятии производит определенные действия или в том числе включая станки хоть это с цифровым
[20:20.680 --> 20:27.760]  управлением но это некоторым там в стороне но это тоже сюда можно как-то привлечь и еще и задачи
[20:27.760 --> 20:37.320]  это системы высокой надежности например если это речь идет о космическом парате там что высокая
[20:37.320 --> 20:46.920]  радиация и недопустимость каких-то ошибок в вычислениях поэтому если это необходимо произвести
[20:46.920 --> 20:52.280]  какие-то вычисления там пересчет новой орбиты то есть какие какой импульс нужно дать сейчас это
[20:52.280 --> 20:58.400]  часто выполняется именно борту они на земле ну и в общем-то нужно что сделать запустить эту
[20:58.400 --> 21:04.640]  программу на нескольких одновременно нескольких процессорах получить разные или одинаковые
[21:04.640 --> 21:12.240]  результаты но их должно быть несколько кстати больше чем сколько если несколько несколько
[21:12.240 --> 21:32.880]  ответов 1 или 2 хватит но минимум хорошо минимум 3 да если 2 ну да а если 3 разных
[21:32.880 --> 21:39.720]  тоже плохо ну ладно их больше двух должно быть естественно чтобы было понятно очень
[21:39.720 --> 21:45.320]  маловероятно то что у вас на трех машинах будет все одинаково абсолютно разные результаты
[21:45.320 --> 21:55.920]  абсолютно разные да вот поправлю значит это последняя часть курса и да вот в первом первой
[21:55.920 --> 22:03.800]  части вот если вот здесь в первой части здесь у нас говорится о том что наша компьютерная
[22:03.800 --> 22:13.000]  система должна быть надежной и бесперебойной то во второй части курса здесь уже вносится то что
[22:13.000 --> 22:21.760]  она может отказать может быть возможность отказа и это тоже уже лежит внутри скажем так фреймворк в
[22:21.760 --> 22:26.800]  который вы будете изучать то здесь предполагается что система может отказать в первой части обычно
[22:26.800 --> 22:32.320]  это не предусматривается считается что она абсолютно надежно в второй части есть частичная
[22:32.320 --> 22:40.800]  автоматизация на пополам решена проблема параллельность автоматического создания параллельности
[22:40.800 --> 22:48.360]  это она есть темно немножко и в упон и пи но не полностью вы можете там рассматривать ну например
[22:48.440 --> 22:53.120]  в упон и пи есть распараллельную циклов можно сказать что это распараллеливание в принципе
[22:53.120 --> 22:58.640]  автоматическое но в целом как бы алгоритм да частично где-то можно а в целом это нужен
[22:58.640 --> 23:07.160]  человек пока еще так да по поводу теперь давайте перейдем знаете к чему к рассмотрению
[23:07.160 --> 23:13.200]  классификации систем почему потому что это тесно связано в том числе с библиотеками программного
[23:13.200 --> 23:20.240]  программирования mpi упон и пи там не типосекс и других которые вы услышите то что они связаны
[23:20.240 --> 23:27.160]  именно с тем а что это за супер компьютеры и почему именно эти библиотеки нужно опять же изучать значит
[23:27.160 --> 23:33.400]  систем классификации их тоже много остановился на той которая в принципе описывает все машины она в
[23:33.400 --> 23:38.480]  общем-то показала свою состоятельность это классификация флина но она вот основана на таких
[23:38.480 --> 23:46.600]  определениях по-английски это будет single multiple instructions in data то есть одни одно много
[23:46.600 --> 23:53.320]  инструкции и данные и вот из этих вот если взять первые буковки то из них можно составить вот
[23:53.320 --> 24:00.840]  такие комбинации ну так получается что оказывается что вот с асд получается single instruction single
[24:00.840 --> 24:06.640]  data это вот обыкновенные последовательные машины у которых есть но инструкция значит прос
[24:06.640 --> 24:13.120]  программа дейта это значит какие-то там переменные которые с которыми работает программа и оказывается
[24:13.120 --> 24:20.040]  что у вас это просто последовательный код который работает с одним потоком данных условно дальше
[24:20.040 --> 24:29.520]  с амд то есть это одни данные и множественные ой множественные данные и вот одна инструкция это
[24:29.520 --> 24:36.560]  значит такие векторные вычисления грубо говоря представьте себе два столбца вектора да там у
[24:36.560 --> 24:45.120]  них там может быть 100 координат и вот операции сложения множество данных это их координаты
[24:45.120 --> 24:51.600]  этих векторов и на выходе тоже получать сразу 100 ответов то есть такие машины они в основном ну
[24:51.600 --> 24:57.040]  на самом деле современных процессорах такое есть внутри и вот векторные конвертные машины не
[24:57.040 --> 25:05.440]  были очень популярны 70 80-е годы как в сша так и советском союзе вот поэтому они существуют а
[25:05.440 --> 25:13.680]  вот машин которые работают над одним набором данных ну в общем-то говоря они широко не
[25:13.680 --> 25:20.960]  представлены они там в отдельных скажем так реализация есть но они вот как я сказал широко
[25:20.960 --> 25:25.840]  не представленных пока вот можно не рассматривать на самом деле современные машины они сидят вот
[25:25.840 --> 25:31.920]  в этом классе множественная инструкция которая действует над множественными данными ммд это вот
[25:31.920 --> 25:36.960]  все остальные высокопроизводительные современные машины то есть все что сейчас есть вот в этот
[25:36.960 --> 25:46.800]  класс попадает но этот класс делится еще на две ветки значит это машины с массовым параллелизмом
[25:46.800 --> 26:03.800]  смотрите это вот машины с распределенной памятью называется почему я пишу здесь на английский
[26:03.800 --> 26:08.760]  потому что часто и по-русски говорят без три билет машинцы или вы будете это встречать в
[26:08.760 --> 26:13.600]  литературе очень часто это попадается русский и английский аналог по-русски я скажу английский
[26:13.600 --> 26:20.200]  вот на доске это часто встречается потому что это все на самом деле пошло все эти определения не
[26:20.200 --> 26:26.160]  у нас эти были сделаны поэтому нужно еще источник по-хорошему смотреть значит что это такое это вот
[26:26.160 --> 26:35.120]  кластеры что такое кластеры это просто супер компьютеры которые построены на скажем так бытовой
[26:35.120 --> 26:41.280]  элементной базе на обыкновенных бытовых процессорах они появились вот на рубеже 80 90
[26:41.280 --> 26:46.680]  годов как раз до этого были специализированы супер компьютеры а после этого начали появляться
[26:46.680 --> 26:52.120]  и вытеснять специализированного кластеры это значит что у нас грубо говоря обыкновенные
[26:52.120 --> 26:59.000]  однопроцессорные компьютеры соединены какой-то сетью они могут быть специализированы немножко
[26:59.000 --> 27:03.600]  отличаться конечно бытовых сама сеть может быть быстрой так или иначе можно в голове себе именно
[27:03.600 --> 27:08.880]  так и представлять его для работы с такими системами я сейчас сразу скобочек скажу и
[27:08.880 --> 27:15.840]  предназначена библиотека mpi когда вот у того или иного компьютера нет прямого доступа к
[27:15.840 --> 27:23.640]  оперативной памяти своего соседа или какого-то еще компьютера и второй широкий класс это машины
[27:23.640 --> 27:31.440]  с общей памятью shared memory machines вот они и называется еще symmetric multiprocessing или uniform
[27:31.440 --> 27:38.040]  memory access то есть это однородный доступ к памяти или симметричный это одно и то же связано с
[27:38.040 --> 27:43.840]  тем что у вас процессоры они подсоединены к одной оперативной памяти и по времени доступа
[27:43.840 --> 27:51.680]  к этой памяти у них одинаковые возможности есть машины с неоднородным доступом к памяти там
[27:51.680 --> 27:58.600]  схемка чуть-чуть попозже будет я поясню это когда у нас дело в том что когда много процессоров
[27:58.600 --> 28:04.240]  подсоединяется к одной оперативной памяти у нас есть шина данных условно да вот это является
[28:04.240 --> 28:10.160]  бутылочным горлышком для этой системы нельзя сейчас технические возможности не позволяют там
[28:10.160 --> 28:15.800]  скажем так 10 тысяч процессоров подключить к одному блоку оперативной памяти это не позволяет
[28:15.800 --> 28:25.000]  технические возможности поэтому ну не 10 тысяч этого совсем загнул на самом деле там например 128 вот
[28:25.000 --> 28:30.760]  а чтобы подключить еще 128 нужно их через какой-то коммутатор подключать и вот когда у вас идет
[28:30.760 --> 28:36.280]  а коммутатор позволяет делать то что у вас то или иной процессор либо к своему блоку оперативной
[28:36.280 --> 28:43.360]  памяти либо к памяти который идет сигнал через коммутатор ну и через коммутатор время доступа
[28:43.360 --> 28:48.760]  будут побольше поэтому называется non-uniform то есть неоднородный доступ к памяти и вообще под
[28:48.760 --> 28:58.960]  памятью здесь подразумевается именно оперативная память и есть еще системы CCNUMA то есть cash
[28:58.960 --> 29:07.480]  coherent с этим вы тоже столкнетесь когда будете программировать на OpenMP там есть такая проблема
[29:07.480 --> 29:16.560]  синхронизации потоков там есть проблема кэш памяти когерентности кэш памяти называется такая
[29:16.560 --> 29:23.840]  проблема она может быть решена программно может быть решена на железном уровне вот это решается
[29:23.840 --> 29:29.120]  на железном уровне это работает быстрее но значительно дороже чем системы не обладающие
[29:29.120 --> 29:40.320]  такой возможностью вот теперь вот по схематике с массивным парализмом как я уже сказал каждый
[29:40.320 --> 29:47.240]  этот вот кластер кластер состоит из узлов современный кластер состоит из такого узла гибридного
[29:47.240 --> 29:54.080]  то есть он может включать себе несколько одновременных то есть несколько процессоров которые
[29:54.080 --> 29:58.640]  подключены к одной оперативной памяти то есть узел может представлять себе из себя машину с
[29:58.640 --> 30:06.960]  общей памятью и в которой также могут находиться видеокарты но представим себе пока что без такого
[30:06.960 --> 30:13.720]  усложнений каждый узел представляет себе просто одно процессор на одноядерную машину одна машина
[30:13.720 --> 30:18.760]  вторая третья они себя представляют просто обыкновенный компьютер однопроцессор и
[30:18.760 --> 30:24.840]  соединены интерконнектом то есть интеркоммуникационной сетью эта сеть должна быть довольно-таки
[30:24.840 --> 30:32.760]  быстрой потому что как вы увидите и на лекции успею надеюсь показать и это вы можете ну по
[30:32.760 --> 30:37.160]  крайней мере должны проследить на семинарах что сеть влияет на самом деле на ускорение работы
[30:37.640 --> 30:47.360]  вашей программы то есть это кластер и а теперь вот каждый узел может состоять из машины с
[30:47.360 --> 30:53.200]  общей памяти вот из такой когда у нас несколько ядер подсоединены к одной оперативной памяти а
[30:53.200 --> 30:59.840]  вот это у нас неоднородный доступ к памяти например здесь ядро или несколько ядер у них
[30:59.840 --> 31:05.320]  есть прямой доступ к своей оперативной памяти но также у них есть доступ через коммутатор к
[31:05.320 --> 31:12.120]  другому блоку оперативной памяти но вот это время дохода сигнала или передачи данных от
[31:12.120 --> 31:17.320]  памяти до ядра через коммутатор но естественно это может быть несколько раз больше чем напрямую
[31:17.320 --> 31:23.760]  получается у нас неоднородный доступ но так или иначе это можно подключать большее количество ядер
[31:23.760 --> 31:34.320]  какие-то вопросы я может быть немножко тороплюсь чтобы успеть но давайте если что-то непонятно
[31:34.640 --> 31:43.680]  давайте задавайте вопросы по этому материалу я пойду дальше что нас еще хотят бы кое-что успеть
[31:43.680 --> 32:00.680]  ну можно добавить да то есть на самом деле если современные как я уже сказал суперкомпьютер то
[32:00.680 --> 32:06.640]  есть вот такая штука вместе с видеокарточкой да у нее через дополнительные скажем так микросхемы
[32:06.640 --> 32:12.560]  как сказала каде через южный мост идет общение потому что у нас там должна быть перекачка данных
[32:12.560 --> 32:19.160]  видеокарту это вы тоже с этим столкнетесь это тоже является неким бутылочным горлышком узким
[32:19.160 --> 32:27.800]  местом в этой системе и вместо однопроцессорной машины сам в простейшем случае у нас будет
[32:27.800 --> 32:32.680]  многоядерная машина с общей памятью еще с видеокартами то есть это современный суперкомпьютер
[32:43.680 --> 32:50.520]  чего с нума а это просто машина с общей памяти просто нужно смотреть на ее документацию это что
[32:50.520 --> 32:54.920]  системы скорее всего это будет система с неоднородным доступом с памяти современно
[32:54.920 --> 33:01.280]  системы обычно именно такие которых 256 там ядер однородной трудно обеспечить но это
[33:01.280 --> 33:07.520]  просто вот если мало вот например у вас компьютер это будет однородный доступ там 4 8 ядер скорее
[33:07.520 --> 33:14.760]  всего так и есть а вот там побольше 128 256 скорее всего там будет неоднородный доступ но конечно
[33:14.760 --> 33:27.240]  это нужно смотреть на документацию как сделано то есть как
[33:27.240 --> 33:35.600]  если есть сомнения мы открываем документацию там все написано что там у них такой-то или
[33:35.600 --> 33:43.200]  такой-то по скажем так по внешнему виду не скажешь но то есть компьютер и компьютер это
[33:43.200 --> 33:49.640]  нужно смотреть устройство реальное устройство и как его как он был сделан скажем так на заводе
[33:49.640 --> 34:01.200]  или на фабрике есть еще вопросы так вот про внутренний прорелизм теперь смотрите сам смысл
[34:01.200 --> 34:07.760]  параллельных вычислений в чем состоит вот у вас есть параллельно машина у нее есть допустим теперь
[34:07.760 --> 34:15.360]  давайте мы будем говорить об mpi то есть будем считать что у нас есть кластер и каждый узер
[34:15.360 --> 34:21.320]  кластера это у нас однопроцессорная машина для простоты теперь у нас программа как она
[34:21.320 --> 34:27.240]  должна использовать всю мощь этого кластера например у него 10 ядер значит каким-то образом
[34:27.240 --> 34:33.400]  нашу программу нужно написать так чтобы отдельные части считались на каждом из 10 процессоров
[34:33.400 --> 34:41.840]  тогда то что считается а эти все части должны быть в каком смысле независимы друг от друга чтобы
[34:41.840 --> 34:49.880]  их можно было запустить одновременно вот такое свойство алгоритма когда отдельные его части
[34:49.880 --> 34:55.920]  можно запустить для вычислений на разных процессорах называется внутренним прорелизмом это не связано
[34:55.920 --> 35:03.920]  не самой не самим суперкомпьютером не с библиотекой параллельных вычислений это связано с самим
[35:03.920 --> 35:10.200]  алгоритмом он обладает таким свойством что его можно распараллелить вот такой свойство называется
[35:10.200 --> 35:17.480]  внутренним параллелизмом и вот давайте теперь об этом дальше поговорим на чем основано или скажем
[35:17.480 --> 35:26.320]  так что нужно достигать что нужно достичь чтобы понять что мы делаем все хорошо так
[35:26.320 --> 35:33.040]  косноязычное оно примерно по смыслу похоже нам нужно достигнуть выигрыша во времени сделать
[35:33.040 --> 35:39.800]  так чтобы программа или алгоритм считался быстрее вот это вот и характеристика говорит о том что
[35:39.800 --> 35:45.880]  наш наш программа считает быстрее это называется ускорение ускорение это просто отношение времен
[35:45.880 --> 35:56.840]  времени работы этой программы на одном процессоре или на одном ядре я буду иногда говорить либо
[35:56.840 --> 36:02.500]  и д Wahl процессор делённый на время работы на нескольких ядрах но очевидно что если у нас все
[36:02.500 --> 36:07.480]  хорошо то время работы на нескольких ядрах должно быть меньше чем время работы на одном
[36:07.480 --> 36:14.500]  ядре правильно в идеальном случае если у нас есть 10 ядер значит время в идеальном случае время
[36:14.500 --> 36:19.100]  работы этого алгоритма, он будет в 10 раз меньше.
[36:19.100 --> 36:20.580]  Ускорение будет десятки равно.
[36:20.580 --> 36:23.640]  И вот идеальный случай показан здесь, в штрихпунктной
[36:23.640 --> 36:24.640]  линии.
[36:24.640 --> 36:28.460]  На графике ускорения от количества процессов у
[36:28.460 --> 36:31.460]  нас будет безсектристо при одинаковом масштабе.
[36:31.460 --> 36:36.540]  Ну, в общем-то это понятно.
[36:36.540 --> 36:40.220]  Однако в реальности не всегда все так гладко.
[36:40.220 --> 36:42.940]  В реальности у нас в какой-то момент график этого ускорения
[36:42.940 --> 36:45.740]  будет отклоняться от этой безсектрисы.
[36:45.740 --> 36:48.540]  И на это есть несколько причин.
[36:48.540 --> 36:54.140]  О причинах мы тоже поговорим чуть позже.
[36:54.140 --> 36:57.140]  Эта причина, ну сейчас если так коротко, то причина
[36:57.140 --> 37:00.180]  связана с самим алгоритмом, что мы не можем полностью
[37:00.180 --> 37:02.100]  распараллелить наш код.
[37:02.100 --> 37:05.780]  Вторая причина лежит в области как раз интеркоммуникационной
[37:05.780 --> 37:06.780]  сети.
[37:06.780 --> 37:10.420]  Потому что она обладает определенными свойствами,
[37:10.420 --> 37:11.900]  задержками временными.
[37:11.900 --> 37:14.020]  И она накладывает как раз некие ограничения на
[37:14.020 --> 37:17.020]  рост ускорения, на рост на нашем графике ускорения.
[37:17.020 --> 37:21.060]  Вот это вот самый первый параметр, который вы будете
[37:21.060 --> 37:22.460]  смотреть на семинаре.
[37:22.460 --> 37:26.860]  Потому что нужно достигать две цели при написании
[37:26.860 --> 37:28.020]  параллельной программы.
[37:28.020 --> 37:30.180]  Первая цель это правильный ответ.
[37:30.180 --> 37:32.740]  Ну что такое правильный ответ?
[37:32.740 --> 37:35.500]  Либо он совпадает с аналитикой, либо он совпадает с ответом
[37:35.500 --> 37:37.660]  полученным на последовательном алгоритме.
[37:37.660 --> 37:44.380]  То есть, что ваше распараллелие не вносит каких-то деструктивных
[37:44.380 --> 37:45.380]  действий.
[37:45.380 --> 37:47.660]  Что ваш ответ остается.
[37:47.660 --> 37:50.820]  А вторая цель – это получить ускорение.
[37:50.820 --> 37:54.100]  Потому что если у вас правильный ответ, но у ускорения такой
[37:54.100 --> 37:59.500]  же или меньше, чем в последовательном алгоритме, смысла нет распараллеливать.
[37:59.500 --> 38:02.260]  Поэтому ускорение – это один из самых важных параметров,
[38:02.260 --> 38:05.820]  который вы будете анализировать и на семинарах вы будете
[38:05.820 --> 38:09.100]  строить вот такой график ускорения от количества
[38:09.100 --> 38:10.100]  процессов.
[38:10.100 --> 38:13.300]  Есть еще один интересный параметр, он называется
[38:13.300 --> 38:15.620]  эффективность работы вашего алгоритма.
[38:15.620 --> 38:17.940]  Это ускорение, деленное на количество процессов.
[38:17.940 --> 38:25.420]  Это на самом деле просто степень загрузки физических
[38:25.420 --> 38:26.420]  ядер.
[38:26.420 --> 38:30.100]  Если у нас ускорение в отдельном случае равно
[38:30.100 --> 38:34.300]  P, то эффективность будет равна единичке, то есть 100%.
[38:34.300 --> 38:37.340]  Это значит, что все ядра работают в полную мощность.
[38:37.340 --> 38:40.140]  Если по какой-то причине ускорение у вас равно меньше,
[38:40.140 --> 38:43.500]  представим себе, что это равно P пополам, то есть мы
[38:43.500 --> 38:45.900]  на этом графике лежим на середине между идеалом
[38:45.900 --> 38:49.180]  и ноликом, вот здесь вот точка, то у нас эффективность
[38:49.180 --> 38:53.820]  будет 0,5, то есть 50%.
[38:53.820 --> 38:58.700]  Значит, по какой-то причине ваши ядра работают не в
[38:58.700 --> 38:59.700]  полную силу.
[39:00.700 --> 39:07.660]  Либо это связано самим железом, либо это связано самим алгоритмом.
[39:07.660 --> 39:10.300]  Тут тоже есть разные причины.
[39:10.300 --> 39:15.740]  Но это тоже показатель того, как хороша ваша программа.
[39:15.740 --> 39:19.100]  И вот если вернуться обратно, для чего предназначены
[39:19.100 --> 39:23.900]  суперкомпьютеры, первая часть, уменьшение времени
[39:23.900 --> 39:26.740]  работы, то здесь хотелось бы достигать именно высокого
[39:26.740 --> 39:29.340]  ускорения при высокой эффективности.
[39:29.340 --> 39:33.340]  Потому что, как вам сейчас уже говорили в самом начале,
[39:33.340 --> 39:35.860]  у вас будет доступ к удаленным машинам.
[39:35.860 --> 39:39.060]  Обычно это машины какого-то коллективного пользования.
[39:39.060 --> 39:41.900]  На этих машина выделяется, здесь маленькие машинки,
[39:41.900 --> 39:45.420]  а есть большие, там много людей, большие задачи решаются.
[39:45.420 --> 39:50.420]  И вот владельцы этих центров коллективного пользования
[39:50.420 --> 39:53.980]  хотели бы, чтобы эти машины использовались наиболее
[39:53.980 --> 39:54.980]  эффективно.
[39:54.980 --> 39:57.500]  Эффективно это значит, что как можно больше ядер
[39:57.500 --> 40:00.660]  использовалось, как можно больше высокой эффективностью.
[40:00.660 --> 40:02.620]  Вот эффективность, вот здесь вот она и сидит.
[40:02.620 --> 40:05.540]  Вот она эффективность, чтобы можно запустить программу
[40:05.540 --> 40:10.380]  на 100 процессах, но она будет использовать 1% всех мощностей.
[40:10.380 --> 40:15.060]  Это не эффективная программа, это не целесообразное использование
[40:15.060 --> 40:18.100]  электроэнергии и вообще ресурсов этого центра коллективного
[40:18.100 --> 40:19.100]  пользования.
[40:19.100 --> 40:22.060]  И есть еще один параметр, такой масштабируемость
[40:22.060 --> 40:23.060]  алгоритма.
[40:23.060 --> 40:27.380]  Вот здесь он показан, например, что это такое?
[40:27.380 --> 40:31.340]  Насколько ваш алгоритм может быть запущен на как
[40:31.340 --> 40:34.180]  можно большем количестве вычислителей, то есть ядер
[40:34.180 --> 40:35.660]  или процессов.
[40:35.660 --> 40:38.860]  Например, ваш график ускорения ведет себя следующим образом.
[40:38.860 --> 40:40.980]  И у него есть очевидный максимум.
[40:40.980 --> 40:41.980]  Правильно?
[40:41.980 --> 40:46.620]  И вот максимум, можно, то есть дальше этого максимума
[40:46.620 --> 40:49.040]  вы по ускорению не продвинетесь.
[40:49.040 --> 40:51.940]  Ускорение меньшее можно получить при меньшем количестве
[40:51.940 --> 40:55.380]  p, чем p со звездочкой, то есть вот здесь и вот здесь.
[40:55.720 --> 40:58.520]  Но очевидно, что такое количество процессов, то
[40:58.520 --> 41:01.400]  же не целесообразно использовать, потому что такое же ускорение
[41:01.400 --> 41:03.800]  можно получить на меньшем количестве.
[41:03.800 --> 41:07.900]  Ну вот и говорится, что это масштабируемость, то
[41:07.900 --> 41:10.960]  количество, то есть можно сказать, что ваша программа
[41:10.960 --> 41:14.560]  масштабируема вот до такого количества p со звездочкой
[41:14.560 --> 41:15.560]  процессоров, почему?
[41:15.560 --> 41:19.160]  Потому что она на этом количестве достигает максимального
[41:19.160 --> 41:20.160]  ускорения.
[41:20.160 --> 41:22.240]  То есть какое минимальное число ядер, при котором
[41:22.240 --> 41:28.020]  вы получаете максимальное ускорение, так? Дальше нет
[41:28.020 --> 41:32.340]  смысла уже расти. Это вот масштабируемость алгоритма.
[41:32.340 --> 41:37.040]  Теперь давайте поговорим о том, что может приводить
[41:37.040 --> 41:42.320]  к тому, что ваш график ускорения отклоняется от идеального.
[41:42.320 --> 41:46.400]  Первая причина – это когда вы не можете полностью
[41:46.400 --> 41:51.040]  распараллелить ваш алгоритм. Это самая первая и фундаментальная
[41:51.040 --> 41:55.100]  причина. Мы посмотрим сейчас, почему это так. Почему не надо
[41:55.100 --> 41:59.040]  браться за те алгоритмы, которые полностью или почти
[41:59.040 --> 42:02.280]  полностью нельзя распараллелить. Эта штука называется законом
[42:02.280 --> 42:05.800]  ДАЛА. Вот представим себе, вот этот прямоугольник
[42:05.800 --> 42:10.680]  – это алгоритм. И вот представим себе, что это одна часть нашего
[42:10.680 --> 42:16.320]  алгоритма. И здесь альфа – это последовательная часть,
[42:16.320 --> 42:19.480]  ее нельзя никаким образом распараллелить. А беленькая
[42:19.480 --> 42:22.720]  часть распараллелить можно. Вот если запустить наш алгоритм
[42:22.720 --> 42:26.800]  на одноядерной машине, мы получим время T1. То есть
[42:26.800 --> 42:30.440]  это время работы нашего алгоритма на последовательной
[42:30.440 --> 42:34.680]  программе. Это все хорошо. Теперь, если мы запустим
[42:34.680 --> 42:39.360]  этот же алгоритм на многопроцессорной машине, то какое время
[42:39.360 --> 42:44.320]  мы получим P процессов? Обычно можно оценивать, ну
[42:44.320 --> 42:49.440]  на лекциях это как будет. P – это обычно процессоры,
[42:49.440 --> 42:53.680]  то есть это P – это процессы. S – это ускорение от speed-up.
[42:53.680 --> 42:56.680]  Слово speed-up. Здесь используется не acceleration, как в физике,
[42:56.680 --> 42:59.680]  а вот все-таки другое слово будет speed-up. Поэтому P – это
[42:59.680 --> 43:03.440]  количество процессов. Какое будет время? Альфа у
[43:03.440 --> 43:08.160]  нас никак не распараллелится. То есть альфа T1 останется,
[43:08.160 --> 43:11.320]  а вот этот белый прямоугольник, его можно поделить на части.
[43:11.320 --> 43:13.480]  Он может быть распараллелен, то есть время может быть
[43:13.520 --> 43:17.320]  уменьшено в P раз. Ну и плюс еще некие накладные расходы.
[43:17.320 --> 43:22.720]  T, S – это время передачи сообщений. Поскольку у нас сообщения
[43:22.720 --> 43:25.760]  работают, вернее процессы работают на кластере, они
[43:25.760 --> 43:29.600]  между собой могут обмениваться, и обычно так и происходит,
[43:29.600 --> 43:32.960]  поскольку они решают одну задачу. S от слова send, то
[43:32.960 --> 43:37.160]  есть передача сообщений – это накладные расходы.
[43:37.160 --> 43:40.240]  Вот такое время для нашего параллельного магаритма.
[43:40.240 --> 43:43.120]  Теперь давайте оценим ускорение. Ускорение – это нужен
[43:43.120 --> 43:47.560]  T1 поделить на вот эту штуку. Пока кажется громоздкой,
[43:47.560 --> 43:51.680]  но можно рассмотреть всякие такие граничные, пограничные
[43:51.680 --> 43:59.000]  варианты. Например, если у нас все можно распараллелить
[43:59.000 --> 44:01.840]  и у нас нет накладных расходов, то если посмотреть вот на
[44:01.840 --> 44:07.200]  эту формулу… Ой. Короче, S будет равно P. Ну в общем,
[44:07.200 --> 44:11.480]  S равно P. Альфа равно 0, и тогда у нас и T, S равно 0, тогда
[44:11.480 --> 44:18.480]  у нас получится T1, грубо говоря, на T1. В общем, это идеальный
[44:21.000 --> 44:25.200]  случай, получается, S равно 5. Потом, если у нас время
[44:25.200 --> 44:28.680]  накладных расходов больше, чем время расчета последовательного
[44:28.680 --> 44:32.120]  алгоритма, есть ли смысл даже из логики параллелить
[44:32.120 --> 44:36.000]  или нет? Ну в общем-то нет, потому что ускорение будет
[44:36.000 --> 44:40.000]  явно меньше единицы, и в общем, нецелесообразно
[44:40.000 --> 44:42.680]  это делать. Интересным случаем представляется, когда
[44:42.680 --> 44:48.120]  у нас накладных расходов нет, а вот альфа, то есть
[44:48.120 --> 44:50.360]  та часть, которую нельзя распараллелить, не равна
[44:50.360 --> 44:53.480]  нулю. То есть есть маленькая часть, которая всегда будет
[44:53.480 --> 44:56.120]  читаться последовательно. Тогда в этом случае ускорение
[44:56.120 --> 44:58.120]  будет представлять себе вот такую формулу. Она
[44:58.120 --> 45:02.840]  простая, но что она говорит? Давайте рассмотрим график
[45:02.840 --> 45:06.840]  ускорения в зависимости от альфа, ну при P, например,
[45:07.400 --> 45:11.360]  100. Вот сюда 100. У нас 100 ядер, это довольно-таки приличное
[45:11.360 --> 45:14.360]  число. И давайте теперь смотреть. Когда у нас альфа
[45:14.360 --> 45:17.160]  равно нулю, то ускорение у нас будет идеальным, будет
[45:17.160 --> 45:22.300]  равно 100. Альфа равно единицы, ускорение падает до чуть
[45:22.300 --> 45:27.300]  больше 50%. То есть оно падает почти в два раза. Альфа равно
[45:27.300 --> 45:33.740]  двойке, падает в три раза. Вот альфа равно пяти, падает
[45:33.740 --> 45:39.460]  больше, чем в пять раз. То есть маленькая часть нераспараллеленного
[45:39.460 --> 45:42.660]  алгоритма приводит к сильному уменьшению ускорения работы
[45:42.660 --> 45:47.260]  программы. И поэтому даже если у вас есть, ну, пара
[45:47.260 --> 45:50.860]  процентов, ну ладно. Но вот как только уже это составляет
[45:50.860 --> 45:54.460]  существенную часть, несколько процентов там до десяти,
[45:54.460 --> 45:58.580]  это прям сильно все портит. Или такой еще график, смотрите.
[45:58.580 --> 46:02.620]  К чему стремится максимальное ускорение при как можно
[46:02.620 --> 46:05.820]  большом количестве процессоров? Вот P стриминг бесконечности,
[46:05.820 --> 46:11.020]  то S у нас стремится к дроби. Один поделит на альфа. Давайте
[46:11.020 --> 46:15.700]  посмотрим на графике. Альфа равно пяти. Зеленый график.
[46:15.700 --> 46:20.460]  P большое, ну две тысячи и считаем, что это почти бесконечность.
[46:20.460 --> 46:26.700]  Ускорение у нас ограничено двадцатью. Альфа равно десять,
[46:26.700 --> 46:31.060]  ограничено десятью. Альфа равно двадцать пять, ограничено
[46:31.060 --> 46:34.940]  двумя. И вот представьте себе, половину программы
[46:34.940 --> 46:39.500]  можно распределить, половину нет. Вы берете супермашину
[46:39.500 --> 46:43.020]  с бесконечным количеством вычислительных ядер. И
[46:43.020 --> 46:47.020]  вы как не хотите, но не можете достичь ускорения больше
[46:47.020 --> 46:58.820]  двойки. Никак. Вот о чем говорит закон Амдалла. Поэтому здесь
[46:58.820 --> 47:04.260]  нужно очень четко понимать, какие задачи можно взять,
[47:04.260 --> 47:09.060]  какие нельзя. Какие можно распараллелить, какие нельзя.
[47:09.060 --> 47:12.740]  Какие можно хорошо, какие параллелиться плохо. Это
[47:12.740 --> 47:19.140]  самый простой закон. Самое первое, на что нужно обратить
[47:19.140 --> 47:22.200]  внимание, когда вы соберете что-либо, там распараллелить
[47:22.200 --> 47:25.620]  какую-либо программу. Нужно сначала проанализировать,
[47:25.660 --> 47:30.660]  можно или нет. Все, на сегодня достаточно. Что успели, то
[47:30.660 --> 47:32.220]  успели. Думайте.

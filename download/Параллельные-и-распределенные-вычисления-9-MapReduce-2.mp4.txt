[00:00.000 --> 00:09.680]  Окей, давайте начинать. Тогда у нас сегодня вторая
[00:09.680 --> 00:17.680]  презентация по MapReduce. Напомню, на чем мы закончили
[00:17.680 --> 00:21.460]  в прошлый раз. Мы с вами разобрали, что вообще такое
[00:21.460 --> 00:24.280]  парадигма MapReduce, из каких частей она состоит, то есть
[00:24.280 --> 00:32.200]  MapSortReduce. Мы с вами разобрали несколько примеров. Например,
[00:32.200 --> 00:35.840]  что MapReduce можно реализовать вообще без всякого ходупа,
[00:35.840 --> 00:38.160]  можно на обычном питоне, на самом деле не только
[00:38.160 --> 00:44.000]  на питоне, можно на плюсах реализовать. И мы с вами
[00:44.000 --> 00:49.120]  разобрали типичную жадачку для MapReduce, то есть wordCount.
[00:49.120 --> 00:51.720]  То есть если нам надо посчитать в каком-то тексте, сколько
[00:51.720 --> 00:55.680]  раз встречается каждое слово, что мы делаем. Мы каждому
[00:55.680 --> 00:58.600]  слову в соответствии ставим единичку, потом на стадии
[00:58.600 --> 01:02.320]  сортировки группируем, и на стадии редюса суммируем
[01:02.320 --> 01:05.400]  все единички для каждого слова. То есть вот такая вот
[01:05.400 --> 01:09.640]  штука получается. Ну и мы с вами разобрали, как это
[01:09.640 --> 01:13.080]  работает в ходупе. На семинарах, насколько я понимаю, вам
[01:13.080 --> 01:16.120]  уже объяснили, как это работает детально, вы поразбирали
[01:16.120 --> 01:18.840]  какие-то примерчики, позапускали на кластере. Было у всех
[01:18.840 --> 01:24.640]  такое. Хорошо. А теперь давайте разберем еще несколько
[01:24.640 --> 01:27.960]  примеров. Например, как решить задачу grep с помощью
[01:27.960 --> 01:33.920]  MapReduce, как вы думаете. Вот дан какой-нибудь документ
[01:33.920 --> 01:37.480]  в такого вида как на рисунке, и нам надо его погрэпать,
[01:37.480 --> 01:47.280]  то есть пофильтровать по какому-то ключу. Вот все
[01:47.480 --> 01:50.520]  описываем, опять же, с точки зрения Map делает то-то,
[01:50.520 --> 01:53.560]  Sort делает то-то, Reduce делает то-то. Sort можно опустить,
[01:53.560 --> 01:56.400]  в принципе. Обычно и так понятно, что он делает.
[01:56.400 --> 02:06.720]  Уже лучше. То есть у нас осталось одно слово, мы сделали
[02:06.720 --> 02:10.280]  бинарный ключик 0 или 1, а дальше вопрос, а зачем нам
[02:10.280 --> 02:13.440]  вообще тащить за собой слова, у которых 0? То есть наша
[02:13.440 --> 02:16.360]  задача сделать grep, то есть выкинуть вообще те строчки,
[02:16.360 --> 02:20.840]  у которых нет совпадений. Поэтому зачем нам ставить
[02:20.840 --> 02:23.200]  в соответствие 0, когда мы просто можем на маппере
[02:23.200 --> 02:28.760]  проверить. Если слово соответствует тому, что мы хотим, мы его
[02:28.760 --> 02:31.800]  печатаем. Если не соответствует, мы его не печатаем. Все.
[02:31.800 --> 02:36.240]  И этого достаточно. То есть на маппере мы делаем такую
[02:36.240 --> 02:40.760]  проверку. Слово у нас, если оно одно, его можно запихать,
[02:40.760 --> 02:43.840]  например, DistributedCache. На семинарах вам показали
[02:43.840 --> 02:50.320]  DistributedCache. Кому-то показали, кому-то нет. Но если не показали,
[02:50.320 --> 02:52.640]  напомните семинаристам, видимо, в следующий раз покажут.
[02:52.640 --> 03:02.000]  И reuse нам не нужен, а значит sort нам тоже не нужен. Вторая
[03:02.000 --> 03:07.680]  задачка groupBy. GroupBy на самом деле можно рассматривать
[03:07.680 --> 03:11.320]  как в SQL. В SQL groupBy сам по себе не бывает. Он обычно
[03:11.320 --> 03:14.440]  идет с какой-то аналитикой. Мы сгруппировали, и поэтому
[03:14.440 --> 03:18.120]  groupBy посчитали, например, сумму или какой-нибудь там
[03:18.120 --> 03:24.360]  квантиль, допустим. Тогда эта аналитика будет у нас
[03:24.360 --> 03:28.160]  считаться на reduce. А если задача просто создать группы,
[03:28.160 --> 03:32.040]  сгруппировать, то нам тогда reducer не нужен, но он все
[03:32.040 --> 03:37.160]  равно будет. Почему reducer все равно будет? Потому что
[03:37.160 --> 03:41.160]  в MapReduce вот эти три стадии есть некая связь. Мы не
[03:41.160 --> 03:47.720]  можем запустить, например, только sort. Если нам нужен
[03:47.720 --> 03:53.160]  sort, то нам нужен reduce, хоть какой-нибудь, который, например,
[03:53.160 --> 03:58.800]  просто печатает то, что получил на вход. То есть map сам
[03:58.800 --> 04:02.640]  по себе может быть. Reduce сам по себе быть не может.
[04:02.640 --> 04:06.800]  Ему нужен sort, и ему нужен map. И sort сам по себе тоже
[04:06.800 --> 04:14.400]  не может. Ему нужен map, и нужен reduce. Вот. А теперь попробуем
[04:14.400 --> 04:18.080]  оптимизировать то, что мы с вами обсуждали прошлый
[04:18.080 --> 04:28.880]  раз. Вот та же задачка wordCount. Мы считаем слово единичка.
[04:28.880 --> 04:32.220]  На маппере сделали слово единичка, на редюсере сложили
[04:32.220 --> 04:36.000]  слово единичка. И возникает такая идея. Если мы видим,
[04:36.000 --> 04:39.120]  что на маппере эти слова единичка уже совпадают.
[04:39.120 --> 04:42.960]  Вот тут есть, если вы посмотрите на эту красную область,
[04:42.960 --> 04:48.480]  вы увидите don't 1, don't 1, need 1, need 1. Уже совпадают.
[04:48.480 --> 04:51.280]  Возникает желание, давайте мы его схлопнем прямо здесь.
[04:51.280 --> 04:55.520]  Зачем тащить кучу пар с собой на редюсер, когда можно
[04:55.520 --> 05:00.200]  здесь получить don't 2, там need 4 и так далее. У нас будет
[05:00.200 --> 05:03.920]  пара одна. Это будет меньше данных, и нам будет быстрее
[05:04.040 --> 05:09.040]  и проще передавать на редюс. Такая оптимизация есть,
[05:09.040 --> 05:18.440]  она называется комбайнер. Вот работает она вот так.
[05:18.440 --> 05:21.080]  Когда у нас отработал маппер, он результаты своей работы
[05:21.080 --> 05:24.520]  сбрасывает, если вы помните, сначала в буфер, потом на
[05:24.520 --> 05:27.680]  диск. Потом происходит стадия локальной группировки
[05:27.680 --> 05:32.560]  и локальной сортировки. Вот то, что здесь написано.
[05:32.560 --> 05:35.400]  Когда мы данные на локальном кусочке, на локальном маппере
[05:35.400 --> 05:38.320]  сгруппировали, мы можем запустить комбайнер. Можем
[05:38.320 --> 05:43.440]  запустить, можем не запускать, как захотим. Кстати, вот
[05:43.440 --> 05:46.800]  эти три стадии, что они напоминают из прошлого
[05:46.800 --> 05:59.920]  раза? Ну да, они напоминают обычный мапредьюс. То есть
[05:59.920 --> 06:03.480]  сортировка просто локальная. Комбайн это тоже по сути
[06:03.480 --> 06:09.800]  такой редьюсер с урезанным функционалом. И запускается
[06:09.800 --> 06:12.720]  он вот здесь, где указано красное стрелочке. То есть
[06:12.720 --> 06:17.120]  когда мы создали вот этот вот файл и готовы его разбрасывать
[06:17.120 --> 06:27.840]  по редьюсерам, мы сначала его утрамбовываем. Вот,
[06:27.840 --> 06:30.480]  у комбайнера по сравнению с редьюсером есть несколько
[06:30.480 --> 06:35.060]  особенностей. Давайте рассмотрим их особенно. Обратим внимание
[06:35.060 --> 06:40.200]  на последнее, я его расскажу отдельно. То, что до последнего
[06:40.200 --> 06:44.480]  в принципе понятно. Hadoop сортирует результаты работы
[06:44.480 --> 06:47.640]  мапа. То есть это небольшой редьюсер, который сортирует
[06:47.640 --> 06:52.320]  и группирует все и считает все. А это работа с маленьким
[06:52.320 --> 06:55.680]  кусочком данных. Поэтому данных мало, сеть вообще
[06:55.680 --> 06:58.680]  не используется и такой комбайнер он обычно работает
[06:58.680 --> 07:03.960]  быстро. Но так как мы комбайнером что-то сагрегировали, то
[07:03.960 --> 07:07.080]  редьюсеру тоже работать проще. То есть по сути комбайнер
[07:07.080 --> 07:12.440]  это такая локальная помощь для редьюсера. А вот дальше
[07:12.440 --> 07:16.160]  вот эта вот последняя часть, что комбайнер может применяться
[07:16.160 --> 07:19.360]  несколько раз, а может не одного раза, а может один
[07:19.360 --> 07:23.000]  и контролировать мы это в принципе не можем. Ну как
[07:23.000 --> 07:25.640]  бы можем, но это надо глубоко залазить в Hadoop и обычно
[07:25.640 --> 07:29.160]  так не делают. Мы сейчас рассмотрим, как так получается.
[07:29.160 --> 07:33.640]  Давайте смотреть. Вот эта вот первая часть это работа
[07:33.640 --> 07:39.600]  мапера. И вот эти вот, вот это вот круг разбитый на кусочки,
[07:39.600 --> 07:46.680]  эти кусочки это как раз файлы. То есть отработал мапер,
[07:46.680 --> 07:50.680]  записал в буфер, буфер скинулся на диск, появился один
[07:50.680 --> 07:56.400]  такой файлик, потом второй, потом третий и так далее.
[07:56.400 --> 07:59.080]  Дальше на этих файликах у нас запускаются комбайнеры.
[07:59.080 --> 08:02.560]  Давайте посмотрим как они запускаются, что при этом
[08:02.560 --> 08:08.760]  происходит. Вот у нас запустился один файл, но допустим этот
[08:08.760 --> 08:11.880]  файл большой или там очень много ключей и мы запустили
[08:11.880 --> 08:15.160]  несколько комбайнеров. И также мы проделали для
[08:15.160 --> 08:20.040]  нескольких еще файлов. После этого когда у нас получился
[08:20.040 --> 08:24.640]  результат, у нас получается несколько уже схлопнутых,
[08:24.640 --> 08:27.200]  уже частично обработанных файлов. А потом мы видим,
[08:27.200 --> 08:30.560]  ну ходу как бы анализирует эти файлы и видит, что у нас
[08:30.560 --> 08:34.320]  опять ключи совпадают. То есть да, у нас уже нету
[08:34.320 --> 08:38.480]  слова единичка, слова единичка. У нас есть, например, need
[08:38.480 --> 08:44.580]  4, need 1, need 6. Ключи совпадают, значения разные, их можно
[08:44.580 --> 08:48.160]  еще раз сложить. Что происходит? Запускается еще раз комбайнер.
[08:48.160 --> 08:55.520]  То есть вот эти вот маленькие результаты комбайнеров,
[08:55.520 --> 09:01.360]  комбайнеров первого этапа, вот этих вот синих, они
[09:01.360 --> 09:06.640]  соединяются вместе, происходит операция merge и запускается
[09:06.640 --> 09:13.240]  еще один комбайнер. После этой второй стадии мы видим,
[09:13.240 --> 09:18.840]  что все равно у нас, там где прошли вторые стадии,
[09:18.840 --> 09:22.840]  у нас все равно есть совпадение. И мы делаем еще раз ту же
[09:22.840 --> 09:25.680]  операцию, то есть опять схлопываем и запускаем еще
[09:25.680 --> 09:29.680]  раз комбайнер. Так можем проделать несколько раз
[09:29.680 --> 09:33.000]  и в самый последний раз это мы замечаем следующее,
[09:33.000 --> 09:36.080]  мы замечаем то, что у нас на одной ноде работало
[09:36.080 --> 09:41.920]  несколько мапперов этой джобы. И мы можем вот эти
[09:41.920 --> 09:45.720]  результаты, уже покомбайненные, еще раз схлопнуть, еще
[09:45.720 --> 09:49.400]  раз объединить и запустить еще раз комбайнер. Ну потому
[09:49.400 --> 09:52.160]  что все равно сеть не используется, все равно это все в рамках
[09:52.160 --> 09:55.840]  одной машинки, почему бы и нет. Мы это сделаем, это
[09:55.840 --> 09:59.640]  называется у нас merge final, то есть объединили уже с
[09:59.640 --> 10:05.480]  нескольких маптасков результаты в один, еще раз скомбайнили
[10:05.480 --> 10:09.200]  и получили уже полностью максимально схлопнутый
[10:09.200 --> 10:18.840]  результат для вот этой ноды. Это я вам рассказал,
[10:18.840 --> 10:21.800]  почему комбайнер может запускаться несколько раз.
[10:21.800 --> 10:25.840]  То есть тут несколько стадий, причем комбайнер запускается
[10:25.840 --> 10:29.560]  на результатах самого себя. А когда может не быть
[10:29.560 --> 10:34.280]  ни одного раза? Вот здесь как раз на схемке указано
[10:34.280 --> 10:39.080]  if a single record is too big to fit in memory buffer. Но это на самом
[10:39.080 --> 10:42.240]  деле не обязательно чтобы был именно single record, может
[10:42.240 --> 10:46.200]  быть например две-три записи. То есть в ходу тоже есть
[10:46.200 --> 10:49.320]  настройки, которые отключат комбайнер, если записей
[10:49.320 --> 10:54.320]  мало. Если у нас допустим одна запись занимает там
[10:54.320 --> 10:58.360]  несколько мегабайт, и этих записей помещается в файл
[10:58.360 --> 11:03.440]  мало, нет смысла запускать комбайнер. Почему комбайнер
[11:03.440 --> 11:06.400]  нет смысла запускать, потому что комбайнер это процесс.
[11:06.440 --> 11:10.520]  Для того, чтобы запустился комбайнер надо пойти в ресурс-менеджер,
[11:10.520 --> 11:14.240]  попросить ресурсы, дождаться этих ресурсов, стартовать
[11:14.240 --> 11:17.280]  виртуальную машину джавы, стартовать в нем вот этот
[11:17.280 --> 11:22.280]  процесс. Потом это все обсчитается и нужно это все завершить.
[11:22.280 --> 11:27.480]  Ну долго, иногда это вообще делать не надо. Поэтому иногда
[11:27.480 --> 11:31.040]  комбайнера просто нету. И даже если мы в коде прописали,
[11:31.040 --> 11:34.520]  что мы хотим комбайнер добавить, он у нас не будет запущен.
[11:37.040 --> 11:39.480]  Это все настраивается на уровне опции, вот можете
[11:39.480 --> 11:44.280]  посмотреть, открыть эту схему, и здесь много вот этих
[11:44.280 --> 11:48.560]  тонких настроек для комбайнера. Обычно в них особо не лезут.
[11:48.560 --> 11:54.560]  Ну по крайней мере, когда мы пишем код для комбайнера,
[11:54.560 --> 11:57.040]  мы должны быть уверены, что он будет на любых настройках
[11:57.040 --> 12:00.720]  работать, то есть задачка не упадет, если комбайнер
[12:00.720 --> 12:03.120]  будет запускаться несколько раз, если он запустится
[12:03.120 --> 12:06.160]  один раз, если он вообще не запустится. Во всех этих
[12:06.160 --> 12:13.320]  случаях задачка не упадет. Чтоб так было, нам и нужно
[12:13.320 --> 12:17.280]  выполнять вот это условие. Комбайнер не может изменять
[12:17.280 --> 12:22.120]  ключ и значение, то есть тип ключа и значения. Ну и
[12:22.120 --> 12:24.280]  если говорить в общем, то он вообще не может менять
[12:24.280 --> 12:26.640]  структуру данных. То есть не может в комбайнере
[12:26.640 --> 12:31.360]  получиться вместо строка число, строка tuple или там
[12:31.360 --> 12:33.600]  строка array. Такого быть не должно.
[12:36.360 --> 12:40.840]  Давайте проверим, как мы это поняли, и попробуем
[12:40.840 --> 12:44.080]  составить комбайнер, ну просто скажите, какую функцию
[12:44.080 --> 12:47.440]  комбайнер будет выполнять для каждого из этих трех
[12:47.440 --> 12:48.440]  кейсов.
[12:59.440 --> 13:03.040]  Пока вы думаете, вопрос, на семинарах уже проходили
[13:03.040 --> 13:06.080]  сортировку с помощью добавления девяток.
[13:16.080 --> 13:18.080]  Ну давайте тогда думать, каким будет комбайнер вот
[13:18.080 --> 13:19.080]  для суммы, например.
[13:33.080 --> 13:38.080]  Так, ну будет просто сумма для одинакового ключа.
[13:38.080 --> 13:42.080]  Окей, для среднего, что? Тут немного сложнее, потому
[13:42.080 --> 13:45.080]  что среднее от среднего, оно неправильно будет считаться.
[13:51.080 --> 13:54.080]  А сумма и количество у нас с структурой меняется.
[13:55.080 --> 13:58.080]  Так, где маркер? То есть у нас было, например, там
[13:58.120 --> 14:00.120]  А, нельзя пальцем рисовать.
[14:05.120 --> 14:06.120]  Не работает.
[14:23.120 --> 14:24.120]  Ну, ну, ну, ну, ну.
[14:24.160 --> 14:25.160]  Ну, не работает.
[14:25.160 --> 14:28.200]  Не работает, я тогда открою блокнот.
[14:55.160 --> 14:56.200]  И буду показывать здесь.
[15:25.200 --> 15:27.200]  А второе параллельное считается суммой.
[15:36.200 --> 15:39.200]  А потом ты делаешь следующую джобу, которая это все будет
[15:39.200 --> 15:40.200]  объединять.
[15:42.200 --> 15:45.200]  Работать такое, конечно, будет, но наша цель, конечно,
[15:45.200 --> 15:46.200]  не будет.
[15:50.200 --> 15:51.200]  Ну, да.
[15:51.240 --> 15:55.240]  Работать такое, конечно, будет, но наша цель, комбайнер,
[15:55.240 --> 15:56.240]  зачем мы делаем?
[15:56.240 --> 15:59.240]  Чтобы оптимизировать, чтобы быстрее работало все.
[15:59.240 --> 16:01.240]  А ты вместо одной джобы сделал две.
[16:02.240 --> 16:04.240]  Поэтому не очень хороший вариант.
[16:12.240 --> 16:15.240]  На самом деле, можно подумать просто немного по-другому.
[16:15.240 --> 16:18.240]  Мы не вот эту структуру пытаемся запихать в одно
[16:18.280 --> 16:19.280]  число.
[16:19.280 --> 16:21.280]  А мы просто здесь структуру поправим.
[16:24.280 --> 16:25.280]  Ну, сделаем вот так.
[16:25.280 --> 16:28.280]  Кто нам мешает на мапере вместо одной единички поставить
[16:28.280 --> 16:29.280]  две единички?
[16:31.280 --> 16:34.280]  И все, у нас мы по одной единичке посчитаем сумму,
[16:34.280 --> 16:36.280]  по другой единичке посчитаем каунд.
[16:37.280 --> 16:39.280]  И структура будет сохраняться.
[16:41.280 --> 16:43.280]  Окей, а что делаем с медианой?
[16:48.240 --> 17:13.240]  Для среднего мы, так как мы не можем посчитать средний
[17:13.240 --> 17:15.280]  от среднего, то есть вот такая штука.
[17:18.240 --> 17:20.240]  работать не будет.
[17:20.240 --> 17:23.920]  Значит нам нужно считать отдельно сумму, отдельно каунт, а это два элемента.
[17:24.560 --> 17:26.560]  На маппере у нас была
[17:27.160 --> 17:30.000]  ключ и значение, где значение это одно число,
[17:30.800 --> 17:35.200]  а на комбайнере мы хотим сделать два элемента, в этом и проблема, и нам пришлось
[17:35.600 --> 17:39.360]  маппер раздувать, то есть менять его структуру, чтобы было два числа.
[17:39.360 --> 17:41.360]  Числа.
[17:46.880 --> 17:48.880]  Количество чего?
[17:50.960 --> 17:53.760]  Так, и что мы дальше с этим будем делать?
[17:56.160 --> 17:58.760]  А количество мы можем посчитать? У нас же комбайнер,
[17:59.480 --> 18:05.040]  он считает что-либо только для локального кусочка, он не может посчитать все количество.
[18:06.640 --> 18:08.640]  И дальше что?
[18:09.360 --> 18:11.360]  То есть мы считаем на комбайнере локальную медиану, да?
[18:13.680 --> 18:15.680]  Не получится.
[18:18.480 --> 18:22.880]  Вот, то есть для того, чтобы посчитать медиану вообще в целом, нам нужно,
[18:22.880 --> 18:28.640]  чтобы наш ряд был отсортирован глобально, то есть весь ряд, чтобы был отсортирован, и мы ищем
[18:29.200 --> 18:33.440]  там центральные значения или два центральных значения поделить на два.
[18:33.920 --> 18:37.600]  Ключевое, что для того, чтобы посчитать медиану, нам надо отсортировать все.
[18:38.640 --> 18:42.400]  Что у нас идет перед комбайнером? Local sort и перед ним маппер.
[18:42.400 --> 18:46.960]  Нигде там отсортировать все, возможностей нету, поэтому
[18:48.800 --> 18:50.800]  получается вот так.
[18:50.800 --> 18:55.600]  То есть для медианы мы не можем придумать комбайнер, потому что медиана требует полной сортировки.
[18:55.840 --> 18:57.840]  Вот мы с вами такие три кейса разобрали.
[19:00.880 --> 19:02.880]  Есть еще in-mapper-комбайнер.
[19:02.880 --> 19:06.880]  Это такой хак, который, с одной стороны, иногда полезный, иногда вредный.
[19:06.880 --> 19:08.880]  Почему я сейчас расскажу.
[19:08.880 --> 19:16.880]  Мы с вами уже обсудили то, что сам по себе комбайнер, он требует не сильно много, но все-таки накладные расходы,
[19:16.880 --> 19:18.880]  то есть не очень много.
[19:18.880 --> 19:28.880]  Мы уже обсудили то, что сам по себе комбайнер, он требует не сильно много, но все-таки накладные расходы на работу с ресурсами, на работу с джавой.
[19:28.880 --> 19:30.880]  Иногда этого делать не хочется.
[19:30.880 --> 19:36.880]  In-mapper-комбайнер, это когда мы прямо в коде делаем логику комбайнера.
[19:36.880 --> 19:40.880]  Ну вот взять опять же тот же word count.
[19:40.880 --> 19:42.880]  Что там делает маппер?
[19:42.880 --> 19:46.880]  Он берет слово, пишет единичку, которую потом мы когда-то к чем-то сложим.
[19:46.880 --> 19:54.880]  Давайте мы не будем ждать чего-то когда-то, а просто на маппере, на обычном будем хранить словарик, вида слова, число.
[19:54.880 --> 20:02.880]  Пришло новое слово, мы или добавили в словарик новый ключ, или добавили единичку к текущему ключу.
[20:02.880 --> 20:04.880]  То есть мы будем вот такой вот словарь накапливать.
[20:04.880 --> 20:08.880]  И вот этот word count у нас будет локально считаться.
[20:08.880 --> 20:16.880]  Это будет хорошо и быстро до того момента, пока наша маппа не займет весь наш контейнер.
[20:16.880 --> 20:22.880]  То есть всю память, которую нам выдали, и потом мы скажем out of memory, все пока.
[20:22.880 --> 20:28.880]  Поэтому in-mapper-комбайнер, он хорош тогда, когда мы точно знаем, что значений будет немного.
[20:28.880 --> 20:40.880]  То есть условно мы, например, считаем всех студентов МФТИ, у которых делим студентов МФТИ на классы.
[20:40.880 --> 20:44.880]  Там хорошисты, отличники, те, у кого удовлетворительно и так далее.
[20:44.880 --> 20:46.880]  Четыре класса.
[20:46.880 --> 20:48.880]  И вот мы считаем, сколько студентов в каждом классе.
[20:48.880 --> 20:52.880]  Мы точно знаем, что у нас количество классов никогда не вырастет.
[20:52.880 --> 20:58.880]  Или там, например, считаем количество студентов на каждой фистех-школе тоже.
[20:58.880 --> 21:02.880]  Фистех-школа количество какое-то константное.
[21:02.880 --> 21:08.880]  В какой-то момент может случиться плюс один какой-нибудь, но плюс тысяча фистех-школ явно не появится.
[21:08.880 --> 21:10.880]  Вот в таких случаях можно использовать in-mapper-комбайнер.
[21:10.880 --> 21:16.880]  А вот для обычного word count, где слов постоянно может быть больше, больше и больше,
[21:16.880 --> 21:18.880]  in-mapper-комбайнер не годится.
[21:18.880 --> 21:20.880]  Вот есть ли какие-то по-комбайнеру вопросы?
[21:30.880 --> 21:36.880]  Тогда, компаратор, на семинаре вам расскажут, как вообще делать сортировку,
[21:36.880 --> 21:42.880]  если нам нужно, если нам нужно, если нам нужно, если нам нужно.
[21:42.880 --> 21:46.880]  И вам расскажут, как вообще делать сортировку,
[21:46.880 --> 21:50.880]  если нам нужно отсортировать данные как-нибудь необычно.
[21:50.880 --> 21:56.880]  Ну, чтобы понять, что значит необычно для ходупа, надо понять, что значит обычно для ходупа.
[22:00.880 --> 22:04.880]  А обычно это значит, что вот стандартный сорт в ходупе, на который мы никак не влияем,
[22:04.880 --> 22:08.880]  вот он как есть, так и есть, что он делает, как именно он сортирует.
[22:08.880 --> 22:14.880]  Он сортирует по возрастанию, это первое.
[22:14.880 --> 22:18.880]  То есть если нам надо найти топы и отсортировать по убыванию, уже не годится.
[22:18.880 --> 22:30.880]  Дальше он сортирует все объекты, даже если это числа, он сортирует их лексикографически, как числа, то есть как текст.
[22:32.880 --> 22:34.880]  Сейчас попробую.
[22:38.880 --> 22:42.880]  Вот, то есть что значит отсортировать числа лексикографически?
[22:42.880 --> 22:52.880]  Это значит, что числа типа, вот такие числа, они будут отсортированы именно в таком порядке.
[22:52.880 --> 23:00.880]  Потому что мы их сравниваем не как числа, сравниваем по символю.
[23:00.880 --> 23:10.880]  То есть, надо как-то с этим справиться, как бы вы справлялись и как бы вы справлялись с такими числами.
[23:10.880 --> 23:12.880]  Так, вот этот вот.
[23:12.880 --> 23:22.240]  То есть надо как-то с этим справиться, вот как бы
[23:22.240 --> 23:25.560]  вы справлялись с этим, имея на руках только маппер
[23:25.560 --> 23:29.280]  и редьюсер, как мы можем преобразовать наш ключ значения
[23:29.280 --> 23:34.440]  на маппере, чтобы сортировка была правильная, да и третий
[23:34.440 --> 23:37.680]  момент, мы сортируем только по первому полю.
[23:37.680 --> 23:46.120]  Вот это то, что делает стандартная сортировка в
[23:46.120 --> 23:47.120]  Hadoop.
[23:47.120 --> 23:49.020]  Давайте подумаем, как можно с ней бороться штатными
[23:49.020 --> 23:50.020]  средствами.
[23:50.020 --> 23:52.400]  Мы знаем маппер, мы знаем редьюсер, внутри сортировки
[23:52.400 --> 23:54.120]  мы пока не лезем, потому что не знаем как.
[23:54.120 --> 24:12.520]  Да, тут добавить, умножить на минус единичку, хорошо.
[24:12.520 --> 24:21.680]  Что делать с вот этим, когда оно по символе сравнивает.
[24:21.680 --> 24:30.800]  А хэш почему?
[24:30.800 --> 24:34.440]  У нас есть набор значений, мы хотим отсортировать,
[24:34.440 --> 24:35.440]  а что суммировать?
[24:35.440 --> 24:54.140]  Ну, я так понял, ты имеешь в виду длину выровнять
[24:54.140 --> 24:57.140]  как-то или что?
[24:57.140 --> 25:05.180]  Да, вот я не совсем понял, куда туда можно приплести
[25:05.180 --> 25:07.780]  хэш, который тоже может разную длину давать.
[25:07.780 --> 25:16.940]  Что с проектором?
[25:16.940 --> 25:19.620]  Чтобы сделать одинаковую длину, нам надо сначала
[25:19.620 --> 25:24.260]  найти самое длинное число, а в данном случае это 4000.
[25:25.060 --> 25:32.620]  И можно, как вы сказали, дописать слева нули, а можно сделать
[25:32.620 --> 25:36.080]  проще и с одной стороны не проще, с другой стороны
[25:36.080 --> 25:37.420]  просто убить двух зайцев.
[25:37.420 --> 25:40.620]  И вот эту проблему починить, и вот эту.
[25:40.620 --> 25:53.220]  Сделать 10 в степени n-1-k, где k это вот это число.
[25:53.220 --> 25:55.500]  Но проблема была в том, что данные сортируются
[25:55.500 --> 25:56.500]  только по возрастанию.
[25:56.500 --> 26:03.380]  А здесь нам надо длину выровнять.
[26:03.380 --> 26:07.540]  Если мы сделаем вот эту вот формулу применим, то
[26:07.540 --> 26:08.540]  у нас получится.
[26:08.540 --> 26:29.820]  Давайте считать, что n у нас равно 4, вот такие у нас
[26:29.820 --> 26:33.860]  будут получаться числа, и мы видим, что у них длина
[26:33.860 --> 26:34.860]  одинаковая.
[26:34.860 --> 26:41.420]  Да, почему я здесь сделал 10 в степени n-1, потому что
[26:41.420 --> 26:43.020]  вдруг у нас тут 0 где-то стоит.
[26:43.020 --> 26:50.900]  Если я сделал 10 в степени n-0, то у нас будет где-то
[26:50.900 --> 26:53.740]  тут стоять 10 тысяч, и оно нам весь порядок испортит.
[26:53.740 --> 26:58.020]  Вот, и таким образом мы справились за одной из
[26:58.020 --> 27:00.500]  сортировок по возрастанию, потому что тут уже сортировка
[27:00.500 --> 27:01.620]  будет инвертирована.
[27:01.620 --> 27:09.500]  Вот, то есть вот такую штуку надо сделать, и с третьей
[27:09.500 --> 27:12.700]  проблемой как справиться, ну поменять местами поля
[27:12.700 --> 27:13.700]  на мапере.
[27:13.700 --> 27:18.020]  Ну хорошо, как бы все это можно сделать, но есть две
[27:18.020 --> 27:19.020]  проблемы.
[27:19.020 --> 27:22.580]  Первая проблема в том, что когда мы стадию сортировки
[27:22.580 --> 27:25.380]  пройдем, на редьюсере нам надо будет все это раскрутить
[27:25.380 --> 27:30.620]  обратно, ну потому что ответ все-таки должен быть
[27:30.620 --> 27:33.140]  какой-то вот такой, более адекватный, потому что мы
[27:33.140 --> 27:36.500]  потом будем как-нибудь диаграммы строить по этому результату,
[27:36.500 --> 27:39.020]  и с вот такими вот девятками непонятно что делать.
[27:39.020 --> 27:43.780]  Ну и потом для того, чтобы вот эту формулу применить,
[27:43.780 --> 27:44.780]  нам надо знать n.
[27:44.780 --> 27:49.300]  Для этого нам надо уже заранее оценить, какой у нас может
[27:49.300 --> 27:50.300]  быть максимальный каунт.
[27:50.300 --> 27:56.340]  Вот, поэтому на семинарах вам покажут, это все конечно
[27:56.340 --> 27:59.620]  костыли, так и делать не надо, а как надо делать?
[27:59.620 --> 28:00.620]  Как надо делать компаратор?
[28:00.620 --> 28:13.260]  То есть вот мы поняли, что вот это конечно не вариант
[28:13.260 --> 28:14.940]  дописывания кучи девяток.
[28:14.940 --> 28:20.700]  Компаратор, зачем он вообще тут надо и откуда он взялся?
[28:20.700 --> 28:23.380]  Ну раз есть сортировка, для любой сортировки нужно
[28:23.380 --> 28:24.380]  сравнение.
[28:24.380 --> 28:27.620]  И в компараторе мы просто прописываем, как именно
[28:27.620 --> 28:30.620]  мы будем сравнивать, потому что если это числа, то ходуб
[28:30.620 --> 28:34.620]  их как-то может сравнить, а если это стулья парты
[28:34.620 --> 28:38.660]  и крокодилы, то как бы непонятно, как их сравнивать.
[28:38.660 --> 28:42.380]  Мы пишем свой класс, или используем существующий,
[28:42.380 --> 28:45.740]  на самом деле на нашем курсе во всех домашках и во всех
[28:45.740 --> 28:49.180]  задачках хватит использования существующего компаратора,
[28:49.180 --> 28:54.540]  причем одного, который называется K-field based компаратор,
[28:54.540 --> 28:57.420]  то есть как бы он так и расшифровывается как компаратор,
[28:57.420 --> 28:58.700]  который базируется на ключах.
[28:58.700 --> 29:07.220]  Если вдруг вам в жизни понадобится использовать какой-то
[29:07.220 --> 29:11.940]  другой компаратор или написать самому, то придется взять
[29:11.940 --> 29:18.540]  вот этот класс, отнаследоваться от него и реализовать функцию
[29:18.540 --> 29:19.540]  compare.
[29:19.540 --> 29:22.980]  Здесь есть Javista?
[29:22.980 --> 29:29.020]  Если Javista нет, то функция compare напишется на Javi достаточно
[29:29.020 --> 29:30.020]  легко.
[29:30.020 --> 29:33.780]  Это просто функция, которая получает на вход два аргумента
[29:33.780 --> 29:37.460]  и возвращает возможные три значения.
[29:37.460 --> 29:40.580]  Один, если один аргумент больше другого, минус один
[29:40.580 --> 29:43.020]  в обратном случае и ноль, если равный.
[29:43.020 --> 29:45.020]  Вот такую штуку вам надо реализовать, обернуть
[29:45.020 --> 29:47.780]  в класс и добавить в ходуб, как здесь на слайде написано.
[29:47.780 --> 29:59.020]  В случае с K-field based компаратор, вам на семинаре покажут,
[29:59.020 --> 30:01.380]  вам нужно будет задать количество полей, с которыми
[30:01.380 --> 30:05.740]  вы работаете и набор ключиков, как вы эти поля будете
[30:05.740 --> 30:06.740]  сортировать.
[30:06.740 --> 30:11.660]  Кто пользовался командой sort в линуксе?
[30:11.660 --> 30:17.740]  Да, в шеле.
[30:17.740 --> 30:25.460]  Когда будет возможность, зайдите в мануал команды
[30:25.460 --> 30:26.460]  sort.
[30:26.460 --> 30:28.340]  То, что там написано, все эти ключики, их можно будет
[30:28.340 --> 30:38.700]  использовать и в компараторе.
[30:38.700 --> 30:41.700]  Дальше мы с вами разобрали комбайнер, разобрали компаратор.
[30:41.700 --> 30:44.620]  Есть еще одна вещь, на которую мы смотрели, как на черный
[30:44.620 --> 30:45.620]  ящик.
[30:45.620 --> 30:56.380]  Имейте в виду partitioner, это тот самый процесс, который
[30:56.380 --> 30:59.780]  отвечает за разбиение данных по редьюсерам, то есть он
[30:59.780 --> 31:02.860]  решает вот эта конкретная пара ключ значения, на
[31:02.860 --> 31:04.060]  какой она редьюсер пойдет.
[31:04.060 --> 31:09.940]  По умолчанию, это решается вот так, hash от K по модуле
[31:09.940 --> 31:10.940]  R.
[31:10.940 --> 31:12.380]  Кому R мы задаем руками.
[31:12.380 --> 31:19.700]  Еще в прошлый раз я вам показывал, что мы можем
[31:19.700 --> 31:23.380]  задать количество редьюсеров руками, а количество мапперов
[31:23.380 --> 31:26.940]  напрямую не задается, оно зависит от размера сприта.
[31:26.940 --> 31:28.820]  Поэтому вот это мы знаем.
[31:28.820 --> 31:30.940]  Ну и опять же иногда нам этого не хватает.
[31:30.940 --> 31:39.500]  Например, в задаче может быть такое, что какие-то
[31:39.500 --> 31:42.380]  ключи, хотя они разные, но они должны обязательно
[31:42.380 --> 31:44.900]  попасть на один редьюсер, потому что мы хотим какую-то
[31:44.900 --> 31:47.620]  общую статистику посчитать по ним.
[31:47.620 --> 31:52.660]  Тогда мы это прописываем в условии вот здесь, в partitioner.
[31:52.660 --> 31:55.580]  То есть для того, чтобы сделать свой partitioner, нам нужно
[31:55.580 --> 31:57.300]  опять-таки писать на джаве.
[31:57.300 --> 32:00.300]  Вот в случае с комбайнером, кстати, нам можно не писать
[32:00.300 --> 32:01.300]  на джаве.
[32:01.300 --> 32:04.340]  Маппер, комбайнер и редьюсер можно реализовать на питоне.
[32:04.340 --> 32:07.020]  Компаратор уже на питоне реализовать не получится,
[32:07.020 --> 32:08.760]  partitioner тоже.
[32:08.760 --> 32:11.520]  Нам придется все-таки взять джаву, взять класс partitioner
[32:11.520 --> 32:14.120]  и реализовать там вот эту вот функцию, она тоже довольно
[32:14.120 --> 32:15.120]  простая.
[32:15.120 --> 32:18.560]  То есть получаем на вход пару ключ значения, константу
[32:18.560 --> 32:24.280]  редьюсеров подаем сюда, сколько их штук и получаем
[32:24.280 --> 32:25.280]  int.
[32:25.280 --> 32:33.320]  Вот этот int – это номер редьюсера от 0 до r.
[32:33.320 --> 32:37.040]  Про свой partitioner мы прямо сегодня поговорим на ближайшем
[32:37.040 --> 32:38.040]  примере.
[32:38.040 --> 32:40.240]  Если это сейчас непонятно, то не страшно.
[32:40.240 --> 32:46.440]  Вопросы по комбайнерам-компараторам появились какие-нибудь?
[32:46.440 --> 32:58.040]  Окей, тогда давайте разберем такую задачку.
[32:58.040 --> 33:05.200]  Вот, называется она обратный индекс, то есть это в таком
[33:05.200 --> 33:09.240]  даже не первом, а в нулевом приближении то, что делают
[33:09.240 --> 33:10.240]  поисковые системы.
[33:10.240 --> 33:14.000]  Там, конечно, все сильно сложнее, но вот в таком
[33:14.000 --> 33:19.520]  самом упрощенном варианте у нас есть какие-то документы,
[33:19.520 --> 33:21.600]  какие-то веб-страницы, которые представлены вот в таком
[33:21.600 --> 33:22.600]  виде.
[33:22.600 --> 33:24.800]  ID-шник и содержимое.
[33:24.800 --> 33:28.960]  Есть какой-то терм, то есть шаблон, по которому мы
[33:28.960 --> 33:32.400]  ищем, и нам надо сделать вот такой вот output, то есть
[33:32.400 --> 33:37.400]  шаблон и набор ID-шников, которые этот шаблон содержит.
[33:37.400 --> 33:40.000]  Давайте подумаем, как это делается на MapReduce.
[34:02.400 --> 34:14.680]  Да, контент – это слова какие-то, набор, да.
[34:14.680 --> 34:15.680]  Зачем так сложно?
[34:15.680 --> 34:18.560]  Ну, можно и так, но это как-то сложно.
[34:18.560 --> 34:25.600]  То есть ты хочешь достать массивы ID-шников, посчитать
[34:25.600 --> 34:30.200]  каждое слово, в скольких документах оно встречается,
[34:30.200 --> 34:33.040]  а потом понять, где терм, ну, как бы можно, но вообще
[34:33.040 --> 34:34.040]  проще.
[35:00.200 --> 35:07.440]  А что мы дальше с этим будем делать?
[35:07.440 --> 35:11.280]  Надо дальше потом проверить, где у нас встретился терм.
[35:11.280 --> 35:15.200]  А если ты отделил слово, то есть если ты разбил контент
[35:15.200 --> 35:17.920]  по частям, как ты будешь проверять?
[35:17.920 --> 35:22.600]  Вот у тебя здесь терм встретился, а здесь не встретился, ID-шник
[35:22.600 --> 35:25.960]  один, что с этим делать?
[35:25.960 --> 35:30.120]  Да, докрутить сюда.
[35:30.120 --> 35:47.160]  Может, ну, ты их только в тюбл объедини, не может.
[36:17.160 --> 36:33.000]  А что еще раз будет в значении?
[36:33.000 --> 36:34.960]  Слово в контенте, терм или нет.
[36:34.960 --> 36:39.800]  А что нам мешает взять контент весь и проверить, в нем терм
[36:39.800 --> 36:43.280]  встречается или нет?
[36:44.280 --> 36:47.920]  То есть вот это вот разбиение его можно не делать, потому
[36:47.920 --> 36:50.880]  что мы в итоге все равно принимаем решение по всему
[36:50.880 --> 36:51.880]  контенту.
[36:51.880 --> 36:53.880]  Или там есть терм, или там нет терма.
[36:53.880 --> 36:56.280]  Но это получается такая же штука, как грэп, только
[36:56.280 --> 36:59.080]  нам потом по-другому нужно данные выдать.
[36:59.080 --> 37:01.800]  Если в грэпе мы это все в столбе выдавали, то здесь
[37:01.800 --> 37:03.640]  надо это все еще в строку объединить.
[37:03.640 --> 37:10.760]  То есть на маппере мы сделали, по сути, грэп, один в один,
[37:10.760 --> 37:12.360]  но правда еще убрали контент.
[37:12.360 --> 37:14.680]  То есть как только мы проверили, есть ли терм в контенте
[37:14.680 --> 37:16.080]  или нет, контента нам больше не нужен.
[37:16.080 --> 37:22.720]  Ну а потом нам остается это все скомпоновать, то есть
[37:22.720 --> 37:26.440]  терм и повытаскивать ID-шники с результатов.
[37:26.440 --> 37:32.960]  Это был такой самый простой вариант даже не обратного
[37:32.960 --> 37:36.520]  индекса, такого скорее поиска.
[37:36.520 --> 37:40.280]  Давайте теперь усложним и заодно посмотрим на тот
[37:40.280 --> 37:42.280]  пример, где нужно писать свой партишнер.
[37:42.280 --> 37:48.480]  Вот теперь мы должны вот эту выдачу из кучи ID-шников
[37:48.480 --> 37:49.720]  как-то проранжировать.
[37:49.720 --> 37:53.040]  Ну опять же, то же, что делают наши поисковые системы.
[37:53.040 --> 37:58.640]  Мы что-то загуглили, у нас выдалось там 10 тысяч,
[37:58.640 --> 38:02.840]  10 тысяч элементов, но в топе мы обычно находим что-то
[38:02.840 --> 38:06.200]  более релевантное, а дальше уже менее релевантное.
[38:06.200 --> 38:08.680]  Как это отранжировать?
[38:08.680 --> 38:11.760]  Ну опять же, самый простой вариант в реальной жизни
[38:11.760 --> 38:14.200]  делается сильно сложнее, потому что вот эту метрику
[38:14.200 --> 38:17.080]  терм фриквенсии, вы наверное знаете, что ее можно легко
[38:17.080 --> 38:23.240]  обмануть, ну ее часто обманывали, делая страницы, где просто
[38:23.240 --> 38:25.840]  писали какой-то рандомный текст и повторяли ключевые
[38:25.840 --> 38:29.960]  слова 10 тысяч раз, видели наверное такие страницы.
[38:29.960 --> 38:32.960]  Не видели?
[38:32.960 --> 38:36.480]  Ну сейчас может такого и нету, но вот где-то года
[38:36.480 --> 38:39.760]  три назад я встречал, что там например, ты ищешь
[38:39.760 --> 38:44.240]  какую-нибудь книжку по хадупу, допустим, и у тебя выскакивает
[38:44.240 --> 38:48.400]  страница, где будет написано хадуп-хадуп-хадуп, потом
[38:48.400 --> 38:50.680]  какой-то рандомный текст, потом опять хадуп-хадуп
[38:50.680 --> 38:54.280]  10 раз, ты открываешь, и там кроме рекламы ничего,
[38:54.280 --> 38:55.680]  и вот этого вот бреда ничего нету.
[38:55.680 --> 39:10.240]  Помогает уменьшать шум, но не до конца, собственно,
[39:10.240 --> 39:14.000]  чтобы обмануть TF и DF, стали делать странички, которые
[39:14.000 --> 39:18.320]  содержат не только кучу упоминаний ключевого слова,
[39:18.320 --> 39:21.480]  а еще между ними какой-то рандомный текст пишут, и
[39:21.480 --> 39:24.800]  тогда уже эта метрика проще обманывается, ну поэтому
[39:24.800 --> 39:26.560]  используются еще всякие другие метрики.
[39:26.560 --> 39:30.920]  Ну а вот TF, она самая простая, она просто считает, сколько
[39:30.920 --> 39:34.320]  раз наш терм встретился, то есть не просто есть нету,
[39:34.320 --> 39:36.120]  а сколько раз встретился.
[39:36.120 --> 39:39.080]  Давайте эту задачку тоже решать, вот решение здесь
[39:39.080 --> 39:42.680]  написано, что мы делаем.
[39:42.680 --> 39:47.800]  На мапе мы делаем тот же самый грэп, но одновременно
[39:47.800 --> 39:53.280]  с грэпом мы еще делаем и ворд-каунт, то есть вот
[39:53.280 --> 39:56.640]  мы в контенте теперь считаем не просто встретился терм
[39:56.640 --> 39:59.440]  или нет, а сколько раз он встретился, и выводим единички.
[39:59.440 --> 40:05.880]  Дальше на редьюсе мы суммируем, но нам нужно сформировать
[40:05.880 --> 40:14.680]  вот такие вот пары, то есть редьюз у нас выдает вот
[40:14.680 --> 40:19.920]  такие пары, терм docid это ключ, вот терм docid это ключ,
[40:19.920 --> 40:22.400]  задача редьюсера просуммировать единички.
[40:22.520 --> 40:26.360]  Хорошо, но нам надо, чтобы был не терм docid это ключ,
[40:26.360 --> 40:30.200]  а чтобы терм был ключом, и вот тут как раз случай
[40:30.200 --> 40:34.080]  использования своего партишнера, то есть после того, как мы
[40:34.080 --> 40:39.200]  вот эти вот тройки сделали, терм docid и сумма, нам остается
[40:39.200 --> 40:43.280]  их перегруппировать вот в такую запись, для того,
[40:43.280 --> 40:46.120]  чтобы такая запись была правильная, надо чтобы
[40:46.120 --> 40:50.000]  все термы одинаковые, независимо от их docid, попали на один
[40:50.000 --> 40:51.000]  редьюзер.
[40:53.400 --> 40:57.360]  Вот, то есть это более сильное условие, чем обычный партишнер.
[40:57.360 --> 41:03.880]  Обычный партишнер нам бы требовал вот этого, хэш
[41:03.880 --> 41:19.160]  от… вот, обычный партишнер нам бы требовал вот этого,
[41:19.160 --> 41:22.320]  а мы сделаем партишнер, который будет делать вот
[41:22.320 --> 41:26.600]  это, то есть более сильное условие, вот когда нам нужно
[41:26.600 --> 41:29.800]  какое-то более сильное условие сделать для партишнера,
[41:29.800 --> 41:30.800]  мы пишем свой.
[41:38.800 --> 41:41.800]  Есть ли какие-нибудь вопросы сейчас?
[41:49.160 --> 42:04.400]  То, что в обычном апредьюсе нам главное добиться, чтобы
[42:04.400 --> 42:08.920]  записи с одинаковыми парами терм docid попали на один редьюзер,
[42:08.920 --> 42:11.240]  а теперь мы делаем более сильное условие, нам надо
[42:11.240 --> 42:15.840]  чтобы не только терм docid был одинаковый, а чтобы…
[42:15.840 --> 42:18.080]  достаточно того, чтобы терм был одинаковый, то
[42:18.080 --> 42:21.200]  есть значений в одном редьюзере становиться больше.
[42:21.200 --> 42:28.480]  Вот, ну и мы получаем вот такую вещь в итоге.
[42:28.480 --> 42:32.280]  Эта вещь – это длинное значение, то есть ключ
[42:32.280 --> 42:35.320]  он маленький, а значение тут может быть просто огромная
[42:35.320 --> 42:36.320]  строка.
[42:36.320 --> 42:38.680]  И давайте подумаем, как ее хранить, и вернемся опять
[42:38.680 --> 42:41.680]  же к тому, о чем я рассказывал в начале, вот комбайнер,
[42:41.680 --> 42:44.440]  когда он не запускается, когда запись очень длинная.
[42:44.440 --> 42:47.400]  Вот как раз пример очень длинной записи, что вообще
[42:47.400 --> 42:51.160]  можно с ними делать, как Hadoop умеет их хранить.
[42:51.160 --> 42:53.960]  Можно их хранить двумя как бы подходами.
[42:53.960 --> 43:02.720]  Подход Stripes – это когда мы храним вот в таком виде,
[43:02.720 --> 43:06.200]  как мы выявили, то есть есть ключ, есть огромная-огромная
[43:06.200 --> 43:07.280]  строка со значения.
[43:07.280 --> 43:13.560]  И подход Payers, когда мы разбиваем эту большую строку, у нас
[43:13.560 --> 43:17.000]  получается дублирование ключа, но зато значение
[43:17.000 --> 43:18.000]  получается маленькое.
[43:18.000 --> 43:22.160]  То есть мы вот этот вот лист большой разбили на тюплые,
[43:22.160 --> 43:25.320]  и каждый тюплый вводим отдельно.
[43:25.320 --> 43:28.000]  Скажите, какие плюсы и минусы вы видите в каждом подходе?
[43:28.000 --> 43:31.920]  Ну, с точки зрения хранения, с точки зрения того, как
[43:31.920 --> 43:34.720]  оно накружает ресурсы – диски, память, сеть.
[43:34.720 --> 43:42.000]  Что во втором?
[43:42.000 --> 43:47.440]  Да, то есть больше данных.
[43:47.440 --> 43:50.400]  Мы просто нагенерили больше данных, что значит грузим
[43:50.400 --> 43:51.400]  диск больше.
[43:51.400 --> 43:54.880]  А в первом?
[43:54.880 --> 43:55.880]  Да.
[43:55.880 --> 44:01.360]  То есть мы получается меньше передаем данных в один
[44:01.360 --> 44:04.080]  момент, но зато мы суммарно получаем больше данных,
[44:04.080 --> 44:05.080]  то есть больше диск грузим.
[44:05.080 --> 44:09.920]  А в первом случае диск мы грузим меньше, но зато
[44:10.080 --> 44:12.640]  вот эта вот огромная строчка, она должна вся уместиться
[44:12.640 --> 44:13.640]  в память.
[44:13.640 --> 44:17.480]  Потому что у нас, да, есть там всякие в ходупе инструменты,
[44:17.480 --> 44:19.040]  которые делают вот этот буфер.
[44:19.040 --> 44:21.800]  Буфер переполняется, мы можем скинуть на диск, но
[44:21.800 --> 44:25.840]  у нас нет никакой возможности разбить одну запись на кусочки,
[44:25.840 --> 44:27.480]  какая бы она огромная ни была.
[44:27.480 --> 44:30.400]  Одна запись, а если она не помещается в оперативку,
[44:30.400 --> 44:31.880]  то на этом все мы падаем.
[44:31.880 --> 44:46.800]  А что именно непонятно, а то, что, а ты имеешь, что
[44:46.800 --> 44:51.520]  почему здесь нету индексов, да, docid1, ну вообще они тут
[44:51.520 --> 44:52.520]  должны быть.
[44:52.520 --> 44:55.520]  То есть мы просто взяли вот этот лист, понятно,
[44:55.520 --> 44:59.200]  что там тюплы, они отличаются, и мы их просто отдельно
[44:59.200 --> 45:00.200]  записали.
[45:00.200 --> 45:06.480]  То есть в случае спэш, да, больше нагрузка на диски,
[45:06.480 --> 45:09.880]  больше нагрузка на сети размыто, все должны передать,
[45:09.880 --> 45:12.760]  но меньше нагрузка на память, потому что записи маленькие.
[45:12.760 --> 45:16.880]  В случае страйпс, наоборот, меньше нагрузка на диски
[45:16.880 --> 45:20.640]  сеть мы передаем реже, но уже если передаем, то передаем,
[45:20.640 --> 45:22.560]  и памяти может тупо не хватить.
[45:22.560 --> 45:31.760]  Вот, а теперь переходим к самому страшному, что есть
[45:31.760 --> 45:37.280]  в MapReduce, это джойны.
[45:37.280 --> 45:39.800]  Напомню, что джойны бывают вот такие, ну на самом деле
[45:39.800 --> 45:44.360]  их там больше видов, но самые базовые, вот их четыре,
[45:44.360 --> 45:47.160]  inner, full, left, right.
[45:47.160 --> 45:49.840]  Скажите вообще, как вы думаете, какая самая большая боль
[45:49.840 --> 46:05.520]  при реализации джойна в MapReduce?
[46:05.520 --> 46:07.920]  Вот вы до этого какие-то задачки на семинарах разбирали
[46:07.920 --> 46:13.800]  по MapReduce, что-то получалось, а вот джойн, чем он отличается
[46:13.800 --> 46:30.240]  от всего того, что мы с вами видели раньше?
[46:30.240 --> 46:41.160]  Почему inner, а остальных чего там, что отличается?
[46:41.160 --> 46:45.320]  То есть узнать, откуда именно пришли данные, с какой таблички,
[46:45.320 --> 47:06.920]  с первой или со второй, да?
[47:06.920 --> 47:11.080]  То есть да, у нас просто есть две таблицы, две разных
[47:11.080 --> 47:15.840]  структуры, а наш MapReduce, с которым мы до этого работали,
[47:15.840 --> 47:20.680]  у него есть один input, то есть минус input равно и вот папка,
[47:20.680 --> 47:23.760]  и маппер тоже один, ну да, их там много процессов,
[47:23.760 --> 47:27.080]  много копий, но сама реализация маппера, сам код, он один
[47:27.080 --> 47:28.080]  и тот же.
[47:28.080 --> 47:31.560]  Получается, что нам нужно написать такой маппер, который
[47:31.560 --> 47:35.440]  умеет работать и с первой таблицей, и со второй таблицей,
[47:35.440 --> 47:38.680]  и еще он умеет понимать, откуда данные пришли.
[47:38.680 --> 47:48.880]  Сейчас мы это будем с вами делать, потому что опять
[47:48.880 --> 47:53.000]  же с точки зрения ходу по ходу из коробки не поймет,
[47:53.000 --> 47:56.480]  что есть таблица A, есть таблица B, он просто понимает, что
[47:56.480 --> 47:59.080]  вот ему приходят какие-то пары, иногда они приходят
[47:59.080 --> 48:00.080]  разные.
[48:00.080 --> 48:06.400]  Поэтому логику джойна, саму как бы реализацию джойна
[48:06.400 --> 48:14.360]  мы вынесем в Reduce, а в маппе мы просто сделаем предобработку
[48:14.360 --> 48:17.240]  и выясним, какие данные откуда пришли.
[48:17.240 --> 48:22.720]  То есть когда у нас есть разные строчки, на маппе
[48:22.720 --> 48:27.760]  мы просто их парсим по какому-то шаблону и ставим
[48:27.760 --> 48:28.760]  какой-нибудь тег.
[48:28.760 --> 48:38.120]  То есть вот теги A и B, если таблица A, если таблица
[48:38.120 --> 48:41.680]  B.
[48:41.680 --> 48:44.800]  Потом мы делаем сортировку, группировку и partitioner только
[48:44.800 --> 48:49.400]  по ключу, без тегов, и получается, что данные с одинаковыми
[48:49.400 --> 48:52.440]  ключами и для таблицы A, и для таблицы B придут в
[48:52.440 --> 48:53.440]  один Reducer.
[48:53.440 --> 48:56.360]  И уже тут мы сможем спокойно сделать джойн, потому что
[48:57.080 --> 49:00.560]  в рамках одного Reducer имеем одинаковые ключи и для A,
[49:00.560 --> 49:02.680]  и для B, нам просто остается их объединить.
[49:02.680 --> 49:10.760]  Давайте это разберем на примеры.
[49:10.760 --> 49:14.000]  Сейчас я зайду на наш GitLab.
[49:26.360 --> 49:32.200]  Итак, я сейчас зайду на наш Github, и я хочу, чтобы вы
[49:32.200 --> 49:35.400]  видели, как мы делаем это, как мы делаем это, как мы
[49:35.400 --> 49:37.960]  делаем это, как мы делаем это, как мы делаем это, как
[49:37.960 --> 49:40.360]  мы делаем это, как мы делаем это, как мы делаем это,
[49:40.360 --> 49:42.560]  как мы делаем это, как мы делаем это, как мы делаем
[49:42.560 --> 49:44.560]  это, как мы делаем это, как мы делаем это, как мы
[49:44.560 --> 49:46.560]  делаем это, как мы делаем это, как мы делаем это,
[49:46.560 --> 49:48.560]  как мы делаем это, как мы делаем это, как мы делаем
[49:48.560 --> 49:50.560]  это, как мы делаем это, как мы делаем это, как мы
[49:50.560 --> 49:52.560]  делаем это, как мы делаем это, как мы делаем это, как
[49:52.560 --> 49:54.560]  мы делаем это, как мы делаем это.
[49:54.560 --> 50:19.560]  Вот видно сейчас, что на экране написано или увеличить?
[50:19.560 --> 50:21.560]  Увеличить, да?
[50:21.560 --> 50:29.560]  Так, это мы уберем.
[50:51.560 --> 50:59.560]  Вот давайте подумаем, как решать такую задачу.
[50:59.560 --> 51:03.920]  То есть нам надо проанализировать сайт Stack Overflow и построить
[51:03.920 --> 51:08.400]  гистограмму количества вопросов, ответов в зависимости
[51:08.400 --> 51:09.560]  от возраста пользователя.
[51:09.560 --> 51:12.400]  Но беда в том, что возраст пользователя у нас в одной
[51:12.400 --> 51:16.120]  таблице, вот в этой маленькой users, а данные про посты
[51:16.120 --> 51:18.120]  для этих пользователей в другой таблице, и нам
[51:18.120 --> 51:22.120]  надо обязательно сделать join.
[51:22.120 --> 51:30.120]  Вот давайте разбираться, как мы его будем делать.
[51:30.120 --> 51:32.120]  На мапе что мы будем делать?
[51:32.120 --> 51:37.120]  Ну первое, мы всегда чистим те данные, которые нам не нужны.
[51:37.120 --> 51:40.120]  То есть сначала подумаем, а что нам вообще нужно?
[51:40.120 --> 51:54.120]  Нам надо оставить PostTypeId, и в принципе все, то есть
[51:54.120 --> 52:20.120]  ownerUserId и PostTypeId.
[52:20.120 --> 52:21.120]  Что здесь будет?
[52:21.120 --> 52:45.120]  А здесь будет IDH.
[52:45.120 --> 52:54.120]  Вот, что делаем дальше на Reducer?
[52:54.120 --> 52:57.120]  Тут решение есть, но оно с тегами, то есть мы с вами
[52:57.120 --> 53:10.120]  можем придумать даже без тегов.
[53:10.120 --> 53:21.120]  А здесь у нас нету IDH, что делать?
[53:21.120 --> 53:31.120]  Заметь IDH, то есть мы это сделаем в Reducer.
[53:31.120 --> 53:38.120]  А тогда как мы будем считать количество?
[53:38.120 --> 53:41.120]  То есть нам придется как-то отдельно суммировать двойки,
[53:41.120 --> 53:46.120]  отдельно суммировать единички, как-то это может проще сделать.
[54:08.120 --> 54:15.120]  Давайте сделаем join, то есть у нас будет из двух таблиц
[54:15.120 --> 54:17.120]  по две колонки, у нас будет одна таблица из нескольких
[54:17.120 --> 54:24.120]  колонок.
[54:24.120 --> 54:26.120]  Здесь, заметьте, мы теги не делаем.
[54:26.120 --> 54:27.120]  Почему?
[54:27.120 --> 54:31.120]  Потому что мы можем с помощью каких-нибудь проверок
[54:31.120 --> 54:34.120]  отличить первую строку от второй строки.
[54:34.120 --> 54:36.120]  Как это отличить?
[54:36.120 --> 54:42.120]  Дело в том, что пост typeID бывает 1 или 2, а 2-летние дети
[54:42.120 --> 55:02.120]  вряд ли сидят на строковерфлоу, h будет обычно больше.
[55:02.120 --> 55:06.120]  Поэтому у нас будет такая строчка.
[55:06.120 --> 55:22.120]  Пост typeID, он у нас тут или 1 или 2, и h.
[55:22.120 --> 55:40.120]  Что нам осталось делать?
[55:40.120 --> 55:42.120]  А сможем ли мы это посчитать в редьюсере?
[55:42.120 --> 55:58.120]  Или надо что-то еще сделать?
[55:58.120 --> 56:01.120]  Нам вообще нужен ownerUserID после того, как мы это склопнули
[56:01.120 --> 56:11.120]  уже?
[56:11.120 --> 56:17.120]  Да, по h агрегировать и здесь можно сразу вот эту структуру
[56:17.120 --> 56:18.120]  немного изменить.
[56:18.120 --> 56:21.120]  То есть, пост typeID 1 или 2, он нам не очень удобный.
[56:21.120 --> 56:31.120]  Это можно сделать даже на мапере, будет еще быстрее.
[56:31.120 --> 56:38.120]  То есть, вот здесь пост typeID мы превратим в такую структуру.
[56:38.120 --> 56:42.120]  У нас будет булевское значение здесь и булевское значение
[56:42.120 --> 56:45.120]  здесь.
[56:45.120 --> 56:49.120]  Точнее, наоборот, 1, 0.
[56:49.120 --> 56:52.120]  То есть, пост typeID, который 1 или 2, мы его превратили
[56:52.120 --> 56:54.120]  в два флага.
[56:54.120 --> 56:59.120]  И если у нас это вопрос, то тут будет, допустим, 0,
[56:59.120 --> 57:00.120]  а здесь будет 1.
[57:00.120 --> 57:13.120]  Если ответ, то наоборот, тут будет 1, тут будет 0.
[57:13.120 --> 57:18.120]  Вот, а тут поэтому у нас получится такая структура.
[57:18.120 --> 57:21.120]  Опять же, 0, 1.
[57:21.120 --> 57:24.120]  1, 0.
[57:24.120 --> 57:27.120]  А вот что делать, если к нам пришла строка с age?
[57:27.120 --> 57:29.120]  То есть, id age.
[57:29.120 --> 57:32.120]  Мы же ее не можем просто выкинуть, когда у нас нет ни
[57:32.120 --> 57:37.120]  вопроса, ни ответа, есть id age.
[57:37.120 --> 57:41.120]  Что тогда мы здесь сделаем?
[57:42.120 --> 57:45.120]  0, 0.
[57:53.120 --> 57:58.120]  То есть, тогда у нас получится тут или 1 или 0, тут или 1
[57:58.120 --> 58:02.120]  или 0, а тут или age или, допустим, тоже 0.
[58:02.120 --> 58:05.120]  То есть, если мы имеем дело со строчками вот такими
[58:05.120 --> 58:09.120]  и мы хотим дописать сюда age, то будет age 0.
[58:10.120 --> 58:14.120]  То есть наша задача с точки вот этого вида и вот этого
[58:14.120 --> 58:17.120]  вида превратить в одну структуру.
[58:19.120 --> 58:21.120]  А дальше мы суммируем.
[58:21.120 --> 58:25.120]  Прямо здесь же на этом редьюсере мы можем просуммировать
[58:25.120 --> 58:28.120]  для owner user id.
[58:35.120 --> 58:37.120]  Вот так.
[58:39.120 --> 58:54.120]  Вот до этого момента пока понятно или есть вопросы
[58:54.120 --> 58:56.120]  и ничего не понятно?
[59:00.120 --> 59:03.120]  А потому что как ты будешь суммировать отдельно двойки,
[59:03.120 --> 59:04.120]  отдельно единички?
[59:04.120 --> 59:07.120]  Когда у тебя есть одна колонка, там написано 1, 1, 1, 1,
[59:07.120 --> 59:09.120]  потом 2, 2, 2, потом опять 1, 1.
[59:09.120 --> 59:11.120]  Как ты будешь это считать?
[59:11.120 --> 59:15.120]  Тебе все равно придется разносить это в два разных поля.
[59:15.120 --> 59:17.120]  То есть отдельно считать количество двоек, отдельно
[59:17.120 --> 59:19.120]  считать количество единиц.
[59:28.120 --> 59:30.120]  А сделали ли мы сейчас все сразу?
[59:30.120 --> 59:32.120]  Можем ли сказать, что это ответ?
[59:38.120 --> 59:42.120]  Посмотрите еще раз, что спрашивается в условии.
[59:42.120 --> 59:44.120]  Вроде, а, тут не видно.
[59:44.120 --> 59:46.120]  Сейчас я поверну тогда выше.
[01:00:03.120 --> 01:00:05.120]  Чего тут еще не хватает?
[01:00:05.120 --> 01:00:06.120]  Структура похожая.
[01:00:06.120 --> 01:00:08.120]  Мы можем выкинуть owner user id,
[01:00:08.120 --> 01:00:10.120]  но нам все равно уже не надо,
[01:00:10.120 --> 01:00:13.120]  и у нас останется три колонки, как мы и хотели.
[01:00:13.120 --> 01:00:15.120]  Но все ли это или не все?
[01:00:18.120 --> 01:00:20.120]  Ну а до сортировки?
[01:00:21.120 --> 01:00:23.120]  Мы просуммировали, по какому ключу?
[01:00:23.120 --> 01:00:26.120]  То есть какой у нас группой был сейчас?
[01:00:32.120 --> 01:00:34.120]  По ключу owner user id.
[01:00:34.120 --> 01:00:39.120]  Мы же по нему сгруппировали на стадии sort.
[01:00:39.120 --> 01:00:41.120]  И по нему сджойнили.
[01:00:41.120 --> 01:00:45.120]  То есть на редюсер у нас пришли пары с одинаковыми
[01:00:45.120 --> 01:00:47.120]  owner user id.
[01:00:47.120 --> 01:00:50.120]  Поэтому мы просуммировали вопросы и ответы
[01:00:50.120 --> 01:00:54.120]  не для возраста, а для одного пользователя.
[01:00:54.120 --> 01:00:58.120]  То есть это количество вопросов и ответов у одного пользователя.
[01:00:58.120 --> 01:01:00.120]  Поэтому что нам еще надо сделать?
[01:01:04.120 --> 01:01:06.120]  Просуммировать по возрасту.
[01:01:06.120 --> 01:01:08.120]  А что это будет?
[01:01:08.120 --> 01:01:10.120]  Может я неправильно понял,
[01:01:10.120 --> 01:01:12.120]  но просуммировать возраст.
[01:01:12.120 --> 01:01:14.120]  Это будет явно что-то не то.
[01:01:16.120 --> 01:01:18.120]  Сгруппировать по возрасту.
[01:01:22.120 --> 01:01:24.120]  То есть нам нужно былоığı пересчитывать.
[01:01:24.120 --> 01:01:26.120]  Дело не было.
[01:01:26.120 --> 01:01:30.120]  И если мы мы лечим вот это вот другое количество вопросов,
[01:01:30.120 --> 01:01:32.120]  то мы и не сможем их сгруппировать.
[01:01:32.120 --> 01:01:36.120]  То есть нам нужна еще одна джоба.
[01:01:36.120 --> 01:01:42.120]  Опять map. На мапе мы оставляем h.
[01:01:42.120 --> 01:01:46.120]  Вот эта сумма, ну давайте ее запишем быстрее.
[01:01:46.120 --> 01:01:50.120]  Some answer, some question.
[01:01:50.120 --> 01:01:54.120]  Вот такая вот у нас тройка.
[01:01:54.120 --> 01:01:58.120]  Но в этих тройках у нас h пока еще может повторяться.
[01:02:02.120 --> 01:02:06.120]  Поэтому на reducer мы делаем h.
[01:02:06.120 --> 01:02:10.120]  Уже по h мы группируем.
[01:02:10.120 --> 01:02:14.120]  Делаем sum от some answer и sum от some question.
[01:02:14.120 --> 01:02:18.120]  Вот такая у нас штука получилась.
[01:02:26.120 --> 01:02:30.120]  Но это только один из способов джойна.
[01:02:30.120 --> 01:02:34.120]  Это такой самый лобовой джойн, который сложный работает.
[01:02:34.120 --> 01:02:38.120]  Он конечно всегда, но сложно.
[01:02:38.120 --> 01:02:42.120]  Есть вот такой джойн. Он называется map.join.
[01:02:42.120 --> 01:02:48.120]  Когда одна из табличек маленькая, мы можем ее зачитать в distributed cache.
[01:02:48.120 --> 01:02:52.120]  Что такое distributed cache, вам расскажут подробнее на семинарах.
[01:02:52.120 --> 01:02:56.120]  То есть это такая область на нодах, которая не относится к HDFS.
[01:02:56.120 --> 01:03:00.120]  Поэтому в нее быстрее писать, с нее быстрее читать.
[01:03:00.120 --> 01:03:04.120]  И при этом она выделяется на всех нодах, где запускается задача.
[01:03:04.120 --> 01:03:06.120]  То есть запустили задачу.
[01:03:06.120 --> 01:03:10.120]  Если для нее нам нужен distributed cache, то на всех нодах,
[01:03:10.120 --> 01:03:14.120]  где будут работать ее мапперы, reducers и комбайнеры,
[01:03:14.120 --> 01:03:18.120]  везде будет создана вот эта папочка, и там будет лежать cache.
[01:03:18.120 --> 01:03:22.120]  Обычно в этот cache кладут какой-то маленький файл.
[01:03:22.120 --> 01:03:26.120]  И вот у нас есть возможность разбить только одну таблицу.
[01:03:26.120 --> 01:03:30.120]  У нас будет джойн, кусочка большой таблицы со всей маленькой таблицей.
[01:03:30.120 --> 01:03:34.120]  То есть по сути мы делаем параллельно маленькие джойны,
[01:03:34.120 --> 01:03:38.120]  которые потом остается только объединить с помощью уже обычного юниона
[01:03:38.120 --> 01:03:40.120]  и получить таблицу.
[01:03:40.120 --> 01:03:44.120]  Вопрос всегда ли будет этот джойн работать.
[01:03:44.120 --> 01:03:48.120]  Вот если мы возьмем четыре джойна, которые были вот тут,
[01:03:48.120 --> 01:03:50.120]  двумя слайдами раньше,
[01:03:50.120 --> 01:03:54.120]  для всех вот этих случаев нам джойн будет работать.
[01:04:04.120 --> 01:04:08.120]  Если кто-то вдруг делал наперед домашку по хайву и читал документацию,
[01:04:08.120 --> 01:04:12.120]  там прям явно документации по хайву написан ответ на вопрос.
[01:04:12.120 --> 01:04:14.120]  Не все, иначе бы не спрашивал.
[01:04:42.120 --> 01:04:46.120]  Давайте вы просто проверьте каждый из этих джойнов,
[01:04:46.120 --> 01:04:48.120]  где будут какие-нибудь проблемы.
[01:04:50.120 --> 01:04:52.120]  А почему с Индер будет проблема?
[01:05:12.120 --> 01:05:16.120]  Ну и что, если у нас что-то не совпадает,
[01:05:16.120 --> 01:05:18.120]  то у нас будет где-то нал.
[01:05:18.120 --> 01:05:22.120]  В случае Энерджойна мы вот эти наловые пары отбрасываем.
[01:05:22.120 --> 01:05:28.120]  Но у нас же эта же таблица скопирована и на втором кусочке, и на третьем кусочке.
[01:05:28.120 --> 01:05:36.120]  То есть те пары, которые вот здесь вот тут вот не поместились,
[01:05:36.120 --> 01:05:40.120]  отбросились, они обязательно будут здесь или здесь.
[01:05:40.120 --> 01:05:44.120]  То есть Индерджойн тут как раз отработает,
[01:05:44.120 --> 01:05:54.120]  потому что все, где нет соответствия, мы выбрасываем.
[01:05:54.120 --> 01:05:56.120]  А почему Фонджейн не будет работать?
[01:06:24.120 --> 01:06:32.120]  А можно более конкретно, что именно мы выбрасываем?
[01:06:32.120 --> 01:06:36.120]  То есть да, то, что тут будет дублирование, это правильно.
[01:06:36.120 --> 01:06:38.120]  А откуда оно возьмется?
[01:06:42.120 --> 01:06:44.120]  А что ты имел в виду?
[01:06:54.120 --> 01:06:56.120]  Про Фонджейна сейчас говорим.
[01:07:24.120 --> 01:07:46.120]  Так, но ведь кусочки 1 и 2 они же друг друга не дублируют.
[01:07:46.120 --> 01:07:54.120]  То есть синие кусочки это одна таблица.
[01:07:54.120 --> 01:07:58.120]  Мы просто ее разбили на куски, но она от этого не перестала быть одной таблицей.
[01:07:58.120 --> 01:08:02.120]  Дублирование будет немного не там.
[01:08:02.120 --> 01:08:06.120]  Вот давайте представим, например, Left Join.
[01:08:06.120 --> 01:08:13.120]  Вот как раз типичный, вообще самый часто используемый джойн в дата инжиниринге,
[01:08:13.120 --> 01:08:19.120]  это именно Left Join, потому что мы берем какую-то таблицу и джоиним ее с каким-то маленьким справочником.
[01:08:19.120 --> 01:08:24.120]  Когда у нас есть большой какой-то датасет,
[01:08:24.120 --> 01:08:27.120]  а нам нужно выдать отчет по какому-то фильтру.
[01:08:27.120 --> 01:08:31.120]  По фильтру времени, по фильтру компании, еще какому-то признаку.
[01:08:31.120 --> 01:08:33.120]  Поэтому Left Join.
[01:08:33.120 --> 01:08:35.120]  И как раз рассмотрим Left Join здесь.
[01:08:35.120 --> 01:08:37.120]  Что у нас будет происходить?
[01:08:37.120 --> 01:08:40.120]  Давайте даже на примере вам покажу.
[01:08:41.120 --> 01:08:44.120]  На таком простом примере просто таблица с числами.
[01:08:52.120 --> 01:08:54.120]  Весь у нас таблица.
[01:08:54.120 --> 01:08:56.120]  Это у нас таблица B такая.
[01:08:56.120 --> 01:08:58.120]  То есть она большая.
[01:08:58.120 --> 01:09:01.120]  У нее два кусочка, три не будем рисовать.
[01:09:01.120 --> 01:09:05.120]  И таблица, ну точнее это таблица A.
[01:09:05.120 --> 01:09:07.120]  У нее два кусочка.
[01:09:07.120 --> 01:09:09.120]  Таблица B.
[01:09:13.120 --> 01:09:15.120]  И таблица A.
[01:09:15.120 --> 01:09:17.120]  У нее два кусочка.
[01:09:17.120 --> 01:09:19.120]  Таблица B.
[01:09:19.120 --> 01:09:21.120]  Таблица B.
[01:09:29.120 --> 01:09:31.120]  Вот, например, вот так.
[01:09:44.120 --> 01:09:46.120]  Вот так можно сделать.
[01:09:49.120 --> 01:09:51.120]  Сейчас я подберу числа, чтобы было понятнее.
[01:10:19.120 --> 01:10:21.120]  Вот так можно сделать.
[01:10:21.120 --> 01:10:23.120]  У нас будет дублироваться Left Join и Full Join.
[01:10:23.120 --> 01:10:25.120]  У нас будет дублироваться Left Join и Full Join.
[01:10:25.120 --> 01:10:27.120]  У нас будет дублироваться Left Join и Full Join.
[01:10:27.120 --> 01:10:29.120]  У нас будет дублироваться Left Join и Full Join.
[01:10:29.120 --> 01:10:31.120]  У нас будет дублироваться Left Join и Full Join.
[01:10:31.120 --> 01:10:33.120]  У нас будет дублироваться Left Join и Full Join.
[01:10:33.120 --> 01:10:35.120]  У нас будет дублироваться Left Join и Full Join.
[01:10:35.120 --> 01:10:37.120]  У нас будет дублироваться Left Join и Full Join.
[01:10:45.120 --> 01:10:47.120]  Иường на ^^
[01:10:49.120 --> 01:10:51.120]  У нас будет дублироваться Left Join и Full Join.
[01:10:51.120 --> 01:10:53.120]  У нас будет дублироваться Left Join и Full Join.
[01:10:53.120 --> 01:10:55.120]  У нас будет дублироваться Left Join и Full Join.
[01:10:55.120 --> 01:11:01.440]  Сейчас проверю точно.
[01:11:25.120 --> 01:11:48.120]  Вот даже здесь мы видим дублирование, что у нас дублировались вот эти вот строчки.
[01:11:48.120 --> 01:11:54.160]  Ещё бывает такое, что дублируются нулы. Вот это даже бывает чаще, что в первом кусочке есть
[01:11:54.160 --> 01:11:59.160]  нал, а во втором как раз мы находим соответствие.
[01:12:12.160 --> 01:12:17.080]  Дублирование хорошо, мы с ним можем справиться, но мы не можем справиться с таким
[01:12:17.080 --> 01:12:22.120]  кейсом, когда у нас одна строчка выдает нал, а другая с таким же
[01:12:22.120 --> 01:12:25.640]  ключом она не выдает нал. Сейчас попробую такой пример сделать.
[01:12:25.640 --> 01:12:55.160]  Здесь, наверное, на этом примере будет проще всего показать на райджойне.
[01:12:55.160 --> 01:13:07.960]  Вот если мы хотим райджойн сделать, смотрите. 1, 1, 1, 4, 1, nal с первым кусочком.
[01:13:07.960 --> 01:13:21.520]  Второй кусочек. 1, 1, 1, 4, 1, 1. То есть в первом случае у нас четвёрка не нашлась,
[01:13:21.520 --> 01:13:27.040]  а во втором случае у нас четвёрка нашлась. А, ну здесь, кстати, я ошибся. Вот так.
[01:13:27.040 --> 01:13:36.880]  То есть смотрите, у нас получаются строчки. Часть ключей встречается с налом,
[01:13:36.880 --> 01:13:42.920]  часть ключей встречается без нала. И чтобы нам завершить джойн, нам надо вот этот весь
[01:13:42.920 --> 01:13:53.320]  результат пройти ещё раз, выкинуть те строчки, которые встречаются с налом, в то время как с этим
[01:13:53.320 --> 01:14:00.480]  же ключом мы встречаемся без нала. То есть вот эти вот 4, 1, nal должны пропасть, 1, 1, nal должна
[01:14:00.480 --> 01:14:06.480]  пропасть. Но это не значит, что все налы нужно удалить, потому что, например, если мы добавим
[01:14:06.480 --> 01:14:18.840]  сюда 5, 1, то 5, 1 нет нигде. Будет 5, 1, nal, и здесь будет 5, 1, nal. И вот этот 5, 1, nal, он должен остаться,
[01:14:18.840 --> 01:14:26.080]  потому что это честный нал, он действительно нал, он нигде не встречается. Вот, а вот от этих, скажем так,
[01:14:26.080 --> 01:14:32.720]  нечестных налов, которые встречаются, но в другом кусочке, мы должны избавиться. А это, ну это редьюсер,
[01:14:32.720 --> 01:14:42.720]  по сути. То есть нам надо вот этот весь датасет сгруппировать ещё раз по ключу, и для каждого
[01:14:42.720 --> 01:14:48.000]  ключа проверить, если там не нал, если там есть не нал, то мы его только и оставляем. Если там есть
[01:14:48.000 --> 01:14:55.520]  только налы, то мы берём один нал, одну запись с налом, все остальные выбрасываем. Поэтому, да, вот в
[01:14:55.520 --> 01:15:00.360]  данном случае, write join не будет работать. Тут зависит от того, как поставить таблицы, но один из
[01:15:00.360 --> 01:15:09.360]  джойнов у нас не работает. И последний кейс джойнов, это такой средний случай между map join
[01:15:09.360 --> 01:15:17.800]  и reduce join, это bucket join. Это когда у нас табличка, вот эта красная, табличка B, она не настолько большая,
[01:15:17.800 --> 01:15:22.920]  чтобы её класть в HDFS, но и не настолько маленькая, чтобы она поместилась в distributed cache. Нам
[01:15:22.920 --> 01:15:31.620]  её приходится разбивать на части, и каждую часть класть в свой distributed cache. Скажите тогда,
[01:15:31.620 --> 01:15:40.560]  какой джойн здесь не будет работать? Вот, учитывая то, что мы сейчас разбирались с дублирующимися вот
[01:15:40.560 --> 01:15:41.120]  этими налами,
[01:15:52.920 --> 01:16:07.120]  а еще, а еще, все никакой джойн не будет работать, поэтому вопрос, зачем нам вообще такая штука нужна?
[01:16:07.120 --> 01:16:15.600]  Такая штука нужна нам для того, если мы имеем возможность заранее вот эти вот кусочки разбить,
[01:16:15.600 --> 01:16:24.160]  то есть не просто как это делает HDFS, разбить по объему, по каким-то частям, а если мы имеем
[01:16:24.160 --> 01:16:29.800]  возможность заранее подготовить данные так, чтобы, например, вот первый кусочек, он соответствовал
[01:16:29.800 --> 01:16:38.240]  интервалу от одного до A, второй кусочек от A до B, третий кусочек там от B до C. Если мы соблюдаем
[01:16:38.240 --> 01:16:46.000]  эти интервалы, то есть они совпадают и в таблице A, и в таблице B, то тогда все будет работать. Ну,
[01:16:46.000 --> 01:16:55.800]  то есть, по сути, нам нужно заранее сделать предобработку. Ну что, мы в принципе закончили,
[01:16:55.800 --> 01:17:03.400]  но можем разобрать еще yarn, чтобы не оставлять время на следующем занятии, у нас будет хайф. Если
[01:17:03.400 --> 01:17:14.560]  у вас есть еще минут семь, можем еще поговорить про yarn. Давайте, то есть что вообще такое yarn?
[01:17:14.560 --> 01:17:19.640]  Это yeti-NAS-ресурс негашейтер, то есть еще один распределитель ресурсов. Собственно,
[01:17:19.640 --> 01:17:26.120]  он распределяет ресурсы, выделяет их на маперы, на редьюсеры, на комбайнеры, на всю эту штуку. Как
[01:17:26.120 --> 01:17:36.200]  он это делает? Он взаимодействует не напрямую с оперативкой, с ядрами процессора, а он оперирует
[01:17:36.200 --> 01:17:43.400]  такой абстракцией, как контейнер. Только это не docker-контейнеры, не виртуальные машины, а контейнер,
[01:17:43.400 --> 01:17:49.800]  он представляет собой какой-то набор выделенных ресурсов, например, два гига оперативки и одно
[01:17:49.800 --> 01:17:56.320]  ядро процессора. На этом контейнере у нас стартует виртуальная машина джавы, стартует процесс
[01:17:56.320 --> 01:18:03.800]  какого-нибудь мапера, и вот в нем мы живем. Вот это самая простая реализация такого планировщика.
[01:18:03.800 --> 01:18:10.440]  Очередь. Пришло приложение, кластер свободен, оно заняло все ресурсы. Пришло второе приложение,
[01:18:10.440 --> 01:18:21.480]  ресурсов нету, оно ждет. Какие плюсы и минусы у такого подхода? Ну плюс понятно, а просто сделать,
[01:18:21.480 --> 01:18:28.320]  то есть обычная очередь. А минус в том, что у разных программ разные приоритеты, разные требования. То есть
[01:18:28.320 --> 01:18:34.320]  может быть application номер два, она супер срочная, да еще и очень маленькая, и она бы быстро отработала,
[01:18:34.320 --> 01:18:42.120]  если бы вот этот ап один жирный не занял бы кучу ресурсов. Поэтому второй тип планировщика,
[01:18:42.120 --> 01:18:48.960]  когда мы на каждое приложение выделяем capacity. Заранее в конфиге прописываем, и получается,
[01:18:48.960 --> 01:18:56.640]  что вот у приложения один capacity 75%. Кластер свободен, допустим, даже ресурсы есть, но он все
[01:18:56.640 --> 01:19:03.760]  равно больше чем 75 никогда не займет. Точно так же, как второе. Вот у него 25. Все, как бы оно не
[01:19:03.760 --> 01:19:09.600]  хотело, оно больше никогда не займет. Какие здесь минусы вы видите у такого планировщика?
[01:19:15.800 --> 01:19:24.040]  Да, ресурс простаивает. То есть такой планировщик, он хорошо подходит для real-time приложений.
[01:19:24.040 --> 01:19:30.560]  Вот когда у вас, мы в принципе в этом курсе почти не будем затрагивать такие приложения,
[01:19:30.560 --> 01:19:37.120]  может быть на семинаре вам расскажут чуть-чуть. Это real-time приложение, когда есть, например,
[01:19:37.120 --> 01:19:43.480]  какой-то сервис на Spark, он слушает порт, и туда постоянно прилетают какие-то данные. В принципе,
[01:19:43.480 --> 01:19:51.240]  с одинаковой периодичностью, с одинаковой нагрузкой, и мы знаем, что в принципе такое приложение
[01:19:51.240 --> 01:19:57.160]  всегда будет требовать примерно одинакового количества ресурсов. Мы на него запланировали
[01:19:57.160 --> 01:20:03.840]  capacity, и оно себе вот как вот на этой желтой части диаграммы, оно будет постоянно эти ресурсы занимать.
[01:20:03.840 --> 01:20:13.800]  Но с другой стороны, даже у таких приложений бывают перепады в том смысле, что вот есть map reduce.
[01:20:13.800 --> 01:20:21.880]  Например, наша программа так написана, что мапперам надо много, потому что много сплитов,
[01:20:21.880 --> 01:20:27.960]  много данных прилетает, но на мапперах мы много чего фильтруем по какому-нибудь критерию,
[01:20:27.960 --> 01:20:34.280]  и к reduce приходит очень мало данных, и у нас остается один reducer, и получается нам надо то очень
[01:20:34.280 --> 01:20:40.440]  много, и мы ждем, то очень мало, и мы простаиваем. Поэтому такой планировщик, он тоже не сильно
[01:20:40.440 --> 01:20:47.760]  подходит, ну иногда не сильно подходит. И третий, самый такой продвинутый, называется честный
[01:20:47.760 --> 01:20:54.720]  планировщик. Но здесь все честно, если ресурсы свободны, то почему бы нам их не дать? Если их
[01:20:54.720 --> 01:21:01.680]  кто-то другой хочет занять, то мы в зависимости от приоритета, вот здесь пополам, application 2
[01:21:01.680 --> 01:21:09.640]  отжирает половину, но в зависимости от приоритета, от настроек, оно может съесть 25%, 10% или 80%.
[01:21:09.640 --> 01:21:25.680]  Что на этой картинке вас смущает? Ну зорчик это то, что приложение начало работать не сразу. А что еще?
[01:21:39.640 --> 01:22:04.200]  Там учитывается два параметра, сколько нужно реально ресурсов, и какой приоритет.
[01:22:04.200 --> 01:22:18.520]  То есть то, каким образом планировщик принимает решение, что нужно отдать 50%, да?
[01:22:18.520 --> 01:22:39.440]  Может, а почему оно должно меняться ступеньками? Ну кроме того, что так написано на следующем слайде,
[01:22:39.440 --> 01:22:57.360]  почему оно еще должно меняться ступеньками? А если ему все время нужно много ресурсов,
[01:22:57.360 --> 01:23:16.000]  тогда вот типа все нормально или какие-то проблемы все-таки есть? В зависимости от приоритета,
[01:23:16.000 --> 01:23:25.160]  если приоритет равный, то пополам, если у кого-то больше, то там уже умножаем на приоритет. Здесь
[01:23:25.160 --> 01:23:31.920]  главная проблема не в том, что сколько ресурсов мы даем, а как быстро мы даем. То есть вот смотрите,
[01:23:31.920 --> 01:23:39.200]  Ярн здесь выдал ресурсы резко, пришло второе приложение, сказала мне 50, вот тебе 50. Что
[01:23:39.200 --> 01:23:47.600]  произошло с вот этими джебами, которые здесь работали, вот в этой части? Их убили. То есть получается,
[01:23:47.600 --> 01:23:54.520]  что вот тут что-то работало, оно доработало до 99,9%, потом их взяли и убили. Если какая-то
[01:23:54.520 --> 01:24:00.480]  польза от того, что здесь вообще работало, пользы никакой нет, потому что раз оно не доработало,
[01:24:00.480 --> 01:24:06.200]  его килинули, значит ходуб его перезапустит еще раз с нуля. Поэтому вот эта вот желтая часть,
[01:24:06.200 --> 01:24:13.400]  она по большей части оказалась бесполезной, такая уж какая-то белая. Поэтому в реальной жизни
[01:24:13.400 --> 01:24:20.840]  ресурсы выделяются постепенно. Вот как вы сказали ступеньками, то есть мы ресурсы не отбираем,
[01:24:20.840 --> 01:24:27.640]  мы их просто не выдаем. Пока приложение работает, оно себе работает. Как только оно начинает,
[01:24:27.640 --> 01:24:35.320]  какой-то маппер завершился, в норме если ресурсы есть, то сразу на его место становится следующий
[01:24:35.320 --> 01:24:44.560]  маппер или редьюсер. А здесь мы не будем выдавать ресурсы, мы просто отберем их в пользу АП2. Иногда
[01:24:44.560 --> 01:24:50.640]  можем с помощью преэппшена отобрать наперед. Даже если приложению 2 сейчас столько не нужно,
[01:24:50.640 --> 01:24:55.120]  мы их отобрали наперед и начинаем их как бы осваивать, занимать их нашими джобами.
[01:24:55.120 --> 01:25:06.320]  Но это еще не все. Все, что осталось сказать, что контейнер это тоже штука достаточно динамичная.
[01:25:06.320 --> 01:25:13.120]  То есть то, что мы взяли, выделили контейнер, два гига оперативки и одно ядро процессора,
[01:25:13.120 --> 01:25:22.040]  такого контейнера может просто не хватить по каким-то причинам. Вот у нас маппер там сильно
[01:25:22.040 --> 01:25:27.680]  разросся. Чаще всего в Spark такое бывает, потому что в Hadoop'е с этим проще. Не хватило памяти,
[01:25:27.680 --> 01:25:35.360]  мы скипклили все на диск. В Spark с этим сложнее, потому что там немного не так построена логика работы
[01:25:35.360 --> 01:25:41.240]  с диском. Там все максимально хранится в памяти. Если памяти не хватает, то часто бывает out of
[01:25:41.240 --> 01:25:56.040]  memory просто. Поэтому контейнеры в Ярне задаются не одной парой значений, а у них три пары. У них
[01:25:56.040 --> 01:26:11.340]  minimum, maximum и step. То есть в начале мы выделили минимальный контейнер. Дальше в Ярне есть две
[01:26:11.340 --> 01:26:18.240]  роли. Есть ресурс-менеджер и нод-менеджер. То есть ресурс-менеджер это глобальная роль,
[01:26:18.240 --> 01:26:25.680]  которая решает сколько контейнеров кому выделить. Нод-менеджер решает сколько контейнеров выделить
[01:26:25.680 --> 01:26:34.480]  в рамках одной ноды и стоит ли растить какой-то контейнер. То есть мы можем, если наша джоба просит
[01:26:34.480 --> 01:26:41.120]  ресурсы, мы можем удовлетворить ее просьбой двумя способами. Дать больше контейнеров или вырастить
[01:26:41.120 --> 01:26:47.800]  те контейнеры, которые есть. А растятся они вот так. То есть есть минимум, например два гига и одно
[01:26:47.800 --> 01:26:55.640]  ядро. И есть step, например там пол гига, но и тоже одно ядро, потому что мы не можем ядра пополам
[01:26:55.640 --> 01:27:03.280]  выдавать. И вот если мы хотим наш контейнер вырастить, мы его начинаем на вот этот step растить,
[01:27:03.280 --> 01:27:08.280]  растить, растить, растить, пока он не достигнет максимума. Если он достиг максимума, это обычно
[01:27:08.280 --> 01:27:15.320]  какое-то там большое значение вроде десяти ядер и тридцати гигов. Если он достигло максимума и
[01:27:15.320 --> 01:27:22.920]  хочет еще, то мы его удаляем. Тоже часто с таким столкнетесь в спарке, когда будете писать код.
[01:27:22.920 --> 01:27:30.000]  Что вот джоба работает, работает, работает, потом написано not enough physical memory. Где это имеется
[01:27:30.000 --> 01:27:36.440]  в виду physical memory? Не на всем же кластере. В одном контейнере не хватило памяти, мы его кельнули и
[01:27:36.440 --> 01:27:42.920]  перезапустили еще раз. И скорее всего повторится та же история, потому что вина в этом не контейнеры,
[01:27:42.920 --> 01:27:51.600]  а вашего кода. И так он будет по циклу перезапускаться несколько раз. Ну и сами контейнеры,
[01:27:51.600 --> 01:27:57.520]  они тоже бывают разные. То есть вот этот минимум, он не для всех контейнеров одинаковый, а, например,
[01:27:57.520 --> 01:28:03.800]  можно указать, что для мапперов у нас будут одни контейнеры, например, один-один, один гига
[01:28:04.280 --> 01:28:16.360]  для редьюсера будет полтора один. Ну и в последних версиях Hadoop он еще умеет работать с видеокартами,
[01:28:16.360 --> 01:28:21.760]  поэтому туда еще добавляется третье значение, еще и видюхами мы управляем.
[01:28:21.760 --> 01:28:30.600]  Ну в принципе, на этом все. В конце, на последнем слайде можете посмотреть
[01:28:30.680 --> 01:28:38.080]  tutorial, как писать Джобы на Hadoop стриминге. Здесь очень подробное объяснение,
[01:28:38.080 --> 01:28:43.360]  несмотря на то, что статья древняя ей 10 лет и там второй питон, но объяснение там
[01:28:43.360 --> 01:28:53.760]  очень подробно написано что и зачем. Вот на этом все, у нас с вами осталось еще одно занятие,
[01:28:54.120 --> 01:29:01.600]  оно будет по хайву, а дальше будет читать роман Леповсту. Контрольное будет,
[01:29:01.600 --> 01:29:06.480]  скорее всего, в следующий раз, но мы еще с семинаристами обсудим или в следующий раз или через раз.
[01:29:06.480 --> 01:29:11.240]  Все, всем спасибо.

[00:00.000 --> 00:11.000]  Вот, про то, насколько эффективно работает. Давайте напишем определение.
[00:11.000 --> 00:19.000]  Пусть у нас есть опять какое-то семейство хэш-функций, я напоминаю, что в качестве хэш-функций h,
[00:19.000 --> 00:25.000]  там h1, h2 в случае двойного хэширования, я использую не абы какую случайную функцию,
[00:25.000 --> 00:28.000]  а из какого-то маленького семейства, который я умею хорошо сэмплировать.
[00:28.000 --> 00:33.000]  То есть это небольшое семейство функций, которые ведут себя почти так же, как настоящие, случайные,
[00:33.000 --> 00:36.000]  но которые я умею быстро, эффективно вычислять.
[00:36.000 --> 00:41.000]  Так вот, пусть у меня есть какое-то семейство хэш-функций, давайте пускать h с индексом s,
[00:41.000 --> 00:49.000]  это действующие, скажем, из zp в zm, индексированные с-ками.
[00:49.000 --> 00:54.000]  Такое семейство хэш-функций называется k-независимым,
[00:54.000 --> 01:06.000]  k-независимое семейство хэш-функций,
[01:06.000 --> 01:15.000]  если при фиксации любых k элементов из zp, то есть из области определения,
[01:15.000 --> 01:21.000]  при фиксации любых k их образов, вероятность того, что эти элементы попали ровно в эти образы,
[01:21.000 --> 01:25.000]  то есть вероятность того, что x1 пришло в y1, x2 в y2 и так далее, xk в yk,
[01:25.000 --> 01:31.000]  вероятность такая, как если бы эти события были независимы.
[01:31.000 --> 01:42.000]  Давайте это запишем, для любых попарно различных x1 и так далее, xk, лежащих в zp,
[01:42.000 --> 01:51.000]  для любых y1 и так далее, yk, лежащих в zm, вероятность по выбору случайного s события,
[01:51.000 --> 02:00.000]  что хэш-функция в точке x1 равна y1, хэш-функция в точке x2 равна y2 и так далее,
[02:00.000 --> 02:10.000]  хэш-функция в точке xk равна yk, равна такой величине, как если бы все вот эти вот события проходили независимо.
[02:10.000 --> 02:15.000]  То есть, грубо говоря, мы а6 от x1 равно y1, это просто один эксперимент.
[02:15.000 --> 02:19.000]  Мы взяли случайную хэш-функцию, померили, с какой вероятностью в такой-то точке это y1,
[02:19.000 --> 02:25.000]  потом взяли новую хэш-функцию, померили вероятность, с которой в точке x2 она равна y2 и так далее.
[02:25.000 --> 02:31.000]  Как если бы все эти события были независимы, какой была бы здесь величина вероятности?
[02:31.000 --> 02:39.000]  Я верю в вас, вы сможете.
[02:39.000 --> 02:46.000]  Если бы вот этого не было, если бы у меня была просто вероятность одного вот такого события,
[02:46.000 --> 02:50.000]  что h в такой-то точке равна такой-то величине, чему должна быть равна эта вероятность,
[02:50.000 --> 02:55.000]  если у нас все равномерно случайно распределено, если все исходы равновероятны?
[02:55.000 --> 03:03.000]  Да, 1 делит на m в нашем случае. Если у меня h6 бьет в множество размеров m, было бы 1 mt.
[03:03.000 --> 03:09.000]  Ну а поскольку теперь у меня k таких событий, я хочу в данных k-точках, чтобы у меня были данные k значений,
[03:09.000 --> 03:16.000]  если оно ведет себя как независимое, то это просто 1 mt умножается сама на себя k раз, получается 1 на m вкатый.
[03:16.000 --> 03:25.000]  Это определение. Ещё раз, смысл такой, что тут написано k-независимое, это значит, что значения в k-точках
[03:25.000 --> 03:30.000]  ведут себя так, как если бы h была настоящая случайно, то есть эти события были бы независимы.
[03:30.000 --> 03:42.000]  Ну и пример, как можно генерировать что-то, скажем так, похожее на k-независимые семейства hash функций,
[03:42.000 --> 03:52.000]  ну давайте скажем следующее, что h от x можно брать следующей штучкой.
[03:52.000 --> 04:00.000]  a0 плюс a1x плюс и так далее. То есть я рассматриваю вот такой вот многочлен от x,
[04:00.000 --> 04:05.000]  и дальше, как обычно, беру его по модулю p, и потом ещё раз беру по модулю m.
[04:05.000 --> 04:10.000]  Значит, где h-ки это случайные числа из zp.
[04:24.000 --> 04:28.000]  То есть в прошлый раз у нас была такая штука, только у нас была линейная вот здесь функция.
[04:28.000 --> 04:32.000]  Здесь был не многочлен к этой степени, а просто только вот эти два слагаемые a0 и a1x.
[04:32.000 --> 04:36.000]  Я просто беру и расширяю эту штуку на многочлен k минус первой степени,
[04:36.000 --> 04:41.000]  соответственно, здесь ровно k неизвестных коэффициентов, ну не неизвестных, а случайных.
[04:41.000 --> 04:49.000]  И, собственно, можно примерно таким же аргументом показать, что вот это вот событие будет происходить примерно с такой вероятностью.
[04:49.000 --> 04:54.000]  То есть если раньше у нас было всего лишь две неизвестные h-ки, а 0, a1x мы их складывали,
[04:54.000 --> 04:58.000]  брали по модулю p, брали по модулю m, теперь мы берём просто k таких вот величин,
[04:58.000 --> 05:03.000]  ну и можно опять-таки показать, что вероятность будет примерно вот такая.
[05:06.000 --> 05:16.000]  Ну и здесь используется, например, такой факт, что если у вас есть какой-то многочлен на zp,
[05:16.000 --> 05:21.000]  и вам известно его значение в таких-то ка-точках, что по сути, что такое hs?
[05:21.000 --> 05:24.000]  Это многочлен k минус первой степени от x.
[05:24.000 --> 05:28.000]  Вот этот многочлен, он должен быть в такой точке равен такой-б �о величине,
[05:28.000 --> 05:30.000]  в такой точке должен быть такой-б этой величине и так далее.
[05:30.000 --> 05:32.000]  У вас есть k значений.
[05:32.000 --> 05:37.000]  Ну понятно, что многочлен k минус первой степени однозначно восстанавливается по своим k точкам.
[05:37.000 --> 05:43.000]  Если нам известны значения mans в ка-точках, то есть нам известны k точек с значениями в этих точках,
[05:43.000 --> 05:45.000] 跅тн многочленy на значении восстанавливается.
[05:47.000 --> 05:50.000]  Например, там параболGen1 , нсо Mayo 4 albo..?
[05:50.000 --> 05:52.000]  Ну и так далее.
[05:52.000 --> 05:56.680]  многосчетинка минус первой степени по k точку. Это значит, что так же, как и раньше,
[05:56.680 --> 06:04.440]  если бы мы не брали вот этот процент p, тогда вот эта вещь, грубо говоря, равномерно перемешивает
[06:04.440 --> 06:12.680]  все zp. То есть вероятность, если бы мы не брали по модулю m, то когда я навешиваю х-функцию на x1,
[06:12.680 --> 06:21.320]  на x2 и так далее, на xk, то я получаю независимые значения в zp. Потому что любой набор значений
[06:21.320 --> 06:26.320]  я могу получить с фиксированной вероятностью, потому что многочлен дозначно восстанавливается.
[06:26.320 --> 06:29.880]  И когда я беру по модулю m, там надо опять что-то то же самое сделать, что если у меня все значения
[06:29.880 --> 06:33.960]  перемешались случайно, равновероятно, то с какой вероятностью они совпали по модулю m?
[06:33.960 --> 06:44.480]  Ну вот, с такой-то примером. Ну что, нормально? Хорошо. Ну так вот, теперь если мы знаем,
[06:44.480 --> 06:50.280]  что такое независимое семейство и знаем, как их строить, то есть теорема, которая говорит,
[06:50.280 --> 06:55.600]  что при использовании достаточно сильных независимых семейств, вот эта штука работает
[06:55.600 --> 07:05.000]  за единицу. Значит, во-первых, если h это семейство, так как я это назвал, к-независимо,
[07:05.000 --> 07:19.280]  ну пусть будет 5 независимых семейств х-функций, то при использовании линейного пробирования
[07:19.680 --> 07:31.120]  асимптотика ответного запроса будет единичка. При использовании линейного пробирования
[07:31.120 --> 07:48.160]  ответный запрос, как обычно, амортизированная единичка в среднем. Амортизированная потому,
[07:48.160 --> 07:53.080]  что иногда вызывается rehash, и у нас мне нужно как бы за линию все старые элементы переложить
[07:53.080 --> 07:57.360]  в массив вдвое большего размера. Ну в среднем, потому что здесь мотожидание, поскольку все
[07:57.360 --> 08:01.040]  случайно, я хэш-функцию выбрал, у меня там мотожидание какое-то, ну в общем, все в среднем
[08:01.040 --> 08:05.920]  работает. Вот, а если использовать двойное хэширование, то достаточно два независимости.
[08:05.920 --> 08:17.600]  Значит, если h всего лишь два независимая семейства, то при двойном хэшировании тоже
[08:17.600 --> 08:40.960]  будет единичка. Такая же учетная единичка в среднем. Вот, значит, более того, смотрите,
[08:40.960 --> 08:44.800]  ну понятно, два независимая – это хорошее, да вот то же, что было в прошлый раз, это скажем
[08:44.800 --> 08:52.000]  ax плюс b по моделю p по моделю m на вот это вот два независимая. Мы в прошлый раз заказали,
[08:52.000 --> 08:58.840]  что универсально, но там два независимости тоже на самом деле выполняются. Вот, получается,
[08:58.840 --> 09:03.720]  если вы используете двойное хэширование, то есть шаг у вас не плюс один, плюс один, плюс один,
[09:03.720 --> 09:08.960]  а плюс h2 от x, плюс h2 от x и так далее по циклу, тогда вам достаточно использовать ту же самую
[09:08.960 --> 09:14.120]  хэш-функцию, которую мы рассматривали в прошлый раз. Значит, если мы хотим использовать линейное
[09:14.120 --> 09:20.280]  пробирование, оно в каком-то смысле может быть чуть лучше, потому что с точки зрения кэша у вас
[09:20.280 --> 09:23.800]  будет эффективнее, потому что вы двигаетесь только по нескольким подряд идущим ячейкам,
[09:23.800 --> 09:28.400]  ну и память так работает, что если вы посмотрели вот сюда, то посмотреть сюда проще всего. Как бы
[09:28.400 --> 09:33.400]  следующую ячейку смотреть очень просто, в отличие от ячейки, которая где-то далеко. Просто обращение
[09:33.400 --> 09:40.040]  к памяти так устроено, что близлежащие элементы проще всего подгружать. Вот, поэтому с какой
[09:40.040 --> 09:47.920]  точки зрения это может быть даже эффективнее, но формально, если вы здесь используете что-нибудь
[09:47.920 --> 09:53.920]  типа там 4 независимое или 2, ну короче менее независимое что-то, тогда как бы, ну на самом деле
[09:53.920 --> 09:58.280]  там оценка может вырождаться во что-то более худшее, там в логарифм или вообще в корень может
[09:58.280 --> 10:04.400]  выродиться, но обычно это не происходит. Если вы используете линейное пробирование и пишете
[10:04.400 --> 10:08.680]  два независимые семейства, скорее всего тоже будет все хорошо. Если еще rehash почаще делать,
[10:08.680 --> 10:15.480]  то будет вообще замечательно, потому что очень вряд ли кто-то будет строить такой набор тестов,
[10:15.480 --> 10:21.680]  ну как обычно, чтобы заваливать два независимые семейства, но чтобы 5 независимые проходили.
[10:21.680 --> 10:27.640]  Это вряд ли, но с точки зрения теории, опять-таки формально лучше всего использовать 5 независимые,
[10:27.640 --> 10:33.800]  если меньше, то там уже будет формально долго работать, но как получится, короче.
[10:33.800 --> 10:47.560]  Ну хорошо, значит следующий небольшой сюжет это про экономию памяти. Вот представьте, тут как бы
[10:47.560 --> 10:52.640]  я всегда, вот везде не явно предполагал, что элементы, которые добавляю в множество, это какие-то
[10:52.640 --> 10:56.840]  числа, соответственно, которые я могу эффективно хранить. Я говорю, что у меня универсум это zp или
[10:56.840 --> 11:03.120]  вложено в zp и там число это у меня int или long, long, ну короче что-то маленькое. Теперь представьте
[11:03.120 --> 11:07.320]  более реалистичную ситуацию, что вам нужно хранить какие-то более сложные объекты, ну там, не знаю,
[11:07.320 --> 11:14.960]  гиперссылки на сайты или какие-нибудь структуры, на описание какого-нибудь человека, историю его
[11:14.960 --> 11:20.680]  каких-нибудь заказов в магазине, ну короче какую-то сложную структуру вам надо хранить в таблице.
[11:20.680 --> 11:26.200]  Тогда вам не хотелось бы, чтобы каждый элемент вашего массива содержал вот такую большую структуру
[11:26.200 --> 11:32.160]  данных, ну какой-то большой объект, где много информации, которая тяжело класть, тяжело копировать,
[11:32.160 --> 11:38.680]  тяжело сравнивать друг с другом. Такого бы делать не хотелось. В таком случае, если у вас объекты
[11:38.680 --> 11:49.960]  тяжелые, можно делать что-то другое. Например, фильтр Блума. Давайте это тоже запишем, что преимущество
[11:49.960 --> 12:09.160]  здесь такое будет. Экономия памяти. Если объекты из универсума, откуда собственно черпаются все наши
[12:09.160 --> 12:31.800]  элементы тяжелые. Тут сделаем следующий прикол. Давайте заведем массив булевский,
[12:31.800 --> 12:46.680]  просто из 0 единиц. Не массив там чисел, а массив булей. Длины m равной n, где n это количество
[12:46.680 --> 12:55.720]  элементов. Здесь считаем следующее, что у нас заранее известны все элементы, которые кладутся в
[12:55.720 --> 13:03.640]  структуру. Они сначала положились, а потом поступают только запросы типа find. Нам изначально
[13:03.640 --> 13:09.400]  известно количество инсертов. У нас будут только запросы типа insert, которых будет не очень много,
[13:09.400 --> 13:23.120]  их суммарно будет n. И будут запросы типа find. Причем мы позволим себе иногда ошибаться. В целях
[13:23.120 --> 13:30.440]  экономии памяти мы позволим себе иногда возвращать неправильный ответ. Ну вот как-то так получается,
[13:30.440 --> 13:37.200]  что если вы хотите сильно сэкономить в одном месте, то, возможно, вам придется чем-то пожертвовать.
[13:37.200 --> 13:43.600]  Но давайте нам здесь придется пожертвовать корректностью иногда. Вот эта штука find будет
[13:43.600 --> 13:53.960]  работать следующим образом. Find x всегда говорит либо да, либо нет. Нашла она x в структуре или нет.
[13:53.960 --> 13:59.520]  Давайте сделаем следующее. Давайте скажем, что если она говорит нет, то это точный правильный ответ.
[13:59.520 --> 14:11.360]  Это правильный ответ. А если она говорит да, то это скорее всего правильный ответ. А именно это
[14:11.360 --> 14:22.040]  правильный ответ с вероятностью, ну давайте какого-нибудь там, 99 процентов. С вероятностью примерно 99 процентов.
[14:22.040 --> 14:30.600]  Ну вроде нормально. В одном проценте случаев мы будем ошибаться. Кажется не очень критично,
[14:30.600 --> 14:37.560]  но опять-таки с точки зрения какой-то практической реализации иногда ошибаться не страшно, скажем так.
[14:37.560 --> 14:43.880]  Вот. Ну и на самом деле вот эту константу можно сколь угодно улучшать. Там можно сделать
[14:43.880 --> 14:48.840]  ее сколь угодно близкой к единице. Это я чуть-чуть позже еще пропишу. То есть получается, что у нас
[14:48.840 --> 14:58.640]  бывают иногда неправильные ответы да. Это так называемое false positive. То есть нам ложно сказали,
[14:58.640 --> 15:03.280]  что да. Нам ложно сказали положительный ответ. Ложно положительное срабатывание произошло.
[15:07.560 --> 15:19.800]  Давайте сразу формализую. Давайте скажу, чтобы правильный ответ у нас был с вероятностью 1
[15:19.800 --> 15:30.960]  минус Эпсилон. Где Эпсилон, что-то маленькое. Тогда, значит, идея. Смотрите, вот у меня есть большой мой
[15:30.960 --> 15:46.440]  массив. Булевский массив. Давайте скажем, что в нем изначально все нули. Дальше давайте возьмем
[15:46.440 --> 15:58.920]  к хэш функций. Ну из какого-нибудь там опять хорошего достаточно семейства. И на запрос insert
[15:58.920 --> 16:09.800]  x давайте посчитаем от х все вот эти хэш функции. И во всех этих точках поставим единички. Значит,
[16:09.800 --> 16:17.360]  формально для каждого и от 1 до k. Вот если это массив a был, то давайте я скажу a с индексом h
[16:17.360 --> 16:27.600]  it от x равно единице. Ну а то есть вот у меня пришел элемент x. Я считаю от него значение всех этих
[16:27.600 --> 16:33.360]  хэш функций. Все они ведут его в какие-то там конкретные ячейки. Вот я в них ставлю единицы.
[16:33.360 --> 16:56.400]  Тогда давайте поймем, как работает find. Ну вот если до этого find когда-то пришел insert,
[16:56.400 --> 17:06.680]  то понятно, что во всех вот этих вот ячейках, ошитая от x, везде стоит единица. Ну и можно
[17:06.680 --> 17:12.600]  надеяться, что если у меня h достаточно случайно все перемешивает, то можно надеяться, что в другом
[17:12.600 --> 17:17.680]  случае такого произойти не могло. Ну давайте просто так и сделаем. Давайте пройдемся по всем
[17:17.680 --> 17:24.160]  этим ячейкам, проверим, что a во всех этих ячейках единица. Если нет, то x точно не было, его раньше
[17:24.160 --> 17:28.840]  точно не могло добавиться, потому что иначе во всех этих ячейках должна была бы уйти единица. А если
[17:28.840 --> 17:37.000]  везде единица, то давайте скажем, что ну x скорее всего в таблице есть. Давайте опять посчитаем все эти
[17:37.000 --> 17:51.000]  хэш функции. Если внезапно a h it от x равно нулю, тогда return false. Ну return no давайте напишу,
[17:51.000 --> 18:00.800]  что x в структуре точно нет. А иначе, если во всех этих точках была единичка, давайте вернем yes.
[18:00.800 --> 18:09.400]  Весь фильтр Блума.
[18:09.400 --> 18:19.280]  Ну а то есть еще раз смотрите, у меня есть, ну там давайте считать, достаточно большой массив вот
[18:19.280 --> 18:25.800]  здесь вот. Чтобы вставить элемент, я считаю от него в каком-то смысле k случайных каких-то статистик,
[18:25.800 --> 18:33.840]  k каких-то случайных функций и вставляю единички в те места, от которых я посчитал. Тогда ну как бы
[18:33.840 --> 18:39.560]  хотелось бы верить, что вот этот набор единичек, он однозначно соответствует x. Ну это там более-менее
[18:39.560 --> 18:44.200]  правдоподобное предположение. Если мы знаем, что коллизии в хэш функции происходит довольно редко,
[18:44.200 --> 18:50.720]  а мы как раз выбираем такое место хэш функции, чтобы коллизии там было мало. Ну и тогда как раз вот эти
[18:50.720 --> 18:54.480]  единички, они скорее всего там более-менее однозначно соответствуют x. Ну или по крайней мере вероятность того,
[18:54.480 --> 19:00.360]  что есть какой-то другой y с таким же набором хэш функции. Она там маленькая какая-то. Ну и поэтому
[19:00.360 --> 19:05.440]  давайте просто пройдемся по этим ячейкам, проверим, что здесь везде единицы. Если единицы, то x скорее
[19:05.440 --> 19:11.080]  всего был, потому что ну а кто еще мог все эти единички туда поставить. Если хотя бы одна из этих
[19:11.200 --> 19:20.440]  ячеек нулевая, то x точно не было. Если мы вернем no, то это точно верно. Потому что если бы x был,
[19:20.440 --> 19:28.680]  то он бы во все эти ячейки поставил единицу. Ну как бы хорошо, конечно, но иногда мы можем
[19:28.680 --> 19:33.400]  ошибаться. Потому что я вот говорю, что скорее всего вот эти вот единички однозначно задают x,
[19:33.400 --> 19:39.360]  но как бы даже если так, то возможно эти единички, возможно пришли из каких-то разных других там y
[19:39.600 --> 19:45.240]  e и z. Например могло быть такое, что вот эти вот две ячка поставил какой-то y, а эти две ячка
[19:45.240 --> 19:50.160]  поставил какой-то z. Вполне могло быть такое, что какие-то две хэш-функции от y это вот эти
[19:50.160 --> 19:55.060]  две единички какие-то две другие хэш-функции от другого z это вот эти две. То есть мы опять мы
[19:55.060 --> 20:00.420]  покрыли вот те самые позиции, которые соответствуют как бы  lowering и с помощью других элементов на
[20:00.420 --> 20:06.680]  потенциально такое бывает. Но мы считаем, что довольно редко. Надеемся на то, что это редко
[20:06.680 --> 20:14.560]  Вот. Сейчас все пропишем.
[20:14.560 --> 20:27.400]  Идея понятна, как реализовывать. Тут прям очень мало кода,
[20:27.400 --> 20:35.240]  буквально на строчке 10. Да, длины такой же, сколько всего будет элементов.
[20:35.240 --> 20:42.160]  Он всегда размера m, но в этом случае берем m равная n, сколько будет элементов в нём всего.
[20:42.160 --> 20:51.480]  Вот. Значит утверждается, что оптимальнее всего выбрать следующие параметры. Сейчас,
[20:51.480 --> 20:58.920]  пардон. Это я, кстати, обманываю, что m должно быть равно n, оно должно быть чуть больше. Виноват.
[20:58.920 --> 21:07.600]  Да, обманул, извините. Давайте я это сотру. Чуть-чуть побольше надо будет, все-таки виноват.
[21:07.600 --> 21:18.320]  Значит k, нужно брать оптимальное k, равно, ну давайте напишем двоичный алгорифм,
[21:18.320 --> 21:35.800]  1 делит на епсилон, m равно, так, n, давайте в натуральных напишу, значит логарифм 1 делит на
[21:35.800 --> 21:50.880]  епсилон делить на квадрат логарифма 2. Вот так вот. Правильно? Да, вроде правильно. Вот, ну то есть это
[21:50.880 --> 21:56.080]  можно тоже сгруппировать в логарифм 1 делит на епсилон по основанию 2, но будет ещё 2 в знаменателе,
[21:56.080 --> 22:01.280]  поэтому оставлю так. Вот, утверждается, если подобрать такие параметры, то будет как раз
[22:01.280 --> 22:09.920]  из комы заявлена вероятность ошибки примерно 1, точнее примерно епсилон. И, ну и соответственно память
[22:09.920 --> 22:16.080]  у меня, смотрите, память у меня будет линейная по n умножить на логарифм вероятности ошибки. Вот,
[22:16.080 --> 22:21.840]  довольно хорошо. Значит, давайте поймём немножко интуитивно, откуда это берётся. Ну, смотрите,
[22:21.840 --> 22:26.720]  давайте опять погрузимся в идеальный мир, где мы считаем, что у меня h это не х-функция из
[22:26.720 --> 22:30.760]  какого-то маленького семейства функций, а настоящие случайные функции, которые на каждом
[22:30.760 --> 22:37.120]  х принимают независимое значение во всём вот этом вот ZM. Значение любого числа от 0 до и минус 1.
[22:37.120 --> 22:52.800]  Значит, тогда с какой вероятностью я вот здесь могу ошибиться?
[22:56.720 --> 23:09.360]  Значит, давайте сначала скажем следующее. Давайте представим, что у меня давайте
[23:09.360 --> 23:13.560]  простое, ну давайте типа доказательства, там на самом деле просто идея, откуда эти
[23:13.560 --> 23:20.000]  константы, ну там можно вывести или почему они такие, примерно. Значит, понятно, что в среднем у
[23:20.000 --> 23:28.080]  нас единичек будет ну что-то типа nk. Да, ну то есть, по крайней мере, вот если у меня n элементов
[23:28.080 --> 23:33.800]  суммарно добавится, то каждый из них какие-то k элементов заединичит, ну и нам хотелось бы,
[23:33.800 --> 23:39.280]  чтобы, то есть, примерно nk единичек я поставлю. Возможно, какие-то из них склеятся, но там их
[23:39.280 --> 23:51.960]  будет точно не больше, чем nk. Значит, единиц проставим не больше, чем nk. Вот, ну и мы хотим,
[23:51.960 --> 23:55.680]  наверное, чтобы опять, чтобы у нас размер х-таблицы был примерно в два раза больше,
[23:55.680 --> 24:00.760]  чем число элементов, ну потому что нам хочется, чтобы был не очень большой, чтобы память сэкономить,
[24:00.760 --> 24:05.120]  но при этом не очень маленький, чтобы не было сильно много коллизий. Поэтому нам хотелось бы,
[24:05.120 --> 24:14.920]  чтобы m было приближительно 2 nk, а в два раза больше, чем единичек. Вот, тогда, если у нас,
[24:14.920 --> 24:20.560]  смотрите, если у нас каждая вот эта вот штука, это какое-то независимое случайное число от 0
[24:20.560 --> 24:34.640]  до m-1 и при этом у нас единичек примерно половина, то вероятность того, что данный какой-то y,
[24:34.680 --> 24:48.120]  сейчас, момент, ну хочется, например, на следующее, что вероятность того, что h1 от x,
[24:48.120 --> 24:59.720]  давайте так, а h1 от x и так далее, а hkt от x все будут единицами в отсутствие x,
[24:59.720 --> 25:20.600]  в отсутствие x, это примерно 2 в степени минус k. Ну потому что если у меня nk единиц,
[25:20.600 --> 25:26.440]  а длина массива 2 nk, то есть у меня примерно nk единиц, примерно nk нулей, значит тогда вероятность,
[25:26.440 --> 25:32.400]  что в данных конкретных k позициях стоят единицы примерно как раз такая, то есть вот у вас есть
[25:32.400 --> 25:39.080]  такой большой массив длины 2 nk, у вас здесь примерно половина нулей, половина единицы,
[25:39.080 --> 25:45.800]  но если вы фиксируете данное множество позиций и спрашиваете, с какой вероятностью здесь и здесь
[25:45.800 --> 25:50.400]  единицы, ну понятно, если у вас примерно поровнули единиц, то вероятность того, что здесь единица
[25:50.400 --> 25:55.040]  это 1 вторая, вероятность того, что здесь единица 1 вторая и так далее. Это очень на самом деле не
[25:55.040 --> 26:01.440]  формально, это очень грубая оценка, потому что конечно эти события зависимые, но вот типа и так
[26:01.440 --> 26:06.600]  сойдет. Чтобы понять, по крайней мере, откуда это берется, тут такой грубой оценки хватает.
[26:06.600 --> 26:11.680]  Еще раз, если нулей половина, единиц половина, то вероятность того, что здесь единица, это 1
[26:11.680 --> 26:15.920]  вторая, потому что случайная позиция, нулей единиц поровну, вероятность того, что здесь единица 1
[26:15.920 --> 26:20.840]  вторая. Ну и соответственно, если я хочу, чтобы в к позициях были единицы, то вероятность этого 2
[26:20.840 --> 26:28.040]  в минус катых. Значит вероятность вот этого вот false позитива, вероятность вернуть, вероятность
[26:28.040 --> 26:33.240]  увидеть здесь везде единицы даже несмотря на то, что фикса не было, это 2 в минус катых. Ну и вот если 2
[26:33.240 --> 26:38.720]  в минус катых, это как раз одна эпселоновая, точнее просто эпселон, вероятность ошибки была эпселон.
[26:38.720 --> 26:45.640]  Вот, ну и тогда как раз вот это вот, это вот, это вот. Давайте про логарифмируем, например,
[26:45.640 --> 26:54.400]  поставим 2. У меня получится минус k равно лог 2 эпселон. Дальше умножаю обе части на минус 1,
[26:54.400 --> 27:02.800]  получается k равно лог 2 1 директной эпселон. Собственно, откуда это берется примерно.
[27:02.800 --> 27:09.400]  Вот. Ну и дальше, если m подставить вот такое, то, кажется, ровно это и получится. Ну или там
[27:09.400 --> 27:20.400]  примерно так. Окей? Вот. Все, то есть, собственно, если это было непонятно, неважно, вот просто есть
[27:20.400 --> 27:25.640]  какие-то магические каты такие, что если мы хотим ошибаться с вероятностью эпселон, то берем такое
[27:25.640 --> 27:33.320]  k. Если у нас всего будет n вставок, то берем такое m. Если мы не знаем, какой n, мы можем, как обычно,
[27:33.320 --> 27:37.960]  делать rehash. Если нам неизвестно n заранее, мы можем сначала выделить какой-то небольшой массив,
[27:37.960 --> 27:42.120]  туда повставлять, когда элементов стало слишком много, когда единичек стало слишком много,
[27:42.120 --> 27:46.800]  скажем, больше половины. Тогда мы можем опять m вдвое увеличить и все старые элементы перенести
[27:46.800 --> 27:54.960]  в новую табличку. Вот. Ну а после этого, если такие константы фиксированы, то вот, буквально,
[27:54.960 --> 28:02.080]  там 10 строчек и очень часто правильно отвечаем. Разве что с вероятностью эпселон можем ошибиться.
[28:02.080 --> 28:11.800]  Еще раз? Да, кстати, хороший вопрос, откуда взялся натуральный.
[28:11.800 --> 28:27.920]  Один тогда. Я могу распределить, что отношение этих двух логарифмов это двоичный логарифм,
[28:27.920 --> 28:37.800]  но еще остается натуральный логарифм двойки. Да, справедливо. Ну, там будет другая константа.
[28:37.800 --> 28:46.040]  Да. Окей. Ну, значит, это хорошая оценка только для k. Вот. M получается немножко другим. Сейчас
[28:46.040 --> 28:55.400]  я подумаю. 2nk. Ну да, потому что это не очень формальный анализ. Справедливое замечание.
[28:55.400 --> 29:10.160]  Но берется именно такое обычно. Так, еще вопросы, может? Хорошо. Так, значит, зачем это может быть,
[29:10.160 --> 29:19.440]  например, нужно? Казалось бы, зачем нам ошибаться? Ну, давайте два примера. Значит, во-первых,
[29:19.440 --> 29:42.560]  это, скажем, база каких-нибудь вредоносных сайтов. Ну, там, не знаю, пусть вы какой-нибудь Google,
[29:42.560 --> 29:47.880]  вам поступают жалобы от пользователя на какие-то сайты, вы проверяете, да, там реально какой-нибудь,
[29:47.880 --> 29:53.520]  не знаю, малейшее софтвое раздается, что-нибудь плохое, нехороший сайт. Вот вы хотите их все
[29:53.520 --> 29:58.200]  добавлять в какую-то структуру, так чтобы, если пользователь пытается зайти на этот сайт или
[29:58.200 --> 30:04.280]  если он попадается ему в поиске, то вы ему даете предупреждение, там, на этот сайт было много жалоб,
[30:04.280 --> 30:10.080]  будьте внимательны, не скачивайте отсюда ничего. Вполне себе такая нормальная постановка. Вот.
[30:10.080 --> 30:14.920]  Значит, давайте заведем две структуры. Сначала давайте заведем одну большую базу данных для
[30:14.920 --> 30:20.480]  хранения полностью вот этих вот сайтов. Ну, там, можно опять-таки это сделать как хэштаблицу.
[30:20.480 --> 30:27.040]  Давайте я не буду специфицировать, просто открою, что это база данных. База данных всех плохих сайтов.
[30:27.040 --> 30:38.040]  Всех плохих сайтов. Прям полностью, да, то есть каждый элемент это одна большая строка там,
[30:38.040 --> 30:48.440]  такое-то название сайта. Полностью название, да, без сокращений, полностью вся информация в сайте.
[30:48.440 --> 30:54.280]  Вот. И давайте дальше сделаем небольшую к ней дополнительную структурку, которая будет как
[30:54.280 --> 31:02.240]  раз являться фильтром Блума. Фильтр Блума. Вот. И давайте все элементы из этой большой базы
[31:02.400 --> 31:12.680]  добавим в этот маленький фильтр. Это нам очень сильно сэкономит память. То есть у нас есть какая-то
[31:12.680 --> 31:18.560]  большая громоздкая структура. Ну, мы там где-то ее храним где-нибудь далеко на каком-нибудь удаленном
[31:18.560 --> 31:23.400]  хранилище. Вот она большая, с ней часто работать не хочется. Давайте мы ее немножечко сожмем,
[31:23.400 --> 31:27.920]  точнее даже сильно сожмем. Потому что здесь каждый элемент это большая какая-то длинная строка,
[31:27.920 --> 31:32.880]  она занимает много памяти. Вместо этого мы заведем маленький вот такой массив булевский и все
[31:32.880 --> 31:38.600]  элементы оттуда сложим в этот маленький массив с помощью фильтра Блума. И тогда дальше, если какой-то
[31:38.600 --> 31:42.400]  пользователь заходит на какой-то сайт, мы хотим проверить, а лежит ли этот сайт вот в этом
[31:42.400 --> 31:47.200]  множестве большом. Ну, давайте вместо того, чтобы обращаться сразу непосредственно к этой огромной
[31:47.200 --> 31:52.800]  базе, в которой все запросы долго работают, грубо говоря, давайте мы сначала посмотрим на фильтр Блума,
[31:52.800 --> 31:58.560]  лежит ли тот сайт в множестве вредоносных с точки зрения фильтра Блума. Эта штука работает быстро,
[31:58.560 --> 32:04.560]  а здесь там какой-то небольшой булевский массив, вам надо посчитать всего к хэш функции и посмотреть
[32:04.560 --> 32:09.720]  на какие-то каечеек этого массива. Это сильно быстрее, чем искать в этой большой базе данную
[32:09.720 --> 32:14.800]  строчку, проверять на равенстве и так далее. Это сильно эффективнее. Но давайте посмотрим,
[32:14.800 --> 32:22.640]  есть ли в этом множество строка S. Если фильтр сказал нет, то это сайт хороший, точно причем.
[32:22.640 --> 32:28.800]  Мы знаем, что все плохие у нас вот здесь, значит и вот здесь. Если фильтр сказал нет, значит сайта
[32:28.800 --> 32:35.760]  точно нет в множестве плохих. И это отвечает за быстро. Причем это вроде бы и хорошо, потому что у нас
[32:35.760 --> 32:40.360]  скорее всего таких плохих сайтов мало, по крайней мере пользователь, когда заходит на сайт, они
[32:40.360 --> 32:47.720]  обычно хорошие, большая их часть. И тогда, соответственно, мы будем часто получать нет, и это будет
[32:47.720 --> 32:52.680]  работать быстро, то есть мы не будем делать запрос к огромной долгой базе данных. Если нет, то хорошо.
[32:52.680 --> 32:59.080]  А если да, то это либо реально какой-то плохой сайт, либо это нормальный сайт. Ну и мы, наверное,
[32:59.080 --> 33:04.600]  не хотим, чтобы пользователю говорили, что хороший сайт плохой. Тогда чтобы уточнить, если
[33:04.600 --> 33:09.200]  нам сказали да, давайте обратимся сюда и посмотрим, есть ли этот сайт здесь. Поскольку это происходит,
[33:09.200 --> 33:13.320]  мы считаем, что это происходит довольно редко, плохих сайтов мало, и пользователи на них редко
[33:13.320 --> 33:21.440]  заходят. Если с вывело да, то давайте еще обратимся сюда и спросим, есть ли с здесь. Это работает долго,
[33:21.440 --> 33:29.280]  но редко, поэтому мы выиграли. Нам сюда приходится обращаться редко за счет вот этого небольшого
[33:29.280 --> 33:40.600]  фильтра, который на все отвечает быстро. Ну и второй пример. Кратенько давайте скажу,
[33:40.760 --> 33:50.920]  например, словарь. Представьте, что вы пишете какие-нибудь тексты и вы хотите подчеркивать
[33:50.920 --> 33:55.240]  неправильно написанные слова. То есть у вас там есть слова русского языка, например, вы хотите
[33:55.240 --> 33:59.480]  все эти слова добавить в базу данных, потом, когда пользователь что-то печатает, вы хотите
[33:59.480 --> 34:05.800]  подчеркивать ему ошибки. Вот, ну опять же, есть у вас большая данных, большая структура, это словарь.
[34:05.800 --> 34:16.840]  И дальше вы на нем строите небольшой фильтр и потом, когда пользователь что-то вбивает
[34:16.840 --> 34:22.360]  какое-нибудь слово, вы обращаетесь просто к фильтру, смотрите такое слово есть в словаре, да или нет.
[34:22.360 --> 34:28.600]  Если нет, значит слово точно плохое, его нет в этом словаре, значит оно точно плохое, пользователь
[34:28.600 --> 34:32.600]  точно опечатался, давайте подчеркнем ему красненько и скажем, что вот здесь у тебя скорее всего ошибка,
[34:32.600 --> 34:40.280]  точнее, точно ошибка. Вот, а если фильтр сказал да, ну давайте, можно даже не обращаться к словарю,
[34:40.280 --> 34:44.600]  можно просто сказать, окей, это хорошее слово, даже если типа мы ошиблись с маленькой вероятностью,
[34:44.600 --> 34:49.440]  то там, окей, пользователь опечатался, мы не нашли эту опечатку, это происходит редко, не очень страшно,
[34:49.440 --> 34:55.240]  мало опечаток в тексте не сильно критично. То есть многие опечатки мы увидим, потому что их нет в фильтре,
[34:55.240 --> 35:23.480]  какие-то не увидим, ну и ладно. Ну вот, как-то так, окей? Хорошо. Так, дальше еще одна модификация с похожей
[35:23.480 --> 35:43.720]  концепцией того, что мы можем иногда ошибаться, это называется фильтр кукушки. Вот, значит здесь опять
[35:43.720 --> 35:48.880]  идеология такая же, что мы хотим экономить память, мы не хотим хранить все эти объекты в явном виде,
[35:48.880 --> 35:54.240]  потому что они могут быть какие-то тяжелые, большие, сложные, мы не хотим их всех хранить. Вместо
[35:54.240 --> 36:00.160]  булевского массива теперь давайте мы все эти элементы как-нибудь эффективно захэшируем,
[36:00.160 --> 36:05.920]  то есть каждую там строчку, например, название сайта отобразим в что-нибудь небольшое, ну в
[36:05.920 --> 36:10.840]  какую-нибудь маленькую структуру, маленькое число. То есть, во-первых, мы введем, давайте скажем,
[36:10.840 --> 36:17.200]  какую-то функцию, которая все элементы универсума отображает в какую-нибудь маленькую, ну давайте
[36:17.200 --> 36:24.680]  пусть будет zk. Вот это первая внешняя хэш-функция. И теперь вместо элементов будем хранить значение
[36:24.680 --> 36:37.120]  хэш-функции от них. Вместо элемента x храним f от x. То есть вместо элемента храним его хэш-значение.
[36:37.120 --> 36:43.160]  Сам элемент вообще нигде не храним. То есть, если нам пришла команда вот такую длинную строчку
[36:43.160 --> 36:47.320]  вставить в структуру, я такой, да-да, хорошо, я сначала посчитаю хэш, какое-то маленькое число,
[36:47.320 --> 36:52.640]  и дальше я в структуру добавлю вот это маленькое число f от x. Какое-то число вот отсюда. То, что
[36:52.640 --> 36:58.080]  какие-то разные строки могут получить одно и то же хэш-значение, мне не очень страшно. Да, такое
[36:58.080 --> 37:03.760]  бывает, но редко, вероятность маленькая. Мы можем ошибаться иногда. У меня бывают коллизии, это не страшно.
[37:03.760 --> 37:14.040]  Вот. Дальше. Соответственно вместо x мы будем хранить всегда его хэш-значение. Дальше давайте
[37:14.040 --> 37:20.240]  для каждого хэш-значения, для каждого вот этого вот ключа, давайте определим две возможные позиции,
[37:20.240 --> 37:28.640]  куда его можно было бы поломить в нашем массиве. Давайте я, ну, после будет какой-нибудь y.
[37:28.640 --> 37:37.120]  Давайте для каждого y определим две возможные позиции массива, где он может лежать. Вот y может
[37:37.120 --> 37:47.120]  попасть либо сюда, в клетку с номером h, пусть будет 1 от x. Ну ладно, нет, не хочу h1, давайте
[37:47.120 --> 38:01.200]  просто h от x будет. Вторая клетка, где он может лежать, это будет, нет, нет, нет, нет, тут хитрее, виноват.
[38:01.200 --> 38:12.920]  У меня же еще преимущество, что когда я кладу x, я знаю, я могу от него убрать хэш-функцию,
[38:12.920 --> 38:17.600]  то есть, когда мне прошел x, я могу до добавления посчитать в него еще другую хэш-функцию. Так вот,
[38:17.600 --> 38:23.480]  значит, когда у меня появился x, я могу его положить в клетку с номером h от x, либо в клетку с
[38:23.480 --> 38:34.200]  номером, давайте вот так вот напишем, h от x xor g от f от x. Вот, где h и g это тоже какие-то хэш-функции.
[38:34.200 --> 38:47.320]  xor по битовой. Вот, значит, тем самым для каждого конкретного элемента x из универсума, у меня
[38:47.320 --> 38:52.600]  есть всего две позиции, где он может лежать, либо вот здесь вот в клетке h от x, либо в клетке с
[38:52.600 --> 38:56.640]  номером h от x, ну короче, какая другая функция. Они там как-то быстро считаются, у меня есть две
[38:56.640 --> 39:02.520]  конкретные позиции. Тогда, если мне пришел find, то я могу на него легко ответить. Мне говорят find x.
[39:02.520 --> 39:09.600]  Я смотрю в эти две клетки, смотрю, есть ли там f от x, хотя бы в одну из них. Если есть, то я говорю,
[39:09.600 --> 39:12.880]  что элемент присутствует, иначе говорю, что не присутствует. То есть, вот у меня есть две
[39:12.880 --> 39:19.560]  возможные клетки, я кладу в одну из них f от x. Куда хочу, туда и кладу. Давайте запишем, что
[39:19.560 --> 39:44.960]  элементу x соответствуют клетке h от x и h от x xor g от f от x. В любую из них кладем f от x.
[39:44.960 --> 39:52.240]  То есть, как обычно, я не хочу хранить самый элемент x, потому что это длинная строка какая-то.
[39:52.240 --> 39:57.000]  Вместо этого я храню от нее какое-то х значение. То есть, у меня есть две возможные позиции, я
[39:57.000 --> 40:04.240]  смотрю какая из них пустая и кладу туда f от x. Тогда find работает очень просто. Если мне пришел
[40:04.240 --> 40:12.960]  find к какой-нибудь y, то я считаю h от y, я считаю h от y xor, ну как обычно, я считаю те две возможные
[40:12.960 --> 40:20.440]  позиции, где он может лежать и спрашиваю, есть ли хотя бы в одной из этих двух клеток f от y.
[40:20.440 --> 40:26.640]  А есть ли тут f от y.
[40:26.640 --> 40:50.840]  Также просто работает erase. Если мне говорят сделать erase y, то у меня есть
[40:50.840 --> 40:57.680]  всего две возможные клетки, где лежит y. Либо вот здесь, либо вот здесь. Давайте я их обе
[40:57.680 --> 41:02.800]  посмотрю. Если в одной из них написано f от y, давайте f от y сотру и скажу, что там ничего не лежит.
[41:02.800 --> 41:13.480]  Опять смотрим на те же две клетки, смотрим на две клетки. Если в одной из них лежит f от y,
[41:13.480 --> 41:34.520]  то мы его стираем. Мы хотим удалить y, мы вместо y удаляем hash от f от y, потому что сам y я
[41:34.520 --> 41:39.320]  нигде не храню, я могу только hash от f хранить. Давайте найдем f от y, если где-то лежит, то удалим.
[41:39.320 --> 41:54.880]  Это было find и erase. Что делать с insert? Insert работает чуть посложнее, но интуитивно
[41:54.880 --> 42:00.520]  предсказуемо. У меня есть две позиции, куда я хочу положить что-то, и пусть они обе заняты.
[42:00.520 --> 42:05.840]  Но давайте в одну из них положим, а то, что я вытеснил, переместим в другую возможную для
[42:05.840 --> 42:13.320]  него. То есть пусть был массив. Я для x посчитал две возможные клетки, куда можно было его положить,
[42:13.320 --> 42:20.200]  точнее не его, а f от x. Пусть тут занято, пусть тут лежала f от t, пусть тут лежала f от z.
[42:20.200 --> 42:28.960]  Дальше давайте сделаем следующее. Пусть x вытесняет вот эту штуку, то есть вместо
[42:28.960 --> 42:35.920]  вот этого я сюда записываю f от x, и я вытеснил сейчас значение f от z. Но мне нужно его тогда
[42:35.920 --> 42:40.600]  опять куда-то поместить. Раз я его вытеснил, мне нужно обратно его поместить в мою структуру.
[42:40.600 --> 42:48.360]  Куда его надо поместить? Я утверждаю, что я могу однозначно понять, в какую клетку его нужно
[42:48.360 --> 42:54.400]  положить. Потому что вот это была какая клетка? Это была либо hash от z, то есть вот эта клетка,
[42:54.400 --> 43:01.920]  где раньше лежал f от z. Это либо hash от z, либо hash от z xor g от f от z. Ну тогда другая,
[43:01.920 --> 43:09.360]  давайте я там назову клетку i, тогда другая клетка, где может лежать f от y, это xor g от f от z.
[43:09.360 --> 43:20.720]  Так вот, мне только его и надо. В этом и прикол. Да, в этом и прикол выбора ровно таких двух хэш
[43:20.720 --> 43:27.680]  функций, что, зная значение одной, вы однозначно можете понять значение другое, зная только f.
[43:27.680 --> 43:33.800]  Сам элемент вам знать не надо, вам надо знать только f от него. Ну смотрите, вот если пусть f от z
[43:33.800 --> 43:40.280]  лежало вот здесь, в позиции hash от z, тогда чтобы получить соседнюю двойственную к нему позицию,
[43:40.280 --> 43:46.600]  вам нужно это hash от z xor-ить с g от f от z. Ровно это там и написано. И наоборот, если вот эта
[43:46.600 --> 43:51.600]  позиция, где раньше лежало f от z, то чтобы попасть сюда, вам нужно этот xor аннигилировать,
[43:51.600 --> 43:55.880]  но xor, если вы два раза с одним и тем же словом проксорите, вы получите то, что было изначально.
[43:55.880 --> 44:05.400]  Потому что a xor b xor b это a. Два раза xor сам себя убивает. Поэтому, если вы лежали здесь,
[44:05.400 --> 44:11.200]  то чтобы попасть сюда, в другую позицию, где может лежать z, вам нужно просто вот это отбросить,
[44:11.200 --> 44:19.720]  это значит xor-ить с g от f от z. Ну все, мы тогда f от z вытеснили, мы знаем тогда другую позицию,
[44:19.720 --> 44:25.120]  куда он обязан попасть. Но давайте его сюда поместим. Если здесь опять было занято, то я
[44:25.120 --> 44:31.600]  соответственно сюда f от z поместил и вытеснил отсюда какой-то f от y. Тогда опять я для него знаю
[44:31.600 --> 44:39.040]  другую позицию, куда он должен был поместиться, давайте его сюда положим. Он возможно что-то еще
[44:39.040 --> 44:48.560]  вытеснил какой-то там f от u. Тогда мы знаем, куда нужно положить f от u, ну и так далее. К сожалению, да.
[44:48.560 --> 45:01.080]  Вот, и здесь лучшее решение зацикливаний такое. Давайте, скажем, сделаем достаточно много шагов,
[45:01.080 --> 45:09.360]  много это 20, например, или там 10. Если мы в течение десяти раз перекладывали, то значит,
[45:09.360 --> 45:14.100]  наверное, мы зациклились, ну или как бы 10, все равно уже много. Мы хотим, чтобы у нас все работало как
[45:14.100 --> 45:19.600]  бы за единичку, ну там, амортизировано в среднем и так далее. Но если мы сделали 10 таких перекладываний,
[45:19.600 --> 45:24.200]  то уже, наверное, что-то плохо. Либо табличка переполненная, либо мы ушли в какой-то цикл,
[45:24.200 --> 45:28.120]  ну короче, что-то нехорошее. Но у нас есть универсальный рецепт на такие случаи. Давайте
[45:28.120 --> 45:33.840]  rehash сделаем. Давайте возьмем табличку, если надо, в два раза ее расширим. Возьмем новую hash
[45:33.840 --> 45:48.680]  функцию. Ну f менять нельзя, потому что у нас лежат вот эти hash значения, ну можно там поменять hg
[45:48.680 --> 45:54.320]  и все переложить в новую hash табличку в два раза большего размера. То есть если делаем слишком
[45:54.320 --> 46:06.640]  много перекладываний, то делаем rehash. Делаем слишком много перекладываний. Перекладываний,
[46:06.640 --> 46:29.080]  ну скажем там 10. А то запускаем rehash. Вот такая идея. Ну как, понятно? Ну круто. Почему это кукушка?
[46:29.080 --> 46:34.600]  Ну потому что говорят, что кукушки так с яйцами в гнездах поступают, они куда-то прилетают,
[46:34.600 --> 46:41.120]  кладут в гнездо свои яйца, и другие соответственно вынуждают другую кукушку переместить своих
[46:41.120 --> 46:49.560]  детенышей куда-то в другое место. Ну это такая легенда говорят. Ну а так чего, мы добились примерно
[46:49.560 --> 46:54.400]  того же, смотрите. То есть мы сэкономили память, то есть вместо самих строк храним какие-то
[46:54.400 --> 46:58.840]  hash значения от них, что-то более маленькое. Соответственно у нас памяти сильно меньше, чем если
[46:58.840 --> 47:05.360]  мы хранили все элементы. При этом, если элемент в структуре есть, то мы его точно найдем. То есть
[47:05.360 --> 47:10.280]  если у нас х добавлялся, то мы его точно найдем. Мы скажем yes, потому что хотя бы в одной из двух
[47:10.280 --> 47:16.400]  клеток будет написано f от x. А если элемента нет, то вероятность того, что мы скажем да,
[47:16.400 --> 47:21.880]  ложно положительное срабатывание, вероятность неправильного ответа да, она маленькая. Ну потому
[47:21.880 --> 47:27.760]  что это вероятность того, что нашелся другой элемент с тем же hash, который еще и лежит в одной
[47:27.760 --> 47:32.640]  из вот этих вот двух клеток, это маловероятное событие. То есть вам нужно, чтобы f у них совпали,
[47:32.640 --> 47:37.960]  чтобы f от x равно было f от y для какого-то другого элемента y. И еще чтобы у них вот эти вот две
[47:37.960 --> 47:49.040]  ячейки тоже совпали. Одна из них должна совпасть с одной из ячейок для y. Это вероятность дважды
[47:49.040 --> 47:55.440]  маловероятная, что во-первых у вас f совпало, во-вторых у вас hash совпало, грубо говоря. Ну поэтому
[47:55.440 --> 48:01.760]  типа работает хорошо. Опять у нас бывают false positives, но с маленькой вероятностью. Здесь каких-то
[48:01.760 --> 48:07.760]  конкретных оценок я не знаю, но опять же на практике можно поэкспериментировать, да, там какой
[48:07.760 --> 48:12.960]  размер подобрать. Ну размер надо опять наверное брать порядка в два раза большего числа элементов,
[48:12.960 --> 48:19.360]  вставляемых в структуру. Ну и там чем, чем меньше вы хотите вероятность ошибки, тем опять же больше
[48:19.360 --> 48:38.280]  надо расширять ваш массив. Вот такие дела. Так, что по времени? Хорошо. Ну все тогда, значит hash
[48:38.280 --> 48:55.480]  таблицы мы закончили. Последняя структура у нас это skip list. Это как-то переводится на русский,
[48:55.480 --> 49:03.960]  типа список с пропусками, но я буду говорить skip list, потому что это быстрее. Значит,
[49:03.960 --> 49:10.760]  что она умеет? Это на самом деле в каком-то смысле вариация дерева поиска. Давайте так
[49:10.760 --> 49:16.320]  очень грубо скажу, это вариация дерева поиска. То есть это структура, которая опять-таки позволяет
[49:16.320 --> 49:24.800]  вам хранить какое-то множество элементов и обрабатывать запросы к нему как обычно. У нас insert,
[49:24.800 --> 49:42.200]  erase, find. Вот. И еще запросы, которые используют порядок на множестве U. Значит, запросы использующие
[49:42.200 --> 49:57.240]  порядок. Порядок на U. Чем hash таблицы хуже деревьев поиска? С точки зрения симптотики ничем,
[49:57.240 --> 50:03.240]  даже сильно лучше, но они очень сильно забивают на порядок элементов, которые были на U. Скажем,
[50:03.240 --> 50:09.280]  если вы вставили 5 и 10, то они могут находиться в произвольных местах hash таблицы. И, ну скажем,
[50:09.280 --> 50:14.120]  вот если вас спрашивают, там, следующее за пятеркой число, например, у вас там вставлялись какие-то
[50:14.120 --> 50:20.520]  числа, были 5 и 10, вам надо узнать какое минимальное число больше чем 5. Вот скажем, таблицы такого
[50:20.520 --> 50:24.800]  эффективно реализовать нельзя. Вам в худшем случае надо всю таблицу пройти в поисках минимального числа
[50:24.800 --> 50:29.400]  больше чем 5. Вот. А деревья поиска это умеют эффективно отвечать. То есть все запросы, связанные с
[50:29.400 --> 50:33.760]  порядком элементов, там, меньше, больше, сколько элементов меньше и так далее, это дерево поиска
[50:33.760 --> 50:39.200]  легко обрабатывает. Ну, собственно, скиплист тоже одна из вариаций, которая умеет поддерживать какую-то
[50:39.200 --> 50:45.440]  структуру на элементах, какой-то порядок на элементах. Вот. И здесь порядок, на самом деле,
[50:45.440 --> 50:54.800]  будет прям явно выписан. Значит, давайте какой-нибудь пример. А у нас будет такая
[50:54.800 --> 51:01.640]  многоуровневая структура. На нижнем уровне будет просто односвязанный, отсортированный список
[51:01.640 --> 51:07.440]  всех элементов. Давайте я вот так вот нарисую. У меня будут фиктивные элементы слева и справа,
[51:07.600 --> 51:12.840]  минус бесконечность плюс бесконечность. Давайте напишем минус бесконечность. Дальше у меня будет
[51:12.840 --> 51:18.080]  просто односвязанный список, который поддерживает все числа изец в отсортированном порядке. Ну,
[51:18.080 --> 51:34.000]  давайте какой-нибудь пример. 2, 5, 7, 10, 13, 20. Вот и все заканчивается фиктивным элементом плюс
[51:34.000 --> 51:38.320]  бесконечность. То есть на нижнем уровне у меня отсортированный список всех элементов S. Вот все
[51:38.320 --> 51:44.080]  вот это вот, это S, и больше ничего в S нет. То есть я храню по факту просто отсорченную версию S.
[51:44.080 --> 51:55.000]  Дальше. Это у меня нижний уровень, первый уровень. Дальше, на втором уровне я хочу сделать список
[51:55.000 --> 52:00.040]  с пропусками. Я хочу построить тот же самый список, но выкинуть из него примерно половину
[52:00.040 --> 52:04.840]  элементов. Ну, давайте я нарисую красивую картинку. Я выкидываю каждый второй элемент.
[52:04.840 --> 52:11.240]  Значит у меня минус бесконечность, плюс бесконечность остаются, крайний элемент у меня всегда есть. Вот.
[52:11.240 --> 52:16.120]  Давайте построим такой же список с выкидыванием каждого второго элемента. Соответственно двойку
[52:16.120 --> 52:23.840]  я игнорирую, здесь пропускаю, но пишу здесь пятерку. Семерку пропускаю, пишу десятку. Тринадцатку
[52:23.840 --> 52:31.840]  пропускаю, пишу двадцатку. Ну и получается у меня вот такой вот список. Такой односвязанный список.
[52:31.840 --> 52:37.760]  Причем я как бы не просто какие-то элементы удаляю, какие-то оставляю. Я еще и как бы запоминаю,
[52:37.760 --> 52:42.080]  что эта штука ссылается вот сюда. То есть на самом деле у меня для этих элементов будет еще ссылочка
[52:42.080 --> 52:48.200]  вниз. То есть указать не только вправо на следующий элемент того же списка, но еще и на соответствующий
[52:48.200 --> 52:52.920]  ему элемент списка на уровень ниже. То есть вот эта пятерка ссылается на эту пятерку, десятка на
[52:52.920 --> 53:00.800]  десятку, двадцатка на двадцатку. Ну и здесь тоже будут такие ссылочки вниз. Вот дальше еще один
[53:00.800 --> 53:07.360]  уровень выше. Это тоже будет какой-то подсписок вот этого списка 5, 10, 20, из которого выкинута
[53:07.360 --> 53:13.480]  примерно половина элементов. Ну давайте я там опять что-нибудь нарисую. Скажем пятерку я оставлю,
[53:13.480 --> 53:21.400]  десятку удалю и двадцатку тоже оставлю. Вот так вот мне захотелось. Здесь такие ссылки, здесь такие
[53:21.400 --> 53:26.840]  ссылки. И как обычно не забываем, из каждого элемента верхнего слоя проводим стрелку в элемент
[53:26.840 --> 53:36.160]  нижнего слоя, который ему соответствует. Ну и давайте последний у меня будет уровень, где хранится
[53:36.160 --> 53:44.520]  например только двадцать. Это будет аж вот такой вот список, всего лишь с одним содержательным
[53:44.520 --> 53:55.200]  элементом двадцать. Все, вот этот скип-лист. Еще раз, на нижнем уровне у меня просто отсороченный
[53:55.200 --> 54:01.840]  список элементов S, а на каждом более высоком, то есть при переходе с ИТ-го уровня на И плюс первой,
[54:01.840 --> 54:06.800]  у меня остается примерно половина элементов из тех, что были на ИТ-ом уровне. То есть при переходе
[54:06.800 --> 54:13.400]  вверх я примерно половину отбрасываю и оставляю под множество у тех, которые остались в том же
[54:13.400 --> 54:19.440]  порядке. То есть я завожу новый список на этом под множестве, связываю их вот в такую цепочку и
[54:19.440 --> 54:23.880]  еще при этом ссылаешь на то, кто откуда пришел. То есть что эта пятерка соответствовала этой пятерке,
[54:23.880 --> 54:28.160]  это десять это вот этот элемент, двадцать вот этот элемент, плюс бесконечность это вот этот
[54:28.160 --> 54:37.040]  элемент. То есть у меня есть ссылки не только вправо, но и вниз. Ну давайте, например, обсудим как
[54:37.040 --> 54:46.400]  теперь делать find. Давайте попробуем реализовать find какой-нибудь. Что здесь интересно? Ну find
[54:46.400 --> 54:52.760]  десять давайте попробуем. Find у нас всегда будет начинаться вот из этой вот клетки, самой левой,
[54:52.760 --> 54:59.720]  самой верхней. Смотрите, skip list нужен для того, чтобы использоваться вот эти пропуски. То есть,
[54:59.720 --> 55:05.440]  например, смотрите, вот если бы у меня, например, find был бы числа, который скажем 30, то я мог бы
[55:05.440 --> 55:10.120]  сразу смело отсюда перепрыгнуть вот сюда вот и соответственно все, что между ними лежало,
[55:10.120 --> 55:14.640]  если бы у меня число было больше чем 20, я мог бы весь этот кусок памяти сразу не просматривать. Все
[55:14.640 --> 55:20.160]  вот эти элементы я пропустил, я стоял бы сразу здесь. Но поскольку 20 больше чем 10, то я такой
[55:20.160 --> 55:25.440]  прыжок совершить не могу, значит я как бы в этом списке, ну как бы все исчерпал, я не могу идти
[55:25.440 --> 55:31.120]  вправо в этом списке, поэтому я иду вниз, спускаюсь сюда. Теперь могу ли я отсюда переместиться сюда?
[55:31.120 --> 55:36.680]  Могу ли я пропустить вот этот кусок массива? Да, могу, потому что 5 меньше чем 10, и если 10
[55:36.680 --> 55:41.320]  где-то в массиве есть, то точно правее этой пятерки. Поэтому все вот это я могу скипнуть и перепрыгнуть
[55:41.320 --> 55:46.920]  отсюда вот сюда. Теперь то же самое, могу ли я продолжиться направо в этом списке? Могу ли я перейти
[55:46.920 --> 55:52.840]  в 20? Ну не могу, потому что 20 слишком большое, значит здесь я пропускать это не могу, я спускаюсь
[55:52.840 --> 55:57.640]  просто вниз. Ну и вот уже отсюда я вижу, из пятерки вижу 10, вот она десятка, я ее нашел.
[55:57.640 --> 56:05.760]  Вот и так собственно будет у меня работать find. Я сначала встаю сюда и пока могу, прыгаю вправо,
[56:05.760 --> 56:11.560]  то есть пока могу, делаю самые большие возможные вот эти вот скачки, и если я такой скачок сделал,
[56:11.560 --> 56:15.200]  то я действительно ничего не потерял. Я перехожу в число больше либо равное, чем то, что я ищу,
[56:15.200 --> 56:20.360]  навсегда прыжок, perdон, в число меньше либо равное, чем то, что я ищу, но если я сделал прыжок,
[56:20.360 --> 56:25.720]  то мое число, вот это вот которое я ищу, лежит где-то справа, то есть я такой прыжок точно могу
[56:25.720 --> 56:31.200]  сделать. А если я его не делал, то я иду вниз, то есть я спускаюсь в список чуть-чуть более
[56:31.200 --> 56:41.440]  подробный, где больше элементов находится, и уже делаю прыжки там. Вот такая идея. Так,
[56:41.440 --> 57:07.440]  давайте что-нибудь напишу. Какие-нибудь буквы. Так, значит храним несколько связных списков,
[57:07.440 --> 57:27.240]  несколько отсортированных связных списков. Как работает find? Встаем в левую верхнюю точку,
[57:27.240 --> 57:40.320]  то есть встаем в начало самого верхнего списка. Дальше, пока можем двигаемся вправо,
[57:40.320 --> 57:57.480]  если не можем, прыгаем вниз. То есть пока можем, двигаемся вправо, пока можем, то есть пока я могу
[57:57.480 --> 58:04.560]  сделать такой прыжок без потери х, то есть пока ключ, лежащий в следующей ячейке, меньше либо равен х.
[58:04.560 --> 58:21.080]  Если не можем, двигаемся вниз. Если не можем, спускаемся вниз. Ну, если нашли х,
[58:21.080 --> 58:32.880]  останавливаемся, говорим, что х найден. Так, понятно, как find работает? Ну и более-менее тогда
[58:32.880 --> 58:37.720]  понятно, что это работает типа за что-то похожее на логарифм. Потому что если у меня на каждом
[58:37.720 --> 58:42.280]  уровне, как бы при подъеме снизу вверх, если у меня на каждом уровне число элементов вдвое
[58:42.280 --> 58:47.800]  примерно уменьшается, то у меня количество этих скип-листов примерно логарифм. На раз в два раза,
[58:47.800 --> 58:55.720]  на двойку каждый раз делится количество. Вот. Ну и почему скачков будет логарифм?
[58:55.720 --> 59:03.160]  Ну понятно, более-менее. Давайте подумаем.
[59:03.160 --> 59:18.120]  То есть смотрите, в идеальном мире, если бы у меня реально каждый второй элемент был бы на
[59:18.120 --> 59:24.280]  следующем уровне, то здесь все, здесь каждый чётный, здесь каждый с номером, делящимся на 4,
[59:24.940 --> 59:31.760]  так далее, то по факту то, что я делаю, – это поиск, это по факту смотри на двоичную запись
[59:31.760 --> 59:39.360]  моего числа, потому что вот, как бы, ну прыжок, то есть смотрите, прыжок на нижнем уровне – это
[59:39.360 --> 59:44.920]  плюс один к номеру, плюс один, плюс один, плюс один. Прыжок на этом уровне – это плюс два.他们
[59:44.920 --> 59:49.320]  на этом уровне, плюс 4, плюс 8 и так далее. Значит соответственно, если я бошел сверху вниз,
[59:49.320 --> 59:53.300]  то я по факту просто раскладываю моё число в двойечную систему дел Mag Jing и делаю те скачки,
[59:53.300 --> 59:58.180]  которые нужны. То есть если, скажем, мне нужно, там, не знаю, 18 число, я сначала сделал прыжок
[59:58.180 --> 01:00:02.700]  плюс 16, потом спустился вниз, сделал прыжок плюс 2. Соответственно, прыжков будет столько,
[01:00:02.700 --> 01:00:10.740]  какая степень двойки максимально не происходящая n. Понятна идея? Вот, ну все, значит, ответ
[01:00:10.740 --> 01:00:19.900]  примерно за алгоритм. Вот, хорошо. Значит, это с файндом разобрались. Значит, что делать с инсертом?
[01:00:19.900 --> 01:00:29.020]  Давайте с рейзом сначала, с рейзом проще. Давайте сначала запустим файнд, потом, значит, мы найдем
[01:00:29.020 --> 01:00:34.500]  элемент, который надо удалить. Вот, скажем, в этом случае, если бы у меня был rs10, я бы его нашел,
[01:00:34.500 --> 01:00:40.180]  причем я бы не просто его нашел, а нашел бы самое верхнее его вхождение вот этого числа. А то есть,
[01:00:40.180 --> 01:00:44.820]  ну, как раз где эта десятка входит на самом верхнем уровне. Я до сего дошел, мне нужно его удалить.
[01:00:44.820 --> 01:00:49.540]  Давайте его удалим из этого списка, а я умею удалять из списка. Я знаю предыдущий, я знаю следующий,
[01:00:49.540 --> 01:00:55.620]  это стираю и эту стрелку объединяю. Удалять из списка мы умеем. Значит, это удалил, спустился вниз,
[01:00:55.620 --> 01:01:02.340]  это надо удалить. Я знаю предыдущего, я знаю следующего, стер, стрелки перепровел. Поэтому
[01:01:02.340 --> 01:01:14.860]  rs это просто файнд плюс спуск вниз и удаление из списков. Значит, rsx это файнд плюс, значит,
[01:01:14.860 --> 01:01:30.620]  а затем удаление из списков, из списков и спуск вниз, и спуск до самого низа, до самого низа.
[01:01:30.620 --> 01:01:42.820]  Да, видимо, да, видимо скорее да, потому что мне нужно знать. Сейчас, давайте подумаем, надо ли это.
[01:01:42.820 --> 01:02:00.260]  То есть, казалось бы, можно было бы и без этого, наверное. Но зачем?
[01:02:00.260 --> 01:02:08.460]  Загрузили.
[01:02:08.460 --> 01:02:21.820]  Из односвязного? Ну, вот, кажется, не умеем. То есть, если я знаю элемент,
[01:02:21.820 --> 01:02:30.700]  надо знать предыдущий в любом случае. Смотрите, вот, да, вот на самом деле, как бы, вот в этом случае,
[01:02:30.700 --> 01:02:35.300]  смотрите, я когда дошел до этой десятки, я пришел в нее отсюда, поэтому я знаю предыдущий. Поэтому
[01:02:35.300 --> 01:02:39.860]  я могу эту стрелку просто перепривязать. Значит, когда я спустился вниз, на самом деле, видимо,
[01:02:39.860 --> 01:02:44.220]  ну, в нормальный, если бы мы хранили односвязанный список, мне нужно было бы и вот отсюда тоже
[01:02:44.220 --> 01:02:47.500]  спуститься и дойти до этой десятки через эту семерку, соответственно, я бы знал предыдущий.
[01:02:47.500 --> 01:02:51.940]  Тогда мне не нужно было бы хранить указатели назад, и асимплотик бы не испортился. Но давайте,
[01:02:51.940 --> 01:02:55.460]  да, давайте я для удобства скажу, что у меня есть указатели в обе стороны, соответственно, я знаю,
[01:02:55.460 --> 01:03:01.540]  кто у меня был слева, я знаю, кто у меня лежит справа, я могу это сократить. Вот, но можно,
[01:03:01.700 --> 01:03:06.620]  значит, я утверждаю, что можно оптимизировать так, чтобы хранить только именно односвязанный список,
[01:03:06.620 --> 01:03:12.060]  только вправо. Вот, но это тонкость. Давайте для удобства считать, что есть стрелка в обе стороны.
[01:03:12.060 --> 01:03:19.940]  Так, значит, кроме этого и Рейс понятно, как делать? Хорошо, значит, теперь инсерт остался.
[01:03:19.940 --> 01:03:29.620]  Значит, с инсертом идеологически все очень похоже, значит, я встал сюда,
[01:03:30.180 --> 01:03:36.100]  спустился вниз, прыгнул, спустился вниз, прыгнул, прыгнул еще раз, спустился спустился, прыгнул,
[01:03:36.100 --> 01:03:41.860]  спустился и говорю, что вот здесь должен быть x, то есть у меня, скажем, здесь число меньше x,
[01:03:41.860 --> 01:03:49.660]  здесь число больше x, вот, значит, сюда его должен вставить. Вот. Ну, понятно, в этот список я его
[01:03:49.660 --> 01:03:52.820]  как-нибудь могу вставить, это просто. Я знаю предыдущий, я знаю следующий, и сюда его вставляю.
[01:03:52.820 --> 01:03:58.580]  Теперь идея такая. Мне хотелось бы, чтобы у меня на каждом уровне было в 2 меньше, ну как бы,
[01:03:58.580 --> 01:04:03.380]  на следующем уровне был вдвоем меньше элементов, чем на предыдущем. Надо что-то сделать с x.
[01:04:03.380 --> 01:04:11.500]  Давайте сделаем, не думая. Давайте скажем, что мы подбросим монетку, если выпало единичкой,
[01:04:11.500 --> 01:04:19.580]  то я его добавлю вот сюда. Если нолик, то не добавляю, он просто здесь. Если я его сюда добавил,
[01:04:19.580 --> 01:04:24.900]  опять подбрасываю монетку, если выпало единичка, я опять x поднимаю на уровень вверх и добавляю его
[01:04:24.900 --> 01:04:31.780]  вот сюда, в список. Список на один повыше. Опять бросаю монетку, если да, добавил сюда. Дожидаюсь
[01:04:31.780 --> 01:04:42.300]  выпадения нуля. Если здесь выпала монетка нулем, то прекращаю на верхней иду. Все. Ну, смотрите,
[01:04:42.300 --> 01:04:47.860]  мы пока здесь шли, мы можем запомнить, какие последние элементы были на каждом шаге. Вот это,
[01:04:47.860 --> 01:04:54.340]  вот это, вот это, вот это, вот это и вот это. Я здесь добавляю вот такую стрелку в этот x,
[01:04:54.340 --> 01:04:58.500]  дальше откатываюсь назад в рекурсии, помню, что я отсюда, ну как бы здесь был последним,
[01:04:58.500 --> 01:05:04.140]  поэтому здесь добавил x. Здесь отсюда провожу стрелку, здесь отсюда провожу стрелку. Нет
[01:05:04.140 --> 01:05:07.020]  проблем. Ну, соответственно, они также все связаны вот в такой вертикальный список.
[01:05:07.020 --> 01:05:18.660]  Значит, как работает insert? Insert x. Сначала вставляем на нижнем уровне, вставляем на нижнем уровне.
[01:05:18.660 --> 01:05:30.500]  Дальше давайте напишу так, пока монетка выпадает единицей, пока монетка выпадает единицей,
[01:05:30.500 --> 01:05:47.340]  добавляем x в список на уровень повыше. Добавляем x в список на уровень выше.
[01:05:47.340 --> 01:06:05.460]  Вот. Ну и тогда как раз мы добьемся того, что хотели, что у меня на нижнем уровне будет
[01:06:05.460 --> 01:06:10.740]  все элементы храниться, вообще все элементы множества. На втором снизу будет примерно половина,
[01:06:10.740 --> 01:06:15.340]  да, потому что для скольких элементов вот здесь вот монетка выпала единицей, ну для половины
[01:06:15.340 --> 01:06:20.060]  примерно, половину нули, половину единицы. Поэтому на втором уровне примерно n пополам
[01:06:20.060 --> 01:06:24.660]  элементов. Здесь примерно n на 4, потому что чтобы элемент попал сюда, мне нужно, чтобы два раза
[01:06:24.660 --> 01:06:29.260]  выбросшая монетка выпала единицей. Это происходит в разности у 1 четвертая. Поэтому здесь будет
[01:06:29.260 --> 01:06:33.420]  примерно четверть всех элементов. Здесь, соответственно, еще в два раза меньше 1 восьмая, ну и так далее.
[01:06:33.420 --> 01:06:39.900]  То есть теперь у меня уже будет неточная оценка, да, не всегда на этом уровне n делить на 2 в степени
[01:06:39.900 --> 01:06:44.620]  элементов, а примерно, ну в среднем там как раз столько элементов. Там математическое ожидание
[01:06:44.620 --> 01:06:52.380]  что элементов будет такое как надо. Вот, ну и соответственно можно сказать, что при такой реализации
[01:06:52.380 --> 01:07:07.020]  все запросы работают за логарифом в среднем. Отвержение. Все запросы обрабатываются
[01:07:07.020 --> 01:07:34.020]  за логарифом в среднем. Вот. Кажется нет. Вы про то, что не амортизирован или?
[01:07:37.020 --> 01:07:41.020]  У нас, по сути, все оценки не от текущего количества элементов, а от максимального количества элементов,
[01:07:41.020 --> 01:07:51.020]  которое было сделано по памяти. Разве? Нет. У нас выразится все много списков, в которых будет один элемент.
[01:07:51.020 --> 01:08:00.020]  А почему один элемент в них будет? Ну, по удалению все элементы примерно одного, а в какой-то момент их было дофига.
[01:08:00.020 --> 01:08:09.020]  Не, ну так у меня высота каждой башни, для каждого икса, количество списков, в которых она есть, оно не зависит от остальных.
[01:08:09.020 --> 01:08:16.020]  Там то, что мы их добавляли, удаляли, у меня высота не меняется от этого. Каждая конкретная башня, она не зависит от остальных.
[01:08:16.020 --> 01:08:23.020]  Поэтому то, что вы остальные удаляли, оно не влияет на высоту икса. И она такая же, как если бы он был один вообще.
[01:08:23.020 --> 01:08:35.020]  Вы про то, что не будет ли, скажем, вот такого, сейчас, вы про вот такую картинку, когда здесь минус бесконечность, здесь плюс бесконечность?
[01:08:35.020 --> 01:08:41.020]  Да, может быть столбисно так сами.
[01:08:41.020 --> 01:08:51.020]  Еще раз, если у вас x, а, окей. Ну, короче, нет, подождите.
[01:08:51.020 --> 01:09:05.020]  Я понял. Это справедливое замечание. Можно сказать тогда, что если у меня есть верхний, то есть смотрите, если у меня есть пустой список вот такой, который ведет из минус бесконечности в плюс бесконечность,
[01:09:05.020 --> 01:09:10.020]  его можно удалить. Понятно, этот список можно отбросить и сократить всю высоту на один.
[01:09:10.020 --> 01:09:22.020]  Тогда, если у вас один х какой-то был высокий, то он был высокий сам по себе. И он был высокий независимо от того, что здесь были какие-то элементы, здесь были какие-то элементы.
[01:09:22.020 --> 01:09:31.020]  Поэтому от того, что вы их вставили или удалили, у вас мотожидание высоты этой башни будет все равно единица. И, соответственно, и память будет тоже единица в среднем.
[01:09:31.020 --> 01:09:40.020]  Если вы как раз удалите все эти заведомо ненужные штуки крайние, тогда будет без лишней поправки.
[01:09:40.020 --> 01:09:47.020]  Я поверю в мотожидание, потому что в связи с удалинками двадцатки был столбец у четырех элементов.
[01:09:47.020 --> 01:09:55.020]  Да, но еще раз, смотрите, я когда вставлял двадцатку, двадцатка высокая не потому что... А, я понял, я понял, про что вы говорите.
[01:09:55.020 --> 01:10:01.020]  Еще раз? Это сложно проверять, да, это очень сложно проверять.
[01:10:01.020 --> 01:10:14.020]  Это не страшно. Я понял, давайте я так скажу. Я когда вот здесь проговаривал, как я строю скиплист, я говорил, что у меня здесь много n элементов, здесь n пополам, здесь n на четыре.
[01:10:14.020 --> 01:10:23.020]  Я каждый второй добавлял. На самом деле я так не делаю. На самом деле я для каждого элемента подбрасываю монетку и добавляю на уровень выше, если выпало единица.
[01:10:23.020 --> 01:10:34.020]  Вот, поэтому эта двадцатка, она бы была, ну типа, то есть в этом случае она обязательно будет высоты четыре, потому что я каждый второй добавлял.
[01:10:34.020 --> 01:10:42.020]  А в нормальном случае она бы была высоты в среднем один. Вот. Тогда как раз таких бы проблем не было.
[01:10:42.020 --> 01:10:44.020]  Вот. Да.
[01:10:44.020 --> 01:10:54.020]  Еще раз?
[01:10:54.020 --> 01:11:08.020]  Да, да, да, возможно. То есть если х очень высокий, скажем, вот до сюда долез, то нам придется добавить новый список. Вполне возможно.
[01:11:08.020 --> 01:11:12.020]  Еще раз?
[01:11:12.020 --> 01:11:30.020]  Это все можно не думать. Все происходит настолько редко, что это не повлияет ни на что. Вот если вы просто в тупую бросаете монетку, то у вас не будет таких очень высоких башен.
[01:11:30.020 --> 01:11:34.020]  Ну, типа, бывают редкие логарифмические, но больше логарифма не будет.
[01:11:34.020 --> 01:11:45.020]  Так, хорошо. Последнее, что я здесь хочу сказать, это, например, про, собственно, про порядок.
[01:11:45.020 --> 01:12:00.020]  А, нет, сейчас, более важная вещь. Зачем я это вообще рассказал? Зачем нам это, если у нас есть деревья поиска? Пока что никакого выигрыша нет.
[01:12:00.020 --> 01:12:08.020]  Зачем это? Бонус этой штуки в том, давайте замечание, скип-лист хорошо распараллеливается.
[01:12:08.020 --> 01:12:29.020]  Что это значит? Вот представьте, что у вас есть одна большая структура, и у вас есть много процессоров, которые готовы обрабатывать запросы, поступающие к этой структуре.
[01:12:29.020 --> 01:12:34.020]  Не знаю, у вас какая-то большая база данных, и у вас много компьютеров, готовые отвечать на запросы.
[01:12:34.020 --> 01:12:43.020]  Тогда эта штука хорошо параллелится в том смысле, что мы можем отдавать запросы разным процессорам, разным компьютерам,
[01:12:43.020 --> 01:12:47.020]  и они могут почти независимо друг от друга на все отвечать.
[01:12:47.020 --> 01:12:57.020]  Ну, например, вот представьте, что у вас пришел блок из десяти инсертов. Там инсерт х, инсерт у и так далее. Пришло куча инсертов.
[01:12:57.020 --> 01:13:02.020]  Давайте мы все эти инсерты считали, раздали десяти компьютерам, и они параллельно независимо это делают.
[01:13:02.020 --> 01:13:08.020]  Если каждый работает за логарифом, тогда время будет не десять логарифом, а один логарифом.
[01:13:08.020 --> 01:13:13.020]  Если они все параллельно работают, то время не умножается, а распараллеливается в каком-то смысле.
[01:13:13.020 --> 01:13:20.020]  И скип-лист идеально подходит на то, чтобы распараллелиться.
[01:13:20.020 --> 01:13:24.020]  Потому что смотрите, в каждый момент времени, по факту, что мне делает инсерт?
[01:13:24.020 --> 01:13:28.020]  Он как-то там ходит, ходит, ходит. Вот здесь, давайте здесь.
[01:13:28.020 --> 01:13:32.020]  Шли, шли, шли. Потом взяли и сюда добавили х.
[01:13:32.020 --> 01:13:35.020]  Потом поднялись и сюда добавили х и так далее.
[01:13:35.020 --> 01:13:40.020]  Но главное, что в каждый момент времени у вас будет корректный скип-лист.
[01:13:40.020 --> 01:13:45.020]  То есть пока у вас, скажем, один процессор спускается сюда, вставил сюда один х,
[01:13:45.020 --> 01:13:50.020]  другой в каком-то другом месте спустился еще на последний уровень, добавил у.
[01:13:50.020 --> 01:13:54.020]  Потом этот сюда поднялся, добавил сюда х, тот куда-то еще поднялся, тоже добавил у.
[01:13:54.020 --> 01:13:59.020]  Каждая такая процедура, каждая вставка не меняет корректности скип-листа.
[01:13:59.020 --> 01:14:03.020]  То есть, скажем, если бы у меня этого всего не было, это был бы тоже корректный скип-лист.
[01:14:03.020 --> 01:14:08.020]  Потом я поднимаюсь на уровень вверх, этот добавляю, это будет тоже корректный скип-лист и так далее.
[01:14:08.020 --> 01:14:22.020]  Поэтому, если у меня несколько конкурирующих процессоров друг с другом соревнуются и делают разные процедуры, то в любой момент времени все хорошо, у меня корректный скип-лист, поэтому я могу над ним независимо делать вот эти операции.
[01:14:22.020 --> 01:14:30.020]  Вставлять сюда, туда, сюда, увеличивать эту башню и так далее. Я все это распараллеливаю, они друг с другом не конфликтуют.
[01:14:30.020 --> 01:14:40.020]  В отличие, скажем, от дерева поиска какого-нибудь. Потому что если вы в дерево поиска хотите что-то вставить, удалить, вы сначала спускаетесь, потом делаете повороты.
[01:14:40.020 --> 01:14:49.020]  Вы делаете этот поворот, когда вы повернули, у вас, возможно, какой-то элемент изданного по дереву пошел в другое. И, соответственно, тот процессор должен был пойти не туда, а сюда, грубо говоря.
[01:14:49.020 --> 01:15:03.020]  Если нарисовать повороты, то там как раз повороты друг с другом очень сильно конфликтуют. Если вы пошли в одно по дереву, надо в другое, вы что-то повернули, там все пошло по одному месту.
[01:15:03.020 --> 01:15:16.020]  А здесь все хорошо, в любой момент времени у вас корректный скип-лист. Даже когда вы вставляете снизу вверх или когда вы удаляете сверху вниз, то есть когда я рыс делал, мы удаляли сверху вниз, у нас в каждом момент времени корректный скип-лист.
[01:15:16.020 --> 01:15:22.020]  И тогда они друг другу не мешают. Вот такая прелесть.
[01:15:22.020 --> 01:15:42.020]  Тройку, четверку сразу? Ну а что, какая проблема? Дошли до сюда. Один, скажем, поставил сюда 4, другой поставил сюда 3.
[01:15:42.020 --> 01:16:01.020]  Дальше этот поднимается сюда, смотрит на предыдущего, проводит такую ссылку. Этот поднимается сюда и видит уже в этот момент, что он поднялся сюда, и тут оказалось четверка.
[01:16:01.020 --> 01:16:10.020]  Здесь у меня минус, здесь 4, я сюда вставляю тройку и вот так вот стрелочки перепривязываю. Кажется, проблем не будет никаких.
[01:16:10.020 --> 01:16:20.020]  Еще раз, от того, что после каждой конкретной встатки у вас корректный скип-лист, у вас все эти связочки не конфликтуют.
[01:16:20.020 --> 01:16:43.020]  Если вот по этому списку идти или что? Так я это не так делаю. Смотрите, у меня find также работает как insert. Я сначала встаю в левую верхнюю точку, потом иду по верхнему списку пока могу.
[01:16:43.020 --> 01:16:51.020]  Я не по нижнему списку, я, например, по верхнему иду в первую очередь. Тогда будет логарифм как раз.
[01:16:51.020 --> 01:16:57.020]  Ну и, например, почему эта штука поддерживает порядок? Ну понятно, у нас здесь на нижнем уровне все элементы отсорчены.
[01:16:57.020 --> 01:17:07.020]  И, например, если бы у нас был запрос, например, next x, который просил бы меня найти минимальное число, которое больше, чем x.
[01:17:07.020 --> 01:17:25.020]  Ну тогда понятно, давайте найдем x и возьмем следующее от него. За счет того, что у меня на нижнем уровне все отсорчено, и вообще на каждом уровне все отсорчено, то всякие такие запросы, связанные с порядком, меньше, больше и так далее, они переделываются на скип-лист.
[01:17:25.020 --> 01:17:41.020]  Более того, можно, например, хранить длины всех вот этих вот скачков, можно ввести дополнительное поле в каждой ячейке и хранить не просто указатель на следующего чувака, а сколько элементов между ними было и сколько элементов я пропустил.
[01:17:41.020 --> 01:17:47.020]  В этом случае было бы у меня 1, 2, 3, 4, 5 элементов между ними, и здесь был бы параметр 5.
[01:17:47.020 --> 01:18:00.020]  И тогда можно показать, что все эти количества можно поддерживать, при инсертах и рейзах их можно поддерживать, и тогда мы могли бы не просто узнавать следующий, но и по номеру узнавать элемент.
[01:18:00.020 --> 01:18:10.020]  Узнать катую порядковую статистику в нашем множестве. Если нам нужна четвертая, то 5 это слишком много, я спускаюсь сюда. Вот здесь могу сделать прыжок, потому что здесь всего один, это вторая статистика.
[01:18:10.020 --> 01:18:15.020]  Спускаюсь, спускаюсь, ну в общем, вот как раз десятка была бы вот эта, потому что здесь еще было бы плюс один.
[01:18:15.020 --> 01:18:20.020]  Короче, если бы мы хранили длины прыжков, то могли бы, например, еще искать катую порядковую статистику в этом порядке.
[01:18:20.020 --> 01:18:23.020]  Как нам ее поддерживать?
[01:18:23.020 --> 01:18:29.020]  Ну вот можно, можно, я утверждаю, что можно при инсерте и рейзе эти числа хорошо пересчитывать.
[01:18:29.020 --> 01:18:31.020]  Утверждение.
[01:18:31.020 --> 01:18:33.020]  Ну все, спасибо большое за внимание.

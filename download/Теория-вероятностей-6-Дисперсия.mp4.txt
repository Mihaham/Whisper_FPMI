[00:00.000 --> 00:18.080]  да давайте начинать здравствуйте друзья значит в общем ситуация такова что мы
[00:18.080 --> 00:22.080]  теперь с вами будем до конца семестра видимо проводить лекции вам они потому что я уехал
[00:22.080 --> 00:31.680]  и видимо уж как минимум до конца семестра не вернусь но надеюсь на качестве лекции это не
[00:31.680 --> 00:39.680]  скажется так значит теперь давайте к делу начнем мы с вами фроша остановились меня вообще нормально
[00:40.680 --> 00:56.440]  так но по поводу экзамена знаете сейчас вообще сложно о чем-то конкретном говорить как будет
[00:56.440 --> 01:05.440]  устроена жизнь там у нас в июне я тоже не умею это предсказывать но пока никаких официальных
[01:05.440 --> 01:10.320]  распоряжений на эту тему не было и поэтому на сегодняшний день видение такое что экзамен
[01:10.320 --> 01:15.240]  будет в обычном формате то есть он будет в аудитории если я не приеду то принимать просто
[01:15.240 --> 01:23.080]  буду не я есть много людей и семинаристов которые и других преподаватели которые в этом
[01:23.080 --> 01:27.400]  семестре не участвует в давание вероятности которые могут принять экзамен проблем с этим
[01:27.400 --> 01:32.400]  никаких нет но может быть ситуация как-то изменится но на текущий момент видение такое что экзамен будет
[01:32.400 --> 01:42.000]  в обычном формате меня нормально слышно видно да да да да прекрасно просто не очень хорошая
[01:42.000 --> 01:50.960]  интернет поэтому если какие-то сбои пожалуйста сразу говорить так ну хорошо значит мой фрош
[01:50.960 --> 01:57.360]  раз с вами остановились на понятии математического ожидания и в общем-то довольно полно о нем
[01:57.360 --> 02:02.920]  поговорили подробно давайте теперь двинемся дальше говорим про дисперсию
[02:02.920 --> 02:10.320]  слишком жирно
[02:16.720 --> 02:18.200]  красно так дисперсия
[02:18.200 --> 02:25.520]  но прежде чем я определю что такое дисперсия
[02:25.520 --> 02:34.680]  давайте я поясню какая интуиция этого понятия зачем она вообще нужна но опять же важна
[02:34.680 --> 02:42.120]  статистическая составляющая то есть статистическая страна дело такой параметр распределения который
[02:42.120 --> 02:48.040]  важен со статистической точки зрения в том смысле что если у вас есть представить себя
[02:48.040 --> 02:53.080]  обычную ситуацию чтобы вы смогли один тоже эксперимент произвести независимо много раз и
[02:53.080 --> 02:59.880]  есть у вас какие-то результаты эксперимента до которой мы воспринимаем независимо случайная
[02:59.880 --> 03:08.200]  величина вот на практике это просто там миллион чисел или тысяча чисел и вы хотите сделать вывод
[03:08.680 --> 03:17.660]  о том как распределена распределены ваша случайная величина по вот этим числам ну один
[03:17.660 --> 03:21.680]  из варианта писать средняя до узнать какое математическое ожидание случайной величины
[03:21.680 --> 03:28.080]  то есть если в эти величины творции подельное количество то вы получите приблизительно да
[03:28.080 --> 03:32.040]  если ваш крики до довольно большим то это будет высокая точность вы получите приблизительно
[03:32.040 --> 03:38.880]  математическое ожидание, случайно, речное. Вот, но понятно, что среднее значение, но оно мало
[03:38.880 --> 03:46.160]  о чем говорит, оно говорит о том, что, ну, если у вас нет никакой другой информации, то среднее
[03:46.160 --> 03:52.080]  значение просто говорит о том, что вы ожидаете в будущем вот такое вот значение, да, что вот, что
[03:52.080 --> 03:56.680]  вот вокруг него будут сосредоточены все ваши числа. А вот насколько сильно вокруг него они
[03:56.680 --> 04:03.840]  будут сосредоточены? На этот вопрос отвечает значение дисперсии. Вот дисперсия это как раз мера
[04:03.840 --> 04:09.560]  отклонения случайного лично от ее математического ожидания. Иными словами, разброс может быть очень
[04:09.560 --> 04:15.240]  большой. От того, что вы нашли ожидаемое значение, еще рано радоваться, из этого не следует, что вы
[04:15.240 --> 04:19.960]  поймете, как ваша случайная величина будет вести себя в будущем, потому что она относительно этого
[04:19.960 --> 04:25.160]  ожидаемого значения может отклоняться очень сильно. И вот эта вот мера отклонения, это есть дисперсия,
[04:25.320 --> 04:29.280]  если дисперсия маленькая, то математическое ожидание является хорошим приближением в том смысле,
[04:29.280 --> 04:34.640]  что вокруг него все будет сосредоточено. Если дисперсия большая, то вам нужно это отклонение,
[04:34.640 --> 04:40.960]  конечно, учитывать при каком-то прогнозировании. Значит, теперь что же это такое с теоретической
[04:40.960 --> 04:49.200]  точки зрения? Дисперсия случайного лично кси — это по определению математическое ожидание квадрата
[04:49.200 --> 04:54.760]  разности между случайной величиной и ее математическим ожиданием. То есть квадрат разности — это
[04:54.760 --> 05:01.360]  способ посчитать отклонение, то есть это такое квадратичное отклонение случайной величины
[05:01.360 --> 05:06.120]  от ее среднего, а потом вы еще раз берете среднее, получаете среднее квадратическое отклонение,
[05:06.120 --> 05:17.040]  ну или иными самими дисперсиями. Дисперсию можно посчитать иначе, и зачастую, как правило,
[05:17.040 --> 05:21.960]  так делать проще. Считать вот не так, как здесь написано, не по определению, а по эквивалентной
[05:21.960 --> 05:28.560]  формуле, которая легко доказывается. Значит, утверждение стоит в том, что дисперсия — это
[05:28.560 --> 05:35.120]  мат ожидания квадрата случайной величины минус квадрат мат ожидания случайной величины.
[05:35.120 --> 05:49.440]  Ну почему так? Просто надо формулой линейности, свойством линейности математического ожидания
[05:49.440 --> 05:58.620]  воспользоваться. Смотрите, берем нашу дисперсию по определению. Это есть мат ожидания от квадрата
[05:58.620 --> 06:07.220]  разности, но этот квадрат разности можно раскрыть. Что такое квадрат разности? Это
[06:07.220 --> 06:20.220]  кси квадрат минус удвоенное произведение кси и математическое ожидание кси и плюс
[06:20.220 --> 06:30.180]  квадрат математического ожидания. Дальше как раз линейность, которая говорит о том, что вы
[06:30.180 --> 06:37.240]  можете вот эту всю сумму представить в виде суммы математического ожидания. Ну еще константная
[06:37.240 --> 06:40.940]  длинность за знак математического ожидания. У вас получается от ожидания от кси квадрата
[06:40.940 --> 07:00.900]  минус. Давайте посмотрим на вот это второе слагаемое. В нём есть константы. Это двойка и мат ожидания
[07:00.900 --> 07:05.500]  кси. Это просто число, это не случайная величина, поэтому их за знак математического ожидания
[07:05.500 --> 07:12.100]  можно вывести. У вас получится два мат ожидания кси. Умножить на мат ожидания того, что осталось,
[07:12.100 --> 07:18.980]  но осталось просто кси. И плюс мат ожидания кси в квадрате.
[07:25.900 --> 07:30.020]  Вот, ну теперь просто мы уничтожаем от мат ожидания кси в квадрате. У нас
[07:30.020 --> 07:34.820]  остаётся мат ожидания кси в квадрате, и у нас остаётся мат ожидания от кси квадрата минус мат
[07:34.820 --> 07:38.780]  ожидания кси в квадрате. Что это требовалось?
[07:47.540 --> 07:54.500]  Ну давайте теперь разберём какие-то примеры, а потом поговорим о свойствах дисперсии. То есть
[07:54.500 --> 07:59.460]  давайте посчитаем просто дисперсию для основных распределений, о которых мы с вами говорили и
[07:59.540 --> 08:03.300]  когда мы мат ожидания тоже считали. Начнём в дискретных.
[08:03.300 --> 08:12.100]  Так, в какой мы там последовательности рассматриваем? Давайте в той же самой
[08:12.100 --> 08:14.140]  последовательности будем рассматривать дисперсию.
[08:14.140 --> 08:21.940]  Ага, значит, Бернольевская, биномиальная, равномерная по освобождению. Прекрасно.
[08:21.940 --> 08:27.220]  Итак, сначала Бернольевское распределение.
[08:27.220 --> 08:40.940]  Ну, чтобы найти дисперсию, мы будем в основном читать вот эту вторую формулю,
[08:40.940 --> 08:44.980]  которая в утверждении написана. Значит, поэтому, чтобы найти дисперсию, нам было неплохо найти
[08:44.980 --> 08:51.460]  мат ожидания квадрата случайно-речным. От того, что мы вернулись к случайно-речному
[08:51.460 --> 08:55.420]  изведению в квадрат, она, конечно, не поменяется, потому что у неё два значения, 0 и 1. От изведения
[08:55.420 --> 09:01.260]  нуля в квадрат, единицы в квадрат, они не меняются. Поэтому кси квадрат просто равно кси, а значит,
[09:01.260 --> 09:10.220]  мат ожидания кси квадрат равно мат ожидания кси. Как мы знаем, это P. А дисперсия – это значит
[09:10.220 --> 09:20.540]  мат ожидания кси квадрата минус квадрат мат ожидания кси. Ну, то есть, это P минус P в квадрат.
[09:20.540 --> 09:36.180]  Окей, двигаемся дальше. Равномерная на множестве чисел от 1 до n.
[09:41.220 --> 09:49.740]  Опять, сперва, найдем мат ожидания кси квадрата. Ну, смотрите, значит, если кси принимает значение 1
[09:49.740 --> 09:57.220]  и тогда m, равномерность восприятия означает, что все эти значения имеют одинаковую вероятность 1n.
[09:57.220 --> 10:03.580]  То кси квадрат принимает значение, равное квадратам этих чисел. То есть, 1, 4, 9, 16, 25 и так далее.
[10:03.580 --> 10:12.460]  Их вероятность – это 1n. То есть, на самом деле, мат ожидания кси квадрата – это будет просто сумма
[10:12.460 --> 10:21.340]  значений нашей случайночной кси квадрат. Сумма платья от 1 до n и квадрат. И умножительная вероятность,
[10:21.340 --> 10:28.300]  которая равна 1n. То есть, квадрат по 9 на n. Что равно… Сумма квадратов – это n на n плюс 1 на 2
[10:28.300 --> 10:39.740]  и 2n плюс 1 поделить на 6. Значит, получится n плюс 1 умножить на 2n плюс 1 и поделить на 6.
[10:39.740 --> 10:49.780]  Давайте еще сразу вспомним, что у нас была теорема о замене переменных в интеграле Лебеда,
[10:49.780 --> 10:55.780]  которая говорит, что если мы считаем от ожидания от функции случайного вектора или случайной
[10:56.180 --> 11:04.780]  то есть интеграл от этой функции по мере равной распределению naszej случайной верчины или firms.
[11:04.780 --> 11:11.820]  Соответственно, если у вас есть дискрепная случайная верчина и вы считаете мат ожидания
[11:11.820 --> 11:17.980]  от функции, ну например theeб怎么 ты fem, вот как мы только что делали. То есть интеграл от
[11:17.980 --> 11:24.420]  хsquad по мере равной распредел taking hits поrenched virtuality. Соверш memang disk этот будет просто
[11:24.420 --> 11:31.200]  сумма квадратов значений случайного величины uplift на вероятности этих значений. Это точно
[11:31.200 --> 11:35.680]  то, что мы с вами написали. Для дискретного распределения это и так понятно, потому что
[11:35.680 --> 11:39.700]  когда вы возводите случайную смотрему в квадрат, в квадрат возводится ее значение. То есть вы
[11:39.700 --> 11:45.420]  получаете случайную смотрему, значение которого равно квадратам, и вероятности те же самые
[11:45.420 --> 11:54.300]  остаются. В точности т albums об замене переменных в интегронAnnibale. Вы можете найти дисперсию.
[11:54.300 --> 12:05.260]  Вычитаем из квадрата мат ожидания, который равен, как мы только что посчитали, n
[12:05.260 --> 12:08.060]  плюс один умножить на 2, n плюс один поделить на 6.
[12:08.060 --> 12:16.740]  Вычитаем в квадрат математическое ожидание. Мат ожидания для равноверного мы считали это
[12:16.740 --> 12:20.340]  n плюс один пополам, значит вычитаем n плюс один пополам в квадрате.
[12:20.340 --> 12:32.820]  Так, ну если это аккуратненько посчитать, общий знаменатель будет 12, будет общий множественный
[12:32.820 --> 12:39.220]  n плюс один, и в числителе еще будет 4n плюс два минус 3n минус три, то есть получится на самом
[12:39.220 --> 12:44.340]  деле n минус один, и в итоге получается n квадрат минус один длить на 12.
[12:44.340 --> 12:51.100]  Есть какие-то вопросы?
[13:02.100 --> 13:10.060]  Вопросов нет, окей. У меня есть вопрос. Да, пожалуйста. Как долго мы будем повторять тот курс,
[13:10.060 --> 13:19.900]  который мы уже прослушали? То есть это совсем очень маленькие изменения, как-то по два раза
[13:19.900 --> 13:24.140]  одно и то же. Вы имеете в виду, что у вас уже были дискретные распределения, но дискретным
[13:24.140 --> 13:28.780]  распределением мы уделяем там 10 процентов лекций. Если это проблема, мы можем не уделять, но мне
[13:28.780 --> 13:34.300]  кажется, что за счет того, что я провожу там параллели с какими-то вещами, которые я вам
[13:34.300 --> 13:40.820]  рассказываю в первый раз, то это полезно. Но может быть кому-то кажется, что это слишком просто,
[13:40.820 --> 13:46.620]  но прекрасно на самом деле, что вы понимаете. Не переживайте, у нас всего 4 примера дискретных
[13:46.620 --> 13:50.380]  распределений. Остальные будут абсолютно непрерывные. Я так понимаю, про них Иван Георгиевич
[13:50.380 --> 14:03.020]  не говорил. Ничего страшного, еще пять минут и перейдем к абсолютно непрерывным. Дайте возможность
[14:03.020 --> 14:07.900]  тем людям, которые либо это подзабыли, либо хотят повторить. Бенмяльное распределение.
[14:07.900 --> 14:18.980]  Здесь можно читать в лоб. Я не буду этого делать, так же, как мы делали, когда читали математическое
[14:18.980 --> 14:25.580]  ожидание. Можно посчитать в лоб от ожидания касси квадрата, сделать какие-то вычисления и после
[14:25.580 --> 14:34.580]  этого найти дисперсию, которая окажется равной NP умножить на 1-P. Я сейчас напомню свойства дисперсии,
[14:34.580 --> 14:40.460]  и потом вернемся к этому вопросу. Видим, что дисперсию можно посчитать просто как сумму
[14:40.460 --> 14:45.940]  независимых, как дисперсию суммы независимых вернувских случайных увеличений. Ну и наконец
[14:45.940 --> 14:59.300]  полосоновское распределение. Опять же, если вы считаете мат ожидания квадрат случайной величины
[14:59.300 --> 15:07.780]  для полосоновского распределения, то вы либо потеряете о замене переменных, да либо понимаете,
[15:07.780 --> 15:13.460]  что значение случайной величины просто называется в квадрат, поэтому у вас получается сумма значений,
[15:13.460 --> 15:23.180]  то есть и в квадрате по и от 0 до бесконечности, умножить на вероятность, то есть на e в степени
[15:23.180 --> 15:33.380]  минус лямбда, на лямбда в степени и и потерять на e факториал. Ну опять, значит, слагаемая при
[15:33.380 --> 15:38.020]  и равном нулю исчезает, потому что она равна нулю, поэтому можно начинать суммировать с единицы,
[15:38.020 --> 15:48.740]  суммировать с единицы, и получится что? Получится и на e в степени минус лямбда,
[15:48.740 --> 16:00.380]  на лямбда в степени и и поделить на e минус 1 факториал. Ну а дальше делаем следующее. Мы первые
[16:00.380 --> 16:17.980]  множители и развиваем на сумму e минус 1 и плюс 1, и получается вот такая вот вещь. Ну и соответственно
[16:17.980 --> 16:25.620]  перепишем это дело в виде двух сумм. Первая будет сумма, значит, первая, вот когда мы возьмем слагаемая,
[16:25.620 --> 16:30.460]  в котором есть множитель e минус 1, он сократится с e минус 1 факториал. Более того, при e равно 1 вы
[16:30.460 --> 16:38.420]  получите 0, поэтому суммировать можно начинать с e равно 2 до бесконечности, e в степени минус
[16:38.420 --> 16:46.300]  лямбда на лямбда в степени и поделить на e минус 2 факториал. Ну и плюс то же самое, только сумма
[16:46.300 --> 16:57.540]  по e, начиная с единицы, и в знаменателе будет e минус 1 факториал. Далее, если вы из первого слагаемого
[16:57.540 --> 17:03.140]  вынести лямбда в квадрате, вы получите, конечно, просто ряд пейлора для экспонента, и экспонент
[17:03.140 --> 17:07.380]  сократится и останется просто лямбда в квадрате. То же самое со вторым слагаемым, который даст просто
[17:07.380 --> 17:21.780]  лямбда. Ну и значит дисперсия. Это есть мат ожидания x2 минус квадрат мат ожидания, что равно просто лямбда.
[17:28.500 --> 17:33.060]  Вот. Ну и теперь давайте перейдём к абсолютно непрерывным распределениям, тоже в той же
[17:33.060 --> 17:39.020]  последности, которые были для математического ожидания. Что мы там считали? Мы сначала считали
[17:39.020 --> 17:46.100]  равномерное. Да, нет, не здесь. Мы сначала считали вот равномерное, потом экспоненциальное,
[17:46.100 --> 17:54.260]  а потом нормальное. Давайте также сделаем равномерное распределение на отрезке от АДВМ.
[18:03.060 --> 18:10.300]  По тому же принципу сначала найдем от ожидания x2, но здесь мы в точности применяем теорему
[18:10.300 --> 18:18.980]  Либега о замене переменных. То есть интегрируем x2. Интегрировать мы должны по мере
[18:18.980 --> 18:24.100]  соответствующей равной распределению случайно вещества x. Она абсолютно непрерывная, поэтому интеграл
[18:24.100 --> 18:29.940]  Либега по этой мере превратится в интеграл Либега по классической мере Либега. Надо просто вынести
[18:29.940 --> 18:36.260]  из-под дифференциала плотность, которая равна единице поделить на b-а. Ну и на индикатор, но этот
[18:36.260 --> 18:41.340]  индикатор можно занести в пределы интегрирования и написать интеграл тадабэ. Началось вот такой
[18:41.340 --> 18:52.660]  интеграл, который равен там b в кубе минус a в кубе поделить на 3 b-а. Ну или b квадрат плюс ab
[18:52.660 --> 19:05.020]  плюс a квадрат поделить на 3. Далее дисперсия. Это мат ожидания x2 и минус квадрат мат ожидания
[19:14.020 --> 19:18.820]  минус a плюс b в квадрате поделить на 4.
[19:18.820 --> 19:27.180]  Опять, если привести к общему знаменателю, то что получится? Значит общий знаменателем будет
[19:27.180 --> 19:39.860]  12 и несложно видеть, что получится a минус b в квадрате. Вот это дисперсия с равномерным
[19:40.300 --> 19:43.060]  Хорошо, теперь экспоненциально.
[19:53.060 --> 20:02.660]  Значит, читаем мат ожидания x2. Это опять по тяреме замене перемен в интеграле бега вы просто
[20:02.660 --> 20:07.460]  интегрируете x2 в нужной на плотность случайно вечной x. То есть будет интеграл от 0 до бесконечности
[20:07.460 --> 20:13.740]  x2 лямдо e в степени минус лямдо x dx. Если вдруг кто-то не понимает, откуда берутся интегралы,
[20:13.740 --> 20:20.900]  которые я пишу, пожалуйста спрашивайте. Заносим экспоненту по дифференциал, получается минус xd
[20:20.900 --> 20:26.620]  от e в степени минус лямдо x. Дальше интегрируя по частям, первая слагаемая очевидно будет равно нулю,
[20:26.620 --> 20:35.500]  останется интеграл от 0 до бесконечности e в степени минус лямдо x dx, который равен 1 дриет на лямдо.
[20:37.460 --> 20:49.220]  Ну и следовательно дисперсия. Это есть мат ожидания x2, то есть 1 дриет на лямдо,
[20:49.220 --> 20:52.340]  минус квадрат мат ожидания, то есть 1 дриет на лямдо квадрат.
[20:52.340 --> 21:04.100]  Можно вопрос? Второй знак равенства в первой строчке. Там было слева x2, а справа x2 уже нет.
[21:04.100 --> 21:14.260]  Потому что я написал, конечно глупость, сейчас я это исправлю, спасибо большое.
[21:14.260 --> 21:20.180]  Значит да, здесь должен быть x2 и здесь должно быть 2x.
[21:27.100 --> 21:32.580]  Здесь должно быть 2x. Ну еще раз интегрируя по частям,
[21:32.580 --> 21:42.580]  мы получим интеграл от 0 до бесконечности минус 2x, дифференциал от e в степени минус лямдо x,
[21:42.580 --> 21:55.020]  еще на лямдо надо поделить. И это есть интеграл от 0 до бесконечности, 2 делить на лямдо.
[21:55.020 --> 22:15.300]  E в степени минус лямдо x dx, что равно 2 делить на лямдо квадрат. А значит дисперсия,
[22:15.380 --> 22:26.740]  это есть мат ожидания x2, то есть 2 делить на лямдо квадрат, минус квадрат мат ожидания x,
[22:26.740 --> 22:34.500]  минус 1 делить на лямдо квадрат, что равно 1 делить на лямдо квадрат.
[22:34.500 --> 22:43.980]  Еще раз почитать, нигде не ошибся.
[22:43.980 --> 22:52.900]  Дифференциал от e в степени минус лямдо x, 2x в степени минус лямдо x,
[22:52.900 --> 23:14.420]  еще раз. И нормальное распределение. Давайте посчитаем для стандартного нормального,
[23:14.420 --> 23:20.900]  а потом после того, как напишем свойства дисперсии, вернемся к нормальному распределению
[23:20.900 --> 23:30.820]  параметра мат. Ну опять нам нужен так называемый второй момент, то есть мат ожидания x2.
[23:30.820 --> 23:38.980]  Потеряемое замене переменных, это есть интеграл от x2 умножить на плотность, то есть 1 делить на
[23:38.980 --> 23:52.060]  корень из 2p, e в степени минус x квадрат пополам dx. Ну и заносим экспоненту вместе с x как множителем
[23:52.060 --> 24:02.300]  внутрь дифференциала, то есть получится 1 поделить на корень из 2px дифференциал, а еще минус
[24:02.300 --> 24:10.580]  перед этим делом дифференциал от e в степени минус x квадрат пополам. Интегрируя по частям,
[24:10.580 --> 24:16.820]  мы получим интеграл от минус бесконечности до бесконечности, 1 поделить на корень из 2p,
[24:16.820 --> 24:28.220]  e в степени минус x квадрат пополам dx. Это просто интеграл от плотности нормального распределения,
[24:28.580 --> 24:40.340]  который n единицы. А дисперсия это тогда тоже единица, потому что мат ожидания равнули,
[24:40.340 --> 24:45.860]  то есть квадрат мат ожидания минус мат ожидания в квадрате, мат ожидания в квадрате ноль,
[24:45.860 --> 24:51.900]  получаем единицу. Так, прекрасно, теперь давайте поговорим про свойства,
[24:51.900 --> 24:54.460]  после чего вернемся к двум примерам, которые мы не досчитали.
[25:05.460 --> 25:10.700]  Во-первых, для свойств мне будет полезно понятие кавариации случайных величин.
[25:10.700 --> 25:20.100]  Когда я буду считать дисперсию суммы, я ее сведу к кавариации. Кавариация это некоторая
[25:20.100 --> 25:24.220]  мера зависимости случайных величин, то есть, по сути, чем случайная величина более зависимая,
[25:24.220 --> 25:29.420]  тем больше у них кавариация. Почему эта мера зависимости, я сейчас тоже объясню чуть позже,
[25:29.420 --> 25:33.540]  когда поговорю про неравенство, потому что в униковском. Что такое кавариация? Кавариация
[25:33.540 --> 25:47.900]  случайных величин x и e, это есть мат ожидания вот такого вот произведения, x минус e x,
[25:47.940 --> 26:01.260]  это минус e x. В частности, понятно, что дисперсия это кавариация случайной величины самой собой.
[26:05.260 --> 26:10.940]  Просто по определению дисперсии и кавариации. Дисперсия это мат ожидания.
[26:11.380 --> 26:15.820]  Наверное, для нас мат ожидания это. Да, спасибо, конечно.
[26:27.820 --> 26:35.140]  Во-вторых, дисперсия, конечно, не отрицательна. Это следует из определений и свойств математических
[26:35.660 --> 26:41.320]  дисперсий, этому мат ожидания от квадрата случайной величины. То есть от не отрицательной
[26:41.320 --> 26:44.340]  случайной величины. А мат ожиданий от не lautей случайной величины,shelfvin, самому по
[26:44.340 --> 26:52.740]  себе не отрицательный. но некоторые аналог свойства линейности если вы берете дисперсию от
[26:52.740 --> 26:56.940]  константа умноженной на случайная вечноénNEAR, вот эта константа выносится из-под дисперсии в
[26:56.940 --> 27:09.900]  в квадрате, это тоже очевидным свойством следует изобреление дисперсии, если вы берете дисперсию
[27:09.900 --> 27:15.980]  от константа на кси, то, во-первых, здесь появится квадрат внутри мат ожидания первого, да и с того,
[27:15.980 --> 27:22.580]  что случайно вы считаете квадратом, во-вторых, второе тоже возойдется в квадрат, поэтому у вас будет
[27:22.580 --> 27:36.260]  ц квадрат выестся впереди дисперсии. так хорошо далее далее понятно, что к вариации можно
[27:36.260 --> 27:42.940]  посчитать иначе, значит, можно посчитать как мат ожидания произведения минус произведение мат ожидания
[27:42.940 --> 27:53.980]  он оказывается в полной аналогии с формулой дисперсии, давайте я быстренько это напишу,
[27:56.540 --> 28:06.060]  значит, квадрат от кси это, это есть мат ожидания по определению от кси минус е кси на это минус
[28:06.320 --> 28:16.460]  вы считаете это произведение и получаете мат ожидания от кси это минус кси умножить на е это
[28:16.460 --> 28:24.740]  минус это умножить на мат ожидания кси и плюс произведение мат ожиданий. но дальше раскрываете
[28:24.740 --> 28:30.560]  по линейности, раскрываете по линейности и получается очевидным образом то,
[28:30.560 --> 28:35.660]  что требуется. у вас есть три одинаковых слагаемых, у двух стоит знак минус, у третьего
[28:35.660 --> 28:44.500]  Поэтому получается, действительно, мат ожидания произведения, минус произведение мат ожидания.
[28:52.500 --> 29:03.500]  Дальше, ну теперь давайте с суммой. И что будет, если мы считаем дисперсию от суммы?
[29:03.500 --> 29:10.420]  Ну понятно, что у нас даже с сумможением на константу тут линейность не получилась с суммой,
[29:10.420 --> 29:16.540]  тем более, значит, верна следующая формула. Дисперсия суммы, n случайных величин для
[29:16.540 --> 29:29.700]  общности, это есть сумма дисперсий, плюс сумма по всем различным i и g к вариации x и t, x и g t.
[29:33.500 --> 29:47.420]  Вот, ну это тоже, в общем-то, очевидное следствие того, что первое свойство,
[29:47.420 --> 29:53.540]  что дисперсия, сейчас давайте я вернусь к доказательству этого свойства по столке,
[29:53.540 --> 29:57.780]  я еще скажу, что кавриация белинейна. Значит, это очевидное следствие того,
[29:57.780 --> 30:04.500]  что дисперсия это кавриация случайно величины самой собой, но еще надо использовать то,
[30:04.500 --> 30:09.740]  что кавриация белинейна, то есть, а именно, во-первых, асимметрично, то есть кавриация
[30:09.740 --> 30:34.100]  x и t равна кавриации x и t, а во-вторых, кавриация от a1x1 плюс a2x2t равна a1 умножить на кавриация
[30:34.100 --> 30:44.260]  x и 1t плюс a2 на кавриация x и 2t. То есть, иными словами, она линейна как по первому аргументу,
[30:44.260 --> 30:47.300]  так по второму. В силу симметричности, то же самое верно и для второго аргумента.
[30:47.300 --> 30:53.020]  Ну, с симметричностью все очевидно, просто определение симметричное, того,
[30:53.020 --> 30:56.260]  что вы перестаете две случайных величины, и у вас правая часть не изменится с того,
[30:56.260 --> 31:03.700]  что умножение коммутативно. То есть, с симметричностью понятно, с белинейностью тоже понятно,
[31:03.700 --> 31:09.300]  в силу свойства линейности математического ожидания. Если вы смотрите на определение кавриации,
[31:09.300 --> 31:18.340]  если вы вместо x напишете сумму x1 плюс x2 или a1x1 плюс a2x2t2, то все это по линейности
[31:18.340 --> 31:24.980]  математического ожидания вынесется и получится a1 кавриация x1t плюс a2 кавриация x2t. Ну, вы либо с
[31:24.980 --> 31:29.620]  Ваном Генриховичем это проделали, либо, если нет, можете поупражняться и доказать это самостоятельно,
[31:29.620 --> 31:36.380]  это очевидное упражнение. Вот. Значит, как из этого следует, что дисперсия суммы равна сумме дисперсии
[31:36.380 --> 31:45.260]  плюс сумму кавриации? Ну, кстати, сумма дисперсии, это, конечно, та же самая сумма кавриации. То есть,
[31:45.260 --> 31:50.020]  можно было бы написать просто сумму кавриации по всем, вообще, возможным и ежели. Это было бы
[31:50.020 --> 31:54.860]  то же самое. Но мне удобнее такая формула, потому что я для независимых случайных величин из этого
[31:54.860 --> 31:59.980]  сделал некоторый вывод чуть позже. Поэтому давайте вот в таком виде эту формула оставим. Значит,
[31:59.980 --> 32:10.340]  доказываем. Итак, давайте дисперсию просто перепишем как кавриацию случайной величины самой
[32:10.340 --> 32:18.580]  собой. Дисперсия суммы, это будет кавриация этой суммы самой собой.
[32:26.100 --> 32:32.140]  И дальше победленность кавриации. Это просто сумма всевозможных кавриаций. По всем возможным
[32:32.140 --> 32:40.420]  и ежи кавриации х и т, что, в общем-то, и требуется. В этой формуле обратите внимание,
[32:40.420 --> 32:45.300]  что здесь имеется в виду всевозможные пары и ежи. То есть, каждая пара считается дважды. И может
[32:45.300 --> 32:48.780]  быть меньше, чем ж, и наоборот, и может быть больше, чем ж, при этом значения и ежи будут
[32:48.780 --> 32:53.220]  одинаковые. И кавриации в силу симметричности тоже будут одинаковые. То есть, каждая кавриация,
[32:53.220 --> 32:59.340]  на самом деле, в этой сумме считается дважды. Ну и наконец, следствие, которое я обещал, есть
[32:59.340 --> 33:08.860]  из чайной величины попарно-независимая, то и дисперсия суммы равна сумме дисперсии.
[33:08.860 --> 33:28.340]  Понятно, что независимость в совокупности – это более сильное
[33:28.340 --> 33:31.900]  свойство, чем попарная независимость, поэтому для независимости в совокупности это тоже верно.
[33:31.900 --> 33:49.100]  То дисперсия суммы равна сумме дисперсии. Это следует из-за того, что кавриация для
[33:49.100 --> 33:54.900]  независимого случайных величин равна нулю. Да, то есть это очевидное следствие. То есть,
[33:55.140 --> 34:03.300]  слагаем, сразу обнуляется. Если кси независимо с это, то кавриация кси-эта равна нулю.
[34:03.300 --> 34:13.220]  В свою очередь, это следует из-за того, что для независимого случайных величин
[34:13.220 --> 34:17.180]  отжидание произведения равно произведением от ожидания. А кавриация, как я написал выше,
[34:17.180 --> 34:20.260]  это отжидание произведения минус произведением от ожидания. То есть, это будет ноль, конечно,
[34:20.260 --> 34:23.820]  независимо от случайных величин. Надеюсь, что тут всё очевидно. Если есть какие-то вопросы,
[34:23.820 --> 34:31.260]  пожалуйста, задавайте. Теперь мы можем вернуться к нашим двум примерам с бенемиальным
[34:31.260 --> 34:38.420]  распределением и с нормальным и действительно доказать, что из этих свойств следует то,
[34:38.420 --> 34:47.660]  что нам нужно. Во-первых, с бенемиальным распределением я уже говорил, что вы можете
[34:47.660 --> 34:55.900]  его воспринимать как просто сумму независимого бенемиального случайных величин. То есть,
[34:55.900 --> 35:02.780]  иными словами, если вы возьмёте кси-1 и кси-эн независимые в совокупности, это важно, иначе у вас
[35:02.780 --> 35:07.140]  не получится в сумме бенемиальное распределение. С точки зрения дисперсии суммы это неважно,
[35:07.140 --> 35:12.340]  даже если они попарно независимые, то всё равно всё получится. Но вот чтобы получить именно
[35:12.340 --> 35:17.100]  бенемиальное распределение, вам нужна независимость в совокупности. Пусть кси-1 и кси-эн это
[35:17.100 --> 35:31.420]  Бернулевские с параметром p независимые в совокупности. А тогда их сумма тоже имеет
[35:31.420 --> 35:37.940]  бенемиальное распределение с параметром p, то есть то же самое распределение, которое у случайной
[35:37.940 --> 35:43.380]  величины кси, а значит у них одинаковая дисперсия. То есть дисперсия кси будет равна дисперсии суммы
[35:43.380 --> 35:51.780]  этих случайных величин. Так они независимы попарно, тем более в совокупности, то дисперсия
[35:51.780 --> 36:01.300]  суммы это сумма дисперсий. Дисперсию для Бернулевских мы считали, получаем np1-p.
[36:06.500 --> 36:11.060]  Ну и нормальное распределение с параметром i7². Здесь тот же трюк, который мы делали для
[36:11.060 --> 36:16.980]  математического ожидания. То есть возьмем стандартное нормальное и преобразуем его так,
[36:16.980 --> 36:25.620]  чтобы получилась наша нормальная с параметрами i7². Как это делается? Если взять стандартное
[36:25.620 --> 36:31.020]  нормальное, то я напоминаю, что если мы умножим его, эту случайную величину, на корень i7² и
[36:31.020 --> 36:36.020]  прибавим a, то мы получим нормальное распределение с параметрами i7².
[36:36.020 --> 36:48.100]  Ну а значит дисперсия кси совпадает с дисперсией корень i7² это плюс а.
[36:48.100 --> 37:00.980]  Далее дисперсия суммы. У нас для неё есть формула. И в этой сумме второе слагаем это
[37:01.020 --> 37:07.140]  константа. А константа, она независима с чем угодно. Поэтому это есть дисперсия от первой
[37:07.140 --> 37:16.420]  случайной величины, умножить на дисперсию от второй случайной величины. Но дисперсия
[37:16.420 --> 37:22.820]  константа это конечно ноль, потому что отклонение константа от самой себя это ноль. У константа
[37:22.820 --> 37:29.100]  ожидание тоже равно этой самой константе, поэтому это просто ноль. А есть второе, из первого слагаемого,
[37:29.100 --> 37:32.980]  корень сима в квадрате выносится и возводится в квадрат. То есть получается сима в квадрате
[37:32.980 --> 37:39.140]  на дисперсию это, который мы считали, для стандартного нормального распределения дисперсия равна
[37:39.140 --> 37:45.620]  единице. Получается сима в квадрате. Итак, у нормального распределения первые параметры равны
[37:45.620 --> 37:52.500]  математическому ожиданию, а второй равен дисперсию. Есть ли какие-то вопросы?
[37:58.740 --> 38:01.460]  Есть какая-то формула для дисперсии в виде одного интеграла?
[38:01.460 --> 38:08.020]  Или там придется считать два и потом вычитать?
[38:08.020 --> 38:19.820]  В любом случае придется считать два интеграла, какой бы вы формулой не пользовались. Если вам
[38:19.820 --> 38:26.620]  известно мат ожидания, то вы считаете один интеграл. Каким вы определением не воспользовались,
[38:26.620 --> 38:32.540]  первым или вторым, у вас везде фигурирует математическое ожидание. Если будете считать,
[38:32.540 --> 38:37.700]  вот так у вас будет просто один интеграл внутри, будет другой интеграл. Если будет считать вторым
[38:37.700 --> 38:42.140]  образом, у вас будет разность двух интегралов. Никак по-другому вы не посчитаете.
[38:42.140 --> 38:45.580]  В общем случае.
[38:50.780 --> 38:59.020]  Хорошо. Теперь мы поговорим про важные неравенства, связанные с понятием математического
[38:59.020 --> 39:05.620]  ожидания дисперсии. И первое из них, чтобы не убегать далеко от понятия ковариации, я вам
[39:05.620 --> 39:12.460]  обещал, что я объясню, почему ковариация это мера зависимости случайных величин. Но уже в
[39:12.460 --> 39:16.620]  некотором смысле мы это поняли. Если случайная величина независима, то ковариация равна нулю.
[39:16.620 --> 39:23.540]  А что если независимая? Ковариация тогда может быть сколько угодно большой по модулю. Она может
[39:23.540 --> 39:28.580]  быть положительная, может быть отрицательная, может быть сколько угодно большой. Поэтому вот прямо в
[39:28.580 --> 39:33.860]  точности ковариацию как меру зависимости использовать не очень удобно, ее бы удобней было бы отнормировать.
[39:33.860 --> 39:39.820]  Инормирует ее вот чем. Значит рассматривает так называемый коэффициент корреляции случайных
[39:39.820 --> 39:50.740]  величин или просто корреляцию случайных величин. Это есть ковариация, деленная на корень из
[39:50.740 --> 40:06.180]  произведения дисперсии. Вот. И утверждается, что эта величина, какие бы ни были случайные
[40:06.180 --> 40:10.460]  величины, если у них есть дисперсия, то эта величина находится в отрезке от минус единицы
[40:10.460 --> 40:20.340]  до единицы. И вот это уже гораздо более адекватная мера зависимости, потому что если вы
[40:20.340 --> 40:24.740]  достигли экстремальных значений на границах интервалов, если это минус один или один,
[40:24.740 --> 40:29.540]  это означает, что случайные величины очень сильно зависимы, а именно они линейно зависимы.
[40:29.540 --> 40:37.620]  И это практически следствие, так называемое неравенство Коши Буниковского,
[40:37.620 --> 40:41.100]  который вы прекрасно знаете, но давайте его напомним.
[40:54.300 --> 41:03.740]  Которая можно для случайных величин сформулировать следующим образом. Пусть есть две случайные
[41:03.740 --> 41:08.100]  величины такие, что у них конечный второй момент, то есть мат ожидания к си квадрата меньше
[41:08.100 --> 41:21.740]  бесконечности, мат ожидания в квадрате меньше бесконечности. Тогда математическое ожидание
[41:21.740 --> 41:37.980]  модуля произведения, во-первых, конечна, а во-вторых, оно меньше ли бы равно, чем корень из произведения
[41:37.980 --> 41:51.700]  квадратов математических ожиданий. Из этого, конечно, сразу следует то, что коэффициент
[41:51.700 --> 41:57.060]  корреляции находится в отрезке от минусы единицы до единицы, как это увидеть, но вместо кси это нужно
[41:57.060 --> 42:04.020]  поставить разность между кси и мат ожиданием и этой мат ожиданием, тогда вы здесь получите,
[42:04.020 --> 42:10.100]  вот здесь вы как раз получите то, что у вас в определении к вариации кси минус е кси на это
[42:10.100 --> 42:16.140]  минус е это, а здесь у вас будет корень из произведения дисперсии. Вот, поэтому утверждение о том,
[42:16.140 --> 42:21.820]  что коэффициент корреляции находится в отрезке от минусы единицы до единицы, сразу следует из
[42:21.820 --> 42:26.980]  неравенства Кошмарниковского. Давайте, неравенство Кошмарниковского доказывается в две строчки,
[42:26.980 --> 42:30.420]  поэтому давайте я его быстренько напомню, как его можно доказать. Есть разные способы.
[42:30.420 --> 42:39.380]  Мода один из, на мой взгляд, простых. Ещё я как я сказал, что если вы достигли экстремального
[42:39.380 --> 42:45.300]  значения, то есть если корреляция это минус е и е, то из этого следует, что случайная
[42:45.300 --> 42:49.700]  причина линии независима с вероятностью 1. Мы сейчас увидим в процессе доказательств, что действительно так.
[42:49.700 --> 42:57.300]  Значит, первый случай, который мы рассмотрим отдельно, это если мат ожидания 0, кси в квадрате,
[42:57.300 --> 43:09.900]  в квадрате 0. Значит, в этом случае, так как кси квадрат это не отрицательная случайная личина и
[43:09.900 --> 43:14.180]  свойство математического ожидания, мы знаем, что с вероятностью 1 случайная личина равна 0.
[43:17.260 --> 43:23.460]  Раз она равна 0, то просто обе части этого неравенства равны 0, то есть им от ожидания
[43:23.460 --> 43:32.660]  модуля произведения равна 0. Ну, кси это 0, от умножения на это она останется нулём, поэтому
[43:32.660 --> 43:38.260]  мат ожидания будет 0. И корень из е кси квадрат на е квадрат тоже 0.
[43:45.300 --> 43:50.260]  Ну, в силу симметрии мы тем самым разобрали из случаев, когда мат ожидания в квадрат 0. То есть
[43:50.260 --> 43:56.700]  мы можем теперь считать, что оба мат ожидания и кси квадрата, и это квадраты минули.
[43:56.700 --> 44:06.980]  Значит, давайте теперь рассмотрим, отнормируем нашу случайную личину для удобства. Рассмотрим
[44:06.980 --> 44:17.500]  кси с волной равное кси поделить на корень из е кси квадрат, и это с волной равно это поделить на
[44:17.500 --> 44:39.660]  корень из е квадрата. Понятно, что если мы сейчас рассмотрим, если мы рассмотрим мат
[44:39.660 --> 44:53.660]  ожидания от кси с волной в квадрате, то мы получим единицу. Правда же, если мы сделаем кси с волной в
[44:53.660 --> 44:58.420]  квадрат, мы получим просто кси квадрат поделить на корень из е кси квадрат в квадрате, то есть
[44:58.420 --> 45:09.340]  просто е кси квадрат. И мат ожидания с волной в квадрате это тоже единица. Поэтому мат ожидания от
[45:09.340 --> 45:21.060]  кси с волной минус эт с волной в квадрате, это есть два, ну я возложу разность в квадрат,
[45:21.060 --> 45:25.180]  я получаю квадрат первого слагаемого минус двойное произведение плюс квадрат второго слагаемого.
[45:25.180 --> 45:31.820]  Два мат ожидания равны единице, то есть это будет два минус два мат ожидания кси с волной эт с волной.
[45:31.820 --> 45:39.340]  Давайте модули напишем, чтобы доказать именно то, что я хотел, то есть здесь будет тоже модуль
[45:39.340 --> 45:45.300]  стоять. Ну вот, это мат ожидания не отрицательные случайно уличны, поэтому эту штуку не отрицать.
[45:45.300 --> 45:53.180]  Откуда следует, что действительно мат ожидания модуля кси с волной эт с волной меньше равно единицы.
[45:53.180 --> 45:59.460]  На самом деле это точно то, что мы хотели, потому что если мы теперь вспомним, что такое кси с волной
[45:59.460 --> 46:09.860]  эт с волной, то мы получим мат ожидания от модуля кси эт, деленное на корень из произведения
[46:09.860 --> 46:16.340]  кси, мат ожидания кси квадрата, мат ожидания в квадрате. И эта штука меньше, но единица по
[46:16.340 --> 46:25.940]  линейностям мат ожидания это точно то, что мы хотели доказать. В каком случае достигается равенство
[46:25.940 --> 46:35.140]  единицы? Равенство единицы достигается в том случае, когда вот здесь вот стоит тоже равенство. То есть
[46:35.140 --> 46:43.020]  нулю должно быть равно мат ожидания квадраты разности модуля кси с волной минус эт с волной.
[46:43.020 --> 46:54.220]  Значит равенство достигается тогда и только тогда, когда мат ожидания от модуля кси с волной
[46:54.220 --> 47:02.220]  минус эт с волной в квадрате равно нулю. А так как случайная вещественная не отрицательна, то это
[47:02.220 --> 47:11.940]  верно тогда и только тогда, когда с вероятностью 1 модуль кси с волной равен модулю эт с волной.
[47:14.020 --> 47:20.900]  Это и есть на самом деле линейная зависимость. Почему это линейная зависимость? Ну потому,
[47:20.900 --> 47:25.220]  что т с волной это кси поделить на константу, а эт с волной это эт поделить на константу. То есть
[47:25.220 --> 47:30.620]  получается, что одна случайная вещественная линия не зависит от другой. Есть ли какие-то вопросы?
[47:30.620 --> 47:53.340]  Да, это правда, но мы просто доказывали здесь нечто большее, чем то, что нам нужно. Мы доказывали
[47:54.340 --> 48:01.780]  если мы доказывали просто мат ожидания произведения, то то, что нам надо. Допустим,
[48:01.780 --> 48:09.180]  мы хотим рассмотреть ситуацию, когда кавариация равна единице. Что это означает? Это означает,
[48:09.180 --> 48:11.900]  что в неравенстве к Кашибуниковскому, во-первых, надо модуль урать, во-вторых,
[48:11.900 --> 48:16.420]  поставить равенство. И тогда мы в том месте, где я написал модуль случайных величин,
[48:16.420 --> 48:20.060]  писали бы сами случайные величины без модуля и получили бы то же самое для самих случайных
[48:20.060 --> 48:28.220]  величин без модуля. Просто надо аналогичное доказательство проделать для случайных
[48:28.220 --> 48:38.140]  величин без модуля. Так, хорошо. Полезный неравенство к Кашибуниковскому часто применяется. В частности,
[48:38.140 --> 48:45.020]  я промотивировал рассмотрение этой меры зависимости от случайных величин. Дальнейшие два
[48:45.020 --> 48:48.980]  неравенства, о которых я поговорю, так называемые неравенства Маркова и Неравенства Чебышова,
[48:48.980 --> 48:57.220]  это так называемые концентрационные неравенства. Отдельный очень важный раздел теории вероятности
[48:57.220 --> 49:02.060]  он о том, насколько хорошо случайные величины сконцентрированы вокруг своего математического
[49:02.060 --> 49:07.540]  ожидания. Вот я уже вам говорил, что дисперсию можно воспринимать как меру такой концентрации.
[49:07.540 --> 49:11.940]  Чем больше дисперсии, тем сильнее случайные величины разбросаны вокруг математического
[49:11.940 --> 49:17.820]  ожидания. И строгое утверждение о том, что это действительно так, называется неравенство Чебышова.
[49:17.820 --> 49:23.980]  А неравенство Маркова, это тоже важный неравенство, из которого неравенство Чебышова просто явно следует.
[49:23.980 --> 49:31.740]  Давайте я напомню, что такое неравенство Маркова. Так, друзья, 10 секунд перерыв,
[49:31.740 --> 49:34.060]  мне нужно сходить за зарядкой для ноутбука.
[50:17.820 --> 50:45.980]  Так, продолжаем. Нет, как я обещал, неравенство Маркова. Неравенство Маркова. Неравенство Маркова
[50:45.980 --> 50:53.060]  звучит следующим образом. Представьте, что у вас есть случайная величина, которая не отрицательна.
[51:15.980 --> 51:44.500]  Да, прошу прощения за переполох. Значит, итак, пусть кси не отрицательная случайная величина,
[51:44.500 --> 51:52.980]  не отрицательная случайная величина, и пусть есть какое-то положительное число А. Ещё заодно
[51:52.980 --> 52:05.740]  предположено, что мотождание кси меньше бесконечности. Тогда вероятность того,
[52:05.740 --> 52:22.060]  что кси больше забрано, чем А, меньше забрано, чем мотождание кси поделительно А. Ну, я думаю,
[52:22.060 --> 52:26.380]  что для многих интенсивно понятно, что действительно так, и доказательства, вообще говоря,
[52:26.380 --> 52:40.540]  очень простое. Заметим, что в силу того, что кси не отрицательна, она больше либо равна,
[52:40.540 --> 52:54.340]  чем А, умножительный индикатор того, что кси больше чем А. То есть, что я сделал здесь? Я взял
[52:54.340 --> 53:02.340]  значение случайной величины кси, и в той ситуации, когда оно меньше, чем А, я его занулил. От этого оно
[53:02.340 --> 53:06.020]  могло только уменьшиться, не могло никак увеличиться, потому что оно не отрицательное.
[53:06.020 --> 53:11.420]  Я вместо отрицательного числа написал ноль. А в случае, если значение случайной величины хотя бы А,
[53:11.420 --> 53:16.980]  я его тоже уменьшил, я его превратил в А. Поэтому это неравенство, конечно, верно. Если у нас есть
[53:16.980 --> 53:21.140]  неравенство между двумя случайными величинами, то неравенство математического ожидания сохраняется.
[53:21.140 --> 53:30.100]  Из этого следует, что мат ожидания кси больше либо равно, чем мат ожидания от А, умножительный
[53:30.100 --> 53:37.220]  индикатор того, что кси больше чем А. Теперь это А можно вынести за знак математического ожидания,
[53:37.220 --> 53:46.980]  и получается А, умножительная вероятность того, что кси больше чем А. Ну, чтобы получить искомое
[53:46.980 --> 53:55.980]  Ну и не нравится, что осталось просто на А поделить обе части. Действительно получается, что вероятность того, что КС больше на ЧМА меньше собрана, чем в ожидании КС поделить на А.
[54:02.980 --> 54:03.980]  Ну и не нравится Чебышова.
[54:16.980 --> 54:22.980]  Оно звучит следующим образом. Теперь нам не важно, какой знак случайной величины, нам важно только, чтобы не был второй момент.
[54:23.980 --> 54:29.980]  Значит пусть мы от ожидания КС в квадрате меньше бесконечности, и пусть ε это какое-то положительное число.
[54:29.980 --> 54:43.980]  Тогда вероятность того, что модуль КС минус мы от ожидания КС, хотя бы ε, меньше либо равна, чем дисперсия КС поделить на ε в квадрате.
[54:43.980 --> 54:48.980]  И это как раз о том, что дисперсия является мерой отклонения, извините, случаемой.
[54:49.980 --> 55:01.980]  Вероятность того, что модуль КС минус мы от ожидания КС, хотя бы ε, меньше либо равна, чем дисперсия КС поделить на ε в квадрате.
[55:01.980 --> 55:13.980]  И это как раз о том, что дисперсия является мерой отклонения, извините, случаемой.
[55:14.980 --> 55:18.980]  То есть чем меньше дисперсия, тем меньше правая часть этого неравенства.
[55:19.980 --> 55:27.980]  И соответственно, тем меньше вероятность того, что отклонение, вероятность отклонения, случаемой, мы от ожидания на ε.
[55:28.980 --> 55:34.980]  Это очевидное следствие неравенства Маркова. Достаточно свести просто одно неравенство к другому.
[55:35.980 --> 55:43.980]  Как это сделать? Ну, давайте в качестве вот этого случайного числа КС в неравенстве Маркова возьмем модуль КС минус ЕКС.
[55:44.980 --> 55:47.980]  То есть возьмем случайную числу. Это равный модуль КС минус ЕКС.
[55:48.980 --> 55:53.980]  Это будет в квадрате, извиняюсь, КС минус ЕКС в квадрате.
[55:54.980 --> 56:01.980]  Это будет случайная величина из неравенства Маркова, которая в неравенстве Маркова обозначена КС.
[56:02.980 --> 56:08.980]  А в качестве А мы возьмем просто Экс. Значит, по неравенству Маркова,
[56:09.980 --> 56:12.980]  по неравенству Маркова,
[56:14.980 --> 56:18.980]  так как это не отрицательно, мы имеем право неравенство применить,
[56:18.980 --> 56:25.980]  то есть мы получаем вероятность того, что это больше либо равна, чем Эксимум в квадрат,
[56:27.980 --> 56:31.980]  меньше либо равна, чем мотождание Экса поделить на Эксимум в квадрате.
[56:36.980 --> 56:38.980]  А мотождание Экса, это есть дисперсия.
[56:39.980 --> 56:41.980]  Это есть дисперсия.
[56:42.980 --> 56:44.980]  Тем самым неравенство к бушову доказано.
[56:49.980 --> 56:51.980]  Есть какие-то вопросы.
[56:54.980 --> 56:57.980]  Ну и как пример неравенства применения и неравенства к бушову,
[56:58.980 --> 57:02.980]  давайте докажем очень важное утверждение, которое называется закон больших чисел в форме к бушову.
[57:04.980 --> 57:07.980]  Вот наконец-то у нас первая предельная теория.
[57:09.980 --> 57:11.980]  Закон больших чисел.
[57:18.980 --> 57:22.980]  Чисел в форме к бушову.
[57:35.980 --> 57:39.980]  Ну это утверждение о том, что если вы будете тысячу раз подбрасывать монетку
[57:40.980 --> 57:43.980]  и поделите количество решек на тысячу, то будет примерно 0,5.
[57:44.980 --> 57:47.980]  Это формально звучит следующим образом.
[57:48.980 --> 57:50.980]  Это важный момент.
[57:51.980 --> 57:56.980]  Это тот момент, когда мы наше теоретическое определение вероятности свели к физическому определению вероятности.
[57:57.980 --> 58:04.980]  Как я уже говорил, исторически сначала вероятностью считалась частотность появления события,
[58:05.980 --> 58:09.980]  а потом после того, как появилась Эксиматика Калмогорова,
[58:09.980 --> 58:15.980]  наоборот, из такого теоретического определения вероятности, как вероятности меры,
[58:16.980 --> 58:18.980]  мы получаем физическое определение вероятности,
[58:19.980 --> 58:21.980]  что действительно просто совпадает в пределе,
[58:22.980 --> 58:27.980]  совпадает с частотностью происхождения события, когда вы его независимо много раз повторяете.
[58:28.980 --> 58:36.980]  Итак, пусть X1, X2 и так далее – это бесконечная последовательность независимых случайных величин.
[58:39.980 --> 58:41.980]  Независимых совокупностей?
[58:42.980 --> 58:59.980]  Это хороший вопрос, да, на самом деле попарно независимости достаточно,
[59:00.980 --> 59:05.980]  но когда я пишу слово «независимость» и не уточняю, что он попарно, я имею в виду независимость совокупности.
[59:06.980 --> 59:12.980]  Почему я здесь пишу более сильное условие, не ослабляя его?
[59:13.980 --> 59:18.980]  Потому что я хочу продемонстрировать именно такой закон больших чисел,
[59:19.980 --> 59:23.980]  который лежит в основе физического определения вероятности.
[59:24.980 --> 59:28.980]  Я его докажу, потом сразу сформулирую обобщение, и все места, которые можно ослабить, я ослаблю.
[59:29.980 --> 59:31.980]  Здесь я имею в виду независимость совокупности.
[59:31.980 --> 59:36.980]  Пусть X1, X2 – это независимые случайные величины.
[59:37.980 --> 59:45.980]  Давайте опять слишком сильное утверждение по требованию,
[59:46.980 --> 59:50.980]  что они одинаково распределены. Это тоже не обязательно, можно это ослабить.
[59:51.980 --> 59:55.980]  Одинаково распределенные.
[59:55.980 --> 59:59.980]  Давайте еще считать, что у них есть мотождание дисперсия.
[01:00:00.980 --> 01:00:06.980]  Мотождание кситы равно a. Дисперсия кситы равнеет c2.
[01:00:15.980 --> 01:00:18.980]  И давайте еще введем какое-то положительное число.
[01:00:18.980 --> 01:00:28.980]  Тогда вероятность того, что кси1 и талия плюс кси n поделить на n
[01:00:31.980 --> 01:00:36.980]  минус a по модулю больше, чем епсилон,
[01:00:36.980 --> 01:00:39.980]  да, то есть вероятность того, что империческое среднее
[01:00:40.980 --> 01:00:43.980]  отличается от теоретического среднего, больше, чем на епсилон,
[01:00:44.980 --> 01:00:48.980]  стремится к нулю, примерно к стремящемуся бесконечности.
[01:00:50.980 --> 01:00:52.980]  Давайте это дважды поделим.
[01:00:53.980 --> 01:00:56.980]  Вот здесь у нас есть мотождание дисперсии.
[01:00:57.980 --> 01:01:00.980]  И вот здесь у нас есть мотождание дисперсии.
[01:01:00.980 --> 01:01:04.980]  Мы будем показывать об обобщении без конечности.
[01:01:08.980 --> 01:01:10.980]  Давайте это дваждение докажем.
[01:01:13.980 --> 01:01:15.980]  Или давайте проще поставим.
[01:01:16.980 --> 01:01:18.980]  Я сразу сформулирую об общении,
[01:01:20.980 --> 01:01:22.980]  и будем сразу доказывать об обобщении.
[01:01:23.980 --> 01:01:25.980]  Еще раз, значит, вот этот закон, который я здесь формулировал,
[01:01:25.980 --> 01:01:33.580]  чем обобщение. Во-вторых, он более прикладной. То есть вот это физическое
[01:01:33.580 --> 01:01:38.300]  понятие вероятности о том, что вы вероятность воспринимаете как
[01:01:38.300 --> 01:01:43.900]  частотность появления события для именно независимых испытаний. Это как раз
[01:01:43.900 --> 01:01:47.980]  вот закон больших чисел в форме ЧБ-шоу в таком оригинальном виде. Но он очень
[01:01:47.980 --> 01:01:56.460]  легко обобщается до следующего. Это как-то связано с сходимостью по мере?
[01:01:56.460 --> 01:02:01.900]  Это в точности сходимость по мере, да, это в точности сходимость по мере, которая в теории вероятности
[01:02:01.900 --> 01:02:05.180]  называется сходимость по вероятности. Мы об этом еще поговорим чуть позже.
[01:02:11.180 --> 01:02:17.340]  То есть это сходимость на самом деле по вероятностной мере, вот этого отношения
[01:02:17.340 --> 01:02:24.140]  к сиадинку стали, плюс к сиадинку поделить на m, к константе a. Так, значит обобщение. Пусть есть
[01:02:24.140 --> 01:02:31.100]  попарная независимость. Знаете, даже не нужно попарной независимости, можно еще ослабить попарной
[01:02:31.100 --> 01:02:36.460]  некоррелируемость, так называемая. То есть попарная кавариация равна нулю. Это еще более слабое
[01:02:36.460 --> 01:02:41.900]  свойство, чем попарная независимость. Потому что из того, что кавариация равна нулю, следует
[01:02:41.900 --> 01:02:47.580]  независимость. Прошу прощения, из того, что есть независимость следующая кавариация равна нулю,
[01:02:47.580 --> 01:02:52.220]  но обратная вообще говоря не нет. То есть может такое быть, что кавариация ноль, а при этом никакой
[01:02:52.220 --> 01:02:57.420]  независимости даже по парной нет. Значит пусть есть последствия случайных величин таких, что для
[01:02:57.420 --> 01:03:02.140]  всех различных i и g кавариация x и t, x и g, t равна нулю.
[01:03:02.140 --> 01:03:16.140]  Что еще надо сказать? Надо еще сказать, что все дисперсии ограничены. То есть нам одинаковая
[01:03:16.140 --> 01:03:19.020]  распределенность на самом деле нужна была для того, чтобы дисперсии были одинаковые,
[01:03:19.020 --> 01:03:23.820]  но одинаковые дисперсии тоже не обязательно, достаточно чтобы они были ограничены. Пусть
[01:03:23.820 --> 01:03:32.300]  существует такая константа c, что для любого i дисперсия x и t меньше чем c.
[01:03:39.300 --> 01:03:50.620]  Ну и на самом деле этого достаточно. Давайте еще для удобства обозначим сумму первых
[01:03:50.620 --> 01:04:00.860]  m случайных величин за s, и рассмотрим опять какое-то epsilon больше нуля,
[01:04:04.460 --> 01:04:12.780]  и рассмотрим произвольную последовательность w, m, которая стремится к бесконечности с ростом m.
[01:04:12.780 --> 01:04:36.820]  Тогда вероятность того, что модуль sn минус мотождание sn поделить на корень из n умножить
[01:04:36.820 --> 01:04:47.300]  на w, m больше чем epsilon стремится к нулю при настримящемся бесконечности. Как увидеть,
[01:04:47.300 --> 01:04:55.660]  что это действительно обобщение закона больших чисел в форме Чебушева. Ну смотрите,
[01:04:55.660 --> 01:05:06.760]  надо в качестве w, m взять корень из n, и вы получите в точности первый вариант,
[01:05:06.760 --> 01:05:19.140]  то есть более слабый вариант, вы получите в точности zbch в форме Чебушева. Ну смотрите,
[01:05:19.140 --> 01:05:27.580]  действительно, если я возьму в качестве w, m корень из n, я узнаю, что я получу n. А мотождание
[01:05:27.580 --> 01:05:34.900]  суммы, если случайные величины одинаково распределены, у них у всех мотождания одинаковые,
[01:05:34.900 --> 01:05:41.260]  то мотождание суммы это n умножить на 1 мотождание, nA. Поэтому nA с n сократится,
[01:05:41.260 --> 01:05:48.660]  останется просто A, и вы в точности получите вот это выражение из просто закона больших
[01:05:48.660 --> 01:05:54.660]  чисел в форме Чебушева. То есть закон больших чисел в форме Чебушева далек от оптимальности,
[01:05:54.660 --> 01:06:00.020]  вот этот n знаменатель, это очень грубо, это n можно заменить на любую функцию,
[01:06:00.020 --> 01:06:06.940]  которая растет в бесконечности быстрее, чем корень из n. Вот, а доказательство идентичное,
[01:06:06.940 --> 01:06:11.940]  то есть это обобщение просто рождается из того, что вы берете доказательство закона больших
[01:06:11.940 --> 01:06:17.500]  чисел и смотрите, с чего это доказательство можно ослабить. Поэтому давайте сразу этот
[01:06:17.500 --> 01:06:28.060]  усиленный вариант доказывать. Значит, дальше у нас в курсе еще будет усиленный закон больших
[01:06:28.060 --> 01:06:33.100]  чисел, его не надо путать с обобщением закона больших чисел, о котором я вас говорил. Значит,
[01:06:33.100 --> 01:06:38.500]  если в обычном законе больших чисел, как правильно было замечено, речь идет про сходимость по мере,
[01:06:38.500 --> 01:06:46.180]  то в усиленном законе больших чисел будет идти речь про сходимость почти всюду. Так, это все один
[01:06:46.180 --> 01:06:52.860]  и тот же закон больших чисел, то есть это не то чтобы какое-то супермощное обобщение. Хорошо. Так,
[01:06:52.860 --> 01:06:57.380]  что делаем? Пользуемся неравностью Чебушева, смотрим на нашу вероятность, которую мы должны
[01:06:57.380 --> 01:07:03.020]  с вами оценить. Да, конечно, она стремится к нулю, вероятность того, что модуль СН минус ЕСН
[01:07:03.020 --> 01:07:15.380]  поделить на корень ЕЗН ВН больше чем ЭСН. И умножаем обе части неравенства на знаменатель,
[01:07:15.380 --> 01:07:25.340]  получим вероятность того, что модуль СН минус ЕСН больше чем ЭПСИВАН корень ЕЗН ВН.
[01:07:25.340 --> 01:07:31.700]  По неравенству Чебушева это меньше ебраночьим дисперсия СН
[01:07:31.700 --> 01:07:45.060]  поделить на квадрат правой части, то есть ЭПСИВАН квадрат НВН квадрат. Дальше, в силу того,
[01:07:45.060 --> 01:07:49.940]  что все кавариации ноль, я напомню, что дисперсия сумма, это сумма дисперсии плюс все возможные
[01:07:49.940 --> 01:07:54.620]  кавариации разных случайных величин. Они все ноль, поэтому дисперсия сумма, это просто сумма
[01:07:54.620 --> 01:08:05.380]  дисперсии. Ну и вы уже видите наверняка, почему это сумма дисперсии с нулю, потому что мы
[01:08:05.380 --> 01:08:10.020]  предположили, что все дисперсии ограничены одной и той же константой С, поэтому числитель не
[01:08:10.020 --> 01:08:17.740]  превосходит С умножить на Н ЭПСИВАН в квадрате НВН в квадрат, Н сокращается, и вот то место,
[01:08:17.740 --> 01:08:22.420]  где мы понимаем, что нам корень ЕЗН нужно домножить на растущую функцию, потому что корень
[01:08:22.420 --> 01:08:27.640]  ЕЗН возвелся в квадрат, с числителем сократился, и вот именно эта растущая функция осталась
[01:08:27.640 --> 01:08:34.300]  в номинации. Что и требовалось, есть ли какие-то вопросы?
[01:08:41.220 --> 01:08:48.860]  Ну и последнее неравенство, последнее утверждение на сегодня, которое нам в курсе тоже потребуется,
[01:08:48.860 --> 01:08:53.940]  я сразу продемонстрирую пару очевидных применений, это неравенство Йенцина.
[01:09:01.220 --> 01:09:10.100]  Неравенство Йенцина. Ну в частности, из этого неравенства следует, что дисперсия не отрицательна,
[01:09:10.100 --> 01:09:17.060]  хотя это так очевидно, да, и следует то, что мотождание модуля больше собрано, чем модуль
[01:09:17.060 --> 01:09:22.020]  мотождания. Неравенство Йенцина, это некоторые утверждения, которые это все обобщают. Значит, пусть есть
[01:09:22.020 --> 01:09:29.300]  некоторая выпуклая бареллевская функция, некоторая выпуклая бареллевская функция,
[01:09:29.300 --> 01:09:42.620]  ну вот, например, квадрат или модуль, да, пусть Ж, ну бареллевская, да, когда мы говорим выпуклая
[01:09:42.620 --> 01:09:52.060]  функция, мы, наверное, считаем, что она непрерывная. Тогда, наверное, про бареллевость говорится
[01:09:52.060 --> 01:09:58.100]  необязательно, пусть просто уже выпуклая, она автоматически бареллевская, которая действует из РВР.
[01:09:58.100 --> 01:10:06.540]  И пусть мотождание модуля ФС меньше бесконечности, если это случайно у ЧНАС в конечном первом моменте.
[01:10:06.540 --> 01:10:08.340]  Тогда
[01:10:14.140 --> 01:10:19.420]  мотождание Ж от Си больше собрано, чем Ж от мотождания КСи.
[01:10:24.180 --> 01:10:32.380]  Вот, ну в частности, если вы вместо Ж подставите квадрат или модуль, вы получите, собственно,
[01:10:32.380 --> 01:10:37.020]  известные вам свойства математического ожидания. Во-первых, то, что мотождание модуля больше
[01:10:37.020 --> 01:10:41.180]  вон очень модным от ожидания, во-вторых, то, что беспрерывный интерцептор. Значит,
[01:10:41.180 --> 01:10:45.260]  как это утверждение доказать в общем случае? Ну, довольно просто, надо просто воспользоваться
[01:10:45.260 --> 01:10:53.460]  определением выпуклости. Значит, что означает функция выпуклая? Ну, это означает, что какой бы вы
[01:10:53.460 --> 01:11:01.540]  не взяли, какой бы вы не взяли аргумент, вы через точку, если вы возьмете график функции,
[01:11:01.540 --> 01:11:05.900]  какой бы вы не взяли аргумент, через соответствующую точку на графике функции вы можете провести прямую,
[01:11:05.900 --> 01:11:11.700]  так что у вас целиком вся кривая окажется выше этой прямой. То есть, иными словами,
[01:11:11.700 --> 01:11:17.780]  любого х0, давайте прямо запишем то, что я сказал, любого х0 вы можете выбрать прямую,
[01:11:17.780 --> 01:11:22.500]  но на самом деле у вас точка фиксирована, через которую вы эту прямую проводите,
[01:11:22.500 --> 01:11:29.140]  поэтому на самом деле вы можете выбрать наклон. Существует некоторая гамма, лямбда от х0 такое,
[01:11:29.140 --> 01:11:41.980]  что g от x, это такое, что для всех вообще x, g от x минус g от x0 больше всего равно, чем лямбда от x0 умножить на x минус x0.
[01:11:41.980 --> 01:11:50.460]  Это точно то, что я сказал, потому что прямая, которая проходит через эту точку,
[01:11:50.460 --> 01:11:57.340]  она лежит полностью ниже, чем весь график функции, только не единственная в этой точке x0,
[01:11:57.340 --> 01:12:06.940]  его пересекательность касательная. Теперь давайте сделаем следующее, давайте в качестве x0 возьмем
[01:12:06.940 --> 01:12:24.220]  мотождание x, а в качестве x возьмем x. Что мы получим? Мы получим, что g от x минус g от мотождания x
[01:12:24.220 --> 01:12:33.780]  больше равно, чем лямбда от мотождания x умножить на x минус мотождание x.
[01:12:33.780 --> 01:12:39.820]  А теперь возьмем мотождание от обеих частей этого неравенства.
[01:12:39.820 --> 01:12:47.140]  Да, имеем право, потому что мы знаем, что от правой части мы мотождание можем взять из-за того,
[01:12:47.140 --> 01:12:51.980]  что мотождание может быть конечное. То есть у меня в правой части все не случайно, кроме x.
[01:12:51.980 --> 01:12:57.540]  Да, в этой правой части только x случайно, в состоянии не случайно, поэтому мотождание от правой части
[01:12:57.540 --> 01:13:08.140]  существует. И значит верно следующее неравенство, что мотождание от g от x минус g от мотождания x
[01:13:08.140 --> 01:13:19.420]  больше либо равно, чем мотождание от правой части. По линейности это есть лямбда от x, это константа.
[01:13:19.420 --> 01:13:32.980]  Умножаем отдел на мотождание x минус мотождание x, то есть на ноль. И по линейности в левой части
[01:13:32.980 --> 01:13:40.460]  мы получаем, что мотождание от g от x минус g от мотождания x это тоже какая-то константа,
[01:13:40.460 --> 01:13:48.020]  больше нулю, что и требуется. Тем самым неравенство янса тоже доказано.
[01:13:48.020 --> 01:13:51.780]  Есть ли какие-то вопросы?
[01:13:59.780 --> 01:14:04.820]  Ну сразу примеры, как я обещал, я это устно произнес, давайте еще пропишу на всякий случай.
[01:14:04.820 --> 01:14:12.900]  Для тех, кому будет лень пересматривать это видео, кто будет смотреть только конспект,
[01:14:12.900 --> 01:14:21.660]  вдруг такие люди найдутся. Те примеры, которые мы с вами уже разбирали. Во-первых, если в качестве g
[01:14:21.660 --> 01:14:32.500]  выбрать с модуль x, вы получите, что мотождание модуля x больше либо равно, чем мотождание от
[01:14:32.500 --> 01:14:42.580]  x. Во-вторых, если в качестве g от x выбрать x квадрат, то вы получите известное вам
[01:14:42.580 --> 01:14:47.300]  не отрицательное с дисперсиумом. Отжидание x квадрата больше равно, чем квадрат мотождания.
[01:14:47.300 --> 01:14:54.140]  Значит, дальнейшая тема, о которой мы с вами будем говорить, это условное математическое
[01:14:54.140 --> 01:15:01.340]  ожидание. Это, наверное, одна из самых сложных тем этого курса, если не самая сложная. Я не хочу
[01:15:01.420 --> 01:15:07.020]  разрывать эту тему на две части. Я по-любому придется разрывать. Мы за одну лекцию целиком
[01:15:07.020 --> 01:15:12.140]  успеем полностью обсудить все, что касается условного мотождания. Но я оставшиеся восемь минут
[01:15:12.140 --> 01:15:18.620]  не хочу тратить на то, чтобы давать определение, потому что я боюсь, что ухудшится понимание. На
[01:15:18.620 --> 01:15:22.660]  следующей лекции мы целиком с вами разберем большой блок, касающийся мотождания, чтобы было
[01:15:22.660 --> 01:15:28.340]  понятно условное мотождание. Давайте я пару минут потрачу на то, чтобы объяснить, о чем там
[01:15:28.340 --> 01:15:34.580]  пойдет речь и что это вообще с условным мотожданием такое с точки зрения интуиции. Я когда
[01:15:34.580 --> 01:15:38.540]  говорил про математическое ожидание, я на следующей лекции еще это повторю, но просто чтобы
[01:15:38.540 --> 01:15:45.660]  анонсировать. Когда я говорил про математическое ожидание, одна из мотиваций к его рассмотрению
[01:15:45.660 --> 01:15:50.700]  была статистическая. То есть мы говорили о том, что если мы хотим предсказать, например, в будущем
[01:15:50.700 --> 01:15:55.300]  значение случайной величины, мы проведем много раз эксперимент, посмотрим, чему равно значение
[01:15:55.300 --> 01:15:59.500]  в среднем. Ну и скажем, значит где-то в будущем мы ожидаем вот это значение, ну или примерно его.
[01:15:59.500 --> 01:16:06.380]  Представьте, что у вас есть некоторый процесс. Это не просто одна и та же случайная величина,
[01:16:06.380 --> 01:16:17.620]  которую вы много раз реализуете, а стоимость цена какой-нибудь акции, валюты, там еще что-то,
[01:16:17.620 --> 01:16:25.020]  что изменяется непрерывно во время, в общем изменяется. И у вас на самом деле
[01:16:25.060 --> 01:16:34.260]  сегодня одно распределение, а завтра уже какое-то другое. И довольно сложно делать прогнозы. В принципе,
[01:16:34.260 --> 01:16:39.380]  если вы знаете какое распределение там вашей случайной величины, там вашей стоимости акции,
[01:16:39.380 --> 01:16:45.380]  еще чего-то будет через год, вот вам интересно на самом деле, допустим, что там будет через год
[01:16:45.380 --> 01:16:53.300]  с вашим случайным процессом. Вот вы знаете, скажем, средний через год, ну например. Ну вы скажете,
[01:16:53.300 --> 01:16:59.140]  ну окей, я ожидаю, что значение будет равно 100, потому что 100 это ваше среднее. А теперь представьте,
[01:16:59.140 --> 01:17:04.420]  что прошло полгода, и за эти полгода появилось очень много информации относительно того,
[01:17:04.420 --> 01:17:09.340]  как ваш процесс был устроен. И вы уже, конечно, будете предсказывать нечто другое, потому что за
[01:17:09.340 --> 01:17:13.260]  счет того, что вы стали обладать большим объемом информации, ваше предсказание может очень сильно
[01:17:13.260 --> 01:17:22.940]  изменить. Вы снова в качестве предсказания будете использовать среднее, но это среднее уже будет
[01:17:22.940 --> 01:17:29.620]  функцией от истории. То есть вот за эти полгода, как изменялось значение вашего случайного
[01:17:29.620 --> 01:17:35.060]  процесса, будет влиять на ваше среднее. То есть условное математическое ожидание — это функция от
[01:17:35.060 --> 01:17:39.780]  условия. В качестве условия мы подразумеваем, вот как раз историю, вот у вас есть какая-то
[01:17:39.780 --> 01:17:46.140]  история вашего случайного процесса в течение полгода. В зависимости от того, какая, какая
[01:17:46.140 --> 01:17:49.840]  у вас история есть, вы получаете то или иное значение от ожидания. Вот это и есть условный
[01:17:49.840 --> 01:17:56.400]  от ожидания, о котором мы с вами в следующий раз будем говорить. В общем-то, на этом все.
[01:17:56.400 --> 01:18:07.280]  Если есть какие-то вопросы, пожалуйста, задавайте. Если вопросов нет, всем спасибо.
[01:18:07.280 --> 01:18:10.120]  До встречи в следующую субботу. Хорошей недели. До свидания.

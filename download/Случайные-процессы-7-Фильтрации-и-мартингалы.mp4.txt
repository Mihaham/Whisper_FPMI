[00:00.000 --> 00:09.600]  Давайте я напомню, чем мы закончили.
[00:09.600 --> 00:15.840]  Значит, Jensen для условных средних.
[00:15.840 --> 00:24.080]  Выглядит это так.
[00:24.080 --> 00:43.140]  Выпуклая функция от условного мат ожидания оценивается через условные
[00:43.140 --> 00:50.740]  мат ожидания от композиций, значит, V выпуклая.
[00:50.740 --> 01:01.580]  Ну вот мы закончили на том, что я предложил это доказать каким-нибудь способом.
[01:01.580 --> 01:05.140]  Ну и вот указал стандартный способ.
[01:05.140 --> 01:19.340]  Это делается сначала для функций простых, ну а потом, если это для простых верно, то с помощью приближений
[01:19.340 --> 01:24.860]  распространяется. Ну это стандартный способ.
[01:24.860 --> 01:28.660]  А другой способ я предложил такой.
[01:28.660 --> 01:35.500]  Просто проверить это, ну попытаться проверить это по определению.
[01:35.500 --> 01:44.020]  Ведь тут что нужно, значит, смотрите, что такое неравенство для условных мат ожиданий для функций.
[01:44.020 --> 01:51.900]  Это значит неравенство для интегралов по множествам из сигма-логебры.
[01:51.900 --> 01:59.820]  Поэтому такой как бы напрашивающий способ проинтегрировать по множеству и пытаться применить обычного Янцина.
[01:59.820 --> 02:09.700]  Но я потом стал в электричке это обдумывать и понял, что таким способом, наверное, не сделаешь.
[02:09.700 --> 02:13.580]  Ну вот не знаю, может быть, у кого-нибудь из вас так получилось сделать.
[02:13.580 --> 02:21.020]  Никто не думал над этим. Ну вот попробуйте, это любопытно, можно ли так сделать.
[02:21.020 --> 02:44.020]  Ну а стандартный способ такой. Сначала для простых, потом для ограниченных измеримых, потом для всех.
[02:44.020 --> 02:52.020]  Эти дальнейшие шаги рутинные, а давайте посмотрим вот этот первый шаг.
[02:52.020 --> 03:01.020]  Тут уже понятно, что если для каких-то функций верно, и они равномерно сходятся, ну в пределе получится.
[03:01.020 --> 03:08.020]  А для общих, ну там можно монотонные устаревать пределы. Это уже рутинная вещь.
[03:08.020 --> 03:14.020]  А вот давайте посмотрим, как это сделать для простых функций.
[03:14.020 --> 03:22.020]  Значит, смотрите, кто такая простая функция?
[03:22.020 --> 03:35.020]  Значит, простая функция это линейная комбинация нескольких индикаторов, множеств измеримых.
[03:35.020 --> 03:41.020]  Но, конечно же, не обязательно из-под сигма-алгебры. У нас ведь какая ситуация?
[03:41.020 --> 03:50.020]  У нас под сигма-алгебра, значит, она часть большой сигма-алгебры.
[03:50.020 --> 03:54.020]  И функции измеримые относительно вот этой большой.
[03:54.020 --> 04:00.020]  А если бы функция была измерима относительно А, то тут вообще делать было бы нечего,
[04:00.020 --> 04:05.020]  потому что тут тогда условная мат ожидания совпадает с самой функцией.
[04:05.020 --> 04:15.020]  Поэтому содержательно это становится, когда функция как раз F относительно A не измерима, а измерима относительно большей.
[04:15.020 --> 04:23.020]  И вот тогда, значит, для такой функции как выглядит условная мат ожидания?
[04:23.020 --> 04:31.020]  Ну, условная мат ожидания это, очевидно, будет вот что.
[04:31.020 --> 04:35.020]  Это из-за линейности.
[04:35.020 --> 04:42.020]  Значит, константы вынесутся, и здесь будет стоять вот что.
[04:42.020 --> 04:47.020]  Значит, вот так выглядит условная мат ожидания.
[04:47.020 --> 04:53.020]  Давайте теперь заметим, что сумма индикаторов множеств...
[04:53.020 --> 05:00.020]  Да, я забыл сказать, тут я имею в виду, конечно, что они дизюнкные.
[05:00.020 --> 05:03.020]  Значит, они дизюнкные, так, эти множества.
[05:03.020 --> 05:11.020]  Так всегда можно простую функцию записать с помощью дизюнкных множеств, перечислив все ее разные значения.
[05:11.020 --> 05:26.020]  Значит, смотрите, что у нас есть. У нас поточечна, вот это, раз это разбиение пространства, то сумма их индикаторов поточечна тождественно равна единице.
[05:26.020 --> 05:39.020]  Поэтому, когда мы переходим к условному мат ожиданию, то для условного мат ожидания получается вот такая вещь.
[05:41.020 --> 05:53.020]  Значит, вот это равно единице почти всюду.
[05:53.020 --> 06:03.020]  Ну, конечно, здесь тоже, как всегда для измеримых функций, тут, конечно, неравенство тоже почти всюду.
[06:03.020 --> 06:11.020]  Значит, смотрите, от единицы условное мат ожидания единицы.
[06:11.020 --> 06:22.020]  Но условное мат ожидания, как мы помним, это не одна какая-то функция, это целый класс измеримых между собой функций.
[06:22.020 --> 06:31.020]  Ну, конечно, когда у вас это верно почти всюду, можно заменить на версии так, что будет просто всюду верно.
[06:31.020 --> 06:42.020]  Значит, смотрите, какая тут оказалась интересная вещь.
[06:42.020 --> 06:52.020]  Еще с учетом того, что вот эти неотрицательные, смотрите, что у нас получается.
[06:52.020 --> 07:01.020]  У нас получились при каждом х, при каждом х, когда это неотрицательно и это единица.
[07:01.020 --> 07:07.020]  Это просто числа неотрицательные, сумма единицы.
[07:07.020 --> 07:21.020]  И тогда получаем, что почти всюду, ну, то есть всюду, где сумма единицы и они неотрицательные, а это все почти всюду выполнено.
[07:21.020 --> 07:29.020]  А у нас тут стоит, вот тут стоит, выпуклая комбинация чисел.
[07:29.020 --> 07:34.020]  Вот здесь стоит, видите, выпуклая комбинация чисел.
[07:34.020 --> 07:43.020]  Поэтому получается v от того, что тут написано, функция v выпуклая.
[07:43.020 --> 07:54.020]  Это значит, что когда аргумент есть выпуклая комбинация чего-то, то значение оценивается выпуклой комбинацией этих чего-то.
[07:54.020 --> 08:03.020]  И поэтому получается, из-за выпуклости получается вот такая вещь.
[08:03.020 --> 08:13.020]  Сейчас, только я не те числа стал вставлять.
[08:13.020 --> 08:22.020]  Наоборот, вот эти числа, они умножаются на значения, вот так.
[08:22.020 --> 08:28.020]  Потому что эти значения, берется их выпуклая комбинация.
[08:28.020 --> 08:38.020]  Вот как раз вот эти вот условные мат ожидания, они выступают как коэффициенты.
[08:38.020 --> 08:48.020]  А что же такое написано в правой части?
[08:48.020 --> 09:00.020]  В правой части как раз написано условное мат ожидания тоже простой функции v от f.
[09:00.020 --> 09:08.020]  Потому что f принимало значения c1, cn на каких-то множествах дизюнкных.
[09:08.020 --> 09:18.020]  А v от f соответственно принимает значения на этих же дизюнкных множествах, но другие значения, в которые v подставили значение f.
[09:18.020 --> 09:28.020]  Но это и есть, это тоже простая функция.
[09:28.020 --> 09:38.020]  И точно также вот эта сумма, это есть ее условное мат ожидания.
[09:38.020 --> 09:45.020]  Потому что v от f, кто такой v от f в этом представлении?
[09:45.020 --> 09:51.020]  А в этом представлении это такая же штука, но тут вместо c появились v от c.
[09:51.020 --> 10:04.020]  Ну вот так что видите, получается, что для простых функций это более-менее следствие, непосредственно можно сказать следствие выпуклости, определение выпуклости.
[10:04.020 --> 10:10.020]  У нас такое определение выпуклости функции можно считать.
[10:10.020 --> 10:24.020]  А дальше, значит дальше, от простых к ограниченным равномерный предел, а от ограниченных к неограниченным, ну вот с помощью срезок там дальнейший предел.
[10:24.020 --> 10:29.020]  Но это я уже не буду на этом останавливаться.
[10:29.020 --> 10:37.020]  При предельном переходе в каком месте?
[10:37.020 --> 10:44.020]  Нет, нет, но почему же от простых к ограниченным как раз все окей.
[10:44.020 --> 10:56.020]  Потому что представьте себе, что f равномерно сходится fn, ну вот пусть для fn это верно, и fn равномерно сходится к f.
[10:56.020 --> 11:10.020]  Ну тогда композиции будут равномерно сходиться, выпуклая функция она же не прывна, а и fn тогда в этом случае принимают значение в каком-то отрезке.
[11:10.020 --> 11:21.020]  Нет, ну как, если функции сходятся равномерно, то с предельным переходом под интегралом проблем не возникает.
[11:21.020 --> 11:39.020]  Так что смотрите, если, давайте вот это я уберу, значит с этим закончили.
[11:39.020 --> 11:58.020]  Значит смотрите, если fn равномерно сходится к f, то v от fn равномерно сходится к v от f.
[11:58.020 --> 12:27.020]  Дальше, из сходимости равномерной функции следует, если fn сходится равномерно к f, то из свойств условных средних следует, что и условные средние тоже равномерно сходятся.
[12:27.020 --> 12:42.020]  Так что, ну а поскольку всякую ограниченную измеримую можно равномерно приблизить простыми, то для ограниченных всё окей.
[12:42.020 --> 12:54.020]  Больше забот может доставить следующий случай, смотрите, какой следующий случай. Следующий случай, когда вы это знаете для ограниченных всех измеримых.
[12:54.020 --> 13:02.020]  А функция f у вас не ограниченная, вот тогда как быть, вот тогда возникает какой вопрос.
[13:02.020 --> 13:21.020]  Если функция f уже не обязательно ограниченная, тогда, разумеется, в этом случае требуется, чтобы она была интегрируема, конечно, и чтобы вот это была интегрируема.
[13:21.020 --> 13:26.020]  Потому что, если вы выпуклую подставили ограниченную, она может перестать быть интегрируемой.
[13:26.020 --> 13:33.020]  Ну, например, была у вас f интегрируемая, а f квадрат нет, вот вы её в квадрат подставили, потеряли интегрируемость.
[13:33.020 --> 13:39.020]  Поэтому в этом случае, конечно, дополнительно требуется, что это тоже интегрируемо.
[13:39.020 --> 13:47.020]  Вот тогда вот это тут уже равномерных, тут уже равномерными переходами не обойтись.
[13:47.020 --> 13:50.020]  Тут равномерными переходами не обойтись.
[13:50.020 --> 13:56.020]  А значит, тут это надо отдельно обосновывать.
[13:56.020 --> 14:01.020]  Ну, в этом случае, в этом случае используются стандартные срезки.
[14:01.020 --> 14:06.020]  В этом случае функция обрезается.
[14:06.020 --> 14:18.020]  Сначала замечаем, что равенство в силе, когда функция заменена на такую, там, где была от минус n до n, она прежняя.
[14:18.020 --> 14:26.020]  Там, где она больше n, она от n делаем принудительно, а там, где меньше минус n, делаем минус n принудительно.
[14:26.020 --> 14:28.020]  Для таких верно.
[14:28.020 --> 14:33.020]  Ну и после этого, после этого надо проверить.
[14:33.020 --> 14:41.020]  Давайте это не будем делать, потому что это нас как-то совсем уведёт куда-то в прошлый семестр, в интегралы.
[14:41.020 --> 14:49.020]  В этом случае надо проверить, что будет по-прежнему сходимость.
[14:49.020 --> 14:54.020]  Этот последний шаг, это действительно надо проверить.
[14:54.020 --> 14:57.020]  Ну, в этом случае смотрите, что получается.
[14:57.020 --> 15:08.020]  Когда вы функцию так заменяете на такие срезки, так, ну вот давайте я их напишу, на такие заменяете.
[15:08.020 --> 15:20.020]  Значит, f, это если f по модулю меньше либо равно n, n если f больше n, и минус n если f меньше минус n.
[15:20.020 --> 15:22.020]  Вот на такие заменяете.
[15:22.020 --> 15:31.020]  То такие функции сходятся к f в L1.
[15:31.020 --> 15:37.020]  Значит, они в среднем сходятся.
[15:37.020 --> 15:47.020]  А v от них, v от них сходятся тоже в среднем к v от f.
[15:47.020 --> 15:50.020]  Ну, это в общем довольно легко проверить.
[15:50.020 --> 16:03.020]  И поэтому нам нужно ещё воспользоваться тем, нам нужно воспользоваться таким, таким, значит, свойством.
[16:03.020 --> 16:07.020]  Ну, это свойство вытекает из того, что мы обсуждали.
[16:07.020 --> 16:27.020]  Если phi n-ные сходятся в L1 к phi, то а условные мат ожидания сходятся в L1 к условному мат ожидания.
[16:27.020 --> 16:29.020]  Ещё нам такое свойство.
[16:29.020 --> 16:39.020]  Ну, это свойство вытекает из того способа построения условного среднего, который мы обсуждали.
[16:39.020 --> 16:54.020]  Мы обсуждали, что интеграл, ну, мы обсуждали, что среднее, взятие условного среднего, это такое на пространстве L1, это такое сжатие, оно норму не увеличивает.
[16:54.020 --> 17:13.020]  Поэтому следствием того, что у нас, смотрите, условное среднее, это такой линейный оператор на L1, который не увеличивает норму и на функциях A измеримых тождественной.
[17:13.020 --> 17:20.020]  Ну, значит, на самом деле вот примерно этим всё и определяется.
[17:20.020 --> 17:23.020]  И поэтому получается такое свойство.
[17:23.020 --> 17:39.020]  Ну, тогда понятно, что можно дальше, ну, тогда вот можно такими пользоваться и переходить к пределу с помощью этого соображения в нерайности.
[17:39.020 --> 17:55.020]  Но из Янсона следует, что условное среднее, что оно сжатие также на всех Lp.
[17:55.020 --> 18:19.020]  Значит, следствие, значит, если P, значит, уже не обязательно единица, так, и F лежит в Lp, то тогда условное среднее, условное среднее тоже лежит в Lp.
[18:19.020 --> 18:26.020]  И норма, ну, давайте, чтобы нормы не писать, давайте напишем через мат ожидания.
[18:26.020 --> 18:38.020]  Мат ожидания условного среднего в степени P оценивается через мат ожидания просто самого F.
[18:38.020 --> 18:42.020]  Ну, это надо применить Янсона.
[18:42.020 --> 18:45.020]  К какой функции? Ну, понятно, к какой.
[18:45.020 --> 18:51.020]  Нужно в качестве Vat взять модуль T в степени P.
[18:51.020 --> 18:56.020]  Вот такую, ну, разумеется, надо убедиться, что это выпуклая функция.
[18:56.020 --> 18:59.020]  Как убедиться, что это выпуклая функция?
[18:59.020 --> 19:11.020]  Ну, два, например, дважды продиференцировав ее и воспользовавшись тем, что получится неотрицательное производное.
[19:11.020 --> 19:20.020]  Значит, применив эту функцию, ну, собственно, как применяется здесь это нерая инстинкция Янсона?
[19:20.020 --> 19:28.020]  Тут ведь так, чтобы его применять, вообще-то ведь надо уже заранее знать, что Vat F интегрируемо.
[19:28.020 --> 19:32.020]  А в этом следствии это часть утверждения.
[19:32.020 --> 19:46.020]  Видите, так что, чтобы вот этого написать, надо в Янсоне знать, что эта штука в Lp, что условное среднее в Lp.
[19:46.020 --> 19:51.020]  Вот если мы это уже знаем, то Янсон говорит, что будет вот так.
[19:51.020 --> 19:57.020]  Но откуда мы знаем, что условное среднее от функции за Lp, что она тоже в Lp?
[19:57.020 --> 19:59.020]  Но это опять с помощью срезок.
[19:59.020 --> 20:07.020]  Когда мы обрезаем эту функцию вот тем стандартным способом, то что же получается?
[20:07.020 --> 20:20.020]  Получается, что вот те срезки сходятся к нашей функции в Lp, и их нормы, их Lp-нормы, оцениваются ее Lp-нормой.
[20:20.020 --> 20:26.020]  Но тогда для них это не равнится, можно применять, они же ограниченные срезки.
[20:26.020 --> 20:38.020]  Ну и тогда получается, что тут можно, как обычно это делают, когда что-то оценки какие-то доказывают,
[20:38.020 --> 20:41.020]  сводят к функциям неотрицательным.
[20:41.020 --> 20:50.020]  Если функция f неотрицательна, то вот этой части с минусом нет, и просто она по уровню n обрезается.
[20:50.020 --> 21:03.020]  И тогда эти срезки к ней возрастают, ну и тогда видно, что условные средние возрастают к условному среднему.
[21:03.020 --> 21:11.020]  Ну и поскольку все они в Lp ограничены, то предел тоже ограничен в Lp.
[21:11.020 --> 21:21.020]  В итоге оказывается и функция нужная в Lp, и норма ее нужным образом оценивается.
[21:21.020 --> 21:31.020]  Поэтому получается, видите, что условное среднее, оно не только на L1-сжатие, как это было по исходному построению,
[21:31.020 --> 21:39.020]  но и на всех Lp. Ну я тут, кстати, исключил бесконечность, ее, конечно, можно включить.
[21:39.020 --> 21:48.020]  То, что для бесконечности это верно, то есть что Supremum оценивается, это тоже в базовых свойствах, так что это тоже верно, конечно.
[21:48.020 --> 21:57.020]  Но тут уже никакой Jensen не требуется, поэтому я тут включил в следствие только то, что реально с Jensen связано.
[21:57.020 --> 22:03.020]  Так, теперь, значит, вот этот надо рассматривать.
[22:03.020 --> 22:32.020]  Так, теперь, значит, вот этот надо рассматривать как некое длинное, так сказать, техническое отступление,
[22:32.020 --> 22:38.020]  которое нужно для того, чтобы обсудить оставшиеся два класса процессов.
[22:38.020 --> 22:42.020]  Это Мартин Галлы и Марковские процессы.
[22:42.020 --> 22:53.020]  Я вот напоминаю, что в нашем курсе несколько классов конкретных процессов,
[22:53.020 --> 23:01.020]  ну самые важные это Виннеровский и Пуассоновский, и несколько классов общих процессов,
[23:01.020 --> 23:08.020]  представителями которых Виннеровский и Пуассоновский являются.
[23:08.020 --> 23:14.020]  А эти общие классы, вот у нас был класс процессов с независимыми превращениями,
[23:14.020 --> 23:23.020]  потом у нас был класс Гауссовских процессов, и еще парочка сейчас появится там как раз,
[23:23.020 --> 23:28.020]  где нужны вот эти условные и средние. Для тех двух условных никаких средних не нужно было.
[23:28.020 --> 23:38.020]  Значит, появятся у нас еще сейчас процессы, значит, еще два класса процессов, Мартин Галлы и Марковский.
[23:38.020 --> 23:44.020]  Ну и в конце у нас появится некий подкласс, но он такой очень специфический,
[23:44.020 --> 23:53.020]  поэтому это отдельно обсуждается. Это Марковские цепи и ветвящиеся процессы, это просто отдельные примеры у нас будут.
[23:53.020 --> 24:02.020]  Так что это будет некий такой микс, так сказать, представителей конкретных процессов и конкретных классов.
[24:02.020 --> 24:12.020]  Значит, вот следующий раздел фильтрации и Мартин Галлы.
[24:12.020 --> 24:26.020]  Значит, вот есть основное вероятностное пространство,
[24:26.020 --> 24:42.020]  и есть набор под сигма-алгебр, значит, вот в этой основной сигма-алгебре,
[24:42.020 --> 24:51.020]  чтобы подчеркнуть, что это сигма-алгебры некой другой природы, я их буду обозначать другой буквы F,
[24:51.020 --> 24:56.020]  чтобы не путать с основной сигма-алгеброй даже и без индексов.
[24:56.020 --> 25:05.020]  Но при этом давайте предполагать, что T из какого-то множества напрямой,
[25:05.020 --> 25:18.020]  но в действительности для определения Мартин Галла это не очень важно, и для определения фильтрации важно, чтобы это множество индексов было упорядочено.
[25:18.020 --> 25:30.020]  Но для наших целей никаких других множеств индексов, кроме таких стандартных, что это полупрямая или натуральные числа или отрезок,
[25:30.020 --> 25:34.020]  никаких других у нас не будет кроме этих стандартных.
[25:34.020 --> 25:56.020]  Теперь говорят, что вот эта вот Ft фильтрация, если F от s содержится в F от t при s меньше t.
[25:56.020 --> 26:17.020]  Я смотрю, чтобы не сойти с обозначений конспекта, потому что тут канонических каких-то нет обозначений, но, естественно, я стараюсь следовать тем, что в конспекте.
[26:17.020 --> 26:27.020]  Смотрите, в большинстве приложений, какой практический смысл этой фильтрации.
[26:27.020 --> 26:44.020]  T, как обычно, символизирует время, множество t символизирует время, а F от t это сигма-алгебра событий о наступлении или ненаступлении которых до момента времени t стало известно.
[26:44.020 --> 26:55.020]  Вот в большинстве реальных задач и приложений вот такой практический смысл, и вот ради этого все это и вводится, чтобы обсуждать такие события.
[26:55.020 --> 27:18.020]  Ну и вот определение. Процесс Xat согласован с фильтрацией,
[27:18.020 --> 27:30.020]  если Xat Ft измеримо для всех t, ну из этого множество индексов.
[27:30.020 --> 27:44.020]  Ну вот банальный пример. Ну он банальный, но, как ни странно, бывает часто полезен.
[27:44.020 --> 28:00.020]  F от t это сигма-алгебра, порожденная случайными величинами Xs до момента t включительно.
[28:00.020 --> 28:10.020]  Вот видите, пожалуйста, получается фильтрация, потому что с ростом t это увеличивается.
[28:10.020 --> 28:21.020]  И по определению Xat t измерим. Так что вот часто такой используемый пример.
[28:21.020 --> 28:33.020]  Теперь важное определение.
[28:33.020 --> 28:37.020]  Сейчас, ну раз оно важное, давайте я даже сотру все, что было.
[29:03.020 --> 29:27.020]  Значит, определение. Так давайте я тоже, чтобы смотрю, чтобы не было расхождений.
[29:27.020 --> 29:37.020]  Прямо в точности буду писать, как конспекты.
[29:37.020 --> 29:49.020]  Значит, мартингау относительно фильтрации.
[29:49.020 --> 29:54.020]  Ну фильтрации еще, кстати, иногда называют потоками сигма-алгебр.
[29:54.020 --> 30:12.020]  Если, ну это естественно процесс, если все вот эти входят в L1, то есть интегрируемые, значит у них есть условный средний.
[30:12.020 --> 30:27.020]  И, значит, да, значит Xat согласован. Согласован с этой фильтрацией.
[30:27.020 --> 30:32.020]  Ну то есть каждая Xt измерима относительно этой Ft.
[30:32.020 --> 30:52.020]  И условное среднее Xat относительно Fs есть Xs при s меньше либо равном t.
[30:52.020 --> 31:07.020]  Значит, это приобретает особо наглядный смысл, если эти функции лежат в L2, не в L1.
[31:07.020 --> 31:26.020]  Я напомню, что когда функция из L2, то условное среднее это артагональная проекция на подпространство измеримых относительно сигма-алгебр функций.
[31:26.020 --> 31:37.020]  И тут смотрите, что происходит. Когда мы варьируем вот этот индекс S, то это получаются такие возрастающие подпространства в L2.
[31:37.020 --> 31:49.020]  И происходит следующее, когда вы функцию в момент времени t артагонально проектируете на подпространство с меньшим временем,
[31:49.020 --> 31:55.020]  то должна получиться то, что было у этой функции в этот меньший момент времени.
[31:55.020 --> 32:05.020]  То есть такая получается интересная спираль в Гильбертовом пространстве с такими согласованными проекциями.
[32:05.020 --> 32:14.020]  У вас какая-то кривая функция, ксиатем можно считать, что такая кривая в Гильбертовом пространстве.
[32:14.020 --> 32:21.020]  И у вас еще такая вот кривая сигма-алгебра, и они друг с другом согласованы.
[32:21.020 --> 32:29.020]  Вот смысл этих артагональных проекций, когда вы проектируете с большего на меньшее, то получаете функцию в меньший момент времени.
[32:29.020 --> 32:43.020]  Вот простейшие примеры, но они оказываются очень распространенными.
[32:43.020 --> 33:04.020]  Значит давайте сделаем время. Сейчас только давайте я посмотрю, чтобы порядок примеров тоже совпадал с тем, что в конце.
[33:04.020 --> 33:09.020]  Ну это конечно не обязательно, но все-таки лучше, чтобы так было.
[33:09.020 --> 33:14.020]  Значит смотрите, пусть время это дискретное время, натуральные числа.
[33:14.020 --> 33:24.020]  И ксиенные, сейчас только давайте будут не ксиенные, а этэнные независимые случайные величины.
[33:24.020 --> 33:33.020]  Так, ну вот тут я уже кажется отошел от обозначений конспекта.
[33:33.020 --> 33:41.020]  Да, ну ладно, не буду исправляться, немножко отошел.
[33:41.020 --> 33:48.020]  Значит со средними, со средними ноль.
[33:48.020 --> 33:56.020]  А ксиенные, это их сумма от 1 до n.
[33:56.020 --> 34:02.020]  Ну вот у меня в конспекте сумма sn, и мартингал будет не ксин, а sn.
[34:02.020 --> 34:12.020]  Но тут давайте, чтобы не было путаницы, раз в определении буква кси, то и пусть здесь будет буква кси.
[34:12.020 --> 34:27.020]  Потому что, смотрите, мартингалом является не вот эта последовательность независимых, а мартингалом является последовательность сумм.
[34:27.020 --> 34:31.020]  Видите, вот этих независимых.
[34:31.020 --> 34:37.020]  Ну давайте сейчас это проверим, но прежде чем...
[34:37.020 --> 34:43.020]  А, фильтрация, да, совершенно справедливый вопрос.
[34:43.020 --> 34:53.020]  Фильтрация Fn это порожденная первыми n.
[34:53.020 --> 34:58.020]  Ну то есть, так сказать, самопорожденная фильтрация.
[34:58.020 --> 35:10.020]  Ну вообще эти самопорожденные фильтрации очень часто используются в качестве того относительно чего меряются всякие свойства мартингальные, марковские свойства и так далее.
[35:10.020 --> 35:21.020]  Значит смотрите, тут еще уместно заметить про терминологию, что за странное название мартингал.
[35:21.020 --> 35:37.020]  Но в точности не очень понятно, что имели в виду, так сказать, отцы-основатели, они это как-то не прокомментировали,
[35:37.020 --> 35:57.020]  но в принципе есть, ну у слова мартингал есть два таких, так сказать, ну вот помимо вот этого уже чисто математического термина, то, так сказать, житейских, два таких есть значения.
[35:57.020 --> 36:07.020]  Одно, это такое довольно старинное французское значение, это там часть, часть уздечки.
[36:07.020 --> 36:15.020]  Ну имелось ли в виду это отцами-основателями, ну не очень ясно.
[36:15.020 --> 36:38.020]  Еще это такое уже более позднее, но тоже довольно старое, тоже французское, такое немного жаргонное выражение для стратегии игры, ну вот такой типа, игры там типа Орлянки, в которой при проигрыше удваиваются ставки.
[36:38.020 --> 36:44.020]  Ну вот что они имели в виду, ну вот не спросишь, так сказать, теперь.
[36:44.020 --> 36:55.020]  Но термин, математический термин, это как раз в отличие от этих чисто лингвистических, он не очень старый, ну вот в тридцатых годах он возник.
[36:55.020 --> 37:13.020]  Но теперь я думаю, что если наугад тыкнуть в интернет, то скорее всего большинство ссылок попадет скорее всего на этот математический термин, а вот не на те два, ну так сказать, несколько архаичных.
[37:13.020 --> 37:27.020]  Значит, давайте проверим, значит, давайте проверим, почему, значит, это так.
[37:27.020 --> 37:55.020]  Значит, проверка, пусть k меньше n, значит, следующее, что когда мы берем, проектируем на fk, то должно получиться вот это.
[37:55.020 --> 38:15.020]  Ну понятно, что достаточно проверить для предыдущего, значит, достаточно проверить для k равного n-1, вот это достаточно проверить, потому что потом за несколько шагов можно к меньшему прийти будет.
[38:15.020 --> 38:34.020]  Давайте, значит, тогда заметим, что эта штука, это же сумма, так, и в этой сумме смотрите, кто стоит.
[38:34.020 --> 38:54.020]  Тут стоят, значит, n-, ну первые стоят, так, и потом стоит последняя, но первые измеримы относительно вот этой, так, ну давайте это я подробнее подробнее напишу.
[38:54.020 --> 39:09.020]  Так, а значит, первые измеримы, поэтому это будет их сумма просто, значит, которая есть по определению, что у нас есть.
[39:09.020 --> 39:37.020]  Так, а значит, первые измеримы, поэтому это будет их сумма просто, значит, которая есть по определению предыдущие, так, плюс условное мат ожидания последней штуки относительно сигма алгебры, порожденной предыдущими.
[39:37.020 --> 39:54.020]  Так, и поэтому нам надо убедиться, что последнего просто нет, нам надо убедиться, что вот это равно нулю, вот в этом надо теперь убедиться.
[39:54.020 --> 40:22.020]  Ну давайте в этом убедимся, давайте в этом убедимся.
[40:22.020 --> 40:47.020]  Значит, смотрите, значит, что это значит, то есть это значит, что когда вы интегрируете по множеству a вот эту, это n, то это должно стать нулем для всех a,
[40:47.020 --> 41:06.020]  для всех a из сигма алгебры, порожденной предыдущими.
[41:06.020 --> 41:15.020]  А как устроена, как устроен индикатор такого множества?
[41:15.020 --> 41:27.020]  Значит, смотрите, значит, это индикатор множества, ну или лучше сказать, функция измеримая относительно сигма алгебры, порожденной несколькими.
[41:27.020 --> 41:40.020]  Но значит, из этого следует, что это есть результат подстановки в некую барелевскую функцию,
[41:40.020 --> 42:03.020]  барелевскую на Rn-1. Вот этих функций, потому что множество измеримые, то есть множество из сигма алгебры, порожденные несколькими случайными величинами,
[42:03.020 --> 42:17.020]  ну и вообще функции измеримой относительно сигма алгебры, порожденной несколькими случайными величинами, это не что иное, как результат подстановки этих случайных величин,
[42:17.020 --> 42:22.020]  ну этих функций лучше сказать, ну в барелевские функции.
[42:22.020 --> 42:39.020]  Когда вы перебираете всевозможные барелевские функции на Rn-1 и подставляете в них в качестве аргументов вот эти вот порождающие функции, то это дает вам все измеримые функции.
[42:39.020 --> 42:57.020]  Но тогда, смотрите, что у вас получается, тогда у вас получается интеграл по омега вот от такой вот штуки умножить еще на вот это.
[42:57.020 --> 43:23.020]  Но вспоминаем, что они независимы, так, поэтому это получается интеграл вот от этих на интеграл вот последний, значит, это независимость, вот это из-за независимости.
[43:23.020 --> 43:33.020]  Значит, вспоминаем, что когда случайные величины независимы, то интеграл произведения раскалывается.
[43:33.020 --> 43:42.020]  Но тут одна из них, это вот эта, а последняя с ними независима, значит, они раскололись.
[43:42.020 --> 44:02.020]  Но интеграл ноль, потому что сейчас, я вот только забыл, я должен был указать, что они с нолевым от ожидания.
[44:02.020 --> 44:10.020]  Но вот здесь как раз видно, зачем это надо. Здесь как раз видно, зачем это надо, чтобы мат ожидания было ноль.
[44:10.020 --> 44:14.020]  Это как раз ровно для того, чтобы этот кусок пропал.
[44:14.020 --> 44:21.020]  Ну вот, значит, видите, вот важный предел мартингала.
[44:21.020 --> 44:36.020]  Значит, важность этого предела, важность этого примера еще и в том, что многие более общие мартингалы являются пределами таких вот, таких вот дискретных.
[44:36.020 --> 44:52.020]  Это на самом деле такой вроде как игрушечный пример, но он на самом деле вот такой очень типичный из-за того, что пределами таких можно все описать.
[44:52.020 --> 45:14.020]  Вот теперь давайте рассмотрим еще один пример, значит, еще один пример.
[45:14.020 --> 45:19.020]  Значит, давайте рассмотрим еще один пример.
[45:19.020 --> 45:24.020]  Значит, пример.
[45:24.020 --> 45:32.020]  Пусть кси, ну какая-то интегрируемая случайная величина.
[45:32.020 --> 45:36.020]  Пока никакого мартингала нет.
[45:36.020 --> 45:39.020]  Но есть фильтрация.
[45:39.020 --> 45:49.020]  Видите, в предыдущем примере не было фильтрации, а был сразу мартингал.
[45:49.020 --> 45:52.020]  И мы под него подверстали фильтрацию.
[45:52.020 --> 45:58.020]  Тут наоборот, пока мартингала никакого нет и процесса нет, а есть только фильтрация.
[45:58.020 --> 46:10.020]  Значит, положим кси от t просто равным условному среднему.
[46:10.020 --> 46:15.020]  Оно есть, потому что интегрируемо.
[46:15.020 --> 46:24.020]  Значит, это получается мартингал.
[46:24.020 --> 46:29.020]  Но это следует из свойств условных средних сразу.
[46:29.020 --> 46:38.020]  Потому что, когда вы это мы обсуждали, что если берешь,
[46:38.020 --> 46:47.020]  ну давайте посмотрим, почему это следует из условных средних,
[46:47.020 --> 46:52.020]  из свойств условных средних.
[46:52.020 --> 46:57.020]  Как это проверить?
[46:57.020 --> 47:16.020]  Тут у меня еще на этот счет, кстати, отдельная будет теорема без доказательства.
[47:16.020 --> 47:26.020]  Ну да, это у нас такое свойство было условных средних.
[47:26.020 --> 47:28.020]  Значит, у нас было какое свойство?
[47:28.020 --> 47:35.020]  Что если одна сигма-алгебра вложена в другую сигма-алгебру,
[47:35.020 --> 47:46.020]  то когда вы взяли условное среднее относительно меньший,
[47:46.020 --> 47:52.020]  и применили его к условному, то есть сначала спроектировали на большее,
[47:52.020 --> 48:02.020]  а потом спроектировали на меньшее, то это то же самое, что сразу проектировать на меньшее.
[48:03.020 --> 48:07.020]  Ну оно и понятно.
[48:07.020 --> 48:13.020]  Это у нас было, но на уровне проекции это понятно, когда у вас есть замкнутое подпространство,
[48:13.020 --> 48:16.020]  а в нем другое замкнутое подпространство.
[48:16.020 --> 48:22.020]  И когда вы спроектировали на большее, а потом проекцию большего спроектировали на меньшее,
[48:22.020 --> 48:24.020]  это то же самое, что вы сразу спроектировали на меньшую.
[48:24.020 --> 48:28.020]  Вот поэтому это получается мартингал.
[48:28.020 --> 48:34.340]  ну кажется что это какой-то такой ну страшно специальный случай так так на первый взгляд что
[48:34.340 --> 48:43.420]  видите как-то мартингал получился видите из одной случайной величины так но оказывается что это
[48:43.420 --> 48:50.940]  наоборот что это почти всегда так вот давайте я вам приведу без ну и без доказательства ну
[48:50.940 --> 48:58.460]  доказательства не очень трудно но оно у нас бы в сторону увело поэтому доказательства не
[48:58.460 --> 49:13.660]  будет а как факты это очень полезно знать значит теорема теорема пусть пусть мартингал
[49:13.660 --> 49:34.940]  мартингал к сиате равномерно интегрируем то есть это значит следующее что когда вы берете
[49:34.940 --> 49:50.780]  интеграл от к сиате по множеству где к сиате больше либо равно и берете супремум по т таких то
[49:50.780 --> 50:00.740]  это дело стремится к нулю когда r идет к бесконечности то есть смотрите когда когда одна случайная величина
[50:01.420 --> 50:10.460]  ну это понятное свойство ну там вытекает из там всяких ну понятно простых свойств интегралов
[50:10.460 --> 50:15.780]  ну если угодно это абсолютная непрывность интеграл ли бега так но это когда одна
[50:15.780 --> 50:22.900]  фиксирована а тут требуется видите чтобы эта штука стремилась по т равномерно к нулю вот это
[50:22.900 --> 50:30.180]  называется равномерная интегрируемость ну например например из неравенства чебышова
[50:30.180 --> 50:42.300]  и из неравенства каши следует что это верно если у них квадраты если у них равномерно
[50:42.300 --> 50:50.820]  ограничены интеграла от квадратов вот если это так то это условия равномерной интегрируемости
[50:50.820 --> 51:06.540]  выполнена так тогда тогда существует такая к си интегрируемая значит которая ну которая вот
[51:06.540 --> 51:27.020]  дает их как условные средние от одной и той же ну а ну в одну стор в одну сторону это совсем
[51:27.020 --> 51:33.020]  простое утверждение в одну сторону это из неравенства янсона легко вытекает в другую
[51:33.020 --> 51:39.500]  сторону ну то есть то есть когда такая есть то из неравенства янсона довольно легко извлечь
[51:39.500 --> 51:51.980]  что будет верно вот это так а и фильтрация какая угодно нет плейте фильтрация уже есть ведь
[51:51.980 --> 52:01.300]  у нас уже мартингал есть он же мартингал относительно фильтрации и значит так что вопрос вопрос откуда
[52:01.300 --> 52:10.220]  взять кси вот откуда эту кси взять вот вот в чем вопрос так но в каком-то смысле кси оказывается
[52:10.220 --> 52:16.940]  их ну ну неким пределом так сказать но у кси нет такого явного описания но но оно есть и поэтому
[52:16.940 --> 52:22.740]  поэтому вот этот странный пример на первый взгляд какой-то очень технический он на самом
[52:22.740 --> 52:30.500]  деле большинство реальных мартингалов охватывает бывают бывают неравномерно интегрируемый
[52:30.500 --> 52:37.940]  мартингал и такой конечно бывает но но но это надо довольно таки ну проявить некое усердие чтобы
[52:37.940 --> 52:46.420]  построить так вот с качестве упражнения можете придумать мартингал который не является равномерно
[52:46.420 --> 52:51.900]  интегрируем тогда он такого виду иметь не будет ну вот скажем с дискретным временем с натуральным
[52:51.900 --> 53:00.300]  временем может такой пример попробовать придумать так вот но значит но это ну некие некие усилия
[53:00.300 --> 53:07.100]  прилагают прилагаются а в приложениях большинство мартингалов вот вот даже
[53:07.100 --> 53:15.660]  этому довольно жесткому условию удовлетворяют но это такое вот в этой науке довольно удобное
[53:15.660 --> 53:20.900]  условие с квадратами всегда имеет дело ну и вот многие многие конкретные мартингалы этому
[53:20.900 --> 53:29.500]  условию удовлетворяют ну не все конечно но обратите внимание чтобы проверить да давайте
[53:29.500 --> 53:40.260]  я еще еще в качестве задачи значит упражнения но это уж вот совсем на янсона упражнения кси от
[53:40.260 --> 53:48.820]  т ну неважно мартингал не мартингал равномерно интегрируемо просто семейство семейство случайных
[53:48.820 --> 53:58.780]  величин равномерно интегрируемо тогда и только тогда когда существует выпуклая функция выпуклая
[53:58.780 --> 54:09.020]  функция в которая растет быстрее которая растет которая растет на бесконечности быстрее
[54:09.020 --> 54:20.620]  выпуклое не отрицательное ну или там давить ну да не отрицает выпуклое не отрицательное
[54:20.620 --> 54:31.420]  которое растет на бесконечности быстрее ты а и такая такая что просто ограничены интегралы
[54:31.420 --> 54:45.500]  вот от этих композиций так ну вот в частности вот это утверждение что квадратов ограниченных
[54:45.500 --> 54:52.260]  хватает ну это простое следствие этого упражнения если в качестве в например можно взять т квадрат
[54:52.260 --> 54:59.540]  годится вот ну тут видите получается значит смотрите смотрите как получается получается что
[54:59.540 --> 55:08.020]  мартингал имеет вид вот очень такой конкретной полученной проектированием одной и той же
[55:08.020 --> 55:17.540]  случайные величины в точности тогда когда можно подобрать выпуклую функцию ну быстро растущую на
[55:17.540 --> 55:27.060]  бесконечности так что у них окажутся ограниченными интегралы значит ну геометрически вот это
[55:27.060 --> 55:34.580]  выглядит так значит у вас ну вот в особенности когда это все в л2 происходит то геометрически
[55:34.580 --> 55:44.340]  это выглядит очень наглядно у вас есть такая вот ну так сказать ну такая вот кривая замкнутых
[55:44.340 --> 55:50.740]  подпространств расширяющихся так значит при каждом ты у вас есть некое замкнутое подпространство
[55:50.740 --> 55:59.860]  и растут со временем так и мартингал и это просто получается проектированием фиксированного
[55:59.860 --> 56:05.460]  элемента ну вот на эту спираль так сказать раскручивающиеся вот примерно такой наглядный
[56:05.460 --> 56:24.660]  смысл так теперь значит про мартингал и значит про мартингал и ну ну кстати сказать вот эта
[56:25.080 --> 56:33.340]  она не очень просто доказывается но но есть сравнительно простой случай в котором в котором
[56:33.340 --> 56:40.620]  она довольно deportа ток Fear box это вот какой случай если время дискретно если время дискретно то
[56:40.620 --> 56:47.020]  есть натурально время натурально так то есть это просто последовательная с unexpectedly так и vocês
[56:47.020 --> 56:52.260] аны вот это условия то есть видите это так сказать частный случай так вот в этом случае
[56:52.260 --> 57:05.680]  это ксиен, элементы гильбертового пространства, и кси, вот эта желаемая кси, из них почти что явно
[57:05.680 --> 57:12.540]  строится, а именно строится так. Смотрите, что у вас получается. У вас получается ограниченная
[57:12.540 --> 57:19.860]  последовательность функций в гильбертовом пространстве, но в функциональном анализе
[57:19.860 --> 57:27.460]  доказывается, что из такой последовательности можно извлечь слабосходящуюся подпоследовательность.
[57:27.460 --> 57:33.340]  То есть слабосходящуюся, это значит не по норме гильбертового пространства, а так, что только
[57:33.340 --> 57:39.540]  скалярные произведения будут с фиксированными векторами сходиться, ну как бы координаты, можно
[57:39.540 --> 57:45.260]  так сказать, координаты будут сходиться, а по норме она не обязательно будет сходиться. Так вот,
[57:45.260 --> 57:57.060]  если так сделать, то всякий такой предел слабой, сходящейся подпоследовательности, он и будет
[57:57.060 --> 58:03.740]  вот этим желаемым кси. Но чтобы это проверить, ну вот надо воспользоваться этим сведением из
[58:03.740 --> 58:08.140]  функционального анализа. Сейчас, а я вот только забыл, у вас функциональный анализ сейчас ведь
[58:08.140 --> 58:15.940]  еще есть, а не было у вас вот там такого факта, что из ограниченной последовательности в гильбертовом
[58:15.940 --> 58:26.980]  пространстве можно извлечь слабосходящуюся? Ну в ноябре, понятно, ну то есть это уже с диска стерто.
[58:26.980 --> 58:36.700]  Ну в общем, нет, ну в общем, так сказать, вот то есть этот факт какой-то, который, ну по крайней
[58:36.700 --> 58:42.380]  мере, так сказать, если даже его и не было, то наверняка будет, ну в общем, факт понятный. Ну он,
[58:42.380 --> 58:49.740]  кстати, и доказывается не очень сложно, но это уж точно не, так сказать, предмет этого нашего курса.
[58:49.740 --> 59:06.220]  Вот теперь, значит теперь, теперь спрашивается, когда мартингал сходится. Значит есть,
[59:06.220 --> 59:24.420]  когда он сходится в l1, значит теорема, теорема дуба, значит теорема дуба,
[59:24.420 --> 59:43.460]  значит пусть, значит ксин мартингал относительно, значит, возрастающей последовательности сигмалги.
[59:43.460 --> 01:00:05.500]  Этот мартингал сходится в l1, ну то есть в среднем, тогда и только тогда, когда есть такая функция
[01:00:05.900 --> 01:00:27.260]  из l1, что она имеет, ну что она его порождает. Значит, это равносильно, это равносильно тому,
[01:00:27.340 --> 01:00:40.700]  что, ну вот эта последовательность равномерно интегрируемая. То есть видите, да, да, при этом,
[01:00:40.700 --> 01:00:55.580]  при этом еще кси-энная атомига сходится кси-атомига почти всюду. У нас тут пока не было разговоров про
[01:00:55.580 --> 01:01:09.140]  сходимости почти всюду, так, но, но, но, то есть смотрите, получается вот какая картина,
[01:01:09.140 --> 01:01:17.900]  ну правда, в этой картине время должно стать дискретным, ну чтобы говорить про сходящиеся
[01:01:17.900 --> 01:01:24.860]  последности, так, вот, вот в этом специальном виде, но он, впрочем, для приложения, конечно,
[01:01:24.860 --> 01:01:30.620]  один из самых распространенных, когда мартингал это последовательность, то смотрите, что оказывается,
[01:01:30.620 --> 01:01:37.100]  что мартингал сходится в среднем в точности, когда он имеет вот этот очень специфический вид,
[01:01:37.100 --> 01:01:48.340]  так, и при этом есть сходимость еще и почти всюду, но, но, но тут эти сходимости они неравносильны,
[01:01:48.340 --> 01:01:53.940]  ну вот, если вы там что-то припоминаете из курса интегрирования, то там есть два вида
[01:01:54.020 --> 01:01:59.840]  таких основных сходимости почти всюду и в среднем, они между собой не очень связаны, так,
[01:01:59.840 --> 01:02:05.900]  если что-то сходится в среднем, то не обязательно сходиться почти всюду, ну, там есть, правда,
[01:02:05.900 --> 01:02:10.500]  какие-то связующие теоремы, типа, если сходится в среднем, то можно выбрать
[01:02:10.500 --> 01:02:15.180]  подпоследовательность, которая сходится почти всюду, так, а если сходится почти всюду,
[01:02:15.180 --> 01:02:20.420]  не обязательно сходиться в среднем, но если еще добавить равномерную интегрируемость,
[01:02:20.420 --> 01:02:28.420]  то будет сходимость в среднем. Но вообще говоря, из сходимости в среднем для всех последовательностей
[01:02:28.420 --> 01:02:36.020]  ничего сказать нельзя. Но тут тот случай, когда из сходимости в среднем вытекает сходимость почти
[01:02:36.020 --> 01:02:42.780]  всюду. Вот если бы был общий случай, ну кто-то сходится в среднем, хорошо, тогда есть подпоследовательность,
[01:02:42.780 --> 01:02:49.860]  сходящаяся почти всюду, но не вся сама. А тут не надо никаких подпоследователей брать, сразу вся
[01:02:49.860 --> 01:02:56.820]  исходная последовательность сходится почти всюду. Это довольно полезное утверждение. Вот это,
[01:02:56.820 --> 01:03:03.820]  можно сказать, основная теорема о сходимости марктингалов. Она доказывается не то чтобы длинно,
[01:03:03.820 --> 01:03:18.420]  но это явно, так сказать, не умещается вот в этот, так сказать. А если взять время непрывной,
[01:03:18.420 --> 01:03:26.020]  говорите про направленности, то с L1 всё будет так же, а вот почти всюду уже не так будет. Поэтому
[01:03:26.020 --> 01:03:35.420]  не совсем такая будет теорема. Но вот та часть, которая с сходимостью в среднем, она останется.
[01:03:35.420 --> 01:03:55.740]  Так, теперь, ну вот, что-то я ещё тут хотел сказать про марктингалы. Вот важная вещь.
[01:04:05.420 --> 01:04:20.140]  Вот ещё важная вещь. Вот ещё важная вещь про марктингалы. Эта теорема без доказательства,
[01:04:20.140 --> 01:04:30.100]  её просто полезно знать как факт. Сейчас про марктингалы мы сегодня закончим, и будет одна
[01:04:30.100 --> 01:04:46.180]  теорема с простой формулировкой, с простым доказательством. Значит, связь с процессами
[01:04:46.180 --> 01:04:59.220]  с независимыми превращениями. Значит, как только у нас появляются какие-то новые классы процессов,
[01:04:59.300 --> 01:05:05.700]  их у нас немного, то всегда бывает поучительно как-то между собой их скрестить. Вот у нас будут
[01:05:05.700 --> 01:05:15.820]  независимыми превращениями гауссовские, марктингалы и марковские. И вот можно много устроить из этого,
[01:05:15.820 --> 01:05:22.500]  мне кажется, почти для всего курса, для присутствующих точно можно устроить такие задачи.
[01:05:22.500 --> 01:05:30.740]  Поскрещивать какие-то комбинации из этих четырёх, что получится, когда вы поскрещиваете что-то.
[01:05:30.740 --> 01:05:43.540]  Но некоторые скрещивания являются поглощающими, там ничего, так сказать, не происходит нового,
[01:05:43.540 --> 01:05:48.540]  а в некоторых случаях при скрещиваниях что-то такое специфическое появляется.
[01:05:48.540 --> 01:06:09.420]  Давайте посмотрим такой пример. Пусть CRT марктингал. Я не пишу вот эти f, потому что
[01:06:09.420 --> 01:06:16.860]  определение само за собой тянет, он марктингал не сам по себе, а относительно какой-то фильтрации.
[01:06:16.860 --> 01:06:24.740]  Например, хотя бы им самим порождённый, но надо помнить, что марктингальность это всегда что-то
[01:06:24.740 --> 01:06:33.180]  апеллирующее к сигмалгипам каким-то, но для сокращения я не буду полностью всё это выписывать.
[01:06:33.180 --> 01:06:44.540]  Марктингал и XAT, пусть они в L2. Это, естественно, дополнительное ограничение, потому что в
[01:06:44.540 --> 01:06:56.220]  определении марктингала не требуется, чтобы они были в L2. Давайте посмотрим, что будет с приращениями.
[01:06:56.220 --> 01:07:20.220]  Давайте вычислим. Я даже хочу буквы использовать такие, как у меня в конспекте.
[01:07:20.220 --> 01:07:41.100]  Давайте сосчитаем к авариацию, вычислим.
[01:07:50.220 --> 01:08:15.820]  К авариацию разности. Видите, мы вычисляем не к авариацию самого процесса, а вычисляем к авариацию приращений.
[01:08:15.820 --> 01:08:26.380]  Почему? Потому что когда мы говорим о процессах с независимыми приращениями, то приращения должны быть независимы.
[01:08:26.380 --> 01:08:37.060]  Но если они независимы, то они должны быть и некоррелированы. А вот сейчас мы намереваемся проверить
[01:08:37.060 --> 01:08:46.180]  вот это более слабое свойство, некоррелированность приращений. Это не то же самое, что независимость, это немножко послабее.
[01:08:46.180 --> 01:09:08.180]  Вот это хороший вопрос. Давайте, раз уж я забыл, то чуть позже это сделаем, что у марктингала среднее постоянно.
[01:09:08.180 --> 01:09:26.180]  Вот это я забыл отметить, но это мы сейчас сделаем. Давайте сосчитаем. Смотрите, что тут получится.
[01:09:26.180 --> 01:09:34.180]  Ну как полагается? Полагается честно перемножать их.
[01:09:56.180 --> 01:10:22.180]  Вот так получается.
[01:10:22.180 --> 01:10:32.180]  Ну сейчас, правда, может быть, я зря все расписал. Может быть, было даже покороче. Сейчас, ну давайте, раз уж расписал, так расписал.
[01:10:32.180 --> 01:10:52.180]  Значит, вот что замечаем. Замечаем, что кси Т минус кси С
[01:10:52.180 --> 01:11:08.180]  артагонально под пространство АФС измеримых. Ну когда С меньше либо равно Т.
[01:11:08.180 --> 01:11:17.180]  Ну почему? Ну потому что у них одинаковые проекции по определению марктингала.
[01:11:17.180 --> 01:11:27.180]  У них одинаковые проекции, поэтому разность артагональна этому подпространству.
[01:11:27.180 --> 01:11:47.180]  Ну что это дает тогда? Давайте посмотрим тогда, что это дает. Разность артагональна, ну и что?
[01:11:47.180 --> 01:11:57.180]  Ну, да, значит, скалярное произведение нулевое. Это значит кси Т минус кси С на кси С будет ноль.
[01:11:57.180 --> 01:12:17.180]  Но из этого следует, что кси Т, кси С есть кси С в квадрате.
[01:12:17.180 --> 01:12:30.180]  То есть смотрите, что происходит. Когда мы берем ковариацию самого процесса, не приращение самого процесса, то она равна вот этому.
[01:12:30.180 --> 01:12:41.180]  Но теперь мы, теперь мы смотрим вот на это и видим, значит, смотрите, что здесь получается.
[01:12:41.180 --> 01:12:51.180]  Значит, здесь, здесь получается квадрат вот этого, так? Вычитается квадрат вот этого.
[01:12:51.180 --> 01:13:02.180]  Ну там все с подожиданиями, да? Здесь меньше Т2, наоборот, вычитается, то есть смотрите, вот этот и вот этот, они сокращаются, так?
[01:13:02.180 --> 01:13:11.180]  А и вот этот и вот этот, они тоже сокращаются, потому что этот будет квадрат этого, ну он с плюсом, а тут с минусом.
[01:13:11.180 --> 01:13:22.180]  Значит в итоге получается ноль, так? Значит в итоге получается ноль. То есть видите, оказалось, что приращение, значит приращение,
[01:13:22.180 --> 01:13:38.180]  артагонально, так? Значит приращение артагонально. Итог, приращение артагонально.
[01:13:38.180 --> 01:13:57.180]  Дальше, значит, в общем случае, вот это я забыл сразу сказать, это надо было отметить.
[01:13:57.180 --> 01:14:16.180]  Значит в общем случае, средняя константа, так? Значит откуда это следует?
[01:14:27.180 --> 01:14:53.180]  Ну да, ну это можно, это можно написать так. Значит, кси вот это, вот это, это есть, вот это есть среднее, это есть среднее.
[01:14:53.180 --> 01:15:17.180]  Вот условного среднего, так? Ну и теперь, ну и теперь если сравнить, то поскольку среднее, среднее условного среднего всегда равно
[01:15:17.180 --> 01:15:36.180]  абсолютно среднему, то получается, что средние равны, а из этого следует, что приращение, приращение еще не коррелированный.
[01:15:36.180 --> 01:15:47.180]  Поэтому, значит, смотрите, что получается. Получается, ну это правда не какой угодный мартингал, а это мартингал из l2, так?
[01:15:47.180 --> 01:15:59.180]  Ну и последняя теорема, значит, в другую сторону. Ну это так сказать, ну это не совсем обратно, а значит теорема, значит, ну вот заключительная теорема этого раздела,
[01:15:59.180 --> 01:16:10.180]  единственная теорема, которую мы докажем. Ну сейчас, тут доказательств две строчки, но посмотрим, сейчас я успею их написать за минуту.
[01:16:10.180 --> 01:16:35.180]  Значит, теорема такая, если, если ксиат-т процесс с независимыми приращениями, так, а и среднее постоянно,
[01:16:35.180 --> 01:16:55.180]  то, то ксиат-т мартингал относительно порожденной инфильтрации.
[01:16:55.180 --> 01:17:13.180]  Так, сейчас вот, но вот я не понимаю, сейчас, но кажется, уже нет у нас времени, потому что мы еще перрыв не делали.
[01:17:13.180 --> 01:17:25.180]  Сейчас, пять минут еще есть. А, ну за пять минут, за пять минут мы успеем добежать до границы, как говорил тот персонаж.
[01:17:25.180 --> 01:17:37.180]  Значит, доказательства. Значит, с меньше t, значит, смотрите, что получаем.
[01:17:37.180 --> 01:17:56.180]  Значит, условное, среднее можно записать так. Значит, это есть вот это плюс вот это.
[01:17:56.180 --> 01:18:16.180]  Вот так. Значит, при этом это будет, при этом это будет вот это, ну и равное ксиас.
[01:18:16.180 --> 01:18:32.180]  Значит, так как условное, среднее разности равно нулю.
[01:18:32.180 --> 01:18:54.180]  Значит, почему? Ну а оно равно, оно равно просто их обычному среднему разности.
[01:18:54.180 --> 01:19:09.180]  Значит, давайте вот это проверим. Значит, давайте вот это проверим. Почему так?
[01:19:09.180 --> 01:19:27.180]  Ну, потому что вот эта разность, она независима вот этой сигмалгеброй.
[01:19:27.180 --> 01:19:33.180]  Ну, независима, это значит, она независима с каждым событием из этой сигмалгебры.
[01:19:33.180 --> 01:19:40.180]  И поэтому из определения следует, что условное, среднее нулевое.
[01:19:40.180 --> 01:19:49.180]  А почему она, почему эта разность независима с сигмалгеброй?
[01:19:49.180 --> 01:19:56.180]  Ну, потому что она независима из-за приращений.
[01:19:56.180 --> 01:20:13.180]  Так, потому что кси, т-ксис независима со всеми кситау, кситау при тау меньше либо равном с.
[01:20:13.180 --> 01:20:20.180]  Ну, это, это из-за независимости приращений.
[01:20:20.180 --> 01:20:30.180]  Из-за независимости приращений.
[01:20:30.180 --> 01:20:41.180]  А когда случайная величина независима, ну, с каждой случайной величиной из некого набора,
[01:20:41.180 --> 01:20:48.180]  то она независима со всеми из сигмалгебрами порожденных.
[01:20:48.180 --> 01:20:52.180]  Ну, давайте вот это я уже в следующий раз поясню, чтобы это не комхать,
[01:20:52.180 --> 01:20:55.180]  потому что это действительно в некотором пояснении нуждается. Почему так?
[01:20:55.180 --> 01:21:00.180]  Ну, это интуитивно понятно, но, ну, в общем, строго говоря, это надо еще пояснить.
[01:21:00.180 --> 01:21:05.180]  Вот этот момент, ну, он уже к маркингалам не имеет отношения.
[01:21:05.180 --> 01:21:12.180]  Ну, это на самом деле нечто, что можно было бы в качестве заготовки сделать в условных средних.
[01:21:12.180 --> 01:21:18.180]  А, значит, в конспекте, я в конспекте, между прочим, по-моему, из-за этого задачу сделал.
[01:21:18.180 --> 01:21:27.180]  Да, в конспекте это у меня задача, но давайте я в следующий раз это поясню,
[01:21:27.180 --> 01:21:30.180]  потому что все-таки это, так сказать, теориям с доказательством,
[01:21:30.180 --> 01:21:35.180]  наверное, не очень хорошо, так сказать, куски доказательств загонять в задачи.
[01:21:35.180 --> 01:21:41.180]  Хотя в этом что-то тоже есть. Все, давайте на этом закончим.

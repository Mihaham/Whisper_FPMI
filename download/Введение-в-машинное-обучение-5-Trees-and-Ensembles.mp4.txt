[00:00.000 --> 00:11.100]  Мы начинаем. Итак, ребята, сегодня мы с вами продолжаем беседу про различные методы машинного
[00:11.100 --> 00:17.000]  обучения. Сегодня нас ждет ужас, страшное и великое дерево. Вот кто-то из вас, скорее всего,
[00:17.000 --> 00:20.800]  на алгоритмах все эти деревья уже строил, там можно было строить сбалансированные деревья,
[00:20.800 --> 00:25.080]  вообще структура данных дерева много где используется. В машинке бинарное дерево тоже всеми
[00:25.080 --> 00:29.880]  любимо. И это один из немногих алгоритмов, которые можно использовать, при этом не имея никакого
[00:29.880 --> 00:34.920]  градиентного метода оптимизации под капотом, скажем так. По сути, у нас здесь все будет нажат на
[00:34.920 --> 00:40.080]  оптимизации сидеть и, тем не менее, это работает. За это стоит на них обратить внимание. Итак,
[00:40.080 --> 00:45.720]  сегодня в меню. Ты чего не работаешь? Сегодня в меню. Во-первых, мы поговорим про решающие деревья
[00:45.720 --> 00:51.320]  концептуально. Что это такое? С чем их едят? Почему в век нейронок они все еще актуальны? Во-вторых,
[00:51.320 --> 00:55.520]  мы с вами разберемся непосредственно, как строится дерево и, более того, построим с вами его прям по
[00:55.520 --> 01:01.000]  шагам. Раз, два, три, четыре, пять. Более того, у вас будет опциональная домашка. Опциональная в том
[01:01.000 --> 01:06.160]  плане, что чтобы получить отличный по курсе, не обязательно делать, но она вызывает часто страдания
[01:06.160 --> 01:10.120]  у тех, кто не подготовлен писать много кода на питоне. Поэтому, если вы не решаете, мы вас за это
[01:10.120 --> 01:15.240]  не журием, скажем так. У вас будет краска четвертая, опциональная домашка по реализации дерева на
[01:15.240 --> 01:19.960]  питончики. Крайне рекомендую ее решить, но скажем так, если совсем не смогли, ну не смогли.
[01:19.960 --> 01:30.400]  Она дает баллы, то есть, грубо говоря, она есть в числителе, но нет знаменателей, ваша оценка.
[01:30.400 --> 01:35.360]  Грубо говоря, вы можете, решив ее, получить там 105 процентов баллов за курс, ну типа то, что
[01:35.360 --> 01:39.880]  национально. То есть, если вы ее не сделаете, то не столь страшно. Крайне рекомендую сделать,
[01:39.880 --> 01:46.080]  но опять же up to you. Но, тем не менее, она является хорошим таким, не пререквизитом, а подготовкой к
[01:46.080 --> 01:52.600]  пятой домашке, которая обязательная. Там вам надо будет написать дерево с нуля, но вам, типа, дадут
[01:52.600 --> 01:57.760]  игнатуру класса какую-то минимальную, типа у него есть какие-то методы, типа feed, тратата. Вам надо
[01:57.760 --> 02:03.320]  все написать. Короче, как кнн, только у вас простор больше, а злобные тесты на время более злобные.
[02:03.320 --> 02:23.360]  Ребят, вы на фистехе первый год учитесь? Прикольно, а кто на фистехе первый год? Ну вот, не так много.
[02:23.360 --> 02:28.800]  Ребят, ну тогда привыкайте, это фистеха. Здесь основная суть, чтобы вы знали, а не чтобы вы сдали.
[02:28.800 --> 02:36.320]  Работает в обе стороны, можно узнать и не сдать. Такое тоже бывает. Я один раз пришел на экзамен по...
[02:36.320 --> 02:45.840]  Что это было? По статфизу, короче, кажется, там была прям боль. Нас на статфизе гоняли в хвосты,
[02:45.840 --> 02:50.000]  в гриву. Это был последний семестр бакловриата. Мы статфизчев, на-говори, работали, но не очень
[02:50.000 --> 02:58.200]  хорошо. Мы диплом писали. Я с Фуфма. Ладно, давайте перейдем к машинам обучения, а то у нас так сегодня
[02:58.200 --> 03:03.800]  семинар сгорит. Короче, что такое дерево, как работает, как построить. Как построить, я вас прошу
[03:03.800 --> 03:08.360]  сегодня очень внимательно отнесись к этому и понять. Ну а потом поговорим про то, почему деревья
[03:08.360 --> 03:15.240]  хороши, как их можно объединить между собой, чтобы порешать их проблемы. И внезапно посмотрим на
[03:15.240 --> 03:20.280]  деревья сквозь призму линейных моделей. Потому что на самом деле линейные модели и деревья это
[03:20.280 --> 03:24.760]  суть одно и то же в разных, очень разных признаковых пространствах. Именно поэтому я не раз говорил,
[03:24.760 --> 03:30.760]  что линейные модели это вообще, наверное, корень почти всего. Итак, поехали. Так, номер раз. Датсет
[03:30.760 --> 03:34.800]  вам уже знаком. Это те же самые Ирис и Фишер. Мы их с вами видели на первом занятии, возможно,
[03:34.800 --> 03:39.560]  на втором и так далее. Три класса, четыре признака. Ну тут мы с вами только два нарисовали. И,
[03:39.560 --> 03:43.800]  соответственно, необходимость сепульки и петульки используя отделить три класса друг от друга.
[03:43.800 --> 03:49.800]  То можем сказать про то, что мы видим на экране. Смотрите, давайте каждый раз, вот просто будем
[03:49.800 --> 03:55.240]  действовать по шагам. Будем брать какой-то признак. В данном случае вот два признака взяли, не помню
[03:55.240 --> 04:01.080]  какие. Это PL и PV. Petal Length, Petal Width. И по ним проводить просто какой-то порог. То есть по одному
[04:01.080 --> 04:07.000]  признаку. Условно признак там больше или равен 2.5, иди направо, иначе иди налево. То есть каждый раз
[04:07.000 --> 04:12.680]  мы будем, по сути, от нашего какого-то пространства делить его пополам. Ну или уже от нашего
[04:12.680 --> 04:19.000]  полупространства делить его опять же пополам. Окей? Ладно, подпространством это некорректно.
[04:19.000 --> 04:23.200]  Называть это неподпространство это, не знаю, полупространство. Короче, одна часть пространства и
[04:23.200 --> 04:29.240]  другая. Все. Вот отделили все красное допустим снизу, сверху у нас осталось. Второй шаг. Говорим,
[04:29.240 --> 04:34.800]  все что справа от вот этой прямой зеленое, слева непонятно. Теперь делим опять. Все что ниже этой
[04:34.800 --> 04:38.680]  прямой желтое, но тут есть второе условие, которое говорит, что все что ниже этой красное, оно
[04:38.680 --> 04:44.240]  доминирует то, что оно было раньше, ближе к корню дерева. Все поделили. Получается, что мы с одной
[04:44.240 --> 04:47.960]  стороны построили какое-то абсолютно деревянное решение, причем реально деревянное дерево,
[04:47.960 --> 04:53.520]  которое у нас просто умеет тыфами делить какое-то наше пространство на подобности, а с другой стороны
[04:53.520 --> 04:59.120]  посмотрите на разделяющую поверхность. Она нелинейная. Мы никаких с вами ядер не придумали,
[04:59.120 --> 05:04.160]  ничего мы с вами делали, но получили нелинейную разделяющую поверхность. Возникает, собственно,
[05:04.160 --> 05:14.560]  вопрос. А. Как это дело строить? И Б. А насколько это хорошо? Пл меньше 2,5 и Пл.
[05:18.040 --> 05:31.040]  Пл больше 2,5 и меньше или равен 5? От 2,5 до 5. Вот у вас закрытый открытый. Пл больше 5 направо,
[05:31.040 --> 05:41.120]  Пл налево полз, направо тру. Пл меньше 5, вот. Это желтые, да, все правильно. Короче, вопрос,
[05:41.120 --> 05:47.800]  как это дело строить? То, что выглядит неплохо. Согласны? Ну вот сейчас мы как раз это разберем. И
[05:47.800 --> 05:52.840]  собственно, в чем второй плюс деревья? Первое. Давайте сразу оговоримся о некоторые вводные.
[05:52.840 --> 05:58.960]  Во-первых, в каждом листе дерева мы предсказываем с вами здесь какую-то метку класса, да, ну или мы
[05:58.960 --> 06:03.920]  там можем предсказывать число абсолютно так же. Вот вам задача регрессии. Посмотрите, это дерево
[06:03.920 --> 06:08.720]  точно так же. У нас в зависимости от аргумента меняется предсказание. Просто если такой-то такая
[06:08.720 --> 06:13.100]  константа, если больше такая константа и так далее. То есть, по сути, это кусочно-постоянная
[06:13.100 --> 06:18.880]  функция. Согласны? Что можно сразу сказать про дерево? В классических деревьях, опять говорю
[06:18.880 --> 06:22.220]  классические, потому что есть неклассические деревья или там неоклассические как угодно
[06:22.220 --> 06:27.320]  назовите, в классических деревьях в листе сидит константа. То есть, каждый лист на ответственно
[06:27.320 --> 06:33.120]  какой-то константе. Будь то метка класса, число и так далее. Есть всякие там более, скажем так,
[06:33.120 --> 06:36.480]  новомодные деревья, которые в каких-то статях встречаются, но на практике редко используются,
[06:36.480 --> 06:43.040]  там в лист запихивают прям линейную модель и так далее, но это все не очень удобно. Хорошо? Вот я
[06:43.040 --> 06:46.440]  сразу хочу с вами оговориться об одной вещи, чтобы у вас оно все равно в голове откладывал.
[06:46.440 --> 06:52.480]  Давайте договоримся, что вот отныне и до упора, я это уже на третьем занятии говорил, у нас в листик
[06:52.480 --> 06:56.920]  дерева и вообще в задачи пластикации мы не метку класса предсказываем, мы предсказываем
[06:56.920 --> 07:02.680]  вероятность на распределение над метками классов. Хорошо? То, что так будет гораздо проще и думать,
[07:02.680 --> 07:06.120]  мыслите и вообще работать со всеми моделями. То, что как средние две метки, непонятно,
[07:06.120 --> 07:10.960]  у вас два распределения, как средние два вектора, ну положили, поделили пополам. Все понятно. Так что
[07:10.960 --> 07:15.240]  здесь мы везде будем предсказывать вероятность класса. Как нам ее взять, тоже с вами разберемся.
[07:15.240 --> 07:20.400]  Ну и, собственно, вопрос, как вот эту красоту построить? Тут я сразу хотел бы еще один вопрос
[07:20.400 --> 07:25.520]  задать вам, вот особенно на этой картинке понятно, как вы думаете, способны ли деревья
[07:25.520 --> 07:30.480]  работать вне выпуклой оболочки, скажем так, точек из обучающей выборки?
[07:30.480 --> 07:49.000]  Да, все во все стороны мы с вами покрываем, но вот в регрессии чуть лучше видно. Мы можем
[07:49.000 --> 07:54.640]  предсказывать, смотрите, здесь у нас будет правило условно х меньше, чем там 0,1, тогда предсказывай
[07:54.640 --> 08:00.360]  константу. Но о чем стоит помнить? У вас дерево в каждой конкретной точке, ну условно на каждой
[08:00.360 --> 08:05.840]  подобности, предсказывает константу, в том числе вот от крайней точки и до упора влево, от крайней
[08:05.840 --> 08:10.400]  точки до упора вправо, если это одномерный случай, многомерно может сами обобщить. Поэтому к
[08:10.400 --> 08:14.520]  экстраполяции дерево не очень предвосположено, оно просто предсказывает константу вне вот этой
[08:14.520 --> 08:18.640]  самой выпуклой оболочки, обучающей выборке. Почему об этом важно помнить, это от ваших признаков
[08:18.640 --> 08:23.920]  зависит, но иногда у вас там может быть, что вы условно находитесь где-то на границе, и за границей
[08:23.920 --> 08:28.000]  вас всегда будет константа предсказывать. Условно линейная модель может хотя бы линейный тренд вытащить
[08:28.000 --> 08:32.480]  куда-то наружу, за область определения. Может он не факт, что будет правильно, но тем не менее. Дерево
[08:32.480 --> 08:36.960]  неспособно, оно вот вне области определения, оно ничего не знает. Оно просто берет свое крайнее
[08:36.960 --> 08:42.520]  значение из области определения и просто его до бесконечности распространяет. Это такая маленькая
[08:42.520 --> 08:50.640]  вещь, о ней потом почему-то любят люди забывать. Это понятно? Хорошо. Кто такое выпуклая оболочка?
[08:50.640 --> 09:03.200]  Ай-ай-ай, на контрольный вас об этом спросит. Простите, сейчас я отвечу. Слушаю вас. Так,
[09:03.200 --> 09:13.880]  давайте очень коротко, я на лекции. Бу-бу-бу-бу-бу, спасибо. Смотрите, у вас есть их телеграммы?
[09:13.880 --> 09:23.560]  Можете мне в телегу написать, я вам все пришлю. Я вам скину все ссылки, они уже подключатся.
[09:23.560 --> 09:32.360]  Хорошо, я вам где-то 18.30 все пришлю, потому что сейчас я на лекции. Договорились, спасибо.
[09:34.360 --> 09:41.080]  Простите, орг-моменты. Поехали. О, все, люди убежали. Деревья страшные. Короче, как дел строить?
[09:41.080 --> 09:46.080]  Смотрите, строится дерево максимально прямолинейно и тут можно вспомнить алгоритмы.
[09:46.080 --> 09:52.360]  Короче, кто помнит что такое жадный алгоритм? Ну, замечательно. Вот если вы понимаете,
[09:52.360 --> 09:57.800]  как работает жадный алгоритм, то вы понимаете, как строится дерево. Смотрите, первое. Давайте
[09:57.800 --> 10:03.080]  предположим, что у нас с вами есть один признак, нам по нему вот надо поделиться. У нас один признак,
[10:03.080 --> 10:08.160]  нам надо выбрать наилучшую, скажем так, границу, по которой разделить наши данные. Предсказываю их
[10:08.160 --> 10:14.920]  константами. Чем можем сделать? Ну, мы можем какую-нибудь середину выбрать или мы можем,
[10:14.920 --> 10:21.280]  на самом деле, перебрать возможные границы, ну, медиану или выбрать какую-то границу, на самом
[10:21.280 --> 10:25.360]  деле, и сказать, вот здесь у нас будет разделение. Тогда для левой и правой части мы предсказываем
[10:25.360 --> 10:31.760]  свои константы, правильно? Ну да, ну, собственно, у нас дерево всегда предсказывает константу в
[10:31.760 --> 10:35.840]  каждом листе. Пока у нас дерево с вами, вот изначально это просто будет одно дерево из одной
[10:35.840 --> 10:39.640]  вершины. Значит, допустим, предсказываем среднее. Почему среднее оптимальное,
[10:39.640 --> 10:44.560]  тоже потом скажу. Предсказываем среднее с вами. Хорошо, теперь нам нужно где-то поделить это на
[10:44.560 --> 10:50.360]  два листа, то есть две константы начать предсказывать, так, чтобы у нас дерево стало лучше. Тут, собственно,
[10:50.360 --> 10:55.480]  возникает вопрос, что такое лучше? Пока мы это не облекли в какие-то формальные критерии, типа,
[10:55.480 --> 10:59.480]  средней кватратической ошибки, например, или средней абсолютной меньше. Что такое лучше, непонятно.
[10:59.480 --> 11:06.320]  Окей? Мы должны найти точку, в которой мы проведем какие-то константы, выберем новые константы,
[11:06.320 --> 11:11.320]  и станет лучше. В этом дерево и заключается. Я возвращаюсь обратно на этот слайд. Смотрите,
[11:11.320 --> 11:16.120]  собственно, что мы делаем. Мы находимся, допустим, на всю выборку смотрим. Я сразу,
[11:16.120 --> 11:23.880]  на всякий случай. Так, что такое мат-индукция, все помнят? Что происходит с физтехом? База
[11:23.880 --> 11:30.360]  индукции, шаг индукции. Если выражение верно на и там шаге, тогда вот вам переход на и плюс
[11:30.360 --> 11:39.720]  первый. Если вы ледницу, например, докажете, значит вы доказали для любого и. Ура! Ну,
[11:39.720 --> 11:44.840]  потому что по умолчанию строят бинарное дерево, потому что у вас у операции сравнение больше или
[11:44.840 --> 11:50.840]  меньше два выхода, да или нет. Вы можете, на самом деле, деревья с множеством выходов строить,
[11:50.840 --> 11:56.120]  и такое даже умеет, например, катбуст. Но вообще говоря, можно с помощью той же самой бинаризации
[11:56.120 --> 12:00.800]  привезти к бинарному дереву все равно. Категориальный признак, например, разбить на кучу бинарных
[12:00.800 --> 12:06.200]  признаков, и дерево тогда тоже построится. Просто раньше было гораздо удобнее строить бинарное
[12:06.200 --> 12:11.200]  дерево чисто из вычислительных соображений, его удобнее хранить и так далее. Так вот, давайте
[12:11.200 --> 12:16.480]  скажем так. Пока что мы с вами выбрали какой-то признак, и по нему пробегаемся по всем возможным
[12:16.480 --> 12:22.240]  трешхолдам, вот этим вот разделением, точкам, где мы разделяем. Причем для каждого трешхолда мы
[12:22.240 --> 12:27.680]  собственно бьем нашу выборку на левую подвыборку и правую подвыборку. Они у нас не пересекаются,
[12:27.680 --> 12:35.920]  это логично? Логично. После чего мы с вами повторяем операцию опять, опять, опять. Короче,
[12:35.920 --> 12:42.360]  рекурсия. Вот. Возникает, собственно, вопрос. Да, кто не увидел морфюса, вот вам кусочек морфюса
[12:42.360 --> 12:48.960]  с рекурсией. Возникает маленький вопрос, собственно. А почему, точнее, а по какому критерию нам разделять?
[12:48.960 --> 12:54.960]  Вот давайте введем некоторый функционал просто, который будет измерять, насколько подвыборка
[12:54.960 --> 12:59.680]  стала упорядочней, лучше. Потому что наша конечная цель какая? Мы же в каждом листе константа
[12:59.680 --> 13:04.800]  предсказываем, правильно? То есть эта константа должна быть максимально похожа на все объекты,
[13:04.800 --> 13:09.160]  обучающие выборке, которые туда попали. Ну, собственно, вот эта самоднародность мы с вами и будем
[13:09.320 --> 13:27.840]  пытаться посчитать. До свидания. До свидания. Восстание машины не начало. Итак, вот давайте введем
[13:27.840 --> 13:31.400]  некоторый функционал. Пока что не будем говорить, что это. Просто нам нужен какой-то функционал,
[13:31.400 --> 13:35.520]  который умеет оценивать упорядоченность левой подвыборки и правой подвыборки,
[13:35.520 --> 13:39.800]  правильно? Исходно. Короче, для любой выборки из элементов он умеет оценивать, грубо говоря,
[13:39.800 --> 13:45.840]  степень хорошести. И, соответственно, ну ладно, здесь будет степень нехорошести раз на минимум,
[13:45.840 --> 13:49.600]  степень нехорошести. Поставим минус. И, соответственно, при этом нам нужно учесть,
[13:49.600 --> 13:54.320]  насколько много объектов попало влево, насколько много объектов попало вправо. Ну почему? Простой
[13:54.320 --> 13:58.160]  пример. Вот у нас есть с вами выборка, мы с вами отделили один объект, знаете, вот такой очень
[13:58.160 --> 14:03.680]  тоненький кусочек колбасы. Вот один объект из класса зеленые. Все, теперь у нас есть сравнение,
[14:03.680 --> 14:08.400]  крайний объект взяли и говорим, вот этот зеленый, про все остальные, ну не знаю рандомно. С одной
[14:08.400 --> 14:13.480]  стороны, зеленый мы классифицируем теперь 100% точно. С другой стороны, на фоне 10 миллионов
[14:13.480 --> 14:20.120]  объектов это не то, чтобы полезно вообще. Ну типа нас осталось 999 тысяч, 900, бла-бла-бла,
[14:20.120 --> 14:31.080]  объектов, которые и плохо классифицируются. Да? Зачем мы на Q делим? На Q мы делим, ну на самом
[14:31.080 --> 14:35.360]  деле да, вы правы, на самом деле что такое Q? Вы правы, на Q можно особо не делить. Зачем мы делим
[14:35.360 --> 14:39.800]  на Q? Q это мощность исходного множества, чтобы у нас всегда просто была доля, чтобы было просто
[14:39.800 --> 14:45.000]  понятно. Иначе у вас начнет постепенно условно, когда у вас, если на Q не делить, а у вас выборка
[14:45.000 --> 14:50.560]  там размером, я не знаю, миллиард, то у вас будет большое число. Вот тогда у вас получается
[14:50.560 --> 14:55.080]  мощность здесь миллиард, здесь два, типа они слишком в разных порядках. Так вы избавляетесь и у вас
[14:55.080 --> 15:00.960]  всегда число от 0 до 1. То есть просто с точки зрения нормировки. Пока понятно, что происходит? Да,
[15:00.960 --> 15:09.440]  что такое? Аж мы сейчас с вами про это целые 10 минут будем говорить. Ну смотрите, раз на минимум,
[15:09.440 --> 15:13.600]  то есть смотрите, мы хотим сделать так, чтобы вот эта сумма стала меньше. То есть тогда это
[15:13.600 --> 15:18.640]  мера неупорядоченности. Я, простите, оговорился. Сейчас я дверь закрою.
[15:18.640 --> 15:39.320]  Погромче повелось, говорите. Сейчас тоже скажу. Не переживайте, сейчас разберем. Вот если один
[15:39.320 --> 15:44.800]  признак у нас есть, мы пробегаемся по всем возможным разбеяниям. То есть, допустим, длина стебля
[15:44.800 --> 15:51.200]  2,5, длина стебля 5, длина стебля 7,5 и так далее. Что значит по всем возможным? Смотрите, по идее,
[15:51.200 --> 15:56.800]  у нас с вами признак числовой непрерывный. Вот, он же у нас континуальный, непрерывный. Мы можем
[15:56.800 --> 16:02.160]  любое множество значений выбрать, континуум целый. Но у нас выборка-то обучающая конечная,
[16:02.160 --> 16:07.080]  правильно? Поэтому нам не имеет смысла выбирать все возможные значения. Нам максимум имеет
[16:07.080 --> 16:12.040]  смысла выбирать значения, которые лежат между двумя объектами. Потому что при сдвиге вот в рамках
[16:12.040 --> 16:16.920]  вот нас а, один объект, второй объект, если мы между ними подвигаем, у нас наш критерий
[16:16.920 --> 16:21.720]  информативности не поменяет то, что он для подвыборки работает. У нас подвыборки такие же останутся.
[16:21.720 --> 16:27.520]  Поэтому по всем вот возможным, в худшем случае, мы прибираемся. То есть, сколько у вас? 10 объектов,
[16:27.520 --> 16:31.760]  соответственно, между ними у нас сколько? 9 возможных трешфлодов. На самом деле 11,
[16:31.760 --> 16:37.840]  2 крайних тоже работает. Хотя нет, зачем 2 крайних нет? 9. В лучшем случае у вас это n элементов в
[16:37.840 --> 16:41.880]  минус 1. На самом деле работают по подвыборкам, потому что на миллиардах объектов смысла не имеет.
[16:41.880 --> 16:46.920]  Все, пока понятно? Еще вопрос есть, кроме того, что такое аш, про него будем говорить. Аша это
[16:46.920 --> 16:54.960]  какая-то мера неупорядоченности. Хорошо? Классно. Ну тогда давайте как раз-таки говорим. Ашка вообще
[16:54.960 --> 17:00.000]  называют критерием информативности. Information criteria, критерии информативности. И на них
[17:00.000 --> 17:05.540]  зиждется вообще построение деревьев. Например, вот у нас есть какое-то простое задание по
[17:05.540 --> 17:10.240]  классицировать шарики. У нас одномерная с вами выборка, то есть у нас одно число признак и два
[17:10.240 --> 17:16.800]  класса. Нам нужно как-то поделить выборку так, чтобы она была наиболее похожа на упорядоченную,
[17:16.800 --> 17:21.960]  на однородную в каждом из листьев. Вот здесь хорошо видно, что происходит. Мы с вами можем
[17:21.960 --> 17:26.080]  пробегать, собственно, поставили трешфл здесь. И что, допустим, будем предсказывать? Давайте
[17:26.080 --> 17:30.960]  предсказывать на 1, 2, 3. Наиболее часто встречающийся класс, например, чтобы минимизировать
[17:30.960 --> 17:35.160]  количество ошибок. Простейший случай вот деревянный. Вообще можно предсказывать вектор
[17:35.160 --> 17:39.600]  вероятности как раз к сосредним, но давайте совсем деревянное дерево возьмем. Собственно,
[17:39.600 --> 17:44.680]  отделили оранжевый. Оранжевый налево, тогда там у нас 100-процентные попадания. Все остальные
[17:44.680 --> 17:49.360]  направо, тут примерно 50 на 50. Ну допустим, желтый доминирует, все равно 50 процентов ошибок.
[17:49.360 --> 17:55.080]  Соответственно, потом попробовали сдвинуться куда? Вот сюда. Теперь отодвигаем сюда, тогда синий
[17:55.080 --> 18:00.880]  налево. Там условно 5 шестых, что это синий. Маленькая ошибка. Справа у нас желтый доминирует уже
[18:00.880 --> 18:05.960]  где-то семь десятых желтых. Нормально. Можем сдвинуться сюда. Теперь у нас справа 50 на 50,
[18:05.960 --> 18:10.080]  слева 50 на 50. Не очень хорошо. Можем сдвинуться сюда и так далее. Короче, по всем трешхолдам
[18:10.080 --> 18:15.640]  прошлись. Выбрали оптимальное разбиение вот здесь. Желтые направо, эти налево. Здесь, соответственно,
[18:15.640 --> 18:20.760]  предсказываем желтые. Здесь предсказываем, ну, наверное, синий не доминирует. Теперь опять же
[18:20.760 --> 18:25.120]  по каждой подвыбраке повторяем ту же самую операцию. Пробегаемся по всем здесь, пробегаемся по всем
[18:25.120 --> 18:32.200]  здесь, ищем две подвыбраки и, собственно, смотрим, насколько стало лучше. Окей? Ну и, соответственно,
[18:32.200 --> 18:38.360]  вот мы с вами пока нарисовал, потом будем дальше. Вопрос, как измерять вот эту самую упорядоченность.
[18:38.360 --> 18:46.000]  Способ первый деревянный. Давайте посчитаем. Вот, деревянный способ. Просто напросто количество
[18:46.000 --> 18:51.560]  ошибок классификации. Ну, по сути, вы всегда предсказываете доминирующий класс, тогда вы смотрите,
[18:51.560 --> 18:56.320]  сколько объектов вы классифицировали неправильно. Способ, на самом деле, плохой по множеству причин,
[18:56.320 --> 19:01.080]  особенно в многоклассовой классификации. То, что может быть дерево, может быть, хорошо отделяет
[19:01.080 --> 19:05.960]  условно второй-третий класс от всех остальных, и оно полезно, но при этом оно все равно ошибается
[19:05.960 --> 19:10.680]  много. Но это способ, чисто чтобы вот так на словах рассказать, по факту используется вот эта парочка.
[19:10.680 --> 19:21.880]  Во-первых, энтропия. Все помнят чутку энтропии. Сейчас, да, это по сути до доли этого класса,
[19:21.880 --> 19:25.840]  то есть нормированная частота, если мы говорим про выборку. Вот у вас попали объекты в выборку,
[19:25.840 --> 19:30.680]  под выборку. У вас есть объекты одного класса второго. Пит, соответственно, частота этого класса,
[19:30.680 --> 19:36.480]  и так далее. Нормированы на общий объем. Ну вот, и соответственно, можем посчитать либо энтропию
[19:36.480 --> 19:43.680]  для каждой под выборки, либо, соответственно, посчитать критерии джинни или неоднородность джинни,
[19:43.680 --> 19:54.560]  и так далее. Не путать, а? Да, не-не, это неправильных, а смотрите, у вас два класса, например, то есть у вас
[19:54.560 --> 20:04.760]  P0 это доля класса 0 в под выборке. P1 это доля класса 1 в под выборке. P1 плюс P2 будет давать единицу,
[20:04.760 --> 20:13.400]  если у вас всего два класса. Бинарная классикация. P0 нулевой класс, P1 первый класс. Помните,
[20:13.400 --> 20:18.480]  мы же с вами в каждом этом листе предсказываем константу, правильно? Мы будем предсказывать
[20:18.480 --> 20:23.160]  с вами вектор вероятностей. Краски, если у нас два класса, это P0, P1. Если десять классов,
[20:23.160 --> 20:28.080]  то P0, P1, P2, P3, и так далее, да по десять. Все, пока понятно? Да?
[20:34.760 --> 20:44.840]  Аж от левой, аж от правой, где вы аж от всей выборки считаете?
[20:44.840 --> 20:56.080]  Конечно, у вас разные, все, вы поделили пространство на две части, перерос в левый,
[20:56.080 --> 21:00.640]  в правый, все же вообще независимо. Пожалуйста, еще вопрос есть?
[21:00.640 --> 21:07.760]  Не-не-не, размер дерева мы пока вообще никак не трогаем от гиперпараметра. Мы хотим сделать
[21:07.760 --> 21:12.400]  наиболее однородные объекты в наших листьях, однородные с точки зрения нашей цельевой
[21:12.400 --> 21:17.200]  переменной. Пластикации, например, мы можем с вами считать энтропию. Так, на всякий случай,
[21:17.200 --> 21:22.040]  краткий ликбез. Энтропия, скажем так, неформальное определение, мера хаоса. Чем выше энтропия,
[21:22.040 --> 21:28.200]  тем больше неоднородность. Теория информации повсеместно, в принципе, энтропия Шенона или
[21:28.200 --> 21:34.800]  Шенона, кто как переводит. Шенона обычно? Шенона, спасибо. Энтропия Шенона, собственно,
[21:34.800 --> 21:39.720]  повсеместно используется, ввел в 1951 году, по-моему, могу ошибаться, короче, где-то вот в районе
[21:39.720 --> 21:46.760]  50-х годов. И давайте тогда принесу пару вещей, скажу так, мне здесь будет видно, где у нас
[21:46.760 --> 21:54.360]  заметки, вот у нас заметки. Смотрите, приведу простой пример, так как у нас пока распределение
[21:54.360 --> 22:02.800]  все дискретные, у нас все очень просто. Интересно, можно это как-нибудь сделать на черном фоне?
[22:02.800 --> 22:18.800]  Никто не умеет? Ладно, пускай будет на белом. Чего? Долго. Смотрите, коллеги, давайте нарисуем с
[22:18.800 --> 22:26.240]  вами вот такую штуку. Вот у нас с вами есть распределение, у которого есть четыре исхода,
[22:26.240 --> 22:38.360]  0, 1, 2 и 3. Тут у нас будет вероятность 100%, тут 0%, ну и тут, соответственно, тоже 0, 0. Хорошо?
[22:38.360 --> 22:44.760]  Согласны, что у нас может быть случайная величина с таким распределением дискретное? Хорошо,
[22:44.840 --> 22:59.680]  вот вам второе распределение, словно 50%, вот он тут у нас будет 25%, и тут будет 25%, а тут нет.
[22:59.680 --> 23:12.720]  Ну, иногда бывают такие новости, знаете, тут, соответственно, 0%, опять 0, 1, 2, 3. Хорошо,
[23:12.800 --> 23:18.360]  ну и вот третье распределение, соответственно, там будет, я думаю, вы уже догадались, что там
[23:18.360 --> 23:31.320]  будет, там, соответственно, у нас 1, 2, 3, 4, 0, 1, 2, 3, по 25% везде. Смотрите, мы с вами вытаскиваем,
[23:31.320 --> 23:35.640]  собственно, вот у нас три распределения, допустим, у вас чан с шариками. Распределение шариков,
[23:35.640 --> 23:40.440]  частота шариков разных цветов в одном чане, все красные, во втором есть красные, синие, зеленые,
[23:40.440 --> 23:46.760]  нет черных, в третьем все поровну. Логично? Вот, собственно, вопрос, насколько хаотичен будет
[23:46.760 --> 23:53.400]  наш выбор шарика из чана? Если у нас распределение вот 100% шариков красные, у нас все шарики,
[23:53.400 --> 23:58.920]  которые мы вынем будут какие? Одинаковые. У нас мера хаоса, у нас нет никакого хаоса,
[23:58.920 --> 24:04.000]  мы детерминированно получаем шарик красного цвета. Все, энтропия, на самом деле, здесь будет
[24:04.000 --> 24:09.280]  наименьшее, его, кстати, можете проверить, собственно, минус P, лог P, на самом деле можете
[24:09.280 --> 24:16.800]  посчитать, где здесь минимум достигается для любого распределения и где максимум достигается. А, что?
[24:16.800 --> 24:26.920]  Ну, окей, давайте здесь, скажем так, будет плюс эпсилон, бесконечно мало какое-то число, чтобы у
[24:26.920 --> 24:32.920]  вас логарифм не сломался, логарифм от нуля немножко не работает. Ну, если у вас логарифм P, где P0,
[24:32.920 --> 24:36.880]  берите тогда предел какой-нибудь, не знаю, или смотрите что-нибудь, короче, у вас эта штука
[24:36.880 --> 24:43.560]  улетает тогда куда? Вот, можем просто определить логарифм нуля нулем, потому что у вас получается,
[24:43.560 --> 24:49.120]  что вот эта штука, какое число, это 0, 0 на какое-то приближение, все равно дает 0. Просто иначе
[24:49.120 --> 24:54.880]  он уйдет у вас в минус бесконечности и будет нам не счастлив. Ну вот, предел все равно нулевой,
[24:54.880 --> 25:00.080]  все правильно. Ну, просто народ к пятому курсу не любит считать предел, особенно по лопиталю,
[25:00.080 --> 25:03.920]  а любит как бы, ну вот взяли приблизительно каким-то P плюс эпсилон, это констант,
[25:03.920 --> 25:09.280]  и констант на 0, 0 всего, молодцы. Хорошо, короче, понятно, здесь у нас соответственно шанс уже
[25:09.280 --> 25:14.600]  вытащить объекты другого класса выше, но при этом, допустим, черных все равно нет, правильно? Здесь
[25:14.600 --> 25:18.080]  у нас все максимально рандомно, мы вытаскиваем какой-то шарик, мы ничего не можем сказать про
[25:18.080 --> 25:23.200]  наш исход, правильно? Грубо говоря, вот вам энтропия на пальцах. Чем выше у вас энтропия,
[25:23.200 --> 25:28.240]  тем больше хаос у вас будет в исходных экспериментах. По факту, энтропия тоже, ну,
[25:28.520 --> 25:31.620]  она сильно связана с тем, сколько понадобится, можем двигать камеру,
[25:31.620 --> 25:35.480]  сколько понадобится Bit, ш receive, чтобы закодировать ваш сигнал. Чем выше у сигнала
[25:35.480 --> 25:39.120]  энтропия, тем больше вам надо информ Presidential In position, чтобы закодировать. Чем более он
[25:39.120 --> 25:46.880]  однородный, тем меньше вам надо информации, чтобы его кодировать. Логично? Понятно, нет? Я так
[25:46.880 --> 25:50.400]  понимаю, придется вам отдельно рассказать про cross-entropy, потому что по сути Log loss это
[25:50.400 --> 25:54.600]  Cross-entropy, но судя по вопросам про энтропию это вызывает некоторые вопросы. Ничего, починим.
[25:54.600 --> 26:00.720]  Хорошо, короче, энтропия, мера неупорядоченности. И давайте с вами на неё краски внимательно
[26:00.720 --> 26:05.040]  посмотрим, краски на примере этого. Слушайте, коллеги, можно я тогда на будущие занятия,
[26:05.040 --> 26:10.360]  наверное, попрошу это, вот у нас там стоит вот этот столб с этим прожектором, я, наверное, попрошу
[26:10.360 --> 26:14.280]  потом от него это, чуть подальше, наверное, сидеть вот на этом ряду, потому что этот шуток постоянно
[26:14.280 --> 26:19.960]  болтается, мне кажется, у вас в голове уже скоро всё закурится, нет? У вас нет такого? Просто я вижу,
[26:19.960 --> 26:23.880]  что он оболтается, у меня ощущение, вот как я в машине с ноутбуком еду, у меня немножко в глазах
[26:23.880 --> 26:30.640]  хребит. Ладно, если вам удобно, то up to you. Хорошо, короче, вот давайте энтропии разберёмся,
[26:30.640 --> 26:34.960]  собственно, считаю, делим, на самом деле это не просто так разделение, это разделение оптимальное
[26:34.960 --> 26:39.720]  энтропии, почему, смотрите, правая подвыборка, почти вся оранжевая, получается, энтропия у неё
[26:39.720 --> 26:45.280]  очень сильно просела в этот момент. Левая подвыборка, всё равно синие доминирует оранжевое,
[26:45.280 --> 26:51.640]  опять энтропия стала меньше, она более упорядочная. Далее, нам выгодно разбить на самом деле левую вот
[26:51.640 --> 26:56.400]  так, получается, опять все оранжевые, здесь синий, жёлтый, дальше выгодно вот этот разбить,
[26:56.400 --> 27:01.120]  ну потому что здесь оранжевых больше, мера просто-напросто больше, потом это опять разбиваем пополам,
[27:01.120 --> 27:08.840]  всё, мы все дерево побили, теперь у нас с каждым листей энтропия какая? Чему равна? Нулю, всё
[27:08.840 --> 27:12.480]  правильно. Ну потому что всё, энтропия ноль, у нас нет никакого хаоса, у нас всё однородно. И,
[27:12.480 --> 27:17.800]  соответственно, вот мы с вами это посчитали. В бинарном случае энтропию, точнее, в многоклассном
[27:17.800 --> 27:23.440]  случае, в принципе, энтропия это вот ПК, лог ПК, на всякий случай тут могут сразу задать вопрос,
[27:23.440 --> 27:29.720]  часто задают, а по какому основанию? Ну, ответ, собственно, в принципе, все логарифмы похожи друг
[27:29.720 --> 27:34.000]  на друга с точностью до константа, ровно поэтому тут торчит константа. В принципе, обычно считают
[27:34.000 --> 27:38.800]  по основанию два, иногда по натуральному логарифму, в целом, если вы не занимаетесь обработкой
[27:38.800 --> 27:43.920]  сигналов цифровой, то по барабану, потому что энтропию вы будете сравнивать между собой для
[27:43.920 --> 27:49.080]  разных классов, для разных, точнее, простите, не классов, а выборок. Вот что важно. А по какому
[27:49.080 --> 27:53.760]  основанию? Он с точностью до константа и вообще неважно. Обычно берут те, которые реализованы
[27:53.760 --> 27:58.480]  хорошо в компе, ну, например, там либо двойку, либо ешку, ту, что удобнее, десятку, по-моему,
[27:58.480 --> 28:03.080]  реже берут, но я могу ошибаться, я от этого всё-таки несколько подальше. Ну и на всякий случай,
[28:03.080 --> 28:08.720]  я надеюсь, понятно, что для бинарного случая можно переписать вот в таком виде. Тобственно,
[28:08.720 --> 28:16.800]  ничего вам не напоминает вот эта запись. Лог-лоз, там все дела. Помните, у нас там было
[28:16.800 --> 28:24.600]  P лог-q, минус 1 минус P лог-q, лог-1 минус q. Поэтому она и называется кросс натуропия. Тут у нас одно
[28:24.600 --> 28:34.080]  распределение, и мы смотрим, насколько правдоподобен логариф, вероятно, данного распределения
[28:34.080 --> 28:39.720]  по нему самому. В кросс натуропии мы смотрим на правдоподобие предсказанного распределения по
[28:39.720 --> 28:45.360]  истному распределению, только и всего. А это предусловие «да», это бинарный случай, в общем
[28:45.360 --> 28:53.080]  случае вот у вас такая сумма. Хорошо. Вероятность положительного класса. Но если у вас дерево,
[28:53.080 --> 28:58.600]  то соответственно вероятность каждого класса будет соответствовать чему. Доли данного класса в
[28:58.600 --> 29:04.440]  том листе, куда он попал. Например, у нас в листе 6 к 1, тогда вероятность синих будет 6 седьмых,
[29:04.440 --> 29:11.600]  вероятность желтых будет одна седьмая. По всем листям считаем, да. Но это итоговый. А при
[29:11.600 --> 29:16.440]  построении, собственно, каждый лист мы бьем пополам, до тех пор, пока не выполнено будет одно из
[29:16.440 --> 29:19.760]  условий. Потому что у нас, когда дерево строится, вы видите, у нас дерево может строиться либо до
[29:19.760 --> 29:24.400]  упора, когда смысла больше нет, но представьте себе, что у вас достаточно неоднородные данные,
[29:24.400 --> 29:29.040]  и у вас там миллиарды объектов. Бить дерево до упора, может быть, там, не знаю, у них, конечно,
[29:29.040 --> 29:33.840]  глубина лагериста будет от числа этих объектов, но все равно он может быть очень глубоким. Как
[29:33.840 --> 29:40.360]  правило, ставят максимальную глубину дерева, не более 50 и все. 50 глубже не пойдем. Или другое
[29:40.360 --> 29:45.560]  дело, говорят, минимальный прирост информативности. То есть, например, насколько минимум должна упасть
[29:45.560 --> 29:51.040]  на тропии при разбиении, чтобы мы приняли это разбиение. Ну, словно, скорее всего, вот такие
[29:51.160 --> 29:55.440]  случаи просто-напросто не рассматриваются в реальных деревьях, потому что там прирост на тропии
[29:55.440 --> 29:59.720]  мизерный относительно общей выборки, а надо еще одну ноду ставить, еще одно биение ставить.
[29:59.720 --> 30:14.240]  Это просто-напросто не выгодит. Вот. Ну, по сути, у нас такой trade-off. Мы за точность покупаем скорость.
[30:14.240 --> 30:27.820]  Не, а между нулем и двоечкой, собственно, у нас с вами жадная оптимизация, мы говорим,
[30:27.820 --> 30:35.640]  наибольшей прирост информативности должен быть, то есть наибольшее падение тропии. Да, если бы мы
[30:35.640 --> 30:40.400]  шли до максимальной глубины, мы могли бы по-разному на самом деле делить, но в реальной жизни мы не
[30:40.400 --> 30:45.000]  можем себе позволить строить деревья до упора, и вторая проблема на самом деле есть. Чем глубже у
[30:45.000 --> 30:51.520]  вас дерево, бинга, тем больше у вас дерево переобучается, причем дерево это делает вообще в лоб. Дерево,
[30:51.520 --> 30:56.040]  по сути, просто делит вам ваше пространство признаковое на какие-то кусочки, в котором говорит,
[30:56.040 --> 31:01.120]  здесь то, здесь это. В худшем случае дерево тупо запомнит для каждого объекта его кусочек пространства,
[31:01.120 --> 31:06.320]  скажут, там его метка класса или его целевая переменная, и все, у вас ошибка дерева ноль. Кроме
[31:06.320 --> 31:12.040]  случая, когда у вас у выборки есть эти, как их называют, объекты разных классов с абсолютно
[31:12.040 --> 31:16.720]  одинаковыми этими координатами. Но если у вас объекты разные, одинаковые, а метка класса разная,
[31:16.720 --> 31:21.800]  тогда дерево не может предсказать константу, которая удовлетворяет обоих. Хорошо, и давайте теперь
[31:21.800 --> 31:26.480]  посмотрим на, собственно, неоднородность джинни. Пожалуйста, не путайте из экономики с критерием джинни,
[31:26.480 --> 31:30.280]  кажется, я уже даже один раз и говорил критерии джинни, короче, джинни, все. Это не то, что вот в
[31:30.280 --> 31:34.600]  экономике считают там индекс джинни, есть там индекс джинни, это не про нас. Я на всякий случай,
[31:34.600 --> 31:40.040]  опять же, рекомендую, есть статья от Яконова в его блоге «Анализ малых данных» про критерии джинни и
[31:40.040 --> 31:45.720]  что не надо упудать с индексом джинни. Смотрите, на всякий случай тут проверим ваше интуитивное
[31:45.720 --> 31:49.120]  понимание теории вероятности. Хотя, говорил мне мой преподаватель, что интуитивного понимания
[31:49.120 --> 31:53.440]  краски у человека нет. Теория вера, поэтому его сложно учить. Я думаю, кто-то понял, кто был
[31:53.440 --> 32:00.880]  мой преподаватель. Клятчий вопрос. Итак, смотрите, посмотрите внимательно, пожалуйста, сюда,
[32:00.880 --> 32:07.640]  что такое неоднородность джинни? Вот посмотрите внимательно, подумайте, что на самом деле она нам
[32:07.640 --> 32:25.680]  показывает? Дисперсию. Ну, смотрите, вот P плюс — это вероятность того, что у вас объект класса плюс,
[32:25.680 --> 32:36.280]  правильно? Бинго. Если мы берем два объекта из нашей подвыборки, какова вероятность того,
[32:36.280 --> 32:43.080]  что они окажутся что? Одного класса или разных классов? Разных классов. Абсолютно верно. Смотрите,
[32:43.080 --> 32:47.520]  мы смотрим, вот это вероятность того, что два объекта одного класса просумированы по всем классам.
[32:47.520 --> 32:51.600]  Соответственно, если мы хотим получить два объекта разных классов, один минус вероятности того,
[32:51.600 --> 32:56.200]  что они одного класса. Получается, чем более у нас однородная выборка, тем больше вероятности того,
[32:56.200 --> 33:03.160]  что мы пару объектов возьмем одного класса. Логично. На самом деле на практике индекс джинни и индекс,
[33:03.160 --> 33:07.480]  вот я опять оговариваюсь, критерий джинни, простите, вырезать, вряд ли вырезать будем,
[33:07.480 --> 33:13.200]  но короче, это оговорка. Критерий джинни — это вот. На самом деле, если нарисовать их для бинарного
[33:13.200 --> 33:18.200]  класса, для бинарных классов, смотрите, у нас по оси XP плюс, а тут соответственно наша самая
[33:18.200 --> 33:24.720]  энтропия или джинни. Можем видеть, что в принципе джинни, вот он синенький, энтропия, вот она
[33:24.720 --> 33:28.920]  фиолетовенькая. Если джинни умножить на два, то энтропия с джинни вообще говоря ведутся очень
[33:28.920 --> 33:33.520]  похоже. То есть, грубо говоря, чем более у нас неоднородная выборка, тем больше они штрафуют.
[33:33.520 --> 33:38.600]  Максимум, логично, все достигают в 0,5 для бинарной классикации, но потому что максимальная энтропия
[33:38.600 --> 33:46.040]  для бинарного случая будет в краске 0,5-0,5. И в принципе разницы между ними на практике особо нет,
[33:46.040 --> 33:50.760]  в последнее время вроде как больше использовали джинни, мне лично больше импонирует энтропия,
[33:50.760 --> 33:54.360]  просто то, что энтропия у нас почти везде сидит, плюс она смотрит на распределение целиком,
[33:54.360 --> 33:58.880]  а джинни говорит только, почему про выбор двух объектов одного класса, а почему не три объекта,
[33:58.880 --> 34:04.040]  почему не 10, по-хорошему там взять 2, 3, 4, 5, 10 и вот это все просуммировать, но тогда ряд долго
[34:04.040 --> 34:15.320]  считать и не очень удобно. Какую идею? Смотрите, энтропия смотрит на неоднородное распределение
[34:15.320 --> 34:18.880]  в целом, то есть это, грубо говоря, сколько информации надо, чтобы декодировать данные за это
[34:18.880 --> 34:25.640]  распределение. Джинни это вероятность взять пару объектов одного класса, точнее, наоборот,
[34:25.640 --> 34:31.320]  вероятность, что взяв пару объектов, они будут разного класса. А почему мы не берем еще тройку
[34:31.320 --> 34:42.360]  объектов или четверку или пятерку? А вот это любопытно.
[34:42.360 --> 35:00.760]  Да, 1 минус норм это вектор, ну, кстати, да, но, правда, без корня. Любопытно слушать, не думал об этом.
[35:12.360 --> 35:16.680]  Да, да, да, согласен.
[35:27.480 --> 35:33.840]  Какой график? А, вот это, вот это. А missed class, это, собственно, первый самый критерий,
[35:33.840 --> 35:37.720]  который мы с вами обсуждали, просто ошибка. То есть, смотрите, она линейно
[35:37.720 --> 35:42.800]  растет, собственно, пока у нас один класс доминирует, но у него вероятности меньше и меньше,
[35:42.800 --> 35:45.720]  она просто линейно растет. Потом опять линейно падает, потому что другой класс стал максимум.
[35:45.720 --> 35:52.000]  Собственно, глядя вот сюда, можно видеть, что у нас мисс классификация штрафует слишком мало за
[35:52.000 --> 35:57.400]  малые ошибки. Видите, он очень медленно растет относительно и джинни энтропии. И джинни энтропии
[35:57.400 --> 36:01.880]  видят, что у нас вероятность сразу стала, скажем так, не нулевой для второго класса, а надо,
[36:01.880 --> 36:07.280]  давай за это бить указкой, потому что не надо так делать. Мисс классификация такой, ну ладно, что-то там.
[36:07.280 --> 36:10.200]  Вот. Так, тут еще вопрос есть?
[36:24.200 --> 36:28.640]  Ну, в принципе-то вы можете это перенормировать в любое, но как бы, да?
[36:32.640 --> 36:45.040]  Ну, кстати, возможно, поэтому его в последних деревьях используют. Вот, смотрите, есть различные
[36:45.040 --> 36:52.440]  реализации, тут где-то даже, наверное, будет. Бла-бла-бла, бла-бла-бла. Нет, похоже, не будет. Возможно,
[36:52.440 --> 36:58.280]  я вытяну этот слайд. Короче, есть различные алгоритмы построения деревьев. ID3, это кто-то там
[36:58.280 --> 37:04.720]  4-5, карт, вот сам последний classification and regression trees, который в эскалерне сидит, они все были с разными,
[37:04.720 --> 37:10.840]  условно, способами построения, но по факту сейчас, ну вот дерево вы напишете в домашке, если захотите,
[37:10.840 --> 37:15.240]  но по факту, конечно, выгоднее использовать как и библиотеки, которые вам векторное умножение
[37:15.240 --> 37:20.120]  под капотом используют, так и деревья строят, потому что они с кучей всяких доработок,
[37:20.120 --> 37:23.360]  имплементации и так далее работают быстро. Причем, желательно, не эскалерна, а какой-нибудь
[37:23.480 --> 37:31.840]  cutboost, xgboost или lightgbm, они под капотом используют краски, умеют строить деревья. По факту, там гораздо лучше, да.
[37:31.840 --> 37:44.160]  Потому что, смотрите, у вас необходимость построить хороший классикатор, то есть чтобы он максимально,
[37:44.160 --> 37:51.320]  как сказать, максимально точно предсказывал вам метки классов. Грубо говоря, у нас классикатор
[37:51.320 --> 37:56.920]  тем лучше, чем меньше вероятность совершать ошибку. То есть получается, что у нас джинни и энтропия
[37:56.920 --> 38:04.160]  штрафуют слабо в районе там 0,6 или 0,5, это вероятность правильного класса, то что и так и так у вас
[38:04.160 --> 38:09.120]  какой-то рандом, но штраф тем сильнее, чем ближе вы к нулю, то есть либо у вас вообще однородная
[38:09.120 --> 38:14.320]  выборка, либо она неоднородная, и сразу за это получается штраф модели. А мисс классикиш, ну вообще
[38:14.320 --> 38:18.560]  все равно, высовка вероятность низкая, то есть он никак не разделяет точность, что она почти
[38:18.560 --> 38:40.920]  стопроцентная, что она почти 0,5. Да, и чего? У вас не может быть, если у вас единицы минус
[38:40.920 --> 38:48.000]  п квадрат, еще раз, у вас сумма по обоим классам, тогда у вас будет 1 минус 0 и 1 минус 1, получается 0.
[38:48.000 --> 38:57.320]  Так, хорошо, коллеги, ну и смотрите, в чем еще на самом деле плюс вот этого всего дела? В том, что в
[38:57.320 --> 39:03.280]  отличие от всех пока что предыдущих алгоритмов, кроме, наверное, КННа, решающие деревья вообще в лед
[39:03.280 --> 39:09.040]  и в лоб обобщаются на регрессию. Нам единственное, что надо сделать, это другой критерий информативности,
[39:09.040 --> 39:13.080]  ввести и вместо энтропии считать, например, что? Вот вы для выборки предсказываете константу,
[39:13.080 --> 39:19.280]  что можно считать? Ну МСЕ, а что по факту будет МСЕ? Точнее, какая оценка оптимальна с точки
[39:19.280 --> 39:27.760]  зрения МСЕ? У вас есть выборка, вы хотите предсказать значение для нее константой, минимизируя МСЕ?
[39:27.760 --> 39:38.440]  Средняя. Ну по сути, тогда вы будете просто, например, дисперсию считать относительно среднего,
[39:38.440 --> 39:46.000]  а если вы МАЕ оптимизируете, уже вопрос со звездочкой, медиану, бинго. Ну те, у кого тяровер был,
[39:46.000 --> 39:49.720]  видимо, это уже понимают. То есть если мы с вами минимизируем, например, вот эту величину,
[39:49.720 --> 39:55.280]  отклонение от некоторой константы, то как раз таки оптимальное с точки зрения, это по сути МЛЕ,
[39:55.280 --> 40:01.880]  максимум лайк и худ скимаш, оценка максимального правдоподобия на вот эту цешку, это средняя. Если
[40:01.880 --> 40:10.440]  вы посчитаете точно также оценку максимального правдоподобия на МЛЕ, то у вас получится медиана
[40:10.440 --> 40:15.120]  и так далее. Если хотите, кстати, можете ко мне потом пристать, можем с вами это попробовать даже
[40:15.120 --> 40:19.680]  вывести где-нибудь прямо после-после. Ну или можете сами повыводить, я вам могу просто,
[40:19.680 --> 40:25.960]  у меня даже на планшете это все написано, я могу просто сейчас скинуть потом, если хотите. Я там
[40:25.960 --> 40:30.760]  тоже писал на комнате семинаров. Окей, короче, плюс дерево в том, что вы его один раз написали,
[40:30.760 --> 40:34.600]  вы туда вставили критерии для регрессии, работает регрессия, критерии для классификации,
[40:34.600 --> 40:38.920]  работает классификация. И плюс в чем? Он вам строит нелинейную разделяющую поверхность или
[40:38.920 --> 40:44.240]  нелинейную аппроксимацию, при этом, по сути, дешево. Ну как дешево? У вас с одной стороны вроде
[40:44.240 --> 40:48.640]  как дерево строить долго, но с другой стороны, во-первых, жадные алгоритмы работают неплохо,
[40:48.640 --> 40:53.360]  во-вторых, деревья не такие глубокие, там глубина обычно, ну словно до 50 до 70, это не так сложно.
[40:53.360 --> 40:59.400]  И дерево замечательно в двух апостасях. Во-первых, его абсолютно замечательно применять,
[40:59.400 --> 41:04.120]  вам, по сути, на применение просто кучу ифов надо провести и все. Это работает быстро. Для каждого
[41:04.120 --> 41:08.920]  объекта вы просто, по сути, кидаете объект и у нас куда-то падает. И второе, дерево абсолютно
[41:08.920 --> 41:14.640]  феноменально объяснять, как работает. Вы по дереву можете четко сказать, куда, кто попал и почему.
[41:14.640 --> 41:20.040]  У вас все вот эти вот разделения, они вам всем известны. Они очень хорошо интерпретируются,
[41:20.040 --> 41:24.880]  именно поэтому их любят во всяких банках, там страховых и так далее. Но у дерева есть минус,
[41:24.880 --> 41:31.000]  который мы с вами уже вроде как озвучили. Дерево переобучается просто как нехорошее дерево,
[41:31.000 --> 41:45.680]  как буратино, я не знаю, которое много врет. Как мы работаем на практике, давайте еще раз. Вот у
[41:45.680 --> 41:51.400]  нас есть с вами выборка, одномерная или многомерная, не важно. Мы просто методично,
[41:51.400 --> 41:56.920]  занудно берем каждый признак, для него выбираем все возможные трешхолды или между какими-то
[41:56.920 --> 42:01.200]  подгруппами по этому признаку, если там объекты однородные, грубо говоря, или рандомно их
[42:01.200 --> 42:05.520]  накидываем, потому что если у вас там условно выборка в 10 миллионов объектов дороговато,
[42:05.520 --> 42:10.000]  как ты их накидываете. По всем трешхолдам считайте разбийния направо-налево, считайте критерии
[42:10.000 --> 42:18.280]  информативности. Для всех трешхолдов, да, просто перебираем в жадную. Почему на самом деле недолго,
[42:18.280 --> 42:21.560]  потому что вы, во-первых, можете предыдущий статистик предыдущего шага на самом деле
[42:21.560 --> 42:26.840]  переиспользовать во многом, и так далее. Я и говорю, дерево строится жадно, почему?
[42:26.840 --> 42:30.960]  Потому что даже если вы на него посмотрите, это кусочная постоянная функция. О каких гридентах
[42:30.960 --> 42:35.680]  может идти речь? У вас там нет гридентов, у нас гридент всегда 0 либо неопределен в точке разрыва, все.
[42:35.680 --> 42:44.400]  Да, можно, собственно, я говорю, на больших выборках обычно или упорядчивают по подвыборкам,
[42:44.400 --> 42:48.800]  какой-то предпочет делают, или просто рандомно накидывают, да и все, потому что вам надо как-то
[42:48.800 --> 42:56.000]  получить эти самые трешхолды. Вы каждый по отдельности перебираете. Смотрите, в этом еще одна фишка
[42:56.000 --> 43:01.960]  дерева. Все признаки разморуются отдельно, вообще независимо друг от друга. Вы взяли один признак,
[43:01.960 --> 43:06.880]  по нему у вас объект упорядочно. Потом вы взяли второй признак, а потом вы выбираете тот признак и
[43:06.880 --> 43:12.280]  то разбиение, которое оптимально с точки зрения вот этого самого критерия, который вы выбрали. То есть
[43:12.280 --> 43:23.320]  дерево действует жадно. Вот у вас разбиение, вот у вас была подвыборка, вы перебрали все возможные
[43:23.320 --> 43:28.200]  признаки и все трешхолды для каждого из признаков и выбрали ту пару трешхолд-признак, которые
[43:28.200 --> 43:32.120]  оптимальны с точки зрения того, что у вас прирост информативности максимальный или меньше всего
[43:32.120 --> 43:36.680]  стала энтропия после этого. Теперь вы попали в две новые подвыборки, полностью делаете то же самое.
[43:36.680 --> 43:44.960]  Все, рекурсии, пока не уйдете до конца. Вот, с регрессией абсолютно все то же самое. Ну и смотрите,
[43:44.960 --> 43:51.240]  собственно, основная беда деревьев, как я уже сказал, к чем? Переобучаются очень сильно, но благо,
[43:51.240 --> 43:58.840]  так пруйник давайте потом тогда скажу, но благо есть у деревьев, помимо того, что они слабые,
[43:58.840 --> 44:05.360]  у них есть еще и сила. Их слабость, можно обратить краски в силу. И тут опять вопрос,
[44:05.360 --> 44:14.320]  кто знаком с процедурой бутстрапа? Ну вот сейчас мы к нему подойдем. Понял, никто не знаком. Давайте
[44:14.320 --> 44:19.040]  тогда опять же на пальцах вам объясню. Но для начала придется вам показать простенькую вещь.
[44:19.040 --> 44:25.040]  Смотрите, давайте введем понятие бутстрапированные выбраки. Что такое бутстрапированные выбрака? Вот у
[44:25.040 --> 44:32.240]  нас есть выбрака из к-элементов. Давайте на доску чуть-чуть перейдем. Вот у нас есть в
[44:32.320 --> 44:41.760]  выбраке это у нас х. Пусть у х размер n. Внимание, вопрос, как мы с вами можем сгенерировать из х
[44:41.760 --> 44:46.680]  новую выбраку размера n так, чтобы она полностью не дублировала х, но сохраняла плюс-минус
[44:46.680 --> 44:57.440]  зависимости из х? Звучит как бред, правда? Бинго, давайте построим новые выбраки, собственно,
[44:57.440 --> 45:08.680]  тоже размером n, хоть и не поменьше x1, x2, x3 и так далее, xk. Вот они у нас, они тоже все размера n,
[45:08.680 --> 45:16.640]  но при этом мы их будем строить как? Мы будем выбирать объекты с возвратом или выброс
[45:16.640 --> 45:22.640]  повторений. То есть грубо говоря, мы каждый раз, вот n раз, выбираем из х случайный элемент и все,
[45:22.640 --> 45:28.000]  и кладем его обратно. Тогда у нас получится, если мы такое четыре раза приведем, у нас получатся
[45:28.000 --> 45:33.240]  четыре выбраки, которые с одной стороны очень похожи на оригинальный х, то что все объекты в
[45:33.240 --> 45:38.880]  них содержатся и в оригинальном х, правильно? С другой стороны, они с ним полностью не совпадают,
[45:38.880 --> 45:42.760]  потому что какие-то объекты случайно попали туда n раз, несколько раз точнее, n у нас уже
[45:42.760 --> 45:52.600]  зарезервировано, какие-то не попали вовсе. Да, все эти выбраки,
[45:52.600 --> 45:57.760]  вот четыре строем независимо, то есть что мы делаем? Мы берем новую выбраку и n раз просто, по сути,
[45:57.760 --> 46:02.680]  можете себе представить каким образом. У вас объекты лежат где-то, вы в выбраку себе просто их
[46:02.680 --> 46:09.760]  индекса накидываете, то есть в этой выбраке у меня будут объекты 1, 1, 1, 7, 4, 4, 4, 28, 25, 32 и так далее.
[46:09.760 --> 46:13.400]  То есть, соответственно, у вас объекты могут быть одного класса, могут быть разных классов,
[46:13.400 --> 46:21.000]  один и тот же объект может вообще не попасть и так далее. Нет, почему? Каждая выбрака размером n,
[46:21.000 --> 46:31.400]  точно так же, чтобы у вас было n различных выборок, но при этом такого же объема. Зачем? Смотрите,
[46:31.400 --> 46:36.280]  во-первых, общие слова. Бусттрап это очень сильный статический метод, чтобы получать какие-то
[46:36.280 --> 46:42.320]  оценки. Например, вы можете посчитать какую-нибудь оценку, я не знаю, на ваш параметр по каждой из
[46:42.320 --> 46:45.880]  бусттрапированных выборок и получить целое распределение на оценку зависимости от того,
[46:45.880 --> 46:49.960]  что у вас туда попало. То, что вы за счет случайности краски выбора объекта, грубо говоря, вы можете
[46:49.960 --> 46:54.360]  какой-то выбор случайно не взять или что-то еще. Потому что если мы предполагаем, что у нас
[46:54.360 --> 46:57.840]  оригинальные данные пришли из какого-то распределения, то бустстрапированная выборка
[46:57.840 --> 47:08.560]  по-хорошему тоже должна оттуда же идти, все объекты оттуда же. Вот. Еще раз, все, давайте я нарисую тогда
[47:08.560 --> 47:23.320]  1. Вот у меня есть выборка х. Я выбираю вот сюда, просто random choice и получаю выборку х с крышкой.
[47:23.320 --> 47:34.640]  То есть я n раз выбираю случайно объект из х. Да, практически. То есть я по сути случайно,
[47:34.680 --> 47:47.640]  размер х с крышкой n. Вы каждый раз выбираете случайно. Да, выброс повторения. Вы по сути объект
[47:47.640 --> 47:52.120]  взяли и сказали так, красный будет здесь, но вы его по сути обратно вернули. Вы можете каждый раз
[47:52.120 --> 47:57.880]  случайно, с вероятностью условно, получается, если выборка размера n, то 1 делить на n в степени n,
[47:57.880 --> 48:02.360]  это вот вероятность вообще, что у вас выборка будет одного объекта. Ну типа она конечно маленькая,
[48:02.360 --> 48:10.160]  но и так далее. Ладно, это на самом деле. Да, по умолчанию у вас размер будет строфирован в
[48:10.160 --> 48:13.800]  выборке такой же, как и оригинальный. По факту это такой же гиперпараметр, вы можете его сделать
[48:13.800 --> 48:22.020]  меньше. Не-не-не, количество может быть любым, то есть это вы выбираете для ПК штук. Собственно,
[48:22.020 --> 48:27.840]  зачем нам это надо? Смотрите, теперь внимание на флайды. Давайте я сюда вот отойду, чтобы у меня
[48:27.840 --> 48:32.120]  было хотя бы как-то на записи видно. Собственно, пусть у нас есть выборка краски из-за объектов.
[48:32.120 --> 48:40.400]  Собственно, каждый раз выбираем объект с возвращением из X и получаем n различных дат сетов. Вот,
[48:40.400 --> 48:45.200]  ну тут разве что нотация другая, тут размер выборки был n, тут наоборот m. Короче, не запутаться,
[48:45.200 --> 48:51.640]  пожалуйста. Соответственно, в чем прикол? Давайте сюда посмотрим. Что это у нас такое? Мат ожидания
[48:51.640 --> 48:58.800]  B житой от X минус Y от X. B житой это как раз-таки то. Это житая модель, обученная на житой
[48:58.800 --> 49:07.880]  бутстрапированной выборке. Согласны? Соответственно, минус Y от X вот такая штука. Окей? Тогда у нас
[49:07.880 --> 49:14.040]  средняя ошибка всех n моделей. Это что такое? Все ошибки просуббировали и поделили на n. Согласны?
[49:14.040 --> 49:21.880]  Ну, вроде пока все логично, никакой магии, скажем так, не произошло. Верно? Тут вопросов нет?
[49:21.880 --> 49:31.480]  Y от X это истинное значение таргета. Нет, это сигнал, то есть это наша выборка, по сути. Для каждого
[49:31.480 --> 49:36.480]  X мы знаем ответ. B житой это модель обученная на житой бутстрапированной выборке, которая
[49:36.480 --> 49:42.320]  что-то там предсказала. Все. А теперь начинается магия. Теперь давайте сделаем два, собственно,
[49:42.320 --> 49:48.600]  предположения. Во-первых, что у нас все ошибки не смещенные, то есть ошибка на каждой из под
[49:48.600 --> 49:53.400]  выборок не смещенная, то есть ее от ожидания равно нулю. Хорошо? Нормальное предположение,
[49:53.400 --> 49:58.840]  потому что если у нас ошибка смещенная, что-то у нас не то с моделью. Второе, что ошибки независимы
[49:58.840 --> 50:05.600]  друг от друга. Точнее, что у них корреляция нулевая. Окей? Договорились? А теперь следить за магией.
[50:05.600 --> 50:11.600]  Ну, давайте тогда посчитаем, собственно, вот у нас будет модель. А от X это что? Это средняя из
[50:11.600 --> 50:17.240]  N моделей. То есть у нас была N модель из бутстрапированных выборок. Давайте тогда посчитаем. Тогда у нас
[50:17.240 --> 50:24.040]  ошибка для среднего. Это мат ожидания вот этой штуковины. Согласны? Тут как бы по математике давайте
[50:24.040 --> 50:29.680]  пройдемся. Мат ожидания или что-то то же самое. 1 делить на N е житой от X, потому что это ошибка
[50:29.680 --> 50:35.320]  каждой из моделей. Это все у нас в квадрате. Или мы можем N из квадрата вытащить. Получается 1 делить
[50:35.320 --> 50:43.440]  на N квадрат вот от этой штуки. Согласны? Умеем как бы квадрат-суммы расписывать вот таким
[50:43.440 --> 50:49.200]  образом. Бином-ньютон, там все дела. Хорошо? И это мы с вами только что сказали, что оно равно 0,
[50:49.200 --> 50:54.280]  потому что у нас корреляция, ошибка нулевая. Остается вот эта часть. И получается это 1 делить
[50:54.280 --> 51:02.040]  на N квадрат. А это что такое? Это ошибка N моделей. Ну и все. Получается 1 делить на N, ошибка одной
[51:02.040 --> 51:08.360]  модели. Масс ошибка с вами упала в N раз только что. Вот ошибка была для всех этих моделей средняя.
[51:08.360 --> 51:20.680]  1 делить на N, вот эта штука. А здесь 1 делить на N квадрат. Заметили? Чего о любой подвыборке?
[51:20.680 --> 51:30.760]  E1 это смотрите нет, у вас 10 подвыборок условно. Вы обучили 10 моделей. E1 это ошибка на каких-то
[51:30.760 --> 51:37.680]  там данных. Модель обычно на 1 подвыборке. Обучены на 1, а тестируем вообще на отложенных данных.
[51:37.680 --> 51:44.240]  E2 обучены на 2, тестируем на отложенных. Короче, внимание, вопрос, где здесь обман? Потому что у
[51:44.240 --> 51:47.320]  нас ошибка только что в N раз упала. Зачем нам вообще все остальные методы машинного обучения?
[51:47.320 --> 51:58.560]  Делаем кучу. Предположение правильное. На самом деле вот эта вот штука, что у нас ошибки не смещенные,
[51:58.560 --> 52:04.560]  бог с ними еще не скоррелированы, это наши мечты по факту такого не будет никогда. По факту, что мы
[52:04.560 --> 52:09.240]  с вами сказали, давайте у нас будет множество выборок, которые вообще независимы друг от друга,
[52:09.240 --> 52:13.800]  из одного этого распределения. Тогда мы можем с вами с минимальной ошибкой все это описать.
[52:13.800 --> 52:17.240]  Логично у нас чем больше данных, тем меньше у нас будет ошибка. По сути мы замощаем с вами
[52:17.240 --> 52:23.200]  все пространство какими-то объектами. Можем себе позволить. Но тем не менее это нас привело к тому,
[52:23.200 --> 52:28.960]  что был изобретен метод собственно беггинга. Беггинг это что? Это краски bootstrap aggregating,
[52:28.960 --> 52:35.560]  так и расшифровывается. И он направлен на что? На то, чтобы полечить основную беду наших
[52:35.560 --> 52:41.160]  деревьев. Потому что смотрите, у нас вот эта ошибка состоит из двух частей. У нас вот эта
[52:41.160 --> 52:46.600]  часть это ошибка каждого алгоритма, правильно? А это то, как они похожи друг на друга. Ошибку каждого
[52:46.600 --> 52:51.000]  алгоритма мы вряд ли как-то сможем исправить. Его построили и все, он готов. А вот сделать
[52:51.000 --> 52:55.200]  алгоритмом максимально непохожими друг на друга позволит нам второй члену меньше. И
[52:55.200 --> 52:59.880]  пускай он будет, скажем так, не нулевой, он все равно будет существенно меньше, чем если бы
[52:59.880 --> 53:06.440]  деревья были одинаковые, например. Если б кориация между ними была сто процентов. Короче, какое
[53:06.440 --> 53:10.680]  предложение? Давайте я вам сначала попытаюсь это на пальцах тогда объяснить, чтобы вы немножко
[53:10.680 --> 53:16.360]  оживились от мотанка, это страшно. А потом соответственно повторю. Смотрите, в чем суть? У вас
[53:16.360 --> 53:22.720]  каждое дерево может быть подстроено, правильно? Вот вашу под выборку. Теперь наша цель сделать
[53:22.720 --> 53:27.920]  модель, которая, собственно, модели так, чтобы у них ошибки были максимально непохожи друг на
[53:27.920 --> 53:32.560]  друга, правильно? Грубо говоря, модели должны быть непохожи друг на друга, значит у них
[53:32.560 --> 53:37.960]  предсказания разные, значит у них ошибки разные, верно? Тогда у нас вот этот результат будет плюс-минус
[53:37.960 --> 53:43.680]  достигаться. Ну так давайте тогда возьмем и обернем слабость деревьев, а именно их способность
[53:43.680 --> 53:48.960]  переобучаться нам во благо. Давайте возьмем нашу обучающую выборку, набудстрапируем из нее несколько,
[53:48.960 --> 53:55.760]  и на каждый из будстрапированных выборок переобучим наше дерево. Что тогда получается? У нас N деревьев,
[53:55.760 --> 54:03.720]  каждый переобучается под что-то свое, а потом мы их усредняем. Теперь, собственно, внимание, что
[54:03.720 --> 54:09.360]  происходит? У нас с вами краткие. Происходит ровно вот эта самая история, только здесь у нас вот этот
[54:09.360 --> 54:15.120]  член не в ноль обращается, а он просто меньше, чем он был бы, потому что у вас теперь ошибки между
[54:15.120 --> 54:21.280]  собой не полностью скоррелированы, а значит у вас все равно ошибка упадет не в N раз, но хотя бы в
[54:21.280 --> 54:26.280]  два-три раза совершенно спокойно может упасть. Если говорить на пальцах, что происходит? Здесь заходит
[54:26.280 --> 54:32.440]  классная вот эта история, всегда я, наверное, рассказываю, называется мудрой столпы. Представь себе
[54:32.440 --> 54:38.320]  повторальная картинка, вот средневековая, да, он самый, средневековая деревушка, ну вот как в фильмах,
[54:38.320 --> 54:42.400]  знаете, там все красиво, птички бают, какой-то тамада выходит на сцену, говорит, уважаемые
[54:42.400 --> 54:49.240]  крестьяне, да, не эй-холопы, что пришли, уважаемые крестьяне, вот бык, кто из вас угадает его вес,
[54:49.240 --> 54:53.360]  точность туда фунта, понятное дело, где-то там, в Британии условно работает, тот получит этого быка,
[54:53.360 --> 54:58.800]  ну а бык это же вообще подъем хозяйства, все круто, а соответственно ходят ассистенты и записывают,
[54:58.800 --> 55:06.280]  что там человек сказал. Ну в итоге записали, посмотрели и средний, усреднив все предсказания,
[55:06.280 --> 55:11.440]  получилось, что там условно вес быка 1148 фунтов, из балды говорю, не знаю сколько бык висит,
[55:11.440 --> 55:18.040]  а предсказанная средняя 1147, короче, почти с точки до фунта угадали, хотя каждый оценивал на глаз,
[55:18.040 --> 55:22.120]  ну понятное дело, быка никому не дали, потому что типа никто не угадал, средниковая история,
[55:22.120 --> 55:26.200]  все дела, быка вообще жалко, математика, конечно, штука хорошая, ладно, это все байки. Собственно,
[55:26.200 --> 55:30.320]  история в чем? Смотрите, у вас получается куча различных оценок от каждого человека по
[55:30.320 --> 55:36.320]  отдельности была записана. Каждый человек на глаз умеет каким-то образом оценивать вес,
[55:36.320 --> 55:40.120]  ну вы можете на глаз прикинуть, там сколько человек рядом висит, по этому все равно будет
[55:40.120 --> 55:43.800]  плюс-минус 10 процентов, скорее всего, но у нас глаз привык, и условно, я понимаю, что здесь
[55:43.800 --> 55:48.840]  примерно 250 грамм, а здесь, например, где-то 180, но это я на ощупь не на глаз, но визуально они
[55:48.840 --> 55:55.000]  тоже, один меньше другого. Поэтому что мы с вами делаем? У нас каждый, грубо говоря, человек
[55:55.000 --> 55:59.040]  предсказывает какую-то оценку, правильно? Но они все обусловлены на одного и того же быка,
[55:59.360 --> 56:03.420]  и мы можем вам сказать что что, что у человека есть какая-то случайная ошибка, он либо завышает
[56:03.420 --> 56:08.960]  оценку либо занижает, окей? Получается, когда мы с вами много предсказаний независимых, что самое
[56:08.960 --> 56:12.540]  важное, они между собой не совещались, они просто посмотрели на бока и их записали, они даже ничего
[56:12.540 --> 56:17.800]  голоса не говорили, много не зависимых предсказаний записали, у среднили, у нас с вами люди-то, в
[56:17.800 --> 56:22.660]  среднем, скорее всего случайно, завышают или занижают одинаково часто, нет систематической
[56:22.660 --> 56:28.320]  ошибки. А потому в среднем у нас ошибка расслоптана, ноль. Потому что у нас предсказание независимое
[56:28.320 --> 56:32.080]  друг от друга, это очень важно. Они друг на другу не влияют, они завышают и
[56:32.080 --> 56:37.920]  занижают одинаково. Вероятно, в итоге мы получаем более точную оценку. Согласны?
[56:37.920 --> 56:41.600]  Вот вам мудрость толпы. А теперь, собственно, вопрос для самопроверки
[56:41.600 --> 56:47.280]  второй. Называется нос императора. Древняя какая-то восточная страна, и император
[56:47.280 --> 56:52.160]  очень сильно комплексовал, что у него длинный нос. Его, соответственно, советник
[56:52.160 --> 56:57.180]  решил помочь и отправил гонцов во все края империи, чтобы те опросили граждан
[56:57.180 --> 57:03.020]  и спросили у них, какой длины у императора нос. Вот гонцы все вернулись, в итоге
[57:03.020 --> 57:07.140]  усреднили, что у них получилось, и получилась абсолютно среднестатистическая
[57:07.140 --> 57:10.380]  длина носа в этой самой империи. Император обрадовался и больше не
[57:10.380 --> 57:14.620]  комплексовал ходить на люди. А нос у него, правда, был длинный. Внимание, вопрос,
[57:14.620 --> 57:26.860]  почему не работает? Что случилось? Откуда оно взялось? Вариант да. Бинго. Это какие-то
[57:26.860 --> 57:31.180]  там древние века, там императора как бы видели, разве что члены гвардии и кто-нибудь
[57:31.180 --> 57:34.660]  там приближенный к императорской семье. У них каждое предсказание не было
[57:34.660 --> 57:39.840]  основано на X. Они ничего не знали про X. Они просто прикидывали, какая длина нос
[57:39.840 --> 57:46.240]  у императора. Они по сути знали примерно длину носа у своего окружения и говорили,
[57:46.240 --> 57:52.240]  сколько в среднем. Вот вам средний получил, длина в среднем. Поэтому вся эта история работает,
[57:52.240 --> 57:58.120]  когда если у вас оценки вашего алгоритма все еще информативны, то есть если у вас есть оценки,
[57:58.120 --> 58:02.520]  которые предсказывают что-то корректно, но с большой дисперсией, вы можете их усреднить и
[58:02.520 --> 58:06.560]  получить хороший результат. Если же у вас алгоритм просто какой-то рандом полный предсказывает,
[58:06.560 --> 58:11.440]  то есть его даже в среднем оценка плохая, то ничего не поможет. Мы на самом деле на следующих
[58:11.440 --> 58:15.880]  занятиях с вами поговорим про смещение и дисперсию, точнее про bias и variance, если говорить таким
[58:15.880 --> 58:21.640]  более корректным языком, смещение и разброс. И краски у деревьев, в чем суть? У деревьев смещение
[58:21.640 --> 58:28.920]  очень маленькое. То есть у них bias, но составляющая составная часть ошибки, она говорит, что на каждой
[58:28.920 --> 58:34.360]  подвыборке дерево очень хорошо опсилимирует свою подвыборку. Но при этом для разных подвыборок
[58:34.360 --> 58:39.520]  предсказания могут сильно разниться. То, что оно переобучено. Усреднение позволяет вам краски от
[58:39.520 --> 58:45.520]  дисперсии, variance, разброса избавиться частично. На самом деле тут даже видно почему, потому что
[58:45.520 --> 58:50.440]  у вас разброс дисперсии, дисперсия у вас будет в квадрате. При усреднении у вас n в квадрате,
[58:50.440 --> 58:55.840]  соответственно, n в квадрате вытаскивается, одна n-ка остается. Это лучше. Но это мы с вами более
[58:55.840 --> 59:09.400]  глубоко разберем. Ну смотрите, здесь я, к сожалению, могу отослать к литературе по бутстрапу. Напишите
[59:09.400 --> 59:13.640]  в чате, я вам прям скину, что почитать, почему бутстрап выгоднее, чем просто разбиение. Потому что
[59:13.640 --> 59:19.520]  по сути вы себе генерируете новые подвыборки. Они все равно статистически обладают почти теми же
[59:19.520 --> 59:23.480]  самыми свойствами. Если вы побьете на маленькие подвыборки, то с одной стороны вы правы, с другой
[59:23.480 --> 59:28.960]  стороны у вас просто выборка будет меньше, больше переобучений и так далее. Хотя по факту, конечно,
[59:28.960 --> 59:40.760]  у вас больше информации не становится. Да не то чтобы. Но смотрите, дерево по сравнению с каким-нибудь
[59:40.760 --> 59:46.440]  ядром для свм-а считать гораздо дешевле, поэтому 10 раз вы посчитаете на 10 процессоров. Возьмите и
[59:46.440 --> 59:52.320]  посчитайте. Ну вот это, кстати, мы с вами можем посмотреть, в том числе в домашке.
[59:52.320 --> 01:00:00.560]  Не, смотрите, обычно деревья строят там десятки, сотни, режут тысячи, больше не строят обычно.
[01:00:00.560 --> 01:00:12.440]  Да, рандом. А, не, погодите, это бутстрап. Во, все. Не, все. Лес так и строится. Смотрите,
[01:00:12.440 --> 01:00:16.600]  я ж совсем забыл. Просто на бутстрапированных выборках-то у нас деревья все равно будут
[01:00:16.600 --> 01:00:20.480]  сильно похожи с какого-то момента, потому что у нас все равно в среднем распределение сохраняется.
[01:00:20.480 --> 01:00:25.440]  Я ж не договорил. Собственно, вопрос, как нам сделать бутстрап, точнее беггинг над деревьями,
[01:00:25.440 --> 01:00:30.520]  более эффективным, а нам надо сделать деревья более непохожими? Давайте, мы каждый раз, по сути,
[01:00:30.520 --> 01:00:35.600]  выбираем случайное множество объектов. Ну вот, бутстрапируем выборку, на нем учим. Что мы еще
[01:00:35.600 --> 01:01:02.080]  можем сделать случайно? Давайте. Смотрите, что произошло. Это средняя ошибка для десяти
[01:01:02.080 --> 01:01:07.600]  моделей различных. Здесь мы взяли модель, которая усредняет предыдущие десять предсказания,
[01:01:07.600 --> 01:01:14.280]  то есть усредняем, поэтому оно на стуре и сидит. Ага. Ну так чего, ребят, что можно еще случайно
[01:01:14.280 --> 01:01:24.140]  выбирать? Гиперпараметры сложно. Что еще? Границу можно разделять, да, а можно еще признаки выбирать.
[01:01:24.140 --> 01:01:30.560]  Короче, люди сидели-сидели и подумали, а давайте-ка введем метод RSM, random subspace method,
[01:01:30.560 --> 01:01:34.480]  метод случайных подпространств. Давайте мы каждый раз будем случайным образом выбирать
[01:01:34.480 --> 01:01:38.720]  под множество признаков, именно, что случайно у вас склонны 50 признаков, из них 25 случайных
[01:01:38.720 --> 01:01:44.800]  выбрали, и только над ними строите дерево. Тут два варианта. Либо вы выбираете в самом начале для
[01:01:44.800 --> 01:01:49.280]  всего дерева целиком, либо вы его выбираете в каждой вершине вообще по отдельности. То есть
[01:01:49.280 --> 01:01:54.080]  каждой вершине вы говорите, выбери оптимальный признак из пятого, седьмого, десятого и двадцать
[01:01:54.080 --> 01:01:59.520]  пятого, и все. Тем самым деревья у вас становится, с одной стороны, вроде как слабее, но с другой
[01:01:59.520 --> 01:02:04.280]  стороны, мы их глубину не ограничиваем, поэтому они все равно хорошо подстроятся. А во-вторых,
[01:02:04.280 --> 01:02:08.240]  у нас с вами деревья гораздо менее похожи друг на друга, потому что они теперь на разных признаках
[01:02:08.240 --> 01:02:14.560]  вообще построены. И, собственно, когда люди взяли и объединили между собой random subspace method,
[01:02:14.560 --> 01:02:19.760]  метод случайных подпространств и bagging, родился random porost. По праву, один из наиболее, наверное,
[01:02:19.760 --> 01:02:24.840]  универсальных и удобных алгоритмов в машинном обучении, в частности для табличных данных.
[01:02:24.840 --> 01:02:31.560]  Bagging – это bootstrap aggregating, то есть вы построили n bootstrap выборок,
[01:02:31.560 --> 01:02:38.800]  обучили на них модели, усреднили предсказания. Нет, bootstrap – это метод генерации выборки,
[01:02:38.800 --> 01:02:45.440]  а bagging – это, по сути, механизм машинного обучения, как из этих самых выборок получить модели.
[01:02:45.440 --> 01:02:49.760]  Просто в статах вы можете по bootstrap выборкам всякие статистики считать и так далее.
[01:02:49.760 --> 01:02:59.880]  У вас каждая модель предсказывает вектор вероятности? Именно поэтому я говорил вначале,
[01:02:59.880 --> 01:03:04.400]  всегда, пожалуйста, начинайте думать векторами вероятности. Одни метки классов, а не вас. Всё,
[01:03:04.400 --> 01:03:10.880]  это как бы детский сад. Вектор вероятности можно совершенно спокойно средний. Ну и, соответственно,
[01:03:10.880 --> 01:03:15.960]  получается, что random forest работает достаточно хорошо, он переобучается, но очень паршиво
[01:03:15.960 --> 01:03:23.520]  переобучается на фоне остальных моделей. Ну, собственно, random forest – это что? Мы берем и,
[01:03:23.520 --> 01:03:27.720]  скажем так, я скажу, наверное, каноничную версию, но мое мнение может отличаться от других. То,
[01:03:27.720 --> 01:03:31.000]  что я сказал, два варианта. Либо на каждое дерево отдельная подножица признаков,
[01:03:31.000 --> 01:03:35.200]  либо в каждой вершине вообще. Он в каждой вершине выбирает случайное подножицу признаков,
[01:03:35.200 --> 01:03:42.280]  и вот на bootstrap выбирает оптимальный threshold. То есть у вас деревья теперь случайные и по
[01:03:42.280 --> 01:03:46.880]  подвыборкам и по признакам. За счет этого они меньше похожи друг на друга, за счет этого у вас
[01:03:46.880 --> 01:03:51.800]  достигается большее падение вот здесь. И плюс здесь мы перестаем бояться переобучения, в каком
[01:03:51.800 --> 01:03:55.200]  смысле, то, что да, пожалуйста, пускай переобучается, мы потом деревью усредним,
[01:03:55.200 --> 01:04:01.520]  переобучение исчезнет. Ну, точнее, его эффект снижется в n раз, где n число деревьев. Есть,
[01:04:01.520 --> 01:04:05.960]  конечно, но если вы начнете слишком много деревьев строить, например, у вас там выборка размером
[01:04:05.960 --> 01:04:10.080]  тысячи и вы 500 деревьев построили, то вы заметите, что у вас куча деревьев всё равно похожа друг на
[01:04:10.080 --> 01:04:14.240]  другу. Потому что у вас случайно постоянно выбираются похожие траектории и так далее.
[01:04:14.240 --> 01:04:19.400]  Поэтому random forest переобучить можно, надо сильно постараться. Это не панацея, но именно поэтому
[01:04:19.400 --> 01:04:24.920]  он является классным деполтным алгоритмом. Он агрегирует на собой деревья, а поэтому он что? Он
[01:04:24.920 --> 01:04:29.880]  строит нелинейную разделяющую поверхность или нелинейную зависимость. Во-вторых, он достаточно
[01:04:29.880 --> 01:04:35.440]  устойчив к переобучению. И третье, я думаю, кто-то из вас уже заметил, давайте я на это дело на
[01:04:35.440 --> 01:04:40.160]  всякий случай обращу внимание. Тут пока всякие классные штуки. А как вы думаете, как деревья
[01:04:40.160 --> 01:04:44.720]  работают со скоррелированными признаками? Если у нас сильная корреляция между признаками
[01:04:44.720 --> 01:04:54.560]  деревьев хуже или лучше? Ну, лучше вряд ли, а если сильная корреляция? Не хуже. Бинго! У нас же
[01:04:54.560 --> 01:05:00.120]  дерево при построении смотрит на каждый признак по отдельности, правильно? Поэтому дереву абсолютно
[01:05:00.120 --> 01:05:04.240]  всё равно на то, что они там скоррелированы. Максимум, что можно сделать, если вы просто вот в худший
[01:05:04.240 --> 01:05:09.600]  случай, у вас два признака, а потом вы берёте вторую и сто раз его дублируете. Дерево просто
[01:05:09.600 --> 01:05:14.400]  будет медленнее считаться, потому что ему просто будет дольше это всё считать. Всё. В остальном ему по
[01:05:14.400 --> 01:05:22.960]  барабану, оно не сломается. Никаких огромных эффициентов не появится и так далее. В смысле порядок.
[01:05:22.960 --> 01:05:33.080]  Вообще неважно. Если у вас у Н признаков падение одинаковое, ну по-хорошему выбирайте случайный,
[01:05:33.080 --> 01:05:44.600]  вот и всё. Не гарантируем. В среднем просто. Мы условно в среднем выбираем какое-то подможество
[01:05:44.600 --> 01:05:49.120]  признаков. Как правило, если условно в каждой вершине мы выбираем условно одну-вторую признаков,
[01:05:49.120 --> 01:05:54.120]  то скорее всего мы должны это сделать там N раз, чтобы покрыть все. Да?
[01:05:54.120 --> 01:06:11.280]  Не-не-не-не-не. Погодите. Мы ещё раз. Дерево строится как? Множество признаков, перебираются все
[01:06:11.280 --> 01:06:15.720]  возможные признаки, для каждого признака все возможные трешхолды. Мы всегда для одного признака
[01:06:15.720 --> 01:06:20.360]  в ноде что-то выбираем. А какой признак, какой трешхолд мы выбираем уже одном образом. Теперь мы
[01:06:20.360 --> 01:06:24.840]  добавили две случайности. Первая, у нас выборка буддстрапированная, то есть не исходная выборка,
[01:06:24.840 --> 01:06:29.680]  а та, которую мы получили методом буддстрапа. Вторая, у вас случайное подможество признаков,
[01:06:29.680 --> 01:06:38.360]  а не все, которые были в каждой ноде. Вы в каждой ноде ставите трешхолд на один признак и на N?
[01:06:38.360 --> 01:06:42.960]  На один признак, но выбираете вы оптимальные не из всего множества, а из случайного подможества.
[01:06:42.960 --> 01:06:57.840]  Это гиперпараметр. То есть количество, размер подможества вы сами выбираете условно. В каждой
[01:06:57.840 --> 01:07:01.560]  ноде выбирай там пять признаков, из них выбирай оптимальный. Это гиперпараметр ваш.
[01:07:01.560 --> 01:07:08.700]  Бинго, не случайно выбираю признак, а случайно выбираю подможество, из которого будет выбрано
[01:07:08.700 --> 01:07:12.440]  оптимальный. В пределе вы можете выбирать один случайный признак, по нему тогда просто трешхолд
[01:07:12.680 --> 01:07:18.920]  На самом деле такое тоже есть. Называется extremely randomized trees.
[01:07:18.920 --> 01:07:23.360]  Короче, ребят, смотрите, сейчас у нас еще минут десять лекции будет. Пожалуйста, найдите себе
[01:07:23.360 --> 01:07:28.000]  силы. Я понимаю, всем очень хочется написать контрольную и пойти домой. Дайте, пожалуйста,
[01:07:28.000 --> 01:07:33.320]  мы закончим. Не смотрите на то, что у нас звонок звенит. Еще у нас немножко время есть. Итак,
[01:07:33.320 --> 01:07:44.880]  смотрите. Во-первых, плюсов деревьев на самом деле много. То есть, а они у нас работают достаточно
[01:07:44.880 --> 01:07:51.480]  хорошо на нелинейных зависимости. Они работают, когда у нас есть коррелированные признаки. Если у
[01:07:51.480 --> 01:07:58.160]  нас признаки в разных шкалах, что будет с деревом? Дерево вообще по барабану, дерево их все
[01:07:58.160 --> 01:08:03.680]  раздельно рассматривать по отдельности. Все наши проблемы, которые были раньше, пока что решаются.
[01:08:03.680 --> 01:08:11.840]  Четвертое. Если у нас пропуски в данных, что делать с деревом? Ну, в смысле, у вас бывает данные,
[01:08:11.840 --> 01:08:15.840]  вот допустим, я не знаю, у вас датчик вышел из строя в какой-то момент, молния рядом ударила,
[01:08:15.840 --> 01:08:22.680]  и там нету данных на этот момент. Ничего и не известно. Линейная модель сломается, потому что
[01:08:22.680 --> 01:08:27.200]  линейная модель должна вместо нан умножить на что-то. Если умножить нан на вес, вы получите
[01:08:27.200 --> 01:08:32.480]  все равно нан. Нота number. Что с деревом? Сейчас я вернусь, у меня даже специальный слайд для этого есть.
[01:08:32.480 --> 01:08:37.480]  Вот, пожалуйста. Тут у нас попал объект, который попал признак, который неизвестен для нашего
[01:08:37.480 --> 01:08:43.200]  объекта. На inference, например. Что мы делаем? Мы можем спустить объект по левому поддереву и по
[01:08:43.200 --> 01:08:49.040]  правому поддереву, и мы же знаем вес левого и правого поддерева на обучение. Сколько объектов
[01:08:49.040 --> 01:08:54.880]  пошло туда, сколько сюда. С этими весами мы можем усреднить предсказание. Вот все. Дерево еще и
[01:08:54.880 --> 01:09:00.840]  устойчиво к пропускам данных в разумных пределах. Получается корреляция все равно разной шкалы,
[01:09:00.840 --> 01:09:05.480]  все равно пропуски в данных до определенного предела, все равно переобучается, но можно это
[01:09:05.480 --> 01:09:10.040]  побить тем, что мы их посредняем. Плошная радость. Собственно, ровно поэтому на самом деле деревья и
[01:09:10.040 --> 01:09:14.680]  во многом поднились во весь рост, особенно с приходом градиентного бустинга, который мы с
[01:09:14.680 --> 01:09:19.920]  вами разберем на следующих занятиях, который в 2001 году был Фридманом представлен, и градиентный
[01:09:19.920 --> 01:09:25.920]  бустинг немного-немало привел к тому, что, как его называют, раз-два-три-четыре-пять, не только
[01:09:25.920 --> 01:09:31.960]  своем умер, но еще и нейронные сети того времени, по сути, обратно ушли отдохнуть. Нейронные сети в
[01:09:31.960 --> 01:09:36.600]  конце девяностых были очень популярны, в 2001-м выходит градиентный бустинг, все говорят, блин,
[01:09:36.600 --> 01:09:42.120]  оно работает быстрее, эффективнее, вообще хорошо. Зачем нам ваши нейронки? Так что умение пользоваться
[01:09:42.120 --> 01:09:47.600]  деревьями сейчас вам позволит делать что? Вот на табличных данных сейчас уже нейронки есть хорошие,
[01:09:47.600 --> 01:09:52.000]  которые даже лучше деревьев работают. Но по простоте использования и по скорости, а еще
[01:09:52.000 --> 01:09:56.920]  интерпретируемость, например, для деревьев и для ансамблей деревьев есть замечательный теоретически
[01:09:56.920 --> 01:10:03.120]  обоснованный метод оценки значимости признаков SHAP, Shapely Editive Values, это работа, лучшая работа года
[01:10:03.120 --> 01:10:09.440]  по версии НИПСа 2017 года, по-моему, возможно 18-го, могу ошибаться. Короче, деревья ваши друзья,
[01:10:09.440 --> 01:10:14.720]  и более того, почти на любой задаче вы можете их в качестве бейслайна использовать. Ваша копилка
[01:10:14.720 --> 01:10:19.840]  бейслайнов пополнилась. Собственно, если данных мало, признаков объектов это КНН, если это
[01:10:19.840 --> 01:10:23.680]  классикация, можно добавить наивный базовый классикатор. Главное, правильно приорное распределение
[01:10:23.680 --> 01:10:27.760]  выбирать. Линейно-логистическая регрессия всегда влет, может какие-нибудь там признаки еще
[01:10:27.760 --> 01:10:32.480]  погенерить. Дерево туда же добавляется, если вот данные нелинейные, то у вас будет явно видно,
[01:10:32.480 --> 01:10:37.280]  что логистическая регрессия и линейная регрессия сильно уступают рандомпоросту, потому что он
[01:10:37.280 --> 01:10:42.880]  может нелинейные границы описывать, а соответственно линейные модели не могут. Это вам звоночек. Собственно,
[01:10:42.880 --> 01:10:48.640]  и последнее. У меня это еще не все про рандомпорост. Он вообще как бы обалдел в своих плюсах.
[01:10:48.640 --> 01:10:54.960]  Во-первых, он наиболее универсальный, у него есть куча различных доработок. Вот про extremely
[01:10:54.960 --> 01:10:59.480]  randomized forest я вам говорил. Также есть классная версия isolation forest, про него как-нибудь в следующий
[01:10:59.480 --> 01:11:05.560]  раз, если захотите, можно поговорить. Если коротко, random forest может использоваться как фильтр аномалий.
[01:11:05.560 --> 01:11:10.080]  Вот у вас в данных бывают какие-то аномалии, то есть объекты, которые обладают именно каким-то
[01:11:10.080 --> 01:11:14.680]  очень странным признаковым описанием. Вот у вас распределение ваших данных и вот тут торчит какая-то
[01:11:14.680 --> 01:11:21.320]  точка. Как это можно сделать? Смотрите, мы с вами можем строить дерево, но преследовать другую цель,
[01:11:21.320 --> 01:11:27.440]  не минимизировать какую-то энтропию или еще что-то. Мы можем пытаться на каждом шаге отделить один
[01:11:27.440 --> 01:11:33.600]  объект. Хорошо? Тогда что получается? Мы на каждом шаге отделяем один объект. Те объекты, которые с
[01:11:33.600 --> 01:11:38.960]  краю, их просто проще отделить будет. Поэтому когда дерево строится, оно будет сначала отделять тех,
[01:11:38.960 --> 01:11:44.400]  кто с краю, а потом тех, кто ближе к центру, потому что там надо больше границ провести. Вы опять же можете
[01:11:44.400 --> 01:11:48.920]  также построить n случайных деревьев, например, на буддстрапированных выборках и на случайных
[01:11:48.920 --> 01:11:53.760]  признаках под множество, и посмотреть на каждый объект. Допустим, можете дерево строить до глубины
[01:11:53.760 --> 01:11:58.440]  k, если вас там долго, и можете посмотреть на все объекты, которые до определенной глубины были
[01:11:58.440 --> 01:12:03.000]  отделены в среднем. Значит, они находятся где-то с краев от вашего распределения, значит, скорее
[01:12:03.000 --> 01:12:08.680]  всего это аномалия. Короче, деревья – плашная красота. И последняя, но тем не менее, ребят, вот
[01:12:08.680 --> 01:12:14.040]  все, последняя формула, и я вас включу на перерыв, потом будет контрольная. Смотрите, out of back оценка.
[01:12:14.040 --> 01:12:21.400]  Дерево, обученное на буддстрапированных выборках, позволяет вам получить оценку на отложенных
[01:12:21.400 --> 01:12:28.280]  данных, не имея валидационной выборки. Бабаху. Что происходит? Мы же с вами когда буддстрапировали,
[01:12:28.640 --> 01:12:31.480]  как fostered, ну наш же каждый объект попадал в буддстрапированную выборку случайно,NONindBROW,
[01:12:31.520 --> 01:12:37.080]  правильно? Значит, для каждой буддстрапированной выборки есть под множество объектов,
[01:12:37.080 --> 01:12:40.440]  которые в нее не попали из исходной. Да хватит звенеть, мы поняли.
[01:12:40.440 --> 01:12:47.620]  интересно. Получается, для каждого объекта, обучающего выборки, мы можем найти под множество
[01:12:47.620 --> 01:12:52.040]  моделей, которые этот объект не видели на обучении, правильно, потому что не обучено
[01:12:52.040 --> 01:12:56.720]  на соответственность corespot. Но тогда мы можем взять и для каждого объекта получить предсказания
[01:12:56.720 --> 01:13:03.380]  только в модели, которую вы не видели. Более того, мы тогда берем не все n-моделей, а лишь k из этих
[01:13:03.380 --> 01:13:08.420]  моделей, их предсказания усредняем, и тогда это получается оценка сверху или снизу на нашу ошибку?
[01:13:08.420 --> 01:13:14.120]  Сверху, потому что у нас меньше ансамбль, соответственно, мы меньше снижаем ошибку,
[01:13:14.120 --> 01:13:18.840]  соответственно, у нас получается оценка сверху на нашу ошибку на отложенных данных, при этом нам
[01:13:18.840 --> 01:13:23.600]  не надо валидацию отрубать, а валидация там 10-20-30 процентов данных, вообще говоря, это дорого.
[01:13:23.600 --> 01:13:28.480]  Короче, деревья ваши друзья, и те, которые на улице кислород дают, как водоросли, и те,
[01:13:28.480 --> 01:13:33.600]  которые здесь. Ну и последняя, наверное, красивая картинка, это уже вам так, подумайте, смотрите,
[01:13:33.600 --> 01:13:40.000]  слева развиляющая поверхность для рандомпороста, справа для кнн. Вам не кажется, что они похожи?
[01:13:40.000 --> 01:13:47.840]  Ну, немножко похожи. По факту, у вас деревья в некотором смысле могут быть приинтерпретированы,
[01:13:47.840 --> 01:13:51.760]  как некоторая аппроксимация поведения кнн, потому что в пределе у вас что? У вас каждый объект
[01:13:51.760 --> 01:13:56.720]  можно обвести своим квадратиком и предсказать ему нужную константу. То же самое делает метод
[01:13:56.720 --> 01:14:01.360]  одного ближайшего соседа в какой-то норме. Но при этом дерево, в отличие от кнн, не будет
[01:14:01.360 --> 01:14:05.040]  страдать, если у вас огромное признаковое пространство и огромное количество объектов,
[01:14:05.040 --> 01:14:13.080]  потому что оно не квадратично по сложности, в отличие от кнн. Но если дерево переобучено,
[01:14:13.080 --> 01:14:23.160]  у кнн, если дерево переобучилось, то ему тоже грустно, надо тогда ансамбль брать,
[01:14:23.160 --> 01:14:29.960]  тогда у него все равно будет трейнер не ноль. У леса не будет нулевая трейнерная ошибка,
[01:14:29.960 --> 01:14:38.000]  если будет, значит, лес вы тоже смогли переобучить. Ну хорошо, значит, дерево переобучили. Ну да,
[01:14:38.040 --> 01:14:43.200]  здесь дерево переобучено и не очень хорошо. Окей. Ну, собственно, вот, что мы с вами сегодня разобрали.
[01:14:43.200 --> 01:14:47.280]  Я вроде выделил все основные моменты. Мы с вами про деревья будем говорить еще минимум два занятия,
[01:14:47.280 --> 01:14:54.800]  но деревья, пожалуйста, не списывайте со счетов, это классная вещь. Про категориальные фичи я
[01:14:54.800 --> 01:14:59.520]  сказал буквально два слова. Категориальные фичи можно бинаризовать и с ними работать. В некоторых
[01:14:59.520 --> 01:15:03.840]  случаях можно категориальные фичи использовать просто для биения. Кадбус, например, так умеет,
[01:15:03.840 --> 01:15:05.080]  но там ничего особо не меняется.

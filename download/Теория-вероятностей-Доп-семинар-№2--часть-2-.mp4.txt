[00:00.000 --> 00:16.160]  Гауссовские вектора.
[00:16.160 --> 00:19.440]  Смотрите, нормальная случайная величина в одномерном случае
[00:19.440 --> 00:21.880]  — это очень важное распределение на самом деле, потому что
[00:21.880 --> 00:26.520]  оно в CPT встречается, ну а CPT — это очень часто используемый
[00:26.520 --> 00:27.520]  объект.
[00:27.520 --> 00:33.080]  И в целом оно такое хорошее, гауссовское, и захотелось
[00:33.080 --> 00:37.520]  его каким-то образом обобщить на многомерный случай.
[00:37.520 --> 00:38.520]  Вот.
[00:38.520 --> 00:40.920]  Ну и сейчас мы, наверное, будем какую-то аналогию
[00:40.920 --> 00:44.240]  между одномерным и многомерным случаем проводить.
[00:44.240 --> 00:55.200]  То есть случайные величины и вот случайные вектора
[00:55.200 --> 00:56.200]  гауссовские.
[00:56.200 --> 01:07.200]  Значит, в одномерном случае у нас просто нормальная
[01:07.200 --> 01:12.200]  случайная величина параметризовалась средним и дисперсией.
[01:12.200 --> 01:13.200]  Таким вот образом.
[01:13.200 --> 01:17.800]  Мы с вами в одномерном случае нормальную случайную
[01:17.800 --> 01:19.720]  величину определяли как случайную величину с
[01:19.720 --> 01:20.720]  такой вот плотностью.
[01:20.720 --> 01:34.560]  Но у такого определения была небольшая проблема.
[01:34.560 --> 01:37.200]  Ведь константы — это тоже, по сути, нормальная случайная
[01:37.200 --> 01:42.280]  величина просто с нулевой дисперсией, а плотность
[01:42.280 --> 01:43.960]  вот уже существовать не будет, потому что на ноль
[01:43.960 --> 01:44.960]  делить плохо.
[01:44.960 --> 01:48.480]  Собственно, поэтому мы придумали другое определение
[01:48.480 --> 01:50.440]  для нормальных случайных величин в одномерном случае
[01:50.440 --> 01:52.280]  через характеристическую функцию.
[01:52.280 --> 01:55.120]  То есть мы можем называть случайную величину нормальной,
[01:55.120 --> 01:59.680]  если у нее характеристическая функция имеет вот тот вид,
[01:59.680 --> 02:01.720]  который мы с вами посчитали в начале семинара.
[02:01.720 --> 02:06.840]  Вот такое.
[02:06.840 --> 02:09.120]  И вот такое определение конвенционально, потому что
[02:09.120 --> 02:14.480]  если у вас дисперсия нулевая, то у вас остается вот, получается,
[02:14.480 --> 02:17.800]  Е в степени ИАТ, а это и есть просто характеристическая
[02:18.120 --> 02:19.120]  функция константы.
[02:19.120 --> 02:20.720]  То есть вот определение через характеристическую
[02:20.720 --> 02:25.600]  функцию согласуется с константным случаем, то есть со случаем
[02:25.600 --> 02:26.600]  нулевой дисперсии.
[02:26.600 --> 02:27.600]  Вот.
[02:27.600 --> 02:33.000]  А давайте попробуем понять, чем вот случай с константой
[02:33.000 --> 02:34.000]  он как бы отличается.
[02:34.000 --> 02:36.000]  Ну, по сути, он отличается тем, что у вас просто все
[02:36.000 --> 02:38.280]  распределение теперь находится на множестве меры ноль,
[02:38.280 --> 02:39.280]  то есть в одной точке.
[02:39.280 --> 02:41.280]  У одной точки мера ноль.
[02:41.280 --> 02:47.160]  А когда вы рассматриваете уже какой-нибудь многомерный
[02:47.720 --> 02:51.960]  у вас уже нульмерных подпространств очень много.
[02:51.960 --> 02:55.200]  Допустим, если вы рассматриваете плоскость, то вот такая
[02:55.200 --> 02:57.880]  прямая, это тоже нульмерное пространство, но в ней,
[02:57.880 --> 02:59.720]  по идее, какой-то гауссовский одномерный вектор может
[02:59.720 --> 03:00.720]  быть определён.
[03:00.720 --> 03:02.960]  Поэтому какое-то определение через плотность также бы
[03:02.960 --> 03:04.800]  столкнуть с проблемами, и даже этих проблем было бы
[03:04.800 --> 03:05.800]  больше.
[03:05.800 --> 03:07.800]  Потому что здесь, по сути, нас только константы напрягало,
[03:07.800 --> 03:12.760]  а здесь нас уже будут напрягать все нульмерные, ну, получается
[03:12.760 --> 03:14.800]  все гауссовские вектора, которые определены на
[03:14.800 --> 03:17.120]  множестве нулевой меры в нашем пространстве.
[03:17.200 --> 03:19.080]  Поэтому нужно какое-то более вот конвенциональное
[03:19.080 --> 03:20.080]  и общее определение.
[03:20.080 --> 03:23.280]  И вот если мы в одномерном случае определили в итоге
[03:23.280 --> 03:27.080]  хорошо нормальную случайную величину через характеристическую
[03:27.080 --> 03:32.720]  функцию, в многомерном случае сразу сказали гауссовский
[03:32.720 --> 03:33.720]  вектор.
[03:33.720 --> 03:37.000]  Это такой вектор, у которого характеристическая функция
[03:37.000 --> 03:38.000]  имеет следующий вид.
[03:38.000 --> 03:42.320]  Здесь давайте небольшое замечание, мы до этого только
[03:42.320 --> 03:44.760]  характеристические функции случайных величин рассматривали,
[03:44.760 --> 03:46.840]  а теперь у нас ещё появляются характеристические функции
[03:46.840 --> 03:49.840]  случайных векторов, определяются они вот таким вот образом.
[03:49.840 --> 03:54.400]  Вот, и гауссовский вектор, это такой вектор, у которого
[03:54.400 --> 03:57.400]  характеристическая функция имеет вот такой вид.
[04:13.400 --> 04:16.400]  Вот это, это скалярное произведение, я так пишу.
[04:16.960 --> 04:21.960]  Это, это вот здесь?
[04:21.960 --> 04:25.960]  Это, ну это просто типа е.
[04:25.960 --> 04:28.960]  Ну это, типа жирная буква.
[04:28.960 --> 04:30.960]  Вот.
[04:30.960 --> 04:34.960]  И вот, на самом деле, каноничное определение, гауссовский
[04:34.960 --> 04:37.960]  вектор, это такой вектор, у которого характеристическая
[04:37.960 --> 04:38.960]  функция вот такая.
[04:38.960 --> 04:45.960]  Теперь давайте подумаем, что здесь ещё написано.
[04:46.520 --> 04:48.520]  В одномерном случае мы с вами определяли случайную
[04:48.520 --> 04:51.520]  величину, по сути, вот над ожиданием и дисперсией.
[04:51.520 --> 04:54.520]  В многомерном случае примерно всё аналогично задаётся,
[04:54.520 --> 04:57.520]  но теперь у вас вектор средних и матрица ковариаций.
[05:01.520 --> 05:03.520]  Ну, для чего нам вектор средних?
[05:03.520 --> 05:05.520]  То есть, по сути, смотрите, в одномерном случае у вас
[05:05.520 --> 05:06.520]  плотность выглядела вот так.
[05:07.080 --> 05:10.080]  Вот у вас получается центр распределения, а дисперсия
[05:10.080 --> 05:12.080]  определяла, насколько у вас как бы разброс большой
[05:12.080 --> 05:13.080]  относительно этого центра.
[05:13.080 --> 05:16.080]  А когда вы в многомерном случае, вот теперь мы на
[05:16.080 --> 05:19.080]  плоскость сверху смотрим, вам нужно задать точку
[05:19.080 --> 05:23.080]  центра опять вашего распределения, поэтому теперь у вас
[05:23.080 --> 05:26.080]  мотожидание, это как бы вектор мотожиданий, то есть,
[05:26.080 --> 05:28.080]  какая-то точка, по сути, какой-то вектор.
[05:28.080 --> 05:29.080]  Вот.
[05:29.080 --> 05:32.080]  А дисперсия, в общем-то, это как бы вектор мотожиданий,
[05:32.640 --> 05:33.640]  то есть, какая-то точка, по сути, какой-то вектор.
[05:35.260 --> 05:36.020]  Вот.
[05:36.020 --> 05:38.420]  А дисперсия у нас поменялась на матрицу ковариаций.
[05:38.420 --> 05:41.420]  Матрица ковариаций – это такая матрица квадратная,
[05:42.420 --> 05:44.420]  в которой на и懂 житом месте стоит ковариация
[05:44.420 --> 05:46.420]  и, и жите компоненты вектора.
[05:46.420 --> 05:48.420]  Получается, что на диагональ будет стоять дисперсии,
[05:48.420 --> 05:51.260]  потому что ковариация и, и, ну, типа
[05:51.260 --> 05:53.260]  и, и компоненты, и, и и компоненты – это просто
[05:53.260 --> 05:54.260]  дисперсии и, и компоненты.
[05:54.260 --> 05:55.260]  Вот.
[05:55.260 --> 05:58.260]  Ну и по сути, вот эти 2 штуки также как и в одномерном
[05:58.260 --> 06:01.260]  случае, вот эти 2 задают однозначно гауссовский
[06:01.260 --> 06:08.300]  Ну, то есть, по сути, вектор А говорит, где у вас центр колокола будет, а матрица
[06:08.300 --> 06:12.460]  кавариации говорит, ну, будет ли у вас этот колокол, допустим, вдоль какой-то компоненты вытянуть.
[06:12.460 --> 06:17.380]  То есть, если у вас почти линейная зависимость, у вас как бы колокол будет вот такой вот приплюснутый
[06:17.380 --> 06:25.260]  и вытянутый вот так вдоль вот этой оси. Понятно? Это вид сверху, если что. Вот, супер.
[06:25.260 --> 06:31.140]  Что можно сказать про вектор средних? Ну, вектор средних — это просто произвольный
[06:31.140 --> 06:37.340]  вектор из пространства Rn. Ну, давайте теперь у нас гауссовский вектор.
[06:37.340 --> 06:44.660]  Си принадлежит Rn. Ну, в отмерном пространстве. Что такое в матрице кавариации? Ну, мы уже более-менее
[06:44.660 --> 06:52.620]  определили. Это такая матричка, у которой на item-житом месте стоит кавариация кси и кси ж.
[06:52.620 --> 07:03.420]  Вот. Ну, более-менее какое-то понятное определение. Что можно сказать про вектор средних? Ну, вот он
[07:03.420 --> 07:08.020]  любой, а матрица кавариации — не любая. Она, понятно, что должна быть симметричная в силу того,
[07:08.020 --> 07:11.580]  что у вас кавариация симметрична, но еще также она должна быть неотрицательно определена.
[07:11.580 --> 07:20.260]  Неотрицательно определена — это значит, что для любого вектора х, х-транспонированная σх,
[07:20.260 --> 07:29.060]  у вас больше либо равно 0. Такая штука. Ну, как это доказать? Вот это можно расписать как сумму.
[07:29.060 --> 07:41.140]  У вас будет по сути сумма х и х-ж кавариация кси и кси ж. Вот. А дальше вы суммируете сначала
[07:41.140 --> 07:46.020]  по первой компоненте, потом по второй. То есть пользуетесь просто белинейностью. Так, надо
[07:46.100 --> 07:59.140]  что-то стереть. Это не так важно, поэтому я не на новой доске пишу. Вот. Ну, вот это можно по
[07:59.140 --> 08:09.900]  белинейности собрать вот в такую штуку. То есть это будет просто кавариация суммы кси и ксы кси и сумма
[08:09.900 --> 08:18.700]  х-ж кси ж. Ну, а это по сути дисперсия. Ну, а дисперсия больше либо равно 0. Ну, все,
[08:18.700 --> 08:26.620]  мы с вами доказали, что вот эта сигма — неотрицательно определенная матричка. Вот. То есть
[08:26.620 --> 08:31.740]  в одномерном случае у нас были какие-то похожие ограничения, то есть у нас среднее могло быть
[08:31.740 --> 08:37.900]  любым, а дисперсия должна была быть больше 0. Ну, больше либо равно 0. Так и вот в многомерном
[08:37.900 --> 08:41.580]  случае у нас вот этот вектор любой, а вот эта матричка должна быть неотрицательно
[08:41.580 --> 08:45.100]  определенная. То есть тоже какое-то обобщение одномерного случая на многомерный случай.
[08:45.100 --> 09:01.780]  Окей. Так, это я сотру. Не так интересно. Дальше. Задали мы, значит, с вами через х функции,
[09:01.780 --> 09:08.620]  поняли, что такое H, что такое сигма. У гауссовского вектора на самом деле есть
[09:08.620 --> 09:14.540]  и эквивалентное определение, потому что вот такое определение через характеристическую функцию на
[09:14.540 --> 09:20.380]  самом деле мало каких-то полезных свойств может нам дать о гауссовском векторе. Поэтому у гауссовского
[09:20.380 --> 09:47.900]  вектора есть еще два эквивалентных определения. Первое, значит, смотрите, самое главное определение
[09:48.060 --> 09:51.900]  вот такое. То есть вектор называется гауссовским, если его характеристическая функция вот такая.
[09:51.900 --> 09:59.500]  Соответственно, эквивалентное определение, давайте, эквивалентное определение два. Получается,
[09:59.500 --> 10:21.140]  если с какими-то параметрами. То есть линейная комбинация компонент гауссовского вектора,
[10:21.140 --> 10:25.220]  вот здесь мы взяли скалярное произведение, то есть по сути мы с вами взяли просто линейную
[10:25.220 --> 10:30.260]  комбинацию компонентов вот этого вектора с какими-то константами, с какими-то коэффициентами.
[10:30.260 --> 10:36.260]  А если вот любая линейная комбинация имеет нормальное распределение, ну вот здесь константа
[10:36.260 --> 10:47.460]  тоже считается нормальным распределением. И второе эквивалентное определение через
[10:47.460 --> 10:53.060]  преобразование стандартных независимых нормальных случайных величин. Сейчас мы это аккуратно
[10:53.060 --> 11:07.460]  запишем. То есть, если существует матричка А, размера получается м на м, и вектор В,
[11:07.460 --> 11:24.260]  который просто будет из РН такой, что кси представима вот в таком вот виде. Вот. Где тета,
[11:24.260 --> 11:47.380]  тета М. Независимая, так здесь надо стереть, иначе видно не будет. Независимая стандартная
[11:47.380 --> 11:55.820]  случайная величина нормальная. То есть, как бы это какое-то линейное преобразование независимых
[11:55.820 --> 12:00.620]  стандартных нормальных случайных величин. То есть, еще раз, гауссский вектор это такой вектор,
[12:00.620 --> 12:06.100]  у которого характеристическая функция вот такая первое определение. Второе, любая линейная комбинация
[12:06.100 --> 12:12.300]  его компонент имеет нормальное распределение. И третье определение, что вот ваш вектор на самом
[12:12.300 --> 12:16.940]  деле был получен путем того, что вот взяли независимые нормальные случайные величины
[12:16.940 --> 12:25.820]  стандартные, домножили на матричку какую-то слева и добавили какой-то вектор констант. Вот. Тогда
[12:25.820 --> 12:35.180]  это тоже тогда ксигаусский вектор. Три эквивалентных определения. Супер. Продолжаем аналогию какую-то.
[12:35.180 --> 12:41.380]  Вот здесь у нас появилось линейное преобразование какого-то вида, и хочется продолжить аналогию
[12:41.380 --> 12:49.980]  с одномерным случаем. В одномерном случае, что мы с вами имели? В одномерном случае у нас было
[12:49.980 --> 13:00.700]  следующее. Так, только здесь, наверное, писать будет неудобно. Ну ладно. Нет. Не пошло. Вот в одномерном
[13:00.700 --> 13:09.500]  случае, если кси имела нормальное распределение с параметрами асимма квадрат, то альфа кси плюс
[13:09.500 --> 13:18.860]  бета будет иметь нормальное распределение с параметрами альфа а плюс бета, запятая альфа квадрат
[13:18.860 --> 13:27.900]  сигма квадрат. Вот. Ну вот. Это утверждение вызвало какие-то вопросы, но вот мы его можем... Мы сейчас
[13:27.900 --> 13:31.780]  докажем в многомерном случае, а это просто частный случай одномерного. Одномерный просто частный
[13:31.780 --> 13:40.980]  случай многомерного будет. И хочется понять, если у нас был теперь гауссовский вектор с
[13:40.980 --> 13:51.980]  вектором средних а и с матрицей кавариации, то что можно сказать про распределение вот
[13:51.980 --> 14:00.780]  линейного преобразования гауссовского вектора? Как что он будет распределен? Ну вот, на самом деле,
[14:00.780 --> 14:09.300]  опять какая-то наследственность от одномерного случая будет. Ну давайте так же докажем. А утверждение.
[14:09.300 --> 14:26.340]  Утверждение такое, что вот распределение будет... так, аа плюс б, а второй параметр у нас будет
[14:26.340 --> 14:35.740]  ксиа-транспонировано. Вот. То есть давайте посмотрим на одномерный случай. Так же домножается вот
[14:35.740 --> 14:42.160]  на эту... тут на константу, там на матричку добавляется вот этот вот сдвиг, параметр сдвига. И вот здесь
[14:42.160 --> 14:48.180]  потом на а в квадрате домножали. Здесь мы домножаем как бы нашу матричку на а и на транспонируемую.
[14:48.180 --> 14:59.580]  Какая-то аналогия есть. Вот. Ну давайте это докажем. Это вот утверждение. Доказательство. Давайте так же,
[14:59.580 --> 15:03.980]  как и в одномерном случае, вспомним, как у нас выглядит характеристическое преобразование,
[15:03.980 --> 15:17.780]  характеристическая функция линейного преобразования. Вот. По определению для случайных векторов,
[15:17.780 --> 15:29.500]  это получается E в степени и акси плюс b. Вот. То есть мы просто взяли с вами определение характеристической
[15:29.500 --> 15:33.860]  функции для случайного вектора, и вот здесь просто воспользовались определением для линейного
[15:33.860 --> 15:40.100]  преобразования случайного вектора. Получили с вами вот такое выражение. Вот. Ну и нетрудно заметить,
[15:40.100 --> 15:50.420]  что в силу линейности к вариации, которая у нас есть, у нас E в степени и bt у нас вылазит из-под
[15:50.420 --> 15:59.020]  математического ожидания, и остается по сути характеристическая функция вектора акси в какой
[15:59.020 --> 16:20.260]  точке? Ну, в точке A транспонированной. Это понятно? Ну, потому что, смотрите, вот у нас мы вот эту
[16:20.260 --> 16:29.660]  b-щику вынесли уже за мотож, ну, за х-функцию. Осталось у нас вот такое выражение. Да? Это
[16:29.660 --> 16:33.820]  эквивалентно тому, что акси транспонировано на A транспонировано на T. Ну, просто вот такое
[16:33.820 --> 16:41.740]  перемножение. Ну, потому что здесь у вас что получится? Эта матричка какого-то, ну, вот,
[16:41.740 --> 16:48.460]  квадратного размера умножайте на вектор. У вектора размер на стену на 1. То есть у вас здесь будет,
[16:48.460 --> 17:08.060]  по сути, одна строчка. Окей? Сейчас. Значит, вот у этой штучки у нас размерность. Н1? Да, я,
[17:08.060 --> 17:14.100]  по сути, A перекинул вот сюда. Да, ну, это вот делается вот таким вот нехитрым преобразованием.
[17:14.100 --> 17:31.340]  Отлично. Вот. Получили вот похожее выражение, как в одномерном случае, если вспомните,
[17:31.340 --> 17:34.660]  что мы делали с характеристическими функциями. Вот, и теперь мы можем с вами
[17:34.660 --> 17:39.540]  посчитать характеристическую функцию линейного преобразования. Так, я пока
[17:39.540 --> 17:45.700]  эквивалентное определение тогда сотру, или мне вот отсюда стереть? Давайте я отсюда сотру тогда.
[17:45.700 --> 18:04.540]  Давайте с вами посчитаем характеристическую функцию. Теперь вот пусть кси, гауссовский вектор
[18:04.540 --> 18:14.620]  с параметрами асима. Давайте найдем с вами характеристическую функцию вот такого вот
[18:14.620 --> 18:30.380]  преобразования. Здравствуйте, Дмитрий. Вы вовремя? Вот, ну, вот просто по доказанному утверждению мы с
[18:30.380 --> 18:39.740]  вами домножаем вот это на характеристическую функцию. А характеристическая функция у нас это
[18:39.740 --> 19:03.420]  Е в степени ИА. В точке получается а транспонированная Т, минус пополам. Ну и что у нас здесь было? Сигма, а транспонированная Т, а транспонированная Т.
[19:03.420 --> 19:16.860]  Просто применили утверждение о характеристической функции линейного преобразования вектора. Ну вот,
[19:16.860 --> 19:22.380]  в гауссовском случае. Ну и теперь, если мы с вами параметры все соберем вместе, у нас получится
[19:22.380 --> 19:34.220]  характеристическая функция вот такая. У нас здесь будет всего просто линейности. Так, так, так, так, так, так.
[19:34.220 --> 20:02.460]  А перебрасываем вот сюда. Получается А плюс В. И вычитаем. Ну, А тоже перебрасываем. Что завершает доказательство утверждения
[20:02.460 --> 20:11.020]  о том, что линейное преобразование гауссовского вектора имеет тоже гауссовские, гауссовское распределение вот с такими вот параметрами.
[20:11.020 --> 20:31.420]  Вот. Ну, в одномерном случае мы примерно тоже самое проделывали. Вопросы? Ответы? Ну ладно. Так, это мы с
[20:31.420 --> 20:38.940]  вами выяснили. Вопрос еще может вот такой возникнуть. Но все-таки нам иногда может
[20:38.940 --> 20:45.900]  хотеться плотность гауссовского вектора получить. Ну вот, вопрос такой. А когда существует плотность
[20:45.900 --> 20:55.500]  гауссовского вектора? Ну, на самом деле плотность гауссовского вектора идейна. Давайте подумаем просто.
[20:55.500 --> 21:01.500]  По сути, ваше распределение должно быть такой же размерности, как и то пространство,
[21:01.500 --> 21:09.140]  которое вы рассматриваете. Если это правда, то тогда у вас матрица к вариации должна быть
[21:09.140 --> 21:22.820]  полного ранга. Так вот, утверждение. Утверждение. Если матрица к вариации полноранговая,
[21:22.820 --> 21:40.740]  тогда существует плотность гауссовского вектора. И эта плотность имеет следующий вид.
[21:52.820 --> 22:09.740]  Вот. Ну то есть, вот эта плотность на самом деле очень похожа на одномерную. У нас там тоже был
[22:09.740 --> 22:20.500]  один, а корень из двух Пи сигма квадрат в знаменателе. Было? На Е в степени Х минус А в квадрате. Ну то
[22:20.500 --> 22:27.300]  есть, в одномерном случае у нас плотность как выглядит? Один на корень из два Пи сигма квадрат,
[22:27.300 --> 22:35.180]  Е в степени Х минус А в квадрате на два сигма в квадрате. Ну и вот опять можно какую-то
[22:35.180 --> 22:40.940]  наследственность заметить. Что как бы вот у вас что-то за дисперсию, отвечающую знаменателе,
[22:40.940 --> 22:47.420]  стоит. Здесь в степени одна вторая? Нет. Ну, в общем, да. На самом деле, в степени одна вторая,
[22:47.420 --> 22:57.740]  если сигма квадрат положить как исходный констант. Определитель матрицы. Вот. Здесь вот как раз тоже
[22:57.740 --> 23:02.660]  видите обратная матрица какая-то стоит. Вот здесь мы делили с вами на сигма в квадрате. Здесь вот как
[23:02.660 --> 23:06.860]  раз-таки мы делить на матрицу не можем, поэтому мы по сути здесь домножаем на вот обратную матрицу.
[23:06.860 --> 23:16.380]  И вот как видите, как бы вот одномерный случай очень как бы похож на многомерный. Где?
[23:16.380 --> 23:26.140]  Сигма полноранговая. Ну, значит, у нее положительный определитель. Ну, ранг равен размеру матрицы,
[23:26.140 --> 23:34.140]  значит, все столбцы линии независимы, значит, определитель не нулевой. Ну, не выраженная, да.
[23:34.140 --> 23:39.620]  Ну, можно сказать полноранговая, можно сказать не выраженная. Да ладно, хорошо. Я просто люблю
[23:39.620 --> 23:47.900]  уговорить полноранговая. Супер. Но плотности обычно не нужны, хотя может казаться, что нужны. На
[23:47.900 --> 23:53.340]  самом деле все задачи на гауссовские виктора решаются без плотностей, по крайней мере из тех,
[23:53.340 --> 24:02.180]  что у нас в программе есть. Что дальше? Так, это мы все обсудили. Про плотность мы с вами обсудили.
[24:02.180 --> 24:07.860]  Вот. Ну, как бы плотность я написал, показал, что она похожа на одномерный случай, и все,
[24:07.860 --> 24:13.540]  мы про это забываем. Мы больше плотности сегодня пользоваться не будем. Это нам не интересно.
[24:13.540 --> 24:16.780]  А интересно нам следующее.
[24:16.780 --> 24:43.580]  А интересно нам следующее. А давайте подумаем. Вот у нас есть гауссовский вектор.
[24:43.580 --> 24:57.580]  И компоненты. Компоненты кси. Давайте кси катай вот так. Гауссовская. И вопрос в том,
[24:57.580 --> 25:02.660]  как они связаны между собой. То есть то, что вектор гауссовский, и то, что компоненты нормальные. Ну,
[25:02.660 --> 25:11.940]  нормальные, по сути. Так, товарищи, какая тут связь? Кто-нибудь может вызнает?
[25:11.940 --> 25:26.260]  Ну, это неправда. Верно в правую сторону. Если вектор гауссовский, то, ну вот, давайте так.
[25:26.260 --> 25:33.940]  В право верно, в лево неверно. А в общем случае, но в частном верно. То есть, смотрите, если вектор
[25:33.940 --> 25:38.620]  гауссовский, почему компоненты гауссовские? Ну, потому что у нас есть вот это определение, что любая
[25:38.620 --> 25:42.580]  линейная комбинация имеет нормальное распределение. Ну, тогда если вы возьмете вот этот вектор с
[25:42.580 --> 25:47.700]  единичкой на том месте, которое вас интересует, то у вас, по сути, вот в этой линейной комбинации
[25:47.700 --> 25:52.300]  останется только кси катая компонента. И по вот этому определению эквивалентному у вас будет как
[25:52.300 --> 25:57.140]  раз-таки нормальная случайно величина. Поэтому из того, что вектор гауссовский, следует, что, ну,
[25:57.140 --> 26:10.940]  каждая его компонента гауссовская. Да, это следует, это следует вот из третьего определения. Так. Хотел
[26:10.940 --> 26:21.900]  что-то написать? Справа, ну да. Каждая компонента гауссовская, гауссовская, просто в определении 2,
[26:21.900 --> 26:32.060]  λ равняется много нольков, потом единичка на катом месте, и потом еще много нольков.
[26:32.060 --> 26:41.620]  А в обратную сторону. Вот тут надо два случая рассмотреть. Ну, давайте в общем случае сначала.
[26:41.620 --> 27:02.140]  В общем случае это неправда. Контр-пример. Рассмотрим случайную величину кси, имеющую распределение
[27:02.140 --> 27:10.980]  нормальное с параметами 0,1, и случайную величину это, которая просто принимает значение плюс-минус 1,
[27:10.980 --> 27:24.900]  с вероятностью одна вторая. Утверждение. Случайная величина кси, это имеет нормальное распределение с
[27:24.900 --> 27:34.700]  параметами 0,1. Ну, они независимы. Утверждение. Кси, это имеет нормальное распределение с параметами
[27:35.300 --> 27:45.100]  Доказательство. Доказательство не сложное, вероятность того, что кси эта меньше либо равно x,
[27:45.100 --> 27:52.700]  Просто по формуле полной вероятности это одна вторая на вероятность того, что кси меньше либо равно x,
[27:52.700 --> 28:01.540]  Плюс 1 вторая на вероятность того что, кси больше либо равно-минус x. В силу того, что распределение
[28:01.540 --> 28:04.740]  асимметрично, эти штуки равны, и это у нас просто вероятность того, что
[28:05.200 --> 28:07.200]  кси меньше либо равно х.
[28:07.700 --> 28:12.620]  Таким образом, функция распределения ксен совпадает с функцией распределения исходной случайно-вечной.
[28:12.820 --> 28:17.580]  Значит, если у кси было распределение n01, значит и у кси тет такое же распределение n01.
[28:24.620 --> 28:29.340]  Ну, этот значок рассмотрим. Это глаз, масоны.
[28:32.140 --> 28:34.140]  Вот.
[28:37.140 --> 28:39.980]  Собственно, короче, ничего содержательного.
[28:43.540 --> 28:45.540]  Так.
[28:47.340 --> 28:50.460]  Смотрите, с учетом этого, давайте теперь рассмотрим с вами вектор.
[28:51.500 --> 28:53.500]  Рассмотрим тогда вектор. Давайте вот сюда перейду.
[28:55.020 --> 28:57.380]  Вот такой. Из кси и кси это.
[28:57.940 --> 28:59.940]  Что мы можем сказать про его компоненты?
[28:59.980 --> 29:07.260]  Ну, каждый компонент имеет гауссовское распределение. Ну, то есть нормальное распределение. Потому что кси по условиям n01, кси это тоже имеет
[29:08.100 --> 29:10.100]  распределение n01 по
[29:10.940 --> 29:12.940]  повод доказанному утверждению.
[29:13.220 --> 29:17.220]  Но это не гауссовский вектор. Ну, давайте просто получим противоречие с каким-нибудь
[29:18.500 --> 29:20.980]  определением. Давайте поломаем определение номер два.
[29:22.100 --> 29:26.500]  Любая линейная комбинация должна иметь нормальное распределение. Ну, давайте просто возьмем сумму.
[29:28.220 --> 29:30.220]  Компонент.
[29:30.900 --> 29:33.100]  Ну, то есть, по сути, линейную комбинацию с 1,1.
[29:34.340 --> 29:36.340]  А кси вынестица у нас будет 1 плюс 1.
[29:37.740 --> 29:41.780]  Вот. Ну, и в силу того, что они независимы, это с вероятностью 1,2
[29:43.140 --> 29:45.140]  ноль.
[29:47.020 --> 29:49.020]  И с вероятностью 1,2
[29:50.100 --> 29:53.940]  это нормальная случайная величина с какими-то параметрами. Ну, видимо,
[29:54.340 --> 29:56.340]  0,2.
[29:58.500 --> 30:00.980]  Сейчас мы на двоечку домножаем.
[30:02.180 --> 30:04.180]  Значит, ну, 0,4.
[30:04.220 --> 30:10.600]  Потому что 2 в квадрате домножается. Ну а понятно, что это, ну, не нормальная случайная величина, потому что есть точка, у которой положительная мера.
[30:10.980 --> 30:16.660]  У нас это либо константа. Ну, то есть, на нормальной случайной величины у нас либо константа, ну, тогда вся вероятность должна быть в одной точке
[30:17.100 --> 30:21.620]  сконцентрирована. Либо, это уже, честно, непрерывная, абсолютно непрерывная случайная величина, тогда она вся должна быть
[30:21.620 --> 30:28.060]  Нормальная. А вот это какая-то смесь дискретной части непрерывной, так что это нам не подходит, и это ненормальная случайная величина
[30:28.700 --> 30:30.700]  следовательна из того, что
[30:32.300 --> 30:39.540]  компоненты гауссовские не следуют, в общем случае, что вектор гауссовский. Но, вот давайте так замечание напишу.
[30:43.380 --> 30:45.380]  Так, соответственно, это мы с вами доказали.
[30:51.700 --> 30:53.700]  Но нужно сделать
[30:58.420 --> 31:00.420]  замечание.
[31:04.700 --> 31:06.700]  Но если ксика
[31:09.460 --> 31:18.620]  независимая, нормальная компоненты, то вектор.
[31:21.860 --> 31:23.860]  Гауссовский.
[31:24.140 --> 31:31.100]  Ну, это более-менее очевидно, потому что, вот, у нас есть третье определение, где по сути просто гауссовский вектор определяется как линейное преобразование
[31:31.500 --> 31:37.900]  независимых стандартных нормальных случайных величин. Если вы составите вектор из независимых случайных величин, но с другими параметрами,
[31:38.660 --> 31:40.660]  ну, вы с такими параметрами можете просто получить
[31:42.780 --> 31:45.140]  гауссовский вектор путем того, что возьмете сначала
[31:46.140 --> 31:52.580]  вектор, составленный из независимых стандартных нормальных случайных величин, а затем просто их подомножаете на что-то, чтобы у вас получились вот те
[31:54.140 --> 31:59.020]  независимые нормальные случайные величины, которые у вас были, ну, с какими-то другими коэффициентами. Понятно?
[32:00.620 --> 32:07.460]  То есть, если вектор гауссовский, то каждая компонента нормальная, но если каждая компонента нормальная, в общем случае, не следует, что вектор гауссовский.
[32:10.380 --> 32:12.380]  Вот, но если они независимые,
[32:13.220 --> 32:19.900]  если они независимы компоненты, тогда, если вы из них составите вектор, этот вектор будет гауссовским.
[32:21.660 --> 32:23.660]  Вот, это важно понимать.
[32:26.660 --> 32:28.660]  Ну, и вот, собственно, если мы вот,
[32:29.220 --> 32:31.220]  тот пример еще вот, такая небольшая философия,
[32:32.260 --> 32:34.500]  давайте еще небольшая философия сейчас будет,
[32:35.300 --> 32:37.300]  минутная.
[32:38.180 --> 32:40.180]  Философия заключается в следующем.
[32:42.380 --> 32:56.060]  То есть, смотрите, мы с вами вот тут рассмотрели вектор кси, кси это, да, и можно рассмотреть,
[32:58.580 --> 33:02.580]  допустим, вектор вот такой, кси-кси, ну, где кси у нас
[33:04.740 --> 33:06.500]  N01, а
[33:06.500 --> 33:08.220]  это, это, ну, вот,
[33:08.220 --> 33:10.980]  такая случайная величина, что она равна плюс-минус емничке,
[33:11.460 --> 33:13.460]  с вероятностью 1,2.
[33:13.820 --> 33:16.180]  Вот, можем рассмотреть вот такой вектор и вот такой вектор.
[33:17.580 --> 33:19.580]  То есть, вот этот вектор,
[33:19.740 --> 33:24.260]  он нормальный с вероятностью 1,2 и с вероятностью 1,2, он сконцентрирован в одной точке.
[33:25.220 --> 33:27.220]  То есть,
[33:28.860 --> 33:31.660]  по сути, с вероятностью 1,2 вы находитесь в точке ноль.
[33:33.340 --> 33:40.780]  Сейчас, как бы это аккуратно сказать. Нет, если без преобразований, то смотрите, у нас одна координата всегда нормальная, а вторая нормальная,
[33:40.780 --> 33:42.620]  но со знаком. То есть, либо
[33:42.620 --> 33:49.020]  то же самое, либо с минусом. То есть, получается, что все ваше распределение, оно вот на этих двух осях, как бы
[33:49.500 --> 33:52.700]  находится. То есть, если вам так повезло, что этот знак
[33:53.380 --> 33:59.340]  единичка просто, ну, то есть, положительный, тогда у вас, по сути, вектор кси-кси, и у вас все значения вдоль вот этой оси находятся.
[33:59.660 --> 34:02.540]  Ну, то есть, у вас плотность, можно вот так нарисовать, если на нее сбоку смотреть.
[34:02.980 --> 34:07.500]  Если у вас получилось так, что это выпала минус единичка, тогда у вас вектор лежит вот на этой прямой.
[34:08.380 --> 34:11.300]  Вот. А если мы с вами вот такой вот вектор нарисуем,
[34:12.820 --> 34:14.820]  то он всегда будет просто лежать на
[34:16.300 --> 34:17.980]  вот такой прямой. И
[34:17.980 --> 34:21.540]  смотрите, вот этот вектор гауссовский, а вот этот вектор не гауссовский.
[34:21.820 --> 34:26.480]  То, что вот этот вектор не гауссовский, мы с вами показали. А то, что вот этот вектор гауссовский, понять несложно.
[34:26.820 --> 34:28.820]  Можно просто проверить вот это вот определение
[34:29.740 --> 34:31.300]  на
[34:31.300 --> 34:33.260]  и вот на линиейную комбинацию, например.
[34:33.260 --> 34:37.020]  любая линейная комбинация здесь будет либо 0, либо какая-то вот нормальная
[34:37.020 --> 34:38.940]  сущность влечена с киндекоэффициентами.
[34:38.940 --> 34:41.940]  И вот как бы в чем суть?
[34:41.940 --> 34:44.900]  Во-первых, понятно, что плотности у такого вектора не будет, потому что, ну, это выраженное
[34:44.900 --> 34:47.420]  распределение, в смысле оно на множестве меры ноль.
[34:47.420 --> 34:49.180]  И матрица к вариации
[34:49.180 --> 34:51.780]  будет как раз таки не полноранговая, то есть выраженная,
[34:51.780 --> 34:53.340]  она будет вот такая.
[34:53.340 --> 34:55.700]  Ну, 1, 1, 1, 1.
[34:55.700 --> 34:57.300]  Поэтому у вас и плотности нет,
[34:57.300 --> 35:00.100]  поэтому и все ваше распределение лежит как бы на прямой,
[35:00.100 --> 35:03.180]  а не на плоскости.
[35:03.220 --> 35:06.300]  Если вы сядете вот в эту точку, вот в эту точку,
[35:06.300 --> 35:08.640]  и будете смотреть как бы на график плотности, виден АFl, то
[35:08.640 --> 35:11.020]  либо вы просто будете рассматривать только эту прямую,
[35:11.020 --> 35:14.260]  то у вас на ней как бы будет обычное одномерное нормальное распределение,
[35:14.260 --> 35:16.120]  справедливо?
[35:16.120 --> 35:17.800]  Вот, то есть как бы
[35:17.800 --> 35:19.900]  даже если у вас матрица к вариации выраженная,
[35:19.900 --> 35:22.660]  на самом деле это просто нормальное распределение, но в подпроспранвсе
[35:22.660 --> 35:23.980]  меньше размерности.
[35:23.980 --> 35:27.240]  ну просто в подпроспрансе каком-то.
[35:27.240 --> 35:31.860]  Это все еще нормальное распределение. Все еще гал officialsский вектор, но меньше размерности.
[35:31.860 --> 35:33.720]  А вот в чем здесь противоречие?
[35:33.720 --> 35:38.140]  В тем, что вот как вы не смотрите, но у вас никогда не будет гауссовский вектор, даже вот на меньшем подпространстве,
[35:38.140 --> 35:41.360]  который вот таким вот крестиком как бы располагается.
[35:42.360 --> 35:44.160]  Идеи непонятны в чем отличие?
[35:44.160 --> 35:50.560]  Между, ну, неполноранговым вектором и вообще не гауссовским вектором.
[35:57.060 --> 35:58.560]  Или вы меня пугаете?
[36:02.560 --> 36:08.820]  А в суть в том, что мы тихо не можем бы к пространству вот это нормально?
[36:08.820 --> 36:11.860]  Ну то есть смотрите вот это случайная величина, но вот этот вектор,
[36:11.860 --> 36:16.120]  он нормальный, просто если вы вот вторую лишнюю координату берете,
[36:16.120 --> 36:19.960]  это просто продублирование координации, на самом деле это просто вот...
[36:19.960 --> 36:22.100]  как бы здесь избыточная информация какая-то есть,
[36:22.100 --> 36:25.140]  но это все еще нормальная случайная величина, просто меньше подпространства.
[36:25.140 --> 36:28.640]  А вот это не в каком меньшем подпространстве не является нормальной случайной величиной,
[36:28.640 --> 36:32.640]  потому что ваша нормальная случайная личность не может как бы вот на таких вот подпроссансовых располагаться никогда.
[36:32.640 --> 36:39.640]  То есть вот здесь вы садитесь в эту точку и смотрите вот на график плотности и увидите перед собой такой красивый график плотности, да?
[36:39.640 --> 36:42.640]  А здесь что-то дурацкое получится.
[36:49.640 --> 36:53.640]  Ну, у вас как бы плотность и вот вдоль этой прямой, и вдоль вот этой прямой, да?
[36:54.640 --> 36:57.640]  Да, да, да, да.
[37:01.640 --> 37:07.640]  Потому что вот это подпространство, которое мы рассматриваем, оно действительно, оно не является подпространством.
[37:07.640 --> 37:12.640]  То есть если мы рассматриваем с вами область из вот этих двух прямых, оно не является линиейным подпространством.
[37:12.640 --> 37:14.640]  Понятно?
[37:14.640 --> 37:17.640]  Вот это справедливо замечание.
[37:18.640 --> 37:23.640]  То есть да, оно как бы меньше размерности, но оно не является подпространством, поэтому это не нормальный вектор.
[37:23.640 --> 37:25.640]  Ну, не гауссовский вектор.
[37:26.640 --> 37:29.640]  Так, мы близимся к задачам.
[37:31.640 --> 37:36.640]  Самое важное, то, что нужно сказать перед задачей, это самое главное свойство гауссовских векторов.
[37:36.640 --> 37:39.640]  За счет которого, по сути, все задачи решаются.
[37:48.640 --> 37:50.640]  А...
[37:51.640 --> 37:53.640]  Свойство такое...
[37:55.640 --> 37:56.640]  Давай так.
[37:56.640 --> 37:58.640]  Диаметр.
[37:59.640 --> 38:01.640]  Диаметр.
[38:01.640 --> 38:03.640]  Диаметр.
[38:04.640 --> 38:05.640]  Давай так.
[38:05.640 --> 38:07.640]  Теорема.
[38:07.640 --> 38:09.640]  Ну, хотя это утверждение, ладно, пусть будет теорема.
[38:09.640 --> 38:11.640]  А...
[38:11.640 --> 38:13.640]  Вот пусть ксигауссовский вектор.
[38:15.640 --> 38:17.640]  Тогда...
[38:19.640 --> 38:21.640]  Компоненты независимы его...
[38:27.640 --> 38:29.640]  То есть ксига.
[38:34.640 --> 38:36.640]  Если они не коррелированы.
[38:36.640 --> 38:39.640]  Ну, на самом деле, тогда и только тогда, когда они не коррелированы.
[38:50.640 --> 38:54.640]  Ну, очевидно, что из независимости следует некоррелированность.
[38:54.640 --> 38:57.640]  То есть в одну сторону очевидно, и мы даже доказывать не будем.
[38:57.640 --> 39:04.640]  Гораздо более интересно то, что, смотрите, чтобы компоненты гауссовского вектора были независимы, достаточно нулевой к вариации.
[39:04.640 --> 39:07.640]  То есть, как бы в общем случае, стрелочка отсюда-сюда не верна.
[39:07.640 --> 39:11.640]  Но вот когда мы рассматриваем именно гауссовский вектор, именно его компоненты,
[39:11.640 --> 39:15.640]  если они не коррелированы, то они уже независимы.
[39:15.640 --> 39:17.640]  Доказательства.
[39:19.640 --> 39:21.640]  А... доказать, на самом деле, очень просто.
[39:21.640 --> 39:23.640]  Нужно просто еще одну теорему вспомнить.
[39:23.640 --> 39:27.640]  Давайте так. Критерии независимости в терминах характеристических функций.
[39:37.640 --> 39:41.640]  Критерии независимости в терминах характеристических функций выглядят следующим образом.
[39:41.640 --> 39:46.640]  Случайные величины независимы тогда и только тогда, когда...
[39:46.640 --> 39:53.640]  совместная характеристическая функция распадается в произведение характеристических функций.
[39:53.640 --> 39:58.640]  То есть, что-то похожее у нас было с функциями распределений.
[39:58.640 --> 40:02.640]  И еще вот такое же утверждение есть на самом деле и в терминах характеристических функций.
[40:02.640 --> 40:05.640]  Доказывается вот на лекциях.
[40:05.640 --> 40:08.640]  Но давайте с учетом этого докажем вот этот факт.
[40:08.640 --> 40:10.640]  На самом деле, все очень просто.
[40:10.640 --> 40:14.640]  Смотрите, если все ваши компоненты не коррелированы,
[40:14.640 --> 40:18.640]  то у вас, на самом деле, матрица кавариаций
[40:22.640 --> 40:24.640]  имеет диагональный вид.
[40:24.640 --> 40:29.640]  то у вас на самом деле тогда матрица ковариаций
[40:32.360 --> 40:35.960]  имеет диагональный вид.
[40:42.040 --> 40:47.760]  А вот здесь везде 0. Ну просто компоненты все не коррелированы.
[40:47.920 --> 40:50.240]  Значит ковариация между ними равна 0.
[40:53.800 --> 40:56.800]  Почему из этого будет следовать независимость компонентов?
[40:56.800 --> 40:58.800]  Ну вот воспользуемся критериями независимости.
[40:58.800 --> 41:03.600]  Вспомним, как у нас выглядит характеристическая функция гауссовского вектора.
[41:05.480 --> 41:08.520]  Характеристическая функция гауссовского вектора это у нас на самом деле E в степени.
[41:08.520 --> 41:11.440]  Вот сейчас эскалярное произведение здесь как сумму перепишу.
[41:11.440 --> 41:25.600]  А минус, так, вот так.
[41:25.600 --> 41:31.640]  Да, давай здесь G сделаем.
[41:31.640 --> 41:38.480]  G от 0 до N. Вот смотрите, а так как вот эта штучка диагональная,
[41:38.480 --> 41:46.520]  то по сути это же тоже теперь просто сумма, а ты E в квадрате с коэффициентом sigma E в квадрате.
[41:46.520 --> 41:53.600]  Ну потому что вы просто, когда вот этот векторочек будете на маточку умножать,
[41:53.600 --> 41:57.920]  у вас каждая компонента просто по отдельности умножится, а потом вы просто ее еще раз умножите
[41:57.920 --> 42:16.400]  на себя и по сути вот это можно переписать как сумму sigma, давайте так, GG на TG в квадрате по G от единички до N.
[42:16.400 --> 42:25.600]  И в силу этого, что можно сказать? Ну дальше просто заметим, что это теперь все на самом деле
[42:25.600 --> 42:29.720]  распадается в произведениях характеристических функций компонентов, потому что это все на самом деле
[42:29.720 --> 42:35.480]  теперь просто произведение таких вот характеристических функций.
[42:35.480 --> 42:53.800]  Ну здесь можно там для однозначности квадратики расставить, допустим, но это не обязательно.
[42:54.400 --> 43:02.680]  И теперь заметьте, что вот эта эта же характеристическая функция, вот эта эта
[43:02.680 --> 43:06.880]  характеристическая функция нормального распределения каждой компоненты.
[43:06.880 --> 43:11.360]  То есть, у вас совместная характеристическая функция, распалась в произведениях характеристических функций.
[43:13.360 --> 43:18.800]  Теперь вспоминаем, что вот мы с вами только что обсудили критерии независимости,
[43:18.800 --> 43:21.720]  что случайная величина независимы тогда и только тогда, когда совместная
[43:21.720 --> 43:23.280]  характеристического функция
[43:23.280 --> 43:26.600]  распадается в произведение характеристических функций.
[43:26.600 --> 43:29.280]  Ну и все, мы это с вами получили, что если матрица диагональная,
[43:29.280 --> 43:33.280]  вот эта штучка просто распадается на сумму, и мы распадаемся в произведение.
[43:33.280 --> 43:35.160]  Получили, что компоненты независимы
[43:35.160 --> 43:36.320]  тогда и только тогда,
[43:36.320 --> 43:39.960]  когда они не коррелированы.
[43:39.960 --> 43:42.880]  Справедливо?
[43:42.880 --> 43:45.640]  Справедливо.
[43:45.640 --> 43:49.400]  Вот, верно, на самом деле и даже более строгое утверждение, что, смотрите, если
[43:49.400 --> 43:51.520]  ваша матрица кавариаций
[43:52.520 --> 43:56.760]  представима в блочно-диагональном виде,
[44:00.000 --> 44:01.560]  а вот все остальное нули,
[44:01.560 --> 44:05.700]  то вот эти подвиктора будут друг от друга не зависеть.
[44:05.700 --> 44:09.060]  То есть мы с вами рассмотрели случаи, когда у вас просто блочно-диагональный вид,
[44:09.060 --> 44:10.780]  в смысле, все элементы просто диагональные,
[44:10.780 --> 44:12.100]  но если здесь у вас будут
[44:12.100 --> 44:15.140]  блоки на диагонале, которые не нулевые, а все остальное нули,
[44:15.140 --> 44:18.100]  то у вас вот эти подвиктора будут независимы между собой.
[44:18.100 --> 44:20.000]  Ну, доказательства абсолютно аналогичны.
[44:20.000 --> 44:23.280]  просто там с индексами чуть посложнее
[44:23.280 --> 44:24.960]  их попереставлять, но
[44:24.960 --> 44:29.280]  то же самое будет выполнено
[44:29.280 --> 44:32.320]  ну и все, мы готовы решать задачу
[44:32.320 --> 44:39.320]  собственно давайте ее решать
[44:42.680 --> 44:45.560]  мы сначала вспомогательную задачу еще решим
[44:45.560 --> 44:47.760]  небольшую
[44:47.760 --> 44:50.160]  но она важна для понимания того как
[44:50.160 --> 44:57.160]  третью решать
[45:07.800 --> 45:13.560]  я все отправлю потом
[45:13.560 --> 45:15.960]  ну да
[45:15.960 --> 45:20.280]  ну а что
[45:20.280 --> 45:22.320]  потом запись посмотрите если умираете
[45:22.320 --> 45:23.840]  можете домой пойти отдохнуть
[45:23.840 --> 45:26.560]  мы тут с Ильей и у нас это
[45:26.560 --> 45:29.480]  тусовка по ходу намечается сегодня
[45:29.480 --> 45:32.920]  автопати
[45:32.920 --> 45:35.680]  да, отдыхаем
[45:35.680 --> 45:38.120]  так, ну давайте вот такое замечание, смотрите
[45:38.120 --> 45:39.840]  вот пусть у нас есть гауссовский вектор
[45:39.840 --> 45:44.320]  из двух компонентов состоящий
[45:44.320 --> 45:47.440]  на самом деле
[45:47.440 --> 45:49.520]  кси можно представить в таком виде
[45:49.520 --> 45:55.280]  кси можно представить в виде кси1 плюс кси2
[45:55.280 --> 45:58.760]  такое что кси1
[45:58.760 --> 46:00.400]  ну давайте так, кси2
[46:00.400 --> 46:05.160]  это какая-то функция от это
[46:05.160 --> 46:06.680]  а кси1
[46:06.680 --> 46:13.760]  независимо с этим
[46:13.800 --> 46:15.320]  утверждение
[46:15.320 --> 46:17.120]  если вам дан гауссовский вектор
[46:17.120 --> 46:19.000]  то вы любую его компоненту можете
[46:19.000 --> 46:20.760]  представить как сумму двух компонентов
[46:20.760 --> 46:21.880]  одна из которых
[46:21.880 --> 46:23.960]  это просто какая-то функция от другой
[46:23.960 --> 46:25.120]  компоненты
[46:25.120 --> 46:26.960]  а второе, ну вот первое слагаемое, а второе
[46:26.960 --> 46:28.560]  слагаемое это что-то что независит от вот
[46:28.560 --> 46:30.080]  этой второй компоненты
[46:30.080 --> 46:35.440]  доказательства
[46:35.440 --> 46:38.000]  давайте попробуем наш
[46:38.000 --> 46:40.640]  вектор кси это
[46:40.640 --> 46:42.040]  давайте поймем какого вида мы будем
[46:42.080 --> 46:44.640]  искать с вами кси1 и кси2
[46:44.640 --> 46:47.480]  смотрите кси2 мы будем искать
[46:47.480 --> 46:54.480]  в виде лямбда это
[46:54.480 --> 46:55.800]  почему в таком виде потому что на
[46:55.800 --> 46:57.360]  самом деле все что мы хорошо умеем делать
[46:57.360 --> 46:58.840]  с гауссовскими векторами это линейные
[46:58.840 --> 47:00.080]  преобразования
[47:00.080 --> 47:02.320]  вот и соответственно кси2 мы будем искать
[47:02.320 --> 47:04.840]  в таком виде
[47:04.840 --> 47:08.120]  а кси1, ну понятно как
[47:08.120 --> 47:09.600]  кси1 у нас будет
[47:09.600 --> 47:13.040]  кси-кси2
[47:13.040 --> 47:16.200]  просто из вот того определения
[47:16.200 --> 47:17.480]  ну и вот теперь утверждается что
[47:17.480 --> 47:19.200]  существует лямбда при котором кси1 и кси2
[47:19.200 --> 47:23.080]  будут независимы
[47:23.080 --> 47:26.400]  это первое
[47:26.400 --> 47:27.520]  ну почему это так
[47:27.520 --> 47:29.040]  потому что по сути все что мы сейчас с
[47:29.040 --> 47:30.560]  вами делаем
[47:30.560 --> 47:34.560]  это мы исходный вектор
[47:34.560 --> 47:41.560]  домножаем на матричку вот такого вида
[47:43.920 --> 47:46.040]  гауссовские вектора замкнуты относительно
[47:46.040 --> 47:48.200]  линейных преобразований
[47:48.200 --> 47:50.880]  мы это обсуждали
[47:50.880 --> 47:52.800]  соответственно то что получится
[47:52.800 --> 47:55.440]  во-первых получится кси1 и кси2
[47:55.440 --> 47:56.600]  а во-вторых
[47:56.600 --> 47:59.320]  ну на самом деле это будет
[47:59.320 --> 48:01.240]  опять гауссовский вектор
[48:01.240 --> 48:03.440]  и если вот у исходного гауссовского
[48:03.440 --> 48:04.600]  вектора у вас
[48:04.600 --> 48:06.720]  параметры были
[48:06.720 --> 48:08.800]  ну какие-нибудь я не знаю
[48:08.800 --> 48:10.360]  пусть будет вектор средних ноль
[48:10.360 --> 48:11.920]  а матрица кавариаций
[48:11.920 --> 48:15.240]  сигма
[48:15.240 --> 48:18.040]  то вот у этого этот вектор тоже будет
[48:18.040 --> 48:20.920]  гауссовский
[48:20.920 --> 48:22.840]  ну видимо с вектором средних ноль
[48:22.840 --> 48:23.840]  а
[48:23.840 --> 48:25.840]  матрица кавариаций
[48:25.840 --> 48:27.320]  у него вот по тому правилу который мы
[48:27.320 --> 48:29.040]  обсудили поменяется то есть если вот это
[48:29.040 --> 48:30.520]  матрица А
[48:30.520 --> 48:32.920]  то новая матрица кавариаций у него будет
[48:32.920 --> 48:37.600]  сигма отранспонирована
[48:37.600 --> 48:39.640]  и утверждается что можно подобрать такой
[48:39.640 --> 48:40.640]  коэффициент лямда
[48:40.640 --> 48:42.640]  что вот эта матрица будет диагональна
[48:42.640 --> 48:44.440]  то есть компоненты будут независимы по
[48:44.440 --> 48:46.760]  предыдущему утверждению
[48:46.760 --> 48:48.200]  то есть мы сделаем некоррелированные
[48:48.200 --> 48:49.880]  компоненты кси1 кси2
[48:49.880 --> 48:51.360]  вот и соответственно они будут
[48:51.360 --> 48:53.440]  независимы по предыдущему утверждению
[48:53.440 --> 48:55.600]  который мы с вами доказали
[48:55.600 --> 48:56.640]  ну супер
[48:56.640 --> 48:58.320]  а можно искать
[48:58.320 --> 49:00.120]  можно решать какие-то уравнения искать из
[49:00.120 --> 49:01.160]  них как бы
[49:01.200 --> 49:03.000]  какие лямда нам нужны
[49:03.000 --> 49:05.160]  но самый простой способ это
[49:05.160 --> 49:08.160]  решение уравнения некоррелированности
[49:17.400 --> 49:18.600]  что мы по сути хотим
[49:18.600 --> 49:19.680]  мы хотим чтобы после этого
[49:19.680 --> 49:20.960]  преобразования
[49:20.960 --> 49:23.960]  кавариация кси1 кси2
[49:23.960 --> 49:26.600]  была равна нулю
[49:26.600 --> 49:29.400]  кавариация кси1
[49:29.440 --> 49:32.360]  кси1 у нас это
[49:32.360 --> 49:33.800]  по сути давайте перепишем это
[49:33.800 --> 49:35.840]  через определение кси1 кси2
[49:35.840 --> 49:39.520]  кси1 это у нас кси минус лямда это
[49:39.520 --> 49:42.520]  а кси2 у нас это просто лямда это
[49:43.680 --> 49:45.800]  теперь пользуясь биллинейностью кавариации
[49:45.800 --> 49:47.560]  мы с вами поймем например следующее
[49:47.560 --> 49:50.920]  что это кавариация, что это лямда
[49:50.920 --> 49:52.320]  кавариация
[49:52.320 --> 49:54.080]  кси это
[49:54.080 --> 49:56.000]  а минус
[49:56.000 --> 49:57.600]  лямда квадрат
[49:57.600 --> 50:00.600]  кавариация это это
[50:07.560 --> 50:09.080]  ну и у нас по уравнению вот это должно
[50:09.080 --> 50:10.840]  все равняться нулю
[50:10.840 --> 50:12.240]  отсюда либо лямда 0
[50:12.240 --> 50:15.120]  но лямда 0 на самом деле нам не подходит
[50:15.120 --> 50:18.240]  либо лямда
[50:18.240 --> 50:20.920]  равняется кавариация
[50:20.920 --> 50:22.440]  кси это
[50:22.440 --> 50:25.440]  поделительно кавариация это это
[50:28.120 --> 50:29.400]  ну и утверждение
[50:29.400 --> 50:31.440]  что действительно мы с вами только что
[50:31.440 --> 50:32.440]  взяли кси
[50:32.440 --> 50:34.160]  разложили
[50:34.160 --> 50:35.840]  в сумму двух компонент
[50:35.840 --> 50:37.920]  одна из компонент кси1 это будет просто
[50:37.920 --> 50:40.040]  кси минус лямда мы с вами подобрали
[50:40.040 --> 50:41.120]  вот такое лямда
[50:41.120 --> 50:42.840]  минус лямда это а вторая компонента это
[50:42.840 --> 50:45.440]  просто будет лямда это
[50:45.440 --> 50:48.000]  откуда брать вот эти штуки
[50:48.000 --> 50:51.000]  вот то что в числе не то что в знаменателе
[50:53.760 --> 50:55.720]  ну да из исходной матрицы кавариации
[50:55.720 --> 50:56.880]  то есть у вас была исходная матрица
[50:56.880 --> 50:57.800]  кавариации
[50:57.800 --> 50:59.200]  и здесь у вас на самом деле есть и
[50:59.200 --> 51:01.080]  дисперсия обеих случайных величин и их
[51:01.080 --> 51:02.280]  кавариация попарная
[51:02.280 --> 51:04.080]  поэтому если вот вам дана эта матрица
[51:04.080 --> 51:06.240]  вы знаете это это значит находите лямда
[51:06.240 --> 51:07.760]  значит умеете раскладывать любую
[51:07.760 --> 51:09.800]  компоненту в сумму
[51:09.800 --> 51:11.160]  зависящие от другой компоненты и
[51:11.160 --> 51:13.280]  независимая вот с другой
[51:13.280 --> 51:15.080]  все и это сейчас выстроили у нас задачу
[51:15.080 --> 51:20.080]  собственно
[51:20.080 --> 51:23.080]  собственно третью задачу
[51:40.840 --> 51:43.840]  так задача 3
[51:44.600 --> 51:47.600]  задача 3 следующее
[51:48.600 --> 51:51.600]  где у меня условия
[51:56.000 --> 51:59.000]  пусть x, y, z
[52:01.000 --> 52:04.000]  гауссовский вектор
[52:06.000 --> 52:09.000]  а имеющие вот такое вот распределение
[52:10.000 --> 52:13.000]  вектор средних 0 и матрица кавариации
[52:13.000 --> 52:14.000]  сигма
[52:14.000 --> 52:17.000]  а матрица кавариации сигма нам дана
[52:17.000 --> 52:19.000]  и она выглядит следующим образом 3
[52:19.000 --> 52:20.000]  минус 1
[52:20.000 --> 52:21.000]  1
[52:21.000 --> 52:24.000]  минус 1 2 минус 1
[52:24.000 --> 52:27.000]  1 минус 1 4
[52:27.000 --> 52:30.000]  вот такая матрица кавариации
[52:30.000 --> 52:32.000]  и нас просят найти математическое
[52:32.000 --> 52:33.000]  ожидание
[52:33.000 --> 52:36.000]  синус
[52:36.000 --> 52:37.000]  y
[52:37.000 --> 52:40.000]  а е в степени x плюс 2 z
[52:42.000 --> 52:45.000]  вот здесь 2
[52:51.000 --> 52:53.000]  вот задача на самом деле страшная
[52:53.000 --> 52:55.000]  если ее так сходу решать
[52:55.000 --> 52:57.000]  то есть первое что может прийти в голову
[52:57.000 --> 52:58.000]  это
[52:58.000 --> 53:00.000]  ну матрица невыраженная
[53:00.000 --> 53:02.000]  можно видимо плотность найти и видимо
[53:02.000 --> 53:04.000]  что-то там тройной интеграл еще
[53:04.000 --> 53:07.000]  но видимо можно как-то проще
[53:07.000 --> 53:10.000]  и действительно можно проще
[53:10.000 --> 53:12.000]  вот при помощи той задачи которую мы
[53:12.000 --> 53:14.000]  только что с вами решили
[53:14.000 --> 53:15.000]  мы сейчас эту задачу в принципе
[53:15.000 --> 53:17.000]  достаточно быстро решим
[53:17.000 --> 53:20.000]  в чем идея
[53:22.000 --> 53:25.000]  идея в том
[53:25.000 --> 53:27.000]  чтобы разложить
[53:27.000 --> 53:28.000]  ну вот получается когда у вас какая-то
[53:28.000 --> 53:29.000]  сложная функция вот
[53:29.000 --> 53:31.000]  грубо говоря здесь синус от чего-то
[53:31.000 --> 53:32.000]  и
[53:32.000 --> 53:33.000]  ешка от чего-то
[53:33.000 --> 53:35.000]  идея в том чтобы разложить одну из вот
[53:35.000 --> 53:37.000]  этих компонентов либо это либо вот эту
[53:37.000 --> 53:39.000]  вот по предыдущей задачи
[53:39.000 --> 53:40.000]  на что-то что зависит
[53:40.000 --> 53:41.000]  ну мы будем сами раскладывать вот это
[53:41.000 --> 53:43.000]  на что-то что зависит от y
[53:43.000 --> 53:45.000]  и что не зависит от y
[53:45.000 --> 53:47.000]  ну для чего это нужно давайте вот сейчас
[53:47.000 --> 53:48.000]  план
[53:48.000 --> 53:50.000]  я тогда план напишу
[53:52.000 --> 53:54.000]  мы хотим с вами
[53:54.000 --> 53:56.000]  x плюс 2 z
[53:57.000 --> 53:58.000]  представить как что-то
[53:58.000 --> 54:00.000]  зависище от y
[54:00.000 --> 54:02.000]  плюс что-то не зависище от y
[54:06.000 --> 54:08.000]  вот если мы так сделаем
[54:08.000 --> 54:09.000]  вдруг нам получится
[54:09.000 --> 54:11.000]  вдруг у нас так получится
[54:11.000 --> 54:12.000]  сейчас
[54:15.000 --> 54:17.000]  я так же раскладывал
[54:19.000 --> 54:20.000]  да
[54:20.000 --> 54:23.000]  все я так же решал просто чтобы
[54:23.000 --> 54:24.000]  а
[54:24.000 --> 54:25.000]  а
[54:25.000 --> 54:26.000]  а
[54:26.000 --> 54:27.000]  а
[54:27.000 --> 54:28.000]  вот
[54:28.000 --> 54:29.000]  соответственно
[54:29.000 --> 54:31.000]  если мы вот так с вами разложим
[54:33.000 --> 54:34.000]  вот эту сумму
[54:34.000 --> 54:35.000]  то что у нас получится
[54:35.000 --> 54:37.000]  у нас получится что математическое ожидание
[54:37.000 --> 54:38.000]  синус
[54:38.000 --> 54:40.000]  y
[54:40.000 --> 54:43.000]  вот можно равенство будет написать вот здесь вот такое
[54:43.000 --> 54:45.000]  это будет математическое ожидание синус y
[54:45.000 --> 54:47.000]  на е в степени
[54:47.000 --> 54:49.000]  и вот x плюс 2 z мы с вами разложили под
[54:49.000 --> 54:51.000]  такому вот правилу
[54:51.000 --> 54:53.000]  плюс что-то
[54:53.000 --> 54:54.000]  и вот это
[54:54.000 --> 54:56.000]  мы разложили под такому вот правилу
[54:56.000 --> 54:58.000]  плюс у
[54:58.000 --> 55:00.000]  и причем у у нас
[55:00.000 --> 55:02.000]  независимый с y
[55:02.000 --> 55:04.000]  ну на самом деле тогда
[55:07.000 --> 55:10.000]  это можно разложить в произведение математических ожиданий
[55:11.000 --> 55:13.000]  вот таким вот образом
[55:18.000 --> 55:19.000]  ну почему
[55:19.000 --> 55:21.000]  потому что у вас вот это не зависит от y
[55:21.000 --> 55:23.000]  и вот это не зависит от вот этого y
[55:23.000 --> 55:25.000]  соответственно вот это вот выражение e
[55:25.000 --> 55:27.000]  оно не зависит от é в степени u
[55:27.000 --> 55:29.000]  и соответственно мы можем тогда
[55:29.000 --> 55:31.000]  воспользоваться вот таким вот свойством что
[55:33.000 --> 55:35.520]  мотождание произведения равно произведению мотожданий в силу независимости
[55:36.000 --> 55:38.500]  ну и все, на самом деле вот это легко считать
[55:38.500 --> 55:41.000]  там есть небольшая техника, мы сейчас обсудим, какая
[55:41.000 --> 55:43.000]  и вот это на самом деле легко считать
[55:43.000 --> 55:45.000]  тоже небольшая техника, но
[55:45.000 --> 55:47.000]  и считается несложно
[55:47.000 --> 55:48.000]  так или я понятна
[55:49.000 --> 55:50.000]  супер
[55:51.000 --> 55:52.000]  вот
[55:53.000 --> 55:55.200]  В каком виде мы будем U искать?
[55:55.200 --> 55:59.240]  Ну, очевидно, в таком же виде, как и в предыдущей задаче, то есть U у нас это
[55:59.240 --> 56:01.680]  x-αy
[56:01.680 --> 56:05.480]  плюс 2z
[56:05.480 --> 56:07.080]  Мы хотим, чтобы
[56:07.080 --> 56:09.360]  U не зависело от
[56:09.360 --> 56:10.440]  y
[56:10.440 --> 56:13.240]  Ну, видимо, нужно опять решать уравнение несмещенности
[56:13.240 --> 56:15.720]  Уравнение несмещенности такое
[56:15.720 --> 56:17.760]  кавариация y
[56:17.760 --> 56:18.640]  и U
[56:18.640 --> 56:19.600]  равно 0
[56:19.600 --> 56:26.600]  Ну, U у нас вот так определено
[56:29.800 --> 56:32.800]  Ну и все, по сути нам осталось отсюда альфа найти
[56:32.800 --> 56:35.760]  Опять-таки, по белинейности это все можно пораскрывать
[56:35.760 --> 56:37.000]  Это, по сути, будет
[56:37.000 --> 56:39.000]  кавариация
[56:39.000 --> 56:40.680]  yx
[56:40.680 --> 56:43.120]  минус альфа кавариация
[56:43.120 --> 56:45.520]  yy
[56:45.520 --> 56:47.600]  плюс 2
[56:47.600 --> 56:50.880]  плюс 2 кавариации yz
[56:50.880 --> 56:53.000]  И все это равно 0
[56:53.000 --> 56:54.440]  Вот это
[56:54.440 --> 56:58.080]  Вот это, и вот это мы с вами знаем из вот этой матрики кавариации, которая нам дана
[56:58.080 --> 56:59.880]  по условию
[56:59.880 --> 57:02.200]  Ну и все, отсюда альфа находится
[57:02.200 --> 57:07.400]  Ну вот, я дома считал, у меня альфа получилась вот такая
[57:07.400 --> 57:09.160]  Там, если что, решение будет
[57:09.160 --> 57:12.600]  можно будет проверить меня
[57:12.600 --> 57:17.200]  Если я нигде не наложал, то у меня альфа получилась равная минус 3 вторых
[57:17.200 --> 57:24.200]  Возможно, там пересчитать нужно, где-то я может что-то
[57:28.720 --> 57:33.880]  Ну смотри, кавариация yx, ну а что?
[57:33.880 --> 57:35.160]  Да, там еще на двоечку
[57:35.160 --> 57:40.640]  А ты теперь решаешь параллельно, да?
[57:40.640 --> 57:44.520]  Спасибо
[57:44.560 --> 57:47.840]  Супер
[57:47.840 --> 57:50.280]  Ну вот, у меня получилась альфа минус 3 вторых
[57:50.280 --> 57:53.160]  Все, соответственно, мы можем вернуться вот к этому шагу решения
[57:53.160 --> 57:56.480]  И теперь, по сути, нам нужно посчитать математическое ожидание вот такое
[57:56.480 --> 57:58.480]  и математическое ожидание вот такое
[57:58.480 --> 58:02.560]  То есть, по сути, два интеграла по отдельности посчитать
[58:02.560 --> 58:06.000]  Это, на самом деле, уже считать достаточно легко
[58:06.000 --> 58:10.200]  Но нужно знать как
[58:10.200 --> 58:12.680]  То есть, если не знать, как это считать, то это сложно
[58:12.680 --> 58:16.640]  А вот если знать, то это легко
[58:16.640 --> 58:21.480]  Давайте начнем с какого интеграла? Ну, давайте начнем с первого интеграла
[58:21.480 --> 58:23.880]  Как найти математическое ожидание
[58:23.880 --> 58:26.120]  sin y
[58:26.120 --> 58:28.080]  на e в степени минус
[58:28.080 --> 58:33.320]  Что у нас там? 3 вторых?
[58:33.320 --> 58:38.880]  Как вот такое мы должны посчитать?
[58:38.880 --> 58:40.320]  Есть проще метод
[58:40.320 --> 58:44.200]  Рассказываю
[58:44.200 --> 58:47.600]  Смотрите, во-первых, давайте
[58:47.600 --> 58:49.280]  Запишем с вами
[58:49.280 --> 58:52.200]  Вот мы тоже раскроем и запишем как интегральчик
[58:52.200 --> 58:54.200]  с плотностью
[58:54.200 --> 58:58.040]  То есть, у нас здесь будет sin y
[58:58.040 --> 58:59.680]  на e в степени
[58:59.680 --> 59:02.600]  минус 3 вторых y
[59:02.600 --> 59:03.960]  и на плотность
[59:03.960 --> 59:06.360]  плотность это у нас 1 поделить на корень
[59:06.360 --> 59:08.440]  из двух π
[59:08.440 --> 59:13.960]  Так, у y дисперсию можно взять из матрицы к вариации
[59:13.960 --> 59:16.960]  Это 2
[59:20.760 --> 59:22.040]  e в степени
[59:22.040 --> 59:23.760]  Да, минус 3 вторых y, спасибо
[59:23.760 --> 59:25.040]  А здесь будет
[59:25.040 --> 59:26.720]  y минус
[59:26.720 --> 59:28.880]  Так, ну, средняя ноль
[59:28.880 --> 59:31.560]  Это все, если что, из матрицы к вариации беру
[59:31.560 --> 59:36.240]  на 2 сигма квадрата, то есть тоже на 4
[59:36.240 --> 59:39.040]  То есть, просто плотность случайно величины y записали
[59:39.040 --> 59:40.640]  Ну и вот
[59:40.640 --> 59:45.640]  от какой функции мы с вами берем мотаж?
[59:45.640 --> 59:46.840]  Что можно заметить?
[59:46.840 --> 59:48.840]  На самом деле, вот это можно сюда занести
[59:48.840 --> 59:51.760]  И это, по сути, у нас будет просто
[59:51.760 --> 59:55.200]  смещение, на самом деле, вот этой вот случайно величины y
[59:55.200 --> 59:56.480]  Это будет сдвиг
[59:56.480 --> 59:58.040]  Ну, почему это будет какой-то сдвиг?
[59:58.040 --> 01:00:01.280]  Потому что на самом деле это будет просто
[01:00:01.280 --> 01:00:02.680]  интеграл
[01:00:02.680 --> 01:00:05.200]  по r
[01:00:05.200 --> 01:00:07.520]  sin y
[01:00:07.520 --> 01:00:10.520]  Там собирать можно полный квадрат какой-нибудь
[01:00:10.520 --> 01:00:17.440]  А здесь у нас будет e в степени y минус 3
[01:00:17.440 --> 01:00:22.440]  Это с минусом, значит, там с плюсом будет плюс 3 в квадрате
[01:00:22.440 --> 01:00:23.640]  на 4
[01:00:23.640 --> 01:00:27.160]  Ну, ему тут еще лишнее слагаемое добавили, поэтому нужно еще на него поделить
[01:00:27.160 --> 01:00:31.560]  Ну, нужно на него еще намножить
[01:00:31.560 --> 01:00:38.560]  Так, а добавили мы с вами 9 четвертых
[01:00:39.160 --> 01:00:41.240]  Сейчас, подожду секундочку
[01:00:41.240 --> 01:00:42.920]  Почему плюс 3?
[01:00:42.920 --> 01:00:44.800]  Потому что здесь минус был
[01:00:44.800 --> 01:00:48.280]  Мы вот 3 вторых y сюда переносим
[01:00:48.280 --> 01:00:53.160]  И, получается, здесь будет y в квадрат плюс 6y
[01:00:53.160 --> 01:00:56.160]  Ну, здесь вот минус
[01:00:56.160 --> 01:00:59.160]  И вот здесь вот минус
[01:00:59.160 --> 01:01:03.040]  Тогда здесь будет y плюс 3 плюс 9 четвертых
[01:01:03.040 --> 01:01:05.760]  9 четвертых мы с вами здесь вычили, значит, надо намножить
[01:01:05.760 --> 01:01:07.440]  Ну, вынесли ее так как константа
[01:01:07.440 --> 01:01:09.160]  Вот такая штучка получилась
[01:01:09.160 --> 01:01:12.520]  А это у нас что такое? Это, на самом деле,
[01:01:12.520 --> 01:01:16.400]  9 четвертых математических ожиданий
[01:01:16.400 --> 01:01:21.560]  Нормальные случайные величины с параметрами 0
[01:01:21.560 --> 01:01:24.160]  Мат ожидания 3, минус 3
[01:01:24.160 --> 01:01:28.840]  Дисперсия 2
[01:01:28.840 --> 01:01:30.160]  Да, дисперсия 2
[01:01:30.160 --> 01:01:32.600]  А вот, ну вот
[01:01:32.600 --> 01:01:34.520]  Случайные величины вот такое распределение
[01:01:34.520 --> 01:01:39.920]  От функции sin y
[01:01:39.920 --> 01:01:41.280]  Вот, и теперь вопрос
[01:01:41.280 --> 01:01:43.400]  Как найти им от ожидания sin y
[01:01:43.400 --> 01:01:50.400]  Ну вот, если y имеет вот такое распределение?
[01:01:52.240 --> 01:01:55.520]  Такой интеграл, по-честному, считать не очень приятно
[01:01:55.520 --> 01:01:57.080]  Можно проще
[01:01:57.080 --> 01:02:00.800]  А проще, на самом деле, это просто вспомнить характеристические функции
[01:02:00.800 --> 01:02:04.960]  Мы с вами знаем, наверное, что sin y
[01:02:04.960 --> 01:02:08.120]  По формуле Эйлера
[01:02:08.120 --> 01:02:11.520]  Это e в степени
[01:02:11.520 --> 01:02:15.400]  И получается y
[01:02:15.400 --> 01:02:18.880]  Минус e в степени минус i y
[01:02:18.880 --> 01:02:22.400]  На 2e
[01:02:22.400 --> 01:02:24.320]  Справедливо?
[01:02:24.320 --> 01:02:31.320]  Ну, наверное, мы можем все этому тоже навесить
[01:02:36.000 --> 01:02:41.160]  Пока что никаких интегралов с комплексными числами у нас нет
[01:02:41.160 --> 01:02:43.640]  Ну, там можно, в общем, да, ладно, будем считать
[01:02:43.640 --> 01:02:47.560]  Короче, здесь мы трюками пользуемся
[01:02:47.560 --> 01:02:51.520]  Вот, здесь у нас будет математическое ожидание e в степени i y
[01:02:51.520 --> 01:02:55.040]  Минус математическое ожидание e в степени минус y
[01:02:55.040 --> 01:03:02.040]  Теперь, заметьте, что вот это у вас, по сути, характеристические функции
[01:03:02.040 --> 01:03:02.800]  Ну, похоже
[01:03:02.800 --> 01:03:07.040]  В качестве t здесь нужно взять единичку, а здесь в качестве t нужно взять минус единичку
[01:03:07.040 --> 01:03:08.560]  То есть, вот это на 1
[01:03:08.560 --> 01:03:12.680]  А вот это еще, по сути, с минус единичка идет
[01:03:12.680 --> 01:03:15.120]  И вот это это хар функция
[01:03:15.120 --> 01:03:18.960]  А хар функцию вы легко можете восстановить, если вы знаете параметры
[01:03:18.960 --> 01:03:23.720]  нормального распределения
[01:03:23.720 --> 01:03:26.000]  Справедливо?
[01:03:26.000 --> 01:03:29.400]  Ну и все
[01:03:29.400 --> 01:03:32.200]  Парни, вы что, издеваетесь?
[01:03:32.200 --> 01:03:36.400]  Я даю вам способ, как не считать интеграл
[01:03:36.400 --> 01:03:39.200]  Вот это получается у нас характеристическая функция
[01:03:39.200 --> 01:03:42.040]  Вот этой вот случайной величины в точке
[01:03:42.040 --> 01:03:43.880]  Ну, 1 получается
[01:03:43.880 --> 01:03:48.520]  Минус характеристическая функция
[01:03:48.560 --> 01:03:50.120]  В точке минус 1
[01:03:50.120 --> 01:03:56.320]  И поделить это все на 2e
[01:03:56.320 --> 01:04:00.840]  Справедливо?
[01:04:00.840 --> 01:04:04.560]  Ну, давайте вот это вот y штих у нас будет какое-то, видимо
[01:04:04.560 --> 01:04:08.440]  Чтобы для красоты, наверное, здесь лучше вот y штих уже везде писать
[01:04:08.440 --> 01:04:16.120]  Потому что мы немножко у него распределение, конечно, попортим
[01:04:16.200 --> 01:04:19.360]  Это вот такой трюк, что, типа, если вам нужно таким от ожиданий считать
[01:04:19.360 --> 01:04:21.880]  Чаще всего проще хар функциям обратиться
[01:04:21.880 --> 01:04:24.920]  И вот хар функцию y штих вы, наверное, знаете
[01:04:24.920 --> 01:04:26.120]  Хар функции y штих
[01:04:26.120 --> 01:04:28.400]  Ну, вот здесь такую сношечку напишу
[01:04:28.400 --> 01:04:33.120]  Характеристическая функция y штих
[01:04:33.120 --> 01:04:34.560]  В точке t
[01:04:34.560 --> 01:04:36.840]  Ну, это просто характеристическая функция нормальной случайной величины
[01:04:36.840 --> 01:04:38.080]  Вот с такими параметрами
[01:04:38.080 --> 01:04:41.880]  То есть это е в степени минус i at
[01:04:41.880 --> 01:04:45.920]  Ну, то есть, видимо, 3t
[01:04:45.920 --> 01:04:51.280]  Потому что средняя i at минус sigma квадрат
[01:04:51.280 --> 01:04:54.720]  Ну, то есть 2t квадрат пополам
[01:04:54.720 --> 01:04:57.080]  Ну, то есть, минус t квадрат
[01:04:57.080 --> 01:05:03.520]  Сейчас я проверю, что у него то же самое было
[01:05:03.520 --> 01:05:05.280]  Вроде да
[01:05:05.280 --> 01:05:08.600]  Ну и все, и соответственно, вот ваша характеристическая функция
[01:05:08.600 --> 01:05:11.160]  Берете ее в точке 1
[01:05:11.160 --> 01:05:15.760]  В точке 1 минус t квадрат
[01:05:15.760 --> 01:05:19.240]  Ну и давайте вот здесь равенство продолжу писать
[01:05:19.240 --> 01:05:23.160]  А я уже продолжил
[01:05:23.160 --> 01:05:26.680]  Так, идем дальше
[01:05:26.680 --> 01:05:28.640]  Значит, вот константа, которую мы вынесли
[01:05:28.640 --> 01:05:34.320]  И теперь пользуемся вот этим вот утверждением
[01:05:34.320 --> 01:05:40.040]  Так, характеристическая функция в точке 1 у нас будет
[01:05:40.040 --> 01:05:43.360]  Ну, видимо, так, мы делим это все на двои
[01:05:43.400 --> 01:05:47.280]  Характеристическая функция в точке 1 это будет е в степени
[01:05:47.280 --> 01:05:56.320]  i3 минус 1
[01:05:56.320 --> 01:05:57.840]  Сейчас одну секунду, как-то у меня по-другому
[01:05:57.840 --> 01:05:59.920]  Сейчас, сейчас, сейчас решаем
[01:05:59.920 --> 01:06:04.120]  Минус, в точке минус 1
[01:06:04.120 --> 01:06:12.280]  Здесь будет минус 3i минус 1
[01:06:12.280 --> 01:06:14.600]  Да, справедливо
[01:06:14.600 --> 01:06:25.520]  Дальше, что я сделал
[01:06:25.520 --> 01:06:28.920]  Да, е в минус 1 мы вынесем
[01:06:28.920 --> 01:06:33.360]  У нас в скобочках останется е в степени 3 минус е в степени 3
[01:06:33.360 --> 01:06:35.120]  Вот, ну, заметим следующее
[01:06:35.120 --> 01:06:38.360]  Что у нас на самом деле опять через косинус и синус и распишем
[01:06:38.400 --> 01:06:44.080]  Е в степени 3 и вот это е в степени минус 3
[01:06:44.080 --> 01:06:46.680]  Косинус у нас сократится и останется только синус трех
[01:06:46.680 --> 01:06:54.600]  Ну, ок
[01:06:54.600 --> 01:07:07.200]  У меня получилось, что это синус в точке а будет
[01:07:07.200 --> 01:07:09.840]  Я просто в общем виде дорешивал
[01:07:09.840 --> 01:07:11.600]  Сейчас давайте поймем, правильно это или неправильно
[01:07:11.600 --> 01:07:24.640]  Так, так, так, так, так, так, так
[01:07:24.640 --> 01:07:35.920]  Может быть и синус трех
[01:07:35.920 --> 01:07:39.280]  Ну, давай посмотрим быстро
[01:07:39.280 --> 01:07:43.600]  Е в степени минус 1 мы вынесли 3 минус 3
[01:07:43.600 --> 01:07:47.320]  То есть у нас будет косинус 3 минус косинус 3
[01:07:47.320 --> 01:07:49.320]  А здесь будет потом синус 3
[01:07:49.320 --> 01:07:53.920]  Синус 3, прошу прощения, плюс синус 3
[01:07:53.920 --> 01:07:56.080]  Ну, кажется, да, синус трех будет
[01:07:56.080 --> 01:07:57.920]  Короче
[01:07:57.920 --> 01:08:04.600]  Да, справедливо
[01:08:04.600 --> 01:08:10.840]  Правда, у меня почему-то получилось, что здесь синуса
[01:08:10.840 --> 01:08:12.840]  должен получиться, то есть вот этого параметра
[01:08:12.840 --> 01:08:20.480]  Ничего странного, подожди
[01:08:20.480 --> 01:08:25.280]  Можешь сейчас быстренько дорешать тогда и вот понять,
[01:08:25.280 --> 01:08:25.840]  что здесь минусом
[01:08:25.840 --> 01:08:28.960]  Но в целом этот нотограмм мы почти досчитали
[01:08:28.960 --> 01:08:32.960]  А ответ получился
[01:08:32.960 --> 01:08:45.440]  Ну, видимо, 5 четвертых
[01:08:45.440 --> 01:08:52.000]  Ну, вот, на синус 3 или синус минус 3
[01:08:52.000 --> 01:08:54.800]  У меня почему-то синус минус 3 получилось
[01:08:54.800 --> 01:08:57.960]  Так, кто-нибудь может проверить решение, пожалуйста
[01:08:57.960 --> 01:09:10.760]  Так, кто-нибудь может проверить решение, пожалуйста
[01:09:10.760 --> 01:09:17.080]  А, одну секунду
[01:09:27.960 --> 01:09:40.360]  В общем, здесь ашки вылазят, здесь минус, я вот здесь потерял
[01:09:40.360 --> 01:09:44.240]  Вот в определении характеристической функции здесь вот минус 3 будет
[01:09:44.240 --> 01:09:45.840]  Потому что здесь же i, a, t будет
[01:09:45.840 --> 01:09:50.560]  Понятно?
[01:09:50.560 --> 01:09:52.560]  Да
[01:09:52.560 --> 01:09:56.720]  Вот здесь вот там же i, a, t, а это как раз минус а будет
[01:09:56.720 --> 01:10:01.800]  Ну, а это минус 3, прошу прощения, значит будет минус i, 3
[01:10:01.800 --> 01:10:03.960]  Да, и здесь значит будет плюс, здесь минус
[01:10:03.960 --> 01:10:05.840]  И итоговый ответ с минус троечка будет
[01:10:05.840 --> 01:10:06.600]  Вот такой
[01:10:06.600 --> 01:10:11.960]  Все, первый интеграл мы с вами посчитали
[01:10:11.960 --> 01:10:13.080]  Ответ вот такой
[01:10:13.080 --> 01:10:18.640]  А как считать второй интеграл, понятно?
[01:10:18.640 --> 01:10:33.560]  Так, я сейчас сразу пометку сделаю, потому что у меня
[01:10:33.560 --> 01:10:39.520]  Я на четверть ошибся немножко там с коэффициентом
[01:10:39.520 --> 01:10:47.520]  А это у нас задача 5 четвертых получилось
[01:10:47.520 --> 01:10:52.360]  Все остальное вроде верно
[01:10:52.360 --> 01:10:59.000]  Так, теперь давайте на второй множественный посмотрим
[01:10:59.000 --> 01:11:05.800]  То есть на второй мотож, который нам надо посчитать
[01:11:05.800 --> 01:11:10.160]  А, как посчитать его?
[01:11:10.160 --> 01:11:14.640]  Ну, на самом деле здесь уже должно быть не очень сложно
[01:11:14.640 --> 01:11:23.040]  Просто по определению u, мы с вами u нашли вот в таком
[01:11:23.040 --> 01:11:25.360]  вот виде, то есть это математическое ожидание
[01:11:25.360 --> 01:11:32.200]  E в степени x минус альфа y плюс глаза
[01:11:32.200 --> 01:11:39.680]  Понятно, что вот это так, во-первых, давайте такое вот замечание
[01:11:39.680 --> 01:11:48.640]  Если вас просят найти математическое ожидание E в степени случайно величина
[01:11:48.640 --> 01:11:52.720]  Как это можно посчитать через то, с чем мы работали сегодня?
[01:11:52.720 --> 01:11:57.920]  Ну да, кажется, что это просто характеристическая функция x
[01:11:57.920 --> 01:11:59.840]  Вопрос только в какой точке?
[01:11:59.840 --> 01:12:04.840]  Если на E будет минус и, видимо
[01:12:04.840 --> 01:12:11.840]  Вот, то есть осталось только понять какое распределение
[01:12:11.840 --> 01:12:13.840]  имеет вот эта случайная величина, которая у вас
[01:12:13.840 --> 01:12:19.920]  стоит в показателе экспонента Но любая линейная комбинация
[01:12:19.920 --> 01:12:21.760]  ну альфа у нас, кстати, 3 вторых, так что здесь надо
[01:12:21.760 --> 01:12:28.200]  переписать Да, это нормальное распределение
[01:12:28.200 --> 01:12:30.840]  Осталось только понять с какими параметрами
[01:12:30.840 --> 01:12:37.840]  Сейчас найдем математическое ожидание x плюс 3 вторых, y плюс 2z
[01:12:37.840 --> 01:12:41.160]  Ну так как у нас исходные мы тоже все были нулевые
[01:12:41.160 --> 01:12:45.040]  по линейности это будет просто сумма нулей, то есть 0
[01:12:45.040 --> 01:12:47.600]  Нужно найти дисперсию Ну самый верный способ найти
[01:12:47.600 --> 01:12:50.760]  дисперсию этих штук это просто посчитать к вариации
[01:12:50.760 --> 01:12:51.760]  этой штуки самой собой
[01:12:51.760 --> 01:13:08.800]  Все вот это у нас, если что, есть Типа кавариация xx, кавариация
[01:13:08.800 --> 01:13:13.080]  xy, все это в материке есть И я дома это посчитал
[01:13:13.080 --> 01:13:17.400]  и у меня получилось, что дисперсия равна 37 вторых
[01:13:17.400 --> 01:13:21.400]  В общем меня можно проверить Но идея здесь такая, раскрываем
[01:13:21.400 --> 01:13:25.040]  это просто по линейности и все кавариации у вас известны
[01:13:25.040 --> 01:13:30.640]  Ну не будем дальше считать, я дома не зря это считал
[01:13:30.640 --> 01:13:38.080]  Вот, супер, а характеристическая функция
[01:13:38.080 --> 01:13:46.840]  Не, нет, кавариация случайной величины самой собой это
[01:13:46.840 --> 01:13:50.480]  дисперсия Ну просто посмотрите на определение
[01:13:50.480 --> 01:13:51.800]  кавариации и на определение дисперсии
[01:13:51.800 --> 01:14:05.600]  Так, сейчас справедливо все вроде бы Значит, чтобы
[01:14:05.600 --> 01:14:08.480]  найти вот это математическое ожидание, значит нам нужно
[01:14:08.480 --> 01:14:11.120]  характеристическую функцию Понять как выглядит характеристическая
[01:14:11.120 --> 01:14:17.520]  функция вот эта случайная величина x плюс 3 вторых y
[01:14:17.600 --> 01:14:26.880]  плюс 2z в точке t пока в точке t Вот, так как это имеет нормальное
[01:14:26.880 --> 01:14:33.080]  распределение с параметрами 0 37 вторых, если я не ошибся
[01:14:33.080 --> 01:14:41.400]  в подсчете дисперсии, тогда характеристическая функция
[01:14:41.400 --> 01:14:45.680]  у нас имеет следующий вид Е в степени, ну вот, средняя
[01:14:45.680 --> 01:14:48.480]  ноль, поэтому то, что с минимой частью отпадает,
[01:14:48.480 --> 01:14:56.360]  минус 37 вторых в квадрате, а нет, в квадрат не надо
[01:14:56.360 --> 01:14:59.120]  возводить, просто минус 37 вторых, потому что это у
[01:14:59.120 --> 01:15:05.560]  нас так уже дисперсия пополам Короче, на 4, 37 четвертый,
[01:15:05.560 --> 01:15:08.200]  да Вот еще одну багу у себя нашел
[01:15:08.200 --> 01:15:15.160]  37 четвертых Вот
[01:15:15.160 --> 01:15:17.960]  И, соответственно, мы второй интеграл с вами посчитали
[01:15:17.960 --> 01:15:28.160]  t квадрат, ну мы поставляем ему, да, хорошо, давай t квадрат
[01:15:28.160 --> 01:15:33.520]  И дальше мы вот эту характеристическую функцию рассматриваем в точке минус и
[01:15:33.520 --> 01:15:49.600]  Ну и у нас получается, да, получается Е в степени 37 четвертых
[01:15:49.600 --> 01:16:01.200]  Ну и тогда итоговый ответ
[01:16:01.200 --> 01:16:03.560]  Это есть просто произведение вот тех двух интегралов,
[01:16:03.560 --> 01:16:05.360]  которые мы с вами посчитали У первого интеграла у нас
[01:16:05.360 --> 01:16:11.920]  получился синус минус 3 на Е в степени 5 четвертых,
[01:16:11.920 --> 01:16:14.160]  а у второго у нас математическое ожидание получилось Е в
[01:16:14.160 --> 01:16:18.320]  степени 37 четвертых Вот, ну это, видимо, еще как-то
[01:16:18.320 --> 01:16:22.760]  даже сокращается Итого ответ синус от минус трех
[01:16:22.760 --> 01:16:29.080]  умножить на Е в степени 42 четвертых, да, 21 вторая,
[01:16:29.760 --> 01:16:33.920]  ну вот ответ То есть идея такая, у вас сначала какая-то
[01:16:33.920 --> 01:16:37.240]  сложная функция, вы одно из слагаемых разваливаете
[01:16:37.240 --> 01:16:41.160]  на независимую-зависимую компоненту с тем, что осталось,
[01:16:41.160 --> 01:16:43.480]  и затем пользуетесь независимостью этих компонентов, чтобы
[01:16:43.480 --> 01:16:46.480]  матожи по отдельности считать А по отдельности матожи чаще
[01:16:46.480 --> 01:16:48.680]  всего какие-нибудь вот такие через хард-функции считаются
[01:16:48.680 --> 01:16:51.680]  Это вот как один из способов подсчетов
[01:16:51.680 --> 01:16:53.680]  Окей?
[01:16:59.080 --> 01:17:06.360]  Так, ну все С характеристическими функциями мы тоже закончили
[01:17:06.360 --> 01:17:10.840]  Обсудили, в принципе, все, что хотели Так, а вот здесь
[01:17:10.840 --> 01:17:13.040]  у меня еще, я помню, я балгу не заметил
[01:17:13.040 --> 01:17:21.080]  Теница здесь должна быть Ну и дальше все поправить
[01:17:21.080 --> 01:17:30.920]  Так, ну и у нас последняя тема, которую час не подскажете
[01:17:30.920 --> 01:17:51.720]  Нормально, к 12 закончим Может быть, кому-то это пригодится
[01:17:51.720 --> 01:18:06.680]  Так, это все, это все Да, ну мы все более-менее разберем
[01:18:06.680 --> 01:18:15.000]  сегодня, я надеюсь Ну там в зависимости, у меня 6 задач,
[01:18:15.000 --> 01:18:19.720]  допустим, на контрольных обычно Ну я там придумываю
[01:18:19.800 --> 01:18:24.880]  Обычно что-то, чтобы скучно не было Там, чтобы что-то
[01:18:24.880 --> 01:18:29.520]  порешать было, а то Если все все решат, то не интересно
[01:18:29.520 --> 01:18:34.080]  будет Так, ну и у нас последняя тема
[01:18:34.080 --> 01:18:37.280]  На самом деле она очень большая Давайте мы вот ограничимся
[01:18:37.280 --> 01:18:41.120]  40 минутами на ее обсуждение Поэтому детально мы, наверное,
[01:18:41.120 --> 01:18:44.280]  ее прям очень рассмотреть не сможем Но как решать
[01:18:44.280 --> 01:18:52.560]  задачу мы поймем однозначно Все остальное мы в принципе
[01:18:52.560 --> 01:18:57.800]  достаточно подробно рассмотрели Так, а во сколько начала
[01:18:57.800 --> 01:18:58.800]  у нас было?
[01:18:58.800 --> 01:18:59.800]  19-20?
[01:18:59.800 --> 01:19:00.800]  19-15?
[01:19:00.800 --> 01:19:09.560]  Смотри, еще час Не, мы там 6 сидели тогда
[01:19:09.560 --> 01:19:13.560]  Я сегодня против бить рекорды Сегодня хочу спать
[01:19:13.560 --> 01:19:18.200]  Так, условные математические ожидания
[01:19:18.200 --> 01:19:23.560]  Последняя тема, и у нас там будет 2 маленьких задачки
[01:19:23.560 --> 01:19:26.440]  и одна вот прям хорошая задачка Прям настоящая
[01:19:26.440 --> 01:19:27.440]  Как?
[01:19:27.440 --> 01:19:28.440]  3 задачи?
[01:19:28.440 --> 01:19:29.440]  3 задачи?
[01:19:29.440 --> 01:19:37.200]  Ну 2 очень маленькие, очень короткие Держись здесь
[01:19:37.200 --> 01:19:41.400]  Надо размяться
[01:19:41.400 --> 01:19:50.600]  Не, а вот статистика будет у вас Вот почему по статистике
[01:19:50.600 --> 01:19:52.600]  семинар шел 6 часов?
[01:19:52.600 --> 01:19:54.240]  Потому что на самом деле там уже просто очень много
[01:19:54.240 --> 01:19:57.500]  материала Я на самом деле даже не все рассказываю
[01:19:57.500 --> 01:20:00.440]  сегодня что хотел В планах было гораздо больше
[01:20:00.440 --> 01:20:03.960]  Вот, но если все рассказывать нормально, то типа курсы
[01:20:03.960 --> 01:20:07.400]  становятся все сложнее И типа за час уже не расскажешь
[01:20:08.400 --> 01:20:11.160]  пол семестра Такие дела
[01:20:11.160 --> 01:20:19.640]  Это семинар века, он будет, я думаю до моего выпуска
[01:20:19.640 --> 01:20:23.120]  это произойдет 24 часа я думаю мы будем
[01:20:23.120 --> 01:20:27.920]  тервер рассказывать как-нибудь Не, надо, надо, надо как-то
[01:20:27.920 --> 01:20:31.280]  это Терверство, ты там все
[01:20:31.280 --> 01:20:38.760]  Так, сейчас я еще выдохну и продолжим
[01:21:01.760 --> 01:21:09.840]  Ну поехали Так, условное математическое ожидание
[01:21:09.840 --> 01:21:19.200]  Значит, если вы помните, то в начале мы с вами, когда
[01:21:19.200 --> 01:21:22.000]  вот только с какими-то условностями знакомились, мы рассматривали
[01:21:22.000 --> 01:21:28.520]  объекты вигда, то есть просто условные вероятности одного
[01:21:28.520 --> 01:21:32.080]  события относительно другого Ну вот, здесь что угодно может
[01:21:32.080 --> 01:21:34.400]  быть, допустим какое-нибудь событие А
[01:21:34.400 --> 01:21:37.800]  Вот, ну и вы помните, что это определялось каким-нибудь
[01:21:37.800 --> 01:21:46.680]  вот таким вот образом Если вероятность B больше нуля,
[01:21:46.680 --> 01:21:49.600]  а если она равна нулю, тогда это определялось как угодно
[01:21:49.600 --> 01:21:52.560]  Ну мы нулем обычно полагали Вот, и на самом деле если
[01:21:52.560 --> 01:21:55.400]  рассмотреть вот такую функцию, у которой вот сюда любое множество
[01:21:55.400 --> 01:22:01.120]  принимает, то это ведь частная мера будет
[01:22:01.120 --> 01:22:04.920]  Ну потому что, грубо говоря, у вас есть нож, что все элементарных
[01:22:04.920 --> 01:22:06.760]  исходов Теперь вы знаете, что у вас
[01:22:06.760 --> 01:22:11.360]  произошло событие B, и вы вот здесь по сути
[01:22:11.360 --> 01:22:13.120]  нормируете на вероятность этого события B
[01:22:13.120 --> 01:22:16.120]  То есть теперь вы знаете, что все, что происходит, происходит
[01:22:16.120 --> 01:22:19.960]  внутри вот этого вот события И на самом деле это теперь
[01:22:19.960 --> 01:22:23.600]  мера, мы ее как бы вот отсюда все перенесли вот на этом
[01:22:23.600 --> 01:22:27.000]  Оно чтобы Вот, такие объекты мы рассматривали
[01:22:27.000 --> 01:22:29.680]  Что дальше можно быть логично рассматривать?
[01:22:29.680 --> 01:22:35.120]  Ну вот, как бы, что кажется логичным Логичным, например,
[01:22:35.120 --> 01:22:38.280]  может быть следующее Во-первых, вот здесь рассматривать
[01:22:38.280 --> 01:22:41.760]  событие вида х и равняется к в качестве событий А
[01:22:41.760 --> 01:22:44.960]  Просто какую-то дискретную случайную величину рассмотреть
[01:22:44.960 --> 01:22:49.000]  И ввести условное математическое ожидание следующим образом
[01:22:49.000 --> 01:23:05.680]  Вот так Ну, то есть, теперь мы здесь, вот у нас
[01:23:05.680 --> 01:23:07.960]  есть какое-то событие Рассмотрим какую-нибудь случайную величину
[01:23:07.960 --> 01:23:09.560]  дискретную Теперь все события,
[01:23:09.560 --> 01:23:11.160]  которые мы вот сюда будем подставлять будут вот такого
[01:23:11.160 --> 01:23:13.280]  вида Вот То есть на самом деле
[01:23:13.280 --> 01:23:15.600]  мы теперь получили какую-то новую дискретную случайную
[01:23:15.600 --> 01:23:17.160]  величину И вот у этой новой дискретной
[01:23:17.160 --> 01:23:20.360]  случайной величины, которую мы вот так обозначили необычно
[01:23:20.360 --> 01:23:23.520]  Можно определить математическое ожидание так же, как мы определяли
[01:23:23.520 --> 01:23:26.520]  математическое ожидание до этого Только по сути у нас
[01:23:26.520 --> 01:23:28.720]  у обычной случайной величины это х на вероятность того,
[01:23:28.720 --> 01:23:32.800]  что все равняется х А здесь у нас при условии b
[01:23:32.800 --> 01:23:37.240]  Вот И на самом деле уже в тот момент,
[01:23:37.240 --> 01:23:39.160]  когда мы с вами вот такие штуки рассматривали
[01:23:39.160 --> 01:23:41.240]  можно было перейти к условным математическим ожиданиям
[01:23:41.240 --> 01:23:46.680]  Но вот по программе мы так не делаем обычно
[01:23:46.680 --> 01:23:53.120]  Дальше Смотрите, здесь у нас вот это событие
[01:23:53.120 --> 01:23:57.880]  b оно фиксировано На самом деле давайте попробуем
[01:23:57.880 --> 01:24:02.720]  с вами рассмотреть в качестве множества b
[01:24:02.720 --> 01:24:05.880]  событие вот такого вида Какая-то другая дискретная
[01:24:05.880 --> 01:24:08.120]  случайная величина равна какой-то константе
[01:24:08.120 --> 01:24:12.000]  Ну, почему не множество b?
[01:24:12.000 --> 01:24:15.280]  Ну, множество b может быть любым Вот, тогда у нас на самом
[01:24:15.280 --> 01:24:17.160]  деле уже появляется условное математическое ожидание
[01:24:17.160 --> 01:24:31.200]  вот такого вида Получается, что все равняются х
[01:24:32.200 --> 01:24:38.160]  И так, получается, это равняется х Поделить на вероятность
[01:24:38.160 --> 01:24:44.640]  того, что это равняется х Вот Ну и это уже какое-то
[01:24:44.640 --> 01:24:46.760]  условное математическое ожидание в дискретном случае
[01:24:46.760 --> 01:24:49.120]  одной случайной величины относительно другой случайной
[01:24:49.120 --> 01:24:50.560]  величины То есть если мы знаем, что там
[01:24:50.560 --> 01:24:55.680]  В общем, если мы знаем, что какая-то случайная величина
[01:24:55.680 --> 01:24:58.280]  приняла какое-то значение и она как-то связана вот
[01:24:58.280 --> 01:25:01.480]  с этой случайной величиной То как бы вот эта вероятность
[01:25:01.480 --> 01:25:07.280]  каким-то образом учитывается вот в этой случайной величине
[01:25:07.280 --> 01:25:09.800]  Ну ладно, сейчас это мы аккуратно скажем чуть
[01:25:09.800 --> 01:25:15.360]  позже Вот А какая мораль?
[01:25:15.360 --> 01:25:16.680]  Какая у всего этого мораль?
[01:25:16.680 --> 01:25:20.160]  Ну, на самом деле условное математическое ожидание
[01:25:20.160 --> 01:25:27.640]  это случайная величина Понятно ли это?
[01:25:27.640 --> 01:25:34.080]  Хорошо, допустим, это понятно Здравствуйте
[01:25:34.080 --> 01:25:36.200]  Дальше давайте следующий переход сделаем
[01:25:36.200 --> 01:25:38.360]  Ладно, про случайную величину будем чуть позже поговорим
[01:25:38.360 --> 01:25:42.160]  Давайте следующий переход сделаем Заметим, что вот если
[01:25:42.160 --> 01:25:44.600]  у нас в условии стоит какая-то случайная величина
[01:25:44.600 --> 01:25:51.080]  Случайная величина на самом деле порождает сигма-алгебру
[01:25:51.080 --> 01:25:53.400]  Просто сигма-алгебра, порожденная случайной величиной, мы
[01:25:53.400 --> 01:25:56.360]  ее определяем таким вот образом Это просто про образы
[01:25:56.480 --> 01:26:04.960]  всех баррельских множеств Поэтому, на самом деле, в общем
[01:26:04.960 --> 01:26:08.740]  случае гораздо лучше рассматривать не условное
[01:26:08.740 --> 01:26:10.920]  от ожидания относительно случайных величин А сразу
[01:26:10.920 --> 01:26:12.760]  условное математическое ожидание относительно
[01:26:12.760 --> 01:26:15.200]  сигма-алгебра Потому что, если это относительно
[01:26:15.200 --> 01:26:19.800]  случайная величина То это относительно вот такой
[01:26:19.800 --> 01:26:22.440]  сигма-алгебры, а если у вас какая-то другая сигма-алгебра
[01:26:22.440 --> 01:26:24.600]  То как бы это и тот случай захватится
[01:26:24.600 --> 01:26:32.600]  То есть давайте теперь рассматривать УМО относительно Сигма-Алгебры.
[01:26:32.600 --> 01:26:43.600]  То есть это не то чтобы более общее, но вот какой-то более широкий взгляд на условные математические ожидания.
[01:26:43.600 --> 01:26:46.600]  Просто если у вас в условии стоит другая случайная величина, она порождает Сигма-Алгебру,
[01:26:46.600 --> 01:26:50.600]  и соответственно это будет просто каким-то частным случаем того, что мы сейчас будем делать.
[01:26:50.600 --> 01:26:52.600]  Вот, и давайте определение дадим уже.
[01:26:55.600 --> 01:26:56.600]  Давайте дадим определение.
[01:26:56.600 --> 01:27:04.600]  Итак, пусть у нас есть кси, это случайная величина, на вероятностном пространстве Омега ФП.
[01:27:07.600 --> 01:27:13.600]  Пусть мы с вами рассматриваем какую-то под Сигма-Алгебру В, то есть она меньше.
[01:27:14.600 --> 01:27:25.600]  Тогда УМО, тогда случайная величина ЭТО, которая обычно еще обозначается вот таким вот образом.
[01:27:31.600 --> 01:27:35.600]  То есть как бы это разное обозначение, мы ее обозначим ЭТО, она вот обычно обозначается вот так.
[01:27:35.600 --> 01:27:38.600]  Называется УМО, если выполнено два свойства.
[01:27:39.600 --> 01:27:52.600]  Первая. Вот эта штучка является измеримой относительно вот этой меньше Сигма-Алгебры, это важно, мы скоро поймем почему.
[01:27:55.600 --> 01:27:58.600]  И второе выполнено интегральное свойство.
[01:27:59.600 --> 01:28:19.600]  Интегральное свойство заключается в следующем, что если мы с вами считаем математическое ожидание кси на индикатор А, вот что за события А мы сейчас поймем, они должны совпадать с математическими ожиданиями ЭТО на индикатор А, где А принадлежит меньше Сигма-Алгебры.
[01:28:19.600 --> 01:28:24.600]  Так, сейчас, про оба свойства мы поговорим.
[01:28:26.600 --> 01:28:29.600]  Сначала ответим на вопрос, когда существует УМО.
[01:28:29.600 --> 01:28:47.600]  Ну вот на самом деле, если существует конечный абсолютный момент, то есть утверждение, если математическое ожидание модуля кси меньше бесконечного, то это значит, что у нас есть конечный абсолютный момент.
[01:28:48.600 --> 01:28:56.600]  Модуля кси меньше бесконечности, тогда УМО существует и почти, наверное, единственно. То есть оно существует и единственно, почти, наверное.
[01:29:00.600 --> 01:29:05.600]  То есть мы разобрались, что такие объекты действительно существуют чаще всего, если с мат. ожиданиями все хорошо.
[01:29:05.600 --> 01:29:10.600]  А во-вторых, что УМО, ну вот это объект, который должен вылитворять УМО таким свойством.
[01:29:10.600 --> 01:29:13.600]  Дальше я предлагаю посмотреть на примерчик.
[01:29:14.600 --> 01:29:18.600]  И на этом примере понять, в чем суть.
[01:29:30.600 --> 01:29:33.600]  Да, потому что это на самом деле Сигма-Алгебра, порожденная случайно-величной.
[01:29:34.600 --> 01:29:36.600]  Это в принципе то же самое.
[01:29:44.600 --> 01:29:46.600]  Давайте рассмотрим с вами вот такой пример.
[01:29:47.600 --> 01:29:51.600]  Пусть у нас Омега, это просто четыре точки.
[01:29:58.600 --> 01:30:04.600]  Ну все равновероятны пусть будут, то есть Сигма-Алгебра это у нас просто будет в два степени Омега.
[01:30:06.600 --> 01:30:11.600]  А мера, ну равновероятна. То есть мера всех точек.
[01:30:13.600 --> 01:30:15.600]  Равна одной четверти.
[01:30:18.600 --> 01:30:21.600]  И собственно предлагается рассмотреть под Сигма-Алгебру вот такую.
[01:30:21.600 --> 01:30:41.600]  Здесь у вас восемь, так сколько здесь у вас элементов получается?
[01:30:41.600 --> 01:30:46.600]  Здесь у вас 16 элементов, а вот здесь у нас будет 4 элемента.
[01:30:46.600 --> 01:30:49.440]  будет 4 элемента всего. И вот предлагается вот такую
[01:30:49.440 --> 01:30:51.480]  подсигма-алгебру, в этой сигма-алгебре рассмотреть.
[01:30:51.480 --> 01:30:55.320]  Вот. И нужно еще какую-нибудь случайную величину кси
[01:30:55.320 --> 01:31:00.680]  задать. Ну давайте зададим с вами кси от омегаитова
[01:31:00.680 --> 01:31:04.520]  ну просто равно 1. То есть от омега 1 это будет 1, от омега
[01:31:04.520 --> 01:31:07.080]  2 это будет 2, от омега 3 это будет 3, от омега 4 это будет
[01:31:07.080 --> 01:31:11.960]  4. Давайте рисуночку сделаем. Рисуночка такой. То есть
[01:31:11.960 --> 01:31:16.600]  получается наша омега 1, вот наша омега 2, вот наша
[01:31:16.600 --> 01:31:24.720]  омега 3, вот наша омега 4. Четыре точки. Ну и получается
[01:31:24.720 --> 01:31:27.280]  наша кси их переводит, то есть они все равно вероятны
[01:31:27.280 --> 01:31:29.760]  между собой, на них вероятность одинаково задана. И наша
[01:31:29.760 --> 01:31:38.760]  кси переводит их в 3, в 2, в 1 и в 4 соответственно.
[01:31:38.800 --> 01:31:43.920]  И теперь вот наша под сигasst沒關係 по сути какие
[01:31:44.000 --> 01:31:53.160]  под множество здесь рассматривает. То есть в исходной сиг vér
[01:31:53.240 --> 01:31:57.480]  алгебре у нас здесь были множество всех этих 4-х точек.
[01:31:57.560 --> 01:31:59.560]  А вот в этой под сигassed алгебре есть множество,
[01:31:59.560 --> 01:32:01.560]  омега1, омега4. Такое множество есть. Ну пустое множество.
[01:32:01.720 --> 01:32:05.800]  Вот такое множество. Ну и все. Ну в смысле, и все 4 точки.
[01:32:05.800 --> 01:32:08.400]  Окей? И теперь, в чем суть?
[01:32:10.480 --> 01:32:12.480]  Что такое условное математическое ожидание?
[01:32:14.760 --> 01:32:16.760]  X относительно
[01:32:20.920 --> 01:32:22.920]  вот данной сигма-алгебры.
[01:32:23.480 --> 01:32:27.340]  Это какая-то случайная величина, которая как бы усредняет вот на этих...
[01:32:28.480 --> 01:32:30.480]  на этих множествах, которые на самом деле...
[01:32:30.880 --> 01:32:37.240]  Ну вот эта сигма-алгебра, она более крупная, в ней меньше мелких элементов, ну вот это видно. То есть здесь у вас просто 16 элементов, там и
[01:32:37.480 --> 01:32:42.240]  одна точка есть, и всевозможные... В общем, здесь все комбинации, точка есть, а здесь вот только
[01:32:42.480 --> 01:32:47.640]  сигма-алгебра более крупная, она меньше элементов в себе содержит. Вот. Она как бы должна
[01:32:49.240 --> 01:32:56.520]  с точки зрения определения, ну, во-первых, быть измеримой, но проблемы с измеримостью в дискретном случае практически не бывает, там это чуть позже обсудим,
[01:32:56.520 --> 01:32:58.520]  то есть пока что измеримость не совсем существенна,
[01:32:59.520 --> 01:33:02.480]  но она должна как бы с точки зрения математического ожидания
[01:33:03.440 --> 01:33:10.040]  совпадать с исходной случайной величиной. То есть давайте посмотрим на интегральное свойство. О чем говорит интегральное свойство?
[01:33:13.360 --> 01:33:15.360]  Интегральное свойство говорит о том, что математическое ожидание
[01:33:17.560 --> 01:33:19.560]  кси
[01:33:19.720 --> 01:33:24.280]  на индикатор А должно равняться математическому ожиданию это на индикатор А.
[01:33:25.080 --> 01:33:27.080]  Любого А
[01:33:27.440 --> 01:33:31.240]  принадлежащая вот эта вот более крупная сигма-алгебра, ну, вот эта.
[01:33:35.440 --> 01:33:37.440]  То есть, грубо говоря,
[01:33:39.320 --> 01:33:42.400]  с точки зрения средних значений каких-то
[01:33:43.640 --> 01:33:45.360]  они совпадают.
[01:33:45.360 --> 01:33:51.640]  То есть, грубо говоря, если у вас есть, вот пример можно какой-нибудь другой еще придумать, что если у вас вот выпадает кубик,
[01:33:52.640 --> 01:33:59.960]  а под сигма-алгебр можно рассмотреть, например, четные числа и отдельно нечетные, то средние значения у вас будут отличаться.
[01:34:02.560 --> 01:34:05.240]  Как бы, ну ладно, я думаю это понятно.
[01:34:06.040 --> 01:34:10.240]  То есть, если вы знаете, что у вас выпали четные значения, то у вас
[01:34:10.960 --> 01:34:16.480]  средняя, как бы это, во-первых, у вас средняя это будет теперь случайная величина, то есть у мой это случайная величина, потому что
[01:34:17.480 --> 01:34:22.640]  если вы, допустим, предположили, что выпали четные числа, то тогда у вас
[01:34:23.440 --> 01:34:25.760]  математическое ожидание равно, ну сколько там, два
[01:34:27.080 --> 01:34:29.080]  плюс четыре, плюс шесть?
[01:34:30.280 --> 01:34:32.880]  Да, четыре. А если выпало один, три, пять?
[01:34:34.720 --> 01:34:37.840]  Три. Вот. Соответственно, почему это случайная величина?
[01:34:37.960 --> 01:34:42.800]  Потому что если у вас выпадут четные числа, то у вас, грубо говоря, значение вашего ума это
[01:34:43.080 --> 01:34:49.040]  четыре, а если выпадает нечетное число, ну то есть вы знаете, что выпало нечетное число, то у вас, как бы, значение три.
[01:34:49.760 --> 01:34:53.160]  Вот. И сейчас вот, что такое интегральное свойство, чуть-чуть позже скажем.
[01:34:54.200 --> 01:34:59.320]  Но идея в чем? Идея в том, что у вас есть информация не вся. То есть, на самом деле, сигма-алгебры
[01:34:59.440 --> 01:35:03.120]  это, на самом деле, какой-то аналог той информации, которую вы владеете об эксперименте.
[01:35:03.480 --> 01:35:08.120]  То есть, вот исходная сигма-алгебра, она все говорила. Она говорила, вот, выпала
[01:35:08.920 --> 01:35:10.120]  единичка,
[01:35:10.160 --> 01:35:15.840]  или выпала двойка, или выпала двойка, или тройка. То есть, она очень точно могла информацию дать вам об эксперименте.
[01:35:16.200 --> 01:35:22.360]  Вот эта более бедная сигма-алгебра, она может вам только более грубые предсказания дать. То есть, она может сказать, что выпало либо
[01:35:22.360 --> 01:35:25.520]  единичка, либо четверка, либо двойка, либо тройка. То есть, выпало только четное число.
[01:35:26.960 --> 01:35:28.960]  Вот. И с этой точки зрения,
[01:35:29.280 --> 01:35:31.520]  условное математическое ожидание, это как бы
[01:35:33.000 --> 01:35:35.000]  лучший предсказатель значения
[01:35:36.000 --> 01:35:41.640]  с точки зрения того, что у вас вот информации меньше, то есть, чем вы хотели, у вас информация как бы ограниченное количество,
[01:35:42.480 --> 01:35:46.040]  не вся. И вот, УМО, это на самом деле
[01:35:46.680 --> 01:35:51.760]  наилучший предсказатель в средне квадратичном подходе, но это на статистике будет сказано. То есть, наилучшие предсказания,
[01:35:52.560 --> 01:35:55.440]  если вы знаете, вот, не всю информацию об эксперименте.
[01:35:56.160 --> 01:36:01.640]  И давайте вот на этом примере попробуем понять, какое распределение у нас будет у условного математического ожидания.
[01:36:05.000 --> 01:36:10.000]  То есть, мы уже с вами поняли, что на самом деле это у вас определено, почти наверно, единственное.
[01:36:10.000 --> 01:36:15.000]  То есть, на самом деле, за счет вот интегрального свойства измеримости, это сдается однозначно.
[01:36:16.000 --> 01:36:18.000]  Во-первых, это у нас
[01:36:18.000 --> 01:36:22.000]  Это у нас это случайная величина в нашем случае, действует изомегабар.
[01:36:24.000 --> 01:36:26.000]  Вот, вот, вот, вот.
[01:36:27.000 --> 01:36:29.000]  И вот, вот, вот, вот, вот.
[01:36:30.000 --> 01:36:32.000]  И вот, вот, вот.
[01:36:34.000 --> 01:36:36.000]  Вот, вот, вот.
[01:36:38.000 --> 01:36:40.000]  Вот, вот, вот.
[01:36:41.000 --> 01:36:43.000]  Вот, вот.
[01:36:43.000 --> 01:36:48.000]  Случайная величина в нашем случае действует из-за мегаметра.
[01:36:48.000 --> 01:36:52.000]  Давайте поймем, какое распределение у нее будет.
[01:36:52.000 --> 01:36:56.000]  То есть у нас у кси было равномерное распределение на этих четырех точках,
[01:36:56.000 --> 01:37:01.000]  то есть это было равномерное распределение на значениях 1, 2, 3, 4.
[01:37:01.000 --> 01:37:04.000]  Почему это действует из-за мегаметра, а не из-за...
[01:37:04.000 --> 01:37:08.000]  Случайная величина – это всегда отображение измеримое.
[01:37:08.000 --> 01:37:12.000]  Это не ограничено на сульную систему?
[01:37:12.000 --> 01:37:14.000]  Нет.
[01:37:16.000 --> 01:37:21.000]  Во-первых, просто потому что у вас всегда случайная величина –
[01:37:21.000 --> 01:37:25.000]  это измеримое отображение из пространства элементарных исходов В.
[01:37:26.000 --> 01:37:30.000]  Просто грубо говоря, оно у вас не умеет отличать вот это значение от вот этого.
[01:37:30.000 --> 01:37:40.000]  А, если вот вы его понимаете, у него в сульной сигналке в ренте нет...
[01:37:40.000 --> 01:37:44.000]  А, так может быть, у нас все есть пробилы.
[01:37:44.000 --> 01:37:48.000]  Потому что в аппаратке там дополнение не может привести туда.
[01:37:48.000 --> 01:37:50.000]  Понятно?
[01:37:50.000 --> 01:37:54.000]  Вот, ну и давайте поймем просто, какие значения она принимает.
[01:37:54.000 --> 01:37:58.000]  То есть нам нужно... Вот утверждается, что с учетом интегрального свойства
[01:37:58.000 --> 01:38:02.000]  и определения исходной случайной величины х, мы можем с вами распределение это
[01:38:02.000 --> 01:38:04.000]  однозначно восстановить.
[01:38:04.000 --> 01:38:08.000]  Ну, распределение это – это нам, по сути, вероятности омег мы с вами знаем.
[01:38:08.000 --> 01:38:12.000]  Нам нужно понять, какие значения у них будут.
[01:38:19.000 --> 01:38:22.000]  То есть смотрите, какие значения у нас будут.
[01:38:23.000 --> 01:38:27.000]  То есть смотрите, у омег у всех одинаковая вероятность 1 четверть.
[01:38:27.000 --> 01:38:31.000]  Вопрос, какие значения ваша случайная величина на этих омегах будет принимать.
[01:38:43.000 --> 01:38:49.000]  Ну вот, ответ на этот вопрос помогает найти интегральное свойство.
[01:38:50.000 --> 01:38:54.000]  Ну, смотрите, у нас какие множества а есть вот в этой мелкой сигма-алгебре?
[01:38:54.000 --> 01:38:57.000]  Пустое множество, наверное, нельзя рассматривать.
[01:38:57.000 --> 01:38:59.000]  Ну, здесь будет просто ноль.
[01:39:02.000 --> 01:39:09.000]  У нас есть множество, например, мы там рассматривали с вами омега 1, омега 4.
[01:39:11.000 --> 01:39:17.000]  С точки зрения интегрального свойства, математическое ожидание ψ
[01:39:17.000 --> 01:39:20.000]  на индикатор давайте вот это вот событие А обозначим.
[01:39:23.000 --> 01:39:28.000]  Должно совпадать с математическим ожиданием это на индикатор А.
[01:39:28.000 --> 01:39:31.000]  Ну, это просто по определению условного математического ожидания.
[01:39:31.000 --> 01:39:34.000]  Давайте поймем, чему равно то, что слева написано.
[01:39:34.000 --> 01:39:36.000]  То, что слева написано, чему равно?
[01:39:38.000 --> 01:39:41.000]  Ну, это просто сумма значений с вероятностями А.
[01:39:41.000 --> 01:39:44.000]  Ну, и только те омеги нам подходят, которые попадают сюда.
[01:39:44.000 --> 01:39:48.000]  То есть значение это у нас будет в силу того, что мы с вами определили случайно величину,
[01:39:48.000 --> 01:39:50.000]  что она омега и тому сопоставляет ита.
[01:39:50.000 --> 01:40:04.000]  То есть это будет единичка умножить на его вероятность на 1 четверть плюс А4 умножить на 1 четверть.
[01:40:06.000 --> 01:40:07.000]  Понятно?
[01:40:10.000 --> 01:40:11.000]  Вот так.
[01:40:12.000 --> 01:40:14.000]  Что у нас вот здесь записано?
[01:40:17.000 --> 01:40:21.000]  Здесь у нас записано с вами, ну, вот какое-то неизвестное.
[01:40:21.000 --> 01:40:25.000]  Это омега 1 на 1 четверть.
[01:40:27.000 --> 01:40:32.000]  Плюс это омега 4 тоже на 1 четверть.
[01:40:34.000 --> 01:40:36.000]  Ну, и как видите, они как бы должны быть равны.
[01:40:36.000 --> 01:40:38.000]  То есть вот здесь тоже можно на крайность поставить.
[01:40:38.000 --> 01:40:41.000]  Ну, и на 1 четверту, наверное, сейчас мы можем сократить.
[01:40:47.000 --> 01:40:48.000]  Вот.
[01:40:48.000 --> 01:40:49.000]  Одну секундочку.
[01:40:49.000 --> 01:40:55.000]  5 равняется это омега 1 плюс это омега 2.
[01:41:05.000 --> 01:41:06.000]  Окей?
[01:41:06.000 --> 01:41:07.000]  То есть смотрите, еще раз.
[01:41:07.000 --> 01:41:12.000]  У нас вот здесь с вами, так, где, где, где, где, так, сейчас.
[01:41:12.000 --> 01:41:13.000]  Вот 1,4.
[01:41:13.000 --> 01:41:15.000]  В среднем оно что здесь выпадает?
[01:41:15.000 --> 01:41:16.000]  2,5?
[01:41:17.000 --> 01:41:21.000]  В силу равновероятности у вас вот этот исход и вот этот они как бы равновероятны.
[01:41:28.000 --> 01:41:34.000]  Ну, отсюда на самом деле у вас это омега 1 совпадает с это омега 2.
[01:41:34.000 --> 01:41:36.000]  И равняется 2,5.
[01:41:37.000 --> 01:41:38.000]  Почему?
[01:41:38.000 --> 01:41:40.000]  А, они омега 4?
[01:41:40.000 --> 01:41:42.000]  Да, да, прошу прощения, они омега 4.
[01:41:42.000 --> 01:41:43.000]  Да.
[01:41:43.000 --> 01:41:44.000]  Я, я, я описался.
[01:41:45.000 --> 01:41:46.000]  Идеи непонятно?
[01:41:48.000 --> 01:41:49.000]  Почему они непонятны?
[01:41:51.000 --> 01:41:54.000]  Потому что они равновероятны и с точки зрения нашей сигма алгебры они неотличимы.
[01:41:54.000 --> 01:41:56.000]  То есть это по сути один и тот же элементарный исход.
[01:41:57.000 --> 01:42:00.000]  Сейчас, а как у нас определяется это вообще?
[01:42:00.000 --> 01:42:02.000]  Вообще, то есть, да.
[01:42:03.000 --> 01:42:05.000]  А предъявление у них есть?
[01:42:05.000 --> 01:42:06.000]  Еще раз.
[01:42:06.000 --> 01:42:10.000]  Это, это просто условно-математическое ожидание и оно должно удовлетворять двум условиям.
[01:42:10.000 --> 01:42:12.000]  Оно является сигмой измеримой?
[01:42:14.000 --> 01:42:15.000]  Так.
[01:42:16.000 --> 01:42:22.000]  А что оно означает, что там вот этот равенство, что оно равнов, там оно ожидание?
[01:42:22.000 --> 01:42:24.000]  Да, сейчас, сейчас я скажу.
[01:42:24.000 --> 01:42:25.000]  Вот это равенство?
[01:42:25.000 --> 01:42:32.000]  Нет, что оно равнов, там оно ожидание, кси при условных условиях.
[01:42:34.000 --> 01:42:36.000]  Это просто обозначение.
[01:42:36.000 --> 01:42:43.000]  Уму вот так обычно обозначают, но мы просто, чтобы не писать вот такую конструкцию, мы обозначили вот эту случайную величину через это.
[01:42:44.000 --> 01:42:45.000]  Окей.
[01:42:45.000 --> 01:42:47.000]  А почему это случайная величина?
[01:42:47.000 --> 01:42:49.000]  Мы пообщаем, как играют.
[01:42:50.000 --> 01:42:52.000]  Чуть-чуть позже, хорошо скажу.
[01:42:52.000 --> 01:42:59.000]  Идеально, потому что, смотри, у тебя значение ее, ну то есть это как бы среднее значение при условии того, что что-то произошло.
[01:43:00.000 --> 01:43:01.000]  Справедливо?
[01:43:01.000 --> 01:43:02.000]  Да.
[01:43:02.000 --> 01:43:04.000]  То есть у тебя среднее значение меняется от исхода.
[01:43:04.000 --> 01:43:10.000]  Видимо, там есть какая-то зависимость от исхода, следовательно, ну это грубо говоря случайная величина.
[01:43:10.000 --> 01:43:12.000]  Более детально чуть позже скажу.
[01:43:13.000 --> 01:43:14.000]  Вот.
[01:43:14.000 --> 01:43:16.000]  А почему у вас значение должно быть равное?
[01:43:16.000 --> 01:43:18.000]  То есть как бы такой вопрос может возникнуть.
[01:43:18.000 --> 01:43:21.000]  Ну смотрите, вот ладно, здесь как раз таки стреляет измеримость.
[01:43:22.000 --> 01:43:24.000]  Предположим, что они не равны.
[01:43:25.000 --> 01:43:29.000]  Предположим, что θ от ω1 равняется a,
[01:43:31.000 --> 01:43:35.000]  и это не равняется b, которому равняется θ от ω4.
[01:43:37.000 --> 01:43:38.000]  Да?
[01:43:38.000 --> 01:43:44.000]  По первому условию у нас функция должна быть измерима относительно меньше бедной фигмы алгебры.
[01:43:45.000 --> 01:43:47.000]  Но тогда возьми прообраз
[01:43:49.000 --> 01:43:50.000]  от a.
[01:43:50.000 --> 01:43:53.000]  Прообраз от a это будет только ω1.
[01:43:53.000 --> 01:43:55.000]  Потому что вот это принимает другое значение.
[01:43:56.000 --> 01:43:59.000]  А ω1 у нас уже не лежит вот в той фигма алгебре, которую мы с вами определили.
[01:44:03.000 --> 01:44:04.000]  Понятно?
[01:44:05.000 --> 01:44:06.000]  Непонятно.
[01:44:07.000 --> 01:44:09.000]  Ну то есть она не в субъектах значениях.
[01:44:09.000 --> 01:44:15.000]  Да, то есть они у вас, грубо говоря, на одном и том же множестве разбиения, они должны принимать одно и то же значение.
[01:44:15.000 --> 01:44:17.000]  Потому что если они будут принимать у вас разные значения,
[01:44:17.000 --> 01:44:20.000]  то у вас, грубо говоря, прообраз должен еще побиться,
[01:44:21.000 --> 01:44:23.000]  а у вас и так уже как бы мы разбиение с вами взяли.
[01:44:24.000 --> 01:44:26.000]  Мы сейчас это чуть-чуть аккуратнее докажем,
[01:44:27.000 --> 01:44:29.000]  подзадачка будет небольшая, мы это аккуратнее покажем.
[01:44:29.000 --> 01:44:34.000]  Но на самом деле они должны принимать одинаковое значение на областях разбиения.
[01:44:37.000 --> 01:44:38.000]  Да.
[01:44:38.000 --> 01:44:40.000]  Поэтому на самом деле вот это они равны
[01:44:41.000 --> 01:44:42.000]  двум с половиной.
[01:44:43.000 --> 01:44:45.000]  И что в принципе соотносится с тем, что мы с вами ожидали.
[01:44:45.000 --> 01:44:49.000]  Потому что, смотрите, если выпадает Омега-1, у нас случайная величина возвращает единичку.
[01:44:50.000 --> 01:44:52.000]  Если выпадает Омега-4, то наша случайная величина возвращает четверку.
[01:44:53.000 --> 01:44:55.000]  Значит, в среднем, если мы знаем, что произошло либо это, либо это,
[01:44:56.000 --> 01:44:58.000]  лучшее предсказание это два с половиной.
[01:44:59.000 --> 01:45:01.000]  Потому что оно ровно посередине между вот этим и вот этим.
[01:45:02.000 --> 01:45:04.000]  И, соответственно, оно будет как бы с наименьшей ошибкой
[01:45:05.000 --> 01:45:08.000]  вне зависимости от того, какой на самом деле Омега-1 или Омега-4 выпал.
[01:45:09.000 --> 01:45:10.000]  Окей?
[01:45:11.000 --> 01:45:13.000]  Ну и здесь на самом деле несложно понять, что здесь у нас 2 и 3.
[01:45:13.000 --> 01:45:15.000]  Здесь тоже будет 2 с половиной.
[01:45:16.000 --> 01:45:20.000]  Вот ровно по аналогичной, все если аналогично проделать, вы поймете это.
[01:45:21.000 --> 01:45:23.000]  Можно еще там пример какой-нибудь, ну сейчас не будем рассматривать.
[01:45:24.000 --> 01:45:26.000]  Например, вот с игральной костью тоже можно рассмотреть.
[01:45:27.000 --> 01:45:29.000]  Какие-нибудь дискретные примеры очень хорошо заходят.
[01:45:30.000 --> 01:45:34.000]  То есть, грубо говоря, за счет измеримости, то есть, ладно, измеримость на самом деле даже в дискретном случае существенна.
[01:45:35.000 --> 01:45:36.000]  Это первое.
[01:45:37.000 --> 01:45:39.000]  А второе, за счет интегрального свойства мы с вами восстановили значение,
[01:45:40.000 --> 01:45:42.000]  ну то есть вот здесь мы уже можем написать, что это 2 с половиной.
[01:45:43.000 --> 01:45:45.000]  И вот здесь, что это 2 с половиной.
[01:45:46.000 --> 01:45:48.000]  Мы с вами восстановили распределение вот этой случайной величины.
[01:45:52.000 --> 01:45:55.000]  То есть, чисто за счет вот тех двух условий, которые в определении даны.
[01:45:56.000 --> 01:46:00.000]  Ну и на самом деле, условно-математическое ожидание существует единственно почти наверно.
[01:46:01.000 --> 01:46:02.000]  Там.
[01:46:03.000 --> 01:46:06.000]  То есть, это единственная случайная величина, которая сюда подходит на такую роль.
[01:46:07.000 --> 01:46:09.000]  В том числе для недискретных.
[01:46:10.000 --> 01:46:13.000]  В том числе для недискретных, то есть, да, это в общем случае верно.
[01:46:14.000 --> 01:46:16.000]  Так, ну теперь мотивация понятна, да?
[01:46:17.000 --> 01:46:20.000]  То есть, грубо говоря, условно-мат ожидание это какое-то, это предсказание,
[01:46:21.000 --> 01:46:25.000]  при условии того, что у вас не полная информация о результатах эксперимента, а ограниченная.
[01:46:27.000 --> 01:46:28.000]  Вот.
[01:46:29.000 --> 01:46:31.000]  Ну и вроде нормальный пример рассмотрели.
[01:46:32.000 --> 01:46:36.000]  Собственно все, давайте теперь, какой-то гуманитарную часть мы закончили.
[01:46:36.000 --> 01:46:38.000]  Теперь можно более содержательные вещи какие-то порешать.
[01:46:39.000 --> 01:46:41.000]  Прям совсем содержательные.
[01:47:07.000 --> 01:47:08.000]  Да, мы сейчас этой докажем.
[01:47:09.000 --> 01:47:10.000]  Это как раз наша первая задача.
[01:47:12.000 --> 01:47:14.000]  Как раз таки наша первая задача вот такая.
[01:47:15.000 --> 01:47:16.000]  Вот пусть у нас...
[01:47:17.000 --> 01:47:18.000]  Так, задача.
[01:47:21.000 --> 01:47:24.000]  Пусть у нас вот эта сигма алгебра, которая в условии стоит,
[01:47:25.000 --> 01:47:27.000]  является сигма алгебры, порожденной разбиением.
[01:47:37.000 --> 01:47:40.000]  То есть, сигма алгебра, вот, порожденная разбиением,
[01:47:41.000 --> 01:47:44.000]  это вот ровно на самом деле какое-то более общее описание того случая, которое у нас было.
[01:47:45.000 --> 01:47:48.000]  Потому что у нас по сути какое разбиение порождало?
[01:47:49.000 --> 01:47:50.000]  Ну вот эти две точки и вот эти две точки.
[01:47:51.000 --> 01:47:52.000]  Это разбиение всего пространства.
[01:47:53.000 --> 01:47:56.000]  Ну потому что объединение их всех дает все пространство, и они не пересекаются.
[01:47:57.000 --> 01:47:59.000]  Давайте вот теперь с вами что-то более общее скажем.
[01:48:00.000 --> 01:48:03.000]  И пусть, теперь пусть у нас вот эта вот условная сигма алгебра,
[01:48:03.000 --> 01:48:05.000]  которая в условии стоит, порождена разбиением.
[01:48:10.000 --> 01:48:12.000]  Ну оно может не счетное, оно может быть не счетное.
[01:48:13.000 --> 01:48:15.000]  Прошу прощения, оно может быть счетное.
[01:48:20.000 --> 01:48:23.000]  Не счетное это плохо, а вот счетное оно может быть.
[01:48:26.000 --> 01:48:29.000]  Ну и вероятность каждого такого множества разбиения больше нуля.
[01:48:29.000 --> 01:48:33.000]  Ну это просто такое техническое требование, чтобы у нас там с условной вероятностью вопроса не возникло.
[01:48:34.000 --> 01:48:38.000]  И вот утверждается, что в дискретном случае, когда мы строим УМО относительно разбиения,
[01:48:39.000 --> 01:48:41.000]  мы можем с вами вот явно задать УМО.
[01:48:42.000 --> 01:48:47.000]  Ну то есть явно задать ту случайную величину, которую мы с вами рассматривали.
[01:48:52.000 --> 01:48:54.000]  В прошлой задаче мы ее вот это обозначали.
[01:48:55.000 --> 01:48:56.000]  Вот, ну теперь можно так записать.
[01:48:56.000 --> 01:49:03.000]  То есть это неважно, это в моем повествовании это абсолютно эквивалентные записи.
[01:49:08.000 --> 01:49:14.000]  Ну вот здесь n это как бы какой-то индекс, это не натуральные числа, а какой-то индекс.
[01:49:15.000 --> 01:49:21.000]  Давайте тогда n от единички до n, так будет лучше.
[01:49:26.000 --> 01:49:27.000]  Вот.
[01:49:29.000 --> 01:49:36.000]  То есть смотрите, в случае разбиения мы можем с вами понять, как вот явным образом выглядит наше условное математическое ожидание.
[01:49:37.000 --> 01:49:38.000]  Это мы сейчас докажем.
[01:49:40.000 --> 01:49:42.000]  То есть идея в чем?
[01:49:43.000 --> 01:49:46.000]  То есть ну вот это функция ОТОМИГА, то есть у нас какая-то случайная величина,
[01:49:47.000 --> 01:49:49.000]  а у нас есть вот эта функция ОТОМИГА,
[01:49:49.000 --> 01:49:54.000]  а у нас только одно слагаемое из этой суммы выживает, то в которое наше ОМИГА попало.
[01:49:55.000 --> 01:50:04.000]  И в нем у нас значение константное, вот такое, ну и как Илья сказал, это усреднение, просто еще нормированное на меру этого множества.
[01:50:06.000 --> 01:50:07.000]  Доказательства?
[01:50:07.000 --> 01:50:08.000]  Первое.
[01:50:09.000 --> 01:50:20.000]  В силу того, что, давайте просто проверим, давайте попробуем найти условное математическое ожидание, чтобы оно удовлетворяло определению условного математического ожидания.
[01:50:21.000 --> 01:50:23.000]  То есть вот это у нас есть.
[01:50:24.000 --> 01:50:35.000]  В силу того, что, давайте просто проверим, давайте попробуем найти условное математическое ожидание, чтобы оно удовлетворяло определение условного математического ожидания.
[01:50:36.000 --> 01:50:39.000]  То есть очевидно, что это представимо вот в таком виде.
[01:50:43.000 --> 01:50:44.000]  Какая-то константа.
[01:50:50.000 --> 01:50:51.000]  Мы не знаем какая константа.
[01:50:53.000 --> 01:50:55.000]  Но точно оно представимо вот в таком виде.
[01:51:00.000 --> 01:51:04.000]  Да, g порождена, давайте вот так, в сигму алгебру, порожденное вот таким разбиением.
[01:51:06.000 --> 01:51:07.000]  Спасибо за замечание.
[01:51:08.000 --> 01:51:09.000]  Час позднее уже.
[01:51:10.000 --> 01:51:11.000]  Час позднее уже.
[01:51:13.000 --> 01:51:14.000]  Шарики за ролики закатываются.
[01:51:18.000 --> 01:51:20.000]  Сейчас мы скажем, сейчас мы это скажем.
[01:51:20.000 --> 01:51:29.000]  Смотрите, то есть утверждается, что у mo, какое бы оно ни было, оно на самом деле это константа на индикатор множестве из разбиения.
[01:51:31.000 --> 01:51:33.000]  Как раз таки просто такая же логика, как у нас была до этого.
[01:51:34.000 --> 01:51:35.000]  Ну пусть не так.
[01:51:36.000 --> 01:51:39.000]  То есть пусть существует, доказательство,
[01:51:42.000 --> 01:51:43.000]  а неравная b,
[01:51:44.000 --> 01:51:48.000]  ну такое, что вот на одном dn у вас ваше это
[01:51:50.000 --> 01:51:51.000]  вещество.
[01:52:11.000 --> 01:52:13.000]  То есть вот пусть у вас есть какие-то элементарные исходы,
[01:52:15.000 --> 01:52:17.000]  которые принадлежат одному множеству из разбиения,
[01:52:17.000 --> 01:52:22.160]  значения. То есть она не константа, она ADN. То есть принимает два разных значения. Но у нас же
[01:52:22.160 --> 01:52:27.120]  должна быть измеримость. А что такое измеримость? Это значит, что прообраз
[01:52:27.120 --> 01:52:38.000]  должен принадлежать сигма-алгебре, порожденный... Короче, измеримость должна быть относительно
[01:52:38.000 --> 01:52:45.560]  вот такого чего-то. А значит, самые маленькие элементы у нас это здесь разбиение, ну вот,
[01:52:45.560 --> 01:52:49.960]  элементы разбиения. То есть к какому-то разбиению он должен принадлежать. То есть вот это множество,
[01:52:49.960 --> 01:52:57.560]  оно должно быть объединением элементов разбиения, вот этих вот. Но если у нас так получилось,
[01:52:57.560 --> 01:53:01.840]  что у нас ADN, грубо говоря, нужно еще поделить на ту часть, где у тебя принимает оно значение A и
[01:53:01.840 --> 01:53:06.600]  где принимает значение B, ну, значит, вот этим разбиением оно у тебя уже не порождается,
[01:53:06.600 --> 01:53:21.240]  это сигнал-алгебра. Проблема. Всё. Противоречие. С измеримостью.
[01:53:21.240 --> 01:53:40.840]  Ну оно содержит какое-то под... Смотри, у тебя получается DN распадается на D1 вот так сверху,
[01:53:40.840 --> 01:53:48.760]  и DN2, то есть вот здесь у тебя он принимает значение A, здесь принимает значение B. Ну и соответственно,
[01:53:48.760 --> 01:53:53.760]  про образ Ашки у тебя это вот это множество. Значит, оно у тебя должно лежать вот в этой сигнал-алгебре,
[01:53:53.760 --> 01:53:57.840]  а в этой сигнал-алгебре у тебя нет, потому что у тебя только вот это большое множество там лежит.
[01:53:57.840 --> 01:54:05.840]  Ну всё. Ну да, окей. По рукам. С измеримостью. То есть всё, что мы сейчас делаем, это просто
[01:54:05.840 --> 01:54:12.840]  проверяем определение умо. А теперь, по сути, что нам нужно сделать? Нам нужно подобрать такую
[01:54:12.840 --> 01:54:17.880]  константу, чтобы выполнялось интегральное свойство в условно-математическом ожидании. Ну вот,
[01:54:17.880 --> 01:54:22.640]  ровно то же самое, что мы с вами делали в том примере руками, сейчас мы более в общем виде сделаем.
[01:54:22.640 --> 01:54:31.200]  То есть должно выполняться интегральное свойство. Давайте просто запишем это интегральное свойство.
[01:54:31.200 --> 01:54:43.840]  Вот так. Смотрите, это мы уже поняли, нужно вот в такой форме искать. Значит, мы можем это
[01:54:43.840 --> 01:54:59.760]  переписать как сумма cн-ых на индикатор дн-ых на индикатор. Так, ну здесь. Замечание. Смотрите,
[01:54:59.760 --> 01:55:05.360]  вот здесь А у нас должно быть из вот этой сигнал-алгебры. Все элементы этой сигнал-алгебры
[01:55:05.360 --> 01:55:09.480]  это просто объединение каких-то вот этих элементов разбиения. Значит, справедливо,
[01:55:09.520 --> 01:55:18.240]  что вот интегральное свойство можно проверить только для какого-то декатова. Думаю,
[01:55:18.240 --> 01:55:30.240]  что справедливо. Вот. Ну и теперь заметим, что вот здесь у нас произведение индикаторов,
[01:55:30.240 --> 01:55:40.880]  соответственно выживет только один индикатор. Так, давайте я перейду. Да-да-да-да. Как раз и выживет
[01:55:40.880 --> 01:55:48.560]  у нас получается цк, констант вылезет, а там будет просто мотождание индикатора, то есть вероятность
[01:55:48.560 --> 01:55:58.960]  dk. Вот. Итого отсюда мы нашли с вами константу, которая нас интересует. Вот. Здесь равенство.
[01:55:58.960 --> 01:56:05.320]  Следовательно, если мы с вами вспомним, что мы начинали с вами вот отсюда и пришли с вами вот
[01:56:05.320 --> 01:56:15.040]  к этому, можно выразить цк. Цк, это у нас есть не что иное, как мотождание кси на индикатор
[01:56:15.040 --> 01:56:27.360]  dk поделить на вероятность dk. Ну и все. Мы с вами доказали. И на самом деле вот там сейчас свойства
[01:56:27.360 --> 01:56:31.720]  немножко обсудим условного математического ожидания. Там всегда все проверяется так. Если вы хотите
[01:56:31.720 --> 01:56:36.160]  доказать, что что-то является условным, ну вот, что условное математическое ожидание представимо
[01:56:36.160 --> 01:56:41.040]  в каком-то вот конкретном виде, то нужно проверить, что выполнено два свойства. Что выполнена первая
[01:56:41.040 --> 01:56:45.080]  измеримость, а второе, чтобы интегральное свойство выполнялось. Если это выполнено,
[01:56:45.080 --> 01:57:00.000]  то вы нашли умо, оно там единственное почти наверно. Окей? Супер. Так. Друзья, немного осталось.
[01:57:00.000 --> 01:57:13.720]  Это мы обсудили. Это мы обсудили. Теперь немножко про основные свойства. Их 10.
[01:57:13.720 --> 01:57:25.320]  Нет, доказывать не будем. Идея доказательств везде одинаковая. То есть вы проверяете просто вот
[01:57:25.320 --> 01:57:33.560]  то же, что мы сейчас сделали, измеримость и... Ну и это, наверное, можно будет найти где-нибудь в
[01:57:33.560 --> 01:57:38.640]  конспектах. По-любому это должно быть. Так что это не очень интересно. Но просто мы сейчас
[01:57:38.640 --> 01:57:43.720]  некоторыми свойствами будем пользоваться, поэтому, наверное, хотелось бы хотя бы их
[01:57:43.720 --> 01:57:49.960]  выписать. И примерно интуитивно хотя бы понимать, почему они верны, потому что на самом деле за ними
[01:57:49.960 --> 01:58:13.080]  интуиция стоит какая-то. А сколько сейчас времени? О, блин. Идем на рекорд. Нормально, ничего. Идем
[01:58:13.080 --> 01:58:21.760]  дальше. Так. С вашего позволения, уже час поздний, я просто выпишу свойства, буду писать, и мы будем
[01:58:21.760 --> 01:58:32.800]  их обсуждать. Основные свойства условного математического ожидания. Ну, поехали. Первое. Если
[01:58:32.800 --> 01:58:41.680]  кси измеримо относительно вот той вот более мелкой сигма-алгебры, которая у нас стоит в условии,
[01:58:41.680 --> 01:58:56.640]  то математическое ожидание кси при условии, это просто кси. Ну там почти, наверное. Ну идея такая,
[01:58:56.640 --> 01:59:01.360]  что если вам вот информация, которая в этой сигма-алгебре говорит, даёт всё как бы об
[01:59:01.360 --> 01:59:08.800]  эксперименте, который кси описывает, ну получается всё. То есть если вы берёте какую-то полную сигму-алгебру,
[01:59:08.800 --> 01:59:14.000]  которая измеримость, это как бы говорит о том, что они связаны между собой, и как бы вся информация,
[01:59:14.000 --> 01:59:17.440]  которая содержится в этой сигму-алгебре, позволяет восстановить вот эту случайную величину, например.
[01:59:17.440 --> 01:59:28.040]  И соответственно, вот в принципе логично это свойство выглядит. То есть у вас лучший предсказатель,
[01:59:28.040 --> 01:59:35.720]  если вы знаете всё о случайной величине, сама случайная величина. А второе. Ну это просто линейность.
[01:59:35.720 --> 01:59:45.600]  Я опускаю какие-то технические требования, но то, чтобы моторы по отдельности существовали,
[01:59:45.600 --> 01:59:51.200]  вот. Но это более-менее понятно, что это просто будет какая-то вот такая штука.
[01:59:58.040 --> 02:00:09.640]  Так, линейность обсудили. Ну тут обсуждать нечего. Дальше есть монотонность, что если у вас одна
[02:00:09.640 --> 02:00:16.600]  случайная величина меньше либо равна другой случайной величине, то тогда и их уможки,
[02:00:16.600 --> 02:00:26.280]  относительно одной и той же сигму-алгебре, будут связаны вот так. Ну тоже более-менее идейно всё
[02:00:26.280 --> 02:00:39.960]  понятно? Какой-то переход с неравенствами. Так, вот дальше интересное условие, интересное
[02:00:39.960 --> 02:00:48.480]  свойство. Так, это третье было. Четвертое. А если кси не зависит от вот той сигмы-алгебры,
[02:00:48.480 --> 02:01:03.120]  которая стоит в условии, то у нас математическое ожидание кси при условии сигмы-алгебры вырождается
[02:01:03.120 --> 02:01:08.240]  в математическое ожидание кси. Ну то есть идея такая. Информация, которая вот здесь содержится,
[02:01:08.240 --> 02:01:14.320]  ничего не говорит вам о вот этой случайной величине. Тогда лучше предсказания, лучше прогноз,
[02:01:14.320 --> 02:01:18.560]  это просто среднее. Ну какое есть. Вы ничего дополнительного не имеете, значит,
[02:01:18.560 --> 02:01:24.080]  если вы там будете минимизировать квадратичное отклонение вашего предсказания относительно
[02:01:24.080 --> 02:01:36.280]  истинного значения, то ничего лучше вот среднего нет. Ну смотри. Событие ксиприн
[02:01:36.280 --> 02:01:40.080]  принадлежит какому-то баррелевскому множеству, для любого баррелевского множества.
[02:01:40.080 --> 02:01:47.200]  Сиприн принадлежит баррелевскому множеству независимо с любым событием из этой сигмы-алгебры.
[02:01:47.200 --> 02:02:04.400]  Да, да. Ну короче, для любого, для любого. Да. А из вот этой сигмы-алгебры? Ну то есть независимо
[02:02:04.400 --> 02:02:07.680]  из двух сигмалегебров легко определяется? То есть это просто независимо из всех,
[02:02:07.680 --> 02:02:13.440]  парная независимость всех элементов оттуда и оттуда. Вот. А как по случайной величине построить
[02:02:13.440 --> 02:02:18.080]  сигма, ну событие какое-то? Можно вот такие рассмотреть? Да, ну это по сути прообразы,
[02:02:18.080 --> 02:02:31.000]  потому что здесь вы когда омеги вычисляете, так и раз. Вот. Это более-менее понятно. Следующее
[02:02:31.000 --> 02:02:37.640]  свойство. Пятое. Телескопическое свойство. Вот им мы сегодня пользоваться не будем,
[02:02:37.640 --> 02:02:46.640]  оно скорее такое теоретическое, что если у вас есть вот сигма-алгебра F, в нее может быть вложено
[02:02:46.640 --> 02:02:52.560]  две сигма-алгебры. Вот, допустим, одна вложена вот так, а в нее вложена сигма-алгебра вот так.
[02:02:52.560 --> 02:03:00.600]  То есть вот это это вообще самая мелкая, ну наоборот самая крупная, она меньше всего
[02:03:00.600 --> 02:03:03.880]  элементов содержит, относительно вот этой. Это самая мелкая, потому что она много мелких
[02:03:03.880 --> 02:03:07.880]  элементов содержит, а это самая крупная. Тогда телескопическое свойство заключается в следующем.
[02:03:07.880 --> 02:03:28.840]  И второе телескопическое свойство.
[02:03:37.880 --> 02:03:44.160]  Выглядит точно так же. Вот. То есть грубо говоря, самая крупная сигма-алгебра,
[02:03:44.160 --> 02:03:47.400]  которая меньше всего элементов содержит, то есть там только самые большие множества,
[02:03:47.400 --> 02:03:52.760]  маленьких там нет, она ожирает более мелкие сигма-алгебры.
[02:03:52.760 --> 02:04:06.280]  Так, ну и там еще буквально пару свойств осталось.
[02:04:06.280 --> 02:04:17.320]  Так, формула полной вероятности. Если вы возьмете математическое ожидание
[02:04:17.320 --> 02:04:27.200]  от условного математического ожидания, то это будет просто математическое ожидание
[02:04:27.200 --> 02:04:32.680]  исходной случайной величины. Но это вроде более-менее понятно. То есть на каждом
[02:04:32.680 --> 02:04:38.000]  куске мы грубо говоря предсказываем как надо, если мы их усердним, то мы получим мотож.
[02:04:38.000 --> 02:04:44.800]  Вот. И осталось, ну давайте наверное вот последнее свойство, седьмое.
[02:04:44.800 --> 02:04:53.920]  Которое утверждает следующее, что измеримый множитель можно вытаскивать из-под условия.
[02:04:53.920 --> 02:05:06.280]  То есть если у вас из-под, вернее вот, если у вас кси в данном примере, если у вас кси
[02:05:06.400 --> 02:05:17.260]  измеримо относительно условия, тогда кси можно выносить. Ну то есть про кси вы грубо
[02:05:17.260 --> 02:05:23.720]  говоря все знаете, поэтому, ну поверьте мне, о ней можно не усерднять, мы просто берем ее значение.
[02:05:23.720 --> 02:05:29.440]  А про это мы знаем меньше, поэтому вот ее давайте как-то усерднять и искать
[02:05:30.200 --> 02:05:40.680]  А про кси мы все знаем, вытащим ее и все. Возьмем значение. Вот такие вот 7 свойств. Давайте задачка.
[02:05:40.680 --> 02:05:47.920]  Дальше сейчас первая задачка будет просто поприменять эти свойства. Это самая банальная
[02:05:47.920 --> 02:05:54.600]  и базовая задача. Я бы наверное отдельно отметил, что вот самым, ну сегодня нам будет больше всего
[02:05:54.600 --> 02:06:11.320]  приглашаться вот это свойство, а вот это свойство. То есть относительно измеримый, независимый и вот
[02:06:11.320 --> 02:06:18.320]  это свойство. И линейность нам еще пригодится. То есть вот эти четыре свойства, их как бы,
[02:06:18.400 --> 02:06:25.120]  особенно сейчас на них внимание уделить, пока я не стер доски. Сейчас отру с доски и начнем решать.
[02:06:25.120 --> 02:06:31.840]  То есть линейность, вот это, вот это и вот это. Так-с.
[02:06:31.840 --> 02:06:49.440]  Задачка номер один. Это уже не номер один, это вторая в данном разделе.
[02:06:49.440 --> 02:07:08.480]  Кси независимый сакка. Кси имеет такое же распределение как это и распределены как нормальная
[02:07:08.480 --> 02:07:18.360]  случайная величина с параметрами 34. Нас просят найти условное математическое ожидание,
[02:07:18.360 --> 02:07:39.240]  кси минус это в квадрате при условии. Решение. Как будем решать?
[02:07:48.360 --> 02:07:55.040]  Ну это все просто на свойства. То есть смотрите, давайте сначала вот это просто раскроем то,
[02:07:55.040 --> 02:08:02.280]  что внутри. Это будет кси в квадрате минус два кси, это плюс это в квадрате при условии это.
[02:08:02.280 --> 02:08:08.120]  Дальше я сказал обратить внимание на линейность. То есть это у нас будет несколько
[02:08:08.120 --> 02:08:14.880]  математических условных математических ожиданий. Вот таких вот. Минус два условных
[02:08:14.880 --> 02:08:25.880]  математических ожидания кси это при условии это. Плюс условное математическое ожидание это в квадрат
[02:08:25.880 --> 02:08:34.240]  при условии это. Справедливо? Первое раскрываем по независимости, то есть это просто будет
[02:08:34.240 --> 02:08:40.600]  математическое ожидание кси в квадрате. Да, последнее по измеримости у нас раскроется.
[02:08:40.600 --> 02:08:50.040]  Со вторым что делаем? Да, выносим измеримый множитель. Измеримый множитель это, его мы вынесли,
[02:08:50.040 --> 02:08:54.760]  и осталось мотождание кси при условии это. Кси не зависит от это, поэтому это мотождание кси.
[02:08:54.760 --> 02:09:05.240]  А последнее по измеримости, да. Ну, халява, да. Ну, можно досчитать это быстро. То есть смотрите,
[02:09:05.240 --> 02:09:12.920]  у нас кси и это имеют вот такое распределение. Тогда математическое ожидание кси это 3. Это нам
[02:09:12.920 --> 02:09:20.280]  сейчас пригодится. Так, и нам нужно мотождание квадрата. Ну, давайте так. Дисперсия кси это 4,
[02:09:20.280 --> 02:09:29.880]  а это то же самое, что и мотождание кси в квадрате минус мотождание кси в квадрате. Вот это у нас 9.
[02:09:29.880 --> 02:09:42.640]  Следовательно, математическое ожидание кси в квадрате это у нас 4 плюс 9. 13.
[02:09:42.640 --> 02:09:52.440]  Вроде проверили мы все. Так, ну и все. Мы получили, что ответ это в квадрате. Минус,
[02:09:52.440 --> 02:10:05.680]  так, 3. Ну, минус, видимо, 6 это. Плюс 13. Все. Ну, и вот как бы у нас получилась какая-то случайная
[02:10:05.680 --> 02:10:22.000]  величина. Вот. И тут вот, наверное, интересно заметить. Следующее. Обратите внимание,
[02:10:22.000 --> 02:10:27.520]  что у нас условное математическое ожидание по сути получилось функцией от условия. Если в
[02:10:27.520 --> 02:10:35.520]  условии стоит случайная величина, то уможка это функция от этой, функции от условия. Потому
[02:10:35.520 --> 02:10:48.600]  что тяремка есть такая. Пусть у вас есть две случайные величины. Кси является этой измеримой
[02:10:48.600 --> 02:11:02.560]  тогда и только тогда, когда существует f-баррельская, такая что кси равняется f от
[02:11:02.560 --> 02:11:08.120]  этой. То есть, грубо говоря, одна случайная величина измерима относительно другой,
[02:11:08.120 --> 02:11:13.280]  только тогда, когда она является функцией баррельской от условия. Ну, вот так. Поэтому,
[02:11:13.280 --> 02:11:22.120]  когда мы будем искать умо относительно именно случайных величин, то у нас будет получаться
[02:11:22.120 --> 02:11:29.400]  функция от условия. Функция от тех случайных величин, которые стоят в условии. Так, супер,
[02:11:29.400 --> 02:11:41.720]  с этим разобрались. Дальше. Почти перешли уже к последней задаче. Этот листочек рассмотрели.
[02:11:41.720 --> 02:11:56.680]  Так, тот листочек вроде тоже. Супер. Вот, теперь давайте немножко про гауссовские вектора поговорим
[02:11:56.680 --> 02:12:07.640]  в контексте условных математических ожиданий. Потому что, смотрите, как бы те свойства, которые
[02:12:07.640 --> 02:12:12.000]  мы изучили у гауссовского вектора, а именно вот то, то свойство самое главное, которое было
[02:12:12.000 --> 02:12:18.200]  связано с независимостью, на самом деле помогает решать вот задачи на умо, потому что вы вот
[02:12:18.200 --> 02:12:25.480]  раскладываете составляющую на зависимую и независимую часть. Ту, которая зависит, вы по измеримости,
[02:12:25.480 --> 02:12:35.480]  грубо говоря, считаете, а ту, что не зависит, вы просто в от ожиданий превращаете. Вот. То есть,
[02:12:35.480 --> 02:12:44.560]  опять-таки, идея ровно та же, что и просто в гауссовских векторах изначально. Задача.
[02:12:44.560 --> 02:13:05.360]  Привет, последняя. Так, задача. Пусть у нас есть гауссовский вектор. Гауссовский вектор с
[02:13:05.360 --> 02:13:15.840]  параметрами. Ну, короче, вектор средний вот такой, а матрица к вариации вот такая. И у нас проще найти
[02:13:15.840 --> 02:13:31.120]  условные математические ожидания вот такого вида. Так, как будем решать?
[02:13:36.360 --> 02:13:48.880]  Не, матрицу подбирать не надо. Ну, надо, но не надо. Ну, смотрите, давайте так. Утверждение.
[02:13:51.080 --> 02:14:03.360]  Ну, вернее так, план решения. План решения. Вот пусть у вас x,y это гауссовский вектор.
[02:14:03.360 --> 02:14:13.600]  Понятно, что вот, если вы из вот этих двух компонентов смотрите вектор, то есть, первая
[02:14:13.600 --> 02:14:16.880]  компонента будет x minus это, а вторая x minus это, это тоже будет гауссовский вектор, потому что это
[02:14:16.880 --> 02:14:23.960]  просто линейное образование исходного гауссовского вектора. Да, идея абсолютно такая же. То есть,
[02:14:23.960 --> 02:14:29.280]  если x,y у нас гауссовский вектор, и вас просят посчитать математическое ожидание x при условии y,
[02:14:29.280 --> 02:14:37.800]  то делается, да, это действительно таким вот образом, чтобы вы разбиваете x на что-то, что зависит от y,
[02:14:37.800 --> 02:14:50.400]  плюс что-то, что не зависит от y. Ну, вот ровно так же, как мы делали с вами до этого. Ну и все. И это,
[02:14:50.400 --> 02:14:54.680]  на самом деле, очень просто дальше дорешивается, потому что это будет альфа-ю по измеримости,
[02:14:54.760 --> 02:15:03.760]  а второе слагаемое, это просто математическое ожидание вот этой вот независимой составляющей.
[02:15:03.760 --> 02:15:12.640]  Теперь осталось применить вот этот план к нашей задаче. И решить я, собственно. Так, решение.
[02:15:12.640 --> 02:15:27.160]  Мы, видимо, хотим с вами то, что у нас вот x минус это представить в следующем виде. Альфа на
[02:15:27.160 --> 02:15:41.200]  то, что в условии, то, что в условии у нас x плюс это, плюс у. У должно быть независимо с x плюс это.
[02:15:41.200 --> 02:15:57.280]  Да, да. Да, да, да. Так, ну понятно, в каком виде мы будем искать. У этого у нас будет просто x
[02:15:57.280 --> 02:16:07.440]  минус это, минус альфа, x плюс это. Такого вида будем искать. Вот, ну и все. И дальше,
[02:16:07.440 --> 02:16:15.480]  если мы хотим независимости, нам достаточно потребовать, чтобы кавариация x плюс это у у нас
[02:16:15.480 --> 02:16:29.160]  определенно вот таким вот образом была равна нулю. Ну и давайте, наверное, досчитывать не будем.
[02:16:29.160 --> 02:16:35.880]  Вот понятно, что делать, да? Если мы вот это проскрываем просто по белинейности,
[02:16:35.880 --> 02:16:48.520]  то мы с вами все кавари... Не, ладно, давайте засчитаем. Точно? Не, матричку никуда не надо. Все,
[02:16:48.520 --> 02:16:58.440]  решаем дальше. Так, кавариация. В смысле, мы же матричку не искали. Просто вот это уравнение
[02:16:58.440 --> 02:17:10.400]  решаем и находим альфа. Да, хорошо. Отсюда можно альфа найти. Вот какой-то альф отсюда получится.
[02:17:10.400 --> 02:17:17.080]  Супер. И дальше, ну вот, переходим к плану решения, который у нас вот здесь был. То есть,
[02:17:17.080 --> 02:17:25.920]  мы с вами рассматриваем. А, ну просто мы пишем дальше. Вот это равняется то, что нас интересует.
[02:17:25.920 --> 02:17:31.080]  Х мы разложили вот так. То есть, мы теперь это считаем как математическое ожидание,
[02:17:31.080 --> 02:17:39.480]  условное. Вот с тем коэффициентом, который мы нашли. Альфа на условия. То есть, здесь будет кс
[02:17:39.480 --> 02:17:51.480]  плюс это. Плюс у. Ну, у мы вот в такой форме искали. Си минус это. Минус альфа на кс плюс это.
[02:17:51.480 --> 02:18:01.440]  При условии. Так, а в условии у нас было кс плюс это. Ну супер. Дальше по линейности. Вот это будет
[02:18:01.440 --> 02:18:07.200]  измеримо. Оно просто превратится в альфа кс плюс это. А вот это независимо, поэтому это просто будет
[02:18:07.200 --> 02:18:10.440]  математическое ожидание этой штуки. Ну и дальше останется только от ожидания этой штуки посчитать,
[02:18:10.440 --> 02:18:16.160]  но это понятно как делать. Это просто сумма компонент гауссского вектора. Она тоже будет
[02:18:16.160 --> 02:18:19.760]  иметь нормальное какое-то распределение с каким-то параметром. Нужно будет этот параметр найти,
[02:18:19.760 --> 02:18:26.120]  но если альфа знаем, то вот в от ожидания диспер все это штуки можно найти. Ну и все. А там
[02:18:26.120 --> 02:18:30.800]  только мот ожидания достаточно будет. Нам же это мот ожидания выродится. Мот ожидания легко находится.
[02:18:30.800 --> 02:18:38.160]  Ну вроде все, да? Не будем тогда дорешивать. Хорошо, что вы поняли как такие задачи решать.
[02:18:38.160 --> 02:18:56.640]  Так, давайте ответ сейчас скажу. Что-то там на 7 делить надо было. 2 минус кс плюс это – ответ.
[02:18:56.640 --> 02:19:08.280]  2 минус кс плюс это на 7. Ну вот и опять то, что мы с вами обсуждали. Ответ – это функция от
[02:19:08.280 --> 02:19:15.280]  условия какая-то. Ну все замечательно. Задачу решили. Вот теперь еще немножко теории и задачи.
[02:19:15.280 --> 02:19:24.720]  Вот. Но вряд ли вам вот такую задачку дадут на контрольной. Скорее всего на контрольной нужно
[02:19:24.720 --> 02:19:40.720]  будет вот там что-то с плотностями поделать. С условными плотностями. Наверное, у меня нет
[02:19:40.720 --> 02:19:46.720]  времени мотивировать, что такое условная плотность и для чего она нужна. Но зато я
[02:19:46.720 --> 02:19:54.080]  могу быстренько аналогию какую-то провести несложно, чтобы было чуть понятнее. То есть,
[02:19:54.080 --> 02:20:04.440]  помните мы в начале семинара с вами рассматривали? Ну вот мы в начале семинара рассматривали
[02:20:04.440 --> 02:20:13.400]  условные математические ожидания. Вот эти обе дискретные были. А это равняется y,
[02:20:13.400 --> 02:20:30.200]  как сумма, ну по сути, x. Так, на вероятность x при условии это равняется y. Мы там как-то
[02:20:30.200 --> 02:20:40.360]  это мотивировали, что вот существует такой объект. И на самом деле вот мы могли с вами
[02:20:40.560 --> 02:20:47.400]  умо, по сути, зная эти условные вероятности, мы могли с вами посчитать умо в дискретном случае
[02:20:47.400 --> 02:20:53.380]  просто по определению условного математического ожидания. Хочется в абсолютно непрерывном случае
[02:20:53.380 --> 02:20:58.440]  иметь что-то похожее, какой-то похожий объект, и вот похожий объект есть, и этот объект называется
[02:20:58.440 --> 02:21:02.960]  условная плотность. То есть так же, как и в дискретном случае у нас были вероятности в непрерывном плотности,
[02:21:02.960 --> 02:21:09.060]  когда обе случайные величины были дискретные, у нас здесь были вероятности, а теперь
[02:21:09.060 --> 02:21:12.580]  мы с вами перейдем к плотностям условно.
[02:21:27.420 --> 02:21:35.540]  Так, сейчас, наверное, нужно условную плотность все-таки аккуратно как-то ввести.
[02:21:39.060 --> 02:21:49.060]  Ладно, давайте так ведем ее тогда. Условная плотность, то есть сейчас пока что вот это
[02:21:49.060 --> 02:22:03.740]  немножко в сторонке стоит, условная плотность обозначается вот так. Это такая функция,
[02:22:03.740 --> 02:22:20.700]  что вероятность того, что ксикрина держит b при условии того, что вторая случайная величина равна
[02:22:20.700 --> 02:22:25.820]  y, это просто интегральчик по множеству b, это условие плотности.
[02:22:25.820 --> 02:22:39.300]  Вот так. Ну, то есть идея какая? Грубо говоря, условная плотность — это плотность вот этой
[02:22:39.300 --> 02:22:45.820]  случайной величины при условии того, что с это происходит что-то, это приняло какое-то конкретное
[02:22:45.820 --> 02:22:58.780]  значение. То есть, грубо говоря, если у вас есть какое-нибудь, допустим, двумерное распределение,
[02:22:58.780 --> 02:23:03.820]  и вот, допустим, одна случайная величина у вас по х, а вторая по y, тогда если вы знаете,
[02:23:03.820 --> 02:23:08.420]  что вот та, что по y приняла какое-то конкретное значение, то теперь, по сути, у вас вот здесь
[02:23:08.420 --> 02:23:12.740]  какая-то новая плотность. Ну, допустим, у вас какой-нибудь такой, в общем, какая плотность у вас,
[02:23:12.740 --> 02:23:19.340]  поверхность в двухмерном пространстве, в трехмерном пространстве, поверхность такая,
[02:23:19.340 --> 02:23:23.300]  которая удовлетворяет условиям плотности, то есть она интегрируется в единичку и не отрицательно.
[02:23:23.300 --> 02:23:27.940]  Вот. И вы знаете, допустим, что у вас вот одна случайная величина приняла значение вот такое.
[02:23:27.940 --> 02:23:33.860]  Тогда вы как бы делаете такой срез этой плотности, и вот, по сути, ваша новая плотность, она вот,
[02:23:33.860 --> 02:23:46.780]  вот это срез, вот это вот, сейчас, вот эта поверхность двумерная. То есть эта двумерная
[02:23:46.780 --> 02:23:52.420]  поверхность заметает под собой какую-то область, и вот если вы знаете, что вторая случайная величина
[02:23:52.420 --> 02:23:55.940]  приняла какое-то значение, то вы, грубо говоря, режете и смотрите на этот срез, и эта плотность,
[02:23:55.940 --> 02:24:00.660]  по сути, ваша при условии того, что вторая случайная величина принимает такое значение. Вот.
[02:24:00.660 --> 02:24:09.020]  То есть если какое-нибудь распределение в круге посмотреть, предположим, что вы
[02:24:09.020 --> 02:24:17.420]  рассматриваете какую-нибудь случайную точку в четверть окружности. Тогда, если вы знаете,
[02:24:17.420 --> 02:24:23.220]  что у вас y-ковая координата приняла какое-то вот такое конкретное значение, то ваше распределение
[02:24:23.220 --> 02:24:27.780]  находится теперь, значит, вот здесь. Ваша плотность, это, по сути, вот какое-то равномерное распределение
[02:24:27.780 --> 02:24:32.540]  вот на этой штучке. К сечению, да? Ну да, да, да, по сути, сечение. То есть это плотность, вот,
[02:24:32.540 --> 02:24:41.900]  ну да. Это теперь равномерное распределение вот на этой вот прямой. Такой вот объект. Для чего
[02:24:41.900 --> 02:24:49.780]  его, собственно, вводится? Ну, чтобы, на самом деле, в непрерывном случае у нас был аналог вот такой
[02:24:49.780 --> 02:25:03.860]  формулы. Ну, мы в начале ее, на самом деле, мы руками помахали в самом начале. Да, это обе случайно
[02:25:03.860 --> 02:25:09.980]  величины дискретны. Да, ну, примерно до этого можно дойти, это не очень сложно. Вот. Ну, более-менее
[02:25:09.980 --> 02:25:15.420]  понятно, да, что вот какое-то было условное распределение, условная случайная величина. Вот. И мы
[02:25:15.420 --> 02:25:19.280]  дальше просто по определению мы тоже в дискретном случае вот так вот определили ее. Вот,
[02:25:19.280 --> 02:25:24.460]  на самом деле, хочется тоже самое сделать что-то и в непрерывном случае, но в непрерывном случае
[02:25:24.460 --> 02:25:29.780]  вот, вероятность как-то плохо определяет для конкретных точек. Нужно определять какие-то
[02:25:29.780 --> 02:25:42.220]  плотности. Ну и поэтому, вот, вот такие объекты определяются, при каким-то офиксированном значении
[02:25:42.220 --> 02:25:53.420]  просто определение плотности на самом деле. Супер! Тогда первое утверждение, которое хочется
[02:25:53.420 --> 02:25:57.740]  сказать. Как искать условную плотность? Ну, очевидно, что нам условная плотность понадобится дальше,
[02:25:57.740 --> 02:26:01.940]  чтобы считать условные мотожи. А если существует совместная плотность,
[02:26:01.940 --> 02:26:12.940]  тогда условная плотность
[02:26:12.940 --> 02:26:32.860]  есть просто совместная плотность подъедет на плотность в точке y, если плотность в точке y
[02:26:32.860 --> 02:26:47.300]  больше 0, ну и ноль иначе. Ну тоже похоже очень, потому что у нас условные вероятности также
[02:26:47.300 --> 02:26:56.420]  определялись по совместной вероятности поделить на вероятность условия. Вот, и предположим, что мы
[02:26:56.420 --> 02:27:03.180]  с вами знаем теперь, предположим, что мы с вами теперь знаем условную плотность, как тогда искать
[02:27:03.180 --> 02:27:16.060]  условное математическое ожидание? Ну вот, ответ на этот вопрос дает тоже теорема, что на самом деле,
[02:27:16.060 --> 02:27:21.700]  если вот у нас есть плотность вот такая условная, то чтобы найти математическое ожидание вот такой
[02:27:21.820 --> 02:27:41.540]  штучки, нужно просто ее проинтегрировать по условной плотности. Ну и это вот как раз, это ничего не
[02:27:41.540 --> 02:27:47.300]  доказывает, но это завершает аналогию вот этого примера, ну вот непрерывного случая, дискретного
[02:27:47.300 --> 02:27:51.500]  случая. Потому что здесь также можно функции навесить на это и на это, ну и вот здесь вот то
[02:27:51.500 --> 02:28:03.500]  же самое записано. Какую-то аналогию это завершает. Ну а собственно этого я и хотел. Так, я сейчас
[02:28:03.500 --> 02:28:16.460]  проверю, что я нигде не соврал. Фкс при условии да, да, кс при, так, х, у, окей, так, а то, что выше,
[02:28:16.460 --> 02:28:25.980]  я говорю тоже верно. Вот, ну и все, и теперь собственно мы можем уже решать задачу. Давайте сначала мы
[02:28:25.980 --> 02:28:33.420]  с вами обсудим план решения задачи вот такого вида, который у нас будет четвертым, и затем
[02:28:33.420 --> 02:28:52.100]  быстренько решим последнюю задачу, и на этом все. План решения. Там будут скорее всего непрерывные
[02:28:52.100 --> 02:29:01.860]  случайные величины, поэтому план решения такой. Первое. Найти совместную плотность. То есть вас
[02:29:01.860 --> 02:29:11.580]  просят посчитать условное математическое ожидание f от х при условии у. Найти, значит первый шаг,
[02:29:11.580 --> 02:29:23.580]  это найти совместную плотность. Вот такую. Так, совместно. Если случайные величины независимы,
[02:29:23.580 --> 02:29:39.300]  то это просто произведение. Второй шаг. Найти условную плотность по вот тому первому пункту,
[02:29:39.300 --> 02:29:57.660]  который написан на этой доске. Вот по вот этому. Нашли условную плотность. Третий пункт по
[02:29:57.660 --> 02:30:16.260]  вот второму, вот по второму утверждению на этой доске. Необходимо взять интегральчик. Так,
[02:30:16.260 --> 02:30:36.340]  это у нас будет интегральчик по f от х при условии у. Вот. И последний шаг. У нас вот здесь
[02:30:36.340 --> 02:30:41.340]  было фиксированное значение, но вместо него теперь можно подставить саму случайную величину.
[02:30:41.340 --> 02:30:51.260]  То есть, на примере задачи посмотрим. Замена у мало на у больше. Это все корректно работает,
[02:30:51.260 --> 02:30:57.940]  если что. Ну вот такой план. План решения. Ну и давайте его применять.
[02:30:57.940 --> 02:31:21.220]  А который час? Так, сейчас я подумаю. Я потом поделюсь материалами, где это можно найти.
[02:31:21.220 --> 02:31:42.340]  Так, текст, текст, текст, текст, текст. Ну и собственно вот. Какие обычно задачи бывают?
[02:31:42.340 --> 02:31:49.700]  Вот последняя задача, она обычно какая? Вам сначала нужно каким-то образом поупрощать
[02:31:49.700 --> 02:31:57.100]  умо, а потом посчитать умо через вот условную плотность. Ну и давайте посмотрим на такую
[02:31:57.100 --> 02:32:16.900]  задачу. Задача номер четыре. Ну и последняя сегодня. Так. Хорошо, что я ее решил. Звучит у нее условие так.
[02:32:16.900 --> 02:32:37.780]  Найти условную плотность p от x при условии y поделить на x. Если x имеет равномерное распределение
[02:32:37.780 --> 02:32:58.900]  на отрезке 0,1, а y имеет плотность 4y в кубе на индикатор. y принадлежит от 0 до 1.
[02:32:58.900 --> 02:33:25.500]  Сейчас. Первый раз слышу эту песню.
[02:33:25.500 --> 02:33:40.300]  Давайте решать. Осталось немного. Значит, смотрите. Первое, что у нас просят в этой
[02:33:40.300 --> 02:33:48.940]  задаче, это найти вот такую условную плотность. А второе, при помощи нее найти условное
[02:33:48.940 --> 02:33:58.580]  математическое ожидание xy при условии y поделить на x. Вот. Ну, заметьте, что вот в лоб такая вот
[02:33:58.580 --> 02:34:03.580]  условная плотность вам не поможет найти вот то, что здесь нужно. Поэтому, на самом деле, решение.
[02:34:03.580 --> 02:34:14.980]  Первое. Давайте с конца пойдем. Это же. Тут будет сложновато. Тут будет сложновато, потому что
[02:34:14.980 --> 02:34:20.860]  здесь произведение, линейностью воспользоваться не можем. Можем измеримый множитель вынести. Давай
[02:34:20.860 --> 02:34:27.220]  это сделаем. Понимаешь, да, как это сделать? Вот смотри. На самом деле, подсказка условий есть. Мы
[02:34:27.220 --> 02:34:33.060]  должны оставить функцию от x слева, а справа должен быть y поделить на x. То есть нам нужно и от y избавиться.
[02:34:33.060 --> 02:34:43.220]  То есть первое. Давайте я вот здесь равенство такое напишу. Это, на самом деле, y на x вынести и
[02:34:43.220 --> 02:34:51.420]  останется математическое ожидание x в квадрате при условии y поделить на x. Да, да. Ну, понятно,
[02:34:51.420 --> 02:34:58.820]  что y на x измерим относительно y на x. Да? Все, собственно. А теперь, если у нас будет условная
[02:34:58.820 --> 02:35:05.020]  плотность, то мы можем ее x в квадрате помножено на нее проинтегрировать, и тогда мы найдем вот эту УМО.
[02:35:05.020 --> 02:35:12.020]  Собственно, для этого нас в первом пункте просят найти сначала условную плотность. Это как бы первый
[02:35:12.020 --> 02:35:19.220]  шаг. Вы ее найдете. Второй шаг. Это вы вот это вот упростите до такого состояния, чтобы можно было
[02:35:19.220 --> 02:35:27.900]  воспользоваться вот тем утверждением по вычислению УМО через условную плотность. Понятно? Супер. Вроде не
[02:35:27.900 --> 02:35:37.100]  сложно. Ну и, соответственно, давайте тогда первый пункт решать. Нам нужно найти с вами вот такую
[02:35:37.100 --> 02:35:47.100]  условную плотность. Что нам для этого нужно сделать? Сейчас я решение свое возьму. Да, нам нужно найти
[02:35:47.100 --> 02:35:54.700]  совместную плотность. То есть, в силу теоремы, которую мы обсудили, условная плотность x при условии
[02:35:58.900 --> 02:36:11.300]  Ну дела. Ну дела. Я нашел ошибку в самом начале решения. Я, короче, сегодня всю ночь считал вот такую
[02:36:11.300 --> 02:36:18.660]  штуку. Ну и на самом деле она сложнее получается. О, это значит у нас сейчас гораздо проще все получится.
[02:36:18.660 --> 02:36:33.980]  Мы справимся. Спасибо большое. Да, независимо. То есть нам нужно найти условную плотность по вот
[02:36:33.980 --> 02:36:37.900]  той теоремке, которую мы обсудили. Условная плотность через совместную плотность выражается. То
[02:36:37.900 --> 02:36:45.420]  есть на самом деле в числителе у нас будет, здесь будет что у нас получается плотность x запятая y поделить на
[02:36:45.420 --> 02:37:00.580]  x в точке s поделить на плотность, видимо, y поделить на x в точке t. Если плотность, ну давайте так,
[02:37:00.580 --> 02:37:13.980]  на индикатор того, что p от y поделить на x в точке t больше 0 иначе 0.
[02:37:13.980 --> 02:37:26.660]  Окей? Так, ну смотрите. Это был первый шаг. Второй шаг. Видимо, нам нужно найти совместную плотность.
[02:37:26.660 --> 02:37:32.980]  А совместную плотность как нам найти вот таких вот двух случайных величин? Ну, во-первых,
[02:37:32.980 --> 02:37:41.020]  мы с вами знаем, что совместная плотность, сейчас я посмотрю как я. Мы, во-первых, знаем с вами,
[02:37:41.020 --> 02:37:45.580]  что совместная плотность исходных случайных величин, это просто произведение плотностей,
[02:37:45.580 --> 02:38:00.700]  потому что они независимы. Тогда, что мы можем с вами сказать? Тогда мы можем сделать с
[02:38:00.700 --> 02:38:09.980]  вами преобразование. То есть дальше вспоминаем первую половину семестра и пользуемся теоремой
[02:38:09.980 --> 02:38:13.260]  о плотности преобразования. Ну, то есть нам нужно намножить на екобиан обратного
[02:38:13.260 --> 02:38:17.980]  преобразования и там что-то выражать. Давайте это аккуратно проделаем. То есть мы вводим с
[02:38:17.980 --> 02:38:26.540]  вами новые случайные величины у и в. У мы с вами оставляем в x, в мы кладем с вами y поделить на x.
[02:38:26.540 --> 02:38:30.620]  Спасибо большое.
[02:38:30.620 --> 02:38:43.020]  Дальше ищем обратное преобразование. Ну, обратное преобразование здесь очень легко ищется.
[02:38:43.020 --> 02:38:54.300]  Вот, ну вы, если что, проверяйте меня, пожалуйста. Дальше мы ищем екобиан обратного преобразования.
[02:38:54.300 --> 02:39:07.740]  Екобиан на обратном преобразовании это будет единичка по v0, по uv, по vu. То есть просто u будет.
[02:39:07.740 --> 02:39:19.100]  Вроде справедливо. Я не соврал. Вот, ну и тогда совместная плотность
[02:39:19.100 --> 02:39:39.180]  x и y поделить на x в точке st, ну в новой точке uv. Это у нас екобиан, то есть u умножить на плотность
[02:39:39.180 --> 02:39:51.260]  x, x у нас это u в точке u умножить на плотность y, а y у нас это uv. Блин, ну все по-человечески
[02:39:51.260 --> 02:39:55.980]  получается. У нас сейчас решение в два раза короче получится, чем у меня дома. Потому что я там перепутал
[02:39:55.980 --> 02:40:06.780]  и x на y читал. Ладно, не страшно, не страшно. Так, ну вот здесь план решения, я его сейчас
[02:40:06.780 --> 02:40:12.300]  сотру. В принципе мы уже все что нам нужно было практически взяли отсюда. Сейчас вот
[02:40:12.300 --> 02:40:17.420]  за используем плотности вот эти, когда будем считать вот эту плотность совместную.
[02:40:17.420 --> 02:40:38.780]  Так, ситуация. Давайте как-нибудь плотность совместную теперь выразим там, подставим
[02:40:38.780 --> 02:40:46.380]  значение из условия. То есть у нас x при условии y поделить на x, совместная плотность,
[02:40:46.380 --> 02:41:02.260]  прошу прощения, в точке uv. Так, что у нас такое u? u это у нас x, x у нас имеет какое распределение по
[02:41:02.260 --> 02:41:07.460]  условию? Короче, оно от 0 до 1 лежит, поэтому модуль спокойно снимаем, потому что u будет
[02:41:07.460 --> 02:41:17.140]  положительным. Дальше плотность x. Плотность x это 4x в кубе, подставляем u. То есть у нас будет
[02:41:17.140 --> 02:41:31.180]  4u в кубе на индикатор того, что u принадлежит от 0 до 1. Отлично. И теперь плотность y. Плотность
[02:41:31.180 --> 02:41:40.380]  y это у нас индикатор того, что u принадлежит от 0 до 1. Ну по сути, если у нас u от 0 до 1,
[02:41:40.380 --> 02:41:56.940]  v от 0 до 1. Ну ладно, u принадлежит от 0 до 1. Вот. Согласны? Вроде как не наложал. Дальше следующий шаг у нас
[02:41:56.940 --> 02:42:02.940]  какой? Смотрите, совместную плотность мы с вами нашли, нам нужно найти с вами плотность y поделить
[02:42:02.940 --> 02:42:11.100]  на x. Для этого нам нужно, наверное, убрать лишнюю переменную, короче, выинтегрировать ее. Дальше,
[02:42:11.100 --> 02:42:17.780]  так, это получается был второй шаг, третий шаг. А третий шаг следующий. Мы берем с вами и интегрируем
[02:42:17.780 --> 02:42:27.340]  вот это чудо по первой координате. Так, давайте соберем все вместе. Это будет 4u в четвертой. На
[02:42:27.340 --> 02:42:36.300]  индикатор u от 0 до 1. Так, а v это у нас что такое? Отношение двух положительных случайных величин.
[02:42:36.300 --> 02:42:43.700]  Оно тоже положительно. Ну ладно, значит здесь будет у нас интеграл от 0 до, отсюда у нас следует,
[02:42:43.700 --> 02:42:59.140]  что u меньше либо равно, чем 1 на v. До минимума из единички и 1 на v. Так, согласны?
[02:42:59.140 --> 02:43:08.740]  Вроде как правда. Так, дальше, наверное, нужно случаи разобрать аккуратно. Давайте аккуратно
[02:43:08.740 --> 02:43:20.500]  эти случаи рассмотрим. Если что, это мы считаем, ого, вот это экспрессия. Это если что у нас плотность
[02:43:20.500 --> 02:43:36.420]  у поделить на х в точке v. Ну вот этот интегральчик у нас вроде как очевидно, что равен 5, 4 пятых в пятый
[02:43:36.420 --> 02:43:55.220]  в подстановке от 0 до минимума из единички и 1 до 9. Вот, ну и соответственно плотность в точке v.
[02:43:55.220 --> 02:44:17.580]  Это у нас вот два случая надо рассмотреть. Если v меньше единицы, то есть v принадлежит от 0 до 1,
[02:44:17.580 --> 02:44:25.740]  тогда получается у нас вот это будет больше единички. Значит у нас верхний предел это единичка,
[02:44:25.740 --> 02:44:38.500]  тогда это будет 4 пятых минус 0. Так, а если v больше единички будет, тогда здесь у нас получится,
[02:44:38.500 --> 02:44:57.300]  получается 4 пятых, а 1 поделить на v в пятый. Вроде как справедливо. Так, отлично. Так, ну если что у
[02:44:57.300 --> 02:45:02.380]  нас вот, кстати, сейчас здесь небольшие коллизии, вот здесь у нас st было, там uv. Давайте здесь тоже на uv поменяем,
[02:45:02.380 --> 02:45:20.300]  чтобы все красиво было. Это безрадница, короче. А, ну просто условная плотность, если плотность
[02:45:20.300 --> 02:45:26.180]  в знаменателе больше 0, то и так, а если она равна 0, тогда здесь 0 просто определяется. То есть по сути
[02:45:26.180 --> 02:45:31.700]  это же то же самое, что на индикатор домножить. Ну то есть там просто такая системка у нас была,
[02:45:31.700 --> 02:45:39.980]  если положительно, тогда так, а если 0, тогда ну 0. Ну я просто на индикатор домножил и все. Так,
[02:45:39.980 --> 02:45:45.100]  ну теперь давайте попробуем с вами вот эту условную плотность уже представить в требуемом
[02:45:45.100 --> 02:46:02.820]  виде. В каком-нибудь виде, в котором с ней уже можно будет работать хорошо. Ну да. Итак,
[02:46:02.820 --> 02:46:16.620]  p от x при условии y поделить на x в точке uv, это у нас что такое? Это у нас что такое? А соответственно,
[02:46:16.620 --> 02:46:42.260]  если v меньше 0, то это 0. А если v, видимо даже меньше и равна 0, если v принадлежит от 0 до 1,
[02:46:42.260 --> 02:47:00.700]  и последнее, если у нас v будет больше либо равной 1. Нет, все красиво будет, я обещаю. Я тоже думал,
[02:47:00.700 --> 02:47:08.900]  что дома не очень приятно, а потом дорешал. Вот, в общем, если от 0 до 1, то у нас в знаменателе
[02:47:08.900 --> 02:47:15.220]  4 пятых просто будет. Ну то есть домножаем на 5 четвертых. А вот здесь у нас нужна совместная
[02:47:15.220 --> 02:47:28.180]  плотность. А совместную плотность мы где выписывали? Вот это чудо, да, у нас? Что-то, ну да. Не густо. Так,
[02:47:28.180 --> 02:47:33.500]  значит там uv четвертый, да, у нас будет. Ну давай смотреть на индикаторы. Кажется,
[02:47:33.500 --> 02:47:41.340]  что половина индикаторов у нас упростится. Ну смотри, а v от 0 до 1 у нас здесь лежит,
[02:47:41.340 --> 02:47:52.460]  значит, вот смотри, v от 0 до 1, значит, вот это больше 1, значит, у нас только этот индикатор
[02:47:52.460 --> 02:48:00.340]  выживает. Значит, индикатор того, что u принадлежит от 0 до 1. Ну там, короче, может,
[02:48:00.340 --> 02:48:05.140]  где-то скобочки нужны замкнутые включать, ну типа, то есть включать концы. Не совсем важно,
[02:48:05.140 --> 02:48:09.540]  потому что множество меры 0 ничего не значит в небеременном случае, но вообще аккуратно
[02:48:09.540 --> 02:48:14.140]  нужно, конечно, все расставить. Так, и если v больше или в равной 1, то у нас что получится?
[02:48:14.140 --> 02:48:37.340]  Так, это у нас, на этом мы делим. Вv пятый будет в числителе, uv четвертый, ого. Так,
[02:48:37.340 --> 02:48:41.780]  здесь v больше или в равно 1, значит, у нас вот этот индикатор теперь работает, то есть на самом
[02:48:41.780 --> 02:48:53.940]  деле индикатор того, что u принадлежит от 0 до 1 в этой. Вот так. Ну, потому что у нас
[02:48:53.940 --> 02:49:01.980]  есть 2 индикатора на u, по сути. Вот. Ну вот наша условная плотность. Ну, мы почти уже уфинишим,
[02:49:02.380 --> 02:49:12.020]  да-да, это считается спокойно. Дальше. Мы хотим, наверное, посчитать. Мы помним,
[02:49:12.020 --> 02:49:16.460]  что мы там упростили, и в итоге нам нужно x в квадрате считать при условии y поделить на x,
[02:49:16.460 --> 02:49:31.860]  так ведь? По теореме, вот эта штука, это не что иное, как интеграл x в квадрате,
[02:49:31.860 --> 02:49:41.100]  ну ладно, если x это у, то это получается будет u в квадрате, умножить на условную плотность.
[02:49:41.100 --> 02:50:04.100]  Вот так. Да? В точке v. Да, прошу прощения, вот здесь нужно зафиксировать, там теоремки,
[02:50:04.100 --> 02:50:13.940]  вот это значение фиксировано. Вот так. А потом мы вместо v подставляем вот ту случайную величину,
[02:50:13.940 --> 02:50:21.500]  которую у нас. Да. Вот такой интегралчик нам надо посчитать. Ну давайте аккуратно посчитаем.
[02:50:21.500 --> 02:50:33.220]  Да, вроде все корректно. Ладно, нормально. Давайте считать. Тут опять будет пару условий. Видимо,
[02:50:33.220 --> 02:50:41.580]  если плотность нулевая, нас ничего не интересует. Вот этот случай можно не рассматривать. Теперь,
[02:50:41.580 --> 02:50:53.900]  если v принадлежит от 0 до 1, ну да, там будет нулевая, то есть нас это не особо интересует. Так,
[02:50:53.900 --> 02:50:58.220]  если v будет принадлежать от 0 до 1, то у нас плотность вот такая. Еще нужно u в квадрате
[02:50:58.220 --> 02:51:08.260]  намножить. То есть будет 5 u в 6, то здесь будет интеграл 5 u в 6 на индикатор. Какой у нас там
[02:51:08.260 --> 02:51:17.020]  индикатор выжил? А ну, это все интегрируем от 0 до 1. Так, и потом у нас еще второй интегралчик будет.
[02:51:17.020 --> 02:51:34.940]  Если v больше либо равно 1, 5 u в 6. В какой вы выносим? 5. В 5 выносим. Ну, 5 в 5, видимо, выносим.
[02:51:34.940 --> 02:51:44.580]  Здесь тоже можно константу вынести. 5 в 5 выносим. У нас остается u в 6. Только вопрос в пределах
[02:51:44.580 --> 02:51:54.420]  интегрирования здесь будет от 1 до 1 на v. Да, при v больше либо равно 1. Соответственно,
[02:51:54.420 --> 02:52:08.420]  ну, это кажется, что не сложно считается. 5 седьмых. Давайте запишем еще ду для красоты,
[02:52:08.420 --> 02:52:19.700]  чтобы не подумали, что мы не образованные. В принадлежит от 0 до 1. А вот здесь у нас получится
[02:52:19.780 --> 02:52:31.820]  тоже 5 седьмых. Но еще мы интегрируем это будет u в 7 на 7. Ну да, и делить на v квадрат.
[02:52:31.820 --> 02:52:42.700]  Если v принадлежит. Ну, короче, если v больше нички. Вот. И теперь самое интересное. По сути ответ
[02:52:42.700 --> 02:53:03.740]  надо записать. Так, это я могу уже спокойно стирать. Так, так, так, так, так, так, так, так, так.
[02:53:03.740 --> 02:53:17.020]  Так. Ага. Значит, у нас по условию спрашивали, математическое ожидание x и y при условии,
[02:53:17.020 --> 02:53:26.140]  при условии чего? y поделить на x? Да, мы с вами это упростили до вот такого состояния,
[02:53:26.140 --> 02:53:33.420]  что это y поделить на x на условное математическое ожидание x квадрате при условии y поделить на
[02:53:33.420 --> 02:53:42.540]  x. Вот. А вот это мы с вами только что посчитали. Ну, мы посчитали с вами прификсированным v. Вот.
[02:53:42.540 --> 02:53:49.020]  Теперь, по сути, мы вместо v в этом ответе просто y поделить на x подставляем. Ну. И все получается.
[02:53:49.020 --> 02:53:57.660]  Это теоремка есть. Ну, типа мы, ну, на самом деле это вопрос в том, что мы, смотри.
[02:53:57.660 --> 02:54:08.780]  Да, мы посчитали для v, то есть мы прификсированном значении посчитали условное
[02:54:08.780 --> 02:54:13.980]  математическое ожидание. То есть, если у тебя v, ну, то есть, если у тебя условия зафиксированы,
[02:54:13.980 --> 02:54:20.580]  то у тебя умок какая-то константа. Ну, в данном случае. Ну, какое-то значение принимает. А теперь,
[02:54:20.580 --> 02:54:24.500]  ну, как бы, у нас же должно получиться функция от условия, а в условии у нас, по идее, случайная
[02:54:24.500 --> 02:54:32.260]  величина. И мы, по сути, теперь начинаем вот эту вот v фиксированную варьировать, ну, за счет того,
[02:54:32.260 --> 02:54:37.620]  что мы считаем, что теперь v не фиксированная, а вот просто случайная y поделить на x. Вот. Ну,
[02:54:37.620 --> 02:54:43.620]  это как бы, это показательство, это не доказательство, ничего. Там, ну, доказательство можно аккуратно,
[02:54:43.620 --> 02:54:58.060]  что так можно делать. Так. Ну, и ответ. Пять седьмых, коль скоро y на x. Так, еще нужно вот на
[02:54:58.060 --> 02:55:05.540]  это намножить. y поделить на x. Пять седьмых y поделить на x. Коль скоро у нас y поделить на x.
[02:55:05.540 --> 02:55:20.720]  От 0 до 1 лежит. И 5 поделить на 7 на v в квадрате. Так, y поделить на x. Нужно еще,
[02:55:20.720 --> 02:55:29.900]  на условии у нас y поделить на x, то есть это будет y в квадрате, x в квадрате. Да, x поделить на y.
[02:55:29.900 --> 02:55:41.980]  Спасибо. x поделить на y. Коль скоро y поделить на x. Нет, здесь в квадрате, но у нас еще y на x отсюда.
[02:55:41.980 --> 02:55:53.660]  И мы делим на него в квадрате, поэтому у нас все сокращается. Вот. Ну, и собственно, это вот ответ.
[02:55:59.900 --> 02:56:14.340]  Нормально? Что еще за дачку? Так, а сейчас сколько 0,53? Ну ладно, нормальный такой средне...
[02:56:14.340 --> 02:56:21.260]  Среднестатистический семинар получился. Нет, все, хватит. Расходимся.

[00:00.000 --> 00:15.680]  15.30. Я думаю, можно начинать. Всем доброго дня. Для начала, давайте я напомню, на чем основались
[00:15.680 --> 00:23.040]  прошлый раз. Мы дали определение дисперсии. Дисперсии случайно увеличены. Мы определили
[00:23.040 --> 00:30.360]  ее по вот такой формуле, а также доказали формулу для вычленения дисперсии. Кроме того,
[00:30.360 --> 00:34.640]  я в прошлый раз не сказал про стандартное отклонение. Есть еще такой термин стандартного
[00:34.640 --> 00:42.880]  отклонения. Обычно обозначается маленькой буквой сигма. Это просто корни дисперсии. Ну,
[00:42.880 --> 00:49.200]  дело в том, что если у вас кси измеряется в метрах или в рублях или еще в чем-нибудь,
[00:49.200 --> 00:57.000]  то дисперсия будет уже измеряться в квадратных метрах или в квадратных рублях. Чтобы иметь дело
[00:57.000 --> 01:04.000]  с правильной размерностью, обычно корни извлекают из дисперсии. Мы также начали обсуждать свойства
[01:04.000 --> 01:10.440]  дисперсии. Первое свойство было дисперсия положительна, что очевидно, потому что
[01:10.440 --> 01:16.240]  просто квадрат случайно увеличен и всегда положительный. Следует из первого свойства
[01:16.240 --> 01:24.280]  математического ожидания. Кроме того, можно даже сказать, когда дисперсия равна нулю, это означает,
[01:24.280 --> 01:31.760]  что вот такое математическое ожидание равно нулю. А если математическое ожидание квадрата случайно
[01:31.760 --> 01:35.280]  увеличено равно нулю, мы в прошлый раз обсуждали, что это означает, что случайно лично просто
[01:35.280 --> 01:45.000]  почти наверно равна нулю. Ну, то или иными словами, кси равна своему вот ожиданию почти наверно.
[01:45.000 --> 01:54.120]  Ну, то есть такое бывает только когда случайная величина имеет константное распиление. Второе
[01:54.120 --> 02:00.000]  свойство, которым в прошлый раз было, это как меняется дисперсия при линейном преобразовании.
[02:00.000 --> 02:10.280]  При сдвиге на константу она не меняется, а при умножении она множественно выносится в квадрате.
[02:10.280 --> 02:22.360]  Это тоже очень просто. Давайте я сразу сформулирую третье свойство. Третье свойство такое, если
[02:22.360 --> 02:30.920]  случайные величины независимы, то тогда дисперсия суммы равна сумме дисперсии. Кроме того,
[02:30.920 --> 02:39.720]  если есть несколько случайных величин, которые попарно независимы, то тогда тоже дисперсия
[02:39.720 --> 02:47.720]  суммы это сумма дисперсии. Чтобы это доказать, я использую кавариацию, про которую мы тоже
[02:47.720 --> 02:55.280]  начинали говорить в прошлый раз. Я напомню, что это такое. Кавариация двух случайных величин
[02:55.280 --> 03:05.600]  определяется по следующей формуле. Для кавариации есть формула, аналогичная формуле, которая есть для
[03:05.600 --> 03:12.080]  дисперсии. Эту формулу я, кажется, в прошлый раз не доказывал, поэтому докажу ее сейчас.
[03:12.080 --> 03:19.640]  Наказательство. Пишу определение, раскрываю скобочки. Математическое ожидание – это константа.
[03:19.640 --> 03:29.480]  Константу можно выносить за знаком от ожидания. Кроме того, от ожидания суммы 4 слагаемых можно
[03:29.480 --> 03:35.920]  развить в сумму от ожидания полеиндиста. Получается от ожидания предведения. Дальше второе слагаемое,
[03:35.920 --> 03:41.080]  от ожидания кси, выносится из от ожидания. Получается от ожидания кси, множественное от ожидания
[03:41.080 --> 03:46.760]  это. Аналогично третье слагаемое. Четвертое слагаемое – это просто константа, от ожидания
[03:46.760 --> 03:55.240]  константа. Это и есть та же самая константа. Можно сократить подобные слагаемые, и остается как
[03:55.240 --> 04:05.560]  раз заявленная формула. В прошлый раз я начал обсуждать свойства дисперсии. Одно из свойств
[04:05.560 --> 04:15.640]  заключается в том, что ковриация кси-кси – это то же самое, что дисперсия кси, что очевидно,
[04:15.640 --> 04:21.400]  просто посмотрите на определение дисперсии и ковриации. Второе свойство заключается в том,
[04:21.400 --> 04:28.600]  что если случайные личины независимы, то тогда ковриация равна нулю. Для этого нужно просто
[04:28.600 --> 04:34.120]  посмотреть на формулу для ковриации, которую мы только что доказали, и вспомнить свойство
[04:34.120 --> 04:38.320]  от ожидания, которое говорит, что если случайные личины независимы, то от ожидания предведения –
[04:38.320 --> 04:45.120]  это предведение от ожидания. Поэтому второе свойство тоже уже по сути доказано. Третье
[04:45.120 --> 04:53.880]  свойство заключается в том, что ковриация билинейна, билинейная симметричная форма. То,
[04:53.880 --> 05:01.160]  что ковриация симметрична – это очевидно, я все равно это напишу. А еще она билинейна по каждой
[05:01.160 --> 05:10.360]  кардинации, я напишу по первой кардинации. Билинейность любых констант А и В, можно вот так
[05:10.360 --> 05:18.520]  по линейности раскрывать. Для приличия можно даже как-нибудь доказать свойства линейности к
[05:18.520 --> 05:29.160]  ковриации. Например, по определению левая часть – это такое математическое ожидание. Теперь просто
[05:29.160 --> 05:38.520]  правильно группируем слагаемые, ну и раскрываем по дистрибутивности. А теперь мы уже здесь
[05:38.520 --> 05:44.880]  использовали линейность мотоожидания, когда вот тут раскрыли мотоожидание суммы. И здесь еще
[05:44.880 --> 05:56.640]  раз используем линейность мотоожидания. Получаем А на мотоожидание, по сути, на ковриацию. В общем,
[05:56.640 --> 06:02.760]  то есть правую часть. Билинейность ковриации напрямую следует из линейности математического
[06:02.760 --> 06:11.560]  ожидания. Окей, теперь можно доказать третье свойство дисперсии, которое я уже сформулировал,
[06:11.560 --> 06:21.320]  то что для независимых случайных величин дисперсия суммы – это сумма дисперсии. Мы знаем,
[06:21.320 --> 06:29.520]  что случайные величины независимые, тогда их ковариация равна нулю. Что такое дисперсия суммы?
[06:29.520 --> 06:37.880]  Дисперсия – это тоже ковариация по первому свойству ковариации. Теперь можно раскрыть по
[06:37.880 --> 06:45.520]  линейности, по билинейности ковариации. Можно раскрыть, что это ковариация C кси плюс ковариация
[06:45.520 --> 06:58.080]  и это плюс ковариация это кси плюс ковариация это это. Окей, две из четырех ковариаций равны нулю,
[06:58.080 --> 07:05.320]  и остается только ковариация кси кси и ковариация это это. То есть дисперсия кси плюс дисперсия для
[07:05.320 --> 07:12.600]  двух случайных величин доказали. Если их несколько, то, в принципе, все то же самое.
[07:12.600 --> 07:21.760]  Пишем, что это ковариация суммы запятая сумма и, опять же, раскроем по линейности. Понятно,
[07:21.760 --> 07:37.160]  что у нас получится N квадрат слагаемых. Там будут все возможные ковариации C и кси ижи по всем
[07:37.160 --> 07:45.880]  возможным и ижи. Вот, среди них можно выделить тех вариаций, где i равно j, то есть ковариация кси и
[07:45.880 --> 07:56.600]  кси и и можно посмотреть на оставшиеся. Там, понятно, дело будут просто ковариации. Я еще вот здесь двойку
[07:56.600 --> 08:05.600]  напишу, а вот эти i меньше, чем j, что было похоже на формулу квадрата суммы. Все возможные удвоенные
[08:06.000 --> 08:16.000]  ковариации получатся. Ковариация кси и кси это, понятное дело, дисперсия, а оставшиеся ковариации
[08:16.000 --> 08:21.600]  просто равны нулю, поскольку мы предположили, что случайная величина попарно независима. Я думаю,
[08:21.600 --> 08:31.760]  что все свойства можно считать доказанными. В качестве примера я посчитаю дисперсию
[08:31.760 --> 08:40.160]  случайной величины, которая имеет биномиальный распределение. Причем двумя способами. Во-первых,
[08:40.160 --> 08:46.480]  я первый способ посчитаю просто по формуле. Формула для дисперсии вот такая. Математическое
[08:46.480 --> 08:52.160]  ожидание мы уже в прошлый раз считали. Математическое ожидание должно быть NP. Осталось
[08:52.160 --> 09:00.480]  посчитать математическое ожидание квадрата. Для этого есть просто формула. То, что это сумма
[09:00.480 --> 09:07.500]  k квадрат по всем возможным значениям k. Вероятность, что кси равно k. Распределение
[09:07.500 --> 09:14.040]  биномиальное. Значения принимаются от нуля до n, и вероятность из значений тоже можно написать.
[09:14.040 --> 09:25.720]  Далее я использую хитрую формулу, что k на cg по k это n на cg-1 по k-1, которую я еще в
[09:25.720 --> 09:38.280]  прошлый раз использовал. И с помощью этой формулы тут получится kn вот так. Я еще буду
[09:38.280 --> 09:47.600]  суммировать от единицы до n, поскольку нулевой слагаемое равно нулю. Понятно, что выносится за
[09:47.600 --> 09:57.800]  скобочку вот это вот n. И еще p можно вынести за скобочку. Вынесли. Получилось k от единицы до n,
[09:57.800 --> 10:15.640]  k на c, на p в k-1. А теперь n-k я напишу как n-1-k-1. Здесь хочется сделать замену. k-1 равно m,
[10:15.640 --> 10:23.840]  чтобы суммирование было от нуля до n-1. Сделаю это прямо. Пользуюсь прекрасным свойством доски,
[10:23.840 --> 10:37.560]  что можно стирать. m будет минус m. Вот тут m теперь от нуля до n-1, и тут будет m плюс 1.
[10:37.560 --> 10:47.840]  Окей, теперь разобьем эту сумму на две суммы. В одной будет просто m, а в другой будет вот это
[10:47.840 --> 10:56.080]  вот плюс единичка. Вот смотрите, вот эта первая сумма, мы ее в прошлый раз считали, она очень
[10:56.080 --> 11:01.560]  похожа на математическое ожидание биномеральности случайной личины, только уже с параметром n-1p.
[11:01.560 --> 11:11.880]  Это математическое ожидание у этой случайной личины z, где z имеет распределение биномеральное
[11:11.880 --> 11:20.400]  с параметром n-1.p. И тут должно получиться n-1 умножить на p, если мне не изменяет
[11:20.400 --> 11:29.200]  память. Вот, а вторая сумма, ее можно просто по биному ньютона свернуть в единицу и останется
[11:29.200 --> 11:39.440]  просто n-p. Хорошо, все, осталось просто записать ответ. Дисперсия равна то, что мы вот здесь
[11:39.440 --> 11:48.840]  насчитали. Я раскрою скобочки n квадрат p квадрат минус n-p квадрат плюс n-p и минус квадратом
[11:48.840 --> 11:56.680]  от ожидания, то есть минус n-p в квадрате. Что-то сократилось и осталось n умножить на p минус
[11:56.680 --> 12:05.240]  p квадрат или можно еще написать как n-p на 1 минус p. Окей, можно считать немножко по-другому,
[12:05.240 --> 12:12.360]  а вспомнить, а именно вспомнить, что такое биномеральное распределение. Биномеральное распределение
[12:12.360 --> 12:21.600]  это количество успехов в схеме испытаний Bernoulli. То есть xi это сумма следующих индикаторов,
[12:21.600 --> 12:38.080]  n индикаторов, где событие ai означает, что на иным шаге случился успех. Вот. Утверждение,
[12:38.080 --> 12:48.760]  которое я не буду доказывать, что эти индикаторы независимы. Это не то чтобы что-то сложное,
[12:48.760 --> 12:55.080]  возможно вы чем-то таким уже на семинарах занимались. То есть можно показать то, что вот
[12:55.080 --> 13:02.560]  эти события независимы. Это в принципе логично, потому что у нас же темы испытаний Bernoulli. Мы как
[13:02.560 --> 13:09.240]  бы проводим независимое n испытаний, а потом из независимости этих событий будет следовать
[13:09.240 --> 13:20.560]  независимость этих индикаторов. Тут нужно все-таки делать какую-то проверку, но я просто
[13:20.560 --> 13:31.040]  сформулирую это как утверждение. Я даже не так. Даже напишу, что это упражнение. И если мы знаем,
[13:31.040 --> 13:37.600]  что они независимы, то тогда дисперсию x можно расписать как сумму дисперсий вот этих индикаторов.
[13:37.600 --> 13:45.640]  Что такое дисперсия индикатора? Дисперсия индикатора это от ожидания квадрата минус квадрат
[13:45.640 --> 13:51.760]  от ожидания. Кроме того, индикатор принимает значение только ноль или один, а поэтому он в
[13:51.760 --> 13:57.640]  квадрате просто равен себе. Поэтому вот эту двоечку можно убрать. Теперь надо сказать,
[13:57.640 --> 14:04.080]  чему равно математическое ожидание индикатора. Математическое ожидание индикатора события это
[14:04.080 --> 14:11.280]  его вероятность, а его вероятность это вероятность то, что на этом шаге существует успех, то есть это
[14:11.280 --> 14:17.720]  просто p. Его подставляем вот сюда, получаем p минус p квадрат. В таких слагаемых n штук,
[14:17.720 --> 14:24.920]  поэтому дисперсия равна n на p минус p квадрат, что в общем-то сошлось с ответом, полученным первым
[14:24.920 --> 14:33.160]  способом. И еще одно понятие, которое нужно обсудить, говоря о дисперсии и ковариации,
[14:33.160 --> 14:44.400]  это понятие корреляции по пленнику. Корреляции случайно включен кси и это, при этом мы сейчас
[14:44.400 --> 14:56.480]  предполагаем, что они не константные. Корреляции называются по вариации, деленные на стандартное
[14:56.480 --> 15:07.120]  отклонение. Здесь важно, что они не константные, чтобы дисперсии были не нулевые. Ну и соответственно,
[15:07.120 --> 15:16.960]  если корреляция равна нулю, то случайные личины называются некоррелированными.
[15:16.960 --> 15:27.880]  Можно заметить, что вот это пленно очень похоже на формулу для косинуса угла между векторами,
[15:27.880 --> 15:35.560]  если вы знаете, как искать косинус угла между векторами. Поэтому я формулирую утверждение,
[15:35.560 --> 15:49.440]  корреляция всегда от минус единицы до единицы. Причем равенство бывает только тогда, когда одна
[15:49.440 --> 16:00.680]  случайная величина линейно выражается через другую. Это утверждение, благодаря ему,
[16:00.680 --> 16:07.520]  можно воспомнить корреляцию как такую вот числовую характеристику независимости случайных величин,
[16:07.520 --> 16:14.400]  потому что если случайные величины независимы, то корреляция равна нулю. Потому что ковариация
[16:14.400 --> 16:20.480]  равна нулю, значит и корреляция равна нулю. А если они вот совсем зависимы, то есть вот такая вот
[16:20.480 --> 16:28.880]  линейная зависимость, такая наиболее сильная зависимость, то корреляция настигает свои крайние
[16:28.880 --> 16:39.440]  значения, то есть плюс-минус единицы. Нужно сказать, что тем не менее понятие некоррелированности
[16:39.440 --> 16:49.440]  независимости путать не нужно, потому что в одну сторону это верно, если независимость, то корреляция
[16:49.440 --> 16:57.360]  равна нулю, а в другую сторону это уже неверно. В премьере когда это неверно, я привозил на прошлый
[16:57.360 --> 17:11.360]  лекции. Поэтому, конечно же, нельзя сказать, что корреляция отражает всю степень зависимости
[17:11.360 --> 17:22.800]  случайных величин. Независимость это более емкое понятие, нельзя понятие независимости вместить
[17:22.800 --> 17:34.160]  вот всего лишь в одну числовую характеристику. Нужно доказать утверждение. Нужно доказать, что
[17:34.160 --> 17:48.880]  модуль кавариации не больше чем произведение дисперсии. Распишем по определению, что такое дисперсия,
[17:48.880 --> 17:56.960]  что такое кавариация. Слева получается математическое ожидание вот такое, а справа вот что-то такое.
[17:56.960 --> 18:05.760]  Это верно просто потому, что это частный случай не нравится Кашибуниковскому, которое было доказано
[18:05.760 --> 18:12.400]  на прошлой лекции. Произведением отождаем квадратов, хотя бы квадратом отождаем
[18:12.960 --> 18:23.680]  Также я заявлял, что мы можем сказать, когда достигается равенства. Действительно можем, потому что мы уже в прошлый раз выяснили,
[18:23.680 --> 18:28.000]  когда достигается равенства не равенства Кашибуниковского, а оно, я напомню, достигается, когда
[18:28.000 --> 18:35.680]  члены личной линии независимы. То есть тут будет равенство. Равенство, если существуют такие
[18:35.680 --> 18:48.640]  действительные АВ, такие, что А на кси минус мутаж дания кси, плюс В на это минус мутаж дания это, равно нулю почти наверно.
[18:49.280 --> 18:56.800]  Кроме того, понятно, что А не равно нулю, потому что если А равно нулю, то тогда
[18:58.960 --> 19:05.760]  случайная личность это почти наверно является константой, а мы сказали, что
[19:06.560 --> 19:13.920]  корреляцию мы считаем только для неконстантных случайных личин. А раз А не равно нулю, можно написать, что кси
[19:14.480 --> 19:21.920]  равно минус В на А умножить на это, плюс какая-то константа.
[19:24.880 --> 19:28.720]  Неправильно разделил на А, надо все делить на А, а не только первое слагаемое.
[19:29.440 --> 19:36.800]  Вот так получилось. Окей, вот это константа, вот это тоже константа, и мы получили линиенную
[19:36.800 --> 19:45.360]  зависимость, как и было ранее заявлено. Окей, давайте сделаем 5-минутный перерыв и после этого
[19:45.360 --> 19:52.240]  продолжим. Продолжаем. Переходим к последнему разделу той части курса, где мы говорим про
[19:52.240 --> 19:58.480]  вероятность. Это произведет, посвящен на самом деле, будет закону больших чисел. Но для начала
[19:59.680 --> 20:06.560]  нужно доказать несколько неравенств. Неравенство Маркова и неравенство Чебышова. Неравенство Маркова.
[20:07.760 --> 20:17.440]  Включаем, начиная кси, должна принимать не отрицательные значения и дана действительно
[20:17.440 --> 20:24.640]  константа А тоже не отрицательная. Давайте даже скажем положительно, А больше нуля. Неравенство Маркова
[20:24.640 --> 20:32.560]  утверждает, что вероятность, что кси больше, чем А, не больше, чем вот ожидание кси делить на А.
[20:33.280 --> 20:39.360]  Для доказательства возьмем математическое ожидание кси и кси представим в виде суммы двух
[20:39.360 --> 20:46.160]  случайных величин. Сейчас напишу каких. Тут даже можно вот такой знак поставить, чтобы сильнее
[20:46.160 --> 20:53.840]  получилось неравенство. Кси представляем в виде суммы вот такой случайной величины и вот такой
[20:53.840 --> 21:02.000]  случайной величины. Понятно дело то, что ровно один этих двух индикаторов равен нулю, а один равен
[21:02.000 --> 21:08.080]  единице, поэтому то, что вот здесь написано, это просто во всех точках вероятного пространства равно
[21:08.080 --> 21:16.560]  кси. Пишем по линии математического ожидания. Теперь скажем то, что поскольку кси принимает
[21:16.560 --> 21:24.240]  только не отрицательные значения, второе слагаемое не отрицательное, кроме того вот такая случайная
[21:24.240 --> 21:31.920]  величина кси на индикатор больше, чем случайная величина А на индикатор. Ну просто потому что или
[21:31.920 --> 21:41.800]  обе они равны нулю, или если они не равны нулю, то в левой части написано кси, в правой части написано
[21:41.800 --> 21:49.680]  А, а еще так они не равны нулю, только если кси больше чем А. Вот поэтому можно написать, что это
[21:49.680 --> 21:57.320]  больше и равно, чем мотождание А на индикатор того, что кси хотя бы А. Константы вносятся за знак
[21:57.320 --> 22:04.040]  мотождания, получается мотождание индикатора. Мотождание индикатора это вероятность. Вот осталось
[22:04.040 --> 22:12.120]  разделить это все на А и доказать с неравенством марки. Второе неравенство, неравенство Чебышова.
[22:12.120 --> 22:20.680]  Оно утверждает, что для любого Эпсилон больше нуля, вероятность кси отличается от своего
[22:20.680 --> 22:31.680]  мотождания больше, чем на Эпсилон, не превосходит дисперсии, деленной на Эпсилон квадрат. Это
[22:31.680 --> 22:40.120]  практически прямое следствие неравенства Маркова, потому что ну надо просто взять случайную величину
[22:40.120 --> 22:48.480]  вот такую квадрате, а в качестве А взять Эпсилон квадрат. Тогда если мы подставляем
[22:48.480 --> 22:56.240]  этого неравенства Маркова, мы получаем вероятность, что вот такой квадрат больше, чем Эпсилон квадрат,
[22:56.240 --> 23:05.440]  не меньше, чем мотождание кси минус мотождание кси квадрате делить на Эпсилон квадрат. Как раз
[23:05.440 --> 23:09.400]  у числителей появляется дисперсия, а вот это неравенство равносильно вот этому неравенству.
[23:09.400 --> 23:17.960]  Неравенство Чебышова позволяет с помощью дисперсии оценить разбор значений случайной величины, как-то
[23:17.960 --> 23:27.600]  понять, насколько мы можем отличаться от своего мотождания. Например, давайте в качестве Эпсилон
[23:27.600 --> 23:40.520]  возьмем, например, 10 корней из дисперсии. И поставим неравенство Чебышова. Получится,
[23:40.520 --> 23:52.680]  что вероятность вот такая не больше, чем 0,01. Это значит, что с вероятностью 0,99 случайная
[23:52.680 --> 24:00.560]  величина принимает значение близки к своему мотомическому ожиданию, близки в том смысле,
[24:00.560 --> 24:13.080]  что не больше, чем на 10 корней из дисперсии отличается. Вероятностью 0,99 кси это мотождание кси
[24:13.080 --> 24:22.120]  плюс минус 10 корней из дисперсии. Когда я пишу плюс минус, я имею ввиду, что мы в интервале вот
[24:22.120 --> 24:28.760]  таком. Поэтому очень круто, когда дисперсия маленькая, когда мы что-то знаем про значение
[24:28.760 --> 24:37.240]  случайной величины. Такие они. Окей. Все, что не нравится с Чебышовым, мы можем доказать закон
[24:37.240 --> 24:44.960]  больших чисел. Закон больших чисел в форме Чебышова в следующем семестре у вас будет много
[24:44.960 --> 24:53.960]  законов больших чисел в каких-то других формировках. В этом семестре только один
[24:53.960 --> 25:05.720]  будет в форме Чебышова. Как он звучит? Есть кси1, кси2, кси3, последовательность независимых,
[25:05.720 --> 25:14.560]  одинаково распределенных случайных величин. При этом мы знаем, что существует конечная дисперсия
[25:14.560 --> 25:21.760]  ну, например, кси1. Я замечу, что, во-первых, если существует дисперсия, то автоматически
[25:21.760 --> 25:27.060]  должно существовать и математическое ожидание, потому что дисперсия определяется через
[25:27.060 --> 25:34.920]  математическое ожидание. Во-вторых, формировки теоремы у нас тут случайные величины имеют
[25:34.920 --> 25:41.320]  одинаковое распределение, а дисперсия это характеристика, которая зависит только от
[25:41.320 --> 25:50.560]  распределения случайных величин. Таким образом, если у кси1 существует дисперсия, то и у остальных
[25:50.560 --> 25:56.680]  случайных величин тоже существует дисперсия, и они причем равны. Можете еще раз, что такое NORSP?
[25:56.680 --> 26:05.840]  Давайте я напишу. N это независимые, OR это одинаково распределенные, то есть имеют одинаковое
[26:05.840 --> 26:11.500]  распределение, то есть одинаковый набор значений, и эти значения они принимают с одними теми же
[26:11.500 --> 26:19.960]  вероятностями. SV это случайные величины. Окей, я не закончил формировку. Формировка такая, вероятность,
[26:19.960 --> 26:28.440]  что средняя отличается от математического ожидания. Давайте я вот так напишу. Вот тут
[26:28.440 --> 26:38.600]  напишу А. Вот здесь напишу, что А это математическое ожидание. Это вероятно, что среднее значение
[26:38.600 --> 26:46.720]  ксишек отличается от математического ожидания больше, чем на епсилон. Тремиться к нулю для
[26:46.720 --> 26:56.240]  любого положительного епсилона при н-сремящемся бесконечности. Вот так. Идейно, что здесь происходит.
[26:56.240 --> 27:05.000]  Идейно, по сути, закон Больших учитель, это о том, что средняя архитектическая в каком-то смысле
[27:05.000 --> 27:17.200]  стремится к своему математическому ожиданию. Окей. Доказательства. Я буду применять неравенство
[27:17.200 --> 27:27.360]  Чебышова. Давайте вспомним, что такое неравенство Чебышова. Посмотрели. В качестве кси в неравенстве
[27:27.360 --> 27:36.560]  Чебышова я возьму вот это среднее арифметическое. Дальше я утверждаю, что вот это А это еще и
[27:36.560 --> 27:44.400]  математическое ожидание кси. Потому что, смотрите, математическое ожидание среднего
[27:44.400 --> 27:51.200]  арифметического 1n можно вынести как константу. Тут получится мат ожидания суммы. Это сумма
[27:51.200 --> 28:01.040]  математических ожиданий. Получается сумма мат ожиданий с нитой. Но случайные величины имеют одинаковое
[28:01.040 --> 28:06.640]  распределение, поэтому все математические ожидания равны, и они равны числу А. Слагаем их на штук,
[28:06.640 --> 28:13.760]  поэтому тут получается А. Окей. То есть, вот эта вероятность, это есть не что иное, как вероятность
[28:13.760 --> 28:25.600]  модуля кси минус мат ожидания кси. И тут применимо неравенство Чебышова. Это не больше, чем дисперсия
[28:25.600 --> 28:36.640]  кси делить на эпсилон квадрат. Окей. Дисперсия среднего арифметического зеленого на н делить на
[28:36.640 --> 28:44.960]  эпсилон квадрат. Константы из дисперсии выносятся в квадрате, в участии н квадрат и эпсилон квадрат
[28:44.960 --> 28:55.920]  в знаменателе, в числителе остается дисперсия суммы. Вот. Случайные величины по условию независимые, тут
[28:55.920 --> 29:04.880]  кстати становится понятно, что мы используем на самом деле попарную независимость. Хватает слабой
[29:04.880 --> 29:11.080]  такой вот независимости, только попарной, чтобы закон больших чисел в форме Чебышова работал.
[29:11.080 --> 29:19.560]  Получается n на дисперсию одного элемента, n сокращается. Вот. Ну дисперсия константа,
[29:19.560 --> 29:25.480]  эпсилон константа, n в знаменателе, значит это все стремится к нулю. И закон больших чисел доказан.
[29:25.480 --> 29:35.360]  Вот. Что тут можно сказать? Я сделаю два замечания. Первое замечание стоит в том, что в формировке
[29:35.360 --> 29:41.120]  законы больших чисел в форме Чебышова можно даже условия независимости ослабить до условия
[29:41.120 --> 29:48.280]  некоррелированности. Независимость можно ослабить до некоррелированности случайных
[29:48.280 --> 29:56.480]  величин. И тоже все будет работать. Ну это связано с тем, что когда мы доказывали вот здесь вот третье
[29:56.480 --> 30:02.520]  свойство дисперсии, то что сумма дисперсии для независимых случайных величин, мы единственное,
[30:02.520 --> 30:07.960]  что использовали, это то, что к вариатору она нулю. То есть по сути мы использовали только некоррелированность.
[30:07.960 --> 30:16.320]  Второе замечание стоит в том, что можно даже ослабить вот это условие одинаковой распределенности,
[30:16.320 --> 30:23.400]  то есть можно не требовать одинаковой распределенности. И вместо этого просто потребовать,
[30:23.400 --> 30:37.040]  чтобы сумма дисперсии не слишком сильно росла. На самом деле закон больших чисел это один из самых
[30:37.040 --> 30:44.120]  важных результатов в теории вероятности наравне с центральной предельной теоремой, которую я только
[30:44.120 --> 30:49.080]  сформулирую, но доказывать не буду. Вам ее обязательно докажут в следующем семестре,
[30:49.520 --> 30:56.920]  Максим Евгеньевич, я надеюсь. Центральная предельная теорема без доказательств. Опять есть
[30:56.920 --> 31:04.440]  последовательность независимых одинаково распределенных случайных величин. Опять же должна
[31:04.440 --> 31:14.560]  существовать конечная дисперсия, когда любых действительных А и Б, причем не только действительных,
[31:14.560 --> 31:27.920]  можно в качестве А и Б брать плюс-минус бесконечность. Вероятность вот такая. Я опять
[31:27.920 --> 31:36.200]  же обозначу математическое ожидание буквой А, а стандартное отклонение буквой сигма,
[31:36.200 --> 31:49.800]  когда вероятность вот этого события сходится к следующему интегралу. Выглядит, конечно,
[31:49.800 --> 31:56.720]  довольно ужасно, наверное, по крайней мере, если еще есть первый раз на это смотреть. Какой-то
[31:56.720 --> 32:06.320]  вопрос? У нас в условии А фигурирует как результат мат ожидания и любое А из Р и плюс-минус бесконечность.
[32:06.320 --> 32:17.120]  Ой, спасибо за комментарий. Это, конечно, беда. Давайте тогда вот так сделаем. Вместо А и Б
[32:17.120 --> 32:27.840]  напишем х и у. Теперь все должно нормально быть. Что здесь по сути-то происходит? По сути,
[32:27.840 --> 32:41.120]  эта теория говорит о том, что мы знаем, что сумма будет примерно nA, потому что средний это
[32:41.120 --> 32:47.120]  примерно A, значит сумма это примерно nA из закона больших чисел. И вот эта теория говорит о том,
[32:47.120 --> 32:56.120]  как ведет себя отклонение от вот этого среднего. Оказывается, оно стремится к нормальному закону,
[32:56.120 --> 33:10.920]  по распределению, к нормальному закону. Что бы это ни значило. Представьте, что вы бросаете кубик и вы
[33:10.920 --> 33:19.280]  очень хотите выбросить 6. Вот вы бросили его, выпало 1, бросили, выпало 3, бросило снова, снова. Вот
[33:19.280 --> 33:28.280]  так вот 10 бросков сделали и шестерка не выпала. Вы в этот момент можете начать разочаровываться в
[33:28.280 --> 33:36.400]  теории вероятности, но на самом деле вам просто не повезло. То есть такое бывает, что вы бросаете
[33:36.400 --> 33:44.120]  кубик и вот хотите шестерку выкинуть, а она не выпадает. Но вот закон больших чисел говорит о том,
[33:44.120 --> 33:52.760]  что вот если много раз выкинуть шестерку тысячу раз, то вот она выпадет. Тут уже как бы неважно,
[33:52.760 --> 34:00.000]  везучие вы, невезучие, она выпадет. Причем выпадет примерно тысяча девять на шесть раз, плюс-минус.
[34:00.000 --> 34:12.800]  А центральная предельная теория говорит о том, как будет располагаться, как распределено отклонение
[34:12.800 --> 34:23.680]  от вот этого среднего. То есть если вы, давайте на примере, пусть шестерка выпадает на 1,6 и вы,
[34:23.680 --> 34:33.640]  допустим, бросаете тысячу раз шестерку, тысячу раз бросаете кубик и тогда количество шестерка будет
[34:33.640 --> 34:42.720]  иметь биномиальное распределение с параметрами NP, тысяча и одна шестая. Биномиальное распределение
[34:42.720 --> 34:52.720]  это сумма независимых бернуливских случайных увеличений, поэтому тут применимо закон
[34:52.720 --> 34:59.440]  больших чисел и центральная предельная теория. Так вот, вот вы провели эксперимент, бросили кубик
[34:59.440 --> 35:06.080]  тысячу раз и посчитали, сколько шестерок выпало. Центральная предельная теория говорит,
[35:06.080 --> 35:12.520]  что вот если вы вот такой эксперимент проведете очень-очень много раз, ну, например, тысячу раз,
[35:12.520 --> 35:20.800]  то есть сделайте миллион бросков, а потом построите гистограмму того, что у вас получилось,
[35:20.800 --> 35:32.720]  то есть там по оси X откладываете количество шестерок в каждом из экспериментов, а по оси Y то,
[35:32.720 --> 35:42.560]  как часто такое количество шестерок у вас случалось, то у вас будет что-то такое и оно
[35:42.560 --> 35:51.120]  будет опроксимироваться нормальным законом. В общем, так себе у меня получается рисовать.
[35:51.120 --> 35:59.600]  Ну, я думаю, многие из вас такие графики уже когда-то видели и вас это не сильно удивляет.
[35:59.600 --> 36:11.480]  Ладно, окей, на этом часть курса, которая относится к теории вероятности, подходит к концу. В следующий раз мы уже
[36:11.480 --> 36:20.360]  начнем заниматься действительным анализом, то есть теория и меры. Пока я скорее просто сформулирую,
[36:20.360 --> 36:29.640]  зачем нам заниматься действительным анализом. Нам нужно заниматься действительным анализом,
[36:29.640 --> 36:36.080]  чтобы ответить на некоторые вопросы. Потому что когда мы говорили про вероятности пространства,
[36:36.080 --> 36:46.160]  тут были какие-то омега, f, p, непонятно, что такое f, что такое p. Когда мы пытаемся вести вероятность,
[36:46.160 --> 36:52.400]  какую-то классическую модель мы можем построить. Вот даже если уже говорите про геометрическую вероятность,
[36:52.400 --> 37:00.120]  то мы в геометрической вероятности уже вероятность определяем, как там, условно там,
[37:00.120 --> 37:07.120]  многомерный объем, делить там на многомерный объем. А объем уже можно считать не у всех множеств.
[37:07.120 --> 37:15.240]  То есть отсюда следует вывод, что, наверное, вероятность в общем случае можно считать далеко не у всех множеств.
[37:15.240 --> 37:22.200]  То есть есть какой-то набор множеств, которые называются событиями, и у них вероятность можно считать, а у остальных не можно.
[37:22.200 --> 37:28.840]  Вот возникает вопрос, что это такое за f, какими свойствами оно должно обладать.
[37:29.560 --> 37:41.560]  Также возникает вопрос, что такое p, как водить вероятность, и что такое вообще вероятность.
[37:41.560 --> 37:47.560]  Потом мы начали говорить про случайные личины, и там еще больше вопросов возникло.
[37:47.560 --> 37:57.560]  Что такое случайная личина в общем случае? Что такое распределение случайной личины в общем случае?
[37:58.280 --> 38:03.280]  Что такое математическое ожидание случайной величины в общем случае?
[38:03.280 --> 38:12.280]  Далее, когда мы вот здесь говорили про законы больших чисел и центральную предельную теорему, тут возникали какие-то сходимости.
[38:12.280 --> 38:22.280]  Но хочется понять, что это за сходимость, и какое вообще смысл в это все складывается.
[38:22.280 --> 38:26.280]  Что такое сходимость случайных величин?
[38:28.280 --> 38:36.280]  В общем, вторая часть курса нужна для того, чтобы научиться отвечать на вот эти вопросы.
[38:36.280 --> 38:46.280]  Если есть какие-то вопросы по сегодняшней лекции или по курсу, то можно спросить.
[38:46.280 --> 38:53.280]  Если нет, то на этом все. Всем пока. До следующего раза.
[38:53.280 --> 38:54.280]  До свидания.
[38:54.280 --> 38:55.280]  До свидания.

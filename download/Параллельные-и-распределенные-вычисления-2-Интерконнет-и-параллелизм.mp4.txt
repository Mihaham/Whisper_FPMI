[00:00.000 --> 00:11.680]  Так, ну приветствую тех, кто пришел на наше второе занятие. Вот, потом будет уже другой человек,
[00:11.680 --> 00:17.920]  а теперь для начала давайте вспомним, что было на прошлом неделе назад занятие. Тогда мы
[00:17.920 --> 00:24.920]  рассмотрели, кто такой вообще суперкомпьютер, классификацию суперкомпьютера по методным
[00:24.920 --> 00:34.120]  организации вычислений, классификация по флину, ну и в дальнейшем классификация машин с массовым
[00:34.120 --> 00:42.800]  параллелизмом, то есть это распределенная память и общая память, два больших подкласса. Машин с
[00:42.800 --> 00:49.280]  массовым параллелизмом, еще раз повторяю, что MPI, который вы будете изучать, он предназначен для,
[00:49.280 --> 00:56.200]  в основном был изначально предназначен для машин с распределенной памятью, а все остальные,
[00:56.200 --> 01:06.000]  к примеру, OpenMP, это для машин с общей памятью. Также мы рассмотрели самые главные характеристики
[01:06.000 --> 01:15.520]  параллельной программы, это ускорение, эффективность, масштабируемость и также рассмотрели то, как и
[01:15.520 --> 01:22.120]  почему у нас, вернее, в параллельных программах график ускорения может отклоняться от идеального
[01:22.120 --> 01:29.600]  луча, от идеальной бисептрисы. Насколько помню, мы закончили на том и успели рассмотреть не только
[01:29.600 --> 01:34.760]  закон AMDAL, но и то, как влияет сеть, или это не рассмотрели, вот это я не помню точно,
[01:34.760 --> 01:41.960]  не рассмотрели, закон AMDAL мы рассмотрели, а вот потом я что-то не очень помню.
[01:45.520 --> 01:54.960]  На чем мы остановились? Закон AMDAL, да? А потом уже все. Так, ладно, хорошо, тогда это рановато, сейчас мы...
[01:54.960 --> 02:08.120]  Ага, или не ага. Да, тогда еще один момент. Так, ну, да.
[02:08.120 --> 02:22.680]  Теперь я не вижу. На секунду. Так, наверное.
[02:22.680 --> 02:30.960]  Так, понятно.
[02:38.720 --> 02:39.640]  Вот здесь.
[02:39.640 --> 02:46.120]  Мы здесь остановились, да?
[02:46.120 --> 03:10.880]  Вот здесь, да? И все. Правильно? Так, ну, зато теперь я не вижу. Ну, ладно. Вот так. Да, значит,
[03:10.880 --> 03:19.000]  давайте тогда с этого начнем. Итак, закон AMDAL о чем нам говорит? Он нам говорит о влиянии той части
[03:19.000 --> 03:27.400]  алгоритма, которая не может быть распараллелена. То есть, в итоге, отклонение ускорения может быть
[03:27.400 --> 03:35.280]  связано с тем, что... с несколькими причинами. Первая причина это в самом алгоритме, когда у него
[03:35.280 --> 03:43.120]  есть такие части, которые невозможно распараллелись. Вторая связана с железом как таковым, когда у вас
[03:43.120 --> 03:50.040]  процессы работают на тех или иных узлах, и если они решают одну большую задачу, то для передачи
[03:50.040 --> 03:54.560]  данных, рано или поздно им придется обмениваться, и вот для передачи данным им придется воспользоваться
[03:54.560 --> 04:00.320]  сетью, которая соединяет узлы. Она называется интеркоммуникационной сетью или интерконнектом,
[04:00.320 --> 04:08.840]  и вот здесь есть тоже определенные проблемы, ну как проблемы, влияние этой сети на то, что вы
[04:08.840 --> 04:15.680]  хотите получить на ускорение. Каким образом? Ну вот, к примеру, смотрите, вы хотите посчитать сумму
[04:15.680 --> 04:22.080]  массива. Вообще эта задача обладает довольно-таки высокой степенью внутреннего параллелизма, как можно
[04:22.080 --> 04:28.720]  видеть. Массив, грубо говоря, можно разить на пары, и каждую пару независимо рассуммировать. Ну и
[04:28.720 --> 04:35.560]  потом их там слить. Это мы тоже попробуем успеть, ну попробуем успеть сегодня рассмотреть. Итак,
[04:35.560 --> 04:41.840]  если у нас время одной рифметической операции tau c, а длина массива n, то есть он состоит из n
[04:41.840 --> 04:49.360]  элементов, то получается, что время работы последовательного алгоритма будет n tau c,
[04:49.360 --> 04:56.720]  но здесь вроде все нормально. Теперь предположим, что у нас есть p процессов. Здесь я говорю, что,
[04:56.720 --> 05:03.440]  когда я говорю о процессах, я предполагаю, что они работают каждый на своем собственном процессоре,
[05:03.440 --> 05:09.680]  то есть физическом воплощении вычислительного устройства или вычислительном ядре. И сейчас
[05:09.680 --> 05:15.920]  я предполагаю, что один процесс работает на одном узле. Вот так, упрощенная картина такая. Значит,
[05:15.920 --> 05:22.360]  если мы собираемся запустить нашу программу на p процессах, то что? Чему будет равно время работ
[05:22.360 --> 05:30.400]  нашего алгоритма? Вот в идеальном случае это будет 1 поделить на p. То есть в идеальном случае,
[05:30.400 --> 05:37.680]  когда у нас каждый из процессов работает на своем процессоре. То есть n tau c поделить на p. Теперь
[05:37.680 --> 05:46.560]  в реальности на самом деле к этому времени добавляется еще прибавочка. Прибавочка связана
[05:46.560 --> 05:51.800]  как раз с временем на пересылки. Ну, не важно пока. Накладные расходы. В данном случае,
[05:51.800 --> 05:57.080]  если мы рассматриваем только Interconnect, значит, это именно те самые времена на пересылки.
[05:57.080 --> 06:10.800]  Теперь представим себе, что у нас просуммировано было p процессами, и они единожды передали свой
[06:10.800 --> 06:16.520]  результат какому-то главному процессу. Ну, для простоты напишем, что это один раз они это сделали
[06:16.520 --> 06:26.760]  пересылку. Тогда если посчитать ускорение, то получим следующее выражение. То есть n tau c поделили
[06:26.760 --> 06:37.600]  вот на вот это выражение, сократили и получили выражение для ускорения. Что здесь интересного?
[06:37.600 --> 06:44.920]  Смотрите, если у нас нет пересылок, то у нас получается идеальный случай. Вот 1 поделить на
[06:44.920 --> 06:49.280]  вот эту дробь, получится p. s равно p. Это та самая бисектриса, которая нам дает идеальный случай.
[06:49.280 --> 06:59.440]  Теперь все портит. Второе слагаемое в знаменателе. Что здесь стоит за величины? Tau s это значит
[06:59.440 --> 07:04.480]  характерное время пересылки. Tau c – характерное время выполнения одной арифметической операции.
[07:04.480 --> 07:08.360]  N – это длина массива. То есть сколько арифметических операций нужно сделать.
[07:08.360 --> 07:19.280]  Ну, теперь рассмотрим разные случаи. Ну, вернее, начала пару, два случаев. Один и потом чуть-чуть
[07:19.280 --> 07:26.200]  дальше второй. Значит, у нас есть такое выражение. И мы теперь хотим, чтобы найти такой пограничный
[07:26.200 --> 07:31.440]  случай, когда у нас ускорение примерно равно единице. Ну, пусть оно будет немножко меньше,
[07:31.440 --> 07:37.680]  чем единица. Если оно немножко меньше, чем единица, представим себе, это значит, что p
[07:37.680 --> 07:46.400]  велико. И тогда у нас это слагаемое мало. Если s чуть-чуть меньше, чем единица, значит вот это
[07:46.400 --> 07:52.000]  выражение порядка единицы или немного больше. Ну, может быть порядка единицы, потому что к нему
[07:52.000 --> 07:57.560]  добавляется еще малая часть. Получается, что если мы собираемся найти пограничный случай, когда оно
[07:57.560 --> 08:02.320]  вообще стоит, не стоит считать. Если ускорение меньше единицы, то, конечно же, совсем прям не стоит.
[08:02.320 --> 08:09.040]  Если s больше единицы, то там нужно смотреть, насколько больше. Итак, пограничный случай. s порядка
[08:09.040 --> 08:14.600]  единицы или чуть меньше. Получается, что второй слагаемый знаменатель порядка единицы. Теперь как
[08:14.600 --> 08:25.480]  его оценить? Вот есть характерные времена икей. Ethernet и InfiniBand. Ну, сейчас Ethernet и получше.
[08:25.480 --> 08:32.680]  Для оценки нам, в общем-то, не принципиально. Итак, что здесь есть? Это время, которое требуется на
[08:32.680 --> 08:40.120]  передачу вот такого количества данных. Тут видно, что если мы используем интернет, то время растет с
[08:40.120 --> 08:48.160]  увеличением объема данных. А когда мы говорим об InfiniBand, мы видим, что время, в общем-то,
[08:48.160 --> 08:55.120]  не очень растет при росте данных. Какие еще есть характеристики этой сети? Есть у них пропускная
[08:55.120 --> 09:02.040]  способность. Сколько они могут передать данных в секунду? Это раз. Ну, отсюда можно, в принципе,
[09:02.040 --> 09:12.160]  достать. У них есть время, спустя которое они после одной передачи могут начать вторую передачу.
[09:12.160 --> 09:22.320]  Есть время, некое ожидание. Мы видим, здесь данные не передаются, а время уже идет. Время подготовки
[09:22.320 --> 09:27.680]  данных передачи. Вот это время, вот эта ступенька, она называется латентностью. Ну, если мы говорим,
[09:27.680 --> 09:34.000]  например, о наиболее быстрой сети InfiniBand, то для оценки нам как раз вот эта латентность и потребуется.
[09:34.000 --> 09:41.280]  Ну, видно, что если мы говорим об Ethernet, то это в районе 50 микросекунд, то Ethernet это 3-5
[09:41.280 --> 09:48.800]  микросекунд. Сейчас даже немного меньше. Давайте до определенности возьмем 5 микросекунд.
[09:48.800 --> 09:59.880]  Выфорки хорошие получаются. 5 на 5 делится, все такое. Ой, куда я попал? Вот сюда вернулись.
[09:59.880 --> 10:11.200]  Итак, TAUS 5 микросекунд. Хорошо. Ну, как вы думаете, маленькое это или большое время? Достаточно
[10:11.200 --> 10:24.560]  мало. Я просто не расслышал, мало или большое. Вот, смотря с чем сравнивать. Ну, вы так, на своем
[10:24.560 --> 10:42.440]  жизненном опыте, вот как вы думаете, это маленькое? По графику? Ну, да, похуже, но это для красивых цифр.
[10:42.440 --> 10:53.120]  Ну, неважно, 5 микросекунд. Микросекунды это коротенькое время. Да, это мы сейчас рассмотрим.
[10:53.400 --> 10:58.840]  По жизни, вроде, кажется, маленькое время. Давайте сравним с арифметической операцией. Как сравнить с
[10:58.840 --> 11:08.680]  арифметической операцией? Ну, да, пусть там за один такт одна вычислительная операция. Нам оценки
[11:08.680 --> 11:15.920]  нужны, нам точно пока. Нам прикинуть нужно. За один такт и какой-нибудь возьмем 2 ГГц. Почему 2 ГГц?
[11:15.920 --> 11:22.320]  Потому что, если мы обратную величину возьмем, то тут пятерка тоже. Красивые цифры.
[11:22.320 --> 11:32.600]  Сколько получается? 10 минут 10. Теперь мы можем поделить одно на другое, получить 10 четвертый. То
[11:32.600 --> 11:40.800]  есть время передачи по сети в 10 тысяч раз больше, чем время одной операции. То есть это большая,
[11:40.800 --> 11:47.080]  на самом деле, разница. Очень большая. И даже, если, например, мы вообще возьмем сеть, которая
[11:47.080 --> 11:52.280]  побыстрее, например, там одна микросекунда, ну, у нас будет результат в 5 раз меньше. 2000. Ну,
[11:52.280 --> 11:58.600]  не принципиально. Здесь главное понять следующее, что сеть у нас влияет и каким образом. А тем,
[11:58.600 --> 12:06.120]  что она замедляет. Замедляет почему? Потому что она входит в формулы для ускорения. И получается,
[12:06.120 --> 12:13.640]  что у нас для того, чтобы получить ускорение порядка единички, нужно то, что N должно быть,
[12:13.640 --> 12:19.880]  то есть N количество элементов массиве в простом, в простой задаче суммы элементов, оно должно быть
[12:19.880 --> 12:27.800]  уже 10 четвертый. Да, если там что-то сеть получше, ну, поменьше будет. Но все равно. Это ускорение
[12:27.800 --> 12:33.720]  равно единице. Мы еще не добиваемся большого ускорения. Так, а если мы хотим ускорение, ну,
[12:33.720 --> 12:41.200]  хотя бы это уже для простоты P поделить на 2. Почему П поделить на 2? Если вернем сюда. Если вот это
[12:41.200 --> 12:49.000]  выражение будет 1 на P, то S будет P на пополам. Ну, для оценки тоже. То есть, второе слагаемое будет
[12:49.000 --> 12:55.840]  равно 1 на P. Тогда у нас количество элементов для достижения такого ускорения, это уже неплохое,
[12:55.840 --> 13:04.600]  нормально. P, например, 100, ускорение 50. Ну, с этим можно жить. P на 10 четвертый. Если P 100,
[13:04.600 --> 13:12.640]  здесь 10 четвертый, значит уже миллион элементов должно быть даже на такой простой задаче. Все. И
[13:12.640 --> 13:23.120]  получается, что вот это вот отношение Tau S на Tau C, оно на самом деле очень часто встречается при
[13:23.120 --> 13:31.720]  оценках на ускорение и времени работы параллельных программ. В дальнейшем мы сейчас поймем. Таким образом,
[13:31.720 --> 13:38.840]  что отсюда какой вывод? Если из Дакона Амдалла можно сделать вывод о том, что нужно браться за
[13:38.840 --> 13:44.760]  распараллелие такой задачи, которую в принципе можно распараллелить. Либо ту часть, которая
[13:44.760 --> 13:49.920]  нельзя распараллелить, она пренебрежима мала. То здесь уже мы взяли алгоритм, который можно
[13:49.920 --> 13:55.240]  распараллелить. Распараллелили. Вы запустили. И видим, что у нас ускорение тоже далеко от
[13:55.240 --> 14:02.520]  идеального может быть. Почему так? Ну, вот почему то, что сеть влияет. Кстати, например, тоже такой
[14:02.520 --> 14:11.400]  пример. Ну, не знаю. Вы, наверное, это будете проходить? Наверное. Я не знаю. Ну, давайте. Вот это, наверное.
[14:11.400 --> 14:22.840]  Правильно? Смотрите, вот график. То есть, если вы будете строить график ускорения от количества
[14:22.840 --> 14:33.640]  процессов. Ну, он будет, во-первых, начинаться там в точке 1.1. Это вот идеальный случай. Так вот,
[14:33.640 --> 14:39.480]  оказывается, что даже если у вас есть неплохая машина, даже если одна и та же программа, то
[14:39.640 --> 14:46.760]  ускорение может пойти вот так. Вот так. И может даже как-то вот так.
[14:50.440 --> 14:54.760]  Одна и та же программа, одна и та же машина. Но ускорение разное.
[14:54.760 --> 15:12.120]  Давайте еще раз ускорение выпишем. Ускорение это будет чему равна 1, 1 на p плюс 1 на n, tau s, tau c.
[15:12.120 --> 15:21.760]  Ничего не меняется. Все остается тоже так. Но что-то чуть-чуть меняется. Начальная данная немного
[15:21.760 --> 15:29.720]  меняется. И вот так себя ведет ускорение. Первый случай, второй случай, третий. Я надеюсь,
[15:29.720 --> 15:37.400]  вы это разберете. Ну, тут, например, какой-то p. p со звездочкой. Или p0. Давайте. Смотрите,
[15:37.400 --> 15:49.080]  а если при p0 чему будет равно 1 на p0 плюс 1 на n, tau s, tau c. Какая здесь переменная,
[15:49.080 --> 16:00.760]  как она называется. Что здесь может гулять, если мы зафиксировали p? Вот это. Получается,
[16:00.760 --> 16:10.360]  что это? Ну, порядка. Ладно, напишем вот так. Порядка 1. Ну, давайте вот так. p0.
[16:10.360 --> 16:35.600]  Напишем. Итак, что здесь будет? 1 плюс здесь будет p, правильно? p0 на n, tau s, tau c. Значит,
[16:35.600 --> 17:04.080]  давайте не так напишем. Вот здесь четвертый поставим. Итак, смотрите, если у нас или у вас n,
[17:04.080 --> 17:15.080]  например, тысяча. Давайте с третьего случая начнем. n, ну, примерно 10 с третьей. Ускорение
[17:15.080 --> 17:30.880]  чему будет равно? p0, 10, p0 плюс 1. Меньше это единица? Ну, вроде меньше, да. Так,
[17:30.880 --> 17:46.600]  второй случай. n равно, ну, пусть будет 2 на 10 четвертый. s будет примерно p0. Так, 1 плюс p0 пополам.
[17:46.600 --> 18:00.040]  Ну, как-то так. Чему это равно? Ну, не знаю. Пусть будет порядка единицы. Зависит от p0. Немножко
[18:00.040 --> 18:10.480]  больше. Может быть, немножко меньше. Ну, как-то так. И третий случай, например, 10 шестой. s будет
[18:10.480 --> 18:27.760]  порядка p0, 1 плюс p0 делить на 100. У нас p0, ну, тут, например, там 10, да. У нас это будет p0,
[18:27.760 --> 18:40.720]  вот большая единица. Давайте так напишем. Ну, то есть, уже ускорение будет большое. Ну,
[18:40.720 --> 18:47.680]  здесь до 10 даже может быть. Пусть будет 10. Это может показать, что 10, 20 такого рода. Уже такое
[18:47.680 --> 18:53.960]  может наблюдаться это точно. Все у нас зависит. Программа одноэтажа, машина одноэтажа. Все зависит
[18:53.960 --> 19:03.720]  от сложности задачи. Вот от этого. А почему такая зависимость? Потому что у нас есть эта штука.
[19:03.720 --> 19:09.600]  Время пересылки. Если бы его не было, у нас не было бы никаких зависимостей. Все было бы хорошо.
[19:09.600 --> 19:18.840]  Значит, какой отсюда вывод? Что на хорошей машине нужно считать хорошие задачи. Большие задачи,
[19:18.840 --> 19:24.120]  сложные задачи. Нет смысла считать что-то простенькое, которое можно посчитать в карманном
[19:24.120 --> 19:32.360]  калькуляторе. Ну, вот и все. Вот такой вывод. То есть, первый мы разобрали. Закон нам дал это. Сам
[19:32.360 --> 19:38.720]  алгоритм может быть не идеален. Второе влияние сети. И третье сейчас мы тоже рассмотрим. Это уже то,
[19:38.720 --> 19:48.960]  как вы подойдете к процессу написания алгоритма. Это железо. А есть еще с точки зрения программы.
[19:48.960 --> 19:56.240]  Можете пойти не с той стороны и у вас уже время будет, может быть, не самое хорошее. Так,
[19:56.240 --> 20:02.120]  ну давайте тогда дальше пойдем. Сейчас, правда, будет вам для вас не очень, наверное, интересно.
[20:02.120 --> 20:08.680]  Ну, я постараюсь этот неинтересный момент и пропустить. Просто они связаны
[20:16.760 --> 20:25.240]  немножко с физикой. Ну, потому что есть задача. В общем, с математикой я не знаю. Я просто не
[20:25.240 --> 20:32.480]  математик. Может и есть такие задачи. А вот из физики они точно есть. В чем они связаны? Они
[20:32.480 --> 20:43.760]  связаны с локальной зависимостью по данным. Что тут добавить надо? Они связаны с локальной
[20:43.760 --> 20:49.960]  зависимостью по данным. И это хорошо просто видно. Просто сейчас не пугайтесь. Я, может быть,
[20:49.960 --> 20:56.040]  быстро поскочу. Задача связана с распространением тепла. Есть нагретый стержень. Он нагрет до
[20:56.040 --> 21:01.880]  температуры единичка. Нулевой момент времени слева и справа присоединяются большие резервуары
[21:01.880 --> 21:10.680]  с константной температурой. Их температура равна нулю. Что будет со стержнем? А? Флаждация будет.
[21:10.680 --> 21:16.360]  Причем с двух сторон одновременно и симметрично. Вот как этот процесс описывается? Процесс
[21:16.360 --> 21:20.440]  описывается вот таким уравнением. Вы должны знать правильное уравнение частных производных.
[21:20.440 --> 21:24.520]  Вот, все замечательно. Я не знаю, были у вас вычислительные методы или нет, но эти
[21:24.520 --> 21:30.440]  производные приближаются конечными разностями. Отличие состоит в том, от равно и вот примерно
[21:30.440 --> 21:35.480]  равно в том, что здесь нет предела из определения производной. То есть здесь производная будет равна
[21:35.480 --> 21:40.200]  пределу вот такого отношения, когда у нас доминация стремится к нулю. Правильно? Как
[21:40.200 --> 21:44.600]  только мы этот предел убираем, не рассматриваем бесконечно мало величины, а рассматриваем
[21:44.600 --> 21:51.160]  конечные величины, то мы здесь можем написать примерно равно. Напоминаю, что индексы сверху это
[21:51.160 --> 21:59.640]  отвечают за время. Известно вам это все, да? Вот. Нет? Ну мы разбиваем область нашего интегрирования,
[21:59.640 --> 22:07.840]  область решения разбиваем на дискретизируем, так называем, по пространству. То есть вот наш стержень.
[22:07.840 --> 22:22.800]  Вот это стержень, и мы его разбиваем. Ну дальше там тоже будет видно. Разбиваем
[22:22.800 --> 22:34.040]  на координаты, на отрезки такие элементарно. У них есть какой-то delta x ширина, вот это l.
[22:34.040 --> 22:41.080]  Если мы уж рассматриваем, у нас есть производная по времени, направляет еще
[22:41.080 --> 22:52.800]  вертикально ось по времени. Ось по времени. И это называется эволюционная задача. Это значит,
[22:52.800 --> 22:59.640]  что у нас наши температуры, они со временем изменяются. Они изменяются, как по пространству на
[22:59.640 --> 23:04.320]  каждом шаге по времени. И в итоге со временем тоже они меняются. То есть каждая точка будет
[23:04.320 --> 23:11.840]  меняться. То есть можно, двигаясь во времени, мы можем получить новые температуры. Вот здесь.
[23:11.840 --> 23:21.640]  Ну и так далее. И вот где-то еще тоже новые температуры получаем. Так вот, точки по пространству,
[23:21.640 --> 23:29.240]  они обозначаются, какая-то точка неизвестная. А и буква. И пишется индекс внизу. То есть
[23:29.240 --> 23:37.120]  температура в точке i в момент времени n нулевое. Это будет вот здесь. Это вот какой-то момент
[23:37.120 --> 23:56.800]  н. Вот этой будет у, н, и. Вот это будет у, и, плюс один, n. Вот это у, н, и, минус один. А здесь
[23:56.800 --> 24:07.480]  будет у, н, плюс один, точки и. Ну это дальше будет видно. То есть то, что сверху, это относится
[24:07.480 --> 24:23.520]  ко времени. Это время. А вот это пространство. Получается, что когда мы пишем такую производную,
[24:23.520 --> 24:31.160]  то у нас раскрываем на конечной разности, подставляем в это уравнение и получаем вот такой вид.
[24:31.160 --> 24:39.280]  Если мы берем самую простую явную схему, когда у нас справа отравно стоит, стоит температура на
[24:39.280 --> 24:45.720]  том же временном слое. А вот здесь мы видим, что у нас стоит температура в будущем. Ну пока можно
[24:45.720 --> 24:50.840]  сильно не задумываться. Если мы выразим неизвестные через известные, а вот эти у нас величины известные,
[24:50.840 --> 24:56.760]  то получим выражение. Каждый раз, используя эту формулу, мы в какой-то точке можем находить
[24:56.760 --> 25:03.800]  температуру, зная соседние точки в настоящем. Все таким образом, зная уравнение, мы можем найти
[25:03.800 --> 25:10.400]  полную температуру вдоль стежни в любой момент времени в будущем. Ну вот в этом заключается задача.
[25:10.400 --> 25:17.520]  В чем, собственно, заключается соль? Почему она интересна? Дело в том, что, ну это ладно,
[25:17.520 --> 25:24.000]  я опущу. Вот если рассмотреть шаблон разной схемы, вот он шаблон, вот здесь он тоже, на экранчике можно
[25:24.000 --> 25:40.480]  его выделить таким образом. Он показывает зависимости. Показывает зависимости, то есть мы не можем
[25:40.480 --> 25:45.200]  просто так разделить наш стержень и отдельные его части, считать каждым из процессов от и до,
[25:45.200 --> 25:50.320]  вне зависимости от других процессов. Почему? Потому что у нас есть шаблон. Вот такой разный
[25:50.320 --> 25:55.600]  схемы. Видно, что температура в будущем зависит от температуры в этой же точке в настоящем и
[25:55.600 --> 26:03.160]  в соседних точках тоже в настоящем. Вот шаблон. Теперь, если мы попытаемся хотя бы на двух
[26:03.160 --> 26:10.320]  процессах его рассмотреть, ну это примерно то же самое, что я нарисовал, если у нас есть на 10
[26:10.320 --> 26:16.680]  сегментов, то можно рассматривать температуру внутри сегментов, можно на концах элементарных
[26:16.680 --> 26:22.200]  отрезков. Это вопрос договоренности. Вот если на концах, то у нас будет 11 точек, включая вот внешний,
[26:22.200 --> 26:27.680]  9 внутренних и еще плюс 2 внешних. И как поделить, например, на два процесса? Если у нас кластер
[26:27.680 --> 26:33.720]  однородный, то есть он состоит из примерно одинаковых процессоров, значит мы можем примерно пополам
[26:33.720 --> 26:40.200]  разделить 6 элементов одному процессу направить, 6 другому. И вот теперь, что здесь получается? Вот
[26:40.200 --> 26:48.600]  мы их разделили наш стержень на почти равные части, на две почти равные части. Одну часть
[26:48.600 --> 26:58.400]  обчитывает один процесс, вторую часть обчитывает другой процесс. Вот так. Да, здесь есть некая
[26:58.400 --> 27:04.840]  воображаемая граница между двумя процессами. Теперь, если наложить на вот эту картинку шаблон
[27:04.840 --> 27:09.000]  нашей разности схемы, мы видим, что для того, чтобы посчитать температуру в этой точке, в шестой
[27:09.000 --> 27:15.440]  точке в будущем, нам необходима температура в седьмой точке в настоящем. Но она не хранится в
[27:15.440 --> 27:23.280]  процессе номер ноль. Значит ее нужно получить. Точно так же у процесса номер один. Накладываем
[27:23.280 --> 27:28.240]  шаблон, видим, что температуру точка в шесть у него отсутствует, она не хранится у него. Значит ее
[27:28.240 --> 27:33.640]  нужно вот как раз здесь и нужно пересылать данные. И здесь есть зависимость по данным, уже локальность,
[27:33.640 --> 27:39.760]  существует локальная такая зависимость. Это к чему она приводит? К тому, что каждый шаг по
[27:39.760 --> 27:46.280]  времени нужно обмениваться данными. То есть данными нужно обмениваться. Хорошо, мы рассмотрели сейчас два
[27:46.280 --> 27:52.880]  процесса, а что если процессов больше? Но это тоже пропущу, это как обмениваться, это сейчас не суть,
[27:52.880 --> 27:59.840]  можете потом потом рассмотреть. Тут просто создаются эффективные ячейки, они черточками
[27:59.840 --> 28:05.480]  показаны. Вот у них и записывается. Это он даже на семинарах рассказать. И вот тут заключается метод
[28:05.480 --> 28:10.520]  геометрического параллелизма. Чем он заключается? Он заключается в том, что наша расчетная область
[28:10.520 --> 28:17.160]  делится на части и каждая часть относится к какому-то вычислителю, процессу. Он часто используется
[28:17.160 --> 28:24.080]  этот способ, например, в задачах математической физики, либо при обработке, ну при визуализации,
[28:24.080 --> 28:30.320]  при обработке массивов данных каких-то. То есть он часто применяется, когда просто данные, грубо
[28:30.320 --> 28:35.400]  говоря, делятся на части и каждая часть обрабатывается тем или иным процессом. Ну вот теперь давайте
[28:35.400 --> 28:44.640]  рассмотрим, а как подход к тому как вы будете передавать данные влияет на ускорение. Это уже
[28:44.640 --> 28:50.960]  правильный подход, разумная логика. Ну в общем здесь трудно сказать как лучше правильно, но этому нужно
[28:50.960 --> 28:57.160]  научиться, понятно. И вот сейчас вы находитесь здесь в аудитории для того, чтобы на этом научиться,
[28:57.160 --> 29:04.880]  хотя бы отчасти. Итак, если мы разделим на 7 процессов наш стерженек, то что будет? Значит первое,
[29:04.880 --> 29:12.360]  время расчета на последовательном алгоритме в последовательном варианте будет каким? Тауце,
[29:12.360 --> 29:17.880]  это у нас уже известно время выполнения какой-то одной рифметической операции, n количество
[29:17.880 --> 29:24.000]  элементов массива, то есть количество точек, где смотрится температура, m это количество шагов по
[29:24.000 --> 29:28.920]  времени. То есть мы смотрим температуру, собираемся посмотреть распределение температуры в двуристесте на
[29:28.920 --> 29:37.480]  каком-то моменте по времени. Дело в том, что вот здесь есть дельта х, а есть еще шаг по времени,
[29:37.480 --> 29:44.280]  дельта t, ну в производном он есть, когда конечные разности рассматриваются. Если мы собираемся
[29:44.280 --> 29:49.240]  какой-то момент времени рассмотреть, то вот этот момент времени, поделенный на дельта t, дает нам
[29:49.240 --> 29:57.560]  количество шагов по времени. Ну хорошо, это есть, теперь грубо говоря, вот эти все части, они в итоге
[29:57.560 --> 30:04.880]  хранятся в каждом из процессов. Стрелочками показаны направление передачи данных. Если детальнее
[30:04.880 --> 30:11.840]  рассмотреть на уровне функции mpi, то что получается? Каждая стрелочка это означает функцию передачи
[30:11.840 --> 30:17.880]  данных. Вы наверное в семинаре уже у вас были, есть такая mpi send функция. Мы рассматриваем блокирующие
[30:17.880 --> 30:24.280]  операции, значит это send, а галочка это прием mpi receive. Получается, что каждый из процессов,
[30:24.280 --> 30:30.400]  если рассмотреть внутренний, что он делает? Он делает отправку своей левой, своего влево элемента
[30:30.400 --> 30:43.760]  влево, отправка, должен принять вот этот элемент слева, принять элемент вот этот справа и отправить
[30:43.760 --> 30:51.400]  свой крайний элемент вправо. То есть по факту он должен сделать 4 пересылки, 2 отсылки, 2 приема.
[30:51.400 --> 30:57.120]  И получается, что у нас время параллельного алгоритма вроде как должно быть, каждый из них
[30:57.120 --> 31:08.520]  занимает TOS и должно быть таким. T1 делено на P и плюс 4 пересылки. 4 умножить на TOS это за
[31:08.520 --> 31:17.760]  один шаг по времени и таких шагов m. Вот получается вот такое выражение. А теперь вопрос. Это пока из
[31:17.760 --> 31:23.440]  общих соображений, а достижимы ли такое время? И если достижимо, то как?
[31:23.440 --> 31:36.160]  T1 это время последовательного алгоритма, когда считает один процесс от и до всю задачу. Он
[31:36.160 --> 31:43.680]  должен просчитать в каждый момент времени все температуры, их n штук, вычисление одной температуры
[31:43.680 --> 31:50.640]  TOS и он должен просчитать во всем времени до конечной точки, до конечного времени таких m шагов.
[31:50.640 --> 31:57.480]  Понятно? Да, количество процессов.
[31:57.480 --> 32:08.040]  Время P это количество процессов. Время параллельного алгоритма или parallel.
[32:08.040 --> 32:17.400]  Вот это? Так я и говорю, вот если мы рассмотрим один процесс, то ему нужно сделать 4 акта пересылки,
[32:17.440 --> 32:32.960]  2 отправки и 2 приема. Каждый из них TOS. А вот это m. В одном итоге, если он хочет процесс,
[32:32.960 --> 32:38.840]  что делать? Вот на одном временном шаге, для того чтобы ему сделать расчет и получить температуру в
[32:38.840 --> 32:43.720]  будущем, ему нужно получить вот эти граничные точки, а свои отдать. Вот как только он обменяется,
[32:44.600 --> 32:48.520]  у него есть вся информация, вся база, чтобы сделать следующий шаг по времени получить.
[32:48.520 --> 32:56.080]  Поэтому на одной временной итерации он делает всего лишь 4 акта пересылки. То есть 2 отсылки,
[32:56.080 --> 33:03.240]  2 приема. А потом он несколько раз за всю программу, конечно, на одном. Вот он следующий
[33:03.240 --> 33:10.480]  слой получил, например во второе время, потом снова они обменялись, получил третье время,
[33:10.480 --> 33:16.960]  снова обменялись, четвертое, снова обменялись, пятый и так далее и так далее. То есть на каждой
[33:16.960 --> 33:23.760]  итерации он должен обменяться данным для того, чтобы получить новые времена. Эти обмены нужны
[33:23.760 --> 33:28.120]  для чего? Для того, чтобы исчезать времена в крайних точках. Во внутренних нет, но у него
[33:28.120 --> 33:40.720]  достаточно информации. Есть еще вопрос? Хорошо. Итак, достижим или это время? Давайте попробуем
[33:40.720 --> 33:49.800]  сделать какие-то подходы. Первый подход. Давайте сделаем следующее. Пусть все процессы с ранком
[33:49.800 --> 33:56.240]  больше нуля, ранк это номер процесса, ранком больше нуля отправляют влево. Влево это вот на
[33:56.240 --> 34:01.960]  такой мниматической картинке. Если мы запишем номера процессов, то влево это будет, значит,
[34:01.960 --> 34:08.960]  процессу с ранком на единицу меньше. На картинке вот так удобно. Влево они отсылают. Нулевой они
[34:08.960 --> 34:14.680]  отсылают. Почему? Потому что у него слева никого нет. Так, они все отсылают влево. Давайте напишем.
[34:14.680 --> 34:24.120]  Все отсылают вправо. Стрелочками показываем. Потом они принимают. Ну, например, влево принимают
[34:24.120 --> 34:30.000]  справа. Вот такая картинка. Здесь номера процессов. Здесь они тоже представлены выше. Влево по
[34:30.000 --> 34:38.480]  вертикали это количество итераций. В итоге это придем ко времени. Количество итераций. Что получается?
[34:38.480 --> 34:49.560]  Какое время получается? Как вы думаете? Вот сейчас немножко посмотрите, подумайте. Напоминаю,
[34:49.560 --> 34:58.640]  что здесь речь идет о процессах с блокировкой. Что это такое? Напоминаю, что когда процесс выполняет
[34:58.640 --> 35:04.320]  функцию приема или отправки, он останавливается в работу до тех пор, пока эта операция не закончится.
[35:04.320 --> 35:16.840]  Например, нулевой процесс и первый. Стрелочками я буду рисовать скорость их работы. Этот быстро
[35:16.840 --> 35:23.360]  работает, а вот этот работает медленнее. Допустим, в какой-то момент времени нулевой процесс собирается
[35:23.360 --> 35:35.160]  отослать данные с блокировкой. Это значит, что он остановит свою работу. Стоп. И будет ждать. Чего
[35:35.160 --> 35:43.000]  ждать? Пока процесс номер один не дойдет до точки приема. Логическая точка приема. Вот это вот
[35:43.000 --> 35:50.920]  расстояние. Это будет у нас время ожидания. ДЛТТ какой-то waiting. Время ожидания. Как только процесс
[35:50.920 --> 35:58.280]  один достиг этой точки, значит пусть будет какое-то время ожидания. В смысле время пересылки.
[35:58.280 --> 36:10.680]  Идет пересылка. Пересылка. Пересылка. Она, можно сказать, одинаково заканчивается,
[36:10.680 --> 36:20.720]  они одновременно стартуют. С одинаковой скоростью. Вот так. Ну и наоборот, если здесь прием,
[36:20.720 --> 36:30.400]  а здесь отсылка, значит вот это будет взять, пока это и начнет отсылку. Что тут еще? Смотрите,
[36:30.400 --> 36:36.280]  во-первых, называется с блокировкой, а во-вторых, они называются еще синхронного обмена. Здесь сразу
[36:36.280 --> 36:41.840]  видно, почему синхронный. Здесь у них была рассинхронизация во времени, а потом один другого
[36:41.840 --> 36:47.440]  подождал, и они одновременно начали свою работу. То есть можно сказать, что они теперь синхронизованы
[36:47.440 --> 36:59.040]  во времени. Правильно? Итак, значит, вот эти все пересылки являются синхронными функционами
[36:59.040 --> 37:11.080]  приема передачи. Вот. Смотрите, если два процесса, вот они идут и доходят до точки,
[37:11.080 --> 37:23.800]  где у них операция send-send. Этот шлет сюда, этот шлет сюда. Они оба останавливаются до тех пор,
[37:23.800 --> 37:32.120]  пока не получат данные. Смогут они получить данные? Не смогут. Правильно? Потому что они никак не
[37:32.120 --> 37:38.800]  могут пройти. Допустим, немножко дальше, вот здесь вот где-то у них есть функция приема. Но они до них
[37:38.800 --> 37:45.280]  никогда не дойдут. Правильно? Потому что они остановили свою работу на функциях отправки. А здесь
[37:45.280 --> 37:54.320]  где у нас есть? Вот здесь. Называется Tupik, по-английски Deadlock. Вам нужен перерыв или мы продолжим?
[37:54.320 --> 38:05.640]  Ну вот. Значит, Tupik и Чува, ваша программа попросту зависит, так называемая. Но зависеть дальше не
[38:05.640 --> 38:11.280]  будет работать, потому что процессы, два из них, они остановятся, друг друга будут ждать. И вот здесь
[38:11.280 --> 38:17.240]  все остальные тоже будут их ждать в ожидании, потому что никто из них не сможет принять данные. Все они
[38:17.240 --> 38:26.040]  будут в ожидании, в остановленном режиме. Итак, если вы бы так организовали пересылки, то время
[38:26.040 --> 38:35.360]  такого алгоритма будет бесконечной. Чему будет равно ускорение? Рано нулю. Поздравляю. Это то,
[38:35.360 --> 38:41.000]  от чего добиваемся? Наверное, нет. Мы собираемся хотя бы больше единиц и как можно больше,
[38:41.120 --> 38:51.360]  как таблетка от жадности. Побольше, побольше. Потому что вот здесь у нас два процесса будут друг
[38:51.360 --> 38:57.080]  другу слать. Если это операция с блокировкой, они просто остановятся на этих функциях и не
[38:57.080 --> 39:04.080]  смогут дойти до функции приема. Они инициализируют отправку, но не могут ее закончить, потому что на
[39:04.080 --> 39:11.920]  той стороне не смогут принять. Конечно, что так не нужно делать, мы уже понимаем. Нужно делать
[39:11.920 --> 39:20.320]  как-то по-другому. А как по-другому? Вот смотрите, если мы поменяем где-то местами операторы, то есть
[39:20.320 --> 39:33.920]  функция приема и передачи, то есть у нас на логическом уровне на одном и том же будет пары. Тогда
[39:33.920 --> 39:41.680]  у нас будет send-receive. Вот здесь, да, вот если перенести сюда. А здесь будет receive-send. То есть там,
[39:41.680 --> 39:46.200]  где есть send, на таком же временном логическом уровне должен receive обязательно быть. И нужно
[39:46.200 --> 39:55.360]  тоже мысли такими парами. Send значит где-то receive. Где-то send-receive. Receive-send. Вот. Итак,
[39:55.360 --> 40:00.320]  что нужно сделать? Нужно поменять местами. Поменяем местами в самый простой случай. Мы и другие
[40:00.320 --> 40:06.240]  пока не будем рассматривать. Значит здесь мы тоже поменяем местами. Ну, например, send, значит если
[40:06.240 --> 40:10.720]  мы отправляем влево, значит нам нужно принять справа. Send с отправка вправо, нужно поменять
[40:11.560 --> 40:18.160]  но давайте поменяем. Попробуем, что получится. Итак, все отправляют точно так же влево,
[40:18.160 --> 40:25.080]  теперь принимают справа. Вот галочки есть, все принимают справа. Теперь отправляют вправо,
[40:25.080 --> 40:35.880]  принимают слева. Вот такая картинка у нас получается. Еще раз, это номера процессов,
[40:35.880 --> 40:45.920]  вот это вот операции, которая связана с количеством пересылок. Что получается?
[40:45.920 --> 40:53.000]  Вроде как нормально все получается, да? У нас нет, мы не видим стрелочек направленных друг
[40:53.000 --> 41:03.800]  друга, не видим галочек, расходящиеся из одной границы. Да, вы правильно заметили,
[41:03.800 --> 41:10.600]  мы видим число 12. Как получилось число 12? Что это такое? 6 умножить на 2. А что такое 6?
[41:10.600 --> 41:19.040]  Минус 1. То есть это количество внутренних границ. Всего их 7, количество внутренних границ
[41:19.040 --> 41:25.080]  будет 7 минус 1, 6 и два раза. То есть в итоге что получается? Как будто бы, если представить себе
[41:25.080 --> 41:32.200]  так умозрительно или что, как будто бы сигнал о пересылках идет слева направо, а потом справа
[41:32.200 --> 41:40.320]  налево. Да, это как вот такая звука, волна или звук идет. Смотрите, первый, когда начинает
[41:40.320 --> 41:50.000]  отправку, нулевой прием, они заблокированы. Да, вот они заблокированы. Они заблокированы,
[41:50.000 --> 41:57.960]  и теперь второй процесс не может отправить, потому что первый заблокирован. И вот этот
[41:57.960 --> 42:03.840]  обмен произойдет тогда, когда закончится вот этот. Поэтому во временном, вот это время грубо
[42:03.840 --> 42:09.720]  говоря, итерация или время. То есть во времени это будет следующий шаг, следующая итерация. Когда
[42:09.720 --> 42:14.480]  будет этот обмен, третий не может обменяться с вторым, второй заблокирован. А четвертый не может
[42:14.480 --> 42:20.680]  обменяться с третьим. Почему? Потому что ему нужен прием, а третий остановлен на пересылке. Таким
[42:20.680 --> 42:28.040]  образом, они будут передавать данные друг за другом. Этот, потом этот, потом этот. И получается,
[42:28.040 --> 42:34.840]  что у нас будет p-1 граница, сначала в одну сторону, на одной границе, они обменялись в одну сторону,
[42:34.840 --> 42:41.320]  и на этой же в другую сторону. Получается 2p-1, то есть 2 умножить на 6 в 12 итераций. Вот получается
[42:41.320 --> 42:50.960]  такое замечательное выражение. Лучше ли оно от бесконечности? Ну не тот, который мы в самом
[42:50.960 --> 42:57.640]  начале рассматривали. У нас там была четверка? Да, его, наверное, можно улучшить. Но это уже неплохо,
[42:57.640 --> 43:05.680]  неплохо. Мы уже, по крайней мере, получаем какое-то ускорение. Если у нас n большое, а p не очень
[43:05.680 --> 43:13.160]  большое, то ускорение даже может быть очень даже неплохим. Но как его улучшить? Смотрите, вот если
[43:13.160 --> 43:18.920]  посмотреть на обведенные обмены, то что можно заметить про эти обведенные обмены? Что можно про них
[43:18.920 --> 43:32.200]  сказать? А еще что? Ну да, в некотором смысле, да. Здесь не ускорение влияния, а можно сказать
[43:32.200 --> 43:39.200]  следующее, что они могут быть выполнены одновременно. Когда идет обмен между нулевым и первым,
[43:39.200 --> 43:45.440]  второй с первым не может общаться, но ничто не мешает им общаться с третьим. Правильно? Он-то
[43:45.440 --> 43:51.280]  не заблокирован. Первый заблокирован, вот эта пара заблокирована, а вот эта нет. А четвертый
[43:51.280 --> 44:01.160]  с пятым тоже нет. Тоже могут общаться. И получается, что они могут обмениваться сообщениями одновременно.
[44:01.160 --> 44:11.040]  Правильно? Итак, значит, мы нашли механизм, при котором можно улучшить характеристики временной
[44:11.040 --> 44:15.920]  нашего обмена. Но давайте теперь рассмотрим. Видно, да, что это четные сначала принимают,
[44:15.920 --> 44:20.200]  а нечетные отправляют. Ну и потом, четные должны отправить, нечетные принять. Ну вот,
[44:20.200 --> 44:28.040]  давайте это и запишем. Значит, семь процессов. Если у нас это нечетные, то они отправляют влево,
[44:28.160 --> 44:35.120]  все нечетно отправляют влево. Значит, и в этот же момент четные принимают справа.
[44:35.120 --> 44:47.800]  Потом на этой же границе нечетные должны принять, а четно отправить. Все, произошел обмен на одной из
[44:47.800 --> 44:57.880]  границ за две итерации. Отлично? Просто замечательно. И у нас еще остался обмен на этих границах.
[44:57.880 --> 45:02.240]  Точно так же, по такой же схеме. И что здесь теперь получается?
[45:02.240 --> 45:22.400]  Можем, но это неинтересно. Да. А какая разница, какое количество? Вот здесь у нас нечетное
[45:22.400 --> 45:30.560]  количество. Вот смотрите. Ну, четно одного и просто не будет. Вот этого не будет. Вот их
[45:30.560 --> 45:44.000]  четное количество. Просто не будет вот этого обмена. Все. Но он никак не останется. Он должен
[45:44.000 --> 45:49.960]  участвовать тоже в обмене. Вот смотрите, вот просто выкиньте шестой процесс с номером 6 и вот эту
[45:49.960 --> 45:55.360]  границу. Вот так. Видите, все все равно обменились. Просто они тоже четные и нечетные. Какая разница,
[45:55.360 --> 46:03.040]  сколько их? Да, и вот здесь вот что получается? Четыре вирминых итерации. Четыре TOs. Вот мы,
[46:03.040 --> 46:10.920]  в общем-то, и достигли того, что хотели. Получается у нас четыре TOs. И мы, опять же, приходим к той самой
[46:10.920 --> 46:17.840]  формуле. Если теперь, смотрите, мы рассмотрели влияние интерконнекта, да, и все равно вот здесь
[46:17.840 --> 46:27.000]  она появляется. Теперь, если TO1 это mn TOc, поделить на вот это выражение. Получится вот такая штука.
[46:27.000 --> 46:33.920]  И видно, что она очень похожа на то, что было в интерконнекте. Да, только без четверки с точностью
[46:33.920 --> 46:42.880]  до вот этого множителя. Вот опять это у нас вышла дробь. Отношение пересылки, времени пересылки
[46:42.880 --> 46:49.360]  к отношению к времени элементарной операции. Ну все, опять мы видим, что чем больше у нас n,
[46:49.360 --> 47:00.240]  тем лучше ускорение. Дробь нашего знаменателя уменьшается, дробь растет. Замечательно. А здесь
[47:00.240 --> 47:05.760]  еще у нас время пересылок само по себе не зависит от p. Мы можем брать сколько угодно много процессов,
[47:05.760 --> 47:19.320]  и нам за это ничего не будет. Это ж приятно. Все, вот здесь мы завершили рассмотрение именно
[47:19.320 --> 47:26.040]  геометрического парализма и как раз остановимся, подведем итоги. Какие здесь можно итоги подвести?
[47:26.040 --> 47:35.760]  Можно следующее, что то, как вы сделаете обмен, тоже может влиять на ускорение. Оно может быть
[47:35.760 --> 47:43.000]  хорошим, как в этом случае. Оно может быть неплохим, когда, например, время пересылок зависит от p.
[47:43.000 --> 47:49.000]  А может оно совсем неудовлетворительным, когда это у нас получилось равно нулю. И все зависит
[47:49.000 --> 47:56.720]  уже от того, как вы сделаете, например, в данном случае обмен. Как вы его запрограммируете. Таким
[47:56.720 --> 48:02.520]  образом, получается у нас по крайней мере три причины того, почему у нас ускорение может
[48:02.520 --> 48:10.720]  быть неиндеально. Еще раз, это свойство самого алгоритма. Насколько у него там высока степень
[48:10.720 --> 48:20.520]  внутреннего парализма. Второе влияние, по-моему, по себе железа суперкомпьютера. Поскольку все
[48:20.520 --> 48:25.600]  суперкомпьютеры современные, они основаны на том, что узлы соединены с сетью, то сеть в том или
[48:25.600 --> 48:32.520]  ином виде всегда присутствует. Но даже если внутри узла есть несколько ядер и они соединены воедино,
[48:32.520 --> 48:36.760]  то есть это является компьютером с общей памятью, все равно там есть какие-то задержки. С памятью идет
[48:36.760 --> 48:42.800]  работа, так или иначе. Ну и там свои есть уже тоже. Там синхронизация потоков, возможно, она тоже
[48:42.800 --> 48:47.920]  делает замедление. То есть этот ООС в итоге можно свести. Это какие-то накладные расходы. В случае
[48:47.920 --> 48:54.080]  с кластером это время на пересылку данных. В случае машин с общей памяти это просто накладные расходы,
[48:54.080 --> 48:58.440]  ну, например, связаны с синхронизацией потоков. То есть к этому все равно можно свести. То есть,
[48:58.440 --> 49:07.840]  так или иначе, это вопрос железа. И третий вопрос это то, как вы запрограммируете домодействие в данном
[49:07.840 --> 49:15.640]  случае процессов. Просто так с конточка, ну, получится плохо. Если подумать лучше, если еще подумать,
[49:15.640 --> 49:21.240]  ну, наверное, совсем замечательно будет. И опять же, это все влияет на ускорение в конечном итоге.
[49:21.240 --> 49:29.280]  То есть получается несколько таких слоев или там ступеней, где можно получить идеальность. И
[49:29.280 --> 49:35.840]  хотелось бы, конечно, чтобы все во всех слоях, во всех ступеньках, все это было, в общем-то,
[49:35.840 --> 49:45.560]  сделано как можно лучше. Здесь есть какие-то вопросы? Итак, теперь давайте пройдем дальше. У нас
[49:45.560 --> 49:55.080]  есть еще немного времени. Значит, это был геометрический параллелизм. Это один из методов
[49:55.080 --> 50:00.920]  параллельных вычислений, когда, как я уже сказал, вы какую-то область расчетную делите на части.
[50:00.920 --> 50:11.960]  Каждая часть вычисляется своим вычислителем. Какие еще есть методы параллельных вычислений? Ну,
[50:12.000 --> 50:18.280]  например, метод сдваивания. В принципе, он вам должен быть знаком, и он вполне очевиден.
[50:18.280 --> 50:26.840]  Ну, например, есть у нас массив из восьми элементов. Как получить сумму? Можно просто просуммировать.
[50:26.840 --> 50:39.920]  Вот. Как еще можно получить сумму? Вот представим себе, что мы используем четыре процесса. Если
[50:39.920 --> 50:45.320]  четыре процесса, то есть каждый из них может сложить два числа, да? Он получает ответ.
[50:50.600 --> 50:55.320]  Ну, чуть позже получит ответ, а сейчас мы просто видим время расчета. Ну, это примерно то же самое,
[50:55.320 --> 51:02.160]  что было раньше. Каждый из них получает ответ. Сумма первых двух чисел, третьего четвертого,
[51:02.160 --> 51:06.800]  пятого, шестого, седьмого, восьмого. Вот ответ у четырех процессов. Что дальше с этим делать?
[51:06.800 --> 51:13.360]  Нужно сложить вот эти попарно два ответа, например, да? Складываем попарно два ответа,
[51:13.360 --> 51:25.520]  получаем из четырех два. И нужно теперь еще разочек сложить. Получим сумму. Вот. Теперь, если мы,
[51:25.520 --> 51:37.680]  смотрите, если мы, например, будем использовать, то есть видно, что мы попарно складываем и потом…
[51:44.720 --> 51:50.120]  Да, тут нужно заметить, что вот здесь процессы они сложили и какому-то одному между собой они
[51:50.120 --> 51:56.960]  обмениваются данными только между собой. Здесь тоже только между собой. И потом они данные скидывают
[51:56.960 --> 52:02.280]  только какому-то одному из них. Неважно, первому или второму. Просто, например, можно было бы сделать
[52:02.280 --> 52:13.720]  следующее, что четыре процесса… П равно четырем, да? Они получили четыре числа. Ну вот. А, Б, С и Д.
[52:13.720 --> 52:22.080]  И теперь они могли бы что сделать? Просто взять и вот этот id равно нулю, например, или rank равен нулю,
[52:22.080 --> 52:30.760]  и все последовательно скинуть сюда. Да? Вот так. У нас получится в этом случае, они друг за дружком
[52:30.760 --> 52:40.080]  идут, но в данном случае получится 4 tau s. Вот так. 4 tau s. А здесь явно будет меньше. Правильно?
[52:40.080 --> 52:49.160]  По крайней мере, вот если мы отсюда идем, то будет 2 tau. То есть время на самом деле будет уменьшаться.
[52:49.160 --> 52:58.200]  Если у нас количество элементов это является степень двойки, то количество пересылок будет
[52:58.200 --> 53:07.120]  вот таким. То есть логарифм по основанию 2. Логарифм по основанию 2. Но это существенно меньше
[53:07.120 --> 53:12.840]  величина. Не так ли? Чем просто количество… Ну, в данном случае это будет 4 tau s. Это количество
[53:12.840 --> 53:18.000]  процессов изначальных, которые пересылок данных. Представляете, если будет тут, например, 100
[53:18.000 --> 53:27.120]  процессов. Ну, допустим, давайте не 100. Ну, 128. Хорошо. То они 128 tau s сделают. А в методе 2 они
[53:27.120 --> 53:47.560]  нисколько они сделают. Ну, например, 7. Замечательно. То есть 7 и 128 – это вообще большая разница. Больше
[53:47.560 --> 53:53.840]  чем 10 раз. Почти в 20. То есть это хороший. Но у него есть, конечно же, свои какие-то недостатки.
[53:53.840 --> 54:05.240]  Какого они рода? Итак, какого рода? Сейчас чуть-чуть позже. Значит, мы просто сейчас
[54:05.240 --> 54:12.480]  смотрим время распределения одного. Просто сейчас посмотрим на ускорение. Время одного. Время работы
[54:12.480 --> 54:19.640]  нашего алгоритма на одном процессе. И время, которое затрачивается на нескольких процессах,
[54:19.640 --> 54:31.040]  на п штуках. То есть если вернуться обратно, у нас будет tau c поделить на п. И еще алгоритм. И
[54:31.040 --> 54:44.480]  ускорение будет примерно вот таким. Значит, ускорение такое. А вот эффективность не очень
[54:44.480 --> 54:49.040]  хорошая, в общем-то говоря. Ускорение может быть довольно-таки неплохим. Мы с увеличением n,
[54:49.040 --> 54:54.600]  то есть с увеличением размера задачи, у нас числитель растет больше, быстрее, чем знаменатель.
[54:54.600 --> 55:03.760]  Таким образом мы достигаем неплохого ускорения. А вот что связано с эффективностью. Мы s поделили
[55:03.760 --> 55:11.360]  на p. А p у нас n пополам. Вот такое. И здесь вот это можно в принципе сократить. Примерно
[55:11.360 --> 55:19.560]  вы 2 алгоритм 2n. А здесь что получается? Что с ростом n у нас эффективность падает. Правильно? То
[55:19.560 --> 55:28.240]  есть у нас чем больше n, тем менее эффективно работают процессоры, то есть физические вычислители.
[55:28.240 --> 55:35.280]  Почему? Да потому что они в основном находятся в простое. Сначала все участвуют в расчете в самом
[55:35.280 --> 55:41.040]  начале. Потом только половина, потом половина, половина, половина, половина, и так далее. То есть
[55:41.040 --> 55:47.280]  каждую итерацию при пересылке из вычислений выкидывается половина работающих процессов. И в итоге
[55:47.280 --> 55:53.040]  большая часть процессов или большая часть процессоров как физических вычислителей, она
[55:53.040 --> 56:00.200]  простаивает. Чем больше n, тем больше таких простаивающих процессоров будет. Поэтому это
[56:00.200 --> 56:05.960]  конечно является неким недостатком, но зато показывает неплохое ускорение. И скорость передачи
[56:05.960 --> 56:11.120]  данных, время передачи данных, оно существенно меньше, чем если бы пользовались каким-либо
[56:11.120 --> 56:19.600]  другим способом. Здесь представлена сумма элементов массива, но ничего не мешает таким образом
[56:19.600 --> 56:27.140]  делать рассылку или сбор данных с разных процессов. То есть это в принципе то же самое. По этим
[56:27.140 --> 56:46.020]  стрелочкам просто идут сбор данных. Он здесь и представлен. Конечно можно что угодно делать,
[56:46.020 --> 56:49.940]  если это вам получится сделать и с хорошим ускорением. Почему нет?
[56:57.140 --> 57:06.600]  Почему? Не уверен в этом? У нас так или иначе, вот если мы говорим об этом методе, у нас все равно
[57:06.600 --> 57:15.160]  процессы будут не участвовать в итоге в присылках. Но если вернемся, вот здесь вот, смотрите,
[57:15.160 --> 57:21.520]  вот здесь было. Вот сколько стрелочек, если так. Здесь меньше, здесь два раза меньше. Каждый раз
[57:21.520 --> 57:31.620]  два раза меньше стрелочек. Так или иначе, но от этого никуда не деться. Так, значит,
[57:31.620 --> 57:39.580]  эффективность здесь не очень хорошая. Это, ну то есть, ну да, такой алгоритм. Все, он обладает не
[57:39.580 --> 57:46.060]  очень хорошей эффективностью, потому что идет простой. Вот, например, если у нас количество таких
[57:46.060 --> 57:51.880]  элементов в районе тысячи, то ускорение можно получить, ну, конечно, не супер прикольно, но,
[57:51.880 --> 57:59.240]  например, сотня, то эффективность просто будет совсем не очень хорошей, 20 процентов. Так, а теперь,
[57:59.240 --> 58:10.580]  как можно было бы улучшить? Вот у нас большой-большой-большой-большой массив, он состоящий из
[58:10.580 --> 58:16.220]  n элементов, да? Как бы нам все-таки сделать его поинтереснее, эффективность там не очень хорошая.
[58:16.220 --> 58:26.700]  Смотрите, здесь у нас n большое, p, ну, поменьше, чем n. Значит, если мы поделим наш вот этот массив
[58:26.700 --> 58:33.700]  на части, и каждая часть будет обсчитана одним процессом, то получится что? Что на каждый из
[58:33.700 --> 58:41.080]  процессов пойдет n поделить на p элементов. Последовательный алгоритм будет считать n на
[58:41.080 --> 58:48.700]  tau c примерно, но единичку мы не рассматриваем, большой массив. Тогда что дальше будет получаться? Время
[58:48.700 --> 58:59.500]  расчета. Время вот этой части будет tau c n на p, они работают одновременно, значит, оно остается,
[58:59.500 --> 59:08.220]  и плюс, то есть только здесь на самом деле tau, а нет, сложение, да, хорошо, tau c плюс алгоритм,
[59:08.220 --> 59:21.940]  почему от p? Потому что p процессов, и они потом, алгоритм, помните, тот алгоритм от количества
[59:21.940 --> 59:34.060]  частей. Вот, а потому что, вопрос, потому что они будут вот столько раз еще и складывать их,
[59:34.060 --> 59:40.020]  но на самом деле здесь, по идее, должно, конечно, добавиться где-то, но мы пока без участия в сети
[59:40.020 --> 59:45.860]  смотрим, потому что они будут потом столько раз еще складывать свои результаты предыдущие. Итак,
[59:45.860 --> 59:51.100]  если посмотреть ускорение, то чему оно равно? Вот это делим на вот это, получаем следующее выражение,
[59:51.500 --> 01:00:00.260]  немножко сокращаем здесь, и вот у нас вот такая вот величина. Ускорение, значит, там у нас оно было,
[01:00:00.260 --> 01:00:10.380]  давайте вернемся, n поделить на алгоритм 2n, у нас не зависело от количества процессов, но у нас
[01:00:10.380 --> 01:00:16.980]  здесь, правда, p, число процессов было половинка от общего размера, здесь у нас уже не так,
[01:00:16.980 --> 01:00:24.700]  здесь у нас уже поинтересней, здесь у нас опять портящая часть, небольшая слагаемая есть,
[01:00:24.700 --> 01:00:38.260]  вот, и чем больше p, ну, тем вообще она немножко хуже становится, вот, потому что, вот смотрите,
[01:00:38.260 --> 01:00:52.220]  если у нас есть, вот, например, давайте еще раз два, еще раз два, восемь чего-то, то есть восемь
[01:00:52.220 --> 01:01:06.100]  процессов, они что делают, они потом начинают складывать, вот так сложили, нет, чтобы сложить
[01:01:06.100 --> 01:01:11.820]  им сначала нужно переслать, сейчас они получили свои суммы и теперь пересылают, да, наверное,
[01:01:11.820 --> 01:01:20.700]  отменить. Вот восемь процессов у нас получили свои результаты, теперь они должны слить воедино,
[01:01:20.700 --> 01:01:33.140]  начало вот так кладут кого-то переслали данные вот так вот так и переслали данные получилось
[01:01:33.140 --> 01:01:42.260]  из двух из попарно произошло сложение было 8 чисел 4 пары 4 ответа получились потом еще
[01:01:42.260 --> 01:01:52.180]  кладут здесь уже участвовали четыре процесса дальше будут участвовать два процесса и в итоге
[01:01:52.180 --> 01:02:08.860]  ответ будет у одного сколько вот таких видов 1 2 3 не а вот n это что здесь данном случае это
[01:02:08.860 --> 01:02:14.420]  количество процессов потому что они получили смотрите они вот здесь получили свой результат
[01:02:14.420 --> 01:02:21.020]  вот здесь они посчитали вот он за такое время они получили результат данном случае этот результат
[01:02:21.020 --> 01:02:29.860]  я здесь изобразил виде кружка и таких результатов 5 штук 8 и вот видно что вот эта тройка 3 это
[01:02:29.860 --> 01:02:39.140]  логарифм 8 здесь их вот 8 штук 8 по основанию 2
[01:02:39.140 --> 01:02:59.060]  правильно теперь мы взяли п процессов так каждому вот смотрите 1 2 3 4 5 6 7 данном случае 7 штук
[01:02:59.060 --> 01:03:05.780]  ну так получилось к сожалению не совсем правильно то есть у нас не степень двойки ну да ладно вот
[01:03:05.780 --> 01:03:12.660]  это граница условно между процессами вот каждый из процессов взял n поделить на п элементов сложил
[01:03:12.660 --> 01:03:22.540]  их вот за такое время получил ответ получилось сколько ответов от суммирования внутри каждого
[01:03:22.540 --> 01:03:31.940]  а п штук верно а теперь эти п штук начинают по методу сдваивания вкладывать свои ответы вот
[01:03:31.940 --> 01:03:40.420]  по такой схеме их же п представьте что сейчас не 7 8 вот вот такая схема будет будет раз сложение
[01:03:40.420 --> 01:03:53.420]  вот то есть плюсик 2 3 3 плюсика это 3 tau c а 3 это логарифм 8 то есть п по основанию 2
[01:04:10.420 --> 01:04:25.420]  да верно а будет 8 tau c отсылок можно сделать но будет дольше они может каждый какому-то
[01:04:25.420 --> 01:04:31.340]  одному но они будут последователем дружок за дружку отправляет данные будет 8 tau c правильно или
[01:04:31.340 --> 01:04:44.100]  п tau c п а в нашем случае будет логарифм 2 логарифм п по основанию 2 tau c то есть несколько раз меньше
[01:04:44.100 --> 01:04:53.300]  пересылаем но пока нет давайте пока без этого здесь пока не участвуют пересылки хотя бы
[01:04:53.300 --> 01:05:07.260]  с этим разобраться как то как они считают теперь прояснилась ситуация да так ну вот и что мы
[01:05:07.260 --> 01:05:13.980]  теперь видим мы теперь видим вот в таком случае что у нас эффективность до этого она у нас не
[01:05:13.980 --> 01:05:22.140]  зависела вернее зависела но очень нехорошо от n от п совсем никак не зависела правильно если вернуться
[01:05:22.300 --> 01:05:29.640]  здесь от п никак не зависела а от n она уменьшалась нехорошо зависимость была здесь тоже есть зависимость
[01:05:29.640 --> 01:05:37.100]  от n но мы уже можем в некотором смысле невелировать увеличением количества вычислителей п увеличивая
[01:05:37.100 --> 01:05:45.320]  п а мы тоже плохо делаем до ну опять увеличивая n мы на самом деле здесь уже там мы уменьшали
[01:05:45.320 --> 01:05:51.500]  а здесь вот а здесь мы ее делаем немножко лучше потому что у нас другая зависимость немножко ну
[01:05:51.500 --> 01:06:08.500]  В итоге получается то, что когда мы используем большое количество элементов и некоторое количество процессов, мы можем получить более лучшие показатели как ускорения, так и эффективности.
[01:06:21.500 --> 01:06:46.500]  Вот здесь вот они все делают сумму еще раз. Вот здесь он свою сумму делает. Вот это сумма. Это сумма из N на P элементов. Да? И здесь сумма.
[01:06:46.500 --> 01:06:56.500]  Куда?
[01:06:56.500 --> 01:07:10.500]  Ну давайте все-таки о процессах пока говорить. Ну вот он сделал. Процесс сделал сумму. Вот он сделал. Потом они начинают два попарно складывать вот эти свои результаты. Они складывают это независимо друг от друга. Одновременно.
[01:07:10.500 --> 01:07:29.500]  Получили четыре. Вот эти две суммы сложили.
[01:07:29.500 --> 01:07:49.500]  Нет. Что значит мы и где мы проходим? Еще раз. Смотрите. Ну я просто не буду рисовать, но зачем? Еще раз. Вот N элементов. Можно ведь первому процессу, допустим, одной пересылкой, давайте даже так, одной пересылкой.
[01:07:49.500 --> 01:08:06.500]  Каждый из процессов знает сколько их штук. Он знает количество. Он знает свой номер. Нулевой берет первый N на P элементов. Может он взять? Может. Первый берет, то есть следующий, второй с номером один, следующий N на P элементов, потом следующий N на P и так далее.
[01:08:06.500 --> 01:08:25.500]  Каждый из них знает где он в этой, где его часть в большом массиве находится. Может такое? Может. Да мы даже можем не распределять. Одновременно все это. За О от единицы делается.
[01:08:25.500 --> 01:08:49.500]  Да. То есть N на P умножить на его ранг и будет место начала. Смотрите. Ну вы когда запускаете программу, он будет запускаться. Мы не сделаем. Они одновременно сразу работают.
[01:08:49.500 --> 01:09:00.500]  Когда семинар будет, понимаете, как всегда, лучше 100 раз услышать, чем 100 раз, ой, один раз увидеть, чем 100 раз услышать. Вот один раз нужно это руками проделать и все станет на свои места.
[01:09:00.500 --> 01:09:25.500]  Вот здесь вот просто смотрите. Адрес начала будет N на P умножить на ранг. Вот это вот A. Вот отсюда он начнет стартовать. А закончит он на A N на P ранг. Плюс один, там минус один. Вот так. Все. Вот его начало и конец.
[01:09:30.500 --> 01:09:55.500]  Да, да.
[01:09:55.500 --> 01:10:09.500]  Под компотом я не являюсь разработчиком. Я вам не могу сказать, честно говоря. Но что я могу сказать следующее? Что когда вы запускаете процесс, у вас там есть функция.
[01:10:09.500 --> 01:10:24.500]  МПИ, ком, ранг. Все процессы там. Тут коммуникатор, имя коммуникатора. И вот сюда вы записываете ранг. Сюда будет записано ранг процесса. Вот сюда.
[01:10:24.500 --> 01:10:35.500]  После этого все. У вас еще в самом начале, еще когда у вас там Int Main, вот здесь вот Main, у вас уже начинают работать все процессы. Вот их там несколько штук.
[01:10:35.500 --> 01:10:45.500]  Они потом, каждый выполняет эту функцию, сюда записано их ранг. Все. И только после этого вы начинаете уже реализовать свой алгоритм.
[01:10:45.500 --> 01:10:55.500]  У каждого процесса уже есть свой номер. Они знают, сколько их есть. Если еще каждый знает размер этого массива, все.
[01:10:55.500 --> 01:11:05.500]  Они каждый вот такую штукку проделают, и он знает начало и конец своего участка. Сразу все одновременно.
[01:11:05.500 --> 01:11:13.500]  Они знают. И если это участки, они будут примерно одинаковы, N на P. Ну там нужно с остатком еще поиграться. Ну это бок с ним.
[01:11:20.500 --> 01:11:27.500]  Это уже второй вопрос. Это можно делать дополнительные пересылки. Может этот нулевой им раскидать части.
[01:11:27.500 --> 01:11:35.500]  Это лучше всего. Ну то есть у каждого будет своя маленькая часть. Не нужно большой хранить и там память сильно много занимать.
[01:11:35.500 --> 01:11:43.500]  Это уже вторая производная. Это уже не имеет о сильном отношении. То есть это подгоревительное действие до самого алгоритма.
[01:11:43.500 --> 01:11:49.500]  Мы сейчас рассматриваем саму структуру алгоритма. Как там на самом деле будет делать? Это уже вторая на самом деле производная.
[01:11:49.500 --> 01:11:57.500]  Даже пусть у них вон там. Сразу у всех хранится допустим полностью. Может он там генерируется одинаковый.
[01:11:57.500 --> 01:12:03.500]  От 0 до N минус 1. Просто элемент равен индексу. Может такое? Ну хотим.
[01:12:03.500 --> 01:12:09.500]  Например, у каждого хранится одинаковый массив, но он знает свою часть. И знает все одновременно.
[01:12:09.500 --> 01:12:13.500]  И получит суммы свои тоже одновременно. И таких сумм будет P штук.
[01:12:13.500 --> 01:12:21.500]  И тогда потом, когда они методом сдваивания дойдут до конечного ответа, то таких шашков будет логарифм P по основанию 2.
[01:12:21.500 --> 01:12:33.500]  Правильно? Все. Вот. Итак, здесь прояснилась ситуация? Или нет?
[01:12:33.500 --> 01:12:43.500]  Так, хорошо. Да, и теперь смотрите. Если у нас N достаточно большое, то получится...
[01:12:43.500 --> 01:12:53.500]  Ну это вот, вернее, можно рассмотреть следующее. Что, значит... То есть, если P достаточно маленькая, а N большая,
[01:12:53.500 --> 01:12:59.500]  то у нас эффективность будет к чему стремиться? Все больше и больше к единичке приближаться, да?
[01:12:59.500 --> 01:13:07.500]  А S все больше и больше к P. То есть, перед этим, когда у нас количество процессов равнялось половине длины массива,
[01:13:07.500 --> 01:13:13.500]  то у нас эффективность сюда падала. А здесь она, наоборот, может расти.
[01:13:13.500 --> 01:13:21.500]  А теперь представим, что P равно N поделить вот над логарифм N по основанию 2.
[01:13:21.500 --> 01:13:29.500]  N по основанию 2. Что... Это будет интересно. Что в этом случае будет?
[01:13:29.500 --> 01:13:38.500]  Что S... Ну, как я уже сказал, если N больше P, то этот будет к единичке стремиться, а этот будет к P.
[01:13:38.500 --> 01:13:45.500]  А в этом случае S будет стремиться к P пополам, и E, то есть эффективность, будет стремиться к 1 и 2, без P.
[01:13:45.500 --> 01:13:53.500]  50%. Вот если P вот сюда поставим... Давайте проделаем, что это будет.
[01:13:53.500 --> 01:13:57.500]  Равно у нас уже конец. И новое мы ничего не будем рассматривать больше,
[01:13:57.500 --> 01:14:05.500]  поэтому давайте старое до конца рассмотрим. Вот 1 плюс P N логарифм P 2.
[01:14:06.500 --> 01:14:16.500]  А теперь у нас P равно N логарифм 2 N. Вот так, да? Вот.
[01:14:16.500 --> 01:14:31.500]  Подставляем сюда. Что это получается? Значит, это будет P 1 1 плюс 1 поделить на логарифм 2 N.
[01:14:31.500 --> 01:14:42.500]  А здесь будет что? Логарифм по основанию 2 N. Логарифм 2 N.
[01:14:42.500 --> 01:14:47.500]  Равно. Так.
[01:14:55.500 --> 01:15:00.500]  То есть я к тому, что если у нас P не очень большое и зависит от N так, то мы можем
[01:15:00.500 --> 01:15:05.500]  получить хорошее ускорение. Ускорение P пополам это очень неплохое,
[01:15:05.500 --> 01:15:10.500]  а эффективность 50% тоже в принципе... Ну, это не то, к чему стоит стремиться,
[01:15:10.500 --> 01:15:20.500]  но вполне себе нормальное. Так, и сюда переходим. К чему это равно? P 1.
[01:15:20.500 --> 01:15:39.500]  Дальше. 1 плюс 1 логарифм N. На что множить? На логарифм N минус логарифм логарифм N.
[01:15:39.500 --> 01:15:44.500]  Вот так. Так получается.
[01:15:48.500 --> 01:15:57.500]  Получается P 1 1 плюс. Вот это чему будет равно?
[01:15:58.500 --> 01:16:15.500]  1 минус логарифм логарифм N поделить на логарифм N. Вот это чему будет равно?
[01:16:15.500 --> 01:16:21.500]  Ну, очень маленькое. То есть это да. Ну, такое. Давайте стремиться к нулю. Хотя маленькое.
[01:16:21.500 --> 01:16:27.500]  Это маленькая штучка. Ну, вот и получается. P поделить на 2 примерно.
[01:16:27.500 --> 01:16:34.500]  Ну, чуть-чуть меньше, чем P на 2. Вот так давайте напишем P на 2.
[01:16:38.500 --> 01:16:44.500]  То есть теперь, если мы вот таким образом совместим как бы геометрический параллелизм,
[01:16:44.500 --> 01:16:50.500]  когда мы разделили наш массив на части и метод сдваивания, то это сильно
[01:16:50.500 --> 01:16:54.500]  улучшает на самом деле метод сдваивания и может привести вот к хорошим
[01:16:54.500 --> 01:16:58.500]  показателям как ускорение, так и эффективности.
[01:16:59.500 --> 01:17:07.500]  Ну, все. Теперь чего? Больше, к сожалению, мы не увидимся вот так, в таком виде.
[01:17:07.500 --> 01:17:15.500]  Поэтому дальше у вас будут другие лекторы. Ну, я надеюсь, что то, что мы
[01:17:15.500 --> 01:17:20.500]  рассмотрели, а мы рассмотрели коротко, конечно, бегло-пробежались по устройству,
[01:17:20.500 --> 01:17:27.500]  по основным характеристикам, про то, как можно анализировать теоретический алгоритм.
[01:17:27.500 --> 01:17:34.500]  То, что влияет, какие причины приводят к тому, что наши программы могут работать
[01:17:34.500 --> 01:17:39.500]  не так, как бы хотелось. И на это есть множество причин. И рассмотрели, ну, успели
[01:17:39.500 --> 01:17:44.500]  два метода параллельных вычислений. Геометрический более-менее такой ясный,
[01:17:44.500 --> 01:17:48.500]  непонятный, то есть мы делим на части. И вот метод сдваивания, которые могут быть
[01:17:48.500 --> 01:17:53.500]  применены при разработке, при написании параллельных программ.
[01:17:55.500 --> 01:18:01.500]  Давайте, какие есть вопросы у вас. Если есть, то задавайте. На этом можно закончить.
[01:18:01.500 --> 01:18:15.500]  У меня было в ноутбуке, но он завершил свое существование на сегодня. Поэтому у меня вот здесь,
[01:18:15.500 --> 01:18:21.500]  вот здесь еще есть метод один. Но я не успел на доске рассмотреть, то есть я его написал так,
[01:18:21.500 --> 01:18:29.500]  то, что было в ноутбуке. И было еще вопрос, есть еще такая тема, но это больше такое на интерес,
[01:18:29.500 --> 01:18:36.500]  в принципе, она для вас не особо будет там где-то использована. Топология соединения, вот есть
[01:18:36.500 --> 01:18:45.500]  процессоры и узлы, кластеры. Вот как их теперь соединить? Как соединить так, чтобы было хорошо,
[01:18:45.500 --> 01:18:52.500]  чтобы было надежно, чтобы была высокая скорость передачи данных, чтобы легче можно было добавлять
[01:18:52.500 --> 01:19:00.500]  телину узлы. Это называется топологией соединения узлов в машине. Вот про это можно было бы еще
[01:19:00.500 --> 01:19:08.500]  поговорить, но ни то, ни другое вот я не успел. Ну, это, по-моему, в прошлые годы я где-то об этом
[01:19:08.500 --> 01:19:15.500]  рассказывал, поэтому можете за прошлые годы где-то записи найти. Это топология соединения узлов
[01:19:15.500 --> 01:19:25.500]  в машине. Там, в принципе, более-менее там все логично, понятно, но в том смысле, что все, что не делается,
[01:19:25.500 --> 01:19:30.500]  все основано на каких-то логических соображениях, что хорошо или что плохо. Но там есть интересная
[01:19:30.500 --> 01:19:37.500]  топология, можете потом там почитать, это гиперкуб. У него интересные свойства есть и вообще забавный
[01:19:37.500 --> 01:19:42.500]  способ построения этого гиперкуба. То есть, когда вы можете прочитать, там, аинмерный гиперкуб,
[01:19:42.500 --> 01:19:47.500]  что это такое, и непонятно может быть. Но, на самом деле, если почитать про его свойства, как он
[01:19:47.500 --> 01:19:52.500]  создается, как он конструируется, то, в принципе, становится довольно-таки понятно и становится
[01:19:52.500 --> 01:19:59.500]  понятно хорошего свойства. Почему? Используют для некоторых задач, именно специально делают машину,
[01:19:59.500 --> 01:20:05.500]  с определенной топологией расположения узлов. Потому что она хорошо, алгоритм на нее хорошо
[01:20:05.500 --> 01:20:12.500]  накладывается. И у нее есть там интересные свойства. Вот можете там запомнить, почитать или
[01:20:12.500 --> 01:20:19.500]  посмотреть в прошлые года. Еще какие вопросы?
[01:20:28.500 --> 01:20:32.500]  Хорошо тогда, удачи вам в вашем изучении.

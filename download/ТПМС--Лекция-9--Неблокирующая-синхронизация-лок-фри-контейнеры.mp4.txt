[00:00.000 --> 00:11.240]  Ну что, давайте начнем. Сначала восстановим несправедливость, потому что оказывается,
[00:11.240 --> 00:14.560]  что в конце прошлой лекции я так увлекся вот все этими монадами и функциональным
[00:14.560 --> 00:18.320]  программированием и забыл рассказать про важную вещь, прям вот важную очень для нас,
[00:18.320 --> 00:23.680]  для нашего курса, про еще один экзекютер. Но давайте восстановим немного контекста. В прошлый раз
[00:23.680 --> 00:31.840]  мы с вами говорили про различные... В прошлый раз мы говорили про фьючи. Вот они перед вами,
[00:31.840 --> 00:38.600]  фьюча. И это для нас было еще одно средство выразительности. То есть с помощью фьюч можно
[00:38.600 --> 00:44.720]  было бы описывать какие-то асинхронные цепочки задач, потом их исполнять в пуле поток. Для этого
[00:44.720 --> 00:50.720]  у нас был метод subscribe асинхронный, и у нас была via, которая позволяла указать, где же именно
[00:50.720 --> 00:56.400]  кулбек будет вызван. Ну а дальше мы могли с помощью этих subscribe и via писать комбинаторы,
[00:56.400 --> 01:01.280]  которые реализуют синхронные и асинхронные продолжения. Мы это называли зеном, но вот знающие
[01:01.280 --> 01:06.720]  люди говорят, что нужно назвать их совсем по-другому, мапы flatmap, потому что функторы монады. Ну в общем,
[01:06.720 --> 01:14.120]  не так это важно сейчас. Важно, что изучая фьючи, мы столкнулись, обнаружили вернее, что тредпул,
[01:14.120 --> 01:21.720]  что нам по-прежнему нужен тредпул. Что и фьюча, и файберы, которые у нас были. Давайте я открою
[01:21.720 --> 01:26.200]  условия новой задачи, которые у нас уже на самом деле есть, а вы про это, может быть, еще не знаете.
[01:26.200 --> 01:32.520]  И файберы, и фьюча, и стеклоскрути, на которых у нас еще не было, на которые будут, которые есть,
[01:32.520 --> 01:37.280]  и плюс-плюс, вы их видели, наверное, коэвейт, вот все это. Это разные выразительные средства,
[01:37.280 --> 01:41.240]  с помощью которых вы, как пользователь и разработчик, описываете свои конкурентные
[01:41.240 --> 01:47.080]  активности, вы работчики запросов. Вот, и для вас, как для пользователей, все эти средства очень
[01:47.080 --> 01:50.680]  сильно отличаются, но вот выглядят они совершенно по-разному, вряд ли вы файбер с фьючами спутаете.
[01:50.680 --> 01:58.960]  Но если мы смотрим на эти инструменты не с вашей позиции, а с позиции среды исполнения, с позиции
[01:58.960 --> 02:03.280]  тредпула, то в принципе никакой разницы между всеми этими механизмами нет, потому что для
[02:03.280 --> 02:09.360]  тредпула все это цепочки задач. Исполняется одна задача, потом она завершается, и в конце, видимо,
[02:09.360 --> 02:15.120]  как-то планирует на будущее запуск следующий. Ну, например, файбер заводит таймер и планирует
[02:15.120 --> 02:22.560]  задачу, которая сделает резюм его корутина. Или мы подвешиваем на фьючо колбэк, который будет,
[02:22.560 --> 02:27.440]  который запустится тогда, когда вот какая-то асинхронная задача выполнится. Ну, то есть,
[02:27.440 --> 02:31.440]  вот такие цепочки задач, мы их планируем там динамически, статически, и в общем дальше они
[02:31.440 --> 02:37.160]  бегут в этом тредпуле. А дальше мы говорим, но вот есть разные средства, и они используют один
[02:37.160 --> 02:42.720]  и тот же механизм исполнения задач, тредпул. И как они его используют? Они просто вызывают
[02:42.720 --> 02:49.120]  Submit, запланировать задачу на исполнение. Так вот, если задуматься, то, ну, во-первых,
[02:49.120 --> 02:54.520]  это хорошо, это такой, у нас получается модульность, у нас есть тредпул, у нас есть фьючо и файберы,
[02:54.520 --> 02:59.080]  они друг от друга ничего не знают. И поэтому тредпул подходит для разных инструментов,
[02:59.080 --> 03:04.160]  он универсален. Но можно пойти еще дальше и заметить, что мы в прошлый раз и сделали,
[03:04.160 --> 03:09.240]  что вот все эти средства, и файберы, и фьючи, и стеклоскорутина, которых у нас пока не было,
[03:09.240 --> 03:17.080]  они всем, ну, в общем-то не полагаются на то, как именно задачи будут исполняться. Вот тредпул
[03:17.080 --> 03:21.360]  то обещает этого достаточно, то есть, просто достаточно обещания исполнить задачу, где,
[03:21.360 --> 03:29.200]  когда именно эта реализацию файберов вообще-то не беспокоит. Поэтому сам тредпул вот во всей этой
[03:29.200 --> 03:34.120]  конструкции, во всем этом дизайне можно обобщить до понятия экзекьютора, сервиса, который исполняет
[03:34.120 --> 03:39.760]  задачи. Экзекьютор, но вот он, у него единственный метод, экзекьют, запланировать задачу на исполнение,
[03:39.760 --> 03:46.840]  где-то, когда-то она будет выполнена. Ни файберы, ни фьючи сами не понимают, когда именно. Вот, а вы,
[03:46.840 --> 03:51.640]  как пользователи, сами настраиваете уже конкретный рантайм, то есть, конкретный экзекьютор, который
[03:51.640 --> 03:59.000]  будет задачей исполнять. И, может быть, вам, может быть, вы берете в качестве экзекьютора тредпул и
[03:59.000 --> 04:04.880]  запускаете в нем файберы. Это вы уже делать умеете. А, может быть, вы берете, как мы показывали,
[04:04.880 --> 04:13.920]  как мы смотрели в прошлый раз, в качестве экзекьютора, manual executor, который запускает задачу в
[04:13.920 --> 04:19.840]  ручную. Вот мы говорим execute и кладем задачу в очередь этого экзекьютора, просто задача лежит в
[04:19.840 --> 04:25.840]  очереди, не исполняется. Кладем еще одну, а потом вручную запускаем одну задачу, запускаем еще три
[04:25.840 --> 04:33.760]  задачи. Если вы подставите под файберы такой экзекьютор, то вы получите детерминированное
[04:33.760 --> 04:38.800]  однопоточное исполнение файберов. Сами файберы при этом не поменяются, они просто зависят от
[04:38.800 --> 04:45.600]  экзекьютора, а не от конкретного тредпула для планирования. А за счет того, что механика
[04:45.600 --> 04:51.560]  исполнения поменялась, за счет того, что теперь это такая ручная очередь, то исполнение получилось
[04:51.560 --> 04:58.480]  детерминированное, можно там uni-test писать, не меняя реализацию файберов. Вот, это были два
[04:58.480 --> 05:03.080]  экзекьютора, про которые мы в прошлый раз говорили, тредпул и manual, но я не поговорил про еще один
[05:03.080 --> 05:10.240]  очень важный, очень любопытный для нас в курсе, это executor-strand. Помните, когда-то давно я говорил
[05:10.240 --> 05:14.960]  вам на лекции про кэши, что вот есть mutex и он не слишком хорош в точке зрения протоколок
[05:14.960 --> 05:20.400]  герентности. Ну, потому что нужно постоянно двигать данные между кэшами для разных
[05:20.400 --> 05:25.960]  критических секций. То есть вот мы знаем, что цена синхронизации в процессоре – это коммуникация по
[05:25.960 --> 05:31.280]  шине памяти между кэшами, между ядрами, которые синхронизируют свои кэши. Чем меньше синхронизации,
[05:31.280 --> 05:38.480]  тем быстрее все работает. Вот, а теперь представим, что у нас ядер много, там, не знаю, десятки, 64 ядра,
[05:38.480 --> 05:43.520]  и мы запускаем на них критические секции, которые обращаются к одним и тем же данным. И вот в итоге
[05:43.520 --> 05:48.440]  у нас на синхронизации получается очень много коммуникаций, просто чтобы критически, чтобы потоки
[05:48.440 --> 05:54.040]  договорились, кто из них в каком порядке исполняется. А кроме того, сами критические секции исполняются
[05:54.040 --> 05:57.600]  неэффективно, потому что вы поработали с данными на одном кэше, теперь они лежат в вашем кэше,
[05:57.600 --> 06:02.800]  видимо, в состоянии modified, вы их поменяли. А потом критическая секция запустилась на другом ядре,
[06:02.800 --> 06:07.520]  и там уже данных нет, потому что modified – исключительное состояние, оно исключает копии в других кэшах.
[06:07.520 --> 06:12.520]  И вот данные едут с одного кэша на другой. Когда протоколок герентности нагружается, процессоры
[06:12.520 --> 06:19.000]  занимаются обслуживанием всех этих сообщений ядра, какая-то не очень полезная работа. Так вот,
[06:19.000 --> 06:27.080]  это с одной стороны, а с другой стороны, mutex в нашем случае плох еще и потому, что он плохо
[06:27.080 --> 06:34.040]  вписывается в нашу модель, где число потоков ограниченное. Вот мы все задачи наши запускаем в
[06:34.040 --> 06:39.840]  одном пуле из там фиксированного числа потоков. И если какая-то задача в этом пуле берет mutex
[06:39.840 --> 06:46.160]  на продолжительное время, то пока она его держит, другие задачи, соответственно, будут ее ждать,
[06:46.160 --> 06:52.800]  блокировать свои потоки, воркеры в пуле. И таким образом, другие задачи, ну, какие-то третьи
[06:52.800 --> 06:57.640]  задачи, которыми mutex вообще не нужен, не смогут исполняться, потому что на одном потоке mutex держит
[06:57.640 --> 07:02.720]  и работает под ним, а на другом потоке, на других потоках mutex аждут. И вот у нас вся система тормозится.
[07:02.720 --> 07:10.560]  Вот. Предлагается обе эти проблемы решить с помощью нового инструмента, нового экзекьютора,
[07:10.560 --> 07:16.160]  то есть нового механизма запуска задач, который называется Strand или, как это ни странно,
[07:16.160 --> 07:26.160]  асинхронный mutex. Где же он у меня был? Вот что мы упустили в прошлый раз. Strand – это
[07:26.160 --> 07:31.360]  не самостоятельный экзекьютор, это декоратор. Он оборачивает пул потоков. Собственных потоков
[07:31.360 --> 07:37.880]  у него нет. То есть все, что он запускает, он запускает через тот пул, который он оборачивает.
[07:37.880 --> 07:42.480]  Вернее, даже Strand не знает, что он оборачивает. Какой-то экзекьютор, в данном случае, это пул поток.
[07:42.480 --> 07:50.960]  И какую же нам гарантию дает Strand? Все, что мы в него положим, все задачи, будут исполняться строго
[07:50.960 --> 07:58.880]  последовательно, друг за другом. Причем порядок исполнения будет уважать порядок отправки задач
[07:58.880 --> 08:06.880]  в Strand. То есть мы в него бросаем критические секции, фактически, а он их асинхронно последовательно
[08:06.880 --> 08:17.020]  исполняет. В чем здесь профит? В том, что мы теперь можем избежать дополнительной нагрузки на протокол
[08:17.020 --> 08:23.680]  к герентности. Вот обычному mutex, чем больше потоков с ним работают, тем хуже. А Strand – это такой
[08:23.680 --> 08:29.240]  очень странный примитив, потому что чем больше на него нагрузки, тем под капотом будет меньше
[08:29.240 --> 08:34.800]  синхронизации и тем эффективнее все будет работать. То есть чем больше нагрузки, тем меньше лишней
[08:34.800 --> 08:41.120]  работы он делает. Это довольно интересный эффект. Где он может выигрывать? За счет чего он может
[08:41.120 --> 08:46.040]  выигрывать? За счет того, что он не пытается данные возить между разными потоками. Он наоборот говорит,
[08:46.040 --> 08:50.680]  что вот если у вас есть критические секции, то им и так исполняться последовательно по своей
[08:50.680 --> 08:56.580]  задумке. Так вот, нужно их просто свести в один поток и там поочередно исполнить. Просто друг за
[08:56.580 --> 09:02.520]  другом. Тогда между ними не нужна будет синхронизация, мы просто запускаем там три критические секции
[09:02.520 --> 09:08.320]  подряд. Между ними синхронизация больше не нужна. Кроме того, все эти секции теперь в пределах вот
[09:08.320 --> 09:14.920]  такой вот серии запуска будут работать над одним и тем же кышом. А значит, не потребуется инвалидация,
[09:14.920 --> 09:25.720]  все данные будут под ногами. Вот на данном ядре. Понятна идея? Ну, пока минус в том, что он не то,
[09:25.720 --> 09:32.720]  чтобы минус, но это все-таки Executor, то есть он вписан в наш framework и нам требуется framework,
[09:32.720 --> 09:38.680]  чтобы с ним работать. Кроме того, но это вообще интересная история. Смотрите, он это асинхронный
[09:38.680 --> 09:43.800]  мьютакс, говорю я о нем так. То есть, мы говорим Execute и вот задача как-то улетает куда-то в какую-то
[09:43.800 --> 09:50.960]  очередь, там ждет своего часа. А если вы решаете задачи про файберы, давайте откроем задачи про
[09:50.960 --> 10:05.080]  файберы, которые вы прямо сейчас должны решать. Это задача мьютакс. Вот, то там вас просят сделать
[10:05.080 --> 10:10.280]  синхронный мьютакс для файберов. Совершенно обычный, да? Вы берете его, получаете и оказываетесь
[10:10.280 --> 10:18.920]  в критической секции. Так вот, вы же знаете по другой задаче, по задаче slip4, внимание,
[10:18.920 --> 10:25.680]  все связано, что файберы с помощью переключения контекста, ну, коррутины, могут асинхронные вызовы
[10:25.680 --> 10:34.520]  превращать в синхронные. Понимаете это, да? Вот, ну, в этом, в этом задаче, в задаче slip4 занимались,
[10:34.520 --> 10:39.600]  вы превращали асинхронный таймер, асинхронная операция async weight, в синхронную операцию slip4,
[10:39.600 --> 10:44.800]  которой мы заходим и выходим, когда файбер уже проснулся. Так вот, если вы умеете делать
[10:44.800 --> 10:51.400]  асинхронный мьютакс, хороший, если вы умеете делать из асинхронных интерфейсов синхронные,
[10:51.400 --> 10:58.640]  то вы, объединив свои познания, которые вы получили из задачи slip4 и из задачи вот этой
[10:58.640 --> 11:09.120]  новой прегзекьюторы, можете потом вернуться в задачу мьютакс и сделать там вот вообще
[11:09.120 --> 11:15.280]  клевую хорошую реализацию синхронного мьютакса еще из гарантии лукфрии, который будет сериями
[11:15.280 --> 11:19.560]  критической секции исполнять. То есть, он будет гораздо эффективнее, чем обычный мьютакс, потому
[11:19.560 --> 11:24.240]  что, ну, а иначе зачем мы все это своими руками пишем, чтобы оно было эффективнее, чем стандартный
[11:24.240 --> 11:31.840]  инструмент. Ну, в общем, время придет, и вы все это осмыслите, я надеюсь. Ну, все это вместе должно
[11:31.840 --> 11:36.400]  сложиться в какой-то момент. Может быть, сейчас нет. Ну, в общем, имейте в виду, что все связано,
[11:36.400 --> 11:40.520]  все задачи связаны, они вот так накапливаются, и вот этот пазл наш он увеличивается, становится
[11:40.520 --> 11:52.480]  сложнее. Пожалуйста, старайтесь держаться в теме, не выпадать из нее. Что именно? Я пока и не
[11:52.480 --> 12:00.240]  объяснил, как он работает, это очередная домашка его придумать. Он исполняет задачи последовательно,
[12:00.240 --> 12:07.960]  которые у него бросают. Ну, для критических секций параллельности и так не было с самого начала.
[12:07.960 --> 12:16.000]  Ну, значит, задачи, которые исполняют критические, ну, то есть, критические секции планируются
[12:16.000 --> 12:25.440]  через стренд. Не критические секции планируются через пул. Вот, у тебя есть пул потоков,
[12:25.440 --> 12:34.200]  над ним стренд. Может быть, даже много стрендов, может быть, тысячу стрендов. Вот, это, ну, такое
[12:34.200 --> 12:39.120]  очень, мне кажется, изящное применение, я смогу вам показать осенью в курсе распределенных систем.
[12:39.120 --> 12:43.640]  Вот, в RPC Framework есть такая сущность, канал, вот, в него запросы падают от пользователя,
[12:43.640 --> 12:48.240]  и в него прилетают события из сети, что сообщения снаружи приходят. Их нужно упорядочить и
[12:48.240 --> 12:53.400]  последовательно обрабатывать. Вот, каждый такой канал логический в RPC Framework — это отдельный
[12:53.400 --> 12:58.840]  стренд. Он для исполнения использует отдельный стренд. А под ним — один общий пул потоков. И вот,
[12:58.840 --> 13:02.680]  задачи, которые хотят исполняться параллельно, они исполняются, планируются прямо в пул. Задачи,
[13:02.680 --> 13:08.560]  которые хотят исполняться последовательно, планируются в стренд. Но все вместе, все вместе задачи,
[13:08.560 --> 13:14.960]  все в совокупности, работают в одном пуле поток. Вот, через стренд мы планируем только те задачи,
[13:14.960 --> 13:19.880]  которые хотят, ну, то есть, критические секции, которые обращаются к некоторым данным. То есть,
[13:19.880 --> 13:23.920]  раньше у нас данные защищал Mutex, а теперь может быть, что данные защищают какой-то стренд.
[13:23.920 --> 13:28.440]  Вот, для своих данных — какой-то отдельный стренд.
[13:28.440 --> 13:48.080]  Да, спасибо. Последствия рефакта. Итак, ну, смотрите, что еще в этой истории удивительного — то,
[13:48.080 --> 13:54.920]  что мы хотим с вами в задачи построить некоторый лог-free Mutex и некоторый лог-free
[13:54.920 --> 14:03.960]  стренд. Мы хотим, это, собственно, тема сегодняшнего занятия, мы хотим с вами изучить новую технику
[14:03.960 --> 14:13.880]  синхронизации. Ну, почему? Потому что взаимное исключение нас немного тревожит. Но, чтобы объяснить,
[14:13.880 --> 14:19.360]  почему, давайте вообще вспомним про взаимное исключение. Мы с вами когда-то курс начали
[14:19.360 --> 14:24.440]  с изучения Mutex — это такой базовый примитив синхронизации. Нам нужно из разных потоков
[14:24.640 --> 14:29.560]  обращаться к одним и тем же данным. Мы эти данные защищаем Mutex, и все обращения оборачиваем
[14:29.560 --> 14:35.240]  в критической секции. То есть, говорим Mutex-Log, обращаемся к данным, потом говорим Mutex-Unlock.
[14:35.240 --> 14:42.000]  Секция закончилась. И чего мы от такого инструмента ожидали? Мы ожидали двух гарантий. Во-первых,
[14:42.000 --> 14:48.080]  мы ожидали взаимного исключения, собственно. Это safety-свойство. Ничего плохого не происходит.
[14:48.080 --> 14:52.360]  Не может быть такого, что два потока находятся одновременно в критической секции между вызовами
[14:52.360 --> 15:01.120]  Log и Unlock. И была вторая гарантия — гарантия прогресса. Что если Mutex свободен и за него
[15:01.120 --> 15:09.040]  борются потоки, то какие-то потоки Mutex захватывают. Система не может стоять на месте. И вот эта
[15:09.040 --> 15:16.240]  гарантия прогресса, пока неформальная, она у нас формализовалась в две конкретные гарантии уже,
[15:16.240 --> 15:25.240]  в две строгие. Это гарантия свободы от взаимной блокировки, гарантия глобального прогресса. Если
[15:25.240 --> 15:31.320]  Log свободен, то кто-то его захватывает обязательно. И вторая гарантия, более сильная, это гарантия
[15:31.320 --> 15:37.440]  локального прогресса. Каждый поток, который встает на захват блокировки, обязательно ее получает.
[15:37.440 --> 15:44.480]  Вот если вы писали самый простой спин-лог с операцией Exchange, то вы знаете, что если
[15:44.480 --> 15:50.960]  много потоков борются за этот спин-лог, то кто-то обязательно выигрывает. Но каждый отдельный
[15:50.960 --> 15:55.400]  поток может в принципе, ну в теории, проигрывать бесконечно долго. Ему ничего не гарантируется.
[15:55.400 --> 16:00.880]  Это глобальный прогресс. Локальный прогресс, то есть свобода от голодания, что каждый поток,
[16:00.880 --> 16:05.920]  который борется за блокировку, ее получает. Вот так было в TicketLock. Там у нас была очередь,
[16:05.920 --> 16:11.320]  и вы встали в очередь, и все, вот неизбежно до вас очередь дойдет. Вот, ну две такие гарантии —
[16:11.360 --> 16:18.360]  локального и глобального прогресса. Вот мы сегодня говорим про прогресс. Так вот, две эти гарантии,
[16:18.360 --> 16:25.400]  на самом деле, в определении своем, полагались на некоторое поведение планировщика. А именно,
[16:25.400 --> 16:31.880]  что планировщик работает, ну так, честно, хорошо, то есть он запускает все потоки, которые в нем есть.
[16:31.880 --> 16:41.080]  Вот если поток зашел в критическую секцию, захватил блокировку, то он рано или поздно это
[16:41.080 --> 16:47.800]  блокировку отпустит и пойдет дальше. В смысле, ему додадут работу и отпустит блокировку. То есть мы не
[16:47.800 --> 16:55.720]  предполагаем, что какой-то поток может надолго, например, зависнуть или вообще навсегда. Вот сегодня
[16:55.720 --> 17:02.640]  мы от этого предположения попробуем отказаться. Мы сегодня будем считать, что планировщик у нас
[17:02.640 --> 17:08.840]  может взять и остановить любой поток между любыми вообще инструкциями на произвольное время.
[17:08.840 --> 17:19.080]  И мы хотим даже в таком случае иметь гарантии прогресса. Вот смотрите, в такой ситуации локи
[17:19.080 --> 17:24.960]  не работают. Почему? Потому что поток захватил блокировку, а потом планировщик взял его и остановил
[17:24.960 --> 17:35.040]  на день, навсегда. Все, в системе никакого прогресса больше не будет. С другой стороны, наверное, такое не
[17:35.040 --> 17:40.280]  слишком реально, чтобы поток остановился навсегда. Все-таки потоки этой, там, не машины, они не
[17:40.280 --> 17:44.160]  взрываются, не сгорают. Ну, то есть если у вас сгорел компьютер, то вообще никаких потоков,
[17:44.160 --> 17:51.320]  больше нет совсем, ни одного. Можно не беспокоиться за это. Но все же, мы говорим сейчас про гарантию
[17:51.320 --> 17:55.800]  прогресса не с позиции того, что поток может остановиться навсегда. Такое только в теории возможно,
[17:55.800 --> 18:04.240]  а даже там. А про то, что мы не хотим, чтобы пауза одного потока, скажем, вот поток зашел в критическую
[18:04.240 --> 18:09.720]  секцию, взял спинлог, а его с ядра вытеснил планировщик, просто по кванту времени. Так вот,
[18:09.720 --> 18:15.680]  мы не хотим, чтобы такие ситуации, такие паузы влияли на другие потоки, потому что у нас их
[18:15.680 --> 18:22.120]  вообще ограниченное число, у нас тредпул. Понимаете, это проблема? То есть, когда мы работаем с мютоксами,
[18:22.120 --> 18:29.000]  потоки начинают друг от друга сильно зависеть, потому что внутри критической секции под мютоксом
[18:29.000 --> 18:36.520]  поток вытеснили, все прогресс, система глобально остановился. Мы сегодня хотим добиться более
[18:36.520 --> 18:41.200]  сильной гарантии, мы хотим сегодня добиться неблокирующей синхронизации. Это пока не
[18:41.200 --> 18:48.040]  формальное определение. Неблокирующая синхронизация – это синхронизация, при которой пауза произвольного
[18:48.040 --> 18:55.440]  потока между произвольными инструкциями не может привести к тому, что в системе остановится
[18:55.440 --> 19:02.280]  прогресс. Вот система совершает прогресс, где бы какой поток не остановился. Разумеется,
[19:02.280 --> 19:13.880]  глобальный прогресс. Из нашего установки задач следует, что мютоксы мы использовать уже не можем,
[19:13.880 --> 19:19.440]  видимо. Мы хотим придумать какой-то альтернативный инструмент. Но прежде, чем про него говорить,
[19:19.440 --> 19:25.120]  давайте формулизуем, что именно я хочу. Гарантия неблокирующей синхронизации – это гарантия
[19:25.120 --> 19:37.600]  которая выражает, формулизуется в двух конкретных гарантиях. Это weight freedom и log freedom. Давайте
[19:37.600 --> 19:43.720]  начнем, наверное, с weight freedom. Она более простая в определении. Мы скажем, что пусть у нас есть
[19:43.720 --> 19:48.440]  некий какой-то разделяемый объект, какая-то структура данных пусть, к ней обращаются
[19:48.440 --> 19:56.440]  разные потоки. Мы скажем, что отдельный метод этой структуры данных многопоточной weight free
[19:56.440 --> 20:06.800]  свободен от ожидания, если этот вызов завершается независимо от того, как ведут себя другие потоки,
[20:06.800 --> 20:13.760]  как они работают, где они останавливаются, насколько они останавливаются. Вот вызов начинается,
[20:13.760 --> 20:19.800]  и если процессор дает потоку, который этот вызов делает работать, то вызов непременно завершится.
[20:19.800 --> 20:27.040]  Гарантия понятна? Это самая сильная гарантия, которую можно представить. Это гарантия локального
[20:27.040 --> 20:34.720]  прогресса. Это вот гарантия свободы от голоданий, только не блокирующая, только мы не требуем
[20:34.720 --> 20:39.320]  ничего от планировщика здесь. Планировщик может сделать что угодно, торматить любые потоки на
[20:39.320 --> 20:48.840]  любое время. Вторая гарантия lock freedom, она более слабая, она про глобальный прогресс. Вот пусть мы
[20:48.840 --> 20:54.560]  пишем некоторую многопоточную структуру данных, к ней обращаемся из разных поток, и вот зафиксируем
[20:54.560 --> 21:03.920]  какую-то точку во времени, и вот сейчас стартовали какие-то вызовы. Мы скажем, что реализация,
[21:04.120 --> 21:10.880]  вся реализация этой структуры lock free, если в будущем, независимо от того, как действует
[21:10.880 --> 21:17.120]  планировщик, завершится хотя бы один вызов. Может быть, те вызовы, которые прямо сейчас
[21:17.120 --> 21:24.480]  стартовали, никогда не завершатся, но какой-то вызов в будущем обязательно завершится. Если
[21:24.480 --> 21:27.920]  говорить про бесконечное исполнение, то можно сказать следующее, что в бесконечном исполнении
[21:27.920 --> 21:34.240]  завершается бесконечное количество вызовов. Вот обратите внимание, что я говорю о гарантии прогресса,
[21:34.240 --> 21:40.080]  weight freedom и lock freedom, я не говорю, что там, я вот строго определяю понятие прогресса в
[21:40.080 --> 21:45.200]  терминах завершающихся вызовов. Мне не важно, как структура данных устроена. Если вызови
[21:45.200 --> 21:50.840]  завершаются, значит прогресс есть. Вот я так его формулирую. И вот weight freedom – это гарантия
[21:50.840 --> 21:55.560]  прогресса завершаемости каждого отдельного вызова, независимо от того, как работают или не
[21:55.640 --> 22:03.440]  работают другие потоки. А lock freedom – это гарантия глобальная, что какой-то из вызовов завершится в будущем.
[22:03.440 --> 22:13.360]  Ну вот, мы хотим сегодня придумать механизм синхронизации, который будет одной или другой
[22:13.360 --> 22:23.360]  гарантии удовлетворять. Мы хотим вот неблокирующуюся синхронизацию. Ну, у нас-то точно для конечного
[22:23.360 --> 22:29.560]  числа поток, так что тут есть некоторые тонкости в определении. Число потоков будет бесконечное,
[22:29.560 --> 22:34.040]  но оно в природе бесконечным не бывает, и у нас уж точно не бывает, потому что у нас стритпулы.
[22:34.040 --> 22:40.320]  Вот, поэтому давай говорить, что число потоков конечное. Да, вот если у нас число потоков
[22:40.320 --> 22:43.600]  конечное, то гарантия lock freedom переформулируется как бесконечное исполнение, бесконечное число
[22:43.600 --> 22:50.520]  вызова завершается. Хорошо. Почему мы вообще об этом говорим? Потому что мы сегодня хотим делать,
[22:50.520 --> 23:00.560]  ну какова наша цель вообще? Зачем нам такой инструмент? Мы хотим делать более эффективную
[23:00.560 --> 23:06.240]  среду исполнения, более эффективный рантайм для наших фьюч и файберов. Вот мы пока занимаемся тем,
[23:06.240 --> 23:12.120]  что оптимизируем, ну пишем хорошие фьючи и файберы, фьючи еще не пишем, файберы пишем,
[23:12.120 --> 23:17.000]  в которых уже много всего происходит. Мьютокс, секундвары, там какие-то операции, каналы скоро
[23:17.000 --> 23:27.560]  появятся. Мы можем их хорошо делать, но под ними пока очень плохой тритпул. Он в смысле корректный
[23:27.560 --> 23:31.960]  тритпул, но очень медленный тритпул, потому что там одна общая очередь. Если вы запустите файберы
[23:31.960 --> 23:38.600]  под профилировщиком, то вы увидите, что большую часть времени занимают вызовы lock-a-unlock в вашем
[23:38.600 --> 23:44.040]  исполнении. Это довольно печально. Вот мы бы хотели планировщик сделать более эффективным, и мы
[23:44.040 --> 23:50.840]  займемся этим через неделю. Но пока нам нужен просто инструмент. Вот смотрите, с чего мы начали
[23:50.840 --> 23:55.040]  делать тритпул. Мы же начали с изучения мьютокса. Мы изучили инструмент, а потом с помощью него
[23:55.040 --> 24:00.960]  сделали тритпул. Вот так же и сейчас. Мы хотим изучить некоторый инструмент, такой подход,
[24:00.960 --> 24:06.320]  не блокирующий синхронизация, как альтернатива мьютоксам, а дальше его использовать, чтобы
[24:06.320 --> 24:12.160]  оптимизировать рантайм, в котором исполняются файберы, фьючи, карутины будущей, что угодно.
[24:12.160 --> 24:18.240]  Замысел понятен? То есть мы говорим сейчас не про то, как писать код, а про то, как его
[24:18.240 --> 24:28.040]  исполнять эффективно. И вот для этого нам нужен новый инструмент. Отлично. Мы уже поняли, что мьютокса
[24:28.040 --> 24:34.120]  использовать нельзя. Если поток зашел в критическую секцию, его заблокировали, его вытеснили с ядра,
[24:34.120 --> 24:41.600]  то все, никакого прогресса нет. Как же быть? Ну, смотрите, да, вот, пожалуйста, lock-freedom означает,
[24:41.600 --> 24:45.840]  что у нас нет, ну, как бы название говорит о том, что как будто нет локов. На самом деле,
[24:45.840 --> 24:50.640]  если у вас просто нет локов, то это еще не lock-freedom. Lock-freedom — это когда у вас есть гарантия
[24:50.640 --> 24:56.680]  прогресса, что в будущем хотя бы один вызов завершится. Это не одно и то же. Чуть позже покажу
[24:56.680 --> 25:03.800]  пример сегодня. Итак, возвращаемся к истории. С помощью чего мы будем делать вот такую
[25:03.800 --> 25:11.440]  не блокирующую синхронизацию? Что у нас вообще есть? Ну, мьютоксов нет, значит, мы спускаемся вот
[25:11.440 --> 25:35.600]  к совсем базовым инструментам. Давайте починим интернет. Да. Нет, потоки на бесконечное время
[25:35.600 --> 25:40.720]  остановиться вообще не могут. Ну, планировщик работает разумно. Зачем он останавливает потоки
[25:40.720 --> 25:46.340]  на бесконечное время? Потоки могут останавливаться на какое-то время. И вот пауза потока под
[25:46.340 --> 25:52.160]  критической секцией, она мешает прогрессу других потоков. И мы хотим вот это влияние исключить.
[25:52.160 --> 25:57.600]  Цель такая. Я ответил на вопрос.
[25:57.600 --> 26:16.400]  Еще раз. Мы хотим, чтобы прогресс был, даже если поток остановится на бесконечное время.
[26:16.400 --> 26:20.760]  Поток в компьютере не будет останавливаться на бесконечное время. Но он может останавливаться
[26:20.760 --> 26:24.720]  на какое-то время. И если он остановится под критической секцией, это проблема. Так вот,
[26:24.760 --> 26:30.440]  если мы подготовимся к худшему случаю, то мы сможем обеспечить прогресс и вот просто в
[26:30.440 --> 26:36.480]  реальном компьютере. Просто худший случай, он для нас упрощает модель. Мы не думаем,
[26:36.480 --> 26:40.960]  как именно планировщик работает. Мы считаем, что он просто вот работает, ну, может, поток
[26:40.960 --> 26:45.040]  остановится навсегда. Мы не строим там какие-то вероятностные модели, что он там останавливает
[26:45.040 --> 26:48.640]  поток на такое время, там какие-то распределения. Нет. Мы говорим просто, вот, худший случай,
[26:48.640 --> 26:56.400]  может быть. Итак, что у нас есть? У нас остается атомик для синхронизации. Как обращаться к общим
[26:56.400 --> 27:03.560]  данным? Ну, атомики есть. Правда, вы, наверное, знаете, что в атомиках есть фундаментальная
[27:03.560 --> 27:12.040]  проблема. Ну, не то, что проблема, а ограничение. Все атомарные операции в компьютере ограничены
[27:12.040 --> 27:19.040]  одной ячейкой памяти. Может быть, это нам вообще мешает, наверное, если мы хотим что-то сложное
[27:19.040 --> 27:26.760]  делать. Ну, ладно. Какие операции у атомика есть? Load, Store, Exchange, Compare, Exchange. Ну, наверное,
[27:26.760 --> 27:32.800]  вы уже поработали с разными операциями и поработаете еще больше дальше, но, наверное,
[27:32.800 --> 27:38.200]  у вас уже есть интуиция, какая операция из этих самых, из этих операций самая выразительная,
[27:38.200 --> 27:50.000]  самая мощная. Ну, наверное, это Compare, Exchange. Что? T5.1 это не атомарная операция. Это сискол,
[27:50.000 --> 27:54.600]  во-первых, во-вторых, про блокирующее ожидание. Блокирующее ожидание к нашей
[27:54.600 --> 28:00.800]  лекции сегодня не имеет, оно просто перпендикулярно. Мы хотим из разных потоков работать с общими
[28:00.800 --> 28:08.160]  данными. Блокирующее ожидание это вот не решает эту проблему. Решает проблема Mutex. Мы берем
[28:08.160 --> 28:13.000]  Mutex, а потом знаем, что вот мы одни и можем состояние менять. Вот какую проблему мы решаем. А
[28:13.000 --> 28:17.760]  сейчас уже не можем так делать, потому что Mutex больше нет. Потому что он нас лишает глобального
[28:17.760 --> 28:24.240]  прогресса. Так вот, у нас остаются атомарные операции и самая, наверное, выразительная из них
[28:24.240 --> 28:32.160]  это операция Compare, Exchange. Что она делает? Она берет, она сравнивает содержимое атомарной ячейки
[28:32.160 --> 28:37.560]  памяти с некоторым ожидаемым значением. Если значение совпало, то ячейка перезаписывается
[28:37.560 --> 28:43.400]  на новое значение. Если не совпало, у нас не работает интернет сегодня. Нет, ну мы так жить не
[28:43.400 --> 28:52.440]  сможем, конечно. Если вы скачиваете фильм сейчас, то остановитесь, пожалуйста. Это нам не помогает.
[28:52.440 --> 29:02.600]  Вот, Compare, Exchange. Что она делает? Она, значит, сравнивает содержимые ячейки с ожидаемым
[29:02.600 --> 29:08.840]  значением. Если успешно сравнение завершилось, то перезаписывает и возвращает true. Если не успешно,
[29:08.840 --> 29:17.140]  то через ссылку, через первый аргумент возвращает прочисленные значения и говорит пользователю
[29:17.140 --> 29:27.320]  false. Операция провалилась, перезапись не состоялось. Смотрите, вы это, наверное, уже сами
[29:27.320 --> 29:33.160]  видели. С вами хорошо представляете себе, что Compare, Exchange. Да, как мы собираемся этой операции
[29:33.160 --> 29:42.200]  пользоваться? Давайте я покажу общий паттерн. Некоторый общий паттерн. Как с помощью этой
[29:42.200 --> 29:48.720]  операции мы будем строить сегодня почти всё с гарантией lock-free? Мы будем использовать касс-луп.
[29:48.720 --> 29:57.080]  Вот пусть мы хотим реализовать какую-то операцию атомарную. Fetch-increment, fetch-add. Что мы делаем?
[29:57.080 --> 30:03.640]  Мы заводим цикл себе и в нем делаем такой снимок текущего состояния ячейки памяти,
[30:03.640 --> 30:10.840]  ну или какого-то более сложного состояния. Потом локально применяем изменения, а потом
[30:10.840 --> 30:15.880]  пытаемся их зафиксировать в разделяемом состоянии с помощью кассы. Если получилось,
[30:15.880 --> 30:22.040]  то операция состоялась. Если не получилось, то почему не получилось? Видимо, состояние ячейки
[30:22.040 --> 30:29.680]  поменялось. Значит, ну ладно, мы проиграли. У нас операция завершилась неудачей, попытка
[30:29.680 --> 30:35.120]  завершилась неудачей. Но это означает, что какая-то другая операция завершилась успешно. Так что мы
[30:35.120 --> 30:39.280]  пробуем. И вот это пример такого lock-free кода. Самое простое, которое можно представить себе.
[30:39.280 --> 30:51.880]  Вот lock-free-increment. Ну это хорошее замечание. Да, выгоднее. Вот.
[30:51.880 --> 31:03.200]  Можно сделать вот так вот, да. Да, это будет, ну код стал лучше. Идея понятна. Вот lock-free-код.
[31:03.200 --> 31:11.200]  Просто у меня ломается документация. Я хочу, чтобы явно были шаги с нынешним состоянием,
[31:11.200 --> 31:17.240]  а потом делаем попытку зафиксировать изменения. Поэтому код так написан. Вопрос.
[31:21.880 --> 31:31.600]  Так это же есть гарантия lock-free. Глобальный прогресс. Какие-то операции завершаются,
[31:31.600 --> 31:35.160]  какие-то могут не завершаться. Это же вот буквально определение lock-free.
[31:35.160 --> 31:44.000]  Эта реализация lock-free, она не weight-free. В weight-free каждый вызов обязан завершаться. Здесь не обязан.
[31:44.000 --> 31:51.360]  Ну как это в теории так. На практике, скорее всего, конечно, каждый завершится.
[31:51.360 --> 32:02.760]  Итак, пример. И вы, ну, в нем видите и сами уже хорошо макаратно, наверное,
[32:02.760 --> 32:09.320]  встречали в своих домашках, что почему-то в C++, в Atomic у вас не один compare exchange,
[32:09.320 --> 32:14.680]  а два compare exchange weak и strong. И вы, наверное, знаете, чем они отличаются, что compare exchange
[32:14.680 --> 32:20.200]  strong – это вот compare exchange, которую вы ожидаете. А compare exchange weak – это такая странная версия,
[32:20.400 --> 32:25.760]  которая допускает ложно-отрицательные срабатывания. То есть она может сказать вам,
[32:25.760 --> 32:31.120]  что сравнение завершилось неудачей и не перезаписать ячейку, хотя содержимые ячейки
[32:31.120 --> 32:37.400]  совпадают прямо сейчас с тем, что вы передали первым аргументом. Вот. И, кажется, настал момент,
[32:37.400 --> 32:43.720]  чтобы разобраться, почему так происходит. Вот смотрите, есть good bolt. Смотрим в него.
[32:43.720 --> 32:51.760]  Смотрим на реализацию compare exchange weak и compare exchange weak, почему-то. Strong.
[32:51.760 --> 32:58.160]  Будет очень тяжелый день.
[33:13.720 --> 33:29.720]  Итак, смотрим на реализацию compare exchange weak и strong на нашем компьютере, вот на моем компьютере,
[33:29.720 --> 33:36.160]  например. И оказывается, что реализация одинаковая. Это log compare exchange. То есть мы
[33:37.160 --> 33:44.280]  лочим шину памяти и делаем такую сложную, неатомарную само по себе операцию. Итого,
[33:44.280 --> 33:49.280]  под x86 реализация не отличается. И вот возникает разумный вопрос, а зачем же нам две реализации
[33:49.280 --> 33:54.240]  compare exchange weak и strong, когда они устроены одинаковыми? Ну, дело, видимо, в том,
[33:54.240 --> 33:58.520]  что есть разные архитектуры, и, видимо, на какой-то другой архитектуре может быть
[33:58.520 --> 34:08.600]  другая реализация. Вот давайте посмотрим. Вот ARM, и на нем уже, смотрите, есть реализация weak,
[34:08.600 --> 34:21.920]  вот она такая. И реализация strong, она отличается. И давайте я попробую пояснить вам разницу. Дело
[34:21.920 --> 34:28.480]  в том, что на ARM вообще нет инструкции compare exchange, вот просто с такой семантикой. Вместо
[34:28.480 --> 34:37.200]  этого есть две отдельные инструкции. Вот такая вот, это load acquire exclusive register.
[34:37.200 --> 35:04.360]  И есть парная ей инструкция. Семантика у них следующая. Вот вы читаете ячейку памяти с
[35:04.360 --> 35:12.720]  помощью этой инструкции. И как будто бы берете на нее такую аппаратную блокировку, ну, условно так.
[35:12.720 --> 35:20.840]  А в этой операции вы пишете в ячейку, но запись, она условная, то есть запись совершается только если
[35:20.840 --> 35:29.640]  с момента вот такого вот чтения не было других записей в ту же самую ячейку на других ядрах. Идея
[35:29.640 --> 35:37.680]  понятна? Это в каком-то смысле проще и модульнее, чем compare exchange. У нас есть отдельная инструкция
[35:37.680 --> 35:42.680]  для чтения, отдельная инструкция для записи. Очень разумная архитектура. Вот, и запись
[35:42.680 --> 36:00.520]  условная. В общем случае такой набор операций называется load link and store conditional. Вот, разные
[36:00.520 --> 36:05.000]  архитектуры поддерживают такие операции, то есть прочесть с блокировкой, записать, если значение не
[36:05.000 --> 36:14.200]  изменилось. И как мы ими пользуемся? Ну, смотрите, вот наивная попытка. Мы читаем содержимое ячейки в
[36:14.200 --> 36:27.240]  регистр, потом сравниваем его с ожидаемым значением. Это значение, прочитанное в регистр. И если, ну,
[36:27.240 --> 36:36.240]  не совпало, то операция завершается провалом, да? А если совпало, то мы делаем store conditional. То
[36:36.240 --> 36:46.080]  есть мы пишем в ячейку новое значение, если оно не изменилось. В чем подвох? В реализации. В том,
[36:46.080 --> 36:54.680]  что вот эти инструкции, они скорее всего работают не на уровне отдельных ячеек, а на уровне целых
[36:54.680 --> 37:01.040]  кэшлиний. И может быть такое, что, ну, может быть false sharing. То есть вы взяли блок такой аппаратный
[37:01.040 --> 37:07.760]  на ячейку памяти, потом делаете в нее запись, но между вашим чтением и записью просто в ту же
[37:07.760 --> 37:14.320]  самую кэшлинию, но в другую ячейку была сделана запись на другом ядре. И вот у вас из кэша кэшлинию
[37:14.320 --> 37:21.160]  вымало. И store conditional, вот эта инструкция, store release exclusive register, она проваливается,
[37:21.160 --> 37:26.320]  но не потому, что значение поменялось в ячейке, а просто потому, что, ну, вот так устроен процессор.
[37:26.320 --> 37:32.680]  И вот это и есть ложно-положительное срабатывание. Вот такая реализация наивная,
[37:32.680 --> 37:37.680]  она может вернуть вам false, даже если значение ячейки совпадает. Просто операция записи
[37:37.680 --> 37:48.200]  провалилась, потому что кэшлиния, которую вы прочли, пропала из кэша. Понятна причина. Как
[37:48.200 --> 37:55.600]  теперь на такой архитектуре сделать strong версию? Ну вот мы читаем, сравниваем, и если не совпало,
[37:55.600 --> 38:03.680]  то операция проваливается. А иначе мы делаем store conditional и смотрим, как он завершился,
[38:03.680 --> 38:11.520]  результат будет в этом регистре. Если он завершился успехом, то операция возвращает true. А если он
[38:11.520 --> 38:18.160]  завершился провалом, то мы не знаем почему. Может быть, потому что ячейку перезаписали на другой
[38:18.160 --> 38:25.720]  значение, а может быть, потому что там кэшлиния уехала на другое ядро. Поэтому в этом случае мы
[38:25.720 --> 38:33.160]  пробуем заново. Вот получается такой встроенный цикл. Вот в ВИК версии в ней цикла внутри нет,
[38:33.160 --> 38:42.320]  а в strong версии есть. И вот поэтому это две разные версии. Вот, а почему у нас вообще compare
[38:42.320 --> 38:46.280]  exchange в библиотеке? Ну потому что на некоторых, ну потому что нельзя с помощью кассы выразить
[38:46.280 --> 38:50.880]  такую семантику. То есть с помощью этих двух инструкций касс можно выразить, а наоборот нельзя,
[38:50.880 --> 38:56.600]  поэтому кажется, что нельзя с такими же гарантиями. Поэтому в стандартной библиотеке у Atomic именно
[38:56.600 --> 39:06.800]  compare exchange такой общий знаменатель. И какая из этого мораль? Если, ну такое правило не совсем строгое,
[39:06.800 --> 39:14.600]  которым вообще можно пользоваться. Если вы пишете compare exchange в цикле, то используете ВИК
[39:14.600 --> 39:23.840]  версию. На x86 разницы не будет, но на процессорах, где разница есть, не будет overhead просто лишнего.
[39:23.840 --> 39:29.320]  То есть ваш цикл и так здесь исправляет вот это ложно-положительное, ложно-отрицательное
[39:29.320 --> 39:37.480]  срабатывание. Да, кстати, вот на армии, если у вас есть вот такая вот пара операций, то все остальные
[39:37.480 --> 39:45.160]  операции тоже выражаются через нее. То есть Fitch.at это вот, это тоже самый цикл. Вот Fitch.at на
[39:45.160 --> 39:52.360]  армии реализован вот буквально так. То есть если вы скомпилируете этот код на армии, то он и
[39:52.360 --> 40:06.400]  даст вам код Atomic с операции Fitch.at. Вот буквально. Это одно и то же. Хорошо, вот есть такие атомарные
[40:06.400 --> 40:11.280]  операции. Но смотрите, есть такая атомарная операция. Но дело довольно плохо, потому что она
[40:11.280 --> 40:17.440]  работает с одной ячейкой памяти. Вот раньше вы брали Mutex и могли сделать сразу несколько шагов
[40:17.440 --> 40:22.880]  атомарно для других процессов, для других поток, относительно других поток. А сейчас вы можете
[40:22.880 --> 40:29.240]  сделать атомарно манификацию только отдельной ячейки памяти. И вот в этом большая сложность
[40:29.240 --> 40:35.400]  Lock-free. Раньше у вас операции над разделяемым состоянием, над структурой данных какой-то,
[40:35.400 --> 40:43.160]  могут быть, могут состоять из нескольких шагов. И работая с блокировками, каждый поток видел
[40:43.160 --> 40:48.760]  какое-то разумное, согласованное состояние всей структуры. Когда вы работаете с Lock-free,
[40:48.760 --> 40:54.440]  у вас локов нет, и разные потоки работают вместе, одновременно трогают по одной ячейке,
[40:54.440 --> 41:00.840]  и в итоге могут наблюдать друг друга и какие-то промежуточные состояния вашей структуры данных.
[41:00.840 --> 41:09.240]  Это гораздо сложнее. Можно ли это обойти? Можно ли работать сразу с несколькими ячейками памяти?
[41:09.240 --> 41:16.120]  Ну давайте совсем коротко расскажу, что иногда можно. Вот, скажем, есть на интеллах такая
[41:16.120 --> 41:22.920]  особенная инструкция compare exchange 16b. Она означает, что у вас есть CAS сразу на двух соседних
[41:22.920 --> 41:29.880]  машинных словах. В каких-то жизненных ситуациях это может вас здорово выручить, вообще говоря.
[41:29.880 --> 41:36.560]  Но это операция специфичная для конкретного процессора, поэтому у нас к ней доступа нет.
[41:36.560 --> 41:44.760]  Есть еще такая штука, как транзакционная память в интеллах, опять же. Это такая оптимизация,
[41:44.760 --> 41:51.120]  которая позволяет вам на уровне процессора запускать транзакции, которые атомарно читают
[41:51.120 --> 41:58.840]  и пишут сразу несколько ячеек памяти. И все это реализовано очень эффективно, прямо через
[41:58.840 --> 42:04.440]  протоколк эгерентности. Вот протоколк эгерентности — это такой механизм для обнаружения конфликтов
[42:04.440 --> 42:10.400]  буквально для хардварных транзакций. Но я про это пока не рассказываю, и, может быть, вообще не
[42:10.400 --> 42:14.800]  рассказываю, скорее всего не рассказываю, потому что это всего лишь оптимизация. Это не может быть
[42:14.800 --> 42:19.760]  решением задачи. Вот железная транзакция, аппаратная транзакция в процессоре, она может прерваться
[42:19.760 --> 42:24.480]  просто потому, что она ступила себе на ногу, потому что она вытеснила свою же ячейку, свою же кошелинью
[42:24.480 --> 42:31.480]  из кыша. Такое может случиться, это неприятно. Поэтому для таких транзакций должен быть обязательно
[42:31.480 --> 42:39.360]  фолбэк на какое-то более надежное решение, вот на лог-фри или на блокировку. Так что мы остаёмся
[42:39.360 --> 42:47.240]  с кассом. Но, оказывается, и про это я, может быть, тоже не расскажу вам, что касс — это, короче,
[42:47.400 --> 42:52.760]  силу атомарных операций можно прямо измерить числом. Вот можно сказать, насколько хорош фич-эт,
[42:52.760 --> 43:01.280]  насколько хорош касс. Вот, оказывается, что касс в бесконечный раз лучше, чем фич-эт. И с помощью
[43:01.280 --> 43:06.440]  касса можно сделать всё, что угодно. В частности, можно с помощью касса сделать мульти-касс. То есть,
[43:06.440 --> 43:12.680]  операцию, которая атомарно сравнивает сразу много ячеек, а потом атомарно их перезаписывает. И это,
[43:12.680 --> 43:18.240]  разумеется, уже будет не инструкция процессора, это будет целый лог-фри алгоритм. И в какой-то
[43:18.240 --> 43:24.360]  из последних лекций, когда мы будем говорить про лог-фри, реализацию канала и селекта в языке
[43:24.360 --> 43:29.000]  котлин, я расскажу вам, как эту штуку можно сделать, как её можно воспользоваться, самое главное,
[43:29.000 --> 43:34.120]  и зачем её вообще пользоваться. Но сегодня это слишком сложно для нас, поэтому мы будем работать
[43:34.120 --> 43:43.600]  просто с одним кассом. Всё, что у нас есть, это один касс. Давайте с ним жить. Ну и что мы будем
[43:43.600 --> 43:48.000]  делать? Мы, давайте, наконец, что-нибудь полезное сделаем. Давайте сделаем что-нибудь с гарантией
[43:48.000 --> 43:55.680]  лог-фри. Мы поступим странно и сделаем лог-фри стэк. Вот не знаю, какие у вас ожидания. Делать,
[43:55.680 --> 44:01.600]  скажем, лог-фри стэк выглядит, возможно, не самой полезной задачей на свете. Зачем в конце концов
[44:01.600 --> 44:10.600]  стэк? Вот вам нужен стэк в жизни вообще? Часто вы им пользуетесь? Нет. Когда вы пишете промышленный
[44:10.600 --> 44:17.000]  код, вы никогда не пользуетесь стэком, примерно никогда. Но для нас стэк, на самом деле, это более
[44:17.000 --> 44:22.000]  хитрая штука. Лог-фри стэк – это совсем не контейнер для хранения данных. Вот оказывается,
[44:22.000 --> 44:32.080]  что хороший Mutex устроен примерно как лог-фри стэк. И это совершенно не очевидно сейчас. Но если вы
[44:32.080 --> 44:36.920]  будете решать домашнюю работу, если вы решите Exeggutor, напишите там Strand, а потом напишите
[44:36.920 --> 44:48.720]  лог-фри Mutex в задаче Prom Mutex, вот здесь вот, то, вот здесь вот, то, возможно, вы удивитесь,
[44:48.720 --> 44:55.320]  что внутри там будет почему-то лог-фри стэк. Так что пока я подробно не объясняю, зачем нам
[44:55.320 --> 45:00.280]  лог-фри стэк, когда-нибудь мы в это увидим. А пока мы просто считаем, что вот давайте сделаем
[45:00.280 --> 45:08.960]  структуру данных многопоточную с операциями push и pop, tri-pop, с которой могут работать разные
[45:08.960 --> 45:15.800]  потоки, в которые нет взаимного исключения и есть глобальный прогресс. Как мы будем этот стэк
[45:15.800 --> 45:20.440]  делать? Мы будем представлять стэк в виде односвязанного списка. Ну и забегая вперед,
[45:20.440 --> 45:27.960]  вообще все лог-фри это, как правило, списки, даже массив лог-фри тоже списки окажется. В этом
[45:27.960 --> 45:33.760]  списке узлы хранят значение, ну и указательный следующий узел. Голова вершины стэка — это
[45:33.760 --> 45:43.440]  голова односвязанного списка. Ну и давайте теперь напишем операции push и tri-pop. Задача ясна,
[45:43.440 --> 45:49.800]  что мы сейчас собираемся делать. Вот сделать push в голову стэка, в вершину стэка односвязанного
[45:49.800 --> 45:56.040]  голова односвязанного списка и pop забрать узел значения из головы односвязанного списка.
[45:56.040 --> 46:09.400]  Ну с чего мы начнем? Наверное, мы алоцируем узел. Тут есть некоторый подвох, потому что,
[46:09.400 --> 46:15.240]  возможно, вы уже проиграли лог-фри. Потому что, может быть, в вашем локаторе есть блокировки,
[46:15.240 --> 46:22.000]  скорее всего, там есть. Поэтому формально... Кот перестал только что быть лог-фри. Но все-таки
[46:22.000 --> 46:26.920]  в локаторе на быстром пути локов нет, как правило. То есть он редко заходит, все-таки хороший локатор
[46:26.920 --> 46:35.560]  редко берет локи. Поэтому, ну, такое вот некоторое несовершенство уже. Ну чем, если он будет часто...
[46:35.560 --> 46:40.280]  Редко, потому что он хочет... Ну, потому что это синхронизация. Локатор там, не знаю, может быть,
[46:40.280 --> 46:45.440]  в тред-локальном кэше уже что-то хранит, чтобы локации обслуживать без синхронизации. Ну,
[46:45.440 --> 46:52.640]  хороший локатор так непременно делает. Вот. К мютоксам и к синхронизации хороший локатор
[46:52.640 --> 46:59.320]  прибегает в последний момент. Давайте ускоримся. Дальше мы хотим этот узел вставить в голову
[46:59.440 --> 47:10.240]  списка, в голову стека. Ну вот давайте прочтем текущую голову. И прицепим к ней наш узел.
[47:10.240 --> 47:21.600]  Вот. А дальше, если мы пытаемся сделать вот так вот, то, наверное, это будет довольно провально.
[47:21.600 --> 47:29.520]  Потому что зашли два потока, в пушь сделал, алоцировали свои узлы, прицепились к голове,
[47:29.520 --> 47:37.800]  а потом обе перезаписали голову, и в итоге одна из тавок потерялась. Видимо,
[47:37.800 --> 47:47.760]  мы хотим здесь сделать запись, но только если голова по-прежнему та же. Ну, смотрите,
[47:47.800 --> 47:56.360]  что так и просится сделать. Наверное, нам нужен кастлуп. Вот мы прочитали топ, подвесили к нему
[47:56.360 --> 48:07.920]  текущий свой узел, а потом, если топ не изменился с момента нашего чтения, то мы перезаписали его
[48:07.920 --> 48:23.480]  на себя. Да? Ну вот, кастлуп, мы читаем текущее состояние стека, меняем его, возможно, и пытаемся
[48:23.480 --> 48:30.320]  зафиксировать изменения с помощью каст. У нас свик-версия, потому что зачем нам стронг? Если
[48:30.320 --> 48:38.400]  даже каст ошибся, он попробует заново. Ну вот, тот же самый код понятен, он очень прост,
[48:38.400 --> 48:51.640]  давайте если что-то непонятно, то лучше сейчас узнать. Ну как, мы ацируем узел, читаем голову стека,
[48:51.640 --> 49:00.000]  пытаемся к ней прицепиться, а теперь попытаемся поменять топ на себя. Мы хотим, чтобы в ячейке топ
[49:00.000 --> 49:07.520]  были написаны мы теперь, но мы хотим сделать такую перезапись, только если с момента нашего
[49:07.520 --> 49:25.800]  чтения топ не поменялся. Потому что, если мы напишем такой код... Нет. Ну как, мы сначала цепляемся
[49:25.800 --> 49:32.120]  к голове, а потом вставка в односвязанный список, в голову односвязанного списка. Да,
[49:32.120 --> 49:40.680]  ну а куда еще, мы же с головой работаем все время. Понятно, да? Вот, код можно написать чуть
[49:40.680 --> 49:53.320]  аккуратнее. Вот как было замечено, можно вынести все это и вообще написать вот так вот. Сэкономить
[49:53.320 --> 50:12.320]  усилия. Нет. Мы не пытаемся себя подставить в качестве следующего. Мы... Вот, Джон. Ну, сейчас...
[50:12.320 --> 50:34.600]  Ну и вообще стоит написать, наверное, вот так. Мы читаем картоп, подвешиваемся к текущей
[50:34.600 --> 50:38.880]  голове, а потом пытаемся, если эта текущая голова не изменилась, она написана здесь,
[50:38.880 --> 50:45.440]  то пытаемся поменять ее на себя. А если проверка провалилась, то мы здесь увидели новое актуальное
[50:45.440 --> 51:01.960]  значение топа. Сюда его перечитали. Вот, теперь пишем tri-pop симметрично. Ну тут, я надеюсь,
[51:01.960 --> 51:09.760]  уже без комментариев особых обойдется. Если список пустой, если стэк пустой, то мы говорим,
[51:09.760 --> 51:20.160]  что ничего нет. Иначе мы что пытаемся сделать? Мы пытаемся сдвинуть топ вперед. Мы ожидаем там
[51:20.160 --> 51:27.200]  сейчас текущее значение, а хотим записать current top next. И вот если получилось, то мы извлекли
[51:27.200 --> 51:48.800]  current top из головы стэка. Похоже на стэк? Вот, и можно его даже запустить и убедиться,
[51:48.800 --> 51:55.560]  что он работает. Вот у нас стресс-тест, там потоки кладут стэк, потом берут из стэка и ожидает,
[51:55.560 --> 52:05.600]  что сумма пуши и попов, она сойдется. Довольно естественное ожидание. Давайте этот тест запустим.
[52:13.320 --> 52:19.000]  Ну, сначала потерпим, потому что опять не собрал позиторию вовремя.
[52:19.000 --> 52:33.600]  Пока поищем стэки баги. Вдруг они у нас есть? Вдруг я их написал здесь неосторожно. Ну,
[52:33.600 --> 52:43.400]  я не просто так написал этот код. Давай оставим все как есть. Итак, он не работает. Ну, значит,
[52:43.440 --> 52:57.160]  мы где-то ошиблись, и вы невнимательно смотрите за лекцией. Ну, давайте искать, где я ошибся.
[52:57.160 --> 53:08.680]  Аллоцируем узел, читаем голову, цепляемся к ней, и потом, если не compare and change new node,
[53:08.680 --> 53:19.920]  next, new node, мы меняем top. Здесь вроде все правильно, да? А тут?
[53:19.920 --> 53:24.520]  Kartop от локальной переменной.
[53:49.920 --> 54:12.440]  Какая строчка? Локальная переменная. Ну, здесь мы увидели, значит, пустой стэк. Что ж плохого-то?
[54:12.440 --> 54:28.520]  Что? Ну, там стэк развалился совсем. Он почти ничего не сделал. А, возможно... Ну,
[54:28.520 --> 54:36.800]  потому что другой код запускается. Запускается неправильный код, а правильный код не запускается.
[54:36.800 --> 54:49.520]  Вот, все работает. Ну, осторожно. Значит, варианты сошлись. Просто в коде утечки. Вот,
[54:49.520 --> 54:55.920]  и это неудивительно, потому что я в коде написал new, а дырить не написал. Это было...
[54:55.920 --> 55:04.600]  Это было странно. На самом деле, это было не странно, потому что, если его написать,
[55:04.600 --> 55:10.000]  лучше не станет. Станет хуже. Потому что теперь у нас use of the free. Ну, и еще бы,
[55:10.000 --> 55:21.320]  чего вы хотели. Смотрите на нашу реализацию. Смотрите на pop. Вот у нас есть tri-pop. Приходит
[55:21.320 --> 55:32.680]  поток t1. Стэк сейчас выглядит так. Приходит поток t1. Он читает в current top. Я пишу abc,
[55:32.680 --> 55:42.680]  в смысле адреса узлов. Читает адрес a. Потом приходит t2. Он читает current top b. Ну,
[55:42.680 --> 55:56.200]  и дальше t2 завершает свой tri-pop. В частности, в нем t2 делает... Что? Дырит a. А теперь просыпается
[55:56.200 --> 56:08.800]  поток t1, и он увидел a, и он делает этот касс. И в чем беда? Ну, беда вот в этом месте. Разумеется,
[56:08.800 --> 56:15.080]  сам касс, он не может завершиться успешно, потому что current top уже поменялся. Ну, на самом деле,
[56:15.080 --> 56:19.880]  это еще сложнее. Но вот беда не в том, что у нас здесь написано не null pointer, а тут теперь null
[56:19.880 --> 56:26.640]  pointer. Беда в том, что мы идем вот под эти висящие ссылки теперь. Мы читаем поле next по адресу,
[56:26.640 --> 56:41.240]  который уже освобожден. И вот здесь возникает user-free. Что значит заранее? Вот здесь прочитать.
[56:41.240 --> 56:52.720]  Я боюсь, что... Зачем я трачу это время? Боюсь, что то же самое здесь произойдет. Какая разница?
[56:52.720 --> 57:01.880]  Это не поможет никак. Управление памятью здесь сломано. И как можно было бы поступить? Проще
[57:01.880 --> 57:06.880]  всего переписать это все на язык, где вы дырит и не пишете руками, где у вас есть автоматическая
[57:06.880 --> 57:11.920]  сборка мусора. Вот писать lock-free на языке с автоматической сборкой мусора гораздо-гораздо
[57:11.920 --> 57:17.560]  проще. Вот я утверждаю, что вообще сложность lock-free не вот в этом коде, а в коде управления памятью.
[57:17.560 --> 57:30.040]  Ну, как можно еще было бы поступить? Смотрите, мы здесь освобождаем память, на которую ссылается
[57:30.600 --> 57:37.280]  а потом читаем посылки. Ну вот сборки мусора у нас нет, но может быть мы хотели бы сделать
[57:37.280 --> 57:48.760]  какой-то ручной референс каунтинг. Ну, смотрите, хочется использовать shared-pointer. Но shared-pointer,
[57:48.760 --> 57:55.440]  вы, кажется, не чувствуете проблемы пока. Shared-pointer, они немного поточные совсем. Вот мы берем
[57:55.440 --> 58:08.240]  shared-pointer и в одном потоке пишем в него, а в другом потоке читаем. Конечно же,
[58:08.240 --> 58:16.640]  нет никакого атомика от shared-pointer. Ну, я объясню, что вы попробуйте его написать. Я думаю,
[58:16.640 --> 58:23.440]  что его напишет полтора человека в аудитории. В смысле, что такой атомик shared-pointer это атомик,
[58:23.440 --> 58:29.440]  который читает shared-pointer. Сейчас, давай разберемся просто с shared-pointer. Вот shared-pointer,
[58:29.440 --> 58:36.120]  в нем атомарный счетчик ссылок, но он совсем не многопоточный. Почему? Потому что, ну вот вы в
[58:36.120 --> 58:40.000]  одном потоке читаете shared-pointer. Что вы должны сделать для того, чтобы его прочитать, скопировать
[58:40.000 --> 58:46.320]  его? Вы должны пойти в control-блок и увеличить референс каунт. А другой поток перезаписывать
[58:46.320 --> 58:53.160]  этот shared-pointer. И вот вы здесь прочли pointer на control-блок, потом другой поток,
[58:53.160 --> 58:59.880]  благополучно shared-pointer, опустил нем счетчик до нуля и control-блок удалил. А потом вы увеличиваете
[58:59.880 --> 59:06.320]  по этому pointer счетчик ссылок. Мне кажется, что мы это уже только что видели. Вот так что сам
[59:06.320 --> 59:13.440]  по себе shared-pointer не помогает. Но, возможно, вы хотите воспользоваться лог-фри, атомик
[59:13.440 --> 59:19.640]  shared-pointer. Так тоже можно. И вот лог-фри-стэк на атомик shared-pointer выглядит как нормальный
[59:19.640 --> 59:29.640]  лог-фри-стэк. Вот ничего сложного там нет. Только у вас теперь не атомик от node-звездочки, а атомик
[59:29.640 --> 59:36.120]  от shared-pointer. Но shared-pointer такая сложная штука, это явно не атомарная инструкция в процессоре.
[59:36.120 --> 59:43.840]  Это какой-то дико навороченный, дико запутанный лог-фри-алгоритм. Вот, поэтому мы пока пренебрегаем
[59:43.840 --> 59:47.440]  всем вот этим, мы считаем, что у нас пока его нет, мы его пока не написали. Кто-нибудь из вас,
[59:47.440 --> 59:55.040]  возможно, его напишет, но это очень сложно. Можно потратить, скажем, неделю времени и не написать
[59:55.040 --> 01:00:00.400]  его. Будьте осторожны, когда вы пишете атомик shared-pointer. Но, тем не менее, вот так задачу можно
[01:00:00.400 --> 01:00:04.880]  было бы решить, но с большим overhead-ом, потому что это некоторый более общий инструмент,
[01:00:04.880 --> 01:00:12.880]  он не знает про специфику нашего stack-а. Можно было бы проще. Почистить в конце – это все равно,
[01:00:12.880 --> 01:00:17.080]  что не чистить. Ну, потому что программа работает бесконечно, демон работает бесконечно,
[01:00:17.080 --> 01:00:26.200]  никакого конца нет. 24 на 7. Это не поможет. Ну, давай объясню, почему, раз мы об этом говорим.
[01:00:26.200 --> 01:00:30.800]  Можно было немножко жульничать и сказать, что давайте вот мы вытащили узел stack-а,
[01:00:30.800 --> 01:00:39.320]  мы не будем его удалять, а мы вместо этого прибережем его на будущее, положим его в pull,
[01:00:39.320 --> 01:00:45.520]  в другой lock-free stack. И когда нам понадобится узел, мы оттуда достанем и повторно положим,
[01:00:45.520 --> 01:00:51.280]  повторно будем его использовать. Ну, то есть, смотрите, что я хочу сделать. Я хочу сделать
[01:00:51.280 --> 01:00:58.280]  такой интрузивный stack. У него есть операция push и tri-pop, но тут просто я получаю готовый узел
[01:00:58.280 --> 01:01:04.280]  и просто его запихиваю в stack. А здесь я его извлекаю из stack-а и не думаю, как с ним работать.
[01:01:04.280 --> 01:01:15.600]  И теперь, что я делаю? Я пишу такой вспомогательный аллокатор. Когда я делаю операцию push,
[01:01:15.600 --> 01:01:22.360]  я у этого аллокатора беру ноду, в нее кладу значения и кладу эту ноду в stack с данными.
[01:01:22.360 --> 01:01:33.800]  Когда я делаю pop-ы stack-а, я извлекаю узел из stack-а, беру оттуда значения, а потом возвращаю узел аллокатора,
[01:01:33.800 --> 01:01:43.240]  который сам по себе тоже lock-free stack. Раз stack, два stack. Понятно, да? Ну, то есть,
[01:01:43.240 --> 01:01:50.000]  мы так перекладываем между двумя stack-ами эти узлы. Будет ли это работать? Ну, кажется,
[01:01:50.040 --> 01:02:09.720]  я уже показал. Не должно. Почему не должно? Потому что аллокаторы системные, они уже делают пулинг,
[01:02:09.720 --> 01:02:16.480]  они уже переиспользуют память. Поэтому мы всего лишь написали руками кусочек аллокатора. И было
[01:02:16.480 --> 01:02:22.040]  бы странно, если бы он заработал. Смотрите, что происходит. Ну, да, теперь мы память не удаляем,
[01:02:22.040 --> 01:02:26.520]  мы ее переиспользуем. И вроде бы вот висящей ссылки быть не может уже. Ссылки на узел,
[01:02:26.520 --> 01:02:32.000]  который освобожден. Ну, ссылка на переиспользованный узел может быть. Смотрите,
[01:02:32.000 --> 01:02:39.720]  вот это называется ABA-проблема. У нас был stack с данными, ABC, и был вот пул аллокатора.
[01:02:39.720 --> 01:02:47.240]  Пришел поток T1, прочел в current top A, прочел в current top next B, готовится сделать CAS,
[01:02:47.240 --> 01:02:56.280]  переставить голову stack с A на B. Потом приходит поток T2. Он извлекает из stack узел A, извлекает
[01:02:56.280 --> 01:03:03.520]  из stack узел B и возвращает память аллокатора. Вот теперь такая конструкция. В данных узел C,
[01:03:03.520 --> 01:03:14.360]  в пуле узлы B и A. Да? А теперь поток T3 приходит и делает push. Он из пулы A берет узел B и
[01:03:14.360 --> 01:03:23.520]  засыпает. Потом приходит поток T2, он тоже делает push, он из пулы A берет узел A и кладет его на
[01:03:23.520 --> 01:03:36.520]  вершину stack. А теперь просыпается поток T1, и он делает compare exchange с A на B. Схватили
[01:03:36.520 --> 01:03:45.920]  проблему? То есть, у нас раньше stack был ABC, на вершине было A, а теперь stack стал AC. Но
[01:03:45.920 --> 01:03:53.480]  поток T1 думает, что stack не изменился, потому что на вершине по-прежнему узел A. И он
[01:03:53.480 --> 01:04:01.040]  переставит эту вершину stack на узел B, а узел B вообще не в stack сейчас. И вот мы разломали память.
[01:04:01.040 --> 01:04:13.400]  Вот. Это называется ABA, и вот она у stack есть. Беда. Как быть? Очень сложно, я не буду рассказывать.
[01:04:13.400 --> 01:04:20.920]  На семинарах поговорим, и в домашке вы напишете. Если захотите, конечно. Ну потому что сложность,
[01:04:20.920 --> 01:04:26.320]  она не в этом. Она не в... Сложность не в лог-фри-стэке. Вот такой stack напишет и ребенок в конце концов.
[01:04:26.320 --> 01:04:33.200]  Сложность, ну, тем более вот такой с дыритом. Сложность в том, чтобы сделать дырит и при этом
[01:04:33.200 --> 01:04:39.160]  ничего не разломать. Вот это настоящая сложность лог-фрик, особенно когда вы пишете на C++. Но если
[01:04:39.160 --> 01:04:46.840]  вы пишете лог-фри-мьютакс, то вам предлагается подумать. Потому что было бы странно, просить
[01:04:46.840 --> 01:04:50.120]  вас написать лог-фри-мьютакс и при этом говорить, что я вам не рассказываю, как написать хороший
[01:04:50.120 --> 01:04:55.680]  лог-фри-стэк. И при этом говорю, что stack – это мьютакс. Мьютакс – это stack, вернее. А может быть,
[01:04:55.680 --> 01:05:00.840]  это все слишком непонятно, вы меня просто не слушаете. Так тоже можно. Ладно, давайте про stack
[01:05:00.840 --> 01:05:08.320]  закончим. Мы в какой-то степени его сделали, а в какой-то степени нет. Давайте мы в такой же
[01:05:08.320 --> 01:05:15.720]  степени, непонятно, сделаем теперь очередь. То есть структура данных будет сложнее. Итак,
[01:05:16.360 --> 01:05:21.480]  проблемы у нас те же самые будут с проблемой памяти, но мы пока заботимся про сам контейнер. Мы делаем
[01:05:21.480 --> 01:05:28.880]  очередь. И очередь – это будет тоже односвязанный список. У него будет голова и хвост. И голова – очереди.
[01:05:28.880 --> 01:05:36.280]  И список ориентирован от головы к хвосту. Извлекаем мы отсюда, добавляем мы вот сюда. И у нас
[01:05:36.280 --> 01:05:40.800]  будут два поинта. Head и tail указывают на первый узел списка, на голову и на хвост, соответственно.
[01:05:40.800 --> 01:05:48.200]  И чтобы избежать некоторого… Да, и вот узел списка. Next, теперь atomic, но скоро увидим почему.
[01:05:48.200 --> 01:05:59.720]  Очередь у нас немного сложнее, поэтому мы себя хотим обезопасить от граничного случая,
[01:05:59.720 --> 01:06:06.480]  когда очередь пустая. Поэтому мы скажем, что очередь пустой не бывает. В ней всегда есть такой
[01:06:06.480 --> 01:06:18.080]  узел Sentinel, который в себе значение не содержит. Он просто… Логически пустая очередь – это очередь
[01:06:18.080 --> 01:06:30.480]  из одного физического узла, без значения внутри. Как из этой очереди извлечь голову?
[01:06:30.480 --> 01:06:45.360]  Ну, в смысле, мы хотим извлечь первый элемент из очереди. Ну, я понимаю, что такой очередь с вами.
[01:06:45.360 --> 01:06:53.400]  Как ее написать? Ну, кажется, что извлечение из головы очереди – это извлечение головы из
[01:06:53.400 --> 01:06:59.640]  односвязанного списка все еще. Поэтому, кажется, TRI-POP должен быть устроен каким-то похожим образом.
[01:06:59.640 --> 01:07:11.040]  Да, я не сказал, у этого стека, который мы изучили, есть свое имя – это stackTriber, а очередь,
[01:07:11.040 --> 01:07:16.520]  которую мы сейчас пишем, называется очередь у Майкла Скотта. Вот так их можно в интернете легко найти.
[01:07:16.520 --> 01:07:41.320]  Итак, что мы делаем? Как обычно, мы читаем голову. Если… Это Sentinel. Как узнать, что очередь пустая?
[01:07:41.320 --> 01:07:56.760]  Ну, вот тут нужно еще кое-что другое написать, но я пока не стану. Если очередь… Если голова… Если
[01:07:56.760 --> 01:08:02.640]  Next в голове указывает на no pointer, то, видимо, очередь состоит из одного физического узла – это пустая
[01:08:02.640 --> 01:08:18.520]  очередь. Ну, а иначе мы делаем какие-то привычные вещи. Мы пытаемся передвинуть голову вперед. Мы
[01:08:18.520 --> 01:08:37.800]  ожидаем увидеть, что currentHead хотим currentHeadNext. Если так получилось, то значение мы возьмем отсюда.
[01:08:37.800 --> 01:08:46.800]  Понятно, да? То есть мы извлекли один узел, а значение взяли из следующего. И вот этот
[01:08:46.840 --> 01:08:53.720]  следующий узел, следующая голова сама стала теперь Sentinel. То есть мы передали эту роль дальше
[01:08:53.720 --> 01:09:05.040]  по очереди. Вот. Ну и снова мы здесь, видимо, не пишем the read, оставляем это до лучших светлых
[01:09:05.040 --> 01:09:09.720]  времен. Да, Машке вы тоже напишите. Это делать на другим способе, не таким, как для стэк.
[01:09:09.720 --> 01:09:19.040]  Tripop готов, потому что тут ничего принципиально нового не происходит. Вот. А push – он меняется,
[01:09:19.040 --> 01:09:27.680]  потому что push теперь в два раза сложнее. Почему в два? Потому что… Ну, что такое push в конец
[01:09:27.680 --> 01:09:36.120]  односвязанного списка? Нам нужно к последнему узлу прицепить еще один, а потом передвинуть tail.
[01:09:36.120 --> 01:09:47.520]  И вот в этой операции мы уже трогаем две разделяемые ячейки памяти. Мы трогаем next у
[01:09:47.520 --> 01:09:55.080]  последнего узла, и это могут делать разомного потока. А потом мы двигаем тоже общий tail. Вот
[01:09:55.080 --> 01:10:00.720]  две ячейки памяти. Если бы у вас был mutex, то было бы просто. Взяли бы mutex, потрогали бы две
[01:10:00.720 --> 01:10:08.240]  ячейки и всё. Отпустили бы mutex. Mutex теперь нет. Поэтому два этих шага, вообще говоря, теперь они
[01:10:08.240 --> 01:10:18.720]  наблюдаются разными потоками. Ну ладно, давайте не будем пока плохого, давайте писать. Мы лазируем
[01:10:18.720 --> 01:10:45.120]  новый узел. А дальше… Напишу пока так, потом объясню зачем. Читаем хвост.
[01:10:45.120 --> 01:10:54.320]  Если… Ну и мы хотим прицепиться к next у текущего хвоста. Но может быть,
[01:10:54.320 --> 01:11:08.160]  тут написано не null pointer. Что это значит? Что прямо сейчас выполняется какой-то конкурирующий
[01:11:08.160 --> 01:11:21.240]  push. Согласны? Ну отложим нашу вставку. Если же current tail next null pointer, то есть у нас
[01:11:21.240 --> 01:11:28.160]  есть теперь pointer на последний узел в списке. Ну давайте попытаемся этот null pointer поменять на себя.
[01:11:38.160 --> 01:12:01.960]  Согласны? Это первый шаг. Вот, а второй шаг. Второй шаг.
[01:12:08.160 --> 01:12:29.600]  Положили себя в tail. Пишу ерунду, простите. Мы пытаемся прицепить себя к хвосту списка,
[01:12:29.600 --> 01:12:49.880]  если получилось, то вот мы пишем tail sort. Где? Я так не думал. Ну сейчас. Да, но нет. Вот,
[01:12:49.880 --> 01:12:58.640]  это похоже на правду. Я где-то ошибся. Я ошибся, потому что у меня value в узле это на самом деле
[01:12:58.640 --> 01:13:24.200]  optional и мне нужно написать вот так. И добрось. Сейчас узнаем. Ну что ж, очередь работает. То есть у нас
[01:13:24.200 --> 01:13:34.880]  снова течет, но по крайней мере сколько в него положили, столько потом и достали. Правда есть
[01:13:34.880 --> 01:13:44.040]  беда. Вот смотрите, что умеет наш framework. Он умеет тестировать lookfree. Каким образом? По
[01:13:44.040 --> 01:13:51.040]  определению. Вот можно вставить в исполнение нового adversary, который перехватывает сбои,
[01:13:51.040 --> 01:13:58.320]  и он будет вести себя так. Он может не только потоки переключать как-то хаотичным образом,
[01:13:58.320 --> 01:14:04.080]  а он может их парковать. Ну что такое lookfree? Пауза бесконечная какого-то произвольного
[01:14:04.080 --> 01:14:07.640]  потока между любыми инструкциями не должна привести к тому, что остановится прогресс,
[01:14:07.640 --> 01:14:15.160]  что вызовы перестанут завершаться. Что делает наш stress-тест? Он вот в этом adversary иногда
[01:14:15.160 --> 01:14:25.000]  паркует потоки на бесконечное время. И при этом он просит, что, пожалуйста, когда вы завершаете
[01:14:25.000 --> 01:14:35.200]  вызовы на очереди, то сообщаете мне отверстие, что они заканчиваются, что прогресс происходит.
[01:14:35.200 --> 01:14:43.520]  И если отверстие останавливает поток, а потом после этого в него сыпется сообщение, что вызовы
[01:14:43.520 --> 01:14:48.440]  завершаются, видимо, реализация lookfree. А если вызовы перестанут завершаться, то, видимо,
[01:14:48.440 --> 01:15:00.920]  не lookfree. Ну и давайте узнаем, является ли наша реализация lookfree. Ну, не является, она зависла.
[01:15:00.920 --> 01:15:13.200]  Почему? Где нужно было остановить потоки, чтобы другие потоки не смогли завершаться?
[01:15:13.200 --> 01:15:24.920]  Ну смотри, у нас здесь два шага. Первый шаг, второй шаг. Первый подвесится к концу,
[01:15:24.920 --> 01:15:34.920]  второй передвинут tail. И вот здесь мы, смотрите, видим на шаге 1, что другой поток прошел через
[01:15:34.920 --> 01:15:41.520]  шаг 1, но, возможно, не прошел через шаг 2. Ну вернее, как мы сначала прочли tail,
[01:15:41.600 --> 01:15:47.000]  потом прочли tail next, и видим, что там не был pointer. Может быть, за это время tail уже
[01:15:47.000 --> 01:15:53.400]  поменялись и сдвинули вперед, и все нормально. А может быть, другой поток просто уснул между
[01:15:53.400 --> 01:16:01.280]  цеплянием своего узла и передвижением tail. И вот это конкурентный push, но он не завершится больше,
[01:16:01.280 --> 01:16:09.280]  его остановили. А мы его здесь ждем, мы крутимся, пока он не завершится. Вот этот код, в нем нет
[01:16:09.280 --> 01:16:14.240]  блокировок явных, ну в смысле, у него нет локов, но он не лок-фри, потому что он все еще ждет другого
[01:16:14.240 --> 01:16:27.520]  потока. По сути, он такой спинлок вот в этом месте. Что делать? Что? Мне кажется, ты неверно
[01:16:27.520 --> 01:16:32.400]  истолковываешь суть. Засыпать можно в лок-фри, это отдельная история, я про него тоже сегодня не
[01:16:32.400 --> 01:16:39.000]  успею. Давайте, у нас осталось совсем немного времени, придумаем решение. Ну, смотрите,
[01:16:39.000 --> 01:16:42.720]  у нас есть операция, она пришла, увидела, что другая операция, возможно, находится между первым и
[01:16:42.720 --> 01:16:50.000]  вторым шагом. Что ей нужно сделать? Нашей текущей операции. Мы же не знаем, чего ждать,
[01:16:50.000 --> 01:16:54.280]  другая операция никогда не завершится, просто никогда. У нас лок-фри, поток может остановиться
[01:16:54.280 --> 01:17:04.120]  на бесконечное время. Ждать некого, надеяться не на кого. Ну, сейчас самому мы не можем, у нас уже
[01:17:04.120 --> 01:17:12.640]  как бы другой push in progress. Откатиться это не прогресс, нужно докатить, нужно помочь другой
[01:17:12.640 --> 01:17:18.280]  операции завершиться, чтобы нам она не мешала. Вот она уже подвесила свой узел, но не передвинула
[01:17:18.280 --> 01:17:23.720]  тейл. Давайте ей поможем. Вообще говоря, она может помощь ей не просить, может быть она работает,
[01:17:23.720 --> 01:17:34.840]  но мы здесь этого не понимаем, поэтому мы вот так вот консервативно попробуем. Что я хочу сделать?
[01:17:34.840 --> 01:17:50.840]  Вот так вот. Вот это help, это базовая техника в лок-фри. У вас операции могут быть сложные,
[01:17:50.840 --> 01:17:54.280]  составные. Вот если вы видите, что другая операция между двумя шагами находится,
[01:17:54.280 --> 01:18:05.320]  то попробуйте ей помочь. Нет, ну вообще лок-фри нет никакого общего решения. Mutex такой общий
[01:18:05.320 --> 01:18:10.440]  инструмент. Лок-фри это такое искусство изготовления, вручную структуру данных,
[01:18:10.440 --> 01:18:16.600]  такой крафтовый искусство. То есть ты вот строишь, мастеришь свою специальную реализацию. Люди этим
[01:18:16.600 --> 01:18:22.720]  занимаются очень долго. Написали миллион статей, как сделать крафтовый лок-фри стэк, мют, стэк,
[01:18:22.720 --> 01:18:30.960]  очередь, хэш-таблицу. Больше особо ничего не делали. Там вот много разных способов. Здесь касс,
[01:18:30.960 --> 01:18:35.000]  потому что можем помогать мы, а могут быть другие потоки, которые тоже помогают. Вот мы решили
[01:18:35.000 --> 01:18:39.840]  помочь, а другой уже помог, и все убежало вперед, поэтому мы не можем просто стор сделать. Мы делаем
[01:18:39.840 --> 01:18:54.640]  касс. Понятно, да? Вот, запускаем и смотрим, помогло ли это. Нет, работает. Ну, утечки памяти,
[01:18:54.640 --> 01:18:59.920]  но работает теперь, потому что это лок-фри уже. То есть вот здесь вот пауза отдельного потока не
[01:18:59.920 --> 01:19:09.760]  приводит к простою. Но есть одна беда. Вот смотрите, я здесь извлек узел, а потом напишу в нем
[01:19:09.760 --> 01:19:26.240]  что-нибудь такое. Ну, просто какой-то мусор в адрес напишу. Почему нет? Могу я написать так
[01:19:26.240 --> 01:19:36.560]  или нет? Я же извлек узел уже. Вот, все разломалось снова. Почему? Потому что мне еще кое-что нужно
[01:19:36.680 --> 01:19:44.360]  сделать. Вот я здесь пишу, смотрите, на шаге два, tail store new node. Вот, я, смотрите, зашел в push,
[01:19:44.360 --> 01:19:52.200]  я привязал свой узел к концу списка, потом я заснул, tail подвинули вперед, прошло очень
[01:19:52.200 --> 01:19:59.080]  много вставок, а потом я взял и сбросил tail назад. Куда-то далеко в прошлое. Я не учел,
[01:19:59.080 --> 01:20:04.920]  что мне могли помочь. Поэтому здесь я тоже должен вот это закомментировать и написать здесь.
[01:20:04.920 --> 01:20:11.520]  Ну, и не знаю, мне чувствует прекрасно, почему-то от меня требуют, чтобы я написал здесь compare
[01:20:11.520 --> 01:20:21.120]  exchange strong, хотя это вроде бы совершенно необязательно. Вот, если мне не помогли,
[01:20:21.120 --> 01:20:25.120]  то я сам уж точно доделаю. А если помогли, то мой кассо уже ничего просто не сделает.
[01:20:25.120 --> 01:20:39.480]  Ну, еще раз, вот я для этого вам это и рассказываю, что не нужно так рассуждать. Это неправильное,
[01:20:39.480 --> 01:20:46.720]  это наивное рассуждение. То есть нельзя заменить сложную реальность простым правилом. Ну,
[01:20:46.720 --> 01:20:51.520]  какая разница? Если даже наш касс провалился, на следующей этой рации какой-нибудь push
[01:20:51.520 --> 01:20:57.720]  дотолкает. Ну, кто-нибудь вот здесь дотолкает в крайнем случае, поэтому проблем на самом деле нет.
[01:20:57.720 --> 01:21:05.440]  Так что можно было бы написать wick. Ну, вот мне так чувство прекрасного диктует, что все-таки push,
[01:21:05.440 --> 01:21:15.280]  вот если ему не помогли, то уж сам он точно должен завершиться. И не знаю, может быть,
[01:21:15.280 --> 01:21:22.720]  даже для линейзуемости это, наверное, не очень важно. Ну, в общем, вот так. Ладно, давайте какие-то
[01:21:22.720 --> 01:21:27.720]  заключительные слова, но заканчивается время. Не блокирующая синхронизация – это альтернатива,
[01:21:27.720 --> 01:21:38.520]  не вютоксом. Вот можно приделать к лог-фри реализации ожидания. Это называется even count.
[01:21:38.520 --> 01:21:44.720]  Вот если вы писали кондвар, то в принципе можно почти что как кондвар сделать кондвар для
[01:21:44.720 --> 01:21:51.200]  лог-фри. Он будет немного хитрее устроен, намного хитрее устроен, но в принципе как вютокс,
[01:21:51.200 --> 01:21:58.840]  в принципе как ваш кондвар. Только вы его логику немного размотаете наружу. Но наша лекция не про
[01:21:58.840 --> 01:22:06.240]  это, а про то, как строить примитивы синхронизации, как строить структуры данных, как работать с
[01:22:06.240 --> 01:22:14.280]  разделяемым состоянием, не используя мютоксы. Потому что мютоксы – это блокирующий примитив,
[01:22:14.280 --> 01:22:19.680]  и если поток вытеснен под блокировкой, то глобальный прогресс остановится. Мы хотим использовать
[01:22:19.680 --> 01:22:25.000]  не блокирующую синхронизацию, где пауза отдельного потока не влияет на другие. И вот у нас дальше
[01:22:25.000 --> 01:22:33.560]  будут задачи, и во многих, в разных местах можно будет использовать именно лог-фри. Чем больше
[01:22:33.560 --> 01:22:39.280]  лог-фри, чем больше глобального прогресса, чем меньше зависимость от планировщика, тем в итоге
[01:22:39.280 --> 01:22:45.240]  меньше простоев на уровне исполнения будет. Тем меньше потоки будут зависеть друг от друга от того,
[01:22:45.240 --> 01:22:50.440]  как они исполняются, как их планирует планировщик, когда он их вытесняет, когда нет. Понятная идея?
[01:22:50.440 --> 01:22:58.880]  Тогда мы готовимся к семинару, где вам расскажут, как во всех этих примитивах управляться с
[01:22:58.880 --> 01:23:04.840]  памятью, а через неделю мы поговорим про то, как сделать из нашего медленного тредпула очень
[01:23:04.840 --> 01:23:09.880]  хороший и очень сложный тредпул, который будет исполнять вот именно файберы, именно карутины,
[01:23:09.880 --> 01:23:13.480]  именно фьючи максимально эффективно. На сегодня все, спасибо.

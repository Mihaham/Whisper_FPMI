[00:00.000 --> 00:12.440]  Так, давайте потихоньку начнем. Все равно уже по времени надо. В начале все равно
[00:12.440 --> 00:20.440]  пару слайдов про мотивацию. То, что мы сегодня будем обсуждать, они в некотором смысле не такие
[00:20.440 --> 00:31.240]  важные, хотя показывают полную картинку того, что сегодня будет происходить. Все помним,
[00:31.240 --> 00:41.600]  рассматривали много раз этот градиент. Вроде бы с ним никаких проблем не было, но теперь такой
[00:41.600 --> 00:51.400]  вопрос возникает. Пусть, соответственно, есть у меня мои эти иксы. Я как-то хочу измерять между
[00:51.400 --> 01:01.880]  ними расстояние. До этого я всегда это делал в евклидовой норме. Здесь ввожу какую-то произвольную
[01:01.880 --> 01:12.640]  норму. Так, такой вопрос. Банок и пространство у вас уже были на МКН? Да, были. Супер. А сопряженное
[01:12.640 --> 01:24.880]  пространство? Не было. Ну ладно, я в принципе на справедливости ради рассчитывал, что эту лекцию
[01:24.880 --> 01:35.200]  должна быть перед предыдущей, но посмотрел, что электро у вас еще не успел рассказать. Думал,
[01:35.200 --> 01:45.000]  что успеет, но не успел. Последний он вам рассказывал про что. Там не было того,
[01:45.000 --> 01:53.080]  что мне нужно было. Есть банок и пространство, ничего сверхъестественного нету. Внутри
[01:53.080 --> 01:59.440]  просто норма, и это норма причем еще и полная. Главное то, что есть норма. Я как-то хочу измерять
[01:59.440 --> 02:07.040]  расстояние между, соответственно, своими иксами. Исходя из этого, что я могу вообще сказать про мой
[02:07.040 --> 02:13.720]  градиент f от x-катова? Вроде как градиент это же некоторые операторы, которые действуют на мой
[02:13.720 --> 02:21.460]  x и как-то его преобразуют. И на самом деле, в общем случае, никто вам не гарантирует то,
[02:21.460 --> 02:28.120]  что оператор, который вам подействовал на x, отобразил его в то же самое банок и пространство,
[02:28.120 --> 02:35.560]  в котором лежали x. И получается, что когда мы делаем вот такой вот градиентный спуск,
[02:35.560 --> 02:41.120]  мы складываем x из одного банка и пространства, градиент с другого банка и пространства,
[02:41.120 --> 02:47.360]  и получаем вообще не пойми, что, если это вообще можно сложить между собой адекватно. Но на самом
[02:47.360 --> 02:54.520]  деле ничего сверхъестественного нету. Я думаю, на Функане вам все четко ведут, как выводится,
[02:54.520 --> 03:01.880]  соответственно, сопряженная норма в сопряженном пространстве, в е со звездочкой. И вы на самом
[03:01.880 --> 03:06.440]  деле для себя откроете много чего интересного, потому что не зря мы тоже проходили сопряженные
[03:06.440 --> 03:13.620]  нормы. Потому что для каких-то обычных норм, типа нормы P, где, соответственно, P вас может принимать
[03:13.620 --> 03:21.760]  от значения от 1 до бесконечности, сопряженная норма будет нормой Q. Причем P и Q связаны с
[03:21.760 --> 03:29.200]  отношением вот таким вот. То есть, например, для первой нормы будет норма бесконечности сопряженной.
[03:29.200 --> 03:38.240]  Для второй нормы, и что хорошо, сопряженной нормой будет она же сама. То есть, вы видите из
[03:38.240 --> 03:45.720]  этого соотношения, что для евклидовой нормы сопряженной будет она же. И в этом плане,
[03:45.720 --> 03:50.960]  когда то, что мы делали с вами до этого, хорошо, потому что когда мы работали с евклидовой нормой,
[03:50.960 --> 03:56.680]  мы попадали в то же самое баноково пространство, когда считали градиент. Ну и, соответственно,
[03:56.680 --> 04:04.120]  все операции были валидны, когда мы складывали и получали X из того же банокового пространства.
[04:04.120 --> 04:12.080]  Но теперь, соответственно, у нас появилась мотивация. Кто мне сказал, что только с евклидовой
[04:12.080 --> 04:18.000]  нормой можно жить? Потому что все остальные нормы, они же тоже вводились в некотором смысле
[04:18.000 --> 04:25.320]  естественным образом. То есть, евклидовая это просто вариант так считать шар, на такой шар
[04:25.320 --> 04:31.760]  смотреть на другой. Например, если я хочу померить расстояние между двумя вероятностными
[04:31.760 --> 04:40.040]  распределениями, то, конечно, я могу это сделать евклидовой метрике, но как-то неестественным это
[04:40.040 --> 04:47.640]  кажется, для распределения так мерить. Есть более натуральные физичные вещи, которые позволяют
[04:47.640 --> 04:52.640]  это делать. Ну и, соответственно, от евклидовости хочется как-то отходить, как-то, соответственно,
[04:52.640 --> 05:00.200]  переходить к произвольным нормам, которые могут возникнуть. Хочу измерять расстояние. Ну и,
[05:00.200 --> 05:07.280]  соответственно, этим мы сегодня и займемся. Я надеюсь, что на фонкане я вам дополню эту картинку и
[05:07.280 --> 05:12.640]  покажу действительно, что к чему у нас сопряжена норма. Оптимизационный вариант сопряженности
[05:12.640 --> 05:19.640]  фонкановский у вас схлопнутся, и картинка закроется. Сейчас это нужно просто для мотивации, просто
[05:19.640 --> 05:26.040]  потому что исторический метод, который мы будем обсуждать, был изобретен именно так. То есть мы
[05:26.040 --> 05:36.920]  поняли, что если у нас есть пространство, не обязательно, что градиент лежит в нём же. Ну,
[05:36.920 --> 05:41.680]  соответственно, что? Возникает следующее. Давайте тогда, раз x не лежит в этом Баннхам пространстве,
[05:41.680 --> 05:47.200]  я веду просто какое-то отображение, которое у меня в это Баннхова пространство эти x и будет
[05:47.200 --> 05:55.480]  переводить. Тогда теперь у меня x-катый лежит в Баннховом пространстве, e со звёздочкой,
[05:55.480 --> 06:06.280]  вот градиент тоже там, ну и, соответственно, получается, что всё тервалидно, но теперь я сделал
[06:06.280 --> 06:14.320]  как бы шаг градиентного спуска в другом Баннхам пространстве, так называемом зеркальном пространстве,
[06:14.320 --> 06:20.200]  вот поэтому как бы и методы будут называться зеркального спуска. И мне бы нужно вернуться в
[06:20.200 --> 06:26.000]  исходное. Понятно, что если я веду как-то это отображение довольно хорошо, то я, соответственно,
[06:26.000 --> 06:32.680]  по обратному отображению просто вернусь в исходный Баннхова пространстве и получу x-катая плюс 1. Вот вся
[06:32.680 --> 06:38.360]  идея, она такая вот, функа не стоит, довольно общая и выглядит довольно сложно, потому что метод,
[06:38.360 --> 06:46.240]  ну вот он так как-то записывается, ну вот если еще обратное отображение применю, но выглядит не
[06:46.240 --> 06:52.440]  user-friendly. Справедливости ради, ну вот результат получается конца 70-х годов, Аркадий Семёнович
[06:52.440 --> 07:00.240]  Немировский, Дмитрий Юдин, советские математики, вот, и в новых лекциях, которые вот Аркадий
[07:00.240 --> 07:06.000]  Семёнович уже считает в штатах, он, например, уже не рассказывает, ну я не видел в конспектах,
[07:06.000 --> 07:11.720]  может, на самих лекциях он это рассказывает, именно зеркальность свою, почему метод,
[07:11.720 --> 07:18.360]  который они придумали в конце 70-х, называется зеркальным спуском, они просто его вводят и все. А вот
[07:18.360 --> 07:25.280]  эту физику, которая у них шла на самом деле от Баннхова в пространстве, они, ну не рассказывают
[07:25.280 --> 07:30.720]  Аркадий Семёновичу, не знаю почему, просто может потому что довольно абстрактно. Ну вся книжка,
[07:30.720 --> 07:36.280]  классическая по оптимизации Немировского Юдина, она довольно жесткая из-за того,
[07:36.280 --> 07:43.560]  что получилось сильно абстрактно. Ну ладно, мы от абстракции отойдем и как раз пойдем к тому,
[07:43.560 --> 07:49.600]  как мы будем измерять расстояние, обоим измерять в следующем образом. Сначала давайте введем
[07:49.600 --> 07:57.240]  определение сильно выпуклости, и здесь ничего не меняется по сравнению с тем, что мы делали в
[07:57.240 --> 08:01.640]  Евкалида 2-м случае. Функция называется сильно выпуклой, только единственное, что здесь норма
[08:01.640 --> 08:09.000]  теперь стала произвольная. Раньше стояла 2, здесь теперь произвольная норма. Определение
[08:09.000 --> 08:16.360]  сильно выпуклости. Ну я напоминаю, что в курсе мы рассматриваем, что эти определения мы вводим
[08:16.360 --> 08:24.480]  только для каких-то выпуклых множеств. Хорошо, так, и теперь вот ключевой объект сегодняшней лекции,
[08:24.480 --> 08:32.200]  с которой мы на самом деле уже знакомились ранее. Здесь он будет определяться чуть-чуть по-другому.
[08:32.200 --> 08:38.000]  Это дивергенция Брегмана, которую мы с вами вводили для функций. Но теперь мы эту дивергенцию
[08:38.000 --> 08:43.560]  будем вводить также для функции некоторой D, которую только-только что мы определили,
[08:43.560 --> 08:49.040]  сильно выпуклой. Но теперь она будет нужна для измерения расстояния. До этого это нужно было
[08:49.040 --> 08:54.560]  скорее для измерения расстояния как вспомогательный критерий, а здесь это будет уже такой полноценный
[08:54.560 --> 09:02.680]  измеритель расстояния. Вот смотрите, соответственно, функция D у нас. Один сильно выпуклая.
[09:02.680 --> 09:12.120]  Дивергенция Брегмана вводится вот следующим образом. Ну вот такое вот расстояние. Пока вещь немного
[09:12.120 --> 09:19.680]  абстрактная, на примерах посмотрим, что действительно имеет место. Хорошо. На самом деле
[09:19.680 --> 09:24.080]  определение дивергенции Брегмана можно давать по-разному. Нам нужно будет сегодня именно один
[09:24.080 --> 09:29.240]  сильно выпуклая для одной сильно выпуклой функции. Как мы помним, раньше мы давали это определение
[09:29.240 --> 09:37.200]  просто для выпуклой функции. Никто не запрещает. Поэтому так. Давайте посмотрим на примерчиках.
[09:37.200 --> 09:44.120]  Вот моя дивергенция Брегмана. Давайте я ее попытаюсь породить функцией D, которая равна,
[09:44.120 --> 09:52.120]  я включаю норме в квадрате пополам. Какую дивергенцию Брегмана при этом мы породим? У кого-то,
[09:52.120 --> 10:05.400]  может быть, сразу есть ответ. Ну давайте еще по определению сделаем. Косинус. Ну давайте посмотрим.
[10:05.400 --> 10:13.200]  Не совсем косинус получится. Давайте смотреть. По определению пишу. Просто ничего сложного.
[10:13.200 --> 10:22.680]  Градиент это будет просто y, а здесь будет x-y. Вот так вот. Давайте одну вторую скобочки вынесу,
[10:22.680 --> 10:35.480]  еще у меня останется x-квадрат. Вот только y-квадрат. И здесь что у меня? y-x удвоенное,
[10:35.480 --> 10:47.880]  и здесь у меня плюс удвоенный y-квадрате. Вот что получается. Одна вторая x-квадрате.
[10:47.880 --> 11:00.960]  Так. Запись остановилась, что ли? Нет, вроде идет. Вот. Ну что это? Что получилось?
[11:00.960 --> 11:05.240]  Нарбораздности? Конечно.
[11:08.080 --> 11:14.160]  То есть, по факту, дивергенция Брегмана в евклидовом случае это просто евклидовое расстояние.
[11:14.160 --> 11:23.440]  Ну с одной и второй дополнительной. Вот. Окей. Да, соответственно, порождает нам евклидовое расстояние в квадрате.
[11:23.440 --> 11:32.000]  Вот. Более такой, что ли, специфичный пример. В первую очередь, на самом деле его называют таким вот
[11:32.000 --> 11:38.720]  ключевым примером использования дивергенции Брегмана. Это вот, соответственно, энтропийная
[11:38.720 --> 11:45.680]  функция, которую мы рассматриваем на вероятностном симплексе. Вот. На вероятностном симплексе.
[11:45.680 --> 11:51.040]  Вот. Я не знаю, может быть кто-то уже знает. Что может породить энтропийная функция?
[11:51.040 --> 11:59.480]  С чем она связана обычно? В теории информации возникает распределение. Ну симплекс – это как раз
[11:59.480 --> 12:07.640]  распределение вероятностного. Вот. Энтропийная функция у нас порождает дивергенцию кульбакалебдера. Вот.
[12:07.640 --> 12:14.920]  Такую прям классическую вещь, с помощью которой измеряют расстояние между двумя распределениями.
[12:14.920 --> 12:22.080]  Это ровно о том, что я говорил. О том, что, в принципе, по евклидовой норме измерять расстояние между
[12:22.080 --> 12:27.440]  распределениями как-то неестественно. Вот KL-дивергенция считается действительно
[12:27.440 --> 12:34.040]  таким классическим вариантом измерения расстояния между двумя распределениями. Вот. И оказывается,
[12:34.040 --> 12:43.600]  здесь все действительно корректно, потому что неравенство Пинскера гарантирует, что энтропийная
[12:43.600 --> 12:52.920]  функция будет 1 сильно выпукло и на симплексе относительно первой нормы. Вот. Поэтому действительно
[12:52.920 --> 12:58.080]  такой D мы можем порождать дивергенцию Брегмана, которую мы сейчас рассуждаем. Ну неравенство Пинскера
[12:58.080 --> 13:03.040]  можно найти в интернете. Оно ровно об этом и говорит, что энтропийная функция на симплексе 1 сильно выпукло.
[13:03.040 --> 13:12.680]  Вот. Хорошо. Здесь еще более специфичные примеры, какие можно порождать дивергенция Брегмана. Ну вот,
[13:12.680 --> 13:18.960]  например, третий примерчик. Это в некотором смысле обобщение предыдущего. До этого у нас как бы был
[13:18.960 --> 13:23.800]  вектор распределений. Здесь насматривается распределение. Ну это из-за того, что там клантовые
[13:23.800 --> 13:30.800]  вещи рассматриваются. Вот. И вот так вот это все безобразие обобщается. Вот. Это мы рассматривать
[13:30.800 --> 13:35.440]  это особо сильно, конечно, не будем. Вот. Вторую будем хорошо сегодня рассматривать. Да. Два остальных
[13:35.440 --> 13:41.080]  примерчика. Ну, чтобы просто посмотреть, что действительно есть какие-то красивые вещи, которые
[13:41.080 --> 13:47.600]  это все зашиты, в том числе вот такие какие-то там сильные модные молодежные, типа клантовые. Вот.
[13:47.600 --> 13:54.280]  Ну и называется прикольно. Клантовая дивергенция фонеймана, как в мультфильмах. Гипер-дупер,
[13:54.280 --> 14:04.120]  не знаю, там галактика зеленого, ну что-то такое. Вот. Ладно. Окей. Давайте по свойствам пробежимся,
[14:04.120 --> 14:11.600]  какие есть вообще у дивергенции Брегмана. Одно сразу же очевидно, это симметричность. Когда мы
[14:11.600 --> 14:23.080]  смотрим на KL-дивергенцию, тут видно, что x и y между собой явно находятся в неравном положении,
[14:23.080 --> 14:29.960]  потому что симметричности явно относительно x и y нет. Если поменяем местами, это станет
[14:29.960 --> 14:34.920]  абсолютно другое расстояние. Вот. Дальше есть, соответственно, сильная выпуклась,
[14:34.920 --> 14:42.920]  которую мы предположили для D, и она нам дает довольно хорошее свойство. Вот такое вот. То,
[14:42.920 --> 14:51.160]  что у нас дивергенция Брегмана не просто положительная, не отрицательная. Вот. Она еще и ограничивается
[14:51.160 --> 14:57.840]  снизу одной-второй x-y в квадрате, причем здесь норма, опять же, не эвклидовая, а произвольная. Но
[14:57.840 --> 15:02.960]  это следует ровно из определения сильной выпуклости, потому что, если мы глянем на
[15:02.960 --> 15:09.240]  определение дивергенции Брегмана, вот здесь выражение написано, если мы перелеснем на предыдущие
[15:09.240 --> 15:15.920]  и перенесем вот эти два члена вправо, то как раз мы получим, что дивергенция Брегмана больше
[15:15.920 --> 15:22.880]  либо равна, чем, соответственно, mu пополам x-y в квадрате, но mu у нас равно единице. Просто потому,
[15:22.880 --> 15:27.840]  так как мы взяли один сильно выпуклый фонус. Вот. Тоже важное свойство, которое мы сегодня будем
[15:27.840 --> 15:38.120]  использовать. Дальше, что не отрицательность, уже обсудили. Вот. И довольно такое неочевидное и,
[15:38.120 --> 15:42.760]  ну, в некотором смысле неприятное свойство, хотя с которым не особо нам придется взаимодействовать.
[15:42.760 --> 15:48.640]  Вот. То, что на самом деле дивергенция Брегмана, в общем, в случае не является выпуклой по второму
[15:48.640 --> 15:53.440]  аргументу. Там для Евклидова норма может быть все и хорошо, но в общем случае ничего хорошего нет.
[15:53.440 --> 15:59.040]  Вот. То есть, ну вот такое неочевидное. С одной стороны, именно с теоретической точки зрения,
[15:59.040 --> 16:09.320]  расстояние. Способ измерения расстояния. Вот. Не симметричный, еще и не выпуклый. Вот. Но при
[16:09.320 --> 16:15.360]  этом довольно физичный. Довольно физичный для некоторых частных случаев. Сегодня, соответственно,
[16:15.360 --> 16:24.760]  с этим всем безобразием и взаимодействием. А симметричность, это только для Каэль дивергенции или для всех?
[16:24.760 --> 16:33.120]  Нет. Ну, смотрите, опять же, для Евклидовой ее нет. Ой, она есть. Вот. А для Каэль уже нет. Ну вот,
[16:33.120 --> 16:40.320]  если мы глянем, например, на те примеры, которые там есть еще. Например, сюда. Вот здесь тоже нету.
[16:40.320 --> 16:48.960]  В третьем случае тоже нет. Ну, потому что это вообще не Каэль. В четвертом случае тоже нет. То есть,
[16:48.960 --> 16:57.200]  в общем случае симметричности тут нет. Ну, это, на самом деле, кажется, не особо страшно. Потому
[16:57.200 --> 17:05.280]  что, да, каких-то хороших свойств нету. Но при этом есть и физичность, которая на самом деле много
[17:05.280 --> 17:12.520]  будет чего нам вытягивать. Вот. Понадобится нам следующее свойство для дивергенции Брегмана. Это
[17:12.520 --> 17:19.080]  так называемое равенство параллелограмма или теорем Пифагора тоже в литературе ее называют для
[17:19.080 --> 17:24.960]  дивергенции Брегмана. Немироски, например, это называют просто magic property. Брегман divergence.
[17:24.960 --> 17:31.240]  Ну, как угодно это можно называть. На самом деле, это довольно простое свойство. Мы его сейчас с вами
[17:31.240 --> 17:39.600]  спокойненько и докажем, просто используя определение дивергенции Брегмана. Вот. Я туда же не особо буду
[17:39.600 --> 17:47.400]  сам что-то писать, хотя ладно, можно как писать. Первая строчка. Просто определение дивергенции
[17:47.400 --> 17:56.400]  Брегмана. Первый. Вот. Подставляю точки. Просто dz, dx. Ну и, соответственно, разница. dx умножить на
[17:56.400 --> 18:05.200]  градиенты dx. Вторая строчка. Вторая строчка. Определение v, x, y. Дивергенция Брегмана в точках x,
[18:05.200 --> 18:12.960]  y. Вот. Тоже все понятно, все четко. Выписано определение. Дальше с ними делать небольшая алгебра.
[18:12.960 --> 18:19.040]  Что тут, соответственно, что-то можно поуничтожать. Например, увидеть, что x тут одинаковые. Вот. Дальше
[18:19.040 --> 18:29.840]  что. Дальше я выделю вот здесь вот из y. Добавлю сюда умный 0 минус z плюс z. Вытащу вот этот кусочек.
[18:29.840 --> 18:34.840]  Вот у меня он здесь вытащился. Вот. Ну и там в силу того, что теперь вот эти вещи схлопнутся,
[18:34.840 --> 18:42.840]  потому что здесь x минус z, а там z минус x. Вот. Здесь возникнет вот такое вот. Такое вот скалярное
[18:42.840 --> 18:51.000]  произведение. Ну это ровно то, что нам нужно, потому что вот в этой строчке написано дивергенция
[18:51.000 --> 19:00.640]  Брегмана z, y. Вот. Минус то скалярное произведение, которое у нас и было в условиях свойств. Вот.
[19:00.640 --> 19:06.680]  Окей. Такое довольно простое свойство, которое нам нужно потом будет использовать при доказательстве.
[19:06.680 --> 19:15.480]  Хорошо. Вот. Теперь как раз переходим к методу, которая называется методом зеркального спусков. Вот.
[19:15.480 --> 19:21.720]  Решаем мы с вами задачу опять же. Безусловная оптимизация. Целева функция выпукла. Множество,
[19:21.720 --> 19:28.320]  на которое мы оптимизируем, выпукло. Вот. Здесь как бы все ограничения, которые есть, они вносятся во множество x.
[19:28.320 --> 19:39.200]  Соответственно, на слайде написана итерация метода зеркального спуска. То есть выглядит довольно
[19:39.200 --> 19:45.360]  страшновато, потому что мы понимаем, что дивергенция вещь не самая простая. Вот. И как вот этот
[19:45.360 --> 19:51.320]  аргуменимум считать по факту, на что вы нуждают делать. На каждую итерацию говорят, ну давай-ка вот
[19:51.320 --> 19:58.640]  ты посчитаешь градиент и потом отрешаешь этот аргуменимум. Который пока непонятно, как решать,
[19:58.640 --> 20:04.200]  на самом деле, потому что это же дополнительная задача оптимизации. Вот. Которую, ну в общем случае,
[20:04.200 --> 20:08.880]  видимо, придется решать каким-то численным методом. Вот. Который будет вносить дополнительную
[20:08.880 --> 20:14.960]  неточность. Но это кажется сложным. То есть вроде как выглядит итерация просто, в одну строчку записали,
[20:14.960 --> 20:23.360]  все и радуются. Вот. Но аргуменимум вещь не самая очевидная. Вот. Мы прекрасно знаем, что часто этот
[20:23.360 --> 20:28.240]  аргуменимум в каких-то хороших частных случаях может расписаться аналитически. И так, на самом деле,
[20:28.240 --> 20:34.400]  сейчас и будет, но на первый взгляд довольно сложно. Вот. Давайте посмотрим, что же будет, когда у нас...
[20:34.400 --> 20:46.480]  Мы породим нашу дивергенцию Брегмана. Евклидова норма в квадрате пополам. Вот. Кто-то может сразу
[20:46.480 --> 20:57.760]  предугадать, что может получиться в данном случае. Здесь я сразу напишу то, что у нас. Мы знаем,
[20:57.760 --> 21:04.720]  что у нас дивергенция Брегмана. Это просто Евклидовар в стене в квадрате. Так. Ну что, давайте посмотрим,
[21:04.720 --> 21:10.600]  что там получается. Тут на самом деле не так все сложно. Посмотрим на скалярное произведение. Так
[21:10.600 --> 21:17.640]  как у меня аргуменимум. Так как у меня аргуменимум. Вот. Я двоечку вот здесь домножу на двоечку и то и другое.
[21:17.640 --> 21:23.840]  Домножаю на двоечку и здесь, соответственно, пропадает. Там двоечка появляется. Получается вот как-то вот так.
[21:23.840 --> 21:38.600]  Так. Х. И здесь у меня х-хк в квадрате. Вот. Чем вообще всегда хороший аргуменимум,
[21:38.600 --> 21:45.520]  это то, что мы туда можем добавлять много чего, что не зависит от х. Вот. Ну я здесь давайте добавлю
[21:45.520 --> 21:54.520]  вот еще вот такое вот выражение. Так. Оно от ха не зависит, поэтому на аргуменимум не влияет,
[21:54.520 --> 21:59.680]  потому что аргуменимум просто возвращает значение оптимальное. Ну х, на котором достигается
[21:59.680 --> 22:05.240]  оптимальное значение, но не значение функции. Да, на функцию это влияет. Вот. На что похоже? На что
[22:05.240 --> 22:15.560]  похоже становится? Вот это выражение. Квадрат суммы. Квадрат суммы. Да. То есть чего-то все равно не
[22:15.560 --> 22:27.080]  хватает чуть-чуть. Вот. И это чуть-чуть я добавлю. Опять же. Потому что оно будет не зависеть от х. Вот это
[22:27.080 --> 22:37.480]  я добавлю. Вот так вот я это добавлю. Опять же на аргуменимум это все не влияет безобразие. Вот. И тогда
[22:37.480 --> 22:42.760]  что у меня здесь получится? Кто-то может проверить у меня, что здесь все получается ровно вот так.
[22:42.760 --> 22:58.720]  Вот. Согласны? Вот. То есть тут вот как раз у меня вылезет х ката минус х. Вот. Норма в
[22:58.720 --> 23:03.960]  квадрате, норма в квадрате. Все. Это так четко получается. Вот. Поэтому вот этот аргуменим,
[23:03.960 --> 23:11.880]  эквивалент на аргуменимуму х на множестве х, где мы соответственно делаем вот что-то вот такое.
[23:11.880 --> 23:28.800]  Ну я пока дописываю какой-то метод быстренько говорим. Так. Какой? Простой вроде вопрос. Обычный
[23:28.800 --> 23:35.680]  градиентный спуск что ли? Да. Это обычный градиентный спуск только с чем? С проекцией. Так?
[23:35.680 --> 23:45.880]  С проекцией. То есть видно что происходит. Мы взяли х. Сделали шаг. Так. Ну например посчитали
[23:45.880 --> 23:56.720]  какую-то точку у. А дальше что? Эту точку у мы спроекцировали. Все. Градиентный спуск
[23:56.720 --> 24:04.120]  с проекцией. Если бы соответственно у вас бы х равнялся просто всему р. Вот. Тогда у вас было бы
[24:04.120 --> 24:10.120]  просто вот. Градиентный спуск. То есть на самом деле вот то, что здесь написано, в Евклидовом случае
[24:10.120 --> 24:14.000]  это просто обычный градиентный спуск с проекцией или нет с Евклидовым. Ну понятно с Евклидовой
[24:14.000 --> 24:20.880]  проекцией. Вот. Или без нее, когда у вас соответственно решается задача безусловная. Ну в любом случае.
[24:20.880 --> 24:29.000]  Получается, что ведение диригенция Брегмана просто обобщает нам результаты нашего градиентного
[24:29.000 --> 24:36.880]  спуска, с которым мы с вами уже неплохо так повзаимодействовали. В том числе когда мы с проекцией
[24:36.880 --> 24:45.440]  делали. Хорошо. Вот. Но над чем хочется подумать еще? Это конечно здорово, что мы опять пришли
[24:45.440 --> 24:53.440]  градиентно на спуску, но вроде как мы и не за ним приходили, потому что с ним мы уже как раз более
[24:53.440 --> 25:02.240]  чем разобрались. Вот. Хочется подумать над тем, что возникает вот в диригенции Брегмана. То есть
[25:02.240 --> 25:09.720]  что она даст в общем случае. А можете потом тогда скинуть еще презентацию с вот этими вашими записями?
[25:09.720 --> 25:19.320]  Да, хорошо, хорошо. Спасибо. В принципе, записи-то они во многом дублируются на слайде. Вот. Вот.
[25:19.320 --> 25:24.520]  Тут вот то, что мы сводили вот этому спуску, оно вроде не дублируется или там дальше будет? Ну хорошо.
[25:24.520 --> 25:30.880]  Без проблем. Без проблем. Так. Ладно. Давайте в общем случае, это диригенция Брегмана, это же не
[25:30.880 --> 25:44.960]  просто дефигевое расстояние, а что-то более хитрое. Вот. Ну давайте выписывать. Вот. И вообще с точки
[25:44.960 --> 25:52.240]  зрения аргуминиума, как раз диригенция расписала, с точки зрения аргуминиума тут можно оставить только
[25:52.240 --> 26:06.240]  dx-градиент dxk на x. Вот. Ну и давайте я возьму и скажу, что у меня x это просто все пространство
[26:06.240 --> 26:14.320]  Rd. Вот. Я вот здесь вот его замажу на Rd. Вот. Тогда этот аргуминиум можно найти просто из самого
[26:14.320 --> 26:24.000]  простейшего условия оптимальности. Градиент функции, у которой мы ищем минимум равен нулю. Условия оптимальности
[26:24.000 --> 26:29.760]  супер простые. Ну то есть там понятно, мы знаем и для более сложных вариантов мы здесь. Я вот просто
[26:29.760 --> 26:39.120]  запишу вот это. Условия оптимальности. Так. Градиент d в точке x со звездой, ну x со звездой имеется вот как
[26:39.120 --> 26:49.160]  раз веду. У нас потом это будет подставлен как xk, а то плюс 1. Вот. Минус градиент dxk. Это все равно нулю. Вот.
[26:49.160 --> 26:59.760]  Ну вот я здесь это подставлю. Ровно это же выражение получилось. Вот. Ну что? Есть ли какие-то в некотором
[26:59.760 --> 27:04.480]  смысле флешбеки с первых слайдов? А они на самом деле здесь есть, потому что я вот чуть-чуть
[27:04.480 --> 27:13.560]  переписал и получилось вот так. То есть тот зеркальный шаг градиентного спуска, который предлагали
[27:13.560 --> 27:21.600]  Немировский Юдин, где мы просто вводили вот эту функцию phi, которая у нас отображала x в сопряженное
[27:21.600 --> 27:31.680]  пространство. Вот. И получался зеркальный спуск. Вот она та же самая идея, только здесь вот,
[27:31.680 --> 27:38.440]  когда мы ввели конкретную дивергенцию Брегмана, у нас соответственно что появилось? У нас четко
[27:38.440 --> 27:44.840]  появилось что такое phi. Вот. И в данном случае phi это просто вот градиент вот этой порождающей
[27:44.840 --> 27:51.040]  функции дивергенции Брегмана. Вот. Ну и здесь те же самые идеи, которые мы в принципе уже
[27:51.040 --> 28:01.560]  обсудили выше, излагаются про то, что у нас соответственно это отображение, что здесь соответственно
[28:01.560 --> 28:08.480]  там сначала отображаем x в сопряженное, делаем шаг, получаем вектор сопряженного и обратно
[28:08.480 --> 28:14.160]  возвращаемся с помощью обратного отображения. Вот. На самом деле вот опять же это в некотором смысле что-то
[28:14.160 --> 28:20.360]  общее. В жизни все будет проще, потому что вот тот аргминиум, который нам нужно отрешивать,
[28:20.360 --> 28:24.960]  он либо имеет аналитическое решение, сегодня мы только с такими будем взаимодействовать,
[28:24.960 --> 28:30.880]  ну с одним мы уже взаимодействовали и получили метод проекции градиента. Вот. Либо соответственно
[28:30.880 --> 28:36.520]  отрешивается это методом оптимизации, ну можно бы с хорошей точностью отрешать просто этот
[28:36.520 --> 28:41.520]  аргминиум и считать что ошибка там, она не попится. Ну либо учесть в ошибку. Вот. Ну действительно,
[28:41.520 --> 28:47.440]  если там точность решения хорошая, то ошибка не внесет большого вклада, чтобы аргминиум отрешивать
[28:47.440 --> 28:55.080]  не точно. Вот. Хорошо. Переходим как раз к доказательству сходимости метода зеркального
[28:55.080 --> 29:01.440]  спуска в произвольном случае. Вот. Для этого нам понадобится гладкость. Ну гладкость, ну мы от нее
[29:01.440 --> 29:07.000]  еще ни разу не отказывались. Вот. Но здесь, как вы понимаете, в силу того, что мы теперь измеряем
[29:07.000 --> 29:17.280]  расстояние не в евклидовой норме, гладкость тоже должна перетерпеть свои изменения. Вот. И в силу того,
[29:17.280 --> 29:23.600]  что у нас градиент теперь живет в сопряженном пространстве, то он измеряется в сопряженном
[29:23.600 --> 29:31.320]  пространстве расстояния уже в сопряженной норме. В сопряженной норме, причем как я и говорил,
[29:31.320 --> 29:36.480]  это действительно те сопряженные нормы, которые вы там рассматривали на семинарах, когда вы говорили
[29:36.480 --> 29:42.720]  про сопряженные по фентилю функции. Вот. В этом плане, я надеюсь, на функционе эта картинка реально
[29:42.720 --> 29:48.200]  схлопнется до конца. Вот. Поэтому, смотрите, когда мы рассматриваем, здесь просто вот было до этого 2,2,
[29:48.200 --> 29:55.680]  теперь стало норма со звездочкой, в ней мы измеряем градиент, расстояние между градиентами. Вот. И,
[29:55.680 --> 30:02.960]  соответственно, здесь норма P, где мы измеряем расстояние между ексами. Ну вот такое вот общение.
[30:02.960 --> 30:10.760]  Как вы понимаете, уже обсуждали, что вторая норма сопряженная, она же сама, поэтому в Euclidean случится
[30:10.760 --> 30:25.120]  здесь тоже все прекрасно работает. Окей. Свойства, которые мы с вами доказывали для гладких функций,
[30:25.120 --> 30:34.360]  для гладких функций. Ну давайте я его в некотором смысле проскочу, потому что мы его уже делали,
[30:34.360 --> 30:39.640]  то есть что там делалось. Мне формула Newton-Levnitsa записывалась, потому что сейчас делается ровно
[30:39.640 --> 30:46.960]  те же самые шаги. Дальше в интеграл заносился, потому что не зависит от tau, и, соответственно,
[30:46.960 --> 30:52.760]  вычитался вот это скалярное произведение, вот оно здесь появилось и здесь в двух местах с разными
[30:52.760 --> 31:01.600]  знаками просто лумный ноль добавляется. Вот. Дальше что делалось? Дальше вставился модуль справа и
[31:01.600 --> 31:07.560]  слева, просто потому что нам нужно вот это выражение рассмотреть по модулю, вставился модуль. Дальше
[31:07.560 --> 31:16.320]  в силу того, что модуль суммы меньше суммы модулей, модуль заносился под знак интеграла. Вот. А дальше
[31:16.320 --> 31:21.520]  единственное отличие, которое нам нужно будет сделать по сравнению с тем, что у нас было,
[31:21.520 --> 31:28.240]  это использовать обобщенный вариант Коши-Буниковского-Шварца. То есть мы с вами работали,
[31:28.240 --> 31:37.760]  что x, y меньше либо равен x и в клидовую норму y и в клидовую норму. Вот. Можно, соответственно,
[31:37.760 --> 31:44.000]  обобщить этот вариант на то, что у вас для одного из векторов норма p, для второго нормы,
[31:44.000 --> 31:50.040]  соответственно, q, которая сопряжена с p. Ну такой вариант, соответственно, вариант Коши-Буниковского-Шварца,
[31:50.040 --> 31:56.880]  разницу градиентов, поэтому этот модуль, который у меня здесь, я расписываю как разница градиентов в
[31:56.880 --> 32:05.000]  норме со звездой. Ну и соответственно, разница по аргументам, это просто по этой норме. А дальше,
[32:05.000 --> 32:11.000]  соответственно, что? Я могу использовать спокойно мое определение теперегладкости, потому что оно
[32:11.000 --> 32:21.680]  мне как раз дано было, когда разность градиентов учислялась по сопряженной норме. Вот. Я пользуюсь
[32:21.680 --> 32:30.360]  гладкостью и соответственно, что у меня убилось? Убилось здесь как раз у меня возник y-x в квадрат,
[32:30.360 --> 32:38.640]  просто умножено tau, как раз в норме p. Она вылезла у меня. Вот. А интеграл потом берется и вылезает
[32:38.640 --> 32:49.840]  одна вторая. Получается ровно то же самое свойство, которое у нас было в Евклидном случае. То есть
[32:49.840 --> 32:56.160]  просто обобщение, опять же, того предыдущего свойства, которое мы раньше с вами рассматривали. Вот. А я,
[32:56.160 --> 33:02.640]  кстати, задам такой вопрос, может быть, кто в курсе, было ли у вас это вообще или нет. Как
[33:02.640 --> 33:07.560]  вообще между собой соотносится норма? Ну вот, например, есть у меня первая норма, а есть вторая.
[33:07.560 --> 33:16.840]  Евклидова. Что я могу сказать? Я измеряю x. Какую-то норму x. Какой-то знак я могу здесь поставить.
[33:16.840 --> 33:34.840]  Вообще могу или нет? Не было, да? Тоже не было. Ну ладно, дойдем. Никакой знак тут не поставить,
[33:34.840 --> 33:40.120]  потому что если взять параболу и модуль, то в каком-то месте будет одно выше, а в каком-то
[33:40.120 --> 33:48.880]  другое. Или я фигню говорю. Ну подумайте, подумайте. На самом деле, знак тут четко ставится.
[33:48.880 --> 33:57.760]  Назвала, что или пространства вложены в друг друга. Вот, это, кстати, хорошо. На самом деле,
[33:57.760 --> 34:01.960]  в конечном мерном случае, мы, конечно, знаем, что нормы эквивалентны. То есть можно использовать
[34:01.960 --> 34:09.520]  любую норму, а другая и снизу и сверху. Например, норма какая-то P, она, например, с двух сторон,
[34:09.520 --> 34:19.560]  просто важна константа. Ограничиваться, например, какой-то нормой. Давайте буковка T, пусть будет T.
[34:19.560 --> 34:26.960]  Тут важны эти константы C1 и C2. А так, в принципе, в конечном мерном пространстве всегда выполнено
[34:26.960 --> 34:32.680]  вот такое для любых норм. В принципе, вы можете пользоваться одной, зная, что в другой тоже все
[34:32.680 --> 34:38.720]  будет хорошо, просто потому что выполнены такие свойства. Но, на самом деле, там все равно есть
[34:38.720 --> 34:46.920]  соотношение между нормами, в частности. Ну вот такое соотношение, что у вас чем больше, как называется,
[34:46.920 --> 34:54.600]  степень или порядок нормы. Как это было? Даже не знал, как это называется. Ну, порядок нормы.
[34:54.600 --> 35:04.800]  Ой, господи, что я написал? Тут неправильно. Вот так. Чем порядок нормы больше, тем, соответственно,
[35:04.800 --> 35:11.240]  она измеряет что-то более-менше. Ну, тут видно, на самом деле, когда норма бесконечности, это просто
[35:11.240 --> 35:18.880]  максимум среди X, модулей X. Если у вас вектор состоит из одинаковых, например, элементов,
[35:18.880 --> 35:26.040]  например, из единичек, то норма бесконечности вам вернет единичку, а норма, соответственно,
[35:26.040 --> 35:33.440]  вторая, вам вернет корень из D размерность вектора. У вас как раз будет сумма D и вы возьмете корень из них.
[35:33.440 --> 35:40.280]  Ну, то есть понятно, где корень из D раз больше. На самом деле, вот это все строго можно показать
[35:40.280 --> 35:52.040]  для произвольных норм, что если у вас там P меньше Q, то тогда у вас норма Pt больше либо равна
[35:52.040 --> 36:02.600]  нормы Qt. Ну ладно, это не вопрос сегодняшней лекции. Просто премиум это когда-нибудь.
[36:02.600 --> 36:11.560]  Вот, хорошо. Так, ну возвращаемся к доказательству. Разобрались, как гладкость меняется, какое
[36:11.560 --> 36:20.160]  свойство из нее вытекает. Нам бы нужно теперь разобраться с самим зеркальным спуском, потому что
[36:20.160 --> 36:27.240]  если мы, конечно, нам нужно использовать саму итерацию метку, чтобы что-то подоказывать. Давайте
[36:27.240 --> 36:33.200]  попробуем выписать условия оптимальности для вот этого арга минимума. Мы просто минимизируем
[36:33.200 --> 36:38.040]  какую-то функцию. Как будут выглядеть условия оптимальности в общем случае, если бы, например,
[36:38.040 --> 36:47.880]  здесь стояла какая-то функция g от x. Как бы мы выписали условия оптимальности? Здесь x.
[36:47.880 --> 37:00.480]  Вы производите набло нулевое. Набло нулевое? Серьезно? У нас множество x, которое выпуклое,
[37:00.480 --> 37:05.760]  и оно не обязательно Rd. Я здесь его положил Rd, просто чтобы была понятна связь с тем,
[37:05.760 --> 37:11.200]  что было у Немировского с Юдиным. А так это произвольное множество. Simplex, например,
[37:11.200 --> 37:30.240]  ограниченное множество. Как там будет? Договорились. Пусть будет так. В данном случае
[37:30.240 --> 37:39.040]  у нас их звездочка, она сразу перетекает в x ката плюс 1, и соответственно это все можно вот так
[37:39.040 --> 37:46.400]  записать. Здесь у нас соответственно x и с нашего множества x. Для любого x это должно выполняться.
[37:46.400 --> 37:53.560]  Вот, как-то так. Хорошо, отлично. Это мы разобрались. Условия оптимальности проходили,
[37:53.560 --> 38:00.040]  когда у нас множество, это произвольное. Я его здесь записал тоже. Вот, пожалуйста.
[38:00.040 --> 38:04.400]  Условия оптимальности.
[38:12.400 --> 38:18.840]  Так, и дальше. Здесь я просто подставил функцию g, потому что мы уже с вами поняли,
[38:18.840 --> 38:31.520]  что там под аргумением у нас стоит какая-то функция f от x ка x g от x равно плюс d от x
[38:31.520 --> 38:40.960]  минус nabla d x ката x. Ну и соответственно градиент g точки x ка плюс 1 у меня будет просто равен
[38:40.960 --> 38:54.360]  nabla f x ка плюс d x ка плюс 1 nabla минус nabla d x ка. Вот, он здесь в принципе и реализован.
[38:54.360 --> 39:14.240]  Вот этот вот. Хорошо, теперь смотрите, над чем поработаем. Так, вот с помощью нашего
[39:14.240 --> 39:21.760]  магического свойства про дивергенцию Брегмана попробуем разобраться вот с этим безобразием,
[39:21.760 --> 39:31.480]  которое у нас написано на слайд. То есть nabla d x ката плюс 1 минус nabla d x ка умножить на
[39:31.480 --> 39:43.920]  x ка плюс 1 минус x. Вот. Отдельно я могу его выписать. К сожалению, на слайде не поставил паузу.
[39:43.920 --> 40:05.120]  Давайте попробуем вот с этим всем разобраться, чему это будет соответственно равно. Откройте magic
[40:05.120 --> 40:24.720]  property на слайдах и посмотрите. Продиктуйте, чему это будет мне равно. Разность дивергенции Брегмана.
[40:24.720 --> 40:31.520]  Ну вот мне интересно каких. Там понятно, будут три дивергенции Брегмана. Вот. Мне интересно,
[40:31.520 --> 40:51.360]  чего они будут? Что там за аргументы-то будут?
[40:51.360 --> 41:18.200]  x ка плюс 1, x, x, x ка, x, x ка. Так, так будет, да? Наверное. Ну я не знаю, я вас слушаю. Ну я
[41:18.200 --> 41:28.480]  говорил x ка плюс 1, x. x ка плюс 1, x, по-моему. Ну есть у меня подозрения, что это не так. Ну давайте
[41:28.480 --> 41:47.440]  попробуем дальше. Дальше что? Плюс v от x. от x. x ка. x ка. Минус v от x ка плюс 1, x ка. Вот так вот,
[41:47.440 --> 41:57.240]  значит, получилось. Ну, точно со знаками нигде не напутали. Вот. На самом деле ответ похожий.
[41:57.240 --> 42:06.560]  Вот здесь он выписан. То есть вот это было правильно. Вот здесь надо было поменять местами. То есть
[42:06.560 --> 42:12.840]  аккуратненько просто это свойство расписать. Вот. Мы можем это сделать заново, чтобы было четко
[42:12.840 --> 42:19.920]  понятно, что мы здесь нигде не ошиблись. Вот. Что мне здесь соответственно? В качестве хочется
[42:19.920 --> 42:27.520]  подставить в качестве x, а x ка плюс 1, x ка плюс 1. В качестве z, x. В качестве y, x ка. Дальше,
[42:27.520 --> 42:38.920]  соответственно, что я должен выписывать? z, x. x. x ка плюс 1. Вот как раз. z, x. Выписал. Дальше,
[42:38.920 --> 42:52.960]  соответственно, x, y. x это сколько? x ка плюс 1, y, x ка. Ну и в последний z, y это x, x ка. Все. То есть,
[42:52.960 --> 43:00.520]  вроде, все четко. То, что мы сделали. Здесь все правильно. Так. Хорошо. Дальше, соответственно,
[43:00.520 --> 43:10.080]  взяли вот это, взяли гладкость. Взяли вот это, взяли гладкость. С предыдущего слайда гладкость тоже
[43:10.080 --> 43:16.440]  мы с вами доказали. Я здесь просто свойство гладкости подставил конкретные точки. Конкретные
[43:16.440 --> 43:23.760]  точки x ка плюс 1 и x ка. Вот. Дальше, соответственно, вот это все безобразие домножается на гамма.
[43:23.760 --> 43:29.000]  На гамма. Зачем это делается? Чтобы, соответственно, вот они, эти гаммы совпали
[43:29.000 --> 43:35.200]  здесь. Вот. То есть, все безобразие домножилось на гамма. И, что видно, хорошо будет получаться.
[43:35.200 --> 43:42.240]  Градиент здесь, видите, одинаковый. Градиент здесь одинаковый. И точечка даже одна одинаковая.
[43:42.240 --> 43:48.680]  Точечка даже одна одинаковая. Поэтому, соответственно, когда я вот сложу эти два
[43:48.680 --> 43:54.200]  скалярных произведения между собой, когда я сложу два этих скалярных произведений между собой,
[43:54.200 --> 44:01.880]  у меня что будет получаться? Гамма градиентов от x ка. Ну и здесь еще у меня, соответственно,
[44:01.880 --> 44:10.280]  минус. И это с плюсом, и это с минусом. Вот здесь он минус стоит. Вот. Что здесь тогда будет получаться?
[44:10.280 --> 44:19.960]  Здесь будет получаться. Х ка. Минус х. Вот оно.
[44:27.960 --> 44:38.040]  Так. Дальше, соответственно, вот это все перенесено просто из гладкости. Ну и дивергенс они тоже болтаются.
[44:38.040 --> 44:50.400]  Вот. Ну и это опять же тоже перемысл. Ничего тут сверхъестественного не происходит. Ну давайте
[44:50.400 --> 44:58.400]  тогда дальше листик добавлю и дальше ручками заведу. Хорошо. Здесь мы, соответственно, пришли
[44:58.400 --> 45:07.200]  вот к чему-то вот такому. Главное, чтобы он вот откопировал. Нет, только линию откопировал.
[45:07.200 --> 45:20.640]  Так. Ну давайте, ладно, здесь будем смотреть и доводить. Так. Справа, соответственно, у меня что
[45:20.640 --> 45:32.480]  останется? Справа у меня останется f x ка. x ка минус x гамма. Вот. А слева у меня будет гамма
[45:32.480 --> 45:49.520]  l пополам x минус x ка плюс один x ка в квадрате. Справа у меня там еще оставалось гамма f x ка плюс
[45:49.520 --> 46:01.000]  один минус f x ка. Плюс там, соответственно, была дивергенция Брегман. Если вправо перенести,
[46:01.000 --> 46:15.480]  будет x x ка минус v x x ка плюс один. Так. Окей. Давайте доведем. Что здесь, соответственно, будет? Во-первых,
[46:15.480 --> 46:32.960]  хочется воспользоваться выпуклостью для вот этой части f x ка x ка минус x минус минус f x ка. Как
[46:32.960 --> 46:52.200]  я это могу оценить по выпуклости? Как я это могу оценить по выпуклости? Кто-нибудь понимает,
[46:52.200 --> 46:59.960]  как можно это оценить по выпуклости? У нас как раз же выпуклость. Две точки. Ну, возможно, меньше
[46:59.960 --> 47:09.640]  либо равно минус f x. Все правильно. Да, выпуклость же всегда как можно представить? Вот у вас,
[47:09.640 --> 47:18.440]  соответственно, что у вас есть функция. Она подпирается. У вас есть какая-то текущая точка x ка.
[47:18.440 --> 47:27.080]  Дальше вы смотрите, что там происходит. Градиент в этой точке есть. И вы смотрите на какую-то точку x.
[47:27.080 --> 47:34.760]  Вот. Ну, здесь, соответственно, что у нас должно получиться? У нас должно
[47:35.760 --> 47:50.120]  f от x ка плюс x минус x ка. Оно как раз у нас меньше либо равно, чем f от x. Ну,
[47:50.120 --> 47:55.480]  мы, соответственно, домножаем на минус единицу, потому что здесь обратные вещи. Здесь, соответственно,
[47:55.480 --> 48:08.160]  остается то, что у нас как раз было наверху, и знак меняется. Вот. Отлично. Соответственно,
[48:08.160 --> 48:16.400]  да, это у нас в эту сторону. Поэтому вот то, что у нас было до этого, мы просто можем по выпуклости
[48:16.400 --> 48:29.520]  поменять. Здесь у нас, соответственно, что вместо скалярного произведения и вот этого f ки встает f ка в точке x.
[48:36.320 --> 48:44.200]  Хорошо. Хорошо. На самом деле уже очень близко, потому что мы когда-то... Что-то забыл. Забыл, забыл, забыл.
[48:44.200 --> 48:54.280]  Забыл. Я, соответственно... Там же у нас было три норм. Эти... Дивергенция еще была вот такая.
[48:54.280 --> 49:10.320]  Вот. Я ее подзабыл. Такая вида. Хорошо. Смотрите. А теперь давайте вспомним хорошее свойство
[49:10.320 --> 49:14.440]  Дивергенции Брегмана. Что мы про нее знаем? Как она подпирается снизу?
[49:20.920 --> 49:31.560]  Как, соответственно, разность х? Отлично. Тогда... Это же означает, что если знак просто поменяю,
[49:31.560 --> 49:41.520]  то у меня будут вот так вот. Вот. Я вот эту Дивергенцию Брегмана, соответственно, сейчас таким вот образом и оценю.
[49:41.520 --> 50:08.880]  Так. Дивергенцию Брегмана мне оценивается вот здесь вот. И можно заметить, что они вот как раз вот с этим будут схлопываться.
[50:08.880 --> 50:14.840]  Здесь она будет вылезать одна-вторая. Это с плюсом будет. Вот так.
[50:24.040 --> 50:28.080]  Так. Ну что? Что осталось-то, в принципе? Что осталось, в принципе?
[50:28.080 --> 50:37.840]  Когда мы доказывали, например, просто в выпуклом случае, что он считается то же самое, что r было.
[50:37.840 --> 50:48.240]  В дыхе тут на расстоянии k минус расстояние x в квадрате здесь тоже было. xk плюс 1 минус x в квадрате.
[50:48.240 --> 50:54.480]  Но это явно мешается. Ну, с помощью гаммы это все безобразие и убивается. Вот.
[50:54.480 --> 51:03.480]  Вот гамма берется меньше, чем один делить на l. Вот. И это все здесь уходит, просто становится меньше нуля.
[51:03.480 --> 51:09.040]  Вот. И остается что-то очень хорошее.
[51:09.040 --> 51:26.080]  Так. Ну и все. Дальше делается стандартный трюк. Суммируем, усредняем.
[51:26.080 --> 51:41.680]  Вот. Здесь, когда мы, соответственно, будем суммировать, все соседние будут уничтожаться между собой.
[51:41.680 --> 51:52.080]  Вот. И мы будем, и мы получим в итоге 1 делить на k vx x0 минус vx xk.
[51:52.080 --> 52:03.920]  Договорились? Договорились. Вот. Это у нас вещь не отрицательная, поэтому меньше либо равна нуля, нулему я можно и оценить и убрать.
[52:03.920 --> 52:13.280]  Хорошо. Чуть-чуть осталось. Для этого функция выпуклая. Применяем неравенство енсу.
[52:22.080 --> 52:35.600]  Меньше либо равно, а гамма еще забыл, vx x0. Только там под знаком суммы вроде как f не нужно. Ой, да-да-да, конечно-конечно.
[52:35.600 --> 52:50.000]  Все. То есть средняя точка. Здесь давайте вот так вот опозначу. Средняя точка минус f. Сразу звездочку можно поставить.
[52:50.880 --> 53:00.880]  Меньше либо равно, чем vx звездочка x0, гамма k. Можно поставить гамму как 1 делить на e или, соответственно, у вас сразу же.
[53:00.880 --> 53:10.880]  Результат. Все. Супер. Отличается от градиентного спуска или нет в выпуклом случае, кто помнит, кто читал пособия?
[53:20.880 --> 53:34.880]  Отличается или нет? Что мы в итоге получили? Мы оценили разность в функции v, ее раньше не было. В градиентном спуске вроде бы.
[53:34.880 --> 53:44.880]  В градиентном спуске здесь стояла просто норма в квадрате. А мы сейчас норму вводим как v или что?
[53:45.760 --> 53:57.760]  В это у нас суть семинара то, что мы как раз меняем измерение расстояния с нормой Евклидовой на v. Вот. И здесь, соответственно, оно и возникает.
[53:57.760 --> 54:03.760]  Я не успел. А когда мы сказали, что новая норма, это будет дивергенция Брегмана?
[54:03.760 --> 54:09.760]  Это не норма. Еще раз, там она не является нормой. Это просто некоторые измерительные расстояния.
[54:10.640 --> 54:16.640]  Для нее многие свойства норм не выполняются, в общем случае. Но как измерять расстояние, оно более чем.
[54:16.640 --> 54:24.640]  Это некоторый объект, у которого есть 2x, и мы можем измерять между ними расстояние с помощью дивергенции Брегмана.
[54:26.640 --> 54:28.640]  А метрика, она это является?
[54:28.640 --> 54:34.640]  Нет. Вот. Вот так вот.
[54:35.520 --> 54:43.520]  Соответственно, в некотором смысле проблемы, с другой стороны, как бы, часто здесь проблемы не видно,
[54:43.520 --> 54:51.520]  потому что физичность, которая есть в частных случаях некоторых, она закрывает все огрехи.
[54:51.520 --> 54:57.520]  И как раз именно с теоретической точки зрения о физичности результатов, дальше хочется поговорить,
[54:57.520 --> 55:01.520]  но давайте сделаем перерыв 5 минут. А дальше продолжим.
[55:02.400 --> 55:04.400]  Давайте продолжать.
[55:04.400 --> 55:10.400]  Теперь я предлагаю обсудить, зачем мы в это завязывались.
[55:10.400 --> 55:16.400]  Изначально обсуждали, что у этого всего есть геометрия задачи на другую,
[55:16.400 --> 55:20.400]  и хочется эту геометрию как-то учесть.
[55:20.400 --> 55:24.400]  А проявляем это как-то в теории. Давайте смотреть.
[55:24.400 --> 55:30.400]  Что мы получали, например, для градиентного спуска с евклидовой проекцией?
[55:31.280 --> 55:33.280]  Евклидовой проекции.
[55:33.280 --> 55:39.280]  Там мы, соответственно, получали, ну, даже можно отсюда это все выразить,
[55:39.280 --> 55:41.280]  потому что мы поняли, что это частный случай.
[55:41.280 --> 55:51.280]  l2 это константа гладкости, когда у нас евклидова норма в определении используется.
[55:51.280 --> 55:55.280]  Здесь, соответственно, у нас x0, x звездой в квадрате,
[55:55.280 --> 55:59.280]  евклидова норма делить на k и еще на 2.
[56:00.160 --> 56:02.160]  Хорошо.
[56:02.160 --> 56:06.160]  А сейчас, соответственно, вот у нас получилось l,
[56:06.160 --> 56:11.160]  дивергенция Брегмана, x звездой x0 делить на k.
[56:11.160 --> 56:15.160]  Я гамму поставил просто равным 1 делить на l.
[56:15.160 --> 56:20.160]  Вопрос. Лучше-хуже это получилось или нет? Или вообще разницы нет?
[56:22.160 --> 56:26.160]  Это вроде кашка одна и та же, то есть сублинейная сходимость.
[56:27.040 --> 56:29.040]  Но числитель разный.
[56:32.040 --> 56:35.040]  Но какой из них хуже, какой из них лучше? Какие мысли?
[56:43.040 --> 56:46.040]  Давайте просто посмотрим на вот это определение.
[56:46.920 --> 56:48.920]  Вот.
[56:48.920 --> 56:52.920]  И про то, что я уже в некотором смысле чуть-чуть затронул,
[56:52.920 --> 56:55.920]  то, что у нас норма...
[56:55.920 --> 56:57.920]  Давайте я вот так напишу.
[56:57.920 --> 57:00.920]  Это пусть будет qt-норма, это пусть будет pt-норма.
[57:00.920 --> 57:04.920]  Они связаны вот таким соотношением единицы.
[57:04.920 --> 57:07.920]  И давайте вообще первой возьмем равные единички,
[57:07.920 --> 57:09.920]  тогда, соответственно, у равного бесконечности.
[57:09.920 --> 57:11.920]  И пусть у нас будет 1,
[57:12.800 --> 57:15.800]  тогда, соответственно, у равного бесконечности.
[57:15.800 --> 57:17.800]  И пусть у нас вот здесь норма бесконечности,
[57:17.800 --> 57:20.800]  здесь стоит, соответственно, первая норма.
[57:20.800 --> 57:23.800]  До этого мы, в принципе, писали что-то очень похожее.
[57:23.800 --> 57:27.800]  Мы писали вот как-то так.
[57:27.800 --> 57:30.800]  Вторая норма l.
[57:30.800 --> 57:33.800]  Ну, тут разность х было тоже по второй норме.
[57:33.800 --> 57:37.800]  Вот. Как я уже сказал, в принципе,
[57:38.680 --> 57:41.680]  у нас норма бесконечность,
[57:41.680 --> 57:44.680]  она меньше второй нормы.
[57:44.680 --> 57:47.680]  Причем, уже обсудили, в худшем случае,
[57:47.680 --> 57:52.680]  там превосходить может в корень из n раз.
[57:52.680 --> 57:54.680]  В корень из d раз или d размер.
[57:54.680 --> 57:56.680]  То есть, когда норма бесконечности
[57:56.680 --> 57:58.680]  просто максимум по модулю,
[57:58.680 --> 58:00.680]  вторая норма это сумма квадратов,
[58:00.680 --> 58:02.680]  корень из суммы квадратов.
[58:02.680 --> 58:04.680]  Соответственно, здесь у вас просто возвращают
[58:04.680 --> 58:06.680]  максимальные элементы,
[58:06.680 --> 58:08.680]  одинаковые, например, единичка,
[58:08.680 --> 58:10.680]  а вторая норма вернет корень из d.
[58:10.680 --> 58:12.680]  Аналогично, на самом деле,
[58:12.680 --> 58:16.680]  вещь справедлива и для
[58:16.680 --> 58:18.680]  второй нормы и первой.
[58:18.680 --> 58:20.680]  Что понятно, вторая норма меньше,
[58:20.680 --> 58:22.680]  чем первая норма.
[58:22.680 --> 58:24.680]  Но опять же, смотрите, во второй норме,
[58:24.680 --> 58:26.680]  если мы возьмем вектор из одинаковых,
[58:26.680 --> 58:28.680]  из всех единичек, то он нам вернет
[58:28.680 --> 58:30.680]  корень из d, как раз на то, что мы обсудили.
[58:30.680 --> 58:32.680]  А первая норма это сумма модулей.
[58:32.680 --> 58:34.680]  Он нам вернет d.
[58:34.680 --> 58:36.680]  То есть, здесь и здесь,
[58:36.680 --> 58:38.680]  в лучшем,
[58:38.680 --> 58:40.680]  в крайнем случае,
[58:40.680 --> 58:42.680]  это разница в корне из d.
[58:42.680 --> 58:44.680]  Понятно, что на каких-то
[58:44.680 --> 58:46.680]  нулевых векторах это разницы
[58:46.680 --> 58:48.680]  между ними нет.
[58:48.680 --> 58:50.680]  То есть, с одной стороны,
[58:50.680 --> 58:52.680]  они одинаковые,
[58:52.680 --> 58:54.680]  то есть, коэффициент единичка,
[58:54.680 --> 58:56.680]  а в худшем случае в d раз
[58:56.680 --> 58:58.680]  больше правая будет.
[58:58.680 --> 59:00.680]  Поэтому, когда мы вот здесь
[59:00.680 --> 59:02.680]  пишем вот такое,
[59:02.680 --> 59:04.680]  кутэ норма,
[59:04.680 --> 59:06.680]  на данном случае мы уже рассматриваем
[59:06.680 --> 59:08.680]  бесконечность,
[59:08.680 --> 59:10.680]  а здесь стоит вторая норма.
[59:10.680 --> 59:12.680]  То есть, вот здесь слева стоит что-то больше.
[59:12.680 --> 59:14.680]  Потому что вот здесь вот
[59:14.680 --> 59:16.680]  q.
[59:16.680 --> 59:18.680]  Так?
[59:18.680 --> 59:20.680]  Хорошо.
[59:20.680 --> 59:22.680]  А здесь справа,
[59:22.680 --> 59:24.680]  смотрите, вторая норма,
[59:24.680 --> 59:26.680]  а здесь норма,
[59:26.680 --> 59:28.680]  тут один был,
[59:28.680 --> 59:30.680]  первая норма.
[59:30.680 --> 59:32.680]  Больше второй.
[59:32.680 --> 59:34.680]  То есть, что получается?
[59:34.680 --> 59:36.680]  Вот здесь, когда мы делаем вот это,
[59:36.680 --> 59:38.680]  мы какой-то,
[59:38.680 --> 59:40.680]  ну, давайте будем говорить,
[59:40.680 --> 59:42.680]  вторую норму, оцениваем вторую норму.
[59:42.680 --> 59:44.680]  А здесь мы что-то,
[59:44.680 --> 59:46.680]  в лучшем случае, значительно
[59:46.680 --> 59:48.680]  меньшее, чем вторая норма,
[59:48.680 --> 59:50.680]  оцениваем
[59:50.680 --> 59:52.680]  через что-то,
[59:52.680 --> 59:54.680]  что может быть значительно больше,
[59:54.680 --> 59:56.680]  чем вторая норма.
[59:56.680 --> 59:58.680]  Понятная идея, да?
[59:58.680 --> 01:00:00.680]  Санта L2
[01:00:00.680 --> 01:00:02.680]  может быть значительно больше,
[01:00:02.680 --> 01:00:04.680]  чем L.
[01:00:04.680 --> 01:00:06.680]  Угу.
[01:00:06.680 --> 01:00:08.680]  Понятно почему?
[01:00:10.680 --> 01:00:12.680]  Я надеюсь, что да.
[01:00:16.680 --> 01:00:18.680]  Хоть какой-то
[01:00:18.680 --> 01:00:20.680]  сигнал,
[01:00:20.680 --> 01:00:22.680]  что двигаться можно дальше.
[01:00:22.680 --> 01:00:24.680]  Вот.
[01:00:24.680 --> 01:00:26.680]  Хорошо.
[01:00:26.680 --> 01:00:28.680]  Смотрите, с другой стороны,
[01:00:28.680 --> 01:00:30.680]  есть дивергенция Брегман.
[01:00:30.680 --> 01:00:32.680]  То есть, мы поняли, что L нам идет в плюс.
[01:00:32.680 --> 01:00:34.680]  Вот L-ки нам идут в плюс.
[01:00:34.680 --> 01:00:36.680]  L у нас будет лучше.
[01:00:36.680 --> 01:00:38.680]  Давайте вот здесь сотру,
[01:00:38.680 --> 01:00:40.680]  здесь L-ка идет в минус, а там нам в плюс.
[01:00:40.680 --> 01:00:42.680]  С другой стороны, дивергенция Брегман.
[01:00:42.680 --> 01:00:44.680]  Мы про дивергенцию Брегмана знаем,
[01:00:44.680 --> 01:00:46.680]  что она,
[01:00:46.680 --> 01:00:48.680]  ну, звездочка,
[01:00:48.680 --> 01:00:50.680]  неважно, там Y.
[01:00:50.680 --> 01:00:52.680]  Одна вторая, X, Y.
[01:00:52.680 --> 01:00:54.680]  Причем не обязательно во второй норме.
[01:00:54.680 --> 01:00:56.680]  То есть, тут произвольна норма.
[01:00:56.680 --> 01:00:58.680]  Если мы, опять же, рассматриваем, например,
[01:00:58.680 --> 01:01:00.680]  П эту норму,
[01:01:00.680 --> 01:01:02.680]  вот, П равную единице,
[01:01:02.680 --> 01:01:04.680]  то вот это будет
[01:01:04.680 --> 01:01:06.680]  больше либо равно, чем
[01:01:06.680 --> 01:01:08.680]  Евклидова норма в квадрате.
[01:01:08.680 --> 01:01:10.680]  Вот. То есть, вот именно вот с этой точки
[01:01:10.680 --> 01:01:12.680]  зрения правой оценки
[01:01:12.680 --> 01:01:14.680]  дивергенция больше,
[01:01:14.680 --> 01:01:16.680]  чем Евклидова норма в квадрате.
[01:01:16.680 --> 01:01:18.680]  То есть, вот здесь вот это идет в минус,
[01:01:18.680 --> 01:01:20.680]  а уже норма будет в плюс.
[01:01:20.680 --> 01:01:22.680]  Понятная идея, да?
[01:01:22.680 --> 01:01:24.680]  Можно еще раз?
[01:01:24.680 --> 01:01:26.680]  Что мы вообще сейчас
[01:01:26.680 --> 01:01:28.680]  пытаемся сделать, чем мы занимаемся?
[01:01:28.680 --> 01:01:30.680]  Мы хотим понять,
[01:01:30.680 --> 01:01:32.680]  какой метод будет сходиться быстрее.
[01:01:32.680 --> 01:01:34.680]  Вот у нас есть оценка сходимости.
[01:01:34.680 --> 01:01:36.680]  Что нам нужно? Столько вот столько итераций.
[01:01:36.680 --> 01:01:38.680]  Вот. Вот она оценка.
[01:01:38.680 --> 01:01:40.680]  Ну, вот для нее я выписал там
[01:01:40.680 --> 01:01:42.680]  эту гамму подставил.
[01:01:42.680 --> 01:01:44.680]  А здесь, соответственно,
[01:01:44.680 --> 01:01:46.680]  нужна для градиентного спуска
[01:01:46.680 --> 01:01:48.680]  с проектов. Какая из них больше
[01:01:48.680 --> 01:01:50.680]  непонятна. Я хочу сравнить.
[01:01:50.680 --> 01:01:52.680]  Вот. Ну и сравниваем.
[01:01:52.680 --> 01:01:54.680]  Что мы именно сравниваем?
[01:01:54.680 --> 01:01:56.680]  Мы для разной гаммы поставляем
[01:01:56.680 --> 01:01:58.680]  и сравниваем?
[01:01:58.680 --> 01:02:00.680]  Нет, смотрите. У нас есть два частных случая.
[01:02:00.680 --> 01:02:02.680]  Ну, например,
[01:02:02.680 --> 01:02:04.680]  градиентный спуск с проекции,
[01:02:04.680 --> 01:02:06.680]  который на самом деле укладывается тоже
[01:02:06.680 --> 01:02:08.680]  в ней
[01:02:08.680 --> 01:02:10.680]  в сетап зеркального спуска.
[01:02:10.680 --> 01:02:12.680]  Но для него мы оценку
[01:02:12.680 --> 01:02:14.680]  и так получали вот.
[01:02:14.680 --> 01:02:16.680]  Вот. А есть произвольный случай,
[01:02:16.680 --> 01:02:18.680]  когда у нас там
[01:02:18.680 --> 01:02:20.680]  обязательно Евклидова норма
[01:02:20.680 --> 01:02:22.680]  порождает дивергенцию Брегмана.
[01:02:22.680 --> 01:02:24.680]  Вот получается такая скорость
[01:02:24.680 --> 01:02:26.680]  сходимости, такая гарантия сходимости.
[01:02:26.680 --> 01:02:28.680]  Я хочу понять, какая из этих оценок
[01:02:28.680 --> 01:02:30.680]  вообще лучше. Вот. Стоит ли
[01:02:30.680 --> 01:02:32.680]  вообще было связываться именно вот
[01:02:32.680 --> 01:02:34.680]  исходя из этих оценок
[01:02:34.680 --> 01:02:36.680]  с зеркальным спуском?
[01:02:36.680 --> 01:02:38.680]  Вот. Если просто право оценка
[01:02:38.680 --> 01:02:40.680]  всегда хуже, то зачем она нужна?
[01:02:40.680 --> 01:02:42.680]  Ну, именно вот с точки зрения теории.
[01:02:42.680 --> 01:02:44.680]  Вот.
[01:02:44.680 --> 01:02:46.680]  Ну и вроде как разобрались,
[01:02:46.680 --> 01:02:48.680]  что константа L в левой оценке
[01:02:48.680 --> 01:02:50.680]  хуже.
[01:02:50.680 --> 01:02:52.680]  Вот.
[01:02:52.680 --> 01:02:54.680]  И что измеряет расстояние,
[01:02:54.680 --> 01:02:56.680]  в левой оценке лучше.
[01:02:56.680 --> 01:02:58.680]  Ну вот и в этом игра.
[01:02:58.680 --> 01:03:00.680]  Ну и сейчас в частном случае
[01:03:00.680 --> 01:03:02.680]  увидим, что
[01:03:02.680 --> 01:03:04.680]  эту игру можно выиграть.
[01:03:04.680 --> 01:03:06.680]  И правая оценка будет лучше вот эта.
[01:03:06.680 --> 01:03:08.680]  То есть мы хотим получить такую же
[01:03:08.680 --> 01:03:10.680]  скорость сходимости либо лучше,
[01:03:10.680 --> 01:03:12.680]  чем у градиентного спуска?
[01:03:12.680 --> 01:03:14.680]  Конечно, конечно. Мы же зачем-то
[01:03:14.680 --> 01:03:16.680]  зачем-то это все это начали.
[01:03:16.680 --> 01:03:18.680]  Вот. Абстрактные слова про то,
[01:03:18.680 --> 01:03:20.680]  что да, мы учили геометрию задачи,
[01:03:20.680 --> 01:03:22.680]  это, конечно, хорошо. Вот. Но хотелось бы
[01:03:22.680 --> 01:03:24.680]  их увидеть в реальности.
[01:03:24.680 --> 01:03:26.680]  Ну точнее как в теории. В теорию, чтобы
[01:03:26.680 --> 01:03:28.680]  они как-то проявились. Вот.
[01:03:28.680 --> 01:03:30.680]  Тогда будет
[01:03:30.680 --> 01:03:32.680]  супер-хорошая мотивация
[01:03:32.680 --> 01:03:34.680]  это исследовать в реальности. Вот.
[01:03:34.680 --> 01:03:36.680]  Рыфро, а можно тогда их распечатать?
[01:03:36.680 --> 01:03:38.680]  Почему у нас справа константа L лучше,
[01:03:38.680 --> 01:03:40.680]  чем слева?
[01:03:40.680 --> 01:03:42.680]  Смотрите. Потому что
[01:03:42.680 --> 01:03:44.680]  вот норма бесконечность, вот здесь
[01:03:44.680 --> 01:03:46.680]  она меньше, чем
[01:03:46.680 --> 01:03:48.680]  вторая норма.
[01:03:48.680 --> 01:03:50.680]  Меньше либо она, чем вторая норма.
[01:03:50.680 --> 01:03:52.680]  То есть вот это что-то
[01:03:52.680 --> 01:03:54.680]  маленькое, а здесь
[01:03:54.680 --> 01:03:56.680]  это что-то большее.
[01:03:56.680 --> 01:03:58.680]  И наоборот справа стоит здесь
[01:03:58.680 --> 01:04:00.680]  что-то большое по сравнению вот с этим.
[01:04:00.680 --> 01:04:02.680]  То есть условно, давайте вот здесь, например,
[01:04:02.680 --> 01:04:04.680]  это
[01:04:04.680 --> 01:04:06.680]  1, это, например, пусть будет
[01:04:06.680 --> 01:04:08.680]  x.
[01:04:08.680 --> 01:04:10.680]  Ну пусть будет d. Давайте вообще
[01:04:10.680 --> 01:04:12.680]  напишу. 1 здесь будет d.
[01:04:12.680 --> 01:04:14.680]  Вот это, например, в d раз больше
[01:04:14.680 --> 01:04:16.680]  корень из d раз больше
[01:04:16.680 --> 01:04:18.680]  и это корень из d. Вот.
[01:04:18.680 --> 01:04:20.680]  Понятно, что вот здесь вот будет
[01:04:20.680 --> 01:04:22.680]  L2 равно единичке,
[01:04:22.680 --> 01:04:24.680]  вот.
[01:04:24.680 --> 01:04:26.680]  А здесь будет L равно 1 делить
[01:04:26.680 --> 01:04:28.680]  на d. Понятно?
[01:04:28.680 --> 01:04:30.680]  Примерная идея.
[01:04:30.680 --> 01:04:32.680]  То есть вот это что-то большое.
[01:04:32.680 --> 01:04:34.680]  То есть мы корень из d применяем норму
[01:04:34.680 --> 01:04:36.680]  и корень из d применяем норму.
[01:04:36.680 --> 01:04:38.680]  А слева мы почему-то сравним
[01:04:38.680 --> 01:04:40.680]  норму единицы и норму d?
[01:04:40.680 --> 01:04:42.680]  Ну, смотрите, давайте
[01:04:42.680 --> 01:04:44.680]  более формально. У нас есть
[01:04:44.680 --> 01:04:46.680]  что-то, давайте вот у нас есть
[01:04:46.680 --> 01:04:48.680]  A.
[01:04:48.680 --> 01:04:50.680]  Это норма бесконечности.
[01:04:50.680 --> 01:04:52.680]  Норма бесконечности.
[01:04:52.680 --> 01:04:54.680]  Здесь, соответственно, она меньше
[01:04:54.680 --> 01:04:56.680]  B, где B у нас, соответственно,
[01:04:56.680 --> 01:04:58.680]  норма
[01:04:58.680 --> 01:05:00.680]  Евклидова. Есть у нас
[01:05:00.680 --> 01:05:02.680]  C.
[01:05:02.680 --> 01:05:04.680]  Это норма 1.
[01:05:04.680 --> 01:05:06.680]  Вот так у нас все
[01:05:06.680 --> 01:05:08.680]  расположено.
[01:05:08.680 --> 01:05:10.680]  Ну и что мы, соответственно, делаем?
[01:05:10.680 --> 01:05:12.680]  Мы вот здесь, вот у разницы градиента,
[01:05:12.680 --> 01:05:14.680]  вычисляем норму бесконечности.
[01:05:14.680 --> 01:05:16.680]  То есть у какого-то...
[01:05:16.680 --> 01:05:18.680]  Давайте я лучше вот так напишу.
[01:05:18.680 --> 01:05:20.680]  Вот так. A-B это для градиентов.
[01:05:20.680 --> 01:05:22.680]  И A-B
[01:05:22.680 --> 01:05:24.680]  здесь B большое.
[01:05:24.680 --> 01:05:26.680]  Здесь у нас, соответственно, вторая
[01:05:26.680 --> 01:05:28.680]  норма, здесь первая.
[01:05:28.680 --> 01:05:30.680]  Вот.
[01:05:30.680 --> 01:05:32.680]  Как-то так.
[01:05:32.680 --> 01:05:34.680]  И что мы здесь, соответственно, оценим? У нас есть
[01:05:34.680 --> 01:05:36.680]  норма бесконечности, вот здесь.
[01:05:36.680 --> 01:05:38.680]  Норма бесконечности, которая A.
[01:05:38.680 --> 01:05:40.680]  Вот. Мы хотим вот так вот
[01:05:40.680 --> 01:05:42.680]  сделать через B.
[01:05:42.680 --> 01:05:44.680]  Так? Вот она, B.
[01:05:44.680 --> 01:05:46.680]  Вот она, A.
[01:05:46.680 --> 01:05:48.680]  Тут разность градиентов просто было стоять.
[01:05:48.680 --> 01:05:50.680]  Здесь, соответственно, разность х.
[01:05:50.680 --> 01:05:52.680]  То есть вот отсюда вытечет
[01:05:52.680 --> 01:05:54.680]  B меньше L2
[01:05:54.680 --> 01:05:56.680]  на A.
[01:05:56.680 --> 01:05:58.680]  Вот.
[01:05:58.680 --> 01:06:00.680]  Ну тогда почему, например, как можно, например,
[01:06:00.680 --> 01:06:02.680]  понять, какой порядок у L?
[01:06:02.680 --> 01:06:04.680]  Ну давайте A делить на B.
[01:06:04.680 --> 01:06:06.680]  Вот это отсюда.
[01:06:06.680 --> 01:06:08.680]  А вот отсюда будет
[01:06:08.680 --> 01:06:10.680]  у L2 порядок
[01:06:10.680 --> 01:06:12.680]  B делить на A.
[01:06:12.680 --> 01:06:14.680]  Вот. Ну и теперь смотрим, как они соотносятся.
[01:06:14.680 --> 01:06:16.680]  A меньше B.
[01:06:16.680 --> 01:06:18.680]  A меньше B.
[01:06:18.680 --> 01:06:20.680]  Вот. A. A большое
[01:06:20.680 --> 01:06:22.680]  меньше, чем B большое.
[01:06:22.680 --> 01:06:24.680]  Вот. Здесь, соответственно,
[01:06:24.680 --> 01:06:26.680]  на A маленькое мы умножаем.
[01:06:26.680 --> 01:06:28.680]  Соответственно, если мы умножаем на меньшее число,
[01:06:28.680 --> 01:06:30.680]  то становится меньше.
[01:06:30.680 --> 01:06:32.680]  Если мы делим на большее число,
[01:06:32.680 --> 01:06:34.680]  то становится тоже меньше.
[01:06:34.680 --> 01:06:36.680]  И получается, что L меньше,
[01:06:36.680 --> 01:06:38.680]  чем L2.
[01:06:38.680 --> 01:06:40.680]  Меньше либо ровно, чем L2.
[01:06:40.680 --> 01:06:42.680]  Понятно?
[01:06:42.680 --> 01:06:44.680]  Ну...
[01:06:44.680 --> 01:06:46.680]  Ну вообще, почему
[01:06:46.680 --> 01:06:48.680]  L2
[01:06:48.680 --> 01:06:50.680]  больше, чем L? Понятно.
[01:06:50.680 --> 01:06:52.680]  А почему вообще рассматривали
[01:06:52.680 --> 01:06:54.680]  такие нормы? То есть норма бесконечность,
[01:06:54.680 --> 01:06:56.680]  L норма 1.
[01:06:56.680 --> 01:06:58.680]  И норма 2, L2 норма 2.
[01:06:58.680 --> 01:07:00.680]  Потому что, например, в случае
[01:07:00.680 --> 01:07:02.680]  симплекса у нас как раз
[01:07:02.680 --> 01:07:04.680]  энтропия выпукла относительно
[01:07:04.680 --> 01:07:06.680]  первой нормы, и там как раз
[01:07:06.680 --> 01:07:08.680]  расстояние измеряется в первой норме.
[01:07:08.680 --> 01:07:10.680]  Но мы используем дивергенцию Брегмана
[01:07:10.680 --> 01:07:12.680]  в нашем зеркальном спуске.
[01:07:12.680 --> 01:07:14.680]  А сопряженная е – это норма бесконечности.
[01:07:14.680 --> 01:07:16.680]  Соответственно, там для градиентов
[01:07:16.680 --> 01:07:18.680]  разность будет измеряться в сопряженной норме.
[01:07:18.680 --> 01:07:20.680]  Это норма бесконечности.
[01:07:20.680 --> 01:07:22.680]  А, соответственно,
[01:07:22.680 --> 01:07:24.680]  разность по х
[01:07:24.680 --> 01:07:26.680]  будет измеряться в первой норме.
[01:07:26.680 --> 01:07:28.680]  Ну а Евкрида в случае-то тот,
[01:07:28.680 --> 01:07:30.680]  который мы рассматривали до этого.
[01:07:30.680 --> 01:07:32.680]  Просто вот такой есть.
[01:07:32.680 --> 01:07:34.680]  Стандартный, базовый.
[01:07:34.680 --> 01:07:36.680]  Сейчас мы как раз просто перейдем
[01:07:36.680 --> 01:07:38.680]  тоже к симплексам.
[01:07:38.680 --> 01:07:40.680]  Я покажу, откуда берется эффект.
[01:07:40.680 --> 01:07:42.680]  Что действительно там
[01:07:42.680 --> 01:07:44.680]  константы лучше.
[01:07:44.680 --> 01:07:46.680]  Это Немировский в свое время тоже показывал.
[01:07:46.680 --> 01:07:48.680]  Так что
[01:07:48.680 --> 01:07:50.680]  скорее оттуда уже все взято.
[01:07:50.680 --> 01:07:52.680]  Понятно, да, в чем идея?
[01:07:52.680 --> 01:07:54.680]  В каких-то частных случаях может быть
[01:07:54.680 --> 01:07:56.680]  что L сильный.
[01:07:56.680 --> 01:07:58.680]  Но по дивергенции при этом можно проиграть.
[01:07:58.680 --> 01:08:00.680]  Будем смотреть через частный случай.
[01:08:04.680 --> 01:08:06.680]  Это мы с вами обсудили.
[01:08:06.680 --> 01:08:08.680]  То, что мы сейчас так.
[01:08:08.680 --> 01:08:10.680]  Я хочу, соответственно,
[01:08:10.680 --> 01:08:12.680]  рассмотреть все же дивергенцию Брегмана,
[01:08:12.680 --> 01:08:14.680]  которая равна
[01:08:14.680 --> 01:08:16.680]  дивергенции Кульба Калейбенера.
[01:08:16.680 --> 01:08:18.680]  Все это мы рассматриваем на симплексе.
[01:08:18.680 --> 01:08:20.680]  То, что все это корректно,
[01:08:20.680 --> 01:08:22.680]  мы с вами поняли.
[01:08:22.680 --> 01:08:24.680]  По пинскеру она действительно
[01:08:24.680 --> 01:08:26.680]  энтропийная функция
[01:08:26.680 --> 01:08:28.680]  11 сильно выпукла.
[01:08:28.680 --> 01:08:30.680]  Хочу найти
[01:08:30.680 --> 01:08:32.680]  решение этого аргуменима
[01:08:32.680 --> 01:08:34.680]  в явном виде. Здесь у меня x теперь
[01:08:34.680 --> 01:08:36.680]  это симплекс.
[01:08:36.680 --> 01:08:38.680]  Ну множество понятно.
[01:08:38.680 --> 01:08:40.680]  x' больше 0, сумма
[01:08:40.680 --> 01:08:42.680]  x' равна 1.
[01:08:42.680 --> 01:08:44.680]  Распределение вероятности.
[01:08:44.680 --> 01:08:46.680]  Все вероятности больше 0
[01:08:46.680 --> 01:08:48.680]  и сумма равна 1.
[01:08:48.680 --> 01:08:50.680]  Записываю в стандартном виде.
[01:08:50.680 --> 01:08:52.680]  Ограничение вида неравенств
[01:08:52.680 --> 01:08:54.680]  плюс ограничение
[01:08:54.680 --> 01:08:56.680]  равенства равно 0.
[01:08:56.680 --> 01:08:58.680]  Плюс моя целевая функция, которую хочу решать.
[01:08:58.680 --> 01:09:00.680]  Здесь, соответственно,
[01:09:00.680 --> 01:09:02.680]  можно уже поставить rd.
[01:09:02.680 --> 01:09:04.680]  Так как я в ограничение,
[01:09:04.680 --> 01:09:06.680]  все это уже внес.
[01:09:06.680 --> 01:09:08.680]  Хорошо.
[01:09:08.680 --> 01:09:10.680]  Задача с ограничениями.
[01:09:10.680 --> 01:09:12.680]  Давайте писать лагранжи.
[01:09:20.680 --> 01:09:22.680]  Пу-пу-пум.
[01:09:22.680 --> 01:09:24.680]  Так.
[01:09:24.680 --> 01:09:26.680]  Гамма,
[01:09:26.680 --> 01:09:28.680]  f,
[01:09:28.680 --> 01:09:30.680]  xk,
[01:09:30.680 --> 01:09:32.680]  x,
[01:09:32.680 --> 01:09:34.680]  v,
[01:09:34.680 --> 01:09:36.680]  x,
[01:09:36.680 --> 01:09:38.680]  xk.
[01:09:38.680 --> 01:09:40.680]  Лагранжи.
[01:09:40.680 --> 01:09:42.680]  У меня минус,
[01:09:42.680 --> 01:09:44.680]  так как здесь минус вытащится.
[01:09:44.680 --> 01:09:46.680]  Лямда и x.
[01:09:46.680 --> 01:09:48.680]  Плюс, соответственно,
[01:09:48.680 --> 01:09:50.680]  сумма хитых минус 1.
[01:09:50.680 --> 01:09:52.680]  Все вроде норм.
[01:09:52.680 --> 01:09:54.680]  Согласны?
[01:09:54.680 --> 01:09:56.680]  Должно быть правильно.
[01:09:58.680 --> 01:10:00.680]  Здесь я сразу еще тоже подставлю,
[01:10:00.680 --> 01:10:02.680]  что это сумма хитых
[01:10:02.680 --> 01:10:04.680]  логарифум
[01:10:04.680 --> 01:10:06.680]  хит
[01:10:06.680 --> 01:10:08.680]  делить на
[01:10:08.680 --> 01:10:10.680]  хитк.
[01:10:10.680 --> 01:10:12.680]  Хк это просто фиксирована
[01:10:12.680 --> 01:10:14.680]  какая-то вещь,
[01:10:14.680 --> 01:10:16.680]  с прошлой итерации x.
[01:10:16.680 --> 01:10:18.680]  Я подставил вместо диургенции брегом.
[01:10:18.680 --> 01:10:20.680]  Вот выражение, я его подставил.
[01:10:20.680 --> 01:10:22.680]  Хорошо.
[01:10:22.680 --> 01:10:24.680]  Более-менее
[01:10:24.680 --> 01:10:26.680]  разобрались.
[01:10:26.680 --> 01:10:28.680]  Тут можно заметить, что функция
[01:10:28.680 --> 01:10:30.680]  будет сепарабельно
[01:10:30.680 --> 01:10:32.680]  по каждой группе перемен.
[01:10:32.680 --> 01:10:34.680]  Потому что скалярное произведение
[01:10:34.680 --> 01:10:36.680]  это просто что
[01:10:36.680 --> 01:10:38.680]  гамма,
[01:10:38.680 --> 01:10:40.680]  f, xк,
[01:10:40.680 --> 01:10:42.680]  it компонента,
[01:10:42.680 --> 01:10:44.680]  в данном случае констант
[01:10:44.680 --> 01:10:46.680]  xу оптимизируем, а xк это фиксировано.
[01:10:46.680 --> 01:10:48.680]  Плюс, соответственно,
[01:10:48.680 --> 01:10:50.680]  диургенция брегмана,
[01:10:50.680 --> 01:10:52.680]  которая тоже является
[01:10:52.680 --> 01:10:54.680]  сепарабельной по каждому из x.
[01:10:54.680 --> 01:10:56.680]  Минус, соответственно,
[01:10:56.680 --> 01:10:58.680]  вот это,
[01:10:58.680 --> 01:11:00.680]  лямбда и иксы.
[01:11:00.680 --> 01:11:02.680]  Плюс, соответственно,
[01:11:02.680 --> 01:11:04.680]  это сумма хитых минус 1.
[01:11:04.680 --> 01:11:06.680]  Согласны?
[01:11:06.680 --> 01:11:08.680]  То есть что?
[01:11:08.680 --> 01:11:10.680]  Каждый слагаем, у меня виды суммы,
[01:11:10.680 --> 01:11:12.680]  для каждого члена
[01:11:12.680 --> 01:11:14.680]  зависит только от x этого.
[01:11:14.680 --> 01:11:16.680]  А явный вид, это мы что хотим получить?
[01:11:16.680 --> 01:11:18.680]  Ну что такое явный вид?
[01:11:18.680 --> 01:11:20.680]  Аргуминиум
[01:11:20.680 --> 01:11:22.680]  решать сложно.
[01:11:22.680 --> 01:11:24.680]  Я хочу тупо формулу, как для градиентного спуска.
[01:11:24.680 --> 01:11:26.680]  Вычти градиент, получи ответ.
[01:11:26.680 --> 01:11:28.680]  Вот.
[01:11:28.680 --> 01:11:30.680]  Ну и xкt плюс 1.
[01:11:30.680 --> 01:11:32.680]  Я хочу то же самое. Выведи мне просто формулу
[01:11:32.680 --> 01:11:34.680]  так, чтобы я просто подставил у него числа
[01:11:34.680 --> 01:11:36.680]  и получил xкt
[01:11:36.680 --> 01:11:38.680]  плюс 1.
[01:11:38.680 --> 01:11:40.680]  Вот такое хочется получить.
[01:11:40.680 --> 01:11:42.680]  Ну а пока записит аргуминиум,
[01:11:42.680 --> 01:11:44.680]  который в общем случае решается численно.
[01:11:44.680 --> 01:11:46.680]  Ну методом оптимизации дополнительно.
[01:11:46.680 --> 01:11:48.680]  Вот, что-то вы писали
[01:11:48.680 --> 01:11:50.680]  в изобразии вот такое.
[01:11:50.680 --> 01:11:52.680]  Поэтому еще раз, по каждому иксы
[01:11:52.680 --> 01:11:54.680]  можно минимизировать отдельно.
[01:11:56.680 --> 01:11:58.680]  Ух, что-то я паузу не наставил.
[01:11:58.680 --> 01:12:00.680]  Ладно.
[01:12:00.680 --> 01:12:02.680]  Так.
[01:12:02.680 --> 01:12:04.680]  Давайте по каждому из иксы
[01:12:04.680 --> 01:12:06.680]  будем минимизировать
[01:12:06.680 --> 01:12:08.680]  отдельно.
[01:12:08.680 --> 01:12:10.680]  Так.
[01:12:10.680 --> 01:12:12.680]  Или я это здесь...
[01:12:12.680 --> 01:12:14.680]  А, ну и ладно.
[01:12:14.680 --> 01:12:16.680]  Давайте вот здесь в принципе.
[01:12:16.680 --> 01:12:18.680]  Я это выражение выписал.
[01:12:18.680 --> 01:12:20.680]  Я это выражение выписал.
[01:12:20.680 --> 01:12:22.680]  Как раз в виде суммы
[01:12:22.680 --> 01:12:24.680]  все это занес под одну сумму.
[01:12:24.680 --> 01:12:26.680]  Теперь у меня вот есть каждое
[01:12:26.680 --> 01:12:28.680]  выражение, которое можно по икситам
[01:12:28.680 --> 01:12:30.680]  найти инфим. Здесь я просто ответ
[01:12:30.680 --> 01:12:32.680]  выписал. Соответственно, давайте
[01:12:32.680 --> 01:12:34.680]  попробуем получить, что этот ответ
[01:12:34.680 --> 01:12:36.680]  верный.
[01:12:36.680 --> 01:12:38.680]  Верный.
[01:12:38.680 --> 01:12:40.680]  Ну, ничего тут сложного нет.
[01:12:40.680 --> 01:12:42.680]  Как найти минимум функций?
[01:12:42.680 --> 01:12:44.680]  Давайте возьмем, опять же, условия
[01:12:44.680 --> 01:12:46.680]  оптимальности. В данном случае-то
[01:12:46.680 --> 01:12:48.680]  мы же на инограничном множестве уже
[01:12:48.680 --> 01:12:50.680]  инфимы берем. Поэтому берем просто
[01:12:50.680 --> 01:12:52.680]  условия оптимальности и записываем его.
[01:12:52.680 --> 01:12:54.680]  Логарифм дифференцируем одномерный.
[01:12:54.680 --> 01:12:56.680]  Что там получится?
[01:12:56.680 --> 01:12:58.680]  1 делить на иксы
[01:12:58.680 --> 01:13:00.680]  x и kt
[01:13:02.680 --> 01:13:04.680]  плюс вот этот...
[01:13:04.680 --> 01:13:06.680]  Что там? Из логарифма еще будут
[01:13:06.680 --> 01:13:08.680]  вылезать.
[01:13:08.680 --> 01:13:10.680]  А нет, там же как?
[01:13:10.680 --> 01:13:12.680]  Логарифм на x.
[01:13:12.680 --> 01:13:14.680]  Тогда у нас что останется?
[01:13:14.680 --> 01:13:16.680]  Почему мы дифференцируем?
[01:13:16.680 --> 01:13:18.680]  По какой переменной?
[01:13:18.680 --> 01:13:20.680]  По x. По иксу.
[01:13:20.680 --> 01:13:22.680]  И тому.
[01:13:22.680 --> 01:13:24.680]  Я хочу, соответственно, инфимам искать.
[01:13:24.680 --> 01:13:26.680]  Поэтому ищу условия оптимальности.
[01:13:28.680 --> 01:13:30.680]  Тогда там будет просто
[01:13:30.680 --> 01:13:32.680]  1 делить на иксы.
[01:13:32.680 --> 01:13:34.680]  Логарифма?
[01:13:34.680 --> 01:13:36.680]  Нет, тут же вот коэффициент есть.
[01:13:36.680 --> 01:13:38.680]  Ну так мы же
[01:13:38.680 --> 01:13:40.680]  разве не должны потом еще продиференцировать
[01:13:40.680 --> 01:13:42.680]  эту штуку как сложную функцию?
[01:13:42.680 --> 01:13:44.680]  Откуда достанется этот коэффициент и он сократится?
[01:13:44.680 --> 01:13:46.680]  Ну вот, я вроде бы все это и сделал.
[01:13:46.680 --> 01:13:48.680]  Вот, да?
[01:13:48.680 --> 01:13:50.680]  Как раз они мне сократились.
[01:13:50.680 --> 01:13:52.680]  И еще вот здесь x болтается.
[01:13:52.680 --> 01:13:54.680]  Вот здесь x болтается.
[01:13:54.680 --> 01:13:56.680]  Я его приписал,
[01:13:56.680 --> 01:13:58.680]  потому что у меня как раз тут произведение
[01:13:58.680 --> 01:14:00.680]  логарифма на x. Я продиференцировал
[01:14:00.680 --> 01:14:02.680]  логарифм, но x у меня остался.
[01:14:02.680 --> 01:14:04.680]  И вот это они заболтались
[01:14:04.680 --> 01:14:06.680]  и в итоге единичка только и осталась.
[01:14:06.680 --> 01:14:08.680]  Вот, дальше что я делаю?
[01:14:08.680 --> 01:14:10.680]  Как раз когда логарифм дифференцировал, а x не дифференцировал.
[01:14:10.680 --> 01:14:12.680]  Теперь обратно.
[01:14:12.680 --> 01:14:14.680]  x дифференцировал, логарифм оставляет.
[01:14:18.680 --> 01:14:20.680]  Так. Ну и дальше там
[01:14:20.680 --> 01:14:22.680]  все то, что осталось болтаться.
[01:14:24.680 --> 01:14:26.680]  Градиент.
[01:14:26.680 --> 01:14:30.680]  Лямбда и клюню.
[01:14:30.680 --> 01:14:32.680]  Вот.
[01:14:32.680 --> 01:14:34.680]  Отсюда...
[01:14:34.680 --> 01:14:36.680]  Тут важно, что эта единичка болтается.
[01:14:36.680 --> 01:14:38.680]  Вот это.
[01:14:38.680 --> 01:14:40.680]  Отсюда надо просто
[01:14:40.680 --> 01:14:42.680]  выразить x и t.
[01:14:42.680 --> 01:14:44.680]  x и t при условии,
[01:14:44.680 --> 01:14:46.680]  что это равно 0, выражается
[01:14:46.680 --> 01:14:48.680]  x и t.
[01:14:48.680 --> 01:14:50.680]  Выражается x и t.
[01:14:50.680 --> 01:14:52.680]  Вот. Это x и t получается...
[01:14:52.680 --> 01:14:54.680]  Ну что?
[01:14:54.680 --> 01:14:56.680]  Вот этот кусочек давайте
[01:14:56.680 --> 01:14:58.680]  с обозначу просто.
[01:14:58.680 --> 01:15:00.680]  А там еще и...
[01:15:00.680 --> 01:15:02.680]  С плюс 1.
[01:15:02.680 --> 01:15:04.680]  С плюс...
[01:15:04.680 --> 01:15:06.680]  Минус 1, минус с переносится вправо.
[01:15:06.680 --> 01:15:08.680]  Берется от этого всего
[01:15:08.680 --> 01:15:10.680]  безобразие экспонента.
[01:15:10.680 --> 01:15:12.680]  Вот. И слева, соответственно,
[01:15:12.680 --> 01:15:14.680]  остается x и t делить на x ка т.
[01:15:14.680 --> 01:15:16.680]  Вот. Откуда получается,
[01:15:16.680 --> 01:15:18.680]  что x и t оптимальное
[01:15:18.680 --> 01:15:20.680]  равно x и t ка т
[01:15:20.680 --> 01:15:22.680]  экспонента 1, минус с.
[01:15:22.680 --> 01:15:24.680]  А можете еще раз
[01:15:24.680 --> 01:15:26.680]  показать, откуда у нас логарифм взялся?
[01:15:26.680 --> 01:15:28.680]  Там вроде с предыдущего слайда.
[01:15:28.680 --> 01:15:30.680]  Логарифм это
[01:15:30.680 --> 01:15:32.680]  с дивергенцией Брегмана взялся.
[01:15:34.680 --> 01:15:36.680]  Вот. Дивергенцией Брегмана.
[01:15:36.680 --> 01:15:38.680]  Вот.
[01:15:38.680 --> 01:15:40.680]  Окей.
[01:15:40.680 --> 01:15:42.680]  Нет, подождите.
[01:15:42.680 --> 01:15:44.680]  А откуда
[01:15:44.680 --> 01:15:46.680]  логарифм там взялся?
[01:15:46.680 --> 01:15:48.680]  Где?
[01:15:48.680 --> 01:15:50.680]  Где? Сейчас.
[01:15:50.680 --> 01:15:52.680]  Как вернуться-то?
[01:15:52.680 --> 01:15:54.680]  Ага. Так.
[01:15:54.680 --> 01:15:56.680]  Бум-бум-бум.
[01:15:56.680 --> 01:15:58.680]  Где? В дивергенции Брегмана,
[01:15:58.680 --> 01:16:00.680]  где он взялся?
[01:16:00.680 --> 01:16:02.680]  Да-да-да.
[01:16:02.680 --> 01:16:04.680]  Давайте мы ее найдем в явном виде.
[01:16:04.680 --> 01:16:06.680]  Просто времени на это все, конечно,
[01:16:06.680 --> 01:16:08.680]  уже тратит. Что там у нас?
[01:16:08.680 --> 01:16:10.680]  dx
[01:16:10.680 --> 01:16:12.680]  минус dy
[01:16:12.680 --> 01:16:14.680]  минус
[01:16:14.680 --> 01:16:16.680]  градиент dy
[01:16:16.680 --> 01:16:18.680]  это выражение для дивергенции
[01:16:18.680 --> 01:16:20.680]  Брегмана. Порождается она
[01:16:20.680 --> 01:16:22.680]  энтропийная функция x
[01:16:22.680 --> 01:16:24.680]  логарифм x, y
[01:16:24.680 --> 01:16:26.680]  о, нет. Господи.
[01:16:26.680 --> 01:16:28.680]  Тогда можно понять, куда это берется.
[01:16:28.680 --> 01:16:30.680]  Порождается она энтропийная функция
[01:16:30.680 --> 01:16:32.680]  и получается такое выражение.
[01:16:32.680 --> 01:16:34.680]  Это пример был выше,
[01:16:34.680 --> 01:16:36.680]  что такая дивергенция порождается
[01:16:36.680 --> 01:16:38.680]  энтропийной функцией. По определению
[01:16:38.680 --> 01:16:40.680]  можно проверить просто, что действительно так.
[01:16:40.680 --> 01:16:42.680]  Появляется такой логарифм.
[01:16:42.680 --> 01:16:44.680]  Здесь
[01:16:44.680 --> 01:16:46.680]  находим этот x,
[01:16:46.680 --> 01:16:48.680]  подставляем, получаем
[01:16:48.680 --> 01:16:50.680]  энфимум.
[01:16:50.680 --> 01:16:52.680]  Сюда подставили
[01:16:52.680 --> 01:16:54.680]  лагранжан, оптимальный x.
[01:16:54.680 --> 01:16:56.680]  Здесь у вас
[01:16:56.680 --> 01:16:58.680]  получилось вот такое выражение.
[01:16:58.680 --> 01:17:00.680]  Это значит у нас
[01:17:00.680 --> 01:17:02.680]  наша двойственная функция.
[01:17:02.680 --> 01:17:04.680]  Наша двойственная функция.
[01:17:06.680 --> 01:17:08.680]  Подставлять уже времени
[01:17:08.680 --> 01:17:10.680]  нет.
[01:17:10.680 --> 01:17:12.680]  Двойственная задача
[01:17:12.680 --> 01:17:14.680]  формулируется,
[01:17:14.680 --> 01:17:16.680]  тут все просто.
[01:17:16.680 --> 01:17:18.680]  Максим нашей двойственной функции
[01:17:18.680 --> 01:17:20.680]  λ, так как они вылезли
[01:17:20.680 --> 01:17:22.680]  из ограничений типа неравенства
[01:17:22.680 --> 01:17:24.680]  не больше 0,
[01:17:24.680 --> 01:17:26.680]  nu
[01:17:26.680 --> 01:17:28.680]  оно у нас
[01:17:28.680 --> 01:17:30.680]  из ограничений типа равенства,
[01:17:30.680 --> 01:17:32.680]  поэтому оно свободное у нас.
[01:17:32.680 --> 01:17:34.680]  Давайте быстренько заметим,
[01:17:34.680 --> 01:17:36.680]  что если я хочу решить эту задачу
[01:17:36.680 --> 01:17:38.680]  максимума, как я должен
[01:17:38.680 --> 01:17:40.680]  обращаться с λ, который у меня
[01:17:40.680 --> 01:17:42.680]  больше нуля,
[01:17:42.680 --> 01:17:44.680]  здесь минус стоит.
[01:17:48.680 --> 01:17:50.680]  Что я должен
[01:17:50.680 --> 01:17:52.680]  сделать с λ?
[01:17:52.680 --> 01:17:54.680]  Занулить?
[01:17:54.680 --> 01:17:56.680]  Да, λ у нас
[01:17:56.680 --> 01:17:58.680]  соответственно увеличение λ
[01:17:58.680 --> 01:18:00.680]  увеличивает нашу экспоненту.
[01:18:00.680 --> 01:18:02.680]  А значит в силу того,
[01:18:02.680 --> 01:18:04.680]  что перед экспонентой стоит знак минус,
[01:18:04.680 --> 01:18:06.680]  общее выражение уменьшается.
[01:18:06.680 --> 01:18:08.680]  Поэтому λ нужно
[01:18:08.680 --> 01:18:10.680]  увеличить, то есть лямбда оптимальная
[01:18:10.680 --> 01:18:12.680]  это ноль.
[01:18:12.680 --> 01:18:14.680]  Лямбда оптимальная
[01:18:14.680 --> 01:18:16.680]  это ноль.
[01:18:16.680 --> 01:18:18.680]  Я это все подставляю.
[01:18:18.680 --> 01:18:20.680]  А, ну и дальше что?
[01:18:20.680 --> 01:18:22.680]  Условия кунатакера.
[01:18:22.680 --> 01:18:24.680]  Лагранжан, наша функция лагранжа
[01:18:24.680 --> 01:18:26.680]  берется у нее просто
[01:18:26.680 --> 01:18:28.680]  градиент по х
[01:18:28.680 --> 01:18:30.680]  и он
[01:18:30.680 --> 01:18:32.680]  приравнивается к нулю. Это одно из условий
[01:18:32.680 --> 01:18:34.680]  кунатакера, это первое условие кунатакера.
[01:18:34.680 --> 01:18:36.680]  Лагранжан равен нулю,
[01:18:36.680 --> 01:18:38.680]  градиент лагранжана по х
[01:18:38.680 --> 01:18:40.680]  равен нулю.
[01:18:40.680 --> 01:18:42.680]  Там можно сразу учесть,
[01:18:42.680 --> 01:18:44.680]  что вот эти лямбды равны нулю.
[01:18:44.680 --> 01:18:46.680]  И когда мы это все безобразие
[01:18:46.680 --> 01:18:48.680]  выпишем,
[01:18:48.680 --> 01:18:50.680]  то вот как раз
[01:18:50.680 --> 01:18:52.680]  это и останется.
[01:18:52.680 --> 01:18:54.680]  Это и останется, что записано
[01:18:54.680 --> 01:18:56.680]  здесь, на
[01:18:56.680 --> 01:18:58.680]  слайде.
[01:18:58.680 --> 01:19:00.680]  Лагранжан
[01:19:00.680 --> 01:19:02.680]  градиент по х лагранжана
[01:19:02.680 --> 01:19:04.680]  равен нулю. Первое условие
[01:19:04.680 --> 01:19:06.680]  кунатакера.
[01:19:08.680 --> 01:19:10.680]  Все, а дальше что?
[01:19:10.680 --> 01:19:12.680]  Это все опять же берется,
[01:19:12.680 --> 01:19:14.680]  аккуратненько
[01:19:14.680 --> 01:19:16.680]  дифференцируется.
[01:19:16.680 --> 01:19:18.680]  Ну давайте что?
[01:19:18.680 --> 01:19:20.680]  Продиференцируем, что здесь
[01:19:20.680 --> 01:19:22.680]  остается.
[01:19:22.680 --> 01:19:24.680]  Здесь НАБЛА,
[01:19:24.680 --> 01:19:26.680]  FXKT,
[01:19:26.680 --> 01:19:28.680]  IT
[01:19:28.680 --> 01:19:30.680]  из дивергенции Брегмана.
[01:19:30.680 --> 01:19:32.680]  Мы ее уже с вами дифференцировали.
[01:19:32.680 --> 01:19:34.680]  Вылезает логарифм
[01:19:34.680 --> 01:19:36.680]  X,
[01:19:36.680 --> 01:19:38.680]  IT как раз.
[01:19:38.680 --> 01:19:40.680]  Вот это выражение вылезает отсюда.
[01:19:40.680 --> 01:19:42.680]  X, KT,
[01:19:42.680 --> 01:19:44.680]  IT.
[01:19:44.680 --> 01:19:46.680]  Это минус 1, когда у нас там
[01:19:46.680 --> 01:19:48.680]  X болтаться будет.
[01:19:48.680 --> 01:19:50.680]  Которое у нас все сократит.
[01:19:50.680 --> 01:19:52.680]  Ровно то же самое, что мы сделали здесь.
[01:19:52.680 --> 01:19:54.680]  Вот они.
[01:19:54.680 --> 01:19:56.680]  Раз, это 1.
[01:19:56.680 --> 01:19:58.680]  Плюс логарифм.
[01:19:58.680 --> 01:20:00.680]  Это дивергенция Брегмана.
[01:20:00.680 --> 01:20:02.680]  Отображается.
[01:20:02.680 --> 01:20:04.680]  Производная по ней.
[01:20:04.680 --> 01:20:06.680]  Это вылезло из скалярного произведения.
[01:20:06.680 --> 01:20:08.680]  Ну и плюс, когда продиференцировали
[01:20:08.680 --> 01:20:10.680]  по X, IT,
[01:20:10.680 --> 01:20:12.680]  еще вылезло NU со звездой.
[01:20:12.680 --> 01:20:14.680]  Опять же, условия кунатакера
[01:20:14.680 --> 01:20:16.680]  получились вот таким.
[01:20:16.680 --> 01:20:18.680]  Отсюда можно легко выразить
[01:20:18.680 --> 01:20:20.680]  X, IT со звездой.
[01:20:20.680 --> 01:20:22.680]  Там получится опять же,
[01:20:22.680 --> 01:20:24.680]  когда мы возьмем экспоненту,
[01:20:24.680 --> 01:20:26.680]  перенесем все вправо,
[01:20:26.680 --> 01:20:28.680]  кроме логарифм,
[01:20:28.680 --> 01:20:30.680]  то получится экспонент.
[01:20:34.680 --> 01:20:35.680]  Давайте так напишу.
[01:20:35.680 --> 01:20:37.680]  Минус гамма-градиент.
[01:20:37.680 --> 01:20:38.680]  А мы же уже это делали.
[01:20:38.680 --> 01:20:40.680]  Зачем еще раз это делаем?
[01:20:40.680 --> 01:20:42.680]  Мы же вот на предыдущем слайде
[01:20:42.680 --> 01:20:44.680]  именно вот дифференцировали.
[01:20:44.680 --> 01:20:46.680]  Не, до этого.
[01:20:46.680 --> 01:20:48.680]  Там у меня была еще лямбда.
[01:20:48.680 --> 01:20:50.680]  Я теперь избавился от лямбды.
[01:20:50.680 --> 01:20:52.680]  Я знаю, что они нули.
[01:20:52.680 --> 01:20:54.680]  И теперь у меня уже все должно быть четко.
[01:20:54.680 --> 01:20:56.680]  Экспонента тут будет стоять
[01:20:56.680 --> 01:21:00.680]  минус ню плюс один.
[01:21:00.680 --> 01:21:01.680]  Вот.
[01:21:01.680 --> 01:21:03.680]  Там будут как раз экспоненты от суммы.
[01:21:03.680 --> 01:21:05.680]  Это просто произведение экспонентов.
[01:21:05.680 --> 01:21:07.680]  Ну и что, смотрите,
[01:21:07.680 --> 01:21:09.680]  по факту выражение это готово.
[01:21:09.680 --> 01:21:11.680]  Вот оно.
[01:21:12.680 --> 01:21:14.680]  Так-то я не знаю только с чего.
[01:21:16.680 --> 01:21:18.680]  Я не знаю ню.
[01:21:18.680 --> 01:21:20.680]  Из каких соображений я его могу найти?
[01:21:20.680 --> 01:21:22.680]  Из каких соображений?
[01:21:22.680 --> 01:21:24.680]  Точнее даже вот этот кусочек
[01:21:24.680 --> 01:21:26.680]  экспонента один минус это.
[01:21:28.680 --> 01:21:30.680]  Икса звездой и всегда удовлетворят
[01:21:30.680 --> 01:21:32.680]  ограничения в данном виде?
[01:21:38.680 --> 01:21:40.680]  Больше нуля?
[01:21:40.680 --> 01:21:42.680]  Там в ККТ какие-то еще условия были?
[01:21:42.680 --> 01:21:44.680]  А вы вспомните исходные ограничения.
[01:21:44.680 --> 01:21:46.680]  Они какие были?
[01:21:46.680 --> 01:21:48.680]  Иксы-то больше нуля.
[01:21:48.680 --> 01:21:50.680]  Но здесь все четко, потому что
[01:21:50.680 --> 01:21:52.680]  исходные эти симплексы, они больше нуля.
[01:21:52.680 --> 01:21:54.680]  Но было еще одно ограничение.
[01:21:54.680 --> 01:21:56.680]  Так как это симплекс,
[01:21:56.680 --> 01:21:58.680]  сумма равна единице.
[01:21:58.680 --> 01:22:00.680]  Но исходя из этого условия,
[01:22:00.680 --> 01:22:02.680]  можно и подобрать ню.
[01:22:02.680 --> 01:22:04.680]  То есть чтобы вот это,
[01:22:04.680 --> 01:22:06.680]  этот коэффициент должен быть
[01:22:06.680 --> 01:22:08.680]  подобран так, что сумма
[01:22:08.680 --> 01:22:10.680]  х итых со звездой равна единице.
[01:22:12.680 --> 01:22:14.680]  Ну и все.
[01:22:14.680 --> 01:22:16.680]  Тогда счет получается.
[01:22:16.680 --> 01:22:18.680]  Сумма х итых катых,
[01:22:18.680 --> 01:22:20.680]  экспонента одна, минус гамма,
[01:22:20.680 --> 01:22:22.680]  а вторую экспоненту можно вынести
[01:22:22.680 --> 01:22:24.680]  вообще за пределы скобки,
[01:22:24.680 --> 01:22:26.680]  потому что она от и не зависит.
[01:22:26.680 --> 01:22:28.680]  Экспонента 1 минус
[01:22:28.680 --> 01:22:30.680]  это, это.
[01:22:30.680 --> 01:22:32.680]  Это должно равняться единице.
[01:22:32.680 --> 01:22:34.680]  Соответственно, экспонента
[01:22:34.680 --> 01:22:36.680]  1 минус ню
[01:22:36.680 --> 01:22:38.680]  равно 1 делить на сумму
[01:22:38.680 --> 01:22:40.680]  х икта, экспонента
[01:22:40.680 --> 01:22:42.680]  минус гамма, пу-пу-пу
[01:22:42.680 --> 01:22:44.680]  градиента.
[01:22:44.680 --> 01:22:46.680]  Это просто нормировочный кусочек будет.
[01:22:46.680 --> 01:22:48.680]  И тогда второе ограничение
[01:22:48.680 --> 01:22:50.680]  выполнится.
[01:22:50.680 --> 01:22:52.680]  Да, мы не все честно сделали по кунатакеру,
[01:22:52.680 --> 01:22:54.680]  но в некотором смысле так поигрались.
[01:22:54.680 --> 01:22:56.680]  Вот.
[01:22:56.680 --> 01:22:58.680]  Это и есть выражение, которое у нас получается
[01:22:58.680 --> 01:23:00.680]  для
[01:23:00.680 --> 01:23:02.680]  зеркального спуска в случае,
[01:23:02.680 --> 01:23:04.680]  когда мы работаем на симплексе.
[01:23:06.680 --> 01:23:08.680]  Вот. Явное выражение.
[01:23:08.680 --> 01:23:10.680]  Знаете градиент,
[01:23:10.680 --> 01:23:12.680]  можете спокойно посчитать
[01:23:12.680 --> 01:23:14.680]  новый хк плюс 1.
[01:23:14.680 --> 01:23:16.680]  Ну, вот так по координату.
[01:23:16.680 --> 01:23:18.680]  Вот. И оказывается,
[01:23:18.680 --> 01:23:20.680]  оказывается, что
[01:23:20.680 --> 01:23:22.680]  последний факт, который нам нужен,
[01:23:22.680 --> 01:23:24.680]  что вот как раз в случае симплекса
[01:23:24.680 --> 01:23:26.680]  можно показать, что зеркальный спуск
[01:23:26.680 --> 01:23:28.680]  работает в D
[01:23:28.680 --> 01:23:30.680]  делить на логарифм D
[01:23:30.680 --> 01:23:32.680]  раз быстрее, чем
[01:23:32.680 --> 01:23:34.680]  градиентный спуск с проекцией.
[01:23:34.680 --> 01:23:36.680]  Вот. Откуда берется D?
[01:23:36.680 --> 01:23:38.680]  Откуда берется D?
[01:23:38.680 --> 01:23:40.680]  Это вот как раз, помните, я показывал, что норма
[01:23:40.680 --> 01:23:42.680]  первая в лучшем случае,
[01:23:42.680 --> 01:23:44.680]  в нашем случае в лучшем.
[01:23:44.680 --> 01:23:46.680]  Корень из D раз больше, чем вторая.
[01:23:46.680 --> 01:23:48.680]  Вторая, в свою очередь,
[01:23:48.680 --> 01:23:50.680]  в D раз больше, чем нормы
[01:23:50.680 --> 01:23:52.680]  бесконечности.
[01:23:52.680 --> 01:23:54.680]  И вот она взялась с D.
[01:23:54.680 --> 01:23:56.680]  А проигрыш в логарифм
[01:23:56.680 --> 01:23:58.680]  раз берется из-за того,
[01:23:58.680 --> 01:24:00.680]  что диаметр симплекса
[01:24:00.680 --> 01:24:02.680]  в евклидовой норме равен
[01:24:02.680 --> 01:24:04.680]  единице.
[01:24:04.680 --> 01:24:06.680]  А
[01:24:06.680 --> 01:24:08.680]  соответственно,
[01:24:08.680 --> 01:24:10.680]  в дивергенции Кульбака-Лебнера
[01:24:10.680 --> 01:24:12.680]  он равен логарифму D.
[01:24:12.680 --> 01:24:14.680]  То есть, в дивергенции Брегмана
[01:24:14.680 --> 01:24:16.680]  вы проиграли именно
[01:24:16.680 --> 01:24:18.680]  в оценке
[01:24:18.680 --> 01:24:20.680]  в лог D раз, но выиграли
[01:24:20.680 --> 01:24:22.680]  по L-ке,
[01:24:22.680 --> 01:24:24.680]  по оценке на L.
[01:24:24.680 --> 01:24:26.680]  И за счет этого у вас действительно
[01:24:26.680 --> 01:24:28.680]  зеркальный спуск может работать
[01:24:28.680 --> 01:24:30.680]  быстрее,
[01:24:30.680 --> 01:24:32.680]  чем метод
[01:24:32.680 --> 01:24:34.680]  евклидовой проекции.
[01:24:34.680 --> 01:24:36.680]  Это мы еще даже не говорим про то, что у нас
[01:24:36.680 --> 01:24:38.680]  есть еще какая-то вложенная физика,
[01:24:38.680 --> 01:24:40.680]  что просто по евклидовому
[01:24:40.680 --> 01:24:42.680]  расстоянию плохо для распределения.
[01:24:42.680 --> 01:24:44.680]  А KL хорошо.
[01:24:44.680 --> 01:24:46.680]  Вот, оно физично.
[01:24:46.680 --> 01:24:48.680]  Все, я надеюсь, что конечно под конец
[01:24:48.680 --> 01:24:50.680]  получилось сумбурно,
[01:24:50.680 --> 01:24:52.680]  но понятно,
[01:24:52.680 --> 01:24:54.680]  я надеюсь, была основная идея того,
[01:24:54.680 --> 01:24:56.680]  что происходило.
[01:24:58.680 --> 01:25:00.680]  Вопросы.
[01:25:00.680 --> 01:25:02.680]  Во-первых,
[01:25:02.680 --> 01:25:04.680]  мы же вот эту штуку
[01:25:04.680 --> 01:25:06.680]  только на вероятностном
[01:25:06.680 --> 01:25:08.680]  симплексе рассматриваем.
[01:25:08.680 --> 01:25:10.680]  Ну, а как же задач
[01:25:10.680 --> 01:25:12.680]  у нас не только на нем?
[01:25:12.680 --> 01:25:14.680]  Ну, смотрите, в этом плане
[01:25:14.680 --> 01:25:16.680]  зеркальный спуск,
[01:25:16.680 --> 01:25:18.680]  как я говорил, основной у него такой,
[01:25:18.680 --> 01:25:20.680]  что ли, краеугольная задача, это симплекс.
[01:25:20.680 --> 01:25:22.680]  Это первый пример, который
[01:25:22.680 --> 01:25:24.680]  приводит, и на самом деле это один из
[01:25:24.680 --> 01:25:26.680]  основных примеров, где его используют.
[01:25:26.680 --> 01:25:28.680]  Есть у него особенные применения
[01:25:28.680 --> 01:25:30.680]  для всяких специальных,
[01:25:30.680 --> 01:25:32.680]  когда дивергенция порождена
[01:25:32.680 --> 01:25:34.680]  довольно специальной функцией.
[01:25:34.680 --> 01:25:36.680]  Ну, такие примеры, они
[01:25:36.680 --> 01:25:38.680]  узкие, один мы из них рассмотрим.
[01:25:38.680 --> 01:25:40.680]  Чуть попозже,
[01:25:40.680 --> 01:25:42.680]  когда будем говорить про распределенную оптимизацию,
[01:25:42.680 --> 01:25:44.680]  но он тоже такой
[01:25:44.680 --> 01:25:46.680]  специфичный.
[01:25:46.680 --> 01:25:48.680]  Но вот,
[01:25:48.680 --> 01:25:50.680]  к сожалению, да, вот в этом у
[01:25:50.680 --> 01:25:52.680]  зеркального спуска есть такой
[01:25:52.680 --> 01:25:54.680]  большой недостаток, он
[01:25:54.680 --> 01:25:56.680]  во многом заточен под
[01:25:56.680 --> 01:25:58.680]  какие-то конкретные задачки.
[01:25:58.680 --> 01:26:00.680]  Поэтому
[01:26:00.680 --> 01:26:02.680]  в некотором смысле я его рассказываю
[01:26:02.680 --> 01:26:04.680]  перед контролем, потому что его на контрольной
[01:26:04.680 --> 01:26:06.680]  будет, а чем-то новым
[01:26:06.680 --> 01:26:08.680]  перед контролем вас грузить не хотелось.
[01:26:08.680 --> 01:26:10.680]  Но все равно это очень классный метод, который
[01:26:10.680 --> 01:26:12.680]  с красивой идеей
[01:26:12.680 --> 01:26:14.680]  и с красивой довольно теорией,
[01:26:14.680 --> 01:26:16.680]  который в том числе используется
[01:26:16.680 --> 01:26:18.680]  для многих прикладных задач, в том числе
[01:26:18.680 --> 01:26:20.680]  потому что, помните,
[01:26:20.680 --> 01:26:22.680]  теоретика и игровые задачи, они решаются на симплекс.
[01:26:22.680 --> 01:26:24.680]  В домашних заданиях это есть.
[01:26:24.680 --> 01:26:26.680]  И, соответственно,
[01:26:26.680 --> 01:26:28.680]  во многом там
[01:26:28.680 --> 01:26:30.680]  зеркальный спуск играет довольно
[01:26:30.680 --> 01:26:32.680]  ключевую роль, а игры, ну вот это
[01:26:32.680 --> 01:26:34.680]  вероятность. Это вероятность выбрать
[01:26:34.680 --> 01:26:36.680]  какое-то действие.
[01:26:36.680 --> 01:26:38.680]  Это игра, не знаю, там
[01:26:38.680 --> 01:26:40.680]  reinforcement learning, обучение с подкреплением,
[01:26:40.680 --> 01:26:42.680]  где вы выбираете действие, получаете выигрыш
[01:26:42.680 --> 01:26:44.680]  или, наоборот, проигрыш.
[01:26:44.680 --> 01:26:46.680]  А у вас все
[01:26:46.680 --> 01:26:48.680]  решение задачи
[01:26:48.680 --> 01:26:50.680]  заточено на то, что вам нужно в некотором
[01:26:50.680 --> 01:26:52.680]  смысле создать вектор вероятности,
[01:26:52.680 --> 01:26:54.680]  с какой вероятностью вы
[01:26:54.680 --> 01:26:56.680]  выбираете каждое действие.
[01:26:56.680 --> 01:26:58.680]  Вам нужно обучить этот вектор.
[01:26:58.680 --> 01:27:00.680]  Но, соответственно, зеркальный спуск
[01:27:00.680 --> 01:27:02.680]  играет более чем хороший вариант
[01:27:02.680 --> 01:27:04.680]  работать на симплексе.
[01:27:04.680 --> 01:27:06.680]  К сожалению, за пределы симплекса там
[01:27:06.680 --> 01:27:08.680]  очень мало примерчиков,
[01:27:08.680 --> 01:27:10.680]  где еще, кроме симплекса,
[01:27:10.680 --> 01:27:12.680]  это прям выстреливает.
[01:27:12.680 --> 01:27:14.680]  Так, ладно, задержал уже
[01:27:14.680 --> 01:27:16.680]  сильно.
[01:27:16.680 --> 01:27:18.680]  Еще вопросик.
[01:27:18.680 --> 01:27:20.680]  А почему у нас в данном случае
[01:27:20.680 --> 01:27:22.680]  дивергенцию Брэдмана порождает
[01:27:22.680 --> 01:27:24.680]  именно функция
[01:27:24.680 --> 01:27:26.680]  вот эта вот, как мы говорили,
[01:27:26.680 --> 01:27:28.680]  энтропийная?
[01:27:28.680 --> 01:27:30.680]  Захотелось.
[01:27:30.680 --> 01:27:32.680]  Да, это тоже захотелось.
[01:27:32.680 --> 01:27:34.680]  Но это же тоже очень сильно сужает класс задач,
[01:27:34.680 --> 01:27:36.680]  или нет?
[01:27:36.680 --> 01:27:38.680]  Ну, смотрите, на самом деле дивергенцию Брэдмана,
[01:27:38.680 --> 01:27:40.680]  как вы понимаете, можно поразить далеко не всеми функциями.
[01:27:40.680 --> 01:27:42.680]  Там, во-первых, нужна дифференцированность,
[01:27:42.680 --> 01:27:44.680]  не все функции дифференцируемы.
[01:27:44.680 --> 01:27:46.680]  Вот.
[01:27:46.680 --> 01:27:48.680]  Ну, да,
[01:27:48.680 --> 01:27:50.680]  это, конечно, сужает класс задач.
[01:27:50.680 --> 01:27:52.680]  Во-первых, градиентный спуск
[01:27:52.680 --> 01:27:54.680]  всегда там, он всегда с нами, он всегда здесь,
[01:27:54.680 --> 01:27:56.680]  потому что Евклидова норма порождается.
[01:27:56.680 --> 01:27:58.680]  Расстояние Евклидова порождается.
[01:27:58.680 --> 01:28:00.680]  А все, что остальное,
[01:28:00.680 --> 01:28:02.680]  что можно породить,
[01:28:02.680 --> 01:28:04.680]  далеко не все.
[01:28:04.680 --> 01:28:06.680]  Хороший частный случай, энтропийная функция
[01:28:06.680 --> 01:28:08.680]  и дивергенция кульбокалиппера.

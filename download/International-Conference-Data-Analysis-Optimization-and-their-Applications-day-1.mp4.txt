[00:00.000 --> 00:05.000]  Доброе утро всем!
[00:05.000 --> 00:12.000]  Вы не можете представить, как я счастлив, что я открыл эту конференцию.
[00:12.000 --> 00:23.000]  Я не буду списать людей, которые участвуют в организации этой прекрасной конференции.
[00:23.000 --> 00:35.000]  Позвольте мне поделиться директором нашей школы, профессором Андрея Райгородского.
[00:35.000 --> 00:37.000]  Директор Райгородского.
[00:37.000 --> 00:45.000]  И дальше я буду читать список основных поддержек, может быть, слишком долго.
[00:45.000 --> 01:02.000]  Пуат Алискеро из Национальной институтной университетной школы экономики.
[01:02.000 --> 01:07.000]  Даниэль Беренд из Бангурии.
[01:07.000 --> 01:13.000]  Константин Воронцо из Ломоноса Университета.
[01:13.000 --> 01:20.000]  Александр Гасников из Московской университетной университеты физики и технологии.
[01:20.000 --> 01:23.000]  Я, Борис Голденгорин.
[01:23.000 --> 01:30.000]  Борис Ковалельчук из Центра Вашингтона Университета США.
[01:30.000 --> 01:36.000]  Раис Красиков из Московской университетной университетной университетной технологии.
[01:36.000 --> 01:42.000]  Борис Крохин из Московской университетной университетной университетной университетной технологии.
[01:42.000 --> 01:51.500]  Владимир Макаренко из Квебек
[01:51.500 --> 01:56.000]  из Монреала.
[01:56.000 --> 02:00.000]  В立ке неконippiезно.
[02:00.000 --> 02:07.000]  Национальная институтная университетская школа экономики, Университет Лондона.
[02:07.000 --> 02:11.000]  Панас Пардалос, Университет Флориды.
[02:11.000 --> 02:16.000]  Олег Полин, Московский Университет физики и технологии.
[02:16.000 --> 02:23.000]  Я оцениваю наш директор Андрей Райгородки, Московский Университет физики и технологии.
[02:23.000 --> 02:27.000]  Реб Роберт, Радгерс Университет.
[02:27.000 --> 02:31.000]  Тревор Пеннер, Университет Лондона.
[02:31.000 --> 02:33.000]  И Маргарита Шептова.
[02:33.000 --> 02:42.000]  Пожалуйста, аплодируйте всем людям, которые действительно поддерживают и сделают эту конференцию возможной.
[02:42.000 --> 02:51.000]  И я бы хотел бы поделиться моим величайшим приветом к профессору Борису Милкину.
[02:51.000 --> 02:57.000]  И в этом моменте я надеюсь, что мы будем наслаждаться сегодня.
[02:57.000 --> 03:05.000]  И я поздравляю всех и спасибо, что вы нашли время, чтобы участвовать в этой конференции.
[03:05.000 --> 03:07.000]  Большое спасибо.
[03:10.000 --> 03:16.000]  Наш следующий директор будет Борису Борису Борису Борису Борису Борису.
[03:16.000 --> 03:19.000]  Самому Борису Ивсяковичу и его руководству.
[03:19.000 --> 03:22.000]  Прежде всего, конечно, в лице кафедры.
[03:22.000 --> 03:30.000]  Руководство кафедры, которое выделило и помещение, и средства, и прочее.
[03:30.000 --> 03:34.000]  Это очень трогательно, на самом деле.
[03:34.000 --> 03:41.000]  Я почему-то это говорю, потому что я не уверен, что у меня будет возможность вот так официально поблагодарить руководство.
[03:41.000 --> 03:46.000]  А в конце концов, именно оно определяет наши возможности.
[03:50.000 --> 03:53.000]  Так же самое, частенько даже само существование.
[03:54.000 --> 03:56.000]  Я не уверен.
[03:56.000 --> 04:00.000]  Один из моих боссов всегда смеется надо мной.
[04:00.000 --> 04:06.000]  Я говорю, я хорошо помню порядки Советского Союза, и я придерживаюсь этих порядков.
[04:06.000 --> 04:10.000]  Пока что ни разу мне никто не наказал.
[04:11.000 --> 04:18.000]  Это моя гипотеза, что нынешняя Россия в каких-то аспектах не сильно отличается.
[04:18.000 --> 04:24.000]  И с этими словами, все-таки, наука должна продолжаться.
[04:24.000 --> 04:28.000]  К сожалению, может быть, ни одно слово, что я вижу.
[04:28.000 --> 04:38.000]  На самом деле, я вынужден согласиться с покойным Олегом Ивановичем Ларичевым, который меня очень поддерживал всегда.
[04:38.000 --> 04:48.000]  И когда я вернулся из 20-летних отъездов к центрам научным, он мне сказал, все, наука в России кончилась.
[04:48.000 --> 04:57.000]  Я был значительно моложе и значительно более оптимистичен тогда.
[04:57.000 --> 05:04.000]  Я возразил, но никакие мои возражения не действовали.
[05:04.000 --> 05:09.000]  И сейчас я вижу, что действительно какие-то у нас серьезные есть проблемы.
[05:09.000 --> 05:15.000]  Недавно я слушал обзор публикации в нашей области.
[05:15.000 --> 05:23.000]  Оказалось, что Россия на последнем месте из примерно 80 стран, включая всю Европу.
[05:23.000 --> 05:32.000]  Даже такие маленькие страны, как Словакия, оказывается, дают больше вклад в литературу, чем Россия.
[05:32.000 --> 05:41.000]  Ну вот те, кто здесь присутствует, это, конечно, надежда и опора нашей науки в этой области.
[05:41.000 --> 05:49.000]  И я надеюсь, что эта конференция послужит делу не замирания науки. Спасибо большое.
[06:02.000 --> 06:07.000]  У нас есть еще несколько прекрасных книг, которые были публикованы профессором Миркиным.
[06:07.000 --> 06:14.000]  Начиная с этого, к сожалению, резолюция была довольно плохая.
[06:14.000 --> 06:18.000]  Проблемы.
[06:18.000 --> 06:27.000]  Первый книжок, который был очень популярным в советской математике, был, например,
[06:27.000 --> 06:31.000]  «Старый воздух».
[06:31.000 --> 06:41.000]  Вместе с классическими учреждениями, делом с деференцией, кальфилатом,
[06:41.000 --> 06:46.000]  с probabilностью.
[06:46.000 --> 06:55.000]  Сегодня я буду говорить об одной ноте, которая была произведена профессором Миркиным.
[06:55.000 --> 07:01.000]  Именно эта нота о кластеринге, также известной в литературе как «ко-кластеринг»,
[07:01.000 --> 07:08.000]  и обычно проявляется в литературе, как проповедовано Борисом Миркиным.
[07:08.000 --> 07:14.000]  И, действительно...
[07:14.000 --> 07:16.000]  И, действительно...
[07:16.000 --> 07:21.000]  Пожалуйста, не забудьте.
[07:21.000 --> 07:29.000]  И, действительно, первая бумага о кластеринге была из 1978 года,
[07:29.000 --> 07:32.000]  и она была произведена с Федором Растопцевым,
[07:32.000 --> 07:43.000]  где концепция пары самых талантливых аттрибутных тапсетов была первой в анализе социологических данных.
[07:43.000 --> 07:51.000]  И потом, правильно определённо, «б-кластеринг» и даже «три-кластеринг»,
[07:51.000 --> 07:59.000]  как и «три-доменциальные данные», эти термины были представлены в известном книге в 1996 году,
[07:59.000 --> 08:06.000]  где «б-кластеринг» описывается как «символтаническое кластеринг» для двух ролей и колонн-сетов в данной метрике.
[08:06.000 --> 08:13.000]  «Б-кластеринг» относится к проблемам агрегации, а также к основным функциям интеррелации между ролями и колонн-сетами,
[08:13.000 --> 08:15.000]  как выяснено в данной метрике.
[08:15.000 --> 08:22.000]  Здесь мы видим основную идею. «Б-кластеринг» это не один-доменциальная вещь.
[08:22.000 --> 08:28.000]  Нужно иметь две компоненты, и мы не можем поймать «б-кластеринг» с одной.
[08:28.000 --> 08:30.000]  Нужно иметь две руки.
[08:30.000 --> 08:36.000]  На одной руке есть роли, а на другой руке есть колонны.
[08:36.000 --> 08:41.000]  Я думаю, что эта идея довольно очевидна.
[08:41.000 --> 08:51.000]  Но я бы сказал, что «б-кластеринг» был специализирован в многих разных смыслах.
[08:51.000 --> 08:58.000]  Здесь есть очень хорошая ревью.
[08:58.000 --> 09:02.000]  Думаю, что здесь есть еще несколько ревьев.
[09:02.000 --> 09:09.000]  Но эта ревва стоит читать «Материя и Левера. Б-кластеринг-алгоритмы для биологической данной аналитики».
[09:09.000 --> 09:14.000]  Так что некоторые теоретические проблемы также дискуссированы.
[09:14.000 --> 09:21.000]  Например, один из самых использованных моделей «б-кластеринга» – это «б-кластеринг» абсолютно подобных значений,
[09:21.000 --> 09:33.000] specifically defined as inclusion maximal subā Kh-ma.
[09:33.000 --> 09:49.000]  The same way clusters of higher dimension 3, 4, etc. are defined.
[09:49.000 --> 09:56.000]  И здесь мы приходим к одной специализации кластера,
[09:56.000 --> 09:58.000]  называемой формальной концепцией.
[09:58.000 --> 10:04.000]  Подсмотрим этот простой пример с объектами в ролях
[10:04.000 --> 10:06.000]  и аттрибутами в колоннах.
[10:06.000 --> 10:08.000]  Аттрибуты по бинарии.
[10:08.000 --> 10:11.000]  И здесь вы видите...
[10:11.000 --> 10:14.000]  Я бы назвал это rigid-by-cluster.
[10:14.000 --> 10:19.000]  Аттрибуты 3,4, ровы 3,4 и аттрибуты PC.
[10:19.000 --> 10:24.000]  Аттрибуты по контейментам, подсмотрены с кроссом.
[10:24.000 --> 10:27.000]  Вы не можете подсмотреть ровы или колонны,
[10:27.000 --> 10:30.000]  без получения гаупта.
[10:30.000 --> 10:32.000]  Если вы пытаетесь добавить 2,
[10:32.000 --> 10:36.000]  то вы получите гаупт здесь для аттрибута D.
[10:36.000 --> 10:42.000]  Аттрибуты подсмотрены с кроссом.
[10:43.000 --> 10:46.000]  И это называется формальный контекст
[10:46.000 --> 10:51.000]  по основанию этой дирекции Рудольфа Виле.
[10:54.000 --> 10:57.000]  Но это имеет очень долгую историю,
[10:57.000 --> 11:01.000]  даже в философии, в европейской участии,
[11:01.000 --> 11:04.000]  в «Логике для Рояла»,
[11:04.000 --> 11:07.000]  написанной Дмитрием Норимпием Николь,
[11:07.000 --> 11:10.000]  они представили, что это концепция,
[11:11.000 --> 11:15.000]  концепция, как объем и содержание.
[11:20.000 --> 11:24.000]  И они заметили, что реверс,
[11:24.000 --> 11:27.000]  реверс между объемом и содержанием.
[11:27.000 --> 11:32.000]  Чем больше объем, тем меньше содержание.
[11:33.000 --> 11:36.000]  И потом,
[11:36.000 --> 11:40.000]  Эварист Кулуа представил несколько релаксов,
[11:40.000 --> 11:43.000]  несколько маппингов между
[11:43.000 --> 11:46.000]  сетями полиномерных групп
[11:46.000 --> 11:48.000]  аутомаркетов,
[11:48.000 --> 11:53.000]  которые были в 20-м веке
[11:53.000 --> 11:55.000]  генерализированными абстрактными
[11:55.000 --> 11:58.000]  Кулуа-коннекциями.
[11:58.000 --> 12:01.000]  Здесь были замечательные слова Пирхова,
[12:01.000 --> 12:04.000]  Оре, Вадима, Жарде,
[12:04.000 --> 12:06.000]  и, наконец,
[12:06.000 --> 12:09.000]  Робот Билли написал хорошую интерпретацию
[12:09.000 --> 12:11.000]  о том, что делает пара
[12:11.000 --> 12:14.000]  гулуа-коннекций.
[12:16.000 --> 12:19.000]  Добавил формальную модель
[12:19.000 --> 12:21.000]  для идеи,
[12:21.000 --> 12:23.000]  написанную в 17-м веке
[12:23.000 --> 12:24.000]  Арно и Николин,
[12:24.000 --> 12:26.000]  «Логики для Рояла».
[12:26.000 --> 12:29.000]  Здесь, в форме,
[12:29.000 --> 12:32.000]  есть сетка
[12:32.000 --> 12:35.000]  объекта G
[12:35.000 --> 12:38.000]  и аттрибута M.
[12:38.000 --> 12:40.000]  Рудольф, до смерти,
[12:40.000 --> 12:42.000]  попросил нас
[12:42.000 --> 12:44.000]  сфотографировать
[12:44.000 --> 12:47.000]  Дегенштейн Дегенштейн и так далее.
[12:47.000 --> 12:49.000]  Так что я полюбил
[12:49.000 --> 12:51.000]  этот
[12:55.000 --> 12:56.000]  последний код
[12:56.000 --> 12:58.000]  и использовал эти буквы
[12:58.000 --> 13:00.000]  для объектов и аттрибутов.
[13:00.000 --> 13:03.000]  И здесь у нас две маппинги,
[13:03.000 --> 13:05.000]  из мощного сета объектов
[13:05.000 --> 13:07.000]  до мощного сета аттрибутов
[13:07.000 --> 13:09.000]  и из мощного сета аттребутов
[13:09.000 --> 13:11.000]  для мощного сета объектов.
[13:11.000 --> 13:13.000]  В коде A
[13:13.000 --> 13:15.000]  для сетка объектов A
[13:15.000 --> 13:17.000]  дает все аттрибуты,
[13:17.000 --> 13:19.000]  которые общаются с объектами A.
[13:19.000 --> 13:21.000]  А для сетка объектов B
[13:21.000 --> 13:23.000]  C of P
[13:23.000 --> 13:25.000]  дает вам сетку объектов,
[13:25.000 --> 13:27.000]  которые共атируют все аттрибуты от B.
[13:27.000 --> 13:29.000]  Они имеют хорошиеки
[13:29.000 --> 13:36.000]  которые были заметены в 17-м веке в Викторе Рояле.
[13:36.000 --> 13:42.000]  Если у вас есть большая дистанция, то у вас будет маленький дистанция.
[13:42.000 --> 13:46.000]  Например, у нас есть концепция Мамалия.
[13:46.000 --> 13:48.000]  Мои капитающие.
[13:48.000 --> 13:52.000]  Дистанция у всех этих животных.
[13:52.000 --> 13:56.000]  И дистанция — это их общий продукт.
[13:56.000 --> 14:00.000]  Итак, если вы берете другой концепцией, например, у Тёрбрета,
[14:00.000 --> 14:06.000]  то у вас будет большая дистанция, но маленький дистанция,
[14:06.000 --> 14:12.000]  потому что у Тёрбрета меньше общих атрибутов, чем у Мамалия.
[14:12.000 --> 14:18.000]  В Дамштадт-сколе Рудольфа Филе они заменяли эту КМС
[14:18.000 --> 14:22.000]  с одной первой нотацией,
[14:22.000 --> 14:24.000]  это слабое использование нотации,
[14:24.000 --> 14:26.000]  потому что это две разные нотации,
[14:26.000 --> 14:34.000]  но это делает, что вы их очень редко смешиваете.
[14:34.000 --> 14:38.000]  Но нотация становится намного более элегантной.
[14:38.000 --> 14:42.000]  Итак, сет от атрибутов называется Extend,
[14:42.000 --> 14:46.000]  сет от объектов называется Extend,
[14:46.000 --> 14:48.000]  сет от атрибутов называется Intent,
[14:48.000 --> 14:50.000]  сет от объектов называется Ad,
[14:50.000 --> 14:52.000]  сет от объектов называется Ad,
[14:52.000 --> 14:54.000]  сет от объектов называется Ad,
[14:54.000 --> 14:56.000]  сет от объектов называется Ad,
[14:56.000 --> 14:58.000]  сет от объектов называется Ad,
[14:58.000 --> 15:00.000]  сет от объектов называется Ad,
[15:00.000 --> 15:02.000]  сет от объектов называется Ad,
[15:02.000 --> 15:04.000]  сет от объектов называется Ad,
[15:04.000 --> 15:06.000]  сет от объектов называется Ad,
[15:06.000 --> 15:08.000]  сет от объектов называется Ad,
[15:08.000 --> 15:10.000]  сет от объектов называется Ad,
[15:10.000 --> 15:12.000]  сет от объектов называется Ad,
[15:12.000 --> 15:14.000]  сет от объектов называется Ad,
[15:14.000 --> 15:16.000]  сет от объектов называется Ad,
[15:16.000 --> 15:18.000]  сет от объектов называется Ad,
[15:18.000 --> 15:20.000]  сет от объектов называется Ad,
[15:20.000 --> 15:22.000]  сет от объектов называется Ad,
[15:22.000 --> 15:24.000]  сет от объектов называется Ad,
[15:24.000 --> 15:26.000]  сет от объектов называется Ad,
[15:26.000 --> 15:28.000]  сет от объектов называется Ad,
[15:28.000 --> 15:30.000]  сет от объектов называется Ad,
[15:30.000 --> 15:32.000]  сет от объектов называется Ad,
[15:32.000 --> 15:34.000]  сет от объектов называется Ad,
[15:34.000 --> 15:36.000]  сет от объектов называется Ad,
[15:36.000 --> 15:38.000]  сет от объектов называется Ad,
[15:38.000 --> 15:40.000]  сет от объектов называется Ad,
[15:40.000 --> 15:42.000]  сет от объектов называется Ad,
[15:42.000 --> 15:44.000]  сет от объектов называется Ad,
[15:44.000 --> 15:46.000]  сет от объектов называется Ad,
[15:46.000 --> 15:48.000]  сет от объектов называется Ad,
[15:48.000 --> 15:50.000]  сет от объектов называется Ad,
[15:50.000 --> 15:52.000]  сет от объектов называется Ad,
[15:52.000 --> 15:54.000]  сет от объектов называется Ad,
[15:54.000 --> 15:56.000]  сет от объектов называется Ad,
[15:56.000 --> 15:58.000]  сет от объектов называется Ad,
[15:58.000 --> 16:00.000]  сет от объектов называется Ad,
[16:00.000 --> 16:02.000]  сет от объектов называется Ad,
[16:02.000 --> 16:04.000]  сет от объектов называется Ad,
[16:04.000 --> 16:06.000]  сет от объектов называется Ad,
[16:06.000 --> 16:08.000]  сет от объектов называется Ad,
[16:08.000 --> 16:10.000]  сет от объектов называется Ad,
[16:10.000 --> 16:14.000]  В этом релизе
[16:14.000 --> 16:16.000]  создается партия,
[16:16.000 --> 16:18.000]  и больше того,
[16:18.000 --> 16:20.000]  это создает летисы.
[16:20.000 --> 16:22.000]  Летисы, которые означают,
[16:22.000 --> 16:24.000]  что каждое два концепта
[16:24.000 --> 16:26.000]  имеет инфинул
[16:26.000 --> 16:28.000]  и суперинфинул,
[16:28.000 --> 16:30.000]  которые уникальны,
[16:30.000 --> 16:32.000]  как вы видите здесь.
[16:32.000 --> 16:34.000]  Диаграмм этого партии.
[16:34.000 --> 16:36.000]  Также,
[16:36.000 --> 16:38.000]  Гуголь сказал,
[16:38.000 --> 16:40.000]  что его не нужно назвать
[16:40.000 --> 16:42.000]  пастой диаграммой,
[16:42.000 --> 16:44.000]  потому что Вирков использовал это
[16:44.000 --> 16:46.000]  гораздо раньше,
[16:46.000 --> 16:48.000]  но в литературе иногда
[16:48.000 --> 16:50.000]  это называют пастой диаграммой.
[16:50.000 --> 16:52.000]  Есть хорошие правила
[16:52.000 --> 16:54.000]  этих прайм и двумя праймами операций,
[16:54.000 --> 16:56.000]  двумя праймами,
[16:56.000 --> 16:58.000]  но это не главный момент сегодня.
[16:58.000 --> 17:00.000]  Здесь я бы хотел
[17:00.000 --> 17:02.000]  упомянуть,
[17:02.000 --> 17:04.000]  что концепты
[17:04.000 --> 17:06.000]  создают
[17:06.000 --> 17:08.000]  компрессию
[17:08.000 --> 17:10.000]  объекта
[17:10.000 --> 17:12.000]  и аттрибута.
[17:12.000 --> 17:14.000]  Здесь у нас есть
[17:14.000 --> 17:16.000]  эквивалентные классы
[17:18.000 --> 17:20.000]  от аттрибута
[17:20.000 --> 17:22.000]  и аттрибута, которые
[17:22.000 --> 17:24.000]  имеют одинаковое поддержание
[17:24.000 --> 17:26.000]  в объекте.
[17:26.000 --> 17:28.000]  Для этого они имеют
[17:28.000 --> 17:30.000]  одинаковое поддержание 1, 2, 3, 4,
[17:30.000 --> 17:32.000]  для этого есть один элемент
[17:32.000 --> 17:34.000]  эквивалентной классы
[17:34.000 --> 17:36.000]  и эти два элемента
[17:36.000 --> 17:38.000]  поддерживают 3, 4.
[17:40.000 --> 17:42.000]  Но эта зеленая
[17:42.000 --> 17:44.000]  относится к
[17:44.000 --> 17:46.000]  интенту, который является
[17:46.000 --> 17:48.000]  закрытым сетом эффекта,
[17:48.000 --> 17:50.000]  максимумом в этой
[17:50.000 --> 17:52.000]  эквивалентной классе,
[17:52.000 --> 17:54.000]  в соответствующей эквивалентной классе.
[17:54.000 --> 17:56.000]  Это максимум здесь,
[17:56.000 --> 17:58.000]  это здесь, это
[17:58.000 --> 18:00.000]  это уникально,
[18:00.000 --> 18:02.000]  это минимум, и так далее.
[18:04.000 --> 18:06.000]  Итак, в
[18:08.000 --> 18:10.000]  интенсивном
[18:10.000 --> 18:12.000]  классе аттрибутов
[18:12.000 --> 18:14.000]  создают компрессию
[18:16.000 --> 18:18.000]  возможных сетов
[18:18.000 --> 18:20.000]  аттрибутов с
[18:20.000 --> 18:22.000]  респектом к данным,
[18:22.000 --> 18:24.000]  даже по кругу.
[18:24.000 --> 18:26.000]  Здесь
[18:26.000 --> 18:28.000]  есть хорошие теории,
[18:28.000 --> 18:30.000]  что каждый сет
[18:30.000 --> 18:32.000]  может быть представлен
[18:32.000 --> 18:34.000]  как концептный сет,
[18:34.000 --> 18:36.000]  но это
[18:36.000 --> 18:38.000]  слайд, который
[18:38.000 --> 18:40.000]  более важен для нас,
[18:40.000 --> 18:42.000]  потому что это показывает,
[18:42.000 --> 18:44.000]  что концепт
[18:44.000 --> 18:46.000]  относится к
[18:46.000 --> 18:48.000]  депенденции в данном.
[18:48.000 --> 18:50.000]  Аттрибутов
[18:50.000 --> 18:52.000]  A implies B
[18:52.000 --> 18:54.000]  when A'
[18:54.000 --> 18:56.000]  is the subset of B'.
[18:56.000 --> 18:58.000]  So each object having all
[18:58.000 --> 19:00.000]  attributes from A also
[19:00.000 --> 19:02.000]  has all attributes from B.
[19:02.000 --> 19:04.000]  The implications
[19:04.000 --> 19:06.000]  satisfy Armstrong's rules,
[19:06.000 --> 19:08.000]  which are known in database
[19:08.000 --> 19:10.000]  theory for functional dependence.
[19:10.000 --> 19:12.000]  And actually they
[19:12.000 --> 19:14.000]  are sort of equivalent to
[19:14.000 --> 19:16.000]  horn theory, horn sub theory
[19:16.000 --> 19:18.000]  of
[19:18.000 --> 19:20.000]  propositional logic.
[19:20.000 --> 19:22.000]  So since we have
[19:22.000 --> 19:24.000]  these nice
[19:24.000 --> 19:26.000]  rules, we can think
[19:26.000 --> 19:28.000]  of implication bases,
[19:28.000 --> 19:30.000]  which are smallest
[19:30.000 --> 19:32.000]  subset of implications from which
[19:32.000 --> 19:34.000]  all other can be inferred
[19:34.000 --> 19:36.000]  using Armstrong.
[19:36.000 --> 19:38.000]  And there are efficient algorithm
[19:38.000 --> 19:40.000]  tools for computing implications.
[19:46.000 --> 19:48.000]  So here,
[19:48.000 --> 19:50.000]  these implications can be visualized
[19:50.000 --> 19:52.000]  in a list.
[19:52.000 --> 19:54.000]  For example, we have implication
[19:54.000 --> 19:56.000]  B implies
[19:56.000 --> 19:58.000]  C, because
[19:58.000 --> 20:00.000]  whenever you have B, you have also C.
[20:00.000 --> 20:02.000]  And that is seen as
[20:02.000 --> 20:04.000]  the concept corresponding to
[20:04.000 --> 20:06.000]  B
[20:06.000 --> 20:08.000]  is below
[20:08.000 --> 20:10.000]  the concept corresponding to C.
[20:10.000 --> 20:12.000]  This
[20:12.000 --> 20:14.000]  concept corresponds to B,
[20:14.000 --> 20:16.000]  because there is no concept
[20:16.000 --> 20:18.000]  with intent B,
[20:18.000 --> 20:20.000]  because E' gives 3, 4,
[20:20.000 --> 20:22.000]  and B4 gives B, C.
[20:24.000 --> 20:26.000]  So you can see this
[20:26.000 --> 20:28.000]  lattice as a compression
[20:28.000 --> 20:30.000]  of boolean lattice
[20:30.000 --> 20:32.000]  of all attributes
[20:32.000 --> 20:34.000]  or boolean lattice of all objects.
[20:36.000 --> 20:38.000]  In this, for example,
[20:38.000 --> 20:40.000]  in this two examples, it has
[20:40.000 --> 20:42.000]  nine elements, and the
[20:42.000 --> 20:44.000]  boolean lattice of the power
[20:44.000 --> 20:46.000]  set would have 16.
[20:46.000 --> 20:48.000]  But in practice,
[20:48.000 --> 20:50.000]  the difference can be much, much
[20:50.000 --> 20:52.000]  larger.
[20:52.000 --> 20:54.000]  So the compression is much higher.
[20:54.000 --> 20:56.000]  Not only this,
[20:58.000 --> 21:00.000]  these implications
[21:00.000 --> 21:02.000]  are
[21:02.000 --> 21:04.000]  I would call them deterministic
[21:04.000 --> 21:06.000]  dependencies, when
[21:06.000 --> 21:08.000]  you always have B if you have
[21:08.000 --> 21:10.000]  A. But in data analysis,
[21:10.000 --> 21:12.000]  you
[21:12.000 --> 21:14.000]  often have situations
[21:14.000 --> 21:16.000]  when you have B
[21:16.000 --> 21:18.000]  having A only with some
[21:18.000 --> 21:20.000]  probability. And this was
[21:20.000 --> 21:22.000]  captured in data mining
[21:22.000 --> 21:24.000]  under the name association rule.
[21:24.000 --> 21:26.000]  B is
[21:26.000 --> 21:28.000]  associated
[21:28.000 --> 21:30.000]  with A, with
[21:30.000 --> 21:32.000]  confidence, which is actually
[21:34.000 --> 21:36.000]  conditional probability
[21:36.000 --> 21:38.000]  of B having
[21:38.000 --> 21:40.000]  A. And S
[21:40.000 --> 21:42.000]  is the support.
[21:42.000 --> 21:44.000]  And it says how many rows
[21:44.000 --> 21:46.000]  justify
[21:46.000 --> 21:48.000]  this dependence,
[21:48.000 --> 21:50.000]  this association.
[21:50.000 --> 21:52.000]  And what is the minimal representation
[21:52.000 --> 21:54.000]  of the set of association rules?
[21:54.000 --> 21:56.000]  Because everything can be associated with
[21:56.000 --> 21:58.000]  everything. Power set on the left
[21:58.000 --> 22:00.000]  side and power set on the
[22:00.000 --> 22:02.000]  right side. But
[22:02.000 --> 22:04.000]  a nice,
[22:04.000 --> 22:06.000]  very nice and simple result shows
[22:06.000 --> 22:08.000]  that actually you can use
[22:08.000 --> 22:10.000]  the edges of
[22:10.000 --> 22:12.000]  the
[22:12.000 --> 22:14.000]  concept of the lattice diagram
[22:14.000 --> 22:16.000]  as a minimal representation
[22:16.000 --> 22:18.000]  of association rules.
[22:18.000 --> 22:20.000]  Note first
[22:20.000 --> 22:22.000]  that every edge
[22:22.000 --> 22:24.000]  is an association rule. For example,
[22:24.000 --> 22:26.000]  C implies B, C
[22:26.000 --> 22:28.000]  with probability 2
[22:28.000 --> 22:30.000]  over 3.
[22:30.000 --> 22:32.000]  B, C implies B, C, D with probability
[22:32.000 --> 22:34.000]  1 over 1.
[22:34.000 --> 22:36.000]  So on one direction,
[22:36.000 --> 22:38.000]  on the other direction.
[22:38.000 --> 22:40.000]  How would you go for this?
[22:40.000 --> 22:42.000]  So to
[22:42.000 --> 22:44.000]  store association rules, you
[22:44.000 --> 22:46.000]  need,
[22:46.000 --> 22:48.000]  you can use this diagram.
[22:48.000 --> 22:50.000]  This is the minimal representation of
[22:50.000 --> 22:52.000]  association. And if
[22:52.000 --> 22:54.000]  you want to
[22:56.000 --> 22:58.000]  make a threshold for support
[22:58.000 --> 23:00.000]  considering only
[23:00.000 --> 23:02.000]  highly supported rules, you notice
[23:02.000 --> 23:04.000]  that support goes down from
[23:04.000 --> 23:06.000]  top to bottom, and you
[23:06.000 --> 23:08.000]  need to make
[23:08.000 --> 23:10.000]  a cut somewhere
[23:10.000 --> 23:12.000]  to get as much
[23:12.000 --> 23:14.000]  best supported
[23:14.000 --> 23:16.000]  patterns
[23:16.000 --> 23:18.000]  as you like.
[23:18.000 --> 23:20.000]  I leave this empty because
[23:20.000 --> 23:22.000]  I think
[23:22.000 --> 23:24.000]  no time for formal
[23:24.000 --> 23:26.000]  considerations, but you
[23:26.000 --> 23:28.000]  understand that this can be generalized
[23:28.000 --> 23:30.000]  to K-dimensional
[23:30.000 --> 23:32.000]  data as well.
[23:34.000 --> 23:36.000]  And here we have, for example,
[23:36.000 --> 23:38.000]  a theorem about
[23:38.000 --> 23:40.000]  reducing by clusters
[23:40.000 --> 23:42.000]  of similar values
[23:42.000 --> 23:44.000]  to three concepts.
[23:46.000 --> 23:48.000]  Not nice to read.
[23:48.000 --> 23:50.000]  I'd rather show you this
[23:50.000 --> 23:52.000]  picture, which shows
[23:52.000 --> 23:54.000]  how the reduction goes.
[23:54.000 --> 23:56.000]  I used theta
[23:56.000 --> 23:58.000]  instead of epsilon.
[24:00.000 --> 24:02.000]  So here we have
[24:02.000 --> 24:04.000]  two by clusters of similar
[24:04.000 --> 24:06.000]  value for theta equal
[24:06.000 --> 24:08.000]  one. So the difference
[24:08.000 --> 24:10.000]  is no larger than one.
[24:10.000 --> 24:12.000]  This is another
[24:12.000 --> 24:14.000]  by cluster of similar value
[24:14.000 --> 24:16.000]  with three two, three two.
[24:16.000 --> 24:18.000]  You cannot merge them.
[24:20.000 --> 24:22.000]  But you
[24:22.000 --> 24:24.000]  can introduce another
[24:24.000 --> 24:26.000]  dimension, which is
[24:26.000 --> 24:28.000]  called the scale,
[24:28.000 --> 24:30.000]  interordinal scale, with
[24:30.000 --> 24:32.000]  binary attributes of this sort.
[24:34.000 --> 24:36.000]  And then by clusters
[24:36.000 --> 24:38.000]  become these
[24:38.000 --> 24:40.000]  parallelepipeds in
[24:40.000 --> 24:42.000]  three-dimensional data, but now
[24:42.000 --> 24:44.000]  they are binary. And you
[24:44.000 --> 24:46.000]  have all FCA machinery
[24:46.000 --> 24:48.000]  for computing.
[24:48.000 --> 24:50.000]  And as a by product,
[24:50.000 --> 24:52.000]  you reduce
[24:52.000 --> 24:54.000]  dependencies of this form,
[24:56.000 --> 24:58.000]  both deterministic and probabilistic,
[24:58.000 --> 25:00.000]  to implications
[25:00.000 --> 25:02.000]  on binary attributes.
[25:02.000 --> 25:04.000]  The implications and
[25:04.000 --> 25:06.000]  basis of them can be efficiently
[25:06.000 --> 25:08.000]  computed by existing FCA.
[25:08.000 --> 25:10.000]  FCA states for
[25:10.000 --> 25:12.000]  form of determinants.
[25:12.000 --> 25:14.000]  So with this
[25:14.000 --> 25:16.000]  I'm approaching
[25:16.000 --> 25:18.000]  the end.
[25:22.000 --> 25:24.000]  Why this
[25:24.000 --> 25:26.000]  reduction is good? Because we
[25:26.000 --> 25:28.000]  can use the whole
[25:28.000 --> 25:30.000]  machinery developed in the
[25:30.000 --> 25:32.000]  domain of formal concept analysis.
[25:32.000 --> 25:34.000]  And although
[25:34.000 --> 25:36.000]  the number of concepts can
[25:36.000 --> 25:38.000]  be exponential and even
[25:38.000 --> 25:40.000]  hard to estimate,
[25:40.000 --> 25:42.000]  which is
[25:42.000 --> 25:44.000]  given by the
[25:44.000 --> 25:46.000]  results of counting
[25:46.000 --> 25:48.000]  concepts,
[25:48.000 --> 25:50.000]  but there are polynomial delay
[25:50.000 --> 25:52.000]  algorithms for computing the concept.
[25:52.000 --> 25:54.000]  And nice
[25:54.000 --> 25:56.000]  anti-monotonicity property
[25:56.000 --> 25:58.000]  of natural interestingness
[25:58.000 --> 26:00.000]  measures like support.
[26:00.000 --> 26:02.000]  You can
[26:02.000 --> 26:04.000]  show this with this
[26:04.000 --> 26:06.000]  slide. If you want
[26:06.000 --> 26:08.000]  dependency,
[26:08.000 --> 26:10.000]  let's say, association
[26:10.000 --> 26:12.000]  rules with
[26:12.000 --> 26:14.000]  support higher than
[26:14.000 --> 26:16.000]  one half,
[26:16.000 --> 26:18.000]  you use your
[26:18.000 --> 26:20.000]  algorithm with
[26:20.000 --> 26:22.000]  polynomial delay.
[26:22.000 --> 26:24.000]  This one
[26:24.000 --> 26:26.000]  and this one.
[26:26.000 --> 26:28.000]  You can do this
[26:28.000 --> 26:30.000]  if you take k
[26:38.000 --> 26:40.000]  size and
[26:40.000 --> 26:42.000]  the number of k concepts
[26:42.000 --> 26:44.000]  that you want to take
[26:44.000 --> 26:46.000]  as the best one.
[26:46.000 --> 26:48.000]  So
[26:50.000 --> 26:52.000]  and
[26:52.000 --> 26:54.000]  dependencies on numerical
[26:54.000 --> 26:56.000]  data can be reduced
[26:56.000 --> 26:58.000]  to binary implications
[26:58.000 --> 27:00.000]  and computed by existing
[27:00.000 --> 27:02.000]  FCA
[27:02.000 --> 27:04.000]  in this
[27:06.000 --> 27:08.000]  slide. So with this
[27:08.000 --> 27:10.000]  I would like to finish and thank
[27:10.000 --> 27:12.000]  you for your attention
[27:12.000 --> 27:14.000]  and thank you Boris
[27:14.000 --> 27:16.000]  for being with us
[27:16.000 --> 27:18.000]  and stay long with you.
[27:18.000 --> 27:20.000]  Много и благая лета.
[27:22.000 --> 27:24.000]  Спасибо, коллеги.
[27:44.000 --> 27:46.000]  У меня что-то очень
[27:46.000 --> 27:48.000]  хорошее. Давайте посылайте,
[27:48.000 --> 27:50.000]  я попробую это все подготовить.
[27:50.000 --> 27:52.000]  Спасибо большое, Сергей.
[27:52.000 --> 27:54.000]  Спасибо большое. Вопросы?
[27:54.000 --> 27:56.000]  Вопросы. Или мы не предусмотрели?
[27:56.000 --> 27:58.000]  Предусмотрели, пожалуйста.
[27:58.000 --> 28:00.000]  Да.
[28:00.000 --> 28:02.000]  Если заинтересовала эта тематика,
[28:02.000 --> 28:04.000]  то могу вот предложить такую
[28:04.000 --> 28:06.000]  списку литературы.
[28:06.000 --> 28:08.000]  Thank you.
[28:08.000 --> 28:10.000]  Thank you very much.
[28:10.000 --> 28:12.000]  My talk
[28:12.000 --> 28:14.000]  is a kind of
[28:14.000 --> 28:16.000]  review
[28:16.000 --> 28:18.000]  of clustering
[28:18.000 --> 28:20.000]  as
[28:20.000 --> 28:22.000]  classification
[28:22.000 --> 28:24.000]  activity.
[28:24.000 --> 28:26.000]  In machine learning
[28:26.000 --> 28:28.000]  clustering is considered
[28:28.000 --> 28:30.000]  as a kind of
[28:30.000 --> 28:32.000]  auxiliary
[28:32.000 --> 28:34.000]  activity
[28:34.000 --> 28:36.000]  thing.
[28:36.000 --> 28:38.000]  I have
[28:38.000 --> 28:40.000]  a
[28:40.000 --> 28:42.000]  taxonomy
[28:42.000 --> 28:44.000]  of computer sciences
[28:44.000 --> 28:46.000]  created by the
[28:46.000 --> 28:48.000]  Association for
[28:48.000 --> 28:50.000]  Computing Machinery.
[28:50.000 --> 28:52.000]  And clustering never
[28:52.000 --> 28:54.000]  raises in there
[28:54.000 --> 28:56.000]  higher than
[28:56.000 --> 28:58.000]  second level from bottom.
[29:00.000 --> 29:02.000]  Which is, in my view,
[29:02.000 --> 29:04.000]  is wrong.
[29:04.000 --> 29:06.000]  And I'm looking now
[29:06.000 --> 29:08.000]  at the possibility
[29:10.000 --> 29:12.000]  of what I'm doing.
[29:14.000 --> 29:16.000]  I want to
[29:18.000 --> 29:20.000]  I want to move
[29:20.000 --> 29:22.000]  to move the slide.
[29:24.000 --> 29:26.000]  Вот это.
[29:30.000 --> 29:32.000]  Где стрелка?
[29:32.000 --> 29:34.000]  Здесь ее нет?
[29:36.000 --> 29:38.000]  Ну ладно.
[29:38.000 --> 29:40.000]  Я буду
[29:40.000 --> 29:42.000]  так двигаться.
[29:42.000 --> 29:44.000]  Thank you.
[29:44.000 --> 29:46.000]  So first I'll
[29:46.000 --> 29:48.000]  discuss a few words about
[29:48.000 --> 29:50.000]  what is classification.
[29:50.000 --> 29:52.000]  Then about clustering
[29:52.000 --> 29:54.000]  view this way.
[29:54.000 --> 29:56.000]  And then of some
[29:56.000 --> 29:58.000]  of the
[29:58.000 --> 30:00.000]  some of the
[30:00.000 --> 30:02.000]  some of the
[30:02.000 --> 30:04.000]  some of the
[30:04.000 --> 30:06.000]  and then of some
[30:06.000 --> 30:08.000]  current
[30:08.000 --> 30:10.000]  issues related
[30:10.000 --> 30:12.000]  to this
[30:12.000 --> 30:14.000]  presentation.
[30:16.000 --> 30:18.000]  So classification as a general
[30:18.000 --> 30:20.000]  form.
[30:20.000 --> 30:22.000]  I studied this
[30:22.000 --> 30:24.000]  when I was writing
[30:24.000 --> 30:26.000]  this book in Dimex
[30:26.000 --> 30:28.000]  in the
[30:28.000 --> 30:30.000]  University of New Jersey.
[30:30.000 --> 30:32.000]  I spent a year
[30:32.000 --> 30:34.000]  or more studying what
[30:34.000 --> 30:36.000]  classification was.
[30:36.000 --> 30:38.000]  So it's a division of objects
[30:38.000 --> 30:40.000]  oriented for
[30:40.000 --> 30:42.000]  structuring the phenomenon
[30:42.000 --> 30:44.000]  under consideration,
[30:44.000 --> 30:46.000]  establishing relations between
[30:46.000 --> 30:48.000]  aspects of this phenomenon,
[30:48.000 --> 30:50.000]  and keeping knowledge.
[30:50.000 --> 30:52.000]  So I've written
[30:52.000 --> 30:54.000]  a book, first part
[30:54.000 --> 30:56.000]  of this book is devoted
[30:56.000 --> 30:58.000]  to the concept of classification.
[30:58.000 --> 31:00.000]  A few examples.
[31:00.000 --> 31:02.000]  Say in biology
[31:04.000 --> 31:06.000]  we have here
[31:06.000 --> 31:08.000]  several lines showing
[31:08.000 --> 31:10.000]  development of
[31:10.000 --> 31:12.000]  considerations.
[31:12.000 --> 31:14.000]  So the current system
[31:14.000 --> 31:16.000]  is a very recent
[31:16.000 --> 31:18.000]  one.
[31:18.000 --> 31:20.000]  Literally 20 years ago
[31:20.000 --> 31:22.000]  due to the activities
[31:22.000 --> 31:24.000]  of Wuze who
[31:24.000 --> 31:26.000]  understood that there is
[31:26.000 --> 31:28.000]  a specific class
[31:28.000 --> 31:30.000]  of living
[31:30.000 --> 31:32.000]  organism Achaia
[31:32.000 --> 31:34.000]  which I have
[31:34.000 --> 31:36.000]  no time to
[31:36.000 --> 31:38.000]  devote to this.
[31:40.000 --> 31:42.000]  So the
[31:42.000 --> 31:44.000]  what was good about
[31:44.000 --> 31:46.000]  this classification?
[31:46.000 --> 31:48.000]  So the
[31:48.000 --> 31:50.000]  Linneus
[31:50.000 --> 31:52.000]  introduced
[31:52.000 --> 31:54.000]  a taxonomy
[31:54.000 --> 31:56.000]  over a productive
[31:56.000 --> 31:58.000]  organism which was very lucky
[31:58.000 --> 32:00.000]  because Charles Darwin could
[32:00.000 --> 32:02.000]  reinterpret this as
[32:02.000 --> 32:04.000]  an evolution, as a tree
[32:04.000 --> 32:06.000]  of life, evolution of survival
[32:06.000 --> 32:08.000]  of the fittest, and all this
[32:08.000 --> 32:10.000]  related to this I have no
[32:10.000 --> 32:12.000]  time to discuss, but here we see
[32:12.000 --> 32:14.000]  how important this
[32:14.000 --> 32:16.000]  classification was.
[32:16.000 --> 32:18.000]  Other classifications created by
[32:18.000 --> 32:20.000]  Linneus, including
[32:20.000 --> 32:22.000]  mineralogy,
[32:22.000 --> 32:24.000]  also along these lines
[32:24.000 --> 32:26.000]  had no successes
[32:26.000 --> 32:28.000]  because the
[32:28.000 --> 32:30.000]  productive organs of minerals
[32:30.000 --> 32:32.000]  were wrongly identified
[32:32.000 --> 32:34.000]  by him.
[32:34.000 --> 32:36.000]  Another example
[32:36.000 --> 32:38.000]  which we as Russians
[32:38.000 --> 32:40.000]  should be very proud of
[32:40.000 --> 32:42.000]  is
[32:42.000 --> 32:44.000]  periodic chart
[32:44.000 --> 32:46.000]  by Mendeleev.
[32:46.000 --> 32:48.000]  The structure of
[32:48.000 --> 32:50.000]  the elementary
[32:50.000 --> 32:52.000]  particles or atoms
[32:52.000 --> 32:54.000]  I don't know if they are not elementary,
[32:54.000 --> 32:56.000]  then the structure
[32:56.000 --> 32:58.000]  allows
[32:58.000 --> 33:00.000]  to say a lot of
[33:00.000 --> 33:02.000]  physical and chemical properties
[33:02.000 --> 33:04.000]  of them.
[33:04.000 --> 33:06.000]  And this is another
[33:06.000 --> 33:08.000]  example where the structure of
[33:08.000 --> 33:10.000]  protein can tell you
[33:10.000 --> 33:12.000]  a lot of the
[33:12.000 --> 33:14.000]  properties of the activities
[33:14.000 --> 33:16.000]  of this. And
[33:16.000 --> 33:18.000]  one more example is
[33:18.000 --> 33:20.000]  max classification
[33:20.000 --> 33:22.000]  that
[33:22.000 --> 33:24.000]  he claimed
[33:24.000 --> 33:26.000]  in my view wrongly
[33:26.000 --> 33:28.000]  that there is a zero
[33:28.000 --> 33:30.000]  sum game of
[33:30.000 --> 33:32.000]  two classes,
[33:32.000 --> 33:34.000]  capitalist and proletarians,
[33:34.000 --> 33:36.000]  and all this sociology
[33:36.000 --> 33:38.000]  and politics
[33:38.000 --> 33:40.000]  implied by this.
[33:40.000 --> 33:42.000]  So the
[33:42.000 --> 33:44.000]  what is important maybe to point
[33:44.000 --> 33:46.000]  out that classification is not
[33:46.000 --> 33:48.000]  science.
[33:48.000 --> 33:50.000]  It was introduced by the great
[33:50.000 --> 33:52.000]  Aristotle and was
[33:52.000 --> 33:54.000]  thrown out
[33:54.000 --> 33:56.000]  at the very
[33:56.000 --> 33:58.000]  dawn of the sciences
[33:58.000 --> 34:00.000]  with a lot of other
[34:00.000 --> 34:02.000]  speculations.
[34:02.000 --> 34:04.000]  This is why
[34:04.000 --> 34:06.000]  I think the greatest minds
[34:06.000 --> 34:08.000]  like Mendeleev and
[34:08.000 --> 34:10.000]  Wuze never had any
[34:10.000 --> 34:12.000]  chance to receive a
[34:12.000 --> 34:14.000]  Nobel Prize.
[34:14.000 --> 34:16.000]  Because Nobel Prize is given for discovery
[34:16.000 --> 34:18.000]  of facts of not
[34:18.000 --> 34:20.000]  the regularities.
[34:24.000 --> 34:26.000]  I also kind of came
[34:26.000 --> 34:28.000]  after many thinking, much
[34:28.000 --> 34:30.000]  thinking, came to conclusion
[34:30.000 --> 34:32.000]  that there are only five
[34:32.000 --> 34:34.000]  aspects of any process
[34:34.000 --> 34:36.000]  and these aspects are structure,
[34:36.000 --> 34:38.000]  function and history.
[34:38.000 --> 34:40.000]  If we are talking of
[34:40.000 --> 34:42.000]  the nature and if
[34:42.000 --> 34:44.000]  we talk of the societies,
[34:44.000 --> 34:46.000]  then we should include here
[34:46.000 --> 34:48.000]  attitude and action
[34:48.000 --> 34:50.000]  to other aspects.
[34:50.000 --> 34:52.000]  And
[34:52.000 --> 34:54.000]  so this
[34:54.000 --> 34:56.000]  the relating classifications
[34:56.000 --> 34:58.000]  I've given you already
[34:58.000 --> 35:00.000]  that Mendeleev's periodic chart
[35:00.000 --> 35:02.000]  gives you relation between structure
[35:02.000 --> 35:04.000]  and function. In biology
[35:04.000 --> 35:06.000]  the structure and function
[35:06.000 --> 35:08.000]  and in politics
[35:08.000 --> 35:10.000]  this at least maxim
[35:10.000 --> 35:12.000]  taxonomy
[35:12.000 --> 35:14.000]  gave you structure
[35:14.000 --> 35:16.000]  related to
[35:16.000 --> 35:18.000]  attitude and action.
[35:20.000 --> 35:22.000]  So this was about the goal
[35:22.000 --> 35:24.000]  of classification.
[35:24.000 --> 35:26.000]  Now about the forms.
[35:26.000 --> 35:28.000]  I know basically only of three forms
[35:28.000 --> 35:30.000]  of classification.
[35:30.000 --> 35:32.000]  Taxonomy, which is mathematically
[35:32.000 --> 35:34.000]  rooted tree.
[35:34.000 --> 35:36.000]  Typology,
[35:36.000 --> 35:38.000]  which is a partition
[35:38.000 --> 35:40.000]  or even
[35:40.000 --> 35:42.000]  just representatives
[35:42.000 --> 35:44.000]  of
[35:44.000 --> 35:46.000]  hidden partition
[35:46.000 --> 35:48.000]  and ranking.
[35:50.000 --> 35:52.000]  And here I give examples like very popular
[35:52.000 --> 35:54.000]  currently league tables
[35:56.000 --> 35:58.000]  of universities
[35:58.000 --> 36:00.000]  and of scientists.
[36:02.000 --> 36:04.000]  And now I want to look
[36:04.000 --> 36:06.000]  at
[36:06.000 --> 36:08.000]  the clustering.
[36:08.000 --> 36:10.000]  How it works?
[36:10.000 --> 36:12.000]  So for me clustering
[36:12.000 --> 36:14.000]  is a hierarchical classification.
[36:14.000 --> 36:16.000]  If classification
[36:16.000 --> 36:18.000]  is done according to
[36:18.000 --> 36:20.000]  theoretical views
[36:20.000 --> 36:22.000]  over the universe,
[36:22.000 --> 36:24.000]  then clustering is done over
[36:24.000 --> 36:26.000]  data, over data table
[36:26.000 --> 36:28.000]  which is a very
[36:28.000 --> 36:30.000]  partial knowledge.
[36:32.000 --> 36:34.000]  And still we have a very similar
[36:34.000 --> 36:36.000]  two structures.
[36:36.000 --> 36:38.000]  It's a hierarchy
[36:38.000 --> 36:40.000]  akin to taxonomy
[36:40.000 --> 36:42.000]  and partition,
[36:42.000 --> 36:44.000]  which is akin to typology.
[36:44.000 --> 36:46.000]  And also clustering gives us
[36:46.000 --> 36:48.000]  one more structure
[36:48.000 --> 36:50.000]  which is not considered
[36:50.000 --> 36:52.000]  in classification theory.
[36:52.000 --> 36:54.000]  One cluster structure
[36:54.000 --> 36:56.000]  or single cluster.
[36:56.000 --> 36:58.000]  It's original structure
[36:58.000 --> 37:00.000]  because when
[37:00.000 --> 37:02.000]  we apply this
[37:02.000 --> 37:04.000]  to data of relatively
[37:04.000 --> 37:06.000]  unexplored phenomena
[37:06.000 --> 37:08.000]  we don't have all the clusters.
[37:08.000 --> 37:10.000]  We have only one or few
[37:10.000 --> 37:12.000]  and this is what we can explore.
[37:14.000 --> 37:16.000]  About ranking.
[37:16.000 --> 37:18.000]  This is a very different matter.
[37:18.000 --> 37:20.000]  That ranking is
[37:20.000 --> 37:22.000]  part of what is considered
[37:22.000 --> 37:24.000]  decision making,
[37:24.000 --> 37:26.000]  not data analysis or statistics.
[37:26.000 --> 37:28.000]  So in decision making
[37:28.000 --> 37:30.000]  there are very different approaches.
[37:30.000 --> 37:32.000]  And
[37:32.000 --> 37:34.000]  I was trying
[37:34.000 --> 37:36.000]  to
[37:36.000 --> 37:38.000]  kind of
[37:38.000 --> 37:40.000]  reconcile
[37:40.000 --> 37:42.000]  to put ranking
[37:42.000 --> 37:44.000]  into the clustering perspective.
[37:50.000 --> 37:52.000]  And a few words about this.
[37:54.000 --> 37:56.000]  This is how
[37:56.000 --> 37:58.000]  the problem looks like.
[37:58.000 --> 38:00.000]  Here I have
[38:02.000 --> 38:04.000]  data of about
[38:04.000 --> 38:06.000]  2006.
[38:06.000 --> 38:08.000]  I've taken from a book about
[38:10.000 --> 38:12.000]  biggest
[38:14.000 --> 38:16.000]  cities in the world.
[38:16.000 --> 38:18.000]  And here we have two features.
[38:18.000 --> 38:20.000]  Housing,
[38:20.000 --> 38:22.000]  which is the price
[38:22.000 --> 38:24.000]  of a three-star
[38:24.000 --> 38:26.000]  hotel
[38:26.000 --> 38:28.000]  one night.
[38:28.000 --> 38:30.000]  And food is
[38:30.000 --> 38:32.000]  price of
[38:34.000 --> 38:36.000]  McDonald's
[38:36.000 --> 38:38.000]  burger.
[38:40.000 --> 38:42.000]  I can't remember
[38:42.000 --> 38:44.000]  unfortunately. Big Mac, yes.
[38:44.000 --> 38:46.000]  Thank you very much.
[38:46.000 --> 38:48.000]  Price of Big Mac.
[38:50.000 --> 38:52.000]  So when we look at
[38:52.000 --> 38:54.000]  this, I've taken
[38:54.000 --> 38:56.000]  10 largest cities.
[38:56.000 --> 38:58.000]  And clustering is
[38:58.000 --> 39:00.000]  on the left part.
[39:00.000 --> 39:02.000]  So according to the similarity we can see
[39:02.000 --> 39:04.000]  that Copenhagen is
[39:04.000 --> 39:06.000]  a unique position.
[39:06.000 --> 39:08.000]  It's tremendously
[39:08.000 --> 39:10.000]  expensive over food.
[39:10.000 --> 39:12.000]  And not that expensive
[39:12.000 --> 39:14.000]  over housing.
[39:14.000 --> 39:16.000]  Unlike the Moscow, Tokyo and
[39:16.000 --> 39:18.000]  New York in which housing is
[39:18.000 --> 39:20.000]  extremely expensive.
[39:20.000 --> 39:22.000]  But the food is much less.
[39:22.000 --> 39:24.000]  So when we look
[39:24.000 --> 39:26.000]  from the position of stratification,
[39:26.000 --> 39:28.000]  then the stratification
[39:28.000 --> 39:30.000]  is much different.
[39:30.000 --> 39:32.000]  It is on the right.
[39:32.000 --> 39:34.000]  So we should draw a line
[39:34.000 --> 39:36.000]  and axis along
[39:36.000 --> 39:38.000]  which, not along,
[39:38.000 --> 39:40.000]  orthogonal to
[39:40.000 --> 39:42.000]  which stratas
[39:42.000 --> 39:44.000]  going.
[39:44.000 --> 39:46.000]  And then London and Copenhagen
[39:46.000 --> 39:48.000]  will go into the same strata.
[39:48.000 --> 39:50.000]  But Sydney
[39:50.000 --> 39:52.000]  and Beijing
[39:52.000 --> 39:54.000]  would go into the middle,
[39:54.000 --> 39:56.000]  a medium strata.
[39:56.000 --> 39:58.000]  Not in that cluster which is
[39:58.000 --> 40:00.000]  on the left part.
[40:00.000 --> 40:02.000]  So this stratification
[40:02.000 --> 40:04.000]  is a bit different
[40:04.000 --> 40:06.000]  activity.
[40:06.000 --> 40:08.000]  I made
[40:08.000 --> 40:10.000]  a criterion similar
[40:10.000 --> 40:12.000]  to, oh,
[40:12.000 --> 40:14.000]  2K means one.
[40:20.000 --> 40:22.000]  Okay, I did something wrong.
[40:26.000 --> 40:28.000]  Yeah, okay.
[40:28.000 --> 40:30.000]  So in the down,
[40:30.000 --> 40:32.000]  in the bottom part is this criterion
[40:32.000 --> 40:34.000]  that we analyzed
[40:34.000 --> 40:36.000]  with my PhD student
[40:36.000 --> 40:38.000]  Michael Orlov
[40:38.000 --> 40:40.000]  who successfully defended
[40:40.000 --> 40:42.000]  his PhD. And we found
[40:42.000 --> 40:44.000]  wonderful results over this.
[40:44.000 --> 40:46.000]  Unfortunately, neither
[40:46.000 --> 40:48.000]  myself, not
[40:48.000 --> 40:50.000]  nobody,
[40:50.000 --> 40:52.000]  nor anybody else
[40:52.000 --> 40:54.000]  would pick this up.
[40:54.000 --> 40:56.000]  So it's basically
[40:56.000 --> 40:58.000]  this direction I consider is dead.
[41:00.000 --> 41:02.000]  Now about
[41:04.000 --> 41:06.000]  classification goals.
[41:06.000 --> 41:08.000]  How they are reflected in the clustering.
[41:08.000 --> 41:10.000]  The structuring
[41:10.000 --> 41:12.000]  is exactly the goal
[41:12.000 --> 41:14.000]  of clustering.
[41:14.000 --> 41:16.000]  So predominantly clustering is
[41:16.000 --> 41:18.000]  structuring.
[41:18.000 --> 41:20.000]  About establishing relations.
[41:20.000 --> 41:22.000]  So far clustering
[41:22.000 --> 41:24.000]  is used tremendously
[41:24.000 --> 41:26.000]  little.
[41:26.000 --> 41:28.000]  Very few projects
[41:28.000 --> 41:30.000]  relate to this.
[41:30.000 --> 41:32.000]  As to the
[41:32.000 --> 41:34.000]  forming and keeping knowledge,
[41:34.000 --> 41:36.000]  I think it's not time.
[41:36.000 --> 41:38.000]  Time has not
[41:38.000 --> 41:40.000]  come yet. We have very
[41:40.000 --> 41:42.000]  rare attempts at automatic
[41:42.000 --> 41:44.000]  building taxonomies
[41:44.000 --> 41:46.000]  from
[41:46.000 --> 41:48.000]  tax collections.
[41:48.000 --> 41:50.000]  Not quite successful,
[41:50.000 --> 41:52.000]  but this is
[41:52.000 --> 41:54.000]  an area of
[41:54.000 --> 41:56.000]  current activity of
[41:56.000 --> 41:58.000]  few people.
[42:00.000 --> 42:02.000]  My own attempts in this direction
[42:02.000 --> 42:04.000]  that
[42:04.000 --> 42:06.000]  when I was
[42:06.000 --> 42:08.000]  head of a department,
[42:08.000 --> 42:10.000]  sorry, of a laboratory
[42:10.000 --> 42:12.000]  for data analysis,
[42:12.000 --> 42:14.000]  then we introduced
[42:14.000 --> 42:16.000]  the concept of relative
[42:16.000 --> 42:18.000]  grouping in our
[42:18.000 --> 42:20.000]  program system.
[42:20.000 --> 42:22.000]  And it did work.
[42:22.000 --> 42:24.000]  Unfortunately, it's all
[42:24.000 --> 42:26.000]  disappeared, including the
[42:26.000 --> 42:28.000]  main person
[42:28.000 --> 42:30.000]  who did it, this
[42:30.000 --> 42:32.000]  Peter Rostovtsev.
[42:32.000 --> 42:34.000]  He died 20 years
[42:34.000 --> 42:36.000]  ago, so it's all
[42:36.000 --> 42:38.000]  gone.
[42:38.000 --> 42:40.000]  And in that institute
[42:40.000 --> 42:42.000]  I think this activity
[42:42.000 --> 42:44.000]  is not going
[42:44.000 --> 42:46.000]  anymore.
[42:46.000 --> 42:48.000]  With respect to
[42:48.000 --> 42:50.000]  relating
[42:50.000 --> 42:52.000]  gene evolutionary trees
[42:52.000 --> 42:54.000]  with the species
[42:54.000 --> 42:56.000]  evolutionary tree,
[42:56.000 --> 42:58.000]  this activity is still
[42:58.000 --> 43:00.000]  existing, including
[43:00.000 --> 43:02.000]  my own attempts.
[43:02.000 --> 43:04.000]  It was a model developed
[43:04.000 --> 43:06.000]  called Mirkin
[43:06.000 --> 43:08.000]  Muchnik-Smith.
[43:10.000 --> 43:12.000]  But
[43:12.000 --> 43:14.000]  to me,
[43:14.000 --> 43:16.000]  it's a little bit
[43:16.000 --> 43:18.000]  too formal.
[43:18.000 --> 43:20.000]  And as
[43:20.000 --> 43:22.000]  well,
[43:22.000 --> 43:24.000]  there are maybe a few
[43:24.000 --> 43:26.000]  dozen papers published,
[43:26.000 --> 43:28.000]  but no real
[43:28.000 --> 43:30.000]  development.
[43:30.000 --> 43:32.000]  So I'm turning now to
[43:32.000 --> 43:34.000]  the simplest form.
[43:34.000 --> 43:36.000]  The last one about
[43:36.000 --> 43:38.000]  partition
[43:38.000 --> 43:40.000]  looked
[43:40.000 --> 43:42.000]  from the view
[43:42.000 --> 43:44.000]  of another partition of the same
[43:44.000 --> 43:46.000]  set.
[43:46.000 --> 43:48.000]  My first
[43:48.000 --> 43:50.000]  publication in English
[43:50.000 --> 43:52.000]  was in the journal
[43:52.000 --> 43:54.000]  The American Statistician.
[43:54.000 --> 43:56.000]  And currently I'm
[43:56.000 --> 43:58.000]  trying to compile another
[43:58.000 --> 44:00.000]  paper to the same journal
[44:00.000 --> 44:02.000]  with some kind
[44:02.000 --> 44:04.000]  of specification of this.
[44:04.000 --> 44:06.000]  Because not too many people are
[44:06.000 --> 44:08.000]  looking.
[44:08.000 --> 44:10.000]  And I'm going to...
[44:10.000 --> 44:12.000]  How much time do I have?
[44:12.000 --> 44:14.000]  Ten minutes?
[44:18.000 --> 44:20.000]  Let me check.
[44:26.000 --> 44:28.000]  So I'm
[44:28.000 --> 44:30.000]  at 11.55.
[44:30.000 --> 44:32.000]  Okay. So I have enough
[44:32.000 --> 44:34.000]  time. Thank you.
[44:36.000 --> 44:38.000]  So this is the problem.
[44:40.000 --> 44:42.000]  Partition R
[44:42.000 --> 44:44.000]  interpreted in terms of
[44:44.000 --> 44:46.000]  partition S of the same data set.
[44:50.000 --> 44:52.000]  So the major tool here
[44:52.000 --> 44:54.000]  is what the people call
[44:54.000 --> 44:56.000]  Pearson chi-square.
[44:56.000 --> 44:58.000]  Pearson...
[45:00.000 --> 45:02.000]  I'll speak about him.
[45:02.000 --> 45:04.000]  I have somewhere a picture in my slides.
[45:06.000 --> 45:08.000]  This chi-square
[45:08.000 --> 45:10.000]  basically is looking for
[45:10.000 --> 45:12.000]  statistical independence.
[45:14.000 --> 45:16.000]  It considers there is no association
[45:16.000 --> 45:18.000]  when this table shows
[45:18.000 --> 45:20.000]  statistical independence
[45:20.000 --> 45:22.000]  and the index
[45:22.000 --> 45:24.000]  by Pearson measures
[45:24.000 --> 45:26.000]  this. He has proven
[45:28.000 --> 45:30.000]  a beautiful theorem
[45:30.000 --> 45:32.000]  that allows us
[45:32.000 --> 45:34.000]  to
[45:36.000 --> 45:38.000]  define statistical
[45:38.000 --> 45:40.000]  hypothesis of independence
[45:40.000 --> 45:42.000]  but nothing else.
[45:42.000 --> 45:44.000]  There is no way...
[45:46.000 --> 45:48.000]  Here all the theory is
[45:48.000 --> 45:50.000]  expressed. I don't want to
[45:50.000 --> 45:52.000]  dwell on this.
[45:52.000 --> 45:54.000]  I have somewhere...
[45:56.000 --> 45:58.000]  No, I don't.
[46:00.000 --> 46:02.000]  No, I don't, unfortunately.
[46:02.000 --> 46:04.000]  There is a paper,
[46:04.000 --> 46:06.000]  a recent paper
[46:08.000 --> 46:10.000]  called...
[46:10.000 --> 46:12.000]  So chi-square shows
[46:12.000 --> 46:14.000]  that the
[46:14.000 --> 46:16.000]  features
[46:16.000 --> 46:18.000]  are interrelated.
[46:18.000 --> 46:20.000]  So what?
[46:20.000 --> 46:22.000]  Unfortunately,
[46:22.000 --> 46:24.000]  this paper fails to explain
[46:24.000 --> 46:26.000]  what to do next.
[46:28.000 --> 46:30.000]  There are citations
[46:30.000 --> 46:32.000]  of four different approaches
[46:32.000 --> 46:34.000]  what to do,
[46:34.000 --> 46:36.000]  and each of them basically comes through
[46:36.000 --> 46:38.000]  in my view to nothing.
[46:38.000 --> 46:40.000]  It's just more computations
[46:40.000 --> 46:42.000]  but no
[46:42.000 --> 46:44.000]  definitive answer.
[46:44.000 --> 46:46.000]  So what is
[46:46.000 --> 46:48.000]  exactly related
[46:48.000 --> 46:50.000]  and how much it is related
[46:50.000 --> 46:52.000]  if there is a relation?
[46:52.000 --> 46:54.000]  None of these four approaches
[46:54.000 --> 46:56.000]  gives us any...
[46:58.000 --> 47:00.000]  When I approach this
[47:00.000 --> 47:02.000]  there is a very elementary perspective
[47:02.000 --> 47:04.000]  of interpretation of a cluster.
[47:06.000 --> 47:08.000]  So interpretation of a cluster
[47:08.000 --> 47:10.000]  in k-means.
[47:10.000 --> 47:12.000]  Cluster, multi-dimensional cluster
[47:12.000 --> 47:14.000]  is characterized by its center.
[47:14.000 --> 47:16.000]  Center is a vector
[47:16.000 --> 47:18.000]  of the averages
[47:18.000 --> 47:20.000]  within cluster averages of features.
[47:20.000 --> 47:22.000]  So you take this vector
[47:22.000 --> 47:24.000]  and relate it
[47:24.000 --> 47:26.000]  to the grand mean vector,
[47:26.000 --> 47:28.000]  to the mean over
[47:28.000 --> 47:30.000]  the cluster, but the entire data set.
[47:30.000 --> 47:32.000]  And this is what
[47:32.000 --> 47:34.000]  you basically analyze,
[47:34.000 --> 47:36.000]  the relative differences.
[47:36.000 --> 47:38.000]  And then you apply this
[47:38.000 --> 47:40.000]  to
[47:40.000 --> 47:42.000]  categories,
[47:42.000 --> 47:44.000]  dummy categories,
[47:48.000 --> 47:50.000]  quantified,
[47:50.000 --> 47:52.000]  so 0, 1.
[47:52.000 --> 47:54.000]  Then the average
[47:54.000 --> 47:56.000]  is basically
[47:56.000 --> 47:58.000]  is the
[47:58.000 --> 48:00.000]  frequency,
[48:00.000 --> 48:02.000]  the probability of this category.
[48:04.000 --> 48:06.000]  And the
[48:06.000 --> 48:08.000]  center, within cluster center
[48:08.000 --> 48:10.000]  is
[48:10.000 --> 48:12.000]  conditional probability.
[48:14.000 --> 48:16.000]  So the relative difference
[48:16.000 --> 48:18.000]  come to what is put here
[48:18.000 --> 48:20.000]  as Q,
[48:20.000 --> 48:22.000]  Q in
[48:22.000 --> 48:24.000]  below a coefficient
[48:24.000 --> 48:26.000]  that we introduced
[48:26.000 --> 48:28.000]  myself and
[48:28.000 --> 48:30.000]  this Peter Rostov
[48:30.000 --> 48:32.000]  back in 70s
[48:32.000 --> 48:34.000]  to analyze sociology data.
[48:34.000 --> 48:36.000]  In our day of sociology data,
[48:36.000 --> 48:38.000]  all features were almost
[48:38.000 --> 48:40.000]  independent.
[48:40.000 --> 48:42.000]  So the common
[48:42.000 --> 48:44.000]  measure, the conditional
[48:44.000 --> 48:46.000]  probability did not work
[48:46.000 --> 48:48.000]  at all.
[48:48.000 --> 48:50.000]  So we came up with this
[48:50.000 --> 48:52.000]  index, which I called
[48:52.000 --> 48:54.000]  Relative Increment of Probability.
[48:54.000 --> 48:56.000]  When I came to the United States
[48:56.000 --> 48:58.000]  with this
[48:58.000 --> 49:00.000]  title, Relative
[49:00.000 --> 49:02.000]  Index of Probability
[49:02.000 --> 49:04.000]  and abbreviation
[49:04.000 --> 49:06.000]  RIP,
[49:06.000 --> 49:08.000]  my host, Phipps Araby,
[49:08.000 --> 49:10.000]  late Phipps Araby, he said to me,
[49:10.000 --> 49:12.000]  should not use it.
[49:12.000 --> 49:14.000]  I said, why?
[49:14.000 --> 49:16.000]  Just don't.
[49:16.000 --> 49:18.000]  Change it somehow.
[49:18.000 --> 49:20.000]  I changed.
[49:20.000 --> 49:22.000]  But later,
[49:22.000 --> 49:24.000]  while sitting in the
[49:24.000 --> 49:26.000]  University, I discovered
[49:26.000 --> 49:28.000]  that a bit earlier,
[49:28.000 --> 49:30.000]  Ketley,
[49:30.000 --> 49:32.000]  the founding father of statistics,
[49:32.000 --> 49:34.000]  Adolf Ketley, a Belgian
[49:34.000 --> 49:36.000]  statistician,
[49:36.000 --> 49:38.000]  introduced
[49:38.000 --> 49:40.000]  this already
[49:40.000 --> 49:42.000]  in 1834
[49:42.000 --> 49:44.000]  in his memoir.
[49:44.000 --> 49:46.000]  So this was
[49:46.000 --> 49:48.000]  quite a discovery. I was
[49:48.000 --> 49:50.000]  happy. Here you have a picture
[49:50.000 --> 49:52.000]  as well as
[49:52.000 --> 49:54.000]  Pearson. Unfortunately,
[49:54.000 --> 49:56.000]  Pearson is not an entity anymore.
[49:56.000 --> 49:58.000]  The University
[49:58.000 --> 50:00.000]  College
[50:00.000 --> 50:02.000]  in London, I was
[50:02.000 --> 50:04.000]  sitting just next to this building,
[50:04.000 --> 50:06.000]  it had a
[50:06.000 --> 50:08.000]  Pearson building.
[50:08.000 --> 50:10.000]  Not anymore.
[50:10.000 --> 50:12.000]  Pearson
[50:12.000 --> 50:14.000]  name is erased.
[50:14.000 --> 50:16.000]  Because
[50:16.000 --> 50:18.000]  his
[50:18.000 --> 50:20.000]  views,
[50:20.000 --> 50:22.000]  philosophical views,
[50:22.000 --> 50:24.000]  contradict the current
[50:24.000 --> 50:26.000]  liberal values.
[50:26.000 --> 50:28.000]  So everything
[50:28.000 --> 50:30.000]  related to the name of
[50:30.000 --> 50:32.000]  Pearson in the United Kingdom
[50:32.000 --> 50:34.000]  is erased.
[50:34.000 --> 50:36.000]  As well as
[50:36.000 --> 50:38.000]  the many monuments
[50:38.000 --> 50:40.000]  in the United States have been
[50:40.000 --> 50:42.000]  erased because
[50:42.000 --> 50:44.000]  they were only slaves
[50:44.000 --> 50:46.000]  at that time.
[50:46.000 --> 50:48.000]  To a Russian person,
[50:48.000 --> 50:50.000]  this is a little bit
[50:50.000 --> 50:52.000]  difficult to understand.
[50:52.000 --> 50:54.000]  So what I found out
[50:54.000 --> 50:56.000]  basically, that this beautiful
[50:56.000 --> 50:58.000]  concept of Pearson
[50:58.000 --> 51:00.000]  coefficient, in fact,
[51:02.000 --> 51:04.000]  that this is
[51:06.000 --> 51:08.000]  a rather complicated formula,
[51:08.000 --> 51:10.000]  in fact,
[51:10.000 --> 51:12.000]  can be derived with
[51:12.000 --> 51:14.000]  elementary considerations
[51:14.000 --> 51:16.000]  with no
[51:16.000 --> 51:18.000]  statistical independence
[51:18.000 --> 51:20.000]  in mind,
[51:20.000 --> 51:22.000]  just as the average
[51:22.000 --> 51:24.000]  of these
[51:24.000 --> 51:26.000]  Kepler index values.
[51:26.000 --> 51:28.000]  And this
[51:28.000 --> 51:30.000]  gives me a real
[51:30.000 --> 51:32.000]  instrument. So this
[51:32.000 --> 51:34.000]  decomposition gives me
[51:34.000 --> 51:36.000]  two things. First, it gives me
[51:36.000 --> 51:38.000]  operational interpretation of
[51:38.000 --> 51:40.000]  cos squared as an association
[51:40.000 --> 51:42.000]  measure.
[51:42.000 --> 51:44.000]  In the textbooks and statistics
[51:44.000 --> 51:46.000]  they say the chi square is
[51:46.000 --> 51:48.000]  not a measure of association.
[51:48.000 --> 51:50.000]  It only can
[51:50.000 --> 51:52.000]  be used to test
[51:52.000 --> 51:54.000]  statistical tests.
[51:54.000 --> 51:56.000]  So I give it
[51:56.000 --> 51:58.000]  basically
[51:58.000 --> 52:00.000]  the shows
[52:00.000 --> 52:02.000]  increase in
[52:02.000 --> 52:04.000]  probability of
[52:04.000 --> 52:06.000]  categories of one feature
[52:06.000 --> 52:08.000]  when we learn
[52:08.000 --> 52:10.000]  the category
[52:10.000 --> 52:12.000]  of the other feature.
[52:12.000 --> 52:14.000]  And also natural measure of
[52:14.000 --> 52:16.000]  local associations.
[52:16.000 --> 52:18.000]  I have
[52:18.000 --> 52:20.000]  prepared the paper
[52:20.000 --> 52:22.000]  for a similar event.
[52:22.000 --> 52:24.000]  A friend of mine from Canada
[52:24.000 --> 52:26.000]  gets 85.
[52:28.000 --> 52:30.000]  Nishisato,
[52:30.000 --> 52:32.000]  a Canadian statistician.
[52:34.000 --> 52:36.000]  So here I give you an example
[52:36.000 --> 52:38.000]  how it can be used
[52:38.000 --> 52:40.000]  for the analysis
[52:40.000 --> 52:42.000]  of relations.
[52:42.000 --> 52:44.000]  Here is a survey, which is very relevant
[52:44.000 --> 52:46.000]  to us statistically,
[52:46.000 --> 52:48.000]  politically, because
[52:48.000 --> 52:50.000]  it gives us opinions
[52:50.000 --> 52:52.000]  of voters in the United States.
[52:54.000 --> 52:56.000]  It's about
[53:00.000 --> 53:02.000]  30,000 people
[53:02.000 --> 53:04.000]  were asked by a very
[53:04.000 --> 53:06.000]  respected
[53:06.000 --> 53:08.000]  organizations
[53:08.000 --> 53:10.000]  what was their income.
[53:10.000 --> 53:12.000]  Four groups of income.
[53:12.000 --> 53:14.000]  The first group
[53:14.000 --> 53:16.000]  is 30,000 or less.
[53:16.000 --> 53:18.000]  The fourth group is
[53:18.000 --> 53:20.000]  100,000
[53:20.000 --> 53:22.000]  or more.
[53:22.000 --> 53:24.000]  The first group is poor and the fourth group
[53:24.000 --> 53:26.000]  is rich.
[53:26.000 --> 53:28.000]  And
[53:32.000 --> 53:34.000]  the
[53:34.000 --> 53:36.000]  problems relate to
[53:36.000 --> 53:38.000]  voting behavior.
[53:38.000 --> 53:40.000]  Do they vote for Republican?
[53:40.000 --> 53:42.000]  Do they vote for Democrats?
[53:42.000 --> 53:44.000]  Or undecided?
[53:44.000 --> 53:46.000]  Undecided.
[53:46.000 --> 53:48.000]  And the value of
[53:48.000 --> 53:50.000]  chi squared
[53:50.000 --> 53:52.000]  is absolutely negligible
[53:52.000 --> 53:54.000]  and shows
[53:54.000 --> 53:56.000]  that there is
[53:56.000 --> 53:58.000]  no relation
[53:58.000 --> 54:00.000]  between voting
[54:00.000 --> 54:02.000]  and income.
[54:02.000 --> 54:04.000]  But when we look
[54:04.000 --> 54:06.000]  at the
[54:06.000 --> 54:08.000]  Catalan indices,
[54:08.000 --> 54:10.000]  we see a little
[54:10.000 --> 54:12.000]  bit different picture.
[54:12.000 --> 54:14.000]  We see indeed that
[54:14.000 --> 54:16.000]  voting for Democrats has nothing
[54:16.000 --> 54:18.000]  to do with income.
[54:18.000 --> 54:20.000]  It's just the same.
[54:20.000 --> 54:22.000]  There are small numbers here.
[54:22.000 --> 54:24.000]  But voting for Republicans
[54:24.000 --> 54:26.000]  is quite meaningful.
[54:26.000 --> 54:28.000]  If you are poor, you are
[54:28.000 --> 54:30.000]  30% less inclined
[54:30.000 --> 54:32.000]  to vote for Republicans.
[54:32.000 --> 54:34.000]  If you are rich, you are
[54:34.000 --> 54:36.000]  20% more inclined
[54:36.000 --> 54:38.000]  to vote for
[54:38.000 --> 54:40.000]  Republicans.
[54:40.000 --> 54:42.000]  So this is
[54:42.000 --> 54:44.000]  elementary
[54:44.000 --> 54:46.000]  considerations
[54:46.000 --> 54:48.000]  give us a very
[54:48.000 --> 54:50.000]  natural
[54:50.000 --> 54:52.000]  interpretation
[54:52.000 --> 54:54.000]  of the contingency
[54:54.000 --> 54:56.000]  table.
[54:56.000 --> 54:58.000]  So my conclusion.
[54:58.000 --> 55:00.000]  The view of
[55:00.000 --> 55:02.000]  clustering via
[55:02.000 --> 55:04.000]  classification
[55:04.000 --> 55:06.000]  enriches this area of clustering.
[55:06.000 --> 55:08.000]  You kind
[55:08.000 --> 55:10.000]  of get a picture
[55:10.000 --> 55:12.000]  what should be done, what
[55:12.000 --> 55:14.000]  has been done and how it should
[55:14.000 --> 55:16.000]  be. Gives directions
[55:16.000 --> 55:18.000]  for novel developments
[55:18.000 --> 55:20.000]  may bring forth unexpected
[55:20.000 --> 55:22.000]  results.
[55:26.000 --> 55:28.000]  And
[55:28.000 --> 55:30.000]  the last
[55:30.000 --> 55:32.000]  about the contingency table
[55:32.000 --> 55:34.000]  business is not complete
[55:34.000 --> 55:36.000]  and can be enhanced
[55:36.000 --> 55:38.000]  with effective,
[55:38.000 --> 55:40.000]  however simple
[55:40.000 --> 55:42.000]  tools. Thank you very
[55:42.000 --> 55:44.000]  much.
[55:44.000 --> 55:46.000]  Oh, here are some references
[55:46.000 --> 55:48.000]  that I was citing
[55:48.000 --> 55:50.000]  while speaking.
[55:54.000 --> 55:56.000]  Any questions?
[56:04.000 --> 56:06.000]  Hello everyone and thanks to the organizing committee.
[56:08.000 --> 56:10.000]  Today I'm going to talk about
[56:10.000 --> 56:12.000]  the method
[56:12.000 --> 56:14.000]  which I proposed.
[56:14.000 --> 56:16.000]  Well, not
[56:16.000 --> 56:18.000]  fully successful, but still
[56:18.000 --> 56:20.000]  it works in some
[56:20.000 --> 56:22.000]  occasions.
[56:22.000 --> 56:24.000]  I titled
[56:24.000 --> 56:26.000]  today's talk as classification
[56:26.000 --> 56:28.000]  using marginalized
[56:28.000 --> 56:30.000]  maximum likelihood estimation
[56:30.000 --> 56:32.000]  and black box variational inference.
[56:32.000 --> 56:34.000]  Okay.
[56:34.000 --> 56:36.000]  So,
[56:36.000 --> 56:38.000]  can I continue?
[56:38.000 --> 56:40.000]  Okay, so
[56:40.000 --> 56:42.000]  I'm going to give you a very
[56:42.000 --> 56:44.000]  brief introduction
[56:44.000 --> 56:46.000]  of the
[56:46.000 --> 56:48.000]  black box variational
[56:48.000 --> 56:50.000]  inference.
[56:50.000 --> 56:52.000]  I'm going to explain the
[56:52.000 --> 56:54.000]  essence of the proposed method,
[56:54.000 --> 56:56.000]  sharing the experimental
[56:56.000 --> 56:58.000]  setting and the results
[56:58.000 --> 57:00.000]  of the black box
[57:00.000 --> 57:02.000]  variational inference.
[57:02.000 --> 57:04.000]  And I will draw a conclusion
[57:04.000 --> 57:06.000]  and explain the future works.
[57:06.000 --> 57:08.000]  So, originally
[57:08.000 --> 57:10.000]  the
[57:10.000 --> 57:12.000]  recipe for
[57:12.000 --> 57:14.000]  black box variational inference
[57:14.000 --> 57:16.000]  is what we can see
[57:16.000 --> 57:18.000]  if I can show it
[57:18.000 --> 57:20.000]  on the, well,
[57:20.000 --> 57:22.000]  how can I show the
[57:22.000 --> 57:24.000]  laser?
[57:24.000 --> 57:26.000]  Okay, so what we are seeing
[57:26.000 --> 57:28.000]  here in the left
[57:28.000 --> 57:30.000]  hand side of the screen
[57:30.000 --> 57:32.000]  that on the top, the problem
[57:32.000 --> 57:34.000]  in classical
[57:34.000 --> 57:36.000]  recipe, we were integrating
[57:36.000 --> 57:38.000]  over two distributions
[57:38.000 --> 57:40.000]  and then taking derivative
[57:40.000 --> 57:42.000]  and optimizing the objective
[57:42.000 --> 57:44.000]  function. In the newer recipe
[57:44.000 --> 57:46.000]  which was proposed with a group
[57:46.000 --> 57:48.000]  of researchers from Google
[57:48.000 --> 57:50.000]  research group,
[57:50.000 --> 57:52.000]  Rangas and his colleagues,
[57:52.000 --> 57:54.000]  they switched this
[57:54.000 --> 57:56.000]  integration and
[57:56.000 --> 57:58.000]  taking derivatives
[57:58.000 --> 58:00.000]  and then the rest of the recipe
[58:00.000 --> 58:02.000]  is the follow. And this brings
[58:02.000 --> 58:04.000]  several
[58:04.000 --> 58:06.000]  simplicity
[58:06.000 --> 58:08.000]  and this is one of the reasons
[58:08.000 --> 58:10.000]  they proposed this
[58:10.000 --> 58:12.000]  switch.
[58:12.000 --> 58:14.000]  And the reason, so
[58:14.000 --> 58:16.000]  I essentially
[58:16.000 --> 58:18.000]  relied on this
[58:18.000 --> 58:20.000]  principle, so taking
[58:20.000 --> 58:22.000]  derivative and integrating and then
[58:22.000 --> 58:24.000]  optimizing the black box,
[58:24.000 --> 58:26.000]  optimizing the objective function.
[58:26.000 --> 58:28.000]  And why it is called black box?
[58:28.000 --> 58:30.000]  Because we don't specify
[58:30.000 --> 58:32.000]  any specific normal distribution.
[58:32.000 --> 58:34.000]  We can have just
[58:34.000 --> 58:36.000]  merely a bunch of data
[58:36.000 --> 58:38.000]  and a neural network
[58:38.000 --> 58:40.000]  and try to feed it through
[58:40.000 --> 58:42.000]  the, well,
[58:42.000 --> 58:44.000]  slightly modified
[58:44.000 --> 58:46.000]  version of classical
[58:46.000 --> 58:48.000]  conventional
[58:48.000 --> 58:50.000]  learning
[58:50.000 --> 58:52.000]  methods actually.
[58:52.000 --> 58:54.000]  So, to be a bit
[58:54.000 --> 58:56.000]  more specific, we define the
[58:56.000 --> 58:58.000]  inputs as set
[58:58.000 --> 59:00.000]  X, which usually
[59:00.000 --> 59:02.000]  in my book I consider them as a
[59:02.000 --> 59:04.000]  V-dimensional vector and
[59:04.000 --> 59:06.000]  capital Y represent
[59:06.000 --> 59:08.000]  the corresponding target values
[59:08.000 --> 59:10.000]  which for
[59:10.000 --> 59:12.000]  the classification task this
[59:12.000 --> 59:14.000]  work I
[59:14.000 --> 59:16.000]  convert them to one-hot encoded
[59:16.000 --> 59:18.000]  vectors. And
[59:18.000 --> 59:20.000]  by assuming that, so this is
[59:20.000 --> 59:22.000]  probably the main
[59:22.000 --> 59:24.000]  point of this talk.
[59:24.000 --> 59:26.000]  I assume that during the generation
[59:26.000 --> 59:28.000]  of data points there is a
[59:28.000 --> 59:30.000]  set of existent variables Z.
[59:30.000 --> 59:32.000]  It could be
[59:32.000 --> 59:34.000]  equal to the number of classes
[59:34.000 --> 59:36.000]  here or we can
[59:36.000 --> 59:38.000]  play with this actually.
[59:40.000 --> 59:42.000]  Which, which, how many,
[59:42.000 --> 59:44.000]  how we should determine
[59:44.000 --> 59:46.000]  the set of hidden variables.
[59:46.000 --> 59:48.000]  And just pursuing
[59:48.000 --> 59:50.000]  the principles of
[59:50.000 --> 59:52.000]  maximum likelihood estimation, it is not
[59:52.000 --> 59:54.000]  hard to show that the equation
[59:54.000 --> 59:56.000]  one can be derived.
[59:56.000 --> 59:58.000]  And for optimizing this
[59:58.000 --> 01:00:00.000]  equation one,
[01:00:00.000 --> 01:00:02.000]  you know it better than me, there are a lot
[01:00:02.000 --> 01:00:04.000]  of optimization methods
[01:00:04.000 --> 01:00:06.000]  for this work. I adopt
[01:00:06.000 --> 01:00:08.000]  a black box variational inference
[01:00:08.000 --> 01:00:10.000]  as I'm going to explain.
[01:00:10.000 --> 01:00:12.000]  However, optimizing
[01:00:12.000 --> 01:00:14.000]  this equation,
[01:00:14.000 --> 01:00:16.000]  our proposed
[01:00:16.000 --> 01:00:18.000]  objective function
[01:00:18.000 --> 01:00:20.000]  is not straightforward and instead
[01:00:20.000 --> 01:00:22.000]  of doing that, I
[01:00:22.000 --> 01:00:24.000]  propose to
[01:00:24.000 --> 01:00:26.000]  optimize the
[01:00:26.000 --> 01:00:28.000]  evidence lower bound or
[01:00:28.000 --> 01:00:30.000]  so called elbow.
[01:00:30.000 --> 01:00:32.000]  I'm sorry, so.
[01:00:34.000 --> 01:00:36.000]  Well, probably I should
[01:00:36.000 --> 01:00:38.000]  not, I don't know, I will try
[01:00:38.000 --> 01:00:40.000]  to make it as
[01:00:40.000 --> 01:00:42.000]  technical as possible, but this is the definition
[01:00:42.000 --> 01:00:44.000]  of elbow. So
[01:00:44.000 --> 01:00:46.000]  for two given distributions,
[01:00:46.000 --> 01:00:48.000]  rx and yx, here
[01:00:48.000 --> 01:00:50.000]  I showed the distribution,
[01:00:50.000 --> 01:00:52.000]  the definition of elbow,
[01:00:52.000 --> 01:00:54.000]  and by just recalling this
[01:00:54.000 --> 01:00:56.000]  definition of elbow,
[01:00:56.000 --> 01:00:58.000]  we can show, and applying
[01:00:58.000 --> 01:01:00.000]  the Jensen equality, we can
[01:01:00.000 --> 01:01:02.000]  easily obtain the equation
[01:01:02.000 --> 01:01:04.000]  to be here in the
[01:01:04.000 --> 01:01:06.000]  lower part of the slide.
[01:01:06.000 --> 01:01:08.000]  So it's just a bit of
[01:01:08.000 --> 01:01:10.000]  probably basic math.
[01:01:10.000 --> 01:01:12.000]  That's which I'm pretty sure
[01:01:12.000 --> 01:01:14.000]  all of you can do it by far better than me.
[01:01:14.000 --> 01:01:16.000]  And then
[01:01:16.000 --> 01:01:18.000]  we just,
[01:01:18.000 --> 01:01:20.000]  we obtained a new
[01:01:20.000 --> 01:01:22.000]  objective function, which I just
[01:01:22.000 --> 01:01:24.000]  repeated on the top of the,
[01:01:24.000 --> 01:01:26.000]  this slide for the
[01:01:26.000 --> 01:01:28.000]  convenience, and now we can just
[01:01:28.000 --> 01:01:30.000]  apply the optimality condition,
[01:01:30.000 --> 01:01:32.000]  easily first order optimality condition
[01:01:32.000 --> 01:01:34.000]  that is taking derivatives from
[01:01:34.000 --> 01:01:36.000]  this one, an equation
[01:01:36.000 --> 01:01:38.000]  3e, I just
[01:01:38.000 --> 01:01:40.000]  chose the corresponding
[01:01:40.000 --> 01:01:42.000]  equations as to the paper
[01:01:42.000 --> 01:01:44.000]  just for the convenience.
[01:01:44.000 --> 01:01:46.000]  And this equation
[01:01:46.000 --> 01:01:48.000]  3e
[01:01:48.000 --> 01:01:50.000]  can be optimized using
[01:01:50.000 --> 01:01:52.000]  Monte Carlo gradient estimator.
[01:01:52.000 --> 01:01:54.000]  And this work, to be
[01:01:54.000 --> 01:01:56.000]  more precise, in this
[01:01:56.000 --> 01:01:58.000]  result which I'm going to show you,
[01:01:58.000 --> 01:02:00.000]  I used the so-called variational inference,
[01:02:00.000 --> 01:02:02.000]  sorry, reparameterization
[01:02:02.000 --> 01:02:04.000]  trick, which is
[01:02:04.000 --> 01:02:06.000]  quite familiar in the work
[01:02:06.000 --> 01:02:08.000]  of
[01:02:08.000 --> 01:02:10.000]  the variational
[01:02:10.000 --> 01:02:12.000]  autoencoders
[01:02:12.000 --> 01:02:14.000]  by Derek,
[01:02:14.000 --> 01:02:16.000]  I forgot his name,
[01:02:16.000 --> 01:02:18.000]  but this is one of the
[01:02:18.000 --> 01:02:20.000]  first applications of
[01:02:20.000 --> 01:02:22.000]  the reparameterization trick.
[01:02:22.000 --> 01:02:24.000]  Here I also, Kingma,
[01:02:24.000 --> 01:02:26.000]  sorry, I mentioned the reference here.
[01:02:26.000 --> 01:02:28.000]  So I
[01:02:28.000 --> 01:02:30.000]  pursued this method,
[01:02:30.000 --> 01:02:32.000]  and if
[01:02:32.000 --> 01:02:34.000]  we want to be a bit more specific
[01:02:34.000 --> 01:02:36.000]  how we can pursue this method,
[01:02:36.000 --> 01:02:38.000]  we can assume that there exists
[01:02:38.000 --> 01:02:40.000]  a transformation rule
[01:02:40.000 --> 01:02:42.000]  such that we can draw the sample
[01:02:42.000 --> 01:02:44.000]  epsilon i
[01:02:44.000 --> 01:02:46.000]  for n data points
[01:02:46.000 --> 01:02:48.000]  from this simpler
[01:02:48.000 --> 01:02:50.000]  and parameter-free distribution
[01:02:52.000 --> 01:02:54.000]  which is indeed independent
[01:02:54.000 --> 01:02:56.000]  of the theta, the parameter of
[01:02:56.000 --> 01:02:58.000]  our model which we are trying to derive.
[01:02:58.000 --> 01:03:00.000]  And then by applying the
[01:03:02.000 --> 01:03:04.000]  by using a deterministic
[01:03:04.000 --> 01:03:06.000]  pass T
[01:03:06.000 --> 01:03:08.000]  epsilon i and theta,
[01:03:08.000 --> 01:03:10.000]  so we can
[01:03:10.000 --> 01:03:12.000]  transform
[01:03:12.000 --> 01:03:14.000]  our hidden variables
[01:03:14.000 --> 01:03:16.000]  Zi, as I show here,
[01:03:16.000 --> 01:03:18.000]  is equal to T epsilon i and theta,
[01:03:18.000 --> 01:03:20.000]  and epsilon i is
[01:03:20.000 --> 01:03:22.000]  derived from our
[01:03:22.000 --> 01:03:24.000]  distribution, parameter-free
[01:03:24.000 --> 01:03:26.000]  distribution,
[01:03:26.000 --> 01:03:28.000]  and, well,
[01:03:28.000 --> 01:03:30.000]  with this, let me give you an example
[01:03:30.000 --> 01:03:32.000]  to be more specific.
[01:03:32.000 --> 01:03:34.000]  So assume that we derive
[01:03:34.000 --> 01:03:36.000]  the epsilon from a normal distribution,
[01:03:36.000 --> 01:03:38.000]  normal and standard,
[01:03:38.000 --> 01:03:40.000]  normal and standard
[01:03:40.000 --> 01:03:42.000]  Gaussian distribution,
[01:03:42.000 --> 01:03:44.000]  and then we're doing the location
[01:03:44.000 --> 01:03:46.000]  and transformation of epsilon,
[01:03:46.000 --> 01:03:48.000]  we can show that
[01:03:48.000 --> 01:03:50.000]  Z is nothing but
[01:03:50.000 --> 01:03:52.000]  equal to epsilon
[01:03:52.000 --> 01:03:54.000]  multiplied by sigma plus mu,
[01:03:54.000 --> 01:03:56.000]  and then we can
[01:03:56.000 --> 01:03:58.000]  conclude that Z is indeed
[01:03:58.000 --> 01:04:00.000]  derived from this normal
[01:04:00.000 --> 01:04:02.000]  distribution with parameter mu and phi.
[01:04:02.000 --> 01:04:04.000]  And what it brings us,
[01:04:04.000 --> 01:04:06.000]  this brings us to the point
[01:04:06.000 --> 01:04:08.000]  that we can replace the Z
[01:04:08.000 --> 01:04:10.000]  in our equation
[01:04:10.000 --> 01:04:12.000]  with this deterministic
[01:04:12.000 --> 01:04:14.000]  function
[01:04:14.000 --> 01:04:16.000]  and
[01:04:16.000 --> 01:04:18.000]  because we cannot
[01:04:18.000 --> 01:04:20.000]  take the, we cannot compute
[01:04:20.000 --> 01:04:22.000]  the gradients of Z
[01:04:22.000 --> 01:04:24.000]  because they are interactable.
[01:04:24.000 --> 01:04:26.000]  Sorry, I forgot even to mention it.
[01:04:26.000 --> 01:04:28.000]  And this is why we
[01:04:28.000 --> 01:04:30.000]  replace with the Z
[01:04:30.000 --> 01:04:32.000]  with a
[01:04:32.000 --> 01:04:34.000]  re-parameterization trick actually.
[01:04:34.000 --> 01:04:36.000]  And, well,
[01:04:36.000 --> 01:04:38.000]  we just replace it, we can
[01:04:38.000 --> 01:04:40.000]  come up with the equation 4B
[01:04:40.000 --> 01:04:42.000]  and then applying the principles
[01:04:42.000 --> 01:04:44.000]  of Monte Carlo gradient estimator.
[01:04:44.000 --> 01:04:46.000]  So we just draw
[01:04:46.000 --> 01:04:48.000]  samples epsilon L,
[01:04:48.000 --> 01:04:50.000]  L number of samples,
[01:04:50.000 --> 01:04:52.000]  and then we evaluate the functions,
[01:04:52.000 --> 01:04:54.000]  our equation 4B,
[01:04:54.000 --> 01:04:56.000]  and then
[01:04:56.000 --> 01:04:58.000]  we compute the empirical
[01:04:58.000 --> 01:05:00.000]  means of these
[01:05:00.000 --> 01:05:02.000]  quantities actually.
[01:05:02.000 --> 01:05:04.000]  Just for the convenience,
[01:05:04.000 --> 01:05:06.000]  I named this equation
[01:05:06.000 --> 01:05:08.000]  4B as G, so
[01:05:08.000 --> 01:05:10.000]  this is the
[01:05:12.000 --> 01:05:14.000]  just simpler
[01:05:14.000 --> 01:05:16.000]  notation of the
[01:05:16.000 --> 01:05:18.000]  Monte Carlo gradient estimator
[01:05:18.000 --> 01:05:20.000]  and we can
[01:05:20.000 --> 01:05:22.000]  summarize the process
[01:05:22.000 --> 01:05:24.000]  of this MLE as
[01:05:24.000 --> 01:05:26.000]  follows, so it's just
[01:05:26.000 --> 01:05:28.000]  the slightly
[01:05:28.000 --> 01:05:30.000]  modified version
[01:05:30.000 --> 01:05:32.000]  of the stochastic
[01:05:32.000 --> 01:05:34.000]  gradient descent, so we apply
[01:05:34.000 --> 01:05:36.000]  M is the batch of
[01:05:36.000 --> 01:05:38.000]  the data set, we just derive
[01:05:38.000 --> 01:05:40.000]  a set of
[01:05:40.000 --> 01:05:42.000]  noises epsilon
[01:05:42.000 --> 01:05:44.000]  and then the rest of the
[01:05:44.000 --> 01:05:46.000]  update is the same.
[01:05:46.000 --> 01:05:48.000]  And we continue this process
[01:05:48.000 --> 01:05:50.000]  until we converge.
[01:05:50.000 --> 01:05:52.000]  And, well, the
[01:05:52.000 --> 01:05:54.000]  source code is also available in the
[01:05:54.000 --> 01:05:56.000]  GitHub if any of you is interested.
[01:05:56.000 --> 01:05:58.000]  So, this is
[01:05:58.000 --> 01:06:00.000]  the essence of the parameter and
[01:06:00.000 --> 01:06:02.000]  to test the
[01:06:02.000 --> 01:06:04.000]  efficiency of the proposed
[01:06:04.000 --> 01:06:06.000]  method, I, well, actually
[01:06:06.000 --> 01:06:08.000]  I compared the method
[01:06:08.000 --> 01:06:10.000]  with more algorithms
[01:06:10.000 --> 01:06:12.000]  here, but because of the,
[01:06:12.000 --> 01:06:14.000]  well, it would be very long,
[01:06:14.000 --> 01:06:16.000]  so I decided to limit the report
[01:06:16.000 --> 01:06:18.000]  with these three competitors,
[01:06:18.000 --> 01:06:20.000]  Adaboost, conventional
[01:06:20.000 --> 01:06:22.000]  MLE with no
[01:06:22.000 --> 01:06:24.000]  marginalization
[01:06:24.000 --> 01:06:26.000]  and the third
[01:06:26.000 --> 01:06:28.000]  work, which I just mentioned,
[01:06:28.000 --> 01:06:30.000]  the CLSP, was
[01:06:30.000 --> 01:06:32.000]  probably such a good name, well,
[01:06:32.000 --> 01:06:34.000]  definitely such a good naming, sorry
[01:06:34.000 --> 01:06:36.000]  for that.
[01:06:36.000 --> 01:06:38.000]  And, well,
[01:06:38.000 --> 01:06:40.000]  I applied my
[01:06:40.000 --> 01:06:42.000]  algorithm and these three competitors
[01:06:42.000 --> 01:06:44.000]  on four real-world data sets,
[01:06:44.000 --> 01:06:46.000]  Iris, MNIST,
[01:06:46.000 --> 01:06:48.000]  ForestCover
[01:06:48.000 --> 01:06:50.000]  and Wine data set.
[01:06:50.000 --> 01:06:52.000]  And probably I should mention
[01:06:52.000 --> 01:06:54.000]  that I
[01:06:54.000 --> 01:06:56.000]  applied this method and also,
[01:06:56.000 --> 01:06:58.000]  well, initially I proposed
[01:06:58.000 --> 01:07:00.000]  this method for imbalanced
[01:07:00.000 --> 01:07:02.000]  classification of
[01:07:02.000 --> 01:07:04.000]  outlier, imbalanced
[01:07:04.000 --> 01:07:06.000]  outlier detection, but
[01:07:06.000 --> 01:07:08.000]  since my affiliation changed,
[01:07:08.000 --> 01:07:10.000]  so I couldn't propose,
[01:07:10.000 --> 01:07:12.000]  I couldn't introduce those data sets
[01:07:12.000 --> 01:07:14.000]  which are, was devoted
[01:07:14.000 --> 01:07:16.000]  to that lab.
[01:07:16.000 --> 01:07:18.000]  And over there I obtained quite
[01:07:18.000 --> 01:07:20.000]  better result than what I'm going to show
[01:07:20.000 --> 01:07:22.000]  and I also just was, wanted
[01:07:22.000 --> 01:07:24.000]  to study the performance of
[01:07:24.000 --> 01:07:26.000]  algorithm over non-linear
[01:07:26.000 --> 01:07:28.000]  data set like
[01:07:28.000 --> 01:07:30.000]  moon shape or different
[01:07:30.000 --> 01:07:32.000]  dimensionality of hypercube
[01:07:32.000 --> 01:07:34.000]  Gaussian distribution.
[01:07:34.000 --> 01:07:36.000]  And I used the Rookhawk as my
[01:07:36.000 --> 01:07:38.000]  metric.
[01:07:38.000 --> 01:07:40.000]  So here is the results, the last
[01:07:40.000 --> 01:07:42.000]  two,
[01:07:42.000 --> 01:07:44.000]  I must add that,
[01:07:44.000 --> 01:07:46.000]  if I use linear
[01:07:46.000 --> 01:07:48.000]  activation function,
[01:07:48.000 --> 01:07:50.000]  I add the
[01:07:50.000 --> 01:07:52.000]  li to my algorithms
[01:07:52.000 --> 01:07:54.000]  and if I use real-world,
[01:07:54.000 --> 01:07:56.000]  I add that re.
[01:07:56.000 --> 01:07:58.000]  So the last two rows are
[01:07:58.000 --> 01:08:00.000]  the proposed algorithm
[01:08:00.000 --> 01:08:02.000]  of mine and, well,
[01:08:02.000 --> 01:08:04.000]  we can see that it wins
[01:08:04.000 --> 01:08:06.000]  over two real-world
[01:08:06.000 --> 01:08:08.000]  data sets and the results for
[01:08:08.000 --> 01:08:10.000]  forest
[01:08:10.000 --> 01:08:12.000]  coverage
[01:08:12.000 --> 01:08:14.000]  data set is also
[01:08:14.000 --> 01:08:16.000]  not bad.
[01:08:18.000 --> 01:08:20.000]  And over the
[01:08:20.000 --> 01:08:22.000]  Gaussian
[01:08:22.000 --> 01:08:24.000]  hypercube,
[01:08:24.000 --> 01:08:26.000]  conventional MLE
[01:08:26.000 --> 01:08:28.000]  wins most of the
[01:08:28.000 --> 01:08:30.000]  cases, however,
[01:08:30.000 --> 01:08:32.000]  the proposed method
[01:08:32.000 --> 01:08:34.000]  maximum, marginalized
[01:08:34.000 --> 01:08:36.000]  maximum likelihood
[01:08:36.000 --> 01:08:38.000]  wins two cases
[01:08:38.000 --> 01:08:40.000]  when we have
[01:08:40.000 --> 01:08:42.000]  two informative features
[01:08:42.000 --> 01:08:44.000]  and eighteen non-informative
[01:08:44.000 --> 01:08:46.000]  random features.
[01:08:46.000 --> 01:08:48.000]  So probably I should mention that
[01:08:48.000 --> 01:08:50.000]  these two plus zero means that we have
[01:08:50.000 --> 01:08:52.000]  two informative features and
[01:08:52.000 --> 01:08:54.000]  no
[01:08:54.000 --> 01:08:56.000]  non-informative features,
[01:08:56.000 --> 01:08:58.000]  two plus eighteen means that
[01:08:58.000 --> 01:09:00.000]  we have two informative and eighteen
[01:09:00.000 --> 01:09:02.000]  noises and so forth.
[01:09:02.000 --> 01:09:04.000]  So in these two cases, these algorithms win
[01:09:04.000 --> 01:09:06.000]  the competition
[01:09:06.000 --> 01:09:08.000]  and overall
[01:09:08.000 --> 01:09:10.000]  the obtained results are quite
[01:09:10.000 --> 01:09:12.000]  similar, but I
[01:09:12.000 --> 01:09:14.000]  cannot say that it's a
[01:09:14.000 --> 01:09:16.000]  big victory.
[01:09:16.000 --> 01:09:18.000]  Well, I need to
[01:09:18.000 --> 01:09:20.000]  probably work more to understand
[01:09:20.000 --> 01:09:22.000]  how can I improve the
[01:09:22.000 --> 01:09:24.000]  results. It's competitive,
[01:09:24.000 --> 01:09:26.000]  but it's not. Here is the result for
[01:09:26.000 --> 01:09:28.000]  moon-shaped data set in two
[01:09:28.000 --> 01:09:30.000]  cases, balanced and unbalanced.
[01:09:30.000 --> 01:09:32.000]  Again, the results
[01:09:32.000 --> 01:09:34.000]  are similar to other
[01:09:34.000 --> 01:09:36.000]  competitors, but I cannot say
[01:09:36.000 --> 01:09:38.000]  that it didn't win, unfortunately.
[01:09:38.000 --> 01:09:40.000]  So as a
[01:09:40.000 --> 01:09:42.000]  conclusion, I propose
[01:09:42.000 --> 01:09:44.000]  a marginalized
[01:09:44.000 --> 01:09:46.000]  version of the conventional
[01:09:46.000 --> 01:09:48.000]  maximum likelihood estimation
[01:09:48.000 --> 01:09:50.000]  and I operate with two
[01:09:50.000 --> 01:09:52.000]  different versions used
[01:09:52.000 --> 01:09:54.000]  separating the
[01:09:54.000 --> 01:09:56.000]  activation function during
[01:09:56.000 --> 01:09:58.000]  the training process.
[01:09:58.000 --> 01:10:00.000]  The results appear to be
[01:10:00.000 --> 01:10:02.000]  valid,
[01:10:02.000 --> 01:10:04.000]  competitive, but I
[01:10:04.000 --> 01:10:06.000]  cannot claim that it's
[01:10:06.000 --> 01:10:08.000]  unfortunately very effective, although
[01:10:08.000 --> 01:10:10.000]  there were cases that the algorithm
[01:10:10.000 --> 01:10:12.000]  wins over the competitors.
[01:10:12.000 --> 01:10:14.000]  As for the future
[01:10:14.000 --> 01:10:16.000]  works,
[01:10:16.000 --> 01:10:18.000]  I see several future works.
[01:10:18.000 --> 01:10:20.000]  First of all and probably the most
[01:10:20.000 --> 01:10:22.000]  important one for me
[01:10:22.000 --> 01:10:24.000]  is to study the
[01:10:24.000 --> 01:10:26.000]  dimensionality of the hidden
[01:10:26.000 --> 01:10:28.000]  variables, how we should study the
[01:10:28.000 --> 01:10:30.000]  hidden variables, should be just equal
[01:10:30.000 --> 01:10:32.000]  to the number of clusters or we can
[01:10:32.000 --> 01:10:34.000]  this is
[01:10:34.000 --> 01:10:36.000]  something which is very interesting
[01:10:36.000 --> 01:10:38.000]  for me or applying
[01:10:38.000 --> 01:10:40.000]  the different transformation
[01:10:40.000 --> 01:10:42.000]  rules also can be of interest
[01:10:42.000 --> 01:10:44.000]  instead of just
[01:10:44.000 --> 01:10:46.000]  the re-parameterization
[01:10:46.000 --> 01:10:48.000]  trick.
[01:10:48.000 --> 01:10:50.000]  And well, of course,
[01:10:50.000 --> 01:10:52.000]  extending the list of
[01:10:52.000 --> 01:10:54.000]  competitors and well,
[01:10:54.000 --> 01:10:56.000]  there are a lot to do,
[01:10:56.000 --> 01:10:58.000]  but let me
[01:10:58.000 --> 01:11:00.000]  finish with
[01:11:00.000 --> 01:11:02.000]  this and thank you for your
[01:11:02.000 --> 01:11:04.000]  attention. If there is any
[01:11:04.000 --> 01:11:06.000]  questions,
[01:11:06.000 --> 01:11:08.000]  I would be happy to answer.
[01:11:08.000 --> 01:11:10.000]  Yes?
[01:11:10.000 --> 01:11:12.000]  Yes?
[01:11:28.000 --> 01:11:30.000]  Well, actually this is one of
[01:11:30.000 --> 01:11:32.000]  the future works which I
[01:11:32.000 --> 01:11:34.000]  need to investigate.
[01:11:34.000 --> 01:11:36.000]  At the moment I cannot
[01:11:36.000 --> 01:11:38.000]  recommend anything unfortunately,
[01:11:38.000 --> 01:11:40.000]  but yeah.
[01:11:40.000 --> 01:11:42.000]  But in the paper,
[01:11:42.000 --> 01:11:44.000]  I at least cited two, three
[01:11:44.000 --> 01:11:46.000]  more methods
[01:11:46.000 --> 01:11:48.000]  for tackling, well,
[01:11:48.000 --> 01:11:50.000]  for optimizing the objective function.
[01:11:50.000 --> 01:11:52.000]  I can check it.
[01:11:52.000 --> 01:11:54.000]  At the moment, unfortunately, I cannot remember
[01:11:54.000 --> 01:11:56.000]  the names, but yeah.
[01:12:02.000 --> 01:12:04.000]  Yes?
[01:12:08.000 --> 01:12:10.000]  Well, usually
[01:12:10.000 --> 01:12:12.000]  XGBoost
[01:12:12.000 --> 01:12:14.000]  gave us better results
[01:12:14.000 --> 01:12:16.000]  than Adaboost, yeah.
[01:12:16.000 --> 01:12:18.000]  But not all the time, yeah.
[01:12:18.000 --> 01:12:20.000]  Usually it gives us better results.
[01:12:20.000 --> 01:12:22.000]  Well, as I mentioned during my talk,
[01:12:22.000 --> 01:12:24.000]  I cannot say that it's very successful
[01:12:24.000 --> 01:12:26.000]  results,
[01:12:26.000 --> 01:12:28.000]  but well,
[01:12:28.000 --> 01:12:30.000]  it's still
[01:12:30.000 --> 01:12:32.000]  in progress and I hope I can.
[01:12:32.000 --> 01:12:34.000]  Yeah, XGBoost usually gives
[01:12:34.000 --> 01:12:36.000]  by far better results than Adaboost.
[01:12:38.000 --> 01:12:40.000]  Yes?
[01:12:52.000 --> 01:12:54.000]  That's true, that's true.
[01:12:58.000 --> 01:13:00.000]  Well,
[01:13:00.000 --> 01:13:02.000]  when I was working on
[01:13:02.000 --> 01:13:04.000]  this, that
[01:13:04.000 --> 01:13:06.000]  specific anomaly
[01:13:06.000 --> 01:13:08.000]  detection task,
[01:13:08.000 --> 01:13:10.000]  one of the approaches which gave us
[01:13:10.000 --> 01:13:12.000]  good results, in addition,
[01:13:12.000 --> 01:13:14.000]  this algorithm works really good
[01:13:14.000 --> 01:13:16.000]  over those datasets, but I cannot
[01:13:16.000 --> 01:13:18.000]  share the results because
[01:13:18.000 --> 01:13:20.000]  of the changes.
[01:13:20.000 --> 01:13:22.000]  But another algorithm which gave
[01:13:22.000 --> 01:13:24.000]  me good results at that
[01:13:24.000 --> 01:13:26.000]  specific task was to train
[01:13:26.000 --> 01:13:28.000]  just one,
[01:13:28.000 --> 01:13:30.000]  for example, what we called,
[01:13:30.000 --> 01:13:32.000]  we were training an autoencoder
[01:13:32.000 --> 01:13:34.000]  on just the normal
[01:13:34.000 --> 01:13:36.000]  dataset and after that
[01:13:36.000 --> 01:13:38.000]  for classification
[01:13:38.000 --> 01:13:40.000]  we were giving
[01:13:40.000 --> 01:13:42.000]  a test and we were
[01:13:42.000 --> 01:13:44.000]  measuring the reconstructed
[01:13:44.000 --> 01:13:46.000]  output of the
[01:13:46.000 --> 01:13:48.000]  autoencoder
[01:13:48.000 --> 01:13:50.000]  with the
[01:13:50.000 --> 01:13:52.000]  normal distribution and abnormal.
[01:13:52.000 --> 01:13:54.000]  If the one
[01:13:54.000 --> 01:13:56.000]  which gives us the shorter distance
[01:13:56.000 --> 01:13:58.000]  for this test
[01:13:58.000 --> 01:14:00.000]  data
[01:14:00.000 --> 01:14:02.000]  point, we would
[01:14:02.000 --> 01:14:04.000]  use the same label
[01:14:04.000 --> 01:14:06.000]  for this. This gives us
[01:14:06.000 --> 01:14:08.000]  very good results instead
[01:14:08.000 --> 01:14:10.000]  of handling because of the class
[01:14:10.000 --> 01:14:12.000]  imbalance usually we just
[01:14:12.000 --> 01:14:14.000]  learned one representation
[01:14:14.000 --> 01:14:16.000]  and this was one of the good
[01:14:16.000 --> 01:14:18.000]  methods and it worked
[01:14:18.000 --> 01:14:20.000]  quite good.
[01:14:20.000 --> 01:14:22.000]  You're welcome, thank you for your question.
[01:14:26.000 --> 01:14:28.000]  Okay, thank you
[01:14:28.000 --> 01:14:30.000]  everyone.
[01:14:32.000 --> 01:14:34.000]  Thank you.
[01:14:54.000 --> 01:14:56.000]  Reactor of the school would like
[01:14:56.000 --> 01:14:58.000]  to say a couple of
[01:14:58.000 --> 01:15:00.000]  warm words to Boris
[01:15:00.000 --> 01:15:02.000]  Mirkin and then
[01:15:02.000 --> 01:15:04.000]  your turn you will continue, okay?
[01:15:06.000 --> 01:15:08.000]  Thank you, thank you very much.
[01:15:30.000 --> 01:15:32.000]  Thank you, thank you very much.
[01:16:00.000 --> 01:16:02.000]  К сожалению, я все-таки не могу сказать,
[01:16:02.000 --> 01:16:04.000]  что здесь толпа народа,
[01:16:04.000 --> 01:16:06.000]  но мой замысел
[01:16:06.000 --> 01:16:08.000]  был в том, что конец
[01:16:08.000 --> 01:16:10.000]  декабря это конец света.
[01:16:10.000 --> 01:16:12.000]  Вообще у всех отчетный период,
[01:16:12.000 --> 01:16:14.000]  а у нас тут еще была
[01:16:14.000 --> 01:16:16.000]  страда.
[01:16:16.000 --> 01:16:18.000]  Мы занимались защитами
[01:16:18.000 --> 01:16:20.000]  кандидатских диссертаций
[01:16:20.000 --> 01:16:22.000]  примерно двадцати аспирантов,
[01:16:22.000 --> 01:16:24.000]  которые мерзавцы
[01:16:24.000 --> 01:16:26.000]  дотянули до самого последнего,
[01:16:26.000 --> 01:16:28.000]  то есть вот выбрали просто последние даты
[01:16:28.000 --> 01:16:30.000]  в декабря, чтобы зачтиться в срок.
[01:16:30.000 --> 01:16:32.000]  Ну и соответственно, в общем,
[01:16:32.000 --> 01:16:34.000]  целыми днями мы сидели и защищали
[01:16:34.000 --> 01:16:36.000]  этих аспирантов. Вот Константин
[01:16:36.000 --> 01:16:38.000]  Вячеславович в этом активно
[01:16:38.000 --> 01:16:40.000]  участвовал, и Сергей Олегович
[01:16:40.000 --> 01:16:42.000]  тоже. Дистанционно,
[01:16:42.000 --> 01:16:44.000]  но очень активно
[01:16:44.000 --> 01:16:46.000]  напрягли мы Сергей Олегович, очень
[01:16:46.000 --> 01:16:48.000]  сильно этим. В общем,
[01:16:48.000 --> 01:16:50.000]  казалось, что там это никак не организовать,
[01:16:50.000 --> 01:16:52.000]  поэтому решили перенести
[01:16:52.000 --> 01:16:54.000]  наконец до января, но вот
[01:16:54.000 --> 01:16:56.000]  не знаю, как надо еще пиарить
[01:16:56.000 --> 01:16:58.000]  среди студентов такие мероприятия.
[01:16:58.000 --> 01:17:00.000]  Вроде вся информация была выложена,
[01:17:00.000 --> 01:17:02.000]  но тем не менее
[01:17:02.000 --> 01:17:04.000]  как-то вот не очень много
[01:17:04.000 --> 01:17:06.000]  студентов, что жаль, потому что тут
[01:17:06.000 --> 01:17:08.000]  собралась, конечно, замечательная совершенно
[01:17:08.000 --> 01:17:10.000]  аудитория. Я этому
[01:17:10.000 --> 01:17:12.000]  обстоятельству очень рад.
[01:17:12.000 --> 01:17:14.000]  Ну и, конечно, я
[01:17:14.000 --> 01:17:16.000]  всячески поздравляю
[01:17:16.000 --> 01:17:18.000]  юбиляра, в честь которого
[01:17:18.000 --> 01:17:20.000]  собственно это мероприятие собрано.
[01:17:20.000 --> 01:17:22.000]  Мы его очень работаем,
[01:17:22.000 --> 01:17:24.000]  ценим, уважаем.
[01:17:24.000 --> 01:17:26.000]  И Пистех школы наша, конечно,
[01:17:26.000 --> 01:17:28.000]  славится
[01:17:28.000 --> 01:17:30.000]  решением задач
[01:17:30.000 --> 01:17:32.000]  нашими профессорами, нашими сотрудниками
[01:17:32.000 --> 01:17:34.000]  научными, которым наш
[01:17:34.000 --> 01:17:36.000]  юбиляр имеет прямейшие отношения.
[01:17:36.000 --> 01:17:38.000]  Вот, поэтому я считаю,
[01:17:38.000 --> 01:17:40.000]  что очень здорово, что это произошло здесь
[01:17:40.000 --> 01:17:42.000]  на площадке Пистеха
[01:17:42.000 --> 01:17:44.000]  в рамках Пистех школы
[01:17:44.000 --> 01:17:46.000]  Прикладной математики и информатики.
[01:17:46.000 --> 01:17:48.000]  Вот, друзья, я прошу прощения.
[01:17:48.000 --> 01:17:50.000]  Я, честное слово,
[01:17:50.000 --> 01:17:52.000]  там все еще существую
[01:17:52.000 --> 01:17:54.000]  в апостасе математика
[01:17:54.000 --> 01:17:56.000]  не только административного работника,
[01:17:56.000 --> 01:17:58.000]  но у меня этих апостасей
[01:17:58.000 --> 01:18:00.000]  три. Я очень люблю читать
[01:18:00.000 --> 01:18:02.000]  лекции и делаю это повсюду.
[01:18:02.000 --> 01:18:04.000]  Вот, вчера был, нет, позавчера был
[01:18:04.000 --> 01:18:06.000]  в Липецке, вот, а скоро
[01:18:06.000 --> 01:18:08.000]  поеду на УРАХ.
[01:18:08.000 --> 01:18:10.000]  Ну и в четверг начинается собственно семестер.
[01:18:10.000 --> 01:18:12.000]  Я буду читать обычные свои
[01:18:12.000 --> 01:18:14.000]  курсы, которые здесь читаю, вот.
[01:18:14.000 --> 01:18:16.000]  И математикой я занимаюсь,
[01:18:16.000 --> 01:18:18.000]  но, к сожалению, очень много времени все-таки съедает
[01:18:18.000 --> 01:18:20.000]  организационная работа.
[01:18:20.000 --> 01:18:22.000]  Это директор ФПМИ,
[01:18:22.000 --> 01:18:24.000]  и я сейчас побегу к ректору.
[01:18:24.000 --> 01:18:26.000]  Вот. Спасибо.
[01:18:26.000 --> 01:18:28.000]  Спасибо, да.
[01:18:30.000 --> 01:18:32.000]  Ясно, что без вас ничего бы не было.
[01:18:32.000 --> 01:18:34.000]  Спасибо.
[01:18:34.000 --> 01:18:36.000]  Это для меня просто. Спасибо большое.
[01:18:36.000 --> 01:18:38.000]  Спасибо вам.
[01:18:38.000 --> 01:18:40.000]  Ну дай бог, хорошо.
[01:18:40.000 --> 01:18:42.000]  Да?
[01:18:42.000 --> 01:18:44.000]  Там все время
[01:18:44.000 --> 01:18:46.000]  административные институты
[01:18:46.000 --> 01:18:48.000]  приняли это так, что
[01:18:48.000 --> 01:18:50.000]  я бы как-то ничего бы
[01:18:50.000 --> 01:18:52.000]  в двадцать четвертого
[01:18:52.000 --> 01:18:54.000]  не формулировал.
[01:18:54.000 --> 01:18:56.000]  А они просто
[01:18:56.000 --> 01:18:58.000]  не писали.
[01:18:58.000 --> 01:19:00.000]  Я не знаю, где это было так написано.
[01:19:00.000 --> 01:19:02.000]  Это Борис что ли сделал? Не Борис Стахович,
[01:19:02.000 --> 01:19:04.000]  а Борис Трохин.
[01:19:04.000 --> 01:19:06.000]  Нет, нет, я написал.
[01:19:06.000 --> 01:19:08.000]  Я написал, что где, кому нужно
[01:19:08.000 --> 01:19:10.000]  это все провозьмут. Заходите ему
[01:19:10.000 --> 01:19:12.000]  заранее, приходите с паспортом.
[01:19:12.000 --> 01:19:14.000]  Мы студентам не нужен.
[01:19:14.000 --> 01:19:16.000]  Видите, вот.
[01:19:16.000 --> 01:19:18.000]  Очень досадно, да.
[01:19:18.000 --> 01:19:20.000]  Но как-то, видимо, не доглядели мы.
[01:19:20.000 --> 01:19:22.000]  Мне кажется, что это нормально было написано,
[01:19:22.000 --> 01:19:24.000]  что если кому-то нужен,
[01:19:24.000 --> 01:19:26.000]  то напишите нам.
[01:19:26.000 --> 01:19:28.000]  А я не знаю, Борис Стахович,
[01:19:28.000 --> 01:19:30.000]  как было-то?
[01:19:30.000 --> 01:19:32.000]  Они подумали, что он тоже нужен так.
[01:19:32.000 --> 01:19:34.000]  Ой, господи.
[01:19:34.000 --> 01:19:36.000]  Да, ты, Стахович, но студентам
[01:19:36.000 --> 01:19:38.000]  подмои крайне рекомендуется.
[01:19:38.000 --> 01:19:40.000]  Они очень интересные студенты.
[01:19:40.000 --> 01:19:42.000]  Не, ну что по-моему, это мы и рассылку
[01:19:42.000 --> 01:19:44.000]  сделали, и в тот сетях это повесили.
[01:19:44.000 --> 01:19:46.000]  В каком-то одном студенте
[01:19:46.000 --> 01:19:48.000]  из высшей школы экономики
[01:19:48.000 --> 01:19:50.000]  вы должны были объявить
[01:19:50.000 --> 01:19:52.000]  сертификаты.
[01:19:52.000 --> 01:19:54.000]  Но сертификат я тоже подтвердил,
[01:19:54.000 --> 01:19:56.000]  что готов подписать.
[01:19:56.000 --> 01:19:58.000]  Да, да.
[01:19:58.000 --> 01:20:00.000]  Это не поздно.
[01:20:00.000 --> 01:20:02.000]  Сертификаты, пожалуйста.
[01:20:02.000 --> 01:20:04.000]  Ну, придется, да, такого плутевки
[01:20:04.000 --> 01:20:06.000]  нет.
[01:20:06.000 --> 01:20:08.000]  Дай бог, да.
[01:20:08.000 --> 01:20:10.000]  Да, нормально.
[01:20:10.000 --> 01:20:12.000]  На самом деле студенты
[01:20:12.000 --> 01:20:14.000]  настолько загруженные по жизни,
[01:20:14.000 --> 01:20:16.000]  что они сейчас на канипалах,
[01:20:16.000 --> 01:20:18.000]  завтра начинают
[01:20:18.000 --> 01:20:20.000]  пассимент.
[01:20:20.000 --> 01:20:22.000]  Они считают, что последний день канипала
[01:20:22.000 --> 01:20:24.000]  надо посидеть дома.
[01:20:24.000 --> 01:20:26.000]  Как-то так.
[01:20:26.000 --> 01:20:28.000]  Четыре пары.
[01:20:28.000 --> 01:20:30.000]  Четыре пары.
[01:20:30.000 --> 01:20:32.000]  Четыре пары.
[01:20:32.000 --> 01:20:34.000]  Ну, нагрузка приличная.
[01:20:34.000 --> 01:20:36.000]  Спасибо вам.
[01:20:42.000 --> 01:20:44.000]  Спасибо.
[01:20:44.000 --> 01:20:46.000]  Спасибо.
[01:20:46.000 --> 01:20:48.000]  Спасибо.
[01:20:48.000 --> 01:20:50.000]  Спасибо.
[01:20:50.000 --> 01:20:52.000]  Спасибо.
[01:20:52.000 --> 01:20:54.000]  Спасибо.
[01:20:54.000 --> 01:20:56.000]  Спасибо.
[01:20:56.000 --> 01:20:58.000]  Спасибо.
[01:20:58.000 --> 01:21:00.000]  Спасибо.
[01:21:00.000 --> 01:21:02.000]  Спасибо.
[01:21:02.000 --> 01:21:04.000]  Спасибо.
[01:21:04.000 --> 01:21:06.000]  Спасибо.
[01:21:08.000 --> 01:21:10.000]  Спасибо.
[01:21:10.000 --> 01:21:12.000]  Спасибо.
[01:21:12.000 --> 01:21:14.000]  Спасибо.
[01:21:14.000 --> 01:21:16.000]  Спасибо.
[01:21:16.000 --> 01:21:18.000]  Спасибо.
[01:21:18.000 --> 01:21:20.000]  Спасибо.
[01:21:20.000 --> 01:21:22.000]  Спасибо.
[01:21:22.000 --> 01:21:24.000]  Спасибо.
[01:21:24.000 --> 01:21:26.000]  Спасибо.
[01:21:26.000 --> 01:21:28.000]  Спасибо.
[01:21:30.000 --> 01:21:32.000]  Спасибо.
[01:21:32.000 --> 01:21:34.000]  Спасибо.
[01:21:34.000 --> 01:21:36.000]  Спасибо.
[01:21:36.000 --> 01:21:38.000]  Спасибо.
[01:21:38.000 --> 01:21:40.000]  Спасибо.
[01:21:42.000 --> 01:21:44.000]  Спасибо.
[01:21:44.000 --> 01:21:46.000]  Спасибо.
[01:21:46.000 --> 01:21:48.000]  Спасибо.
[01:21:48.000 --> 01:21:50.000]  Спасибо.
[01:21:50.000 --> 01:21:52.000]  Спасибо.
[01:21:52.000 --> 01:21:54.000]  Спасибо.
[01:21:54.000 --> 01:21:56.000]  Спасибо.
[01:21:56.000 --> 01:21:58.000]  Спасибо.
[01:21:58.000 --> 01:22:00.000]  Спасибо.
[01:22:00.000 --> 01:22:02.000]  Спасибо.
[01:22:02.000 --> 01:22:04.000]  Спасибо.
[01:22:04.000 --> 01:22:06.000]  Спасибо.
[01:22:06.000 --> 01:22:08.000]  Спасибо.
[01:22:08.000 --> 01:22:10.000]  Спасибо.
[01:22:10.000 --> 01:22:12.000]  Спасибо.
[01:22:12.000 --> 01:22:14.000]  Спасибо.
[01:22:14.000 --> 01:22:16.000]  Спасибо.
[01:22:16.000 --> 01:22:18.000]  Спасибо.
[01:22:18.000 --> 01:22:20.000]  Спасибо.
[01:22:20.000 --> 01:22:22.000]  Спасибо.
[01:22:22.000 --> 01:22:24.000]  Спасибо.
[01:22:24.000 --> 01:22:26.000]  Спасибо.
[01:22:26.000 --> 01:22:28.000]  Спасибо.
[01:22:30.000 --> 01:22:32.000]  Спасибо.
[01:22:32.000 --> 01:22:34.000]  Спасибо.
[01:22:34.000 --> 01:22:36.000]  Спасибо.
[01:22:36.000 --> 01:22:38.000]  Спасибо.
[01:22:38.000 --> 01:22:40.000]  Спасибо.
[01:22:40.000 --> 01:22:42.000]  Спасибо.
[01:22:42.000 --> 01:22:44.000]  Спасибо.
[01:22:44.000 --> 01:22:46.000]  Спасибо.
[01:22:46.000 --> 01:22:48.000]  Спасибо.
[01:22:48.000 --> 01:22:50.000]  Спасибо.
[01:22:50.000 --> 01:22:52.000]  Спасибо.
[01:22:52.000 --> 01:22:54.000]  Спасибо.
[01:22:56.000 --> 01:22:58.000]  Спасибо.
[01:22:58.000 --> 01:23:00.000]  Спасибо.
[01:23:00.000 --> 01:23:02.000]  Спасибо.
[01:23:02.000 --> 01:23:04.000]  Спасибо.
[01:23:04.000 --> 01:23:06.000]  Спасибо.
[01:23:06.000 --> 01:23:08.000]  Спасибо.
[01:23:08.000 --> 01:23:10.000]  Спасибо.
[01:23:10.000 --> 01:23:12.000]  Спасибо.
[01:23:12.000 --> 01:23:14.000]  Спасибо.
[01:23:14.000 --> 01:23:16.000]  Спасибо.
[01:23:16.000 --> 01:23:18.000]  Спасибо.
[01:23:18.000 --> 01:23:20.000]  Спасибо.
[01:23:22.000 --> 01:23:24.000]  Спасибо.
[01:23:26.000 --> 01:23:28.000]  Спасибо.
[01:23:28.000 --> 01:23:30.000]  Спасибо.
[01:23:30.000 --> 01:23:32.000]  Спасибо.
[01:23:32.000 --> 01:23:34.000]  Спасибо.
[01:23:34.000 --> 01:23:36.000]  Спасибо.
[01:23:36.000 --> 01:23:38.000]  Спасибо.
[01:23:38.000 --> 01:23:40.000]  Спасибо.
[01:23:40.000 --> 01:23:42.000]  Спасибо.
[01:23:42.000 --> 01:23:44.000]  Спасибо.
[01:23:44.000 --> 01:23:46.000]  Спасибо.
[01:23:46.000 --> 01:23:48.000]  Спасибо.
[01:23:48.000 --> 01:23:50.000]  Спасибо.
[01:23:50.000 --> 01:23:52.000]  Спасибо.
[01:23:52.000 --> 01:23:54.000]  Спасибо.
[01:23:56.000 --> 01:23:58.000]  Спасибо.
[01:23:58.000 --> 01:24:00.000]  Спасибо.
[01:24:00.000 --> 01:24:02.000]  Спасибо.
[01:24:02.000 --> 01:24:04.000]  Спасибо.
[01:24:04.000 --> 01:24:06.000]  Спасибо.
[01:24:06.000 --> 01:24:08.000]  Спасибо.
[01:24:08.000 --> 01:24:10.000]  Спасибо.
[01:24:10.000 --> 01:24:12.000]  Спасибо.
[01:24:12.000 --> 01:24:14.000]  Спасибо.
[01:24:14.000 --> 01:24:16.000]  Спасибо.
[01:24:16.000 --> 01:24:18.000]  Спасибо.
[01:24:18.000 --> 01:24:20.000]  Спасибо.
[01:24:20.000 --> 01:24:22.000]  Спасибо.
[01:24:24.000 --> 01:24:26.000]  Спасибо.
[01:24:26.000 --> 01:24:28.000]  Спасибо.
[01:24:28.000 --> 01:24:30.000]  Спасибо.
[01:24:30.000 --> 01:24:32.000]  Спасибо.
[01:24:32.000 --> 01:24:34.000]  Спасибо.
[01:24:34.000 --> 01:24:36.000]  Спасибо.
[01:24:36.000 --> 01:24:38.000]  Спасибо.
[01:24:38.000 --> 01:24:40.000]  Спасибо.
[01:24:40.000 --> 01:24:42.000]  Спасибо.
[01:24:42.000 --> 01:24:44.000]  Спасибо.
[01:24:46.000 --> 01:24:48.000]  Спасибо.
[01:24:48.000 --> 01:24:50.000]  Спасибо.
[01:24:50.000 --> 01:24:52.000]  Спасибо.
[01:24:52.000 --> 01:24:54.000]  Спасибо.
[01:24:54.000 --> 01:24:56.000]  Спасибо.
[01:24:58.000 --> 01:25:00.000]  Спасибо.
[01:25:00.000 --> 01:25:02.000]  Спасибо.
[01:25:02.000 --> 01:25:04.000]  Спасибо.
[01:25:04.000 --> 01:25:06.000]  Спасибо.
[01:25:06.000 --> 01:25:08.000]  Спасибо.
[01:25:08.000 --> 01:25:10.000]  Спасибо.
[01:25:10.000 --> 01:25:12.000]  Спасибо.
[01:25:12.000 --> 01:25:14.000]  Спасибо.
[01:25:14.000 --> 01:25:16.000]  Спасибо.
[01:25:16.000 --> 01:25:18.000]  Спасибо.
[01:25:18.000 --> 01:25:20.000]  Спасибо.
[01:25:20.000 --> 01:25:22.000]  Спасибо.
[01:25:22.000 --> 01:25:24.000]  Спасибо.
[01:25:24.000 --> 01:25:26.000]  Спасибо.
[01:25:26.000 --> 01:25:28.000]  Спасибо.
[01:25:28.000 --> 01:25:30.000]  Спасибо.
[01:25:30.000 --> 01:25:32.000]  Спасибо.
[01:25:32.000 --> 01:25:34.000]  Спасибо.
[01:25:34.000 --> 01:25:36.000]  Спасибо.
[01:25:36.000 --> 01:25:38.000]  Спасибо.
[01:25:38.000 --> 01:25:40.000]  Спасибо.
[01:25:40.000 --> 01:25:42.000]  Спасибо.
[01:25:42.000 --> 01:25:44.000]  Спасибо.
[01:25:44.000 --> 01:25:46.000]  Спасибо.
[01:25:48.000 --> 01:25:50.000]  Спасибо.
[01:25:50.000 --> 01:25:52.000]  Спасибо.
[01:25:54.000 --> 01:25:56.000]  Спасибо.
[01:25:56.000 --> 01:25:58.000]  Спасибо.
[01:25:58.000 --> 01:26:00.000]  Спасибо.
[01:26:00.000 --> 01:26:02.000]  Спасибо.
[01:26:02.000 --> 01:26:04.000]  Спасибо.
[01:26:04.000 --> 01:26:06.000]  Спасибо.
[01:26:06.000 --> 01:26:08.000]  Спасибо.
[01:26:08.000 --> 01:26:10.000]  Спасибо.
[01:26:10.000 --> 01:26:12.000]  Спасибо.
[01:26:12.000 --> 01:26:14.000]  Спасибо.
[01:26:14.000 --> 01:26:16.000]  Спасибо.
[01:26:16.000 --> 01:26:18.000]  Спасибо.
[01:26:18.000 --> 01:26:20.000]  Спасибо.
[01:26:20.000 --> 01:26:22.000]  Спасибо.
[01:26:22.000 --> 01:26:24.000]  Спасибо.
[01:26:24.000 --> 01:26:26.000]  Спасибо.
[01:26:26.000 --> 01:26:28.000]  Спасибо.
[01:26:28.000 --> 01:26:30.000]  Спасибо.
[01:26:30.000 --> 01:26:32.000]  Спасибо.
[01:26:32.000 --> 01:26:34.000]  Спасибо.
[01:26:34.000 --> 01:26:36.000]  Спасибо.
[01:26:36.000 --> 01:26:38.000]  Спасибо.
[01:26:38.000 --> 01:26:40.000]  Спасибо.
[01:26:42.000 --> 01:26:44.000]  Спасибо.
[01:26:44.000 --> 01:26:46.000]  Спасибо.
[01:26:46.000 --> 01:26:48.000]  Спасибо.
[01:26:48.000 --> 01:26:50.000]  Спасибо.
[01:26:50.000 --> 01:26:52.000]  Спасибо.
[01:26:52.000 --> 01:26:54.000]  Спасибо.
[01:26:54.000 --> 01:26:56.000]  Спасибо.
[01:26:56.000 --> 01:26:58.000]  Спасибо.
[01:26:58.000 --> 01:27:00.000]  Спасибо.
[01:27:00.000 --> 01:27:02.000]  Спасибо.
[01:27:02.000 --> 01:27:04.000]  Спасибо.
[01:27:04.000 --> 01:27:06.000]  Спасибо.
[01:27:06.000 --> 01:27:08.000]  Спасибо.
[01:27:08.000 --> 01:27:10.000]  Спасибо.
[01:27:12.000 --> 01:27:14.000]  Спасибо.
[01:27:14.000 --> 01:27:16.000]  Спасибо.
[01:27:16.000 --> 01:27:18.000]  Спасибо.
[01:27:18.000 --> 01:27:20.000]  Спасибо.
[01:27:20.000 --> 01:27:22.000]  Спасибо.
[01:27:22.000 --> 01:27:24.000]  Спасибо.
[01:27:24.000 --> 01:27:26.000]  Спасибо.
[01:27:26.000 --> 01:27:28.000]  Спасибо.
[01:27:28.000 --> 01:27:30.000]  Спасибо.
[01:27:30.000 --> 01:27:32.000]  Спасибо.
[01:27:32.000 --> 01:27:34.000]  Спасибо.
[01:27:34.000 --> 01:27:36.000]  Спасибо.
[01:27:36.000 --> 01:27:38.000]  Спасибо.
[01:27:40.000 --> 01:27:42.000]  Спасибо.
[01:27:42.000 --> 01:27:44.000]  Спасибо.
[01:27:44.000 --> 01:27:46.000]  Спасибо.
[01:27:46.000 --> 01:27:48.000]  Спасибо.
[01:27:48.000 --> 01:27:50.000]  Спасибо.
[01:27:50.000 --> 01:27:52.000]  Спасибо.
[01:27:52.000 --> 01:27:54.000]  Спасибо.
[01:27:56.000 --> 01:27:58.000]  Спасибо.
[01:27:58.000 --> 01:28:00.000]  Спасибо.
[01:28:00.000 --> 01:28:02.000]  Спасибо.
[01:28:02.000 --> 01:28:04.000]  Спасибо.
[01:28:04.000 --> 01:28:11.600]  бетча, по небольшому количеству бетча, мы получим хорошую оценку
[01:28:11.600 --> 01:28:15.760]  для мю и сигма параметра для чистой бетча.
[01:28:15.760 --> 01:28:22.720]  Таким образом, ритория между Н, всем бетчам, и М, базентинным бетчам,
[01:28:22.720 --> 01:28:27.760]  что означает вероятность изменить чистую бетчу,
[01:28:27.760 --> 01:28:32.720]  что больше, чем половина, присутствует в этой фигурке.
[01:28:34.720 --> 01:28:42.400]  Мы видим в этой фигурке, и мы видим, что в Y-косе полный бензин,
[01:28:43.120 --> 01:28:50.800]  в X-косе базентинский бензин, и мы видим гипотезу между размерами каждого косе.
[01:28:50.800 --> 01:29:00.000]  Таким образом, ритория между Н, всем бетчам, и М, базентинским бетчам,
[01:29:00.000 --> 01:29:10.400]  по 2%. Например, 20 базентинских бетчей, по 1000 базентинских бетчах.
[01:29:11.040 --> 01:29:16.960]  Таким образом, если ритория базентинского бетча найдена новым методом, то вероятность,
[01:29:17.840 --> 01:29:24.800]  что любой другой код в данной фигурке содержит базентинский бензин,
[01:29:25.360 --> 01:29:29.440]  это не так. Так что, наша цель,
[01:29:30.560 --> 01:29:37.200]  это вычислять большинство чистых бетчей, и оставлять статистические параметры,
[01:29:37.200 --> 01:29:43.280]  как Му и Сигма, из неназначных базентинских бензинов в данной фигурке.
[01:29:44.080 --> 01:29:50.320]  И оставляем статистические параметры, и я сейчас покажу, что это алгоритм-1.
[01:29:56.720 --> 01:29:59.760]  Оставляем статистические параметры.
[01:30:02.720 --> 01:30:06.320]  В степле 1 мы выбираем бетч Н-сампляр,
[01:30:06.320 --> 01:30:08.560]  мы выбираем бетч Н-сампляр,
[01:30:09.920 --> 01:30:11.680]  это начальник.
[01:30:14.000 --> 01:30:22.720]  В степле 2 мы оставляем Му и Сигма для каждого бетча по статистическому параметру,
[01:30:25.520 --> 01:30:32.720]  и в степле 3 мы получаем истограмм каждого бетча Му и Сигма.
[01:30:33.200 --> 01:30:37.840]  Это значит, что, например, если я делаю это тысячу раз,
[01:30:38.720 --> 01:30:43.680]  то получаю тысячу результатов от Му и Сигма, и я могу создать истограмму.
[01:30:47.040 --> 01:30:54.560]  И в степле 4 медиа Му и Сигма, мы думаем, будет чистая.
[01:30:55.280 --> 01:30:57.280]  Это идея алгоритма-1.
[01:31:00.560 --> 01:31:04.800]  Алгоритм-2, он говорит о новой технике для того, чтобы вывести субъективные данные.
[01:31:07.360 --> 01:31:12.800]  В степле 1 мы dividим истограмму в бетч-сампляр.
[01:31:13.680 --> 01:31:21.760]  Например, в этой фигурке мы можем увидеть, что бетч equal to N-сампляр.
[01:31:21.760 --> 01:31:33.680]  Если θ equal to 19, мы считаем actual number of the data item in every bin.
[01:31:33.680 --> 01:31:47.760]  Например, для х equal to 117, количество данных 118.
[01:31:51.760 --> 01:32:00.080]  В степле 3 мы оформляем количество данных в каждом бетч-сампляре по интегралу нормальной ковы
[01:32:00.800 --> 01:32:02.000]  и считаем F.
[01:32:03.280 --> 01:32:28.560]  Например, для х equal to 170, Му 118, Сигма 16, N equal to 8,019, F equal to 182.
[01:32:30.080 --> 01:32:45.120]  В степле 4, если рация между F и C выше 1 и ψ, данные данных в этой бетч-сампляре сомневаются.
[01:32:46.080 --> 01:33:02.160]  Например, здесь мы можем увидеть, что рация между F и C для х equal to 117 около 1.
[01:33:02.160 --> 01:33:06.160]  Так что эта бетч-сампляра не сомневается.
[01:33:06.960 --> 01:33:20.240]  И мы делаем эксперимент, и мы видим, что данные с артифичными данными 100% аккуратны.
[01:33:20.240 --> 01:33:27.760]  Если мы добавляем артифичные данные, коррупционные данные,
[01:33:28.160 --> 01:33:35.040]  то аккуратность машинной учебы, классификация машинной учебы,
[01:33:35.600 --> 01:33:47.520]  снизится до 78%, и если вы используете наш систему и выключите субъективные данные из данных,
[01:33:48.480 --> 01:33:55.760]  то аккуратность машинной учебы не 100%, а очень высокая.
[01:33:55.760 --> 01:34:04.640]  Так что это хороший способ чистить данные, прежде чем начать использовать машинную учебу.
[01:34:05.120 --> 01:34:14.480]  Следующая часть этого работа, делаясь с коррупцией существующих данных,
[01:34:14.480 --> 01:34:18.480]  с сингл-финшер-учебной учебой, с определенным уровнем.
[01:34:19.840 --> 01:34:27.920]  Мы продолжаем рассматривать то, что часть данных в функции коррупционной,
[01:34:27.920 --> 01:34:33.920]  то есть то, что часть данных в функции коррупционной.
[01:34:35.600 --> 01:34:43.040]  Наша цель здесь, это найти центральный уровень каждого замена в дистрибуции,
[01:34:43.680 --> 01:34:51.280]  в случае, когда верхний связь на количество коррупционных данных известен.
[01:34:51.280 --> 01:34:58.320]  Эта продолжительность предыдущего делается с сингл-финшер-учебной учебой,
[01:34:58.320 --> 01:35:04.320]  где первое делается с попыткой найти замену коррупционной учебы,
[01:35:04.320 --> 01:35:12.320]  а второе делается с неисчастной попыткой.
[01:35:13.120 --> 01:35:19.120]  Эстограмм этого замена цветен зеленым,
[01:35:19.120 --> 01:35:29.120]  где черный вертикал в коррете эстограмма отличается от зеленого замена с лабелом плюс и минус один.
[01:35:31.360 --> 01:35:40.160]  И лабел бизонтиндата имеет инверсантный лабел в связи с лабелом
[01:35:40.160 --> 01:35:45.440]  безбизонтиндата, с тем же значением, чтобы достигнуть нашего цельа,
[01:35:46.160 --> 01:35:53.200]  мы изменили генеральный метод, который связан с инфлюенцией бизонтиндата.
[01:35:55.680 --> 01:35:59.840]  Таким образом, алгоритм номер 3 найти центральный уровень.
[01:36:02.160 --> 01:36:09.120]  Центральный уровень зета каждого замена изменивается ферлоэквизионом,
[01:36:11.120 --> 01:36:17.360]  л is the number of data items that are labeled as minus one,
[01:36:18.240 --> 01:36:24.160]  l plus one is the number of data items that are labeled as plus one,
[01:36:25.040 --> 01:36:32.880]  n is the number of data items in the bin, and xi is the number of corrupted data items.
[01:36:32.880 --> 01:36:43.200]  Степа 1. Возьмите данный датасет. Сортируйте данный датасампель по значению и создайте
[01:36:47.120 --> 01:36:54.800]  этот эстограмм. Степа 2. Возьмите номер замена в каждой бинке и подсчитайте
[01:36:54.800 --> 01:37:01.520]  количество замен, которые были названы плюсом 1 и минусом 1.
[01:37:03.520 --> 01:37:10.560]  Степа 3. Возьмите центральный уровень в каждой бинке, центральный уровень зета в каждой бинке
[01:37:11.200 --> 01:37:19.600]  в equation, в соответствии с equation 1 и 2, и субъективность на размере
[01:37:19.600 --> 01:37:36.320]  максимума х, например, здесь 8, так что мы можем увидеть, что мы конкурсируем зета
[01:37:36.320 --> 01:37:52.880]  для каждого звука, например, в специфическом бинке, например, в бинке 117 или в бинке 171,
[01:37:53.760 --> 01:37:56.960]  и мы конкурсируем зета для
[01:37:59.920 --> 01:38:01.040]  плюсом 1
[01:38:01.040 --> 01:38:10.560]  и минус 1, и мы подсчитываем, что максимум данного бизонтинга в каждой бинке
[01:38:12.640 --> 01:38:16.880]  8, не выше 8.
[01:38:19.840 --> 01:38:27.840]  В общем, в котором коррупционные данные являются частью датасета и могут появляться в двух методах.
[01:38:27.840 --> 01:38:32.240]  В целом, вся функция коррупционна,
[01:38:34.080 --> 01:38:42.480]  одна опция, и вторая часть функции в датасете коррупционна, и вторая часть чистая.
[01:38:44.080 --> 01:38:51.120]  Обратите внимание, что есть несколько правил коррупционировать всю функцию, включая
[01:38:51.120 --> 01:38:56.160]  и ввертая классификацию датасета,
[01:38:56.720 --> 01:39:07.600]  выбирая датасет рандомных данных, или производить классификацию, не консистируя с классификацией других не коррупционных функций.
[01:39:07.600 --> 01:39:22.720]  Например, мы можем увидеть, что здесь, внутри истограмма, мы имеем коррупционные данные с разным
[01:39:22.720 --> 01:39:38.160]  лабелем, и черная линия отличается между лабелем минус 1, лабелем плюс 1, и коррупционным данным внутри истограммы.
[01:39:41.680 --> 01:39:45.120]  Метод, чтобы изменить влияние коррупционных данных.
[01:39:46.080 --> 01:39:52.800]  Наша техника измена рандомных деревьев с решением дерева, но каждый решительный деревьев,
[01:39:53.680 --> 01:39:58.160]  который создан в зависимости от значения рандомных векторов,
[01:39:58.720 --> 01:40:06.640]  представляет сет рандомных колонн, выбранных из тренировочных данных.
[01:40:06.640 --> 01:40:13.680]  Датасета, большие числа деревьев, созданы для создания рандомных деревьев.
[01:40:15.040 --> 01:40:18.240]  После того, как эти деревья созданы, каждый
[01:40:20.720 --> 01:40:28.400]  датасет из тренировочных данных пройдет через этот решительный дерево.
[01:40:29.120 --> 01:40:36.400]  Когда датасет приезжает к деревьям,
[01:40:37.280 --> 01:40:49.440]  его цельфикация сравнится с классом, плюс или минус 1. Когда цельфикация и класс согласны,
[01:40:50.400 --> 01:40:56.800]  то правильная цель цель деревьев увеличивается, а другим образом,
[01:40:57.600 --> 01:41:04.400]  значение правильного цель деревьев увеличивается.
[01:41:08.960 --> 01:41:16.400]  Таким образом, от одной функции до несколько функций, представив центральный уровень как инструмент,
[01:41:16.400 --> 01:41:16.880]  shorten,
[01:41:23.760 --> 01:41:31.840]  Hence, we use C 44.5 as a test case.
[01:41:35.440 --> 01:41:43.200]  anniversary
[01:41:43.200 --> 01:41:50.200]  В первом этапе мы видим рандомные векторы, которые выбирают от тренировки данных.
[01:41:50.200 --> 01:41:57.200]  Это пример рандомных векторов, и это лабель этого примера.
[01:41:57.200 --> 01:42:05.200]  В втором этапе, используя любой рандомный вектор-альгоритм, мы будем строить классифицированный вектор.
[01:42:05.200 --> 01:42:10.200]  Это мы видим здесь. Это основано на этих результатах.
[01:42:10.200 --> 01:42:21.200]  И каждый вектор от тренировки данных пройдет по этому решению.
[01:42:21.200 --> 01:42:24.200]  Рядомые и вектор-альгоритмы находятся.
[01:42:25.200 --> 01:42:40.200]  Например, здесь мы видим, что эта версификация получает правильную 30 раз и правильную 1 раз.
[01:42:40.200 --> 01:42:45.200]  Здесь 20, правильная, 10 и правильная.
[01:42:45.200 --> 01:42:52.200]  И третья правильная 3 и правильная 5.
[01:42:54.200 --> 01:42:58.200]  Степа 4.
[01:42:58.200 --> 01:43:03.200]  Большое количество этих классификаций будет создавать.
[01:43:03.200 --> 01:43:09.200]  Каждый новый вектор от теста данных пройдет по этому вектору.
[01:43:09.200 --> 01:43:12.200]  И мы получим форст результатов.
[01:43:12.200 --> 01:43:21.200]  Фильм-кассификация каждого вектора будет определена по классификации.
[01:43:22.200 --> 01:43:27.200]  Это решение вектор-альгоритм.
[01:43:27.200 --> 01:43:35.200]  Если вы приходите к самолеву, это означает, что это решение вектор-альгоритм.
[01:43:35.200 --> 01:43:37.200]  И правильные и правильные.
[01:43:37.200 --> 01:43:41.200]  И они получены от каждого вектор-альгоритма.
[01:43:41.200 --> 01:43:47.200]  Это означает, что решение не только по классификации,
[01:43:47.200 --> 01:43:54.200]  это решение вектор-альгоритм.
[01:43:54.200 --> 01:44:04.200]  Это означает, что я верю в эту классификацию больше, чем верю в эту классификацию.
[01:44:04.200 --> 01:44:13.200]  Потому что, когда я перейду в тренировку данных вектор-альгоритм,
[01:44:13.200 --> 01:44:20.200]  я вижу, что в большинстве случаев вектор-альгоритм дает хорошие результаты.
[01:44:20.200 --> 01:44:26.200]  А в большинстве случаев вектор-альгоритм дает плохие результаты.
[01:44:26.200 --> 01:44:34.200]  Это, например, что мы делаем на реальном данном вектор-альгоритме.
[01:44:34.200 --> 01:44:48.200]  И мы можем увидеть, например, здесь, что в этом вектор-альгоритме я получаю 22 правильные и 0 правильные.
[01:44:49.200 --> 01:45:01.200]  И здесь, например, в этом вектор-альгоритме я вижу 5 правильных и 2 правильных.
[01:45:01.200 --> 01:45:06.200]  Это на реальном данном вектор-альгоритме.
[01:45:06.200 --> 01:45:17.200]  И здесь, например, я вижу 3 правильных, но 348 правильных.
[01:45:17.200 --> 01:45:24.200]  Итак, алгоритм 4 идентифицирует и фильтрует данные вектор-альгоритма.
[01:45:24.200 --> 01:45:28.200]  Сначала выбираем номер 3, чтобы быть генератором.
[01:45:28.200 --> 01:45:30.200]  Например, к.
[01:45:30.200 --> 01:45:33.200]  К может быть 500.
[01:45:33.200 --> 01:45:38.200]  Как сейчас я хочу создать 3?
[01:45:38.200 --> 01:45:46.200]  В степи 2, для к от 1 до bk,
[01:45:47.200 --> 01:45:50.200]  вектор theta k будет генератором,
[01:45:50.200 --> 01:46:00.200]  где theta k представляет датасампель-селект для создания 3,
[01:46:00.200 --> 01:46:07.200]  конструкция 3, используя алгоритм 3.
[01:46:07.200 --> 01:46:14.200]  Это значит, что сейчас в степи 2 я строю рандо-форст.
[01:46:14.200 --> 01:46:23.200]  В степи 3, каждый инстанкт из тренировки дата пройдет по этому решению.
[01:46:23.200 --> 01:46:32.200]  И для каждого лифта, если количество инстанктов, которые правильно и неправильно оцениваны,
[01:46:32.200 --> 01:46:40.200]  считают, то процедура правильного и правильного оценивания оценивана.
[01:46:40.200 --> 01:46:51.200]  В степи 4, каждый инстанкт из тренировки дата пройдет по этому решению и получит оценивание.
[01:46:51.200 --> 01:47:02.200]  И в степи 5, каждый новый инстанкт получит результаты оценивания рандо-форст.
[01:47:02.200 --> 01:47:11.200]  И в степи 6, каждый инстанкт из тренировки дата пройдет по этому решению и получит оценивание.
[01:47:11.200 --> 01:47:21.200]  В степи 7, каждый инстанкт из тренировки дата пройдет по этому решению и получит оценивание.
[01:47:21.200 --> 01:47:32.200]  И для каждого лифта, если количество инстанктов, которые правильно и неправильно оцениваны,
[01:47:32.200 --> 01:47:45.200]  то процедура правильного оценивания для каждого инстанкта будет определена по сравнению с правильным оцениванием для плюс 1,
[01:47:45.200 --> 01:47:56.200]  и по сравнению с правильным оцениванием для плюс 1, которые правильны для каждого лифта.
[01:47:56.200 --> 01:48:13.200]  Это важно. Это то, как мы можем решить, что мы дадим, например, другую классификацию,
[01:48:13.200 --> 01:48:24.200]  если мы можем увидеть, что определенный лифт дает нам 100% правильные результаты.
[01:48:24.200 --> 01:48:35.200]  Например, если какой-то лифт все время дает мне правильные результаты в классификации, в финансификации,
[01:48:35.200 --> 01:48:45.200]  я могу изменить финансификацию, например, от плюс 1 до минус 1.
[01:48:45.200 --> 01:48:59.200]  И это эксперимент и результаты. Мы можем увидеть, когда вы используете для сет-имиды какой-то тренировочный сет,
[01:48:59.200 --> 01:49:18.200]  то мы видим, что мы получаем 88% успеха.
[01:49:18.200 --> 01:49:25.200]  И если мы изменили правильный оценивание, мы изменили его к 92.
[01:49:25.200 --> 01:49:38.200]  И мы можем увидеть все время, что результат новой техники лучше всего правильного оценивания.
[01:49:38.200 --> 01:49:46.200]  Это значит, что с этим способом наш правильный оценивание будет лучше.
[01:49:46.200 --> 01:49:53.200]  Так что это результат нового алгоритма оценивания и коррупции данных.
[01:49:53.200 --> 01:49:57.200]  Конклюзия и будущая работа.
[01:49:57.200 --> 01:50:02.200]  Три метода по делению с коррупцией данных.
[01:50:02.200 --> 01:50:07.200]  Безотинный данный инструмент.
[01:50:07.200 --> 01:50:11.200]  Коррупция данных.
[01:50:11.200 --> 01:50:22.200]  И несколько функций, которые полностью коррупционированы.
[01:50:22.200 --> 01:50:30.200]  Идея для классификации используя репутацию источника.
[01:50:30.200 --> 01:50:42.200]  И учебный алгоритм, который базируется на репутации данных в последней части.
[01:50:42.200 --> 01:50:45.200]  Так что это мой разговор.
[01:50:45.200 --> 01:50:52.200]  Если у вас есть вопросы, мы будем рады.
[01:51:00.200 --> 01:51:17.200]  Я не слышу ничего.
[01:51:17.200 --> 01:51:21.200]  Можете задать мне вопросы?
[01:51:21.200 --> 01:51:25.200]  Проводите.
[01:51:25.200 --> 01:51:28.200]  Спасибо за твой разговор.
[01:51:28.200 --> 01:51:33.200]  Интересно узнать, как вы изменили гиперпериметры рандома,
[01:51:33.200 --> 01:51:39.200]  в течение этих четырех версий алгоритма, которые вы предложили?
[01:51:39.200 --> 01:51:48.200]  Мы получили оригинальный алгоритм рандома и код.
[01:51:48.200 --> 01:51:56.200]  Мы получили оригинальный код для тех, кто взял рандом, Лео Брайман.
[01:51:56.200 --> 01:52:03.200]  Он дает нам код в фортране.
[01:52:03.200 --> 01:52:07.200]  И оригинальный код, который он писал.
[01:52:07.200 --> 01:52:14.200]  И мы его переводили в C++, и мы получили этот код.
[01:52:14.200 --> 01:52:29.200]  Этот оригинальный код Лео Брайман использовал для нас, чтобы улучшить его технику и результаты, которые вы можете увидеть здесь.
[01:52:29.200 --> 01:52:34.200]  Это не Python или что-то. Это оригинальный код Браймана.
[01:52:34.200 --> 01:52:45.200]  Ну, в принципе, вы можете помнить, например, что было количество оригинальных кодов, которые вы использовали?
[01:52:45.200 --> 01:52:53.200]  Потому что я хотел бы понять, как рандомные гиперпериметры рандома меняются.
[01:52:53.200 --> 01:52:57.200]  Когда вы используете свой метод или перед использованием.
[01:52:57.200 --> 01:53:00.200]  У вас есть такие результаты?
[01:53:00.200 --> 01:53:05.200]  Вы знаете, это немного старый артикаль.
[01:53:05.200 --> 01:53:10.200]  Так что я не помню точно, но я могу проверить, вы знаете.
[01:53:10.200 --> 01:53:14.200]  Если вы прислали имен или что-то, я могу проверить этот вопрос.
[01:53:14.200 --> 01:53:16.200]  Хорошо, хорошо, спасибо.
[01:53:16.200 --> 01:53:19.200]  Не забудьте, что у меня есть конкретные результаты.
[01:53:19.200 --> 01:53:21.200]  Хорошо, спасибо.
[01:53:21.200 --> 01:53:22.200]  Спасибо.
[01:53:22.200 --> 01:53:25.200]  Еще какие-то вопросы?
[01:53:25.200 --> 01:53:28.200]  У нас есть еще одна вопроса.
[01:53:28.200 --> 01:53:29.200]  Спасибо.
[01:53:29.200 --> 01:53:34.200]  Может быть, эта вопроса не должна быть адресованной вам офису, профессору Долеву,
[01:53:34.200 --> 01:53:42.200]  но факт, что рандомные данные называют Бизантином, я думаю, это сильно политически неправильно.
[01:53:42.200 --> 01:53:45.200]  Я думаю, что грековские коллеги могут быть обвинены этим.
[01:53:45.200 --> 01:53:53.200]  И, как мы русские, мы должны быть в частности обвинены Бизантином, Романом, также.
[01:53:53.200 --> 01:53:59.200]  Так что этот терм звучит хеленофобически и немного русофобически.
[01:53:59.200 --> 01:54:03.200]  Но, может быть, он был предпочитан ранее.
[01:54:03.200 --> 01:54:07.200]  Но я не поддерживаю использование этого терма для коррупции данных.
[01:54:07.200 --> 01:54:08.200]  Спасибо.
[01:54:08.200 --> 01:54:10.200]  Хорошо.
[01:54:10.200 --> 01:54:12.200]  Спасибо.
[01:54:12.200 --> 01:54:17.200]  В то время, когда я слышал о вас, я должен вам рассказать, но хорошо.
[01:54:18.200 --> 01:54:22.200]  Это что-то из истории.
[01:54:22.200 --> 01:54:25.200]  Да, я думаю так.
[01:54:25.200 --> 01:54:29.200]  Он был предпочитан ранее.
[01:54:29.200 --> 01:54:33.200]  Я бы хотел бы еще раз спасибо вам.
[01:54:33.200 --> 01:54:37.200]  Моя хибуя действительно ограничена.
[01:54:37.200 --> 01:54:41.200]  Я бы хотел бы еще раз спасибо вам.
[01:54:41.200 --> 01:54:43.200]  Моя хибуя действительно ограничена.
[01:54:43.200 --> 01:54:51.200]  Или по языку вашего вася, мы с вами посвящены.
[01:54:51.200 --> 01:54:55.200]  И, что еще, Барен Даниэлл.
[01:54:55.200 --> 01:54:57.200]  Спасибо снова.
[01:54:57.200 --> 01:55:02.200]  Спасибо.
[01:55:02.200 --> 01:55:09.200]  Дорогие коллеги, я надеюсь, что вы слышите меня.
[01:55:09.200 --> 01:55:13.200]  Меня зовут Яков Карандашев.
[01:55:13.200 --> 01:55:21.200]  Я презентирую интуитивный институт системной анализы русской академии
[01:55:21.200 --> 01:55:24.200]  и Рудеяна Университета.
[01:55:24.200 --> 01:55:31.200]  Мы с коллегами делали эту работу.
[01:55:31.200 --> 01:55:37.200]  Позвольте мне поделиться, чтобы показать вам наш результат.
[01:55:37.200 --> 01:55:43.200]  Топика моего презентации – анамалия детекции с нейронетворами,
[01:55:43.200 --> 01:55:46.200]  использованием ингредиентов анамалии.
[01:55:46.200 --> 01:55:50.200]  Что насчет этого?
[01:55:50.200 --> 01:55:54.200]  Проблема, который мы рассматриваем,
[01:55:54.200 --> 01:55:59.200]  дается от нашего клиента,
[01:55:59.200 --> 01:56:04.200]  который производит полные цветы.
[01:56:04.200 --> 01:56:07.200]  Это русская производительница,
[01:56:08.200 --> 01:56:15.200]  и они попросили нас
[01:56:15.200 --> 01:56:22.200]  сделать оптимизацию процесса
[01:56:22.200 --> 01:56:24.200]  изображений.
[01:56:24.200 --> 01:56:28.200]  Какие изображения?
[01:56:28.200 --> 01:56:30.200]  Какая полная телесканера?
[01:56:30.200 --> 01:56:32.200]  Это большая коробка,
[01:56:32.200 --> 01:56:35.200]  где вы можете входить,
[01:56:35.200 --> 01:56:38.200]  и этот аппарат
[01:56:38.200 --> 01:56:40.200]  изображает вам
[01:56:40.200 --> 01:56:43.200]  один из изображений на фоне,
[01:56:43.200 --> 01:56:45.200]  и этот изображение
[01:56:45.200 --> 01:56:47.200]  изображен в X-ray,
[01:56:47.200 --> 01:56:52.200]  в X-ray рамке спектра.
[01:56:52.200 --> 01:56:54.200]  Это разнообразно от,
[01:56:54.200 --> 01:56:56.200]  может быть вы знаете,
[01:56:56.200 --> 01:56:59.200]  других сканеров, которые обычно
[01:56:59.200 --> 01:57:01.200]  используют в аэропорту,
[01:57:01.200 --> 01:57:04.200]  но в аэропорту обычно используют
[01:57:04.200 --> 01:57:07.200]  миллиметровые сканеры.
[01:57:07.200 --> 01:57:10.200]  Вот здесь X-ray сканеры.
[01:57:10.200 --> 01:57:12.200]  Они не очень популярны,
[01:57:12.200 --> 01:57:16.200]  но они имеют большие преимущества,
[01:57:16.200 --> 01:57:18.200]  потому что они могут
[01:57:18.200 --> 01:57:22.200]  посмотреть не только на поверхность вашего тела,
[01:57:22.200 --> 01:57:25.200]  но и на вашу тело.
[01:57:25.200 --> 01:57:27.200]  Конечно,
[01:57:27.200 --> 01:57:29.200]  в аэропорту
[01:57:29.200 --> 01:57:31.200]  эти сканеры
[01:57:33.200 --> 01:57:35.200]  имеют
[01:57:35.200 --> 01:57:37.200]  некоторые опасности,
[01:57:37.200 --> 01:57:39.200]  но
[01:57:39.200 --> 01:57:41.200]  действительно
[01:57:41.200 --> 01:57:43.200]  доза
[01:57:43.200 --> 01:57:45.200]  дозы X-ray
[01:57:45.200 --> 01:57:47.200]  радиации
[01:57:47.200 --> 01:57:49.200]  очень маленькие.
[01:57:49.200 --> 01:57:51.200]  Здесь вы можете увидеть
[01:57:51.200 --> 01:57:53.200]  сравнение дозы
[01:57:53.200 --> 01:57:55.200]  от
[01:57:55.200 --> 01:57:57.200]  натуральных процессов
[01:57:57.200 --> 01:57:59.200]  или
[01:57:59.200 --> 01:58:01.200]  официальных процессов.
[01:58:01.200 --> 01:58:03.200]  И
[01:58:03.200 --> 01:58:05.200]  как производитель
[01:58:05.200 --> 01:58:07.200]  этого аппарата
[01:58:07.200 --> 01:58:09.200]  говорит,
[01:58:09.200 --> 01:58:11.200]  доза
[01:58:11.200 --> 01:58:13.200]  получена
[01:58:13.200 --> 01:58:15.200]  от тела
[01:58:15.200 --> 01:58:17.200]  100
[01:58:17.200 --> 01:58:19.200]  меньше
[01:58:19.200 --> 01:58:21.200]  чем доза
[01:58:21.200 --> 01:58:23.200]  от
[01:58:23.200 --> 01:58:25.200]  типического
[01:58:25.200 --> 01:58:27.200]  дозы
[01:58:29.200 --> 01:58:31.200]  я не могу показать
[01:58:31.200 --> 01:58:33.200]  это
[01:58:33.200 --> 01:58:35.200]  окей
[01:58:35.200 --> 01:58:37.200]  типичный доза дозы
[01:58:37.200 --> 01:58:39.200]  когда вы
[01:58:39.200 --> 01:58:41.200]  делаете
[01:58:41.200 --> 01:58:43.200]  медицинскую обзору
[01:58:43.200 --> 01:58:45.200]  в
[01:58:45.200 --> 01:58:47.200]  в медицинском центре
[01:58:47.200 --> 01:58:49.200]  вы типичные
[01:58:49.200 --> 01:58:51.200]  обзорите дозу
[01:58:51.200 --> 01:58:53.200]  и
[01:58:55.200 --> 01:58:57.200]  доза
[01:58:57.200 --> 01:58:59.200]  радиации
[01:58:59.200 --> 01:59:01.200]  около
[01:59:01.200 --> 01:59:03.200]  0,1
[01:59:03.200 --> 01:59:05.200]  милизиверта
[01:59:05.200 --> 01:59:07.200]  и
[01:59:07.200 --> 01:59:09.200]  этот аппарат
[01:59:09.200 --> 01:59:11.200]  полбодисканера
[01:59:11.200 --> 01:59:13.200]  100 меньше
[01:59:13.200 --> 01:59:15.200]  так что
[01:59:15.200 --> 01:59:17.200]  это не очень опасно
[01:59:17.200 --> 01:59:19.200]  и
[01:59:19.200 --> 01:59:21.200]  они
[01:59:21.200 --> 01:59:23.200]  типично
[01:59:23.200 --> 01:59:25.200]  используют
[01:59:25.200 --> 01:59:27.200]  в
[01:59:27.200 --> 01:59:29.200]  в тюрьмах
[01:59:29.200 --> 01:59:31.200]  ты русский?
[01:59:31.200 --> 01:59:33.200]  конечно
[01:59:33.200 --> 01:59:35.200]  ты же все смеешься
[01:59:35.200 --> 01:59:37.200]  нет, если это возможно
[01:59:37.200 --> 01:59:39.200]  я с удовольствием переключусь
[01:59:39.200 --> 01:59:41.200]  потому что мы с детьми
[01:59:41.200 --> 01:59:43.200]  мы с детьми
[01:59:43.200 --> 01:59:45.200]  мы с детьми
[01:59:45.200 --> 01:59:47.200]  мы с детьми
[01:59:47.200 --> 01:59:49.200]  я с удовольствием переключусь
[01:59:49.200 --> 01:59:51.200]  потому что мы с детьми
[01:59:51.200 --> 01:59:53.200]  спасибо
[01:59:55.200 --> 01:59:57.200]  а также
[01:59:57.200 --> 01:59:59.200]  на каких-то заводах
[01:59:59.200 --> 02:00:01.200]  откуда нельзя ничего выносить
[02:00:01.200 --> 02:00:03.200]  но в принципе потенциально
[02:00:03.200 --> 02:00:05.200]  можно использовать в аэропортах
[02:00:05.200 --> 02:00:07.200]  поскольку как я сказал дозы очень низкие
[02:00:07.200 --> 02:00:09.200]  гораздо не
[02:00:09.200 --> 02:00:11.200]  окей
[02:00:11.200 --> 02:00:13.200]  так
[02:00:13.200 --> 02:00:15.200]  значит что
[02:00:15.200 --> 02:00:17.200]  представляют собой данные
[02:00:17.200 --> 02:00:19.200]  данные это
[02:00:19.200 --> 02:00:21.200]  одноканальные картинки
[02:00:21.200 --> 02:00:23.200]  вот здесь вот слева
[02:00:23.200 --> 02:00:25.200]  изначальное изображение
[02:00:25.200 --> 02:00:27.200]  то есть это
[02:00:27.200 --> 02:00:29.200]  картинка форматики
[02:00:29.200 --> 02:00:31.200]  15 бит
[02:00:31.200 --> 02:00:33.200]  на фиксель
[02:00:33.200 --> 02:00:35.200]  и
[02:00:35.200 --> 02:00:37.200]  что требуется
[02:00:37.200 --> 02:00:39.200]  да я как-то пропустил это
[02:00:39.200 --> 02:00:41.200]  что требуется
[02:00:41.200 --> 02:00:43.200]  находить все подозрительные объекты
[02:00:43.200 --> 02:00:45.200]  подчащееся к телу человека
[02:00:45.200 --> 02:00:47.200]  то есть чтобы человек ничего не пронес
[02:00:47.200 --> 02:00:49.200]  ничего не вынес
[02:00:49.200 --> 02:00:51.200]  в общем запрещен
[02:00:51.200 --> 02:00:53.200]  вот
[02:00:53.200 --> 02:00:55.200]  сейчас этим занимается
[02:00:55.200 --> 02:00:57.200]  то есть
[02:00:57.200 --> 02:00:59.200]  совместно с этими сканерами
[02:00:59.200 --> 02:01:01.200]  сидит оператор который
[02:01:01.200 --> 02:01:03.200]  смотрит на эти картинки
[02:01:03.200 --> 02:01:05.200]  то есть человек заходит его фотографирует
[02:01:05.200 --> 02:01:07.200]  и оператор смотрит на это изображение
[02:01:07.200 --> 02:01:09.200]  применяют к нему различные
[02:01:09.200 --> 02:01:11.200]  фильтры увеличивает уменьшает
[02:01:11.200 --> 02:01:13.200]  смотрит какие-то в общем подозрительные места
[02:01:13.200 --> 02:01:15.200]  ну секунд может
[02:01:15.200 --> 02:01:17.200]  10 15 20
[02:01:17.200 --> 02:01:19.200]  у него уходит на то чтобы
[02:01:19.200 --> 02:01:21.200]  визуально смотреть это и пропустить
[02:01:21.200 --> 02:01:23.200]  человек
[02:01:23.200 --> 02:01:25.200]  соответственно задача оптимизации
[02:01:25.200 --> 02:01:27.200]  конечно очень важна
[02:01:27.200 --> 02:01:29.200]  и первая задача
[02:01:29.200 --> 02:01:31.200]  которую мы
[02:01:31.200 --> 02:01:33.200]  поставили для себя
[02:01:33.200 --> 02:01:35.200]  это собственно
[02:01:35.200 --> 02:01:37.200]  как с этими снимками вообще обходиться
[02:01:37.200 --> 02:01:39.200]  потому что изначально на них ничего не видно
[02:01:39.200 --> 02:01:41.200]  просто черный на белом фоне
[02:01:41.200 --> 02:01:43.200]  естественно нужно было
[02:01:43.200 --> 02:01:45.200]  как-то отфильтровать
[02:01:45.200 --> 02:01:47.200]  поменять может контрастность
[02:01:47.200 --> 02:01:49.200]  яркость какие-то применить
[02:01:49.200 --> 02:01:51.200]  адаптивные гистограммы
[02:01:51.200 --> 02:01:53.200]  вот здесь вот показана
[02:01:53.200 --> 02:01:55.200]  процедура некоторая которую мы для себя
[02:01:55.200 --> 02:01:57.200]  подобрали это
[02:01:57.200 --> 02:01:59.200]  несколько операций которые приводят
[02:01:59.200 --> 02:02:01.200]  к тому что по крайней мере
[02:02:01.200 --> 02:02:03.200]  на фото становятся видны внутренние органы
[02:02:03.200 --> 02:02:05.200]  кости
[02:02:05.200 --> 02:02:07.200]  ну то есть она не становится
[02:02:07.200 --> 02:02:09.200]  становится более
[02:02:09.200 --> 02:02:11.200]  радуированной более
[02:02:11.200 --> 02:02:13.200]  приятной для визуального
[02:02:13.200 --> 02:02:15.200]  осмотра
[02:02:17.200 --> 02:02:19.200]  вот зачем нам
[02:02:19.200 --> 02:02:21.200]  это нужно было значит
[02:02:21.200 --> 02:02:23.200]  мы рассмотрели эту задачу
[02:02:23.200 --> 02:02:25.200]  ну то есть что здесь являются
[02:02:25.200 --> 02:02:27.200]  потенциально запрещенными
[02:02:27.200 --> 02:02:29.200]  опасными объектами вообще задача изначально
[02:02:29.200 --> 02:02:31.200]  стала даже не так а просто находить
[02:02:31.200 --> 02:02:33.200]  все что не относится к телу человека вот здесь
[02:02:33.200 --> 02:02:35.200]  скажем видно на рисунке что у него что-то
[02:02:35.200 --> 02:02:37.200]  на шее какая-то цепочка
[02:02:37.200 --> 02:02:39.200]  также есть что-то в карманах
[02:02:39.200 --> 02:02:41.200]  на руках
[02:02:41.200 --> 02:02:43.200]  ну там на поясе
[02:02:43.200 --> 02:02:45.200]  вот то есть все эти объекты
[02:02:45.200 --> 02:02:47.200]  необходимо автоматически выделить
[02:02:47.200 --> 02:02:49.200]  и собственно
[02:02:49.200 --> 02:02:51.200]  мы
[02:02:51.200 --> 02:02:53.200]  как-то мне за временем следить
[02:02:57.200 --> 02:02:59.200]  хорошо
[02:03:05.200 --> 02:03:07.200]  то как мы решали эту задачу
[02:03:07.200 --> 02:03:09.200]  вот нам дали ее
[02:03:09.200 --> 02:03:11.200]  мы подумали ну наверно
[02:03:11.200 --> 02:03:13.200]  классический компьютер vision
[02:03:13.200 --> 02:03:15.200]  какого-нибудь нейросеть для компьютерного зрения
[02:03:15.200 --> 02:03:17.200]  мы вполне могли бы применить
[02:03:17.200 --> 02:03:19.200]  какую нейросеть ну наверно
[02:03:19.200 --> 02:03:21.200]  для сегментации изображений
[02:03:21.200 --> 02:03:23.200]  поскольку здесь изображение нужно на нем выделить
[02:03:23.200 --> 02:03:25.200]  какие-то объекты
[02:03:25.200 --> 02:03:27.200]  то есть мы поставили задачу себе как
[02:03:27.200 --> 02:03:29.200]  сегментацию
[02:03:29.200 --> 02:03:31.200]  и чтобы свести эту задачу
[02:03:31.200 --> 02:03:33.200]  к обучению
[02:03:33.200 --> 02:03:35.200]  с учителем нам был необходим
[02:03:35.200 --> 02:03:37.200]  размеченный датасет
[02:03:37.200 --> 02:03:39.200]  то есть нам нужно
[02:03:39.200 --> 02:03:41.200]  не просто набор картин
[02:03:41.200 --> 02:03:43.200]  у нас много, нам дали сразу 1600 картин
[02:03:43.200 --> 02:03:45.200]  потом еще добавили 3000
[02:03:45.200 --> 02:03:47.200]  снимков
[02:03:47.200 --> 02:03:49.200]  нам нужен размеченный датасет
[02:03:49.200 --> 02:03:51.200]  и
[02:03:51.200 --> 02:03:53.200]  собственно мы это
[02:03:53.200 --> 02:03:55.200]  делали вручную с помощью
[02:03:55.200 --> 02:03:57.200]  label.me
[02:03:57.200 --> 02:03:59.200]  я думаю многие из вас знакомы с этой программой
[02:03:59.200 --> 02:04:01.200]  то есть фактически человек
[02:04:01.200 --> 02:04:03.200]  вручную смотрит на эти снимки
[02:04:03.200 --> 02:04:05.200]  уже визуально
[02:04:05.200 --> 02:04:07.200]  обработанное
[02:04:07.200 --> 02:04:09.200]  чтобы было легче выделять
[02:04:09.200 --> 02:04:11.200]  вручную такие
[02:04:11.200 --> 02:04:13.200]  подозрительные вещи
[02:04:13.200 --> 02:04:15.200]  и естественно
[02:04:15.200 --> 02:04:17.200]  мы попробовали
[02:04:17.200 --> 02:04:19.200]  также сервисы
[02:04:19.200 --> 02:04:21.200]  такие как Яндекс.Лока
[02:04:21.200 --> 02:04:23.200]  на который можно
[02:04:23.200 --> 02:04:25.200]  аутсорсить подобные задачи
[02:04:25.200 --> 02:04:27.200]  чтобы другие люди занимались вот этой
[02:04:27.200 --> 02:04:29.200]  вручной разметкой
[02:04:29.200 --> 02:04:31.200]  поэтому
[02:04:31.200 --> 02:04:33.200]  опыту
[02:04:33.200 --> 02:04:35.200]  это оказалось
[02:04:35.200 --> 02:04:37.200]  не очень удобно
[02:04:37.200 --> 02:04:39.200]  потому что во-первых нужно
[02:04:39.200 --> 02:04:41.200]  людей походу заставлять это делать
[02:04:41.200 --> 02:04:43.200]  как-то усложнять им задачу
[02:04:43.200 --> 02:04:45.200]  потому что нужно за ними проверять
[02:04:45.200 --> 02:04:47.200]  за ними нужно проверять
[02:04:47.200 --> 02:04:49.200]  и то качество, которое они дают
[02:04:49.200 --> 02:04:51.200]  на выходе
[02:04:51.200 --> 02:04:53.200]  всякое бывает
[02:04:53.200 --> 02:04:55.200]  кто-то хорошо размечает вот здесь пример
[02:04:55.200 --> 02:04:57.200]  на русском
[02:04:57.200 --> 02:04:59.200]  вот
[02:04:59.200 --> 02:05:01.200]  а кто-то
[02:05:01.200 --> 02:05:03.200]  просто тыкает
[02:05:03.200 --> 02:05:05.200]  более-менее рядом и всё
[02:05:05.200 --> 02:05:07.200]  какие-то объекты выделяются
[02:05:07.200 --> 02:05:09.200]  не сильно хорошо
[02:05:09.200 --> 02:05:11.200]  но тем не менее
[02:05:11.200 --> 02:05:13.200]  с помощью Яндекс.Локи
[02:05:13.200 --> 02:05:15.200]  а также с помощью самостоятельной доработки
[02:05:15.200 --> 02:05:17.200]  моих коллег
[02:05:17.200 --> 02:05:19.200]  мы разметили эти 1600 снимков
[02:05:19.200 --> 02:05:21.200]  и
[02:05:21.200 --> 02:05:23.200]  дальше
[02:05:23.200 --> 02:05:25.200]  начали применять
[02:05:25.200 --> 02:05:27.200]  обучение с учительным с помощью нейросетей
[02:05:27.200 --> 02:05:29.200]  в качестве нейросетей
[02:05:29.200 --> 02:05:31.200]  мы выбрали Юнет
[02:05:31.200 --> 02:05:33.200]  почему Юнет?
[02:05:33.200 --> 02:05:35.200]  Юнет это моя любимая архитектура нейросетева
[02:05:35.200 --> 02:05:37.200]  я её очень
[02:05:37.200 --> 02:05:39.200]  обожаю
[02:05:39.200 --> 02:05:41.200]  использую везде, где только можно
[02:05:41.200 --> 02:05:43.200]  она очень красивая
[02:05:43.200 --> 02:05:45.200]  имметричная
[02:05:45.200 --> 02:05:47.200]  у неё прозрачная архитектура
[02:05:47.200 --> 02:05:49.200]  ну понятная архитектура
[02:05:49.200 --> 02:05:51.200]  её легко учить
[02:05:51.200 --> 02:05:53.200]  она достаточно быстро учится
[02:05:53.200 --> 02:05:55.200]  1600 снимков оказалось
[02:05:55.200 --> 02:05:57.200]  вполне достаточно
[02:05:57.200 --> 02:05:59.200]  чтобы её обучать
[02:06:05.200 --> 02:06:07.200]  я даже здесь привёл
[02:06:07.200 --> 02:06:09.200]  ещё пару ссылок
[02:06:09.200 --> 02:06:11.200]  на другие мои работы, которые я не представляю
[02:06:11.200 --> 02:06:13.200]  на этой конференции
[02:06:13.200 --> 02:06:15.200]  но мне было бы приятно
[02:06:15.200 --> 02:06:17.200]  если бы вы посмотрели их
[02:06:17.200 --> 02:06:19.200]  или задали какие-нибудь вопросы
[02:06:19.200 --> 02:06:21.200]  это тоже по применению
[02:06:21.200 --> 02:06:23.200]  архитектуры Юнет
[02:06:23.200 --> 02:06:25.200]  с модификациями
[02:06:25.200 --> 02:06:27.200]  в данном случае это не конволюционная
[02:06:27.200 --> 02:06:29.200]  Юнета, обычно полно связанная
[02:06:29.200 --> 02:06:31.200]  мы её использовали
[02:06:31.200 --> 02:06:33.200]  для другой задачи
[02:06:33.200 --> 02:06:35.200]  предсказание химической кинетики
[02:06:35.200 --> 02:06:37.200]  когда у вас есть временной ряд
[02:06:37.200 --> 02:06:39.200]  и вам нужно предсказывать
[02:06:39.200 --> 02:06:41.200]  по текущему значению t
[02:06:41.200 --> 02:06:43.200]  значение концентрации химических элементов
[02:06:43.200 --> 02:06:45.200]  момент t плюс дельта t
[02:06:45.200 --> 02:06:47.200]  эта же самая архитектура
[02:06:47.200 --> 02:06:49.200]  очень успешно справилась
[02:06:49.200 --> 02:06:51.200]  и получилась достаточно хорошо
[02:06:51.200 --> 02:06:53.200]  здесь на рисунке пример
[02:06:53.200 --> 02:06:55.200]  предсказания
[02:06:55.200 --> 02:06:57.200]  видно, что они чуть-чуть не совпадают
[02:06:57.200 --> 02:06:59.200]  но это самый худший пример
[02:06:59.200 --> 02:07:01.200]  в самом деле они обычно совпадают
[02:07:01.200 --> 02:07:03.200]  другая работа
[02:07:03.200 --> 02:07:05.200]  сейчас она ещё не опубликована
[02:07:05.200 --> 02:07:07.200]  на английском, пока только в русском журнале
[02:07:07.200 --> 02:07:09.200]  связана с фоторетографией
[02:07:09.200 --> 02:07:11.200]  в микроэлектронике
[02:07:11.200 --> 02:07:13.200]  здесь тоже мы применяли
[02:07:13.200 --> 02:07:15.200]  архитектур Юнет для задач фоторетографии
[02:07:15.200 --> 02:07:17.200]  когда вы предсказываете
[02:07:17.200 --> 02:07:19.200]  по вашему шаблону
[02:07:19.200 --> 02:07:21.200]  то есть у вас есть
[02:07:21.200 --> 02:07:23.200]  фоторетографический шаблон
[02:07:23.200 --> 02:07:25.200]  с помощью которого вы хотите
[02:07:25.200 --> 02:07:27.200]  создавать некоторые структуры
[02:07:27.200 --> 02:07:29.200]  в кристалле
[02:07:29.200 --> 02:07:31.200]  и вы предсказываете то
[02:07:31.200 --> 02:07:33.200]  во что они превратятся
[02:07:33.200 --> 02:07:35.200]  после проявления
[02:07:35.200 --> 02:07:37.200]  нескольких процессов физико-химических
[02:07:37.200 --> 02:07:39.200]  Юнет архитектура
[02:07:39.200 --> 02:07:41.200]  отлично справляется с этой задачей
[02:07:41.200 --> 02:07:43.200]  и вообще изначально
[02:07:43.200 --> 02:07:45.200]  её можно назвать
[02:07:45.200 --> 02:07:47.200]  такой простенькой моделью
[02:07:47.200 --> 02:07:49.200]  для перевода картинки
[02:07:49.200 --> 02:07:51.200]  в картинку
[02:07:51.200 --> 02:07:53.200]  когда у вас что-то переходит
[02:07:53.200 --> 02:07:55.200]  в такого же типа объект
[02:07:57.200 --> 02:07:59.200]  вот
[02:07:59.200 --> 02:08:01.200]  для нашей задачи
[02:08:01.200 --> 02:08:03.200]  сегментации рентгеновских изображений
[02:08:03.200 --> 02:08:05.200]  мы чуть-чуть её
[02:08:05.200 --> 02:08:07.200]  сократили
[02:08:07.200 --> 02:08:09.200]  сделали всего трёхуровневый
[02:08:09.200 --> 02:08:11.200]  чтобы она была компактнее
[02:08:11.200 --> 02:08:13.200]  меньше параметров
[02:08:13.200 --> 02:08:15.200]  чтобы быстрее работала
[02:08:15.200 --> 02:08:17.200]  в принципе качество
[02:08:17.200 --> 02:08:19.200]  а нет мы пробовали
[02:08:19.200 --> 02:08:21.200]  разные 3 и 5 уровни
[02:08:21.200 --> 02:08:23.200]  да в принципе
[02:08:23.200 --> 02:08:25.200]  5 уровней чуть получше оказалось
[02:08:25.200 --> 02:08:27.200]  мы здесь представлены на рисунке трёхуровневых
[02:08:27.200 --> 02:08:29.200]  значит
[02:08:29.200 --> 02:08:31.200]  для тех кто может быть
[02:08:31.200 --> 02:08:33.200]  не знает я вкратце объясню
[02:08:33.200 --> 02:08:35.200]  что здесь нарисовано
[02:08:35.200 --> 02:08:37.200]  что представляет собой это
[02:08:37.200 --> 02:08:39.200]  ну или давайте на
[02:08:39.200 --> 02:08:41.200]  оригинальном рисунке объясним
[02:08:41.200 --> 02:08:43.200]  что здесь происходит
[02:08:43.200 --> 02:08:45.200]  значит архитектура юнет
[02:08:45.200 --> 02:08:47.200]  это архитектура
[02:08:47.200 --> 02:08:49.200]  которая в чём-то напоминается вроде как
[02:08:49.200 --> 02:08:51.200]  автоэнкодер
[02:08:51.200 --> 02:08:53.200]  то есть комбалиционный
[02:08:53.200 --> 02:08:55.200]  но на самом деле она сильно
[02:08:55.200 --> 02:08:57.200]  от неё отличается
[02:08:57.200 --> 02:08:59.200]  она вот этими прямыми стрелочками
[02:08:59.200 --> 02:09:01.200]  которые являются
[02:09:01.200 --> 02:09:03.200]  skip connection
[02:09:03.200 --> 02:09:05.200]  прямые соединения
[02:09:05.200 --> 02:09:07.200]  которые соединяют вход к выходам
[02:09:07.200 --> 02:09:09.200]  для информации без искажений
[02:09:09.200 --> 02:09:11.200]  а уже вся нейросетевая часть
[02:09:11.200 --> 02:09:13.200]  вот эти вот комбалиционные слои
[02:09:13.200 --> 02:09:15.200]  которые синенькими квадратами
[02:09:15.200 --> 02:09:17.200]  прямогольниками показаны
[02:09:17.200 --> 02:09:19.200]  это уже всё residuals
[02:09:19.200 --> 02:09:21.200]  то есть как бы остатки
[02:09:21.200 --> 02:09:23.200]  потому что основной поток информации
[02:09:23.200 --> 02:09:25.200]  он идёт без изменений
[02:09:25.200 --> 02:09:27.200]  по этим прямым стрелкам
[02:09:27.200 --> 02:09:29.200]  критентальным стрелкам
[02:09:29.200 --> 02:09:31.200]  все дополнения
[02:09:31.200 --> 02:09:33.200]  изменения, модификации
[02:09:33.200 --> 02:09:35.200]  идут в обход
[02:09:35.200 --> 02:09:37.200]  и на самом деле чем больше уровней
[02:09:37.200 --> 02:09:39.200]  у этой сети
[02:09:39.200 --> 02:09:41.200]  тем в принципе может быть
[02:09:41.200 --> 02:09:43.200]  меньше эти уровни
[02:09:43.200 --> 02:09:45.200]  и значимых, потому что я уверен
[02:09:45.200 --> 02:09:47.200]  что начиная с некоторого уровня
[02:09:47.200 --> 02:09:49.200]  это
[02:09:49.200 --> 02:09:51.200]  изменения будут
[02:09:51.200 --> 02:09:53.200]  незначительны
[02:09:53.200 --> 02:09:55.200]  я это не проверял, но
[02:09:55.200 --> 02:09:57.200]  по общим соображениям скорее всего
[02:09:57.200 --> 02:09:59.200]  это так, поэтому 3-5 уровней
[02:09:59.200 --> 02:10:01.200]  вполне достаточно
[02:10:01.200 --> 02:10:03.200]  больше наверное и не нужно
[02:10:03.200 --> 02:10:05.200]  вот
[02:10:05.200 --> 02:10:07.200]  так
[02:10:07.200 --> 02:10:09.200]  и
[02:10:09.200 --> 02:10:11.200]  чем хороша это значит
[02:10:11.200 --> 02:10:13.200]  нейросеть тем, что
[02:10:13.200 --> 02:10:15.200]  она может переводить картинку в картинку
[02:10:15.200 --> 02:10:17.200]  вы на входе подаёте некоторое изображение
[02:10:17.200 --> 02:10:19.200]  а на выходе вы ожидаете другое изображение
[02:10:19.200 --> 02:10:21.200]  но того же самого например размера
[02:10:21.200 --> 02:10:23.200]  в данном случае
[02:10:23.200 --> 02:10:25.200]  у нас на выходе ожидается
[02:10:25.200 --> 02:10:27.200]  вот такая карта аномалий
[02:10:27.200 --> 02:10:29.200]  то есть это бинарная маска
[02:10:29.200 --> 02:10:31.200]  где белым отмечены
[02:10:31.200 --> 02:10:33.200]  зоничками отмечены места
[02:10:33.200 --> 02:10:35.200]  в которых
[02:10:35.200 --> 02:10:37.200]  человек выделил подозрительные
[02:10:37.200 --> 02:10:39.200]  объекты, а черным отмечен
[02:10:39.200 --> 02:10:41.200]  фон, там где вроде ничего
[02:10:41.200 --> 02:10:43.200]  страшного
[02:10:43.200 --> 02:10:45.200]  не наблюдалось
[02:10:45.200 --> 02:10:47.200]  дальше следовала
[02:10:47.200 --> 02:10:49.200]  стандартная архитектура обучения
[02:10:49.200 --> 02:10:51.200]  мы используем Pytorch
[02:10:51.200 --> 02:10:53.200]  и обучили эту сеть
[02:10:53.200 --> 02:10:55.200]  вот самые
[02:10:55.200 --> 02:10:57.200]  первые результаты
[02:10:57.200 --> 02:10:59.200]  как оно получилось
[02:10:59.200 --> 02:11:01.200]  на левом изображении
[02:11:01.200 --> 02:11:03.200]  исходная тип картинка
[02:11:03.200 --> 02:11:05.200]  посередине это
[02:11:05.200 --> 02:11:07.200]  маска, которая предсказана нейросетью
[02:11:07.200 --> 02:11:09.200]  а на выходе мы их
[02:11:09.200 --> 02:11:11.200]  наложили на предобработанное изображение
[02:11:11.200 --> 02:11:13.200]  чтобы было лучше видно
[02:11:13.200 --> 02:11:15.200]  вот эту маску
[02:11:15.200 --> 02:11:17.200]  и видно, что нейросеть
[02:11:17.200 --> 02:11:19.200]  выделяет основные аномалии
[02:11:19.200 --> 02:11:21.200]  но
[02:11:21.200 --> 02:11:23.200]  часть вещей
[02:11:23.200 --> 02:11:25.200]  она отмечает плохо
[02:11:25.200 --> 02:11:27.200]  на укрупнённом справа элементе
[02:11:27.200 --> 02:11:29.200]  это всё нарисовано
[02:11:29.200 --> 02:11:31.200]  в кармане
[02:11:31.200 --> 02:11:33.200]  указка точно не работает
[02:11:33.200 --> 02:11:35.200]  в кармане видно
[02:11:35.200 --> 02:11:37.200]  что телефон
[02:11:37.200 --> 02:11:39.200]  она не выделила
[02:11:39.200 --> 02:11:41.200]  выделила только маленький зелёный
[02:11:41.200 --> 02:11:43.200]  элемент этого
[02:11:43.200 --> 02:11:45.200]  то есть
[02:11:45.200 --> 02:11:47.200]  обращать заказчику
[02:11:47.200 --> 02:11:49.200]  показывая ему
[02:11:49.200 --> 02:11:51.200]  он нам даёт много таких примеров
[02:11:51.200 --> 02:11:53.200]  что вот это она выделила
[02:11:53.200 --> 02:11:55.200]  а вот это нет
[02:11:55.200 --> 02:11:57.200]  и с этим
[02:11:57.200 --> 02:11:59.200]  тяжело бороться
[02:11:59.200 --> 02:12:01.200]  потому что мы не можем пофиксить это
[02:12:01.200 --> 02:12:03.200]  вручную, сказав, а ну да у нас там
[02:12:03.200 --> 02:12:05.200]  телефоны не размечаются
[02:12:05.200 --> 02:12:07.200]  мы не можем сказать конкретно
[02:12:07.200 --> 02:12:09.200]  что в каком случае
[02:12:09.200 --> 02:12:11.200]  отмечается нейросеть
[02:12:11.200 --> 02:12:13.200]  а что нет
[02:12:13.200 --> 02:12:15.200]  это всегда такой тонкий момент
[02:12:15.200 --> 02:12:17.200]  с нейросетями
[02:12:17.200 --> 02:12:19.200]  нельзя, например, сказать
[02:12:19.200 --> 02:12:21.200]  что будет
[02:12:21.200 --> 02:12:23.200]  и нельзя улучшить результат
[02:12:23.200 --> 02:12:25.200]  даже осознав
[02:12:25.200 --> 02:12:27.200]  что она вот что-то
[02:12:27.200 --> 02:12:29.200]  какие-то вещи не видит
[02:12:29.200 --> 02:12:31.200]  чтобы это сделать
[02:12:31.200 --> 02:12:33.200]  нужно существенно что-то менять
[02:12:33.200 --> 02:12:35.200]  либо менять обучающую выборку
[02:12:35.200 --> 02:12:37.200]  либо архитектуру
[02:12:37.200 --> 02:12:39.200]  либо способ обучения
[02:12:39.200 --> 02:12:41.200]  либо какие-то серьёзные манипуляции
[02:12:41.200 --> 02:12:43.200]  что
[02:12:47.200 --> 02:12:49.200]  что было дальше
[02:12:49.200 --> 02:12:51.200]  нам предложено
[02:12:51.200 --> 02:12:53.200]  разработано
[02:12:53.200 --> 02:12:55.200]  идея была следующая
[02:12:55.200 --> 02:12:57.200]  точнее даже не идея
[02:12:57.200 --> 02:12:59.200]  была выявлена следующая проблема
[02:12:59.200 --> 02:13:01.200]  заказчик сказал, что
[02:13:01.200 --> 02:13:03.200]  на картинках отлично распознаются
[02:13:03.200 --> 02:13:05.200]  пуговицы
[02:13:05.200 --> 02:13:07.200]  пуговицы, молнии, какие-нибудь кольца
[02:13:07.200 --> 02:13:09.200]  часы, в общем, всё металлическое
[02:13:09.200 --> 02:13:11.200]  прекрасно распознаётся, но оно реально
[02:13:11.200 --> 02:13:13.200]  очень хорошо видно
[02:13:13.200 --> 02:13:15.200]  оно очень твёрдое, непрозрачно
[02:13:15.200 --> 02:13:17.200]  для ремгена и поэтому выглядит как
[02:13:17.200 --> 02:13:19.200]  чёрные области
[02:13:19.200 --> 02:13:21.200]  а вот какие-то другие
[02:13:21.200 --> 02:13:23.200]  подозрительные вещи плохо распознаются
[02:13:23.200 --> 02:13:25.200]  но мы пронозировали
[02:13:25.200 --> 02:13:27.200]  это и поняли, что да, действительно так
[02:13:27.200 --> 02:13:29.200]  оказалось, что
[02:13:29.200 --> 02:13:31.200]  просто
[02:13:31.200 --> 02:13:33.200]  проблема в самих данных
[02:13:33.200 --> 02:13:35.200]  1600 снимков, хоть их много
[02:13:35.200 --> 02:13:37.200]  но в них
[02:13:37.200 --> 02:13:39.200]  реально нет тех
[02:13:39.200 --> 02:13:41.200]  подозрительных объектов, каких-то
[02:13:41.200 --> 02:13:43.200]  полупрозрачных пакетов, каких-то
[02:13:43.200 --> 02:13:45.200]  не знаю
[02:13:45.200 --> 02:13:47.200]  каких-то, в общем, вещей, которые
[02:13:47.200 --> 02:13:49.200]  не столь
[02:13:49.200 --> 02:13:51.200]  однозначно
[02:13:51.200 --> 02:13:53.200]  видимые
[02:13:53.200 --> 02:13:55.200]  то есть их трудно
[02:13:55.200 --> 02:13:57.200]  ну их просто нет
[02:13:57.200 --> 02:13:59.200]  потому что они нам давали с проходной
[02:13:59.200 --> 02:14:01.200]  какого-то завода
[02:14:01.200 --> 02:14:03.200]  где люди обычно нормальные
[02:14:03.200 --> 02:14:05.200]  они проходят туда
[02:14:05.200 --> 02:14:07.200]  в халатах без ничего
[02:14:07.200 --> 02:14:09.200]  и как бы самое запрещенное, что можно
[02:14:09.200 --> 02:14:11.200]  что могут быть они принести
[02:14:11.200 --> 02:14:13.200]  это вот действительно какие-то типочки
[02:14:13.200 --> 02:14:15.200]  какие-нибудь там у них
[02:14:15.200 --> 02:14:17.200]  молнии и кнопки на одежде
[02:14:17.200 --> 02:14:19.200]  вот, и все
[02:14:19.200 --> 02:14:21.200]  то есть они реально добросовестные
[02:14:21.200 --> 02:14:23.200]  люди, естественно
[02:14:23.200 --> 02:14:25.200]  никаких важных
[02:14:25.200 --> 02:14:27.200]  опасных объектов, хоть выбраки нет
[02:14:27.200 --> 02:14:29.200]  то есть, ну
[02:14:29.200 --> 02:14:31.200]  не может заказчик нам дать 1600
[02:14:31.200 --> 02:14:33.200]  снимков с запрещенным объектом
[02:14:33.200 --> 02:14:35.200]  ему нужно поморочиться
[02:14:35.200 --> 02:14:37.200]  чтобы это сделать, попросить
[02:14:37.200 --> 02:14:39.200]  множество людей, специально
[02:14:39.200 --> 02:14:41.200]  спрятать что-то там, в общем
[02:14:41.200 --> 02:14:43.200]  и мы подумали
[02:14:43.200 --> 02:14:45.200]  что может быть как-то обойтись
[02:14:45.200 --> 02:14:47.200]  обойтись без этого
[02:14:47.200 --> 02:14:49.200]  и сделать
[02:14:49.200 --> 02:14:51.200]  какой-нибудь собственный генератор аномалий
[02:14:51.200 --> 02:14:53.200]  то есть, просто программу
[02:14:53.200 --> 02:14:55.200]  которая бы накладывала на эти изображения
[02:14:55.200 --> 02:14:57.200]  какие-нибудь полутоновые
[02:14:57.200 --> 02:14:59.200]  объекты, похожие
[02:14:59.200 --> 02:15:01.200]  на какие-нибудь
[02:15:01.200 --> 02:15:03.200]  удивительные вещи
[02:15:03.200 --> 02:15:05.200]  вот, ну в общем
[02:15:05.200 --> 02:15:07.200]  мы сделали такой генератор
[02:15:07.200 --> 02:15:09.200]  ой, а я думал у нас полный
[02:15:09.200 --> 02:15:11.200]  экран, да? Заголовки все
[02:15:11.200 --> 02:15:13.200]  закрыты
[02:15:13.200 --> 02:15:15.200]  ну, в общем
[02:15:15.200 --> 02:15:17.200]  в общем мы сделали генератор
[02:15:17.200 --> 02:15:19.200]  аномалий, которые
[02:15:19.200 --> 02:15:21.200]  генерируют два типа объектов
[02:15:37.200 --> 02:15:39.200]  да, не, в заголовках ничего интересного
[02:15:39.200 --> 02:15:41.200]  вот
[02:15:41.200 --> 02:15:43.200]  значит, идея генератора в том
[02:15:43.200 --> 02:15:45.200]  чтобы генерировать
[02:15:45.200 --> 02:15:47.200]  разные типа объектов, вот в качестве таких объектов
[02:15:47.200 --> 02:15:49.200]  мы взяли два типа, это
[02:15:49.200 --> 02:15:51.200]  многоугольники, там, треугольники
[02:15:51.200 --> 02:15:53.200]  квадраты, всякие, в общем
[02:15:53.200 --> 02:15:55.200]  многоугольники, которые
[02:15:55.200 --> 02:15:57.200]  просто случайно генерируются, и второе это
[02:15:57.200 --> 02:15:59.200]  мы взяли известную всем базу
[02:15:59.200 --> 02:16:01.200]  данных в вашем МНИСК, где всякие
[02:16:01.200 --> 02:16:03.200]  элементы одежды, там
[02:16:03.200 --> 02:16:05.200]  сумочки, юбки, штаны, шорты
[02:16:05.200 --> 02:16:07.200]  обувь, вот, и они
[02:16:09.200 --> 02:16:11.200]  в общем их
[02:16:11.200 --> 02:16:13.200]  как раз просто
[02:16:13.200 --> 02:16:15.200]  брали как некоторые
[02:16:15.200 --> 02:16:17.200]  ну, V-образные формы, да
[02:16:17.200 --> 02:16:19.200]  то есть, не совсем такие прямоугольные
[02:16:19.200 --> 02:16:21.200]  вот, а
[02:16:21.200 --> 02:16:23.200]  интересные различные формы, вот их
[02:16:23.200 --> 02:16:25.200]  накладывали в случайные места, уменьшали
[02:16:25.200 --> 02:16:27.200]  увеличили, да
[02:16:29.200 --> 02:16:31.200]  вот
[02:16:31.200 --> 02:16:33.200]  ну, в помощи генератора скажу честно
[02:16:33.200 --> 02:16:35.200]  о, класс
[02:16:35.200 --> 02:16:37.200]  ну, удалось
[02:16:37.200 --> 02:16:39.200]  удалось действительно
[02:16:39.200 --> 02:16:41.200]  получить большую
[02:16:41.200 --> 02:16:43.200]  выборку
[02:16:43.200 --> 02:16:45.200]  полутоновых, каких-то
[02:16:45.200 --> 02:16:47.200]  вот, таких
[02:16:47.200 --> 02:16:49.200]  накладываемых объектов
[02:16:49.200 --> 02:16:51.200]  и это
[02:16:51.200 --> 02:16:53.200]  расширило выборку, то есть
[02:16:53.200 --> 02:16:55.200]  генератор в данном случае рассматривается просто как
[02:16:55.200 --> 02:16:57.200]  аугументация нашей обучающей
[02:16:57.200 --> 02:16:59.200]  уголки, то есть мы
[02:16:59.200 --> 02:17:01.200]  ну, вот, стандартный способ
[02:17:01.200 --> 02:17:03.200]  аугументации, это когда вы там
[02:17:03.200 --> 02:17:05.200]  картинку поворачиваете
[02:17:05.200 --> 02:17:07.200]  сдвигаете, обрезаете, чуть-чуть
[02:17:07.200 --> 02:17:09.200]  увеличиваете, меняете
[02:17:09.200 --> 02:17:11.200]  яркость, вот, а здесь еще
[02:17:11.200 --> 02:17:13.200]  дополнительно накладываете, не шум
[02:17:13.200 --> 02:17:15.200]  шум было бы слишком легко, гауссовый
[02:17:15.200 --> 02:17:17.200]  какой-нибудь шум накладывать, нет
[02:17:17.200 --> 02:17:19.200]  здесь вы именно накладываете какие-то прям
[02:17:19.200 --> 02:17:21.200]  такие пятна
[02:17:21.200 --> 02:17:23.200]  пятна с некоторыми объектами
[02:17:23.200 --> 02:17:25.200]  вот, ну
[02:17:25.200 --> 02:17:27.200]  так, а теперь я
[02:17:27.200 --> 02:17:29.200]  переключить снимать, а, вот
[02:17:29.200 --> 02:17:31.200]  ну, собственно, давайте покажу вам результат, вот
[02:17:31.200 --> 02:17:33.200]  ну
[02:17:35.200 --> 02:17:37.200]  здесь, вот, на левом рисунке
[02:17:37.200 --> 02:17:39.200]  оригинально уже предобработана
[02:17:39.200 --> 02:17:41.200]  картинка, чтобы было видно, по серединке
[02:17:41.200 --> 02:17:43.200]  это наш некоторый промышленный
[02:17:43.200 --> 02:17:45.200]  прототип, который без
[02:17:45.200 --> 02:17:47.200]  генератора, третий уже с генератором
[02:17:47.200 --> 02:17:49.200]  вот, видно, что генератор
[02:17:49.200 --> 02:17:51.200]  ну, в общем
[02:17:51.200 --> 02:17:53.200]  на наш взгляд, конечно, оценить
[02:17:53.200 --> 02:17:55.200]  численно, к сожалению, результаты
[02:17:55.200 --> 02:17:57.200]  мы не смогли, потому что у нас очень
[02:17:57.200 --> 02:17:59.200]  малая
[02:17:59.200 --> 02:18:01.200]  маленькая эволюционная выборка
[02:18:01.200 --> 02:18:03.200]  то есть мы проверяли результаты
[02:18:03.200 --> 02:18:05.200]  на эволюционную выборку, которую нам дал
[02:18:05.200 --> 02:18:07.200]  заказчик, там около 30
[02:18:07.200 --> 02:18:09.200]  или 40 картинок, но
[02:18:09.200 --> 02:18:11.200]  эти картинки в большом количестве
[02:18:11.200 --> 02:18:13.200]  изобилуют всякими подозрительными
[02:18:13.200 --> 02:18:15.200]  объектами, вот здесь, вот, видите
[02:18:15.200 --> 02:18:17.200]  женщина нарисована, да
[02:18:17.200 --> 02:18:19.200]  на теле которой
[02:18:19.200 --> 02:18:21.200]  огромное количество всяких
[02:18:21.200 --> 02:18:23.200]  непонятных вещей, там
[02:18:23.200 --> 02:18:25.200]  и какие-то колбы, пистолет
[02:18:25.200 --> 02:18:27.200]  на запястье, какой-то
[02:18:27.200 --> 02:18:29.200]  нож или пуля в ноге, в общем
[02:18:29.200 --> 02:18:31.200]  что попало, вот
[02:18:31.200 --> 02:18:33.200]  ну,
[02:18:33.200 --> 02:18:35.200]  видно, что
[02:18:35.200 --> 02:18:37.200]  на крайнем правом
[02:18:37.200 --> 02:18:39.200]  хотя большинство объектов
[02:18:39.200 --> 02:18:41.200]  в том числе проволока на ноге
[02:18:41.200 --> 02:18:43.200]  там, вот, на правой ноге, проволока
[02:18:43.200 --> 02:18:45.200]  это тоже очень важный объект, который
[02:18:45.200 --> 02:18:47.200]  у нас просили размечать, вот, она
[02:18:47.200 --> 02:18:49.200]  тоже была замечена
[02:18:49.200 --> 02:18:51.200]  размечена, вот, но при этом
[02:18:51.200 --> 02:18:53.200]  на животе
[02:18:53.200 --> 02:18:55.200]  и на груди
[02:18:55.200 --> 02:18:57.200]  объекты прям
[02:18:57.200 --> 02:18:59.200]  выделены красным, то есть
[02:18:59.200 --> 02:19:01.200]  это ложные срабатывания, во многих
[02:19:01.200 --> 02:19:03.200]  случаях, вообще, область груди
[02:19:03.200 --> 02:19:05.200]  и живота самая тяжелая, потому что там
[02:19:05.200 --> 02:19:07.200]  там очень много всего
[02:19:07.200 --> 02:19:09.200]  много внутренних органов, и они всегда
[02:19:09.200 --> 02:19:11.200]  разные, по-разному расположены
[02:19:11.200 --> 02:19:13.200]  и часто трудно увидеть
[02:19:13.200 --> 02:19:15.200]  преди них что-то полутоново
[02:19:15.200 --> 02:19:17.200]  вот так
[02:19:17.200 --> 02:19:19.200]  но это другая
[02:19:19.200 --> 02:19:21.200]  другой пример
[02:19:21.200 --> 02:19:23.200]  давайте я сразу
[02:19:23.200 --> 02:19:25.200]  перейду вот к этим результатам
[02:19:25.200 --> 02:19:27.200]  это
[02:19:27.200 --> 02:19:29.200]  там время 5 минут
[02:19:29.200 --> 02:19:31.200]  ну ладно, значит
[02:19:31.200 --> 02:19:33.200]  как
[02:19:33.200 --> 02:19:35.200]  сказал наш этот
[02:19:39.200 --> 02:19:41.200]  как обратная связь
[02:19:41.200 --> 02:19:43.200]  от нашего заказчика, вот слева
[02:19:43.200 --> 02:19:45.200]  это то, как они видят, то есть как это должно
[02:19:45.200 --> 02:19:47.200]  было быть, вот, у человека
[02:19:47.200 --> 02:19:49.200]  должна была распознаться бутылка
[02:19:49.200 --> 02:19:51.200]  на поясе
[02:19:51.200 --> 02:19:53.200]  телефон в кармане
[02:19:53.200 --> 02:19:55.200]  там еще какие-то
[02:19:55.200 --> 02:19:57.200]  небольшие вещи на груди
[02:19:57.200 --> 02:19:59.200]  вот, справа
[02:19:59.200 --> 02:20:01.200]  это то, что распознала наша нейросеть
[02:20:01.200 --> 02:20:03.200]  видно, что бутылку она даже не увидела
[02:20:03.200 --> 02:20:05.200]  она ее не заметила, и мы тоже ее
[02:20:05.200 --> 02:20:07.200]  не заметили, потому что справа
[02:20:07.200 --> 02:20:09.200]  это предобработка изображения
[02:20:09.200 --> 02:20:11.200]  как мы ее сделали, и мы ее
[02:20:11.200 --> 02:20:13.200]  сделали чересчур агрессивной, то есть она
[02:20:13.200 --> 02:20:15.200]  настолько
[02:20:17.200 --> 02:20:19.200]  подняла яркость изображения
[02:20:19.200 --> 02:20:21.200]  что даже
[02:20:21.200 --> 02:20:23.200]  вот такие пластиковые вещи
[02:20:23.200 --> 02:20:25.200]  как бутылка
[02:20:25.200 --> 02:20:27.200]  они просто пропали
[02:20:27.200 --> 02:20:29.200]  вот, этот косяк обнаружился
[02:20:29.200 --> 02:20:31.200]  очень поздно, на самом деле, через полгода
[02:20:31.200 --> 02:20:33.200]  начала работы, то есть только потом
[02:20:33.200 --> 02:20:35.200]  мы поняли, что нужно
[02:20:35.200 --> 02:20:37.200]  эту агрессивную предобработку
[02:20:37.200 --> 02:20:39.200]  улучшать
[02:20:39.200 --> 02:20:41.200]  делать ее менее агрессивной
[02:20:41.200 --> 02:20:43.200]  то есть так, чтобы
[02:20:43.200 --> 02:20:45.200]  были видны элементы одежды
[02:20:45.200 --> 02:20:47.200]  были видны элементы вот такие
[02:20:47.200 --> 02:20:49.200]  полупрозрачные, и заново
[02:20:49.200 --> 02:20:51.200]  переразмечать картинки
[02:20:51.200 --> 02:20:53.200]  потому что, если мы на картинках
[02:20:53.200 --> 02:20:55.200]  их не выделяли, то значит
[02:20:55.200 --> 02:20:57.200]  петь вообще этому не научится
[02:20:57.200 --> 02:20:59.200]  вот, и пришлось это все переделывать
[02:20:59.200 --> 02:21:01.200]  ну, то есть здесь
[02:21:01.200 --> 02:21:03.200]  к сожалению
[02:21:05.200 --> 02:21:07.200]  а, нет, на самом деле
[02:21:07.200 --> 02:21:09.200]  ну да, да
[02:21:09.200 --> 02:21:11.200]  вот, но проблема оказалась
[02:21:11.200 --> 02:21:13.200]  в том, что даже после как мы это переделали
[02:21:13.200 --> 02:21:15.200]  ничего принципиально не поменялось
[02:21:15.200 --> 02:21:17.200]  потому что не было таких изображений
[02:21:17.200 --> 02:21:19.200]  где бы были бутылки на человеке
[02:21:19.200 --> 02:21:21.200]  или еще какие-то, то есть таких изображений
[02:21:21.200 --> 02:21:23.200]  было буквально единицей из этих 1600
[02:21:23.200 --> 02:21:25.200]  снимков
[02:21:25.200 --> 02:21:27.200]  вот, и поэтому проблема по-прежнему
[02:21:27.200 --> 02:21:29.200]  остается, проблема с генератором
[02:21:29.200 --> 02:21:31.200]  его нужно улучшать
[02:21:31.200 --> 02:21:33.200]  вот, ну давайте я покажу еще
[02:21:33.200 --> 02:21:35.200]  может, несколько примеров того
[02:21:35.200 --> 02:21:37.200]  что, что сеть видит, что не видит
[02:21:37.200 --> 02:21:39.200]  так, эту женщину я уже показывал
[02:21:39.200 --> 02:21:41.200]  с представляемой записью
[02:21:41.200 --> 02:21:43.200]  здесь
[02:21:47.200 --> 02:21:49.200]  так
[02:21:49.200 --> 02:21:51.200]  ну
[02:21:51.200 --> 02:21:53.200]  в общем-то, ладно
[02:21:53.200 --> 02:21:55.200]  я
[02:21:55.200 --> 02:21:57.200]  если мне осталось еще парочка
[02:21:57.200 --> 02:21:59.200]  минут, хотел рассказать про другие
[02:21:59.200 --> 02:22:01.200]  методы, которые мы не использовали
[02:22:01.200 --> 02:22:03.200]  значит, как я сказал
[02:22:03.200 --> 02:22:05.200]  когда мы увидели эту задачу, мы подумали
[02:22:05.200 --> 02:22:07.200]  что это задача сегментации
[02:22:07.200 --> 02:22:09.200]  и решали ее как задача
[02:22:09.200 --> 02:22:11.200]  сегментации, то есть сводили ее
[02:22:11.200 --> 02:22:13.200]  к задаче обучения с учителем
[02:22:13.200 --> 02:22:15.200]  и сделали обучающую выборку
[02:22:15.200 --> 02:22:17.200]  вручную размеченную
[02:22:17.200 --> 02:22:19.200]  вот, но
[02:22:19.200 --> 02:22:21.200]  спустя уже там буквально
[02:22:21.200 --> 02:22:23.200]  ну, фактически целый год работы
[02:22:23.200 --> 02:22:25.200]  мы заметили, что есть работы
[02:22:25.200 --> 02:22:27.200]  которые отличаются
[02:22:27.200 --> 02:22:29.200]  принципиально по постановке задачи
[02:22:29.200 --> 02:22:31.200]  и, возможно
[02:22:31.200 --> 02:22:33.200]  имеет смысл рассмотреть задачи не как задачи
[02:22:33.200 --> 02:22:35.200]  сегментации, а как задачи
[02:22:35.200 --> 02:22:37.200]  детекции, выделения аномалий
[02:22:37.200 --> 02:22:39.200]  аномалия детекция
[02:22:39.200 --> 02:22:41.200]  это другая область
[02:22:41.200 --> 02:22:43.200]  не только компьютерного зрения
[02:22:43.200 --> 02:22:45.200]  то есть, что такое детекция аномалий
[02:22:45.200 --> 02:22:47.200]  это когда у вас есть какие-то данные
[02:22:47.200 --> 02:22:49.200]  которые в большинстве случаев
[02:22:49.200 --> 02:22:51.200]  в 99% случаев
[02:22:51.200 --> 02:22:53.200]  они нормальные
[02:22:53.200 --> 02:22:55.200]  но
[02:22:55.200 --> 02:22:57.200]  бывают какие-то
[02:22:57.200 --> 02:22:59.200]  выбросы, случайности
[02:22:59.200 --> 02:23:01.200]  какие-то шорохи
[02:23:01.200 --> 02:23:03.200]  какие-то, в общем
[02:23:03.200 --> 02:23:05.200]  вещи аномальные
[02:23:05.200 --> 02:23:07.200]  вот, и это совершенно другая область
[02:23:07.200 --> 02:23:09.200]  здесь нельзя
[02:23:09.200 --> 02:23:11.200]  ставить задачу
[02:23:11.200 --> 02:23:13.200]  как задачу обучения с учительным, потому что
[02:23:13.200 --> 02:23:15.200]  обучающих примеров положительных
[02:23:15.200 --> 02:23:17.200]  почти нет
[02:23:17.200 --> 02:23:19.200]  то есть, 1% 100
[02:23:19.200 --> 02:23:21.200]  вот, и для аномалий детекции
[02:23:21.200 --> 02:23:23.200]  мне очень понравились пары работ
[02:23:23.200 --> 02:23:25.200]  я их просто здесь приведу
[02:23:25.200 --> 02:23:27.200]  это не наши работы, а мы хотели бы их
[02:23:27.200 --> 02:23:29.200]  попробовать в будущем
[02:23:29.200 --> 02:23:31.200]  вот, это первая патч SVD
[02:23:31.200 --> 02:23:33.200]  SVD
[02:23:33.200 --> 02:23:35.200]  это Singular Review
[02:23:35.200 --> 02:23:37.200]  чего-то там
[02:23:37.200 --> 02:23:39.200]  Decomposition
[02:23:39.200 --> 02:23:41.200]  вот, и
[02:23:41.200 --> 02:23:43.200]  они говорят, что их алгоритм
[02:23:43.200 --> 02:23:45.200]  прекрасно работает
[02:23:45.200 --> 02:23:47.200]  выделяя всевозможные аномалии
[02:23:47.200 --> 02:23:49.200]  на изображениях
[02:23:49.200 --> 02:23:51.200]  вот, есть такая база данных
[02:23:51.200 --> 02:23:53.200]  MBTech
[02:23:53.200 --> 02:23:55.200]  AD
[02:23:55.200 --> 02:23:57.200]  она публично доступна
[02:23:57.200 --> 02:23:59.200]  там вот, изображения из разных областей
[02:23:59.200 --> 02:24:01.200]  зубные щетки
[02:24:01.200 --> 02:24:03.200]  электроники
[02:24:03.200 --> 02:24:05.200]  фотографии
[02:24:05.200 --> 02:24:07.200]  микросхем
[02:24:07.200 --> 02:24:09.200]  ткани, еще что-то
[02:24:09.200 --> 02:24:11.200]  и везде есть какие-то примеры
[02:24:11.200 --> 02:24:13.200]  аномалий, например, в электронике
[02:24:13.200 --> 02:24:15.200]  это когда ножка распаяна
[02:24:15.200 --> 02:24:17.200]  или в ткани
[02:24:17.200 --> 02:24:19.200]  когда есть какое-нибудь изменение
[02:24:19.200 --> 02:24:21.200]  в рисунке
[02:24:21.200 --> 02:24:23.200]  они говорят, что у них прекрасно
[02:24:23.200 --> 02:24:25.200]  она работает, при том, что учится
[02:24:25.200 --> 02:24:27.200]  совершенно без учителя
[02:24:27.200 --> 02:24:29.200]  просто на правильных примерах
[02:24:29.200 --> 02:24:31.200]  то есть, принцип работы
[02:24:31.200 --> 02:24:33.200]  этих алгоритмов
[02:24:33.200 --> 02:24:35.200]  если на пальцах
[02:24:35.200 --> 02:24:37.200]  он совсем другой
[02:24:37.200 --> 02:24:39.200]  он состоит в том, что вы создаете
[02:24:39.200 --> 02:24:41.200]  некоторую библиотеку правильных примеров
[02:24:41.200 --> 02:24:43.200]  то есть, правильных
[02:24:43.200 --> 02:24:45.200]  без аномалий
[02:24:45.200 --> 02:24:47.200]  и эта библиотека
[02:24:47.200 --> 02:24:49.200]  строится не в исходном пространстве
[02:24:49.200 --> 02:24:51.200]  картинок, а в некотором другом
[02:24:51.200 --> 02:24:53.200]  пространстве, например
[02:24:53.200 --> 02:24:55.200]  в пространстве
[02:24:55.200 --> 02:24:57.200]  репрезентации нейросети
[02:24:57.200 --> 02:24:59.200]  и в этом
[02:24:59.200 --> 02:25:01.200]  латентном пространстве
[02:25:01.200 --> 02:25:03.200]  вы создаете большую
[02:25:03.200 --> 02:25:05.200]  выборку
[02:25:05.200 --> 02:25:07.200]  хороших примеров
[02:25:07.200 --> 02:25:09.200]  а потом сравниваете ваши примеры
[02:25:09.200 --> 02:25:11.200]  допустим, какие-нибудь
[02:25:11.200 --> 02:25:13.200]  аномальные
[02:25:13.200 --> 02:25:15.200]  если в этом латентном пространстве
[02:25:15.200 --> 02:25:17.200]  ваши вектора кажутся далеко
[02:25:17.200 --> 02:25:19.200]  от тех объектов
[02:25:19.200 --> 02:25:21.200]  которые в вашей библиотеке
[02:25:21.200 --> 02:25:23.200]  значит, можно
[02:25:23.200 --> 02:25:25.200]  поставить вопрос
[02:25:25.200 --> 02:25:27.200]  с вероятностью того
[02:25:27.200 --> 02:25:29.200]  что это аномальная вещь
[02:25:29.200 --> 02:25:31.200]  и даже целые конкурсы
[02:25:31.200 --> 02:25:33.200]  проводятся
[02:25:33.200 --> 02:25:35.200]  есть сайт PapersWizCode, очень популярный
[02:25:35.200 --> 02:25:37.200]  я думаю, многие знают о нем
[02:25:37.200 --> 02:25:39.200]  и здесь
[02:25:39.200 --> 02:25:41.200]  люди соревнуются
[02:25:41.200 --> 02:25:43.200]  в задачах детекции аномалий
[02:25:43.200 --> 02:25:45.200]  на различных датастетах
[02:25:45.200 --> 02:25:47.200]  шанхай, тех датастет
[02:25:47.200 --> 02:25:49.200]  где нужно в толпе людей разглядеть
[02:25:49.200 --> 02:25:51.200]  какие-то необычные вещи
[02:25:51.200 --> 02:25:53.200]  например, кто-то там на машине заехал
[02:25:53.200 --> 02:25:55.200]  другой датастет
[02:25:55.200 --> 02:25:57.200]  как раз mbt-kd
[02:25:57.200 --> 02:25:59.200]  в общем
[02:25:59.200 --> 02:26:01.200]  вот patch-core
[02:26:01.200 --> 02:26:03.200]  еще один алгоритм
[02:26:03.200 --> 02:26:05.200]  который занял первое место
[02:26:05.200 --> 02:26:07.200]  он точно так же построен
[02:26:07.200 --> 02:26:09.200]  как patch-svdd
[02:26:09.200 --> 02:26:11.200]  очень интересные алгоритмы
[02:26:11.200 --> 02:26:13.200]  хочется попробовать их в будущем
[02:26:13.200 --> 02:26:15.200]  может быть, удастся применить задачи
[02:26:15.200 --> 02:26:17.200]  фегментации примгеновских нюх
[02:26:17.200 --> 02:26:19.200]  все, спасибо за внимание
[02:26:19.200 --> 02:26:21.200]  я на этом закончу
[02:26:23.200 --> 02:26:25.200]  да
[02:26:39.200 --> 02:26:41.200]  ну да, это конечно был наш недосмотр
[02:26:45.200 --> 02:26:47.200]  мы смотрели разные фильтры
[02:26:47.200 --> 02:26:49.200]  фильтры, связанные
[02:26:49.200 --> 02:26:51.200]  с выравниванием гистограмма
[02:26:51.200 --> 02:26:53.200]  если у вас есть
[02:26:53.200 --> 02:26:55.200]  исходная гистограмма
[02:26:55.200 --> 02:26:57.200]  с левой внизу оригинальная
[02:26:57.200 --> 02:26:59.200]  то есть у вас в основном
[02:26:59.200 --> 02:27:01.200]  все пиксели имеют либо
[02:27:01.200 --> 02:27:03.200]  оттенки почти белые
[02:27:03.200 --> 02:27:05.200]  либо почти черные
[02:27:05.200 --> 02:27:07.200]  и мы хотим их чуть-чуть выровнять
[02:27:07.200 --> 02:27:09.200]  более сделать
[02:27:09.200 --> 02:27:11.200]  разнообразие повысить
[02:27:11.200 --> 02:27:13.200]  и начинаем
[02:27:13.200 --> 02:27:15.200]  их выравнивать
[02:27:15.200 --> 02:27:17.200]  с помощью всяких процедур
[02:27:17.200 --> 02:27:19.200]  порогового отвлечения
[02:27:19.200 --> 02:27:21.200]  мы просто делаем белым
[02:27:21.200 --> 02:27:23.200]  и так
[02:27:23.200 --> 02:27:25.200]  постепенно-постепенно
[02:27:25.200 --> 02:27:27.200]  гистограмма действительно выравнивается
[02:27:27.200 --> 02:27:29.200]  она становится более разнообразной
[02:27:29.200 --> 02:27:31.200]  ну и это тоже
[02:27:31.200 --> 02:27:33.200]  Crash Trunk
[02:27:33.200 --> 02:27:35.200]  это как раз обрезание
[02:27:35.200 --> 02:27:37.200]  по порогу
[02:27:37.200 --> 02:27:39.200]  Equalize HIST
[02:27:39.200 --> 02:27:41.200]  это эквализация гистограмма
[02:27:41.200 --> 02:27:43.200]  это библиотечная функция из OpenCV
[02:27:43.200 --> 02:27:45.200]  очень классно работает
[02:27:45.200 --> 02:27:47.200]  особенно Equalize Adapt HIST
[02:27:47.200 --> 02:27:49.200]  когда ваша картинка бьется на маленькие сектора
[02:27:49.200 --> 02:27:51.200]  8 на 8 скажем
[02:27:51.200 --> 02:27:53.200]  и в них делает
[02:27:53.200 --> 02:27:55.200]  эквализацию гистограммы и тогда
[02:27:55.200 --> 02:27:57.200]  адаптивность получается в разных местах
[02:27:57.200 --> 02:27:59.200]  они будут по-разному, но в итоге картинка становится
[02:27:59.200 --> 02:28:01.200]  совершенно другой, она выглядит потрясающе
[02:28:01.200 --> 02:28:03.200]  Equalize Adapt HIST
[02:28:03.200 --> 02:28:05.200]  он же известный как КЛАХЕ
[02:28:05.200 --> 02:28:07.200]  вот этот алгоритм
[02:28:07.200 --> 02:28:09.200]  в общем
[02:28:09.200 --> 02:28:11.200]  с помощью них мы делали
[02:28:11.200 --> 02:28:13.200]  но чуть-чуть
[02:28:13.200 --> 02:28:15.200]  с параметрами мы перестарались
[02:28:15.200 --> 02:28:17.200]  реально много
[02:28:17.200 --> 02:28:19.200]  да
[02:28:19.200 --> 02:28:21.200]  больше вопросов
[02:28:23.200 --> 02:28:25.200]  она точно слышит
[02:28:25.200 --> 02:28:27.200]  в зуме
[02:28:29.200 --> 02:28:31.200]  они не пишут там ничего
[02:28:31.200 --> 02:28:33.200]  в писем нет
[02:28:33.200 --> 02:28:35.200]  тогда спасибо большое
[02:28:35.200 --> 02:28:37.200]  за вопросы мне
[02:28:37.200 --> 02:28:39.200]  спасибо
[02:28:39.200 --> 02:28:41.200]  я предъявляю следующую водоправку
[02:28:41.200 --> 02:28:43.200]  для вас
[02:28:49.200 --> 02:28:51.200]  Привет всем
[02:28:51.200 --> 02:28:53.200]  меня зовут Мария Пириган
[02:28:53.200 --> 02:29:03.200]  я так жаль расти
[02:29:03.200 --> 02:29:05.200]  я очень признательна
[02:29:05.200 --> 02:29:07.200]  организаторам конференции
[02:29:07.200 --> 02:29:09.200]  за возможность поздравить Бориса Григорьевича
[02:29:09.200 --> 02:29:14.040]  его наше восхищение и благодарность за все, что он сделал в науке Аддант. И, конечно,
[02:29:14.040 --> 02:29:20.560]  сделает еще. И во всей вот полифонии работ юбиляра для нас особенно важным, конечно, оказался его
[02:29:20.560 --> 02:29:27.200]  междисциплинарный подход, который, как удивление, Борис Григорьевич сформулировал еще в те времена,
[02:29:27.200 --> 02:29:33.160]  когда, я думаю, слово «междисциплинарный» было чем-то неприличным. И вообще, наверное, очень
[02:29:33.160 --> 02:29:38.280]  интересно, как реагировали коллеги на эти работы, когда в ранних работах еще у Бориса Григорьевича
[02:29:38.280 --> 02:29:46.040]  буквально интегрированы даже четыре, не менее четырех научных парадигм. Конечно, большое значение
[02:29:46.040 --> 02:29:52.600]  для нас, наверное, это работы по кластеризации, ну и особенно важно, конечно, ваши работы по
[02:29:52.600 --> 02:29:59.200]  интерпретации ваших объемных статологических данных, которые дали нам огромное количество идей. И вот мы
[02:29:59.200 --> 02:30:07.680]  продолжаем разучиться, пытаемся тоже строить модели такого анализа. Мы пытаемся интерпретировать
[02:30:08.400 --> 02:30:17.640]  данные социальных медиа. Конечно, ландшафт социальных медиа в мировом контексте он достаточно
[02:30:17.640 --> 02:30:24.400]  стабилен, в то время как, конечно, цифровая жизнь русскоязычных авторов, особенно в последний
[02:30:24.400 --> 02:30:30.080]  год, вытерпело колоссальные изменения, но в количественном отношении практически ничего не
[02:30:30.080 --> 02:30:39.080]  изменилось. Были только спады 25 февраля и перед объединением мобилизации, что естественно. А в принципе,
[02:30:39.080 --> 02:30:44.400]  цифровые показатели, они как бы такие достаточно стабильные. Поменялось из-за количественных данных
[02:30:44.400 --> 02:30:50.880]  стабилий, то качественная жизнь, конечно, изменилась кардинально. И, собственно, русскоязычные
[02:30:50.880 --> 02:30:57.720]  акторы так и сделали цифровую релокацию, и бенфитаром ее, конечно, стал телеграмм. По сути дела,
[02:30:57.840 --> 02:31:05.720]  в нашем современном русскоязычном медиа пространстве мы имеем только то, что делал нам Павел
[02:31:05.720 --> 02:31:13.960]  Олегович Дуров. В контакте телеграмм наша цифровая земля. Одноклассники тоже пытаются сейчас как
[02:31:13.960 --> 02:31:19.200]  очень активно использовать ситуацию, делают ребрендил, делают какие-то очень новые модели,
[02:31:19.200 --> 02:31:23.800]  но в принципе у них есть какой-то минимальный природ, но, тем не менее, все-таки это платформа
[02:31:23.800 --> 02:31:33.480]  находится на периферии цифровой жизни. Цели исследования, они определены заказчиком,
[02:31:33.480 --> 02:31:40.080]  который Струкоплекс Москвы, на основании которого делали где-то более 30 проектов,
[02:31:40.080 --> 02:31:46.280]  что позволило нам говорить о том, что мы создали какую-то модель. Задача заключалась в том, что,
[02:31:46.280 --> 02:31:53.760]  особенно еще в старые добрые времена, еще не такие давние, когда в Москве было более 200,
[02:31:53.760 --> 02:31:59.800]  скажем одновременно, строительных площадок, то для Струкоплекса было очень важно представить
[02:31:59.800 --> 02:32:06.560]  себе реакции москвичей. То есть, во-первых, где, конечно, очень важна предиктивная аналитика,
[02:32:06.560 --> 02:32:12.120]  то есть где могут возникнуть конфликты и при наличии уже существующей социальной
[02:32:12.120 --> 02:32:17.400]  напряженности, выявить их причины непосредственно и дать рекомендации, как их быстро нивелировать.
[02:32:17.400 --> 02:32:24.920]  Мы эти задачи, мы, собственно, на протяжении несколько лет, когда мы этим занимались,
[02:32:24.920 --> 02:32:31.440]  все время меняются наши модели. На сегодняшний день у нас наибольшая эффективность показала некий
[02:32:31.440 --> 02:32:37.120]  такой алгоритм, который совмещает три вот такие сферы. Неросетевой семантический анализ,
[02:32:37.120 --> 02:32:47.320]  который мы делаем вместе с Самсаном Харламовым, руководитель департамента интеллектуальной систем,
[02:32:47.320 --> 02:32:55.520]  математические модели делает Нелли, и статологические процедуры, которые подтверждают или нет результаты,
[02:32:55.520 --> 02:33:05.040]  проводят наши коллеги Сан Санчих, Асень Калаевич Расхотников, статологическое агентство Столица и
[02:33:05.040 --> 02:33:13.120]  центровой банестики город при правительстве Москвы. Собственно, методы используются самые разнообразные,
[02:33:13.120 --> 02:33:20.160]  полифоничные, они, в принципе, меняются, но в целом, скажем, и вот эта вот модель алгоритма,
[02:33:20.160 --> 02:33:26.120]  она тоже постоянно изменяется, потому что, во-первых, все время каждым разом понимаешь,
[02:33:26.120 --> 02:33:32.920]  что нужно исправить, во-вторых, эта среда столь неустойчива и столь быстро меняется,
[02:33:32.920 --> 02:33:41.320]  что требует постоянно вносить в нее какие-то изменения. Мне кажется, на сегодняшний день,
[02:33:41.320 --> 02:33:46.680]  что меня, например, беспокоит больше всего и является большой проблемой, это определение
[02:33:46.680 --> 02:33:53.120]  искусственных сущностей. Еще, скажем, три года назад было очевидно, что для анализа подобных
[02:33:53.120 --> 02:33:58.960]  вещей, когда мы говорим о приятии каких-то анализатологических проблем, что, конечно,
[02:33:58.960 --> 02:34:03.160]  очень важно всех искусственных сущностей сначала отделить, да их выбросить, поскольку они нам
[02:34:03.160 --> 02:34:09.240]  как бы мешают определить это восприятие живых людей. Но что мы видим сегодня? Сущенцы полной
[02:34:09.240 --> 02:34:15.200]  растерянности. Во-первых, стоит ли вообще их выбрасывать, потому что, ну, конечно, после уже
[02:34:15.200 --> 02:34:21.360]  GPT-3 появилась какая-то такая растерянность, но после того, как в ноябре был представлен
[02:34:21.360 --> 02:34:28.280]  чат GPT, и мы видим, что на нем можно делать. А завтра уже буквально нам, кажется, грозят
[02:34:28.280 --> 02:34:37.160]  представить GPT-4, который, если GPT-3 было 175 миллиардов параметров, то GPT-4 обещает, что ли,
[02:34:37.160 --> 02:34:43.440]  100 триллионов больше. То есть, очевидно, что эти искусственные сущности, то есть мы с ними,
[02:34:43.440 --> 02:34:50.360]  даже если сейчас уже, в собственном случае, чат GPT, конечно, превосходит биологических актов,
[02:34:50.360 --> 02:34:56.440]  то есть нас с вами. То есть, чат GPT-4, то есть от GPT-4, очевидно, что эти искусственные сущности,
[02:34:56.440 --> 02:35:02.280]  то есть мы с ними даже соревноваться не сможем. Более того, вот уже анализ показывает, что они
[02:35:02.280 --> 02:35:09.040]  во многом определяют качество и движение функционных потоков. И вот что с этим делать, мне кажется,
[02:35:09.040 --> 02:35:14.800]  что сейчас вот это такая очень актуальная проблема, но это, я думаю, что это вот уже сейчас и сегодня,
[02:35:15.280 --> 02:35:23.560]  я думаю, что нужно этим будет заниматься. Но вот в нашей модели, мне кажется, что самым таким
[02:35:23.560 --> 02:35:32.360]  интересным это возможность выделения семантических акцентов, которые являются семантических акцентов,
[02:35:32.360 --> 02:35:39.120]  причем которые выражены не эксплицитно, а имплицитно. Поскольку коммуникация и вообще наше
[02:35:39.120 --> 02:35:44.720]  восприятие, оно, конечно, во многом определяется вот этими имплицитными сферами. И один из,
[02:35:44.720 --> 02:35:49.920]  вот, помимо семантического ядра и вот этих семантических моделей, это позволяет сделать
[02:35:49.920 --> 02:35:56.160]  нам ассоциативный поиск и анализ ассоциативных сетей. То есть, на это практика вообще ассоциативные
[02:35:56.160 --> 02:36:03.200]  эксперименты, это такая достаточно распространенная практика психолингвистики и психологии,
[02:36:03.200 --> 02:36:09.680]  которые имеют целый ряд недостатков и совершенно справедливых. Но вот на материале больших данных
[02:36:09.680 --> 02:36:16.240]  с помощью нейросетей, вот это выделение ассоциации, конечно, ну, по крайней мере, вроде бы дает
[02:36:16.240 --> 02:36:22.640]  такие возможности интерпретировать имплицитные по тексту значения, которые во многом и определяют
[02:36:22.640 --> 02:36:29.200]  наши реакции, наши отношения, потому что наши ассоциативные связи мы не можем контролировать и
[02:36:29.200 --> 02:36:37.200]  не можем их поделать, как все остальное. Да, и вот, собственно, поскольку, ну, и заказчики,
[02:36:37.200 --> 02:36:42.560]  собственно, да и мы с вами, конечно, любим какой-то, помимо вот этих больших моделей, какое-то что-то
[02:36:42.560 --> 02:36:49.200]  такое, какие-то цифры, которые нам могут сразу показать, что вот это результат каких-то исследований.
[02:36:49.200 --> 02:36:55.200]  Мы вот придумали поэтому индекс социального, стресса социального благополучия. Ну, у нас,
[02:36:55.200 --> 02:37:00.320]  собственно, я не буду нас наостанавливать, это у нас отдельно описано в работе, но
[02:37:00.320 --> 02:37:12.600]  на его последней, последней работе у нас эта методика достаточно хорошо описана. И, в принципе,
[02:37:12.600 --> 02:37:18.280]  он в общем работает, по крайней мере, показательно. Шкала вот социального, шкала их, которая одинаковая,
[02:37:18.280 --> 02:37:26.280]  ну, в принципе, тоже получается, что приходится двигать по-разному. И вот, собственно, наши коллеги
[02:37:26.280 --> 02:37:31.840]  делают психологические исследования достаточно традиционно, но вот фокус-группы, они, конечно,
[02:37:31.840 --> 02:37:40.200]  базируются на тех результатах, которые дают нам тематическая сеть и, собственно, позволяют
[02:37:40.200 --> 02:37:45.200]  делать более, таким, корректными, проводить саму методику фокус-групп. И, что интересно,
[02:37:45.200 --> 02:37:53.200]  мы уже буквально недавно попробовали результаты фокус-групп тоже проанализировать по этой же
[02:37:53.200 --> 02:37:57.640]  нерестевой модели. То есть, то, что никогда не делают. Обычно социологи проводят фокус-группы,
[02:37:57.640 --> 02:38:02.680]  как их там, что-то такое количество описывают. А вот, собственно, нерестерка позволила вытащить,
[02:38:02.680 --> 02:38:08.120]  опять же, семантические нюансы, и вот, показался тоже очень любопытно. И, естественно, что,
[02:38:08.120 --> 02:38:14.040]  когда все три модели показывают нам, ну, подтверждают один тот же результат, мы, в общем-то,
[02:38:14.040 --> 02:38:19.880]  очень довольны и представляем нюанс. И можем говорить о какой-то там болельности результатов.
[02:38:19.880 --> 02:38:24.920]  Сегодня мы хотим представить вам очень кратко результаты одного исследования,
[02:38:24.920 --> 02:38:32.160]  которые посвящены большой кольцевой линии. Ну, просто, конечно, просто потому, что вот у нас
[02:38:32.160 --> 02:38:37.680]  последние данные у этой БКЛ есть, на самом деле, ничего мне там интересного нет. Ну, помимо того,
[02:38:37.680 --> 02:38:43.480]  что в результатах исследования нет, там никаких страшных конфликтов нет. БКЛ, собственно,
[02:38:43.480 --> 02:38:49.560]  она только сейчас запускается, несмотря ни на что. Это один из самых, ну, практически,
[02:38:49.560 --> 02:38:56.080]  так, не, почему один, это самый, самый объемный проект, вот, про комплексы Москвы, поскольку,
[02:38:56.080 --> 02:39:02.280]  видите, БКЛ, она такая, вот, бирюзовая. Она 96 километров, говорит мне, где вообще нет
[02:39:02.280 --> 02:39:10.680]  какой длинной ветки, 31 станции метро на ней будет, 19 переходов на другие, линии метро там, 4
[02:39:10.680 --> 02:39:18.120]  перехода на МЦК, там сколько-то, 6 станций. В общем, это очень объемный проект. Мы делали,
[02:39:18.120 --> 02:39:25.800]  он, собственно, БКЛ строится с 2011 года, первая ветка запущена была в 2018 году, и мы делали
[02:39:25.800 --> 02:39:32.760]  несколько срезов. Ну, собственно, берем данные, естественно, все релевантные данные, используя
[02:39:32.760 --> 02:39:37.880]  эту нерестивую технологию Танцваныча, Тораллайт провидационный сетевой анализ,
[02:39:37.880 --> 02:39:43.800]  Аптомэк контент анализ, хотя я, вот, искренне не понимаю, зачем он нужен, но заказчики очень
[02:39:43.800 --> 02:39:50.240]  любят. И, ну, табло, это просто для визуальной аналитики и рисования красивых картинок. Вот,
[02:39:50.240 --> 02:39:55.400]  делали, повторяем, очень много замеров, потому что большой проект, и, естественно, что он, как
[02:39:55.400 --> 02:40:03.320]  бы, очень важен. Ну, вот сейчас мы хотим показать два датасета, просто последних. Это вот 2021 год
[02:40:03.320 --> 02:40:12.800]  и 22-й практически год, что вообще было удивительно, потому что, ну, понятно, что в 2021 году интерес
[02:40:12.800 --> 02:40:19.760]  к этому проекту был большой, закономерно, да, но что в 2022 году будет кто-то интересоваться и
[02:40:19.760 --> 02:40:24.000]  генерирует контент про БКЛ. Для меня, например, было, я думала, что вообще там не будет его ничего,
[02:40:24.000 --> 02:40:30.240]  тем не менее, посмотрите. Если учесть, что вот первый датасет – это 9 месяцев, а второй – 12,
[02:40:30.240 --> 02:40:35.440]  то есть, ну, практически, ну, то есть, соответственно, по месяцам, да, объем генерируемого контента,
[02:40:35.440 --> 02:40:43.000]  что вообще, конечно, удивительно. Ну, первая, первые характеристики касаются, естественно,
[02:40:43.000 --> 02:40:50.600]  цифровых источников, и вот первый датасет отражает достаточно такую типичную картину для того
[02:40:50.600 --> 02:40:57.400]  времени, еще года назад. Это микроблоки, месседжеры и социальные сети, видео в четвертом месте. Ну, вот,
[02:40:57.400 --> 02:41:05.040]  конечно, тип источников по охвату показывают уже сами, дают, конечно, очень любопытный результат.
[02:41:05.040 --> 02:41:10.960]  Вот это твиттер, о котором выходят лидеры, коллеги, конечно, не говорит о том, что твиттер у нас когда-то
[02:41:10.960 --> 02:41:18.240]  был первый там по популярности цифровой платформы. Никогда этого не было в рукоязычной среде. То есть,
[02:41:18.240 --> 02:41:24.560]  это единственных живых людей там было очень мало, это, как правило, белинтвы, которые живут, скажем,
[02:41:24.560 --> 02:41:29.640]  в англоязычной среде, где твиттер, ну, скажем, для американских, дали британских пользователей,
[02:41:29.640 --> 02:41:35.360]  твиттер это вообще основная среда обитания, и русскоязычные белинтвы, они также переходят
[02:41:35.360 --> 02:41:41.760]  в твиттер. В принципе, вот эта картинка показывает только о том, какие колоссальные бюджеты были
[02:41:41.760 --> 02:41:46.960]  потрачены на информационное спровождение. Вот в твиттере были основаны только вот эти, знаете,
[02:41:46.960 --> 02:41:53.160]  бюджеты вот этих пиарщиков и журналистов, которые, значит, гонялись там туда-сюда. Ну, бюджеты огромные.
[02:41:53.160 --> 02:42:01.760]  Это, собственно, просто называется, по-моему, такая совершенно чудовищная ситуация. Ну, и обратите внимание,
[02:42:01.760 --> 02:42:07.440]  что уже тогда телеграм был на первом месте в контакте YouTube. Очень необычное распределение для
[02:42:07.440 --> 02:42:15.600]  типа в источниках, повторяя, это первый датасет. Ну, цифровые следы, конечно, на них безусловно следует
[02:42:15.600 --> 02:42:21.360]  обращать внимание и невозможно не обращать внимание, но, коллеги, опять же, мы знаем огромное количество
[02:42:21.360 --> 02:42:26.480]  механизмов на сегодняшний день, которые позволяют вот то искутленным образом создавать вот эти
[02:42:26.480 --> 02:42:35.920]  цифровые следы, которые, ну, делают совершенно неактуальную реальную картину цифровой
[02:42:35.920 --> 02:42:40.360]  оценки пользователей. То есть, опять же, большинство из них, то есть, достаточно очень дешево, очень
[02:42:40.360 --> 02:42:47.240]  просто нагнать количество там лайков, дизлайков, прочее. Поэтому этот показатель тоже такой относится
[02:42:47.240 --> 02:42:54.080]  с большим таким осторожностью. Ну, распределение цифровых следов тоже традиционное, конечно,
[02:42:54.080 --> 02:43:01.640]  показывает наибольшую активность в контакте, который у нас понятный, любимый, удобный. Динамика
[02:43:01.640 --> 02:43:09.320]  вот общего числа сообщений, числа уникальных сообщений, количества просмотров и количество
[02:43:09.320 --> 02:43:15.640]  активности акторов по первому датсету дает одну и ту же картину. Практически нейтральные поля
[02:43:15.640 --> 02:43:23.920]  только два всплеска. Они относятся к 26 марту и к 6 апрелю. И, знаете, тоже совершенно чудесная
[02:43:23.920 --> 02:43:29.160]  ситуация. То есть, те времена недалекие, вот эти всплески связаны были с тем, что первый всплеск это
[02:43:29.160 --> 02:43:37.080]  тем же было в активной подштанине и вообще в сетях обсуждений, как назвать новую станцию. То есть,
[02:43:37.080 --> 02:43:41.720]  вот еще год назад москвичей очень сильно волновала, как назвать новую станцию. Это, значит,
[02:43:41.720 --> 02:43:49.800]  эти были вот эти вот связанные всплески. А второй всплеск с открытием народного полчания. Там
[02:43:49.800 --> 02:43:54.960]  с этой станцией просто было связано много проблем. Второй датсет, который отражает, собственно,
[02:43:54.960 --> 02:44:03.640]  вот этот наш 22-й год. Он показывает, конечно, тут опять же социальные сети уже выделяются,
[02:44:03.640 --> 02:44:08.800]  поскольку развитие телеграмма, то здесь уже следует, конечно, разделять каналы и чаты.
[02:44:08.800 --> 02:44:14.360]  Мы соберем первую часть телеграмма. Вот видео и блоги, которые выходят, соответственно,
[02:44:14.360 --> 02:44:20.280]  третье и четвертое место. Среди источников вот это уже ситуация говорит о том, что, конечно,
[02:44:20.280 --> 02:44:26.840]  бюджета на информационных спровождениях уже практически не тратили. Сидеть. Реальная картина
[02:44:26.840 --> 02:44:32.080]  цифрового поведения русскоязычных пользователей в ВКонтакте, Телеграмм и Ютуб. Причем обратите
[02:44:32.080 --> 02:44:38.960]  внимание, что Инстаграм и Фейсбук тоже как бы приветствуют в Комнатзору, вполне себе живы,
[02:44:38.960 --> 02:44:45.800]  хотя, конечно, резко похудели. В данном случае вот эта же динамика упоминаний, динамика активности,
[02:44:45.800 --> 02:44:52.320]  динамика вовлеченности и активности аудитории показывает совершенно другую картину. Конечно,
[02:44:52.320 --> 02:44:58.800]  за этом году пользователи волновали совершенно другие проблемы. Единственное, что вызывало
[02:44:58.800 --> 02:45:04.560]  какую-то резкую реакцию, это сообщение, информационное сообщение о сроках. И вот этот пик,
[02:45:04.560 --> 02:45:09.600]  видите, активности аудитории, связан с тем, что было сообщение мэра о том, что завершили,
[02:45:10.600 --> 02:45:18.720]  и он наконец-то его запустит в ход. Вот мы поедем на распределение, как следовало ожидать,
[02:45:18.720 --> 02:45:26.800]  конечно, мужчины были активны при обсуждении таких проблем. Люди 55 лет и старше, но вот что меня
[02:45:26.800 --> 02:45:32.680]  потрясло, совершенно геолокация акторов, то есть 22-х год. Но вообще надо сказать, что удивительно,
[02:45:32.680 --> 02:45:38.640]  что все московские вот эти проекты, они дают какую-то совершенно безумную геолокацию, что даже там
[02:45:38.640 --> 02:45:43.600]  появляются названия стран, которых я даже и не знала. Но здесь, вы представьте себе,
[02:45:43.600 --> 02:45:48.800]  какой-то отоф Кука, сидит человек на острове Кука и генерирует контакт по БКЛ. При том,
[02:45:48.800 --> 02:45:55.840]  при всем, что, конечно, геолокация в аналитике вызывает много, ну, конечно, есть у них очень
[02:45:55.840 --> 02:46:00.920]  много проблем, и, конечно, с ней нужно относиться понятно, что там не все точно, но, тем не менее,
[02:46:00.920 --> 02:46:07.880]  общая картина вполне себе. Ну и по странам геолокация, конечно, вот очень нравится сочетание
[02:46:07.880 --> 02:46:16.040]  зеленограда нелегко. Ну, это по количеству, да, по количеству активности это проблема. Вот тональность,
[02:46:16.040 --> 02:46:21.440]  и сантимент аналия, безусловно, и квастеризация по тональности, это, конечно, очень важный такой
[02:46:21.440 --> 02:46:27.680]  момент, первичный, который, ну, без которого нельзя обойтись. И вот он дает, ну, как в первом-втором
[02:46:27.680 --> 02:46:41.000]  датасете дает показания больше тональности по нейтральному контенту, и рост динамики
[02:46:41.000 --> 02:46:48.400]  сообщений 2021 году, поскольку минут 21 году стала активная фаза строительства, поэтому, собственно,
[02:46:48.400 --> 02:46:53.720]  вот этот первый датасет с этим связан, когда была активная фаза строительства, и строительство
[02:46:53.720 --> 02:46:58.640]  вышло вот на поверхность, то есть, на исключение, поскольку с этим столкнулись. Но, тем не менее,
[02:46:58.640 --> 02:47:05.000]  все показывает, собственно, видите, преимущество нейтральных сообщений. Ну, вот топ источников с
[02:47:05.000 --> 02:47:10.520]  позитивной тональностью, ВКонтакте, вот его Телеграмм, надо сказать, что это, конечно, в основном
[02:47:10.520 --> 02:47:15.480]  нейтральная и позитивная тональность, это ангажированные медийные ресурсы. И вообще, это
[02:47:15.480 --> 02:47:21.720]  же совершенно потрясающе, каким образом вот наши журналисты и пящики, которые вот какие-то безумные
[02:47:21.720 --> 02:47:28.480]  остаивают бюджеты на это путешествие, они умудряются добиться прямо противоположных целей тому,
[02:47:28.480 --> 02:47:34.600]  за что им платят, на тех целях, которые они ставят. Чем больше позитивного нейтрального контента они
[02:47:34.600 --> 02:47:41.240]  генерируют, тем больше вызывает оторжение у аудитории. И реальная реакция пользователей вот этот вот он
[02:47:41.240 --> 02:47:50.560]  показывает. Ну, второй датасет также показывает преимущество нейтрального контента, да,
[02:47:50.560 --> 02:47:59.240]  но и тональность разных типов соответствует разными аудиториями. Тем не менее, видите, показывают,
[02:47:59.240 --> 02:48:11.240]  что в комментариях негативных сообщений больше. И это уже такой очень важный звонок. Очевидно,
[02:48:11.240 --> 02:48:15.880]  что сообщено, что вот эмоциональная реакция распространялась в большей степени в сообществах.
[02:48:15.880 --> 02:48:22.680]  То есть, как в офлайн-среде, так и в онлайн-среде эмоции. Вирусное распространение эмоций, конечно,
[02:48:22.680 --> 02:48:29.080]  наиболее эффективно в толпе. То есть, толпа в цифровых образах соответствует сообществу.
[02:48:29.080 --> 02:48:41.080]  Но вот это, я уже говорила, что вот очень вот проекты, наши проекты вообще показывают,
[02:48:41.080 --> 02:48:48.800]  что отношение к результатам тональности очень должно быть осторожным. Потому что распределение
[02:48:48.800 --> 02:48:54.680]  по тональности, оно далеко не всегда свидетельствует о реальном положении вещей. Если мы хотим говорить
[02:48:54.680 --> 02:49:01.520]  о каких-то истинных реакциях, о настоящих реакциях пользователей. В то время как агрессия,
[02:49:01.520 --> 02:49:08.360]  анализ агрессии показывает это очень чётко. И даже минимальные показатели агрессии
[02:49:08.360 --> 02:49:14.200]  выявляют истинные отношения, истинные проблемы, которые будут услышать в сюжете.
[02:49:14.200 --> 02:49:23.400]  Собственно, здесь чаты месседжеров соцсети показывают нам тип агрессии. И главное,
[02:49:23.400 --> 02:49:30.160]  здесь что-то на комментарии, которые показывают достаточно большие проблемы. Это солодная таблица.
[02:49:30.160 --> 02:49:33.560]  Видите, я всё это закрасила специально в пастельные тона, потому что так страшно,
[02:49:33.560 --> 02:49:38.280]  что с собой не испугаться. Агрессия, особенно в русскоязычном генеральном контексте,
[02:49:38.280 --> 02:49:47.680]  это всегда просто чудовищные данные. Поскольку у меня уже нет времени, у нас очень много всего.
[02:49:47.680 --> 02:49:54.280]  Давайте, собственно, тональность разных, то есть тональность рата перейдут, собственно, к основному.
[02:49:54.280 --> 02:50:02.080]  Семантические акценты негативные. Посолили выделить прежде всего претензии к проектировщикам,
[02:50:02.080 --> 02:50:08.160]  то есть по первым датесетам и по второму. Это претензии к приеме, потому что не учитывается,
[02:50:08.160 --> 02:50:16.880]  ну, жители считают, что были нарушены грабитарительные закономерности, не учтены их нужды.
[02:50:16.880 --> 02:50:22.400]  Совершенно возмущение против отсутствия диалога с властями и качества проведения публичных
[02:50:22.400 --> 02:50:28.760]  чтений. Публичные чтения – это вообще особый разговор. Очень много критиковали расположение новых
[02:50:28.760 --> 02:50:35.040]  станций, которые оказались бы непродуманных. Собственно, снижение уровня жизни из-за шума.
[02:50:35.040 --> 02:50:41.520]  Жителям обещали страховать их жизнью, что не везде было сделано. Расположение выходов,
[02:50:41.520 --> 02:50:49.520]  перенос зимних переходов, повышенный уровень шума, работы в ночное время, тротуары,
[02:50:49.800 --> 02:50:55.280]  которые переносится под окнами домов, опасность техногенных катастроф, ухудшение транспортной
[02:50:55.280 --> 02:51:02.400]  ситуации, конечно, разрушение зеленой зоны. Перемещение, собственно, главная одна из важных претензий,
[02:51:02.400 --> 02:51:06.920]  то, что, естественно, жители старой Москвы, то, что вот эти новые ветки, которые касаются только
[02:51:06.920 --> 02:51:11.640]  жителей новой Москвы, для них только вредны. Собственно, это еще Лужков, кстати, говорят,
[02:51:11.640 --> 02:51:19.040]  завершал строение литни сознательно только в рамках старой Москвы и сознательно продлевал
[02:51:19.040 --> 02:51:24.720]  их далее. И, естественно, потому что это совершенно равномерно вызывает негативные реакции
[02:51:24.720 --> 02:51:32.560]  в жизни старой Москвы. Ну, усложнение ситуации транспортировок, ну и многое другое. И, конечно,
[02:51:32.560 --> 02:51:41.760]  эти конфликты накладываются на старые конфликты, связанные, например, с строительством линии
[02:51:41.760 --> 02:51:49.440]  с ультиноватором севастопольского протест-проекта, да, проспекта. А, скажем, во втором датасетте они
[02:51:49.440 --> 02:51:55.640]  накладываются на проблемы, связанные с секс-инновацией по-разному. Во втором датасетте тоже было
[02:51:55.640 --> 02:52:00.560]  достаточно много повторящихся, таких же, вот, повторящихся, и, конечно, было намного меньше
[02:52:00.560 --> 02:52:06.040]  этих акцентов, но появились, например, новые. Кстати говоря, вы знаете, удивительно, скажем,
[02:52:06.040 --> 02:52:11.120]  такое сожаление о том, что вот качество строительства, которое очень плохое, а в советские времена
[02:52:11.120 --> 02:52:17.760]  как раз оно было замечательное, не было никаких протечек. Вот, неудачно, да, и вот появится тоже
[02:52:17.760 --> 02:52:23.400]  новое, что БКЛ — это устаревший проект, который потерял свою актуальность. Несколько других акцентов,
[02:52:23.400 --> 02:52:31.760]  просто за неимением времени. Я показываю рейтинг стали напряженности, вот, позволил выделить
[02:52:31.760 --> 02:52:37.440]  Воронцовскую станцию метро, которой возможны потенциальные конфликты, очень низкий уровень,
[02:52:37.440 --> 02:52:41.640]  но все-таки он существует. И вот, собственно, еще Каховская, Дюзинная и Нанаповская,
[02:52:41.640 --> 02:52:48.120]  которые тоже выделили среди этих 31. И вот индекс социальной напряженности, индексы показали,
[02:52:48.120 --> 02:52:53.520]  что, во-первых, индекс, конечно, снизился в 22 году, но, в принципе, он показал достаточно низкие
[02:52:53.520 --> 02:53:01.360]  значения для социального стресса и для социального получия. И, разумеется, что в 22 году эти данные,
[02:53:01.360 --> 02:53:07.200]  вообще, уменьшились в два раза, потому что очевидно, что, несмотря на активность пользователей,
[02:53:07.200 --> 02:53:14.440]  проблемы были совершенно другие у людей. Эти данные мы проверили на четырех математических моделях,
[02:53:14.440 --> 02:53:24.760]  которые тоже у нас представлены. И вот агрессионные модели, модель премдеряда,
[02:53:24.760 --> 02:53:33.920]  первого ряда, математические модели с пользой МАДО. У нас это все в статье написано подробно.
[02:53:33.920 --> 02:53:44.320]  Модель основана стахастических и дифференциальных уравнениях с использованием PNN и сравнительной
[02:53:44.320 --> 02:53:53.000]  анализой. Результаты математического анализа математической модели подтвердили данное
[02:53:53.000 --> 02:54:01.000]  мерситеволоанализа. И дальше были проведены логические округ-группы, которые также позволили
[02:54:01.000 --> 02:54:07.000]  уточнить и подтвердить результаты. В данном случае, вот эта идентичность результатов по тремоделям
[02:54:07.000 --> 02:54:12.600]  дала нам уверенность в том, что мы сделали правильные выводы, которые связаны с тем,
[02:54:12.600 --> 02:54:18.520]  что анализ покойной расприятия москвичей. Особые, конечно, проблемы связаны с тем,
[02:54:18.520 --> 02:54:23.720]  когда закончится ситуация, потому что очень долго длится, есть немного проблем. И вот единственный
[02:54:23.720 --> 02:54:29.840]  потенциальный конфликт связан со станцией Воронцовская. А также позволило сделать
[02:54:29.840 --> 02:54:34.960]  ряд рекомендаций для того, чтобы нивелировать существующее социальное напряжение, которое
[02:54:34.960 --> 02:54:43.680]  все равно присутствует и которое нужно как-то решать. Я уложилась. Спасибо.
[02:54:49.520 --> 02:54:52.520]  Давайте начнем с Дарьи Победой.
[02:54:52.520 --> 02:55:02.520]  Спасибо, очень интересный заплак. Я хотел признать, мне кажется, что для применения моих методов,
[02:55:02.520 --> 02:55:09.520]  анализы публичных подвижений, вот у меня такой конкретный вопрос. Вот недовольство жителей
[02:55:09.520 --> 02:55:15.520]  каким-то переходом, а фиксируется, что какие-то жители, наоборот, довольные этим переходом.
[02:55:15.520 --> 02:55:22.520]  Я думаю, что представление возможно делать, но какие-то активные люди, мы не хотим здесь
[02:55:22.520 --> 02:55:28.520]  переходить таким там, а 90% наоборот, опять вот, какие-то такие данные тоже не вспоминают.
[02:55:28.520 --> 02:55:33.520]  Да, и мы пытаемся, конечно, их нужно пытаться вытащить. Естественно, что мы делаем, то есть по
[02:55:33.520 --> 02:55:39.520]  по трем классам, значит, позитивно-негативные и нейтральные классы. Конечно, позитивно,
[02:55:39.520 --> 02:55:43.520]  если кто-то говорит, конечно, мы это делаем. И главное, по агрессии, тоже для отсутствия
[02:55:43.520 --> 02:55:50.520]  наличия на агрессии. Здесь главная проблема в том, что когда люди довольны, они не доволь
[02:55:50.520 --> 02:55:57.520]  даже каким-то одним. Вы знаете, мне тоже потрясла ситуация с метро в пистех.
[02:55:57.520 --> 02:56:03.520]  Вот метро от линии в пистех. Например, это же еще во времена, когда она, значит, тоже очень
[02:56:03.520 --> 02:56:07.520]  давно затевалась, и это должна была быть ветка, когда она на Дмитровке там рядом
[02:56:07.520 --> 02:56:11.520]  с центром Федорова. Когда еще был же Федоров, он даже туда собирался ее тянуть.
[02:56:11.520 --> 02:56:15.520]  И она уже была нарисована. И тут, значит, Федоров умирает, и все это замирает.
[02:56:15.520 --> 02:56:19.520]  20 лет. То есть люди, а люди покупали квартиры, думали, что там метро будет.
[02:56:19.520 --> 02:56:25.520]  Люди, значит, нетерпение. И через 20 лет, когда тянут это метро, и надоле от проекта,
[02:56:25.520 --> 02:56:28.520]  я знаю хорошо этот район, я была уверена, что вообще все будет спокойно.
[02:56:28.520 --> 02:56:31.520]  Вообще все будут счастливы там и в чепки бросать воздух.
[02:56:31.520 --> 02:56:36.520]  Куда там? Там были, значит, поскольку Дмитровка достаточно широкая, но там кусок
[02:56:36.520 --> 02:56:42.520]  был по наземному, наземный метро. И вот люди, которые жили прямо напротив, прямо
[02:56:42.520 --> 02:56:48.520]  вдоль Дмитровки, стали писать все вот негативные отзывы. Их было очень много,
[02:56:48.520 --> 02:56:52.520]  а позитивные, естественно, те, кто были довольны, они как 20 лет ждали, дождались,
[02:56:52.520 --> 02:56:56.520]  они себя довольны и ждут, когда откроют метро. Но, вот надо сказать, что тоже меня
[02:56:56.520 --> 02:57:02.520]  удивило, но, в принципе, типичная реакция для Собянина, он пошел после вот этих
[02:57:02.520 --> 02:57:06.520]  вот следов, после этих данных, он переделал, пошел на резкое дорожание проекта
[02:57:06.520 --> 02:57:11.520]  и сделали его под землю. Но вообще, вот это, конечно, да, этих людей там процент
[02:57:11.520 --> 02:57:16.520]  в отношении... Спасибо. Мы будем, конечно, вы понимаете, что мы применяем уже
[02:57:16.520 --> 02:57:21.520]  только то, что мы понимаем в ваших работах. Так, пересматриваем. Не так просто разобраться.
[02:57:21.520 --> 02:57:27.520]  То есть, мы, конечно, были бы счастливы какой-то еще модель использовать.
[02:57:27.520 --> 02:57:30.520]  Профессор Барантьев. Да, нет.
[02:57:30.520 --> 02:57:36.520]  Вот такой большой объем данных, и вы пишете, что это все про БКР. Не понятно,
[02:57:36.520 --> 02:57:42.520]  а что вы глазами это не проверили. А как вы понимаете, что в этих данных нет грязи
[02:57:42.520 --> 02:57:47.520]  или мало грязи? Какой процент, аморийки были ли какие-то эксперименты
[02:57:47.520 --> 02:57:52.520]  в специальном теле? Вообще, как концентровать данные, чтобы получить, что
[02:57:52.520 --> 02:57:56.520]  пробыкали? Это какой-то скидочный учебный слух по вашему поводу?
[02:57:56.520 --> 02:58:01.520]  То есть, у нас от начала было очень много проблем с этим. Это, в принципе, мы
[02:58:01.520 --> 02:58:05.520]  использовали очень много етворяйств, то есть, много разных вообще примеров. Это
[02:58:05.520 --> 02:58:09.520]  было еще несколько лет назад, когда мы только начинали делать проекты. И, в принципе,
[02:58:09.520 --> 02:58:14.520]  на сегодняшний день проблем практически нет, потому что, как показали в сравнении
[02:58:14.520 --> 02:58:19.520]  результаты, про аналитик. У нас же заказчик-то топ, ты упитанный, пожалуйста, тебе.
[02:58:19.520 --> 02:58:23.520]  Они дорогие, конечно, они самые лидеры на рынке, поэтому они цены там безумно
[02:58:23.520 --> 02:58:28.520]  останавливают. Но наш заказчик, берем, собственно говоря, у них данные,
[02:58:28.520 --> 02:58:32.520]  поэтому там, конечно, приходится их чистить, но уже в ходе работы это уже не
[02:58:32.520 --> 02:58:37.520]  проблема. Вот когда, например, начинали работать, когда друг там в каких-то
[02:58:37.520 --> 02:58:41.520]  режимах, то, конечно, пришлось перейти на крито, но другие в отборщики. Это была
[02:58:41.520 --> 02:58:45.520]  катастрофа. То есть, тоже раз-два, я знаю, что это. Ну, это вот тоже путем, то есть,
[02:58:45.520 --> 02:58:50.520]  это можно как делать, но это совершенно. Очень про аналитик наша. Все.
[02:58:50.520 --> 02:58:53.520]  А поэтому проблемы решают.
[02:59:02.520 --> 02:59:06.520]  Если мы сделаем произвольную постобработку вот этой матрицы, вопрос
[02:59:06.520 --> 02:59:10.520]  соответствует ли это какому-нибудь регуляризатору? Ответ – да. Вот он
[02:59:10.520 --> 02:59:15.520]  написан. То есть, мы можем просто какой-то евристический алгоритм применить вот
[02:59:15.520 --> 02:59:19.520]  к этой матрице, рассматривая последовательность векторов слов в тексте
[02:59:19.520 --> 02:59:24.520]  просто как пучок временных рядов. Что-то с ними сглазить, разрядить, еще что-то
[02:59:24.520 --> 02:59:29.520]  сделать. И что бы мы ни делали, это будет регуляризатор.
[02:59:29.520 --> 02:59:34.520]  Ну, вот, собственно, да. Частный случай здесь рассмотрен. Сейчас я это не буду.
[02:59:34.520 --> 02:59:39.520]  Да. И оказалось, что такой регуляризатор одновременно улучшает несколько
[02:59:39.520 --> 02:59:44.520]  характеристик тематической модели. Разреженность, различность тем,
[02:59:44.520 --> 02:59:49.520]  когерентность тем и так далее. Теперь немножечко про приложение тематического
[02:59:49.520 --> 02:59:56.520]  моделирования. Их всякое много разных. Вот. Но поскольку мы занимаемся разными
[02:59:56.520 --> 03:00:01.520]  проектами в области digital humanities, цифровых, гуманитарных исследований,
[03:00:01.520 --> 03:00:05.520]  то там, к тематической модели, предъявляется сразу совокупность
[03:00:05.520 --> 03:00:09.520]  требований. И вот все то, что мы умеем записывать как регуляризаторы,
[03:00:09.520 --> 03:00:13.520]  это надо применять вместе. То есть, тематическая модель, она должна быть
[03:00:13.520 --> 03:00:18.520]  интерпретируемой, иерархической, темпоральной, то есть, время учитывать,
[03:00:18.520 --> 03:00:22.520]  если это соцсети или научные публикации, мультимодальной там и так далее.
[03:00:22.520 --> 03:00:27.520]  То есть, все и сразу мы хотим. Вот регуляризаторов много всяких разных.
[03:00:27.520 --> 03:00:31.520]  Я их тут в качестве пиктограммок изобразил, чтобы можно было глазом сразу
[03:00:31.520 --> 03:00:36.520]  понимать, что про что, и вместо формул потом писать на языке пиктограммок,
[03:00:36.520 --> 03:00:42.520]  что хотим. И вся вот эта вот техника, она у нас реализована в проекте,
[03:00:42.520 --> 03:00:46.520]  который называется BigRTM. Да, забыл сказать, главная RTM – это
[03:00:46.520 --> 03:00:50.520]  аддитивная регулизация тематических моделей. Приставка Big подсказывает,
[03:00:50.520 --> 03:00:54.520]  что большие данные мы умеем обрабатывать. И самое главное, в сравнении
[03:00:54.520 --> 03:00:59.520]  с байсовским походом, в чем принучество? В том, что унификация появляется,
[03:00:59.520 --> 03:01:03.520]  в том, что можно этот алгоритм реализовать один раз в промышленном коде,
[03:01:03.520 --> 03:01:07.520]  а потом регуляризаторы менять как перчатки и даже комбинировать их
[03:01:07.520 --> 03:01:12.520]  с помощью суммирования взвешенного. А вот в байсовском тематическом моделировании
[03:01:12.520 --> 03:01:17.520]  каждую модель надо вывести заново все формулы, потом заново реализовать в коде,
[03:01:17.520 --> 03:01:21.520]  потом отладить. Ну и поэтому получается, что большинство тематических моделей,
[03:01:21.520 --> 03:01:26.520]  а их сотни в литературе – это вот модели на одну статью. И каких-то
[03:01:26.520 --> 03:01:31.520]  коммерческих приложений индустриальных они, как правило, не находят, кроме LDA,
[03:01:31.520 --> 03:01:36.520]  которые все знают уже двадцать лет. Вот, собственно, Bigarton – наш
[03:01:36.520 --> 03:01:42.520]  опенсорсный продукт. С 2014 года мы его развиваем, есть документация на сайте
[03:01:42.520 --> 03:01:47.520]  и так далее. Хочется немножечко про приложения. Да, это вот эксперимент,
[03:01:47.520 --> 03:01:52.520]  который показывает, что он где-то в десять-двадцать раз быстрее ближайших
[03:01:52.520 --> 03:01:57.520]  конкурентов. Это GenSim и VocalWabbit. Просто эффективная распараллеленная реализация
[03:01:57.520 --> 03:02:03.520]  сделана на ядрах. Без графических карт, кстати. И дальше у меня примеры
[03:02:03.520 --> 03:02:08.520]  примерно десяти-двенадцати разных приложений, но по времени я уже не успеваю
[03:02:08.520 --> 03:02:13.520]  об этом рассказать. Главная суть – что вы всегда увидите в середине картинки
[03:02:13.520 --> 03:02:18.520]  вот такой вот мешок регуляторов. То есть это те ограничения, которые мы
[03:02:18.520 --> 03:02:22.520]  накладываем на модель, чтобы она удовлетворяла тем свойствам, которые мы хотим
[03:02:22.520 --> 03:02:28.520]  потребовать от тематической модели. Занимаемся ли мы разведочным поиском?
[03:02:28.520 --> 03:02:33.520]  Вот парочка исследований была недавно, кандидатскую диссертацию Анастасия Янина
[03:02:33.520 --> 03:02:40.520]  защитила. Или занимаемся классификацией научных статей по рубрикаторам.
[03:02:40.520 --> 03:02:47.520]  Или мы занимаемся поиском в социальных сетях дискурса, который относится к
[03:02:47.520 --> 03:02:57.520]  обсуждению межнациональных отношений. Или делаем сценарный анализ записей
[03:02:57.520 --> 03:03:03.520]  колл-центра. Это мне уже напоминают, что у меня закончилось время.
[03:03:03.520 --> 03:03:10.520]  А вот я не знаю, как теперь снять это напоминание. Я сейчас уже быстренько
[03:03:10.520 --> 03:03:17.520]  заканчиваю. Да, topic detection tracking – это обработка новостных потоков.
[03:03:17.520 --> 03:03:22.520]  Выявление поляризованных мнений в политических новостях – отдельная задача,
[03:03:22.520 --> 03:03:28.520]  очень интересная, рассказал бы, но времени нет. Но главная мысль, что все вот эти
[03:03:28.520 --> 03:03:33.520]  задачи, разнообразные в области текстовой аналитики, они решаются с помощью
[03:03:33.520 --> 03:03:38.520]  вот этого вот самого инструмента аддитивной регуляризации, где мы просто
[03:03:38.520 --> 03:03:42.520]  говорим, какие мы хотим потребовать вещи от этой модели, и делаем из этих
[03:03:42.520 --> 03:03:47.520]  требований линейную комбинацию. Банковские транзакционные данные можно
[03:03:47.520 --> 03:03:52.520]  обрабатывать так же, как и тексты. Ну и так далее.
[03:03:52.520 --> 03:04:00.520]  Резюмирую. Я утверждаю, что если бы… вот лемма, с которой я начал, она кажется
[03:04:00.520 --> 03:04:04.520]  совершенно классической, вроде бы она в учебниках должна была быть. Но специалисты
[03:04:04.520 --> 03:04:10.520]  по оптимизации говорят, что они такой леммы не знают, такой теоремы, почему-то.
[03:04:10.520 --> 03:04:15.520]  Если бы вообще сообщество знало об этом результате, то я думаю, что вот эти 20
[03:04:15.520 --> 03:04:19.520]  лет развития вероятностного тематического моделирования, они бы прошли бы совсем
[03:04:19.520 --> 03:04:25.520]  по-другому. Не под знаком байсовского вывода, а под знаком вот такой вот
[03:04:25.520 --> 03:04:30.520]  регуляризации, потому что она радикально упрощает математику. Сейчас тематическое
[03:04:30.520 --> 03:04:34.520]  моделирование живет нейросетевыми моделями, и вот здесь возникает еще одна
[03:04:34.520 --> 03:04:39.520]  заманчивая возможность. Эту же лему можно использовать в тех случаях, когда в вашей
[03:04:39.520 --> 03:04:44.520]  нейронной сети есть векторные параметры, которые по какой-то причине вы хотите
[03:04:44.520 --> 03:04:50.520]  сделать неотрицательными нормированиями. Такие случаи бывают. И уж точно, если этот
[03:04:50.520 --> 03:04:56.520]  инструмент строить, например, в PyTorch, то с его помощью можно строить любые
[03:04:56.520 --> 03:05:00.520]  вероятностные тематические модели, эффективно распараллеливая это на
[03:05:00.520 --> 03:05:05.520]  графических картах. Этого мы пока еще не делали, делюсь идеей, над этим мои
[03:05:05.520 --> 03:05:10.520]  студенты сейчас работают. Ну вот, собственно, да. И все на этом. Спасибо за внимание.

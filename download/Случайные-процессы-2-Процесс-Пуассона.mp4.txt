[00:00.000 --> 00:12.600]  Давайте продолжать и начинать нашу вторую лекцию.
[00:12.600 --> 00:19.720]  Я напомню, что в прошлый раз мы вели понятие случайного
[00:19.720 --> 00:25.660]  процесса, его важнейшую характеристику, вероятностное
[00:25.660 --> 00:29.760]  это семейство конечномерных распределений ввел теория
[00:29.760 --> 00:34.360]  Калмогорова о том, что нам достаточно сдавать процесс
[00:34.360 --> 00:39.160]  через некое семейство функций, которое согласовано в определенном
[00:39.160 --> 00:42.960]  смысле, также как согласовано семейство конечномерных
[00:42.960 --> 00:43.960]  распределений.
[00:43.960 --> 00:47.840]  Ну и некоторые другие понятия.
[00:47.840 --> 00:50.480]  Сегодня мы будем с вами углублять наш взгляд на
[00:50.480 --> 00:56.040]  случайные процессы и начнем мы, пожалуй, с важнейших
[00:56.040 --> 01:00.480]  численных характеристик после функции распределения
[01:00.480 --> 01:02.720]  это мотождание, дисперсия и так далее.
[01:02.720 --> 01:10.200]  Первое, я не буду подробно писать, здесь все очень просто,
[01:10.200 --> 01:13.600]  что мы будем называть математическим ожиданием случайного процесса.
[01:13.600 --> 01:21.280]  Мотождание случайного процесса это функция времени, которая
[01:21.280 --> 01:25.680]  в каждый момент времени равна математическому ожиданию
[01:25.680 --> 01:27.240]  соответствующего сечения.
[01:27.240 --> 01:29.840]  Все тривиально.
[01:29.840 --> 01:37.160]  То есть это значит, что мы Т зафиксировали, то, что
[01:37.160 --> 01:40.880]  мы получим, это случайная величина, вот ее математическое
[01:40.880 --> 01:44.240]  ожидание в этот момент Т.
[01:44.240 --> 01:47.240]  Так что если нам нужно посчитать мат ожидания какого-то
[01:47.240 --> 01:51.440]  процесса, мы просто проходимся по всем Т, вычисляем мат ожидания
[01:51.440 --> 01:54.120]  соответствующей случайной величины, ну и получаем
[01:54.720 --> 01:55.860]  функции этого Т.
[01:55.860 --> 01:58.840]  То, что мотождание при разных Т может быть разным.
[01:58.840 --> 02:01.200]  Вот эта функция называется математическим ожиданием
[02:01.200 --> 02:03.020]  случайного процесса.
[02:03.020 --> 02:06.640]  Она не обязана быть определена для всех Т, потому что мотождание
[02:06.640 --> 02:08.920]  не обязательно быть определенным всегда.
[02:08.920 --> 02:11.360]  У некоторых случайных ключин есть мотождание, у каких-то
[02:11.360 --> 02:12.360]  нет мотождания.
[02:12.360 --> 02:18.960]  Так что вот такая функция называется мотожданием.
[02:18.960 --> 02:21.760]  Вот так и все, где она определена.
[02:21.760 --> 02:23.800]  Точно также можно вести понятие дисперсии.
[02:23.800 --> 02:36.600]  значит дисперсия это функция времени которая в каждый момент времени равна дисперсии соответствующего
[02:36.600 --> 02:47.960]  сечения случайного процесса точно также как мат ожидания вот ну здесь я ничего нового
[02:47.960 --> 02:56.720]  он практически не сообщил а вот новое для вас понятие это корреляционная функция я ее буду
[02:56.720 --> 03:06.880]  обозначать буква r это функция двух моментов времени давайте так напишем t и s это функция
[03:06.880 --> 03:17.440]  двух моментов времени которая для каждой пары t и s времен равна корреляционному моменту
[03:17.440 --> 03:26.240]  соответствующих сечений корреляционный момент у него синоним кавариация я сейчас все распишу
[03:26.240 --> 03:36.440]  кавариация от кси т запятая кси с а что это такое это есть математическое ожидание по
[03:36.440 --> 03:49.200]  определению кавариации это кси от t минус мат ожидания кси от t умножить на кси от с минус
[03:49.200 --> 03:56.640]  мат ожидания и от с вот по определению что такое кавариация двух случайных величин вот здесь мы
[03:56.640 --> 04:04.240]  ее написали и если раскрыть скобки вспомнить свойства математического ожидания то тогда
[04:04.240 --> 04:09.200]  можно переписать это в эквивалентном виде более простом математическое
[04:09.200 --> 04:29.000]  ожидание кси от т на кси от с минус математ
[04:29.000 --> 04:35.400]  могут быть коррелированы, могут быть зависимыми даже, так что вот эта функция
[04:35.400 --> 04:41.000]  показывает корреляцию между двумя различными ее сечениями.
[04:41.000 --> 04:47.100]  Что мы взяли t зафиксировали, взяли s зафиксировали, это у нас какие-то
[04:47.100 --> 04:51.700]  случайные величины уже вполне определенные, и здесь мы вычисляем
[04:51.700 --> 04:57.500]  корреляционный момент или к вариацию. Вот по вот этим формулам и получается
[04:57.500 --> 05:08.900]  функция rcts. Эта функция, она наследует свойства кавриаций, ну например, она не
[05:08.900 --> 05:13.140]  чувствительно к аддитивным постоянным, то есть если вы сюда добавите какую-то
[05:13.140 --> 05:18.500]  константу, не случайную величину a, сюда добавите b, тогда кавриация не изменится.
[05:18.500 --> 05:23.580]  Вы вольны для упрощения себе жизни в вычислении добавлять сюда и сюда любые
[05:23.580 --> 05:30.100]  константы. Вот если вы умножите здесь на a, вы эту a можете вынести линейно за
[05:30.100 --> 05:36.860]  кавриацию. Вот такие вот примитивные свойства можно назвать. Какие еще можно
[05:36.860 --> 05:43.660]  свойства назвать? Корреляционный момент связан с дисперсией. Каким образом?
[05:43.660 --> 05:48.900]  Корреляционный момент случайной величины сама собой дает дисперсию. В нашем
[05:48.900 --> 06:03.460]  случае в наших терминах rtt есть кавриация xiat, это есть дисперсия, то есть d с индексом
[06:03.460 --> 06:10.180]  xiat. Так что если мы знаем корреляционную функцию, то мы можем вычислить дисперсию,
[06:10.180 --> 06:20.260]  просто взяв одинаковые аргументы у функции r. Еще есть свойство симметричность, есть свойство
[06:20.260 --> 06:25.780]  вот такое. Если мы поменяем их местами, то это равносильно тому, что мы поменяем местами
[06:25.780 --> 06:34.540]  множители в произведении, но понятно, что ничего не изменится. Так что вот есть такое свойство
[06:34.540 --> 06:49.060]  симметричность. Есть еще одно свойство очень полезное, значит rx ts. Так, t и s, по-моему там
[06:49.060 --> 07:04.060]  в квадрате. Сейчас я сначала напишу, меньше либо равно rxtt на rxss. Да, вот так вот. Это еще называется
[07:04.060 --> 07:10.100]  неравенством Коши Банюковского для корреляционной функции. Ну, в общем-то, это даже не то,
[07:10.100 --> 07:16.740]  что называется, это как бы следствие неравенства Коши Банюковского в теории вероятностей для
[07:16.740 --> 07:20.820]  корреляционного момента. Потому что корреляционный момент определяет скалярные произведения, это
[07:20.820 --> 07:29.420]  свойство скалярного произведения. Вот, хорошо. Вот такие свойства есть. Потом у нас будут лекции,
[07:29.420 --> 07:36.980]  где мы гораздо глубже уйдем в изучение свойств корреляционной функции. А пока вот я напишу самые-самые
[07:36.980 --> 07:45.940]  такие примитивные, которые нам вот сейчас понадобятся. Кроме корреляционной функции у нас в курсе еще будет
[07:45.940 --> 07:55.820]  ковариационная функция. Мы ее будем обозначать буквой k. Ковариационная функция по определению,
[07:55.820 --> 08:06.220]  это есть математическое ожидание и от t на кси от s. То есть это корреляционная функция ts,
[08:06.220 --> 08:18.900]  корреляционная функция, плюс, получается плюс мат ожидания кси от t на кси, мат ожидания кси от s.
[08:18.900 --> 08:30.020]  То есть она определена для процессов, не только у которых мат ожидания равны нулю, но если мат
[08:30.020 --> 08:38.300]  ожидания равно нулю у процесса всюду, то тогда ковариационная функция совпадает с корреляционной,
[08:38.300 --> 08:47.220]  потому что это просто 0 и это равный. В каких-то случаях удобно вычислять корреляционную, в каких-то
[08:47.220 --> 08:54.780]  случаях ковариационную. У нас будут теоремы, которые формулируются гораздо проще, компактнее и
[08:54.780 --> 09:03.260]  доказываются проще для ковариационной функции, а не для корреляционной. Ну в общем, мы будем
[09:04.100 --> 09:11.020]  одинаковое количество времени иметь из той и другой функции. Обращаю внимание здесь на некий
[09:11.020 --> 09:19.780]  разлад в терминологии, в некоторых книжках, например, Миллер Панков, они называют вот это
[09:19.780 --> 09:26.940]  корреляционной функцией, а вот это ковариационной. То есть тут есть вот такая вот, обратите на это
[09:26.940 --> 09:34.100]  внимание, я рассказываю так, как у меня в моих пдфках, так как вот в классических книжках авторов
[09:34.100 --> 09:43.700]  на тангус Горбачёв, там еще много где. Вот это у нас будет корреляционной, а это ковариационной.
[09:43.700 --> 09:55.820]  Так, ну вот, вот такие вот понятия. Что еще можно здесь вести? Говоря о корреляционных моментах,
[09:55.820 --> 10:01.460]  мы можем вводить, кстати говоря, еще и взаимные корреляционные функции. Вот есть у нас два
[10:01.460 --> 10:12.700]  процесса, кси и это. Тогда взаимной корреляционной функцией будем называть вот такую функцию. Видите
[10:12.700 --> 10:19.540]  здесь, какой индекс кси-это, потому что он для двух процессов пишется. Это есть просто ковариация для
[10:19.540 --> 10:34.100]  кси-ат на это атес. Вот, это взаимная корреляционная функция двух процессов. Так, ну что еще можно сказать?
[10:34.100 --> 10:43.900]  Какие еще есть характеристики? А, характеристическая функция. Так, сейчас я сначала напишу, так сходу по
[10:43.900 --> 10:53.300]  памяти не очень помню, она нам редко пригождается, фи. Значит, для кси-ат атес, это есть математическое
[10:53.300 --> 11:07.180]  ожидание экспонента и на эс на кси-ат, вот так. Вот, то есть характеристической функцией процесса
[11:07.180 --> 11:18.460]  называется вот такая функция, кстати, двух переменных, с и t, такая, что каждый момент времени t, она равна
[11:18.460 --> 11:29.460]  характеристической функции, видите, зависит от s, соответствующего течения, вот так. Это характеристическая
[11:29.460 --> 11:35.620]  функция процесса. Здесь, кстати, сразу но возникает. Вот, мы помним в теории вероятности, что
[11:35.620 --> 11:43.900]  характеристическая функция однозначно определяет распределение случайной величины. Вот, в процессах это не так.
[11:43.900 --> 11:52.060]  Если мы задаем хар-функцию вот таким образом, то хар-функция однозначно определит только одномерное
[11:52.060 --> 11:59.700]  распределение процесса, то есть только распределение кси-ат. Она ничего не говорит о том, как связаны
[11:59.700 --> 12:08.900]  между собой сечения в разные моменты времени. То есть вот, если у вас где-то спросят, то имейте в виду,
[12:08.900 --> 12:14.860]  что хар-функция случайного процесса однозначно определяет только одномерное распределение. Она
[12:14.860 --> 12:23.040]  не может восстановить распределение всего процесса, из нее нельзя вывести в общем случае семейство
[12:23.540 --> 12:30.680]  конечномерных распределений. Хотя есть такие процессы, уникальные. Мы, кстати, будем с ними иметь дело и,
[12:30.680 --> 12:36.500]  как правило, только с ними мы им будем иметь дело, у которых семейство конечномерных распределений
[12:36.500 --> 12:45.440]  определяется одним только одномерным распределением. Вот, какие процессы есть, сегодня полосоновский
[12:45.440 --> 12:50.720]  процесс будем рассматривать. Там достаточно задать некоторые его свойства и одномерные распределения,
[12:50.720 --> 12:55.720]  и тогда из них можно однозначно получить свойства все остальные,
[12:55.720 --> 13:00.720]  то есть семейство конечномерных распределений тоже из одного одномерного можно восстановить.
[13:00.720 --> 13:07.720]  Ну и для таких процессов, естественно, харфункция процесса однозначно определяет не только одномерное,
[13:07.720 --> 13:11.720]  но и всё, ну просто потому что из одноверного можно вывести любое другое,
[13:11.720 --> 13:18.720]  просто такие вот особенные процессы, а в общем случае нет, харфункция не определяет всё.
[13:20.720 --> 13:27.720]  Так, что ещё могу здесь назвать? С таких понятий вроде бы всё ввёл.
[13:29.720 --> 13:33.720]  Ещё хочу пояснить по поводу вот этих вот всех вещей.
[13:36.720 --> 13:40.720]  Обратите внимание, что математическое ожидание – это мат ожидания ксиатэ.
[13:40.720 --> 13:45.720]  Сюда входит только одно сечение ксиатэ, сюда не входят никакие другие сечения,
[13:45.720 --> 13:48.720]  только одно – т зафиксировали, вычислили мат ожидания.
[13:48.720 --> 13:56.720]  Это означает, что математическое ожидание, оно определяется только одномерным распределением процесса.
[13:56.720 --> 13:59.720]  Неважно, какое у него там двумерное, трюмерное распределение,
[13:59.720 --> 14:03.720]  вот ожидание определено только одномерным распределением.
[14:03.720 --> 14:08.720]  Вот это можно явно показать, записав это математическое ожидание
[14:08.720 --> 14:13.720]  в терминах интеграла Риммана с Тель-Тиеса или Лебега с Тель-Тиеса.
[14:13.720 --> 14:26.720]  Смотрите, как интеграл от x на dfc от x точкой запятой t.
[14:26.720 --> 14:31.720]  То есть мы пишем интеграл, от чего мы берём, то есть это всё заменяем на x,
[14:31.720 --> 14:36.720]  потом пишем дифференциал, и дальше идёт функция распределения этого чего-то.
[14:36.720 --> 14:40.720]  Но оно зависит от t, поэтому мы вот таким вот обозначением пользуемся.
[14:40.720 --> 14:44.720]  Так что вот эта запись, она просто явно нам показывает,
[14:44.720 --> 14:48.720]  что мат ожидания является функцией только одномерного распределения.
[14:48.720 --> 14:50.720]  И дисперсия тоже.
[14:50.720 --> 14:56.720]  Её можно написать в интеграл x минус, получается вот так вот,
[14:56.720 --> 15:04.720]  mxt в квадрате на dfxt.
[15:04.720 --> 15:10.720]  Ну если вы не очень помните, что эта запись, значит это ничего страшного,
[15:10.720 --> 15:14.720]  это просто для понимания я написал.
[15:14.720 --> 15:18.720]  Если эта функция распределения имеет плотность,
[15:18.720 --> 15:23.720]  тогда на этот дифференциал вы просто смотрите, как она плотность умножить на dx.
[15:23.720 --> 15:27.720]  Плотность зависит от x и t вообще говоря, плотность умножить на dx.
[15:27.720 --> 15:32.720]  Если это дискретное распределение, тогда вместо интеграла пишете сумму,
[15:32.720 --> 15:39.720]  а вместо df вы пишете вероятность того, что xt равняется x.
[15:39.720 --> 15:41.720]  Вот и всё.
[15:41.720 --> 15:45.720]  И вот этих как бы двух случаев нам в принципе достаточно.
[15:45.720 --> 15:50.720]  А вот эта запись, она вот самая общая, она в себя всё включает,
[15:50.720 --> 15:55.720]  в том числе когда и смешанное какое-то распределение, не дискретное, не непрерывное.
[15:55.720 --> 16:01.720]  Мы можем написать похожие вещи и для корреляционной функции.
[16:01.720 --> 16:08.720]  Вот корреляционная функция, это будет тоже уже двумерный интеграл получается.
[16:08.720 --> 16:10.720]  Давайте я напишу здесь.
[16:10.720 --> 16:20.720]  Значит rcts можно тоже выразить как двумерный интеграл от чего?
[16:20.720 --> 16:32.720]  x минус мат ожидания, xi от t на y минус мат ожидания, это от t.
[16:32.720 --> 16:34.720]  Вот.
[16:34.720 --> 16:39.720]  И умножить на d2f.
[16:39.720 --> 16:45.720]  Значит xi x запятая y, t запятая s.
[16:45.720 --> 16:48.720]  И вот здесь будет стоять уже двумерное распределение.
[16:48.720 --> 16:52.720]  Ну а здесь уже двукратный интеграл или бегость Ильтьеса.
[16:52.720 --> 16:56.720]  Ну это такие формулы абстрактные, но мы не будем ими пользоваться.
[16:56.720 --> 17:02.720]  Я просто говорю, чтобы вы понимали, что вот эти величины, они зависят от одномерных распределений.
[17:02.720 --> 17:05.720]  Вот эта штука не определяется одномерным распределением.
[17:05.720 --> 17:07.720]  Эта штука определяется двумерным распределением.
[17:07.720 --> 17:14.720]  То есть для её вычисления важно знать, как распределены сечения xi от t и xi от s.
[17:14.720 --> 17:21.720]  И одного только распределения xi от t, хоть и во все моменты времени t, в том числе и s,
[17:21.720 --> 17:25.720]  недостаточно знать для вычисления этой характеристики.
[17:25.720 --> 17:27.720]  Нужно двумерное распределение знать.
[17:27.720 --> 17:32.720]  Да.
[17:32.720 --> 17:34.720]  Так, ой.
[17:34.720 --> 17:36.720]  Нет, нет, нет, здесь xi.
[17:36.720 --> 17:38.720]  Слушайте, тут что-то всё вообще неправильно.
[17:38.720 --> 17:41.720]  Тут xi, а здесь s.
[17:41.720 --> 17:44.720]  О, да, спасибо.
[17:44.720 --> 17:49.720]  Это у нас было там, когда взаимное корреляционное, а здесь нет.
[17:49.720 --> 17:52.720]  Значит, x, y, m, xi, t, s.
[17:52.720 --> 17:54.720]  Сейчас всё правильно.
[17:54.720 --> 17:56.720]  Ну вот.
[17:56.720 --> 17:59.720]  Всё, все вот эти вот понятия мы ввели.
[17:59.720 --> 18:01.720]  Есть ли пока какие-то вопросы?
[18:01.720 --> 18:05.720]  Ну, пока всё достаточно тривиально.
[18:05.720 --> 18:10.720]  И по большому счёту мало чего нового пока мы узнаём.
[18:10.720 --> 18:14.720]  Вот пока только корреляционная функция нового для вас понятия.
[18:14.720 --> 18:16.720]  Так, ну хорошо.
[18:16.720 --> 18:19.720]  Основные численные характеристики мы ввели.
[18:19.720 --> 18:23.720]  И будем потом с ними всё время работать.
[18:23.720 --> 18:31.720]  Теперь переходим к большой теме новой, которую мы сегодня рассмотрим.
[18:31.720 --> 18:33.720]  Поассоновский процесс.
[18:33.720 --> 18:39.720]  Это первый процесс, такой содержательный, который мы с вами изучим, рассмотрим.
[18:39.720 --> 18:45.720]  И он очень важен, потому что он встречается в приложениях, в многочисленных приложениях.
[18:45.720 --> 18:52.720]  И мы будем очень много к нему возвращаться, о нём говорить.
[18:52.720 --> 18:57.720]  И мы его будем изучать и на первом задании сейчас, первые лекции.
[18:57.720 --> 19:00.720]  И на втором задании у нас он ещё будет.
[19:00.720 --> 19:02.720]  У него огромное количество свойств.
[19:03.720 --> 19:10.720]  Он очень много где встречается.
[19:10.720 --> 19:12.720]  Поассоновский процесс.
[19:12.720 --> 19:15.720]  Но сначала я начну с некой вводной.
[19:15.720 --> 19:23.720]  Поассоновский процесс.
[19:23.720 --> 19:28.720]  Начну с некой вводной.
[19:28.720 --> 19:35.720]  Давайте мы представим себе ось времени и какие-то события, которые могут наступать.
[19:35.720 --> 19:41.720]  И нас интересует, сколько событий произошло на участке времени.
[19:41.720 --> 19:47.720]  Вот допустим у нас есть ось времени t, здесь у нас ноль, здесь у нас момент t.
[19:47.720 --> 19:51.720]  И нас интересует, сколько событий произошло на этом интервале времени.
[19:51.720 --> 19:53.720]  То есть время идёт в какой-то момент.
[19:53.720 --> 19:59.720]  В какой-то момент раз произошло событие, потом снова пустота, раз снова произошло событие, снова пустота и так далее.
[19:59.720 --> 20:03.720]  И вот этот счётчик событий мы здесь обозначим за kt.
[20:03.720 --> 20:11.720]  Так что этот счётчик событий, у него график его траектории, он следующий.
[20:11.720 --> 20:15.720]  Сначала, допустим, событий не было в нулевой момент времени.
[20:15.720 --> 20:22.720]  Поэтому до следующего события, пока оно произойдёт, здесь будет ноль событий.
[20:22.720 --> 20:27.720]  Потом событие какое-то произошло, неважно какое произошло, и счётчик наш скакнул.
[20:27.720 --> 20:29.720]  Ну, допустим, на единицу скакнул.
[20:29.720 --> 20:31.720]  Потом снова пустота, снова ничего.
[20:31.720 --> 20:34.720]  Потом снова что-то произошло в момент t, он снова скакнул.
[20:34.720 --> 20:36.720]  Ну, допустим, тоже на единицу.
[20:36.720 --> 20:40.720]  Так что его траектории это какие-то ступеньки.
[20:40.720 --> 20:44.720]  Кусочно постоянные не убывающие функции.
[20:44.720 --> 20:45.720]  Возвращающие.
[20:45.720 --> 20:46.720]  Вот счётчик.
[20:46.720 --> 20:49.720]  На него можно смотреть, как на счётчик событий на некотором интервале.
[20:49.720 --> 20:52.720]  Теперь давайте представим себе следующее.
[20:52.720 --> 20:57.720]  Что мы не просто какой-то произвольный счётчик будем рассматривать, не просто произвольные события.
[20:57.720 --> 21:03.720]  Давайте мы некоторые предположения ведём о ситуации, с которой мы имеем дело.
[21:03.720 --> 21:11.720]  Давайте мы предположим, что сколько событий произошло здесь, не зависит от того, сколько событий произошло здесь.
[21:11.720 --> 21:15.720]  Вот сколько событий произошло, это случайная величина.
[21:15.720 --> 21:24.720]  И мы будем считать, что на непересекающихся интервалах времени вот это число событий произошедших, они как случайные величины независимы.
[21:24.720 --> 21:34.720]  Ну и вообще, если мы возьмём много интервалов непересекающихся по времени и получим множество случайных величин,
[21:34.720 --> 21:42.720]  которые равны сколько произошло событий на этих интервалах времени, пусть все эти случайные величины независимы в совокупности.
[21:42.720 --> 21:47.720]  То есть самую сильную такую, возьмём независимость.
[21:47.720 --> 22:01.720]  Вот, то есть будем считать, что k от там t и плюс один минус k от t и для всех i, для любых t итых,
[22:01.720 --> 22:08.720]  такие, что вот эти вот интервалы не пересекаются, пусть все они независимы в совокупности.
[22:08.720 --> 22:13.720]  Вот это множество независимо в совокупности.
[22:13.720 --> 22:17.720]  k начинается в нуле, как я сказал до этого.
[22:17.720 --> 22:25.720]  Ещё давайте предположим, что то, сколько событий произошло на интервале,
[22:25.720 --> 22:31.720]  ну допустим t плюс h минус kt, зависит только от длины этого интервала.
[22:31.720 --> 22:36.720]  То есть сколько событий произошло здесь, как случайная величина, её распределение,
[22:36.720 --> 22:40.720]  не зависит от того, где вы этот интервал возьмёте, лишь бы он был одинаковым.
[22:40.720 --> 22:48.720]  То есть если вы возьмёте интервал и возьмёте его фиксированную длину и рассмотрите вот этот участок времени,
[22:48.720 --> 22:55.720]  то случайная величина, сколько здесь произошло событий, она имеет такое же распределение, как здесь.
[22:55.720 --> 22:57.720]  Это всё наше предположение, давайте так предполагать.
[22:57.720 --> 22:59.720]  То есть это однородность.
[22:59.720 --> 23:07.720]  То есть пусть вот эта вещь зависит только от h.
[23:07.720 --> 23:13.720]  Это то, что называется однородностью процесса, то есть не важно, где мы его рассматриваем.
[23:13.720 --> 23:19.720]  Итак, мы рассматриваем счётчик каких-то событий, происходящих в разные случайные моменты времени.
[23:19.720 --> 23:27.720]  Вот такой, что он начинается из нуля, он однороден, не важно, где вы его рассматриваете,
[23:27.720 --> 23:32.720]  сколько произошло событий, не зависит от участка на этом интервале времени.
[23:32.720 --> 23:38.720]  И пусть сколько событий произошло здесь, не зависит от числа событий, произошедших здесь, здесь и так далее.
[23:38.720 --> 23:40.720]  Вот мы рассмотрим такую модель.
[23:40.720 --> 23:44.720]  Такие предположения очень естественные, обратите внимание.
[23:44.720 --> 23:52.720]  То есть такие простейшие, естественные предположения о потоке событий, которые происходят, которые разворачиваются во времени.
[23:53.720 --> 24:03.720]  И мы ведём с вами ещё одно предположение, которое значительно сузит наше рассмотрение.
[24:03.720 --> 24:07.720]  С одной стороны, а с другой стороны оно всё равно будет оставаться достаточно общим.
[24:07.720 --> 24:15.720]  Мы будем считать, что вот это приращение, это не просто какая-то случайная величина,
[24:15.720 --> 24:20.720]  а то, что она является полосоновской случайной величиной.
[24:20.720 --> 24:28.720]  То есть пусть это полосоновская случайная величина.
[24:28.720 --> 24:34.720]  Давайте я вам сразу напомню, что Xi имеет распределение полосона с параметром лямбда.
[24:34.720 --> 24:42.720]  Это означает, что она принимает значение 0, 1, 2 и так далее до бесконечности.
[24:42.720 --> 24:52.720]  И вероятность вот этого события, это есть E в степени минус лямбда на лямбда в степени k, на k факториал.
[24:52.720 --> 24:55.720]  k равняется 0, 1, 2 и так далее.
[24:55.720 --> 25:01.720]  Где лямбда это параметр, лямбда больше нуля, который называется ещё интенсивностью.
[25:01.720 --> 25:08.720]  Вот мы будем с вами предполагать, что вот эта случайная величина имеет полосоновское распределение.
[25:08.720 --> 25:11.720]  Поэтому предположение зависит только от h.
[25:11.720 --> 25:18.720]  Так что вообще говоря, её аргумент, а лямбда, который там стоит, зависит от этого h.
[25:19.720 --> 25:21.720]  Вот, да.
[25:35.720 --> 25:43.720]  Что именно будет зависеть? Её распределение будет зависеть от разности t и плюс первого и t итого.
[25:43.720 --> 25:50.720]  То есть, все вот эти приращения, эти случайные величины со своими какими-то распределениями, которые зависит от этой разности,
[25:50.720 --> 25:54.720]  если разности разные, то их распределение тоже разное.
[25:54.720 --> 25:58.720]  Ну а это свойство говорит о том, что они независимы.
[25:58.720 --> 26:03.720]  Оно не говорит о том, как они именно распределены, о том что все они в совокупности независимы.
[26:03.720 --> 26:09.720]  Вот. О том, как именно распределены мы говорим вот здесь, когда я говорю, что вот эта вещь,
[26:09.720 --> 26:14.520]  во-первых, зависит только от аж, а во-вторых, давайте предполагать, что это поассоновская
[26:14.520 --> 26:20.080]  случайная величина. Вот где у нас, кстати, впервые появляется распределение. Потому что без
[26:20.080 --> 26:23.640]  распределения, если мы его нигде не ведем, мы ничего не получим. Где-то оно должно быть
[26:23.640 --> 26:30.160]  введено. Вот это первое место, где оно появляется. Почему вдруг мы говорим о поассоновской случайной
[26:30.160 --> 26:37.800]  величине? Ведь много есть распределений. В общем-то, вот этот выбор поассоновского распределения
[26:37.800 --> 26:44.960]  для приращения, он естественным образом возникает в приложениях. И это основано на так называемом
[26:44.960 --> 26:50.120]  законе редких событий и теореме поассона, которая у вас должна была быть по теории вероятности.
[26:50.120 --> 26:58.960]  Она, грубо говоря, говорит о том, что если у вас есть очень много событий, которые могут
[26:58.960 --> 27:04.240]  происходить, а могут не происходить, и вероятность того, что они произойдут, очень маленькая, но этих
[27:04.240 --> 27:11.360]  событий очень много, тогда вот эта случайная величина, которая равна тому, сколько событий
[27:11.360 --> 27:19.880]  произошло, вот она имеет распределение близкое к поассону. Формально говоря, это означает, что если
[27:19.880 --> 27:28.200]  у вас есть набор бернулевских случайных величин, n штук, и они имеют распределение
[27:28.200 --> 27:37.080]  вернули p,n, причем с ростом n, их вероятности того, что они происходят, вот эти p,n, они стремятся
[27:37.080 --> 27:47.000]  к нулю, но стремятся к нулю так, что n, p,n стремится к некоей лямде больше нуля, то есть у вас количество
[27:47.000 --> 27:54.080]  случайных величин растет, вероятности того, что они станут равными единицами падает, и вот это n
[27:54.080 --> 28:02.760]  рост и падение p связаны вот таким образом, то тогда вот эта случайная величина, сумма, то есть сколько
[28:02.760 --> 28:08.160]  произошло, ведь это нолики единички, их сумма, это сумма всех единичек, то есть сколько всего
[28:08.160 --> 28:14.720]  произошло событий, вот эта вещь, она сходится к случайной величине, которая имеет распределение
[28:14.720 --> 28:20.120]  поассон с вот этим параметром лямда, который вот здесь фигурирует, который является пределом n, p,n,
[28:20.120 --> 28:27.800]  вот эта теорема поассона и закон редких событий, то что еще называется, вот эта ситуация, когда у вас
[28:27.800 --> 28:35.600]  чего-то много и это происходит редко, а вас интересует, сколько произошло всего этого чего-то, вот это
[28:35.600 --> 28:42.640]  очень частая ситуация, ну начинают обычно примеры с физики, когда там радиоактивный распад, там
[28:42.640 --> 28:49.720]  какой-то датчик ловит частицы, и частицы редко попадают на экран, и там что-то загорается, но
[28:49.720 --> 28:56.240]  частиц очень много, и поэтому там сколько частиц там на каком-то интервале вы зафиксировали,
[28:56.240 --> 29:03.760]  это поассоновская случайная величина, как показывают эксперименты, там раньше там значит число
[29:03.760 --> 29:11.560]  телефонных вызовов из данного района, люди звонят куда-то редко, ну скажем так, ну не каждую секунду
[29:11.560 --> 29:18.520]  вы куда-то звоните, да, вы звоните редко, но людей много, и поэтому сколько за заданный интервал времени
[29:18.680 --> 29:25.080]  произошло звонков, это тоже поассоновская случайная величина, ну близка к ней, то есть это такая полезная
[29:25.080 --> 29:32.920]  модель, там где-нибудь в IT технологиях, что это может быть такое, какой-нибудь сайт, люди на него
[29:32.920 --> 29:39.800]  заходят редко, но людей много, опять же, и поэтому сколько людей зашло на сайт, это снова поассоновская
[29:39.800 --> 29:46.600]  величина, то есть удобно приближать, мы сейчас не будем говорить о том, насколько вообще адекватна
[29:46.600 --> 29:53.720]  такая модель в каждой конкретной ситуации, но я просто говорю, что вот такое часто бывает, можно много
[29:53.720 --> 30:01.480]  таких вот ситуаций себе вообразить, когда чего-то много, это происходит редко, и тогда, если опираться
[30:01.480 --> 30:07.440]  вот на этот закон редких событий, выходит, что сколько событий произошло, это поассоновская
[30:07.440 --> 30:14.120]  случайная величина, вот поэтому и естественно вот здесь вот эту вещь рассмотреть как поассоновскую
[30:14.120 --> 30:21.320]  случайную величину, то есть рассмотреть конкретно такую модель, итак, мы с вами имеем дело с потоком каких-то
[30:21.320 --> 30:26.800]  событий, которые происходят в разные моменты времени, в случайные, и мы рассматриваем счетчик
[30:26.800 --> 30:34.080]  числа событий на интервале 0t, начинаем его из нуля, пусть у него будут независимые приращения, пусть он
[30:34.080 --> 30:39.520]  будет однородный, и пусть эти вещи распределены по поассонам, тогда то, с чем мы имеем дело,
[30:39.520 --> 30:45.320]  называется поассоновским процессом, вот этот счетчик называется поассоновским процессом,
[30:45.320 --> 30:52.200]  давайте я теперь все, о чем я здесь поговорил и рассказал, соберу в одном месте и дам уже
[30:52.200 --> 31:15.800]  формальное определение, процесс k от t, k больше либо равно нулю, называется поассоновским, если первое
[31:15.800 --> 31:25.880]  k от 0 равно нулю, ну кстати говоря, если мы подставим сюда 0, мы получим сечение, а сечение это
[31:25.880 --> 31:31.560]  случайная величина, а как это случайная величина равна нулю, ведь случайная величина это функция,
[31:31.560 --> 31:38.240]  она зависит от исхода, вот она что, для всех исходов равна нулю или для каких-то, вот для почти всех,
[31:38.240 --> 31:56.000]  почти наверное, вот так вот, второе k от t это процесс с независимыми приращениями, то есть для
[31:56.000 --> 32:04.480]  любых n там больше либо равных, не знаю чего сейчас подумаю, для любых t1 меньше либо равно t2 меньше
[32:04.480 --> 32:17.480]  либо равно и так далее, меньше либо равно tn, значит k от t1, запятая k от t2 минус k от t1 и так далее,
[32:17.480 --> 32:33.680]  k от tn минус k от tn минус 1, ну наверное n больше либо равно 2, логично взять, вот эти вещи независимы
[32:33.680 --> 32:52.840]  в совокупности, вот так и третье, значит для любых t больше s больше либо равных 0, k от t минус k от s есть
[32:52.840 --> 32:58.680]  полосоновская случайная величина, ну и мы с вами рассмотрим простейший вид полосоновского процесса,
[32:58.680 --> 33:08.520]  когда здесь получается некая лямбда на t минус s, лямбда больше нуля это свободный параметр,
[33:08.520 --> 33:16.640]  вот который однозначно определяет процесс, вот такую вот ситуацию рассмотрим, видите,
[33:16.640 --> 33:22.560]  здесь распределение зависит только от разности между t и s, это полосоновская случайная величина,
[33:22.560 --> 33:31.280]  так что это однородный, процесс однородный, вот это мы будем называть полосоновским процессом,
[33:31.280 --> 33:42.160]  ну и, сколько там времени-то, сложно сказать, да, а нет, еще время есть, допережило, ну давайте
[33:42.160 --> 33:49.600]  изучать свойства этого процесса, сегодня все оставшееся время будем изучать его свойства,
[33:49.600 --> 33:57.000]  свойств у него огромное количество, на самом деле мы будем чуть ли не весь семестр изучать его
[33:57.000 --> 34:01.560]  свойства одного только этого процесса, но оно стоит того, потому что это действительно вещь,
[34:01.560 --> 34:11.640]  так, я вот может быть только вот эту вещь оставлю, пусть она тут где-нибудь написана, вот, то есть это
[34:11.640 --> 34:21.760]  когда x имеет полосоновское распределение с параметром лямбда, ну какие свойства можно назвать,
[34:21.760 --> 34:27.800]  значит, смотрите, которые следуют только из вот этих трех, трех свойств, трех вот этих вещей,
[34:27.800 --> 34:38.560]  ну с чего начать, ну, во-первых, смотрите, ну, давайте так, свойства, свойства, свойства
[34:39.200 --> 34:53.320]  полосоновского процесса, ну, k от t, это есть k от t минус k от нуля, да, потому что k от нуля ноль,
[34:53.320 --> 35:02.160]  почти всюду, а вот эта разность, она имеет распределение полосона, вон какое, это полосон с параметром
[35:02.160 --> 35:12.560]  лямбда t, t минус ноль, это t, вот, так что мы нашли распределение сечения, вот, а раз мы нашли
[35:12.560 --> 35:17.840]  распределение сечения, одномерную функцию распределения можно записать для этой вещи,
[35:17.840 --> 35:23.960]  мы можем найти все численные характеристики, которые связаны с одномерным распределением,
[35:23.960 --> 35:29.800]  например, математическое ожидание k от t, если мы t зафиксируем, мы получаем случайную
[35:29.800 --> 35:35.400]  величину вот с таким вот распределением, ну, какие у нее там свойства, давайте,
[35:35.400 --> 35:40.920]  я тут напишу, математическое ожидание вот такой кси, это есть вот эта лямбда, и она, кстати,
[35:40.920 --> 35:50.440]  дисперсия равна, вот, так что и здесь получается, что это есть то, что стоит под полосоном, у нас
[35:50.440 --> 35:56.200]  под полосоном стоит лямбда t, обратите внимание, так что это лямбда t, и она равна дисперсии,
[35:56.200 --> 36:08.440]  дисперсия k от t, значит, в среднем значения процесса растут, дисперсия тоже растет, вот,
[36:08.440 --> 36:16.000]  дальше k от t, это по ассоциативной величине, по ассоциативной величине принимает значение 0,
[36:16.000 --> 36:24.600]  1, 2 и так далее, значит, k от t принимает значение 0, 1, 2 и так далее, вот, более того, для любых t
[36:24.600 --> 36:30.600]  больше s, вот это приращение тоже принимает, получается, значение 0, 1, 2 и так далее,
[36:30.600 --> 36:36.800]  потому что она является по ассоциативной величиной, то есть, они все не отрицательны,
[36:36.800 --> 36:44.320]  вот эти приращения, а это значит, что процесс всюду не убывает, каков бы вы интервал не рассмотрели,
[36:44.320 --> 36:50.920]  приращение этого процесса на этом интервале будет либо 0, либо 1, либо 2 и так далее, но не может
[36:50.920 --> 36:57.840]  быть отрицательным, значит, все траектории этого процесса растут, и так как они дискретны,
[36:57.840 --> 37:05.920]  0, 1, 2 и так далее, не может быть промежуточных значений, но это значит, что траектории процесса это
[37:05.920 --> 37:10.960]  какие-то кусочно постоянные, оно сюда идет, потом в какой-то случайный момент времени происходит
[37:10.960 --> 37:18.360]  скачок, например, на единицу, потом постоянно какое-то случайное время, потом снова скачок,
[37:18.360 --> 37:25.760]  допустим, тоже на единицу и так далее, вы, кстати, обратите внимание, что траектории у процесса это
[37:25.760 --> 37:31.720]  кусочно постоянные функции, не убывающие, а математическое ожидание, непрерывная функция,
[37:31.720 --> 37:39.840]  то есть аналог какая-то такая, вот математическое ожидание лямбда t, а сами траектории вот они
[37:39.840 --> 37:47.880]  такие вот дискретные, то есть надо сразу же себе запомнить, что вид траекторий никак не связан
[37:47.880 --> 37:54.800]  с видом математического ожидания там дисперсии, то есть по-всякому может быть, здесь мы видим,
[37:54.800 --> 38:00.000]  что здесь ступенька, какие-то разрывные функции, какая-то разрывная функция траектория,
[38:00.000 --> 38:06.520]  а мат. ожидания вполне себе хорошая, непрерывная, даже гладкая, сколько угодно дифференцируемая
[38:06.520 --> 38:17.160]  функция, вот она в данном случае возрастает. Так, ну вот, что еще можно здесь сказать,
[38:17.160 --> 38:31.720]  давайте попробуем вычислить корреляционную функцию этого процесса, корреляционную
[38:31.720 --> 38:39.960]  функцию, значит, я вот здесь начну, корреляционная функция плацсонского процесса, то есть что мы
[38:39.960 --> 38:45.200]  должны сделать, мы просто пишем определение, ну а что, мы же больше ничего не знаем, пишем
[38:45.200 --> 38:57.600]  определение, мат. ожидания КАТ на КАТС минус мат. ожидания КАТС, ну давайте перерыв,
[38:57.600 --> 39:05.440]  я потом досчитаю, я все равно не начал. Так, ребята, давайте продолжать,
[39:05.440 --> 39:16.120]  возвращаемся к расчету, мы с вами вычисляем корреляционную функцию плацсоновского процесса,
[39:16.120 --> 39:23.680]  вот это есть ковариация его сечений КТ и КС, которые мы будем сейчас считать, ну мы уже
[39:23.680 --> 39:31.760]  мат. ожидания посчитали, вот это есть лямбда Т, это есть лямбда С, так что в принципе это мат.
[39:31.760 --> 39:42.640]  ожидания КТ на КАТС и минус лямбда в квадрате ТС, вот, нам нужно посчитать только вот это,
[39:42.640 --> 39:51.520]  мат. ожидания вот этого произведения, ну как мы будем это делать, в каждом случае это как бы
[39:51.520 --> 39:56.520]  своя ситуация, хотя если процесс имеет независимые приращения, то тут подход он
[39:56.520 --> 40:12.080]  единый, значит, давайте мы рассмотрим сначала ситуацию, когда Т больше С, тогда мат. ожидания КАТ КАТС,
[40:12.080 --> 40:23.520]  мы можем записать так, мы можем вот сюда, где КАТ добавить минус КАТС, то есть минус КАТС плюс
[40:23.520 --> 40:36.480]  КАТС вот к этому добавить, такой умный ноль, да, мат. ожидания КАТ минус КАТС КАТС и получается плюс
[40:36.480 --> 40:46.320]  мат. ожидания К в квадрате от С, вот, поняли, что я сделал, потому что как зависит между собой
[40:46.320 --> 40:52.800]  сечение, мы сейчас не понимаем, но мы зато знаем, что приращения независимы, поэтому для них
[40:52.800 --> 40:59.960]  мат. ожидания произведения приращений равно произведению мат. ожиданий этих приращений, вот,
[40:59.960 --> 41:07.640]  так что очень логично и удобно переходить к приращениям, вот здесь мы перешли, вот я перешел
[41:07.640 --> 41:15.840]  к приращению КАТС, это на самом деле тоже приращение, это есть КАТС минус КАТ0 и получается,
[41:15.840 --> 41:24.040]  что у нас есть интервал 0С и СТ, вот они не пересекаются, так что это приращения на
[41:24.040 --> 41:29.840]  не пересекающихся интервалах, мы смотрим на пункт 2, они независимы, значит, мат. ожидания
[41:29.920 --> 41:37.520]  произведения приращения равно произведению мат. ожиданий, вот, мат. ожидания вот этого
[41:37.520 --> 41:42.760]  приращения, ну а что это за приращение, это же есть원 пла són с параметром лямбда t
[41:42.760 --> 41:52.080]  минус С, а его мат. ожидания это лямбда на t минус S. умножить на мат. ожидания вот этого
[41:52.080 --> 41:58.200]  приращения, а это есть по осон с параметром лямбда С, его мат. ожидание и лямбда С.
[41:58.200 --> 42:08.920]  умножить на лямбдс плюс здесь мотожидание квадрата а мотожидание квадрата это есть дисперсия
[42:08.920 --> 42:23.640]  плюс квадрат мотожидания правильно дисперсия ну так как это по ассо над лямбдс дисперсию мы
[42:23.640 --> 42:33.600]  знаем это лямбдс вот это снова по ассо лямбдс мотожидание лямбдс и в квадрат возводим лямбда
[42:33.600 --> 42:45.920]  в квадрате с в квадрате все вычислили так что получается плюс плюс лямбдс и плюс лямда в
[42:45.920 --> 42:52.680]  квадрате с в квадрате вот это мы что вычислили это мы вот это мотожидание вычислили произведение
[42:52.680 --> 43:01.480]  вот потом ну вот здесь у нас даже что-то сокращается уже здесь да лямбда в квадрате
[43:01.480 --> 43:08.280]  с в квадрате сократиться с тем то есть мы можем сократить вот это здесь получится лямбда тс
[43:08.280 --> 43:20.000]  лямда в квадрате тс плюс лямбдс когда мы подставим это сюда и вычтем лямда в квадрате тс то
[43:20.000 --> 43:30.880]  мы получим что р к тс равняется лямбдс вот это лямбдс она единственная выживет здесь мы рассмотрели
[43:30.880 --> 43:36.880]  случай т больше с но у нас здесь полная симметрия если мы рассмотрим с если с больше
[43:36.880 --> 43:48.280]  т будет то тогда мы получим лямбдс то есть лямбдс когда у нас т больше с лямбдс когда у нас
[43:48.280 --> 43:56.920]  т меньше с вот но если мы рассмотрим где-то равенство то в общем-то это тоже ничему не будет
[43:56.920 --> 44:02.800]  противоречить хотя там у нас по сонот нуля будет нехорошо да ну отдельно можно рассмотреть
[44:02.800 --> 44:10.480]  случай t равняется с давайте отдельно рассмотрим то есть а что такое отдельно смотрите р к т т а
[44:10.480 --> 44:17.160]  мы же знаем что дисперсия правильно у нас было такое свойство для колоритационной функции а это
[44:17.160 --> 44:22.280]  есть лямбда т так что на самом деле здесь можно где-то даже равенство написать как-нибудь вот так
[44:22.280 --> 44:30.680]  вот ну и это можно написать в компактном виде смотрите вот какая буквы здесь стоит та которая
[44:30.680 --> 44:35.600]  меньше здесь с меньше значит она стоит здесь ты меньше значит она стоит когда равенство неважно
[44:35.600 --> 44:49.200]  так что это есть лямбда на минимум т и с это есть корреляционная функция процесса по осуна вот
[44:49.200 --> 45:02.320]  рк т с это лямбда на минимум т и с эта функция она определена всюду на вот этом множестве 0
[45:02.320 --> 45:10.160]  плюс бесконечность в квадрате вот то есть она если я нарисую здесь тест вот при всех вот этих вот
[45:10.160 --> 45:20.120]  она определена во всех точках она рана лямбда минимум т с если график ее посмотреть трехмерный
[45:20.120 --> 45:28.280]  получается до каждой точке составляется число пирамида такая будет вот пирамида у которой ребро
[45:28.280 --> 45:37.000]  идет вот здесь максимально пирамида то что это пирамида кстати важно на какой-то какой-то
[45:37.000 --> 45:44.960]  задачи это полезное наблюдение потому что когда считаешь допустим интеграл двукратный от этой
[45:44.960 --> 45:50.820]  штуки то двукратный интеграл такой функции это объем объем пирамида легко посчитать это там вот
[45:50.820 --> 45:57.800]  это на это и там на высоту тривиально считается просто обратите на это внимание через такое
[45:57.800 --> 46:06.400]  практическое практически техническое замечание по поводу этой функции так ну вот мы вычислили
[46:06.400 --> 46:20.360]  корреляционную функцию плацонского процесса вот значит ну едем тогда дальше вот эти свойства
[46:20.360 --> 46:28.040]  которые здесь написаны они на самом деле однозначно определяют семейство конечномерных
[46:28.040 --> 46:34.200]  распределений то есть пользуясь этими тремя свойствами можно найти функцию вероятность или
[46:34.200 --> 46:44.400]  функцию распределения для вектора составленного из сечений этого процесса вот но я не буду это
[46:44.400 --> 46:50.280]  делать в общем-то это технически такая большая работа я просто скажу как это вообще делается
[46:50.280 --> 46:55.800]  смотрите тут все на самом деле тривиально мы не знаем как распределенные сечения между собой
[46:55.800 --> 47:03.760]  собственно это наша задача это узнать находя вот это семейство распределений но мы знаем что вот
[47:03.760 --> 47:12.840]  эти независимы и мы знаем как они распределены значит вектор составленный из сечений процесса
[47:12.840 --> 47:19.700]  мы знаем его распределения потому что вероятность того что этот равен чему-то этот
[47:19.700 --> 47:24.940]  равен чему-то и так далее равно произведению вероятности что это прав一 чему то это прав
[47:24.940 --> 47:30.200]  на чему ты так далее потому что они независимы вот а чему равные вероятности для каждого
[47:30.200 --> 47:34.880]  из них мы знаем потому что мы знаем это распределение так что мы знаем как распределяли
[47:34.880 --> 47:44.960]  векторы из приращений. Ну а просто сечение процесса, это некое линейное
[47:44.960 --> 47:49.360]  преобразование вот такого вектора. Ну и в общем-то перейти от одного к другому,
[47:49.360 --> 47:55.000]  это чисто технический момент, он не принципиален. Главное, что тут надо
[47:55.000 --> 47:59.920]  понимать, что если у вас задано распределение вот этого вектора для
[47:59.920 --> 48:07.720]  произвольных n, произвольных t, то вы можете, воспользуясь этим преобразованием
[48:07.720 --> 48:11.920]  линейным, перейти уже к распределению вектора. Эта операция, она вполне себе
[48:11.920 --> 48:18.280]  однозначная, формула есть, все дела. Вот, то есть вот эти свойства, они действительно
[48:18.280 --> 48:22.440]  однозначно определяют семейство конечномерных распределений. Я говорил
[48:22.440 --> 48:26.440]  на прошлой лекции, что мы не будем задавать процессы как функции исхода и
[48:26.440 --> 48:30.200]  времени, мы будем их определять через конечномерные, семейство конечномерных
[48:30.200 --> 48:33.760]  распределений. Ну конечно, это не значит, что мы будем это семейство явно
[48:33.760 --> 48:39.160]  выписывать каждый раз. Вот в данном случае достаточно задать такие вот
[48:39.160 --> 48:44.280]  аксиоматические свойства, то есть задать процесс аксиоматическим образом. И
[48:44.280 --> 48:49.000]  получается для него, что семейство конечномерных распределений по нему
[48:49.000 --> 48:55.400]  восстанавливается однозначно. Вот, окей. Дальше едем.
[48:55.400 --> 49:00.840]  Какие еще свойства мы можем получить и вообще какие свойства нам интересны
[49:00.840 --> 49:06.440]  для этого процесса? Вот мы знаем, что он скачет. Он сначала постоянен, потом
[49:06.440 --> 49:12.680]  скакнул. Постоянен, скакнул. А на сколько он скачет? На 1, на 2? Может ли он
[49:12.680 --> 49:18.720]  скакать больше, чем на 1? На сколько он в среднем скачет? Дальше. Я говорю о
[49:18.720 --> 49:23.480]  том, что скачки происходят в случайный момент времени. А как распределены эти
[49:23.480 --> 49:29.960]  моменты времени? Их распределение каково? А вот эти интервалы между скачками, вот
[49:29.960 --> 49:34.040]  эти длинные интервалы, они никакое распределение имеют? Являются ли они
[49:34.040 --> 49:39.040]  зависимыми или независимыми эти интервалы? В общем-то много вопросов возникает
[49:39.040 --> 49:44.640]  про этот процесс. И в принципе, только исходя из этих определений, можно дать
[49:44.640 --> 49:48.840]  ответа на все эти вопросы. Рассматриваю их, правда, по отдельности. Отдельно
[49:48.840 --> 49:53.760]  рассмотрим вопросы о том, насколько скачет процесс. Отдельно рассмотрим вопрос
[49:53.760 --> 49:56.840]  о том, в какие моменты происходят скачки и так далее.
[49:56.840 --> 50:02.280]  Но вот мне нравится больше другой подход. На самом деле, мы сейчас с вами
[50:02.280 --> 50:09.120]  рассмотрим и докажем одну теорему, из которой вот эти все свойства, они сразу
[50:09.120 --> 50:14.520]  одним махом следуют. То есть мы сразу же ответим на все эти вопросы, одним махом
[50:14.520 --> 50:21.800]  доказав всего одну теорему. Вот. Эта теорема называется теоремой о явной
[50:21.800 --> 50:26.720]  конструкции Буасоновского процесса.
[50:28.840 --> 50:35.080]  Это первая, по-моему, наша с вами, да, первая с вами теорема, которую мы будем
[50:35.080 --> 50:40.600]  доказывать. Она большая, мы будем ее доказывать все оставшееся время, наверное,
[50:40.600 --> 50:42.600]  но
[50:43.400 --> 50:51.120]  там много и технических деталей, о которых полезно вспомнить теорию вероятности тоже.
[50:51.120 --> 51:16.720]  Так, теорема. Явная конструкция Буасоновского процесса. Значит, так, пусть
[51:17.720 --> 51:35.320]  кси n это независимые случайные величины с распределением показательным параметром
[51:35.320 --> 51:46.240]  лямбда. Вот. Ну, то есть, функция плотности f кси n, у них у всех одинаково, она
[51:46.240 --> 51:56.440]  равняется лямбда на е в степени минус лямбда х, при х больше либо равных ноль. Вот.
[51:56.440 --> 52:02.640]  Пусть дана последовательность независимых, ну, в совокупности, естественно, в
[52:02.640 --> 52:07.680]  совокупности. Ну, я, если не говорю, как именно независимые, то в совокупности у
[52:07.680 --> 52:13.080]  нас никогда с вами не будет попарных независимости, если мы говорим о
[52:13.080 --> 52:18.440]  независимости. И вообще не только у меня, а в любых других книжках, учебниках, если не говорят,
[52:18.440 --> 52:22.840]  о какой независимости идет речь, всегда подразумевают независимость в совокупности.
[52:22.840 --> 52:37.040]  Сразу запомните это. Независимость в совокупности случайные величины. Вот. И обозначим, обозначим
[52:37.040 --> 52:49.320]  s, n, sumo их до n. Вот. Ну и пусть для определенности s0 равняется нулю, потому что здесь у нас n от единицы,
[52:49.320 --> 53:05.960]  нам будет удобно s0 задать как ноль. Вот. Тогда, тогда процесс вот такой, x от t, который равен
[53:05.960 --> 53:18.560]  supremo при n больше либо равных единице n таких, что sumo k равно от единицы до n в секатах меньше
[53:18.560 --> 53:26.040]  либо равна t, ой, а, ну да, то есть s, n меньше, меньше либо равно t, можно так, можно всяко написать.
[53:26.040 --> 53:42.320]  Да, вот этот процесс, то есть тут вот s, n является по осоновским процессом с параметром лямда,
[53:42.320 --> 53:46.160]  тем самым лямда, которая вот здесь в показательном распределении находится,
[53:46.160 --> 53:58.040]  процессом с параметром лямда, вот, то есть его семейство конечномерных распределений совпадает
[53:58.040 --> 54:03.720]  семейством конечномерных распределений процесса, которые определяется вот таким образом, раз
[54:03.720 --> 54:08.360]  семейство конечномерных распределений совпадает, нам неважно с чем мы имеем дело, с этой конструкцией
[54:08.360 --> 54:15.040]  или с той конструкцией, это одно и то же. С точки зрения нашего интереса вероятностными
[54:15.040 --> 54:23.480]  свойствами. А я вам напоминаю, что нас только это интересует. Вот, мы будем доказывать эту теорему
[54:23.480 --> 54:33.440]  доказательства. Значит, смотрите, скетч доказательства следующий. Как мы будем это делать?
[54:33.440 --> 54:38.720]  Для того, чтобы доказать, что это полуслуженский процесс, а у нас кроме этого определения больше
[54:38.720 --> 54:45.160]  ничего нет. Значит, нам надо что сделать? Нам нужно взять вот эту конструкцию x от t и просто
[54:45.160 --> 54:51.240]  проверить все пункты в этом определении. Если они все будут выполнены, то тогда это полуслуженский
[54:51.240 --> 54:56.120]  процесс. Как я говорил, вот эта вещь однозначно определяет семейство конечномерных распределений.
[54:56.120 --> 55:01.800]  Все. Так что, если этот процесс удовлетворяет этим свойствам, значит, у них одинаковый
[55:01.800 --> 55:07.680]  семейство конечномерных распределений. Значит, это полуслуженский процесс. Вот. Так что мы будем
[55:07.680 --> 55:15.600]  для этого процесса проверять вот эти свойства. Так. Ну, там, в общем-то, тривиальные. Напустим,
[55:15.600 --> 55:23.000]  если мы возьмем t равное нулю, x от нуля рассмотрим, здесь будет 0. Ну, тогда Sn должно быть равна нулю.
[55:23.000 --> 55:28.200]  То есть, значит, самый наибольший n, когда это возможно, n равняется нулю. Значит, x от нуля
[55:28.200 --> 55:34.320]  равняется нулю. То есть, вот этот первый случай, он тривиально следует из просто из определения вот
[55:34.320 --> 55:41.280]  этой вещи. Вот. Так что с первым пунктом так-то понятно. Но самое сложное — это доказать вот эти
[55:41.280 --> 55:47.400]  два пункта. То есть, проверить, что у такого процесса с супремумом каким-то. То есть, посмотрите, x от
[55:47.400 --> 55:53.520]  t — это супремум, а нам нужно приращение того процесса. Приращение с супремумом, в общем, такая вещь,
[55:53.520 --> 55:59.920]  это нетривиальная. Вот. Нам надо доказать, что они независимы в совокупности. И еще определить,
[55:59.920 --> 56:05.920]  что приращение имеет вот это распределение. Значит, как мы с вами поступим? Как будем доказывать эту
[56:05.920 --> 56:14.520]  теорему? Первое, что мы сделаем — это мы определим, как распределены snt. Я имею в виду не каждое snt
[56:14.520 --> 56:23.240]  по отдельности, а вектор из snt сначала определим. Это будет первый шаг в доказательстве. А второе — мы
[56:23.240 --> 56:33.160]  выпишем приращение вот этого процесса, составим вектор из этих приращений и найдем это распределение,
[56:33.160 --> 56:43.560]  вектор из приращений, в терминах sn. То есть, выразим приращение x через sn. И, зная распределение
[56:43.560 --> 56:51.600]  для sn, мы найдем распределение векторов приращения x. И мы увидим, что это распределение расщепится.
[56:51.600 --> 56:58.840]  Вот эта большая вероятность, она расщепится на n произведений, как и должно быть. Потому что мы
[56:58.840 --> 57:05.560]  доказываем независимость в совокупности. И каждый множитель там имеет ровно такое распределение,
[57:05.560 --> 57:12.320]  какое надо. Пуассоновское с вот этим вот параметром. То есть, вот наш будет подход. Итак, ищем,
[57:12.320 --> 57:21.080]  как распределен вектор из snt. Пишем вероятность, функцию вероятности для приращения x и выражаем
[57:21.080 --> 57:26.280]  ее в терминах sn. И пользуемся вот этой нашей конкретикой. То, что sn это не какая-то произвольная
[57:26.280 --> 57:31.440]  случайная величина, она получена в результате сумм вот этих величин. То есть, мы этой конкретикой
[57:31.440 --> 57:38.000]  воспользуемся. И путем некоторых преобразований там, короче, сначала будет огромное выражение на всю
[57:38.000 --> 57:43.280]  доску, потом она просто схлопнется и получится красивое короткое выражение в самом конце. Вот и все.
[57:43.280 --> 57:49.000]  Вот это вся идея. То есть, идея сама по себе очень простая. Ну, нужно вот напрячься немножечко,
[57:49.000 --> 57:56.680]  чтобы нигде не запутаться и провести от начала до конца, чисто технически. Итак, ну давайте по
[57:56.680 --> 58:02.800]  порядку. Давайте мы не будем бросаться сразу в омут. Здесь много чего предстоит делать. Давайте
[58:02.800 --> 58:09.120]  разберемся со snt. Значит, смотрите, кси, независимые в совокупности, имеют показательное распределение.
[58:09.120 --> 58:15.320]  Сразу сумма, говорим, она по ирлангу распределена. Помним, да, ирланг, n лямбда — это распределение
[58:15.320 --> 58:24.560]  суммы для вот этих кси. Окей. Как связаны ски с кси? Смотрите, ведь мы знаем, что они независимо
[58:24.560 --> 58:29.320]  распределены и знаем их распределение. Поэтому, чтобы найти распределение ски, то есть, вектора из
[58:29.320 --> 58:34.640]  с, нам хочется как-то связать их распределение с распределением кси. То есть, это все, что мы знаем.
[58:34.640 --> 58:42.920]  Вот смотрите, если мы напишем s1, s2 и так далее, sn, вот этот вектор осмотрим, то он связан кси
[58:42.920 --> 58:49.960]  линейным образом. Это будет единичка, нолик, нолик и так далее, единичка, единичек, нолик и так
[58:49.960 --> 58:55.320]  далее, единичка, единичка, единичка, нолик и так далее, и так далее, единица, так далее,
[58:55.320 --> 59:08.440]  единица и так далее, единица в самом конце. Вот. Согласны? Это то, как связан вектор
[59:09.040 --> 59:19.680]  с вектором кси. Так. Отлично. Теперь смотрите. Вот этот вектор. Мы знаем плотности каждой компоненты.
[59:19.680 --> 59:26.400]  Вон она там выписана. Они независимы. Значит, плотность всего этого вектора равна произведению
[59:26.400 --> 59:32.480]  плотностей всех их. Так? Здесь осуществляется линейное преобразование. В теории вероятностей
[59:32.480 --> 59:37.880]  была формула, по которой мы можем вычислить плотность этого вектора, если осуществляется
[59:37.880 --> 59:42.680]  некоторое преобразование здесь. Кстати, не обязательно линейное. Вот. Ну, в нашем случае даже
[59:42.680 --> 59:48.200]  еще проще. Здесь линейное преобразование. То есть, я напоминаю вот в таких вот квадратных скобках,
[59:48.200 --> 59:59.240]  что, значит, если у нас... Сейчас. Тут мне самому не запутаться. Значит, если у нас есть вектор y
[59:59.240 --> 01:00:11.640]  и он равнеется a на x, вот, то тогда плотность вектора y в точке y это что такое? Это плотность
[01:00:11.640 --> 01:00:18.800]  x в точке в какой? Мы бы хотели написать x, но мы хотим выразить все равных y. Но x равен a в минус 1
[01:00:18.800 --> 01:00:27.160]  на y, поэтому мы пишем a в минус 1 на y. И здесь единственное нужно умножить на модуль определителя
[01:00:27.160 --> 01:00:39.680]  вот этого. Не терминант, а в минус 1. Вот. Мы будем пользоваться вот этим способом, вот этой формулой.
[01:00:39.680 --> 01:00:52.920]  Так что... Давайте я там сотру. Особо не пригодится. Чего? Ну, теорема такая.
[01:00:52.920 --> 01:01:06.240]  Что надо на модуль умножать. Ну, не быть модуля не может, потому что детерминант может
[01:01:06.240 --> 01:01:10.360]  отрицательно наставить, а функция плотности не может быть отрицательной. Ну, это, конечно,
[01:01:10.360 --> 01:01:16.560]  не объяснение, почему там модуль, но просто как бы модуль должен быть. Вот. Теорема такая,
[01:01:16.560 --> 01:01:23.640]  можно посмотреть, как она доказывается. Там стоит модуль. Так, хорошо. Ну и давайте теперь найдем плотность
[01:01:23.640 --> 01:01:35.880]  s. Нам же это надо найти. Плотность s в точках, допустим, x1 и так далее, xn. Вот. Это что такое? Это
[01:01:35.880 --> 01:01:47.680]  есть плотность вот этих векторок си. Где? А в минус 1, в данном случае, а в минус 1 умножится
[01:01:47.680 --> 01:01:56.160]  вот на этот вектор. Ну, то есть нам нужно выразить си через s, получается. Здесь у нас будет x1. Ну,
[01:01:56.160 --> 01:02:04.360]  обратная матрица, она же какая? 1, все нули, минус 1, 1, все нули, 0, минус 1, 1, все нули и так далее. Вот.
[01:02:04.360 --> 01:02:16.840]  То есть x2-x1. Смотрите на x как на, как на эти, как на наши s, как на наши суммы. Поэтому,
[01:02:16.840 --> 01:02:28.400]  чтобы получить x2, надо x2 вычислить s1. Вот. Ну, хорошо. Вот. xn-xn-1. Вот. Это мы написали вот это.
[01:02:28.400 --> 01:02:36.040]  Ну, детерминат этой матрицы равен единице. Детерминат обратной матрицы это единица разделит
[01:02:36.040 --> 01:02:43.000]  на детерминат исходной матрицы. То есть это единица. Поэтому модуль детерминат а в минус 1 это единица.
[01:02:43.000 --> 01:02:51.480]  И вот так вот связаны плотности с икси. Вот таким вот образом. Мы знаем, какова плотность икси. Это
[01:02:51.480 --> 01:03:02.640]  есть произведение по всем и от единицы до n. Значит, чего? Лямда умножить на e в степени минус лямда.
[01:03:02.640 --> 01:03:14.800]  Умножить на то, что тут стоит. Это x и минус x и минус первое. Вот. И нам не нужно забывать,
[01:03:14.800 --> 01:03:21.040]  что наша плотность определена, когда этот документ больше либо равен нуля. Это равносильно тому,
[01:03:21.160 --> 01:03:26.680]  что мы запишем вот это экспоненты умножить на индикаторную функцию того, что x больше либо равен нуля.
[01:03:26.680 --> 01:03:31.760]  Если x отрицательный там ноль, x положительно это будет лямда экспонента. Поэтому мы вот тут
[01:03:31.760 --> 01:03:39.360]  напишем индикаторную функцию x и t больше x и t минус первое. Вот. Таким вот компактным образом,
[01:03:39.360 --> 01:03:45.280]  чтобы без этих фигурных скобок, если то там. Вот. Можно компактно записать это вот таким образом.
[01:03:45.280 --> 01:03:55.480]  Вот. Ну а теперь что? Лямду выносим как лямду в степени n. Здесь у нас,
[01:03:55.480 --> 01:04:00.800]  смотрите, произведение экспонента. Это экспонента суммы. Здесь телескопическая сумма. И выживают все,
[01:04:00.800 --> 01:04:09.880]  точнее, умирают все кроме последнего, x и n. Получается e в степени минус лямда x и n. Вот. А здесь
[01:04:09.880 --> 01:04:14.560]  произведение индикаторов, то есть все вот эти условия должны быть выполнены. И их можно
[01:04:14.560 --> 01:04:22.920]  заменить на один индикатор. Индикатор того, что, давайте так напишем, ноль меньше x1,
[01:04:22.920 --> 01:04:29.560]  меньше x2, меньше и так далее, меньше xn. Вот. Все вот эти индикаторы можно записать как один
[01:04:29.560 --> 01:04:40.000]  индикатор. Вот. Вот мы нашли функцию плотности для s. Она имеет вот такой вид. Ну выглядит,
[01:04:40.000 --> 01:04:46.920]  мягко скажем, не очень. Ну что поделать? Зато это правильно. Выглядит сложно, зато это правильно.
[01:04:46.920 --> 01:04:57.160]  Так что нам предстоит с этим работать. Окей. Следующий шаг делаем. Теперь мы рассмотрим вот
[01:04:57.160 --> 01:05:03.440]  такую вероятность. Мы рассмотрим вероятность. Значит, а, ну, кстати говоря, тут мы для удобства,
[01:05:03.440 --> 01:05:09.720]  видите, тут у меня и от единицы, тут есть x0, а кто такой x0? x0 у меня здесь не было. Ну,
[01:05:09.720 --> 01:05:18.400]  просто мы взяли x0, равный нулю. Просто для того, чтобы вот эти обозначения, они все были
[01:05:18.400 --> 01:05:25.400]  совпадающими. Согласованными, скажем так. Чтобы или симметричными, чтобы одинаковая, одинаковая
[01:05:25.400 --> 01:05:37.080]  запись была. Теперь мы рассмотрим вероятность того, что x от t1 равняется, так, вот я не помню,
[01:05:37.080 --> 01:05:45.840]  как тут проще. Нет, тут, наверное, через k. Да, вот так мы запишем k1. x от t2 минус x от t1
[01:05:45.840 --> 01:06:00.000]  равняется k2 минус k1 и так далее, до x от tn минус x от tn минус 1 равняется kn минус kn минус 1.
[01:06:00.000 --> 01:06:05.760]  Вот мы рассмотрим вот такую вероятность. На самом деле, мы могли здесь написать какой-нибудь k1,
[01:06:05.760 --> 01:06:13.760]  k2, k3 и так далее, до kn, где они произвольные, принимают значение 0, 1, 2 и так далее. Но,
[01:06:13.760 --> 01:06:22.160]  по-моему, нам будет удобно, если мы введем вот эти значения сюда именно таким образом. Но
[01:06:22.160 --> 01:06:29.120]  только потребуем, чтобы больше a был больше, чем меньше k. То есть, чтобы было 0, меньше либо
[01:06:29.120 --> 01:06:35.360]  равно k1, меньше либо равно k2, меньше либо равно и так далее, меньше либо равно kn. И пусть 0 это
[01:06:35.360 --> 01:06:40.800]  будет наш k0. Нам тоже будет удобно такого обозначения вести. И мы рассмотрим вот эту вещь
[01:06:40.800 --> 01:06:51.240]  для произвольных k, произвольных n, вот таких. И нам нужно показать, что вот эта вероятность равна
[01:06:51.240 --> 01:06:57.840]  произведению вероятностей вот этих вот равенств, которые написаны здесь. То есть, что оно расщепится
[01:06:57.840 --> 01:07:02.880]  все. И более того, нам надо показать, что вероятность приращения, она там такая какая надо,
[01:07:02.880 --> 01:07:10.320]  чтобы было поасоновское распределение. Вот. И давайте мы вот что сделаем. Мы возьмем вот это
[01:07:10.320 --> 01:07:19.640]  событие, которое здесь написано, и выразим его в терминах s. Сум. Вот этих именно s. Ну,
[01:07:19.640 --> 01:07:37.280]  как мы это сделаем? Я вот тут сотру. Это нам уже не нужно абсолютно. Ну, смотрите, тут все просто.
[01:07:37.280 --> 01:07:49.520]  Начинаем от нуля. Теперь смотрите, что означает, что xt1 равняется k1. Это значит,
[01:07:49.520 --> 01:08:00.560]  что вот этот supremum равен k1. Это значит, что максимальный номер в этой сумме, чтобы оно еще
[01:08:00.560 --> 01:08:11.920]  не превосходило t, это k1. Если мы возьмем k1 плюс 1, вот эта сумма станет больше t. Понятно? И мы уже
[01:08:11.920 --> 01:08:24.320]  выйдем за нее. Так что вот это событие xt1 равняется k1. Оно означает вот что. Что у нас здесь есть t1,
[01:08:24.320 --> 01:08:40.160]  и что у нас сумма, которая отвечает k1, она меньше t1. А вот следующее за ней sk1 плюс 1. Плюс 1 к k1
[01:08:40.160 --> 01:08:46.720]  прибавляется. Не к индексу единицы прибавляется, а к этому k прибавляется. Вот она идет дальше. Вот.
[01:08:46.720 --> 01:08:53.360]  Обратите внимание. Вот если вы этот момент поймете, то все нормально. Обратите на это внимание.
[01:08:53.360 --> 01:09:00.400]  Супремум берется. А вот тут стоят оценки. Вот это sn. Здесь максимальный номер, что он не
[01:09:00.400 --> 01:09:14.600]  превосходит t. Значит, а мы говорим, что xt1 равняется t1 равняется k1. Значит sk1 меньше ли бы равен t,
[01:09:14.600 --> 01:09:22.520]  а вот следующий за ним s уже будет больше равен. Значит, наибольший номер равен k1. xt1 равен k1.
[01:09:22.520 --> 01:09:31.640]  Вот. Значит, у нас все вот эти sk первые, они лежат вот в этом интервале, а следующий за ними
[01:09:31.640 --> 01:09:47.360]  лежит здесь. Дальше. Вот на это посмотрим. xt2 минус xt1. Он равен k2 минус k1. Вот. Это означает,
[01:09:47.360 --> 01:10:03.200]  что у нас все sk до sk2 лежат до t2, а следующие за ним это уже k2 плюс 1. Вот. Это уже событие,
[01:10:03.200 --> 01:10:12.600]  которое заключается в том, что xt1 равен k1, а xt2 минус xt1 равен k2 минус k1. Вот. Но это еще
[01:10:12.600 --> 01:10:18.000]  равносильно тому, что, видите, если xt1 равен k1, мы можем вместо него сюда подставить, сократить,
[01:10:18.000 --> 01:10:25.640]  и получить, что xt2 равняется k2. И логика та же самая, что до k2 они все сюда входят, а вот следующий
[01:10:25.640 --> 01:10:33.200]  за ним уже в этот интервал не входит. Он лежит правее от t2. Ну и так далее. И так далее до конца.
[01:10:33.200 --> 01:10:41.080]  Так что вот эта вся вероятность, вот эта вся вероятность равно. Смотрите, чему это равно все.
[01:10:41.080 --> 01:10:51.040]  Это есть вероятность того, что все вот эти s попали вот сюда. Я вот так в фигурных скобках
[01:10:51.040 --> 01:11:00.920]  нарисую. s1 и так далее, sk1 принадлежат интервалу 0, t1. Они имеют непрерывное распределение. Это
[01:11:00.920 --> 01:11:05.280]  неважно. Принадлежит, точка не принадлежит, это неважно. Это все непрерывное распределение имеет.
[01:11:05.280 --> 01:11:10.240]  Поэтому я не обращаю внимания здесь, какую надо квадратную скобку делать или круглую. Это вообще
[01:11:10.240 --> 01:11:27.640]  неважно. Вот. Запятая следующие за ними sk1 плюс 1 и так далее до sk2 принадлежит t1, t2. Запятая
[01:11:27.640 --> 01:11:42.520]  и так далее. Ну вот тут, кстати, очень сложно. sk с индексом n-1 плюс 1 и так далее до skn принадлежит
[01:11:42.520 --> 01:11:52.560]  от tn-1 до tn. Вот как я и обещал, мы выразили вероятность, связанную с превращениями
[01:11:52.560 --> 01:12:02.400]  x в терминах s. А у s мы распределение знаем. У вектора s мы распределение знаем. Значит,
[01:12:02.400 --> 01:12:09.240]  мы знаем распределение s1, s2 и так далее до skn. А здесь нужно вычислить вероятность,
[01:12:09.240 --> 01:12:14.240]  связанную с этими s-ками. Так что понятно, что раз мы знаем распределение s-ок и нам
[01:12:14.240 --> 01:12:19.800]  нужно найти вероятность, связанную с ними, ну как-то это ее посчитать можно. Все. Дальше дело
[01:12:19.800 --> 01:12:27.040]  техники. Вычисления вот этой вероятности. Все самое идейное мы уже сделали. Давайте теперь
[01:12:27.040 --> 01:12:51.240]  вычислять вот эту вероятность. Это я могу стирать? Так. Ну что, давайте мы будем вычислять эту
[01:12:51.240 --> 01:13:00.160]  штуку. Ну, вероятность. Вот такая вот. Что такое вероятность? От того, что некий случайный
[01:13:00.160 --> 01:13:14.360]  вектор s1, s2, skn попал в какую-то область. Это есть интеграл по этой области от плотности. Так?
[01:13:14.360 --> 01:13:23.720]  Интеграл от плотности по области. Так. Ну вот будет такой большой-большой интеграл. Многократный.
[01:13:23.720 --> 01:13:39.360]  От чего? От плотности s в точках x1 и так далее, xn. А, смотрите, у нас сколько тут? Kn штук. Kn. О как. Здесь
[01:13:39.360 --> 01:13:50.480]  у нас будет стоять dx1 и так далее, dxkn. И по какой области? А вот тут она написана области. Значит,
[01:13:50.480 --> 01:14:02.640]  у нас теперь переменными являются иксы. Что x1 и так далее xk1 попал вот сюда. Что xk1
[01:14:02.640 --> 01:14:24.640]  плюс 1 и так далее xk2 попал вот сюда и так далее xkn минус 1 плюс 1 xkn попал вот сюда. Во! Вот мы
[01:14:24.640 --> 01:14:34.920]  вот по этой области интегрируем вот эту плотность. Ну что, страшно? Мне тоже, потому что я не помню,
[01:14:34.920 --> 01:14:41.680]  что делать дальше, но мы сейчас сообразим. Я помню, что только там все на самом деле не сложно. Так. Ну
[01:14:41.680 --> 01:14:47.760]  что нам, что нам, собственно, надо делать? Надо воспользоваться, наконец-то, конкретикой. Конкретика
[01:14:47.760 --> 01:14:57.160]  у нас для PS. А, я ее стер, да? Так. То есть нам нужно взять вместо... Ну давайте так, я напишу интегралы.
[01:14:57.160 --> 01:15:09.120]  Вот тут вот переписывается вот это. Вот. И какая у нас там была плотность? Вот черт. Значит там лямбда.
[01:15:09.120 --> 01:15:28.840]  Колька элементов это kn на e в степени минус лямбда. Последний из х, да? Последний из х. Так. И на
[01:15:28.840 --> 01:15:44.640]  индикатор того, что 0 меньше x1 меньше и так далее меньше xkn. Вот. dx1 и так далее dxkn. Так. Вот. Ну вы
[01:15:44.640 --> 01:15:56.440]  там проверьте. Я же писал формулу для плотности, по-моему, такая. Так. Ну вот. Хорошо. Только сейчас,
[01:15:56.440 --> 01:16:03.920]  я, по-моему, все-таки что-то забыл. Ща-ща-ща-ща. Да. Я еще одну вещь забыл в область дописать. Смотрите.
[01:16:03.920 --> 01:16:21.600]  Это касается tn tknt момента. Вот у нас там xtn равен xknt. Так. Это означает, что вот
[01:16:21.600 --> 01:16:36.560]  сюда попал xknt, а вот сюда попал xkn плюс один. Для того, чтобы это supremum был. То есть это
[01:16:36.560 --> 01:16:42.080]  должен быть максимальным. Значит следующий за ним должен превышать. Поэтому вот сюда нужно
[01:16:42.080 --> 01:17:01.880]  будет еще кое-что добавить. Что skn плюс один превышает tn во. Вот тогда xtn будет равен xkn. Вот так. Так что
[01:17:01.880 --> 01:17:11.400]  вот здесь нужно будет добавить xkn плюс один больше чем tn во. И вот теперь будет нам гораздо проще
[01:17:11.400 --> 01:17:26.560]  вычислять. Все вот это дело. Так. Ну хорошо. Так. Ну все. Начинаем упрощать. Хватит. А то у нас тут
[01:17:26.560 --> 01:17:31.760]  что-то все растет и растет. Да. И не видно этому ни конца ни края. Давайте упрощать. Значит вот это все
[01:17:31.760 --> 01:17:40.640]  равно. Это равно. Я вот тут вот подальше начну. Вот тут вот. Теперь смотрите. Как упрощать будем?
[01:17:40.640 --> 01:17:53.240]  Ну. Во-первых. А кстати. А у меня что-то нету. xk. Сейчас-сейчас-сейчас-сейчас. xk. n плюс один у меня
[01:17:53.240 --> 01:18:05.880]  еще нету. Да. Видите я сюда добавил skn плюс один. skn плюс один. Значит здесь надо идти dkn плюс один.
[01:18:05.880 --> 01:18:13.320]  И здесь идти dkn плюс один. Сколько у нас этих s, столько нам нужно x и всего. Так. И вот здесь давайте
[01:18:13.320 --> 01:18:24.160]  сделаем плюс один. И здесь сделаем плюс один. Так. Вот. Вот теперь точно правильно. Мысли. Уже все.
[01:18:24.160 --> 01:18:37.480]  Ух ты мой. Так. Ладно. Тогда ускоряемся. Да. Значит смотрите в чем суть. Ладно. Я тут не успел. Но
[01:18:37.480 --> 01:18:45.600]  смотрите в чем суть. Это уже чисто технические детали. Во-первых. Вот этот. Когда мы интегрируем
[01:18:45.600 --> 01:18:51.520]  вот по этой области. Обратите внимание. Мы производим интегрирование по этой области. Вот это
[01:18:51.520 --> 01:19:00.680]  больше tnt. Значит xn плюс один точно больше всех остальных x. Потому что эти лежат здесь. А эти
[01:19:00.680 --> 01:19:08.480]  и подавно меньше. Эти меньше tnt. А он больше. И раз мы интегрируем по такой хитрой области. Это
[01:19:08.480 --> 01:19:15.320]  означает что знак меньше здесь нам на самом деле не нужен. Мы его можем просто выкинуть. Потому
[01:19:15.320 --> 01:19:22.360]  что интегрируя по этой области. Уже заведомо у нас будет так что это больше всего остального. Так
[01:19:22.360 --> 01:19:29.320]  что мы просто можем взять и убрать это отсюда. Из этого индикатора. И смотрите тогда что получается.
[01:19:29.320 --> 01:19:40.120]  Вот эта вещь зависит только от xkn плюс один. И вот у нас дифференциал. Мы можем этот интеграл
[01:19:40.120 --> 01:19:49.200]  вытащить вовнутрь. И проинтегрировать. И у нас останется что-то внутри. Интеграл от чего-то.
[01:19:49.200 --> 01:19:59.640]  Вот давайте я напишу. kn плюс один. Значит там получается у нас интеграл. От чего? От tn да
[01:19:59.640 --> 01:20:10.520]  плюс бесконечности. Это вот эта область наша. Вот e в степени минус лямбда xkn плюс один. Вот dxkn
[01:20:10.520 --> 01:20:18.720]  плюс один. И умножить на интеграл от всего остального. И индикатор. Ноль меньше x1 меньше
[01:20:18.720 --> 01:20:33.200]  и так далее. Меньше xkn. Вот так dx1 и так далее dxkn. Ну вот вытащили. Ну вот это выражение там
[01:20:33.200 --> 01:20:39.840]  можно преобразовать. Посмотрите в пдфках. Тут интеграл экспоненты простейший. Теперь вот эта вещь.
[01:20:39.840 --> 01:20:48.160]  Теперь смотрите. Для расчета вот этой вещи мы можем воспользоваться той же самой идеей. Какой
[01:20:48.160 --> 01:20:56.800]  мы воспользовались, когда мы от этого избавились. Если вот эти иксы больше tn минус один, то они
[01:20:56.800 --> 01:21:03.200]  точно больше, чем все остальные иксы. Поэтому из этого индикатора соответствующий знак меньше можно
[01:21:03.200 --> 01:21:09.360]  выкинуть. Мы уже интегрируем по нужной нам траектории. И вот этот индикатор с этим меньше лишним,
[01:21:09.360 --> 01:21:19.480]  он нас не ограничивает. Так что вот весь этот набор иксов можно поделить на несколько частей вот
[01:21:19.480 --> 01:21:26.120]  этих вот. Вот эти иксы потом. То есть индикатор то, что эти иксы упорядочены. Умножить на индикатор,
[01:21:26.120 --> 01:21:31.880]  что эти иксы упорядочены и так далее. И никак эти индикаторы между собой уже не взаимодействуют.
[01:21:31.880 --> 01:21:37.520]  Знак меньше между ними мы убрали, потому что мы уже интегрируем по нужной нам области. Так,
[01:21:38.160 --> 01:21:45.720]  вот этот индикатор распадется на произведение индикаторов с иксами, которые не взаимодействуют. Так
[01:21:45.720 --> 01:21:52.760]  что весь этот интеграл является произведением интегралов по этой области, по этой, по той,
[01:21:52.760 --> 01:22:01.040]  по той и так далее. Вот это первая идея здесь. То есть вот этот индикатор, он расщепляется на
[01:22:01.040 --> 01:22:08.840]  произведение индикаторов, на группы. Дальше, дальше-то что? Мы должны будем проинтегрировать вот
[01:22:08.840 --> 01:22:17.440]  по такой области, где все иксы упорядочены. Если бы они не были упорядочены, тогда интеграл по
[01:22:17.440 --> 01:22:26.720]  этой области от единицы, это просто объем этой области, а это параллелепипед многомерный. Так
[01:22:26.720 --> 01:22:38.240]  что это будет t1-0 в степени k1. Объем этого множества t2-t1 в степени k2-k1-1. Видите,
[01:22:38.240 --> 01:22:45.560]  вот они наши разности появляются, которые потом будут у нас в поассоновском распределении находиться
[01:22:45.560 --> 01:22:54.080]  и так далее. Это если бы не было индикатора. А у нас индикаторы с упорядоченными иксами. Это значит,
[01:22:54.080 --> 01:23:00.980]  что мы рассматриваем не параллелепипед, а симплекс в нем. И объем этого симплекса равен объему
[01:23:00.980 --> 01:23:09.800]  параллелепипеда разделить на факториал пространства. То есть получается, что интеграл по вот этой области
[01:23:09.800 --> 01:23:17.120]  с упорядоченными иксами. Это есть объем этого пространства как есть параллелепипеда, то есть t1 в
[01:23:17.120 --> 01:23:26.920]  степени k1 и разделить на факториал пространства k1 факториал. Чувствуете t1 в степени k1 разделить на k1
[01:23:26.920 --> 01:23:31.640]  факториал. Вот оно. Вот оно. Откуда вот эти факториалы в знаменателе потом для поассоновского
[01:23:31.640 --> 01:23:40.400]  распределения возникают. Все. Это вся идея. То есть мы отщепили вот этот. Мы пришли сюда,
[01:23:40.400 --> 01:23:45.520]  расщепили этот индикатор на произведение индикаторов, потому что у нас область такая. И
[01:23:45.520 --> 01:23:51.840]  дальше заметили, что то, что мы получили, это просто объемы симплексов вот в этих вот параллелепипедах.
[01:23:51.840 --> 01:24:03.120]  Объем, формула объема мы знаем. Так что получается так, что мы получим некоторое выражение, вот уже
[01:24:03.120 --> 01:24:12.160]  конечное, которое зависит от t1, t2 и так далее и k1, k2 и так далее. И вот это выражение, оно будет
[01:24:12.160 --> 01:24:23.640]  из себя представлять не что иное, как произведение вероятностей приращения для каждого x в отдельности
[01:24:23.640 --> 01:24:29.640]  и для k. Так что это будет как бы доказывать вот этот второй пункт, что они действительно независимы
[01:24:29.640 --> 01:24:35.960]  в совокупности. Вот. Ну и так как там вот вылезут вот эти вот факториалы и так далее, выражение,
[01:24:35.960 --> 01:24:40.640]  которое мы получим, это будет поассоновское распределение. Так что мы автоматически докажем
[01:24:40.640 --> 01:24:50.040]  на самом деле и третий пункт тоже. Вот. Ну, пожалуй, на этом все. Да, вот я немножко не успел, но
[01:24:50.040 --> 01:24:58.320]  дальше идут чисто детали технического плана. Мне кажется, что вы способны посмотреть просто мои
[01:24:58.320 --> 01:25:03.600]  ПДФки, где очень аккуратно вот эти все интегралы выписаны. В общем, там двух действий только не
[01:25:03.600 --> 01:25:09.840]  хватает. Это вы посмотрите. А идейно, что потом с этим делать, мы поговорим уже на следующей лекции.

[00:00.000 --> 00:16.440]  Так, коллеги, значит, с того места, на котором мы остановились, мы с вами получили условия
[00:16.440 --> 00:23.240]  независимости, точнее говоря, получили, что если две случайные причины независимые,
[00:23.240 --> 00:29.760]  то математическое ожидание произведения равно произведению математических ожиданий. Правда,
[00:29.760 --> 00:35.840]  отметили, что это также имеет место, когда ковыриация между этими случайными величинами равна нулю.
[00:35.840 --> 00:42.040]  Ну и там нарисовали некую диаграмму. Так, ну давайте теперь воспользуемся этими знаниями,
[00:42.040 --> 00:51.400]  ну, например, вот для чего. Давайте посчитаем дисперсию суммы случайных величин. Просто
[00:51.400 --> 00:58.560]  получим формулу. Это есть математическое ожидание кси центрированное плюс это
[00:58.560 --> 01:05.640]  центрированное в квадрате. Правильно, да? Раскрываем скобочки, пользуемся линейностью.
[01:05.640 --> 01:11.960]  Математическое ожидание кси центрированного в квадрате это дисперсия кси. Математическое
[01:11.960 --> 01:23.320]  ожидание это центрированное в квадрате, это дисперсия это. И плюс два математических ожидания,
[01:23.320 --> 01:28.760]  их произведение. Но произведение, математическое ожидание произведения центрированных случайных
[01:28.760 --> 01:40.520]  величин, как раз и есть ковыриация. Что мы видим из этой формулы? Из этой формулы мы,
[01:40.680 --> 01:47.200]  выводы какие можем сделать? Ну, во-первых, если случайные величины не коррелированы,
[01:47.200 --> 01:53.400]  то дисперсия суммы равна сумме дисперсии, потому что эта ковыриация равна нулю. Если
[01:53.400 --> 02:00.600]  случайные величины независимы, то дисперсия суммы равна сумме дисперсии. Ну, чаще всего вот в
[02:00.600 --> 02:06.720]  такой формулировке это свойство и приводит. Ну, еще раз обращаю ваше внимание, что совсем
[02:06.720 --> 02:12.760]  не обязательно независимы случайных величин, чтобы дисперсия была аддитивной. В принципе,
[02:12.760 --> 02:27.680]  достаточно вот того, что ковыриация их равна нулю. Ну и теперь мы можем, хочу вернуться к анонсу того,
[02:27.680 --> 02:35.120]  что я делал в прошлый раз. Почему все-таки у мат ожидания кси два обозначения? E кси и M кси.
[02:35.120 --> 02:44.600]  Значит, ну и повторюсь, значит, E кси это expected value. Ожидание, ну, собственно, это ожидание,
[02:44.600 --> 02:51.160]  а вот это мин, среднее. Почему такие две буквы, как бы, так сказать, возникли? Чтобы понять это,
[02:51.160 --> 02:58.720]  давайте рассмотрим n штук случайных величин, которые независимы, одинаково распределенные
[02:58.720 --> 03:07.720]  случайные величины. NURS-V такое стандартное обозначение. Ну, заметьте, я не говорю независимую
[03:07.720 --> 03:15.920]  совокупности, поскольку, ну, это излишнее свойство, просто кси n независимы, одинаково распределенные
[03:15.920 --> 03:26.320]  случайные величины. И введу такую, в общем-то, известную вам функцию среднее значение. Среднее
[03:26.320 --> 03:35.720]  значение. Ну а теперь давайте попробуем применить наше знание. Чему равно математическое ожидание
[03:35.720 --> 04:05.280]  кси среднего? А где минус? Вот это? Так, значит, ну, поскольку это одинаково
[04:05.280 --> 04:12.200]  распределенные случайные величины, то у них у всех одинаково от ожидания дисперсия существования,
[04:12.200 --> 04:20.440]  которые предполагаем. Итак, чему равно от ожидания кси среднего? Ну, давайте примените.
[04:20.440 --> 04:39.360]  Коллеги, согласны? А чему равна дисперсия кси среднего? Напомню, что это независимые одинаково
[04:39.360 --> 04:46.800]  распределенные случайные величины. Из-под знака дисперсии константа как выносится? В квадрате,
[04:46.800 --> 04:53.640]  1 на n квадрат, получается дисперсия суммы. Дисперсия суммы для независимых случайных
[04:53.640 --> 05:00.000]  величин равна сумме дисперсий. Получается n дисперсий делить на n квадрат. То есть дисперсия
[05:00.000 --> 05:09.280]  кси среднего равна дисперсии кси делить на n. Понятно, да? Ну, вообще, я как бы исхожу из того,
[05:09.280 --> 05:16.080]  что вы такие вещи, ну, легко делаете. Вот, ну и давайте воспользуемся, собственно, теперь неравенством
[05:16.080 --> 05:25.120]  Чебышева для случайной величины кси средняя и получим. Вероятность того, модуль кси средняя
[05:25.120 --> 05:33.520]  отклоняется от своего мотожидания, а мотожидание кси среднего это екси, на величину больше или
[05:33.520 --> 05:41.880]  равную епсилон меньше или равна дисперсии кси среднего, то есть дисперсии кси делить на n,
[05:41.880 --> 05:53.400]  епсилон квадрат. Вот что получается. Ну и как мы это интерпретируем? Среднее значение
[05:53.400 --> 06:00.520]  при достаточно большом n со сколь угодно большой вероятностью находится в сколь угодно малой
[06:00.520 --> 06:10.920]  окрестности математического ожидания. Видите, да? То есть при n стремящейся к бесконечности кси средняя,
[06:10.920 --> 06:22.720]  то есть средняя, и мотожидание это одно и то же. Но в чем разница? Екси это интеграл либега от
[06:22.720 --> 06:28.760]  случайной величины, определенный на неком вероятностном пространстве, то есть элемент
[06:28.760 --> 06:35.360]  абстрактной модели. А что такое кси средняя? Это эксперимент, это экспериментальные данные,
[06:35.360 --> 06:42.120]  например монет мы можем подбрасывать или кубик. И таким образом, ну я возвращаюсь к тому, о чем
[06:42.120 --> 06:49.920]  говорил в самом начале, ну вот можно сказать, что, например, вот это неравенство, одно из, да, говорит
[06:49.920 --> 06:56.560]  о том, что теория вероятностей это не абстрактная дисциплина, а естественно научная дисциплина,
[06:56.560 --> 07:04.920]  потому что, значит, характеристики, которые, так сказать, вычислимы в модели, могут быть
[07:04.920 --> 07:11.600]  экспериментально подтверждены, ну как законома, например. Есть у вас абстрактная модель, как
[07:11.600 --> 07:20.080]  электрончики там движутся по решетке металла под воздействием электрического поля, а есть эксперимент,
[07:20.080 --> 07:26.480]  вы просто меряете напряжение, силу тока, вот сопротивление либо получаете, либо так светя,
[07:26.480 --> 07:33.920]  но в системе, то есть вот эту величину можно подтвердить экспериментально, потому что она
[07:33.920 --> 07:42.920]  приближается сколь угодно высокой степенью величиной кси средняя. Поэтому это, как я уже
[07:42.920 --> 07:52.640]  говорил, ну причем это, конечно, не общепризнанный термин, там это шутка такая, да, это, можно сказать,
[07:52.640 --> 08:01.480]  второй закон Ньютона для теории вероятностей. Вот, значит, или одна из формулировок его. Ну вот,
[08:01.480 --> 08:06.920]  давайте на этом зафиксируемся, это очень важно, но для того, чтобы это как бы осмыслить, надо самим,
[08:06.920 --> 08:15.760]  как называется, покрутить в голове, вот я зафиксировал этот факт, комментарии дал, и давайте
[08:15.760 --> 08:28.000]  двинемся дальше. Значит, мы с вами изучили ковариацию, надо сказать, что еще есть такая величина вводится,
[08:28.000 --> 08:35.640]  можно с ней столкнуться, особенно во всякого рода приложениях, это корреляция или коэффициент
[08:35.640 --> 08:41.520]  корреляции, которая определяется вот так для двух случайных величинам.
[08:41.520 --> 08:57.840]  Корреляция, это ее определение. Какими свойствами она обладает? Ну, во-первых,
[08:57.840 --> 09:09.080]  Ро по модулю меньше чего? Единицы, потому что в прошлый раз, когда мы свойства дисперсии
[09:09.080 --> 09:15.840]  выписывали, мы показали на основании неравенства Кашибуниковска, что ковариация кси-эта по модулю
[09:15.840 --> 09:25.040]  меньше или равна корне квадратного из произведения дисперсии, поэтому Ро меньше равно единице. Второе
[09:25.080 --> 09:36.440]  свойство, если кси-эта независима, то чему равна корреляция или коэффициент корреляции? Ро равно нулю.
[09:36.440 --> 09:45.400]  Ну и третье свойство, которое выделяют в корреляции, я вот здесь напишу, давайте считать, что это
[09:45.400 --> 09:56.040]  линейно связано с кси и посчитаем коэффициент корреляции таких случайных величин. Ну, прежде
[09:56.040 --> 10:09.320]  всего, надо ка вариацию посчитать. Кси-эта равно математическому ожиданию кси-эта минус мат
[10:09.320 --> 10:19.960]  ожидание кси умножить на мат ожидание это равно математическое ожидание axi плюс b умножить на кси
[10:19.960 --> 10:30.760]  минус мат ожидания кси умножить, ну сразу мат ожидания это напишу, а мат ожидания кси плюс b
[10:30.760 --> 10:42.560]  равно. Ну, собственно, пользуемся линейностью. От этого остается а на мат ожидания кси квадрат,
[10:42.560 --> 10:59.360]  член с b сокращается, минус а на мат ожидания кси в квадрате. А это что такое? Это а умножить на
[10:59.360 --> 11:12.520]  дисперсию кси. Правильно, да? Ну и вот здесь, вот здесь напишу, чтобы какое-то время пожило.
[11:12.520 --> 11:22.560]  Ну и тогда получается, что коэффициент ρ равен в числителе а дисперсия кси, в знаменателе,
[11:22.560 --> 11:28.960]  во-первых, дисперсия кси. Чему равна дисперсия эта через дисперсию кси, если вот такая связь?
[11:28.960 --> 11:51.560]  А квадрат дисперсии кси. Ну и получается, что это равно знак а, то есть плюс или минус единица.
[11:51.560 --> 12:04.760]  В какой строчке? Здесь, вот в этой строчке. Минус а екси в квадрате, минус а дисперсии,
[12:04.760 --> 12:17.640]  значит равно. Вот это, да? То есть получается коэффициент корреляции, если случайные величины
[12:17.640 --> 12:26.480]  линейно связаны. Ну достигает своего максимального по модулю значения 1 или минус 1, а если они
[12:26.480 --> 12:34.200]  независимы, то он равен нулю. Это, так сказать, приводит к тому, что иногда в технических приложениях
[12:34.200 --> 12:43.280]  коэффициент корреляции считают мерой зависимости случайных величин. Это такое качественное
[12:43.280 --> 12:48.920]  утверждение. Количественно оно, по большому счету, ничего не значит. Существует большое количество
[12:48.920 --> 12:56.880]  примеров, когда две случайные величины, которые являются некоррелированными, то есть кавариация равна
[12:56.880 --> 13:04.720]  нулю, при этом являются зависимыми. Поэтому в обратную сторону не работает. Но есть, как бы,
[13:04.720 --> 13:12.160]  важный случай, в котором это вполне работает. Ну вот нам настало время познакомиться с
[13:12.160 --> 13:24.920]  нормальным случайным вектором. Значит корреляция не превосходит единицей, для независимых равна нулю,
[13:24.920 --> 13:36.080]  для линейной связи равна либо плюс, либо минус 1, зависит от знака А. И еще раз повторюсь из того,
[13:36.080 --> 13:42.840]  что корреляция равна нулю, то есть кавариация равна нулю из определения, не следует, что случайные
[13:42.840 --> 13:49.400]  величины кси и это независимы, за исключением выделенных случаев. И вот один выделенный
[13:49.400 --> 13:59.360]  важный случай мы сейчас рассмотрим. Так, ну давайте введем вектор кси, кси1, ксик, как компонент у него.
[13:59.360 --> 14:11.800]  Математическое ожидание кси равно математическому ожиданию кси1, математическое ожидание ксик и
[14:11.800 --> 14:23.000]  обозначим это вектором m. Также введем матрицу, которую обозначим r, которая равна математическое
[14:23.000 --> 14:38.680]  ожидание кси, центрированное на кси, центрированное, транспонированное. Это что за объект? Это матрица
[14:38.680 --> 14:54.240]  k на k, у которой элементами является кавариация ксиитая, ксижитая и ж1к. Вот этот объект, естественно,
[14:54.240 --> 15:10.800]  называется матрицей кавариации. По диагонали этой матрицы стоит что? Дисперсии, потому что кавариация
[15:10.800 --> 15:18.480]  самой на себя случайно влечены, это дисперсия. Напоминаю, закрепляем знания, полученные на прошлой
[15:18.480 --> 15:31.600]  лекции. Вот и вводится нормальный случайный вектор, который зависит от двух параметров m и r. Вводится
[15:31.600 --> 15:42.680]  это нормальный случайный вектор своей плотностью, значит f это будет уже векторная функция. Ну вот
[15:42.680 --> 15:55.520]  тут я напишу вот так mr и плотность это выглядит таким образом. Единицы делить на 2p в степени n
[15:55.520 --> 16:12.360]  пополам, корень квадратный из детермината r, e в степени минус x вектор минус m вектор транспонированная,
[16:12.360 --> 16:25.920]  r в минус 1 и x транспонированная минус m делить пополам. Но смотрите, у нас появились объекты в
[16:25.920 --> 16:33.400]  знаменателе детерминат и в показателе степени r минус 1, значит мы предполагаем, что эти объекты
[16:33.400 --> 16:41.800]  существуют. Но вообще просто по построению матрица r больше или равно нуля, с точки зрения положительной
[16:41.800 --> 16:52.480]  определенности. Мы пока требуем r строго больше нуля для того, что детерминат не равнялся нулю и
[16:52.480 --> 17:00.040]  обратная матрица существовала. И вот такой нормальный вектор называется невыраженным нормальным
[17:00.040 --> 17:05.280]  вектором, в отличие от выраженного нормального вектора, у которого r может равняться нулю, но это
[17:05.280 --> 17:22.880]  чуть позже. Пока считаем r больше нуля строго. 2p в степени n пополам, n, k, прошу прощения. Вот тут,
[17:22.880 --> 17:30.880]  коллеги, справьте, у нас размерность k. Спасибо. Вот, ну давайте вот это сотру.
[17:52.880 --> 18:05.840]  Так.
[18:05.840 --> 18:22.560]  Давайте предположим, что у нас компоненты этого вектора не коррелированы. То есть вот
[18:22.560 --> 18:33.240]  тот самый случай, когда ксиитая, ксижитая равно нулю для любого i неравного j. Коэффициент
[18:33.240 --> 18:38.720]  корреляции равен нулю, ковариация равна нулю. Вот рассматриваем такой случай. В этом случае,
[18:38.720 --> 18:52.920]  значит, тогда матрица выглядит так. Дисперсия кси-1, дисперсия кси-k, 0, 0. Правильно, да?
[18:52.920 --> 19:06.440]  Детерминат тогда ее, это произведение дисперсии ксиджитых j от единицы до k, а r в минус 1, в этом
[19:06.440 --> 19:21.240]  простом случае, это единица на дисперсии кси-1, единица на дисперсии кси-k, 0, 0. Ну и давайте это
[19:21.240 --> 19:27.680]  теперь просто подставим в нашу функцию плотности и посмотрим, что получится. Стираем вот здесь.
[19:36.440 --> 20:05.000]  Я без индекса буду писать, с вашего позволения. Вот об этой функции идет речь. Равно единица
[20:05.000 --> 20:21.160]  на 2π в степени k пополам. Детерминат у нас превратится, вот такое произведение дисперсия кси-1, дисперсия
[20:21.160 --> 20:37.320]  дисперсия кси-k. Экспонент у нас превратится вот в такую штуку, может так напишу, эксп, минус сумма
[20:37.320 --> 21:00.780]  x житая в квадрате, x житая минус m житая в квадрате делить на два дисперсия кси-житая. Вот посмотрите,
[21:00.780 --> 21:30.060]  правильно? Да, спасибо. Равно ж от единицы до k, единица на корень квадратной 2π дисперсии
[21:30.060 --> 21:43.260]  кси-житая, е в степени минус x житая минус m житая в квадрате делить на две дисперсии кси-житая.
[21:43.260 --> 21:55.700]  То есть функция плотности вот этого вектора, вот с такой плотностью, если компоненты вектора
[21:55.700 --> 22:02.140]  не коррелированы, развалилось вот с такое произведение. А что здесь каждый из сомножителей
[22:02.140 --> 22:10.740]  представляет? Функция плотности нормальной случайной величины. Функция плотности вектора равна
[22:10.740 --> 22:21.460]  произведению плотностей компонент. Это и означает, что кси-1, кси-n не только не коррелированы, но и
[22:21.460 --> 22:29.300]  независима. То есть нормальный случайный вектор – это тот самый случай, когда из некоррелированности
[22:29.300 --> 22:37.740]  следует независимость. Мы привели именно этот пример, потому что само по себе нормальное
[22:37.740 --> 22:46.580]  распределение и нормальный вектор – очень важный объект в нашей дисциплине. И вот для него есть
[22:46.580 --> 22:53.700]  такое замечательное свойство. То есть из независимости, из некоррелированности компонент
[22:53.700 --> 23:01.260]  нормального случайного вектора следует их независимость. Вот отметите про себя этот факт.
[23:01.260 --> 23:19.140]  Так, дальше. Ну вот эти две доски тогда начну занимать.
[23:19.140 --> 23:37.300]  Напомню, что мы с вами обсуждаем мат ожидания, а нормальный случайный вектор мы ввели в связи
[23:37.300 --> 23:46.660]  с использованием свойств к авариации. Ну давайте еще там про мат ожидания чуть-чуть поговорим.
[23:46.660 --> 24:01.980]  Ну, помним про то, что случайная величина – это на самом деле измеримая функция на множество
[24:01.980 --> 24:11.980]  элементарных исходов, а математическое ожидание – интеграл Либега. Поэтому и существует достаточно
[24:11.980 --> 24:17.820]  большое количество всяких неравенств, ну как там в метрических пространствах, например. Для
[24:17.820 --> 24:24.420]  случайных величин или, точнее говоря, их мат ожиданий тоже существует всякие неравенства. Их много,
[24:24.420 --> 24:35.500]  но мы сейчас познакомимся или я напомню вам где-то такой минимум. Итак, имеет место такое неравенство.
[24:35.500 --> 24:46.500]  Математическое ожидание кси1 умножить на кси2 меньше или равно, чем математическое ожидание кси1 в
[24:46.500 --> 24:58.820]  степени r в степени единица на r умножить на математическое ожидание модуль кси2 в степени s,
[24:58.820 --> 25:10.940]  в степени единица на s, где единица на r плюс единица на s равно единице и r больше единицы. Что это
[25:10.940 --> 25:19.100]  за неравенство? Это неравенство Гольдера, просто вместо интегралов написал мат ожиданий. Еще одно
[25:19.100 --> 25:29.860]  неравенство. Математическое ожидание кси1 плюс кси2 в степени r, все это единица на r меньше или равно
[25:29.860 --> 25:42.700]  математическое ожидание кси1 в степени r в степени единицы на r плюс математическое ожидание кси2 в
[25:42.700 --> 25:51.380]  степени r в степень единицы на r. Это что за неравенство Минковского? Опять же, вместо функции написал
[25:51.380 --> 25:58.220]  случайные илличины, а вместо интегралов написал мат ожидания. А вот следующее неравенство вряд ли
[25:58.220 --> 26:07.940]  вам знакомо. Выглядит оно вот так. Пусть у больше v больше нуля, то есть заданы два числа, тогда
[26:07.940 --> 26:20.740]  математическое ожидание модуль это в степени u корень, так сказать, у той степени, больше ли равно
[26:20.740 --> 26:30.660]  математическому ожиданию это в степени v корень в этой степени. Знаете, да, это неравенство Липунова.
[26:30.660 --> 26:46.860]  Значит, какой факт из него сразу следует? Что если у вас есть, существует мат ожидания это в степени u,
[26:46.860 --> 26:55.700]  то существует мат ожидания u в степени v для всех v меньше u, но больше нуля. Или если u и v целые,
[26:55.700 --> 27:04.540]  то часто вот как это, так сказать, апеллируют какому факту. Если у случайной влечены существует
[27:04.540 --> 27:13.580]  начальный момент катого порядка, то существуют все начальные моменты более низших порядков. Мы
[27:13.580 --> 27:22.180]  сегодня этим, кстати, воспользуемся. Так, значит, неравенство Липунова мы доказывать не будем,
[27:22.180 --> 27:31.380]  ну, по крайней мере, так сказать, доказательства Липунова. Мы сейчас получим еще одно неравенство,
[27:31.380 --> 27:37.020]  которое тоже вам по названию, по крайней мере, знакомо, но, может быть, в такой формулировке не
[27:37.020 --> 27:46.740]  сталкивались. Значит, давайте рассмотрим g от x, выпуклую функцию, ну так, для определенности выпуклую
[27:46.740 --> 27:57.900]  вниз, уточнимся. И тогда можно утверждать следующее, что для любых x и y g от x больше или равно,
[27:57.900 --> 28:07.780]  чем g от y, плюс некоторое число g' от y, которое совершенно случайно совпадает с обозначением
[28:07.780 --> 28:18.180]  производной, на x-y. Это для выпуклой вниз функции. Ну, если функция дифференцируемая, то это
[28:18.180 --> 28:24.020]  действительно просто производная, а если не дифференцируемая, то это танго с наклона опорной
[28:24.020 --> 28:31.940]  плоскости, опорной племой. Теперь давайте, так сказать, придадим вероятностный шарм. Давайте в
[28:31.940 --> 28:38.700]  качестве x возьмем некую случайную личину x, а в качестве y возьмем математическое ожидание x.
[28:38.700 --> 28:45.940]  Ну, естественно, предполагаемое существование. Ну и что тогда получается? Тогда получается g от
[28:45.940 --> 28:59.180]  x больше или равно g от мат ожидания x, плюс g' от мат ожидания x на x минус мат ожидания x.
[28:59.180 --> 29:08.180]  Для всех Омега из исходного вероятностного пространства имеет место вот такое неравенство.
[29:08.180 --> 29:16.620]  По свойству интеграла Либега, аналогичное ранжирование имеет место и для мат ожиданий.
[29:16.620 --> 29:24.660]  Поэтому пишем. Берем мат ожидания, так сказать, с двух сторон, если на сленге говорить. Мат ожидания
[29:24.660 --> 29:31.060]  g от x больше или равно. Но вот это уже число, поэтому вот мат ожидания самому себе и равно. А
[29:31.060 --> 29:43.820]  мат ожидания вот этой величины чему равно? Нулю, потому что константы выносятся. Мат ожидания x минус
[29:43.820 --> 29:50.700]  мат ожидания x равно нулю. Поэтому получается, собственно, вот такое неравенство, которое называется
[29:50.700 --> 30:06.420]  неравенство Янсона. Неравенство Янсона для случайных величин. Ну, кстати, не знаю, стоит ли тратить на это время.
[30:20.700 --> 30:35.500]  Давайте, видимо, напишу. Значит, если теперь взять в качестве кси случайную величину модуль
[30:35.500 --> 30:51.020]  это в степени v, а в качестве g от x взять модуль x в степени u на v, поскольку это больше единицы,
[30:51.020 --> 30:58.300]  то это выпукло вниз функция. Подставить неравенство Янсона и как бы аккуратно две строчки провести
[30:58.300 --> 31:07.460]  выкладки, то вы получите неравенство Липунова. Но с точки зрения научных приоритетов неравенство
[31:07.460 --> 31:13.740]  Липунова появилось раньше, чем неравенство Янсона. Поэтому, так сказать, неравенство Липунова по праву
[31:13.740 --> 31:22.700]  носит имя великого российского математика Липунова, а не является просто следствием неравенства Янсона.
[31:22.700 --> 31:31.900]  Вот так. Ну, это вот давайте тот минимум про вероятностные неравенства, которые полезно знать,
[31:31.900 --> 31:36.700]  хотя еще раз повторю, в каких-то конкретных задачах могут быть всякие другие.
[31:36.700 --> 31:54.820]  Так, и значит, по-моему, последний объект, который нам нужно изучить из, так сказать,
[31:54.820 --> 32:01.580]  математических ожиданий, это условное математическое ожидание. Я напомню,
[32:01.580 --> 32:10.940]  что мы с вами вводили условную функцию распределения, ну, которая мы понимаем так,
[32:10.940 --> 32:22.740]  кси меньше х при условии, что это равно у. Значит, случай, когда вероятность вот этого события не равна нулю,
[32:22.740 --> 32:28.180]  не представляет никаких проблем, пока ограничимся им. Более того, будем считать,
[32:28.180 --> 32:36.540]  что это дискретная случайная величина. И тогда условным от ожидания мы вводим самым естественным образом.
[32:36.540 --> 32:46.660]  Условное от ожидания кси при условии, что это равно у, ну, иногда пишут равно у, иногда как бы нет,
[32:46.660 --> 33:01.060]  это есть интеграл х на условную функцию распределения, то есть простым от ожидания это xdf функция распределения,
[33:01.060 --> 33:12.140]  а условным от ожидания это х на d условную функцию распределения. Вот так мы ее определим.
[33:12.140 --> 33:20.900]  Скажу попозже пару слов на эту тему. Так, теперь давайте вот какую последствию проделаем.
[33:20.900 --> 33:28.340]  Выпишем функцию распределения случайной величины кси. Это по определению вероятность того,
[33:28.340 --> 33:37.860]  что кси меньше х и по формуле полной вероятности это сумма вероятности того, что кси меньше х при
[33:37.860 --> 33:47.180]  условии, что это равно некому y житому. Здесь я уже пользую то, что это дискретная случайная величина,
[33:47.180 --> 34:03.460]  сумма по ж. И это равно в наших терминах сумма f кси это х при условии y на вероятность того,
[34:03.540 --> 34:13.340]  что это равно y житое. Я бы здесь не стал выписывать отдельно непрерывный дискретный случай из общего
[34:13.340 --> 34:18.940]  интеграла Либега, но мы помним, что это всего лишь такая мнимоническая запись. Просто чтобы не
[34:18.940 --> 34:24.380]  выписывать каждый раз непрерывное через плотность, а дискретное через суммы, мы вот пользуемся
[34:24.380 --> 34:36.140]  записью интеграла Либега Стилтеса. Так, значит, вот и кси такая. Пойду вот сюда по кругу.
[34:54.380 --> 35:11.820]  Давайте выпишем теперь мат ожидания кси. Это есть интеграл df кси от х. Вместо dfc подставляем
[35:11.820 --> 35:31.800]  вот эту формулу и получаем сумма вероятностей того, что это равно y житое на интеграл х df
[35:31.800 --> 35:47.840]  кси это х при условии y житое. Да, вроде так. Теперь обращаем внимание, что вот это как раз наша
[35:47.840 --> 35:58.280]  екси при условии это равно y житое с одной стороны, а с другой стороны это просто некая
[35:58.280 --> 36:07.120]  функция от y житого. Функция от y житого. И что мы здесь видим в этой сумме? Значение
[36:07.120 --> 36:13.320]  функции в точке y житое умножено вероятность того, что случайно чина примет это значение. По
[36:13.320 --> 36:27.000]  нашим определениям, по всем, это есть математическое ожидание фи вот это. Правильно, да? Вот функция
[36:27.000 --> 36:41.000]  на вероятность. Ну или перепишем это в такой общей как бы так принятой форме. То есть сначала
[36:41.000 --> 36:47.720]  берется условное мат ожидания, а потом берется, так сказать, мат ожидания по той случайной
[36:47.720 --> 36:54.520]  величине, которая была условием, потому что условным от ожидания это случайная величина. Вот фи от y житого,
[36:54.520 --> 37:04.280]  ну фи от этого. Тут, конечно, возникают, могут, ну есть вопросы, а будет ли эта функция измеримая? То
[37:04.280 --> 37:12.400]  есть, ну давайте мы это просто постулируем. И вот, собственно, как бы сказать, основное уравнение
[37:12.400 --> 37:26.200]  условных математических ожиданий. Я иногда здесь ставлю внизу это, чтобы было понятно, о чем идет
[37:26.200 --> 37:32.320]  речь с точки зрения внешнего от ожидания. Ну это необщепринято. Вот, иногда ставят, иногда не ставят.
[37:32.320 --> 37:40.360]  И теперь как бы пару слов. На самом деле, наше определение условно-математическое ожидание
[37:40.360 --> 37:48.600]  через условную функцию распределения имеет ряд родовых как раз травм, связанных с тем, что вот
[37:48.600 --> 37:54.120]  эта вероятность может равна нулю. Точнее говоря, нас интересуют такие случаи, когда эта вероятность
[37:54.120 --> 38:04.760]  равна нулю. И мы с вами на прошлой лекции получили, что в качестве условной функции плотности надо
[38:04.760 --> 38:16.400]  взять вот такую. И вот если такую взять, то в смысле не там предельного перехода, который мы сделали,
[38:16.400 --> 38:24.960]  можно считать, что это условная плотность. Вот. Ну вот, это родовая травма переносится и на
[38:24.960 --> 38:34.400]  условные математические ожидания. Поэтому более такой глубокий и правильный подход состоит в том,
[38:34.400 --> 38:39.920]  что вот это берется за определение условно-математического ожидания. Условное
[38:39.920 --> 38:45.320]  математическое ожидание — это такая функция, одно из ее свойств, что когда вы берете по ней
[38:45.320 --> 38:52.720]  мат ожидания по условию, вы получаете мат ожиданий с одной случайной влечены. Вот. Этот подход,
[38:52.720 --> 39:02.320]  ну там, теоретически безупречен, но у нас, к сожалению, нет на него времени. Тем не менее, так как мы это
[39:02.320 --> 39:11.840]  сделали, он более наглядный. Здесь как бы, ну так все естественно. Условная функция распределения есть,
[39:11.840 --> 39:17.120]  вот и определяем условное мат ожидания. Здесь это, так сказать, выглядит гораздо более абстрактно,
[39:17.120 --> 39:27.720]  но, тем не менее, такой подход существует, и он, с теоретической точки зрения, там более общий,
[39:27.720 --> 39:35.280]  более гармоничный. Но мы с вами, еще раз повторю, определим вот так, в виду экономии времени. Так,
[39:35.360 --> 39:46.120]  вроде по мат ожиданиям и свойствам все, что я хотел сказать, поэтому мы с вами приступаем к следующей теме.
[39:46.120 --> 39:56.080]  Ага, ну сейчас будет звонок, я так понимаю, и мы приступим к ней после перемены.
[39:56.080 --> 40:10.840]  Следующая наша тема, это характеристические функции. То есть, для любой случайной влечены,
[40:10.840 --> 40:18.360]  мы можем определить такую функцию, фи, кси, акте, которая есть математическое ожидание,
[40:18.360 --> 40:29.160]  е в степени, и, т, кси. Не слышу звонка, да? А был, да? Ну все, дыхайте тогда, что-то.
[40:29.160 --> 40:37.880]  Так, прошу прощения, я все-таки в части вот этой формулы приведу один пример, существенный,
[40:37.880 --> 40:46.600]  как бы важный, но простой. Давайте рассмотрим, какую случайную влечину с, которая равна сумме
[40:46.600 --> 40:56.360]  неких ксиджитов, но в случайном количестве. То есть, ксиджиты – это независимо одинаково
[40:56.360 --> 41:03.000]  распределенные случайные влечины, ну а n – дискретная случайная влечина некоторая. И мы хотим
[41:03.000 --> 41:11.600]  найти мат ожидания s. Воспользуемся вот этой формулой и напишем, что это мат ожидания по n,
[41:11.600 --> 41:23.600]  на математическое ожидание суммы ксиджитых, g от единицы до n, при условии, что n равно некому n
[41:23.600 --> 41:34.560]  малому. Это равно сумма. Если n большое равно n малому, то это мат ожидания суммы независимых
[41:34.560 --> 41:43.320]  одинаково распределенных величин и равна на n на мат ожидания кси. А мат ожидания, чтобы взять по n,
[41:43.320 --> 41:54.300]  надо умножить вероятность того, что n равно n малое. И по всем n взять сумму. Равно, равно, мат
[41:54.300 --> 42:01.760]  ожидания кси выносим за скобки, а то, что остается в скобках, это мат ожидания n. И получаем такую,
[42:02.440 --> 42:08.360]  в этом смысле, естественную формулу. Мат ожидания s равно мат ожидания кси умножить на мат
[42:08.360 --> 42:16.520]  ожидания n. Если повозиться аккуратно по такой же технологии, то можно получить дисперсию s,
[42:16.520 --> 42:27.400]  которая окажется равна дисперсии кси умножить на мат ожидания n. Такой вполне ожидаемый член.
[42:27.400 --> 42:35.360]  Но это не все. А еще есть дисперсия n, умноженная на мат ожидания кси в квадрате.
[42:35.360 --> 42:48.840]  Итак, в практическом применении вот этот, конечно, член много чего портит. Эти формулы
[42:48.840 --> 42:54.040]  довольно широко применяются, потому что существует довольно много моделей, в которых,
[42:54.520 --> 43:01.320]  вот рассматриваются такие случайные величины. Сумма независимых случайных величин, но взятая
[43:01.320 --> 43:09.320]  в случайном количестве. Вот, тогда давайте теперь уже все и перейдем к характеристическим функциям.
[43:09.320 --> 43:18.520]  Значит, характеристическая функция определяется для любой случайной величины. Это есть
[43:18.520 --> 43:25.000]  математическое ожидание e в степени i. Это корень из минус единицы, комплексная i,
[43:25.000 --> 43:32.560]  текси. Через интеграл Лебега-Стилтеса записывается вот так. Для дискретного
[43:32.560 --> 43:38.400]  случая это будет вот такая сумма, для неправильного случая это будет вот такой интеграл. Что это такое
[43:38.400 --> 43:48.200]  с точки зрения анализа математического общего? Что это за объекты? Знакомы вам? Это фурье
[43:48.200 --> 43:55.760]  преобразования, фурье образ. То есть, на самом деле, характеристическая функция это фурье образ
[43:55.760 --> 44:02.000]  функции распределения. То есть, как математический объект, он из себя ничего нового не представляет.
[44:02.000 --> 44:11.080]  Ну, надо сказать, что для теории вероятности это всего лишь, по большей части, всего лишь такой
[44:11.080 --> 44:19.840]  удобный инструмент, техника. Какого-то глубокого теоретического смысла в нём нет, но техника очень
[44:19.840 --> 44:29.840]  удобная. Давайте в этом убедимся. Но перед тем, как мы это убедимся, давайте рассмотрим основные
[44:29.840 --> 44:46.600]  свойства. Первое свойство, значит, это функция fx от t по модулю всегда меньше или равна единице,
[44:46.600 --> 44:57.960]  а вот fx от 0 равно единице для любой x. То есть, это комплекснозначная функция, модуль которой меньше
[44:57.960 --> 45:08.920]  единицы и максимума она достигает в точке t равно 0. По крайней мере, это очевидно. По крайней мере,
[45:08.920 --> 45:15.520]  это говорит о том, что характеристическая функция любой случайной влечины существует. У любой
[45:15.520 --> 45:23.160]  случайной влечены есть фурье образ. Второе не бесполезное свойство, эта функция равномерно
[45:23.160 --> 45:30.920]  непрерывна. То есть, для любого epsilon существует h, но больше нуля не пишу для краткости. Такой,
[45:30.920 --> 45:44.680]  что модуль xt плюс h минус fx от t меньше или равно epsilon для любого t. Равномерно непрерывная
[45:44.680 --> 45:54.920]  функция. Значит, третье свойство может быть самое важное или, так сказать, наиболее продуктивное
[45:54.920 --> 46:08.160]  с точки зрения применения аппарата. Если xi и это независимые случайные влечины, то характеристическая
[46:08.160 --> 46:23.440]  функция их суммы равна чему? Произведению характеристических функций. Ну, здесь пару слов скажем.
[46:23.440 --> 46:36.360]  Значит, характеристическая функция суммы это математическое ожидание e в степени it xi плюс
[46:36.400 --> 46:47.960]  это равно математическому ожиданию e в степени it xi умножить на математическое ожидание e в степени it это.
[46:47.960 --> 47:02.560]  Это понятный переход. Скажу два слова, потому что важно. Есть такие факты, которые не сложные,
[47:02.720 --> 47:09.840]  если их не усвоишь, потом бывает сложно. Вот пусть у нас xi и это независимые случайные влечины,
[47:09.840 --> 47:18.520]  тогда случайные влечины phi от xi и psi от it тоже независимые случайные влечины для любых
[47:18.520 --> 47:30.760]  баррельских естественных функций. Понятно, почему? Всем понятно? Здесь комплексная,
[47:30.920 --> 47:38.360]  но если расписать там будет phi, грубо говоря, sin, a, psi, cos. Значит, вот так вот это разложится в
[47:38.360 --> 47:51.520]  такое произведение, а это по определению есть. Вот, значит, следующее свойство.
[48:00.760 --> 48:13.040]  Следующее свойство четвертому назовем. Они не по важности, я их перечисляю. Значит,
[48:13.040 --> 48:25.720]  пусть у нас это равно a xi плюс b, тогда характеристическая функция это равна e в степени it b на
[48:25.720 --> 48:35.240]  характеристическую функцию xi, взятую в точке a t. Не вызвать затруднение у вас расписать.
[48:35.240 --> 48:41.040]  Коллеги, если кому-то что-то непонятно, вы говорите, я чуть поподробнее скажу,
[48:41.040 --> 48:47.360]  просто здесь дольше писать. Вот, точнее говоря, если вы это сделаете, то у вас все получится. И
[48:47.360 --> 49:11.520]  еще свойства выделим. Ну, выпишем еще раз определение. Давайте к раз эту функцию по t продифференцируем.
[49:17.360 --> 49:44.360]  Вот так. И возьмем в точке t равно нулю. Если t равно нулю, то это у нас превратится,
[49:44.480 --> 49:59.100]  вот здесь напишу, и в степени k на математическое ожидание xi в степени k. Правда, да? Но при
[49:59.100 --> 50:04.520]  условии, что этот момент существует. Если он существует, то только тогда можно дифференцировать.
[50:05.400 --> 50:19.400]  Согласны, да? Ну, и отсюда вспоминаем неравенство Липунова. Следует, что если каты,
[50:19.400 --> 50:25.240]  производные в нуле, точнее говоря, если существует каты, начальный момент случайно
[50:25.240 --> 50:33.280]  влечены, то все производные в нуле характеристической функции до катова, первая, вторая, третья каты будут
[50:33.280 --> 50:42.000]  определяться вот такой формулой. Будут определяться вот такой формулой. Вроде ничего не забыл. Если
[50:42.000 --> 50:51.040]  чего забыл, по ходу вспомним. Значит, все свойства, кроме третьего, носят абсолютно технический
[50:51.040 --> 51:01.760]  характер. Третье свойство несет некую идеологию. То есть, вот тот факт, что если независимые
[51:01.760 --> 51:06.880]  случайно влечены, то любые функции от них тоже независимы. А все остальное, ну, просто так,
[51:06.880 --> 51:12.600]  технические, технические свойства выписали для удобства. Не потому, что это какие-то серьезные
[51:12.600 --> 51:28.080]  результаты. Это вообще, говоря как бы, общие свойства интеграла Фурье. Вот. Ну и, собственно, из этих
[51:28.080 --> 51:37.320]  свойств уже видно, какой практически прог с этой характеристической функцией. Во-первых,
[51:37.320 --> 51:43.880]  можно моменты считать легко, так сказать, если у вас есть характеристическая функция. И можно
[51:43.880 --> 51:48.520]  характеристические функции суммы независимых случайных величин тоже легко выписывать.
[51:48.520 --> 52:01.280]  Так, ну давайте для примера просто, значит, для примера просто возьмем распределение
[52:01.280 --> 52:07.360]  Бернули и посчитаем его характеристическую функцию. Это дискретная случайная величина,
[52:07.360 --> 52:18.840]  поэтому фи B от P в точке T мы должны вон по формуле взять сумму. У нас случайно влечена,
[52:18.840 --> 52:29.040]  принимает значение 0 и 1, значит это будет E в степени IT 0, 0 приняла значение случайно влечена,
[52:29.040 --> 52:35.920]  на вероятность того, что она равна 0, это мы Q обычно обозначаем, плюс E в степени IT на 1,
[52:35.920 --> 52:43.600]  номератность того, что она равнилась, это P, то есть это получается Q плюс P на E в степени IT.
[52:43.600 --> 52:57.000]  Теперь давайте возьмем биномиальную величину с параметрами N и P. Кто мне с ходу скажет,
[52:57.000 --> 53:07.280]  почему ее равна характеристическая функция? Вспоминаем, что это сумма независимых Бернули,
[53:07.280 --> 53:14.640]  а характеристическая сумма функции независимых Бернули равна произведению, поэтому ее характеристическая
[53:14.640 --> 53:29.000]  функция Q плюс P на E в степени T в степени N. Для примера. Теперь давайте еще один важный
[53:29.000 --> 53:55.120]  пример, это с вашего позволения Сатру. Теперь давайте рассмотрим нормальное,
[53:55.120 --> 54:05.240]  стандартное нормальное распределение N0,1 и выпишем о характеристическую функцию. Это по
[54:05.240 --> 54:13.400]  определению интеграл от минус бесконечности до бесконечности E в степени ITх единица на корень
[54:13.400 --> 54:27.040]  из 2P е в степени минус x квадрат пополам dx. По определению, е в степени Tx комплексная
[54:27.040 --> 54:38.400]  часть, это будет sin Tx, начетную функцию 0 уйдет, останется только единица на корень из 2P интеграл
[54:38.400 --> 54:53.400]  cos Tx е в степени минус x квадрат пополам dx. Теперь давайте посчитаем производную по Т этой
[54:53.400 --> 55:13.200]  функции. Производная по Т, это будет равно единица на корень из 2P, cos это sin, я напишу sin Tx,
[55:13.200 --> 55:24.400]  минус запихну в минус x, это я по Т продиференсировал, но здесь е в степени минус x квадрат пополам dx.
[55:24.400 --> 55:41.120]  Так вот, давайте теперь этот интеграл возьмем по частям. Этот фокус Феймана,
[55:41.120 --> 56:02.240]  знаком вам такого выражения? Итак, sin Tx это будет у, а dv будет минус x на е в степени минус x квадрат
[56:02.240 --> 56:16.920]  пополам dx, то есть d от е в степени минус x квадрат пополам, равно, вот сюда перейду,
[56:32.240 --> 56:51.920]  равно единица на корень из 2P остаётся, тут будет у на v sin Tx, это не x квадрат пополам,
[56:51.920 --> 57:17.680]  минус бесконечность до бесконечность, минус u dv, значит, минус v du, du это T, cos Tx, потому что
[57:17.680 --> 57:28.000]  теперь дифференцируем по x, значит, на е в степени минус x квадрат пополам dx. Так вот,
[57:28.000 --> 57:34.960]  это вот с этим сравниваем и видим, что а, и прошу прощения, а вот этот корень ещё, вот он здесь,
[57:34.960 --> 57:44.320]  значит, и видим, что вот это на самом деле phi от T, и мы получаем, идём в начало и получаем вот такое
[57:44.320 --> 57:55.400]  уравнение, phi штрих от T равно минус T на phi от T, граничное условие phi от нуля, как мы знаем, равна
[57:55.400 --> 58:02.200]  единице, характеристическая функция в нуле всегда равна единице, правильно, да? Но вот это 0, когда мы
[58:02.200 --> 58:15.640]  предел подставим, вот. Решение от уравнения, кто подскажет, е в степени минус T квадрат пополам,
[58:15.640 --> 58:25.520]  решение от уравнения, и это и есть характеристическая функция стандартного нормального распределения. Ну,
[58:25.560 --> 58:29.880]  сейчас давайте что-нибудь из этого факта постараемся выжать.
[58:29.880 --> 58:49.520]  Перво-наперво такая техническая деталь. Давайте введём величину x по правилу,
[58:49.520 --> 59:09.400]  это a умножить, не a, а m, ой, прошу прощения, не m, sigma, sigma умножить на n 0,1 плюс m, sigma больше нуля.
[59:09.400 --> 59:18.760]  Так, поставим себе такое ограничение. Ну и рассмотрим функцию вероятности этой случайной
[59:18.760 --> 59:32.520]  величины. Это вероятность того, что xi меньше x, это равно вероятности того, что n 0,1 меньше,
[59:32.520 --> 59:43.520]  чем x минус m, делить на sigma, помним sigma больше нуля, правильно, да? А это у нас по определению,
[59:43.640 --> 59:54.920]  есть функция распределения n 0,1, взятая в точке x минус m, делить на sigma. Ну и давайте плотность
[59:54.920 --> 01:00:05.560]  f xi от x получим, продиференцируем. Значит, производная вот этой функции, это плотность стандарта
[01:00:05.560 --> 01:00:13.440]  равного распределения и на производную аргумента. То есть получаем единица делить на sigma корень
[01:00:13.440 --> 01:00:21.520]  из 2p, e в степени минус, вместо x подставляем x минус m в квадрате, делить на 2 sigma квадрат.
[01:00:21.520 --> 01:00:31.880]  Что это такое? Это функция плотности произвольного нормального распределения. Какое мы вот отсюда
[01:00:32.040 --> 01:00:38.120]  делаем? Линейное преобразование нормальной случайной величины оставляет ее в том же классе.
[01:00:38.120 --> 01:00:47.920]  Но здесь не существенно, что я с n 0,1 начал, да? Вот, значит, итак, первый вывод. Нормальная
[01:00:47.920 --> 01:00:59.440]  величина остается в том же классе. Теперь давайте рассмотрим характеристичку.
[01:01:02.720 --> 01:01:10.880]  Пока не пользуемся, сейчас попользуемся. А теперь давайте выпишем характеристическую
[01:01:10.880 --> 01:01:22.480]  функцию вот этой вот x. В соответствии со свойством, по-моему, которое было 4,
[01:01:22.480 --> 01:01:29.560]  которое я стер, когда линейная связь между случайными величинами, это будет равно е в степени
[01:01:29.560 --> 01:01:39.520]  и t, m на характеристическую функцию стандартной равновечины, взятая в точке sigma t, на e в степени
[01:01:39.520 --> 01:01:51.160]  минус sigma квадрат t квадрат, делить пополам. Вот характеристическая функция, ну, нормальной
[01:01:51.160 --> 01:02:00.960]  случайной величины с произвольными параметрами m и sigma. И, наконец, еще один приятный факт.
[01:02:00.960 --> 01:02:25.600]  Пусть у нас есть набор вектор из нормальных случайных величин, каждый со своим параметром
[01:02:26.080 --> 01:02:36.640]  м от ожидания дисперсии. Ну, и их там каштук. Рассмотрим случайную величину, которая равна
[01:02:36.640 --> 01:02:58.400]  их сумме. Характеристическая функция ее, поскольку они это сумма независимых случайных величин,
[01:02:58.400 --> 01:03:05.400]  по свойству это произведение характеристических функций. И это получается, вот сюда смотрим,
[01:03:05.400 --> 01:03:19.160]  e в степени и t, сумма м житых, минус t квадрат, сумма sigma житых в квадрате, делить пополам.
[01:03:19.160 --> 01:03:28.800]  А это что за характеристическая функция чего? Это нормальное распределение
[01:03:29.280 --> 01:03:52.760]  параметрами с суммой м от ожидания и суммой дисперсии. Я что-то запамятовал. Тут мы неявно
[01:03:52.760 --> 01:03:58.960]  пользуемся взаимооднозначным отображением между функциями распределения и характеристическими
[01:03:58.960 --> 01:04:04.720]  функциями. Но это свойство интеграла Fourier. Между образами и прообразами существует
[01:04:04.720 --> 01:04:10.840]  взаимооднозначное соответствие. Поэтому из того факта, что я получил характеристическую функцию s,
[01:04:10.840 --> 01:04:21.680]  я отсюда делаю вывод, что функция распределения s вот такая. Ну вот такой факт интересный и
[01:04:21.680 --> 01:04:27.960]  любопытный, что сумма нормальных случайных величин, независимых пока, на самом деле любых,
[01:04:27.960 --> 01:04:34.040]  но пока независимых, сумма нормальных случайных величин независимых имеет нормальное распределение
[01:04:34.040 --> 01:04:49.240]  вот с такими параметрами. Ну и давайте дальше. Что еще нам нужно знать про характеристические
[01:04:49.240 --> 01:04:56.480]  функции? Для того, чтобы это стало у нас еще более серьезным инструментом, помимо того,
[01:04:56.480 --> 01:05:03.680]  что существует взаимооднозначное соответствие, давайте вот какую рассмотрим постановку. Пусть у
[01:05:03.680 --> 01:05:10.640]  нас есть последовательность функции распределения. Ну какая-то последовательность функции
[01:05:10.640 --> 01:05:17.280]  распределения. Ну точнее говоря, есть случайные величины, им соответствует функция распределения и
[01:05:17.280 --> 01:05:25.800]  им соответствует последовательность характеристических функций. В достаточно широком классе,
[01:05:25.800 --> 01:05:32.920]  если вот эта последовательность образов поточительно сходится к какой-то функции,
[01:05:32.920 --> 01:05:40.360]  то и последовательность прообразов тоже сходится к какой-то функции, и фурье образом предела
[01:05:40.360 --> 01:05:50.120]  является вот этот предел. Ну как бы там есть ограничение, но класс достаточно широк. Но у
[01:05:50.120 --> 01:05:55.560]  нас есть одна особенность. Нас не интересует любой предел вот этой последовательности. Мы
[01:05:55.560 --> 01:06:02.040]  хотим, чтобы эта последовательность была функцией распределения. И здесь существует тонкость. Как
[01:06:02.040 --> 01:06:09.400]  говорят, есть нюанс. Для того, чтобы с ним разобраться, давайте еще одну сделаем полезную упражнение.
[01:06:09.400 --> 01:06:19.200]  Пусть у нас случайная величина равномерно распределена на минус аа. Найдем мы характеристическую
[01:06:19.200 --> 01:06:27.720]  функцию. Значит, это единица делить на 2а, это плотность интегрирования от минус а до а,
[01:06:27.720 --> 01:06:37.160]  а не от минуса бесконечности до бесконечности, е в степени itx dx. Ну опять комплексная часть
[01:06:37.160 --> 01:06:52.880]  обнуляется и получается интеграл cos tx от минус а до а единица делить на 2а dx. Это у нас
[01:06:52.880 --> 01:07:08.840]  что такое получается. Получается sin tx делить на 2at от минус а до а. Ну и получается это sin at
[01:07:08.840 --> 01:07:20.560]  делить на at. Правда, мы тут незаметно воспомнились тем, что t не равно 0. Ну а когда t равно 0,
[01:07:20.560 --> 01:07:35.400]  то всегда единица. Ну а теперь можем примером проиллюстрировать особенности, которые могут
[01:07:35.400 --> 01:07:44.240]  возникать при предельных переходах в пространстве функций распределения и в пространстве характеристических
[01:07:44.240 --> 01:08:00.920]  функций. Значит, давайте рассмотрим как раз для примера. Значит, такой случай, ну ксиен имеет
[01:08:00.920 --> 01:08:11.240]  равномерное распределение на минус нn. Тогда характеристическая функция phi на t будет иметь
[01:08:11.240 --> 01:08:26.480]  вид sin nt делить на nt. При t не равно 0. При n, стремящемся к бесконечности, смотрим вот этот предельный
[01:08:26.480 --> 01:08:36.680]  переход в пространстве образов. Это стремится к чему? К нулю. Для любого t не равно 0. Ну а когда t
[01:08:36.680 --> 01:08:44.280]  равно 0, это единица. Давайте посмотрим, как ведет себя последовательность функции распределения.
[01:08:44.280 --> 01:08:57.760]  Я нарисую одну минус нn, единица, одна вторая. Вот функции распределения, случайно влечены равномерно
[01:08:57.760 --> 01:09:02.920]  распределенные на минус нn. При n, стремясь к бесконечности, поточнее это к чему сходится?
[01:09:02.920 --> 01:09:30.720]  Функции f от x. Поточечный предел, прям стремясь к бесконечности такой функции. Что? Поточечный
[01:09:30.720 --> 01:09:42.320]  предел. В каждой точке к чему эта функция стремится, если t начинает разъезжаться? К 1 и 2. И это не
[01:09:42.320 --> 01:09:50.000]  функция распределения. Видите? Сходимость-то есть, но не к функции распределения. Это нас не устраивает,
[01:09:50.000 --> 01:09:58.120]  и поэтому мы хотели бы знать, откуда это возникло. Ну собственно, почему? А вот ровно поэтому,
[01:09:58.200 --> 01:10:04.640]  потому что характеристическая функция у вас разрывна в нуле. Вот если этого нет,
[01:10:04.640 --> 01:10:10.920]  то это соответствующий теоретический результат. То есть если сходимость к функции непрерывной в нуле,
[01:10:10.920 --> 01:10:17.080]  то гарантируется, что функции распределения имеют пределом в функцию распределения,
[01:10:17.080 --> 01:10:21.480]  по крайней мере, во всех точках непрерывности последней.
[01:10:21.480 --> 01:10:35.480]  Итак, значит, если мы хотим воспользоваться аппаратом характеристических функций для
[01:10:35.480 --> 01:10:42.720]  исследования симптотики случайных величин функции распределения, то мы должны смотреть, чтобы предел
[01:10:42.720 --> 01:10:49.000]  у нас здесь был непрерывный в нуле. Вот если он непрерывный в нуле, тогда можно и обратное
[01:10:49.000 --> 01:10:53.480]  преобразование взять, и станет понятно, куда поточно сходится функция распределения.
[01:10:53.480 --> 01:11:03.560]  Вот после этого последнего замечания давайте перейдем к доказательству. Такого тоже вполне
[01:11:03.560 --> 01:11:12.880]  уже результата, хотя мы можем этого не делать. Мы с вами сейчас докажем теорему мавролапласа,
[01:11:13.000 --> 01:11:20.500]  она будет следовать из десятка результатов, которые мы получим позже, но для того,
[01:11:20.500 --> 01:11:24.160]  чтобы продемонстрировать, как работает аппарат характеристических функций,
[01:11:24.160 --> 01:11:32.320]  давайте это доказательство проведем сейчас с помощью аппарата характеристических функций.
[01:11:32.320 --> 01:11:46.440]  Так, пусть мюен принадлежит биномиальному распределению с параметрами n и p. Ну,
[01:11:46.440 --> 01:11:55.240]  например, подбрасывание монеты, любая схема подойдет. Значит, мотожадание мюен чему равно?
[01:11:55.240 --> 01:12:16.720]  n на p. А дисперсия мюен? n, p, q. Теперь давайте введем случайную величину,
[01:12:16.720 --> 01:12:28.960]  ну так наша любимая буква это n, которая есть мюен минус мотожадание мюен и делить на корень
[01:12:28.960 --> 01:12:38.280]  квадратный из дисперсии мюен, что на самом деле означает мюен минус np делить на корень квадратный
[01:12:38.280 --> 01:12:50.680]  из npq. Мотожадание это n, чему равно? 0. Вычли из случайной величины и мотожадания. Дисперсия
[01:12:50.680 --> 01:12:58.240]  это n, чему равно? 1. Совершенно справедливо. Вот такие случайные величины, иногда называют
[01:12:58.240 --> 01:13:02.960]  нормированными. То есть, если из любой случайной величины вычислить все мотожадание и поделить на
[01:13:02.960 --> 01:13:09.720]  корень дисперсии, то получившаяся случайная величина будет иметь нулевое мотожадание и единичную дисперсию.
[01:13:09.720 --> 01:13:17.520]  Ну и давайте найдем характеристическую функцию это n, которую я обозначу.
[01:13:17.520 --> 01:13:34.080]  По определению, это математическое ожидание e в степени it мюен минус e мюен делить на корень
[01:13:34.080 --> 01:13:45.480]  квадратный npq. По свойствам характеристической функции константу можно вынести e в степени
[01:13:45.480 --> 01:14:06.160]  минус it. Тут я поставил значение, а тут поставил мат ожидания. Тут p, np и tpn на корень
[01:14:06.160 --> 01:14:16.440]  квадратный из npq. И на характеристическую функцию мюенная, умноженную на константу
[01:14:16.440 --> 01:14:23.280]  единиц на корень из npq. По свойствам характеристическая функция, это значение
[01:14:23.280 --> 01:14:32.560]  характеристической функции мюенная, взятое в точке t делить на корень из npq.
[01:14:32.560 --> 01:14:36.360]  Правильно, да?
[01:14:36.360 --> 01:14:40.240]  Равно.
[01:14:54.160 --> 01:14:56.000]  Равно.
[01:14:56.000 --> 01:15:06.800]  E в степени i минус it делить на корень квадратный npq в степени n. А характеристическая функция,
[01:15:06.800 --> 01:15:16.920]  вот это, я ее стерано здесь было написано, это q плюс p на e в степени it. То есть на e в степени
[01:15:16.920 --> 01:15:28.880]  it на корень из npq. Так, а вот здесь, извините, я забыл b.
[01:15:28.880 --> 01:15:44.760]  И в степени n. Пока мы просто пользуемся тем, на что обратили внимание, когда выписывали
[01:15:44.760 --> 01:15:55.800]  свойства. Ну что, одинаковые степени. Вношу под скобки. Получаю q на e в степени минус itp
[01:15:55.800 --> 01:16:10.320]  на корень из npq плюс p. А здесь смотрим it минус itp. Это it единицы минус p. Это q. Значит itq делить
[01:16:10.320 --> 01:16:20.600]  на корень квадратный из npq. Все это в степени n. Равно. Равно. Разлагаем экспоненту комплексный
[01:16:20.600 --> 01:16:38.200]  в ряд. q единица минус itp на корень из npq. t в квадрате пополам минус t квадрат пополам
[01:16:38.680 --> 01:16:50.120]  на p квадрат на npq. И плюс о малый от т квадрата на 2, а на npq. Это и плюс о малый от единицы на
[01:16:50.120 --> 01:17:01.600]  e, но я чуть попозже, ну в смысле в конце напишу. Плюс p то же самое. Единица плюс itq корень
[01:17:01.600 --> 01:17:16.280]  npq тот же самый минус t квадрат пополам только q квадрат на npq. Плюс о мало единицы на n и все
[01:17:16.280 --> 01:17:36.760]  это в степени n. Ну осталось две строчки у нас буквально. Значит раскрываем скобки p плюс q это
[01:17:36.760 --> 01:17:46.160]  единица. Вот это член смотри чего будет представлять со знаком минус itpq делить на npq корень, а вот это
[01:17:46.160 --> 01:17:57.560]  член со знаком плюс itqp на корень из pq. То есть они сокращаются. Значит здесь вот это q с этим q
[01:17:57.560 --> 01:18:04.280]  сокращается, это p с этим p сокращается. Получается t квадрат на 2np со знаком минус,
[01:18:04.280 --> 01:18:12.080]  здесь pp сокращается, qq сокращается. t квадрат делить на 2nq со знаком плюс. p
[01:18:12.080 --> 01:18:22.440]  плюс q равно единицы значит это просто минус t квадрат на 2n плюс о мало единицы на n в степени n.
[01:18:22.440 --> 01:18:44.120]  Придел. Просто чудесным образом совпало с характеристической функцией стандартного
[01:18:44.120 --> 01:18:49.600]  нормального распределения. Она непрерывна в нуле и поэтому мы отсюда делаем вывод,
[01:18:49.600 --> 01:19:04.120]  что вероятность того, что это n будет меньше x стремится к функции распределения стандартного
[01:19:04.120 --> 01:19:11.080]  нормального распределения. То есть вот к такому интегралу минус бесконечности до x единица на
[01:19:11.080 --> 01:19:22.560]  корень из 2p есть степень минус u квадрат пополам на du. Причем поскольку предельная функция не
[01:19:22.560 --> 01:19:28.360]  убывающая ограниченная, то поточечная сходимость стремится поточечна. Означает равномерную
[01:19:28.360 --> 01:19:41.440]  сходимость. Знаете такой факт. Ну вот собственно мы с вами доказали результат вот этот вот,
[01:19:41.440 --> 01:19:51.480]  который называется теоремы муавролапласа и показывает, что усреднение с нормировкой
[01:19:51.480 --> 01:19:59.560]  результатов экспериментов в схеме Бернуле при большом n по сути дела совпадает со стандартной
[01:19:59.560 --> 01:20:05.520]  нормальной случайной величиной. Это первый наш пример, когда мы сталкиваемся с тем,
[01:20:05.520 --> 01:20:11.600]  что концентрация меры в окрестности нормального распределения. То есть сходится к нормальному
[01:20:11.720 --> 01:20:19.760]  распределению. Так, ну тогда все. Давайте так как-то не успеваем, ну будем стараться.
[01:20:19.760 --> 01:20:24.280]  Спасибо коллеге, давайте отдыхайте. Благодарю вас.
[01:20:41.600 --> 01:20:43.600]  С вами был Игорь Негода.

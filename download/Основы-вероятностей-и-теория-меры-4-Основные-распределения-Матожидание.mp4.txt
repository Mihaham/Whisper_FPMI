[00:00.000 --> 00:14.400]  Это у нас четвертая, по-моему, да?
[00:14.400 --> 00:17.040]  Быстро вспоминаем, на чем мы остановились.
[00:17.040 --> 00:22.760]  Базовое понятие теории вероятности распределения.
[00:22.760 --> 00:35.520]  Я напомню, что в дискретном случае распределением,
[00:35.520 --> 00:50.400]  дискретным распределением мы называем соответствием
[00:50.400 --> 00:55.600]  между значениями и вероятностями этих значений, здесь у нас
[00:55.600 --> 00:59.440]  соответственно ПК это число больше или равное нулю и сумма
[00:59.440 --> 01:03.240]  по всем возможным К у нас должна быть одна единица.
[01:03.240 --> 01:05.200]  Хк это просто какие-то значения.
[01:05.200 --> 01:10.720]  Соответственно, когда мы говорим о распределении
[01:10.720 --> 01:17.280]  случайной величины, распределении случайной величины, у нас
[01:17.280 --> 01:22.520]  возникает понятие, ну, иногда называют фазовым пространством,
[01:22.520 --> 01:24.760]  то есть множество значений случайной величины кси.
[01:24.760 --> 01:34.680]  Соответственно, если у нас дискретная теория вероятностей,
[01:34.680 --> 01:36.800]  а мы пока понимаем, что работать мы можем только
[01:36.800 --> 01:39.760]  с ней, потому что для других вариантов у нас математического
[01:39.760 --> 01:45.560]  аппарата не хватает, то соответственно у вас множество значений
[01:45.560 --> 01:47.200]  случайной величины кси не более чем счетно.
[01:47.200 --> 02:00.600]  И тогда у нас соответственно Хк по всем К это и есть множество
[02:00.600 --> 02:02.960]  значений случайной величины кси, то есть это те значения,
[02:02.960 --> 02:07.040]  которые принимает случайная величина, а Пк это есть вероятность
[02:07.040 --> 02:10.240]  того, что случайная величина кси принимает значение
[02:10.240 --> 02:11.240]  хк.
[02:11.240 --> 02:14.720]  То есть я к чему?
[02:14.720 --> 02:18.600]  К тому, что понятие распределения не привязано к случайной
[02:18.600 --> 02:20.240]  величине, оно может быть само по себе.
[02:20.240 --> 02:23.880]  Ну или в контексте, что же мы берем распределение
[02:23.880 --> 02:25.680]  к какой-то конкретной случайной величине.
[02:25.680 --> 02:29.600]  Теперь быстро пройдемся по тем распределениям,
[02:29.600 --> 02:31.680]  с которыми вы работаете сейчас на семинарах.
[02:31.680 --> 02:38.560]  Заодно вспомним, какие у них обозначения.
[02:38.560 --> 02:55.800]  Так, ну погнали.
[02:55.800 --> 03:04.200]  Соответственно первое распределение Бернулевское.
[03:04.200 --> 03:06.600]  Бернулевское распределение.
[03:06.600 --> 03:09.160]  Совершенно беспонтовое, ну потому что у нас здесь
[03:09.160 --> 03:20.320]  к принимает всего два значения, 0 и 1, то есть к бывает либо
[03:20.320 --> 03:21.920]  ноликом, либо единичкой.
[03:21.920 --> 03:24.440]  Соответственно П нулевой это у нас единичка минус
[03:24.440 --> 03:29.760]  П, П первой это просто П, где П это чисел К от 0 до
[03:29.760 --> 03:30.760]  1.
[03:30.760 --> 03:35.720]  Соответственно обозначение для этого распределения
[03:35.720 --> 03:38.720]  Берн П, потому что понятно, что параметр этого распределения
[03:38.720 --> 03:39.720]  только 1.
[03:39.720 --> 03:42.480]  Это вот это П, физический смысл этой чиселки это
[03:42.480 --> 03:44.640]  вероятность успеха.
[03:44.640 --> 03:50.240]  То есть у нас один эксперимент, соответственно ему соответствуют
[03:50.240 --> 03:53.360]  два значения 0 или 1, вероятность единички П, вероятность
[03:53.360 --> 03:54.760]  0 к единичке минус П.
[03:54.760 --> 03:55.920]  Ну неинтересно.
[03:55.920 --> 03:58.800]  Два значения, две вероятности.
[03:58.800 --> 04:04.280]  Второе было веселее, это биномиальное распределение.
[04:04.880 --> 04:10.240]  Теперь смотрите, фамилий тут немного, они постоянно
[04:10.240 --> 04:14.400]  перекрываются, поэтому главное смотрите на существительное,
[04:14.400 --> 04:15.400]  которое идет.
[04:15.400 --> 04:21.840]  Биномиальное распределение тут уже повеселее, тут значения
[04:21.840 --> 04:33.240]  могут быть от 0 до n, хк это у нас тоже к, а Пк это вероятность
[04:33.240 --> 04:35.440]  того, что в схеме испытаний вернули, у нас будет ровно
[04:35.440 --> 04:36.440]  к успеху.
[04:36.440 --> 04:42.440]  То есть опять у вас есть значения и вероятности
[04:42.440 --> 04:43.920]  этих значений.
[04:43.920 --> 04:48.240]  Обозначается оно традиционно, причем вот прям вот традиционно
[04:48.240 --> 04:53.120]  в том плане, что в литературе это частое обозначение,
[04:53.120 --> 04:54.120]  биномиальное nP.
[04:54.120 --> 04:55.520]  Потому что понятно, что у этого распределения у
[04:55.520 --> 04:59.120]  вас два параметра, это n, число испытаний и P это вероятность
[04:59.120 --> 05:00.120]  успеха.
[05:00.120 --> 05:07.240]  Следующее распределение геометрическое.
[05:07.240 --> 05:16.080]  Распределение номера первого успеха в схеме испытаний
[05:16.080 --> 05:18.480]  вернули, если мы не ограничиваем число испытаний.
[05:18.480 --> 05:26.360]  Соответственно k здесь у нас 1, 2, может быть любое
[05:26.360 --> 05:27.960]  далее число натуральное.
[05:27.960 --> 05:32.600]  Я говорю, тут есть расхождение, даже если в Википедию залезть,
[05:32.600 --> 05:34.640]  там как бы два варианта для геометрического распределения,
[05:34.640 --> 05:38.080]  это номер первого успеха или число неудач перед
[05:38.080 --> 05:39.080]  первым успехом.
[05:39.080 --> 05:42.600]  Вещи связаны, но тогда если это число неудач перед
[05:42.600 --> 05:44.520]  первым успехом, у вас тут от нуля будет начинаться.
[05:44.520 --> 05:49.560]  Просто каждый раз, если там будет что-то, вы уточняете
[05:49.560 --> 05:51.320]  это, которое геометрическое.
[05:51.320 --> 05:53.360]  Но поскольку все знают эту путаницу, всегда в условиях
[05:53.360 --> 05:55.240]  задачи там уточняется.
[05:56.080 --> 06:03.000]  Ну вот, соответственно, хк у нас снова к, а вероятности
[06:03.000 --> 06:05.440]  у нас, мы с вами их считали, там было единичка минус
[06:05.440 --> 06:09.360]  п в степени k минус 1, то есть у вас k минус 1 было неудача
[06:09.360 --> 06:10.360]  и потом первый успех.
[06:10.360 --> 06:17.120]  Во всех этих случаях мы проверяли вот эту вот,
[06:17.120 --> 06:19.960]  то что сумма всех пк, по всему возможному k будет равна
[06:19.960 --> 06:20.960]  единичке, все хорошо.
[06:21.920 --> 06:24.720]  И потом неожиданный отряд, мы ввели Пуасоновская,
[06:24.720 --> 06:33.840]  да здесь, тут геомпе, тоже там литература такой встречается.
[06:33.840 --> 06:35.000]  Параметра у нас только один.
[06:35.000 --> 06:43.200]  И четвертое это Пуасоновская, Пуасоновская, Пуас, господи,
[06:43.200 --> 06:49.520]  как подхреново, извините, низко пишу, Пуасоновская.
[06:49.520 --> 06:53.520]  Которая, соответственно, к у нас здесь опять целая
[06:53.520 --> 07:07.440]  от нуля, хк это снова к, а вероятности задаются
[07:07.440 --> 07:11.720]  таким выражением, лямда в степени k делить на кофактериона
[07:11.720 --> 07:13.160]  и е в степени минус лямда.
[07:13.160 --> 07:16.240]  Лямда это какая-то положительная чиселка, которая и будет
[07:16.240 --> 07:19.920]  в параметрах распределения, обозначаться оно будет вот
[07:19.920 --> 07:20.920]  так.
[07:20.920 --> 07:23.560]  Пуасоновская с параметром лямда.
[07:40.000 --> 07:42.560]  Законный вопрос, который в этот момент возникает,
[07:42.560 --> 07:45.480]  что это за бред, то есть мы так смотрим.
[07:45.880 --> 07:48.360]  Ну, здесь действительно все хорошо, это действительно
[07:48.360 --> 07:50.840]  распределение, у вас есть зависимость между значениями
[07:50.840 --> 07:53.880]  и вероятностями, вероятности все не отрицательные, ну
[07:53.880 --> 07:56.680]  лямда больше нуля все хорошо, сумма действительно равна
[07:56.680 --> 08:00.440]  единице, потому что видно то, что это и есть ретриоризовожение
[08:00.440 --> 08:02.840]  экспонента, просто нормирующий множеством ле в степени
[08:02.840 --> 08:03.840]  минус лямда.
[08:03.840 --> 08:04.840]  Это же видно?
[08:04.840 --> 08:05.840]  Да.
[08:05.840 --> 08:06.840]  Хорошо.
[08:06.840 --> 08:09.160]  Ну а дальше возникает вопрос, откуда вообще эта фигня
[08:09.160 --> 08:10.160]  взялась.
[08:10.880 --> 08:16.360]  Соответственно, разобраться в этом вопросе нам помогает
[08:16.360 --> 08:17.360]  теория М.Пуасон.
[08:23.360 --> 08:24.360]  Теория М.Пуасон.
[08:28.360 --> 08:29.360]  Ну погнали.
[08:29.360 --> 08:31.800]  Пускай у нас случайно лечена ксиентая, распределена
[08:31.800 --> 08:36.600]  биномиальна с параметрами N и PmT.
[08:37.040 --> 08:40.880]  Соответственно, когда вы будете задавать в тексте
[08:40.880 --> 08:43.200]  или условиям будет сказано, что случайная величина
[08:43.200 --> 08:46.520]  имеет какое-то распределение, это будет знак эквивалентности
[08:46.520 --> 08:50.160]  и дальше то обозначение распределения, которое я
[08:50.160 --> 08:51.160]  выписал.
[08:51.160 --> 08:54.960]  Подозревается то, что у меня N бегает по натуральным
[08:54.960 --> 08:58.000]  числам, то есть для каждого натурального числа у меня
[08:58.000 --> 09:01.840]  есть своя случайная величина ксиенты, которая имеет биномиальное
[09:01.840 --> 09:05.000]  распределение с такими параметрами.
[09:05.400 --> 09:07.600]  При этом вот этот индекс, что значит, то есть у вас
[09:07.600 --> 09:10.680]  P как-то зависит от N, при этом известно то, что N умножить
[09:10.680 --> 09:14.360]  на Pm будет стремиться к лямду, которая больше 0 при N, стремящемся
[09:14.360 --> 09:15.360]  к бесконечности.
[09:15.360 --> 09:19.840]  То есть последовательность Pm, это последовательность
[09:19.840 --> 09:22.040]  чисел, которая удовлетворяет нашим требованиям, то есть
[09:22.040 --> 09:28.480]  это от 0 до 1, да, вот отсюда, это числа от 0 до 1, при этом
[09:28.480 --> 09:30.760]  видно, что они стремятся к 0, причем так стремятся
[09:30.760 --> 09:32.600]  к 0, что вот это произведение стремится к положительной
[09:32.600 --> 09:33.600]  чиселке лямды.
[09:33.600 --> 09:37.560]  Ага, что утверждается в этом случае, что вероятность
[09:37.560 --> 09:41.600]  того, что ксиента будет равна K, ну, любому натуральному
[09:41.600 --> 09:52.880]  К, ну вот, у нас 0 натуральный, ну ладно, давайте я так
[09:52.880 --> 09:58.000]  напишу, натуральный источник с 0, может быть 0 входит,
[09:58.000 --> 09:59.000]  тогда всё равно не важно.
[09:59.000 --> 10:02.560]  В общем, это будет стремиться при стремящемся к бесконечности
[10:02.720 --> 10:05.520]  к лямду в степени K, делить на K factorial на E в степени
[10:05.520 --> 10:06.520]  минус лямда.
[10:06.520 --> 10:12.440]  Теория очень простая, доказывается быстро.
[10:12.440 --> 10:16.320]  Давайте просто честно выпишем вот эту вот вероятность.
[10:16.320 --> 10:18.440]  Ведь это что?
[10:18.440 --> 10:23.320]  Это наша вот эта вот ПКТ, да, вот отсюда, то есть это
[10:23.320 --> 10:25.800]  вероятность того, что наша случайная личина принимает
[10:25.800 --> 10:30.600]  значение K, да, где, вот оно, наша случайная личина
[10:30.600 --> 10:33.960]  принимает вот это вот наше значение ХК, которое у нас
[10:33.960 --> 10:34.960]  сейчас просто есть К.
[10:34.960 --> 10:40.600]  Ну, то есть это просто С из Н по К, ПНТ в степени
[10:40.600 --> 10:44.440]  К, на единичку минус ПНТ в степени N-K.
[10:44.440 --> 10:49.200]  Я могу врать, поэтому аккуратно следите, что я тут не обманываю
[10:49.200 --> 10:50.200]  вас.
[10:50.200 --> 10:54.440]  Распишем вермиальный коэффициент, что у нас там, N factorial, K factorial
[10:54.440 --> 10:58.840]  на N-K factorial, ну, N factorial и N-K factorial сократим, подразумевается
[10:58.840 --> 10:59.840]  что?
[10:59.840 --> 11:04.120]  K у вас сейчас фиксированная чиселка, N стремится к бесконечности.
[11:04.120 --> 11:08.280]  Поэтому там K factorial оставим в покое, вот здесь внизу, да,
[11:08.280 --> 11:09.400]  у нас останется что?
[11:09.400 --> 11:15.800]  Там получается произведение N на N-1 и так далее, последний
[11:15.800 --> 11:18.480]  множитель будет N-K плюс 1.
[11:18.480 --> 11:19.480]  Не водил?
[11:19.480 --> 11:20.480]  Не водил.
[11:20.480 --> 11:24.840]  Так, соответственность теперь вот здесь.
[11:24.840 --> 11:25.840]  Давайте посмотрим.
[11:25.840 --> 11:29.080]  Причем, что я знаю про ПНТ, я знаю вот эту вот вещь,
[11:29.080 --> 11:33.000]  то есть ПНТ хороша в связке с N, ну окей, давайте это
[11:33.000 --> 11:34.000]  и сделаем.
[11:34.000 --> 11:36.840]  То есть ПНТ умножить на N в степени K, поскольку
[11:36.840 --> 11:40.120]  я давно добавил N в степени K, я на него поделю.
[11:40.120 --> 11:46.600]  Так, а здесь мы оставляем единичку минус ПНТ в степени
[11:46.600 --> 11:47.600]  N-K.
[11:47.600 --> 11:53.000]  Хорошо, мне не нравится вот это N-K, поэтому давайте
[11:53.000 --> 11:59.040]  перепишем вот так вот.
[11:59.040 --> 12:01.880]  Сейчас все нормально, да?
[12:01.880 --> 12:04.920]  Давайте еще перепишем, чтобы стало совсем хорошо.
[12:04.920 --> 12:11.520]  У меня здесь в числителе K множителей, в знаменателе
[12:11.520 --> 12:16.080]  K множителей, ну в эту степень, я сейчас как бы эту степень
[12:16.080 --> 12:18.320]  раздербаню на множители по одному множителю в каждую
[12:18.320 --> 12:21.520]  скобку запихну, причем скобка конечное число, у меня K
[12:21.520 --> 12:24.440]  фиксированная чиселка, она к N никак не привязана,
[12:24.440 --> 12:26.560]  и у меня получится конечное число множителей, поэтому
[12:26.560 --> 12:28.440]  арифметика проделывается, тут будет работать.
[12:28.440 --> 12:33.080]  У меня что получится, единичка, единичка минус 1 НТ, единичка
[12:33.080 --> 12:40.160]  минус 2 НТ, последнее будет единичка минус K минус 1 НТ.
[12:40.160 --> 12:41.160]  Не наврал?
[12:41.160 --> 12:42.160]  Не наврал.
[12:42.160 --> 12:48.200]  Дальше у меня идет PНТ умножить на N в степени K, единичка
[12:48.200 --> 12:51.840]  минус PНТ в степени K.
[12:51.840 --> 12:54.520]  Числитель я пока писать не буду, давайте сейчас посмотрим,
[12:54.520 --> 12:55.520]  что у нас происходит.
[12:55.520 --> 13:02.400]  Это чиселка, все нормально, вот это конечное произведение
[13:02.400 --> 13:06.080]  скобок, каждый из которых сходится к единице, это
[13:06.080 --> 13:08.400]  видно, то есть это к одному.
[13:08.400 --> 13:12.200]  Смотрим сюда, здесь у нас конечная степень, ну конечная
[13:12.200 --> 13:17.120]  в смысле она от N не зависит, не растет, а в основании
[13:17.120 --> 13:20.360]  степени у нас последовательность, которая сходится к лямбде,
[13:20.360 --> 13:23.440]  значит вся вот эта дырень сойдется к лямбде в степени
[13:23.440 --> 13:24.440]  K.
[13:24.440 --> 13:30.200]  Так, теперь смотрим на знаменатель, на это, что такое PНТ?
[13:30.200 --> 13:32.720]  Это последовательность, которая стремится к нулю,
[13:32.720 --> 13:33.720]  это очевидно.
[13:33.720 --> 13:37.440]  Если это произведение стремится к конечной чиселке, N бесконечно
[13:37.440 --> 13:39.520]  большая, значит PНТ должна быть бесконечно малой.
[13:39.520 --> 13:44.680]  Единичка минус бесконечно малая, в конечной степени
[13:44.800 --> 13:46.520]  все нормально, это у нас сойдет к единичке.
[13:46.520 --> 13:49.840]  То есть единственный вопрос, который остался, который
[13:49.840 --> 13:52.680]  сложный для нас, это что будет происходить с вот
[13:52.680 --> 13:54.480]  этой вот дыренью?
[13:54.480 --> 13:57.920]  На что это похоже?
[13:57.920 --> 13:58.920]  Который?
[13:58.920 --> 14:04.600]  Ну они там первый, второй, ну с Ешкой, это похоже на
[14:04.600 --> 14:06.360]  замечательный предел с Ешкой.
[14:06.360 --> 14:08.600]  Давайте подгоним так, чтобы было совсем похоже, что
[14:08.600 --> 14:09.600]  у нас там было?
[14:09.600 --> 14:13.160]  У нас там было N знаменатели, правильно?
[14:13.960 --> 14:17.840]  А тут мы сделаем PНТ умножить на N.
[14:17.840 --> 14:21.720]  Чтобы стало совсем похоже на замечательный предел,
[14:21.720 --> 14:23.880]  мы показатель сделаем, какой нам надо.
[14:23.880 --> 14:29.560]  N разделить на PНТ умножить на N.
[14:29.560 --> 14:36.960]  Соответственно, вот такие вещи, ну вы нам это не делали,
[14:36.960 --> 14:39.360]  там типа сложные функции для последовательности.
[14:39.760 --> 14:44.600]  Вы же знаете, что единица минус х в степени х, у вас
[14:44.600 --> 14:46.960]  будет стремиться, когда х стремится куда?
[14:46.960 --> 14:52.880]  Вот эта стремится к нулю, значит, первенутая стремится
[14:52.880 --> 14:53.880]  к бесконечности.
[14:53.880 --> 14:56.640]  Ну что, вот эта вот вещь у вас будет стремиться
[14:56.640 --> 14:57.640]  к Е в минус первый.
[14:57.640 --> 15:03.440]  Было такое, ну это делали.
[15:03.440 --> 15:05.840]  А еще вот эту вот хрень, ну чтобы равенство было
[15:05.840 --> 15:07.880]  верно, мне нужно возрести в какую степень, у меня же
[15:07.880 --> 15:10.120]  возводилось в степень N, и я добавил вот эту вот
[15:10.120 --> 15:11.120]  дрянь.
[15:11.120 --> 15:13.800]  А вот эта дрянь стремится к лямбде.
[15:13.800 --> 15:18.440]  На мотоне вы должны аккуратно были доказывать, что так
[15:18.440 --> 15:20.560]  переходить к пределам можно.
[15:20.560 --> 15:21.560]  Было такое.
[15:21.560 --> 15:22.560]  Ну было, поэтому.
[15:22.560 --> 15:26.360]  Ну вот, теперь давайте собирать, что в итоге у нас получилось.
[15:26.360 --> 15:29.920]  Это к единице неважно, это к единице неважно.
[15:29.920 --> 15:32.200]  У нас каф факториал в знаменателе, лямбда в степени каф в
[15:32.200 --> 15:33.200]  числителе.
[15:33.200 --> 15:40.520]  И вот этот числитель, е в минус первый и еще лямбда,
[15:40.520 --> 15:43.520]  и того е в степени минус лямбда.
[15:43.520 --> 15:48.520]  Все.
[15:48.520 --> 16:00.960]  В чем ценность данного результата?
[16:00.960 --> 16:09.760]  Пока я стираю, вы пытаетесь придумать ответ на этот
[16:09.760 --> 16:10.760]  вопрос.
[16:10.760 --> 16:11.760]  Да.
[16:11.760 --> 16:20.800]  То есть, у вас есть вот ксиенты, последовательность
[16:20.800 --> 16:24.600]  случайных величин, которые так распределены, при этом
[16:24.600 --> 16:27.520]  вот этот вот параметр, который от N зависит, ведет себя
[16:27.520 --> 16:29.040]  так, что вот это.
[16:29.040 --> 16:32.240]  Запитая тогда, будет везать вот следующую историю.
[16:32.240 --> 16:49.320]  В чем ценность, она вообще есть?
[16:49.320 --> 16:52.280]  Мы объяснили, откуда взялись вот эти непонятные штуки.
[16:52.280 --> 16:54.400]  И почему мы именно экспоненту начали раскладывать в
[16:54.400 --> 16:57.040]  ретрибер, а не логарифм, например.
[16:57.040 --> 17:01.600]  Я думаю, многим логарифм нравится, это же такая субъективная
[17:01.600 --> 17:02.600]  история.
[17:02.600 --> 17:03.600]  Кому что нравится.
[17:03.600 --> 17:04.600]  Вот.
[17:04.600 --> 17:05.600]  Есть ли еще вариантов?
[17:05.600 --> 17:06.600]  Наверное, вариантов нет.
[17:06.600 --> 17:07.600]  Смотрите.
[17:07.600 --> 17:15.360]  Тервер ведь как бы история очень древняя, то есть,
[17:15.360 --> 17:19.160]  как бы вопрос о ксерматизации, он был решен недавно.
[17:19.160 --> 17:23.880]  То есть, как бы Гильберт это сформулировал, проблему
[17:23.880 --> 17:26.360]  как бы Комагоров ее зафиксировал.
[17:26.360 --> 17:29.760]  Нельзя сказать, что Комагоров решил, там было много шагов
[17:29.760 --> 17:32.680]  к этому решению, просто он как бы все оформил окончательно.
[17:32.680 --> 17:40.400]  Но тервер, наука древняя и вот все теми вещами, которые
[17:40.400 --> 17:42.200]  мы занимаемся, занимались давно.
[17:42.200 --> 17:46.920]  И в частности, вопрос с бенемиальным распределением
[17:46.920 --> 17:48.600]  он достаточно простой.
[17:48.600 --> 17:52.440]  То есть, предположим, у вас есть какое-то производство
[17:52.440 --> 17:54.600]  или ловля, я не знаю, что-нибудь.
[17:55.280 --> 18:02.000]  По следовательности испытаний, предположим, вот станок
[18:02.000 --> 18:05.080]  у вас работает, форд какой-нибудь древний.
[18:05.080 --> 18:08.840]  Станок хороший, но все-таки он иногда сбои дает.
[18:08.840 --> 18:12.720]  Иногда у вас проскакивает бракованный товар.
[18:12.720 --> 18:15.040]  Будем считать, что брак – это успех у нас.
[18:15.040 --> 18:19.240]  И в этом случае у вас вероятность, она предположим, какая-нибудь
[18:19.240 --> 18:20.240]  вот такая.
[18:20.240 --> 18:23.080]  То есть, поя маленькая, это где-то 1,2.
[18:23.560 --> 18:27.120]  И дальше возникает вопрос о том, какая вероятность
[18:27.120 --> 18:31.120]  то, что в пароте, состоящей из тысячи изделий, брака
[18:31.120 --> 18:34.800]  будет, предположим, меньше или равно трех.
[18:37.640 --> 18:39.520]  Вроде как простая же история.
[18:39.520 --> 18:41.160]  То есть, у вас что получается?
[18:41.160 --> 18:45.240]  Случайно чинок С имеет бенемиальное распределение, где у вас
[18:45.240 --> 18:51.200]  тысячи было испытаний, а вероятность успеха – 1,2.
[18:51.200 --> 18:55.640]  И вы просто смотрите вероятность того, что кси меньше или
[18:55.640 --> 18:56.640]  равно, чем 3.
[18:56.640 --> 18:58.840]  Вам что для этого нужно сделать?
[18:58.840 --> 19:02.640]  Вам нужно просуммировать пока от нуля до трех, что
[19:02.640 --> 19:13.640]  С из тысячи по три на 1,2 в степени 3 на 1,2.
[19:14.080 --> 19:27.080]  Потом 1,2 в степени 3 на 199,2.
[19:27.080 --> 19:29.920]  Так, тут к, а тут тысячи минус к.
[19:29.920 --> 19:33.080]  Так, сейчас проверяем, я вру или не вру.
[19:33.080 --> 19:41.480]  Ну да, но это невозможно посчитать объективно.
[19:41.480 --> 19:45.600]  Нам с машинами как бы легко, для машины это достаточно
[19:45.600 --> 19:46.600]  простая история.
[19:46.600 --> 19:49.880]  Вы представляете, вот это, если у вас k равно единичке,
[19:49.880 --> 19:52.080]  то есть, возводить такое число в 999 степени – это
[19:52.080 --> 19:53.080]  как?
[19:53.080 --> 19:59.600]  То есть, как получить вот эту вот чиселку?
[19:59.600 --> 20:03.640]  Дальше приходит на помощь терраэмплассон, который
[20:03.640 --> 20:04.640]  говорит о чем?
[20:04.640 --> 20:07.880]  То, что вот эти наши вероятности, они близки к тем штукам.
[20:07.880 --> 20:11.240]  То есть, вместо того, чтобы считать вот эту сумму, можно
[20:12.000 --> 20:13.000]  посчитать вот такую сумму.
[20:13.000 --> 20:16.880]  Сумма по k от 0 до 3, лямбда в степени k делить на k факториально
[20:16.880 --> 20:18.160]  и е в степени минус лямбда.
[20:18.160 --> 20:19.800]  Вопрос только, какую лямбду взять.
[20:19.800 --> 20:31.480]  Ну, n умножить на pn, то есть, n это у нас тысячи, а pn
[20:31.480 --> 20:32.480]  это у нас 1,2.
[20:32.480 --> 20:41.080]  Ну, то есть, лямбда у нас получается, сколько, 5?
[20:41.080 --> 20:43.280]  Потому что посчитать вот эти вот вещи по таблицам
[20:43.280 --> 20:47.720]  радиса, ну, как бы, е в степени минус 5, 5 я думаю мы как-то
[20:47.720 --> 20:50.880]  возведем в степень, там сколько, от 0 до 3, факториально
[20:50.880 --> 20:53.080]  посчитаем, ну, е по таблицам радиса мы найдем.
[20:53.080 --> 20:55.720]  Ну, по крайней мере, сумму из этих четырех слагаемых
[20:55.720 --> 20:56.920]  мы посчитаем хорошо.
[20:56.920 --> 21:00.920]  Понятно, что для как бы исследователей, которые
[21:00.920 --> 21:02.880]  занимались этими вещами до как бы появления числительных
[21:02.880 --> 21:04.880]  машин, это было некоторое спасение.
[21:04.880 --> 21:10.280]  Вместо бенемиального распределения использовать плацоновское
[21:10.280 --> 21:11.280]  распределение.
[21:11.280 --> 21:12.640]  То есть, идея плацоновского распределения простая,
[21:12.640 --> 21:15.360]  это приближение бенемиального распределения, которое позволяет
[21:15.360 --> 21:18.160]  вот эти вот хрень хоть как-то читать, оценивать.
[21:18.160 --> 21:22.240]  Какая реакция присутствующих на мой текст, который я
[21:22.240 --> 21:24.240]  произнес, пока я стираю, вы можете его произнести.
[21:24.240 --> 21:34.480]  Че, никакой, или вы подождете, пока я к вам повернусь.
[21:34.480 --> 21:38.280]  Даже не знаю, в какой своей части.
[21:39.280 --> 21:40.280]  Я верю, что лучший.
[21:40.280 --> 21:50.400]  Какая вам радость от этого, но это первая история раз,
[21:50.400 --> 21:52.320]  но предположим то, что у нас нет вычислительных
[21:52.320 --> 21:53.320]  машин.
[21:53.320 --> 21:55.520]  Все-таки какие комментарии по поводу текста, который
[21:55.520 --> 21:56.520]  я произнес до этого.
[21:56.520 --> 22:01.560]  Кажется, что как бы в рете вы, Иван Генрихович, потому
[22:01.560 --> 22:03.640]  что, еще раз, это предельный переход.
[22:03.640 --> 22:07.080]  То есть, да, премьер стремящимся к бесконечности, который
[22:07.080 --> 22:09.960]  на фига не стремится к бесконечности, у нас n фиксированный.
[22:09.960 --> 22:13.200]  Большое, да, но что такое большое вот в этих вопросах.
[22:13.200 --> 22:17.440]  Как бы вы через Маттан проходили, вы понимаете, что как бы
[22:17.440 --> 22:19.920]  существует n большое, начиная с которого мы попадаем
[22:19.920 --> 22:20.920]  в обсвал на крестность.
[22:20.920 --> 22:24.120]  О том, ну существует номер, начиная с которого мы попадаем
[22:24.120 --> 22:25.120]  в обсвал на крестность.
[22:25.120 --> 22:27.800]  Но насколько большой этот номер, никто не знает.
[22:27.800 --> 22:29.640]  Все знают, что он есть.
[22:29.640 --> 22:34.160]  И поэтому применять вот этот результат для этого
[22:34.160 --> 22:36.760]  значения n, ну как-то странно.
[22:37.440 --> 22:38.440]  По меньшей мере.
[22:38.440 --> 22:40.400]  Это то, что вы должны были сказать.
[22:40.400 --> 22:41.400]  Иван Генрихович, это за фигня.
[22:41.400 --> 22:42.400]  Это ясно?
[22:42.400 --> 22:43.400]  Ну вот.
[22:43.400 --> 22:47.120]  Конечно же, все интересуют скорость сходимости в теореме
[22:47.120 --> 22:52.280]  Пуассона и существует уточнение про скорость сходимости
[22:52.280 --> 22:53.280]  в теореме Пуассона.
[22:53.280 --> 22:54.720]  Я сейчас формулирую.
[22:54.720 --> 22:58.680]  Ну, результат самый точный, какой я нашел на данный
[22:58.680 --> 22:59.680]  момент.
[22:59.680 --> 23:00.680]  То есть, он доказан Прохоровым.
[23:00.680 --> 23:02.800]  Ну, Прохоров – это 20 век.
[23:02.800 --> 23:03.800]  Вот.
[23:03.800 --> 23:07.080]  Чтобы просто вы оценили степень.
[23:07.080 --> 23:10.520]  Так, уточнение.
[23:10.520 --> 23:14.440]  Понятно то, что когда они пользовались в 19 веке вот
[23:14.440 --> 23:17.080]  этими результатами, у них тоже было уточнение в теореме
[23:17.080 --> 23:18.080]  Пуассона.
[23:18.080 --> 23:19.920]  Это просто я машу руками для того, чтобы обосновать,
[23:19.920 --> 23:22.880]  почему я вот эту вещь вообще ввожу, чтобы вы увидели
[23:22.880 --> 23:24.440]  хотя бы, что это правда.
[23:24.440 --> 23:27.680]  Понятно то, что математики в 19 веке доказывали очень
[23:27.680 --> 23:30.240]  кондовые результаты, просто у них там с мотопаратом
[23:30.240 --> 23:31.240]  было сложно.
[23:31.400 --> 23:32.720]  Эти результаты доказывали очень мощные.
[23:32.720 --> 23:37.480]  И понятно то, что у них были оценки скорости сходимости
[23:37.480 --> 23:38.480]  вот здесь.
[23:38.480 --> 23:40.980]  Так, результат Прохорова, который я нашел.
[23:40.980 --> 23:41.980]  Смотрите.
[23:41.980 --> 23:46.940]  Пускай у нас случайная величина КСи принимает биномеральное
[23:46.940 --> 23:50.520]  распределение с параметрами NP.
[23:50.520 --> 23:52.000]  Принимает биномеральное распределение… Извините.
[23:52.000 --> 23:53.920]  Случайная величина КСи с биномеральным распределением
[23:53.920 --> 23:56.480]  с параметрами NP.
[23:56.480 --> 23:59.600]  Это Пуассоновское распределение.
[24:00.000 --> 24:01.920]  Это имеет Пуассоновское распределение с параметром
[24:01.920 --> 24:02.920]  лямбда.
[24:02.920 --> 24:06.840]  При этом известно, что они связаны вот этим соотношением.
[24:06.840 --> 24:08.720]  Никто никуда не стремится, просто тупо равно.
[24:08.720 --> 24:11.560]  Вот эти три чиселки связаны равенством.
[24:11.560 --> 24:18.000]  Тогда утверждается следующее, что сумма по К от 0 до плюс
[24:18.000 --> 24:23.480]  бесконечности, вероятность того, что КСи равно К, минус
[24:23.480 --> 24:27.920]  вероятность того, что это равно К… Понятно, сумма
[24:28.080 --> 24:29.080]  по всем К.
[24:29.080 --> 24:30.080]  Да, смотрите.
[24:30.080 --> 24:32.360]  Понятно, что биномеральное принимает значение от
[24:32.360 --> 24:33.360]  нуля до n.
[24:33.360 --> 24:35.880]  Ну как бы подозумевается то, что просто обнуляется
[24:35.880 --> 24:38.140]  вот эта хрень.
[24:38.140 --> 24:43.200]  Она будет меньше либо равна, чем 2 лямбда делить на n,
[24:43.200 --> 24:45.680]  на минимум из 2 лямбда.
[24:45.680 --> 24:52.560]  То есть давайте посмотрим в нашей истории.
[24:53.040 --> 25:01.000]  В нашей истории у нас n было 1000, p была 1,200, ну соответственно
[25:01.000 --> 25:02.440]  лямбда это была пятерка.
[25:02.440 --> 25:05.960]  То есть вот эта вот штука у нас оказалась какая?
[25:05.960 --> 25:11.720]  10 делить на 1000 и умножить на 2.
[25:11.720 --> 25:14.360]  То есть 20 тысячных – это сколько?
[25:14.360 --> 25:15.360]  1,50.
[25:15.360 --> 25:18.360]  Я сейчас не вру, нет.
[25:18.360 --> 25:22.360]  Ну вроде да.
[25:22.440 --> 25:25.440]  Вроде 1,50.
[25:25.440 --> 25:26.440]  Ну это хорошо.
[25:26.440 --> 25:31.440]  То есть это у нас 2%.
[25:31.440 --> 25:34.160]  Ну как бы для тех времен точность вполне себе хороша.
[25:34.160 --> 25:37.160]  Ну вот.
[25:37.160 --> 25:40.640]  Ну вот самый точный результат, который я нашел.
[25:40.640 --> 25:42.440]  То есть действительно по крайней мере вот эта вещь
[25:42.440 --> 25:46.800]  показывает то, что я вот там руками махал, это законно
[25:46.800 --> 25:49.320]  вполне и точность вот она прям вычисляется по этой
[25:49.320 --> 25:50.320]  форме.
[25:50.320 --> 26:01.360]  Ну вот это те дискретные распределения, с которыми
[26:01.360 --> 26:16.360]  вы будете работать на семинарах.
[26:16.400 --> 26:20.680]  Вопрос, который у нас, давайте про плацоновский небольшой
[26:20.680 --> 26:21.680]  текст.
[26:21.680 --> 26:24.720]  Сейчас я прошу прощения, я не отвлечен, наверное
[26:24.720 --> 26:26.520]  это будет последний мое отвлечение за весь курс.
[26:26.520 --> 26:33.800]  Что такое е?
[26:33.800 --> 26:37.440]  Как все знают, начиная наверное кто с девятого, кто с десятого
[26:37.440 --> 26:38.440]  класса.
[26:38.440 --> 26:44.240]  Это вот такой предел.
[26:44.240 --> 26:47.240]  В пределе с Тремяющимся к Бесконечной такой последовательность.
[26:47.240 --> 26:49.160]  Ну на этом мы отрабатывали термин, вы отрабатывали
[26:49.160 --> 26:52.440]  термин Верштарас, но доказали, что это последовательность
[26:52.440 --> 26:55.280]  там монотонно, ограниченно, значимый есть предел, предел
[26:55.280 --> 26:57.680]  какой-то странный, обозначим его за е.
[26:57.680 --> 27:03.640]  А что потом на чем происходит с е?
[27:03.640 --> 27:11.040]  Е у числа?
[27:11.040 --> 27:13.360]  Прости, но по-моему, чтобы посчитать производную
[27:13.360 --> 27:15.200]  вот эту функцию можно было сделать и так.
[27:15.200 --> 27:21.640]  И тоже вполне спокойно посчитать производную.
[27:21.640 --> 27:30.440]  Ну то есть как бы использование е вот здесь, вот оно было.
[27:30.440 --> 27:33.840]  Ну просто я хочу обратить ваше внимание на момент,
[27:33.840 --> 27:35.160]  который должен был быть у вас напряг.
[27:35.160 --> 27:39.760]  Число, которое появилось откуда-то, ну вообще непонятно.
[27:39.760 --> 27:41.600]  Мы отрабатывали термин Верштарас и получили какую-то
[27:41.600 --> 27:44.120]  левую последовательность, предел, который не знаем,
[27:44.120 --> 27:45.120]  обозначили ok.
[27:45.120 --> 27:47.280]  А потом оказалось то, что это число вылезает вообще
[27:47.280 --> 27:48.280]  отовсюду.
[27:48.280 --> 27:49.280]  Это понятно?
[27:49.280 --> 27:52.240]  Особенный момент, который должен был вас впечатлить,
[27:52.240 --> 27:55.080]  когда вы считали первообразную обратную пропорциональность
[27:55.080 --> 27:57.160]  и вдруг неожиданно оказывалось вот так.
[27:57.160 --> 28:02.000]  Это что за бред, господи, эта функция проще только
[28:02.000 --> 28:04.720]  линейная, но казалось бы, да?
[28:04.720 --> 28:08.360]  Причем тут е, вот в этой истории, которая определялась
[28:08.360 --> 28:09.360]  вот так.
[28:09.360 --> 28:12.360]  Вы, наверное, не задумывались над этим, да?
[28:12.360 --> 28:13.360]  Ну да.
[28:13.360 --> 28:18.640]  Какой ответ на мой вопрос, который я не задал, наверное?
[28:18.640 --> 28:25.000]  То есть понятно, что е является пределом этой последовательности,
[28:25.000 --> 28:26.000]  но так получилось.
[28:26.000 --> 28:32.840]  Но истинный смысл е другой какой?
[28:32.840 --> 28:37.600]  Кто может дать нормальное определение числа е?
[28:38.560 --> 28:41.560]  Производная что?
[28:41.560 --> 28:46.120]  Это такое основание показательной функции, что ее производная
[28:46.120 --> 28:47.120]  равна самой себе.
[28:47.120 --> 28:50.480]  Много ли функции улетворяют вот этому-то требованию?
[28:50.480 --> 28:51.480]  Две.
[28:51.480 --> 28:52.480]  И это?
[28:52.480 --> 28:54.280]  И тождественный ноль.
[28:54.280 --> 28:57.080]  Ну и линейное преобразование, да, конечно.
[28:57.080 --> 28:58.080]  Это понятно?
[28:58.080 --> 28:59.080]  Да?
[28:59.080 --> 29:00.080]  С коэффициентом масштаба.
[29:00.080 --> 29:01.760]  Ну вот, то есть е это то основание показательной
[29:01.760 --> 29:05.720]  функции, что ее производная равна самой себе.
[29:05.720 --> 29:07.840]  И это истинное, то есть как решение дифференциального
[29:07.840 --> 29:13.080]  уравнения, там f' равняется f.
[29:13.080 --> 29:16.440]  Та же самая история, то есть почему, как бы вот, просто
[29:16.440 --> 29:17.440]  проблема в чем?
[29:17.440 --> 29:20.440]  Проблема в педагогике.
[29:20.440 --> 29:24.240]  То есть впервые, ну просто идя по естественным стадиям
[29:24.240 --> 29:27.160]  своего математического развития, вы столкнулись,
[29:27.160 --> 29:29.720]  могли столкнуться с е вот в этот момент, когда вы
[29:29.720 --> 29:31.800]  считали пределу последовательности.
[29:31.800 --> 29:34.280]  И это очень хороший повод, потому что термин бешенаса
[29:34.280 --> 29:35.280]  и так далее.
[29:35.880 --> 29:36.880]  Ну вот.
[29:36.880 --> 29:40.080]  Но это не значит, что истинная природа числа е заложена
[29:40.080 --> 29:41.080]  бетом.
[29:42.080 --> 29:43.700]  То же самое и здесь.
[29:43.700 --> 29:44.700]  Ну вот.
[29:44.700 --> 29:48.000]  Да, плацоновское распределение у вас встречается как приближение
[29:48.000 --> 29:49.000]  биномиального распределения.
[29:49.000 --> 29:51.160]  Типа мы здесь не можем считать, поэтому будем считать вот
[29:51.160 --> 29:54.840]  это теорему, которая это обосновывает, но это не
[29:54.840 --> 29:56.480]  значит, что это истинная природа плацоновского
[29:56.480 --> 29:58.060]  распределения.
[29:58.060 --> 29:59.720]  Ид militia?
[29:59.720 --> 30:02.660]  Дальше вы будете с ним встречаться, все время встречаться,
[30:02.660 --> 30:05.200]  встречаться, а по кеям эта история у вас будет плацоновский
[30:05.200 --> 30:11.520]  процесс, на случайных процессах, и вот там вы приблизитесь к пониманию того, чем прекрасно это
[30:11.520 --> 30:20.880]  распределение. Сейчас хорошо? Такая интрига. Вот, это первая мысль. И вторая мысль, которая
[30:20.880 --> 30:29.120]  хочется сказать о том, что вы должны проникнуться идеей распределения. Еще раз, когда вы работаете в
[30:29.120 --> 30:36.120]  теории вероятности, история в чем заключается? Вы не работаете с омега маленьким, как с результатом,
[30:36.120 --> 30:40.720]  потому что если у вас есть результат эксперимента, то случайность ушла, уже все, уже эксперимент
[30:40.720 --> 30:49.040]  случился, вы уже подписали. Чего, не надо делать, я буду так акцентировать на этом момент, на это ваше
[30:49.040 --> 30:55.280]  внимание. А мы же с вами, у вас было собрание со мной на первом курсе? Да, я же говорил об этом, не
[30:55.280 --> 31:01.760]  подписывать или нет. Я обычно всем этом говорю на собрании 31 августа с первым курсом. Нет? Вы не
[31:01.760 --> 31:16.800]  помните? Плохо. Ну так вот, поэтому распределение случайных величин, это то, что нас действительно
[31:16.800 --> 31:21.520]  интересует от случайной величины, потому что не конкретное значение, а то, какие значения, с
[31:21.520 --> 31:28.800]  какими вероятностями случайная величина принимает. Как задаются распределения в дискретном случае,
[31:28.800 --> 31:36.800]  это просто, это соответствие между значениями и вероятностями. История простая. А что делать,
[31:36.800 --> 31:41.920]  если у нас значение становится бесконечное число? То есть, как в этом случае нам задавать
[31:41.920 --> 31:48.440]  распределение? Соответственно, мы с вами позже построим всю теорию с баррельской сигма алгеброй,
[31:48.440 --> 31:56.320]  и дальше будет тема о том, как задавать меру на баррельских сигма алгебрах. Это будет решаться
[31:56.320 --> 32:01.440]  через функции распределения. Кто-то из семинаристов уже сейчас вам даст функции распределения,
[32:01.440 --> 32:11.800]  вы начнётесь с ними работать сейчас, но это будет основной объект для работы на тервере. Я к этому
[32:11.800 --> 32:17.720]  тексту вернусь, когда мы будем с мерами работать, когда мы будем строить меры, я вернусь к этому
[32:17.720 --> 32:22.360]  обсуждению, к функции распределения. Извините, зря я это начал, мне тут написано, но зря. Всё,
[32:22.360 --> 32:27.960]  второй объект, который у нас есть, это вопрос независимости случайных величин. Мы с вами
[32:27.960 --> 32:49.680]  определили независимость для событий. Что нам делать, если у нас две случайных величины?
[32:49.680 --> 32:57.320]  Я напомню, что сейчас всё, что мы делаем, мы делаем только для дискретных вероятностных
[32:57.320 --> 33:06.840]  пространств, потому что для других у нас просто нет, потому что для других мы пока мотопарад не
[33:06.840 --> 33:16.400]  построили. Ну погнали, то есть если у нас есть с вами две случайные величины, у них соответственно
[33:16.400 --> 33:25.280]  есть их фазовое пространство и их значения, то есть хи, кси и хи. Это у нас множество значений
[33:25.280 --> 33:42.520]  случайных величин и кси. Т.е. мы будем говорить, что кси независима с этой, если любого значения
[33:42.520 --> 33:54.600]  случайных величин и кси, и любого значения не случайных это, будет верно, что два события,
[33:54.600 --> 34:13.240]  кси равно х ката и это равно y ката, будут независимы. Все понимают, что вот это вот написано
[34:13.240 --> 34:19.560]  событие. Т.е. это как бы все омега такие, что кси от омега стало равно х ката. И здесь то же самое,
[34:19.560 --> 34:25.520]  все омега такие, что этот омега равно y ката. То есть это означает, что вероятность того, что кси
[34:25.520 --> 34:33.720]  равно х ката и это равняется y ката, будет равно произведению вероятности кси равно х ката на
[34:33.720 --> 34:45.960]  вероятность, это равняется y ката. В чем самые важные здесь истории? Вот это. Для любых.
[34:45.960 --> 34:59.160]  Обычная реакция вот на такое определение у людей какая?
[34:59.160 --> 35:28.920]  А кто у вас? А, хорошо. Ну вообще обычно напрягаются по
[35:28.920 --> 35:35.240]  поводу того, что вот это для любых. Потому что кажется, то есть смотрите, если у нас у кси,
[35:35.240 --> 35:40.160]  у это бесконечное множество значений, то есть что получается? Бесконечное число равности должно
[35:40.160 --> 35:47.760]  быть выполнено. То есть тут возникает сомнение о том, что такое в принципе бывает. Ну в этот момент
[35:47.760 --> 35:54.000]  у людей. На семинарах мы специально решаем задачку о том, что вам нужно доказать, что существует две
[35:54.000 --> 35:58.040]  независимые пулассоновские случайные величины. Ну то есть прямо предъявить вероятностное
[35:58.040 --> 36:05.000]  пространство случайной величины на них пулассоновские, которые оказываются независимыми. Так что на семинарах
[36:05.000 --> 36:12.880]  эту задачку решить сейчас не будем тратить на это время. Хорошо? Так, давайте одну задачку сделаем, чтобы...
[36:12.880 --> 36:24.840]  Давайте возьмем две случайные величины. Кси имеет пулассоновское распределение с параметром,
[36:24.840 --> 36:33.160]  ну давайте лямбда 1, это имеет пулассоновское распределение с параметрами лямбда 2. И они независимы.
[36:33.160 --> 36:41.600]  Ну и вопрос, что мы можем сказать про сумму этих случайных величин. Когда вы спрашивают,
[36:41.600 --> 36:45.320]  что вы можете сказать, спрашивают, какое у нее распределение. Еще раз, от случайных величин всегда
[36:45.320 --> 36:50.720]  интересно, какое у нее распределение. Потому что по распределению можно понять, какие значения
[36:50.720 --> 36:59.520]  она принимает, с каким вероятностью, какие более вероятные, какие менее вероятные. Ну погнали.
[36:59.520 --> 37:04.600]  То есть что нам нужно сделать? Во-первых, понятно то, что множество значений случайной величины кси
[37:04.600 --> 37:11.680]  это от нуля и дальше, это от нуля и дальше. Поэтому кси плюс это она будет принимать вот те самые
[37:11.680 --> 37:21.160]  значения, не отрицательные и целые. Окей, то есть вот это вот будет ее фазовое множество
[37:21.160 --> 37:30.560]  состояний. Погнали. Получается, что для любого к мы сейчас будем считать вероятность того,
[37:30.560 --> 37:39.480]  что кси плюс это равно к. То есть как бы значение мы понимаем, какие она будет принимать. Сейчас нам
[37:39.480 --> 37:43.600]  для того, чтобы задать распределение, нужно найти вероятности, с которыми она принимает эти значения.
[37:43.600 --> 37:50.320]  Соответственно, как мы это будем делать? Смотрим, у нас тут две случайные величины. Одновременно мы
[37:50.320 --> 37:55.320]  про них ничего не можем сказать, а отдельно про каждую можем сказать. Это прямая история
[37:55.320 --> 37:59.960]  про формулу полной вероятности. Помните такую? То есть мы суммируем по всевозможным значениям
[37:59.960 --> 38:07.040]  какой-нибудь одной случайной величины, но предположим, пускай это будет кси. И по формуле полной
[38:07.040 --> 38:15.120]  вероятности вот здесь у нас будет кси плюс это равно к при условии того, что кси равно и. То есть
[38:15.120 --> 38:19.000]  произведение условной вероятности на вероятность условия. Это в точности форму полной вероятности.
[38:19.000 --> 38:25.840]  Теперь смотрим здесь. Поскольку мы рассматриваем вот это вот событие только при тех Омега,
[38:25.840 --> 38:33.080]  когда выполнено вот это, я получается вот здесь, могу заменить на и. Кси же у меня равно и.
[38:33.080 --> 38:44.280]  Получается, я могу там написать и. Кстати, я получу следующую вероятность того, что это равняется k
[38:44.280 --> 38:51.640]  минус и при условии того, что кси равно и, умножить на вероятность того, что кси равно и. Теперь
[38:51.640 --> 38:56.400]  смотрим вот здесь. Вероятность вот этого события при условии вот этого события. Что про эти два
[38:56.400 --> 39:06.320]  события можно сказать? Это равна чиселке и кси равна чиселке. Они независимы. Это из-за
[39:06.320 --> 39:10.960]  определения независимости случайных величин. Случайная величина независима, если для всех
[39:10.960 --> 39:15.800]  значений, для любой пары значений этих двух случайных величин, вот такие два события будут
[39:15.800 --> 39:23.080]  независимы. Получается вот эти два события у вас независимы. А если они независимы, то что
[39:23.080 --> 39:29.040]  можно сказать по условному вероятности? Она равна безусловной. То есть вот это можно выкинуть.
[39:29.040 --> 39:41.600]  Еще заметим такую штуку. Вот видите, здесь написано это равняется k минус и. А и пробегает куча
[39:41.600 --> 39:45.440]  значений. То есть понятно, что как только вот эта чиселка у вас становится отрицательной,
[39:45.440 --> 39:52.120]  эта вероятность обнуляется. Поэтому вот тут вот можно закончить на каком значении? Накатом,
[39:52.120 --> 39:58.480]  да? Потому что после этого у нас уже тут отрицательные числа и вероятность обнуляется. Что у нас
[39:58.480 --> 40:05.320]  получается? Сумма по и от 0 до k. Здесь у нас вероятность того, что это равняется k минус и.
[40:05.320 --> 40:11.000]  Это умеет пуласоновское распределение. То есть там тоже пуласоновское принимает конкретное
[40:11.000 --> 40:23.400]  значение. Это что такое? Лямбда 2 в степени k минус и делить на k минус и факториал. Ну на
[40:23.400 --> 40:37.600]  e в степени минус лямбда 2. Тут же все понятно, да? Здесь проще писать, потому что у нас получается
[40:37.600 --> 40:46.960]  лямбда 1 в степени и и факториал на e в степени минус лямбда 1. Соответственно мы выносим за
[40:46.960 --> 40:55.040]  скобку все, что у нас не под индексом и. То есть e в степени минус лямбда 1 плюс лямбда 2.
[40:55.040 --> 41:08.200]  Что у нас тут еще есть? Здесь прямо не хватает нам для бенемиального коэффициента. Смотрите,
[41:08.200 --> 41:12.000]  вот тут вот и факториал, тут k минус и факториал. Не хватает еще k факториал в
[41:12.000 --> 41:16.000]  числителе, чтобы получился бенемиальный коэффициент. Давайте я его сделаю. То есть я до него
[41:16.000 --> 41:22.240]  домножу, значит я до него разделю. Получится вот так вот. И у меня получится тогда вот здесь что?
[41:22.240 --> 41:36.840]  c из k по i. То есть если я на k, то я его домножу. Лямбда 1 в степени и на лямбда 2 в степени k
[41:37.440 --> 41:50.360]  Что нам эта хрень напоминает? Бином она напоминает. То есть вот эта хрень у нас получается лямбда 1
[41:50.360 --> 42:01.160]  плюс лямбда 2 в степени k. То есть вероятность того, что случайная личина равна k, равно вот этому.
[42:01.160 --> 42:06.880]  То есть получается кси плюс это. Какое имеет распределение? Плассоновское,
[42:06.880 --> 42:16.760]  но с другим параметром. У нас получилось лямбда 1 плюс лямбда 2. Так, это все видят. Если есть
[42:16.760 --> 42:34.200]  вопрос, можно сейчас задать. На самом деле непонятный и нетреальный результат. То есть если мы
[42:34.200 --> 42:40.800]  складываем случайную величину, которая независима, потому что если мы кси сложим, то есть если у вас
[42:40.800 --> 42:46.600]  была плассоновская случайная начинава, если вы ее сложите самой собой, вы получите два
[42:46.880 --> 42:57.300]  умножить на kси. Этоrier2 Умножить на кси.
[43:03.300 --> 43:13.080]  Если кси имеет плассоновское распределение, нет не так. Я не спрашиваю, почему не работает это решение,
[43:13.080 --> 43:15.680]  Я спрашиваю, почему точно не плацомовское распедление?
[43:15.680 --> 43:18.680]  Может быть, как-то по-другому там получать?
[43:18.680 --> 43:19.680]  Да.
[43:19.680 --> 43:20.680]  Два.
[43:20.680 --> 43:26.880]  Да, ну то есть, если я вместо этого беру ту же самую кси,
[43:26.880 --> 43:29.440]  получается кси плюс кси.
[43:29.440 --> 43:31.000]  Это просто 2 умножить на кси.
[43:31.000 --> 43:33.920]  У этой случайночной плацомовское распедление или нет?
[43:33.920 --> 43:34.920]  Нет.
[43:34.920 --> 43:35.920]  Нет.
[43:35.920 --> 43:36.920]  Почему?
[43:36.920 --> 43:41.560]  Это зависит от событий, но это кси и кси.
[43:41.560 --> 43:44.320]  То есть, не работает это решение, это хорошо.
[43:44.320 --> 43:47.200]  Может быть, как-то по-другому там получается плацомовское
[43:47.200 --> 43:48.200]  распедление.
[43:48.200 --> 43:50.560]  Это точно, то есть, 2 умножить на кси.
[43:50.560 --> 43:52.680]  Если кси имеет плацомовское распедление, имеет распедление
[43:52.680 --> 43:54.080]  точно не плацомовское распедление.
[43:54.080 --> 43:55.080]  Объяснение очень простое.
[43:55.080 --> 43:56.080]  Какое?
[43:56.080 --> 44:00.920]  Я вам стукну сейчас.
[44:00.920 --> 44:01.920]  Кого-нибудь.
[44:01.920 --> 44:02.920]  До кого дотянусь.
[44:02.920 --> 44:03.920]  Ура!
[44:03.920 --> 44:08.320]  Она принимает только четные значения.
[44:08.320 --> 44:11.320]  Еще раз, плацомовская случайная величина принимает
[44:11.320 --> 44:15.000]  все целые неотрицательные значения с такими вероятностями,
[44:15.000 --> 44:16.000]  которые там выписаны были.
[44:16.000 --> 44:18.000]  Лянда, все пеникаде, линкофакталерога и так далее.
[44:18.000 --> 44:22.160]  То есть, она каждое неотрицательное целое значение принимает
[44:22.160 --> 44:23.720]  с положительной вероятностью.
[44:23.720 --> 44:24.720]  Это ясно?
[44:24.720 --> 44:25.720]  Кивайте.
[44:25.720 --> 44:28.600]  2 умножить на кси принимает только четное значение.
[44:28.600 --> 44:31.520]  Она равна единице с нулевой вероятностью, у нее не может
[44:31.520 --> 44:32.960]  быть плацомовского распедления.
[44:32.960 --> 44:33.960]  Да?
[44:33.960 --> 44:34.960]  Да.
[44:34.960 --> 44:35.960]  Ладно.
[44:35.960 --> 44:41.520]  Поэтому, мысль в чем?
[44:41.520 --> 44:45.280]  Если вы складываете какие-то зависимые плацомовские случайные
[44:45.280 --> 44:46.760]  величины, вы получаете что-то.
[44:46.760 --> 44:50.600]  Например, 2 умножить на кси – это какая-то новая
[44:50.600 --> 44:52.200]  случайная величина, которая понимает только что-то
[44:52.200 --> 44:56.840]  неотрицательное целое значение.
[44:56.840 --> 45:00.080]  А вот когда мы складываем независимые, почему-то распределение
[45:00.080 --> 45:02.640]  сохраняется, причем достаточно мило, то есть параметры
[45:02.640 --> 45:04.200]  складываются.
[45:04.240 --> 45:06.800]  Эта фигня будет выполнена очень часто.
[45:06.800 --> 45:10.000]  Если вы складываете две биномиальные, у которых
[45:10.000 --> 45:13.760]  одинаковая п, разные н, но одинаковая п, там тоже
[45:13.760 --> 45:17.560]  будет биномиальная, там будет n1 плюс n2 и p.
[45:17.560 --> 45:22.640]  Потом, когда вы откроете для себя непрерывные случайные
[45:22.640 --> 45:25.200]  величины, там будет та же самая история, то есть нормальная
[45:25.200 --> 45:27.720]  случайная величина, которую скорее всего вы слышали,
[45:27.720 --> 45:28.720]  вот эти колокола.
[45:28.720 --> 45:33.600]  Если вы складываете независимые нормальные случайные величины,
[45:33.600 --> 45:36.600]  вы снова получаете нормальную случайную величину, только
[45:36.600 --> 45:38.360]  там параметры будут другие.
[45:38.360 --> 45:40.120]  Вот если они независимые, там все что угодно может
[45:40.120 --> 45:42.160]  получиться, а если независимые, то получается то же самое
[45:42.160 --> 45:43.160]  распределение.
[45:43.160 --> 45:46.920]  Так что мой текст о том, что это понятие центральной
[45:46.920 --> 45:50.360]  теории вероятности, я надеюсь, я смог вас сейчас убедить,
[45:50.360 --> 45:52.720]  что и для случайных величин это тоже важно.
[45:52.720 --> 45:53.720]  Хорошо?
[45:53.720 --> 45:54.720]  Окей.
[45:54.720 --> 46:01.360]  Следующее понятие, второе по степени центральность,
[46:01.360 --> 46:02.360]  это математическое ожидание.
[46:02.360 --> 46:18.480]  Случайная величина.
[46:18.480 --> 46:23.000]  Соответственно обозначение, было принято два обозначения
[46:23.000 --> 46:27.640]  для математического ожидания, мат ожидания кси или вот
[46:27.640 --> 46:29.280]  мат ожидания кси.
[46:29.280 --> 46:33.880]  Соответственно и это из expectation, ну как ожидания,
[46:33.880 --> 46:34.880]  да?
[46:34.880 --> 46:38.200]  Ну вот м это, ну это просто, это мин, ну как средний.
[46:38.200 --> 46:42.560]  Ну вот, в советской, в зарубежной литературе практически
[46:42.560 --> 46:46.240]  вся использовала вот это обозначение советское.
[46:46.240 --> 46:52.360]  Вообще будем честным, в двадцатом веке как бы теория
[46:52.360 --> 46:55.200]  вероятности делался советской наукой, ну то есть вот такие
[46:55.200 --> 46:58.200]  самые яркие результаты были получены именно советскими
[46:58.200 --> 47:00.960]  учёными, там Комагоров, Прохоров.
[47:00.960 --> 47:01.960]  Ну вот.
[47:01.960 --> 47:07.080]  А и тут было два варианта, либо expectation, либо мин.
[47:07.080 --> 47:08.080]  Ну вот.
[47:08.080 --> 47:11.440]  Соответственно после развала Союза было две группировки,
[47:11.440 --> 47:14.880]  которые между собой соперничали, соответственно это кафе
[47:14.880 --> 47:17.920]  теории вероятности на михмате МГУ, ну вот.
[47:17.920 --> 47:20.560]  И кафе математической статистики случайных процессов на
[47:20.560 --> 47:21.560]  михмате МГУ.
[47:21.560 --> 47:24.880]  Соответственно я был с кафе теории вероятности, а Рыгородский
[47:24.880 --> 47:26.800]  был с кафе математической статистики.
[47:26.960 --> 47:31.240]  Рыгородский просто упорствовал долгие годы и писал вот так,
[47:31.240 --> 47:33.040]  соответственно мы все писали вот так.
[47:33.040 --> 47:34.040]  Ну вот.
[47:34.040 --> 47:37.520]  В общем, он сломался, он уже года три пишет как все.
[47:37.520 --> 47:38.520]  Ну вот.
[47:38.520 --> 47:42.520]  То есть вот это обозначение уходит в прошлое, ну вот.
[47:42.520 --> 47:46.680]  Постепенно как-то вымывается, потому что редакторы, когда
[47:46.680 --> 47:50.040]  отправляете статью на рецензию, они поправляют то, что давайте
[47:50.040 --> 47:51.560]  все-таки как все писать будем.
[47:51.560 --> 47:54.080]  Поэтому мы с вами будем писать только вот так, хотя встречается
[47:54.080 --> 47:55.080]  и эммочка.
[47:55.560 --> 47:57.560]  Но говорят Рыгородский уже года три.
[47:57.560 --> 47:58.560]  Ну вот.
[47:58.560 --> 48:02.160]  Раньше это было смешно, потому что он писал М, и мой вот
[48:02.160 --> 48:04.280]  этот текст прям заходил, потому что у вас дискретное
[48:04.280 --> 48:06.760]  анализ сейчас идет, и там встречается это.
[48:07.760 --> 48:08.760]  Ну вот.
[48:08.760 --> 48:11.760]  В чем идея математического ожидания?
[48:11.760 --> 48:15.960]  В суть, это просто понятие среднего от ваших наблюдений.
[48:15.960 --> 48:18.280]  То есть давайте рассмотрим историю, то, что у вас есть
[48:18.280 --> 48:23.840]  случайный эксперимент, и вы проводите серию из
[48:23.920 --> 48:24.920]  Н большой экспериментов.
[48:27.920 --> 48:29.920]  Ну то есть Н большое – это прям много.
[48:29.920 --> 48:32.920]  Эксперимент.
[48:35.920 --> 48:38.480]  Я напомню, как бы, терверус продиктован нам жизнью, поэтому
[48:38.480 --> 48:41.120]  все эти понятия, которые мы тут вводим, они все-таки
[48:41.120 --> 48:42.120]  из жизни приходят.
[48:42.120 --> 48:45.000]  И те определения, которые проявляются, они продиктованы
[48:45.000 --> 48:46.000]  им.
[48:46.000 --> 48:47.000]  Ну вот.
[48:47.000 --> 48:49.720]  И у вас есть какая-то числовая характеристика.
[48:50.600 --> 48:53.600]  У вас есть какая-то числовая характеристика.
[48:53.600 --> 48:57.360]  Я сейчас не говорю про случайную величину, потому что случайная
[48:57.360 --> 49:00.400]  величина – это промат-модель, а сейчас мы как бы опустились
[49:00.400 --> 49:02.720]  назад на случайный эксперимент.
[49:02.720 --> 49:08.120]  Ну то есть вы снимаете вот при этих Н экспериментах
[49:08.120 --> 49:12.080]  эту числовую характеристику, и у вас получается наблюдение
[49:12.080 --> 49:14.720]  кси1 и так далее кси Н большое.
[49:14.720 --> 49:16.760]  Ну это те числа, которые вы сняли.
[49:16.800 --> 49:21.000]  И среднее ваших наблюдений, очевидно, это просто среднее
[49:21.000 --> 49:22.000]  арифметическое.
[49:22.000 --> 49:26.600]  Ну то есть вы провели много-много измерений, а потом просто
[49:26.600 --> 49:28.400]  берете среднее асимметическое от этой истории.
[49:28.400 --> 49:32.680]  Теперь давайте соображать.
[49:32.680 --> 49:37.920]  Вот любая из этих ктишечек, она и является, я значение
[49:37.920 --> 49:45.160]  тут подбирал, кси1 это есть как бы числовая характеристика
[49:45.160 --> 49:47.920]  результата эксперимента, то есть Омега с индексом
[49:47.920 --> 49:50.720]  И – это результат нашего случайного эксперимента,
[49:50.720 --> 49:52.600]  который получился при Итым испытании, то есть у нас
[49:52.600 --> 49:57.320]  тут И меняется от единички до Н большого.
[49:57.320 --> 49:58.480]  При этом мы что понимаем?
[49:58.480 --> 50:01.120]  Мы понимаем то, что у нас каждому результату эксперимента
[50:01.120 --> 50:05.200]  поставлен соответствующий элементарный исход, число
[50:05.200 --> 50:08.440]  которых, ну пускай у нас будет Н маленькое.
[50:08.440 --> 50:09.440]  В общем, идея в чем?
[50:09.440 --> 50:13.080]  То, что у нас обычно Н маленькое у нас меньше, чем Н большое.
[50:15.160 --> 50:19.920]  Обозначение понятное, верхний индекс – это результат
[50:19.920 --> 50:22.000]  случайного эксперимента, а нижний индекс – это просто
[50:22.000 --> 50:25.520]  номер моего случайного, номер элементарного исхода
[50:25.520 --> 50:26.840]  просто в моем Омега Большом.
[50:26.840 --> 50:30.600]  В связи с этим, что мы с вами получаем?
[50:30.600 --> 50:34.960]  Мы получаем то, что среди вот этих Омега с верхними
[50:34.960 --> 50:38.600]  индексами часто встречаются вот эти вот наши Омега.
[50:38.600 --> 50:43.240]  И тогда мы можем вести такие вещи как А и Т, это что будет
[50:43.320 --> 50:44.320]  такое?
[50:44.320 --> 50:48.720]  Это будет число тех Омегаитов, что, извините, это будет
[50:48.720 --> 50:56.480]  число тех Омега-житых, что Омега-житая равняется
[50:56.480 --> 50:57.480]  Омегаитовым.
[50:57.480 --> 51:04.680]  Ну, то есть это сколько раз вот в наших вот этих
[51:04.680 --> 51:07.240]  вот экспериментах выпало конкретный элементарный
[51:07.240 --> 51:08.240]  исход?
[51:08.240 --> 51:09.240]  Все понятно?
[51:09.240 --> 51:10.240]  Все, да?
[51:10.240 --> 51:14.080]  Ну, например, сумма всех аитов будет равна чему
[51:14.080 --> 51:15.080]  получается?
[51:15.080 --> 51:16.080]  А?
[51:16.080 --> 51:17.080]  Н большое.
[51:17.080 --> 51:18.080]  Н большое, конечно.
[51:18.080 --> 51:21.640]  При этом и у нас меняется от единички до н маленького.
[51:21.640 --> 51:27.560]  Теперь давайте пойдем вот сюда, вернемся.
[51:27.560 --> 51:36.320]  Вот здесь вот хочется собрать вместе те как бы ксишки,
[51:36.320 --> 51:39.480]  которые соответствуют одной Омеге.
[51:39.480 --> 51:43.120]  Число, мы их знаем, сколько их будет аит, и получается,
[51:43.120 --> 51:46.200]  что у нас вот здесь вот, в числителе будет что происходить?
[51:46.200 --> 51:51.760]  Сумма по и от единички до н маленького, аит умножить
[51:51.760 --> 51:55.120]  на кси от Омегаит с нижним индексом.
[51:55.120 --> 51:59.360]  Ну, просто сколько раз Омегаиты у нас вот тут вот попалось,
[51:59.360 --> 52:02.840]  вот мы их все вместе и собрали, и количество их будет вот
[52:02.840 --> 52:03.840]  такое.
[52:03.840 --> 52:06.360]  И все это мы делим на Н большое.
[52:07.000 --> 52:08.960]  Причем я могу переписать это еще вот так вот.
[52:18.960 --> 52:21.640]  Причем я говорю, сейчас мы как бы говорим о том,
[52:21.640 --> 52:24.040]  просто человек сидел, провел очень много испытаний,
[52:24.040 --> 52:28.360]  снял показания и работает с третьим объектом.
[52:28.360 --> 52:29.680]  Мы просто этот объект немножко преобразовали.
[52:29.680 --> 52:32.800]  То есть сейчас мы говорим про реальную жизнь.
[52:33.240 --> 52:37.120]  Теперь давайте посмотрим, что вот это такое.
[52:37.120 --> 52:38.800]  Что такое аиты?
[52:38.800 --> 52:43.800]  Это количество выпавших у нас Омегой.
[52:43.800 --> 52:46.720]  Сколько раз у нас выпало Омега маленькая, и ты деленный
[52:46.720 --> 52:47.720]  на Н большое.
[52:47.720 --> 52:50.720]  Какая идеализация вот этой дроби?
[52:50.720 --> 52:52.720]  Это что такое?
[52:52.720 --> 52:55.720]  Еще раз.
[52:55.720 --> 52:58.720]  Вероятность чего?
[52:58.720 --> 53:00.720]  Омегаит.
[53:00.720 --> 53:02.720]  Идейно.
[53:03.640 --> 53:05.640]  Равно я, наверное, не хорошо пишу.
[53:05.640 --> 53:06.640]  Давайте я стрелочку напишу.
[53:06.640 --> 53:09.760]  То есть когда мы строили с вами математическую модель
[53:09.760 --> 53:12.080]  случайного эксперимента, мы говорили, что вероятность
[53:12.080 --> 53:13.080]  это что?
[53:13.080 --> 53:16.760]  Это идеализация частоты, которой у нас там есть статистическая
[53:16.760 --> 53:17.760]  устойчивость частот.
[53:17.760 --> 53:21.160]  А сейчас получается, что вот этот новый объект, который
[53:21.160 --> 53:24.880]  мы хотим вести, если мы его будем идеализировать,
[53:24.880 --> 53:25.880]  его нужно вот так.
[53:25.880 --> 53:30.720]  То есть приходим к определению, к математическому ожиданию
[53:30.720 --> 53:35.360]  случайной величины х называется, я напоминаю, что у нас сейчас
[53:35.360 --> 53:38.200]  все в дискретном случае, то есть мы работаем только
[53:38.200 --> 53:41.840]  с дискретной случайной величиной, называется сумма
[53:41.840 --> 53:45.360]  по всем элементарным исходам, вероятность этих элементарных
[53:45.360 --> 53:49.800]  исходов на значение случайной величины в этом элементарном
[53:49.800 --> 53:50.800]  исходе.
[53:50.800 --> 53:56.400]  Почему мат ожидания определяется именно так, я надеюсь, я обосновал.
[53:56.400 --> 54:01.880]  Это определение в математике, но оно продиктовано такими
[54:01.880 --> 54:02.880]  простейшими соображениями.
[54:02.880 --> 54:05.880]  Хорошо?
[54:05.880 --> 54:11.040]  Теперь пошли свойства математического ожидания.
[54:11.040 --> 54:33.120]  Так, первое, если случайная величина кси имеет распределение,
[54:33.120 --> 54:39.760]  имеет вот такое распределение, хк, пк, пк, то математическое
[54:39.760 --> 54:43.440]  ожидание случайной величины кси можно посчитать как
[54:43.440 --> 54:49.760]  сумма по значению случайной величины значений, домноженных
[54:49.760 --> 54:51.280]  на вероятности этих значений.
[54:51.280 --> 55:00.120]  Ну, доказательство очевидно, доказательство этой истории
[55:00.120 --> 55:03.300]  очевидно, потому что мат ожидания это есть сумма
[55:03.300 --> 55:11.700]  по всем омегам маленьким, п от омега, на кси от омега,
[55:11.700 --> 55:15.500]  при этом мы понимаем, что вот этот множитель у нас
[55:15.500 --> 55:19.380]  часто повторяется, то есть значения случайной величины
[55:19.380 --> 55:23.460]  от разных случайных экспериментов часто одинаковые.
[55:23.460 --> 55:24.460]  И мысль в чем?
[55:24.460 --> 55:27.540]  То, что мы можем вот эту сумму перегруппировать.
[55:27.540 --> 55:34.540]  Перегруппировать так, то есть в рамках одной группы
[55:34.540 --> 55:37.220]  мы соберем такие, что у нас будут одинаковые вот
[55:37.220 --> 55:38.220]  эти множители.
[55:38.220 --> 55:40.780]  То есть в результате мы будем суммировать по значениям
[55:40.780 --> 55:45.100]  случайной величины, внутри у нас будет по таким омега,
[55:45.100 --> 55:49.820]  что кси от омега равняется хк, здесь у нас будет п от
[55:49.820 --> 55:55.660]  омега умножить на хк, где мы бегаем по всем значениям
[55:55.660 --> 55:57.460]  случайной величины кси.
[55:57.460 --> 55:59.460]  Здесь, соответственно, хк от омега, независимо
[55:59.460 --> 56:01.340]  от того, можно вынести за знак суммы, получается
[56:01.340 --> 56:04.180]  просто суммированная вероятность элементарных исходов, на
[56:04.180 --> 56:05.860]  которых кси от омега принимает значение хк.
[56:05.860 --> 56:14.100]  Ну так это и есть наша вероятность, вероятность того, что кси
[56:14.100 --> 56:18.860]  равняется хк, а это в наших обозначениях и есть пк.
[56:18.860 --> 56:27.100]  Идея ясна?
[56:27.100 --> 56:35.340]  Какой важный вывод мы делаем вот из этой истории?
[56:35.340 --> 56:39.540]  Какой важный вывод мы делаем из этой истории?
[56:39.540 --> 56:41.260]  Там такая боль на лице.
[56:41.260 --> 56:42.740]  Чего вы от нас хотите?
[56:42.740 --> 56:44.740]  Читайте давайте, мы пишем.
[56:44.740 --> 56:47.860]  Чего еще думать что-ли надо?
[56:47.860 --> 56:53.780]  У нас тут был созвон, ой нет, там нельзя, это внутренняя
[56:53.780 --> 56:56.940]  информация 1С, ладно, не буду говорить, что я тут
[56:56.940 --> 56:57.940]  вспомню.
[56:57.940 --> 56:58.940]  Зачем подзапись?
[56:58.940 --> 57:01.900]  Ну так чего, кто-нибудь родит какой-нибудь вывод?
[57:01.900 --> 57:09.860]  Это мы молодцы, да, тяжелая была, употели.
[57:09.860 --> 57:21.100]  Смотрите, как бы, идея, в чем мы делаем, в те веры,
[57:21.100 --> 57:24.020]  вообще в любой науке, когда вы занимаетесь, то есть
[57:24.020 --> 57:27.700]  у вас есть дисциплина, в чем смысл, то есть вам
[57:27.700 --> 57:31.140]  водятся объекты, а дальше вы изучаете свойства этих
[57:31.140 --> 57:32.140]  объектов.
[57:32.140 --> 57:37.100]  Ну согласитесь, то есть там вам вели линейное пространство
[57:37.100 --> 57:39.900]  и дальше вы это линейное пространство сбоку в профиль
[57:39.900 --> 57:44.620]  там в разрезе изучаете, это понятно, любая дисциплина
[57:44.620 --> 57:45.620]  так строится.
[57:45.620 --> 57:48.820]  Теперь вера, то есть как бы у нас ну типа там случайный
[57:48.820 --> 57:51.220]  эксперимент, но на самом деле мы всегда работаем
[57:51.220 --> 57:53.940]  с математической моделью, у нас математическая дисциплина
[57:53.940 --> 57:57.660]  есть, вот у нас есть мат-модель, дальше мы там вводим объекты
[57:57.660 --> 57:59.940]  и с ними работаем, изучаем их свойства.
[57:59.940 --> 58:06.380]  Соответственно, у нас что получается в те веры, мы
[58:06.380 --> 58:08.980]  ввели новый объект с случайной величины, дальше рассматриваем
[58:08.980 --> 58:12.780]  различные характеристики этого нового объекта, что
[58:12.780 --> 58:15.580]  у нас там есть, у нас есть распределение случайно
[58:15.580 --> 58:17.380]  величины, это какая-то ее характеристика, теперь
[58:17.380 --> 58:21.580]  новая возникла история с математическим ожиданием,
[58:21.680 --> 58:23.520]  но указана характеристика случайной величины.
[58:23.520 --> 58:24.820]  Это понятно?
[58:24.820 --> 58:26.700]  А вот эта теорема говорит о чем?
[58:26.700 --> 58:31.060]  То, что на самом деле математическое ожидание является характеристикой
[58:31.060 --> 58:34.940]  не случайной величины, а чего?
[58:34.940 --> 58:39.280]  Ее распределение, то есть мы сейчас показали, что
[58:39.280 --> 58:41.900]  важно какое распределение случайной величины.
[58:41.900 --> 58:45.140]  И эта характеристика именно распределением самой
[58:45.140 --> 58:46.140]  случайной величины.
[58:46.140 --> 58:48.860]  Еще раз, случайная величина как функция от случайного
[58:48.860 --> 58:53.540]  эксперимент а от элементарного исхода она нам не очень интересно нам интересно
[58:53.540 --> 58:57.340]  распределение и вот мы сейчас выяснили то тот объект который мы водим вводим
[58:57.340 --> 59:03.980]  является характеристикой именно распределение это ясно хорошо два
[59:03.980 --> 59:07.980]  замечания первое замечание вот тут вот добавим еще один результат который я
[59:07.980 --> 59:14.340]  доказывать не буду что если я хочу посчитать если я хочу посчитать
[59:14.340 --> 59:17.180]  математическое ожидание функции от случайной величины
[59:17.180 --> 59:21.400]  ну какой тут функция от случайной величины это будет новый случайно
[59:21.400 --> 59:30.300]  величина так отяснайте вы меня пугайте еще раз ну как бы вот это вот у вас phi
[59:30.300 --> 59:35.440]  как это числовая функция числовая функция отчесу функции это binsten
[59:35.440 --> 59:38.540]  функция это понятно поэтому это новый объект это какая-то новая случайная
[59:38.540 --> 59:50.540]  Ну, например, мат ожидания кси в квадрате, соответственно, как считать вот эту вещь?
[59:50.540 --> 59:55.540]  Ну, либо по определению, бегая по всем омегам маленьким, по всему вероятностному пространству,
[59:55.540 --> 01:00:02.540]  и суммирую p от омега на phi кси от омега, либо, ну непонятно как, здесь мы уже этим пользоваться не можем.
[01:00:02.540 --> 01:00:07.540]  А, либо, да, извините, либо искать распределение вот этой новой случайной величины.
[01:00:07.540 --> 01:00:14.540]  Ну, например, кси у нас была полосоновская, значит 2 умножить на кси надо писать,
[01:00:14.540 --> 01:00:18.540]  то есть она принимает вот такие четные значения вот с такими вот вероятностями.
[01:00:18.540 --> 01:00:22.540]  Прописывать это распределение, потом выписывать вот этот вот ряд.
[01:00:22.540 --> 01:00:26.540]  Оказывается, что можно так не делать. Можно для распределения случайной величины кси,
[01:00:26.540 --> 01:00:36.540]  но просто суммировать вот такие вещи. То есть мы по-прежнему отталкиваемся от распределения случайной величины кси,
[01:00:36.540 --> 01:00:46.540]  но суммируем как бы не ее значение, а phi от ее значения. Можно я не буду это доказывать, это делается точно так же.
[01:00:46.540 --> 01:00:54.540]  Да, просто вот здесь вот мы собираем как бы не по значениям phi, а от кси, а по значениям кси все еще.
[01:00:54.540 --> 01:00:58.540]  Мы же суммировать можем как хотим, ну сумму нашу большую разбивать.
[01:01:00.540 --> 01:01:05.540]  Так, это первое замечание, которое есть. Второе замечание, вот тут вот я наврал.
[01:01:06.540 --> 01:01:09.540]  Кто может придумать, в чем я здесь наврал? В определении математического вождания.
[01:01:09.540 --> 01:01:17.540]  Вот докопайтесь до этого определения. Я напомню, что живем мы все еще в рамках дискретного вероятностного пространства,
[01:01:17.540 --> 01:01:24.540]  то есть у нас омега не более чем счетно, а если счетно, то там мера еще есть, как бы счетные суммы тоже можно составлять.
[01:01:26.540 --> 01:01:35.540]  Кто сказал что? Да, да. То есть беда состоит в чем? То, что вот эта сумма, видите как я пишу, по всем омегам маленьким.
[01:01:35.540 --> 01:01:39.540]  Если у вас омега большое счетное множество, то как я суммирую эту сумму?
[01:01:40.540 --> 01:01:45.540]  В этот момент у нас еще всплывает в мозгу факт из математического анализа, я надеюсь он у вас был.
[01:01:45.540 --> 01:01:51.540]  Вы понимаете к чему я апеллирую? Что условно сходящийся ряд можно так перенумеровать, что он сойдется к чему угодно.
[01:01:51.540 --> 01:01:53.540]  Кивайте, чтобы помнить этот результат.
[01:01:56.540 --> 01:02:02.540]  Очень плохо киваете вообще. Нет, в смысле то, что есть люди, которые этого не знают.
[01:02:02.540 --> 01:02:04.540]  Саша, это было?
[01:02:04.540 --> 01:02:06.540]  Нет, возможно, что такого не было.
[01:02:06.540 --> 01:02:10.540]  Но факт в том, что если ряд условно сходящийся, его можно перенумеровать так, он сойдется куда угодно.
[01:02:10.540 --> 01:02:12.540]  А тут вообще нумерации нет.
[01:02:12.540 --> 01:02:14.540]  Поэтому вот здесь, если ряд сходится абсолютно.
[01:02:15.540 --> 01:02:31.540]  Теперь смотрите.
[01:02:31.540 --> 01:02:37.540]  Когда конец нашего курса будет состоять в том, что мы будем строить интеграбль либега. Зачем нам это нужно?
[01:02:37.540 --> 01:02:39.220]  Зачем нам это нужно?
[01:02:39.220 --> 01:02:45.540]  Если у вас ω перестает быть счетным множеством,
[01:02:45.540 --> 01:02:48.380]  то понятно, что сумма должна стать интегралом, и это
[01:02:48.380 --> 01:02:53.420]  не интеграл Риммана, потому что интеграл Риммана строится
[01:02:53.420 --> 01:02:54.420]  на rn.
[01:02:54.420 --> 01:02:59.180]  У нас сейчас пространство, и непонятно, какой природы.
[01:02:59.180 --> 01:03:01.380]  И там мы будем строить интеграл Риммана.
[01:03:01.380 --> 01:03:03.860]  И главное отличие интеграла Риммана от интеграла Риммана
[01:03:03.860 --> 01:03:06.840]  заключается в том, что для интеграла Риммана в принципе
[01:03:06.840 --> 01:03:08.680]  и нет понятия условной сходимости.
[01:03:08.680 --> 01:03:12.800]  И сейчас должно быть понятно, почему.
[01:03:12.800 --> 01:03:18.040]  Потому что у нас идет суммирование по всему, неважно какая
[01:03:18.040 --> 01:03:21.320]  последовательность, а получается условную сходимость в принципе
[01:03:21.320 --> 01:03:22.320]  не может быть.
[01:03:22.320 --> 01:03:27.120]  Ладно, это и есть.
[01:03:27.120 --> 01:03:29.220]  Замечу то, что практически всегда, когда вы будете
[01:03:29.220 --> 01:03:32.000]  считать какое-то математическое ожидание, вы будете пользоваться
[01:03:32.000 --> 01:03:33.480]  именно этой формулой.
[01:03:33.480 --> 01:03:36.600]  Внимание, это формула для подсчета мат ожидания.
[01:03:36.600 --> 01:03:39.080]  Поскольку вы будете пользоваться только ей, то к концу курса
[01:03:39.080 --> 01:03:41.360]  у вас будет ощущение, что это и есть определение.
[01:03:41.360 --> 01:03:42.360]  Это не так.
[01:03:42.360 --> 01:03:45.520]  Определение оно вот и продиктовано оно вот этими соображениями.
[01:03:45.520 --> 01:03:53.080]  А в общем случае, для произвольного пространства, это мы к концу
[01:03:53.080 --> 01:03:54.080]  курса построим.
[01:03:54.080 --> 01:03:56.800]  Это будет интеграл Олибек.
[01:03:56.800 --> 01:04:03.520]  Теперь пошли по таким арифметическим свойствам математического
[01:04:03.520 --> 01:04:04.520]  ожидания.
[01:04:04.520 --> 01:04:07.200]  Так, второе.
[01:04:07.200 --> 01:04:19.480]  Мат ожидания – это есть линейный оператор на множестве
[01:04:19.480 --> 01:04:20.480]  случайных величин.
[01:04:20.480 --> 01:04:23.440]  Тут оговорка на множестве случайных величин, для
[01:04:23.440 --> 01:04:25.160]  которых в принципе мат ожидания существует.
[01:04:25.160 --> 01:04:28.920]  То есть, что это значит?
[01:04:28.920 --> 01:04:32.120]  Это значит, что для любых случайных величин кси,
[01:04:33.000 --> 01:04:42.960]  и любых чисел а и b, тут уточнение таких, что существует
[01:04:42.960 --> 01:04:46.000]  мат ожидания кси и мат ожидания это.
[01:04:46.000 --> 01:04:51.240]  Будет верно, что мат ожидания а кси плюс b это, есть а
[01:04:51.240 --> 01:05:00.640]  мат ожидания кси плюс b мат ожидания это.
[01:05:00.720 --> 01:05:01.720]  Показательство очевидно.
[01:05:01.720 --> 01:05:05.200]  То есть, посмотрели вот туда на определение.
[01:05:05.200 --> 01:05:08.080]  Вы понимаете, если вы тот ряд, который в правом верхнем
[01:05:08.080 --> 01:05:13.120]  углу выпишете вот для этой хрени, он у вас абсолютно
[01:05:13.120 --> 01:05:16.720]  сходится, он спокойно распадается на два ряда, чиселки вы
[01:05:16.720 --> 01:05:19.720]  выносите за знак суммы и получается мат ожидания
[01:05:19.720 --> 01:05:24.720]  этого и мат ожидания этого.
[01:05:24.720 --> 01:05:25.720]  Все.
[01:05:25.720 --> 01:05:26.720]  Третье.
[01:05:26.720 --> 01:05:29.240]  Различные варианты для неравенства.
[01:05:29.240 --> 01:05:32.680]  Соответственно, если у вас кси больше либо равно
[01:05:32.680 --> 01:05:37.640]  нуля, то мат ожидания кси тоже будет больше либо равно
[01:05:37.640 --> 01:05:38.640]  нуля.
[01:05:38.640 --> 01:05:46.160]  Если кси у вас больше либо равно, чем это, то мат ожидания
[01:05:46.160 --> 01:05:49.080]  кси будет больше либо равно, чем мат ожидания это.
[01:05:49.080 --> 01:05:52.080]  Так, но это очевидно, опять же, исходя из правого верхнего
[01:05:52.080 --> 01:05:53.080]  угла.
[01:05:53.080 --> 01:05:54.080]  Это видно?
[01:05:54.080 --> 01:05:56.760]  Ну потому что там все слогами у вас не отрицательны, значит
[01:05:56.760 --> 01:05:57.760]  сумма не отрицательна.
[01:05:57.760 --> 01:06:03.960]  Ну вот, здесь что мы с вами получаем, ну отсюда, это перенесли
[01:06:03.960 --> 01:06:07.040]  влево и свели к этому, видно?
[01:06:07.040 --> 01:06:09.520]  Ну и используем еще линейность, чтобы там мат ожидания на
[01:06:09.520 --> 01:06:10.520]  конце.
[01:06:10.520 --> 01:06:14.400]  Так, и следующая вещь, которая нам нужна, то что модуль
[01:06:14.400 --> 01:06:18.280]  мат ожидания кси будет меньше либо равно, мат ожидания
[01:06:18.280 --> 01:06:19.280]  модуль кси.
[01:06:19.280 --> 01:06:21.800]  Ну в предположении, что все математические ожидания
[01:06:21.800 --> 01:06:24.960]  существуют из тех, которые выписаны, но это следует
[01:06:24.960 --> 01:06:25.960]  отсюда.
[01:06:26.960 --> 01:06:29.960]  Угу, хорошо.
[01:06:29.960 --> 01:06:32.960]  Так, четвертое.
[01:06:36.960 --> 01:06:38.960]  Мы до двадцати минут, да?
[01:06:44.960 --> 01:06:48.960]  Ну давайте, самое важное я скажу, хорошо.
[01:06:48.960 --> 01:06:53.960]  Если две случайные величины независимы, то мат ожиданий
[01:06:53.960 --> 01:07:00.960]  и их произведение равняются произведению мат ожидания.
[01:07:00.960 --> 01:07:04.960]  И вот тут всякий раз, то есть когда мы с вами сталкиваемся
[01:07:04.960 --> 01:07:07.960]  с понятием независимости, у нас стандартно все ломается.
[01:07:07.960 --> 01:07:10.960]  Вот здесь снова все сломалось, потому что у нас что?
[01:07:10.960 --> 01:07:15.960]  Ну кси это просто какой-то оператор на множестве случайных
[01:07:15.960 --> 01:07:18.960]  величин, линейный, который себя хорошо ведет.
[01:07:18.960 --> 01:07:19.960]  Окей, да.
[01:07:19.960 --> 01:07:22.960]  Получается что, что если мы умножаем случайные величины,
[01:07:22.960 --> 01:07:26.960]  и мы сделаем вещь такую нестандартную для линейного
[01:07:26.960 --> 01:07:29.960]  оператора, он себя начинает вести очень странно.
[01:07:29.960 --> 01:07:34.960]  Почему-то мы получаем произведение математических ожиданий
[01:07:34.960 --> 01:07:36.960]  этих случайных величин.
[01:07:36.960 --> 01:07:39.960]  Доказательство этой истории очень простое.
[01:07:39.960 --> 01:07:44.960]  Ну вы просто честно расписываете вот то, что у вас есть.
[01:07:44.960 --> 01:07:47.960]  То есть когда мы расписываем мат ожидания кси умножить
[01:07:48.960 --> 01:07:51.960]  то есть опять у нас что?
[01:07:51.960 --> 01:07:55.960]  У нас есть множество значений случайночной кси, множество значений
[01:07:55.960 --> 01:07:56.960]  случайночной это.
[01:07:56.960 --> 01:08:01.960]  Это у нас просто сумма по всем омега, кси от омега,
[01:08:01.960 --> 01:08:05.960]  это от омега, п от омега.
[01:08:05.960 --> 01:08:13.960]  Ну соответственно пускай у нас вот это есть хк, а множество
[01:08:13.960 --> 01:08:16.960]  значений случайночной это, это будет так.
[01:08:16.960 --> 01:08:21.960]  Давайте здесь мы сделаем и, а здесь мы сделаем ж.
[01:08:21.960 --> 01:08:22.960]  Вот так.
[01:08:22.960 --> 01:08:23.960]  То есть у нас что получается?
[01:08:23.960 --> 01:08:30.960]  Это сумма по и, сумма по ж, сумма по омега таким, что
[01:08:30.960 --> 01:08:37.960]  кси от омега это есть хит, это от омега это есть ужит,
[01:08:37.960 --> 01:08:41.960]  п от омега хит ужит.
[01:08:41.960 --> 01:08:44.960]  Я делаю то же самое, что и раньше, я просто перегруппировал
[01:08:44.960 --> 01:08:45.960]  мою сумму.
[01:08:45.960 --> 01:08:49.960]  Дальше я вижу, что вот эти два множителя они не зависят
[01:08:49.960 --> 01:08:50.960]  от индекса суммирования.
[01:08:50.960 --> 01:08:52.960]  Я его могу вынести.
[01:08:52.960 --> 01:08:55.960]  Соответственно получается я суммирую вероятность
[01:08:55.960 --> 01:08:57.960]  элементарных исходов, для которых выполнены вот
[01:08:57.960 --> 01:09:00.960]  эти два требования, ну и получаю результат.
[01:09:00.960 --> 01:09:09.960]  У меня будет хит на у ж, да, на вероятность, ну того,
[01:09:09.960 --> 01:09:12.960]  что выполнено это и это.
[01:09:12.960 --> 01:09:19.960]  То есть кси равняется хит, это равняется у жит.
[01:09:19.960 --> 01:09:22.960]  Ну то есть это вероятность всех таких омег маленьких,
[01:09:22.960 --> 01:09:24.960]  что выполнено и это и это.
[01:09:24.960 --> 01:09:26.960]  Ну то, что у меня здесь было записано.
[01:09:26.960 --> 01:09:27.960]  Понятно же, да?
[01:09:27.960 --> 01:09:28.960]  Теперь давайте посмотрим.
[01:09:28.960 --> 01:09:30.960]  То есть фактически под вероятностью, что написано.
[01:09:30.960 --> 01:09:33.960]  Это пересечение двух событий.
[01:09:33.960 --> 01:09:35.960]  Через точку запятой это логическое и, но логическое
[01:09:35.960 --> 01:09:38.960]  и в теоретическом множественном виде это пересечение двух
[01:09:38.960 --> 01:09:39.960]  событий.
[01:09:39.960 --> 01:09:42.960]  А мне было сказано то, что случайные величины у меня
[01:09:42.960 --> 01:09:43.960]  были независимы.
[01:09:43.960 --> 01:09:46.960]  То есть для любых и и ж, вот эти два события независимы.
[01:09:46.960 --> 01:09:49.960]  То есть их можно расписать как произведение вероятностей.
[01:09:49.960 --> 01:09:59.960]  Кси равняется хит, это равняется у ж.
[01:09:59.960 --> 01:10:00.960]  Дальше я что делаю?
[01:10:00.960 --> 01:10:04.960]  Я вот этот объединяю с этим множителем, ну а у ж
[01:10:04.960 --> 01:10:05.960]  объединяю со своим.
[01:10:05.960 --> 01:10:06.960]  Дальше.
[01:10:06.960 --> 01:10:12.960]  Вот эта хрень от и не зависит, поэтому я ее могу вынести
[01:10:12.960 --> 01:10:13.960]  вот сюда.
[01:10:13.960 --> 01:10:15.960]  От ж, извините, от ж.
[01:10:15.960 --> 01:10:17.960]  Вот это от ж не зависит, я могу вынести за вот этот
[01:10:17.960 --> 01:10:18.960]  знак суммы.
[01:10:18.960 --> 01:10:26.960]  У меня получится хит на вероятность того, что кси
[01:10:26.960 --> 01:10:31.960]  равняется хит умножить на сумму по ж.
[01:10:31.960 --> 01:10:35.960]  То есть у ж на вероятность того, что это равняется
[01:10:35.960 --> 01:10:36.960]  у ж.
[01:10:36.960 --> 01:10:41.960]  В результате вот эта вещь у меня получается, мата
[01:10:41.960 --> 01:10:48.960]  ожидания это, она выносится вот за эту сумму, а вот эта
[01:10:48.960 --> 01:10:50.960]  вещь у меня получается мата ожидания кси.
[01:10:50.960 --> 01:10:56.960]  Я получил то, что я и хотел доказать.
[01:10:56.960 --> 01:10:58.960]  То есть получается мата ожидания, которое изначально
[01:10:58.960 --> 01:11:02.960]  было каким-то простым таким, простой вещью объективно.
[01:11:02.960 --> 01:11:04.960]  То есть это просто линейный оператор на множестве
[01:11:04.960 --> 01:11:05.960]  наших функций.
[01:11:05.960 --> 01:11:06.960]  Все.
[01:11:06.960 --> 01:11:08.960]  Ну то есть как бы в линале это часто встречалось, в
[01:11:08.960 --> 01:11:10.960]  функане будет сейчас встречаться.
[01:11:10.960 --> 01:11:12.960]  Когда возникает понятие независимости, он очень
[01:11:12.960 --> 01:11:13.960]  странно будет себя вести.
[01:11:13.960 --> 01:11:15.960]  А дальше будет вообще класс.
[01:11:15.960 --> 01:11:18.960]  Потому что после этого мы ведем понятие дисперсии,
[01:11:18.960 --> 01:11:22.960]  которая будет играть роль такой квадратичной формы.
[01:11:22.960 --> 01:11:24.960]  То есть это линейная форма, там будет квадратичная
[01:11:24.960 --> 01:11:25.960]  форма.
[01:11:25.960 --> 01:11:28.960]  В случае независимых случайных величин она себя начинает
[01:11:28.960 --> 01:11:30.960]  вести как линейную форму.
[01:11:30.960 --> 01:11:32.960]  То есть дисперсия суммы независимых будет суммой
[01:11:32.960 --> 01:11:33.960]  дисперсии.
[01:11:33.960 --> 01:11:36.960]  Хотя она с точки зрения функана квадратичная форма.
[01:11:36.960 --> 01:11:38.960]  Ну линавая функана.
[01:11:38.960 --> 01:11:40.960]  В общем, будет очень прикольно.
[01:11:40.960 --> 01:11:41.960]  Все.
[01:11:41.960 --> 01:11:43.960]  А мы же закончили в 20 минут?
[01:11:43.960 --> 01:11:45.960]  А, в 30?
[01:11:45.960 --> 01:11:46.960]  Черт.
[01:11:46.960 --> 01:11:48.960]  Слушайте, а я почему тут, я не знаю.
[01:11:48.960 --> 01:11:50.960]  Так, все, значит продолжаем.
[01:11:50.960 --> 01:11:53.960]  Нет, давайте я еще одно свойство тогда мата ожидания
[01:11:53.960 --> 01:11:55.960]  сформулирую, мы тогда разойдемся.
[01:11:55.960 --> 01:11:59.960]  Слушайте, почему вы молчите все время?
[01:11:59.960 --> 01:12:02.960]  Иван Георгиевич, вы в приоте.
[01:12:02.960 --> 01:12:04.960]  Иван Георгиевич, вы медленный.
[01:12:04.960 --> 01:12:06.960]  Иван Георгиевич, вы перепутали опять все.
[01:12:06.960 --> 01:12:10.960]  Нужна обратная связь от аудитории, иначе невозможно.
[01:12:10.960 --> 01:12:11.960]  Давайте.
[01:12:11.960 --> 01:12:13.960]  И последнее, пятое свойство.
[01:12:13.960 --> 01:12:17.960]  Мне нравится Лукаши Буниковского, известный всем присутствующим.
[01:12:17.960 --> 01:12:20.960]  Соответственно, как оно будет выглядеть для
[01:12:23.960 --> 01:12:27.960]  Тервера, то есть мата ожидания модуля ксиет в квадрате
[01:12:27.960 --> 01:12:29.960]  будет меньше, чем вот это.
[01:12:29.960 --> 01:12:33.960]  Ну, как обычно в предположении, то, что все эти вещи существуют.
[01:12:33.960 --> 01:12:37.960]  Для решения вот этой истории мы что делаем?
[01:12:37.960 --> 01:12:40.960]  Мы вводим две новые случайные величины.
[01:12:40.960 --> 01:12:43.960]  Да, ну, как бы первый случай.
[01:12:43.960 --> 01:12:46.960]  То, что пускай у нас
[01:12:46.960 --> 01:12:49.960]  ни одна из этих штук не равна нулю.
[01:12:49.960 --> 01:12:51.960]  Что тогда?
[01:12:51.960 --> 01:12:53.960]  Мы введем новые случайные величины.
[01:12:53.960 --> 01:12:56.960]  Кси, чертой которой есть, кси, деленное на
[01:12:56.960 --> 01:13:03.960]  корень из мата ожидания, кси в квадрате.
[01:13:03.960 --> 01:13:05.960]  Да.
[01:13:05.960 --> 01:13:08.960]  И мы вводим новые случайные величины.
[01:13:08.960 --> 01:13:11.960]  И мы вводим новые случайные величины.
[01:13:11.960 --> 01:13:14.960]  Мата ожидания кси в квадрате.
[01:13:14.960 --> 01:13:16.960]  Да.
[01:13:16.960 --> 01:13:20.960]  И это чертой, который есть, это делить на корень из
[01:13:20.960 --> 01:13:23.960]  мата ожидания, это в квадрате.
[01:13:30.960 --> 01:13:32.960]  Ну, такая нормировка.
[01:13:32.960 --> 01:13:34.960]  Потому что вот это у нас что?
[01:13:34.960 --> 01:13:36.960]  Это случайная величина, а это просто чиселка.
[01:13:36.960 --> 01:13:38.960]  Я разделил случайную величину на чиселку.
[01:13:38.960 --> 01:13:40.960]  Ничего интересного.
[01:13:40.960 --> 01:13:41.960]  Все хорошо.
[01:13:41.960 --> 01:13:45.960]  Теперь стало то, что я могу сказать.
[01:13:45.960 --> 01:13:46.960]  Да.
[01:13:46.960 --> 01:13:47.960]  Теперь смотрите.
[01:13:47.960 --> 01:13:52.960]  Какое неравенство верно для вот этих вот двух случайных
[01:13:52.960 --> 01:13:53.960]  величин?
[01:13:53.960 --> 01:13:54.960]  Неравенство каши.
[01:13:54.960 --> 01:13:58.960]  Мы его все знаем с какого-нибудь седьмого класса, наверное.
[01:13:58.960 --> 01:14:03.960]  То есть вот такой вот модуль, ну два вот таких вот модуля
[01:14:03.960 --> 01:14:09.960]  у нас меньше либо равные, чем сумму таких квадратов.
[01:14:09.960 --> 01:14:13.960]  Ну, то есть эти что?
[01:14:13.960 --> 01:14:14.960]  Да.
[01:14:14.960 --> 01:14:16.960]  Ну, просто.
[01:14:16.960 --> 01:14:20.960]  Теперь, поскольку как бы вот в это мы верим, то есть
[01:14:20.960 --> 01:14:25.960]  можно навешивать математическое ожидание на наше неравенство,
[01:14:25.960 --> 01:14:26.960]  я его навешу.
[01:14:26.960 --> 01:14:28.960]  Двойку у меня вынесу.
[01:14:28.960 --> 01:14:31.960]  У меня получится два мата ожидания модуля кси с чертой
[01:14:31.960 --> 01:14:33.960]  и это с чертой.
[01:14:33.960 --> 01:14:34.960]  Окей.
[01:14:34.960 --> 01:14:36.960]  Так, теперь давайте смотреть.
[01:14:36.960 --> 01:14:40.960]  Чему это равно мата ожидания квадрата вот этой дыряни?
[01:14:40.960 --> 01:14:47.960]  То есть если я вот это возведу в квадрат, ну давайте.
[01:14:47.960 --> 01:14:49.960]  Мата ожидания кси с чертой в квадрате.
[01:14:49.960 --> 01:14:50.960]  Это у нас что?
[01:14:50.960 --> 01:14:54.960]  Мата ожидания кси в квадрате делить на мата ожидания
[01:14:54.960 --> 01:14:56.960]  кси в квадрате.
[01:14:56.960 --> 01:14:59.960]  О, Господи, что это за хрень?
[01:14:59.960 --> 01:15:01.960]  Давайте смотреть.
[01:15:01.960 --> 01:15:03.960]  Вот это кто?
[01:15:03.960 --> 01:15:04.960]  А?
[01:15:05.960 --> 01:15:07.960]  Ну, мат-объект.
[01:15:07.960 --> 01:15:08.960]  Кто это?
[01:15:08.960 --> 01:15:10.960]  Это случайная величина, ну возведенная в квадрат,
[01:15:10.960 --> 01:15:11.960]  ну это новая случайная величина.
[01:15:11.960 --> 01:15:13.960]  Это кто?
[01:15:13.960 --> 01:15:15.960]  Ну мат-объект какой?
[01:15:15.960 --> 01:15:20.960]  Это чиселка, это вот то А из второго свойства, это чиселка.
[01:15:20.960 --> 01:15:23.460]  А мы выясняли, мат ожидания obligations к olsunQU seiner
[01:15:23.460 --> 01:15:25.960]  амперат�а, потому чиселка просто выносится из-под
[01:15:25.960 --> 01:15:27.460]  знака мат ожидания.
[01:15:27.460 --> 01:15:29.960]  То есть у нас получается единицы делить на мата ожидания
[01:15:29.960 --> 01:15:32.460]  к си в квадрате, умножить на мата ожидания к си в квадрате.
[01:15:32.460 --> 01:15:40.380]  Ну то есть зачем я делил вот на эту хрень? Я добился того, что вот мат ожидания квадрата
[01:15:40.380 --> 01:15:50.100]  этой случайночной было 1. Ну и тут тоже будет 1. То есть у меня получается 1 плюс 1. Ну то есть у меня
[01:15:50.100 --> 01:15:58.820]  получается, что в результате мат ожидания вот моего модуля меньше либо равно единичке. Я 2 скоротил.
[01:15:58.820 --> 01:16:07.940]  Хорошо. А давайте теперь вот эту дырянью распишем. Это что такое? Это у меня кси делить на корень из
[01:16:07.940 --> 01:16:17.340]  мат ожидания кси в квадрате. Это делить на мат ожидания это в квадрате из этого корень. Тут
[01:16:17.340 --> 01:16:26.220]  модуля и это меньше либо равно единицы. Можно я напишу 4d? Ну это чиселки, это мат ожидания.
[01:16:26.220 --> 01:16:29.860]  Они выносятся по знакам от ожидания. Потом у меня просто числовое неравенство, которое
[01:16:29.860 --> 01:16:41.980]  я домножаю на эти чиселки и получаю вот это. Все, 4d. Тут на самом деле очень важная история, что если
[01:16:41.980 --> 01:16:51.060]  вдруг достигается равенства, помните там, вы же, у вас где было неравенство к ошибке? Это матан. И
[01:16:51.980 --> 01:16:55.980]  когда вы с евклидовыми пространствами работали, это как неравенство для скалярного
[01:16:55.980 --> 01:17:03.180]  произведения? Может какая боль написана на ваших лицах? Так, подождите, у вас евклидового
[01:17:03.180 --> 01:17:11.900]  пространства был термин такой? Где? На Угеме. Хорошо. И там вы доказывали общий вид неравенства к
[01:17:11.900 --> 01:17:19.180]  ошибке Николаевского для скалярного произведения. Было такое? Да. Хорошо. Ладно. И там было как бы,
[01:17:19.260 --> 01:17:24.340]  что было сказано? Что если у вас неравенство к ошибке Николаевского оказывается равенство,
[01:17:24.340 --> 01:17:41.100]  то тогда что? Там было что-то хорошее. Равенство между кем и кем? Ладно, вернемся к этому обсудку. Я
[01:17:41.100 --> 01:17:45.460]  в общем понял. Надо начать неравенство к ошибке Николаевского для евклидовых пространств в следующий
[01:17:45.460 --> 01:17:49.780]  раз. Напомним это. Все, спасибо, что подошли.

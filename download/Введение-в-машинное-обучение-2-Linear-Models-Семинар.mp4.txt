[00:00.000 --> 00:22.380]  Окей, ребят, ну что, давайте потихоньку начинать.
[00:22.380 --> 00:44.540]  Так, окей, ребят, пока мы не перешли, собственно, к семинару, у меня есть еще
[00:44.540 --> 00:49.500]  парочка очень важных, на самом деле, замечаний, на которые тоже крайне необходимо обратить
[00:49.500 --> 00:54.380]  внимание. Смотрите, я на самом деле про них хотел сказать, но, к сожалению, во время лекции они у
[00:54.380 --> 01:03.100]  меня чуть-чуть сначала вылетели из головы. Ссылочка на практику уже в чатике, но я вам сейчас скажу
[01:03.100 --> 01:09.780]  другую вещь. Давайте вот на эту доску камеру передвинем, я сейчас хочу порисовать. Можно мне
[01:09.780 --> 01:18.740]  порисовать. Классно. Смотрите, в чём очень важно не ошибиться, когда мы с вами говорим про
[01:18.740 --> 01:27.260]  регуляризацию. Первое, помните, что мы с вами изначально сказали, что мы добавляем в нашу матрицу,
[01:27.260 --> 01:32.900]  собственно, вот этот самый фиктивный столбец из единиц, матрица-объект-признак. Так вот,
[01:32.900 --> 01:39.900]  если вы так вдруг на практике сделаете, ни в коем случае не учитывайте ω0 при подсчёте
[01:39.900 --> 01:47.140]  коэффициент регуляризации. Понятно? То есть у вас, когда мы говорим, что мы хотим норму ω, причем
[01:47.140 --> 02:03.500]  неважно какую, отправить на минимум у нас b свободный член, он не принадлежит ω. Понятно,
[02:03.500 --> 02:09.140]  почему, да? Потому что непонятно, потому что иначе вы требуете, чтобы у вас свободный член был
[02:09.140 --> 02:21.540]  как можно ближе к нулю. Что с ней такое, я не могу понять. Эй, работай. Она в режиме, я устала,
[02:21.540 --> 02:31.020]  я понимаю. Слушайте, фиг с ним, мы больше разбираться будем, потом разберём. Смотрите,
[02:31.020 --> 02:36.380]  ещё раз, если вы пытаетесь свободный член загнать к нулю, вот смотрите, вот у вас, допустим, будет
[02:36.380 --> 02:48.580]  вот такой набор точек. Какую прямую надо через него провести? Логично, что вот такую. Правильно? Вот
[02:48.580 --> 02:56.220]  у вас вот такой набор точек. Вот такую прямую надо провести. А если у вас регуляризация учитывает
[02:56.220 --> 03:02.980]  свободный член, то вот здесь у вас клад регуляризации будет сильно больше, чем вот здесь. Это как вообще,
[03:02.980 --> 03:08.380]  почему? Почему вы требуете, чтобы ваша линейная модель обязательно через ноль проходила? Это из
[03:08.380 --> 03:14.060]  каких предположений идёт? Не из каких. Если в это следует из каких-то предположений, что ваша
[03:14.060 --> 03:18.580]  гиперплоскость должна через ноль проходить, тогда замечательно. Если же нет, но вы забыли,
[03:18.580 --> 03:23.180]  что у вас там просто в матрицу объект-признак добавили инфективный столбец, вы просто сделали
[03:23.180 --> 03:28.260]  ровно то самое неправильное предположение, которое приведёт к неправильному решению. Половили?
[03:28.260 --> 03:38.900]  Так мы всего лишь говорим, что у нас, собственно, норма вектора весов, где у нас B в него не входит,
[03:38.900 --> 03:43.340]  стремится к минимуму, что краски приводит к тому, что у него либо, пускай вырубается,
[03:43.340 --> 03:51.100]  либо, грубо говоря, норма минимальная, значит, у нас там решение становится не, решение становится
[03:51.100 --> 03:55.620]  единственным, либо у нас прореживается значение этих самых весов, некоторые признаки выкидываются,
[03:55.860 --> 03:59.140]  всё то, что мы с вами обсуждали. Про свободный член мы ничего с вами не говорили.
[03:59.140 --> 04:10.020]  Кх плюс B, это если у вас в одномерном случае, я просто не умею многомерные картинки рисовать,
[04:10.020 --> 04:22.580]  в общем случае у вас ωх, где ω это уже вектор. А зачем?
[04:22.580 --> 04:36.460]  Ну мы предполагаем, что норма вектора весов как можно меньше, всё. Что такое наклон в многомерном
[04:36.460 --> 04:40.780]  случае? У нас каждый отдельный член должен быть, можно быть, ближе к нулю, если он не влияет на
[04:40.780 --> 04:49.420]  значение ошибки. Вот. Я, честно говоря, вопрос не понимаю. Просто что такое регуляризация, в общем
[04:49.420 --> 05:01.980]  случае мы с вами вроде уже разбирались только что. Потому что наклон у вас зависит, грубо говоря,
[05:01.980 --> 05:05.860]  коэффициент перед каждым признаком зависит от того, какой вклад этот признак делает в решение.
[05:05.860 --> 05:10.820]  Перед свободным членом зависит только от того, насколько у вас ваша целевая переменная сдвинута
[05:10.820 --> 05:17.300]  относительно нуля. Там признаки никак ни на что не влияют. Так, коллеги, это у всех вызывает вопрос,
[05:17.300 --> 05:21.340]  или только здесь. Просто если только здесь, я потом это в конце семинара обсужу, чтобы всех не
[05:21.340 --> 05:34.060]  затягивать. Так, кому непонятно, почему не нужно ограничивать свободный член? Почему не нужно
[05:34.060 --> 05:43.700]  ограничивать свободный член? Кому непонятно? Ну как бы в общем случае, смотрите, у вас задача регрессии,
[05:43.700 --> 05:49.460]  грубо говоря. Вы ищете оптимальную гиперплоску с сетей линейной регрессии. У вас решение,
[05:49.460 --> 05:54.260]  как правило, от того, что вы сдвинете ваши все точки куда угодно, то есть добавите любую константу
[05:54.260 --> 06:00.140]  к ним, у вас суть решения не меняется. Свободный член ровно за эту константу и отвечает. Если вы
[06:00.140 --> 06:03.500]  вдруг его ограничиваете, то вы почему-то говорите, что вы на самом деле не можете никуда сдвинуть,
[06:03.500 --> 06:14.660]  и у вас от сдвига меняется решение. Это что-то странное. Да, именно, это для вектора именно весов
[06:14.660 --> 06:21.500]  без свободного члена. То есть это именно для случая ωх плюс b. Если мы с вами для удобства запихнули
[06:21.500 --> 06:27.180]  в матрицу объект-признак свободный столбец, столбец из единиц, то вы здесь тогда должны
[06:27.180 --> 06:40.300]  норму с отбрасыванием первого члена считать. Да, он вообще, его вообще там нет, это по сути мы
[06:40.300 --> 06:49.460]  для удобства записи только написали и все, больше ни для чего. Все, хорошо? Классно, супер. И второй
[06:49.460 --> 06:53.380]  момент, я думаю, вы на самом деле на это дело уже можете обратить внимание. Коллеги, смотрите,
[06:53.380 --> 06:59.780]  если у нас с вами норма вектора весов вторая используется, и у нас, допустим, вектор весов это
[06:59.780 --> 07:08.380]  1, 1, 1, 10, то из-за какого элемента вектора весов у нас большой коэффициент, большое значение
[07:08.380 --> 07:19.020]  регуляризатора будет? Из-за последнего, правильно? А теперь внимание, вопрос, а если у нас шкалы
[07:19.020 --> 07:23.980]  признаков просто-напросто разные? Первый признак условно порядка единицы, второй порядка единицы,
[07:23.980 --> 07:30.100]  третий порядка единицы, четвертый тоже, а пятый порядка 10 минус второй, а у тоже порядка единицы,
[07:30.100 --> 07:39.020]  и все они одинаково значимы, то какие коэффициенты будут? Все будут порядка одного, кроме того,
[07:39.020 --> 07:43.420]  у кого шкала большая, тогда признак будет маленький, шкала маленькая, признак будет
[07:43.420 --> 07:50.260]  иметь большой вес, согласны? Поэтому регуляризацию вы можете вот такую применять только в условии
[07:50.260 --> 07:56.020]  нормированных данных, потому что иначе вы просто-напросто будете штрафовать признаки в малых шкалах,
[07:56.020 --> 08:00.780]  вы будете пытаться от них избавиться, потому что чем меньше шкала, но больше значимость признака,
[08:00.780 --> 08:16.540]  тем больше у него будет значений коэффициента. Х штрих это х минус мю от х делить на СТД от х,
[08:16.540 --> 08:21.060]  вот вам стандартизация. Минус среднее поделить на дисперсию, для каждого признака по отдельности,
[08:21.060 --> 08:27.580]  ну или мин-макс скейлинг тоже можно, но как правило именно стандартизацию делают 9 из 10 случаев,
[08:27.580 --> 08:31.660]  то есть это именно я вас предостерегаю, потому что, во-первых, на это сейчас не обращать внимания,
[08:31.660 --> 08:37.420]  во-вторых, если вы это сделаете, это именно то, о чем я говорил, это неправильное предположение,
[08:37.420 --> 08:43.300]  которое ломает вам задачу. Если вы не отформировали данные и наложили регуляризацию, вы по сути всем
[08:43.300 --> 08:47.260]  признакам, у которых шкала маленькая, сказали, ты сейчас вообще отсюда улетишь, на тебя неважно.
[08:47.260 --> 08:52.540]  То, что признак маленький, вес тоже маленький, вклад маленький, мы его по сути выкинули. Признак
[08:52.540 --> 08:59.500]  неважный, но в большой шкале, вес маленький, он сильно влияет. Получается фигня. Стандартного
[08:59.500 --> 09:14.020]  отклонения, standard deviation, корень из дисперсии от x. Так это же есть нормировка. Вот, x со штрихом
[09:14.020 --> 09:19.380]  это отнормированный x, x минус среднее делить на дисперсию. Это и есть стандартизация, то есть
[09:19.380 --> 09:23.420]  перед регуляризацией вам надо вот это сделать. Если вы это не сделаете, вы получите какую-то
[09:23.420 --> 09:35.420]  фигню. По причине, обычной выше. Это всем понятно? Хорошо. И возвращаясь назад, это очень похоже на то,
[09:35.420 --> 09:40.220]  что у нас происходило, когда мы говорили про КНН на прошлом занятии. Помните? Если вы признаки
[09:40.220 --> 09:45.500]  не отнормируете, то получается у вас по одной оси, грубо говоря, расстояние в километрах,
[09:45.500 --> 09:50.900]  а по другой сумма в копейках. Получается, различие в одну копейку у вас эквивалентно расстоянию в
[09:50.900 --> 09:55.380]  один километр. Если, грубо говоря, это не соответствует вашим априорным предположениям,
[09:55.380 --> 10:00.700]  то фигня какая-то получается. Поэтому, если у вас нет каких-то предположений о ваших данных,
[10:00.700 --> 10:06.100]  которые вам позволяют не нормировать данные, нормируйте данные. Знаете, вот правило,
[10:06.100 --> 10:12.100]  если вы не понимаете, почему не надо это делать, вы должны это делать. Всё, он нам не нужен.
[10:12.100 --> 10:26.740]  Нет. Ещё раз, смотрите, у вас нормировка, она меняет шкалу признаков. Б отвечает за сдвиг
[10:26.740 --> 10:31.500]  относительно нуля. Грубо говоря, у вас линейное преобразование, в общем случае, это поворот,
[10:31.500 --> 10:37.860]  растяжение, сжатие. Вы за сдвиг, за который бы отвечаете, никак не можете это сделать.
[10:37.860 --> 10:48.020]  И что? А у вас при этом через ноль проходит? По-моему, в общем случае это нельзя гарантировать.
[10:48.020 --> 11:01.500]  Ну, потому что, по идее, да. Для нулевого х тогда у должен быть равен нулю. Разве это верно?
[11:01.500 --> 11:06.980]  По идее, вы тогда ещё у должны отформировать, а вот у лучше вообще не трогать. Целевой
[11:06.980 --> 11:11.060]  переменную лучше не трогать, если вы опять же, если вы не понимаете, зачем вам это надо, вам это не
[11:11.060 --> 11:15.380]  надо. На самом деле, это вообще очень широкое правило, которое работает очень много где. То,
[11:15.380 --> 11:19.300]  что здесь очень много вещей, которые я вам рассказываю сейчас и буду рассказывать дальше,
[11:19.300 --> 11:23.500]  и на самом деле не только в машинном обучении, там на самом деле можно сказать, вот это работа так,
[11:23.500 --> 11:29.140]  так, так-то, но это также на самом деле было на физике. Вот вы механику изучали, а потом,
[11:29.140 --> 11:33.540]  но кто-то не изучал, но тем не менее. Те, кто изучали механику, вы всё равно в школе изучали
[11:33.540 --> 11:42.660]  физику. Даже в школе не было физики. Ну, вы в школе изучали физику, но на уроках было приятно
[11:42.660 --> 11:47.860]  поспать. Тоже вариант. Ну и к чему? Есть, собственно, классическая механика, а есть теория относительности,
[11:47.860 --> 11:52.180]  или там, не знаю, квантовая механика. Это не отменяет просто классической физикой, классической
[11:52.180 --> 11:56.620]  механики, например, термеха. Просто это говорит, что граница применимости есть. В школе-то вам
[11:56.620 --> 12:00.700]  про это не говорили. Тут, на самом деле, то же самое. Тут некоторые вещи, они работают в каких-то
[12:00.700 --> 12:04.980]  предположениях, а когда мы выходим за эти предположения, гораздо более сложные вещи приходится делать.
[12:04.980 --> 12:09.700]  Мы сейчас пока с вами сидим там, где достаточно просто, просто того, что мы только начали. То есть,
[12:09.700 --> 12:14.060]  грубо говоря, простое правило. Если вы понимаете, что правило, которое я вам озвучил, больше не
[12:14.060 --> 12:17.940]  работает, и можете это обосновать, значит, оно больше не работает. Я вот говорю про все эти там вещи.
[12:17.940 --> 12:22.060]  Например, про нормировку. Я могу вам привести пример, когда надо регулировать сегодня свободный
[12:22.060 --> 12:26.940]  член, накладывать и так далее. Только это тогда надо вообще залезть в методы байс, методы оптимизации,
[12:26.940 --> 12:32.340]  в боесовщину, и там повариться где-то еще, не знаю, занятия 5-10. И вот тогда мы найдем частный
[12:32.340 --> 12:37.780]  случай, когда нам необходимо на него регулироваться и накладывать. Сейчас там не надо этого. Хорошо? То
[12:37.780 --> 12:42.900]  есть, просто то, что я говорю, всегда используйте нормировку, если не знаете, что делать, или если
[12:42.900 --> 12:47.580]  вы не видите никаких противоречий. Если видите, это нормально, вы развиваетесь, вы изучаете новые
[12:47.580 --> 12:52.380]  вещи. То есть, это не истинно в последней инстанции. Мы не можем с вами все машинное обучение за там
[12:52.380 --> 12:57.820]  две лекции или даже за 10-20 покрыть. Эту область в ней можно там десятилетиями развиваться. Тут все
[12:57.820 --> 13:07.700]  уловили? Вопросов нет? Тогда давайте на практику посмотрим. Итак, ссылочку в чат я скинул. А что,
[13:07.700 --> 13:18.340]  оно все вырубилось? Там, соответственно, вы можете найти ноутбук. Я, собственно, скинул. Я
[13:18.420 --> 13:22.540]  предлагаю открыть именно решенную версию ноутбука, чтобы самим сейчас не кодить.
[13:22.540 --> 13:31.580]  Ну, слушайте, отсюда всем видно, нет? Просто здесь, кажется, даже гораздо лучше видно.
[13:31.580 --> 13:44.940]  Не, я пока не открыл ничего. Сейчас я QuickTime включу. QuickTime. Ну, слушайте,
[13:44.940 --> 13:50.540]  но доска классная. Она, по ходу, в 4К что ли работает. Я просто на экране практически ничего не вижу.
[13:50.540 --> 13:58.140]  Здесь у меня тут размер шрифтов невыносимый. Интернет, приди, порядок наведи.
[13:58.140 --> 14:01.100]  Сейчас.
[14:01.100 --> 14:05.660]  О, успех.
[14:15.780 --> 14:25.620]  Что, что-нибудь видно или маловато? А он почему-то даже больше не увеличивает. Вот зараза. Ладно,
[14:25.620 --> 14:31.340]  давайте вот так. Так нормально? Окей, на самом деле вы можете просто по ноутбуку дальше идти,
[14:31.340 --> 14:37.860]  там примерно понятно о чем речь. Так что рекомендую. Ой-ой-ой, так только смотрите это. Так как у нас
[14:37.860 --> 14:44.580]  репетиторий по некоторым причинам из MLMivт переехал в MLCourse, я думаю, вы понимаете,
[14:44.580 --> 14:49.540]  по каким причинам. Я думаю, вы слышали о том, сколько организаций на GitHub уже забанили. Вот здесь
[14:49.540 --> 15:02.020]  в ссылочке надо поменять MLMivт на MLCourse. Иначе оно просто не скачается. Вот. Хорошо. Вот. Ну и, собственно,
[15:02.020 --> 15:05.620]  смотрите, сейчас мы с вами... План семинара какой? Мы с вами сначала посмотрим на то,
[15:05.620 --> 15:10.100]  что мы с вами пока декларативно видели на лекции, то есть на ту самую неустойчивость, на то,
[15:10.100 --> 15:15.180]  как у нас работает при использовании регуляризации аналитическое решение, а потом поговорим,
[15:15.180 --> 15:31.140]  почему аналитическое решение, конечно, классно, но при этом оно не используется. Да. 45? Слушайте,
[15:31.140 --> 15:43.380]  понять не имею, если честно. Я люблю random seed 42, потому что 42. Но никакого подаёного смысла нет.
[15:43.380 --> 15:46.940]  Просто зафиксировать random seed, на самом деле, чтобы ноутбук на одном том железе был
[15:46.940 --> 15:54.100]  воспроизводим. Всё. Почему там два раза, видимо, это просто опечатка. Вот. Итак, смотрите. Ну что
[15:54.100 --> 15:58.980]  ж, давайте тогда начнём с простой ситуации. Собственно, у нас есть наша матричка X. Мы её
[15:58.980 --> 16:08.180]  сгенерировали сами. Каким образом? Мы просто взяли получайную матричку из равномерного распределения,
[16:08.180 --> 16:14.060]  и, соответственно, у нас есть omega true, которую мы точно так же придумали. Всё. Тогда мы с
[16:14.060 --> 16:20.660]  вами берём, перенормируем нашу величину, нашу матричку так, чтобы признаков были разные,
[16:20.660 --> 16:28.740]  эти нормы, ненормы, шкалы, и так далее. Ну и получаем наши Y. Вот всё. Можем посмотреть,
[16:28.740 --> 16:35.380]  на самом деле, на наши X и на наши Y. Давайте его даже нарисуем. Я-то вам сейчас просто нарисую PLT,
[16:35.380 --> 16:43.860]  ColorMesh, X. Вот, собственно, у нас матричка из двух признаков. Вы можете увидеть,
[16:43.860 --> 16:55.060]  что первый признак гораздо меньше шкале, чем второй. Согласны? Вот он у нас нарисовался,
[16:55.060 --> 17:02.180]  этот у нас порядка десяток, этот порядка единиц. Вот наша линейная модель, вот наш функционал
[17:02.180 --> 17:04.980]  империатричек-кваритиска, функция империатричек-кваритиска среднего-патриотичная ошибка,
[17:04.980 --> 17:10.140]  сумма квадратов отклонений. Аналитическое решение выглядит так, как мы с вами сказали. Ну давайте
[17:10.140 --> 17:19.140]  вот тогда посчитаем, то есть xtx-1, xtx, xty, точнее. Вот наш omega star, мы его получили. Да,
[17:19.140 --> 17:25.180]  на всякий случай это формально вроде w, но по привычке параметры машин обучения называют omega,
[17:25.180 --> 17:31.260]  а так как на клавиатуре нет omega, обычно omega и w, короче, постоянно друг друга сменяются в
[17:31.260 --> 17:36.700]  точке верии того, что пишем w, а говорим omega. Это, я надеюсь, никого не смущает, окей? Вот наши
[17:36.700 --> 17:44.300]  решения, которые мы получили, 0.47-0.14, вот наш omega true, 0.49-0.13. Ну, мы достаточно близки,
[17:44.300 --> 17:51.060]  с учетом того, что у нас на самом деле был, собственно, нормальный шум добавлен в наш у. Это
[17:51.060 --> 17:54.860]  абсолютно нормально, потому что мы точное решение никак не получим, у нас данные зашумленные. Мы
[17:54.860 --> 18:03.900]  назад уже никак не вернемся. Да, у нас здесь, в данном случае, шум, они абсолютно независимы,
[18:03.900 --> 18:08.740]  они сгенерированы из одного распределения, у них нулевое среднее, единичная дисперсия,
[18:08.740 --> 18:16.140]  и кавриатс, конечно же, равно 0, они независимы вообще. Вот, все, классно. Ну и, соответственно,
[18:16.140 --> 18:20.900]  вот мы его посчитали и так далее. А теперь давайте сделаем все то же самое, но теперь у нас,
[18:20.900 --> 18:28.180]  соответственно, будет что? У нас появляется еще и третий столбец, видите? Минус первый и минус
[18:28.180 --> 18:31.540]  второй равны, на всякий случай, минус первый это последний, минус второй это второй с конца,
[18:31.540 --> 18:36.140]  кстати, в данном случае есть минус третий, а он же нулевой, то есть всего 3 столбца. И они равны с
[18:36.140 --> 18:40.580]  точностью до маленького шума, ε, потому что если они будут полностью равны, у нас просто ничего не
[18:40.580 --> 18:47.460]  обратится, поэтому они просто будут близки. И опять же, мы пытаемся найти наше решение. Вот,
[18:47.460 --> 19:03.300]  заметьте, наше решение истинное, которое у нас было изначально. Что? Простите,
[19:03.300 --> 19:08.900]  сейчас. Смотрите, мы омега тру, мы здесь просто модельную задачу ставим, то есть мы говорим,
[19:08.900 --> 19:12.740]  у нас есть выборка, у нас есть истинная зависимость, вот мы ее только что придумали,
[19:12.740 --> 19:19.540]  а теперь мы на основании истинной зависимости генерируем себе целевые переменные у значения
[19:19.540 --> 19:26.660]  таргета, но зашумляем его, а теперь пытаемся только по х и у восстановить нашу вектор весов. То есть
[19:26.660 --> 19:30.260]  здесь мы просто себе, грубо говоря, придумали нашу зависимость, а теперь мы пытаемся ее восстановить.
[19:30.260 --> 19:39.380]  И мы видим, что у нас здесь веса 0,18 минус 0,85 истинные были, а получил минус 0,69 и 0,68. Видите,
[19:39.380 --> 19:46.020]  опять и не в сумме дают одно и то же, если мы на это посмотрим, но при этом ничего хорошего у нас
[19:46.020 --> 19:51.420]  не получается, в том плане, что у нас решение краски неустойчиво. И заметьте, в чем суть? Вот краски,
[19:51.420 --> 19:56.260]  помните, я вам говорил про неустойчивость, да? У нас уже х зафиксированы, давайте я просто
[19:56.260 --> 20:02.580]  еще раз перестрою y, то есть х я трогать не буду. Смотрите, вот сейчас у нас другое случайное,
[20:02.580 --> 20:08.420]  по идее, случайный шум добавился, можем опять смотреть на нашу омега стар. Да?
[20:08.420 --> 20:16.900]  Мы просто, смотрите, у нас есть какая-то зависимость просто вот с такими-то весами,
[20:16.900 --> 20:21.020]  веса взяли с потолка, я вам могу их сам руками набрать, я буду генератором случайных щелков.
[20:21.020 --> 20:29.620]  Это просто, не, смотрите, мы просто сделали себе обучающую выборку синтетическую, все. Вот истинная
[20:29.620 --> 20:33.220]  омега тру, вот просто она откуда-то пришла, например, из генератора случайных щелков, нам по барабану,
[20:33.220 --> 20:38.220]  она откуда-то пришла, все. Мы пытаемся по x и y восстановить, что там было, откуда мы его взяли,
[20:38.220 --> 20:43.140]  не знаю, вы могли данные людей пронаблюдать, просто здесь на синтетических данных все. И так,
[20:43.140 --> 20:50.460]  заметьте, вот я могу это, грубо говоря, повторить 10 раз, собственно, уро и даже не я этот.
[20:50.460 --> 20:59.580]  Вот, вы можете замечательно увидеть, как это будет меняться постоянно.
[20:59.580 --> 21:09.740]  И смотрите, у нас с вами просто чуть-чуть зашумляется наш y, можно даже поменять дисперсию шума,
[21:09.740 --> 21:15.140]  сделать его еще меньше. И заметьте, у нас с вами выборка обучающих, считайте, не меняется,
[21:15.140 --> 21:23.580]  просто чуть-чуть шум разный, а решение у нас постоянно очень сильно скачан, 0-1, 4-5, 0-0, 1-0 и
[21:23.580 --> 21:28.260]  так далее, 10. Видите, у нас в зависимости от шума, просто случайного, очень сильно меняется выборка,
[21:28.260 --> 21:32.820]  точнее, очень сильно меняется решение. Вот что такое неустойчивость, о которой я вам говорил. То есть,
[21:32.820 --> 21:38.380]  по сути, если мы с вами пронаблюдаем еще, допустим, 5 человек на реальных данных, у нас все равно с
[21:38.380 --> 21:43.580]  какими-то, допустим, шумами, не знаю, там пульс будет считываться, эти шумы приведут к другому решению.
[21:43.580 --> 21:50.100]  Это плохо. А истинное значение у нас вот такое, на самом деле, которое мы бы с вами хотели. Видите,
[21:50.100 --> 21:56.580]  это проблема. Но, на самом деле, сумма-то у нас всегда практически равна. 0,66, 0,67, ну, типа,
[21:56.580 --> 22:03.180]  очень близко друг к другу. Там разница в третьем знаке после запятой. Вот. Чтобы это починить,
[22:03.180 --> 22:08.340]  давайте-ка возьмем скраски за регуляризацию. Что мы тогда можем сделать? Во-первых, обращаю
[22:08.340 --> 22:14.380]  ваше внимание, в данном случае у нас иксы все в одной и той же шкале, никакой перенормировки не
[22:14.380 --> 22:19.780]  происходило. То есть, нормировать их не надо. Окей? Вот. Ну и тогда наше аналитическое решение,
[22:19.780 --> 22:26.620]  то есть, решение вот для вот этой вот штуковины. Вот оно. Получается, NP или налог тра-та-та. Наша
[22:26.620 --> 22:34.180]  лямбда в данном случае равна 0,05. А? Ну, это гиперпараметр. Вот, я выбрал 0,05. Сейчас
[22:34.180 --> 22:43.580]  можем посмотреть, что будет... Как выбирать гиперпараметры глобально? Или что? Чем больше лямбда,
[22:43.580 --> 22:49.420]  тем больше вы обращаете внимание на регуляционный член относительно функций потерь. То есть,
[22:49.420 --> 22:54.220]  грубо говоря, тем больше вы требуете, чтобы у вас было решение с малой нормой, и тем больше вам все
[22:54.220 --> 22:59.260]  равно на то, какая у вас ошибка. Только и всего. Грубо говоря, у вас такая линейная шкала. То есть,
[22:59.260 --> 23:04.420]  чем дальше туда, тем больше вы отдаете предпочтение. Вы на самом деле это можете переписать в каком виде.
[23:04.420 --> 23:10.500]  Вы можете здесь написать, грубо говоря, альфа плюс бета, так чтобы они в сумме давали единицу. Но
[23:10.500 --> 23:15.540]  бета к альфа относилась как к 0,05 к единице. Тогда у вас просто получается линейная выпуклая комбинация
[23:15.540 --> 23:20.340]  двух функций потерь. У вас для функций потерь абсолютно неважно, если вы ее домножите на любую константу.
[23:20.340 --> 23:25.300]  Правильно? Ну вот, можете домножить так, чтобы они в сумме давали единицу. Тогда у вас просто будет
[23:25.300 --> 23:31.500]  линейная их выпуклая комбинация двух функций потерь. Заметьте, вот мы добавили сюда регуляризацию. И
[23:31.500 --> 23:36.180]  давайте теперь сделаем все то же самое. То есть, посмотрим, а что, если у нас с вами меняется каждый
[23:36.180 --> 23:42.660]  раз, собственно, все, что мы хотели с вами сделать. То есть, вот я беру этот код, только теперь
[23:42.660 --> 23:55.060]  омега стар рег будем писать вместо того, что я до этого писал. Вот опять 10 раз. Посмотрите на решение.
[23:55.060 --> 24:00.660]  Видите, у нас данные шумят, а решение получается одно и то же. Вот это, что называется устойчивое
[24:00.660 --> 24:04.580]  решение. Мы можем чуть-чуть поменять данные, ничего не поменяется. Но, заметьте, конечно, если у
[24:04.580 --> 24:09.660]  нас данные будут шуметь сильно, вам, конечно же, не получится гарантировать решение устойчивое,
[24:09.660 --> 24:15.820]  потому что у нас теперь данные очень сильно меняются, мы каждый раз получаем разные решения. А тут
[24:15.820 --> 24:21.020]  дело не в этом. У нас просто каждый раз Y по сути уже разные, потому что у нас сигнал уже меньше шума на
[24:21.020 --> 24:30.620]  самом деле. 100 раз даже стал больше. Он был 0.1, а теперь он 10 по дисперсии. Да, именно. То есть,
[24:30.620 --> 24:34.620]  видите, когда у нас шум, грубо говоря, является именно шумовым, то есть он малый, у нас решение
[24:34.620 --> 24:38.820]  является устойчивым. Оно всегда устойчиво, просто у нас данные уже плохие, это мы никакой регуляризации
[24:38.820 --> 24:43.580]  не починим. Это опять же пример, вот что я вам говорил, garbage in, garbage out. Если данный отстой,
[24:43.580 --> 24:49.100]  хоть 10 раз регулирую, у нас все равно получится какая-то фигня. Но опять же, а вот пример того,
[24:49.100 --> 24:54.300]  что происходит, когда у нас, допустим, большой шум, но давайте у нас будет коэффициент регуляризации,
[24:54.300 --> 25:07.460]  например, 50. Ой, опечатка. Вот, упал. Ну, заметьте, они все равно разнятся, но уже сильно меньше,
[25:07.460 --> 25:17.700]  можно сделать его еще больше. Но почему? Он везде, там в районе уже 0.3-0.4, можно сделать еще больше.
[25:17.700 --> 25:26.900]  Видите, они все приближаются примерно к одной и той же калепсии. Оно выравнивается. Почему? Да
[25:26.900 --> 25:33.020]  потому что у нас регуляризация, просто член второй регуляционный перевешивает на себя. Мы начинаем
[25:33.020 --> 25:38.020]  все больше и больше забивать на нашу ошибку, мы просто смотрим на то, что норма вектора весов
[25:38.020 --> 25:44.500]  была минимальна. Видите, что они почти все одинаковые становятся. Согласны? То есть чем больше
[25:44.500 --> 25:48.620]  у нас члены регуляционного, не надо так делать на практике, это просто вам демонстрация. Чем
[25:48.620 --> 25:57.820]  ближе он туда, тем, соответственно, более однородно у нас получается вектор. Согласны? Вот. Хорошо.
[25:57.820 --> 26:07.220]  Ну что ж, все вроде здесь, разобрались. Что такое истойчивое решение, стало понятней? Замечательно.
[26:07.220 --> 26:12.100]  Ну и собственно, мы с вами можем видеть, что у нас сумма-то на самом деле сохранилась, но мы из всех
[26:12.100 --> 26:17.980]  возможных вот этих самых пар выбрали то, которое доставляет нам наименьшую сумму квадрат.
[26:17.980 --> 26:30.180]  Сейчас до этого дойдем. Вот мы говорим про градиентный спуск. И собственно, почему не
[26:30.180 --> 26:34.660]  используется аналитическое решение? То, что в нем вам надо обращать матрицу xtx или xtx
[26:34.660 --> 26:43.580]  плюс лямд и. А обращать матрицу, сложность будет p куб плюс p квадрат n. Если у вас в матрице,
[26:43.580 --> 26:48.820]  не знаю там, 10 тысяч признаков, то п в кубе, короче, кубическая сложность, вообще говоря,
[26:48.820 --> 26:54.340]  не очень приятная симптомика. Поэтому если у вас много данных, то это дорого. А мы с вами можем
[26:54.340 --> 26:58.620]  просто перейти к градиентному спуску, использовать градиентную оптимизацию. Если у нас функция
[26:58.620 --> 27:03.740]  выпуклая, то мы знаем, что градиентный спуск находит оптимум гарантированно. Правильно? Все это
[27:03.740 --> 27:09.580]  помнят. Ну хорошо, но тем не менее. Градиентный спуск, то есть вы идете всегда по антиградиенту,
[27:09.580 --> 27:15.540]  если у вас функционал, который оптимизируется, является выпуклым. Что такое выпуклый функционал,
[27:15.540 --> 27:21.140]  по крайней мере, все помнят? Это на первой лекции, как правило. Но на всякий случай. Что такое выпуклое
[27:21.140 --> 27:28.180]  множество? Кто не помнит? Выпуклое множество, если вы берете любые две точки в этом множестве и
[27:28.180 --> 27:33.940]  напрямую между этими точками все точки тоже лежат в этом множестве. Окей? Супер. Так вот,
[27:33.940 --> 27:39.220]  если соответственно у нас функция выпуклая, функционал, то градиентный спуск нас приведет
[27:39.220 --> 27:43.500]  в оптимум гарантированно. Если он не выпуклый, то, как понятное дело, гарантии нет. Если у вас там
[27:43.500 --> 27:48.140]  есть локальный минимум, вы можете в него попасть и там сидеть. Никаких проблем. Как бы у нас
[27:48.140 --> 27:53.620]  оптимизация градиентная не работает, не гарантирует нам оптимальное решение, если мы с вами не работаем
[27:53.620 --> 27:58.220]  с выпуклой задачей. Но так как у нас нет ничего лучше, чем градиентная оптимизация практически,
[27:58.220 --> 28:04.340]  мы ее используем всегда. И пытаемся сделать задачу более выпуклой различными способами. Ну,
[28:04.340 --> 28:07.980]  окей, почти всегда, ладно. Иногда мы ее не используем. Все-таки, чтобы быть правдивым.
[28:07.980 --> 28:14.820]  Собственно, вот это называется learning rate или величиной градиентного шага. Так часто делают.
[28:14.820 --> 28:19.060]  Ну, в смысле, ее часто выбирают в зависимости того, как быстро мы хотим сходить. И, собственно,
[28:19.060 --> 28:22.740]  мы с вами точно так же можем градиент явно аналитически выразить. Вот он у нас,
[28:22.740 --> 28:29.380]  2х транспонированное на х омега минус н. Опять же, вывести это, я надеюсь, не у кого проблем нет.
[28:29.380 --> 28:37.060]  Продиференцировать функцию потерь по омеге. Один раз. Хорошо? Не поравнивать, не олел, просто
[28:37.060 --> 28:44.620]  продиференцировать. И, собственно, заметьте, посчитать градиент у нас какой будет? П на н. Вот,
[28:44.620 --> 28:50.300]  П на н. Вместо от П куб или П квадрат на н. Ну, как бы, сильно дешевле. Нам это придется
[28:50.300 --> 28:53.980]  несколько раз повторить, но нам это даже не придется повторять там n раз. То есть, это не
[28:53.980 --> 28:59.060]  ПН квадрат, а ПН, допустим, на 100. Нам 100 шагов, как правило, более чем достаточно, если мы по полной
[28:59.060 --> 29:06.980]  выбраке считаем градиент. А? Н это размер выбраки. А часто используется, собственно, еще градиентный
[29:06.980 --> 29:12.540]  спуск не по всей выбраке, скажем так, не полный градиентный спуск, а градиентный спуск, я не знаю,
[29:12.540 --> 29:18.620]  по бачам или по подвыбракам, говоря по-русски. Но подвыбрака — слово длинное, бач — слово короткое,
[29:18.620 --> 29:22.940]  поэтому в сообществе, как правило, говорят по бачам. Бач — это подвыбрака. Вот мы его будем
[29:22.940 --> 29:26.980]  называть бачом, хорошо? Просто я буду это говорить, меня все равно будет проскакивать, чтобы вас
[29:26.980 --> 29:30.860]  сразу не напрягало. Собственно, как правило, используют градиент даже не по всей выбраке,
[29:30.860 --> 29:35.220]  а по подвыбраке, поэтому даже сложность каждого шага — это П на к, где К — размер подвыбраки.
[29:35.220 --> 29:41.820]  Почему? Сейчас. Потому что, как бы, у нас закон больших чисел, грубо говоря, работает, и если мы с вами
[29:41.820 --> 29:46.060]  градиенту средняем по тысяче элементов или по миллиону, то у нас, в принципе, оценки уже
[29:46.060 --> 29:50.740]  похожи друг на друга будут. Там по десяти элементам, да, он может шуметь. Поэтому если размер бача
[29:50.740 --> 29:55.860]  достаточно большой, то у нас оценка градиента даже на подвыбраке случайной все равно будет
[29:55.860 --> 30:11.060]  достаточно качественной. И все. У вас вопрос был? А это я вам сейчас покажу. Это еще один гиперпараметр,
[30:11.060 --> 30:16.540]  поэтому его тоже надо выбирать. С гиперпараметрами, к сожалению, ситуация такая. Вам надо подобрать
[30:16.540 --> 30:20.820]  хороший гиперпараметр для вашей задачи. Вам их никто не подскажет, у вас нет пути их
[30:20.820 --> 30:25.580]  автоматически выбрать, кроме как какую-то евристику написать, чтобы их автоматом подбирать. Это на то
[30:25.580 --> 30:31.460]  и гиперпараметры. Их надо выбирать эксперту. Вот. Ну и давайте посмотрим как выглядит градиент
[30:31.460 --> 30:35.700]  в спуск. На всякий случай, вот тут построим красивую картинку. Тут просто код, который нам
[30:35.700 --> 30:40.740]  все это рисует. На всякий случай здесь у нас уже, как это называется, пространство, грубо говоря,
[30:40.740 --> 30:46.740]  вытянутое. У нас один признак больше, чем другой в шкалах признаков. Видите, вот у краски. Там один
[30:46.740 --> 30:51.460]  на один умножается, другой на два, третий на три и так далее. Ну и давайте посчитаем краски,
[30:51.460 --> 30:58.860]  как у нас, посмотрим точнее, как себя ведет градиентный спуск. Вот у нас вытянутая какая-то
[30:58.860 --> 31:05.420]  штуковина. Логично, что у нас функция потерь это парабола, в двумерном уже случае. Параболоид
[31:05.420 --> 31:11.820]  получается. Вытянутый параболоид вращение. По-моему, это так называется, правильно? Кто помнит? По-моему,
[31:11.820 --> 31:18.340]  да. Вот мы откуда начали, вот мы куда пошли. Обратите внимание, что у нас на самом деле спуск
[31:18.340 --> 31:23.580]  достаточно эффективный. Мы сначала скачем вниз, потом вверх. Короче, у нас с кимзигдагами ходит.
[31:23.580 --> 31:32.500]  Собственно, внимание, вопрос, почему? Шаг большой, да, а еще почему? Не, оно дошло до оптимума,
[31:32.500 --> 31:39.580]  и шаг у нас, у нас это константное, у нас значение градиента это краски что? Два омега всегда. Ой,
[31:39.580 --> 31:45.300]  простите, это для регуляризации. У нас значение градиента это, собственно, 2х на отклонение. Чем
[31:45.300 --> 31:51.300]  меньше отклонения, тем меньше градиента. Вот. Поэтому, да, он уменьшается. Ну, собственно,
[31:51.300 --> 31:58.980]  почему? А? Не-не-не, тут пока только вторая норма, тут ничего не меняется. Почему оно ползет
[31:58.980 --> 32:13.700]  именно сначала вниз, потом вверх? А почему? А чего я показываю? Я ничего не понял. Не, да,
[32:13.700 --> 32:18.380]  все, каждая точка каждый раз уменьшается. Но почему бы нам, смотрите, оптимальней же было
[32:18.380 --> 32:22.820]  сразу, например, вот сюда шагать, не вот сюда, а на такой же шаг вот сюда. Согласитесь, тогда
[32:22.820 --> 32:31.460]  функция ошибки была бы меньше. Согласны? Почему мы идем сюда, а не сюда? Бинго. Во-первых,
[32:31.460 --> 32:36.580]  мы локально считаем вот здесь. А во-вторых, у нас градиент всегда направлен перпендикулярной
[32:36.580 --> 32:41.140]  линием уровня. Это доказывает, как правило, на первом курсе. У нас градиент всегда перпендикулярной
[32:41.140 --> 32:44.820]  линии уровня нашей функции. Потому что это направление наискорейшего возрастания,
[32:44.820 --> 32:49.700]  если с минусом убывания. Если мы на любой другой угол повернемся, то у нас будет только на синус
[32:49.700 --> 32:53.620]  или на костюм, соответственно, сдвиг, он становится меньше, и так далее. Поэтому мы идем
[32:53.620 --> 32:58.820]  ортогонально вот этой, по сути, прямой, которого здесь. А так как у нас не центрально-симметричный
[32:58.820 --> 33:03.220]  случай, то есть паравал наш вытянут в каком-то направлении, поэтому нам в этом направлении на
[33:03.220 --> 33:08.180]  самом деле идти менее выгодно, чем в этом. Здесь мы как бы быстрее будем идти вниз, поэтому тут
[33:08.180 --> 33:12.100]  градиент больше. И это еще одна причина, почему нормировать данные, вообще говоря, хорошая идея.
[33:12.100 --> 33:17.460]  Потому что если мы сейчас с вами данные отнормируем, давайте я ту же самую картинку построю, но уже с
[33:17.460 --> 33:32.740]  нормировкой. Вот, собственно, смотрите, что без нормировки? Все очень логично, оно идет ровно в
[33:32.740 --> 33:42.380]  центр как и надо, без всяких колебаний. Согласны? Ну и на всякий случай, опять же, что будет,
[33:42.380 --> 33:53.060]  если здесь, наоборот, как бы еще больше его раскинуть. Ладно, оно не смогло от такой большой,
[33:53.060 --> 34:01.300]  как бы, короче, плотлив сломался, это нормально, так бывает. Ну ладно, ему что-то плохо становится,
[34:01.300 --> 34:13.140]  извините, пожалуйста, я верну туда двойку. Короче, идею вы поняли. Вот сейчас, вот это без нормировки.
[34:13.140 --> 34:19.060]  На самом деле мы-то идем правильно, мы всегда идем ортогонально линией уровня. Просто у нас,
[34:19.060 --> 34:23.860]  как бы, в одном направлении вот в этом градиент меньше, потому что она вытянута более, у нас
[34:23.860 --> 34:28.340]  более пологий склон, а в этом направлении у нас склон очень крутой, там градиент большой. Все.
[34:28.340 --> 34:36.420]  Ладно, это, собственно, первая ситуация. Но, во-первых, еще раз, давайте я вам покажу просто красивую
[34:36.420 --> 34:45.860]  картинку. Кстати, этот ноутбук частично базируется на наработках Евгения Соколова из вышки. Замечательный
[34:45.860 --> 34:50.500]  курс, тоже у них есть у них есть репетитории, тут есть ссылка, так что тоже рекомендую посмотреть
[34:50.500 --> 34:55.460]  в качестве доп-материала, классно заходит. Вот, соответственно, вот как направлены антиградиенты
[34:55.460 --> 34:59.500]  везде, так на всякий случай, чтобы вы визуально видели, что они всегда перпендикулярованы линиям
[34:59.500 --> 35:04.020]  уровня. Но, опять же, это, вроде, совсем базовые вещи. А теперь давайте возьмем, собственно,
[35:04.020 --> 35:08.420]  по подвыборкам градиентный спуск или стахастический градиентный спуск с размером выборки под
[35:08.420 --> 35:19.180]  выборки бача равным 10. Раньше мы каждый раз считали градиент по всей выборке целиком. Там размер
[35:19.180 --> 35:31.260]  выборки. Там было 300 объектов, мы каждый раз на 300 объектов считали. Теперь будем на 10. Давайте
[35:31.260 --> 35:38.900]  посчитаем. Мы каждый раз выбираем с вами случайные 10 объектов, для них все считаем. Вот наше
[35:38.900 --> 35:44.100]  аналитическое значение градиента. Ну, вроде, тут копировать соб нечего. Заметьте, оно вроде
[35:44.100 --> 35:48.980]  сходится, но заметьте, что мы гораздо более шумно стали болтаться вокруг центра. Почему так
[35:48.980 --> 36:03.580]  произошло? Ваши ставки. Да. Смотрите, почему это происходит. Да, потому что, во-первых,
[36:03.580 --> 36:08.380]  мы с вами берем только маленькую подвыборку, но на самом деле это не единственная причина. Вторая
[36:08.380 --> 36:12.780]  причина заключается, собственно, в том, что у нас в данных есть шум. Если мы с вами уберем этот шум,
[36:12.780 --> 36:26.340]  то у нас никаких проблем не будет. Смотрите, он по подвыборкам идет, но тем не менее, видите,
[36:26.340 --> 36:31.020]  оно сошлось в центр и замечательно там стало. У нас стахастический градиентный спуск делает,
[36:31.020 --> 36:37.420]  что он каждый раз 10 или размер выборки объектов выбирает, по ним считает ошибку, по ним считает
[36:37.420 --> 36:42.580]  градиент, усредняет этот градиент по 10 объектам, идет туда, куда показал градиент, ну антиградиент.
[36:42.580 --> 36:48.260]  Поэтому если у вас попали объекты, у которых, грубо говоря, все показания завышены, он будет
[36:48.260 --> 36:53.620]  пытаться, наоборот, предсказывать большую величину. Если все показания занижены, меньше и так далее. То
[36:53.620 --> 37:00.140]  есть у нас шум влияет на наши шаги обновления. И, собственно, именно поэтому стараются выбирать
[37:00.140 --> 37:06.300]  достаточный размер подвыборки, потому что чем больше размер подвыборки, тем больше мы объектов
[37:06.300 --> 37:10.020]  усредняем, соответственно, тем меньше у нас шум, потому что мы предполагаем, что шум у нас случайный и
[37:10.020 --> 37:14.620]  центрированный. То есть мы ничего не завышаем, как бы у нас нет систематической ошибки, у нас случайная
[37:14.620 --> 37:24.500]  ошибка. Окей, тут понятно? Ну, грубо говоря, у нас размер бача не позволяет компенсировать тот шум
[37:24.500 --> 37:27.900]  достаточно сильно, поэтому мы болтаемся. То есть когда мы болтаемся вокруг центра, это означает,
[37:27.900 --> 37:33.540]  что у нас, грубо говоря, характерно размер шума сопоставим с тем, что мы получаем в конце, как бы
[37:33.540 --> 37:37.740]  отклонением от оптимума. Это нормально. То есть мы все равно бы увидели, что у нас функция потерь
[37:37.740 --> 37:43.340]  вышла на какой-то плато. Другое дело, что, возможно, мы могли дойти куда-то до более хорошей точки,
[37:43.340 --> 37:52.740]  но мы на тут не смогли добраться. Если уменьшить гридентный шаг, вы тоже правы, у нас будет более
[37:52.740 --> 38:00.540]  устойчивая сходимость. И, собственно, сейчас до этого и дойдем. Не, на самом деле, класс, задавайте,
[38:00.540 --> 38:04.020]  пожалуйста, вопрос. То, что я говорю сейчас, до этого дойдем. На самом деле, вследствие просто того,
[38:04.020 --> 38:08.660]  что, как вы понимаете, курс все-таки не первый год читается, и ноутбуки и материалы дорабатываются,
[38:08.660 --> 38:22.980]  в том числе по вашим вопросам. Так что задавайте вопрос. Ага. Да, ну, собственно, если у вас несколько
[38:22.980 --> 38:28.860]  минимумов, да, если у вас несколько минимумов, то вы попадете в любой локальный минимум и никаких
[38:28.860 --> 38:45.580]  гарантий, что вы пойдете в главный оптимум, у вас нет. Ну, это, скажем так, это работает скорее в том
[38:45.580 --> 38:50.900]  смысле, что вы из глобального минимума с меньшей вероятностью выпадете в локальный, потому что
[38:50.900 --> 38:56.060]  локальный выше глобального. Вот все. А за счет стахастического гридентного спуска, за счет стахастики,
[38:56.060 --> 39:00.700]  у вас более шумные грубые, у вас шаги более случайные, вы можете из него выпрыгнуть. Но вообще говоря,
[39:00.700 --> 39:07.740]  в общем случае и стахастический, и полновыборочный гридентный спуск, если у вас задача не выпуклая,
[39:07.740 --> 39:12.700]  то никаких гарантий на то, что вы найдете глобальный минимум, у вас нет вообще. Все, конец. Здесь как
[39:12.700 --> 39:17.380]  бы полномочия гридентной оптимизации, все. Она не работает, она вообще не дает никаких гарантий.
[39:17.380 --> 39:23.900]  Она работает в тех предположениях, которые есть. Собственно, а про размер шага абсолютно верно
[39:23.900 --> 39:30.500]  заметили. Мы видим, что он болтается где-то вокруг оптима, и на самом деле здесь есть пара вещей. Во-первых,
[39:30.500 --> 39:37.420]  есть красивые условия Робинса Монро, которые обычно вот на этом занятии мы просто говорим, про них
[39:37.420 --> 39:42.980]  все слушают, где-то они остаются на задворках памяти, но на практике их, как правило, не используют. Так
[39:42.980 --> 39:47.380]  же давайте я вам сначала скажу про них, а потом скажу, как делать на практике. Собственно, давайте
[39:47.380 --> 39:51.820]  вспомним чуть-чуть матанта, второй курс, ряды и так далее. Вам ничего не напоминает вот эта штука.
[39:51.820 --> 39:58.100]  У нас вот этот ряд расходится, а ряд уже квадратов сходится. Это еще раз шаг на катом шаге. Размер
[39:58.100 --> 40:10.940]  learning rate на катом шаге. Гармонический ряд. А, ну 1 на n, например, да, 1 на n будет работать.
[40:10.940 --> 40:16.500]  Собственно, если есть условия Робинса Монро, там есть работа классная, ее даже можно почитать. Сейчас
[40:16.500 --> 40:23.540]  мы ее читать, конечно же, не будем, тем более она вон скачивается. Он доказал, что если learning
[40:23.540 --> 40:29.260]  rate, грубо говоря, убывает достаточно медленно, но при этом квадрат всех этих элементов это сходящийся
[40:29.260 --> 40:39.380]  ряд, то вы можете, в принципе, дойти с любой задней точностью до любого оптима. Собственно,
[40:39.380 --> 40:44.060]  почему это происходит? Ну вот здесь есть простое объяснение, что, во-первых, то, что у вас ряд этот
[40:44.060 --> 40:48.620]  расходится говорит о том, что вы можете до любой точки дойти, откуда бы вы ни стартовали, вы всегда
[40:48.620 --> 40:53.460]  можете добраться куда угодно. Он не слишком быстро сходится. А с другой стороны, вы можете, он
[40:53.460 --> 40:58.260]  достаточно быстро сходится, чтобы вы, в конце концов, с любой заданной точностью дошли до оптима.
[40:58.260 --> 41:10.020]  Не, погоди, если у вас стахистический гряднецный спуск, нет, не сойдетесь. Ну и, собственно,
[41:10.020 --> 41:14.940]  давайте посмотрим на пример краски гармонических ряд. Там 1 делить на n, например, будет работать. Ну
[41:14.940 --> 41:20.300]  или, на самом деле, можно любую другую взять, например, 1 делить на n в степени 0,5, 0, 0, 5. Ну,
[41:20.300 --> 41:28.980]  короче, любая штука, которая в квадрате, будет давать степень больше единицы. Вот. Ну вот,
[41:28.980 --> 41:36.300]  пожалуйста, наш стахистический гряднец спуск тогда. Тебе что, плохо что ли? Ладно.
[41:36.300 --> 41:47.820]  Не, мы попали в центр, просто что-то, собственно, стахастика. Давайте я просто шум поменяю,
[41:47.820 --> 41:52.700]  чтобы оно рисовалось лучше. Так как бы, как вы понимаете, оно ничем особо не влияет,
[41:52.700 --> 41:59.260]  мы все равно сошлись, просто видно плохо. Ну и просто на экране достаточно плохо видно. Давай рисуйся.
[42:07.300 --> 42:17.460]  Да, давайте я просто learning rate в начальной сделаю. А? Во, пожалуйста. Ну, короче, там просто
[42:17.460 --> 42:24.620]  начальный learning rate был очень большой, поэтому он очень далеко скакал. А? Это, это, вот это вот
[42:24.620 --> 42:30.460]  самое. Это на градиент. Это тот, а? Не-не-не, в данном случае, а, размер шага. Простите, я думал,
[42:30.460 --> 42:34.060]  размер выборки нет. Да, это именно размер шага. Тот коэффициент, грубо говоря, на который мы
[42:34.060 --> 42:38.740]  его домножаем. Вот, пожалуйста, если он поменьше, то мы и сошлись. Тут у нас уже градиент маленький.
[42:38.740 --> 42:42.500]  Тут мы просто выпрыгивали слишком далеко. Там у нас опять был большой градиент, он несколько
[42:42.500 --> 42:49.140]  раз болтался. Ну, это нормально, на самом деле. Собственно, но так делают обычно, скажем так,
[42:49.140 --> 42:54.820]  на практике редко. Типа, в какую-то ивристику, как он должен сходиться. Плюс, для невыпуклых
[42:54.820 --> 43:00.220]  задач это работает плохо. Поэтому что делают, как правило, на практике? Во-первых, делают так
[43:00.220 --> 43:06.700]  называемый learning rate, scheduler, scheduler. Короче, штуку, которая по какому-то расписанию вам
[43:06.700 --> 43:13.260]  понижает learning rate. Причем это может происходить даже с какими-то умными, скажем так, стратегиями.
[43:13.260 --> 43:20.060]  Ну, например, что делать, если у вас слишком большой learning rate, вы болтаетесь вокруг оптимума? Его,
[43:20.060 --> 43:25.820]  по идее, надо понизить, правильно? Чтобы не болтаться. Но тогда вопрос, а во сколько раз его понижать?
[43:25.820 --> 43:30.860]  Непонятно. Собственно, и когда это делать? Понятное дело, руками это все делать лень,
[43:30.860 --> 43:37.100]  поэтому люди давно уже придумали, как это делать автоматом. Ну, например, берут и делают вот такую
[43:37.100 --> 43:41.860]  зависимость. У вас при выходе на плато, например, это в том же самом пайторче, так и называется,
[43:41.860 --> 43:47.500]  reduce LR on plateau, понизь learning rate на плато, там правило какое? Вот у вас график, грубо говоря,
[43:47.500 --> 43:54.860]  строится от шага. Если видит модель, что learning rate, грубо говоря, точнее loss примерно, верни мне
[43:54.860 --> 43:59.780]  мой график, loss примерно не меняется, то есть по факту он у вас на самом деле будет такой зашумленный,
[43:59.780 --> 44:04.660]  типа вот такой, но видите, что он болтается где-то там около одного и того же уровня. Тогда срабатывает,
[44:04.660 --> 44:09.700]  грубо говоря, правило, что за последние 10 шагов среднее значение ошибки не упало, и тогда у
[44:09.700 --> 44:16.620]  срона у нас LR, например, там домножается на 0.1, ну или на 0.5. Короче, на какое-то число меньше
[44:16.620 --> 44:20.620]  единицы. После этого learning rate падает, как правило, после этого ошибка опять начинает чуть-чуть падать
[44:20.620 --> 44:24.980]  вниз, выходит на новое плато. Так можно сделать несколько раз, но каждый раз у вас будет все меньше,
[44:24.980 --> 44:29.900]  меньше, меньше вот этот вот прирост, потому что это обычно происходит почему? У вас есть какой-то
[44:29.900 --> 44:36.980]  оптимум, который вот условно здесь вот еще вот вниз, здесь еще вниз, и вот здесь вот он у вас. Тут
[44:36.980 --> 44:41.740]  вы могли болтаться по этим краям, потом вы дошли куда-нибудь вот сюда, потом вы дошли уже сюда в
[44:41.740 --> 44:45.820]  самый оптимум. То есть каждый раз вы будете, как правило, меньше, меньше прирост получать, но
[44:45.820 --> 44:58.580]  понижение learning rate это штука классная. Не, погодите, стало плохо, в смысле это хуже,
[44:58.580 --> 45:06.940]  чем было точно не будет? Да, так это конечно может. Бинго, это самый простой случай, он работает
[45:06.940 --> 45:12.500]  только если вы находитесь около, вообще говоря, выпуклой задачи работаете. Собственно, что делают,
[45:12.500 --> 45:17.500]  когда задача не выпуклая, то есть в большинстве случаев. Как правило, на него еще наворачивают
[45:17.500 --> 45:23.420]  что-нибудь типа любой периодической функции, то есть он на самом деле то падает, то потом растет.
[45:23.420 --> 45:27.860]  Зачем это нужно? Чтобы если мы вдруг попали в какой-то локальный минимум, мы попытались из него
[45:27.860 --> 45:32.140]  выпрыгнуть. То есть мы опять его повышаем, наша модель куда-то ускакивает, и потом если мы опять
[45:32.140 --> 45:36.980]  туда же сойдемся, хорошо, если мы выпадем в другой минимум, ну замечательно. Вот все. Ну и
[45:36.980 --> 45:45.180]  собственно вторая вещь запоминается для... Ладно, он мне больше не нужен. Мы из него выпасть не можем,
[45:45.180 --> 45:49.500]  ну мы можем из него выпасть локальный, и поэтому, собственно, я не договорил. Запоминается, собственно,
[45:49.500 --> 45:54.420]  лучшие на текущий момент значения модели, например, по качеству на отложенной выборке,
[45:54.420 --> 45:59.060]  и соответствующие параметры. То есть мы до него добрались, это мы уже помним. Пытаемся дойти куда-то
[45:59.060 --> 46:04.540]  еще. То есть хуже мы точно не сделаем, мы помним все свои на лучшие результаты. Вот. Но опять же,
[46:04.540 --> 46:08.700]  если задача не выпуклая, никаких гарантий, что вы получите глобальный оптимум, вас вообще нет.
[46:08.700 --> 46:15.580]  По объективным причинам. Окей. Ну а теперь давайте посмотрим на последнюю вещь. Смотрите, тут
[46:15.580 --> 46:21.020]  собственно в семинаре две части есть. Первая часть, она обязательная, и мы ее как раз сейчас закончим до
[46:21.020 --> 46:26.740]  восьми. Вторая часть опциональная, поэтому можем ее тоже с вами пройти. Я, когда закончу обязательную,
[46:26.740 --> 46:30.140]  вам про нее скажу. На нее идет еще, наверное, минут десять, то она любопытная, но вообще,
[46:30.140 --> 46:36.020]  говоря опционально, именно опционально. Просто прикольно. Давайте просто сравним время, с
[46:36.020 --> 46:42.700]  которой сходится скорость сходимости СГД и просто СГД. Ну СГД соответственно по бачам, ГД по объектам.
[46:42.700 --> 46:55.140]  Размер. Что-то здесь. Сейчас, момент. Я уже помню эту ошибку.
[47:00.140 --> 47:25.020]  Ну зачем оно так? P, range, len. Вот так. Короче, я тут что-то перемудрился во время с кодом.
[47:25.020 --> 47:42.900]  Так сильно проще. Вот. И что? Где картинка? А, я понял. А вот XSlim, это собственно len residuals.
[47:42.900 --> 47:59.020]  Вот. Собственно, вот наш СГД, как сходится. А где же наш градиентный спуск, простите меня.
[48:12.900 --> 48:25.540]  А, все, спасибо. Вы абсолютно... А, я понял. Это все, я понял. Это ноутбук, который на коленке ковыряли.
[48:25.540 --> 48:32.180]  Спасибо. Вот, собственно, смотрите. Вот у нас градиентный спуск, как сходится.
[48:32.180 --> 48:36.980]  Наметьте, он там за 100 шагов уже где-то около нуля с ошибки находится. В принципе, мы сошлись.
[48:36.980 --> 48:41.380]  Дальше он почти не меняется. Вот стахастический градиентный спуск вроде как идет медленнее,
[48:41.380 --> 48:47.860]  да? Но как бы с другой стороны, можем даже перестроить, если хотите, этот график. Если посмотреть
[48:47.860 --> 48:56.180]  на то, чтобы здесь кстати... А, здесь как раз раньше код был, собственно, для числа объектов. Я понял.
[48:56.180 --> 49:11.140]  Вот. На nobjects, а здесь на batch size. Вот, собственно. А теперь посмотрим на количество объектов,
[49:11.140 --> 49:16.780]  которые понадобилось стахастическому градиентному спуску, чтобы сходить и полному. Наметьте,
[49:16.780 --> 49:22.420]  то есть мы потратили там, я не знаю, сколько это, 10 в пятый, то есть где-то 2 на 10 четвертых объектов
[49:22.420 --> 49:27.820]  для стахастического градиентного спуска, и все, мы уже сидим где-то около оптимума. По всему бачу
[49:27.820 --> 49:34.420]  нам пришлось делать шагов гораздо больше. Видите? То есть вроде как мы шагов-то сделали градиентным
[49:34.420 --> 49:38.980]  спуском меньше, чем стахастическим градиентным спуском, но зато на каждом мы использовали 300
[49:38.980 --> 49:47.260]  объектов, а не 10. От размера выборки. Да, соответственно, нам чем больше объектов в выборке,
[49:47.260 --> 49:51.540]  тем сложнее. На самом деле это не совсем так. Почему? Потому что у нас, грубо говоря, есть векторные
[49:51.540 --> 49:56.860]  инструкции в процессорах, они у нас до какой-то степени могут, грубо говоря, за константное
[49:56.860 --> 50:01.780]  время обрабатывать там и 4 объекта, и 8, и 16, и 32, и так далее. Потому что они одновременно на
[50:01.780 --> 50:06.940]  одном такте будут обрабатываться. Потому что вектор инструкции есть. Но, тем не менее, на больших,
[50:06.940 --> 50:10.900]  грубо говоря, объемах все равно. Чем больше размер выборки, тем дольше по ней считать,
[50:10.900 --> 50:16.580]  тем более устойчивый градиент, но тем дороже это нам будет. То есть все равно стахастический
[50:16.580 --> 50:23.580]  градиентный спуск имеет смысл. Ну вот, на самом деле. Ну и последнее, что хочется сказать этого
[50:23.580 --> 50:29.700]  красивого. Смотрите, две вещи. Первое, помните, я там говорил, есть всякие мопе, смопе и так далее.
[50:29.700 --> 50:34.660]  Короче, что это такое, почитать можно всегда допам, это не особо критично, но от коэффициент
[50:34.660 --> 50:40.420]  детерминации хочется, чтобы вы все знали, помнили и главное понимали. Потому что он очень любим в
[50:40.420 --> 50:46.540]  экономике, эконометрике, в банках и вообще в регрессии. Им очень часто пользуется. Собственно,
[50:46.540 --> 50:51.260]  вот коэффициент детерминации. Давайте не унимательно посмотрим, что это такое. Во-первых,
[50:51.260 --> 50:55.740]  он очень похож на среднюю квадратичную ошибку. Согласны? Вот она у нас тут сидит внутри, по сути.
[50:55.740 --> 51:01.860]  Но при этом он на самом деле что нам показывает. Мы берем среднюю квадратичную ошибку, а просто
[51:01.860 --> 51:07.380]  квадратичную ошибку без усреднения и сравниваем, насколько наша модель, по сути, лучше или хуже,
[51:07.380 --> 51:12.140]  чем просто средняя. Вот все. То есть мы говорим, у нас есть среднее значение нашей целевой
[51:12.140 --> 51:17.940]  переменной, просто предсказанной целевой переменной константой, вообще без всяких параметров. Если наша
[51:17.940 --> 51:23.980]  модель лучше, то, соответственно, числитель будет меньше, чем знаменатель и, соответственно, вот эта
[51:23.980 --> 51:30.460]  штука будет ближе к единице, правильно? Если наша модель каким-то образом хуже, чем средняя, то
[51:30.500 --> 51:36.220]  числитель больше, чем знаменатель, и тогда у нас r квадрат вообще отрицательный. Именно что это r2 на самом
[51:36.220 --> 51:40.060]  деле, а не r квадрат, поэтому не надо думать, что он не может быть отрицательным. Его просто так называют.
[51:40.060 --> 51:47.620]  Эффициент детерминации. Хорошо? И как бы дурацкий вопрос с собеседованием, с экзаменом и так далее. Он
[51:47.620 --> 51:57.340]  сверху ограничен, а снизу? Вот именно. Очень часто, пустя какое-то время, у людей почему-то в голове
[51:57.340 --> 52:01.940]  возникает ощущение, что он и снизу ограничен. Ни разу он снизу не ограничен. Как бы худший случай,
[52:01.940 --> 52:08.500]  когда у нас модель вообще ужасная, любой. Как бы худший осмысленный случай, если наша модель хуже,
[52:08.500 --> 52:12.940]  чем средняя, то он равен нулю. То есть, грубо говоря, если он меньше нуля, то значит наша модель,
[52:12.940 --> 52:18.340]  заменяем на среднюю, уже радуемся, все. Вот. То есть, грубо говоря, осмысленное значение у него от нуля
[52:18.340 --> 52:22.140]  до единицы. Все, что меньше нуля, это бред какой-то. Нашу модель надо просто выкидывать, заново делать.
[52:22.140 --> 52:30.900]  Проверка, в смысле, про вопрос? Если модель меньше нуля, то да.
[52:30.900 --> 52:46.900]  А, ну короче. Да. Ну, ее очень часто используют, да. То есть, по сути, во-первых, она чем хороша? Она у вас
[52:46.900 --> 52:51.540]  не зависит от шкалы вашей целевой переменной, видите? То есть, у вас там могут быть хоть миллионы,
[52:51.540 --> 52:57.780]  хоть нули, 0,001, там, 10-5. У вас все равно, чем ближе к единице, тем лучше. Она нормируется, собственно,
[52:57.780 --> 53:02.060]  в этом ее плюс. Ну плюс, ее экономисты любят там за какие-то свойства. Короче, в банках она вообще
[53:02.060 --> 53:09.220]  сплошь рядом везде в регрессии. Поэтому знать про нее лучше надо, а? Коэффициент детерминации или R2
[53:09.220 --> 53:22.980]  score или R2? Ну, на английском R2 score или коэффициент детерминации, determination coefficient, как он называется?
[53:22.980 --> 53:40.380]  Я не понял вопрос. Давайте сейчас тогда вам тут отвечу, а то уже я не слышу вас просто. Ну ладно,
[53:40.380 --> 53:46.660]  ребята, и последний, собственно, момент. Смотрите, все как бы обязательно на этом заканчивается. В
[53:46.660 --> 53:51.700]  скалерне, понятное дело, все это написано. Вы можете этим замечательно пользоваться. Но простой
[53:51.700 --> 53:56.340]  пример, собственно. Вот почему... А, кстати, очень важные... У меня очень много очень важных
[53:56.340 --> 54:02.380]  замечаний, я понимаю. Но еще одно очень важное замечание. Читайте доки, пожалуйста. Вот все,
[54:02.380 --> 54:06.660]  что вы используете, где угодно, на самом деле, в любом случае, когда прогрете, читайте доки. Но
[54:06.660 --> 54:11.020]  конкретно в машинном обучении, если вы берете какую-то либу, из нее что-то импортируете, читайте,
[54:11.020 --> 54:15.660]  пожалуйста, доки. Что она на самом деле делает? Например, если вы почитаете под капотом,
[54:15.660 --> 54:20.100]  линейная регрессия из-за скалерна, именно вот просто линия regression, под капотом использует
[54:20.100 --> 54:27.500]  аналитическое решение. А, например, вот где он там называется? Какой-нибудь Lassa Ridge, это красая
[54:27.500 --> 54:33.300]  ледяная ледовая регуляризация, или SGD Regressor использует сахастическое решение. Я к чему говорю?
[54:33.300 --> 54:38.300]  Давайте просто посмотрим. Вот у нас решение, грубо говоря, задачки регрессии уже на больших
[54:38.300 --> 54:44.180]  скавычках данных, на большой выборке. 700 признаков. Сколько у нас тут? 100 тысяч объектов. Собственно,
[54:44.180 --> 54:51.940]  вот наша линейная регрессия. Она считается, считается, считается, считается. Ну, скорость
[54:51.940 --> 55:01.180]  досчитается. Вот, она 8 секунд, почти 9 считалось. Вот у нас Ridge, который с альфой 0, короче,
[55:01.180 --> 55:05.780]  это линейная регрессия, но градиентная. Альфа 0, то есть коэффициент регуляризации 0, но он
[55:05.780 --> 55:16.460]  использует уже градиентное решение под капотом. Ну, как бы, одна секунда. Как бы, R2 0,99, 0,99.
[55:16.460 --> 55:20.980]  Разница почти в 10 раз по скорости. Поэтому, пожалуйста, читайте, что модели, которые вы
[55:20.980 --> 55:25.500]  используете, используют под капотом. То, что можно просто взять, типа, его туда, например, дёрнуть,
[55:25.500 --> 55:30.740]  регуляризация вам почему-то не нужна. Работает всё 10 раз дольше, просто пружь не та модель. Вот,
[55:30.740 --> 55:37.300]  как-то так. Ну, что ж, на этом вся обязательная часть занятия закончена. Тут есть анализ
[55:37.300 --> 55:41.100]  нестабильности. Если хотите, можем сейчас посмотреть. Я бы, на самом деле, его куда-нибудь
[55:41.100 --> 55:44.820]  на следующих занятиях вытащил, потому что он нетривиальный, и, мне кажется, все устали. И
[55:44.820 --> 55:49.460]  вторую домашку по линейной регрессии я вам тоже прямо сейчас, на самом деле, выгружу. Во-первых,
[55:49.460 --> 55:54.380]  потому что первая, ну, она очень простая, там, на 20 минут работы. У неё, соответственно, дедлайн
[55:54.380 --> 55:58.220]  будет три недели, чтобы вы могли совершенно спокойно её писать. Ну, всё.

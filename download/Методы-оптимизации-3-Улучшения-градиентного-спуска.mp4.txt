[00:00.000 --> 00:15.440]  Так, сегодня, соответственно, продолжаем разговор с методами
[00:15.440 --> 00:19.400]  в духе градиентного спуска, соответственно, на чем
[00:19.400 --> 00:23.440]  мы остановились в прошлый раз, остановились мы на
[00:23.440 --> 00:28.440]  следующем.
[00:28.440 --> 00:31.060]  Смотрим оценку для градиентного спуска, какая оракульная
[00:31.060 --> 00:32.060]  сложность.
[00:32.060 --> 00:34.920]  Соответственно, у него для решения L гладкая mu сильно
[00:34.920 --> 00:35.920]  выпукло в задачке.
[00:35.920 --> 00:40.280]  Ну и соответственно, сколько нужно там итерации у него
[00:40.280 --> 00:42.920]  сделать, вызвать оракулу, чтобы мы, соответственно,
[00:42.920 --> 00:45.000]  нашли там точность решения Эпсилон.
[00:45.000 --> 00:55.480]  Кто помнит, кто посмотрел это, так, да, это правда,
[00:55.960 --> 00:58.360]  соответственно, вот такая примерно оракульная сложность
[00:58.360 --> 01:00.160]  у нас у градиентного спуска.
[01:00.160 --> 01:03.760]  То есть, столько раз нам нужно посчитать градиент,
[01:03.760 --> 01:08.480]  чтобы найти решение с точки зрения чего, почему мы
[01:08.480 --> 01:13.680]  там искали решение для сильно выпуклой задачи, по аргументу,
[01:13.680 --> 01:16.880]  по функции, по норме градиента, по аргументу.
[01:16.880 --> 01:20.320]  То есть, мы находили прямо точное решение в связи
[01:20.320 --> 01:24.120]  с тем, что для сильно выпуклой задачи у нас решение единственное
[01:24.120 --> 01:25.120]  и существует.
[01:25.120 --> 01:26.120]  Вот.
[01:26.120 --> 01:27.120]  Окей.
[01:27.120 --> 01:28.120]  Это хорошо.
[01:28.120 --> 01:29.120]  Так.
[01:29.120 --> 01:33.640]  Тогда, соответственно, у нас возник такой вопрос,
[01:33.640 --> 01:36.520]  а вообще, вот это то, что мы получили, улучшаем
[01:36.520 --> 01:37.520]  или нет?
[01:37.520 --> 01:39.720]  То есть, можно ли придумать какой-то метод, который
[01:39.720 --> 01:42.320]  будет работать быстрее, чем вот эта оценка?
[01:42.320 --> 01:43.320]  Вот.
[01:43.320 --> 01:45.280]  Оказывается, да, можно придумать.
[01:45.280 --> 01:46.280]  Вот.
[01:46.280 --> 01:48.720]  И идеи об этом, на самом деле, появились довольно
[01:48.720 --> 01:49.720]  давно.
[01:49.720 --> 01:50.720]  Вот.
[01:50.720 --> 01:55.040]  И, соответственно, здесь, 1964 год, наш соотечественник
[01:55.040 --> 01:58.320]  недавно скончался, Борис Теодорович Поляк, предложил
[01:58.320 --> 01:59.880]  следующую схему.
[01:59.880 --> 02:03.440]  То есть, тот же самый градиентный спуск, который мы изучали
[02:03.440 --> 02:08.880]  в прошлый раз, плюс, соответственно, к этому методу добавляется
[02:08.880 --> 02:10.680]  так называемое моментумно-ислагаемое.
[02:10.680 --> 02:11.680]  Вот.
[02:11.680 --> 02:16.560]  Физика у этого безобразия более чем понятна.
[02:16.560 --> 02:19.280]  Вы говорите, что теперь у вас у метода в некотором
[02:19.280 --> 02:24.000]  смысле есть не просто какие-то точки, где он находится,
[02:24.000 --> 02:26.640]  а есть еще и в некотором смысле масса.
[02:26.640 --> 02:28.840]  То есть, вы находитесь в точке, и метод как бы двигается
[02:28.840 --> 02:34.040]  от точки к точке, и вот у шарика, пусть бы, раз, потому
[02:34.040 --> 02:37.400]  что метод называется метод тяжелого шарика, у шарика,
[02:37.400 --> 02:39.600]  который как бы иллюстрирует ваше движение метода, есть
[02:39.600 --> 02:40.600]  инерция.
[02:40.600 --> 02:41.600]  Вот.
[02:41.600 --> 02:45.640]  Есть некоторая инерция, и когда вы его хотите, например,
[02:45.640 --> 02:49.000]  толкнуть по градиенту в текущей точке, вот, эта
[02:49.000 --> 02:50.600]  инерция будет играть роль.
[02:50.600 --> 02:53.560]  Ну и, соответственно, эта инерционно-ислагаемая
[02:53.560 --> 02:55.920]  в некотором смысле будет продлевать вашу траекторию.
[02:55.920 --> 02:56.920]  Вот.
[02:56.920 --> 03:00.040]  И вы будете двигаться, а, частично по градиенту, по
[03:00.040 --> 03:02.840]  антиградиенту, как вы и сказали, вот, как вы и попросили
[03:02.840 --> 03:03.840]  у метода.
[03:03.840 --> 03:06.000]  Плюс, у вас будет сохраняться инерция от предыдущих
[03:06.000 --> 03:07.000]  шагов.
[03:07.000 --> 03:08.000]  Вот.
[03:08.000 --> 03:11.000]  Вся идея очень естественна, очень понятна, и на самом
[03:11.000 --> 03:13.560]  деле она довольно-таки неплохо работает.
[03:13.560 --> 03:14.560]  Вот.
[03:14.880 --> 03:17.280]  На первой картинке, соответственно, у вас сходимость обычного
[03:17.280 --> 03:23.320]  градиентного спуска, находите градиент в текущей точке
[03:23.320 --> 03:24.320]  и шагайте.
[03:24.320 --> 03:25.320]  Вот.
[03:25.320 --> 03:27.320]  А на второй картинке у вас, соответственно, изображен
[03:27.320 --> 03:29.000]  метод тяжелого шарика.
[03:29.000 --> 03:30.000]  То есть, что здесь происходит?
[03:30.000 --> 03:33.040]  Смотрите, вот это у нас градиент, как и на предыдущей картинке,
[03:33.040 --> 03:34.040]  вот это градиент.
[03:34.040 --> 03:35.040]  Вот.
[03:35.040 --> 03:39.320]  А вот эта инерция, которая у нас осталась, вот эта инерция.
[03:39.320 --> 03:40.320]  Вот.
[03:40.320 --> 03:43.600]  И видно, что если мы как раз будем учитывать и инерцию,
[03:43.600 --> 03:47.040]  и, соответственно, градиент, у нас метод будет двигаться
[03:47.040 --> 03:49.520]  лучше и быстрее сходиться к решению.
[03:49.520 --> 03:50.520]  Вот.
[03:50.520 --> 03:51.520]  И аналогично у всех остальных точек.
[03:51.520 --> 03:55.360]  Вот такая простая, в принципе, идея лежала в основе метода
[03:55.360 --> 03:56.360]  тяжелого шарика.
[03:56.360 --> 03:57.360]  Вот.
[03:57.360 --> 03:58.360]  А, окей.
[03:58.360 --> 03:59.360]  Почему?
[03:59.360 --> 04:00.360]  Так.
[04:00.360 --> 04:20.680]  Мы же знаем, что наши шаги при этом довольно маленькие
[04:20.680 --> 04:23.480]  градиентного спуска, потому что мы шаг сделать довольно
[04:23.480 --> 04:24.480]  большим не можем.
[04:24.480 --> 04:25.480]  Потому что мы просто вылетим.
[04:26.480 --> 04:31.280]  Ну, в некотором смысле, да.
[04:31.280 --> 04:32.280]  Вот.
[04:32.280 --> 04:34.920]  Но, как вы видите, у вас, как бы, траектория становится...
[04:34.920 --> 04:37.800]  Смотрите, нет, смотрите, тут какая идея.
[04:37.800 --> 04:40.120]  У вас прошлый градиент, если вы взяли довольно большой
[04:40.120 --> 04:41.880]  шаг, он супер устарел.
[04:41.880 --> 04:43.600]  То есть вы вот по этому направлению, по которому
[04:43.600 --> 04:47.000]  вы шли, вот по этому направлению, вы уже минимизировались.
[04:47.000 --> 04:48.680]  Ну, или очень хорошо минимизировались.
[04:48.680 --> 04:49.680]  Вот.
[04:49.680 --> 04:51.640]  То есть вы нашли вот эту точку, и вот это направление,
[04:51.640 --> 04:53.440]  оно в некотором смысле уже плохое.
[04:53.440 --> 04:54.440]  Вот.
[04:54.720 --> 04:58.800]  А вот новый градиент, скорее всего, указывает в какое-то
[04:58.800 --> 05:00.800]  довольно противоположное направление.
[05:00.800 --> 05:01.800]  Вот.
[05:01.800 --> 05:04.440]  И видите, что на диагонали может находиться что-то очень
[05:04.440 --> 05:05.440]  хорошее при этом.
[05:05.440 --> 05:06.440]  Вот.
[05:06.440 --> 05:10.080]  Да, да, конечно, конечно.
[05:10.080 --> 05:11.080]  Интуиция вот такая.
[05:11.080 --> 05:14.400]  То есть здесь вы минимизировались хорошо, новый градиент
[05:14.400 --> 05:16.800]  у вас указывает, скорее всего, в другое направление,
[05:16.800 --> 05:19.200]  но при этом что-то среднее между вот этим.
[05:19.200 --> 05:21.440]  Оно еще указывает в более классное направление.
[05:21.440 --> 05:23.320]  В данном случае, прямо на картинке видно, что это указывает
[05:23.320 --> 05:24.840]  более сильно в оптимум.
[05:24.840 --> 05:25.840]  Вот.
[05:25.840 --> 05:30.440]  На презентном презентажке, соответственно, есть ссылочка.
[05:30.440 --> 05:34.680]  Можете ее тыкнуть и немного поиграться с методом тяжелого
[05:34.680 --> 05:35.680]  шарика.
[05:35.680 --> 05:36.680]  Вот.
[05:36.680 --> 05:38.320]  Там, соответственно, можно менять шаг, можно менять
[05:38.320 --> 05:39.320]  моментум.
[05:39.320 --> 05:40.320]  Вот.
[05:40.320 --> 05:42.480]  И посмотреть, как он сходится для разных задач.
[05:42.480 --> 05:43.480]  Вот.
[05:43.480 --> 05:44.480]  Да.
[05:44.480 --> 05:47.480]  А что там такое?
[05:47.480 --> 05:48.480]  Так.
[05:48.480 --> 05:49.480]  Сейчас спустить надо.
[05:49.480 --> 05:50.480]  Сейчас.
[05:51.480 --> 05:52.480]  Так.
[05:52.480 --> 05:53.480]  Смотрите.
[05:53.480 --> 05:57.480]  То есть, в принципе, идея кажется естественной.
[05:57.480 --> 05:58.480]  Вот.
[05:58.480 --> 05:59.480]  Это может сработать.
[05:59.480 --> 06:00.480]  Эта реальность и сработает.
[06:00.480 --> 06:03.480]  Мы докажем, что этот метод реально работает классно.
[06:03.480 --> 06:04.480]  Вот.
[06:04.480 --> 06:05.480]  Исходится лучше.
[06:05.480 --> 06:07.480]  Точнее, не этот метод, а его уже модификация.
[06:07.480 --> 06:08.480]  Вот.
[06:08.480 --> 06:09.480]  Хорошо.
[06:09.480 --> 06:10.480]  Вот.
[06:10.480 --> 06:11.480]  Так.
[06:11.480 --> 06:14.480]  Ну, давайте тогда чуть-чуть порассуждаем.
[06:14.480 --> 06:16.480]  Какие вообще видите плюсы и минусы в этот момент?
[06:16.480 --> 06:18.480]  Проградиентный спуск мы в принципе не обсудили.
[06:18.480 --> 06:19.480]  Вот.
[06:19.480 --> 06:20.480]  Давайте про это хотя бы обсудим.
[06:20.480 --> 06:21.480]  Вот.
[06:21.480 --> 06:24.480]  Какие плюсы, какие минусы?
[06:24.480 --> 06:25.480]  Вот.
[06:25.480 --> 06:27.480]  Поверники, вы видите?
[06:27.480 --> 06:28.480]  Вот.
[06:28.480 --> 06:29.480]  Поверники.
[06:29.480 --> 06:32.480]  Что вы видите?
[06:32.480 --> 06:33.480]  Ну, хорошо.
[06:33.480 --> 06:34.480]  Отлично.
[06:34.480 --> 06:35.480]  Проверники.
[06:35.480 --> 06:36.480]  Видите?
[06:36.480 --> 06:37.480]  Вы видите?
[06:37.480 --> 06:38.480]  В jeito, как мы видим?
[06:38.480 --> 06:39.480]  В那 attainment.
[06:39.480 --> 06:45.400]  плюсы, какие минусы у такого метода, который вы только что увидели. В том числе, может быть,
[06:45.400 --> 06:48.880]  минусы какой-то градиентного спуска, на котором он по факту базируется.
[06:48.880 --> 07:01.640]  Да, смотрите, то есть тут тоже хорошая идея. Если у вас градиент действительно сильно меняется,
[07:01.640 --> 07:06.200]  или вы хорошо довольно выбираете шаг, так что у вас градиент, как вот на этой картинке,
[07:06.200 --> 07:20.240]  сильно поворачивается. У вас, соответственно, видите, здесь могут быть довольно сильные такие
[07:20.240 --> 07:25.000]  зигзаги. При этом, если мы посмотрим на траекторию, которая создает тяжелый шарик,
[07:25.000 --> 07:30.400]  эта траектория становится более плавной. По той же ссылке можете пройти и поиграться. Например,
[07:30.400 --> 07:37.040]  задать какой-то довольно большой шаг у градиентного спуска и поварировать моментум. Моментум маленький,
[07:37.040 --> 07:47.160]  асцеляции большие. Моментум становится больше, асцеляции становится меньше. Да, это в некотором
[07:47.160 --> 07:55.840]  смысле тоже игра. Моментум не самый очевидный параметр по подбору. Кажется, что нужно брать
[07:55.840 --> 08:01.440]  довольно большим, но иногда слишком большой становится плохо. И вот это как раз минус метода,
[08:01.440 --> 08:06.480]  то есть в градиентном спуске нам нужно было подбирать шаг. Это мы с вами не обсудили,
[08:06.480 --> 08:12.040]  это в некотором смысле вопрос семинарских занятий, как подбирать шаг в методе градиентного спуска.
[08:12.040 --> 08:18.520]  Ну и в том числе домашнего задания, где вам тоже немного с этим надо поиграться. А в тяжелом
[08:18.520 --> 08:24.240]  шарике есть теперь не только шаг, теперь есть еще моментум, который нужно подбирать. И,
[08:24.240 --> 08:30.800]  как мы понимаем, просто взять его нулевым не интересно, а достаточно большим уже плохо.
[08:30.800 --> 08:37.600]  И это тоже в некотором смысле игра. Окей, что еще? Что еще можете выделить хорошего, плохого?
[08:37.600 --> 08:51.400]  Да, то есть в градиентном спуске мы хранили точку, считали по ней градиент и обновляли х. А
[08:51.400 --> 08:56.800]  здесь нужно хранить две точки, чтобы у нас был моментум. Вот, ну здесь есть какие еще минусы,
[08:56.800 --> 09:02.440]  плюсы выделяю соответственно. Какая-то у него есть физика и интуиция, его довольно легко
[09:02.440 --> 09:07.360]  имплементировать, то есть это не сложнее, чем градиентный спуск и довольно дешево вычислять,
[09:07.360 --> 09:12.200]  если мы считаем, что градиент действительно вычисляется спокойно для данной задачи. Два
[09:12.200 --> 09:18.240]  параметра, то что обсудили, ну и такой что ли ключевой вопрос. Это конечно здорово, мы придумали
[09:18.240 --> 09:22.240]  новый метод, который может работать лучше. Вы в домашних зданиях в том числе посмотрите,
[09:22.240 --> 09:29.680]  что он работает часто действительно хорошо. Вот, ну мы же как-то нацеливались на то, чтобы улучшить
[09:29.680 --> 09:36.040]  оценки для градиентного спуска. Вот, а даст ли метод тяжелого шарика эти улучшения? Да.
[09:36.040 --> 09:47.960]  Ну смотрите, давайте вот просто посмотрим, что там происходит. Вот, то есть здесь мы по факту
[09:47.960 --> 09:59.440]  берем градиент в точке k-1. Ну это было гамма минус 1. Да, что-то такое? Правда или нет? Или там
[09:59.440 --> 10:08.320]  что-то со знаком напутал еще? Тут минус еще, да? Минус. Вот, ну и смотрите, то есть кажется,
[10:08.320 --> 10:15.000]  что во-первых, если бы вы сделали шаг больше, то вот эта точка у вас бы х поменялась. То есть это
[10:15.000 --> 10:20.880]  была бы другая точка. Здесь мы как бы считаем ту точку, в которой мы как бы использовали просто
[10:20.880 --> 10:28.760]  предыдущий шаг. Вот. Плюс соответственно, плюс соответственно, здесь у вас есть этот момент на
[10:28.760 --> 10:34.560]  слагамме, который вам в том числе не дает сделать этот предыдущий шаг довольно большим. Вот. Тоже
[10:34.560 --> 10:39.600]  некоторые отличия. Но главное отличие, конечно, вот в этой точке. И на самом деле точка, в которой
[10:39.600 --> 10:45.120]  считается градиент, вот что в градиентном спуске, что в методе тяжелого шарика, что следующий метод,
[10:45.120 --> 10:51.480]  который мы увидим, она является ключевой. Потому что вот ответ на тот вопрос, который я задал,
[10:51.480 --> 10:58.160]  а вообще метод тяжелого шарика улучшает оценки в градиентном спуске или нет, ответ нет. Вот. То есть
[10:58.160 --> 11:05.240]  в общем случае метод тяжелого шарика может быть не лучше, чем градиентный спуск. Вот. Ну и Борис
[11:05.240 --> 11:10.640]  Теодорович придумал это в 1964 году и, соответственно, долго думал над этой задачей. А как тогда вообще
[11:10.640 --> 11:15.640]  получить метод, который работает лучше? И можно ли вообще доказать что-то для метода тяжелого шарика,
[11:15.640 --> 11:19.800]  что он в каких-то случаях работает лучше? Ну и до конца жизни на самом деле он в некотором смысле
[11:19.800 --> 11:25.520]  интересовался этим вопросом. Даже после того, как показали, что в общем случае метод тяжелого шарика
[11:25.520 --> 11:32.560]  не улучшает работу градиентного спуска и не является оптимальным. Вот. Все равно в каких-то
[11:32.560 --> 11:39.040]  частных случаях Борис Теодорович это было интересно. Вот. Но был у Бориса Теодоровича ученик Юрий
[11:39.040 --> 11:44.440]  Евгеньевич Нестеров, который, соответственно, через 20 лет изобрел следующий метод, так называемый
[11:44.440 --> 11:49.360]  ускоренный градиентный спуск, часто называют методом Нестерева. Вот. Выглядит он следующим образом.
[11:49.360 --> 11:58.320]  Кратенько, то есть это мой полный листинг алгоритма, кратенько его итерацию я вот изложил здесь. Вот.
[11:58.320 --> 12:03.120]  Нестеров, соответственно, вот тяжелый шарик. Давайте посмотрим на отличия. Чем они между собой
[12:03.120 --> 12:14.600]  отличаются? Вообще есть разница или нет между ними? Может тут просто Нестерева две точки берется и все.
[12:14.600 --> 12:30.280]  Ну это новый шаг, который вот мы делаем, соответственно. Вот этот шаг. Как мне обновить
[12:30.280 --> 12:39.960]  x и как мне обновить y? Вот. Кто понимает, в чем разница? Ну вот. Она вообще есть или нет между этими двумя методами?
[12:39.960 --> 12:54.200]  Ну, индекс tau там тоже вроде как был. Супер. Смотрите. Я же как могу сделать? Я могу записать значение
[12:54.200 --> 13:03.400]  для yk. То есть как у меня yk на предыдущей итерации обновлялся? Это xk плюс, соответственно, tau kt xk-xk-1.
[13:03.400 --> 13:19.560]  Так? Вот. И теперь подставить вот сюда. Tau k-1. Что здесь тогда получится? Tau k-1 поставим. Вот. И здесь тоже.
[13:19.560 --> 13:27.440]  Видно, что идея по факту очень похожа на тяжелый шарик. Вот это чисто как раз xk и плюс моментум,
[13:27.440 --> 13:34.480]  который у нас уже был. Но вот здесь вот как раз точка в градиенте стала ключевой. Вот. Нестеров
[13:34.480 --> 13:42.160]  добавляет моментум еще и в точку. Вот. То есть он просит пройти чуть дальше точки этой. Вот.
[13:42.160 --> 13:49.200]  Насколько сильно? Вопрос. То есть в tau kt нужно еще подбирать. Вот. Tau kt-1. Вот. То есть да,
[13:49.200 --> 13:55.200]  вот как раз в тяжелом шарике мы просто брали точку, по ней брали градиент и просто там моментом
[13:55.200 --> 13:59.560]  использовали. А Нестеров эту точку еще проталкивает чуть дальше. То есть это в некотором смысле какой-то
[13:59.560 --> 14:06.800]  взгляд в будущее. Как будто в том числе вы сделали чуть больший шаг методом градиентного спуска в прошлый
[14:06.800 --> 14:13.280]  раз. Ну и точнее, в предыдущей итерации Нестерева. Вот. И нашли вот эту точечку. Вот. И эта идея,
[14:13.280 --> 14:18.800]  что ли, стала реально очень крутой в плане оценок сходимости. И у, соответственно, Юрия Евгеньевича
[14:18.800 --> 14:29.880]  получилось доказать то, что метод Нестера работает в общем случае лучше. Вот. Да. Да, давайте.
[14:37.800 --> 14:49.200]  В плане xk-1 меньше, чем xk-1. Что значит меньше? Ну она может быть отрицательной. Ну это же
[14:49.200 --> 14:54.080]  точки просто. Это какие-то точки, где у вас минимум вы не знаете. Что значит отрицательная?
[14:54.080 --> 15:02.280]  Просто не понимаю тут. С какой точки зрения отрицательная? Вот. На самом деле вопрос довольно
[15:02.280 --> 15:07.840]  интересный. То есть на самом деле вы увидите это в экспериментах, что метод тяжелого шарика,
[15:07.840 --> 15:12.840]  что метод Нестерева, они не монотонны. Я не знаю, кто-то уже наверное делал домашнее задание и вы
[15:12.840 --> 15:18.900]  видели, что градиентный спуск у вас, ну, линейно, как мы доказывали, сходится к решению. Вот. А метод
[15:18.900 --> 15:23.640]  Нестерева, соответственно, и метод тяжелого шарика, ведет, например, себя по иксу не монотонно. То
[15:23.640 --> 15:28.360]  есть он может увеличивать расстояние, а может уменьшать. Вот. Но в среднем как бы тренд ведет
[15:28.360 --> 15:34.440]  сильно лучше, чем для метода градиентного спуска. Вот это как раз тот эффект, который
[15:34.440 --> 15:40.480]  может наблюдаться, который, в принципе, правильно подметили. Вот. Что по факту вы же все равно в некотором
[15:40.480 --> 15:46.400]  смысле делаете градиентным спуском, тоже об этом как раз тоже девушка говорила, чтобы шаг чуть-чуть
[15:46.400 --> 15:53.320]  дальше. Это может быть нехорошо. Вот. Локально. Но глобально, оказывается, это хороший тренд. Вот.
[15:53.320 --> 15:59.200]  Сделать чуть больше и как бы взглянуть чуть-чуть в будущее. Вот. Ну и сейчас мы это как раз подоказываем.
[15:59.200 --> 16:09.080]  Метод Нестерева довольно, ну, по мне так, не то чтобы суперсложно, но по прошлым итерациям,
[16:09.080 --> 16:15.120]  которые когда я его показывал, доказывается немного мутурным. Вот. И долго. Вот. Я поэтому решил взять
[16:15.120 --> 16:20.960]  другой метод тоже, который достигает нужных оценок и показать его. Ну понятно, что в силу того,
[16:20.960 --> 16:26.440]  что метод Нестерева стал суперпопулярен. Придумали кучу разных методов, которые также достигают
[16:26.440 --> 16:32.440]  его оценок. Вот. И делают в некотором смысле такое вот ускорение. Вот. Ну и на самом деле довольно
[16:32.440 --> 16:36.700]  забавная история связана, может не забавная такая, типичная для науки история связана с методом
[16:36.700 --> 16:44.000]  Нестерева. То, что, в принципе, вот он его придумал в 83-м году и где-то до начала десятых про него
[16:44.000 --> 16:50.440]  особо не вспоминали. Вот. Была эта классная теория, был этот метод, было доказано, что он классный. Вот.
[16:50.440 --> 16:56.200]  А потом случился бум машинного обучения. Ну и так оказалось, что эти все сеточки классно обучаются
[16:56.200 --> 17:03.560]  всякими методами с моментумом. Вот. HDD с моментумом, Adam это тоже метод с моментумом. Вот. И оказывается,
[17:03.560 --> 17:07.480]  все эти моментумы работают классно. Тут все вспомнили, что оказывается, эти моментумы были
[17:07.480 --> 17:14.000]  придуманы там 50 лет назад Поликом. Вот. Усовершенствованы там 30 лет, через 30 лет Нестеровым. Вот.
[17:14.000 --> 17:20.640]  Здорово. Вот. Так в некотором смысле вот это все воскресилось и реинкарнировало. Вот. И сейчас эти
[17:20.640 --> 17:26.000]  работы супер популярны. Вот. Именно вот эти все моментумы и ускорение. Просто потому, что это
[17:26.000 --> 17:33.440]  выстрелило на практике. Для тех задач, которые еще не существовали в том виде, в котором они
[17:33.440 --> 17:38.520]  существуют сейчас. И в тот момент, когда создавались методы, этих задач просто не было. Вот. А я, соответственно,
[17:38.520 --> 17:44.560]  рассматриваю вот такой вот метод, который называется линейный каплинг. Вот. И суть этого метода такая,
[17:44.560 --> 17:49.960]  то есть вы строите две последовательности. Одна у вас как бы в некотором смысле это последовательность
[17:49.960 --> 17:58.920]  обычного градиентного спуска. Вот. Ну вот. Авторы это называют как бы ordinary step, ordinary step. Вот. Ну
[17:58.920 --> 18:04.200]  мы увидим, что тут на самом деле шаг довольно будет стандарт для градиентного спуска. А это называется
[18:04.200 --> 18:11.800]  в некотором смысле агрессивный step. Агрессивный step. Вот. И вы связываете вот эти две последовательности
[18:11.800 --> 18:19.320]  между собой. Y и Z через точку X. И как раз эту точку X подаете в градиент. Вот. Интуиция метода
[18:19.320 --> 18:24.960]  Нестерова более, что ли, понятна. Там эти моментумы здесь. Но она не совсем понятна. Но метод тоже даст
[18:24.960 --> 18:30.440]  нужный результат. И чем он хорош, ну вот конкретно почему я его выбрал для лекции, у него довольно
[18:30.440 --> 18:40.280]  простое доказательство. Которое мы сейчас попробуем и воспроизвести. Выход, да, да, да. Выход, то есть в качестве
[18:40.280 --> 18:47.600]  выхода вы возвращаете среднюю точку. Среднюю. Сейчас поймем почему. То есть пока не последнюю. То есть раньше
[18:47.600 --> 18:53.320]  обычно была последняя и мы доказывали, что по последней точке все хорошо. Вот. А сейчас поймем почему среднюю.
[18:53.320 --> 19:08.640]  Ну вот видите, тут еще как бы в некотором смысле проблема, что по средней точке на самом деле вот, я не знаю,
[19:08.640 --> 19:12.960]  смотрели конспект или нет, там можно доказать в принципе сходимость градиентного вспуска по
[19:12.960 --> 19:20.120]  средней точке. Она будет симпатически такая же, как и для последней точки. Вот. Более того, в случае,
[19:20.120 --> 19:26.600]  видите, ускоренных методов, как я говорю, там сходимость, она не монотонная. Вот. И часто средняя
[19:26.600 --> 19:31.160]  точка, она даже лучше, чем какая-то последняя, которая могла быть у вас на пике. То есть там сходимость
[19:31.160 --> 19:37.280]  вот такая иногда. Вот. И взять вот эту точку, ну часто не так хорошо, как взять какую-то среднюю,
[19:37.280 --> 19:45.200]  которая находится, ну вот, где значение функции может быть ниже даже. Вот. Но главное тут понимать,
[19:45.200 --> 19:49.580]  что асимпатически средняя точка не хуже, чем, ну именно с точки зрения О большого. Она даже
[19:49.580 --> 19:57.680]  для градиентного вспуска не хуже, чем последняя. Можно часто имирать минимум, это правда. Вот. Но нам
[19:57.680 --> 20:02.120]  понадобится средняя точка, для нее как раз с доказательством просто все хорошо схлопнется. Да.
[20:02.120 --> 20:16.560]  Да, на самом деле вот современным моментом они основаны на этой идее, что вы берете инерцию,
[20:16.560 --> 20:22.160]  домножаете ее на коэффициент старую инерцию. У вас как бы старая инерция убывает по геометрической
[20:22.160 --> 20:27.840]  прогрессии. Вот. Сейчас рассматриваем такие методы просто потому, что а. это были первые методы,
[20:27.840 --> 20:34.960]  б. хочется для них что-то доказать. Вот. Для ваших методов тоже можно доказать, но чуть посложнее. Вот.
[20:34.960 --> 20:40.520]  К сути опять же вот этого метода, почему я его выбрал, для него получится что-то доказать. Вот.
[20:40.520 --> 20:44.680]  Для Нестера тоже конечно все получается, просто доказательство будет чуть длиннее. Вот.
[20:44.680 --> 21:11.880]  Да. Так. Так. Среди лит на к, это вы в каком случае или что?
[21:14.680 --> 21:25.920]  Ну смотрите, давайте вот просто порассуждаем, что например, видели оценку в выпуклом случае,
[21:25.920 --> 21:32.560]  какая будет для метода Нестера? Что там будет соответственно у него? Вот. Ну не для Нестера,
[21:32.560 --> 21:38.800]  для градиентного спуска. Вот. Один делить на к, да? Это соответственно номер итерации. Вот.
[21:38.800 --> 21:43.240]  Если вы, то есть для каждой точки вы там можете гарантировать, что у вас соответственно она
[21:43.240 --> 21:50.480]  на один делить на к ближе к решению. Так. Если вы соответственно это все усредните по к, от одного до
[21:50.480 --> 21:58.240]  к большого, один делить на к. Вот. Сколько при этом получится? Ну сколько там примерно получится?
[21:58.240 --> 22:08.760]  Логарифм на к, да? Ну не сильно страшно. Вот. То есть лишний логарифм, а на самом деле его даже можно
[22:08.760 --> 22:14.560]  не убрать. То есть в оценке, которую мы получали для последней точки, можно получить и для средней
[22:14.560 --> 22:19.440]  точки ровно такую же оценку, какая у вас приведена в теореме. У вас помнится вот так вот было, и кто
[22:19.440 --> 22:27.320]  опять же читал конспекты, там соответственно вот что-то в таком духе у вас вылезало. Да? Вот.
[22:27.320 --> 22:33.200]  Когда мы доказывали для сходимости в выпуклом случае. Ну и мы говорили, что у нас соответственно методы
[22:33.200 --> 22:38.160]  убывают, соответственно у нас f от x ката постоянно уменьшается. Но можно уже воспользоваться
[22:38.160 --> 22:43.680]  Янсоном, кто знает неравенство Янсона, когда у нас функция выпукла и есть среднее значение точек.
[22:43.680 --> 22:52.800]  Да, поэтому вот это меньше чем f от средней точки. И поэтому сходимость по средней
[22:52.800 --> 22:58.560]  точке она примерно такая же симпатическая. Вот. То есть в принципе проблем в этом особо нету. Вот.
[22:58.560 --> 23:05.360]  Вот даже такая грубая оценка показывает, что логарифм вылезет и не страшно. Вот. А более точно
[23:05.360 --> 23:15.560]  можно вообще показать, что и логарифм не вылезет. Вот. Ну смотрите, во-первых, у вас просто есть
[23:15.560 --> 23:21.160]  градиентный спуск. Вот. А есть какой-то метод, который тут по факту тот же похожий на градиентный
[23:21.160 --> 23:29.840]  спуск. Средняя? Еще раз, средняя точка она не плохая, не хорошая. То есть мы только что показали,
[23:29.840 --> 23:36.840]  что для градиентного спуска по ней тоже есть сходимость. Вот. И сейчас мы тоже увидим, что средняя
[23:36.840 --> 23:42.880]  точка будет более чем норм. Вот. Более того, на самом деле на практике часто какие-то средние
[23:42.880 --> 23:47.360]  взвешивания и экспоненциальные взвешивания дают даже лучший результат, чем последняя точка.
[23:47.360 --> 23:53.360]  Особенно в стахистических методах, когда у вас непонятно, что происходит. Вот. Средняя,
[23:53.360 --> 23:58.200]  последняя точка может быть какая-то просто плохая, вас просто выбило куда-то не туда. Вот. А средняя,
[23:58.200 --> 24:05.440]  она более устойчивая. Вот. И часто брать среднюю довольно хорошо оказывается. Окей. Смотрите,
[24:05.440 --> 24:13.080]  давайте тогда вот этот слайд мы фиксанем. Метод и предположение, которое нам по факту понадобится.
[24:13.080 --> 24:19.640]  L-гладкость и mu-сильная выпуклость. Вот. Вы их можете у себя открыть. Ну и дальше давайте
[24:19.640 --> 24:27.080]  поехали доказывать сходимость. Вот. Что там происходит, соответственно? Ну давайте просто
[24:27.080 --> 24:34.520]  вспоминать, как мы это делали для градиентного спуска. Что мы там делали с вами? Кто помнит,
[24:34.520 --> 24:45.800]  что там для доказательства градиентного спуска нужно было делать? Ну как мы вообще начинали? Что
[24:45.800 --> 24:54.680]  мы хотели оценить, во-первых? Расстояние до чего? До решения, да? Ка плюс первой точки до оптимума.
[24:54.680 --> 25:01.440]  Здесь то же самое. Давайте попробуем это сделать. Я возьму точку z. Вот. Как для тори,
[25:01.440 --> 25:06.600]  для которой я хочу померить расстояние. Посмотрю, как оно меняется. То есть в некотором смысле часто
[25:06.600 --> 25:11.480]  доказательства, да и вообще придумывание метод, это творчество. Вот. Ну и здесь давайте возьмем точку z
[25:11.480 --> 25:17.240]  и посмотрим, как оно меняется. Подставляем итерацию. Давайте вы мне поможете. Что там с итерацией?
[25:17.240 --> 25:29.280]  Что там? Как z обновляется? f от x-ката. Давайте я сразу гаммы сделаю постоянными. Нам потому,
[25:29.280 --> 25:33.680]  что они каты не нужны. Там можно их не варьировать со временем. То есть у нас получится результат и
[25:33.680 --> 25:39.560]  так. Вот. Чтобы эти индексы постоянно не писать. Вот. Я их сразу постоянными сделаю. Вот. Окей. Дальше
[25:39.560 --> 25:45.880]  мы что раскрывали? Чтобы у нас вылезла z-ката минус x звездой. Вот. Дальше вылезало соответственно
[25:45.880 --> 25:58.120]  скалярное произведение. Помнится, да? Вот. И соответственно z-ката минус x звездой. Вот. Плюс гамма в
[25:58.120 --> 26:09.840]  квадрате fx-ката. Согласны? Супер. Вот. Смотрите, тут возникают какие-то вопросы. Помнится в
[26:09.840 --> 26:14.480]  градиентном спуске, когда мы доказывали, у нас вот тут точки совпадали. Дальше мы пользовались
[26:14.480 --> 26:19.160]  выпуклостью и все вроде как было хорошо. Сейчас мы это все поменяли из-за того, что мы хотим там
[26:19.160 --> 26:24.520]  какие-то моменты мы накручивать и так далее. Брать эти выпуклые комбинации. Ну тогда давайте
[26:24.520 --> 26:31.920]  здесь это тоже пофиксим, чтобы точка совпала и как бы там мы уже были ближе к выпуклости. Вот.
[26:31.920 --> 26:37.320]  Я здесь сделаю одинаковую точку. Вот. Но за это, понятно, мне нужно будет расплатиться дополнительным
[26:37.320 --> 26:47.160]  скалярным произведением. Вот. Окей. Дополнительное скалярное произведение. Будет выглядеть
[26:47.160 --> 26:52.080]  следующим образом. То есть вы два эти скалярных произведения сложите, у вас получится нужный
[26:52.080 --> 27:09.840]  результат. Так? Ну давайте посмотрим еще раз. Вот. Я презентацию в чат скинул, вы можете на
[27:09.840 --> 27:15.360]  телефонах там открыть, смотреть. Вот вам нужен по факту вот этот слайд. Алгоритм и два предположения
[27:15.360 --> 27:23.400]  по факту, которым мы будем пользоваться. Вот. Окей. Выглядит так, что вот это в принципе нормально.
[27:23.400 --> 27:28.600]  Это нормально. То есть тут как раз связь какая-то. Тут точка другая, конечно, скалярное произведение,
[27:28.600 --> 27:35.800]  но скорее всего будет все хорошо с этим безобразием. Вот. Вот с этим проблемой нужно как-то оценивать.
[27:35.800 --> 27:40.240]  Ровно такие же проблемы, которые у нас по факту возникали и в градиентном спуске. Как-то оценить
[27:40.240 --> 27:46.960]  норму градиента, как-то оценить скалярное произведение. Вот. Что мы для этого применяли? Давайте
[27:46.960 --> 27:53.440]  вспомним. Как там норму градиента нам, например, оценить? Что там мы делали, чтобы оценить норму
[27:53.440 --> 28:02.960]  градиента? Эль-гладкость. Вот. Соответственно для нормы градиента часто как раз применяется
[28:02.960 --> 28:08.200]  эль-гладкость для того, чтобы ее оценить как-то. Мы с вами в градиентном спуске как делали? Мы
[28:08.200 --> 28:14.320]  оценивали с помощью этого. Добавляли ноль, умный, и соответственно оценивали. Вот. Здесь я чуть
[28:14.320 --> 28:20.160]  сделаю по-другому, похитрее. Воспользуюсь только вот тем свойством, которое я выписал до этого.
[28:20.160 --> 28:29.400]  И для точки Y, потому что в принципе для точки Z мы уже много чего вытащили. Вот. А здесь я хочу как
[28:29.400 --> 28:37.960]  раз эль-гладкость, плюс использовать то, что в YK там тоже используется апдейт с помощью градиента. Вот.
[28:37.960 --> 28:47.160]  Я хочу записать свойства гладкости в двух точках. YK плюс 1 и соответственно, какой второй? YK.
[28:47.160 --> 29:01.120]  Второй. Вот. Давайте попробуем. Или, точнее, лучше. Нет, XK. То есть, чтобы вот у меня вылезла разность
[29:01.120 --> 29:06.880]  вот этих безобразий. Вот. Давайте попробуем просто записать гладкость в точке YK плюс 1 и XK. Как
[29:06.880 --> 29:14.120]  вот это будет выглядеть в этом виде? Вот это не равенство, если я буду подставлять вместо Y,
[29:14.120 --> 29:24.000]  YK, а вместо X и XK. Как будет выглядеть?
[29:24.000 --> 29:36.600]  Сюда? От первой строчки ко второй. Я просто раскрыл квадрат.
[29:36.600 --> 29:48.920]  Ну, A плюс B в квадрате равно A в квадрате. Ну, плюс 2AB плюс B в квадрате. Все. Так,
[29:48.920 --> 30:01.640]  как будет гладкость выглядеть? Так. Дальше, соответственно, какая точка тут будет? XK или какая?
[30:01.640 --> 30:20.600]  YK плюс 1 минус XK. Вот. Чем вот такое неравенство хорошо? Вроде бы я что-то просто записал гладкость
[30:20.600 --> 30:25.400]  в двух точках. А хорошо на тем, что когда мы вот сюда начнем подставлять градиент, ну, точнее,
[30:25.400 --> 30:33.400]  как мы обновляем YK, здесь вылезет градиент с нужным коэффициентом и с минусом. И здесь тоже. Они
[30:33.400 --> 30:45.240]  повылазят. Так. Тогда здесь все получится. YK плюс 1, XK. Ну, эту технику вы, в принципе,
[30:45.240 --> 30:55.240]  видели и в доказательстве выпуклого случая, кто смотрел конспект. Вот. Так. И здесь, соответственно,
[30:55.240 --> 31:15.000]  от XK в квадрате. Окей? Да. Здорово. Просто в гладкости
[31:15.000 --> 31:24.680]  то же самое. Вот оно. Ну, смотрите, главное помнить физический смысл. Сильная выпуклость вам снизу
[31:24.680 --> 31:32.560]  функцию ограничивает параболой. Вот. Ваша функция может как-то реально вести себя как-то вот так.
[31:32.560 --> 31:39.960]  Ну, я просто рисую. Там у вас были пунктирные линии до этого. А гладкость у вас ограничивает ее
[31:39.960 --> 31:49.800]  сверху. Тоже параболой. Параболойдом. Вот. Поэтому вас видите в два направления неравенства. Внизу
[31:49.800 --> 31:58.080]  как бы параболой от сильной выпуклости, а сверху параболой от гладкости. Вот. Соответственно, да,
[31:58.080 --> 32:07.240]  расписали. Здесь это все можно сгруппировать и получить следующее выражение. Вот. И отсюда я
[32:07.240 --> 32:18.840]  хочу вытянуть норму градиента. Оценку на норму градиента. Вот. Ну, значит, если мне, а причем
[32:18.840 --> 32:25.000]  оценку сверху, потому что нам вот здесь его нужно было оценить сверху. В верхней линии. Так. Вот. Как
[32:25.000 --> 32:35.800]  это соответственно вытащить? Как это вытащить? Вот. Как это вытащить? Ну, давайте перенесем в правую
[32:36.600 --> 32:43.960]  часть. Вот. И разделим. Разделим на тот коэффициент, который у меня стоит перед нормой градиента.
[32:43.960 --> 32:51.840]  Могу ли я всегда делить на коэффициент перед нормой градиента? Ну вот, перед нормой градиента у
[32:51.840 --> 32:57.440]  меня стоит вот этот коэффициентик. Вот. А вот будет ли вот это неравенство всегда правдивым,
[32:57.440 --> 33:00.920]  если я буду делить на этот коэффициент?
[33:01.840 --> 33:07.160]  Нужно потребовать что? Чтобы этот коэффициент был положительный. Все понимают, да?
[33:07.160 --> 33:10.640]  Если у вас коэффициент будет отрицательный, у вас неравенство просто
[33:10.640 --> 33:16.560]  поменяет знак. Тогда что нам нужно потребовать? Какие у нас должны быть это,
[33:16.560 --> 33:21.920]  чтобы неравенство было хорошим? Ну вот это соответственно. Вот это, чтобы этот
[33:21.920 --> 33:29.480]  коэффициентик у меня был положительный. Сколько? От нуля до двух делить на L, так?
[33:29.480 --> 33:38.000]  Вот. Все. Вроде бы получили оценку на норму градиента, которая успешно может
[33:38.000 --> 33:45.920]  встать вот в это неравенство, которое я до этого сделал. Так, давайте это спустим
[33:45.920 --> 33:48.400]  тогда вниз.
[33:51.920 --> 34:00.280]  Так, что нам здесь? Это нам уже не нужно. Это мы уже все подоказывали. А вот это
[34:00.280 --> 34:08.800]  как раз мы поднимем. Вот. И подставим соответственно сюда вот эту норму
[34:08.800 --> 34:20.240]  градиента. Здесь у нас получится что? Соответственно разность 2 делить на
[34:20.240 --> 34:31.240]  это 2-nL. Так, и здесь будет разность функций f от xk минус f от yk плюс 1. Вот.
[34:31.240 --> 34:38.240]  Окей. Вроде бы все норм. То есть с первым кусочком разобрались. Пока лучше как так-то
[34:38.240 --> 34:43.600]  и не выглядит. Какие-то точки x непонятные, точки y, они здесь разные. Вот.
[34:43.600 --> 34:49.480]  Ну давайте попробуем дальше разбираться тогда со скалярным произведением.
[34:49.480 --> 34:54.640]  А, да. Здесь соответственно уже становится неравенство. Всего того, что мы сложили
[34:54.640 --> 34:59.600]  два неравенства, мы уже использовали неравенство. Второе это неравенство, которое мы
[34:59.600 --> 35:05.600]  здесь получили. То теперь вот так вот. Вот. Здесь все понятно? Тогда двигаемся к
[35:05.600 --> 35:11.720]  скалярному произведению, к скалярному произведению. Вот. Скалярное произведение f
[35:11.720 --> 35:21.600]  xk, что там, zk, минус xk. Вот. Ну и смотрите, я вот когда-то что смотрю, вот мне здесь
[35:21.600 --> 35:27.360]  реально вот в этом то, что потихоньку получается, есть x, есть y. Вот здесь везде
[35:27.360 --> 35:37.880]  z-ки уже стоят, здесь x. Вот. Точка y выглядит лишней. То есть либо ее нужно использовать,
[35:37.880 --> 35:41.480]  то есть тут как у два варианта. Вот. Кажется, что вот здесь вот, когда мы все равно сейчас
[35:41.480 --> 35:47.400]  будем что-то вытаскивать, а как-то оценивать через гладкость или выпуклость, ну точка z
[35:47.400 --> 35:54.000]  вылезут значения функции. Вылезут значения функции, и значение функции в точке z, оно
[35:54.000 --> 35:59.200]  будет лишним, потому что здесь у нас есть x и здесь есть y. Опять же, это не какая-то строгая
[35:59.200 --> 36:04.440]  интуиция, почему нужно доказывать так. Вот. Потому что часто, ну это доказывается в некотором
[36:04.440 --> 36:10.120]  смысле путем пробы ошибок. Вот. Вы просто пробуете какое-то неравенство, получается с помощью него
[36:10.120 --> 36:14.080]  что-то оценить или нет. Вот. Ну вот здесь, соответственно, давайте попробуем это скалярное
[36:14.080 --> 36:19.680]  произведение с минусом. Сделать так, чтобы у нас вылезло после применения там гладкости
[36:19.680 --> 36:27.200]  либо выпуклости y. Вылезли y. Вот. Как это сделать? Как вот избавиться от z вот здесь?
[36:27.200 --> 36:37.040]  Что у нас есть, чтобы избавиться от z? Смотрите, мы уже использовали одну строку алгоритма,
[36:37.040 --> 36:42.760]  использовали вторую строку алгоритма. Есть еще одна строка, которая связывает z, x и y.
[36:42.760 --> 36:52.760]  Супер. А как будет выражаться z-каты? Через y и, соответственно, x. Вот.
[36:52.760 --> 37:01.960]  Ну давайте tau просто буду оставлять, опять же, умножить на что? Один неделительный tau.
[37:01.960 --> 37:19.840]  x-каты. Плюс. Минус, да, здесь? Минус. Один минус tau или что? Один минус tau на y-каты.
[37:19.840 --> 37:36.440]  Вот так, да? Вот так. Правда ведь или нет? Вот. Я надеюсь, что правда. Если не правда, мы не дойдем.
[37:36.440 --> 37:49.720]  Вот. Хорошо. Вот. Тогда я подставляю сюда вот это выражение, которое продиктовали, и получается здесь
[37:49.720 --> 38:00.920]  что? x-k минус один минус tau y-k минус tau x-k. Так. Ну я просто tau вынес за скобки. Вот. Один
[38:01.520 --> 38:08.640]  делительный tau за скалярное произведение. Поэтому здесь у меня tau еще возникло. Вот. И вроде бы,
[38:08.640 --> 38:15.480]  кстати, сейчас хорошо получится, потому что вылезет как раз еще дополнительный коэффициент вида один
[38:15.480 --> 38:30.840]  минус tau. И здесь будет что? x-k минус y-k. Так. Вроде все норм. Вот. Так. Ну теперь давайте,
[38:30.840 --> 38:35.320]  раз уж что-то хорошее появилось, скалярное произведение, которое кажется, ровно которое
[38:35.320 --> 38:43.960]  нам и нужно, давайте попробуем как-то оценить. Например, с помощью выпуклости. Как вот такое
[38:43.960 --> 38:58.560]  скалярное произведение со знаком минус оценить? Давайте выпуклость. У вас сильно выпуклость даже
[38:58.560 --> 39:17.400]  выписано. А? Сколько будет? f от x минус y-k. Я даже mu писать не буду, и она будет как бы большим
[39:17.400 --> 39:28.600]  просто каким-то. Ну а с минусом она будет. Да? Вот. У вас она там с минусом вылезет mu, ну то есть
[39:28.600 --> 39:37.880]  вот вылезет у вас минус mu пополам y-k минус x-k. Так? В квадрате. Вот. Ну я это просто скажу,
[39:37.880 --> 39:44.040]  что это все равно меньше нуля, поэтому в оценке это можно даже не писать. Вот. Ну мне сейчас она
[39:44.040 --> 39:53.120]  просто не нужна, не понадобится. Вот. Окей. Оценили скалярное произведение. Оценили скалярное
[39:53.120 --> 40:04.320]  произведение. Так? А, да, неравенство, конечно. Неравенство. Вот. Получили довольно хороший кусочек,
[40:04.320 --> 40:09.520]  ну что-то оценивали. Вот. Пора его подставлять. Вот в то, что мы уже наполучали до этого.
[40:09.520 --> 40:35.560]  Так. Надо это все в страничке. Вот. Это скалярное произведение. Тут еще было два гамма. Два гамма.
[40:35.560 --> 40:55.680]  Вот. И что у нас там наполучалось? 1-tau делить на tau f от x-k минус f от y-k. Так? Вот. И смотрите,
[40:55.680 --> 41:05.040]  уже что-то вырисовывается довольно красиво. Я увеличу. Вот. Сейчас. Так. Оценка скалярного
[41:05.040 --> 41:11.520]  произведения. Точно правильно выписали точки? Минус должно быть, да, здесь быть? Вот. Тут
[41:11.520 --> 41:19.560]  должен быть минус. Соответственно, здесь этот минус тоже есть. Вот. И видно, что хорошо получается.
[41:19.560 --> 41:26.760]  Я вот от этого x-а смогу избавиться. У меня будет разница y стоять просто. Y предыдущий и минус y
[41:26.760 --> 41:32.640]  следующий. Да? Но при условии, что я подберу правильно вот эти коэффициенты. Я же могу
[41:32.640 --> 41:37.520]  настраивать метод так, чтобы у меня эти коэффициенты были равны. Но я в частности это и запрошу,
[41:37.520 --> 41:44.160]  чтобы у меня вот эти два коэффициента при настройке метода, ну, были между собой равны.
[41:44.160 --> 41:59.040]  Тау. Вот. Скалярное произведение тогда давайте я подсотру уже. Вот. Мы потихонечку как раз будем
[41:59.040 --> 42:03.840]  переходить к подкатним кусочкам. Здесь у нас что будет? Здесь у нас будет… Мы потребовали,
[42:03.840 --> 42:08.280]  чтобы у нас все совпало. Так мы настроили параметры. Например, выбрали тау. Так,
[42:08.280 --> 42:18.120]  чтобы было выполнено соотношение. И тогда сверху у меня что будет? zk минус x звездой минус 2 гамма f
[42:18.120 --> 42:30.120]  xk. xk минус x звездой. А здесь соответственно в скобочках ну давайте запишу в виде 2 гамма это 2
[42:30.120 --> 42:43.120]  минус это l. Вот. А здесь у меня будет соответственно f от yk минус f от yk плюс 1. Окей? Вот. А теперь
[42:43.120 --> 42:54.640]  смотрите. Дальше уже довольно просто. Переносим в левую часть 2 гамма f от xk. xk минус x звездой.
[42:54.640 --> 42:58.720]  Такое вы уже видели, когда мы доказали для выпуклого случая. То есть влево переносится
[42:58.720 --> 43:03.800]  скалярное произведение. Вот. А вправо остаются соответственно разности.
[43:13.120 --> 43:32.960]  Так. Вот. Что дальше делаем? Дальше просто суммируем по всем k. Так. От нуля до последней
[43:32.960 --> 43:57.160]  итерации. Согласны? Угу. Супер. Так. Я сразу усредню еще. xk k минус x звездой. И здесь что у меня будет
[44:02.960 --> 44:21.960]  в виде 2 гамма f от yk минус f от yk минус xk минус xk минус xk минус xk минус xk минус xk минус xk
[44:21.960 --> 44:23.960]  минус xk.
[47:51.960 --> 48:04.960]  Как раз у х нулевое минус x звездой меньше либо равно чем что-то. f х нулевое минус f от x звездой.
[48:04.960 --> 48:10.960]  Там представляемое произведение, но почему я его не пишу сразу же? Кто понимает, почему я не писал
[48:10.960 --> 48:30.960]  соответственно, да, я оценил разность функций и получил следующее выражение. Ну вообще выглядит как
[48:30.960 --> 48:38.960]  сублинейная сходимость, потому что 1 делить на k все кажется плохо, но не все так печально. Почему?
[48:38.960 --> 48:46.960]  Потому что, во-первых, я что дальше подбираю? Подбираю правильно параметры это и гамма, чтобы вот это выражение,
[48:46.960 --> 48:51.960]  которое у меня здесь стоит, минимизировать. Правильно подобрать это и гамма, соответственно, оптимально это в данном случае
[48:51.960 --> 48:58.960]  это просто 1 делить на l, чтобы вот вторая дробь, которая написана была минимальна. Вот. Я ее подставляю.
[48:58.960 --> 49:04.960]  Сейчас чисто технические вещи идут. Вот. А дальше гамма подставляю, чтобы вот подобрать вот насколько вот, чтобы у меня
[49:04.960 --> 49:10.960]  теперь сумма двух дробей была минимальная. Гамма, соответственно, нужно подставить равное,
[49:10.960 --> 49:18.960]  гамма делить на 1 делить на корень из двух mu l. Это просто через производную выводится
[49:18.960 --> 49:24.960]  найти минимум функций по гамма и найти там минимум функций по это. Вот. Тут ничего сложного не должно быть.
[49:24.960 --> 49:28.960]  Я подставляю и получаю вот такое вот выражение.
[49:28.960 --> 49:32.960]  Где, где, где?
[49:38.960 --> 49:44.960]  Сократилось? Да, мы пользовались. Это не равенство. Было у нас...
[49:44.960 --> 49:50.960]  Что мы предполагали? Мы предполагали, что это у нас от 0 до 2 делить на l, так? Это раз.
[49:50.960 --> 49:56.960]  А что мы еще предполагали? Это правильный вопрос. Мы предполагали вот это. Да?
[49:56.960 --> 50:06.960]  Ну, смотрите, на самом деле, что это. Это получается 2 гамма, это 2-nl, 1-tau делить на tau, так?
[50:06.960 --> 50:14.960]  По факту это... так мы задаем tau. Видите? Здесь мы можем задать tau так, чтобы выполнилось вот это значение.
[50:14.960 --> 50:20.960]  Зная гамма, зная это, можно решить это уравнение и найти tau. Вот.
[50:20.960 --> 50:26.960]  Все. То есть вот это было на самом деле ограничение на tau. Вот. То есть тут проблем с этим нет.
[50:26.960 --> 50:32.960]  Здесь мы все это прогоняем, прогоняем, прогоняем. Вот.
[50:32.960 --> 50:38.960]  И получаем вот такое вот выражение, которое я подчеркну. Вроде бы кажется все плохо,
[50:38.960 --> 50:43.960]  сублинейная сходимость, значит, мы скорее всего проиграли, но на самом деле нет.
[50:43.960 --> 50:49.960]  Вот. Давайте возьмем k вот таким, чтобы у меня просто вот тот коэффициент, который стоял здесь,
[50:49.960 --> 50:56.960]  но он стал в итоге 1 и 2. Ну, k подберем. Понятно, он подбирается l, делить на mu, корень,
[50:56.960 --> 51:00.960]  и там восьмерка еще встает, чтобы одна вторая встала. Так?
[51:00.960 --> 51:05.960]  Это что означает? Что получается, что мы его запустили, вот этот алгоритм наш линейного каплинга,
[51:05.960 --> 51:11.960]  на k итераций, и можем гарантировать, что вот эта точка средняя, которую мы выдали как выход,
[51:11.960 --> 51:16.960]  она в два раза нас приблизила к решению. Так? Вот.
[51:16.960 --> 51:23.960]  Но мы же тогда этот каплинг можем перезапускать, брать как новую стартовую точку вот эту точку.
[51:23.960 --> 51:30.960]  И тогда вот эта точка у нас перекочует влево, вправо, и мы тогда еще в два раза сократим расстояние.
[51:30.960 --> 51:35.960]  Вот. Получается в некотором смысле такая рестартовая процедура. Нашли новую точку,
[51:35.960 --> 51:39.960]  ее взяли как стартовую, заново запустили каплинг.
[51:39.960 --> 51:46.960]  Вот. Получается, что вот если вы запустите этот каплинг на t итераций,
[51:46.960 --> 51:54.960]  то вы получите, что через t вот этих больших как бы эпох, t раз на k итераций вы его запустите,
[51:54.960 --> 52:00.960]  вы расстояние до решения приблизитесь там на 1 делить на 2t.
[52:00.960 --> 52:07.960]  Вот. Ну а дальше вы что, найдете t, потому что вам просто нужно там подобрать точность епсилон,
[52:07.960 --> 52:12.960]  вы по функции должны сойти с точностью епсилон, соответственно t равно вот такому значению.
[52:12.960 --> 52:17.960]  Просто пролагарифмировать. На логарифм двойке. Угу. Вот.
[52:17.960 --> 52:23.960]  А дальше все. Вы каждую итерацию вызываете градиент один раз.
[52:23.960 --> 52:28.960]  Каждую большую эту эпоху вы делаете k итераций, то есть k вызыва градиента,
[52:28.960 --> 52:33.960]  плюс эпох вы делаете t, и того вызыва градиента k умножить на t.
[52:33.960 --> 52:37.960]  Так? Окей?
[52:37.960 --> 52:42.960]  Вот. Получается, что оракульная сложность именно количества вызова градиента метода,
[52:42.960 --> 52:47.960]  который мы получили, будет корень из l делить нами.
[52:47.960 --> 52:51.960]  Вот. А у градиентного спуска просто l делить нами.
[52:51.960 --> 52:57.960]  Так? Вот. Это, в принципе, то, к чему стремились, получить что-то лучше.
[52:57.960 --> 53:06.960]  Вот. Соответственно, оценочка уже в виде теоремы все это выписано, параметры подобраны, k выбрано,
[53:06.960 --> 53:14.960]  tau здесь еще, на tau забыл ограничение написать, ну вот то, которое мы добавляли, 1-tau, равно чему-то там.
[53:14.960 --> 53:19.960]  Вот. Ну, в общем, получается вот такая оценка, которая лучшего градиентного спуска.
[53:19.960 --> 53:24.960]  Вот. Но между тем и даже к этой оценке остаются вопросы.
[53:24.960 --> 53:27.960]  Потому что, да, она лучше, но можно ли еще улучшить?
[53:27.960 --> 53:32.960]  Взять не корень, чтобы было из l делить нами, а корень четвертой степени, например.
[53:32.960 --> 53:36.960]  Вот. Как нам вообще понять то, что полученная оценка, например, не улучшается?
[53:36.960 --> 53:39.960]  Что нужно делать?
[53:40.960 --> 53:45.960]  Да. Нужно получить нижние оценки. Вот. Нужно получить нижние оценки.
[53:45.960 --> 53:49.960]  А в чем суть нижних оценок?
[53:49.960 --> 53:53.960]  Кто помнит, в чем суть нижних оценок?
[53:53.960 --> 54:02.960]  Наоборот, самый худший пример. Самый худший пример, что любой алгоритм работает плохо на нем. Да?
[54:02.960 --> 54:09.960]  Вот. Ну, давайте попробуем придумать такую задачу, на которой любой алгоритм будет работать плохо.
[54:09.960 --> 54:13.960]  Вот. Но вообще вопрос такой. А что значит любой метод?
[54:13.960 --> 54:18.960]  Ну, вот я сказал, любой метод. Что значит любой метод?
[54:23.960 --> 54:27.960]  Ну, вот мы же с вами уже выводили в некотором смысле нижние оценки.
[54:27.960 --> 54:30.960]  По методу мы строим задачу.
[54:30.960 --> 54:36.960]  Мы же как-то описывали, что такое любой метод. Мы явно не брали не все методы, а какие-то ограниченные.
[54:36.960 --> 54:39.960]  По оракулам мы их ограничивали.
[54:39.960 --> 54:44.960]  Помните, мы тогда рассматривали методы, которые оперируют информацию нулевого порядка.
[54:44.960 --> 54:50.960]  И здесь то же самое. То есть хочется ограничить метод как-то по оракулам.
[54:50.960 --> 54:56.960]  Первое условие довольно простое, что у нас у метода есть в некотором смысле какая-то начальная точка.
[54:56.960 --> 55:04.960]  То, откуда мы стартуем, соответственно она формирует то множество достижимости, в которые мы пришли.
[55:04.960 --> 55:10.960]  То есть точки, в которые мы умеем доходить методом.
[55:10.960 --> 55:14.960]  Как формируется? Что мы можем спрашивать в соответственном функции?
[55:14.960 --> 55:18.960]  Мы можем спрашивать у оракула, можем спрашивать градиент.
[55:18.960 --> 55:22.960]  Ну, как раз мы рассматриваем методы, которые умеют работать с градиентом.
[55:23.960 --> 55:27.960]  Спрашивать мы можем только в тех точках, которые мы уже достигли.
[55:27.960 --> 55:31.960]  Что в принципе логично, как работает градиентный спуск.
[55:31.960 --> 55:36.960]  Но самое интересное, как мы считаем множество достижимости.
[55:36.960 --> 55:43.960]  Это линейная комбинация из того, где мы уже были, и градиентов в этих точках.
[55:43.960 --> 55:48.960]  То есть в принципе это предполагает, что мы можем делать любую шагу градиентного спуска, использовать любые моменты.
[55:48.960 --> 55:55.960]  Главное просто считать линейную комбинацию из градиентов, точек, и, соответственно, иксов, которые мы уже достигли.
[55:55.960 --> 56:01.960]  Просто линейная комбинация, не важно с каким коэффициентом.
[56:01.960 --> 56:05.960]  Понятно, что это в некотором смысле вещь ограничивающая.
[56:05.960 --> 56:11.960]  Ну и в качестве выхода у нас просто какая-то точка из МК берется.
[56:11.960 --> 56:15.960]  После К вызова оракула мы, соответственно, говорим, что вот то, что лежит в МК,
[56:15.960 --> 56:23.960]  мы выбираем отсюда какую-то точку, из линейной оболочки градиентов, иксов, и выдаем ее как ответ.
[56:23.960 --> 56:27.960]  Подходят ли методы, которые мы изучали под это определение?
[56:31.960 --> 56:34.960]  Да, подходит. Не зря же мы его рассматриваем.
[56:34.960 --> 56:42.960]  То есть мы как раз берем градиенты в точки, которые мы уже достигли, считаем в этих точках градиент,
[56:42.960 --> 56:48.960]  и дальше следующую точку строим как некоторую комбинацию из старых точек и градиентов.
[56:48.960 --> 56:55.960]  Да, подходят, но, соответственно, все ли методы здесь учтены? На самом деле нет.
[56:55.960 --> 57:00.960]  То есть есть операции, когда мы можем вызывать градиент, но делать что-то кроме линейных комбинаций.
[57:00.960 --> 57:06.960]  Например, брать какие-то скалярные произведения градиентов и иксов.
[57:06.960 --> 57:11.960]  Вроде бы кажется какая-то непривычная комбинация, действие.
[57:11.960 --> 57:16.960]  Но оказывается, что такое действие, если его разрешить, нам оно сейчас запрещено.
[57:16.960 --> 57:20.960]  Может ускорить метод и достигать более высоких скоростей сходимости.
[57:20.960 --> 57:23.960]  Но мы сейчас работаем в таком что ли сетапе.
[57:23.960 --> 57:28.960]  Мы ограничили методы не просто тем, что мы можем вызывать градиенты.
[57:28.960 --> 57:33.960]  Мы можем считать только линейные комбинации градиентов и точек.
[57:33.960 --> 57:37.960]  Вот так вот ограничили. И в этом классе будем работать.
[57:37.960 --> 57:43.960]  Плохая проблема. Тоже результат не из сервиса, соответственно.
[57:43.960 --> 57:47.960]  Выглядится плохая проблема следующим образом.
[57:47.960 --> 57:53.960]  Квадратичная задача, как ни странно. Даже на квадратичной задаче может быть все плохо.
[57:53.960 --> 57:59.960]  Квадратичный кусочек плюс еще один квадратичный кусочек, который завязан на константу mu.
[57:59.960 --> 58:04.960]  Плюс вот такой вот линейный кусочек. Поймем, зачем он потом будет нужен.
[58:04.960 --> 58:09.960]  Смотрите, я не буду проверять, что эта задача является...
[58:09.960 --> 58:12.960]  l гладко сильно выпукла. Она действительно такой является.
[58:12.960 --> 58:14.960]  Она l гладкая, mu сильно выпукла.
[58:14.960 --> 58:18.960]  У вас в домашнем задании в том числе есть задача, как для квадратичной задачи
[58:18.960 --> 58:22.960]  оценить константу липшица l и константу сильно выпуклости mu.
[58:22.960 --> 58:26.960]  Для этой задачи ровно то же самое делается. Как оценить l и как оценить mu.
[58:26.960 --> 58:32.960]  Единственное, что может быть нетривиально доказать, что у матрицы A она ограничена четверкой.
[58:32.960 --> 58:37.960]  Вот такая матрица, 4i и минуса, будет положительно определена.
[58:37.960 --> 58:42.960]  А снизу у вас положительно полуопределена эта матрица.
[58:44.960 --> 58:50.960]  Этот значок обозначает, что у вас матрица положительно полуопределена.
[58:50.960 --> 58:55.960]  А такой значок обозначает, что у вас матрица такого вида будет положительно полуопределена.
[58:55.960 --> 59:06.960]  Получается вот такая матрица, пока не понятно, что с ней делать, даже не понятно, какой она размерности.
[59:06.960 --> 59:10.960]  То есть Нестер говорит, давайте не париться, возьмем размерность равной бесконечности.
[59:10.960 --> 59:13.960]  Можно взять и похитрее, взять конечную размерность.
[59:13.960 --> 59:23.960]  На самом деле, суть не меняется, бесконечность даже получше будет в плане сути, а не каких-то тонкостей и доказательств.
[59:23.960 --> 59:29.960]  Окей, давайте думать, что происходит, почему вообще вот такую проблему хочется рассматривать.
[59:29.960 --> 59:36.960]  Смотрите, исходим из предположения, что у нас начальная точка это ноль.
[59:36.960 --> 59:42.960]  Это в некотором стандартном ограничении, что мы стартуем из нуля для нижних оценок.
[59:42.960 --> 59:50.960]  Понятно, можно взять какую-то другую точку, но у вас задача, можно тогда сместить по оси координат, что вы будете всегда стартовать из нуля.
[59:50.960 --> 59:54.960]  Стартуем из точки ноль-ноль.
[59:54.960 --> 59:57.960]  Скажите мне, а чему вообще равен градиент этой функции?
[59:57.960 --> 01:00:03.960]  Вы опять же можете открыть себе слайд с этой функции и продиктовать, чему равен градиент этой функции.
[01:00:10.960 --> 01:00:13.960]  Совсем простая квадратичная задача.
[01:00:14.960 --> 01:00:18.960]  Чему там равен градиент у нее?
[01:00:18.960 --> 01:00:24.960]  Кто был на семинарах, где вы дифференцировали квадратичные задачи?
[01:00:24.960 --> 01:00:27.960]  Сколько там будет?
[01:00:31.960 --> 01:00:33.960]  Коэффициент.
[01:00:33.960 --> 01:00:43.960]  Плюс, минус, там минус.
[01:00:43.960 --> 01:00:51.960]  Е. Согласны? Это градиент.
[01:00:51.960 --> 01:00:56.960]  А давайте посмотрим, что будет, если мы в этот градиент, но мы же посмотрим, что происходит.
[01:00:56.960 --> 01:01:01.960]  У нас изначально дана просто нулевая точка, мы в ней по факту и только можем считать градиент.
[01:01:01.960 --> 01:01:07.960]  А что происходит с градиентом? Чему он равен, если точка ноль-ноль подставлена?
[01:01:11.960 --> 01:01:14.960]  Если я ноль подставлю, чему вот равен градиент?
[01:01:14.960 --> 01:01:17.960]  Последнему просто вот этому вещь.
[01:01:17.960 --> 01:01:20.960]  Я просто давайте выпишу, градиент пропорционален E1.
[01:01:20.960 --> 01:01:26.960]  То есть в градиенте будет только первая не нулевая координата, так?
[01:01:26.960 --> 01:01:28.960]  Согласны?
[01:01:28.960 --> 01:01:33.960]  Получается, когда мы возьмем линейную комбинацию х0 и градиента,
[01:01:33.960 --> 01:01:39.960]  у нас в этой линейной комбинации будет сколько не нулевых координат?
[01:01:39.960 --> 01:01:42.960]  Только одна, так?
[01:01:42.960 --> 01:01:47.960]  Только одна не нулевая координата.
[01:01:47.960 --> 01:01:50.960]  Причем первая, да?
[01:01:50.960 --> 01:01:52.960]  Согласны?
[01:01:52.960 --> 01:01:54.960]  Идем дальше.
[01:01:54.960 --> 01:01:59.960]  А теперь посчитаем градиент в точке x1, например.
[01:01:59.960 --> 01:02:03.960]  Где x1 принадлежит множеству...
[01:02:03.960 --> 01:02:05.960]  Давайте я так и запишу.
[01:02:05.960 --> 01:02:09.960]  Линейная оболочка, где первая координата не нулевая.
[01:02:09.960 --> 01:02:11.960]  Мы же как раз ее и достигли.
[01:02:11.960 --> 01:02:16.960]  Мы могли после первого вызова получить любую точку, где первая координата не нулевая.
[01:02:16.960 --> 01:02:18.960]  Все остальные нулевые.
[01:02:18.960 --> 01:02:20.960]  По-другому никак.
[01:02:20.960 --> 01:02:23.960]  Посмотрите на вид матрицы.
[01:02:23.960 --> 01:02:26.960]  И подумайте, а что получится тогда?
[01:02:26.960 --> 01:02:33.960]  Что можно сказать про не нулевость координат следующего градиента?
[01:02:33.960 --> 01:02:36.960]  Матрица, вот она.
[01:02:36.960 --> 01:02:41.960]  Она не зря у вас такой имеет странный вид, такой диагональный.
[01:02:41.960 --> 01:02:48.960]  Что будет, если я возьму вектор, где только первая координата не нулевая и домножим эту матрицу?
[01:02:48.960 --> 01:02:53.960]  Где могут быть не нулевые координаты только?
[01:02:53.960 --> 01:03:00.960]  Смотрите, я домножаю на вектор, где у меня вот эта координата не нулевая.
[01:03:00.960 --> 01:03:05.960]  Получается, что первая может быть не нулевой и вторая.
[01:03:05.960 --> 01:03:11.960]  Получается, что в новом градиенте только первая и вторая координаты могут быть не нулевые.
[01:03:11.960 --> 01:03:21.960]  А значит, я апдейт, который получу, взяв там х1, изм1 и градиент х1, стиль, например.
[01:03:21.960 --> 01:03:30.960]  Получается, что вот это максимум только две не нулевых координаты.
[01:03:30.960 --> 01:03:33.960]  Согласны?
[01:03:33.960 --> 01:03:35.960]  И так далее.
[01:03:35.960 --> 01:03:42.960]  Дальше вы будете размораживать одну координату и видеть, что у вас теперь максимум три не нулевые координата.
[01:03:42.960 --> 01:03:53.960]  Получается, задача имеет такой вид, что вы за один вызов оракула в выходе алгоритма добавляете одну не нулевую координату.
[01:03:53.960 --> 01:03:58.960]  Супер. Это важное замечание, которое мы могли найти.
[01:03:58.960 --> 01:04:00.960]  В лучшем случае.
[01:04:00.960 --> 01:04:08.960]  Когда вы говорите про решение, то ваш алгоритм может угадать первые координаты.
[01:04:08.960 --> 01:04:13.960]  Если вы вызвали оракула х1, то первые координаты он может угадать в точности.
[01:04:13.960 --> 01:04:16.960]  А все остальные он не угадает. Согласны?
[01:04:16.960 --> 01:04:18.960]  Просто потому, что у него там ноль.
[01:04:18.960 --> 01:04:21.960]  У него там ноль, и он не может никуда их двигать.
[01:04:21.960 --> 01:04:26.960]  Поэтому давайте поймем, как выглядят решения этой задачи.
[01:04:27.960 --> 01:04:30.960]  Чтобы понять, а что там происходит в координатах.
[01:04:30.960 --> 01:04:35.960]  Где у нас выход будет ноль, а координаты реально не ноль.
[01:04:35.960 --> 01:04:41.960]  Насколько там много мы упустим, если мы вызовем оракулу всего к раз.
[01:04:41.960 --> 01:04:45.960]  То есть первый вывод мы сделали.
[01:04:45.960 --> 01:04:52.960]  К вызовов оракула равно к не нулевых координатах.
[01:04:52.960 --> 01:04:57.960]  Теперь давайте находить решение х звездой нашей задачи.
[01:04:57.960 --> 01:04:59.960]  Как будем находить?
[01:05:03.960 --> 01:05:06.960]  Градиент 0, правильно. Почему это рабочая схема?
[01:05:10.960 --> 01:05:13.960]  Задача у нас сильно выпуклая.
[01:05:13.960 --> 01:05:16.960]  Значит решение единственное и уникальное.
[01:05:18.960 --> 01:05:20.960]  Хорошо, градиент равен 0.
[01:05:20.960 --> 01:05:23.960]  Поищем решение, поищем решение.
[01:05:26.960 --> 01:05:30.960]  Давайте выпишем это все безобразие, потренируемся.
[01:05:32.960 --> 01:05:36.960]  Градиент равен 0. Градиент я уже вверху вот выписал.
[01:05:36.960 --> 01:05:38.960]  Вот у меня градиент.
[01:05:38.960 --> 01:05:46.960]  Кажется, что в силу вида матрицы уникальное что-то будет в первой строчке, в последней.
[01:05:46.960 --> 01:05:50.960]  А во всех центральных будет одинаковая какая-то зависимость.
[01:05:50.960 --> 01:05:52.960]  Давайте выписываем просто.
[01:05:52.960 --> 01:05:54.960]  Для первой строчки что будет?
[01:05:54.960 --> 01:05:57.960]  Когда я буду брать градиент по первой строчке?
[01:06:03.960 --> 01:06:09.960]  У чего у меня здесь будет L mu делить на 8 матрица A умножить на...
[01:06:09.960 --> 01:06:12.960]  Что будет стоять в первой координате у градиента?
[01:06:12.960 --> 01:06:16.960]  Вот здесь у этого градиента в первой координате что будет стоять?
[01:06:19.960 --> 01:06:27.960]  2х1-х2 плюс mu пополам на что?
[01:06:28.960 --> 01:06:30.960]  На х1.
[01:06:32.960 --> 01:06:34.960]  Точно так?
[01:06:34.960 --> 01:06:36.960]  Или что-то все же теряю?
[01:06:38.960 --> 01:06:40.960]  Все нормально, да?
[01:06:41.960 --> 01:06:44.960]  Вообще в градиенте мы на самом деле потерялись,
[01:06:44.960 --> 01:06:48.960]  потому что здесь же когда мы квадратичную задачу дифференцируем,
[01:06:48.960 --> 01:06:50.960]  двоечка-то вылезает еще.
[01:06:50.960 --> 01:06:52.960]  Поэтому здесь вот этих нету двоечек.
[01:06:58.960 --> 01:07:01.960]  Здесь еще вот такая добавочка.
[01:07:01.960 --> 01:07:02.960]  Согласны?
[01:07:02.960 --> 01:07:03.960]  Единичка.
[01:07:03.960 --> 01:07:06.960]  Просто единичка от вот этого единичного вектора.
[01:07:07.960 --> 01:07:11.960]  Получается вот такая вот зависимость.
[01:07:11.960 --> 01:07:14.960]  А теперь давайте выпишем все остальные строки.
[01:07:14.960 --> 01:07:16.960]  Что там будет получаться?
[01:07:22.960 --> 01:07:24.960]  Минус х...
[01:07:24.960 --> 01:07:26.960]  Ну давайте я буду вот так.
[01:07:26.960 --> 01:07:27.960]  И минус один писать.
[01:07:27.960 --> 01:07:29.960]  Плюс х и т.
[01:07:29.960 --> 01:07:31.960]  Два х и т.
[01:07:31.960 --> 01:07:34.960]  Минус х и т плюс один.
[01:07:34.960 --> 01:07:36.960]  Можно даже без скобочек.
[01:07:39.960 --> 01:07:41.960]  И так понятно, что это координата,
[01:07:41.960 --> 01:07:43.960]  потому что индекс мы вверху пишем.
[01:07:43.960 --> 01:07:44.960]  Итерация.
[01:07:46.960 --> 01:07:48.960]  Здесь, соответственно, это уже не первая строка,
[01:07:48.960 --> 01:07:50.960]  поэтому вот этого кусочка там не будет.
[01:07:50.960 --> 01:07:52.960]  Останется только кусочек от mu.
[01:07:52.960 --> 01:07:56.960]  mu х и t равно нулю.
[01:07:59.960 --> 01:08:01.960]  Ну и там еще будет последняя строчка.
[01:08:01.960 --> 01:08:03.960]  Чуть поподробнее я на слайде уже покажу,
[01:08:03.960 --> 01:08:06.960]  чтобы долго с этим не возиться.
[01:08:06.960 --> 01:08:08.960]  Вот, что это такое?
[01:08:08.960 --> 01:08:11.960]  Как мне, например, находить х и т отсюда?
[01:08:11.960 --> 01:08:13.960]  Кто понимает, что это?
[01:08:19.960 --> 01:08:21.960]  Ну смотрите, а сверху вниз тоже непонятно.
[01:08:21.960 --> 01:08:23.960]  Вот я вот тут вот, видите, застрял.
[01:08:23.960 --> 01:08:26.960]  У меня х и т, х1 и х2 как-то завязаны.
[01:08:26.960 --> 01:08:28.960]  Там еще последняя строчка просто есть.
[01:08:28.960 --> 01:08:30.960]  Как вот найти связь, например,
[01:08:30.960 --> 01:08:32.960]  исходя только вот из этой строчки
[01:08:32.960 --> 01:08:34.960]  между всеми х?
[01:08:34.960 --> 01:08:36.960]  Вот, кто понимает?
[01:08:38.960 --> 01:08:40.960]  х2 можно через 1?
[01:08:40.960 --> 01:08:41.960]  Ну можно, можно.
[01:08:41.960 --> 01:08:43.960]  Вообще я намекаю на то,
[01:08:43.960 --> 01:08:44.960]  что вот то, что у вас записано,
[01:08:44.960 --> 01:08:46.960]  это же рекуррента просто.
[01:08:46.960 --> 01:08:49.960]  Есть х и т, х и т плюс первая,
[01:08:49.960 --> 01:08:52.960]  которая выражена через линейные коэффициенты,
[01:08:52.960 --> 01:08:56.960]  х и т плюс бета х и минус первая.
[01:08:56.960 --> 01:08:58.960]  Так?
[01:08:58.960 --> 01:09:00.960]  Такие рекурренты вы не умеете решать?
[01:09:02.960 --> 01:09:03.960]  Умеете.
[01:09:03.960 --> 01:09:05.960]  Что нужно сделать?
[01:09:05.960 --> 01:09:08.960]  Надо написать характеристический многочлен.
[01:09:08.960 --> 01:09:10.960]  В духе лямбда в квадрате равно
[01:09:10.960 --> 01:09:13.960]  альфа лямбда плюс бета.
[01:09:13.960 --> 01:09:14.960]  Характеристический многочлен.
[01:09:14.960 --> 01:09:16.960]  Находите его корни.
[01:09:16.960 --> 01:09:18.960]  Ну и, соответственно, решение у вас
[01:09:18.960 --> 01:09:20.960]  будет выписываться в виде
[01:09:20.960 --> 01:09:22.960]  c какой-то константы 1,
[01:09:22.960 --> 01:09:24.960]  корень 1 в степени
[01:09:24.960 --> 01:09:27.960]  и плюс c2 лямбда 2 в степени.
[01:09:27.960 --> 01:09:30.960]  Соответственно, так решается эта рекуррента.
[01:09:30.960 --> 01:09:32.960]  Одно начальное у нас условие
[01:09:32.960 --> 01:09:34.960]  будет здесь болтаться.
[01:09:34.960 --> 01:09:35.960]  Второе начальное условие у нас
[01:09:35.960 --> 01:09:36.960]  будет болтаться вот здесь,
[01:09:36.960 --> 01:09:37.960]  потому что, видите, мы тут
[01:09:37.960 --> 01:09:39.960]  замыкаем на вот эту z,
[01:09:39.960 --> 01:09:41.960]  потому что, я как сказал,
[01:09:41.960 --> 01:09:42.960]  все линии будут совпадать,
[01:09:42.960 --> 01:09:46.960]  кроме последней и первой.
[01:09:46.960 --> 01:09:49.960]  Решается эта рекуррента.
[01:09:49.960 --> 01:09:52.960]  Давайте я ее не буду решать.
[01:09:52.960 --> 01:09:56.960]  Я просто покажу ответик.
[01:09:56.960 --> 01:09:59.960]  Пожалуйста, рекуррент выписал.
[01:09:59.960 --> 01:10:02.960]  Соответственно, это для последней строчки
[01:10:02.960 --> 01:10:05.960]  значения.
[01:10:05.960 --> 01:10:07.960]  Можно, соответственно, подобрать
[01:10:07.960 --> 01:10:09.960]  вот этот z, который я здесь поставил,
[01:10:09.960 --> 01:10:12.960]  так, чтобы у вас в итоге в рекурренте
[01:10:12.960 --> 01:10:14.960]  вылезло одно из значений лямда,
[01:10:14.960 --> 01:10:17.960]  лямда катых, причем еще с коэффициентом c
[01:10:17.960 --> 01:10:19.960]  равным единице.
[01:10:19.960 --> 01:10:22.960]  Вот так вот z это подбирается.
[01:10:22.960 --> 01:10:24.960]  И при этом q будет равно,
[01:10:24.960 --> 01:10:26.960]  это один из корней многочлена
[01:10:26.960 --> 01:10:28.960]  характеристического,
[01:10:28.960 --> 01:10:30.960]  который меньше единицы.
[01:10:30.960 --> 01:10:32.960]  Соответственно, можно подобрать z так,
[01:10:32.960 --> 01:10:34.960]  чтобы у вас в итоге, по факту,
[01:10:34.960 --> 01:10:36.960]  все вот эти x каты будут выражаться
[01:10:36.960 --> 01:10:37.960]  вот в таком вот виде.
[01:10:37.960 --> 01:10:39.960]  Это можно проделать как упражнение.
[01:10:39.960 --> 01:10:41.960]  Линейную рекурренту вы уж сами решите.
[01:10:41.960 --> 01:10:42.960]  Тут ничего сложного нет.
[01:10:42.960 --> 01:10:45.960]  Согласен, это мы уж делать не будем.
[01:10:45.960 --> 01:10:47.960]  Я думаю, с этим вы справитесь.
[01:10:47.960 --> 01:10:49.960]  Там проходят, где вы проходите,
[01:10:49.960 --> 01:10:52.960]  на комбинаторике или на...
[01:10:52.960 --> 01:10:53.960]  Вот.
[01:10:53.960 --> 01:10:56.960]  Так что...
[01:10:56.960 --> 01:10:57.960]  Соответственно, справитесь,
[01:10:57.960 --> 01:10:58.960]  это рекуррент,
[01:10:58.960 --> 01:11:00.960]  и там ничего сложного нет.
[01:11:00.960 --> 01:11:02.960]  Поняли, как примерно выглядит.
[01:11:02.960 --> 01:11:03.960]  Дальше, соответственно,
[01:11:03.960 --> 01:11:05.960]  вот это то, что мы с вами обсудили,
[01:11:05.960 --> 01:11:07.960]  как меняется решение,
[01:11:07.960 --> 01:11:10.960]  что мы приближаемся только на k-координат.
[01:11:10.960 --> 01:11:12.960]  А дальше смотрите, какой трюк.
[01:11:12.960 --> 01:11:14.960]  Давайте теперь я размерность подберу
[01:11:14.960 --> 01:11:16.960]  и скажу, что у меня размерность равна 2k.
[01:11:16.960 --> 01:11:17.960]  То есть, зная, например,
[01:11:17.960 --> 01:11:19.960]  на период количества вызова фаракула,
[01:11:19.960 --> 01:11:20.960]  я скажу, что у меня размерность
[01:11:20.960 --> 01:11:21.960]  2 раза больше.
[01:11:21.960 --> 01:11:23.960]  А зачем я так сделал?
[01:11:28.960 --> 01:11:29.960]  Правильно.
[01:11:29.960 --> 01:11:31.960]  Чтобы у меня k-координат
[01:11:31.960 --> 01:11:34.960]  гарантированно были нулевыми в выходе.
[01:11:34.960 --> 01:11:36.960]  То есть, k-координат первых
[01:11:36.960 --> 01:11:39.960]  я в лучшем случае подберу,
[01:11:39.960 --> 01:11:42.960]  а вот k последних координат точно не подберу,
[01:11:42.960 --> 01:11:43.960]  потому что там гарантированно 0,
[01:11:43.960 --> 01:11:45.960]  как мы с вами и обсудили.
[01:11:45.960 --> 01:11:46.960]  Вот.
[01:11:46.960 --> 01:11:48.960]  Хорошо.
[01:11:48.960 --> 01:11:49.960]  Соответственно, да.
[01:11:49.960 --> 01:11:53.960]  Тогда у меня начальное приближение к x.
[01:11:53.960 --> 01:11:54.960]  То есть, это у меня просто 0.
[01:11:54.960 --> 01:11:56.960]  Поэтому мне нужно просто посчитать
[01:11:56.960 --> 01:11:58.960]  вот эти q и t.
[01:11:58.960 --> 01:11:59.960]  То есть, ну, тут квадрат стоит,
[01:11:59.960 --> 01:12:00.960]  поэтому здесь двоечка.
[01:12:00.960 --> 01:12:02.960]  Ну, я выразил вот так вот.
[01:12:02.960 --> 01:12:03.960]  Согласны, что это верно?
[01:12:03.960 --> 01:12:05.960]  Я просто разбил на две суммы.
[01:12:06.960 --> 01:12:09.960]  То есть, сумма была от 1 до 2k,
[01:12:09.960 --> 01:12:12.960]  а осталось две суммы от 1 до k.
[01:12:12.960 --> 01:12:13.960]  Так?
[01:12:15.960 --> 01:12:16.960]  Ну, зачем я это сделал?
[01:12:16.960 --> 01:12:18.960]  Просто удобно.
[01:12:18.960 --> 01:12:19.960]  Здесь я что делаю?
[01:12:19.960 --> 01:12:22.960]  Я хочу как раз оценить расстояние до решения.
[01:12:22.960 --> 01:12:25.960]  Я предполагаю, что в первых k-координатах я попал,
[01:12:25.960 --> 01:12:27.960]  а в остальных я уж точно не попал,
[01:12:27.960 --> 01:12:29.960]  поэтому я суммирую все мои непопадания.
[01:12:30.960 --> 01:12:31.960]  Вот.
[01:12:32.960 --> 01:12:36.960]  Выношу коэффициентик, который тут общий будет, q в степени 2k.
[01:12:36.960 --> 01:12:38.960]  Получается вот такая вот сумма.
[01:12:38.960 --> 01:12:39.960]  Так?
[01:12:39.960 --> 01:12:41.960]  Я вот к этой сумме просто и хотел привести,
[01:12:41.960 --> 01:12:43.960]  поэтому я здесь ее расписал чуть похитрее.
[01:12:43.960 --> 01:12:44.960]  Вот.
[01:12:44.960 --> 01:12:47.960]  И тогда вот у меня расстояние до решения
[01:12:47.960 --> 01:12:49.960]  можно оценить вот в следующем образе.
[01:12:50.960 --> 01:12:51.960]  Угу.
[01:12:51.960 --> 01:12:53.960]  Ну, тут простая алгебра у вас,
[01:12:53.960 --> 01:12:55.960]  вот эту сумму выражается вот отсюда
[01:12:55.960 --> 01:12:57.960]  и подставляется вот сюда.
[01:12:58.960 --> 01:12:59.960]  Вот.
[01:13:01.960 --> 01:13:04.960]  Но так как у меня q – это число меньше единицы,
[01:13:04.960 --> 01:13:07.960]  я когда возвожу степень, это становится чем-то
[01:13:07.960 --> 01:13:09.960]  все меньше, меньше, меньше единицы.
[01:13:09.960 --> 01:13:11.960]  Поэтому я просто это могу оценить единицей
[01:13:11.960 --> 01:13:15.960]  и сказать, что в худшем случае я здесь на двоечку просто разделю.
[01:13:15.960 --> 01:13:16.960]  Так?
[01:13:16.960 --> 01:13:17.960]  Вот.
[01:13:17.960 --> 01:13:18.960]  Получилась вот такая вот оценка.
[01:13:18.960 --> 01:13:20.960]  Дальше я подставляю q
[01:13:20.960 --> 01:13:22.960]  и получаю уже итоговый результат,
[01:13:22.960 --> 01:13:24.960]  мою вот эту верхнюю оценку,
[01:13:24.960 --> 01:13:25.960]  мою нижнюю оценку.
[01:13:25.960 --> 01:13:28.960]  То есть получается, что вот снизу
[01:13:28.960 --> 01:13:32.960]  расстояние до моего решения ограничивается вот такой вот формулой.
[01:13:32.960 --> 01:13:33.960]  Так?
[01:13:35.960 --> 01:13:38.960]  Видно, что здесь возникают как раз нужны нам корни.
[01:13:38.960 --> 01:13:40.960]  Корень из l делить на корень из mu.
[01:13:40.960 --> 01:13:43.960]  До этого у нас что-то возникало в духе 1 минус
[01:13:43.960 --> 01:13:45.960]  корень из mu делить на корень из l.
[01:13:45.960 --> 01:13:47.960]  Ну, именно по оракульной сложности.
[01:13:47.960 --> 01:13:49.960]  А здесь у нас как раз то же самое,
[01:13:49.960 --> 01:13:51.960]  но подпирает нас уже снизу.
[01:13:51.960 --> 01:13:52.960]  Да, это вот то, что у нас возникало,
[01:13:52.960 --> 01:13:54.960]  оно нас подпирало сверху.
[01:13:54.960 --> 01:13:55.960]  Вот.
[01:13:55.960 --> 01:13:56.960]  А это у нас подпирает снизу.
[01:13:56.960 --> 01:13:58.960]  Да, коэффициенты может быть какие-то другие.
[01:13:58.960 --> 01:13:59.960]  Двойка какая-то возникла.
[01:13:59.960 --> 01:14:01.960]  Здесь еще эта двойка.
[01:14:01.960 --> 01:14:02.960]  Вот.
[01:14:02.960 --> 01:14:05.960]  Но глобально это получается то же самое.
[01:14:05.960 --> 01:14:06.960]  Вот.
[01:14:08.960 --> 01:14:11.960]  Соответственно, получается, что мы с двух сторон подперли.
[01:14:11.960 --> 01:14:13.960]  Единственное, что я еще отмечу,
[01:14:13.960 --> 01:14:16.960]  что вот здесь вот критерии сходимости по x,
[01:14:16.960 --> 01:14:18.960]  там у нас был по функции,
[01:14:18.960 --> 01:14:20.960]  но опять же между ними можно переходить
[01:14:20.960 --> 01:14:21.960]  в сильно выпуклую задачу,
[01:14:21.960 --> 01:14:22.960]  у вас там будет просто появляться
[01:14:22.960 --> 01:14:25.960]  дополнительный фактор в духе там mu на l,
[01:14:25.960 --> 01:14:26.960]  или там будет появляться
[01:14:26.960 --> 01:14:27.960]  дополнительный фактор mu на l.
[01:14:27.960 --> 01:14:28.960]  Вот.
[01:14:28.960 --> 01:14:30.960]  Логарифма-то не самое главное,
[01:14:30.960 --> 01:14:31.960]  что нас волнует.
[01:14:31.960 --> 01:14:36.960]  Нас волнует вот этот коэффициентик l делить на mu.
[01:14:36.960 --> 01:14:37.960]  Корень из l делить на mu.
[01:14:37.960 --> 01:14:39.960]  Получается вот такой вот схемой,
[01:14:39.960 --> 01:14:43.960]  ну вот, идеи на самом деле довольно нетривиальные.
[01:14:43.960 --> 01:14:44.960]  Вот.
[01:14:44.960 --> 01:14:45.960]  Можно добиться того,
[01:14:45.960 --> 01:14:47.960]  что как раз по координатам не сходитесь,
[01:14:47.960 --> 01:14:49.960]  сколько это координат неправильных,
[01:14:49.960 --> 01:14:51.960]  и там они соответственно оцениваются,
[01:14:51.960 --> 01:14:53.960]  что вот l делить на mu
[01:14:53.960 --> 01:14:54.960]  это то количество,
[01:14:54.960 --> 01:14:55.960]  которое вам нужно вызвать.
[01:14:55.960 --> 01:14:57.960]  Корень из l делить на mu.
[01:14:57.960 --> 01:14:58.960]  Получается, что та оценка,
[01:14:58.960 --> 01:15:01.960]  которую мы получили для линейного каплинга
[01:15:01.960 --> 01:15:02.960]  с рестартами,
[01:15:02.960 --> 01:15:04.960]  она правильная,
[01:15:04.960 --> 01:15:05.960]  то есть она не улучшаемая.
[01:15:05.960 --> 01:15:07.960]  Ну, с точки зрения там o,
[01:15:07.960 --> 01:15:09.960]  до константа получается,
[01:15:09.960 --> 01:15:11.960]  до константа она не улучшаема.
[01:15:11.960 --> 01:15:14.960]  Какие-то численные константы могут быть в нижней оценке лучше.
[01:15:14.960 --> 01:15:16.960]  Вот.
[01:15:16.960 --> 01:15:18.960]  Вот к такому выводу пришли
[01:15:18.960 --> 01:15:20.960]  довольно такой
[01:15:20.960 --> 01:15:22.960]  хороший и жесткий вывод.
[01:15:22.960 --> 01:15:24.960]  На самом деле можно повторить
[01:15:24.960 --> 01:15:27.960]  то же самое для выпуклых задач.
[01:15:27.960 --> 01:15:29.960]  То есть мы сейчас все это проделывали
[01:15:29.960 --> 01:15:31.960]  для сильно выпуклых задач.
[01:15:31.960 --> 01:15:33.960]  Исходимость и нижнюю оценку
[01:15:33.960 --> 01:15:35.960]  можно повторить для выпуклых задач.
[01:15:35.960 --> 01:15:37.960]  Ровно те же самые похожие примеры,
[01:15:37.960 --> 01:15:39.960]  похожий анализ в пособии все будет.
[01:15:39.960 --> 01:15:41.960]  Метод Нестерова
[01:15:41.960 --> 01:15:43.960]  также имеет
[01:15:43.960 --> 01:15:45.960]  такие же верхние оценки,
[01:15:45.960 --> 01:15:47.960]  как и линейный каплинг.
[01:15:47.960 --> 01:15:49.960]  Но в отличии, например, от линейного каплинга
[01:15:49.960 --> 01:15:51.960]  его не нужно рестартовать,
[01:15:51.960 --> 01:15:53.960]  что довольно хорошо.
[01:15:53.960 --> 01:15:55.960]  Ну и, соответственно, да,
[01:15:55.960 --> 01:15:57.960]  получается, что метод Нестерова
[01:15:57.960 --> 01:15:59.960]  именно с точки зрения количества вызовов аракула
[01:15:59.960 --> 01:16:01.960]  у нас является оптимальным.
[01:16:01.960 --> 01:16:03.960]  Вот.
[01:16:03.960 --> 01:16:05.960]  Это фундаментальный результат,
[01:16:05.960 --> 01:16:07.960]  который, соответственно, Юрий Евгеньевич получил.
[01:16:07.960 --> 01:16:09.960]  Очень крутой результат.
[01:16:09.960 --> 01:16:11.960]  И, как ни странно, видите,
[01:16:11.960 --> 01:16:13.960]  он всплыл в нейросетях
[01:16:13.960 --> 01:16:15.960]  через 20-30 лет
[01:16:15.960 --> 01:16:17.960]  после того, как
[01:16:17.960 --> 01:16:19.960]  это все было получено
[01:16:19.960 --> 01:16:21.960]  для выпуклых задач.
[01:16:21.960 --> 01:16:23.960]  Ну и все.
[01:16:23.960 --> 01:16:25.960]  На сегодня, соответственно, у меня все.

[00:00.000 --> 00:11.480]  Давайте начнем наш доп-семинар, значит, что мы сегодня сделаем? Мы сегодня решим какой-нибудь
[00:11.480 --> 00:17.080]  вариантик, в нем будет 6 задач, вот, и, наверное, какую-то вспомогательную теорию, которая нам
[00:17.080 --> 00:24.280]  будет нужна, повторим. Первый вопрос, который хотел бы вам задать, насколько хорошо вы помните
[00:24.280 --> 00:28.840]  сходимости из прошлого семестра? Потому что у нас сходимости перекочевали в Тиар-Вер, и поэтому мы
[00:28.840 --> 00:35.520]  уже очень многое там рассмотрели. Чуть-чуть повторить надо, наверное. Чисто определение и
[00:35.520 --> 00:40.480]  задачку сразу решим. Отлично, хорошо. А, еще я должен сказать, что вот сейчас общего варианта
[00:40.480 --> 00:44.480]  какого-то нет, поэтому нет понимания, как будет выглядеть контрольная. Сколько у вас будет задач,
[00:44.480 --> 00:50.760]  сколько у вас чего будет, поэтому просто какой-то вариант контрольный решим. У всех групп разный,
[00:50.760 --> 01:07.800]  да. Ну, давайте начнем тогда. Первая тема у нас это сходимости. Так, что мы хотим научиться делать
[01:07.800 --> 01:12.600]  в данной задаче? В данной задаче мы хотим научиться считать какие-нибудь сходимости каких-нибудь
[01:12.600 --> 01:19.880]  страшных случайных величин. Что мы должны знать? У нас сходимости бывают разные. Вот, мы с вами
[01:19.960 --> 01:25.160]  сначала определяли сходимости для случайных величин именно. У нас была сходимость為 что,
[01:25.160 --> 01:34.600]  наверное, была сходимость по вероятности, была сходимость по распределению, именно случайных
[01:34.600 --> 01:42.680]  величин. Потом мы с вами рассматривали случайные виктора. И одна из ключевых теорем говорит о том,
[01:42.680 --> 01:48.060]  что вот, сходимость по компонентное случайных величин эквивалентна сходимости викторов. Я это
[01:48.060 --> 02:08.260]  это обозначено следующим образом. Вот. Вот. Вот. Вот этим утверждением, например, можно
[02:08.260 --> 02:12.980]  пользоваться. Для чего? Мы можем с вами взять сходимость там, почти наверное, или
[02:12.980 --> 02:17.460]  по вероятности случайных величин, составить из них вектор и что-то с ними потом сделать.
[02:17.460 --> 02:22.580]  Что мы сейчас разберемся? Вот. Надо помнить, что для сходимости по распределению это неверно. То
[02:22.580 --> 02:35.380]  есть из сходимости компонент не следует сходимость векторов, составленные из этих компонентов. Вот.
[02:35.380 --> 02:44.380]  Итак, мы уже, значит, имеем с вами сходимость, сходимость, сходимость, сходимость, сходимость,
[02:44.380 --> 02:50.620]  почти наверное, по вероятности по распределению сходимость не имеем векторов. В обратную сторону
[02:50.620 --> 02:58.580]  эта стрелочка выполняется, действительно. Но вот есть случай, в котором стрелка вправо работает.
[02:58.580 --> 03:04.540]  Стрелка вправо работает в следующем случае. Пусть у вас одна случайная величина, одна
[03:04.540 --> 03:10.620]  последовательность случайных величин сходится по распределению к случайной величине. А вторая
[03:10.620 --> 03:18.220]  последовательность случайных величин, ттм назовем, сходится по распределению константе. Вот. На самом
[03:18.220 --> 03:26.300]  деле вот из этого будет следовать, что вектор, составленный из этих компонентов, будет сходиться
[03:26.300 --> 03:38.380]  вот так вот. Хорошо, больше не буду. Отлично. Мы уже каким-то образом научились получать сходимости
[03:38.380 --> 03:43.180]  векторов. Что мы с ними можем делать? Дальше на самом деле мы можем просто применять к ним
[03:43.180 --> 03:55.820]  непрерывные функции. Вот. Это называется теорема непрерывности. Если опустить какие-то не особо
[03:55.820 --> 04:09.660]  значимые детали, то если у вас аж это непрерывная функция, где ям эта размерность вот наших векторов,
[04:09.660 --> 04:17.580]  то на самом деле если у вас была какая-то сходимость векторов, вот в каком угодном смысле
[04:17.580 --> 04:24.380]  почти наверно, по вероятности, по распределению, то на самом деле у вас будет и сходимость
[04:24.380 --> 04:35.100]  случайных личин, которые получаются применением какой-то непрерывной функции. Вот. Это теорема
[04:35.100 --> 04:39.540]  наследования сходимости или теорема непрерывности. Вот. Если у вас есть какая-то непрерывная функция,
[04:39.540 --> 04:45.980]  то применив ее к вектору, сходящемуся в каком-то из этих трех смыслов, вы получите новую
[04:45.980 --> 04:49.940]  последовательность векторов, которая сходится вот в новом пространстве в том же смысле. То есть
[04:49.940 --> 04:53.180]  если было почти наверно, то почти наверно, было по вероятности, по вероятности получили,
[04:53.180 --> 05:01.620]  было по распределению, получили по распределению. Вот. Ну и в принципе это уже достаточно много нам
[05:01.620 --> 05:07.660]  дает. Почему? Потому что вот представьте, что у вас есть две случайные величины, две последовательности
[05:07.660 --> 05:20.740]  случайных личин, которые сходятся к чему-то. Это вот пример небольшой. То вы можете составить
[05:20.740 --> 05:29.580]  из них вектор следующим шагом. Он будет сходиться. И можете применить какую-нибудь непрерывную
[05:29.580 --> 05:34.700]  функцию. Ну, например, сложение. Вот. И из этого можно получить, что у вас, допустим, сумма,
[05:34.700 --> 05:46.380]  последовательность сумм сходится в сумме пределов. Понятно, да? То есть в принципе какую-то страшную
[05:46.380 --> 05:52.260]  сходимость можно будет получить из покомпонентных сходимостей, составить из них векторочек и к
[05:52.260 --> 05:56.180]  векторочек применить какую-нибудь непрерывную функцию. Вот. Но опять-таки тоже заглоска в том,
[05:56.180 --> 06:00.580]  что у нас по распределению как бы вектор так просто составить нельзя. Нужно чтобы была
[06:00.580 --> 06:07.180]  сходимость константия. Вот. И, соответственно, вот такой вот отдельный случай называется
[06:07.180 --> 06:24.140]  Лемма-Судского. Что если у вас кси-н сходится по распределению кси-тт-н сходится по распределению
[06:24.140 --> 06:33.740]  константия, то тогда у вас последовательность сумм сходится к сумме пределов. Вот так. Ну и то же
[06:33.740 --> 06:49.740]  самое для произведения, верно? Пока все понятно, наверное, да? Отлично. Вот. Что еще мы, наверное,
[06:49.740 --> 06:55.220]  с вами не разобрали? Это Дельта-метод. Надо его немножко повторить, потому что обычно на него
[06:55.220 --> 07:01.300]  первая задача как раз-таки есть. Доказательства мы разбирали с вами на прошлом ДОП-семинаре в
[07:01.300 --> 07:05.380]  прошлом году, на втором. Вот. Поэтому сейчас давай только формулировкой ограничимся.
[07:05.380 --> 07:19.180]  Значит, что нам дано? Нам дано, что у нас есть последовательность
[07:19.180 --> 07:26.780]  случайных векторов, которая сходится по распределению. Нам дана какая-то числовая
[07:26.780 --> 07:34.420]  последовательность, которая сходится к нулю. Вот. А еще нам дана какая-то функция, дифференцируемая
[07:34.420 --> 07:47.580]  в точке А. Вот. Тогда на самом деле верна следующая импликация. Что из этого следует?
[07:47.580 --> 08:08.900]  Сходится так. Ну, в случае векторов, к скалярному произведению градиента а аж в точке А максим.
[08:08.900 --> 08:15.980]  Вот. В принципе, по теории, наверное, это все, что нам нужно, чтобы решить задачу. Ну,
[08:16.020 --> 08:23.500]  еще, наверное, какие-то базовые теоремы нам пригодятся. Там УЗБЧ, что у вас? Если существует
[08:23.500 --> 08:33.660]  математическое ожидание, то выполнена вот такая сходимость. Это вот УЗБЧ. Ну, УЗБЧ тоже выполнена,
[08:33.660 --> 08:38.380]  то есть тут у вас будет сходимость уже по вероятности. Там чуть меньше условий мы накладываем на случайные
[08:38.380 --> 08:44.260]  величины. Вот. И еще есть CPT, что если у вас существует математическое ожидание дисперсия, то
[08:44.260 --> 08:57.020]  верна вот такая сходимость по распределению. Да, думаю, это все помнит более-менее.
[08:57.020 --> 09:06.820]  Итак. Какой план решения задач в данном разделе? Вы берете изначально какую-то сходимость из
[09:06.820 --> 09:11.700]  классических теорем, из сходимости почти наверное. Ну, сходимость почти наверное можно получить из
[09:11.700 --> 09:17.580]  УЗБЧ, сходимость по вероятности можно получить из УЗБЧ, сходимость по распределению можно
[09:17.580 --> 09:25.820]  получить из CPT. И затем, применяя теорему непрерывности, лему Слуцкого или Дельта-метод,
[09:25.820 --> 09:30.220]  получаете сходимость той страшной функции, той страшной последовательности случайных величин,
[09:30.220 --> 09:50.140]  которых вам требуется найти. План понятен, да? Ну, давайте задачу решим. Мы ее до конца доводить не
[09:50.140 --> 09:55.260]  будем, потому что она достаточно тяжелая, в том смысле, что там вычислять достаточно много.
[09:55.260 --> 10:10.060]  Мы с вами очертим план, доведем до какого-то этапа и бросим ее. Итак, задача первая из нашего
[10:10.060 --> 10:28.540]  сегодняшнего варианта. Итак, пусть у нас есть выборка из гамма распределения с параметрами
[10:28.540 --> 10:54.620]  от это. Вот, рассматриваем вот такие выражения, z это у нас будет средняя квадратов. Вот, и нас
[10:54.620 --> 11:07.020]  спрашивают. Еще мы рассматриваем вот такое выражение. Вот, и, собственно, вопрос, к чему у нас
[11:07.020 --> 11:15.740]  сходится. Так, здесь просили не писать. Вопрос, к чему сходится вот такая последовательность
[11:15.740 --> 11:32.380]  случайных величин. Так, давайте, чтобы какой-то контакт с вами поддерживать, как будем решать? Да,
[11:32.380 --> 11:40.340]  t это z минус y квадрат поделить на y. Вот y у нас определен, вот у нас определен z. x1 xn это просто
[11:40.340 --> 11:45.380]  случайный величин, имеющий вот такое распределение. Вот, и спрашивают, есть у нас вот такая
[11:45.380 --> 11:54.460]  последовательность случайных величин, к чему она будет сходиться. Хорошо, значит, мы будем
[11:54.460 --> 12:00.460]  использовать CPT и дельта метод. Действительно, встает первый вопрос, какой CPT мы будем использовать,
[12:00.460 --> 12:08.780]  для случайных величин или для векторов? Действительно, мы будем использовать его для векторов,
[12:08.780 --> 12:13.580]  потому что CPT для векторов всегда верен, а если вы запишете CPT сначала для вот этой
[12:13.580 --> 12:18.620]  случайной величины, а потом для этой, вы получите две сходимости по распределению, так ведь? Но из
[12:18.620 --> 12:24.900]  них, если вы составите вектор, у вас сходимости не будет вектора. Вот, поэтому, действительно,
[12:24.900 --> 12:34.420]  первый шаг, это необходимо применить CPT для случайных векторов. Вот, ну давайте запишем просто это
[12:34.420 --> 12:43.420]  как-нибудь. Там у нас сначала идет среднее, ну вот у нас по сути записано YZ, такой вот векторочек,
[12:43.420 --> 12:59.060]  минус их математические ожидания. Вот, и это все сходится под CPT к нормальному гауссовскому
[12:59.060 --> 13:09.780]  вектору с нулевым вектором средних и с матрицей к вариации. Так, этот шаг всем понятен. Просто
[13:09.780 --> 13:16.180]  записали с вами CPT для векторов. Отлично. Давайте поймем, как у нас выглядят элементы вот этой
[13:16.180 --> 13:28.300]  матрицы и вот этим от ожидания. Как можно посчитать математическое ожидание? Да.
[13:29.060 --> 13:38.660]  А если вам не разрешат? У нас просто очень хороший семинарист. Наверное, разрешит. Спросим у
[13:38.660 --> 13:54.940]  него потом. Так, ладно. Значит, если что, можно это по определению посчитать. Ну да, это на самом
[13:54.940 --> 13:59.500]  деле несложно получается. Просто давайте кому-то просто могут не дать справочную информацию,
[13:59.500 --> 14:02.460]  поэтому один интеграл посчитаем, все остальные будут читаться аналогично.
[14:02.460 --> 14:29.700]  Вот. Как такой интегральчик посчитать? Вот это, если что, я записал плотность гамма распределения.
[14:29.700 --> 14:34.260]  У гамма распределений несколько параметризаций, у них плотности немножко могут видом отличаться,
[14:35.020 --> 14:38.500]  но в целом они все выглядят примерно вот таким образом. Вот я записал
[14:38.500 --> 14:43.980]  определение математического ожидания в абсолютно непрерывном случае. Как такой интеграл взять?
[14:43.980 --> 14:59.380]  Ну да, это правда. Нужно сделать одну замену переменных. Наверное, вот такую. И мы тогда
[14:59.380 --> 15:07.260]  с вами получим какой интеграл? Здесь смотрите у вас, по сути, х в степени k на θ в степени k,
[15:07.260 --> 15:20.620]  то здесь будет t в степени k, e в степени минус t, гамма k. Вот. И еще из-под дифференциала у нас
[15:20.620 --> 15:35.300]  вылезет с вами, да? dx это у нас dt на θ. Здесь у нас тепь вылезает. Чему равен интеграл 600?
[15:35.300 --> 15:43.860]  Вот смотрите, вот это это просто определение гамма функции, если вы знаменатель тоже можете вынести,
[15:43.860 --> 15:53.420]  это же по сути константа. А это у вас определение просто интеграл от гамма функции. Ну это вот
[15:53.420 --> 16:05.740]  просто определение гамма. И чему оно равно? Ну в определении там k минус 1, поэтому на самом деле
[16:05.740 --> 16:17.340]  вот эта функция это будет гамма от k плюс 1, поэтому у вас получится от этого. Вот. Ну и все, все
[16:17.340 --> 16:23.140]  аналогично. У вас вот тут, допустим, нужно будет в отождание квадрата посчитать. Все такие интегралы
[16:23.140 --> 16:28.180]  считаются одним и тем же способом, вы просто приводите гамма функции к определению гамма
[16:28.180 --> 16:35.300]  функции. Да. И все далее понятно. То есть вы можете найти теперь в отождании x1, в отождании x1 в квадрате,
[16:35.300 --> 16:38.620]  давайте сейчас обсудим, как найти элементы каваритационной матрицы.
[16:38.620 --> 16:51.060]  Так, что является элементами каваритационной матрицы?
[16:51.060 --> 17:05.540]  Ну да, на самом деле здесь у вас матрица 2 в нашем случае будет. Здесь будет дисперсия просто x1,
[17:05.540 --> 17:15.460]  здесь будет дисперсия x1 в квадрате, а здесь будет кавариация x1 x1 в квадрате. Ну здесь по сути
[17:15.460 --> 17:34.380]  тоже самое. В силу симметричности каваритации. Вот это? По-моему вот k на k плюс 1 в квадрате. Вот. Вот
[17:34.380 --> 17:40.020]  это у нас k на 5 получилось. Вот. Каваритационную матрицу мы с вами досчитывать не будем, но просто
[17:40.020 --> 17:45.540]  поймем с вами. Дисперсия это в принципе тоже у вас раскладывается на моменты, кавариация у
[17:45.540 --> 18:00.900]  вас раскладывается на моменты. То есть вот кавариация по определению. Да. Как считать вот
[18:00.900 --> 18:04.900]  мотождания, которые входят в кавариацию? Мы только что с вами обсудили, просто приводим там как-то в
[18:04.900 --> 18:15.380]  интеграле гамма-функции. Все элементы этой матрики они вычислимы. Все окей, да? То есть мы с вами
[18:15.380 --> 18:20.540]  записали по сути цпт и посчитали просто какие элементы стоят вот ну чему равному от ожидания и
[18:20.540 --> 18:36.740]  чему равна матрицей кавариации. Отлично. Следующий шаг какой? Действительно. Теперь мы хотим
[18:36.740 --> 18:40.580]  воспользоваться дельта методом. Смотрите, чтобы дельта метод получился, у нас должна быть сходимость
[18:40.580 --> 18:48.260]  по распределению. Должна быть какая-то числовая последовательность, которая сходится к нулю и
[18:48.260 --> 18:53.020]  должна быть какая-то дифференцируемая в точке а функция. Ну и точку а надо выбрать. Давайте,
[18:53.020 --> 19:00.220]  что мы делаем с этим? Во-первых, последовательность сходящейся по распределению у нас уже есть. Вот,
[19:00.220 --> 19:08.060]  мы ее из цпт только что с вами получили. Это первый шаг. Второй вопрос. Какую последовательность bn возьмем?
[19:08.060 --> 19:14.820]  Во всех таких задачах берется один на корень из n, чтобы у вас вот с этим корень из n убилась.
[19:14.820 --> 19:30.300]  Отлично. Давайте ее как-нибудь обозначим. Пусть она будет от двух переменных ab. То есть она у вас
[19:30.300 --> 19:47.820]  будет выглядеть. Если первый у нас y, первый это a, b-a в квадрате на a. С этим согласны, да? Отлично.
[19:47.820 --> 19:52.740]  Вот и точка a. Осталось еще выбрать точку a. Какую точку возьмем?
[20:00.300 --> 20:09.180]  Да, точка a. Так, у нас колидия сейчас произошла. Давайте здесь функция будет от b, от cb.
[20:09.180 --> 20:22.380]  Аккуратней будет. Потому что точка a у нас как бы в определении теоремы есть. Вот, и мы в качестве этой
[20:22.380 --> 20:31.860]  точки рассматриваем как раз таки вот. Вектор математических нужданий. Вот. Ну и все. На самом
[20:31.860 --> 20:37.660]  деле все, что осталось, это принять как раз таки дельта метод. То есть записать вот эту сходимость.
[20:37.660 --> 20:43.580]  Мы получим на самом деле с вами ответ. Давайте аккуратно запишем. Так, условия дельты метода нам
[20:43.580 --> 21:06.180]  уже не нужны. Вот. То есть применяем нашу функцию. Так, давайте по порядку. Значит,
[21:06.180 --> 21:15.580]  давайте с того, что внутри начнем. Ксиенно bn у нас это что такое? Это по сути вот бескорное вот
[21:15.580 --> 21:21.260]  это выражение. Мы к нему добавляем a, у нас мотожи убиваются. И просто h применяем по сути вот к
[21:21.260 --> 21:29.420]  этому вектору. Ну то есть получаем с вами t. Это вот мы первое слагаемое в числителе только что
[21:29.420 --> 21:41.900]  посчитали. h в точке a чему равно? Да, подставляем вот сюда нашим от ожидания, который мы посчитали.
[21:41.900 --> 21:46.100]  Но на самом деле там действительно получится тета. Аккуратно если проделаете, там получится вот
[21:46.100 --> 21:52.740]  тот тета, который у нас вот в условии. Затем мы это все делим на bn, ну то есть делим на 1 на корень
[21:52.740 --> 22:00.620]  bn, то есть умножаем на корень bn. Вот. И все это по распределению сходится вот к нашему нормальному
[22:00.620 --> 22:15.700]  распределению. Да. И градиент нужно не забыть. В точке a. То есть как у нас будет выглядеть градиент.
[22:15.700 --> 22:37.740]  D по dc. Так давайте посчитаем. 1 на c, то есть будет минус b на c квадрате. А там минус c получается.
[22:37.740 --> 22:48.860]  Минус 1. Не обманул вроде, да? Это первая частная производная. Вторая частная производная,
[22:48.860 --> 23:05.900]  которая нам понадобится по b. Тут одно слагаемое, тут очень просто. Просто будет 1c. Да. Подставь.
[23:05.900 --> 23:11.140]  Если ты тут посчитаешь математическое ожидание, у тебя вот здесь будет k тета. Вот здесь у тебя
[23:11.140 --> 23:20.580]  получится k на k плюс 1 тета в квадрате. Вот. И давай просто поставим вот сюда. Что у тебя получится b?
[23:20.580 --> 23:27.940]  b это у тебя по сути вот это минус вот это в квадрате у тебя получится просто k тета в квадрате. И
[23:27.940 --> 23:32.820]  поделить на k тета у тебя получится тета. Ну аккуратно проделай, просто поставь вместо
[23:32.820 --> 23:40.420]  c и b значение нашего вектора. И у тебя действительно получится тет. То есть в этой задаче в некотором
[23:40.420 --> 23:50.740]  смысле пронзгон произошел. Ну это в условии. Ну обычно делают так специально. Вот. И так градиент
[23:50.740 --> 23:58.180]  мы с вами получили. И получается вот предельный закон распределения у нас будет скалярное
[23:58.180 --> 24:05.140]  произведение нормального гауссовского вектора и нашего градиента в точке математического ожидания.
[24:05.140 --> 24:15.020]  Вот. Ну то есть на самом деле здесь у вас поменяется только матрица кавариаций. Итоговая матрица
[24:15.460 --> 24:24.500]  у вас будет выглядеть следующим образом. Это будет градиент вашей функции. Вот
[24:24.500 --> 24:40.900]  который мы с вами посчитали. В точке a транспонированный. Так. Вопросы?
[24:45.020 --> 24:51.620]  Ну смотрите. У нас есть. Мы можем умножать на константу нормальной случайной величины. Помните?
[24:51.620 --> 24:57.740]  У вас грубо говоря на квадрат увеличивается дисперсия и умножается там средняя. С гауссовскими
[24:57.740 --> 25:01.900]  векторами в многомерном случае происходит то же самое. У вас получается умножается вектор средних
[25:01.900 --> 25:06.620]  на вот этот вектор, который вы подставляете. А матрица кавариаций не на квадрат умножается
[25:06.620 --> 25:22.380]  вот таким вот образом. Понятно, да? Ну да. То есть это многомерный аналог тех свойств,
[25:22.380 --> 25:37.340]  которые у нас были для обычного нормального распорядления. Здесь вот градиент. Вопрос
[25:37.340 --> 25:51.860]  еще раз. Вот мы его записали вот здесь. Мы посмотрели вот на это. Поняли,
[25:51.860 --> 25:59.220]  что вот то, что внутри находится. Да, вот это это h от y z. А h мы положили вот таким вот образом.
[25:59.220 --> 26:10.700]  Получается мы ровно получили t по определению. Справедливо? Вот. Потом мы посмотрели,
[26:10.700 --> 26:20.820]  что такое h в точке a. Мы поняли, что это t. Намножили на b. Ну поделили на bn. Ну то есть
[26:20.820 --> 26:35.820]  умножили на корень z. Вот смотри, у нас проще найти вот такую исходимость. Так ведь,
[26:35.820 --> 26:44.060]  где вот у тебя тета должно здесь получиться. Ты из твоей статистики вычитаешь тета. Вот у нас
[26:44.060 --> 26:47.020]  тета вот здесь появляется. Смотри, мы посчитали с тобой математическое ожидание вот этой штуки.
[26:47.020 --> 26:53.380]  Математическое ожидание ну просто x1. Получили вот. А теперь давай просто вот к такому вектору из
[26:53.380 --> 27:03.060]  вот таких двух чисел составленных применим нашу функцию. То есть вот h в точке a. h у тебя определяется
[27:03.060 --> 27:11.780]  вот таким образом. Вместо c подставь k тета. Вместо b подставь k на k плюс 1 тета квадрата. У тебя
[27:11.780 --> 27:20.580]  получается вот честно тета. Справедливо? Вот. Все это сходится к скалярному произведению
[27:20.580 --> 27:36.060]  градиента функции в точке a на вот гауссовский вектор, который у нас был из цепоты. Да,
[27:36.060 --> 27:40.580]  умножение вектора на гауссовскую случайную величину дает все еще гауссовскую случайную
[27:40.580 --> 27:46.620]  величину, но с другой матрицы кавариации. Новая матрица кавариации выглядит вот таким образом.
[27:46.620 --> 28:03.700]  Вот смотри, вот эти значения мы знаем, мы их можем посчитать. Да, а вот это просто градиент в
[28:03.700 --> 28:12.180]  точке a вашей. И все. Градиент вот мы посчитали. Вместо b и c опять подставляйте те значения,
[28:12.180 --> 28:20.180]  которые у нас в точке a. И вот можно в принципе матрицу кавариации досчитать до конца.
[28:20.180 --> 28:28.740]  Ну и все, ответ. Вот такая последовательность случайных величин сходится к гауссовскому
[28:28.740 --> 28:39.180]  вектору вот с такой матрицей кавариации. Итак, идем дальше. Первую задачу мы с вами разобрали.
[28:39.180 --> 28:46.620]  Мораль первого раздела. У вас есть сходимость из ЗБЧ-ЦПТ или УЗБЧ. Вы применяете какие-то
[28:46.620 --> 28:50.900]  теоремы, которые у нас были. Получаете сходимость того, что от вас требуют. Там
[28:50.900 --> 28:55.660]  Лемма Слуцкого может быть еще понадобится. Теорема непрерывности и все такое. Это первая задача.
[28:55.660 --> 28:57.740]  Идем дальше.
[29:25.660 --> 29:45.620]  Отлично. Следующий раздел у нас это статистики и оценки. Сейчас посмотрю на задачу,
[29:45.620 --> 29:51.060]  которая у нас есть. Да, нужно какие-то, наверное, определения сначала дать. Вот у нас есть некоторое
[29:51.060 --> 29:55.340]  выборочное пространство. Что такое выборочное пространство? Ну, по сути, это вот ваше наблюдение.
[29:55.660 --> 30:00.260]  Оно может быть конечным, бесконечным. То есть, это ваша выборка. Давайте считать,
[30:00.260 --> 30:08.620]  что это примерно одно и то же. Сейчас. ХН. Н, возможно, стремится к бесконечности,
[30:08.620 --> 30:14.300]  а, возможно, у вас конечная выборка. Вот. И на самом деле статистика — это любая измеримая функция
[30:14.300 --> 30:29.100]  из X, R. Вот. А оценка — это любая измеримая функция из X, множество тета. Сейчас мы
[30:29.100 --> 30:36.620]  это множество с вами поймем, откуда берется. Значит, статистика — это просто вот какая-то
[30:36.620 --> 30:40.820]  функция от ваших данных, которые вам дали на вход. Среднее, максимальное значение,
[30:41.140 --> 30:47.140]  минимальное значение. Может быть, сумма кубов. Неважно. Это все статистики какие-то. Медиана.
[30:47.140 --> 30:53.060]  Вот. Дальше у вас появляется понятие параметрического семейства. Мы дальше
[30:53.060 --> 30:59.140]  с вами занимаемся параметрической статистикой. То есть, у вас есть какое-то семейство распределений,
[30:59.140 --> 31:13.500]  как красивое. Вот. Все они, грубо говоря, имеют одинаковую плотность или одинаковую
[31:13.500 --> 31:17.060]  функцию распределения, а просто с разными параметрами. Вот. Ну и пример этой штуки — ну,
[31:17.060 --> 31:24.780]  допустим, семейство всех нормальных распределений. Семейство всех нормальных распределений
[31:24.780 --> 31:31.500]  параметризуется параметрами a и sigma2. Вот. Какие ограничения на a накладываются?
[31:31.500 --> 31:42.180]  Нет. А у нас любые как раз таки. А вот дисперсия должна быть строго положительной. Вот. Вот,
[31:42.180 --> 31:52.420]  допустим, параметрическое семейство нормальных распределений. Вот. Теперь давайте поймем
[31:52.420 --> 31:59.940]  качественное отличие между статистикой и оценкой. Вот, смотрите, вы, допустим, хотите в такой модели
[31:59.940 --> 32:06.980]  понять, какой параметр лучше всего подходит вам на должность sigma2. То есть, как бы вы не знаете,
[32:06.980 --> 32:11.260]  из какого распределения именно пришли ваши данные, но вы знаете, что оно нормально с какими-то
[32:11.260 --> 32:15.460]  параметрами. И вот задача параметрической статистики вообще — это подобрать такие самые лучшие в
[32:15.460 --> 32:23.260]  некотором смысле параметры. Вот. Оценка — это что такое? Оценка — это вот какая-то измеримая
[32:23.260 --> 32:27.940]  функция от ваших данных, которая возвращает вам валидное значение. Допустим, если вы захотите в
[32:27.940 --> 32:36.180]  нормальной модели оценить дисперсию чем-нибудь, что может быть отрицательным, это не дисперсия,
[32:36.180 --> 32:40.660]  потому что у вас дисперсия не может быть отрицательная. Так ведь? То есть, чем оценка
[32:40.660 --> 32:44.260]  отличается от статистики, оценка принимает только те значения, которые вот можно взять,
[32:44.380 --> 32:47.020]  подставить вместо параметров, и у вас получится нормальное распределение. Ну,
[32:47.020 --> 32:55.420]  хорошее распределение. Существующее. Допустим, если вы скажете, что дисперсия — это минус модуль
[32:55.420 --> 33:00.500]  x1, вот это — это измеримая функция от выборки, так ведь? То кажется, что это какая-то лажа,
[33:00.500 --> 33:05.460]  потому что у вас дисперсия отрицательной быть не может. Это какая-то статистика, но не оценка.
[33:05.460 --> 33:14.740]  Так, с этим разобрались. Дальше, когда мы ввели с вами понятие оценок, у нас появляются какие-то
[33:14.740 --> 33:22.620]  свойства, у этих оценочек. Вот, свойства основных четыре. Давайте их в виде схемки нарисуем,
[33:22.620 --> 33:32.460]  наверное, чтобы понять не было. Значит, смотрите, первое свойство — это сильная состоятельность.
[33:32.460 --> 33:46.980]  Сильная состоятельность говорит о том, что вот если вы вашу выборку будете увеличивать,
[33:46.980 --> 33:53.620]  значение, которое дает ваша оценка, будет стремиться к истинному значению параметра θ. То есть
[33:53.620 --> 33:57.820]  какое бы вы истинное значение параметра θ не зафиксировали, ну, вот вы не знаете, какое оно
[33:57.820 --> 34:03.700]  истинное, но оно вот какое-то зафиксированное, θ 0. То вот последовательность ваших оценок должна
[34:03.700 --> 34:06.980]  почти наверно стремиться вот к этому истинному значению. То есть, если вы выборку сделаете
[34:06.980 --> 34:13.820]  бесконечной, вы почти наверно получите то значение, которое настоящее. Вот. Дальше есть
[34:13.940 --> 34:17.640]  просто состоятельность. Просто состоятельность — это вот чуть более слабое условие,
[34:17.640 --> 34:27.820]  у вас сходимость не почти наверная оценок, а по вероякостью. Ну и кажется,
[34:27.820 --> 34:36.480]  понятно, как эти два сло freshmen с собой связаны. По-моему, очевидно, да, почему? Просто из-за того,
[34:36.480 --> 34:41.140]  что сходимость почти наверной сильнее, чем сходимость по вероякостям. И сходимость почти
[34:41.140 --> 34:47.460]  наверное, следует сходимость по вероятности. Вот. Дальше есть свойство ассинтетической нормальности.
[34:47.460 --> 34:56.340]  Записывается оно следующим образом.
[34:56.340 --> 35:17.140]  То есть, грубо говоря, последовательность ваших оценок при бесконечно растущем размере выборки
[35:17.140 --> 35:24.180]  находится вот в районе истинного значения параметра и имеет какое-то нормальное распределение.
[35:24.180 --> 35:35.260]  Вот. Есть утверждение о том, что из-за синтетической нормальности оценки следует состоятельность.
[35:35.260 --> 35:52.980]  Как это доказывать, кто не узнает? Не стесняйтесь, ребята. Так, ну ладно. Ну,
[35:52.980 --> 36:02.100]  давайте просто лему Слуцкого с вами применим вот здесь. Рассмотрим просто один на корень
[36:02.100 --> 36:06.980]  из n. Это сходится к нулю, так ведь? Ну, это как числовая последовательность сходится. Ну,
[36:06.980 --> 36:12.100]  на самом деле и по распределению тоже сходится. И рассмотрим вот нашу вот эту последовательность.
[36:12.100 --> 36:16.100]  По лему Слуцкому мы можем взять умножить левую часть вот на левую часть здесь, правую часть на
[36:16.100 --> 36:21.220]  правую здесь, потому что это сходится константе. Справа у вас получится нолик, а слева у вас
[36:21.220 --> 36:29.220]  получится вот. Дальше мы вспоминаем с вами замечательный факт, что сходимость
[36:29.220 --> 36:42.140]  константе по распределению и по вероятности это одно и то же. Так ведь? Вот. Ну, по сути,
[36:42.140 --> 36:48.700]  мы с вами получили то, что там написано. Окей? Вот. То есть из-за синтетической нормальности следует
[36:48.700 --> 36:54.780]  состоятельность отца. Вот. И последнее условие, которое никак, последнее свойство вернее,
[36:54.780 --> 37:02.060]  которое никак с другими свойствами не связано, это несмещенность. Одну секунду, я запишу определение
[37:02.060 --> 37:27.980]  и потом можно будет. Да. Да. То есть, грубо говоря, вот, к любому тетнулевому истинному значению. То есть,
[37:27.980 --> 37:30.940]  у тебя у неизвестной выборки, если она из нормального распределения, есть какие-то истинные
[37:30.940 --> 37:38.700]  значения параметров. Ну вот, а 0 и sigma квадрат 0, который настоящий, но ты их не знаешь. И вот все
[37:38.700 --> 37:43.700]  вот эти свойства, они о чем говорят, что вот какое бы истинное значение не было, последовательность
[37:43.700 --> 37:51.060]  твоих оценок вот здесь сходится к истинному значению. Вот. Здесь то же самое, просто чуть
[37:51.060 --> 37:55.980]  послабже сходимость. Здесь говорит, что у вас в среднем значение вашей оценки дает истинное
[37:55.980 --> 38:12.660]  значение. Вот. Ну а это асимпатическая нормальность. Вот здесь вот? Да, да, да.
[38:25.980 --> 38:36.700]  Нет, как раз-таки ты сначала фиксируешь, что это 0. Не совсем правда. То есть, для любого
[38:36.700 --> 38:42.620]  тета 0 у тебя должна вот такая последовательность сходиться. То есть, ты сначала фиксируешь оценку и
[38:42.620 --> 38:47.620]  потом говоришь, что для любого тета 0 у тебя вот такая сходимость выполняется. Ну иначе смысл не
[38:47.620 --> 38:51.500]  имеет. Смотри, если ты сначала выбираешь тета 0, а потом последовательность оценок, грубо говоря,
[38:51.500 --> 38:57.540]  чтобы выбрать хорошую оценку, ты сначала должен узнать ее настоящее значение. Это глупо немного.
[38:57.540 --> 39:10.980]  У тебя ты придумал какую-то оценку и она для любого тета 0 должна работать. Одна и та же. Вот.
[39:10.980 --> 39:18.140]  По Лемме Слуцкого домножим левую часть на 1 на корень Z. Она по распределению сходит к 0. Справа
[39:18.140 --> 39:31.300]  получается 0 при изведении? Да. Ну и все, по Лемме Слуцкого это выполнено. Вот. Дальше давайте
[39:31.300 --> 39:35.380]  посмотрим, что с этими свойствами происходит, если мы к ним применяем функции. Давайте применим
[39:35.380 --> 39:39.620]  какую-нибудь непрерывную функцию. Что вы можете сказать про вот такую исходимость?
[39:39.620 --> 39:55.300]  Будет ли вот такая исходимость верна? Почему? По теореме непрерывности. То есть мы их на самом
[39:55.300 --> 39:58.900]  деле не зря проходили. То есть если вы возьмете какую-нибудь непрерывную функцию, примените ее
[39:58.900 --> 40:04.580]  к состоятельной последовательности оценок, то новая последовательность оценок будет также
[40:04.580 --> 40:11.340]  состоятельна для новой оценки. То есть мы теперь оцениваем не сам θ0, а функцию от θ0.
[40:11.340 --> 40:18.100]  Пример может быть разберем, если успеем. Вот. Здесь в принципе тоже самое из той же теоремы
[40:18.100 --> 40:33.100]  о непрерывности. Так, это просто по вероятности сходится. Так, что вы можете вот здесь сказать?
[40:35.260 --> 40:39.420]  Будет ли какое-нибудь наследование исходимости, если мы какую-нибудь непрерывную функцию захотим
[40:39.420 --> 41:00.700]  применить? Будет ли аж, вот если мы применим какую-нибудь непрерывную функцию, возможно,
[41:00.700 --> 41:09.740]  дополнительными условиями? Будет ли она асимпатически нормальная? Будет ли вот это
[41:09.740 --> 41:23.820]  к чему-то сходиться по распределению? На самом деле, да. Если аж это непрерывная деференцируемая
[41:23.820 --> 41:29.740]  функция точки, то мы можем с вами применить что? Дельта метод. Если мы применим с вами к этому
[41:29.740 --> 41:34.660]  дельта метод, то у нас как раз таки получится, что вот здесь у вас будет аж ттн со звездочкой,
[41:34.660 --> 41:39.700]  здесь минус аж от θ0. Вот. Осталось только понять, как у вас асимпатическая дисперсия
[41:39.700 --> 41:46.620]  вот здесь изменится. Это будет сходиться к нормальной случайной величине с параметром 0,
[41:46.620 --> 41:53.100]  сигму ттн. Ну и мы там домножали на градиент. Давайте в одномерном случае рассмотрим. Аж
[41:53.180 --> 42:04.820]  ттн. Согласны? То есть мы просто с вами применили дельта метод для функции аж. И вот из этого мы на
[42:04.820 --> 42:14.220]  самом деле получим на 8 асимпатически нормальную оценку. Для уже других параметров. Ну и давайте
[42:14.220 --> 42:20.660]  какими примеры приведем. Как я уже говорил, в подобных задачах у вас есть всегда источник
[42:20.660 --> 42:30.180]  сходимости. Источник сходимости почти наверное это, например, УЗБЧ. Вот. Ну допустим мы хотим с
[42:30.180 --> 42:37.940]  вами оценить параметр среднего в нормальном распределении. Что мы можем с вами сказать? Ну
[42:37.940 --> 42:43.060]  мы знаем с вами, что последовательность вот такая средних. У вас сходится к математическому
[42:43.060 --> 42:51.460]  ожиданию ха, а это а. То есть на самом деле мы можем сказать, что х средняя это сильно состоятельная
[42:51.460 --> 43:04.020]  оценка параметра а. Если вы захотите оценить, допустим, а квадрат, то есть вас не интересует какое
[43:04.020 --> 43:08.620]  значение среднего у вашего распределения, какое значение принимает а в квадрате, то вы можете
[43:08.620 --> 43:13.820]  применить непрерывную функцию и по теореме непрерывности вы получите, что х средняя в квадрате
[43:13.820 --> 43:22.140]  почти наверное сходится к в квадрате. Это просто примеры, которые иллюстрируют, что есть оценки,
[43:22.140 --> 43:26.260]  которые обладают данными свойствами и что при применении непрерывной функции вот это будет
[43:26.260 --> 43:36.260]  выполняться. Вот. А задачу на вот это мы сейчас с вами решим одну. Когда мы с вами еще что-нибудь
[43:36.340 --> 43:40.460]  скажем про оценки статистики, что мы еще хотим сказать с вами?
[43:45.620 --> 43:50.500]  Какие еще статистики вы знаете, кроме вот, допустим, среднего и чего-то такого еще,
[43:50.500 --> 43:55.780]  что-нибудь, какой-нибудь другой класс статистики вы знаете? Порядковые статистики есть, правда?
[43:55.780 --> 44:04.340]  Что-то такое, думаю, все знаете, да? Это просто какой элемент у вас стоит на
[44:04.340 --> 44:11.140]  катом месте в ассортированном ряду. Порядковые статистики есть, вот. Ну, есть еще выборочные
[44:11.140 --> 44:16.180]  статистики, это просто какие-то вот функции от ваших элементов усредняются по выборке.
[44:16.180 --> 44:27.900]  Тоже такой достаточно широкий класс статистики, оценок. А моменты сюда относятся? Моменты
[44:27.900 --> 44:38.300]  тоже сюда относятся, да. То есть это по сути просто сумма X-катых будет, наверное. Это выборочные
[44:38.300 --> 44:43.940]  моменты. Ну вот, ну там понятно, что это будет сходиться к математическому ожиданию X степени K.
[44:47.140 --> 44:54.940]  Вот. В целом, думаю, ничего сложного быть не должно. В этом разделе какие задачки могут быть? Вам
[44:54.940 --> 45:00.460]  могут дать какое-нибудь семейство распределений, допустим, там равномерное на отрезке 0 тета,
[45:00.460 --> 45:05.100]  и дадут какую-нибудь оценку. Такие задачи были в домашней, их было очень много.
[45:05.100 --> 45:15.420]  Какого плана вот тут могут быть задачки? Вообще идейно. Вам дают какие-то оценки,
[45:15.500 --> 45:24.780]  вы их не сами придумываете, вам их просто дают. Вот, допустим, Xn, там X средняя, n плюс 1
[45:24.780 --> 45:40.860]  на минимум, ну на первую порядковую статистику в модели у 0 тета. И вас могут попросить вот в этой
[45:40.860 --> 45:48.140]  задачи понять, какие свойства есть вот у этих оценок. Ну и дальше вам просто нужно по
[45:48.140 --> 45:56.420]  определению проверить те или иные свойства. То есть посчитать вот такое математическое
[45:56.420 --> 46:00.180]  ожидание, чтобы понять, допустим, что вот эта оценка смещенная, вот эта не смещенная,
[46:00.180 --> 46:07.300]  вот эта тоже не смещенная, по-моему, да, не смещенная. Да, потом применить определение,
[46:07.660 --> 46:12.080]  тут применить УЗБЧа, понять, что вот эта оценка сильно состоятельной будет оценкой,
[46:12.080 --> 46:16.660]  вот только там по-моему 2x, вот так должно быть. 2x средних. Поймете, что она сильно
[46:16.660 --> 46:20.120]  состоятельная, допустим. Ну поэтому просто состоятельная. Примените ЦПТ, поймете,
[46:20.120 --> 46:27.140]  что эта оценка еще асимпатически нормальная. Вот. Тут вот будут проблемы с состоятельностью.
[46:27.140 --> 46:35.820]  Дома такая задача была, но при этом оценка не смещенная, но не асимпатически нормальная. Вот.
[46:35.820 --> 46:44.820]  Эта оценка простосостоятельна, даже сильносостоятельна, но не смещенная.
[46:44.820 --> 46:50.820]  Такие задачки должны были быть дома, и задачка на контрольную нормальная.
[46:50.820 --> 46:54.820]  Вам дают какую-то оценку, какую-то модель, и просят сказать, какие у этой оценки есть свойства.
[46:54.820 --> 46:59.820]  Конкретно какую задачу мы в данном разделе рассмотрим?
[46:59.820 --> 47:02.820]  Это задачка на симпатическую нормальность.
[47:02.820 --> 47:27.820]  Я специально не буду стирать то, что нам пригодится сейчас.
[47:27.820 --> 47:32.820]  Она на самом деле очень простая задача.
[47:32.820 --> 47:36.820]  Что вас в ней просят показать?
[47:36.820 --> 47:48.820]  Вас в ней просят показать, что пусть у нас есть выборка x1,xn из равномерного распределения на отрезке θ пополам θ.
[47:48.820 --> 48:11.820]  И просят показать, что статистика является симпатически нормальной оценкой для функций от параметра логарифма Пета.
[48:11.820 --> 48:29.820]  То есть, грубо говоря, вам дали какую-то выборку, вам дали что-то, что нужно оценить, и нужно проверить, что вот объект, который вам дали, обладает некоторым свойством как оценка.
[48:29.820 --> 48:41.820]  Как будем решать?
[48:41.820 --> 48:51.820]  На θ есть ограничение, что θ больше 0.
[48:51.820 --> 48:56.820]  Как будем решать, товарищи?
[48:56.820 --> 49:01.820]  А потом?
[49:01.820 --> 49:06.820]  А потом какую-нибудь неперерывную функцию применим. Ну, дифференцируемую, самое главное.
[49:06.820 --> 49:15.820]  Ну правда, что у нас по CPT получится? У нас по CPT получается, что корень из n и средний минус. Математическое ожидание здесь какое у нас будет?
[49:15.820 --> 49:17.820]  Ну, разный отрезок пополам.
[49:17.820 --> 49:25.820]  3 четверти θ. Сходится к нормальному распределению с параметрами 0. Дисперсия у такого распределения какая?
[49:25.820 --> 49:29.820]  На 12. В квадрате 12.
[49:29.820 --> 49:35.820]  Ну, то есть, θ на 48.
[49:35.820 --> 49:37.820]  θ в квадрате на 48.
[49:37.820 --> 49:45.820]  Так, почему мы такое получили? От ожидания нормального равномерного распределения это просто полусумма концов?
[49:45.820 --> 49:52.820]  Дисперсия это b-a в квадрате поделить на 4. На 12.
[49:52.820 --> 50:00.820]  Это дисперсия.
[50:00.820 --> 50:07.820]  Какая-то вот справочная информация. Это можно либо там через интегральтик посчитать, либо там справочники где-нибудь найти.
[50:07.820 --> 50:10.820]  Отлично. Ну, смотрите, все складывается хорошо.
[50:10.820 --> 50:22.820]  Давайте просто вот воспользуемся теоремой онаследования симпатической нормальности для функции h равной 4 третьих.
[50:22.820 --> 50:30.820]  Логарифм 4 третьих.
[50:30.820 --> 50:38.820]  Сейчас, а почему у нас там 3 четвертых? У нас же θ минус так пополам. Пополам или еще раз? Почему 3 четвертых?
[50:38.820 --> 50:41.820]  Там же θ минус, θ пополам, пополам.
[50:41.820 --> 50:44.820]  Нет, там плюс. А, там плюс.
[50:44.820 --> 50:48.820]  Там плюс.
[50:48.820 --> 50:56.820]  Здесь минуса нет. Это, возможно, я не стер. Извините.
[50:56.820 --> 51:04.820]  Ну, применяем такую функцию. Пользуемся с вами теоремой онаследования симпатической нормальности.
[51:04.820 --> 51:09.820]  Получаем с вами что корень из n.
[51:09.820 --> 51:15.820]  Тут уже можно как бы не париться насчет дельта метода. Можно просто вот воспользоваться этой теоремкой.
[51:15.820 --> 51:23.820]  Логарифм 4 третьих х средняя.
[51:23.820 --> 51:31.820]  Минус логарифм θ. Сходится к нормальному распределению с параметрами 0.
[51:31.820 --> 51:34.820]  θ квадратно 48.
[51:34.820 --> 51:41.820]  Так, производная нашей функции какая?
[51:41.820 --> 51:47.820]  Производная нашей функции это 1 поделить на 4 третьих.
[51:47.820 --> 51:49.820]  И умножаем еще на 4 третьих.
[51:49.820 --> 51:55.820]  Поэтому просто 1 на t.
[51:55.820 --> 52:01.820]  Вот. И умножаем получается на производную в квадрате в точке θ.
[52:01.820 --> 52:10.820]  Производная в квадрате от 1 на t квадрат. То есть будет 1 на t квадрате.
[52:10.820 --> 52:16.820]  Ну и все. Итоговая дисперсия 1,48.
[52:16.820 --> 52:22.820]  Ну это нам просто зло. А не можно доказать, что наоборот оценка не является симпатически нормальной?
[52:22.820 --> 52:28.820]  Такие задачи очень редки на самом деле. И показывать, что нет сходимости гораздо тяжелее, чем то, что она есть.
[52:28.820 --> 52:34.820]  Ну можно привести пример. Вот у нас был такой хороший пример. Я не думаю, что вам что-то сложное дадут. Я таких задач по крайней мере не видел.
[52:34.820 --> 52:40.820]  Ну вот как показать, что такая оценка не является симпатически нормальной? В модели у 0 θ.
[52:51.820 --> 52:54.820]  Ну что вы можете сказать про разность θ минус?
[52:57.820 --> 52:59.820]  При деле 0. А по знаку?
[53:01.820 --> 53:03.820]  Вот эта стучка всегда больше 0.
[53:04.820 --> 53:12.820]  А теперь посмотрите вот сюда. Чтобы у вас была асинтетическая нормальность, у вас как бы, видите, отличие должно быть на знакопеременной. Плюсик, минусик должен быть.
[53:12.820 --> 53:18.820]  Ну типа вот эта штучка должна сходиться к нормальному распределению. А так у вас по сути значения будут только с одной стороны.
[53:18.820 --> 53:26.820]  Вот так будут они у вас располагаться. То есть а плотность у вас, если бы вы сходились к нормальному распределению, у вас вот такой холмин должен быть.
[53:26.820 --> 53:40.820]  А, как еще такие задачки можно решать? Можно просто найти предельный их закон. Вот пользуются функции распределений. То есть можете расписать функцию распределения этой штуки. Это сделать достаточно просто.
[53:40.820 --> 53:52.820]  Вот. И показать, что предельная какая-нибудь такая штука будет сходиться к чему-то не похожему, ну ни на что. Ну то есть чему-то другому. То есть там может быть, задачка тоже дома такая была.
[53:53.820 --> 54:06.820]  А если, допустим, мы сразу не поняли, что она не будет абсолютно нормальной иначе решать этот вопрос? И в каком моменте вообще может быть что-то так? Ну надо уже понять, что оценка не абсолютно нормальная.
[54:06.820 --> 54:20.820]  В какой момент остановиться? Я не знаю, надо задачу смотреть. Я так сходу не могу сказать. Но обычно не дают чего-то такого сложного прям.
[54:20.820 --> 54:40.820]  И просто посмотреть, с чем она будет сходиться. Ну вот, допустим, дома у нас был пример. По-моему, вот эта задача, если не помню.
[54:40.820 --> 54:58.820]  Там просто, если аккуратно пределы расписать, у вас появляется здесь число E. И там получается, что итоговая функция распределения будет E в степени минус C. То есть вы получаете в пределе экспоненциальное распределение.
[54:58.820 --> 55:12.820]  Там какая-то константа вылазит, да.
[55:12.820 --> 55:22.820]  Что мы можем исследовать? Мы можем исследовать, чему сходится функция распределения. Сходимость по распределению – это сходимость функции распределения в каждой точке.
[55:22.820 --> 55:36.820]  То есть мы можем на самом деле исследовать с вами объект. Это функция распределения по распределению.
[55:36.820 --> 55:46.820]  И можно смотреть, к чему это сходится в пределе.
[55:46.820 --> 55:54.820]  Да. Ну то есть если там что-то вычитать, добавлять, можно смотреть, что здесь будет с добавочками, и смотреть, к чему это сходится.
[55:54.820 --> 56:06.820]  Там может быть какая-то нетрибиальная сходимость. Такая дома задача должна была быть. Не должно быть чего-то такого на контрольной. Если попадется, ну F.
[56:06.820 --> 56:16.820]  И в чем мы приходим к экспонению? Почему это сходится в пункте распределения? А почему она должна сходиться, чтобы мы поняли, что отзакон есть?
[56:16.820 --> 56:30.820]  Какой-то в одной из задачах там получилась, что вот такая последовательность или похожая сходилась. Просто там получалось число E в степени C, какой-то константой.
[56:30.820 --> 56:40.820]  Ну и вот это все сходится к E в степени C. Ну и там еще X, по-моему, тоже. То есть E в степени C-X. В общем, это сходилось к экспоненциальному распределению.
[56:40.820 --> 56:48.820]  Да. Ну проблема в том, что у тебя в пределе получалось не нормальное распределение, а экспоненциальное.
[56:48.820 --> 56:56.820]  Я задачку скину потом, хорошо? Давай сейчас не будем на ней зацикливаться.
[56:56.820 --> 57:04.820]  Так, по данному разделу есть вопросы? Давайте резюме небольшое.
[57:04.820 --> 57:12.820]  В данном разделе вам скорее всего дадут какую-нибудь выборку из какого-нибудь параметрического семейства и дадут какую-нибудь оценку.
[57:12.820 --> 57:20.820]  Ее придумают уже за вас и попросят просто проверить ее на свойства. Вот. Ну все, с вторым разделом разобрались. Идем дальше.
[57:26.820 --> 57:30.820]  В чем основная проблема сейчас у нас?
[57:37.820 --> 57:41.820]  Да, дают мы их сами не придумываем, а еще мы должны у них свойства проверять.
[57:41.820 --> 57:49.820]  Было бы очень круто, если бы у нас были какие-то универсальные методы, которые бы работали всегда вне зависимости от того, какая у нас модель.
[57:49.820 --> 57:54.820]  Ну более-менее всегда там при выполнении каких-то условий регулярности.
[57:54.820 --> 57:58.820]  Вот. И чтобы они сразу обладали какими-то хорошими свойствами.
[58:02.820 --> 58:10.820]  Вот. И дальше мы с вами рассмотрим три таких метода построения оценок, и задачка будет там ими всеми тремя построить оценки.
[58:14.820 --> 58:17.820]  Итак, методы построения оценок.
[58:24.820 --> 58:34.820]  То есть нам нужна какая-то машинерия, которая нам будет сама давать вот эту измеримую функцию от выборки, которая сразу будет обладать какими-то хорошими свойствами.
[58:34.820 --> 58:38.820]  Вот. Первый метод – это метод моментов.
[58:45.820 --> 58:50.820]  Это частный случай метода подстановки. Что такое метод подстановки мы сейчас обсуждать не будем?
[58:50.820 --> 58:55.820]  А в самом таком классическом случае вы решаете вот такую систему уравнений.
[59:12.820 --> 59:14.820]  И так далее.
[59:15.820 --> 59:17.820]  И так далее.
[59:18.820 --> 59:24.820]  Вы составляете систему уравнений, в которых приравниваете теоретические моменты и эмпирические моменты.
[59:26.820 --> 59:30.820]  Вот. Ну вот это понятно выражается каким-то образом через параметры, так ведь?
[59:31.820 --> 59:34.820]  А правая часть – это вот какие-то у вас функции.
[59:34.820 --> 59:36.820]  И на самом деле таким образом вы можете получить оценку.
[59:36.820 --> 59:38.820]  Давайте сразу вот пример.
[59:38.820 --> 59:40.820]  Самый простенький, самый банальный.
[59:40.820 --> 59:43.820]  Пусть у вас N A sigma квадрат.
[59:43.820 --> 59:47.820]  И вы хотите по методу моментов оценить параметры.
[59:48.820 --> 59:50.820]  Что в методе моментов нужно сделать?
[59:50.820 --> 59:52.820]  Ну, нужно составить систему уравнений.
[59:52.820 --> 59:55.820]  Теоретические моменты приравнять к эмпирическим.
[59:55.820 --> 01:00:02.820]  Теоретический момент первый – это просто A средняя приравниваемая к среднему.
[01:00:02.820 --> 01:00:04.820]  Вот.
[01:00:04.820 --> 01:00:06.820]  Второй момент чему равен?
[01:00:11.820 --> 01:00:13.820]  Так, ну давайте с вами вспомним.
[01:00:13.820 --> 01:00:16.820]  Дисперсия – это sigma в квадрате.
[01:00:18.820 --> 01:00:21.820]  dx равняется ex в квадрате.
[01:00:21.820 --> 01:00:24.820]  Минус ex в квадрате.
[01:00:24.820 --> 01:00:26.820]  То есть если мы это сюда перенесем?
[01:00:26.820 --> 01:00:28.820]  Сумма.
[01:00:28.820 --> 01:00:30.820]  Sigma квадрат плюс A квадрат.
[01:00:30.820 --> 01:00:34.820]  Это выборочный второй момент.
[01:00:34.820 --> 01:00:36.820]  Понятно, да?
[01:00:36.820 --> 01:00:38.820]  Количество уравнений у вас будет совпадать с количеством параметров.
[01:00:38.820 --> 01:00:40.820]  Ну, чтобы все хорошо было.
[01:00:40.820 --> 01:00:41.820]  Вот.
[01:00:41.820 --> 01:00:43.820]  Решаете такую систему.
[01:00:43.820 --> 01:00:45.820]  Если у нее решение единственное, и там все хорошо,
[01:00:45.820 --> 01:00:48.820]  то вы получили оценку по методу моментов.
[01:00:50.820 --> 01:00:54.820]  Отсюда в частности следует, чтобы вы получили оценку по методу моментов.
[01:00:54.820 --> 01:00:57.820]  Отсюда в частности следует, что A это у нас эксредняя,
[01:00:57.820 --> 01:01:00.820]  а sigma в квадрате мы оцениваем как
[01:01:00.820 --> 01:01:04.820]  минус x в квадрате.
[01:01:04.820 --> 01:01:06.820]  Понятно, да?
[01:01:08.820 --> 01:01:10.820]  Составляем стенку и решаем.
[01:01:10.820 --> 01:01:12.820]  То есть вот здесь мы просто вместо A квадрат
[01:01:12.820 --> 01:01:14.820]  подставили оценку A
[01:01:14.820 --> 01:01:16.820]  и получили вот то, что нужно было нам.
[01:01:18.820 --> 01:01:20.820]  Есть распределение циклов.
[01:01:20.820 --> 01:01:21.820]  Есть, конечно.
[01:01:21.820 --> 01:01:23.820]  Можно сколько угодно параметров поделить.
[01:01:27.820 --> 01:01:30.820]  Может быть еще раз мы получили sigma в квадрате с квадратом.
[01:01:30.820 --> 01:01:32.820]  Откуда мы не оцениваем?
[01:01:32.820 --> 01:01:34.820]  Вот это откуда мы знаем?
[01:01:34.820 --> 01:01:35.820]  Еще раз.
[01:01:35.820 --> 01:01:36.820]  Вот это теоретический момент.
[01:01:36.820 --> 01:01:37.820]  От ожидания.
[01:01:37.820 --> 01:01:39.820]  Ты знаешь, чему равно вот такое распределение?
[01:01:41.820 --> 01:01:42.820]  Ну, A.
[01:01:42.820 --> 01:01:43.820]  А.
[01:01:43.820 --> 01:01:45.820]  А чему второй момент равен?
[01:01:45.820 --> 01:01:46.820]  Ну, хорошо.
[01:01:46.820 --> 01:01:48.820]  Второй момент может быть равен.
[01:01:48.820 --> 01:01:49.820]  Ну, хорошо.
[01:01:49.820 --> 01:01:51.820]  Второй момент можно посчитать явно.
[01:01:51.820 --> 01:01:53.820]  То есть можно посчитать интеграл от x в квадрате
[01:01:53.820 --> 01:01:55.820]  на плотность x dx.
[01:01:55.820 --> 01:01:56.820]  Такую штуку.
[01:01:56.820 --> 01:01:57.820]  Можно ее найти.
[01:01:57.820 --> 01:01:59.820]  А можно вспомнить, что дисперсия на самом деле
[01:01:59.820 --> 01:02:01.820]  это у нас мотождание в квадрате минус
[01:02:01.820 --> 01:02:03.820]  мотождание квадратов
[01:02:03.820 --> 01:02:05.820]  минус квадрат мотождания.
[01:02:05.820 --> 01:02:06.820]  Вот.
[01:02:06.820 --> 01:02:08.820]  И мы знаем, что дисперсия вот в такой модели
[01:02:08.820 --> 01:02:10.820]  это sigma в квадрат.
[01:02:10.820 --> 01:02:12.820]  И отсюда мы просто получили чему равняется.
[01:02:15.820 --> 01:02:17.820]  Второй момент.
[01:02:17.820 --> 01:02:19.820]  Это такой классический трюк, типа
[01:02:19.820 --> 01:02:21.820]  что вы берете и выражаете через дисперсию,
[01:02:21.820 --> 01:02:23.820]  потому что дисперсия это табличные данные.
[01:02:23.820 --> 01:02:25.820]  Там на Википедии английская она есть, англоязычный.
[01:02:25.820 --> 01:02:27.820]  А мотождание тоже есть.
[01:02:27.820 --> 01:02:29.820]  И в принципе можно легко получить оценку методом моментов.
[01:02:31.820 --> 01:02:33.820]  Всегда ли существует оценка методом моментов?
[01:02:35.820 --> 01:02:37.820]  Ну, у зрения каши у них будет оценка методом моментов.
[01:02:37.820 --> 01:02:38.820]  Какой?
[01:02:38.820 --> 01:02:40.820]  Мотождание.
[01:02:40.820 --> 01:02:42.820]  Да, вот давайте такой пример рассмотрим.
[01:02:42.820 --> 01:02:44.820]  Утверждается, что для распределения каши
[01:02:44.820 --> 01:02:46.820]  вот такой классический метод моментов
[01:02:46.820 --> 01:02:47.820]  не сработает.
[01:02:47.820 --> 01:02:48.820]  Ну да, не сработает.
[01:02:48.820 --> 01:02:50.820]  Если у вас не определены мотождания,
[01:02:50.820 --> 01:02:52.820]  то вы, грубо говоря, такое уравнение составить не можете.
[01:02:52.820 --> 01:02:54.820]  Ну вот, слева нечего поставить.
[01:02:54.820 --> 01:02:56.820]  Но есть обобщенный метод моментов.
[01:02:58.820 --> 01:03:00.820]  Это когда вы составляете уравнения
[01:03:02.820 --> 01:03:04.820]  от каких-то функций.
[01:03:12.820 --> 01:03:14.820]  Так, тут у нас будет просто нет.
[01:03:16.820 --> 01:03:18.820]  Вот.
[01:03:18.820 --> 01:03:20.820]  То есть, вот этот момент момента,
[01:03:20.820 --> 01:03:22.820]  в котором...
[01:03:22.820 --> 01:03:24.820]  Это частный случай,
[01:03:24.820 --> 01:03:26.820]  когда у вас g1
[01:03:26.820 --> 01:03:28.820]  это просто х,
[01:03:28.820 --> 01:03:30.820]  g2
[01:03:30.820 --> 01:03:32.820]  это х в квадрате
[01:03:32.820 --> 01:03:34.820]  и так далее.
[01:03:34.820 --> 01:03:36.820]  Это называется стандартные пробные функции.
[01:03:36.820 --> 01:03:38.820]  На самом деле, сюда можно любые там
[01:03:38.820 --> 01:03:40.820]  баррельские функции подставлять
[01:03:40.820 --> 01:03:42.820]  и вот что-то подобное решать.
[01:03:42.820 --> 01:03:44.820]  То есть, если у вас есть
[01:03:44.820 --> 01:03:46.820]  что-то подобное подставлять
[01:03:46.820 --> 01:03:48.820]  и вот что-то подобное решать.
[01:03:48.820 --> 01:03:50.820]  Так вот, утверждается, что вот таким обобщенным методом моментов
[01:03:50.820 --> 01:03:52.820]  можно найти оценку для распределения каши,
[01:03:52.820 --> 01:03:54.820]  если в качестве
[01:03:54.820 --> 01:03:56.820]  жера смотреть вот такую функцию.
[01:04:00.820 --> 01:04:02.820]  На семинаре такая задачка должна была быть.
[01:04:06.820 --> 01:04:08.820]  То есть, если чуть-чуть подправить,
[01:04:08.820 --> 01:04:10.820]  сделать так, чтобы ваш интеграл сошелся,
[01:04:10.820 --> 01:04:12.820]  то такую систему можно составить и решить.
[01:04:14.820 --> 01:04:16.820]  Ну, как бы это
[01:04:16.820 --> 01:04:18.820]  вообще так никто не делает, на самом деле.
[01:04:18.820 --> 01:04:20.820]  Это как бы максимум такая задачка может попасться,
[01:04:20.820 --> 01:04:22.820]  что во время распределения каши у вас попросят
[01:04:22.820 --> 01:04:24.820]  найти методом моментов оценку.
[01:04:24.820 --> 01:04:26.820]  Это маловероятно, скорее всего, там удобнее будет чем-нибудь другим действовать.
[01:04:28.820 --> 01:04:30.820]  Вот, но такое тоже есть.
[01:04:32.820 --> 01:04:34.820]  А какими свойствами обладает оценка по методу момента?
[01:04:36.820 --> 01:04:38.820]  Давайте по порядку.
[01:04:44.820 --> 01:04:46.820]  Будет ли она сильно состоятельной?
[01:05:00.820 --> 01:05:02.820]  Видите оценка по методу моментов сильно состоятельной?
[01:05:04.820 --> 01:05:06.820]  Почему?
[01:05:06.820 --> 01:05:08.820]  Она точно будет не смещенной.
[01:05:12.820 --> 01:05:14.820]  Нет, она будет смещенной.
[01:05:18.820 --> 01:05:20.820]  Вот, смотри, вот здесь у нас какая оценка на дисперсе получилась?
[01:05:26.820 --> 01:05:28.820]  Вот эта оценка смещенная.
[01:05:30.820 --> 01:05:32.820]  Ну, это вы знаете, да, наверное?
[01:05:32.820 --> 01:05:34.820]  Это точно должно было быть на семинаре.
[01:05:34.820 --> 01:05:36.820]  Но эта штука смещенная.
[01:05:36.820 --> 01:05:38.820]  И вот вам пример, что метод моментов может
[01:05:38.820 --> 01:05:40.820]  вернуть смещенную оценку.
[01:05:40.820 --> 01:05:42.820]  То есть, еще раз, ни один метод
[01:05:42.820 --> 01:05:44.820]  не смещенность не гарантирует.
[01:05:44.820 --> 01:05:46.820]  Но зато вот сильная состоятельность, допустим, гарантируется.
[01:05:46.820 --> 01:05:48.820]  Но почему?
[01:05:48.820 --> 01:05:50.820]  Потому что, смотрите, у вас же по сути вот эта функция от параметра, да, какая-то?
[01:05:54.820 --> 01:05:56.820]  Ну, как вектор функции
[01:05:56.820 --> 01:05:58.820]  приравниваете m от t
[01:05:58.820 --> 01:06:00.820]  к мат ожиданиям.
[01:06:00.820 --> 01:06:02.820]  Вы знаете, что вот у вас есть
[01:06:02.820 --> 01:06:04.820]  сходимость вот такая, по
[01:06:08.820 --> 01:06:10.820]  тогда, на самом деле, если у вас вот эта вот функция m
[01:06:10.820 --> 01:06:12.820]  она непрерывная,
[01:06:12.820 --> 01:06:14.820]  короче, если она обратимая, то есть там монотонно-непрерывная,
[01:06:14.820 --> 01:06:16.820]  то у вас
[01:06:16.820 --> 01:06:18.820]  по наследованию сходимости
[01:06:18.820 --> 01:06:20.820]  ну, мат ожидания, ну
[01:06:24.820 --> 01:06:26.820]  мне сейчас я аккуратно
[01:06:26.820 --> 01:06:28.820]  напишу.
[01:06:28.820 --> 01:06:30.820]  Это у нас какой-то выборочный момент.
[01:06:32.820 --> 01:06:34.820]  А, сейчас.
[01:06:36.820 --> 01:06:38.820]  Ну давайте вот в общем виде
[01:06:38.820 --> 01:06:40.820]  запишем как что-нибудь среднее.
[01:06:40.820 --> 01:06:42.820]  Так.
[01:06:44.820 --> 01:06:46.820]  Ну, вот, вот, вот.
[01:06:46.820 --> 01:06:48.820]  То есть у вас
[01:06:48.820 --> 01:06:50.820]  сильная состоятельность, она сохраняется.
[01:06:52.820 --> 01:06:54.820]  То есть у вас вот
[01:06:54.820 --> 01:06:56.820]  это сходится почти, наверное, по УЗБЧ.
[01:06:56.820 --> 01:06:58.820]  Если вы обратную функцию накинете
[01:06:58.820 --> 01:07:00.820]  и она там непрерывно дифференцируемая, то у вас
[01:07:00.820 --> 01:07:02.820]  и оценка будет сходиться к этому.
[01:07:02.820 --> 01:07:04.820]  А это у вас на самом деле истинное значение параметра.
[01:07:12.820 --> 01:07:14.820]  Так, давай еще раз.
[01:07:16.820 --> 01:07:18.820]  Давай посмотрим
[01:07:18.820 --> 01:07:20.820]  на те уравнения, которые мы записали.
[01:07:26.820 --> 01:07:28.820]  Ну, можно их записать как некоторые
[01:07:28.820 --> 01:07:30.820]  векторные уравнения, а так ведь?
[01:07:30.820 --> 01:07:32.820]  Сейчас я посмотрю,
[01:07:32.820 --> 01:07:34.820]  как это можно было бы аккуратно сделать.
[01:07:42.820 --> 01:07:44.820]  Мы по сути приравниваем сами
[01:07:44.820 --> 01:07:46.820]  каким-то выборочным функциям.
[01:07:52.820 --> 01:07:54.820]  С этим согласен?
[01:07:54.820 --> 01:07:56.820]  Ну по сути я вот переписал просто вот это
[01:07:56.820 --> 01:07:58.820]  в более общем виде.
[01:07:58.820 --> 01:08:00.820]  Ну, вот это у нас какое-то значение,
[01:08:00.820 --> 01:08:02.820]  ну какая-то функция от параметров,
[01:08:02.820 --> 01:08:04.820]  мы ее обозначим за m1 от theta.
[01:08:04.820 --> 01:08:06.820]  Ну то есть в случае нормальной модели
[01:08:06.820 --> 01:08:08.820]  у нас вот m1 от theta от параметров
[01:08:08.820 --> 01:08:10.820]  мы его обозначим за m1 от theta.
[01:08:10.820 --> 01:08:12.820]  Ну, то есть в случае нормальной модели
[01:08:12.820 --> 01:08:14.820]  у нас вот m1 от theta от параметров
[01:08:14.820 --> 01:08:16.820]  это просто a было.
[01:08:22.820 --> 01:08:24.820]  Ну да, по сути здесь у тебя как бы фигурируют
[01:08:24.820 --> 01:08:26.820]  теоретические значения параметров, какие-то функции от них.
[01:08:26.820 --> 01:08:28.820]  А справа какие-то выборочные
[01:08:28.820 --> 01:08:30.820]  характеристики, которые их приближают.
[01:08:30.820 --> 01:08:32.820]  Вот. У тебя есть сходимость
[01:08:32.820 --> 01:08:34.820]  на самом деле, ладно, вот такая.
[01:08:34.820 --> 01:08:36.820]  То есть вот так у тебя есть сходимость по УЗБЧ.
[01:08:42.820 --> 01:08:44.820]  Согласен?
[01:08:52.820 --> 01:08:54.820]  Ну, на самом деле для
[01:08:54.820 --> 01:08:56.820]  выборочных функций более-менее для всех.
[01:08:58.820 --> 01:09:00.820]  Но если от ожидания у них существует,
[01:09:00.820 --> 01:09:02.820]  то это будет выполнено.
[01:09:04.820 --> 01:09:06.820]  Вот. И дальше в чем идея?
[01:09:06.820 --> 01:09:08.820]  Мы можем, если все хорошо и у нас вот эта
[01:09:08.820 --> 01:09:10.820]  функция обратима, и она непрерывна,
[01:09:10.820 --> 01:09:12.820]  мы можем, грубо говоря, в нашем неравенстве
[01:09:12.820 --> 01:09:14.820]  навесить m в минус первой,
[01:09:14.820 --> 01:09:16.820]  воспользоваться теоремой наследования сходимости.
[01:09:16.820 --> 01:09:18.820]  У тебя получится, что к истинному значению
[01:09:18.820 --> 01:09:20.820]  у тебя сходится m в минус первой.
[01:09:28.820 --> 01:09:30.820]  Тут аккуратно в векторном виде переписать.
[01:09:32.820 --> 01:09:34.820]  Так, тут m1, тут mn.
[01:09:34.820 --> 01:09:36.820]  Согласен?
[01:09:40.820 --> 01:09:42.820]  Ну, давай считать, что да.
[01:09:46.820 --> 01:09:48.820]  На что?
[01:09:50.820 --> 01:09:52.820]  Ну, на m какой-то.
[01:09:54.820 --> 01:09:56.820]  На самом деле это любой функционал нам подходит.
[01:09:56.820 --> 01:09:58.820]  Это частный случай метода подстановки.
[01:09:58.820 --> 01:10:00.820]  Что такое метод подстановки?
[01:10:00.820 --> 01:10:02.820]  У вас есть какой-то функционал?
[01:10:02.820 --> 01:10:04.820]  Вот ожидание – это частный случай.
[01:10:04.820 --> 01:10:06.820]  Вы там считаете вот такую штуку, например, да?
[01:10:08.820 --> 01:10:10.820]  На самом деле можно считать там от чего угодно
[01:10:10.820 --> 01:10:12.820]  такой интеграл. Он может там
[01:10:12.820 --> 01:10:14.820]  существовать или не существовать.
[01:10:14.820 --> 01:10:16.820]  Про что говорит метод подстановки?
[01:10:16.820 --> 01:10:18.820]  Давайте сюда вместо истинного значения
[01:10:18.820 --> 01:10:20.820]  функцию распределения поставим эмпирическую
[01:10:20.820 --> 01:10:22.820]  функцию распределения.
[01:10:26.820 --> 01:10:28.820]  Вот эти штуки будут сходиться,
[01:10:28.820 --> 01:10:30.820]  потому что у вас функция распределения
[01:10:30.820 --> 01:10:32.820]  эмпирически сходится к истинным мусорным распределениям.
[01:10:34.820 --> 01:10:36.820]  Это оценка по методу
[01:10:36.820 --> 01:10:38.820]  подстановки. А это как бы частный случай.
[01:10:38.820 --> 01:10:40.820]  Вот здесь функционал может быть... Ну, грубо говоря,
[01:10:40.820 --> 01:10:42.820]  давайте считать, что это вот взятие
[01:10:42.820 --> 01:10:44.820]  от ожидания, от какой-то функции, от случайной вечны.
[01:10:44.820 --> 01:10:46.820]  И вопрос, а почему мы можем агрантуру кэрсить?
[01:10:46.820 --> 01:10:48.820]  Мы предполагаем,
[01:10:48.820 --> 01:10:50.820]  что такая существует. То есть если она
[01:10:50.820 --> 01:10:52.820]  существует, то все хорошо.
[01:10:54.820 --> 01:10:56.820]  Теорема. Если существует...
[01:10:56.820 --> 01:10:58.820]  В общем, если m – это объекция, она у вас
[01:10:58.820 --> 01:11:00.820]  однозначная функция, то тогда мы можем
[01:11:00.820 --> 01:11:02.820]  налезть обратную и получим,
[01:11:02.820 --> 01:11:04.820]  что у вас была сходимость почти наверная.
[01:11:06.820 --> 01:11:08.820]  И оценки по этим методам
[01:11:08.820 --> 01:11:10.820]  тоже получатся.
[01:11:12.820 --> 01:11:14.820]  Сильно светает.
[01:11:16.820 --> 01:11:18.820]  Прылило?
[01:11:20.820 --> 01:11:22.820]  Какими еще свойствами обладает оценка
[01:11:22.820 --> 01:11:24.820]  методом моментов?
[01:11:24.820 --> 01:11:26.820]  Смотрите, мы доказали,
[01:11:26.820 --> 01:11:28.820]  что... Ну, показали аккуратно,
[01:11:28.820 --> 01:11:30.820]  но не очень аккуратно,
[01:11:30.820 --> 01:11:32.820]  что у нас есть сильная
[01:11:32.820 --> 01:11:34.820]  состоятельность оценки по методу моментов.
[01:11:34.820 --> 01:11:36.820]  Она может быть смещенной.
[01:11:36.820 --> 01:11:38.820]  Вот пример смещенной оценки.
[01:11:38.820 --> 01:11:40.820]  Вот. Но есть еще симпатическая нормальность,
[01:11:40.820 --> 01:11:42.820]  и симпатическая нормальность на самом деле
[01:11:42.820 --> 01:11:44.820]  будет из CPT проследовать.
[01:11:44.820 --> 01:11:46.820]  Смотрите, предположим,
[01:11:46.820 --> 01:11:48.820]  что у вас существует отжидание
[01:11:48.820 --> 01:11:50.820] awn,
[01:11:50.820 --> 01:11:52.820]  то есть,
[01:11:52.820 --> 01:11:55.700]  или, видимо,
[01:11:55.700 --> 01:11:58.820]  после этого
[01:11:58.820 --> 01:12:00.820]  она였NAWork
[01:12:00.820 --> 01:12:05.820]  и уenda
[01:12:05.820 --> 01:12:06.820]  полuccvable type
[01:12:06.820 --> 01:12:10.820]  или
[01:12:10.820 --> 01:12:12.820] ולт报у
[01:12:12.820 --> 01:12:17.700]  предположим, что у вас существует от ожидания и дисперсии в этой стуке, тогда у вас есть корень из-за н,
[01:12:17.700 --> 01:12:22.820]  а в качестве среднего вот вы как раз таки рассматриваете средние ваших функций.
[01:12:22.820 --> 01:12:32.660]  ну вот выборочную характеристику такого вида, вы знаете их истинным от ожидания,
[01:12:32.660 --> 01:12:39.260]  ну вот мы полагаем, что это мхт, и оно в силу цпт сходится к нормальному распределению с
[01:12:39.260 --> 01:12:44.380]  какими-то парами примерно, окей? и все дальше применяем просто теория
[01:12:44.380 --> 01:12:51.620]  о наследовании сходимости, теория о наследовании симпатической нормальности для функции м-1 и мы как
[01:12:51.620 --> 01:13:07.500]  раз с вами получим вот, ну нужно потребовать, чтобы обратная функция была дифференцируема,
[01:13:07.740 --> 01:13:13.100]  объекции, чтобы все хорошо было, но вот таким образом можно показать, что оценка по методу
[01:13:13.100 --> 01:13:26.980]  моментов будет также симпатически нормальной, цпт применяем, просто полагаем, что у наших
[01:13:26.980 --> 01:13:30.100]  функций, которые мы рассматриваем, у них существует математическое ожидание и дисперсия,
[01:13:30.100 --> 01:13:43.220]  окей? вот, то есть оценка по методу моментов, она обладает там вот почти всеми качествами,
[01:13:43.220 --> 01:13:58.420]  которые мы рассмотрели, кроме, кроме несмещенности. Ну в смысле, ровно так же как теория о наследовании
[01:13:58.420 --> 01:14:02.020]  симпатической нормальности у нас была. Если вот эта функция дифференцируема непрерывно,
[01:14:02.020 --> 01:14:07.060]  то по сути по дельта методу мы можем сюда применить м-1. Вот тут у вас как-то дисперсия поменяется,
[01:14:07.060 --> 01:14:26.700]  но сходимость такая останется. Так, ладно, наверное.
[01:14:28.780 --> 01:14:30.580]  Давайте еще оценку по...
[01:14:30.580 --> 01:14:46.100]  оценка методом максимального правдоподобия. Давайте это обсудим еще быстро и потом задачку
[01:14:46.100 --> 01:14:52.900]  решим и сделаем перерыв небольшой. Значит, смотрите, вводится функция правдоподобия.
[01:14:52.900 --> 01:15:17.700]  И полагаем оценка по методу максимального правдоподобия.
[01:15:22.900 --> 01:15:43.740]  Да, мы-то это ищем. Ну это просто обозначение, я мог это вот сюда записать, ладно. Вот. И соответственно,
[01:15:43.740 --> 01:15:51.940]  оценка по методу максимального правдоподобия, это такая оценка, на которой эта штука достигает
[01:15:51.940 --> 01:16:03.900]  своего максимума. Вот. Тут есть классический такой достаточно, не знаю, пример. Вот давайте
[01:16:03.900 --> 01:16:10.660]  рассмотрим все нормальные распределения с параметром, у которых средний неизвестно,
[01:16:10.660 --> 01:16:16.060]  а дисперсия фиксирована. Ну и давайте просто это будет пять. Вот. И, предположим, вам дали
[01:16:16.060 --> 01:16:25.580]  какие-нибудь данные, которые вокруг вот этой точки сгруппированы. Давайте рассмотрим вот какую-нибудь
[01:16:25.580 --> 01:16:33.380]  такую произвольную плотность. Понятно, что вот это значение функции правдоподобия для вот такой
[01:16:33.380 --> 01:16:40.460]  плотности будет маленькое, потому что вот вы берете произведение плотностей вот в этих точках и у вас все
[01:16:40.460 --> 01:16:47.300]  не очень маленькие. Понятно, что в некотором смысле лучше подходит плотность, у которой горбик вот
[01:16:47.300 --> 01:16:56.540]  вокруг этих значений, потому что у вас тогда произведение плотностей будет больше. Ну вот,
[01:16:56.540 --> 01:17:01.660]  смотри. Пусть у тебя все данные лежат вот здесь, все твои наблюдения лежат вот тут. Вот,
[01:17:01.660 --> 01:17:09.180]  вот точки я нарисовал. X1 и так далее, Xn. Пусть они даже упорядочены. Вот. Согласись,
[01:17:09.220 --> 01:17:14.740]  что такая плотность будет иметь достаточно маленькое значение. Вернее, такая функция правдоподобия
[01:17:14.740 --> 01:17:19.820]  будет иметь маленькое значение. Почему? Потому что у тебя, грубо говоря, каждый множитель будет
[01:17:19.820 --> 01:17:28.660]  очень маленьким. Вот. А если мы рассмотрим какую-нибудь вот такую плотность, то ее уже произведение будет
[01:17:28.660 --> 01:17:35.540]  побольше. Да, это мы предположили. То есть вот это не будет оценкой максимального правдоподобия,
[01:17:35.540 --> 01:17:40.340]  а вот это уже больше похоже, потому что вот эти значения, как бы каждый множитель будет больше.
[01:17:40.340 --> 01:17:45.860]  Вот. И вот на таком примере достаточно легко понять, что происходит. У вас есть какие-то
[01:17:45.860 --> 01:17:50.100]  параметры, и вы пытаетесь подобрать такие параметры, чтобы вот они были, чтобы вот это
[01:17:50.100 --> 01:17:55.300]  распределение было максимально похоже на те данные, которые вам дали. Идея понятна, да?
[01:17:55.300 --> 01:18:04.660]  Вот. Встает вопрос, как искать такую оценку? Ну да, первое, что заметим с вами,
[01:18:04.660 --> 01:18:11.980]  у вас функция правдоподобия в принципе не отрицательная, ну на самом деле даже положительная.
[01:18:11.980 --> 01:18:17.700]  Обычно там плотности рассматривают, которые не за нуляются, либо там с индикаторами аккуратно
[01:18:17.700 --> 01:18:24.420]  играются. И логарифм непрерывная функция, поэтому если мы ее применим, с максимумом ничего не
[01:18:24.420 --> 01:18:33.820]  произойдет. Поэтому можно рассмотреть логарифмическую функцию правдоподобия. Вот. Это по сути будет просто
[01:18:33.820 --> 01:18:43.620]  сумма логарифм плотностей. Вот. И уже ее максимизировать. Хорошо, как мы будем искать максимум этой функции?
[01:18:43.620 --> 01:18:52.620]  Вот то, что на облупку это не гарантируется, но обычно в задачах действительно достаточно проверить
[01:18:52.620 --> 01:18:58.540]  только необходимые условия. То есть что производная, под это будет нулевая. Или если многомерный параметр,
[01:18:58.540 --> 01:19:02.940]  то у вас градиент, то есть частные производные, все за нуляются. Чаще всего такая точка и подходит.
[01:19:03.900 --> 01:19:11.620]  Понятно, да? Вот. То есть следующий шаг, если у вас одномерный случай, вы просто дифференцируете по параметру.
[01:19:16.220 --> 01:19:22.140]  В точке экстремума у вас должен быть ноль. Вот. И уже решая вот это уравнение правдоподобия,
[01:19:22.140 --> 01:19:27.900]  находите такую оценку, которая, ну, кандидатно хорошую, и чаще всего она и есть хорошая. Она и есть
[01:19:27.900 --> 01:19:32.420]  оценка максимального правдоподобия. Вот. Тут уже теорема гораздо сложнее, то есть тут так просто
[01:19:32.420 --> 01:19:38.180]  на пальцах не показать, но оценка максимального правдоподобия, она на самом деле состоятельна,
[01:19:38.180 --> 01:19:46.140]  а симпатически нормально. Вот. Ну, еще и эффективно. Но это уже не показать как бы за три минуты. Там
[01:19:46.140 --> 01:20:07.140]  достаточно сложная теорема. Задача. Нет. Хорошо, не всегда. В общем, случай не является.
[01:20:10.780 --> 01:20:14.940]  Это вы из доказательства поймете. Там в доказательстве у вас только с вероятностью
[01:20:14.940 --> 01:20:24.580]  единицы будет сходимость. Вот. Почти наверно и сходимости там не будет. Итак, задача. Уже третья.
[01:20:24.580 --> 01:20:40.020]  Итак, пусть x1, xn выборкой из распределения рилея. Давайте обозначим его как-нибудь вот так,
[01:20:40.340 --> 01:20:47.220]  с параметром sigma2. Плотность я сейчас напишу. Проще найти оценку вот этого параметра двумя
[01:20:47.220 --> 01:20:56.060]  методами. Методом максимального правдоподобия и методом моментов. Вот. Плотность. Какая плотность?
[01:21:10.020 --> 01:21:16.260]  Вот так. То есть распределение с такой плотностью называется распределением рилея. И что нас
[01:21:16.260 --> 01:21:22.380]  просят? Нас просят по выборке размера n найти оценку методом моментов и методом максимального
[01:21:22.380 --> 01:21:27.900]  правдоподобия. Сейчас я проверю, что я правильно плотность написал. Иначе мы задачу с вами можем
[01:21:27.900 --> 01:21:37.860]  не решить. А это плохо, да ведь? Так. Нет, вроде все правильно. Давайте по методу моментов.
[01:21:37.860 --> 01:21:53.220]  Как будем решать? Ну да, нам на самом деле, смотрите, параметр один, поэтому нам достаточно только
[01:21:53.220 --> 01:21:59.420]  первый момент найти. Мы хотим составить с вами уравнение такого вида. Чтобы такое уравнение
[01:21:59.420 --> 01:22:03.260]  составить, нам нужно найти математическое ожидание. Давайте найдем математическое ожидание.
[01:22:03.260 --> 01:22:21.780]  Ой, уже индикатор лишний, потому что я уже написал в ограничениях интеграла.
[01:22:21.780 --> 01:22:40.420]  Как будем считать? По-моему, тяжело будет. Ну да, на самом деле тут нужно, вот такой интеграл
[01:22:40.420 --> 01:22:44.700]  появляется, когда вы считаете дисперсию нормального распределения, например. И вот в нашем
[01:22:44.700 --> 01:22:48.780]  случае тоже появился. Что в таком случае делать? Ну на самом деле давайте разобьем просто на
[01:22:48.780 --> 01:23:01.460]  x и x поделить на sigma квадрат. Есть степень минус x в квадрате. Ну и по частям это будем брать.
[01:23:01.460 --> 01:23:10.140]  Так, что у нас получится? Интеграл от этой штуки чему равен?
[01:23:10.140 --> 01:23:23.580]  Вот это все занесется по дифференциалу, и на самом деле будет просто e в степени минус 100.
[01:23:23.580 --> 01:23:33.140]  Да, то есть здесь будет действительно e на 2 sigma квадрат. Подстановки от 0 до бесконечности.
[01:23:33.140 --> 01:23:37.100]  Ну на бесконечности зановляется, 0 тоже зановляется, поэтому вот это просто 0.
[01:23:37.100 --> 01:23:43.660]  Так, дальше формулю интегрируем не по частям. У нас остается так,
[01:23:43.660 --> 01:23:49.700]  берем производную от x это единичку, поэтому она никак не влияет. Ну и по сути нам осталось
[01:23:49.700 --> 01:24:02.940]  посчитать вот такую штуку. Так, вроде не ошибся. Нет, не ошибся. Как это будем считать?
[01:24:02.940 --> 01:24:10.260]  Заметно. Заметно, чтобы множитель вынести, и потом кой не спит на какой-то пицце.
[01:24:10.260 --> 01:24:32.980]  Действительно, давайте просто докидаем туда каких-нибудь множителей,
[01:24:32.980 --> 01:24:38.700]  каких-нибудь констант, чтобы какой-нибудь хороший интеграл получился. Ну а хороший
[01:24:38.700 --> 01:24:43.220]  интеграл это у нас, например, интеграл плотности нормального распределения. А это
[01:24:43.220 --> 01:24:48.180]  почти похоже, кстати, на интеграл плотности нормального распределения. В силу симметрии,
[01:24:48.180 --> 01:24:56.740]  наверное, можно сказать, что это что-то вот такое. Почему?
[01:24:56.740 --> 01:25:21.300]  Да, ты прав. То есть тут ты говоришь минус одна вторая, да? Нет, мы когда бы под дифференциал
[01:25:22.300 --> 01:25:38.340]  пополам. Ну в смысле интеграл от х. Окей? Так, ну смотрите, вот в силу симметрии, наверное,
[01:25:38.340 --> 01:25:43.060]  можно положить вот таким образом, да? То есть продлили просто интеграл чётным образом от
[01:25:43.060 --> 01:25:46.900]  минус бесконечности до плюс бесконечности. Вот, ну и давайте докидаем чего-нибудь, чтобы это было
[01:25:46.900 --> 01:25:51.620]  похоже на плотность нормального распределения. Плотность нормального распределения, что нужно
[01:25:51.620 --> 01:25:59.700]  сделать? Корень из 2АП сигма квадрат. Ну соответственно, все, что мы, на все,
[01:25:59.700 --> 01:26:12.460]  что поделили, давайте намножим. Вот это это единичка. Итоговый ответ. Получается корень
[01:26:12.460 --> 01:26:36.260]  и пополам сигма квадрат. Так, под интегралом минус. Вот этот? Так, я утверждаю, что он должен
[01:26:36.260 --> 01:26:46.380]  был где-то уйти. Давайте посмотрим, где. Вот здесь, когда мы брали интеграл е в степени минус t,
[01:26:46.380 --> 01:26:50.180]  там минус должен был вылезти, поэтому здесь, на самом деле, плюсик должен был быть, да,
[01:26:50.180 --> 01:26:57.860]  по маркам. Ну вот, это уже правильное значение от ожидания. Ну и все, соответственно, оценка
[01:26:57.860 --> 01:27:01.860]  по методу моментов. Это просто мы с вами приравниваем теоретическое значение.
[01:27:06.260 --> 01:27:17.220]  А тут у нас что будет? Сигма квадрат приравниваем к иперическому значению. Ну и отсюда оценка сигма
[01:27:17.220 --> 01:27:31.340]  квадрата, это просто 3 в квадрате на 2П. Согласны? Да, вроде меня так получилось, когда я считал.
[01:27:36.260 --> 01:27:43.980]  Что именно? Мы посчитали математическое ожидание. Оценка по методу моментов, вот она. То есть,
[01:27:43.980 --> 01:27:47.300]  это система уравнений. В нашем случае, так как параметр 1, у нас всего одно уравнение.
[01:27:48.700 --> 01:27:53.260]  Приравниваем математическое ожидание к иперическому математическому ожиданию,
[01:27:53.260 --> 01:27:58.420]  то есть, к среднему. Посчитали мотош, он вот такой. Приравняли, получили чему ровно сигму квадрат.
[01:27:58.420 --> 01:28:08.180]  Окей? Да, одномерная задача. Ну, параметр уравнений в методе моментов столько,
[01:28:08.180 --> 01:28:14.580]  сколько у вас параметры. Так, давайте посчитаем оценку методом максимального продуподобия.
[01:28:28.420 --> 01:28:41.460]  Так, вот это оценка по методу моментов. Давайте ее стирать не будем.
[01:28:49.460 --> 01:28:53.940]  Оценка методом моментов. Так, а теперь давайте оценку методом максимального продуподобия найдем.
[01:28:53.940 --> 01:28:59.540]  Так, значит, нам нужно что первым делом? Составить функцию продуподобия.
[01:28:59.540 --> 01:29:11.620]  Она будет выглядеть как произведение плотностей. Плотности у нас это что такое? Это x на сигма
[01:29:11.620 --> 01:29:20.540]  квадрат. E в степени минус x квадрат на 2 сигма квадрат. Вот, ну, на индикаторы еще.
[01:29:24.340 --> 01:29:28.820]  Так, это просто определение функции продуподобия, произведение плотностей в точках, которые нас интересуют.
[01:29:28.820 --> 01:29:36.900]  Дальше, трюк с логарифмированием, потому что гораздо проще будет нам действовать,
[01:29:36.900 --> 01:29:43.580]  если мы прологарифмируем. Логарифм функции продуподобия, это будет у нас по сути
[01:29:43.580 --> 01:30:07.500]  сумма логаритмов x и так. Минус что? Почему 2n? А почему 2n?
[01:30:13.580 --> 01:30:28.620]  Пока через сумму, да. Просто чтобы давайте лишних шагов пока не
[01:30:28.620 --> 01:30:55.620]  попускать. Согласен? Согласен. Сейчас решаем и перерыв строим. Да, все, теперь с этим согласен.
[01:30:55.620 --> 01:31:10.220]  И получается плюс еще, можно минус, сумма p от 1 до n, x в квадрате на 2 сигма в квадрате.
[01:31:10.220 --> 01:31:17.460]  Вроде больше не денег косячил, так, Вить? Супер. Дальше следующим шагом, что мы должны сделать?
[01:31:17.460 --> 01:31:23.180]  Да, давайте найдем максимум по параметрам, то есть найдем такой параметр, который лучше всего
[01:31:23.180 --> 01:31:29.780]  подходит. Вот, чтобы найти максимум нам нужно найти точку подозреваемую на экстремум. Давайте
[01:31:29.780 --> 01:31:36.780]  продиференцируем, только сразу будем по сигма квадрата дифференцировать. Зачем по сигма? Мы
[01:31:36.780 --> 01:31:44.540]  для сигма квадрата. Ну да, да, да, мы просто делаем сейчас небольшую замену. Пусть это сигма
[01:31:44.540 --> 01:31:54.420]  квадрат и давайте просто дифференцировать по t. По сути, да, мы это и сделаем. Так, по d сигма
[01:31:54.420 --> 01:32:02.140]  квадрат. Ну по сути, просто считайте, что я sigma квадрат за t обозвал, и сейчас будет 0. Так,
[01:32:02.140 --> 01:32:12.060]  производной вот этой штуки будет 0. Производной вот этой штуки это будет? Нет, это будет 1 на
[01:32:12.060 --> 01:32:21.740]  сигма квадрат. А производной а? Ну потому что мы, тут по сути логарифм t, производная просто 1 на t.
[01:32:21.740 --> 01:32:29.860]  Так, с минусом вопрос остался, да? Минус остается. Так, что здесь у нас будет?
[01:32:29.860 --> 01:32:47.980]  Так, а здесь будет минус сумма в квадрате. Да, это у нас по сути t в минус 1, поэтому
[01:32:47.980 --> 01:32:55.700]  плюс вылезет. Делим на 2 сигма в четвертый. Ну теперь похоже на правду, да?
[01:33:00.420 --> 01:33:08.540]  Дальше. Необходимые условия экстремума. Вот в этой точке производной должна быть 0. Вот,
[01:33:08.540 --> 01:33:14.020]  следовательно, у нас появляется уравнение правдоподобия. Ну вот тут n на сигма квадрат.
[01:33:14.020 --> 01:33:28.620]  Это у нас сумма в квадрате, поделить на 2 сигмы в четвертый. Вот, ну положим,
[01:33:28.620 --> 01:33:32.500]  что сигма у нас не 0, там на самом деле по условию сигма больше 0.
[01:33:32.500 --> 01:33:46.300]  Можем сократить на сигма в квадрате. Вот, и отсюда получить уже оценку.
[01:33:46.300 --> 01:34:03.780]  Так, на 2. Вот. Ну сейчас, по сути, мы нашли с вами оценку максимального правдоподобия.
[01:34:03.780 --> 01:34:15.140]  Ну это не совсем понятно, что это такое. На самом деле вот это вот похоже на второй момент.
[01:34:15.340 --> 01:34:23.180]  А вот откуда здесь двоечка появилась уже не совсем очевидно. Ну на самом деле вот
[01:34:23.180 --> 01:34:25.700]  эта оценка будет не смещенной, по-моему. Если вы ее, честно, посчитаете,
[01:34:25.700 --> 01:34:34.500]  математическое ожидание от нее-то. Да, такое бывает. Бывает, что они совпадают. Вот,
[01:34:34.500 --> 01:34:38.700]  допустим, в нормальном оценке параметра A при известном сигме квадрат, то у вас получится
[01:34:38.700 --> 01:34:44.380]  и там, и там среднее. А иногда бывает, что методы дают разные оценки. Иногда совпадают,
[01:34:44.380 --> 01:34:55.740]  иногда отличаются. Понятно все пока? Вот. То есть задачи этого раздела вам дадут какое-нибудь
[01:34:55.740 --> 01:35:00.780]  семейство и скажут, а найдите нам асимпатическую оценку, асимпатически нормальную. Или найдите нам
[01:35:00.780 --> 01:35:04.740]  состоятельную оценку. Могут типа сказать, найдите оценку с такими-то свойствами. Ну тогда вы можете
[01:35:04.740 --> 01:35:09.420]  применить метод какой-нибудь и найти оценку, которую просят. Могут просто сказать, типа
[01:35:09.420 --> 01:35:13.340]  найдите таким-то методом оценку. Там методом максимального продуподобия или методом
[01:35:13.340 --> 01:35:18.260]  математического. Вот. Единственное, что мы еще с вами не обсудили, это метод выборочной квантили.
[01:35:18.260 --> 01:35:35.940]  Буквально пару слов про него скажем и потом точно пойдем на фирер. Да. Ну про эффективность там
[01:35:35.940 --> 01:35:45.260]  аккуратно нужно говорить, там асимпатическая эффективность, но вообще да. А чего? Нет,
[01:35:45.260 --> 01:35:55.820]  все. Пока что хватит. И по задачи они обычно идут по разделам, то есть здесь мы эффективность,
[01:35:55.820 --> 01:36:03.820]  допустим, ее исследовать не хотим и не будем. Так, давайте еще про метод выборочной квантили поговорим.
[01:36:03.820 --> 01:36:10.220]  Вот смотрите, как мы с вами строили сходимость по распределению. Мы брали с вами CPT, а потом
[01:36:10.220 --> 01:36:16.100]  применяли к нему дельта метод и получали какую-то сходимость, так ведь? То есть если нас просят
[01:36:16.100 --> 01:36:20.660]  показать или построить оценку асимпатически нормальную, мы с вами записываем CPT.
[01:36:20.660 --> 01:36:40.980]  Вот так вот. А потом применяем к ней дельта метод. Вот какая сходимость,
[01:36:40.980 --> 01:36:46.820]  которой у нас есть. Вот, но понятно же, что CPT, например, не будет работать, если у вас нет
[01:36:46.820 --> 01:36:53.300]  от ожидания. То есть вам уже нет куда взять вот такую сходимость, чтобы дальше какие-то
[01:36:53.300 --> 01:37:06.700]  применять к ней теоремы. А вас могут попросить построить асимпатически нормальную оценку
[01:37:06.700 --> 01:37:30.300]  для двига распределения каши. Вот, что будем делать? Во-первых, давайте с вами вспомним,
[01:37:30.300 --> 01:37:45.940]  как у нас плотность записывается для каши в таком случае. Вот. Ну просто положим, что параметр
[01:37:45.940 --> 01:37:50.820]  масштаба единичка фиксированный, а вот у нас параметризован только с двига распределения. То
[01:37:50.820 --> 01:37:55.220]  есть плотность у вас выглядит примерно так же, как и в случае нормального распределения. Примерно
[01:37:55.220 --> 01:37:59.940]  какой-то колокол, но у него просто более толстые хвости, поэтому эта штучка не интегрируема. То
[01:37:59.940 --> 01:38:05.980]  есть там от ожидания не будет. Вот. И вас проще построить асимпатически нормальную оценку для
[01:38:05.980 --> 01:38:20.100]  параметра с двига. Что в таком случае сделать? Можно. А что бы вы делали? ОМП сложно строить для
[01:38:20.100 --> 01:38:27.460]  каши. Мы сделали это на семинаре. Там уже для двух значений у нас там появлялся квадратичный,
[01:38:27.460 --> 01:38:32.780]  по-моему, трех или нет. Многочлен третьей степени в знаменателе. Мы что-то его исследовали потом.
[01:38:32.780 --> 01:38:49.980]  Да. Вот тут как раз-таки нам приходит на выручку метод выборочной квантили. То есть нам нужен еще
[01:38:49.980 --> 01:39:01.940]  какой-то источник вот такой сходимости. Если CPT не работает, то есть аналог. Метод называется
[01:39:01.940 --> 01:39:09.140]  метод выборочной квантили. Вот. Что такое квантиль? Квантиль есть как бы теоретическая и квантиль есть
[01:39:09.140 --> 01:39:22.940]  практическая. Теоретическая квантиль — это такое значение XA, что вероятность того,
[01:39:22.940 --> 01:39:31.700]  что ваша случайная величина будет меньше этого значения, ну, равна P. Просто ZP. Да, обозначается
[01:39:31.700 --> 01:39:40.180]  ZPET-квантиль. Это как бы такое значение, что вот ваша случайная величина, такое значение XA,
[01:39:40.180 --> 01:39:45.860]  что ваша случайная величина с такой вероятностью будет меньше этого значения. То есть, грубо говоря,
[01:39:47.860 --> 01:39:56.540]  вот здесь значение функции распределения будет P. Если эта штучка не прерывит. Вот. То есть,
[01:39:56.540 --> 01:40:01.540]  рассматривая Z1-2-ую квантиль, это медианно-теоретическая. То есть, грубо говоря, так же точка XA,
[01:40:01.540 --> 01:40:09.180]  что ваша случайная величина попадет левее этой точки, то есть меньше будет XA с вероятностью 1-2.
[01:40:09.180 --> 01:40:14.300]  Если у вас распределение симметрично, то понятно, что теоретическая квантиль будет находиться как бы
[01:40:14.300 --> 01:40:21.340]  вот в этом горбике. То есть, унимодальная случайная величина с одним горбиком. И это вот как раз наш
[01:40:21.340 --> 01:40:29.980]  луч из каши. Медианно – это и есть параметр theta. Вот. И тогда есть на самом деле теорема
[01:40:29.980 --> 01:40:38.340]  выборочной квантили, которая утверждает, что вот выборочный NP-квантиль стоит циклическому квантилю
[01:40:38.340 --> 01:40:41.020]  в таком смысле.
[01:40:51.820 --> 01:40:52.820]  Вот.
[01:40:58.620 --> 01:41:02.820]  Так, давайте тогда аккуратно. Вот, смотрите, мы выяснили, что есть теоретические квантили, да?
[01:41:02.820 --> 01:41:08.460]  А есть как бы практические квантили. Вот, смотрите, у выборки есть медианно, вы знаете, да?
[01:41:08.460 --> 01:41:12.940]  В случае, когда это нечетная выборка, то это просто средний элемент. Когда выборка четного размера,
[01:41:13.020 --> 01:41:25.620]  то это полусумма средних двух элементов. Вот. И смотрите, ZNP определяется как NP-порядковая статистика.
[01:41:25.620 --> 01:41:29.620]  Вы сортируете возрастание и смотрите, какой элемент у вас стоит на NP этом месте.
[01:41:32.620 --> 01:41:37.460]  Ну, по сути, вот если вы рассмотрите выборочную медианну и теоретическую медианну, то есть P равняется
[01:41:37.980 --> 01:41:44.180]  здесь у вас получится ровно вот этот элемент с точностью до там полусуммы, ну, в общем, пока на это забейте,
[01:41:44.180 --> 01:41:53.660]  который стоит ровно посередине выборки, так ведь? Вот. И есть у вас теорическое значение этой квантили,
[01:41:53.660 --> 01:42:00.220]  то есть вот оно, например. И вот теорема выборочной квантили утверждает, что вот эта вот практическая
[01:42:00.220 --> 01:42:06.340]  квантиль, которая по выборке строится, ну, допустим, практической медианной, она сходится к выборочной
[01:42:06.340 --> 01:42:15.660]  медиане. То же самое там и с 0,75 квантили, то есть вот вы возьмете точку такую, что вероятность того,
[01:42:15.660 --> 01:42:23.780]  что ваша случайная величина будет меньше этой точки 0,75, вот, то есть здесь вот 0,75, то у вас
[01:42:23.780 --> 01:42:30.980]  0,75 квантиль, это называется третий квартиль, то есть третий квартиль, то есть если вы возьмете
[01:42:30.980 --> 01:42:37.060]  вашу выборку, отсортируете, и у вас в ней, допустим, 100 элементов, то это будет элемент,
[01:42:37.060 --> 01:42:43.420]  который стоит на 0,75 месте, так ведь? 0,75 квантиль, и вот эта вот квантиль при растущей выборке,
[01:42:43.420 --> 01:42:52.820]  он тоже будет как бы, он будет у вас сходиться вот к этой точке, к теоретической. Окей? Вот. А теперь
[01:42:52.820 --> 01:43:15.140]  встает вопрос. Это плотность в квадрате в точке zp. Да, плотность в квадрате. Давайте я аккуратно
[01:43:15.140 --> 01:43:32.540]  напишу, извините. Давайте сразу... Вот здесь вот? Скорее всего np плюс один, да. Там аккуратно
[01:43:32.540 --> 01:43:45.380]  нужно посмотреть. Что не закрыто? Спасибо. Не, это справедливо, справедливо, спасибо. А что с минусом?
[01:43:45.380 --> 01:44:04.740]  На самом деле, это та же самая параметризация, просто у вас будет... Они будут симметричны относительно...
[01:44:04.740 --> 01:44:11.080]  Ну короче, это не столь критично, ну хорошо, давайте x- это сделаем. Ну ладно, классическое
[01:44:11.080 --> 01:44:22.400]  определение такое, действительно. Так, давайте пример быстро посмотрим с вами. Вот давайте
[01:44:22.400 --> 01:44:28.840]  построим с вами асинтетически нормальную оценку двумя способами для нормального распределения
[01:44:28.840 --> 01:44:46.860]  с параметрами a sigma квадрат. Да, сейчас я посмотрю, вроде я не ошибся. Я просто примерно
[01:44:46.860 --> 01:44:52.340]  доказательство помню, и там вроде-то именно такая штука вылезает. Да, вроде это правда. Так,
[01:44:52.340 --> 01:44:57.700]  давайте построим двумя способами асинтетически нормальную оценку для параметра a. Первый
[01:44:57.700 --> 01:45:20.200]  способ какой? CPT. Да, построить асинтетически нормальную оценку двумя способами. Вот. А второй
[01:45:20.200 --> 01:45:26.080]  способ какой еще можно? Метод квантилий. Ну понятно, что а у вас будет медианой теоретической,
[01:45:26.080 --> 01:45:29.520]  потому что у вас симметричное распределение относительно точки a, поэтому вероятность того,
[01:45:29.520 --> 01:45:33.240]  что вы попадете левее точки a, у вас будет одна вторая. Это можно аккуратно через интеграл
[01:45:33.240 --> 01:45:39.080]  показать, но мы давайте скажем, что мы в это верим. Поэтому выборочно медиана, она обозначается вот так.
[01:45:41.080 --> 01:45:45.920]  Тоже будет асинтетически нормальной оценкой, но у нее будет другая асинтетическая дисперсия.
[01:45:45.920 --> 01:45:54.720]  Какая асинтетическая дисперсия будет? Вопрос, чему, какое значение принимает функция плотности
[01:45:54.720 --> 01:46:07.040]  в точке a? Сигма квадрат, есть это пи минус х в квадрате, х минус а в квадрате на 2,
[01:46:07.040 --> 01:46:14.120]  сигма в квадрате. Ну вот это все будет единичка, потому что у вас х равен а. И вот это в квадрате,
[01:46:14.120 --> 01:46:21.600]  это получается будет 2 пи сигма квадрат. Но это переходит вверх, поэтому это будет пи сигма
[01:46:21.600 --> 01:46:34.800]  квадрат. Грубо говоря, вы можете оценивать параметр a в нормальной модели как средним,
[01:46:34.800 --> 01:46:41.560]  так и медианой. То есть у вас наверное в каких-нибудь статистических задачках вас уже просили оценить
[01:46:41.560 --> 01:46:46.840]  через средний и через медиану. То есть это две асинтетические нормальные оценки, одна правда
[01:46:46.840 --> 01:46:53.960]  из них хуже. Сейчас поймем почему. Скоро. Ну вот хуже вот эта, потому что у нее дисперсия больше.
[01:46:53.960 --> 01:47:05.560]  Ну потому что у тебя вот это получилось, ты на это делишь, получается перекидываешь
[01:47:06.560 --> 01:47:17.880]  Ну п на 1у сп, одна вторая на одну вторую, одна четверть была. Медиану это одна вторая квантиль.
[01:47:17.880 --> 01:47:31.800]  Так товарищи, не расслабляемся. Нам за час нужно еще сделать много. С этим понятно? Отлично.
[01:47:31.800 --> 01:47:40.200]  Сейчас, подождите одну секунду. Мы еще должны с вами про каши поговорить. Вот смотрите,
[01:47:40.200 --> 01:47:46.160]  все вас просят построить асинтетически нормальную оценку для сдвига каши. Воспользоваться цпт вы
[01:47:46.160 --> 01:47:52.560]  не можете, потому что у вас нет математического ожидания дисперсии и подавна. Вот, поэтому давайте
[01:47:52.560 --> 01:48:00.280]  построим через квантиль, через выборочную квантиль. Понятно ли, что опять-таки медианой здесь будет
[01:48:00.280 --> 01:48:12.400]  это? Да, это правда? Хорошо. На самом деле, смотрите, как определяется квантиль по-честному.
[01:48:12.400 --> 01:48:27.800]  Вот, но в случае непрерывной функции вы можете сказать, что у вас есть квантильная функция,
[01:48:27.800 --> 01:48:36.720]  у вас просто квантильная функция будет обратной от квантили.
[01:48:36.720 --> 01:48:56.600]  Но что не определено? По всем эксамус прямой. То есть, грубо говоря, вы ищете в вашем распределении
[01:48:56.600 --> 01:49:09.440]  такую точку, чтобы, вот, самую маленькую, чтобы вероятность попасть больше, чтобы у вас
[01:49:09.440 --> 01:49:16.480]  функция распределения вот здесь была почти в точности равна тому, чего мы хотите. Если функция
[01:49:16.480 --> 01:49:20.280]  непрерывная, то вот эти точки как бы взаимно однозначно связаны. Инфинум там нужен для
[01:49:20.280 --> 01:49:24.800]  непрерывного случая, когда у вас есть какой-нибудь разрыв. Функция распределения вот такой быть может,
[01:49:24.800 --> 01:49:31.120]  например. Вот, и поэтому там есть определение через инфинум, чтобы, если вы захотели найти
[01:49:31.120 --> 01:49:34.640]  какую-нибудь вот квантиль, которая попадает в разрыв, вы просто взяли вот эту вот точечку.
[01:49:34.640 --> 01:49:41.640]  Понятно, да? Если функция будет непрерывная, то на самом деле у вас есть биекция между ними,
[01:49:41.640 --> 01:49:50.680]  монотонно непрерывная, все хорошо. Есть вот такая биекция. Вот, то есть на самом деле квантили
[01:49:50.680 --> 01:49:56.440]  очень легко находить. Если у вас функция распределения непрерывная, это просто вот обратная функция к распределению.
[01:49:56.440 --> 01:50:04.720]  Понятно, да? Вы фиксируете точку P и находите такую точку вот такая, что вот левее него
[01:50:04.720 --> 01:50:13.720]  вероятность случайной величины будет P. Вот. И, соответственно, на самом деле можно квантильную
[01:50:13.720 --> 01:50:18.160]  функцию в общем случае получить. Ну как в общем случае получить квантильную функцию для распределения
[01:50:18.160 --> 01:50:35.800]  каши? Максим, помоги. Максим. А? Максим. А? Квантильную функцию для распределения каши.
[01:50:35.800 --> 01:50:47.600]  Ну да, на самом деле, чтобы найти квантильную функцию, чтобы по-честному показать, что вот это
[01:50:47.760 --> 01:50:53.160]  у вас будет медиана, вам нужно найти функцию распределения сначала. Функцию распределения
[01:50:53.160 --> 01:51:02.600]  это просто интеграл от этой штуки будет. Да, это Аркангенс. Максим, подскажи, какой там будет Аркангенс?
[01:51:02.600 --> 01:51:17.400]  По-моему, х-минус это. Х-минус это, да. Ну и там еще на самом деле плюс одна вторая. Вот. А отсюда на
[01:51:17.400 --> 01:51:21.360]  самом деле вы уже легко квантильную функцию найдете. Почему? Потому что квантильная это обратная?
[01:51:21.360 --> 01:51:38.000]  Наверное. То есть вот вы получили функцию распределения и вам функцию распределения могут дать там в условии,
[01:51:38.000 --> 01:51:41.480]  допустим. Как найти квантильную функцию? Ну давайте просто обратную функцию найдем.
[01:51:41.480 --> 01:52:05.640]  Один напиток появляется? Вот. Давайте найдем обратную функцию. Обратная функция это у нас будет х-минус
[01:52:05.640 --> 01:52:22.960]  одна вторая. Ко всему этому применяем тангенс и добавляем это. И на пи, да? Внутри тангенса на пи,
[01:52:23.440 --> 01:52:37.160]  вот. Ну и по сути вот у вас квантильная функция обычно вот так обозначается. З от П. То есть вы
[01:52:37.160 --> 01:52:43.920]  фиксируете какой-то уровень П. Если мы аккуратно это перепишем. Теперь это будет выглядеть вот таким
[01:52:43.920 --> 01:52:49.200]  образом. И теперь мы допустим легко показать, что вот это это будет как раз таки медианой нашего
[01:52:49.200 --> 01:52:57.000]  распределения. Ну почему? Давайте найдем такую точку П. Давайте просто подставим. П равный одной
[01:52:57.000 --> 01:53:09.200]  второй. Тангенс нуля это ноль. То есть у вас получится З от П в плотности тета. То есть мы
[01:53:09.200 --> 01:53:15.680]  только что с вами доказали, что медиана это тета. Супер! Если медиана это тета, то теперь мы
[01:53:15.680 --> 01:53:20.840]  можем воспользоваться теоремой выборочной квантили для медианы. То есть для П равна одна вторая.
[01:53:30.840 --> 01:53:41.200]  Мы получаем с вами, что корень З опять выборочно медиана минус это. Сходится к нормальному
[01:53:41.200 --> 01:53:54.480]  распределению с параметрами ноль. Одна четвертая. И на плотность в точке тета. Максим. Максим,
[01:53:54.480 --> 01:54:07.440]  какая плотность точки тета будет? Один на П, да? Ну то есть П в квадрате на 4. Ну вот,
[01:54:07.520 --> 01:54:11.840]  если у вас 1 на 5 есть, то П в квадрате будет 1 на П квадрат. Переносим П в квадрате на 4.
[01:54:11.840 --> 01:54:16.400]  Получается. Одна четвертая у нас появилась как П на 1 минус П, при П равна одна вторая.
[01:54:16.400 --> 01:54:23.280]  Вот. То есть смотрите, вы можете построить на самом деле симпатически нормальную оценку,
[01:54:23.280 --> 01:54:28.400]  даже если у вас нет математического ожидания. Для этого можно воспользоваться выборочной квантилью.
[01:54:28.400 --> 01:54:34.720]  А выборочная квантиль на самом деле, она более-менее всегда существует, потому что чтобы
[01:54:34.720 --> 01:54:40.320]  существовала просто квантиль, вам нужна функция распределения, а функция распределения у вас более-менее
[01:54:40.320 --> 01:54:45.120]  существует всегда, потому что мера однозначно задается в функции распределения. Вот. И нужно,
[01:54:45.120 --> 01:54:49.360]  чтобы просто в окрестности этой точки у вас существовала плотность, чтобы функция распределения
[01:54:49.360 --> 01:54:55.680]  была дифференцируема на самом деле в окрестности ZP. То есть грубо говоря, метод выборочной квантили
[01:54:55.680 --> 01:55:03.520]  можно чуть чаще использовать, чем CPT. Вот. Ну все, давайте режиме, небольшой ее отдых. Значит,
[01:55:03.520 --> 01:55:11.440]  режиме какое? В данном разделе вам могут дать задачу. Построите хорошую оценку, которая обладает
[01:55:11.440 --> 01:55:16.560]  какими-то свойствами для вот такого семейства распределений. Вы просто применяете методы,
[01:55:16.560 --> 01:55:22.120]  а эти методы обладают свойствами. Это там надо написать, например. Вот. Или вас могут просто
[01:55:22.120 --> 01:55:27.200]  напрямую попросить, найдите вот такую-то оценку, допустим, симпатически нормальную, для такого-то
[01:55:27.200 --> 01:55:32.880]  распределения. Или найдите оценку методом максимального продуподобия для такого-то
[01:55:32.880 --> 01:55:44.600]  семейства. Вот. И такие задачи в этом разделе могут быть. Да не, ну не, не могут. Вряд ли. То есть,
[01:55:44.600 --> 01:55:50.600]  на самом деле, несмещенные далеко не всегда существуют, допустим. То есть, для некоторых
[01:55:50.600 --> 01:55:56.960]  функций у вас вообще не существует несмещенных оценок. И это как бы не страшно, с этим можно жить.
[01:55:56.960 --> 01:56:03.920]  Все тогда перерыв. В общем, мы отдохнули полтора часа. Можем продолжать.
[01:56:03.920 --> 01:56:14.560]  Так, друзья, все. Времени больше нет, надо заканчивать. Садыка, ты куда?
[01:56:14.680 --> 01:56:28.680]  Еще 30 секунд осталось. А, наверное.
[01:56:28.680 --> 01:56:34.600]  Все, поехали. У нас очень мало времени.
[01:56:34.600 --> 01:56:53.960]  Нормально. Все хорошо будет, друзья. Так, давайте буквально пару слов скажем про сравнение оценок.
[01:56:53.960 --> 01:57:02.480]  Хорошо? Вот. Значит, как можно сравнивать оценки? Первый подход. А симпатический?
[01:57:05.080 --> 01:57:11.680]  Так. Ну, подход для сравнения симпатически нормальных оценок. Вот. Или асимпатически,
[01:57:11.680 --> 01:57:19.480]  как он еще называется. То есть, вот у вас есть две оценки. Одного и того же параметра там,
[01:57:19.480 --> 01:57:28.960]  или одной и той же функции от параметра. Вот. Ну и получается, логично будет сравнить просто их
[01:57:28.960 --> 01:57:32.440]  дисперсии. Потому что все остальное как бы в записи одинаково, кроме того, какую оценку вы
[01:57:32.440 --> 01:57:37.960]  здесь используете. Но у них разные дисперсии асимпатические. Вот. И давайте просто как бы
[01:57:37.960 --> 01:57:48.760]  графики построим этих асимпатических дисперсий. Соответственно, вот пусть у вас какая-нибудь вот
[01:57:48.760 --> 01:57:56.320]  такая будет первая асимпатическая дисперсия. А для второй оценки у вас будет какая-нибудь вот
[01:57:56.320 --> 01:58:06.120]  такая синтетическая дисперсия вторая. Какая оценка из этих двух лучше? Ну, понятно, что первая,
[01:58:06.120 --> 01:58:12.840]  потому что она как бы, видите, везде получше, в каждой точке. Вот. На самом деле у вас задан частичный
[01:58:12.840 --> 01:58:18.120]  порядок, потому что вот если у вас будет две вот таких оценки, да, которые в какой-то точке одна
[01:58:18.120 --> 01:58:24.720]  лучше, чем другая, в других точках хуже, то как бы они уже в равномерном подходе несравнимы. То
[01:58:24.720 --> 01:58:36.520]  есть вы не можете сказать, что одна оценка в каждой точке лучше, чем другая. Давай вернем старую. Это
[01:58:36.520 --> 01:58:42.920]  не метод максимального продоподобия. Метод максимального продоподобия, да, самую лучшую из
[01:58:42.920 --> 01:58:46.640]  них. То есть если есть какая-то последовательность оценок, которая сходится к какой-то вот предельной,
[01:58:46.640 --> 01:58:51.440]  то вот у метода максимального продоподобия будет самая лучшая синтетическая дисперсия. Ну, в
[01:58:51.440 --> 01:58:58.400]  некотором смысле. Там есть очень много звездочек, которые мы опускаем. Хорошо. Вот. И соответственно,
[01:58:58.400 --> 01:59:02.640]  вы просто сравниваете синтетические дисперсии в каждой точке. Если синтетическая дисперсия в
[01:59:02.640 --> 01:59:08.000]  каждой точке для одной функции, ну, как бы меньше, то понятно, что вот эта функция как бы предпочтительнее.
[01:59:08.000 --> 01:59:14.560]  Такой синтетической дисперсии такая оценка лучше. Вот. Но есть в подходе оценки, вот, допустим,
[01:59:14.560 --> 01:59:20.560]  у вас одна синтетическая дисперсия лучше вот на этом отрезке, да, а другая лучше вот на этом
[01:59:20.560 --> 01:59:25.600]  отрезке. Тогда как бы они в равномерном подходе несравнимы. Тут выбрать лучше уже не получится.
[01:59:25.600 --> 01:59:29.440]  Это называется асимпатический подход. Вы, грубо говоря, в каждой точке сравниваете
[01:59:29.440 --> 01:59:33.920]  асимпатические дисперсии для двух разных оценок. И вот у нас пример был. Мы получили с вами две
[01:59:33.920 --> 01:59:47.880]  оценки. И средняя, и медиановыборщная. Для параметра A в модели E на сигма квадрат. Вот. И у этой
[01:59:47.880 --> 01:59:55.680]  асимпатической дисперсии у нас получилась, вроде бы, P. Как у нас асимпатическая дисперсия
[01:59:55.680 --> 02:00:03.640]  получилась? Кто-нибудь записал? Нормальная для параметра A в нормальном распределении.
[02:00:03.640 --> 02:00:19.480]  Да, вот здесь просто сигма квадрат, потому что это СПТ. А вот здесь у вас будет P на 2, да, на сигма квадрата.
[02:00:19.480 --> 02:00:33.600]  Вот. И если вы сравните, то у вас на самом деле вот эта функция, она как бы меньше, чем вот это в
[02:00:33.600 --> 02:00:41.160]  каждой точечке. Для любого сигма квадрат. Ну и для любого A тоже. Вот. Ну для любых параметров вообще у
[02:00:41.160 --> 02:00:46.600]  вас вот эта функция ведет себя, она хуже, она большее значение принимает, чем вот это. Поэтому в равномерном
[02:00:46.600 --> 02:00:54.560]  подходе, в асимпатическом подходе у вас х среднее лучше чем выборочно медиана
[02:00:54.560 --> 02:01:03.940]  для параметра I в нормальной модели. Вот. tired в асимпатическом подходе
[02:01:06.280 --> 02:01:10.560]  мы сравниваем оценки. Первое в каком подходе в асимпатическом? Что значит, что мы сравниваем
[02:01:10.560 --> 02:01:15.020]  в асимпатическом подходе? Значит мы сравниваем их асимпатически дисперсии? Мы получили с вами две
[02:01:15.020 --> 02:01:19.860]  оценки. Х средняя и выборочная медиана. Вот это по ЦПТ, вот это через выборочную
[02:01:19.860 --> 02:01:24.060]  квантиль. У этого дисперсия вот такая. У первой оценки такая дисперсия, у
[02:01:24.060 --> 02:01:28.220]  второй вот такая оценка. Вот такая дисперсия, у второй оценки, прошу прощения.
[02:01:28.220 --> 02:01:33.780]  Соответственно, в таком подходе х средняя лучше, чем выборочная медиана, потому
[02:01:33.780 --> 02:01:39.180]  что у нее асимпатическая дисперсия меньше, чем у выборочной медианы.
[02:01:39.180 --> 02:01:41.180]  Окей?
[02:01:41.180 --> 02:01:45.180]  Вопрос, какое значение в каждой точке.
[02:01:45.180 --> 02:01:50.180]  То есть, если в каждой точке одна функция лучше, то есть меньше, чем другая, то понятно, что вот оценка,
[02:01:50.180 --> 02:01:57.180]  у которой вот такая асимпатическая дисперсия, она лучше. Если они в каком-то, до какого-то момента
[02:01:57.180 --> 02:02:02.180]  одна лучше, потом на какой-то области другая лучше, то они несравнимы. То есть у вас как бы есть
[02:02:02.180 --> 02:02:08.180]  частичный порядок. Понятно, да? У вас есть несравнимые, есть сравнимые между собой.
[02:02:09.180 --> 02:02:16.180]  Вот. И у вас есть как бы на множествах всех оценок какой-то частичный порядок в таком вот подходе сравнения.
[02:02:16.180 --> 02:02:18.180]  Это понятно, да?
[02:02:23.180 --> 02:02:26.180]  Ну вот в таком подходе они несравнимы, но есть другие подходы для сравнения.
[02:02:32.180 --> 02:02:35.180]  Ну вот по симпатическому подходу именно дисперсии сравниваются.
[02:02:39.180 --> 02:02:41.180]  Одну секунду. Мы дойдем до этого.
[02:02:49.180 --> 02:02:57.180]  Вот. Затем вводится понятие функции, функции потерь.
[02:02:57.180 --> 02:03:03.180]  Это вот некоторая функция, которая... Что про нее можно сказать?
[02:03:03.180 --> 02:03:14.180]  Это функция такая, что она не отрицательная, и g от x и y равно 0 тогда и только тогда, когда g...
[02:03:16.180 --> 02:03:20.180]  В общем, когда у вас там два аргумента. То есть если у вас значения не совпали, то это 0,
[02:03:20.180 --> 02:03:24.180]  а если не совпали, то больше 0. То есть это в некотором смысле какая-то метрика.
[02:03:24.180 --> 02:03:33.180]  Поэтому как бы и рассматривают функции потерь вот такого вида. На функционе у вас уже должна была быть.
[02:03:38.180 --> 02:03:41.180]  Ну и вот такого вида функции потерь у вас тоже рассматривались.
[02:03:43.180 --> 02:03:48.180]  Вот. Вот такие функции ввели. Ну давайте с вами вот на что посмотрим.
[02:03:48.180 --> 02:03:59.180]  Давайте мы с вами посмотрим, насколько у вас отличается от истинного значения.
[02:04:03.180 --> 02:04:07.180]  Ну только понятно, что здесь у вас случайная выборка, а вот это какое-то фиксированное число.
[02:04:07.180 --> 02:04:13.180]  Вам необходимо усреднить. Поэтому на самом деле у вас появляется вот такая новая функция g' от t,
[02:04:13.180 --> 02:04:31.180]  которая просто есть в отожимании функ
[02:04:31.180 --> 02:04:34.180]  Понятно, что происходит? Давайте какой-нибудь конкретный пример разберем.
[02:04:34.180 --> 02:04:39.180]  Давайте разберем квадратичную функцию потерь. Вот эта функция потерь называется квадратичная.
[02:04:39.180 --> 02:04:42.180]  Давайте для нее запишем функцию риска.
[02:04:47.180 --> 02:04:53.180]  Тут просто будет математическое ожидание в арте оценки с истинное значение оценки в квадрате.
[02:05:04.180 --> 02:05:05.180]  Понятно ли, что здесь записано?
[02:05:05.180 --> 02:05:14.180]  То есть смотрите, вот это истинное значение параметра, вот это то значение, которое ваша оценка вернула.
[02:05:14.180 --> 02:05:20.180]  И вы смотрите, насколько оно отличается в среднем. Понятно?
[02:05:23.180 --> 02:05:29.180]  Вот. То есть на самом деле там в машинном обучении такие же метрики используют MSE типа mean square error, чем такого.
[02:05:31.180 --> 02:05:34.180]  Ну и соответственно мы дальше с чем-то таким будем работать.
[02:05:35.180 --> 02:05:46.180]  Ну да, это же штрих от одного параметра.
[02:05:51.180 --> 02:05:56.180]  То есть грубо говоря, вы фиксируете θ0 какое-то, да, истинное значение.
[02:05:56.180 --> 02:06:02.180]  И смотрите, насколько в среднем ваша оценка отличается от настоящего значения.
[02:06:02.180 --> 02:06:07.180]  То есть вот здесь у вас на самом деле функция от выборки какая-то идет, и вы как бы по любой выборке как бы усредняете.
[02:06:08.180 --> 02:06:09.180]  Это понятно?
[02:06:10.180 --> 02:06:11.180]  Супер.
[02:06:16.180 --> 02:06:20.180]  Соответственно дальше мы можем рассуждать вот ровно так же, как и в асимпатическом подходе.
[02:06:21.180 --> 02:06:23.180]  А как у нас было в асимпатическом подходе?
[02:06:25.180 --> 02:06:29.180]  Смотрите, у нас здесь опять появилась функция от одного параметра, от истинного значения параметра θ.
[02:06:29.180 --> 02:06:34.180]  Вы опять можете грубо говоря нарисовать такие графики.
[02:06:35.180 --> 02:06:42.180]  То есть вот эта функция риска для одной оценки, а вот эта будет функция риска там для другой оценки.
[02:06:48.180 --> 02:06:51.180]  Вот, ну и опять можно выбрать оценку, которая лучше вот в таком подходе.
[02:06:52.180 --> 02:06:53.180]  Понятно каким образом?
[02:06:54.180 --> 02:06:56.180]  Так же, которая меньше, но у вас так же они могут быть несравнимы.
[02:06:57.180 --> 02:06:58.180]  Например вот так.
[02:06:59.180 --> 02:07:02.180]  Ну какая-нибудь функция риска, которая выглядит таким вот образом.
[02:07:05.180 --> 02:07:06.180]  Да.
[02:07:07.180 --> 02:07:08.180]  Функция риска или функция потери?
[02:07:09.180 --> 02:07:10.180]  Функция риска.
[02:07:11.180 --> 02:07:15.180]  Функция потери это то, что у вас под мотожаданием стоит, а функция риска это вот уже мотожадание этой штуки.
[02:07:16.180 --> 02:07:20.180]  То есть какой риск у нас при фиксированном θ 0, какой риск мы получаем?
[02:07:21.180 --> 02:07:22.180]  То есть насколько мы рискуем?
[02:07:23.180 --> 02:07:24.180]  Насколько мы отклоняемся от истинного значения?
[02:07:24.180 --> 02:07:29.180]  Ну, я у меня один реальный вопрос.
[02:07:30.180 --> 02:07:40.180]  А в чем как бы, если мы придумали этот метод, то значит где-то асимпатический метод был не особо применен,
[02:07:41.180 --> 02:07:43.180]  потому что они в принципе примеры.
[02:07:44.180 --> 02:07:47.180]  Вот асимпатический метод работает только для асимпатически нормальных оценок.
[02:07:48.180 --> 02:07:50.180]  Оценки асимпатически нормальные далеко не всегда.
[02:07:51.180 --> 02:07:52.180]  Вот этот метод более общий.
[02:07:52.180 --> 02:07:53.180]  Вот.
[02:07:54.180 --> 02:07:58.180]  Ну и соответственно давайте сейчас зафиксируем с вами одну единственную функцию потерь квадратическую, ну квадратичную,
[02:07:59.180 --> 02:08:02.180]  и будем вот сравнивать оценки вот в таком смысле.
[02:08:04.180 --> 02:08:06.180]  Давайте сделаем какое-нибудь с вами замечание.
[02:08:07.180 --> 02:08:10.180]  Существует ли наилучшая оценка, такая оценка, которая лучше любой другой оценки?
[02:08:11.180 --> 02:08:14.180]  Наверное не всегда.
[02:08:15.180 --> 02:08:21.180]  Да, потому что давайте мы с вами оценку зафиксируем просто равной какой-нибудь константе.
[02:08:22.180 --> 02:08:29.180]  Мы же можем с вами просто говорить, что независимость от того, какое у вас распределение, а просто параметр один.
[02:08:30.180 --> 02:08:31.180]  Он никак с данными не связан.
[02:08:32.180 --> 02:08:34.180]  Тогда давайте посмотрим, как у вас будет выглядеть функция риска.
[02:08:35.180 --> 02:08:37.180]  Вот здесь ваше значение тета.
[02:08:38.180 --> 02:08:42.180]  Вот здесь у вас где-то тета 1, и функция риска вот здесь, в этой точке, чему будет равна?
[02:08:43.180 --> 02:08:44.180]  Нулю.
[02:08:45.180 --> 02:08:46.180]  Нулю.
[02:08:47.180 --> 02:08:49.180]  Во всех остальных точках она может как угодно себя вести некрасиво,
[02:08:49.180 --> 02:08:53.180]  но вот у вас есть какая-то адекватная оценка, которая почти всегда дает что-то хорошее,
[02:08:54.180 --> 02:08:57.180]  но вот в этой точке вот эта тривиальная глупая оценка лучше.
[02:08:58.180 --> 02:09:02.180]  Поэтому в равномерном подходе у вас нет во всем классе оценок как бы наилучшей.
[02:09:03.180 --> 02:09:08.180]  Не существуют оценки, потому что вы берете вот такую глупую оценку, и она лучше чем в этой точке, чем любая другая.
[02:09:09.180 --> 02:09:10.180]  То есть несравнимая не на самом деле.
[02:09:11.180 --> 02:09:12.180]  Это понятно, да?
[02:09:12.180 --> 02:09:13.180]  Это понятно, да?
[02:09:21.180 --> 02:09:23.180]  Может больше чем в одной, может где-то...
[02:09:25.180 --> 02:09:26.180]  Нет.
[02:09:27.180 --> 02:09:29.180]  Смотри, ты здесь берешь мотожидание.
[02:09:33.180 --> 02:09:35.180]  Если ты возьмешь константную оценку,
[02:09:36.180 --> 02:09:38.180]  то только в одной точке у тебя здесь будет ноль под мотожиданием.
[02:09:38.180 --> 02:09:41.180]  Во всех остальных точках у тебя квадрат даст какое-то положительное число.
[02:09:42.180 --> 02:09:45.180]  Если просто мотожидание константной считаешь, то у тебя дальше функция риска будет...
[02:09:46.180 --> 02:09:51.180]  Как-то, ну короче, она скорее всего будет расти получается не линейно, а квадратично.
[02:09:52.180 --> 02:09:53.180]  Вот так она будет себя вести.
[02:09:55.180 --> 02:09:56.180]  Вот конкретно для этой штуки.
[02:09:57.180 --> 02:10:00.180]  Потому что у тебя тета ноль начинает отставать от вот этой вот константы фиксированной,
[02:10:01.180 --> 02:10:05.180]  возводишь квадрат, а это мотожидание константы, просто константа, и она вот так квадратично будет расти.
[02:10:05.180 --> 02:10:08.180]  А есть какая-то хорошая оценка, которая в целом всегда хорошо себя ведет,
[02:10:09.180 --> 02:10:14.180]  но в этой точке она, конечно, не такая идеальная оценка, но она как бы не сравнима с ней.
[02:10:16.180 --> 02:10:17.180]  Вот.
[02:10:18.180 --> 02:10:21.180]  Поэтому наилучшие оценки в классе всех оценок в таком подходе не существует.
[02:10:23.180 --> 02:10:26.180]  Поэтому рассматривают более узкий класс, класс несмещенных оценок.
[02:10:27.180 --> 02:10:29.180]  Вот там уже имеет смысл искать наилучшие оценки.
[02:10:36.180 --> 02:10:38.180]  Давайте зафиксируем здесь и далее с вами несмещенные оценки.
[02:10:39.180 --> 02:10:42.180]  Как класс несмещенных оценок и будем искать только среди них наилучшую.
[02:10:50.180 --> 02:10:51.180]  Вот.
[02:10:52.180 --> 02:10:55.180]  По определению, несмещенность оценки говорит о том, что
[02:10:56.180 --> 02:10:58.180]  ее мотожидание это просто тета, для любого тета.
[02:11:02.180 --> 02:11:03.180]  Так ведь?
[02:11:06.180 --> 02:11:08.180]  Что в таком случае можно сказать про функцию риска?
[02:11:14.180 --> 02:11:15.180]  Совпадает с дисперсией.
[02:11:16.180 --> 02:11:17.180]  Ну да.
[02:11:18.180 --> 02:11:22.180]  На самом деле у вас тогда функция риска, это будет просто дисперсия.
[02:11:23.180 --> 02:11:27.180]  Потому что, смотрите, у вас тета 0, это математическое ожидание
[02:11:28.180 --> 02:11:32.180]  тета со звездочкой по вот этому условию, которое мы с вами наложили.
[02:11:32.180 --> 02:11:35.180]  Соответственно, здесь у вас просто дисперсия этой оценки.
[02:11:39.180 --> 02:11:40.180]  Понятно?
[02:11:49.180 --> 02:11:50.180]  Вот.
[02:11:51.180 --> 02:11:55.180]  И дальше, получается, мы выбираем ту оценку, у которой дисперсия будет наименьшая.
[02:11:56.180 --> 02:11:57.180]  Понятно, да?
[02:12:02.180 --> 02:12:05.180]  То есть дальше у нас уже вот эта функция риска на параметры зависит не будет,
[02:12:06.180 --> 02:12:07.180]  потому что у вас всегда здесь математическое ожидание.
[02:12:08.180 --> 02:12:11.180]  У вас в таком случае просто значение функции риска – это вот дисперсия.
[02:12:12.180 --> 02:12:13.180]  Дисперсия данной оценки.
[02:12:15.180 --> 02:12:16.180]  Вот.
[02:12:17.180 --> 02:12:18.180]  И теперь встает вопрос.
[02:12:19.180 --> 02:12:21.180]  Мы хотим выбрать такую оценку, у которой самая маленькая дисперсия.
[02:12:25.180 --> 02:12:27.180]  Дисперсия снизу подперта числом 0.
[02:12:28.180 --> 02:12:29.180]  То есть дисперсия – это не отрицательное число,
[02:12:29.180 --> 02:12:32.180]  поэтому вот кажется, что мы можем бесконечно улучшать оценку,
[02:12:33.180 --> 02:12:34.180]  пока типа дисперсию не сделаем нулевую.
[02:12:35.180 --> 02:12:36.180]  На самом деле, это неправда.
[02:12:37.180 --> 02:12:38.180]  Почему это неправда?
[02:12:39.180 --> 02:12:40.180]  Потому что есть теорема…
[02:12:49.180 --> 02:12:51.180]  Теорема не равен 100 раукрамеру.
[02:12:51.180 --> 02:12:52.180]  О чем говорит неравенность раукраммера?
[02:12:53.180 --> 02:12:54.180]  Что на самом деле…
[02:12:55.180 --> 02:12:56.180]  Вот рассмотрим класс тех несмещенных оценок.
[02:13:01.180 --> 02:13:03.180]  Ну и пусть у нее конечный второй момент у этой оценки.
[02:13:07.180 --> 02:13:08.180]  Вот.
[02:13:09.180 --> 02:13:15.180]  Тогда на самом деле дисперсия этой оценки – это не отрицательное число.
[02:13:16.180 --> 02:13:17.180]  Это не отрицательное число.
[02:13:18.180 --> 02:13:19.180]  Это не отрицательное число.
[02:13:19.180 --> 02:13:27.540]  дисперсия этой оценки больше либо равна чем единицка на какую-то вот непонятную
[02:13:27.540 --> 02:13:36.820]  на какую-то константу но это если ты функцию оцениваешь если это звездочка
[02:13:36.820 --> 02:13:43.140]  несмещенная оценка для тета тогда вот так для функции если ты оцениваешь
[02:13:43.140 --> 02:13:49.500]  функцию там да действительно того что будет еще должно быть условия регулярности
[02:13:49.500 --> 02:13:55.980]  но мы сейчас задачи хотим порешать поэтому мы это проскипаем это еще не всегда выполняться
[02:13:55.980 --> 02:14:04.140]  мотош квадрата ну чтобы дисперсия была у тебя должен быть второй момент
[02:14:04.140 --> 02:14:19.660]  это информация фишер сейчас мы про нее поговорим немножко вот то есть n это n на и маленькая от
[02:14:19.660 --> 02:14:26.820]  это это информация фишера выборки сейчас мы про нее чуть-чуть поговорим вот то есть на самом
[02:14:26.820 --> 02:14:30.300]  деле неравенство у кроммера говорит что вот среди всех несмещенных оценок
[02:14:30.300 --> 02:14:38.300]  средне квадратичным подход выбираем мы считаем лучшую ту у которой меньше дисперсия
[02:14:38.300 --> 02:14:43.860]  бесконечно уменьшать вы не можете потому что она снизу подпёрта каким-то не отрицательным
[02:14:43.860 --> 02:14:52.980]  числом понятно да вот и если в этом неравенстве достигает такая оценка называется эффективной
[02:14:52.980 --> 02:14:59.620]  супер теперь дальше сейчас будет две теоремы
[02:15:22.980 --> 02:15:28.700]  но перед теоремами мы парочку определений ведем давайте сначала разберемся что такое
[02:15:28.700 --> 02:15:40.100]  информация фишера вот какими свойствами она обладает вот как она определяется и что
[02:15:40.100 --> 02:15:52.100]  она примерно обозначает вот значит сначала вводится функция вклада вклады элементов вот и
[02:15:52.100 --> 02:16:04.980]  функция вклада элементов определяется следующим образом сейчас я возьму свой конспект
[02:16:04.980 --> 02:16:26.540]  вот давайте подумаем почему оно выглядит так вот буквально
[02:16:26.540 --> 02:16:35.340]  посмотрим на то что у нас здесь записано здесь у нас по сути записано производная патета
[02:16:35.340 --> 02:16:44.260]  на патета от x вот разные разные наблюдения обладают разной информацией
[02:16:44.260 --> 02:16:51.380]  почему это так потому что смотрите вот допустим вы уже считаете что у вас распределение какое-то
[02:16:51.380 --> 02:16:56.420]  вот такое и если вам начинает приходить наблюдение из области где очень маленькая плотность то они
[02:16:56.480 --> 02:16:59.700]  несут очень много информации они говорят что скорее всего вот то что вы оценили это не совсем
[02:16:59.700 --> 02:17:05.940]  правда вот а если же приходит что-то чтобы очень сильно ожидаете что-то обычное то оно как бы меньше
[02:17:05.940 --> 02:17:11.700]  информации несет а как там теории информации там теорий кодирование типа что-то редкое больше
[02:17:11.700 --> 02:17:20.160]  информации несет чем что-то очень часто примерно такая примерно такая идея за этим стоит то есть
[02:17:20.160 --> 02:17:23.440]  Действительно, вклад выборки у каждого элемента – он
[02:17:23.440 --> 02:17:24.440]  разный.
[02:17:24.440 --> 02:17:25.440]  Вот.
[02:17:25.440 --> 02:17:29.000]  И через вот такую функцию определяется на самом деле
[02:17:29.000 --> 02:17:32.080]  информация фишера, про которую мы вот там говорили.
[02:17:32.080 --> 02:17:40.560]  Это просто усреднение вклада элемента в квадрат.
[02:17:40.560 --> 02:17:49.000]  Ну, это по сути, сколько в среднем один одно наблюдение
[02:17:49.000 --> 02:17:51.520]  несет информации о параметре.
[02:17:51.520 --> 02:17:52.520]  Покажите.
[02:17:52.520 --> 02:18:04.280]  То есть, мы ввели какой-то объект, который называется
[02:18:04.280 --> 02:18:05.280]  вклад выборки.
[02:18:05.280 --> 02:18:08.880]  Он действительно как-то связан с тем, насколько это
[02:18:08.880 --> 02:18:12.680]  ожидаемый элемент пришел относительно того, что вы
[02:18:12.680 --> 02:18:14.440]  уже оценили, или просто вообще, насколько он как
[02:18:14.440 --> 02:18:16.440]  бы много информации в себе несет.
[02:18:16.440 --> 02:18:17.440]  Вот.
[02:18:17.880 --> 02:18:20.360]  Информация одного элемента – это просто усреднение
[02:18:20.360 --> 02:18:22.440]  этого вклада по всем х.
[02:18:22.440 --> 02:18:24.600]  То есть, сколько в среднем один элемент несет в себе
[02:18:24.600 --> 02:18:25.600]  информации.
[02:18:25.600 --> 02:18:27.720]  Это есть информация по фишеру.
[02:18:27.720 --> 02:18:28.720]  Вот.
[02:18:28.720 --> 02:18:34.160]  Первое замечание важное – информация фишера – она
[02:18:34.160 --> 02:18:35.160]  линейна.
[02:18:35.160 --> 02:18:41.360]  То есть, если вы возьмете выборку из независимых
[02:18:41.360 --> 02:18:46.120]  элементов, то информация фишера по вот этим элементам
[02:18:46.120 --> 02:18:50.880]  – это все равно, что сумма информации фишера по отдельным
[02:18:50.880 --> 02:18:51.880]  кусочкам выборки.
[02:18:51.880 --> 02:18:54.880]  Что большая?
[02:18:54.880 --> 02:19:00.680]  Это по выборке уже называется.
[02:19:00.680 --> 02:19:02.200]  Ну, типа, смотрите, вы же понимаете, что вот тут мы
[02:19:02.200 --> 02:19:05.040]  определили с вами для одного элемента, аналогично можно
[02:19:05.040 --> 02:19:06.200]  определить для всей выборки.
[02:19:06.200 --> 02:19:08.200]  Ну, вы вместо плотности берете функцию правдоподобия.
[02:19:08.200 --> 02:19:13.200]  Ну, это как бы небольшой очевидный шаг для меня был.
[02:19:13.200 --> 02:19:14.200]  Ну ладно.
[02:19:15.080 --> 02:19:18.280]  Мы можем определить вклад не одного наблюдения, а
[02:19:18.280 --> 02:19:19.280]  вклад всей выборки.
[02:19:19.280 --> 02:19:20.960]  Сколько информации принесла вся выборка.
[02:19:20.960 --> 02:19:25.040]  А вся выборка принесла информации, вот здесь все
[02:19:25.040 --> 02:19:27.360]  то же самое, а здесь вместо плотности, как бы аналог
[02:19:27.360 --> 02:19:30.080]  плотности для выборки, это функция правдоподобия.
[02:19:30.080 --> 02:19:32.280]  Вот сюда можно записать функцию правдоподобия,
[02:19:32.280 --> 02:19:35.280]  и это будет вклад выборки.
[02:19:35.280 --> 02:19:38.120]  Соответственно, определяется информация фишера для всей
[02:19:38.120 --> 02:19:39.120]  выборки.
[02:19:39.120 --> 02:19:40.760]  Это просто математическое ожидание также вклада
[02:19:40.760 --> 02:19:41.760]  всей выборки в квадрате.
[02:19:41.760 --> 02:19:49.680]  Аналог плотности для выборки это функция правдоподобия.
[02:19:49.680 --> 02:19:51.400]  То есть, просто произведение плотностей во всех точках,
[02:19:51.400 --> 02:19:52.400]  которые тебе даны.
[02:19:52.400 --> 02:19:53.400]  Вот.
[02:19:53.400 --> 02:19:57.960]  Ну, дальше утверждение, что вот если у вас есть две
[02:19:57.960 --> 02:20:02.040]  выборки x и y, и элементы независимые, то на самом
[02:20:02.040 --> 02:20:04.520]  деле у вас информация фишеры линейна.
[02:20:04.520 --> 02:20:07.600]  То есть, на самом деле, почему в неравенстве Рау Крамера
[02:20:07.600 --> 02:20:09.800]  в знаменателе стоит вот такая штука, которую мы
[02:20:09.800 --> 02:20:10.800]  написали?
[02:20:10.800 --> 02:20:13.440]  На самом деле, потому что информация фишеры линейна.
[02:20:13.440 --> 02:20:15.640]  Можно было написать единичку поделительную информацию
[02:20:15.640 --> 02:20:19.640]  фишера всей выборки, но так как информация фишеры
[02:20:19.640 --> 02:20:21.720]  линейна, можно посчитать информацию фишеры одного
[02:20:21.720 --> 02:20:25.160]  элемента и умножить на это.
[02:20:25.160 --> 02:20:28.080]  Вот эквивалентность этих записей следует из линейности
[02:20:28.080 --> 02:20:31.080]  информации фишеры.
[02:20:31.080 --> 02:20:33.720]  Так, что мы дальше хотим понять.
[02:20:33.720 --> 02:20:42.120]  А, про информацию фишеры мы с вами сказали, и теперь
[02:20:42.120 --> 02:20:46.240]  погнали в пару теорем, которые нам нужны будут сейчас для
[02:20:46.240 --> 02:20:47.240]  решения следующей задачи.
[02:20:47.240 --> 02:20:51.320]  Теорема один.
[02:20:51.320 --> 02:21:01.760]  Так, сейчас я её найду.
[02:21:01.760 --> 02:21:03.400]  Эффективная оценка существует не всегда.
[02:21:03.400 --> 02:21:06.040]  Ещё раз, эффективная оценка — это такая оценка, что
[02:21:06.040 --> 02:21:08.160]  в неравенстве Рау Крамера достигается равенство.
[02:21:08.160 --> 02:21:16.640]  Эффективная оценка существует тогда и только тогда, когда
[02:21:16.640 --> 02:21:20.520]  семейство распределений, которое вы оцениваете, экспоненциально.
[02:21:20.520 --> 02:21:30.520]  Вот.
[02:21:31.520 --> 02:21:32.520]  Что такое экспоненциальное семейство распределений?
[02:21:32.520 --> 02:21:35.520]  Это распределения, у которых хотя бы одна плотность имеет
[02:21:35.520 --> 02:21:36.520]  вот такой вид.
[02:21:36.520 --> 02:22:01.520]  Ну, у вас же плотностей много.
[02:22:01.520 --> 02:22:03.960]  Они определены с точностью до почти наверно.
[02:22:03.960 --> 02:22:05.960]  Если хотя бы один вид плотности имеет вот такой
[02:22:05.960 --> 02:22:10.960]  вид, то всё семейство мы называем экспоненциальным.
[02:22:10.960 --> 02:22:13.960]  Ну, ты можешь на множестве меры ноль плотность поменять,
[02:22:13.960 --> 02:22:15.960]  и у тебя интеграл ведь никак не поменяется.
[02:22:15.960 --> 02:22:17.960]  Ты можешь какие-нибудь точки повыкалывать, и у тебя
[02:22:17.960 --> 02:22:18.960]  всё равно плотность будет та же самая, по сути.
[02:22:18.960 --> 02:22:21.960]  Ну, а если одна плотность экспоненциальна, то и другие
[02:22:21.960 --> 02:22:22.960]  плотности?
[02:22:22.960 --> 02:22:24.960]  Ну, просто если хотя бы какая-нибудь плотность вот
[02:22:24.960 --> 02:22:27.960]  в таком виде представима, то всё, это экспоненциальное
[02:22:27.960 --> 02:22:28.960]  семейство.
[02:22:28.960 --> 02:22:31.960]  Не семейство экспоненциальное, а остальные плотности.
[02:22:31.960 --> 02:22:32.960]  Это параметр.
[02:22:32.960 --> 02:22:33.960]  Сейчас проговорим с вами.
[02:22:33.960 --> 02:22:34.960]  Вот.
[02:22:34.960 --> 02:22:38.960]  Давайте с вами поймём, что такое тета, и какие вообще
[02:22:38.960 --> 02:22:41.960]  распределения можно назвать, относящимися к экспоненциальному
[02:22:41.960 --> 02:22:42.960]  распределению.
[02:22:42.960 --> 02:22:43.960]  Ну, я не знаю.
[02:22:43.960 --> 02:22:44.960]  Я не знаю.
[02:22:44.960 --> 02:22:45.960]  Я не знаю.
[02:22:45.960 --> 02:22:46.960]  Я не знаю.
[02:22:46.960 --> 02:22:47.960]  Я не знаю.
[02:22:47.960 --> 02:22:48.960]  Я не знаю.
[02:22:48.960 --> 02:22:49.960]  Я не знаю.
[02:22:49.960 --> 02:22:53.960]  И какие вообще распределения можно назвать, относящимися
[02:22:53.960 --> 02:22:56.960]  к экспоненциальному распределению?
[02:22:56.960 --> 02:22:59.960]  Так.
[02:22:59.960 --> 02:23:08.960]  Значит, вот в этом определении нужно ещё немножко доделать
[02:23:08.960 --> 02:23:09.960]  его.
[02:23:09.960 --> 02:23:10.960]  Продолжим.
[02:23:10.960 --> 02:23:11.960]  То есть, смотрите.
[02:23:11.960 --> 02:23:12.960]  Эффективная оценка существует.
[02:23:12.960 --> 02:23:16.940]  Тогда и только тогда вы оцениваете что-то в экспоненциальном
[02:23:16.940 --> 02:23:17.940]  семействе распределений.
[02:23:17.940 --> 02:23:23.380]  семейство распределений, это такое семейство, для которых плотность имеет вот такой вид,
[02:23:23.380 --> 02:23:33.300]  где а0, θ и так далее, аk, θ — это какие-то линейные зависимые функции.
[02:23:33.300 --> 02:23:42.500]  Вот, аk — это просто количество параметров в модели.
[02:23:42.500 --> 02:23:58.260]  Это просто какая-то статистика. Давайте просто пример сейчас приведем и поймем сами.
[02:23:58.260 --> 02:24:03.540]  Давайте рассмотрим самое какое-то банальное семейство,
[02:24:03.540 --> 02:24:07.060]  относящееся к экспоненциальному семейству, экспоненциальные распределения.
[02:24:07.060 --> 02:24:20.580]  Плотность у них имеет вид. Вообще, λe в степени минус λх на индикатор х больше либо равна нулю,
[02:24:20.580 --> 02:24:23.300]  но вообще она представима вот в таком виде.
[02:24:23.300 --> 02:24:33.060]  Вот это вот, это ваш h от x, функция, которая от параметра никак не зависит,
[02:24:33.060 --> 02:24:39.060]  мы ее как бы как множитель вынесли. Вот, экспонента, а дальше мы просто с вами логарифмируем по сути
[02:24:39.060 --> 02:24:47.220]  то, что здесь написано, то есть это будет логарифм λ, плюс, вернее, минус, λх.
[02:24:47.220 --> 02:24:57.100]  Поняли? Вот, мы получили ровно в таком виде. То есть смотрите, плотность ваша распадает на что-то,
[02:24:57.100 --> 02:25:01.860]  что не зависит от параметра. В нашем случае вот, этот индикатор, он никак не зависит от параметра.
[02:25:01.860 --> 02:25:11.260]  И на экспоненту в степени вот такая сумма. А 0 это вот какая-то функция от параметров,
[02:25:11.260 --> 02:25:17.820]  которая не зависит никак от элемента выборки. Вот она, это логарифм λ. И дальше идет к слагаемым,
[02:25:17.820 --> 02:25:22.620]  где к – это количество параметров. В нашем случае параметры только один, поэтому у нас одно слагаемое,
[02:25:22.620 --> 02:25:34.420]  которое от х будет зависеть. Это вот лямда х. Понятно. Ну и очевидно, что логарифм лямда и лямда – это
[02:25:34.420 --> 02:25:39.580]  линейная независимая функция. Вот. Следовательно, мы получили, что вот экспоненциальное семейство,
[02:25:39.580 --> 02:25:43.420]  семейство экспоненциальных распределений, действительно относится к экспоненциальному
[02:25:43.420 --> 02:25:49.260]  семейству. Вот, упражнения. В этом блоке вас могут попросить показать, что там нормальное распределение,
[02:25:49.260 --> 02:25:53.300]  бета-распределение, гамма-распределение и всякие остальные распределения относятся к этому
[02:25:53.300 --> 02:25:57.340]  семейству. Просто плотность раскладываете? Да, просто плотность раскладываете, показываете,
[02:25:57.340 --> 02:26:02.380]  что оно действительно представимо вот в таком виде, где вот эти функции линейной независимой ровно
[02:26:02.380 --> 02:26:14.140]  к, ну и т.е. к плюс 1 на самом деле. Вот. Если это выполнено, то на самом деле ваше семейство,
[02:26:14.140 --> 02:26:18.620]  которое вы рассматриваете, экспоненциально. И вот теорема утверждает, что эффективная
[02:26:18.620 --> 02:26:23.020]  оценка существует тогда и только тогда, когда вы работаете с экспоненциальным семейством распределений.
[02:26:23.020 --> 02:26:29.100]  Т.е. пример неэкспоненциального семейства распределений, например, это равномерное
[02:26:29.100 --> 02:26:34.620]  распределение на отрезке 0,1. Неэкспоненциальное. Неэкспоненциальное. Ну, у вас там просто константа,
[02:26:34.620 --> 02:26:38.980]  по сути, которую как экспоненту никак не расписать. Ну, у вас там индикатор,
[02:26:39.060 --> 02:26:48.700]  поделись на константу. Она как экспоненту не расписывается. Вот. Это первая теорема. Соответственно,
[02:26:48.700 --> 02:26:53.060]  если вам дадут в данном блоке какую-нибудь задачу с неэкспоненциальным семейством,
[02:26:53.060 --> 02:26:57.020]  ну непонятно, что делать, что от вас хотят. Наверное, просто бы как-то их посравнивать
[02:26:57.020 --> 02:27:00.900]  в средне квадратичном подходе, посчитать к ним дисперсии, чем-нибудь еще. Но вот теоремы
[02:27:00.900 --> 02:27:09.420]  Рао Краймера и все дальше уже можно, в принципе, не применять. Хотя, на самом деле, применять можно,
[02:27:09.420 --> 02:27:16.660]  но как бы эффективную оценку вы никак не построите не для экспоненциального семейства. Вот. Сейчас
[02:27:16.660 --> 02:27:21.140]  могут спросить, существует ли для данного распределения какая-нибудь эффективная оценка.
[02:27:21.140 --> 02:27:29.820]  Вы скажете, нет, не существует, потому что оно неэкспоненциальное, допустим. Да. Тут как бы критерий.
[02:27:29.820 --> 02:27:36.580]  Это первая теорема. Вот. На самом деле, с экспоненциальными семействами там очень много
[02:27:36.580 --> 02:27:39.220]  содержательных задач, и чтобы их решать, нам нужны еще две теоремы.
[02:27:43.220 --> 02:27:54.540]  Про информацию Fisher мы пока с вами немножко забудем. Еще парочку утверждений.
[02:27:59.820 --> 02:28:12.180]  Не представимо в таком виде. Ну, это, кстати, интересный вопрос.
[02:28:12.180 --> 02:28:18.420]  Вот определение экспоненциального типа, что представимо в таком виде, а как показать, что
[02:28:18.420 --> 02:28:36.740]  непредставимо. Так, есть, наверное, какие-то функции, которые в виде экспонента непредставимы.
[02:28:36.740 --> 02:28:44.060]  Хороший вопрос, я подумаю. Давайте пока дальше. Вопрос хороший, я постараюсь на него ответить.
[02:28:44.060 --> 02:28:47.620]  Так. Теорема два, которая нам понадобится.
[02:28:47.620 --> 02:29:07.180]  Критерии эффективности.
[02:29:14.060 --> 02:29:22.100]  Вот. Получается, теорема номер один говорит о том, что у вас эффективная оценка существует
[02:29:22.100 --> 02:29:26.540]  тогда и только тогда, когда вы работаете с экспоненциальным семейством распределений.
[02:29:26.540 --> 02:29:33.620]  Теорема номер два говорит, что если у вас есть какая-то эффективная оценка, вот эта эффективная
[02:29:33.620 --> 02:29:38.940]  оценка, то вот это, то есть вы можете сам параметр оценивать, можете функцию какую-то параметр
[02:29:38.940 --> 02:29:53.580]  оценивать. Вот. То она на самом деле линейно зависит от клаба выборки. Вот. Вот это мы с вами уже
[02:29:53.580 --> 02:30:00.340]  поняли, что это такое, да? Это там типа производная, ну, природная логарифма, а там функции
[02:30:00.340 --> 02:30:05.740]  правдоподобия для всей выборки. Вот. И более того, что можно сказать, что вот это равенство
[02:30:05.860 --> 02:30:18.820]  выполнено тогда и только тогда, когда вот C от T представимо вот в таком виде. То есть как
[02:30:18.820 --> 02:30:24.580]  бы в некотором смысле все эффективные оценки для модели, они линейно между собой зависимы,
[02:30:24.580 --> 02:30:31.940]  потому что они у вас линейно зависимы складом выборки, соответственно, они между собой будут
[02:30:31.940 --> 02:30:36.260]  тоже все линейно зависимы. То есть если вы найдете хотя бы одну эффективную оценку, вы найдете их все,
[02:30:36.260 --> 02:30:40.500]  потому что все остальные будут просто линейно от нее зависеть. Это мы сейчас воспользуемся этим фактом.
[02:30:40.500 --> 02:30:53.660]  Вот. Информация фишера от выборки, да. Так, и наверное нам уже нужна еще одна теорема какая-нибудь.
[02:30:53.660 --> 02:31:03.740]  Или достаточно пока что? Слушайте, а пока что достаточно. Мы с вами только давайте еще
[02:31:03.740 --> 02:31:07.820]  вот одно утверждение докажем вспомогательное, которое очень помогает при решении задач.
[02:31:07.820 --> 02:31:27.940]  Давайте мы с вами поймем, что найти эффективную оценку для экспоненциальной модели,
[02:31:27.940 --> 02:31:51.540]  когда у вас параметры один, очень просто. Так, утверждение, которое нам поможет при решении задач.
[02:31:51.540 --> 02:32:05.340]  Пусть у вас семейство экспоненциально, и пусть размерность параметра это единичка.
[02:32:05.340 --> 02:32:19.300]  В таком случае на самом деле очень легко сразу предъявить для какой функции от параметра
[02:32:19.300 --> 02:32:26.140]  существует эффективная оценка. В таком случае вот через вот такую запись. В таком случае
[02:32:26.140 --> 02:32:33.380]  вот такая статистика, то есть усреднение t от x будет эффективной оценкой.
[02:32:50.300 --> 02:32:57.420]  Чтобы это утверждение доказать, нужно немножко понимать, как доказывается вот это утверждение,
[02:32:57.420 --> 02:33:01.460]  что эффективная оценка существует тогда и только тогда, когда экспоненциальное семейству существует.
[02:33:01.460 --> 02:33:10.100]  Давайте мы сейчас это аккуратно с вами докажем. Данного? Смотрите, мы сказали,
[02:33:10.100 --> 02:33:15.460]  что эффективная оценка существует только если вы что-то оцениваете в экспоненциальном семействе.
[02:33:15.460 --> 02:33:22.860]  Возможно у вас не существуют несмещенные оценки для самого параметра θ. Возможно у вас
[02:33:22.860 --> 02:33:30.980]  существует несмещенная оценка только для какой-то функции от θ. Мы знаем, что все такие эффективные
[02:33:30.980 --> 02:33:35.860]  оценки между собой связаны линейно. То есть на самом деле, если у вас существует несмещенная
[02:33:35.860 --> 02:33:41.300]  оценка для tau от θ, то у вас будут проблемы для поиска несмещенной оценки вот для такой
[02:33:41.300 --> 02:33:46.180]  функции tau квадрат θ, потому что они не линейны между собой. То есть на самом деле,
[02:33:46.180 --> 02:33:49.300]  чтобы найти какую-нибудь эффективную оценку, вам достаточно предъявить хотя бы одну,
[02:33:49.300 --> 02:33:55.780]  а все остальные от нее будут зависеть линейно. Вот, и в одномерном случае найти хотя бы одну
[02:33:55.780 --> 02:34:01.980]  очень просто. Утверждение вот на доске, что чтобы найти эффективную оценку, можно рассмотреть вот
[02:34:01.980 --> 02:34:08.020]  такую оценочку. t от y берется вот из-за определения экспоненциального семейства. Давайте это с вами
[02:34:08.020 --> 02:34:17.820]  докажем. Это как раз таки вот функция с параметром. Доказательство. Оно очень простое.
[02:34:17.820 --> 02:34:35.860]  Вот, докажем утверждение с вами сейчас. Давайте рассмотрим, как у нас выглядит.
[02:34:35.860 --> 02:34:53.940]  Функция вклада от выборки. По определению. Что это у нас такое? Нет, это информация фишер.
[02:34:53.940 --> 02:35:05.580]  А вклад выборки? Мы считаем, что параметры у нас один. Логарифм. Ну и давайте сразу для
[02:35:05.580 --> 02:35:15.100]  выборки, поэтому у нас есть функция правдоподобия. Но мы с вами знаем, что так как семейство
[02:35:15.100 --> 02:35:23.140]  экспоненциальное, то у него плотность представима вот в таком виде. Справедливо? Да, давайте мы с
[02:35:23.140 --> 02:35:28.020]  вами вот просто воспользуемся вот этим определением и поставим его вот сюда. Что мы с вами получим?
[02:35:28.020 --> 02:35:37.460]  Мы с вами получим, что вот это от x. Так, что мы хотим сказать про это? Смотрите, логарифмы h от x при
[02:35:37.460 --> 02:35:42.100]  логарифмировании вылезут в отдельные слагаемые, поэтому когда мы будем брать от них производные,
[02:35:42.100 --> 02:35:49.460]  они все зановятся. Останется на самом деле только вот то, что вот здесь в экспоненте. Так, по порядку.
[02:35:49.460 --> 02:35:54.820]  Смотрите, мы хотим понять, как у нас выглядит вклад наблюдения.
[02:36:10.820 --> 02:36:14.020]  Вот это функция ваша.
[02:36:14.020 --> 02:36:26.660]  А 0 и 1 вот у вас из определения экспоненциального семейства идет. Еще раз у вас параметр одномерный,
[02:36:26.660 --> 02:36:32.980]  у вас только 0 и 1 существует. Еще раз, давайте зарекапим. Теорема 1.
[02:36:32.980 --> 02:36:40.020]  Эффективная оценка существует тогда и только тогда, когда рассматривая семейство распределения
[02:36:40.020 --> 02:36:45.060]  экспоненциально. Экспоненциально значит ее функция плотности представима вот в таком виде.
[02:36:45.060 --> 02:36:52.740]  Это первое утверждение. Второе утверждение. Давайте тогда к утверждению перейдем сразу.
[02:36:52.740 --> 02:36:59.700]  Смотрите, если у нас одномерный случай, то вот такая оценочка из вот этого определения t от x
[02:36:59.700 --> 02:37:04.660]  средняя будет эффективной оценкой для вот такой функции от параметра.
[02:37:05.300 --> 02:37:09.300]  Что значит средняя? У нас же там таблик, это просто какая-то функция от x.
[02:37:09.300 --> 02:37:13.300]  Да, но это смотри, для одного наблюдения у тебя же здесь их будет целая выборка.
[02:37:13.300 --> 02:37:22.980]  Сейчас докажем. Из доказательства будет понятно. Договорились?
[02:37:22.980 --> 02:37:28.820]  Окей, что мы хотим сделать с вами? Мы хотим понять, как у нас выглядит вот эта штука.
[02:37:28.820 --> 02:37:37.300]  Как выглядит вклад наблюдения? По определению, это просто производная патета, логарифма функции
[02:37:37.300 --> 02:37:41.300]  правдоподобия, ну для выборки. Для одного элемента это просто плотность, для выборки это функция
[02:37:41.300 --> 02:37:48.500]  правдоподобия. Давайте ее... Функция правдоподобия у нас по условию, так как семейство экспоненциально выглядит
[02:37:48.500 --> 02:37:58.180]  таким образом. Давайте логарифмировать. У вас вылезут слагаемые с h от x, производные патеты,
[02:37:58.180 --> 02:38:03.300]  они все за нулятых, потому что вот эти штуки от это не зависят. У вас по сути останутся только слагаемые
[02:38:03.300 --> 02:38:13.460]  производные вот таких штучек. То есть на самом деле здесь у вас получится просто n а 0 штрих
[02:38:13.460 --> 02:38:36.340]  плюс сумма t и x на а1. Согласны с этим?
[02:38:36.340 --> 02:38:46.100]  Экспонента у нас пропала при логарифмировании. При логарифмировании у нас вылезли логарифмы вот этих
[02:38:46.100 --> 02:38:51.700]  вот слагаемых, которые от это не зависят, они у нас занулились, потому что мы берем производную. И остались у нас
[02:38:51.700 --> 02:39:00.100]  еще... Мы взяли еще логарифм экспоненты. По сути у нас мы складываем вот это для n элементов выборки,
[02:39:00.100 --> 02:39:02.100]  ну потому что у нас здесь функция правдоподобия.
[02:39:02.100 --> 02:39:04.100]  А что там написано плюс?
[02:39:04.100 --> 02:39:06.100]  Перепишу.
[02:39:10.100 --> 02:39:18.100]  Плюс а1 штрих в точке θ, сумма t и x их по и от единички до n.
[02:39:18.100 --> 02:39:20.100]  Так, куда n?
[02:39:20.100 --> 02:39:26.100]  n, потому что у вас здесь для выборки. Выборка размером n.
[02:39:26.100 --> 02:39:32.100]  Там произведение плотностей записано? Ты их логарифмируешь?
[02:39:32.100 --> 02:39:34.100]  Становится сумма их.
[02:39:38.100 --> 02:39:42.100]  Ну возьмем логарифм от вот этого вот. От произведения вот таких штук.
[02:39:42.100 --> 02:39:46.100]  А это p и x, вот эта вот штукка, которая у нас записана, записана где x это одна звучанная или что?
[02:39:46.100 --> 02:39:48.100]  Вот снизу, тут оттуда сейчас покажешь.
[02:39:48.100 --> 02:39:52.100]  Да, это p от x, это плотность.
[02:39:52.100 --> 02:39:58.100]  Это размерность параметра.
[02:39:58.100 --> 02:40:00.100]  Что?
[02:40:00.100 --> 02:40:02.100]  Это размерность параметра.
[02:40:08.100 --> 02:40:12.100]  Ну да, то есть у тебя может быть распределение, которое двух параметров, а трех зависит.
[02:40:12.100 --> 02:40:18.100]  Мы сейчас рассматриваем частный случай, когда у тебя параметр одномерный, то есть один.
[02:40:18.100 --> 02:40:26.100]  И в таком случае у тебя просто а1 от тета. То есть из всех этих слагаемых у тебя здесь только одно слагаемое, а один от тета.
[02:40:26.100 --> 02:40:30.100]  А откуда вот эти n слагаемых появляются? Ну потому что это функция правдоподобия.
[02:40:30.100 --> 02:40:34.100]  Это просто произведение n плотностей, мы их логарифмируем и берем производную.
[02:40:34.100 --> 02:40:38.100]  Ну в общем, если посчитаете, у вас получится ровно вот такая штука.
[02:40:38.100 --> 02:40:40.100]  Берем?
[02:40:40.100 --> 02:40:42.100]  Что еще намножили?
[02:40:42.100 --> 02:40:44.100]  Ну вот тут наверное, ну второе слагаемое.
[02:40:44.100 --> 02:40:46.100]  Вот это?
[02:40:46.100 --> 02:40:48.100]  Найдомножили, а второе слагаемое?
[02:40:48.100 --> 02:40:50.100]  Да потому что здесь у тебя n слагаемых.
[02:40:52.100 --> 02:40:56.100]  Они разные, их нельзя найдомножать.
[02:40:56.100 --> 02:40:58.100]  Так, ладно, давайте аккуратно доделаем.
[02:40:58.100 --> 02:41:02.100]  Хорошо, а как у нас будет выглядеть функция правдоподобия в данном случае?
[02:41:02.100 --> 02:41:22.100]  Это будет просто произведение h от x экспонента a0 плюс a1 tx.
[02:41:22.100 --> 02:41:30.100]  Tx и t. Согласен? Егор?
[02:41:30.100 --> 02:41:32.100]  Произведение?
[02:41:32.100 --> 02:41:38.100]  Да, функция правдоподобия, произведение от единички dn плотностей в каждой точке.
[02:41:38.100 --> 02:41:42.100]  Я уже понял, почему я что-то спрашивал.
[02:41:42.100 --> 02:41:44.100]  Почему вот это верно, да?
[02:41:44.100 --> 02:41:48.100]  Все, берем логарифм от этого, у нас выживают только вот эти слагаемые.
[02:41:48.100 --> 02:41:50.100]  И у нас их n штук.
[02:41:50.100 --> 02:41:54.100]  N на ноль и плюс tx и t на a1 штрих.
[02:41:54.100 --> 02:41:58.100]  Штрихи появляются производные, потому что мы берем производные.
[02:41:58.100 --> 02:42:02.100]  До этого шага всем понятно? Садык?
[02:42:02.100 --> 02:42:08.100]  Ну, до этого...
[02:42:08.100 --> 02:42:12.100]  Все, отлично. Мы в одном шаге с вами.
[02:42:12.100 --> 02:42:16.100]  На самом деле, товарищи, мы в одном шаге до победы.
[02:42:16.100 --> 02:42:22.100]  Почему? Потому что, смотрите, у нас есть вот эта теорема, критерии эффективности.
[02:42:22.100 --> 02:42:24.100]  Критерии эффективности что утверждает?
[02:42:24.100 --> 02:42:28.100]  Что если у вас есть эффективная оценка, то она линейно зависит от вклада в выборке.
[02:42:28.100 --> 02:42:32.100]  У нас почти есть линейная зависимость от вклада в выборке.
[02:42:34.100 --> 02:42:36.100]  Понимаете?
[02:42:36.100 --> 02:42:38.100]  Почему?
[02:42:38.100 --> 02:42:40.100]  Почему вы не понимаете?
[02:42:50.100 --> 02:42:53.100]  Понятно, кто от чего зависит, кто от чего должен голосовать.
[02:42:53.100 --> 02:42:57.100]  Ну, то есть, я не вижу аналога, я не утвержден.
[02:42:57.100 --> 02:42:59.100]  Хорошо, мы сейчас аккуратно это выпишем.
[02:42:59.100 --> 02:43:01.100]  А мы доказываем теорию.
[02:43:01.100 --> 02:43:05.100]  Нет, мы доказываем утверждение.
[02:43:05.100 --> 02:43:07.100]  Мы хотим воспользоваться теоремой.
[02:43:07.100 --> 02:43:09.100]  Мы хотим сейчас воспользоваться теоремой. Это критерии эффективности.
[02:43:09.100 --> 02:43:15.100]  Если мы показываем, что наша оценка представимого в таком виде, то все будет хорошо.
[02:43:15.100 --> 02:43:19.100]  Давайте поделим на n и поделим на a1, наверное.
[02:43:19.100 --> 02:43:21.100]  Ну да.
[02:43:21.100 --> 02:43:23.100]  Что у нас получится?
[02:43:23.100 --> 02:43:27.100]  У нас получится aθ от х поделить на n, а1 от это.
[02:43:27.100 --> 02:43:31.100]  Ну, полагаем, что это все там в ноль не обращается, и с этим все хорошо будет.
[02:43:31.100 --> 02:43:33.100]  Это будет a0.
[02:43:41.100 --> 02:43:43.100]  А1т от это.
[02:43:43.100 --> 02:43:44.100]  Плюс?
[02:43:44.100 --> 02:43:45.100]  Т средний.
[02:43:45.100 --> 02:43:46.100]  Т средний, да.
[02:43:46.100 --> 02:43:48.100]  Ну, как раз таки, т средний.
[02:43:48.100 --> 02:43:50.100]  Как раз таки, т средний.
[02:43:52.100 --> 02:43:54.100]  Что мы с вами получили?
[02:43:54.100 --> 02:43:58.100]  Мы получили, что вот, это наша оценка, ну или статистика.
[02:43:58.100 --> 02:44:00.100]  Вот, это она.
[02:44:02.100 --> 02:44:04.100]  Минус функция, которую мы оцениваем.
[02:44:04.100 --> 02:44:06.100]  Функция, которую мы оцениваем, вот она.
[02:44:11.100 --> 02:44:16.100]  Линейно зависит, то есть вот это какой-то коэффициент, который зависит от это.
[02:44:16.100 --> 02:44:18.100]  От вклада в выборке.
[02:44:18.100 --> 02:44:22.100]  Вот у вас вклад в выборке, а вот у вас коэффициент, который зависит от этого.
[02:44:22.100 --> 02:44:24.100]  Понятно?
[02:44:28.100 --> 02:44:34.100]  Все, мы получили с вами в одномерном случае способ, как построить эффективную оценку.
[02:44:38.100 --> 02:44:40.100]  Понятненько?
[02:44:41.100 --> 02:44:45.100]  Вот, то есть мы просто с вами, в доказательстве этой теоремы мы чем с вами воспользовались?
[02:44:45.100 --> 02:44:51.100]  Мы просто каким видом у нас выглядит плотность экспоненциального распределения,
[02:44:51.100 --> 02:44:54.100]  записали просто аккуратно, как у нас выглядит вклад в выборке,
[02:44:54.100 --> 02:44:57.100]  и воспользовались теоремой о критерии эффективности,
[02:44:57.100 --> 02:45:05.100]  о том, что у нас оценка эффективна тогда и только тогда, когда она линейно зависит от вклада в выборке.
[02:45:06.100 --> 02:45:11.100]  Отлично, все, вот с этим уже можно решать практически любые задачи, которые у вас могут быть.
[02:45:14.100 --> 02:45:16.100]  Какие задачи у вас могут быть?
[02:45:16.100 --> 02:45:23.100]  Первое, ну, наверное, нужно будет понять, ого, нужно будет понять, является ли семейство экспоненциальным.
[02:45:23.100 --> 02:45:28.100]  Это вот лучше выписать, потому что так аккуратнее и, возможно, вам это поможет в решении.
[02:45:30.100 --> 02:45:33.100]  Мы сейчас с вами сразу задачку решим.
[02:45:36.100 --> 02:45:38.100]  Ну, пока не придут, значит.
[02:45:57.100 --> 02:45:58.100]  Так, задача.
[02:46:05.100 --> 02:46:07.100]  Так, задачка номер четыре.
[02:46:07.100 --> 02:46:15.100]  Пусть у нас есть выборка, x1 и так далее, xn, из геометричества с параметром f.
[02:46:17.100 --> 02:46:18.100]  Встает вопрос.
[02:46:19.100 --> 02:46:30.100]  Первый, найдите какую-нибудь эффективную оценку, любая, для какой-нибудь функции от параметра.
[02:46:30.100 --> 02:46:33.100]  Для какой-нибудь функции от параметра.
[02:46:33.100 --> 02:46:37.100]  То есть, найти какую-нибудь функцию от параметра, для которой существует эффективная оценка.
[02:46:37.100 --> 02:46:38.100]  Это первый вопрос.
[02:46:39.100 --> 02:46:43.100]  А второй вопрос, найти информацию фишер одного наблюдения.
[02:46:44.100 --> 02:46:48.100]  Мы сейчас эту задачу просто двумя способами решим, и все будет хорошо.
[02:46:49.100 --> 02:46:50.100]  Первая.
[02:46:51.100 --> 02:46:54.100]  Так, ну, про дискретную плотность вам, наверное, уже говорили, да?
[02:46:55.100 --> 02:46:59.100]  Что функция вероятности, вот, которую у нас, помните, была в дискретном случае.
[02:46:59.100 --> 02:47:01.100]  То есть, вы каждой точке сопоставляете вероятность.
[02:47:01.100 --> 02:47:03.100]  На самом деле, тоже является плотность в посчитающей мере.
[02:47:05.100 --> 02:47:06.100]  Отлично.
[02:47:06.100 --> 02:47:12.100]  Соответственно, давайте запишем с вами просто плотность геометрического распределения.
[02:47:12.100 --> 02:47:14.100]  Как задается?
[02:47:16.100 --> 02:47:18.100]  Ну, задается как 1 минус по степени x на p.
[02:47:19.100 --> 02:47:23.100]  Геометрическое распределение говорит о том, сколько неудач должно произойти до первой удачи.
[02:47:23.100 --> 02:47:26.100]  То есть, вот, 1 на минус p – это вероятность неудачи.
[02:47:27.100 --> 02:47:30.100]  В степени x – это сколько у нас таких неудач произошло, и потом должна произойти удача.
[02:47:31.100 --> 02:47:33.100]  И х у нас целое?
[02:47:33.100 --> 02:47:34.100]  Да.
[02:47:35.100 --> 02:47:37.100]  Ну, считающая мера, мы считаем, что х у нас, типа, целое.
[02:47:38.100 --> 02:47:43.100]  В данном случае х принадлежит к концу, то есть, натуральным числам.
[02:47:45.100 --> 02:47:46.100]  Вот.
[02:47:46.100 --> 02:47:48.100]  И является ли вот это экспоненциальным распределением?
[02:47:50.100 --> 02:47:51.100]  Да.
[02:47:51.100 --> 02:47:52.100]  Да.
[02:47:52.100 --> 02:47:53.100]  Ну, ответ правильный.
[02:47:53.100 --> 02:47:54.100]  Почему?
[02:47:54.100 --> 02:47:56.100]  Потому что плотность представима вот в таком виде, в каком нам надо.
[02:47:56.100 --> 02:47:58.100]  А в таком виде это в каком?
[02:47:58.100 --> 02:48:00.100]  Берем экспонент, или как?
[02:48:01.100 --> 02:48:05.100]  Берем п, экспонента, 4.
[02:48:06.100 --> 02:48:10.100]  Нет, p, видите, мы все в экспоненту должны, что от параметра зависит, должны закинуть в экспоненту.
[02:48:11.100 --> 02:48:14.100]  У вас получится экспонента, логарифм.
[02:48:15.100 --> 02:48:16.100]  Логарифм от этого это что будет?
[02:48:16.100 --> 02:48:20.100]  Логарифм p, наверное, плюс х, логарифм 1 минус p.
[02:48:22.100 --> 02:48:26.100]  Вот в таком виде у вас представима функция плотности.
[02:48:27.100 --> 02:48:28.100]  Высадим согласны?
[02:48:28.100 --> 02:48:29.100]  Отлично.
[02:48:29.100 --> 02:48:31.100]  Значит, это экспоненциальные распределения.
[02:48:31.100 --> 02:48:38.100]  То есть, на самом деле и биномиальные, и вернули, и геометрическое, вот такие дискреты распределения тоже входят в класс экспоненциальных распределений.
[02:48:39.100 --> 02:48:43.100]  Значит, для них есть смысл искать эффективную оценку.
[02:48:44.100 --> 02:48:46.100]  То есть, условия задачи нормальные.
[02:48:48.100 --> 02:48:51.100]  Теперь давайте воспользуемся утверждением, которое мы только что доказали.
[02:48:51.100 --> 02:49:01.100]  Мы только что с вами доказали, что существует эффективная оценка для вот a0 штрих тета поделить на a1 штрих тета.
[02:49:05.100 --> 02:49:11.100]  В нашем конкретном случае a0 это логарифм p, а 1 это логарифм 1 минус p.
[02:49:13.100 --> 02:49:15.100]  Таким образом, мы с вами получаем что?
[02:49:15.100 --> 02:49:38.100]  Так, t от x у нас это просто x, то есть, x средняя это эффективная оценка для производного логарифма это 1 на p, здесь еще минусик будет, 1 на 1 минус p.
[02:49:40.100 --> 02:49:43.100]  Да, должен вылезти минус вот здесь, когда будете производным брать.
[02:49:43.100 --> 02:49:50.100]  Вот, ну и получается это эффективная оценка для 1 минус p поделить на p. Справедливо?
[02:49:53.100 --> 02:49:57.100]  Все, мы нашли с вами функцию от p, для которой существует эффективная оценка.
[02:50:02.100 --> 02:50:03.100]  Утверждение.
[02:50:05.100 --> 02:50:09.100]  t от x является эффективной оценкой вот такой штуки, когда параметр одномерный.
[02:50:10.100 --> 02:50:15.100]  Параметр у нас одномерный, вот это t от x это просто t от x по сути в твоей плотности.
[02:50:17.100 --> 02:50:26.100]  Логарифм 1 минус p это a1 от teta, ну a1 от p, а логарифм p это a0 от p.
[02:50:27.100 --> 02:50:30.100]  Ну и соответственно пользуемся утверждениями, которые мы только что с вами показали.
[02:50:30.100 --> 02:50:36.100]  Таким образом, мы нашли с вами функцию от p, для которой существует эффективная оценка.
[02:50:40.100 --> 02:50:44.100]  Супер, что мы дальше хотим сказать?
[02:50:45.100 --> 02:50:47.100]  Как будем искать информацию фишера?
[02:50:48.100 --> 02:50:51.100]  Информацию фишера можно найти двумя способами.
[02:50:51.100 --> 02:50:58.100]  По определению, то есть мы сначала считаем, как выглядит вклад наблюдения, а потом считаем информацию фишера через интеграл квадрата.
[02:50:59.100 --> 02:51:02.100]  И второй вариант, воспользоваться критериям эффективности.
[02:51:03.100 --> 02:51:04.100]  Давайте двумя способами.
[02:51:04.100 --> 02:51:10.100]  А там мы и в форму, что информация фишер, это минус на к заданию второй производины.
[02:51:11.100 --> 02:51:12.100]  Логарифм и фишер.
[02:51:13.100 --> 02:51:15.100]  Ну да, так тоже можно, они эквивалентны, там это не сложно показать.
[02:51:17.100 --> 02:51:19.100]  Но это все еще через определение, как бы считать.
[02:51:20.100 --> 02:51:22.100]  Есть какие-то теоремы, которые помогут нам это решить проще.
[02:51:24.100 --> 02:51:25.100]  Вот.
[02:51:26.100 --> 02:51:30.100]  Соответственно в нашем случае теорема, это опять-таки критерии эффективности.
[02:51:30.100 --> 02:51:32.100]  Который был записан на этой доске до того, как я его стер.
[02:51:33.100 --> 02:51:34.100]  Давайте еще раз запишу.
[02:51:41.100 --> 02:51:42.100]  Что у нас там должно быть?
[02:51:43.100 --> 02:51:50.100]  У нас наша оценка, минус функция, которую мы оцениваем, должна линейно зависеть от клада выборки.
[02:51:51.100 --> 02:51:54.100]  Но там еще было одно важное условие, что вот эта константа,
[02:51:54.100 --> 02:51:55.100]  она имеет какой-то конкретный вид.
[02:51:56.100 --> 02:52:01.100]  И какой-то конкретный вид, это на самом деле производная теория на информацию фишер от всей выборки.
[02:52:07.100 --> 02:52:08.100]  Супер, да?
[02:52:09.100 --> 02:52:11.100]  Наверное, появилось понимание теперь, как это дорешивать.
[02:52:14.100 --> 02:52:15.100]  Как сам делить?
[02:52:15.100 --> 02:52:16.100]  Ну, смотрите.
[02:52:16.100 --> 02:52:18.100]  Давайте мы с вами поймем что-нибудь.
[02:52:19.100 --> 02:52:22.100]  Справа, это значит, это н на и умалое.
[02:52:23.100 --> 02:52:26.100]  Да, как и в любой другой задаче, потому что информация фишер линейна.
[02:52:27.100 --> 02:52:29.100]  То есть, смотрите, давайте заметим с вами.
[02:52:30.100 --> 02:52:31.100]  Вот эта кон visas.
[02:52:32.100 --> 02:52:33.100]  И вот эта кон vis.
[02:52:34.100 --> 02:52:35.100]  И вот эта кон vis.
[02:52:36.100 --> 02:52:37.100]  И вот эта кон vis.
[02:52:38.100 --> 02:52:39.100]  И вот эта кон vis.
[02:52:40.100 --> 02:52:41.100]  И вот эта кон vis.
[02:52:42.100 --> 02:52:43.100]  И вот эта кон vis.
[02:52:43.100 --> 02:52:46.840]  Давайте заметим с вами. Вот это у нас известно? Какая оценка нам подходит?
[02:52:46.840 --> 02:52:52.860]  Известно, вот она. Какую функцию мы оцениваем известно? Вот она. Производную
[02:52:52.860 --> 02:52:56.300]  эту функцию, наверное, посчитаем. А по сути нам нужно найти вклад выборки, и тогда
[02:52:56.300 --> 02:53:00.020]  мы найдем информацию фишера. То есть отсюда из критерии эффективности на самом
[02:53:00.020 --> 02:53:06.780]  деле легко найти информацию фишера, то есть информация фишера, она линейна,
[02:53:06.780 --> 02:53:15.420]  поэтому это N на информацию фишер одного элемента. Было что-то такое? Да не, нормально,
[02:53:15.420 --> 02:53:23.300]  уже 11 часов просто, это нормально. Так, ну и что, как у нас будет выражаться? Здесь
[02:53:23.300 --> 02:53:33.620]  будет вклад от выборки. Здесь у нас будет производная нашей функции. Я теперь
[02:53:33.620 --> 02:53:39.300]  просто тета поменяю на п, чтобы вот чуть понятнее было. Так, а в номинателе у нас
[02:53:39.300 --> 02:53:54.340]  будет с вами оценка. Минус tau от тета. Ещё N. И N, да, ты прав, абсолютно. Только хотел написать вот так.
[02:53:54.340 --> 02:54:02.660]  Супер. То есть чем это проще, чем искать по определению, вам не нужно от ожидания квадрата
[02:54:02.660 --> 02:54:12.260]  считать. То есть это чуть-чуть проще будет, но немного. Так, что у нас информации фишера получается?
[02:54:12.260 --> 02:54:23.780]  А производная чему равна? Минус один на п квадрат. Тут минус единичка, так что все. Так, дальше сейчас
[02:54:23.780 --> 02:54:31.500]  вклад выборки посчитаем. Что у нас в номинателе записано? X средняя. А, на самом деле подождите,
[02:54:31.500 --> 02:54:37.020]  мы можем с вами заметить один факт. Ведь вот это равенство верно для любого N, то есть для любого
[02:54:37.020 --> 02:54:40.420]  размера выборки. На самом деле мы можем при N равным единичку рассмотреть данное равенство.
[02:54:40.420 --> 02:54:46.580]  Поэтому вот на самом деле давайте чуть-чуть упростим себе задачу и сотрём здесь N.
[02:54:46.580 --> 02:54:51.940]  Ну потому что критерия эффективности выполнена для любого N.
[02:54:53.940 --> 02:55:00.020]  Для любой выборки, для выборки любого размера это выполнено. Давай рассмотрим его для выборки
[02:55:00.020 --> 02:55:05.780]  размера 1. Вот критерия эффективности. Критерия эффективности утверждает, что оценка является
[02:55:05.780 --> 02:55:10.380]  эффективной, если она линейно представима через вклад наблюдения, ну вклад выборки.
[02:55:10.380 --> 02:55:22.380]  Поделить на вот, ну умноженную вот такую константу. Да, то есть это на самом деле выполнено для любой
[02:55:22.380 --> 02:55:29.020]  выборки, для выборки любого размера. Если выполнено для выборки любого размера, давайте рассмотрим для
[02:55:29.020 --> 02:55:34.060]  выборки размера 1. Что мы с вами получаем?
[02:55:34.060 --> 02:55:42.140]  Просто X.
[02:55:42.140 --> 02:55:54.180]  Так, одну секундочку. Тау, ой, прощепочини, у меня тут немножко поплыли. Тут везде П должна быть.
[02:55:54.180 --> 02:55:57.260]  П, П, П, П, П. Здесь тоже должно быть П.
[02:55:57.260 --> 02:56:02.980]  Потому что тут все было в терминах тета, а мы теперь применяем в наши задачи, а у нас параметр П.
[02:56:02.980 --> 02:56:11.500]  Поэтому мы его назвали П. Так, производная. Вот, Х минус функция, которую мы оцениваем, это единичка
[02:56:11.500 --> 02:56:23.980]  минус П на П. Вот, и вопрос только, чему равен вклад одного наблюдения. Давайте запишем его сразу в
[02:56:23.980 --> 02:56:33.100]  развернутой форме. Это будет производная по П в логарифме плотности. Так, плотность у нас как выглядит?
[02:56:33.100 --> 02:56:46.580]  Мы с вами записывали ее, вот она. 1 минус П от Х на П. Давайте вот это выражение где-нибудь
[02:56:46.580 --> 02:57:00.100]  отдельно почитаем. Вот здесь? Ты прав, кстати, абсолютно. Берем логарифм, получается у нас
[02:57:00.100 --> 02:57:11.740]  экспонента просто умирает. Берем от нее производную, это 1 на П будет, да? Давайте дочитаем. Так, если мы
[02:57:11.740 --> 02:57:20.780]  логарифмируем, у нас экспонента уходит. Логарифм П производный, это что у нас? 1 на П. Так, плюс,
[02:57:20.780 --> 02:57:33.700]  а может и минус. Действительно, минус. Х у нас остается. Производная логарифма.
[02:57:33.700 --> 02:57:56.660]  И делим это все на, так, тут немножко смешалось, прошу прощения. Х минус 1 минус П на П. В принципе,
[02:57:56.660 --> 02:58:09.940]  почти все получилось, осталось чуть-чуть подсокращать там и... Ну да, это почти ответ.
[02:58:09.940 --> 02:58:24.420]  Тогда у тебя здесь вылезло и к средне, и было просто неприятно. А может мы разучили
[02:58:24.420 --> 02:58:33.660]  дифференцировать? Что, мы как-то не умеем дифференцировать, да, уже? Ну, в истях уже не тот.
[02:58:33.660 --> 02:58:40.620]  Смотри, плотность вот у тебя экспоненциальная, логарифм просто закрывает экспоненту, берем
[02:58:40.620 --> 02:58:46.420]  производную от того, что здесь. Производная вот эта 1 на П, плюс производная вот этой штуки. Х умножить
[02:58:46.420 --> 02:59:06.500]  на производный логарифм 1 минус П. Минус 1 поделить на 1 минус П. От одного наблюдения?
[02:59:16.420 --> 02:59:38.620]  Информация одного нет. Информация у тебя же линейная. Они независимы между собой. Как бы
[02:59:38.620 --> 02:59:42.220]  абстрактно, если понимать, то у тебя независимые случайные величины не дают как бы информации друг
[02:59:42.220 --> 02:59:56.980]  другу. Нет. Информация фишер для одного? Было в знаменателе N. Но у нас здесь, видишь,
[02:59:56.980 --> 03:00:05.900]  оценка тогда не X будет, а X средняя. А в X среднем у тебя N зашит в знаменателе. Они сократятся,
[03:00:05.900 --> 03:00:12.380]  там останется сумма. И потом у тебя еще вклад выборки будет считаться уже не как логарифм плотности,
[03:00:12.380 --> 03:00:25.900]  а как логарифм функций правдоподобия. Так, давайте досчитаем. Чуть осталось уже. Смотрите,
[03:00:25.900 --> 03:00:34.100]  приведем просто к общему знаменателю. Что здесь получится? Это нужно на П, наверное,
[03:00:34.100 --> 03:00:53.700]  домножить. А вот это нужно на 1 минус П домножить. В знаменателе что у нас получится? XP делим
[03:00:53.700 --> 03:01:03.180]  на П. Так, давайте досчитаем аккуратно. Потому что плохую задачу бросать, когда она дорешенна
[03:01:03.180 --> 03:01:17.420]  почти. И не сделать в выводах... Ты не поверишь, это правда. Так, П квадрат точно выживает. Тут у
[03:01:17.420 --> 03:01:24.500]  нас что будет? 1 минус П минус XP. А здесь 1 минус П минус XP. Ну по сути сокращается,
[03:01:24.500 --> 03:01:33.020]  только еще минусик вылазит, да? В знаменателе у нас что идет? П на 1 минус П? А в числителе
[03:01:33.020 --> 03:01:42.020]  у нас идет П. Ну еще знак минус плюс. Итого мы получаем, вот эти П сократились. 1 на П в квадрате
[03:01:42.020 --> 03:01:52.300]  на 1 минус П. Это информация Фишера для данного наблюдения. Нет, нет, нет, нет, она не должна
[03:01:52.300 --> 03:01:58.940]  зависеть. Да, да, да, да. То есть, если бы вы взяли выборку большего размера, у вас бы X тоже
[03:01:58.940 --> 03:02:05.700]  сократился, у вас вот здесь было бы X среднее, а здесь у вас было бы, ну, понимаете, да? Функция
[03:02:05.700 --> 03:02:10.540]  правдоподобия между плотностями. И все было бы хорошо. То есть, смотрите, это просто как бы какая-то
[03:02:10.540 --> 03:02:15.860]  чиселка информации Фишера. Сколько информация себе несет одно наблюдение, неважно какое,
[03:02:15.860 --> 03:02:21.820]  это усредненная величина. Вклад наблюдения зависит от X, а как бы информация Фишера,
[03:02:21.820 --> 03:02:28.780]  она уже усреднена по всем X. Давайте какой-нибудь вывод постараемся сделать вот из этого. Мы можем
[03:02:28.780 --> 03:02:32.980]  сделать какой-нибудь вывод из того, что информация Фишера выглядит таким образом.
[03:02:32.980 --> 03:02:49.460]  Не, давайте какие-нибудь крайние случаи просматриваем. Ну, как бы там вот бесконечности
[03:02:49.460 --> 03:02:50.700]  нужно, наверное, поаккуратнее быть.
[03:03:19.460 --> 03:03:27.460]  Да, то есть при фиксированных каких-то это будет, типа мы все знаем сразу, это правда.
[03:03:27.460 --> 03:03:39.620]  Так, давайте вот в общем виде Раокраймера запишем. У вас на самом деле, если вы оцениваете
[03:03:39.620 --> 03:03:44.620]  функцию от это, то у вас в числителе стоит уже не единичка, а производная. Вот, и делите вы на
[03:03:44.620 --> 03:03:51.660]  N информации Фишера от этого по выбору. Такая штучка у нас получается. Это вот не равен второго
[03:03:51.660 --> 03:03:59.900]  Краймера. Чем меньше информации, тем больше ваша дисперсия. Чем больше информации,
[03:03:59.900 --> 03:04:11.180]  которую вы несете, тем меньше ваша дисперсия может быть. Чем меньше информации одного наблюдения,
[03:04:11.180 --> 03:04:15.740]  тем больше дисперсия. То есть тебе каждое наблюдение в среднем мало информации дает,
[03:04:15.740 --> 03:04:22.780]  соответственно дисперсия твоей оценки будет очень большая. Если информация Фишера одного
[03:04:22.780 --> 03:04:25.780]  наблюдения очень большая, то есть тебе много информации приходит от каждого наблюдения,
[03:04:25.780 --> 03:04:32.900]  тогда наоборот дисперсия может быть очень маленькой. Понятно, да? В этом разделе,
[03:04:32.900 --> 03:04:39.620]  в этом разделе, значит вас могут попросить либо сравнить оценки в каких-нибудь подходах,
[03:04:40.180 --> 03:04:44.660]  повторяйте, что еще можно сделать. Либо, что скорее всего вам дадут какое-то экспоненциальное
[03:04:44.660 --> 03:04:49.340]  семейство и попросят предъявить эффективную оценку. В таком случае вот воспользуйтесь просто
[03:04:49.340 --> 03:04:55.820]  утверждением, которое мы доказали. В одномерном случае оно очень хорошо работает. Скорее всего
[03:04:55.820 --> 03:05:05.500]  лучше показать будет. Но оно показывается очень просто. Теоремами можно пользоваться,
[03:05:05.500 --> 03:05:08.860]  утверждением не очень хорошо пользоваться, потому что оно использует как бы элементы
[03:05:08.860 --> 03:05:22.660]  доказательства. Само доказательство этого факта как бы оно появляется при доказательстве того,
[03:05:22.660 --> 03:05:29.340]  что у нас, если семейство экспоненциальное, существует эффективная оценка. Вот это утверждение,
[03:05:29.340 --> 03:05:34.620]  это кусочек доказательства, вот эти аремы об том, что эффективная оценка существует тогда и только
[03:05:34.620 --> 03:05:44.300]  тогда, когда семейство экспоненциально. Не будем вторым способом искать, я передумал. Вторым
[03:05:44.300 --> 03:05:48.900]  способом информацию фишера можно было посчитать по определению. То есть вы бы посчитали вклад,
[03:05:48.900 --> 03:05:54.540]  вклад мы с вами вот вместе посчитали уже здесь. И дальше осталось на самом деле вот это просто
[03:05:54.540 --> 03:06:05.340]  проинтегрировать с плотностью в квадрате. Ну типа там там от ожидания вклада в квадрат идет.
[03:06:07.340 --> 03:06:13.900]  Если бы вы посчитали это честно, то же самое бы получилось на самом деле. И еще вот в этих
[03:06:13.900 --> 03:06:18.380]  задачах как можно сработать. Вот типа смотрите, вас просят предъявить какую-нибудь эффективную
[03:06:18.380 --> 03:06:25.100]  оценку. Вы можете посчитать информацию фишер сначала по определению, а потом сказать,
[03:06:25.100 --> 03:06:32.300]  что я знаю оценку, для которой выполняется равенство в неравенстве раокрамера. И все. Ну типа вы
[03:06:32.300 --> 03:06:39.420]  можете как бы подобрать, просто подобрать. Типа ну видите х средняя, это какая-то очень очевидная,
[03:06:39.420 --> 03:06:44.740]  да это была оценка? На самом деле я вам больше скажу, вот это это среднее значение, ну типа
[03:06:44.740 --> 03:06:49.460]  этом от ожидания в геометрическом распределении. То есть на самом деле мы бы то же самое получили,
[03:06:49.460 --> 03:06:56.460]  если бы мы с вами просто бы решали методом моментов. Метод моментов вам дал бы то же самое. 1
[03:06:56.460 --> 03:07:03.620]  минус p на p равняется х среднему. Потому что вот это там от ожидания х. То есть здесь можно было
[03:07:03.620 --> 03:07:16.500]  и угадать. Неравенство раокрамера? Просто считаешь дисперсию данной оценки? Считаешь
[03:07:16.500 --> 03:07:24.420]  информацию отдельно? По определению считаешь информацию фишера, ну и просто считаешь дисперсию,
[03:07:24.420 --> 03:07:28.260]  как мы обычно считали дисперсию оценок. И показываешь, что у тебя выполнено равенство в
[03:07:28.260 --> 03:07:36.340]  неравенство раокрамера. То есть вот тебе нужно посчитать по сути вот это и вот это по отдельности.
[03:07:36.340 --> 03:07:52.860]  Интеграл, ну от ожидания от вклада элементов в квадрате. В смысле? Для данной задачи. Для
[03:07:52.900 --> 03:07:59.940]  данной задачи информация фишера одного наблюдения, как и информация фишера для любой другой задачи.
[03:08:04.940 --> 03:08:12.140]  Это просто математическое ожидание вклада в квадрате.
[03:08:12.140 --> 03:08:19.980]  Соответственно это у нас что такое? Интеграл. Вклад мы с вами вот тут посчитали.
[03:08:23.860 --> 03:08:34.460]  А тут будет 1 минус П? Минус ХП. Вот. Ну еще это нужно на плотность домножить. Потому что это же
[03:08:34.460 --> 03:08:40.220]  мат ожидания. То есть вот функция ваша. Еще нужно на плотность домножить. А плотность у нас какая была?
[03:08:40.220 --> 03:08:55.420]  1 минус П в степени Х, да. На П. Тут даже что-то посокращается. Здесь будет 1 минус П в степени Х минус 1.
[03:08:55.420 --> 03:09:05.060]  Да, это правда. Вот это в квадрате. ДХ. Нет, ты усредняешь по Х. Да.
[03:09:05.060 --> 03:09:16.380]  Вот этот интеграл превращается в сумму. Ну типа в общем случае интеграл, но интеграл по считающей мере
[03:09:16.380 --> 03:09:20.100]  действительно это сумма. То есть на самом деле здесь будет даже не интеграл. Абсолютно спасибо за
[03:09:20.100 --> 03:09:26.980]  замечание. То есть у вас здесь будет сумма по Х принадлежащим натуральным числам. Вот такого ряда.
[03:09:26.980 --> 03:09:41.140]  Вот. А суммируете вы получается квадрат вот этой штуки. То есть квадрат вклада наблюдения на плотность.
[03:09:41.140 --> 03:09:45.900]  Плотность у вас выглядит вот таким образом. Ну и кажется этот ряд можно посчитать, потому что это
[03:09:45.900 --> 03:09:51.300]  просто какой-нибудь геометрический ряд будет. Ну сумма геометрической прогрессии. Или не совсем.
[03:09:51.300 --> 03:09:58.220]  Наверное не совсем. Ну вот у вас Х это значит будет геометрическая прогрессия, но у вас тут
[03:09:58.220 --> 03:10:03.700]  еще будет Х-овый коэффициент. Ну короче через ряды Тейлора можно посчитать. То есть вы берете
[03:10:03.700 --> 03:10:07.620]  ряд Тейлора, его дифференцируете в области исходимости и у вас там получается. Такую сумму можно
[03:10:07.620 --> 03:10:22.660]  посчитать, но это гораздо сложнее может быть. Чем? Да правда можно. Да.
[03:10:37.620 --> 03:10:45.100]  Ну у тебя если бы вот отсюда вылезло. У тебя же здесь не вклад наблюдения, а был бы вклад выборки.
[03:10:45.100 --> 03:10:51.540]  То есть у тебя еще отсюда бы вылезло много слагаемых. У тебя в любом случае бы здесь
[03:10:51.540 --> 03:11:00.020]  числитель и знаменатель с Х и с Н посокращался бы. Можешь проделать просто. Вот. Еще говорят,
[03:11:00.020 --> 03:11:04.500]  что информацию Фишера можно по-другому считать, да? Что там на Википедии пишут?
[03:11:04.500 --> 03:11:24.220]  Так, да. Наверное это не совсем важно. Давайте пойдем дальше. В общем понятно, да? Что вас в данном
[03:11:24.220 --> 03:11:28.580]  разделе могут попросить посчитать? Информацию Фишера? Не важно для какого распределения.
[03:11:28.580 --> 03:11:32.980]  Попросить какого-нибудь там через раукраймера что-нибудь поценить. Сказать существует ли
[03:11:32.980 --> 03:11:35.860]  эффективная оценка, но если экспоненциально существует, если не экспоненциально и не
[03:11:35.860 --> 03:11:41.500]  существует. Вот. А дальше вас могут попросить привести эффективную оценку. Тогда вы пользуетесь
[03:11:41.500 --> 03:11:46.380]  либо утверждением, либо критерием, либо чем-нибудь вот из того, что мы обсудили. Либо можете угадать.
[03:11:46.380 --> 03:11:59.940]  Понятно? Отлично. Задача номер пять. На самом деле две. Не, они посложнее.
[03:11:59.940 --> 03:12:09.180]  Новая тема. Так, ну давайте сначала доверить на интервалы обсудим, а потом к полным статистикам
[03:12:09.180 --> 03:12:13.900]  перейдем. Достаточно. К оптимальным оценкам и ко всему вот этому.
[03:12:13.900 --> 03:12:31.500]  У вас пробайс уже был? Рано? Это просто один из последних семинаров по идее.
[03:12:31.500 --> 03:12:58.700]  Так, ну ладно, у кого будут бояться, сочувствую. Наверное. Так, задача номер пять. Тема доверить на
[03:12:58.700 --> 03:13:11.540]  интервалы. Вот. Мы сейчас с вами быстренько пару способов разберем основных классических и все
[03:13:11.540 --> 03:13:26.940]  поймем. Доверить на интервалы. Значит так, давайте кто-нибудь расскажите, почему есть мотивация
[03:13:26.940 --> 03:13:39.300]  строить доверительные интервалы. Ну это правда. Давайте быстренько с вами какую-нибудь
[03:13:39.300 --> 03:13:49.140]  игрушечную задачу разберем. Вот вас просят оценить. Вас просят оценить, а какая там величина там? Сейчас.
[03:13:49.140 --> 03:13:54.700]  Вот допустим вы действительно строите плотину и у вас есть случайная величина, которая обозначает
[03:13:54.700 --> 03:14:08.380]  уровень воды в реке. Вот. И вы наверное хотите что-то пооценивать, что-то с этим сделать. Вы
[03:14:08.380 --> 03:14:12.820]  можете взять среднее, но наверное среднее вас не очень интересует. Потому что вас же гораздо
[03:14:12.820 --> 03:14:17.340]  больше будет интересовать там какой максимальный уровень, например, да? Потому что плотину нет
[03:14:17.340 --> 03:14:21.980]  смысла строить для среднего уровня, есть смысл строить для максимального уровня. Вот. Но на
[03:14:21.980 --> 03:14:27.340]  самом деле, типа плотину высотой 10 метров вы скорее всего не построите, вы хотите построить какую-то
[03:14:27.340 --> 03:14:33.260]  плотину, чтобы она там в 99% случаев вас спасла. Ну потому что на 10 метровую плотину у вас линей нет.
[03:14:33.260 --> 03:14:43.180]  Таким образом, ну и вот допустим у вас уровень воды в реке подчиняется какому-нибудь вот такому закону.
[03:14:43.300 --> 03:14:53.740]  Там хотя бы один метр, ну и до тета. На самом деле нам достаточно построить доверичный интервал.
[03:14:53.740 --> 03:14:58.700]  Что такое доверичный интервал для параметра? Это такая пара статистик.
[03:15:13.180 --> 03:15:20.140]  Что ваше значение с большой вероятностью лежит в этом интервале? Больше либо равно. Больше либо равно
[03:15:20.140 --> 03:15:27.420]  уровня доверия. То есть грубо говоря вы говорите, что с вероятностью 99% уровень воды в реке будет
[03:15:27.420 --> 03:15:32.220]  там от одного метра до трех метров. Ну понятно, что может быть сильнее там скакнуть, но вероятность
[03:15:32.220 --> 03:15:36.940]  этого очень мала. Вы строите как бы такой интервальчик, в котором с большой вероятностью
[03:15:36.940 --> 03:15:41.980]  истинное значение параметра будет лежать. Тогда вы можете как бы ориентироваться на только вот
[03:15:41.980 --> 03:15:48.140]  эти значения, из большой вероятности именно они и будут выполняться. Что такое доверительный
[03:15:48.140 --> 03:15:53.580]  интервал? Смотрите, у вас есть вот ваша выборка и вы как бы хотите какую-то оценку для нижней
[03:15:53.580 --> 03:15:59.460]  границы построить, какую-то статистику, которая снизу подобрет и какую-то статистику, которая
[03:15:59.460 --> 03:16:06.940]  сверху подобрет ваш параметр. Так чтобы вероятность того, что ваше истинное значение параметра
[03:16:06.940 --> 03:16:11.460]  попадет между этими двумя статистиками, было больше либо равно гамма. Вот эта гамма чаще всего
[03:16:11.460 --> 03:16:27.980]  полагаю там 0.99. Там 0.95 можно, да. 0.9995 или там всякие рассматривают. Нет, статистики. А,
[03:16:27.980 --> 03:16:35.140]  хотя оценки, да, да, ты прав. Вообще в общем случае это пара статистик, потому что ты можешь
[03:16:35.140 --> 03:16:40.380]  какой-нибудь глупый интервал придумать. Там от минус бесконечности до плюс бесконечности,
[03:16:40.380 --> 03:16:43.620]  когда у тебя параметры только положительные. Ну типа это тоже доверительный интервал будет,
[03:16:43.620 --> 03:16:52.860]  нормально. Ну поэтому это говорят что статистики. Вот, что еще интересного хочется сказать. Да,
[03:16:52.860 --> 03:16:59.060]  наверное, ничего. Наверное, можно решать. Какие у нас есть методы построения доверительных
[03:16:59.060 --> 03:17:07.740]  интервалов? Через центральную статистику это основной? Самый скучный метод это через Чебышова,
[03:17:07.740 --> 03:17:16.060]  да. Мы его даже рассматривать не будем, потому что он плохой. Мы сразу будем через центральную
[03:17:16.060 --> 03:17:25.980]  статистику действовать. Итак, метод центральной статистики. Если что в методичке Родионова-Шабанова,
[03:17:25.980 --> 03:17:34.500]  там разобран пример с неравенством Чебышова. Вот. Кстати, хороший вопрос. Вот смотрите,
[03:17:34.500 --> 03:17:38.380]  вы построили два доверительных интервала для параметра. Одного и того же уровня доверия.
[03:17:38.380 --> 03:17:43.540]  Один оказался вот такой маленький, а второй вот такой большой. Какой из них двух вы предпочтете?
[03:17:43.540 --> 03:17:48.900]  Ну, который поменьше, потому что вы хотите более точно как бы с той же вероятностью в более
[03:17:48.900 --> 03:17:56.220]  маленький отрезочек загнать. Понятно что? Они могут содержаться, могут не содержаться.
[03:17:56.220 --> 03:18:05.340]  Просто тебе, просто Чебышев чем плох? Тем, что у вас там оценка сверху идет через дисперсию
[03:18:05.340 --> 03:18:12.540]  на епсилон в квадрате. На самом деле, если вы епсилон возьмете очень маленький, то у вас
[03:18:12.540 --> 03:18:17.820]  здесь оценка будет для вероятности меньше там десяти. Ну, то есть вот это число может быть
[03:18:17.820 --> 03:18:22.500]  больше единицы. Тогда это какая-то глупая оценка на вероятность получится. Поэтому,
[03:18:22.500 --> 03:18:29.980]  на самом деле, Чебышев вам построит какой-то очень большой интервал. Вот. А центральная
[03:18:29.980 --> 03:18:34.700]  статистика вам построит хороший интервал. Он может быть не самый лучший, но хороший.
[03:18:34.700 --> 03:18:44.740]  Который вам подойдет для решения задач точно. Итак, что же такое центральная статистика?
[03:18:44.740 --> 03:18:55.540]  Центральная статистика это такая функция g, вернее это статистика g от х тета. То есть
[03:18:55.540 --> 03:19:02.460]  она принимает во внимание данные, которые у вас есть и принимает во внимание параметр неизвестный.
[03:19:02.460 --> 03:19:13.300]  Вот. И сейчас будет немножко так крышесносная штука. Первая. Мы с вами говорим, что вот эта
[03:19:13.300 --> 03:19:21.860]  функция, если мы рассмотрим ее как функцию по параметру тета. Давайте даже, чтобы понять,
[03:19:21.860 --> 03:19:25.220]  что это параметру тета, мы с вами ее вот так обозначим. То есть выборка фиксированная,
[03:19:25.220 --> 03:19:35.820]  а функция по тета. Она должна быть перерывная и монотонная. А второе. Сама функция, ну сама
[03:19:35.820 --> 03:19:51.340]  статистика, как видите, от тета зависит, но ее распределение не зависит от тета. Понятненько?
[03:19:51.340 --> 03:20:02.500]  Да, то есть ваша функция на тета как бы учитывает, а распределения нет. То есть в некотором смысле вы
[03:20:02.500 --> 03:20:19.420]  решаете уравнение здесь с неизвестной тета. Сейчас поймем. Задача. В смысле, это вот,
[03:20:19.420 --> 03:20:26.340]  еще раз, вот это это статистика, это какая-то измеримая функция от выборки. Смотрите,
[03:20:26.340 --> 03:20:32.340]  х вы зафиксируете какую-то выборку. Теперь же вы можете смотреть на это как на функцию от тета. То
[03:20:32.340 --> 03:20:44.580]  есть вот условно давайте рассмотрим а на х средняя. Ну типа нормальная модель. Вот какая-то непонятная
[03:20:44.580 --> 03:20:50.620]  статистика, она не будет центральной, я в этом уверен, но давайте просто посмотрим. Видите,
[03:20:50.620 --> 03:20:56.740]  она как бы зависит от параметра неизвестного и от данных. Если мы данные зафиксируем,
[03:20:56.740 --> 03:21:01.300]  то это просто функция по параметру. И вот в этом смысле, что если у вас фиксирована выборка,
[03:21:01.300 --> 03:21:06.220]  то под тетой у вас должна быть непрерывность и монотонность. Сейчас поймем для чего. Вот,
[03:21:06.220 --> 03:21:12.100]  это первое. А второе, ее распределение не должно зависеть от это. Вот, ну давайте сразу задачку
[03:21:12.100 --> 03:21:17.540]  просто решать. Задача очень интересная и очень необычная. Я бы даже сказал нестандартная.
[03:21:20.620 --> 03:21:37.500]  И нетривиальная, кстати. И достаточно сложная. Так, смотрите. Давайте мы с вами сложную задачу
[03:21:37.500 --> 03:21:43.500]  разберем, чтобы вы простую не смогли решить. Ну это как всегда, сложные задачи разбирают,
[03:21:43.500 --> 03:21:48.380]  а потом простенькие не решают. Но мы рассмотрим хорошую задачу. Смотрите,
[03:21:48.380 --> 03:22:00.020]  x1, xn имеет экспоненциальное распределение с параметром тета1. y1, ym имеет экспоненциальное
[03:22:00.020 --> 03:22:08.460]  распределение с параметром тета2. Вас просят построить доверительный интервал для отношения
[03:22:08.460 --> 03:22:36.460]  параметра. Давайте секунду подумаем. Нет, это очень хорошая задача. Какого-то уровня доверия.
[03:22:36.460 --> 03:22:43.420]  Ну уровень доверия произвольный, давайте гамма просто какой-нибудь затоссируем. Ну на самом деле у
[03:22:43.420 --> 03:22:47.300]  нас будет просто функция этого и в зависимости от того, какой гамма тут подставишь, у тебя разные
[03:22:47.300 --> 03:23:04.940]  доверительные интервалы будут. Да. Наверное, это сложный пример. Не, он вообще очень интересный на
[03:23:04.940 --> 03:23:10.340]  самом деле и совсем непонятный, да? Ну смотрите, вот эти все независимо одинаково распределенные,
[03:23:10.340 --> 03:23:15.020]  вот эти независимо одинаково распределенные. Можно было бы, конечно, рассмотреть вот экспоненциальную
[03:23:15.020 --> 03:23:20.300]  модель только, но это было бы слишком просто. Дойти доверительный интервал для тета1 это очень
[03:23:20.300 --> 03:23:24.260]  просто. Мы потом поймем, как в общем случае это делать. Иногда вам могут дать вот какую-нибудь такую
[03:23:24.260 --> 03:23:33.940]  штуку оценить. Хорошо, методом центральной статистики. Нам нужно придумать какую-то
[03:23:33.940 --> 03:23:41.100]  статистику, которая будет зависеть от тета1, тета2, но распределение которой не будет зависеть от
[03:23:41.100 --> 03:23:56.500]  параметра. Идеи? Это плохо, это не работает. Почему это не работает? Потому что смотри,
[03:23:56.500 --> 03:24:00.420]  ты берешь два доверительных интервала и вероятность того, что ты в один из них не попадешь,
[03:24:00.420 --> 03:24:06.660]  типа это не произведение. Ну короче, там сложно будет с вероятностью, там не совсем так очевидно
[03:24:06.660 --> 03:24:15.980]  получится, как ты хочешь. Средние не нужны. Я утверждаю вот что. Давайте рассмотрим статистику тета2.
[03:24:15.980 --> 03:24:34.820]  Не, это просто сложный пример, а простой пример вы потом сами решите. Суть этой задачи в том,
[03:24:34.820 --> 03:24:38.820]  чтобы придумать мотивацию, почему нужно общий случай для какой-то статистики центральной найти,
[03:24:38.820 --> 03:24:43.660]  почему лучше пользоваться каким-то общим методом. Вот есть задачи, в которых общий метод не работает,
[03:24:43.660 --> 03:24:47.980]  но чаще всего общий метод работает. И вот мы к общему методу придем сейчас с вами, решив очень
[03:24:47.980 --> 03:24:56.380]  плохой частный случай. Нет, они все между собой независимы. Иксы не зависят с другими иксами,
[03:24:56.380 --> 03:25:08.460]  иксы не зависят с другими игреками. В общем, на чем идея данной задачи строится? На том,
[03:25:08.460 --> 03:25:25.380]  что, смотрите, если вы... Прошу прощения. На чем решение задачи строится? На том,
[03:25:25.380 --> 03:25:30.380]  что параметр тета это на самом деле параметр масштаба. Если вы домножите экспоненциальное
[03:25:30.380 --> 03:25:35.780]  распределение на константу лямбда, то у нее параметр просто поделится на лямбда. Это вы знаете?
[03:25:35.780 --> 03:25:42.180]  Ну вот такое было. Это называется параметром масштаба, что если вы домножаете на константу,
[03:25:42.180 --> 03:25:46.820]  у вас тоже типа либо умножается на константу, либо делится на константу. Ну давайте это с вами
[03:25:46.820 --> 03:26:01.220]  аккуратно покажем. Вот смотрите, плотность phi от кси, где кси имеет просто экспоненциальное
[03:26:01.220 --> 03:26:10.500]  распределение с параметром 1 пусть будет. phi от кси это просто лямбда кси. Ну просто
[03:26:10.500 --> 03:26:14.340]  на какую-то константу намножаем. Давайте посмотрим, что произойдет с плотностью.
[03:26:14.340 --> 03:26:23.460]  Давайте даже через функцию распределения это сделаем. Функция распределения phi от кси
[03:26:23.460 --> 03:26:35.540]  в точке t, это вероятность того, что phi от кси меньше ли бы равняется t. phi непрерывно
[03:26:35.540 --> 03:26:38.980]  монотонные, ну phi у нас это просто домножение на константу, то есть phi это на самом деле
[03:26:38.980 --> 03:26:48.020]  просто лямбда x. Это на самом деле вероятность того, что кси меньше ли бы равняется, чем phi в
[03:26:48.020 --> 03:26:58.020]  минус 1 от t. И это просто функция распределения кси в точке phi в минус 1 от t. С этим согласны?
[03:26:58.020 --> 03:27:04.100]  Это просто простой трюк, мы его тысячу раз делали на тервере. Просто хотим найти распределение
[03:27:04.100 --> 03:27:14.180]  экспоненциальной случайной величины, умноженной на константу. Так, ну смотрите, обратная функция
[03:27:14.420 --> 03:27:21.300]  это что у нас? 1 на лямбда просто по сути, так ведь будет? Ну и все, вы просто в определение функции
[03:27:21.300 --> 03:27:29.700]  распределения единицы минус экспонента было единица, да, 1 на лямбда t, вот на индикатор того,
[03:27:29.700 --> 03:27:37.420]  что t больше либо на нуля. Все. То есть мы с вами что заметили? Что если мы домножаем с вами на
[03:27:37.420 --> 03:27:42.500]  константу экспоненциальное распределение, то на самом деле это тоже экспоненциальное распределение,
[03:27:42.500 --> 03:27:49.860]  но с параметром лямбда будет. Ну то есть в более общем виде, если кси имеет экспоненциальное
[03:27:49.860 --> 03:27:56.220]  распределение с параметром m каким-нибудь, то лямбда кси будет иметь распределение экспоненциальное
[03:27:56.220 --> 03:28:06.620]  m на лямбда. Понятно? Вот этим фактом мы хотим воспользоваться. Почему мы хотим этим фактом
[03:28:06.620 --> 03:28:21.660]  воспользоваться? Потому что смотрите, если мы домножим... Да, вот здесь получим экспоненциальное
[03:28:21.660 --> 03:28:24.660]  распределение с параметром 1 и здесь получим экспоненциальное распределение с параметром 1.
[03:28:24.660 --> 03:28:31.020]  И получим какую-то по сути отношение экспоненциальных величин с параметром 1, скорее всего ее распределение
[03:28:31.020 --> 03:28:42.780]  будет зависеть от параметра z. Давайте покажем это. То есть тут скорее само решение будет очень
[03:28:42.780 --> 03:28:48.420]  простое, но вот придумать и доказать, что это подходит, сложнее гораздо. Так, а где мне писать?
[03:28:48.420 --> 03:29:03.220]  Да, нет. Мы сейчас общий способ тоже разберем. Поехали. Нам нужно показать, что распределение вот
[03:29:03.220 --> 03:29:07.020]  этой штуки не зависит от параметра. То есть нам можно найти плотность, например, у этой штуки,
[03:29:07.020 --> 03:29:16.260]  так ведь? Давайте просто искать плотность. Так, наверное центральная статистика пострадает
[03:29:16.260 --> 03:29:44.140]  сейчас. Я ее сейчас сотру. Давайте, ребята, крепитесь. Немного осталось. Как показать,
[03:29:44.140 --> 03:29:48.140]  что распределение вот этой штуки будет не зависеть от это? Найти плотность, например. Как
[03:29:48.140 --> 03:29:55.620]  найти плотность отношения двух случайных величин? По теореме Лебега о замене переменной.
[03:29:55.620 --> 03:30:08.140]  Первый шаг. Смотрите, совместная плотность x и y это какая? В силу того, что они независимы,
[03:30:08.140 --> 03:30:13.420]  это просто произведение плотностей. Так, мы тета1 обозначали. То есть это будет тета1
[03:30:13.420 --> 03:30:26.380]  e в степени минус тета1 x на индикатор x больше либо равен нуля. И умножить на тета2 e в степени
[03:30:26.380 --> 03:30:34.460]  минус тета2 на x индикатор того, что... Ой, y, простите. y больше либо равен нуля. С этим согласны?
[03:30:34.460 --> 03:30:39.260]  Две случайные величины независимы, поэтому плотность их совместна. Это просто произведение
[03:30:39.260 --> 03:30:51.220]  плотностей. Дальше делаем замену. Уровняется вот ровно то, что нам нужно. Так, y на x. А v,
[03:30:51.220 --> 03:31:00.100]  ну потому что там дефиаморфизм должен быть, полагаем просто равным y. Да? Было что-то такое у
[03:31:00.100 --> 03:31:09.980]  нас когда-то. Делаем обратную замену. y равняется v, x равняется v... сейчас.
[03:31:09.980 --> 03:31:23.340]  x. Согласен, тета2 на тета1, а v поделить на u. Так, и кабиан.
[03:31:23.340 --> 03:31:34.860]  Так, здесь по u это у нас будет тета2, тета1 на v в квадрате, так ведь?
[03:31:38.860 --> 03:31:43.780]  По v. По v не важно, что здесь будет, потому что у нас по u здесь 0, а здесь будет единичка.
[03:31:44.020 --> 03:31:53.180]  Нет, единичка. Так, согласны с этим? Так, ну и соответственно модуль и кабианы это у нас
[03:31:53.180 --> 03:32:01.580]  тета2 на тета1, v на u в квадрат. Так, а у меня так получилось?
[03:32:01.580 --> 03:32:18.620]  Так, и друзья. Не, все нормально. Дорешаем, я думаю. Так, чуть по-другому просто. Просто все по-другому
[03:32:18.740 --> 03:32:21.140]  пошло, но все хорошо будет.
[03:32:21.140 --> 03:32:41.620]  Себар, а 1,1 и кабианы почему там есть? Тут ничего нету, тут три точки.
[03:32:41.620 --> 03:32:59.380]  Все, давайте считать новую плотность. p2v чему у нас равна? Нет, ну если мы будем брать интеграл,
[03:32:59.380 --> 03:33:02.580]  нам нужно сразу по одной переменной усреднять. Давайте сначала совместную плотность найдем.
[03:33:02.580 --> 03:33:14.940]  Тета1. Дальше. Да, е в степени минус тета1, вместо х подставляем. Тета1 сократится,
[03:33:14.940 --> 03:33:23.620]  тета2 v на u. На индикаторе х больше, либо на 0 мы его можем забить, потому что там все будет
[03:33:23.620 --> 03:33:34.700]  хорошо. Тета2. Так, вместо у мы подставляем с вами v, минус тета v. Тета2, да. Вот, на этот индикатор
[03:33:34.700 --> 03:33:39.620]  забиваем и домножаем на модули кабиана. Модуль кабиана у нас как раз уже вот написан. Сейчас,
[03:33:39.620 --> 03:33:44.460]  почему индикатор не записали? Ну потому что, смотри, вот это понятно, что это больше, либо равна 0?
[03:33:44.460 --> 03:33:50.860]  Ну, вы и у тебя как определены? Это просто положительная случайная величина, которая больше,
[03:33:51.340 --> 03:33:59.900]  а вот эти отношения положительных случайных величин. Хорошо, давайте домножим на индикатор того,
[03:33:59.900 --> 03:34:09.260]  что u больше, либо равна 0, и v больше, либо равна 0. Хорошо, согласен. Так, и тогда давайте найдем плотность u.
[03:34:09.260 --> 03:34:16.180]  Это просто усреднение по координате, которая нам не нужна. То есть это будет интеграл от 0 до
[03:34:16.180 --> 03:34:24.740]  бесконечности. Что у нас, кстати, с тетами произойдет? Вот эта тета сократится вот с этой тетой?
[03:34:24.740 --> 03:34:38.100]  А тета2 будет в квадрате, да. То есть у нас будет тета2 в квадрате, экспонента, минус тета2 v выносится,
[03:34:38.100 --> 03:34:55.660]  1 на u плюс 1. Какая-то такая штука, да, получается? Еще на b на u квадрат, здорово. Этого нам не хватало.
[03:34:55.660 --> 03:35:11.100]  Супер. Не, вообще все замечательно, на самом деле. Почему все замечательно? Нет, приведем это просто
[03:35:11.100 --> 03:35:17.180]  к плотности экспоненциального распределения. Каким образом? Вот смотрите, вот это все у вас под
[03:35:17.180 --> 03:35:25.620]  экспоненты, как бы. Смотрите, по v мы интегрируем, поэтому вот это у вас параметр λ, по сути. Давайте
[03:35:25.620 --> 03:35:30.180]  просто параметр λ здесь тоже соберем, и это у вас будет плотность распределения, так ведь?
[03:35:30.180 --> 03:35:49.180]  Что у нас, получается, должно быть? Тета2 на вот эту константу u плюс 1 на u. Тета2 у нас выносится,
[03:35:49.180 --> 03:36:02.740]  одна вторая так, v остается внутри. Вот, и все это экспонента в степени, потому что у тебя здесь
[03:36:02.740 --> 03:36:10.900]  она в квадрате, одну тета2 вынесем, а одну останем, чтобы у нас плотность осталась. Вот плотность для
[03:36:10.900 --> 03:36:14.780]  экспоненциального распределения, она вот так выглядит. Мы сейчас в таком виде ровно ее и собираем.
[03:36:14.780 --> 03:36:19.740]  Вот у тебя, по сути, переменная, по которой ты дифференцируешь, вот это минус лямда новая.
[03:36:19.740 --> 03:36:25.460]  Соответственно, минус лямда нам нужно собрать вот здесь. Мы собрали вот как раз тета2, ставили.
[03:36:25.460 --> 03:36:38.540]  Чтобы вот этот интеграл досчитать. Можно по частям посчитать, а можно просто выделить плотность и
[03:36:38.540 --> 03:36:52.180]  досчитать вот так. Так, мне нужно попить. Сейчас, ой. Да, мы усоединяем по ненужной нам координате.
[03:37:00.180 --> 03:37:06.260]  Сейчас досчитаем, друзья, секундочку. Не беспокойтесь.
[03:37:08.540 --> 03:37:18.940]  Не, ну, просто дело не в том. Дело в том, что материала становится настолько много,
[03:37:18.940 --> 03:37:21.740]  что его уже невозможно за один доп семинар отказать.
[03:37:21.740 --> 03:37:38.020]  Ну, я не знаю. Так, давайте дособираем плотность. Друзья,
[03:37:38.020 --> 03:37:47.140]  все получится. Давайте, главное верить. Так, еще экспоненты у нас. Минус тета2 у плюс 1 на u.
[03:37:47.140 --> 03:37:56.180]  Такая штучка, так ведь у нас? На v. v это то, почему мы ее дифференцируем. dv здесь будет.
[03:37:56.180 --> 03:38:02.620]  Ой, интегрируем, прошу прощения. dv. Что у нас здесь остается? У нас остается v поделить на u в квадрате.
[03:38:02.620 --> 03:38:11.500]  Так ведь? Вот оно. И тета2 в квадрате. Вот, я более аккуратно просто это переписал. Что мы с этим
[03:38:11.500 --> 03:38:18.340]  можем сделать? Кажется, что, смотрите, v на вот это, это мотождание даже будет. Там даже лучше будет.
[03:38:18.340 --> 03:38:23.860]  Мы сейчас мотождание соберем. Одну тета2 мы вынесем с вами за скобочку. Ну, за интеграл.
[03:38:23.860 --> 03:38:30.300]  У нас остается только тета2. Вот, и чтобы вот это стало плотностью,
[03:38:30.300 --> 03:38:39.100]  нам нужно домножить вот на вот это. Понимаете, да?
[03:38:39.100 --> 03:38:47.460]  Действительно.
[03:38:47.460 --> 03:39:04.500]  Так, продолжаем наше правильство. Вы только следите за мной, я уже могу ошибаться начать.
[03:39:04.500 --> 03:39:16.180]  Так, на что нам нужно домножить, чтобы все получилось? У плюс один. Так, у нас выносится
[03:39:16.180 --> 03:39:24.220]  тета2 в квадрате, так ведь? Ой, это просто тета2. Одну ушку мы сами забираем. И у плюс один еще добавляем.
[03:39:24.220 --> 03:39:37.740]  Потому что нам там домножить нужно, согласен. Интеграл. Тета2. Сейчас проверим, что все хорошо.
[03:39:37.740 --> 03:39:50.140]  v это у нас x будет, а здесь у нас будет тета2 на что? На plus 1 на u и умножить на экспоненту от
[03:39:50.140 --> 03:39:56.900]  минус того же самого v. Вот. Ну, заметим, что вот это на самом деле это у вас просто математическое
[03:39:56.900 --> 03:40:13.420]  ожидание экспоненциального распределения с параметром... Какой параметр? Вот это. У плюс 1 на u
[03:40:13.420 --> 03:40:20.620]  поделить на? И на тета2. Математическое ожидание такой штуки это что? Это 1 поделить на этот коэффициент.
[03:40:20.620 --> 03:40:32.300]  Отлично. Мы с вами получили практически уже. То есть это у нас будет тета2, u, u плюс 1 и делим вот на это.
[03:40:32.300 --> 03:40:41.820]  Делим на u плюс 1, умножаем на u, тета2 сокращается, ушки сокращаются. Ура! У нас
[03:40:41.820 --> 03:40:50.300]  получилось 1 на u плюс 1 в квадрате. Что мы с вами показали таким образом? Мы показали,
[03:40:50.300 --> 03:40:54.660]  что вот это отношение, которое мы с вами рассматривали, вот это вот, его распределение
[03:40:54.660 --> 03:41:01.500]  не зависит от параметра. Посмотрите, их плотность фиксирована. В плотности никак параметр не
[03:41:01.500 --> 03:41:16.940]  фигурирует. Это центральная статистика. Нет, нет, это просто плотность в точке u, вот такая.
[03:41:16.940 --> 03:41:24.660]  А у у это у нас просто по сути, ну это плотность от x, просто переименуй в x и все, это плотность
[03:41:24.660 --> 03:41:30.660]  у тебя просто. Уже плотность от x никак не зависит. Вот, и что мы имеем? Давайте,
[03:41:30.660 --> 03:41:36.900]  я уберу вот эту всю замену переменных, потому что это более-менее очевидно. И давайте подведем
[03:41:36.900 --> 03:41:56.740]  итог того, что мы имеем. Почти. Друзья, давайте не теряйте бдительность. Почти закончили. Так,
[03:41:56.740 --> 03:42:11.540]  мы получили с вами, что вот θ2уθ1х имеет распределение, у которого плотность в точке t
[03:42:11.540 --> 03:42:28.780]  1 поделить на 1 по квадрате. Здорово? Все рады? То есть смотрите, мы получили с вами статистику,
[03:42:28.780 --> 03:42:37.420]  вот это вот g от x, ну в нашем случае у вас данные не просто x, а x и y. g от наших данных и от
[03:42:37.420 --> 03:42:44.460]  параметров, ну в нашем случае от параметра θ1 и θ2. Смотрите, вот эта статистика, она зависит от
[03:42:44.460 --> 03:42:51.100]  наших параметров, она зависит от наших данных, но распределение этой статистики не зависит от
[03:42:51.100 --> 03:42:57.500]  наших исходных данных. Супер, значит мы готовы с вами применить метод центральной статистики,
[03:42:57.500 --> 03:43:03.020]  а метод центральной статистики заключается в том, что вот смотрите, у вас есть центральная
[03:43:03.020 --> 03:43:15.740]  статистика и вы знаете, что оно имеет какое-то распределение. Тогда вы можете взять у него две
[03:43:15.740 --> 03:43:34.180]  квантили, вот возьмем p1 и p2, это две квантили, давайте так, zp1 и zp2. Это две такие квантили,
[03:43:34.180 --> 03:43:42.820]  что p2-p1 это уровень нашего доверия. То есть мы теперь, грубо говоря, вкладываем наши статистики,
[03:43:42.820 --> 03:43:46.620]  для которой мы знаем, как выглядит распределение, не зависящее от параметра, выбираем интервал,
[03:43:46.620 --> 03:43:52.780]  вот для этой статистики, выбираем интервал, в котором оно будет заключено. Давайте конкретный
[03:43:52.780 --> 03:44:01.620]  пример рассмотрим. Сначала найдем функцию, так, ты наверное зря написал, найдем функцию распределения
[03:44:01.620 --> 03:44:08.260]  сначала, чтобы найти квантильную функцию. Функция распределения, это просто интегралов плотности,
[03:44:13.820 --> 03:44:29.100]  ну там от минус бесконечности до x нашей плотности, dt. Вот, а этот интегралчик к чему равен?
[03:44:29.100 --> 03:44:38.260]  Ну это просто полином. Ну это просто полином, да, мы на самом деле можем 1 и т сюда записать. Это
[03:44:38.260 --> 03:44:47.380]  что у нас получится? 1 поделить на 1 плюс t, с минусом, по-моему, только, да? И постановка
[03:44:47.380 --> 03:44:59.860]  от минус бесконечности до x. Только там не минус бесконечность будет, а на самом деле нижняя
[03:44:59.860 --> 03:45:08.220]  граница, там будет единичка. Нолик, да, вы правы, нолик. Прошу прощения. У нас там все индикаторы больше
[03:45:08.220 --> 03:45:15.580]  нуля, да, все справедливо. Вот, ну и того ваша функция распределения имеет вид. В нолике это
[03:45:15.580 --> 03:45:27.700]  будет единичка, вот, минус единичка поделить на 1 плюс x. Справедливо? Это функция распределения,
[03:45:27.700 --> 03:45:34.940]  но почему? Сейчас мы еще запишем с вами индикатор. Ну потому что она выглядит на самом деле вот таким
[03:45:34.940 --> 03:45:44.900]  образом. Х больше либо равно ноль. Ну этот индикатор у нас просто там бы повылазил везде,
[03:45:44.900 --> 03:45:49.300]  если мы аккуратно писали. Вот, то есть на самом деле здесь у нас получится в нуле
[03:45:49.300 --> 03:45:56.900]  ноль. Вот, и потом она стремится к единичке, ну то есть все аксиомы функции распределения
[03:45:56.900 --> 03:46:01.780]  выполнены этой функцией распределения. У этой штуки есть квантильная функция. Как искать
[03:46:01.780 --> 03:46:07.060]  квантильную функцию мы с вами уже обсуждали, так ведь? Нужно найти просто обратную функцию к этой
[03:46:07.060 --> 03:46:19.220]  функции. Это мы легко сделаем. Тут как раз такой еще вид красивый у этой функции. То есть y равняется
[03:46:19.220 --> 03:46:33.860]  единичка минус 1 на 1 плюс x. Тогда обратная это y минус 1. 1 поделить на y минус 1. Это 1 плюс
[03:46:33.860 --> 03:46:53.140]  x мы сюда перенесли, минус 1. Я не ошибся? 1 минус y, согласен. Теперь точно правильно?
[03:46:53.140 --> 03:46:59.660]  Так, y минус 1, меняем знак.
[03:47:03.860 --> 03:47:07.300]  А в ладжу у нас никакой не получается.
[03:47:07.300 --> 03:47:26.820]  А квантильная функция 1 минус 1 на y минус 1. Ну да, все супер.
[03:47:26.820 --> 03:47:36.540]  Да, я согласен. Хорошо, давайте теперь тогда вот какой уровень доверия вы хотите найти. Давайте
[03:47:36.540 --> 03:47:45.060]  просто фиксируем с вами какой-нибудь уровень доверия. 0,95. 0,9. Просто проще будет. Так, какая
[03:47:45.060 --> 03:47:50.860]  точка будет соответствовать? Давайте от нуля до 0,9 квантили возьмем. Давайте нулевой квантили это
[03:47:50.860 --> 03:48:03.180]  будет просто 0, так ведь? z 0. Это просто 0, а z 0,9 это что у нас будет? 1 на 0,1 минус 1,9.
[03:48:03.180 --> 03:48:17.420]  9. Красота. Что из этого следует? Из этого следует, что вот смотрите, вероятность того, что theta2
[03:48:17.420 --> 03:48:30.300]  поделить на theta1, y на x, вот это наша центральная статистика, зажата от нуля до 9, в точности равняется 0,9.
[03:48:30.300 --> 03:48:39.380]  Так ведь? А теперь смотрите, в центральной статистике мы требовали, чтобы по theta1 и theta2 эта штука
[03:48:39.380 --> 03:48:45.900]  была непрерывная и монотонная. Навешиваем обратное преобразование, что получаем?
[03:48:45.900 --> 03:48:53.180]  В силу того, что она монотонно-непрерывная, мы можем навесить обратное и у вас будет то же
[03:48:53.180 --> 03:48:59.900]  самое. То есть у вас будет 0 меньше ли равняется theta2 на theta1, меньше ли
[03:48:59.900 --> 03:49:09.100]  равняется, чем 9ху. И вероятность у вас сохранится. Понятно?
[03:49:09.100 --> 03:49:19.340]  Нас же просили найти доверительный интервал для этой штуки. Вот две статистики, которые
[03:49:19.340 --> 03:49:23.500]  зависят от наших данных. Ну ладно, первая не зависит, а вторая вот зависит вот таким образом.
[03:49:23.500 --> 03:49:28.940]  Мы построили доверительный интервал для статистики, ну вот для вот этих двух, для отношения цены.
[03:49:28.940 --> 03:49:42.780]  Супер, это была сложная задача. Теперь давайте в общем случае как это делать.
[03:49:42.780 --> 03:49:54.580]  Ну мы с вами рассматривали изначально статистику, просто theta2y, 1, ну вот одно любое наблюдение.
[03:49:54.580 --> 03:50:04.420]  На theta1, x1, любое наблюдение. Ребят, давайте теперь в общий случай рассмотрим, что делать в общем случае.
[03:50:04.420 --> 03:50:14.500]  Эта задачка красивая была? Мне кажется очень-очень замечательная.
[03:50:14.500 --> 03:50:25.740]  Так в смысле? Ну вероятность на линейках она так и работает, что чем дальше, тем больше… ладно.
[03:50:25.740 --> 03:50:33.860]  Вот, и как мы сейчас с вами общий метод центральной статистики проговорим, и как вот эти
[03:50:33.860 --> 03:50:37.900]  центральные статистики строить. Сейчас мы с вами центральную статистику по сути просто придумали.
[03:50:37.900 --> 03:50:43.540]  Так ведь? Это сложно и неудобно. Я утверждаю, что есть общая центральная статистика,
[03:50:43.540 --> 03:50:48.180]  которая подходит для всех непрерывных распределений. Теорема.
[03:50:48.180 --> 03:50:59.580]  Вот, пусть у вас, в вашем параметрическом семействе все распределения имеют непрерывную
[03:50:59.580 --> 03:51:08.700]  функцию распределения. Знаете, какую статистику тогда можно рассмотреть? Вот, утверждение.
[03:51:08.700 --> 03:51:35.300]  Имеет гамма распределения с параметрами N1. Понятно, да? Быстенько докажем этот факт.
[03:51:35.300 --> 03:51:39.620]  То есть, смотрите, в таком случае вот это и будет центральной статистикой. Потому что,
[03:51:39.620 --> 03:51:44.100]  смотрите, оно зависит от ваших наблюдений, зависит и от ваших параметров, потому что вы
[03:51:44.100 --> 03:51:49.580]  применяете как бы неизвестную функцию распределения. Но распределение ее не зависит от параметров.
[03:51:49.580 --> 03:51:57.460]  Справедливо? Окей, давайте докажем, что это действительно так. Первый шаг. Понимаете ли вы,
[03:51:57.460 --> 03:52:08.980]  что fθ от x1 имеет равномерное распределение на отрезке 0,1? Покажем быстренько. Давайте,
[03:52:08.980 --> 03:52:24.100]  как мы это покажем? Функция распределения. Да, это просто вероятность по определению
[03:52:24.100 --> 03:52:31.380]  функции распределения θ от x меньше либо равняется t. Так как мы положили, что у нас все монотонные,
[03:52:31.380 --> 03:52:41.100]  то мы можем взять обратную. То это x меньше либо равняется чем f-1 в точке t. Вот, а это на самом
[03:52:41.100 --> 03:52:52.260]  деле опять просто функция распределения x в точке f-1 от m. Ну и все, вы применяете функцию к обратной,
[03:52:52.260 --> 03:53:00.900]  но вы получаете просто t. То есть первое утверждение, если вы просто к случайной величине,
[03:53:00.900 --> 03:53:06.860]  случайную величину вашу засунете в функцию распределения, ее же, вы получите новую
[03:53:06.860 --> 03:53:11.820]  случайную величину, которая распределена у 0,1. На этом, кстати, основан метод обратного
[03:53:11.820 --> 03:53:16.640]  преобразования. Если вы умеете генерировать вот такие штуки, то если вы на них будете вешать
[03:53:16.640 --> 03:53:20.420]  обратные функции распределения, вы будете получать новые случайные величины с той
[03:53:20.420 --> 03:53:24.340]  функцией распределения, с которой вам надо. Показывается точно так же, только для f-1.
[03:53:24.340 --> 03:53:31.500]  Ну да, то есть на самом деле, если вы умеете генерировать 0,1, а компьютер только это и умеет
[03:53:31.500 --> 03:53:40.940]  делать. Он умеет 0,1 делать. Из 0,1 можно составить любое число на отрезке от 0 до 1, применить метод
[03:53:40.940 --> 03:53:47.460]  обратного преобразования и получить. Да, ну там могут быть проблемы, если у вас функция распределения
[03:53:47.460 --> 03:53:51.900]  существует, или если она какая-нибудь очень редко растущая, там другие методы тогда используются.
[03:53:51.900 --> 03:53:56.060]  Но вот есть вот такой метод обратного преобразования. Итак, первый факт, который мы с вами установили,
[03:53:56.060 --> 03:54:15.700]  что вот это, это у 0,1. Если мы на нашу вот, смотри, вот эта штучка имеет функцию распределения f
[03:54:15.700 --> 03:54:30.420]  от t. Если мы ее же засунем в функцию распределения, то есть мы возьмем распределение от любой
[03:54:30.420 --> 03:54:35.340]  функции распределения, мы получим распределение равномерное от распределения. Да. Как бы в чем суть, у вас
[03:54:35.340 --> 03:54:41.620]  какая бы ни была функция распределения, вот такая, да, вы как бы вот, ваша чиселка сгенерированная,
[03:54:41.620 --> 03:54:54.460]  вы ее подставляете в функцию распределения, и она у вас отображается вот сюда. Понятно? То есть смотрите,
[03:54:54.460 --> 03:54:58.860]  вот ваша функция распределения истинная, вы приняли вот это значение, загнали вот сюда,
[03:54:58.860 --> 03:55:05.180]  и оно вот отображается в 0,1, и утверждается, что равномерно. Почему равномерно? Показали только
[03:55:05.180 --> 03:55:12.740]  что. Все, супер. Итак, смотрите, мы уже на самом деле практически получили с вами центральную
[03:55:12.740 --> 03:55:19.060]  статистику, потому что уже f тета от x1 уже не зависит как бы от параметра, так ведь? Но мы
[03:55:19.060 --> 03:55:24.300]  хотим как бы учесть все наблюдения, которые у нас есть, поэтому нам нужно как-то вот эти равномерные
[03:55:24.300 --> 03:55:30.100]  случайные величины усреднить, ну как-то их сложить или что-то еще сделать. Складывать просто равномерные
[03:55:30.100 --> 03:55:34.580]  случайные величины, них хорошо, они плохо складываются. Зато если мы сделаем из них
[03:55:34.580 --> 03:55:38.660]  экспоненциальные распределения, сумма экспоненциальных распределений это гамма распределения вот с такими
[03:55:38.660 --> 03:55:45.020]  параметрами. Под задачу номер два. Покажем, что у логориджа есть вот такой штуки,
[03:55:45.020 --> 03:55:54.340]  это экспоненциальное распределение с параметром один. Доказательства. Итак, пусть к себе имеет
[03:55:54.340 --> 03:56:08.140]  распределение u01, подействуем на него функцией минус логорифм t. Покажем, какую функцию распределения
[03:56:08.140 --> 03:56:13.300]  имеет данная функция распределения. Итак, функция распределения, так это f at,
[03:56:13.300 --> 03:56:29.100]  функция распределения f at в точке x. Это что такое? По определению, это вероятность того, что f at меньше
[03:56:29.100 --> 03:56:37.060]  либо равна x. Так ведь? Монотонное все нормально, обратное, накидываем обратное преобразование,
[03:56:37.060 --> 03:56:47.500]  t меньше либо равняется чем phi в минус 1 от x. Теперь заметим, что это по сути функция распределения
[03:56:47.500 --> 03:56:53.660]  для равномерного распределения вот в этой точке, ну а там в этой точке она равняется просто phi в
[03:56:53.660 --> 03:57:00.700]  минус 1 от x. Потому что у вас функция распределения для равномерной случайной величины, это просто x.
[03:57:00.700 --> 03:57:10.180]  Супер, а обратное преобразование, ну обратная функция к вот этой, это что? Да, это экспонента
[03:57:10.180 --> 03:57:19.720]  в степени минус t. Ну есть в степени минус t. То есть по сути мы с вами получили, что вот это у нас
[03:57:19.720 --> 03:57:24.420]  имеет экспоненциальное распределение. Ну по сути мы применили обратное преобразование q01,
[03:57:24.420 --> 03:57:29.500]  как я уже говорил. Если ваш компьютер умеет генерировать чиселку от 0 до единицы, применяя
[03:57:29.500 --> 03:57:35.500]  обратную функцию, мы получим функцию с тем распределением, которое нам нужно. Вот,
[03:57:35.500 --> 03:57:49.860]  таким образом мы получили с вами, да, мы показали, что функция распределения phi от t имеет просто
[03:57:49.860 --> 03:57:54.140]  функцию распределения, равную функцию распределения экспоненциального распределения. Таким образом,
[03:57:54.140 --> 03:58:00.940]  мы показали, что phi от t имеет экспоненциальное распределение. Супер? Всё, смотрите, дальше здесь
[03:58:00.940 --> 03:58:04.540]  уже свойство экспоненциального распределения. Если мы n слагаемых экспоненциальных сложим,
[03:58:04.540 --> 03:58:08.860]  то у нас с одинаковым параметром, то у нас как раз таки получится гамма распределения с параметром
[03:58:08.860 --> 03:58:14.260]  n1. Всё, мы построили с вами центральную статистику в общем виде. Если у вас функция распределения
[03:58:14.260 --> 03:58:21.140]  непрерывна, то центральную статистику можно построить вот так. Понятно, почему это центральная
[03:58:21.140 --> 03:58:33.220]  статистика? Она зависит от параметра. Тогда вот такую штуку можно воспользоваться, да. Ну,
[03:58:33.220 --> 03:58:36.500]  её нужно как бы записать в явном виде, то есть записать вот суммирование логарифма вот этих
[03:58:36.500 --> 03:58:45.420]  штук. Всё. И давайте вот ещё раз подрекапим, как у нас выглядит метод решения таких задач на
[03:58:45.420 --> 03:58:55.580]  доверительные интервалы. Все задачи на доверительные интервалы вот через центральные статистики
[03:58:55.580 --> 03:59:01.980]  решаются следующим образом. Вы либо придумываете, если какой-то нестандартный случай, центральную
[03:59:01.980 --> 03:59:08.900]  статистику g от х тета. Придумывайте, если что-то нестандартное. Либо пользуйтесь вот такой
[03:59:08.900 --> 03:59:14.340]  стандартной штукой. Показывайте, что это центральная статистика, то есть то, что она непрерывна и
[03:59:14.340 --> 03:59:34.060]  монотонна по тета и её распределение не зависит от параметра. В таком случае, смотрите, у вас g от
[03:59:34.060 --> 03:59:40.140]  х тета с известным распределением зажимается между какими-то квантилиями, так ведь?
[03:59:40.140 --> 03:59:55.380]  Квантили вот у этого распределения. То есть вот вы фиксируете какую-то вероятность. Больше и гамма.
[03:59:55.380 --> 04:00:03.820]  Ну и всё, отлично. Да, гамма обычная просто параметра. Вот и всё. А так как вы потребовали,
[04:00:03.820 --> 04:00:08.620]  что функция непрерывна и монотонна, вы можете взять в некотором смысле обратную и получить,
[04:00:08.620 --> 04:00:23.100]  что у вас тета меньше либо равен, чем g в минус 1, от z по 2 меньше либо равняется,
[04:00:23.100 --> 04:00:32.940]  чем g в минус 1 от z по 1. Ну и давайте скажем, что вот это верно, если функция у вас возрастает.
[04:00:32.940 --> 04:00:42.300]  Потому что если у вас g будет убывать, у вас квантилими сами меняются. Так, вот это верно,
[04:00:42.300 --> 04:00:54.460]  если g х тета возрастающее вот это. Всё, в принципе, мы научились с вами строить
[04:00:54.460 --> 04:01:00.620]  доверительные интервалы методом центральной статистики. Какое?
[04:01:02.940 --> 04:01:18.460]  Почему она покрывает непрерывность и монотонность? Смотрите, у вас f тета,
[04:01:18.460 --> 04:01:25.500]  мы положили, что она непрерывная. То, что она монотонная, мы тоже знаем. Сумма монотонных,
[04:01:25.500 --> 04:01:28.340]  ну и логарифм монотонный от монотонный тоже монотонный. То есть вы получили,
[04:01:28.340 --> 04:01:32.060]  что вот эта штука будет монотонная и непрерывная, как композиция непрерывных.
[04:01:32.580 --> 04:01:37.160]  супер, значит, это центральная статистика, потому что она, видите, ее распределение
[04:01:37.160 --> 04:01:42.860]  не зависит от параметра. Ну и она не прерывная и монотонная, соответственно,
[04:01:42.860 --> 04:01:48.620]  это центральная статистика. Вы берете вместо zP1 и zP2 квантилия вот этого распределения,
[04:01:48.620 --> 04:01:52.420]  потому что у нас центральная статистика имеет именно вот такое распределение.
[04:01:52.420 --> 04:01:55.460]  вот и потом в силу того, что она не прерывная и монотонная,
[04:01:55.460 --> 04:01:58.900]  накидываете обратное и получаете доверительный интервал для параметра тета.
[04:01:58.900 --> 04:02:03.900]  Еще одна задача?
[04:02:03.900 --> 04:02:06.900]  Последняя
[04:02:06.900 --> 04:02:11.900]  Ну, в плане, промолв сучиться, так что, окей, шаг, шаг, шаг
[04:02:11.900 --> 04:02:19.900]  И в какой-то момент, я не получаю то, что мою сумму логипмов умеет в домораспределении
[04:02:19.900 --> 04:02:21.900]  Это невозможно
[04:02:21.900 --> 04:02:24.900]  Мы только что доказали, что так всегда, если у тебя Ft, это непрерывно
[04:02:24.900 --> 04:02:26.900]  Сейчас, ну, просто задача какая-то
[04:02:26.900 --> 04:02:30.900]  Ну, может, я что-то не понял, но она, получается, какая-то очень очевидная, очень простая
[04:02:30.900 --> 04:02:32.900]  Мы говорим в нашем определении непрерывное
[04:02:32.900 --> 04:02:33.900]  Да
[04:02:33.900 --> 04:02:36.900]  А если бы они непрерывное дадут, там уже придется что-то другое придумывать
[04:02:36.900 --> 04:02:39.900]  А, то есть можно проверить на непрерывность?
[04:02:39.900 --> 04:02:40.900]  Ну да
[04:02:40.900 --> 04:02:41.900]  О, определение непрерывное
[04:02:41.900 --> 04:02:44.900]  То есть, еще раз, у вас просят построить доверительный интервал
[04:02:44.900 --> 04:02:48.900]  Первый метод построения, это через Чебышева и через всякие остальные неравенства
[04:02:48.900 --> 04:02:51.900]  Но это грубые доверительные интервалы, мы их не используем
[04:02:51.900 --> 04:02:53.900]  Скорее всего, будет метод центральной статистики
[04:02:53.900 --> 04:02:56.900]  Вот, метод центральной статистики мы с вами обсудили
[04:02:56.900 --> 04:03:00.900]  Либо вам нужно какую-то придумать самим статистику, которая будет центральной
[04:03:00.900 --> 04:03:04.900]  Либо воспользоваться общим случаем, если у вас Ft непрерывно
[04:03:06.900 --> 04:03:09.900]  То есть мы придумываем, если у нас нейния непрерывная
[04:03:09.900 --> 04:03:10.900]  Да
[04:03:11.900 --> 04:03:12.900]  Супер?
[04:03:12.900 --> 04:03:13.900]  Супер
[04:03:13.900 --> 04:03:16.900]  А, кстати, еще по доверительным интервалам нужно одну вещь добавить, друзья
[04:03:24.900 --> 04:03:26.900]  Ну задачка еще одна и все
[04:03:26.900 --> 04:03:27.900]  Давайте
[04:03:30.900 --> 04:03:32.900]  Вот за 47 минут мы точно успеем
[04:03:33.900 --> 04:03:34.900]  Обещаю
[04:03:39.900 --> 04:03:40.900]  Чего?
[04:03:47.900 --> 04:03:49.900]  Ну у меня время очень медленно идет
[04:03:53.900 --> 04:03:55.900]  Последняя задача на полной статистике
[04:03:55.900 --> 04:03:56.900]  Оптимальные оценки
[04:03:56.900 --> 04:03:59.900]  Мы просто с вами там алгоритм обсудим без жесткой теории
[04:03:59.900 --> 04:04:01.900]  Просто как-то задачу решать
[04:04:01.900 --> 04:04:02.900]  И решим задачку
[04:04:07.900 --> 04:04:10.900]  Так, все собрались, приготовились
[04:04:11.900 --> 04:04:13.900]  Еще про доверительные интервалы одну штуку надо сказать
[04:04:16.900 --> 04:04:19.900]  Как строить асинтетически доверительные интервалы?
[04:04:19.900 --> 04:04:22.900]  На самом деле доверительные интервалы можно строить не всегда
[04:04:22.900 --> 04:04:25.900]  Иногда гораздо проще построить асинтетически доверительный интервал
[04:04:25.900 --> 04:04:27.900]  Давайте быстро поем
[04:04:27.900 --> 04:04:28.900]  Это очень просто
[04:04:28.900 --> 04:04:31.900]  Вот пусть у вас есть асимпатический нормальная оценка
[04:04:32.900 --> 04:04:34.200]  Чтобы построить доверительный интервал
[04:04:34.900 --> 04:04:36.900]  Нужны асимпатически нормальные оценки
[04:04:38.900 --> 04:04:40.900]  То, что асимпатически нормальная
[04:04:40.900 --> 04:04:43.900]  Следует, что она представима вот так
[04:04:52.900 --> 04:04:59.180]  справедливо вот то есть возможно там что-то с распределением получилось не
[04:04:59.180 --> 04:05:03.460]  очень удачно но зато у вас получилось симпатически нормальная оценка знаете
[04:05:03.460 --> 04:05:09.940]  что из этого следует из этого следует что вот мы как и в центральной
[04:05:09.940 --> 04:05:12.780]  статистике хотим получить предельное распределение которое будет не зависеть
[04:05:12.780 --> 04:05:19.780]  от параметра нормально но смотрите сейчас она зависит от параметра видите как
[04:05:19.780 --> 04:05:22.460]  и в центральной статистике мы хотим получить чтобы вот это вот распределение
[04:05:22.460 --> 04:05:26.580]  которое справа стоит не зависит от параметра что мы для этого хотели бы
[04:05:26.580 --> 04:05:31.180]  сделать наверное как-то отнормировать чтобы здесь стало n 0 1
[04:05:34.660 --> 04:05:39.380]  так ведь вот если мы отнормируем получим такую штуку то все дальше
[04:05:39.380 --> 04:05:45.620]  рассуждение абсолютно как в центральной статистике да как мы это
[04:05:45.620 --> 04:05:58.100]  сделаем предлагается поделить на это 0 на сигма тета но это не совсем правда
[04:05:58.100 --> 04:06:01.540]  сейчас не будем даваться подробности почему это не совсем правда и тут сейчас
[04:06:01.540 --> 04:06:08.140]  уже становится сложнее гораздо выразить параметр тета ну тут это 0 это 0 это 0
[04:06:08.140 --> 04:06:13.980]  это 0 но на самом деле по лиме сутского мы можем это домножить на вот такую
[04:06:13.980 --> 04:06:21.580]  штуку то есть вместо вот истинного значения который мы не знаем подставить
[04:06:21.580 --> 04:06:28.220]  нашу оценку которую мы знаем смотрите по лиме сутского вот эта штука сходится
[04:06:28.220 --> 04:06:32.900]  к единице почему потому что у вас оценка вот это вот асимпатически нормально для
[04:06:32.900 --> 04:06:38.940]  вот этого параметра мы навешиваем на нее непрерывную функцию так это все еще
[04:06:38.940 --> 04:06:44.220]  сходится соответственно отношение будет сходиться к единичке все и по
[04:06:44.220 --> 04:06:48.380]  лиме сутского вот это сходится константе ну справа типа нам нужно на
[04:06:48.380 --> 04:06:54.420]  единицу все и получим вот вот это у вас сократится и из этого мы получим с вами
[04:06:54.420 --> 04:06:57.420]  что есть вот такая сходимость
[04:06:57.420 --> 04:07:14.700]  сходится к нормальному распределению с параметрами 0 1 справедливо отлично все мы
[04:07:14.700 --> 04:07:17.940]  решили мы построили асимпатически доверительный интервал в чем его
[04:07:17.940 --> 04:07:22.460]  отличие от точного точный он сразу действует а вот тут как бы у вас вот это
[04:07:22.460 --> 04:07:25.660]  распределение только в пределе достигается то есть вам нужно много
[04:07:25.660 --> 04:07:30.760]  наблюдений чтобы построить доверительный интервал а все очень просто просто ты
[04:07:30.760 --> 04:07:33.540]  считаешь что с какого-то момента у тебя это уже почти нормальное распределение
[04:07:33.540 --> 04:07:38.260]  ты берешь квантилию вот этого распределения и строишь также как и с
[04:07:38.260 --> 04:07:41.740]  центральной статистикой то есть как будет выглядеть доверительный интервал
[04:07:41.740 --> 04:07:47.420]  легко понять вы берете квантилию нормального распределения
[04:07:47.420 --> 04:07:53.500]  они через У обычно обозначаются. Обычно берутся центральные, вот такие.
[04:08:09.020 --> 04:08:14.780]  Это в пределе выполняется, так ведь? И все, и смотрите, вы просто домножаете на
[04:08:14.780 --> 04:08:19.660]  вот эту дисперсию, по сути, делите на корень из n, в общем, оставляйте здесь только тета 0.
[04:08:19.660 --> 04:08:24.180]  Просто домножайте на обратные функции с двух сторон неравенства, и все, и у вас тета 0 здесь
[04:08:24.180 --> 04:08:29.640]  остается, а вот здесь остаются хватчики. Ну то есть, вот это переписывается, давайте аккуратно
[04:08:29.640 --> 04:08:44.400]  делаем. Думаешь? Ну тут будет тета так, плюс поделить на корень из n, сигма так, на квантиль.
[04:08:44.400 --> 04:08:50.920]  1 минус гамма пополам. Меньше либо равняется, чем тета 0, и меньше либо равняется, чем то же
[04:08:50.920 --> 04:08:59.160]  самое, только с другой квантилью. Мы построили асимпатический доверительный интервал. Ход
[04:08:59.160 --> 04:09:03.880]  рассуждений примерно такой же, как в центральной статистике, вы получаете что-то, чье распределение
[04:09:03.880 --> 04:09:09.280]  не зависит от параметра. Так ведь? Не зависит от параметра. Здесь вот мы воспользовались таким
[04:09:09.280 --> 04:09:13.640]  трюком и получили вот такую исходимость какому-то распределению без параметра. Но один минус это
[04:09:13.640 --> 04:09:17.240]  асимпатический доверительный интервал, то есть вам нужно достаточно много наблюдений, чтобы начал
[04:09:17.240 --> 04:09:29.360]  работать. Ну типа да. Ну вот допустим обратную функцию для функции распределения нормального
[04:09:29.360 --> 04:09:33.280]  вы не найдете, потому что у вас нет просто функции распределения нормальной, поэтому все квантили для
[04:09:33.280 --> 04:09:38.520]  нормального распределения стандартного это вообще типа табличные данные. Они вычислены там на
[04:09:38.520 --> 04:09:43.560]  компьютере уже давно и типа при решении их можно использовать какие-то константные. Ну у вас точно
[04:09:43.560 --> 04:09:47.760]  вот в праке по моцетатам что-то такое будет? Это прям сто процентов. Все,
[04:09:47.760 --> 04:09:57.120]  продоверительный интервал закончили? Еще одна задачка. Не, ну пока все понятно более-менее.
[04:09:57.120 --> 04:10:10.240]  Отлично, если все понятно, тогда мы сейчас быстренько последнюю задачку разберем и разойдемся. Я буду
[04:10:10.240 --> 04:10:25.040]  скутять по вам, друзья. У нас среду семинар, да? В среду. А потом воскресенье. Так, вот тут у нас
[04:10:25.040 --> 04:10:31.000]  сразу будет несколько определений. На самом деле вот последняя тема, она самая глубокая с точки
[04:10:31.000 --> 04:10:39.600]  зрения теории, а с точки зрения практики нет. Поэтому мы сейчас просто несколько определений с вами
[04:10:39.840 --> 04:10:48.000]  обозначим и покажем как решать задачи, которые скорее всего будут на достаточной оценке. Так,
[04:10:48.000 --> 04:10:56.080]  у нас уже недоверительные интервалы. Недоверительные интервалы. Уровень недоверия.
[04:10:56.080 --> 04:11:16.240]  Так, товарищи, достаточные оценки. Что нам нужно понимать? Первое, есть понятие
[04:11:16.240 --> 04:11:25.720]  о достаточной статистике. Что такое достаточная статистика? Это такая статистика,
[04:11:25.720 --> 04:11:50.160]  что при S3 не зависит от этой. Так, давайте как на это посмотрим с вами. У нас здесь какое-то
[04:11:50.160 --> 04:11:53.960]  условное распределение появляется. Это условное распределение должно зависеть от
[04:11:53.960 --> 04:12:05.000]  тета. Пример. Допустим, вы играете с монеткой. Нужно ли вам хранить все нолики и единички в
[04:12:05.000 --> 04:12:09.240]  каком месте они выпали, чтобы параметр тета оценить, чтобы все сказать про параметр тета.
[04:12:09.240 --> 04:12:14.600]  На самом деле достаточно только среднее знать. Если вы будете знать только среднее, то вы уже
[04:12:14.600 --> 04:12:18.520]  понимаете какой параметр вы его оценили, грубо говоря, во всех возможных хороших смыслах. И
[04:12:18.520 --> 04:12:23.920]  как бы знание, на каких местах у вас выпали единички, на каких нольках, вам ничего не дает. И вот как
[04:12:23.920 --> 04:12:28.800]  раз-таки достаточная статистика S от X, это вот такая статистика, что зная ее, зная, что сумма
[04:12:28.800 --> 04:12:36.240]  вернулистских случайных величин у вас там допустим, вернее средняя, там 0.3, вы уже понимаете,
[04:12:36.240 --> 04:12:43.040]  что ваше распределение не будет зависеть от тета. Вы уже оценили параметры. Да-да-да-да, это как
[04:12:43.040 --> 04:12:48.320]  раз-таки очень сильно связано с умом. Мы на семинаре более внимательно на это посмотрим. Сейчас нам
[04:12:48.320 --> 04:12:57.240]  нужно только задачки научиться решать. Вот, это первое определение. Сразу теорема. Критерий
[04:12:57.240 --> 04:13:11.640]  факторизации. Вот такую в некотором смысле достаточную статистику сложно проверять по
[04:13:11.720 --> 04:13:15.520]  определению. То есть вам нужно найти, чтобы вот условное распределение не зависело и достаточно
[04:13:15.520 --> 04:13:19.720]  сложно проверять. Есть простой на самом деле критерий проверки того, что статистика является
[04:13:19.720 --> 04:13:26.000]  достаточной. Таким критериями является критерий факторизации. Он утверждает, что статистика S от X
[04:13:26.000 --> 04:13:43.720]  достаточно тогда и только тогда, когда ваша плотность представима в виде вот такого
[04:13:43.720 --> 04:13:53.640]  произведения. Давайте сейчас немножко на него посмотрим, помедитируем. То есть как бы смотрите,
[04:13:53.640 --> 04:14:06.680]  понятно, что просто функция какая-то. То есть смотрите, вы можете какую-то часть плотности
[04:14:06.680 --> 04:14:11.880]  как бы откинуть в сторону, да? Она как бы уже от параметра, видите, не зависит. Как бы какая идея.
[04:14:11.880 --> 04:14:16.640]  Зная только, чему S от X равно, вы понимаете, чему у вас равен theta как бы в некотором смысле.
[04:14:16.640 --> 04:14:31.640]  Ну типа смотри, плотность у тебя, давай просто плотность рассмотрим как сумму каких-нибудь параметров,
[04:14:31.640 --> 04:14:50.880]  да? Ты знаешь S от X? Это какие-нибудь коэффициенты перед ними? Там K1, K2, K3. А что-то еще от выборки?
[04:14:50.880 --> 04:15:00.080]  Что-то лишнее? Какие-нибудь еще знания про выборку? Они как бы отдельно, так сейчас, как бы отдельно,
[04:15:00.480 --> 04:15:06.520]  они как бы отдельно рассматриваются. Тут какие-то H от X, а вот тут какие-то статистики. Это 1 от X,
[04:15:06.520 --> 04:15:21.560]  здесь какая-то статистика S2 от X, здесь вот статистика S3 от X будет. Как бы вот эти статистики
[04:15:21.560 --> 04:15:28.460]  влияют на параметры, да? В некотором смысле, они определяют как выглядит функция. А вот эти
[04:15:28.460 --> 04:15:33.260]  статистики? Нет, вот то есть 2-ой множитель как бы не влияет на то, как у тебя параметры будут
[04:15:33.260 --> 04:15:43.980]  видеть твою плотность. Вот. Сейчас сразу за то пример сможем привести. То есть там на самом деле
[04:15:43.980 --> 04:15:48.060]  можно это в лоб проверять, выполняется критерий факторизации или нет. Но на самом деле у нас есть
[04:15:48.060 --> 04:15:52.940]  целое семейство хороших распределений, для которых это сразу выполнено. Я думаю вы сможете сказать,
[04:15:52.940 --> 04:15:59.860]  какое? Экспоненциальное. Но мы как раз его ровно в таком виде примерно и записывали. Мы записывали,
[04:15:59.860 --> 04:16:05.700]  что у нас есть какая-то функция от X независящая, о, вернее от это независящая, на экспоненту от
[04:16:05.700 --> 04:16:13.940]  параметров и каких-то других функций. Вот. На самом деле для экспоненциальных распределений существуют
[04:16:13.940 --> 04:16:19.580]  хорошие достаточные статистики. Понятно, что в качестве от X можно положить равным единичку,
[04:16:19.580 --> 04:16:24.340]  и тогда, грубо говоря, всю плотность учитывается как достаточную статистику. То есть как бы вся
[04:16:24.340 --> 04:16:28.820]  плотность у вас все говорит о распределении, а вы хотите меньше информации взять, но получить
[04:16:28.820 --> 04:16:35.460]  столько же, сколько и от всей большой. Ключевая идея. Вот. И теперь давайте какой-нибудь пример с
[04:16:35.460 --> 04:16:47.180]  вами рассмотрим. Ну вот. Примерно понятно, почему для экспоненциальной модели у вас выполнен критерий
[04:16:47.180 --> 04:17:08.900]  факторизации. Давайте запишем его. То есть у вас в некотором смысле здесь вот единичка,
[04:17:08.900 --> 04:17:13.980]  да, входит? Ну это как бы единичка. А дальше у вас вот какие-то вот статистики вот эти идут. И на самом
[04:17:13.980 --> 04:17:19.060]  деле вот эти статистики будут достаточными. Потому что вот по критерию факторизации, вот у вас
[04:17:19.060 --> 04:17:24.260]  h от X, который от параметра никак не зависит. Мы как бы его игнорируем. И у вас вот параметр
[04:17:24.260 --> 04:17:29.100]  входит через вот эти статистики. То есть как бы от них именно зависит. От вот этих статистик не
[04:17:29.100 --> 04:17:36.420]  зависит. Вот. И утверждается тогда, что вот эти t и t, они как раз таки будут достаточными статистиками.
[04:17:36.420 --> 04:17:45.500]  Вот в частности из этого будет следовать, что для Bernoulli у вас средняя, это будет
[04:17:45.500 --> 04:17:49.700]  достаточная статистика, как мы с вами и проходили. Ну как мы с вами это только что обсуждали. Что
[04:17:49.700 --> 04:17:54.180]  типа не надо знать на каких местах у вас 0 и 1. Достаточно знать, что средняя элементическая равно
[04:17:54.180 --> 04:18:03.780]  чему-то там. Ну как раз таки, как у вас для Bernoulli будет выглядеть? Как плотность будет выглядеть
[04:18:03.780 --> 04:18:26.940]  Bernoulli? Bernoulli не биномиальная. В степени 1-х. Давайте к экспоненциальной форме приведем.
[04:18:26.940 --> 04:18:44.860]  Что у нас в экспоненциальной форме получится? Икс алгорифм п. Плюс 1-х на алгорифм. Вот. Теперь
[04:18:44.860 --> 04:18:52.540]  на самом деле все, что с иксом можно собрать в одном месте. Вот. И на самом деле смотрите,
[04:18:53.140 --> 04:18:58.420]  что здесь у вас в итоге получится? У вас здесь в итоге получится экспоненциальное распределение.
[04:18:58.420 --> 04:19:08.580]  Икс. Так, давайте сначала то, что без икса. Алгорифм 1-п. Прошу прощения, я что-то вертикально пишу.
[04:19:15.580 --> 04:19:19.820]  Так, вот здесь вот значок равенства пишу, обожу его в кружочек, продолжаю равенство вот здесь.
[04:19:22.540 --> 04:19:29.860]  Это получается имеет вид экспоненциальная экспонента. То, что без икса, сначала соберем алгорифм 1-п.
[04:19:29.860 --> 04:19:48.140]  Плюс икс. То, что с иксом соберем, там будет алгорифм п. Вот он у нас. Минус алгорифм 1-п. Вот. Ну и как
[04:19:48.740 --> 04:19:52.940]  утверждалось, t от икс у вас будет являться достаточной статистикой. То есть для одного
[04:19:52.940 --> 04:19:58.020]  наблюдения это икс. Ну, понятно, для многих наблюдений это просто икс средний. Ну, то есть
[04:19:58.020 --> 04:20:05.580]  t штрих от икс будет у вас достаточной статистикой. То есть как более глубоко на семинаре познакомитесь,
[04:20:05.580 --> 04:20:12.580]  но вот сейчас у нас будет несколько цепочек определений друг за другом. Первое определение
[04:20:12.580 --> 04:20:17.060]  достаточность. Если вам дают экспоненциальное семейство, сразу понятно, какие статистики будут
[04:20:17.060 --> 04:20:22.980]  достаточными. Ну, вы просто записываете в таком виде, применяете критерии факторизации и сразу
[04:20:22.980 --> 04:20:29.780]  увидите, какие у вас статистики достаточно. Справедливо? Справедливо. А если не экспоненциальное
[04:20:29.780 --> 04:20:36.500]  семейство, то это нужно уже расписывать плотность и пытаться получить ее вот в таком виде. Для
[04:20:36.500 --> 04:20:43.660]  экспоненциального сразу выполнено, для других нужно посидеть, помучиться. Есть пример, когда для
[04:20:43.660 --> 04:20:47.860]  не экспоненциального распределения это вот работает. Не для экспоненциального семейства.
[04:20:47.860 --> 04:20:58.580]  Есть хороший, понятный, простой критерий вот этой штуки. Супер? Мы близки к победе.
[04:20:58.580 --> 04:21:12.740]  Следующее определение. Нам всего нужно, друзья, три определения сделать и мы решаем задачу за три
[04:21:12.740 --> 04:21:23.180]  минуты. Ну, может быть, за пять. Я буду, я ускорюсь. К часу закончим. К часу обещаю.
[04:21:23.180 --> 04:21:50.100]  Следующее понятие, полные статистики. Так, смотрите, тут такое немножко контроиттуитивное
[04:21:50.100 --> 04:21:55.180]  определение. Если вы его сейчас не поймете, ничего страшного. А знаете, на семинаре главное
[04:21:55.180 --> 04:22:04.380]  понять, как задачи решать, правильно? Давайте просто дадим определение этой штуке. Смотрите,
[04:22:04.380 --> 04:22:18.060]  что если у вас статистика S от X называется полной, если для любой функции Борелевской из того,
[04:22:18.060 --> 04:22:29.260]  что от ожидания равно нулю, следует, что для любого тета F от S от X равняется нулю почти
[04:22:29.260 --> 04:22:36.860]  и всюду. Ну, почти, наверное. Контроиттуитивное определение пока что не совсем понятное,
[04:22:36.860 --> 04:22:41.900]  но в некотором смысле, смотрите, достаточная статистика это значит такая статистика,
[04:22:41.900 --> 04:22:47.460]  которая в себе очень много информации несет. А полная статистика, наоборот, несет в себе
[04:22:47.860 --> 04:22:54.060]  не всю информацию. Если у вас, грубо говоря, ноль оценивается только единственным образом.
[04:22:54.060 --> 04:23:09.620]  Только единственным образом. Ну, смотрите, у вас ноль, вот ожидание F от S от X равно нулю для
[04:23:09.620 --> 04:23:16.100]  любой F Борелевской. Из этого должно следовать, что у вас сама функция везде ноль. То есть,
[04:23:16.100 --> 04:23:19.780]  вы как бы не можете ноль оценить двумя разными способами. У вас какая-то статистика такая,
[04:23:19.780 --> 04:23:23.220]  то есть там с квадратом могут быть проблемы, когда у вас знаки, типа, чередуются, и вы
[04:23:23.220 --> 04:23:28.180]  можете бы читать и прибавлять. Давайте не будем сильно зацикливаться на этом, просто вот определение.
[04:23:28.180 --> 04:23:34.420]  Проверять можно по определению, то есть, смотрите, если вас просят проверить, является ли статистика
[04:23:34.420 --> 04:23:40.300]  полной, можно воспользоваться критерием, который мы сейчас приведем, признаком, вернее. А можно
[04:23:40.300 --> 04:23:44.860]  просто привести функцию или доказать, что для любой F вот это будет полной статистикой. На семинарах,
[04:23:44.860 --> 04:23:48.100]  если что, вы можете обсудить более подробно. Сейчас, к сожалению, времени у нас нет,
[04:23:48.100 --> 04:24:00.260]  если мы хотим задачку решить. Обсудим позже. Хорошо? Значит, вот есть полные статистики. Хороший,
[04:24:00.260 --> 04:24:13.340]  понятный признак. Признак полноты. Он тоже только в экспоненциальном семействе выполняется.
[04:24:14.860 --> 04:24:26.940]  Вот помните, у нас в определении экспоненциального семейства появлялись функции а0, а1 от тета и
[04:24:26.940 --> 04:24:39.100]  так далее. А0 от тета, а1 от тета и так далее. Так вот, если вот это множество содержит хотя бы
[04:24:39.100 --> 04:24:49.260]  одну внутреннюю точку, то тогда вот эти статистики, которые у нас стоят х стояли,
[04:24:49.260 --> 04:24:56.060]  они будут полными. Но у вас вот это множество телесно. То есть, у вас они все линейно независимы
[04:24:56.060 --> 04:25:02.380]  в окрестности какой-нибудь, да? И они содержат одну маленькую точку внутри. Для какого-то тета. То
[04:25:02.380 --> 04:25:08.820]  есть, у вас, грубо говоря, здесь есть параллелограмм какой-то. Да, это инмерное пространство,
[04:25:08.820 --> 04:25:14.500]  и там это используется при доказательстве того, что вот такая статистика будет полной. Да,
[04:25:14.500 --> 04:25:19.740]  вы натягиваете линейную оболочку и показываете, что она вот в этом... Сколько у вас тут функций? Ка
[04:25:19.740 --> 04:25:27.580]  плюс одна получается? Что она не вырождена, то есть, что у нее, грубо говоря, есть объем. Если есть хотя
[04:25:27.580 --> 04:25:32.380]  бы одна точка, ну это для всех наших классических, красивых распределений выполняется экспоненциальных.
[04:25:32.380 --> 04:25:37.460]  То есть, на самом деле, если вот это выполнено, тогда вот эти вот ииты, которые, помните,
[04:25:37.460 --> 04:25:51.540]  у нас с вами шли с коэффициентами аитах, будут полной статистикой. Что левое? А, ну вы просто
[04:25:51.540 --> 04:25:56.380]  смотрите на то, какие у вас функции, если они линейно независимы. Ну смотрите, чтобы у вас отсутствовал
[04:25:56.380 --> 04:26:01.500]  объем, у вас должна быть линейная зависимость, так ведь? Показывайте, что функции линейной независимой
[04:26:01.500 --> 04:26:09.980]  в окрестностях какой-нибудь точки у вас есть объем. Да, есть хотя бы одна точка внутренняя,
[04:26:09.980 --> 04:26:15.500]  все, значит, у вас уже не нулевой объем. Соответственно, вот этот набор статистик ТХ будет как раз
[04:26:15.500 --> 04:26:20.980]  таки тот, который нам нужен. Отлично, это полные статистики. То есть, достаточные — это те,
[04:26:20.980 --> 04:26:25.780]  которые хранят достаточные информации. Полные — это те, которые, грубо говоря, одним образом
[04:26:25.780 --> 04:26:40.980]  представляют ноль через мотож. И теперь мы с вами готовы. Мы с вами готовы к основной теореме
[04:26:40.980 --> 04:26:56.780]  сегодняшнего вечера. Ну да. Так, а где мой листочек? Вот, теорема. Называется теорема
[04:26:56.780 --> 04:27:03.980]  «Лемо на шафе». Это одна из самых таких интересных и нетривиальных теорем-курсов.
[04:27:03.980 --> 04:27:23.380]  Теорема «Лемо на…» Надо на лекции ходить. Лекторов. Так, ладно. Это шутка плохая.
[04:27:23.380 --> 04:27:49.580]  Что? Да можно не вырезать ничего. Я шучу же. Интересное определение.
[04:27:49.580 --> 04:28:00.340]  Непонятно ничего, да? Так, друзья, давайте мы вот что с вами заметим. Вот смотрите, пусть у нас
[04:28:00.340 --> 04:28:07.140]  S от X. Вот мы с вами два определения ввели — полнота и достаточность. Так вот, пусть S от X будет
[04:28:07.140 --> 04:28:25.740]  сразу полный и достаточный. Одновременно оба условия выполняются. Так, только тут какое-то
[04:28:25.740 --> 04:28:39.500]  не очень красивое определение. Одну секундочку. Я сейчас возьму хорошую формулировку этой теоремы,
[04:28:39.500 --> 04:28:51.700]  потому что она очень важная. Так, теорема «Лемо на шафе». Вот. Смотрите, потом у вас есть несмещённая
[04:28:51.700 --> 04:29:11.340]  оценка. Несмещённая оценка. То есть, математическое ожидание вот этой оценки — это вот. Вот из этих
[04:29:11.340 --> 04:29:21.300]  вот условий следует вот такая импликация. Новая оценка, которая получена как условный
[04:29:21.300 --> 04:29:39.780]  математическое ожидание, будет оптимальной. Оптимальная — это наилучшая как раз таки в
[04:29:39.780 --> 04:29:46.780]  средне квадратичном подходе. То есть, в чём суть? У вас есть какая-то полная достаточная статистика,
[04:29:46.780 --> 04:29:55.700]  и есть какая-то оценка несмещённая. Тогда вот эту несмещённую оценку можно улучшить полной
[04:29:55.700 --> 04:30:04.180]  достаточной статистики до самой наилучшей в классе оценки. Там более-менее она всегда существует.
[04:30:04.180 --> 04:30:18.860]  В широком классе случаев существует? Да. Тут как раз таки вот нужно очень много всего потребовать.
[04:30:18.860 --> 04:30:24.540]  Мы сейчас с вами задачку решим и пойдём. То есть, смотрите ещё раз, SLX и полная, и достаточная
[04:30:24.540 --> 04:30:32.140]  одновременно. У вас есть несмещённая оценка какого-то функции от параметров. Вот это, по сути,
[04:30:32.140 --> 04:30:37.340]  одно и то же. В таком случае вы можете улучшить вашу несмещённую оценку, полной достаточной,
[04:30:37.340 --> 04:30:43.300]  и получить оптимальную оценку, то есть самую лучшую в этом классе, для вот той функции,
[04:30:43.300 --> 04:31:03.700]  для которой у вас была и оценка несмещённая. Примерно так, но давайте мы с вами теперь подумаем,
[04:31:03.700 --> 04:31:08.100]  как решать задачу. Теперь, смотрите, вас просят найти. Всё, вот в таких задачах вас сразу просят
[04:31:08.100 --> 04:31:15.340]  найти. Найдите оптимальную оценку для какого-то параметра. Давайте я запишу условия. Последние
[04:31:15.340 --> 04:31:32.740]  задачи. Десять минут, друзья. Смотрите, условия задачи. К сожалению, у нас просто времени не
[04:31:32.740 --> 04:31:36.300]  хватает нормальную систему обсудить. Там, на самом деле, очень много можно всего сказать,
[04:31:36.300 --> 04:31:49.060]  но время ограничено. У нас 11-я аудитория. Пусть вот это будет экспоненциально с параметром
[04:31:49.060 --> 04:31:56.780]  тета. Не, ну не хочется задерживаться просто. Вот, и вас просят найти оптимальную оценку
[04:31:56.780 --> 04:32:17.340]  параметра тету в квадрате. Давайте посмотрим на теорему Лемо на шафе. Она, по сути,
[04:32:17.340 --> 04:32:21.500]  ключевая в данном разделе, и мы будем пользоваться для решения данной задачи.
[04:32:21.500 --> 04:32:36.380]  Что нам нужно сделать? Нам нужно найти полную достаточную статистику. А что дальше нам нужно
[04:32:36.380 --> 04:32:43.420]  сделать? Вот нашли мы с вами полную достаточную статистику. Действительно, нам нужно найти
[04:32:43.420 --> 04:32:48.700]  несмещенную оценку. Но более того, мы с вами кое-что знаем. Мы знаем, что вот это это условное
[04:32:48.700 --> 04:32:53.660]  математическое ожидание. Условное математическое ожидание, помните, это же какая-то бареллевская
[04:32:53.660 --> 04:33:01.100]  функция от условия. Так ведь? Было такое. То есть, на самом деле, нам нужно просто решить уравнение
[04:33:01.100 --> 04:33:11.460]  несмещенности. Всё, тогда будет выполнена теорема Лемо на шафе. Мы находим какую-то полную достаточную
[04:33:11.460 --> 04:33:19.860]  статистику S от X. Решаем уравнение несмещенности. И что мы с вами получаем? Мы как раз попадаем в
[04:33:19.860 --> 04:33:24.700]  условия теоремы Лемо на шафе. Вот полная достаточная, вот у нас несмещенная. Тогда вот то,
[04:33:24.700 --> 04:33:36.100]  что мы получим, будет хорошей оценочкой. Тогда вот эта функция phi от S от X и будет оптимальной
[04:33:36.100 --> 04:33:47.660]  оценкой. Окей? Это уравнение нефункциональное? Да. Ещё что? Уравнение нефункциональное? Ну да,
[04:33:47.660 --> 04:33:50.900]  то есть, найти такую функцию, это сложно иногда бывает. Вот сейчас мы рассмотрим не тривиальный
[04:33:50.900 --> 04:34:08.300]  случай. Вот как раз таки вот это условное отожидание, это вот наша новая оценка phi от S от X. Но вот
[04:34:08.300 --> 04:34:14.300]  эта оценка, она должна быть несмещенной, а S от X должна быть полной достаточной. Ребят,
[04:34:14.300 --> 04:34:17.620]  давайте по формулировке и плану решения. Есть вопросы? Это сейчас самое важное.
[04:34:20.900 --> 04:34:32.300]  Мы находим полную достаточную статистику, решаем уравнение несмещенности, находим такую
[04:34:32.300 --> 04:34:36.620]  функцию, чтобы вот эта штучка была несмещенная. И всё, и мы по сути попадаем с вами в условия
[04:34:36.620 --> 04:34:41.340]  теоремы Лемо на шафе, получаем оптимальную оценку. Получаем, что phi от S от X, оптимальная оценка.
[04:34:41.340 --> 04:34:47.340]  Давайте с вами обсудим план решения. Вот смотрите, если мы работаем с неэкспоненциальным семейством,
[04:34:47.340 --> 04:35:00.260]  тогда вам нужно полноту отдельно проверять по определению. Ну вот здесь у нас будет, да,
[04:35:00.260 --> 04:35:04.780]  действительно, мы оцениваем theta квадрат, должно быть вот так. Наша конкретная задача, вот мы для
[04:35:04.780 --> 04:35:11.380]  theta квадрата ищем, поэтому ищем несмещенную оценку вот такую. Итак, давайте обсудим с вами
[04:35:11.380 --> 04:35:16.140]  план решения. Это в принципе все оптимальные оценки ищутся примерно таким образом. Сначала
[04:35:16.140 --> 04:35:37.860]  вам нужно найти достаточную статистику. Достаточная статистика находится вVIS на
[04:35:37.860 --> 04:35:47.860]  А если вы в экспедиционном смесь, простой признак был, что если у вас там телесин в париллограмм, то есть если у вас есть хотя бы одна внутренняя точка, то статистика будет еще и полной.
[04:35:48.860 --> 04:35:52.860]  Вот эти вот t и x, которые у нас были в определении. Сейчас мы делаем это с вами.
[04:35:53.860 --> 04:35:57.860]  А потом просто нужно найти такую функцию, чтобы вот в отожидании от нее было это в квадрате.
[04:35:57.860 --> 04:36:10.860]  Да, мы работаем с экспедиционным смесь, поэтому по сути, ну вот просто с вами разочек покажем аккуратно, что у нас почти сразу есть полная статистика и достаточно s от x.
[04:36:23.860 --> 04:36:25.860]  Так, давайте решать данную задачу.
[04:36:28.860 --> 04:36:34.860]  То есть как бы там очень много краевых случаев, которые мы, наверное, еще там часа два бы рассматривали, если бы рассматривали.
[04:36:35.860 --> 04:36:38.860]  Вот, но в случае экспедиционного смейста все да более просто.
[04:36:43.860 --> 04:36:49.860]  Шаг первый. Выпишем плотность экспедиционного распределения в каноническом виде.
[04:36:58.860 --> 04:37:00.860]  Что это у нас такое?
[04:37:01.860 --> 04:37:03.860]  Экспонента.
[04:37:04.860 --> 04:37:05.860]  Благорифм λ.
[04:37:06.860 --> 04:37:08.860]  Я уже какие-то странные скобочки пишу.
[04:37:12.860 --> 04:37:15.860]  Так, у нас там что было? Минус λх.
[04:37:17.860 --> 04:37:18.860]  Минус λх.
[04:37:19.860 --> 04:37:20.860]  Согласны?
[04:37:21.860 --> 04:37:22.860]  Да.
[04:37:23.860 --> 04:37:27.860]  Супер. У нас выполнен критерий факторизации.
[04:37:29.860 --> 04:37:31.860]  Вот у нас h от x это единичка.
[04:37:35.860 --> 04:37:36.860]  А вот это?
[04:37:37.860 --> 04:37:41.860]  Это g от s от x и от параметра.
[04:37:46.860 --> 04:37:50.860]  Ну да, это какая-то экспонента, в нее входит наша полная статистика и параметры.
[04:37:50.860 --> 04:37:52.860]  Ну смотрите, полная статистика, вот она у нас.
[04:37:53.860 --> 04:37:55.860]  Ну видно, да, что она входит.
[04:37:56.860 --> 04:37:58.860]  Вот есть функция g, это экспонента.
[04:37:59.860 --> 04:38:01.860]  Вот наша полная статистика.
[04:38:02.860 --> 04:38:04.860]  Ну просто статистика какая-то, которая будет достаточной, прошу прощения.
[04:38:05.860 --> 04:38:07.860]  Критерий факторизации проверяет достаточность.
[04:38:08.860 --> 04:38:10.860]  И вот есть параметр λ, который входит.
[04:38:11.860 --> 04:38:16.860]  Вот, мы только что с вами показали, что если мы просуммируем, то у нас x средняя будет полной статистикой.
[04:38:17.860 --> 04:38:19.860]  Прошу прощения, достаточно, я уже заговариваюсь.
[04:38:20.860 --> 04:38:21.860]  Она будет достаточной статистикой.
[04:38:34.860 --> 04:38:35.860]  Справедливо?
[04:38:46.860 --> 04:38:48.860]  Нет, ладно, справедливо.
[04:38:49.860 --> 04:38:51.860]  Тут на самом деле сумма скорее будет.
[04:38:56.860 --> 04:38:59.860]  Давайте просто аккуратно запишем v от x.
[04:39:00.860 --> 04:39:04.860]  Давайте запишем функцию правдоподобия.
[04:39:17.860 --> 04:39:19.860]  Экспонента.
[04:39:20.860 --> 04:39:21.860]  Так.
[04:39:22.860 --> 04:39:23.860]  Мы получается с...
[04:39:25.860 --> 04:39:26.860]  Уже туплю.
[04:39:29.860 --> 04:39:30.860]  N логарифм...
[04:39:32.860 --> 04:39:33.860]  N логарифм...
[04:39:34.860 --> 04:39:36.860]  N логарифм лямда...
[04:39:37.860 --> 04:39:39.860]  Минус лямда на сумму x.
[04:39:40.860 --> 04:39:43.860]  Да, вы абсолютно правы, здесь вот статистика наша будет достаточной.
[04:39:43.860 --> 04:39:44.860]  Достаточно.
[04:39:45.860 --> 04:39:49.860]  Ну да, почему вот в данном случае средняя будет плохим?
[04:39:55.860 --> 04:39:57.860]  Средняя она будет плохим.
[04:40:00.860 --> 04:40:02.860]  На самом деле, наверное, можно показать, что они будут даже эквивалентны.
[04:40:03.860 --> 04:40:05.860]  Давайте мы покажем с вами, что вот сумма от x.
[04:40:06.860 --> 04:40:08.860]  Потому что в конечном итоге она у нас через функцию войдет, там все будет хорошо.
[04:40:09.860 --> 04:40:11.860]  На самом деле и сумма от x.
[04:40:13.860 --> 04:40:15.860]  Будет являться достаточной статистикой.
[04:40:16.860 --> 04:40:18.860]  Давайте зафиксируем сумму от x.
[04:40:22.860 --> 04:40:23.860]  Вот.
[04:40:24.860 --> 04:40:29.860]  Ну смотрите, a0 на a1 от лямда это какое-то телесное множество, ну потому что они линейные независимо.
[04:40:30.860 --> 04:40:32.860]  У них есть точка, соответственно, вот это еще и будет полной статистикой.
[04:40:39.860 --> 04:40:40.860]  Какую?
[04:40:43.860 --> 04:40:50.860]  Полнота следует из того, что у нас признак полноты, что вот если вот эти коэффициенты, которые от параметров зависят,
[04:40:51.860 --> 04:40:56.860]  образуют, ну короче, линейные независимые в окрестности какой-то точки, то у вас все хорошо будет.
[04:40:57.860 --> 04:40:58.860]  Они линейные независимые.
[04:40:59.860 --> 04:41:00.860]  Так, полное достаточное.
[04:41:03.860 --> 04:41:04.860]  Ну все, друзья.
[04:41:06.860 --> 04:41:11.860]  Мы нашли вот, на первый пункт мы с вами ответили, просто из свойств экспоненциального семейства.
[04:41:11.860 --> 04:41:13.860]  Теперь нам нужно ответить на второй пункт.
[04:41:14.860 --> 04:41:21.860]  Найти какую-то функцию, чтобы вот в ожидании от этой функции было вот такое.
[04:41:24.860 --> 04:41:25.860]  Тета в квадрате.
[04:41:28.860 --> 04:41:35.860]  Что вы можете сказать про математическое ожидание экспоненциального распределения с параметром лямда?
[04:41:36.860 --> 04:41:37.860]  Один на лямдо.
[04:41:38.860 --> 04:41:43.860]  Действительно, то есть смотрите, если мы будем какие-то обычные функции применять, у нас там будет что-то один на лямдо в какой-то степени.
[04:41:44.860 --> 04:41:49.860]  Ну то есть там если мы возьмем второй момент, третий момент и так далее, то ничего хорошего у нас не получится.
[04:41:50.860 --> 04:41:54.860]  Скорее всего нам нужно будет смотреть какие-нибудь статистики вида...
[04:41:57.860 --> 04:41:59.860]  Один поделить на что-нибудь такое, да?
[04:42:00.860 --> 04:42:02.860]  На сумму...
[04:42:03.860 --> 04:42:04.860]  На сумму...
[04:42:05.860 --> 04:42:09.860]  Иксов, да, это уже что-то будет похожее там, ну если мы поделим на среднее, да, например...
[04:42:11.860 --> 04:42:16.860]  То вот это было один на лямдо, теперь если мы это перевернем, то это уже будет лямдо, да, и что-то вот такое в квадрате предлагается посмотреть.
[04:42:20.860 --> 04:42:21.860]  Справедливо?
[04:42:22.860 --> 04:42:23.860]  Ну как бы это просто пока гипотезы выдвигаем.
[04:42:24.860 --> 04:42:28.860]  Какого-то общего метода решения вот таких уравнений, его как бы не существует.
[04:42:28.860 --> 04:42:30.860]  Ну у меня как бы это...
[04:42:33.860 --> 04:42:35.860]  Так, смотрю не ошибаюсь я.
[04:42:41.860 --> 04:42:42.860]  Все правильно?
[04:42:44.860 --> 04:42:45.860]  Все правильно.
[04:42:49.860 --> 04:42:51.860]  Так, и мы хотим что-то вот такое посчитать.
[04:42:52.860 --> 04:42:55.860]  Может быть даже просто как сумму иксов, может быть не как среднее, но что-то вот такое, да, наверное.
[04:42:59.860 --> 04:43:01.860]  Какие есть предложения, как будем действовать?
[04:43:04.860 --> 04:43:09.860]  Нам нужно по сути с вами посчитать математическое ожидание, один поделить на сумму иксов.
[04:43:10.860 --> 04:43:11.860]  Как вот это сделать?
[04:43:17.860 --> 04:43:20.860]  Где иксы-то у вас определены экспоненциально?
[04:43:29.860 --> 04:43:31.860]  Сумма экспоненциальна иксов экспоненциальная.
[04:43:32.860 --> 04:43:34.860]  Сумма экспоненциальна иксов не экспоненциальная, а гамма распределения.
[04:43:35.860 --> 04:43:36.860]  А вот это уже можно использовать.
[04:43:37.860 --> 04:43:41.860]  Смотрите, у вас же здесь и от 1 до n просто складываются экспоненциальные случайные величины.
[04:43:42.860 --> 04:43:46.860]  То есть вот это на самом деле имеет точно такое же распределение, как и 1 на гамма с параметрами...
[04:43:48.860 --> 04:43:50.860]  Не n1, а nθ.
[04:43:55.860 --> 04:43:56.860]  Вот.
[04:43:59.860 --> 04:44:04.860]  Нет, все еще этим пользоваться нельзя.
[04:44:06.860 --> 04:44:09.860]  У тебя все еще математическое ожидание функция случайной величины.
[04:44:10.860 --> 04:44:13.860]  Это не f от математического ожидания x.
[04:44:14.860 --> 04:44:15.860]  Все еще вот так писать нельзя.
[04:44:16.860 --> 04:44:17.860]  Это верно только если f линейно.
[04:44:20.860 --> 04:44:21.860]  В общем случае это неверно.
[04:44:22.860 --> 04:44:23.860]  Только для линейных это верно.
[04:44:24.860 --> 04:44:25.860]  Как вот это посчитать?
[04:44:25.860 --> 04:44:26.860]  По пределению.
[04:44:30.860 --> 04:44:32.860]  1 на x, вот такой интеграл будем считать с вами, да?
[04:44:33.860 --> 04:44:34.860]  Да.
[04:44:35.860 --> 04:44:36.860]  Вот, на плотность гамма распределения.
[04:44:37.860 --> 04:44:39.860]  Плотность гамма распределения мы с вами сегодня уже записывали.
[04:44:42.860 --> 04:44:44.860]  У нас типа x в степени k минус 1.
[04:44:46.860 --> 04:44:47.860]  В степени k минус 1?
[04:44:48.860 --> 04:44:49.860]  В степени k.
[04:44:50.860 --> 04:44:51.860]  В степени k?
[04:44:52.860 --> 04:44:53.860]  В степени k.
[04:44:53.860 --> 04:44:54.860]  В степени k?
[04:44:55.860 --> 04:44:56.860]  В степени k?
[04:44:57.860 --> 04:44:58.860]  G от k.
[04:45:00.860 --> 04:45:01.860]  Вот.
[04:45:02.860 --> 04:45:03.860]  На E в степени.
[04:45:04.860 --> 04:45:05.860]  Минус.
[04:45:06.860 --> 04:45:07.860]  X на t.
[04:45:07.860 --> 04:45:08.860]  Да.
[04:45:09.860 --> 04:45:10.860]  И вот так это можно посчитать.
[04:45:13.860 --> 04:45:14.860]  Понятно.
[04:45:15.860 --> 04:45:16.860]  Ну давайте посмотрим, что у нас получится.
[04:45:17.860 --> 04:45:18.860]  Здесь у вас получится
[04:45:20.860 --> 04:45:22.860]  x в степени k минус 2, а так вот в числителе.
[04:45:23.860 --> 04:45:24.860]  В числителе.
[04:45:25.860 --> 04:45:26.860]  В числителе.
[04:45:27.860 --> 04:45:28.860]  В числителе.
[04:45:29.860 --> 04:45:30.860]  В числителе.
[04:45:31.860 --> 04:45:32.860]  В числителе.
[04:45:33.860 --> 04:45:34.860]  В числителе.
[04:45:34.860 --> 04:45:36.860]  X в степени k минус 2, а так вот в числителе.
[04:45:38.860 --> 04:45:39.860]  На самом деле на t в степени
[04:45:40.860 --> 04:45:41.860]  k минус 2.
[04:45:43.860 --> 04:45:46.860]  T t на в квадрате мы вынесем за интеграл.
[04:45:51.860 --> 04:45:53.860]  E в степени минус x на t.
[04:45:54.860 --> 04:45:55.860]  Гамма функции от k.
[04:45:55.860 --> 04:45:56.860]  Все верно.
[04:45:57.860 --> 04:45:58.860]  И это от 0 до бесконечности.
[04:45:59.860 --> 04:46:01.860]  Чему этот интегралчик равен?
[04:46:03.860 --> 04:46:04.860]  Тут опять сделаем замену.
[04:46:05.860 --> 04:46:06.860]  У нас t.
[04:46:08.860 --> 04:46:09.860]  Минус x на t.
[04:46:10.860 --> 04:46:11.860]  Значит умножим на t.
[04:46:12.860 --> 04:46:13.860]  То есть здесь будет 1 на t.
[04:46:14.860 --> 04:46:15.860]  Тето.
[04:46:16.860 --> 04:46:17.860]  Тето.
[04:46:18.860 --> 04:46:19.860]  Тето.
[04:46:20.860 --> 04:46:21.860]  Тето.
[04:46:22.860 --> 04:46:23.860]  Тето.
[04:46:23.860 --> 04:46:24.860]  Тето.
[04:46:25.860 --> 04:46:26.860]  Тето.
[04:46:27.860 --> 04:46:28.860]  Вот.
[04:46:29.860 --> 04:46:30.860]  А интеграл от вот этой штуки это будет просто
[04:46:31.860 --> 04:46:32.860]  гамма функция.
[04:46:33.860 --> 04:46:34.860]  К.
[04:46:35.860 --> 04:46:36.860]  К минус 1.
[04:46:37.860 --> 04:46:38.860]  К минус 1.
[04:46:39.860 --> 04:46:40.860]  К минус 1.
[04:46:41.860 --> 04:46:42.860]  Да, от k минус 1.
[04:46:43.860 --> 04:46:44.860]  Вы правы.
[04:46:45.860 --> 04:46:46.860]  Вот.
[04:46:47.860 --> 04:46:48.860]  Только здесь должно быть k минус 1 на самом деле.
[04:46:49.860 --> 04:46:50.860]  Здесь степень больше.
[04:46:51.860 --> 04:46:52.860]  То есть отсюда тет у нас тоже уходит.
[04:46:53.860 --> 04:46:54.860]  Сейчас я посмотрю на определение.
[04:46:55.860 --> 04:46:56.860]  Прошу прощения.
[04:47:01.860 --> 04:47:02.860]  Так.
[04:47:03.860 --> 04:47:04.860]  Е в степени х минус тета.
[04:47:05.860 --> 04:47:06.860]  К минус 1.
[04:47:07.860 --> 04:47:08.860]  Тета в степени к.
[04:47:09.860 --> 04:47:10.860]  1 на х.
[04:47:11.860 --> 04:47:12.860]  Гамма к.
[04:47:13.860 --> 04:47:14.860]  Е в степени минус х на тета.
[04:47:15.860 --> 04:47:16.860]  Так.
[04:47:17.860 --> 04:47:18.860]  Сейчас все правильно.
[04:47:19.860 --> 04:47:20.860]  Х.
[04:47:21.860 --> 04:47:22.860]  К минус 2.
[04:47:23.860 --> 04:47:24.860]  Отсюда.
[04:47:25.860 --> 04:47:26.860]  Нет, пока рано.
[04:47:27.860 --> 04:47:28.860]  Пока рано.
[04:47:29.860 --> 04:47:30.860]  Вот здесь у нас пока что собралась гамма функция
[04:47:31.860 --> 04:47:32.860]  от k минус 1.
[04:47:39.860 --> 04:47:40.860]  Нет такого ощущения.
[04:47:41.860 --> 04:47:42.860]  Короче, досчитаем.
[04:47:43.860 --> 04:47:44.860]  У меня получил.
[04:47:45.860 --> 04:47:46.860]  Да, аккуратно досчитаем сейчас.
[04:47:47.860 --> 04:47:48.860]  В смысле неважно.
[04:47:53.860 --> 04:47:54.860]  А потом семинарист у вас не разрешает
[04:47:55.860 --> 04:47:56.860]  пользоваться вольфраммой.
[04:48:05.860 --> 04:48:06.860]  Так, ну мы с вами почти посчитали.
[04:48:07.860 --> 04:48:08.860]  Смотрите.
[04:48:09.860 --> 04:48:10.860]  Получилось.
[04:48:11.860 --> 04:48:12.860]  Гамма от k минус 1, да?
[04:48:13.860 --> 04:48:14.860]  А, ну по сути получилось 1 на k тета.
[04:48:15.860 --> 04:48:16.860]  Все.
[04:48:17.860 --> 04:48:18.860]  Справедливо.
[04:48:19.860 --> 04:48:20.860]  То есть смотрите, вот это у вас гамма функция
[04:48:21.860 --> 04:48:22.860]  от k минус 1.
[04:48:23.860 --> 04:48:24.860]  А наверху k минус 2 факториал.
[04:48:25.860 --> 04:48:26.860]  Наоборот, k минус 1 факториал внизу у вас.
[04:48:27.860 --> 04:48:28.860]  А наверху k минус 2 факториал.
[04:48:29.860 --> 04:48:30.860]  То есть у вас осталось.
[04:48:35.860 --> 04:48:36.860]  Доброе утро.
[04:48:38.860 --> 04:48:39.860]  Действительно, k минус 1.
[04:48:40.860 --> 04:48:41.860]  Согласен.
[04:48:42.860 --> 04:48:43.860]  Вот так.
[04:48:44.860 --> 04:48:45.860]  Ну на самом деле это очень похоже на
[04:48:46.860 --> 04:48:47.860]  у нас в простом от ожидания
[04:48:48.860 --> 04:48:49.860]  гамма наследление ткт
[04:48:50.860 --> 04:48:51.860]  это мы с вами в начале считали.
[04:48:51.860 --> 04:48:52.860]  В начале семинара.
[04:48:53.860 --> 04:48:54.860]  То есть почти похоже.
[04:48:55.860 --> 04:48:56.860]  На какую-то константу он отличается,
[04:48:57.860 --> 04:48:58.860]  но мы потом функцию сможем такую подбить,
[04:48:59.860 --> 04:49:00.860]  чтобы у нас вот эта штука получилась
[04:49:01.860 --> 04:49:02.860]  это в квадрате ровно.
[04:49:03.860 --> 04:49:04.860]  Так ведь?
[04:49:07.860 --> 04:49:08.860]  Вот.
[04:49:09.860 --> 04:49:10.860]  Почти все готово.
[04:49:11.860 --> 04:49:12.860]  Есть одно небольшое замечание.
[04:49:13.860 --> 04:49:16.860]  Так это мы посчитали мотош 1.22.
[04:49:17.860 --> 04:49:18.860]  Да.
[04:49:18.860 --> 04:49:19.860]  Верно.
[04:49:20.860 --> 04:49:21.860]  Я тебе скажу почему.
[04:49:24.860 --> 04:49:25.860]  Потому что эта параметризация
[04:49:26.860 --> 04:49:27.860]  немножко другая.
[04:49:28.860 --> 04:49:29.860]  Нам нужно последнюю строку стереть
[04:49:30.860 --> 04:49:31.860]  и переписать ее заново.
[04:49:37.860 --> 04:49:38.860]  В смысле не надо считать.
[04:49:39.860 --> 04:49:40.860]  Мы только вот до этого момента
[04:49:41.860 --> 04:49:42.860]  дойдем и все.
[04:49:43.860 --> 04:49:44.860]  Я прошу прощения, что ввел вас
[04:49:45.860 --> 04:49:46.860]  заблуждение.
[04:49:46.860 --> 04:49:47.860]  Вот смотрите, в такой параметризации,
[04:49:48.860 --> 04:49:49.860]  я просто уже немножко сам затупил.
[04:49:51.860 --> 04:49:53.860]  Сумма экспоненциальных медичин
[04:49:54.860 --> 04:49:55.860]  с параметром θ
[04:49:56.860 --> 04:49:57.860]  это гамма распределения
[04:49:58.860 --> 04:49:59.860]  n 1 на θ.
[04:50:00.860 --> 04:50:01.860]  Вот в такой параметризации такое.
[04:50:02.860 --> 04:50:03.860]  Я в самом начале семинара говорил,
[04:50:04.860 --> 04:50:05.860]  что для гаммы распределения
[04:50:06.860 --> 04:50:07.860]  существует несколько параметризаций.
[04:50:08.860 --> 04:50:09.860]  Они эквивалентны.
[04:50:10.860 --> 04:50:11.860]  Вы можете как бы параметр масштаба
[04:50:12.860 --> 04:50:13.860]  делать 1 на θ.
[04:50:13.860 --> 04:50:14.860]  То есть можете его менять
[04:50:15.860 --> 04:50:16.860]  на противоположный.
[04:50:17.860 --> 04:50:18.860]  И у вас все получится.
[04:50:19.860 --> 04:50:20.860]  Вот.
[04:50:20.860 --> 04:50:21.860]  На самом деле в нашем случае вот.
[04:50:22.860 --> 04:50:23.860]  Да, да.
[04:50:24.860 --> 04:50:25.860]  Я тут немножко сейчас.
[04:50:26.860 --> 04:50:27.860]  Сейчас я просто перепутал параметризацию.
[04:50:28.860 --> 04:50:29.860]  То есть сейчас на самом деле нужно
[04:50:30.860 --> 04:50:31.860]  сделать все то же самое.
[04:50:32.860 --> 04:50:33.860]  Даже параметризация останется
[04:50:34.860 --> 04:50:35.860]  та же самая.
[04:50:36.860 --> 04:50:37.860]  Просто у нас здесь соотношение будет немножко другое.
[04:50:38.860 --> 04:50:39.860]  Там для разных параметризаций
[04:50:40.860 --> 04:50:41.860]  просто чуть-чуть разные формулки вылазят.
[04:50:41.860 --> 04:50:42.860]  Вот.
[04:50:43.860 --> 04:50:44.860]  В этой же параметризации просто у нас
[04:50:45.860 --> 04:50:46.860]  вот такая штучка будет.
[04:50:47.860 --> 04:50:48.860]  Да, да, да.
[04:50:49.860 --> 04:50:50.860]  Вот.
[04:50:51.860 --> 04:50:52.860]  И что у нас получится?
[04:50:53.860 --> 04:50:54.860]  1 на х.
[04:50:55.860 --> 04:50:56.860]  Плотность какая у нас получится?
[04:50:57.860 --> 04:50:58.860]  Х в степени k-1.
[04:50:59.860 --> 04:51:01.860]  Гамма от k все еще у нас остается внизу.
[04:51:02.860 --> 04:51:03.860]  θ у нас.
[04:51:04.860 --> 04:51:05.860]  θ в степени k вот такое.
[04:51:06.860 --> 04:51:07.860]  И e в степени
[04:51:08.860 --> 04:51:09.860]  минус
[04:51:09.860 --> 04:51:10.860]  θ dx.
[04:51:11.860 --> 04:51:12.860]  То есть видите, это эквивалентные как бы
[04:51:13.860 --> 04:51:14.860]  ну одно и то же.
[04:51:15.860 --> 04:51:16.860]  Можно изначально полагать гамма распределение вот таким,
[04:51:17.860 --> 04:51:18.860]  а можно как бы на 1 на θ менять.
[04:51:19.860 --> 04:51:20.860]  Это одно и то же.
[04:51:21.860 --> 04:51:22.860]  И вот в такой параметризации у вас сумма экспоненциальных
[04:51:23.860 --> 04:51:24.860]  имеет вот такое распределение?
[04:51:25.860 --> 04:51:26.860]  Вот.
[04:51:27.860 --> 04:51:28.860]  А в другой параметризации немножко
[04:51:29.860 --> 04:51:30.860]  там вот 1 на θ меняется просто на θ.
[04:51:31.860 --> 04:51:32.860]  Понятно, да?
[04:51:33.860 --> 04:51:34.860]  Извините, что вел у вас заблуждение.
[04:51:36.860 --> 04:51:37.860]  Что мы теперь имеем с вами?
[04:51:37.860 --> 04:51:38.860]  Да, теперь на самом деле все хорошо.
[04:51:43.860 --> 04:51:44.860]  Выносим θ квадрат.
[04:51:45.860 --> 04:51:46.860]  Выносим θ квадрат.
[04:51:47.860 --> 04:51:48.860]  Да, все правильно.
[04:51:49.860 --> 04:51:51.860]  И делаем замену xt на
[04:51:52.860 --> 04:51:53.860]  на что?
[04:51:54.860 --> 04:51:55.860]  На θ.
[04:51:56.860 --> 04:51:57.860]  Просто у нас там когда дает xt.
[04:51:58.860 --> 04:51:59.860]  А, нет.
[04:52:00.860 --> 04:52:01.860]  Давайте досчитаем
[04:52:02.860 --> 04:52:03.860]  и все, и пойдем по домам.
[04:52:04.860 --> 04:52:05.860]  Давайте, друзья, напрягитесь.
[04:52:05.860 --> 04:52:06.860]  Давайте, друзья, напрягитесь.
[04:52:18.860 --> 04:52:19.860]  Давайте досчитаем.
[04:52:23.860 --> 04:52:25.860]  Значит у нас остается и в степени k-2.
[04:52:28.860 --> 04:52:29.860]  xt на θ.
[04:52:30.860 --> 04:52:31.860]  Здесь у нас что будет?
[04:52:32.860 --> 04:52:33.860]  θ в степени k-2 тоже оставим?
[04:52:33.860 --> 04:52:34.860]  k-2 тоже оставим?
[04:52:35.860 --> 04:52:36.860]  А e в степени −xθ
[04:52:39.860 --> 04:52:40.860]  на γ функцию от k.
[04:52:46.860 --> 04:52:47.860]  И вынеси с вами θ в квадрате.
[04:52:52.860 --> 04:52:53.860]  И теперь делаем замену
[04:52:54.860 --> 04:52:55.860]  t равняется чему?
[04:52:56.860 --> 04:52:57.860]  xt.
[04:53:00.860 --> 04:53:01.860]  Так ведь?
[04:53:01.860 --> 04:53:03.860]  Вот, в таком случае dx это у нас что такое?
[04:53:04.860 --> 04:53:05.860]  Это на самом деле dt
[04:53:06.860 --> 04:53:07.860]  на θ.
[04:53:08.860 --> 04:53:09.860]  Одна θ у нас как раз таки сократится.
[04:53:10.860 --> 04:53:11.860]  Все правильно получается.
[04:53:12.860 --> 04:53:13.860]  То есть это будет θ
[04:53:14.860 --> 04:53:15.860]  от 0 до бесконечности.
[04:53:16.860 --> 04:53:17.860]  Вот, ну и тут будет по сути
[04:53:18.860 --> 04:53:19.860]  γ функция сейчас у нас соберется
[04:53:20.860 --> 04:53:21.860]  t в степени k-2
[04:53:22.860 --> 04:53:23.860]  e в степени −t
[04:53:25.860 --> 04:53:26.860]  делим на γ функцию
[04:53:27.860 --> 04:53:28.860]  точки k
[04:53:29.860 --> 04:53:30.860]  dt.
[04:53:31.860 --> 04:53:32.860]  И все, и вот это у нас получилось.
[04:53:33.860 --> 04:53:34.860]  θ
[04:53:39.860 --> 04:53:40.860]  здесь вот, это γ функция
[04:53:41.860 --> 04:53:42.860]  в точке k-1.
[04:53:43.860 --> 04:53:44.860]  Вот, здесь k.
[04:53:45.860 --> 04:53:46.860]  Что у нас получилось?
[04:53:48.860 --> 04:53:49.860]  Делим на k-1.
[04:53:50.860 --> 04:53:51.860]  На k-1?
[04:53:52.860 --> 04:53:53.860]  Да.
[04:53:54.860 --> 04:53:55.860]  Ну потому что γ функция в точке k
[04:53:56.860 --> 04:53:57.860]  это k-1 факториал.
[04:53:58.860 --> 04:53:59.860]  Вот здесь у вас получается
[04:53:59.860 --> 04:54:00.860]  γ функция от k-1.
[04:54:01.860 --> 04:54:02.860]  Соответственно здесь будет k-1 факториал.
[04:54:03.860 --> 04:54:04.860]  Вот, отлично, получили с вами почти.
[04:54:05.860 --> 04:54:06.860]  Но нам нужно для θ квадрат.
[04:54:07.860 --> 04:54:08.860]  То есть на самом деле давайте проделаем
[04:54:09.860 --> 04:54:10.860]  теперь то же самое,
[04:54:11.860 --> 04:54:12.860]  только для 1 поделить на x в квадрате.
[04:54:15.860 --> 04:54:16.860]  Все, теперь я утверждаю, что мы будем
[04:54:17.860 --> 04:54:18.860]  рассматривать оценку вот такого вида.
[04:54:19.860 --> 04:54:20.860]  Вот это в квадрате будет?
[04:54:21.860 --> 04:54:22.860]  То есть это у нас будет
[04:54:23.860 --> 04:54:24.860]  γ распределение в квадрате?
[04:54:27.860 --> 04:54:28.860]  Здесь будет 1 на x в квадрате?
[04:54:29.860 --> 04:54:30.860]  Нет, нет, нет.
[04:54:31.860 --> 04:54:33.860]  Гамма т 1.1 в параметре.
[04:54:34.860 --> 04:54:35.860]  Да, здесь будет параметр 1.1 на θ.
[04:54:36.860 --> 04:54:37.860]  Спасибо большое.
[04:54:38.860 --> 04:54:39.860]  Вот это очень важное замечание.
[04:54:40.860 --> 04:54:41.860]  Вот, потому что там от параметризации
[04:54:42.860 --> 04:54:43.860]  много зависит.
[04:54:44.860 --> 04:54:45.860]  И там как бы параметризация есть советская, есть
[04:54:46.860 --> 04:54:47.860]  есть зарубежная.
[04:54:48.860 --> 04:54:50.860]  И на самом деле, я не знаю, я помню даже
[04:54:51.860 --> 04:54:52.860]  задачки давал, типа там нужно было
[04:54:53.860 --> 04:54:54.860]  что-то доказать, а оно не доказывалось,
[04:54:55.860 --> 04:54:56.860]  потому что у гамма распределения
[04:54:57.860 --> 04:54:58.860]  допустим есть адидитивность по одному
[04:54:59.860 --> 04:55:00.860]  то есть в большинстве от параметризации
[04:55:01.860 --> 04:55:02.860]  это могли в каком-то случае не доказать.
[04:55:03.860 --> 04:55:04.860]  Это?
[04:55:07.860 --> 04:55:08.860]  Допсемная.
[04:55:09.860 --> 04:55:10.860]  Так.
[04:55:13.860 --> 04:55:14.860]  Здесь что у нас будет?
[04:55:15.860 --> 04:55:16.860]  k-3, так ведь?
[04:55:17.860 --> 04:55:18.860]  В степени k-3 тоже оставим.
[04:55:19.860 --> 04:55:20.860]  Соответственно у нас θ в кубе вылезет,
[04:55:21.860 --> 04:55:22.860]  так ведь?
[04:55:23.860 --> 04:55:24.860]  Вот.
[04:55:25.860 --> 04:55:26.860]  Отсюда опять от замены 1 θ вылезает.
[04:55:27.860 --> 04:55:28.860]  То есть у нас θ в квадрате?
[04:55:29.860 --> 04:55:30.860]  Гамма от k-2.
[04:55:31.860 --> 04:55:32.860]  Да.
[04:55:33.860 --> 04:55:35.860]  То есть на самом деле мы здесь получим с вами
[04:55:36.860 --> 04:55:37.860]  θ в квадрат.
[04:55:40.860 --> 04:55:41.860]  θ в квадрат.
[04:55:42.860 --> 04:55:43.860]  k-1 на k-2.
[04:55:44.860 --> 04:55:45.860]  Ура, друзья!
[04:55:46.860 --> 04:55:47.860]  Мы почти с вами решили задачу.
[04:55:48.860 --> 04:55:49.860]  Давайте подытожим.
[04:55:50.860 --> 04:55:51.860]  Почти все.
[04:55:52.860 --> 04:55:53.860]  Вы же помните, какой у нас был план решения?
[04:55:54.860 --> 04:55:55.860]  Мы нашли с вами полную достаточную статистику,
[04:55:56.860 --> 04:55:57.860]  а затем должны были решить уравнение
[04:55:57.860 --> 04:55:58.860]  несмещенности?
[04:55:59.860 --> 04:56:00.860]  Мы почти его решили.
[04:56:01.860 --> 04:56:02.860]  У нас еще константа мешается.
[04:56:03.860 --> 04:56:04.860]  Потому что пока что математическое ожидание
[04:56:05.860 --> 04:56:06.860]  этой штуки, нет это в квадрате.
[04:56:07.860 --> 04:56:08.860]  А какое же оно?
[04:56:13.860 --> 04:56:14.860]  Да.
[04:56:15.860 --> 04:56:16.860]  То есть давайте резюмировать.
[04:56:17.860 --> 04:56:18.860]  Давайте резюмировать.
[04:56:19.860 --> 04:56:20.860]  Потому что мы потратили достаточно много времени
[04:56:21.860 --> 04:56:22.860]  на гамма-функцию.
[04:56:23.860 --> 04:56:24.860]  Я просто забыл про то, что там параметризация
[04:56:25.860 --> 04:56:26.860]  другая.
[04:56:27.860 --> 04:56:28.860]  Если вы даже на Википедию зайдете,
[04:56:29.860 --> 04:56:30.860]  там короче две параметризации есть.
[04:56:31.860 --> 04:56:33.860]  А мой товарищ третью придумал.
[04:56:44.860 --> 04:56:45.860]  Все, давайте подытожим.
[04:56:48.860 --> 04:56:49.860]  Прошу прощения за заминку.
[04:56:50.860 --> 04:56:51.860]  Ну вот, до часа и закончили как раз.
[04:56:51.860 --> 04:56:52.860]  Первое.
[04:56:53.860 --> 04:56:54.860]  Мы нашли с вами полную и достаточную
[04:56:55.860 --> 04:56:56.860]  статистику по критерию факторизации
[04:56:57.860 --> 04:56:58.860]  и по телесности множества.
[04:56:59.860 --> 04:57:00.860]  По телесности множества.
[04:57:01.860 --> 04:57:02.860]  И эта статистика у нас оказалась.
[04:57:03.860 --> 04:57:04.860]  Сумма иксов.
[04:57:09.860 --> 04:57:10.860]  Мы нашли полную и достаточную.
[04:57:11.860 --> 04:57:12.860]  А вторым шагом мы должны были решить уравнение
[04:57:13.860 --> 04:57:14.860]  несмещенности.
[04:57:15.860 --> 04:57:16.860]  Найти такую функцию phi.
[04:57:17.860 --> 04:57:18.860]  И мы нашли полную и достаточную.
[04:57:18.860 --> 04:57:19.860]  Мы должны были решить уравнение
[04:57:20.860 --> 04:57:21.860]  несмещенности.
[04:57:22.860 --> 04:57:23.860]  Найти такую функцию phi,
[04:57:24.860 --> 04:57:25.860]  что phi от суммы иксов
[04:57:26.860 --> 04:57:27.860]  будет равняться это в квадрате.
[04:57:28.860 --> 04:57:29.860]  Ну от ожидания от phi от суммы иксов.
[04:57:30.860 --> 04:57:31.860]  Мы почти решили.
[04:57:32.860 --> 04:57:33.860]  Потому что если мы возьмем phi
[04:57:34.860 --> 04:57:35.860]  1 поделить на вот это в квадрате,
[04:57:36.860 --> 04:57:37.860]  то мы получим как раз таки.
[04:57:42.860 --> 04:57:43.860]  Давайте просто домножим.
[04:57:45.860 --> 04:57:46.860]  Откуда у нас k1 и k2 обрались?
[04:57:48.860 --> 04:57:49.860]  Количество наблюдений.
[04:57:50.860 --> 04:57:51.860]  Количество чего?
[04:57:52.860 --> 04:57:53.860]  Ну количество наблюдений у нас.
[04:57:54.860 --> 04:57:55.860]  Ну да.
[04:57:56.860 --> 04:57:57.860]  Ну да.
[04:57:58.860 --> 04:57:59.860]  Ну да.
[04:58:00.860 --> 04:58:01.860]  То есть на самом деле у вас, смотрите.
[04:58:08.860 --> 04:58:09.860]  Тут вот это k1 и k2.
[04:58:10.860 --> 04:58:11.860]  Это на самом деле k.
[04:58:12.860 --> 04:58:13.860]  Прошу прощения.
[04:58:14.860 --> 04:58:15.860]  Все опять параметризация упёрлась.
[04:58:16.860 --> 04:58:17.860]  В общем вот то, что мы там k подразумевали,
[04:58:18.860 --> 04:58:19.860]  это размер выборки.
[04:58:20.860 --> 04:58:21.860]  То есть я там просто в какой-то момент
[04:58:22.860 --> 04:58:23.860]  сделал скачок.
[04:58:24.860 --> 04:58:25.860]  У нас была гамма n1 на θ,
[04:58:26.860 --> 04:58:27.860]  а потом там откуда-то появились k.
[04:58:28.860 --> 04:58:29.860]  Вот эти k это n если что.
[04:58:30.860 --> 04:58:31.860]  Прошу прощения.
[04:58:32.860 --> 04:58:33.860]  Вот тут вот этот товарищ мой придумал параметризацию.
[04:58:34.860 --> 04:58:35.860]  Прошу прощения.
[04:58:40.860 --> 04:58:42.860]  Вот ожидание получилось n-1
[04:58:43.860 --> 04:58:44.860]  на n-2.
[04:58:45.860 --> 04:58:46.860]  Вот.
[04:58:46.860 --> 04:58:47.860]  Понятно.
[04:58:49.860 --> 04:58:51.860]  По линейности математического ожидания
[04:58:52.860 --> 04:58:53.860]  какую фи мы должны взять, чтобы
[04:58:54.860 --> 04:58:55.860]  на это ожидание получилось вот такое?
[04:59:01.860 --> 04:59:02.860]  n-1
[04:59:03.860 --> 04:59:04.860]  на n-2
[04:59:05.860 --> 04:59:06.860]  1 поделить на
[04:59:07.860 --> 04:59:08.860]  сумму хоп
[04:59:09.860 --> 04:59:10.860]  в квадрате.
[04:59:11.860 --> 04:59:12.860]  То есть на самом деле эта задача,
[04:59:13.860 --> 04:59:14.860]  она не очень сложная.
[04:59:14.860 --> 04:59:15.860]  В чем ее сложность заключалась?
[04:59:16.860 --> 04:59:17.860]  Понять какую параметризацию нужно взять
[04:59:18.860 --> 04:59:20.860]  и не продолбаться в этом.
[04:59:21.860 --> 04:59:22.860]  То есть найти полную достаточную статистику,
[04:59:23.860 --> 04:59:24.860]  решить уровни несмещенности.
[04:59:25.860 --> 04:59:26.860]  Самая большая проблема здесь была с
[04:59:27.860 --> 04:59:28.860]  уровнением несмещенности, потому что вам нужно было
[04:59:29.860 --> 04:59:30.860]  вот такой вот крокодил оценить.
[04:59:31.860 --> 04:59:32.860]  То есть там
[04:59:33.860 --> 04:59:34.860]  линейностью не сделаешь, как бы никак не сделаешь,
[04:59:35.860 --> 04:59:36.860]  нужно как-то вот по определениям считать.
[04:59:37.860 --> 04:59:38.860]  Потому что иногда вот уровни несмещенности
[04:59:39.860 --> 04:59:40.860]  здесь более простые попадаются.
[04:59:41.860 --> 04:59:42.860]  Если бы нас попросили
[04:59:42.860 --> 04:59:43.860]  с уровнением несмещенности
[04:59:44.860 --> 04:59:45.860]  найти вот такой параметр,
[04:59:46.860 --> 04:59:47.860]  но это было бы очень просто.
[04:59:48.860 --> 04:59:49.860]  Это на дисперсию похоже у экспоненциального распределения.
[04:59:50.860 --> 04:59:52.860]  Ну просто там какой-нибудь второй момент
[04:59:53.860 --> 04:59:54.860]  выборочный бы подошел, нормированный,
[04:59:55.860 --> 04:59:56.860]  но вот второй момент бы какой-нибудь подошел.
[04:59:57.860 --> 04:59:59.860]  Понятно, да, в чем проблема?
[05:00:00.860 --> 05:00:02.860]  То есть это мы гораздо бы проще оценили.
[05:00:03.860 --> 05:00:05.860]  Нам дали просто здесь не очень хорошую функцию,
[05:00:06.860 --> 05:00:08.860]  для которой нужно построить оптимальную оценку.
[05:00:09.860 --> 05:00:10.860]  Поэтому нам пришлось здесь немножко ползиться,
[05:00:10.860 --> 05:00:11.860]  когда мы решали уравнение несмещенности
[05:00:12.860 --> 05:00:13.860]  через функции.
[05:00:14.860 --> 05:00:15.860]  Когда мы искали такую функцию, чтобы вот это было выполнено.
[05:00:16.860 --> 05:00:17.860]  Вот.
[05:00:18.860 --> 05:00:19.860]  Что еще?
[05:00:20.860 --> 05:00:21.860]  Вот получается
[05:00:22.860 --> 05:00:24.860]  в этих задачах, когда вас могут попросить,
[05:00:25.860 --> 05:00:26.860]  что вас могут попросить вообще сделать?
[05:00:27.860 --> 05:00:28.860]  Проверить достаточность,
[05:00:29.860 --> 05:00:30.860]  проверяется по критерию факторизации.
[05:00:31.860 --> 05:00:32.860]  По определению
[05:00:33.860 --> 05:00:34.860]  По определению, если
[05:00:35.860 --> 05:00:37.860]  просто для экспоненциальных пользуемся свойством.
[05:00:38.860 --> 05:00:39.860]  Хорошо.
[05:00:40.860 --> 05:00:41.860]  Вот.
[05:00:42.860 --> 05:00:43.860]  Найти оптимальную оценку.
[05:00:44.860 --> 05:00:45.860]  В таком случае ищете сначала достаточную оценку,
[05:00:46.860 --> 05:00:47.860]  потом проверяете, что она полная,
[05:00:48.860 --> 05:00:49.860]  либо сразу полную достаточную, если это экспоненциальное подсмещение,
[05:00:50.860 --> 05:00:51.860]  и решаете уравнение несмещенности.
[05:00:52.860 --> 05:00:54.860]  Чтобы в отождании фи от этой штуки было вот таким.
[05:00:55.860 --> 05:00:56.860]  Потеряем релемма на шафе,
[05:00:57.860 --> 05:00:58.860]  вот эта оценка у нас получилась оптимальной.
[05:00:58.860 --> 05:01:00.860]  Если есть вопросы, задавайте, пока.
[05:01:01.860 --> 05:01:02.860]  Это буквально все статьистика.
[05:01:03.860 --> 05:01:04.860]  Ещё одна задача.
[05:01:05.860 --> 05:01:06.860]  Да, кстати, есть у меня пару секретных.
[05:01:11.860 --> 05:01:12.860]  5 секретных.
[05:01:13.860 --> 05:01:14.860]  4 секретных.
[05:01:15.860 --> 05:01:16.860]  4 секретных.
[05:01:17.860 --> 05:01:18.860]  3 секретных.
[05:01:19.860 --> 05:01:20.860]  4 секретных.
[05:01:21.860 --> 05:01:22.860]  3 секретных.
[05:01:23.860 --> 05:01:24.860]  2 секретных.
[05:01:25.860 --> 05:01:26.860]  3 секретных.
[05:01:26.860 --> 05:01:28.860]  Поехали!
